 2
 
摘要 
 
視覺監控研究課題已經引發世界各地學者的興趣及大量的投入，也已經有許多成熟的技
術。至今雖然已是百家爭鳴，然而，仍有許多的技術挑戰尚待吾人繼續研究，尤其是如何佈
署大型智慧型監控系統、如何在多重攝影機間協調並傳送不同參數、以及適應各種偵測場景
或情境並提昇穩定度。本計畫重點在研究上述的三個主要的關鍵挑戰，建構“合作式無縫隙多
重距離多物件的追蹤系統”。第一年研究的重點在單攝影機上進行多重距離多目標物件的追
蹤。本計畫主要研究項目包含：運用動態條件隨機過程與混合高斯模型進行背景判斷與前景
物件擷取、探討光線變化問題、多重距離與多物件追蹤、卡曼濾波器估測、遮蔽偵測、Mean 
Shift 比對。這套系統將來可以應用在寬闊區域，長期的監控移動目標與追蹤，完成合作式智
慧型監控系統建構。 
 
關鍵詞：多物件追蹤，多重距離追蹤，多攝影機無縫隙追蹤。 
 
 
Abstract 
 
Video surveillance has been an active and important area of research. A significant amount of 
video-based detection and tracking technologies have been extensively studied in computer vision 
and related fields. There are several technical challenges that need to be addressed to enable the 
widespread deployment of intelligent surveillance system, including large system deployment, 
multi-scale handoff and collaboration, and contextual awareness, event adaptive detection and 
reliability. The objective of this project is to engage the advanced research of the above mentioned 
key issues and to build the framework of a seamless multi-distance multiple objects tracking system 
across fixed cameras. We focus on the improvement of multiple targets tracking on single camera in 
the first year. The main research issues in this project are as follows: adopting Dynamic Conditional 
Random Filed and Gaussian Mixture Model for background modeling and foreground extraction, 
exploring the illumination change problem, tracking of multiple targets for multi-distance 
environment, employing Kalman filter for prediction, handling occlusion detection, matching with 
Mean Shift, implementing the corporation and data fusion mechanism for multiple cameras. The 
resultant system can be applied to wide area campus monitoring and fulfill the collaborative 
intelligent surveillance.  
 
Keywords -- Multiple Object Tracking, Multi-distance Tracking, Multi-camera Tracking. 
 4
 
圖一：全球數位安全監控系統之產值分析(資料來源:Datamonitor) 
 
圖二：全球 DVR 市場規模及成長率 (資料來源：Frost & Sullivan) 
 
2、研究計畫目的及重要性 
 
圖三：本研究計畫示意圖—合作式無縫隙多重距離多物件追蹤系統 
 
本計畫擬建構一“合作式無縫隙多重距離多物件的追蹤系統”，如圖三所示。第一年研究的
重點在單攝影機上進行多重距離多目標物件的追蹤。這套系統將來可以使用在寬闊區域，即時
 6
機的數量超過人的負荷時，智慧型視覺監控可以提供即時且無間斷的服務。智慧型監控系統可
以根據時間和空間上的不同參數提高警示訊息正確率。影像分析和資訊擷取的基本技術已經有
許多的文獻討論，下一個階段是研究如何利用這些技術來建造一個大規模的系統建置與資訊融
合。然而，部署多部攝影機的大型系統是一項挑戰。尤其追蹤系統的目標是從監視區域擷取相
關物件的不同參數資訊，由許多不同參數的攝影機在統一的平台上做運算，藉著及時的影像分
析、多物件模型和長期的圖形分析來提供更廣泛的警示訊息。大型系統建置的問題還包括成本
降低、為了省電採用低功率的硬體設備及攝影機、自動化校正攝影機、自動化錯誤偵測、以及
發展系統管理工具。本計畫的視覺監控關鍵技術包含以下幾大項：多攝影機的協調與資料整
合、移動物偵測、移動物體分類、追蹤、與嵌入式系統實現。 
 
3.1. 多攝影機的追蹤技術 
    影像監控系統的能力通常仰賴攝影機提供的資訊完整度與否，目前在電腦視覺這一塊，視
覺追蹤技術大部分還是以單一攝影機的追蹤為主，只有少部分文獻有在探討多台攝影機的追蹤
技術。因為多台攝影機的追蹤是一個極具挑戰的題目，必須要對每台攝影機的偵測軌跡做時間
跟空間上的定位。 
    最近提出的多台攝影機的追蹤演算法大部分是利用“色彩資訊”當作場景追蹤的線索。例
如：利用場景中追蹤物的色彩投影量當作物件比對的依據[20]、當某個攝影機無法繼續追蹤物
件時，找出合適的攝影機作場景交換[21]、利用多台攝影機搭配聲音接收器將物件建構成 3D
模組[22]、或是只用多個聲音接收器接收聲音來源，對該位置做交點，達到建構物件的 3D 模
組效果[23]。因為色彩資訊容易被光源、陰影、物件破碎及攝影機控制等影響，由其是在校園
這種視野較大的地方，有些方法就必須對同步的攝影機做環境上的限制，就像[24]中提到，作
者雖然提出了攝影機利用時間與空間的資訊，作座標位置自動校正的方法，但也限制當發現相
似的追蹤物時，追蹤物不能快速的移動。 
 
3.2 多攝影機之物件比對 
在多部攝影機做物件的比對牽涉到在不同部攝影機中一系列不同影像中尋找物件之間相
符合的點，有兩個著名的方法：一為幾何方法在相同空間中根據幾何特徵轉換建立相符合點，
另一個為以辨識為基礎的方法。幾何方法的範例為 Cai 等學者[25,26]在不同攝影機中利用特徵
的位置、強度和幾何做比對，而以辨識為基礎的範例為 Krumm 等學者[22]利用彩色直方圖做
物件比對。一般來說，物件比對需要影像座標轉換為輔助，但是還是有一些方法不需要影像座
標轉換輔助，例如 Javed 等學者[27]在拍攝不同範圍的攝影機中利用空間關係去建立影像中相
符合的關係 
 
3.3 物件追蹤 
   當我們得到影像中的物件區域，我們就需要用追蹤方法在視訊系列中作物件追蹤。卡曼濾
波器(Kalman Filter)在視覺追蹤領域裡被廣泛的使用。Broida和Chellappa使用卡曼濾波器在雜
訊影像中來追蹤位置[28]。Rosales和Sclaroff使用擴充的卡曼濾波器從二維的動量(2D motion)
來估計物件的三維軌跡(3D trajectory)[29]。Comaniciu和Meer使用從圓形範圍計算出的權重直
 8
Chen [47] Multiple Motion, ellipse template,  
joint visual probabilistic data 
association filter 
Complex Tracking people 
Guler [48] Multiple 
Uncalibrated 
Adaptive background subtraction Simple, 
indoor 
Tracking object, 
handoff 
Liu [49] Catadioptric 
camera 
Feature points on human body and 
color feature 
Complex Mobile Robots vision 
Canals [50] PTZ camera Multi-block model of the object Simple drone 
表一：物件追蹤方法的文獻與相關屬性[97] 
 
4、研究方法、步驟與結果 
    本計畫重點在探討電腦視覺中的多重距離多目標追蹤。本計畫擬建構一“合作式無縫隙多
重距離多物件的追蹤系統”，如圖三所示。第一年研究的重點在單攝影機上進行多重距離多目
標物件的追蹤。 
 
 
圖五：整體計畫構面與關連圖 
 
    本計畫合作式無縫隙多重距離多物件的追蹤系統架構包含以下幾大項：前景物件擷取、前
景和陰影的判別、移動物偵測、多重距離與多目標追蹤、卡曼濾波器估測、遮蔽偵測，研究計
畫架構及子系統關聯圖如圖六。步驟分述如下： 
 10
2, )(  ),()()( exsifxnxabxg ttstt =+=                     (1) 
係數 10 ≤≤ a ， )(
,
xn ts 為獨立且期望值為零的高斯雜訊， )(2, xtsσ 代表 )(, xn ts 的變異數， )(, xn ts 是
影子的雜訊模型， )(2
,
xtsσ 等於 )(2,2 xp tbσ ，而影子的雜訊和背景的雜訊是互相獨立的，因此，影
子中點的高斯分佈強度會隨著期望值和變異數改變。同樣地，我們可以對背景的邊緣資訊建立
模型，在時間 t，背景中的區域 X，其邊緣向量 )(
,
xe tb 定義為 Tvtbhtb xexe ))(),(( ,, ，在此，
)()()(
, rtlt
h
tb xbxbxe −= 和 )()()(, dtutvtb xbxbxe −= 分別為垂直差異和水平差異，由此可知，背景模型
)(
,
xe tb 和假設的獨立背景雜訊相符合，使用平均值 )(, xteµ 和共軛矩陣∑ te x, )( ，可以計算四個相
鄰點的強度平均值與變異值。 
T
dtbutbrtbltbte xxxxx ))()(),()(()( ,,,,, µµµµµ −−=                         (2) 
∑ 





+
+
=
te
dtbutb
rtbltb
xx
xx
x
, 2
,
2
,
2
,
2
,
)()(0
0)()()(
σσ
σσ
                   (3) 
如此再加入我們在所發展的前景物件邊緣殘破修補策略，可使前景物件外型更加完整並貼近原
物件，彌補因遠距離物件模糊而產生的物件切割誤差。 
   
(a)                 (b)                  (c) 
圖七：物件切割結果: (a)輸入視訊，(b)一般的背景相減法結果產生很多雜訊及殘影，(c)
我們的方法之結果。 
 
4.2 多重距離與多目標追蹤 
    追蹤移動的物件在電腦視覺領域上是很重要的問題。影像物件追蹤演算法是利用物件在視
訊中的連續位置來追蹤物件。一般來說，物件追蹤可依其視訊連續的特性分成兩類，一種是利
用其影像的特性(紋理、區塊比對和輪廓) [51,52]；另一種是利用物理特性的動量追蹤(移動方
向和速度) [53]。雖然在近幾年，有很多的方法被提出，然而還是有許多關鍵問題仍未能被解
決。交錯就是一個很顯著的物件追蹤的問題，在追蹤的系統上就可能會遇到被靜態的場景或是
動態移動中的物件所遮掩。在本計畫中，我們提出一個新的物件追蹤演算法來處理物件部份重
疊和多種距離(遠和近)之追蹤。我們合併物件的特徵與與卡曼濾波器所預測的位置來作物件追
蹤，並以直方圖為基礎的新演算法來處理物件交錯和多種距離的移動物件追蹤。圖八顯示追蹤
演算法流程，主要分兩大部分:視訊物件一致性比對(Video Object Correspondence)和遮蔽處理演
 12
4.3 卡曼濾波器估測 
    卡曼濾波器能成功的用在不同的預測或是系統狀態決策上[54]，為了追蹤物件，我們必須
要計算出物件在每個影格的位置與速度，所以我們採用卡曼濾波器。而卡曼濾波器的運作有兩
個步驟：預測與修正。預測的步驟是採用狀態模型來預測新的狀態 ( | 1) ( 1| 1) 1ˆ ˆk k k k kX AX ω− − − −= + ，其中
( | 1)ˆ k kX − 為第k影格的狀態與共協矩陣，A為狀態 ( | 1)ˆ k kX − 從k到k-1影格的狀態轉換矩陣。 1kω − 為雜訊
並假設其為高斯(mean=0)分布。且 ( | 1) ( 1| 1) ,Tk k k kP AP A Q− − −= +  ( | 1)k kP − 為第k影格的狀態與共協矩陣，而Q
是由 1kω − 求得的共協矩陣， Tk kE Qω ω  =  。在預測步驟方面，我們用卡曼濾波器從前張到現在的
影格所推斷出的狀態來計算視訊物件的位置。接著計算新視訊物件之位置與預測前張影格之視
訊物件位置的一致性，度量 ( | 1)ˆ ,k k k kZ HX υ−= +  kZ 為在影格k的外部觀察，而H為度量矩陣
(measurement matrix)。度量雜訊(measurement noise) kυ 假設成附加的白色的高斯(mean=0)分布，
共協矩陣定義為 Tk kE Rυ υ  =  。實際上雜訊共協矩陣Q與度量的雜訊共協矩陣R會隨著時間與度量
不斷的改變，為簡化及運算速度考量，我們還是假設它們為常數。在更新步驟上，新的度量法
用下列的處理公式合併並用來更新卡曼濾波器的模型: 
1
( | 1) ( | 1)
T T
k k k k kK P H HP H R
−
− −
 = +                         (7) 
( )( | ) ( | 1) ( | )ˆ ˆ ˆk k k k k k k kX X K Z HX−= + −                      (8) 
                         ( )( | ) ( | 1)k k k k kP I K H P −= −                           (9) 
    在度量更新時，首要的任務是計算出卡曼放大係數 kK 。下一個步驟就是確實的度量出 kZ ，
然後用合併的 kZ 來更新卡曼濾波器模型的狀態。此外，我們會從第k影格且位置在x和y的物件
取得系統狀態 ( | 1)ˆ k kX − 。我們將使用卡曼濾波器有效的定位出物件，也就是說我們不需要去尋找
整張畫面，系統會在預測的座標 ( | 1)ˆ k kX − 中心定義一個收尋視窗並在其附近區域尋找。我們的卡
曼濾波器追蹤步驟程序如下： 
Step 1：最開始(k=0)。此步驟先從第一張影格中的矩形遮罩計算出位置並得到狀態 (0|0)ˆX 。初始
的容錯度( (0|0)P )設為很大。 
Step 2：預測(k>0)。此階段，我們用卡曼濾波器來預測物件的相關位置 ( | 1)ˆ k kX − ，並且再用公式
(10)提到的特徵樣板定一個收尋中心來找物件。 
Step 3：修正(k>0)。這部分我們定位出物件(位於我們在Step 2預測的 ( | 1)ˆ k kX − 附近)，並使用對此
物件比對的矩形遮罩之位置來做狀態修正。 
如果物件的追蹤是連續的，那麼Step 2與Step 3會一直執行。對於視訊物件追蹤使用卡曼濾
波器的好處是其能容忍小的交錯問題。換句話說，如果物件不在濾波器下的預測狀態附近，我
們就知道此物件可能是被某些物件所遮蓋。同時，我們就不會去更新此物件的度量修正而一直
保持此物件的預測直到找到物件為止。
 
 14
下，交錯比對函數會做修改，圖十顯示遠端比對模式(畫藍線)與近端比對模式(畫紅線)。 
圖十：多種距離的比對模式 
 
首先，我們提出下列的方式計算物件的物理特性來決定遮蔽的物件 i
n
VO 屬於哪個模式(遠距離或
近距離)： 
               
VO
Image
H 11,                  if  350  ,
H 3( )
2,                                                otherwise. 
VOi
n
or Cnt
DM VO
 ≥ ≥
= 


                (10) 
其中 ( )i
n
DM VO 定義為物件 i
n
VO 的比對模式， VOH 為物件 inVO 的高， ImageH 為輸入影像的高，而 VOCnt
為 i
n
VO 物件的影像像素數量。當物件在大到足以切成K部分，那麼我們會將其分類在近端比對
模式。在遠端的應用，我們可以很簡單的察覺到物件的尺寸小於傳統的追蹤應用，因此我們將
用兩個不同的方法去解決遮蔽問題。要實現追蹤，遮蔽物件的色彩資訊必須以機率分布表示，
所以我們用色彩直方圖來表示。我們將資料轉換成HSV色彩空間，然後我們從飽和度(saturation)
和亮度(brightness)將彩度(hue)分割出來並拿來建成我們的一維(1-D)色彩模型。彩度的機率分
布能用後投影方法(back-project method)來計算: 
                             
( )( ) ( )
255 ( ),
M c
c
I c
BPI c
δ
δ
=
= ×
                              (11) 
其中的M(c)表示物件之色彩c的彩色直方圖值，I(c) 表示收尋視窗之色彩c的彩色直方圖值，而
BPI(c)為色彩c的後投影收尋影像(back-project search image)。藉著色彩c的信賴比例度(scaled 
confidence measure)將色彩c取代各個像素，灰階的後投影收尋影像就會產生，如公式(11)。 
    平均轉移(Mean Shift)演算法是隨著機率分布的梯度(gradient)向上找來找出最近最高的樣
式(最高峰)。在我們系統，平均轉移(Mean Shift)演算法是由下列的步驟組成： 
步驟Far-1：選擇由卡曼濾波器估計出之位置與交錯物件的物理特性所衍生出的收尋視窗。 
步驟Far-2：選擇收尋視窗的初始位置。我們預設在收尋視窗的中心。 
步驟Far-3：計算收尋視窗的平均位置。 
步驟Far-4：收尋視窗的中心設在從步驟Far-3計算出的平均中心位置上。 
     
    由於在遠端比對模式時，物件通常都很小，因此資訊很少。所以為了精確的解決遮蔽問題，
我們將用前面提到的公式(10)來檢查平均位移(Mean Shift)的收尋視窗內的資訊: 
 16
卡曼濾波器的預測與 Partical Filter 的比對合併。將提出一個新演算法把交錯物件根據
物理資訊(高度與像素數量)分成兩類：遠端比對類與近端比對類。在不同的模式下，
交錯比對函數做修改。遮蔽偵測圖解如圖七。 
(3). 多重距離與多目標追蹤測試與驗證：系統採用 Visual C++ 7.0，延續目前所發展的雛
型，進一步在台北大學校園展開實地測試。 
(4). 研究成果： 
(i) 已達成衍生委託計畫案乙件----工研院安全與辨識中心。 
(ii) 行人辨識的部份已發表一篇 EI 論文 [55]: D.T. Lin* and L.W. Liu, Pedestrian 
Identification with Distance Transform and Hierarchical Search Tree , Lecture 
Notes in Artificial Intelligence , LNAI Vol. 5712, pp. 431–438, Sept. 2009。 
(iii) 多攝影機追蹤整合部份已發表一篇國際研討會論文  [56]: D.T. Lin and K.Y. 
Huang, Collaborative Pedestrian Tracking with Multiple Cameras: Data Fusion and 
Visualization, IEEE World Congress on Computational Intelligence, pp. 
1257—1264,  Barcelona, Spain, July, 2010. 
    本計畫重點在建構“合作式無縫隙多重距離多物件的追蹤系統”。第一年研究的重點在單攝
影機上進行多重距離多目標物件的追蹤，成果豐碩，包括工研院安全與辨識中心技術轉移一件
，及兩篇論文發表。本計畫主要研究核心技術包含：運用動態條件隨機過程與混合高斯模型進
行背景判斷與前景物件擷取、探討光線變化問題、多重距離與多物件追蹤、卡曼濾波器估測、
遮蔽偵測、Mean Shift 比對。這套系統可以應用在寬闊區域，長期的監控移動目標與追蹤，完
成合作式智慧型監控系統建構。 
 
參考文獻 
[1] S. J. Maybank and T. N. Tan, “Special section on visual surveillance—introduction,” 
International Journal Computer Vision, vol. 37, no. 2, pp. 173–174, 2000. 
[2] R. T. Collins, A. J. Lipton, and T. Kanade, “Introduction to the special section on video 
surveillance,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 22, pp. 745–746, 
Aug. 2000. 
[3] C. Regazzoni and V. Ramesh, “Special issue on video communications, processing, and 
understanding for third generation surveillance systems,” Proc. IEEE, vol. 89, pp. 1355–1367, 
Oct. 2001. 
[4] L. Wang, W. Hu, and T. Tan, “Recent developments in human motion analysis,” Pattern 
Recognition, vol. 36, no. 3, pp. 585–601, 2003. 
[5] A. Hampapur, L. Brown, J. Connell, A. Ekin, N. Haas,M. Lu, H. Merkl, S. Pankanti, A. Senior, 
and Y.L. Tian,“Smart Video Surveillance [Exploring the concept of multiscale spatiotemporal 
tracking ,”  IEEE Signal Processing Magazine,, Vol. 22, No. 2. (2005), pp. 38-51, 2005 
[6] S. Haritaoglu, D. Harwood and L. S. Davis, “W4: Real-Time Surveillance of People and Their 
Activities,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 
 18
International Conference on CVPR, pp. 521-527, 1999. 
[25] Q. Cai and J. K. Aggarwal, “Tracking human motion using multiple cameras,” in Proc. Int. Conf. 
Pattern Recognition,Vienna, Austria, pp. 68–72, 1996. 
[26] Q. Cai and J. K. Aggarwal, “Tracking human motion in structured environments using a 
distributed- camera system,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 
21, no. 11, pp. 1241–1247, 1999. 
[27] O. Javed, S. Khan, Z. Rasheed, and M. Shah, “Camera handoff: tracking in multiple 
uncalibrated stationary cameras,” in Proc. IEEE Workshop Human Motion (HUMO’00), 
Austin, TX, pp. 113–118, 2000.  
[28] T. J. Broida and R. Chellappa. “Estimation of object motion parameters from noisy images,” 
IEEE Trans. on Pattern Analysis and Machine Intelligence, 8(1):90–99, 1986. 
[29] R. Rosales and S. Sclaroff. “3D trajectory recovery for tracking multiple objects and trajectory 
guided recognition of actions,” Proceedings of the IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition, 2:117–123, 1999. 
[30] D. Comaniciu, V. Ramesh, and P. Meer. “Kernel-Based Object Tracking.” 2003. 
[31] A. D. Jepson, D. J. Fleet, and T. F. El-Maraghi. “Robust Online Appearance Models for Visual 
Tracking.” IEEE Trans on Pattern Analysis and Machine Intelligence, pages 1296–1311, 2003. 
[32] H. Tao, H. S. Sawhney, and R. Kumar. “Object Tracking with Bayesian Estimation of Dynamic 
Layer Representations.” IEEE trans. on Pattern Analysis and Machine Intelligence, pages 
75–89, 2002. 
[33] J. Kang, I. Cohen, and G. Medioni. “Object reacquisition using geometric invariant appearance 
model.” International Conference on Pattern Recongnition (ICPR), pages759–762, 2004. 
[34] V. Salari and I. K. Sethi. “Feature point correspondence in the presence of occlusion.” IEEE 
trans on Pattern Analysis and Machine Intelligence, 12(1):87–91, 1990. 
[35] C. J. Veenman, M. J. T. Reinders, and E. Backer. “Resolving Motion Correspondence for 
Densely Moving Points.” IEEE trans on Pattern Analysis and Machine Intelligence, pages 
54–72, 2001. 
[36] T. H. Chang, S. Gong, and E. J. Ong. “Tracking multiple people under occlusion using multiple 
cameras.” Proc. 11th British Machine Vision Conference, 2000. 
[37] S. L. Dockstader and A. M. Tekalp. “Multiple camera fusion for multi-object tracking.” Proc. 
IEEE Workshop on Multi-Object Tracking, pages 95–102, 2001. 
[38] S. Khan and M. Shah. “Tracking people in presence of occlusion.” Asian Conference on 
Computer Vision, 5, 2000. 
[39] H. Roh, S. Kang, and S. W. Lee. “Multiple people tracking using an appearance model based on 
temporal color.” Proc. International Conference on Pattern Recognition, 4:643– 646, 2000. 
[40] D. Beymer and K. Konolige. “Real-time tracking of multiple people using continuous 
detection.” IEEE Frame Rate Workshop, 1999. 
[41] M. Isard and J. MacCormick, “BraMBLe: A Bayesian Multiple- Blob Tracker,” Proc. Int. Conf. 
Pedestrian Identification with Distance
Transform and Hierarchical Search Tree
Daw-Tung Lin and Li-Wei Liu
Department of Computer Science and Information Engineering
National Taipei University
151, University Rd., San-Shia, Taipei, 237 Taiwan
dalton@mail.ntpu.edu.tw
Abstract. This work develops a novel and robust hierarchical search
tree matching algorithm, in which the Distance Transform based pedes-
trian silhouette template database is constructed for eﬃcient pedestrian
identiﬁcation. The proposed algorithm was implemented and its per-
formance assessed. The proposed method achieved an accuracy of 89%
true positive, 92% true negative and low false positive 8% rates when
matching 1069 pedestrian objects and 568 non-pedestrian objects. The
contributions of this work are twofold. First, a novel pedestrian silhou-
ette database is presented based on the Chamfer Distance Transform.
Second, the proposed hierarchical search tree matching strategy utiliz-
ing Fuzzy C-means clustering method can be adopted for mapping and
locating pedestrian objects with robustness and eﬃciency.
Keywords: Video surveillance, pedestrian identiﬁcation, distance trans-
form, pedestrian silhouette, hierarchical search tree.
1 Introduction
Pedestrian identiﬁcation and tracking is an important and yet challenging task
for video surveillance. Various surveillance applications requires the detection of
pedestrian to ensure safety or traﬃc management in daily activities of interest
such as in transportation stations, public buildings, business market places, etc..
Much progress has been made in the detection and tracking of people in the
computer vision research community. Wren et al. propose a real-time system
”Pﬁnder” for tracking people and interpreting their behavior using a multiclass
statistical model [1]. Another typical model is ”W 4” proposed by Haritaoglu
et al. [2]. W 4 is a real time visual surveillance system for detecting and track-
ing multiple people in an outdoor environment. Stauﬀer and Grimson develop
a visual monitoring system that passively observes moving objects and learns
patterns of activity [3]. Sidla et al. presents a vision based pedestrian detection
 This work was supported in part by the National Science Council, Taiwan,
R.O.C. grants NSC96-2221-E-305-008-MY3 and Ministry of Economics grant No.
97EC17A02S1032 Construction of Vision-Based Intelligent Environment (II). Mr.
Shun-Chun Lee is also commended for his work in the preliminary study.
J.D. Vela´squez et al. (Eds.): KES 2009, Part II, LNAI 5712, pp. 431–438, 2009.
c© Springer-Verlag Berlin Heidelberg 2009
Pedestrian Identiﬁcation with Distance Transform 433
(a) (b) (c) (d) (e) (f) (g) (h)
Fig. 2. Examples of diﬀerent heights of pedestrian silhouette template images: (a) 92
pixels, (b) 84 pixels, (c) 76 pixels, (d) 68 pixels, (e) 60 pixels, (f) 52 pixels, (g) 44 pixels
and (h) 36 pixels
data, we extract the edge image from the selected silhouettes by using Canny
Edge detection and then compute the distance transform for each Canny Edge
image of object silhouettes that are considered as pedestrian. Fig. 1 illustrates
the schematic ﬂow chart of the pedestrian silhouette database construction pro-
cedure.
The edge image Sin of the selected silhouettes is given as
Sin =
{
(x, y)|En(x, y) = 1 and (x, y) ⊆ Objin,
}
, (1)
where Sin denotes the edge set of the ith foreground object Objin in the nth frame
extracted from the above mentioned background substraction and shadow justiﬁ-
cation procedure. En(x, y) represents the edge image calculated via Canny Edge
Detection operation on the foreground object image. To identify pedestrians in
a video sequence, we need to collect a lot of template images of pedestrians with
diﬀerent sizes, walking directions and various shapes. In this paper, we classify
all template images into 24 categories based on diﬀerent image heights. Fig. 2
shows eight examples of diﬀerent heights.
The next step is to estimate the similarity between two images. Typically,
Euclidean distance is the traditional method of evaluating the similarity of two
images. However, the high computational complexity makes it diﬃcult to im-
plement in real-time applications. Thus, we apply the 3-4 Chamfer Distance
Transform (3-4 DT) due to its simplicity and robustness [10]. The DT equation
is shown in below.
DT (x + dx, y + dy) = min {ra + sb}, (2)
where r denotes the horizontal or vertical moving steps, s is the diagonal moving
steps and a, b are the weights of two diﬀerent steps (i.e., a = 3, b = 4 in 3-4
DT). Thus, the value of (x+dx, y+dy) of 3-4 DT is to ﬁnd the shortest distance
from (x, y) to (x + dx, y + dy).
Fig. 3 illustrates two examples of DT images. Fig. 3(a) and (d) are the orig-
inal object images, Fig. 3(b) and (e) show the edge images after Canny Edge
Detection, and Fig. 3(c) and (f) present the resultant Distance Transform of the
corresponding edge images. The distances in the DT images are intensity-coded,
i.e., the gray value of (x, y) represents the distance to the nearest edge point
where lighter color denotes larger distance and darker color indicates shorter
distance.
Pedestrian Identiﬁcation with Distance Transform 435
(a) (b) (c) (d)
Fig. 4. Example of minimum distance searching: (a) target edge image E, (b) source
edge image S, (c) EdistS,E(6, 1) = 30.7, and (d) EdistS,E(−2, 1) = 4.6
Reduce Search Size in 
Half
Find
Initial Search Center
Compute Eight Neighbors
(EDist'S,E(x+dx, y+dy))
Move to the Point with
Minimum Distance
Search Size < 1
Source
Image S
Get the Minimum
Distance
Destination
Image E
Distance
Transform
No Yes
Fig. 5. The block diagram of template image matching algorithm
than the original distance. Only when |S| and |E| are similar, the measure
EDist′S,E(dx, dy) will be a smaller value. Finally, the modiﬁed chamfer distance
EDist′S,E(dx, dy) will be more appropriate.
4 Fast Pedestrian Identification with Hierarchical Search
Tree
Matching a foreground object one by one with N template images is ineﬃcient.
We can construct a matching hierarchy with coarse-to-ﬁne search mechanism.
The idea is that each search level is coarsely divided with larger chamfer distance.
Thus, we could speed up the matching process by grouping similar template im-
ages together and representing them by an instance image, rather than matching
the individual template images one by one. The grouping algorithm is performed
in various levels and results in a hierarchy search tree. The clustering is achieved
iteratively by utilizing a top-down approach and applying a ”Fuzzy C-means”-
like algorithm at each level of the hierarchical structure. The distance measure
of Fuzzy C-means is replaced with the chamfer distance EDist′S,E(dx, dy) and
the cluster center is chosen as
Ci = min
S∈Gi
∑
E∈Gi
EDist′S,E(dx, dy), (5)
where Ci denotes the center of ith cluster and Gi is the set of ith cluster.
To identify each object Objin extracted from the current video frame, the
system will read the pedestrian silhouette database given by Section 2 and create
the hierarchical search tree for recognition. Then we compute the Canny edge
image SObjin of the ith object by using Equation (1) and the distance transform
Pedestrian Identiﬁcation with Distance Transform 437
Table 1. Experimental results of pedestrian identiﬁcation
1000 Frames True False
Positive 996 (89%) 122 (8%)
Negative 446 (92%) 73 (11%)
(a) (b) (c) (d)
Fig. 8. Some detection results of pedestrian identiﬁcation: (a) and (b) show the results
of pedestrians; (c) and (d) show the results of non-pedestrian objects
24 scales according to their height ranging from 20 to 112 pixels with step size
4 pixels. Thus, we obtained a hierarchical search tree. Fig. 7 shows a partial
view of the hierarchical search tree. We can observe from Fig. 7 that the objects
possess high similarity in the leaf level.
The experiments were conducted on another 1000 frames video sequence which
was distinct from the template sequence. The test video clips contains 1069
pedestrian objects and 568 non-pedestrian objects. The proposed system results
in a detection rate of about 89% true positive, 92% true negative and low false
positive 8% rate as shown in Table 5. Fig. 8 demonstrates a few detection results
on several test video clips in which label ”P1” denotes the object is identiﬁed as
a pedestrian. The number follows label ”P1” is the object ID. Notably, Fig. 8(a)
shows that our identiﬁcation method can ﬁnd pedestrian even with occlusion
occurs (indicated in green bounding boxes). Figures 8(c) and (d) show the iden-
tiﬁcation of non-pedestrian with people riding the bicycle (object ID:9) and
moving vehicle (object ID:1), respectively.
6 Conclusion
This work has two principal contributions. First, an oﬀ-line pedestrian silhouette
database is constructed based on 3-4 Chamfer Distance Transform. The second
contribution is that this work presents a novel hierarchical search tree matching
strategy utilizing Fuzzy C-means clustering method. Therefore, the proposed
scheme can be adopted for various size of pedestrian matching in robust and
eﬃcient manner. The proposed method achieved an accuracy of 89% true pos-
itive, 92% true negative and low false positive 8% rates when matching 1069
pedestrian objects and 568 non-pedestrian objects. The proposed method can
be easily extended to people tracking and counting applications in surveillance.
Collaborative Pedestrian Tracking with Multiple Cameras: Data
Fusion and Visualization
Daw-Tung Lin, Member, IEEE and Kai-Yung Huang
Abstract— Multi-camera tracking is a current trend in video
surveillance. This paper proposes a framework for a collabora-
tive multiple-camera tracking system for seamlessly tracking
pedestrians across adjacent cameras. This study develops a
system consisting of several single camera tracking clients and
an information fusing server, and then use a TCP/IP network
to exchange information between tracking clients. This work
inspires a paradigm of human visual perception, collaboration
and fusion through distributed cameras and computers. The
proposed system is described in two sections corresponding to
the two major elements of the system: A client part responsible
for single camera object detection and tracking, and a server
part responsible for the multiple cameras collaborative tracking
on the other hand. To improve the performance of moving
pedestrian matching, gait analysis is adopted based on the feet
distance change of the moving objects. Furthermore, this study
proposes a cameras switching algorithm to determine whether
or not the pedestrian has left the field of view. Simulation results
show that the developed system performs object matching
and seamless tracking in various environments robustly. The
tracking accuracy is as high as 96.9% and 99.7% for two test
video sequences, respectively. The resulting system is promising
and can be applied to wide-area monitoring and collaborative
intelligent surveillance.
I. INTRODUCTION
Video surveillance is an active and important area of
research. Object tracking is one of the major components
of video surveillance system. Though a significant amount
of tracking technologies have been extensively studied in
computer vision fields, several technical challenges must
still be addressed to enable the widespread deployment of
intelligent surveillance systems. The objective of this study is
to build a framework for collaborative and continuous track-
ing across fixed cameras for multiple objects. Hu et al. [1]
categorized a multiple-camera tracking system into several
parts, including installation, calibration, camera switching,
and object matching.
The goal of camera installation is to solve the problem of
covering the entire area with the minimum number of cam-
eras. Pavlidis et al. proposed a solution for multiple camera
installation for parking lot safety monitoring [2]. Calibration
is a key process for multiple cameras tracking, and provides
correspondence between different views. Homography is a
popular technique for matching the correspondence between
each view plane [3], [4], [5], [6]. Stein applied motion
This work was supported in part by the National Science Council, Taiwan,
R.O.C. grants NSC98-2221-E-305-005 and Ministry of Economics grant No.
98EC17A02S1032 Construction of Vision-Based Intelligent Environment
(II).
Daw-Tung Lin is with the Department of Computer Science and Informa-
tion Engineering, National Taipei University, Sanshia, 23741 Taipei County,
Taiwan (email: dalton@mail.ntpu.edu.tw).
trajectory to fit the projection matrix of each view [7].
Camera switching considers the hand-off issue with adjacent
cameras when the object moves across different views. Cai
et al. established a tracking confidence measure for each
object [3]. When the confidence of the object is lower
than a pre-set threshold, the system searches for the highest
confidence in all cameras for object continuous tracking.
Javed et al. used FOV (field of view) lines characteristics
to determine whether or not the system should replace the
camera to track the object. Numerous techniques have been
proposed for object matching. Fukuda et al. used hair color
to analyze persons and installed cameras on the ceiling
to handle occlusion and the overlook an indoor area [8].
Cai and Aggarwal performed object matching using the
Multi-variance Gaussian function, in which the features of
objects are substituted into this function and the variables
are adjusted based on the physical characteristics of the
object [9]. Zhu et al. used color dissimilarity to estimate
the likelihood and to match two object with appearances [4].
Koutsia et al. presented a grid-based fusion approach, which
determined whether the object is equal by the grid-based
position of a ground map [10]. Ringer et al. established a
kinematic model to analyze the parts of arms and legs, which
can detect the slight variation of limbs movement [11]. Khan
et al. used Bayes law to estimate the matching probability
of each candidate. In this approach the maximum likelihood
is computed with all cameras, and contrast between objects
is determined by position information [5]. Zhu et al. used
the distance between FOV lines and the feet of the object
to predict when the object steps into the view of another
camera. They also adopted the color histogram for identify-
ing objects [4]. Javed et al. presented the space of brightness
transfer function for a pair of cameras. This technique adjusts
the disparity of color in two cameras and uses the color
information and space-time features of the object to integrate
it in non-overlapping areas [12]. Kettnaker and Zabih adopted
Markov probabilities and a decomposable model to compare
the relation of objects [13].
This study constructs a system consisting of several single
camera tracking clients and a fusing objects server. The
proposed system uses the TCP/IP protocol to exchange
information and enable fusion. For single camera tracking,
we propose a background update method to deal with back-
ground changes. To obtain more precise object segmentation,
Canny edge detection is also applied to improve foreground
segmentation results. Next, shadow removal is utilized to
constrain the shadow effect. Meanwhile, the Kalman filter is
applied to track the object. To solve the occlusion problem,
WCCI 2010 IEEE World Congress on Computational Intelligence 
July, 18-23, 2010 - CCIB, Barcelona, Spain IJCNN
978-1-4244-8126-2/10/$26.00 c©2010 IEEE 1257
C. Shadow Detection
In fact, the objects obtained by the foreground segmen-
tation method can also include shadows, which reduces
segmentation accuracy. To resolve this problem, this study
adopts the characteristics of shadow low brightness and
similar chroma. The shadow Ishadow(x, y) is distinguished
using the HSV color space and based on the rules described
below:
Ishadow(x, y) =

1, if α ≤ IV (x,y)BV (x,y) ≤ β
and |IS(x, y)−BS(x, y)| ≤ TS
and |IH(x, y)−BH(x, y)| ≤ TH
0, otherwise.
,
(6)
where IV (x, y), IS(x, y), and IV (x, y) denote the fore-
ground pixel values, and BH(x, y), BS(x, y), and BV (x, y)
represent the background image pixel values in the HSV
color space, respectively. α and β delineates the range of
similarity in V, while TS and TH permit the chroma with
small distortion [16].
D. Object Tracking
Moving object tracking is an essential task in vision
surveillance. After obtaining foreground objects, multiple
objects are matched one by one according to their appearance
features. The similarities between pedestrians are measured
by their color histograms, positions, object sizes, and gait
analysis (as Section II-E illustrates). This study uses the
Kalman Filter to track the position and moving vector of
pedestrians [17]. In essence, the Kalman Filter uses the state
model to predict the new state of motion status. This process
is divided into two parts: prediction and correction.
Xˆ(k|k−1) = A(k;k−1)Xˆ(k−1|k−1) + wk−1 (7)
P(k|k−1) = A(k;k−1)P(k−1|k−1)AT(k;k−1) +Qk−1, (8)
where Xˆ(k|k−1) is the state at frame k, A(k;k−1) denotes the
transition matrix of the linear dynamic system governing the
error, wk−1 is an independent Gaussian random process of
n-vectors (same dimension as Xˆ), P(k|k−1) is the covariance
matrix at frame k, Qk−1 = E(wk−1w′k−1).
One advantage of using the Kalman Filter is that it can
tolerate small occlusions and the disappearance of objects.
If the object is not found in the predicted state, the system
performs predictions repeatedly until the object is found
in the current frame. This characteristic of the Kalman
Filter helps solve the occlusion problem. The algorithm
for resolving occlusion problem has been detailed in our
previous study [15].
E. Gait Feature Analysis
In addition to color histogram, position, and object size
information, gait feature analysis is used to identify the
similarities between pedestrians for tracking. First, gait in-
formation is obtained from individual pedestrian motions.
Gait recognition has recently gained significant attention
from many researchers [18], [19], [20]. This study uses
the Canny edge detection to determine the contour of a
 ,c cx y x
y
(a) (b) (c)
Fig. 2. (a) Object contour. (b) Object centroid and the coordinates. (c) The
resultant object skeleton.
pedestrian’s silhouette. Then, the centroid (xc, yc) of those
boundary is obtained. A simple skeletonization technique
is used to search on the boundary points and pick the
maximum distance from the centroid. Thus, we choose three
points on the boundary with the largest distance in the upper
body, left foot, and right foot. Figure 2 shows the results
of the pedestrian skeleton. Generally, distance sequence
of feet skeletons denote the characteristics of the moving
pedestrians. However, the distances between feet may include
abrupt change due to period estimation error. Low-pass filter
is then applied to filter out the noise by low pass filter in
frequency domain [21], [22]. For example, we analyze one
pedestrian walking through a gallery with occlusive effect. In
this case, the feet and upper body may be covered by some
lamps, generating noise in the distance sequences. Figure 3
demonstrates that this method can deal with different walking
posture sequences and can help compute the gait period
information after the noise is filtered out.
After smooth curve of distance is obtained, the extreme
points can be determined using a simple equation:
λ =
{
i ∈ N | d˜ [i] > d˜ [i− 1] ∧ d˜ [i] > d˜ [i+ 1]
}
, (9)
where λ is a set of extremal points, and d˜ [i] is the smoothed
distance value of index i. Afterward, set λ is a time sequence
of feet distance change and represents the gait feature. In
practice, objects could be occluded or distorted due to noise
or in-perfect segmentation. Thus, low-pass filter is necessary
to filter out noise. Figure 4 presents the correct experimental
results of gait feature tracking for both small and large
objects.
III. MULTIPLE CAMERAS TRACKING
Multiple camera collaborative tracking is a trend in mod-
ern computer vision, and especially for wide-area video
surveillance. However, the object appearing in one camera
view is often different from that shown by other cameras.
Therefore, we must adjust the 3-D axes of each camera.
However, there are also other problems in multiple camera
tracking systems, such as camera switching, object matching,
and occlusion handling.
A. System Architecture
Surveillance tracking of multiple video objects over a
wide area requires cameras to work cooperatively and syn-
chronously as a single network. Multiple-camera tracking
1259
Fig. 7. The snapshot of the implemented homography algorithm. The lower
wide picture presents the stitched image of two different views in the upper
part of the figure.
can place
√
a2i + b
2
i = 1. Then, we constrain ρ > 0.
After normalization process, the line is transformed as li :
mix− y + di = 0, where mi = sinθcosθ , di = ρcosθ . Following
Dimitropoulos’s method, we define the lines, lV : mV x −
y+dV = 0 and lG : mGx−y+dG = 0, in the FOV and the
true coordinates, respectively. Since the corresponding line-
based matrix equation can be considered as A = Mb, the
corresponding matrix M can be estimated by least-square
method M = [ATA]−1AT b. In practice, the precise set
of matching datum ensures homography accuracy. However,
this method does not always work ideally. When the positions
of matching points are located exactly on a line, the process
fail to finish the correspondence of homography. Because the
basic concept of homography involves reaching the goal of
plane matching, a single line is incapable of generating plane.
Thus, to avoid the problem mentioned above, the points are
not picked on the same line. Figure 7 presents the result of
implementing the homography algorithm.
C. Camera Switching
To maintain the labels of detected objects across multiple
cameras, we need to keep track of the same objects and
switch between corresponding cameras. We utilize the FOV
lines for camera switching for overlapping views [25]. Let
Ci be the view of camera i. In each camera, the FOV is a
rectangular plane within the boundaries, and the boundaries
is presented as four lines: x = 0, x = width, y = 0, and
y = height on the image plane, called FOV lines. After
the FOV line and plane are mapped into the coordination of
real world location, the FOV line and the center of plane are
defined as lmi and Cicenter , respectively, where lmi denotes
the m-th FOV line in plane i, and Cicenter represents the
center of plane i. The advantage of this approach is that it
can find the next camera and track the object quickly. The
proposed process includes the following steps:
1) Use Cicenter to find the relative position of each plane.
2) If the coordinates of object Ok is in the range of plane
i, then define the object as Oik. Estimate when the
object will arrive at the FOV line lmj in plane j by
computing the moving vector.
3) If the object steps on the FOV line lmj , then the object
immediately enters the plane j. Thus, switch to the
next camera for tracking.
4) Refresh the object as Ojk based on the new location.
Figure 8 illustrates this camera switching algorithm. In
Fig. 8(a), two planes i and j are transformed from the
cameras Ci and Cj . If the object arrives at the FOV line lleftj ,
this event is denoted as Slefti,j , meaning that the object steps
on line lleftj of plane j from plane i. At this time, the system
prepares to switch to camera j, while the object is currently
tracked by camera i. When the object arrives at the line lrighti
as Fig. 8(b) shows, the object prepares to leave the plane i.
Then, the system tracks the same object using camera j, and
defines the object as Oj . This camera switching not only
helps determine the status of moving objects, but also keep
retained identified objects. Algorithm 1 explicitly describes
the camera switching process. When Osk=1, the object Ok
enters the overlapping area and prepares to switch to the
next cameras.
D. Object Matching
The basic issue of tracking objects in multiple views is
how to distinguish each object in different cameras. The
similarity of the same object should be estimated by features
in adjacent frames. Several pedestrian features including
2-D location (L), color histogram (C), and gait motion
period (P ) are used for object matching. We apply a Multi-
variance Gaussian model to match the pedestrian in different
cameras [9], [3]. The similarity of each feature is computed
as follows:
PL (Oi | σ) = 1(2pi)σ2 exp
(
− (xi−xj)2+(yi−yj)22σ2
)
,
PC (Oi | σ) = 1
(2pi)
3
2 σ2
exp
(
− (ri−rj)2+(gi−gj)2+(hi−hj)22σ2
)
,
PP (Oi | σ) = 1
(2pi)
1
2 σ2
exp
(
− ((pi−pj)22σ2
)
,
(11)
where PL, PC , and PP represent the relation of location
((xi, yi)), colors (ri, gi, and hi) and gait period (pi) features,
respectively, for objects Oi and Oj , and i 6= j. We define
each group P (Oi | σ) as a standard Gaussian model with a
zero mean and a variance 1. This process transforms three
relations into probabilities and computes the relation between
each object in the previous and current frames. Based on the
Bayes’ theorem, the relative function R(Oi, Oj) is defined
as the maximum likelihood:
R(Oi, Oj) = PWLL (Oi | σ)PWCC (Oi | σ)PWPP (Oi | σ) ,
(12)
where Oi and Oj denote the objects appearing in different
cameras, and WL, WC , and WP are the weights of each
feature group. The probability relation can be further trans-
formed into the distance form: Di,j = − log(R(Oi, Oj)).
1261
(a) Near distance case (b)Far distance case
Fig. 10. The implemented multiple camera tracking system performed at different distances.
Video Video Features Correct Missing False Correct
clips length(min:sec) tracking tracking tracking fraction(%)
Video 1 5:27 P +D 9253 14 419 95.5%
P +D + C + Pd 9387 13 286 96.9%
Video 2 6:12 P +D 11275 11 33 99.6%
P +D + C + Pd 11287 1 31 99.7%
TABLE I
PERFORMANCE EVALUATION BY FRAMES.
Video Features False Missing
clips tracking tracking
Video 1 P +D 8/33 2/33
P +D + C + Pd 7/33 0/33
Video 2 P +D 1/28 1/28
P +D + C + Pd 1/28 0/28
TABLE II
PERFORMANCE EVALUATION BASED ON PEDESTRIAN LONG-TERM TRACKING.
is proposed to deal with background changes. To obtain
more precise object segmentation, the Canny edge detection
technology is used to improve the foreground segmentation
results. Next, shadow removal is utilized to reduce the
shadow effect. Meanwhile, the Kalman filter is applied to
track the object. To solve the occlusion problem, we apply
Kalman filter prediction to estimate the position of the
object. To improve the performance of moving pedestrian
matching, gait analysis is adopted. Each camera conveys the
features of pedestrian to match multiple pedestrians from
multiple cameras. The homography of camera FOV mapping
is computed when the camera is connected to the server. Fur-
thermore, this study proposes a camera switching algorithm
and uses it to determine whether or not the object has left
the FOV. Simulation results show that the proposed system
can fulfill the pedestrian matching and seamless tracking
in various environments robustly. The tracking accuracy is
as high as 96.9% and 99.7% for two test video sequences,
respectively. The resulting system is promising and can be
applied to wide-area monitoring and collaborative intelligent
surveillance.
REFERENCES
[1] W. Hu, T. Tan, L. Wang, and S. Maybank. A survey on visual
surveillance of object motion and behaviors. IEEE Transactions on
Systems, Man, and Cybernetics, Part C: Applications and Reviews,
34(3):334–352, 2004.
[2] I. Pavlidis, V. Morellas, P. Tsiamyrtzis, and S. Harp. Urban surveil-
lance systems: From the laboratory to the commercial world. Pro-
ceedings of the IEEE, 89(10):1478–1497, 2001.
[3] Q. Cai and J.K. Aggarwal. Tracking human motion using multiple
cameras. In International Conference on Pattern Recognition, vol-
ume 13, pages 68–72, 1996.
[4] L.-J. Zhu, J.-N. Hwang, and H.-Y. Cheng. Tracking of Multiple Objects
Across Multiple Cameras with Overlapping and Non-Overlapping
Views. In IEEE International Symposium on Circuits and Systems,
pages 1056–1060, May 2009.
1263
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                日期：99 年 8 月 12 日 
一、 參加會議經過 
 
IEEE World Congress on Computational Intelligence(WCCI)國際研討會於99/7/18—99/7/23在西
班牙巴塞隆納召開。論文收錄於EI. IEEE WCCI 2010今年結合三個國際知名的研討會於一
身:2010 IEEE International Conference on Neural Netwroks (IJCNN 2010), 2010 IEEE International 
Conference on Fuzzy Systems (FUZZY-IEEE 2010)及2010 IEEE Congress on Evolutionary 
Computation (IEEE CEC 2010)。本次會議收到2525篇論文投稿，經審查結果，最後接受1715篇
論文，分口頭報告（oral presentation）及58展示報告(poster presentation)約二比一，論
文接受率約68%。 
   
 
計畫編號 NSC 98-2221-E-305 -005 
計畫名稱 無縫隙多重距離多物件追蹤與嵌入式系統實作 
出國人員
姓名 林道通  
服務機構
及職稱 國立台北大學資工系教授 
會議時間 
99 年 7月 18 日
至 
99 年 7月 23 日 
會議地點 西班牙 巴塞隆納 
會議名稱 IEEE World Congress on Computational Intelligence 
發表論文
題目 
Collaborative Pedestrian Tracking with Multiple Cameras: Data 
Fusion and Visualization 
無研發成果推廣資料 
