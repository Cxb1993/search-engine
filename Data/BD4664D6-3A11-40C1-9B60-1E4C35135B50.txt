- 2946 -
A CMAC-Q-Learning Based Dyna Agent 
 
Yuan-Pao Hsu1  Wei-Cheng Jiang2  Hsin-Yi Lin3 
 
1,2Department of Computer Science and Information Engineering, 
National Formosa University, 
Yunlin, Taiwan 632, R.O.C. 
1hsuyp@nfu.edu.tw 
2enjoysea0605@yahoo.com.tw 
3Robotics Control Technology Department, 
Intelligent Robotics Technology Division, 
Industrial Technology Research Institute, 
Hsinchu, Taiwan 310, R.O.C. 
alieta@itri.org.tw 
 
Abstract: In the this paper, a CMAC-Q-Learning based Dyna agent is presented to relieve the problem of learning 
speed in reinforcement learning, in order to achieve the goals of shortening training process and increasing the learning, 
speed. We combine CMAC, Q-learning, and Prioritized sweeping techniques to construct the Dyna agent in which a 
Q-learning is trained for policy learning; meanwhile, model approximators, called CMAC-Model and 
CMAC-R-Model, are in charge of approximating the environment model. The approximated model provides the 
Q-learning with virtual interaction experience to further update the policy within the time gap when there is no 
interplay between the agent and the real environment. The Dyna agent switches seamlessly between the real 
environment and the virtual environment model for the objective of policy learning. A simulation for controlling a 
differential-drive mobile robot has been conducted to demonstrate that the proposed method can preliminarily achieve 
the design goal. 
 
Keywords: Q-learning, CMAC, Dyna Agent 
 
1. INTRODUCTION
Reinforcement learning, or RL for short, method 
focuses on learning the mapping relationship, known as 
policy, between ‚Äústates‚Äù and ‚Äúactions‚Äù such that the 
largest accumulated reward is attainable from its 
successive interactions with environment by following 
the policy [1]. Rather than the supervised learning 
neural network needs desired output for computation, 
the RL method acquires optimal or near optimal policy 
solely from interplays with the world, making it is 
applicable to tasks such as machine learning, pattern 
recognition, decision making, etc. 
The RL can be trained according to a series of 
trial-and-error process, however the time of the training 
is usually tedious before the policy converges. This 
drawback prevents the RL from practical applications on 
real-time control in the real world. To deal with this 
learning time problem, the Dyna algorithm was hence 
proposed [1] [2]. 
Based on the original Dyna concept we propose a 
modified architecture, called CAMC-Q-Learning based 
Dyna agent, in which the following techniques are 
included: (1) CMAC [3], (2) Q-learning [4], (3) 
Prioritized sweeping [5]. In the architecture, the CAMC 
is used for model learning, the Q-learning is the core for 
policy learning, and the Prioritized sweeping is used for 
speeding up the policy learning. The main goal of the 
proposed Dyna architecture focuses on speeding up the 
learning time in order that it, with its simple structure, 
can be applicable to the real world tasks. 
 
2. BACKGROUND 
2.1 MDPs 
A reinforcement learning task that satisfies the 
Markov Property is called a Markov Decision Process, or 
MDP. A particular finite MDP is defined by its state and 
action sets and by the one-step dynamics of the 
environment. Given any state and action, s and a, the 
transition probability of each possible next state, s‚Äô is 
p(s‚Äô| s, a). Also, the expected reward of executing action, 
a, in current state, s, transiting to next state, s‚Äô, is r(s, a, 
s‚Äô). 
The objective is to find a policy which at time step t 
selects an action at given the state st and maximizes the 
expected discounted future cumulative reward, or return: 
rt + »ñrt+1 + »ñ2rt+2 +..., where »ñ ¬è [0, 1] is a factor called 
discount rate. 
 
2.2 Q-Learning 
One of the breakthrough developments for RL was 
the presentation of Q-learning. Characterized being a 
model-free algorithm, Q-learning learns values of 
state-action pairs, Q, in order to approximate Q*, the 
values of optimal state-action pairs. The corresponding Q 
is updated by 
)),()','(max (),(),(
'
asQasQrasQasQ
a
 JE     (1) 
where E is learning rate. Even the algorithm has been 
SICE Annual Conference 2008
August 20-22, 2008, The University Electro-Communications, Japan
PR0001/08/0000-2946 ¬•400 ¬© 2008 SICE
- 2948 -
3. COMBINED ARCHITECTURE 
3.1 CMAC-Model 
CMAC-Model (Fig. 5) is responsible for building 
environment model. It maps input, current state-action 
pair (s, a), from real experience into the memory to 
output the actual output, (s‚Äô, r), and adjusts the memory 
to approximate the actual output to the desired output 
according to the error between them. 
The CMAC-Model acts as a virtual environment for 
replacing real environment, providing the system with 
the simulated experience. Successively retrieving a series 
of (s, a) pair from a queue, which is constructed by the 
Prioritized sweeping method, as its input the 
CMAC-Model outputs a series of (s‚Äô, r) pair for the 
planning update. Fig. 6 lists the algorithm in which D 
represents learning rate of the CMAC and c represents 
the number of hit memory cells per (s, a) pair. 
 
.
.
.
.
.
.
Learning
Rule
.
.
.
-
+
Actual
(s‚Äô, r)
Desired
(s‚Äô, r)
Update
MemoryTiling 1
Tiling 2
Tiling N
(s, a)
¬¶
 
Fig. 5 CMAC-Model architecture 
 
Fig. 6 CMAC-Model algorithm 
 
3.2 CMAC-R-Model 
The inverse model of environment is another key 
factor for the Dyna agent to work as it expected to be. 
Generally, the agent interacts with real world should 
inevitably produce cyclic sequence path as shown in Fig. 
8, for simplicity the subscript in the figure denotes time 
step only. For instance, a robot at state sn executes action 
an, say turn right, transiting to state sn+1, then executes 
action an+1, say turn left, consequently the robot goes 
back to a state the same as the state sn. If the inverse 
model approximator uses only state, si, as its input, the 
backward tracing sequence would be like sn+2 √Ü (sn, 
an+2); sn √Ü (sn+1, an+1); sn+1 √Ü (sn, an); sn √Ü (sn+1, an+1), 
meaning that the backward tracing is trapped in the 
cyclic path sn √Ü (sn+1, an+1), sn+1 √Ü (sn, an) as illustrated 
in Fig. 7. That the path from state-action pair, (sn-1, an-1), 
is never backward traceable. Hence, The CAMC-R- 
Model in our design concatenates state, s, and action, a, 
to be its input in order to cope with environment with 
cyclic path to obtain the correct predecessor. 
The architecture and algorithm of the CAMC-R- 
Model is just the same as the CMAC-Model (Fig. 5) 
except that the output (s‚Äô, r) is replaced by (s, a) which 
stands for the predecessor of the input state-action pair, 
(s, a).  
 
sn-1 sn sn+1an-1 anan-2
an+1
sn+2
an+2
 
Fig. 7 Cyclic path 
 
3.3 The Combined Architecture 
Based on the Dyna-Q, Fig. 8 illustrates the 
architecture of the proposed Dyna agent. In the design, 
we use CMAC to implement the model learning and the 
Prioritized sweeping concept for the search control by in 
contrast with Fig. 3. The Q-learning is trained by using 
real experience acquired from the interaction between the 
agent and environment; meanwhile, the CMAC-Model 
(CMAC-R-Model) constructs an environment model 
according to the real experience as well. The constructed 
environment model provides the Q-learning with virtual 
interaction experience to further update the policy and 
value function of the Q-learning within the time interval 
when there is no interplay between the agent and the real 
environment. The Dyna agent switches seamlessly 
between the real environment and the virtual 
environment model for the objective of learning from 
‚Äúwithout‚Äù to ‚Äúwith‚Äù rapidly.  
 
Q-learning
Environment CMAC-Moddel & CMAC-R-Model
Real 
experience
Simulated 
experience
Direct RL 
update
Planning update
Prioritized 
sweeping
Model 
learning
 
Fig. 8 Proposed Dyna agent architecture 
 
The algorithm of the agent is depicted as a flowchart 
as shown in Fig. 9. The main difference between our 
design and the Dyna-Q is that the planning update is not 
Clear memory, ws,a(F) = 0, where s ¬è states, a ¬è actions, 
F¬è CMAC-tiles 
Do forever: 
(a) If model learning: (s, a) √Ö environment  
              else: (s, a) √Ö first(PQueue) 
(b) F= features(s, a)              (hit memory in CMAC) 
(c) (s‚Äô, r) = sum(F)                            (output) 
(d) If model learning:       (update contents of hit memory) 
ws, a(F) = ws, a(F) + D/c[desired(s‚Äô, a) ‚Äì actual(s‚Äô, a)]  
(e) go to (a) 
