Bounds on the Capacity of the
Additive Inverse Gaussian Noise
Channel
Intermediate Report of NSC Project
“Capacity Analysis of Multiple-Users Communication
Systems using Joint Estimation and Detection”
Date: 31 May 2012
Project-Numbers: NSC 99-2628-E-009-002
NSC 100-2628-E-009-003
Project Duration: 1 August 2010 – 31 July 2013
Funded by: National Science Council, Taiwan
Author: Stefan M. Moser
Organization: Information Theory Laboratory
Department of Electrical and Computer
Engineering
National Chiao Tung University
Address: Engineering Building IV, Office 727
1001 Daxue Rd.
Hsinchu 30010, Taiwan
E-mail: stefan.moser@ieee.org
Contents
1 Introduction 3
2 Channel Model 3
3 Mathematical Preliminaries 5
4 Known Bounds on Capacity 6
5 New Bounds on Capacity 7
6 Asymptotic Capacities 10
7 Proof Outlines 11
7.1 Upper Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
7.2 Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
8 Discussion 12
Bibliography 12
2
• We only consider a one-dimensional setup where the position of the molecule
is described by a single random coordinate W . The transmitter is set at
coordinate 0 and the receiver is along the moving direction of the fluid at a
certain distance d. Without loss of generality we will set d = 1.
• The fluid is moving with constant drift velocity v > 0.
• Once the molecule reaches W = d, it is absorbed and not released again.
• The transmitter and receiver are perfectly synchronized and have potentially
infinite time to wait for the arrival of the molecules.
• There are no other molecules from the same type interfering with the commu-
nication.
• The channel is memoryless, i.e., the trajectories of individual information car-
rying molecules are independent. Moreover, in case a molecule overpasses
another, the receiver still can distinguish between them and put them into
correct order.
The basic ideas behind these simplifications are related to Shannon’s approach when
he introduced the additive Gaussian noise channel [2] and also focused solely on the
impact of the noise, but neglected many other aspects like delay, synchronization,
or interference.
A Wiener process is described by independent Gaussian position increments, i.e.,
for any time interval τ , the increment in position ∆W is Gaussian distributed with
mean vτ and variance σ2τ . The increments of nonoverlapping intervals are assumed
to be independent. Here, v denotes the drift velocity of the fluid and σ2 is a channel
parameter that describes the strength of the Brownian motion and relates to the
viscosity of the fluid, the temperature, the size and structure of the molecules, etc.
In our setup of the communication, the positions of transmitter and receiver are
fixed, i.e., we need to “invert” the Wiener process to describe the random time it
takes for the molecule to travel from position 0 to position d = 1. This random time
N has an inverse Gaussian distribution1 that is described by its probability density
function (PDF)
fN (n) =
√
λ
2pin3
exp
(
−λ(n− µ)
2
2µ2n
)
I{n > 0}. (1)
Here, I{·} denotes the indicator function that takes on the values 1 or 0 depending
on whether the statement holds or not. The PDF (1) depends on two parameters:
µ denotes the average traveling time
µ =
d
v
=
1
v
(2)
and λ relates to the Brownian motion parameter σ2 via
λ =
d2
σ2
=
1
σ2
. (3)
Usually we write N ∼ IG(µ, λ).
1The name is a bit unfortunate: it is called “inverse” because we have “inverted” the problem
of random position for given time to random time for given position. However, an inverse Gaussian
random variable has nothing to do with 1/G for G being Gaussian distributed.
4
In particular, this means that
E[N ] = µ, (12)
E
[
1
N
]
=
1
µ
+
1
λ
, (13)
E
[
N2
]
= µ2 +
µ3
λ
, (14)
E
[
1
N2
]
=
1
µ2
+
3
λ2
+
3
µλ
, (15)
Var(N) =
µ3
λ
, (16)
Var
(
1
N
)
=
1
µλ
+
2
λ2
. (17)
Moreover, we have
E[logN ] = log µ+ e
2λ
µ Ei
(
−2λ
µ
)
. (18)
Similarly to Gaussian random variables, inverse Gaussians are closed under scal-
ing: for any α > 0,
N ∼ IG(µ, λ) =⇒ αN ∼ IG(αµ, αλ). (19)
However, while the sum of two independent Gaussians is Gaussian again, this prop-
erty only holds for inverse Gaussians with similar parameters: if
λ1
µ21
=
λ2
µ22
, (20)
then it holds that for N1 ∼ IG(µ1, λ1) and N2 ∼ IG(µ2, λ2) with N1 ⊥N2,
N1 +N2 ∼ IG
(
µ1 + µ2,
(√
λ1 +
√
λ2
)2)
. (21)
Finally, it is interesting to note that the inverse Gaussian distribution is differential-
entropy maximizing when the following three constraints are given:
E[N ] = α1, (22)
E
[
N−1
]
= α2, (23)
E[logN ] = α3. (24)
4 Known Bounds on Capacity
In [1], two analytical bounds on capacity are presented. Firstly, an upper bound is
derived based on the fact that differential entropy h(Y ) under an average constraint
E[Y ] ≤ m+ µ is maximized by an exponential distribution:
h(Y ) ≤ 1 + log(m+ µ). (25)
This leads to the bound
C = max{h(Y )− h(N)} (26)
≤ 1
2
log
λe(m+ µ)2
2piµ3
− 3
2
e
2λ
µ Ei
(
−2λ
µ
)
. (27)
6
0 1 2 3 4 5 6 7 8 9 10
0
1
2
3
4
5
6
 
 
Upper bound (30)
Upper bound (31)
Upper bound (32)
Known upper bound (27)
Known lower bound (29)
Lower bound (34)
I
(X
;Y
)
[b
it
s]
Delay m
Figure 1: Bounds on capacity as a function of m. The drift velocity has been set to
v = 2 and the channel parameter λ is assumed to be λ = 14 .
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
 
 
Upper bound (30)
Upper bound (32)
Known upper bound (27)
Known lower bound (29)
I
(X
;Y
)
[b
it
s]
Delay m
Figure 2: Bounds on capacity as a function of m identical to Fig. 1, but zoomed in
at low values of m.
8
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
 
 
Upper bound (30)
Upper bound (31)
Upper bound (32)
Known upper bound (27)
Known lower bound (29)
I
(X
;Y
)
[b
it
s]
Velocity v
Figure 4: Bounds on capacity as a function of v identical to Fig. 3, but zoomed in
at low values of v.
Note that this lower bound can be simplified considerably, but for the price of
losing its tightness for large values of m or v: it can be shown that the complicated
second last term (second and third row) can be lower-bounded by − log (1 + 12):
C ≥ log m
λ
+
µ
m
− λ
µ
+ kλ+
3
2
log
λ
µ
− 3
2
e
2λ
µ Ei
(
−2λ
µ
)
− log 3
2
− 1
2
log
2pi
e
. (37)
6 Asymptotic Capacities
The upper bound (27) and the lower bound (34) are asymptotically tight, i.e., when
we let v or m tend to infinity, these two bounds coincide. Hence, we can state the
exact asymptotic capacity.
Theorem 3. The capacity of the AIGN channel as defined in (4) is asymptotically,
when the average-delay constraint m is loosened to infinity while all other parameters
are kept constant, as follows:
lim
m↑∞
{
C(m)− logm} = 1
2
log
λe
2piµ3
− 3
2
e
2λ
µ Ei
(
−2λ
µ
)
. (38)
In the asymptotic regime when the drift velocity v of the fluid medium tends to
infinity while all other parameters are kept constant, the capacity is as follows:
lim
v↑∞
{
C(v)− 3
2
log v
}
=
1
2
log
λm2e
2pi
. (39)
10
8 Discussion
We should point out that in [1] the authors have already concluded from numerical
analysis that their upper bound (27) is very tight. We have now formally proven
this by providing an analytical lower bound that is tight in the asymptotic regime.
Due to the tightness of the known upper bound (27), it obviously is very difficult
to find improved upper bounds. So, our focus in the search for upper bounds lay
mainly in the low-delay and in the low-velocity regimes. We tried in particular to
find a bound that would show that the strange behavior of (27) to increase again
as v ↓ 0 is an artifact of the bounding technique. We were able to find bounds that
were strictly better than (27) and, in particular, we did find bounds that grew more
or less monotonically in v, as we would expect. However, there is still considerable
room for improvement.
Note that the capacity for v = 0 is strictly larger than zero because even if
there is no drift, the molecules still have a positive probability of arriving due to
the Brownian motion. It is very weird that in this situation of v = 0, the noise that
hurts communication at large speeds becomes the only mean of communication and
is therefore highly beneficial!
In the case when the capacity is analyzed as a function of the average-delay
constraint m, the general picture is better. While the bound (27) remains strictly
bounded away from zero, we have found an upper bound that tends to zero as m ↓ 0
(both (30) and (31), but the former is considerably simpler). Unfortunately, the
slope of convergence of our upper bound (30) and of the lower bound (29) do not
coincide. The exact asymptotic capacity for v = 0 and the capacity growth rates for
v ↓ 0 and m ↓ 0 are projects of our future research.
References
[1] K. V. Srinivas, Raviraj S. Adve, and Andrew W. Eckford, “Molecular communi-
cation in fluid media: The additive inverse Gaussian noise channel,” December
2010, arXiv:1012.0081v2 [cs.IT]. [Online]. Available: http://arxiv.org/abs/1012
.0081v2
[2] Claude E. Shannon, “A mathematical theory of communication,” Bell System
Technical Journal, vol. 27, pp. 379–423 and 623–656, July and October 1948.
[3] Amos Lapidoth and Stefan M. Moser, “Capacity bounds via duality with appli-
cations to multiple-antenna systems on flat fading channels,” IEEE Transactions
on Information Theory, vol. 49, no. 10, pp. 2426–2467, October 2003.
[4] Amos Lapidoth, Stefan M. Moser, and Miche`le A. Wigger, “On the capacity of
free-space optical intensity channels,” IEEE Transactions on Information The-
ory, vol. 55, no. 10, pp. 4449–4461, October 2009.
[5] Thomas M. Cover and Joy A. Thomas, Elements of Information Theory, 2nd ed.
New York: John Wiley & Sons, 2006.
12
行政院國家科學委員會補助國內專家學者出席國際學術會議報告
	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 102	 年	 08	 月	 30	 日
報告人姓名
莫詩台方
服務機構
及職稱 交通大學電機工程學系
	 	 	 	 	 時間
會議
	 	 	 	 	 地點
2012/7/1–6;	 Boston,	 MA,	 
USA 本會核定
補助文號
NSC	 100-2628-E-009-003
會議
名稱
	 (中文)	 2012國際電機電子協會	 資訊技術國際研討會
(英文)	 2012	 IEEE	 International	 Symposium	 on	 Information	 Theory	 (ISIT'2012)
發表
論文
題目
	 (中文)
(英文)	 Bounds	 on	 the	 Capacity	 of	 the	 Additive	 Inverse	 Gaussian	 Noise	 Channel
報告內容應包括下列各項：
1、 參加會議經過	 process
The International Symposium on Information Theory (ISIT) is the world most famous 
and best yearly conference in the field of information theory. The ISIT was organized 
in nine parallel sessions during five full days. Each session was centered on one of 
the various hot topics in information theory. In addition there was a plenary speech 
every  morning  given  by  some of  the  world’s  best  researcher  in  the  field:  Frank 
Kschischang (University of Toronto), Prakash Narayan (University of Maryland), Ueli 
Maurer  (ETH  Zürich),  and  Predrag  Cvitanovic  (Georgia  Institute  of  Technology). 
Moreover, the famous Shannon Lecture was hold by the recipient of the Shannon 
Award 2012, Abbas El Gamal (Stanford University). The Shannon award represents 
the  highest  honor  any  researcher  can  achieve  in  the  field  of  information  theory. 
Additional events included the Awards Luncheon, and the banquet.
2、 與會心得	 review/gain
There  are  two  main  purposes  when  attending  an  international  high-quality  
conference:  firstly,  one is  given the great  opportunity  to  present  the  newest  own 
results and discuss them with other researchers from around the world. This is a very 
fruitful process leading to new ideas and solutions to problems that one could not 
solve beforehand. Secondly, one attends many talks of outstanding researchers and 
learns about the newest ideas and developments in one’s working field.
My purpose of attending the International Symposium on Information Theory was to 
keep in touch with the leading researches in my field and their results.  I  had the 
表 Y04
附
件
三
國科會補助計畫衍生研發成果推廣資料表
日期:2012/06/15
國科會補助計畫
計畫名稱: 多用戶通訊系統中使用聯合估計與檢測之容量分析(2/3)
計畫主持人: 莫詩台方
計畫編號: 100-2628-E-009-003- 學門領域: 通訊
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
None. 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
