一、報告內容
中文摘要
為了確保一個應用軟體的整體可靠度，這個系統裡的軟體元
件必須在有限資源限制下，去達成某些可靠度的要求。這些
資源包括人力、中央處理單元所需的花費時間和這些消逝的
時間等等。在不失一般性的情況下，我們讓這些資源以出力
函數來表示之。因此，為了要建立一個高可靠度的軟體系
統，一個專案經理必須要事先知道如何有效率地分派這些資
源。為此，我們檢驗兩種出力函數分配的最佳化問題：(1)在
一個系統中當總可出力測試值是固定下，如何讓剩餘的軟體
錯誤的總數是最少的，及(2)當我們要求某一系統的可靠度在
某一固定值時如何讓總出力測試值是最少的。我們分別提出
了兩個有效率的最佳化演算法來解決上述的問題。實驗結果
說明了我們提出的方法可以解決這些出力測試分配問題並且
明顯地改善了軟體系統的品質與可靠度。這些在模型裡的參
數是可直接從錯誤的資料集中計算出。由於系統的動態特性
與資料集的不充足，準確的參數值是很難決定的。因此，敏
感性分析是常常在這階段中用來處理這個參數值問題。我們
討論的軟體系統的可靠度問題是假設這系統是可被分解成有
限個數的元件所組合而成的。為了要去評估一個元件為基礎
的軟體可靠度，我們提出一個新的方法去評估這個系統的可
靠度。並且我們也對這些參數做了敏感度分析以此來決定那
一個元件影響這個系統的可靠度最明顯。再者，我們也對最
佳化資源分配問題做敏感度分析。這方法是非常實用的，譬
如，我們可以利用這個策略來決定在這個系統中那一個參數
是最敏感的，有了這些資訊我們的測試團隊就可以用較多的
測試資源讓這個參數預測值的更準確。最後我們以三個一般
性的例子做驗證，這個實驗結果也說明了我們提出的策略是
有效率性的。
關鍵詞:敏感度分析，軟體測試，軟體可靠度，軟體可靠度成
長模型，軟體測試資源分配
Abstract
To develop a good reliable software system, a
project manager must determine in advance how to
effectively allocate these resources. Due to the
dynamic properties of the system and the insufficiency
of the failure data, the accurate parameter values are
hard to determine. Therefore, the sensitivity analysis
is often used in this stage to deal with this problem. In
this paper, we study two optimal resource allocation
problems in a component-based software during
testing phase: 1) minimization of the remaining faults
when a fixed amount of testing-effort is given, and 2)
minimization of the required amount of testing-effort
when a specific reliability requirement is given.
Several useful optimization algorithms based on
Lagrange multiplier method are proposed.
Furthermore, we present the sensitivity analysis on
these allocation problems in order to determine which
of the parameters affects the system most. Finally,
three numerical examples are evaluated to validate
and show the effectiveness of the proposed approach.
Therefore, a software project manager can allocate the
limited testing-effort efficiently and thus achieve the
highest reliability for the software applications.
Keywords：Sensitivity Analysis，Software Reliability，
Software Reliability Growth Modeling ， Software
Testing，Software Testing Resource Allocation
1. Introduction
With the great advancement of computer technology,
software designers are motivated to integrate
commercial off-the-shelf (COTS) software components
for rapid software development. To ensure high
reliability for such applications using software
components as their building blocks to construct a
software system, dependable components have to be
deployed to meet the reliability requirements.
Software takes a large portion of the system cost and
requires the achievement of high reliability. In fact,
Software reliability can be viewed as a powerful
measure of quantifying software failures and it is
defined as the probability of failure-free software
operation for a specified period of time in a specified
environment [1]. Measuring or predicting software
reliability is a challenge and its result is important as a
quantitative assessment of the performance of the
underlying software systems. This is especially true
before systems are release to the market. Therefore,
research efforts in software reliability engineering have
been conducted over the past three decades and many
software reliability growth models (SRGMs) have been
proposed [1-4]
To ensure the overall reliability of a software
application, software components in the system have to
meet certain reliability requirements, subject to some
resource constraints [5-9]. These resources include
human power, CPU hours, and elapsed time, etc.
Without loss of generality, we call all these resources
as the testing-effort [10-13]. Hence, to develop a good
reliable software system, a project manager must
r = the error detection rate per unit testing-effort at
testing time t and r>0.
Besides, the conditional reliability function after the
last failure occurs at time t is obtain by
))]()((exp[)()( tmttmtttRtR  (3)
Therefore, the mean value function of a distributed
component-based system with N components can be
formulated as:



N
i
iiii
N
i
ii tWravtmvtM
11
)))(exp(1()()( (4)
where the weighting vector (v1, v2, ..., vN) represents
the importance of each component and is obtained
based on the system architecture. In practice, the
frequency of a component being executed affects the
overall system reliability. A higher frequency indicates
a greater effect on the performance of the system. This
fact shows that the components should have distinct
weights according to the architecture of the software
system.
2.2. Assumptions of testing-resource allocation for
module software systems
In this section, we will consider several resource
allocation problems based on an SRGM with testing-
effort function during software testing phase. .
Assumptions [5-7, 16, 17]:
1. The Software system is composed of N independent
components that are tested individually. The
number of software faults remaining in each
component can be estimated by an SRGM with
testing-effort function.
2. For each component, the failure data have been
collected and the parameters of each component can
be estimated.
3. The transfer of control among software components
follows a Markov process.
4. The total amount of testing-resource expenditures
available for the testing processes is fixed and
denoted by W.
5. If any of the software components is faulty, the
whole software system fails.
6. The system manager has to allocate the total testing-
resources W to each software component to
minimize the number of faults remaining in the
system during the testing period.
The object here is to minimize the total number of
faults remaining in the software system after the testing
phase. From Eq. (4) and the above assumptions, the
number of remaining faults in the system can be
estimated by
  

N
i
iiii
N
i
iiii WravtWrav
11
)exp())(exp( .(5)
2.3. Two optimum testing-effort allocation
problems
In this section we describe the problem of allocating
testing-resources to software components so that the
software applications can be constructed effectively,
given that the applications have prescribed reliability
requirements [5-9]. Therefore, assume a software
system with N components that can be designed for
execution in a given application. In order to make the
model analytically tractable, we consider systems with
different architecture styles and utilize the Markov
process to model the failure behaviors of the
applications. Besides, the testing-resources are
characterized in terms of the testing-effort and the
prescribed reliability requirements are respectively
measured by the remaining faults in the applications.
Hence, to improve the software reliability, the manager
of software development has to effectively assign the
testing-resource to the N components under the
prescribed reliability requirements.
2.3.1. Minimizing the number of remaining faults
given a fixed amount of testing-effort. The
optimization problem is that the total amount of
testing-effort is fixed, and we want to allocate these
efforts to each component to minimize the number of
remaining faults in the system. Explicitly speaking,
suppose the total amount of testing-effort is W, and
component i is allotted Wi testing-effort, and thus the
optimization problem can be represented as follows:
Minimize: 


N
i
iiii Wrav
1
)exp( , Subject to
NiWWW i
N
i
i ...,,2,1,0,
1


(6)
Thus, the optimal solution 0
iW is
.,...,2,1],ln)[ln(
10 Nirav
r
W iii
i
i  

 




N
i
i
lN
i
iiii
r
Wravr
where
1
1
)/1(
))(ln/1(
ln  .
Hence, we get ),..,,( 00201 NWWW as an optimal
solution to Eq. (11). However, the above solution may
have some negative values, thus making
),..,,( 002
0
1 NWWW infeasible for Eq. (6). If this is
the case, the infeasible solution can be modified with
the following steps.
Algorithm 1 for minimum remaining faults:
Step A.1: Set l=0.
Step A.2: Calculate the following equations
.,...,2,1],ln)[ln(
1
,
)/1(
))(ln/1(
ln
1
1 lNirav
r
W
r
Wravr
iii
i
iN
i
i
lN
i
iiii







 
Step A.3: Rearrange the index i such
that .... **2*1 lNWWW 
Step A.4: If 0* lNW then ),..,,( 00201 NWWW is derived
and go to Step B.1
Else
update 0* lNW and l=l+1.
End If
Step A.5: Go to Step A.2.
Step B.1: Set h=0, ai=(1+p%) ai.
Step B.2: Calculate the following equations
.,...,2,1],ln)[ln(
1
,
)/1(
))(ln/1(
ln
1
1 hNirav
r
W
r
Wravr
iii
i
iN
i
i
hN
i
iiii


 



 
Step B.3: Rearrange the index i such that
.... ##2
#
1 hNWWW 
Step B.4: If 0* hNW then
),..,,( 21 iii
a
N
aa WWW is derived and go to Step C.
Else
update 0# lNW and l=l+1.
End If
Step B.5: Go to Step B.2.
Step C: Calculate the following equations
iap
MRF,
%100
)exp(
|)]exp(%)1()exp([)exp(|
1
0
,11
0
 
  




N
j
jjjj
N
ij
j
a
iiii
a
jjjj
N
j
jjjj
Wrav
WrapvWravWrav ii
%.100,,, p
MRF
a
a
MRF
SMRF ii
i
ap
i
i
ap
ap 
Thus the value of objection function given by Eq. (6) is
 

N
i
iiii Wrav
1
0 )exp( .
Afterward, As for the parameter vi and ri in Eq. (6), we
can also apply the similar algorithm to conduct the
sensitivity analysis for the minimum remaining faults.
3.2. Sensitivity analysis and optimum solution of
Section 2.3.2
Secondly, to solve the optimum solution problem in
Section 2.3.2, the Lagrange multiplier method [21] can
be applied and Eq. (7) be transformed as the following
equation:
Minimize:
 

N
i
iiii
N
i
iN ZWravWWWWL
11
21 ))exp((),,...,,(  (14)
By the Kuhn-Tucker conditions, the necessary
conditions for a minimum of Eq. (14) to exist are:
01)exp(),,...,,( 21 

iiiiiN
i
WrravWWWL
W
 (15)
0)exp(),,...,,(
1
21  


N
i
iiiiN ZWravWWWL 
(16)
Thus, we get the solution ),..,,( 00201 N and it has
the following form:
.,...,2,1],)
1
[ln(
1
1
0 Ni
rZ
rav
r
lN
i i
iii
i
i 



However, the above solution may have some negative
element values, thus making infeasible for Eq. (7). For
this reason, the infeasible solution can be corrected
with Algorithm 2. On the other hand, assume
),..,,( 002
0
1 N is the optimum solution for Eq. (7)
and
p
r
N
rr iii ),..,,( 21  is the relative optimum
solution when ir is changed by 100p%. Let
irp
MTE , be the relative change of the minimum total
amount of testing-efforts for Eq. (7) when ir is
changed by 100p%. Also, let
irp
SMTE ,
be the
sensitivity of the relative change of the minimum total
amount of testing-efforts to ir when ir is changed by
100p% as the ratio of relative change for the two
quantities. That is,
2.17% change of the minimum remaining faults in
Example 1. Furthermore, Figures 1-3 depict the
relationships between the relative change of the
minimum remain faults,
iap
MRF , , and the relative
change of parameter ai. Also, Figures 4-6 illustrate the
sensitivity relationship for
iap
SMRF , . In order to
present the importance of each parameter, the curves in
the figures are ordered by its sensitivity decreasingly.
however, Figures 7-12 are the corresponding
sensitivity analysis results for ri.
4.2. Examples of minimizing the total amount of
testing-effort expenditures
Assume the total number of remaining faults Z=100.
One has to allocate the expenditures to each
component to minimize the total amount of testing-
effort expenditures. Using algorithm 2 in Section 3.2
and Table 1, the optimal solutions for Eq. (7) are
derived and shown in Table 5. Table 6 gives a
description about the most sensitive parameters in
these examples by Algorithm 2. For example, a 10%
change of r5 will imply a 2.04% change of the optimal
amount of testing-effort expenditures in example 3
with Algorithm 2. Besides, Figures 13-15 show the
relationships between
irp
MTE , and ri, and Figures 16-
18 are for
iap
MTE , and ai.
5. Conclusions
In this paper, we consider two kinds of software
testing-resource allocation problems. The first
problem is to minimize the number of remaining faults
given a fixed amount of testing-efforts. The second
problem is to minimize the required amount of testing-
effort when a specific reliability requirement is given.
Several optimization algorithms based on Lagrange
multiplier method are proposed to help software
project managers to handle these problems and to make
the best decisions. On the other hand, we present the
sensitivity analysis on these allocation problems in
order to determine which of the parameters affects the
system most. Finally, three numerical examples are
evaluated to validate and show the effectiveness of the
proposed approach. Therefore, a software project
manager can allocate the limited testing-effort
efficiently and thus achieve the highest reliability for
the software applications. For the future works, we
will focus on topics including comparisons with other
approaches, sensitivity analysis of testing-effort of a
component-based software based on some economics
criteria.
二、參考文獻
[1] M. R. Lyu. Handbook of Software Reliability
Engineering. McGraw-Hill, 1996.
[2] O. Berman and N. Ashrafi, "Optimization Models
for Reliability of Modular Software Systems," IEEE
Trans. on Software Engineering, vol. 19, No. 11, pp.
1119-1123, November 1993.
[3] J. D. Musa, A. Iannino, and K. Okumoto (1987).
Software Reliability, Measurement, Prediction and
Application. McGraw-Hill.
[4] J. D. Musa (1998). Software Reliability
Engineering: More Reliable Software, Faster
Development and Testing. McGraw-Hill.
[5] P. Kubat and H. S. Koch, "Managing Test-
Procedure to Achieve Reliable Software," IEEE Trans.
on Reliability, vol. 32, No. 3, pp. 299-303, September
1983.
[6] H. Ohtera and S. Yamada, "Optimal Allocation &
Control Problems for Software-testing Resources,"
IEEE Trans. on Reliability, 39, vol. 2, pp. 171-176,
June 1990.
[7] Y. W. Leung, "Software Reliability Growth Model
with Debugging Efforts," Microelectron. Reliab. Vol.
32, No. 5, pp. 699-704, 1992.
[8] R. H. Hou, S. Y. Kuo, and Y. P. Chang, " Needed
Resources for Software Module Test, Using the Hyper-
Geometric Software Reliability Growth Model," IEEE
Trans. on Reliability, Vol. 45, No. 4, pp. 541-549,
December 1996.
三、計畫成果自評
本研究內容與原計畫均相符並達成原計畫之預期目
標。相關的研究成果己發表在以下的國際學術會議
論文集中：
1. J. H. Lo “Effect of the Delay Time in Fixing a Fault on
Software Error Models,”Proceedings of the 31rd IEEE
Annual International Computer Software and Applications
Conference (COMPSAC 2007), Vol. 2, pp. 711–716, July
23-27, 2007, Beijing, China. (EI, INSPEC)
2. J. H. Lo “Considering Both Failure Detection and Fault
Correction Activities in Software Reliability Modeling,”
Proceedings of the IEEE Region 10 Annual International
Conference (TENCON 2006), pp. 1-4, Nov. 2006, Hong
Kong, China. (EI)
Considering Both Failure Detection and Fault 
Correction Activities in Software Reliability Modeling 
Jung-Hua Lo 
Department of Informatics, Fo Guang University 
160, Linwei Rd., Jiaosi Shiang, 
Yilan County 26247, Taiwan (R.O.C.) 
Abstract-Software reliability is widely recognized as one of 
the most significant aspects of software quality and is often 
determined by the number of software uncorrected faults in the 
system.  In practice, it is essential for fault correction prediction, 
because this correction process consumes a heavy amount of 
time and resources to predict whether reliability goals have been 
achieved.  Therefore, in this paper we discuss a general 
framework of the modeling of the failure detection and fault 
correction process.  Under this general framework, we not only 
verify the existing non-homogeneous poisson process (NHPP) 
models but also derive several new NHPP models.  In addition, 
we show that these approaches cover a number of well-known 
models under different conditions.  Finally, numerical examples 
are shown to illustrate the results of the integration of the 
detection and correction processes.  
I. INTRODUCTION
The fault-detection and fault-correction are critical 
processes in attaining good software quality.  During the 
software detection process, testing cases are run and 
ultimately failures are detected.  After detection, the 
debugging team should analyze the failure, locate the fault 
and fix the fault [1-4].  That is, the fault correction process 
affects the reliability of a software product significantly and 
we should pay more attention to it. Numerous stochastic 
models for the software failure phenomenon have been 
developed to measure software reliability, and some of them 
are based on NHPP [1-3].  Most of these models have focused 
on the failure detection process. Consideration of fault 
correction process in the existing models is limited [5-7].  
However, the correction process may be conducted to develop 
stopping rules for testing, control test strategies, and allocate 
testing-resource under budget limits of a project.  Therefore, 
we develop a method to give equal priority to modeling both 
the failure detection and correction processes.  Numerical 
examples have been performed based on real data sets, and 
the results show that the proposed models obtain a better 
result in estimating the number of initial faults and also 
indicate a goodness-of-fit in terms of the mean-of-squares 
error criterion.  
This paper is organized as follows. In Section 2, the 
properties of the related models are reviewed.  An integration 
model of failure detection and fault correction processes is 
proposed in Section 3. The experiments and results are 
presented in Section 4.  Finally, the conclusions are made in 
Section 5. 
II. SOME SRGMS BASED ON NHPP 
In this section, we show that several classical SRGMs 
based on NHPP can be directly derived according to various 
error detection rates.  
Goel-Okumoto model [8]
The most well-known SRGM based on NHPP is the model 
proposed by Goel and Okumoto.  This model assumes that 
the error detection rate per error in the testing phase is 
constant.  Thus, the MVF is derived by  
0,0),1()( >>−= − baeatm bt ,
where a is the expected number of errors to be eventually 
detected and b represents the error detection rate per error. 
.
Delayed S-shaped model [9]
This model is an S-shaped curve for the cumulative 
number of failures detected such that the failure rate initially 
increases and later decreases.  Yamada et al. assume that the 
error detection rate is a time-dependent function described by 
an S-shaped curve because the testers’ skills will gradually 
improve as time goes by. That is, the error detection rate is 
bt
tb
+1
2
.  Thus, we have 
0,0],)1(1[)( >>+−= − baebtatm bt ,
where a is the expected number of errors to be eventually 
detected and b represents the error detection rate. 
Inflected S-shaped model [10]
This model assumes that faults in a program are mutually 
dependent so that some faults are not detectable before some 
others are removed [4]. Suppose the error detection rate is 
)1)(1( cce
bc
bt ++ − .  Then the MVF can be derived as follows:
)
1
1(
bt
bt
ce
ea
−
−
+
− , a > 0, b > 0, c > 0. 
The parameter a is the expected number of errors to be 
eventually detected, b is the error detection rate, and  c is the 
1-4244-0549-1/06/$20.00 ©2006 IEEE.
0,0)),(()( >>−= λλ atma
dt
tdm
)].()([)( tmtm
dt
tdm
c
c
−= μ
we have, )]exp(1[)( tatm λ−−= , and 
]1[)( ttc eeatm
μλ
μλ
λ
μλ
μ
−−
−
−
−
+=
where a is the expected number of initial faults, and the initial 
condition m(0) and mc(0) equal zero.ʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳʳϭʳ
ʳ
IV. NUMBERICAL EXAMPLES
A Criteria for comparison
(1) The Accuracy of Estimation (AE) [1-3] is defined as:  
aM
aM a− .
Note Ma is the actual cumulative number of detected faults 
during the testing process and after the test, and a is the 
estimated number of initial faults.  Generally, Ma is obtained 
from software fault tracking after software testing.  In 
addition to the better fit to the actual observed data, a good 
model should have the ability to predict the behavior of future 
failures well.  
(2)The Mean of Square Faults (MSF) is defined as: 
   
[ ]2
1
)(
k
k
i i
mitm¦
=
−
From the measure of MSF, we can obtain quantitative 
comparisons for long-term predictions between actual and 
predicted values.  A lower MSF indicates better performance 
of fitting the real software data.  
(3)The Relative Errors (RE) [1-3] is defined as:  
.
)(
Z
Z
z
tm
RE
−
=
Suppose Z failures were observed by the end of test time tz,
we use the failure data up to time ti ( ztit ≤ ) to estimate the 
parameters of m(t).  Moreover, a lower absolute value of RE 
indicates better performance of fitting the real software data.  
 (4) The Mean of Relative Errors (MRE) is defined as:  
.
1
)(1 ¦
=
−
=
l
t
t
z
t Z
Z
z
tm
l
MRE
Note ti is the time when the observed i failure is detected and 
l is the cumulative number of detected faults after the test.  
Besides, in order to check the predictive validity of the 
model, the procedure can be executed for various values of ti
and the result can be shown by plotting the curve of the RE 
for different values of  tz.
(5)The 100p% upper and lower confidence limits for m(t) are 
defined as [16]:  
)(ˆ)(ˆ tmptm η+  and )(ˆ)(ˆ tmptm η−
Suppose )(ˆ tm  is the estimate for m(t), then the 100p% upper 
and lower confidence limit for m(t) can be bounded 
approximately as follows:  
≥+ )(ˆ)(ˆ tmptm η m(t) )(ˆ)(ˆ tmptm η−≥
where pη  is the 100(1+p)/2 percentage point of the standard 
normal distribution.  For example, pη is 1.645 when p is 0.9.  
B Performance analysis
The data set was reported by Ohba (1984).  It was 
observed in software testing of a PL/I database application 
program, which consists of approximately 1,317,000 lines of 
code (LOC).  During the testing period, about 328 software 
faults were removed.  After a long period of testing, 30 faults 
are still detected. Table 1 shows the estimated number of 
initial faults, MSF, and AE of the proposed models and the 
six other NHPP models for the purpose of comparison.  The 
lower the MSE and AE of a model are, the better they fit the 
observed data.  Figure 1 depicts the fitted curves of the 
cumulative numbers of failures and the 90% bounds for the 
proposed model.  Finally, we calculate the RE and MRE in 
prediction for this data set and the results are plotted in Figure 
2.  We can see from Table 1 and Figures 1-2 that the 
proposed model gives a better fit to the observed data and 
predicts the future behavior well.  Particularly, the proposed 
model has the smallest value of MSE (8.28) and the estimated 
number of initial faults (387.64) is very close to the real 
observed number of initial faults (358).  
TABLE I 
COMPARISON RESULTS FOR OHBA DATA SET
Model a AE(%) MSF 
Proposed model  387.64 8.28 108.9 
G-O  model 513.15 43.34 222.09
Delayed S-shaped Model 374.05 14.48 168.67
Inflected S-shaped Model 389.10 8.69 133.53
Yamada Exponential model  828.25 131.35 140.66
Yamada  Rayleigh model 459.08 28.23 268.42
Yamada Weibull model 565.35 57.91 122.09
