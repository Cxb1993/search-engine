The RSST segmentation method [11-12] is proposed to 
improve spiky phenomenon by using the global 
information. Given a frame with m√ón macroblocks, the 
link weight wijc in the spanning tree can be defined as 
the absolute difference value of the chrominance DC 
signals between adjacent blocks i and j, i.e., 
j
Cb
i
Cb
j
Cr
i
Cr
c
ij ddddw  ,           (1) 
where di and dj denote the DC values of the 
chrominance signals for the adjacent macroblocks i and 
j respectively. Starting from the least-weighted link, the 
RSST segmentation method merges the vertices of the 
least weighted link into one new vertex, and then save 
the link to a new spanning tree. The new vertex value is 
replaced by the mean of the two adjacent vertices. By 
searching the least-weighted link and perform the 
above process repeatedly, a new spanning tree is 
constructed with the saved links. Finally, the image is 
partitioned into N regions by cut the new spanning tree 
at the N-1 largest weighted links. 
  The image segmentation using the motion vectors 
may also be obtained by defining the link weight wijm
as:   
j
y
i
y
j
x
i
x
m
ij mmmmw  ,     (2) 
where mi and mj denote the motion vectors for the 
adjacent blocks i and j respectively. By projecting the 
motion segmented regions onto the color segmented 
regions, we may acquire the regions of moving human. 
The segmentation method for extracting the human 
region is developed on the basis of the projection 
operator [13] that is defined as: 
m
M
i MRIp  ),(     
(3) 
where Ii denotes the color segmented region and RM
denotes the motion segmented region. In Eq. (3), the 
projection operation finds the color segmented regions 
that overlap with the motion segmented regions. Based 
on the projection operation, the group operator G [13] 
defined as Eq. (4) is used to collect all the regions 
founded in (3).  
}),(:{),,( g
M
ii
M
i MRIpIgRIG   (4) 
Fig. 2 shows the segmented human region obtained by 
using the group operator. 
2.2. OAMV Motion Feature 
Generally, a complete human action often takes a few 
seconds. Hence, several GOPs are required to extract 
the visual features for recognizing the human action. 
To obtain the prominent motion feature for human 
action a new GOP-based motion descriptor called the 
object-based accumulative motion vector (OAMV) is 
proposed. Fig. 3 describes the concept of the OAMV 
motion feature. In Fig. 3, the jumping action is 
illustrated and the red line segments denote the motion 
vectors. It is obviously that the motion vectors between 
the adjacent frames have high correlations. If the 
motion vector for a tracked block on the successive 
frames is accumulated, then the trajectory of the 
tracked block may be constructed to describe the body 
movements. 
By continuously tracking each block in the region of 
human body within each GOP, the OAMV motion 
feature is constructed with the following steps: 
1. Initialize a starting frame and record the center 
position for each tracked block in the region of 
human body. 
2. For the P-frame, by using the extracted motion 
vectors the new center position for each block is 
updated with the center position of the tracked 
block. Record the updated center position for each 
tracked block within the object region.  
3. Repeat the process in step 2 until all the P frames in 
the GOP are searched. 
4. For each GOP, construct the trajectory (OAMV 
motion feature) for each block in the region of 
human body.  
In Fig. 4(a)-(b), the GOP-based OAMV features for the 
tumbling down action are illustrated. Each MB within 
the human region is tracked for time period of a GOP 
and then the OAMV feature is constructed. 
2.3. Polar Histogram 
In [10], the polar histogram is used to extract the 
distribution pattern for each human action such that the 
human actions may be identified. Here, the polar 
histogram is also applied to extract the distribution 
pattern for the OAMV motion feature in the 
compressed domain. The OAMV features for different 
human actions will distribute with different patterns in 
the polar histograms. Fig. 5 illustrates the OAMV 
feature on the polar coordinate for the running and 
walking actions. Although the running and walking 
actions looks alike, the OAMV feature still can 
discriminate the two similar actions. The polar 
histogram is generated by transforming the OAMV 
motion features from Cartesian coordinate to polar 
coordinate. There are totally 32 bins in the polar 
histogram. In order to enhance the polar histogram the 
squared polar histogram is applied. 
3. Human Action Analysis 
   
Generally, a human action often takes a few seconds 
(several GOPs) to complete. By using the polar 
histograms among the successive GOPs and the Hidden 
Markov Models, we may identify the various kinds of 
human actions. Since the human actions in the video 
shot may change irregularly, the ergodic HMM [14] is 
appropriate to be applied for identifying the human 
9th JCIS 2006 8 to 11 October 2006
984
[9] B. Ozer, W. Wolf, A. N. Akansu (2000) Human activity detection 
in MPEG sequences. In: Proc. Workshop on human motion, Dec. 
7-8, 2000, pp. 61-66 
[10]R. V. Babu, B. Anantharaman, K. R. Ramakrishnan, S. H. 
Srinivasan (2002) Compressed domain action classification 
using HMM. Pattern Recognition Letters, Aug. 2002, 23(10): 
1203-1213 
[11]J. Morris, M. J. Lee, A. G Constantinides (1986) Graph theory 
for image analysis: an approach base on the shortest spanning 
tree. In Proc. Inst. Elect. Eng., Apr. 1986, 133(2):146-152 
[12]E. Tuncel, L. Onural (2000) Utilization of the recursive shortest 
spanning tree algorithm for video-object segmentation by 2-D 
affine motion modeling. IEEE Trans. Circuit and System for 
Video Technology, Aug. 2000, 10(5):776-781 
[13]A. Alatan, E. Tuncel, L. Onural (1997) A rule-based method for 
object segmentation in video sequences. In: Proc. IEEE 
international conference on image processing ICIP, Oct. 1997, 
2:522-525 
[14]L. Rabiner, B.-H. Juang (1993) Fundamentals of speech 
recognition. Prentice-Hall 
Video Partial 
Decoder
Compressed 
Video
Visual Feature 
Extraction
Color
Human Region 
Segmentation
Object-based 
Accumulative 
Motion Vector
Human Action 
Feature 
Extraction
HMM Action 
Identification
Actions
Motion
Fig. 1 Block diagram of the human action analysis system. 
(a)  (b)
(c)  (d)
Fig. 2 (a) I-frame for a human walking video clip. (b) Color 
segmentation mask. (c) Motion segmentation mask. (d) Color 
segmented regions projected onto motion segmented regions.  
(a) (b)
Fig. 3 Motion vectors for the jumping action on (a) frame #122 and 
(b) frame #131. 
(a) GOP #4 
(b) GOP #6 
Fig. 4 GOP-based OAMV features for the tumbling down action are 
illustrated in (a) to (b). Each MB within the human region is tracked 
for time period of a GOP and then the OAMV feature is constructed. 
(a)  (b)
Fig. 5 The OAMV features on the 2-D polar coordinate for (a) 
running action and (b) walking action. 
(a)
(b)
Fig. 6 Images on the left side show the moving human with different 
actions and images on the right side illustrate the segmented regions.  
    
(a) Running 
(b)Crouching 
Fig. 7 The 2-D polar distributions for the OAMV features of two 
kinds of actions are illustrated on the left side and the corresponding 
polar histograms are shown on the right side. 
Table 1 Accuracy analysis of action identification. 
Actions The number
 of test 
video clips
The number of
false action  
classification 
The correct 
recognition
rate 
Walking 15 3 80% 
Running 15 1 93% 
Jumping 15 3 80% 
Crouching 15 2 87% 
Tumbling down 15 2 87% 
9th JCIS 2006 8 to 11 October 2006
986
