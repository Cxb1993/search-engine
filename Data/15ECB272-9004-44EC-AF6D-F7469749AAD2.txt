 1 
1. 前言 
在人與人的交流行為中，情緒表達佔據了相當重要的位置，可幫助人們了解對方的內心感受並讓
自己能做出相對的反應；在各種的情緒表達行為上，包含肢體動作(臉部表情與身體姿勢)、說話
語調與用字遣詞等。實際上，人在互相交流的時候，口語，僅佔了 7%，而肢體動作則佔了 93%，
其中又以臉部表情所佔最多，高達 55%，由此可見臉部表情對於人類在溝通時的重要性。因此若
能讓機器人辨識使用者的情緒變化並做出反應，對於提升人機互動的親合力將有相當大的提升。 
 
2. 研究目的 
為了提升人機的親和力，本計畫提出「多辨識器組合之強健式臉部表情辨識技術」的主題，我們
將提出一套即時的自動化臉部表情辨識系統，包含有人臉偵測技術、臉部特徵點抽取技術、臉部
特徵點追蹤技術與臉部表情辨識技術。在本計畫中，將對自然表情(即無表情)及六種基本情緒表
情(高興、傷心、生氣、驚訝、厭惡與害怕)進行分析與辨識，正確的偵測出使用者的情緒表達，
以做為人機互動的重要資訊。 
 
3. 文獻探討 
    為了能使電腦認識人類的表情變化，已有許多學者投入大量的心力在電腦視覺研究上，並致
力於能讓電腦理解人類的表情。在近幾年來，表情辨識已經成為一個熱門的課題，因此有許多學
者提出各種方法，來對表情進行辨識。表情辨識方法主要可以分成兩種， 其中一種是使用Facial 
Action CodingSystem(FACS)[1][2][3] ， 另一種則是以特徵為基礎(Feature-based) 
[24][5][6][7][8][9][10]的辨識方法。 
FACS是Ekman和Friesen於1978 年所提出用於人臉表情描述的編碼系統，在這套系統中，會依據
人臉肌肉的分佈，以及一些肌肉群的運動狀況，定義出動作單元(Action Units)，每個動作單元表
示臉部上特定區域的移動狀況，如眉毛上升、嘴角上揚等，共定義了44 種動作單元(如圖一所示)，
透過動作單元的組合，來進行表情判斷。Tian[2]等人發展出一套自動臉部分析系統(Automatic Face 
Analysis，AFA)，能依照人臉上永久或暫時性的特徵，對人臉正面影像序列進行分析，辨識出每
種單獨的動作單元。Donato[3]等人發現使用GaborWavelet來進行特徵擷取，再進行上半部和下半
部人臉的FAUs分類，比傳統的幾何方法可以達到更好的效果。 
除了基於動作單元的表情辨識方法之外，也有基於紋理特徵等方法的表情辨識研究。Bartlett 和
Littlewort[4]等人將輸入的影像序列，偵測出正面的人臉位置，並經過Gabor Wavelet 擷取出紋理
特徵，最後再使用一連串的SVM 分類器來分類出7 種不同的主要表情(包含自然、生氣、猶豫、
恐懼、快樂、悲傷和驚訝)。Ma 和Khorasni[5]則使用離散餘弦轉換(Discrete Cosine Transform)對
整張影像進行特徵偵測和抽取，並使用前饋式類神經網路(Feedforward NeuralNetworks)來進行辨
識。Dubuisson 和Davoine[6]等人則先利用主成分分析法(Principal Component Analysis)和
LDA(Linear Discriminant Analysis)進行前處理將影像降低維度後，再進行辨識。有些基於2D 或
3D(Model Template-Based)的表情辨識方法[7][8][9][10]，這些方法計算於3D 模型中，特徵點的幾
何變化或是對應於2D 的紋理特徵變化，最後再經過辨識器，進行表情辨識。 
參考文獻 
1. P. Ekman and W.V Freisen, “The facial action coding system: a technique for the measurement of 
 3 
a. 地標點分類： 
把地標點分成角點形態地標點與邊緣形態地標點並且使用不同的地標點搜尋方式。角點
形態地標點，使用矩形的搜尋範圍，可以避免因正確地標點不在法線上所造成的誤差。 
 
b. 使用 Adaboost 偵測器來偵測角點形態地標點： 
Adaboost 偵測器對於有特殊紋理結構的物件能夠有效的偵測，再搭配所提出的候選點選
擇方式，可以成功的定位角點位置。 
 
c. 使用角點位置重新初始化五官形狀位置： 
此方式可以大幅減少不同的臉部結構及臉部表情使用同一個平均形狀樣板所造成的初始
位置誤差，因此可以大幅提升定位的準確度。 
 
d. 不同的五官地標點使用不同的搜尋長度： 
因為各地標點的分散程度不同，所以根據各五官地標點的分佈情形給予相對應的搜尋範
圍，可以減少因搜尋範圍短產生的局部最佳解及搜尋範圍長所增加的誤判機率。 
 
(2) 表情辨識 
我們提出一種混合權重式區域方向特徵(WLDP)和二元化形態(LBP)的表情辨識方法。一
開始 WLDP 和 LBP 會分別對人臉影像進行特徵抽取，接著利用 PCA 分別對 WLDP 和 LBP 所
抽取出來的特徵進行特徵降維處理，最後將兩種特徵進行混合，產生出一種對人臉具有分辨能
力的混合特徵，並使用 SVM 分類器來進行表情辨識。實驗的資料庫是使用著名的 Cohn-Kanade
表情資料庫，該資料庫由於具有完整的表情資料，因此是表情辨識領域的研究學者們常用來進
行研究的一套表情資料庫。本論文所提出的方法，對 Cohn-Kanade 表情資料庫進行 7 類表情辨
識，並使用 10-fold person-independent cross-validation 架構，可以得到高達 91.1%的辨識率，效
果優於目前多種主要的表情辨識演算法。這顯示我們所提出的權重式區域方向特徵和辨識方法
較先前的演算法具有進步性，已達到本計畫的研究目標。下圖為整合系統的測試結果畫面，所
拍攝的影像為畫面的主體，在左上角顯示所偵測和抓取的人臉影像，而在臉部的右上方則以圖
畫方式顯示所辨識出來的表情結果，如下列圖形所示。 
 
 
AN ADABOOST-BASED FACIAL EXPRESSION RECOGNITION METHOD 
 
YEA-SHUAN HUANG, SHUN-HSU CHUANG, FANG-HSUAN CHENG  
 Dept. of Computer Science & Information Engineering 
Chung Hua University, Hsinchu, Taiwan 
E-mail: yeashuan@chu.edu.tw 
ABSTRACT: 
A method of combining Weighted Local Directional 
Pattern (WLDP) and Local Binary Pattern (LBP) for facial 
expression recognition is proposed. First, WLDP and LBP 
are applied to extract human facial features. Second, 
principle component analysis (PCA) is used to reduce their 
feature dimensions respectively. Third, both reduced facial 
features are merged to form the final feature vector. 
Fourth, support vector machine (SVM) is used to recognize 
facial expressions. Experiment on the well known Cohn-
Kanade expression database, a high accuracy rate up to 
91.1% for recognizing seven expressions can be achieved 
with a person-independent 10-fold cross-validation scheme. 
 
Keywords:  
Facial Expression Recognition; Local Binary Pattern; 
Weighted Local Directional Pattern; Principal Component 
Analysis; Support Vector Machine; 
1. Introduction 
In recent years, Human-Computer Interaction (HCI) 
becomes more and more popular. If a computer can 
know a person’s feeling or emotions and react 
immediately, it will be regarded as very friendly. In 
general, there are many ways for a computer to 
communicate with human beings, such as by speaking, 
by gesturing and by making facial expressions. 
Researchers demonstrate that information passed by 
language accounts for 7 percent, information passed by 
tone of voice accounts for 38 percent, however, 
information passed by speaking people expressions 
accounts for 55 percent during human intercourses. 
Therefore, a lot of research effort has been devoted to 
facial expression recognition. However, there is no 
matured product or technology today so that facial 
expression recognition is still an active research topic.  
In this section, we will briefly introduce the existed 
methods for facial expression recognition. In 1978, 
Ekman and Friesen defined six universal facial 
expressions [1], including happiness, sadness, anger, 
surprise, fear and disgust. They proclaimed these 
expressions are stable not only across different races, 
but also across different cultures. In order to make 
recognition procedure more standardized, a set of 
muscle of movements, known as Action Units, was 
created by psychologists, thus forming the so-called 
Facial Action Coding System (FACS) [2]-[3], and these 
action units can be transformed into six universal 
expressions by rules proposed in [4]. 
To recognize these six universal expressions, we 
can in general classify these algorithms into two 
categories; one is feature-based methods and the other is 
appearance-based methods. Recently, Valstar et al. [5]-
[6] have demonstrated that geometric feature-based 
methods provide better or equal performance than 
appearance-based approaches in Action Unit recognition. 
In [7],  Kotsia et al. use shape and texture information to 
recognize six expressions and got superior performance 
on Cohn-Kanade. However, feature-based methods 
usually require robust facial feature localization in order 
to get good recognition results. For appearance-based 
methods, image filter such as Gabor-wavelet, is in 
general applied to the whole or a partial face image to 
extract the facial texture frequencies [8]-[12]. Although, 
the appearance-based method by using Gabor-wavelet 
gets excellent performance, but due to convolve face 
images with banks of Gabor filters, it is hard to be 
applied on real-time facial expression recognition 
systems. There are some methods [13] which use LBP to 
extract effective appearance features and get similar 
performance as Gabor-wavelet method, but it is still 
suffer much from non-monotonic illumination variation, 
random noise, and change in pose, age, expression. 
Some subspace methods such as 2DLPP [14] are also 
used to extract appearance information for facial 
expression recognition but it still have some drawback at 
illumination changes. Hence, we try to combine another 
feature to use the edge information to enhance the 
recognition performance. This paper proposes a novel 
algorithm by using the combined features of Boosting 
LBP and Boosting WLDP to recognize human facial 
expressions. The proposed method can enhance the 
performance than only use LBP or LDP method. 
The rest of this paper is organized as follows. 
Section 2 describes the preprocessing step and LBP and 
WLDP feature extraction method. Section 3 illustrates 
the boosting algorithm and fusion feature. Section 4 
 
Fig. 4: The basic LBP operator 
One variation of the LBP code is known as uniform 
pattern which is in general represented as LBPu2. The 
LBPu2 means the LBP binary string contains at most 
two bitwise transitions from 0 to 1 or vice verse. For 
example, 00110000, 00000000 and 11000000 are 
uniform patterns, but 11010100 and 01010101 are not 
uniform patterns. In general, LBPu2 has better or similar 
recognition performance than LBP. Therefore, LBPu2 is 
used as feature in our experiments and in the following 
discussion LBP indeed represent LBPu2 for simplicity. 
2.3 Weighted Local Directional Pattern (WLDP) 
The Local Directional Pattern [16] (LDP) is an eight-bit 
binary code assigned to each image pixel by comparing 
its 8-directional edge responses. The eight Kirsch masks 
(m0~m7) shown in Fig. 5 of different directions are used 
to extract the 8-direction edge response values of each 
image pixel. 
 
Fig. 5: Kirsch masks in eight directions. 
After applying the eight Kirsch masks on an image 
pixel, eight edge response values m0, m1,…, m7, can be 
calculated and each edge response value represents the 
edge strength of one specific direction, which 
m0,m1,…,m7 means the 0,45,…,275 degree of the edge 
directions. Among the eight edge response values, the k 
largest absolute values are selected and their 
corresponding bits are set to 1. Of course, the other 8-k 
bits not corresponding to the k largest absolute values 
are set to 0. Accordingly, each image pixel has an eight-
bit LDP code. The LDP(xc,yc) means LDP code at 
coordinate (xc,yc), s(x) means a thresholding function 
and mi  means the i-th Kirsh mask is used. The LDP 
operator can be defined as follow. 
 


7
0
2)(),(
i
t
icc msyxLDP  (3) 
where 
 






|)(||| if ,0
|)(||| if ,1
)(
Mkx
Mkx
xs
th
th
 (4) 
 },...,,{ 710 mmmM   (5) 
Fig. 6 illustrates an example of obtaining LDP code with 
parameter k=3. 
 
Fig. 6: The example of LDP code calculation. 
The LDP operator by selecting the k largest 
absolute edge response values to encode the direction of 
the edge, but if the target image has no obviously edges 
such as white wall or the facial skin. The LDP algorithm 
still forced to encode the target image and get LDP code. 
Hence, we propose a novel weighting method to avoid 
the situation of the same importance to the facial skin 
and to the facial part such as eye, eyebrow, nose and 
mouth. The proposed WLDP method is illustrated as 
follow. 
For each image pixel at coordinate (x,y), beside 
computing its LDP code, a corresponding weighting 
value ),( yxWLDP is also calculated by (3). 
 


k
r
th
LDP Mr
yxv
yxW
1
|)(|
),(
1
),(  (6) 
where 
 
 

1
1
1
1
),(),(
i j
jyixIyxv  (7) 
 },...,,{ 710 mmmM   (8) 
The motivation of the weighting function is that 
some of the facial regions such as eyebrows, eyes, noses 
and mouth have darker pixel value then others and has 
stronger edge response value. For this proposes, the left 
term of the weighting function is designed by summing 
all pixel values from its neighbors of the center 
coordinate at (x, y) and the right term is designed by 
summing the top k absolute edge response values. Fig. 7 
shows an example of the original image, the LDP 
encoding image and the weighting image.  
   
 (a)                         (b)                         (c) 
the training data each time. To avoid over-fitting, we 
further used the first group of the training data as the 
validation data to test the generalization ability of the 
trained classifier. This above process was repeated ten 
times and each group is in turn selected to serve the 
testing data which is omitted from the training process. 
Finally, the average recognition result on the 10 testing 
dataset is reported. 
Table 1 shows the recognition performance of the 
proposed algorithm and other existed methods. Due to 
the different environmental setup such as image pre-
processing, and different image sequences selected from 
the Cohn-Kanade database, we cannot compare their 
recognition rates directly, but this table still has the 
valuable for a research reference. Table 1 illustrates that 
WLDP gets better performance than the original LDP 
and the proposed algorithm with concatenating both 
Boosting LBP-PCA and Boosting WLDP-PCA gets 
almost the best performance. 
 
Table 1: Recognition Performance 
Method Recognition 
Rate(%) 
Block-Based LBP[11] 88.9 
Boosting LBP[11] 91.4 
Block-Based LBP 
Block-Based LDP 
Block-Based WLDP 
Boosting LBP-PCA 
Boosting LDP-PCA 
Boosting WLDP-PCA 
Proposed method (Boosting LBP-
PCA+Boosting WLDP-PCA) 
85.90 
86.73 
87.71 
88.77 
87.33 
88.31 
91.10 
 
The confusion matrix of the proposed method in 
seven expressions recognition (AN=Anger, DI=Disgust, 
FE=Fear, HA=Happiness, NE=Neutral, SA=Sadness, 
SU=Surprise) is illustrated in Table II. 
 
Fig. 8: The confusion matrix of the proposed method. 
In Table 1, the proposed method gets best 
performance than only use Boosting LBP-PCA or 
Boosting WLDP-PCA method individually. We also 
implement the block-based method for comparing LBP, 
LDP and WLDP performance in [33]. In our 
experiments, the Block-Based WLDP gets best 
performance in the three methods, but still lower than 
[33] proposed. One of the possible reasons is that most 
of our samples in harder recognize expressions (105 
Anger, 129 Fear, 331 Neutral and 153 Sadness images) 
in Table II is more than [33] (108 Anger, 99 Fear, 320 
Neutral and 126 Sadness images) used and remaining 
obviously expressions in ours (120 Disgust, 270 
Happiness and 219 Surprise images) is less than [33] 
(120 Disgust, 282 Happiness, 225 Surprise images) used. 
 
5. Conclusion 
This paper proposed a novel weighting method for 
LDP operator and fusion feature of Boosted-LBP and 
Boosted-WLDP are also proposed. We achieve the 
successful recognition rate 91.10% on Cohn-Kanade 
database with proposed fusion feature than only use 
Boosted-LBP and Boosted-WLDP. 
Facial expression recognition is a challenge task 
due to each expression has different emotional 
representations. Until now, the research on facial 
expression recognition still focuses on six universal 
expressions and there is still a long way to go in 
cognizing facial expression from the psychology and 
physiology. 
 
ACKNOWLEDGEMENT 
 
The authors would like to thank the National 
Science Council, R.O.C. for the funding support of the 
project NSC 99-2221-E-216-050. 
 
REFERENCES 
 
[1] P. Ekman, W.V. Friesen, “Facial Action Coding System: A 
Technique for the Measurement of Facial Movement,” 
Consulting Psychologists Press, Palo Alto, 1978. 
[2] P. Ekman and W.Friesen, “Manual for the facial action 
coding system,” Consulting Psychologists Press, 1977 
[3] J. C. T. Kanade and Y. Tian, “Comprehensive database for 
facial expression analysis,” in Proceeding of IEEE 
International Conference on Face and Gesture Recognition, 
pp. 46-53, March 2000. 
[4] M. Pantic and L. Rothkrantz, “Automatic analysis of facial 
expressions: The state of the art,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 22, pp. 
1424-1445, December 2000. 
[5] M. Valstart, I.Patras, M. Pantic, “Facial action unit 
detection using probabilistic actively learned support 
vector machines on tracked facial point data,” IEEE 
Conference on Computer Vision and Pattern Recognition 
Workshop, vol. 3, pp. 76-84, 2005. 
[6] M. Valstar, M. Pantic, “Fully automatic facial action unit 
detection and temporal analysis,” IEEE Conference on 
Computer Vision and Pattern Recognition Workshop,  pp. 
149, 2006. 
  
Invitation Letter 
 
 
Dear PCM 2010 Authors, 
  
We are very pleased to inform you that your paper: 
 
Facial landmark detection by combining object detection and active 
shape model 
                                                                      
authored by: 
 
Hsu, Ting-Chia; Huang, Yea-Shuan; Cheng, Fang-Hsuan  
                                                                      
has been selected for presentation at the IEEE 11th Pacific-Rim 
Conference on Multimedia (PCM 2010), which will be held in Shanghai, 
China, from the 21st to 24th September 2010. 
 
As the hosting organization of this great event, it is our honor to 
invite you to come to Shanghai, China and attend the conference. 
Please check out the conference website, 
http://pcm2010.fudan.edu.cn/, for registration, travel and 
accommodation information. You can also contact our Local 
Organization Committee in case you need any help:  
http://pcm2010.fudan.edu.cn/org_com.htm 
 
We look forward to seeing you at the conference.  
 
 
Best regards,  
        
( Xiangyang XUE ) 
 
General Co-Chair PCM 2010 
Professor School of Computer Science, 
Fudan University, China 
 
表 Y04 
分時間講解論文，在場聽眾事後表示本論文發表非常清楚和完整，充分達到研究心得交
流目的。下二圖為 PCM 會議和論文發表時的照片 
 
圖一  PCM 會場與看板 
 
圖一  PCM 論文發表 
 
 此會議參加人數比預期的少，故可以有較多的時間彼此討論。由於會議專注於多媒體(特
別是影像和電腦視覺)的技術和應用，而所有與會人員都是從事於此領域研究，所以有很
好經驗交談和學習的機會。 
 
 
 
二、與會心得 
International Conference on
Machine Learning and Cybernetics 2011
Dear Author,
Congratulations. Your paper has been accepted for publication in the proceedings of the
International Conference on Machine Learning and Cybernetics (ICMLC) 2011.
Please verify the following items to ensure their accuracies:
(1) Please confirm the following:
Paper ID: 3747
Title: An Adaboost-Based Facial Expression Recognition Method
Authors: Yea-Shuan Huang,Chung-Hua University,Taiwan
SHUN-HSU CHUANG,Chung-Hua University,Taiwan
Fang-Hsuan CHENG,Chung-Hua University,Taiwan
Please be reminded your paper will be rejected if the title, names of author(s) or
the order of author list of your final version is different from the original version.
(2) If your paper exceeds six pages, you must pay HKD 540 (USD 70 #) per extra
page.
(3) Your paper will NOT be published in the conference proceedings unless you
COMPLETE EVERY STEP of the following tasks by 7-May-2011
a. Register on or before 7-May-2011
The registration fee:
Non-IEEE Member: USD 550 (equivalent to HKD 4270 #)
IEEE Member*: USD 500 (equivalent to HKD 3890 #)
Student*: USD 450 (equivalent to HKD 3500 #)
Please pay in Hong Kong Currency.
* Identification(s) may be required. Please bring your identification(s) to
the conference.
# As of 20-April-2011, the exchange rate is HKD 7.78 for one USD.
The registration fee of each participant should be settled by credit card
payment. For detail, please see the other attachment (creditcardpayment.
doc)
Please be reminded when you settle the registration fee, attach the Paper
ID, Paper Title and the Registrant Name as a remark.
Page 1/2
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           100 年 7 月 17  日 
報告人姓名  
黃雅軒 
 
服務機構 
及職稱 
 
  中華大學資工系  副教授 
 
     時間 
會議 
     地點 
2011.07.10~2011.07.15 
中國  桂林 
本會核定 
補助文號 
NSC 99-2221-E-216-050 
099-B02-007(產學計畫) 
會議 
名稱 
International Conference on Machine Learning and Cybernetics 
(ICMLC) 
發表 
論文 
題目 
An Adaboost-Based Facial Expression Recognition Method 
報告內容應包括下列各項： 
一、參加會議經過 
 
 此次會議共有 4 天，第一天是 Tutorial，題目為：Essentials of research methodology and 
effective dissemination of research results。第二天到第四天則是會議的口頭發表和海報展
示時間，大會還安排二個 Keynote speeches 和一個 Panel Discussion，他們的題目分別為 
 Keynote 1: (7/11) Machine Learning Challenges for Human Brain Decoding by Prof. 
Seong-Whan Lee. 
 Keynote 2: (7/12) Fuzzy Forecasting Based on High-Order Fuzzy Time Series and 
Genetic Algorithms by Prof. Shyi-Ming Chen. 
 Panel Discussion: (7/12) The Genesis of an Innovative Research Topic by Dr. T.K. Ho, 
Prof. H. Yan, Prof. V. Marik, and Prof S. Kwong.  
 我的論文安排於第二天(7/11)下午時段(15:10~16:50) 發表，題目為” An Adaboost-Based 
Facial Expression Recognition Method”，此篇論文結合了二種紋理特徵(WLDP 和 LBP)，
來進行臉部表情辨識。下圖為此會議的大會場景照片。 
附
件
三 
表 Y04 
 
 
三、建議 
 
 本校老師多參與國際性會議，除了介紹研究成果，增加學校的知名度以外，也能快速擴
展視野，建立合作管道，對未來研究和教學有很大的幫助。 
 鼓勵學校的研究生參與這種研究與應用結合的國際性會議，讓他們更了解研究的實用價
值，以激發學習和研究的熱誠。 
 
四、攜回資料名稱及內容   
 
會議論文集一冊和一片光碟 
 
 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：黃雅軒 計畫編號：99-2221-E-216-050- 
計畫名稱：多辨識器組合之強健式臉部表情辨識技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 7 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：■已技轉 □洽談中 □無 
其他：（以 100 字為限） 
已發表二篇 EI 會議論文：(1)An Adaboost-Based Facial Expression Recognition Method
和(2)A novel ASM-based two-stage facial landmark detection method，分別發表於
International Conference on Machine Learning and Cybernetics (ICMLC) 和 Pacific Rim 
Conference on Multimedia (PCM)國際會議中。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本計畫提出一種混合權重式區域方向特徵(WLDP)和二元化形態(LBP)的表情辨識方法。一
開始 WLDP 和 LBP 會分別對人臉影像進行特徵抽取，接著利用 PCA 分別對 WLDP 和 LBP 所抽
取出來的特徵進行特徵降維處理，最後將兩種特徵進行混合，產生出一種對人臉具有分辨
能力的混合特徵，並使用 SVM 分類器來進行表情辨識。實驗的資料庫是使用著名的
Cohn-Kanade 表情資料庫，該資料庫由於具有完整的表情資料，因此是表情辨識領域的研
究學者們常用來進行研究的一套表情資料庫。本論文所提出的方法，對 Cohn-Kanade 表情
資料庫進行 7類表情辨識，並使用 10-fold person-independent cross-validation 架構，
可以得到高達 91.1%的辨識率，效果優於一般常見的表情辨識演算法。這顯示我們所提出
的權重式區域方向特徵和辨識方法具有良好的表情辨識效果，具有研究和應用的價值。 
 
另外，雖然動態形狀模型(Active Shape Model, ASM)已經成功的被應用在臉部特徵點定
位上，然而當人們臉部有著誇張的臉部表情變化時，如：驚訝、大笑和挑眉等，其特徵點
定位的結果仍有明顯的誤差率。為了克服這個問題，本計畫提出串接式多階段的臉部特徵
點定位方法。第一階段，我們使用 Adaboosting 學習演算法來定位臉部特徵點中較有明顯
鑑別度之特徵點，而這些有明顯鑑別度的特徵點都是角點類型的地標點，如左右內眼角、
