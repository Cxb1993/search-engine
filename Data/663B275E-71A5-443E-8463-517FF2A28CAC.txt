 2
quantization noise, we propose to detect the 
recompressed images and then to locate the 
tampered blocks. 
 
Keywords: image tampering detection, JPEG 
compression, periodicity analysis, quantization 
noise model 
 
二、緣由與目的 
    隨著數位影像處理的技術日趨成熟，數位
影像資料已可被輕易地仿造與竄改
(tampering)，某些人造、合成或部份偽造的
影像(fake image)不僅幾可亂真，且對人眼而
言，幾乎察覺不出該影像有任何不尋常之
處。因此，當數位相片(photograph)的內容越
來越廣泛地用來做為某些事件發生的證據
時，影像資料內容的可信賴度(authenticity)
就變得日趨重要。現有的偵測方法約可分為
兩大類：一類是藉由數位浮水印技術加入外
藏驗証資料的主動式方法(active approach)，
另一類則是不需外藏驗証資料的被動式方法
(passive approach)或稱為非侵入式方法
(non-intrusive approach)。 
儘管目前仍有許多研究持續探討以主動
式 方 法 從 事 影 像 真 偽 鑑 定 (image 
authentication)，然而，由於多數數位影像在
取像過程中或之後並未藏入數位浮水印，且
目前仍未有被廣泛認可或使用的浮水印技
術；因此，以被動式方法進行影像資料內容
的可信賴度分析就成了近來熱門的研究課
題。以下在本計畫中所述的數位影像鑑識
(digital image forensics)的研究，均著重在被
動式方法的分析與研究，而數位浮水印相關
的課題並不在本計畫的研究範圍中。 
數位影像鑑識的範疇涵蓋甚廣，舉例來
說，像是判斷影像是否直接由相機拍攝而
得、偵測影像內容是否遭到竄改以及辨識影
像的來源相機模組等，儘管如此，其主要精
神乃是要利用影像本身所包含的某些基本特
性以達到鑑識影像的目標。因此，基於上述
在目前數位影像鑑識領域的應用，如何去研
發更可靠的數位影像鑑識方法，令其延伸至
更多的應用層面，成為近年來快速竄起的研
究趨勢。以下即簡述相片取得方式判斷、相
片竄改偵測和來源相機模組辨識等三種目前
常見的應用： 
1)相片取得方式判斷(Photograph Acquiring 
Diagnosis) 
    相片取得方式判斷(photograph acquiring 
diagnosis)的主要目的在於判斷相片是否直接
由相機拍攝而得，這對於裁定相片的真實性
方面，是相當有參考價值的。舉例來說，在
相機成像的過程中，採用了色彩濾波器陣列
的內插(color filter array interpolation)來協助
還原相片中所有像素的顏色，因而使得相機
最後所輸出之相片的像素值會具有特定的關
聯性，當相片是直接由相機拍攝而得時，這
樣的關聯性表現就會相當顯著；反之，則關
聯性的表現就不甚明顯。因此，透過量度關
聯性的表現強弱，可以達到判斷相片是否直
接由相機拍攝而得的依據。另一個常見的特
性是利用相機響應函數 (camera response 
function, CRF)模型；當相片中被偵測出 CRF
的模型時，便可得知該相片是由相機直接拍
攝而得。此外，相片的特徵尚包含如 CCD感
應器所產生的雜訊(sensor pattern noise)，而此
類雜訊一般具有規則性，故可藉由偵測雜訊
存在與否，得知是否為由相機直接照出來的
照片。 
2) 相片竄改偵測 (Photograph Tempering 
Detection) 
    相片竄改偵測 (photograph tempering 
detection)旨在判斷相片是否遭到接合
(splicing)或是重新取樣(re-sampling)等不當
的竄改行為，並進一步地從相片中找出可能
遭到竄改的區塊，其用途除了判別相片的真
偽之外，在斷定相片是否足以做為物證上也
是相當重要。由於相片被竄改之後，相片中
和相機成像過程有關的資訊也將隨之變化，
而現今相機中多數包含後處理程序，如白平
衡校正(white-point correction)以及 JPEG壓縮
等，故這些後處理程序所產生的影像特徵亦
可納入考量。目前主要是利用相片中和相機
成像過程以及 JPEG 壓縮此類常見的影像後
處理等相關特徵資訊的變化情形來判斷相片
是否遭到竄改。 
3)相機模組辨識(Camera Model Identification) 
 4
賴度指標，使其在做為證據時更具說服力。 
 
三、結果與討論 
In the third year of this project, we focus 
on the tampering detection issues for JPEG 
images. Given a compressed image, we assume 
tampering operations always involved 
recompression, and then propose to 
characterize the tampered images by both 
periodicity and quantization noise analysis. 
 
3.1 Tampering detection by blocking 
periodicity analysis 
Since JPEG image format has been a 
popularly used image compression standard, 
tampering detection in JPEG images now plays 
an important role. The artifacts introduced by 
lossy JPEG compression can be seen as an 
inherent signature for compressed images. 
When a compressed image is tampered and/or 
recompressed, the tampered image may inherit 
the primary compression artifacts. Some 
research attempt relies on detection of blocking 
artifacts. In [33], Luo et al. proposed a blocking 
artifact characteristics matrix (BACM) to 
measure the symmetrical property of the 
blocking artifacts introduced by JPEG encoder. 
Their experiments show that, once a JPEG 
image has been cropped and recompressed, the 
regular symmetry of BACM will be destroyed. 
However, from our simulation, we observed 
that this symmetric property measured in 
spatial domain is highly content dependent and 
not always provides as a reliable measurement. 
Sometimes it is very difficult to determine 
whether they have been tampered or not from 
the asymmetry in their corresponding BACMs. 
Motivated from the above observation, in 
our research, we aim to analyze the blocking 
artifacts from its periodic property and try to 
devise a robust method insensitive to different 
image contents. We first use a simple and 
effective method [34] to measure the pixel 
blockiness. For each pixel ),( yx , we compute 
the local pixel difference ),( yxf  as its 
blockiness by 
)1,(),1()1,1(),(),( +−+−+++= yxIyxIyxIyxIyxf , (1) 
where ),( yxI is intensity value of pixel ),( yx . 
Assume we classify image pixels into two 
classes: within-block pixels and across-block 
pixels, for images with mostly smooth regions, 
the local pixel difference ),( yxf  should be 
comparatively large for across-block pixels and 
close to zero for within-block pixels. Moreover, 
when comparing ),( yxf  within a small spatial 
neighbourhood, we found that ),( yxf  of 
within-block pixels are highly similar to each 
other, while across-block pixels have very 
different local pixel differences. 
Therefore, we assume, in a small 
neighbourhood of ),( vyuxf ++  with 1,0 ≤≤ vu , 
the within-block pixel ),( yxf  is linearly 
correlated to its neighbours. Thus, we model 
the probability of ),( yxf ’s belonging to 
within-block pixels as a Gaussian:  
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛ ++−
−
×=
∑
2
2
,
,
2
)),(),((
exp                        
2
1}|),(Pr{
σ
α
πσ
vyuxfyxf
ckwithin_bloyxf
vu
vu
. (2) 
 
Then, by Bayes’ rule, the posterior probability 
of each ),( yxf ’s belonging to within-block 
pixels becomes 
 
)_Pr()_|),(Pr(           
)),(|_Pr(
blockwithinblockwithinyxf
yxfblockwithin ∝ , (3) 
 
and should be proportional to equation (2) if 
equal prior probabilities for both classes is 
assumed. Next, we use EM algorithm to 
estimate each pixel’s probability of being a 
within-block pixel and the unknown weights 
 6
detection results in Table 1 show that the 
performance of our proposed method indeed 
outperforms BACM in most cases. 
 
3.2 Detecting doubly compressed images 
based on quantization noise model 
Since JPEG has been a popularly used 
image compression standard, forgery detection 
in JPEG images now plays an important role. 
Forgeries on compressed images often involve 
recompression and tend to erase those forgery 
traces existed in uncompressed images. We 
could, however, try to discover new traces 
caused by recompression and use these traces 
to detect the recompression forgeries.  
Forgeries on JPEG images often involve 
recompression and thus change the original 
compression characteristics. Most existing 
forgery detection techniques attempt to detect 
the inconsistency on compression 
characteristics. Luo et al. [33] use a spatial 
domain method to detect the change on 
symmetric property of blocking artifacts for 
shifted and recompressed images. Our earlier 
work analyses the blocking artifacts from their 
periodicity and proposes a blocking periodicity 
model to detect whether an image has been 
cropped and recompressed or not. In frequency 
domain analysis, Benford’s law has been used 
to model the statistic change on DCT 
coefficients caused by recompression [36, 37]. 
Also, He et al. [38] proposed to detect and 
locate doubly compressed regions via DCT 
coefficient analysis. However, although these 
frequency domain methods try to detect the 
change on DCT distributions, these methods 
may fail to detect the recompression forgeries 
when a JPEG image has been spatially shifted 
or cropped with misaligned block boundaries 
from the original image. On the other hand, the 
spatial domain methods, which rely on 
detecting the abnormality of blocking artifacts, 
may fail to detect the recompression forgeries 
without any shifted or misaligned block 
boundaries. Recently, Farid [39] pointed out 
that JPEG ghosts property may reveal the trace 
of recompression. Assume we recompress a 
compressed image with different quality factors 
and measure the difference of DCT coefficients 
before and after the recompression for each 
setting. Then this difference (i.e. the JPEG 
ghosts) is minimized when the recompression 
quality factor is the same as the primary quality 
factor. Although the idea is simple, this 
approach needs a global test for all possible 
compression quality factors. In addition, once 
the forged regions have been shifted with  
misaligned block boundaries, one will have to 
detect the JPEG ghosts for all 64 possible 
alignments. 
Motivated from the above discussion, in 
this paper, we aim to build a novel and 
theoretical formulation to model the 
compression characteristics. In JPEG lossy 
compression, the quantization error, which is 
the difference between original signal and 
quantized digital value, is usually treated as the 
compression distortion. Here, we introduce 
another perspective to analyze the quantization 
noise for single and doubly compressed images. 
We first model the quantization noise as 
ncnccAx ′′+′′=′+′==  , (9)
where A  is a 64x64 DCT component basis 
matrix, x  is the original intensity of one 8x8 
block, c  is the DCT coefficients vector, c′  
and c ′′  are the quantized DCT coefficients 
vectors after first and second compression, 
respectively, and n′ , n ′′  are their 
 8
parameter 
2
iσ  by the iterative algorithms such 
as EM. Next, we obtain the posterior 
probability )|( 2 kp nω of each block: 
∏∏
== +==
dim
1 2,1,
2,
dim
1
,22 )|()|(
)|(
)|()|(
i ikik
ik
i
ikk wnpwnp
wnp
nwpp nω .  (16)
Fig. 7 shows two recompression forgery 
examples, with aligned and misaligned block 
boundaries, and their corresponding posterior 
maps. In either case, our proposed quantization 
noise model successfully characterizes the 
difference between the doubly compressed 
region and the single compressed surrounding 
area. 
After we introduced a new and robust 
quantization noise model to characterize the 
differences between single and double 
compressions, the only difficulty to be resolved 
now is: where can we get the original and 
un-quantized image? Since the only 
information we have is the JPEG image, we 
then propose to approximate the ground truth 
image from the JPEG image using the image 
restoration techniques including deblocking 
process [41] and low frequency compensation 
[42]. Finally, since the quantization noise 
measured with the estimated ground truth is no 
longer bounded by the quantization interval 
especially in single compression case, we 
modify our quantization model for single 
compression 1ω  using the more general 
Laplacian distribution: 
∏∏
==
−==
dim
1
,,
dim
1
1,1 ),0|ˆ()|()|(
i
iikik
i
ikk qccLwnpp ωn  .  (17)
To verify the robustness of the proposed 
quantization noise model, here we first assume 
that the un-quantized ground truth images are 
available during the fogery detection. In this 
experiment, each image size is 1024x1024. We 
first crop a subimage with size 480x480 and 
compress this subimage with JPEG quality 
factor 1QF . Next, we copy this compressed 
subimage back into the original raw image and 
then compress the spliced image with JPEG 
quality factor 2QF . We test 500 images for 
each quality setting 1QF  and 2QF  and derive 
a ROC curve for each test image. Fig. 8 shows 
the averaged ROC curve when 1QF  equals to 
50. From Fig. 8, the proposed quantization 
noise model achieves high performance in 
detecting the copy-paste-recompression 
forgery. 
Table 2 shows the detection accuracy of 
each quality setting. Here we use the averaged 
area of ROC curve to measure the detection 
rate. When 21 QFQF < , the detection accuracy 
are almost close to 100%; while  21 QFQF > , 
the detection accuracy ranges from 60% to 90%. 
The degraded performance in the case of 
21 QFQF >  is because the quantization noise is 
dominated by the quantization step of the 
second compression and become less 
distinguishable between single and double 
compression cases. Note that, when 21 QFQF = , 
since the DCT coefficient does not change after 
recompression, this recompression case is not 
detectable with the proposed quantization noise 
model. 
Using the proposed framework, we now 
approximate a reliable ground truth image and 
apply it to forgery detection. Fig. 9 shows the 
detection result, where the forged region in Fig. 
9(a) is first compressed with quality factor 50 
and then recompressed with quality factor 80. 
Fig. 9(b) indicates the posterior map, where the 
 10
Source Identification,” Master Thesis, Department 
of Computer Science, National Tsing Hua 
University, 2007. 
[31] L.W. Huang, “A Study of Image Forensics Using 
Sensor Pattern Noise and Camera Response 
Function,” Master Thesis, Department of Computer 
Science, National Tsing Hua University, 2007.  
[32] S.H. Chen and Chiou-Ting Hsu, “Source Camera 
Identification Based on Camera Gain Histogram,” 
Proc. ICIP 2007, San Antonio, Texas, USA, Sep. 
2007. 
[33] W. Luo, Z. Qu, J. Huang, and G. Qiu, “A Novel 
Method for Detecting Cropped and Recompressed 
Image Block,” Proc. ICASSP, vol.2, pp. 217-220, 
April 2007. 
[34] Z. Fan and R.L. de Queiroz, “Identification of 
Bitmap Compression History: JPEG Detection and 
Quantizer Estimation,” IEEE Trans. on Image 
Processing, vol. 12, no. 2, pp. 230-235, Feb. 2003. 
[35] A.C. Popescu and H. Farid, “Exposing Digital 
Forgeries in Color Filter Array Interpolated 
Images,” IEEE Trans. on Signal Processing, vol. 53, 
no. 10, pp. 3948-3959, 2005. 
[36] D. Fu, Y. Q. Shi, and W. Su, “ A Generalized 
Benford’s Law for JPEG Coefficients and Its 
Applications in Image Forensics,” SPIE, 2007. 
[37] B. Li, Y. Q. Shi, and J. Huang,, “Detecting Doubly 
Compressed JPEG Images by Mode Based First 
Digit Features,” Proc. MMSP, 2008. 
[38] J. He, Z. Lin, L. Wang, and X. Tang, “Detecting 
doctored JPEG images via DCT coefficients 
analysis,” in European Conference on Computer 
Vision, Graz, Austria, 2006. 
[39] H. Farid, “Exposing digital forgery from JPEG 
ghosts,” IEEE Trans. on Information Forensic and 
Security, vol.1, 2009. 
[40] A. Zakhor, “Iterative procedures for reduction of 
blocking effects in transform image coding,” IEEE 
Trans. on Circuit and System for Video Technology, 
vol. 2, no. 1, March 1992. 
[41] Y. Kin, C. S. Park, and S. J. Ko, “Fast POCS based 
post-processing technique for HDTV,” IEEE Trans. 
on Consumer Electronics, vol. 49, no. 4, November 
2003. 
[42] Y. C. Liaw, W. Lo and Z. C. Lai, “Image restoration 
of compressed image using classified vector 
quantization,” Pattern Recognition, pp 329-340, 
2002. 
[43] Y.Y. Li, “A Study of Feature Analysis for Camera 
Source Identification,” Master Thesis, Department 
of Computer Science, National Tsing Hua 
University, 2007. 
[44] L.W. Huang, “A Study of Image Forensics Using 
Sensor Pattern Noise and Camera Response 
Function,” Master Thesis, Department of Computer 
Science, National Tsing Hua University, 2007.  
[45] C.Y. Chen, “Image Forgery Detection by JPEG 
Blcok Inconsistency,” Master Thesis, Department 
of Computer Science, National Tsing Hua 
University, 2008. 
[46] H.Y. Lin, “Blind Camera Fingerprinting and Its 
Application in Tamper Detection,” Master Thesis, 
Department of Computer Science, National Tsing 
Hua University, 2008. 
[47] S.H. Chen and Chiou-Ting Hsu, “Source Camera 
Identification Based on Camera Gain Histogram,” 
Proc. ICIP 2007, San Antonio, Texas, USA, Sep. 
2007. 
[48] Y. L. Chen and Chiou-Ting Hsu, “Image Tampering 
Detection by Blocking Periodicity Analysis in 
JPEG Compressed Images,” Proc. MMSP, Oct. 
2008. 
[49] C.C. Hsu, T.Y. Hung, C.W Lin, and Chiou-Ting Hsu, 
“Video Forgery Detection using the Correlation of 
Noise Residue,” Proc. MMSP, Oct. 2008 
[50] Y. L. Chen and Chiou-Ting Hsu, “Detecting Doubly 
Compressed Images Based on Quantization Noise 
Model and Image Restoration,” Proc. MMSP, Oct. 
2009. 
 
 
 
 
 12
 
Fig.4. The three non-overlapping regions in peak window. 
 
 
(a) mean ratio 
 
(b) variance ratio 
 
(c) entropy 
Fig.5. The three features (mean ratio, variance ratio and entropy) for the 500 test images, where 
the first 250 images are single compressed and the 251st to the 500th images are 
cropped-and-recompressed images. 
R1
R2 
R3
 14
  
(a) (b) 
Fig. 9. (a) A forged image, where the red square indicates the forged region; and (b) the posterior 
map of (a). 
 
 
Table 1. Detection accuracy (%) of single compression(Quality Factor=QF1) and double 
compression(Quality Factor=QF2,  cropped-and-recompressed with Quality Factor  QF1) 
QF1 
QF2 
70 75 80 85 90 95 
Proposed 80.8 84.4 89.2 94 97.2 99.2 50 
BACM 78.4 83.6 90.4 93.6 95.4 95.2 
Proposed 72.6 84 83.4 93.6 96 99 60 
BACM 73.6 79.4 86.4 92 93.8 95.2 
Proposed 66 80.2 80.2 90.7 96.6 99 70 
BACM 68.8 73.4 78.8 89.8 95.2 95.4 
Proposed 69.6 81.6 77.4 86.7 95.8 99.4 80 
BACM 72.2 73.8 79.6 91.8 94.4 95.4 
Proposed 65.9 75.7 78.3 88.7 97 98.8 90 
BACM 65 70.2 76.6 86.8 94.6 95.8 
 
Table 2. Detection accuracy(%), which indicates the average of ROC curve. For each quality 
setting, there are 500 forged images. 
        QF1 
QF2 50 60 70 80 90 
50  83.8 94.7 98.3 99.4 
60 76.8  89.9 97.8 99.5 
70 82.7 84.1  95.7 99.5 
80 66.1 89.4 88.5  99.2 
90 57.2 66.1 65.4 93.7  
 
 
泳池邊舉行，和多位學者相談甚歡，也遇到幾位下個月即將前往開羅參加 ICIP09 的學
者，大家相約下個月於開羅再見。6 月晚上為 banquet，地點在 Rio 目前最著名的餐廳
Porcão Rio，一開始大家看到美味的 buffet bar即把美味菜餚放在盤中，後來才發現所有
當地教授的盤子是空的，原來 buffet bar 中提供的只是開胃菜，主菜是之後不斷送上來
的各式烤肉，大家也喝了很多當地著名的 caipirinha 及美味甜點。此次的會議，或許由
於拉丁美洲距離亞洲實在太遠，所見到的來自亞洲的學者即少(如：香港教授一位，台
灣的教授就我一個，日本早稻田大學教授一位)，或許也因為如此，此行中來自不同國
家的學者互相交流的機會也大為增加。 
我的論文是安排在7日上午9:30到11:00之間的poster session中，此次所發表的論文名
稱為“Detecting Doubly Compressed Images Based on Quantization Noise Model and Image 
Restoration”，是我們在image forensics方面的研究成果。在我的poster session中，很高興
和多位從事此一領域研究的專家學者交換研究心得，對於我們未來在此方向的研究頗有
助益。 
研討會於7日晚上結束後，我於Rio又多停留兩日，前往 Ipanema海灘及著名的Sugar 
Loaf參觀，只可惜此行在 Rio的一星期中，只有一天是晴天，其它皆是又濕又冷的天氣，
海灘上空盪盪的，和原本想像的差很多。之後我即於 10日搭機離開，於 11日返回美國
繼續我於馬里蘭大學的進修。 
 
 
二、攜回資料 
 
攜回資料有大會會議論文 CD資料片一片。 
 
             
image and its uncompressed version. Since it is impractical to 
assume that the uncompressed image is available during 
forgery detection, in this paper, we further propose to 
approximate the uncompressed ground truth image using 
image restoration techniques. 
The rest of this paper is organized as follows. Section II 
describes the quantization noise model. Section III presents 
the proposed forgery detection approach based on image 
restoration. Section IV shows the experimental results. Finally, 
section V gives the conclusion. 
 
II. QUANTIZATION NOISE MODEL 
In JPEG lossy compression, the quantization error, which is 
the difference between the original signal and the quantized 
digital value, is usually treated as the compression distortion. 
Here, we introduce another perspective to analyze the 
quantization noise for single and doubly compressed images. 
We first model the quantization noise as 
ncnccAx ′′+′′=′+′==  , (1)
where A  is a 64x64 DCT component basis matrix, x  is the 
original intensity of one 8x8 block, c  is the DCT coefficients 
vector, c′  and c ′′  are the quantized DCT coefficients vectors 
after first and second compression, respectively, and n′ , n ′′  
are their corresponding quantization noises. In (1), the 
quantized DCT coefficients c′  and c ′′  are represented by 
⎥⎦
⎤⎢⎣
⎡
′⋅′=′ i
i
ii q
croundqc  , and 
⎥⎦
⎤⎢⎣
⎡
′′
′⋅′′=′′
q
croundqc iii , 
(2)
where q′ , q ′′  are the corresponding quantization steps, and i  
indicates the index of the 64 DCT components. 
In equation (1), our goal is to analyze the difference 
between n′  and n ′′ . The quantization constraint set (QCS) 
theorem [16] showed that the un-quantized DCT coefficient 
should be bounded by 
⎥⎦
⎥⎢⎣
⎢ ′+′≤≤⎥⎦
⎥⎢⎣
⎢ ′−′
22
qccqc  .  (3)
In other words, the quantization noise cc ′−  is bounded by 
the quantization interval. Similarly, we could also derive the 
quantization noise of double compression by 
⎥⎦
⎥⎢⎣
⎢ ′′≤′′−′≤⎥⎦
⎥⎢⎣
⎢ ′′−
22
qccq  .  (4)
Combining equations (3) and (4), we obtain 
⎥⎦
⎥⎢⎣
⎢ ′′+⎥⎦
⎥⎢⎣
⎢ ′≤′′−≤⎥⎦
⎥⎢⎣
⎢ ′′+⎥⎦
⎥⎢⎣
⎢ ′−
22
)
22
( qqccqq  .  (5)
From equation (5), the quantization noise between c ′′  and c   
is now no longer bounded by the last quantization step q ′′ . 
Assume the un-quantized DCT coefficient c  is available, 
since we could obtain the quantization step q ′′  from the JPEG 
header, we could then distinguish whether the quantization 
noise of each DCT coefficient follows QCS theorem or not. 
Fig. 1 shows the histogram of quantization noises of the 
first AC term after single and double compression, 
respectively. In Fig. 1(a), q′ = 5, thus the quantization noise is 
bounded by [-2,2] . In Fig. 1(b), although the quantization step 
q ′′  obtained from the JPEG header is also 5, from equation (5) 
and given that the primary quantization step q′ =12, the 
quantization noise n ′′  is no longer bounded by [-2,2] and does 
not follow the QCS theorem anymore. (Note that, here the 
distribution is a little bit different from what calculated by 
equation (5) because the computation of FDCT and IDCT in 
the second compression may result in rounding errors).  
     
  
(a) (b) 
 
(c) 
Fig. 1. The quantization noise histogram of 1st AC term: (a) single 
compression case, quantization step=5;  (b) double compression case, 1st 
quantization step=12, 2nd quantization step=5; and (c) triple compression case, 
1st quantization step=12, 2nd quantization step=7, 3rd quantization step=5. 
 
Fig. 2 shows the pdfs of quantization noises of the first nine 
DCT terms in zig-zag scan order. Similar to Fig. 1, the pdf 
after single compression is nearly uniform and bounded by the 
quantization interval. On the other hand, the pdfs of 
quantization noises after double compression behave more 
like Gaussians. We now explain the two different pdfs from a 
theoretical perspective. Assume the two quantization noises 
cc ′−  and cc ′′−′ after each single compression are two 
independent uniform distributions. Thus, the density of the 
quantization noise cc ′′−  after double compression equals the 
convolution of the two uniform densities cc ′−  and cc ′′−′  
[17]. Moreover, if further recompression is conducted on the 
image, then the pdf of the quantization noise between the final 
DCT coefficient and its un-quantized coefficient would be the 
convolution of a sequence of uniform densities and become a 
near Gaussian. For example, as shown in Fig. 1(c), the 
quantization noise distribution of a triple compressed image is 
indeed more like Gaussian than the double compression case 
in Fig. 1 (b). Therefore, we could characterize the 
distributions of quantization noises for single compression and 
recompression with completely different models. Note that, 
distortion, and loss of high frequency details, should be 
eliminated or compensated. 
Fig. 4 shows the framework of our approach. Below we 
will describe our deblocking process and low frequency 
compensation. Although other complex image restoration 
techniques are also applicable to obtain a good approximation, 
here we just attempt to adopt a simple and effective method to 
demonstrate the feasibility of the proposed quantization noise 
models.  
 
A. Deblocking  
Deblocking process is the most intuitive method to 
eliminate compression distortions. Most existing approaches 
are applied on spatial domain by filtering only the image 
pixels around block boundaries. Although the spatial-domain 
deblocking approaches achieve good performance in visual 
quality, these methods usually fail to recover the true pixel 
values because the quantization noise affects not only the 
boundary pixels but the whole 8x8 block. 
Moreover, since our quantization noise model is formulated 
in DCT domain, here, we adopt the DCT-domain deblocking 
method [18]. Consider one DCT block b and its one-pixel- 
diagonally-shifted block 'b . From [18], the shifted block 'b  
should have more nonzero values in high order AC terms than 
b  because of the blocking artifacts. Also, both 'b  and b   
should still have similar high frequency information. 
Therefore, the DCT-domain deblocking process would adjust 
the number of nonzero AC terms of  'b  according to b . We 
modify this approach for our ground truth approximation by 
the following three steps. 
 
z Step 1: Adjust the number of nonzero AC terms of  
'b  according to b [18]. 
z Step 2: For each block b , project the DCT 
coefficients to a convex set according to QCS:  
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
⎥⎦
⎥⎢⎣
⎢×−<⎥⎦
⎥⎢⎣
⎢−
⎥⎦
⎥⎢⎣
⎢×+>⎥⎦
⎥⎢⎣
⎢+
=
otherwise ,              
2
ˆ  if      ,
2
ˆ
2
ˆ if       ,
2
ˆ
,
,,,
,,,
,
ik
i
ikik
i
ik
i
ikik
i
ik
ik
c
qccqc
qccqc
c κ
κ
 . 
 (9)
 
z Step 3: Repeat step 1, until the DCT coefficients of 
all blocks are no longer changed.  
 
In equation (9), the parameter κ  is chosen as 3 in our 
experiment because we assume the quantization noise of 
double compression would not exceed three times of the 
quantization interval. Fig. 5 shows the quantization noise 
distribution measured using the deblocked image as the 
estimated ground truth and the one with the un-quantized 
ground truth. From Fig. 5, with the deblocked DCT 
coefficients, the difference in quantization noise distribution 
for single and double compression cases, though different 
from the ideal case, is still distinguishable. 
B. Low frequency compensation 
In addition to blocking artifacts, another compression 
distortion is the loss of higher frequency detail. Research on 
image restoration has pointed out that it is almost impossible 
to predict the original higher frequency detail without any 
prior knowledge. Therefore, many learning-based methods 
have been introduced to deal with this problem. The codebook 
design in vector quantization (VQ) methods is useful to 
represent general higher frequency contents, such as texture or 
edge. Here, we adopt the VQ based approach [19] for reliable 
ground truth approximation. 
Nevertheless, our experiments show that high frequency 
compensation (HFC) usually results in poor performance. The 
reason is because VQ based approach attempts to compensate 
high frequency detail in terms of visual quality but not in 
terms of accuracy. Moreover, most traditional compensation 
methods are conducted in spatial domain and are often less 
effective on DCT coefficients. Therefore we modify the 
method proposed in [19] and compensate the DCT 
coefficients directly. In addition, instead of HFC, we 
introduce the idea of low frequency compensation (LFC). 
Only the first fifteen DCT coefficients in zig-zag scan order 
would be compensated.  
Fig. 6 shows the distributions of quantization noise 
magnitude. The quantization noise distribution after low 
frequency compensation now better approximates the ideal 
case. 
 
  
  
(a) (b) (c) (d) 
Fig. 5. The quantization noise distributions, where the 1st row is obtained 
when the ground truth is available, and the 2nd row is obtained with the 
estimated ground truth after the deblocking process: (a) DC term; (b) 2nd AC 
term; (c) 4th AC term; and (d) 8th AC term. 
 
  
  
  
(a) (b) (c) (d) 
Fig. 6. The magnitude distribution of quantization noise; where the 1st row is 
obtained with ground truth, the 2nd row is obtained with the estimated 
groundtruth after deblocking process, and the 3rd row is obtained with the 
estimated groundtruth after deblocking and low frequency compensation: (a) 
DC term; (b) 1st AC term; (c) 5th AC term; and (d) 10th AC term. 
V. CONCLUSION 
In this paper, we propose a novel approach to detect the 
double compression forgeries. We have shown that the 
proposed quantization noise model indeed characterize the 
change on compression characteristics before and after 
recompression and have justified the effectiveness of the 
proposed model with promising experimental results. The 
contributions of this work include: (1) a theoretical and 
content-independent model is proposed to detect double 
compression forgeries; and (2) the image restoration 
techniques are adopted to resolve the practical forgery 
detection problem. Using this restoration perspective, many 
other image acquisition properties could also be involved in 
this framework. In addition, the proposed approach can 
successfully locate the forged region as small as 8x8 block, 
either with aligned or misaligned block boundary cases. With 
these advantages, we believe the proposed model could also 
be combined with other existing forensic features to solve 
more complex forgery problems on compressed images. 
 
 
 
(a) 
 
(b) 
 
(c) 
Fig. 8. (a) A compressed image with quality factor 80; (b) the difference map 
between (a) and its restored result; and (c) the enlarged view of the small 
square  in (b). 
 
 
  
(a) (b) 
Fig. 9. (a) A forged image, where the red square indicates the forged region; 
and (b) the posterior map of (a). 
 
REFERENCES 
[1] A.C. Popescu and H. Farid, “Exposing Digital Forgeries by Detecting 
Traces of Re-sampling,” IEEE Trans. on Signal Processing, vol. 53, 
no.2 ,pp 758-767, 2005. 
[2] S. Lyu and H. Farid, “How Realistic is Photorealistic?” IEEE Trans. on 
Signal Processing, vol. 53, no. 2, pp. 845-850, Feb. 2005. 
[3] A.C. Popescu and H. Farid, “Exposing Digital Forgeries in Color Filter 
Array Interpolated Images,” IEEE Trans. on Signal Processing, vol. 53, 
no. 10, pp. 3948-3959, 2005. 
[4] A. Swaminathan, M.Wu, and K.J.R. Liu, “Non-intrusive Component 
Forensics of Visual Sensors Using Output Images,” IEEE Trans. on 
Info. Forensics and Security, vol. 2, no. 1, pp. 91-106, March 2007. 
[5] J. Lukas and J. Fridrich, “Digital Camera Identification From Sensor 
Pattern Noise,” IEEE Trans. on Info. Forensics and Security, vol. 1, no. 
2, pp. 205-214, June 2006. 
[6] J. Lukas and J. Fridrich, “Estimation of Primary Quantization Matrix in 
Double Compressed JPEG Images,” Digital Forensic Research 
Workshop, Cleveland, Ohio, Aug. 2003. 
[7] S. Ye, Q. Sun, and E.C. Chang, “Detecting Digital Image by 
Measuring Inconsistencies of Blocking Artifact,” Proc. ICME, pp. 12-
15, July 2007. 
[8] J. Fridrich, M. Goljan, and R. Du, “Steganalysis based on JPEG 
compatibility,” SPIE Multimedia Systems and Applications IV, Denver, 
CO, August 2001, pp. 275-280. 
[9] Z. Fan and R.L. de Queiroz, “Identification of Bitmap Compression 
History: JPEG Detection and Quantizer Estimation,” IEEE Trans. on 
Image Processing, vol. 12, no. 2, pp. 230-235, Feb. 2003. 
[10] W. Luo, Z. Qu, J. Huang, and G. Qiu, “A Novel Method for Detecting 
Cropped and Recompressed Image Block,” Proc. ICASSP, vol.2, pp. 
217-220, April 2007. 
[11] Y. L. Chen, and C. T. Hsu, “Image Tampering Detection by Blocking 
Periodicity Analysis in JPEG Compressed Images,” Proc. MMSP, 
2008. 
[12] D. Fu, Y. Q. Shi, and W. Su, “ A Generalized Benford’s Law for JPEG 
Coefficients and Its Applications in Image Forensics,” SPIE, 2007. 
[13] B. Li, Y. Q. Shi, and J. Huang,, “Detecting Doubly Compressed JPEG 
Images by Mode Based First Digit Features,” Proc. MMSP, 2008. 
[14] J. He, Z. Lin, L. Wang, and X. Tang, “Detecting doctored JPEG 
images via DCT coefficients analysis,” in European Conference on 
Computer Vision, Graz, Austria, 2006. 
[15] H. Farid, “Exposing digital forgery from JPEG ghosts,” IEEE Trans. 
on Information Forensic and Security, vol.1, 2009. 
[16] A. Zakhor, “Iterative procedures for reduction of blocking effects in 
transform image coding,” IEEE Trans. on Circuit and System for Video 
Technology, vol. 2, no. 1, March 1992.  
[17] A. Papoulis, “Probability, random variables, and stochastic process,” 
4th edition, 2002. 
[18] Y. Kin, C. S. Park, and S. J. Ko, “Fast POCS based post-processing 
technique for HDTV,” IEEE Trans. on Consumer Electronics, vol. 49, 
no. 4, November 2003. 
[19] Y. C. Liaw, W. Lo and Z. C. Lai, “Image restoration of compressed 
image using classified vector quantization,” Pattern Recognition, pp 
329-340, 2002. 
 
96年度專題研究計畫研究成果彙整表 
計畫主持人：許秋婷 計畫編號：96-2628-E-007-142-MY3 
計畫名稱：數位影像鑑識之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 4 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
