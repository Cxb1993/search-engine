control techniques have be developed in this research 
to successfully assist the robot to accomplish the 
autonomous operations. Remote operated system has 
been also implemented on this robot so that the robot 
can provide a reliable performance in a harsh 
environment or on untrained operations where autonomy 
is not possible. 
英文關鍵詞： Service Robot, Dual Robot Arms, Stereo Vision 
 
 I 
 
 
目錄 
1. 中文摘要 ................................................................................................................................................ 1 
2. English Abstract: ..................................................................................................................................... 1 
3. 前言 ........................................................................................................................................................ 2 
4. 研究目的 ................................................................................................................................................ 2 
5. 文獻探討 ................................................................................................................................................ 2 
6. 研究方法 ................................................................................................................................................ 4 
I. Progress of Hardware Design .......................................................................................................... 4 
II. Robot vision for object recognition ................................................................................................ 13 
1. Simple object ......................................................................................................................... 13 
2. Complex object ..................................................................................................................... 13 
3. Stereo Vision Cameras for 3D Imaging .................................................................................. 14 
III. Tele-control ............................................................................................................................... 15 
IV. Object operation parameters definition and parameter-based robot arm autonomous operations . 17 
7. 結果與討論 .......................................................................................................................................... 22 
8. 參考文獻 .............................................................................................................................................. 23 
 
 
 2 
 
3. 前言 
 
本研究計畫目的在使用三年時間研製出ㄧ部全自主式(autonomous)運作的具雙機器手臂及立體視
覺的新式服務型機器人，實現以立體視覺及其他感應訊號快速即時引導機器人移動定位，並配合立體
視覺使用雙手臂進行多種動作的技術與系統整合平台。這套技術與系統平台包含能即時操控機器手臂
作動之快速位置與力量控制系統合成方法，可即時以高動力準確度方式移動手臂終端挾持器(grippers; 
end effectors)內工作物件，並對該物件施以一定力量之操作。具多機器手臂系統及立體視覺系統之全自
主式機器人，能夠連續接收環境資訊並考慮結構特徵及動力限制下，進行即時軌跡規劃及最佳運動模
式；本計劃也將發展多攝影機連續於不同位置及角度提供的物件及工作空間資訊下，可對多機器手臂
系統進行遠端半自動遙控之系統合成方法，及使用一般立體攝影機和微處理器條件下，能進行自動辨
識出各工作物件之空間座標及週遭條件，並提供全自主式機器人和多手臂系統運動及軌跡規劃所需參
數之有效自動辨識系統；本計劃也探討同時可對雙機器手臂進行力量和位置控制並執行協同複雜技術
操作之命令系統。 
有效整合上述多項創新技術後，可創造可以在許多不同控制環境下全自主式地使用雙機器手臂執
行複雜工作的智慧型機器人。此種新型服務型機器人，具備高度發展潛力，可以進入工廠和家庭，取
代現在只能操作擺放於固定位置的機械手臂，具有相當大的產業價值。 
 
4. 研究目的 
 
本計劃預計研發的機器人必須能在複雜的工作環境中移動定位，並且使用立體視覺和機器手臂，
精確且快速的使用適當力量條件下進行物件操作。這個移動式機器人將有立體視覺能力，讓機器人在
操作過程的視角下可以辨識 3D 目標物件。這個機器人也將有兩個多軸手臂控制器讓這個機器人可以透
過兩支機器手臂從事許多工作。立體視覺公式將會在雙眼擷取影像後提供物件座標作為初始操控規劃
使用，同時連續的視覺伺服機制及技術將會繼續引導操控機器手臂對 3D 物件做精準的定位和操作。以
力回饋為基礎的線上視覺引導操控設計和即時動態控制技術將會在這個研究中被開發來協助機器人完
成自動化工作。在計畫的第三年，也將發展出機器人的遠端控制系統，可讓自動化機器人得以在惡劣
的環境中、或針對事先未定義物件，提供緊急工作指示和操作能力，或是臨時執行一些事先未經訓練
的緊急任務。 
 
5. 文獻探討 
 
目前具雙手臂的輪型移動的研究相當多， ARMAR-III 的底盤有 3 個全向輪採 120 度擺放，其手臂
具有 7 個自由度，5 指手掌則是具有 8 個自由度，在硬體設計上較特別為單一眼睛具有 2 個攝影機，
左右共 4 個，此機器人具有 3D 物件辨識與定位能力，其建立 3D 物件的方式與總計劃所提之概念類似，
透過立體視覺的資訊來判斷實體物件的形狀與位置[1]；2007 年 Grunwald、Schreiber、Albu-Schaffer 與
Hirzinger 學者研製出 DLR 雙機械手臂系統，其設計目的在於減輕機械手臂重量，達到整體機構輕量化，
 4 
 
6. 研究方法 
I. Progress of Hardware Design 
Due to the limited financial support of this project, the content of the robot hardware design is not 
completely new, but a restructured version of an existing mobile robot. We modified the mechanism to get rid 
of the existing drawbacks, to ensure the new mobile robot can fit for these requirements of the current project. 
錯誤! 找不到參照來源。 is the overview of the mobile robot. There are a total of four driving wheels, two 
on the each side of the robot. This enables this robot to move forward, backward and turning around. 
Furthermore there are two degrees of freedom at the bottom of the chassis. These components can be used to 
maintain balance when walking on an uneven terrain. This robot has two arms and hands. Left and right arms 
are the original designs from an existing mobile robot. Both left and right hands are the new version of the 
design. A stereo vision system is installed on the top of the robot. The head can rotate up and down, also left 
and right. In addition, a computer and motor drivers are installed inside the Robot’s chassis. All of the motor 
drives are connected by series communications with RS-232. 
 
Fig. 1 Overview of the armed mobile robot 
 
The detail specification is shown in Table 1. The height of robot is 140 ~ 150 cm. There are totally 34 
degrees of freedom. DC servo motors are used to actuate the robot, while harmonic drivers and pulleys are 
used to reduce the speed. The camera used as the eye is a Logitech webcam 5000. Maximum loading is 650 
grams by the hand. There are seven degrees of freedom in each arm. 錯誤! 找不到參照來源。 shows the 
rotating range of each degree of freedom. 
Because the gripping ability of the older hand is very limited, it needs to re-design a new one. The degree 
of freedom of the new robotic hand takes reference to the real human hand. There are 5 degrees of freedom on 
the new hand as shown in 錯誤! 找不到參照來源。. It’s a simplified model from the humans. DOF-1~2 for 
the finger’s joint, these joints can change the direction and angle of fingers. DOF-4~5 are for opening and 
closing three fingers. 
 
 6 
 
 
After the three finger hand design, we designed a new type of five fingers hand. Because of the objects 
for the robot to grasp are all things in human’s daily, we want the robot hand to best resemble the real human’s 
hand. Furthermore, we wish that the robot hand size is closer to the real human so that the robot hand can 
grasp the target items with higher confidence. The simulation results of grasping using the three finger hand 
can be seen in Fig. 3. 
The new design of the five fingers hand has 8 degrees of freedom and 18 joints in total as shown in 錯誤! 
找不到參照來源。 and 錯誤! 找不到參照來源。. The whole mechanism can be divided into three kinds of 
mechanisms: fingers bent/straighten mechanism, thumb twisting/bending mechanism and fingers 
outreach/contract mechanism. 
 
Table 3 Degrees of freedom and the joint definitions 
DOF Joint DOF Joint 
1 J1,1 5 J2,2+J2,3+J2,4 
2 J1,2 6 J3,2+J3,3+J3,4 
3 J1,3 7 J4,2+J4,3+J4,4 
4 J2,1+J4,1+J5,1 8 J5,2+J5,3+J5,4 
 
 
Fig. 4 Free body diagram 
 
Fingers bent and straighten mechanism 
The index finger, middle finger and ring finger share the same mechanism design. 錯誤! 找不到參照來
源。 shows the mechanism design of the index finger. Motor are embedded in a first knuckle and the helical 
gears are used to turn 90 degrees from the output axis. A coordinated motion of the two sets of four-bar 
 8 
 
 
Fig. 7 Little finger mechanism design 
Thumb twisting and bending mechanism 
Accounting for the many important activities of the hands, we set each of three joints as an independent 
degree of freedom, and it needs a larger torque than the other fingers. 
And also we use worm gears reducer mechanism to increase the reduction ratio. Since this mechanism 
has a self-locking function, the worm gears will not reversely rotate when the thumb bents resistant objects. 
The motor embedded in the last knuckle uses a helical gear to drive as shown in 錯誤! 找不到參照來源。. 
 
Fig. 8 Thumb bending mechanism design 
 
The twisting joint design is co-axial to the motor 1, and is fitted with a spur gear. Motor 1 and the 
bending mechanism rotate on the axis, and avoid interference linkage as shown in 錯誤! 找不到參照來源。. 
 10 
 
 
Fig. 11 The whole robot hand mechanism design 
 
 
Fig. 12 Real robot hand mechanism assembly 
 12 
 
 
Table 5 Robot hand mechanism crawl and hold the test 
   
Catch the ball Grasping disc Grasping notebook 
   
Grasping a box Grip PET bottle Grip the cup 
   
Holding the phone Grip plier Clip business card  
 
The experiments shows that the mechanical hand can grasp a tennis ball (sphere), a disc (disk), a 
rectangular box (box), a plastic bottle, a cup (cylinder), a telephone, and a pair of pliers. Also shown in 錯誤! 
找不到參照來源。, the robot hand can also pinch the business card between the fingers and thumb clip. 
 
Because we have designed the new robot hand, we make a comparison between the 3 fingers and the 5 
fingers hand designs as shown in 錯誤! 找不到參照來源。. 
 
 14 
 
II. Robot vision for object recognition 
1. Simple object 
For the simple appearance object we defined, by using the color and edge information to detect object 
localization and orientation. There are three major steps for the recognition. First step is the pre-processing of 
the input images. In this step, we extract the target color region by using the HSV color space as shown in Fig. 
15. 
 
 
Fig. 15 Input image pre-processing 
 
The second step is matching the edge map with the object’s database. It is shown in Fig. 16. In the step two, 
we get the correct orientation of the object. For the robot to operate this object, it needs to know the 
localization in the 3D space. In the third step, by using the stereo vision, calculate the object center in right 
and left camera.  
 
 
Fig. 16 Match with the database 
 
2. Complex object 
For the complex object recognition, we use the SIFT (Scale Invariant Feature Transform)[22] to extract 
features from the object’s appearance. SIFT is calculate based on the following four steps. 
 
a. Find scale-space extrema 
b. Keypoint localization & filtering. (improve keypoints and throw out bad ones) 
c. Orientation assignment (remove effects of rotation and scale) 
d. Create descriptor (using histograms of orientations) 
e. Image with features is shown in the Fig. 15 
For recognize the specific object, we can build up the database by different viewing angles 2D images. In 
 16 
 
 
III. Tele-control 
Tele-control description 
Tele-control operation allows the operator to control a mechanical or robot from the safe distance through 
the wireless communication [17]. In the project, the tele-operation is designed to control a service robot with 2 
arms. The robot will be attached 2 webcam to transmit the image to the operator and also received the 
command from the operator by the wireless. The operator will command directly for the robot by pointing on 
the screen or type the command for the robot. The system is shown in Fig. 19. 
 
 
Fig. 19 The model used in the project 
 
In order to create the tele-operation function, the following issues must be addressed 
The robot must be programmed to receive the command from the operator through the wireless and the 
robot will take the role as the client. The operator will take the role as the server and transmit the command 
for the robot through wireless by pointing on the screen displayed the image transmit from the client. The 
tele-operation is used in wireless LAN network. The speed through the wireless must be fast enough and 
stable. 
The interface of the server and client are firstly designed based on the targets: 
● Both of the server and client must be firstly confirm as logged in before operate the task, by provide the 
username of each other. 
● The server and client communicate with each other by the receive the data region. 
● The server order the command for the client by typing on the command region. 
In the client the interface is almost the same except the command region. 
 
 
Some basic operations have been completed in the server and the client 
The function in server: 
● Receive the image and current coordinate from the client. 
● Display the image received from client. 
● Order command for the client by pointing on the displayed image receive from the client or type in the 
command region. 
 
The function in Client: 
● Receive the image from 2 webcams. 
● Transmit the images and current coordinate (x,y) of the detecting object to the server. 
 
The operation of the server and the client has been tested as shown in Fig. 20. 
 18 
 
system comprises 2 cameras. It is setup at the client side. When a person sitting at the server wants to grasp an 
object, they will point at that object on the screen. By doing the image processing to recognize and localize 
the object in the Robot vision for object recognition part, the stereo vision will calculate and give the relative 
coordinate of the object, which is pointed, with the current robot’s position. The red ball is the object pointed 
on the screen and it is marked by the red ellipse. After the object is marked, the relative position (X, Y, and Z) 
of the ball to the robot’s position is calculated as the previous stereo algorithm. 
 
 
Fig. 22 Position of the ball in the camera image 
 
After getting the position of the ball by the stereo vision, Robot will control 2 motors at the platform to 
approach the ball by reducing the vector error between the position of robot and the position of the ball:  
 With , , , , ,b b b r r rX Y Z X Y Z
r r r r r r
 are the coordinate of the ball and 
robot. The approaching task will finish when robot enter the working zone of the hand described in the 
hardware part. 
II) Inverse Kinematic for the 7-DOF arm 
For the grasping task the inverse kinematic for the 7 -DOF manipulator is derived. By assigning the 
coordinate as shown in Fig. 23, where the DH table is built, the angle of each joint can be calculated for the 
grasping task when the position of the object is determined by the stereo vision. 
 
 
IV. Object operation parameters definition and parameter-based robot arm autonomous 
operations 
 
Integration of a specific task in one program function is called parameter based function. Parameter 
based for industrial robot is the program that receives the output data of the image processing part and the data 
input from an interactive user and commands the task for industrial robot as shown in Fig. 23. By using the 
parameter based programming, the robot can reproduce or duplicate the same task with another complicated 
object without re-programming. The data of the image processing are the location (O), the grasping point (A) 
of the objects, the location of the target task (B) or the position of the circle obstacle (C). The data input from 
an interactive user are illustrated by the direction of the engagement or other commands. 
 20 
 
 
The center coordinate in the image space of the rectangle cubic is calculated based on the color filter and the 
volume filter. The maximum of the fx coordinate and the fy coordinate in the frame coordinate system is 640 
pixels and 480 pixels, respectively. The intersection of the 2 dotted lines is the center of the camera. The 
coordinate A
 (xf1, yf1), B (xf2, yf2) of the object as shown in Fig. 25, in the image space coordinate are calculated 
by using edge detection algorithm and then using the linear equation the angle theta of the object is derived.  
As describe in [27] and [28], the aim of the image based visual control is to minimize the error by using the 
interaction matrix sL  which is defined as the following: 
 
2
2
1 0 (1 )
10 1
f
f f f f
s
f
f f f f
x
x y x y
Z ZL
y
y x y x
Z Z
 
−
− + 
 =
 −
+ − − 
 
 (1) 
where fx  and fy  are the 2D point coordinates that are projected by a 3D point (X, Y, Z) into the camera 
frame. In this matrix, Z is the depth of the 3D point. To use this control loop, the depth Z must be estimated at 
each sampling time. Thus, sL  in Eq. 8 cannot be directly used, and an estimation or approximation method 
must be used.  
Moreover, from Fig. 25 it can be realized that, the stereo vision cannot always assure the view from the 3rd 
camera can always watch the full object or have at least three points for the purpose of controllability as 
mentioned in [28]. When the system is setup for a real application, some environmental parameters that can 
affect the system performance need a further consideration. These include the error of the setup device, the 
error associated with image processing when environments change and especially the error associated with the 
calibration of the camera and the system. As a result, it is necessary to retune or redesign the controller 
parameters and to recalibrate the system to effectively cope with the changed environment.  
To alleviate the burden of the controller tuning, reduce the time spent in recalibrating the system, and 
improve the learning process in the presence of uncertain parameters, this study applied artificial intelligence 
methods to reduce tracking errors.  
Fig. 25 Orientation of a rectangle cubic viewed in 3rd camera. 
 22 
 
Table 7 Rule table of the fuzzy controller 
 
 
In the experiment, the algorithm that was applied to command the robot to approach and grasp the object is 
described in Fig. 27. This method uses a combination of stereo vision and image-based visual servoing with a 
fuzzy controller. The experimental data demonstrate that the accuracy of the system can be effectively 
improved using the IBVSFC. The error of the system is small, approximately 1 mm. Using the eye-in-hand 
structure and IBVSFC will help alleviate the burdens of the high computational cost associated to the system 
calibration while providing a high success rate and high accuracy when working under weak calibrations. 
Moreover, the structure of the system is not complicated; it is easy to set up in complex environments, and it 
has a lower cost compared with other systems on the market. 
 
 
 
Fig. 27 Multitask working flow chart 
 24 
 
8. 參考文獻 
[1] T. Asfour, K. Regenstein, P. Azad, J. Schr¨oder, A. Bierbaum, N. Vahrenkamp and R. Dillmann, 
“ARMAR-III: An Integrated Humanoid Platform for Sensory-Motor Control,” IEEE-RAS International 
Conference on Humanoid Robots, pp. 169-175, 2006 
[2] Albu-Schaffer A., Haddadin S., Ott Ch., Stemmer A., Wimbock T. and Hirzinger G., “The DLR 
lightweight robot：design and control concept for robots in human environments,” Industrial Robot：An 
International Journal, pp. 376-385, 2007. 
[3] H. Iwata and S. Sugano, “Design of human symbiotic robot TWENDY-ONE,” IEEE International 
Conference on Robotics and Automation, pp. 580-586, 2009. 
[4] RIBA機器人網站，http://rtc.nagoya.riken.jp/RIBA/ 
[5] T. Mukai, M. Onishi, T. Odashima, S. Hirano and Luo Zhiwei, “Development of the Tactile Sensor 
System of a Human-Interactive Robot “RI-MAN”,” IEEE Transactions on Robotics, vol. 24, pp. 505-512, 
2008. 
[6] Wakamura 機器人網站，http://www.mhi.co.jp/kobe/wakamaru/english/ 
[7] 台灣大學羅仁權教授網站，http://www.ee.ntu.edu.tw/profile?id=758 
[8] TOYOTA Partner 機器人網站，http://www.toyota.co.jp/en/special/robot/index.html 
[9] EMIEW 2 機器人網站，http://www.hitachi.co.jp/rd/research/robotics/emiew2_01.html 
[10] I. Kawabuchi, “A Designing of Humanoid Robot Hands in Endo skeleton and Exoskeleton Styles,” 
Humanoid Robots, New Developments, Book edited by Armando Carlos de Pina Filho, pp. 401-426, 
2007. 
[11] T. Mouri, H. Kawasaki, K. Yoshikawa, J. Takai, and S. Ito, “Anthropomorphic Robot Hand: Gifu Hand 
III,” Proc. of Int. Conf. ICCAS2002, pp.1288-1293, 2002. 
[12] Shadow 機器人公司網站，http://www.shadowrobot.com/ 
[13] 台灣大學 黃漢邦教授 機器人實驗室網站，http://robot0.me.ntu.edu.tw/tc/lab_research_BE.htm 
[14] Azad, P., Asfour, T., and Dillmann, R., “Stereo-based 6D object localization for grasping with humanoid 
robot systems”, Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference 
on Oct. 29 2007-Nov. 2 2007 Page(s):919 – 924 
[15] Grundmann, T., Eidenberger, R., Zoellner, R.D., Zhixing Xue, Ruehl, S., Zoellner, J.M., Dillmann, R., 
Kuehnle, J., Verl, A., “Integration of 6D Object Localization and Obstacle Detection for Collision Free 
Robotic Manipulation”. 2008 IEEE/SICE International Symposium on System Integration: SI 
International 2008 - The 1st Symposium on Systems Integration, art. no. 4770428, pp. 66-71. 
[16] Zhixing X., Kasper, A., Zoellner, J.M., and Dillmann, R., “An automatic grasp planning system for 
service robots”, Advanced Robotics, 2009. ICAR 2009. International Conference on 22-26 June 2009, 
1 – 6 
[17] http://codeproject.com 
[18] Nakajima, S., “Concept of a Novel Four-wheel-type Mobile Robot for Rough Terrain, RT-Mover,” 
Intelligent Robots and Systems, pp. 23257–3264. 
[19] Nakajima, S., “Development of Four-wheel-type Mobile Robot for Rough Terrain and Verification of Its 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                              日期：  101 年  10 月 29  日 
 
計畫編號 NSC98－2221－E－011－083－MY3 
計畫名稱 具全自主操作 3D 物件能力之雙臂服務型機器人 
出國人員
姓名 林其禹 
服務機
構及職
稱 
台灣科技大學機械系教授 
會議時間 
100 年 9 月 13 日
至 
100 年 9 月 15 日 
會議地
點 
Livingstone, Zambia 
會議名稱 
(中文) 第十屆 IEEE 第八區 Africon 國際研討會 
(英文)  The 10th IEEE Region 8 Africon 2011 Conference 
 
 
發表論文
題目 
(中文)以 DSP 為基具立體視覺的的捕手機器人系統 
(英文) The DSP Based Catcher Robot System with Stereo Vision 
  
 
在研討會上午發表了一篇論文，題目是”The DSP Based Catcher Robot System with Stereo 
Vision”，內容探討使用簡單的設備，包含DSP和一套立體視覺系統，讓一個雙自由度機
器手臂系統得以接住從4m外投來的球。 
 
 
二、與會心得 
本研討會為難得在非洲主辦，以控制為主軸的國際研討會，聚集包含機械、電機、電
子、土木等各領域人員。本人很難得可以在以非洲學者為主的研討會內跟許多人溝
通和交換意見，也爭取到一些參訪新國家的機會，也順便宣傳招收優秀非洲學生到
台灣就讀，功效還算不錯。 
 
三、建議 
非洲的大學數目很少，許多優秀學生缺乏進入研究所就讀機會。台灣正在積極招攬外
籍學生到台灣就讀研究所，因此非洲正是一個非常適當的區域。值得台灣各大學前
去開發。 
 
 四、攜回資料名稱及內容 
大會論文集 
 
performance, or qualifications as determined by IEEE policy and/or applicable laws. For more information on 
the IEEE policy visit http://www.ieee.org/nondiscrimination 
 
Having trouble with the link? Simply copy and paste the entire address listed below 
into your web browser:  
http://www.cvent.com/d/9Q8uz_PH7keGc1E6me4n1A/bn65/P1/5S?  
IEEE may contact you regarding similar IEEE conferences, services, and/or technical 
products. If you prefer not to be contacted please click the link below Opt-Out  
 
 
Your payment for the 2011 IEEE AFRICON event has been successfully processed. Please save this email 
for your records. 
 
Event Title: 2011 IEEE AFRICON 
Registration Confirmation Number: KWNMND7V96P 
 
Transaction 
Information: 
Item  Transaction Information Quantity Amount 
R01 – IEEE Member $500.00 1 $500.00 
Transaction Total $500.00 
 
If you have any questions about this transaction or email, please contact 2011 IEEE AFRICON directly at 
africon11reg@ieee.org. 
 
 
To view the details of your registration, go to: 
Click here 
 
 
 
to combine all the features in above researches to apply 
vision technology to lead the rapid movement of arms. We 
want to design a catcher robot that has stereo vision and 
movement that resemble a human's. In the future, the system 
can be widely applied to any humanoid robots to control 
robotic arms that move freely. We will achieve our final goal 
to allow a humanoid robot to catch a ball. 
 
II. ROBOT SYSTEM 
    The structure of the robot in this paper is composed of 
a two-dimension robotic arm and a vision processing 
equipment as shown in Fig. 1. the vision equipment is made 
up with two sets of CCD, a DSP, and a monitor. The 
purpose of the equipment is to decide the falling place of a 
moving ball and moves the arms to catch the ball. The 
following paragraphs are the introduction concerning the 
structure of the robot, CCDs, and processing systems. 
 
 
Fig. 1 Details of Equipment 
 
2.1 Robot Structure 
The structure of the catcher robot consists of a 
two-section arm and two motors. The Fig. 2 illustrates how 
the arms are moved. The working area of the arms is a 
circle with diameter of 60 cm. the distance from the 
forearm to the upper arm is 9.4 cm. The distance from the 
surface of the catching area to the lens of the camera is 26.3 
cm. Please see Fig. 3 for the specification. The two-section 
arm is made of aluminum. The location of Motor 1 and 
Motor 2 are shown in Fig. 2, their specifications are shown 
in Table 1 below. The two-section arm of the catcher robot 
is made up of real simple structure. 
 
Table 1 SPECIFICATION OF MOTOR 
 Motor 1 Motor 2 
Max Speed (RPM) 421.4 400 
Max Torque (kg-cm) 77 46.7 
Weight(g) 432 177 
 
 
Fig.2 Mechanism of the Robot Arm 
 
   
Fig.3 Robot Measurements 
 
2.2 Robot Cameras 
The CCD we use is so-called the analogue camera with 
NTSC (National Television System Committee) format and 
is about 0.38 mega pixels (768 x 494). The NTSC is the 
standards for color signals, with 30 frames per second, 
which is normal on the market. The light sensor we use is 
1/3 inch. The camera format size means the size of 
effective square of the sensor. The actual pixel distance of 
the sensor can be calculated using the camera resolution 
and the size of the sensor. The horizontal pixel distance of 
the actual sensing circuit is about 0.00625 mm pixels. The 
vertical is 0.007287 mm pixels. These two data will be used 
in calculating the 3-D coordinates. On account of the 
differences in the optical theories and characteristics of 
various lenses, we take into consideration of focal length, 
angle view, and aperture. The focal length is related to the 
angle view. The shorter the focal length, the bigger the 
angle view that allows you to see wider and the far away 
objects look smaller. On the contrary, if the focal length is 
longer, then the angle view is shorter and can see narrower 
with bigger objects in the distance. Because the sensing 
distance is from 30 cm to 4 meters, so the distance is quite 
long and we have to make sure the objects are clear enough 
to be recognized. After some experiments, we decided to 
set the focal length at 12 mm, about 22 degrees for 
horizontal angle view, and 17 degrees for vertical angle 
view. There are many ways to place the camera, but they 
are divided in optical axes. The most common method is to 
place it in parallel optical axes with a distance of 6 between 
to simulate the human vision. 
 
2.2.1 The Calibration of the Cameras 
We referred to image square calibration [13], 
eight-point algorithm [14], automatic calibration [15], and 
ways that don't need calibration[16] to align the image 
center with the paper ruler at the same height of the camera 
to make sure the cameras are placed at the same height. We 
also ensure that they parallel with the distance of 6 cm 
between. The placement is shown in Fig. 4. The (a) and (b) 
in the picture represent the calibrated screen shot of 
cameras on the left and the right. They are a very simple 
image calibration. The differences of actual measurement 
after the calibration are acceptable. 
 
   
（a）                     （b） 
Fig.4 CCD Calibration：（a）Left-CCD（b）Right-CCD 
 
Fig.8 Object Detection Processing 
 
3.2.1 Background Image Subtraction 
In multiple moving targets detection processing, it will 
execute background image subtraction first to set the 
captured image as the background image Fb (x, y). Then 
we minus the input image Ft (x, y) by time. The result will 
be set as D (x, y). We have set a proper standard value as 
Td. If the value after the subtraction is higher than Td, then 
it will be deemed as the pixel in the foreground; if it's lower, 
then it's in the background. The judgment formula is shown 
in (3-1). 
 
⎩⎨
⎧
<−⇐
≥−⇐=
dtt
dtt
Tyxfyxfif
Tyxfyxfif
yxD
),(),(,0
),(),(,1
),(     (3-1) 
 
3.2.2 Low-pass Filters 
The low-pass filters is the method to smooth an image. 
Because we do the image subtraction with brightness (Y), 
so there will be many noise and fragment caused when the 
weather has too many differences in bright or too 
complicated a background. To deal with the noise problem, 
we use an even mask of 3 x 3 squares which is shown in 
Fig. 9. the mask will calculate all the reaction values from 
left upper corner to right lower corner to make a the whole 
image smoother. 
 
 
Fig.9 Low-pass Filters：3×3 Mask 
 
3.2.3 Image Characteristics 
The image size, image border, and core coordinates of 
the image of the moving object will be calculated after it is 
filtered in its dynamic search range. Judging the size of an 
image is to differentiate the target and other smaller noises. 
The border of the image will influence the range of 
dynamic search. The image processing action is done 
according to the range of dynamic search. The smaller the 
range, the less the data to be processed by CPU. Therefore, 
dynamic search will lessen unnecessary calculations. Then 
we calculate the differences in image border with image 
border value and core coordinates to decide whether the 
there are objects that are mistaken as targets. If there were 
such objects, then it would shrink the dynamic search range 
to conduct another image differentiation. The core 
coordinates will also be used in the 3-D corresponding 
method. The final processed image will be treated as the 
target image to complete the target detection procedure. 
 
3.3 Determining Location by Stereo Vision Processing 
    We will explain the characteristics of the Stereo 
Vision system[20,21] with Fig. 10. The P' and Q will be 
projected on the qR in the right image while P and Q' also 
on the pR. The P and P' will be projected on the qL in the 
left image while Q and Q' also on the qL. The 3-D vision 
can find P through the pL in the left corresponding to the 
pR in the right. It can also locate Q through the qL in the 
left corresponding to the qR in the right. As a result, the 
robot can know which point in the image corresponds to the 
location in the space and further calculates the depth for 
space coordinates. 
 
 
Fig.10 Stereo Vision for Matching 
 
   In the algorithm of 3-D coordinates calculation for 
visual system, it is a priority to find a proper corresponding 
method to locate the corresponding point in the stereo 
image pair. Once found, it will calculate the 3-D 
coordinates based on the 3-D coordinate algorithm. 
 
3.3.1 Stereo Matching 
The research on stereo reaction is use asymmetrical 
theory [22] to locate the corresponding point in the 3-D 
image for dept information. If the point is well-located, 
then the depth will also be more accurate. We hope that we 
don't have to adopt complicated algorithm to locate the 
proper corresponding point. Because the target is a red ball, 
it is difficult to spot a special image feature point on the 
target to be the corresponding point. As a result, when 
capturing image area of the target, we wish to differentiate 
image areas that don't belong to the target for reduction of 
differences in value. 
 
3.3.2 Space Location (3-D Coordinate Calculation) 
Before calculating the 3-D coordinates, it is necessary 
to define the coordinates in Fig. 11. The center point 
between the two cameras is set as the origin of the world 
coordinate to define all the locations. Then we will have to 
use the values of the cameras. First, we will use the 
triangulation to get the depth, which is shown in Fig. 12. 
3.4 Predict the Catch Position 
After the calculation of the target's 3-D coordinates, we 
apply the information to Lagrange interpolation formula and 
linear equation X=aZ+b to predict the falling location. When 
the ball is thrown four meters before the sensing field, it will 
detect five spatial coordinates if only taking gravity into 
account. The details are shown in Fig. 15 and Fig 16, which 
are respectively the horizontal and vertical profile of the 
ball's movement. The X-Z surface is a straight movement so 
it only credits two points to calculate its curve. 
 
 
Fig.15 Flying Ball Trajectory on Y-Z Plan 
 
 Fig.16 Flying Ball Trajectory on X-Z Plan 
 
3.4.1 Lagrange interpolation formula 
   We assume that there are n points (Y1, Z1), (Y2, Z2), … 
(Yn, Zn) in the Y-Z surface shown in Fig. 15, then we can 
use Langrange interpolation formula (3-10) to simulate the 
n-1 formula of Y(Z). Then we can apply the coordinates of 
the five points detected into it to get the curve formula of 
Y-Z surface and the falling location on Y axis. 
 
)YY)...(YY)(YY(
)YY)...(YY)(YY(Z....
)YY)...(YY)(YY(
)YY)...(YY)(YY(Z
)YY)...(YY)(YY(
)YY)...(YY)(YY(Z)Z(Y
1nn2n1n
1n21
n
n23212
n31
2
n13121
n32
1
−
−
−−−
−−−×+
+−−−
−−−×+
−−−
−−−×=
   (3-10) 
 
3.4.2 Linear Function X=aZ+b 
   We assume that there are two points (X1, Z1) and (X4, 
Z4) shown in Fig. 16, we can use a simultaneous linear 
equations in 2 unknowns (3-11) to get the linear equation 
X(Z) in X-Z surface and the falling location in the X axis. 
 
   
⎩⎨
⎧
+=
+=⇒+=
b ZaX
b ZaX
        baZX(Z)
44
11        (3-11) 
 
3.4.3 Catching Surface of the Robot 
    The falling information will be sent to the motors of 
the arms through RS232. The ball catching range (a circular 
surface of 60 cm diameter) is divided into 100 squares (No. 
0-99) as shown in Fig. 17. Each square is assigned a 
location value for the arms, which later allows the arms to 
spin to the certain square. The ball and the catching surface 
are covered with Velcro to enable cohesion. The 0, 9, 90, 
and 90 squares on the rim are squares that the arm can't 
reach at. 
 
 
Fig.17 The ball catching range of the arm and the division of the surface 
 
IV. CATCHING EXPERIMENTS OF THE CATCHER ROBOT 
   In the experiment, we hand throw balls at a fixed 
distance of four meters. The laboratory has stable light 
source and no air flows. We have thrown 500 times of balls 
not faster than 16m/s and all of them are within the 
catching range. The catching surface is divided in I, II, III, 
IV Quadrants. The center area is the origin of the ball 
catching area (Fig. 18) to research the hitting rate and 
falling analysis of the balls. 
 
  
Fig.18 The catching surface Quadrants division and the origin catching 
point 
 
4.1 Actual Statistics and Analysis 
   There are 500 times of throwing in total. The hitting 
rate of all Quadrants are over 60% (see Table 2), the Fig. 
19 shows the caught status of each Quadrant. There are 
more hitting times in III and IV Quadrants, but it is because 
the throwing tendency of each individual. In general, the 
successful rate is almost 70%. The major causes for "Miss" 
are that the arm is too slow to move to the location or the 
balls have surpassed the sensing range to result in 
misjudgment. In the future, we hope to come out with an 
effective algorithm based on accumulated ball throwing 
times to keep improving the origin location of the arm. We 
also want to replace the current static camera vision system 
with dynamic visual tacking system in the future to elevate 
the successful rate. The result of the research can be viewed 
as YouTube [23]. 
 
Table 2 The hitting rate the falling average of each Quadrants 
 Ⅰ Ⅱ Ⅲ Ⅳ All 
Success(number) 62 62 95 105 324
Miss(number) 31 29 57 59 176
Total(number) 93 91 152 164 500
Average 
(Success / Total) 0.67 0.68 0.63 0.64 0.65
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                              日期：  101 年  10 月 29  日 
 
計畫編號 NSC98－2221－E－011－083－MY3 
計畫名稱 具全自主操作 3D 物件能力之雙臂服務型機器人 
出國人員
姓名 林其禹 
服務機
構及職
稱 
台灣科技大學機械系教授 
會議時間 
101 年 5 月 7 日
至 
101 年 5 月 11 日 
會議地
點 
Karlsruhe, Germany 
會議名稱 
(中文) 第九屆競爭工程工具和方法國際研討會 
(英文) The Ninth International Symposium on Tools and Methods of 
Competitive Engineering 
 
發表論文
題目 
(中文)從控制機器人到網路和實際交會之智慧系統 
(英文)From controlled robots to intelligent cyber-physical systems 
• Enhancement of company output  
ADVANCED ENGINEERING APPROACHES 
• Design optimization methods  
• Product development methodologies  
• Robust decision making  
• Design and innovation theories  
• Specific topics of engineering  
• Advancements in architectural construction  
 
研討會每天除了 4-6 個平行 session 外，也安排七場 Plenary speech。本人被邀請擔任其
中一場安排在 5 月 10 日的 Plenary speech，發表的專題演講題目
為”From controlled robots to intelligent cyber-physical systems”，內容是將本人近六年來在
智慧型機器人和智慧型系統上的研發成果作一綜合報告。發表演講時，會場中有約 120
位聽眾，並有 6 位聽眾於本人演講結束後提出問題，本人並接著提出說明。除減低的註
冊費外，本研討會並未提供 Plenary speech 的講員任何經費補助。 
 
二、與會心得 
本研討會為一跨領域技術的國際研討會，聚集包含機械、電機、電子、資工、化工、
土木、和交通各領域人員，非常難得可以在同一研討會內交換意見，可以促進一些
跨領域交流機會，值得參加。 
 
三、建議 
本人受邀擔任本研討會之大會講員(plenary speaker)，為難得替台灣技術發光的機
會，但大會的規則是不補助額外經費，需要講員自籌經費。建議國科會建議類似補
助系統，當計畫主持人被邀請擔任大會講員而沒經費補助時，可以向國科會申請參
加研討會的經費。 
 
 四、攜回資料名稱及內容 
大會論文集 
 
• 2008年 11月 26 日
From Controlled Robots to Intelligent     
Cyber-physical Systems
Chyi-Yeu Lin, Ph.D.
Director, Center for Intelligent Robots Research
Professor and Chairman, Dept. of Mechanical 
Engineering
NTUST Advanced Intelligent Robot LAB
OUTLINE
3
Opening Robot Show (4’)
 l if i l i i b (4’)Mu t unct ona  Enterta n ng Ro ots 
 DOC-1/DOC-2/DOC-3
Robot Theater (11’)   
 Face Robots
 Humanoid Robots
 R b t Th to o  ea er
Robot Catcher(2’)
Balloon Shooting (3’)
Two-armed Service Robot (2’)
Parameter-based Object Operation (5’)
Object Tracking (3’)
On-windshield warning system (5’)
NTUST Advanced Intelligent Robot LAB
iDOC-1 Funct ons
Teach spelling Face recognition Serve tea   
St k h t b T h th Pl G bac  c arac er cu es eac  ma ay o ang
NTUST Advanced Intelligent Robot LAB
DOC-3 Functions
DOC-3 Functions
Gobang game Chinese Chess gameMath teaching
Spelling teach
Story reading
S  i i
PDA controllerGo game
ong s ng ng
Demo
Face recognition
NTUST Advanced Intelligent Robot LAB2010.03.12
Face Robot
 Function
y Numbered musical notation recognition
y Mandarin singing voice synthesis
y Mouth patterns matching voice content
y Autonomous news broadcasting
y Mimic human facial expressions
y Tracking of human faces
2007 2008 2009 20102006
NTUST Advanced Intelligent Robot LAB
Face Robot – 2nd & 3rd Generation
1. Real Human 
Molding
Mold 1
2007 2008 2009
2. Alginate Mold 
(Cavity)
3. Plaster Mold (Core) 
+
Mold 2
4. Silicon Rubber Mold 
(Cavity) Mold 3
Mold 4
5. Clay Mold (Core) 
6 FRP M ld (C it )
Mold 6
+.  o  av y  
7. FRP Mold (Core) 
Mold 5 Mold 7
8. Urethane 
Injection 
Mold 8
NTUST Advanced Intelligent Robot LAB
Facial Expressions 
Control Point (CP)
Relation between Expressions and CPs
2 D.O.F Eye Ball Unit  (for each eye) 1st Generation Facial CPs and Pulling Mechanism (16 CPs)
Relation between Expressions and CPs
0 D.O.F Eye Balls Unit 
(Because of Sony PTZ big solution) Relation between Expressions and CPs
2nd Generation Facial CPs and Pulling Mechanism (15 CPs)
A pair of 2 D.O.F Eye Ball Units 3rd Generation Facial CPs and Pulling Mechanism (14 CPs)
NTUST Advanced Intelligent Robot LAB2010.03.12
id R b  d h l d R bHumano o ots an Two-w ee e o ots
NTUST Advanced Intelligent Robot LAB
 Specification of NTUST Theatric Robot Two-wheeled Robot
17
      –  
無法顯示圖像。您的電腦可能沒有足夠的記憶體來開啟圖像，或圖像可能已毀損。請重新啟動您的電腦，並再次開啟檔案。如果仍然出現紅色 x，您可能必須刪除圖像，然後再次插入圖像。無法顯示圖像。您的電腦可能沒有足夠的記憶體來開啟圖像，或圖像可能已毀損。請重新啟動您的電腦，並再次開啟檔案。如果仍然出現紅色 x，您可能必須刪除圖像，然後再次插
入圖像。
Camera
Laptop
無法顯示圖像。您的電腦可能沒有足夠的記憶體來開啟圖像，或圖像可能已毀損。請重新啟動您的電腦，並再次開啟檔案。如果仍然出現紅色 x，您可能必須刪除
圖像，然後再次插入圖像。
無法顯示圖像。您的電腦可能沒有足夠的記憶體來開啟圖像，或圖像可能已毀損。請重新啟動您的電腦，並再次開
啟檔案。如果仍然出現紅色 x，您可能必須刪除圖像，然後再次插入圖像。
無法顯示圖像。您的電腦可能沒有足夠的記憶體來開啟圖像，或圖像可能已毀損。請重新啟動您的電腦，並再次開啟
檔案。如果仍然出現紅色 x，您可能必須刪除圖像，然後再次插入圖像。
7 D O F H d
Gyro
Sensor   
Tilt 
Sensor
Height 124 cm
. . an
Weight 50 kg
DOF 20(Arms) + 2(Wheels)
Vision 1 CCD cameras
Sensor Gyro
Actuator DC motor
Battery Ni-MH
CPU Pentium 4  M 2.0 GHz*2
i d ( i )OS W n ows XP Ma n
Linux (Body)
NTUST Advanced Intelligent Robot LAB
Team
NTUST Advanced Intelligent Robot LAB 21
 Robot Theater From Taiwan® - Notation Reading and Singing Show
VIDEO
Image input B doun ary 
identification
Image de-warping and 
background removalImage preprocessing
Input Image
Mark  locating Musical notation 
recognition
A Flow Chart of Image Capture
Accuracy of Manipulation
Dynamic BalanceVisual Servoing
A Flow Chart of Musical Notation Recognition 
Notation *.txtA waveform file of 409 
Mandarin Syllables 
HNM (Harmonic plus Noise 
Model) 
HumanizationImage Recognition
Technical Difficulty
HNM 
Parameter Analysis
HNM 
Parameter file HNM Synthesis
SingingA Flow Chart of Voice Synthesis 
NTUST Advanced Intelligent Robot LAB 23
 Robot Theater From Taiwan® - “The Phantom of the Opera”
VIDEO
Accuracy of Manipulation
Virtual SimulationIn-house Rehearsal
Numerical Simulations Runtime Training
Dynamic BalanceVisual Servoing
Gait Pattern Gait Training
HumanizationImage Recognition
Technical Difficulty
Generator System
PC Based Controller
Gait Pattern System
NTUST Advanced Intelligent Robot LAB 25
Integrated
 Robot Theater From Taiwan® - Musical Dancing Show
VIDEO
 
Movie
Accuracy of Manipulation
Virtual Choreographing
Dynamic BalanceVisual Servoing
HumanizationImage Recognition
Technical Difficulty Multi-Robot Integration SystemOff-Stage Manipulation Staff
NTUST Advanced Intelligent Robot LAB
Balloon Shooting System
(visual tracking application)
• Design an auto shooting system of robot arm with a laser 
device that can shoot balloon on the projection screen of 
i i    nteract on game system.
NTUST Advanced Intelligent Robot LAB
Research
• Two systems in the research:
1. Auto shooting system of industrial robot arm
2 Interaction game system.
NTUST Advanced Intelligent Robot LAB
• Balloon tracking and shooting experiment
NTUST Advanced Intelligent Robot LAB
Height 133cm
O d
Specification
D F Hea :
Arm:
Hand:
2
7 ×2
8
3
(Right hand)
(Left hand)
Chassis:
Total:
2
29
Actuators DC Servo Motor
Reduction Harmonic Drive Gear
Mechanism Timing-Belt/Pulley
Camera Webcam ×2 
Max. Load 650g  (Right Hand)
Left Arm Right Arm
θ1 -180° ~ 180° 180° ~ -40°
θ 17° 196° 180° 40°
Moveable range of two arms
Overview of mobile robot
2 - ~ ~ -
θ3 -180° ~ 180° 90° ~ -90°
θ4 -10° ~ 130° 120° ~ 0°
θ5 -180° ~ 180° 90° ~ -68°
θ6 -47° ~ 47° 35° ~ -35°
θ7 -115° ~ 47° 60° ~ -60°
NTUST Advanced Intelligent Robot LAB
Arrangement of 8 DOF Hand
NTUST Advanced Intelligent Robot LAB
Gesture Recognition
Experiement Demo 
Model Generation
A set of image
Model generation by osm-bundler 
3D point cloud model
Object Operation Parameters   
Definition and 
Parameter-Based Robot Arm 
Autonomous Operations
See attached WORD file
NTUST Advanced Intelligent Robot LAB
台灣科技大學智慧型機器人研究中心 Center for Intelligent Robots, NTUST
NTUST Advanced Intelligent Robot LAB
NTUST Advanced Intelligent Robot LAB
Water Pouring
Image processing Industrial robot (parameter based)
A( R R R )
 
xA,yA,zA, xA, yA, zA
B(xB,yB,zB,RxB,RyB,RzB)
Func=(A,B,HeightA,HeightB)
HeightA (A)
HeightB (B) 
B
A
Moving Object Tracking by Combining 
Efficient Second-Order Minimization and    
PD Neural Controller
NTUST Advanced Intelligent Robot LAB
台灣科技大學智慧型機器
人研究中心 Center for  
Intelligent Robots, 
NTUST
NTUST Advanced Intelligent Robot LAB
台灣科技大學智慧型機器
人研究中心 Center for  
Intelligent Robots, 
NTUST
NTUST Advanced Intelligent Robot LAB
On-Windshield Warning System
(A f  h t    f th  sa e approac o warn you o e
potential obstacle in front of the moving 
hi l )ve c e
Coordinate systems – setup and  
t f ti
59
rans orma on
61
Intersection point calculation
⎥⎥
⎤
⎢⎢
⎡
+=
+=
= tbytP
taxtP
tP y
x
00
00
)(
)(
)(
⎪⎧ = )(),( tPvuP xx
⎥⎦⎢⎣ += tcztPz 00)(
⎪⎩
⎨
=
=
)(),(
)(),(
tPvuP
tPvuP
zz
yy 10,10 ≤≤≤≤ vu
63
System simulation interface 
P t tti Si l tiarame er se ng mu a on 
System simulation
65
 
NTUST Advanced Intelligent Robot LAB
Control system
67
 
hardware Software
69
Warning determination - vertical  
PedestrianDriver Windshield
The height of
the driver(Hd)
The height of
the pedestrian(Hp)
The height of
the windshield(Hw)
The distance of the pedestrian
and the windshield(Dpw)
The distance of the 
driver and the 
windshield(Ddw)
)(
)(
dwpw
dwwdp
dpow DD
DHHH
HH +
−−+=
Hpow：目標於擋風玻璃
上的高度
NTUST Advanced Intelligent Robot LAB
E i t
71
xper men  -
setup
雷射裝置
攝影機
影像屏幕 擋風玻璃
NTUST Advanced Intelligent Robot LAB 73
雷射點
雷射
雷射裝置
攝影機
影像屏幕 擋風玻璃
NTUST Advanced Intelligent Robot LAB
Experiment – outcome
75
NTUST Advanced Intelligent Robot LAB 77
Thank You
98年度專題研究計畫研究成果彙整表 
計畫主持人：林其禹 計畫編號：98-2221-E-011-081-MY3 
計畫名稱：具全自主操作 3D 物件能力之雙臂服務型機器人 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 1 1 100%  專利 已獲得件數 1 1 100% 件  
件數 1 1 100% 件  
技術移轉 
權利金 300 300 100% 千元  
碩士生 1 1 100%  
博士生 3 3 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 3 3 100%  專利 已獲得件數 1 1 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
