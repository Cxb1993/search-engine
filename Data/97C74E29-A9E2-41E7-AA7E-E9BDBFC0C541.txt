 I 
 
 
摘要 
 
傳統的語音辨識主要是擷取發音腔道模型的參數，如梅爾頻率倒頻譜係數(MFCC)，建立
語音之聲學模型，在隱藏式馬可夫模型(HMM)的架構下進行語音辨識。基本觀念是用語音與
文本資料庫訓練其聲學模型與語言模型，無需具有語音學與語言學的知識，就可以建構一個語
音辨識系統，可以說是忽略知識的模式(knowledge-ignorant modeling)。這樣發展出來的系統，
在多樣環境下，辨識效果不理想。新世代語音辨識則是要引進語音學與語言學知識，構成一個
以知識為基礎(knowledge-based)加上資料驅動(data-driven)的模式，以進一步改進語音辨識系
統。在這個模式下，首先要做的就是進行語音屬性與事件之偵測，發展語音屬性與事件模型。
本研究即針對語音屬性與事件偵測，以及其模型方法進行研究，本研究探討使用小波轉換係
數、時頻域參數、二維倒頻譜(two-dimensional cepstrum)等的方法，以及條件隨機場(conditional 
random field)、隨機森林(random forest)等機率模型技術。除了語音屬性與事件偵測之外，也探
討音素(phone)與音素邊界(phone boundary)的辨識，所發展的方法係針對發音方法(articulation 
manner)類別，有可能作跨語言的使用。 
 
關鍵詞：語音辨識、聲學模型、二維倒頻譜、條件隨機場、隨機森林、音素邊界 
 
 III 
 
目錄 
 
摘要 
一、前言 
二、語音屬性與事件 
三、以條件隨機場作中文語音屬性的辨識 
 3-1 屬性偵測器 
 3-2 條件隨機場 
 3-3 實驗 
四、不知語言內容的自動音素分段 
 4-1 候選音素邊界之偵測 
 4-2 音素邊界點之確認 
 4-3 實驗 
五、爆發音事件偵測 
 5-1 隨機森林模型的建立 
 5-2 二維倒頻譜係數 
 5-3 實驗 
六、濁音起始時間之估測 
 6-1 濁音起始點偵測 
 6-2 實驗 
七、結果與討論 
八、計畫成果自評 
參考文獻 
 
 
 
 
 2 
 
(一) 語音屬
性與事件之
偵測 
 
(二) 語音事
件之整合與
證據確認 
(四) 語音辨
識之強化、
及後處理 
(五) 外加已
有知識 
(三) 語料庫、知識庫、模型、工具 
語音
訊號 
辨識
結果 
共享平台 
 
圖 1-2 新世代自動語音辨識系統—第二階段之系統規劃 
 
其差別在於更加強已有的語音學與語言學知識，並將語音事件整合與證據確認合併。這個階段
的研究包括(1) 語音屬性、韻律屬性、與語音事件偵測、(2) 自動標音及語音資料庫確認、(3) 語
音屬性與事件之辨識模型研究、(4) 語音事件整合、證據確認、與後處理、(5) 國語及方言之
音節階層事件偵測、及 (6) 跨環境之強健性語音屬性與事件偵測器。 
 
本子計畫是進行語音屬性與事件之偵測，以及語音屬性與事件之辨識模型研究。語音辨識
採用的特徵參數，過去大多是描述發音過程中發音腔道(vocal tract)的頻域特性，常用的特徵參
數是梅爾頻率之倒頻譜係數(Mel-scale Frequency Cepstral Coefficients，MFCC)，將一段語音訊
號轉換成以一序列的特徵參數向量來表示。語音辨識系統中的聲學模型，就由大量語音資料的
特徵參數向量序列所訓練出來，以隱藏式馬可夫模型(Hidden Markov Models , HMM)來代表一
個語音辨識單位，串接 HMM 就可以描述一句話，這套以統計模型為基礎的 HMM 方法能有效
建立語音辨識系統[Rabiner and Juang 1993]。雖然 HMM 模型參數描述一個發音狀況到下一個
發音狀態的轉移機率，但不見得能對應到發音的過程，所以並沒有真正描述語言與語音上的細
節，在環境良好，沒有太多噪音干擾的情況下，它能解決許多語音辨識的問題，也足以建構一
些特定用途的語音辨識系統。 
 
若是環境中有噪音干擾，語音辨識的正確率就大幅下降，這是環境不匹配的問題。目前有
許多針對環境不匹配所發展出來的強健性語音辨識方法(Robust speech recognition)，例如特徵
參數補償(Feature Compensation) [Acero 1992]、平行模型組合(Parallel model combination, PMC) 
[ Gales and Young 1993]、模型調道(model adaptation) [Gauvain and Lee 1994][Lee and Huo 2000] 
[ Huo and Lee 2001]、推測匹配(Stochastic matching)[Sankar and Lee 1996] [Surendran, et al. 1999]
等，基本上是在沒有語音語言知識的做法中作聲學技術的改進。反觀人類對於語音的聽辨能
力，在噪音干擾嚴重與發音差異大的情況下，還是可以交談，依然聽得懂對方說的話。人類可
以在一段語音中抓住語音的關鍵點，而這些關鍵點與某些聲學事件出現有關，這些聲學事件不
太受到複雜環境的干擾。此外，人類對於語言的了解，必然對於其聽辨語言有幫助，能依據語
言知識，判別一個發音的存在，所以發音不太正確，或有口音的情形下，仍能聽得懂對方的意
思。 
 
由於目前以統計模型與資料驅動的語音辨識方法，能改進者有限，國際上許多學者認為有
必要將語音知識引進資料驅動的模式中，建立一個以知識為基礎(knowledge based)的語音辨識
模式。這個構想涉及兩個觀念，一是語言特徵不能只有單一的發音腔道參數，如 MFCC，而是
要包含跨音框與跨音段的語音訊號，二是語音辨識模型不能只限於描述狀態轉換的 HMM，而
要有新的模型來處理更多語音訊息。本子計畫的研究重點就在探討語音屬性與事件的偵測，以
及發展語音屬性與事件的辨識模型。 
 4 
 
(lateral)。表 2-1 列出中文個種音節之聲母-韻母(Initial-Final)模型的屬性。 
 
表 2-1  中文音節之屬性 
 
 
 
 
三、以條件隨機場作中文語音屬性的辨識 
3-1 屬性偵測器 
 
就中文語音辨識而言，常使用聲母-韻母(initial-final)模型來描述一個音節(syllable)，因此
我們辨識的是聲母-韻母模型中的語音屬性。我們設計一組中文語音屬性偵測器，每一個偵測
器專司一個語音屬性的偵測，把輸入的語音訊號轉換成離散的屬性標籤，以便後續的整合與確
認程序，以確定語音屬性的標記，最後完成中文音節的辨識，其系統架構如圖 3-1 所示。 
 
 
圖 3-1 以語音屬性偵測器作中文音節的辨識架構圖 
 
語音屬性偵測器可以針對不同屬性做不同的設計，在初步研究中我們先做一個簡單的實
驗，就用 MFCC 參數與 HMM 來實作這些屬性偵測器，主要研究目的在探討以條件隨機場
(conditional random field) [Lafferty, et al. 2001]作語音屬性確認的能力。 
 6 
 
 
3-3 實驗 
 
實驗語料為在台灣錄製的 TCC300 語料庫[ Lin and Wang 2008]，錄音格式為 16-bit PCM，
取樣頻率為 16kHz，使用的訓練集包括 24742 句話，總和長度約 24 小時，測試集包含 2595 句
話，總和長度為 2.5 小時，表 3-1 列出前端屬性偵測器得到的屬性辨識正確率與精確率，正確
率不考量插入型錯誤。 
 
表 3-1 屬性偵測器之辨識正確率及精確率 
 Correction (%) Accuracy (%) 
manner 81.39 76.34 
Final onset type 86.12 82.59 
Final ending type 87.14 82.89 
place 83.82 80.77 
aspiration 88.61 84.72 
voiced 86.71 83.93 
 
在發音方式的辨識中，最不易被偵測出的是擦音，正確率約 71%，許多擦音被錯誤地辨識
成塞擦音與擦音。在發音部位的辨識中，三種舌尖音會互相混淆，以至於影響聲母的辨識結果，
在韻母節尾的型態上，鼻音結尾的韻母容易混淆，事實上韻尾鼻音常常是發音不準確的。 
 
將屬性標籤經過條件隨機場的資訊統整之後，最後的中文聲母-韻母辨識結果，見表 3-2。
對照傳統的 CI-HMM 與 RCD-HMM，採用本方法可以有較好的準確率(accuracy)，沒有太多插
入錯誤，所以與正確率(correction)相差不大。 
 
 
表 3-2 中文聲韻母辨識結果比較，辨識過程皆不納入語言模型 
System Correction (%) Accuracy (%) 
CI HMM 69.61 54.91 
RCD HMM 71.84 44.12 
HMM/CRF 61.60 58.25 
 
 
四、不知語言內容的自動音素分段 
 
對於不知其語言內容的一句語音，若能先進行自動音素分段，就能在做語音辨識之前先知
道其所含有的音素數目與音素邊界，這對於提升語音辨識正確率會有幫助。自動音素分段也可
以對錄音的語音訊號作音素邊界偵測與標記，有助於語音資料庫的建立。 
 
在過去一些自動音素分段(phone segmentation)與偵測的研究中[Mporas, et al. 2008]，最好的
實驗結果是可以在正負 20 ms 容忍度之內，有 90%的含蓋率(inclusion rate)，但是必須透過提供
一些已知的資訊幫助找出正確的標記點，這樣的作法在不能獲得語文資訊的情況下，就會不實
用。本研究提出一個不知其語音內容的二階段偵測音素邊界的方法，第一階段使用離散小波轉
換(Discrete Wavelet Transform)，找出音素邊界點的候選位置，第二階段用二維倒頻譜參數建立
音素邊界模型，加上頻譜轉變量測(spectral transition measurer, STM)，進行音素邊界點之確認，
得出偵測音素邊界點的結果 
 
4-1 候選音素邊界之偵測 
 8 
 
 
圖 4-2 小波轉換特徵參數抽取流程圖 
 
 
Frame 
圖 4-3 能量累加圖 
 
 
為了偵測其頻譜的改變，我們定義一個累加能量比值， 
)1()(
)()(
,
,
++
=
mEmE
mE
mER
kkacc
kacc
k                                (4-3) 
 10 
 
 
圖 4-5 標記出候選音素邊界點的例子 
 
4-2 音素邊界點之確認 
 
接下來要對所有候選音素邊界點作確認，採用的方法有二；一是計算頻譜轉變量(spectral 
transition measure, STM )，二是利用音素邊界模型計算其機率。 
(1) 頻譜轉變量 
 我們以 MFCC 係數計算頻譜轉變量(STM)，其計算方法如下， 
)()(
2
2
nmMFCCnma i
n
i +×= ∑
−=
                             (4-7) 
)()( 2
1
mamSTM i
D
i
∑
=
=                                (4-8) 
D 為 MFCC 維度，我們取 D=13。如果 m 這個音框位置是音素邊界點， )(mSTM 會是個大值。
我們設一個門檻，若 )(mSTM 大於門檻值，就可以認定 m 是音素邊界點。 
 
(2) 音素邊界點模型 
 音素邊界點是相連音素的訊號發生改變處，不同的音素相連情況，給予不同的音素邊界類
型。我們對數量大的邊界類型建立 GMM 模型，於是當一個音素邊界點出現時，可以用這些邊
界類型的模型來計算此音素邊界點機率，以判斷其是否真的是一個音素邊界點。我們的做法是
取音素邊界點的兩邊各 400 取樣點作為一個查驗區，用訓練語料在各邊界類型的查驗區建立
GMM 模型。在查驗區上取 300 點的音框計算倒頻譜，音框位移 100 點，因此就有六個音框的
倒頻譜。對六個音框的倒頻譜係數沿音框計算其傅立葉轉換，就得到二維倒頻譜(Two- 
Dimensioned Cestrum, TDC)。其計算方法如下， 
10   ,10           ),(),(
1
0
−≤≤−≤≤= ∑
−
=
MmNkWmnxmkX knn
N
n
           (4-9) 
)2exp( NknjW knn pi−=                   (4-10) 
 12 
 
Nasals（鼻音）  
m  n  ng  em  en  
eng  nx  
   
Semivowels（半
母音）& Glides
（流音）  
l  r  w  y  hh  
hv  el  
   
Vowels（母音） 
iy  ih  eh  ae  aa  
aw  ay  ah  ao  oy  
ow  uh  uw  ux  er  
ey  ax  ix  axr  ax-h  
Silence（靜音） pau  epi  #h  
  
 
我們統計其中出現次數最高的八種，見表 4-2，對這八種音素邊界類型分別建立 GMM 模
型。 
表 4-2 出現次數最高的八種音素邊界類型 
 
 
在進行音素邊界點確認時，我們是先用八個 GMM 模型對該邊界點作檢驗，如果其機率大
於門檻值，我們就認為此邊界點正確，若低於門檻值就作 STM 的檢驗，若 STM 值大於門檻值，
我們也認為此邊界點正確。 
 
經過此驗證之後，許多不合格的候選音素邊界點就被刪掉，圖 4-7 展示一個經過確認程序
驗證後的音素邊界點偵測結果，與圖 4-5 比較，可以看出許多候選邊界點被移除(紅色直線表
示被移除)，確認過的大部分都能與語料中人工標記的位置符合。 
 
 14 
 
 
圖 4-8 R-值的基本概念 
 
圖中虛線表示沒有差入時的擊中率，理想的目標是在多分率 OV=0 的情形下有 100%的擊
中率。若是擊中率達不到 100%，也要求不要有錯誤的分割點，所以傾向於不要多分(over 
segmentation)。因此當有一個分割音素的方法得到一個結果是擊中率為 78%，而多分率
OV=10，我們評量此方法的效果，就計算其 R 值。 
2
)(1 21 rabsrR +−=              (4-15) 
圖 4-8 所標示的距離 1r 與 2r 都要越小越好。 
22
1 )()100( OSHRr +−=            (4-16) 
2)100(2 −+−= HROSr             (4-17) 
 100×=
ref
hit
N
N
HR (擊中率)            (4-18) 
 100)1( ×−=
ref
b
N
NOS (多分率)           (4-19) 
 100×
+
=
faref
fa
NN
N
FAR (假警報率)          (4-20) 
faN ：誤認是邊界點的總數 
 
我們以 TIMIT 中的 Training set 作為測試語料，其資料量見表 4-3。 
 
表 4-3 測試語料 
TIMIT 
Train Set 
語者數 句數/語者 語料總數 
女性語料  176 10 1760 
男性語料  328 10 3280 
總計  504 10 5040 
 
而以 TIMIT 中的 Test set 一部份作為建立八種音素邊界類型，其資料量見表 4-4。 
 16 
 
表 4-8  與其他研究結果比較 
 Method  F(%) RCL(%) OS(%) 
1 Jump function [Aversano et 
al. 2001] 
 73.58 0 
2 STM [ Dusan and Rabiner 
2006] 
 75.3  
3 DISTCOMP [Almpanidis et 
al. 2009] 
73.6 74.5  
4 DISTCOMPR [Almpanidis 
et al. 2009] 
74.7 75.4  
5 SVF [Almpanidis et al. 
2009] 
67.7 70.4  
6 DCF [Almpanidis et al. 
2009] 
70.1 73.3  
7 Our method 81.4 97.7 -3.7 
 
 
五、爆發音事件偵測 
 
爆發音(burst)是在發出塞音與塞擦音時發生的現象，是判斷塞音與塞擦音存在的語音事
件，圖 5-1 展示兩個爆發音出現的情形，在塞音的最前端，有閉塞到爆發的轉換(closure-burst 
transition)，以紅色框出，其轉換點就是爆發音起點(burst onset)。 
 
 
圖 5-1 兩個爆發音出現的位置 
 
如果我們可以有效的偵測爆發音事件，估算其爆發音起點位置，就可以更準確的辨識塞音與塞
擦音。因為語音事件是發音方式產生的現象，在不同語言中也同樣出現，進行爆發音事件偵測
對跨語言的語音辨識會有幫助，是值得做的一項研究。 
 
5-1 隨機森林模型的建立 
 
隨機森林(random forest) [Breiman 2001]的演算法係由 L. Breiman 於 2001 年提出，是一個
分類與回歸方法，我們將此方法改用於聲學訊號的處理，需要做一些調整。隨機森林是由一群
決策樹(decision tree)組成，它包含三個觀念，(1)分類與回歸樹(Classification and Regression Tree, 
CART)[ Breiman, et al. 1984]、(2)拔出與集成(bootstrapping and aggregation, 簡稱 bagging) 
[ Breiman 1996]、及(3)隨機子空間(random subspace)[ Ho 1998]。 
 18 
 
可以用 DFT 公式計算 
kn
N
jN
n
emnxmkX
pi21
0
),(),( −
−
=
∑=                              (5-3) 
kv
N
jN
n
emkX
N
mvc
pi21
0
|),(|log1),( ∑
−
=
=                           (5-4) 
如果將(5-3)式再加上一層 IDFT 演算，就得到二維倒頻譜係數(two-dimensional cepstral 
coefficient, TDC coefficient )[Ariki, et al. 1989][ Pai and Wang 1993]， 
mu
M
jM
m
emvc
M
uvX
pi21
0
),(1),(ˆ ∑
−
=
=                             (5-5) 
這個新的係數 ),( uvX) 中，v 是倒頻譜之指數，即 quefrency，u 則是沿著音框所對應的頻域指數。
事實上倒頻譜的計算不只有(5-4)式，也可以在頻域上取梅爾頻帶，這樣就可得到梅爾頻率倒頻
譜係數(Mel Frequency Cepstral Coefficient, MFCC)，然後再用 DCT 轉換得到二維倒頻譜(TDC)。 
)}(|),(|log{),( 1
1
kBmkXmlY l
f
fk
l
l
∑
+
−
=
=                         (5-6) 
)(kBl 表示第 l 個梅爾頻帶的三角形濾波器。 
)
)
2
1(
cos(),(1),(~
1 L
vl
mlY
L
mvc
L
l
pi−
= ∑
=
                         (5-7) 
另外一種做法，是從線性預估分析(Liner prediction analysis, LP analysis)來計算頻譜，取代(5-1)
的演算，再求得倒頻譜係數，稱為 LPCC。 
)
)
2
1(
cos(),(~1),(ˆ
1
0 M
um
mvc
M
uvY
M
m
pi−
= ∑
−
=
                      (5-8) 
 
得到的二維倒頻譜係數構成一個矩陣，我們並不需要使用整個矩陣來描述語音，因為重要
的語音訊息會在低階指數部份，通常採用其一個角落就足夠了。圖 5-2 描述一個從 TDC 取出
特徵參數向量的做法。 
 
 
圖 5-2 一個從 TDC 取出特徵參數向量的做法 
 
在圖 5-2 的例子中，每個音框計算 240 個 LPCC 參數，以 10 個音框組成 240x10 的矩陣，再計
算得到 240x10 的 TDC 矩陣，但是只是取代低階指數部分的 55 個參數，組成一個 55 維的特徵
向量，來描述一段語音訊號，紅色箭頭表示從 TDC 矩陣取出係數的順序，先從列開始。 
 
Extract 55 
coefficients 
55x1 
vector 
 20 
 
 
在訓練階段，我們以塞音的爆發音起點(burst onset)為塞音的起始邊界，由此點的前 5 個音
框與後 5 個音框組成一個時頻片段，來求得其 TDC 係數並構成特徵向量。利用訓練語料的特
徵向量來建構隨機森林，在決策樹分裂結點時，是隨機抽取 56 個參數中的 8 個(即 d = 8)來作
決策， Dd ≈ ，這與理論上的估計相近。 
 
在測試階段，我們可以計算隨機森林偵測器對每一個輸入的特徵向量 x(t)所作的投票結
果，對 9 個類別計算 9 個得票數，寫成向量形式表示如下， 
)),(()( Θℜ= txtv              (5-9) 
)(tv 是每一個音段類別的得票數，因此我們計算其事後機率，以向量形式表示如下， 












−=
−=
−=
==
ℵ
))(|(
))(| (
))(| (
))(|()(
txspeechnoncp
txburststopvoicelesscp
txburststopvoicedcp
txpt
M
c
v
     (5-10) 
 
上式中ℵ是總投票數，此向量紀錄各音段類別出現的事後機率，其中最前兩項是我們要的，合
併寫成 
))(|( txburstcp =              (5-11) 
在確認階段，我們定義一個函數， 
))(())(|)(max
)(|(log()( bursttcI
txcp
txburstcp
t
burstc
b =
=
=
≠
η        (5-12) 
其中 I(A)是一個指標值， 



=
=
= falseAeventif
trueAeventif
AI
     ,0
     ,1)(           (5-13) 
累計屬於同一音段類別的 )(tbη ，得到 
)()( zsr b
sz
z
z
η∑
=
=              (5-14) 
若得到的結果是 0)( =zsr ，表示該音段是非爆發音的音段，若 0)( >zsr ，表示該音段是爆發音
音段。為了判斷的慎重，我們加入一個門檻值λ ， 0>λ ，判斷方式如下。 
zz sinonsetburstnosr       ,0)( =  
zz sinonsetburstweakasr        ,)(0 λ≤<          (5-15) 
zz sinonsetburststrongasr        ,)( λ>  
於是在 λ>)( zsr 時，就確定爆發音的起點是在 
}|)({ maxarg)( maxargˆ z
t
z
t
b sttst ∈== ηη         (5-16) 
我們可以稍微升高λ 值，以降低偵測爆發音的等錯誤率(EER)，讓 *λλ =  
 
若在一個短時段中被找到一個以上的爆發音起點，我們就得訂一個規則把不可能的去掉，
要求兩個爆發音起點之間的間隔不能太近，通常可訂為 20ms。若同時有多個可能，就選其中
)( zsr 最大者，其餘去掉，圖 5-4 是一個決定爆發音起點的例子。 
 
 22 
 
爆發音偵測也可以用高斯混合模型法(GMM)或支持向量機法(SVM) [ Burges 1998]，但是
訓練模型相當費時，比較其結果，隨機森林偵測器得到的結果最好，更重要的是所需的訓練語
料很少，上述實驗只用了四個人的語音，就有相當好的結果。圖 5-7 展示 GMM、SVM、及隨
機森林偵測器作爆發音偵測的結果。 
 
 
 
 
圖 5-7  
 
將爆發音偵測結果的資訊，即其事後機率，視為額外參數，加入到一段語音辨識的 MFCC
參數向量中，可以進一步提升對於塞音與塞擦音的辨識正確率，此結果見圖 5-8，圖中 BOI 標
示有加入爆發音偵測結果的資訊。 
 
 
 
圖 5-8 爆發音偵測可以提升對於塞音與塞擦音的辨識正確率 
 
 
 
 
 24 
 
),( kns 是指第 k 個音框，N 是音框長度。 
 
頭兩個自相關函數的差值為 
)1(1)1()0(' kkkk RRRR −=−=                   (6-4) 
若 'kR 發生大的改變，就表示有濁音起始點，因此定義下式 
2|| ' 1' 1' −+ −=∆ kkk RRR                     (6-5) 
濁音起始點就在 
'argˆ k
t
v Rmant ∆=       ]1  ,1[ −++−∈ ttttk vv             (6-6) 
當得出 vtˆ 之後，計算其與爆發音起始點的差距，就是我們估測的 VOT， 
bv tt ˆˆˆ −=τ               (6-7) 
 
6-2 實驗 
  
我們使用的實驗語料來自 TIMIT 語料庫[Garofolo, et al. 1993]，這個語料庫中的語音是美式
英語，它是讀語(read speech)，涵蓋盡量多的語音學上的變化(phonetic variability)。所有語句由
麥克風錄音，以 16KHz 取樣，取樣數據為 16-bit PCM 格式。全部發音人數為 630 人，其中男
性 438 人，女性 192 人，語音來由 8 個不同方言地區(dialect region)，標示成 DR1~DR8。這個
語料庫又分成訓練集(training set)與測試集(testing set)，訓練集有 462 個發音人，含 326 位男性
與 136 位女性，測試集則有 168 位發音人，含 112 位男性與 56 位女性。語句的文本內容有三
類； 
SA－Shibboleth 方言 
SX－phonetically-compact 
SI－phonetically-diverse 
每個發音人錄音 10 句，平均長度約三秒，這些語句都已作了文本、語音，及時間的標記。 
 
我們從測試集中取 1348 個語句作實驗，其中含有 2344 個詞有詞首塞音(word-initial stop)，
1440 個詞有詞中塞音(word-medial stop)，分散在 968 個不同的詞中。表 6-1 列出這些塞音的分
布。 
 
表 6-1 實驗語句中塞音的分布。 
 
 
詞中塞音(word-medial stop)的偵測較難，我們先以詞首塞音(word-initial stop)為實驗對象。
實驗語料中塞音後接的母音的出現個數，列在表 6-2。 
 
 
 
 26 
 
我們只用 40 個訓練語句，建立爆發音與濁音起始點的偵測模型。這些訓練語料從 DR1 集
合中取出，計算 24 階的 LP 係數，導出 LPC 頻譜，取 20~7500Hz 範圍，計 240 點，然後對連
續 10 個音框作二維倒頻譜(TDC)演算，取 55 個 TDC 係數，建立 30 個決策樹的森林模型。 
 
在做強迫對齊時，我們選擇在音素階層作對齊(標示成 HMM-FA-PL)或在狀態階層做對齊
(標示成 HMM-FA-SL)。根據強迫對齊的結果，再作隨機森林的起始點偵測，加起始點偵測者，
標示為 HMM-FA-PL+OD 及 HMM-FA-SL+OD。 
 
找到爆發音與濁音起始點後，與人工標音比較，計算三個差值; 
|ˆˆ| bbb ttt −=∆                   (6-8) 
|ˆ| vvv ttt −=∆                   (6-9) 
|ˆ| τττ −=∆                   (6-10) 
 
在容忍範圍為 5ms、10ms、15ms 及 20ms 下，分別計算正確率(accuracy rate)，表 6-3 列出 6 個
塞音的爆發音起始點偵測結果，在 20ms 的容忍範圍下，正確率為 99.1%。 
。 
 
表 6-3 六個塞音的爆發音起始點估測結果 
Tolerance limits 
 <5ms <10ms <15ms <20ms 
/b/ 53.5 84.4 96.4 98.7 
/d/ 64.2 83.0 97.8 100.0 
/g/ 65.3 83.2 97.1 99.4 
/p/ 66.8 82.4 96.7 98.7 
/t/ 64.4 77.3 93.9 98.9 
/k/ 66.1 85.0 96.9 99.2 
voiced 59.5 83.7 97.1 99.3 
voicelsee 65.8 81.9 95.8 99.0 
bilabial 58.3 83.7 96.5 98.7 
alveolar 64.3 80.5 96.1 99.5 
velar 65.9 84.5 96.8 99.3 
Average 62.6 82.8 96.5 99.1 
 
表 6-4 列出不同後接母音情形下的濁音起始點偵測結果，在 20ms 的容忍範圍下，正確率
為 97.0%。 
 
表 6-4 不同後接母音的濁音起始點偵測結果 
Tolerance limits 
 <5ms <10ms <15ms <20ms 
high 54.3 85.6 95.0 97.9 
mid-high 56.0 83.6 96.1 98.5 
 28 
 
形下，得到 79.7%的擊中率。在語音事件偵測方面，提出以隨機森林模型的方法，作爆發音事
件偵測與濁音起始點偵測，只需要很少數的訓練語料，就可以建立隨機森林模型，在 20ms 的
容忍範圍下，爆發音與濁音起始點偵測的正確率分別達到 99.1%與 97.0%，VOT 的估測正確率
達到 96.5%。在對塞音與塞擦音的語音屬性辨識上，也得到超越過去研究報告的結果，將可應
用在語音資料庫的標音，以及改進自動語音辨識的效能。 
 
 
八、計畫成果自評 
 
本計畫是在探討新的語音辨識技術，利用語音屬性之辨識與語音事件之偵測，來改進現有
的語音辨識方法，等於是把語音學知識(phonetic knowledge)加入過去全由資料驅動(data driver)
的語音辨識模式。所發展出來的辨識模型有別於現行的隱藏式馬可夫模型(HMM)，以上的方
法採用英語語料庫 TIMIT 與在台灣錄音的國語語料庫 TCC300 進行實驗，很具有參考價值。
在這一系列的語音屬性辨識與語音事件偵測研究中，有一位博士生及三位碩士生以此相關研究
題目完成其畢業論文，在國際期刊發表論文二篇，在國際研討會發表論文四篇。兩篇在國際期
刊發表的論文分別刊登在 IEEE Trans. on Audio, Speech, and Language Processing,與 Journal of 
Acoustical Society of America，提出將隨機森林模型方法應用在爆發音事件的偵測，以及濁音起
始時間的估算，是創新的構想。 
 
 
 
參考文獻 
 
[1] A. Acero (1992), Acoustical and Environmental Robustness in Automatic Speech Recognition, 
Kluwer Academic Publishers, 1992 
[2] G. Almpanidis, M. Kotti, and C. Kotropoulos (2009), “Robust Detection of Phone Boundaries 
Using Model Selection Criteria With Few Observations,” IEEE Transactions on Audio, Speech, 
and Language Processing, vol.17, no.2, pp.287-298, 2009 
[3] Y. Ariki, S. Mizuta, M. Nagata, and T. Sakai (1989), “Spoken-word recognition using dynamic 
features analyzed by two-dimentional ceprteum,” IEE Proceedings, v. 136, pp. 130140, 1989. 
[4] G. Aversano, A. Esposito, and M. Marinaro (2001), “A new Text-Independent Method for 
Phoneme Segmentation,” Proc.IEEE International Workshop on Circuits and Systems, vol.2, pp. 
516-519, 2001 
[5] M. Bishop (2006), Pattern Recognition and Machine Learning, Springer, 2006. 
[6] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone (1984), Classification and 
Regression Trees. Pacific Grove, CA: Wadsworth & Brooks/Cole Advanced Books & Software, 
1984 
[7] L. Breiman (1996), “Bagging predictors,” Machine Learning, vol. 24, pp. 123–140, August 
1996. 
[8] L. Breiman (2001), “Random forests,” Machine Learning, vol. 45, no. 1, pp. 5–32, 2001. 
[9] F. Brugnara, R. De Mori, D. Giuliani, and M. Omologo (1992), “Improved connected digit 
recognition using spectral variation functions,” in Proc. Int. Conf. Spoken Lang. Process., 1992, 
vol. 1, pp. 627–630 
[10] C. J. C. Burges (1998), “A tutorial on support vector machines for pattern recognition,” Data 
Mining and Knowledge Discovery, vol. 2, pp. 121–167, 1998. 
[11] C. Sidney Burrus, Ramesh A. Gopinath, and Haitao Guo (1998), Introduction to Wavwlets and 
Wavelet Transforms: A Primer, Prentice Hall, 1998. 
 30 
 
Speech Recognition," IEEE Trans. on Acoustics, Speech, and Language Processing. vol.19, 
no.5, pp.1253-1264, 2011. 
[32] C.Y. Lin and H.C. Wang (2011), "Automatic estimation of voice onset time for word-initial 
stops by applying random forest to onset detection," Journal of Acoustical Society of America. 
(accepted) 
[33] Y. C. Lin and H. C. Wang (2005), “Nasal detection in continuous Mandarin speech,” Oriental 
COCOSDA workshop 2005, Jakarta, Indonesia, 2005 
[34] R. Lippmann (1997), “Speech Recognition by Human and Machines,'' Speech Communication, 
Vol. 22, pp. 1-14, 1997. 
[35] C. F. Lu and H. C. Wang (2010), “A Metric-based Phone Segmentation Method using Wavelet 
Transform,” Oriental COCOSDA workshop 2010, Kathmandu, Nepal, 2010, paper_ 10 
[36] S. Mallat (1989), “Multifrequency channel decomposition of images and wavelet models,” 
IEEE Transactions on Acoustics, Speech, and Signal Processing, vol.37, pp.2091-2110, 1989. 
[37] C. Mitchell, M. Harper, and L. Jamieson, “Using explicit segmentation to improve HMM phone 
recognition,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Vol. 1, pp. 229-232, 
1995. 
[38] I. Mporas, T. Ganchev, and N. Fakotakis (2008), “A Hybrid Architecture For Automatic 
Segmentation Of Speech Waveforms,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process 
2008, pp. 4457-4460. 
[39] P. Niyogi and M. M. Sondhi (2002), “Detecting stop consonants in continuous speech,” Journal 
of Acoustical Society of America, vol. 111, pp. 1063–1076, February 2002. 
[40] H.-F. Pai and H.-C. Wang (1993), “Two-dimensional cepstral distance measure for speech 
recognition,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 
vol. 2, pp. 672–675, 1993. 
[41] L. R. Rabiner and B. H. Juang (1993), Fundamentals of speech recognition, Prentice-Hall, 
1993. 
[42] O. J. Rasanen, U. K. Laine, and T. Altosaar (2009), “An Improves Speech Segmentation 
Quality Measure: the R-value,” Interspeech 2009 
[43] A. Sankar and C.-H. Lee (1996), "A Maximum-Likelihood Approach to Stochastic Matching for 
Robust Speech Recognition," IEEE Trans. Speech and Audio Proc., Vol. 4, No. 3, pp. 190-202, 
May 1996. 
[44] O. Scharenborg, V. Wan and M. Ernestus, “Unsupervised speech segmentation: an analysis of 
the hypothesized phone boundaries,” J. Acoust. Soc. Amer Vol. 127(2),  pp. 1084-1095, 2010. 
[45] K. T. Sung, H. C. Wang (2006), “A study of knowledge-based features for obstruent detection 
and classification in continuous Mandarin speech,” International Symposium on Chinese Spoken 
Language Processing, ISCSLP2006, Singapore, 2006. 
[46] C. Surendran, C.-H. Lee and M. Rahim (1999), "Non-Linear Compensation for Stochastic 
Matching," IEEE Trans. on Speech and Audio Proc., Vol. 7, No. 6, pp. 643-655, 1999. 
 
 
1 
 
 
行政院國家科學發展委員會 
補助國內專家學者出席 國際學術會議 報告 
 
報告人：王小川 
國立清華大學電機工程學系 教授 
 
會議名稱： 
2010 國際語音資料庫與評量技術之協調與標準化委員會--亞洲區會議(Oriental COCOSDA 
2010) 
日期：99 年 11 月 24-25 日 
地點：尼泊爾 加德滿都 
 
發表論文及文件 
(1) “A metric-based phone segmentation method using wavelet transform,” Ching-Feng Lu, 
Hsiao-Chuan Wang. 
(2) “Oriental COCSDA 2010 - Country Report: Activities on Language Resource and 
Technologies in Taiwan”, Hsiao-Chuan Wang. 
(3) “Oriental COCOSDA 2011 in Taiwan”, Hsiao-Chuan Wang. 
 
一、參加會議經過 
語音資料庫與評量技術之協調與標準化委員會(International Committee for Co-ordination 
and Standardization of Speech Database and Assessment Techniques, COCOSDA)是 1991 年由
國際口述語言處理研討會(International Conference on Spoken Language Processing, ICSLP)
的工作委員會倡議成立，隨後各地區分別成立地區委員會。 1995 年在日本成立 Oriental 
COCOSDA， 邀請亞洲國家加入。 
 
1997 年 3 月 21 日在香港舉行 Oriental COCOSDA 會議，由日本召集，出席者有日本、韓
國、中國、台灣，及香港的代表，本人代表台灣出席，會議中決定 1998 年起每年在亞洲國
家輪流召開委員會，並舉辦講習會(Oriental COCOSDA Workshop)。第一次講習會於 1998
年在日本筑波舉行，此後於 1999 年在台北舉行，2000 年在中國北京舉行，2001 年在韓國
Taejon 舉行。其後加入的國家漸增，講習會儘量由新加入國家主辦，2002 年起主辦 Oriental 
COCOSDA Workshop 的城市為泰國 Hua Hin，新加坡，印度新德里，印尼雅加達，馬來西
亞檳城，越南河內，2008 年回到日本京都，2009 年原定新疆烏魯木齊，後改在北京，2010
年在尼泊爾加德滿都。每次會議除了發表論文之外，也舉行委員會議，由各國代表作 Country 
report，近年來是由我代表台灣作報告。(見 Oriental COCOSDA 2010 Country Report)。2011
年的會議預定由台灣主辦，本人代表台灣在會議中報告，並歡迎各國參加(見 Oriental 
COCOSDA 2011 in Taiwan)。 
 
 
3 
 
A Metric-based Algorithm for Automatic Phone Segmentation 
 
 
Ching-Feng Lu and Hsiao-Chuan Wang 
Department of Electrical Engineering, National Tsing Hua University 
Hsinchu, Taiwan 
g9761589@oz.nthu.edu.tw ; hcwang@ee.nthu.edu.tw  
 
 
Abstract 
 
This paper proposes a metric-based algorithm for 
the phone segmentation without prior knowledge about 
the text content. It detects phone boundaries in a 
sequential manner. Starting from a beginning point, 
the discrete wavelet transform is applied to variable 
length frames in order to search for a candidate phone 
boundary. Once a candidate phone boundary is 
detected, the verification process follows to confirm the 
existence of a phone boundary. This verification 
process is accomplished by detecting the spectral 
change based on a model selection criterion and the 
normalized spectral variation function (SVF). If the 
phone boundary is confirmed, this location is 
considered the new beginning point for finding next 
phone boundary. The experiment was conducted on 
TIMIT corpus, and the performance was measured in 
F1-score and R-score. The average F1-score and 
R-score of 640 test utterances are 72.4% and 75.1% 
respectively  with 20 ms tolerance. 
 
1. Introduction 
 
The phone segmentation is one of annotation 
processes in the development of speech corpus. 
Manual segmentation is believed to be more accurate, 
but is expensive and tedious. The inconsistence may be 
involved when the speech data are labeled by different 
persons. It is desirable to label the phones in an 
automatic manner. Some methods have been proposed 
to annotate the speech signal with the assistance of text 
and phonetic information [1][2]. But in many cases, the 
prior information of text content may not be available. 
Without any prior information of text content, it is hard 
to label phone boundaries accurately. A way to find 
phone boundary is to detect the change in a feature 
domain. The big change of speech feature implies the 
existence of phone boundary. This can be 
accomplished by model-based approach or 
metric-based approach.  
The model-based approach means we need to 
generate phone or phone boundary models using 
training data. The methods of hidden Markov model 
(HMM) and artificial neural network (ANN) are the 
popular ones for generating models in speech 
processing. A good result reported by Hosom [2] 
which combined HMM and ANN techniques to 
perform speech segmentation on TIMIT corpus.  He 
got an inclusion rate of 92.57% in the tolerance of 20 
ms. Kuo and Wang [3] used the HMM/SVM method to 
improve the phone segmentation. Usually a large 
training database is needed for generating good 
models. 
The metric-based method does not need a large 
training database. The spectral change may imply the 
existence of a phone boundary. Delta cepstral function 
(DCF) [4] and spectral variation function (SVF) [5] 
have been used for detecting the spectral change. 
Almpanidis et al. [6] investigated the phone boundary 
detection using model selection criteria based on the 
DISTBIC hybrid segmentation algorithm. Their 
experiments on NTIMIT database achieved F1-score of 
74.7% with 20 ms tolerance in phonemic segmentation. 
Aversano et al. [7] used perceptually modifies 
spectrum in detecting phoneme boundaries. Their 
experiments got a correct segmentation around 74%.  
This paper develops a metric-based algorithm for 
phone segmentation without prior knowledge about the 
text content. Starting from a beginning point, the 
discrete wavelet transform (DWT) is applied to 
variable length frames to search for a candidate phone 
boundary. A verification process follows to confirm 
the existence of a phone boundary. Once a phone 
boundary is confirmed, this location is considered the 
new beginning point for finding next phone boundary. 
A model selection criterion combined with the SVF is 
applied for the verification process. The experiment 
was conducted on TIMIT corpus, and the performance 
was measured in F1-score and R-score. The average 
F1-score and R-score of 640 test utterances are 72.4% 
and 75.1% respectively  with 20 ms tolerance. 
5 
 
at point 16)1(80 ×−+= CPBn .  
 
3. Verification of Phone Boundaries 
 
When a candidate phone boundary (CPB) is located, 
an interval covers 500 sample points before CPB and 
500 sample point after CPB is used to confirm the 
existence of phone boundary. Two frame-based 
features are derived.  
(a) Wavelet parameters 
The frame length is 256 sample points and with 
128 sample point shift. The discrete wavelet transform 
is applied to each frame to get a set of 256 wavelet 
parameters. The spectral variation function (SVF) at 
frame m is defined by the following equation. 
11
11 ,)(
+−
+−
=
mm
mm
mSVF
SS
SS
           
(2) 
where m is frame index, •  means vector norm, and 
••,  represents the inner product. The value of 
)(mSVF  is within the range of ]1,1[− . A more 
convenient expression is to define the distance between  
two frames. 
)|)(|max
)(1(
2
1)(
mSVF
mSVF
mD
m
SVF −=
          
(3) 
The value of )(mDSVF  is in the range of ]1,0[ .  
(b) Mel-frequency cepstral coefficients  
The frame length is 320 samples and the frame 
shift is 32 samples. Mel-frequency cepstral coefficients 
(MFCC) are calculated for each frame and each frame 
is converted to a 13 dimensional vector.  A model 
selection criterion used for detecting spectral change is 
applied based on MFCC. Bayesian information 
criterion corrected (BICC) is define as 
1
ln]}|,...,,[ln{2)( 21
−−
+−=
kN
NkNMlMBICC zNz SSS   
(4) 
 
2
)1( +
+=
QQQk  
where N is sequence length, Q is dimension of feature 
vector, and }|],...,,[ln{ 21 zN Ml SSS  indicates the 
log-likelihood of a sequence of feature vectors 
corresponding to a model 
zM . If this sequence of 
feature vectors is generated by a concatenation of two 
statistics models 
xM  and yM , their corresponding 
BICCs are calculated with sequence lengths 
xN  and 
yN  where yx NNN += . Then the model change is 
determined by using following measure, 
)()()( zyx MBICCMBICCMBICCBICC −+=∆        
(5) 
0>∆BICC  implies that the sequence 
],...,,[ 21 NSSS  comes from one model zM , 
and 0<∆BICC  implies that the sequence 
],...,,[ 21 NSSS  comes from two models xM  
and yM . In order words, 0<∆BICC  implies 
the existence of phone boundary.  
    The verification of phone boundary (PB) 
is accomplished by the following procedure. 
 
If 0>∆BICC , 
       Then If 
mSVF ThmD <)(  
           Then PB= false,   Else 
PB= true 
       Else If LSVF ThmD <)(  
               Then PB= false 
             Else If HSVF ThmD >)(  
                        Then PB= 
true 
                         Else If 
aSVF ThmD >)(  
                             Then 
PB= true,   Else PB= false 
 
The thresholds are determined empirically.  
65.0,5.0,35.0,001.0 ==== HmaL ThThThTh  
 
4. Phone Segmentation Algorithm 
 
The phone segmentation algorithm is summarized 
by the following procedure.  
 
(1) Initialization: 
    SP= beginning point of an utterance 
(2) Iteration: 
    Searching for candidate phone boundary (CPB) 
    Verifying phone boundary (PB) 
    If PB= true, Set SP= PB location 
    If PB= false, Shift SP by 10 ms to get a new SP 
(3) Termination: 
    PB= false at the end of the utterance  
 
7 
 
Table 3 shows that the hit rate can be 70% in 
average for all test utterance. The difference 
between male and female speakers is 
insignificant. The over segmentation is in 
negative value and does not affect in various 
tolerances. The negative value of over 
segmentation means that the proposed 
algorithm tends to produce a result of missed 
detection. 
In order to investigate the effectiveness of 
phone segmentation for various phone 
concatenations, we group the phones into six 
categories; stop and affricate (SA), fricative 
(F), nasal (N), vowel (V), semivowel and 
glide (SG), and silence (S). Then there are 35 
concatenation types of these categories. In the 
test data of 640 utterances, the total number 
of phone boundaries is 24284. Among them, 
the 12 most frequent phone boundaries which 
cover 80.5% of total phone boundaries are 
given in Table 4. The results are measured in 
the case of 20 ms tolerance. 
 
Table 4. 12 most frequent phone boundaries 
Boundar
y Type 
refN
 
findN
 
Percentag
e  in test 
data (%) 
Hit 
Rate 
(%) 
SA-SA 286
2 
1640 11.78 57.3
0 
V-SA 262
2 
2170 10.79 82.7
6 
SA-V 257
2 
2227 10.59 86.5
8 
SG-V 196
2 
1121 8.07 57.1
3 
F-V 190
3 
1162 7.83 84.7
0 
V-F 188
5 
1553 7.76 82.3
8 
V-N 167
3 
1239 6.88 74.0
5 
V-SG 105
6 
559 4.34 52.9
3 
N-V 893 680 3.67 76.1
4 
SA-SG 767 633 3.15 82.5
2 
SA-F 703 456 2.89 64.8
6 
F-SA 645 396 2.65 61.3
9 
 
In Table 4 we can find that the top five hit 
rates are for boundary types of SA-V, F-V, 
V-SA, SA-SG, and V-F. The low hit rates 
happen at V-SG, SG-V, and SA-SA. It seems 
reasonable because the spectral change is 
obvious at these phone boundaries. The 
boundary between vowel and semivowel is 
always difficult to be identified. Table 5 gives 
the boundary types of easy and difficult to be 
detected in terms of hit rate. 
 
Table 5 The boundaries of easy and difficult 
to be detected 
HR (%) Boundary 
Type 
HR (%) Boundary 
Type 
92.40 S-V 25.13 SA-S 
86.76 SG-SA 29.54 N-N 
86.58 SA-V 35.26 V-S 
84.70 F-V 35.42 F-F 
83.33 SG-F 37.26 SG-S 
82.76 V-SA 38.88 N-S 
82.52 SA-SG 45.07 V-V 
82.38 V-F 46.34 N-SA 
 
In Table 5 we can find that some cases are 
with very low frequent in the test database. 
Their performance is not significant in the 
over all evaluation. But it gives a clear picture 
of how those boundaries are difficult to be 
detected. 
 
6. Conclusion 
 
This paper presents a method to find 
phone boundaries in a sequential manner. The 
study also shows some phonetic interests. The 
performance is comparative to the results 
presented by Aversano et al. [7]. The problem 
9 
 
Oriental COCOSDA 2010 – Country Report 
 
 
 
 
 
 
 
11 
 
 
 
 
 
 
 
 
 
 
13 
 
 
 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：王小川 計畫編號：97-2221-E-007-075-MY3 
計畫名稱：新世代自動語音辨識技術之研究 – 第二階段--子計畫三：語音屬性與事件之辨識模型研
究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 2 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
