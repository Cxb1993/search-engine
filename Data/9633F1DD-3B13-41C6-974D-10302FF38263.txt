 ii
Contents 
1. 前言........................................................................................................................................................................... 1 
2. 研究目的................................................................................................................................................................... 2 
3. 研究方法................................................................................................................................................................... 3 
3.1 JAVA-BASED 應用程式 ............................................................................................................................................ 3 
3.2 JSP 網頁 ................................................................................................................................................................ 12 
3.3 PDA ....................................................................................................................................................................... 18 
4.4 XML 資料整合與轉換 ......................................................................................................................................... 21 
4. 軟體與硬體架構..................................................................................................................................................... 22 
4.1 PCCLUSTER........................................................................................................................................................... 22 
4.2 GRID....................................................................................................................................................................... 22 
4.3 軟體總架構 ........................................................................................................................................................... 23 
4.4 硬體總架構圖 ....................................................................................................................................................... 23 
4.5 軟體總架構圖 ....................................................................................................................................................... 24 
4.6 網站地圖 ............................................................................................................................................................... 24 
5. 結果與討論............................................................................................................................................................. 25 
5.1  PC CLUSTER ....................................................................................................................................................... 25 
5.2 GRID .............................................................................................................................................................. 26 
5.3 APPLICATION 與直接在 GRID 上測試之效能比較 ..................................................................................... 27 
6. 分析與比較............................................................................................................................................................. 27 
我們使用的平台的各種比較： ...................................................................................................................................... 27 
7. 結論與未來發展..................................................................................................................................................... 28 
8. REFERENCE........................................................................................................................................................ 29 
9. 使用說明書............................................................................................................................................................. 30 
9.1 APPLICATION ......................................................................................................................................................... 30 
9.2 PDA ....................................................................................................................................................................... 32 
9.3 XML ...................................................................................................................................................................... 38 
10. 生物資訊軟體安裝說明 .................................................................................................................................... 39 
11. 附錄 (APPENDIX) ............................................................................................................................................... 41 
11.1 附錄 A：CLUSTER 叢集系統安裝手冊.............................................................................................................. 41 
11.2 附錄 B：GRID 系統安裝手冊 ............................................................................................................................ 53 
 1
1. 前言 
    人類基因體定序計劃(human genome project) 在科學界是一項吸引眾人目光的偉大工
程，人類基因組是由大約三十億鹼基對(A, T, C, G)所組成。如果將這些資訊儲存成文字檔，
大約需要兩千片磁碟片才足夠。而這只是一個開端而已，隨之而來的是更大量的分析資料，
因而需要建立許多不同的巨型資料庫來儲存。而這些基因資料的解析如果沒有藉由電腦的輔
助，這麼大量的資訊就如同一本沒有按姓名筆劃排列的電話號碼簿一般，雜亂無章，是一點
利用價值都沒有。生物資訊學 －一個結合資訊科學與生物學的新名詞，正隨著這一波資訊潮
流蓬勃發展，進而可能使整個生物醫學研究全面改觀。隨著電腦發明而蓬勃發展，從前人們
解析 DNA，使用的是速度緩慢的電腦，但是，這已不敷需求了，現在所擴展出來的資料量不
是一台個人電腦可以獨立完成的，需要的是一台計算能力跟儲存空間卓越的超級電腦；然而，
超級電腦的價格相當昂貴，一般人不可能負擔，難道生物資訊就這樣被限制住了嗎？不會的！
近年來電腦運算處理的速度以指數攀升，配置上透過新興的 Grid(格網)與既有的 PCCluster(個
人電腦叢集)技術做整合，便可以與超級電腦相較勁，提高了生物資訊的普遍性。 
    格網就是運用寬頻網路，將各個地方的機器集合在一起，聚合成一台虛擬的超級電腦，
格網的便利性，是如同飲水一樣方便，我們後端的使用者，可以在任何時間，任何地點的使
用它，只要你有一台電腦，插上網路線，連結到格網，就可以使用這一台強大的電腦了，格
網的發展對於學校､一般的實驗室很有利，因為不用擁有很多的經費，就能夠架設格網來處
理龐大的資料，而格網對於生物資訊的發展更是一個不可多得的工具。格網運用在生物資訊
上面就叫做「生物格網」，格網使得生物資訊的發展有很大的突破，突破了之前傳統電腦時期
所遭遇的瓶頸，不僅僅提升了發展的速度，也有更大的儲存空間可以供比對後資料庫的儲存。 
    個人電腦叢集在高速計算領域已被證實具有很優越的價格性能比(Price/Performance)，且
使用叢集技術在 Linux 作業系統平台上，透過 PVM (Parallel Virtual Machine)和 MPI 
(Message-Passing Interface)介面傳遞資料庫，來執行高速及平行計算與處理技術是可行的，恰
好生物軟體所需的便是這種大量的平行高速的運算，這對生物資訊的應用領域及學術發展將
有莫大的助益。 
    生物資訊的發展，侷限於資料量的過於龐大，以致無法建立生物資料庫來存放已知基因
序列；發展生物軟體，用來將新的序列與資料庫內的資料做比對。但是中間牽扯到的一個重
要的問題是：現在各地所開發生物資訊所用的系統不同，資料庫也有所不同。由於現在發展
生物資訊的各個研究單位可能使用不同的平台，不同的資料庫來存放他們的研究資料。因此
存放資料的格式，便會有所差異。這會造成在資源共享上有所困難。為了改善這個問題，我
們將生物軟體的比對結果，如 Blast、FastA 的比對結果，轉換成 XML 的方式來儲存。利用
XML 的跨平台特性，增加生物資訊的流通性與ㄧ致性。 
  工欲善其事，必先利其器，有了這些利器，生物資訊的研發也就克服很多麻煩了，也許
還有很多技術上需要去克服，不過這都一定可以迎刃而解的，我們報告的主軸就是要幫生物
 3
在網站部份，我們為了網頁的ㄧ致性以及我們所開發的應用程式的相容性，全部的頁面
都是用 JSP 來完成，在登入使用者部份，利用了 JDBC 連結 mysql 做為檢測使者帳號密碼，
不僅可以讓管理更加方便，在將來更可以作進一步的運用與開發。另外，為了讓我們的網站
有更完整的安全性，我們運用了 session 的概念，session 物件表示目前個別使用者的 session
狀況，用此機制可以識別每一個使用者，讓他們一定要先行登入才允許使用我們所建構的資
源，如果 session 過期了，一定要重新登入，這樣做可以保證我們的網站資源不被別人所盜用。
我們網站主要的功能，就是可以讓使用者直接在瀏覽器上面，使用生物資訊的軟體，然後，
將結果回傳在網頁上。 
網站另一個附加功能則是提供了可以讓使用者查看機器的狀態，你可以從裡面得知我們
Grid 與 PCCluster 環境中機器的狀態，這我們叫做 monitor 部份，此處又增加了我們報告的便
利性！ 
PDA 主要的功能為將 Fasta、ClustalW、MpiBlast…等生物軟體的指令 GUI 化，利用遠方
的 Grid 或 Cluster 主機進行 DNA 序列的比對。附加功能可以對遠方主機下一些簡單的 Unix
指令、以及傳送檔案、並可開啟一些簡單的文件檔閱讀。 
XML (Extensible Markup Language─可延伸式標注語)，是公定、開放的標準，任何人都
可以拿來使用。XML 碼都是純文字檔。使用簡便，容易閱讀，方便資料庫存取。且其大多數
的解析器以 JAVA 寫成，具有 JVM (java virtual machine)的跨平台功能，因此有利於異質系統
間的資訊互通。轉換的方式是利用開源的生物資訊學基礎庫(Bio -java) 所提供的功能略加修
改而得。Bio-java 是一套為了開發複雜的序列分析的軟體運用 java 所開發。其中提供了一些
生物序列處理和文件格式轉和功能。我們將其標準輸出重導向，並簡化其標籤存放。 
3. 研究方法 
3.1 Java-Based 應用程式 
z 使用的語言:Java 
z 使用的 API:Globus Java Cog 
z 自行開發的 API(Grid 部份): 
A. GridJob(Submit Job to Grid System, include Application and JSP version) 
B. GridFTP(UpLoad DNA sequence to the Grid System, only implements it on Application.) 
C. SimpleCreateProxy(Cog Kit include this function GUI version on its API, only 
implements it on JSP) 
D. ProxyDestroy(Destroy the CA File) 
z 自行開發的 API(Cluster 部份): 
A. CommandClient(include upload, Submit job, lamboot, lamhalt on the Cluster System) 
B. JFileComboBox(給 PDA 使用來選擇上傳檔案用的 GUI 元件) 
 5
 
Bio 軟體應用(FASTA) 
當使用者者按下我們所打紅色圈圈的
地方則會跳出 JFileChooser 以便讓使
用者將要使用的 DNA sequence 上傳
至遠端的 Grid System，建議使用者要
使用時請先檢查你的 CA 是否過期。 
 
 
Bio 軟體應用(FASTA) 
當檔案上傳完畢之後你會發現剛剛空
的 JTextField 會出現使用者上傳檔案
的檔名，以及剛剛不能按的 Fasta-OK
按鈕已經可以使用了，建議使用者先
確 認 自 己 的 設 定 完 全 無 誤 （ 點
Selectivity Options 和 Scoring Options
可以做細部設定），選擇好要使用的應
用程式以及要搜尋的資料庫，然後就
可以準備按下 Fasta-OK 執行工作了。 
 
 
 
Bio 軟體應用(FASTA) 
Selectivity Options : 
Fasta 的細部設定，可以更改 ktup、
OPTCUT 等等的設定值。 
 7
 
Bio 軟體應用(FASTA) 
當工作結束後，程式會將所花費的時
間告知使用者，由上圖我們知我剛剛
做的工作花了 346 秒，這個秒數的計
算包括將工作送到主機上，等待主機
完成工作，將主機工作後的訊息收
回，根據 bio 軟體 output 大小可能會
影響應用程式的效率，若 bio 軟體
output 大於約 15mb，這支程式會將
output 存放在其目錄下的 temp.dat，
Java Swing GUI 套件處理大量字元時
若主機記憶體不夠容易發生 out of 
memory，為了避免這個問題發生以及
為了程式穩定性故採用此方法。 
 
Bio 軟體應用(ClustalW) 
上傳檔案跟 FASTA 是一樣的方法，由
上圖我們知道檔案已經上傳到主機端
了，接下來選擇看你所要建的樹格式
然後按下 ClustalW-OK，然後靜靜的
等待作業完成後做下一個該做的工作 
 
 
 
Bio 軟體應用(ClustalW) 
當工作完成後 clustalw 會將出來的結
果建成某一個檔案，且你也會發現 Get 
the result of the tree 這個按鈕能用了，
你需要去擔心是建成那個檔案嗎？答
案是不須要的，你不需要去了解到底
最後檔案叫啥名字（其實下方的 RSL 
Command 有透露出訊息），你只要安
心的按下 Get the result of the tree 把結
果收回來就對了 
 
 9
 
Bio 軟體應用(ping host 和 resource broker
for cluster) 
按下按鈕後出現的等待畫面，根據
Cluster 系統的大小和電腦速度可能等
待的時間會有所不同 
 
 
Bio 軟體應用(ping host 和 resource broker
for cluster) 
等到全部測試完畢後所出現的畫面。 
這是偵測完畢後程式所做的簡略報
告，其偵測的主機內容是根據所設定
的 lamhost file 來決定 
 
Note: 
程式是依據 cluster 主機上的 lamhost
這個檔去測試每一臺主機，如果某一
台主機不是活的，則這個程式上不會
出現死的主機的資訊(由左圖知道
dna05 是死的) 
 
 
Bio 軟體應用(ping host 和 resource broker
而 Settings 也含有 lamboot 和 lamhalt
功能，點下後執行畫面如左圖。 
這是 lamboot 成功的畫面，若有錯誤
則會彈出錯誤訊息。 
 
 11
 
Bio 軟體應用(Grid monitor) 
能夠做到 View,Update,Verify 
View 是查看遠端機器 machinefile 的
內容.。 
Update 是如果在 Client 端修改後看是
否要將之修改後的檔案上傳 
Verify 就是 Auto Detect,它可以根據
machinefile 上面的 server List 去回報
那一臺機器是否能用,能夠正確回報
CA 是否過期,機器有無開啟,CA 是否
是合法的。 
 
 
Bio 軟體應用(Grid monitor) 
若每臺電腦都正確且可以使用則會出
現這個畫面，如果有錯，會跳出錯誤
訊息。 
 
 
Bio 軟體應用(Grid monitor) 
點下去 view 後所看到的內容 
Update 也經過測試後沒有問題。 
 13
網頁介面分為上方、左方與右邊。 
 
上方為一 Flash 帳號登入部分，登入後
會出現程式功能 Bar。 
 
左邊以項目排程為導向。 
 
右邊中間則是內容的部分。 
成功登入之後右上方顯示歡迎訊息，並
彈出功能 Bar。 
 
功能 Bar 上有：Machine Monitor、Bio 
Application 與 Job Submission 三種操作
工具。 
 
 
點選 Machine Monitor 後會出現目前服
務中的主機狀況，可查看主機是否正常
連線(包含是否開機、是否安裝 CoG)。
正常者顯示綠色 OK 圓圈，異常者顯示
紅色禁止圓圈，並在 Error Log 中顯示其
異常原因以方便偵錯。 
 
下方另有 Test Other Machine 可讓 User
自行輸入 IP 位置查看其他機器狀況。 
 15
左圖為自行輸入一序列後，點 Save 
Sequence 彈出一視窗告訴 User 檔案已
經存到 /home/test/data/data.txt。 
左圖為自行選擇一序列檔後，點 Send
彈出一視窗告訴 User 檔案已經儲存。 
左圖為點選 Run Fasta 後彈出執行結果。 
 17
點選左邊 mpiBlast 進入 mpiBlast 介面。 
 
同 Fasta，User 可自行更改 mpiBlast 的
Location、選擇比對的 Database、以及
Application。 
 
預設比對的 DNA Database 為 Yeast.nt、
Application 為 blastn。 
點選 Job Submission 進入操作介面。 
 
由於 Job Submission 必須使用完整的
Globus RSL 指令才能進行操作，故介面
提供了各種 RSL 快速按扭和其參考連結
方便 User 使用。 
 
 
 19
 Number of CPU 可以設定工作處理時
的 CPU 數目，預設為 2 顆。         
IP of server 可以設定遠方主機的位
址，預設位址為 140.128.102.184 
Working mode 可以選擇處理的模式要
Grid 或 Cluster。 
 在 Input Comand 下指令按 OK 可以對
遠端主機下指令。 
在 Enter the passwd 打 proxy 密碼按
Create 可以 proxy inite。 
 
 Fasta︰ 
首先先選擇一個 DNA 序列上傳，再
選 擇 比 對 的 Database ， 以 及
Application，最後點 OK 即可命令遠
端主機比對 DNA 序列。 
Test鍵可以測試遠端主機的Fasta是否
可以正常運作，預設比對的 DNA 序
列為、Database 為 mp34comptfx、
Application 為 varsplic_sprot.fas。 
 
 21
4.4 XML 資料整合與轉換 
FastA 轉換前 
 
Fasta 比對後的結果，分為三部份，1.柱狀
圖 (Histogram)  
2.相似序列表  
3.序列並列分析 
並以文字檔儲存。 
FastA 轉換後 
 
轉換成 XML 格式之後，用 tag 表示 
比對的各種資訊，我們也可以多加入屬於
自己 tag，讓我們可以更方便的辨識。最主
要的就是增加資料的交換性，與可讀性。
 
 23
CPU Pentium III-750 Pentium III-750 
RAM 512MB x2 512MB 
Hard Disk 80 GB x1 , 60GB x1 80GB 
Network Interface Card 1000M/bit 1000M/bit 
Switch HUB 1000M/bit  
 
beta1 gamma1 bear 
140.128.101.186 140.128.102.186 140.128.102.180 
beta1.hpc.csie.thu.edu.tw gamma1.hpc.csie.thu.edu.tw bear.hpc.csie.thu.edu.tw 
Pentium III-750 Pentium III-750 Pentium III-750 
512MB x2 512MB 512MB x2 
80 GB   80GB 80 GB  
1000M/bit 1000M/bit 1000M/bit 
1000M/bit  1000M/bit 
 
4.3 軟體總架構 
系統平台 生物軟體 網頁平台 
Red Hat Linux 9.0 hmmer-2.3.2.bin.intel-linux.tar jakarta-tomcat-5.0.25 
PC Cluster (NFS +NIS) fasta34t24b1.shar.Z JSP 
PVM 3.4.3 clustalw-mpi-0.13.tar.gz MySQL 5.0.0 
MPI tree-puzzle-5.1.tar.gz 
 fastDNAml_1.2.2p.tar   
PDA XML 轉換與整合 
research Pocket PC 1.5 BioJava 1.4pre1 
dbXML2_0bin.bin 
XML-DBMS, Java version 1.01  
 
 
4.4 硬體總架構圖 
 25
 
 
5. 結果與討論 
5.1  PC Cluster 
 
 
 
 
 
 
 
PCCluster
0
50
100
150
200
250
2 4 8 16 32 64
# of Processor
E
xe
cu
tio
n 
Ti
m
e 
(s
ec
)
mpiBLAST
FASTA
CLUTALW_MPI
TREE-PUZZLE
 27
mpiBLAST on Grid
0
5
10
15
20
25
30
2 4 8
CPU
時
間
(s
ec
)
第一次
第二次
第三次
 
5.3 Application 與直接在 Grid 上測試之效能比較 
Grid
0
50
100
150
200
250
2 4 8
# of processor
E
xe
cu
tio
n 
tim
e 
(s
ec
)
FASTA on Grid
FASTA through AP
mpiBLAST on Grid
mpiBLAST through
AP
   
我們利用兩個生物資訊的軟體，實際測試效能，很明顯的 CPU 數目增加會節省執行的時間，
所以我們架設的機器能夠有效的處理這龐大的資料量，並減少花費時間。 
 
6. 分析與比較 
      我們使用的平台的各種比較： 
平台  方便性 實用度 執行效率 
Application 中 高 高 
PDA 中 中 低 
JSP 網頁 高 高 高 
 
 29
 
8. Reference 
[1] Oswaldo Trelles. On the Parallelization of Bioinformatic Applications, Bioinformatics, Vol. 2, 
2001. 
[2] PARACEL BLAST-Accelerated BLAST software optimized for Linux clusters. 
[3] Kuo-Bin Li, ClustalW-MPI: ClustalW Analysis Using Distributed and Parallel Computing, 
Bioinformatics, Vol. 19, No. 12, 2003, pp. 1585-1586. 
[4] Karsten Hokamp, Denis C. Shields, Kenneth H. Wolfe and Daniel R. Caffrey. Wrapping up 
BLAST and other applications for use on Unix clusters, Bioinformatics, Vol. 19, No. 3, 2003, 
pp. 441-442. 
[5] Heiko A. Schmidt, Korbinian Strimmer, Martin Vingron and Arndt von Haeseler, 
TREE-PUZZLE: maximum likelihood phylogenetic analysis using quartets and parallel 
computing, Bioinformatics, Vol. 18, No. 3, 2002, pp. 502-504. 
[6] Raymond K. Wong and William Shui: Utilizing Multiple Bioinformatics Information Sources: 
An XML Database Approach. BIBE 2001: 73-80. 
[7] http://www.biojava.org , BioJava   
[8] http://www.globus.org/, The Globus Project. 
[9] http://www.epm.ornl.gov/pvm/;PVM (Parallel Virtual Machine) 
 
[10] http://www.lam-mpi.org/; MPI (Message-Passing Interface) 
[11] ftp://ftp.virginia.edu/pub/fasta/ FASTA 
[12] ftp://ftp.ebi.ac.uk/pub/software/unix/clustalw/ ;ClustalW 
[13] http://mpiblast.lanl.gov/; mpiBLAST 
[14] http://www.tree-puzzle.de; Tree Puzzle 
 
 
 
 
 
 
 
 
 
 
 
 
 
 31
    
   接下來指定 usercert 和 userkey，確定無誤後按下 Next 
 
這個 CA 檔案一般都是放在遠端 Grid 機器/etc/grid-security/certificates 上，根據系統
的不同其檔名可能不一，將整個資料夾的檔案抓回來測試即可，CA 檔一般大概是長
這樣“ffa40f5d.0＂，接著按下 Next 即設定好 CA 部份 
 
4. 接著設定 Windows 所信任的機器： 
修改 c:\windows\system32\driver\etc\hosts 檔，修改方式如下： 
 
將其所信任的機器加到這個檔案裡面，如紅色地區所示，設定好即完成 Java Cog Kit
設定 
 
Note: Java Cog Kit 無法在使用 Dynamic IP 上的機器運作，如 ADSL 撥接，NAT，電
話撥接等 
 
 
 33
StrongARM 處理器，所以選擇 pjavawince-1_1-beta1-arm.zip。 
 
    請至 http://www.javatwo.net/experts/moli/book/下載所需的版本。 
 
    壓縮檔內容如下︰ 
 
開啟 ActiveSync，瀏覽 PocketPC 的 \Windows\「開始」 
 
將 pjavawince.arm.CAB 複製到該目錄。 
 
 
      
完成後執行 pjavawince.arm.CAB 進行安裝。 
 35
 
的 “\Program Files\Java\bin\pjava.exe” –file “%1” 
改成“\Program Files\Java\bin\pjava.exe” -classpath “\lib\swingall_fix.jar” –file “%1” 
 
至此準備工作就完成，接下來只需將 BioCE 資料夾（包括 BioCE.class、BioCE$1.class ~ 
BioCE$8.class、CommandClient.class、JFileComboBox.class、fasta.dat、clustalw.dat、mpiblast.dat、
DNA 資料夾）複製到 PocketPC 上，直接點選 BioCE.class 即可啟動。 
操作說明 
                           基本設定： 
 
           Number of CPU 可以設定工作處理時的 
                            CPU 數目，預設為 2 顆。                           
 
IP of server 可以設定遠方主機的位址，預設位址
140.128.102.184。 
 
Working mode 可以選擇處理的模式要 Grid 或 Cluster。  
 
 
 37
              Flesh 為更新目錄底下的檔案。 
 
Save File: 
 
按下 SAVE 會出現 Save File 對話框，輸入檔案名稱，將
會把檔案儲存在 DNA 目錄下。 
 
 
                            Fasta: 
 
首先先選擇一個 DNA 序列上傳，再選擇比對的
Database，以及 Application，最後點 OK 即可命令遠端主機比
對 DNA 序列。 
 
Test 鍵可以測試遠端主機的 Fasta 是否可以正常運作，預
設比對的 DNA 序列為、Database 為 mp34comptfx、Application
為 varsplic_sprot.fas。 
 
 
ClustalW: 
 
跟 Fasta 一樣，先選擇一個 DNA 序列上傳，再選擇比對
時輸出的 tree 方式，再點 OK 會開始比對，並顯示比對過程。 
 
             如果要得到 Tree，請點 Tree，會傳回 Tree。 
 
 
 
 
MpiBlast: 
 
跟 Fasta 一樣，先選擇一個 DNA 序列上傳，再選擇比對
的 Database，以及 Program，再點 OK 會開始比對，並顯示比
對過程。 
 
如果要得到比對結果，請點 Job 會傳回比對結果。 
 
 
 
 39
10. 生物資訊軟體安裝說明 
z mpiBLAST安裝 
1. mpiBlast使用上利用到NCBI的函式庫，所以得裝上NCBI的Tools才能使用 
下載點:ftp://ftp.ncbi.nih.gov/toolbox/ncbi_tools/CURRENT/ (可找到最新版) 
安裝NCBI tool  ./ncbi/make/makedis.csh 
2. 繼續下載mpiblast下載點 http://mpiblast.lanl.gov/releases/mpiBLAST-1.2.1.tar.gz 
./configure --prefix=/path/to/mpiBLAST-1.2.1 
--with-ncbi=/path/to/ncbi 
--prefix=:mpiblast所在目錄 
,--with-ncbi=:Ncbi toolbox所在目錄 
3. make 
4. as root account 
make install 
5. 然後在/etc/下作個mpiblast.conf設定檔，內容如下： 
/blast/db/share/ (要share的資料夾) 
/blast/db/yeast_db/ (本機資料夾) 
6. 在來在mpi使用者的目錄下，建立.ncbirc設定檔，內容如下： 
[NCBI] 
Data=/usr/local/ncbi/data (會用到的評分矩陣) 
7. 可到ftp://ftp.ncbi.nih.gov/blast/db/FASTA/抓取想要的DB 
接下來format抓下來的db，以yeast.nt為例 
mpiformatdb -f /etc/mpiblast.conf -N 12 -i yeast.nt -o T -p F 
-N 為想切的fragment數 
-i 資料庫名字 
-o Parse options 
T – True: Parse SeqId and create indexes. 
F – False: Do not parse SeqId. Do not create indexes. 
-p Type of file  
    T – protein  
 F – nucleotide 
8. Query database 
time mpirun -np 2 ./mpiblast --config-file= /mpiblast.conf -p 
blastn -d yeast.nt -i test.fas -o results.txt 
 
z FASTA 
1. 下載點ftp://ftp.virginia.edu/pub/fasta/fasta3.shar.Z(可得到最新版本) 
 41
11. 附錄 (Appendix) 
11.1 附錄 A：Cluster 叢集系統安裝手冊 
[1] 叢集系統的架設： 
z 設定網路卡 
/etc/sysconfig/network 
NETWORKING=yes 
HOSTNAMME=dual1 
NISDOMAIN=dual1 
 
/etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=eth0 
ONBOOT=yes 
BOOTPROTO=static 
IPADDR=140.128.102.196 
NETMASK=255.255.255.0 
 
我們架設的 CLUSTER 只有一張網路卡，並沒有連外。 
預先設定 /etc/hosts 與 /etc/hosts.equiv 檔案，以利後面的安裝步驟： 
− /etc/hosts：IP和主機名稱對照表 
 
 
 
 43
 
authconfig 執行畫面 ( Server Side ) 
3. 用 ntsysv 指令設定 ypserv 服務，使得重新開機後，ypserv 服務能自動啟動。 
 
4. 新增帳號 
− adduser hpc1234：新增 hpc1234 帳號 
− password ckitcc：修改 hpc1234 帳號的密碼 
− cd /var/yp 
− make：更新帳戶資訊 
 
 45
 
[3] Setup NFS 
z Configure Server side 
1. 設定 /etc/exports 
 
2. 設定完成後，需重新啟動 NFS 服務 
 
3. 用 ntsysv 指令設定 nfs 服務，使得重新開機後，nfs 服務能自動啟動(如下圖) 
 47
 
[root@cluster1 root]# mount –a 
[root@cluster2 root]# mount 
/dev/hda1 on / type ext3 (rw) 
none on /proc type proc (rw) 
usbdevfs on /proc/bus/usb type usbdevfs (rw) 
none on /dev/pts type devpts (rw,gid=5,mode=620) 
none on /dev/shm type tmpfs (rw) 
cluster1:/home on /home type nfs (rw,addr=163.23.43.241) 
cluster1:/usr/local on /usr/local type nfs (rw,addr=163.23.43.241) 
[root@cluster2 root]# 
[4] Setup RSH 
z Configure Client side 
1. 
− 先檢查 /etc/hosts.equiv 有沒有加入 cluster1 ( Server Side ) 
 
[root@cluster2 root]# cat /etc/hosts.equiv 
cluster1 
cluster2 
 
2. 用 ntsysv 指令設定 rsh 服務，使得重新開機後，rsh 服務能自動啟動 
 49
1. 設定環境變數 export PVM_ROOT=/usr/share/pvm3   
在/etc/bashrc裡面設定(Server Client 皆設定) 
 
2. 測試 pvm 環境 
− su abc123：切換成其他 user 帳號 
★：要先切換成其他 user 帳號，不能用 root 帳號測試 
− 在 cshost 檔案中設定所有叢集主機名稱 
− pvm cshost：進入 pvm 環境，並載入叢集主機設定檔 
− conf：顯示叢集主機資訊 
− halt：離開 pvm 環境 
 
 
 
[6] Setup MPI 
 51
-rw-r--r--    1 chungyi  chungyi       232  6月 16 05:27 known_hosts 
 
[chungyi@cluster1 .ssh]$ ssh-keygen -t rsa 
Generating public/private rsa key pair. 
Enter file in which to save the key (/home/chungyi/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/chungyi/.ssh/id_rsa. 
Your public key has been saved in /home/chungyi/.ssh/id_rsa.pub. 
The key fingerprint is: 
ac:eb:02:5a:aa:7c:fd:f9:f9:31:0b:51:2a:02:8d:6f chungyi@cluster1 
[chungyi@cluster1 .ssh]$ cp id_rsa.pub authorized_keys2 
[chungyi@cluster1 .ssh]$ cd 
[chungyi@cluster1 chungyi]$ 
 
 
 
 
5. 測試 LAM 是否可以啟用 
− 如果看到第五行的 “Woo hoo!”，表示所有叢集主機可以正常使用 
 
 53
11.2 附錄 B：Grid 系統安裝手冊 
1. Introduction 
GT3 is an implementation of the Open Grid Services Infrastructure (OGSI) 
version 1.0. Globus is using OGSI as their infrastructure for their GT3 base 
services. They also add some management services. OGSI represents the reference 
open source standard implementation of the Open Grid Services Infrastructure 
standard. 
The following Figure shows the key areas identified as a basis for Grid 
computing. Each pillar would be included in most of Grid implementations. 
These key areas are: Resource management, Information services, and Data 
management. 
 
 
2. Account Requirement 
z globus or root account 
 Toolkit environment 
 For installation and execution of Toolkit. 
z Any other user account 
 End user environment. 
 For jobs execution on the Grid. 
3. Software 
z JAVA SDK 
http://java.sun.com 
http://java.sun.com/j2se/1.4.2/download.html 
(j2sdk-1_4_2_03-linux-i586-rpm.bin) 
z Apache Ant 
http://ant.apache.orgs 
http://ant.apache.org/bindownload.cgi 
 55
 
 
4.3. Install Apache Ant & JUnit 
4.3.1. Use root account 
4.3.2. Uncompress Apache Ant 
¾ tar -xzvf apache-ant-1.6.0-bin.tar.gz 
4.3.3. Unzip JUnit and copy junit.jar to the lib directory of your Apache Ant 
installation directory 
¾ unzip junit3.8.1.zip 
¾ cp junit3.8.1/junit.jar apache-ant-1.6.0/lib 
4.4. Setup environment variable 
4.4.1. Use root account 
4.4.2. Add Java and Ant install directory to system path 
¾ vi /etc/profile 
 …(omits)… 
 if [ -z "$INPUTRC" -a ! -f "$HOME/.inputrc" ]; then 
 INPUTRC=/etc/inputrc 
 fi 
9 # JAVA PATH 
9 JAVA_HOME=/usr/java/j2sdk1.4.2_03 
9 PATH=$PATH:$JAVA_HOME/bin 
9 export JAVA_HOME 
9 # ANT PATH 
9 ANT_HOME=/usr/local/apache-ant-1.6.0 
9 PATH=$PATH:$ANT_HOME/bin 
9 export ANT_HOME 
 export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE 
INPUTRC 
 57
4.6. Setup environment variable 
4.6.1. Use root account 
4.6.2. Add Globus install directory and set globus environment variable 
¾ vi /etc/profile 
 fi 
 # JAVA PATH 
 JAVA_HOME="/usr/java/j2sdk1.4.2_04" 
 PATH="$PATH:$JAVA_HOME/bin" 
 export JAVA_HOME 
  
 # ANT PATH 
 ANT_HOME="/grid_software/ant/apache-ant-1.6.1" 
 PATH="$PATH:$ANT_HOME/bin" 
 export ANT_HOME 
  
9 #GLOBUS PATH 
9 GLOBUS_LOCATION="/usr/local/globus" 
9 PATH="$PATH:$GLOBUS_LOCATION/bin" 
9 export GLOBUS_LOCATION 
  
 export PATH USER LOGNAME MAIL HOSTNAME HISTSIZE 
INPUTRC 
  
9 . $GLOBUS_LOCATION/etc/globus-user-env.sh 
 59
You have to send “hostcert_request.pem” file to the CA server to require a 
certificate for your machine, then Server will send back the 
“hostcert.pem” file. Finally you must replace this file with your original 
empty file. 
4.7.6. Setup user certificate 
4.7.6.1. Use you own user account. 
¾ grid-cert-request 
 user/.globus/userkey.pem 
 user/.globus/usercert_request.pem 
 user/.globus/usercert.pem (zero size) 
This user certification step is almost same as host, first send the 
“usercert_request.pem” to CA server, then take “usercert.pem” back from 
CA server. Last step is replacing this file with your original empty file. 
4.7.7. Test CA setup 
4.7.7.1. We can test if CA is setting correctly by use grid-proxy-init command, this 
command is used to initiate a proxy to enable access to other machine. 
Correct result will show below. 
¾ grid-proxy-init 
 
4.8. Setup grid-mapfile 
4.8.1. This step will add all accessible certificate users to our machine, so 
anyone you want to let them running job on you computer must be added. 
¾ Vi /etc/grid-security/grid-mapfile 
 
4.9. Setup gatekeeper 
4.9.1. In order to let the jog to run and communication to each other, we have to 
setup the port that allow job to pass. 
¾ vi /etc/services 
Add this line “gsigatekeeper  2119/tcp  # Globus 
Gatekeeper” to /etc/services. 
 61
 
4.11. Test to run a simple job 
4.11.1. This is a very exciting step of all things you done above. If this work, 
then I have to say “Congratulations!!”. But if not, you would better to 
check all step above and ensure that nothing you forget to setup or make 
any mistake. One last thing I must remind you is that you must run 
“grid-proxy-init” before this step. 
4.11.2. Test local site 
¾ globus-job-run <local ip> /bin/date 
4.11.3. Test remote site 
¾ globus-job-run <remote ip> /bin/date 
 
 
5. Install and setup mpich-g2 
5.1 Use root account 
5.2 Uncompress mpich-g2 
¾ tar –zxvf mpich.tar.gz 
5.3 Configuration 
5.3.1 You have to change <install directory> to the directory you want to 
install. In our case it is “/usr/local/mpich-1.2.5” 
¾ ./configure --prefix=<install directory> 
--with-device=globus2:-flavor=gcc32dbg 
5.4 Make install 
5.4.1 Make all the configuration 
¾ make 
5.4.2 Install 
¾ make install 
 
6. Run mpi program 
6.1 Compiler a mpi program 
6.1.1 First you have to choice a mpi program, you can choice one at “<mpi 
計畫成果自評部份：  
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值、是否適合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜
合評估。 
 
本計畫為生物網格(Bio-Grid)發展計畫，由於現在為後基因體(Post Genome 
era)時代，不管是生物相關的資料(Bio-Data)或者是序列每天都呈現指數的成長，
而對於序列比對所需要的計算資源更是非常的龐大，也因此本計畫想利用台灣甚
至是世界各地的個人電腦(PCs)、大型主機(Mainframe)或是叢集電腦(PC Clusters)
等資源建構一生物網格(Bio-Grid)的系統，利用此系統創造出龐大的運算資源提
供序列比對(Sequence Alignment)以及大量的儲存空間儲存生物資料。而發展生物
網格(Bio-Grid)技術就產業界來講，提供了全新的思維，因為在公司方面，可以
利用生物網格技術去發展生物科技，而不再是一定要購置大型電腦主機甚至是超
級電腦(SuperComputer)，而可以選擇低廉的成本來建構與發展。而對於國家發展
方面，可以利用生物網格的技術，將國內各地所得到的研究資料加以共用、分享
與交流，分享各個專門研究所得到的成果，並且可以藉此加快與國際接軌的腳
步，甚是領先世界各國成為生物網格(Bio-Grid)發展的先驅，進而帶動國內與國
外的大小生物科技產業的蓬勃發展。 
由於此計畫為生物網格之計畫，因此每位參與的人員必須深入了解網格的功
能與意義，當然架設網格系統是第一個需要獲得的基本訓練。在生物資訊領域，
最重要的就是生物資料(Bio-Data)，所以每位參與的人員必須深入了解怎麼儲存
與維護這些生物資料，當然前提是儲存在網格架構下的平行資料庫中，因此當然
要訓練每個人對於生物資料庫有最基本的認知。 
計畫成果自評 
由於此計畫為三年之生物網格計畫，因此執行年限將分成三年陸續完成。 
第一年： 
在實驗室中，架設叢集電腦系統，使之能執行所謂的平行計算程式，並利用
網格技術，使多組的叢集電腦可以透過網際網路互相通訊並且在此網格架構上執
行平行運算。再者，當電腦設備利用網格技術架設成功之後，陸續加入數台或者
更多台個人電腦加入網格環境的行列之中，甚至是大型主機也予以考慮加入，如
此本計畫的生物網格架構會在第一年有一個最基本的雛型出現。 
 
第二年： 
針對平行資料庫的發展與應用加以熟悉，並將其應用在各個網格節點之上，
進而在本生物網格環境之下可以正確的運作。待一切安裝完成便開始設計如何整
合各種不同架構的生物資料庫以及如何利用此網格架構分享各個實驗室所擁有
的研究結果與資源。並以此網格架構，妥善利用所有節點的計算資源，以加速運
算的速度。並要設立一個自動更新資料庫的機制，使得本資料網格儲存平台的生
 62
 
相關發表論文 
[1] Chao-Tung Yang, I-Hsien Yang, Shih-Yu Wang, Ching-Hsien Hsu and Kuan-Ching Li, “A 
Recursively-Adjusting Co-Allocation Scheme with Cyber-Transformer in Data Grids,” 
Future Generation Computer Systems, Elsevier B.V., vol. x, no. x, pp. xx-xx, 2008. (ISSN 
0167-739X, SCI JCR IF=0.722, EI) 
[2] Chao-Tung Yang, Po-Chi Shih, Cheng-Fang Lin and Sung-Yi Chen, “A Resource Broker 
with an Efficient Network Information Model on Grid Environments,” The Journal of 
Supercomputing, Springer Netherlands, vol. 40, no. 3, pp. 249-267, June 2007. (ISSN 
1573-0484, SCI JCR IF=0.398, EI) 
[3] Chao-Tung Yang, I-Hsien Yang, Kuan-Ching Li and Shih-Yu Wang, “Improvements on 
Dynamic Adjustment Mechanism in Co-Allocation Data Grid Environments,” The Journal 
of Supercomputing, Springer Netherlands, vol. 40, no. 3, pp. 269-280, June 2007. (ISSN 
1573-0484, SCI JCR IF=0.398, EI) 
[4] Chao-Tung Yang, Po-Chi Shih and Sung-Yi Chen, “A Domain-Based Model for Efficient 
Measurement of Network Information on Grid Computing Environments,” IEICE 
Transactions on Information and Systems, Special Issue on Parallel/Distributed 
Computing and Networking, Oxford University Press, vol. E89-D, no. 2, pp. 738-742, 
February, 2006. (ISSN 1745-1361, SCI JCR IF=0.242, EI) 
[5] Chao-Tung Yang, Yu-Lun Kuo and Chuan-Lin Lai, “Designing Computing Platform for 
BioGrid,” International Journal of Computer Applications in Technology (IJCAT), Special 
Issue on “Applications for High Performance Systems”, vol. 22, no. 1, pp. 3-13, 
Inderscience Publishers, UK, January 2005. (ISSN 1741-5047, EI, 
NSC92-2213-E-029-025) 
 
4 C-T. YANG, Y-L. KUO AND C-L. LAI 
 
1 INTRODUCTION 
Grid computing, most simply stated, is distributed 
computing taken to the next evolutionary level. The goal is 
to create the illusion of a simple yet large and powerful  
self-managing virtual computer out of a large collection of 
connected heterogeneous systems sharing various 
combinations of resources. The standardisation of 
communications between heterogeneous systems created the 
internet explosion. The emerging standardisation for sharing 
resources, along with the availability of higher bandwidth, 
are driving a possibly equally large evolutionary step in grid 
computing (Global Grid Forum, 2004; Foster and 
Kesselman, 1999; Foster, 2002; SETI@home, 2004). 
The infrastructure of grid is a form of networking. Unlike 
conventional networks that focus on communication among 
devices, grid computing harnesses unused processing cycles 
of all computers in a network for solving problems too 
intensive for any stand-alone machine. A well-known grid 
computing project is the SETI (search for extraterrestrial 
intelligence) @home project (2004), in which PC users 
worldwide donate unused processor cycles to help the 
search for signs of extraterrestrial life by analysing signals 
coming from outer space. The project relies on individual 
users to volunteer to allow the project to harness the unused 
processing power of the user’s computer. This method saves 
the project both money and resources. 
Another key technology in the development of grid 
networks is the set of middleware applications that allows 
resources to communicate across organisations using a wide 
variety of hardware and operating systems. The globus 
toolkit (The Globus Project, 2004) is a set of tools useful for 
building a grid. Its strength is a good security model, with a 
provision for hierarchically collecting data about the grid, as 
well as the basic facilities for implementing a simple, yet 
world-spanning grid. 
There are some existing parallel version bioinformatics 
applications which can be installed and conducted on a PC 
cluster (Trelles, 2001). Some examples are, HMMer (2003), 
FASTA (2004), mpiBLAST (2004), ClustalW-MPI  
(Li, 2003), Wrapping up BLAST (Hokamp et al., 2003), and 
TREE-PUZZLE Schmidt et al. (2002) using these parallel 
programmes to work sequence alignment always save much 
time and cost. Especially for some companies, the PC 
clusters can be used to replace mainframe systems or 
supercomputers and save much hardware cost. In regards to 
efficiency and cost, use of parallel version software and 
cluster systems is an excellent means and will become more 
and more popular in the near future. As we know, 
bioinformatics tools can speed up analysis of large-scale 
sequence data, especially in regards to sequence alignment. 
Therefore, we used the FASTA bioinformatics application 
in the grid system for our TestBed. 
The organisation of this paper is as follows: In Section 2, 
we make a background review of bioinformatics, cluster 
computing, grid computing, and BioGrid. In Section 3, we 
state the FASTA software. Section 4, explains our hardware 
and software configuration. In Section 5, we show some of 
the user interface. In Section 6, grid computing environment 
is proposed and constructed on multiple Linux PC Clusters 
by using globus toolkit (GT) and SUN grid engine (SGE). 
The experimental results are also conducted by using 
FASTA software to demonstrate the performance achieved. 
The experimental results are presented and discussed. 
2 BACKGROUND REVIEW 
2.1 Bioinformatics 
Bioinformatics is the marriage of biology and information 
technology. The discipline encompasses any computational 
tools and methods used to manage, analyse and manipulate 
large sets of biological data. Essentially, bioinformatics has 
three components: 
• the creation of database allowing the storage and 
management of a large biological dataset 
• the development of algorithms and statistics to 
determine relationships among members of large 
dataset 
• the use of these tools for the analysis and interpretation 
of various types of biological data, including DNA, 
RNA, and protein sequences, protein structures, gene 
expression profiles and biochemical pathways. 
2.2 Cluster computing 
A Beowulf cluster is a parallel computer system. It suits 
applications that can be partitioned into tasks, which can 
then be executed concurrently by a number of processors. 
The previous study lists four benefits that can be achieved 
with clustering. These can also be thought of as objectives 
or design requirements: 
• Absolute scalability. It is possible to create large 
clusters that far surpass the power of even the largest 
standalone machines. A cluster can have dozens of 
machines, each of which is a multiprocessor. 
• Incremental scalability. A cluster is configured in such 
a way that it is possible to add new systems to the 
cluster in small increments. Thus, a user can start out 
with a modest system and expand it as needs grow, 
without having to go through a major upgrade in which 
an existing small system is replaced with a larger system. 
• High availability. Because each node in a cluster is a 
standalone computer, the failure of one node does not 
mean loss of service. In many products, fault tolerance 
is handled automatically in software. 
• Superior price/performance. By using commodity 
building blocks, it is possible to put together a cluster 
with equal or greater computing power than a single 
large machine, at much lower cost. 
2.3 Grid computing 
Grid computing (or the use of a computational grid) is 
applying the resources of many computers in a network to a 
6 C-T. YANG, Y-L. KUO AND C-L. LAI 
Indiana University BioGrid, Minnesota University  
(Dwan et al., 2001) and Singapore BioGrid (Hokamp  
et al., 2003) to mention a few. 
The EuroGrid aim of Grid opens new perspectives for 
bioinformatics. The integration of a platform dedicated to 
biology into GRID opens up new perspectives in terms of 
computing resources and data storage. Many genomes have 
been sequences and their annotation requires larger  
and larger databases. The storage and the exploitation of 
these genomes and of the huge flux of data coming from 
post-genomics put a quickly growing pressure on the 
computing tools and resources in the laboratories. 
The UK BioGrid is called MyGrid. MyGrid is an  
e-Science grid project that aims to help biologists and 
bioinformaticians to perform workflow-based in silico 
experiments, and help to automate the management of such 
workflows through personalisation, notification of change 
and publication of experiments (Moreau et al., 2003). 
The NC grid is providing the computing data storage, and 
networking capabilities to support the genomics revolution; 
members of the North Carolina Genomics and 
Bioinformatics Consortium are working with computer and 
networking companies to create the North Carolina 
Bioinformatics Grid. An information network will be built 
at Indiana University for large data and computationally 
intensive applications in several sciences, using advanced 
data grid technologies. With national and international 
collaborations in physics, bioinformatics, geology, and 
computer science, this will provide scientists access to local 
and globally distributed computing resources. 
The Singapore BioGrid is applying Clustal-G on grid 
system to implement sequence alignment. These worldwide 
projects all use grid technology to implement 
bioinformatics. And the greater majority wants application 
of the grid system to save much time in solving 
bioinformatics problems. Therefore BioGrid technology is 
the most popular and effective means of solving biology 
data problems. 
3 PARALLEL BIOINFORMATICS APPLICATIONS 
3.1 FASTA 
The popular tool for searching sequence databases is a 
programme called FASTA. FASTA compares two 
sequences by trying to align them, and is also used to 
lookup sequences in a database. FASTA provides very fast 
searches of sequence databases. 
FASTA distribution contains search programmes that are 
analogous to the main BLAST modes, with the exception of 
PHI-BLAST and PSI-BLAST, as well as programmes for 
global and local pair-wise alignment and other useful 
functions. The FASTA programmes listed here all compile 
easily on a Linux system: 
 
 
 
• fasta: compares a protein sequence against a protein 
database or a DNA sequence against a DNA database 
using the FASTA algorithm 
• ssearch: compares a protein sequence against a protein 
database or DNA sequence against a DNA database 
using the Smith-Waterman algorithm 
• fastx/fasty: compares a DNA sequence against a protein 
database, performing translations on the DNA sequence 
• tfastx/tfasty: compares a protein sequence against a 
DNA database, performing translations on the DNA 
sequence database 
• align: computes the global alignment between two 
DNA or protein sequences 
• lalign: computes the local alignment between two DNA 
or protein sequences. 
The FASTA package contains many programmes, and they 
are inconveniently named after both the version number of 
the package and the parallel programming library that was 
used to build them. Nicknames are provided for most 
programmes in Table1. 
Table 1   FASTA function 
Nickname(s) Binary 
fasta mp34compfa 
ssearch mp34compsw 
fastx mp34compfx 
fasty mp34compfy 
tfastx mp34comptfx 
tfasty mp34comptfy 
4 OUR SYSTEM ENVIRONMENT 
4.1 Hardware and software configuration 
The test environment is described in Tables 2 and 3.  
We build three clusters to form a multiple cluster 
environment. These clusters are being set at our laboratory 
now. Each master node is running SGE Master daemon and 
SGE execute daemon to run, manage and monitor incoming 
jobs and globus toolkit v2.4. Each slave node is running 
SGE execute daemon to execute incoming jobs only. 
Table 2   Software configuration 
OS and software configuration 
Linux Redhat 9.0 
Globus toolkit (GT 2.4) 
SGE (Sun Grid Engine 5.3) 
MPICH and MPICH-G2 
FASTA (MPI version for parallel computing) 
 
 
 
 
 
 
 
 
 
8 C-T. YANG, Y-L. KUO AND C-L. LAI 
 
Figure 3   The system status of BioGrid 
 
Figure 4   The summary page of BioGrid 
10 C-T. YANG, Y-L. KUO AND C-L. LAI 
 
Figure 7   Init the proxy 
 
Figure 8   Select query sequence and database 
12 C-T. YANG, Y-L. KUO AND C-L. LAI 
 
Figure 11   The sequence query result message 
6 EXPERIMENTAL RESULTS 
In this paper, we used FASTA applicable program in the 
BioGrid environment. The program we used does support 
the version of parallel computing, that is to say, a program 
for execution in the parallel PC cluster and grid-computing 
environment. In order to make the software execute 
flawlessly in the grid structure; we must edit it using 
MPICH-G2 for successful operation. In the test, we have 
used two databases, namely tremblenew.fas and trembl.fas. 
Database tremblenew.fas comes in the size of 23 MB and 
the other one at 153 MB. The two are used for mapping, the 
results of the analyses are given in Tables 4 and 5. 
Laboratory figures suggest that by gradually increasing 
processors, the time consumed drops significantly.  
As tremblenew.fas is smaller, the time consumed does not 
change once a specific number of processors are  
reached. When the larger database is used for test, we would 
realise time consumed does go down remarkably while  
 
more processors are added. Therefore, the larger and more 
complex the database becomes, the better the BioGrid 
environment performs. In the test, we have used np3  
(2 CPU work), np5 (4 CPU work), np7 (6 CPU work), np9 
(8 CPU work), npl 1 (10 CPU work) and npl3 (12 CPU 
work) for the order mapping. The performance analyses are 
given in Figures 12 and 13. 
Table 4   The execution time for the trembl_new.fas 
database 
No. # of processors 2 4 6 8 10 12 
Time (sec) 101 93 86 83 81 75 
Table 5   The execution time about the trembl.fas database 
No. # of processors 2 4 6 8 10 12 
Time (sec) 277 190 158 156 152 146 
 
 
Figure 12   The execution time of FASTA using processors from 2 to 12 (trembl_new.fas) 
G. Min et al. (Eds.): ISPA 2006 Ws, LNCS 4331, pp. 687 – 696, 2006. 
© Springer-Verlag Berlin Heidelberg 2006 
On Integration of GUI and Portal of Cluster and Grid 
Computing Platforms for Parallel Bioinformatics* 
Chao-Tung Yang1,**, Tsu-Fen Han1, Heng-Chuan Kan2, and William C. Chu3 
1 High Performance Computing Laboratory 
1,3 Department of Computer Science and Information Engineering 
Tunghai University, Taichung City 40704, Taiwan 
ctyang@thu.edu.tw, g942814@thu.edu.tw, chu@csie.thu.edu.tw 
2 Biotechnology Group, Southern Business Unit 
National Center for High-performance Computing 
Hsinshi, Tainan County 74147, Taiwan 
n00hck00@nchc.org.tw 
Abstract. In this paper, we implement an experimental distributed computing 
application for parallel bioinformatics. The system consists of the basic cluster 
and grid computing environment and user portal to provide a useful graphical 
interface for biologists who are not specialized in Information Technology (IT) 
to be able to easily take advantages of using high-performance computing re-
sources. Finally, we perform several experimentations to demonstrate that 
cluster and grid computing platform indeed reduces the execution time of the 
biology problem. 
1   Introduction 
Bioinformatics is the combination of biology and information technology. These 
include any computational tools and methods to manage, analyze and manipulate 
large sets of biology data. Thus, computing technologies are vital for bioinformatics 
applications. Biology problems often need to repeat the same task for millions of 
times such as searching sequence similarity over the existed databases or comparing a 
group of sequences to determine evolutionary relationship. In such cases, the high 
performance computers to process this information are indispensable. Biological 
information is stored on many different computers around the world. The easiest way 
to assess this information is to connect these computers together through the network. 
This approach requires high-performance computing infrastructures with access to 
huge databases of information. 
Many advances in computing technology and computer science over the past 30 
years have dramatically changed much of our society. The computing technologies 
today represent promising future possibilities. Currently, it is still very difficult for 
                                                          
*
 This work is supported in part by National Science Council, Taiwan R.O.C., under grants 
no. NSC95-2221-E-029-004 and NSC95-2218-E-007-025. 
**
  Corresponding author. 
 On Integration of GUI and Portal of Cluster and Grid Computing Platforms 689 
or LAN). Beowulf clusters provide an effective and low-cost solution for delivering 
enormous computational powers to applications and are now used virtually every-
where. More specifically, a Beowulf cluster is a high-performance, high-throughput 
and high-availability computing platform [1]. 
To make use of multiple processes and executing each on a separate processor, we 
need to apply parallelism of computing algorithms. There are two common types of 
parallelism: MPI [14] and PVM [20]. PVM is a master-worker approach which is the 
simplest and easiest to implement. It relies on being able to break the computation 
into independent tasks. A master then coordinates the solution of these independent 
tasks by worker processes. MPI cannot (or cannot easily) be broken into independent 
tasks. In this kind of parallelism, the computation is broken down into communicat-
ing, inter-dependent tasks. Here we used LAM/MPI [15] for our cluster system and 
MPICH [16] for our Grid system. 
2.3   Grid Computing 
Grid computing [2][3] enables virtual organizations to share geographically distrib-
uted resources as they pursue common goals, assuming the absence of central loca-
tion, central control, omniscience, and an existing trust relationship. In our system, we 
adopted a Grid middleware called Globus toolkit 4.0 [21] as the infrastructure for our 
BioGrid. The toolkit includes software for security, information infrastructure, re-
source management, data management, communication, fault detection, and portabil-
ity. Java CoG [13] (Commodity Grid Kits) combines Java technology with Grid com-
puting is used to develop advanced Grid services and accessibility to basic Globus 
resources. It allows for easier and faster application developments. It also encourages 
collaborative code reuses and avoids the duplicate efforts among problems involving 
environments, science portals, Grid middleware, and collaborative pilots. 
3   Implementation 
Our implementation can be divided into two parts: the basic high-performance com-
puting environment and the user portals. 
3.1   High-Performance Computing Environment 
The high-performance computing environment comprises of the BioGrid [4][19] and 
BioCluster [6][7]systems. Both of them execute three bioinformatics software: mpiB-
LAST [17], FASTA and ClustalW. The hardware architecture is shown in Figure 1. In 
BioGrid portion, we construct a BioGrid testbed which includes four separated nodes. 
Each node is installed with Globus toolkit 3.0 for the Grid infrastructure, and 
MPICHG2 for message-passing. 
The Redhat 9.0 Linux distribution is installed on each node. The idea of the Linux 
cluster is to maximize the performance-to-cost ratio of computing by using low-cost 
commodity components and free-source Linux and GNU software to assemble a 
parallel and distributed computing system. Software support includes the standard 
 On Integration of GUI and Portal of Cluster and Grid Computing Platforms 691 
other components in the system. For example, GetRSL combines the RSL string. 
JobMonitor uses two parameters, Gridjob and RSL to start up the GlobusRun and 
monitors the job process. Then, it submits jobs to the Grid server and receives the 
responses from the Grid server. GridFTP could upload DNA sequences to the Grid 
System. In addition to the functions that Java CoG kit provides, there are several 
APIs developed specifically for our system. For instance, ProxyDestroy destroys the 
CA Files to protect the Grid system. Note that the machine files can be configured 
from the application site for our system as shown in Figure 2. 
For cluster, we also develop a series of capability. In server site, a program named 
CommandClient was written to receive the commands from the client. Users could 
configure the cluster system from the application site which includes information of 
how many CPUs being used, lam/mpi, PVM, lamhost file locations. And we can 
monitor the CPU and memory information to know which machines in our cluster 
system are alive. It also has the capability of lamboot and lamhalt from remote site as 
shown in Figure 3. 
We implement GUI for Bioinformatics software applications on both of the Grid 
and Cluster systems. Only application service interface is visible, however, imple-
mentation details such as distributed processing and parallel processing are invisible 
from users. 
The JSP web page utilizes a variety of advanced technologies such as JavaServer 
Pages, HTML, JavaScript, ActionScript (in Flash) and Tomcat. The portal composes 
of three parts: Machine Monitor, Bioinformatics software application and Job submis-
sion. Figure 5 demonstrates that the Machine Monitor (MM) can display critical in-
formation such as remote machine is still working or not and check if the Java CoG 
kit was installed on the machine. If the machine halts unexpected the Error Log deliv-
ers the details as seen in Figure 6. 
  
Fig. 2. Configure the machine file for Grid 
system     
Fig. 3. The capability of lamboot and lamhalt 
from remote site  
 
 On Integration of GUI and Portal of Cluster and Grid Computing Platforms 693 
  
Fig. 7. The Bioinformatics software Application Fig. 8. The Job Submission 
 
Fig. 9. System configuration and bioinformatics software application 
4   Performance Evaluation 
We conduct the experimentation on a 16-processor Linux PC cluster. Figure 10 shows 
experimental results of the parallel versions of all bioinformatics software. All parallel 
applications are executed by using from 2, 4, 8 to16 processors to compare the execu-
tion times. To obtain more accurate data, we execute five times per experiment and 
calculate the average time of execution. From the results, it is clearly found that the 
parallel system can reduce significant times from performing the sequence alignments. 
 On Integration of GUI and Portal of Cluster and Grid Computing Platforms 695 
5   Conclusions 
In the paper, we have built the basic platform for high-performance computing envi-
ronment using the Linux PC cluster and Grid. The experiment results illustrate that 
both environments save significant times in mapping and efficacy. It performs much 
better than regular computers. The user portals can help biologists and scientists to 
easily take control of the bioinformatics software as well as the Grid and Cluster 
systems. Multiple interfaces allow users to work with bioinformatics software from 
everywhere. 
References 
1. R. Buyya, High Performance Cluster Computing: System and Architectures, Vol. 1 and 
Vol. 2, Prentice Hall PTR, NJ, 1999. 
2. Foster, C. K., eds., The Grid 2: Blueprint for a New Computing Infrastructure, Morgan 
Kaufmann, 2nd edition, 2004. 
3. Foster, “The Grid: A New Infrastructure for 21st Century Science”, Physics Today, Vol. 
55, No. 2, 2002, pp. 42-47. 
4. Michael Karo, Christopher Dwan, John Freeman, Jon Weissman, Miron Livny, Ernest 
Retzel, Applying Grid Technologies to Bioinformatics, Proceedings of HPDC-10’01, 
2001, pp. 0441. 
5. Kuo-Bin Li, “ClustalW-MPI: ClustalW Analysis Using Distributed and Parallel Comput-
ing,” Bioinformatics, vol. 19, no. 12, pp. 1585-1586(2), 2003.  
6. Trelles O., Andrade M.A., Valencia A., Zapata E.L., and Carazo J.M., Computational 
Space Reduction and Parallelization of a new Clustering Approach for Large Groups of 
Sequences, Bioinformatics, vol.14, no.5, 1998, pp.439-451. 
7. Trelles O., “On the parallelization of bioinformatics applications,” Briefings in Bioinfor-
matics, May 2001, (vol.2) 2. 
8. T. L. Sterling, J. Salmon, D. J. Backer, and D. F. Savarese, How to Build a Beowulf: A 
Guide to the Implementation and Application of PC Clusters, 2nd Printing, MIT Press, 
Cambridge, Massachusetts, USA, 1999. 
9. Chao-Tung Yang, Yu-Lun Kuo, Kuan-Ching Li, and Jean-Luc Gaudiot, “On Design of 
Cluster and Grid Computing Environments for Bioinformatics Applications,” Distributed 
Computing - IWDC 2004: 6th International Workshop, Lecture Notes in Computer Sci-
ence, Springer-Verlag, Arunabha Sen, Nabanita Das, Sajal K. Das, et al. (Eds.), Kolkata, 
India, vol. 3326, pp. 82-87, Dec. 27-30, 2004. 
10. Chao-Tung Yang, Po-Chi Shih, Sung-Yi Chen, and Wen-Chung Shih “An Efficient Net-
work Information Modeling using NWS for Grid Computing Environments,” Grid and 
Cooperative Computing - GCC 2005: Fourth International Conference, Lecture Notes in 
Computer Science, vol. 3795, pp. 289-300, Springer-Verlag, November 2005. 
11. FASTA main page, ftp://ftp.virginia.edu/pub/fasta/ 
12. Human Genome Project, 
http://www.ornl.gov/sci/techresources/Human_Genome/home.shtml 
13. Java Cog kit, http://www-unix.globus.org/cog/ 
14. MPI Forum main page, http://www.mpi-forum.org/ 
15. LAM-MPI (Message Passing Interface) main page, http://www.lam-mpi.org/ 
 
H. Jin et al. (Eds.): ICA3PP 2007, LNCS 4494, pp. 302–313, 2007. 
© Springer-Verlag Berlin Heidelberg 2007 
Design and Implementation of Computational 
Bioinformatics Grid Services on GT4 Platforms* 
Chao-Tung Yang1,**, Tsu-Fen Han1, Ya-Ling Chen1, Heng-Chuan Kan2,  
and William C. Chu1 
1
 High-Performance Computing Laboratory 
Department of Computer Science and Information Engineering 
Tunghai University, Taichung City, 40704, Taiwan 
{ctyang,g942814,chu}@thu.edu.tw 
2
 Southern Business Unit 
National Center for High-Performance Computing 
Hsinshi, Tainan, 74147, Taiwan 
n00hck00@nchc.org.tw 
Abstract. Availability of computer resources is key factor limiting use of 
bioinformatics analyses as a result of the growing computational demands. Grid 
computing provides a way to meet these requirements. But it is complicated to 
build a grid for users. This paper describes an approach to solve this problem 
using Grid Service technologies. Building the grid based on accepted standards 
and platforms makes the development and deployment of the grid much easier. 
A bioinformatics grid computing environment (BioGrid) which consists of the 
distributed computing application for bioinformatics is presented in this paper. 
Based on this environment, we propose the architecture of bioinformatics 
applications which is delivered using Grid Services constructed with the Globus 
Toolkit 4. We developed a simple program which is defined as the client-server 
application with grid services. It provides users an approach of grid services to 
impose grid resources and customize their own grid applications. 
1   Introduction 
Biology is defined as the study of living things. In the course of that study, biologists 
collect and interpret data from the interaction of species and populations, to the 
function of tissues and cells within an individual organism. In the past, using 
sophisticated laboratory technology allows us to collect data faster than we can 
interpret it. There are vast volumes of DNA sequence data, and we need to figure out 
which parts of that DNA control the various chemical processes of life and determine 
the function of new proteins from the known function and structure of some proteins 
[1], [2], [6], [7], [9], [10], [11], [15]. 
                                                          
 
*
 The work is supported in part by National Science Council, Taiwan R.O.C., under grant no. 
NSC95-2221-E-029-004. 
**
 Corresponding author. 
304 C.-T. Yang et al. 
• [blastall] performs BLAST searches using one of five BLAST programs: blastn, 
blastp, blastx, tblastn, or tblastx. 
• [blastpgp] performs searches in PSI-BLAST or PHI-BLAST mode. blastpgp 
performs gapped blastp searches and can be used for iterative searches in psi-blast 
and phi-blast mode. 
• [bl2seq] performs a local alignment of two sequences. bl2seq allows the 
comparison of two known sequences using blastp or blastn. Most of the bl2seq 
command-line options are similar to those for blastall. 
• [formatdb] is used to format protein or nucleotide source databases. It converts a 
FASTA-format flat file sequence database into a BLAST database. 
The mpiBLAST algorithm consists of two primary steps: (i) databases are 
segmented and put on a shared storage device; (ii) mpiBLAST queries are run on each 
node. The mpiBLAST partitioning schema is shown in Figure 1. It uses multi-
threading to segment databases, assigning distinct portions of the database to each 
processor. It wraps the standard NCBI formatdb called mpiformatdb to format the 
database. Command line arguments specify the number of fragments. mpiformatdb 
formulates command line arguments that force NCBI formatdb to format and divide 
the database into many fragments of approximately equal size. When mpiformatdb 
execution is complete, the formatted fragments are placed in shared storage. 
Alignment of the database is accomplished by the local sequence alignment algorithm 
implemented in the NCBI [10] development library. If a node does not have 
fragments needed by a search, the fragments are copied from shared storage. 
Fragments are assigned to nodes using an algorithm that minimizes the number of 
fragments copied during each search. 
2.2   FASTA 
FASTA is another popular tool for searching sequence databases. It compares any two 
sequences by trying to align them from a database [8]. FASTA is capable of 
delivering very fast search results from sequence databases. Like BLAST It is 
available both as a service over the Web and as a downloadable set of programs. In 
the experimental environment of this paper, MPI versions of the programs that 
include the libary-vs-library comparison programs are listed below: 
• [fasta]: Compares a protein sequence against a protein database (or a DNA 
sequence against a DNA database) using the FASTA algorithm  
• [ssearch]: Compares a protein sequence against a protein database (or DNA 
sequence against a DNA database) using the Smith-Waterman algorithm  
• [fastx /fasty]: Compares a DNA sequence against a protein database, performing 
translations on the DNA sequence  
• [tfastx /tfasty]: Compares a protein sequence against a DNA database, performing 
translations on the DNA sequence database  
• [align]: Computes the global alignment between two DNA or protein sequences 
• [lalign]: Computes the local alignment between two DNA or protein sequences 
 
306 C.-T. Yang et al. 
 
Fig. 2. Web Services architecture stack 
2.4   WSRF 
The WS-Resource Framework (WSRF) has been proposed as a means of expressing the 
relationship between stateful resources and Web Services. Web Services specifications 
define a rendering of the WS-Resource approach in terms of specific message 
exchanges and related XML definitions. These specifications allow the programmer to 
declare and implement the association between a Web service and one or more stateful 
resources. They describe the means by which a view of the state of the resource is 
defined and associated with a Web Services description, forming the overall type 
definition of a WS-Resource. They also describe how the state of a WS-Resource  
is made accessible through a Web Service interface, and define related mecha- 
nisms concerned with WS-Resource grouping and addressing. The WSRF has five 
separate specification documents that provide the normative definition of the frame- 
work: WS-ResourceProperties, WS-ResourceLifetime, WS-RenewableReferences, WS-
ServiceGroup, and WS-BaseFaults. 
3   System Design and Implementation 
3.1   System Design 
It uses a GT4 core based Grid Services as a thin middleware layer to our application. 
The implementation language of Grid Services is Java. The user portal enables 
interactions between the application user and the application, obtaining parametric 
inputs for problems and reporting results upon application execution completion. Our 
portal uses Java-based Application as interface for easy control of the parallel 
bioinformatics software. We have also developed various basic services for the Grid 
systems. 
The Java-based Application uses the grid service to connect to the Grid system. We 
develop several grid services. The grid services represent a simple gram job and allow 
for submitting jobs to a gatekeeper. GridFTP can upload DNA sequences to the Grid 
System. It allows user to compare the specific sequences with the target database. 
308 C.-T. Yang et al. 
3.2   Specifications of Grid Services 
This section describes the deployment details of our grid services. Our application is a 
set of grid services that offers standard bioinformatics applications. Figure 4 
illustrates the software architecture of Grid Services application. At top layer, the 
BioGrid users can develop their specific application in Java or .NET application to 
query the Job_Dispatcher service on infoBio. Through the Job_Dispatcher service, 
BioGrid users can submit jobs to each node. At bottom layer, MPICH program such 
as mpiBLAST is wrapped by grid service on each node. It also provides GridFTP 
service to upload DNA sequences to the Grid System. It allows user to compare the 
specific sequence with target database. 
3.2.1   Deployment and Configuration 
The Grid Services is deployed on Linux server with GT4. The resource details are 
saved in a configuration file to allow easy modification of the computed resources 
available to the service. The configuration file is read by the service at deploy time. 
The interesting details include, the domain name of the resource's head node, the 
number of computed nodes, memory etc. The Grid Services including Job_dispatcher 
and mpiblast_services are deployed on infoBio server and the master nodes of each 
VO, which are implemented. When mpiblast_service is deployed, it returns a message 
to mpiblast_client and delivers a notice that a new VO joined grid environment.  
There are five steps for writing and deploying a WSRF Web Service. We describe 
the details of developing a grid service by using mpiBLAST_service as an example in 
the following: 
1. Define the service’s interface: The first step is to define the service interface. It 
needs to specify what the service provides. In the example, we want our 
do_mpiBLAST operation to receive four parameters. 
2. Implement the service: The next step is implementing that interface. We need to 
provide the implementation of our remotely accessible methods (do_mpiblast). 
3. Define the deployment parameters: This is done with the WSDD language and 
using a JNDI deployment file. Deployment descriptor is one of the key 
components of the deployment phase. It tells the Web Services container how it 
should publish the service. 
4. Compile everything and generate a GAR file: This is done with the Ant tool. Using 
the provided Ant buildfile and the handy script, building a web services as the 
following:  
./globus-build-service.sh -d <service base directory> -s 
<service’s WSDL file> 
If it works fine, the GAR file will be generated. 
5. Deploy service: This is done with a GT4 tool. The GAR file contains all the file 
and information, which the web server needs to deploy the web service. 
Deployment is done with a GT4 tool, then unpacks the GAR file and copies the file 
into key location in the GT4 directory tree. If it works fine, we can find the service 
in the list of services as shown in Figure 5. 
 
310 C.-T. Yang et al. 
env_nt. This mechanism is represented in Figure 6. At each VO, the mpiBLAST 
program chosen is executed for each database fragment file. mpiBLAST splits the 
target database across each node in each VO. Instead of the database segmentation of 
functionality immediately, we use stable number of database fragments according to 
the number of available workers. When all VOs end their local execution, the 
scheduler has to assemble the partial results at each VO to create a unique result file. 
The output of program will be saved as temp.dat in user’s directory. 
4   Experimental Results 
4.1   Testbed 
Our grid testbed is based on BioGrid system, which includes two separate virtual 
organizations, which have two developed bioinformatics packages: mpiBLAST, 
FASTA. Hardware configuration is shown as in Figure 7. Both virtual organizations 
(VOs) contain four nodes. The Fedora Core 4 Linux distribution has been installed on 
each node. We assessed the performance of the environment by executing mpiBLAST 
program using 2, 4, 8, and 16 processors. Table 1 shows the hardware configuration 
of our experimental environment. The following section describes an introduction of 
the databases that we adapted for our experimentation. 
• Swiss-Port (uniprot_sprot.fasta): Swiss-Prot was set up in 1986 as a kind of protein 
database with remarks and Swiss Institute of Bioinformatics and EMBL Outstation 
- The European Bioinformatics Institute, EBI, worked together to launch it in 1987. 
It consists of a large number of orders, each of which contains a specific format. 
The formatting is made as identical to that of EMBL Nucleotide Sequence 
Database as possible. 
• NCBI (nr database): The NR Protein database contains sequence data from the 
translated coding regions from DNA sequences in GenBank, EMBL and DDBJ as 
well as protein sequences submitted to PIR, SWISSPROT, PRF, PDB (sequences 
from solved structures). 
Internet
mbio01 bio01 ~ bio03 mbio02 bio04 ~ bio06
InfoBio
Server
BioGrid
User
 
Fig. 7. Experimental environment 
312 C.-T. Yang et al. 
5   Conclusion 
Biologists access to grid environments using the serial command line or other 
unfriendly way in the past. Web services provide flexible, extensible, and widely 
adopted XML-based mechanisms for describing, discovering, and invoking network 
services. Building the basic platform of grid computing environment is attempted in 
this study. Globus toolkit v.4 which serves as middleware is used for message transfer 
and communication between various Grid platforms. Results show that the BioGrid 
indeed saves significant time in sequence alignment problems with increasing number 
of processors used in the computations. In this paper, we proposed the architecture of 
bioinformatics applications which is delivered using Grid Services constructed with 
the Globus Toolkit 4.0.1. It provides users an approach of grid services to impose grid 
resources and customize their own grid applications. We developed a simple program 
which is defined as the client-server application with grid services. It provided users 
an approach of grid services to impose grid resources and customize their own grid 
applications. 
References 
1. Alexander, S., Bernhard, M., Roland, P., Johannes, R., Thomas, T., Zlatko, T.: Client-
Server Environment for High-Performance Gene Expression Data Analysis. 
Bioinformatics 19(6), 772–773 (2003) 
2. Bala, P., Jaroslaw, P., Miroslaw, N.: BioGRID – An European Grid for Molecular 
Biology. In: Proceedings of the 11th IEEE International Symposium on High Performance 
Distributed Computing, p. 412 (2002) 
3. Foster, I.: The Grid: A New Infrastructure for 21st Century Science. Physics Today 55(2), 
42–47 (2002) 
4. Foster, I., Kesselman, C.: The Grid 2: Blueprint for a New Computing Infrastructure 
(Elsevier Series in Grid Computing), 2nd edn. Morgan Kaufmann, San Francisco (2004) 
5. Foster, I., Kesselman, C.: Globus: A metacomputing infrastructure toolkit. The 
International Journal of Supercomputer Applications and High. Performance 
Computing 11(2), 115–128 (1997) 
6. Gernot, S., Dietmar, R., Zlatko, T.: ClusterControl: A Web Interface for Distributing and 
Monitoring Bioinformatics Applications on a Linux Cluster. Bioinformatics 20, 805–807 
(2004) 
7. Fumikazu, K., Tomoyuki, Y., Akinobu, F., Xavier, D., Kenji, S., Akihiko, K.: OBIGrid: A 
New Computing Platform for Bioinformatics. Genome Informatics 13, 484–485 (2002) 
8. FASTA, http://fasta.bioch.virginia.edu 
9. Micha, B., Campbell, A., Virdee, D.: A GT3 based. BLAST grid service for biomedical 
research. In: Proceedings of the UK e-Science All Hands Meeting (2004) 
10. mpiBLAST, http://mpiblast.lanl.gov 
11. Oswaldo, T., Miguel, A., Alfonso, V., Zapata, E.L., Carazo, J.M.: Computational Space 
Reduction and Parallelization of a new Clustering Approach for Large Groups of 
Sequences. Bioinformatics 14, 439–451 (1998) 
12. Pierce, M., Fox, G., Youn, C., Mock, S., Mueller, K., Balsoy, O.: Interoperable Web 
services for computational portals. In: Proceedings of the 2002 ACM/IEEE Conference on 
Supercomputing, pp. 1–12 (2002) 
ARTICLE  IN  PRESS
Future Generation Computer Systems ( ) –
www.elsevier.com/locate/fgcs
A Recursively-Adjusting Co-allocation scheme with a Cyber-Transformer in
Data GridsI
Chao-Tung Yanga,∗, I-Hsien Yanga, Shih-Yu Wanga, Ching-Hsien Hsub, Kuan-Ching Lic
aHigh Performance Computing Laboratory, Department of Computer Science and Information Engineering, Tunghai University, Taichung 40704, Taiwan, ROC
bDepartment of Computer Science and Information Engineering, Chung Hua University, Hsinchu 300, Taiwan
c Parallel and Distributed Processing Center, Department of Computer Science and Information Management, Providence University,
Taichung 43301, Taiwan, ROC
Received 16 February 2006; received in revised form 4 November 2006; accepted 16 November 2006
Abstract
A co-allocation architecture was developed in order to enable parallel downloads of datasets from multiple servers. Several co-allocation
strategies have been coupled and used to exploit rate differences among various client–server links and to address dynamic rate fluctuations by
dividing files into multiple blocks of equal sizes. However, a major obstacle, the idle time of faster servers having to wait for the slowest server
to deliver the final block, makes it important to reduce differences in finish times among replica servers. In this paper, we propose Recursively-
Adjusting Co-Allocation, a dynamic co-allocation scheme for improving data transfer performance in Data Grids. The experimental results show
that our approach can reduce the idle time spent waiting for the slowest server and decrease data transfer completion times. We developed Cyber-
Transformer, a new toolkit with a friendly GUI interface that makes it easy for inexperienced users to manage replicas and download files in Data
Grid environments. We also provide an effective scheme for reducing the cost of reassembling data blocks.
c© 2007 Elsevier B.V. All rights reserved.
Keywords: Data Grid; Globus; GridFTP; Co-allocation; Recursively-Adjusting; Data transfer
1. Introduction
Data Grids aggregate distributed resources for solving large-
size dataset management problems [1,2,4,7,9,25–30]. Most
Data Grid applications execute simultaneously and access
large numbers of data files in Grid environments. Certain
data-intensive scientific applications, such as high-energy
physics, bioinformatics applications and virtual astrophysical
observatories, entail huge amounts of data that require data file
management systems to replicate files and manage transfers and
distributed data access. The Data Grid infrastructure integrates
data storage devices and data management services into a grid
environment that consists of scattered computing and storage
I This paper is supported in part by National Science Council, Taiwan, ROC,
under grants no. NSC94-2213-E-029-002 and NSC95-2221-E-029-004.
∗ Corresponding author. Tel.: +886 4 23590415; fax: +886 4 23591567.
E-mail addresses: ctyang@thu.edu.tw (C.-T. Yang), g922906@thu.edu.tw
(I.-H. Yang), g932813@thu.edu.tw (S.-Y. Wang), chh@chu.edu.tw
(C.-H. Hsu), kuancli@pu.edu.tw (K.-C. Li).
resources, perhaps located in different countries/regions yet
accessible to users [2,9,27–30].
In Data Grid environments, access to distributed data is
typically as important as access to distributed computational
resources. Distributed scientific and engineering applications
require transfers of large amounts of data between storage
systems, and access to large amounts of data generated by
many geographically distributed applications and users for
analysis and visualization, among others. Recently, large-scale
data-sharing scientific communities such as those described
in [1,4] have begun using this technology to replicate their
large datasets over several sites. Replicating popular content in
distributed servers is widely used in practice [11,13,15].
One way to improve download speeds is to determine the
best replica locations using replica selection techniques [15].
This method selects servers that will provide optimum transfer
rates because bandwidth quality can vary unpredictably due
to the shared nature of the Internet. Another way is to use
co-allocation technology [13] to download data. Co-allocation
0167-739X/$ - see front matter c© 2007 Elsevier B.V. All rights reserved.
doi:10.1016/j.future.2006.11.005
Please cite this article in press as: C.-T. Yang, et al., A Recursively-Adjusting Co-allocation scheme with a Cyber-Transformer in Data Grids, Future Generation
Computer Systems (2007), doi:10.1016/j.future.2006.11.005
ARTICLE  IN  PRESS
C.-T. Yang et al. / Future Generation Computer Systems ( ) – 3
Fig. 1. Data Grid co-allocation architecture [15].
need is to transfer a single file faster than a single host can.
Thus, it tends to be effective only for large files, though how
large depends on how many hosts and how fast the end-to-end
transfer is.
Fig. 1 shows the Data Grid co-allocation architecture,
which is an extension of the basic template for resource
management [6] provided by the Globus Toolkit. The
architecture consists of an information service, broker/co-
allocator and local storage systems. Applications specify the
characteristics of desired data and pass the attribute description
to the broker. The broker queries available resources and gets
replica locations from information services [5] and replica
management services [15], and then gets a list of physical
locations for the desired files.
The candidate replica locations are passed to a replica
selection service [15], which was presented in a previous
work [19]. This replica selection service provides estimates
of candidate transfer performance based on a cost model
and chooses appropriate amounts to request from the better
locations. The co-allocation agent then downloads the data
in parallel from the selected servers. We use GridFTP [1,8,
12], a high-performance, secure, reliable data transfer protocol
optimized for high-bandwidth wide-area networks, to enable
parallel data transfers. Among its many features are security,
parallel streams, partial file transfers, third-party transfers and
reusable data channels. Its partial file transfer ability allows files
to be retrieved from data servers by specifying the start and end
offsets of the file sections.
In a previous work [19] we proposed a replica selection
cost model and a replica selection service. In [13], the author
proposes an architecture for co-allocating Grid data transfers
across multiple connections by exploiting the partial copy
feature of GridFTP. He also provides Brute-Force,History-Base
and Dynamic Load Balancing for allocating data blocks. Brute-
Force Co-Allocation works by dividing file sizes equally across
available flows without addressing bandwidth differences
among the various client–server links. The History-based Co-
Allocation scheme keeps block sizes per flow proportional to
predicted transfer rates.
The Conservative Load Balancing dynamic co-allocation
strategy divides requested datasets into “k” disjoint blocks
of equal size. Available servers are assigned single blocks
to deliver in parallel. When a server finishes delivering
a block, another is requested, and so on, until the entire
file is downloaded. The loadings on the co-allocated flows
are automatically adjusted because the faster servers will
deliver more quickly providing larger portions of the file.
The Aggressive Load Balancing dynamic co-allocation strategy
presented in [13] adds functions that change block size
deliveries by: (1) progressively increasing the amounts of data
requested from faster servers and (2) reducing the amounts of
data requested from slower servers or ceasing to request data
from them altogether.
The co-allocation strategies described above do not address
the shortcoming of faster servers having to wait for the slowest
server to deliver its final block. In most cases, this wastes much
time and decreases overall performance. Thus, we propose an
efficient approach called Recursively-Adjusting Co-Allocation
and based on a co-allocation architecture. It improves dynamic
co-allocation and reduces waiting time, thus improving overall
transfer performance.
3. Dynamic allocation strategy
In this work, we focused on reducing the idle time in
parallel downloading connections. The approach proposed
in the present paper, a dynamic allocation mechanism
called Recursively-Adjusting Co-Allocation, will be stated in
Section 3.1. Then we will describe how to determine when
to stop continuous adjustment in Section 3.2. Finally, in
Sections 3.3 and 3.4, we state the GUI interface and workflow
of our tool.
3.1. Recursively-Adjusting Co-allocation
The co-allocation strategy described above is the most
efficient approach to reducing the influence of network
variations between clients and servers. However, the idle
time of faster servers waiting for the slowest server to
deliver the last block is still a major factor affecting overall
efficiency, which Conservative Load Balancing and Aggressive
Load Balancing [13] cannot effectively avoid. In real-world
networking environments, the available bandwidth of a Replica
Server might change dynamically as a result of networking
configuration or load variations. Previous algorithms could not
adapt to these dynamisms. Therefore, the greater the degree
of bandwidth variation the greater the time needed. Thus,
overall efficiency depends on several factors. Our strategy can
overcome this obstacle, and improve data transfer performance.
Recursively-Adjusting Co-Allocation works by continuously
adjusting the workload of each replica server to correspond
to its real-time bandwidth during file transfers. The goal is
to make the expected finish time of all servers the same. As
Fig. 2 shows, when an appropriate file-section is first selected,
it is divided into proper block sizes according to the respective
server bandwidths. The co-allocator then assigns the blocks
to servers for transfer. At this moment, it is expected that the
transfer finish time will be consistent at E(t1). However, since
server bandwidths may fluctuate during segment deliveries,
actual completion times may vary (solid line, in Fig. 2).
When the quickest server finishes its work at time t1, the next
section is assigned to the servers. This allows each server to
finish its assigned work-load by the expected time at E(t2).
Please cite this article in press as: C.-T. Yang, et al., A Recursively-Adjusting Co-allocation scheme with a Cyber-Transformer in Data Grids, Future Generation
Computer Systems (2007), doi:10.1016/j.future.2006.11.005
ARTICLE  IN  PRESS
C.-T. Yang et al. / Future Generation Computer Systems ( ) – 5
Define final section
SE j = UnassignedFileSize
}
Define new section to be allocated SE j
Monitor each selected replica server
Allocate blocks to each selected replica server
Monitor each download flow
}
end while loop
3.2. Determining when to stop continuous adjustment
Our approach gets new sections from whole files by dividing
unassigned file ranges in each round of allocation. These
unassigned portions of the file ranges become smaller after each
allocation. Since adjustment is continuous, it would run as an
endless loop if not limited by a stop condition. However, when
is it appropriate to stop continuous adjustment? We provide
two monitoring criteria, LeastSize and ExpectFinishedTime, to
enable users to define stop thresholds.
When a threshold is reached, the co-allocation server will
stop dividing the remainder of the file and assign that remainder
as the final section. The LeastSize criterion specifies the
smallest file we want to process, and when the unassigned
portion of UnassignedFileSize drops below the LeastSize
specification, division stops. The ExpectFinishedTime criterion
specifies the remaining time transfer is expected to take. When
the expected transfer time of the unassigned portion of a file
drops below the time specified by ExpectFinishedTime, file
division stops. The expected value of the rest time is determined
by:
UnAssignedFileSize
/
n∑
i=1
Bi . (3)
These two criteria determine the final section size allocated.
Higher threshold values will induce fewer divisions and
yield lower co-allocation costs, which include establishing
connections, negotiation, reassembly, etc. However, although
the total co-allocation adjustment time may be lower,
bandwidth variations may also exert more influence. By
contrast, lower threshold values will induce more frequent
dynamic server workload adjustments and, in the case of
greater network fluctuations, result in fewer differences in
server transfer finish times. However, lower values will also
increase co-allocation times, and hence, increase co-allocation
costs. Therefore, the Internet environment, transferred file sizes
and co-allocation costs should all be considered in determining
optimum thresholds.
3.3. Cyber-Transformer
Cyber-Transformer is a new toolkit for replica management
and parallel file transfers in Data Grid environments that not
only accelerates data transfer rates but also manages replicas
over various sites. Its friendly interface makes it easy for users
to monitor replica sources and add files as replicas for automatic
Fig. 4. The transaction flow of Cyber-Transformer.
cataloging by our Replica Location Service. Moreover, we
provide a function for administrators to delete and modify
replicas. Cyber-Transformer can be invoked with either the
logical file name of a data file or a list of replica source host
names. If a user searches for a file with a logical file name,
Cyber-Transformer will query the Replica Location Service to
find all corresponding replicas, then start contacting replica
sources to begin parallel transfer. The file is then obtained from
these sources and combined into a single file.
3.4. Workflow
Fig. 4 shows the Data Grid transaction flow. Users must first
obtain the Grid Proxy Certification provided by Simple CA to
gain access to the Grid, and then may connect with every site
in the Data Grid using the GridFTP Browser. The system will
automatically authenticate the certification as users connect to
sites in the Data Grid. The security mechanism of our Grid
environment is described above.
Steps 4 and 5 depict the replica selection service. Users can
query the replica location service for replica information, and
the replica location service will report results. Users can then
choose some or all replicas from the various sites as replica
sources and transfer them in parallel.
The data transfer service is invoked in Step 6. The replica
information chosen by the user is passed to the GridFTP Job
Controller, which dynamically adjusts the size of each replica
transfer to the conditions presented in the information. Replica
job sizes are dynamically adjusted until all job transfers have
been completed. The portions from the various replica sources
are then gathered into a complete file.
4. Experimental results and analyses
In this section, we discuss the performance of our
Recursively-Adjusting Co-Allocation strategy. We evaluate
Please cite this article in press as: C.-T. Yang, et al., A Recursively-Adjusting Co-allocation scheme with a Cyber-Transformer in Data Grids, Future Generation
Computer Systems (2007), doi:10.1016/j.future.2006.11.005
ARTICLE  IN  PRESS
C.-T. Yang et al. / Future Generation Computer Systems ( ) – 7
Fig. 9. Block numbers for various α values.
Fig. 10. Idle times for various LeastSize values.
Fig. 11. Block numbers for various LeastSize values.
time. Although this results in more block numbers, the increase
is not excessive. Fig. 10 indicates that we may infer that
the Recursively-Adjusting scheme performs better with smaller
LeastSize threshold values for most file sizes because smaller
final blocks are less influenced by network variations.
Our work focuses on improving the speed at which Grid
users can download files from the client side. Downloading
speed increases through parallel downloading and the efficient
Replica Co-Allocation algorithm. Consequently, the overall
completion time is reduced.
We analyzed the effect of faster servers waiting for the
slowest server to deliver the last block for each scheme.
Fig. 12 shows total idle times for various file sizes. Note
that our Recursively-Adjusting Co-Allocation scheme achieved
significant performance improvements over other schemes for
Fig. 12. Idle times for various methods; servers were at PU, DL and HIT.
Fig. 13. Combination times for various methods; servers were at PU, DL and
HIT.
every file size. These results demonstrate that our approach
efficiently reduces the differences in server finish times. The
experimental results shown in Fig. 13 indicate that our scheme
begins block reassembly as soon as the first blocks have been
completely delivered reduces combination time, thus aiding
co-allocation strategies like Conservative Load Balancing and
Recursively-Adjusting Co-Allocation that produce more blocks
during data transfers.
5. Conclusions
Using the parallel-access approach to downloading data
from multiple servers reduces transfer times and increases
server resilience. The co-allocation architecture provides a
coordinated agent for assigning data blocks. A previous
work showed that the dynamic co-allocation scheme leads to
performance improvements, but cannot address the idle time of
faster servers that must wait for the slowest server to deliver
its final block. This study proposes the Recursively-Adjusting
Co-Allocation scheme to improve data transfer performance
using the co-allocation architecture in [13]. In this approach, the
workloads of selected replica servers are continuously adjusted
during data transfers, and we provide a function that enables
users to define a final block threshold according to their Data
Grid environment. We also developed a new toolkit, Cyber-
Transformer, with a friendly GUI interface that makes it easy
for inexperienced users to manage replicas and download files
Please cite this article in press as: C.-T. Yang, et al., A Recursively-Adjusting Co-allocation scheme with a Cyber-Transformer in Data Grids, Future Generation
Computer Systems (2007), doi:10.1016/j.future.2006.11.005
ARTICLE  IN  PRESS
C.-T. Yang et al. / Future Generation Computer Systems ( ) – 9
Ching-Hsien Hsu received the B.S. and Ph.D. degrees
in computer science from Tunghai University and
Feng Chia University, Taiwan, in 1995 and 1999,
respectively. From 2001 to 2002, Dr. Hsu had been
an Assistant Professor in the Department of Electrical
Engineering at Nan Kai College. He joined the
Department of Computer Science and Information
Engineering, Chung Hua University in 2002, and has
become an Associate Professor since August 2005. His
research interests include parallel and distributed processing, high performance
computing, parallelizing compilers, grid and pervasive computing. He is a
member of the IEEE Computer Society.
Kuan-Ching Li received the Ph.D. and M.S. in Elec-
trical Engineering and Licenciatura in Mathematics
from University of Sao Paulo, Brazil in 2001, 1996
and 1994, respectively. After he received his Ph.D., he
had been post-doc scholar in the Dept. of Electrical
and Computer Engineering, University of California -
Irvine, California, USA and the Department of Elec-
trical Engineering - Systems, University of Southern
California, California, USA. Professor Li is currently
an Associate Professor in the Department of Computer Science and Informa-
tion Engineering at the Providence University, Taiwan. Prior to join Providence
in February 2003, he was a research scientist at the University of California -
Irvine. His research interests include cluster and grid computing, parallel soft-
ware design, and life sciences computing.
Please cite this article in press as: C.-T. Yang, et al., A Recursively-Adjusting Co-allocation scheme with a Cyber-Transformer in Data Grids, Future Generation
Computer Systems (2007), doi:10.1016/j.future.2006.11.005
250 C.-T. Yang et al.
1 Introduction
Grids offer a way to solve Grand Challenge problems like protein folding, drug dis-
covery, financial modeling, earthquake simulation, and climate/weather forecasting,
among others. Grids enable organizations to make optimal use of information tech-
nology resources. And grids offer a means to act as a utility bureau in providing
information technology to commercial clients who pay only for what they use, as
with electricity or water [1, 3–12, 14–21].
Grid computing involves sharing, over an open-standards network, heterogeneous
resources from various hardware and software platforms, computer architectures, and
computer languages located in different places and belonging to different administra-
tive domains. In short, it involves vitalizing computing resources. Functionally, one
can classify grids as:
• computational and
• data.
Regardless of grid type, bandwidth management is a question of manipulating
a number of variables to support the system and maximizing grid performance.
As Grid Computing becomes a reality, there is a need to manage and monitor avail-
able resources worldwide, as well as a need to convey these resources to everyday
users.
Most grids serving research and academic communities in North America and
Europe utilize the Globus Toolkit® as their core middleware. The Globus Informa-
tion Service, Monitor and Discover Service (MDS) provides good system-related in-
formation support on CPU speeds, CPU loading, memory utilization, etc., but no
network-related information support. Therefore, we use the open-source program,
Network Weather Service (NWS) [2], for network information.
NWS can measure point-to-point network bandwidth and latency that may be im-
portant for grid scheduling and load balancing. NWS detects all network states during
time periods selected by the user. Because this kind of side-to-side measure results in
N(N − 1) network measurement processes, the time complexity is O(N2). Our net-
work model focuses on solving the problem of reducing this time complexity without
losing too much precision. There is another question. We want to know how NWS
parameters (time period, frame size) influence our model and whether the NWS mea-
surement value is inaccurate compared with real-world networks.
In this paper, we first describe a resource selection consideration and strategy
which contains four phases and ten steps. And we implement a Grid resource bro-
ker based on these strategies. Second, we provide approximate measurement models
for network-related information using NWS for future scheduling and benchmark-
ing. Third, we provides a uniform interface for using our resource broker to access-
ing available and appropriate resources via user credentials. Fourth, we constructed
a grid platform using Globus Toolkit that integrates the resources of five schools in
Taichung integrated grid environment resources (TIGER). The resource broker runs
on top of TIGER. Therefore, it provides security and current information about avail-
able resources and serves as a link to the diverse systems available in the Grid.
The remainder of this paper is organized as follows. Related studies are presented
in Sect. 2 and the resource selection and strategy is introduced in Sect. 3. Our network
252 C.-T. Yang et al.
2.4 MPICH-G2
MPICH-G2 [2] is a grid-enabled implementation of the MPI v1.1 standard. That is,
using services from the Globus Toolkit® (e.g., job startup, security); MPICH-G2 en-
ables coupling of multiple machines, potentially with different architectures, to run
MPI applications. MPICH-G2 automatically converts data in messages sent between
machines with different architectures and supports multi-protocol communication
by automatically selecting TCP for inter-machine messaging and, where available,
vendor-supplied MPI for intra-machine messaging. Existing parallel programs writ-
ten for MPI can be executed over the Globus infrastructure after just recompilation.
3 Resource selection and strategy
Our resource broker is built on top of the Globus Toolkit. It makes use of Globus
services, such as resource allocation, information, and GridFTP service. Our network
monitor includes a measurement tool, cluster information provider, as well as NWS
for forecasting network bandwidth (see Fig. 1).
Grid Resource Brokering involves four main phases: Resource Discovery, which
generates lists of potential resources, Application Modeling, which enables users to
characterize application behavior, Information Collection, which collects dynamical
resource information, System Selection, which filters out resources that do not sat-
isfy user requirements, then selects the best set of resources depending on system
information, and Job Execution, which includes file transferring, pre-compilation,
job execution, and result retrieval. These phases and the steps are shown in Fig. 2.
Fig. 1 Resource broker architecture
254 C.-T. Yang et al.
Fig. 3 Proxy initialization
Fig. 4 Application modeling
256 C.-T. Yang et al.
Fig. 6 Job has been compiled and run
Step 7: Job Submission Before actually running a job, the application must be
submitted to the resource set described in Step 6. This step is performed in two parts.
The first action transfers the machine list, needed for MPI, to the first machine on
our machine list via GridFTP because the first choice is the best based on application
modeling. The second action transfers the application to all resources.
Step 8: Preparation Tasks Preparation may involve setup, compilation, and other
actions needed to ready resources to run the application, and making sure program
files, data files, argument files, and other set files are placed correctly. After the pro-
gram is uploaded to target resources, the broker simultaneously sends the compiled
operation to all machines and to the compilation source program. When compilation
is finished, the broker begins parallel program execution.
Step 9: Progress Monitoring While the job runs, users can monitor the progress
of their application and may elect to cancel or re-submit jobs. Historically, such mon-
itoring has typically been done by repetitively querying resources for status informa-
tion. However Globus is only able to interrupt users when jobs finish. GRAM pro-
vides basic status information such as running, finished, and failed. GRAM estimates
how much time is needed to finish jobs, so only “running” is reported.
Step 10: Job Completion Users must be notified when jobs finish. The broker
must able to interrupt users upon job completion. Sending e-mail or voice messages
to user cell phones may implemented in the future.
While programs run, our broker shows a progress bar to indicate job status, and
waits for completion. When jobs finish, the broker automatically retrieves results and
258 C.-T. Yang et al.
Fig. 8 Network measurement
model
Fig. 9 Domain-based network
measurement model
domain to control and maintain its hosts. The domains may each use a different net-
work infrastructure: Fast Ethernet, Gigabit, or InfiniBand. This design ensures that
local fluctuations won’t affect the entire grid system.
Some questions about the model remain:
• how to select a representative host in each domain to form a central domain without
loss of generality?
• how to accurately evaluate host-to-host network information?
These questions are explored in detail below. Constructing a domain-based grid is
the key issue. Domains must first be constructed by the schools or organizations. One
260 C.-T. Yang et al.
Fig. 11 Bandwidth usage in
case 1
Case 1:
Assume the inner domain bandwidth use shown in Fig. 11. This is complex be-
cause the usage between Alpha2 and Alpha3 may not affect the Bridge bandwidth
much. We use an algorithm to calculate target bandwidth.
First, left domain bandwidth fluctuation is examined, ignoring pulse or bandwidth
noise fluctuation:
Use = CountIf
( |Lij [k] − B_inavg|
B_inavg
> Pflu
)
> (Nflu ∗ Pvaflu), k = 1, . . . ,Nflu∀ij
in left domain (1)
We then calculate the remaining bandwidth use, ignoring the maximal and mini-
mal bandwidth values by first Sort (Lij ), then compute:
Brem =
∑Nflu−1
k=2 Lij [k]
Nflu − 2 (2)
Finally, the target bandwidth is calculated as follows:
Btar = Brem
B_inavg
× B_outavg × α (3)
The symbol α here indicates a value converted from internet bandwidth to LAN
and is used throughout.
Case 2:
We assumed that bandwidth use occurs in the same organization but not with other
members of the domain, as shown in Fig. 12. Figure 13 shows the general topology
of this network architecture. Using a simple test, we discovered that target bandwidth
almost always follows bridge bandwidth. So we summarize briefly that target band-
width is almost always equal to bridge bandwidth.
Case 3:
We assumed that bandwidth use occurs between two domains, as shown in Fig. 14.
Bandwidth use between Alpha2 and Lz02 will affect the available bridge bandwidth,
so we summarize briefly that target bandwidth is almost always equal to bridge band-
width.
262 C.-T. Yang et al.
Fig. 15 The TIGER experimental environment
Table 1 Resource specification
Resource CPU Type speed Mem Network OS kernel
THU alpha1 AMD Athlon(tm) MP 2400+ x 2 1G 10/100 fedora core 1 2.4.22
alpha2 AMD Athlon(tm) MP 2000+ x 2 768 MB 10/100 fedora core 1 2.4.22
alpha3 AMD Athlon(tm) MP 1800+ x 2 512 MB 10/100 fedora core 1 2.4.22
alpha4 AMD Athlon(tm) MP 1800+ x 2 512 MB 10/100 fedora core 1 2.4.22
LZ lz01 Celeron 900 256 MB 10/100 fedora core 1 2.4.20-31.9
lz02 Celeron 900 256 MB 10/100 fedora core 1 2.4.20-31.9
lz03 Celeron 900 384 MB 10/100 fedora core 1 2.4.20-31.9
lz04 Celeron 900 256 MB 10/100 fedora core 1 2.4.20-31.9
HIT gridhit0 Pentium 4 2.8G 512 MB 10/100 fedora core 1 2.4.20-8
gridhit1 Pentium 4 2.8G 512 MB 10/100 fedora core 1 2.4.20-8
gridhit2 Pentium 4 2.8G 512 MB 10/100 fedora core 1 2.4.20-8
gridhit3 Pentium 4 2.8G 512 MB 10/100 fedora core 1 2.4.20-8
PU hpc09 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc10 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc11 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc12 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc13 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc14 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc15 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
hpc16 AMD Athlon(tm) XP 2400+ 1G 10/100 fedora core 1 2.4.22
264 C.-T. Yang et al.
Fig. 18 Console execution vs.
RB execution with 1 CPU
Fig. 19 Console execution vs.
RB execution with 2 CPU
Fig. 20 Resource Broker vs.
random host selection
266 C.-T. Yang et al.
14. Yang C-T, Chu WC (2004) Grid computing in Taiwan. In: Proceedings of 10th international workshop
on future trends of distributed computing systems (FTDCS 2004), Suzhou, China, May 26–28, 2004,
pp 201–204
15. Yang C-T, Ho H-C (2005) An e-learning platform based on grid architecture. J Inf Sci Eng 21(5):115
16. Yang C-T, Kuo Y-L, Lai C-L (2005) Designing computing platform for BioGrid. Int J Comput Appl
Technol (IJCAT), special issue applications for high performance systems 22(1):3–13, Inderscience
Publishers, ISSN (Paper): 0952-8091, UK
17. Yang C-T, Lai C-L (2004) Apply cluster and grid computing on parallel 3D rendering. In: Proceedings
of the 2004 IEEE international conference on multimedia and expo (ICME 2004), Grand Hotel, Taipei,
Taiwan, June 27–30, vol 2, 2004, pp 859–862
18. Yang C-T, Lai C-L, Shih P-C, Li K-C (2004) A resource broker for computing nodes selection in
grid environments. In: Jin H, Pan Y, Xiao N (eds), Grid and cooperative computing—GCC 2004:
third international conference, lecture notes in computer science, vol 3251. Springer, Oct 2004, pp
931–934
19. Yang C-T, Shih P-C, Li K-C (2005) A high-performance computational resource broker for grid com-
puting environments. In: Proceedings of the international conference on advanced information net-
working and applications (AINA 2005), 2005, Tamkang University, Taipei, Taiwan, March 28–30,
vol 2, pp 333–336
20. Yang C-T, Shih P-C, Chen S-Y (2006) A domain-based model for efficient network information on
grid computing environments. IEICE Trans Inf Syst, special issue on parallel/distributed computing
and networking E89-D(2):738–742
21. Yang C-T, Yang I-H, Li K-C, Wang S-Y (2006) Improvements on dynamic adjustment mechanism in
co-allocation data grid environments. J Supercomput (accepted)
22. Zhang X, Freschl JL, Schopf JM (2003) A performance study of monitoring and information services
for distributed systems. In: Proceedings of HPDC, IEEE CS Press, August 2003, pp 270–282
Chao-Tung Yang received a B.S. degree in computer science and information engineering from Tunghai
University, Taichung, Taiwan in 1990, and the M.S. degree in computer and information science from
National Chiao Tung University, Hsinchu, Taiwan in 1992. He received the Ph.D. degree in computer and
information science from National Chiao Tung University in July 1996. He won the 1996 Acer Dragon
Award for outstanding Ph.D. Dissertation. He has worked as an associate researcher for ground operations
in the ROCSAT Ground System Section (RGS) of the National Space Program Office (NSPO) in Hsinchu
Science-based Industrial Park since 1996. In August 2001, he joined the faculty of the Department of
Computer Science and Information Engineering at Tunghai University, where he is currently an associate
professor. His researches have been sponsored by Taiwan agencies National Science Council (NSC), Na-
tional Center for High Performance Computing (NCHC), and Ministry of Education. His present research
interests are in grid and cluster computing, parallel and high-performance computing, and internet-based
applications. He is both member of the IEEE Computer Society and ACM.
Po-Chi Shih received the B.S. and M.S. degrees in Computer Science and Information Engineering from
Tunghai University in 2003 and 2005, respectively. He now is studying Ph.D. degree at Computer Science
in National Tsing Hua University, Hsinchu, Taiwan from September 2005. His present research interests
are grid computing and internet-based applications.
J Supercomput (2007) 40: 269–280
DOI 10.1007/s11227-006-0022-3
Improvements on dynamic adjustment mechanism
in co-allocation data grid environments
Chao-Tung Yang · I-Hsien Yang · Kuan-Ching Li ·
Shih-Yu Wang
Published online: 31 March 2007
© Springer Science+Business Media, LLC 2007
Abstract Several co-allocation strategies have been coupled and used to exploit rate
differences among various client-server links and to address dynamic rate fluctua-
tions by dividing files into multiple blocks of equal sizes. However, a major obstacle,
the idle time of faster servers having to wait for the slowest server to deliver the fi-
nal block, makes it important to reduce differences in finishing time among replica
servers. In this paper, we propose a dynamic co-allocation scheme, namely Recursive-
Adjustment Co-Allocation scheme, to improve the performance of data transfer in
Data Grids. Our approach reduces the idle time spent waiting for the slowest server
and decreases data transfer completion time.
Keywords Data Grid · Dynamic · Recursive · GridFTP · Co-allocation · Data
transfer
1 Introduction
In Data Grid environments, access to distributed data is typically as important as ac-
cess to distributed computational resources [1, 2, 4, 7, 9]. Distributed scientific and
C.-T. Yang () · I.-H. Yang · S.-Y. Wang
High-Performance Computing Laboratory, Department of Computer Science and Information
Engineering, Tunghai University, Taichung City 40704, Taiwan R.O.C.
e-mail: ctyang@thu.edu.tw
I.-H. Yang
e-mail: g922906@thu.edu.tw
S.-Y. Wang
e-mail: g932813@thu.edu.tw
K.-C. Li
Department of Computer Science and Information Engineering, Providence University,
Taichung 43301, Taiwan R.O.C.
e-mail: kuancli@pu.edu.tw
Improvements on dynamic adjustment mechanism 271
As datasets are replicated within Grid environments for reliability and perfor-
mance, clients require the abilities to discover existing data replicas, and create and
register new replicas. A Replica Location Service (RLS) [3, 15] provides a mech-
anism for discovering and registering existing replicas. Several prediction metrics
have been developed to help replica selection. For instance, Vazhkudai and Schopf
[14, 16, 17] used past data transfer histories to estimate current data transfer through-
puts.
In our previous work [19], we proposed a replica selection cost model and a replica
selection service to perform replica selection. In [13], the author proposes a co-
allocation architecture for co-allocating Grid data transfers across multiple connec-
tions by exploiting the partial copy feature of GridFTP. It also provides Brute-Force,
History-Base, and Dynamic Load Balancing for allocating data block. Brute-Force
Co-Allocation works by dividing file sizes equally across available flows without ad-
dressing bandwidth differences among the various client-server links. The History-
based Co-Allocation scheme keeps block sizes per flow proportional to predicted
transfer rates.
The Conservative Load Balancing dynamic co-allocation strategy divides re-
quested datasets into “k” disjoint blocks of equal size. Available servers are assigned
single blocks to deliver in parallel. When a server finishes delivering a block, an-
other is requested, and so on, till the entire file is downloaded. The loadings on the
co-allocated flows are automatically adjusted because the faster servers will deliver
more quickly providing larger portions of the file. The Aggressive Load Balancing
dynamic co-allocation strategy presented in [13] adds functions that change block
size de-liveries by: (1) progressively increasing the amounts of data requested from
faster servers, and (2) reducing the amounts of data requested from slower servers or
ceasing to request data from them altogether.
The co-allocation strategies described above do not handle the shortcoming of
faster servers having to wait for the slowest server to deliver its final block. In most
cases, this wastes much time and decreases overall performance. Thus, we propose
an efficient approach called Recursive-Adjustment Co-Allocation and based on a co-
allocation architecture. It improves dynamic co-allocation and reduces waiting time,
thus improving overall transfer performance.
3 Co-allocation architecture
The co-allocation of data transfers enables the clients to download data from multi-
ple locations by establishing multiple connections in parallel [6, 13]. Figure 1 shows
the co-allocation of data transfers in Grid, which is an extension of the basic template
for resource management [6] provided by Globus Toolkit. The architecture consists of
three main components: an information service, broker/co-allocator, and local storage
systems. Applications specify the characteristics of desired data and pass the attribute
description to a broker. The broker queries available resources and gets replica loca-
tions from information services [5] and replica management services [13, 15], and
then gets a list of physical locations for the desired files.
The candidate replica locations are passed to a replica selection service [13, 15],
which was presented in a previous work [19]. This replica selection service provides
estimates of candidate transfer performance based on a cost model and chooses ap-
Improvements on dynamic adjustment mechanism 273
Fig. 2 The adjustment process
Fig. 3 The flowchart of
recursive-adjustment
co-allocation
The Recursive-Adjustment Co-Allocation process is as follows. A new section of
a file to be allocated is first defined. The section size, “SEj ,” is:
SEj = UnassignedFileSize × α, (0 < α < 1), (1)
where SEj denotes the section j such that 1 j  k, assuming we allocate k times
for the download process, and thus, there are k sections, while Tj denotes the time
section j allocated. UnassignedFileSize is the portion of file A not yet distributed for
downloading; initially, UnassignedFileSize is equal to the total size of file A. α is the
rate that determines how much of the section remains to be assigned.
In the next step, SEj is divided into several blocks and assigned to “n” servers.
Each server has a real-time transfer rate to the client of Bi , which is measured by the
Network Weather Service (NWS) [18]. The block size per flow from SEj for each
server “i” at time Tj is:
Si = (SEj +
n∑
i=1
UnFinishSizei ) × Bi/
n∑
i=1
Bi − UnFinishSizei (2)
Improvements on dynamic adjustment mechanism 275
Fi
g.
4
O
ur
G
rid
FT
P
cl
ie
nt
to
ol
Improvements on dynamic adjustment mechanism 277
Fig. 7 Idle times for various
methods
Fig. 8 Idle times for various α
values and block numbers for
various α values
Fig. 9 Idle times for various
LeastSize values and block
numbers for various LeastSize
values
Figure 9 shows that the LeastSize threshold value in our Recursive-Adjustment
method is also an important factor affecting total wait time and block numbers. In this
experiment, we set the α value to 0.5 and tested various LeastSize values. The results
indicate that decreasing the LeastSize threshold value effectively reduces the total
wait time. Although this results in more block numbers, the increase is not excessive.
Figure 9 also indicates we may infer that the Recursive-Adjustment scheme performs
Improvements on dynamic adjustment mechanism 279
7. Donno F, Gaido L, Ghiselli A, Prelz F, Sgaravatto M (2002) DataGrid prototype 1. In: TERENA
Networking Conference, June 2002. http://www.terena.nl/conferences/tnc2002/Papers/p5a2-ghiselli.
pdf
8. Global Grid Forum. http://www.ggf.org/
9. Hoschek W, Jaen-Martinez J, Samar A, Stockinger H, Stockinger K (2000) Data management in an
international data grid project. In: First IEEE/ACM international workshop on grid computing—grid
2000, Bangalore, India, December 2000
10. IBM Red Books, Introduction to Grid Computing with Globus. IBM Press, www.redbooks.ibm.com/
redbooks/pdfs/sg246895.pdf
11. Stockinger H, Samar A, Allcock B, Foster I, Holtman K, Tierney B (2002) File and object replication
in data grids. J Clust Comput 5(3):305–314
12. The Globus Alliance. http://www.globus.org/
13. Vazhkudai S (2003) Enabling the co-allocation of grid data transfers. In: Proceedings of fourth inter-
national workshop on grid computing, November 2003, pp 41–51
14. Vazhkudai S, Schopf J (2003) Using regression techniques to predict large data transfers. Int J High
Perform Comput Appl (IJHPCA) 17:249–268
15. Vazhkudai S, Tuecke S, Foster I (2001) Replica selection in the globus data grid. In: Proceedings of
the 1st international symposium on cluster computing and the grid (CCGRID 2001), May 2001, pp
106–113
16. Vazhkudai S, Schopf J (2002) Predicting sporadic grid data transfers. In: Proceedings of 11th IEEE
international symposium on high performance distributed computing (HPDC-11 ’02) July 2002, pp
188–196
17. Vazhkudai S, Schopf J, Foster I (2002) Predicting the performance of wide area data transfers. In:
Proceedings of the 16th international parallel and distributed processing symposium (IPDPS 2002),
April 2002, pp 34–43
18. Wolski R, Spring N, Hayes J (1999) The network weather service: a distributed resource performance
forecasting service for metacomputing. Future Gener Comput Syst 15(5-6):757–768
19. Yang C-T, Chen C-H, Li K-C, Hsu C-H (2005) Performance analysis of applying replica selection
technology for data grid environments. In: PaCT 2005, lecture notes in computer science, vol 3603,
Springer, September 2005, pp 278–287
20. Zhang X, Freschl J, Schopf J (2003) A performance study of monitoring and information services
for distributed systems. In: Proceedings of 12th IEEE international symposium on high performance
distributed computing (HPDC-12 ’03), August 2003, pp 270–282
Chao-Tung Yang received a BS degree in computer science and information engineering from Tunghai
University, Taichung, Taiwan in 1990, and the MS degree in computer and information science from Na-
tional Chiao Tung University, Hsinchu, Taiwan in 1992. He received the PhD degree in computer and
information science from National Chiao Tung University in July 1996. He won the 1996 Acer Dragon
Award for outstanding PhD Dissertation. He has worked as an associate researcher for ground operations
in the ROCSAT Ground System Section (RGS) of the National Space Program Office (NSPO) in Hsinchu
Science-based Industrial Park since 1996. In August 2001, he joined the faculty of the Department of
Computer Science and Information Engineering at Tunghai University, where he is currently an associate
professor. His researches have been sponsored by Taiwan agencies National Science Council (NSC), Na-
tional Center for High Performance Computing (NCHC), and Ministry of Education. His present research
interests are in grid and cluster computing, parallel and high-performance computing, and internet-based
applications. He is both member of the IEEE Computer Society and ACM.
G. Min et al. (Eds.): ISPA 2006 Ws, LNCS 4331, pp. 579 – 588, 2006. 
© Springer-Verlag Berlin Heidelberg 2006 
Metropolitan-Scale Grid Environment:  
The Implementation and Applications of TIGER Grid* 
Chao-Tung Yang1,**, Tsu-Fen Han1, Wen-Chung Shih2, Wen-Chung Chiang3,  
and Chih-Hung Chang3 
1 High-Performance Computing Laboratory 
Department of Computer Science and Information Engineering 
Tunghai University, Taichung 40704, Taiwan 
ctyang@thu.edu.tw, g942814@thu.edu.tw 
2 Department of Computer and Information Science 
National Chiao Tung University, Hsinchu 30010, Taiwan 
gis90805@cis.nctu.edu.tw 
3 Department of Information Management 
Hsiuping Institute of Technology, Dali, Taichung 412, Taiwan 
wcchiang@mail.hit.edu.tw, chchang@mail.hit.edu.tw 
Abstract. Internet computing and Grid technologies promise to change the way 
we tackle complex problems. Harnessing these new technologies effectively, it 
will transform scientific disciplines ranging from high-energy physics to life 
sciences. This paper describes a metropolitan-scale Grid computing platform 
named TIGER Project (standing for Taichung Integrating Grid Environment 
and Resource), which basically interconnects universities and high schools’ 
Grid computing resources and sharing available resources among them, for 
investigations in system technologies and high performance applications. This 
novel project shows the viability of implementation of such project in a 
metropolitan city. 
1   Introduction 
Grid computing offers a model for solving massive computational problems using 
large numbers of computers arranged as clusters embedded in a distributed 
telecommunications infrastructure [1, 3, 4, 5, 7, 9]. Grid computing has the design 
goal of solving large problems for any single supercomputer, whilst retaining the 
flexibility to work on multiple smaller problems. Grid computing involves sharing 
heterogeneous resources (based on different platforms, hardware/software, computer 
architecture, computer languages), located in different places belonging to different 
administrative domains over a network using open standards [2, 8, 10, 12]. In short, it 
involves vitalizing computing resources. Numerous next generations, large scale 
advanced applications are being developed on innovative technology infrastructure. 
The development of new types of information technology infrastructure continues to 
                                                          
*
 This work is supported in part by National Science Council, Taiwan R.O.C., under grants 
no. NSC95-2213-E-029-004 and NSC95-2218-E-007-025. 
**
 Corresponding author. 
 Metropolitan-Scale Grid Environment 581 
different speed and total storage of more than 2TB in 2005. All these institutions are 
in Taiwan, and each is at least 10 Km from THU. Our all machines have Globus 4.0.1 
or above installed. Figure 2 shows the status of various ports used for grid testbed in 
one monitor page. The detail hardware specification of each educational unit is listed 
in Table 1. 
Internet
THU
Li-Zen High 
School (LZ)
HITCeleron 900 MHz
256 MB RAM
60 GB HD
AMD Athlon(tm) XP 2400+
1024 MB RAM
120 GB HD
Pentium 4 2.8 GHz
512 MB RAM
80 GB HD
PU
Da-Li High 
School (DL)Athlon MP 2000 MHz *21 GB RAM
60 GB HD
Pentium 4 1.8 GHZ
128 MB RAM
40 GB HD
Pentium 4 2.5 GHZ
512 MB RAM
80 GB HD
 
Fig. 1. TIGER Grid Platform 
 
Fig. 2. The TIGER monitor page for status of various ports 
 
 Metropolitan-Scale Grid Environment 583 
3.1   Grid Resource Broker 
As Grid Computing becomes a reality, there is a need to manage and monitor 
available resources world-wide, as well as a need to convey these resources to 
everyday users. This work describes a resource broker as shown in Figure 4 whose 
main function is to match available resources to user needs. The resource broker 
provides a uniform interface for accessing available and appropriate resources via 
user credentials [15, 16]. We also focus on providing approximate measurement 
models for network-related information using NWS for future scheduling and 
benchmarking. 
We first propose a network measurement model for gathering network-related 
information (including bandwidth, latency, forecasting, error rates, etc.) without 
generating excessive system overhead. Second, we consider inaccuracies in real-
world network values in generating an approximation values for future use. We 
constructed a grid platform using Globus Toolkit that integrates the resources of five 
schools in Taichung integrated grid environment resources (TIGER). The resource 
broker runs on top of TIGER. Therefore, it provides security and current information 
about available resources and serves as a link to the diverse systems available in the 
Grid. 
 
 
Fig. 4. The computational grid resource broker 
3.2   Network Information Model 
Network Weather Service (NWS) can measure point-to-point network bandwidth and 
latency that may be important for grid scheduling and load balancing [6]. NWS 
detects all network states during time periods selected by the user. Because this kind 
of site-to-site measurement results in N(N-1) network measurement processes, the 
 Metropolitan-Scale Grid Environment 585 
 
Fig. 6. The Gridftp client tool 
3.4   Bioinformatics Grid Applications 
Biology databases are diversified and massive; as a result, researchers must compare 
each sequence with a vast number of other sequences efficiently. A number of 
programs such as Blast, FASTA, ClustalW, etc., have been written to rapidly search a 
database for a query sequence. Comparison, whether of structural features or protein 
sequences, lies in the heart of bioinformatics. These activities require high-speed, 
high-performance computing power to search through and analyze huge amounts of 
data, and industrial-strength databases perform a wide range of data-intensive 
computing functions. The emergence of Grid computing and Cluster computing 
would meet these requirement. Biological data exists all over the world as various 
web services, which help biologists to search and extract useful information. The data 
formats produced from various biology tools are heterogeneous. It needs powerful 
tools to handle this issue. The process of information integration of heterogeneous 
biological data is complex and difficult. 
This work also describes an approach to solve this problem by using XML 
technologies. We implement an experimental distributed computing application for 
bioinformatics [14]. Which consist of basic high-performance computing environment 
(Grid and PC Cluster system), multiple interface of user portal to provide a useful 
graphical interface as shown in Figure 7 for biologists who are not specialized in IT to 
be able to benefit directly from the usage of high-performance technology, and a 
translation tool for biology data to convert them into XML format. 
3.5   Hybrid Parallel Loop Scheduling 
For self-scheduling of parallel loops, we propose a general approach called HPLS 
(Hybrid Parallel Loop Scheduling) and conduct the experimental results on the TIGER. 
This approach utilizes performance functions to estimate the performance ratio of each   
 
 Metropolitan-Scale Grid Environment 587 
4   Conclusions 
The rise of grid computing and its application discipline brings to computer science 
faculty members new opportunities and challenges, both in education and in research. 
In this paper, we described a metropolitan-scale Grid computing platform named 
TIGER Project (standing for Taichung Integrating Grid Environment and Resource), 
which basically interconnects universities and high schools’ Grid computing 
resources and sharing available resources among them, for investigations in system 
technologies and high performance applications.. In addition, we also describe the on 
going related projects and previous results. With interdisciplinary collaboration and 
using this platform, researchers, faculties and students from computer science, 
chemistry, physics, biology and engineering programs can interact among themselves. 
References 
1. The Globus Project, http://www.globus.org/ 
2. MPICH-G2, http://www.hpclab.niu.edu/mpi/ 
3. TIGER Project, http://gamma2.hpc.csie.thu.edu.tw/ganglia/, 2006. 
4. The Globus Alliance, http://www.globus.org/ 
5. The National Science Foundation Middleware Initiative, http://www.nsf-middleware.org/ 
6. Network Weather Service, http://nws.cs.ucsb.edu/ 
7. I. Foster et al., The Physiology of the Grid: An Open Grid Services Architecture for 
Distributed Systems Integration, available at http://www.globus.org/research/papers/ 
ogsa.pdf 
8. Ian Foster, Carl Kesselman, The Grid 2: Blueprint for a New Computing Infrastructure, 
Morgan Kaufmann Publishers Inc., San Francisco, CA, 2003. 
9. William Gropp , Ewing Lusk , Anthony Skjellum, Using MPI (2nd ed.): portable parallel 
programming with the message-passing interface, MIT Press, Cambridge, MA, 1999. 
10. H. Jordan, and G. Alaghband, Fundamentals of Parallel Processing; Prentice Hall, 2003. 
11. Wen-Chung Shih, Chao-Tung Yang, and Shian-Shyong Tseng, “A Hybrid Parallel Loop 
Scheduling Scheme on Grid Environments,” Grid and Cooperative Computing - GCC 
2005: Fourth International Conference, Lecture Notes in Computer Science, vol. 3795, pp. 
374-385, Springer-Verlag, November 2005. 
12. B. Wilkinson and M. Allen, Parallel Programming: Techniques and Applications Using 
Networked Workstations and Parallel Computers, 2nd Edition; Prentice Hall, 2005. 
13. Chao-Tung Yang, Chun-Hsiang Chen, Kuan-Ching Li, and Ching-Hsien Hsu, 
“Performance Analysis of Applying Replica Selection Technology for Data Grid 
Environments,” PaCT 2005, Lecture Notes in Computer Science, vol. 3606, pp. 278-287, 
Springer-Verlag, September 2005. 
14. Chao-Tung Yang, Yu-Lun Kuo, Kuan-Ching Li, and Jean-Luc Gaudiot, “On Design of 
Cluster and Grid Computing Environments for Bioinformatics Applications,” Distributed 
Computing - IWDC 2004: 6th International Workshop, Lecture Notes in Computer 
Science, Springer-Verlag, Arunabha Sen, Nabanita Das, Sajal K. Das, et al. (Eds.), 
Kolkata, India, vol. 3326, pp. 82-87, Dec. 27-30, 2004. 
15. Chao-Tung Yang, Chuan-Lin Lai, Po-Chi Shih, and Kuan-Ching Li, “A Resource Broker 
for Computing Nodes Selection in Grid Environments,” Grid and Cooperative Computing 
- GCC 2004: Third International Conference, Lecture Notes in Computer Science, 
Springer-Verlag, Hai Jin, Yi Pan, Nong Xiao (Eds.), vol. 3251, pp. 931-934, Oct. 2004. 
 H. Zhuge and G.C. Fox  (Eds.): GCC  2005,  LNCS 3795, pp. 287 – 299, 2005. 
© Springer-Verlag Berlin Heidelberg 2005 
An Efficient Network Information Model Using NWS  
for Grid Computing Environments* 
Chao-Tung Yang1,** , Po-Chi Shih1,2, Sung-Yi Chen1, and Wen-Chung Shih3 
1 High-Performance Computing Laboratory, Department of Computer Science  
and Information Engineering, Tunghai University,  
Taichung, 40704 Taiwan, R.O.C. 
ctyang@thu.edu.tw 
2 Department of Computer Science, National Tsing Hua University,  
Hsinchu, 30013 Taiwan, R.O.C. 
shedoh@gmail.com 
3 Department of Computer and Information Science, National Chiao Tung University,  
Hsinchu 300, Taiwan, R.O.C. 
gis90805@cis.nctu.edu.tw 
Abstract. Grid computing technologies enable large-scale aggregation and 
sharing of resources via wide-area networks focused on sharing computational, 
data, and other resources to form general-purpose services for users. In this 
paper, we address network information gathering and focus on providing 
approximate measurement models for network-related information using 
Network Weather Service (NWS) for future scheduling and benchmarking. We 
propose a network measurement model for gathering network-related 
information including bandwidth, latency, forecasting, error rates, etc., without 
generating excessive system overhead. We consider inaccuracies in real-world 
network values in generating approximation values for future use. 
Keywords: Network information, NWS, Globus, Grid computing, Bandwidth. 
1   Introduction 
Grid computing is commonly used by scientists and researchers to solve complex 
problems in parallel and distributed paradigm [1, 2, 3, 4, 5, 12, 13]. A key issue in 
Grid computing environments is providing a centralized interface that enables users to 
make use of various resources easily. Grid computing technologies includes many 
elements such as user authentication, job description, information gathering, job 
scheduling, and resource dispatching. In this work, we concerned with information 
gathering. Without precise information, no scheduling strategy or algorithm will work 
well. 
                                                          
*
  The authors would like to acknowledge the National Center for High-Performance 
Computing for sponsoring the Taiwan UniGrid project, under the national project “Taiwan 
Knowledge Innovation National Grid”. This work is supported in part by National Science 
Council Taiwan, under grants no. NSC93-2213-E-029-026, NSC94-2213-E-029-002, and 
NSC93-2119-M-002-004. 
**
 Corresponding author. 
 An Efficient Network Information Model Using NWS 289 
 
• Security Services: Grid Security Infrastructure (GSI) 
• Data Movement and Management: Global Access to Secondary Storage (GASS) 
and GridFTP. 
GRAM is designed to provide a single common protocol and API for requesting 
and using remote system resources, by providing a uniform, flexible interface to local 
job scheduling systems. The Grid Security Infrastructure (GSI) provides mutual 
authentication of both users and remote resources using GSI (Grid-wide) PKI-based 
identities. GRAM provides a simple authorization mechanism based on GSI identities 
and a mechanism to map GSI identities to local user accounts. 
MDS is designed to provide a standard mechanism for publishing and discovering 
resource status and configuration information. It provides a uniform, flexible interface 
to data collected by lower-level information providers. It has a decentralized structure 
that allows it to scale, and it can handle static (e.g., OS, CPU types, and system 
architectures) or dynamic data (e.g., disk availability, memory availability, and 
loading). A project can also restrict access to data by combining GSI (Grid Security 
Infrastructure) credentials and authorization features provided by MDS. 
GridFTP [3] is a high-performance, secure, and reliable data transfer protocol 
optimized for high-bandwidth wide-area networks. The GridFTP protocol is based on 
FTP, the highly-popular Internet File Transfer protocol. 
2.2   Network Weather Service 
The Network Weather Service, though not targeted on clusters, is a distributed system 
that periodically monitors and dynamically forecasts the performance that various 
network and computational resources can deliver over a given time interval. The 
service operates a distributed set of performance sensors (network monitors, CPU 
monitors, etc.) from which it gathers system condition information. It then uses 
numerical models to generate forecasts of what the conditions will be for a given time 
period. It also uses mathematical models to forecast each condition and the Mean 
Absolute Error (MAE) and Mean Square Error (MSE) rates. NWS is a widely used 
measurement tool for Grid environments. Studies on topics, such as load balancing, 
scheduling, brokering, replica selection, etc., are available [9, 11]. 
3   Network Information Model 
We constructed a network measurement model to solve a complete point-to-point 
network measurement problem. Consider the twelve-node grid environment shown in 
Figure 1. The lines linking the nodes represent site-to-site network measurement. A 
“node” or “site” here represents a single machines or a personal computer. This model 
is often used for local grids or cluster environments when the scale is not too large. In 
large-scale grid environments this kind of architecture results in excessive bandwidth 
overhead. In order to reduce the total number of times of NWS measurement, we 
proposed the “domain” concept shown in Figure 2 to partition the network 
measurement environment. 
 An Efficient Network Information Model Using NWS 291 
 
Here we discuss how to select borders. One way is to conduct an all-pair network 
test and select the worst one in each domain. However, this may not work in a real 
grid environment because the organizations owning the domains may each control 
their hosts according to different policies. Another way is to let domain administrators 
select borders according to network topology or architecture (maybe select the host in 
the deepest topology away from the router who connects to WLAN). These two 
methods are not smart and not scalable, so we propose an alternative way to select 
borders. 
1. When the Grid is first being built up, pick some domains (perhaps 2~4 if the 
total number of domains is more than that) to start with. Then, perform one of 
the above ways to select borders and save them in a border list. 
2. The other domains (not selected in step one) should select a border one by one 
according to network topology, or test all hosts in each domain against each 
border in border list to select one and add to the list. We take Figure 3 for 
example. The top and right domains have already selected their borders by step 
1. Now we take left domain to perform step 2.We perform network tests to know 
the bandwidth of R1, R2, … , R5 and T1, T2, … ,T5. Then select the one who has 
the minimal bandwidth (minimize Rn+Tn for n = 1 to 5) 
3. Repeat step 2 until every domain has found a best border. 
This simplifies construction complexity to avoid all pair network testing and makes 
the grid environment scalable for adding new domains. When a new domain will join 
the existing grid, only step 3 need be performed.  
 
Fig. 3. Border selection diagram 
About the second question, in order to obtain the all-pair network values lost in out 
model, we use a few measured values from NWS to estimate the lost value. 
Figure 4 shows an example of a network estimation model. The line connecting 
alpha1 and lz01 is part of the central-domain, which we called the Bridge in our 
experimental environment. The solid lines mean that our domain model has gotten the 
 An Efficient Network Information Model Using NWS 293 
 
Use = CountIf(
flu
avg
avgij P
inB
inBkL
>
−
_
_][ ) > (Nflu×Pvaflu), k = 1, …, Nflu., 
ij∀  in left domain. 
(1) 
The function CountIf is used to count the times if the condition in brackets is 
true. 
avg
avgij
inB
inBkL
_
_][ −
 is the percentage of bandwidth down. Nflu×Pvaflu = 8 is set as 
our default value. For example, the formula Use is true if the percentage of bandwidth 
down large than 30%  is more than 8 times over last 10 network values measured by 
NWS. The remaining bandwidth usage is then calculated by ignoring the maximal and 
minimal bandwidth values via sorting Lij list. 
Brem = 
2
][
1
2
−
∑−
=
flu
N
k
ij
N
kL
flu
 
(2) 
Finally, the target bandwidth is calculated as follows: 
Btar = α×× avg
avg
rem outB
inB
B
_
_
 (3) 
The symbol α here indicates a value converted from internet bandwidth to LAN 
and is used throughout this paper. 
Case 2 
We assumed that bandwidth use occurs within organizations but not with other 
members of the domain (machine pc1), as shown in Figure 6. Figure 7 shows the 
general topology of network architecture. The network transfer will go through the 
top switch. So we claim that the target bandwidth almost follows bridge bandwidth. 
  
Fig. 6. Bandwidth usage in Case 2  Fig. 7. Topology for Case 2 
Case 3 
We assumed that bandwidth use occurs between two domains, as shown in Figure 8. 
Bandwidth use between alpha2 and lz03 will affect the available bridge bandwidth. So 
we claim that the target bandwidth almost follows bridge bandwidth just like Case 2. 
 An Efficient Network Information Model Using NWS 295 
 
• GridProxyInit, which creates a limited-life proxy for authorized users to access 
grid resources.  
• GridConfigureDialog, which uses the CoG Kit UITool to enable users to 
configure the number of process and the grid server host name.  
• GridJob, which creates GramJob instances. This class represents a simple gram 
job, allowing submitting jobs to a gatekeeper, canceling them, sending signal 
commands, and registering and unregistering callbacks. 
• GetRSL, RSL provides a common interchange language for describing 
resources. The various Globus Resource Management architecture components 
manipulate RSL strings to perform their management functions in cooperation 
with the other system components. GetRSL combines RSL strings. 
• JobMonitor, uses two parameters, Gridjob and RSL to start GlobusRun and 
monitor the job process.  
• GlobusRun is a factory method for creating and exporting credentials, and 
submitting jobs to the grid server and receiving the responses from it.  
We also developed some APIs for our system. For example, ProxyDestroy, which 
destroys CA files to protect the grid system from the application site we can configure 
the machinefile for the grid system. 
 
Fig. 10. Domain information is used to show network information 
 
Fig. 11. lz-domain is selected 
 An Efficient Network Information Model Using NWS 297 
 
prediction result of our algorithm. The curve of target bandwidth fits the actual 
bandwidth as our expectation. But there exists some delay caused by Nflu times sliding 
window detection period. So we summarize that our algorithm can predict the 
network behavior with a little delay. 
Case 2 
In this case, the 800MB file was transferred from alpha2 to an external server called 
pc1, as shown in Figure 15. The connection speeds shown for lz01~alpha1 and 
alpha1~alpha2 were obtained by NWS; besides, lz01~alpha2 was measured for 
comparison. In this case, the curve of lz01~alpha1 is almost the same as lz01~alpha2. 
This result fits our claim that the target bandwidth goes with the border bandwidth. 
Case 3 
In this case, the 800MB file was transferred from alpha2 to lz02. The connection 
speed between lz01~alpha1 shown in Figure 16 was obtained using NWS. The others 
were measured for experimental comparison. The situation of lz01~alpha2 can be 
substitute with lz01~alpha1 by experiment result. The situation of alpha1~lz02 can be 
substituted with alpha1~lz01 by the two situation above, and lz02~alpha2 can be 
substituted with lz01~alpha1. The experimental result confirms the inference above. 
 
CASE 2
0
10
20
30
40
50
60
70
80
50 65 80 95 11
0
12
5
14
0
15
5
17
0
18
5
20
0
21
5
23
0
24
5
Time Stamp = 5 sec
Ba
n
dw
id
th
 (M
bi
ts
/s
e
c)
lz01_alpha1
lz01_alpha2
pc1_alpha1
pc1_alpha2
alpha1_alpha2
CASE 3
0
5
10
15
20
25
30
35
50 65 80 95 11
0
12
5
14
0
15
5
17
0
18
5
20
0
21
5
23
0
24
5
Time Stamp = 5 sec
Ba
n
dw
id
th
 
(M
bi
ts
/s
ec
)
lz01_alpha2
lz02_alpha2
lz01_alpha1
lz02_alpha1
 
Fig. 15. Experimental results for transferring 
an 800MB file from alpha2 to the pc1 
Fig. 16. Experimental results for transferring 
an 800MB file from alpha2 to lz02 
Through the above three experiments, we can summarize the result under 3 cases. 
Case 1: the target bandwidth can be predicted by our algorithm within acceptable 
error and delay. Case 2: the bridge bandwidth can be use to substituted with target 
bandwidth. Case 3: almost likes Case 2. 
The second experiment tested NWS measurement values for various packet 
(frame) sizes. There exists an inaccuracy problem with NWS. We want to know if we 
can improve the accuracy by adjusting some setting of NWS. The most effective 
parameter is the packet size used by NWS sensor. The results of varying packet sizes 
shown in Figures 17 and 18 indicate that the measured bandwidth increased as packet 
size increased and approached the maximum bandwidth. Our test indicated that a 512-
Kbits packet size is most suitable, which can avoid wasting too much bandwidth in 
our grid environment. 
 An Efficient Network Information Model Using NWS 299 
 
7. IBM Redbooks, “Introduction to Grid Computing with Globus”, http://www.redbooks. 
ibm.com/redbooks/pdfs/sg246895.pdf 
8. Java CoG Kits, http://www.cogkit.org/ 
9. Network Weather Service, http://nws.cs.ucsb.edu/ 
10. The Globus Alliance, http://www.globus.org/ 
11. R. Wolski, N. Spring and J. Hayes, “The Network Weather Service: A Distributed 
Resource Performance Forecasting Service for Metacomputing,” Future Generation 
Computer Systems, 15(5-6):757-768, 1999. 
12. Chao-Tung Yang, Chuan-Lin Lai, Po-Chi Shih, and Kuan-Ching Li, “A Resource Broker 
for Computing Nodes Selection in Grid Environments,” Grid and Cooperative Computing - 
GCC 2004: Third International Conference, Lecture Notes in Computer Science, Springer-
Verlag, Hai Jin, Yi Pan, Nong Xiao (Eds.), vol. 3251, pp. 931-934, Oct. 2004. 
13. Chao-Tung Yang, Po-Chi Shih, Kuan-Ching Li, “A High-Performance Computational 
Resource Broker for Grid Computing Environments,” Proceedings of the International 
Conference on Advanced Information Networking and Applications (AINA 2005) INA’2005 
The First International Workshop on Information Networking and Application, vol. 2, pp. 
333-336, Tamkang University, Taipei, Taiwan, March 28-30, 2005. 
14. X. Zhang, J. Freschl, and J. Schopf, “A Performance Study of Monitoring and Information 
Services for Distributed Systems”, Proceedings of 12th IEEE International Symposium on 
High Performance Distributed Computing (HPDC-12 ‘03), pp. 270-282, August 2003. 
15. H. Zhuge, “The Future Interconnection Environment,” IEEE Computer, 38(4): 27-33, 
2005. 
 Performance Analysis of Applying Replica Selection Technology 279 
 
large and powerful self-managing virtual computer, which is a huge collection of 
connected heterogeneous systems. The emerging mechanism is resources sharing 
through the availability of high bandwidth network. The “computational Grid” is a 
term used to provider the users a better performance, especially in terms of speed and 
throughput. The term “Data Grid” aggregate distributed resources to produce results 
for large size problems. Most of these Data Grid applications are executed 
simultaneously and access a large number of shared data files in Grid. 
In certain data intensive scientific applications, such as high-energy physics, 
bioinformatics applications and astrophysical virtual observatory, we confront with 
huge amount of data. A Data Grid provides two essential basic services, which are a 
secure, reliable, efficient data transport protocol and replica management [2]. The 
high-speed transport protocol, GridFTP, extends the popular FTP protocol with some 
new features required for Data Grid applications, such as partial file transfer and 
third-party transfer [5]. The replica management service take advantage of replica 
catalog with GridFTP transfer to provide for the creation, registration, location and 
management of data replicas [1]. 
In this paper, we build a Grid environment based on three existing PC Cluster 
environments and perform performance analysis of data transfers using GridFTP 
protocol over these systems. In addition, based on experimental results, it is proposed 
a cost model to pick the best replica, in real and dynamic network situations. In this 
paper, we propose a cost model according to the three significant parameters: network 
bandwidth, CPU load and I/O state. Although the network situation is constantly 
changing and the storage equipments are busy or idle, we can use our cost model to 
determine the best replica immediately. The replica selection can be conducted 
accurately because our cost model is based on the system monitoring information that 
update continuously. 
2   Background Review 
2.1   Globus Toolkit 
The Globus Project [10, 11, 12] provides software tools that make it easier to build 
computational Grids and Grid-based applications. These tools are collectively called 
The Globus Toolkit. The Globus Toolkit is used by many organizations to build 
computational Grids that can support their applications. The composition of the 
Globus Toolkit can be pictured as three pillars: Resource Management, Information 
Services, and Data Management. Each pillar represents a primary component of the 
Globus Toolkit and makes use of a common foundation of security. GRAM 
implements a resource management protocol, MDS implements an information 
services protocol, and GridFTP implements a data transfer protocol. They all use the 
GSI security protocol at the connection layer [8, 11, 12, 13]. 
2.2   NWS 
The Network Weather Service (NWS) [16] is a generalized and distributed 
monitoring system for producing short-term performance forecasts based on historical 
performance measurements. The goal of the system is to dynamically characterize and 
 Performance Analysis of Applying Replica Selection Technology 281 
 
 
Fig. 1. Replica selection scenario 
3.2   System Factors 
We propose a replication selection model for Data Grid environments. In this 
environment, we can treat a biological database as a replica of Data Grid. When we 
execute large-scale data intensive applications in these environments, a site has both 
data stores and computational capabilities. To determine the best database from many 
of same replications is a significant problem. In our model, we consider three system 
factors that affect the replica selection: 
− Network bandwidth: Network bandwidth is one of the most significant factors in 
Data Grid, since the size of a data file in Data Grid environment is usually very 
large. In other words, the data file transfer time is tightly dependent on network 
bandwidth situations. Because network bandwidth is unstable and dynamic factor, 
we should often measure and predict it as most accurate as possible. NWS 
(Network Weather Service) is a powerful toolkit for such purpose, 
− CPU load: a Grid platform consists of a number of heterogeneous systems, built 
with different system architectures, e.g., cluster platforms, supercomputers, PCs. 
CPU load is a dynamic system factor, and if the CPU load of a system is heavy, it 
will certainly affect the data file download process from this site. The measurement 
of CPU status is done through the Globus Toolkit / MDS, 
− I/O state: Data Grid nodes consist of different heterogeneous storage systems. The 
size of data in Data Grid is huge. If I/O state of the site that we would like to 
download file from is very busy, it will directly affect the data transfer 
performance. We measure the I/O state using sysstat utilities. 
3.3   Replica Selection Cost Model 
The target function of a cost model for distributed and replicated data storage is the 
score of information from information service. We listed different influencing factors 
 Performance Analysis of Applying Replica Selection Technology 283 
 
Figure 2 shows the hardware and network configuration of our Data Grid testbed.  
The THU site is located in Tunghai University, Taichung City; Li-Zen site is located 
at Li-Zen High School, Taichung County, while HIT site is located in Hsiuping 
Institute of Technology, Taichung County, all in Taiwan. 
 
Fig. 2. Our Data Grid testbed 
4.1   FTP Versus GridFTP 
The Globus Project surveyed available protocols and technologies, implemented some 
prototypes, and settled on using FTP and its existing extensions as a base, and then 
extending it again to add missing required functionality. The Globus alliance propose 
a common data transfer and access protocol named GridFTP that provides secure, 
efficient data movement in Grid environments. This protocol, which extends the 
standard FTP protocol, provides a superset of the features offered by the various Grid 
storage systems currently in use. 
In Grid environments, access to distributed data is typically as important as access 
to distributed computational resources. Distributed scientific and engineering 
applications require transfers of large amounts of data between storage systems, and 
access to large amounts of data by many geographically distributed applications and 
users for analyzing and visualization. We note that GridFTP protocol is extended 
from FTP protocol, and suitable for Grid environments. Figure 3 shows the 
performance of FTP and GridFTP by transferring four different file sizes. We 
transferred these files (256, 512, 1024 and 2048 megabytes) from THU site alpha01 to 
HIT site gridhit3 in our first experiment. 
4.2   GridFTP with Parallel Data Transfer 
Using multiple TCP streams can improve aggregate bandwidth over using a single 
TCP stream in WAN environments. We apply this feature of GridFTP protocol to 
transfer different sizes files in Data Grid environments. GridFTP (as well as normal 
FTP) defines multiple wire protocols, or MODES, for the data channel. Most normal 
 Performance Analysis of Applying Replica Selection Technology 285 
 
performance of GridFTP transferring 256, 512, 1024 and 2048 megabytes files with 1, 
2, 4, 8 and 16 TCP streams from THU site alpha02 to Li-Zen site lz04. According to 
the experiment result, we observed that parallel data transfer technique showed better 
performance for larger file sizes. Parallel data transfer really improves aggregate 
bandwidth, with the establishment of multiple data channels. 
4.3   Replica Selection Cost Model 
According to the replica selection scenario in 3.1, a user logins the local site THU site 
alpha1, and specifies the characteristics of the desired data and passes this attribute 
description to replica catalog server. The replica catalog server queries its database 
and produces a list of logical files that contain data with the specified characteristics. 
The replica catalog server returns the information of physical locations for all 
registered replicas of the desired logical files. In this experiment, there is only one 
logical file, file-a, conform to user’s request, and the size of file-a is 1024 megabytes.  
Table 1. The value of replica selection cost model and file transfer time 
alpha1 Alpha4 hit0 lz02 
BW
jiP−  88.25 29.09 20.91 
CPU
jP  98.67 99.56 98.33 
OI
jP
/
 2.88 100.00 3.78 
Replica Selection Cost model 80.76 43.228 26.939 
Practical Data transfer time 101.9 128.09 164.99 
  
(a)     (b) 
Fig. 5. GUI of replica selection cost model program 
 Performance Analysis of Applying Replica Selection Technology 287 
 
2. B. Allcock, J. Bester, J. Bresnahan, A. Chervenak, I. Foster, C. Kesselman, S. Meder, V. 
Nefedova, D. Quesnel, S. Tuecke, “Secure, Efficient Data Transport and Replica 
Management for High-Performance Data-Intensive Computing,” IEEE Mass Storage 
Conference, 2001. 
3. B. Allcock, S. Tuecke, I. Foster, A. Chervenak, and C. Kesselman, “Protocols and Services 
for Distributed Data-Intensive Science,” ACAT2000 Proceedings, pp. 161-163, 2000. 
4. K. Czajkowski, S. Fitzgerald, I. Foster and C. Kesselman, “Grid Information Services for 
Distributed Resource Sharing,” Proceedings of the Tenth IEEE International Symposium 
on High-Performance Distributed Computing (HPDC-10), IEEE CS Press, August 2001. 
5. K. Czajkowski, I. Foster, N. Karonis, C. Kesselman, S. Martin, W. Smith and S. Tuecke, 
“A Resource Management Architecture for Metacomputing Systems,” Proc. IPPS/SPDP 
‘98 Workshop on Job Scheduling Strategies for Parallel Processing, pp. 62-82, 1998. 
6. R. L. De, C. Costa and S. Lifschitz, “Database Allocation Strategies for Parallel BLAST 
Evaluation on Clusters”, Proceedings of the Distributed and Parallel Databases, Vol. 13, 
Issue1, pp. 99-127, Hingham, MA, USA, January 2003. 
7. I. Foster, “The Grid: A New Infrastructure for 21st Century Science,” Physics Today, 
55(2):42-47, 2002. 
8. I. Foster, C. Kesselman, “Globus: A Metacomputing Infrastructure Toolkit,” Intl J. 
Supercomputer Applications, 11(2):115-128, 1997. 
9. I. Foster and C. Kesselman, The Grid: Blueprint for a New Computing Infrastructure, 
Morgan-Kaufmann, 1999. 
10. I. Foster, C. Kesselman and S. Tuecke, “The Anatomy of the Grid: Enabling Scalable 
Virtual Organizations,” Intl J. Supercomputer Applications, 15(3), 2001. 
11. Global Grid Forum, http://www.ggf.org/ 
12. The Globus Project, http://www.globus.org/ 
13. Introduction to Grid Computing with Globus, http://www.ibm.com/redbooks/ 
14. SETI@home: Search for Extraterrestrial Intelligence at home, http://setiathome.ssl. 
berkeley. edu/ 
15. SYSSTAT utilities home page, http://perso.wanadoo.fr/sebastien.godard/ 
16. R. Wolski, N. Spring and J. Hayes, “The Network Weather Service: A Distributed 
Resource Performance Forecasting Service for Metacomputing,” Journal of Future 
Generation Computing Systems, Vol. 15, No. 5-6, pp. 757-768, October 1999. 
17. X. Zhang, J. Freschl, and J. Schopf, “A Performance Study of Monitoring and Information 
Services for Distributed Systems,” Proceedings of HPDC, August 2003. 
On Design of Cluster and Grid Computing Environment Toolkit 83 
There are available parallel versions of bioinformatics applications [1], which can 
be installed and performed on PC-based clusters; for example, HMMer [11], FASTA 
[12], mpiBLAST [13], ClustalW-MPI [2], Wrapping-up BLAST [3], and TREE-
PUZZLE [4], among others. Using these parallel versions to work on sequence align-
ment always can save much time and cost. Especially for some biotechnology and 
high-tech companies, PC-based clusters are already replacing mainframe systems or 
supercomputers, by saving large amount of hardware cost. 
Recent advances in computer technology, especially in the development of grid 
tools, make them excellent option for performing parallel application programs and 
resources usage. Computational grids enable sharing a wide variety of geographically 
distributed resources and allow selection and aggregation of distributed resources 
across multiple organizations for solving large-scale computational and date intensive 
problems in science. BioGrid is a large-scale distributed computing environment, 
including computer systems, storage systems, and other devices. BioGrid system can 
improve performance more than parallel computing on PC-based clusters. There are 
several BioGrid research projects under development in the world, such as, EuroGrid 
BioGrid [15], Asia Pacific BioGrid [14], UK BioGrid [17, 18], North Carolina Bi-
oGrid [19, 20], Osaka University BioGrid, Indiana University BioGrid, Minnesota 
University [6] and Singapore BioGrid [2], and others.
In this paper, FASTA bioinformatics application software is added and ported to a 
grid system, and recompiled with MPICH-G2. There are several other MPI based 
biology software applications that can be easily added to a grid system, for example, 
mpiBLAST [13], ClustalW [2] etc. BioGrid systems are important and necessary, in 
order to accelerate the sequence alignment experiment process. The use of parallel 
version softwares and cluster systems are excellent good way to achieve high effi-
ciency and with reduced cost. As we already know, bioinformatics tools can speed up, 
by several orders of magnitude, analysis the large-scale sequence data, especially 
about sequence alignment.
The organization of this research paper is as follows. In section 2, we introduce the 
FASTA software and our hardware and software configurations, also, the user inter-
face. Also in this section, it is discussed the proposed grid computing environment 
built over multiple Linux PC Clusters by using Globus Toolkit and SUN Grid Engine. 
The experimental results using FASTA software on this grid environment is discussed 
in section 3, and finally, conclusions are discussed in section 4. 
2   System Environments and User Interface 
We built three PC-based clusters to form a grid computing environment. Each master 
node executes SGE master daemon. SGE execute daemon to run, manage, and moni-
tor incoming job. Each slave node executes SGE slave daemon to execute incoming 
job only. These PC-based clusters are located in three different sites of a building to 
form a BioGrid system, as shown in Figure 1, in the High-Performance Computing 
Laboratory, Department of Information Engineering and Sciences at Tunghai Univer-
sity, Taiwan. We use SCMS monitor to monitor our grid system and the status of our 
BioGrid system are shown in Figures 2 and 3. 
On Design of Cluster and Grid Computing Environment Toolkit 85 
Fig. 4. Query sequence and database selection           Fig. 5. FASTA function selection 
3   Experimental Results 
We performed parallel version of FASTA application program in our BioGrid  
environment. In order to make the software execute flawlessly in the grid structure; 
we have edited it using MPICH-G2 to success in its operations. During this test, we 
have used two databases, namely tremble_new.fas and trembl.fas. The size of the 
former database is 23MB, while the latter one is of 153MB.  
Figure 6 shows the result, by gradually increasing the number of processors, we 
can observe that the time consumed for execution decreases significantly. As trem-
ble_new.fas is smaller, the time consumed does not change, once a specific number 
of processors are reached.  
When the larger database is used for experiment, we would realize the time con-
sumed does go down remarkably while more processors are added. Therefore, the 
larger and more complex the database becomes, better is the BioGrid environment’s 
performance.
During our experimental test, we have used np3 (2 CPUs are used), np5 (4 CPUs 
are used), np7 (6 CPUs are used), np9 (8 CPUs are used), np11 (10 CPUs are used) 
and np13 (12 CPUs are used) for the order mapping. The performance analyses are 
shown in Figures 6 and 7. 
 	

 
 







     

 








Fig. 6. Execution time of FASTA using 2 to 12 processors (trembl_new.fas) 
On Design of Cluster and Grid Computing Environment Toolkit 87 
6. Global Grid Forum. Available form: http://www.ggf.org 
7. I. Foster, C. Kesselman, 1999. The Grid: Blueprint for a New Computing Infrastructure.
Morgan Kaufmann. 
8. I. Foster, 2002. The Grid: A New Infrastructure for 21st Century Science. Physics Today,
55(2), pp. 42-47. 
9. SETI@home: Search for Extraterrestrial Intelligence at home. Available form: 
http://setiathome.ssl.berkeley.edu/ 
10. The Globus Project. Available form: http://www.globus.org/ 
11. HMMER: profile HMMs for protein sequence analysis. Available form: 
http://hmmer.wustl.edu/ 
12. The FASTA main page. Available form: ftp://ftp.virginia.edu/pub/fasta/ 
13. mpiBLAST main page. Available form: http://mpiblast.lanl.gov/index.html
14. Asia Pacific BioGRID Initiative, Available form: http://www.apbionet.org/apbiogrid/ 
15. BioGRID - European grid for molecular biology. Available form: http://biogrid.icm.edu.pl/ 
16. BioGrid: Construction of a supercomputer network main page. Available form: 
http://www.biogrid.jp/ 
17. L. Moreau and et. al. (2003) On the Use of Agents in a Bioinformatics Grid, In S. Lee, S. 
Sekguchi, S. Matsuoka, and M. Sato, editors, Proc. of the 3rd IEEE/ACM CCGRID 2003.
IEEE Computer Society. 
18. The UK MyGrid project site. Available form: http://www.mygrid.org.uk
19. North Carolina Bioinformatics Grid (BioGrid) web site. Available form: 
http://www.ncbiogrid.org 
20. North Carolina Genomics and Bioinformatics Consortium web site. Available form: 
http://www.ncgbc.org 
 
 
參與國際會議可以讓國內的教師或研究人員或是一般的研究生以及大學生更了解目
前國際上相關研究的發展現況與未來趨勢，也可以拓展其國際視野，對於國內的教師或
研究人員或一般的研究生以及大學生幫助相當大，因此國內教師或研究人員、一般的研
究生以及大學生應該積極參與大型的國際會議。 
 
另外，若是國內學術單位有爭取到在國內舉辦大型國際會議的機會，不僅能促進國
際學術交流與合作，還能夠開拓國內的教師或研究人員、一般的研究生以及大學生的視
野，提昇學校、國家的國際能見度。 
 
攜回 NPC 2006論文集一本。 
 
二、與會心得 
一如往常 NPC conference，今年的重點還是 Grid Computing, Network and Peer-to-Peer 
technology相關研究與技術。Grid Computing是近幾年來的所興起的一個研究方向，較為
人所知的乃是 Professor Ian Foster所開發的 Globus Toolkit Project。本人發表的文章是 A 
Dynamic Adjustment Mechanism for Data Transfer in Data Grids 於 10月 3日下午之 Session 
3A: Scheduling, Overlay Network報告。 
 
NPC會議的規模年年增長，今年的與會者眾多，已達到歷年新高的一百多位，這顯
示出了這個會議的重要性以及其在平行與分散式計算相關國際會議中的指標性地位，並
且吸引大量學者專家競相在此會議投稿發表論文。由今年論文的投稿總數以及發表數量
可見，目前世界各地學者在 Grid Computing、平行與分散式計算、以及點對點網路傳輸
(Peer-to-Peer)架構的相關研究仍然持續地在創新以及進展中。 
 
由於電腦硬體不斷的快速發展，以及新興高速網路架構的出現，網際網路連線頻寬
