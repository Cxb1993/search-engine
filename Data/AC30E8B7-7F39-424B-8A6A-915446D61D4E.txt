a conventional AR model, each pixel in the to-be-
interpolated frame is modeled as a linear combination 
of temporal neighborhood and spatial neighborhood. 
This project proposed a temporal AR model (called 
TAR) utilizing temporal neighborhood； and a spatial 
AR model (called SAR) utilizing spatial neighborhood. 
We also proposed a mechanism which selects TAR or SAR 
adaptively according to motion information in the 
video sequence. By selecting appropriate AR model, 
unnecessary variables can be eliminated from 
regression process and computational cost can be 
greatly reduced.  
  In addition, we also proposal variable training 
window sizes for the AR model. In conventional auto-
regressive model, fixed training window size is 
adopted within a video sequence. Therefore, it may 
result in worse interpolated frames. We proposed a 
scheme which selects the best training window sizes 
in the encoder side. The best training window sizes 
are encoded into a binary file and transferred to the 
decoder side. When the decoder side performs FRUC, 
this binary file provides better training window 
sizes to improve the quality of the interpolated 
frames. The experiments show that, with variable 
training window sizes, auto-regression model can 
achieve better interpolated frame quality Althought 
it has more computational cost in the encoder side. 
In the second year of the project, we design an auto-
regressive model with various training window sizes 
for multiple description coding (MDC). Our approach 
utilizes the best window size information to enhance 
the error resilience of multiple description coding. 
In our MDC structure, we encode a video stream in two 
descriptions: one contains all odd frames, and the 
other contains all even frames. Both are encoded 
standard H.264/AVC coder. Besides, the descriptor 
with all odd frames will also carry the best window 
size maps for all the even frames. Similarly, the 
descriptor with all even frames will also carry the 
best window size maps for all the odd frames. In the 
decoder side, when there is any packet loss, we 
conceal the missing frame by using auto-regressive 
行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   
□期中進度報告 
 
研究自動迴歸技術在提升影片播放速率與多重描述視訊編碼上之應用 
Study on auto-regressive model for its applications on frame-rate up conversion 
and multiple description video coding 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 100－2628－E－009－026－MY2 
執行期間： 100年 08月 01日至 102年 07月 31日 
計畫主持人： 蔡文錦 
計畫參與人員：楊巧安 黃致遠 蕭成憲  
              王敬嚴 林建儒 高彬倬 胡振達 陳宣勝 邱柏瑞 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
□涉及專利或其他智慧財產權，□一年□二年後可公開查詢執行單位 
 1 
 Abstract 
In numerous issues for video compression processing, frame rate up-conversion (FRUC) is 
discussed all the time. Error resilient technique is also important because video streaming over 
networks has become popular in recent years. Multiple Description Coding (MDC) is a strong way 
dealing with erroneous transmission in various network environments.  
In the first year of the project, an adaptive auto-regressive model (AR model) is proposed for 
FRUC. In a conventional AR model, each pixel in the to-be-interpolated frame is modeled as a 
linear combination of temporal neighborhood and spatial neighborhood. This project proposed a 
temporal AR model (called TAR) utilizing temporal neighborhood; and a spatial AR model (called 
SAR) utilizing spatial neighborhood. We also proposed a mechanism which selects TAR or SAR 
adaptively according to motion information in the video sequence. By selecting appropriate AR 
model, unnecessary variables can be eliminated from regression process. Compared to STAR model 
[2] which utilizes joint temporal-spatial neighborhood for each pixel, computational cost can be 
greatly reduced with the proposed method. In addition, the experiment results show that visual 
quality can also be improved.  
  In addition, an AR model with variable training window sizes is proposed for FRUC. In 
conventional auto-regressive model, fixed training window size is adopted within a video sequence. 
Therefore, it may result in worse interpolated frames. We proposed a scheme which selects the best 
training window sizes in the encoder side. The best training window sizes are encoded into a binary 
file and transferred to the decoder side. When the decoder side performs FRUC, this binary file 
provides better training window sizes to improve the quality of the interpolated frames. The 
experiments show that, with variable training window sizes, auto-regression model can achieve 
better interpolated frame quality Althought it has more computational cost in the encoder side. 
In the second year of the project, we design an auto-regressive model with various training 
window sizes for multiple description coding (MDC). Our approach utilizes the best window size 
information to enhance the error resilience of multiple description coding. In our MDC structure, we 
encode a video stream in two descriptions: one contains all odd frames, and the other contains all 
even frames. Both are encoded standard H.264/AVC coder. Besides, the descriptor with all odd 
frames will also carry the best window size maps for all the even frames. Similarly, the descriptor 
with all even frames will also carry the best window size maps for all the odd frames. In the decoder 
side, when there is any packet loss, we conceal the missing frame by using auto-regressive model 
and corresponding training window sizes. 
According the experimental results, the proposed method is efficient and outperforms other 
method in objective and subjective quality. 
  
 
Keywords: Frame rate up-conversion, adaptive auto-regressive model, training window, error 
concealment, multiple description coding. 
 
 3 
 第一年計畫報告 
1. 文獻探討/Related Work 
1.1 MC-FRUC 
MC-FRUC can achieve better visual quality than frame average by exploiting the motion 
redundancy between frames. The Figure 1-1 shows the overall architecture of bi-direction motion 
compensated interpolation. 
 
Figure 1-1 : Bi-directional motion estimation diagram. Each block is assumed to be 
experienced a translational motion. 
 First the to-be-interpolated frame is divided into non-overlapping blocks. For each block, the 
bi-direction motion search is performed. The bi-direction motion search will find a motion vector v 
for block B(i, j) by minimizing the bi-directional sum of square error (SBSE) in the search window.  
The bi-directional motion search can be interpreted by SBSE[B(i, j),𝑣𝑣] = � (𝐹𝐹𝑡𝑡−1[𝑠𝑠 − 𝑣𝑣] − 𝐹𝐹𝑡𝑡+1[𝑠𝑠 + 𝑣𝑣])2
𝑠𝑠∈𝐵𝐵(𝑖𝑖,𝑗𝑗)  
( 1 ) 
𝑣𝑣𝑖𝑖,𝑗𝑗 = 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎
𝑣𝑣
{SBSE[B(i, j),𝑣𝑣]} 
( 2 ) 
, where S is a 2-D vector representing a pixel location, the 𝐹𝐹𝑡𝑡−1, 𝐹𝐹𝑡𝑡, and 𝐹𝐹𝑡𝑡+1denote the previous, 
to-be-interpolated, the following frames, respectively. The 𝑣𝑣𝑖𝑖,𝑗𝑗 represents the bi-direction motion 
search’s result for block B(i,j) in to-be-interpolated frame 𝐹𝐹𝑡𝑡. After the motion search, we then use 
the motion information to interpolate the block. Also, to-be-interpolated block B�(i, j) in 𝐹𝐹𝑡𝑡 is given 
by (3). 
 5 
spatial-temporal support order (support order, for short). When L is set to 1, the pixel is modeled as 
the weighted sum of 9 pixels in previous frame, 9 pixels in following frame, and 4 pixels in current 
frame. The (k, l) represents the pixel location within the training window. The (u, v) represents 
looping index for each element in spatial-temporal neighborhood, called support region. The optimal 
solution for weighting vector is, the one that minimizes the distortion  ε between training 
windows  R�t−1 and Rt−1 to have best fitting weighting vector. 
ε = 𝐸𝐸� Rt−1 − R�t−1� = � � 𝐸𝐸 ��Rt−1(𝑘𝑘, 𝑙𝑙) − R�t−1(𝑘𝑘, 𝑙𝑙)�2�𝑊𝑊𝑊𝑊
𝑙𝑙=0
𝑊𝑊𝑊𝑊
𝑘𝑘=0
 
( 5 ) 
Since the actual pixel values in the to-be-interpolated frame are not available in FRUC (In (5), 
for example, Rt−1), equation (5) can’t be used for deriving correct weighting coefficients. An 
iterative method, called self-feedback weight training loop algorithm was proposed with STAR 
model to deal with such issue. The self-feedback weight training loop consists of two parts. The 
pixels in training windows R�t−1 and R�t+1 are first interpolated by using their spatial-temporal 
neighborhood with the weighting vector 𝑤𝑤��⃗ , which consists of each element of Wp, Wf, and Ws, 
rewritten in 1-D manner. Then, the pixels in R�t−1 and R�t+1 are used to approximate the training 
window R�t using the same weighting vector 𝑤𝑤��⃗ , as illustrated in Figure 1-3 and the equation (6) 
below.  
 
Figure 1-3 : Self-feedback algorithm diagram R�t( k, l ) =  � � R�t−1(k + u, l + v)
v) ≤L ×  Wp(u, v)−L≤(u, + � � R�t+1(k + u, l + v)v) ≤L ×  Wf(u, v)−L≤(u,+ � � R�t(k + u, l + v){v=0,−L≤u<0} ×  Ws(u, v){v<0,−𝐿𝐿≤𝑢𝑢≤𝐿𝐿}∪  
( 6 ) 
After R�t−1, R�t, R�t+1 have been interpolated, the jointly distortion is defined as follows: 
D(i) =  ��E ��R�t−1i+1 (k, l) −  R�t−1i (k, l)�2�Wy
l=0
Wx
k=0
+ ��E ��R�t+1i+1 (k, l) −  R�t+1i (k, l)�2� + �� E ��R�ti+1(k, l) − Rt(k, l)�2�Wy
l=0
Wx
k=0
Wy
l=0
Wx
k=0
 
( 7 ) 
 7 
threshold, maximum iteration times (iMAX) …etc. 
Step 2: Use Bi-MCI’s result as initial value for training windows R�𝑡𝑡−10  and R�𝑡𝑡+10  
Step 3: Initialize iteration index 
Step 4&5: Use formulas mentioned before to construct corresponding matrices and vector for 
least square method. After least square method performed, the new weighting vector is obtained. 
Then the D(i) from (7) is calculated. 
Step 6: Test if D(i) is less than predefined threshold or not. If it does, then the procedure is 
done. Else, increase the iteration index, write back the new training window’s result as next 
iteration’s initial value and loop again. 
2. 研究方法一/TAR-model and SAR model 
 
Figure 2-1 : ATAR diagram. 
In the proposed TAR model, each pixel in to-be-interpolated frame t-1 is modeled as linear 
combination of temporal neighborhood, and all pixels in the same training window will share the 
same weighting coefficients. R�t−1( k, l ) =  � � Rt−2(k + u, l + v)
v) ≤L ×  Wp(u, v)−L≤(u, + � � Rt(k + u, l + v)v) ≤L ×  Wf(u, v)−L≤(u,  
( 10 ) 
The R�t−1  means the training window in frame t-1, and Wp, Wf  represents weighting 
coefficients in previous temporal neighborhood and following temporal neighborhood, respectively. 
The ( k, l )  represents pixel location in training window, and (u, v)  is looping index for each 
temporal neighborhood support region. Assuming that support order is 1, the length of weighting 
vector will be 18 as illustrated in figure 2-1. 
Self-feedback weight training algorithm similar to that used in STAR model is also applied in 
the proposed TAR model. Since only temporal neighborhood is used, the formula (6) is modified as 
follows for the approximated pixel in training window R�t in the proposed TAR. 
 9 
MA𝑚𝑚𝑣𝑣 = (Absolute MV����������������𝑊𝑊)2 + (Absolute MV����������������𝑊𝑊)2 
( 14 ) 
where MA𝑚𝑚𝑣𝑣 represents the mean of motion vector’s magnitude in training window. When MA𝑚𝑚𝑣𝑣 
is larger than a predefined threshold δ, we adopt the adaptive SAR (ASAR) as our regression-based 
FRUC. Otherwise, adopt adaptive TAR (ATAR). 
𝑠𝑠𝑠𝑠𝑙𝑙𝑠𝑠𝑠𝑠𝑠𝑠𝑎𝑎𝑜𝑜𝑎𝑎 𝑎𝑎𝑜𝑜𝑚𝑚𝑠𝑠𝑙𝑙: �𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴,               𝑎𝑎𝑜𝑜 𝛥𝛥 < 𝛿𝛿
𝐴𝐴𝑆𝑆𝐴𝐴𝐴𝐴,            𝑜𝑜𝑠𝑠ℎ𝑠𝑠𝑎𝑎𝑤𝑤𝑎𝑎𝑠𝑠𝑠𝑠 
( 15 ) 
 After defining the selection criterion, we have to verify the validity of it. 
 
Figure 2-3 : The validity of selection criterion of the test sequence Foreman_QCIF. 
The figure 2-3 shows  𝑆𝑆𝑆𝑆𝐸𝐸𝑡𝑡−𝑠𝑠 and  MA𝑚𝑚𝑣𝑣 for all the training windows in Foreman_QCIF 
sequence, where the training windows are sorted in an ascending order of MA𝑚𝑚𝑣𝑣, defined in 
formula (14). In figure 2-3, the left coordinate is SSEt-s’s value, and the right coordinate is the value 
of MA𝑚𝑚𝑣𝑣. Let 𝑆𝑆𝑆𝑆𝐸𝐸𝑡𝑡−𝑠𝑠 denote the difference between TAR’s SSE and SAR’s SSE, where SSE 
means the sum of square error. Then, 𝑆𝑆𝑆𝑆𝐸𝐸𝑡𝑡−𝑠𝑠 can be formulated as (16) below. 
𝑆𝑆𝑆𝑆𝐸𝐸𝑡𝑡−𝑠𝑠 = � � � ��Rt−1(𝑘𝑘, 𝑙𝑙) − R�t−1𝑇𝑇𝐴𝐴𝐴𝐴(𝑘𝑘, 𝑙𝑙)�2 + �Rt+1(𝑘𝑘, 𝑙𝑙) − R�t+1𝑇𝑇𝐴𝐴𝐴𝐴(𝑘𝑘, 𝑙𝑙)�2𝑊𝑊𝑊𝑊
𝑙𝑙=0
𝑊𝑊𝑊𝑊
𝑘𝑘=0+ �Rt(𝑘𝑘, 𝑙𝑙) − R�t𝑇𝑇𝐴𝐴𝐴𝐴(𝑘𝑘, 𝑙𝑙)�2 � �
− � � � ��Rt−1(𝑘𝑘, 𝑙𝑙) − R�t−1𝑆𝑆𝐴𝐴𝐴𝐴(𝑘𝑘, 𝑙𝑙)�2 + �Rt+1(𝑘𝑘, 𝑙𝑙) − R�t+1𝑆𝑆𝐴𝐴𝐴𝐴(𝑘𝑘, 𝑙𝑙)�2𝑊𝑊𝑊𝑊
𝑙𝑙=0
𝑊𝑊𝑊𝑊
𝑘𝑘=0+ �Rt(𝑘𝑘, 𝑙𝑙) − R�t𝑆𝑆𝐴𝐴𝐴𝐴(𝑘𝑘, 𝑙𝑙)�2� � 
( 16 ) 
Note that the calculation of 𝑆𝑆𝑆𝑆𝐸𝐸𝑡𝑡−𝑠𝑠 in figure 2-3 is based on available to-be-interpolated frames, 
that is, the ground truth if the to-be-interpolated frame is known. Rt−1, Rt, Rt+1 in formula (16) 
denote  the ground truth of the  training windows in to-be-interpolated frames t-1, t, and t+1 
0
2
4
6
8
10
12
14
16
18
20
-4000
-3000
-2000
-1000
0
1000
2000
3000
4000
1 53 10
5
15
7
20
9
26
1
31
3
36
5
41
7
46
9
52
1
57
3
62
5
67
7
72
9
78
1
83
3
88
5
93
7
98
9
10
41
10
93
11
45
11
97
12
49
SSEt-s
MAmv
 11 
encoding architecture. 
 
Figure 3-1 Encoder architecture for the proposed MSTAR model with variable training 
window size 
First the source images are encoded by standard H.264 encoder as well as STAR model 
implemented with four different training window sizes, four by four, eight by eight, sixteen by 
sixteen and thirty-two by thirty-two. Second, to obtain the best training window sizes, the PSNR 
information is generated by comparing the difference between these interpolated frames with varied 
training window sizes and the original source frames which exist in the encoder side. Figure 3-1 
illustrates the MSTAR model implemented with four training window sizes. In this example, we 
divide the frame in foreman sequence by using four training window sizes.  
 
 
Figure 3-1 STAR model implemented with four different training window sizes 
 To determine the best training window sizes, we adopt quadtree-based algorithm as shown in 
 13 
  
Figure 3-3 Quadtree-based algorithm of selecting training window sizes 
 
 15 
Maximum iteration times 2, 4, 6 
The jointly distortion threshold 50 
Support order 1 
Table 1 Parameters for regression model 
Four test sequences: Foreman, News, Mobile and Coastguard with CIF (352x288) resolution are 
used for performance evaluation. Five quantization parameters of H.246 encoder, 26, 28, 30, 32 and 
34, are used. 
4.2 Simulation of TAR and SAR 
We will examine the subjective and objective visual quality in this section. The figure 4-1 shows 
frame by frame PSNR of test sequence Foreman_QCIF for different methods. Horizontal coordinate 
is to-be-interpolated frame index, and the vertical coordinate is PSNR (dB). FA represents Frame 
Average method; MCI represents motion compensated interpolation with bi-directional motion 
estimation [1] (Bi-MCI, here we use MCI for short); and STAR represents the spatial-temporal AR 
model [2]. The Proposed_S represents the proposed SAR, Proposed_T represents the proposed TAR, 
and the Proposed_AST represents the method with adaptive selection between SAR and TAR. 
Proposed_UB is the performance upper bound of the proposed adaptive schema because it selects 
the best AR model (SAR or TAR) according to ground truth of the to-be-interpolated frames. 
 
Figure 4-5 : Frame by frame PSNR of Foreman_QCIF 
In FA, each pixel is interpolated by the co-located pixels in temporal neighborhood with the 
same weight (That is, 0.5). Since the FA didn’t use the motion information from video sequence, it 
has the best computation efficiency. But the visual quality of the frames interpolated by using FA is 
not acceptable, especially in large motion part of the video. 
The MCI produces better visual quality than FA since it exploits the motion information in 
video sequence. However, the performance of MCI-like methods for strongly depends on the 
27.5
30
32.5
35
37.5
40
42.5
45
47.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14
FA
MCI
STAR
Proposed_S
Proposed_T
Proposed_AST
Proposed_UB
 17 
 Figure 4-7 : Frame by frame PSNR of Football_CIF 
The Football_CIF sequence is used to test if the model is able to handle the sequence with 
large motion or not. In figure 4-3, it is observed that all the regression-based FRUC algorithms 
performed closely with no significant difference between them (less than 0.1dB). Even though the 
proposed methods no significantly gain in large-motion sequences, we still have the advantage in 
reduced computation loading. The table 2 shows the performance results of various methods, where 
maximum support 1 and iteration 1 time’s result in QCIF and CIF. 
Sequence Name FA MCI 4x4 MCI 8x8 STAR Proposed_S Proposed_T Proposed_AST Proposed_UB
Akiyo_QCIF 50.143 49.796 50.641 51.393 50.641 51.742 51.742 52.002
Coastguarad_QCIF 33.873 34.805 37.328 39.105 37.356 39.053 39.064 40.098
Foreman_QCIF 34.426 37.364 37.704 38.514 37.736 39.462 39.492 40.256
Mobile_QCIF 32.528 31.568 33.723 35.566 33.707 36.336 36.320 36.680
Akiyo_CIF 46.162 46.613 47.054 47.641 47.055 48.113 48.113 48.350
Football_CIF 19.896 20.399 21.164 21.191 21.263 21.151 21.248 21.646
Foreman_CIF 31.011 34.224 34.694 34.686 34.717 34.763 34.796 35.553
Mobile_CIF 24.830 27.218 27.512 29.092 27.477 29.932 29.873 30.234
Mother_and daughter_CIF 43.317 43.646 44.292 44.498 44.308 44.543 44.561 44.974
News_CIF 38.138 38.146 37.937 38.306 37.939 38.712 38.712 39.102  
Table 2 : PSNR table of FRUC algorithms. 
The table 2 gives the average PSNR of different sequences using FRUC algorithms. Compared 
to STAR model, the Proposed_AST have better visual quality among all test sequences except 
Coastguard_QCIF. In this experiment result, the average gain of Proposed_AST to STAR model is 
0.39dB, and the maximum average gain is 0.97dB for Foreman_QCIF. The following table shows 
the performance results of AR-based methods. The maximum support order is 6 for all methods and 
maximum iteration time is 5 for STAR, and 2 for proposed methods. 
18.5
19.5
20.5
21.5
22.5
23.5
24.5
25.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14
FA
MCI
STAR
Proposed_S
Proposed_T
Proposed_AST
Proposed_UB
 19 
happened around the ear, mouth, and helmet. Figure 4-4 (b) is MCI8x8 in the same frame. Blurring 
effect is eliminated for motion compensation. Still, the artifact around the mouth occurred because 
its discontinuity of adjacent block’s motion vectors. 
The figure 4-4 (c) is STAR model’s interpolation result, for maximum iteration 1 and support 
order is 1. It alleviated the artifact around the mouth a little, and improved overall visual quality. But 
it also required much computation cost than proposed method. And a little blurring effect occurred 
at the edge of the helmet, since it may contain too many unreliable spatial-temporal neighborhoods 
in moving filed. The figure 4-4 (d) is proposed method Proposed_AST’s interpolation result. 
Though the artifacts around the mouth are not totally alleviated, we improved the blurring effect at 
the edge of the helmet. 
 
Figure 4-9 : Mobile_QCIF PSNR vs. iteration times with maximum support order 3 
The above figure shows the PSNR of test sequence Mobile_QCIF with different iteration times. 
Experimental results show that these two different methods approach to different convergence point. 
Since the self-feedback’s regression target is not the same pixel value in these two methods, they 
will not converge to the same visual quality level. This figure also shows that the self-feedback loop 
algorithm can produce better PSNR through different maximum iteration times.  
Since we use less number of variables in the proposed AR model, the LSM’s computation 
loading can be significantly reduced, comparing to STAR model. The time complexity to compute 
the matrix inverse is Ο(𝑎𝑎4), where n is equal to the number of variables in AR model. Assuming that 
support order is set to 1, the STAR model will have 22 variables for LSM; while our proposed 
method will use only 17 variables in average. This is because that TAR uses 18 variables, SAR uses 
8 variables, and the ratio between TAR and SAR selected in our method is about 9:1. Following 
table shows the ratio between TAR and SAR selected in the proposed method, where the selection 
criteria threshold is set to 4.  
  
34
34.5
35
35.5
36
36.5
37
37.5
38
38.5
1 2 3 4 5 6 7 8 9 10
STAR
Proposed_AST
 21 
is set to 1 in this table. The xxx_IT1, xxx_IT2, xxx_IT5 means the xxx AR model is performed 
iteratively once, two times, and five times, respectively. The percentages in the table show that the 
proposed model consume only 36% to 75% clocks compared to STAR model for iteration once. 
Since STAR_IT5 (iteration 5 times) and Proposed_AST_IT2 (iteration two times) have similar 
visual performance, we also compare their execution times in this table and the results show that the 
proposed model consume only 14% to 36% clocks, compared to STAR model. The proposed model 
can save up to 86% clocks in Football_CIF, because it adaptively chooses SAR about 72% in whole 
process, the average number 10.8 variables for LSM. 
 
4-3 Simulation of multiple training windows sizes 
4-3-1 Frame-by-frame distortion performance observation 
We will examine the subjective and objective visual quality in this section. The Figure 4-6 shows 
frame by frame PSNR of test sequence Mobile_CIF for different methods. Horizontal axis denotes 
to-be-interpolated frame index, and the vertical axis is PSNR (dB). The STAR curve represents the 
spatial-temporal AR model [2]. The Proposed MSTAR curve represents the proposed 
spatial-temporal AR model with variable training window size. 
 
Figure 4-6 Frame by frame PSNR of Mobile_CIF 
 The parameters of regression-based methods (STAR, Proposed) in fare set as following: 
maximum iteration time = 6 and maximum support order = 1. In this experimental result, the gain of 
MSATR to STAR model is up to 0.46dB. 
25.5
26
26.5
27
27.5
28
28.5
29
29.5
30
0 5 10 15 20 25 30 35 40 45 50
PS
N
R(
dB
) 
frame no. 
mobile 
MSTAR
STAR
 23 
 
Figure 4-8 The relationship between bitrate and PSNR in Mobile_CIF sequence. 
  
Figure 4-9 The relationship between bitrate and PSNR in Foreman_CIF sequence. 
QP26 
QP28 
QP30 
QP32 
QP34 
25.5
26
26.5
27
27.5
28
28.5
500 1000 1500 2000 2500 3000
PS
N
R(
dB
) 
bit rate(KB) 
mobile-CIF 
MSTAR-it6
STAR-it6
MSTAR-it4
STAR-it4
MSTAR-it2
STAR-it2
QP26 
QP28 
QP30 
QP32 
QP34 
30.5
31
31.5
32
32.5
33
33.5
100 200 300 400 500 600 700 800
PS
N
R(
dB
) 
bit rate(KB) 
foreman-CIF 
MSTAR-it6
STAR-it6
MSTAR-it4
STAR-it4
MSTAR-it2
STAR-it2
 25 
 Figure 11 Percentage of each training window size for each sequence 
4-3-3 Subjective Quality 
This section observes the subjective quality of the interpolated frames using STAR model and 
Proposed MSTAR method, respectively. 
     
Figure 4-13 The 2th interpolated frame in Mobile_CIF. (a) STAR model (b) Proposed MSTAR 
method 
 Figure 4-13 and Figure 4-14 illustrate the interpolated results for 2th frame of Mobile_CIF and 
96th frame of News_CIF with maximum iteration 6 and support order is 1. Figure 4-13 (a) and 
Figure 4-14 (a) are STAR model’s interpolation result. The blurring is happened in the marked 
window with color red. Figure 4-13 (b) and Figure 4-14 (b) are proposed method’s interpolation 
results. The blurring effect in the marked window is improved. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
mobile foreman coastguard news
%
 
32x32
16x16
8x8
4x4
(a) (b) 
 27 
 第二年計畫報告 
1. 文獻探討/Introduction and Related Works 
Watch the video stream in any device over network is extremely popular. In video transmission, 
video data try to reduce the bit rate for transmission over communication channels. A highly 
compressed video data was usually adopted. Due to the unreliable communication environments, the 
data packet may be lost during the transmission. The packet lost lead to video can’t decode.in 
addition because of motion compensation, this error may propagate to succeeding frames. There are 
many techniques that try to overcome these problems. Multiple Description Coding (MDC) [18]is 
one of them. It generates different encoded versions for the same source. The version called 
description. Each description can decode and transmitted independently. Generally, all the 
descriptions are central decoding, the high quality video is reconstructed. While only one or few 
description are available at the decoder. The receiver still be able to retrieve video but with penalty. 
In recent years there have been many MDC techniques developed for different purpose. In [19], the 
input sequences is split in temporal domain, [20] in spatial domain and [21] in frequency domain. In 
this paper, we adopt one of the temporal-MDC structure named even and odd MDC structure. It spilt 
the original sequences into two descriptors. One description has all odd frames and the other has all 
even frames. 
 
1.1 Motivation  
In STAR model, fixed training window sizes are used. In our experiment, we have observed over 
50% regions use different training window sizes can get better quality and performances. In the 
proposed method, we design a STAR model using various training window sizes. The best training 
windows sizes of each region was found out by the method 
Multiple description coding providing a way that deal the date losses. Each descriptor can decode 
independently. We consider MDC with STAR model effect improve the error concealment. 
The multiple training window sizes are effective get higher quality than fixed sizes. And encode the 
selected training windows sizes is not a difficult issue. Based on these reasons, first, we generate the 
windows sizes map that including all selected train windows sizes in encoder side.  
It is encode to a binary file. Second, construct MDC structure, then sent the video stream and 
training window sizes information to decoder side. When a frame is lost, the frame was 
reconstructed by STAR model and training window sizes of missing frame. 
2. 研究方法/Proposed System 
2.1 Determine window sizes 
   The proposed method utilizes various training window sizes, which is based on the STAR model 
with fixed training windows size. We call various training window sizes STAR (VSTAR). 
 29 
descriptors. One description has all odd frames and the other has all even frames. The odd frame 
description should include even frame’s window sizes map (WSM). The even frame description 
include odd frame’s. Figure 2-2 shows the architecture. 
 
Figure 2-13 architecture of video stream 
When a frame is lost, decoder utilizes STAR model and the windows sizes map(WMP) of missing 
frame to conceal the missing frame. 
In encoder, generate WSM of frame use VSTAR encoder. Then add the WSM to corresponding 
descriptor. Figure 2-3 show the flowchart of encoder. 
 31 
Non – available Available Frame t+1 : Motion copy 
Frame t : STAR use t-3,t-1,t+1, W32 
Non - available Non - available 
Table 1 decoder strategy under different packet loss circumstances 
  
In this case, the frame was continuous loss (i.e. lose frame t and t+1 both). It was not suitable for 
conceal frame T with window sizes map of frame T. Because the frame T+1 also losing. The 
situation is of encoder and decoder were different already. In encoder, the frame T+1 have best 
quality. But in decoder, the frame T+1 are break. Therefore we don’t use the window sizes map, set 
all this frame’s window sizes was 32. that can get better quality. 
 
Figure 2-15 the flowchart of decoder strategy when packet loss 
 
The Figure 2-4 shows decoder trying his best to get the best quality reference frame. 
 
 33 
 
(B) Foreman sequence at 3% packet lose rates 
 
(C) Foreman sequence at 5% packet lose rates 
QP_20 
QP_22 
QP_24 
QP_26 
QP_28 
32
32.5
33
33.5
34
34.5
35
35.5
36
200000 400000 600000 800000 1000000
PS
N
R(
dB
) 
bitrate(byte) 
foreman (loss rates 3%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 
QP_22 
QP_24 
QP_26 
QP_28 
31.5
32
32.5
33
33.5
34
34.5
35
35.5
200000 400000 600000 800000 1000000
PS
N
R(
dB
) 
bitrate(byte) 
foreman (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 35 
   
(B) News sequence at 3% packet loss rates 
 
(C) News sequence at 5% packet loss rates 
  
QP_20 QP_22 
QP_24 
QP_26 
QP_28 
34
34.5
35
35.5
36
36.5
37
37.5
38
100000 150000 200000 250000 300000 350000 400000 450000 500000
PS
N
R(
dB
) 
bitrate(byte) 
News (loss rates 3%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 QP_22 
QP_24 
QP_26 
QP_28 
34
34.5
35
35.5
36
36.5
37
37.5
38
100000 150000 200000 250000 300000 350000 400000 450000 500000
PS
N
R(
dB
) 
biterate(byte) 
News (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 37 
 
(B) Football sequence at 3% packet loss rates 
 
 
(C) Football sequence at 5% packet loss rates 
QP_20 
QP_22 
QP_24 QP_26 
QP_28 
27.6
27.8
28
28.2
28.4
28.6
28.8
29
500000 1000000 1500000 2000000
PS
N
R(
dB
) 
bitrate(byte) 
football (loss rates 3 %) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 
QP_22 
QP_24 QP_26 QP_28 
27
27.2
27.4
27.6
27.8
28
28.2
28.4
28.6
28.8
500000 1000000 1500000 2000000
PS
N
R(
dB
) 
bitrate(byte) 
football (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 39 
 
(B) Coastguard sequence at 3% packet loss rates 
 
 
(C) Coastguard sequence at 5% packet loss rates 
 
QP_20 
QP_22 QP_24 
QP_26 
QP_28 
27.5
28
28.5
29
29.5
30
30.5
31
31.5
500000 1000000 1500000 2000000 2500000
PS
N
R(
dB
) 
bitrate(byte) 
Coastguard (loss rates 3%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
QP_20 
QP_22 QP_24 
QP_26 
QP_28 
27.5
28
28.5
29
29.5
30
30.5
31
31.5
500000 1000000 1500000 2000000 2500000
PS
N
R(
dB
) 
bitrate(byte) 
Coastguard (loss rates 5%) 
it2  wsm
it4  wsm
it6  wsm
it2  w32
it4  w32
it6  w32
MCI
 41 
 Figure 3-20 Frame by frame performance of Coastguard sequence  
3-3 Subjective Quality 
Figure 3-6 shows the subjective visual quality comparison for the 57th frame on the Coastguard 
sequence with Qp20, support order 1 and maximum iteration 6. 
We have observed the visual quality has obviously improved. Untidy area and boundary effect have 
been removed. 
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
1 13 25 37 49 61 73 85 97
PS
N
R(
dB
) 
Coastguard : Qp24 ,it 6 
WSM
W32
MCI
Frame NO 
 43 
  
Figure 3-22 the 11th conceal frame in football (A) WSM (B) MCI (C) No error 
Figure 3-7 illustrates the comparison of WSM and MCI with football sequence, Qp22 and iteration 
time 6. The blurring effects occur on the NO.97 player and some object. The blurring effects are 
improved by WSM. 
 45 
[12] B.-D. Choi, J.-W. Han, C.-S. Kim and S.-J. Ko, "Frame rate up-conversion using perspective 
transform," Consumer Electronics, IEEE Transactions on, vol. 52, pp. 975-982, 2006. 
[13] T. Thaipanich, P.-H. Wu and C.-C. Kuo, "Low complexity algorithm for robust video frame rate 
up-conversion (FRUC) technique," Consumer Electronics, IEEE Transactions on, vol. 55, pp. 220-228, 
2009. 
[14] S.-J. Kang, D.-G. Yoo, S.-K. Lee and Y. Kim, "Multiframe-based bilateral motion estimation with 
emphasis on stationary caption processing for frame rate up-conversion," Consumer Electronics, IEEE 
Transactions on, vol. 54, no. 4, pp. 1830-1838, November 2008 
[15] G. Dane and T. Nguyen, "Optimal temporal interpolation filter for motion-compensated frame rate up 
conversion," Image Processing, IEEE Transactions on, vol. 15, no. 4, pp. 978-991, April 2006. 
[16] Y. Zhang, D. Zhao, S. Ma, R. Wang and W. Gao, "A Motion-Aligned Auto-Regressive Model for 
Frame Rate Up Conversion," Image Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1248-1258, 
MAY 2010 
[17] N. Jacobson, Y.-L. Lee, V. Mahadevan, N. Vasconcelos and T. Nguyen, "A Novel Approach to FRUC 
Using Discriminant Saliency and Frame Segmentation," Image Processing, IEEE Transactions on, vol. 
19, no. 11, pp. 2924-2934, November 2010. 
 
第二年 
 
[18]  V. K. Goyal, "Multiple description coding: Compression meets the network," IEEE Signal Process., vol. 
18, no. 5, pp. 74-93, Sep. 2001.  
[19]  A. Reibman, H. Jafarkhani, Y. Wang and M. Orchard, "Multiple description video using rate-distortion 
splitting," IEEE International Conference on Image Processing, pp. 978-981, Oct. 2001.  
[20]  N. Franchi, M. Fumagalli, R. Lancini and S. Tubaro, "Multiple Description Video Coding for Scalable 
and Robust Transmission Over IP," IEEE Transactions on Circuits and Systems for Video Technology, 
vol. 15, no. 3, March 2005.  
[21]  S. Adedoyin, W. A. C. Fernando, H. A. Karim, C. T. E. R. Hewage, A. M. Kondoz, "Scalable Multiple 
Description Coding with Side Information Using Motion Interpolation," Consumer Electronics, IEEE 
Transactions on, vol. 4, no. 54, Nov. 2008.  
 
 
 
 
 
 
 47 
第二年 
 完成編碼端之 AR model 之研究與設計。 
我們在編碼端預先根據 AR model做出的 interpolate frame 和 original frame計算出
最好的training window 大小，接著把這個資訊儲存起來並做壓縮，叫做training window 
sizes map，並送到解碼端。 
 完成各種用來減少權重向量的方法之研究，包括 
 對 training window大小對權重向量資料量，以及影響畫面品質之研究。 
實驗結果顯示，利用多重的 trainging window大小，相較於原先固定大小的方
法，確實能夠得到較好的品質。 
 對權重向量表示法的精確度及資料量之研究。 
我們在編碼端得到 training window sizes map，並利用 quad tree的結構來進
行編碼。由於可以在編碼端獲得原始資料，因此我們可以得到最精確的大小， 
 對於結合其他方法(如 FC, MCI)所能減少的權重向量資料量之研究。 
我們傳送 training window sizes map當作 MDC的 redundant information。而
相較於權重向量資料，training window sizes map 的資料量是遠小於的。而
且 training window sizes map也相對的較容易編碼，而壓縮效率也很好。 
 完成植基於 AR model 的多重描述編碼法(稱 AR-based MDC)之研究與設計。 
我們在解碼端的部分使用 training windows size map加強 MDC的容錯能力，當有 frame
遺失時，我們可以藉由其他 descriptor收到的相鄰 frame，配合對應的 training window 
sizes map和 STAR model做補正。 
 完成對本計畫所設計 AR-based MDC的實作。 
我們將傳送 weight factor 改成 training window sizes map，這樣可以使資料量變小，
也有益於壓縮，品質也能維持。我們還是能獲得預期的結果。 
 完成對其它 MDC方法的實作，以及對各種 MDC方法的實驗(不同視訊資料、封包遺失率、
封包 loss pattern等) 與效能的分析比較。我們利用模擬的方法來進行實驗，使用了 4
種性質不同的視訊資料，也採用了 4種不同的封包遺失率和多種 loss pattern，而實驗
結果也顯示，無論在甚麼情況下，我們確實的加強了 MDC 的品質和容錯能力。有效的結
合了 AR-Model和 MDC。 
 論文發表: 本計畫有衍生出另一研究成果已發表於重要之國際會議ICIP2013中，論文題
目為 :“Error resilient coding using multiple reference frames”。其他成果則
尚在撰寫中。 
 49 
 一、參加會議經過 
由於本人目前離校研修，在美國與 USC大學的教授做視訊方面研究，因此，此次
出席在墨爾本的會議是從美國加州出發，飛行了 16小時才到達。 由於經費不足,本
人沒有參加會議第一天的任何 Tutorial。 因此，所搭乘的飛機是在會議的第二天(9/16)
一早才抵達墨爾本，並直接前往會場註冊參加 technical program。 
    此次的會議有八場 lecture sessions 在二樓不同會議室同時進行，同時間也有八個 
poster sessions 集中在一樓大廳進行。因此，只能從其中挑選有興趣的主題參加。我
主要參加的 HEVC1、Error resilient video transmission、Video quality assessment等場次
的 lecture session，以及大多數的 post session。因為八個 post sessions 集中在一起，可
以在較短時間內快速瀏覽眾多篇論文，且可以和作者面對面討論，因此，本人花較多
的時間在 post sessions。此次會議共接受了九百多篇論文， poster占了一半，數量很
多。除了快速瀏覽每篇論文外，本人對於 quality assessment 、visual saliency 
modeling、high- dimensional video coding、3D video coding、HEVC 這些方面的 papers
會花較多時間去了解作者的想法。對於有興趣但沒研究過的主題，透過跟作者的討論
能快速對此領域有所瞭解。 
    本人所發表的論文是在會議第三天下午的 post session，來瀏覽後有興趣並與本
人討論的約十來個人，不算多，因此我們有充裕的時間交換意見。有人覺得他所做的
某方面研究可以結合本人的方法試試，因此給了些建議；也有人提到他讀過本人其他
多篇論文，很高興可跟我討論；大部分的人則是針對本人論文中所提的方法及實驗，
詢問更詳細的作法。 
 
二、與會心得 
由於最近對新的領域 visual quality assessment有研究興趣，對於該領域相關的背景知
識希望能迅速累積，而此次參加研討會剛好是最佳時機。在 post session中，有很多篇
相關的論文，而透過跟作者面對面的討論，更讓我能快速掌握論文中的關鍵技術或重
要觀念，對於一個新領域相關知識的迅速累積，覺得這是本次最大的收穫。 
 
三、考察參觀活動(無是項活動者省略) 
無 
 
四、建議 
建議核發多一些出席國際會議之經費。目前核發的經費連參與一次國際會議(且只
待三天)都不夠，因此連 Tutorial都無法報名參加，覺得有些可惜，因為搭飛機的
時間很長，無法參與完整的會議，也無法多留一兩天參觀該城市，覺得有點遺憾。     
 
五、攜回資料名稱及內容 
  一本 ICIP2013 的 Program Guide.  
一個 USB 裡有會議論文集的全文電子檔。 
 
 
 
六、其他 
 
 
表 Y04 
100年度專題研究計畫研究成果彙整表 
計畫主持人：蔡文錦 計畫編號：100-2628-E-009-026-MY2 
計畫名稱：研究自動迴歸技術在提升影片播放速率與多重描述視訊編碼上之應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 7 8 88%  
博士生 2 0 200%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 one in ICIP2012, 
and one in 
ICIP2013 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
