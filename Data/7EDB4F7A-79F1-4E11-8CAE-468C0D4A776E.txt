1. 簡介 
1.1. 前言 
過去五年來，本研究團隊執行國科會的提升私大研發能量專案-資訊社會之基本技術及
其 應 用 之 研 究 ： 第 一 子 計 畫 主 動 式 網 路 之 發 展 與 應 用 （ 計 畫 編 號 ： NSC 
89-2745-P-155-003,NSC 90-2745-P-155-003,NSC 91-2745-P-155-003 ， 執 行 期 限 ：
89/12/01~92/11/30），以及 YAN 為基礎之主動階層式路由架構及品質服務路由設計與製作
（NSC92-2213-E-155-037，執行期限：89/8/01~92/7/31）、主動式可程式化通訊閘道器之設
計與製作（NSC93-2213-E-115-044、NSC94-2213-E-115-044、NSC95-2213-E-115-044，執行
期限：93/8/01~95/7/31），主要研究主動式網路及可程式化網路及其相關應用。可程式化網
路利用中介軟體，重新整合通訊協定或服務，使新的通訊協定或是服務可快速的發展與佈
建。然而可程式化網路的功能並不能線上更新網路節點，必須靠人為介入來更新網路設備
中的軟體。主動式網路則進一步將可程式化網路變得更方便。主動式網路的技術，使網路
節點可動態的接收執行指令、程式碼所在的網路位址，與被傳送或被計算的資料，並且執
行這些程式來處理封包資料或是改變網路硬體處理封包的行為。 
在「主動式網路之發展與應用」提升私大研發能量計畫中，本團隊基於主動式網路與
可程式化網路技術，提出一個稱為 Yuan Active Network (YAN)主動式可程式化網路架構。
YAN 是一個可程式化網路架構，在網路節點上提供運算環境。這些裝有 YAN 平台的交換
機與路由器為主動式節點。使用者可佈建特殊的或個人化的執行碼在這些節點的運算環境
上，藉此改變處理封包與資料的行為。藉此，YAN 可利用原有的網路元件，快速的融合新
的通訊協定與服務，提升網路的附加功能，提升網路的附加功能，降低因為新服務所必須
的網路升級成本。網路服務提供者可以利用主動式網路在不改變原本網路架構下快速的佈
建新的通訊協定、或提供新的網路服務。此外，主動式網路服務是因為需求才被動態佈建
的，當一個服務的功能被另一個新服務所取代時，舊有的服務因為失去需求而自動的在網
路上消失，不需網路管理者的介入。因此，主動式網路可打破通訊協定與服務由大通訊公
司把持的現況，讓小廠商可以提供特殊的、個人化的、或是區域性的網路服務，並且加速
通訊協定技術及服務的發展。 
在 YAN 平台設計完成後，網路服務的研發都可以根據 YAN 的規範來設計。因為網路
變得可程式化，且服務可以動態的載入 YAN 網路上，因此許多新的網路研究與行動代理人
研究將以 YAN 當為開發平台。而 YAN 平台本身也開啟許多新的研究方向，列舉如下： 
1. Programmable Packet Filter 
2. Service Composition 
3. Active Fault Tolerant 
4. Active Service Deployment 
5. Active Service Measurement 
6. Distributed Resource Management 
7. Network Resource Discovery 
8. Customized Routing Algorithm 
9. Active Network Security 
10. Peer-to-peer over Active Networks 
11. Grid Computing over Active Networks 
12. Cluster Computing over Active Networks 
 
組。網路介面模組都可執行可程式化的封包攔截機制，將封包轉送到對應運算模組，或是
在網路介面之間高速交換封包。運算模組則是控制模組的擴充，負責執行複雜的主動式服
務運算、常駐服務或特殊硬體服務。特殊硬體服務，如多媒體即時轉碼器，需要特殊的硬
體才可以即時的解壓縮後再壓縮；或是加密通訊所需的加密運算晶片，可利用特殊硬體模
組直接在通訊閘上擴充功能。 
APCG 的軟體平台乃擴充 YAN 主動式網路架構模式，如圖 2 所示，分為 YAN 作業平
台(YAN Platform)、加值服務（Value-added Service）、應用服務（Application Service）。YAN
作業平台是主動式網路作業環境，提供服務佈建、服務管理、資源管理、訊息交換、封包
攔截、與路由機制。 
z 主動式服務管理者（Active Service Manager；ASM）負責管理 APCG 上的所有軟
體模組管理，包括內建軟體模組與可佈建軟體模組。 
z 內建軟體模組，如 Distributed Message Dispatcher Agent (DMDA)、網路資源管理
器、節點資源管理器、攔截管理（Filter Manager）、路由表管理（Routing Table 
Manager）等。 
z DMDA 負責 APCG 中各個軟體元件中資料與控制訊號的轉送機制。 
z 封包轉送引擎（Programmable Forwarding Engine；PFE）包含一個可控制與設定的
封包攔截器（Packet Filter & Filter Manager），用以設定封包攔截器攔截特定的封
包，轉給該特定主動式服務來處理。 
z 主動式節點資源管理（Active Node Resource Manager, ANRM)負責管理一個主動式
節點上的資源，包括運算資源、記憶體資源、與網路資源。 
 
 
圖 2、APCG 軟體架構 
 
在 APCG 中，控制元件與可程式化的網路介面所建制的 APCG 軟體模組大致相同，差
異在於主動式節點資源總管與封包轉送引擎。控制點不直接與任何的服務網路介面相連，
故所有的訊息皆是透過網路點接受後，再利用 DMDA 轉送到控制點上，所以控制點沒有封
包轉送引擎。另外，在控制點上有一個主動式節點資源總管（Active Node Resource 
Manager；ANRM）。ANRM 負責收集 APCG 上所有點的資源，成為 APCG 回應外界詢問該
APCG 節點資源的窗口。ANRM 上所表示的資源就是代表這個 APCG 所能提供的資源。網
路介面並無 ANRM 窗口對外回應。 
APCG 系統將 APCG 軟體架構分別建置在 APCG 硬體的控制元件，與可程式化的網路
介面中，以提供主動式與可程式化網路的服務。 
 多媒體串流快取服務 
 多媒體串流轉送服務 
 應用層網路即時多播 
z 舉辦主動式網路研討會 
z 論文發表 
 
1.4. 具體成果 
1.4.1. 系統開發 
z APCG 開發硬體平台設計與安裝 
z APCG 軟體發展(依網路點、控制點、與運算點架構分別描述) 
 網路點(Network Node) 
 Programmable Forwarding Engine  
z Programmable Packet Filter & Manager 
z 高效能路由表管理 
 控制點與運算點 
 服務佈建管理 
z APCG 服務佈建管理器 
z APCG 服務描述語言與處理器 
z APCG 服務佈建演算法 
z APCG 服務元件載入載出控制 
 服務運算管理 
z APCG 服務工作排程演算法 
z 服務管道(service pipe)調整演算法 
 APCG 節點資源管理與監控 
z 運算資源管理與監控 
z 記憶體資源管理與監控 
z 儲存資源管理與監控 
z 網路資源管理與監控 
 Distributed Message Dispatcher Agent 
z 服務註冊控制器 
z 訊息轉送器 
z 訊息排程器 
z 服務管道(pipe)路由表 
 主動式可程式化服務品質保證路由 
z 客製化路由 
z 應用層路由 
 加值服務元件 
z 自我穩定基礎建設式 P2P 引擎 
 P2P-based 的 APCG 叢集建立演算法 
 資料快取控制演算法 
z 跨 APCG 節點的資源管理系統 
 多媒體服務 
z 應用層 VoD 服務 
14. Ming-Hong Wu, Yung-Mu Chen, Tein-Yaw Chung, Chih-Hung Hsu, “A Profile-Based Network Selection 
with MIH Information Service,” in Proc. ICS 2006, vol. 2, pp. 697-702, 2006 (Best Paper Award 
Nominated) [NSC-95-2213-E-155-005] 
15. Liang-Yi Huang, Yung-Mu Chen, Tein-Yaw Chung and Chih-Hung Hsu, “Adaptive VoIP Service QoS 
Control based on Perceptual Speech Quality,” in Proc. IEEE ICACT 2007, pp. 885-890, 2007 
[EI][NSC-95-2213-E-155-005] 
16. Tein-Yaw Chung, Yang-Hui Chang, Chia-Yu Liu, Yung-Mu Chen, “SSIF: Stable Self-Organized 
Infrastructure-based Peer-to-Peer Network,” in Proc. IEEE ICACT 2007, pp. 1441-1446, 2007 
[EI][NSC-95-2213-E-155-005] 
17. Wei-Cheng Wang, Chih-Hung Hsu, Yung-Mu Chen, Tein-Yaw Chung, “SCTP-based Handover for VoIP 
over IEEE802.11 WLAN Using Device Virtualization,” in Proc. IEEE ICACT 2007, pp. 1073-1076, 
2007 [EI][NSC-95-2213-E-155-005] 
18. Ming-Yen Lai, Tein-Yaw Chung, Yung-Mu Chen, and Chih-Hung Hsu, “Reducing Power Consumption 
in Network Discovery with User Motion Detection in Heterogeneous Networks,” in Proc. IEEE WCNC 
2007, pp. 3268-3273, 2007 [NSC-95-2213-E-155-005] 
 
1.4.3.3. 學生畢業論文 
1. 主動式網路輔助之應用層即時分層多播，林康司，元智大學資工系碩士學位論文。 
2. 點對點網路之多媒體串流暫存伺服器服務，邱世雄，元智大學資工系碩士學位論
文。 
3. ORMAN : 重疊式網路資源管理系統，翁政豪，元智大學資工系碩士學位論文。 
4. PT-SPF: 以 Patricia Trie 為基礎的高效能全狀態封包過濾器，陳建源，元智大學
資工系碩士學位論文。 
5. Design and Implementation of a Stable Self-Organized Infrastructure-based (SSIF) P2P 
Network，劉家佑，元智大學資工系碩士學位論文。 
6. 叢集演算法在疊代網路之運用，葉璿霆，元智大學資工系碩士學位論文。 
7. 主動式服務佈建策略，黃倫妮，元智大學資工系碩士學位論文。 
8. PNECOS: 對等式網路編碼串流系統，王志誠，元智大學資工系碩士學位論文。 
 
2. 系統設計 
主動式可程式化通訊閘道器（APCG）在 YAN 架構上是一個 YAN-S 的節點。APCG 主
要因應在現有的通訊網路架構下，佈建主動式路由器的困難點：成本。因此本計畫將主動
式節點建置在網路閘道器（Gateway）的位置。相對於建置主動式節點到核心網路（Core 
Network）的成本，建立主動式節點在接近使用者的通訊閘道上是比較便宜的。APCG 可以
就使用者的特性，自動架構 APCG 內的主動式服務，使得 APCG 漸漸的被客製化
（Customized）。更進一步，若是 APCG 下的使用者也有主動式服務，APCG 可以與使用者
的主動式服務相互的合作，完成更複雜的網路服務。 
 
2.1. APCG 架構 
2.1.1. 網路架構 
APCG 在網路中扮演的角色如圖 4 表示，APCG 扮演使用者連結網際網路的閘道器的
角色。APCG 間，使用 YAN 主動式軟體中加值服務層（Value-added Service Layer）上的同
儕服務（Peer-to-peer Service, P2P Service），架構成一個應用程式層的網路結構。圖 32 中
的 Core Network 不一定強制限制是主動式節點，如此可降低佈建主動式環境的困難度。 
在 APCG 下的使用者會影響 APCG 上所執行的主動式元件或是服務種類。以圖 32 左
上角的網域為例，該網路需要一個 Streaming Proxy 來提升使用者觀看網路多媒體串流的品
質。其運作方式有二種。第一種是由 APCG 下載 Streaming Proxy 的執行碼，且 APCG 本身
  
(a) APCG 硬體架構                         (b)APCG 雛形架構 
圖 5、APCG 硬體架構。 
 
圖 6、APCG 軟體架構。 
 
2.1.3. APCG 軟體架構 
APCG 的軟體平台乃採用 YAN 主動式網路架構，其架構如圖 6，分成三大部分，YAN
作業平台(YAN Plateform)、加值服務（Value-added Service）、應用服務（Application Service）。
YAN 作業平台是主動式網路作業環境，提供服務佈建、服務管理、資源管理、訊息交換、
封包攔截、並包含預設的路由機制。當一部 APCG 沒有任何主動式網路服務需求時，APCG
佈建 P2P 多播機制的軟體元件，並在應用層中佈建一個串流暫存服務。如此使用者即可收
到多媒體多播串流。此時 APCG 除了扮演閘道器外，又扮演一個多播節點的角色。 
 
2.1.3.1. 網路點 
 
 
圖 8、網路點軟體架構。 
APCG 硬體雛形是利用三個不同的 Linux 主機來實作，各 Linux 主機都是 YAN 主動式
網路平台，但是對於 APCG 所扮演的角色(網路點、控制點)不同，在軟體模組有些差異。 
網路點負責收送網路中的封包。圖 8 是網路點的軟體架構。網路點中包含 ASM、
DMDA 、 PFE 、預設路由機制等等。較原來 APCG 軟體架構中，新增 Service 
Intra-synchronization（SIS）模組。因為 APCG 雛形是由數個獨立的 Linux 主機所組成，因
此 ASM 必須存在於每一部 Linux 主機。每部 Linux 主機上的 ASM 間必須有一致的主動式
服務及軟體模組的相關資訊，因此在 ASM 上必須有一個同步機制 SIS。SIS 間我們建立一
個同步的通訊協定，利用一個邏輯的通訊環（Token Ring）來同步 SIS 間的主動式服務記錄。 
節點資源管理（Node Resource Manager，NRM)負責管理單一網路點上的資源，包括運
算資源、記憶體資源、與網路資源。網路資源是由在封包轉送引擎中的網路資源監測模組
來收集，在利用 DMDA 送到 NRM 上。運算資源與記憶體資源則是利用點資源監測模組
（Node Resource Monitor）來監測，同樣的利用 DMDA 送到 NRM 上。NRM 上的資源訊息
並不能代表整個 APCG 的資源。 
在封包轉送引擎中包含一個網路資源監測模組（Network Monitor）。在 APCG 中，網路
資源也是一個主動式服務是否可被執行的重要依據，因此需要網路資源的數據來確保主動
式網路服務的通訊品質。在網路資源監測模組中我們收集下列幾個數據：網路頻寬、剩餘
頻寬、通訊延遲、封包轉送引擎中封包丟棄率（Packet Dropping Rate）等數據。這些數據
會被彙整到節點資源管理模組中（Node Resource Manager）。 
在網路點軟體架構中的 Middleware 是 IEEE P1520 中所定義的 Lower Interface、Upper 
Interface、Value-added 介面。APCG 是以 YAN 為基礎擴充發展，因此這三層介面是由 SOAP
來實作。我們會佈建通訊相關的主動式服務在網路點上。例如圖 8 中的 P2P Forwarding 
Service，便會建立一個 P2P 的通訊網路。而 P2P Forwarding Service 會建立一個應用層的路
由表（Application Level Routing Table），專門處理應用層 P2P 網路拓樸路由的工作。圖 8
中的 Customized Routing Service 是針對不同服務需求，所開發出來的客製化的路由通訊協
2.2.1. 服務佈建機制 
2.2.1.1. 單點佈建機制 
單點佈建機制（Node Deployment Strategy, NDS）將一個 APCG 的服務佈建到一個能滿
足該服務所需資源的 APCG 節點上。透過傳送 YAN 架構的 ASDP 訊息到 APCG 佈建點，
來將該服務的啟動資料送到該佈建點。除了所需資源外，執行程式碼(執行檔或原始碼)的來
源是 ASDP 中最重要的資料。如圖 10（a）所示，一個 ASDP 佈建訊息只能佈建一個服務
元件。圖 10（a）中，該路徑有多種服務點要佈建，所以擁有服務的 APCG 需要送出兩個
不同的 ASDP 訊息給網路中的其他 APCG。另一方面，若是 End-User 要佈建自己所提供的
服務，也可使用安裝 APCG 中 YAN-Client 的中介元件(Middleware)，便可使用 ADSP 佈建
自己的服務。 
 
 
圖 10、服務佈建策略 
 
2.2.1.2. 路徑佈建機制 
路徑佈建機制用一個 ASDP 訊息將主動式服務佈建到通訊路徑上的 APCG 中。如圖 
10(b)所示，同一個 ASDP 訊息佈建一個主動式元件到通訊通道的每一個主動式節點。這種
服務通常是為了確保這條通訊路徑，符合某個特殊的通訊品質要求，例如應用層多媒體串
流服務。 
 
2.2.1.3. 廣域佈建機制 
廣域佈建機制把 ASDP 訊息，以類似氾洪(Flooding)的方式，傳送到網路上的 APCG 中。
該策略傳送 ASDP 的方式是利用 YAN 架構的 Flooding 服務。ASDP 封包在網路中，會被
APCG 的封包過濾截器攔截。如圖 10(C)所示，ASDP 的訊息以佈建發起端為中心，向外佈
建，最後整個主動式網路都有該服務。廣域佈建機制通常用在基礎元件或服務的佈建上，
例如新的路由機制等廣域性的主動式服務。 
再利用得到的運算資源除以 iζ ，得到需要的網路頻寬。當可得的網路頻寬小於需要的網
路頻寬，則利用可得的網路頻寬乘以 iζ ，得到一個小於已經分配運算資源的需要的配運
算資源。利用這新的運算資源需求來分配運算資源。 
 
z Spring-base Series Compression (SSC) 
SSC 是分配主動式節點的資源給軟體元件的演算法，每一個軟體元件的運算資源需
求與頻寬需求都定義成彈簧，並且將主動式節點所擁有的資源定義成一個長度區間。SSC
則將這些彈簧串接起來，而這些彈簧的總長度必須小於等於這個長度區間。當主動式節
點的資源不足時，SSC 會計算出一個力來壓縮這些彈簧(使用的資料)，讓彈簧的總長度
能等於這個長度區間。 
 
z Spring-base Resource Allocation (SBRA) 
SBRA 演算法將前述三項技術整合成一個分配 APCG 節點的運算資源與通訊資源的資
源分配演算法，期望達到將單一節點的資源做最有效的使用，並可滿足 End-to-End 服務
品質的要求。 
 
z Spring-base Resource Compensation Algorithm (SRCA) 
SRCA 則是處理因為資源被壓縮所導致額外的延遲的機制，藉由在服務路徑上有可得
資源的節點來補償導致額外的延遲。 
 
當主動式路徑決定資源分配的位置與資源的量後，主動式網路服務及可以按照 SMRA 所
規劃的佈建位置，將這服務的軟體元件佈建到傳輸路徑上的 APCG 節點上。SRMA 整合 SRD、
Correlation Index、SSC、SBRA 與 SRCA 來規劃服務元件佈建在 APCG 節點與資源分配的計
畫，藉此提升 APCG 網路資源的使用效率，增加主動式網路的服務能力，同時保證點對點間
的通訊延遲。 
 
2.3. 資源監測系統 
資源監測系統，監測並提供網路拓樸中各節點之資源狀態資料，對計算系統與網路系
統的資源管理，和服務佈署有極高的價值。APCG 在其加值服務層內，也提供許多計算與網
路服務，因此，如何管理及監測為數眾多的分散式資源(Storage、Bandwidth、CPU)，亦是
APCG 重要的研究課題。 
ASM(Active Service Manager)所管理的軟體模組包括內建軟體模組與可佈建軟體模
組。其中，控制節點上的 ANRM(Active Node Resource Manager)負責聚集(aggregate)APCG
節點的資源資訊。而這些資訊的收集和擷取皆是由 APCG 節點上的資源監測器(Resource 
Monitor；RM)來達成。總結來說，ASM 利用 ANRM 提供的資訊，來決定如何在 APCG 中
散佈和建置軟體模組；而 ANRM 必須從 APCG 節點上的 RM 收取並整匯資源資訊，提供
主動式服務管理者必要的決策資料。 
RM 必須擁有下列特性以利提高 APCG 的適用性(flexibility)： 
z 遠端部署(Remote Deployment)：任何建置在 APCG 上服務，皆有其不同的資源需
求，這些被建置的服務，可透過ASM利用ANRM傳送服務本身客製化(Customized)
的資源監測模組，給予 RM 線上即時式(on-the-fly)載入執行，監測對於服務必要
可以簡便的管理 ICM。而對於直接存取監測資料的要求，則是由分配者元件直接
與 ICM 溝通互動。 
z 資料收集模組(Information Collector Module；ICM)：ICM 在代理器中是扮演收集
監測資料的角色，以模組化的方式設計，以因應不同的監測與應用。藉由分配者
元件的管理，使用者可以依照情況選擇適當的模組來收集特定的監測資料。ICM
是可以客製的(customized)，只要撰寫者遵守程式撰寫規則，即可寫出自己的
ICM，並交由代理器載入系統執行。如此增加整個代理器的延伸性(scalability)及適
用性(flexibility)。 
 
從功能性分類，代理器可以分成網路管理、模組管理、與錯誤處理三部分： 
z 網路管理：主要負責需傳送、或接收的資料和要求。分配者元件屬於此類。 
z 模組管理：注重如何取得和管理監測資料。模組管理者元件、XML 處理器元件、
與資料收集模組歸於此類。 
z 錯誤處理：針對代理器在運行時可能發生的錯誤，進行處理。事件紀錄產生器元
件和事件處理者元件屬於此類。 
 
每個 APCG 節點，服務軟體是以模組的方式建置在其平台上。APCG 提供的建置的功能
分為兩方面： 
z 服務的建置：可分為本地(local)和遠端(remote)兩種方式，意即服務的執行者是
由服務的發起者本身擔任或是指定其他 APCG 節點擔任。 
z 管理端的建置：也可分為本地(local)和遠端(remote)兩種方式，意即服務的管理
任務─監測服務的執行狀態，是由服務的發起者本身擔任或是指定其他 APCG 節點
擔任。 
因此，APCG 在管理與服務建置的選擇上有三種選擇： 
z 本地端執行服務，本地端執行資源管理。 
z 遠端執行服務，由遠端執行資源管理。 
z 遠端執行服務，本地端執行資源管理，或本地端執行服務，遠端執行資源管理。 
 
對於第三種選擇，服務的執行和監測的執行是分別列屬於不同的 APCG 節點。不論是本
地端執行服務，遠端執行管理；或者是遠端執行服務，本地端執行管理，執行服務和執行
管理的 APCG 節點都必須透過網路互相傳遞資料。反之，在第一項和第二項的情況下，執行
服務和執行管理的 APCG 節點皆為同一節點，資料在節點內部繞送即可。 
 
2.3.2. 操作指令 
代理器可供操作的指令分成模組管理與資料及操作管理兩類： 
z 模組管理：管理 ICM 的相關指令，包含 Module load、Module unload、Module status
和 Module query 等指令。 
z 料及操作管理：收集監測資料的相關指令，也包含設定 ICM 的監測方式。此類指
令包含 Module initiate、Module run、Module pause、Module terminate、Module set
和 Module get。 
 
Module load 指令為載入 ICM 至代理器中，ICM 被載入前，代理器會對 ICM 作驗證的
動作，確保 ICM 的可執行性及安全性。Module unload 指令為從代理器中卸下 ICM。代理
Module initiate 設定 ICM 的參數。 
Module run 啟動 ICM。 
Module pause 暫停 ICM 的監測動作。 
Module terminate 終止 ICM 的監測動作。 
Module set ICM 在執行時改變其參數。 
資料及操作
 
管理
 
Module get 取得監測資料。 
 
ICM 是負責收集資源狀態資料的模組，代理器同時提供數種不同的 ICM 以因應不同的
監測需求，表 2 顯示預設提供的 ICM。ICM 架構可分為下列三部分，如圖 14 所示： 
 
表 2、代理器提供的 ICM 一覽表。 
ICM 監測項目 
Memory ICM 系統記憶體相關資訊。 
System ICM CPU、系統開機時間、整體效能
等資訊。 
Bandwidth ICM 測量兩節點之間的 available 
bandwidth。(兩節點都需要執
行 監 測 代 理 器 且 包 括
Bandwidth ICM) 
Disk ICM 儲存媒體相關資訊。 
Network ICM 監測網路效能。 
Process ICM 監測特定服務程序。 
 
 
圖 14、 ICM 架構圖。 
 
2.3.3. 描述資源監控模組與指令 
在網路部份中，本代理器使用 XML 文件做為資料交換的媒介。XML 文件的優點是，透
過 XML 的標籤語言的描述，資料可以跨系統地交換。使得代理器在異質性的網路系統中，
也能順暢的運行無礙。在監測部分，代理器使用模組化的方式收集監測資料，使用者可動
態地管理 ICM 以增加效率。ICM 的運行模式是使用執行緒(thread)定期擷取監測資料，並
且將監測資料記錄存於本地資料庫之中。代理器提供一套 API 給使用者自行撰寫資料收集
模組。使用者可以撰寫客製化的 ICM，並交由代理器載入執行監測動作。同時，本研究的
實作也提供了遠端部署(Remote Deployment)的功能。使用者可從遠端的節點上，傳輸一個
資料收集模組給予本地端的代理器，並交由代理器載入執行。 
ANRM 與 NRM 是以 XML 文件當作資料交換的媒介。所有的代理器所使用的指令都需要以
XML 文件的格式傳送給 NRM 執行。每一種指令的 XML 文件基本格式都是相同的，但是對於
需要夾帶大量資料的指令，如 Module initiate、Module set、Module get 等，則是在元
 
圖 16、傳送 ICM 的 XML 文件。 
 
2.4. 封包轉送引擎與封包過濾器 
APCG 的封包轉送引擎(Programmable Forwarding Engine)開發中，本團隊提出兩種封包
過濾器(Packet Filter)改良設計與實做，已移植於 FreeBSD 作業系統中。 
 
2.4.1. 封包過濾快取 
本團隊提出一個叫 Packet Filter Cache (PFC)的快取架構，來增進現有封包過濾器的效
能。如圖 17 所示，PFC 可以在已存在的封包過濾器之前，以 Hash 值對應並儲存過濾規則，
達成 O(1)的記憶體存取速度。此快取可使封包在比對過程中，加速比對過程，並可快速被
分配到過濾器規則所設定的網路中介軟體或應用層服務。 
 
圖 17、PFC 架構圖 
 
圖 18、BPF 與 PFC 在不同的 Flow 狀態下封包處理時間比較圖 
2.5. 自我穩定基礎建設式 P2P 引擎 
2.5.1. SSIF 架構 
在 APCG 的軟體架構中，不論是控制點與運算點都有一個基礎的 P2P 服務存在，以提
供 APCG 上服務或資源搜索的應用。整個 APCG 軟體所構成的網路，是以一套自我穩定基
礎建設式(Self-stabilizing Infrastructure-based；SSIF)的對等式網路，來作為虛擬的網路拓樸
結構，管理加入監測系統的 APCG 端點(peer)，並提供服務或資源搜索的服務。 
 
圖 22、SSIF 網路拓樸結構。 
 
SSIF 為一個三階層結構(Three-level hierarchical structure)、穩定(stable)、區
域導向(location-aware)、叢集式(cluster-based)的對等式網路架構。每一個 APCG 節點
亦是 SSIF 上的一個 peer。SSIF 有效地分散資源管理所需的開支，如圖 22 所示，SSIF 網
路拓樸由四種端點(peer)構成： 
 
z 一般端點(Regular peer)：加入 SSIF 的一般節點，並不負責網路拓樸的維持，一
般端點同時兼具有兩種功能。一般端點以用戶端(client)的身分送出請求
(request)要求超級端點(super peer)、或叢集端點(cluster header)提供服務和
查詢資料。同時，一般端點以服務端(server)的身分，提供服務給其他一般端點，
如提供網路服務、使用儲存空間與中央處理器資源。由於 APCG 是扮演接取網路節
點(access node)的角色，故多數的一般端點是使用者終端。但當一個 APCG 剛啟
動，又處於無人連接的狀況，此時 APCG 的 SSIF 引擎會以一般端點加入 P2P 網路。 
z 超級端點(super peer)：為 SSIF 中，管理一般端點的中間管理者。每個超級節點
皆擁用獨立的廣播領域(Broadcasting domain)作為與其直轄的一般端點溝通的
管道。超級端點協助廣播領域中的端點發佈(publish)、轉送(forward)資訊，並
且處理廣播領域中所有一般端點的加入(join)和離開(leave)的操作。通常 APCG
扮演超級端點，提供服務給使用者終端所扮演的一般端點。 
z 叢集領導者(Cluster leader)：為網路拓樸的主要管理者。SSIF 以區域導向
(location-aware)為原則，將地域相近(close geographically)的 APCG 端點統合
成一叢集，並以能力最強的 APCG 端點為叢集領導者。叢集領導者為新加入 SSIF
的 APCG 端點與一般使用者提供定位(positioning)資訊，決定新端點應加入哪個
叢集之中，並且提供資訊搜尋的功能。此外叢集領導者與其他的叢集領導者之間，
可建立邦聯關係(confederation relationship)。擁有邦聯關係的叢集群，分享
彼此的端點資訊，進而能提供更高效率的全域搜尋(global searching)服務。 
z 起始聯繫(Bootstrap)：起始聯繫提供註冊，及維持和追蹤所有叢集領導者(APCG)
當 SSIF 中的一個叢集過大或過小時，SSIF 必須進行遷移或合併的動作。在遷移的情
況下，從原有叢集中被移出的端點將自組成另一個新的叢集。SSIF 會在新叢集和原有叢集
之間建立邦聯關係，而擁有邦聯關係的叢集在資料整合時將共享資訊。在合併的情況下，
端點成員過少的叢集會將所有的成員移至其他的叢集。不論合併還是遷移，SSIF 以漸進的
方式，周期性地少量的移動端點，減少叢集反覆建立和摧毀的震盪現象發生。 
 
2.5.1.4. 資料查詢 
藉由使用 SSIF 的網路結構，資料的搜尋以區域性的方式查詢；當某一端點發出搜尋請
求時，必先由所屬區域的超級端點負責回覆。如果超級端點無法在區域中搜尋到符合請求
的結果，則區域管理者將搜尋請求轉送更高階層的叢集領導者。當叢集領導者也無法搜索
到檔案資料時，則叢集領導者將搜索訊息轉送到其他叢集領導者中搜索。此種漸進式的搜
尋方法，比傳統的氾濫式(flooding)搜尋更能節省網路頻寬的使用量。 
 
2.5.2. APCG 節點動態叢集演算法 
在 SSIF 中，整個 APCG 所構成的網路會依照動態叢集演算法，進行叢集的分群、叢集
拓樸的建立與叢集管理者的選取。在 SSIF 架構中，我們發展出數種叢集演算法，包括叢集
式拓樸建置(Cluster-Based Topology，CBT)、以叢集重心(Cluster Gravity)為基礎的叢集演算
法、K-mean 叢集演算法。 
 
2.5.2.1. CBT 叢集演算法 
CBT 系統架構是一個以實體拓樸資訊，建立叢集的方法。每個節點都會隸屬一個叢集，
而叢集的形成主要是依據節點與節點之間 Router 個數，將相鄰的節點形成的一個區域性叢
集。在叢集中，每個叢集中都會有一個節點為叢集管理者，負責管理叢集中的成員節點。
叢集管理者的資訊會記錄在叢集管理伺服器中，叢集管理者會透過叢集管理伺服器取得其
他叢集管理者的資訊，並與其建立溝通的通道，形成一個 Dominating Set，互相溝通訊息。 
當網路上的 APCG 主機要加入 P2P 網路時，需向起始聯繫(Bootstrap)取得所有叢集
管理者的位址資料。當要 APCG 取得資料後，會依據資料中的位址去偵測（trace）與叢集
管理者的距離。如果偵測出來的結果並沒有距離 N 個 hop 以內的叢集管理者，則新加入社
群的 APCG 主機就會自己成為叢集管理者，建立新的叢集。如果偵測的結果在 N 個 Hop
內，新節點則向叢集內的叢集管理者提出加入的申請。叢集管理者會依據叢集內的節點個
數來判斷是否允許新節點加入，當叢集管理者不允許加入時，新節點則重新偵測下一個距
離自己 N 個 Hop 內的叢集 
 
2.5.2.2. 叢集重心為基礎的叢集演算法 
叢集重心為基礎的叢集演算法為 SSIF 的預設叢集演算法。網路重心猶如一個城市中，
人口較密集且資源較集中的區域。換句話說，就是在一個叢集中，網路節點較密集的位置。
APCG 節點透過 SSIF 所使用的一個叫動態混合式網路定位(Dynamic Hybrid Network 
Positioning，DHNP)方法，可知道自己在網路中的邏輯位置。而一個網路重心的位置(x_g, y_g)
可以下列方程式計算： 
N
Xn
gx Nn
∑
∈=_  
iCATV 隨選視訊服務(Video-on-Demand)平台，由三部份所構成：傳輸面(Transmission 
Plane)、串流化面(Streamization Plane)和管理面(Management Plane)。 
 
圖 23、iCATV 系統架構圖 
 
如圖 23 所示，iCATV 系統架構中之傳輸面負責影片資料片段之散佈與收集；串流化面
主要將影片片段組合與提供串流化之影片播放服務；管理面則分為兩部份，底層負責收集
各個使用者設備的相關資訊，如記憶體使用率、CPU 使用率、可用頻寬、儲存媒介的使用
率等，上層針對收集來之資訊，做出有效之資源管理與決策，提供系統對服務品質做相關
控制與維持。 
 
3.1.1.1. 傳輸面(Transmission Plane) 
傳統主從式架構存在單一伺服器服務能力上限，與頻寬需求的問題，導致傳統的主從
式 VOD 系統建置成本高昂、延展性不佳、服務品質的低落。iCATV 採用 APCG 的 P2P 引
擎，以地域性為導向，建置叢集式拓樸(Cluster-Based Topology，CBT)，提供有效率之資料
散佈、資料搜尋、及 P2P 拓樸維護方法，提供 VOD 資料穩定的傳輸管道。CBT 根據實體
拓樸，建置地域性的 APCG 叢集(Cluster)。每個叢集會選出一能力較強的 APCG 節點擔任
叢集管理者(Cluster Leader)，透過叢集管理者負責轉送不同叢集節點之訊息。叢集管理者之
間會建立類似 Chord 的環狀邏輯拓樸，以便於搜尋節點以及資料。 
 
3.1.1.2. 串流化面(Streamization Plane) 
為提供高品質畫面與即時性的隨選視訊串流服務，iCATV 將採用以同步間隔單元
(Synchronization Interval Unit，以下簡稱 SIU)作為基本的分割影片單位，依據影片播放時間
分割影片，與一般根據檔案大小分割之方法相異。因此，根據不同影片與不同片段圖片
(Group of picture, GOP)多寡，每一 SIU 的檔案大小可能不一樣。每一 SIU 使用一份數據表
(Profile)，如圖 24 所示，其內容包含每一 SIU 之傳輸速率與下載提前時間(pre-load time)等
有關 SIU 之資訊。 
錄，即 SOS 表。SOS 表內包含 SIU 之分佈資訊。依據此資訊執行平順演算法與下載排程演
算法，決定下載檔案的時間點與傳輸速率。當一段 SIU 下載/播放完畢後，使用者端的 APCG
元件隨即執行暫存取代演算法，決定此一 SIU 是否暫存以供其他使用者播放，提供代理伺
服器服務，減輕影片伺服器負擔，並提供平順的播放效果及高效率的多媒體傳輸服務。 
 
圖 26、iCATV 操作流程圖 
 
3.1.4. 平順演算法 
在使用對等式網路的實際環境下，每個使用者環境皆不同，使用者本身不同的暫存空
間大小，以及提供影片資料片段之使用者，其可提供之上傳傳輸速率不同，會使得正在播
放影片要求服務的使用者，因為可用之影片資料不足，導致影片播放中斷；或是因下載過
多資料，超出可容納的資料上限，導致暫存空間不足發生溢位(Overflow)。 
 圖 29、BR 轉換模式之一 
如圖 29 所示，SIU2 為暫存空間限制，SIU3 為傳輸速率限制，S*則為原始理想傳輸累
積線。因為傳輸速率限制，使得在傳輸 SIU3 時，只能使用虛線段 S**的傳輸累積速率，此
將造成在 t3 時沒有足夠的資料可供播放，導致影片播放中斷。如果希望以速率 S**來傳輸
SIU3，則必須將傳輸時間設定比 t2 提早 Tw3，形成新的傳輸累積線 S***，此一過程稱為
BR 轉換模式。 
 
 
圖 30、BR 轉換模式之二 
如圖 30 所示，因為 SIU3 提早於 t*傳輸，所以必需檢查原本 SIU2 的傳輸累積線(1)，
再加上提前傳輸的 SIU3 傳輸累積線(2)，該兩者的總累積線(3)是否會超過剩餘的暫存空間
大小。若不超過則代表 BR 轉換模式可行，形成 BR 轉換模式下的新傳輸累積線。 
 
3.1.4.3. BRR 傳輸轉換模式 
承接 BR 轉換模式，如圖 31 所示，當 SIU3 為傳輸速率限制時，原本的傳輸累積線(1)
透過 BR 轉換模式得到傳輸累積線(2)，再結合成傳輸累積線(3)。但是因為本身傳輸速率的
限制，即使提早傳輸時間成為傳輸累積線(4)，結合後仍可觀察到會在 t2 造成可用暫存空間
不足的現象，使得影片播放中斷。因此如能將傳輸累積線(3)的傳輸速率降低，以此避免在
t2 發生之暫存空間不足，就可順利的傳輸以平順地播放影片，此種方法及稱為 BRR 轉換模
式。 
在下載排程時，可能所有擁有 SIU 使用者，其上傳速率都不足以使用 BB rate 提供代理
伺服器服務，為更加減輕影片伺服器之負擔，採用簡易的多來源方式。iCATV 將一個 SIU
片斷，根據檔案大小平均分配給不同的來源端。每一來源端所需傳送的資料量減少，而等
待傳輸之時間長度不變，因此所需之上傳速率相對減低。只需使用少量的處理器計算能力，
即可增加使用代理伺服器服務的機率，減輕影片伺服器之負擔。 
 
3.1.6. 執行畫面 
3.1.6.1. 用戶端 
 
 
圖 33、參數設定 
 
 
圖 34、調整最大上傳頻寬 
 
 
圖 35、選擇快取暫存策略 
 
 
圖 39、發佈影片 
 
3.2. 應用層即時多媒體分層多播服務 - ALMA 
在資料網路(Data Network)中，路由器多半不提供多播的傳輸服務，導致多媒體服務使
用多次的單播傳輸來服務使用者，加重網路頻寬負荷與資料源的輸出頻寬需求。在現有的
網路上建立一個 P2P(Peer to Peer)多播網路，利用對等(Peer)節點來轉送多播資料，以分散
資料源的網路負荷。但是對等節點的穩定性遠不如主從式架構中的伺服器，且對等節點頻
繁的上下線導致通訊拓樸與通訊品質不穩定，這也會造成 P2P 網路維護的成本。另外，應
用層的傳輸是利用 Peer 節點間建立虛擬的通道(Tunnel)，節點與通道形成應用層的網路拓
樸。在無法詳細的搜集網路拓樸架構的前提下，應用層拓樸與網路實體拓樸會不一致。當
兩拓樸的差異性大時，在應用層資料繞送路徑，對於實體拓樸將不是最佳繞送路徑; 這會
導致通訊資源的浪費。 
本研究中提出 ALMA(Application-Level Layered Multicast with Active service Assistance)
演算法，以 APCG 中的主動式網路(Active Networks)元件，建構 P2P 的應用層即時多媒體
分層多播服務(Application-Level Layered Multicast)。ALMA 利用分層多播搭配主動式網路
的方法，克服使用者節點間環境的異質性(Heterogeneous)，解決傳統 RLM(Receiver Driven 
Layered Multicast)所遇到的問題。讓使用者節點可以依據其網路環境，觀賞符合其頻寬的影
片品質。並利用 APCG 動態佈建服務的功能與資源存取服務，建立趨近於實體網路拓樸的
應用層拓樸。 
所有對等(Peer)節點在加入服務時必須先丟出主動式封包，該封包在經過事前先佈建在
網路上的 APCG 節點時，會被封包欄截器主動攔截並分析。透過此機制盡量讓來自相同自
主系統(Autonomous System, AS)的使用者，能在同一個 APCG 叢集(Cluster)中建立多媒體多
播樹(Relay Tree)。如果在同一個區域網路(LAN)且閘道器(Gateway)為 APCG 節點時，可以
利用廣播(Broadcasting)的特性來減少頻寬的浪費。另外，影片傳輸時，透過為主動式封包
經過 APCG 節點，分析傳輸路徑是否發生壅塞，判斷是否要利用主動式網路的機制，尋找
另一條可用的應用層路徑，以確保傳輸品質。 
 
3.2.1. 基本環境架構 
本系統將網路節點分為三種身分，如圖 40 所示：(1)叢集成員(Cluster Member)；(2)叢
集管理者(Cluster Head) ；(3)使用者節點(Peer User)。另外，叢集成員又可分為叢集邊界節
點(Border Node)和非叢集邊界節點(Non Border Node)。 
叢集成員就是每個自主系統(Autonomous System；AS)下的 APCG，每個自主系統即是
一個叢集。圖 40 中的叢集管理者，叢集邊界節點都是叢集成員之一，即 APCG 節點。 
叢集管理者是叢集成員中連線密度最高的 APCG，主要在於紀錄和整理所有叢集中
APCG 回報的資訊。透過叢集管理者整理的資訊，讓其他叢集中的 APCG 可以決定多媒體
資料如何導向，以及決定使用者節點加入多媒體多播服務時連上某個節點。叢集邊界節點
 
圖 41、ALMA 中的 Base Layer 
 
圖 42、ALMA 中的 Enhanced Layer 
 
為了分層而建構多個樹，勢必會造成每個多播樹結構差異過大，增加管理的複雜度。
如果能夠透過主動式節點的輔助，則可減化多播樹的管理。當使用者節點第一次加入到基
本層的多播樹時，會透過主動式的封包送出加入需求。ACPG 利用主動式網路攔截主動式
封包的特性，將主動式封包攔起，並查詢 APCG 叢集中是否有其他使用者節點正在利用服
務，以決定發出加入訊息的使用者節點，應該連上哪個較合適的節點。透過這樣的方式，
讓每個叢集之間的連線不會產生多餘的繞送。與其他純 P2P 多播服務相比，純 P2P 最大的
缺點就是在於產生的堆疊拓樸(Overlay Topology)與實體(Physical Topology)拓樸差異過大。
因此透過上述模式，讓拓樸不會產生多餘的繞送。而當節點從服務離開時，則透過傳統的
樹狀拓樸演算法，防止其他使用者節點收不到多媒體資料。 
 
3.2.2.1. 節點加入 
ALMA 中，同個 APCG 叢集下的節點只會建立一個多媒體多播樹。當使用者節點要加
入多媒體多播服務時，它的第一個多播樹一定會是基本層的多播樹。當使用者要使用多播
服務時，會發出一個含有加入訊息的主動式封包，讓事先佈建在網路中的 APCG 攔截。因
此，只要是在同一個相同自主系統(AS)下的節點，它們的主動式封包都會被相同的幾個
APCG 所攔截。 
如果叢集中只有一個節點A，則該節點A所發出的主動式封包會被一直送到其他APCG
叢集。若是這些 APCG 叢集中至少存在著一個已經加入該服務的節點 B。節點 A 將會建立
連線到節點 B。如果節點 A 發出的主動式封包到目標節點的過程中沒有發現任何 APCG，
則直接連上目標節點。另外，如果節點 A 的 APCG 叢集中存在著一條屬於該服務的跨 APCG
叢集連線，則節點 A 取得該連線的來源端與目的端。如圖 43 為例，P1 透過 APCG 叢集
 
圖 45、使用者 C離線 
 
圖 46、使用者 D離線 
 
如圖 44、 圖 45 與圖 46 所示，當使用者 C 離線時，便會從同一個多播樹但不包含跨
自主系統連線的使用者中取得最下層的 N 當作替補者，取代原本 C 所在的位置。接著離線
的使用者 D，那麼同樣會是以父節點所在的自主系統為基礎，取 O 或者 I 為替補者。 
優先選擇離去節點自主系統內的多播樹最末端節點原因在於，如果直接連上離去節點
的子節點，那麼便可能發生因為父節點的上傳頻寬只能再負載一個子節點，造成離去節點
的其他子節點必須重新搜尋父節點的狀況。除非不得已，不然每個自主系統內多媒體多播
樹的最末端節點都是最優先的選擇對象。 
 
3.2.2.3. 局部廣播 
為了減少頻寬的浪費，尤其在同一個閘道器下(也就是同一個 Broadcast Domain)可能會
有多個使用者在看影片時，便很容易發生多個同影片的串流進出同個閘道器的狀況，如圖 
47 所示。如果能讓同一個區域網路下的使用者透過廣播的機制，那麼便可以大幅減少頻寬
的浪費，如圖 48 所示。 
局部廣播的前提必須在同一個匣道器內必須已經有使用者在接受影片，且此閘道器為
APCG 節點。當同個閘道器下的其他使用者上線後，必須先發出廣播封包，檢查是否有其
他已經加入服務的使用者，且檢查閘道器是否為 APCG。如果同閘道器下已經有 A 使用者
在觀賞影片，新加入的使用者便會提醒 A 使用者和閘道器。A 使用者再發出控制訊息的主
動式封包給閘道器以及影片來源節點，通知來源將串流接收目標改設定為閘道器，讓 APCG
閘道器啟動轉送串流，改以廣播模式服務到區域網路內。為了防止第一個接收檔案的使用
者離線後，造成影片來源端停止傳送，當其他接收廣播的使用者要開始接收廣播時，仍然
必須讓來影片源端知道其存在(即有加入的動作)。 
例如：當使用者 A 為區網中的第一個使用者，接著使用者 B 和 C 上線。原本傳送給 A
 
圖 50、沒有跨自主系統的連線數目 
 
4. 計畫成果自評 
 
本計畫承續本研究團隊前三年主動式網路及其服務研究的工作，將 YAN 主動式網路架
構實作成可能產品化的雛形系統。 
APCG 架構的優勢是可在不變動現有的網際網路架構下，使網際網路變得可程式化。
因為 YAN 架構為一個中繼（Middleware）軟體架構，提供服務元件化（Componentized）、
服務可集合化(Service Composition)、服務佈建（Service Deployment）、主動式服務描述語
言等等主動式網路特性。利用這些特性，新的網路服務可以快速的佈建到 APCG 系統上。 
在計畫中，我們亦提供許多主動式服務，VOD 系統使用 APCG 提供簡單的 Proxy 快取
服務、資源管理元件的管理服務，驗證單一服務佈建；ALMA 應用層的多播服務，驗證在
APCG 上佈建應用層路由機制的能力。 
在三年中，共有十位學生參與(博士生兩位、碩士班八位)。共發表期刊論文四篇，會議
論文十八篇，碩士學位論文八篇。此外仍有三篇期刊在審查中。並舉辦 2005 年主動式網路
研討會。 
 
 
IEICE TRANS. COMMUN., VOL.E88–B, NO.10 OCTOBER 2005
4023
PAPER
D2MST: A Shared Tree Construction Algorithm for Interactive
Multimedia Applications on Overlay Networks
Tein-Yaw CHUNG†a), Member and Yen-Din WANG†b), Nonmember
SUMMARY Interactive multimedia applications (IMAs) require not
only adequate bandwidth to support large volume data transmission but
also bounded end-to-end transmission delay between end users. This
study proposes a Delay and Degree constrained Multicasting Spanning Tree
(D2MST) algorithm to build an any-to-any share tree for IMAs. D2MST
comprises root selection and spanning tree generation. A weighting func-
tion is defined based on the novel concept of network center and gravity
to choose the root of a share tree. From the root, a spanning tree is built
by incrementally connecting nodes with larger “power” to the tree so that
the degree constraint is satisfied. Simulation results show that D2MST can
successfully generate a ∆-constraint MST in which a high percentage of
nodes can interact within the bounded delay.
key words: interactive multimedia, delay bound, ∆-constraint, multicast-
ing, shared tree, degree constraint, network gravity, overlay network
1. Introduction
Multimedia applications have recently become very popular
on Internet. Many applications, such as video conferences
and on-line games, require multicasting support. However,
network layer multicasting may consume large network ca-
pacity and cause serious network performance degradation
when it is misused. Hence most autonomous systems (ASs)
and ISPs are unwilling to make it available to users in other
ASs. Recently researchers have turned to develop multi-
casting solutions on the application level, known as overlay
network multicasting [1], [3]–[5].
Overlay multicasting is classified into two types, end
system multicasting and infrastructure multicasting. In end-
system multicasting, every end-host may join a multicasting
group and acts as a multicasting node. No router support
is needed in this approach. NARADA [1] and NICE [5]
are some good examples of end-system multicasting. Con-
versely, infrastructure multicasting initially generates a mul-
ticasting backbone with special nodes called proxies or hub
nodes. Other end hosts link to the backbone and access mul-
ticasting services. Examples of infrastructure multicasting
include OVERCAST [3], RMX [4] and OMNI [6].
Although current overlay multicasting schemes have
enhanced multicasting eﬃciency for some multimedia ap-
plications, such as Internet radio broadcasting, they do not
suit interactive multimedia applications (IMAs) in which
Manuscript received December 13, 2004.
Manuscript revised April 12, 2005.
†The authors are with the Department of Computer Science
and Engineering, Yuan Ze University, Taoyuan, Taiwan, R.O.C.
a) E-mail: csdchung@saturn.yzu.edu.tw
b) E-mail: lion.w@netlab.cse.yzu.edu.tw
DOI: 10.1093/ietcom/e88–b.10.4023
real time interaction is strongly required. For example, cur-
rent overlay multicasting systems mainly emphasize single
source multicasting. In IMAs, however, many users may be-
come a data source and thus many multicast trees must be
formed, one for each data source, if the single source mul-
ticasting scheme is applied. This will significantly increase
the cost of tree maintenance. Furthermore, although current
overlay multicasting schemes have addressed the constraint
of end host capability and attempted to build a tree with the
lowest delay possible, they have not addressed the bounded
delay, or ∆-constraint, required for IMAs.
This study presents a degree and delay constraint mul-
ticast spanning tree algorithm, D2MST, for an IMA. D2MST
utilizes end system multicasting model and creates a share
tree for all peer users of an IMA. Every peer user of the IMA
can employ the share tree to multicast data without breaking
the limitation of end host capacity and delay bound. As in-
dicated in [2], constructing a share tree with minimum delay
under degree constraint of end hosts is an NP complete prob-
lem. Therefore, D2MST applies two cost functions, merit
and power, to generate a high-performance share tree eﬃ-
ciently. Given the peer users of an IMA, D2MST chooses a
root from the peer users with the maximum “merit,” which
is based on novel concepts of network center and network
gravity. Then, peer users of the IMA are chosen one by one
in the decreasing order of their “power” value and linked
to the share tree using a Shortest Path Degree Constraint
(SPDC) algorithm proposed in this study.
In this paper, we do not produce protocols that are
ready for deployment. Indeed, we do not focus on several is-
sues that crop up on practice, such as dealing with dynamic
joining and leaving participants in the session, and building
a distributed version of the protocol. Instead, we study the
performance of a share tree with respect to root selection and
the eﬀect of various system parameters, such as node degree
and bandwidth, on the performance of a share tree. On the
other hand, it is observed that with addition of some simple
protocols, D2MST can be readily applied to applications in-
cluding distributed group teaching and panel discussion over
Internet that are scheduled for a given time. For instance, to
join a panel discussion, peer users are requested to register
at a portal before a scheduled time. When the panel discus-
sion is about to start, an assistant can create a share tree for
the registered users by requesting each peer to connect to
its parent peer in a sequence according to the computation
result of the D2MST algorithm.
Extensive simulation has been conducted to analyze
Copyright c© 2005 The Institute of Electronics, Information and Communication Engineers
CHUNG and WANG: D2MST: A SHARED TREE CONSTRUCTION ALGORITHM
4025
where 0 ≤ α, β ≤ 1, α + β ≤ 1, merit [n] ≤ 1, and
T1 = maxk∈N d (k, g) − d (n, g)
maxk∈N d (k, g) (6)
T2 =
H − 2d (n, center)
H
(7)
Notably, each term in Eq. (6) is normalized and has a
value smaller than 1. A node with large degree potentially
has a large merit. Additionally, T1 becomes large when
a node is close to the network gravity. Similarly, T2 has
a large value when a node is close to the network center.
Therefore, given α and β, the larger the merit of a node, the
better the node as a root.
2.2 SPDC Algorithm
Once a root is specified, D2MST begins to links other peer
users to generate a spanning tree under the degree constraint
of the peer users. In a tree, the cost of an edge close to
the root contributes to the end-to-end delay of many peer
users at the low level of the tree. Therefore, a node close to
the root should be linked early to the spanning tree; i.e., a
low-cost edge should be incorporated into the spanning tree
early to lower the end-to-end delay between many nodes.
However, if a peer user is close to the root but has a small
degree, linking it early to the spanning tree raises the height
of the tree or the end to end delay between many nodes.
Conversely, connecting a node with a large degree early can
lower the height of the spanning tree. However, if the node
is a long way from the root, the long delay between the node
and the root is added to all other nodes linked down below
the node. Therefore, to generate a minimal spanning tree
under degree constraint is a challenging NP complete prob-
lem. In D2MST, a cost function “power” is proposed as a
reference to prioritize peer users being linked to the span-
ning tree. In D2MST, each node n ∈ N has a power value
and is defined as follows:
power [n] = γ fnfmax + θ × T3 + (1 − γ − θ) × T1, (8)
where 0 ≤ γ, θ ≤ 1, γ + θ ≤ 1, power [n] ≤ 1,
T3 = maxk∈N d (k, root) − d (n, root)
maxk∈N d (k, root) . (9)
In Eq. (9), d (n, root) is the distance between node n and
root, maxk∈N d (k, root) is the maximum d (k, root) for all
k ∈ N.
A node with a large degree and close to the root and the
network gravity has a high power value and is linked to the
spanning tree early. Additionally, power is a function of T1.
Consider two peer users with the same degree and distance
away from the root. According to the power function, the
peer user closer to the network gravity is linked first to build
a short spanning tree.
In each iteration, the peer user with the largest power
value among the candidate (unconnected) peer users is cho-
sen and linked to the share tree using the SPDC (Shortest
Fig. 2 SPDC algorithm for tree construction.
Path with Degree Constrained) algorithm shown in Fig. 2.
To connect peer user w, SPDC chooses from the nodes al-
ready linked in the share tree with spare degree, the node
through which w has the lowest delay away from the root.
The process repeats until all candidate peer users are linked
to the share spanning tree.
3. Simulation and Analysis
A computer simulation was used to examine the perfor-
mance of D2MST, based on the following metrics:
• Average maximum end-to-end delay: the mean of the
maximum end to end delays of all studied topologies
linked with a given number of peer users.
• Average end-to-end delay: the mean of end to end
delays between any pair of peer users.
• Average in-bound: the mean percentage of peer user
pairs in the topologies with a given number of peer
users whose end to end delay satisfies the ∆-constraint.
A feasible algorithm has a small average maximum
end-to-end delay and average end-to-end delay and large av-
erage in-bound. Based on these metrics, the eﬀect of param-
eters in cost function merit and power was measured.
3.1 Simulation Environment
In the simulation, the system was configured as follows:
1. Peer users were located in a two-dimensional geo-
graphical area and each side has length 150 ms.
2. The number of peer users participating in an IMA is
50, 100, 150, 200, 250 and 300.
3. The degree of a peer user is distributed among 2, 3 or 4
[2], [7], which is proportional to the uplink network ca-
pacity: uplink bandwidth 2 Mbps has degree 2; 3 Mbps
has degree 3, and 4 Mbps has degree 4.
4. Each peer user produces a constant delay 3 ms in relay-
ing data packets.
5. Only one peer user can transmit stream data in an IMA
CHUNG and WANG: D2MST: A SHARED TREE CONSTRUCTION ALGORITHM
4027
peer users rises further, the height of the D2MST spanning
trees also rises owing to the degree constraint of peer users,
meaning that the small average delay between node pairs is
oﬀset by the rise in cumulative delay of packet relay and
transmission between node pairs. Figure 5 depicts the im-
pact of the number of peer users. As illustrated in Fig. 5,
when the number of peer users increases from 50 to 150, the
performance of D2MST spanning trees increases gradually.
Once the number of peer users passes 150, the performance
of D2MST spanning trees declines.
To further investigate the influence of parameters in
cost function merit and power on the performance of
D2MST, the performance of D2MST was simulated with
various value combinations of α, β, γ and θ as presented
in Table 2. In the simulation, only one topology was gener-
ated for a given number of peer users. Thus, the simulation
tested the performance of D2MST in a practical environment
in which D2MST was only performed once for a given set
of peer users.
Figure 6 to Fig. 9 shows that the distance between a
Table 2 Combinations of α, β, γ and θ in case studies.
α = 0.1, β = 0.2 γ = 0.3, θ = 0.4
α = 0.3, β = 0.4 γ = 0.3, θ = 0.7
α = 0.3, β = 0.7 γ = 0.7, θ = 0.1
α = 0.9, β = 0.1 γ = 0.9, θ = 0.1
Fig. 6 Average end-to-end delay in α = 0.1, β = 0.2 combinations.
Fig. 7 Average end-to-end delay in α = 0.3, β = 0.4 combinations.
peer user and the root has larger impact on the performance
of D2MST than a peer user’s degree (θ > γ) does when link-
ing peer users to the spanning tree. That is, if node A has a
larger degree than node B but is further away from the root
than node B, node B may need to be linked to the spanning
tree before node A to obtain a better D2MST spanning tree.
Also, a node should be linked to the spanning tree first if
it has the same degree and distance away from the root as
the other node but is closer to the network gravity; that is,
(γ + θ < 1). This case is illustrated in Fig. 6 to Fig. 9 where
D2MST always performs better when (γ = 0.7, θ = 0.1) than
when (γ = 0.9, θ = 0.1). Similar results also occur in Fig. 10
to Fig. 13.
Figure 6 to Fig. 13 demonstrate that the combination
(γ = 0.3, θ = 0.4) performs better in various combinations
of α and β than the other three γ and θ combinations.
Therefore, in the following simulations, the performance of
D2MST was studied in various combinations of α and β un-
der (γ = 0.3, θ = 0.4).
Figure 14 and Fig. 15 present the simulation re-
sults, showing that D2MST generally performs better at
(α = 0.3, β = 0.4), than at any other α and β combination.
This finding indicates that the distance of a peer user away
from the network center is as significant as the node de-
gree in choosing a root. Additionally, the distance of a peer
user away from the network gravity has the largest weight
Fig. 8 Average end-to-end delay in α = 0.3, β = 0.7 combinations.
Fig. 9 Average end-to-end delay in α = 0.9, β = 0.1 combinations.
CHUNG and WANG: D2MST: A SHARED TREE CONSTRUCTION ALGORITHM
4029
deciding the order of connecting nodes into the spanning
tree.
The results in Fig. 6 to Fig. 15 also show that in
some situations, such as when the number of peer users
is 50, the coeﬃcient setting of (α = 0.3, β = 0.4) and
(γ = 0.3, θ = 0.4) may not always build the best share tree.
This observation shows that a simple coeﬃcient setting
does not necessarily yield an optimal share tree since the
problem of D2MST is NP-complete in nature. However,
D2MST with the coeﬃcient setting of (α = 0.3, β = 0.4) and
(γ = 0.3, θ = 0.4) mostly performs quite well.
4. Conclusion
Building a share spanning tree that has minimum end-to-
end delay has been an NP-complete problem. This study
proposes a heuristic scheme called Delay and Degree Con-
straint Minimum Spanning Tree (D2MST) to resolve this
diﬃculty. D2MST comprises two stages - the root specifica-
tion and the shortest path tree construction with degree con-
straint (SPDC). This study proposes two cost functions—
merit and power—for each stages respectively. Novel con-
cepts, called network center and network gravity, are pro-
posed to help choose a good root of the share tree. The con-
cept of network gravity is also used in cost function power
in identifying the order of linking nodes to a share tree. A
computer simulation was performed to examine the perfor-
mance of D2MST in average maximum end-to-end delay,
average end-to-end delay, and average in-bound rate. The
simulation results show that by employing the concept of
network center and gravity, D2MST outperforms the previ-
ous algorithms. Furthermore, the results demonstrate that
the distance of a node away from the network center and
gravity is equally significant as the node degree of a peer
user in building a high-performance share spanning tree.
Although D2MST performs well with a given set of
peer users, it does not address the issues of dynamic joining
and leaving of peer users in a session. In the future, we will
continue to develop a set of protocols that enable D2MST to
cope with this problem cropped up on practice.
Acknowledgement
The authors would like to thank the National Science Coun-
cil of the Republic of China, Taiwan for financially support-
ing this research under Contract No. NSC93-2213-E-155-
044.
References
[1] Y. h. Chu, S.G. Rao, S. Seshan, and H. Zhang, “A case for end sys-
tem multicast,” IEEE J. Sel. Areas Commun., vol.20, no.8, pp.1456–
1471, 2002.
[2] N.M. Malouch, Z. Liu, D. Rubenstein, and S. Sahu, “A graph the-
oretic approach to bounding delay in proxy—Assisted, end-system
multicast,” Proc. IEEE IWQoS 02, pp.106–115, 2002.
[3] J. Jannotti, D.K. Giﬀord, K.L. Johnson, M.F. Kaashoek, and
J.W. O’Toole, Jr., “Overcast: Reliable multicasting with an overlay
network,” Proc. USENIX OSDI 00, pp.197–212, 2000.
[4] Y. Chawathe, S. McCanne, and E.A. Brewer, “RMX: Reliable mul-
ticast for heterogeneous networks,” Proc. IEEE INFOCOM 00,
pp.795–804, 2000.
[5] S. Banerjee, B. Bhattacharjee, and C. Kommareddy, “Scalable appli-
cation layer multicast,” Proc. ACM SIGCOMM Computer Comm.
Review 02, pp.205–217, 2002.
[6] S. Banerjee, C. Kommareddy, K. Kar, B. Bhattacharjee, and
S. Khuller, “Construction of an eﬃcient overlay multicast infras-
tructure for real-time applications,” Proc. IEEE INFOCOM 03,
pp.1521–1531, 2003.
[7] R. Pi, J.D. Song, and L.J. Hong, “Application level multicast in
hierarchical topology,” Proc. IEEE CCECE 03, pp.805–808, 2003.
[8] T.S.E. Ng and H. Zhang, “Towards global network positioning,”
Proc. ACM SIGCOMM Workshop on Internet Measurement 01,
pp.25–29, 2001.
[9] X. Jia, “A distributed algorithm of delay-bounded multicast rout-
ing for multimedia applications in wide area networks,” IEEE/ACM
Trans. Netw., vol.6, no.6, pp.828–837, 1998.
[10] R.S. Cahn, Wide Area Network Design Concepts and Tools for Op-
timization, Morgan Kaufmann Publishers, 1998.
Tein-Yaw Chung received his M.S. and
Ph.D. degrees in Department of ECE from North
Carolina State University (NCSU), Raleigh,
NC, USA, in 1986 and 1990, respectively. Af-
ter that, he joined IBM, RTP, NC and worked
at network architecture development. At 1993,
he joined the Department of Computer Sience
and Engineering, Yuan Ze University, Taiwan.
Now, he services as an associate professor. His
research interests included Computer Network,
Distributed System, Multimedia Communica-
tion, Intelligent Network Systems and Mobile Networking. He is a member
of IEEE Communication Society.
Yen-Din Wang received his M.S. in CSE
from Yuan Ze University. His research interests
included overlay network and multimedia com-
munication.
CHEN et al.: QMNF: QOS MULTICAST ROUTING PROTOCOL ON PROGRAMMABLE NETWORK
1159
QMNF is simple and easy to implement. Besides, to over-
come the problem associated with the single-pass resource
reservation, QMNF employs a two-pass resource reserva-
tion scheme to promote the resource utilization and prevent
unnecessary resource reservation. Simulation results show
that QMNF can achieve much higher call acceptance rate
than that of QMBF with a moderate increase in path-probing
packets. Furthermore, simulation results also reveal that
QMNF can distribute traﬃc load evenly over the network
and hence can help to prevent a hot spot in the network.
The rest of this article is organized as follows. Sec-
tion 2 then describes the basic function of QMNF, N-hop
flooding, and provides an overview of QMNF. Next, Sect. 3
presents the design of QMNF. Additionally, Sect. 4 analyzes
the performance of QMNF. Finally, Sect. 5 draws conclu-
sions.
2. Overview of QMNF
2.1 Network Model
The following assumptions are made in QMNF:
1. Nodes are interconnected by full-duplex and asymmet-
ric communication links.
2. A multicast tree exists. A node that wants to join a
multicast group sends a JOIN-PROBE packet to find
candidate paths that connect itself to the multicast tree.
3. A new node can use Session Initiation Protocol (SIP)
[19] or Session Directory Protocol (SDP) [20] to obtain
the address of the root of the multicast tree.
4. QMNF is built upon a layered routing architecture
which refers to the IEEE P1520 programmable net-
work model [14], [15]. QMNF is built by using an N-
hop flooding service, the forwarding table of OSPF and
a QoS path reservation scheme, by accessing a N-hop
flooding access API, a routing table access API and a
path reservation access API, as shown in Fig. 1. The
OSPF protocol involves a topology database that pro-
vides connectivity information for the network, and a
minimum-hop forwarding table. A network topology
Fig. 1 QMNF in a layered routing architecture.
is relatively static, so the OSPF protocol uses event-
based triggering for link state flooding.
2.2 N-Hop Flooding Service
The N-hop flooding component is associated with a hop
count number, N, a finite response time (timeout) and a
receive-once-policy to limit the flooding overhead. N is
maximum number of hops that a packet can traverse in its
life time. When N equals one, the N-hop flooding service
only transmits a packet to all adjacent nodes; when N is
infinity, the service covers the entire network. Thus, each
N-hop flooding disseminates a packet to a sub-network that
is dominated by the initiating node with an N-hop radius.
In every flooding, router nodes play diﬀerent roles de-
pending on their distance away from the source in the sub-
graph dominated by the source. The sub-graph has three
node types, initiating node or source, Intermediate Node
(IN) and Boundary Node (BN). The initiating node is the
router that invokes an N-hop flooding service. The interme-
diate nodes are routers that are away from the initiating node
by less than N hops. The boundary nodes are those routers
that are exactly N hops away from the initiating node. Fig-
ure 2(a) shows a two-hop flooding service centered at node
S. In the flooding as illustrated, node S is the initiating node,
A is an IN, and B and C are BNs. INs reduce hop count, N,
in the packets by one, and flood the delivered packets to their
neighbors, while BNs do not flood delivered packets.
The receive-once policy means that a router drops a
packet that it received previously from the same initiating
node. IN does not flood a packet twice, and BN will not
forward a packet twice to its users at the upper layer. The
response time is the time when an initiating node waits for
responses from the BNs, and is used in QMNF to collect fea-
sible path information from BNs at an initiating node. The
initiating node disregards any response that arrives after the
Fig. 2 QMNF floods packets by two-hop flooding.
CHEN et al.: QMNF: QOS MULTICAST ROUTING PROTOCOL ON PROGRAMMABLE NETWORK
1161
3.2 JOIN Process
A node wishing to join a multicast group enters a join pro-
cess in which QMNF explores feasible paths between the
node and any OTNs of the group, as discussed previously.
Significantly, feasible path exploration in a N-hop flooding
is exhaustive, i.e., it discovers all feasible paths. Accord-
ingly, QMNF requires both INs and CBNs to send responses
to their PBNs. As depicted above, QMNF utilizes a receive-
once policy, so an IN discards all duplicate JOIN-PROBE
messages from an initiating node. However, these mes-
sages carry useful path information about links that never
been visited by other JOIN-PROBE messages before. Thus,
QMNF requires an IN to send the path information car-
ried by a JOIN-PROBE message that is to be discarded to
its PBN. The ACK-flag value distinguishes between the re-
sponses from an IN and a CBN. If the ACK-flag value is one
(ACK-1), then the response is from an IN, while the ACK-
flag value is two (ACK-2), then the response is from a CBN.
Figure 3 describes in detail the logic of the join process ac-
cording to the messages and parameters defined above.
3.3 Touch Tree Process
When a JOIN-PROBE packet arrives at an OTN, the AtDest-
flag in the packet is enabled and the JOIN-PROBE packet
is then sent back to the PBN. Therefore, the PBN can for-
ward the collected path information to the OTN. When the
OTN receives the QoS path information in a JOIN-PROBE
packet, even though some of the JOIN-PROBE packets may
arrive later at this OTN, the OTN computes a list of can-
didate paths from the source to this OTN. The OTN only
reads and appends the candidate path collected in the JOIN-
PROBE, so the OTN has a light computing overhead. The
main computing overhead is the QoS routing path comput-
ing on routers. Therefore, the computing overhead is dis-
tributed in the network. Additionally, when further collected
QoS path information arrives at the OTN, the OTN com-
putes another candidate list and appends it to the old one.
3.4 CONFIRM Process and RELEASE Process
When an OTN collects a candidate path list, it reserves a
QoS path back to the SOURCE by QoS path reservation.
The resource reservation procedure is divided into CON-
FIRM and RELEASE processes. As depicted in Fig. 4,
the OTN sends a CONFIRM packet back to the SOURCE
for resource reservation along a selected path that consists
of router nodes called the Reservation Intermediate Nodes
(RIN). The QoS requirements in the CONFIRM packet are
checked along the selected path back to the SOURCE dur-
ing the reservation process. If the reservation is successful,
then a QoS path is set up and the group member join process
is completed.
When an RIN finds that the selected path does not sat-
isfy the QoS requirements, it sends a RELEASE packet back
Fig. 3 The JOIN process.
Fig. 4 Resource reservation process in QMNF.
to the original OTN for releasing the resources. Upon re-
ceiving a RELEASE packet, an RIN releases its resource
and forwards the RELEASE packet back to the OTN. When
the RELEASE packet arrives at the OTN, the OTN deletes
this path information in the candidate list and selects the
CHEN et al.: QMNF: QOS MULTICAST ROUTING PROTOCOL ON PROGRAMMABLE NETWORK
1163
ploys the path traversed by the first path-probing packet re-
ceived. Flood-2 works similar to Flood-1, except that the
destination waits for a period of time to collect path-probing
packets flooded from the source and selects a feasible path
with the maximum bandwidth. In QMBF-22, every router
maintains a QoS state and the least-cost routing information
of a sub-graph of the topology dominated by itself with two-
hop radius. Based on the information, a router computes at
most two feasible QoS branches from itself to some edge
nodes of the sub-graph. The radius was set to two because
the experiments have demonstrated that both QMBF [12]
and the N-hop flooding [22] perform best with a flooding
radius of two. Conversely, QMNF was not compared with
QMRP because QMBF has been proved to perform much
better than QMRP [11].
The simulations evaluated the performance of proto-
cols in terms of bandwidth utilization, number of messages,
average call acceptance rate and standard deviation of band-
width utilization.
• Average bandwidth utilization: the mean of the band-
width utilization of all links.
• Number of messages: the number of packets that sent
by a routing protocol but except the least-cost routing
protocols such as event-based OSPF and the periodi-
cally broadcasts protocol in QMBF [12].
• Average call acceptance rate: the ratio of the number
of accepted calls to the number of calls arrived in the
system.
4.1 Simulation Environment
To evaluate the performance of the protocols, a packet-level
event-driven simulator was developed by C++. The simu-
lations used an 8 × 8 mesh topology to evaluate the perfor-
mance of diﬀerent routing algorithms. All link bandwidths
of the network were 100 Mbps. A content provider node was
initialized as the root of multicast trees that could oﬀer up to
100 video channels, and 5000 call requests were generated
for each simulation run. The source node of a call was uni-
formly selected from the set of nodes. The inter-arrival time
of call requests was exponentially distributed with λ = 1
call/sec. The bandwidth requirement of a call was 3 Mbps
and the end-to-end delay was 400 ms. The duration of a call
was 3600 seconds for a video program.
4.2 Results and Analysis
As shown in Fig. 9, the number of routing messages in-
creased when the number of join requests increased. Also,
the number of routing messages of QMNF was more than
that of PIM-DM and QMBF but less than that of Flood-1
and Flood-2. PIM-DM always reserves resource along the
shortest path. Consequently, a new call is blocked easily
when the network is busy. Thus as shown in Fig. 10, PIM-
DM had the lowest message overhead but also the lowest
Fig. 9 Eﬀect of number of requests on number of messages.
Fig. 10 Eﬀect of number of requests on average call acceptance rate.
call acceptance rate as shown. QMBF generated fewer path-
probing messages than QMNF. However, as indicated previ-
ously, this experiment did not calculate the QoS state update
messages in QMBF, and therefore ignored a large portion
of the control overhead of QMBF. In a practical environ-
ment, QMBF must flood the network with QoS state update
messages when applying each one-pass resource reservation
during flooding, generating many messages in the network.
Figure 10 also reveals that the average call acceptance
rate decreased with increasing values of number of requests.
With fixed network resources, more resources will be re-
served when more requests arrived. Consequently, new
calls cannot obtain suﬃcient resources and are blocked in
busy networks. QMNF uses the N-hop flooding to re-
duce flooding overhead and increase the probability of suc-
cess. Thus, QMNF has a reasonable success rate even when
many requests arrive. Furthermore, QMBF employs one-
pass scheme leading to unnecessary resources reservation,
and also has lower average call acceptance rate than QMNF.
Moreover, Flood-1 and Flood-2 search the entire network to
find out a feasible QoS path, and both can obtain a high call
success rate.
Figure 11 demonstrates that the QMNF had higher
CHEN et al.: QMNF: QOS MULTICAST ROUTING PROTOCOL ON PROGRAMMABLE NETWORK
1165
3261, June 2002.
[20] M. Handley and V. Jacobson, “SDP: Session description protocol,”
IETF RFC 2327, April 1998.
[21] A. Adams, J. Nicholas, and W. Siadak, “Protocol independent
multicast—dense mode (PIM-DM),” IETF RFC 3973, Jan. 2005.
[22] Y. M. Chen, T. Y. Chung, and Y. H. Chang, “Simple QoS routing al-
gorithms on layered routing architecture,” Proc. Active Networking
Workshop, pp.14–20, Sept. 2002.
Yung-Mu Chen received the B.S. and M.S.
degrees in Computer Science and Engineering
from the Yuan Ze University, Taiwan, in 2001
and 2003, respectively. He is currently work-
ing toward the Ph.D. degree at Yuan Ze Univer-
sity, Taiwan. His primary research interests in-
clude Mobile Network, Overlay Network, Pro-
grammable Network, and Distributed System.
He is a student member of ACM and IEEE.
Tein-Yaw Chung received the A.S degree
in electronic engineering from National Taipei
Institute of Technology, Taiwan in 1980, and
the M.S. and Ph.D. degrees in electrical and
computer engineering from the North Carolina
State University, Raleigh, NC, USA, in 1986
and 1990, respectively. From Feb. 1990 to Feb.
1992, he was with the Network Service Divi-
sion, IBM, RTP, NC, USA and involved in the
research and development of heterogeneous net-
work interconnection. Since May 1992, he has
been with Yuan-Ze University, where he is now an associate professor in the
Department of Computer Science and Engineering. His current research in-
terests include overlay networking, multimedia communication and wire-
less communications. He is a member of IEEE.
Chun-Chu Yang received the B.S. de-
gree in Computer Science and Engineering from
Yuan Ze University, Taiwan, in 2004. Now, she
is working toward her M.S. degree at National
Chengchi University, Taiwan. Her current re-
search interests include P2P network, Semantic
Web, and Agent.
Pei-Chun Chen received the B.S. de-
gree in Computer Science and Engineering from
Yuan Ze University, Taiwan, in 2004. Now,
she is working toward her M.S. degree at Na-
tional Central University, Taiwan. Her current
research interests include wireless communica-
tions and mobile computing.
Journal of Internet Technology Volume 7 (2006) No.1 
 
 
24
minimize information loss and maintenance cost as peer 
nodes up and down frequently.  
In this paper, we propose an overlay topology called 
cluster-based topology (CBT) based on the power index of 
peer nodes. Instead of just building an overlay topology 
based on distance proximity as in Grapes, we also consider 
the stability, bandwidth, CPU power and storage of peer 
nodes in CBT construction. Thus, CBT is more stable than 
Grapes. CBT is also different from Brocade in that it has a 
self-organizing mechanism. In CBT, any node can become 
a cluster head and a protocol is used to build a backbone 
topology among cluster heads without any support of a 
server such as a landmark node. 
To exploit the community and physical proximity 
information of CBT in search performance enhancement, 
we propose a stable hierarchical searching protocol based 
on Chord. The proposed stable hierarchical Chord (or 
SH-Chord) runs over CBT and builds a SH-Chord ring for 
each CBT cluster. SH-Chord inherits the nice feature of 
Chord in its simplicity. Moreover, if peer nodes are evenly 
distributed to clusters, the logical search in SH-Chord has 
O(log N ) logical lookup hops, where N is the number of 
peer nodes in the systems. In SH-Chord, each Chord ring 
uses a Chord server. Chord servers run at cluster heads, 
help Chord rings in ring stabilization, and perform 
inter-ring search routing. Chord servers are simple. They 
do not maintain a routing table but to rout a search request 
using the routing information provided by CBT. To balance 
load, we also propose a power index based scheme, in 
which virtual nodes are assigned to peer nodes based on 
relative power index between adjacent real peer nodes. 
Extensive simulations have been performed to study the 
behavior of CBT and SH-Chord. Simulation results show 
that SH-Chord outperforms Chord in search path length 
and search successful rate. 
The rest of paper is organized as follows: In section 2, 
we propose a power based clustering scheme and develop a 
cluster construction algorithm. In section 3, we introduce 
the architecture of a CBT peer node and the service 
primitives. In section 4, we propose SH-Chord and 
introduce how a Chord ring is maintained and stabilized 
and how load is balanced. In section 5, we describe how 
SH-Chord is implemented over CBT. In section 6, we 
analyze the behavior of CBT and SH-Chord with 
simulation results. Finally in section 7, we conclude the 
paper. 
 
2 Virtual Cluster-based Topology 
 
In a peer-to-peer system, self-organizing mechanisms 
should be incorporated in peer nodes to minimize 
administrative overhead. Each peer node should be 
initialized to playing equivalent function and be added 
specialized functions seamlessly. Based on this principle, 
we propose our distributed algorithm for constructing a 
virtual cluster-based topology (CBT). Figure 1 shows an 
overlay network with CBT organization. CBT is simple 
and is built based on power-index of peer nodes. In CBT, 
peer nodes in close physical proximity are placed in a 
cluster and one peer node with the largest power index in 
the cluster is selected as a cluster head. During normal 
operation, the power index of each peer node is updated 
and cluster head is re-elected to make a cluster more stable. 
In the following sub-sections, we will detail the 
power-based clustering scheme and CBT topology 
construction. 
 
2.1 Power-based Clustering 
Peer nodes may up and down arbitrarily. An overlay 
topology that is built solely based on physical proximity 
may suffer from frequent large topology change. Thus, we 
make peer nodes with more power play different role in 
overlay topology maintenance. The power index P of peer 
node Ni is defined as follows: 
        TCMCCCBC)P(N iipiii 4321 +++=        (1) 
Where Ci is the coefficient and each coefficient is set as 
follows: C1 =1, C2=1, C3=1 and C4=100. Bi, Cpi and Mi are 
normalized value between 0 and 100. Bi is the normalized 
maximum link bandwidth Bw from Ni to a set of peer node 
Nj and is defined as follows: 
   wi )*-*)/(*-*(B  B 363 1056101001056100=  
We assume the minimum bandwidth is 56 Kbps and the 
maximum bandwidth is 100Mbps. Mi is the normalized 
memory size mi sharable to the peer-to-peer system by Ni 
and is defined as 
)*-*.)/(*-*(m  M ii 696 10128104210128100=  
Cpi is the normalized processor power ci in system clock 
100 MHz to 2.4 GHz and is defined as 
 )*-*.)/(* -*(c  C  ipi 696 10100104210100100=  
Ti is the living time of Ni in the unit of hours. Since Ti 
increases as Ni remains up, eventually it will dominate P(Ni) 
if Ni is stable. Also, in our algorithm, a CBT cluster head 
only needs to do a little more computation, messages 
exchanges, and information storage, thus, we use small 
Router  
Physical 
link 
Logical link 
CBT link 
Cluster head
Peer node 
Figure 1 Overlay network with CBT organization 
Journal of Internet Technology Volume 7 (2006) No.1 
 
 
26
2.1.3 CBT Maintenance 
CBT is very simple in maintenance. During regular 
operation, a cluster head periodically updates the power 
index of its cluster members as they report to it. Since the 
power index of a peer node increases as it remains alive, 
the order of power index of peer nodes in a cluster may 
change from time to time. If a backup cluster head has a 
larger power index than its cluster head, it is selected as a 
cluster head and a cluster head change process is invoked 
as in the case of the cluster head leave. On the other hand, 
if a peer node has a larger power index than that of the 
backup cluster head, it replaces the existing backup cluster 
head. The cluster head then initializes the new backup 
cluster head. 
 
3 CBT Based Peer Node Architecture 
 
Various existing P2P search algorithms, such as CAN, 
Chord, Tapestry, and Pastry, can be implemented on CBT 
to exploit the benefit of physical proximity and nodal 
power index of peer nodes. Figure 3 shows the architecture 
of a peer node based on CBT. The architecture includes 
three main components: topology component, search 
component and message dispatcher component. The 
topology component implements CBT logic and provides 
APIs for search component above. The search component 
implements a selected search algorithm. Different search 
algorithm can be implemented in different search 
component and share the same CBT information. 
The message dispatcher component serves as a 
component manager of a peer node. When a search 
component is installed or removed, it must register with or 
delete from the message dispatcher component. When 
application users issue commands, the message dispatcher 
component will dispatch the commands to appropriate 
components for execution. 
The user commands include user access management 
commands, topology common commands, and P2P search 
commands. Table 1 shows commands of each category. 
Access management commands are message dispatcher 
specific commands that are used to manage user access to 
the P2P system. With access management commands, a 
P2P search can maintain state of users and perform 
message routing to users. Topology common commands 
allow users to access state of topology component. P2P 
search commands are used to invoke P2P search 
component to search for files. 
To optimize search performance, a set of topology 
aware search APIs, as shown in Table 2, is also provided to 
allow search components to access CBT specific 
information. Note that GetNext can invoke a peer node to  
Table 1 APIs of access management, topology component, and search 
Component  
Command 
category 
Command Function description 
Open_Port Get an access point from the 
P2P system. 
Register_Port Register user’s parameters. 
Access 
Management 
Command 
Close_Port Close and release resource 
allocated to a user. 
CBT_Join Join CBT. 
CBT_Leave Leave CBT. 
Topology 
Common 
Command 
CBT_State Return a structure including 
current state of the peer node 
and cluster head address. 
Search_Join Join a search component. 
Search_Leave Leave a search component. 
Search_Publish Publish a file index to a 
search component. 
Search_UnPublish Remove a file index from a 
search component. 
Search API 
Search_Request Search for a file index. 
 
 
 
Figure 3 Architecture of a CBT-based peer node 
Journal of Internet Technology Volume 7 (2006) No.1 
 
 
28
Given two neighboring nodes V1 and V2, V1 < V2, with 
power-index P1 and P2 respectively, we insert a virtual 
node V1’ to V1 where 
 
⎥⎥
⎤⎢⎢
⎡
+
−+=
21
112
1
'
1
*)(
PP
PVV
VV                     (2) 
 
The power-indexed load balance scheme can enable a node 
with larger power index to store more keys. Figure 5 shows 
an example of virtual node insertion according to (2), in 
which virtual node N8 will be inserted by real node N5 and 
virtual N125 will be inserted by real node N35. Thus, with 
virtual node, real node N35 stored keys in the range of (N9, 
N125) instead of (N6, N35) without load balance. 
Since power index is updated periodically for each 
node, more elaborate scheme is required to reduce virtual 
node update frequency. According to (2), virtual node 
identifier will be updated when power index ratio between 
adjacent nodes changes. However, an identifier update 
requires a node deletion and a node insertion. This will 
cause large overhead for Chord maintenance. Thus, we 
propose a threshold based load balance scheme as shown in 
Figure 6. In our scheme, a new virtual node is inserted 
when the new virtual node identifier has a value larger than 
the existing largest virtual node identifier of the node over 
a threshold value. Also, a virtual node is deleted when its 
value is smaller than the new virtual node identifier over a 
threshold value. Since each node may insert several virtual 
nodes, the algorithm in Figure 6 is recursive to emulate the 
load balance behavior of (2). 
Since each node executes virtual node update 
simultaneously, each node will redistribute keys according 
to its computation results. A node will relocate keys to its 
predecessor if its predecessor has inserted a virtual node 
with larger value. On the other hand, a node will remove 
keys to its successor if the value of its virtual node has 
become smaller. With this fully distributed algorithm, load 
balance won’t cause much overhead and can effectively 
reduce key loss rate. Simulation results in section 6 will 
demonstrate the performance of our load balance 
algorithm. 
 
 
Figure 5 Power-index load balanced scheme using virtual node insertion 
 
4.2.2 SH-Chord Maintenance 
SH-Chord uses CBT as its basis, and thus, has three 
basic differences in ring maintenance from Chord. First, 
SH-Chord has to deal with Chord ring switching. A Chord 
node may switch from a Chord ring to another Chord ring 
when it changes clusters while it is up. This is because 
SH-Chord builds a Chord ring on each CBT cluster. When 
the cluster head of a Chord node changes due to CBT 
operation, it must re-join the Chord ring on the new cluster. 
Second, SH-Chord can use cluster heads to help its 
Chord ring maintenance. In SH-Chord, a cluster head is 
always a Chord node and serves as a bootstrap node. Any 
node insertion to a ring must go through a cluster head. 
Thus, a cluster head can maintain a list of Chord node 
identifiers on the ring. Using this list, it can search for a 
successor node before which a new Chord node should join 
to speed up ring insertion process. Furthermore, since a 
cluster head periodically monitors state of its cluster 
members, a cluster head knows the state of all nodes. When 
a node is down, a cluster head can detect it and check to 
see if it is a Chord node. If yes, it notifies the successor and 
predecessor of the failed node to modify their predecessor 
and successor respectively. With the help of a cluster head, 
SH-Chord ring maintenance becomes much more efficient 
than Chord. 
Third, SH-Chord implements a power-indexed load 
balance scheme that needs to do virtual node insertion and 
deletion algorithm as shown in Figure 6. In couple with 
 
Figure 6 Virtual node insertion and deletion in power-index load balance 
scheme 
Begin 
// called periodically to determine virtual node update 
Input: real node identifier V, threshold Vth; 
Output: virtual node insertion or deletion; 
Compute power index Vnew based on new power index. 
Balance: Let Vmax=max{Vi};  
If no virtual node exists 
Vmax=V; 
Compute △V = Vnew- Vmax;  
If △V > Vth  
Insert Vnew; 
Else if △V < -Vth  
Delete Vmax; 
Go to Balance; 
Endif  
End 
Journal of Internet Technology Volume 7 (2006) No.1 
 
 
30
In Figure 9 we show the CBT versus Chord ring 
topology as a two-layer logical topology. From the figure 
we see that the search path at a Chord ring is independent 
of CBT. However, since CBT limits the physical distance 
among peer nodes, thus a Chord ring can only span the 
area scoped by a CBT cluster. 
 
6 System Analysis and Simulation 
 
In this section, we first analyze the performance limit 
of SH-Chord in comparison with Chord and then use 
computer simulation to study the performance of CBT and 
SH-Chord. 
 
6.1 Performance Analysis 
We analyze the proposed power-indexed load balance 
scheme with a simple model to derive a performance 
bound on SH-Chord in key redistribution rate. Given peer 
nodes V={Vi}, their corresponding key insertion rate, and 
life time at t are Rt ={ri} and Tt ={Ti (t)}, the total key 
insertion rate to the Chord ring is 
∑
=
=
)(
1
)(
tN
i
irtr                             (3) 
Assume the node identifiers of V span a hashing space L 
that is segmented into line intervals {li | LlmI I ==U 1 }. To 
simplify the analysis, assume node identifiers evenly 
divide L, i.e. jill ji ,,∀=  and keys are evenly distributed 
in space L. Denote the total keys covered by line interval li 
up to t as Ki (t) and be stored in node Vi. Assume the 
maintenance cycle of Chord ring is 1 unit time and new 
peer nodes can insert keys only after they encounter the 
first maintenance cycle. Also we assume that keys inserted 
by a departed node will be deleted after a certain period of 
time. Then 
0)0(;))()(()1()( =∗−+−= iiii KL
lttrtKtK ω         (4) 
where ∑= =
)(
1
)(
tN
i
iwtω  is the total key purged rate from  
the P2P network. If Vi is off abnormally after t, then Ki (t) 
are lost. On the other hand, if a new peer node Vj is inserted 
to Chord ring at li after t and divide li into li ‘ and lj , Kj(t) 
will be equal to
i
j
ij l
ltKtK ∗= )()( . 
Given peer node arrival rateλand departure rateμ 
per unit time, then µλ −+−= )1()( tNtN . If the P2P network 
reaches stable state, i.e.,          and             , 
i.e., keys are non-overlapped and span the whole spectrum 
of hashing space L, then at any unit of time, we have to 
redistribute (λ+μ)*K keys at Chord ring if all peer nodes 
depart normally. At worst, we lostμ*K keys per unit time 
if all peer nodes depart abnormally. 
In SH-Chord, on the other hand, L is divided by 
power indexes of peer nodes in (2) instead of by node 
identifiers. To simplify our analysis, we see that as time 
goes by, peer nodes of server class will have large power 
indexes while other peer nodes up and down frequently and 
hence have very small power indexes. Thus, if a peer node 
Vi with power index Pi is located between two peer nodes 
Vi-1 and Vi+1 with Pi-1 and Pi+1 respectively and  
11 & +−>> iii PPP , then 1
1
111
11
'
11
*)(
−
−
−−
−− ≈+
−+= ⎥⎥
⎤⎢⎢
⎡
i
ii
ii V
PP
PVV
VV  
and 1
1
11'
1
*)(
+
+
+ ≈+
−+= ⎥⎥
⎤⎢⎢
⎡
i
ii
ii
i V
PP
PVV
VV . Thus, Vi stores keys  
located within almost two line intervals of li and li+1 as that 
in Chord. An ephemeral peer node, on the other hand, will 
store at most the same amount of keys as in Chord if it is 
located between two other ephemeral peer nodes. While at 
another extreme, an ephemeral peer node will only store 
keys with value exactly equal to its node identifier if it is 
located between two server peer nodes. Thus, at worst, 
SH-Chord performs similar to Chord with (λ+μ)*K keys 
redistribution per unit time, while at optimal condition, it 
only redistributes λ+μ keys per unit time. Thus, Chord 
will have K times of more key redistribution rate than that  
of SH-Chord. With the approximation approach in Figure 6, 
at optimal condition, SH-Chord has 
thV
K2  times less key 
redistribution rate on the average than Chord. Therefore,  
SH-Chord is much more stable and incurs much less 
overhead than Chord. 
 
6.2 System Simulation 
We construct a network of 100 routers for system 
simulation. In the simulation, we randomly generate 100 
peer nodes per hour on the average to join the peer-to-peer 
system. To limit the overhead of a cluster head, a cluster 
can at most have 1024 cluster members. The simulation 
time is 1000 hours. In the simulation, the up time of a peer 
node is heavy-tail distributed as follows: 
αγα
1
),(
−== kkFt                      (5) 
 
where k=3, α =0.35, and γ is randomly distributed 
between (0.1, 1). We select the value of k andα so that a 
peer node may remain up more than 1000 hours. According 
to the selected parameters, t has minimal value 3 and has 
maximal value larger than 1000. 
 
6.3 Analysis of CBT Characteristics 
As stated previously, cluster head change has strong 
impact on CBT users, such as SH-Chord. Thus, we first 
study the stabilization characteristics of CBT. Figure 10 
shows the cumulative distribution of cluster head change 
NtNt ≈∞→ )(lim NLKtikt =≈∞→ )(lim
Journal of Internet Technology Volume 7 (2006) No.1 
 
 
32
cluster size lower than 100. It is interesting to see that the 
distribution curves are very close for different cluster 
diameters. This is mainly because of the cluster size 
constraint. In Figure 14, we see that the number of clusters 
in the peer-to-peer system is almost the same. This explains 
why we have such close membership distribution for 
different cluster diameters. 
 
6.4 Comparison of SH-Chord and Chord 
In this section, we compare the performance of 
SH-Chord and Chord using simulation. In the simulation, 
we set cluster diameter in CBT to be 5. We will first study 
the performance of our power-indexed load balance 
algorithm in terms of key loss rate. In our simulation, all 
departed peer nodes are assumed to be off abnormally and 
hence cause key loss. Then we study the search efficiency 
of SH-Chord and Chord. 
Figure 15 and 16 show the key loss rate distribution 
for different key insertion rates in Chord and SH-Chord 
respectively. Four key insertion rates are studied in our 
simulation, one key per node per hour (1/node/hr), 
2/node/hr, 5/node/hr, and 10/node/hr. The results show that 
the probability of high loss rate increases as key insertion 
rate increases in both Chord and SH-Chord. At key 
 
35
37
39
41
43
45
N
um
be
r  
of
  c
lu
st
er
diameter = 3 hops
diameter = 4 hops
diameter = 5 hops
diameter = 6 hops
 
Figure 14 Number of clusters for different cluster diameters 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
50 1800 3550 5300 7050 8800 10550 12300 14050
key loss per hour
C
um
ul
at
iv
e 
D
ist
rib
ut
io
1 key/node/hour
2 keys/node/hour
5 keys/node/hour
10 keys/node/hour
 
Figure 15 Key loss rate distribution of Chord at different key insertion 
rates 
insertion rate 10/node/hr, Chord has more than 30 percent 
of time suffering from over 1500 keys loss per hour and 
has heavy-tailed distribution. On the other hand, SH-Chord 
is much more stable and the loss rate distribution 
converges quickly to 1 at 2300 keys loss per hour. Figure 
17 compares the average of key loss rate for Chord and 
SH-Chord at 4 different key insertion rates. The result 
shows that with power-indexed load balance scheme, 
SH-Chord is much more stable than that of Chord at key 
insertion rate 10/node/hr. 
Chord constructs a global Chord ring and hence all 
searches require a global search. On the other hand, 
SH-Chord constructs a Chord ring on each cluster of CBT, 
and hence searches go from local Chord ring to adjacent 
Chord rings, until the key interested is found or not found. 
Figure 18 shows the routing hops distribution in key 
searching for different community of interest in terms of 
ratio of key searched located in local area and remote area. 
Figure 19 shows the average number of hops a search takes 
for both Chord and SH-Chord. The results show that 
SH-Chord requires less than half of hop counts over Chord 
in key searching at locality to remote ratio 1:1 and less than 
one fifth of hop counts at ratio 9:1. Therefore, SH-Chord is 
much more efficient comparing to that of Chord when 
community interest is regional. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
50 300 550 800 1050 1300 1550 1800 2050 2300 2550
Key lost rate per hour
C
um
ul
at
iv
e 
D
ist
rib
ut
io
1 key/node/hour
2 keys/node/hour
5 keys/node/hour
10 keys/node/hour
 
Figure 16 Key loss rate distribution of SH-Chord at different key insertion 
rates 
0
500
1000
1500
2000
2500
3000
3500
1:1 1:2 1:5 1:10
A
ve
ra
ge
 k
ey
 lo
ss
 ra
te
  .
SHC
Chord
 
Figure 17 Average key loss rate for different key insertion rate 
Journal of Internet Technology Volume 7 (2006) No.1 
 
 
34
Biographies 
 
Tein-Yaw Chung received the A.S 
degree in electronic engineering from 
National Taipei Institute of Technology, 
Taipei, Taiwan in 1980, and the M.S. 
and Ph.D. degrees in electrical and 
computer engineering from the North 
Carolina State University, Raleigh, NC, 
USA, in 1986 and 1990, respectively. From Feb. 1990 to 
Feb. 1992, he was with the Network Service Division, IBM, 
RTP, NC, USA and involved in the research and 
development of heterogeneous network interconnection. 
Since May 1992, he has been with Yuan-Ze University, 
where he is now an associate professor in the Department 
of Computer Science and Engineering. Prof. Chung holds 
several patterns in the area of heterogeneous network 
interconnection while he worked in IBM. His current 
research interests include active networking, peer-to-peer 
networking, multimedia communication and mobile 
computing.  
 
 
Chia-Hung Tsia received the B.E. 
degree in electronic engineering form 
Chung Yuan Christian University, 
Taiwan in 2001, and the M.S. degree in 
computer science and engineering from 
Yuan-Ze University, Taiwan in 2003. 
His research interests were in 
peer-to-peer networking, multimedia communication and 
overlay networks. He current is a wireless mobile software 
engineer in research and design Department, HTC 
Corporation. 
 
 
Ming-Hong Wu received the B.E. 
degree from Yuan-Ze University, Taiwan 
in 2004. He is currently working toward 
the M.S. degree in computer science and 
engineering at the same university. He is 
interested in overlay network, decision 
algorithms for seamless handoffs in 4G 
network. 
 
 
 
 
 
 
Ming-Yen Lai received the B.E. degree 
from Yuan-Ze University, Taiwan in 
2004. He is currently working toward 
the M.S. degree in computer science and 
engineering at the same university. His 
research interests are in overlay 
networks, wireless and mobile networks, 
handoff. 
 
 
 
 
 
Figure 1 The PFC architecture 
 
2.1. Architecture 
The packet filter cache (PFC), as depicted in Figure 1 
has two novel mechanisms - filter rule caching and 
unmatched flow caching. Traditional warm cache stores 
process instructions to accelerate packet processing. 
However, it requires a large cache to increase packet 
processing speed and has a low cache hit rate. Rather than 
caching a set of instructions, PFC caches hashed filter rules 
to increase the speed of the filter rule over that of a 
traditional linear search associated with a warm cache. A 
hashed filter rule uses only a small cache; even a small 
cache can cache many filter rules. Therefore, caching 
hashed filter rules can substantially increase the hit rate. 
 
Table 1: Example of filter rule table 
Rule Src Addr Dst Addr Src Port Dst Port Action
R1 140.138. * 140.*  Eq ftp User1
R2 140.138. 144.* 140.*  Eq ftp User2
R3 140.138. 145.* 140.*  Eq www User2
R4 140.138. * 140.*  Lt 1023 User3
R5 140.138. * 140.* Eq ftp  User4
 
Table 2: Example of packet filter cache mask table 
Rule Mask Action 
R1 16,8,0,16 User1 
R2 24,8,0,16 User2 
R3 24,8,0,16 User2 
R4 16,8,0,6 User3 
 
Consider Table 1 as an example. A mask set can be 
generated as presented in Table 2, according to the prefixes 
of the fields in the filter rules. For example, [16, 8, 0, 6] is 
a 4-dimensional tuple that represents a mask, 
corresponding to rule R4 in Table 2; each mask field 
corresponds to the number of prefix bits associated with the 
IP source, the IP destination, the source port and the 
destination port. All filters with a particular mask are 
mapped to a particular cache table; that is these rules 
require the same number of bits in the IP source, the 
destination fields and so on, to check the filter rule. The 
concatenated prefixes of each field of the filter rule are then 
hashed to represent a filter rule. For example, R4 in Table 1 
is represented by the hashed value of the concatenation of 
140.138, 140, 0 and 1024. 
The PFC architecture includes four components: 
traditional packet filter, cache center, cache dispatcher, and 
feedback handler as shown in Figure 1. The solid lines in 
the Figure 1 represent the possible packet travel paths and 
the dashed lines represent the possible control flows. The 
traditional packet filter in principle can be any existing 
packet filters. In the paper, without loss of generality, we 
use BPF as our filter engine. BPF is one of the most 
popular packet filter engines and used in most BSD 
systems. The Cache Center includes a hash function and 
cache tables. When a packet arrives, the Cache Center will 
do hash function for the packet and determine where the 
packet should go. The Cache Dispatcher forwards a packet 
to each of its matched user space. If the packet is miss hit, 
it is forwarded to BPF for processing. The Feedback 
Handler receives filter rule feedbacks from the packet filter 
and then writes the filter rules to the cache tables in the 
Cache Center and the Cache Dispatcher. Filter update 
creates or removes cache tables from the Cache Center 
when filter rules are inserted or deleted. 
As can be seen from Figure 1, PFC does not need to 
re-engineer the body of existing BPF. What needs to be 
modified to the existing BPF is to create feedback links and 
to connect them to the PFC Feedback Handler. Therefore, 
PFC can be applied easily to any existing packet filter 
architectures. The following sections offer further detail on 
PFC. 
 
2.2. Cache Table Generation And Maintenance 
In PFC, each cache table is associated with a mask 
and each cache entry in a cache table is an entry of (hash, 
checksum, flag, dispatch). Figure 2 reveals an example of 
that records another user space. When the Cache 
Dispatcher receives a hash key, it calls the packet filter 
forwarding function to dispatch the packet to the destined 
user space. Then, the *next field is checked if the *next 
field is not NULL. 
Feedback functions report the matching of a filter rule 
to the Feedback Handler. They send messages with the 
tuple (mask, hash, checksum, user space, flag). The user 
space field may be of one of two types－one is real user 
space that is associated with a matched filter rule, and the 
other is NULL that is associated with an unmatched filter 
rule generated by a feedback function. The Feedback 
Handler analyzes the messages and updates the respective 
cache entry in the Cache Center. Figure 4 presents 
processing by the Feedback Handler. 
 
 
Figure 4 Pseudo Code for Feedback Handler 
 
Figure 1 presents an example to illustrate how PFC 
works. The Cache Center is assumed to have three cache 
tables. In this example, a packet matches a packet filter and 
causes collision at checksum value in cache table mask 1. 
The packet is then forwarded to BPF for processing. After 
the Feedback Handler receives new feedback with a new 
checksum and a flag from the feedback function, it updates 
the cache entry with a new checksum and flag. Then, the 
remainder of this flow will be forwarded directly to the 
user space. 
Another case involves multi-matched filter rules that 
apply when multiple filter rules with a single hash and 
checksum value are matched, when a packet falls through 
BPF. When a packet has been processed in BPF, each 
matched filter rule will return a feedback message to the 
Feedback Handler. If more than one matched filter rule 
shares a single hash and checksum, then a multi forward 
link list is generated to record the user spaces to be 
forwarded when a packet matches these filter rules. 
Packets that do not match any rule will generate an 
unmatched hash with BLOCK in its flag field. Blocking 
these packets prevents some denial of service attacks and 
prevents unmatched packets from falling through all filter 
rules. The blocking function is required for firewall and 
RSVP services. When a packet does not match any filter 
rules, it falls through BPF and triggers the sending of an 
unmatched feedback back to the Feedback Handler, which 
randomly selects a cache table and generates an entry with 
a block flag. When PFC receives another packet in the 
same flow as the preceding one, the packet is blocked in 
PFC and not passed to BPF. 
 
2.4. Updating Packet Filter 
Packet Filter Update maintains the consistency of the 
cache table when users use packet filter tools to insert or 
delete filter rules from the user space. When a filter rule is 
deleted or inserted into BPF, the Packet Filter Update 
searches the cache table in PFC to determine whether the 
filter rules have been cached. If they have, the Packet Filter 
Update will simply delete the corresponding entry from the 
cache table. Using such a simple scheme eliminates the 
possibility of conflict due to filter rule update. 
 
3. Performance Analysis 
 
This section analyzes the performance of PFC by 
performing computer simulation. The performance of PFC 
under various traffic loads, flow interval distributions and 
filter matching ratios is investigated. The performance of 
PFC considered herein, is specified as the size of the PFC 
cache required to perform well, the relation between the 
cache hit rate and the cache size, and the effect of traffic 
interval distribution. 
 
3.1. Simulation Approach 
A simulation environment for PFC, including a set of 
1000 filter rules and an FCFS queue for packet processing, 
is implemented. Each traffic flow has a mean traffic 
interval that is dynamically generated according to a 
probability distribution. To simplify the simulation, the 
packet arrival times are grouped into fixed time slots. 
Packets of a flow arrive at the FCFS queue, according to a 
Poisson process, unless stated otherwise. 
One hundred packets were passed through 1000 BPF 
filter rules and a constant hit rate was assumed to yield 
more accurate simulation results. BPF is assumed to use a 
default length of the CFG lookup algorithm (directed 
acyclic control flow graph [7]). PFC uses the XOR hash 
function and 20 hash tables. 
The processing time of BPF with various PFC hit rates 
is measured. Figure 5 presents the results, which will be 
later used in the simulation analysis. In the experiment, 
measurements were made using i386 processors that were 
running FreeBSD 4.9-Stable, using a 100Mbit/sec Ethernet. 
The testing machine had a 1816 MHz processor and 128 
MB RAM. 
 
 
Figure 5 Processing time simulation for PFC and the 
filter rules, PFC should perform very well. Some flows that 
match the same rules are generated to study the effect of 
the filter matching percentage. The traffic arrival interval of 
these flows is uniformly distributed from 5 to 15. Here, 0.8 
similarity means that 80% of the flows hit 10% of the 
packet filter rules. The simulation results plotted in Figure 
8 reveal that the hit rate of PFC increases as more flows 
match a filter rule. The results also show that if 20% of 
flows exhibit a matching pattern, then the hit rate improves 
to near 20% over that obtained when the traffic flows do 
not exhibit a matching pattern. 
 
Figure 8 Cache hit rate under various similar Rates. 
 
 
Figure 9 Per-packet processing time in BPF and in PFC under 
various percentages of un-matched flows. 
 
3.5. Simulating Processing Time 
The hit rate of PFC can be converted into a processing 
time overhead. Figure 9 and 10 present the processing time 
per packet in BPF and PFC, where in PFC, the buffer size 
is 16384 bytes and the packet arrival interval is uniformly 
distributed from 5 to 15. Figure 9 indicates that at 5000 
flows, PFC with unmatched caching, uses under 5% of the 
processing time used by BPF. As the number of flows 
increases, the frequency of hash collision in PFC increases 
so the average per packet processing time approaches that 
of BPF. Figure 10 indicates the average per packet 
processing time of BPF and PFC under various degrees of 
filter matching. The results demonstrate that that PFC 
needs only half of the per packet processing time required 
by BPF when 20% of 40,000 traffic flows exhibit a filter 
matching pattern. As the fraction of traffic flows that 
exhibit a filter matching pattern increases, PFC can reduce 
packet processing time further. Overall, Figs. 9 and 10 
reveal that PFC markedly outperforms traditional BPF 
without a cache. 
 
 
Figure 10 Per-packet processing time in BPF and in PFC 
under various percentages of similar rates. 
 
4. Conclusions 
 
This work presents a new high-performance packet 
filter architecture, called Packet Filter Cache, for network 
monitoring tools. The PFC architecture adds a filter rule 
cache in front of an existing packet filter. The filter rule 
cache stores the hash value of a filter rule as a hash table 
entry that can be found in a constant memory access. PFC 
exploits of the hash lookup speed to improve filtering 
performance substantially. The design of hash lookup and 
checksum verification can also achieve the Least Recently 
Used effect. Moreover, PFC also caches unmatched packet 
flows to achieve a high hit rate. The lookup performance of 
PFC can be improved by reducing the number of distinct 
masks or the number of cache tables according to the 
Controlled Prefix Expansion (CPE) scheme. Analysis and 
computer simulation are employed to show that PFC can 
considerably reduce processing time – by a factor of 
around four at a cache hit rate of 70%. PFC is an add-on 
cache architecture and so can be readily applied to any 
existing packet filter without re-engineering the existing 
filter module.  
Although using a checksum can avoid most hashing 
conflict, it is not perfect. Currently, we are investigating an 
offline algorithm to generate a conflict free hashing among 
filter rules. A tradeoff study is also on-going regarding to 
the degree of CPE application and PFC performance. 
 
Acknowledgement 
The authors would like to thank the National Science 
Council of the Republic of China, Taipei, Taiwan for 
financially supporting this research under Contract No. 
NSC-95-2213-E-155-005. 
 
Reference 
 
An Overlay Resource Monitor System
*Chih-Chen Wang, *Yung-Mu Chen, Cheng-Hao Weng, *Tein-Yaw Chung
Department of Computer Science and Engineering, Yuan Ze University, Taoyuan, Taiwan,
R.O.C.
E-mail: *{swswggg, armor, csdchung}@netlab.cse.yzu.edu.tw
Abstract This study proposes an overlay resource monitor system Objective of ORMAN is to build a robust, stable topology
which is highly scalable, extendable and self-configurable called for host management, also provides an efficient paradigm to
ORMAN. The monitor system is build upon a Cluster-Based aggregate and search data. In ORAMN, We also design a
Topology P2P system and consisted of a monitor agent. The monitor agent that can load modules on-line for monitoring
monitor agent contains a set of sensors, called Information monitor agnaticanload mde monitorin
Collection Modules (ICMs). ICMs collect the states of monitored abttribut dnamically it thes monitora oRMANis
attributes, record the attributes and compute the statistic values of able to plugIn or pull out the specific monitor moduleinto
monitored attributes in each node. ORMAN provides several system. In data representation and medium of transmission,
resource management mechanisms, such as event notification, ORMAN exploits XML technology achieving atomic
admission control, fault tolerance and load balance. ORMAN also protocol transaction, preventing spanning multiple protocols.
provides a set of APIs for developers to design customized ICMs The article is organized as follows. We discuss related
and plug them into system on the fly. works in Section 2. In Section 3, we give an overview of
Keywords Agent, Aggregation, Distributed Resource Monitoring, ORMAN. In Section 4, we detail the protocols of ORMAN.
Monitoring System, Overlay Network. Finally, Section 5 concludes.
1. Introduction 2. Related Works
With the development of distributed network system, the The most widely-known network management standard,
research to manage and monitor distributed resources [1]-[8] SNMP, has some drawbacks tured up but not solved yet,
has received more attention. In peer-to-peer network, every e.g. inefficient transferring management data, no standard
peer must provide its healthy status to management station formats for storing and processing of management data, and
for maintaining entire topology efficiently. In grid inefficient software development cycles.
computing, the deployment of tasks must take nodes' To overcome these disadvantages, T. KLIE [11] employed
capabilities into account to balance the system load. In other XML technology and proposed a SNMP-to-XML gateway
words, a stable monitoring and management system is a key that transparently translates HTTP requests for XML
substantial for resource management and service documents to SNMP operations. This method, however,
deployment. could cause load unbalance in a large network topology, and
A management system consists of three components: not scalable. In traditional resource management systems,
a. Network Elements (NEs): Monitored nodes in central control limits the scalabililty of network topology,
network. Usually, these nodes are asked executing a and could make single point failure happen.
"Agent" in background to collect resource A resource management system for distributed networkinginformation. Every "agent" also provides a interface must track every host's status in the topology and
for system managers accessing its collected resource reconfigure monitoring components while hosts entering or
information. leaving network to maintain entire system steady andb. Network Managers (NMs): Specialized nodes to reliable.
collect, evaluate, and publish resource information Astrolabe [12] used a multi-level hierarchical architecturefrom NEs. to provide scalable management and self-organizational
c. Communication Protocol (CP): A set of rules for data capabilities for distributed network system, also given
exchange between NEs and NMs. dynamic-installed SQL programs achieving On-the-fly data
In a typical client-server network management platform, aggregation. But, Astrolabe is not suitable for aggregating
SNMP [9] standard defines the CP and data exchange format high variable-rate monitoring data, and needs to build
between NMs and NEs. Besides, SNMP introduces the another topology to accomplish data diffusion.
Management Information Base (MIB) [9] to maintain the In the one of most complicated environments, p2p
monitoring data in NMs. With MIB, users can retrieval data, (peer-to-peer) network, peers join or depart arbitrarily. The
query and configure the NMs' setting But a central-control
mechaismakestoo uch ommuicaton oerhed on Kazaa [14] usually have no structured topologies to manage
topology and involves too many resources in the central priiat.Teei ogaateo idn eial
managementnode. i~~~~~~nformation in bounded time. The flooding-based routingInthsinetiaio,wepopseane vely eouc schemes are clearly not scalable and their traffic load also
management system called ORMAN (Abbreviation of
Overlay Resouce MANager.), based on CBT grows up linearly with the size of the system.
* ^ ~~~~~~Thes cond generation of p2p systems, for instance, Chord(Cluster-Based Topology) [10] to build an infrastructure for [15] Pastiy [16], Tapestry [17] and CAN [18] are more
managing hosts and routing management data. The'
ISBN 89-5519-129-4 - 1875 - Feb. 20-22, 2006 ICA0T2006
Update-Upk-Downj algorithm to summarize data in a program that can load or unload on-line.
cluster. e. Module Manager: Module Manager manages the
Every cluster leader keeps the summarized data which affairs of ICMs, including loading, unloading,
providing inner-cluster and cluster-to-cluster searching. authentication and remote deployment.
When a cross-cluster query occurs, this means a node in View monitor agent in a different light, the six
another cluster try to search desirable data between clusters, components can be classified into two categories:
ORMAN need to collaborate these clusters to accomplish a. Network Management: Preside over the transmission
this request. Besides, the organization between clusters will affairs of data, for instance, data format
effect the selection of searching algorithm. As shown in transformation, routing table maintenance and
Figure 1, cluster leaders link to each other arbitrarily. This notification. Dispatcher, XML Processor and Event
pattern allows clusters exchange information to each other Handler are belongs to this category.
arbitrarily but not efficient. As shown in Figure 2, clusters b. Module Management: In charge of managing
are organized as a circle. In this pattern, we can apply modules, gathering and maintaining the data. Event
lookup scheme in Chord to provide search service. Log Generator, ICMs and Module Manager are
When a node issues a query, the local cluster leader will belongs to this category.
resolve this request. If the local cluster leader can't resolve it, Monitor agent has three features. First, XML-based data
for example, no matching file, then the cluster leader regards processing enables straight-forward data presentation and
this query as it self's query and issues it to other cluster. convenient data transformation. Second, modular
Also, to reduce the overhead of repeat query the same architecture allows the system administrator develops
request, cache mechanism must be addressed. customized monitor modules for new rising network
W #niiiig services. ORMAN also provides a set of APIs for developers to
design customized ICMs and plug them into system on the fly.
Final, as mentioned above, monitor agent provides
deploying module dynamically. With this feature, the system
administrator can bundle a specified application with a
monitor module and deploy it easily.
The procedure of remote deployment as follows. First, the
Module I I Event H1andler package which includes an application and a monitor
module are encoded by Base64 [20] algorithm. Then, the
encoded content is embedded in the transmitted XML
document. When monitor agent receives this XML
I frforiatLtDrtlIl031atiOElildocument, monitor agent decodes the content, installs theCo I ec or l ectr _
MOc4 . I ,, Event 1.og monitor module and executes the application.
Figure 3. The Architecture of ORMAN Monitor Agent 4. The Protocols ofORMAN
ORMAN uses a modular XML-based monitor agent,
which collects attributes from local host. As shown in Figure In this section, we introduce the protocol of ORMAN,
2, the architecture of ORMAN monitor agent consists of six including node joining/departure, cluster splitting/merging
components: data aggregation and data search.
a. Dispatcher: Dispatcher is in charge of dispatching g RE N
query requests and operating commands to I RALAN.N"br-o Mlembm - l)
corresponding component, also it is a communication ELSE II(LAuN.Numbrof ORNAN Mnbmrj'oining node hecomes supr trde of the fir chluskr;interface of entire agent. ELS: IIRLAN.NrLetCrutAeDitn1ce > thre-%uhok))
I real a nw ciuhsterb. XML Processor: In monitor agent, any request or ELSE
command must be encapsulated in a XML document. jolining noide becm a super node
Hence, XML Processor takes charge transforming j io"ig fodcR xtsan metmbr f thbnearest tsupr nde
data representation format between XML documents IF| ENl} PRX)CuEOUREl
and agent inner data structure like linked list. Figure 4. The Pseudo-code of Peer Join Scenario
c. Event Log Generator and Event Handler: They cope Figure 4 presents the pseudo-code for node join algorithm.
with error-handling and event recording. When an ORMAN always regards any new node as Regular node.
event occurs, whether it is an error message (e.g. When a new node joins ORMAN, there are two phases to do.
service has been shut down.) or a notification (e.g. a First, the new node broadcasts JOIN message to the LAN
new module has been loaded.), according to (local area network) to find whether there is a member of
preconfigured parameters, Event Handler takes ORMA4AN. If yes, the new node becomes a member of the
actions (e.g. send a mail to the administrator or group which is associated with the already existent member.
publish a notification to other agent.) and orders If no, the new node contacts Bootstrap. Then Bootstrap
Event Log Generator record this event in local file. assigns a cluster to the new node for joining, according the
d. Information Collector Module (ICM): ICM is actual shortest distance (e.g. Round-Trip-Delay) between the new
attributes collector in local host and a modular node and each cluster leader. When the cluster which the
ISBN 89-5519-129-4 - 1877 - Feb. 20-22, 2006 ICA0T2006
Regular node A issues a query request to the cluster leader C. [15] I. Stoica, R. Morris, D. Karger, M. Kaashoek, and H. Balakrishnan,
C finds a match result in its table and sends a super node list "Chord: A Scalable Peer-to-Peer Lookup Service for Internet
to A. A picks a- node S ftom the list and contactsApplications," Proc. ACM SIGCOMM, 2001.to A. A piCKS up a super nodle S from the i1st andl contacts [16] A. Rowstron and P. Druschel, "Pastry: Scalable, Decentralized
with S. S verifies the request and send a qualified node list Object Location and Routing for Largescale Peer-to-Peer Systems,"
back to A. Finally, A gets a node list satisfies its request. Proc. IFIP/ACM Int'l Conf. Distributed Systems Platforms,
Then, A can selects a node T form the list to do with other 2001.
operations, such requiring sharing filesegments. -[17] B.Y. Zhao, J.D. Kubiatowicz, and A.D. Joseph, "Tapestry: Anoperations, such as requiring sharing file segments. Infrastructure for Fault-Tolerant Wide-Area Location and Routing,"
At the present time, we organize clusters as a circle. Technical Report UCB/CSD-01-1 141, Univ. of California Berkeley,
Cluster-to-cluster search can be applied with the lookup 2001.
scheme in Chord. [18] s. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker, "AScalable Content-Addressable Network," Proc. ACM SIGCOMM,
2001.
5. Conclusion and Future Works [19] P. Yalagandula and M. Dahlin. "A Scalable Distributed Information
Management System", in Proc. ofACMSIGCOMM, August, 2004.
ORMN/IAN can attain to several goals: 1) Robustness, 2) [20] The Basel6, Base32, and Base64 Data Encodings, RFC3548,http://www.ietf.org/rfc/rfc3548.txt
Management overhead distributing, 3) Flexibility, 4) High [21] JXTA platform, http://wwwjxta.org/
efficient Data Aggregation and Data Searching. In the future,
we will introduce positioning algorithms to improve the
locality of ORMAN and apply different search algorithms
such as Interactive Deepening Search on Data Searching to
compare the performance. The authentication and security
issues in ORMAN must be addressed. We now also are
working on porting ORMAN to the JXTA platform [21].
Reference
[1] Y. Jiang, C.-K. Tham, and C.-C. Ko, "Challenges and approaches in
providing QoS monitoring," Intl. J. ofNetwork Management, Vol. 10,
No. 6, pp. 323 - 334 Nov./Dec. 2000
[2] A. H. Asgari et al., "Building Quality-of-Service Monitoring
Systems for Traffic Engineering and Service Management," J. of
Netw. and Sys. Management, Vol. 11, No. 4, pp. 399 - 426, Dec.
2003
[3] D. W.-K. Hong and C. S. Hong, "A QoS management framework
for distributed multimedia systems", Intl. J. of Netw. Management,
Vol. 13, No. 2, March/April 2003
[4] I. Satoh, "Configurable Network Processing for Mobile Agents on
the Internet", Cluster Computing: the J. of Netw., Software Tools
and Appl., Vol. 7, No. 1, pp. 73 - 83, Jan. 2004
[5] R. V. Renesse, K. P. Birman, and W. Vogels, "Astrolabe: A robust
and scalable technology for distributed system monitoring,
management, and data mining", ACM Trans. on Comput. Sys., Vol.
21, No. 2, pp. 164 - 206, May 2003
[6] C. Estan, S. Savage, and G. Varghese, "Automatically inferring
patterns of resource consumption in network traffic", in Proc. of
ACMSIGCOMM2003, Vol. 33, No.4, pp. 137-148, Oct. 2003.
[7] L. Cherkasova et al., "Measuring and characterizing end-to-end
Internet service performance", ACM Trans. on Internet Technol., Vol.
3, No. 4, pp. 347 - 391, Nov. 2003.
[8] L. Lymberopoulos, E. Lupu, and M. Sloman, "An Adaptive
Policy-Based Framework for Network Services Management," J. of
Netw. and Sys. Management, Vol. 11, No. 3, pp. 277 - 303, Sept.
2003.
[9] D. Harrington, R. Presuhn, and B. Wijnen, "An Architecture for
Describing Simple Network Management Protocol (SNMP)
Management Frameworks", IETF STDO062, Dec. 2002
[10] T. Y Chung, et al., "Architecture and Implementation of
Cluster-based Peer-to-Peer Topology and Its Application in Search",
to appear in J. ofInternet Technol. (JIT), 2005.
[11] T. Klie and F. Strau,B, "Integrating SNMP Agents with XML-Based
Management Systems", IEEE Commun. Mag., pp. 76-83, July 2004
[12] R. V. RENESSE, K. P. BIRMAN and W. VOGELS, "Astrolabe: A
Roubust and Scalable Technology for Distributed System
Monitoring, Management, and Data Mining", ACM Trans. on
Comput. Sys., Vol. 21, No. 2, May 2003, Pages 164-206.
[13] Gnutella, "The Gnutella Home Page," http://www.gnutella.com/,
2005.
[14] KaZaa, "The Kazaa Home Page," http://www.kazaa.com/, 2003.
ISBN 89-5519-129-4 - 1879 - Feb. 20-22, 2006 ICA0T2006
2 Problem Statement
First, the network model is described, the definition and
objectives of peer-to-peer file distribution model are given.
2.1 Network Model
Consider a set of peer users dispersed in the Internet. The
following properties are assumed about a file distribution
network in overlay networks:
• The number of peer nodes increases and decreases dy-
namically denoting typical user behavior for overlay
network applications.
• Links are asymmetric, i.e., the transmission capacities
from nodes v1 to v2 and from v2 to v1 are not equal,
according to current network configuration, such as
ADSL.
• A node can connect directly to any other node in the
network. As long as the transport network is strongly
connected, any two nodes can set up a logical link.
Therefore, links have very different propagation de-
lays.
• After a peer finishes downloading a file, it immedi-
ately publishes the file and becomes a peer server.
Let N be a set of vertices denoting the peer clients
interested in file F . Since in current practice such as
ADSL most up-link bandwidth is smaller than the down-
link bandwidth, here in this study, we assume that the up-
link bandwidth of a server peer dominates the download-
ing bandwidth between peers. Moreover, when two peers
are far away, the effective downloading bandwidth between
them tends to become smaller due to congestion or bottle-
neck in the transport network. Thus, the downloading ca-
pacity Cij from node i to node j is defined as Cij = αijBi,
where Bi is the up-link bandwidth of node i and αij is a
coefficient inversely proportional to the delay latency dij
between node i and node j and has a value between 0 and
1.
Let a file distribution network T at ti be Ti with
vertex set Nti={v|v ∈ N} where v has F , and link set
Eti ={luv |u, v ∈ N} where v downloads F from u.
2.2 P2P File Capacity Amplification Prob-
lem
If node i requests node j for file downloading at ti, the
service completion time of node i, Ai, is given by
Ai = ti + Tsi (1)
where ti is the service requesting time by node i and Tsi
is the downloading service time spent for node i to finish
file downloading. In particular, if node i downloads F from
node j, then
Tsi = Wji + Td (LF , Cji) (2)
where Wji is the waiting time before the downloading re-
quest from node i is served by node j and Td (LF , Cji) de-
notes the service time to download file F of size LF from
node j to node i, defined in Eq.(3).
Td (LF , Cji) = Dji +
LF
αjiBj
(3)
In Eq.(3), Dji denotes the delay latency, which is equal
to half of the round-trip delay between j and i, and Cji
denotes the service capacity from node j to node i. After
Ai, node i ∈ NAi can offer downloading service to other
client peers.
Definition 1. Let Fs be the source for file F , and S =
{t1, t2, · · · , tn} be the request timing sequence generated
by the peer clients v1, v2, · · · , vn ∈ N for F . The file ca-
pacity amplification problem T (N, Fs, S) is to select client
peers awaiting for downloading service in an order to max-
imize the cumulative service capacity CFs (t) for file F at
any time t, where
CFs (t) =
∫ t
0
BFs (t) dt (4)
and
BFs (t) =
∑
j∈Nt∧Aj≤t
Bj . (5)
By ignoring the possibility of peer node leaving and
free-riding, CFs (t) becomes a monotonic increasing func-
tion. A greedy approach for solving T (N, Fs, S) is to in-
clude a node j with maximal B in Nt with minimal Aj
or Tsj , i.e., maximizing BFs (t) whenever possible. The
greedy approach treats the system as a multi-server queu-
ing system and adopts a priority scheduling scheme to max-
imize the server throughput and minimize the average ser-
vice time of the system.
Lemma 1. The greedy approach to including a node j with
maximal B in Nt is not optimal.
Proof. A proof by counter-example is given herein. Peer
i and node j are waiting at peer server S for service at
t, and have Bi and Bj respectively and Bi ≤ Bj . Two
other peers, k and l, make requests after t and are close
to i but far from j. Therefore, the effective capacity from
peer i to k and l is much larger than that from peer j to
k and l due to network bottleneck, i.e., Cik >> Cjk and
Cil >> Cjl. Assume that the service times for node j and
i are given by ∆t; the service times for node k and l by
node j are given by ∆t1, and the service times for node k
and l by node i are given by ∆t2, where ∆t1 >> ∆t2.
Also assume that serving node j first makes nodes k and
l direct their requests to j. Hence, the available times of
Table 1. Notations definitions.
D Half round-trip time between the request-
ing and supplying peers.
Dmax Maximal delay between a peer server and
its peer clients.
LT Lifetime (online time) of each peer in the
system.
LTmax Longest online time of all peers.
BU Upload capacity of the selecting peer.
BD Download capacity of the selecting peer.
BUmax Maximum possible upload bandwidth of
all peers.
Pi Number of the requesting peer’s neighbors
within a cluster
Pimax Maximum number of neighbors in a clus-
ter.
D
Dmax
Delay latency factor.
LT
LTmax
Lifetime factor.
BU
BUmax
Bandwidth factor.
Pi
Pimax
Penetration index.
populated area with few supplying nodes. Thus, once the
peer client becomes a peer server, it can potentially serve
peers within its neighborhood with a large service capacity.
A long-distance first downloading strategy can also deploy
peer servers in remote areas quickly so that other distant
peer clients can download file from nearby peer servers.
Considering both ideas above, the purpose of CAP
is to give priority to a peer whose neighborhood is more
populated and is some distance from the peer server. Based
on the above analysis, the variables and some properties
involved in the penetration index function are defined as
shown in Table 1. To evaluate the potential of a peer client,
this study defines a heuristic cost function potential P as
follows:
P = α×
(
D
Dmax
)
+ β ×
(
LT
LTmax
)
+ γ ×
(
BU
BUmax
)
+ (1− α− β − γ)×
(
Pi
P imax
)
(7)
where 0 < α, β, γ < 1 and α + β + γ ≤ 1. Note that
the potential P also includes the peer living time. Thus,
a stable peer client will be given a higher priority when
a peer server selecting peer clients for services. This can
keep Nt close to a monotonously increasing set, and hence
improves the efficiency of file capacity amplification.
To implement these strategies, a new protocol must be
associated with each of them, except for FCFS in which the
arrival sequence of requests determines its scheduling. In
HCF, a peer must include its credit in the service request,
based on which peer servers can schedule their service pri-
ority. In CA, a peer must indicate its up-link bandwidth in
its request to help peer servers scheduling their services. In
Table 2. System parameters in the simulation.
Parameter Description
Delay per phys-
ical link Di
Di is uniformly distributed over
[0, 60] msec; Dmax=500 ms.
File size LF 650 Mbytes
Bandwidth
B [9]
70% with BU=1Mbps,
BD=64Kbps; 23% with
BU =2Mbps, BD=256Kbps; 7%
with BU=3Mbps, BD=512Kbps.
Life time LT [9] 60% of the peers maintain the
session duration at most 60 min-
utes; 84% of the peers stay no
longer for 10 minutes if they do
not find desired files.
Service capacity
C
Cij = αB
U
i , α=1 if Dij ≤100
msec; α=0.7 if 100 msec<
Dij ≤250 msec; α=0.5 if 250
msec< Dij .
Number of re-
sponses R [10]
A positive response to a query
message means that the file trans-
fer between two peers can be es-
tablished. R=1 for 87% peers;
R is uniformly distributed over
[2, 15] for 7% peers; R is uni-
formly distributed over [16, 500]
for 6% peers.
CAP, each peer must configure a set of common global pa-
rameters Dmax, LTmax , B
U
max, and Pimax. When a peer
client makes a request, it must include a time stamp in the
request and report its uplink bandwidth, living time and
number of its neighbors. A peer server can compute de-
lay latency based on the time stamp in the request and then
use the information given by the peer client to calculate P
and determines the service priority of its clients. At first
sight CAP seems difficult to implement. However, the only
thing special for CAP is that it requires the P2P network
supports clustering and can report to a peer the number of
active peers in its cluster.
4 Simulation Results and Performance Anal-
ysis
To evaluate the performance of each strategy, a simulation
environment for the FCA model was implemented based
on parameters measured in existing operational P2P net-
works. This section introduces the simulation environment
and discusses the simulation results.
4.1 Simulation Environment
In simulation, the topology generator GT-ITM [8] is used
to generate a topology which includes the location of nodes
 0
 500
 1000
 1500
 2000
 2500
 3000
 0  10  20  30  40  50  60  70
N
um
be
r o
f n
od
e 
wi
th
 th
e 
file
Hour 
 (a) Comparison of CA and CAP in different queue sizes.
 CA buffer =  50
CAP buffer =  50
 CA buffer = 100
CAP buffer = 100
 CA buffer = 150
CAP buffer = 150
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 5  10  15  20  25  30  35  40
N
um
be
r o
f b
lo
ck
ed
 re
qu
es
ts
 ( x
 10
00
0)
Hour 
 (b) Cumulative blocking rates in CAP.
Freq:  1/min, buffer: 100
Freq:  1/min, buffer: 300
Freq: 10/min, buffer: 100
Freq: 10/min, buffer: 300
Freq:100/min, buffer: 100
Freq:100/min, buffer: 300
Figure 2. Performance of CA and CAP in different queue
sizes and request frequencies.
fic across a wide area network. Furthermore, when clusters
are large, a cluster head has a heavy load in managing its
cluster. Therefore, the cluster radius should not be large.
Figure 2(a) shows the behavior of peers with differ-
ent queue sizes, clearly indicating that peers with larger
queue sizes distribute a file faster than those with smaller
one since peers with larger queue size have more and bet-
ter chance to choose their clients. However, a large queue
size might overload a peer server. The experimental results
show the improvement in performance from queue size 100
to 150 is not as much as that from queue size 50 to 100.
That is because of the choice of clients become less signif-
icant on the speed of file distribution.
Figure 2(b) shows the impact of the file request fre-
quency and servers’ queue size, clearly indicating that more
requests are blocked at a larger request frequency, particu-
larly at a small queue size. However, when queue size is
large, the file distribution speed does not change much, be-
cause most requests are queued in the buffer. Therefore
the speed of file distribution is affected by the total effec-
tive service capacity but not the queue size. The results in
Fig. 2(b) also confirm this observation. Figure 2(b) shows
the number of requests blocked stops increasing about 25
hours after the file starts distributing.
 0
 500
 1000
 1500
 2000
 2500
 3000
 0  10  20  30  40  50  60  70
N
um
be
r o
f n
od
e 
wi
th
 th
e 
file
Hour
alpha=0.3, beta=0.6, gamma=0.1
alpha=0.2, beta=0.4, gamma=0.4
alpha=0.1, beta=0.2, gamma=0.7
alpha=0.0, beta=0.1, gamma=0.9
gamma=1
Figure 3. Relationship between α, β and γ.
Comparing server’s queue size and clients’ request
frequency shows that servers influence the performance of
file distribution more than clients do, although both servers
and clients are greedy: servers intend to serve the client that
could serve other peers more, and clients tend to forward
their requests to peer servers with large capacity. Servers
with large queues can queue more requests, and thus have
more opportunities to choose a good client for service, than
servers with small queues. Consequently, the file can be
distributed faster as more “good” peer nodes become file
distributors.
4.2.2 Service Scheduling Strategy Parame-
ters
After comparing different system parameters, the following
simulation results focus on the relationship between α, β,
and γ in the 5-hop structured peers. In all simulations, the
upload bandwidth of the initial source was set to 256 kbps,
and the queue size of each peer was 100.
Figure 3, considering α, β, and γ shows that choos-
ing peer clients with large bandwidth spreads a file to many
nodes quickly. However, Fig. 4, which compares the aver-
age delay between nodes for file downloading, shows that
the maximum γ value does not perform best in reducing the
average delay, which represents the average distance of file
downloading from a peer server to a peer client. After 12
hours, the combination of α = 0.1, β = 0.2 and γ = 0.7
was found to reduce the average delay the most, generating
less traffic load than γ = 1. Additionally, the file distribu-
tion with α = 0.1, β = 0.2, γ = 0.7 was slightly better
than that in α = 0, β = 0.1, γ = 0.9. Thus the lifetime
factor does help facilitate file distribution by serving stable
peers first.
Figures 5 shows the influence of the Bandwidth and
Penetration factors. Bandwidth factor helps increase the
distribution speed. In Fig. 6, all factors are combined to
observe their behaviors. Clearly, the ratio of α = 0.2, β =
References
[1] H. Sunaga, et al., “Technical Trends in P2P-Based
Communications,” IEICE Trans. Commun., vol. E87-
B, no. 10. 2004, pp. 2831–2846.
[2] S. Androutsellis-Theotokis and D. Spinellis, “A sur-
vey of peer-to-peer content distribution technologies,”
ACM Comput. Surv., vol. 36, no. 4, 2004, pp. 335–
371,.
[3] D. Xu, et al., “On Peer-to-Peer Media Streaming,”
Proc. IEEE ICDCS 2002, Vienna, Austria, 2002, pp.
363–371.
[4] Y. Qiao et al., “Looking at the Server Side of Peer-to-
Peer Systems,” Proc. ACM LCR 2004, Houston, USA,
2004, pp. 1–8.
[5] M. Gupta, P. Judge, and M. Ammar, “A Reputa-
tion System for Peer-to-Peer Networks,” Proc. ACM
NOSSDAV 2003, Monterey, USA, 2003, pp. 144–152.
[6] S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina,
“The EigenTrust Algorithm for Reputation Manage-
ment in P2P Networks,” Proc. ACM WWW 2003, Bu-
dapest, Hungary, pp. 640–651.
[7] T. Y. Chung, et al., “Architecture and Implementation
of Cluster-based Peer-to-Peer Topology and Its Ap-
plication in Search,” J. of Internet Technol. (JIT), vol.
7, no. 1, 2006, pp. 23–34.
[8] GT-ITM: Georgia Tech Internetwork Topology Mod-
els, http://www.cc.gatech.edu/projects/gtitm/
[9] S. Saroiu, P. K. Gummadi, and S. D. Gribble, “A
Measurement Study of Peer-to-Peer File Sharing Sys-
tems,” Proc. MMCN 2002, Jan. 2002.
[10] D. Nogueira et al., “A Methodology for Work-
load Characterization of file-sharing peer-to-peer net-
works,” Proc. IEEE WWC-5, Austin, USA, 2002, pp.
118–126.
[11] M. T. Schlosser, T. E. Condie, and S. D. Kamvar,
“Simulating a File-Sharing P2P Network,” Proc. First
Workshop on Semantics in P2P and Grid Computing,
2002.
Fig. 1. SOP system architecture.
The remainder of this paper is organized as follows. Section
2 describes the SOP architecture. Section 3 then defines an
SIU, and presents the SSA. Next, Section 4 describes the
proxy cache replacement algorithm. Section 5 summarizes the
simulation results and the performance analysis. Conclusions
are finally drawn in Section 6.
II. SOP ARCHITECTURE
SOP operates on a peer-to-peer network with clustering
as depicted in Fig. 1, in which peer nodes are split into
many clusters based on their geographical proximity, and each
cluster forms a search space. When searching for a resource,
a peer searches a local cluster first and then proceeds to other
remote clusters. A peer user can restrict its search to a local
cluster, or flood the whole peer-to-peer network with its search.
According to Fig. 1, SOP has three major components, a
video server, a proxy server and a super-object server (SOS).
A video server is a peer node that publishes its video programs
via a peer-to-peer network. Every peer node that retrieves a
film from a video server becomes a proxy server. A film is
streamed to a peer user client in a sequence of SIUs. A proxy
server publishes its cached SIUs to a super-object server (SOS)
[7] for the film in its local cluster. SOS is a software module
that can be run by agent or active networking technologies
whenever a new film is played back in a cluster. SOS maintains
all SIU caching information for a film in a local cluster. A
system such as Chord [6] chooses a peer node with a key that
matches that of the film to run the SOS for that film. Therefore,
a peer client can easily locate an SOS without a control head,
allowing the SIU directory services to be distributed to peer
nodes when many video programs are played back at the peer
nodes of a cluster.
When a peer node wants to play a video program, it first
locates if there exists a SOS for the video program. If it does,
the peer user requests the SOS for the SIU profiles in the
video program and a list of SIU caching information within the
cluster. Every proxy server performs a simple call admission
control to ensure every SIU is transmitted to other peer nodes
in a promised data rate. If an SIU is not available in the local
SOS or downloading from other proxy servers is infeasible
due to resource constraint, the peer node just sends a request
to the original video server for downloading.
When a peer node wishes to play a video program, it must
first locate an SOS for it. If the node finds an SOS, then
Fig. 2. Buffer-constraint smoothing example.
the peer user requests the SOS for the SIU profiles in the
video program and a list of SIU caching information within
the cluster. A peer must transmit a downloading request to the
chosen proxy peer before it decides to download an SIU from
a proxy peer. On the other hand, each proxy server performs
a simple call admission control to ensure that every SIU is
sent to other peer nodes at a promised data rate. If an SIU
is unavailable in the local SOS, or if downloading from other
proxy servers is infeasible owing to resource restrictions, then
the peer node simply sends a request to the original video
server for downloading.
III. SIU AND SSA ALGORITHM
SIU is the basic caching unit in SOP. Each SIU comprises
several groups of pictures (GOPs), and has a unique identity. A
GOP is formed of one I frame, along with the B and P frames
before the subsequent I frame. This study assumes that each
SIU comprises video data requiring 5 seconds of playback
time. SOP adopts an SIU-based smoothing algorithm (SSA),
which processes each SIU offline and generates a transmission
profile for it. This section presents the smoothing algorithm
concept and SSA.
A. Smoothing Algorithm
Smoothing algorithms have previously been proposed to
maximize the reduction in rate variability when sending a
stored video from a server to a client across a network.
Previous smoothing algorithms [4] consider both the buffer
constraint of a client and the bandwidth constraint between a
server and a client when calculating a feasible transmission
schedule S∗ comprising a set of time intervals and associated
transmission rates, indicating the periods over which the server
must send in a constant-bit-rate (CBR) fashion.
Figure 2 shows an example of a feasible transmission sched-
ule S∗ that does not cause the the client to starve (indicated
by A (t) < D (t), where A (t) denotes the cumulative amount
of data arriving at the client over [0, t], and D (t) denotes the
cumulative amount of data consumed by the client at t) or
overflowing (indicated by A (t) < B (t), where B (t) denotes
the maximum cumulative data that can be received by the
client over [0, t]) during playback. However, S’ is infeasible
since it hits B (t), i.e. overflows the client buffer, during
video playback. The example in Fig. 2 is a buffer constraint
smoothing problem. Considering the buffer size of a client,
TABLE I
SUMMARY OF BR ALGORITHM SYMBOLS.
ts SIU finish transmission time.
te SIU start transmission time.
tp SIU playback time.
BR rate The minimal allowable transmission rate.
[tp, ts] Time interval between tp to ts.
Tw#1 Early pre-loading time before SIU playback.
SIU List All SIU profiles for the video program.
at a transmission rate below its BR rate if the preceding SIU
is also downloaded in the BR rate.
The BRR algorithm first calculates a new cumulative curve
associated with the preceding SIU downloaded in BR rate. A
test similar to the BR algorithm is conducted for the current
SIU to try to find a lower feasible transmission rate than its
BR rate. If a lower rate is found, then the new BRR rate
and the start transmission latency are stored in the respective
SIU profile. To lower the computation overhead, the BRR
algorithm only tests whether current SIUi can be pre-loaded
at the playback time of SIUi−2.
After SSA finishes processing a film, an SIU profile is
generated for each SIU consisting of an SIU serial number, the
transmission rate and respective startup transmission latency
of each transmission mode, and a peak bit. The peak bit
is referred during selective caching when a peer determines
whether an SIU should be cached at a higher priority than a
regular SIU. The profile of each SIU for a film is saved in
a file in the video server. This file is then downloaded to the
SOS, and thence to the peers watching the film. According to
the SIU profiles, peers can schedule their SIU downloading
services from those peers that provide proxy services.
IV. SIU CACHING REPLACEMENT ALGORITHM
Peers in a P2P network can join and leave dynamically. In
particular, peers are generally end-hosts with heterogeneous
and limited capability and up/down link bandwidth. Hence, a
peer probably only caches some of the SIUs that it had played
since it started accessing a video program.
This section presents four different SIU caching strategies,
namely access-unaware, local access-aware (LAA), regional
access-aware (RAA) and global cooperative caching (GCC),
to enable a peer to determine which SIU is most valuable
and which SIU should be replaced when its buffer is full. In
the access-unaware and LAA strategies, a peer makes caching
and replacement decision alone, without any information from
other peers. These strategies are simple with little overhead.
Conversely, in the RAA and GCC methods, a peer adopts the
SIU caching information and other peers’ behavior collected
by SOS for SIU caching and replacement decision. RAA and
GCC only cause small overhead on the SOS, but potentially
improve the view of the peer access pattern and the effective-
ness of caching. For the convenience of discussion, Table 2
defines the variables adopted in the descriptions. Note that the
residual time of an SIU is defined as the residual playback
time of a film in the considered peer based on the assumption
TABLE II
SYMBOLS OF SIU CACHING STRATEGIES.
Cf The size of Free cache.
N The number of SIUs in a film.
SPi SIUi priority.
Si The size of SIUi.
LNPi The number of peers in SIUi’s revenue window
recorded locally in a peer.
CNPi The number of peers in SIUi’s revenue window
recorded by SOS.
Rbi SIUi’s BB rate.
B The peer’s uplink bandwidth.
RT Residual time of the film.
that the peer watches the entire film and then leaves the system
immediately.
A. Access-Unaware Caching
In the access-unaware method, a peer does not track whether
other peers have requested downloading service from it, hence
its name. A peer receiving a new SIUi, caches it if the
following rules are satisfied:
1) SIUi is not downloaded from a proxy peer (i.e., it is
from the video server)
2) B ≥ Rbi
3) Cf ≥ Si
The above rules indicate that an SIU is cached by a peer
only if it cannot be downloaded from other peers, the peer is
capable of offering downloading service for the SIU, and the
peer has a large enough cache for the SIU.
B. Access Aware Caching
Access aware schemes define a new priority function for
SIUs. This priority function is based on the assumption that
users do not always watch the whole film in video playback,
meaning that the earlier SIUs have higher access rates in the
film. The caching priority of an SIU is defined as follows:
SPi = 1− (i/N)2 (2)
The priority function gives larger priority values to SIUs
with smaller sequence numbers. However, the priority value
decreases quickly as the sequence number rises.
1) Local Access Aware Algorithm: In the local access-
aware (LAA) method, each peer logs its own peer access
information and adopts the following cost function to evaluate
SIUi as a reference in the caching and replacement decision.
Vi = SPi ×
(
LNPi ×Rbi
) (3)
The cost function gives priority to SIUs that potentially
provide the highest bandwidth savings on a server.
In LAA, each proxy peer determines SIU caching and
replacement on its own and proxy peers do not cooperate with
each other. LAA provides a relay-and-forward service to other
peers that have previously requested SIU downloading via the
cost function defined in Eq. (3), and also offers prefix and
selective caching service as in the access-unaware method.
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
606321.5
H
it 
ra
te
 (%
)
Arrival rate (users/hour) 
 (a) 2 : 1 : 1 : 0
Un-access
LAA
RAA
GCC
 5
 10
 15
 20
 25
 30
 35
606321.5
H
it 
ra
te
 (%
)
Arrival rate (users/hour) 
 (b) 1 : 2 : 1 : 0
Un-access
LAA
RAA
GCC
 5
 10
 15
 20
 25
 30
 35
606321.5
H
it 
ra
te
 (%
)
Arrival rate (users/hour) 
 (c) 1 : 1 : 2 : 0
Un-access
LAA
RAA
GCC
 5
 10
 15
 20
 25
 30
 35
 40
606321.5
H
it 
ra
te
 (%
)
Arrival rate (users/hour) 
 (d) 1 : 1 : 1 : 1
Un-access
LAA
RAA
GCC
 5
 10
 15
 20
 25
 30
 35
 40
606321.5
H
it 
ra
te
 (%
)
Arrival rate (users/hour) 
 (e) LAA’s hit rate with various ratio of last mile link types.
2:1:1:0
1:2:1:0
1:1:2:0
1:1:1:1
Fig. 7. Performance of caching and replacement methods by the single source scheme with various ratios of last-mile link types.
VI. CONCLUSIONS
Overlay networking must face heterogeneity of computing
capacity and access link of end hosts in continuous media
transmission, especially when end hosts provide proxy ser-
vices. This study has presented an SIU-based overlay proxy
service (SOP) to solve the buffer and link bandwidth constraint
of end hosts, and to support true VoD services. An SIU-based
smoothing algorithm (SSA) has been developed to obtain a
transmission profile for each SIU by offline processing SIUs
with respect to buffer and bandwidth constraint. Moreover, a
set of distributed caching and replacement strategies has been
proposed to improve the SIU hit rate. Extensive simulation re-
sults indicate that SSA with the access-aware caching approach
can significantly improve the proxy hit rate, and can lower the
server load by 15%–35% server load at various user arrival
rates. The extensive simulation experiment has demonstrated
that SOP provides a satisfactory solution to exploit end-host
resources for true VoD service.
ACKNOWLEDGMENT
This paper was sponsored in part by “Aim for the Top
University Plan” of Yuan Ze University and Ministry of
Education, Taiwan, R.O.C., and the National Science Council,
Taiwan, R.O.C. under Contract No. NSC-95-2213-E-155-005.
REFERENCES
[1] Y.-H. Chu, S. G. Rao, and H. Zhang, “A case for end system multicast,”
Proc. ACM SIGMETRICS’00, pp. 1–12, 2000.
[2] T.-Y. Chung and Y.-D. Wang, “D2MST: A shared tree construction
algorithm for interactive multimedia applications on overlay networks,”
IEICE Trans. Commun., vol.E88-b, no.10 pp.4023–4029, Oct. 2005.
[3] Y. Cui, B. Li, and K. Nahrstedt, “ostream: Asynchronous streaming
multicast in application-layer overlay networks,” IEEE J. Sel. Areas
Commun., vol.22 no.1, pp.91–106, Jan. 2004.
[4] L. Gao, Z.-L. Zhang, and D. Towsley, “Proxy-assisted techniques for
delivering continuous multimedia streams,” IEEE/ACM Tran. Netw., vol.
11, no. 6, pp.884–894, Dec. 2003.
[5] S. Jin and A. Bestavros, “OSMOSIS: Scalable delivery of real-time
streaming media in ad-hoc overlay networks,” Proc. of IEEE ICDCS’03,
pp. 214–219, 2003.
[6] I. Stoica, et al, “Chord :a scalable peer-to-peer lookup protocol for internet
applications,” IEEE/ACM Tran. Netw., vol. 11, no. 1, pp. 17–32, Feb.
2003.
[7] Z. Xu and Y. Hu, “Exploiting spatial locality to improve peer-to-peer
system performance,” Proc. WIAPP’ 03, pp. 121–125, 2003.
On the other hand, Vivaldi is a decentralized, light weight,
adaptive synthetic coordinate system that computes coordi-
nates which predict Internet latencies with low error. In
Vivaldi, a peer computes its coordinate based on measurements
from itself to a few other peer nodes in its neighborhood
without using a landmark node. However, potentially Vivaldi
suffers from excessive repositioning when peer nodes are
highly dynamic. Thus, a hybrid approach must be developed
to achieve both stability and decentralization in network posi-
tioning.
III. SSIF P2P NETWORK
This section presents the architecture and the novel concepts
and mechanisms adopted in SSIF.
A. SSIF Architecture
SSIF defines three types of nodes, regular peer, cluster
leader and bootstrap server, each with different functionalities.
Bootstrap Server (BS) is a well-known registration server
that supplies yellow page services to all peers in an SSIF
network. BS maintains all active cluster leaders and their
network positions. When a peer joins SSIF, the peer can
download a list of cluster leaders from BS for joining .
Also, BS serves as the only bootstrap landmark node with
a Cartesian coordination location (0, 0).
A peer joins an SSIF network as a regular peer and can be-
come a cluster leader according the clustering algorithm after
a period of time. A regular peer is just a service user of the
SSIF network while a cluster leader acts as an infrastructure
node. A cluster leader offers landmark and resource search
and routing services to their cluster members.
B. Power Based Clustering
A new metric calledpower index is defined for cluster leader
selection in SSIF. The power index represents the capability
and stability of a peer based on its computing power, random
access memory size, access link capacity and living time. It
is defined as below:
PI (Ni) = av x CP (N,)T(N") + Q x BW (Ni)T(Ni) (1)
where
BW (Ni) 100 x 106 56 x 103 (2)
CP (Ni) Y- 100 x 1062.4 x 109- 100 x 106
Z- 128 x 106
+ 2048 x 106 - 128 x 106
T(Ni) = 1 + 30 x 24 x 60
In Eq. 1, a and 3 are weighting constants and are set as one
in this study; BW is the normalized access link bandwidth of
a peer X; CP is the normalized power of a peer including its
processor computing power Y and random memory size Z,
and Ti is a function of a peer's living time t.
As illustrated in Eq. 2, it is assumed that the minimum
and maximum bandwidth of a peer is 56 Kbps and 10OMbps
respectively. The computing power in Eq. 3 is calculated based
on the system clock of processor and is normalized with
respect to the range 100 M Hz to 2.4 GHz. The memory power
is a normalized value based on the size of RAM in the range
of 128MB to 2GB. t is in a unit of minute and increases as a
peer remains up.
The bandwidth, computing power and memory size of a
peer is considered less important than its stability or living
time. Therefore, T is placed as the order over BW and CP,
and has a dominating impact on the value of a peer's power
index. Based on Eq. 1, the bandwidth and computing power of
a peer do not have a major impact on the power index. Thus,
SSIF tends to keep using existing infrastructure nodes as long
as they are stable instead of replacing them with a powerful
new peer soon.
C. Dynamic Hybrid Network Positioning (DHNP)
SSIF utilizes DHNP scheme to provide network positioning
service. In DHNP, a peer uses three landmark nodes as
reference for its coordinate computation. Instead of using a
pre-configured landmark node [7], DHNP employs cluster
leaders as landmark nodes. SSIF uses a Cartesian coordination
system and the bootstrap server located on the origin (0, 0).
A peer always refers to BS and two other cluster leaders
randomly selected as landmark nodes.
In order to achieve more accurate network positioning,
DHNP employs the coordinate tuning approach used in Vivaldi
[5] to fine tuning the coordinate of landmark nodes when
a cluster leader is added in the landmark set. Peers can
correct their coordinates when they join an SSIF network. The
advantage of employing a hybrid approach in DHNP is that the
coordinate fine tuning can always start from a cluster leader's
coordinate instead of a fixed origin as suggested in Vivaldi.
Since a cluster leader already has a close to precise coordinate
when it first joins an SSIF network, the fine tuning can quickly
converge to a precise coordinate that measures distance RTT
(Round-Trip Time) among peers well. Another advantage to
apply Vivaldi among landmark nodes is that a landmark node
is a stable and powerful peer and thus DHNP does not suffer
from the frequent re-positioning problem that may occur in
Vivaldi based on regular peers.
Instead of making every peer tune its position with many
other peers, DHNP applied Vivaldi only among a cluster leader
and its peers. During join process, a peer will keep the distance
RTT to its cluster leader. When a cluster leader adjusts its
coordinate, it also broadcasts the new coordinate to all other
peers in its cluster. Each peer then tunes its coordinate by
Vivaldi. As indicated in [5], network position tuning with close
neighbor nodes can obtain a more precise coordinate. Thus,
the hierarchical position tuning approach employed by DHNP
can both quickly achieve a precise coordinate and reduce
positioning overhead.
ISBN 978-89-5519-131-8 93560 - 1442 - Feb. 12-14, 2007 ICACT2007
time, if the cluster is within a confederation, it will check if
the total number of peer members within the confederation
is less than 0.8U first. If no, the cluster remains unchanged;
otherwise, the cluster leader invokes the cluster merge process.
The cluster leader also helps to speed up the migration of
current peer members by broadcasting a message to request
all peer members to invoke their migration process.
180
160
140
a)
-0
E
a)G. Cluster Confederation
Clusters re-organization frequently will damage the perfor-
mance of resource searching and sharing. Accordingly, SSIF
uses a novel unit called confederation as a searching domain
to improve the performance. When a cluster operates in a
confederation mode, it shares the same searching domain with
other clusters in the same confederation. All searching mes-
sages transferred between clusters in the same confederation
are seen as intra-domain search. In the intra-domain search,
the searching acknowledgement from other cluster in the same
confederation will be sent directly to the peer without going
through a cluster leader. On the contrary, searching messages
transferred between non-confederated clusters are seen as
inter-domain searching and the acknowledgement must go
through cluster leaders. Thus the searching overhead between
non-confederated clusters is higher than confederation clusters.
In SSIF, if two clusters are closer than a threshold 0.5D
in their cluster gravities, BS will inform them to form a
confederation. Thus, the confederation relationship is related
to geographical proximity. On the other hand, when the gravity
of a cluster changes due to peer join and leave, a cluster may
depart from a confederation if its gravity is away from all other
gravities of its confederated clusters more than a threshold
0.8D. With the confederation scheme, the probability of a peer
migrating among the confederated clusters is much larger than
that to a non-confederated cluster when peers join and leave
along the time. Therefore, resource publishing and searching
become much efficient and stable.
IV. SIMULATION AND ANALYSIS
In this section, we carry out several simulations to analyze
the performance of SSIF with a network consisting of 100
backbone routers generated by Inet-3.0 [8]. In the simulation,
300 nodes are added to the topology per hour. The total sim-
ulation time is 200 hours. In the simulation, the up time of a
peer node is generated according to the following distribution:
0.01 x X21,
H(x) x-25 x 100,
70
x 5x 1340 + 100
0 <x < 25
25 < x < 95
95 < x < 100
120
100
80
60
40
20
0
D: Radius of cluster U: Maximun number of peers in a cluster
M: Migration NM: Non-Migration
case 1: D:2000 U:100 M case 7: D:2000 U:150 M case 13: D:2000 U:200 M
case 2: D:2000 U:100 NM case 8: D:2000 U:150 NMcase 14: D:2000 U:200 NM
case 3: D:2500 U:100 M case9: D:2500U:150M case 15: D:2500U:200M
case 4: D:2500 U:100 NM case 10: D:2500 U:150 NMcase 16: D:2500 U:200 NM
case 5: D:3000 U:100 M casell: D:3000U:150M casel7: D:3000U:200M
case 6 D:3000 U:100 NM case 12: D:3000 U:150 NMcase 18: D:3000 U:200 NM
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18
Simulation Case
Fig. 2. Average number of cluster members.
2
1.5
1
0.5
35
30
25
20
15
z
(7) E
where x is a random number between 0 and 100. H (x) is a
staircase function with three steps in which each stair step has
a different slop. The step size is defined according to the life
time distribution of a real P2P system in [9], where x is in
a unit of minute. In simulation results, parameter D denotes
the radius of a cluster (a RTT value in a unit of milliseconds)
and parameter U denotes the maximum number of peers in a
cluster.
10
5
0
D: Radius of cluster U: Maximun number of peers in a cluster
Inside of Domain (ISD)
Outside of Domain (OSD) X
ISD + OSD
case 1: D:2000 U:100 case 4: D:2500 U:100 case 7: D:3000 U:100
case 2: D:2000 U:150 case 5: D:2500 U:150 case 8: D:3000 U:150
casq 3: D:20PO U:20q casq 6: D:25PO U:20q casq 9: D:30PO U:20q
1 2 3 4 5 6 7 8 9
Simulation Case
Fig. 3. Migration rate in different clustering strategy.
1 2 3 4 5 6
Simulation Case
7 8 9
Fig. 4. Node type distribution of 30 peers in different clustering strategy.
ISBN 978-89-5519-131-8 93560
case 1: D:2obo U:100 case 7: D:3obo U100
case 2: D:2000 U:150 case 8: D:3000 U:150
case 3: D:2000 U:200 case 9: D:3000 U:200
case 4: D:2500 U:100
case 5: D:2500 U:150 D: Radius of a cluster
case 6: D:2500 U:200 U: Maximun number of peers in a cluster
Cluster Leader
Candidate Cluster Leader X
Unwaked Infrastructure Node El
X X X ---x
0
LI)
- 1444 - Feb. 12-14, 2007 ICACT2007
(or called infrastructure-nodes (INs) for convenience) to the
SSIF network evenly around the simulated network. Figure 4
shows that most INs can be upgraded to either a cluster leader
or a candidate cluster leader automatically in the experiment
without the intervention of the service provider. As shown in
Fig. 4, when the SSIF network is less stable, i.e. peer changes
cluster more frequent, more INs are promoted to a cluster
leader. The simulation results confirm that SSIF allows a
service provider to actively assist the stability and performance
of an SSIF network by deploying powerful peer nodes in
run time without interrupting or changing configuration of an
operating SSIF network.
The average time required for a deployed IN to become a
cluster leader is shown in Fig. 5. The result shows that with
the power index function used in the study, it takes around
800-1200 minutes on average to make an IN a cluster leader
in various clustering parameters. The reason why it takes so
long in making an IN a cluster leader is because SSIF selects
stable and powerful peers as cluster leaders and thus even
without an IN, SSIF is still stable and robust. Thus, it takes
time for the deployed INs to transfer from a regular peer to a
cluster leader.
D. Performance of Gravity Centered Clustering
The effect of gravity-centered clustering is measured based
on link stretch. The link stretch is defined as:
y x (8)
x
where x is the distance of a peer away from the nearest cluster
center and y is the distance of a peer to its cluster center.
The ratio is used to evaluate how well a peer chooses its
cluster to join. If a peer joins a cluster it should, the link
stretch is zero; otherwise, the link stretch is non-zero and the
larger the ratio is the poorer the clustering scheme is. In the
simulation experiment, a cluster center can either be the cluster
gravity or the cluster leader. Figure 6 shows that peer migration
does improve link stretch substantially, especially when U and
D are small. Figure 7 also reveals that the gravity centered
clustering can improve the stretch performance of the cluster
leader centered approach by 10%-15%. In particular, based
on our observation, the gravity-centered clustering can reduce
cluster overlap. When distant peers migrate to their nearby
clusters, the left peers also change the cluster gravity. As a
result, the gravities among clusters distinguish more clearly.
Therefore, the gravity centered clustering makes clustering
more locality aware.
E. Evaluation ofDHNP
We compare the performance between GNP and DHNP in
this section based on the following error function:
6 (dHlH2, dHl H2) (dNlN. ^S )2
where dH1H2 denotes the measured distance between host
HI and H2, dslH2 denotes the distance on geometric space
S. Note that the measurement is done only between peers
within the same cluster. In the experiment, GNP uses a pre-
configured landmark set while DHNP employs a landmark
set that changes dynamically throughout the experiment. The
simulation results depicted in Fig. 8 reveal that DHNP with
peer migration consistently offers smaller error than that of
GNP, which randomly selected 3 landmark nodes from a
landmark set of size 3, 6 and 12 respectively. In particular,
DHNP performs better even when the GNP based scheme uses
up to 12 landmarks for position measurement.
V. CONCLUSIONS
Stability and scalability are in great demand for many P2P
applications to offer high quality and large scale services.
This study presented a stable and self-organized infrastructure
(SSIF) P2P networking scheme to generate a P2P network to
meet the demand. SSIF allows a service provider to actively
deploy powerful and stable servers to improve the performance
of an SSIF network in run time without interruption and re-
configuration. Meanwhile SSIF places peers in close proximity
in the same cluster to improve the efficiency of resource
sharing. A novel concept called confederation was adopted
to further enhance the performance of resource sharing when
peers in close proximity are many. Some novel schemes such
as dynamic hybrid network positioning (DHNP) and gravity-
centered clustering presented also contribute to reduce the gap
between geographical distance and logical position predicted
among peers. Simulation results confirm that the presented
novel schemes in SSIF do perform as expected. Thus, we
believe that SSIF can serve well for need of current P2P
applications.
ACKNOWLEDGMENT
This paper was sponsored in part by "Aim for the Top
University Plan" of Yuan Ze University and Ministry of
Education, Taiwan, R.O.C., and the National Science Council,
Taiwan, R.O.C. under Contract No. NSC-95-22 13-E- 155-005.
REFERENCES
[1] C. K. Yeo, B. S. Lee, and M. H. Er, "A survey of application level
multicast techniques," Comput. Commun., vol. 27, no. 15, pp. 1547-
1568, Sept. 2004
[2] H. Sunaga et al., "Technical Trends in P2P-Based Communications,"
IEICE Trans. Commun., vol. E87-B, no. 10. pp. 2831-2846, Oct. 2004.
[3] S. Androutsellis-Theotokis and D. Spinellis, "A survey of peer-to-peer
content distribution technologies," ACM Comput. Surv., vol. 36, no. 4,
pp. 335-371, Dec. 2004.
[4] K. Lua, et al., "A survey and comparison of peer-to-peer overlay network
schemes," IEEE Commun. Surveys Tuts., Second Quater, pp. 72-93,
2005
[5] F. Dabek, et al., "Vivaldi: A Decentralized Network Coordinate System."
in Proc. ACM SIGCOMM, vol. 34, no. 4, pp. 15-26, Aug. 2004.
[6] Q. Lv, et al., "Search and Replication in Unstructured Peer-to-Peer
Networks," in Proc. ACM ICSC, pp. 84-95, June 2002.
[7] K. Shin, etc., "Grapes: topology-based hierarchical virtual network for
peer-to-peer lookup services," in Proc. IEEE ICPPW 2002, pp. 159-164,
Aug. 2002.
[8] J. Winick and S. Jamin. "Inet-3.0: Internet Topology Generator," U. of
Michigan Technical Report CSE-TR-456-02.
[9] S. Sen and J. Wang, "Analyzing Peer-to-Peer Traffic Across Large
Networks," IEEE/ACM Trans. Netw., vol. 12, pp. 219-232, Apr. 2004.
ISBN 978-89-5519-131-8 93560 - 1446 - Feb. 12-14, 2007 ICACT2007
