dedicated cores respectively. We also conduct a 
schedulability analysis under the protocol. Moreover, 
we propose algorithms for the task allocation problem 
and validate the efficacy of the proposed framework. 
英文關鍵詞： Dedicated cores, many-core systems, real-time 
scheduling, task synchronization, task allocation 
 
 2
Traditional multi-core techniques are not suitable for many-core systems; thus, this project explores the 
strategy of executing application tasks and their (system) services separately so that blocking among tasks 
can be better managed, and the number of cores required to satisfy timing constraints can be reduced. The 
objective is to develop a methodology (and a scheduling framework) to manage transitive and direct 
blocking and preemption costs among tasks with respect to their schedulability considerations. To this end, 
we dedicate a set of cores, called service cores, to running service tasks that provide the common services 
requested by application tasks running on application cores, and provide interactivity between application 
tasks and service tasks via an RPC-like mechanism. We define three core minimization problems with 
respect to task allocation. First, we propose an optimal algorithm for service core minimization when the 
application core configuration is given. We then propose approximation algorithms for application core 
minimization when the service core configuration is given. Finally, based on the algorithms, we develop a 
heuristic algorithm to minimize the total number of application and service cores without violating timing 
constraints. The results of simulations conducted to evaluate the proposed dedicated-core framework are 
encouraging in terms of the minimum number of cores required and the core utilization. 
 
研究目的： 
 
Every task may have code segments, called critical sections, containing shared resources, such as 
variables or data structures, that can be accessed or manipulated by other tasks. Suppose the shared 
resources are protected by semaphores to ensure the mutual exclusion of execution in critical sections. 
With multitasking support, tasks can be blocked locally and/or remotely on cores because of task 
synchronization. This raises the following technical question: how can priority inversions be managed 
when a number of tasks competing for the computing cycles of multiple cores block each another, directly 
or indirectly, during their execution? In our system model, we consider a set of real-time periodic tasks 
running in a many-core system. Let each task be associated with a time period and a worst-case execution 
time, and let the relative deadline of each task be equal to its period. Suppose each task can lock a set of 
semaphores for at most a specific amount of time during its execution. Two semaphore-locking activities 
overlap if their locking intervals overlap. Note that blocking may occur when tasks attempt to lock the 
same semaphore or compete for the computing cycles of the same core. Blocking among a large number 
of tasks could become extremely complex and cores could easily be left in the idle state if the tasks are not 
allocated to cores appropriately. This in turn would degrade the system performance and offset the benefits 
of utilizing many cores. The situation is likely to become even more serious because the number of cores 
in a system is expected to increase in the coming years. To resolve the problem, we explore how many 
cores are required for a set of real-time tasks to ensure that no deadline is violated. This objective is to 
help system developers to assess the blocking costs in real-time applications. 
 
文獻探討： 
Task scheduling is a critical area in the design of real-time systems, and many excellent fixed and 
dynamic priority-driven scheduling algorithms have been proposed for independent tasks [2, 17, 18, 21]. 
However, tasks that involve resource sharing may give rise to priority inversions and incur unnecessary 
blocking costs as a consequence. To resolve the problem, a number of synchronization protocols have 
 4
1.1 A Dedicated-Core Framework and Problem Definitions 
The framework contains a set of cores, called service cores, dedicated to running service tasks for the 
provision of common services; for example, the protection of codes executed in a critical section is 
modeled as a service task. All application tasks run on the remaining cores, called application cores. An 
example of the dedicated-core framework is illustrated in Figure 1. Before presenting a synchronization 
protocol for application and service tasks, we define tasks problems. 
 
. 
Figure 1: An example of the dedicated-core framework 
 
Based on the dedicated-core framework, we will consider three possible system designs: (1) The 
allocation of application tasks is determined by some design constraints or preference, and the system 
designers need to minimize the number of cores to run service tasks in order to service the critical sections 
of application tasks. (2) Given a set of service tasks allocated to a collection of service cores, we need to 
schedule a set of application tasks on a minimized set of application cores. (3) Given a set of application 
tasks, we need to derive a set of service tasks and minimize the number of cores required to schedule 
application and service tasks. We formally define the three core minimization problems as follows: 
 
Problem Definition 1: The Service Core Minimization Problem 
Instance: Consider a set of real-time periodic application tasks ΠA = {τA1, τA2,…, τAN}, where each 
taskτAi is associated with its period/relative deadline pi, worst-case execution time ci, and a set of 
semaphores Si ={si1 , si2 , ..., sij}, where each sik may be locked byτ
A
i for at most tik time units. Suppose 
that every application task has been pre-allocated to run on a certain core. 
Objective: The objective is to find a set of service tasks ΠS = {τS1, τS2,…, τAK} and their allocation to 
the service cores such that the critical sections guarded by each semaphore sik in S, for all i, are serviced 
by its corresponding service task. No deadline violation is possible, and the number of service cores is 
minimized. 
 
Problem Definition 2: The Application Core Minimization Problem 
Instance: Consider a set of real-time periodic application tasks ΠA = {τA1, τA2,…, τAN}, where each 
 6
lower-priority application task running on the same core. When a higher-priority application task is 
blocked because the corresponding service task is servicing a lower-priority application task, the former is 
said to be remotely blocked by the latter. On the other hand, a lower-priority application task suffers from 
remote preemption if the corresponding service task is servicing a higher-priority application task. Given a 
configuration of application and service tasks distributed over many cores, the schedulability of the tasks 
and their scheduling behavior are explained by the following theorem. 
 
Theorem 1. A set of real-time periodic application tasksΠA = {τA1, τA2,…, τAN} is schedulable if the 
following equation holds for eachτAi, where i=1, 2, ..., N. 
 
 
 
 
2. Task Allocation 
In the following, we utilize the schedulability test derived under the dedicated-core framework to 
develop an optimal prune-and-search algorithm for the service core minimization problem, as well as two 
approximation algorithms for the application core minimization problem. We then utilize the algorithms to 
resolve the core minimization problem with dedicated cores in an iterative fashion. 
 
2.1 Service Core Minimization 
First, we resolve the service core minimization problem for a given application core configuration. 
Because the problem is known to be NP-hard, we try to derive good quality solutions within a reasonable 
amount of time. Although the proposed algorithm is based on the rate-monotonic scheduling algorithm for 
task scheduling (and its schedulability analysis), we should point out that it could go with other scheduling 
algorithms. Before presenting the proposed algorithm, we define some terms. Consider a set of application 
tasks ΨA, in which each task τAi has been allocated to a certain application core. Let ΨS={τS1, τS2, ..., τSK} 
be the set of service tasks for ΨA, where overlapping critical sections are serviced by a single service task. 
A configuration of a service task set is denoted as a set of subsets of service tasks, e.g., { (τS1, τS2), (τS3), 
(τS4, τS5) }, where service tasks in the same subset are allocated to the same service core, and the service 
task set is partitioned exclusively into the subsets of the configuration. A service core is monopolized if it 
only has one service task, such as a core for (τS3); otherwise, it is shared, such as one for (τS1, τS2). 
A k-configuration Ck contains exactly k shared service cores. Given a k-configuration Ck and 
a l-configuration Cl, the binary operator · of the two configurations yields another configuration Cm, where 
m=k + l, such that subsets of the shared cores in Ck or Cl stay in Cm; and every service task that does not 
belong to any subset of a shared core contributes to a single-element subset. If any service task appear in 
more than one subset, the resulting configuration is invalid; otherwise, it is valid. A valid configuration 
is schedulable if all the application tasks are guaranteed to meet their deadlines (cf. Theorem 1). A 
configuration set Ck is complete in terms of configurations with exactly k shared service cores (or 
k-configurations) if the set contains all schedulable k-configurations. Given two configuration sets Ck and 
Cl, the binary operator × of the two sets yields another configuration set Ck ×Cl = { Ck ·Cl | ׊ Ck א Ck and 
 8
 
 
2.3 Core Minimization with Dedicated Cores 
We resolve the core minimization problem with dedicated cores by solving the service core and 
application core minimization problems in an iterative manner. To this end, we propose the core 
minimization next-fit (CMNF) and core minimization first-fit (CMFF) algorithms. The only difference 
between them is the algorithm adopted for application core minimization, i.e., RMNF-CD or RMFF-CD. 
The steps of CMNF and CMFF are as follows. Given an application task set ΨA, a preliminary application 
core configuration is initialized (as described below). Next, the algorithms apply the service core 
minimization algorithm to the initial application core configuration to find an optimal service core 
configuration. Then, an application core minimization algorithm (i.e., RMNF-CD or RMFF-CD) is 
applied to the derived service core configuration to find a better application core configuration. As the 
service core minimization algorithm is optimal, the process repeats until the application core minimization 
algorithm can no longer reduce the number of application cores required to execute the application tasks. 
 
The initial application core configuration can be chosen according to some design constraints or 
preferences. In this paper, we propose to initialize an application core configuration in a load-balancing 
manner such that the utilization of each application core is slightly lower than 41%. The value is chosen 
based on the worst-case achievable utilization √2−1 ≈ 41% for independent tasks in multi-core systems, as 
reported in [23]. Because resource sharing among tasks could impact the schedulability in real-time 
systems, the utilization for our case should be slightly lower than that for independent tasks. Note that if 
the utilization value is set too high, some application tasks could miss their deadlines, so no feasible 
service core configuration would be found. On the other hand, if the utilization value is set too low, 
service tasks could be allocated to a small number of service cores. This could result in a situation where 
the remote preemption/blocking cost of application tasks would be significant, and the local preemption 
cost that application tasks could tolerate would be low. Consequently, further reduction of the number of 
application cores would be impossible, and the computing cycles of the application cores would be 
wasted. 
 
 
 10
 
Figure 2. The impact of the critical-section ratio on the number of cores 
 
Figure 3 shows the impact of the critical-section ratio on the number of cores derived by CMNF and 
CMFF. The total number of cores (i.e., TOTAL-NF and TOTAL-FF) increased as ∆ increased. The 
increase was primarily due to the increase in the number of service cores, as the number of application 
cores only changed slightly. This is because the utilization of service tasks increased with ∆, so more 
service cores were needed. Although the utilization of application tasks decreased with ∆, the potential 
decrease in the number of application cores was offset by the higher remote blocking/preepmtion cost they 
would incur. We observe that, as the critical section ratio increased, CMNF started to outperform CMFF 
when ∆ = 15% in terms of the number of cores used. This occurred because tasks were subject to further 
remote blocking/preemption costs with an increased ∆, and RMFF-CD tended to use fewer application 
cores than RMNF-CD. Consequently, the number of service cores used under CMFF increased more 
quickly than under CMNF to compensate for more competition on the application cores. Moreover, for the 
same reason, CMFF used fewer application cores and more service cores than CMNF. As 5% ≤ ∆ ≤ 17.5%, 
the results show that CMNF used 18% more application cores than CMFF, while CMFF used 45% more 
service cores than CMNF. 
 
 
Figure 4. The impact of the number of application tasks on the average core utilization 
 
 12
結論： 
In this project, we explore the task synchronization and allocation problems in many-core real-time 
systems, and propose a dedicated-core framework to better separate the concerns of block and preemption 
costs from task allocation. A popular RPC-like mechanism is adopted to model the interactivity between 
application and service tasks. We also define three core minimization problems with respect to the 
constraints on core configurations and prove that they are NP-hard. To solve the problems, we propose an 
optimal prune-and-search algorithm for service core minimization, and two approximation algorithms for 
application core minimization. The algorithms are then used to resolve the core minimization problem 
with dedicated cores in a systematic fashion. The results of simulations conducted to evaluate the 
proposed framework show that, with a small number of cores dedicated to system services, a many-core 
system can achieve a reasonable performance, compared to the worst-case achievable utilization for 
independent tasks in multi-core systems reported in [23]. 
 
三、 參考文獻 
[1]. ITRS 2007 Edition - System Drives. Technical report, The International  Technology  Roadmap for Semiconductors, 
2007. 
[2]. N. C. Audsley. Optimal Priority Assignment and Feasibility  Of Static Priority Tasks with Arbitrary Start Times. 
Technical report, University of York, 1991. 
[3]. T. P. Baker. Stack-Based Scheduling of Realtime Processes. Real-Time Systems, 3(1):67–99, 1991. 
[4]. S. Baruah and N. Fisher. The Partitioned Multiprocessor Scheduling of Sporadic Task Systems. In Proc. of IEEE RTSS, 
pages 321–329, 2005. 
[5]. S. Baruah and J. Goossens. Rate-Monotonic Scheduling on Uniform Multiprocessors. IEEE Trans. on Computers, 
52(7):966–970, 2003. 
[6]. A. Block, H. Leontyev, B. B. Brandenburg, and J. H. Anderson. A Flexible Real-Time Locking Protocol for 
Multiprocessors. In Proc. of IEEE RTCSA, pages 47–56, 2007. 
[7]. S. Boyd-Wickizer, H. Chen, R. Chen, Y. Mao, F. Kaashoek, R. Morris, A. Pesterev, L. Stein, M. Wu, Y. Dai, Y. Zhang, 
and Z. Zhang. Corey: An Operating System for Many Cores. In Proc. of USENIX OSDI, pages 43–57, 2008. 
[8]. B. Brandenburg and J. Anderson. Optimality Results for Multiprocessor Real-Time Locking. In Proc. of IEEE RTSS, 
pages 49–60, 2010. 
[9]. M.-I. Chen and K.-J. Lin. Dynamic Priority Ceilings: A Concurrency Control Protocol for Real-Time Systems. 
Real-Time Systems, 2(4):325–346, 1990. 
[10]. Y.-S. Chen, L.-P. Chang, T.-W. Kuo, and A. K. Mok. An anomaly prevention approach for real-time task scheduling. 
Journal of Systems and Software, 82(1):144–154, 2009. 
[11]. Y.-S. Chen, C.-Y. Yang, and T.-W. Kuo. FL-PCP: Frequency Locking for Energy-Efficient Real-Time Task 
Synchronization. In Proc. of IEEE RTCSA, pages 451–458, 2007. 
[12]. E. G. Coffman, M. R. Garey, and D. Johnson. Approximation Algorithms for Bin Packing:  A Survey. Approximation 
Algorithms for NP-hard Problems, pages 46–93, 1997. 
[13]. S. Funk, J. Goossens, and S. Baruah. On-Line Scheduling on Uniform Multiprocessors. In Proc. of IEEE RTSS, pages 
183–192, 2001. 
[14]. N. Guan, M. Stigge, W. Yi, and G. Yu. New Response Time Bounds for Fixed Priority Multiprocessor Scheduling. In 
Proc. of IEEE RTSS, pages 387–397, 2009. 
[15]. D. S. Johnson. Near-optimal Bin Packing Algorithms. PhD thesis, Massachusetts Institute of Technology, 1973. 
[16]. K. Lakshmanan and D. de Niz and R. Rajkumar. Coordinated Task Scheduling, Allocation and Synchronization on 
Multiprocessors. In Proc. of IEEE RTSS, pages 469–478, 2009. 
[17]. J. Y.-T. Leung and J. Whitehead. On the Complexity of Fixed-Priority Scheduling of Periodic, Real-Time Tasks. 
Performance Evaluation, 2:237–250, 1982. 
[18]. C. L. Liu and J. W. Layland. Scheduling Algorithms for Multiprogramming in a Hard Real Time 
Environment. Journal of the ACM, 20(1):46–61, 1973. 
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期： 102年 4月23 日 
一、參加會議經過 
       IEEE International Conference on Computer Communications  (INFOCOM )  係
網路通訊領域頂尖之國際學術會議，也屬中研院資創中心Top Conference列表中
Rank 1之會議，是重要的研究成果發表與交流場合，每年吸引數百位全世界相關領
域研究專家學者與會。今年會議於2013年4月14日至19日在義大利杜林舉行，由該
領域專家學者所組成的審議委員會從1613篇的submission papers中接受了280篇
regular papers，約17%的接受率。今年本人除了論文有幸被該會議接受為regular 
paper，前往發表研究成果。參加該會議有助提昇台灣在此領域的參與度，對本人研
計畫編號 NSC 100－2628－E－001－003－MY2 
計畫名稱 即時眾核心系統之程序同步與配置 
出國人員
姓名 修丕承 
服務機構
及職稱 中研院資創中心 
會議時間 
102 年 4 月 14 日
至 
102 年 4 月 19 日 
會議地點 義大利杜林 
會議名稱 
(中文) 
(英文)  IEEE International Conference on Computer Communications    
       (INFOCOM ) 2013 
發表論文
題目 
(中文) 
(英文)  Extend Your Journey: Introducing Signal Strength into  
      Location-based Applications 
附件五 
 3
考慮使用者感受。 
 
       本人的論文被安排在Main Conference最後一天下午的第一個 session (LL: 
Localization and Location Privacy) 報告。報告後，與會專家學對我們的研究很感興
趣，提出的幾個問題相當有建設性。特別是，當多個Users分享同一個Channel頻寬
時，訊號強度和Data Rate的關係可能會受到Base Station頻寬分配策略的影響。這個
問題在今年3月Prof. Peter Scheuermann (Northwestern University, USA)訪問中研院資
創中心時也有提到，建議本人應該往多個Users分享頻寬的方向繼續這方面的研究。
類似國際學者訪問交流的目的，於頂尖國際會議發表論文最大好處之一就是可以獲
得頂尖研究人員保貴的建設性意見。除了我們的論文，這個session另外有三篇相近
的研究成果報告，共同的特色在於研究議題相當具有新穎性。本人聆聽了此領域的
其他專家學者報告他們最新的研究成果，了解新的研究趨勢，與他們交流對往後我
們在這方面研究的延伸性相當有幫助，同時也讓此領域頂尖的學者知道我們團隊正
在進行的研究。 
 
    這次的與會，遇到了Prof. Jie Li (University of Tsukuba)、逄愛君教授 (台大資
工)、魏宏宇教授  (台大電機)、林靖茹博士 (中研院資創中心)等。除了這幾位舊識
的教授，還認識了Nei Kato (Tohoku University)和Leung Ka Cheong (The University of 
Hong Kong)。本人的研究資歷尚淺，正在積極培養自己的「學術網絡」，創造往後
合作交流的機會，認識他們算是重要的收穫之一。此次與會最大的感觸是，有別於
以往參加此領域其他國際會議(例如IEEE ICC和IEEE GLOBECOM)的經驗，今年台
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
