 2
行政院國家科學委員會專題研究計畫成果報告 
 
計畫編號：NSC 97-2221-E-194-052 
執行期限：97 年 8 月 1 日至 98 年 7 月 31 日 
 
主持人：劉興民 國立中正大學資訊工程學系 damon@computer.org 
共同主持人： 
計畫參與人員：鄭靜怡、陳立婷、劉記顯、蔡俊宏、鍾承軒、楊鈞豪 
 
 
一、中文摘要 
 
在諸多領域例如醫學與塑膠射出成型產
業，處理對象大多是三維的離散數值資
料。面對動輒數十萬甚至百萬的三維資
訊，視覺化是最強大的工具之一。然而針
對三維斷層掃描(computed tomographic)與
核磁共振(magnetic resonance image)的三維
資料的視覺化 (visualization)，體積成像
(volume rendering)技術已逐漸取代早期表
面成像(surface rendering)的方法。 
體積成像技巧最核心的概念是針對三
維 資 料 的 立 體 像 素 (voxel) 作 分 類
(classification)後，沿著視點方向由前到後
(抑或由後至前)，利用色彩的不透明度
(opacity)作混合(blending)的視覺效果。但是
由於此視覺化技巧是基於疊合的方式，導
致不同組織與構造會互相遮蔽。此外，不
同組織構造間也常常不容易找出明顯的邊
界分隔。例如核磁共振的人腦三維體積資
料，其不同組織所代表之函數值分佈範圍
常有重疊，故該類資料不容易以函數數值
與一階導數作簡單的分類。 
最近在分析體積資料構造方面的方法
已走向分析二階導數資訊，最常見的做法
之一是利用海森矩陣(Hessian matrix)。然而
大多數的研究只涵蓋於分析特徵值以對不
同種類的構造作分類，尚未對特徵向量做
更深入的研究。本計畫的目的除將特徵值
分析之分類實做外，也嘗試利用特徵向量
來加強表現立體像素之結構，以期更利於
三維純數資料視覺化與分析。 
 
關鍵詞：體積成像、海森矩陣、特徵值、
特徵向量 
 
Abstract 
In many fields such as medical science 
and plastic injection forming, data usually 
contain 3D discrete scalar information. 
Visualization is one of the most powerful 
tools to explore such millions of 3D data set. 
In use of visualization to present CT 
(computed tomographic) and MRI (magnetic 
resonance image) data, however, traditional 
surface rendering is inappropriate and is 
replaced by volume rendering. 
The main idea of volume rendering is to 
classify voxels of 3D data, blending the 
voxels according to their opacities in a front 
to end (or back to front) order along the view 
direction. This visualization scheme is 
mainly based on blending, thus causing 
occlusion between different tissues and 
structures. Particular boundaries between 
different tissues or structures are often hard 
to be clearly determined. In 3D MRI data set 
of a brain, for example, function value 
distributions among various tissues may 
overlap with each other; therefore such kind 
of data sets are difficult to be classified 
simply using function values and first-order 
derivatives. 
Methods analyzing volume data 
structures tend to investigate second-order 
derivatives, the most popular way is using 
 4
Feature Detection and Derivatives 
Voxels need to be assigned colors and 
opacities before shading. This process is 
usually called classification or segmentation. 
Approaches to classify voxels into several 
categories have been studied for years. The 
classification criteria usually vary 
significantly; they can use voxel values, or 
gradient magnitudes, or local curvatures. 
Typically, voxels are classified using value, 
yielding several segments or tissues. Due to 
the characteristic of volume rendering, 
different segments may occlude with each 
other and cause the resultant image less 
informative. The main problem is that not 
only the object boundary but also the 
homogenous region in a segment needs to be 
displayed. Following we introduce some 
previous researches toward feature detection. 
 
Neighborhood analysis and local structure 
A volume dataset can be considered as a 
three-dimensional signal or function. For a 
given CT or MRI volume data, we assume 
the sampling rate satisfies Nyquist theorem 
requirement so that analysis of derivatives is 
meaningful. Otherwise, the signal cannot be 
reconstructed. Derivatives usually imply the 
relations to neighborhood. Consequently, 
gradient has been widespreadly used for 
analyzing signals and functions. 
Gradient, the first-order derivative, 
contains two information: the direction and 
the magnitude. The direction of gradient can 
be used for lighting voxels and the 
magnitude of gradient is used to detect 
boundaries. David Ebert [12] used the 
direction of gradient for lighting effect and 
used the magnitude of gradient for modifying 
voxel opacity to enhance boundary. He also 
calculated the halo potential among 
neighbors for reinforcing features. 
 
Second-order derivatives and Hessian 
matrix 
Boundaries can be detected using voxels 
with higher gradient magnitudes, but indeed 
gradient only specifies the main growing 
direction and strength. Besides, the 
computation of gradients only considers the 
orthogonal axes of volume data, while 
ignoring neighboring voxels in diagonal 
directions. In recent year the analysis of 
second-order derivatives for volume data has 
been a widely used method for detecting 
structure. Line, plate, and blob, the three 
basic types of structure, are typically treated 
as vessels, boundaries, and nodules, 
respectively, in medical volume data. 
Hessian matrix represents the 
second-order derivative of three-dimensional 
scalar data. The general usage of Hessian 
matrix is to solve the matrix for eigenvalues, 
then apply some discriminators for the 
eigenvalues to analyze local structures. The 
three eigenvalues are usually sorted by their 
values for ordering. The sorting criteria and 
discriminators are different in many 
researches. Yoshinobu Sato [13] [14] sorted 
the eigenvalues preserving sign notation and 
presented the possible structures of different 
eigenvalue orientations. Jiri Hladuvka [15] 
also treated the eigenvalues in the same 
ordering and added Laplacian of Gaussian 
for isotropic edge detection as used in image 
processing. Different from Yoshinobu Sato’s 
method, Jiri Hladuvka used less number of 
discriminators and combined threshold for 
generating sparse data sets. Alejandro F. 
Frangi [16] sorted eigenvalues by their 
absolute values different from the 
aforementioned approaches. He reserved the 
sign notations of eigenvalues for verifying 
bright and dark sides. Our research proposed 
multi-scale analysis to Hessian matrix that 
produced different scales of results and then 
combined with scale selection for generating 
final image.  
 
四、研究方法 
 
In the preprocessing part, we detect 
local structures using voxel derivatives, 
first-order and second-order derivatives, 
gradient and Hessian matrix, respectively. By 
solving Hessian matrix, we gain three 
eigenvalues and their corresponding 
eigenvectors for each voxel. We consider 
both eigenvalues and eigenvectors are 
valuable structure information, however, 
many researches simply ignore the important 
role of eigenvectors. We use eigenvalues to 
determine local structures. Similar to a 
footprint function in splatting method, 
the weight of detailed volume texture can be 
set to a higher value if the detailed image is 
desired and vice versa. In it, arithmetic 
operations for this composition can be 
performed using GPU, but its computational 
cost is high because the operation includes 
weighted sum and interpolation between 
different resolution data. Alternative way to 
implement multi-texture is to use OpenGL 
extensions. This implementation with 
OpenGL extensions needs only to provide 
commands and parameters of texture 
operations, and all computing tasks are left to 
GPU. But its shortcomings are complex 
parameter knowledge and lack of adjustable 
weights in texture composition. 
Aiming at a higher performance and 
capability of providing adjustable weights in 
texture composition, we decided to 
implement the multi-texture rendering using 
shading language. In GPU implementation, 
we obtain texture colors using a simple 
command and allow for adjustable weights in 
texture composition up to user’s choice. This 
approach remedies the complex computation 
and texture re-binding overhead in CPU 
implementation. 
In GPU implementation, we need 1) two 
shader programs, 2) two three-dimensional 
textures, and 3) one weighting parameter for 
texture composition. The two shader 
programs are vertex shader and fragment 
shader, where vertex shader maintains 
coordinate transformations and fragment 
shader handles pixel colors. The two textures, 
tex0 and tex1, and the weighting parameter, 
texCom, are passed to shader program as 
input. We use a build-in function to get a 
texture color at cartain position, colors at 
other texture coordinates will be interpolated 
automatically from vertex shader. For 
example, color of a pixel on a slice can be 
computed using the following expression: 
 
  
);0.1(*),1(3
*),0(3
texComtexCtexDtexture
texComtexCtexDtexturetexColor
−+
=
where the parameter texC is the texture 
coordinate. We use a command to 
accomplish multi-texture mapping in shading 
language and leave alpha blending up to 
OpenGL fixed pipeline. 
 
 
Figure 2: Gradient interpolation. Left: 
pre-normalized gradient texture. Right: 
non-normalized gradient texture. 
 
Since we implement the rendering 
procedure using shading language, next we 
like to add lighting effect to objects. In 
typical texture-based slice rendering, lighting 
effect is hard to be implemented at an 
interactive frame rate. We can perform 
per-voxel shading to the three-dimensional 
texture, but the computational cost increases 
as the resolution of volume data grows. 
Texture re-binding overhead is also 
inevitable when re-computing for the change 
of view and light direction. Since we can 
obtain texture color using a look-up table in 
shader program, we pre-computed all 
gradients and passed that information to 
shader program to construct the look-up table. 
The idea is simple but some issues are 
noteworthy. First, the texture format of 
signed floating point is not allowed in 
OpenGL. Texture initialization needs a 
specification of element type; we use the 
macro GL_RGBA32F_ARB specified in 
OpenGL extensions, and then the gradient 
texture remains signed floating format. 
Second, we need to decide to compute 
gradient value using pre-normalization 
method or post-normalization method 
(Figure 2). In per-pixel shading, pixels are 
usually not sample points in original data but 
are interpolated data. Consequently, texture 
colors and gradient values are also 
interpolated. Gradient value includes 
direction and magnitude, so it is incorrect to 
normalize the gradient texture. We must 
reserve original gradient magnitudes, 
otherwise, it may cause artifacts in shading 
(see Figure3). 
 
 6
 
Figure 5: Texture composition with different 
weights. They are of value (a) 1.0, (b) 0.7, (c) 
0.3, and (d) 0.0 assigned to weight of the 
high resolution texture. 
 
We show in Figure 5 the results of 
texture composition using different 
weighting parameters to MRbrain volume 
dataset. The weighting value of high 
resolution texture from Figure 5 (a) to (d) are 
1.0, 0.7, 0.3, and 0.0, respectively. In other 
words, Figure 5 (a) contains high resolution 
texture only and the folding of brain tissue is 
clear but sort of aliased. When increasing the 
weighting value on low resolution texture as 
shown in Figure 5 (b) to Figure 5 (c), the 
folding gets more and more smooth but 
finally blurred when the image contains low 
resolution texture only in Figure 5 (d). 
 
Spreading kernel for boundary 
enhancement 
Eigenvectors indicate local voxel 
structure information and directions. Inspired 
from works such as reconstruction and 
footprint function, we construct a per-voxel 
spreading kernel for boundary enhancement. 
Every voxel has its own spreading kernel, 
each one with a different boundary 
orientation. The main functionality of a 
spreading kernel is to spread energy (opacity 
value) to homogeneous neighboring region, 
i.e., the surface where the voxel sits on. As 
we discuss in previous chapter, we do not use 
eigenvalues to construct spreading kernels 
since the range of eigenvalues varies a lot. 
Instead, we predefined values and set them to 
be the three standard deviations of Gaussian 
for construction a spreading kernel. In the 
following we show a comparison of imaging 
results between the ones use and not use the 
spreading kernel. Note that textures are 
generated using table look-up of color map, 
therefore, the resulting images with or 
without a spreading kernel only differ in the 
way whether voxels spread out their opacity 
values to neighbors or not. 
 
 
Figure 6: Boundary enhancement using 
spreading kernel. Left: no spreading kernel 
enhanced image. Right: spreading kernel 
enhanced image. 
 
六、計畫成果自評 
 
In this work, we implement a 
three-dimensional texture-based volume 
rendering system. In order to make the 
rendering result illustrative and informative, 
we fully exploit eigenvalues and 
eigenvectors, the second order derivatives, of 
voxels. In rendering algorithm, we 
implement multi-texture rendering and 
per-pixel lighting using shading language. 
Finally, a GUI for selecting color maps helps 
users to render the volume data in their 
various aspects. 
The significant contributions of the 
work are described as below: 
z Surface extraction using first- and 
second-order derivatives.  
z Surface enhancement using 
eigenvectors 
z Alternative and interactive rendering 
environment. 
 
During the twelve months of this project 
period, this project produced one master 
thesis. Moreover, we are preparing a 
manuscript that will be submitted to an 
international journal. 
 
七、總結 
 
 8
 10
Rendering, Proceedings of ACM 
SIGGRAPH Computer Graphics, 
Volume 25, pp. 285-288. 
[11]  Lee Westover, 1990, Footprint 
Evaluation for Volume Rendering, 
Proceedings of ACM SIGGRAPH, pp. 
367-376. 
[12]  David Ebert and Penny Rheingans, 
2000, Volume Illustration 
Non-Photorealistic Rendering of 
Volume Models, Proceedings of the 
Conference on Visualization'00, pp. 
195-202. 
[13]  Yoshinobu Sato, Carl-Fredrik Westin, 
Abhir Bhalerao, Shin Nakajima, 
Nobuyuki Shiraga, Shinichi Tamura, 
and Ron Kikinis, 2000, Tissue 
Classification Based on 3D Local 
Intensity Structures for Volume 
Rendering, IEEE Transactions on 
Visualization and Computer Graphics, 
Volume 2, pp. 160-180. 
[14]  Yoshinobu Sato, Shin Nakajima, 
Nobuyuki Shiraga, Hideki Atsumi, 
Shigeyuki Yoshida, Thomas Koller, 
Guido Gerig, and Ron Kikinis, 1998, 
Three-dimensional Multi-scale Line 
Filter for Segmentation and 
Visualization of Curvilinear Structures 
in Medical Images, Medical Image 
Analysis, Volume 2, pp. 143-168. 
[15]  Jiri Hladuvka, Andreas Konig, and 
Eduard Groller, 2001, Exploiting 
Eigenvalues of the Hessian Matrix for 
Volume Decimation, 9th International 
Conference in Central Europe on 
Computer Graphics and 
Visualisation'01, pp. 124-129. 
[16]  Alejandro F. Frangi, Wiro J. Niessen, 
Koen L. Vincken, and Max A. 
Viergever, 1998, Multiscale Vessel 
Enhancement Filtering, Medical Image 
Computing and Computer-Assisted 
Intervention, pp. 130-133. 
出席國際學術會議心得報告 
                                                             
計畫編號 97-2221-E-194-052 
計畫名稱 
Exploiting eigenvector and analyzing multi-scale voxel structure to 
enhance volume rendering ［研究運用特徵向量以及多比例立體像素結
構分析對體積成像加強視覺效果］ 
出國人員姓名 
服務機關及職稱 
劉興民 
國立中正大學資訊工程學系副教授 
會議時間地點 98 年 3 月 9 日至 3 月 13 日 美國夏威夷州檀香山市 
會議名稱 ACM Symposium on Applied Computing 
發表論文題目 Two-dimensional non-photorealistic rendering drawings on mobile devices 
 
一、參加會議經過 
 
這個研討會是由國際著名的 ACM Special Interest Group on Applied Computing 所
支持。論文的接受率也是相當的競爭，根據大會所公佈的資訊是共收到 1084 份
論文投遞，接受了其中的 315 篇來做口頭發表，和 96 篇來做 poster 報告。這個
大規模的研討會每年提供了相關學者專家在 Engineering，Information Systems，
AI and Agents，Software Development 和 Distributed Systems 領域技術上有一個國
際性交流的機會。 
 
此次會議舉行的地點在風光明媚的夏威夷州檀香山市。議程期間大會安排了許
多場次的 keynote speech 和 tutorial，我也都儘可能的去聆聽學習。我的海報展覽
被安排在第二天的下午。因為行程安排上的原因，只能在前一天的早上到達，
因此稍為有時差上的困擾。也因為行程緊湊，顧不得旅途疲備的情況下，在第
二天就做了海報的報告。之後在旅館略為休息後，再為次日培養充沛的體力。 
 
 
在大會結束的前一天晚上，舉行了一個盛大的晚宴餐會，地點是在一個民俗的
文化村裏面，讓所有參加的人享受了一頓美食，並有賓至如歸的感覺。此宴會
除了讓大家稍為放輕鬆一下，同時也頒發了最佳學生論文獎。在那個場合中認
Two-Dimensional Non-Photorealistic Drawings on Mobile Devices 
Damon Shing-Min Liu 
National Chung Cheng University 
168 University Rd. Min-Hsiung, 
Chia-Yi County, 621 Taiwan 
+886 5 2720411 ext.33118 
damon@cs.ccu.edu.tw 
Chi-Hsien Liu 
National Chung Cheng University 
168 University Rd. Min-Hsiung, 
Chia-Yi County, 621 Taiwan 
+886 5 2720411 ext.23139 
lch96m@cs.ccu.edu.tw 
Ching-I Cheng 
National Chung Cheng University 
168 University Rd. Min-Hsiung, 
Chia-Yi County, 621 Taiwan 
+886 5 2720411 ext.23139 
chengcy@cs.ccu.edu.tw 
 
 
ABSTRACT 
This paper presents four simple and fast methods for 2D non-
photorealistic drawings on mobile phones. Our system exploits 
several image processing techniques, to create the four painterly 
drawing styles, namely, Daub, Printmaking, Line drawing and 
Embossing, which can be applied to 2D images. All computations 
are performed on a mobile phone. Taking into careful 
consideration the limited hardware support of mobile phones and 
its limited capabilities, we keep the painterly drawing methods of 
low complexity. To reduce the computations on the mobile phone, 
we consider only important features that best represent each styles, 
and select the corresponding image processing techniques that can 
achieve the desired effect of the drawing style. These methods are 
not complex for a mobile device, to avoid the high computational 
costs of similar techniques for non-photorealistic rendering 
implemented on common desktop computers.  
Categories and Subject Descriptors 
I.4.9 [Computing Methodologies]: Image Processing and 
Computer Vision – application.  
General Terms 
Algorithms, Performance, Design. 
Keywords 
Visualization on handheld devices, mobile graphics, image 
processing, stylish painting, interaction with mobile devices. 
 
1. INTRODUCTION 
Mobile phones have become ubiquitous, over the past one or two 
decades, many with a range of capabilities like mp3 player, video 
calling and GPS. Users now regard mobiles as more than just a 
telephone, these additional capabilities and mobile quality 
become increasingly important. For example, the black and white 
display mobile phones of the earlier decade are now taken over 
with phones capable of color display [1]. Almost all mobile 
phones now have integrated cameras, making photo taking an 
important feature, and the resolution and quality of photos an 
important criterion. Yet mobile camera development nowadays 
mainly focuses on improving photo resolution and image quality, 
and may use various color enhancement techniques to improve 
the clarity, quality, and colors of the photographs taken. However, 
the majority of mobile camera phones have yet to take heed of 
non-photorealistic drawing, and the interest users have for it. 
Some camera phones only regard this feature as an appendant 
service, and some offer only simple displays such as monochrome, 
or some even do not offer such features. From many camera 
phones in the market today, we can see that most non-
photorealistic drawing implemented often results in severe 
blurring of the photographs, making it lose its appeal to users, and 
then having the designers think it unpopular, and not pay much 
attention to it. 
In this paper, we present four non-photorealistic drawing 
techniques for 2D images on a mobile phone. The four non-
photorealistic styles developed are Daub, Printmaking, Line 
drawing and Embossing. We use several image processing 
methods in our system. Through combining these methods, we 
can render the image in a completely different, non-photorealistic 
style. 
However, mobile phones capabilities are restrained by the fact 
that resources on such devices are extremely scarce, as it only has 
a very small amount of memory, and has limited hardware 
support for computation. Image processing requires large amounts 
of computation on image pixels numerous times. On mobile 
devices, in particular, this would be excessively time-consuming. 
We give a table listing of the time needed for each non-
photorealistic drawing style in Section 4. Our methods employed 
are all suitable for being used on mobile devices or smartphones, 
producing great results in both speed and non-photorealistic effect. 
We implemented our system on a simulator - mobile 5.0 pocket 
pc or smartphone and a real mobile device - MIO A700 running 
Windows Mobile 5.0 with an Intel Xscale 520 MHz with 128MB 
of main memory. 
The rest of this paper is structured as follows. Section 2 discusses 
related work to our non-photorealistic technique. In Section 3, a 
detailed approach and workflow to simulate the four non-
photorealistic styles, and the underlying image processing 
techniques used are given. The results generated and computation 
time taken are shown in Section 4. Section 5 concludes the paper 
and discusses the directions we would like to continue with. 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
SAC’09, March 8-12, 2009, Honolulu, Hawaii, U.S.A. 
Copyright 2009 ACM 978-1-60558-166-8/09/03…$5.00. 
 
are incorporated into image 3, resulting in the image having black 
color in areas of high luminance, and bold dark lines as borders in 
areas of low luminance. 
if (Value of Gray Scale > 70) 
    temp = R Value of image3 - Value of Gray Scale; 
    R Value of image3 = (byte)(temp > 0 ? temp : 0); 
    temp = G Value of image3 - Value of Gray Scale; 
    G Value of image3 = (byte)(temp > 0 ? temp : 0);  
    temp = B Value of image3 - Value of Gray Scale; 
    B Value of image3 = (byte)(temp > 0 ? temp : 0); 
3.2 Printmaking Algorithm 
 
As edges are a major characteristic of non-photorealistic drawing 
in printmaking style, simulation begins with an edge detection 
using Sobel operators (3x3 filters) on the input image. We convert 
the color model from RGB to CMY by computing the Invert 
function of the image, where Invert(R, G, B) = (255-R, 255-G, 
255-B), to reflect the fact that this algorithm has an effect which 
is presented only on print. Then the image is converted to of 
grayscale values using the luminance value of 
R*0.299+G*0.587+B*0.114, simulating the black and white 
overall look in actual printmaking. Averaging filters are used to 
smooth the grayscale image, preventing the borders from having 
unwanted discontinuities due to edge enhancement. Sharpening is 
done next using a Sharpen filter [9] to add more emphases on the 
smaller aspects in the image, and enhance edges that are blurred. 
We keep a copy of the image processed so far, naming it image 4. 
(a)                                              (b) 
(c)                                              (d) 
Figure 2. Results of Daubing. (a), (b) Orignal image.  
(c), (d) Their corresponding Daubing image. 
The Sharpen filter is: 
 
To simulate the non-uniform thickness of border lines for 
printmaking, processing on the image continues with a smoothing 
operation, along with an edge detection using Sobel operators. 
The resulting image should be one without many edges of minor 
details. Then we combine this resulting image with image 4, 
obtaining the final printmaking simulation image that draws 
differently with respect to different edge thickness. The steps are 
illustrated in Figure 3. 
3.3 Line Drawing Algorithm 
 
As it can be seen in the flow chart of Figure 4, the first three 
image processing techniques used in the non-photorealistic line 
drawing is exactly the same as the first three steps of printmaking. 
Line drawing and printmaking rendering styles differ in that line 
drawing aims to keep every small aspect of information existing 
in the image. Therefore, it has no need for smoothing, which 
otherwise would rid the image of minor details. The image is 
sharpened as in the last step, enhancing all small segments of 
edges in the image. 
3.4 Embossing Algorithm 
Figure 4. Block diagram of Line drawing algorithm. 
Figure 5. Block diagram of Embossing algorithm.  
Figure 3. Block diagram of Printmarking algorithm. To simulate the embossing style, edge information is needed. We 
perform edge detection first, using one of Prewitt’s filters for 
convolution on the image.  
Prewitt’s filter used is: 
In the near future, we will continue on implementing other 
traditional artistic styles for mobile devices such as watercolor, 
oil-painting, or modern styles like those in comic strips. As it can 
be seen in Table 1, the printmaking effect takes more time to 
render compared to other non-photorealistic styles, we shall try to 
cut down the time cost needed by altering the method to reduce 
the amount of computations, without sacrificing the quality of 
rendering result. 
To demonstrate the speed performance of our approaches, we 
have experimented the four non-photorealistic drawing styles on a 
mobile device mio A700 to test the methods. We can see that the 
metallic look of the embossing effect in Figure 6(e) and 7(e) 
requires only a very short time to achieve, as it can be seen in 
table below. 
Table 1. Timing results in seconds. 
 
Peppers Girl 
Daubing algorithm 1.6 2.3 
Printmaking algorithm 4.4 5.7 
Line drawing algorithm 2.3 2.9 
Embossing algorithm 0.5 0.6 
Image 
Algorithm 
6. REFERENCES 
[1] Lee, J., Byun, S.-K., Lee, J.-D., Kim, T.-Y., Evaluation of 
technological innovation in the cellular phone display, In 
Proceedings of Portland International Conference on 
Management of Engineering and Technology, Portland 
International Conference, 2003, 140- 149. 
[2] Canny, J., Computational Approach to Edge Detection, IEEE 
Trans Pattern Analysis and Machine Intelligence,  1986, 
679-698. 5. CONCLUSION 
[3] Prewitt, J.M.S., Object Enhancement and Extraction, Picture 
Processing & Psychopictorics, 1970, 75-149. 
This paper presents four non-photorealistic drawing techniques 
for 2D images, Daubing, Printmaking, Line drawing and 
Embossing. The methods used to simulate each style are kept 
simple for speedy computation on mobile devices and will not 
cause the images to blur. The methods are mostly based on a 
series of non-complex image processing techniques, to suppress 
the amount of calculation needed. We can see from Table 1 that 
the average time needed to render an image in a non-
photorealistic style using our methods is only less than 3 seconds. 
[4] Sobel, I., Camera Models and Machine Perception, Stanford 
AI Memo, 121, 1970. 
[5] Doug, D. C. and Anthony, S., Stylization and Abstraction of 
Photographs, In ACM Computer Graphics (Proceeding of 
SIGGRAPH 2002), 2002, 769–776. 
[6] Son, M., Kang, H., Lee, Y., and Lee, S., Abstract Line 
Drawings from 2D Images, In Proceeding of Pacific 
Graphics, IEEE Press, Maui, Hawaii, 2007. All non-photorealistic techniques presented here require an edge 
enhancing look, so the edges obtained from the edge detection 
algorithms cannot contain too much redundant small details, and 
more importantly, must not be too fragmented or discontinuous. 
We take care of these problems by smoothing through an 
averaging filter of low computation cost. 
[7] Kang, H., Lee, S. and Chui, C., Coherent Line Drawing, In 
Proceeding of ACM Symposium on Non-photorealistic 
Animation and Rendering, San Diego, CA, 2007, 43-50. 
[8] Sourin A., Symposium on Interactive 3D Graphics, In 
Proceedings of the 2001 symposium on Interactive 3D 
graphics, SIGGRAPH ACM, 2001, 77-84. 
Our methods for rendering 2D images in a non-photorealistic 
style on mobile devices is not only considered fast for such 
limited hardware support but also generates excellent results that 
do not blur the image. 
[9] Gonzalez, R. C., and Woods, R. E., Digital Image 
Processing, Prentice Hall; 2nd edition, 2002. 
 
