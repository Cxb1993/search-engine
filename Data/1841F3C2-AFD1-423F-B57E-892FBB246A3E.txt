Abstract 
This report develops techniques and methodologies for system design, motion 
control, human-robot interaction, navigation and chip-based implementation of a 
human-like symbiotic robot with a human-robot interaction module, dual dexterous 
arms and a two-wheeled self-balancing mobile platform.  The research work is 
divided into three parts. The first part is devoted to designing posture controller of 
two-wheeled self-balancing mobile platform, and developing an experimental 
human-robot interactive head module using small-size servo motors and SoPC 
(system on programmable chip) technology.  The second part not only develops 
anthropomorphic dual arms with cooperative control, in order to circumvent the 
dangerous abnormal operations and protect the surrounding people from dangerous 
collision, but also pays attention to motion planning and coordination control of the 
dual arms, and The third part is to achieve integrated control for the dual arms and the 
two-wheeled self-balancing mobile platform, and to construct path planning and a 
human-aware navigation system. All the proposed technologies and designed modules 
must be realistically implemented and verified on the developed experimental robot.  
Through experimental results, all the proposed technologies are proven useful and 
effective not only in establishing and improving actual functions, sensing, recognition, 
autonomy, environment adaptation, and agile motions of the current human symbiotic 
service robots, but also in extending the applications of the proposed technologies to 
perform more complicated tasks and missions in the fields of edutainment, servicing, 
hobbies and recreation and even marketing. 
Keywords: human symbiotic robot, two-wheeled self-balancing control, tactile 
sensing, smell sensing, FM-modulated ranging sensor, control of dual arms,  
two-arm configuration, and system integration. 
 ii
Contents 
 
ENGLISH ABSTRACT...............................................................................................ii 
CHINESE ABSTRACT............................................................................................. iii 
CONTENTS.................................................................................................................iv 
LIST OF FIGURES ....................................................................................................iv 
LIST OF TABLES.......................................................................................................iv 
CHAPTER 1  INTRODUCTION.............................................................................1 
1.1  INTRODUCTION ...................................................................................................1 
1.2  LITERATURE REVIEW ..........................................................................................6 
1.3  MOTIVATION AND OBJECTIVE ...........................................................................12 
1.4  ORGANIZATION OF THE REPORT ........................................................................13 
CHAPTER 2  INTELLIGENT ADAPTIVE DYNAMIC SIMULTANEOUS 
TRACKING AND STABILIZATION USING FUZZY WAVELET NETWORKS
......................................................................................................................................14 
2.1  INTRODUCTION .................................................................................................14 
2.2  DYNAMIC MODEL AND PROBLEM STATEMENT..................................................16 
2.3  CONTROLLER SYNTHESIS..................................................................................20 
2.4  INTELLIGNET MOTION CONTROL USING FUZZY WAVELET NETWORKS.............26 
2.5  SIMULATIONS AND DISCUSSION ........................................................................32 
2.5  CHAPTER CONCLUSIONS...................................................................................38 
CHAPTER 3  FACE DETECTION, IDENTIFICATION AND TRACKING 
USING SUPPORT VECTOR MACHINES AND FUZZY KALMAN FILTER..40 
 iv
CHAPTER 6  HUMAN-AWARE AUTONOMOUS NAVIGATION...............108 
6.1  INTRODUCTION ...............................................................................................108 
6.2  OPERATIONAL SCENARIO ................................................................................109 
6.3  SKIN COLOR DETECTION ................................................................................ 111 
6.4  OBSTACLE AVOIDANCE ...................................................................................112 
6.4  HUMAN-AWARE NAVIGATION GENERATION....................................................113 
6.5  OVERALL EXPERIMENTS AND DISCUSSION .....................................................117 
6.6  CHAPTER CONCLUSIONS.................................................................................121 
CHAPTER 7  CONCLUSIONS AND FUTURE WORK...................................122 
7.1  CONCLUSIONS.................................................................................................122 
7.2  FUTURE WORK ...............................................................................................124 
REFERENCES.........................................................................................................125 
附錄一 計畫成果自評                                               137
 vi
function of the input. (b) Membership function of the output ...............................42 
ure 3.4. The flow chart of the face identification ...................................................44 Fig
Fig
Figure 3.5. The results of the Haar wavelet transform. (a) The original image; (b) the 
result of the Haar wavelet transform; (c) the position relationship picture of 
sub-image...............................................................................................................45 
ure 3.6. Training Face images and average face ψ . ..............................................46 
Figure 3.7. (a)Membership function for the mean of the innovations. (b) Membership 
function for the second-order moment of the innovations. (c) Membership function 
for the slew rate of the second-order moment 2rσ .(d) The membership function of the 
weighting factorα . ................................................................................................51 
ure 3.8. Pictures o  the experimental human-robot interactive system. .................53  
 
Fig f
Figure 3.9.Comparison of the true position, KF and FKF position estimates .............54 
Figure 3.10. Performance comparison of the KF and FKF estimated states. ..............55  
 
Fig
 
Figure 3.11.Comparison of the true position, KF and FKF position estimates ...........56  
Figure 3.12. Comparison of the KF and FKF estimated states....................................56  
Figure 3.13. Experimental results of the fuzzy skin color adjuster. (a) Normal light 
intensity.  (b) Stronger light intensity. (c) Darker light intensity ................................57  
Figure 3.14. Experimental pictures of separating skin color. (a) Original;(b)processed
................................................................................................................................58 
ure 3.15.The result by using Sobel edge process....................................................58 
Figure 3.16. Experimental results of the detection process using the oval model.......58 
Figure 3.17. Feature extraction of a human face. ........................................................58  
Figure 3.18. Database of human faces .........................................................................59  
Figure 3.19. Haar faces ................................................................................................59  
Figure 3.20. Experimental pictures of face identification............................................60  
Figure 3.21. Experimental pictures of the face tracking..............................................62  
 viii
Figure 4
 
Figure 4.25 (a) Grinder ; (b) Post
f
 
Figure 4.29 Angular pos
 
Figure 4.33 (a) ~ (f): 
Figure 5.10. Illustration of Step 3 ..............................................................................101  
.23. Transformation of coordinate and Grabbing diagram .............................85  
Figure 4.24 (a) Relationship of coordinates between two hands (b) Cup parameters. 
(c),(d) Kinds of rotation for pouring ......................................................................87 
ure of grinding ; (c) Trajectory of rotating point.....87  
Figure 4.26. Simulation of the 7-DOF anthropomorphous arm...................................89  
Figure 4.27. Double circular motions ..........................................................................89  
Figure 4.28 Angular position errors o  all the seven joints in the left arm performing the 
circular movement .................................................................................................89 
ition errors of all the seven joints in the right arm performing 
the circular movement..................................................................................................90 
Figure 4. 尖30 Written word: ” ”; (a)origin image,(b)thinned image............................90  
Figure 4.31 Hand- 尖writing process of the first Chinese word: “ ” .............................91  
Figure 4.33 Execution of the pick-place task using the two arms with ball passing 
cooperation.............................................................................................................91  
 pictures of the coffee making process .....................................92  
Figure 5.1 A grid-based environment and planned path representation.......................95  
Figure 5.2. Flow chart of the block building for MPSO..............................................96  
Figure 5.3 Eight situations of moving directions.........................................................97  
Figure 5.4. Relationship between the moving direction and the shielding..................97  
Figure 5.5. Illustration of shielding point determination. (a) Case 1; (b)Case II.........98  
Figure 5.5. Illustration of shielding point determination. (a) Case 1; (b)Case II.........98  
Figure 5.6. Flow chart of the dynamic map shielding .................................................99  
Figure 5.7.  Illustration of searching point movement................................................99  
Figure 5.8. Illustration of Step 1 ................................................................................100  
Figure 5.9. Illustration of Step 2 ................................................................................100  
 x
Figure 6.14. Experimental data of the human-aware navigation to pass through two 
persons .................................................................................................................120  
 
 xii
  
Chapter 1  
Introduction 
 
1.1  Introduction 
Over past and present decades, much research about mobile robots has been gradually paid to 
non-industry or service applications. To move unboundedly throughout environments and to 
achieve assigned tasks, mobile robots must be equipped with appropriate locomotion mechanisms. 
In general, there are several types of locomotion mechanisms, such as legged, wheeled, snaking, 
tracked, swimming, flying and etc. Among these locomotion mechanisms, the wheeled locomotion 
mechanism has been shown as the most efficient one in terms of energy consumption. Because of 
this energy-saving property, wheeled mobile robots have gained wide applications in the fields of 
servicing, manufacturing, education, entertainment, rescue, security, military and even some 
special-purpose services, and etc.  
Among several wheeled mobile robots, self-balancing two-wheeled mobile robots (for short, 
SBTWMRs), or called wheeled inverted pendulums, have been widely investigated in both 
academia and industry. Self-balancing two-wheeled mobile platforms have been successfully 
applied to construct several autonomous service robots [1-10]. Many researchers [3]-[10] have 
shown that the two-wheeled self-balancing platforms have gained many applications, including 
personal transportation vehicles, soccer games and etc. With the advent of modern control 
technology, such platforms with sophisticated control functions and safety features can be cost 
down so that they have highly potential to satisfy stringent requirements of various autonomous 
service robots with various linear speeds, and even personal transportation vehicles. Below are 
illustrations of five state-of-the-art examples.  
In practical applications, EMIEW has been regarded as one of the most successful SBTWMRs 
developed for working helpers. Hosoda et al. [2] detailed the basic design of this kind of 
 1
 
  
Figure 1.5. Sega-Hasbro’s two-wheeled music robot [12]. 
Figure 1.3 displays a kind of two-wheeled self-balancing electric vehicle, named EN-V, which 
was made by GENERAL MOTORS(GM) CORP. This kind of vehicle was presented in Universal 
Exposition/World's Fair in Shanghai City in 2010. This EN-V is constructed using carbon fiber 
material such that its total weight is just about 500 Kg, and its length is about 1.5m. The EN-V is 
equipped with electric motors and lithium group, supporting a maximum speed of 40 Km/Hour. 
Thus, this small car is sufficient to cover the needs of the city’s commuter [13]. 
 
 
Figure 1.6. Two self-balancing electric vehicles “GM EN-V”[13]. 
    
 3
 
 widely addressed by many researchers, little attention has been devoted to the high-performance 
simultaneous trajectory tracking and stabilization problem. The solution of this problem will result 
in a single controller to accomplish out trajectory tracking and regulation in one control framework, 
thereby giving rise to a simple implementation scheme. Therefore, this report is aimed to address 
simultaneous tracking and stabilization problems for two types of nonholonomic wheeled mobile 
robots. 
On the other hand, the development of a variety of humanoid robots has been very popular in 
many countries in recent years. The purpose of designing and making a humanoid robot is to help 
people in operations, or even to substitute for people in some dangerous jobs, such as clearing 
underwater tunnel, treating nuclear waste, firefighting, and etc. To mimic functions of human arms, 
design of anthropomorphous robot arms have become a very challenging issue in the field of 
robotic research, because the kind of robot can help us do many things in conjugation with its arms. 
However in the transportation of heavy and big objects, or in the assembly of components, task 
execution efficiency can be improved by using two arms. Using dual arms to perform difficult task 
execution can provide versatile functions, but it still encounters big technology challenge. Indeed, 
such a robot with two arms would significantly enhance dexterous capabilities for many kinds of 
tasks to be executed. Hence, several methodologies and technologies for task execution of the 
robots with two arms have been developed in both academia and industry, and some of them have 
been shown in a finite number of applications. To sum up, the robot with two arms is playing an 
important and active role in our present and future life. 
Much research for face detection, face tracking, face identifications and interactive facial 
expressions has been widely studied over past decade in the field of mobile service robots, 
especially for human-robot interaction. Human-robot interaction has ready gained widespread 
applications, such as home-caring services, medical robots, military tasks, entertainment, 
manufacturing servicing, security, and so on, and they are now becoming a part of our life. 
Recently, much research takes interest in service robots which interacts with people, such as 
 5
 
 center of gravity. Grasser et al. [17] presented an unmanned mobile inverted pendulum, and Pathak 
et al. [18] studied the dynamic equations of the self-balancing two-wheeled robot by partial 
feedback linearization. Ha et al. [19] presented the trajectory tracking system for navigation of the 
inverse pendulum type self-contained mobile robot; however, this method was limited to simple 
straight line motion and simple turning. For both trajectory tracking and stabilization problems, 
much effort has been spent on nonholonomic mobile robots in [19, 20], but little attention has been 
paid to address the same problems for self-balancing two-wheeled robots. Furthermore, no attempt 
has been paid to design a simultaneous tracking and stabilization controller for self-balancing 
two-wheeled robots with frictions and uncertainties caused by different payloads and terrain [14, 15, 
18, 20].  
From controller design of view, the control of the self-balancing two-wheeled robots can be 
thought of as an under-actuated control problem, which has been investigated by several researchers 
[31-51]. In particular, Lo and Kuo [31] provided a decoupled sliding-mode control to stabilize a 
nonlinear system with four state variables, Lin and Mon [32] offered a hierarchical decoupling 
sliding-mode control to regulate a more general class of under-actuated control systems, and Wang 
et al. [33] presented two systematic sliding-mode design methods for a class of under-actuated 
mechanical systems. However, the approaches [31-33] have not been applied to the self-balancing 
two-wheeled robots yet! 
Fuzzy wavelet networks (FWN) have been proved to excellently approximate time-varying 
nonlinear functions or nonlinear dynamics [34]. This property can be easily applied to controller 
design. For example, Lin [35] brilliantly used FWN to on-line learn a nonsingular terminal sliding 
mode controllers for robot manipulators, thus accomplishing out excellent trajectory tracking 
performance. 
 
1.2.2 Related Work on Face detection, Identification and Tracking 
 Much research for face detection, face tracking, face identifications and interactive facial 
 7
 
 formulations embody the structural minimization principle. SVMs have been gained wide 
acceptance due to their high generalization ability for a wide range of applications and excellent 
performance than other traditional learning machines. In particular, SVMs have been applied to 
many classification or recognition fields, such as isolated handwritten recognition, object 
recognition, speech recognition, spatial data analysis, and so on. Moreover, (SVMs) [45-47] have 
been successfully applied to achieve face detection and face recognition.  
Despite of the Kalman filter having been widely applied to face tracking algorithm for linear 
state estimation problems, they are subjected to the filter divergence problems due to noise 
variations or modeling errors. In order to tackle with these week points, the exponential data 
weighting [48] was used for tuning both process and measurement noise covariance matrices of the 
Kalman filter. Abdelnour et al. applied the fuzzy logic to on-line detection and correction of Kalman 
filter apparent and true divergence. They presented a fuzzy logic supervisor with three inputs, two 
outputs and 24 fuzzy rules. Several researchers in [49-54] have employed fuzzy logic approaches to 
adapt the Kalman filter for various sensor fusion applications. For examples, Sasiadek and Wang 
[49-50] presented fuzzy Kalman filtering schemes to obtain more accurate estimates for 
autonomous robot vehicle and INS/GPS system, and Kobayashi et al [51] developed a fuzzy logic 
Kalman filter to get more reliable and accurate position estimate by fusing the readings from a 
differential global positioning system, a velocity sensor and an odometer. However, the fuzzy 
Kalman filter together has not applied to the face tracking problem. Font-Llagunes and Batlle [52] 
present extended Kalman filter (EKF) with a state-vector composed of the external angular 
measurement for robot odometric information. Armesto et al. [53] present an in-depth analysis and 
performance comparison of the extended Kalman filter, the unscented Kalman filter and three 
particular filters. Kendoual et al. [54] provide a real time 3D vision algorithm for estimation optic 
flow, aircraft self-motion and depth map, using a low-resolution onboard camera and low-cost 
inertial measurement unit. 
 
 9
 
 human hands are nearly straight lines and their speeds are of bell-shape when humans do 
point-to-point hand motion in a front horizontal plane at the height of shoulder. Flash and Hogan 
[69] introduced a cost function defined as an integral of square derivative of hand’s speeds; they 
showed that the optimal trajectory, which gives the minimum of the cost function during a specified 
duration, was similar to human behaviors. Uno and Kawato [70] proposed a cost function, which is 
an integral of sum of square derivatives of joints’ torques, showing that the obtained optimal 
trajectory somehow achieves realization of human-like motions.  
 
1.2.4 Related Work on Global and Local Path Planning  
Self-balancing two-wheeled mobile platforms have been successfully applied to construct 
several autonomous service robots [1-11]. Many researchers [3]-[11] have shown that the 
two-wheeled self-balancing platforms have gained many applications, including personal 
transportation vehicles, soccer games and etc. In order to successfully make the self-balancing 
two-wheeled mobile robots effective to navigate around their working environments, the robots 
must be equipped with the abilities of global and local path planning.  The goal of the global path 
planning is to find the globally optimal path according to a performance index specified by 
designers, based on a given global environmental map or information. This kind of the global path 
planning problem has been extensively explored by several researchers. These researchers have 
presented many global path planning methods, such as visibility graph, Voronoi diagram, cell 
decomposition (connectivity graph), path/graph search strategies (A* algorithm, greedy algorithm, 
depth-first search, breadth-first search, wave-front expansion ), and potential field. In addition, soft 
computing approaches, such as fuzzy logic, neural networks, evolutionary algorithms and biological 
optimization methods, have been widely used to solve the global path planning problems.  
Among many biological optimization methods to solve the global path planning optimization 
problems, GA has been recognized as a heuristic and powerful robust optimization technique [71]. 
 11
 
  
 
13
control modules for simultaneous tracking and stabilization, cooperative dual-arm control, 
human-robot interaction, and human-aware navigation of a  human-like symbiotic robot with a 
human-robot interaction module, dual dexterous arms and a two-wheeled self-balancing mobile 
platform.  The performance and merit of the proposed method and systems are verified by 
conducting several experiments on an experimental human-like symbiotic robot. 
 
1.4  Organization of the Report 
The remaining chapters of the report are organized as follows. Chapter 2 describes simultaneous 
tracking and stabilization of self-balancing two-wheeled mobile platforms. Chapter 3 is devoted to 
developing face detection, identification and tracking for human-robot interaction. Chapter 4 
elaborates the system design, motion control, trajectory planning, and cooperation control of 
anthropomorphic cooperative dual arms. In Chapter 5, global and local path planning using particle 
swarm optimization are presented, and, in Chapter 6, a controller for human-aware navigation is 
reported in details. Finally, Chapter 7 summarizes the report. 
Table 2.1. List of the parameters and variables 
Symbol and unit Parameter and variable name 
xxI , yyI , zzI  
Moment of inertia of the pendulum body with respect to the x, y, z 
axis, respectively.  
rv [m/sec] Reference linear velocity 
rω [m/sec] Reference angular velocity 
R [m] Radius of the wheels 
,x zc c  
The center of mass of the pendulum body is at coordinate bG
( ,0, )b x zOG c c= in β  
τ  Input torque vector for right and left motors 
rτ  Input torque applied to the right motor 
lτ  Input torque applied to the left motor 
waI , wdI  [ 2Kg m ] 
Moment of inertia of a wheel about its axis and about a diameter 
respectively 
rφ , lφ [rad] angles of  the right and left wheels 
θ [rad] Yaw angle  
bM [Kg] Mass of the pendulum body 
wM [Kg] Mass of the each wheel 
α [rad] Tilt angle of the self-balancing two-wheeled robot 
b [m] Half of the distance between both driving wheels 
( , )x y [m/sec] Position of the self-balancing two-wheeled robot  
,v ω  linear and angular velocities of the self-balancing two-wheeled 
robot 
 
 15
      6 3
  0            0            0           1            0            0
1 1cos( )   sin( )       0            0                    
  0            0            1            0                   -
R R
b
R
θ θ× =S(q)
T
b
R
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
                                               (2.3) 
With the matrix  , one obtains ( )S q ( ) ( ) ( )q t S q v t=& . Further, the Lagrange multipliers λ in (2.1) can be 
eliminated by pre-multiplying with , thus obtaining  
TS
( ) ( ( , )) (T T TS MS v S MSv V q q S E q)τ+ + =&& &                                                       (2.4) 
To have a more useful system model, one discards the last two variables of the configuration variables and 
re-define the following two vectors,  and   , and the augmented state vector x. rq rV
 ,   , rr r
r
x
qy
q V v v
V
αωα
θ ωθα
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎡ ⎤⎢ ⎥ ⎢ ⎥⎢ ⎥≡ = ≡ = ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎣ ⎦⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎣ ⎦⎣ ⎦⎣ ⎦
x
&
&                                                                         (2.5)  
Taking the time derivatives of the augmented state vector  gives  x
( )τ=x f(x) + g x&                                                                                     (2.6) 
where  
,
⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥⎣ ⎦⎣ ⎦
1 1
22
f (x) g (x)
f (x) = g (x) =
g (x)f (x)
 
and  
1
cos( )
sin( )
( )
v
v
f
α
θ
θ
ω
ω
⎡ ⎤⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
x , 1 4( ) 0g 2×=x , 
21
1
2 22
23
2 2 2 2 2 2 2 2
2 2 2 2 2 2 2 3
( )
( ) ( ) ( ) ( ( , ))
( )
1 1 1(sin(2 ) ) ( sin(2 )( ) ) ( 2 4 4 ) sin( )
2 2
1 1( sin(2 )) ( 4 4
2 4
T T
b z b z wa b z w b z
b z yy b z b z
f
f f S MS S MSv V q q
f
H M c R M R c I M c M R M c g
D D D
K M c R g I M R c R M c
D D
α α α
α
α α
α θ α α
θ α
−
⎡ ⎤⎢ ⎥=− +⎢ ⎥⎢ ⎥⎣ ⎦
+ + − − −
+ + − −
x
x x
x
& & 
& &
&
α
2
2 2 2 2
)sin( )( )
1 1( ( ) )sin(2 ) (sin( ) )xx yy b z b zI I R M c R R M c vG Gα α
α α
α αθ α θ
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥− − − −⎢ ⎥⎢ ⎥⎣ ⎦
&
& &&
 
 17
c o s
s in
r
vx
vy
q
α
θ
θ
ωθ
ωα
⎡ ⎤⎡ ⎤ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥= = ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥⎣ ⎦ ⎣ ⎦
&
&& &
&
                                                                                  (2.10) 
and 
21 21
22 22
2323
( ) ( ) 0
( ) ( ) 0
0 ( )( )
p
r
y
f x g x
V v f x g x
g xf x
αω τ
τω
⎡ ⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥= = + ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦⎣ ⎦⎣ ⎦
&
& &
&
                                                      (2.11) 
The kinematic part reveals the relations between the position, orientation and inclination of the pendulum 
and their velocities, whereas the dynamic part involves with the relations between the three accelerations 
and the two torques, pτ  and yτ  . Furthermore, from (2.10) and (2.11), it indicates that two controllers for 
pτ  and yτ can be synthesized independently from each other and then combined together to accomplish the 
control goal.   
   Due to some parameter variations in the system model of the robot, it is reasonable to assume that the 
four functions can be decomposed into two terms: nominal and perturbed, i.e.,  
 
( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )
0 0 0
21 21 21 22 22 22 23 23 23
0 0 0
22 22 22 21 21 21 23 23 23
, ,
, ,
f f f f f f f f f
g g g g g g g g g
= + Δ = + Δ = + Δ
= + Δ = + Δ = + Δ
x x x x x x x x x
x x x x x x x x
,
x
        (2.12) 
where these perturbed terms ,( )21fΔ x ( )22fΔ x , ( )23fΔ x , ( )21gΔ x , ( )22gΔ x  and  are bounded. Thus, 
(2.11) can be rewritten by  
( )23gΔ x
   
0 0
21 21 1
0 0
22 22 2
00
23 323
( ) ( ) 0 ( )
( ) ( ) 0 ( )
0 ( ) (( )
p
r
y
f x
)
g x h
V v f x g x h x
x
g x hf x
αω τ
τω
⎡ ⎤ ⎡ ⎤⎡ ⎤
x
⎡ ⎤⎡ ⎤⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥= = + +⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎣ ⎦⎢ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦⎣ ⎦ ⎣ ⎦⎣ ⎦
&
& &
&  
where 1 21 21( ) ( ) ( ( )) ph x f x g x τ= Δ + Δ , 2 22 22( ) ( ) ( ( )) ph x f x g x τ= Δ + Δ  and 
3 2( ) 3 23( ) ( ( )) ph x f x g x τ= Δ + Δ . In applications these three functions are unknown but bounded.  
 
2.2.2 Problem Formulation  
The design goal of the STS control method for the pendulum system model described by (2.10) and 
(2.11) is to keep the trajectories of the pendulum asymptotically follow time-varying reference trajectories 
or desired positions and orientations of a fixed reference configuration in one unified control framework, 
 19
known kinematic model of a mobile robot with differential driving, and apply the STS control approach 
developed by Dixon et al. [23] to the STS goal. The following elucidates some details of our STS controller, 
which will be later shown to be superior to the one by Dixon et al. [23], because the STS controller adopts 
the following new globally invertible transformation.  
 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡ −+−
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
θ
θ
θθθθθθ
~
~
~
1                       0                         0            
0                sin                           cos           
0     cos2sin~-    sin2cos~
2
1 y
x
  
           
θ    
kkkk
z
z
w
                                                   (2.15) 
 where ,  and  are the auxiliary tracking error vectors, and 0k >
(
1)( Rtw ∈
1
[ ] 221 )(  )()( Rtztztz T ∈=
)~ ),(~ ),(~ R∈tt θytx  are given in (2.12). From (2.6), we have the inverse transformation  
2
2 1
2
1  1sin        ( sin 2 cos )        0
2 2
1  -1cos       ( cos sin )     0
2 2
   0                 0                         1
z k
k kx w
y z      
k k
z
z           
θ θ θ
θ θ θ
θ
⎡ ⎤+⎢ ⎥⎡ ⎤ ⎡ ⎤⎢ −⎢ ⎥ ⎥ ⎢ ⎥⎢= +⎢ ⎥ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎣ ⎦⎣ ⎦ ⎢ ⎥⎢ ⎥⎣ ⎦
%
%
%
                                                          (2.16) 
Note that, from (2.16), if ∞∈ Ltztztw )( ),( ),( 21 , then ∞∈ Lttytx )(~ ),(~ ),(~ θ . Moreover, it is easy to show that 
, then 0)( ),( ),(lim 21 =∞→ tztztwt
0)(~ ),(~ ),(~lim =∞→ ttytxt θ                                                                                (2.17) 
Furthermore, differentiating (2.15) and using (2.10), (2.12-3-13) obtain  
( ) ( )21
2
12 sin2  
0      1
1   0
     
cos
  
                    0
cos~sin~      
zvzk
z
z
k
zkvv
k
yxkk
w rr
T
r
r −+⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡ −
⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎥⎦
⎤⎢⎣
⎡−⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡ +−= ωωω
θθ&
( )
⎥⎦
⎤⎢⎣
⎡−⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡ −−=⎥⎦
⎤⎢⎣
⎡=
r
r zvvyx
z
z
z ωω
θθ
     
cos
  
1                   0
cos
      (2.18) 
~sin~   1 2
2
1
&
&&                                               (2.19) 
Combining (2.18) and (2.19) in a vector-matrix form, the dynamics of the tracking error becomes 
μ
μ
=
+=
z
kfZJkw TT
&
&
                                                                             (2.20) 
where  is the skew-symmetry constant matrix defined by 22×∈RJ T
⎥⎦
⎤⎢⎣
⎡=
0      1
1 -    0
TJ                                                                                       (2.21) 
and  is an auxiliary signal given by  1Rf ∈
  )sin(2 21 zvzf rr −= ω                                                                   (2.22) 
Further, [ ] 21 2( ) ( )  ( ) Tt t tμ μ μ= R∈  is called the auxiliary control input having the following form  
 21
⎥⎦
⎤⎢⎣
⎡ +Ω+++= d
d
d
d
d
T
T
dd
t
D zJzkw
fzzkwkzzz
dt
d
δ
δ
δ
&
)
~
(2)( 12
31                                    (2.31) 
which, with (2.28), leads to  
d
T
d
d
d
d
t
D zzzzdt
d
δ
δ&2)( =                                                     (2.32) 
Using (2.28) , we have 
)()()()( 22 ttztztz ddd
T
d δ==                                             (2.33) 
which is the unique solution of (2.32), where •  is standard Euclidean norm. 
To develop the closed-loop error control system of (2.20), one substitutes (2.27) into (2.20) to obtain  
2( )
T T T T
aw k k z J z kf k J z kfμ μa= − + =& +                                          (2.34) 
and then substitutes (2.21) into (2.34) to have  
kfzJkzJkw d
TT
a
T
a ++= μμ ~&                                                   (2.35) 
Moreover, by substituting (2.27) into (2.35) and using (2.33), the closed-loop error system for  is 
obtained from  
)(tw
1
T
aw k Jz kk wμ= −& %                                                               (2.36) 
Next, by differentiating )(~ tz  with respect to time, and substituting  in (2.27) and  in (2.20) into )(tua z&
)(~ tz&  and utilizing (2.27-3-29), one obtains dynamics of the closed-loop error system for )(~ tz  governed by  
 2az kwJ k zμ= −&% %                                                                (2.37) 
The globally exponentially stability of the aforementioned closed-loop error system can be easily proven by 
selecting a radially, unbounded and quadratic Lyapunov function 1 (t)V R∈  by 
21 1( )
2 2
TV t w z z= + % %                                                          (2.38) 
This main result is summarized as below.  
Theorem 2.1: Let 
121   ,  ,  , αkkk  be real positive constants satisfying 1 2 1min( , )kk k α> . Then the control laws 
(2.26-3-30) make the closed-loop error system (2.20) globally exponentially stable, i.e., 
te 32ttytx )(
~~ ,)(,)(~ αα −θ ≤ , where 12 R∈α  and 13 R∈α  are positive constants. 
 
Remark 2.3: The virtual controls for v and  ω  are obtained from (2.23) and expressed by    
22
3
cos
      
r
r
v zv
T T
φ μφω ω
⎡ ⎤⎡ ⎤⎡ ⎤ ≡ = + ⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
                                    (2.39) 
 23
  
2.3.2.2 Sliding-Mode Posture and Speed Torque Control Module 
    Next, move to derive the second sliding-mode posture and speed control law for the torque pτ .  With the 
aforementioned virtual controls of  αω  nd v   in (2.14) and (2.39), both backstepping errors are 
respectively ex
a
pressed by  
1 ( )Kα α α αη ω φ ω α= − = − − , 2v vη φ= −                                                              (2.44) 
and their sliding surfaces are respectively defined by Sα αη=  and vS vη= . Taking the time derivatives of 
both sliding surfaces and using the dynamic part in (2.11) yield 
0 0
21 21 1
0 0
2 22 22 2 2
( ) ( ) ( )
( ) ( ) ( )
p
v v p
S K f g h K
S v f g h
α α α α αη ω α τ
η φ τ φ
= = + = + + +
= = − = + + −
x x x
x x x
& & & &
& & && &
α&
                                                           (2.45)
 
In (2.45), there is only one torque that has to stabilize both dynamics of  Sα  and ; this is so-called 
underactuated control problem. Among several approaches which have been useful for this kind of 
underactuated control problem, a hierarchical decoupling sliding-mode control method in [19] is used to 
accomplish the control goal, and proposed as  follows;  
vS
                                                                                                                                     (2.46) 1 2 vS r S r Sα= +
where  and  are two constants. Differentiating S obtains  1r 2r
1 2
0 0 0 0
1 21 21 1 2 22 22 2 2
0 0 0 0
1 21 2 22 1 21 2 22 1 1 2 2 1 2 2
[ ( ) ( ) ( ) ] [ ( ) ( ) ( ) ]
( ) ( ) ( ( ) ( )) ( ( ) ( ))
v
p p
p
S r S r S
r f g h K r f g h
r f r f r g r g r h r h r K r
α
α
α
τ α τ φ
τ α φ
= +
= + + + + + + −
= + + + + + + −
x x x x x x
x x x x x x
& & &
&&
&&
                                     (2.47) 
   Let the torque pτ  be  
0 0
1 21 2 22 1 1 2 2 2 2 1 10 0
1 21 2 22
1 [ ( ) ( ) ( ( ) ( )) sgn( )]
( ( ) ( )) sp
r f r f r h r h r r K K S
r g r g α
τ φ= − − − + + − −+ x x x xx x
& &α                   
(2.48) 
 where 1sK  is a  real and positive constant. Substituting (2.48) into (2.47) yields  
                                                                                                (2.49) 1 sgn( )sS K S= −&
Similar to the previous case, it is easy to show that S  approaches zero in finite time by selecting the 
Lyapunov function . Worthy of mention is that  must be nonzero for every 
. Once the sliding surface  S  approaches zero, it is easy to show that in finite time from the 
2
2 0.5V = S ))x
0
0 0
1 21 2 22( ( ) (r g r g+x
, vS Sαx →
 25
Assumption 2.1: The norm of optimal weight, W , ω ,and c , are bounded, i.e., 
*
Wb≤ < ∞W , * bω≤ < ∞ω and * cb≤ < ∞c                                          (2.39) 
The actual fuzzy wavelet network estimate, , can be represented by  fˆ
ˆ ˆ ˆ ˆ ˆ( , , )Tϕ=f W x c ω                                            (2.40) 
where ˆ ˆ ˆ, , ,c ˆϕ ωW  are estimates of * * * *, , ,cϕ ωW in the fuzzy wavelet network. By defining * ˆ= −W W W% , 
* ˆϕ ϕ ϕ=% − *, ˆω ω ω−=% , , (2.38) can be rewritten as * ˆc c c= −%
  
* * * * * * *ˆˆ ˆˆ ˆ( , , ) ( ) ( )
T T T T T
f fx cϕ ω ϕ ϕ ϕ ϕ ϕ= + = + + + = + + + +f W ε W W ε f W W W ε% %% % f% %                                (2.41) 
Let  
*T
fϕ= +ε W ε% %                                                               (2.42) 
Then 
ˆ ˆˆT Tϕ ϕ= + + +f f W W ε% %                                                     (2.43) 
Furthermore, * ˆϕ ϕ ϕ= −%  can be expanded by Taylor series as 
   
ˆ ˆ o
c h c h
c
ϕ ϕϕ ω ωω
∂ ∂= + + = + +∂ ∂ A B
% %% % %% o%                                           (2.44) 
where 
 
*
1 2 ˆˆ ˆˆ ˆ( )    --- 
ˆ ˆ ˆ ˆ ˆ ˆ
T
N
nN N
ϕϕ ϕϕ ϕ ϕ ϕ
ω ω ω ω ω ω ×
∂∂ ∂∂ ∂ − ∂ ⎡= = = − = −⎢∂ ∂ ∂ ∂ ∂ ∂⎣ ⎦A
% ⎤⎥                                    (2.45) 
For j=1,…N, i= 1,…,n; 
[ ]1 1 1 2 1 1 2ˆ ˆ ˆ ˆ ˆ ˆ ˆ  . . .    . . . Tn N N N nω ω ω ω ω ω ω= L                                      (2.46)  
{ ( ) ( ) {1
( 1) ( 1)
ˆ
0....0 / / 0....0
ˆ
j
j j j jn
j n N
ϕ ϕ ω ϕ ωω − × − ×
∂
n
⎡ ⎤= ∂ ∂ ∂ ∂⎢ ⎥∂ ⎣ ⎦
L                             (2.47) 
2 2 2
2 2
[2 ( ) ][ 2 ( ) ]
( )
1 ( )
ji i ji ji i jii
i i
ji ji i ji
x c x c
x
x c
ω ωϕ ϕω ω
− − − −∂ = −∂ − −                                (2.48) 
Similarly,  
*
1 2
ˆˆ,
ˆˆ ˆˆ ˆ( )  
ˆ ˆ ˆ ˆ ˆ ˆ
T
N
c c
nN Nc c c c c c
ω ω
ϕϕ ϕϕ ϕ ϕ ϕ
= =
×
∂∂ ∂∂ ∂ − ∂ ⎡= = =− =−⎢∂ ∂ ∂ ∂ ∂ ∂⎣ ⎦B
% L ⎤⎥
]
                        (2.49) 
[ 11 12 1 1 2ˆ ˆ ˆ ˆ ˆ ˆ ˆ  ...    ... Tn N N N nc c c c c c c= L                                         (2.51) 
{ ( ) ( ) {1
( 1) ( 1)
ˆ
0....0 / / 0....0
ˆ
j
j j j jn
j n N
c c
c
ϕ ϕ ϕ
− × − ×
∂
n
⎡ ⎤= ∂ ∂ ∂ ∂⎢ ⎥∂ ⎣ ⎦
L                          (2.52) 
 27
ˆ ˆ
y y
y y y y y
∂ ∂
y y y y
yy
= + + = + +Φ ΦΦ C ω h A C B ω h
% %
∂∂ ωC
% %% %
y ;  ; 
                                      (2.61) 
where  ˆy y= −*C C C% is the vector containing  higher order terms and satisfies ph b≤ˆy y= −*ω ω ω% y ph . 
Substituting (2.61) into (2.60) gives  
y
ˆ ˆ ˆ ˆ( )T T Ty y y y y y y y y yf ε= + + +W Φ W A C B ω W Φ% %% +              (2.62)  
ywhere  *T Ty y y y yε ε= + +W h W Φ% % %  and ε i  assumed to satisfies s maxy ygε < . 
 
Substituting  in (2.57) into  in (2.41) gives yC Sω&
0( )f φ0 0 0 23 3
23 23 3 3 23 0
23
0
0 23 3
23 0
23 23
0 0
23 23 6
( ) ( ) ( ) ( )[ ]
( )
( )
( ( ) ]sgn( ) ]
( ) ( )
ˆ ˆ ˆ( )[ ] ( ) sgn( ) sg
y y y
y y
T T
y y
S f g x h g x f
g x
ff x g S f
g x g x
g x g x g S K
ω
ω
ω
τ φ τ
0 0
23 3 6 230
1ˆ)[ ( ) [ ( ) ] [ /yg x f x K g
φφ= − − − + −x &
ε
= + − + = + +
−− + +
= + + − −y y y y
x x
x
W Φ W Φ
& &
&
% n( )Sω
                                           (2.63) 
and  
−x &
0
23 6
0
23 max 6
ˆ ˆ( )[ (( ) ) ( ) ]
ˆ ˆ( ){ ) ( ) }
T
y y
y y
S S g x S S g S K S
g x S g g S K S
ω
0
23 6
ˆ ˆ( ){[ ] }g x S K S
ω ω ω ω ω
ω ω ω
ε≤ + + + + − −
≤ + + − − −
T
y y y y y y y
T T
y y y y y y y
W A C B ω W Φ
W ((A C B ω ) W Φ
& % %%
% %%                                          (2.64) 
To obtain the parameter updating rules for  we have  the following Lyapunov function 
ω ω≤ + −T Ty y y y y y yW (A C +B ω ) W Φ% %%
ˆˆ ˆ, , ,y y yW C ω
2
3
1 , 0, 0,
2 2 2 2 y y y
y y y
T T T
W C
W C
V Sω ω
ω
λ λ λλ λ λ= + + + > > >
y y y y y yW W C C ω ω% %% % % % 0                                                       (2.65) 
Taking the time derivative of 3V  gives 
3
0 0 0
23 23
ˆˆ ˆ( ) ( ) ( )
ˆˆ ˆˆ ˆ( [ ( )] [ ( )]
y y y
y y
T T T
y y
W C
T T T
y
C
V S S
W g x g x
ω ω
ω
ω
λ λ λ
λ λ
= + − + − + −
− + + − +
y y y
y
y y
y y y y y
W C ω
W C ω
ω
C A ω B W
%% %&& &&&
&& &
% %
If the parameter updating rules are selected as 
                                        (2.66) 
6 23
ˆ[ ) ]
y
T
y
W
K S g x Sω ηλ= − + − + Φ +
y
y
W C
W%
0
23
0
23
ˆ ˆ( )
ˆ ˆ( )
ˆ)
y
y
y W y
T
y C
T
g x S
g x S
ω
ω
0
23ˆ (y g x Sω ω
λ
λ
⎧ = Φ⎪⎪ =⎨ y y
y
W
C A
B W
&
&
λ⎪ =⎪⎩ yω
&
y
W                                                                                   (2.67) 
where 023 ( )g x = 0
R b
Gα
⋅ . Then 3 6 0V K Sω≤ − ≤&  and it implies that  in finite time.  0Sω →
 29
*ˆ T T
p pε ε= + +p p p pW h W Φ% %where   and ε is assumed to satisfies maxp pgε < . 
 
tSubstituted the proposed controller in o 1S&  
0 0 0 0
1 1 21 2 22 1 21 2 22( ) ( ) ( ( ) (S r f r f r g r g
( ) ( ) ( ) ( )
1 1 2 2 1 2 2
0 0
1 21 2 22 1
)) ( ( ) ( ))
ˆ( ( ) ( ))
p
p p S
r h r h r K r
r g r g f f gsgn S K sgn S
ατ α φ+ + + −
⎡ ⎤= + − + −⎣ ⎦
x x
x x x x
&
 
= + + +x x x x& &
                   
.74),one obtains    
                                (2.75) 
With (2
( )ˆ( )p pf f−x x ( )ˆ ˆT Tp p pε= + + +p p p p pW A C B ω W Φ% %% .                                    (2.77) 
and  
( ) ( ) ( )1 1 1sgn Sg S K sgn S+ • −                  (2.78) 0 01 1 21 2 22 ˆ ˆ( ( ) ( )) T Tp p pS r g r g ε⎡ ⎤= + + + +⎣ ⎦p p p p px x W A C B ω W Φ& % %%
*
max
ˆ, T Tp p p p pg gwhere pε ε ε≤ ≤ = + +p pW Φ W h% % . Note that ))  can be made negative by 
 and .  Using the inequa
0 0
1 21 2 22( ( ) (r g r g+x x
lity appropriate selection of  1r 2r A B A B+ ≥ − , one obtains 
( ) ( )
max
max
ˆ ˆ sgn
ˆ ˆ[ ( ) ]
ˆ ˆ[ ( ) ]
ˆ ˆ[ ( ) ] ( )
ˆ ˆ[ (T T) ]
T T
p p p
T T
p
T T
p
T T
p
g S S
S S g S
S g S g S
S g g S
S
ε
ε
⎡ ⎤+ + + + •⎣ ⎦
≥ + + − +
≥ + + − +
= + + + −
≥ + +
p p p p p
p p p p p p p
p p p p p p p
p p
p p p p p
W A C B ω W Φ
W A C B ω W Φ
W A C B ω W Φ
W A C B ω W Φ
W W Φ
% %%
% %%
% %%
% %%
%
                        
p p p p p
p pA C B ω% %
Since 0 01 21 2 22( ) ( ) 0r g r g+ <x x , one obtains  
                  (2.79)  
0 0
1 1 1 21 2 22 1 1 1
ˆ ˆ( ( ) ( )){ S [ ( ) ]}T T SS S r g r g K S≤ + + + −p p p p p p px x W A C B ω W Φ& % %%              (2.80) 
Next , move to find the parameter updating laws for p  In doing soˆˆ ˆ, ,p pW C ω . , we propose 
2
4 1
( )1
2 2 2 2
p p p
T T T
C C w
V S λ λ λ= + + +
p p p p p pC C ω ω W W% % % %% %                                                    (2.81) 
*
p . Differentiating   yields  where * ˆ− =p p pω W W%* ˆ ˆ, ,= − = −p p p p pC C C ω ω W% % 4V
0 0 0 0 0 0
4 1 1 21 2 22 1 1 21 2 22 1 21 2 22
1 1
ˆ ˆˆ( ) ( ) ( )
T T T
SK S λ λ λ− + − + − + −
p p p
p p p
C ω W
C ω W
% %%& &&
0 0
1 1 1 21 2 22
ˆ ˆ ˆ( ( ) ( )) ( ( ) ( )) ( )( ( ) ( ))
ˆ
ˆ[ ( )( ( ) ( )
p p p
p
T T T T T
p
C W
T
S
W
V C S r g r g S r g r g r g r g
K S r g r g
ω
λ
= + + + + +
= − + − + +
p p p p p p p
p
p p
A W x x ω B W x x W Φ x x
W
W Φ x x
%& %%
&
% 0
1 1 21 2 22
0 0
1 1 21 2 22
ˆ
ˆ)] [ ( ( )
ˆ
ˆ[ ( ( ) ( ))]
p
p
T T
C
T
W
S r g
S r g r g
λ
λ
+ − +
+ +
p
p p p
p
p p p
C
C A W x
W
W x x
&
%
&
 If the parameter adjustment rules are chosen by 
0 ( ))]r g+ x
            (2.82) 
T+ −W B%
 31
Motion 
command 
generator
Platform
Trajectory 
generator
Dead-
reckoning
Encoder
*ω Yaw ratecontrol
Posture and 
speed control
Torque speed 
conversion
Cθ
Cδ
CRLC
Rθ
Lθ *Lω
x
p y
θ
⎡ ⎤⎢ ⎥= ⎢ ⎥⎢ ⎥⎣ ⎦
Σ
*v
0.5
-0.5
0.5
Σ
*
Rωω
v
eQEP
LP
RP
0.5
Tilt
Gyro
DSP
Platform
fv
fω
 
Figure2.4. Block diagram of the control architecture for the robot. 
 
parameters obtained f cted to show the 
performance and applicability of the proposed method.   
cing two-wheeled robot 
igure2.3 displays the picture of the experimental self-balancing two-wheeled robot. As shown in 
motors with gearbox and 
two
and generator, slide-mode posture and speed control 
mo
rom simulation results, two experimental results are then condu
 
2.5.1 Brief Description of the Experimental Self-balan
F
Figure2.3 (b), this self-balancing two-wheeled robot is composed of two 24V DC 
 stamped steel wheels with "16  tires, two 12-volt sealed rechargeable lead-acid batteries in series, and 
two motor drivers. The system adopts a digital signal processor (DSP) TI 320F28335 from Texas 
Instrument as a main controller, one gyroscope and one tilt sensor as the two sensors. The DSP mounted on 
the robot platform controls the posture and linear speed of the platform by using one gyroscope and one tilt 
sensor which are employed for measuring the rate and the angle of the inclination of the platform. In 
addition, the DSP-based controller also provides controls for the yaw rate of the platform using one high-
performance optical gyroscope. The two optical encoders installed on the driving wheels are employed to 
achieve dead-reckoning calculation of the platform.  
Figure 2.4 depicts the block diagram of the control architecture for the robot. In Figure2.4, there are 
three main control modules: the virtual motion comm
dule, and, and slide-mode yaw rate control module. The virtual motion command generator provides 
ideal linear and angular speed commands for the speed and yaw rate control modules for steering the robot 
to exactly follow the desired trajectories. The posture and speed control module is responsible for achieving 
 33
[ ]b m  0.2 
[ ]R m  0.25 
[ ]zc m  0.25 
2( / )G m s  9.8
0 2 4 6 8 10 12-1
0
1
2
3
4
5
6
X(m)
Y(
m
)
X-Y plane position
0 10 20 30 40 50 60-12
-10
-8
-6
-4
-2
0
2
t(sec)
po
si
tio
n 
er
ro
rs
(r
ad
)
 
 
X-axis error 
Y-axis error
 
(a)                                                                (b) 
0 10 20 30 40 50 60-1.5
-1
-0.5
0
0.5
t(sec)
or
ie
nt
at
io
n 
an
gl
e 
er
ro
r(
ra
d)
 
0 10 20 30 40 50 60-0.4
-0.2
0
0.2
0.4
0.6
t(sec)
til
t a
ng
le
(r
ad
)
 
(c)                                                                (d) 
Figure 2.5. Simulation result o e initial pos  to 
) tilt angle. 
  Stabilization 
n is conducted to investigate the performance of the proposed FWN-based intelligent 
adap
in i t ia l  pos ture  to  th
f the proposed controller for regulation from th e (0m, 0m, 30 )°
; (c) orientatithe final pose (10m , 5m, 90 ) ° : (a) simulated trajectory in the x-y plane; (b) position errors on 
angle error, (d
      
2.5.3
The first simulatio
tive controllers (3.36) and (3.48) for stabilization. The initial pose of the self-balancing two-wheeled 
mo t ion  robo t  i s  a s sumed  a t  0 0 0( , y , ) (0 m,  0 m, 30 )x θ = o  and  t he  f i na l  pose  was  s e t  by 
( ,  ,  ) = (10 m , 5 m, 90 )x y θ o . Figure 2.5( ult of the mobile robot moving from the 
e  des i red  pos ture .  F igures  2 .5(b)  and 2 .5(c)  depic t  the  pos i t ion 
a) depicts the simulation res
 35
2.5.5 Circular Trajectory Tracking 
The third simulation for the STS motion controller is to steer the self-balancing two-wheeled mobile 
robot to follow a circular trajectory described by 0 0 0( ( ),  ( ),  ( )) ( ,  ,  )r r rx t y t t x v t y v t tθ θ ω= + + +  where 
0.6 m/s,  0.2 rad/sr rv ω= = .  
-4 -3 -2 -1 0 1 2 3-2
0
2
4
6
8
X(m)
Y(
m
)
X-Y plane position
 
 
reference trajectory
simulated trajectory
0 10 20 30 40 50 60-2.5
-2
-1.5
-1
-0.5
0
0.5
t(sec)
po
si
tio
n 
er
ro
rs
(m
)
 
 
X-axis error
Y-axis error
 
(a)                                                               (b) 
0 10 20 30 40 50 60-0.8
-0.6
-0.4
-0.2
0
t(sec)
or
ie
nt
at
io
n 
an
gl
e 
er
ro
r(
ra
d)
0 10 20 30 40 50 60-0.2
-0.1
0
0.1
0.2
0.3
t(sec)
til
t a
ng
le
(r
ad
)
 
(c)                                                                (d) 
Figure 2.7. Simulation result of the proposed controller for circular trajectory tracking: (a) simulated 
trajectory in the x-y plane; (b) position errors; (c) orientation angle error, (d) tilt angle. 
 
Figure 2.7 shows the simulation result of the self-balancing two-wheeled mobile robot with the proposed 
FWN-based intelligent adaptive controllers for circular trajectory tracking. The result indicates that the 
proposed controller perform well for steering the robot to exactly track the desired circular trajectory. 
 
 37
 39
wheeled mobile robot. In this chapter, three uncertain functions in the dynamic robot model have been 
decomposed into two terms: nominal and perturbed. The proposed fuzzy wavelet networks are utilized to 
proximate the nonlinear perturbed part of the mobile robot system model including static friction. The 
backstepping and sliding mode control approaches have been employed to design two control laws, and the 
on-line parameter updating rules for the fuzzy wavelet networks have been derived using the Lyapunov 
stability theory. Through simulations results, the proposed intelligent adaptive STS controllers have been 
shown useful and effective in providing appropriate control actions to steer the mobile robot to achieve 
simultaneous tracking and stabilization. 
  
Figure 3.1. The face detection algorithm. 
 
3.2.1. YCbCr color model 
Human skin color has been used and proven to be an effective feature for face detection 
and its related applications. Although skin color differs from each other, several studies have 
shown that the major difference exists in the intensity rather than the chrominance. The 
system used the YCbCr color model. 
 
 
Figure 3.2. Fuzzy skin color adjuster. 
 
3.2.2. Color skin separation 
 The purpose of the subsection is to segment the quantized color image according to skin 
color characteristics. However, the use of the color space transform still can not eliminate all 
of the effects from the light intensity. In order to reduce the effect, the proposed human-robot 
interactive system use fuzzy logics to adjust the values of the Y parameter in the YCbCr color 
space for attaining better performance. In doing so, consider an M N×  images. As depicted 
in Figure 3.2, the average of the gray level of the pixel is defined by 
 41
separation. 
                                                    
(3.3) 
[86,127]
 
[130,168]
Cb
Cr
∈⎧⎨ ∈⎩
 
3.2.3. Edge processing 
An edge is defined by the boundary between two regions with relatively distinct 
gray-level properties. When the regions are sufficiently homogeneous, the transition between 
two regions can be determined on the basis of gray- level discontinuities alone. In practice, 
the most common way to look for discontinuities is to run a mask through the image. In this 
chapter, Sobel operators are used to detect the edge of image. 
 
3.2.4. Oval model detection 
This subsection aims to explain the use of the mask of oval model to detect human faces. 
Most of human faces look like an ellipse under normal situation, so the kind of information 
can be used to detect and obtain the position of the human-robot interactive system within the 
image. However, the use of the oval model detection is shown to take much time to check the 
center of the ellipse on the pixel level. Thus, an alternative scheme will be used to reduce the 
processing time. The scheme is interpreted as follows. After skin detection, the system cuts 
down the biggest size in the binary image, puts it in the center of the image, and then utilizes 
the above-mentioned ellipse detection method to find a face-like shape. This scheme can 
definitely reduce the range of the center of the ellipse circle, thereby saving processing time. 
 
3.2.5. Extracting features of human faces 
Once the previous scheme has been applied to determine a face-like shape, this 
subsection will go further to ensure whether the detected shape is a truly human face. Feature 
extraction of human faces by min-max analysis will facilitate the success of the face detection. 
From the min-max analysis, we create some profiles of the features of a face region and 
 43
expression is often time consuming due to large dimension of the images. Given an original 
image, the Haar wavelet transform method separates high frequency and low frequency bands 
of the image by high-pass and low-pass filters from the horizontal direction and so does the 
vertical direction of the image. Figure 3.5 shows the transformed results in the grey level. 
 
 
(a)           (b)              (c) 
Figure 3.5. The results of the Haar wavelet transform. (a) The original image; (b) the result of 
the Haar wavelet transform; (c) the position relationship picture of sub-image. 
 
3.3.2. Principal component analysis 
Principal Component Analysis (PCA), called Karhunen-Loeve Transform (KLT) or 
Hotelling Transform, is a classical technique for multivariate analysis. The method can be 
traced back to Pearson [20] in 1901 and developed by Hotelling [13]. Its detailed instructions 
have been given by Jolloffe [21]. Principal component analysis can be employed to find out 
the most important abstract features of faces collected in their database. The main concept 
behind the method is to calculate the eigenvectors of the matrix A which represents a 
collection of all images of interest, and then to use   these eigenvectors to span the column 
space of matrix A. Hence, if the matrix A is constructed from all the faces in the database, 
then a set of eigenvectors can be calculated in order to span the face space sufficiently and 
efficiently. Suppose that there are M different human face images and each image has   
by  pixels. Accordingly, M different human faces can be represented by m 
N
 1N 2N ×  
vectors. 
 45
3.3.3. Recognition method using support vector machine 
One of the merits of SVM is to map the input vector into a higher dimensional feature 
and thus can solve for the nonlinear case. By choosing a nonlinear mapping 
function ( ) Mx Rϕ ∈ , where M N> , the SVM can construct an optimal hyperplane in this 
new feature space. ( ) is the inner-product kernel performing the nonlinear mapping 
into feature sp )ix . Hence, the dual optimization problem 
becomes 
, ik x x
( )k x xace. ( ), ,i ik x x ( ) (Txϕ ϕ= =
( ) (
1 1 1
1 W =
2
l l l
i i j i j i
i i j
)jMAX y y k x xα α α α
= = =
− ⋅∑ ∑∑             (3.9) 
            
1
0 ,
0
i
l
i i
i
C i
y
α
α
=
≤ ≤ ∀⎧⎪⎨ =⎪⎩∑                               (3.10) 
subject to the constraints as in (3.10). The only requirement on the kernel  is to 
satisfy the Mercer’s theorem [9]. Using Kernel functions, without treating the high 
dimensional data explicitly, unseen data can be classified as follows 
( , ik x x )
( )
( )
,  0
,  0
positive class if f x
x
negative class if f x
>⎧⎪∈⎨ <⎪⎩                    
 (3.11) 
where the decision function is 
1
( ) sgn ( )
l
i i i
i
f x y k xα
=
⎛ ⎞= ⋅⎜ ⎟⎝∑ x⋅ ⎠                     (3.12) 
where the kernel function is given by 
( ) 221, 2ik x x e x xσ
⎛= − −⎜⎝ ⎠i
⎞⎟                     (3.13) 
A multi-class pattern recognition system can be obtained by combining two classes SVMs. 
The one-against-one strategy will be used to classify between each pair. One-against-one will 
generate a support vector machine for two classes. 
 
 
 47
respectively. Moreover, the system matrix  and the output matrix C are given by   dA
0
0
1
T
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
1 0
0 1 0
0 0 1
0 0 0
d
T
A = , 4{1,1,1,1}C diag I= =  
where the sampling time T is 0.2 seconds. 
The Kalman filter provides a recursive state estimate ( )Xˆ k k  at time k   through a 
prediction estimate (ˆ 1 )X k k+  given all information up to time  and a new 
measurement  ,  is a Kalman filter gain and 
1k +
( )1Z k + ( 1K k + ) ( )1r k +  is the innovation 
given by 
( ) ( ) ( )ˆ1 1r k 1k CX k k+ = + − +Z                  (3.17) 
( ) ( ) ( )( ) 1ˆ1 1 1T TK k P k k C CP k k C R −+ = + + +            (3.18) 
The Kalman filter can also be described by the following equations. 
(i) One-step-ahead Prediction: 
  ( ) ( )ˆ ˆ1 dX k k A X k k+ =                        (3.19) 
   ( ) ( )ˆ ˆ1 Td dP k k A P k k A BQB+ = + T                  (3.20) 
(ii) Estimation (Measurement Update): 
( ) ( ) ( ) (ˆ ˆ1 1 1 1 1X k k X k k K k r k⎡+ + = + + + +⎣ )⎤⎦            (3.21) 
( ) ( ) ( ) ( )ˆ ˆ ˆ1 1 1 1 1P k k P k k K k CP k k+ + = + − + +           (3.22) 
 
3.4.2. Exponential weight Kalman filter 
This subsection briefly describes a new type of the KF using the exponential data 
weighting method. In order to deal with the variable noise characteristics or the uncertainties 
of the dynamic system modeling, the parameters in the state estimators should be tuned to 
avoid erroneous estimates or filter divergence. The exponential weighting method is thus 
 49
1. Statistical mean of the innovation: 
( )
1
1 l
k
r r
l =
≅ ∑ k                                    (3.25) 
2. Statistical second-order moment of the innovation: 
( )2 2
1
1 l
r
k
r k
l
σ
=
≅ ∑                                   (3.26) 
3. The variation of the innovation second-order moment: 
( ) ( )2 2 1Slew rate r rk kσ σ= − − k                           (3.27) 
where the parameter  is positive and can be chosen by designers. In order to obtain a 
better estimation performance for the proposed FKF, it is important to select appropriate 
triangular membership functions for three fuzzy inputs and one fuzzy output and they are 
illustrated in Figure 3.7 (a), (b) and (c) , respectively. The output of the fuzzy tuner is the 
weighting factor 
1k
α  as shown in Figure 3.7 (d). 
 
Figure 3.7. (a)Membership function for the mean of the innovations. (b) Membership 
function for the second-order moment of the innovations. (c) Membership function 
for the slew rate of the second-order moment 2rσ .(d) The membership function of the 
weighting factorα . 
 51
feasibility, accuracy and performance of the proposed face tracking method. Two computer 
simulations using MATLAB codes are used to examine the proposed moving face estimation 
algorithm with time-varying covariance matrix of the measurement noise process.  Three 
experiments are conducted in order to study the face tracking estimation problem using a 
conventional Kalman filter and the proposed fuzzy Kalman filter. 
 
 
Figure 3.8. Pictures of the experimental human-robot interactive system. 
 
3.5.1. Brief description of the experimental human-robot interactive system 
 Figure 3.8 depicts the pictures of the experimental human-robot interactive system, 
which is composed of a small scare personal computer with required input-output devices, a 
touch screen, an FPGA chip (Nios II EP1C12F324C8 device) from Altera, two DC 6V 
batteries and a human-robot interactive system. The system uses the Nios II developing 
embedded system based on the EP1C12F324C8 device in which the embedded processor 
(Nios II) is employed to control seven servo-motors. One of the main reasons to use the 
EP1C12F324C8 device is that it is a small size, low-cost, low power-consumption, 
programmable and full of computational ability chip with many communicational serial 
communication devices, such as RS-232 and wireless Network. 
 53
0 50 100 150 200 250 300
-10
-5
0
5
10
15
20
Time
E
rro
r
 
 
KF Error
FKF Error
 
Figure 3.10. Performance comparison of the KF and FKF estimated states. 
 
Simulation 2:  In the second simulation, the covariance matrix of the measurement 
noise becomes larger while comparing to Simulation 1, i.e., R =diag{60,60,60,60}. Figure 
3.11 depicts the true state and filter estimates of the face position. The standard deviation of 
the KF is 111.3735, while that of the proposed FKF is 67.9119. Comparison of  the 
simulation results of KF and FKF indicates that the FKF obtains better estimation 
performance and has a smaller standard deviation than the conventional KF does.  
Furthermore, from Figures 3.11 and 3.12, the KF would loss of tracking when the face 
position had an abrupt acceleration, but the FKF still gave a satisfactory performance to track 
the face position. Hence, it can be concluded that the FKF is capable of giving satisfactory 
tracking performance under the situation of abrupt acceleration and variable noise 
characteristics. 
 55
edge is shown in Figure 3.15. The oval model is then compared with the edging images by 
searching. This search procedure works for every point of the picture. The result in Figure 
3.16 shows the detection process using the oval model. After the previous method has been 
applied to determine a face-like shape, the feature extraction of human face by min-max 
analysis will facilitate the success of the face detection. Figure 3.17 shows the face and its x, 
y-profiles. In Figure 3.17, there is a valley at the place where eyes and mouth are in the 
y-profile. The position of the eyes and mouth can then be found in the recognized face.  
 
 
(a) 
 
(b) 
 
(c) 
 3.13. Experimental results of the fuzzy skin color adjuster. (a) NoFigure rmal light 
intensity.  (b) Stronger light inten . (c) Darker light intensity. 
 
sity
 57
invited to join the experiments. For speeding up the experimental procedure, the well-known 
LIBSVM [26] was adopted to do both experiments in which the trained faces are respectively 
shown in Figures 3.18 and 3.19. The procedure of the experiment is described in following 
steps： 
Step1: Use the Haar wavelet transformation to reduce the training samples dimension. 
Step2: Find out the face feature’s data of the training samples by principal component 
analysis (PCA).  
Step3: Use the Euclidean distance to filter out unnecessary training samples. 
Step4: Classify the face feature’s data to recognize the human face by the support vector 
machine. 
Step5: Show the result of the face identification method in Figure 3.20. 
 
Figure 3.18. Database of human faces. 
 
 
Figure 3.19. Haar faces. 
 59
Table 3.5. Data used for the face identification experiment 
Training samples 120 Experiment 2  
Test samples 120 
Training samples 240 Experiment 3 
Test samples 120 
 
Table 3.6. Experimental results of the proposed face identification experiment 2 
Method Item Experiment rate 
Recognition Rate 89% Euclidean distance 
Processing time  7 pictures/sec 
Recognition Rate 90% SVM 
Processing time 5.73 pictures/sec 
Recognition Rate 94% Euclidean distance 
and SVM Processing time 5.9 pictures/sec 
 
Table 3.7. Experimental results of the proposed face identification experiment 3 
Method Item Experiment Results 
Recognition Rate 90% Euclidean distance 
Processing time 6.9 pictures/sec 
Recognition Rate 92% Nonlinear SVM 
Processing time 5.5 pictures/sec 
Recognition Rate 95% Euclidean distance 
and nonlinear SVM Processing time 5.8 pictures/sec 
 
 
 
 61
 63
and then to control the motors to achieve face tracking. Through computer simulations and 
experimental results, the proposed face detection, identification together with the FKF has 
been shown to be capable of obtaining satisfactory performance of continuously tracking any 
moving face. 
 
  
 
Special efforts will be paid to investigate how to use two arms to do something that can must be  carried out 
using the dual arms; in particular, an interesting task called  coffee-making is investigated via two-armed 
cooperation. 
 
4.2. SYSTEMS DESIGN 
4.2.1 Mechatronic Design of the Robot 
As shown in Figure 4.1, the two-armed robot is equipped with a stereo vision camera, a small size 
personal computer with CPU of Intel Core2 Duo E6850 3.0GHz, an embedded PIV controller using a 
StratixII edition of Altera Nios II development kit, two 7 degrees-of-freedom arms, and a two-degrees-of-
freedom robotic head. Figure 4.2 displays the overall system structure of the two-armed robot.  
The stereo vision camera is employed to find the objects to be grasped by the robot via merging three-
dimensional image information between the head and the objects. The small size personal computer is 
utilized to execute the analytic inverse kinematics solver and path smoothing, while the embedded PIV 
controller is adopted to run sixteen PIV control laws in real time with a sampling period of one mini-
seconds.  
 
Figure 4.1 Physical structure of the two-armed robot.  
 65
 
Figure 4.3 Relationship between one joint and its neighboring joint. 
where cos , sinj j jc s jθ θ θ θ≡ ≡ ;the four quantities jθ , , , ja jd jα  are parameters associated with link j and 
joint j. The four parameters jα , , , and ja jd jθ  in Eq.(1) respectively denote  
 
Figure 4.4 Coordinate frames of the anthropomorphous dual arms. 
 
the length, link twist, link offset, and joint angle of link j. These parameters derive from specific aspects of 
the geometric relationship between two coordinate frames, as they will become obvious as below. Since the 
matrix jA  is a function of a single variable, it turns out that three of the aforementioned four quantities are 
 67
 
Figure 4.6 Skeleton of the arm joints and redundancy of the elbow joint. 
Step1: 
Given the posture (roll, yaw, and pitch) and the position of tool-point, the vector  from the wrist to the 
hand reference tool-point can be determined by (4.3), and as shown in Figure 4.5. 
hr
r
ˆ
h xr H lh= ⋅r                                                                               (4.3) 
where ˆ xH  denotes the unit oriental vector of forward finger and denotes the segment length. The posture 
is expressed as Eq.(4.4). 
hl
 , , , ( )  ( ) ( ) z x yψ ρ φψ ρ φ =Yaw Roll Pitch R R R                                           (4.4) 
The vector  is the final posture from the wrist to the hand reference tool-point can be determined by 
Eq.(4.5) 
'hr
r
 
Figure 4.7 Arm configuration. 
 
, , ,'h z x yr hrψ ρ φ= R R Rr r                                                                           (4.5) 
Then the position vector of the wrist can be calculated from Eq.(4.6), and as shown in Figure 4.6. 
     'WST END hr r r= −r r r                                                                                (4.6) 
 69
where ( )AR θ  is the rotation matrix by any axis, and ( , , )WST x y z
WST
rA A A A
r
r
= =r , cos( )C θ= , sin( )S θ= . Note 
that both vectors αˆ  and βˆ  are unit vectors expressed in Eq.(4.13) and Eq.(4.14). 
ˆ WST z
WST z
r sh
r sh
α ×= ×
r
r                                                                                 (4.13) 
ˆ ( )ˆ
ˆ ( )
WST WST WST z
WST WST WST z
r r r s
r r r s
h
h
αβ α
× × ×= =× × ×
r r r
r r r                                                         (4.14) 
where xsh , , denote respectively the unit vectors of in the shoulder frame. For this reason, the position 
of the elbow can then be obtained. 
ysh zsh
Step 4: 
The three angles of the shoulder joint are respectively represented by 1θ , 2θ  and 3θ . 1θ  and 2θ  can be 
obtained from Eq.(4.15) to (4.19). Therefore, with the vector r P , ,[ ,u E x P= , ]TE z,E y Pr , one obtains   
1 2 1( ) ( ) ( )
0
0
10
1
u
y x z
r
R R T Lθ θ
⎡ ⎤⎢ ⎥ ⎡ ⎤⎢ ⎥⋅ ⋅ − ⋅ = ⎢ ⎥⎢ ⎥ ⎣ ⎦⎢ ⎥⎢ ⎥⎣ ⎦
r
                                                             (4.15) 
which, after expansion, gives the three equalities in (4.16) 
1 2 1 ,
2 1 ,
1 2 1
sin cos
            sin
cos cos ,
E x
E y
E z
L P
L P
L P
θ θ
θ
θ θ
− =
=
− =
⎧⎪⎨⎪⎩
                                                                   (4.16) 
From Eq.(4.16), 1θ  can be computed by the atan2 function 
,
1
,
atan2( )E x
E z
P
P
θ −= −                                                                       (4.17) 
From Eq.(4.16), 2θ  is also obtained using the atan2 function,  
,
2
2 1
atan2( )
cos
E yP
L
θ θ=                                                                            (4.18) 
,
2
1
2 , 1
cos , 0
sin
cos , 0
E x
E z
P
L
L P
θ θθ
θ θ
⎧
1= ≠⎪ −⎨⎪ = − =⎩
                                                                    (4.19) 
Finally, 3θ  can be solved from Eq.(4.20). 
 71
1_1 1
5 2 4
3
1
0
( ) ( ) ( )
0 1
1 1
WST U
y z
2
f
r f
R T L A
f
θ −
⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢⎡ ⎤ ⎥⎢ ⎥⋅ − ⋅ = ⋅ =⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎣ ⎦ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣
r
⎦
                                       (4.27)  
Finally, 5θ  is solved using the atan2 function as in (4.28). 
2
5
1
atan2( )f
f
θ =                                                             (4.28) 
 
Figure 4.8 Configuration of arm for solver. 
Step 6: 
Given the  that is denoted in (4.29), as shown in Figure 4.8.   _WST Rr
r
_
ˆ
WST R WST yr r= + Hr r                         (4.29) 
The solution of 6θ  can be obtained from (4.30) to (4.31). 
_1
5 6
0
1
( )
0 1
1
WST R
x
r
A R θ
⎡ ⎤⎢ ⎥ ⎡ ⎤⎢ ⎥⋅ ⋅ = ⎢ ⎥⎢ ⎥ ⎣ ⎦⎢ ⎥⎣ ⎦
r
                                                          (4.30) 
and 
1
_1 1
6 5
3
0
1
( ) ( )  =
0 1
1 1
WST R
x
g
r
R A
g
θ − 2g
⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢⎡ ⎤ ⎥⎢ ⎥⋅ = ⋅ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢⎣ ⎦ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣
r
⎦
                                      (4.31) 
For sake of simplicity, (4.32) is obtained from (4.31). 
 73
 
Figure 4.9 Quadratic uniform B-spline curve. 
 
4.3 TROJECTORY PLANNING AND MOTION CONTROL 
In the beginning, the trajectory planning of the dual arms requires some desired points and uses a B-
Spline algorithm to make the trajectory point smooth. As Figure 4.9, given the via-points, the trajectory of 
end-effector can be generated by using B-spline function. The function is denoted from Eq.(4.39) and 
Eq.(4.40). 
1
2
2
3
1 1 0
( ) 1 2 2 0
1 2 1
T
T
traj
T
P
P
P
λ λ λ
⎡ ⎤⎡ ⎤ ⎢ ⎥⎢ ⎥⎡ ⎤= − ⎢ ⎥⎣ ⎦ ⎢ ⎥ ⎢ ⎥⎢ ⎥−⎣ ⎦ ⎣ ⎦
2
T TP P−                                             (4.39) 
,0 total
total
t t TTλ = ≤ ≤                                                                                (4.40) 
where , , 1P 2P
3 1
3
xP R∈  denote the three reference points of the B-spline curve;  is the starting point;  is 
the middle reference point;  is the ending point; 
1P 2P
3P λ  lies within the interval [0 . ,1]
Afterwards, the inverse kinematic method presented in Section III is then used to solve for the angles 
from the shoulder to the end effecter. After obtaining the angles, actual motion profiles of all joints are 
planned as trapezoid velocity trajectories and all the joints must start and terminate at the  
 
 75
4.4. HAND-WRITING 
4.4.1 Line Segment Tracing 
After thinning, the pixels of each line segment can be separated to three parts using the values of the 
function , where, the pixel at the location X  denotes the number of the neighboring eight pixels 
being unity. The physical meaning of the function  is interpreted as in Figure 4.11 When 
( )P X ( )P X
(P X ) ( ) 1P X = , 
the pixel means the terminal point; when P X( ) 2= , the pixel means the line point; when P X , the 
pixel means the cross point. 
( ) 3≥
 
0 0 0 0 0 1 0 0 1 
0 x 1 0 x 0 0 x 0 
0 0 0 0 0 1 1 0 1 
                 (a)                        (b)                                (c) 
Figure 4.11 (a) Terminal point. (b) Line point. (c) Cross point. 
 
Before the line segment tracing, the type of the point is determined by the value of . In order to 
find the start point and end point of the line segment, one have to generate new images from the origin 
images of words, calculate the types of the image pixels, and then store all the type values into 
corresponding pixels in these new images. Note that the new and original images have the same size.  
Figure 4.11 (b) shows examples of terminal points and cross points, after the thinning process; there are 
eight terminal points and five cross points. The scanning order is shown in Figure 4.12 (a), where 
( )P X
φ represents the oblique angle of the scanning procedure; in the case,  the value of φ  is set by . 30o
X
E1
E2
E3
E4
E5
E6
E7
E8
C1
C2 C3 C4
C5
Y
OX
Y
O
φ
 
(a)                         (b) 
Figure 4.12 (a) The origin image of a word.    (b)The thinning word of image with signed points. 
 77
 Center 
X
Y
O
X
Y
xsh
zsh
ysh
(
,
,
)
c
c
c
x
y
z
( ) ( )y yc x xcT w T w− ⋅ −
( ) ( ) ( ) ( )z c y c x c zT z T y T x R ϕ⋅ ⋅ ⋅
ycw
xcw
 
Figure 4.15 Procedure of coordinate transformation. 
 
4.4.2 Trajectory Planning of Words 
After obtaining the line segments of words, it is necessary to transform the coordinates of pixels in an 
image onto spatial coordinates of the robot working space. This can be done by multiplying several rotation 
matrices and translation matrices, as shown in Figure 4.15. Worthy of mention is that if the procedure that 
only recorded the points of line segments is used for hand-writing, the trajectory is discontinuous. To make 
such writing smooth, some auxiliary points needs to be added such that the trajectory of hand-writing is 
continuous from one line segment to another in order. The auxiliary trajectory is shown in Figure 4.16 The 
auxiliary lines are the pen-putting , pen-lifting, and motion locus. 
 
Figure 4.16 Example for trajectory of word. 
The starting point of pen-put is higher than the starting point of the line segment with a height of hδ , 
and the ending point of pen-lift is also higher than the end point of the line segment with a height of hδ . The 
end point of pen-lift is connected to the beginning point of pen-put of the next line segment. After the action 
 79
 
Figure 4.19 Front view on movements of executing the pick-place task . 
 
that the positions of the four balls are randomly given. To speed up the pick-place task, two arms are 
employed simultaneously; for example, Balls 1 and 2 are picked using the left arm, and Balls 3 and 4 are 
grasped by the right arm. All balls must be placed into their corresponding colorful boxes. If the position of 
a box is beyond the working space of either the right arm or the left arm, namely that the box position can 
be reached by the other arm, then the ball will be passed through one arm to the other. 
The position determination algorithm must be activated if the positions of balls and boxes are unknown. 
Otherwise, the object recognition method mentioned is only applied for color and shape detection of the 
balls and boxes. By finding the ratio between girth and area of an object, a ball will be detected with a 
higher ratio. On the other hand, by determining areas of objects, a box can be detected with a larger area. 
For the unknown position problem, the acquired object image must be shifted by moving the robotic head to 
be in the center of the image according to the position determination method. Since then, the coordinates of 
balls and boxes can be easily obtained from the position determination method.  
 
4.5.2 Trajectory Generation for Pick-Place 
After successful detection of colors and positions of four balls and boxes, the pick-place task will be 
executed from Ball 1 to Ball 4 by means of the dual arms.  The balls are picked and then put into 
corresponding boxes by the two arms, until four balls are placed. If the distance between the ball and the 
corresponding box is too far, then both arms are cooperated together to accomplish the task. 
The motion trajectories of the pick-place task are respectively shown in Figure 4.17. To pick and place 
each ball successfully, the motion path of one arm should have four points. The first point is the starting 
point of picking; the second point is the point of picking; the third point is the start point for placing; the last 
point is the place point for placement. As can be seen, for example, the first point is above Ball 1, and its 
position is higher than Ball 1. The third point is determined by both the first point and the fourth point. The 
 81
where , , 1P 2P
3 1
3
xP R∈  denote the three reference points of the B-spline curve;  is the starting point;  is 
the middle reference point;  is the ending point; 
1P 2P
3P λ  lies within the interval [0 . ,1]
 
W
0
( 89.07 )mm
W
=
0( 63.01 )mmH =
1θ 2θ
1objx
ˆ xH
ˆ yH
61.13ϕ= D
1,
ˆ
obj xH
1,
ˆ
obj yH
1,
ˆ
obj xH
1,
ˆ
obj yH
 
Figure 4.20 Object-level coordinates of the right hand. 
1,
ˆ R
obj xH
1,
ˆ R
obj yH
1,
ˆ L
obj xH
1,
ˆ L
obj yH
 
Figure 4.21 Cooperation of passing a ball from one hand to another. 
ˆ
xHˆ
yH
hl
ˆ
zH
hr
G
ˆ
xH
ˆ
yH
hr
G
ˆ
zH
ˆ( , , )gx gy gz Hh h h
     
ˆ
xH
ˆ
yH
hr
G
ˆ
zH
ˆ( , , )gx gy gz Hh h h
1,
ˆ
obj xH
1,
ˆ
obj yH
1objx
( 132.5 )x mmL =
1
ˆ1 1( ,0, ) objobj obj Hx z
 
(a)                                          (b) 
Figure 4.22 (a) Modification of the hand coordinates. (b) Relation between  and  space. Hˆ 1ˆ objH
 
 83
22
1 0 0 107
0 1 0 8 0
( )
0 0 1 0
1 0 0 0 1 1
gx obj
gy
z
gz obj
h x
h
R
h z
ϕ
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥−⎢ ⎥ ⎢ ⎥ ⎢ ⎥= ⋅⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
⋅
,
                                                         (4.41) 
2,
2,
ˆ ˆˆ( , ) ,  ,
1 1
obj
A obj z
H HR H ϕ⎡ ⎤ ⎡ ⎤= − ⋅ =⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
      x y z                                                 (4.42) 
Note that the angle between coordinates of and  is . 2ˆ objH Hˆ ( 45 )ϕ = o
 
ˆ xH
ˆ yH
2,
ˆ
obj yH
2,
ˆ
obj xH
ˆ ( : )(107, 8,0)H unit mm−
22 2
( ,0, )
objHobj obj
x z
hr
G
ϕ
2,obj xH
 
Figure 4.23. Transformation of coordinate and Grabbing diagram. 
 
Consequently, such a method is easy in order to let hand approach the object by reducing 2objx  until the 
object and the palm are mutually tangent. An alternative is to modify  so as to let the hand approach the 
object.  
2objz
Figure 4.25 illustrates the still pictures for object grabbing. In the start-up phase, the end-effector of one 
arm starts to approach with the reference point ob 2jx . By reducing ob 2jx , the hand will approach the object by 
continuously calculating the posture of the arm by the analytic IK solver. Note that the reduction of 2objx  
can be made by point-to-point movement with a trapezoid velocity profile. When the palm and the object 
are mutually tangent at the stop point, the fingers are allowed to catch the object. 
4.6.2 Lid Rotation  
The lid opening is lid rotation. The reference points for lid rotation are shown in Figure 4.24 (a), where 
 denotes the diameter of the cylindrical bottle; objD 1Holdh  and 1Holdh  are denoted as the height of the holding  
point of the  hand; openθ  is the angle for one step of lid opening; openn θ⋅  is the total angles for open. In other 
 85
,
ˆ ˆˆ( , ) ,  =x,y,z
1 1
p
A z
H HR H θ⎡ ⎤ ⎡ ⎤= ⋅⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
                                                       (4.47) 
In order to achieve the smooth motion, the rotating angle is separated to m steps as in  Eq.(4.48). Then, 
the reference coordinates of the end-effector are computed by Eq.(4.49). 
pour pour mθ θ= ⋅                                                                         (4.48) 
,
,
ˆ ˆˆ( , ) ,  , ,
1 1
new
A p x pour
H HR H θ⎡ ⎤ ⎡ ⎤=⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
       x y z=                                      (4.49) 
4.6.4 Grinding 
After pouring coffee beans into the grinder, one can start grinding the beans by rotating the shaft of the 
grinder. 1Holdh  represents the height for the holding point. 2Holdh  denotes the height for another holding point. 
 is the height of the plane for rotation.rubh rubR  is the radius of the circle for rotation. The relations are shown 
in Figure 4.25. The posture of grinding is shown in Figure 4.24(a). Let the left hand hold the bottom of the 
grinder, and then let right hand rotate the shaft of the grinder with the position ( , , )xc yc zcrub rub rub sh  being 
the center of the circle and rubR  is the radius in grinding plane. The grinding plane parallels the sh shx y−  
plane, as shown in Figure 4.25(b). 
  
ˆ xHˆ yH
,h pr
G hr
Gpr
G
pr
G
hr
G,h pr
G
ˆ yH ˆ xH
,ˆ p yH
,ˆ p xH,ˆ p xH
,ˆ p yH
pourθ pourθ
 
(a)                               (b)                                                    (c)                     (d) 
Figure 4.24 (a) Relationship of coordinates between two hands (b) Cup parameters. (c),(d) Kinds of rotation 
for pouring. 
rubR
rubh
2Holdh
1Holdh
     
rubR
( , , )shx y z
ˆrubx
ˆruby
rubR
ˆshx
ˆshy
( , , )xc yc zc shrub rub rub
 
                               (a)                          (b)                       (c) 
Figure 4.25 (a) Grinder ; (b) Posture of grinding ; (c) Trajectory of rotating point. 
 87
 
Figure 4.26. Simulation of the 7-DOF anthropomorphous arm. 
 
Figure 4.27. Double circular motions. 
 
Figure 4.28 Angular position errors of all the seven joints in the left arm performing the circular movement. 
 
trajectory successfully. This experiment was designed to generate several via-points constituting a circle for 
each arm, to transfer them to the corresponding joint angles via the analytic IK solver, and, finally, and to 
accomplish out the joint tracking control for the seven joints using the PIV control.  
 89
 00:02 
Figure 4.31 Hand-writing process of the first Chinese word: “尖”. 
 
Figure 4.32. Execution of the pick-place task without cooperation. 
 
Figure 4.33 Execution of the pick-place task using the two arms with ball passing cooperation. 
 
4.7.3 Pick-Place Experiment 
In this subsection, the third experimental result was conducted to show the performance of the proposed 
pick-place algorithm using two-armed cooperation. This experiment is aimed to generate the trajectories of 
each colorful ball in order to put it into its corresponding box. The coordinates of balls and boxes are known 
via color detection. Figure 4.32 depicts the still images of the pick-place process by using the two arms 
without passing ball. Figure 4.33 shows the still images of the pick-place task by using two-arm cooperation 
 91
 93
DOFs arms, a 2-DOFs robotic head with a webcam and a laser scanner, a personal computer, a Nios II 
Development Board and so on. The main results of the work are summarized as follows. 
First, seven-degrees-of-freedom dual arms have been constructed by using a mechatronic design 
approach. The modular mechatronic devices of the two arms have been successfully established by motors, 
motor driving modules, and FPGA-based motion controller. Therefore, the 2-DOFs robotic head system is 
successfully built with a webcam and a laser scanner to improve the shortcoming that only one laser scanner 
is not enough to find the location of an object to be grasped. The three dimensional position of the object 
can be obtained by using the distance and the coordinates with respect to the world frame; the combination 
with webcam and laser has successfully determined the distance between the object and the webcam. The 
experiment results have shown that the estimated positions between the robotic head system and the object 
have a maximum error of less than 2 cm. 
Second, the analytic inverse kinematics has been proposed to easily find the final joint angles of all 
seven joints in both seven-degrees-of-freedom arms. Notice that, the analytic IK solver needs some basic 
data including positions and orientations of both end-effectors, and elbow swing angle. The PIV control 
method has been utilized to achieve position control of all the joints because it is easy to find the parameters 
from velocity loop and position loop respectively. Through experimental results, the proposed analytic IK 
solver has been shown efficient in finding the joint angles of the two arms so that the desired destination can 
be reached with the PIV control. 
Third, the task-based motion planning for hand-writing and pick-place tasks have been proposed. The 
algorithm for motion planning of the hand-writing task has been proposed and tested, and the experimental 
results have verified that the proposed algorithm worked well. By combing with the use of image processing 
and the robotic head, the presented pick-place algorithm without obstacle collision has successfully 
achieved the desired mission. 
Fourth, the conceptual cooperation of the dual arms and its application to an example of soaking coffee 
has been investigated by a task decomposition method. The method has been illustrated, and some particular 
efforts have been paid to description of sub-tasks with cooperation. The whole process of soaking coffee has 
been investigated as well, and its experimental results have been successfully conducted to show the 
effectiveness of the proposed task execution with two-armed cooperation. 
 
 
Figure 5.1 A grid-based environment and planned path representation. 
 95
  
(a)   (b) 
Figure 5.3 Eight situations of moving directions. 
 
Figure 5.4. Relationship between the moving direction and the shielding. 
 
After the moving direction has been determined, the following operations will be applied with the starting 
point from the left-bottom, and the goal placed on the right-upper side. Figure 5.4 shows the fundamental 
representations of the operation direction, which can be regarded as the rotation of the map during the 
process of each block searching operations. 
 97
 Figure 5.6. Flow chart of the dynamic map shielding. 
 
 
(a)              (b)    (c) 
Figure 5.7.  Illustration of searching point movement. 
 
3) Searching Point (Pt) Movemnt 
First of all, considering the map in Figure 5.7 (a), the blue region represents the obstacles, and the white 
region denotes free area. Po stands for starting point, and Pt is the movable testing point.  
At the beginning, the testing point will move along the diagonal vector. Next, use Po and Pt as the 
diagonal entries of the two vertices, and the moving point Pt is stopped till the expanded region touches any 
 99
margin, it will go back to the previous point; then continue to move toward another direction, as 
Figure 5.9 shows. 
Step3. Keep searching until it reaches other margins, as Figure 5.10 (a) shows, then the final position of Pt is 
settled down. With both reached margins, the searching block is constructed as a rectangle area 
betweens Po and Pt. After that, record the Po as searched path point, and set the Pt point as new 
starting point. 
 
(a)                          (b) 
Figure 5.10. Illustration of Step 3. 
 
 
 
 
For each particle 
Do 
Initialize particle 
End 
Do 
For each particle 
Calculate fitness value 
If the fitness value is better than the best one (pBest) 
set current value as the new pBest 
End 
Choose the particle with the best fitness value of all as the gBest 
For each particle { 
Calculate particle velocity according equation (5.1) 
Update particle position according equation (5.2) 
End 
While maximum iterations or minimum error criteria is not attained 
Figure 5.11..A typical pseudo code of PSO algorithm. 
 101
Note that the inertia weight ω
 
is suggested by some researchers to lie within the interval 0,1.4⎡ ⎤⎢ ⎥⎣ ⎦ , or the 
interval   so that the algorithm (5.3) has a much faster convergence than the algorithm (5.1) does. 
In addition, Clerc and Kennedy [15] offered another idea to give a constriction factor χ  which may ensure 
fast convergence, and proposed the new velocity of a particle being manipulated by the following equation 
(5.4) 
0.8,1.2ω ⎡∈ ⎢⎣ ⎤⎥⎦
)( )
( ) ( ) ( ) ( )( )
( ) ( ) ( )(12      
1
2
+ d rand
=
+d rand
χ
⎡ ⎤⋅ ⋅⎢ ⎥⎢ ⎥⎢ ⎥⋅ ⋅⎣ ⎦
id id id
id
gd id
v t p t - x t
v t+1
p t - x t                                      (5.4) 
where a typical value of χ is given by 
 
2
2
2
kχ
φ φ φ
=
− − − 4                                                                                 (5.5) 
and  
1 2
4, 1d d and kφ = + > =  
Moreover, Clerc and Kennedy [15] further analyzed the models and techniques to speed up the 
convergence properties of PSO by fine-tuning the PSO parameters. Generally speaking,  and 
 are often used. Figure 5.13 shows a simple test of the mentioned PSO algorithm. The path is 
composed of ten searched points, which stands for the gBest points in each searching with speed limitation 
of each particle equals to 1 m/s. 
1 2
2.05d d= =
0.73χ =
 
Figure 5.13. Illustration of searched points using the PSO search. 
 
 103
planning problem, the reader is referred to [19]. Figure 5.15 shows the simulation results of the used local 
path planner for dynamic obstacle avoidance.  
 
Figure 5.15. Dynamic obstacle avoidance. 
 
 
Figure 5.16. Simulation results of MPSO for Case 1. 
 
Figure 5.17.Simulation results of MPSO for Case 2. 
 105
 
(a)                                         (b)  
Figure 5.20. (a) Illustration of experimental environment; (b)Experimental result of MPSO for global path 
planning. 
 
Experimental setup for the proposed MPSO approach is shown in Figure 5.19, where the robot gets 
started from the initial position(2  and moves towards the goal position set by(7 . Figure 5.20(a) 
illustrates the experimental environment and Figure 5.20(b) shows the tracking results with the intelligent 
adaptive controller, where both errors in x and y coordinates are (0 , respectively. Note that, in Fig. 
20 (b), the blue line denotes the planned path and the red one represents the experimental path. The 
experimental results illustrate that the proposed MPSO method can be capable of finding a feasible smooth 
path for trajectory tracking. 
m,2m) m,7 m)
.2m,0.4 m)
 
5. 5 CHAPTER CONCLUSIONS 
This chapter has developed an MPSO algorithm using grid-based map and block area searching for global 
path planning of mobile service robots.  Through simulations and experimental results, the proposed MPSO 
algorithm has been shown useful in achieving global path planning. The dynamic local path planner based on 
the bubble band techniques has been used to avoid any static and dynamic obstacle between two nearby via-
points found by the MPSO approach. An interesting topic for future research would be to complete the 
realization of dynamic local path planner with the proposed global path planner in order to achieve an entire 
navigation system for mobile robots. 
 
 107
 
 
109
a certain goal in a given situation; the decision range from what path to take to what 
information on the environment to use. In the chapter, cognition is concerned with 
how to recognize the face direction of a human and how to plan a socially acceptable 
path in the presence of the human. Once the path has been determined, the robot not 
only must execute its motion control to exactly follow the planned path, but also must 
perform obstacle avoidance for any unexpected or unpredictable obstacle. By 
including the recognized human face and the planned acceptable path, the 
human-aware navigation system can safely navigate its working space and meet the 
human’s preferences and psychological needs. 
The remainder of this chapter is constructed as follows. Section 6.2 introduces 
an operational scenario for the robot navigating in a dynamic environment which 
humans work in and walk through. In Section 6.3, the skin color detection method is 
investigated. Section 6.4 describes the obstacle avoidance with the laser scanner. 
Section 6.5 describes how to plan a socially acceptable path in the presence of the 
human and how to device an obstacle avoidance algorithm, thereby establishing a 
human-aware navigation system for the robot. Experimental results and discussion are 
shown in Section 6.6, and the concluding remarks of the chapter are stated in Section 
6.7. 
 
6.2 Operational Scenario 
For the purposes of mission execution and other service provisions (such as 
security patrolling, reception, logistics, and etc.), mobile robots need to implement 
their functions properly and reliably such that desired services can be appropriately 
given. With the robotic head module, our two-wheeled self-balancing robot is able to 
recognize the face direction of a human and then plan a socially acceptable path. In 
addition, the robot also needs to completely plan the path for mission execution 
6.3 Skin Color Detection 
For human-aware navigation, human skin color has been used to be an effective 
feature for detecting human face. The system used the YCbCr color model, as Figure 
6.3 shows, and the reason for using YCbCr model is that the human eye is less 
sensitive to chrominance than luminance and the color of skin in YCbCr color space 
has a good effect on skin color separation. Y' is the light component and Cb and Cr 
are the blue and red chroma components. 
 
Figure 6.3. The YCbCr color model. 
 
However, the use of the color space transform still cannot eliminate all of the 
effects from the light intensity. In order to reduce the effect, the proposed head system 
use fuzzy logics to adjust the values of the Y parameter in the YCbCr color space for 
attaining better performance. In doing so, consider an M N×  image, the average of 
the gray level of the pixels is defined by Eq. 6.1 and the output L of the fuzzy tuner is 
used to tune the parameter Y. 
                       1
M N
i j
h
M N
= × ∑∑ pixels
L×
                       (6.1) 
Eq. 6.2 is used to approximate the converted values of the RGB color 
information into the YCbCr components. 
                           (6.2) 
( )0.2989 0.5866 0.1145
0.7132 ( )
0.5647 ( )
Y R G B
Cr R Y
Cb B Y
= × + × + ×⎧⎪ = × −⎨⎪ = × −⎩
 
 
111
1d5d
3d
4d 2d
 
Figure 6.5. Distances in five regions. 
 
6.5 Human-Aware Navigation Generation 
This section is devoted to consider a human-aware path for the robot that 
achieves some desired missions needed in our daily life in indoor environments. Then, 
it is inevitable for the robot to plan human-aware path. To plan the path, we consider 
two criteria, the motion planning safety and visibility; each criterion is represented by 
a set of numerical values stored in a 2-D grid map. A criterion grid G is defined by: 
G = ( , ). ,n pM iH
,n pM  is a matrix containing  cells represented the cost of the coordinate grid in 
the environment;  denotes the localization of a person working in the 
environment by using a laser scanner and a CCD camera mounted on the head module. 
In the following, a human state will be defined which means a number of cost 
parameters. A state is defined by: 
n p×
iH
State = (Face Direction, Range). 
In the follows, more explanations will be given to the structure of the “safety” and 
“visibility” criteria. 
 
6.5.1 Safety and Visibility Criterion 
 
 
113
The section aims at illustrating the main ideas from the safety and visibility 
the motion command to the robot. 
 
 
115
    os
Where the two weights  and  ed from heuristic experience. As 
Cost merged(x, y) = 1w * Cost safety(x, y) + 2w * C t visibility(x, y)     (6.4) 
are obtain1w 2w
shown in Figure 6.7, there is a person standing in an indoor space; in this case, the 
path was planned in front of the person using eq. 6.6. As shown in Figure 6.8 the case 
is about two persons are looking at each other, the path will transverse between the 
two persons according to the visibility and safety criteria. 
 
Figure 6.7. Illustration of the path planning for one standing person. 
 
Figure 6.8. Illustration of the path planning for two standing persons. 
 
 
117
When the robot approaches a human, the fuzzy controller adjusts the weight so 
that the path will significantly detour the obstacle. Eventually, the robot reached the 
destination by the fuzzy controller. Figure 6.9 describes the proposed human-aware 
navigation structure. 
 
6.6 Overall Experiments and Discussion 
The following two experiments aim to investigate whether the robot can easily 
be steered to track the path using the proposed human-aware navigation by first using 
face detection and fuzzy obstacle avoidance method. Before doing both experiments, 
simulations must be conducted to study the feasibility of the proposed fuzzy weight 
decision controller. Afterwards, the following subsequent experiments were 
performed to study the applicability of the proposed face detection and fuzzy control 
method. Figure 6.10 shows the experimental pictures of human-aware navigation. 
Figure 6.11 shows two experimental data of the robot during the human-aware 
experiment. Figure 6.12 displays the interface of the proposed navigation system. 
Through experimental results, the proposed face detection and fuzzy obstacle 
avoidance method have been shown to be effective in not only tracking human-aware 
trajectories but also obstacle avoidance. Figure 6.13 shows the experimental pictures 
of human-aware navigation to pass through two persons. Figure 6.14 shows two 
experimental data of the robot passing through two persons. 
 
Figure 6.11(b). Experimental data of the human-aware navigation. 
 
 
Figure 6.12. Experimental interface of the human-aware navigation. 
 
 
 
 
119
 
 
121
6.7 Chapter Conclusions 
This chapter has presented a human-aware navigation method by combining a 
skin color detection and an obstacle-avoidance method. The operational scenario has 
been presented to show how the robot interacts with humans. The human-aware 
navigation generator has been proposed to guide the robot by taking into account 
“safety” and “visibility” criteria. Both obstacle-avoidance method and face-direction 
detection using the fuzzy weight decision method (WDM) has been presented to 
achieve a human-aware navigation path. Through experimental results, the proposed 
human-aware navigation method has been shown capable of achieving the mission 
execution purpose. 
 
interaction with two cameras. After the use of the cameras to acquire image 
information, the proposed human face detection and the fuzzy Kalman filter have 
been successfully applied to predict the next-step position of the human face in 
advance, and then to control the motors to achieve face tracking. Through 
computer simulations and experimental results, the proposed face detection, 
identification together with the FKF has been shown to be capable of obtaining 
satisfactory performance of continuously tracking any moving face.  
3. This third part has presented techniques and methodologies for system design, 
trajectory planning, motion control, motion planning, two task executions and 
cooperation of a two-armed robot with two 7-DOF arms. The modular design 
concept and the system-on-a-programmable-chip (SoPC) technology are used to 
implement anthropomorphous two-armed robot with two 
seven-degrees-of-freedom arms. The trajectory planning regarding robot’s 
postures is achieved by first finding several via-postures and then connecting them 
using the B-spline curve approach. Given robot’s posture, an analytical inverse 
kinematic solver is proposed to find a set of seven joint angles with respect to 
each selected posture.  Trapezoidal velocity profiles are employed to obtain 
trajectories of all joints simultaneous. To achieve point-to-point movement, the 
PIV joint controllers are presented. Two illustrative tasks, hand-writing and 
pick-place, are employed to show the applicability of the dual arms To 
demonstrate how the dual arms work together, we present a conceptual 
cooperation of the dual arms and its application to coffee making. The 
performance and merits of the proposed methods are well exemplified by 
conducting four experiments on the constructed robot.  
4. An MPSO algorithm using grid-based map and block area searching has been 
proposed for global path planning of mobile service robots, and a elastic band 
 123
 125
using a four-wheeled omnidirectional mobile platform; however, four-wheeled 
omnidirectional mobile platform could not move as fast as the self-balancing 
two-wheeled mobile platform.   Second, if the two-camera vision system is replaced 
by a commercial, inexpensive Kinect system, then the depth information and object 
recognition can be processed efficiently. The use of the Kinect system could speed up 
or facilitate the execution speed of visual servoing, or the algorithm of integrating arm 
control, position control and force control. Third, an effective motion controller 
should be developed for the human-like symbiotic robot which performs tasks at one 
place and then moves and performs other tasks at another place.  
 [9] A. Salerno, and J. Angeles, “The control of semi-autonomous two-wheeled 
robots undergoing large payload-variations,” in Proc. ICRA’04, vol.2, 
pp.1740-1745, Apr 26-May 1, 2004. 
[10] C. C. Tsai, C. K. Chan, and Y. H. Fan, “Planned Navigation of a self-balancing 
autonomous service robot,” International Conference on Advanced Robots and 
its Social Impacts, Taipei, Taiwan, Aug. 2008. 
[11] http://www.hitachi.com/rd/research/robotics/emiew2_01.html 
[12] http://blog.pcnews.ro/2008/06/19/automated-music-personality/, July, 2011. 
[13] http://www.segway.com/en-v/ 
[14] Y.Hosoda, S. Egawa, J. Tamamoto, K. Yamamoto, R.Nakamura and M.Togami 
“Basic design of human-symbiotic robot EMIEW,” in Pro. IEEE/RSJ 
International Conference on Intelligent Robots and Systems, Beijing, China, 
pp.5079-5084, Oct. 9 - 15, 2006. 
[15] S.C. Lin, System design, modeling and control of self-balancing human 
transportation vehicles, D.S. Thesis, Department of Electrical Engineering, 
National Chung Hsing University, Taichung, Taiwan, July 2008. 
[16] M. Sasaki, N. Yanagihara, O. Matsumoto, and K. Komoriya, “Steering control of 
the personal riding-type wheeled mobile platform (PMP),” in Proc. 2005 IEEE 
International Conference on Intelligent Robots and Systems, pp.1697-1702, 2005.  
[17] F. Grasser, A.D’Arrigo, and S. Colombi, “JOE: A Mobile, Inverted Pendulum,” 
IEEE Trans. Industrial Electronics, vol.49, no.1, pp.107-114, February 2002. 
[18] K. Pathak, J. Franch, and S. K. Agrawal, “Velocity and position control of a 
self-balancing two-wheeled robot by partial feedback linearization,” IEEE 
Transactions on Robotics, vol.21, no.3, pp.505-513, June 2005. 
[19] A. Salerno, and J. Angeles, “The control of semi-autonomous two-wheeled 
robots undergoing large payload-variations,” in Proc. ICRA’04, vol.2, 
pp.1740-1745, Apr 26-May 1, 2004. 
[20] Y.-S. Ha, S. Yuta, “Trajectory tracking control for navigation of the inverse 
pendulum type self-contained mobile robot,” Robotics and Autonomous Systems, 
vol.17, pp. 65-80, 1996. 
[21] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon “A human aware mobile 
  127
593-598, July 2005. 
[33] W. Wang, X.D. Liu and J.Q. Yi, “Structure design of two types of sliding-mode 
controllers for a class of under-actuated mechanical systems,” IET Proceeding of 
Control Theory and Applications, vol.1, no.1, pp. 163-172, Jan. 2007. 
[34]  D.W.C. Ho, P.A. Zhang and J. Xu, “Fuzzy wavelet networks for function 
learning,” IEEE Trans. on Fuzzy Systems, vol. 9, no.1, pp.200-211, Feb. 2001. 
[35] C.K. Lin, “Nonsingular terminal sliding mode control of robot manipulators 
using fuzzy wavelet networks,” IEEE Trans. on Fuzzy Systems, vol. 14, no.6, 
pp.849-859, Dec. 2006. 
 [36] A.Albiol, L. Torres, E.J. Delp, “Optimum color spaces for skin detection,” 
Politechnic University of Valencia Spain, vol.1, pp.122-124, 2001. 
[37] J. Pineau, M. Montemerlo, M. Pollack, N. Roy, S. Thrun, “Towards robotic 
assistant in nursing homes：Challenges and results,” Robotics and Autonomous 
System, vol. 42, pp. 271-281, 2003. 
[38] R. C. Arkin, M. Fujita, T. Takagi, R. Hasegawa, “An ethological and emotional 
basis for human-robot interaction,” Robotics and Autonomous System, vol. 42, pp. 
191-201, 2003. 
[39] C. Y. Tsai, K. T. Song, X. Dutoit, H. V. Brussel, M. Nuttin, “Robust visual 
tracking control system of a mobile robot based on a dual-Jacobian visual 
interaction model,” Robotics and Autonomous System, vol. 57 pp. 652-664, 2009. 
[40] M. Turk and A. Pentland, “ Eigenfaces for recognition,” Journal of Cognitive 
Neuro-science, vol.3, no.1, pp.71-86, 1991. 
[41] S. H. Jeng, H. Y. Mark Liao, C. C. Han, M. Y. Chern, and Y. T. Liu, “Facial 
feature detection using geometrical face model: an efficient approach,” Pattern 
Recognition, vol.31, no.3, pp.273-282, 1998.  
[42] K. Sobottka and I. Pitas, “Extraction of facial regions and features using color 
and shape information,” in Proc. 13th International Conference on Pattern 
Recognition, pp.421-425, Vienna, Austria, Aug. 1996.  
[43] H. Wu, Q. Chen, and M. Yachida, “A fuzzy-theory-based face detector,” in Proc. 
13th International Conference on Pattern Recognition, Vienna, Austria, 
pp.406-410, Aug. 1996.  
[44] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back. “Face recognition: A 
convolutional neural network approach.” IEEE Trans. Neural Networks, vol.8, 
pp.98–113, 1997. 
  129
[56] K. Hirai, M. Hirose, Y. Haikawa, and T. Takenaka,“The development of honda 
humanoid robot,” in Proc. of IEEE International Conference on Robotics and 
Automation, 1998, pp. 1321-1326. 
[57] Y. Sakagami, R. Watanabe, C. Aoyama, S. Matsunaga, N. Higaki, and K. 
Fujimura, “The intelligent ASIMO: System overview and integration,” in Proc. of 
IEEE/RSJ International Conference on Intelligent Robots and Systems, 2002, pp. 
2478–2483. 
[58] I. W. Park, J. Y. Kim, J. Lee, and J. H. Oh, “Mechanical Design of Humanoid 
Robot Platform KHR-3 (KAIST Humanoid Robot – 3: HUBO),” in Proc. 
IEEE-RAS Int. Conference on Humanoid Robots, pp. 321-326, 2005. 
[59] T. Morita, H. Iwata, and S. Sugano, “Development of human symbiotic robot: 
WENDY,” in Proc. of IEEE International Conference on Robotics and 
Automation, 1999, pp. 3183–3188. 
[60] H.Iwata, S.Kobashi, T.Aono, T.Kobayashi and S.Sugano, “Development of 
4-DOF Anthropomorphic Tactile Interaction Manipulator with Passive Joint,” 
Journal of Robotics and Mechatronics, 2007. 
[61] K. Kaneko, K. Harada, F. Kanehiro, “Humanoid Robot HRP-3,” in Proc. of 
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp.2471 – 
2478, 2008. 
[62] K. W. Chin, Closed-form and generalized inverse kinematic solutions for 
animating the human articulated structure, Bachelor's Thesis in Computer 
Science, Curtin University of Technology, 1996. 
[63]J. Korein, Geometric investigation of reach, Ph.D. thesis, University of 
Pennsylvania, 1985. 
  131
on Automation Science and Engineering, vol.7, no.2, pp.383-392, 2010.   
[74] M. Clerc and J. Kennedy, “The particle swarm—Explosion, stability, and 
convergence in a multidimensional complex space,” IEEE Trans. Evol.Comput., 
vol. 6, no. 1, pp. 58–73, Feb. 2002. 
[75] S.Y. Ho, H.S. Lin, W.H. Liauh, and S.J. Ho, “OPSO: Orthogonal Particle Swarm 
Optimization and Its Application to Task Assignment Problems”, IEEE 
Transactions on Systems, Man, and Cybernetics, Part A, vol. 38, no. 2, March 
2008. 
[76] S. Y. Ho, L. S. Shu, and J. H. Chen, “Intelligent Evolutionary Algorithms for 
Large Parameter Optimization Problems”, IEEE Transactions on Evolutionary 
Computation,vol.8, no.6, December, 2004. 
[77] H. G. Zhu, C. W. Zheng, X. H.Hu and X. Li, “Path Planner for Unmanned Aerial 
Vehicles Based on Modified PSO Algorithm”, IEEE International Conference on 
Information and Automation, Zhangjiajie, China ,June 20 -23, 2008. 
[78] C. T. Lee,  Trajectory planning and adaptive trajectory tracking control for a 
small scale autonomous helicopter,  Ph.D Dissertation, Department of Electrical 
Engineering, National Chung Hsing University, Taichung, Taiwan, ROC, July 
2010. 
 [79] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon “A human aware 
mobile robot motion planner,” IEEE Transactions on Robotics, vol.23, no.5, 
pp.874-883, October 2007. 
[80] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon “A mobile robot that 
performs human acceptable motions,” in Pro. IEEE/RSJ International Conference 
on Intelligent Robots and Systems, pp.1811-1816, October 2006. 
[81] E. A. Sisbot, A. Clodic, L. F. Marin-U., M. Fontmarty, L. Brethes, and R. Alami, 
“Implementing a human-aware robot system,” IEEE International Symposium on 
  133
附錄一、計畫成果自評 
(1)與原計畫相符程度及達成預期目標情況： 
  (i) 第一年計劃已成功地建立一具有人臉識別與臉部表情識別為人機互動介面
的兩輪自平衡智慧型機器人運動平台。此機器人系統分兩大部分:運動導航與
人機介面。在人臉識別與臉部表情識別為人機介面系統部分，人臉識別與情感
識別的演算法是採用Haar小波轉換、主成分分析(PCA)和支持向量機(SVM)等方
法組成。而在運動控制部分，本機器人的移動必須是依據該運動平台的非線性
動態模型，設計滑動模式速度，偏轉速度等兩控制器以及運動軌跡產生器，最
後發展出同時可軌跡追蹤與點對點運動的控制法則。目前已完成DSP嵌入式運
動控制模組製以及SoPC人機介面模組研製，並成功應用於兩輪自平衡智慧型機
器人測試平台上。實驗結果亦驗證機器人的運動控制、辨別人臉與臉部表情以
及考慮有人存在的導航。本計畫目前已完成第一年計劃之既定項目: 自平衡運
動平台控制與人機互動介面等兩大模組，超音波定位模組已完成，但嗅覺感測
器之研製未完成，因設備太貴無法購買。 
 (ii) 第二年計劃已成功完成半人型雙手臂機器人上身之雙手臂與手指之研製，
同時開發雙手合作協作法則，進行運動軌跡規劃、雙手臂協調控制以及其嵌入
式系統晶片之設計。該機器人包含雙七個自由度的手臂與兩個自由度的立體視
覺鏡頭。機構模組化、馬達驅動控制模組、PIV 控制與SoPC技術被用來完成
實現擬人型機器雙手臂控制模組。為了達到點對點的平滑運動，PIV控制、分
析式的逆向運動學與B-Spline 曲線平滑法被提出來，求得各軸的角度與角度平
滑化。本研究亦設計兩個任務型的運動規劃演算法：中文的寫字與色球的取
放，並使用PIV控制法則與分析式IK (Inverse Kinematics)來完成運動規劃控
制。為了說明如何讓雙眼與雙手一起工作，一個概念式的合作方法被提出，並
應用在泡咖啡範例中。目前已完成雙七個自由度的手臂、雙自由度的機器頭研
製，以及SoPC嵌入式運動控制模組製，並成功應用於三項有趣的雙手合作任
 137
 139
 
(3) 主要發現及其他相關價值：本計畫主要發現或貢獻有五項: 第一項為以該運
動平台的非線性動態模型，設計滑動模式速度，偏轉速度等兩控制器以及運動軌
跡產生器，可達成遠端遙控與軌跡追蹤的功能；第二項為發展人機互動界面系
統；第三項為因研製該雙手臂機器人測試平台所衍生的技術，如感測、致動、運
動控制、系統整合設計與 SoPC 嵌入式系統晶片等關鍵零主件與系統技術；第四
項為以該雙手臂機器人所衍生的路徑規畫與手眼合作的概念與技術。第五項為人
機介面系統、兩輪自平衡智慧型機器人運動平台以及雙手臂系統的整合，並發展
考慮有人存在時的導航方式，用以達成防碰撞以及符合人們心理與安全需要的運
動導航功能。本計畫之主要貢獻技術可提昇一雙手臂機器人的運動控制功能，可
提昇該機器人系統的手眼並用的功能。另外，所研發的人機介面演算法、運動平
台控制器，運動導航法則、手眼並用演算法，已可開發成系統 IC 晶片。 
 
期日）下午13:00-15:20 舉行。筆者在本次會議內報告兩篇論文，第一篇名為「Intelligent Adaptive 
Motion Controller Design for Mecanum Wheeled Omnidirectional Robots with Parameter 
Variations ，第二篇名為「Simultaneous Tracking and Stabilization of a Wheeled Inverted Pendulum: 
a Backstepping Sliding-Mode Approach』，報告中引發不少熱烈的討論與互動。 
    主辦單位為加強學者專家、與會者間的交流，有例行性的大會聚會。 
 
（四）與會心得 
非線性科學，動力學及其應用等學術領域之發展，隨時代的需求，已不斷地推陳出新，產
生許多重要理論與技術。它對各種科學與高科技的發展有顯著的影響力。在此次的研討會中，
相當多有趣的研究主題是有nonlinear dynamics analysis and control , E-infinity theory, 非線性問
題之解析解與近似解，類神經網路與應用等，筆者可預測未來的研究方向是朝往非線性動力學
分析與控制，非線性問題之解析解與近似解，類神經網路與應用等研究主題，多多努力。 
    本次2010 ISND國際研討會，會場是位居上海市中心的上海東華大學的圖書館，各樣議程順
暢。本次大會的兩場大會演講，皆有獨到的技術介紹，亦有相當有趣的技術，相當值得聆聽。 
9個技術會議(Technical Session) 以及9月27日的Poster Session 不只有嶄新的理論，亦有來自許
多業界的實際應用範例。 
 
（五）攜回資料 
參加此次研討會帶回 the2010 ISND國際研討會大會手冊。 
 
（六）Some pictures in 2010 ISND . 
 
2010 ISND會議海報前. 
 2














國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/24
國科會補助計畫
計畫名稱: 子計畫二：智慧型嵌入式感測器與運動控制器之研製
計畫主持人: 蔡清池
計畫編號: 97-2628-E-005-004-MY3 學門領域: 智慧型機器人
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
(1)本年度參與台北機器人展，國內教育部微電腦系統設計競賽，增加與國內各
校研究團隊間的技術與經驗交流，且獲有佳績。本年度電動獨輪車與智慧雙手
臂機器人分別榮獲 2010 年度全國微電腦應用系統設計製作競賽第二名(第一名
從缺)，2010 年度 PMC 智慧型機器人產品創意競賽理想創意組佳作 (成果刊登
於台灣機器人產業發展協會報導)。 
(2)  已參與舉辦 SICE 2010, IROS 2010, WCICA 2011, FIRA 2011, SIRCon 
2011, ISR 2012 等國際級機器人領域相關的學術研討會，積極推動促成更緊密
的國際學術交流與產學合作。 
(3) 擔任 Chair, IEEE SMC TC on Intelligent learning in control systems，
積極推動智慧學習控制系統以及其工業應用。 
(4) 擔任經濟部技審會委員，協助廠商建立智慧機器人技術。 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
