In particular, we achieved major breakthrough with (1) proposing practical and effi-
cient solutions for large-scale pairwise ranking problems based on the technique of data
selection (Lin 2010b). (2) participating in the Yahoo! Learning to Rank Challenge,
a large-scale ranking competition held with the International Conference on Machine
Learning in 2010, and achieving descent performance in terms of accurate ranking (Tsai
et al. 2010). We also extended our interests to (3) cost-sensitive classification and (4)
multilabel classification—two tasks that can be viewed as special cases of ranking, and
proposed practical tools that can conquer the two tasks efficiently and effectively. We
will discuss some results from each piece of our work below.
3 Data Selection Techniques for Large-scale RankSVM
Pairwise ranking, which learns all the order preferences between every two examples, is
a widely-used approach for ranking. Nevertheless, for a given data set of N examples,
pairwise ranking needs to consider Θ(N2) pairs. Hence, it is generally difficult to scale
up pairwise ranking for large data sets.
RankSVM is a popular and promising ranking algorithm (Herbrich, Graepel and Ober-
mayer 2000), but it inherits the difficulty from pairwise ranking. In our work, we propose
a data selection technique, Pruned RankSVM, that selects the most informative pairs
before the actual training. Because the set of selected pairs is much smaller than the set
of all pairs, the training can be done more efficiently. Some experimental results are listed
in Table 1 and 2. In particular, Table 1 shows the ranking accuracy achieved from each
approach and Table 2 shows the size of the associated optimization problem. We can
see that Pruned RankSVM can achieve similar performance to the original RankSVM,
while being much faster. In addition, Pruned RankSVM performs better than SVR, a
non-pairwise approach, while being similarly efficient. Thus, Pruned RankSVM can be a
leading choice in for SVM-based ranking.
The work leads to a Master’s thesis (Lin 2010b) and is being submitted for future
journal publication (Lin, Jan and Lin 2011).
2
Table 3: online validation performance in Yahoo! learning to rank challenge
ORSVM Poly-ORSVM ORBoost RankSVM RankLR BoltzRank Ensemble
ERR 0.4527 0.4456 0.4473 0.4428 0.4503 0.4394 0.4565
NDCG 0.7820 0.7643 0.7634 0.7550 0.7721 0.7459 0.7870
achieves ERR of 0.4565 (NDCG 0.7870), which ranked the 19-th in the international
competition. In addition, we reach the following major findings:
• First, pointwise approaches, in particular ORSVM with the nonlinear perceptron
kernel, yield the best overall performance as individual learners. They are also
computationally more efficient than pairwise or listwise ranking approaches.
• Cost-sensitive, weighting, and query-based normalization settings that respect the
ERR criteria can fine-tune the ranking performance of the basic approaches to get
better online ERR results.
• Ensemble learning by stacking can improve the ranking quality over individual
approaches.
The work has been presented in the workshop of the challenge at the International
Conference on Machine Learning and has been compiled as a technical report (Tsai et al.
2010).
5 Cost-sensitive Classification via Pairwise Compar-
ison
From Section 3, we see that pairwise comparison can be applied to rank the examples.
In fact, pairwise comparison can also be used to rank the labels. A specific setup that
focuses on ranking the labels is to assign a cost vector to every example, where each
component of the cost vector denotes the preference level of each label. The setup is
called cost-sensitive classification (Lin 2008).
Starting from our previous NSC project (NSC-98-2218-E-002-019), we study the cost-
sensitive classification problem, and design a specific pairwise cost-sensitive classification
algorithm called cost-sensitive one-versus-one (CSOVO; Lin 2008, 2010a). We continue
4
Table 5: Test Hamming Loss of Different Approaches
data set Binary Relevance PLST Compressive Sensing
delicious 0.01813 ± 0.00003 0.01819 ± 0.00003 0.01954 ± 0.00003
corel5k 0.00940 ± 0.00002 0.00944 ± 0.00002 0.01021 ± 0.00002
mediamill 0.03003 ± 0.00006 0.03015 ± 0.00006 0.04332 ± 0.00007
yeast 0.19916 ± 0.00211 0.20320 ± 0.00204 0.29390 ± 0.00189
emotions 0.30069 ± 0.01053 0.26417 ± 0.01231 0.32889 ± 0.00638
to the traditional Binary Relevance approach (while being much faster) and is superior
to the modern Compressive Sensing approach (Hsu et al. 2009) in terms of both accuracy
and efficiency.
A short paper of this work appeared in the Second International Workshop of Learning
from Multi-label Data (Tai and Lin 2010a). We have submitted the extended paper to
the Machine Learning Journal (Tai and Lin 2010b).
7 Progress Review
In the original proposal, we set the following goals for the first year:
1. comparing major collaborative ranking algorithms using real-world data sets like
the Netflix one
2. extending the ordinal ranking algorithms that we have derived to the area of col-
laborative ranking
3. analyzing the computational bottlenecks in existing pairwise ranking approaches,
and developing techniques for conquering the bottlenecks
Because of the diversity and richness of the ranking, we change our focus on the first
and second items to zoom into some tasks that are more important than the original plan
of collaborative ranking. What we have actually done are:
1. comparing major query ranking algorithms using an real-world data set: the Yahoo!
Learning to Rank Challenge (Section 4).
6
Lin, H.-T. (2010a). A simple cost-sensitive multiclass classification algorithm using one-
versus-one comparisons. Technical report, National Taiwan University.
Lin, H.-T. and L. Li (2006). Large-margin thresholded ensembles for ordinal regression:
Theory and practice. In J. L. Balcaza´r, P. M. Long, and F. Stephan (Eds.), Algorithmic
Learning Theory, Volume 4264 of Lecture Notes in Artificial Intelligence, pp. 319–333.
Springer-Verlag.
Lin, K.-Y. (2010b). Data selection techniques for large-scale RankSVM. Master’s thesis,
National Taiwan University.
Lin, K.-Y., T.-K. Jan, and H.-T. Lin (2011). Data selection techniques for large-scale
RankSVM. Technical report, National Taiwan University.
Sculley, D. (2009). Large scale learning to rank. In NIPS ’09 Workshop on Advances in
Ranking.
Tai, F. and H.-T. Lin (2010a). Multi-label classification with principle label space trans-
formation. In Second International Workshop on learning from Multi-Label Data.
Tai, F. and H.-T. Lin (2010b). Multi-label classification with principle label space trans-
formation. Technical report, National Taiwan University.
Tsai, M.-F., S.-T. Chen, Y.-N. Chen, C.-S. Ferng, C.-H. Wang, T.-Y. Wen, and H.-T.
Lin (2010). An ensemble ranking solution to the yahoo! learning to rank challenge.
Technical report, National Taiwan University.
Volkovs, M. and R. Zemel (2009). Boltzrank: learning to maximize expected ranking gain.
In ICML ’09: Proceedings of the 26th Annual International Conference on Machine
Learning, pp. 1089–1096.
8
2Existing multi-label classification approaches usually fall into one of the two cate-
gories (Tsoumakas et al, 2010a): Algorithm Adaptation (AA) or Problem Transforma-
tion (PT). As its name suggests, AA directly extends some specific algorithms to solve
the multi-label classification problem. Typical members of AA include Adaboost.MH
(Schapire and Singer, 2000), Rank-SVM (Elisseeff and Weston, 2001), Multi-label C4.5
(Clare and King, 2001) and ML-KNN (Zhang and Zhou, 2007). PT approaches, on the
other hand, transform the multi-label classification problem to one or more reduced
tasks. Typical members of PT include Label Power-set (LP), Binary Relevance (BR)
and Label Ranking (LR; Fu¨rnkranz et al, 2008). LP reduces multi-label classification
to multi-class classification by treating each distinct label set as a unique multi-class
label. BR, also known as one-versus-all, reduces multi-label classification to many dif-
ferent binary classification tasks, each for one of the labels. LR approaches transform
the multi-label classification problem to the task of ranking all the labels by relevance
and the task of determining a threshold of relevance. As can be seen from above, an
advantage of PT over AA is that any algorithm which deals with the reduced tasks
can be easily extended to multi-label classification via the transformation.
In this paper, we propose a new way to perceive PT approaches: the hypercube
view. The view describes all possible label sets in the multi-label classification problem
as the vertices of a high-dimensional hypercube. The view not only unifies LP, BR and
LR under the same framework, but also allows us to design better methods that make
use of the geometric properties of those label-set vertices. We demonstrate the use of the
hypercube view with a novel method, Principle Label Space Transformation (PLST),
which captures the key unconditional correlations between labels using a flat in the
high-dimensional space. The method only uses a simple linear encoding of the vertices
and a simple linear decoding of the predictions, both easily computed from the Singular
Value Decomposition (SVD) of a matrix composed of the label-set vertices. Moreover,
by keeping only the key correlations, PLST can dramatically decrease the number of
reduced tasks to be solved without loss of prediction accuracy. Such a computational
advantage is especially important for scaling up multi-label classification algorithms to
a larger number of labels (Tsoumakas et al, 2010a).
Another recent work, multi-label prediction via Compressive Sensing (CS; Hsu
et al, 2009), also seeks to perform multi-label classification with a linear encoding of
the label-sets vertices. CS operates under the assumption of sparsity in the label sets
and thus can describe the label-set vertices with a small number of linear random
projections as its encoding. Although the encoding component of CS is linear, the
decoding component is not. In particular, for each incoming test instance, CS needs to
solve an optimization problem with respect to its sparsity assumption. That is, CS can
be time consuming during prediction. In our experiments, we will demonstrate that
PLST is not only computationally more efficient than CS, but also outperforms CS in
terms of prediction accuracy.
The paper is organized as follows. In Section 2, we give a formal setup of the multi-
label classification problem and introduce the hypercube view. Then, in Section 3, we
unify BR and CS under the same framework via the hypercube view and describe our
proposed method: PLST. Finally, we present the experimental results in Section 4 and
conclude in Section 5.
4Algorithm 2 Binary Relevance
1. training: for k = 1 to K, learn a relevance function rk(x) from {
(
xn,yn[k]
)
}N
n=1
.
2. predicting: for each input vector x, compute r(x) ≡
[
r1(x), r2(x), · · · , rK(x)
]
. Then, re-
turn round
(
r(x)
)
, where round(·) maps each component of the vector to the closest value
in {0, 1}.
Using the hypercube view, the k-th iteration of BR can be thought as projecting the
vertices to the k-th dimension (axis) before training. In addition, the relevance vector
r(x) ≡ [r1(x), r2(x), · · · , rK(x)] can be viewed as a point in R
K and the round(·)
operation maps the point to the closest vertex of the hypercube in terms of the `1-
distance.
Despite its effectiveness, BR is often criticized for neglecting the unconditional cor-
relation between labels, which may carry useful information in multi-label classification
tasks. Furthermore, the training complexity of BR is linear to the number of labels K,
which can still be expensive if K is too large. Recently, Hsu et al (2009) attempted to
address this problem through Compressive Sensing. Their work will be explained later
in this section.
Hypercube View of Label Ranking: Label Ranking (LR) approaches learn two issues
(jointly or separately) from the multi-label classification data set: the order of label
relevance, and the threshold for label presence. Note that BR is a special case of LR
when the ordering and the thresholding are taken from an underlying relevance-scoring
function r(x).
Using the hypercube view, the ordering issue in LR can be thought as learning
a length-K path from [0, 0, · · · , 0] to [1, 1, · · · , 1] using the hypercube edges. Each
vertex yn in the training examples then represent multiple length-K edge-paths that
goes through yn with the ideal thresholding at ‖yn‖1 =
∑K
k=1 yn[k].
Hypercube View of Compressive Sensing: Under the assumption that the label sets Y
are sparse (i.e. containing only a few elements), it is possible to compress the label
sets and learn to predict the compressed labels instead. Such a possibility allows Com-
pressive Sensing (CS; Hsu et al, 2009) to reduce the number of sub-tasks in BR to be
computationally feasible for data sets with a large K. In particular, each label set Y
(vertex y) can be taken as a K-dimensional signal. The theory of compressive sensing
states that when the signals are sparse, one does not need to sample at the Nyquist
rate in order to accurately recover the original signals. Thus, as the sketch of CS in
Algorithm 3 shows, when all y contain only a few 1’s, CS only needs to solve M  K
sub-tasks instead of K for multi-label classification.
Algorithm 3 Compressive Sensing
1. pre-processing: compress {(xn,yn)} to {(xn,hn)}, where h = Ps · y using an M by K
random projection matrix Ps with M determined by the assumed sparsity level s.
2. training: for m = 1 to M , learn a function rm(x) from {(xn,hn[m])}
N
n=1
.
3. prediction: for each input vector x, compute r(x) =
[
r1(x), r2(x), · · · , rM (x)
]
. Then, ob-
tain a sparse vector yˆ such that Ps · yˆ is “closest” to r(x) using an optimization algorithm.
Finally, return yˆ.
6Algorithm 4 Linear Label Space Transformation
1. pre-processing: consider an M -flat F described by a reference point o and an M by K
projection matrix P. Then, encode {(xn,yn)} to {(xn,hn)}, where hn = P(yn − o)
corresponds to a vector under the coordinate system of F .
2. training: for m = 1 to M , learn a function rm(x) from {(xn,hn[m])}
N
n=1
.
3. prediction: for each input vector x, compute r(x) =
[
r1(x), r2(x), · · · , rM (x)
]
. Then,
return D(r(x)) where D : RM → {0, 1}K is a decoding function from the M -flat to the
vertices of the hypercube.
data set with all label sets containing at least (K−1) labels is hypercube sparse with the
number of occupied vertices being at most K + 1 2K , but is by no means label-set
sparse.
3.1 Linear Label Space Transformation
Inspired by the hypercube sparsity, we now study a simple framework that focuses on
a subspace instead of the whole hypercube in RK . The framework takes an M -flat
as the subspace and encodes each vertex y of the hypercube to a vector h under the
coordinate system of the M -flat by projection. Then, the original multi-label classifi-
cation problem with {(xn,yn)}
N
n=1 becomes a multi-dimensional regression problem
with {(xn,hn)}
N
n=1. After obtaining a multi-dimensional regressor r(x) that predicts h
well, the framework will then map r(x) back to a vertex of the hypercube in RK using
some decoder D. The framework will be named Linear Label Space Transformation
(LLST), as shown in Algorithm 4.
Note that BR and CS are both special cases of LLST. For BR, we can set P = I
with an arbitrary reference point o and take the round function as D. The usual BR
operates with M = K, which means that many regressors rm are needed when K is
large. To reduce the number of regressors, we can also use BR with M < K by taking
onlyM random rows of I as the projection matrix P. We will call the approach Partial
BR (PBR) to distinguish it from the full BR. PBR equivalently discards some of the
labels during training and hence the test performance may not be satisfactory. We will
see such results in Section 4.
CS seeks to reduce the number of regressors by considering a flat with M  K.
Its projection matrix P is chosen randomly from an appropriate distribution (such as
Gaussian, Bernoulli, or Hadamard) and the reference point o of F is simply 0, the
origin of RK as well as the most label-set-sparse vertex. The decoding algorithm D
corresponds to the reconstruction algorithm in the terminology of CS, and requires
solving an optimization problem for each different x.
3.2 Linear Label Space Transformation with Round-based Decoding
As discussed above, CS may suffer from its slow decoding algorithm, while the round-
based decoding in BR can be more efficient. Next, we study a special form of LLST that
is coupled with an efficient round-based decoding scheme. In particular, the decoding
scheme first maps a prediction vector r(x) under the coordinate system of the M -flat
back to a corresponding point y˜ in RK . Then, the scheme rounds y˜ to the closest
vertex yˆ of the hypercube in terms of the `1 distance. The resulting approach, as
8Proof Using the fact that {pm}
M
m=1 forms an orthonormal basis, we can uniquely
decompose y = (o+PTh+ p⊥), where
p⊥ = y − o−P
T
h = (I−PTP)(y − o)
is orthogonal to every pm.
Then, from LLSTR, consider the point y˜ = o+PT r(x). From Lemma 1,
∆(yˆ,y) ≤
4
K
‖y˜ − y‖2
=
4
K
∥∥∥o+PT r(x)− y
∥∥∥2
=
4
K
(∥∥∥PT (r(x)− h)
∥∥∥2 + ‖p⊥‖2
)
(3)
=
4
K
(
‖r(x)− h‖2 + ‖p⊥‖
2
)
. (4)
Here (3) comes from the fact that p⊥ is orthogonal to every pm; (4) is because
{pm}
M
m=1 forms an orthonormal basis. uunionsq
We can take a closer look at the two terms in the right-hand-side of the bound (2).
The first term describes the squared prediction error between h and r(x), two vectors
represented under the coordinate system of the M -flat. The second term describes an
encoding error for projecting y to the closest point on the M -flat. The training step of
LLST/LLSTR aims at reducing the first term by learning from {(xn,hn)}
N
n=1.
The second term, on the other hand, does not depend on x and denotes a trade-off
on the choice of M . In particular, the second term generally decreases when M in-
creases, at the expense of more computational cost for learning the functions {rm}
M
m=1.
For instance, in PBR, if we take the center of the hypercube as o, the second term is
simply K−MK . When using the full BR, there is no encoding error but many regres-
sors are needed; when using PBR with M  K, we can use fewer regressors but the
resulting HL may be large because of the large encoding error.
3.3 Principle Label Space Transformation
For a fixed value of M , the analysis of LLSTR indicates that it is important to use
an M -flat that makes the encoding error as small as possible. Next, we propose an
approach that focuses on finding such an M -flat. In particular, the proposed Principle
Label Space Transformation (PLST) approach is a special case of LLSTR that seeks
for a reference point o ∈ RK and an M by K matrix P by solving
min
o,P
1
N
N∑
n=1
‖yn − o−P
T
hn‖
2 (5)
such that PPT = I.
The objective function of (5) is the empirical average of the encoding error on the
training set S. Because PLST makes an optimal use of the budget on the M  K
basis functions, we can take advantage from the hypercube sparsity to reduce the
computational cost in multi-label classification.
10
Table 1 Data Set Statistics
data set domain N K cardinality distinct hypercube sparsity nonzero
delicious text 16105 983 19.02 15806 1.93×10−292 25
corel5k text 5000 374 3.52 3175 8.25×10−110 5
mediamill video 43507 101 4.38 6555 2.59×10−27 18
yeast biology 2417 14 4.24 198 1.21×10−2 11
emotions music 593 6 1.87 27 4.22×10−1 3
downloaded from Mulan (Tsoumakas et al, 2010b) and cover a variety of domains, sizes
and characteristics, as shown in Table 1. We include data sets with a particularly large
number of labels such as delicious, corel5k and mediamill to test the effectiveness
of CS and PLST in reducing the dimension of the label space.
The cardinality column of Table 1 is defined as the average number of labels per
example. The distinct column of Table 1 shows the number of distinct label sets, or
using the hypercube view, the number of vertices occupied by examples. Dividing the
value of distinct by 2K in Table 1, we see that hypercube sparsity indeed exists in
every data set.
On the other hand, the nonzero column of Table 1 shows the maximum number of
non-zero entries in yn. Comparing the value of nonzero to K in Table 1, we see that
most data sets come with a strong label-set sparsity except yeast.
In all experiments, we randomly partition each data set into 90% for training and
10% for testing. We record the mean and the standard error of the test HL over 20
different random partitions.
We test PBR, PLST and CS with Ridge Linear Regression (RLR; Hastie et al,
2001) and M5P Decision Tree (M5P) as the underlying regression algorithm. We im-
plement Ridge Linear Regression with λ = 0.01 in MATLAB, and take the M5P
Decision Tree from WEKA (Hall et al, 2009) with its default settings. For CS, We
follow the recommendation from Hsu et al (2009) to use the Hadamard matrix as the
projection matrix P. Then, we take the best-performing reconstruction algorithm in
their work, CoSaMP, as the decoding function D and set the sparsity parameter for
the reconstruction algorithm to the nonzero column in Table 1.
4.1 Analysis of Results
Fig. 2 shows the test HL of PBR, PLST and CS at different sizes of the reduced
sub-tasks. On the left-hand-side of Fig. 2, the approaches are coupled with a linear
regressor: RLR; on the right-hand-side, the approaches are coupled with a non-linear
regressor: M5P. First of all, we see that regardless of the type of the regressor used,
PLST is always capable to reach reasonable performance while reducing the label space
to a lower dimensional M -flat. In particular, PLST with M  K regressors is capable
of achieving the same or better HL than the full BR on all data sets.
When using RLR as the regressor, first, the HL curve of PLST is always below the
curve of PBR across all M in all data sets, which demonstrates that PLST is the more
effective choice in the LLSTR family. Second, for data sets without a strong label-set
sparsity (yeast and emotions), as expected, PLST outperforms CS for both small and
12
0 100 200 300 400 500 600 700 800 900 1000
0
0.005
0.01
0.015
0.02
0.025
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(a) delicious, RLR
0 100 200 300 400 500 600 700 800 900 1000
0
0.005
0.01
0.015
0.02
0.025
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(b) delicious, Ideal
0 50 100 150 200 250 300 350 400
0
0.005
0.01
0.015
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(c) corel5k, RLR
0 50 100 150 200 250 300 350 400
0
0.005
0.01
0.015
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(d) corel5k, Ideal
0 10 20 30 40 50 60 70 80 90 100
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(e) mediamill, RLR
0 10 20 30 40 50 60 70 80 90 100
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(f) mediamill, Ideal
0 5 10 15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(g) yeast, RLR
0 5 10 15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(h) yeast, Ideal
0 1 2 3 4 5 6
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(i) emotions, RLR
0 1 2 3 4 5 6
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
H
a
m
m
L
o
s
s
Output Dimension
 
 
PLST
CS
PBR
(j) emotions, Ideal
Fig. 3 Test Hamming Loss of LLST algorithms: RLR versus Ideal
14
be seen in Fig. 2. These results demonstrate that PLST can take advantage of the
hypercube sparsity to efficiently solve multi-label classification problems.
5 Conclusion
We presented the novel hypercube view for problem transformation approaches to
multi-label classification. The view offers geometric interpretations to many existing
algorithms including Binary Relevance (BR), Label Power-set, Label Ranking and
Compressive Sensing (CS). Inspired by this view, we introduced the notion of hy-
percube sparsity and took it into account by Principle Linear Space Transformation
(PLST). We derived the theoretical guarantee of PLST and conducted experiments to
compare PLST with BR and CS. Experimental results verified that PLST is successful
in reducing the computational effort for multi-label classification, especially for data
sets with large numbers of labels. Most importantly, when compared with CS, PLST
not only enjoys a faster decoding scheme, but can also reduce the multi-label classifi-
cation problem to simpler and fewer regression sub-tasks. The advantages as well as
the empirical superiority suggest that PLST should be a preferred choice over CS in
practice.
As demonstrated through experiments, PLST was able to achieve similar perfor-
mance with substantially less dimensions compared to the original label-space. An
immediate future work is to conclude how to automatically and efficiently determine a
reasonable parameter M for PLST.
Note that PLST can be viewed as a special case of the kernel dependency esti-
mation (KDE) algorithm, proposed by Weston et al (2002) for predicting structured
outputs. As pointed by Dembczynski et al (2010a), KDE can be readily used for multi-
label classification by treating it as a particular case of structured output prediction.
Nevertheless, to the best of our knowledge, neither KDE’s linear form nor its use for
multi-label classification has been seriously studied. Our paper is the first to focus on
KDE’s linear form for multi-label classification—PLST—which readily leads to promis-
ing performance. Another plausible future work is to evaluate the usefulness of KDE’s
non-linear form for multi-label classification.
Acknowledgments
A preliminary version of this paper appeared in the Second International Workshop
of Learning from Multi-label Data and we thank the reviewers of the workshop for
their many useful suggestions. We also thank Krzysztof Dembczynski, Weiwei Cheng,
Eyke Hu¨llermeier and Willem Waegeman for valuable discussions. This research was
supported by the National Science Council of Taiwan via NSC 98-2221-E-002-192.
References
Barutcuoglu Z, Schapire RE, Troyanskaya OG (2006) Hierarchical multi-label predic-
tion of gene function. Bioinformatics 22(7):830–836
16
Table 2 Test HL of full BR versus PLST and CS at the Optimal Reduction Size of PLST
data set K M∗ M∗/K (%) full BR (K) PLST (M∗) CS (M∗)
delicious 983 129 13.1 0.01813 ± 0.00003 0.01819 ± 0.00003 0.01954 ± 0.00003
corel5k 374 16 4.3 0.00940 ± 0.00002 0.00944 ± 0.00002 0.01021 ± 0.00002
mediamill 101 11 10.9 0.03003 ± 0.00006 0.03015 ± 0.00006 0.04332 ± 0.00007
yeast 14 4 28.6 0.19916 ± 0.00211 0.20320 ± 0.00204 0.29390 ± 0.00189
emotions 6 1 16.7 0.30069 ± 0.01053 0.26417 ± 0.01231 0.32889 ± 0.00638
(the more accurate result between PLST and CS is marked in bold)
Table 3 Time of full BR versus PLST and CS at the Optimal Reduction Size of PLST
full BR (K) PLST (M∗) CS (M∗)
data set K M∗ M∗/K regression regression encode + regression encode +
(%) (sec) (sec) decode (sec) (sec) decode (sec)
delicious 983 129 13.1 4417.90 577.38 154.38 579.71 886.41
corel5k 374 16 4.3 560.11 23.96 7.76 24.13 2.97
mediamill 101 11 10.9 105.96 11.55 8.39 11.43 57.14
yeast 14 4 28.6 0.70 0.20 0.02 0.18 1.03
emotions 6 1 16.7 0.06 0.01 0.00 0.01 0.15
(the faster result between the corresponding columns of PLST/CS is marked in bold)
form
⋃Q
q=1{(q,xqn, yqn)}
Nq
n=1, where Nq is the number of examples (q,xqn, yqn) in query q.
In the training part of set 1 in the competition, d = 519, K = 5, Q = 19944 and
∑Q
q=1 Nq =
473134. We will generally use N to denote the number of training examples that are given
to a learning algorithm, and T to denote the number of iterations that an iterative learning
algorithm uses for training.
2. Ordinal Ranking SVM
Ordinal Ranking SVM (ORSVM; Li and Lin, 2007) is an approach for solving ordinal rank-
ing (regression). The approach reduces ordinal ranking to SVM-based binary classification
by thresholding an underlying raw score for each instance. We apply the approach in this
competition as follows. First, we combine all examples from all queries q to a big training
set with ordinal ranking examples {(xqn, yqn)}. Then, we transform each example to K − 1
binary classification examples of the form
{(
(xqn, ek), 2[[k < yqn]]− 1
)}K−1
k=1
,
where ek is a vector that contains a single 1 at the k-th component and 0 otherwise. The
transformed examples are then fed to an SVM solver using a composite kernel defined by
the summation of a nonlinear kernel for the xqn part and a linear kernel for the ek part.
We use the learned decision function from the solver to predict a relevance score for any
test instance x by plugging (x, 0, 0, . . . , 0) into the function. Finally, we order the instances
in the test queries by their predicted relevance scores to form the ranked list.
From the reduction view of the approach, the time complexity of ORSVM is similar
to that of a usual binary SVM on N(K − 1) examples, with a space complexity of O(N2)
for storing the nonlinear kernel part. Following the analysis of the LIBSVM solver that
we adopt (Chang and Lin, 2001), the time complexity of our ORSVM implementation1 is
O(TNKd) when considering a nonlinear kernel on xqn that can be evaluated within O(d)
and caching most of the kernel evaluations.
An important feature of ORSVM is that we can assign a cost vector per example to
indicate the penalties for different kinds of erroneous predictions (Li and Lin, 2007). This
setting enables us to encode the relative importance of each instance and each prediction.
To match the evaluation criteria of the competition, we adopt an ERR-oriented cost setting.
In particular, the k-th component of the cost vector for an instance xqn is defined as the
difference to the optimal ERR. That is,
the k-th cost component for instance xqn
= (optimal ERR of query q)− (ERR when (q,xqn, yqn) is mis-predicted as rank k).
2.1 Experiments
We adopt the perceptron kernel (Lin and Li, 2008) as the nonlinear kernel for x in ORSVM
because the kernel allows for faster parameter selection. Since the given data set is large,
we select the C parameter in ORSVM with 100 smaller data sets, each of which consists of
1. downloadable from http://www.work.caltech.edu/~htlin/program/libsvm/
2
Table 3: Poly-ORSVM with Different Terms
random terms random terms non-cross terms
C parameter 0.1 1 0.01
internal validation ERR 0.4467 0.4485 0.4486
Table 4: Poly-ORSVM with Different Degrees of Expansions
degree-1 (linear) degree-2 degree-3 degree-4
best C selected from internal validation 0.1 0.01 0.001 0.001
training time (min) ≈ 19 ≈ 8 ≈ 6 ≈ 13
online ERR 0.4413 0.4432 0.4451 0.4456
online NDCG 0.7566 0.7587 0.7625 0.7643
Table 3 reports the results on our internal validation data set, which is a random 20%
of the original training set. We keep the same number of random terms to the non-cross
terms in the experiments. From Table 3, when the numbers of appended terms are the
same, Poly-ORSVM with non-cross terms is better than or similar to Poly-ORSVM with
random terms. Thus, expanding only the non-cross terms, namely, the nth powers of each
features, is a promising way for Poly-ORSVM.
To improve the degree-2 Poly-ORSVM, we also append the 3rd, and 4th powers as new
feature terms. The results are listed in Table 4 and demonstrate that Poly-ORSVM can
efficiently achieve promising results with those new feature terms. In particular, using the
3rd, and 4th powers of each feature shows better results compared with the ones with only
linear or the 2nd power of feature. Although the ranking performance of Poly-ORSVM is
not as good as that of ORSVM (online ERR 0.4527), the time cost is reduced dramatically.
4. Ordinal Regression Boosting
Ordinal Regression Boosting (ORBoost; Lin and Li, 2006) is a boosting-like approach for
ordinal regression with large-margin thresholded ensembles. The is similar to ORSVM, but
adjusts the underlying scores and thresholds adaptively with the help of a base learner. In
our experiments, we study two variants of ORBoost: ORBoost with all margins (ORBoost-
All) and ORBoost with left-right margins (ORBoost-LR). ORBoost-All attempts to mini-
mize the absolute cost between the predicted and actual ordinal ranks, whereas ORBoost-LR
attempts to minimize the classification cost.
As done by (Lin and Li, 2006), we take the decision stumps and perceptrons as base
learners of ORBoost. The time complexity of ORBoost is O(T · dN logN) for decision
stumps and O(T ·TP N logN) for perceptrons, where T is the number of boosting iterations
and TP is the number of internal perceptron learning iterations.
In addition to the ORBoost variants and the base learners, we also explore some instance-
level weighting schemes to better match the evaluation criteria of the competition. In
particular, for an instance xqn, we design the following schemes to emphasize high-rank
instances and balance the influence of queries with excessive instances.
• weight by rank: w1(xqn) = yqn + 1
4
Table 7: ORBoost-All with Decision Stump under Different Weighting Schemes
T = 1000 training validation online training time (min)
ERR NDCG ERR NDCG ERR NDCG
w1 0.4502 0.7511 0.4539 0.7526 0.4464 0.7636 431
w2 0.4497 0.7534 0.4562 0.7556 0.4467 0.7651 487
w3 0.4505 0.7503 0.4548 0.7498 0.4462 0.7618 465
w4 0.4516 0.7502 0.4550 0.7500 0.4463 0.7594 493
T = 5000 training validation online training time (min)
ERR NDCG ERR NDCG ERR NDCG
w1 0.4526 0.7577 0.4538 0.7530 N/A N/A 2375
w2 0.4533 0.7610 0.4560 0.7560 0.4471 0.7664 2481
w3 0.4536 0.7573 0.4531 0.7524 0.4473 0.7634 2205
w4 0.4556 0.7590 0.4551 0.7510 0.4459 0.7602 2238
Table 8: RankSVM with Different Settings
internal validation online
settings ERR NDCG ERR NDCG
overall normalization 0.4253 0.699 N/A N/A
query-level normalization 0.4387 0.7213 0.4290 0.7226
query-level normalization with weight scheme 0.4479 0.7449 0.4428 0.7550
binary example of the form
(
xqn − xpm, 2[[yqn < ypm]]− 1
)
Then, the binary examples are sent to a linear SVM solver (without the bias term) to learn
a decision function, which can be directly used to compute the relevance score for any test
instance.
For this competition, in view of efficiency, we construct only pairs within the same query.
That is, only pairs with q = p are considered. The off-line construction and loading the
data into memory takes O(
∑
q N
2
q ). Then, similar to Poly-ORSVM, we use LIBLINEAR
(Fan et al., 2008) to solve the binary classification problem with O(Td) time complexity. To
improve the performance with respect to the ERR criteria, we also carry out a weighting
scheme to emphasize important pairs. In particular, for a pair with rank (yqn, ypm), its
weight is set to max(yqn, ypm)
|yqn−ypm|.
In addition to the usual overall normalization of the features, we also experiment with
another setting: query-level normalization. The goal of the setting is to include some
query-level information between examples into training.
5.1 Experiments
Table 8 lists the results of RankSVM with different settings. As shown in the table, with
query-level feature normalization, RankSVM can get better ranking quality over that with
overall normalization. This enhancement may be due to the fact that, within a query,
6
Table 10: BoltzRank with Different Feature Sets
N1 N2 training ERR online ERR time per iteration (min)
use all 700 features
15 0.4356 0.4325 150
45 0.4362 0.4339 480
use 6 features selected by AdaRank
6 0.4409 0.4380 1
6 6 0.4429 0.4394 5
use 10 features selected by AdaRank
10 10 0.4412 0.4393 8
use 12 features selected by AdaRank
12 12 0.4429 0.4393 10
a pair-wise one. Then, BoltzRank operates with gradient decent with respect to an er-
ror function, for which we take the sum of the ERR criteria and a KL-divergence-based
regularization term.
Even with the sampling process, BoltzRank can still be quite slow. Thus, we try con-
ducting feature selection using AdaRank (Xu and Li, 2007) before BoltzRank training. As
we shall see next, AdaRank not only speeds up the training time of BoltzRank, but also
helps get more accurate results.
7.1 Experiments
In our experiments with BoltzRank, one hidden layer with N1 hidden nodes is used for
the point-wise neural network, and N2 hidden nodes for the pair-wise one. We set the
learning rate of gradient descent to 0.1. In general, BoltzRank takes 100 to 300 iterations
to converge.
Table 10 shows the performance of BoltzRank with different training features. As shown
in the table, training on the feature set selected by AdaRank can achieve better results
than training on the whole feature set. In addition, using the pair-wise neural networks can
improve the performance over using only the point-wise ones.
8. Final Ensemble
After introducing the six individual methods and their respective results, below we describe
how we combine these approaches.
We obtain 20 models from the six approaches in the their best settings using a random
80% of set 1. These 20 models includes 12 for ORSVM, 1 for Poly-ORSVM, 3 for ORBoost,
1 for RankSVM, 1 for RankLR, and 2 for BoltzRank. We then use the outcomes of these 20
models as features to learn an ensemble using the remaining 20% of set 1. After carefully
studying the advantages and disadvantages of the six approaches for ensemble learning in
task 1, we eventually take RankLR with the 2nd order polynomial expansion as the final
choice. The best online results of each approach as well as the final ensemble are listed in
8
Hsuan-Tien Lin. From Ordinal Ranking to Binary Classification. PhD thesis, California
Institute of Technology, 2008.
Hsuan-Tien Lin and Ling Li. Support vector machinery for infinite ensemble learning.
Journal of Machine Learning Research, 9:285–312, 2008.
Hsuan-Tien Lin and Ling Li. Large-margin thresholded ensembles for ordinal regression:
Theory and practice. In ALT ’06: Proceedings of Algorithmic Learning Theory, pages
319–333, 2006.
D. Sculley. Large scale learning to rank. In NIPS ’09 Workshop on Advances in Ranking,
2009.
Maksims Volkovs and Richard Zemel. Boltzrank: learning to maximize expected ranking
gain. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine
Learning, pages 1089–1096, 2009.
Jun Xu and Hang Li. AdaRank: a boosting algorithm for information retrieval. In SI-
GIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Re-
search and development in information retrieval, pages 391–398, 2007.
10
98年度專題研究計畫研究成果彙整表 
計畫主持人：林軒田 計畫編號：98-2221-E-002-192- 
計畫名稱：自動評等演算法之設計與分析 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 3 0 100%  
研討會論文 2 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
