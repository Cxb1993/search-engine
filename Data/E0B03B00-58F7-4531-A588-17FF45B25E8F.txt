An Expectation-Maximization Approach to
Supervised Distance Metric Learning
Chin-Chun Chang
Department of Computer Science and Engineering,
National Taiwan Ocean University, Keelung, Taiwan.
e-mail: cvml@mail.ntou.edu.tw
Abstract
The success of many pattern recognition and machine learning
problems relies on proper distance metrics. In related literatures, four
properties, namely, local analysis, maximum-margin-based analysis,
neighborhood re-analysis, and wrapper-like analysis, are found to be
advantageous to supervised distance metric learning. In this report,
a supervised distance metric algorithm with these four properties is
proposed. In the proposed algorithm, a distance metric is learned
under the framework of the expectation-maximization algorithm with
a stochastic nearest neighbor model and loss functions related to the
leave-one-out loss of the nearest neighbor classification. Experimental
results on several UCI data sets show that the proposed approach is
competitive to the state-of-the-art algorithm of supervised distance
metric learning. At last, the proposed approach with the k-nearest
1
high-dimensional data.
In the presence of highly correlated features, the distance-metric learning
algorithm producing a general metric matrix may be more effective. Many algo-
rithms have been proposed to learn a general metric matrix. Although some of
them aim to learn a transformation matrix L such that LTx may extract impor-
tant information from the sample vector x, a metric matrix as LLT is learned in
fact. Algorithms of supervised metric-matrix learning may be categorized into
the following main approaches:
• Feature weighting : Feature weights may be determined in a transformed space
such as relevant component analysis [10, 11, 12].
• Linear discriminant analysis : This approach may be based on the homoscedas-
tic data model [13, 14, 15, 16], or the heteroscedastic data model [17, 18] to find
a subspace where the ratio of the inter-class scatter to the intra-class scatter
is maximized. Some extensions, such as subclass discriminant analysis [19, 20]
and complement subspace analysis [14, 21], have been developed. The intra-
class and the inter-class scatter matrix can also be defined by techniques of local
analysis, such as the k -nearest neighbor technique [22, 23], and the concept of
the locality-preserving projection [24].
• Margin-based approaches : This approach learns a metric matrix through max-
imizing sample or hypothesis margins [25, 26, 27, 28, 29, 30], where the sample
margin is defined as the distance from a sample to the discriminatory bound-
ary, and the hypothesis margin is referred to the distance that the classifier
can be moved without altering classification results [31, 32].
• Graph-based approaches : The graph-based approach regards each sample as a
node in a weighted graph. The weight of the edge between two samples may
be defined by their class labels and the local manifold structure around them.
3
identical class labels while samples of different semantics or labels have large
margins. In such a metric space, the k-nearest neighbor algorithm often has
good performance. The contribution of this report are twofold.
• First, by regarding indicator variables for the nearest neighbors of samples as
latent variables and by making use of a stochastic nearest neighbor model [44,
9], local analysis and neighborhood re-analysis are enabled naturally under the
framework of the EM algorithm [45].
• Second, under the EM framework, two log-likelihood loss functions based
on the exponential and logistic losses are used and optimized by the eigen-
decomposition. Since these two loss functions are related to the leave-one-out
error, the proposed approach also makes use of maximum-margin-based anal-
ysis and wrapper-like analysis.
This report is organized as follows. Section 2 gives the problem definition and
an EM algorithm for solving this problem. Section 3 discusses how to reduce the
dimensionality of the learned space. Section 4 presents the experimental results.
Concluding remarks are drawn in the last section.
2 Methodology
2.1 Problem Formulation
Let X = {x1 . . .xN} be the training set consisting of N i.i.d. sample vectors in
Rd. In addition to the training set, the supervisor also provides two index sets
Mi and Hi for every training sample such that for all j ∈ Mi, xi and xj have
different semantic meanings or labels, and for all j ∈ Hi, xi and xj have similar
semantic meanings or labels. For convenience, we call the sample in Mi and Hi
the miss and the hit of xi, respectively. Ideally, we would like to learn a metric
5
tioned on metric matrix M may be modeled with the exponential loss and the
logistic regression as
• Exponential function:
pExp(NMM(xi),NHM(xi),xi|M) = exp(‖NMM(xi)−xi‖2M−‖NHM(xi)−xi‖2M),
(3)
• Logistic regression:
pLogit(NMM(xi),NHM(xi),xi|M) = 1
1 + exp(−(‖NMM(xi)− xi‖2M − ‖NHM(xi)− xi‖2M))
.
(4)
Now, by integrating the likelihood loss for every training sample together,
likelihood loss functions for giving an estimate of the metric matrix M may take
the following form:
LExp/Logit(M) =
N∏
i=1
∏
j∈Hi
( ∏
k∈Mi
pExp/Logit(xk,xj,xi|M)hij×mik
)
.
The metric matrix maximizing L(M) induces a metric space where the hypothesis
margin [31, 32] is large. By taking the logarithm, the log-likelihood function is
then given by
lnLExp/Logit(M) =
N∑
i=1
∑
j∈Hi
hij
(∑
k∈Mi
mik ln p
Exp/Logit(xk,xj,xi|M)
)
.
By following the EM algorithm [45], the metric matrix may be estimated through
iteratively applying the following two steps.
• The E-step. Calculate the negative expected log-likelihood loss function as
follows:
7
This lower bound function is used as the function to be optimized at the
M-step in the EM algorithm. For later use, the gradient of QLogit(M|Ml)
with respect to M is expressed as follows:
∂
∂M
QLogit(M|Ml) = −
N∑
i=1
NMMl(xi)−NHMl(xi)
1 + exp(tr(MT (NMMl(xi)−NHMl(xi)))
.
(7)
• The M-step. Obtain Ml+1 through minimizing QExp/Logit(M|Ml).
Ml+1 ← argmin
M
QExp/Logit(M|Ml) (8)
subject to M = LLT ,
LTSwL = I0, (9)
where the constraint (9) is to establish a coordinate system in the learned space
with Sw =
∑N
i=1NHMl(xi) and I0 a diagonal matrix with entries zero or one.
The M-Step may be solved as follows.
2.2 Optimization of Problem (8) with respect to QExp
Define Sb as Sb =
∑N
i=1NMMl(xi). In addition, denote by Ψ and Λ the unit
eigenvector matrix and the eigenvalue matrix of the matrix pencil (Sb,Sw + λI),
where λ ≥ 0 is a regularization parameter. As shown in [47], the metric matrix
M optimizing Problem (8) with respect to QExp may be obtained by M = LLT
where L = ΨΛ−
1
2Θ withΘ a diagonal matrix whose diagonal entryΘii is defined
as
Θii =

1 if Λii > 1;
0 otherwise.
9
3 Dimensionality Reduction
By ignoring the component less relevant to discrimination, the dimensionality of
the learned space may be reduced without losing much discriminatory informa-
tion. Thus, after learning the metric matrix, we may reduce the dimensionality
of the learned space for saving the computational cost of subsequent processes.
For a diagonal metric matrix, we may select the feature corresponding to a large
diagonal element in the learned metric matrix. For a general metric matrix, we
may define a transformation matrix L as L = ΨΘ
1
2 , where M = ΨΘΨT is an
eigen-decomposition of the learned metric matrix M with eigenvalues θ1, . . . , θd
in descending order. Thus, if the dimensionality of the reduced learned space is
d′, the image of the sample vector x in the reduced learned space is L(:, 1 : d′)Tx,
and L(:, 1 : d′)L(:, 1 : d′)T is a low-rank approximation of M, where L(:, 1 : d′)
is the matrix formed by the first d′ columns of L. Alternatively, a ratio ρ may
be specified instead and the dimensionality of the reduced learned space may be
defined by the least number d′ ≤ d satisfying
∑d′
i=1 θi
tr(M)
≥ ρ.
4 Experimental Results
The proposed algorithms with respect to the exponential loss and the logistic
loss are referred as to GI-RELIEF and LOGDML, respectively. In the following
experiments, the proposed algorithms and a variety of approaches of distance
metric learning, such as RCA [48], Fisher’s discriminant analysis (FDA) [16],
local Fisher’s discriminant analysis (LFDA) [24]1, ITML [28]and LMNN [49]2
were implemented in the MATLAB programming language. The experiments
1http://sugiyama-www.cs.titech.ac.jp/~sugi/software/LFDA/index.html
2http://www.weinbergerweb.net/Downloads/LMNN.html
11
0 0.5 1
RCA (0.96,  3)
FDA (0.96,  1)
LFDA (0.94,  2)
ITML (0.96,  1)
LMNN (0.95,  4)
LOGDML (0.96,  4)
GI−RELIEF (0.96,  3)
Iris
0 0.5 1
RCA (0.98, 13)
FDA (0.71,  2)
LFDA (0.93,  3)
ITML (0.71,  3)
LMNN (0.94, 12)
LOGDML (0.99, 11)
GI−RELIEF (0.99,  6)
Wine
0 0.5 1
RCA (0.89,  2)
FDA (0.88,  1)
LFDA (0.90,  4)
ITML (0.88,  4)
LMNN (0.83,  2)
LOGDML (0.93,  3)
GI−RELIEF (0.96,  3)
Balance
0 0.5 1
RCA (0.87, 10)
FDA (0.84,  1)
LFDA (0.86, 10)
ITML (0.90, 15)
LMNN (0.90,  8)
LOGDML (0.92,  6)
GI−RELIEF (0.92,  6)
Ionosphere
0 0.5 1
RCA (0.87, 53)
FDA (0.78,  1)
LFDA (0.88,  6)
ITML (0.77, 48)
LMNN (0.85, 52)
LOGDML (0.89,  6)
GI−RELIEF (0.88,  9)
Spambase
Figure 1: A comparison of classification accuracy.
classification accuracy, the proposed approaches GI-RELIEF and LOGDML are
the best or the second best. Next, in order to test the effectiveness of these seven
methods when the sample vector contains a lot of irrelevant components, the
dimensionality of the sample vector of each test data set was tripled. For example,
the dimensionality of the iris data set was augmented to 9. The augmented
component is a zero-mean Gaussian random noise with standard deviation equal
to that of relevant components. Figure 2 shows that GI-RELIEF is still the
best or the second best. However, LOGDML may become less accurate. This is
because the gradient of logistic loss is more dependent on the samples difficult
to be classified. It is interesting to notice that some methods with respect to
the noise injected data set may be more accurate than that with respect to the
original data set. One possible cause is that injected noise components may be
regarded as a form of regularization [51].
13
where a˜i and a
∗
i denote the estimated age and the ground truth age, respectively.
This experiment followed the leave-one-person-out experiment protocol. For
each training set, the hit set of a training image contains the image with the
absolute age difference not greater than 5, and the miss set of a training image
includes the image with the absolute age difference greater than 15. The k-nn
algorithm was used for age estimation. Figure 3 shows the experimental results.
The MAE and CS(10) with respect to the original AAM feature are about 6.74
and 0.8, respectively, and the proposed approach LOGDML can improve them to
5.8 and 0.84, respectively. This experimental result is competitive to the state-
of-the-art method based on the AAM features, and proves that the proposed
approach can learn an accurate metric matrix.
5 Conclusion
In this report, four properties, namely, local analysis, maximum margin-based
analysis, neighborhood reanalysis, and wrapper-like analysis, have been found
useful for supervised distance metric learning. By regarding the indicator variable
for the nearest neighbor as the latent variable and by making use of a stochas-
tic nearest neighbor model and hypothesis margin-based loss functions, we have
found that the EM algorithm provides a unified framework for developing algo-
rithms of supervised distance metric learning with these four properties. Two
algorithms, namely, GI-RELIEF and LOGDML, have been developed under this
framework. Experimental results reveal that these two algorithms are competitive
to the state-of-the-art algorithm in terms of both accuracy and efficiency. Be-
sides, the proposed algorithms with the k-nearest neighbor algorithm have been
applied on face image-based age estimation, and tested against the FG-NET ag-
15
0 5 10 15 20 25
5.5
6
6.5
7
7.5
8
MAE
k
ab
so
lu
te
 e
rro
r
←k=15,MAE=5.83k=12,MAE=5.80→
←k=6,MAE=6.74
 
 
GI−RELIEF
LOGDML
ORIGINAL
(a) MAE
0 10 20 30 40
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CS
age
 
 
GI−RELIEF
LOGDML
ORIGINAL
(b) CS
Figure 4: Experimental results on the FG-NET aging data set.
ing data set. The average absolute error of the estimated age is about 5.8 years,
and about 84 percent of the test face images have absolute errors less than 10
years. That result is very close to that of the state-of-the-art approach with the
AAM feature, which is based on more advanced classifiers, and thus also proves
the practicability of the proposed approach.
Acknowledgment
This work was supported financially by National Science Council under the grant
NSC 100-2221-E-019-061- and in part by NTOU-RD981-05-02-04-01.
17
[9] Y. Sun, S. Todorovic, and S. Goodison, “Local-learning-based feature selec-
tion for high-dimensional data analysis,” IEEE Trans. on Pattern Analysis
and Machine Intelligence, vol. 32, no. 9, pp. 1610–1626, 2010.
[10] A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall, “Learning a Maha-
lanobis metric from equivalence constraints,” Journal of Machine Learning
Research, vol. 6, pp. 937–965, 2005.
[11] D. Y. Yeung and H. Chang, “Extending the relevant component analysis
algorithm for metric learning using both positive and negative equivalence
constraints,” Pattern Recogn., vol. 39, no. 5, pp. 1007–1010, 2006.
[12] S. C. H. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma, “Learning distance met-
rics with contextual constraints for image retrieval,” in Proceedings of IEEE
Conf. Computer Vision and Pattern Recognition, 2006.
[13] R. A. Fisher, “The use of multiple measurements in taxonomic problems,”
Annals of Eugenics, vol. 7, pp. 179–188, 1936.
[14] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd ed.
Boston: Academic Press, 1990.
[15] M. Loog, R. P. W. Duin, and R. Haeb-Umbach, “Multiclass linear dimension
reduction by weighted pairwise Fisher criteria,” IEEE Trans. on Pattern
Analysis and Machine Intelligence, vol. 23, no. 7, pp. 762—766, 2001.
[16] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification, Second ed.
John Wiley & Sons, Inc., 2001.
19
[25] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell, “Distance metric learning
with application to clustering with side-information,” in Advances in Neural
Information Processing Systems 15, S. T. S. Becker and K. Obermayer, Eds.
Cambridge, MA: MIT Press, 2003, pp. 505–512.
[26] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric dis-
criminatively, with application to face verification,” in Proceedings of the
2005 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR’05), 2005, pp. 539–546.
[27] L. Yang, R. Jin, R. Sukthankar, and Y. Liu, “An efficient algorithm for
local distance metric learning,” in Proceedings of the Twenty-first National
Conference on Artificial Intelligence, 2006.
[28] J. V. Davis, B. Kulis, P. Jain, and I. S. Dhillon, “Information-theoretic met-
ric learning,” in Proceedings of the 24th international conference on Machine
learning, 2007, pp. 209–216.
[29] R. Jin and S. Wang, “Regularized distance metric learning: Theory and
algorithm,” in Advance in Neural Information Processing Systems, vol. NIPS
23, 2009.
[30] K. Q. Weinberger and L. K. Saul, “Distance metric learning for large mar-
gin nearest neighbor classification,” Journal of Machine Learning Research,
vol. 10, pp. 207–244, 2009.
[31] K. Crammer, R. Gilad-bachrach, A. Navot, and N. Tishby, “Margin anal-
ysis of the LVQ algorithm,” in Advances in Neural Information Processing
Systems 14. MIT press, 2002, pp. 462–469.
21
[40] A. Howard and T. Jebara, “Transformation learning via kernel alignment,”
in International Conference on Machine Learning and Applications, 2009.
[41] S. Wang and R. Jin, “An information geometry approach for distance metric
learning,” in Proceedings of the 12th International Conference on Artificial
Intelligence and Statistics, 2009.
[42] L. Yang, R. Jin, L. Mummert, R. Sukthankar, A. Goode, B. Zheng, S. C. H.
Hoi, and M. Satyanarayanan, “A boosting framework for visuality-preserving
distance metric learning and its application to medical image retrieval,”
IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 32, no. 1,
pp. 30–44, 2010.
[43] C. Shen, J. Kim, L. Wang, and A. van den Hengel, “Positive semidefinite
metric learning using boosting-like algorithms,” Journal of Machine Learn-
ing Research, vol. 13, pp. 1007–1036, 2012.
[44] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov, “Neighbour-
hood components analysis,” in Advances in Neural Information Processing
Systems 17, L. K. Saul, Y. Weiss, and L. Bottou, Eds. Cambridge, MA:
MIT Press, 2005, pp. 513–520.
[45] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from
incomplete data via the EM algorithm,” Journal of the Royal Statistical
Society. Series B, vol. 39, no. 1, pp. 1–38, 1977.
[46] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge Univ.
Press, 2004.
[47] C. C. Chang, “Generalized iterative RELIEF for supervised distance metric
learning,” Pattern Recognition, vol. 43, pp. 2971–2981, 2010.
23
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/31
國科會補助計畫
計畫名稱: 使用提升法之監督者式距離度量學習及其應用於人臉年齡估計
計畫主持人: 張欽圳
計畫編號: 100-2221-E-019-061- 學門領域: 圖形辨識 
研發成果名稱
(中文) 使用Expectation-Maximization方法的監督式距離學習演算法
(英文) An Expectation-Maximization Approach to Supervised Distance Metric Learning
成果歸屬機構
國立臺灣海洋大學 發明人
(創作人)
張欽圳
技術說明
(中文) 許多圖型識別與機器學習問題仰賴適當的距離度量。在相關文獻裡，我們發現四
個有利於監督式距離度量學習的性質:區域分析、最大邊際分析、鄰近區重分析、
與包裝法似的分析。在這個報告裡，我們提出具備這四個性質的監督式距離度量
學習演算法。此演算法建構在EM演算法、隨機式最近鄰居模型、與最近鄰居分類
之leave-one-out loss架構之下。使用UCI公開資料集測試後，顯示我們的方法
與最新的監督式距離度量學演算法比較下毫不遜色。我們結合k-nn演算法來處理
Fg-net年齡資料集，亦證實本方法的可行性。
(英文) The success of many pattern recognition and machine learning problems relies on proper 
distance metrics. In related literatures, four properties, namely, local analysis, 
maximum-margin-based analysis, neighborhood re-analysis, and wrapper-like analysis, 
are found to be advantageous to supervised distance metric learning. In this report, a 
supervised distance metric algorithm with these four properties is proposed. In the 
proposed algorithm, a distance metric is learned under the framework of the expectation-
maximization algorithm with a stochastic nearest neighbor model and loss functions 
related to the leave-one-out loss of the nearest neighbor classification. Experimental 
results show that the proposed approach is competitive to the state-of-the-art algorithm of 
supervised distance metric learning. 
產業別 其他專業、科學及技術服務業
技術/產品應用範圍 圖型識別; 電腦視覺相關距離學習問題
技術移轉可行性及
預期效益
此演算法以Matlab或C++皆可輕易實現,並可能提升距離度量的準度
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
