1行政院國家科學委員會專題研究計畫成果報告
支援嵌入式作業系統之動態更新與重組之系統及軟體元件庫的製作
Component Library and System Support for Dynamic Update and
Reconfiguration in an Embedded Operation System
計畫編號：NSC 96-2628-E-260-009
執行期限：96 年 8 月 1 日至 97 年 7 月 31 日
主持人：姜美玲 國立暨南國際大學資訊管理系
計畫參與人員：沈柏曄、林振緯、吳俊泓、莊尚益、鄭凱宇
國立暨南國際大學資訊管理系
摘要
元件化作業系統是近年來嵌入式作業系
統設計與發展上的趨勢，透過軟體元件庫的
置換，可依照嵌入式系統不同平台及各種特
殊應用的需求，快速的修改與發展作業系
統。而動態元件更新是個可以讓元件化作業
系統線上即時更新的方式，作業系統可以在
不用關機的情況下更新或新增內部的元件。
欲完整的提供動態元件更新的功能，除了作
業系統本身必需提供動態更新的機制，支援
元件動態更新的元件伺服器與元件庫亦是重
要的一環。
本計畫的目的是研究與製作支援嵌入式
作業系統之動態元件更新與重組的系統平
台，我們以本實驗室現有的元件化嵌入式作
業系統 LyraOS 為平台，除了改進其動態元
件更新機制，使之具有延展性、彈性、和高
效率，不但可以安全地更新元件內部的程式
碼和資料結構，亦可以動態修改元件所釋出
的介面。同時，我們製作支援元件動態更新
的元件伺服器與元件庫，以完整的提供動態
元件更新的功能。除此之外，針對嵌入式環
境的限制，我們探討如何讓整個系統在擁有
動態更新功能的同時，將負擔降至最低。
另一方面，由於快閃記憶體已成為嵌入式
系統主要的儲存設備，我們亦設計與實作一
快閃記憶體儲存系統之元件，針對快閃記憶
體的特性，探討並實作其管理策略以及清除
策略，並儲存於元件庫內。
關鍵詞：嵌入式作業系統、動態更新、軟體
元件、元件庫、快閃記憶體
Abstract
Component-based operating systems are
the design trend for the development of
embedded operating systems. Through the use
of software component library, operating
systems can be updated and extended
according to the different embedded system
platforms and to meet the specific applications’ 
needs. Dynamic component update is a
mechanism that allows component-based
operating systems to update components
on-the-fly. Operating systems can be patched
or added extra functionalities without the need
of rebooting the machines. To fully support and
provide the dynamic update service, the
component server and component library are
also very important.
The purpose of this project is to develop a
system platform to support the dynamic
component update and on-line reconfiguration
in an embedded operating system. We use our
component-based embedded operating system -
LyraOS as the research platform, and enhance
its dynamic component update functionality to
become more extensible, flexible, and efficient.
Such that, updates can be safely made to not
only kernel/user codes and data structures but
also component exported interfaces. We have
also developed a component server and
3功能，在元件需要被呼叫或存取時才做下
載。這樣可確保只有真正被需要的元件才會
存在於嵌入式系統上，讓系統的資源可以做
更有效的運用。同時，我們亦製作支援此元
件動態更新的元件伺服器 (Component
Server)與元件庫 (Component Library)，以支
援元件動態更新的功能。
除此之外，由於快閃記憶體 (Flash
Memory) 具備輕、薄、省電、耐震、不具揮
發性、存取速度快、再加上價格日漸合理等
優點，廣泛的被使用在行動電腦、嵌入式系
統以及各種消費性電子產品之上，諸如：數
位相機、MP3 Player、隨身碟、Set-Top
Boxes、行動電話、個人數位助理 (PDAs)、
印表機、網路設備、以及各種掌上和筆記型
電腦之上，快閃記憶體已成為嵌入式系統主
要的儲存設備，因此，如何設計一個好的
Flash File System 以有效的存取快閃記憶體
也成了嵌入式系統軟體一個重要的課題。
然而，快閃記憶體通常被廠商分割為一
個個 Erase Unit，而 Erase Unit 中的資料無
法直接更改，在寫入資料之前必須先將原有
的資料抹除 (Erase)。但因為 Erase 的動作
不僅費時而且耗電，加上每個 Erase Unit 的
Program/Erase cycle 的次數有限 (通常為
10,000 至 1,000,000 次)，所以必須盡量設
法避免或減少每個 Erase Unit 被抹除的次
數，以增加其使用的壽命。此外，抹除的動
作是以整個 Erase Unit 為單位，而根據廠商
的不同，Erase Unit 的大小並不相同，資料
寫入單位的大小亦不盡相同。因此，在進行
抹除動作之前必須將 Erase Unit 中仍屬有
效的資料移到別處儲存，這也增加了快閃記
憶體在管理上的複雜度。
因此，我們亦針對 NAND-type 快閃記
憶體來設計與實作一快閃記憶體儲存系統
之元件，儲存於元件庫，並針對快閃記憶體
的 特 性 ， 探 討 其 清 除 策 略 (Cleaning
Policy)，降低抹除與資料寫入的動作，以有
效地提升快閃記憶體儲存系統之效能，進而
延長快閃記憶體的壽命和達到省電的功效。
三、文獻探討
國內外有許多大學及研究機構致力於
作業系統核心架構的探討。其中動態更新以
及記憶體保護皆是常見的議題。關於動態更
新以及記憶體保護機制，目前國內外較有具
體研究成果者敘述如下：
華盛頓大學的 SPIN 可以下載一般程
式碼到核心程式內以擴充原有作業系統之
功能。然而 SPIN 並沒有真正區分、制訂系
統軟體元件，也不允許應用程式下載系統元
件，以及剔除不需要的元件。哈佛大學的
VINO 與 SPIN 類似，不同的只是它們保護
核心程式的方式。SPIN 使用一個安全的程
式語言 Modula 3 ，而 VINO 則是靠
Software Fault Isolation（SFI）及 Transaction
技術。
Linux 則提供 Loadable Kernel Module
（LKM）的機制，可以在系統運行時，透過
線上新增及移除 Kernel Module，增加系統
核心的延展性。然而，Linux 的 Loadable
Kernel Module 卻無法針對 Kernel Module
做線上的更新，也無法移除正在使用中的
Kernel Module，Kernel Module 的置換只能
在 Kernel Module 沒在使用的情況下，移除
然後再新增。此外，被載入核心的 Kernel
Module 彼此間也並沒有保護機制的存在，
所有 Kernel Module 跟核心一樣擁有系統
的最高權限。
MIT 大 學 所 提 出 的 Mondriaan
Memory Protection（MMP），是一個結合硬
體與軟體的記憶體保護架構，此架構透過在
CPU 內 新增一個叫 MMP 的硬體，並在
作 業 系 統 內 實 作 一 層 名 為 Memory
Supervisor 的 MMP 硬體抽象層跟作業系
統作結合，可以在對現有作業系統最小的影
響下，提供一個有效的記憶體保護方式。針
對 Linux 等提供模組載入或是動態元件更
新的作業系統，可將 Kernel Modules 或各
個 系 統 元 件 切 割 到 不 同 的 Protection
Domains，有效的提供原先作業系統核心內
部所沒有的保護功能。然而，雖然 MMP 的
作者認為 MMP 會是未來 CPU 內的一個
標準，但在目前嵌入式硬體環境下，針對特
殊硬體所設計出來的功能，其應用仍然有
限。
Bell Labs 的 Pebble 使 用 了
Microkernel 的架構，並只用了元件化的架
構，將大部分的系統元件移到 User Level，
5圖一：LyraOS 元件化作業系統
圖二：LyraOS 系統架構圖
而隨著嵌入式系統及網路技術發展的
突飛猛進，嵌入式硬體的功能可說是日漸強
大，提供網路介面的嵌入式硬體以及各種網
路相關的應用也是越來越多，靜態的元件重
組已經無法滿足嵌入式應用的多元化需
求，提供元件化嵌入式作業系統動態元件更
新的功能是目前研究中的重點。此外，為了
面對未來複雜的多媒體系統環境，以及處理
動態元件更新對系統所可能造成的危害，提
供元件化作業系統一個適合的記憶體保護
機制也是新一版 LyraOS 的設計重點。
我們選用 ARM Integrator/CP920T 來當
我們的嵌入式硬體平台，如圖三，其包含了
Integrator Compact Platform baseboard 及
Integrator ARM920T Core Module。選用
ARM920T 當核心處理器是因為我們利用
其 MMU（Memory Management Unit）來達
到所需的記憶體管理及保護等功能。
圖三：ARM Integrator/CP920T
在國科會計畫的支持下，我們在
LyraOS 內設計並研究如何實作動態元件更
新機制，以及元件間的保護機制，除了需解
決動態元件更新對系統安全所可能造成的
危害，對系統效能所造成的影響，也針對嵌
入式系統環境的限制，做資源的有效運用。
我們首先進行 LyraOS 於目標嵌入式
硬體平台的移植工作，除了 bootloader 的
準備和開發環境的轉換外，需針對硬體環境
改寫 LyraOS 的硬體抽象層的相關函式及
元件，需要新增或修改的地方包括：LyraOS
的啟動與載入、開機程序元件的移植、CPU
基本功能元件的移植、螢幕輸出元件的移
植、實體記憶體管理元件的移植、中斷處理
元 件 的 移 植 、 Context Switch 、 Thread
Create、Timer Interrupt Driver、NIC（Network
Interface Card）Driver、…等。
關於元件間記憶體保護機制的設計與
實作方面，我們主要是利用在各種處理器常
見的 MMU 所提供的 Paging 以及記憶體
保護等機制，配合上作業系統核心的控制，
希望將各個元件切割成不同的 Protection
Domains，讓各個元件間的 Address Space
彼此獨立，設計出一個適用於元件化嵌入式
作業系統的記憶體保護機制。
關於動態元件更新機制的設計與實作
方面，我們在元件化嵌入式作業系統上，探
討適合於嵌入式環境下的動態元件更新架
構，及元件間的保護方式，設法解決動態更
新可能對系統安全所造成的危害，及其它衍
生出的問題。
Vega
Microkernel:
(HAL+minimum kernel core)
Configuration
tool
Small system
Medium system
Large systemcomponents
Hardware (ARM/ AMD Elan SC400 / x86 PC)
Kernel Core
Components
LyraNET
Networking
LyraDD Device Driver
LyraFILE
File System
Java Virtual Machine
POSIX subset API
Embedded Browser /Desktop
LyraOS Framework
API
Hardware Abstraction Layer
7在元件的置換方面，元件的置換可以用
來更新系統內的元件，拿 Task Scheduler 為
例子，不同的作業系統使用的排程演算法
(Scheduling Algorithm) 不盡相同，但是根據
應用程式的需求，我們可能在不同的時機
下，需要使用不同的排程演算法，此時我們
便可動態下載新的排程元件，透過元件的置
換，將原本系統內舊有的排程元件置換成新
的。但是在元件置換的中間，元件狀態轉換
(State Transfer) 和如何查探出元件置換的
安全點 (Safe Point)，便是元件置換的主要
課題。
支援元件動態更新的元件伺服器與元件庫
的設計與實作
在嵌入式系統資源有限的情況下，為了
不對嵌入式設備造成負擔，我們過一個存放
各種元件的元件伺服器，在系統有需要額外
的元件時，提供動態下載的功能，接著再透
過我們設計的動態更新機制，線上新增、置
換、及移除不必要的元件。此外，我們提供
Demand-loading (Load on Call) 的功能，讓
真正執行到該元件時才做下載更新的動
作，這樣可確保只有真正需要的元件才會存
在於嵌入式系統上，讓系統的資源可以做更
有效率的運用。因此，我們製作支援此元件
動態更新的元件伺服器與元件庫，支援動態
更新的線上更新與重組的功能，以完整的提
供動態元件更新的功能。
針對嵌入式環境的限制，我們開發的元
件 動 態 更 新 機 制 基 本 上 屬 於 一 個
Client/Server 的架構，元件在元件伺服器
(Server) 端做好 Linking，下載到嵌入式設
備 (Client) 端後，由 LyraOS 的核心負責載
入，Relocation 的部份則藉由 ARM 硬體架
構 內 所 提 供 的 Fast Context Switch
Extension (FCSE) 機制來做處理。透過此種
Server-Side Pre-Linking 的方式，Client 端的
作 業 系 統 只 需 有 個 簡 單 的 Absolute
Loader ， 而 不 需 提 供 複 雜的 Dynamic
Loader/Linker，也不需在 Client 端保留
Kernel 以及 User Library 的 Symbol Table
來提供鏈結。如此，不但降低了嵌入式系統
內記憶體的使用量，更降低了元件載入時對
系統所造成的負擔，耗電量也會因此減少。
關於載入的元件型態，我們將載入的元
件 分 為 Trusted Component (Kernel
Component)和 Un-trusted Component (User
Component) 兩種。Trusted Component 擁有
跟作業系統核心一樣的權限 (i.e.在 Kernel
Mode 執行)，可以直接呼叫/存取系統內各個
元件所釋出的介面，以及核心內的全域變
數；Un-trusted Component 則是執行在 User
Mode，可以避免元件執行時任意存取/修改
其他 Kernel/User Component 的資料，危害
系統的安全。
由 於 我 們 採 用 了 Server-Side
Pre-Linking 的方式，我們需在元件伺服器
端保留了 Client 端的 Kernel Symbol Table
和 User Library Symbol Table，裡面包含了
Kernel 和 User Library 所 Export 的函式以
及全域變數等在記憶體內的位址 (Virtual
Address)，分別提供給 Trusted Component
以及 Un-trusted Component 做 Linking。如
圖五所示，(1) 在 Client 端向元件伺服器端
發出 Request 後，(2) 元件伺服器端會根據
Client 端找出相對於 Client 端的 Kernel /
User Library Symbol Table，然後至元件庫內
找出所要下載的元件，依照元件的型態，
Trusted Component 則 直 接 跟 Kernel
Symbol Table 做 Linking ， Un-trusted
Component 則跟 User Library Symbol Table
做 Linking，最後產生 Linked Component
Image， (3) 然後將此 Linked Component
Image 傳 回 給 Client 端 ， 完 成 整 個
Server-Side Linking 的動作。然而，相同的
元件和相同的 Client Kernel / User Library
Symbol Table 也只需做一次 Linking，每次
有同樣 Symbol Table 的 Client 下載相同
的元件時，便可直接傳回 Linked Component
Images，省去再做一次 Linking 的動作。
圖五：Server-side Pre-Linking
9(5) 快閃記憶體儲存系統元件的設計與實
作。
(6) FTL-based 快閃記憶體儲存管理系統的設
計與實作。
(7) FTL-based 快閃記憶體儲存系統快速清除
策略的設計與實作。
(8) 系統整合及測試、效能評估。
關於元件動態更新機制，我們將來希望
達成不僅能針對新舊元件的程式碼做置
換，並將所有相同類別所衍伸的物件做更新
的目標。另外，如果更新失敗時，我們也希
望提供復原的功能（Failure Recovery），讓
作業系統能夠繼續的正常運作。
此外，元件版本間的問題也是我們所需
要考量的，眾多元件彼此間也經常會有相依
性的問題存在，某些元件也可能需要特定版
本的元件才能運作，當所下載或更新的元件
還需要下載或更新其他元件才能運作的情
況，我們也需要解決，除了將有相依性的元
件全部下載更新外，考量下載後不一定會執
行到的情況下，我們也讓可以讓元件達到
Demand-loading 的功能，讓真正執行到該元
件時才做下載更新的動作，如此一來，只有
真正需要的元件才會存在於嵌入式設備上。
六、參考文獻
[1] Bryan Henderson, “Linux Loadable Kernel 
Module HOWTO,” September 2006. 
http://www.tldp.org/HOWTO/Module-HOWT
O/.
[2] LyraOS homepage,
http://163.22.34.199/joannaResearch/LyraOS/
index.htm.
[3] Chi-Wei Yang, C. H. Lee, and R. C. Chang,
“Lyra: A System Framework in Support-ing
Multimedia Applications,” in Proceedings of 
IEEE International Conference on Multimedia
Computing and Systems’99, Italy, June 1999.
[4] Z. Y. Cheng, M. L. Chiang, and R. C. Chang,
“A Component Based Operating System for 
Resource Limited Embedded Devices,” in 
Proceedings of IEEE International Symposium
on Consumer Electronics (ISCE’2000), Hong 
Kong, Dec. 5-7, 2000.
[5] eCos, http://sources.redhat.com/ecos/.
[6] MicroC/OS-II, http://www.ucos-ii.com/.
[7] Mei-Ling Chiang and Ching-Ru Lo,
“LyraFILE: A Component-Based VFAT File
System for Embedded Systems,” International 
Journal of Embedded Systems, Vol. 2, Nos.
3/4, pp. 248-259, 2006.
[8] Chun-Hui Chen, LyraDD: Design and
Implementation of the Device Driver Model
for Embedded Systems, Master Thesis,
Department of Information Management,
Na-tional Chi-Nan University, June 2004.
[9] Chi-Wei Yang, Paul C. H. Lee, and R. C.
Chang, “Reuse Linux Device Drivers in 
Embedded Systems,” in Proceedings of the 
1998 International Computer Symposium
(ICS’98), Taiwan, 1998.
[10] Mei-Ling Chiang and Yun-Chen Lee,
“LyraNET: A Zero-Copy TCP/IP Protocol
Stack for Embedded Systems,” Real-Time
Systems, Vol. 34, No. 1, pp. 5-18, Sep. 2006.
[11] David A Rusling, The Linux Kernel,
http://www.tldp.org/LDP/tlk/tlk.html, 2002.
[12] Tool Interface Standard - TIS, Executable
and Linking Format (ELF) Specification,
Version 1.2, 1995.
http://www.x86.org/ftp/manuals/tools/elf.pdf.
[13] K. Sollins, The TFTP Protocol (Revision 2),
July 1992. http://www.ietf.org/rfc/rfc1350.txt.
[14] Steve Furber, ARM System-on-chip
Architecture, Second Edition,
Addison-Wesley, ISBN 0-201-67519-6, 2000.
[15] David Seal, ARM Architecture Reference
Manual Second Edition, Addison-Wesley,
ISBN 0-201-73719-1, 2001.
[16] A. Wiggins and G. Heiser, “Fast 
Address-Space Switching on the StrongARM
SA-1100 Processor,” in Proceedings of the 5th 
Australasian Computer Architecture
Conference, Australia, January 2000.
[17] Adam Wiggins, Harvey Tuch, Volkmar
Uhlig, and Gernot Heiser, “Implementation of 
Fast Address-Space Switching and TLB
Sharing on the StrongARM Processor,” in 
Proceedings of the 8th Asia-Pacific Computer
Systems Architecture Conference
(AC-SAC’03), Japan, September 23-26, 2003.
[18] Da-Wei Chang and Ruei-Chuan Chang, “OS 
Portal: an Economic Approach for Mak-ing an
Embedded Kernel Extensible,” Journal of 
Systems and Software, Vol. 67, No. 1, pp.
19-30, July 2003.
[19] Craig A. N. Soules, Jonathan Appavoo,
Kevin Hui, Robert W. Wisniewski, Dilma Da
Silva, Gregory R. Ganger, Orran Krieger,
Michael Stumm, Marc Auslander, Michal
Ostrowski, Bryan Rosenburg, Jimi Xenidis,
“System Support for Online 
Reconfigura-tion,” in Proceedings of USENIX 
2003 Annual Technical Conference, pp.
A Server-side Pre-linking Mechanism for Updating 
Embedded Clients Dynamically 
Bor-Yeh Shen1 and Mei-Ling Chiang2 
1Department of Computer Science, 
National Chiao Tung University, Hsinchu, Taiwan, R.O.C. 
2Department of Information Management, 
National Chi-Nan University, Puli, Taiwan, R.O.C. 
byshen@cs.nctu.edu.tw, joanna@ncnu.edu.tw 
Abstract. To allow embedded operating systems to update their components 
on-the-fly, dynamic update mechanism is required for operating systems to be 
patched or added extra functionalities in without the need of rebooting the 
machines. However, embedded environments are usually resource-limited in 
terms of memory size, processing power, power consumption, and network 
bandwidth. Thus, dynamic update for embedded operating systems should be 
designed to make the best use of limited resources. In this paper, we have 
proposed a server-side pre-linking mechanism to make dynamic updates of 
embedded operating system efficiently. Applying this mechanism can reduce 
not only memory usage and CPU processing time for dynamic update, but also 
data transmission size for update components. Power consumption can be 
reduced as well. Performance evaluation shows that compared with the 
approach of Linux loadable kernel modules, the size of update components can 
be reduced about 14-35% and the overheads in embedded clients are minimal. 
Keywords: Embedded System, Operating System, Dynamic Update, Modules, 
LyraOS. 
1   Introduction 
Dynamic update allows operating systems to update their components on-the-fly 
without rebooting the whole systems or stopping any system services. This opens up a 
wide range of opportunities: fixing bugs, upgrading services, improving algorithms, 
adding extra functionalities, runtime optimization, etc. Although many operating 
systems have already supported different kinds of mechanisms to extend their kernels, 
they usually do not aim at resource-limited environments. For instance, Linux uses a 
technique called loadable kernel modules (LKMs) [1]. By using this technique, Linux 
can load modules, such as device drivers, file systems, or system call to extend the 
kernel at run time. However, LKMs may take lots of overheads in embedded 
environments. Since embedded systems are usually resource limited, in order to keep 
the added overheads minimal while providing dynamic update in an embedded 
 
Fig. 1.  LyraOS system architecture 
3   Related Work 
Linux Loadable Kernel Modules (LKMs) [1] are object files that contain codes to 
extend the running kernel. They are typically used to add support for new hardware, 
file systems, or for adding system calls. When the functionality provided by an LKM 
is no longer required, it can be unloaded. Linux uses this technology to extend its 
kernel at run time. However, Linux modules can be removed only when they are 
inactive. Another problem of LKMs is its space overheads. It needs additional kernel 
symbol table in client site and additional symbol table in loadable modules due to 
dynamic symbol linking. Dynamic symbol linking also takes lots of time during 
module loading. Our approach can eliminate these overheads. Besides, the LKMs 
require privilege permission to perform kernel modules loading. All of these modules 
are located in the kernel level and have the same permission as kernels. Thus, 
operating systems may crash because a vicious module is loaded in the kernel. 
In operating system portal (OSP) [2], all the dynamically loadable modules are 
located on the server host. A user-level process is responsible for loading, linking and 
transmitting these modules to the clients. A kernel-level module manager is installed 
on the client to make the client kernel extensible. The server-side linking mechanism 
proposed in OSP is similar to our server-side pre-linking mechanism. Unlike the OSP 
framework, our server-side pre-linking mechanism can perform component linking on 
the server-side previously. We do not have to know the starting address of 
components on each client host because components will be relocated by client’s 
relocation hardware. Thus, the components processing time on server hosts can also 
be saved since we do not need to link components for each request of clients. 
SOS [7] is a dynamic operating system for mote-class sensor nodes. It uses 
dynamically loadable software modules to create a system supporting dynamic 
addition, modification, and removal of network services. The SOS kernel provides a 
set of system services that are accessible to the modules through a jump table in the 
4.1   Server-side Pre-linking 
Since embedded environments are usually resource-limited, we implement the server-
side component pre-linking mechanism to keep the imposed overheads minimal while 
providing dynamic component update in an embedded operating system. 
As mentioned above, components in our design and implementation have been 
linked on the server-side before components are requested by embedded clients. 
These components are linked according to their types (i.e., trusted or un-trusted) and 
symbol tables of embedded clients. The trusted component will be linked with the 
kernel symbol of the embedded client while the un-trusted one will be linked with the 
user library symbol table of the client. We do not need to know where the component 
will reside in the client-side memory (i.e., the starting address of the component). All 
of the updatable components will be linked at the same starting virtual address 
through the linker script we defined. Then the components will be relocated by the 
client-side relocation hardware that we will describe later. Because the updatable 
components can be linked previously, we can save the component processing time on 
server-side. 
Figure 2 shows our server-side pre-linking architecture. In our system, there is a 
component server on the server-side responsible for handling client requests. The 
component server on the server host receives request from the embedded client 
kernels and performs tasks as follows. If a pre-linked component is found in the pre-
linked component storage, the component server will send the pre-linked component 
to the embedded clients immediately. Otherwise, the component server will link the 
components on demand. 
In our implementation, the server-side component pre-linking can save not only 
the memory and the disk storage on embedded clients but also the component 
transmitting time because we downgrade the sizes of updatable components. Besides, 
it eliminates the need for clients to perform dynamic linking. Furthermore, the power 
consumption of embedded devices can be also decreased. 
 
 
Figu. 2.  Server-side pre-linking architecture 
Table 1.  Component manager API 
Methods Descriptions 
CM::Add(name, ver) The CM::Add() method adds a new component name with version ver from a remote component server and returns component’s ID. 
CM::GetCID(name, ver) The CM::GetID() method returns component ID of component name (version ver). 
CM::Invoke(cid, mid, arg) The CM::Invoke() method invokes a method mid of a component cid and passes arguments arg through the component manager. 
CM::Register (mid, fptr) The CM::Register() method registers method’s ID mid and its address fptr to the component manager. 
CM::Remove(cid) The CM::Remove() method removes component whose ID is cid. 
CM::Update(old, new) The CM::Update() method updates a component from component ID old to component ID new. 
 
Figure 4 shows our component interface. This function is implemented by 
developers and will be linked as the entry point of updatable components during 
server-side pre-linking. Every updatable component has to implement this interface to 
register its methods and transfer its states. As the component jumps to the entry point, 
the component will invoke the register method to register its exported methods to the 
component manager. Therefore, other components can invoke these methods through 
the component manager without using static component interface. When we want to 
remove a component, all of the component information including current states of the 
component and function pointers of the component exported methods should be 
removed. Methods in Table 1 provide a component communication interface. In our 
system, components must communicate with each other through the component 
manager. This is because we provide dynamic component exported interface in our 
system and these interfaces of components are managed by the component manager. 
4.3   Component Relocation 
The component relocation in our system implementation takes advantage of the ARM 
fast context switch extension (FCSE) mechanism [10]. The FCSE is an extension in 
the ARM MMU. It modifies the behavior of an ARM memory translation. This 
modification allows our components to have their own first 32MB address space. 
Thus, we make each component have its own address space and relocate in the first 
32MB of memory. As shown in Figure 5, there is only one page table in our system. 
The 4GB virtual address space is divided into 128 blocks, each of size 32MB. Each 
block can contain a component which has been compiled to use the address ranging 
from 0x00000000 to 0x01FFFFFF. Each block is identified with a 7-bit PID (Process 
ID) register. Through the FCSE mechanism, we can switch between components’ 
address spaces by changing the PID register and do not have to flush caches and 
TLBs. The same functionality can be achieved by other architectures which provide 
paging and an address space identifier (ASID) found on many RISC processors such 
as Alpha, MIPS, PA-RISC, and SPARC. 
un-trusted components run in user mode, they may also have vicious codes to affect 
other un-trusted components. Therefore, they should locate in different protection 
domains and use the client access types. Thus, we can avoid the situation that the 
current un-trusted components will be affected as we load a new un-trusted 
component into our system. Although ARM only supports 16 domains which may be 
less than the number of un-trusted components concurrently in our system, we can 
apply other approaches such as domain recycling [11,12] to resolve this problem. 
5   Performance 
This section presents the performance evaluation of dynamic component update in 
LyraOS. We compare the space overheads of our architecture with the Linux loadable 
kernel modules. The experimental environment consists of a client and a server host 
that are connected via a 100 Mbits/sec Ethernet. The server host is a Pentium 4 
3.2GHz machine with 1GB RAM, running Linux 2.4.26. The client host is an ARM 
Integrator/CP920T development board with 128 MB RAM, running LyraOS 2.1.12. 
5.1   Comparison of Space Overheads 
Table 2 shows the loader sizes of the client kernel. We compare the sizes of LyraLD 
to the sizes of Linux LKMs linker/loader under kernel version both 2.4 and 2.6. The 
fundamental difference between Linux 2.4 and Linux 2.6 is the relocation and linking 
of kernel modules are done in the user level or kernel level. Loadable kernel modules 
in Linux are ELF object files which can be loaded by a user program called insmod. 
In Linux 2.4, insmod does all the work of linking Linux kernel module to the running 
kernel. While the linking is done, it generates a binary image and then passes it to the 
kernel. In Linux 2.6, the insmod is a trivial program that only passes ELF objects 
directly to the kernel, and then the kernel does the linking and relocation. In Table 2, 
the Linux 2.4 module linker/loader shows the static and dynamic size of the insmod 
program on Linux 2.4.26. The Linux 2.6 module linker and module loader are 
measured from the object files of kernel/module.c and kernel/kmod.c in the Linux 
2.6.19 source tree. All symbols in these programs and object files have already been 
stripped. From the table we can see that, the size of LyraLD is less than 1% of the 
module linker/loader under Linux 2.4 and is about 7% of the module linker/loader 
under Linux 2.6. 
Table 2.  Sizes of loaders 
Loader Size  
Linux 2.4 module linker/loader 618,712 bytes 133,140 bytes 
(static linked) 
(dynamic linked) 
Linux 2.6 module linker 
Linux 2.6 module loader 
14,088 bytes 
2,060 bytes 
(kernel/module.o) 
(kernel/kmod.o) 
LyraLD (LyraOS loader) 1,140 bytes  
5.2   Component Loading/Pre-linking Time 
Table 5 shows the component client-side loading and server-side pre-linking time of 
those components we described above. The component loading only takes a few 
milliseconds. From the Table 4 and Table 5, we can see that the component loading 
time is not related to the sizes of the components. This is because the loader has to 
initialize some of the ELF sections. For example, BSS is a memory section where un-
initialized C/C++ variables are stored. If there is a BSS section in a component, it 
needs clear to zero while the component is loaded into memory. Besides, from the 
server-side pre-linking time we can see that embedded clients save lots of linking time 
when new components are loaded since the linking has been done previously on the 
server. We should know that the server-side pre-linking runs on a Pentium4 3.2GHz 
machine, and the frequency of ARM920T processors is only about 200MHz. It could 
cause large overheads if the component linking is performed on the embedded clients. 
5.3   Component Invocation Time 
In Figure 6, we invoke a method of each component we described above. “Direct 
Invocation” measures the invocation time of the direct component invocation. That is, 
direct component invocation invokes methods directly without calling the component 
manager and system calls. “Trusted Component” measures the invocation time of the 
trusted component invocation through the component manager. “Un-trusted 
Component” measures the invocation time of the un-trusted component invocation 
through the system call and the component manager. From the figure we can see that, 
it only adds a few overheads by providing dynamic component exported interface and 
memory protection for un-trusted components. Besides, relocation by hardware also 
keeps the overhead of switching between components’ address space minimal. 
 
0
10
20
30
40
50
60
70
80
90
Semaphore Signal Serial driver Timer driver Interrupt handler Task scheduler
Components
Inv
oc
ati
on
 T
im
e (
μ
s)
Direct Invocation Trusted Component Un-trusted Component
 
Fig. 6.  Component invocation time 
A New FTL-based Flash Memory Management Scheme with Fast Cleaning
Mechanism
Mei-Ling Chiang Chen-Lon Cheng Chun-Hung Wu
Department of Information Management,
National Chi-Nan University, Puli, Taiwan, R.O.C.
{joanna, s94213528, s95213502} @ncnu.edu.tw
Abstract
Due to the advantages of non-volatility, lightweight,
low power consumption, and shock resistance, flash
memory has been widely used as the storage of
embedded systems and mobile devices. However,
unlike hard disk, flash memory does not support
update-in-place operation, and each block on flash
memory has the limited erasure cycles. Therefore, flash
memory needs a different storage management scheme
designed specifically for flash memory characteristics.
Many researches adopt the log-based approach which
needs an efficient cleaning mechanism to reclaim the
storage space occupied by obsolete data.
In this paper, we have designed and implemented a
new flash translation layer for flash memory
management and proposed a fast cleaning mechanism
for space reclamation. It is based on the DAC
technique to separately cluster hot and cold data in
flash memory. For the NAND-type flash memory with
large capacity, it maintains multiple LRU lists to
reduce the time of selecting blocks for erasure in
cleaning activity. Moreover, we modify the cost-benefit
policy with different weight that considers the attribute
of each block on flash memory in selecting a block for
cleaning. Simulation results show that our proposed
flash memory management scheme with the proposed
fast cleaning mechanism could efficiently decrease the
number of erase operations, speed up the cleaning
activity, and enhance system performance.
1. Introduction
Nowadays, embedded systems and mobile devices
have been rapidly developed. Because flash memory
has many advantages such as lightweight, low power
consumption, and shock resistance, it has been widely
used in those devices. However, unlike hard disk, flash
memory could not support update-in-place operation, it
must perform erase operation before data in flash
memory could be overwritten. This erase operation
would increase system overhead. Additionally, the
erasure unit is a block in flash memory, and the number
of erasing cycles for flash memory blocks is limited.
Different manufacturers of flash memory may also have
different number of erasing cycles. Uneven erasing
cycles for each block on flash memory would shorten
flash memory lifespan. Therefore, how to reduce erase
times of flash memory blocks and make flash memory
blocks be erased evenly are the key issues to reduce
system overhead and prolong flash memory lifespan.
Two methods of flash memory management are
often used in designing flash-memory based storage
systems. The first one is to design the file system for
flash memory characteristics, and it is called native
flash file system, such as JFFS [1], YAFFS [2], and
MFFS [3]. Most of them adopt the log-based approach
to overcome limitations of flash memory. The design
based on the log-based approach also has the advantage
of developing the capability of fast crash recovery.
Another method is to use flash translation layer (FTL)
[4] that is a software layer between file systems and
flash memory. The main function of FTL is to make
flash memory emulate block devices and translate the
logical addresses of block devices to physical addresses
of flash memory. Because FTL can make use of the
existent file systems and needs not redesign a new file
system, so FTL is widely used on mobile devices.
Because of the non-update-in-place characteristic of
log-based flash memory management, the design of
flash-memory based storage systems needs a cleaning
mechanism to reclaim space occupied by invalid data.
How to perform cleaning activity is an important issue.
The cleaning policy determines when to start or stop
the cleaning operation, which block should be selected
for cleaning, how many blocks should be cleaned in
each cleaning activity, and where to rewrite the valid
data in the cleaned block. Those operations would
affect the performance of flash-memory based storage
systems. An efficient cleaning policy could decrease
the number of erase operations and the number of valid
data pages copied during cleaning, and it could also
Figure 1: Architecture of MTD subsystem
2.3. Flash translation layer
Many researches [4,7,8] use the Flash Translation
Layer (FTL) to redirect logical blocks in file systems to
physical pages in flash memory by building up a
translation table. Then any read and write requests in a
file system could accurately access data stored on flash
memory via the translation table.
The logical-to-physical address translation
mechanism can be divided into two types: page-level [4]
and block-level [9]. The differences are the memory
required by translation table and address translation
time. The page-level translation scheme maps each
logical sector number to each physical page number.
The block-level translation scheme divides the logical
sector number into logical block number and logical
page offset, and stores the mapping information of
logical block number to physical block number in
translation table. Thus, using block-level translation
scheme to access data on flash memory needs extra
operations to translate logical block number and logical
page offset to physical address. Whereas, using page-
level address mapping would consume larger amount
of memory for the translation table.
3. Related Work
3.1. FTL-based flash storage systems
True flash file system (TrueFFS) [8] enables flash
memory to emulate hard disks and support disk-based
file systems by using a software block device driver.
This driver divides flash memory into numerous fixed-
sized blocks for efficient management and is
responsible for translating disk sectors or blocks to
flash memory blocks. To avoid uneven block erasure
on flash memory, TrueFFS uses the dynamic wear-
leveling that uses statistical allocation for newly written
data, and uses the static wear-leveling that makes static
data be written to high erase cycles of blocks.
Kawaguchi et al. [7] designed a flash memory
device driver to support conventional UNIX file
systems transparently. It adopts a sequential log data
structure and modifies the cost-benefit policy of LFS
by using a different cost measure for flash memory. In
particular, it separates cold and not-cold blocks during
cleaning, and uses two separate segments for cleaning
to avoid mixing the data blocks with different attributes.
Experimental results show that separate segment
cleaning has notable effect on performance.
Kim et al. [9] proposed a log block scheme to
combine the advantages of coarse-grained (i.e. block-
level) and fine-grained (i.e. page-level) address
translations for mass storage CompactFlash systems.
So it could efficiently handle small-sized writes and
long sequential writes. By storing multiple updates of
translation information with a single atomic write
operation to a dedicated flash memory region, the
consistency of translation information and system fast
startup can be achieved by scanning this region.
Chang et al. [10] proposed a flexible management
scheme for large-scale flash memory storage systems
through investigating the behaviors of realistic access
patterns on high-capacity disks. They also proposed a
tree-based management scheme with variable
allocation granularities and an efficient logical-to-
physical address translation method.
3.2. Mechanisms for Hot-and-cold data
identification and separation
The purpose of hot-and-cold data separation is to
reduce the overhead of coping valid data during
cleaning [5,7,13]. The hot data generally indicates its
access times are higher than others; otherwise the data
is cold. Because the hot data is written frequently, the
hot data would soon become obsolete and invalid.
Therefore, a block mixed with hot data and cold data
may contain valid data and invalid data at the same
time. If a block to be reclaimed also contains valid data,
and the cleaning activity needs to copy valid data to
other free flash memory space. This operation would
cause extra system overhead. If a block is full of hot
data, these hot data would soon be updated and thus
become obsolete and invalid together. Cleaning this
The Cleaning Index Cycle-Leveling (CICL) policy
[14] integrates wear-leveling into the cleaning activity
to avoid occurring some blocks with least worn-out on
the flash memory. Especially, it uses a devised cleaning
index to determine which block to be reclaimed.
Adaptive cleaning policy [15] adopts greedy policy
to reclaim blocks for uniform access and CAT policy
for localities of reference. Its goal is to achieve better
cleaning performance for the different access behaviors.
3.4. Other related work
The eNVy [16], a large flash-memory-based storage
system, takes the flash memory as a linear memory
array rather than an emulated disk. It uses a large flash
memory and a small battery backed SRAM to provide
in-place update semantics by using the copy-on-write
and page remapping techniques. It also uses the hybrid
cleaning that combines the advantages of FIFO and
locality gathering to minimize the cleaning cost for
uniform access and high localities of reference.
Wu et al. [17] designed an efficient initialization
and crash recovery of flash file system with the log
structure. Yim et al. [18] also proposed a method for
fast instant start-up to reduce the mounting time of file
system in flash memory. Park et al. [19] proposed a
technique with the log block FTL algorithm to improve
the robustness of the FAT file system. Chang [20]
proposed the dual-pool algorithm for wear leveling. It
does not require complicated tuning, and could respond
to the changes of spatial locality in workload.
Reliable Flash File System (RFFS) [21] is
developed for embedded systems with NAND flash
memory. It reduces the mounting time with its specific
flash memory management scheme and the recovery
overhead occurring in power failure or unpredictable
obstacle. Core Flash File System (CFFS) [22] is
designed for the file system characteristics of different
access patterns and file usage patterns. It uses a
pseudo-hot-cold separation scheme which allocates
different blocks for metadata and data to reduce
garbage collection overhead.
4. System design and implementation
4.1. Architecture overview
We have designed and implemented a flash
translation layer with fast cleaning named FTL/FC for
flash memory management in embedded systems or
mobile devices. The architecture is shown in Figure 3.
When users perform a data access request, disk-based
file system (i.e. FAT, EXT3, etc.) would notify
FTL/FC of the logical address of data in file system.
Then, FTL/FC would translate the logical address to
physical address and find the corresponding position on
flash memory. It then performs read/write/erase
operations through flash memory driver. FTL/FC also
must decide what kind of data placement policy and
which cleaning policy to use for reducing cleaning cost
and prolonging flash memory lifespan.
Figure 3: Architecture of FTL-based flash
memory management
4.2. Data layout on flash memory
In a NAND-type flash memory, each page typically
contains 512 bytes of data, and an extra 16 bytes for
recording information of per page. Figure 4 depicts the
structure of our stored information in each flash
memory page. The logical_addr field stores the logical
block number of written data. The age field indicates
the time when the page is updated and marked as
invalid. The page_status field indicates the page status,
such as free, valid, or invalid. The region_attr field
indicates which region the page belongs to.
Figure 4: Per page structure in flash memory
Because the unit of erasure in flash memory is a
block, we build per block structure as shown in Figure
5 in RAM for the convenience of flash memory
management. The blk_stauts field indicates the block
status, such as free and in_used. The blk_number field
indicates the block number on flash memory. The
Table 1 shows the different types of LRU lists in our
FTL/FC. For each region on flash memory, when the
system writes data to the region, it would write the data
to a free block which is allocated from the free list and
move the block to valid list if no free page is in the
valid list. Otherwise, the system would write the data to
the free page of the block in the valid list.
Table 1: Different types of LRU lists
List Type Description
Valid List Each block contains all valid pages.
Clean List Each block contains all invalid pages.
Dirty List Each block contains at least an invalid
page. Each dirty list is maintained in
LRU order.
Free List Each block has been erased and marked
free.
Cleaning Policy
For the selection of a block to reclaim, we first
select a block from the clean list. Because the pages of
blocks in clean list are all invalid, the cleaning cost is
the lowest. If there are no blocks in the clean list, we
would select a block from one of the dirty lists in the
three regions. However, if there are too many blocks in
the three regions, selecting a suitable block to be
reclaimed would take lots of time to calculate each
value of blocks according to the formula of the
designated cleaning policy. Therefore, to avoid this
problem, we only select a victim block among the first
blocks of each dirty list in the three regions. Then we
adopt the cost-benefit policy whereas we consider the
weight of blocks to calculate the fitness value of those
blocks using the formula: weight
u
uage 
2
)1( , where
u is the percentage of the valid data in the block and
the age is the time since the most recent modification
(i.e., the last block invalidation). Subsequently, we
would select the block with the maximal value to
reclaim. The weight represents a value which we give
the blocks in the same region according to the attribute
of the region. If the blocks are in the hot region, we
would give a small weight. Because the data of blocks
in the hot region are updated frequently, the data have
higher opportunity to become invalid. Thus, we let the
block be given more time to accumulate more invalid
data for reducing cleaning cost and system overhead.
On the other hand, if the blocks are in the cold region,
the data have lower opportunity to become invalid, so
they are given a large weight. If the blocks are in the
neutral region, they would be given the weight value
smaller/larger than the weight of cold/hot regions.
5. Performance evaluation
5.1. Experimental environment
We have implemented a simulator which consists of
512 MB NAND-type flash memory in MTD [6]
module of Linux. Table 2 lists our experimental
environment and setting. We also implement 4 different
cleaning policies including greedy policy, cost-benefit
policy, CAT policy, and our proposed fast cleaning
policy. Moreover, for observing the effect of flash
memory utilization with different cleaning policies, we
also use these 4 cleaning policies to measure the
performance under different flash memory utilizations.
We use the number of erase operations, the number of
pages copied during cleaning, and the time of selecting
a block for cleaning to measure the performance of
FTL-based flash memory management systems.
Table 2: Experimental environment and setting
Experimental Environment NAND Flash [9,24]
CPU: Pentium 4 3.2GHz Block Size: 16 KB
Memory : 512MB Page Size: (512 + 16) B
Flash memory: 512MB Page read time: 35.9 us
OS: Linux 2.6.11 Page write time: 226 us
MTD module: blkmtd.o Block erase time: 2 ms
We modify the Postmark [23] benchmark to build a
workload generator which could create specific
percentage of initial data on flash memory and update
files according to the desired localities of reference.
The workload generator would generate workloads for
diferent localities of reference. The notation ‘x/y’ 
represents x% of all accesses goes to y% of the data
and (1-x)% goes to the remaining (1-y)% of the data.
Besides, the unit of written data is 4 Kbytes, and 512
Mbytes of data are written to flash memory totally.
5.2. Comparison of cleaning policies with flash
memory management based on DAC
In this experiment, the greedy policy, cost-benefit
policy, and CAT policy are tested on the original DAC
technique [5] with three regions. Our proposed fast
cleaning policy is tested on the DAC technique with
three regions, whereas, multiple LRU lists are used to
maintain block lists in each region and the proposed
weighted cost-benefit policy is used.
0%-19.63% fewer erasure, 1.7%-64.51% fewer pages
copied, 99.8% less time of selecting blocks for erasure,
and 0%-57.41% less cleaning time than the other
policies under 90/10 locality of reference and different
flash memory utilizations. Although the performance of
our proposed scheme is comparable with that of the
flash memory management with DAC/CAT, the
cleaning time of our proposed scheme is greatly less
than that of the flash memory management with
DAC/CAT.
5.3. Comparison of different flash memory
management schemes
We compare our proposed scheme with different
flash memory management schemes, such as none hot-
and-cold identification, two-level LRU, and DAC.
None hot-and-cold identification does not separate hot
and cold data. Two-level LRU separates data into hot
and cold by the DAC technique with two regions, and
the blocks in the same region would be sorted by the
time of last page invalidation. DAC indicates the data
would be separated into hot and cold data in three
regions. Besides, flash memory management schemes
with none hot-and-cold identification and two-level
LRU would adopt greedy policy. The flash memory
management scheme with DAC adopts CAT policy.
Our proposed fast cleaning is tested on the DAC with
three regions, whereas, multiple LRU lists are used to
maintain block lists in each region and the proposed
weighted cost-benefit policy is used.
Figures 12-14 show that our proposed scheme
incurs 0.6%-28.32% fewer erase operations, and 1.7%-
57.28% fewer pages copied, and 21.67%-65.17% less
cleaning time than the other flash memory management
schemes under 90/10 locality of reference and 90%
flash memory utilization.
0
10000
20000
30000
40000
50000
60000
70000
80000
50/50 60/40 70/30 80/20 90/10
Localities of reference
N
um
be
ro
fe
ra
se
op
er
at
io
ns
Greedy+None Greedy+2_LRU CAT+DAC Fast Cleaning+DAC
Figure 12: The number of erasure under
different flash memory management schemes
0
500000
1000000
1500000
50/50 60/40 70/30 80/20 90/10
Localities of reference
N
um
be
ro
fp
ag
es
co
pi
ed
Greedy+None Greedy+2_LRU
CAT+DAC Fast Cleaning+DAC
Figure 13: The number of pages copied under
different flash memory management schemes
0
200
400
600
800
1000
50/50 60/40 70/30 80/20 90/10
Locality of reference
T
im
e
of
pe
rf
or
m
in
g
cl
ea
ni
ng
(S
ec
on
d)
Greedy+None
Greedy+2_LRU
CAT+DAC
Fast Cleaning+DAC
Figure 14: The cleaning time under various
flash memory management schemes
As shown in Figure 15, under various flash memory
utilizations, our proposed flash memory management
with fast cleaning incurs 0%-28.32% fewer erase
operations, 1.7%-74.75% fewer pages copied, and 0%-
72.73% less cleaning time than the other flash memory
managements for 90/10 locality of reference.
0
100
200
300
400
500
600
700
50 60 70 80 90
Flash memory utilizations (%)
Ti
m
e
of
pe
rf
or
m
in
g
cl
ea
ni
ng
(S
ec
on
d)
Greedy+None
Greedy+2_LRU
CAT+DAC
Fast Cleaning+DAC
Figure 15: Comparison of cleaning time under
various flash memory utilizations
出席國際學術會議心得報告 
                                                             
計畫編號 96-2628-E-260-009- 
計畫名稱 支援嵌入式作業系統之動態更新與重組之系統及軟體元件庫的製作 
出國人員姓名 
服務機關及職稱 
吳俊泓 
國立暨南國際大學資訊管理所 碩士班研究生 
會議時間地點 97/07/29～97/07/31 Chengdu, China 
會議名稱 
(中文) 2008 國際嵌入式系統及嵌入式軟件會議(ICESS＇08) 
(英文) 2008 International Conference on Embedded Software and Systems. 
發表論文題目 A New FTL-based Flash Memory Management Scheme with Fast Cleaning Mechanism 
 
一、參加會議經過 
參加會議經過如下: 
z 7/28 上午起程，經由香港轉機後下午到大陸，抵逹賓館後，處理相關 check-in 事
務與準備隔天的報告 
z 7/29 上午主要是參加 Opening Ceremony。中午參加餐敍，下午則參加 Technical 
Session，並於Session中的Embedded Software Optimization and Simulation中發表 1 
篇會議論文(A New FTL-based Flash Memory Management Scheme with Fast 
Cleaning Mechanism)，與會討論熱烈。下午另外參加 1 場 Technical Session。晚上
參加餐敘，與各國友人聯誼，並交換研究心得。 
z 7/30 上午參加 Technical Session 中的 Network Protocols Session，中午餐敍，下午
參觀各領域 Poster Session，晚上參與大會 Banquet，大會結束。 
z 7/31 參加主辦單位主辦的 Local Tour，參觀成都熊貓基地與杜甫草堂等地。晚上
則與各國學者與研究生餐敘，交換研究心得。 
z 8/1 上午起程返國。 
 
二、與會心得 
嵌入式系統與軟體的設計與應用是現今相當熱門的主題，從硬體設計、軟體實作、軟硬
體協同設計、省電設計、即時排程、作業系統, 到通訊各領域皆包括在其中，因此範圍甚廣，
也廣受各國資訊研究人材以及各業界人士的重視。該會議在 Opening Ceremony 中的 keynote
亦相當精彩，介紹中國大陸在 Wireless Sensor Networks 領域發展的國家計畫與 Loongson 
Processor。 
在會議中，同時出席了相當多的專家學者，亦有相當多的業界人士，從不同的觀點切入，
與會者討論及反應都相當熱烈。而如何將各相關技術加以整合，賦予創意，以及如何的運用
是其中的重點。因此，國科會提倡及補助國內專家學者出席國際會議的用意相當好，出席國
際會議，與各國友人聯誼，可以了解各國的研究現況，並可交換心得，增加合作的機會，有
助於研究水準的提昇，收獲甚多。 
make each flash memory block be erased evenly to
prolong the flash memory lifespan and improve the
performance of flash-memory storage systems.
In this paper, we have designed and implemented a
flash translation layer for data management in NAND -
type flash memory with large capacity. Since the
capacity of flash memory is getting mor e and more
larger, when a flash file system performs cleaning
activity, it must scan all flash memory blocks to select a
suitable block for reclaiming space according to the
designated cleaning policy. This operation would take a
large amount of time and increase system overhead for
cleaning activity. Therefore, to address this problem,
we have proposed a fast cleaning mechanism which is
based on the DAC [5] technique for separating hot and
cold data in flash memory, whereas uses multiple LRU
Lists to speed up the selection of the blocks for erasure
in cleaning activity. In addition, for obtaining better
cleaning performance, each block on flash memory is
assigned a weight according to its attribute and the
cost-benefit policy is adopted with the weight of b lock
to select a block for cleaning. Our design goal is to
fasten the cleaning processing time and reduce system
overhead for performing cleaning activity in a large
NAND-type flash memory.
Simulation results show that our proposed flash
memory management scheme with fast cleaning
mechanism incurs 0.6%-28.32% fewer erase operations,
1.7%-57.28% fewer flash memory pages copied, and
43.21%-65.17% less cleaning time than other flash
memory management schemes with different cleaning
mechanisms under 90% flash memory utilization.
2. Background technologies
2.1. Flash memory characteristics
Flash memory is a non-volatile that can hold stored
data without supplying power. It is mainly divided into
two major types, i.e. NOR-type and NAND-type.
NOR-type flash memory provides fast random access
speed and execute-in-place (XIP) functionality, so it is
usually used for storing program codes (e.g. computer's
BIOS). In contrast, NAND-type flash memory has
characteristics of low cost, large capacity, and high
performance for write and erase, thus it is used widely
as storage in embedded systems and mobile devices.
NAND-type flash memory chip is composed of
many blocks, and each block has fixed number of
pages. Each page in blocks can be divided into two
parts: data area and spare area, where data area is
primarily used to store the data of writing requests, and
spare area often stores the information of written data,
error detection and correction checksum.
In the general NAND-type flash memory, a block
typically has 32 pages and page size is 16 Kbytes. Data
area and spare area in each page are respectively 512
bytes and 16 bytes. For the new large flash memory
which has higher density and performance, a block can
have 64 pages, and each page contains 2,048 bytes of
data area and 64 bytes of spare area. Before a flash
memory block can be rewritten, it should be erased first.
The read and write operations are executed by a page
unit, but the erase operation is executed by a block unit.
Although flash memory has many advanta ges to
replace hard disks for the storage of embedded systems
and mobile devices, it still has limitations to overcome.
Since data in flash memory can not be overwritten ,
when the data is modified, the new data must be written
to another free page, and this page is then marked as a
valid page. The page which contains the old data would
be marked as an invalid page. When the amount of free
blocks is less than a threshold, the cleaning activity (i.e.
garbage collection) would be started to reclaim
invalidated pages in block by performing erase
operation. However, the unit of data for erase operation
is large than that for write operation, so that a block
selected for erasure may contain valid pages and
invalidated pages. Consequently, performing erase
operation needs to copy these valid pages in the erased
block to another free pages and then mark old pages as
invalid pages. This extra operation would decrease
flash memory performance. Besides, the number of
erasing cycle for each block on flash memory is limit ed.
Each block on flash memory can be erased between
100,000 to 1,000,000 times. When erasing cycle of a
block exceeds this boundary, it may cause the
operation of writing data to be unsuccessful or cause
the operation time to become unpredictable; the
lifetime of using flash memory may be shorten too.
2.2. Memory technology device subsystem
Memory Technology Device (MTD) subsystem [ 6]
supports various kinds of memory devices in Linux,
especially flash memory devices such as PCMCIA
devices, Common Flash Interface (CFI) onboard NOR
flash, M-Systems’ DiskOnChip. MTD Subsystem
mainly provides an interface for memory devices to let
hardware drivers communicate with the upper system.
MTD Subsystem does not care about what storage
formats are used or how to present the device's contents,
and just provide some routines to carry out read, write,
and erase operations on memory devices. The
architecture of MTD subsystem is shown in Figure 1.
block would incur lowest garbage collection overhead.
Therefore, Many researches focus on hot-and-cold data
separation to reduce system overhead.
Dynamic dAta Clustering (DAC) [5] logically
partitions flash memory storage space into many
regions, where each region can consist of a set of
contiguous or fragmental blocks on flash memory. The
objective of DAC is to cluster the data with the similar
write access frequencies in the same regions. Whenever
the update times of data are changed, the page that
stores data in region must be moved. A page of stored
data thus may be moved among the regions.
Figure 2 shows the operation of DAC. If the update
times of the data increase, the pages of stored data
would be moved to the top region; otherwise, if the
update times of the data decrease, the pages of stored
data would be moved to the bottom region. Therefore,
the hottest data are clustered to the top region and th e
coldest data are clustered to the bottom region.
Figure 2: The operation of DAC technique [5]
The two-level LRU lists [11] uses two fixed length
lists to store the logical block addresses of the written
data, which are sorted by their access times in the lists.
If the address of the written data exists in the hot list
(i.e. first-level list), the data would be marked hot;
otherwise, if the address of the written data exists in the
candidate list (i.e. second-level list), the data would be
marked cold. So, when the file system sends a write
request to flash memory, the flash translation layer
would check the address of the written data. The
address of the written data will be added to the
candidate list if it does not exist in any list. It would be
moved to the head of the hot list if it exists in the hot
list. Otherwise, it would be moved from the candidate
list to the head of the hot list if it already exists in the
candidate list. Besides, when the operation which
moves the address of the written data from the
candidate list to the hot list makes the hot list full, the
last entry would be moved from the hot list to the head
of the candidate list.
The Register-Based Hot-Cold Data Identification
[12] uses a register table to identify data as hot or cold.
It partitions logical addresses into several clusters, and
each cluster maps one bit in the register table. When
the write operation is performed, the system would set
the corresponding mapping bit of the cluster which the
written data belongs to in the register table to 1.
Therefore, the mapping bit of the cluster in the register
table would often be set to 1 if the data is hot ;
otherwise, it remains to be 0 if the data is cold.
3.3. Flash memory cleaning policies
The Greedy policy always selects a bloc k which
contains the most amount of invalid data. Its goal is to
reclaim the most amount of space to reuse in each
garbage collection. Greedy policy works well for
uniform access [5,7,13]. However, it dose not consider
the erasing cycles of blocks and the data attributes of
blocks. Under high localities of reference, it may cause
the uneven wearing of blocks and increase the system
overhead for coping valid data in the reclaimed block.
In the cost-benefit policy [7], the cleaner calculates
the value of each block on flash memory according to
the formula and selects the maximal value to reclaim
free space:
u
uage
2
)1(
cost
benefit  , where u is the
percentage of the valid data in the block  and (1-u) is
the percentage of free space reclaimed. The age is the
time since the most recent modification (i.e., the last
block invalidation). 2u stands for the cleaning cost to
erase block (i.e. u to read valid data from the block and
u to write them to another block). The block’s age
would became bigger if the block has not  been
reclaimed for a long time, and the block could have
more chances to be selected in the next cleaning
activity. Thus, blocks which contain invalid data can be
reclaimed more evenly than the greedy policy does.
The Cost Age Time (CAT) Policy [1 3] selects the
block with the minimal value to reclaim according to
the formula: 　 ageu
u 1
1
number of cleaning, where u
is the percentage of the valid data in the block and the
age is defined as the elapsed time since the erased
block was created. The
u
u
1  stands for the cleaning
cost which is defined as the cleaning cost of every
useful write to flash memory and the number of
cleaning is the number of times a block has been erased.
CAT not only considers the minimal cleaning cost but
also gives the hot block more time to accumulate
invalid data for reclaiming space and reducing valid
data copying. Besides, to provide wear-leveling, CAT
let a block with the lower erase cycles be given more
chances to be selected for cleaning.
num_invalid_page indicates how many invalid pages
the block contains.
Figure 5: Block structure in RAM
4.3. The proposed fast cleaning mechanism
We design and implement a FTL-based flash
memory management with fast cleaning. The proposed
fast cleaning uses the hot-and-cold data identification,
data placement policy, and cleaning policy to reduce
system overhead and improve system performance.
Hot-and-Cold Data Identification
We base on DAC technique to separate hot and cold
data. The flash memory is partitioned into three logical
regions, which are cold region, neutral region, and hot
region. Whereas, in each region, we maintain multiple
LRU lists according to the amount of invalid pages a
block has. That is, the blocks which have the same
amount of invalid pages are put into the same dirty list
and the blocks in the same dirty list are sorted by the
time since the most recent modification . Figure 6
shows the multiple LRU lists maintained in each region.
If the block contains all valid pages, the block would
be moved to the valid list. When the amount of invali d
pages of a block increases, the block would be moved
to another list according to the amount of invalid pages
it has. The block would be moved to the clean list when
it contains all invalid pages.
Data Placement Policy
When writing requests are performed, the system
must allocate free pages to store data. The data could
be classified into hot and cold attributes. If the system
stores the hot and cold data in the same block, the
cleaning activity needs to copy valid data to another
free flash memory space for reclaiming blocks. This
operation would cause a lot of extra system overhead.
To address the above problem, hot and cold data are
separately stored to different blocks. When the system
writes new data to flash memory, it would be written to
the cold region. If the data are updated, the data would
be considered as hot and be written to the upper region
according to DAC technique. Then, the bock which
contains obsolete data would be moved to the suitable
dirty list according the amount of invalid pages if there
are no free pages in it.
Figure 6: Structure of multiple LRU lists in each region
Figures 7-9 shows the result. Under 90/10 locality
of reference and 90% flash memory utilization , our fast
cleaning policy outperforms the greedy policy, cost -
benefit policy, and CAT policy by 19.63%, 14.7%, and
0.6% respectively for the number of erasure on the
flash memory. Our fast cleaning policy outperforms the
greedy policy, cost-benefit, and CAT policy by 45.26%,
36.8%, and 1.7% respectively for the number of pages
copied during cleaning
0
10000
20000
30000
40000
50000
60000
70000
50/50 60/40 70/30 80/20 90/10
Localities of reference
Nu
mb
er 
of 
era
se 
op
era
tio
ns
Greedy Cost-Benefit CAT Weight Cost-Benefit
Figure 7: The number of erase operations
200000
400000
600000
800000
1000000
1200000
1400000
50/50 60/40 70/30 80/20 90/10
Localities of reference
Nu
mb
er 
of 
pa
ge
s c
op
ied
Greedy Cost-Benefit CAT Fast Cleaning
Figure 8: The number of pages copied
0
50
100
150
200
250
300
350
50/50 60/40 70/30 80/20 90/10
Localities of reference
Ti
me
 of
 se
lec
tin
g b
loc
ks
 fo
r
era
su
re 
(S
ec
on
d)
Greedy
Cost-Benefit
CAT
Fast Cleaning
Figure 9: Time of selecting blocks for erasure
Figure 9 compares the time of selecting blocks for
erasure. When the capacity of flash memory is very
large, to calculate the values of all blocks according to
the designated policy and select a suitable block to
reclaim would take a large amount of time in cleaning.
Our flash memory management with fast cleaning
mechanism could incur the least time.
We use the amount of erase operations and pages
copied during cleaning and the measured time for
selecting blocks for erasure to calculate the time of
cleaning activity. The hardware specification is shown
in Table 3. Figure 10 shows that the fast cleaning
policy incurs 21.83%-57.41% less cleaning time than
the other cleaning policies.
Table 3: NAND-type flash memories [9,24]
Read Time 10.2μs (1B), 35.9μs (512B)
Write Time 201μs (1B), 226μs (512B)
Erase Time 2ms (16KB)
0
100
200
300
400
500
600
700
800
900
50/50 60/40 70/30 80/20 90/10
Localities of reference
Ti
me
 of
 pe
rfo
rm
ing
 cl
ea
nin
g (
Se
co
nd
)
Greedy
Cost-Benefit
CAT
Fast Cleaning
Figure 10: Comparison of the cleaning time
0
100
200
300
400
500
600
50 60 70 80 90
Flash memory utilizations (%)
Ti
me
 of
 pe
rfo
rm
ing
 cl
ea
nin
g
(S
ec
on
d)
Greedy
Cost-Benefit
CAT
Fast Cleaning
Figure 11: Comparison of cleaning time under
various flash memory utilizations
When flash memory utilization increases, the
number of erasure and pages copied during cleaning
would increase a lot, thus system overhead would
increase as well. Figure 11 shows that our proposed
flash memory management with fast cleaning incurs
