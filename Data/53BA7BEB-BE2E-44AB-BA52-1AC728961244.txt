 1
行政院國家科學委員會專題研究計畫成果報告 
造紙業之排程最佳化 
Scheduling Optimization in Paper Industry  
計畫編號：NSC－95－2221－E－011－098－MY3, 
NSC－96－2221－E－011－098－MY3, 
NSC－97－2221－E－011－098－MY3. 
執行期限：95 年 8 月 1 日至 98 年 7 月 31 日 
主持人：廖慶榮 教授   台灣科技大學工業管理系 
計畫參與人員：趙謙文  台灣科技大學工業管理系博士班 
鍾翠萍 台灣科技大學工業管理系博士班 
王福氣  台灣科技大學工業管理系博士班 
郭有維  台灣科技大學工業管理系碩士班 
張麗珍  台灣科技大學工業管理系碩士班 
 
中文摘要 
造紙業在一個國家中扮演著重要的角色，因幾乎所有生活必需品與該產業息息相
關。在本研究中，我們依序探討造紙業在主生產排程、細部生產排程與決策面下，三個
不同階段的排程問題，並分別提出有效且簡易的演算法來求解，希望未來有機會應用在
造紙業的排程問題上。研究主要區分為以下三個部分： 
Part I: 造紙業中總加權提早及延遲最小化之單機排程問題 
根據我們瞭解，目前在台灣的造紙業相當重視整個製程的準時完成，亦即考慮提早 
(earliness) 及延遲（tardiness）時間。因此，我們可將造紙業的排程問題視為總加權提早
及延遲最小化之單機排程問題。由於此問題係 NP-hard，無最佳化演算法可應用，所以
我們利用變動鄰域搜尋法 (variable neighborhood search；VNS) 結合塔布搜尋法來求
解。提出的 VNS/TS 演算法具有幾項特色，包括：鄰域間的使用比率、從鄰域中隨機選
取五個可行解、B/F 局部搜尋法、VNS 與 TS 的結合等。根據實驗結果，VNS/TS 演算
法不僅在求解品質上有顯著的效果，同時在計算時間方面也表現不錯。 
Part II: 造紙業中總完成時間最小化之混合式排程問題 
細部排程問題可區分為紙漿與造紙的排程，實際上，由於造紙流程中所有的產品並
不是以單一流程的方式進行加工，因此常發生因不良的排程延誤出貨時間，尤其又以造
紙的排程更為複雜。因此，我們將目標設定為總完成時間最小化之排程問題，考慮如何
在不延誤主生產排程的情況下，將細部生產排程達到最佳化。在此部分中，探討的生產
系統包含兩個階段：第一階段為單一機台，第二階段包含了兩台等效之機台（簡稱為1 2×  
hybrid flowshop）。有些產品須經過兩階段之機台，而有些僅需經過第二階段之機台，
此排程問題定義為「跨越第一階段之1 2× 混合式流程」。我們提供 non-permutation 
schedule (NPS) 之啟發式演算法以求解。實驗結果顯示，此啟發式演算法可有效地得到
品質不錯的解。 
 3
to be processed on the second stage. The addressed scheduling problem is a 1 2×  hybrid 
flowshop with missing operations at the first stage. In this note we develop a heuristic for the 
problem to generate a non-permutation schedule (NPS) from a given permutation schedule. 
Computational results demonstrate that the heuristic can efficiently generate better NPS 
solutions. 
Part III: A Modified Meta-RaPS for the one-dimensional cutting stock problem in paper 
industry.  
Paper industry and some contracted customers have a special corporation in the practice. 
That is, some contracted customers are willing buy small items with specific widths at a lower 
price. When the leftover fits the acceptable width decided by contracted customers, we sell it 
to bring in additional profit. Hence, we not only consider the production of orders but also 
take the corporation into account in order to maximizing the profit and develop the optimal 
scheduling system of paper industry. First, we proposed a model including the number of 
used stocks and the selling to maximizing the profit. Then, a modified Meta-RaPS 
(Meta-heuristic for Randomized Priority Search) is used to obtain an approximate solution. 
The method is composed of a construction heuristic and an improvement heuristic. In 
construction heuristic, at each step to choose the next item, a dispatching rule is applied 
according to the residual width of the stock. In improvement heuristic, we first reduce the 
original problem to sub-problem and then use CPLEX to improve. According to the 
computational results, the metaheuristic performs well in solution and computation time.  
Keywords: Paper industry; Scheduling; Single machine; Hybrid flowshop; Missing 
operations; one-dimensional cutting stock; Weighted earliness/tardiness; 
Makespan; Variable neighborhood search; Tabu search; Meta-RaPS. 
 5
2. Literature review 
In this section, we review the related research on the 1D-CSP and paper industry. There are 
three common objectives in the 1D-CSP or paper industry. The three objectives are to minimize the 
total trim loss, minimize the total number of stock cut, and minimize the cost of stock cut. However, 
in our research, the objective is to maximize the total profit of the stock cut including the cost and 
revenue. In the rest of this section, we will introduce the related research about 1D-CSP and the 
methods applied to solve these problems.  
1D-CSP is essential in the production planning of various industries such as paper, steel, 
plastic, aluminum and wood (Brown, 1971; Dyckhoff et al., 1985; Dyckhoff and Finke, 1992; 
Hinxman, 1980; Gilmore and Gomory, 1963, 1961, 1965). Among the varying CSP, 
One-dimensional Cutting Stock Problem (1D-CSP) is more common which is proposed by Gilmore 
and Gomory (1961, 1963). Stadtler (1990) raised the producing window frames made out of 
aluminium. The decision problem is to determine how many aluminium profiles of a given standard 
length have to be bought to fulfill production requirements. Moreover, Umetani et al. (2003) 
proposed an approach which is based on metaheuristics, and incorporated an adaptive pattern 
generation technique.  
There are three types of algorithms usually applied to the 1D-CSP. The first, sequential 
heuristic procedure (SHP) is a kind of greedy heuristic algorithm. That sequentially adds new 
patterns to the current partial solution until all demands are satisfied. In each step, it generates a 
number of cutting pattern candidates and selects one with small trim-loss and high frequency. 
Robert (1975) developed a formulation of the one-dimensional trim problem when there is a fixed 
charge associated with using a cutting pattern. He presented a heuristic procedure to balance the 
potentially conflicting objectives of minimizing both trim loss and pattern change.  
The second is a heuristic based on pattern combinations. It starts from a solution obtained by 
an LP based approach, and then the number of different patterns is reduced by combining two 
different patterns into one pattern that covers the same amount of product. Foerster and Wäscher 
(2000) proposed a pattern reduction method, KOMBI, by extending the idea suggested by Diegel et 
al. (1993). The main process is to iteratively combine 2, 3, 4 or more patterns to achieve a minimal 
input cutting plan.  
The third is to develop exact algorithms to obtain optimal solutions for small size problems, like 
branch-and-bound (BAB). Francois (2000) considered the cutting stock problem which finds a cutting 
of stock material to meet demands. Then they proposed an integer programming formulation for the 
problem to minimize the number of different patterns that are used. Belov and Schetiauer (2006) 
considered 1D-CSP and two-dimensional two-stage guillotine constrained cutting problem. They 
investigated a combination of LP relaxation at each branch-and price node and mixed-integer cuts.  
Moreover, Umetani et al. (2003) applied iterated local search algorithm with adaptive pattern 
generation (ILS-APG) to 1D-CSP. The main component of this algorithm is a procedure to build up 
an initial solution and a local search method. The local search is first applied to the initial solution, 
and then to new solutions obtained by randomly perturbing the best solution obtained so far. The 
neighborhood of the local search is given by a subset of cutting patterns which have prospect of 
improving the current solution.  
In recent years, there are some researchers working on the 1D-CSP with multiple stock lengths. 
Gradisar and Trkman (2005) considered 1D-CSP whose characteristic lies in that all stock lengths 
can be different. They present a new approach combining two existing methods including 
sequential heuristic procedure and branch and bound. Poldi and Arenales (2009) tackled the CSP 
with limited supply of different lengths of stock material. Then, they evaluated different types of 
rounding heuristics to convert a fractional solution into a feasible integer solution with minimizing 
the waste.  
 7
0    ,    and integers,      1, 2, ,i iR or SLB R SLU i m= ≤ ≤ = L    (8) 
0 ,    and integers,     1, 2, , ;   1, 2, ,ij jx d i m j n≤ ≤ = =L L    (9) 
Now, we explain the objective and constraints in detail. In objective formulation, profit equals 
income minus cost. The income includes selling to general customers and contracted customers, i.e., 
1 1
m m
D i sell ii iC D C R= =× + ×∑ ∑ . The cost is from the cost of raw stocks, i.e., mC L m⋅ ⋅ . As 
1
m
D iiC D=×∑  is a constant, our aim is to maximize the profit of selling and indirectly to minimize 
the cost of raw stocks.  
The right-hand side of constraint set (2) means the quantity of produced item j equals the 
demand of item j. Thus, in this paper, no over-production problem occurs. Constraint set (3) shows 
that the fraction width iD  equals the sum of width of all items appeared on stock i . In constraint 
set (4), the sum of three fraction widths ( , ,i i iD R S ) on stock i  is equal to the width of raw stock. 
Constraint set (5) ensures that the sum of all item widths on stock i  cannot exceed the width of 
raw stock and is at least equal or larger than the shortest item width. Constraint set (6) reveals the 
lower and upper bound of the number of used stocks. The lower bound is an ideal situation where 
all items cut perfectly on stocks. Contrarily, the upper bound is a worst case which each stock has 
only one piece on it. Constraint set (7) notes that the residual of stock i  either selling at a lower 
price or disposing of it. Constraint set (8) is to set the range of acceptable width for contracted 
customers. It may equal to zero if the residual width is unacceptable or the stock is cut perfectly. 
Constraint set (9) is an integer restriction for the number of pieces of item j on stock i . Moreover, 
we limit the range from zero to its required demand.  
4. The proposed modified meta-raps algorithm 
1D-CSP can be solved with exact methods (brand and bound, dynamic programming) or 
approximation algorithms. Generally, time complexity of approximation algorithms is low, but it 
may generate a solution with bad objectives due to the so-called ending conditions. To minimize the 
influence of ending conditions, we propose a modified Meta-RaPS (Metaheuristic for Randomized 
Priority Search) which reduces the original problem to a sub-problem, and then use CPLEX to 
improve it. In this chapter, we first describe the general Meta-RaPS approach, followed by our 
proposed modified Meta-RaPS. 
3.1. General Meta-RaPS approach 
Meta-RaPS is a metaheuristic developed by DePuy et al. (2002). It evolved from a computer 
heuristic designed by Arcus (1966) to solve the assembly line balancing problem—COMSOAL 
(Computer Method of Sequencing Operations for Assembly Lines). The theme of Meta-RaPS is the 
use of randomness as a mechanism to avoid local optima, which is similar to other 
above-mentioned metaheuristics. Meta-RaPS is an iterative searching procedure. At each iteration, 
it constructs a feasible solution through the utilization of a construction heuristic in a randomized 
fashion, and then applies an improvement heuristic to improve the feasible solution if desirable. The 
best solution is then reported after a number of iterations. 
 The construction heuristic generates a feasible solution by adding basic elements step by step. 
The basic elements in a solution for a given combinatorial problem, such as, the cities in a tour for 
TSP, or the columns in a cover for SCP, are described by a finite set BE {1,2, , }n= L , and the 
feasible basic elements ( FE BE⊆ ) are those elements eligible for selection at each construction 
step. Each feasible basic element is characterized by a greedy score ( for FEiP i∈  ) according to 
some priority rule. Dependent on the definition of the priority rule, the best feasible element might 
 9
Step 3. Compute jPI  for all items with unsatisfied demand and calculate their sum SPI .  
Step 4. If 0SPI =  and there are items with unsatisfied demand, set 1m m= +  and return to 
Step 3; Otherwise, randomly generate a percentage q  and compare it with the priority 
percentage p . If q p≤ , choose the item with maximum priority index *PI , such that 
*
jPI PI≥  for all items; otherwise, randomly choose a item in candidate list. The 
candidate list includes the items with difference between jPI  and 
*PI  within the 
restriction percentage r  of *PI  such that * (1 )jPI PI r≥ × − .  
Step 5. Compare the resulting objective function value Z  with the incumbent *Z  and take into 
the improvement percentage i  account. If *(1 )Z i Z≥ − × , randomly choose k  stocks 
(at most K  stocks) with unsold fraction width longer than SLU . Under the same 
objective, solve the sub-problem by CPLEX. The resulting solution replaces the original 
partial solution. 
Step 6. If the resulting solution Z  is better than the incumbent, update *Z Z= . If t I= , stop; 
otherwise, set 1t t= +  and return to Step 2. 
We now explain the metaheuristic in detail. The first two steps initialize the incumbent *Z , 
the iteration number t  and the number of used stocks m . In Step 3, we compute the priority index 
of all items and then sum up them to obtain SPI . In Step 4, if the sum of priority indices equals to 
zero and there are items with unsatisfied demand, it indicates that all widths of unassigned items 
with unsatisfied demand are longer than the residual width. Therefore, increase the number of used 
stocks and return to Step 3. Contrarily, if the sum of priority indices is larger than zero, the priority 
percentage p  is used to determine the percentage of time the next item is added to the solution by 
using the best priority value. In the remaining time (1 p− ), the next item added to the solution is 
randomly chosen from those feasible items whose priority values are within the best r  of the best 
priority value. In Step 5, once a solution has been constructed, an improvement algorithm may 
proceed to further improve it. The improvement heuristic is performed if the construction stage’s 
solution value is better than the improvement percentage i  of the best solution value so far. Form 
the discarded stocks with over-long width, improvement process chooses k  stocks randomly first 
(at most K  stocks). These stocks build up a sub-problem which covers the same amount of items 
on them and CPLEX is carried out to improve. As CPLEX always obtains the optimum solution, its 
resulting solution replaces the original partial solution. Finally, in Step 6, we check whether the 
specified number of iterations I  has been met.  
5. Computational experiments 
 To verify the effectiveness of the proposed algorithm, two performance measures are presented 
to be the standard. Formal experiment consists of Meta-RaPS parameter setting, small-size instance 
and large-size instance.  
The generating scheme of test instances is the same as CUTGEN1 (1995). In order to generate 
test instances, five parameters need to be considered as follows.  
The first parameter is the number of different types of items, n . It sets to 10 and 15 in 
small-size instances, and ranges from 25 to 75 in large-size instances. The second parameter is the 
width of a raw stock, L . For the addressed problem, we set 10,000. The third and fourth 
parameters are the lower and upper bound for the relative size of order lengths in relation to L , 
respectively. That is, 1 2   ( 1, 2, , )iv L l v L i m⋅ ≤ ≤ ⋅ = L . We define that 1 0.0001v =  and 2v  ranges 
from 0.25 to 1.00. The fifth parameter is the average demand per order length, d . It sets to 5 in 
small-size instances and ranges from 5 to 20 in large-size instances. This provides 8 classes in 
 11
parameters r  and i , an increment size of 5% and 10% were used respectively. The 
number of iterations, I , was fixed to 1000. 
Step 3. For each problem in the subset, run Meta-RaPS using an appropriate technique for setting 
parameters. In this step, the technique consisted of sequentially varying each parameter 
over its domain until a value was found while keeping other values fixed. First, the r  
parameter was varied while fixing p  and i  to zero. The r  parameter associated with 
the best solution value was used. Second, the p  parameter was varied while maintaining 
r  at its setting and i  at zero. Finally, the i  parameter is varied while keeping p  and 
r  at their settings. 
Step 4. Use the overall best parameter settings obtained in Step 3 for the entire collection of 
problems. In this case, the 15 instances, and repeat the process for each setting 
1 2( , , , , )n L v v d . 
It should be noted that the parameter selection methodology presented in this section may not 
produce the optimal combination of parameter values. The intention is to provide a systematic 
method of selecting good parameters. From the results, in order to obtain solutions with the best 
performance, we set 0.1,   0.3,   0.3p r i= = = . Besides, as the influence of number of iterations on 
the result is very small, we set 100.I =  
5.3. Experiments on small problems 
 In this section, test instances are generated by using the same scheme of CUTGEN1 (1995). 
The optimal solution of each instance is found by CPLEX. The modified Meta-RaPS is carried out 
with 0.1,   0.3,   0.3,   100p r i I= = = =  which is determined by preliminary experiment. Both 
CPLEX and our modified Meta-RaPS were run 10 test instances on each class to obtain an average 
solution. The results are shown in Table 1. 
 
Table 1. Comparison with CPLEX and modified Meta-RaPS  
n v2 
CPLEX Modified Meta-RaPS 
Time* objQ  pQ  Time* 
10 0.25 0.00 3.70 0.00 0.18 
10 0.50 0.02 2.54 0.59 0.41 
10 0.75 0.13 2.77 0.00 0.26 
10 1.00 0.05 2.45 0.53 0.34 
15 0.25 0.02 4.71 0.00 1.27 
15 0.50 0.06 4.17 1.14 0.44 
15 0.75 0.32 3.90 1.12 0.32 
15 1.00 –** – – – 
Average 0.09 3.46 0.48 0.46 
* time in seconds. 
** cannot obtain a solution within 1 hour 
 
 It can be observed that both performance measures of modified Meta-RaPS are close to 0 and 
its computation time is less than 1 second. Therefore, our algorithm is able to find a good solution 
in a short time.  
5.4. Experiments on large problems 
 In this section, the proposed modified Meta-RaPS was evaluated by instances of 36 classes. 
 13
 It shows that the modified Meta-RaPS obtains a good objective value for large-size problems 
in a short time. However, the performance of finding the number of used stocks works worse in 
large problem than that in small problems. Additionally, the results seem to reveal that under the 
same ( ,  )n d , the performance of objective value is better for lower 2v . In the influence of 
different scales n , the average performance is similar. 
 In short, the experimental results show the outstanding performance of the modified 
Meta-RaPS to help scheduler to make a good decision in a short time.  
6. Conclusions 
In this research, we have considered the 1D-CSP with selling residuals to contracted customers 
at a lower price. The objective is to maximize the profit. We have developed a metaheuristic to deal 
with the problem. By performing extensive computational experiments, we have demonstrated that 
the result generated by the heuristic performs well. Also, the heuristic has a simple structure that 
can be easily understood by practitioners. Therefore, it provides an easy implementation for real 
paper industry. Future research may be conducted to extend the objective to the two-dimensional 
cutting stock problem. It is also of interest to consider whether accept all orders or partial orders.  
References 
Arcus, A.L. (1966). COMSOAL: A computer method of sequencing operations for assembly lines. 
International Journal of Production Research, 4, 259-277. 
Belov G., & Scheithauer G. (2006). A branch-and-cut-and-price algorithm for one-dimensional 
stock cutting and two-dimensional two-stage cutting. European Journal of Operational 
Research, 171 (1), 85-106. 
Bishoff E.E., & Waesher G. (1995). Cutting and packing. European Journal of Operational 
Research, 84, 503-505. 
Brown A.R. (1971). Optimum packing and depletion: the computer in space and resource usage 
problem. New York: Macdonald—London and American Elsevier Inc..  
DePuy, G.W., Moraga, R.J., & Whitehouse, G. (2005). Meta-RaPS: A simple and effective 
approach for solving the traveling salesman problem. Transportation Research Part E: 
Logistics and Transportation Review, 41 (2), 115-130. 
DePuy, G.W., Whitehouse, G.E., & Moraga, R.J. (2002). Using the Meta-RaPS approach to solve 
combinatorial problems. In: Proceedings of the 2002 Industrial Engineering Research 
Conference, May 19-21, Orlando, Florida. Computers & Industrial Engineering, submitted for 
publication. 
Diegel A, Chetty M, Van Schalkwyck S, & Naidoo S. (1993). Setup combining in the trim loss 
problem—3to2 & 2to1. Working paper, Business Administration, University of Natal, Durban, 
First Draft. 
Dyckhoff H., Kruse H.J., Abel D., & Gal T. (1985). Trim loss and related problems. The 
International Journal of Management Science, 13, 59-72.  
Dyckhoff H., & Finke U. (1992). Cutting and packing in production and distribution: typology and 
bibliography. Heidelberg: Springer.  
Foerster H, & Wäscher G. (2000). Pattern reduction in one-dimensional cutting stock problem. 
International Journal of Production Research, 38, 1657–1676. 
Gau T., & Wäscher G. (1995). CUTGEN1: A problem generator for the standard one-dimensional 
cutting stock problem. European Journal of Operational Research, 84, 572-579. 
Gilmore P.C., & Gomory R.E. (1961). A linear programming approach to the cutting stock problem. 
 15
Part II. Scheduling optimization for minimizing makespan in paper industry 
1. Introduction  
The paper industry plays an important role in a country, and almost all daily necessities are 
directly related to the industry. The industry includes pulp and papermaking. In this research, we 
investigate the papermaking in a paper factory that manufactures various products, including coated 
paper, industrial board, tissues and so on. The system is composed of two workstations (stages), 
special and basic, in series. At the first workstation, the special flow includes coating, drying, and 
calendaring. It always focuses on customer’s request. The second workstation contains two 
machines of basic flow. For instance, coated paper must be processed on the two workstations, but 
the tissue needs only to be processed on the second workstation. Fig. 1 depicts the production 
system, which is classified as a two-stage hybrid flowshop with missing operations at the first stage 
in the literature.  
 
 
Fig. 1. The production system in a papermaking factory. 
 
Consider the following scenario. There are M  stages in series, each consisting of one or more 
identical machines. There are n  jobs to be processed on the machines. All jobs have to follow the 
same stage sequence 1, 2, …, ,M  and the job sequence is the same for all stages. Such a problem 
is termed M-stage hybrid flowshop scheduling problem (Gupta and Tunc, 1991; Néron et al., 2001). 
In what follows, we briefly review the related research on two-stage hybrid flowshop with the 
makespan objective, the most common objective studied for the problem. The famous Johnson’s 
rule (Johnson, 1954) can efficiently obtain an optimal solution for the problem if both stages have 
only one machine. However, the problem becomes NP-hard if any of the two stages contains more 
than one machine (Gupta, 1988), and hence enumeration procedures and heuristics are proposed as 
solution methods for the problem. Arthanary and Ramaswamy (1971) developed a branch and 
bound algorithm for solving the problem with two identical machines at stage 1 and one at stage 2 
(a 2 1×  hybrid flowshop). Gupta and Tunc (1991) developed two heuristics, based on Johnson’s 
rule and Longest Processing Time first (LPT) rule, for the case where the first stage contains only 
one machine and the second stage consists of multiple identical machines (a 1 m×  hybrid 
flowshop). In their heuristics, jobs are assigned to the various machines at each stage according to a 
generated permutation schedule, i.e., the job sequence is the same at each stage. From a practical 
point of view, the two-stage hybrid flowshop is a common machine environment in which there 
exist many practical applications, such as the glass-container industry (Paul, 1979) and the 
photographic film manufacturing industry (Tsubone, 1993). 
Santos et al. (1995) further showed that finding the best permutation schedule often produces 
 17
repeatedly moving a job from the busiest machine to the least busy machine. In the second phase, 
an even better balance is achieved by interchanging jobs between machines. The two phases are 
repeated iteratively until no improvement is generated. With the makespan objective and zero job 
release times, the job sequence on each machine can be arbitrary. 
The two improvement phases of França et al. (1993) are modified here to address the parallel 
machine scheduling problem at stage 2. In the new design, we need to consider the job sequence on 
each machine at stage 2 since there are non-zero release times, resulting from the schedule at stage 
1, for jobs at stage 2. To reduce the idleness of machines, the jobs at stage 2 should be reassigned 
according to the schedule at stage 1. 
Before giving the formal outline of the proposed heuristic, we introduce the following theorem. 
Theorem 1.  Let maxC  be the makespan of a given PS in a 1 2×  hybrid flowshop. Let maxC  be 
the makespan of the PS in a relaxed hybrid flowshop where the number of machines at the second 
stage is equal to or more than the number of jobs. If max maxC C= , then no improvement can be 
made by replacing the PS with an NPS. 
Proof. Gupta and Tunc (1991) consider a 1 m×  hybrid flowshop and analyze the case when the 
number of machines at the second stage is equal to or more than the number of jobs. i.e., there is no 
possibility of any bottleneck at the second stage. In this case, maxC  is computed under the 
assumption that no waiting time is incurred for any job at stage 2. It is clear that maxC  is a lower 
bound for PS in a 1 m×  (including1 2× ) hybrid flowshop. Thus, no improvement can be made by 
replacing PS with an NPS if max maxC C= .     
We now present the steps of the heuristic, which deals with only the operations at stage 2, for 
generating an NPS as follows: 
Phase 0.  Initial PS 
Let [1] [2] [ ]( , ,..., )nJ J Jπ =  be a PS obtained by using a specified heuristic, e.g., a heuristic of 
Gupta and Tunc (1991). In constructing a schedule at stage 2, the operations are started as soon as 
possible. If max maxC C= , then no improvement can be made (Theorem 1); stop. 
Phase 1.  Missing operations rescheduling 
For each machine, move all those jobs with missing operations to the beginning of the machine 
and sequence them in LPT order. 
Phase 2.  Job moving 
Let 1A  be the busier machine with larger completion time and 2A  be the other one. Denote 
by iC  the job completion time on iA  ( 1, 2)i = . All jobs are considered in the following steps. 
Step 1. Compute 1 1 2d C C= −  and 2 1 / 2d d= . 
Step 2.  Select a job j  on 1A  such that 2(2, )p j d≤ , and go to Step 3. If not found, select 
the one with 1(2, )p j d<  and go to Step 3. Otherwise, go to Phase 3. 
Step 3.  Move job j  from 1A  to 2A  and schedule it according to the schedule at stage 1. 
Update 1C , 2C , maxC  and relabel 1A  and 2A . If maxC  is reduced, return to Step 1; 
else undo Step 3 and go to Phase 3. 
Phase 3.  Job reassigning 
Step 1.  Compute 1 2d C C= − . 
Step 2.  Select a pair of jobs 1j A∈  and 2j A′∈  such that 0 (2, )p j< −  (2, )p j d′ < , and go 
to Step 3. Otherwise, go to Step 4. 
Step 3.  Assign jobs j  and j′  to 2A  and 1A  respectively according to the schedule at 
stage 1. Update 1C , 2C , maxC  and relabel 1A  and 2A . If maxC  is reduced, return 
 19
NPS is clearly better than the PS especially for large values of %q . 
4.  For the PS by LPT and NEH, most percentage improvements for 20%q =  and 40%  in the 
dominant data set fall in the range of 1-10%. This is quite effective. The maximum percentage 
improvements in many cases are larger than 20%. In general, the improvements of the PS by 
LPT and NEH are greater than those by Johnson’s rule and Dannenbring.  
5.  The percentage improvements are gradually reduced with the increase in the number of jobs. 
The main reason is that the PS solution on large-size problems has more chance to be closer to 
its lower bound because more jobs have larger release times at stage 2 for large-size problems. 
Thus, less improvement can be made by replacing a PS with an NPS. 
In summary, the percentage of instances where a PS can be improved by an NPS is high, 
except for the non-dominant data set and the problems with no missing operations ( % 0q = ). The 
maximum percentage improvements in many cases are larger than 20%. For all the tested instances, 
the proposed heuristic took at most 0.02 seconds to generate an NPS. Therefore, it is worthwhile to 
apply the heuristic to a 1 2×  hybrid flowshop when missing operations exist. 
 
Table 1. Percentage improvement of NPS over PS in the dominant data set ( 0%)q =  
n Johnson’s rule  LPT NEH Dannenbring Avg Max N  Avg Max N Avg Max N Avg Max N 
5 1.17 13.69 19  0.41 6.31 15 1.33 12.24 23 1.52 15.79 21
10 0.76 9.97 30  0.46 4.73 35 1.75 9.76 44 1.14 11.71 30
20 0.41 4.71 34  0.23 1.44 43 1.47 6.30 65 0.40 4.81 45
30 0.13 1.39 35  0.10 0.63 44 1.73 5.15 85 0.16 3.62 37
40 0.05 0.46 28  0.03 0.39 25 1.39 3.90 95 0.08 0.63 44
50 0.03 0.22 29  0.03 0.31 33 1.32 3.17 96 0.05 0.38 31
60 0.02 0.25 23  0.02 0.18 20 1.08 2.48 98 0.05 0.34 41
70 0.01 0.23 17  0.01 0.23 14 0.81 2.02 96 0.03 0.28 31
80 0.01 0.10 18  0.02 0.19 24 0.90 2.13 100 0.03 0.24 33
90 0.01 0.22 16  0.01 0.12 14 0.78 1.93 99 0.02 0.18 27
100 0.01 0.08 12  0.00 0.08 10 0.72 1.60 99 0.02 0.16 28
Mean 0.24 2.85 23.7  0.12 1.33 25.2 1.21 4.61 81.8 0.32 3.47 33.5
 
 
Table 2. Percentage improvement of NPS over PS in the dominant data set ( 20%)q =  
n  Johnson’s rule  LPT NEH Dannenbring  Avg Max N  Avg Max N Avg Max N Avg Max N 
5  2.64 20.67 36  4.45 20.09 58 2.40 16.18 36 2.63 17.09 39
10  2.19 11.07 60  5.86 15.57 86 3.87 16.26 60 3.41 19.92 59
20  0.66 4.55 58  5.11 11.46 96 5.09 18.97 92 1.37 5.55 84
30  0.21 2.05 55  3.58 8.80 94 5.06 15.53 93 0.58 4.42 81
40  0.11 0.76 57  2.65 6.19 94 4.89 15.41 98 0.32 2.08 79
50  0.07 0.53 55  2.25 6.01 94 4.78 12.35 98 0.17 1.14 73
60  0.06 0.48 51  1.98 4.81 98 4.72 12.14 100 0.14 0.75 78
70  0.03 0.23 41  1.71 3.75 98 4.78 14.18 100 0.08 0.28 75
80  0.02 0.22 30  1.62 3.29 96 4.51 11.71 100 0.07 0.27 72
90  0.02 0.18 26  1.23 2.91 97 4.37 11.90 100 0.06 0.37 70
100  0.01 0.20 22  1.33 3.03 97 4.23 10.03 100 0.05 0.22 66
Mean  0.55 3.72 44.6  2.89 7.81 91.6 4.43 14.06 88.8 0.81 4.74 70.5
 21
Table 6. Percentage improvement of NPS over PS in the non-dominant data set ( 40%)q =  
n  Johnson’s rule  LPT NEH  Dannenbring  Avg Max N  Avg Max N Avg Max N  Avg Max N 
5  0.44 13.74 8  3.60 23.83 39 0.53 20.11 8  0.74 16.15 9 
10  0.53 10.06 16  4.48 21.66 60 0.51 10.60 12  0.60 8.02 15 
20  0.16 2.41 18  2.73 19.54 51 0.46 6.12 19  0.24 4.55 24 
30  0.05 1.66 12  1.82 15.50 48 0.18 3.25 15  0.09 1.96 15 
40  0.01 0.59 7  0.49 8.18 27 0.12 4.06 10  0.04 0.97 10 
50  0.01 0.16 9  0.52 7.75 32 0.04 1.19 8  0.02 0.52 8 
60  0.00 0.00 0  0.00 0.00 0 0.05 1.56 8  0.01 0.17 5 
70  0.00 0.00 0  0.00 0.00 0 0.02 0.79 4  0.00 0.22 4 
80  0.00 0.00 0  0.00 0.00 0 0.01 0.88 2  0.00 0.19 2 
90  0.00 0.00 0  0.00 0.00 0 0.00 0.00 0  0.00 0.00 0 
100  0.00 0.00 0  0.00 0.00 0 0.01 0.88 2  0.00 0.15 3 
Mean  0.11 2.60 6.36  1.24 8.77 23.36 0.18 4.49 8  0.16 2.99 8.63
5. Conclusions and future research 
In this research we have considered a 1 2×  hybrid flowshop with missing operations at the 
first stage of special flow. The objective is to minimize the makespan. We have developed a 
heuristic for the problem to derive a non-permutation schedule (NPS) from a given permutation 
schedule (PS). By performing extensive computational experiments, we have demonstrated that the 
NPS generated by the heuristic performs well and the additional computational effort is negligible. 
Also, the heuristic has a simple structure that can be easily understood by practitioners. Therefore, 
it provides an easy implementation for real paper industry. Future research may be conducted to 
extend the results to a multi-stage hybrid flowshop. It is also of interest to consider other criteria 
such as total flow time and total tardiness. It is particularly worthwhile to investigate the due-date 
related criteria because it has been shown that a PS can be improved significantly by an NPS for 
such criteria in a non-permutation flowshop (Liao et al., 2006). 
References 
Arthanary, T. S., & Ramaswamy, K. G. (1971). An extension of two machine sequencing problem. 
Opsearch, 8, 10-22. 
Brah, S. A., & Loo, L. L. (1999). Heuristics for scheduling in a flow shop with multiple processors. 
European Journal of Operational Research, 113, 113-122. 
Campbell, H. G., Dudek, R. A., & Smith, M. L. (1970). A heuristic algorithm for the n job, m 
machine sequencing problem. Management Science, 16, 630-637. 
Dannenbring, D. G. (1977). An evaluation of flow-shop scheduling heuristics. Management Science, 
23, 1174-1182. 
França, P. M., Gendreau, M., Laporte, G., & Müller, F. M. (1993). A composite heuristic for the 
identical parallel machine scheduling problem with minimum makespan objective. Computers 
and Operations Research, 21, 205-210. 
Guinet, A., Solomon, M. M., Kedia, P. K., & Dussauchoy, A. (1996). A computational study of 
heuristics for two-stage flexible flowshop. International Journal of Production Research, 34, 
1399-1415. 
Gupta, J. N. D. (1972). Heuristic algorithms for multistage flowshop scheduling problem. AIIE 
 23
Part I. Scheduling optimization for minimizing single machine with weighted 
earliness and tardiness in paper industry. 
1. Introduction 
The paper industry plays an important role in a country, and almost all daily necessities are 
directly related to the industry. The industry includes pulp and paper. Pulp is used as a raw material 
to manufacture paper. It is produced by a mechanical or chemical process. For mechanical process, 
the lignin is separated by grinding of the wood. For chemical process, the lignin is dissolved by 
cooking the wood with the particular chemicals. Paper products can be divided into four paper 
categories: industrial paper, printing and writing, sanitary paper, and other paper products. A 
production system in a paper factory is composed of two stages, screening and dewatering, and 
coating, in series. At the first stage, pulp is spread on a screen and it is further dewatered by 
gravitational force, pressure, heat, drying section of paper machines. At the coating stage, the paper 
sheets are processed in a flowshop environment. To produce various paper types, there are missing 
operations for some jobs on the specific machines, i.e., some paper types may bypass some 
machines at the stage. With the advent of just-in-time (JIT), the criterion involving both earliness 
and tardiness penalties has become more important in paper industry. An early job may result in 
inventory carrying cost, such as opportunity cost of the money invested in inventory, storage and 
insurance costs and deterioration. Contrarily, a tardy job may result in customer dissatisfaction, 
contract penalties, loss of sale and loss of reputation. 
In summary, pulp and paper production has many features in common with the continuous 
production. They are characterized by the properties of process streams, unique chemistry and 
physics of each of its processes, and the mechanical design of the process equipment (Farla et al., 
1997; Yin et al., 2003). Consider the process in a single type of paper, and the missing operations 
are ignored temporarily at the coating stage. Its process can be regarded as the continuous 
production. Such a continuous production is classified as the single machine problem in the 
literature. The objective is to minimize total weighted earliness and tardiness. Therefore, we 
consider the earliness and tardiness problem with a common due date on a single machine. We 
propose a hybrid approach to solve the problem. Our approach applies variable neighborhood 
search (VNS) in combination with tabu search (TS), where VNS is a metaheuristic based on the 
principle of systematic change of neighborhood during the search (Hansen and Mladenović, 2001). 
2. Literature review  
To address the scheduling problem in the paper industry, we consider the earliness and 
tardiness problem with a common due date on a single machine. The field of common due date 
scheduling can be distinguished into two cases, unrestrictive and restrictive ones. In the 
unrestrictive case, which was first introduced by Kanet (1981), the common due date has to be 
determined, or its value is greater than or equal to the sum of processing times of all jobs, i.e., it has 
no influence on the optimal schedule. On the other hand, it is called restrictive if the common due 
date is known and may affect the optimal schedule. A comprehensive survey on the common due 
date scheduling can be found in Gordon et al. (2002). 
For the restrictive common due date problem on single machine, Biskup and Feldmann (2001) 
generated a total of 280 benchmark problem instances with 10, 20, 50, 100, 200, 500 and 1000 jobs 
and restricted the common due date with 0.2, 0.4, 0.6, and 0.8 of the sum of all processing times. 
This problem is well known in the scheduling literature as NP-hard (Hall et al., 1991) such that 
exact solution methods are not suitable for solving large-size instances of the problem. Thus, they 
 25
iTL  Size of the tabu list, 1TL  for insertion move and 2TL  for swap move 
The problem considered in this paper can be stated as follows. There is a common due date d  
for a set of n  jobs which is to be processed on a continuously available single machine. The 
machine can process only one job at a time. Each of the n  jobs is available for processing at time 
zero and has a known processing time jp , earliness weight jα , and tardiness weight jβ . The 
objective is to determine a feasible schedule S  with minimum total weighted earliness and 
tardiness, i.e., 
1
( ) ( )
n
j j j j
j
f S E Tα β
=
= +∑  (1) 
The earliness and the tardiness of job j  are calculated as max(0,  )j jE d C= −  and 
max(0,  )j jT C d= − . Using the standard scheduling notation (Pinedo 2002), the problem can be 
written as 1/ /j j j j jd d E Tα β= +∑ ∑ . 
4. The proposed VNS/TS algorithm 
Due to the complexity of the problem, metaheuristics are recommended for searching good 
solutions in reasonable computation times. The metaheuristics that have been used for this problem 
include SA, TAR, GA, TS, GA+TS, among others. In this paper, we propose a hybrid algorithm 
that uses tabu search within variable neighborhood search (VNS/TS). The components of the 
proposed VNS/TS algorithm are described below. 
4.1. Variable Neighborhood Search 
VNS is different from most local search heuristics in that it uses two or more neighborhoods, 
instead of one, in its structure. In particular, it is based on the principle of systematic change of 
neighborhood during the search. In addition, to avoid costing too much computational time, the best 
number of neighborhoods is often two (Glover and Kochenberger, 2003, p. 147), which is followed 
by our algorithm. The two neighborhoods employed in our algorithm are defined below: 
1. Insertion moves ( 1η = ): Randomly identifies two particular jobs, one for each set ES  and TS , 
and places one job in the position that directly precedes the other job.  
2. Swap moves ( 2η = ): Randomly identifies two particular jobs, one for each set ES  and TS , 
and places each job in the position previously occupied by the other. 
The steps of our VNS structure are described below: 
Step 1.  Find an initial solution x . Set 1η =  and 0i = .  
Step 2.  Repeat the following steps until N  accumulative no-improving iterations: 
(i) Generate five schedules x′  at random from the η th neighborhood of x .  
(ii) If the due date is loose, apply a local search method to each schedule. 
(iii) Choose the best schedule x′′ . If x′′  is better than the incumbent, update x x′′=  
and return to (i). 
(iv) If 1η = , set 2η =  and return to (i); otherwise 1i i= + . 
(v) If 10i = , set 1η =  and 0i = . Return to (i). 
We now elaborate on the steps in more detail. In Step 1, we use the best initial solution method 
for the respective case. That is, we employ the initial solution method of Hino et al. (2005) for the 
tight due date case (i.e., 0.2h =  and 0.4) and the method of Feldmann and Biskup (2003) for the 
loose due date case (i.e., 0.6h =  and 0.8).  
In Step 2(i) we generate five, instead of one, schedules x′  to extend the probability of 
searching for a better solution. We will demonstrate by experiments that this approach can improve 
 27
5. Computational experiments 
To verify the performance of the VNS/TS algorithm, we tested the same set of problems as 
Biskup and Feldmann (2001), Feldmann and Biskup (2003) and Hino et al. (2005). The algorithm 
was code in C++ and implemented on a Pentium 4 3.2GHz with 512MB memory. Each instance 
was run 10 times and the best solution was selected. 
The benchmark problem instances were provided by Biskup and Feldmann (2001), which can 
be obtained at http://people.brunel.ac.uk/~mastjjb/jeb/orlib/ schinfo.html (Beasley, 2005). There are 
seven different numbers of jobs ( 10,20,50,100,200,500,1000n = ) with four restrictive due-date 
factors ( 0.2,0.4,0.6,0.8h = ), where the common due date was determined by the expression 
1[ ]
n
jjd h p== ×∑ . For each combination, 10 different problem instances were generated, resulting in 
a total of 280 problem instances.  
Before conducting a formal experiment for comparing with existing algorithms, the following 
four preliminary experiments will be carried out: 
1. Determine if the insertion move can improve the algorithm. If yes, determine the best ratio of 
the two neighborhoods. 
2. Determine the best number of generated schedules in a neighborhood. 
3. Evaluate the algorithm with and without the use of the B/F local search. 
4. Evaluate the algorithm with and without the combination of TS. 
 All of the four experiments were conducted for two job sizes 200n =  and 500. To have a fair 
comparison, 1 second of computation time is set as the stopping criterion for all the instances with 
200n = , and 7 seconds for all the instances with 500n = . To measure the effectiveness of 
algorithms, we compute the percentage improvement (PI) of the solution value obtained ( aF ) with 
respect to the benchmark value provided by Biskup and Feldmann (2001; BFF ) as follows: 
PI 100 a BF
BF
F F
F
−= ×   (2) 
The benchmark values from Biskup and Feldmann (2001) are upper bounds on the optimal 
objective function values, except for the instances with 10 jobs which were solved optimally with 
LINDO software. 
The result of the first experiment is for all the eight combinations of n  and h , ratio 1:10 
always gives the best solution, and hence it is chosen as the insertion/swap ratio in our algorithm. 
The result of the second experiment shows that generating 5 points in a neighborhood is 
superior to generating 1 point or 10 points.  
The third experiment evaluates the algorithm with and without the use of the B/F local search, 
which is implemented only for the loose due date (i.e., 0.6,0.8h = ). The results show that the B/F 
local search always results in an obvious improvement. 
The fourth experiment evaluates the algorithm with and without the combination of TS, i.e., to 
compare the hybrid VNS/TS with the pure VNS. The results show that the hybrid VNS/TS indeed 
has a better performance. 
In the formal experiment, we compare our VNS/TS algorithm with the best algorithms for the 
problem in the literature. The following parameters, determined by a series of pilot experiments, are 
used in our algorithm: 1 7TL = , 2 7TL =  (for 10,20,50n = ), 14 (for 100,200n = ), 21 (for 
500,1000n = ); 600MaxIter =  for 0.2,0.4h =  and 150MaxIter =  for 0.6,0.8h = , where 
MaxIter is the maximum number of iterations. 
We compare our results with current best methods presented by Hino et al. (2005). They 
developed two metaheuristics and two hybrid algorithms: GA, TS, HTG (TS+GA), and HGT 
(GA+TS). The hybrid algorithms simply apply the metaheuristics in a sequential form. Among the 
 29
Table 1. 
Comparison with HGT and GA of Hino et al. (2005) 
n   h   HGT GA  VNS/TS  
10  0.2  0.12  0.12  0.00 + 
  0.4  0.19  0.19  0.00 + 
  0.6  0.01  0.03  0.00 + 
  0.8  0.00  0.00  0.00  
20  0.2  −3.84  −3.84  −3.84  
  0.4  −1.62  −1.62  −1.63 + 
  0.6  −0.71  −0.68  −0.72 + 
  0.8  −0.41  −0.28  −0.41  
50  0.2  −5.70  −5.68  −5.70  
  0.4  −4.66  −4.60  −4.66  
  0.6  −0.31  −0.31  −0.34 + 
  0.8  −0.23  −0.19  −0.24 + 
100  0.2  −6.19  −6.17  −6.19  
  0.4  −4.93  −4.91  −4.94 + 
  0.6  0.04  −0.12  −0.15 + 
  0.8  −0.11  −0.12  −0.18 + 
200  0.2  −5.76  −5.74  −5.78 + 
  0.4  −3.75  −3.75  −3.75  
  0.6  0.07  −0.13  −0.15 + 
  0.8  0.07  −0.14  −0.15 + 
500  0.2  −6.41  −6.41  −6.42 + 
  0.4  −3.58  −3.58  −3.56 − 
  0.6  0.15  −0.11  −0.11  
  0.8  0.13  −0.11  −0.11  
1000  0.2  −6.74  −6.75  −6.75  
  0.4  −4.39  −4.40  −4.37 − 
  0.6  0.42  −0.05  −0.05  
  0.8  0.40  −0.05  −0.05  
Mean    −2.06 −2.12  −2.15  
+ VNS/TS is the best one. 
− Either HGT or GA is the best one. 
 
 31
787-801. 
Baker, K. R., & Scudder, G. D. (1990). Sequencing with earliness and tardiness penalties: A review. 
Operations Research, 38, 22-36. 
Cheng, T. C. E., & Kahlbacher, H. G.. (1991). A proof for the longest-job-first policy in 
one-machine scheduling. Naval Research Logistics, 38, 715-720. 
Crainic, T. G., Gendreau, M., & Farvolden, J. M. (2000). Simplex-based tabu search for the 
multicommodity capacitated fixed charge network design problem. INFORMS Journal on 
Computing, 12, 223-236. 
Drummond, L. M. A., Vianna, L. S., Silva M. B., & Ochi, L. S. (2002). Distribution parallel 
metaheuristics based on GRASP and VNS for solving the traveling purchaser problem. 
Proceedings of the 9th International Conference on Parallel and Distributed System, 257-263. 
Farla, J., Blok, K., & Schipper, L. (1997). Energy efficiency developments in the pulp and paper 
industry: A cross-country comparison using physical production data, Energy Policy, 25, 
745-758. 
Feldmann, M., & Biskup, D. (2003). Single-machine scheduling for minimizing earliness and 
tardiness penalties by meta-heuristic approaches. Computers and Industrial Engineering, 44, 
307-323. 
Gagné, C., Gravel, M., & Price, W. L. (2005). Using metaheuristic compromise programming for 
the solution of multiple-objective scheduling problems. Journal of the Operational Research 
Society, 56, 687-698. 
Gendreau, M., Guertin, F., Potvin, J.-Y., & Taillard, E. D. (1999). Parallel tabu search for real-time 
vehicle routing and dispatching. Transportation Science, 33, 381-390. 
Glover, F. (1986). Future paths for integer programming and links to artificial intelligence. 
Computers and Operations Research, 13, 533-549. 
Glover, F. (1993) A user’s guide tabu search. Annals of Operations Research, 41, 3-28. 
Glover, F., & Kochenberger, G. A. (2003). Handbook of metaheuristics. Boston, MA: Kluwer 
Academic Publisher. 
Gordon, V., Proth, J.-M., & Chu, C. (2002). A survey of the state-of-the-art of common due date 
assignment and scheduling research. European Journal of Operational Research, 139, 1-25. 
Hall, N. G.., Kubiak, W., & Sethi, S. P. (1991). Earliness-tardiness scheduling problems, II: 
Deviation of completion times about a restrictive common due date. Operations Research, 39, 
847-856. 
Hansen, P., & Mladenović, N. (1997). Variable neighborhood search for the p-Median. Location 
Science, 5, 207-226. 
Hansen, P., & Mladenović, N. (2001). Variable neighborhood search: principles and applications. 
European Journal of Operational Research, 130, 449-467. 
Hino, C. M., Ronconi, D. P., & Mendes, A. B. (2005). Minimizing earliness and tardiness penalties 
in a single-machine problem with a common due date. European Journal of Operational 
Research, 160, 190-201. 
Hoogeveen, J. A., & van de Velde, S. L. (1991). Scheduling around a small common due date. 
European Journal of Operational Research, 55, 237-242. 
James, R. J. W. (1997). Using tabu search to solve the common due date early/tardy machine 
scheduling problem. Computers and Operations Research, 24, 199-208. 
Kanet, J. J. (1981). Minimizing the average deviation of job completion times about a common due 
date. Naval Research Logistics Quarterly, 28, 643-651. 
Liaw, C. F. (2003). An efficient tabu search approach for the two-machine preemptive open shop 
scheduling problem. Computers and Operations Research, 30, 2081-2095. 
Mladenović, N., & Hansen, P. (1997). Variable neighborhood search. Computers and Operations 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                      98 年 5 月 10 日 
報告人姓名  
廖慶榮 
服務機構
及職稱
 
台灣科技大學工管系教授 
 
 會議 時間 地
點 
自 2009 年 5 月 1 日至
2009 年 5 月 4 日止 
地 點：美國，佛羅里達
州，奧蘭多 (Orlando)
核定 
補助 
補助編號:  
NSC 95-2221-E-011-098-MY3 
會議名稱 (中文) 生產與作業管理學會 2009 年之全球性挑戰和機會 
(英文) POMS 2009: GLOBEL CHALLENGES AND 
OPPORTUNITIES 
發表論文題目 (中文) 應用變動鄰域搜尋法求解育嬰配方推廣之醫院拜訪排程 
(英文) An application of variable neighborhood search to hospital call 
scheduling of infant formula promotion 
一、參加會議經過及其學術地位、重要性： 
POMS (Production and Operations Management Society) 2009 會議係國際作業
管理與應用領域十分重要的國際會議，該學會所出版的學術期刊 Production and 
Operations Management 是 MS/OR 領域排名第一的刊物，且已是美國著名刊物
Business Week 的 20 個頂尖期刊之一（該期刊以此 20 個期刊作為全美商學院
MBA 的排名）。本研討會對我國作業管理與應用的研究發展助益良多，研討會
議程共 4 天，發表 200 多篇論文，每篇均經過多位專家審查推薦才被接受。參與
論文發表的國家約 10 多國; 包括 USA、Netherlands、Australia、Vietnam、Japan、
Taiwan、Hong Kong、China、Switzerland、Austria、Belgium、China 等。 
本屆 POMS 年度研討會副標題訂為全球的挑戰與機會「Global Challenges and 
Opportunities」，來自全球 45 個國家約 800 位學者以及研討會論文摘要參與了此
次盛會。研討會除了召開 POMS Board Meeting 外，分別以各 College 舉辦 Panels、
Workshops 及 Tutorials；Colleges 包括：Service Operations、Sustainable Operations、
Production Innovation and Technology Management、Human Behavior in Operations 
Management、Healthcare Operations Management、Supply Chain Management 等。
然而，本屆研討會之主軸顯然是在供應鏈與作業管理 (Operations and Supply 
Chain Management)。至於，本人和學生所發表的研討會論文則歸屬於 Healthcare 
Operations Management，被安排於 5 月 1 日上午 8 點 30 分第一場次第一順位發
表。 
