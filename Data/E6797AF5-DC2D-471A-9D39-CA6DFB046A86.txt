I 
目錄 
 
中文摘要.............................................................................................................................................................. 1 
Abstract ................................................................................................................................................................ 1 
1. 前言................................................................................................................................................................. 1 
2. 進階模擬退火................................................................................................................................................. 2 
2.1 產生函數 (GENERATING DISTRIBUTION)..................................................................................................................................... 2 
2.2 接受準則 (ACCEPTANCE CRITERION).......................................................................................................................................... 2 
2.3 退火排程 (ANNEALING SCHEDULE)............................................................................................................................................ 3 
2.4 再退火排程 (REANNEALING SCHEDULE).................................................................................................................................... 3 
3. 方法................................................................................................................................................................. 3 
4. 實驗結果......................................................................................................................................................... 5 
4.1 MACKEY-GLASS混沌時間序列 (CASE 1) .................................................................................................................................. 5 
4.2 THE LOGISTIC MAP (CASE 2) ........................................................................................................................................................ 6 
4.3 比較 ............................................................................................................................................................................................ 7 
5. 結論................................................................................................................................................................. 7 
參考文獻.............................................................................................................................................................. 8 
2 
在過去的 20年間，神經網路(neural networks, 
NNs)已經被用於藉由成對的輸入-輸出樣式準確
地 塑 模 非 線 性 系 統 。 倒 傳 遞 神 經 網 路
(back-propagation neural network, BPNN)是一個眾
所周知的 NN 並且已經被運用在許多應用中，像
是臉部辨識(face recognition) [5]與垃圾郵件過濾
系統(spam filtering systems) [6]。不幸的是，標準
的 BPNN在求解複雜問題時，有陷入局部最佳解
的傾向。這是因為傳統的 BPNN使用最陡坡降法
(the steepest descent method)以修正權重。此外，
BPNN 的最佳網路拓樸(network topology)是藉由
試誤法來選取的。因此，與 BPNN相關的兩個最
佳化議題受到關注，即最佳的網路拓樸與權重。 
基因演算法(genetic algorithms, GAs)已將被
用於這兩種最佳化問題。例如：Kim et al. [7] 使
用 GA最佳化 BPNN的網路拓樸與參數，包括在
隱藏層中神經元的數目、學習率與慣性項
(momentum term)，並且使用這個 GA 基礎的
BPNN以估計成本；Wang與 Huang [8] 採用一個
二元編碼的 GA 以求得 BPNN 的網路拓樸與參
數，包括在隱藏層中神經元的數目、學習率與慣
性項，並將此 GA基礎的 BPNN應用於混沌時間
序列；Lee與Kang [9] 使用一個GA最佳化 BPNN
的網路拓樸與參數，包括在隱藏層中神經元的數
目、學習率與慣性項。然而，本計畫聚焦於最佳
化 BPNN的權重。 
本計畫提出一個可供選擇的隨機全域最佳化
方法 (stochastic global optimization method)於
BPNN。Metropolis 等人 [10] 首先使用蒙地卡羅
方法(Monte Carlo method)模擬金屬的物理退火過
程。他們的方法直到 Kirkpatrick 等人 [11] 使用
模擬退火 (simulated annealing, SA)算法求解像是
旅行者銷售問題(traveling salesman problems)的組
合最佳化問題後才受到重視。許多 SA 算法已經
被用於具有連續變數的最佳化問題，這些算法已
被證明在統計上收斂到全域最佳解 [12, 13]。 
進階模擬退火 (advanced simulated annealing, 
ASA) 算法是被設計於極小化一個具有邊界解空
間的多維度目標函數。由 Ingber  [12, 14] 所提出
的 ASA 算法的求解能力是優於二元編碼的 GA 
[15]。具有指數退火排程 (exponential annealing 
schedule)的ASA算法收斂會快於具有倒數退火排
程(inverse annealing schedule)的快速 SA算法，更
快於具有對數退火排程 (logarithmic annealing 
schedule)的標準 SA算法。 
因此，本計畫聚焦於 ASA 算法。ASA 算法
已經被採用在許多應用上。例如：Wu [16]發展一
個 ASA 算法求解受限制全域最佳化 (constrained 
global optimization, CGO) 問題，實驗結果指出
ASA 算法可以收斂至 CGO 問題的全域最佳解；
Li等人 [17] 提出一個 ASA基礎的土地資料同化
系統(land data assimilation scheme)，結論指出他們
的 ASA 算法可以成功的使用於高度非線性的土
地表面模型(land surface models)。 
為了克服標準 BPNN與傳統時間序列分析方
法的限制，本計畫使用一個 ASA算法在固定網路
拓樸下，更新 BPNN 的權重。此 ASA 算法基礎
的 BPNN (ASA-BPNN)是使用兩個從文獻上[18, 
19] 收集而來之標竿的混沌時間序列(chaotic time 
series)問題而加以評估。 
2. 進階模擬退火 
ASA算法使用四個運算元，說明如下。 
2.1 產生函數 (generating distribution) 
ASA算法的產生函數已經被證明，在一個給
定的退火排程下，在統計上收斂到能量函數(或稱
成本本函數) ( )E ⋅ 的全域最佳解 [15]。ASA使用產
生函數 )(yG 產生候選解，如式(1)所示： 
 1 , ,
1( )
2( ) ln(1 1/ )
1,2, ,
N
n n g n g n
G
y T T
n N
=
= + +
=
∏y
…
     (1) 
其中 
y = 1 2[ , , , ]Ny y y" ，介於 ]1,1[− 區間的隨機變
數  
,g nT =第 n個決策變數的產生溫度  
N = 決策變數的個數 
2.2 接受準則 (acceptance criterion) 
Metropolis接受準則 [10] 被用於接受或拒絕
候選解，如式(2)所示： 
trial current
current trial current trial
trial current
1                                           if ( ) ( )
( , , ) ( ) ( )exp   if ( ) ( ) a
a
E E
A T E E E E
T
<⎧⎪= ⎡ ⎤−⎨ ≥⎢ ⎥⎪ ⎣ ⎦⎩
x x
x x x x x x
4 
調整 [20]。ASA-BPNN使用實數表示這些權重，
圖 2 顯示候選解的表述，其中權重即代表決策變
數 nx ( 1,2,...,n N= )。 
 
max max,p qI L
IW
1 1,I L
IW
1 2,I L
IW "
候選解
1
1
Lb 2
1
Lb " max
1
qL
b
1 1,I L
LW
2 2,I L
LW "
1max ,pI L
LW
1
2
Lb
,p qI L
IW 1
qL
b
1,pI L
LW
1x 2x Nx1Nx −  
圖 2  ASA-BPNN之候選解表述 
圖 3為 ASA-BPNN的虛擬程式碼，各步驟說
明如下。 
begin
步驟1：初始化參數設定與正規化期望輸出
0←k
0←p
(a) 產生 , , max(0), (0),  z , ,  and  g n a a g nT T z k
0current xx ←
(b) 產生一個隨機的起始點
elseif                                                                      then 
步驟6：執行再退火排程
end
步驟2：計算成本函數
endWhile               
步驟3：建立試驗候選解 trialx
步驟4：接受或拒絕候選解
if                                      then )()( currenttrial xx EE <
trialcurrent xx ←
endIF
trialcurrent xx ←
1+← pp
()))(/)()(exp( randkTEE a >− trialcurrent xx
步驟5：執行退火排程
1/
, , ,( 1) (0) exp( )
N
g n g n g nT k T z k+ ← × − ×
1/( 1) (0) exp( )Na a aT k T z p+ ← × − ×
1+← kk
1/
, , ,( 1) (0) exp( )
N
g n g n g n nT k T z β+ ← × − ×
for每1000個退火週期 do
endFor
0x
trial, current, ( )
u l
n n n n nx x y x x← + −
While                domaxkk <
Nn ,,2,1 …=
Procedure ASA-BPNN
end
( )E ⋅
 
圖 3  ASA-BPNN的虛擬程式碼 
步驟 1：初始化參數設定與正規化期望輸出 
許多參數像是 , (0)g nT 、 (0)aT 、 az 、 ,g nz 與最大
的退火週期( maxk ) 必須被設定。一個起始點 0x 是
隨機地從權重與偏權重的下界與上界而產生 
, ,[ , ]p q p q
l u
I L I LIW IW = ,1 ,1[ , ]p p
l u
I ILW LW 與
1, 1,[ , ]
q q
l u
L Lb b =
2, 2,
1 1[ , ]
l ub b 並且被定義為 currentx 。 
每一個期望輸出 ,d jy  ( total1, 2, ,j n= … )會經過正
規化，如式(9)所示： 
min
,'
, max min min totalmax min ( ) , 1, 2, ,
d j d
d j
d d
y y
y E E E j n
y y
−= − + =− …  
(9) 
其中 
'
,d jy = 正規化後的期望輸出 j  
min
dy = 期望輸出向量 total,1 ,1 ,[ , , ]
T
d d d d ny y y=Y … 的極
小值 
max
dy = dY 的極大值 
minE = 正規化後期望輸出的極小值 
maxE = 正規化後期望輸出的極大值 
 [ min max,E E ]通常設定為[0.2, 0.8]。 
步驟 2：計算成本函數 ( )E ⋅  
正規化的均方根誤 (normalized root mean 
square error, NRMSE)是 BPNN 的性能指標
(performance index)，它被用來當作 ASA-BPNN的
( )E ⋅ ，如式(10)所示： 
train
' 2
, ,
1
train
train
( - )
( ) , 1, 2,...,
n
d j o j
j
y y
NRMSE E j n
n
== ⋅ = =
∑
 
 (10) 
步驟 3：建立試驗候選解 
試驗的候選解使用式(11)而產生，如下所示： 
trial, current ,
trial,
( ),
[ , ] 1, 2,...,
u l
n n n n n
l u
n n n
x x y x x
x x x n N
= + −
∈ =       (11) 
其中 ny 是由 , ( )g nT k 與均勻隨機變數 (0,1)nU 而
建立，如式(12)所示： 
2 (0,1) 1
, ,sgn( (0,1) 0.5) ( )[(1 1/ ( )) 1]
[ 1,1], 1, 2, ,
nU
n n g n g n
n
y U T k T k
y n N
−= − + −
∈ − = …
 
(12) 
步驟 4：接受或拒絕候選解 
式(2)被用於定義接受或者拒絕一個候選解。 
步驟 5：執行退火排程 
6 
比較表 1(參數設定 ,g nz = az = 20 與 maxq = {3, 
4, 5})與表 2的實驗結果，可得知 ASA-BPNN的
平均學習與平均測試的準確度是優於標準的
BPNN。 
圖 4(a)為繪製學習樣式之個數與正規化後的
期望輸出之時間序列圖。圖 4(b)顯示 ASA-BPNN
的收斂圖。圖 4(c)顯示學習樣式之個數與塑模誤
差(modeling error)的直方圖，由圖中可知誤差皆
位於[−0.03, 0.02]之間。 
圖 5(a)繪製測試樣式與正規化後的期望輸出
與網路輸出的曲線。由圖中可知，期望輸出與網
路輸出是擬合的。圖 5(b)顯示樣式之個數與歸納
誤差(generalization error)的直方圖，由圖中可知
誤差皆位於[−0.02, 0.02]之間，意指測試的結果理
想。 
0 100 200 300 400 500 600 700 800 900 1000
0.2
0.4
0.6
0.8
1
(a) Number of training patterns
N
or
m
al
iz
ed
 d
es
ire
d 
ou
tp
ut
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
0
0.1
0.2
0.3
0.4
(b) Number of generations
N
or
m
al
iz
ed
 R
M
S
E
0 100 200 300 400 500 600 700 800 900 1000
-0.04
-0.02
0
0.02
0.04
(c) Number of training patterns
E
rro
r
 
圖 4  ASA-BPNN於Case 1所獲得之最佳解的學
習結果圖示 
0 10 20 30 40 50 60 70 80 90 100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(a) Number of testing patterns
O
ut
pu
t
normalized desired output
actual network output
0 10 20 30 40 50 60 70 80 90 100
-0.04
-0.03
-0.02
-0.01
0
0.01
0.02
0.03
0.04
(b) Number of testing patterns
E
rro
r
 
圖 5  ASA-BPNN於Case 1所獲得之最佳解的測
試結果圖示 
4.2 The Logistic map (Case 2) 
一個具有 1100 輸入樣式的資料集(dataset)依
據 the Logistic map方程式而產生，如式(15)所示： 
( 1) 4 ( )[1 ( )]y t y t y t+ = −         (15) 
初始的設定為 )0(y = 0.2，一個輸入變數 )(ty
被用以預測 )1( +ty 。 
表 3列示 ASA-BPNN於 Case 2的實驗結果。
由表中可知，在參數設定( ,g nz = az = 10, maxq = 
4)、( ,g nz = az = 20, maxq = 5)和( ,g nz = az = 30, 
maxq = 5)下，可獲得理想的平均學習與測試的
NRMSEs。 
表 3  ASA-BPNN的實驗結果 (Case 2) 
,g nT (0)= 2000, aT (0)= 1000 and maxk = 10000 
,g nz = az = 10 
maxq = 3 maxq = 4 maxq = 5 
平均 
學習的 
NRMSE
0.02281 
平均 
學習的 
NRMSE 
0.01678 
平均 
學習的 
NRMSE
0.02914 
平均 
測試的
NRMSE 
0.02246 
平均 
測試的
NRMSE 
0.01668 
平均 
測試的
NRMSE 
0.02947 
MCCT 33.39 MCCT 40.72 MCCT 48.73 
,g nz = az = 20 
maxq = 3 maxq = 4 maxq = 5 
平均 
學習的 
NRMSE
0.04332 
平均 
學習的 
NRMSE 
0.02293 
平均 
學習的 
NRMSE
0.01947 
平均 
測試的
NRMSE 
0.04333 
平均 
測試的
NRMSE 
0.02242 
平均 
測試的
NRMSE 
0.01897 
MCCT 30.74 MCCT 41.56 MCCT 49.95 
,g nz = az = 30 
maxq = 3 maxq = 4 maxq = 5 
平均 
學習的 
NRMSE
0.04700 
平均 
學習的 
NRMSE 
0.02337 
平均 
學習的 
NRMSE
0.01880 
平均 
測試的
NRMSE 
0.04764 
平均 
測試的
NRMSE 
0.02289 
平均 
測試的
NRMSE 
0.01842 
MCCT 33.03 MCCT 39.57 MCCT 50.56 
 
表 4 為在這三組參數設定下所獲的結果之
Tukey 檢定，由此多重比較可知，在這三種參數
設定下所獲的平均學習與測試的NRMSEs在統計
上是不顯著的(因為它們的區間都包含 0)，亦即它
們的平均學習與測試的 NRMSEs是相近的。本計
畫選取的最佳參數為( ,g nz = az = 20, maxq = 5)，因
為它可以獲的最佳的學習 NRMSE=0.00121 與測
試 NRMSE=0.00117。 
表 4  Tukey檢定 
( ,g nz = az = 10, maxq = 4) subtracted from: 
Parameter settings            Lower     Center   Upper 
( ,g nz = az = 20, maxq = 5)  −0.00500    0.00269  0.01037 
( ,g nz = az = 30, maxq = 5)  −0.00566    0.00202  0.00970 
8 
參考文獻 
 
[1] C. Chatfield, Time-Series Forecasting, 
Chapman & Hall/CRC, 2001. 
[2] M. B. Priestley, Spectral Analysis and Time 
Series, Academic Press, 1981. 
[3] G. Zhang, et al., “Forecasting with Artificial 
Neural Networks: The State of Art,” 
International Journal of Forecasting, vol. 14, 
no. 1, 1998, pp. 35-62. 
[4] N. R. Rigozo, et al., “Comparative Study 
between for Classical Spectral Analysis 
Methods,” Applied Mathematics and 
Computation, vol. 168, no. 1, 2005, pp. 
411-430. 
[5] T. H. Sun and F. C. Tien, “Using 
Backpropagation Neural Network for Face 
Recognition with 2D+3D Hybrid Information,” 
Expert Systems with Applications, vol. 35, no. 
1-2, 2008, pp. 361-372. 
[6] H. Xu and B. Yu, “Automatic Thesaurus 
Construction for Spam Filtering Using Revised 
Back Propagation Neural Network,” Expert 
Systems with Applications, vol. 37, no. 1, 2010, 
pp. 18-23. 
[7] G. H. Kim, et al., “Neural Network Model 
Incorporating a Genetic Algorithm in 
Estimating Construction Costs,” Building and 
Environment, vol. 39, no. 11, 2004, pp. 
1333-1340. 
[8] T. Y. Wang and C. Y. Huang, “Applying 
Optimized BPN to a Chaotic Time Series 
Problem,” Expert Systems with Applications, 
vol. 32, no. 1, 2007, pp. 193-200. 
[9] J. Lee and S. Kang, “GA Based Meta-Modeling 
of Bpn Architecture for Constrained 
Approximate Optimization,” International 
Journal of Solids and Structures, vol. 44, no. 
18-19, 2007, pp. 5980-5993. 
[10] N. Metropolis, et al., “Equation of State 
Calculations by Fast Computing Machines,” 
The Journal of Chemical Physics, vol. 12, no. 6, 
1953, pp. 1087-1092. 
[11] S. Kirkpatrick, et al., “Optimization by 
Simulated Annealing,” Science, vol. 220, no. 
4598, 1983, pp. 671-680. 
[12] L. Ingber, “Very Fast Simulated 
Re-Annealing,” Mathematical and Computer 
Modelling, vol. 12, no. 8, 1989, pp. 967-973. 
[13] H. Szu and R. Hartley, “Fast Simulated 
Annealing,” Physics Letters A, vol. 122, no. 3-4, 
1987, pp. 157-162. 
[14] L. Ingber, “Simulated Annealing: Practice 
Versus Theory,” Mathematical and Computer 
Modelling, vol. 18, no. 11, 1993, pp. 29-57. 
[15] L. Ingber and B. Rosen, “Genetic Algorithms 
and Very Fast Simulated Reannealing: A 
Comparison,” Mathematical and Computer 
Modelling, vol. 16, no. 11, 1992, pp. 87-100. 
[16] J. Y. Wu, “Advanced Simulated Annealing for 
Solving Constrained Global Optimization 
Problems,” Far East Journal of Experimental 
and Theoretical Artificial Intelligence, vol. 3, 
no. 2, 2009, pp. 143-168. 
[17] X. Li, et al., “A Very Fast Simulated 
Re-Annealing (VFSA) Approach for Land Data 
Assimilation,” Computers & Geosciences, vol. 
30, no. 3, 2004, pp. 239-248. 
[18] M. C. Mackey and L. Glass, “Oscillation and 
Chaos in Physiological Systems,” Science, vol. 
197, no. 4300, 1977, pp. 287-289. 
[19] J. Moody and C. J. Darken, “Fast Learning in 
Networks of Locally-Tuned Process Units,” 
Neural Computation, vol. 1, no. 2, 1989, pp. 
281-294. 
[20] L. Fausett, Fundamentals of Neural Networks: 
Architectures, Algorithms and Applications 
Prentice-Hall, 1994. 
[21] J. Y. Wu, “Cerebellar Model Articulation 
Controller for Predicting Chaotic Time Series,” 
The 12th Conference on Artificial Intelligence 
and Applications, 2007, pp. 508-515. 
1 
目錄 
一、參加會議經過 ............................................................................................................................................. 1 
二、與會心得 ..................................................................................................................................................... 4 
三、攜回資料名稱及內容 ................................................................................................................................. 5 
附錄：所發表的會議論文 
 2
 
 
圖 1 ISNN 2010 6 月 7 日議程海報 
 
圖 2 ISNN 2010 6 月 8 日議程海報 
 
圖 3  ISNN 2010 6 月 9 日議程海報 
 
圖 4  ISNN 2010 6 月 7 日開幕式一 
 
圖 5  ISNN 2010 6 月 7 日開幕式二 
 
圖 6  ISNN 2010 6/7 Plenary Talks 一  
 4
 
圖 13  本人於 ISNN 2010 6 月 9 議程海報前留影
 
圖 14  本人與輔仁大學研究生合影 
二、與會心得 
第七屆國際神經網路研討會(The 7th International Symposium on Neural Networks)是由上海交
通大學與香港中文大學主辦。ISNN 2010 的徵文主題為： 
1. Computational Neuroscience and Cognitive Science 
2. Models, Methods and Inference 
3. Vision and Auditory Modelling 
4. Control, Robotics and Hardware 
總計收到 591 篇投稿論文，其中挑選 332 篇在研討會中進行口頭報告或海報報告。會議論文集 
由 Springer 發行於 Lecture Notes in Computer Science (LNCS 6063): Advances in Neural Networks 與
Lecture Notes in Electrical Engineering(vol. 67): Advances in Neural Network Research and 
Applications。本次所發表的論文“Forecasting Financial Time Series via an Efficient CMAC Neural 
Network”收錄在 LNEE。LNCS 與 LNEE 皆被 EI 資料庫索引。 
技術論文的發表安排在 6/7~6/9。本人於 6/9 的場次 WAA: Forecasting Financial Time Series via an 
Efficient CMAC Neural Network 發表論文。在口頭報告的過程以及會後，與澳洲研究者就預測模型
的預測方向的正確性議題進行討論，並和中國大陸的研究者就股價預測與金融海嘯的影響交換意
見，在此過程獲益良多。與會心得摘要如下： 
1. 日後將更積極參加被 EI 資料庫索引的國際研討會，藉以提升國際視野與積累研究能量。 
2. 參加與研究領域相關的國際研討會，可以瞭解該研究領域發展趨勢與主流的方法。 
 6
圖 17  ISNN 2010 會議手冊 
 
圖 18  ISNN 2010 參加證 
 
圖 19  ISNN 2010 手提包 
 
 
 
本次所發表的會議論文請見附錄。 
 
Z. Zeng & J. Wang (Eds.): Adv. in Neural Network Research & Appli., LNEE 67, pp. 73–82. 
springerlink.com                                                     © Springer-Verlag Berlin Heidelberg 2010 
Forecasting Financial Time Series via an Efficient  
CMAC Neural Network  
Chi-Jie Lu1 and Jui-Yu Wu2,* 
1
 Department of Industrial Engineering and Management, 
Ching Yun University, Taiwan 
jerrylu@cyu.edu.tw 
2
 Department of Business Administration,  
Lunghwa University of Science and Technology, Taiwan 
jywu@mail.lhu.edu.tw 
Abstract. Cerebellar model articulation controller neural network (CMAC NN) 
has many advantages, such as very fast learning, reasonable generalization 
capability and robust noise resistance. Thus, CMAC NNs are conventionally 
used in robot control. To solve financial time series forecasting, this paper 
presents an efficient CMAC NN scheme. The proposed CMAC NN transforms 
continuous values of input variables to discrete indexes by using a quantization 
operator. To enhance generalization ability, the CMAC NN employs high 
quantization resolution and a large generalization size. To perform many-to-few 
mappings, the CMAC NN uses an efficient and fast hashing code based on 
bitwise XOR operator. The proposed CMAC NN was used to Nikkei 225 
closing cash indexes collected from Japanese stock market. The forecasting 
results of the proposed CMAC NN were compared with those of support vector 
regression (SVR), which is statistical/ machine learning algorithm. 
Experimental results indicate that the performance of the CMAC NN was better 
than SVR in the tested case. Therefore, the CMAC NN may be considered as an 
efficient tool for forecasting financial time series. 
Keywords: cerebellar model articulation controller, neural network, stock price 
prediction, support vector regression. 
1   Introduction 
Over the past three decades, there has been a growing interest in financial time series 
forecasting [1, 2]. Stock index forecasting is regarded as a challenging task of the 
financial time series prediction process since the stock market is a complex, 
evolutionary, and nonlinear dynamic system [3].  Many techniques have been applied 
to the domain of time series prediction, and are traditionally categorized as statistical 
methods and spectral analysis [4, 5]. Zhang et al. [6] found that the traditional 
statistical methods based on linear models, such as Box-Jenkins approach, have 
difficulty in measuring the performance of real systems with nonlinear behavior. 
Rigoze et al. [7] compared the advantages and drawbacks of four classical spectral 
analytical methods. These approaches are difficult to apply for general practitioners; 
 Forecasting Financial Time Series via an Efficient CMAC Neural Network 75 
The indexes ijs  depends on 
min
ix , 
max
ix  and iN . Larger values of iN  improve the 
accuracy of representation, but require a large weight table w . 
 
(2) →S A  
A sensory cell comprises random tables [19]. The size of each random table is 
calculated by: 
1, 1,2, ,i iC N g i n= + − = …  (3) 
where 
iC = size of the random table of input vector ix  
g = generalization size 
The indexes ijs  are mapped to random table i , and segment mappings are created 
based on g . The quantization and segment mappings generate natural interpolation, 
and give the CMAC NN the ability to generalize [20]. A large g  increases 
generalization ability, but reduces the approximation accuracy. 
 
(3) → →A P Y  
A vector a  in cell A  corresponds to a table w  in cell P . The table w  stores many 
weights. An actual output value in cell Y  can be computed by the matrix operation: 
T
oy = a w  (4) 
where 
 
a = 
1
2
k
a
a
a
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
# , w = 
1
2
k
w
w
w
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
#  
oy = actual output, k = size of w and T = transpose of a matrix. 
2.2   SVR 
The SVR is an adaptation of recently introduced statistical/ machine learning theory 
based classification paradigm namely, support vector machines. For illustrating the 
concept of SVR, a typical regression problem is formulated. Consider a dataset 
,
( ,  )ij d jG x y= ( 1,2,...,i n= , total1, 2,...,j D= ), where ijx  are input patterns and ,d jy  is 
desired output. The goal of the regression analysis is to determine a function ( )f x , so 
as to predict accurately the 
,d jy . The regression problem can be classified as linear 
and nonlinear regression problems. As the nonlinear regression problem is more 
difficult to deal with, SVR was mainly developed for dealing with the nonlinear 
regression problem. SVR algorithms have been successfully applied in time series 
prediction, such as and financial time series forecasting [21, 22]. When constructing a 
 Forecasting Financial Time Series via an Efficient CMAC Neural Network 77 
Step 2: Create random tables and weight table w  
Equation (3) is used to compute the size of each random table, and the random tables 
are then created. To control the range of the index of generated address table, each 
random table consists of uniform random numbers created from the interval [ 0, / 2k ]. 
A table w  is generated based on k  as defined in Section 2, with initial weights set to 
“0”. To reduce the probability of hash collision, this paper used a large value of k = 
10000. 
 
Step 3: Generate address table 
To implement many-into-few mapping, the proposed CMAC NN used a hash coding 
based on bitwise XOR operator [24]. The detail description of the hash coding can be 
found in the literature [15]. 
 
Step 4: Calculate actual output 
Equation (4) is used to measure the values 
,o jy  ( train1, 2, ,j D= … ). 
Step 5: Learning rule 
Equation (6) is employed to modified the weights in w , as follows: 
max
( )( 1) ( ) , 1, 2, , 1, 2, ,d ol l
y y
w h w h l k h epoch
g
β −
+ = + = =… …  (6) 
where β = learning rate 
 
Step 6: Evaluate training accuracy  
A normalized root mean square error (NRMSE) is used as a performance index to 
evaluate a training accuracy, as follows: 
train
' 2
, ,
1
train
( )D d j o jj y yNRMSE
D
=
∑ −
=  
(7) 
where 
trainD = number of input patterns for training 
 
Steps 3–6 are repeated until the termination condition maxepoch  is met. 
 
Step 7: Measure testing accuracy 
Input pattern j  ( train train total1, 2, ,j D D D= + + … ) is used for testing. The actual 
outputs of recall of the CMAC NN can be calculated using the w  obtained in the 
training stage (Steps 1−6), and then the generalization accuracy of the CMAC NN can 
be measured by a testing NRMSE. 
4   Experimental Results 
For evaluating the performance of the proposed CMAC NN forecasting model, the 
daily Nikkei 225 closing cash index dataset is used in this paper. In forecasting Nikkei 
 Forecasting Financial Time Series via an Efficient CMAC Neural Network 79 
of the SVR model with combinations of different parameter sets are summarized in 
Table 2. From Table 2, it can be found that the parameter set (C=23, ε =2-9) gives the 
best forecasting result (minimum testing RMSE) and is the best parameter set for 
SVR model in forecasting Nikkei 225 closing cash index. 
 
Table 1. Model selection results of the proposed
CMAC NN model-Nikkei 225 
 
   Table 2. Model selection results of the SVR 
forecasting model-Nikkei 225 
 
 
The prediction performance is evaluated using the following performance 
measures, namely, the root mean square error (RMSE), mean absolute difference 
(MAD), mean absolute percentage error (MAPE), directional accuracy (DA), correct 
up trend (CP) and correct down trend (CD). The definitions of these criteria can be 
found in Table 3. RMSE, MAD and MAPE are measures of the deviation between 
actual and predicted values. 
Table 3. Performance measures and their definitions 
Metrics Calculation Metrics Calculation 
RMSE 
total
2
, ,
1
total
( )
D
d j o j
j
y y
RMSE
D
=
−
=
∑ DA 
total
1total
100 D
j
j
DA d
D
=
= ∑ , where 
, , 1 , , 11 ( )( ) 0
   
0
o j o j d j d j
j
y y y y
d
otherwise
− −
− − ≥⎧
= ⎨⎩
 
MAD 
total
, ,
1
total
D
d j o j
j
y y
MAD
D
=
−
=
∑
 
CP 
total
11
100 D
j
j
CP d
n
=
= ∑ , where 
, , 1 , , 11 ( )( ) 0
   
0
o j o j d j d j
j
y y y y
d
otherwise
− −
− − ≥⎧
= ⎨⎩
 
MAPE 
total
, ,
1 ,
total
D
d j o j
j d j
y y
y
MAPE
D
=
−
=
∑
 
CD 
total
12
100 D
j
j
CD d
n
=
= ∑ , where 
, , 1 , , 11 ( )( ) 0
   
0
o j o j d j d j
j
y y y y
d
otherwise
− −
− − ≥⎧
= ⎨⎩
 
* 1n  is number of data points belong to up trend and 2n  is number of data points belong to down trend. 
 Forecasting Financial Time Series via an Efficient CMAC Neural Network 81 
3. Atsalakis, G.S., Valavanis, K.P.: Surveying Stock Market Forecasting Techniques - Part II: 
Soft Computing Methods. Expert Systems with Applications 36, 5932–5941 (2009) 
4. Chatfield, C.: Time-Series Forecasting. Chapman & Hall/CRC, New York (2001) 
5. Priestley, M.B.: Spectral Analysis and Time Series. Academic Press, New York (1981) 
6. Zhang, G., Patuwo, B.E., Hu, M.Y.: Forecasting with Artificial Neural Networks: The 
State of Art. International Journal of Forecasting 14, 35–62 (1998) 
7. Rigozo, N.R., Echer, E., Nordemann, D.J.R., et al.: Comparative Study between for 
Classical Spectral Analysis Methods. Applied Mathematics and Computation 168, 411–
430 (2005) 
8. Albus, J.S.: A New Approach to Manipulator Control: The Cerebellar Model Articulation 
Controller (CMAC). ASME Journal of Dynamic Systems, Measurement, and Control 97, 
220–227 (1975) 
9. Albus, J.S.: Data Storage in the Cerebellar Model Articulation Controller (CMAC). ASME 
Journal of Dynamic Systems, Measurement, and Control 97, 228–233 (1975) 
10. Wong, Y.F., Sideris, A.: Learning Convergence in the Cerebellar Model Articulation 
Controller. IEEE Transactions on Neural Networks 3, 115–121 (1992) 
11. Lin, C.M., Chen, L.Y., Chen, C.H.: RCMAC Hybrid Control for MIMO Uncertain 
Nonlinear Systems Using Sliding-Mode Technology. IEEE Transactions on Neural 
Networks 18, 708–720 (2007) 
12. Peng, Y.F.: Robust Intelligent Sliding Model Control Using Recurrent Cerebellar Model 
Articulation Controller for Uncertain Nonlinear Chaotic Systems. Chaos, Solitons & 
Fractals 39, 150–167 (2009) 
13. Wang, S., Jiang, Z.: Valve Fault Detection and Diagnosis Based on CMAC Neural 
Networks. Energy and Buildings 36, 599–610 (2004) 
14. Hung, C.P., Wang, M.H.: Diagnosis of Incipient Faults in Power Transformers Using 
CMAC Neural Network Approach. Electric Power Systems Research 71, 235–244 (2004) 
15. Wu, J.Y., Lu, C.J.: Applying Classification Problems via a Data Mining Approach Based 
on a Cerebellar Model Articulation Controller. In: 1st Asian Conference on Intelligent 
Information and Database Systems, pp. 61–66. Dong Hoi City, Vietnam (2009) 
16. Lin, C.J., Lee, J.H., Lee, C.Y.: A Novel Hybrid Learning Algorithm for Parametric Fuzzy 
CMAC Networks and Its Classification Applications. Expert Systems with 
Applications 35, 1711–1720 (2008) 
17. Zhou, H., Chen, J., Wu, H., et al.: CMAC-Based Short-Term Electricity Price Forecasting. 
In: Sixth International Conference on Advances in Power System Control, Operation and 
Management, Hong Kong, pp. 348–353 (2003) 
18. Qiaolin, D., Jing, T., Jianxin, L.: Application of New FCMAC Neural Network in Power 
System Marginal Price Forecasting. In: The 7th International Power Engineering 
Conference, Singapore, pp. 1–57 (2005) 
19. Lee, J.: Measurement of Machine Performance Degradation Using a Neural Network 
Model. Computers in Industry 30, 193–209 (1996) 
20. Handelman, D.A., Lane, S.H., Gelfand, J.J.: Integrating Neural Networks and Knowledge-
Based Systems for Intelligent Robotic Control. IEEE Control Systems Magazine 10, 77–87 
(1990) 
21. Huang, C.L., Tsai, C.Y.: A Hybrid SOFM-SVR with a Filter-Based Feature Selection for 
Stock Market Forecasting. Expert Systems with Applications 36, 1529–1539 (2009) 
22. Lu, C.J., Lee, T.S., Chiu, C.C.: Financial Time Series Forecasting Using Independent 
Component Analysis and Support Vector Regression. Decision Support Systems 47, 115–
125 (2009) 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 ■未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
本計畫部分的成果已發表於國際會議中。The ICEIE (2010), Kyoto, Japan,1-3 
August,pp.38-43. 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
未來進一步發展的可能性可分為兩個部分： 
一、可與其它隨機最佳化方法基礎的 BPNN 的實驗結果之比較。本計畫所提出的 ASA-BPNN
將與人工免疫系統(artificial immune system, AIS)基礎的 BPNN(AIS-BPNN)以及基因演
算法基礎的 BPNN 就預測混時間序列的性能進行比較，其中 AIS-BPNN 的研究成果已投稿於
TAAI 2010，並獲接受。 
二、可嘗試將 ASA-BPNN 應用於預測真實世界的財務時間序列(financial time series)並
與其它神經網路(例如：小腦模型神經網路)的預測能力進行比較。與本計畫相關的研究已
進行並已於國際會議中發表。 
Chi-Jie Lu, Jui-Yu Wu*, ＇Forecasting Financial Time Series via an Efficient CMAC 
Neural Network,＇ The 7th International Symposium on Neural Networks, Shanghai, 
China, 6-9 June 2010, pp. 73-82. 
