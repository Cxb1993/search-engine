  
中文摘要 
關鍵字: 視訊傳播系統、可調式視訊編碼、平行處理、內嵌式系統、電腦視覺，速率-失真度-計算
最佳化、H.264/AVC。 
嶄新的視訊傳播系統創造了具彈性、可調適的視訊數位資料項的技術需求，不同型態的前端裝置
如 PC、PDA、多媒體終端機、智慧型手機、車用多媒體裝置等均已透過不同型態的網路(有線或無
線)連結到各別的多媒體伺服器，異質化的視訊處理環境大大增加信號處理的複雜度。然而，配合無
線網路的快速發展，無所不在的視訊傳播系統正一步一步成長中。 
可調式視訊編碼技術，是一項可解決當前化的視訊處理環境的信號處理問題的備受矚目的視訊傳
播技術，本計畫討論 SVC 的主要關鍵技術及其編碼效率，並提出一適合平行化處理、以軟體為主的
快速 SVC 編碼架構，據以改善兼顧速率-失真的 SVC 的編碼效率，軟體式平行處理架構具有彈性的
處理方式，更適合用以架設即時的 SVC 編碼器；值得一提的是，本計劃所提出的軟體式 SVC 平行處
理架構，不僅可提供 SVC 時間可調適、空間可調適、及畫質可調適之三種視訊調適服務，我們也設
計一個即時 SVC 平台調適架構，所謂的平台調適指的是我們所提出的軟體式 SVC 可應用於不同處理
速度的硬體平台，即處理器計算速度也包含 SVC 視訊調適服務的考慮範圍，直到目前為止，就筆者
個人知識所及，仍未有相關文獻報導相關作法。可調適平台的 SVC 編碼器可進一步擴展 SVC 的潛在
應用層面。 
本計畫為一三年期計畫，第一年，我們將設計一基於線邊對齊模式的快速模式決策，這個方法的
基本概念在於使用電腦視覺技術，同時改善 H.264/AVC 的編碼效率及提出符合人類視覺系統的編碼
結果；第二年，根據第一年的研究成果，我們將提出一即時軟體式的 H.264/AVC 平行處理架構，這
個架構不僅可進一步提昇 H.264/AVC 的編碼效率，而且可提供平台調適性(Platform Scalability)，即在
計算能力不同的工作平台上，進行即時的視訊編碼動作；第三年則整合前兩年的研究成果，擴展為
包含時間可調適、空間可調適、畫質可調適、及平台可調適四維度的 SVC 編碼系統，實驗結果驗證
本計畫提出的方法的可行性。 
 報告內容 
ABSTRACT 
This report presents a novel block matching scheme with 
edge alignment strategy on H.264 video coding, which uses 
multiple references and multiple block sizes for motion 
estimation in order to improve the rate-distortion performance. 
In H.264, the computational complexity is linearly dependent 
on the number of allowed reference frames and block sizes 
using the full exhaustive search. Many fast block-matching 
algorithms reduce the computational complexity of motion 
estimation by carefully designing the search patterns with 
different shapes or sizes which have significant impact on the 
search speed and distortion performance. However, the search 
speed and the distortion performance conflict often with each 
other for these methods. In this paper, given a block in the 
current frame, we first apply a fast approximate method based 
on edge alignment to obtain a good initial motion vector as 
well as a tight initial bound of distortion measure. Then, 
considering the edge orientation of the block, a modified 
hexagon search is used to fine tune the motion vector in low 
computational complexity. The proposed algorithm also pays 
attentions to the characteristics of multiple reference frames 
and multiple block sizes in H.264. Computer simulation 
results show that the proposed method gives good 
performance and spans a new way to design a cost-effective 
real-time video coding system. 
1. Introduction 
As the Internet advances, the demand of new ways to 
represent, integrate, store and exchange multimedia 
information (such as text, image, audio, and video) has 
increased. A remarkable change occurred in the video-driven 
applications such as teleconference, videophone, and 
image-based multimedia services, because of the increased 
role of synthetic information and new two-way 
communication systems. Advanced video coding techniques 
that yield reconstructed images with good subjective image 
quality to make efficient use of the available bandwidth 
facilitate the development of video-based applications over the 
Internet. Among these methods, H.264 is an emerging 
international video coding standard, made by Joint Video 
Team (JVT) consisting of ITU-T Video Coding Experts Group 
(VCEG) and ISO/IEC MPEG Video Coding Group [1]. 
Comparing to MPEG-4 [2], H.263 [3], and MPEG-2 [4], 
H.264 achieves 39%, 49%, and 64% of bit-rate reduction, 
respectively [5]. 
The performance improvement achieved by H.264 comes 
mainly from the prediction part [1], [6] involving the motion 
estimation (ME) at quarter-pixel with variable block sizes and 
multiple reference frames. These novel techniques greatly 
reduce the prediction errors. In H.264, there are seven kinds of 
block size (16x16, 16x8, 8x16, 8x8, 8x4, 4x8, 4x4) and up to 5 
reference frames for motion compensation (MC). The 
simulation results of the reference software of H.264 [7] show 
that it achieves more than 15% of bit rate reduction compared 
with only using block size 16x16 using variable block sizes 
and 5%-10% of bit rate reduction using multiple reference 
frames [8]. However, the complexity of ME in developing 
H.264 is obviously very high. Hence, it is a crucial issue to 
reduce the complexity of ME for H.264-based real-time video 
applications. In H.264, the factors to affect the performance of 
ME include: (1) the complexity of mode decision when doing 
motion estimation [9]; (2) the complexity of reference frames 
when doing motion estimation [10]; (3) the number of search 
points to complete a motion estimation process. As reported 
by Huang et al. [10], it takes more than 80% of execution time 
on doing motion estimation for the reference software of 
H.264/AVC, JM [7] to encode a video sequence. 
The block-matching algorithm (BMA) is adopted by H.264. 
Among all BMAs, the well-known full-search block-matching 
algorithm (FSBMA) aims at finding the position with minimal 
block distortion from all possible candidate motion vectors 
over a predetermined neighborhood search window. Although 
FSBMA produces the best quality, it demands the most 
computation. Many fast BMAs, such as three-step search (TSS) 
[11], Diamond Search (DS) [12], and Hexagon Based Search 
(HEXBS) [8], [13] have been proposed to speed up the 
FSBMA with acceptable distortion performance. In [15], Xu 
and He introduce an additional early termination scheme with 
adaptive thresholds for Hybrid Unsymmetrical-Cross 
Multi-Hexagon-Grid Search (UMHES) [14]. Gonzalez-Diaz 
and Diaz-de-Maria propose a motion classification-based 
search (MCS), which use a classifier based on  some motion 
cues to choose the best search patterns [16].  These 
algorithms can save a lot of search points compared with 
FSBMA through carefully designing the search patterns. 
Although the computing power of processors has been 
rapidly improved for the past decade, software-based real-time 
video encoders remain as a challenge even with the help of 
fast BMAs. Given a block in the current frame, the process to 
find the best match from previous frames using a fast BMA 
cannot be interrupted; traditional BMAs stop only when all the 
search points confined by a specific search pattern are 
examined. Thus, the search process might violate the real-time 
constraint –the time the process allowed to complete its work. 
A computation-aware scheme for software-based block motion 
estimation was proposed by Tai et al. [17] to allow the search 
process of block matching to stop once a specific amount of 
computation has been performed.  However, the 
computation-aware BMA faces two problems. First, it brings 
another time-consuming operator –to dynamically determine 
the target amount of computation power allocated to a frame, 
and then allocates this to each block on a 
computation-distortion-optimization manner [17]. To solve the 
problem, some heuristic approaches, which offer faster 
 how much the pixels agree according to certain criteria. On 
such function, often used in video coding because of its speed, 
is the sum of absolute differences (SAD) metric, i.e., 
 
|),(
),(|),(
1
1
1
1
1
,
jvyiuxI
jyixIvuSAD
t
N
i
N
j
tyx
++++−
++=
−
−
=
−
=
∑∑       (1) 
where  is the displacement and 
is called the 
displaced frame difference. Given a block in the current frame, 
the full searching technique tries all possible alignments and is 
too slow in practice. Besides block matching techniques with 
full or partial pixel information, feature-based alignment 
methods are possible to speed up the process of motion 
estimation. 
),( vuu =r
() 1Ijy t−+ − |),,(| jvyiuxixIt +++++
 
x
y
-1 1
-1
1
0
C
B
+( )yx,θl
h1
h2
Fig. 1. An edge model in a 4 x 4 block B. The circle C is inscribed in B, 
and  ),( yx  are the coordinates of the centers of gravity of the gray 
(color) values inside C. 
At the lowest level of computer vision, potentially useful 
visual patterns such as edges and line segments can be 
extracted from an image without any priori knowledge of the 
image content. In our approach, segmentation and detailed 
object representation are not required. A given image is 
partitioned into a set of non-overlapping square blocks. Each 
block is coded as either a uniform block or an edge block. The 
edge in each block is detected by the moment-preserving edge 
detection technique which was proposed in our previous work 
[27], and the image can be reconstructed according to the 
parameters of these blocks. The continuous two-dimensional 
edge model specified by four parameters, two representative 
gray (color) values h1 and h2, an edge translation l, and an 
orientation angle θ   for an edge in square block B is shown 
in Fig.1. The edge translation l is defined as the length from 
the center of the edge model to the transition, and is confined 
within the range of 2−  to 2+ . The parameter θ  specified 
the direction of the edge and is confined within the range of 0 
to 180 degrees. The solution to the edge detection problem in a 
given block is analytic and this means that the edge detection 
process can be performed very fast for large-database 
applications with no need for special hardware. 
 
B
B’
B
B’
Edge Alignment
 
(a) 
 
B
B’
B
B’
Edge Alignment
 
(b) 
 
Fig. 2. The process of edge alignment is to transform the edge pattern of a 
block to that of the other by translation and rotation: (a) the edge 
orientations for blocks B and B’ are the same; (b) the edge orientations 
for blocks B and B’ are different. 
 
In our approach, there are two cases to align the edge 
patterns of two blocks. First, given a pair of edge blocks (B, B’) 
with the same edge orientation, the edge of B is said to be 
aligned properly with that of B’ by translating B’ if necessary 
such that the edge pattern of B’ exactly coincides with that of 
B, shown in Fig. 2(a). What would happen to the process of 
edge alignment if the edge orientations for B and B’ are 
different? In this case, the edge pattern of B is first translated 
and then rotated by an angle which is equal to the difference 
between the edge orientations of B and B’ in order to overlap 
the two edge patterns, shown in Fig. 2(b). Obviously, the 
former is a special case of the second. The problem is what is 
the amount of translation for the second case? 
Given two edge blocks B and B’ characterized with the edge 
orientation and translation as ),( lθ  and ),( l′′θ , respectively, 
we would like to align the edge pattern of B with that of B’ by 
rotating and translating the edge pattern of B, shown in Fig. 3. 
The proposed edge alignment process consists of two steps. 
First, we translate the center of B’ to the center of B, which 
results in the first displacement for B’: 
 matching for uniform blocks. Before answering the problem, 
we first define the rule to decide the type of a block --an edge 
type or a uniform type. An image block B is defined to be an 
edge block if 
 
τ>− || 12 hh                     (8) 
 
where  are the block contrast of B and || 12 hh − τ  is a 
predefined threshold. Obviously, many image blocks will be 
classified as edge blocks if we use a small value of τ . In this 
work, we set the value of τ  to be a half of the average of the 
block contrasts from current frame in order to avoid resulting 
too many uniform blocks. However, we still have a lot of 
uniform blocks whose motion vectors are not obtainable 
through employing the process of edge alignment. As a matter 
of fact, for a small block size, the correlation between 
neighboring motion vectors is high. For each uniform block U, 
its motion vector  is simply predicted by the motion 
vectors of the edge blocks within the same macroblock as 
follows 
Uu
r
 
∑∑
∈∈
==
EB
B
EB
BBU wuwu 1,
rr       (9) 
 
where E is the set of edge blocks of the macroblock containing 
U, Bu
r  is the motion vector of the edge block B, and  is 
the weighting of B. The edge block which is far away from the 
uniform block U is supposed to give little impact on the 
motion vector of U. Thus, the value of  is computed by 
Bw
Bw
 
max),(1, dUBdaaaw BEB BBB −== ∑ ∈′ ′   (10) 
 
where d(B,U) is the Euclidean distance between B and U in 
terms of center coordinates and dmax is the maximal distance to 
U for all edge blocks in E. Similarly, we can interpolate the 
potential gradient orientation of the uniform block U by 
, which is further used to fine tune the motion 
vector. 
∑
∈
=
EB
BBU w θθ
The advantages of the proposed edge alignment for block 
matching are threefold: (1) it quickly determines the motion 
vectors with lesser SAD values for an image block in the 
current frame; (2) motion vectors of sub-pixel accuracy  are 
obtained; (2) edge information which is visually important to 
human perception is preserved during video compression. 
Without loss of generality, suppose we have an ideal object of 
a single color moving on a uniform background. Given a block 
B on the boundary of the object in the current frame, the edge 
pattern of B can then be modeled as a step edge which 
classifies the pixel values of B into two classes –one of them 
is represented by  and the other is represented by  
( ). Let 
Bh1
Bh2
BB hh 21 < Bˆ  be the corresponding block of B in the 
reference frame characterized with two representative pixel 
values  and h ( ), too. Then, we have  
and . As shown in Fig. 5(a), the value of SAD for B 
and 
Bh ˆ1
Bˆ
2
Bh ˆ1
Bh ˆ2< Bh ˆ1Bh1 ≈
BB hh ˆ22 ≈
Bˆ
1SAD
Bh2
|
|
222
3
22
2
hn
SAD
hn
SAD
B
B
 with their edge patterns aligned properly is 
 
 |    (11) ||| ˆ222
ˆ
111
BBBB hhnhhn −+−=
 
where n1 and n2 are the numbers of pixels of B represented by 
and , respectively. Figs. 5(b) and 5(c) show two 
non-edge alignment cases and their SAD values are 
Bh1
 
.|,
||||
,|,
||||
22212
ˆ
2
ˆ
1221
ˆ
111
12111
ˆ
2
ˆ
2112
ˆ
1111
nnnh
hhnhhn
nnnh
hhnhhn
B
BBBB
B
BBBB
+=−
+−+−=
+=−
+−+−=
  (12) 
 
 
B Bˆ
Bh ˆ2
Bh1
Bh2
Bh ˆ1
(a)
Bh1 Bh ˆ1
Bh2
Bh ˆ2
B
Bh1
Bh2
B
Bh ˆ1
ˆ
Bh ˆ2
(b)
Bh1
Bh2 Bh ˆ2
Bh ˆ1
Bh1
Bh ˆ2
B
Bh1
Bh2
B
Bh ˆ1
ˆ
Bh ˆ2
(c)
Bh1
Bh2 Bh ˆ2
Bh ˆ1
Bh2
Bh ˆ2
 
Fig. 5. Aligning two ideal step edges: (a) the edge patterns of B and Bˆ  
are aligned properly; (b) and (c) are two cases that do not align the edge 
patterns of B and Bˆ  properly. 
 
Comparing (11) and (12), it is easy to prove that the value 
of SAD1 is less than that of SAD2 or SAD3. Thus, the edge 
alignment process generates motion vectors with lesser SAD 
values.  
In H.264, motion vectors are required to achieve sub-pixel 
accuracy and this results in high computational complexity in 
general. The line equation for an image block obtained by the 
moment-preserving edge detector is itself with the sub-pixel 
accuracy property, and then the motion vectors obtained 
through the process of the proposed edge alignment process 
meet the special requirement of H.264 without demanding 
additional computations. 
 
3. The Proposed Fast Block Motion Estimation 
In combination of above analyses, given a block B, we 
propose a new searching process which predicts the motion 
vector of B through the process of edge alignment mentioned 
above. The edge orientation of B is used to adaptively select 
 The predictive hexagon search (PHS) patterns are 
configured according to the types of blocks, which are 
obtained from the orientations of blocks. The predictive 
hexagon search pattern is shown in Fig. 8. Fig. 8(a) shows a 
diamond search pattern (DSP) which contains five checking 
points (left, right, up, down dots with distance 1 around the 
center pot).  DSP is first applied to start our searching flow. 
Considering the searching along edge directions of image 
blocks, four search patterns –vertical edge search pattern 
(VESP), horizontal edge search pattern (HESP), diagonal edge 
search pattern (DESP), and anti-diagonal edge search pattern 
(ADESP) shown in Figs. 8(b)-8(e), respectively, are used as 
the subsequent steps. In this paper, the block size 44×  is 
adopted, which can be assumed to be small for high-resolution 
images. Instead of representing edges in any directions, the 
detected edges are mapping to 4 different directions. This 
assumes that the possible directions of an edge in a 44×  
block are limited to multiples of 45º, or equivalently, , 
i= 0, 1, …, 3. If the actual direction of an edge is not a 
multiple of 45º, it is quantized to be the nearest multiple of 45º. 
After the direction of an edge is given, we use the 
corresponding search pattern to search the final best match 
block in the reference frames. 
°× 45i
(a) (b) 
(d) (c)  
Fig. 9. Four cases for the first two steps of the block searching with (a) 
vertical edge type, (b) horizontal edge type, (c) diagonal edge type, and (d) 
anti-diagonal edge type. 
-9  -8  -7  -6  -5   -4  -3  -2   -1   0    1    2   3    4    5   6    7    8   9
-9
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
8
9
1
1
1
1
1 2
2
2
2
2
3 3
3
4
4
4
5
5
5
5
 
Fig. 10. Search path example for an image block with edge type A to find 
the motion vector (5,-2) in five steps. 
 
The purpose of the searching process is to find a search 
point with minimum rate-distortion cost (RD-Cost) which is 
defined as 
 
)()( uRateSADuJ rr ×+= λ      (13) 
 
where λ  is a regulation parameter. Given an edge block B 
with the edge orientation θ  in the current frame, the type of 
B is determined according to the following rule 
 
⎪⎪⎩
⎪⎪⎨
⎧
<≤
<≤
<≤
<≤<≤
=
oo
oo
oo
oooo
5.1575.112,
5.1125.67,
5.675.22,
1805.1575.220,
θ
θ
θ
θθ
ifA
ifH
ifD
orifV
Btype
  (14) 
 
where V, D, H, and A represent the vertical type, diagonal type, 
horizontal type, and anti-diagonal type, respectively. Fig. 9 
illustrates four cases to start the block searching along the 
edge directions for VBtype = , ,HBtype = DBtype = , and 
ABtype = . For each case, the block searching procedure starts 
from applying the DSP which calculates the first 5 search 
points. The search pattern for the successive steps to be 
applied depends on the block type – VESP, HESP, DESP, and 
ADESP for VBtype = , , , and HBtype = DBtype = ABtype = , 
respectively. Suppose that the minimum RD-Cost point is one 
of the corner points in the first step, the minimum RD-Cost 
point will be the center point in next searching step. It would 
calculate 4 new points for , shown in Figs. 9(a) 
and 9(b) and 5 new points for , shown in Figs. 
9(c) and 9(d). It is interesting to notice that no new points will 
be added to calculate the RD-Cost using the block searching 
procedure if the minimum RD-Cost point is located on the 
center point of the edge search patterns. In this case, we apply 
the DSP as the final step of the searching procedure. Fig. 10 
shows an example of the block searching procedure for 
demonstration. The performance of the searching procedure 
depends on the length of a line in the search window with its 
orientation similar to that of an input block. 
},{ HV
},{ ADtype ∈
Btype
B
∈
The algorithm of the block searching procedure is 
summarized as follows. 
 
Algorithm. Proposed Block Searching Procedure. 
Input: a 4x4 block B with center coordinates Bc
r  and the 
predictive motion vector Bu
r . 
Output: the final motion vector Bu
r  of B. 
Method: 
(1) The DS with BB uc
rr +  as the center point is used. 
(2) If the minimum RD-Cost point is located on the center 
point of DS, then return the center point as the final point 
of the motion vector. 
(3) Else do the following sub-steps: 
(3.1) Decide typeB (the type of B) using (14). 
 the block are used for calculation as shown in Fig. 11. The 
equations are listed in (16)-(19) and expressed as: 
 
⎪⎪⎩
⎪⎪⎨
⎧
=
=
∑∑
∑∑
= =
××
= =
××
3
2
3
0
44816
1
0
3
0
44816
8
1
8
1
i j
ijb
i j
iju
uu
uu
rr
rr
                
      (16) 
⎪⎪⎩
⎪⎪⎨
⎧
=
=
∑∑
∑∑
= =
××
= =
××
3
0
3
2
44168
3
0
1
0
44168
8
1
8
1
i j
ijr
i j
ijl
uu
uu
rr
rr
            
          (17) 
1,0,
4
1 12
2
12
2
4488 ≤≤= ∑∑+
=
+
=
×× jiuu
i
is
j
jt
stij
rr                (18) 
⎪⎪⎩
⎪⎪⎨
⎧
≤≤≤≤=
≤≤≤≤=
∑
∑
+
=
××
+
=
××
1
4484
1
4448
30,10,
2
1
10,30,
2
1
i
is
itij
j
jt
itij
jiuu
jiuu
rr
rr
              (19) 
 
In addition, we calculate the orientation for each block of 
larger size by averaging the orientation values of the 44×  
blocks covered by the block. Combining the predictive motion 
vectors listed in (15)-(19) and the proposed block search 
procedure, we obtain one motion vector for a 1616×  block, 
two motion vectors for  blocks, two motion vectors for 
 blocks, four motion vectors for  blocks, eight 
motion vectors for 8  blocks, eight motion vectors for 
 blocks, and 16 motion vectors for  blocks. The 
SAD values for all blocks of variable block sizes are also 
obtained. 
816×
4×
168×
84×
88×
4× 4
4. Simulation Result 
In order to evaluate the proposed approach, a series of 
experiments was conducted on a 1.8 GHz PC with 960 MB 
main memory. For motion estimation, the search window is 
from -16 to 16, the number of reference frame is 5, and the 
number of block types is 7. The methods simulated for 
performance comparison with Full Search (FS) using seven 
test video sequences –‘Container,’ ‘Foreman,’ ‘News,’ ‘Silent,’ 
‘Paris,’ ’Mobile,’ and ‘Tempete’ include UMHEX [14], Xu 
and He’s Method [15], Motion Classification-based Search 
(MCS) [16], and the proposed method. All the methods are 
implemented into the H.264/AVC reference software 
JM11.0[7]. In our environment the UMHEX is adopted with 
the early termination here. All the test sequences are in QCIF 
format. 
 Table 1 illustrates the number of search points with 
different methods and different video sequences. For 
block-matching, the number of search points decides the 
computational complexity of motion estimation. Based on the 
simulation results of Table 1, except MCS, the number of 
search points by the proposed method is much smaller than the 
compared methods. The proposed edge alignment process is 
used to generate the predictive motion vector (PMV) quickly 
and the proposed predictive hexagon search (PHS) is used to 
further enhance the accuracy of the motion vector. We include 
the experimental results obtained from the proposed method 
with and without PHS to demonstrate the effectiveness of the 
edge alignment process. In our method, the information (h1, h2, 
and θ ) for every block in the previous frames is properly 
indexed, which is used to locate the most similar block for a 
block in the current frame. Thus, the predictive motion vector 
based on the edge alignment process does not introduce 
additional search points.  Fig. 12 shows the performance 
comparison in terms of search points using the 7 test video 
sequences. Table 2 shows the average PSNR per frame. In 
order to have a fair comparison, the PSNR values for the 
reconstructed frames using the compared methods are almost 
the same. Accordingly, the proposed method has better 
capability in filtering unnecessary search points. 
Table 3 shows the total execution time in terms of micro 
seconds (ms) to encode a video sequence with different 
methods and different video sequences. Fig. 13 shows the 
performance comparison in terms of speedup improvement 
which is defined as 
 
Speedup(A) = (1- f(A)/f (FSBMA))*100% 
 
where f(A) is the execution time to encode a video sequence 
using method A. In average, the proposed method has 84% 
speedup improvement as compared with FS. However, 
UMHEX, MCS and Xu and He’s method have 74%, 82% and 
81% speedup improvement, respectively. The proposed 
method is obviously faster than the compared methods.  
The effectiveness of edge alignment affects the performance 
of the proposed video coding scheme. The quality of the 
reconstruction sequences (up to PSNR 40 db) for all compared 
methods is good. Fig. 14 shows an example of reconstructed 
frame 100 for ‘Forman’ using FSBMA, UMHEX, the 
proposed method, and the proposed method without PHS. 
According to the experimental results, the proposed method 
achieves good performance when the input data contain 
obvious edges. In this case, the edge information extracted 
from the proposed moment-preserving edge detector is 
relatively accurate and produce a better predictive motion 
vector which reduces the complexity of further PHS operators. 
For example,   the proposed method outperforms the 
compared methods using the test sequence ‘Mobile’ which 
contains many human-made objects. On the other hand, the 
performance of the proposed method is slightly degraded for 
ill-defined edges which can be founded in low-resolution 
video frames. The accuracy of edge detection affects the 
effectiveness of edge alignment. An improper edge alignment 
process leads to quality degradation in the proposed video 
coding scheme. Fortunately, this problem is not serious to our 
coding scheme since the color information in a block is also 
used to make sure the effectiveness of edge alignment. 
 Proposed 29318729 34797289 30636668 31907496 154862912 155006181 164159478
 
Table 2. Average PSNR per frame with different methods and different video sequences.  
The search range is from -16 to 16, the number of reference frame is 5, and the number of block types is 7. 
 Container Foreman News Silent Paris Mobile Tempete 
Full Search 36.03 35.54 36.48 35.71 34.92 33.19 33.68 
UMHEX[14] 36.01 35.41 36.46 35.70 34.91 33.11 33.61 
Xu & He‘s method[15] 35.99 35.41 36.45 35.71 34.90 33.10 33.60 
MCS[16] 35.93 35.51 36.37 35.73 34.88 33.15 33.63 
Proposed (without PHS) 34.90 34.57 35.31 34.96 34.41 33.16 33.53 
Proposed 35.89 35.28 36.31 35.65 34.73 33.18 33.67 
Table 3. Total execution time (ms) per video sequence with different methods and different video sequences.  
The search range is from -16 to 16, the number of reference frame is 5, and the number of block types is 7. 
 Container Foreman News Silent Paris Mobile Tempete 
Full Search 1842132 1994310 1549468 2454158 8795171 9715372 9283706 
UMHEX[14] 399481 368286 380669 352663 2793489 3267793 3151528 
Xu & He’s method[15] 314032 284943 270503 262064 2035057 2616522 2209211 
MCS[16] 282034 262588 304535 265435 2128639 2320133 2146191 
Proposed (without PHS) 197736 248529 210508 231333 1892125 2120844 2106194 
Proposed  263367 259730 239658 264434 2064770 2311310 2304615 
 
 
Fig. 12. Performance comparison in terms of average search points 
per macro block using 10 test video sequences. 
 
 
Fig. 13. Performance comparison in terms of speedup improvement 
using 10 test video sequenc
 
    
 
                     (a)                                     (b)                                     (c)          
(d)                                     (e) 
 
Fig. 14. Reconstructed frame 100 of ‘Foreman’: (a) original image; (b) FSBMA; (c) UMHEX; (d) proposed method (e)proposed method without PHS
 可供推廣之研發成果資料表 
■ 可申請專利  □ 可技術移轉                                      日期： 年 月 日 
國科會補助計畫 
計畫名稱：結合電腦視覺技術之快速H.264/AVC為基礎的可調適視
訊編碼研究 
計畫主持人：鄭錫齊 
計畫編號：NSC 97-2221-E-019 -032 -學門領域：資訊二 
技術/創作名稱 基於線邊對齊之快速區塊運動補償方法 
發明人/創作人 鄭錫齊 
中文： 
快速地進行區塊運動補償以改進H.264/AVC及SVC執行速度過慢
的問題不是一件容易的事，許多技術和理論都還有研究及改進的空
間，相關的技術也可應用於其他視訊分析的問題上，例如手持裝置
在擷取視訊畫面時，畫面中的物件可能在手不經意的振動，造成畫
面不穩定，有效的運動偵測及補償方法，可用於消除畫面不穩定的
問題。本計畫討論 H.264/AVC 的主要關鍵技術及其編碼效率，並提
出一適合平行化處理、以軟體為主的快速 H.264/AVC 編碼架構，據
以改善兼顧速率-失真的 H.264/AVC 的編碼效率，軟體式平行處理
架構具有彈性的處理方式，更適合用以架設即時的 H.264/AVC 編碼
器。 
技術說明 
英文： 
This project presents a novel block matching scheme with edge 
alignment strategy on H.264 video coding, which uses multiple 
references and multiple block sizes for motion estimation in order to 
improve the rate-distortion performance. In H.264, the computational 
complexity is linearly dependent on the number of allowed reference 
frames and block sizes using the full exhaustive search. Many fast 
block-matching algorithms reduce the computational complexity of 
motion estimation by carefully designing the search patterns with 
different shapes or sizes which have significant impact on the search 
speed and distortion performance. However, the search speed and the 
distortion performance conflict often with each other for these 
methods. In this paper, given a block in the current frame, we first 
apply a fast approximate method based on edge alignment to obtain a 
good initial motion vector as well as a tight initial bound of distortion 
measure. Then, considering the edge orientation of the block, a 
modified hexagon search is used to fine tune the motion vector in low 
computational complexity. The proposed algorithm also pays attentions 
to the characteristics of multiple reference frames and multiple block 
sizes in H.264. Computer simulation results show that the proposed 
method gives good performance and spans a new way to design a 
cost-effective real-time video coding system. 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
         98 年  2  月  11  日 
報告人姓名 鄭錫齊 服務機構 
及職稱 
國立台灣海洋大學教授 
會議期間及地點 2009/2/8~2/11 
泰國曼谷 
 
本會核定 
補助文號 
NSC 97－2221－E－019－
047－ 
會議名稱 (中文)2008 年 IEEE 國際智慧型訊號處理暨通訊系統研討會 
(英文)IEEE International Symposium on Intelligent Signal 
Processing and Communication Systems 2008 (ISPACS 2008) 
 
發表論文題目 (中文)基於線邊對齊之預測搜尋樣本選擇方法及其在快速運動
偵測之應用 
(英文) Predictive Search Pattern Selection Based on Edge 
Alignment for Fast Motion Estimation 
(中文)應用於區域為主物件檢索之具內容察覺影像切割 
(英文) Content Aware Image Segmentation for Region-Based 
Object Retrieval 
 
報告內容： 
一、參加會議經過 
2008年IEEE國際智慧型訊號處理暨通訊系統研討會原訂於97年12月在泰國曼谷舉
行，但因會議舉辦期間，泰國政經情勢不穩，國際機場遭示威群眾包圍因導致機場關避，
大會舉辦單位泰國King Mongkut’s Institute of Technology Ladkrabang (KMITL)決定延後
至2/8/2009-2/11/2009。 
IEEE國際智慧型訊號處理暨通訊系統研討會已經舉行多年，前幾屆分別在全世界各地
舉辦，舉凡美國、澳大利亞、日本、台灣、泰國、香港等亞太重要國家均曾舉辦過，本
屆為第16屆由泰國KMITL承辦，會議大會收到來自超過25個不同國家超過200篇投稿論
文，其中只有150投稿論文被接受參與勝會，討論最新的技術發展趨勢，來自全世界各
知名大學超過200個學者，齊聚一堂共同討論影像處理的最新技術。 
今年該研討會著重在電腦影像、視訊訊號處理及通訊系統相關領域之先進研究討
論，議程概分為 6 個平行討論領域，分別為通訊系統(communications), 訊號處理
(signal processing), 影像處理及電腦視覺(image processing/computer vision), 
電腦影像/視訊的編碼技術及浮水印(image & video coding/ reconstruction/ 
參加了幾場感興趣的議程，並發表第二篇論文: Content Aware Image Segmentation for 
Region-Based Object Retrieval，一開始個人先做約 15 分鐘的內容簡介，隨後與與會的人
士開始展開提問及回答，席中也有其他的作者給予他們的想法，收獲良多。 
第四天(2 月 11 日)一早起床利用空檔時間撰寫出國報告，隨後赴泰國曼谷國際機場
搭機回國，結束為期四天的行程。 
 
三、考察參觀活動(無是項活動者省略) 
無 
四、建    議 
這個研討會集合的世界各國的研究學者聚集在一起，討論主題是智慧型訊號處理暨通
訊系統的最新技術發展，今年是第十六屆在泰國曼谷舉行，投稿的學者來自全球25 個
國家。影像及多媒體處理技術對於發展新一代的多媒體服務、數位內容開發、影像顯示
技術、及影像/視訊通訊系統扮演關鍵的角色，然而相關的研究領域雖然已發展多年，
但是世界各國仍積極地發展最新技術，台灣自然不能置身度外，積極參與對提升台灣競
爭力有極大幫助。國際研討會對擴展研究視野、避免閉門造車有相當大的助益，研究學
者應多多參與相關的國際研討會。 
首先感謝國科會補助教師出國參加研討會的機票及相關費用，然而補助經費確實不
足，因此只能選擇參加較近的國家舉辦的研討會，如果國科會補助的額度能適度提高，
讓老師或學生多一點參與專業議題的機會。相信亦可拓展個人研究的深度及國際觀。 
五、攜回資料名稱及內容 
研討會行程表及研討會論文集之光碟片(CD-ROM) 
六、其    他 
二、與會心得 
 
本次參與 CISIS2009 國際會議之學術交流獲益良多，已認識多位國際知名學者，並於會
場中邀請到台灣訪問座談，初步已獲多位首肯，例如，美國休士頓大學 Shou-Hsua Huang
教授今年六月中旬將在參加台灣所舉辦的國際研討會，黃教授已允諾將撥空到海洋大學
演講，並與本校相關研究領域的學者及研究生進行更深入的探討與指導，本人認為一定
可以為本校帶來更多的重要研究課題及後續國際學生互相交流的機會。在此會議中，有
多位知名國際學者蒞臨現場，並發表他們豐碩的研究成果及專業的研究理論，實在令人
敬佩，這些重要的研究成果皆可以作為未來研究的重要參考。在參與此會議中，除了可
以學習智慧型系統及生物科技研究主題最新的技術與概念外，另一重要目的是為了透國
際會議推廣本校及本實驗室的研發成果，提昇本國及本校在此研究領域的知名度。在聆
聽學習國外各研究學者最新研究成果的同時，可以從邀請演講中發掘新的研究主題及學
習到更完整的科技技術，五場的邀請演講主題及內容分別是： 
(一) 
Keynote of Prof. Eiji Okamoto (Tsukuba University) 
Pairing based cryptography - theory, implementations and applications Pairing 
based cryptography is a new and important research area in security. It has a 
significant property, bilinearity, and using this, a lot of new protocols are 
proposed recently. In this talk, pairing function is introduced, and then its 
fast computation algorithm and implementation on hardware and software are shown. 
Finally its applications to cryptography are introduced. 
(二) 
Keynote of Prof. Sushil Jajodia (George Mason University) 
Topological Analysis of Network Attack Vulnerability This talk will discuss 
issues and methods for survivability of systems under malicious attacks. To 
protect from such attacks, it is necessary to take steps to prevent attacks from 
succeeding. At the same time, it is important to recognize that not all attacks 
can be averted at the outset; attacks that are successful to some degree must 
be recognized as unavoidable and comprehensive support for identifying and 
responding to attacks is required. In my talk, I will describe the recent 
research on attack graphs that represent known attack sequences attackers can 
use to penetrate computer networks. I will show how attack graphs can be used 
to compute actual sets of hardening measures that guarantee the safety of given 
critical resources. Attack graphs can also be used to correlate received alerts, 
hypothesize missing alerts, and predict future alerts, all at the same time. 
Thus, they offer a promising solution for administrators to monitor and predict 
the progress of an intrusion, and take appropriate countermeasures in a timely 
manner. 
應鼓勵國內學者多參與國際研討會，多與國外先進討論與學習，除了可以將各研究
室所研發成果推廣外，更可以提昇計劃合作之可能性，或爭取主辦研討會活動之機會。
除此之外，政府應盡力補助國內所舉辦之國際學術活動，提供旅遊補助經費給國外學生
或需要協助之與會者，讓推廣提昇國內研究成果及知名度之目標得以落實。 
 
五、攜回資料名稱及內容 
 
攜回此會議所發行的論文光碟片及未來各研討會的相關會議資訊。 
low-level features. Different approaches to group visual patterns
extracted from image blocks, regions, or objects have been
proposed to offer a semantic-based representation for image
understanding and analysis applications [1]. The method of
grouping visual patterns into image model based on GHT is one of
the most powerful techniques for image analysis [11,12]. However,
real-time applications of this method have been prohibited due to
the computational intensity in similarity searching from a large
centralized image collection. In this work, we describe a fast CBIR
implementation using region-based GHT. GHT is a powerful tool
to recover object affine transformations for visual object retrieval.
In addition, content-aware image segmentation is proposed to
synchronize segmented regions inside query and target images. The
image retrieval process is separated into two phases. First, a
training procedure used to find and store the stable parameters for
segmenting each database image is performed. The database
images are then sorted in the ascending order in terms of
segmentation parameters. In the retrieval phase, an incremental
image segmentation process based on the stored parameters is
performed to divide a query image into multiple regions for
retrieving visual objects inside database images using GHT. A
voting scheme based on GHT is also proposed to provide object
search method, which is invariant to the translation, rotation,
scaling of image data, and hence, invariant to orientation and
position. Computer simulation results show that the proposed
method gives good performance in terms of retrieval accuracy,
robustness, and execution speed.
2. OBJECT SEARCH USING REGION-BASED GHT
(a) (b) (c)
Fig. 1. Utilization of region matching to estimate the geometrical
transform parameters of an arbitrary object; (a) the visual object; (b)
the target object; (c) the region-based model for generalized Hough
transform.
The problem of object-based image retrieval can be modeled by
GHT on the basis of region information [11,12]. It can cope with
objects, moving with translation, rotation, and scaling. GHT was
initially proposed to represent an object with arbitrary shape using
a so-called R-table. The shape of the object can be described by the
geometric relationship between the object reference point XR and
the edge points Xs of the object boundary. To locate the pattern in
an image, each of the edge points is considered and the
corresponding locations of the reference point are calculated.
Assume that the shape, scale s, and rotation of the desired region
are known. A reference point,  RR yxO , , is chosen at any location
inside the sample region. Then an arbitrary line intercepting the
boundary template at the point M(x,y) can be constructed at this
reference point aiming in the direction of the region border. The
edge direction , the distances of the reference point to region
border, r, and the anglefor the segment OM are then determined
as a geometrical template. Next, a reference table, R-table, is
constructed using as a key and (r,) as data. In the matching
phase, the 4D parameter space yxs  is investigated. When
the figure with the scale s and rotationis the object for extracting,
the edge direction i, which is searched in keys of R-table, is
determined for an edge point  ee yx , in the target image. By using
the values of (ri,) that are indexed by i, the candidate
coordinates  RR yx , of the reference point, are determined by the
following expression:
   ))(sin()(),)(cos()(,   srysrxyx eeRR . (1)
Then a voting process for increasing the value of the coordinates
(s,,x,y) of the 4D parameter space is executed. Calculating
candidate coordinates of the reference point and voting process to
coordinates are executed for all possible values of s and.
After all the edge points of the target image have been processed,
the 4D accumulator array will contain high peak value for
locations where many edge points coincide with many query object
edges. High peaks correspond to reference point locations where
instances of the query object occur in the target image.
As shown in Fig. 1, a visual object (Fig. 1(a)) consisting of a
number of regions might have geometrical transformation with
respect to a target image (Fig. 1(b)). With the perspective
projection and a rigid plane assumption of object surface, there
exists a sub-set of regions that share the same geometric
transformation mapping the visual object into the target image. The
GHT can then be used to detect rotation, translation and scaling of
objects that are represented by the set of regions from target images.
The information stored in the R-table of the object model is
redefined as follows:
),,(,),,,(),,,(
.
.
.
),,(,),,,(),,,(
),,(,),,,(),,,(
),,(,),,,(),,,(
),,(,),,,(),,,(
222111
333
2
3
2
3
2
3
1
3
1
3
1
33
222
2
2
2
2
2
2
1
2
1
2
1
22
111
2
1
2
1
2
1
1
1
1
1
1
11
00
2
0
2
0
2
0
1
0
1
0
1
00
333
222
111
00
kkk n
k
n
k
n
kkkkkkkk
nnn
nnn
nnn
nnn
u
RrRrRr
ererer
RrRrRr
RrRrRr
RrRrRr










(2)
where j
iR are the j-th uniform block and the j-th region with
orientation
i , respectively. The orientation of a region A is
defined as the angle between the major axis of A and the x-axis and
can be obtained from the central moments of A as follows:
2,00,2
1,11 2tan
2
1



 A (3)
where
ts , is the (s+t)th central moment of A. Using the region-
based model, Eq. (1) can also be used to determine the centroid of
the object in study from the target image, except for replacing the
coordinates  ee yx , of edge points with the center coordinates
 AAA yxX , of the region A (cf. Fig. 1(c)).
In the existing Hough transforms, a threshold value in the
yxs  space is required to reduce the number of spurious
peaks. The pixel values of an region with an object are used to
compute the support from the region for a parameter vector
(s,x,y). When the support value of the region for a parameter
vector is too small, the region is not considered to have a vote on
database is retrieved by the system as the topmost retrieval. Also, a
robust system should be stable for all types of queries, i.e., the
system must not break down under specific cases. In order to test
the robustness of the proposed system, not only normal query
images, but also translated, rotated, scaled, and noise added query
images were used to test the system. Fig. 3 shows an example of
retrieval results of our system.
Fig. 3. An example of retrieval results of the system.
Table 1. Image retrieval results for the simulated methods: n refers
to the position of the correct retrieval; the last column indicates the
average time taken for a retrieval.
Test
Mode Method
n=1
(%)
n3
(%)
n5
(%)
n20
(%)
Average
Retrieval
Time
(Seconds)
Normal Chau 91 91 92 92 5.3
Proposed 100 100 100 100 0.115
Scaling Chau 6 7 8 11 5.3
Proposed 100 100 100 100 0.115
Cropping Chau 91 91 91 92 5.3
Proposed 98 100 100 100 0.115
Rotation Chau 23 25 25 26 5.3
Proposed 95 97 97 98 0.115
Translate Chau 91 91 92 92 5.3
Proposed 100 100 100 100 0.115
For retrieval on the basis of visual patterns, the experiments
were conducted on the 100 color images randomly selected from
our database as the query images. The region-based retrieval
technique proposed by Chau et al. [12] was also implemented for
performance comparison. Table 1 presents the retrieval results of
the proposed method and Chau’s method, where n refers to the
position of the correct retrieval. The retrieval performance was
better in the presence of geometry transformation for the proposed
method. However, the retrieval performance of the proposed
system seems more sensitive to rotated images than to scaled or
translated images. The worst-case retrieval accuracy of the system
is up to 99%, which is much better than that of Chau’s method. 
The average retrieval time to answer a query using the proposed
method is much shorter than that for Chau’s method.
5. CONCLUSION
In this paper we have present an object search method using
GHT based on content aware image segmentation. Incorporating
the proposed segmentation parameter learning, the content of
database images is used to guide the query image segmentation and
improve the retrieval effectiveness of region-based GHT. In other
words, the proposed method does not suffer from the problems of
object segmentation used in conventional object search approaches.
The fusion of image segmentation and matching function stabilizes
the segmented regions for region-based image retrieval.
Furthermore, the method using the region-based GHT to find the
correct geometry transformation parameters in object searches
without the main disadvantage of high-computational complexity
in traditional GHT.
The retrieval speed of the system is enhanced without affecting
the robustness of the system by sorting database images in terms of
segmentation parameter in advance and the proposed incremental
query segmentation scheme. Future work will deal with linking
semantic interpretations into regions and increasing the database
size.
REFERENCES
[1] Y. Liu, D. Zhang, G Lu, and W.-Y. Ma, “A Survey of
Content-Based Image Retrieval with High-Level Semantics,”
Pattern Recognition, Vol. 40, No. 1, pp. 262-282, 2007.
[2] J. Z. Wang, J. Li, and G. Wiederhold, “SIMPLIcity:
Semantic –Sensitive Integrated Matching for Picture
Libraries,”IEEE Trans. on Pattern Analysis and Machine
Intelligence, Vol. 23 No. 9, pp. 947-963, 2001.
[3] I. Pratikakis, I. Vanhamel, H. Sahli, B. Gatos, and S. J.
Perantonis, “Unsupervised Watershed-Driven Region-Based
Image Retrieval,” IEE Proc. Vision, Images, and Signal
Processing, Vol. 153 No. 3, pp. 313-322, 2006.
[4] C. Carson, S. Belongie, H. Greenspan, and J. Malik,
“Blobword: Image Segmentation Using E-M and Its
Application to Image Querying,”IEEE Trans. Pattern Anal.
Mach. Intell., Vol. 24, pp. 1026-1038, 2002.
[5] J.-W. Hsieh and W. Eric L. Grimson, “Spatial Template
Extraction for Image Retrieval by Region Matching,”IEEE
Trans. Image Processing, Vol. 12 No. 11, pp. 1404-1415,
2003.
[6] J. Fan, Y. Gao, H. Luo, and G. Xu, “Statistical Modeling and
Conceptualization of Natural Images,”Pattern Recognition,
Vol. 38, pp. 865-885, 2005,.
[7] J. Luo and C.-E. Guo, “Perceptual Grouping of Segmented
Regions in Color Images,”Pattern Recognition, Vol. 36, pp.
2781-2792, 2003.
[8] S.-C. Zhu, “Statistical Modeling and Conceptualization of
Visual Patterns,” IEEE Trans. On Pattern Analysis and
Machine Intelligence, Vol. 25 No. 6, pp. 691-712, 2003.
[9] W. Jiang, G. Er, Q. Dai, and J. Gu, “Similarity-Based Online
Feature Selection in Content-Based Image Retrieval,”IEEE
Trans. Image Processing, Vol. 15 No. 3, pp. 702-712, 2006.
[10] F. Jing, M. Li, H.-J. Zhang, and B. Zhang, “Relevance
Feedback in Region-Based Image Retrieval,” IEEE Trans.
Circuits and Systems for Video Technology, Vol. 14 No. 5, pp.
672-681, 2004.
[11] S.-C. Cheng, C.-T. Kuo, and H.-J. Chen, “Visual Object
Retrieval via Block-Based Visual Pattern Matching,” Pattern
Recognition, Vol. 40 No. 6, pp. 1695-1710, 2007.
[12] C. P Chau and W. C. Siu, “Generalized Hough Transform
Using Regions with Homogeneous Color,”Int. J. Computer
Vision, Vol. 59 No. 2, pp. 183–199, 2004.
speed and the distortion performance conflict often with each other
for these methods. The proposed method offers a fast solution for
block motion estimation on H.264 video coding systems. For each
basic 4x4 block of a macro block, we first apply a fast approximate
method based on edge alignment to obtain a good initial motion
vector as well as a tight initial bound of distortion measure. Then,
considering the edge orientation of the block, a modified hexagon
search [4] is used to fine tune the motion vector in low
computational complexity. Finally, we design a fast intermode
decision method to achieve better rate-distortion performance for
H.264. Computer simulation results show that the proposed
method gives good performance and spans a new way to design a
cost-effective real-time video coding system.
2. BLOCK MATCHING USING EDGE ALIGNMENT
Block matching for motion estimation is an approach to shift or
warp the image blocks relative to each other and to look at how
much the pixels agree according to certain criteria. On such
function, often used in video coding because of its speed, is the
sum of absolute differences (SAD) metric, i.e.,
|),(),(|),( 1
1
1
1
1
, jvyiuxIjyixIvuSAD t
N
i
N
j
tyx  




 (1)
where ),( vuu  is the displacement and
|),(),(| 1 jvyiuxIjyixI tt   is called the displaced
frame difference. Given a block in the current frame, the full
searching technique tries all possible alignments and is too slow
in practice. Besides block matching techniques with full or partial
pixel information, feature-based alignment methods are possible
to speed up the process of motion estimation.
x
y
-1 1
-1
1
0
C
B
+ yx ,
l
h1
h2
Fig. 1. An edge model in a 4 x 4 block B. The circle C is
inscribed in B, and  yx, are the coordinates of the centers of
gravity of the gray (color) values inside C.
B
BˆB
Reference Frame
Current Frame
4x4 coded blocks
Fig. 2. The boundary of the matches sBˆ for a given edge block
B in the current frame might not be coincided with that of coded
blocks in the reference frames.
At the lowest level of computer vision, potentially useful
visual patterns such as edges and line segments can be extracted
from an image without any priori knowledge of the image content.
A given image is partitioned into a set of non-overlapping square
blocks. Each edge block B, shown in Fig. 1, is represented as a
vector ),,,( 21 lhh where h1 and h2 are the two representative gray
(color) values, l is the edge translation, and is the orientation
angle of the edge in B. The solution to the edge detection
problem in a given block is analytic and this means that the edge
detection process can be performed very fast with no need for
special hardware [5].
Given a reference frame, the edge blocks play a role as the
references to search the possible matches for each edge block of
the current frame. For each edge block in the current frame, we
scan the edge blocks within the search window from the
reference image one-by-one in the fashion of left-to-right and up-
to-down to find its possible best match block. Let B be an edge
block in the current frame. Given an edge block B’corresponding
to one of the search point of B in the reference frame, we could
locate the possible match block Bˆ in the reference frame for B
when aligning the edge patterns of B and B’. The SAD value
between B and Bˆ are then computed to judge the match degree.
Assuming the size of searching window is 3232 , the number of
search points to find the best match block is 88 for a 44 edge
block using the proposed edge alignment technique. This
dramatically reduces the number of check points using the full
searching technique when doing block motion estimation.
In practice, it is not necessary to align the edge patterns for a
pair of edge blocks with absolutely different orientations because
these two blocks are impossible to be best match each other. The
process of edge alignment can be further simplified if we assume
the orientations for a match pair of blocks are very similar. Let the
edge block B= ),,,( 21 lhh  centered at ),( BB yx in the current
frame, and the edge block Bˆ = )ˆ,ˆ,ˆ,ˆ( 21 lhh  centered at ),( ˆˆ BB yx be
the corresponding block of B in the reference frame. Suppose that
B’ is the scanned block in the reference frame currently,
),,,( 21  lhhB is the starting point to find the best
match )ˆ,ˆ,ˆ,ˆ(ˆ 21 lhhB nearing B
’, shown in Fig. 2. Obviously, the
orientation of Bˆ equals to that of B’, that is .ˆ   Furthermore,
we should have ll ˆ for Bˆ to be one of the possible matches of B.
The actual problem is: where should the center ),( ˆˆ BB yx of Bˆ be?
Obviously, the location of Bˆ can be obtained by solving the
following linear system
)(tan
sincos
ˆˆ
ˆˆ
BBBB
BBBB
xxyy
l
N
yy
N
xx
l









 . (2)
That is















sin)(
cos)(
ˆ
ˆ
llNy
llNx
y
x
B
B
B
B . (3)
Then, the displacement vector for B is easily obtained from
),( ˆˆ BBBB yyxxu 
 (4)
if Bˆ is a candidate of the best march of B.
The methods simulated for performance comparison with the
proposed method include FSBMA, UMHEX [6], and enhanced
predictive zone search (EPZS) [7]. All the methods are
implemented into the H.264/AVC reference software JM.11 [8].
Fig. 5. Performance comparison in terms of average search points
per macro block using 10 test video sequences.
Fig. 6. Performance comparison in terms of speedup improvement
using 10 test video sequences.
(a) (b) (c)
(d) (e)
Fig. 7. Reconstructed Frame 3 of‘Foreman’: (a) original image; (b)
proposed method; (c) FSBMA; (d) UMHEX; (e) EPZS.
In this section, we experimentally compare our method with
UMHEX and EPZS using ten test video sequences –‘Forman’, 
‘Salesman,’ ‘Mother & Daughter,’ ‘Carphone,’‘Grandma,’’Miss 
America,’‘Clair,’’Suzie,’’Coastguard,’ and ‘Trevor.’All the test
sequences are in qcif format. Fig. 5 shows the performance
comparison in terms of average search points per macro block
using the 10 test video sequences. In order to have a fair
comparison, the PSNR values for the reconstructed frames using
the compared methods are almost the same. Accordingly, the
proposed method has better capability in filtering unnecessary
search points. Fig. 6 shows the performance comparison in terms
of speedup improvement which is defined as
Speedup(A) = (1- f(A)/f (FSBMA))*100%
where f(A) is the execution time to encode a video sequence using
method A. In average, the proposed method has 86% speedup
improvement as compared with FSBMA. However, UMHEX and
EPZS have 66% and 69% speedup improvement, respectively. The
proposed method is obviously faster than the compared methods.
The quality of the reconstruction sequences (up to PSNR 40 db)
for all compared methods is good. Fig. 7 shows an example of
reconstructed frame 3 for ‘Forman’using the proposed, FSBMA,
UMHEX, and EPZS methods. Although the proposed method has
0.5db PSNR degradation in the quality of reconstructed images, it
does not produce noticeable artifacts in the reconstructed images.
Hence, the proposed method meets the requirement of encoding
high quality videos quickly.
5. CONCLUSIONS
In this paper we have presented a fast block matching for video
coding based on edge alignment. A video encoding strategy based
on the low-level computer vision also makes the coded results
following the human perception without the main disadvantage of
high-computational complexity for traditional block motion
estimation. Furthermore, the proposed method is extended to apply
to H.264/AVC. The proposed method is not only fast but also
encoding video sequences in very high quality.
REFERENCES
[1] T. Koga, K. Iinuma, A. Hirano, Y. Iijima, and T. Ishiguro,
“Motion Compensated Interframe Coding for Video
Conferencing,”in Proc. Nat. Telecommunications Conf., 1981, pp.
C9.6.1-C9.6.5.
[2] S. Zhu and K. K. Ma, “A New Diamond Search Algorithm for
Fast Block-Matching Motion Estimation,”IEEE Trans. Image
Process., vol. 9, no. 2, pp. 287-290, Feb. 2000.
[3] T.-H. Tsai and Y. N. Pan, “A Novel 3-D Hexagon Search
Algorithm for Fast Block Motion Estimation on H.264 Video
Coding,”IEEE Trans. Circuits Syst. Video Technol., Vol. 16, No.
12, pp. 1542-1549, Dec. 2006.
[4] C. Zhu, X. Lin, and L. P. Chau, “Hexagon-Based Search
Pattern for Fast Block Motion Estimation,”IEEE Trans. Circuits
Syst. Video Technol., vol. 12, no. 5, pp. 349-355, May 2002.
[5] S.-C. Cheng, “Content-based Image Retrieval Using Moment-
preserving Edge Detection,”Image and Vision Computing, vol. 21
no 9, pp. 809-826, Sep. 2003.
[6]Z. Chen, P. Zhou, and Y. He, “Fast integer pel and fractional 
pel motionestimation for AVC,” presented at the 6th JVT-Fo17
Meeting, Awaji Island, Japan, Dec. 2002.
[7]Alexis M. Tourapis, “Enhanced Predictive Zonal Search for
Single and Multiple Frame Motion Estimation,”Proc. SPIE Vol.
4671, p. 1069-1079, Visual Communications and Image
Processing 2002, C.-C. Jay Kuo; Ed.
[8] http://iphome.hhi.de/suehring/tml/.
approach by performing the segmentation and classification 
simultaneously [3]. 
The core of the ant-colony optimization (ACO) approach 
for array CGH data analysis (ACO-CGH) is a heuristic 
algorithm. ACO is an optimization technique inspired from 
the foraging behavior of real ant colonies [13]. This 
biological inspiration is developed into artificial ant 
colonies for the search of approximation solutions to 
discrete optimization problems. The core of the ACO-CGH 
algorithm is the iteration phase, where the ants generate a 
set P of paths corresponding to feasible solutions to the 
segmentation problem of array CGH data. Then, starting 
from the paths in P, a local search is performed until a 
locally optimal path is found and compared with the 
current-best path. If stopping conditions are met the process 
ends, otherwise the iteration phase is repeated. The 
properties of the proposed algorithm are thoroughly 
analyzed, and the approximation results are encouraging as 
compared to those of the works using a genetic algorithm [7] 
and HMM model [3]. 
The remainder of this paper is organized as follows. 
Section ҈ reviews the basic principles of the ACO 
algorithm. Section ҉ renders the details of the proposed 
method. In Section Ҋ, we present the experimental results 
and the discussions. Finally, a summary is given in Section 
ҋ.
II. ACO ALGORITHM
Fig. 1. The pseudo code of the ACO. 
The ACO algorithm [14] uses a meta-heuristic approach for 
solving hard combinatorial optimization problems. The idea 
for the ACO algorithm came from the ant’s foraging 
behavior which is the indirect communication among ants 
by means of chemical pheromone trails. A pheromone trail 
enables ants to gather food from the source to their nest 
along the shortest path. The artificial ants used in the ACO 
are stochastic constructive procedures that build solutions 
with the help of probability. Once the feasible solutions are 
obtained, the heuristic information of the problem and the 
pheromone trails are dynamically changed. For the sake of 
referencing, the pseudo code of the ACO algorithms is 
shown in Fig. 1. 
The initialization procedure of the ACO includes two 
parts: the problem graph representation and the initial ant 
distribution. First, the underlying problem should be 
represented in terms of a graph, G = <N,E>, where N
denotes the set of nodes, and E the set of edges. The graph 
is connected, but not necessarily complete, such that the 
feasible solutions to the original problem correspond to 
paths on the graph which satisfy problem-domain 
constraints. Second, a number of ants are arbitrarily placed 
on the nodes chosen randomly. Then each of the distributed 
ants will perform the AntBasedSolutionConstruction 
procedure on the graph by constructing a path according to 
the node transition rule described later. 
The PhermoneUpdate procedure in Fig. 1. is used to 
update the intensity of pheromone trail after the end of a 
cycle, which is defined when every ant has constructed a 
solution. At the end of each cycle, the intensity of 
pheromone trails on each edge is updated by the pheromone 
updating rule. The optional procedure Daemon_Actions can 
be used to implement centralize actions that cannot be 
performed by ants. The interaction of the three activities 
within the construct Schedule_Activities is up to the 
programmer to specify. 
III.THE PROPOSED METHOD
In this section, the definition of a DNA copy number 
variation (CNV) problem in array CGH data, and then the 
ACO-CGH algorithm is described in detail. 
A. Problem Definition 
Given a sequence of short DNA fragments (probes) sorted 
by their appearing locations in the DNA sequences, denoted 
by P = {p1, p2,…, pn}. The array CGH approaches offer a 
vector V = {v1, v2,…, vn} of the normalized fluorescence 
intensity ratios, such that if there are more copies of pi in the 
target DNA sequence than in the reference DNA sequence, 
the value of vi will be larger than 0. On the other hand, if 
there are  less copies of pi in the target DNA sequence than 
in the reference DNA sequence, the value of vi will be 
smaller than 0. 
The objective of the CNV problem is to find the minimal 
ordered subset  ni pppp  ,...,,...,,P 21* , where 11 pp  ,
nn pp  , ni 1 , and nn   , and the set of the line 
segments,  nppp  )32, np  1(,...,pp  21S composes a path to the 
sequence P such that the error norm between P and S is no 
more than a error bound . For convenience of presentation, 
we define segment 
ji pp  as the col ion of points from pi
to pj in P, i.e.,
lect
 jp .  The eriiji pppp ,...,, 1	  norm 
between P and S, denoted E(P,S), is then defined as  
ror




	
1
1
1)(S)E(P,
n
i
ii ppe                                 (1) 
843
through them or most passing ants constructed bad-quality 
paths.
E. Sys
recallprecision
tem Architecture 
Learning 
Module
Code
words
Training
Samples
Classifi-
cation
Module
Pattern
Classifier
Input
Sequence
Graph
Construction
ACO
Solution
Construction
Best Solution
Updating
Pheromone
Updating
Stop
Conditions
Checking
Segment
Labeling
Result
Visualization
Post Processing
Fig. 2. The system architecture of the ACO-CGH. 
Fig. em, 
which consists of three layers --pattern classifier, ACO, and 
TAL RESULTS
T used a set 
of 3 M )
d recall for 
ab
2. shows the block diagram of the  ACO-CGH syst
post processing. The functions of the first and second layers 
are discussed above. The post processing layer is added in 
order to label the final segmentation results and then 
visualize them for a display.  A simple voting scheme is 
used to label the final segments: the label of a segment is 
determined by the majority of probe labels belonging to the 
segment. Experimental results show that the voting scheme 
leads to a good classification. 
IV. EXPERIMEN
o illustrate the performance of our method, we 
CL cell lines (Granta-519, HBL-2, and Z138C
whose array CGH profiles were manually analyzed and 
published by deLeeuw et al. [3]. A set of synthetic DNA 
sequences with given ground true is used to train the pattern 
classifier. The algorithm Robust-HMM proposed by [3], are 
also simulated for performance comparison. 
We evaluated the performance of the compared 
algorithms by calculating precision an
errations including gains and losses. Giving a ground 
truth labeling and a predicted labeling of the clones obtained 
by performing these algorithms. The recall, which means 
the portion of true aberrations detected by the algorithm, is 
defined as the ratio between the number of true positives 
and the number of true aberrations. The Precision, which 
means the portion of predicted aberrations that are true, is 
defined as the ratio between the number of true positives 
and the number of predicted aberrations. The precision and 
recall sometimes conflict with each other. The F-measure 
provides a way to consider both precision and recall 
measures together: 
recallprecisionF  2 .    (8)
	
To compare the performance between Robust-HM  
ACO-CGH, Fig. 3. shows the distribution of F-measures 
over the three MC l lines based on box-and-whisker 
pl
mulated algorithms. 
A
In this paper, we have presented a new method for detecting 
recurren es in array CGH 
data. The proposed m
ore test data should be included 
in
his work is supported in part by a research grant from 
National Taiw rant Number: 
NTOU-RD961
] D. Pinkel and D. G. Albertson, “Array Comparative Genomic 
Hybridization and Its Applications in Cancer,” Nat. Genet., pp. 
11-17, 2005. 
M and
L cel
ots which are graphical displays that simultaneously 
describe several important features of a data set with the line 
within the box indicates the median of the distribution, the 
top and bottom edges of the box indicate the third and first 
quartiles, the ends of the whiskers indicate the 95% 
confidence intervals of the distribution [16]. According to 
Fig. 3., the proposed ACO-CGH outperforms the compared 
Robust-HMM method. The ACO-CGH and Robust-HMM 
have mean F-measures of 0.8˃ʳ ̈́ʳ ˃ˁ˃˅ and 0.8˃ʳ ̈́ʳ ˃ˁ˃ˈ,
respectively indicating that using an ACO framework 
improves accuracy over Robust-HMM. 
Fig. 4.(a), (b), and (c) are the F-measures for the Granta-
519, HBL-2, and Z138C MCL cell lines, respectively to 
further compare the accuracies of the si
ccordingly, the ACO-CGH has a better performance in 
terms of average F-measure over the Robust-HMM. Finally, 
in Fig. 5., we show some of the segmentation results of the 
ACO-CGH algorithm at chromosome 1 using the HBL-2 
test data.
V. CONCLUSIONS
t aberrations from DNA sequenc
ethod is robust to outliers and is able 
to integrate prior knowledge about the CNV locations into 
the ACO-CGH algorithm.  
We have demonstrated that on the test data this model 
outperform other compared methods.  To demonstrate the 
performance of the model, m
to the experimental results. 
ACKNOWLEDGEMENTS
T
an Ocean University under G
-05-02-01-01
REFERENCES
[1
[2] W. R. Lai, M. D. Jhhnson, R. Kucherlapati, and P. J. Park, 
“Comparative Analysis of Algorithms for Identifying 
Amplifications and Deletions in Array CGH Data,” 
Bioinformatics, Vol. 21, No. 19, pp. 3763-3770, 2005. 
845
