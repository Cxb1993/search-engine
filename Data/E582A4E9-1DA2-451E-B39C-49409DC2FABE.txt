 2
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                            日期：98 年 9月 20日 
國科會補助計畫 
計畫名稱：遠距點選輸入之手勢互動控制系統開發 
計畫主持人： 李永輝        
計畫編號：NSC 96－2628- E －011 － 007  －MY2
學門領域：工業工程與管理 
技術/創作名稱 遠距點選輸入之手勢互動控制系統 
發明人/創作人 李永輝 
中文： 
利用兩部攝影機發展出的三度空間，以自然手部動作為輸入控制設備，萃取
手部與頭部的膚色，並以投手連線投射到螢幕，進行遠距的點選控制。 
 
技術說明 英文：  
A GOAL-DIRECTED POINTING TRACKING SYSTEM: we 
combine stereoscopic range information and skin-color 
classification to achieve a robust tracking system of the free 
hand movement in 3D. The setup consists of two fixed-baseline 
cameras connected to PC. 
 
可利用之產業 
及 
可開發之產品 
  
智慧環境配合的遠距輸入裝置 
技術特點 
 
三度空間自然姿勢的輸入方法 
推廣及運用的價值 
 
與智慧環境結合，最適合於遠距與大型銀幕的互動需求 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
成果推廣單位（如技術移轉中心）。 
 4
2-D Hand/Head Area Detection 
 
GPTS uses skin detection and background subtraction 
techniques to isolate the image of hand and head on each 
frame, which is then used for direction calculation.  Del/The 
skin detection algorithm is a modified version of that of 
McKenna and Morrison (2004).  Areas of hand and head 
were detected by identifying ellipse cylinder skin areas in the 
ycbcr color space of the images from two cameras.  Only 
pixels of skin-color-liked are collected to a coordinate sheet.  
The most memory-saving format is to use a binary dataset.  
Erosion algorithm was used to filter out the noise. Finally, a 
K-means clustering mechanism (K=3) was used to cluster the 
coordinates sheet into 3 skin-area sets (2-D positions of the 
head and 2 hands).  These operations are applying onto the 
both 2 images in the initial state. 
 
2-D Hand/Head Position Tracking 
 
It is not efficient to scan the whole stereo-image, after 
obtaining the initial 2-D hand and head coordinates, method of 
kernel based object tracking (Comaniciu et al., 2003) was 
adopted.  When color histogram that describe the object 
population, a comparisons of the current and the previous 
target populations were conducted, update by gradient 
information until the correlation coefficient is large enough 
and then the scan stopped.  Since the moving trajectories of 
hand and head are nearby-differentiable, the method works 
efficiently in this application. 
 
 
 
Figure 2: hand/head position tracking and the pointing movement 
 
3-D Goal-directed pointing movement 
 
The trajectories of pointing movement were smoothed by 
Kalman filter (Keskin, et al., 2003). Combine the paired 2-D 
hand and head positions from each image by DLT parameters, 
the 3-D coordinates of the goal directing pointing were 
calculated.  Sequences of hand and head information are then 
used for goal-directed pointing controls.   
The system is capable of recognizing gestures at a speed of 
20 Hz. It was then the velocity and the accelerations of the 
pointing movements were calculated. 
SYSTEM VALIDATIONS 
In order to evaluate the performance of our system, we 
prepared a calibration test and an aiming stability tests. There 
were 16 different markers on the calibration frame. The 
geometric relationships and the 3D Cartesian coordinates of 
the 16 markers were known.  The frame was of a size of one 
cubic meter. The frame was moved around and into the filed of 
view of the two cameras in a distance of 3.6 m. The 3D 
coordinates of the markers were calculated and the standard 
deviations of the differences of the  mean coordinates in three 
axes were presented in Table 1.   
 
Table 1: Reliability of 3-D calibration (N=16) 
Standard Deviation(cm) in 
Experiment    x(cm)     z(cm) 
Moving ˆσRange Moving ˆσ Range 
1   2.26    9.3  1.66  4.60 
2   2.18    8.6  1.83  6.32 
3   2.94   12.1   1.56  5.44 
4   2.54   10.9   1.48  4.89 
5   2.04    8.9  1.42  5.72 
6   2.66   11.2   1.87  6.08 
7   2.29   10.2   1.45  5.42 
8   2.61   11.3   2.03  6.48 
9   1.97    8.5  1.49  5.68 
10     2.74   10.2   1.63  5.52 
2.44   10.1   1.66  5.62 
 
Table 2 presents the aiming stability and the range of 10 
goal-directed pointing movements.  It showed that the aiming 
accuracy, in a distance of 3.2 m which is about 5 times of the 
length of head/hand link, can be achieved in a range of 10 cm 
in X-axis and 6 cm in Z-axis. 
 
            Table 2 Point Stability in 3.2 m 
No.  X-axis  Y-axis   Z-axis 
1  0.012  0.124  0.032 
2  0.011  0.326  0.012 
3  0.006  0.475  0.021 
4  0.011  0.102  0.015 
5  0.007  0.079  0.014 
6  0.003  0.207  0.033 
7  0.008  0.292  0.007 
8  0.016  0.166  0.017 
9  0.004  0.143  0.036 
10  0.012  0.078  0.009 
total σ 0.031 0.739  0.070 
 
 
Reliability test of the 3D coordinates showed that the 
most variations (<0.8 cm) appeared in Y-axis (forward and 
backward movements) 
 6
 
 
 
 
 
 
CONCLUSION 
 
In this paper, GPTS was demonstrated to recognize a 
range of gestures in our 3D video based recognition 
environment in a distance of 3 meters.  The system was 
designed to interact with a music player.  The recognition rate 
of this scenario is greater tan 90% when user’s head and hands 
are well tracked.  Other intended future work includes (a) a 
strong recognition engine using recurrent neural network 
model or hidden Markov model, (b) a more complete elements 
of gesture will be defined that movements in 3D spaces, 
moving speeds, and the strength of movement are more 
meaningful, and (c) interaction with a larger display at a longer 
distance. 
 
Acknowledgements 
This study is supported by a grant from the National Science 
Council, R.O.C. (Project No. NSC NSC 96-2628-E-011 -007 
-MY2).  The authors wish to acknowledge this financial 
support. 
 
References 
[1]Comaniciu, D., Ramesh, V., and Meer, P., 2003, 
Kernel-based object tracking,   IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 25, 564-575. 
[2]Kai, N., Seemann, E., and Stiefelhagen, R., 2004, 
3D-tracking of head and hands for pointing gesture recognition 
in a human-robot interaction Scenario, Automatic Face and 
Gesture Recognition, Proceedings. Sixth IEEE International 
Conference, 565-570. 
[3]Kai, N. and Stiefelhagen, R., 2003, Pointing gesture 
recognition based on 3D-tracking of face, hands and head 
orientation, ICMI’03, November, 5-7, 2003, Vancouver, 
British Columbia, Canada. 
[4]Lee, Y.H., Yeh, C.H., and Wu, S.K., 2008, Accuracy 
measurement of distant goal- directed hand pointing 
movements, Conference Proceeding of the 1st East Asia 
Ergonomics Symposium, Nov. 12 – 14, Kitakyushu, Japan. 
[5]McKenna, S. J. and Morrison, K. 2004, A comparison of 
skin history and trajectory-based representation schemes for 
the recognition of user-specified gestures, Pattern Recognition 
37, 5, 999-1009 
[6]Lee, M. S., Weinshall, D., Cohen-Solal, E., Comenarez, A., 
and Lyons, D., 2001, A Computer Vision System for 
On-Screen Item Selection by Finger Pointing, in Proc IEEE 
Conference on Computer Vision and Pattern Recognition, 
Hawaii, Dec 2001. 
[7]Keskin, C., Erkan, A., Akarun , L., 2003, Real time hand 
tracking and 3D gesture recognition for interactive interfaces 
using HMM, ICANN 2003 
 
 
   
 
 
 2
(二) 會議內容 
JES現有會員約2000人，成員遍及日本各地，相較於台灣的人因學會，JES有更多的
業界、實務界、尤其是職安衛醫師，物理及職能治療師的參與，JES是僅次於美國人
因工程學會次大的組織。大會主題為” 人間工學的過去、現在、與未來”，研討會的
規模相當驚人，論文發表與對談內容亦較實務導向並具本土需求的鏈結性。主題如
下： 
1. Aviation Human Factors 
2. Ergonomic Design 
3. Kansei Information Processing 
4. Information and Social Ergonomics 
5. Medical Safety 
6. Nursing Ergonomics 
7. Dental Ergonomics 
8. Auditory Ergonomics 
9. Clothes 
會議的特色之一，JES推出”人間工学の歴史（写真）”展示。該展覽是由學會成
員石松健男(がとらえた)，透過累積四十多年的事件照片，說明日本企業及社會，
自学会設立至今與人因工程相關的歴史證據。照片由黑白到彩色，內容由傳統到現
代，技術由機器到資訊科技，彌足珍貴。 
 會議的特色之二是”人間工学活用事例展示”。包括感性(人間生活技術戦略)，
人間生活工学研究の活動紹介，ISO/TC159 の活動および人間工学JIS規格，特定
非営利活動法人キッズデザイン協議会，国際ユニヴァーサルデザイン協議会
(IAUD)活動ご紹介，人間工学への二関節筋力学体系の導入，伝統みらい研究セン
ターにおける暗黙値の抽出。顯示JES在企業互動、國際鏈結、以及與政府法人的互
動成效。 
(三) 與會心得 
 JES的發展路線圖: 會議的最大收穫在於瞭解JES的發展計畫。JES為深化該
組織對於日本社會的貢獻，並進一步為其組織的長遠發展準備，特別依其社
會、企業、及學術研究的需求，提出JES的road map規劃。JES發展路線的規劃，
是源發於企業的road map以及政府的road map。 規劃的基本內容包括：
Introduction, underlying considerations, requirement for ergonomics, 
important ergonomics issues, work environments & systems friendly to 
people, diversified needs in human lives, safe and comfortable mobility, safe 
