 intelligent and various, but also enables the interaction more 
humanistic.  
In order to reach the goal, it is necessary for a robot to 
have the capability of emotion perception and 
decision-making based on its own personality and emotion 
similar to human. For emotion detection, there are various 
types of measurement, and the most common measurements 
generally include the physiological signal measurement and 
audio or visual signal measurement. One of the most popular 
and interesting research among three of them is speech 
emotion recognition since the speech is the most common 
communicative way in our daily life. For decision-making of 
emotion, it is believed that a robot which has human-like 
personality that affects its emotional state and action 
selection will lead human-robot interaction to be more 
friendly and effective. 
Therefore, it is expected to design a personality model 
based on emotion psychology with ability to detect emotions 
in speech and to respond according to its personality trait, 
and finally to apply it to a robot or a computer interface to 
improve human-robot interaction. 
As the previous section introduces the concept of 
architecture of emotion perception and emotion generation 
considering personality, the most concerned problem here is 
that there are still some limitations and uncertainties of the 
emotion recognition in speech. That is to say, the error of 
emotional speech recognition exists while personality model 
percepts external stimuli of emotional speech, and 
consequently it will cause certain difference with 
comparison to designed personality trait. Besides, it is found 
that each combination of recognition errors will have 
different influence on the personality model. In order to 
overcome the defect of emotion speech recognition and to 
maintain the consistency of personality trait, adjusting 
model parameters is a method for compensation. Therefore, 
how to figure out the relationship between selection of 
model parameters and the influence of each combination of 
recognition errors on personality model is needed to 
consider and discuss. 
This project proposes the combination of emotion 
speech recognition and an emotion model based on HMM to 
establish the optimistic and pessimistic personality models 
which are capable to detect emotions in speech and to react 
based on its characteristic of personality. Besides, in order to 
maintain the consistency of personality traits, the model 
under the influence of each combination of recognition 
errors is considered and analyzed to compensate the 
difference by adjusting corresponding model parameters 
within acceptable error requirement. 
This project has five chapters where Chapter 1 is the 
introduction of motivation and problem statements. Chapter 
2 discusses literature survey of emotional speech recognition 
and artificial emotion model. In Chapter 3, a typical process 
of emotional speech recognition with assistant tool of data 
mining, and an emotion model based on HMM are 
introduced and used in this project while the results of 
recognition and model simulation are shown at last. In 
Chapter 4, the combination of emotional speech recognition 
and emotion model is presented, and the model under the 
influence of each combination of recognition errors is also 
discussed. In the end, the conclusions and future works are 
discussed in Chapter 5. 
 
II. BACKGROUND AND LITERATURE SURVEY 
2.1 Techniques of Emotional Speech Recognition 
Recently, emotional speech recognition has become a 
popular topic and it can be applied in a wide variety of fields. 
According to previous researches, a process of emotional 
speech recognition generally includes following steps: 
define emotions, establish emotional speech database, select 
features from emotional speech, and classify emotions. The 
related works on each step will be presented in this section. 
 
2.1.1. Emotion Definition 
For emotional speech recognition, the first important task 
is to get a proper definition of emotions. Generally, there are 
3 categories of definition to the emotions [P1: Fragopanagos 
& Taylor 2005]: category labels, activation-evaluation space, 
and time related categories. Category labels classify the 
emotions by using a single word to represent an emotion 
from linguistic analysis. Activation-evaluation space uses 
2-D space to classify emotions shown in Figure 2.1 [P2: 
Zeng et al. 2005]. The vertical axis of Figure 2.1 is activation 
axis which determines the emotion is active or passive. And 
it usually can stand for the movement human tends to do. For 
example, people wave their hands and move their bodies 
when they are happy, so happiness is active while people 
tends not to move when they are sad, so sadness is passive.  
The horizontal axis is evaluation axis which decides the 
motion is positive or negative. It usually can represent the 
emotion-triggered state is benefic or harmful. Likewise, 
happiness is positive and sadness is negative. 
 
 
Figure 2.1: Activation-evaluation space [P2: Zeng et al. 
2005]. 
 
The third one considers time factor to classify emotions. 
For instance, the emotion is defined with a period of time 
started from the beginning time when the emotion is aroused 
to the ending time when the emotion is replaced with other 
emotion. Based on the definitions of emotion mentioned 
above, the category of emotion can be decided according to 
  
In this project, WEKA (Waikato Environment for 
Knowledge Analysis) which is a machine learning software 
written in Java is used for classification. It offers various 
classification algorithms and convenient graphical interface 
so that the most proper classifier for recognition can be 
easily chosen in comparison with numbers of classifier.  
 
2.2 Artificial Emotion Models 
Artificial emotion has become a popular topic. 
Researchers show that emotions have impact on 
decision-making, intelligence, sociality, learning, and so on. 
In order to apply the intelligent function of emotion into 
computer and make computer have intelligence, modeling 
emotion is necessary. In [B1: Andersen & Guerrero 1998], it 
presents five different theoretical approaches to the 
modeling of emotions as follows. A discrete approach is 
built based on the principle of basic emotions. Two 
dimensional approaches classify emotions by certain 
characteristics. The prototype approach combines basic 
emotions with a hierarchical structure, and Plutchik's 
multidimensional approach combines the discrete and 
dimensional approaches. 
According to the theoretical approaches, there are many 
related works on the improvement of emotion model. OCC 
model [B2: Ortony et al. 1998] provided the best 
classification of emotions, and gave a reasoning process, that 
is to say, certain emotion is most likely to occur under the 
corresponding circumstance, but it neglected details of 
emotion generation. The Circumplex model [P14: Posner et 
al. 2005] resulted in a two-valued vector for each emotion 
compare which can be easily compared with each other. 
Picard provided a hidden-markov emotion model [B3: 
Picard 2000] which took three emotions into consideration, 
and emotional process was not complex. [P15: El-Nasr et al. 
2000] proposed a complex emotion model, but formulas of 
calculating emotion intensities were hard to understand. 
 
Related Work 
Theoretical 
Approach
Emotion 
Model
Discrete 
Approach
The OCC 
Model
2-Dimensional 
Approach
The 
Circumplex
Model
3-Dimensional 
Approach
PAD 
Emotional 
State Model
Multi-
Dimensional 
Approach
Plutchik's
Multi-
Dimensional
Model
Prototypes 
Approach
[P16: Shaver et 
al. 1987]
[B4: Mehrabian & 
Russell 1974]
[P14: Posner et al. 2005][B2: Ortony et al. 1998 ] [B5: R. Plutchik 1980 ]  
Figure 2.2: Literature survey of emotion model according to 
the theoretical approach. 
 
In this project, hidden-markov emotion model is used 
since the emotion transfer is dependent on emotion 
probability distribution, and it can be easily chosen and 
understood to form different personality traits similar with 
the changes rule of human emotions basically. 
 
III. RELATED RECOGNITION PROCESS AND EMOTION 
MODELING 
3.1 Emotional Speech Recognition 
Emotional speech recognition has been developed for 
decades with more and more interest. The classification of 
emotional speech is the first important task of emotion 
recognition, and it can be widely used in variety of fields for 
application. The typical process of emotional speech 
classification is depicted in Figure 3.1. The speech samples 
used in this project are obtained from Berlin database. 
Several typical acoustic features based on previous 
researches are selected to proceed with feature extraction. 
And then, the best suitable features are selected by WEKA 
(Waikato Environment for Knowledge Analysis) in [P17: 
Hall et al. 2009] which is a machine learning software 
written in Java as description of emotional speech to analyze 
the characteristics of emotions in speech. Finally, the 
selected features are also classified by WEKA to present 
certain kinds of emotion.  
 
Voiced 
Emotional
Speech 
Data
Emotional
Speech 
Data
Audio 
preprocessing
Feature 
Extraction
Original 
Feature 
Dataset
Feature 
Selection
Selected 
Feature 
Subset
Emotion 
Recognition
Recognized 
Emotional
States  
Figure 3.1: Typical flowchart of emotional speech 
classification 
 
3.1.1. Emotional Speech Database 
In order to analyze emotion factors in speech, the 
database used in this project is Berlin Database of emotional 
speech [P18: Burkhardt et al. 2005], which is an acted 
emotional speech database. This database contains 5 actors 
and 5 actresses, 10 different sentences seen in Table A.1 of 7 
kinds of emotions: anger, boredom, disgust, fear, happiness, 
sadness and neutral. There are 535 speech samples, and the 
length of the samples varies from 3 seconds to 8 seconds. All 
the speech samples are recorded in the format of 16kHz, 16 
bit and single track. 
Before extracting the emotion cues from emotional 
speech samples, speech signal is through a series of process. 
At first, since most audio signals are more or less stable 
within a short period of time, emotional speech signal is 
 relevant features in the data and to train the data to predict 
and learn numeric classes. 
 
 
(a) (b) 
 
(c) (d) 
 Figure 3.4: (a) Interface of Explorer in Weka, (b) data 
displayed graphically in histogram, (c) Interface of Select 
Attributes in Explorer, (d) Interface of Classify in Explorer.
 
In this project, CFSSubsetEval (Correlation-based 
Feature Selection) in the feature selection techniques 
provided by WEKA is used. The algorithm is used as a 
feature evaluator, which tries to identify and discard features 
that are closely correlated with one another. To determine 
the best subset of features, a best-first search strategy is used 
to determine what style of search is performed, while the full 
set of training data as well as 10 cross validation procedure 
are separately used to determine the worth of the attribute 
subset. Table 3.2 shows subsets of features selected by the 
process described as above. 
 
Table 3.2: The selected subsets of specifying features 
Selected features 
Feature 
Using full 
training set 10 cross validation
Intensity-related rslp_var 
vol_min 
slp_mean 
rslp_max 
rslp_var 
fslp_mean 
fslp_var 
Duration-related ZCR_median ZCR_median 
Pitch-related 
Pitch_std 
rpslp_max 
rpslp_var 
fpslp_var 
Pitch_median 
Pitch_std 
rpslp_max 
rpslp_var 
fpslp_var 
MFCC 
MFCC1_min 
MFCC2_mean 
MFCC4_mean 
MFCC1_min 
MFCC2_mean 
MFCC4_max 
MFCC4_min 
MFCC4_mean 
MFCC4_median 
 
In Table 3.2, it can be seen that the subset of features 
by using 10 cross validation contains the subset of all 
features by using full training set. It means that using 10 
cross validation in this case would generate certain 
redundant features. Therefore, the subset of features by 
using full training set is used to be train to predict five 
classes of emotion by using the classification algorithm 
provided by WEKA. 
 
3.1.4. Results of Classification 
The classification algorithms from WEKA used in this 
project include NaiveBayes (standard probabilistic Naïve 
Bayes classifier), BayesNet (basyesian nets), 
MultilayerPerceptron (a feedforward artificial neural 
network model), IB1 (basic nearest-neighbor instance-based 
learner), AdaBoostM1 (a simple generalization of Adaboost 
for more than two classes), VFI (voting feature intervals 
method, simple and fast), NNge (nearest-neighbor method 
of generating rules using nonnested generalized exemplars), 
Decision Stump (one-level decision tree), J48 (C4.5 decision 
tree learner), RandomForest (construction of random 
forests), REPTree (fast tree learner that uses reduced-error 
pruning) and etc. All the following results from Weka’s 
classifiers are based on authentic emotion dataset using 
10-fold cross validation. The results for each classifier in 
WEKA are shown in Table 3.3.  
 
Table 3.3: Results for each classifier in WEKA 
                          Result 
Classifier 
Accuracy 
(%) 
NaiveBayes 85.71 Bayes BayesNet 94.05 
Functions MultilayerPerceptron 89.29 
Lazy IB1 85.71 
Meta AdaBoostM1 48.81 
Misc VFI 78.57 
Rules NNge 89.05 
Decision Stump 48.81 
J48 94.29 
RandomForest 94.05 Trees 
REPTree 91.67 
 
It can be seen that the method of AdaBoostM1 in Meta 
and of Decision Stump in Trees give bad results while the 
method of BayesNet in Bayes and the methods of J48, 
RandomForest and REPTree in Trees perform well. In 
addition, each growing branch in the methods of decision 
tree can be visualized by WEKA. In this project, accuracy of 
classification and the complexity of decision tree are taken 
into consideration, thus, the method of REPTree is used.  
In Figure 3.5, each leaf represents a feature of selected 
feature set and the end leaf stands for a class of emotion. 
That is to say, these four selected features are extracted at 
last from each emotional speech sample, and the emotion 
category of each sample is determined by the range of these 
feature values shown in Figure 3.5. For method of REPTree, 
 B
Aπ
…
…
…
M
P Emotion state 
probability distribution
Markov
process
Stimuli matrix
External stimulus
Initial state
probability distribution
 
Figure 3.6: The architecture of emotion model based on 
HMM. 
 
Here N  is the numbers of emotion states, M  is the 
numbers of external stimuli, A  is emotion transition matrix 
which depicts the transition of emotion states in 
correspondence with changes of stimuli. Also, different A  
under same external stimuli will cause personality traits. So 
A  is also called personality matrix. B  is observation value 
matrix which represents the responses to external stimuli, so 
also called stimuli matrix here. π  is initial probability 
distribution of emotion state while 
( ) ( ) ( ) ( )
1 2, , ,
T T T T
NP p p p⎡ ⎤= ⎣ ⎦L  shows the current 
probability distribution of emotion state corresponding to 
the type and intensity of external stimuli. 
When emotion model based on HMM receive the 
external stimuli, the transition process of emotion states will 
be started from the emotion state ( )0Pπ =  to ( )TP . By 
adjusting proper parameters of the model, different type and 
intensity of emotion state will generate corresponding 
personality traits. 
 
3.2.2.1. External Stimulation 
External Stimuli can be depicted by the observation 
value, the observation matrix and the observation sequence 
of HMM. The congregation of observation value is 
{ }| 1, 2, ,mV V m m M= = = L . The observation matrix 
(e.g. stimuli matrix) is defined as follow: 
( ){ }
( ) ( ) ( )
( ) ( ) ( )
( ) ( ) ( )
1 1 11 2
2 2 21 2,
1 2
b b bN
b b bNB m i M N
b M b M b MN
⎡ ⎤⎢ ⎥⎢ ⎥=⎢ ⎥× ⎢ ⎥⎢ ⎥⎣ ⎦
L
L
M M L M
L
.           (3.8) 
Each element of ( ){ }, M NB m i × : ( )ib m  represents 
the probability that stimuli m  ( )1 m M≤ ≤  affects the 
emotion state i  ( )1 i N≤ ≤ . Therefore, 
( ) ( ) ( ) ( )1 2m NB V b m b m b m= ⎡ ⎤⎣ ⎦L  here is the 
stimulus vector corresponding to the m th of emotion state, 
and satisfying: 
( )
1
1
M
i
m
b m
=
=∑ , ( )1 i N≤ ≤ .                     (3.9) 
 By adjusting the elements of stimulus vector, the type of 
stimulus can be determined. The observation sequence here 
is used to represent an actual stimulus: 1 2 TO O O O= L , 
where t mO V= , 1, 2, .t T= L  T  is the stimuli intensity, 
which stands for the accumulated influence of numbers of 
the same stimuli mV . The m th kind of stimulus with 
intensity T  can be denoted as 
1 2T TO O O O V V Vm m m m= =L L , ( )1 m M≤ ≤ , 1,2, ,t T= L .        (3.10) 
 Also, the i th kinds of emotion state generated by 
stimulus TmO  can be described as the emotion state sequence: 
1 2
T
i T i i iQ Q Q Q S S S= =L L , ( )1, 2,i N= L . Here 
the numbers of S is T . So under the influence of stimulus 
T
mO , the i th kinds of emotion state probability distribution 
( )T
ip can be described as: 
( ) ( ) ( )| , | ,T T T Tp P Q O P S S S Oi i m i i i mλ λ= = L .             (3.11) 
 
 Here is assumption that a kind of stimulus only stimulates 
a corresponding kind of emotion state. In the Eq. 3.11, when 
m i= , stimulus TmO  meets the emotion state sequence 
T
iQ and the probability of the i th emotion state will 
increase. On the contrary, when m i≠ , the probability of 
the other emotion state will decrease. 
 The emotion state probability distribution ( )Tip  can be 
derived by forward-backward algorithm when the emotion 
transition matrix A is determined. The following 
calculation process is depicted in [P21: Wang et al. 2010]. 
According to Eq. 3.10 and Eq. 3.11, the calculation formula 
of forward variable ( )mt iα  is: 
( ) ( ) ( )
( ) ( ) ( ) ( )
1 , |1 1
1 2 1, |1 1
1
m i P V q S b mm i i i
Nm t t mj P V V V V q S i a b mt m m m m t j t ij j
i
α λ π
α λ α
⎧ = = =⎪⎪⎨ ⎡ ⎤+⎪ = = = ∑⎢ ⎥+ +⎪ ⎢ ⎥=⎣ ⎦⎩
L
.    (3.12) 
And the calculation formula of backward variable ( )mt iβ  
is: 
( ) ( )
( ) ( ) ( ) ( )
*| 1,1
1 2 | , 1
1
m i P q S i NT T i
Nm t t T mi P V V V q S a b m jt m m m t j ij j t
j
β
β λ β
⎧ = = = ≤ ≤⎪⎪⎨ + += = =⎪ ∑ +⎪ =⎩
L
.      (3.13) 
Combining the forward and backward variables: ( ) ( ) ( )
( ) ( ) ( ) ( )
, |
| , |
1 1
T m mP O q S i im t i t t
N NT T m mP O P O q S i im m t i t t
i i
λ α β
λ λ α β
⎧ = =⎪⎪⎨⎪ = = =∑ ∑⎪ = =⎩
.          (3.14) 
Defining a new variable ( )mt iγ : 
( ) ( )
( ) ( )( ) ( ) ( )( ) ( )
| ,
|
1
m Ti P q S Ot t i m
m m m mi i i it t t t
NT m mP O i im t t
i
γ λ
α β α β
λ α β
= =
= =
∑
=
.                 (3.15) 
 value: minπ ∗ . Therefore, by comparing Eq 3.23 
and Eq. 3.24, the minimun value of θ  can be 
determined as: 
*
min
Min
Nθ π= .                       (3.25) 
The maximum value of θ  is derived by considering 
that emotions transfer to steady state after a certain number 
of calculate steps within a certain error requirement. 
Therefore, the equation is defined as follow: 
*n
i ip π δ− ≤ , 0δ > .                      (3.26) 
 Where n  is the total number of step, δ  is the error after 
n  times calculation to reach steady state. Based on Eq. 3.26, 
the maximum value of θ : Maxθ  can be derived when n  
and δ  are determined. 
 In conclusion, when determining personality traits with 
the value of θ , the choice of θ  should meet the condition 
of Min Maxθ θ θ≤ ≤  to fit the process of emotion transition. 
 
3.3 Simulation Results 
In this section, the self-regulation of emotion state, and 
transfer process of emotion state under the external stimulus 
based on HMM were simulated. The impact of model 
parameters on emotion was also simulated and discussed at 
last. Besides, in order to simply express and verify the 
mechanism of emotion transition and also to build the 
emotion model used conveniently in robot, here it is 
supposed that the robot possess three common basic emotion 
states (i.e. 3N = ): happiness, anger and sadness 
respectively in [P22: Yu et al. 2008]. The simulations in 
[P20: Wang et al. 2010], [P21: Wang et al. 2010] and [P22: 
Yu et al. 2008] were referred and HMM toolbox in matlab in 
[O1: Murphy 2005] was used to run following simulations. 
 
3.3.1. Simulation of Emotion Self-Regulation 
In emotion self-regulation model, it was supposed that 
emotion state will move slowly from the initial probability 
distribution to an emotional steady state after a long period 
transition. Therefore, the limit probability of matrix A  here 
is [ ]* 1 3 1 3 1 3π = . Other parameters are as follow: 
the initial probability distribution [ ]0.8333 0.125 0.0417π = ; parameter 100θ =  
in order to speed up the emotion state self-transition; and 
additionally emotion intensity is defined as 
( ) ( ) *T TP P πΔ = − . 
0 5 10 15 20 25 30 35 40 45 50 55 60
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
T
P
i Δ(T)
 
 
Happiness
Anger
Sadness
 
Figure 3.7: when θ=100, the curve of emotion state 
self-transition process. 
 
In Figure 3.7, it can be seen that ( )TPΔ  of three emotion 
states decreased to zero eventually after certain period of 
time. That is to say, emotion state probability distribution 
( )TP  is consistent with limit probability distribution 
[ ]* 1 3 1 3 1 3π =  at last. The simulation result is 
so-called ‘neutral’ discussed in Section 3.2.1 and satisfies 
the hypothesis based on emotion psychology. 
 
3.3.2. Simulation of Emotion Stimulus Transfer 
When the emotion stimulus transfer model suffers one 
kind of stimulus, the change curve of emotion state 
probability distribution is shown in Figure 3.8. Here are the 
relevant parameters as following: stimuli parameter is 
1m = ; parameter 12θ =  ( min 9θ = ); the limit probability 
of A  is [ ]* 1 3 1 3 1 3π = ; the initial emotion state 
probability distribution is [ ]0.3568 0.3610 0.2822π = . 
 
0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
p(
T)
 
 
←p1 = 0.7969
←p2 = 0.1179←p3 = 0.0852
Happiness
Anger
Sadness
 
Figure 3.8: The change curve of emotional state probability 
distribution in correspondence with the change of T. 
 
It can be seen that ( )Tp  of emotion state 1 increased from 
1π  to near 1 in correspondence with the change of T , while 
 0 5 10 15 20 25 30 35 40 45 50 55 60
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
T
P
1 Δ(T)
 
 
π*=[4/12 4/12 4/12]
π*=[4.07/12 3.965/12 3.965/12]
π*=[4.14/12 3.93/12 3.93/12]
π*=[4.21/12 3.895/12 3.895/12]
π*=[4.29/12 3.855/12 3.855/12]
π*=[4.32/12 3.84/12 3.84/12]
π*=[4.4/12 3.8/12 3.8/12]
 
Figure 3.12: The change curves of emotion intensity 
correspond to the change of T, while m=2, the initial 
probability distribution of personality transfer matrix is the 
same, but the limit probability is different. 
 
It can be seen that the change rate of emotion intensity 
( )
1
TPΔ  differs from each other with different limit probability 
distribution *π . As mentioned in Section 3.2.1, the limit 
probability distribution 1 2, , , Nπ π π π∗ ∗ ∗ ∗⎡ ⎤= ⎣ ⎦L  represents 
the utmost probability distribution of A , and it is said that 
emotion is neutral when 1 2 Nπ π π∗ ∗ ∗= = =L . Therefore, 
the equation * * 1 3i iπ πΔ = − , where 1,2,3i =  is defined 
as the personality intensity offset to reflect the change curve 
of ( )1
TPΔ . 
When * 0iπ Δ = , the personality intensity offset is zero, 
and it is regarded as neutral personality. It can be seen from 
the curve (red line) of Figure 3.11 and Figure 3.12 as a 
benchmark when the following situations are discussed: 
1. When * 0iπ Δ > , it is a positive personality offset 
for emotion i. The change rate of emotion 
intensity will increase when *iπ Δ  is increasing 
under the influence of stimulus iV . It can be 
seen in Figure 3.11 and the personality for 
* 0iπ Δ >  is optimism. 
2. When * 0jπ Δ < , it is a negative personality offset 
for emotion j. The change rate of emotion 
intensity will decrease when *jπ Δ  is decreasing 
under the influence of stimulus jV . It can be 
seen in Figure 3.12 and the personality for 
* 0jπ Δ <  is pessimism. Here is a particular 
situation when the difference between *iπ Δ  and 
*
jπ Δ  is too large. The change rate of emotion 
intensity will decrease down below zero under 
the influence of stimulus jV . It can be seen from 
the curves (black and yellow lines) of Figure 
3.12 and it is called as the neurotic personality. 
In conclusion, the limit probability distribution *π  can 
adjust the response rate of emotion to stimulus and generate 
different personality traits at the same stimulus condition. 
 
IV. ANALYSIS OF EMOTIONAL-SPEECH –BASED 
PERSONALITY MODEL 
4.1 Emotional-Speech-Based Personality Model 
Based on the Section 3, the combination of emotional 
speech recognition and emotion model based on HMM here 
is presented as emotional-speech-based personality model. 
In the part of personality model, it is built according to 
simulations of emotion stimuli transfer model and impact of 
A on Emotion States discussed respectively in Section 3.3.2 
and Section 3.3.3. In the simulations, each change curves of 
emotion state probability distribution corresponding to the 
change of time has a specific parameter set of emotion model 
and can be described as a kind of personality traits. Then, the 
result of emotion recognition in speech here is used as 
external stimuli of personality model. 
 Since the personality model is based on HMM, it usually 
computes a sequence of input at one time. In order to apply 
emotional-speech-based personality model to human-robot 
interaction, it is necessary to modify the input request to let 
the model receive emotional voice information time by time. 
Thus, the current input data here is loaded at one certain time 
including the previous input information to ensure the 
HMM-based model works functionally. Simply speaking, it 
can be considered that a sequence with all input data is 
divided into the subsets of input sequence in order. 
 Here is a simple test in Figure 4.1 to show a process of the 
model with discrete input and to compare it with the model 
which computes input sequence at one time. According to 
the hypothesis mentioned in Section 3.3 and a series of 
subsequently corresponding simulation results, the 
personality model here consists of three states (i.e. 3N = ): 
happiness, anger and sadness respectively. Also, according 
to Section 3.2.1, emotional steady state without external 
stimuli can be determined by *π . In general, it is suggested 
that emotion states gradually become “neutral” by given [ ]* 1 3 1 3 1 3π = . The corresponding parameters are 
as follow. Stimuli parameter: 1m = .  Here, all the voice 
input is used from the wav files labeled as happy in Berlin 
database and is detected through the recognition showing the 
happy information which can affect the corresponding 
emotion state 1: happiness. [ ]1,10T ∈ ; The limit 
probability of A  is [ ]* 1 3 1 3 1 3π = , 12θ = ; The 
initial emotion state probability distribution is [ ]0.35681 0.36104 0.28215π = . 
 3000 3100 3200 3300 3400 3500 3600 3700 3800 3900 4000
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x 10-3
θ
ΔP
 
 
Happiness
Anger
Sadness
 3980 3982 3984 3986 3988 3990 3992 3994 3996 3998 4000
-1
0
1
x 10-4
θ
ΔP
 
 
Happiness
Anger
Sadness
(a) (b) 
4000 4050 4100 4150 4200 4250 4300 4350 4400 4450 4500
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
x 10-3
θ
ΔP
 
 
Happiness
Anger
Sadness
 1 1.0001 1.0002 1.0003 1.0004 1.0005 1.0006 1.0007 1.0008 1.0009 1.001x 105
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
x 10-6
θ
ΔP
 
 
Happiness
Anger
Sadness
(c) (d) 
Figure 4.3: Simple tests on the determination of maximum 
value of θ: the difference in emotion probability distribution 
between (a) θ=4000 and  3000≤θ≤4000, (b) θ=4000 and 
3980≤θ≤4000, (c) θ=4000 and 4000≤θ≤4500, (d) θ=100000 
and 100000≤θ≤100100. 
 
In Figure 4.3 (a)-(c), the value of 4000θ =  is selected 
as benchmark of maximum value of θ  to see the 
convergence of emotion probability distribution with the 
value of θ  around 4000. It can be seen from Figure 4.3 (c) 
that when θ  is greater than 4000, the value of emotion 
probability distribution is not convergent yet. Here 
100000θ =  is chosen in Figure 4.3 (d), and it can also be 
found that although it still doesn’t reached to the 
convergence condition, the difference value become smaller. 
It confirms that the selection of the maximum value of θ  
discussed in Section 3.2.2.3 is based on error requirement 
after certain times calculate steps to reach steady state. 
 
4.2 Design of Optimistic and Pessimistic Personality 
Model 
Here, the parameter selection of emotional-voice 
based personality model for design of optimistic and 
pessimistic personality is introduced. There is an assumption 
of a scenario in human-robot interaction that certain internal 
emotion of a robot can reach extreme condition after 10 
times voice interactions with human. To make personality 
adjustment simply, the several relevant parameters are given 
at first as follow. Stimuli parameter: 1m = . [ ]1,10T ∈ , 
12θ = ; The initial emotion state probability distribution is 
[ ]1 3 1 3 1 3π = . And then, the limit probability π ∗  
is the only parameter which is used to establish the desire 
personalities. According to the discussion about π ∗  in 
Section 3.3.3, neurotic personality can be built when the 
difference between *iπ Δ  and *jπ Δ . Therefore, the limit 
probability of optimistic and pessimistic personality is 
chosen as [ ]1 2 1 4 1 4π ∗ =  and 
[ ]1 4 1 4 1 2π ∗ =  respectively. It can be derived that, 
even under the condition without external stimuli, emotional 
steady state will gradually transfer to the emotional state 1: 
happiness in optimistic personality model with [ ]1 2 1 4 1 4π ∗ =  while emotional steady state will 
gradually transfer to the emotional state 3: sadness in 
pessimistic personality model with [ ]1 4 1 4 1 2π ∗ = . 
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
p(
T)
 
 
Happiness
Anger
Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
p(
T)
 
 
Happiness
Anger
Sadness
(a) (b) 
Figure 4.4: (a) Optimistic personality with model 
parameters: π=[1/2 1/4 1/4] and θ=12. (b) Pessimistic 
personality with model parameters: π=[1/4 1/4 1/2] and θ=12
 
Figure 4.4(a) and Figure 4.4(b) show the change curves of 
each emotion state probability, which represent the 
optimistic and pessimistic personalities respectively. In 
Figure 4.2, it can be seen that when the model detects the 
happy factor in voice, the corresponding emotion state 
intensity: ( )1
Tp  in Figure 4.4(a) and ( )3
Tp  in Figure 4.4(b) 
increase obviously. After continuously affected by 10 times 
happy information in voice, the corresponding intensity is 
close to 1 ultimately while other states is near 0 relatively. 
Consequently, these two of change curves of emotion state 
probability distribution are used to describe the robot’s 
extreme personalities: optimism and pessimism, and to take 
as the reference personality models for analysis of the 
influence on different stimuli combination for personality 
model at following Section 4.3. 
 
4.3 Analysis of the Influence on Recognition Error for 
Personality Model 
As mentioned above, the reference personality models 
are established by an input sequence of given all happy 
stimuli (i.e. 1m = ) detected by emotional speech 
recognition. In fact, the recognition framework is built and 
tested by an acted emotional speech database, and it is no 
doubt that there is certain difference between natural and 
acted emotional speech. That is to say, it will have some 
mismatches of emotion classification when the recognition 
construction based on acted emotion speech is applied to 
real-life emotional speech detection in human-robot 
interaction. Consequently, the recognition error will cause 
 change rate of emotion probability even the number of 
recognition error is the same. 
 
4.4 Parameter Adjustment for the Compensation on 
Personality Model under the Influence of 
Recognition Error 
According to the discussion on recognition error in 
Section 4.3, not only total number of recognition errors but 
also the combination of recognition error happened in 
different order of discrete inputs will cause the emotion 
curves of personality model have variance. In order to 
maintain the output of the model (i.e. emotion curves), it is 
necessary to adjust model parameters to fit the reference 
emotion curves under the influence of recognition error. 
Therefore, the relationship between model parameters and 
recognition error happened in the number and order of 
discrete inputs is analyzed and discussed in this section. 
Figure 4.8 shows the block diagram of compensation for the 
personality model under the influence of recognition error. 
 
Designed 
Personality Trait
Microphone
Speech 
Y
N
Stimuli Matrix
N
HA
π
Parameter Θ & π*
Adjustment
Personality Modeling
Based on HMM
Match
Y
N
Speech Emotion
Recognition
 
Figure 4.8: Overall block diagram of compensation for the 
personality model under influence of recognition error. 
 
Firstly, in the worst case, all discrete inputs are 
recognition errors, and it is taken into consideration and is 
used to test whether the model under the influence of 
recognition error is improvable by adjusting model 
parameters. For parameter θ adjustment, several tests shows 
that decreasing and increasing θ is helpful separately in 
optimistic and pessimistic personality model. 
Figure 4.9(a) and Figure 4.9(b) shows that change 
curves of emotion state 1 and 3 in optimistic and pessimistic 
personality models with decreasing and increasing model 
parameter θ respectively. It can be seen that curves of 
optimistic personality model are similar with the desire 
curve when θ was equal to 6 and 7 while the curves of 
pessimistic one were distant from the desire curve even 
when θ is 1000. In other words, it can be roughly said that 
optimistic personality model under all the combination of 
recognition error in discrete inputs can be modified by 
decreasing parameter θ appropriately. For pessimistic one, it 
is needed further tests to draw a conclusion. 
 
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
p(
T)
Emotion State 1:Happiness
 
 
Desired Value
θ=12
θ=11
θ=10
θ=9
θ=8
θ=7
θ=6
θ=5
θ=4
θ=3
θ=2
θ=1
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
p(
T)
Emotion State 3:Sadness
 
 
Desired Value
θ=12
θ=18
θ=36
θ=90
θ=120
θ=500
θ=1000
(a) (b) 
Figure 4.9: Simple test on discrete inputs which are all 
recognition errors: (a) θ adjustment in optimistic personality 
model, (b) θ adjustment in pessimistic personality model. 
 
The other model parameter *π  is tested in the same 
condition described as above. In Figure 4.10, the horizontal 
axis is the difference between the maximum and minimum 
value in elements of *π , and the vertical axis is the 
difference value in sum of emotion state 1 and 3 between 
reference model and the model with inputs which are all 
recognition errors in Figure 4.10(a) and Figure 4.10(b) 
respectively. It can be found that both of curves intersected 
the zero line. That is to say, by adjusting appropriate *π , 
both of personality models can match its corresponding 
reference model. Thus, it also can be roughly said that 
personality models under all the combination of recognition 
error in discrete inputs can be modified by adjusting proper 
parameter *π . 
 
1/4 1.15/4 1.3/4 1.45/4 1.6/4 1.17/4
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
Emotion State 1:Happiness
Δp
1
Δπi
1/4 0.85/4 0.7/4 0.55/4 0.4/4 0.25/4
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
Emotion State 3:Sadness
Δp
3
Δπi
(a) (b) 
Figure 4.10: Simple test on discrete inputs which are all 
recognition errors: (a) π* adjustment in optimistic model, (b) 
π* adjustment in pessimistic model. 
 
In order to maintain the consistency of personality trait, 
here are further discussion and simulations about the 
relationship between model parameters and personality 
models under every each combination of recognition error in 
discrete inputs. Several values of model parameters are 
given to observe the change trend in each combination of 
recognition errors. 
For θ adjustment, Figure 4.11(a)-(i) shows the case of 
optimistic model with given values of parameter θ in order 
of number of recognition error in discrete inputs from 1 to 9. 
In Figure 4.11, the horizontal axis is the number of each 
combination of recognition errors, and the vertical axis is the 
difference value in sum of emotion state 1 between reference 
 4.5 Simulation Results 
As results of simulation mentioned above, parameter θ 
and *π  are respectively chosen for the compensation on the 
optimistic and pessimistic personality models under the 
influence of all combinations of recognition error. However, 
considering each combination of recognition errors has 
different influence on personality models, the adjustment of 
corresponding parameter differs from each other. To 
simplify the complexity of parameter adjustment, the 
acceptable range of error is defined by the difference value 
between -0.1 and 0.1. According to the error requirement, 
parameter adjustment for each number of recognition errors 
can be determined. 
 
4.5.1. Parameter θ Selection for Optimistic Model under 
Influence of Recognition Error 
Figure 4.14 shows the detail of parameter θ selection to fit 
in with error requirement in the optimistic personality model 
under influence of each number of recognition errors. In 
each number of recognition errors, several parameter θ are 
chosen to meet error requirement for each combination of 
recognition error. The selected parameter θ and the 
corresponding maximum difference value in each number of 
recognition were shown in Table 4.2. 
 
6 7 8 9 10 11
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
θ
S
um
 o
f Δ
p C
110
-C
010
 
 
first 3 at postion 10
at postion 9
at postion 8
at postion 7
at postion 6
at postion 5
at postion 4
at postion 3
at postion 2
at postion 1
6 7 8 9 10 11
-0.25
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
θ
S
um
 o
f Δ
p C
210
-C
010
6 7 8 9 10 11
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
θ
S
um
 o
f Δ
p C
310
-C
010
(a) (b) (c) 
6 7 8 9 10 11
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
θ
S
um
 o
f Δ
p C
410
-C
010
6 7 8 9 10 11
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
θ
S
um
 o
f Δ
p C
510
-C
010
6 7 8 9 10 11
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
θ
S
um
 o
f Δ
p C
610
-C
010
(d) (e) (f) 
6 7 8 9 10 11
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
θ
S
um
 o
f Δ
p C
710
-C
010
6 7 8 9 10 11
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
θ
S
um
 o
f Δ
p C
810
-C
010
6 7 8 9 10 11
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
θ
S
um
 o
f Δ
p C
1010
-C
010
(g) (h) (i) 
Figure 4.14: Parameter θ selection to meet error requirement 
in emotion state 1: happiness of optimistic personality model 
with given values of θ: 6, 7, 8, 9, 10, 11 under all 
combinations of recognition error in discrete inputs, and the 
number of recognition errors is respectively: (a) 1, (b) 2, (c) 
3, (d) 4, (e) 5, (f) 6, (g) 7, (h) 8, (i) 9. 
 
 
 
 
 
 
Table 4.2: Selected parameter θ and corresponding 
maximum difference value in optimistic personality model 
under influence of each number of recognition error. 
Number of 
recognition error Selected θ  Maximum difference value 
1 9 0.0529 
2 8 0.0909 
8 0.0893 3 7 0.0381 
8 0.0857 4 7 0.0558 
8 0.0781 5 7 0.0721 
8 0.0628 6 7 0.081 
7 7 0.0936 
8 7 0.088 
9 6 0.0942 
 
Eventually, Figure 4.15 shows the change curves of 
emotion state probability distribution which is represented 
as optimistic personality trait, and the dotted lines are the 
designed personality trait while the solid lines are the 
modified personality trait in correspondence with selected 
parameter θ seen in gray rows of Table 4.2 in each figure. 
Other modified personality traits in correspondence with 
selected parameter θ seen in blank rows of Table 4.2 are 
shown in Figure A.2. In Figure 4.15, it can be seen that each 
modified personality trait is close and similar to the designed 
personality trait within the error requirement. In other words, 
the optimistic personality model under influence of number 
of recognition error in discrete inputs of emotional speech is 
able to be nearly eliminated and modified by adjusting 
according parameter θ discussed above within the error 
requirement to maintain the consistency of personality trait. 
 
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C1
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C2
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C3
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
(a) (b) (c) 
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C4
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C5
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C6
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
(d) (e) (f) 
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C7
10:1
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C8
10:45
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
T
P
i(T
)
C9
10:2
 
 
Designed Happiness
Modified Happiness
Designed Anger
Modified Anger
Designed Sadness
Modified Sadness
(g) (h) (i) 
Figure 4.15: Comparison between the change curves of 
emotion state probability distribution in reference optimistic 
 emotion state probability distribution in reference 
pessimistic personality model and the model under influence 
of number of recognition errors. The number of recognition 
is respectively: (a) 1, (b) 2, (c) 3, (d) 4, (e) 5, (f) 6, (g) 7, (h) 
8, (i) 9. 
 
V. 結論以及未來展望 
本計畫的執行結合了語音情緒辨識技術以及基於
隱藏式馬可夫模型的情感模型，在此稱之為基於語音情
緒的個性模型。在語音情緒辨識方面，以Berlin語音情
緒資料庫中的語音樣本測試，經由典型的語音情緒辨識
流程以及其中用於語音情緒資料探勘的協助工具處理
後，辨識正確率幾乎可達到94%。除此之外，此辨識結
果也用來作為基於語音情緒的個性模型之模擬實驗中
的外界刺激輸入。在情感模型方面，在此使用基於隱藏
式馬可夫模型的情感模型，並且用此模型模擬在沒有外
界刺激時，內部情緒自我調整的動態過程；以及模擬受
到相同語音情緒辨識結果所造成之激刺時，其情緒轉移
的動態過程。除此之外，藉由調整適當的模型參數，在
此建立出樂觀以及悲觀的性格特性。隨後，在此也考量
並且討論基於語音情緒的個性模型，受到各種辨識錯誤
組合的影響程度。而為了補償辨識錯誤所造成的差異
性，在此也對各種辨識錯誤組合與模型參數的關聯性做
了探討與分析，其中，為了簡化其複雜度，在此特意地
將各種辨識錯誤組合分為數個群組，以方便討論及分
析。最後，在我們預設可接受的誤差值範圍之內，可以
取得每個群組其相對應的模型參數調整值 
在未來展望方面，希望進一步調整出合適的演算
法，以便更容易地且更準確地能找到最佳的模型參數組
合值，用來修正基於語音情緒的個性模型受到各種辨識
錯誤組合的影響。除此之外，也希望能夠將此模型應用
現實生活的測試中，以進一步評估此模型在人機互動過
程中的實用性以及功能性。在此，將特別地討論情緒在
人機互動情境中的功能性，分別針對辦公室、博物館以
及家庭等人機互動的情境，進一步地探討相關的特徵與
發展方向。 
在辦公室情境之中，為建構一個協助運送文件雜物
並且具備資料搜索功能的可移動式辦公室機器人，使其
在執行協助工作的同一時間裡，可以替眾多使用者提供
不同的服務。在此互動行為中，機器人需要具備情緒感
知的能力，則此機器人將會優先找尋並且提供服務於急
於需要協助的其他辦公室成員。除此之外，機器人本身
可以擁有特定的性格特性，例如：樂觀活潑的性格，則
其相對應的行為反應，或許能夠紓緩辦公室成員在辦公
壓力下的緊張氣氛。 
在博物館的情境之中，機器人可以提供博物館導覽
以及資訊服務；除了能夠提供極為富教育性內容的導覽
解說之外，並且也可以帶領參觀者到博物館內較受關注
的展覽作品位置，同時給予豐富的展覽物資訊解說。然
而目前的機器人與參觀者之互動行為，是已預先設定的
並且固定不變的。在此互動行為中，若將情緒功能應用
在此機器人之中，則能夠做出更為多變的行為反應，使
互動過程更為活潑且有趣，例如：導覽時的解說速度與
內容，或是不時穿插笑話等等。 
在家庭情境之中，機器人除了可以搜尋家中的人所
在位置並且與他們對話；而當家裡沒有人的時候，機器
人則能夠看守屋內，防範宵小入侵。雖然目前的機器人
具備與人交談溝通的能力，但是若要將機器人進而視為
家庭中的一份子，則機器人可能還缺少家人之間的相互
關懷與親切體貼心。在此互動行為中，若機器人夠察覺
到家人情緒的變化，並且表現出合適的行為反應，則機
器人與家人之間的關係將會更為融洽且更為親密。 
以上所述，乃是說明機器人除了本身感測與運動技
術的發展之外，尚須要內部感情或情緒上的發展，以便
於能夠融洽地與人類生活於日常環境之中。 
 
REFERENCES 
Book: 
[B1: Andersen & Guerrero 1998] 
P. A. Andersen and L. K. Guerrero, “Handbook of 
Communication and Emotion: Research, Theory, 
Applications, and Contexts,” First Edition, San Diego, 
London: Academic Press, 1998. 
[B2: Ortony et al. 1998] 
A. Ortony, G. Clore and A. Collins, “The Cognitive 
Structure of Emotions,” First Edition, Cambridge 
University Press, 1998. 
[B3: Picard 2000] 
R. W. Picard, “Affective Computing,” First Edition, 
The MIT Press, 2000. 
[B4: Mehrabian & Russell 1974] 
A. Mehrabian and J. A. Russell, “An Approach to 
Environmental Psychology,” First Edition, The MIT 
Press, 1974. 
[B5: R. Plutchik 1980] 
R. Plutchik, “Emotion: A Psychoevolutionary 
Synthesis,” First Edition, Harpercollins College Div, 
1980. 
 
Papers: 
[P1: Fragopanagos & Taylor 2005] 
N. Fragopanagos and J. Taylor, “Emotion Recognition 
in Human-Computer Interaction,” Neural Networks, 
Vol. 18, pp. 389-405, March 2005. 
[P2: Zeng et al. 2005] 
Z. Zeng, Z. Zhang, B. Pianfetti, J. Tu, and T. S. Huang, 
“Audio-Visual Affect Recognition in 
Activation-Evaluation Space,” in Proceedings of 
IEEE International Conference on Multimedia and 
Expo, pp. 828-831, July 6-8, 2005. 
[P3: Rong et al. 2007] 
J. Rong, Y. P. Chen, M. Chowdhury, and G. Li, 
“Acoustic Features Extraction for Emotion 
Recognition,” in Proceedings of the 6th IEEE/ACIS 
International Conference on Computer and 
 H. Hüttenrauch, A. Green, M. Norman, L. Oestreicher, 
and K. S. Eklundh, “Involving Users in the Design of a 
Mobile Office Robot,” IEEE Transactions on Systems, 
Man, and Cybernetics, Part C: Applications and 
Reviews, Vol. 34, No. 2, pp. 113 - 124, May 2004. 
[P24: Nourbakhsh et al. 1999]  
I. R. Nourbakhsh, J. Bobenage, S. Grange, R. Lutz, R. 
Meyer, and A. Soto, “An Affective Mobile Robot 
Educator with a Full-time Job,” Artificial. Intelligence, 
Vol. 114, No.1-2, pp. 95–124, October 1999. 
[P25: Kawachi et al. 2003]  
N. Kawachi, Y. Koyuu, K. Nagashima, K. Ohnishi, 
and R. Hiura, “Home-use Robot "wakamaru",” 
Mitsubishi Juko Giho., Vol. 40, no. 5, pp. 270-273, 
September 2003. 
 
Online Resource: 
[O1: Murphy 2005] 
Kevin Murphy, “Hidden Markov Model (HMM) 
Toolbox for Matlab,” [Online]. Available: 
http://www.cs.ubc.ca/~murphyk/Software/HMM/hm
m.html, June 8 2005. 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：連豊力 計畫編號：99-2221-E-002-213- 
計畫名稱：新世代辦公大樓之「智慧型服務機器人族」--子計畫六：機器人與人類工作環境共生模式
之分析與設計 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 1 1 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
