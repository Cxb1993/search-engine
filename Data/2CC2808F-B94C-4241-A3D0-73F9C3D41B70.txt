2investigate advanced workload management
issues, including job scheduling, processor
allocation, and load sharing methods.
Keywords: computing cluster, computing grid,
workload management, job
scheduling, processor allocation,
load sharing, simulation
environment
二、前言、研究目的與文獻探討
過去幾年來個人電腦叢集的發展已從
早期需要自行 DIY 建置，演變到今日的由
廠商推出完整的叢集產品系列，甚至也有
更高密度的刀鋒型伺服器(blade server)
架構的推出。而這期間亦有不少業界單位
開始大量使用計算叢集這樣的架構。但他
們馬上面臨到的就是整個平台上整合性的
工作負載管理問題，在這方面，即使有些
諸如 SGE[65], Condor[28], LSF[27]等工
具可以協助進行管理，但這些工具多只是
執行系統管理者設定好的既定政策，對於
設定政策前須先釐定的有關如何妥善安排
工作負載、規劃資源配置及慎選工作排程
方法等更高階但也更重要的問題卻缺乏適
當的參考指引。
最近幾年來，除了原本倡議格網計算的
學、研界外，格網計算這股熱潮也逐漸擴
散到產業界中，一些主要的國際資訊領導
廠商陸續釋放出許多推動格網計算應用及
推出支援格網平台軟體系統的相關訊息。
這表示格網計算正逐漸被業界認可將成為
未來一種極重要的計算平台。不過這一切
都還在起步階段，許多定義還不清楚，許
多問題有待釐清，許多的技術也還尚待進
一步的發展。
雖 然 這 幾 年 來 格 網 計 算 (grid
computing)不論在學界或業界都日漸受到
重視[43,51,55,56]，但這個名詞本身倒是
尚未有一嚴謹且獲得共識的明確定義，而
是處於一種比較像是概念的描述與表達的
階段。從應用的型態來看，格網計算至少
可歸納出三種可能的型態: 目前已常見之
計算格網、資料格網、以及將來應會逐漸
出現之服務格網，其中服務格網需奠基在
前二者的基礎架構之上。計算格網主要以
分享計算能力(即 CPU time)為主，是最早
開始的格網應用型態，而資料格網主要以
分享各地的數據資料為主要用途，這類型
的應用在高能物理、生物資訊、天文物理
等常產生大量觀測及實驗數據且經常由分
屬世界各地不同研究單位成員組成團隊進
行研究工作的領域更形重要。本計劃將著
重在計算格網平台上工作負載管理的應用
與研究。
若將計算格網上應用的目的再加以細
分，可以分成底下兩個目的: 提升計算資
源使用效率以提高整體計算工作的產出、
整合各處資源以解決大型計算問題。第一
個方向屬於 high throughput computing
領域的應用，藉由整合及妥善運用分散於
組織內部各處或不同組織的計算資源，可
在本地資源吃緊的情況下，找到計算格網
平台上其他地方閒置的計算資源，用來解
決眼前的計算需求，因此可有效提升計算
工作進行的速度及系統的整體產出。第二
個方向屬於 high performance computing
的應用領域，藉由結合計算格網平台上各
處的計算資源，以平行處理的方式共同解
決一無法在任何單一計算資源上獲得有效
處理的大型計算問題，這也是早期
metacomputing 主要的應用方向。本計劃
主要著重在第一類的 high throughput
computing 應用，這也是目前業界導入計
算格網平台時最首先的預期效益，不過因
本計劃中所探討的工作排程與資源配置方
法皆可以允許一個平行程式在有需要時跨
叢集執行，因此也可兼顧第二類 high
performance computing 方面的應用。
之前世界上較受矚目的計算格網發展
多是以串連各大超級電腦中心間之高速電
腦設備的實驗性平台為主，已成功地證實
了此一平台的可行性。但在另一方面，由
於近幾年來 PC效能的大幅進步以及 Linux
的出現，除了許多個人桌上原本的大型工
作站已逐漸汰換成工作站或伺服器等級的
PC 外，更促成了 PC cluster 平台的興起
與普及，PC cluster 除了在效能上直逼傳
統的大型平行電腦外，更具有傳統的大型
平行電腦所難以匹敵的優異效能/價格比
值，也因此近ㄧ兩年來世界前五百大電腦
的排行榜上[30]，PC cluster 的架構與日
4資源暫時不足的情況下，可以透過計
算格網的機制，適時地將部份工作
(workload)分散到其他單位的計算資
源去執行。也就是說確保每一個加入
計算格網平台的單位都能獲得整體效
能的提升，而不至於發生單純貢獻出
計算資源但在自己需要額外計算資源
時卻得不到其他單位奧援的窘境。如
此是確保每一個單位都願意加入且持
續留在計算格網內的基本前提。為了
滿足這個前提，工作排程與資源配置
的方法必須做適當的安排，也就是我
們所說具有公平性的 feasible load
sharing。
 Adaptive processor allocation。允許同
時使用橫跨不同單位的計算資源來進
行平行計算是解決所有單位之個別計
算資源皆不足以執行一平行程式狀況
的方法之ㄧ。但是使用橫跨不同單位
的計算資源來進行平行計算的ㄧ個潛
在問題是相對較小的跨單位網路頻寬
會降低該平行程式的執行效能，致使
所需的執行時間通常要比在單一單位
的計算資源上執行時來得長。另ㄧ種
可能的方法是，當遇到所有單位之個
別計算資源皆不足以執行一平行程式
的狀況時，不採用以橫跨不同單位的
計算資源來進行平行計算的方式，而
是利用大部分平行程式 modable 的特
性[27]，自動縮小其所使用的 CPU 數
目至單一單位所能允許的最大量，如
此不僅能讓此平行工作立刻獲得執
行，同時也因能在同ㄧ單位內進行平
行計算，不至於被相對較小的跨單位
網路頻寬所影響而降低執行效能。另
ㄧ方面，當一個平行程式僅需要較少
的 CPU 數目，但該單位的可使用 CPU
數目較此數目為多時，系統可以考慮
自動增加其使用的CPU數目來加速其
計算的進行。
底下是上述這幾個議題透過模擬分析
所得到的效能分析結果。我們使用公開的
實際工作負載記錄作為這一系列模擬中計
算格網平台上的工作負載，以對各式工作
負載分擔模式在實際被應用時可能的表現
進行客觀的效能分析。我們所採用的公開
工作負載記錄是美國聖地牙哥超級電腦中
心內 IBM SP2 平行電腦上的工作負載記
錄。這份工作負載記錄包含了從 1998 年 5
月一直到 2000 年 4 月間共 73496 筆在這部
擁有 128 個處理器之平行電腦上所被執行
過的工作記錄。在根據 completed 欄位摒除
了其中部分未執行完成或是有問題的工作
紀錄後，底下的模擬分析實際上使用了其
中的 56490 筆工作記錄作為模擬中計算格
網平台上的工作負載。
這批工作負載的詳細特徵如表一所
示。在這部 IBM SP2 的電腦上共有五個工
作貯列，而所有這五個貯列都把其中的工
作安排至同一個 128 顆處理器的群組中執
行，屬於多貯列單一處理器群組的架構。
在接下來的模擬分析中，我們假設這個計
算格網平台共由五個不同單位的叢集所組
成，而其上的動態工作負載就分別由表一
中五個貯列的工作負載來代表，不過所有
叢集上的工作都被交付到一個單一集中的
貯列中來等待執行，形成一個單一貯列多
重處理器群組的架構。表二詳列在接下來
的模擬中各個叢集所擁有的處理器數目。
為了模擬異質性計算格網平台的特性，我
們定義了ㄧ個 speed=(sp1,sp2,sp3,sp4,sp5)
向量來代表 5 個不同單位內 CPU 的相對速
度，同ㄧ個單位內的所有 CPU 假設為具有
相 同 的 計 算 速 度 。 另 ㄧ 個
load=(ld1,ld2,ld3,ld4,ld5)向量用來模擬不
同單位上的相對工作負載擁擠程度，方法
是將工作負載記錄中所記載的工作執行時
間乘上該單位的 ld 值作為在模擬實驗中的
實際工作執行時間。我們也定義了ㄧ個
threshold 值作為一個工作在挑選合適的執
行單位時用來過濾掉計算速度較慢單位的
基準，方法是只有計算速度大於等於該工
作原本所屬單位的計算速度乘上此
threshold 值的單位才會列入考量。
6Independent sites 9260 14216 10964 10199 6448 57
Ordinary load sharing policy 4135 191 4758 4799 3881 559
Feasible load sharing policy 4152 193 4750 4798 3939 57
表四: Comparison of load sharing policies with/without adaptive processor allocation.
Adaptive processor allocation
improves performance (%)
All
sites/FCFS
All
sites/SJF
Faster
sites/FCFS
Faster
sites/SJF
Power = 0 50 0 100 68.33
Power = 0.5 100 2.5 100 100
Power = 1 100 30 100 100
四、計畫成果自評
本年度計畫執行順利，產生不少有意義
的研究成果，並已將部份研究成果撰寫成學
術論文投稿至學術研討會議[32]。
五、參考文獻
[1] Sun ONE Grid Engine,
http://wwws.sun.com/software/gridware/
sge.html
[2] Condor, http://www.cs.wisc.edu/condor/
[3] LSF,
http://www.platform.com/Products/Platf
orm.LSF.Family/
[4] Foster, I., The GRID: Blueprint for a
New Computing Infrastructure, Morgan
Kaufmann Publishers, Inc., San
Francisco, California, USA, 1999.
[5] Foster, I. and Gannon, D., “The Open 
Grid Services Architecture Platform”, 
draft-ggf-ogsa-platform-2, Global Grid
Forum.
[6] Tuecke, S., Czajkowski, K., Foster, I.,
Frey, J., Graham, S., Kesselman, C.,
Maquire, T., Sandholm, T., Snelling, D.,
Vanderbilt, P., “Open Grid Services 
Infrastructure Version 1.0”, 
draft-ggf-ogsi-gridservice-29, Global
Grid Forum.
[7] http://www.gridforum.org/
[8] http://www.top500.org
[9] http://www.globus.org
[10]http://www.globus.org/research/testbeds.h
tml
[11] Taiwan Unigrid Project,
http://www.unigrid.org.tw
[12] Hong Kong Grid Initiative,
http://www.hkgrid.org
[13] 中 國 教 育 科 研 網 格 (China Grid),
http://www.chinagrid.edu.cn
[14] ShangHai Grid,
http://www.ssc.net.cn
[15]黃國展、陳俊佑、陳怡超、張西亞，”
計算格網中工作負載分擔效益之研
究”, 第三屆離島資訊技術與應用研討
會，民國92年06月27日，澎湖馬公。
[16]Ahmad, I., “Editorial: resource
management of parallel and distributed
systems with static scheduling:
chalenges, solutions, and new problem”, 
Concurrency—Pract. & Exp., 7(5), pp.
339-347, Aug. 1995.
[17] T. G. Lewis and H. El-Rewini,
Introduction to Parallel Computing,
Prentice-Hall International, Inc., 1992.
[18] Kleinrock, L., Queueing Systems, John
Wiley & Sons, Inc., 1976.
[19] El-Rewini, H., Lewis, T. G., Ali, H. H.,
Task Scheduling in Parallel and
Distributed Systems, Englewood Cliffs,
New Jersey: Prentice Hall, 1994.
[20] Hwang, J. J., Chow, Y. C., Anger, F. D.,
Lee, C. Y., “Scheduling Precedence 
Graphs in Systems with Interprocessor
出席國際學術會議心得報告
計畫編號 NSC 95-2221-E-432-004-
計畫名稱 建構一整合型模擬環境以進行計算叢集與格網上工作負載管理方法之研究
出國人員姓名
服務機關及職稱
黃國展/興國管理學院電子商務學系/助理教授
會議時間地點 May 2-4, 2007
會議名稱 International Conference on Grid and Pervasive Computing
發表論文題目 1. Towards Feasible and Effective Load Sharing in a HeterogeneousComputational Grid
一、參加會議經過
本年度的 GPC 2007 會議地點在法國巴黎市的 MGEN (Mutuelle Générale de
l'Education Nationale), 3 square Max Hymans 75748 PARIS CEDEX 15 舉行。會議於 5 月 2
日上午開始辦理報到註冊，2 日至 4 日三個整天為正式的會議期間, 內容包括邀請演講、
專題討論及論文發表等議程。
在 5 月 2 日開幕當天早上，大會安排了加拿大 St. Francis Xavier 大學的 Laurence T.
Yang 教授進行開幕演講。大會期間另外幾場邀請演講的講者分別是 5 月 3 日早上來自法
國的 Thierry Priol 博士(Scientific Coordinator of the European CoreGRID Network of
Excellence, INRIA Research Director)及 5 月 4 日早上來自日本 Aizu 大學 School of
Computer Science and Engineering 的郭明義教授，以及 5 月 4 日下午來自法國的 Franck
Cappello 博士(Reseach Director at INRIA-futurs (France), Grid'5000 Project co-Leader)。另外
在 5 月 2 日下午大會也安排了ㄧ場由 L. Brunie, M. Parashar 及 JM. Pierson 等三位講者所
共同主持的專題討論(Round Table)，主題是 Pervasive Grids。
本次會議內容豐富, 由於 Grid and Pervasive Computing 涵蓋內容廣泛, 參與會議人
數眾多, 論文發表場次共計 19 場, 有些場次於同一時段在不同地點同時舉行，每個場次
時間長短不一定相同，少則有 3 篇論文,多則 5 篇，每篇論文約有 20 餘分鐘的報告時間。
本次大會各場次主要可分為底下幾個主題大類:
 Pervasive Computing
 Cluster / Grid Computing
 HPC / Applications / Algorithms
 Pervasive and Ubiquitous Computing
 Web Semantics and Services, Applications, Algorithms
 Applications

and effective computational grid, appropriate load sharing policies are important. The load sharing policies have to take
into account several job scheduling and processor allocation issues. These issues are discussed in this paper, including
job scheduling for feasible load sharing benefiting all sites, site selection for processor allocation, multi-site parallel
execution. Several job scheduling and processor allocation policies are proposed and evaluated through a series of
simulations using workloads derived from publicly available trace data. The simulation results indicate that a significant
performance improvement in terms of shorter job response time is achievable.
2. Related Work
Job scheduling for parallel computers has been subject to research for a long time. As for grid computing, previous
works discussed several strategies for a grid scheduler. One approach is the modification of traditional list scheduling
strategies for usage on grid [1, 2, 3, 4]. Some economic based methods are also being discussed [5, 6, 7, 8]. In this paper
we explore non economic scheduling and allocation policies with support for a heterogeneous grid environment.
England and Weissman in [9] analyzed the costs and benefits of load sharing of parallel jobs in the
computational grid. Experiments were performed for both homogeneous and heterogeneous grids.
However, in their works simulations of a heterogeneous grid only captured the differences in
capacities and workload characteristics. The computing speeds of nodes on different sites are
assumed to be identical. In this paper we deal with load sharing issues regarding heterogeneous
grids in which nodes on different sites may have different computing speeds.
For load sharing there are several methods possible for selecting which site to allocate a job. Earlier
simulation studies in our previous work [10] and in the literature [1] showed the best results for a
selection policy called best-fit. In this policy a particular site is chosen on which a job will leave the
least number of free processors if it is allocated to that site. However, these simulation studies are
performed based on a computational grid model in which nodes on different sites all run at the same
speed. In this paper we explore possible site selection policies for a heterogeneous computational
grid. In such a heterogeneous environment nodes on different sites may run at different speeds.
In [11] the authors addressed the scheduling of parallel jobs in a heterogeneous multi-site
environment. They also evaluated a scheduling strategy that uses multiple simultaneous requests.
However, although dealing with a multi-site environment, the parallel jobs in their studies were not
allowed for multi-site parallel execution. Each job was allocated to run within a single site.
The support of multi-site parallel execution [12, 13, 14, 15, 16] on a computational grid has been
examined in previous works, concerning the execution of a job in parallel at different sites. Under
the condition of a limited communication overhead, the results from our previous work [10] and
from [1, 3, 4] all showed that multi-site parallel execution can improve the overall average response
time. The overhead for multi-site parallel execution mainly results from the slower communication
between different sites compared to the intra-site communication. This overhead has been modeled
by extending the execution time of a job by a certain percentage [2, 3, 10].
In [2] the authors further examined the multi-site scheduling behavior by applying constraints for
the job fragmentation during the multi-site scheduling. Two parameters were introduced for the
scheduling process. The first parameter lower bound restricted the jobs that can be fragmented
job is submitted. The local site which a job is submitted from will be called the home site of the job
henceforward in this paper. We assume the ability of jobs to run in multi-site mode. That means a
job can run in parallel on a node set distributed over different sites when no single site can provide
enough free processors for it due to a portion of resources are occupied by some running jobs.
Our simulation studies were based on publicly downloadable workload traces [20]. We used the SDSC’s SP2
workload logs1 on [20] as the input workload in the simulations. The workload log on SDSC’s SP2 contains 73496
records collected on a 128-node IBM SP2 machine at San Diego Supercomputer Center (SDSC) from May 1998 to
April 2000. After excluding some problematic records based on the completed field [20] in the log, the simulations in
this paper use 56490 job records as the input workload. The detailed workload characteristics are shown in Table 1.
Table 1. Characteristics of the workload log on SDSC’s SP2
Number
of jobs
Maximu
m
execution
time
(sec.)
Average
execution
time
(sec.)
Maximu
m
number
of
processor
s
per job
Average
number
of
processor
s
per job
Queue 1 4053 21922 267.13 8 3
Queue 2 6795 64411 6746.27 128 16
Queue 3 26067 118561 5657.81 128 12
Queue 4 19398 64817 5935.92 128 6
Queue 5 177 42262 462.46 50 4
Total 56490
In the SDSC’s SP2 system the jobs in this log are put into five different queues and all these queues
share the same 128 processors on the system. In the following simulations this workload log will be
used to model the workload on a computational grid consisting of five different sites whose
workloads correspond to the jobs submitted to the five queues respectively. Table 2 shows the
configuration of the computational grid under study. The number of processors on each site is
determined according to the maximum number of required processors of the jobs belonged to the
corresponding queue for that site.
Table 2. Configuration of the computational grid.
total site 1 site 2 site 3 site 4 site 5
Number of
processors
442 8 128 128 128 50
To simulate the speed difference among participating sites we define a speed vector,
speed=(sp1,sp2,sp3,sp4,sp5), to describe the relative computing speeds of all the five sites in the
grid, in which the value 1 represents the computing speed resulting in the job execution time in the
original workload log. We also define a load vector, load=(ld1,ld2,ld3,ld4,ld5), which is used to
derive different loading levels from the original workload data by multiplying the load value ldi to
1 The JOBLOG data is Copyright 2000 The Regents of the University of California All Rights Reserved.
sometimes resulting in even worse performance than the original independent-site architecture.
To deal with the site selection issue in a heterogeneous grid, we first propose a two-phase procedure.
At the first phase the grid scheduler determines a set of candidate sites among all the sites with
enough free processors for a specific job under consideration by filtering out some sites according
to a predefined threshold ratio of computing speed. In the filtering process, a lower bound for
computing speed is first determined through multiplying the predefined threshold ratio by the
computing speed of a single processor on the job’s home site, and then any sites with
single-processor speed slower than the lower bound are filtered out. Therefore, adjusting the
threshold ratio is an effective way in controlling the outcomes of site selection. When setting the
threshold ratio to 1 the grid scheduler will only allocate jobs to sites with single-processor speed
equal to or faster than their home sites. On the other hand, with the threshold ratio set to zero, all
sites with enough free processors are qualified candidates for a job’s allocation. Raising the
threshold ratio would prevent allocating a job to a site that is much slower than its home site. This
could ensure a job’s execution time would not be increased too much due to being allocated to a
slow site. However, for the same reason a job may consequently need to wait in the queue for a
longer time period. On the other hand, lowering the threshold ratio would make it more probable for
a job to get allocation quickly at the cost of extended execution time. The combined effects of
shortened waiting time and extended execution time are complicated for analysis. At the second
phase the grid scheduler adopts a site selection policy to choose an appropriate site from the
candidate sites for allocating the job.
Figure 1 compares the performances of two different values, 0 and 1, for the threshold ratio. The
results indicate that when the speed difference among sites is large, speed=(0.6, 0.7, 2.4, 9.5, 4.3),
setting the threshold ratio to 1 can enable the best-fit policy to make performance improvement in a
heterogeneous computational grid compared to the independent-site architecture.
Fig. 1. Performance of best-fit policy with large speed difference among participating sites.
Another possible policy for the second phase of the site selection process is called the fastest one.
The fastest-one policy chooses the site with the fastest computing speed among all the sites with
enough free processors for a job without consideration of the difference between the number of
required processors and a site’s free capacity. To deal with the difficulty in determination of an
appropriate site selection policy, in this section we propose an adaptive policy, which dynamically
changes between the best-fit and the fastest-one policies, trying to make a better choice at each site
Table 3 evaluates the effects of the feasible load sharing policy in a heterogeneous computational
grid with speed=(1, 3, 4, 4, 8) and load=(5, 4, 5, 4, 1). The NJF scheduling policy and the
fastest-one site selection policy are used in the simulations with the computing speed threshold ratio
set to one, ensuring jobs won’t be allocated to the sites slower than their home sites. Table 3 shows
that with the ordinary load sharing policy site 5 got degraded performance after joining the grid,
which may contradict its original expectation. On the other hand, our proposed policy is shown to
be able to achieve a somewhat more feasible and acceptable load sharing result in the sense that no
sites’performances were sacrificed.
Table 3. Average job response times (sec.) for different load sharing policies.
Entire
grid
Site 1 Site 2 Site 3 Site 4 Site 5
Independent
sites
9260 14216 10964 10199 6448 57
Ordinary
load sharing
policy
4135 191 4758 4799 3881 559
Feasible
load sharing
policy
4152 193 4750 4798 3939 57
6. Multi-Site Parallel Execution in a Heterogeneous Grid
In the load sharing policies described in the previous sections, different sites in the computational
grid are viewed as independent processor pools. Each job can only be allocated to exactly one of
these sites. However, one drawback of this multi-pool processor allocation is the very likely internal
fragmentation [4] where no pools individually can provide enough resources for a certain job but
the job could get enough resources to run if it can simultaneously use more than one pool’s
resources.
Multi-site parallel execution is traditionally regarded as a mechanism to enable the execution of
such jobs requiring large parallelisms that exceed the capacity of any single site. This is a major
application area in grid computing called distributed supercomputing [21]. However, multi-site
parallel execution could be also beneficial for another application area in grid computing: high
throughput computing [21]. In our high throughput computing model in this paper, each job’s
parallelism is bound by the total capacity of its home site. That means multi-site parallel execution
is not inherently necessary for these jobs. However, for high throughput computing a computational
grid is used in the space-sharing manner. It is therefore not unusual that upon a job’s submission its
requested number of processors is not available from any single site due to the occupation of a
portion of system resources by some concurrently running jobs. In such a situation, splitting the job
up into multi-site parallel execution is promising in shortening the response time of the job through
reducing its waiting time. However, in multi-site parallel execution the impact of bandwidth and
latency has to be considered as wide area networks are involved. In this paper we summarize the
overhead caused by communication and data migration as an increase of the job’s runtime [2, 10].
of scheduling and allocation policies largely depends on the actual grid configuration and workload. The improvements
presented in this paper were achieved using example configurations and workloads derived from real traces. The
outcome may vary in other configurations and workloads. However, the results show that the proposed policies are
capable of significantly improving the overall system performance in terms of average response time for user jobs.
Acknowledgement
The work of this paper is partially supported by National Science Council and National Center for High-Performance
Computing under NSC 94-2218-E-007-057, NSC 94-2213-E-432-001 and NCHC-KING_010200 respectively.
References
[1] V. Hamscher, U. Schwiegelshohn, A. Streit, and R. Yahyapour, “Evaluation of Job-Scheduling Strategies for Grid
Computing”, Proceedings of the 7th International Conference on High Performance Computing, HiPC-2000, pp. 191-202,
Bangalore, India, 2000.
[2] C. Ernemann, V. Hamscher, R. Yahyapour, and A. Streit, “Enhanced Algorithms for Multi-Site Scheduling”, Proceedings of
3rd International Workshop Grid 2002, in conjunction with Supercomputing 2002, pp. 219-231, Baltimore, MD, USA,
November 2002.
[3] C. Ernemann, V. Hamscher, U. Schwiegelshohn, A. Streit, R. Yahyapour,“On Advantages of Grid Computing for Paralel Job 
Scheduling”, Proceedings of 2nd IEEE International Symposium on Cluster Computing and the Grid (CC-GRID 2002), pp.
39-46, Berlin, Germany, 2002.
[4] C. Ernemann, V. Hamscher, A. Streit, R. Yahyapour, “"On Efects of Machine Configurations on Paralel Job Scheduling in 
Computational Grids", Proceedings of International Conference on Architecture of Computing Systems, ARCS 2002, pp.
169-179, 2002.
[5] R. Buyya, D. Abramson, J. Giddy, H. Stockinger, “Economic Models for Resource Management and Scheduling in Grid
Computing”, Special Issue on Grid Computing Environments, The Journal of Concurrency and Computation: Practice and
Experience(CCPE), May 2002.
[6] R. Buyya, J. Giddy, D. Abramson, “An Evaluation of Economy-Based Resource Trading and Scheduling on Computational
Power Grids for Parameter Sweep Applications”, Proceedings of the Second Workshop on Active Middleware Services
(AMS2000), In conjunction with the Ninth IEEE International Symposium on High Performance Distributed Computing
(HPDC 2000), Pittsburgh, USA, August 2000.
[7] Y. Zhu, J. Han, Y. Liu, L. M. Ni, C. Hu, J. Huai, “TruGrid: A Self-sustaining Trustworthy Grid”, Proceedings of the First
International Workshop on Mobility in Peer-to-Peer Systems (MPPS) (ICDCSW'05), pp. 815-821, June 2005.
[8] C. Ernemann, V. Hamscher, R. Yahyapour, “Economic Scheduling in Grid Computing”, the 8th International Workshop on
Job Scheduling Strategies for Parallel Processing, Lecture Notes In Computer Science; Vol. 2537, pp. 128-152, 2002.
[9] D. England and J. B. Weissman, “Costs and Benefits of Load Sharing in Computational Grid”, 10th Workshop on Job
Scheduling Strategies for Parallel Processing, Lecture Notes In Computer Science, Vol. 3277, June 2004.
[10] K. C. Huang and H. Y. Chang,“An Integrated Processor Allocation and Job Scheduling Approach to Workload Management
on Computing Grid”, Proceedings of the 2006 International Conference on Parallel and Distributed Processing Techniques
and Applications (PDPTA'06), pp. 703-709, Las Vegas, USA, June 26-29, 2006.
