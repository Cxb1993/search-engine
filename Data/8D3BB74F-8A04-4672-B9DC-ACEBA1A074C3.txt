的計算與檔案儲存需求。 
為降低 PACS 系統之擴建成本與設置第二 PACS 系統為考量，本計畫為期 3 年，主要
是研發 Smart Broker Centric 與具有自適性副本管理元件於於協同配置(Co-allocation)資料
網格環境中。藉由導入 Open Source PACS 解決方案，建置於特別設計的網格功能元件之
上，期能證實以網格技術來支援 PACS 系統的可行性，除保有 PACS 的優勢、實際提昇 PACS
影像副本交換效率與一個成本較低廉的 PACS 導入方案。 
於上年度，計畫完成平台架構上所有系統元件的雛型設計開發，平台架構區分
Application、Smart Broker、Cyber Abstraction、Grid Middleware 以及 Fabric 等 5 個階層，
其中較高層以使用者為中心，並以 Smart Broker 作為本架構核心；底層專注於資源整合，
並以 Cyber Abstraction 描述高低二階層之間如何連結。另外設計一個網格知識管理(GKM)
模型以支援各元件計算的參數資料輸入。其成果已在 APSCC08, IPCADS08 與 JNCA 發
表。 
本年度，計畫完成整合各階層元件，使用 Globus Tookit 與雲端計算(Cloud Computing)
結合 Cross-CA 技術，建構跨網格系統的交互驗證機制，提昇計算與儲存資源利用率。實
作 Co-Allocator 元件，該元件能經由 Resource Management System (RMS)提供對應資源，
並透過 GKM 的知識基礎，設計出具有認識網格環境現況的能力，即為自適性。在資料網
格環境中，資料集被重複製為複本且分送到多重的站台。由於資料集的檔案通常很大，如
何有效率的存取及傳輸成為重大的課題。因此先前有學者發展出協同配置的架構，使得同
時從多重站台平行下載資料變成可能，目前發展出數種協同配置法用來解決傳輸時本地端
與伺服端網路傳輸率不斷變動的問題。本研究中，我們採用 TCP 頻寬估計模型與突發模
式等新策略，藉此強化預測性遞迴調整的協同配置法，進一步提高大量資料集於資料網格
中的傳輸效能。我們的方法能有效地找出一群快速伺服器並分配較多的工作量提高其資源
利用率，動態計算出檔案切割量，有效減少各伺服器間的相互等待時間。藉由各項實驗證
明其傳輸的高效能，並具有網路自適應性與高度容錯性，有效因應不同的網格環境。使用
Open Source PACS 系統為應用實例，進行實驗及測詴雛型系統功能及相容性。其成果已在
Journal of Supercomputing 與 CCPE 發表。 
接下來最後一年，我們根據應用實例的實驗結果，進一步改善各階層元件與雛型應用
系統與 GKM 學習樣本調較。並就我們的 PACS 系統實際使用情況與真實醫院 PACS 系統
作整體效能比較。 
 
關鍵詞：網格計算、資料網格、醫療格網、醫療影像儲傳系統、協同配置 
 
English Abstract 
 
PACS (Picture Archiving and Communication System) is a system for archiving, retrieving, 
communicating and displaying medical images. The purpose of PACS is to acquire medical 
images from medical systems, store them in digital formats, and transmit them to remote users 
through networks for diagnostic usages. Furthermore, PACS can be sharing platforms for 
various images. As the development of software and computing technologies, PACS is 
promising to assist doctors in medical diagnoses, instruction and researches. The success of 
PACS depends on not only powerful hardware, but also advanced software utilities and 
operating procedures. By means of the developed grid computing technologies, resources of 
virtual organizations located in different places can be managed and dispatched. Moreover, the 
對於參與本子計畫研究之人員將能對，健康服務格網，醫療資料格網，及 HL7 資料
管理這些重要的技術有更深刻的認識，包含其歷史背景、發展過程、時空環境、遭遇到的
問題與未來發展前景都能有通盤的學習與了解。 
對於參與本子計畫研究之人員將能對服務格網資源的擷取有更多的練習，包括服務格
網資訊監控系統技術應用、RRDTool 技術應用、JRobin 圖表繪製，同時為了將資源提供給
其他子計畫之模組來使用，如何把資料整合並有效利用也將會是學習的重點。包含其歷史
背景、系統發展過程、系統整合測詴與技術應用、遭遇到的問題與未來發展前景都能有通
盤的學習與了解。 
參與人員可以學習到結合不同理論建構問題以及獨立思考能力，在實驗的過程，參與
者可學習到判斷研究成果的正確性以及回饋修正的能力。另外，也可以學習整套研究的方
法，團隊工作的精神，以及撰寫科技論文的經驗。也可以建立觀摩網站供有志從事高效能
醫療影像儲傳系統工作的朋友學習，規劃未來高效能醫療影像儲傳系統計畫的方向。 
由於本計畫為整合資料網格環境與醫療影像儲傳系統，對於參與本計畫研究之人員，
更能訓練橫向整合溝通的能力，與不同團隊之間的協調合作，是一個非常好的訓練。對於
參與研究的學生，不論是作業系統、分散式計算、平行分散式資料庫、PACS、或格網計
算都能有更深一層瞭解。另外，諸如網路與系統安全、網路與系統管理、邏輯分析能力，
如數據的分析技巧及格網計算領域技術問題的處理能力都可以獲得一連貫的訓練。而對碩
士班學生而言，除了核心理論的開發以外，更可以學習到系統整合與系統分析的經驗。 
 
 
技術研發成果說明： 
本計畫將依照研究內容與目標畫分三年進行。第一年計畫完成平台架構上所有系統元
件的雛型設計開發，平台架構區分 Application、Smart Broker、Cyber Abstraction、Grid 
Middleware 以及 Fabric 等 5 個階層，其中較高層以使用者為中心，並以 Smart Broker 作為
本架構核心；底層專注於資源整合，並以 Cyber Abstraction 描述高低二階層之間如何連結。
另外設計一個網格知識管理(GKM)模型以支援各元件計算的參數資料輸入，例如嵌入隱藏
式馬可夫模型(Hidden Markov Model, HMM)與決策樹(Diction Tree)的 Know-How、Grid 知
識模型(GKM)及累積的 System Log(如副本運作及配置情形)，以設計學習型知識資料庫。 
第二年，計畫整合各階層元件，使用 Globus Tookit 與 Cloud Computing 結合 Cross-CA
技術，建構跨網格系統的交互驗證機制，提昇計算與儲存資源利用率。實作 Co-Allocator
元件，該元件能經由 Resource Management System(RMS)提供對應資源，並透過 GKM 的
知識基礎，設計出具有認識網格環境現況的能力，即為自適性。在資料網格環境中，資料
集被重複製為複本且分送到多重的站台。由於資料集的檔案通常很大，如何有效率的存取
及傳輸成為重大的課題。因此先前有學者發展出協同配置的架構，使得同時從多重站台平
行下載資料變成可能，目前發展出數種協同配置法用來解決傳輸時本地端與伺服端網路傳
輸率不斷變動的問題。本研究中，我們採用 TCP 頻寬估計模型與突發模式等新策略，藉
此強化預測性遞迴調整的協同配置法，進一步提高大量資料集於資料網格中的傳輸效能。
我們的方法能有效地找出一群快速伺服器並分配較多的工作量提高其資源利用率，動態計
算出檔案切割量，有效減少各伺服器間的相互等待時間。藉由各項實驗證明其傳輸的高效
能，並具有網路自適應性與高度容錯性，有效因應不同的網格環境。為了量測效率，另外
建立客戶端應用程式以監控管理 Workflow，並且能將工作歷程與結果回饋給 GKM。而本
計畫「具適應性」的各演算法，即可依據此學習型知識資料庫不斷累積 Grid 領域知識並
本計畫若能承蒙貴會支持，透過計畫的功能規畫與研發，建立適合醫療院所之協同配
置資料網格環境中具適應性複本管理的高效能醫療影像儲傳系統平台、相關服務元件及計
算環境、PACS 裝置技術，有助於未來我國面臨高齡化人口所需的健康照護基礎環境建設。
同時透過自由軟體技術及平台對國內相關產業能提供相關技術及支援，對於提昇我國資訊
產業在健康照護方面的競爭力，將有相當助益。 
 
 
可利用之產業及可開發之產品： 
推廣及運用的價值：如增加產值、增加附加價值或營利、增加投資/設廠、增
加就業人數………等。 
2008 年國際情勢的變化，美國的二次房貸金融等危機造成產業蕭條，造成 1929 年以
來全世界經濟大蕭條與通縮的危機。有鑑於此；迎合下一波經濟回升與產品競爭力的考量
之下，勢必在研發設計的創新上累積具競爭力之產品，使企業在逆境中化為轉機的具體作
法與籌碼。居家環境是每個人最熟悉的空間，也是停留時間最長的場所，建構於居家環境
下的健康監測系統能提供長期、持續性的健康監測資料。「遠距居家照護(Tele-homecare)」
研究便是希望結合資訊與通訊科技，使能在被照護者的家中便利、有效地提供個人健康管
理與保健服務，並具有價格效能優勢及可延伸性。亞盾科技的目標是提供最先進的技術，
期盼能強化國家整體發展，不論在學術研究、軍事國防、產業升級，並協助客戶快速提昇
國際競爭力。 
遏止全經濟蕭條與產品競爭力的頹勢能逆勢成長；本計畫提出「遠距居家健康照護監
測系統之設計開發」的設計開發規劃，廠商在現有技術資源與能力上期能透過專家學者的
診斷以解決問題：使用者在家中量測體溫、血壓等生理訊號，經過電話線、有線電視、或
網際網路傳送到「居家照護服務提供者(home healthcare provider)」的資料庫儲存，並作
進一步的資料管理與分析。如發現生理訊號有異常，居家照護服務提供者可轉介使用者至
醫療單位作進一步診治，醫療單位也可讀取居家照護服務提供者的長期監測資料作為診斷
參考。 
悉知東海大學楊朝棟教授於高性能計算與網格計算之研究，在學界為其中之翹楚，為
使得博盛數碼動力公司得以提供客戶在高效能計算與網格計算領域中巨量運算的解決方
案，並滿足周邊相關使用與管理者的需求下，與楊教授合作，期盼能在教授的協助下，將
技術轉移到產業界，並實際應用在各種不同的高效能叢集計算與網格計算領域中。 
本計畫之合作企業之合作目的不僅是在培養人才，更重要的是 PACS 與 Grid 相關研
究技術能量的累積。該合作企業已經與本系執行有過五年小產學之經驗，已培養出多位具
Cluster Computing Systems 與 Grid Computing Environments 相關經驗的人才。期望由此次
開發型小產學之經驗，能進入醫療影像儲存系統技術領域，後續將朝醫療網格與居家服務
網格相關產業技術發展。 
每年配合款應達當年度計畫總經費 30％以上。合作廠商派二位研發人員參與本計畫。
企業得與計畫執行機構協商繳交先期技轉金，額度不得低於計畫總經費之15％(7年授權)。
產學計畫結束後 3 個月內，計畫執行機構應向本會及企業繳交精簡報告及完整結案報告電
子檔。 
 
Int. J. Ad Hoc and Ubiquitous Computing, Vol. 5, No. 4, 2010 235 
Copyright © 2010 Inderscience Enterprises Ltd.
A heuristic QoS measurement with domain-based 
network information model for grid computing 
environments
Chao-Tung Yang*, Chih-Hao Lin
and Ming-Feng Yang 
Department of Computer Science,  
Tunghai University,  
Taichung 40704, Taiwan, ROC 
E-mail: ctyang@thu.edu.tw 
E-mail: ljerome86@gmail.com E-mail: orsonyang@gmail.com 
*Corresponding author 
Wen-Chung Chiang 
Department of Information and Networking Technology,  
Hsiuping Institute of Technology,
Taichung 41280, Taiwan, ROC 
E-mail: wcchiang@mail.hit.edu.tw 
Abstract: Recently, Grid computing is more and more widespread. Therefore, there exists  
a common issue, i.e., how to manage and monitor numerous resources of grid computing 
environments. Mostly, we use Ganglia and Network Weather Service (NWS) to monitor 
machines’ status and network-related information, respectively. But, information provided  
by Ganglia and NWS is not sufficient in some scenarios owing to varied user requirements. 
Therefore, we propose a heuristic Quality of Service (QoS) measurement constructed with 
domain-based information model that provides more effective information to meet user 
requirements. Furthermore, we expect that users could manage and monitor numerous resources 
of grid environments more effectively and efficiently. 
Keywords: grid computing; heuristic; QoS; quality of service; network information model. 
Reference to this paper should be made as follows: Yang, C-T., Lin, C-H., Yang, M-F. and 
Chiang, W-C. (2010) ‘A heuristic QoS measurement with domain-based network information 
model for grid computing environments’, Int. J. Ad Hoc and Ubiquitous Computing, Vol. 5,  
No. 4, pp.235–243. 
Biographical notes: Chao-Tung Yang received his BS in Computer Science from Tunghai 
University, Taichung, Taiwan, in 1990, and the MS in Computer Science from National Chiao 
Tung University, Hsinchu, Taiwan, in 1992. He received the PhD in Computer Science from 
National Chiao Tung University in July 1996. He is a Professor of Computer Science at Tunghai 
University in Taichung, Taiwan. He got the excellent research award by Tunghai University in 
2007. In 2007 and 2008, he got the Golden Penguin Award by Industrial Development Bureau, 
Ministry of Economic Affairs, Taiwan. His present research interests are in grid and cluster 
computing, parallel and multi-core computing, and web-based applications. 
Chih-Hao Lin received his BS in Computer Science from Feng Chia University in 1999,  
and his MS in Computer Science from the Tunghai University, Taiwan, in July 2009.  
His research interests include grid computing, cloud computing, and parallel computing. 
Ming-Feng Yang received his BS in Hsiuping Institute of Technology in 2004. He received his 
MS in Department of Computer Science from Tunghai University in July 2009. He is a software 
engineer at Hsiuping Institute of Technology. His research interests include grid computing, 
cloud computing, and parallel computing. 
Wen-Chung Chiang received his BS from the Department of Applied Mathematics in 1991, and 
his PhD from the Department of Applied Mathematics of the National Chung-Hsing University 
in 2002. He is an Assistant Professor of Department of Information and Networking Technology 
of Hsiuping Institute of Technology. His current research interests include grid computing, 
medical image processing and communication networks. 
A heuristic QoS measurement with domain-based network information model 237
Figure 3 Multi-grid has integrated clusters and grids 
environments into a single Ganglia web portal
(see online version for colours) 
As shown in Figure 3, we could oversee several clusters  
or grids environments via web portal provided by Ganglia. 
2.2 Network information provider 
The NWS (http://nws.cs.ucsb.edu/ewiki/) (Wolski et al., 
1999) is a distributed system that detects computational 
resource and network status by periodic monitors and 
dynamic forecasts over a given time interval. The service 
operates a distributed set of performance sensors (network 
monitors, CPU monitors, etc.) from which it gathers system 
condition information. It then uses numerical models to 
generate forecasts of what the conditions will be for a given 
time period. The NWS system includes sensors for  
end-to-end TCP/IP performance (bandwidth and latency), 
available CPU percentage and available non-paged memory. 
The sensor interface, however, allows new internal sensors 
to be configured into the system. Some functions provided 
by NWS have overlapped with Ganglia; therefore, we 
primarily use NWS for end-to-end TCP/IP measurements. 
As Wolski et al. (1999) mentioned, NWS was designed 
to maximise four possible conflicting functional 
characteristics. It must meet these goals despite the highly 
dynamic execution environment and evolving software 
infrastructure provided by shared meta-computing systems. 
• predictive accuracy 
• non-intrusiveness 
• execution longevity 
• ubiquity. 
So, we choose NWS as primary tool for end-to-end TCP/IP 
measurements and we have excellent work in previous 
project. Except Ganglia, we also successfully deployed 
NWS service into our clusters and grids environments and 
then monitor network status via integrated web portal  
(as shown in Figures 4 and 5). 
Figure 4 NWS service integrated with Ganglia web portal  
(see online version for colours) 
Figure 5 Network statistics produced by NWS measurements 
demonstrated in web portal (see online version for 
colours)
2.3 Quality of Service 
Quality of Service is the ability to provide different service 
priority to different applications, users, or data flows, or to 
guarantee a certain level of performance to a data flow.  
It was widespread adopted in the field of computing 
networking, and we use it as a quality measurement of grid 
environments. 
Quality of Service sometimes refers to the level  
of QoS, i.e., the guaranteed service quality. High QoS  
is an expectable crucial factor of highly reliable and  
high-performance grid environments. 
Some characteristics, like ‘Availability’, ‘Accessibility’ 
or ‘Maintainability’, will also influence user experiences 
about the services provided by our system or services.  
To meet user requirements in diverse scenarios with 
sufficient quality, we are expected to have the ability to 
evaluate our performance in advance or in real-time manner. 
If not, how could we guarantee a certain level of QoS? 
Some researchers have proposed network performance  
A heuristic QoS measurement with domain-based network information model 239
self-optimisation. To guarantee a degree of QoS, we regard 
user requirements as constraints of tasks. With these 
constraints and heuristic QoS measurements we proposed  
in this paper, we could provide more QoS to meet user 
requirements. 
3.1 Domain-based network information model 
In this paper, we adopt Domain-based Network Information 
Model (Yang et al., 2005, 2007a, 2007b) for NWS services 
deployment. The Domain-based Network Information 
Model is designed for solving a complete point-to-point 
bandwidth measurement problem. After investigating  
by experiments in physical environments, we can be sure 
that Domain-based Network Information Model is helpful 
for reducing network measurements. The measurement 
model and design of Domain-based Network Information 
Model are shown as Figures 7 and 8. 
Figure 7 The domain-based network measurement model 
Figure 8 The design of domain-based network information 
model (see online version for colours) 
For example, assume a Grid with n nodes. Each node 
measures the links between itself and all other nodes every 
T seconds (e.g., T = 1~3 s) for a total of NMN (n) network 
measurements. 
NMN( ) ( 1).n n n= × −  (1) 
In large-scale Grid environments, the number of network 
measurements grows quickly. UniGrid and TigerGrid with, 
respectively, 96 and 46 nodes generate NMN(96) = 9120 
and NMN(46) = 2070 measurements. Thus, network traffic 
will be very heavy, particularly when underlying Grid  
intra-traffic is originally busy. 
Our previous work (Yang et al., 2007c) used the 
domain-based network information model shown in  
Figure 9. Figure 10 shows four sites, each containing  
four nodes. The sites each have a head node, e.g., A1, B1, 
C1 and D1, are, respectively, the head nodes of sites  
A, B, C and D. Each head node in this model periodically 
measures the links between itself and the other three  
head nodes. Each head node also periodically measures  
the links between itself and all other nodes in its site.  
Hence, using the domain-based network information  
model, the measurement number will be dramatically 
reduced to 
NMS( , [ ]) NMN( ) NMN( )i in n n n= +¦  (2) 
where ni is the total number of nodes in site i. In UniGrid 
and TigerGrid, the numbers of network measurements will 
be decreased to NMS(31, [4, 8, 8, 5, 8, 5, 7, 2, 1, 1, 3, 1, 4, 
1, 3, 1, 1, 4, 8, 1, 1, 1, 1, 2, 2, 1, 4, 1, 2, 4, 1]) = 1316 and 
NMS(12, [4, 4, 4, 4, 8, 2, 3, 4, 4, 4, 4, 1]) = 292, 
respectively. The reduction rate R is defined as: 
NMN( ) NMS( , [ ])
.
NMN( )
in n nR
n
−
=  (3) 
Compared with NMN(96) and NMN(46), the Rs are 86.01% 
and 85.94%, respectively, which shows the obvious 
efficiency of the model. 
Figure 9 Previous design of domain-based network information 
model (see online version for colours) 
Even though this model can eliminate huge amounts  
of measurement effort and bandwidth use, it lacks network 
information between pairs of nodes belonging to different 
sites (unless both are borders). For example, the link (target) 
between nodes A2 and B1 shown in Figure 7 is not 
measured. 
In this model, it reduces a large number of connections, 
but it lacks network information of nodes except head nodes 
in two different sites. This model carries out an estimation 
A heuristic QoS measurement with domain-based network information model 241
it will measure network information in an equal time 
interval, for example, 30 s. Then, the script will extract 
bandwidth and latency from NWS clique, respectively.  
If successes, it will load bandwidth and latency information 
into database. 
The second routine that we defined to keep raw data as 
plain text files locally is designed for future use. Currently, 
it just provides a different storage than database to keep raw 
information of NWS services. 
3.3 Heuristic approach 
Statistics is helpful in many fields, especially for prediction. 
In this paper, we gathered historical network information of 
grid environments and stored it into database. Applications 
could simply query database for network statistics with 
aggregative functions provided by RDBMS, like Max (), 
Min (), AVG () and so on. After analysing these statistics, 
applications can dynamically adjust their parameters about 
network for better performance without sending request  
to estimate network status between all grid nodes in  
real-time manner. 
Besides, we have planned an innovative method  
to obtain real-time network state that worked with  
Dynamic Domain-based Network Information Model,  
i.e., dynamically deploying clique into dedicated node, 
measuring network state, and then reporting results to 
database, users, or applications. The enhanced version of 
shell scripts that support Dynamic Domain-based Network 
Information Model is currently under development. 
We have designed a simple model for integration  
of Ganglia, NWS and NINO (as shown in Figure 13). 
Ganglia and NINO provide UI for users to manage and 
monitor grid environments. NWS and Ganglia collect 
related information from hosts and network regularly.  
And ‘Smart Broker’ provides parameters to applications  
like Cyber.
Figure 13 Integration of Ganglia, NWS and NINO  
(see online version for colours) 
Smart Broker is the key component for QoS measurement. 
Original version of Cyber provides users an interface for 
tuning up parameters, which is shown as Figure 14. Smart 
Broker will help us to achieve automation of parameters 
self-optimisation in diverse scenarios. Smart Broker works 
as evaluation layer between applications and information 
collection layer. We have pre-defined four task types that 
perform QoS measurement in various ways. 
• download 
• upload 
• computational 
• hybrid. 
For example, Cyber is a typical application for ‘Download’ 
tasks. The QoS measurement we designed is to calculate the 
formula as follows: 
(QoS) (1 ) .E RM HMα α= × + − ×  (4) 
E(QoS) is expected value of QoS and RM is real-time 
bandwidth measurement between nodes. HM is historical 
statistics. α is the constant between real-time measurement 
and historical statistics. In the initial stage, we may set α to 
0.5. With more and more tasks submitted, Smart Broker will 
adjust α dynamically. α is not always the same in different 
grid environments. We just try to use α to predict QoS in 
diverse grid environments more effectively. 
Figure 14 Strategy selection – UI provided by Cyber  
for parameters input (see online version for colours) 
4 Experimental environment and results 
To verify the architect we proposed in the initial stage,  
we do not deploy NWS services and RDBMS onto physical 
environments. And we have built our test bed on virtualised 
environments instead. We have created four virtual 
machines and then installed Fedora 9 as default operation 
system. After deploying NWS services with customised 
scripts we wrote, we also built an open-source database  
for experiment. In this paper, we chose MySql as default 
RDBMS. Figure 15 shown here lists our experimental grid 
nodes in the initial stage. 
A heuristic QoS measurement with domain-based network information model 243
5 Conclusions and future work 
In this paper, we use Domain-based Network Information 
Model for experiments, but it is not a proper model for 
dynamic grid environments. If any grid nodes that cause 
hardware failure or just have been reassigned to another IP, 
we have to manually reconstruct NWS cliques. This has 
already mentioned as drawback of NWS (Legrand and 
Quinson, 2004). In large-scale grid environments, it is a 
complicated task to manage these cliques and hosts’ 
relations. Our future work will be adopting Dynamic 
Domain-based Network Information Model for next 
deployment so as to reduce overheads come from 
complicated management tasks. 
Besides, we defined a standard operation procedure for 
managing grid nodes semi-automatically. And, we could 
simply manage grid nodes via web portal instead of writing 
shell scripts. To guarantee a certain degree of QoS, we  
also proposed a heuristic method to predict QoS from 
diverse grid environments for Download-oriented tasks. 
Furthermore, we expect that users could manage and 
monitor numerous resources of grid environments more 
effectively and efficiently. 
Acknowledgement
This work is supported in part by the National Science 
Council, Taiwan ROC, under grants no. NSC 96-2221-E-
029-019-MY3, NSC 97-2622-E-029-003-CC2 and NSC  
98-2622-E-029-001-CC2. 
References
Krauter, K., Buyya, R. and Maheswaran, M. (2002) ‘A taxonomy 
and survey of grid resource management systems for 
distributed computing’, Softw. Pract. Exper., Vol. 32, No. 2, 
pp.135–164.
Krefting, D., Vossberg, M. and Tolxdorff, T. (2008) ‘Simplified
grid implementation of medical image processing  
algorithms using a Workflow management system’, in 
Olabarriaga, S.D., Lingrand, D. and Montagnat, J. (Eds.): 
Medical Imaging on Grids: Achievements and Perspectives,
MICCAI-Grid Workshop, 6 September, New York,  
NY, http://www.i3s.unice.fr/~johan/MICCAI-Grid08/pdf/
kreftingMICCAIG.pdf
Legrand, A. and Quinson, M. (2004) ‘Automatic deployment of 
the network weather service using the effective network 
view’, Paper presented at the Parallel and Distributed 
Processing Symposium, Proceedings, 18th International,
Santa Fe, New Mexico. 
Que, W-K., Zhang, G-Q. and Wei, Z-H. (2008) ‘Model for  
IP network synthetical performance evaluation’, Computer
Engineering, Vol. 34, No. 8, pp.99–101, ISSN:1000-3428 
(2008)08-0099-03.
Wolski, R., Spring, N.T. and Hayes, J. (1999) ‘The network 
weather service: a distributed resource performance 
forecasting service for metacomputing’, Future Generation 
Computer Systems, Vol. 15, Nos. 5, 6, pp.757–768. 
Yang, C-T. and Chen, S-Y. (2008) ‘A multi-site resource 
allocation strategy in computational grids’, Advances in Grid 
and Pervasive Computing, 25–28 May, Kunming, China, 
pp.199–210.
Yang, C-T., Chen, C-H., Yang, M-F. and Chiang, W-C. (2008a) 
‘MIFAS: medical image file accessing system in co-allocation 
data grids’, IEEE Asia-Pacific Services Computing 
Conference, December, Ilan, Taiwan, pp.769–774. 
Yang, C-T., Yang, M-F. and Chiang, W-C. (2008b) 
‘Implementation of a cyber transformer for parallel download 
in co-allocation data grid environments’, in Shenzhen, G.D. 
(Ed.): Proceedings of the 7th International Conference  
on Grid and Cooperative Computing (GCC2008) and  
Second EchoGRID Conference, 24–26 October, China, 
pp.242–253.
Yang, C-T., Shih, P-C., Lin, C-F. and Chen, S-Y. (2007a)  
‘A resource broker with an efficient network information 
model on grid environments’, The Journal of 
Supercomputing, Vol. 40, No. 3, pp.249–267. 
Yang, C-T., Chen, S-Y. and Chen, T-T. (2007b) ‘A grid resource 
broker with network bandwidth-aware job scheduling for 
computational grids’, Advances in Grid and Pervasive 
Computing, Paris, France, pp.1–12.
Yang, C-T., Chen, T-T. and Tung, H-Y. (2007c) ‘A dynamic 
domain-based network information model for computational 
grids’, Paper Presented at the Future Generation 
Communication and Networking (FGCN), Jeju-Island, Korea, 
Vol. 1, pp.575–578. 
Yang, C-T., Shih, P-C., Chen, S-Y. and Shih, W-C. (2005)  
‘An efficient network information model using NWS for  
grid computing environments’, Grid and Cooperative 
Computing – GCC 2005, Vol. 3795, pp.287–299.
Websites
Ganglia: http://ganglia.info/ 
Network Weather Service (NWS): http://nws.cs.ucsb.edu/ 
ewiki/
NINO: http://nino.sourceforge.net/nino/index.html 
C.-T. Yang et al.
physics in Europe [3] have come to require more and more computing power to
generate results. Those simulation results in turn produce terabytes, even petabytes
of data. Only computer centers with many supercomputers and storage devices are
sufficient to handle these data. However, data grid technologies, developed to solve
these kinds of problems, offer an effective alternative means of utilizing large-
scale computing power and storage capacities to compute and store data. Grids
[11, 12, 20, 21, 27, 29–34] enable sharing of computing power and storage capacities
geographically distributed around the world such that they work together as tremen-
dous virtual computers [1, 2] on experiments and simulations. The Globus Toolkit
[13, 27] is open-source software for building data grid environments. It provides mid-
dleware for creating information infrastructures including resource management, data
management, communication, fault detection, security, and portability.
Data replication and consistency [5, 26] refer to the same data being stored in
distributed sites, and kept consistent when one or more copies are modified. A good
file maintenance and consistency strategy can reduce file access times and access
latencies, and increase download speeds, thus reducing overall computing times. And
if one storage site breaks, users can fetch desired data from another storage site, which
improves overall fault tolerance and contributes to making the entire grid environment
more stable and reliable. Another advantage of data grids is the ability grid users
have to download data in parallel from the better sites they choose or an application
chooses automatically after evaluating with a grid environment evaluation model. The
bandwidth utilization of those parallel links is the most important factor affecting
overall download speeds. Network environments vary, which means that replica sites
also vary in their ability to download data efficiently. Replica files should be kept
consistent and downloaded from storage sites nearest users to reduce download times
and ensure high performance.
This paper presents two services, the Dynamic Maintenance Service (DMS) for
maintaining files in data grid systems, and the One-way Replica Consistency Service
(ORCS) for keeping all copies of files consistent when one is modified. The DMS
automatically maintains data statuses such as access frequency, space available on
storage elements to which data will be replicated or migrated, and the network sta-
tuses of file source sites to other sites. The ORCS provides asynchronous data repli-
cation and replica consistency mechanisms that can reduce replica maintenance costs
and free up storage space for new data or temporary data produced by experiments
or simulations to avoid creating too many identical replicas. Users can easily find the
best replica sites for downloading desired data, thus increasing replica usage rates,
and improving storage device usage efficiency ratios over other strategies.
The contribution of this paper is to help make data grid environments more effi-
cient by using the DMS and ORCS algorithms. Using DMS adjusts data to locations
appropriate to the sites that request the data more often, thus reducing the times re-
quired by those sites to get needed data and improving performance. Using ORCS
improves accessing performance by keeping replica content consistent and improving
data grid storage device usage ratios. The DMS and ORCS algorithms also consider
storage element free space when storing new and temporary data produced while
computing. This decreases the probability of applications crashing or having to re-
submit jobs to other computing resources for processing. Our experimental results
C.-T. Yang et al.
[22, 23], Rashedur M. Rahman et al., proposed a static replica placement algorithm
for placing replicas in the best p candidate nodes to minimize the total response time
of each node using Lagrangian relaxation, which is a heuristic approach [10] to mea-
suring the response time of each client node to its nearest server node. The algorithm
is most likely the p-median problem. They also use user requests and network la-
tency as parameters in deciding when to maintain replicas dynamically. They use a
simulator called OptorSim [19], developed by the EU Data Grid project, to compare
their method, and called dynamic p-median, with static p-median and Best_client.
Static p-median replicates no files to other nodes in the data grid environment when
user requests or network latency change. Best_client replicates the desired data to
client nodes when request ratios for certain files in a node are very high. Simula-
tion results show average response times for the authors’ method are the lowest over
various network loadings and user requests. Although dynamic p-median is a good
method for dynamic replica maintenance, it can’t be used in real grid environments
since p-median is an NP-hard problem that consumes too much computing power
determining new replica locations.
In [20], Sang-Min Park et al. proposed a dynamic replica maintenance algorithm
called Bandwidth Hierarchy based Replication (BHR) that divides sites into many
regions putting sites close to one another in the same regions in the bandwidth hier-
archy. The BHR optimizer terminates replication if a replica duplicate already exists
in another site in the same region. In [4], Ruay-Shiung Chang and Jih-Sheng Chang
indicated that the BHR algorithm performs better than other strategies only when the
storage element capacity is small. We found the following problem in BHR: If a file
must be replicated in a region, BHR replicates it to one other site in the region. If an
attempt is made to replicate the same file to a third site in the same region, BHR will
see that there is already a duplicate file in the region and terminate. Thus, files will
have at most two copies in each region, which means only two links will be available
for parallel data downloading [1, 2, 30–34] in any one region. This limitation leads to
high time costs, thus reducing the effectiveness of an important grid computing fea-
ture and adversely affecting overall performance. Furthermore, the two-copy practice
will cause load imbalances on the sites where the copies are stored.
2.2 Replica consistency
Grid environment files modified by grid users raise the critical problem of maintain-
ing data consistency among the replicas distributed across various machines. Over the
past decade, considerable effort has been devoted to developing several consistency
models. These studies concentrated on trading off consistency for performance and
availability. The various consistency models developed include Strong, Weak, Con-
tinuous, Data-centric, Strict, Sequential, Eventual, Causal, FIFO, and Release. For
instance, the strong consistency approach keeps data consistent across all replicas
simultaneously, which requires many more resources and expensive protocols than
other consistency models. The converse of strong consistency is weak consistency,
which can tolerate inconsistencies for certain periods of time.
Many studies on replica consistency in data grid environments have been pub-
lished [2, 4, 7, 8, 14, 15]. Data grid environments need consistency services to syn-
chronize them when replicas are modified by grid users. The European Data Grid
C.-T. Yang et al.
Fig. 1 The software stack
diagram of each node
Fig. 2 The software stack
diagram of all sites, services,
and portals
• Bottom Layer: shows the software installed on each node in the grid environment.
The major components of the Bottom Layer are the Information Provider and Grid
Middleware. The Information Provider consists of the Ganglia [35] and Network
Weather Service (NWS) [18]. Ganglia gathers machine information such as num-
bers of processors and how many cores each has, the loading on each processor,
total memory size and free space, and disk usage. The NWS gathers inter-node
network bandwidths and each link’s latency. The Grid Middleware consists of the
Globus Toolkit [27], which is used to join nodes to the grid environment.
• Middle Layer: This layer is the Site, consisting of several nodes usually located
in the same place or connected to the same switch or hub. Nodes in the Site are
connected to one another via the Internet. Sites are usually built up as clusters, but
each node has a real IP; the Site’s first node is called the head node.
• Top Layer: This layer holds Applications, Services, the Monitoring Service, and
Records [17]. Services consist of the Anticipative-Recursively-Adjusting Mecha-
nism, Replica Selection Service, One-way Replica Consistency Service, and Dy-
namic Maintenance Service. Services operate on information gathered from the
Monitoring Service and Records. Records can provide machine and file informa-
tion prior to downloading files or adjusting file locations. The Monitoring Service
provides a web front-end page for users to observe variations during job process-
ing.
Relations among the components described above are shown in Fig. 3. The four
services mentioned above are classified as User-side and System-side. The User-
C.-T. Yang et al.
side, which allows users to monitor application operations as the applications serve
their needs, includes the Anticipative-Recursively-Adjusting Mechanism (ARAM)
and Replica Selection Service (RSS). The System-side includes the Dynamic Main-
tenance Service (DMS) and One-way Replica Consistency Service (ORCS), which
automatically direct files to appropriate locations and keep them consistent. Func-
tional details of these services are described below.
• Replica Selection Service: gathers relevant information from the RLS and Infor-
mation Service to determine which sites are better for the ARAM to use for down-
loading files.
• Anticipative-Recursively-Adjusting Mechanism: enables users to download de-
sired data in parallel, dynamically adjusting download speeds according to net-
work bandwidths between server nodes and client nodes, and balancing file site
loadings.
• One-way Replica Consistency Service: keeps files consistent with duplicates stored
in distributed nodes. When one file in a node is updated, it will notify the other
nodes that have the same file to update to the newest version.
• Dynamic Maintenance Service: dynamically replicates, migrates, and deletes grid
environment files according to parameter variations. It reduces execution times,
promotes system stability, and improves storage device usage ratio efficiency.
3.2 ORCS and DMS operation
The DMS maintains replicas; the ORCS keeps file copies consistent. Figure 4 shows
general DMS and ORCS operation. Prior to file maintenance, the Information Service
and Replica Location Service store relevant information in the database for DMS
measurement using the cost model described below. The Information Service and
Replica Location Service functions are described below:
• Information Service [6]: periodically gathers statuses such as CPU idle ratio, mem-
ory usage, storage device free space, and network bandwidth, and records them in
real time in the Information Database (Info. DB) for the DMS to use.
• Replica Location Service (RLS): stores file information such as logical file name,
file size, file physical location, time of file creation or updating, and file access fre-
quency in the File Information Database (File Info. DB). Users can use the Replica
Location Service to search for desired files and the closest sites in the grid envi-
ronment where the files are stored.
Before the Replica Manager triggers the ORCS and DMS, it first queries the In-
formation Service and Replica Location Service, which then separately query the
Information Database and File Information Database to get all file and system status
information. If a Replica Manager determines some files need to be adjusted or kept
consistent, it directs the ORCS and DMS to make the necessary adjustments. After
all adjustments have been made, the ORCS and DMS query the Replica Location
Service to check the new statuses of all files in the grid environment. After checking,
the Replica Location Service records the new information in the File Information
Database.
C.-T. Yang et al.
3.3 Parameters and evaluation model
In this subsection we introduce our affect parameters, define measurable parameters,
and present the evaluation models we use to measure the performance of the two
services described above.
3.3.1 Affect parameters
Because grid environments have many factors that affect performance, we calculated
how the following static and dynamic factors affect overall performance.
• Static Parameters: These factors do not change when the grid environment changes.
As Xuanhua Shi et al. indicated in [25], they include system site attributes such
as CPU type and frequency, each storage element’s hard disk capacity, memory
capacity, and network card transfer rate. In general, faster frequency CPUs, larger
memory and hard disk capacities, and network cards with faster transfer rates are
better choices for executing jobs. Since these cannot be major factors in measuring
grid environment performance due to the changeable nature of grid environments,
we focus on the dynamic factors.
• Dynamic Parameters: These factors change when the grid environment changes.
Job execution consumes computing power and uses memory space downloading
or uploading data, and storing computational results. Thus, CPU usage rate, mem-
ory space, bandwidth, and node free space may all change. Among these, network
bandwidth has the most important influence on performance. The NWS [18] mon-
itors and periodically forecasts the performance of various network and computa-
tional elements. Real-time requirements must be met to achieve high performance.
We use the NWS to measure network bandwidth, and the Linux commands “sar”
and “df” to measure CPU, memory, and hard disk free space.
3.3.2 Cost model
Before files are replicated, migrated, or deleted, their affect factors must be measured
to determine what operations are necessary. Below, we define our strategic parame-
ters.
• BWLAN(i − j): LAN connection bandwidth between node i and node j in Mbit
• BWWAN(i − j): WAN connection bandwidth between site i and j in Mbit
• F_size: File size for transfer in MB
• T_trans(i − j): Time to transfer data from node i to node j
• T_auth(i − j): Time for authenticating transfer of data file from node i to node j
• T_replica_local(i − j): Time for local file replication from node i to node j
• T_replica_remote(i − j): Time for remote file replication from node i to node j
• F_space(i): Node i storage device free space
• FA_Min: Minimum file access rate
• FA_Max: Maximum file access rate
• α: Adjustable parameter for checking whether the storage element free space is
sufficient for replication
C.-T. Yang et al.
Fig. 5 DMS replication
algorithm Check all file access rates
If (File i′s access rate is greater than FA_Max in site j) Then
{
Check site j storage device free space
If (Not enough free space) Then
Find an alternative site closest to site j
If (The same file exists intra-region) Then
Replicate file i to site j from intra-region file site
Else
Replicate file i to site j from best inter-region file site
Else
If (The same file exists intra-region) Then
Replicate file i to site j from intra-region file site
Else
Replicate file i to site j from best inter-region file site
}
If (File i’s access rate is greater than FA_Min in site j ) and
(File i’s access rate is less than FA_Max in site j ) Then
{
Find the nearest file site j that no longer needs file i
Check site j storage device free space
If (Not enough free space) Then
Find an alternative site closest to site j
Migrate file i to alternative site from file site
Else
Migrate file i to site j from file site
}
If (File i’s access rate is less than FA_Min in site j) Then
{
Check the File Info. DB for another site with the same file
If (A site with the file is found) Then
Delete file i in site j
Else
Keep file i
}
• Replication: If the access frequency for file i at site j exceeds the maximum access
rate FA_Max, the DMS first checks to see if the storage device at site j has enough
free space to store the replicated file. If it does, the DMS duplicates the data to site
j using the intra-region copy of file i if such a copy exists, or it creates a duplicate
of file i at site j in the intra-region. If site j does not have enough free space, the
DMS first checks to see if it can duplicate file i in the inter-region. If not, it stores
the duplicated data in the site closest to site j .
• Migration: When an original file site no longer needs a file, or has insufficient free
space to store duplicated data, temporary data, or computing results, but other sites
still need the file, migration is used to move the file to an appropriate location. This
avoids generation of excessive file copies in the data grid system and saves free
space for storing temporary data and job execution results. If the request frequency
of file i in site j is between FA_Min and FA_Max, the DMS first checks to see if
C.-T. Yang et al.
// Once Original Data has been updated
If original data is updated from a super node then
Copy the original data to all master nodes
Add update records to the replication database for tracing
End
// For each Grid Site
If a replica’s access frequency by CN to MN is greater than its threshold then
If the CN has sufficient storage capacity then
Copy a replica from MN to CN
Add a replica update record to the database
Else
Find all replicas with access frequencies smaller than the CN threshold
Sort these CN replica access frequencies in ascending order
Delete replicas one by one from small to large until CN has sufficient storage capacity
If the storage capacity of CN is sufficient then
Copy a replica from MN to CN
Add a replica update record to the database
Else
Copy a replica from MN to a CN with the best resource status
Add a replica update record to the database
End
End
End
Fig. 6 First section of the ORCS algorithm
algorithm must find the nearest CN with the best resource status that has sufficient
storage capacity, as shown in Fig. 11.
In Fig. 12, Node j replica’s access frequency is lower than its threshold value,
thus Node j accesses the MN replica. In contrast, in Fig. 13, grid users access Node j
directly if the last update time of the CN replica is the same as that of the MN replica.
If the latest update times are not equal, the replica will be copied automatically from
the MN to the CN, as shown in Fig. 14. Figures 15 and 16 show the algorithm finding
the nearest CN with the best resource status and sufficient storage capacity because
node j has insufficient storage capacity.
4 Experimental environment and results
We compared and evaluated the performance of the DMS and ORCS algorithms
against other strategies. The Least Frequently Used (LFU), Least Recently Used
(LRU) strategies, and the Bandwidth Hierarchy-based Replication algorithm (BHR)
were tested against the DMS algorithm. The LFU and LRU always replicate when
requests occur, but choose files for deletion differently when storage element free
space is insufficient for replication. LRU chooses the oldest files for deletion, while
LFU chooses the least frequently requested files. The synchronous and asynchronous
consistency strategies were tested against the ORCS. We used a simulator called Op-
torSim, developed by the EU Data Grid [3], to compare the strategies mentioned
above. Our experimental grid environment is shown in Fig. 17. It consisted of four
C.-T. Yang et al.
Fig. 9 Operation 2 of the first
section
Fig. 10 Operation 3 of the first
section
Fig. 11 Operation 4 of the first
section
C.-T. Yang et al.
Fig. 14 Operation 3 of the
second section
Fig. 15 Operation 4 of the
second section
Fig. 16 Operation 5 of the
second section
The evaluation results are shown in Figs. 18 and 19. Figure 18 shows the execution
time for 500 jobs was best when α was set to 0.9, and Fig. 19 shows the storage
C.-T. Yang et al.
Table 1 Parameters used in the
simulation Parameters Values
Number of jobs 500
Number of job types 30
Number of files accessed per job 15
File sizes 250/500/750/1000 MB
Intra-region bandwidth 500 Mbps
Inter-region bandwidth 250 Mbps
Hard disk space at each site 50 GB
FA_Max 10
FA_Min 5
Job Delay 2500 ms
Table 2 Numbers of files in
each size File size Number
250 MB 175
500 MB 90
750 MB 100
1000 MB 85
Fig. 19 Free space for various
alpha values
4.1.2 ORCS parameter setting
In Table 3, we define several parameters used to derive the experimental results shown
below. We submitted 100 writing jobs from the SN and 1000 read jobs from each CN.
Files were 100 MB in size and the access threshold was 10. We used synchronous
replication, asynchronous replication, and the ORCS algorithms in our experimental
environment.
4.2 Results
4.2.1 File management results
Figures 20 and 21 show, respectively, DMS execution times and storage element free
space with and without the migration mechanism. Experimental results show exe-
cution times were better with the migration mechanism than without the migration
C.-T. Yang et al.
Fig. 22 Performance
comparison of four strategies
Fig. 23 Free space comparison
of four strategies
Fig. 24 Execution time
comparison for various
bandwidth ratios with 50 G H.D.
porary data and job results were better than those of the DMS, choosing files for
deletion and downloading files for job execution took considerable time. The BHR
strategy caused jobs to spend excessive time getting needed files. Although the BHR
strategy saved a lot of free space, its execution times were greater than those of the
DMS strategy. Thus, the DMS performed more efficiently than other three replication
strategies.
Figures 24 to 28 show the use of various network bandwidths and storage ele-
ment capacities to demonstrate variations in the four strategies’ performance. For all
variations in hard disk size, LFU and LRU performed better when WAN bandwidth
equaled LAN bandwidth. When there was not enough space to store replicas, tempo-
rary files, and job results, computing elements spent less time getting relevant files
C.-T. Yang et al.
Fig. 28 Execution time
comparison for various
bandwidth ratios with 200 G
H.D.
cution times decreased gradually as the ratio of WAN bandwidth to LAN bandwidth
was increased.
The DMS replication strategy performed better than other replication strategies
when storage element capacity was small because it provided more storage ele-
ment free space for storing replicas, temporary files, and job results. But increas-
ing the storage element capacity sufficiently provided enough free space for the
LFU and LRU replication strategies to store replicas, temporary files, and job re-
sults, reducing the time computing elements needed to get required files from other
storage elements in the LAN and even in the WAN, thus shortening total execu-
tion times. And increasing storage element capacity also increased DMS replication
strategy execution times. This means that as capacity was increased, DMS repli-
cation strategy performance worsened due to inefficient storage element usage ra-
tios, and that the DMS was more effective when the storage element capacity was
small.
4.2.2 File consistency results
There were three replication algorithms in our experiment, Synchronous, Asynchro-
nous, and our ORCS algorithm. We assumed that each file was 100 MB and the
access frequency threshold was 10. When a replica in the CN was the same as the
one in the MN, no transfer was necessary. The experimental file replication times are
shown in Fig. 29. The ORCS replicated files according to grid users’ needs. It con-
sumed less network bandwidth than the Synchronous algorithm and accessed files
more efficiently than the Asynchronous algorithm.
In the next experiment, we compared the storage capacity usage of our ORCS
with that of the Massive Data Oriented Replication Algorithms (MDORA) proposed
by Changqin Huang et al. in [14]. We assumed CN storage capacities of 80 G, a
replication frequency of 200, and file sizes of 5 MB, 50 MB, 100 MB, 1000 MB,
and 2000 MB. The result is shown in Fig. 30. If the CN storage capacity is inade-
quate, MDORA cannot store replicas. ORCS deletes files in ascending order of ac-
cess frequency until there is adequate storage space for replicas. Thus, replicas can
be written, even though the storage space was initially inadequate.
C.-T. Yang et al.
Fig. 31 Replication times for various threshold values
Fig. 32 Numbers of replications for various threshold values
5 Conclusions and future work
This paper presents the Dynamic Maintenance Service (DMS) and the One-way
Replica Consistency Service (ORCS) for improving grid environment performance.
DMS is also aimed at the “one-replica” problem the BHR incurs. It improves grid
system performance and increases storage element usage ratio efficiency by handling
temporary data and results that jobs produce during execution. Via ORCS, we ad-
dressed the principal problems with maintaining consistency among existing repli-
cas. Experimental results show that DMS and ORCS both perform more efficiently
than other strategies and make storage element usage ratios more efficient as well. We
conducted the design and implementation of a data grid system using the components
and services proposed in Sect. 3 to enable general users to use data grid systems and
monitor details of grid resource and file statuses.
C.-T. Yang et al.
19. OptorSim. http://edg-wp2.web.cern.ch/edg-wp2/optimization/optorsim.html
20. Park SM, Kim JH, Ko YB, Yoon W-S (2003) Dynamic data grid replication strategy based on Internet
hierarchy. In: The second international workshop on grid and cooperative computing (GCC2003), pp
838–846
21. Park SM, Kim JH (2003) Chameleon: a resource scheduler in a data grid environment. In:
Proceedings of third international symposium on cluster computing and the grid, p. 258.
http://portal.acm.org/citation.cfm?id=792481
22. Rahman RM, Barker K, Alhajj R (2006) Replica placement design with static optimality and dynamic
maintainability. In: Proceedings of the sixth IEEE international symposium on cluster computing and
the grid (CCGRID’06), pp 434–437
23. Rahman RM, Barker K, Alhajj R (2006) Effective dynamic replica maintenance algorithm for the
grid environment. In: Proceedings of advances in grid and pervasive computing, vol 3947: Grid and
pervasive computing 2006 (GPC2006), pp 336–345
24. Ranganathan K, Foster I Design and evaluation of dynamic replication strategies for a high perfor-
mance data grid. In: Proceedings of international conference on computing in high energy and nuclear
physics
25. Shi XH, Jin H, Qiang WZ, Zou DQ (2003) An adaptive meta-scheduler for data-intensive applications.
In: Proceedings of grid and cooperative computing (GCC’03), pp 830–837
26. Stockinger H, Samar A, Allcock B, Foster I, Holtman K, Tierney B (2002) File and object replication
in data grids. J Cluster Comput 5(3):305–314
27. The Globus Alliance. http://www.globus.org/
28. Vazhkudai S, Tuecke S, Foster I (2001) Replica selection in the globus data grid. In: Proceedings of
the 1st international symposium on cluster computing and the grid (CCGRID 2001), pp 106–113
29. Venugopal S, Buyya R, Ramamohanarao K (2006) A taxonomy of data grids for distributed data
sharing, management, and processing. ACM computing surveys, vol 38, Article 3, March 2006
30. Yang CT, Yang IH, Li KC, Wang SY (2007) Improvements on dynamic adjustment mechanism in
co-allocation data grid environments. J Supercomput 40(3):269–280
31. Yang CT, Wang SY, Fu CP (2007) A dynamic adjustment mechanism for data transfer in data grids. In:
Network and parallel computing: IFIP international conference, NPC 2007. Lecture notes in computer
science, vol 4672. Springer, Berlin, pp 61–70. ISSN 1611-3349
32. Yang CT, Yang MF, Chiang WC (2008) Implementation of a cyber transformer for parallel download
in co-allocation data grid environments. In: Proceedings of the 7th international conference on grid
and cooperative computing (GCC2008) and second EchoGRID conference, October 24–26, 2008 in
Shenzhen, Guangdong, China, pp 242–253
33. Yang CT, Yang IH, Chen CH, Wang SY (2006) Implementation of a dynamic adjustment mechanism
with efficient replica selection in co-allocation data grid environments. In: Proceedings of the 21st
annual ACM symposium on applied computing (SAC 2006) – distributed systems and grid computing
(DSGC) track, vol 1, pp 797–804, Dijon, France, April 23–27, 2006
34. Yang CT, Yang IH, Wang SY, Li KC, Hsu CH (2009) A recursively-adjusting co-allocation scheme
with cyber-transformer in data grids. Future Gener Comput Syst 25(7):695–703
35. Ganglia. http://ganglia.info/
Chao-Tung Yang is Professor of Computer Science at Tunghai Uni-
versity in Taiwan. He was born on November 9, 1968 in Ilan, Taiwan,
R.O.C. and received his B.Sc. degree in Computer Science from Tung-
hai University, Taichung, Taiwan, in 1990, and the M.Sc. degree in
Computer Science from National Chiao Tung University, Hsinchu, Tai-
wan, in 1992. He received the Ph.D. degree in Computer Science from
National Chiao Tung University in July 1996. He won the 1996 Acer
Dragon Award for an outstanding Ph.D. dissertation. He has worked
as Associate Researcher for ground operations in the ROCSAT Ground
System Section (RGS) of the National Space Program Office (NSPO)
in Hsinchu Science-based Industrial Park since 1996. In August 2001,
he joined the faculty of the Department of Computer Science at Tunghai
University. He got the excellent research award by Tunghai University
in 2007. In 2007 and 2008, he got the Golden Penguin Award by Indus-
trial Development Bureau, Ministry of Economic Affairs, Taiwan. His
J Supercomput
DOI 10.1007/s11227-009-0307-4
Implementation of a dynamic adjustment strategy
for parallel file transfer in co-allocation data grids
Chao-Tung Yang · Shih-Yu Wang ·
William Cheng-Chung Chu
© Springer Science+Business Media, LLC 2009
Abstract Co-allocation architecture was developed to enable parallel transferring
of files from multiple replicas stored in the different servers. Several co-allocation
strategies have been coupled and used to exploit the different transfer rates among
various client-server links and to address dynamic rate fluctuations by dividing files
into multiple blocks of equal sizes. The paper presents a dynamic file transfer scheme,
called dynamic adjustment strategy (DAS), for co-allocation architecture in concur-
rently transferring a file from multiple replicas stored in multiple servers within a data
grid. The scheme overcomes the obstacle of transfer performance due to idle waiting
time of faster servers in co-allocation based file transfers and, therefore, provides re-
duced file transfer time. A tool with user friendly interface that can be used to manage
replicas and downloading in a data grid environment is also described. Experimental
results show that our DAS can obtain high-performance file transfer speed and reduce
the time cost of reassembling data blocks.
Keywords Data grid · File replica · Parallel file transfer · Co-allocation · Dynamic
adjustment
C.-T. Yang () · S.-Y. Wang · W.C.-C. Chu
Department of Computer Science, Tunghai University, Taichung, 40704, Taiwan
e-mail: ctyang@thu.edu.tw
W.C.-C. Chu
e-mail: cchu@thu.edu.tw
S.-Y. Wang
Communication System Division Service-Oriented Network System Department, Information and
Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, 31040,
Taiwan
e-mail: Laurence@itri.org.tw
Implementation of a dynamic adjustment strategy
completion times. We also present a new toolkit, called cyber-transformer, with a
friendly client-side GUI interface integrated with the information service, replica lo-
cation service, and data transfer service [25] that makes it easy for inexperienced
users to manage replicas and download files in data grid environments. And we pro-
vide an effective scheme for reducing the cost of reassembling data blocks. Experi-
mental results show that our approach is superior to previous methods and achieves
the best overall performance. We also discuss combination cost and provide an effec-
tive improvement.
The remainder of this paper is organized as follows. Related background review
and studies are presented in Sect. 2 and the co-allocation architecture and our research
approaches are outlined in Sect. 3. In Sect. 4, a powerful toolkit, cyber-transformer, is
proposed by us, and experimental results and a performance evaluation of our scheme
are presented in Sect. 5. Section 6 concludes this research paper.
2 Background review
2.1 Data grid and replications
In data grid environments, access to distributed data is typically as important as ac-
cess to distributed computational resources [1–5, 7, 22]. Distributed scientific and
engineering applications require transfers of large amounts of data between storage
systems, and access to large amounts of data generated by many geographically dis-
tributed applications and users for analysis and visualization, among others.
We used the grid middleware Globus Toolkit [9, 10, 12, 13, 16] as our data grid
infrastructure. The Globus Toolkit provides solutions for such considerations as se-
curity, resource management, data management, and information services. One of its
primary components, MDS [7, 13, 16], is designed to provide a standard mechanism
for discovering and publishing resource status and configuration information. It pro-
vides a uniform and flexible interface for data collected by lower-level information
providers in two modes: static (e.g., OS, CPU types, and system architectures) and
dynamic data (e.g., disk availability, memory availability, and loading). And it uses
GridFTP [3, 13, 16] to provide efficient management and transfer data in a wide-area,
distributed-resource environment. We use GridFTP to enable parallel data transfers.
Among its many features, its partial file transfer ability allows files to be retrieved
from data servers by specifying the start and end offsets of file partitions. This pro-
tocol, which extends the standard FTP protocol, provides a superset of the features
offered by the various grid storage systems currently in use.
The data grid community tries to develop secure, efficient data transport mech-
anisms and replica management services. Another key technology from the Globus
project, called the Replica Catalog [16], is used to register and manage complete and
partial copies of data sets. The Replica Catalog contains mapping information from
a logical file or collection to one or more physical files.
Replica management involves creating or removing replicas at a data grid site
[21]. A replica manager typically maintains a replica catalog containing replica site
addresses and file instances. The replica management service is responsible for man-
aging the replication of complete and partial copies of datasets, defined as collections
Implementation of a dynamic adjustment strategy
Fig. 1 The co-allocation
architecture in data grids
service [21] creates a list of the desired files physical locations. The co-allocation
agent then downloads the data in parallel from the selected servers.
Data grids consist of scattered computing and storage resources located in differ-
ent countries/regions yet accessible to users [8]. As datasets are replicated within grid
environments for reliability and performance, clients require the abilities to discover
existing data replicas, and create and register new replicas. A replica location ser-
vice (RLS) [5] provides a mechanism for discovering and registering existing repli-
cas. Several prediction metrics have been developed to help replica selection. For
instance, Vazhkudai and Schopf [19–21] used past data transfer histories to estimate
current data transfer throughputs.
In [17], the author proposes a co-allocation architecture for co-allocating grid
data transfers across multiple connections by exploiting the partial-copy feature of
GridFTP. It also provides brute-force, history-based, and dynamic load balancing for
allocating data blocks.
• Brute-force co-allocation: brute-force co-allocation (see Fig. 2) works by dividing
files equally among “n” available flows (locations). Thus, if the data to be fetched
is size “S” and there are “n” locations to fetch it from, then this technique assigns
to each flow a data block of size, “S/n”. For example, if there are three sources,
the target file will be divided into three blocks equally. And each source provides
one block for the client. With this technique, although all the available servers
are utilized, bandwidth differences among the various client-server links are not
exploited.
• History-based co-allocation: The history-based co-allocation (see Fig. 3) scheme
keeps block sizes per flow proportional to transfer rates predicted by the previous
results of file transfer results. In the history-based allocation scheme, the block
Implementation of a dynamic adjustment strategy
Fig. 4 The conservative load
balancing process
Fig. 5 The aggressive load
balancing process
• Aggressive load balancing: This method is shown in Fig. 5 and adds functions that
change block size in deliveries by: (1) gradually increasing the amounts of data
requested from faster servers, and (2) reducing the amounts of data requested from
slower servers or stopping requesting data from them altogether.
In our previous work [24, 26], we proposed a replica selection cost model and
a replica selection service to perform replica selection. These co-allocation strate-
gies do not address the shortcoming of faster servers having to wait for the slowest
server to deliver its final block. In most cases, this wastes much time and decreases
overall performance. Thus, we propose an efficient approach, called the dynamic ad-
justment strategy, and based on the co-allocation architecture. It improves dynamic
co-allocation and reduces waiting time, thus improving overall transfer performance.
3 The dynamic adjustment strategy
Dynamic co-allocation, described above, is the most efficient approach to reducing
the influence of network variations between clients and servers. However, the idle
time of faster servers waiting for the slowest server to deliver its last block is still
Implementation of a dynamic adjustment strategy
server devices:
Scorei = P CPUi × RCPU + P Memi × RMem + P BWi × RBW,
and RCPU + RMem + RBW = 1 (2)
After getting the scores for all server nodes, the system calculates the weightingi :
weightingi = Scorei
/ n∑
k=1
Scorek (3)
The weighting is then used to determine the size of the next job:
newPT i = ClientBandwidth × weightingi (4)
where newPT i denotes the next job size for server i, and ClientBandwidth denotes
the current client bandwidth.
When server i finishes transferring of a block, it gets a new job whose size is
calculated according to the real-time status of server i. Each time, our strategy dy-
namically adjusts a job size according to source device loading and bandwidth. The
lighter the loading a source device has, the larger job size is assigned. Figure 6 shows
a flowchart illustrating this new strategy.
Next, the average transfer rate of all replicas can be calculated by total transferred
file size divided the cost time ratio of combination of CPU, memory, and network
bandwidth. We used the dynamic adjustment strategy with various sets of replica
servers and measured overall performances, where overall performance is:
Total Performance = File Size/Total Completion Time (5)
4 An efficient toolkit: cyber-transformer
We gave experimental results for cyber-transformer, a powerful new toolkit for replica
management and data transfer in data grid environments. It not only can accelerate
data transfer rate, but can also manage replicas over all various sites. The friendly
interface enables users to easily monitor replica sources, and add files as replicas
for automatic cataloging by our replica location service. Moreover, we provide a
function for administrators to delete and modify replicas. Cyber-transformer can be
invoked with either the logical file name of a data file or a list of replica sources host
names. When users search for a file by its logical file name, cyber-transformer queries
the replica location services to find all the corresponding replicas, and contacts each
replica source to start parallel transfers. The file is then gathered from replica sources
and finally combined into a single file.
4.1 System components
Cyber-transformer is implemented in the Java Cog Kits [13] library. Figure 7 shows
the system stack of cyber-transformer, consisting of three integrated mechanisms:
Implementation of a dynamic adjustment strategy
Fig. 7 The system stack of
cyber-transformer
Fig. 8 The components of cyber-transformer
• Information service: This service is invoked by the information monitor and pro-
vides replica sources statuses allowing users to monitor all replica source sites in
the data grid. Sites status, such as CPU loading, free memory, hard disk free space,
and bandwidth, are gathered by the information service and reported to the infor-
mation monitor.
Implementation of a dynamic adjustment strategy
Fig. 9 The cyber-transformer transaction flow
effort among problem-solving environments, science portals, grid middleware, and
collaborative pilots.
The Java-based application uses the Java CoG kit to connect to the grid system.
The key characteristics include: GridProxyInit, a JDialog for submitting pass phrases
to grids to extend certificate expiration dates, GridConfigureDialog uses the UITool
in the CoG Kit to enable users to configure process numbers and host names of Grid
servers, and GridJob, which creates GramJob instances. This class represents a simple
gram job and allows for submitting jobs to a gatekeeper, canceling them, sending
signal commands, and registering and unregistering from callbacks. The GetRSL,
RSL provides a common interchange language to describe resources.
The Java CoG GridFTP API does not support downloading files in multiple
streams and simultaneously writing them to the same file, which causes some com-
bination overhead after all transmissions. Thus, we needed an effective method for
writing to a file in parallel. To resolve the situation, we analyzed and rewrote the Java
Implementation of a dynamic adjustment strategy
Fig. 11 The file download process
The class, RandomAccessFile in file.seek(bufOffset) provides a function that
can change the write pointer. This allowed us to write another class to inherit
FileRandomIO from Java CoG, and overwrite the method, public synchronized void
write(Buffer buffer). We also added a method to change the write pointer. This gave
us more transmission time to write data to a file at the same time. All streams write
to assigned file positions, not to the beginning of the file. That does not affect other
streams transferring data and writing files. The new code after our changes is listed
below.
protected long filePointer;
/**
* set write pointer
* @param filePointer write pointer
*/
public void setFileOffset(long filePointer) {
this.filePointer = filePointer;
}
Implementation of a dynamic adjustment strategy
The boldface type shows that the key point in measuring the current transfer speed
is using time difference and file-writing duration. This gives the transfer speed during
file transfers, thus there is no separate reassembly time cost. We overcome one co-
allocation shortcoming, and the completion time is just the sum of the authentication
and data transmission times.
5 Experimental results and analysis
In this section, we discuss the performance of our recursive-adjustment co-allocation
strategy. We evaluate four co-allocation schemes: (1) brute-force (Brute), (2) history-
based (History), (3) conservative load balancing (Conservative), (4) aggressive load
balancing (Aggressive) and (5) dynamic adjustment strategy (DAS). We analyze the
performance of each scheme by comparing their transfer finish times, and the total
idle time faster servers spent waiting for the slowest servers to finish delivering the
last block. We also analyze overall performances in the various cases.
5.1 Input parameters
We used the following experiments to determine input parameters for the three factors
in our strategy: CPU idle state, memory free space, and network bandwidth, and
assign ratios to each of the factors.
At first, to determine the effect of network bandwidth on transfer rates, we mea-
sured average rates using various bandwidth ratios. As Fig. 12 shows, there was little
difference for small file sizes, however, as the file size was increased, a curve became
apparent. The transfer rate decreased at bandwidth ratios smaller than 0.6; the peak
transfer rate occurred at a ratio of 0.8. This means that we set RCPU,RMEM, and RBW
in the ratio 0.1:0.1:0.8.
In the second experiment, we assessed the effect of CPU computing power on
transfer rates. We used three machines with different CPU types, memory sizes fixed
Fig. 12 The partition size evaluation result
Implementation of a dynamic adjustment strategy
Fig. 15 Our data grid testbed
portant factor affecting transfer rate, and that the bandwidth ratio should be set larger
than the other two factors. CPU power and memory size can be used to make a dif-
ference when the bandwidths of several servers are very close.
5.2 Experimental environments
We performed wide-area data transfer experiments using our GridFTP GUI client
tool. We executed our co-allocation client tool on our testbed at Tunghai University
(THU), Taichung City, Taiwan, and fetched files from four selected replica servers:
one at Providence University (PU), one at Li-Zen High School (LZ), and the other
one at Hsiuping Institute of Technology School (HIT). All these institutions are in
Taichung, Taiwan, and each is at least 10 km from THU. Figure 15 shows our data
grid testbed, and Table 1 is the detailed listing. All servers had Globus 3.2.1 or above
installed.
In the following experiments, we set RCPU,RMEM, and RBW in the ratio
0.1:0.1:0.8. We experimented with file sizes of 10 MB, 50 MB, 100 MB, 500 MB,
1000 MB, 1500 MB, and 2000 MB. For comparison, we measured the performance
of conservative load balancing on each size using the same block numbers.
Table 2 shows average transmission rates between THU and each replica server.
These numbers were obtained by transferring files of 100 MB, 500 MB, 1000 MB,
and 2000 MB from a single replica server using our GridFTP client tool, and each
number is an average over several runs.
5.3 Results and analysis
We examined the effect of faster servers waiting for the slowest server to deliver the
last block for each scheme. Figure 16 shows total idle times for various file sizes. Note
that our Dynamic Adjustment Strategy performed significantly better than the other
Implementation of a dynamic adjustment strategy
Fig. 16 Idle times for various methods; servers at PU, LZ, and HIT
Fig. 17 Completion times for various methods; servers are at PU, LZ, and HIT
trary, it decreased. We can infer that the co-allocation efficiency reached saturation
in the DAS2_2 case, and that additional flows caused additional overhead and re-
duced overall performance because the PU file site had worse network bandwidth,
and DAS2_1 choosing PU for file transfer led to the differences between DAS2_1 and
DAS2_2. This means that more download flows do not necessarily result in higher
Implementation of a dynamic adjustment strategy
Fig. 19 Comparison of cyber-transformer transmission rate between LAN and WAN
Fig. 20 The rate of overall
performance downgrade
between LAN and WAN
6 Conclusions
Using the parallel-access approach to downloading data from multiple servers re-
duces transfer times and increases server resilience. The co-allocation architecture
provides a co-ordinated agent for assigning data blocks. A previous work showed that
the dynamic co-allocation scheme leads to performance improvements. However, it
cannot handle the idle time of faster servers having to wait for the slowest server to
deliver its final block. This paper proposes the dynamic adjustment strategy (DAS) to
improve file transfer performances using the co-allocation architecture in data grids.
In our approach, the workloads on selected replica servers are continuously adjusted
during data transfers, and our approach can also reduce the idle times spent waiting
for the slowest servers, and thus decrease file transfer completion times.
We also developed a new toolkit, called cyber-transformer that enables even inex-
perienced users to easily monitor replica source site statuses, manage replicas, and
download files from multiple servers in parallel. Experimental results show the effec-
Implementation of a dynamic adjustment strategy
19. Vazhkudai S, Schopf J (2003) Using regression techniques to predict large data transfers. Int J High
Perform Comput Appl (IJHPCA) 17:249–268
20. Vazhkudai S, Schopf J, Foster I (2002) Predicting the performance of wide area data transfers. In:
Proceedings of the 16th international parallel and distributed processing symposium (IPDPS 2002),
April 2002, pp 34–43
21. Vazhkudai S, Tuecke S, Foster I (2002) Replica selection in the Globus data grid. In: Proceedings
of the 1st international symposium on cluster computing and the grid (CCGRID 2001), May 2001,
pp 106–113
22. Venugopal S, Buyya R, Ramamohanarao K (2006) A taxonomy of data grids for distributed data
sharing, management, and processing. ACM Comput Surv 38(1):1–53
23. Wolski R, Spring N, Hayes J (1999) The network weather service: a distributed resource performance
forecasting service for metacomputing. Future Gener Comput Syst 15(5–6):757–768
24. Yang CT, Shih PC, Chen SY (2006) A domain-based model for efficient network information on
grid computing environments. IEICE Trans Inf Syst E89-D(2):738–742. Special issue on paral-
lel/distributed computing and networking
25. Yang CT, Yang IH, Chen CH, Wang SY (2006) Implementation of a dynamic adjustment mechanism
with efficient replica selection in co-allocation data grid environments. In: Proceedings of the 21st
annual ACM symposium on applied computing (SAC 2006)—distributed systems and grid computing
track, April 23–27, 2006, pp 797–804
26. Yang CT, Wang SY, Fu CP (2007) A dynamic adjustment mechanism for data transfer in data grids.
In: Network and parallel computing: IFIP international conference, NPC 2007, September 17–20.
Lecture notes in computer science, vol 4672. Springer, Berlin, pp 61–70
27. Yang CT, Yang IH, Li KC, Wang SY (2007) Improvements on dynamic adjustment mechanism in
co-allocation data grid environments. J Supercomput 40(3):269–280
28. Zhang X, Freschl J, Schopf J (2003) A performance study of monitoring and information services
for distributed systems. In: Proceedings of 12th IEEE international symposium on high performance
distributed computing (HPDC-12 ‘03), August 2003, pp 270–282
Chao-Tung Yang is a professor of Computer Science at Tunghai Uni-
versity in Taiwan. He was born on November 9, 1968, in Ilan, Tai-
wan, R.O.C. and received the B.Sc. degree in Computer Science from
Tunghai University, Taichung, Taiwan, in 1990, and the M.Sc. degree in
Computer Science from National Chiao Tung University, Hsinchu, Tai-
wan, in 1992. He received the Ph.D. degree in Computer Science from
National Chiao Tung University in July 1996. He won the 1996 Acer
Dragon Award for an outstanding Ph.D. Dissertation. He has worked as
an Associate Researcher for ground operations in the ROCSAT Ground
System Section (RGS) of the National Space Program Office (NSPO)
in Hsinchu Science-based Industrial Park since 1996. In August 2001,
he joined the faculty of the Department of Computer Science at Tunghai
University. He got the excellent research award by Tunghai University
in 2007. In 2007 and 2008, he got the Golden Penguin Award by Indus-
trial Development Bureau, Ministry of Economic Affairs, Taiwan. His
researches have been sponsored by Taiwan agencies National Science Council (NSC), National Center
for High Performance Computing (NCHC), and Ministry of Education. His present research interests are
in grid and cluster computing, parallel and multi-core computing, and Web-based applications. He is a
member of both the IEEE Computer Society and ACM.
