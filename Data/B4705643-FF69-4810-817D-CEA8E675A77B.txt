 2
一、中文摘要 
汽機車竊盜事件日益增加，停車場往往是汽車竊盜集團尋找獵物場所，賦予停車場監視
系統擁有完備之智慧型保全能力，進而建立智慧型開放性空間停車場監視系統實是刻不容
緩。本研究中，將針對戶外無人管理停車場設計一套智慧型戶外停車場監視系統，本研究建
構出一多台攝影機組之視訊監控系統，能夠對整個停車場景進行完備之監控任務，並對特定
目標物進行特寫鏡、追蹤、放大(zoom in)與縮小(zoom out)等工作，主要研發車輛與行人之偵
測、追蹤、與記錄等相關影像及視訊處理技術。本研究利用多組擺設不同方位之攝影機組，
每組攝影機組皆包含兩隻不同功能、性質攝影機，分別為「固定式全場攝影機」與「移動式
特寫攝影機」，其中，固定式全場攝影機主要功能用於監視整個監控場景，提供追蹤目標物之
位置資訊；而移動式特寫攝影機則追蹤、放大與縮小監視目標物，取得目標物之細部資料，
進而對行人做行為分析。本研究計畫的成果不僅能透過雙攝影機組自動偵測、追蹤與紀錄物
件，更進一步可以自動做物件判斷與行為分析，並使此系統於物件查詢、檢索、行為或特定
事件偵測皆能透過這些步驟而得到完美結果。 
關鍵詞：智慧型保全、全場攝影機、特寫攝影機、行為分析、物件偵測、查詢、檢索 
二、英文摘要 
Recently, the stealing events on cars are increasing in Taiwan according to the reports of 
governments. Most of the events occur at parking lot especially on the unsupervised ones. The main 
goal of this project is to give the intelligence of video surveillance and monitoring (VSAM) system. 
It is necessary and urgent to build up the intelligent systems for the unsupervised parking lots. We 
have constructed a prototype system for the opening space. A smart devise with two calibrated 
cameras is designed to grab the entire sense and the clearer, larger, and detail images of moving 
objects. The first one camera grabbing the entire sense detects and tracks the moving objects. The 
position and trajectory information is passed to the second one by the calibrated parameters such as 
the pan, title, and zoom data. It is unnecessary for the second camera to track the objects with the 
video processing or matching processes. Vehicles and pedestrians are the two elementary objects in 
parking lots. Based on the calibrated two-camera device, the high-level behaviors of objects are 
extracted and analyzed. In addition, the event detection techniques based on the object behavior are 
also developed. In this year, several devices will be used to obtain the image from different 
positions. Our system, not only the moving objects can be detected, tracked, and recorded, but also 
their detail behaviors will be identified. 
Keywords: surveillance, intelligent systems, event detection, two calibrated camera device 
 
 4
態轉換機率來計算輸入特徵所產生的一段 symbol sequence。過去 HMM 常用在語音以及
手勢辨識上，而最近常被使用在人體走勢方面的辨識。 
非監督式的方法大部分都使用 self-organizing neural network 來訓練資料，也有少部份使
用其他的訓練方法，以下分別介紹過去非監督式曾經所使用的方法： 
(1) Gaussian mixture model(GMM) : GMM 為一種機率密度分布的函式(PDF)，過去常用在語音
辨識上，對於軌跡的研究，一般來說會使用 PCA 或是 ICA 擷取出軌跡的特徵，並用 GMM
來 model 此 PCA 或是 ICA 所表示的軌跡，像是 Bashir et al. [4]將軌跡利用曲度分割成許多
區段，並用 PCA 來表示此區段軌跡，接著再用 GMM 來 model 此 PCA-based 的區段軌跡
分布。 
(2) self-organizing neural network(SOM) : SOM 為一種自我學習的非監督式類神經網路，可自
動的學習資料的分布，而不需事先定義資料的特性，像是 Johnson et al.[5]用特徵向量來表
示一個軌跡點的位置與速度的資訊，並用兩個競爭式的類神經網路來學習軌跡的分布，
Johnson 也將此方法應用到人與人之間的互動，像是握手等；Sumpter et al.[6]將 Johnson et 
al. [5]的第 2 層類神經網錄加入回饋的階段，使其更能表示物體的行為；而 Hu et al.[7]也
改良了[5]的類神經網路架構，使其訓練的速度更快也更有效率；Owens et al.[8]利用
self-organizing neural network 來找出特徵向量的分布，並用此分布來判斷是否有異常的軌
跡特徵。 
四、成果 
本研究計劃利用多組擺設不同方位之攝影機組，每組攝影機組皆包含兩隻不同功能、性
質攝影機，分別為「固定式全場攝影機」與「移動式特寫攝影機」，其中，固定式全場攝影機
主要功能用於監視整個監控場景，提供追蹤目標物之位置資訊；而移動式特寫攝影機則追蹤、
放大(Zoom in)與縮小(Zoom out)監視目標物，取得目標物之顏色等細部資料，進而對行人做
進一步的資訊萃取。本研究計畫共分為五個模組，分別為(1) 工作區域建立模組(Construction 
of Working Region)、(2) 目標物分割與追蹤(Moving Object Segmentation/Tracking)、(3) 目標
物特徵表示與分類(Object Representation and Classification)、(4) 運動軌跡擷取(Trajectory 
Extraction)與(5) 行為分析/事件偵測模組(Behavior Analysis/Event Detection)等五大模組。分別
針對各模組進行說明如下： 
I： 工作區域建立模組 
本研究計畫針對監控資料作訓練，將訓練所得監控場景位置出現物件之軌跡做一統計，
然後設定一範圍，而本系統便根據此範圍值判斷區域安全性，介於範圍值之區域其安全性便
較高，也就是此區域正常判斷不應該出現物件，一旦此區域出現物件便須傳送警訊予監控者，
因為其有可能屬於異常行為，而介於範圍值之區域便是指物件出現頻繁之區域，對於本計劃
來說，此區域有可能為車輛正常行經路線，故其安全性較低；而利用不同 ROI 區域設定不同
安全性不僅能提高監控環境安全性，相對的也能減少警訊誤報之機率。 
 6
  
(a) (b) 
圖一： 物件偵測 
IV：運動軌跡擷取與分類 
我們將 track 中所得到的中心點以及速度的資訊，用向量表示F=[x,y,dx,dy]，但是使用此
種方式表示速度(velocity)並無法完整呈現速率以及方向，大部分的方法將速度表示成包含速
率(speed 量值)以及方向(direction 正負)，為了突顯速率以及方向的特性，我們將每個軌跡點
的特徵正規化到0 到1 結合後即可形成F=[x,y,s,d]，s 表示速率、d 表示方向， 22 dydxs += , 
)/(tan 1 dxdyd −= 。為了使之後的訓練簡單化，首先將訓練資料的特徵向量長度正規化成相同的
長度，假設所有特徵向量中的最大長度為N，則每個特徵向量需再加上N-m個軌跡特徵(m 為
每個特徵向量的長度)，即假設最後 N-m 個軌跡特徵為第 m 個軌跡特徵的值，其意義為物
體停留在第 m 個軌跡點 N-m 時刻，當訓練完成後則會再調整特徵向量的長度，使其還原成
原本的長度值。 
本計畫將所有的正規化特徵向量帶入fuzzy self-organizing neural network 中訓練，其訓練
結果會顯示整體資料的分布，如圖二所示，(a)為場景中所有的軌跡，(b)為使用SOM 訓練後
的結果。 
  
(a) (b) 
圖二： 軌跡資料 
 8
[3] Wilson, A. F. Bobick, and J. Cassell, “Temporal classification of natural gesture and 
application to video coding,” IEEE conf. Computer Vision Pattern Recognition ,1997, pp. 
948-954 
[4] F. Bashir. A. Khokhar, D. Schonfeld, “Automatic object trajectory-based motion recognition 
using Gaussian mixture models＂, IEEE conf. Multimedia and Expo, 2005 
[5] N. Johnson and D. Hogg, “Learning the distribution of object trajectories for event recognition, 
Image Vision. Computing, 1996.  
[6] N. Sumpter and A. Bulpitt, ＂Learning spatio-temporal patterns for predicting object 
behavior,” Image Vision. Computing, 2000 
[7] W. M. Hu, D. Xie, and T. N. Tan, “A hierarchical self-organizing approach for learning the 
patterns of motion trajectories, IEEE Trans. on Neural Networks, 2004 
[8] J. Owsens and A. Huter, “Application of the self-organizing map to trajectory classification,” 
in Proc. IEEE Int. Workshop Visual Surveillance, 2000. 
[9] Stauffer and W. Grimson, “Adaptive background mixture models for real-time tracking＂in 
Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol.2, 1999, pp. 246-252. 
[10] Toyama, J. Krumm, B. Brumitt, and B. Meyers, “Wallflower: principles and practice of 
background maintenance,＂ in Proc. Int. conf. Computer Vision, 1999,pp. 255-261. 
UN
CO
RR
EC
TE
D 
PR
OO
F
quantize each vector xk, a codeword cj is selected
with the shortest distance dðxk; cjÞ between xk and
cj. The Euclidean distance is the most used metric,
i.e., cj ¼ qðxkÞ ¼ argminci2Ckxk  cik. Assume a
universal set U of all possible codebooks. The better
codebook is obtained by minimizing the square of
errors between two sets X and C,
C ¼ argmin
qj2U
1
Np
XNp
k¼1
dðxk; qjðxkÞÞ2
 !
. (1)
Linde et al. [1] proposed the famous Linde–Bu-
zo–Gray (LBG) algorithm to ﬁnd the codebook for
image compression. However, the results of VQ
methods are much affected by the initialization of
the codebook. They are frequently trapped in the
local solution resulting in poor performance. Be-
sides, the low-utilized codewords in the generated
codebook distort the decoded images. There three
problems are still present in the LBG algorithm.
Patane and Russo [2] proposed the enhanced LBG
(ELBG) algorithm to ﬁnd an optimal codebook by
means of shifting the low-utilized codewords to
another ones with high utility. This is a split-and-
merge-based algorithm for improving the utility of
codewords. Kaukoranta et al. [3] also proposed an
iterative algorithm combining with split and merge
operations (ISM) for the generation of a codebook.
Haber and Seidel [4] modiﬁed the LBG algorithm,
called ILBG, to reduce the codebook errors by a few
additional iteration steps. Huang et al. [5] proposed
a novel algorithm to improve the codeword utility
and the training performance by combining the
genetic algorithm and the simulated annealing
technique. The crossover and mutation operations
were based on the simulated annealing process. The
local optimal solution was avoided.
Recently, many researchers have tried to solve the
problems in ﬁnding the optimal solution using the
neural network-based learning approaches. Lin and
Yu [6] proposed a centroid neural network adaptive
resonance theory (CNN-ART) to improve the
performance of VQ. CNN-ART, an unsupervised
and competitive neural network model, considered
the weights of neurons as the codewords. Although
CNN-ART relieved the dependence on the initiali-
zation of the codebook, it was still affected as can be
seen from the results. Besides, there is a low-utility
problem in their proposed approach with poor
initialization. Laha et al. [7] designed a codebook
using a self-organizing feature map technique.
During the training process, the weights of nodes
were considered to be the codewords. All weights of
nodes built up the codebook. However, the block-
effects frequently distorted the reconstructed
images. This problem was solved by a polynomial
surface ﬁtting technique [7].
On the other hand, researchers tried to speed up
the process of ﬁnding the optimal solution. The tree
search vector quantizer (TSVQ) was the improbable
algorithm using a tree structure [8]. Chang and Lin
[9] eliminated most of the impossible codewords to
decrease the matched number for speeding up the
searching process. Chan and Ma [10] proposed a
fast maximum descent (MD) approach to quickly
ﬁnd the codebook. Pan et al. [11] modiﬁed the L2-
norm pyramid encoding approach to decrease the
searching time. Chen [12] utilized the fuzzy reason-
ing technique to predict the codewords for improv-
ing the searching performance. Huang and Chang
[13] proposed a color ﬁnite-state LBG (CFSLBG)
algorithm to reduce the searching time of the LBG
algorithm.
Since the compression performance is much
affected by the codebook, the codebook generation
has a key role in VQ. In summary, four problems
frequently occur in the clustering process. They are
(1) the initial condition, (2) the local optimum
solution, (3) the low-utilized codeword, and (4) the
sequence of training vectors problems. Given a
training set X ¼ fx1; x2; . . . ; xNpg, Np elements are
partitioned into Nc subsets and the sum of squared
errors in Eq. (1) should be minimized. There are
ðNcÞNp=Nc ways for partition. It is infeasible to ﬁnd
the optimal partition by an exhaustive searching
approach. Iterative optimization is the most fre-
quently used approach. Nc initial codewords
c1; c2; . . . ; cNc are randomly guessed. At each itera-
tion, Np training vectors are sequentially assigned to
the nearest codeword and a new codeword is re-
calculated as the mean of all instances belonging to
the same cluster. These two steps are repeated until
the codewords stabilize. The iterative procedure
guarantees local but not global optimization. The
found solution highly depends on the initial condi-
tion Ci and the sequence of training vectors
x1; x2; . . . ; xNp . They are the well-known problems
of initial condition, local optimization, and sample
sequence. In addition to the sum-of-squared-error
criterion, the codeword utility is another indication
to evaluate the total distortions related to codeword
cj. The equalization of the codeword distortions is
equivalent to the equalization of the codeword
utilities. According to the consequences in Ref. [2],
ARTICLE IN PRESS
SIGPRO : 3084
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
85
87
89
91
93
95
97
99
101
103
Please cite this article as: Chin-Chuan Han et al., A novel approach for vector quantization using a neural network, mean shift, and
principal component analysis-based seed..., Signal Processing (2006), doi:10.1016/j.sigpro.2006.08.006
C.-C. Han et al. / Signal Processing ] (]]]]) ]]]–]]]2
UN
CO
RR
EC
TE
D 
PR
OO
F
codebook was initialized to one. The winner node
was rewarded with a positive learning gain and the
loser nodes were punished with a negative one for
the competitive learning rules. This approach is a
threshold criterion-based clustering approach. A
centroid node is increased as a new cluster when
the Euclidean distances between an input vector and
the existing nodes are larger than a vigilance value.
The CNN-ART approach repeated the incremental
process until the codebook size was Nc. The
algorithm is summarized as follows:
1. Initialization: Initialize the following variables:
the codebook size Nc, the initial codebook C
0 ¼
; , the training set X ¼ fxk : k ¼ 1; 2; . . . ; Npg, a
pre-deﬁned threshold v, and the iteration index
t ¼ 0.
2. Clustering: Given the codebook Ct at iteration t,
calculate the Euclidean distances between vector
xk and the weights in the network. Assign vector
xk to the node with the smallest distance.
3. Node increment or weight updating: If the smallest
distance was larger than threshold v, and the
node number was smaller than value Nc, generate
a new node. Otherwise, update the weights of a
winner with the rewarding rules and those of
losers with the punishing rules.
4. Check the stop criterion: When the state is stable,
the process is terminated.
5. Generate the codebook: If the codebook size was
Nc, and a converged and stable state occurred,
the codebook is assigned as the set of all nodes’
synaptic weights.
Due to the scenarios of the ELBG algorithm and
the CNN-ART algorithm, four problems should be
solved in the VQ process. To overcome these
problems and to improve the performance of the
ELBG or the CNN-ART algorithms, a hybrid
algorithm was developed in the following.
3. PNM for VQ
The architecture of PNM is composed of a PCA-
based seed re-initialization module, an NN-based
clustering module, and an MS-based reﬁnement
module. They were iteratively performed to ﬁnd the
optimal solution.
3.1. NN-based clustering
A simple MINNET network is a competitive
learning algorithm. It determines the nearest dis-
tance between an input vector and those in the
neuron’s weights for the output layer. The dimen-
sions of the input layer is m the same as the number
of each neuron’s synaptic weights because of the full
connection between them. Therefore, the weights in
a neuron are considered as a codeword. The main
difference between the PNM and the CNN-ART
architecture is the number of initial neurons. CNN-
ART repeatedly increases the neurons until Nc
nodes (the codebook size). Whereas Nc neurons are
initialized in PNM, and their weights are randomly
initialized. The functions of MINNET in PNM are
the same as those in CNN-ART. All neurons are
completely interconnected in MINNET. Each neu-
ron got the values from its original neuron and the
lateral inhibition ðÞ from the other neurons. The
output value O
ðtÞ
j of the jth neuron at iteration t is
thus given as follows:
O
ðtÞ
j ¼ f t Oðt1Þj  
X
iaj
O
ðt1Þ
i
 !
and
i; j ¼ 1; 2; . . . ; Nc; tX1, ð2Þ
f tðbÞ ¼
b if bo0;
0 otherwise:

(3)
Here, the initial condition was set as
O
ð0Þ
j ¼ kxk  wjk. (4)
Value Nc denotes the number of neurons in net
MINNET, and 41=Nc. Each node was repeatedly
compared with the other nodes until only a negative
output was generated. Meanwhile, the other neu-
rons all outputted zero. The node with the negative
output was the desired codeword.
Next, let us describe the learning rules in PNM.
Similar to the rules in CNN-ART and LBG
algorithms, the rewarding equation is written below:
w
ðtÞ
j ¼ wðt1Þj þ
1
jcjj þ 1
½xk  wðt1Þj ,
k ¼ 1; 2; . . . ; Np; j ¼ 1; 2; . . . ; Nc, ð5Þ
and value 1=ðjcjj þ 1Þ is its learning rate.
The above learning approach plays a clustering
role in PNM. The training vectors were sequentially
clustered with the cluster centers (e.g., codewords,
the weights of nodes).
ARTICLE IN PRESS
SIGPRO : 3084
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
85
87
89
91
93
95
97
99
101
103
Please cite this article as: Chin-Chuan Han et al., A novel approach for vector quantization using a neural network, mean shift, and
principal component analysis-based seed..., Signal Processing (2006), doi:10.1016/j.sigpro.2006.08.006
C.-C. Han et al. / Signal Processing ] (]]]]) ]]]–]]]4
UN
CO
RR
EC
TE
D 
PR
OO
F
on the ﬁrst axis f1 with 1D value l was also a point
with the coordinate lf1 of dimension m in the
original space. Similarly, the samples were projected
and were re-clustered on the other axes to obtain the
new candidate centers. The distance variances of the
3Nm=Ns candidate clusters were calculated and
sorted. Finally, Nm=Ns candidate centers with the
ﬁrst Nm=Ns smallest variances were selected to be
the new centers.
3.3.2. Adaptive learning rate for seed selection
In this study, the adaptive learning rules were
designed to avoid the divergence. The generation
rate r was adapted due to the distortion d between
two iterations. The distortion d is deﬁned as
d ¼ ð1=NpÞ
PNp
k¼1dðxk; qt1ðxkÞÞ  ð1=NpÞ
PNp
k¼1dðxk; qtðxkÞÞ
ð1=NpÞ
PNp
k¼1dðxk; qt1ðxkÞÞ
,
(10)
q is the mapping function for vector xk and its
corresponding codeword. The adaptive rules de-
scribed in [16] were utilized to vary the learning rate.
Three parameters x;k, and Z should be determined
and three conditions were considered as follows:
1. If the distortion d increased by more than
parameter x ¼ 0:04, the learning rate was multi-
plied by parameter Z ¼ 1:05. The rate r would be
high to generate more possible new seeds. In
addition, the new seeds generated in this iteration
were discarded.
2. If the distortion d decreased, the new seeds were
accepted. The learning rate was multiplied by
parameter k ¼ 0:9. The number of new seeds was
decreased according to the decreasing rate.
3. If the distortion increased by less than parameter
x, the rate was unchanged for ﬁnely adapting the
seed positions.
In summary, these three modules were iteratively
performed. The NN module played a clustering
role, the MS module reﬁned the codewords, and the
PCA module assigned the new and better seeds. The
PCA module tried to ﬁnd the possible codewords
near to the optimal solution. According to the
sample distribution, the large clusters were split into
several smaller clusters to reduce the distortion.
Besides, it discarded the codewords with low utility
and re-assigned the better ones with high utility.
PCA module could solve the problems of codebook
initialization, codeword utility, and local optimiza-
tion. The MS module moved each codeword toward
a better position with a high sample density. It could
improve the performance of codeword utility and
solution optimization. Since these modules were
iteratively performed, a near global optimal solu-
tion was found, the low-utilized codewords were
decreased, the poor initial seeds were discarded, and
the inﬂuences of sample sequences were decreased.
Thereafter, the proposed iterative scheme could
solve the four problems of clustering and designed
as follows:
3.3.3. Algorithm of PNM
Input : A training set X ¼ fx1; x2; . . . ; xNpg of size
Np.
Output : A set of cluster centers C ¼ fc1; c2; . . . ; cNcg
of size Nc.
Step 1: Initialize the parameter r.
Step 2: Cluster the training samples sequentially
to obtain the cluster centers using the NN-based
clustering.
Step 3: For each codeword cj, compute the MS
vector using the training samples in cluster cj, and
shift to the next position.
Step 4: PCA-based seed re-initialization.
Step 4.1: Determine Ns split and Nm ¼ rNc
merged clusters based on their size, e.g. codeword
utility.
Step 4.2: For each split cluster cj:
1. Compute the eigenvectors and eigenvalues from
the samples in cluster cj.
2. Project the samples to the ﬁrst three eigenvectors
f1;f2, and f3, e.g. projection axes.
3. On each projection axis, the samples were
clustered using the 1D projected values.
4. Select Nm=Ns candidate centers with the smallest
variances to be the new seeds in the next
iteration.
Step 4.3: Ignore Nm codewords with low utility.
Step 5: Re-calculate the parameters r and d.
Step 6: Repeat Steps 2–6 until a converged state
occurs, e.g. do0:01.
4. Experimental results
In this section, some experiments were conducted
to show the efﬁciency of the proposed method. The
PSNR value is the popular measurement to evaluate
ARTICLE IN PRESS
SIGPRO : 3084
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
85
87
89
91
93
95
97
99
101
103
Please cite this article as: Chin-Chuan Han et al., A novel approach for vector quantization using a neural network, mean shift, and
principal component analysis-based seed..., Signal Processing (2006), doi:10.1016/j.sigpro.2006.08.006
C.-C. Han et al. / Signal Processing ] (]]]]) ]]]–]]]6
UN
CO
RR
EC
TE
D 
PR
OO
F
the generality of the codebook generated by PNM is
more robust than the others.
The last three experiments were conducted to
compare the algorithms on the codebook initializa-
tion, the codeword utility, and the sequence of
training vectors problems. Similar to the ﬁrst three
experiments, 10 algorithms were executed for
different factors in various codebook sizes. These
results are shown in Tables 4–6.
The statistical utility rates of codewords were
obtained in various compression rates as listed in
Table 4(a). The ‘Lena’ image was encoded and
decoded in this experiment. The compression rates
were set as 0.5625, 0.625, and 0.6875BPP. From this
table, the PNM algorithm generated more highly
utilized codewords than the other algorithms in
various rates. The codebook generated by the PNM
comprises fewer low-utilized codewords. In order to
show the invariance of the initial codebook, 30
initial codebooks were randomly initialized. Image
‘Lena’ was used to generate the encoding codebooks
by 10 algorithms. The PSNR values for 30 initial
ARTICLE IN PRESS
SIGPRO : 3084
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
85
87
89
91
93
95
97
99
101
103
Please cite this article as: Chin-Chuan Han et al., A novel approach for vector quantization using a neural network, mean shift, and
principal component analysis-based seed..., Signal Processing (2006), doi:10.1016/j.sigpro.2006.08.006
Table 1
The comparison of 10 algorithms for encoding two images ‘Lena’ and ‘Pepper’ in various codebook sizes
Size LBG ELBG GVQ GSAVQ C-ART CAL ILBG ISM MD PNM
(a) Image ‘Lena’
32 25.77 27.85 27.72 27.90 26.47 26.57 27.51 26.35 27.56 28.86
64 27.65 28.93 28.70 28.73 28.05 28.15 28.12 27.92 28.61 30.54
128 28.74 29.80 29.84 30.00 28.94 29.04 29.02 29.01 29.65 31.66
256 30.19 32.10 32.21 32.39 30.69 30.89 31.05 30.55 32.05 32.76
(b) Image ‘Pepper’
32 25.67 27.65 27.62 27.61 26.27 26.47 27.42 26.24 27.48 28.75
64 27.64 28.53 28.75 28.52 28.15 28.23 28.03 28.16 28.51 30.32
128 28.54 29.85 29.75 30.13 28.56 28.94 28.99 28.91 29.55 31.25
256 30.49 32.32 32.32 32.09 30.25 30.09 31.98 30.45 32.01 32.62
C-ART, CNN-ART; CAL, CNN-ART-LBG.
Table 2
The PSNR values for various images encoded by the codebook generated by the training vectors of image ‘Lena’
Size LBG ELBG GVQ GSAVQ C-ART CAL ILBG ISM MD PNM
(a) Image ‘Pepper’
32 24.62 26.88 26.69 26.73 25.17 25.37 25.84 25.25 26.55 27.97
64 26.48 27.95 27.61 27.72 27.02 27.15 27.12 26.95 27.48 28.94
128 27.60 28.73 28.74 28.86 27.84 27.96 28.01 27.85 28.65 29.86
256 29.11 31.09 31.36 31.44 29.89 30.01 30.84 29.85 31.03 31.91
(b) Image ‘F16’
32 25.12 27.85 27.29 27.53 25.52 25.92 26.85 25.64 27.15 28.01
64 26.95 28.99 28.01 28.52 27.35 27.75 27.85 27.41 27.94 29.55
128 28.01 29.83 29.14 29.56 28.51 28.81 28.86 28.55 29.06 30.95
256 29.91 31.92 31.16 31.54 30.31 30.71 30.78 30.35 31.05 32.81
(c) Image ‘Sailboat’
32 23.02 25.75 25.19 25.33 23.42 23.82 24.85 23.65 25.15 26.03
64 24.75 26.79 26.02 26.32 25.05 25.55 25.74 25.44 26.01 27.65
128 25.81 27.63 27.04 27.46 26.21 26.62 26.95 26.43 27.02 28.85
256 27.71 29.82 29.06 29.44 28.11 28.55 28.74 27.43 29.03 30.75
(d) Image ‘Tiffany’
32 25.72 27.78 27.65 27.73 26.02 26.42 27.13 26.34 27.52 27.78
64 27.58 28.85 28.71 28.69 27.85 28.15 28.43 27.92 28.53 29.31
128 28.70 29.63 29.64 30.46 29.12 29.42 29.44 29.23 29.55 29.92
256 29.91 31.95 31.26 31.84 30.21 30.51 30.73 30.32 31.22 32.72
C.-C. Han et al. / Signal Processing ] (]]]]) ]]]–]]]8
UN
CO
RR
EC
TE
D 
PR
OO
F
5. Conclusions
In this paper, a novel algorithm has been
proposed to ﬁnd the better solution for VQ. Four
problems in the clustering process have been solved
using the NN-based clustering, the MS-based
reﬁnement, and the PCA-based seed re-initialization
modules. These three modules were repeatedly
performed to obtain the better codebook. Some
experiments were conducted to make a comparison
with the other algorithms on the codebook initi-
alization, the found solution, the codeword utility,
and the sequence of samples problems. From the
experimental results, the proposed algorithm per-
formed better than the others under the PSNR
criterion.
ARTICLE IN PRESS
SIGPRO : 3084
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
65
67
69
71
73
75
77
79
81
83
85
87
89
91
93
95
97
99
101
103
Please cite this article as: Chin-Chuan Han et al., A novel approach for vector quantization using a neural network, mean shift, and
principal component analysis-based seed..., Signal Processing (2006), doi:10.1016/j.sigpro.2006.08.006
Table 4
The comparisons for 10 algorithms for different factors encoding and decoding the image ‘Lena’
BPP LBG ELBG GVQ GSAVQ C-ART CAL ILBG ISM MD PNM
(a) The utility rates in various compression rates
0.5625 81.50 96.67 96.57 96.58 85.45 87.65 90.51 86.15 96.45 97.52
0.625 78.58 95.45 95.51 95.61 81.88 83.58 89.15 82.56 95.28 96.12
0.6875 76.84 94.55 94.12 94.34 79.65 81.75 88.54 80.43 94.05 95.24
(b) The variance of PSNR values using randomly initialized codebooks
Size
32 4.51 1.58 1.15 1.05 4.11 4.01 2.15 2.57 1.65 0.95
64 4.12 1.33 1.12 0.95 3.82 3.72 2.06 2.24 1.54 0.83
128 3.85 1.15 0.93 0.87 3.55 3.35 1.95 2.13 1.34 0.72
256 3.53 0.91 0.85 0.76 3.23 3.13 1.56 2.06 1.23 0.65
(c) The variance of PSNR values using different sequences of training vectors
32 4.21 1.28 1.04 1.01 4.01 3.85 2.21 2.68 1.78 0.96
64 4.02 1.13 1.01 0.89 3.52 3.02 2.18 2.34 1.65 0.82
128 3.55 1.05 0.85 0.81 3.21 2.95 2.01 2.24 1.54 0.70
256 3.23 0.81 0.78 0.71 3.03 2.85 1.72 2.21 1.35 0.61
BPP: Bits per pixel, %.
Table 5
The comparisons of an inside test for different factors
BPP LBG ELBG GVQ GSAVQ C-ART CAL ILBG ISM MD PNM
(a) The utility rates in various compression rates
0.5625 81.45 96.62 96.51 96.52 85.39 87.59 90.45 86.11 96.39 97.47
0.625 78.52 95.41 95.46 95.55 81.82 83.51 89.09 82.52 95.22 96.07
0.6875 76.78 94.49 94.07 94.28 79.61 81.69 88.49 80.37 94.01 95.19
(b) The variance of PSNR values using randomly initialized codebooks
Size
32 4.54 1.62 1.18 1.09 4.15 4.05 2.18 2.61 1.69 0.98
64 4.15 1.36 1.16 0.99 3.86 3.76 2.09 2.28 1.58 0.87
128 3.88 1.19 0.97 0.91 3.58 3.38 1.98 2.17 1.38 0.76
256 3.57 0.95 0.89 0.79 3.27 3.17 1.61 2.11 1.26 0.69
(c) The variance of PSNR values using different sequences of training vectors
32 4.26 1.34 1.09 1.06 4.07 3.92 2.27 2.74 1.83 1.01
64 4.07 1.18 1.07 0.95 3.58 3.07 2.24 2.39 1.71 0.88
128 3.61 1.11 0.91 0.86 3.27 3.01 2.07 2.31 1.61 0.76
256 3.29 0.87 0.84 0.77 3.09 2.91 1.78 2.27 1.41 0.67
BPP: Bits per pixel, %.
C.-C. Han et al. / Signal Processing ] (]]]]) ]]]–]]]10
UN
CO
RR
EC
TE
D 
PR
OO
F
[3] T. Kaukoranta, P. Franti, O. Nevalainen, Iterative split-and-
merge algorithm for vector quantization codebook genera-
tion, Optical Eng. 10 (1998) 2726–2732.
[4] J. Haber, H.-P. Seidel, Using an enhanced lbg algorithm to
reduce the codebook error in vector quantization, in:
Proceedings of the IEEE International Computer Graphics
Conference, 2000, pp. 99–104.
[5] H.-C. Huang, J.-S. Pan, Z.-M. Lu, S.-H. Sun, H.-M. Hang,
Vector quantization based on genetic simulated annealing,
Signal Processing 81 (2001) 1513–1523.
[6] T.-C. Lin, P.-T. Yu, Centroid neural network adaptive
resonance theory for vector quantization, Signal Processing
83 (2003) 649–654.
[7] A. Laha, N.R. Pal, B. Chanda, Design of vector quantizer
for image compression using self-organizing feature map and
surface ﬁtting, IEEE Trans. Image Process. 13 (2005)
1291–1303.
[8] A. Buzo, A.H. Gray, R.M. Gray, J.D. Markel, Speech
coding based upon vector quantization, IEEE Trans.
Acoust. Speech Signal Process. ASSP-92 (1980) 562–574.
[9] C.-C. Chang, I.-C. Lin, Novel full-search schemes for
speeding up image coding using vector quantization, Real
Time Imaging 10 (2004) 95–102.
[10] C.K. Chan, C.K. Ma, A fast method of designing better
codebooks for image vector quantization, IEEE Trans.
Comm. 42 (1994) 237–242.
[11] Z. Pan, K. Kotani, T. Ohmi, Fast encoding method for
vector quantization using modiﬁed L2-norm pyramid, IEEE
Signal Process. Lett. 12 (2005) 609–612.
[12] P.-Y. Chen, An efﬁcient prediction algorithm for image
vector quantization, IEEE Trans. Systems Man Cybernet. 34
(2004) 740–746.
[13] Y.-L. Huang, R.-F. Chang, A fast ﬁnite-state algorithm for
generating RGB palettes of color quantized images, J.
Inform. Eng. 20 (2004) 771–782.
[14] G.A. Carpenter, S. Grossberg, The ART of adaptive pattern
recognition by a self-organization neural network, Compu-
ter 21 (1988) 77–88.
[15] D. Commaiciu, P. Meer, Mean shift: a robust approach
toward feature space analysis, IEEE Trans. Pattern Anal.
Machine Intell. 24 (2002) 603–619.
[16] M.T. Hagan, H.B. Demuth, M.H. Beale, Neural Network
Design, Thomson Learning, 1996.
ARTICLE IN PRESS
SIGPRO : 3084
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Please cite this article as: Chin-Chuan Han et al., A novel approach for vector quantization using a neural network, mean shift, and
principal component analysis-based seed..., Signal Processing (2006), doi:10.1016/j.sigpro.2006.08.006
C.-C. Han et al. / Signal Processing ] (]]]]) ]]]–]]]12
Wallflower algorithm find the foreground object in 
three levels: pixel level, region level, and frame level. 
The foreground pixels were found using the color 
information in the pixel level. The object relocation 
problem was solved in the region level. The lighting 
effect was solved in the frame level. The main goal is to 
reduce the detection errors of foreground. Haritaoglu et 
al.[8] utilized the minimum intensity, the maximum 
intensity, and the maximum intensity difference to 
model the background pixels. Mckenna et al. [9] used 
the color and edge information for background 
construction. All of their approach would like to reduce 
the lighting effects. 
 
In this paper, a histogram-based background image 
was constructed for detecting the foregrounds. First, a 
median filter was performed for removing the noises. 
The differentiating between two images is calculated. 
The pixel with small differentiating value was treated as 
a background pixel. The statistical histograms 
),,( kyxH tc for a pixel ),( yx  on RGB channels are 
counted at frame .255,,1,0},,,{, K=∈ kBGRct  The 
bin with the highest number on each RGB channel is 
assigned as the background value from N sequential 
images. If the differentiating value on the RGB color 
space between the current frame and the background 
image is large than a pre-determined value, the averaged 
value from the previous N frames, this pixel is classified 
as the foreground pixel. Otherwise, it is a background 
pixel and should be updated in the background updating 
process. 
 
The background updating is to keep the effectiveness 
of detection against the change of spaces. The strategies 
is designed as follows: If a pixel ),( yx  belongs to the 
background pixels whose RGB values are ),( yxIc , the 
corresponding bins are increased by one. At the same 
time, the other bins for this background pixel are 
decreased by 1. The updating rules for the background 
pixels are designed as follows:  
 
⎪⎩
⎪⎨
⎧
−
=+= −
−−
;otherwise1),,(
),,(if1),,(
),,(
1
11
kyxH
yxIkkyxH
kyxH
t
c
t
c
t
ct
c
 (1)
.255,,1,0},,,{
),,,(maxarg),( 1
K=∈
= −
kBGRc
kyxHyxB tck
t
c  (2)
 
The updating process for the foreground pixels is 
ignored. Next, the shadow problem is a crucial problem 
because the image data frequently suffer from lighting. 
The shadow pixels are frequently mis-classified as the 
foreground points. The shadow pixels were identified by 
using the Horprasert et al.'s approaches [10]. In their 
approaches, the distortion values of brightness and 
chromaticity were calculated from the statistical data. 
The pixels in moving regions were classified into the 
original background, the shadow background, the 
highlight background and the foreground objects using 
the original color data. In addition, the approach 
proposed by Prati [11] et al. used the following rules to 
remove the shadows. (1) The shadow pixels are the 
pixels whose brightness is smaller than the backgrounds'. 
(2) The variances of chrominance of shadow pixels are 
small. The brightness and chromatic distortions are thus 
defined as follows: 
 
),(
),(),(),(),(
yxB
yxByxIyxByxBr ⋅−=δ  (3)
.
),(),(
),(),(cos),( 1 ⎟⎟⎠
⎞
⎜⎜⎝
⎛ ⋅= −
yxByxI
yxByxIyxCrδ  (4)
 
The shadow pixels are determined in the following 
rules: 
    
.),(     
and ,),(0
0
0
θδ
γδ
<
<<
yxCr
yxBr
 (5)
 
3 Trajectory Feature Extraction 
 
After finding the foreground objects, the next step is 
to continuously track the objects. In general, Kalman 
filter, particle filter, or dynamic Bayesian network are the 
popular methods to track the objects. In addition, four 
types of matching approaches are used during the 
tracking process. These four type approaches could be 
integrated to improve the tracking performance. They are 
(1) region-based, (2) contour-based, (3) feature-based, 
and (4) model-based approaches. The centers of 
foreground objects ),( ii yx  are extracted using the 
tracking algorithms. Three representations of trajectories, 
the point-based [5], the curve-based [12], and the spatio- 
temporal-based [3, 4] representations, are frequently 
used in many studies. In this study, the last one is utilized 
to represent the object trajectories. 
 
First of all, an object center position and its velocity 
at each trajectory point can be represented in a vector 
form ],,,[ iiii yxyx δδ . However, this vector could not 
completely represent the object movement such the 
direction, speed, trajectory, etc. The velocity values are 
calculated as 1−−= iii xxxδ  and 1−−= iii yyyδ  in many 
reported papers. In this study, these values are 
re-formulated as the speed 22 iii yxs δδ += , and the 
direction )/(tan 1 iii xyd δδ−= . The trajectory features are 
represented as a vector of length m, ,,,[ 111 syxTg =  
],,,,,1 mmmm dsyxd K . Since the lengths of trajectory 
vectors are different, vector gT  is extended to a new 
vector of length 4n, where value n is the maximal 
number of trajectory points. n-m points with the positions 
of the last point, zero speed, and zero directional angle, 
are appended to the original data as follows: 
       
 
4.3 Abnormal Activity Detection 
 
Consider an input trajectory of m points 
)],(,),(),,(),,[( 3322110 mm yxyxyxyxT K= . This vector 
is converted to a new vector ,,,,[ 1111 dsyxTg =  
],,,, mmmm dsyxK  and extended to a vector ,[ 1xTn =  
],,,,,,, 111 nnnn dsyxdsy K  of length n. The extended 
vector is inputted the FSOM to find the winner node 
with the minimal Euclidean distance jD . If the 
criteria jj qmD >/ is satisfied, this trajectory is an 
abnormal activity. Here value jq  is a threshold value 
generated from the training samples. That means: If the 
input trajectory is close enough to the winner node jD , 
it is classified as a normal trajectory. Otherwise, it is an 
abnormal one. The threshold value jq  for node jD  is 
determined as follows: Find the distance ijD  for the 
trajectory iT  of length m whose winner node is jD . 
The threshold value jq  for node jD  is assigned as 
the maximal distance of all normalized distance mDij / . 
 
In addition, a partial trajectory could be classified 
which prototype it is by computing its probability. 
Consider a trajectory ,,,,,,,,[ 1111 mmmg syxdsyxT K=  
]md . Calculate the distances between the trajectory and 
the nodes of FSOM. The distance is calculated as the 
weighted sum between the trajectory vector and the 
weight vector of each node as follows: 
 
( )
)()(       
)()()(
2
,4
1
2
,14
2
,24
2
,34
iKvd
vsvyvxr
jii
m
i
jiijiijiij
−+
−+−+−= ∑
=
−−−
 
 
In order to obtain the high accuracy rate, the newer 
trajectory points are assigned with the larger weighted 
values. The weights is decreased with the time as 
 
.,,2,1for  ,)( SieiK S
Si
K==
−−
 
 
Therefore, the probability for each prototype is 
calculated as 
 
.,,2,1for  ,
/1
/1
1
Cj
r
r
P C
i i
j
j K== ∑ =  
    
5 Experimental Results 
 
100 images with moving objects were collected to 
construct the background image using the 
histogram-based technique. The threshold value in the 
experiments was set in a range 10 to 20. Four 
illustrations of background construction and foreground 
detection are given as shown in Fig. 1. In addition, the 
background update in the proposed approach is needless. 
The foreground pixels (read points) and the shadow 
pixels (blue points) are effectively classified as shown 
in Fig. 1(c). The parameters in this experiment are set as 
),( 00 θγ = (0.3, 0.04), and the parameters for the dark 
pixels are set as (0.35, 0.1). 
 
To illustrate the detection of abnormal events, 30 
video samples with normal trajectory features were 
extracted for training. A space image and three trained 
prototypes of normal trajectories are illustrated in Fig. 
2(a) and (b), respectively. On the other hand, 30 normal 
and 12 abnormal samples were tested to show the 
detection results of the proposed method. Some 
examples of abnormal activities are illustrated in Fig. 3. 
In Fig. 3(a), a car turned left into a parking lot of 
bicycles. Similarly, a car turned right into the parking 
lot (See Fig. 3(b)). Next, a car stopped at the restricted 
zone (the yellow zone) as shown in Fig. 3(c). The speed 
of a car is slowed down due to a motorcycle as shown in 
Fig. 3(d). Two cars moved in an illegal 'U' turn as 
shown in Figs. 3(e) and 3(f), respectively. Furthermore, 
the prototypes of input trajectories are also predicted as 
shown in Fig. 4. In Fig. 4(a), the probability for the 
vehicle located at the green path is 32%. It is increased 
to 36% at the blue path. In the other example, the 
vehicle moves straight foreword. Its prototype 
probability is increased from 81% to 99%. 
 
The algorithms were implemented on a PC-based 
machine of Pentium IV, 3.0 GHZ, and 1G RAM. The 
averaged execution time needs about 0.047 to 0.062 
seconds per frame. 28 of the 30 normal activities were 
correctly determined. On the contrary, 11 of the 12 
abnormal activities were correctly detected. The false 
rejection rate (FRR) is 2/30=6%, and the false 
acceptance rate (FAR) is 1/12=8.3%, respectively. The 
main mis-detection occurred at the classification error 
of foreground pixels. That led to the incorrect trajectory 
representation and the incorrect detection results. 
 
6 Conclusions 
 
In this paper, a FSOM verifier has been proposed for 
determine the normal/abnormal trajectories of objects. 
The video data were automatically segmented to 
represent the objects’ trajectories. The prototypes of 
normal activities were trained from the training samples. 
The verifier could be constructed from the video data in 
various monitoring spaces. 
 
References 
 
[1] F. Bashir, A. Khokhar, and D. Schonfeld, 
“Automatic object trajectory-based motion 
recognition using Gaussian mixture models,” in 
Proc. of IEEE Conf. Multimedia and Expo, 2005. 
