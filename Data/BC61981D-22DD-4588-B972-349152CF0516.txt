I 
 
中文摘要及關鍵字 
 
摘要 
本計畫綜合了無失真/近乎無失真壓縮編碼、千兆級無線通訊系統、以及基於Forth之WHDVI編譯
器以及核心等整合性系統。 
首先在無失真/近乎無失真壓縮模組方面，已經實現一decode/encode hybrid codec晶片。支援
QFHD@30fps之hybrid codec更獲得IEEE TCSVT之肯定。此hybrid codec相關成果也曾獲邀參與ASSCC
之口頭報告。在競賽方面，此壓縮模組獲得教育部嵌入式競賽佳作以及旺宏金矽獎優勝，並期許將來
能夠進一步支援多用戶之不同環境需求。 
Wireless transmitting/receiving module部份除了已經實現兩多輸入輸出關鍵性組件之晶片。其中高
吞吐率之QR分解模組之論文已獲IEEE TCAS-I所接收，另外支援高傳輸率之球面解碼器已發表於
ISCAS 2010上。在競賽方面所實現之QR分解模組已經獲得旺宏金矽獎優勝之肯定。在技術上，提出之
QR分解模組成功完成以四個時脈周期進行4x4通道矩陣的分解動作，係目前文獻上以最少時脈週期完
成之作品。 除了現有成果外，此團隊並繼續投入多天線數的球面解碼器之晶片設計。 
藉由Embedded compression高解析度畫面壓縮、高速無線傳輸多輸入輸出關鍵性組件之設計、發送
機與接收機之基頻處理器設計、以及WHDVI-forth等子系統之結合，此計畫所完成之系統能在5.5G系統
頻帶下，達到6Gbps多媒體資料傳輸之頻寬，且支援QFHD (3840x2160) @ 30Hz之畫面要求。將來並期
望能夠朝向Scalable video multi-source support以及high-frequency mobile communication system之系統規
劃建制。 
關鍵字 
無失真壓縮, 些微失真壓縮, 高畫質影像串流控制, 軟硬體整合及流程規劃,參數化Memory/Circuit
控制單元 , 多輸入輸出技術 , 正交分頻多工系統 , 千兆級位元傳輸 , QR分解 , 球面解碼器 , 
WHDVI-Forth核心, 多核心處理器, WHDVI-Forth編譯器 
III 
 
目錄 
中文摘要及關鍵字 ................................................................................................................................................. I 
Abstract and keyword ............................................................................................................................................. II 
目錄 ..................................................................................................................................................................... III 
第 1 部份. 報告內容 ..............................................................................................................................................1 
第 1 章. 前   言 ................................................................................................................................................1 
第 2 章. 研究目的 ..............................................................................................................................................2 
第 3 章. 研究方法 ..............................................................................................................................................4 
第 1 節. 子計畫一 ...........................................................................................................................................5 
1-3-1-1. The Description of Prediction Template ........................................................................ 5 
1-3-1-2. The Description of Probability Model ........................................................................... 5 
1-3-1-3. The Description of Advanced Adjusted Binary Code .................................................... 6 
1-3-1-4. The Description of Content Adaptive Golomb-Rice Code ............................................ 9 
1-3-1-5. 演算法流程及硬體架構............................................................................................. 12 
1-3-1-6. System level inter-module communication interface design and ESL design flow 
construction/application ............................................................................................................. 14 
第 2 節. 子計畫二 ......................................................................................................................................... 17 
1-3-2-1. 初始同步..................................................................................................................... 18 
1-3-2-2. 同步追蹤..................................................................................................................... 19 
1-3-2-3. 通道估測..................................................................................................................... 19 
1-3-2-4. 多輸入輸出偵測......................................................................................................... 20 
第 3 節. 子計畫三 ......................................................................................................................................... 28 
1-3-3-1. 回顧過去的相關嵌入式應用的架構......................................................................... 28 
1-3-3-2. 在 WHDVI 上微處理器發生的轉變 ......................................................................... 28 
1-3-3-3. 基於 Forth 語言的微處理器 ...................................................................................... 29 
1-3-3-4. Forth 語言的三大模型 ................................................................................................ 30 
1-3-3-5. WHDVIForth System HW/SW Design Flow .............................................................. 31 
1-3-3-6. WHDVIForth 微處理器核心架構 .............................................................................. 31 
1-3-3-7. WHDVIForth 多核心微處理器 .................................................................................. 37 
1-3-3-8. WHDVIForth 嵌入式作業系統實現 .......................................................................... 39 
1-3-3-9. WHDVI Forth 與傳統編譯式的作業系統的差異 .................................................... 39 
1-3-3-10. WHDVI Forth 虛擬機架構 ...................................................................................... 40 
1-3-3-11. WHDVI 虛擬機的 structure ...................................................................................... 41 
1 
 
第1部份.   報告內容 
第1章 .    前    言  
鑒於數位家庭產業的蓬勃發展，從防盜到導航，影音娛樂到數位廣播電視，將有越來越多的電子
科技整合在汽車內，政府已決心與相關業者共同推動「數位家庭整合計畫」，期望藉此計畫來扮演火
車頭的腳色，帶領相關電子科技的整合。本整合型計畫「適用於數位家庭整合之無線高畫質視訊傳輸
技術」期望藉由無線通訊技術的結合，提高數位多媒體視訊裝置的整合能力，因此本計畫提出了一個
全新的規格，命名為無線高畫質視訊傳輸技術(wireless high definition video interface：WHDVI)，來整
合數位家庭整中的各類傳輸介面，讓消費者除了享有原來數位視訊裝置服務之外，也可以藉由無線高
畫質視訊傳輸技術來使得各類數位多媒體視訊裝置能夠在共同的傳輸介面來整合。 
中央大學資電學院也有意在數位家庭產業此方向上扮演一個產學界合作的腳色，中大電機系與通
訊系的教授專長包含通訊、晶片設計以及多媒體訊號處理，在這個產業上我們能夠從「數位家庭」的
角度切入。由於數位家庭產業是一個新興的產業，目前尚無相關成熟的無線傳輸標準制定，這讓我們
有機會去發展一個前瞻的多媒體通訊系統，此系統需整合通訊協定、基頻數位訊號處理、SOC晶片設
計技術、影像壓縮處理及嵌入式處理器的技術。 
 
表1-1-1.    本計畫與其它相關標準之特性比較 
 HDMI 1.2 DVI1.0 WirelessHD 
This 
project 
(WHDVI) 
Year 2005 2001 2007 2007 
Transmission 
Media 
wireline wireline wireless wireless 
Max Data 
Rate 
5Gbps 3.7Gbps N/A 6Gbps 
Embedded 
Compression 
No No No Yes 
Rate control No No No Yes 
Capability for 
Digital Home 
Integration 
Low Low High High 
 
3 
 
 
圖1-2-1 無線高畫質視訊傳輸技術系統方塊圖 
 
基於上述對於目前傳輸技術的缺點，本計畫將擬定以無線傳輸的技術來取代HDMI線的傳輸標
準，如圖1-1-1所示。每一個source provider都會有一個wireless interface，主要能夠提供適當的壓縮能
力，然後再把壓縮過後的資料傳送到Display Media中，在每一個Display Media會透過wireless interface
接收到資料，因為會有不同的Source provider，因此還要透過一個Source selector來選擇所需要的視訊
來源。因此本計畫所欲研發的技術核心可以來克服現今高速視訊傳輸介面於數位家庭之多媒體裝置整
合所帶來的瓶頸。 
計畫組織簡介 
鑒於數位家庭產業的蓬勃發展，從防盜到導航，影音娛樂到數位廣播電視，將有越來越多的電子
科技整合在汽車內，政府已決心與相關業者共同推動「數位家庭整合計畫」，期望藉此計畫來扮演火
車頭的腳色，帶領相關電子科技的整合。本整合型計畫「適用於數位家庭整合之無線高畫質視訊傳輸
技術」期望藉由無線通訊技術的結合，提高數位多媒體視訊裝置的整合能力。如圖1所示，本計畫提出
了一個全新的規格，命名為無線高畫質視訊傳輸技術(wireless high definition video interface：
WHDVI)，來整合數位家庭整中的各類傳輸介面，讓消費者除了享有原來數位視訊裝置服務之外，也
可以藉由無線高畫質視訊傳輸技術來使得各類數位多媒體視訊裝置能夠在共同的傳輸介面來整合。 
中央大學資電學院也有意在數位家庭產業此方向上扮演一個產學界合作的腳色，中大電機系與通
訊系的教授專長包含通訊、晶片設計以及多媒體訊號處理，在這個產業上我們能夠從「數位家庭」的
角度切入。由於數位家庭產業是一個新興的產業，目前尚無相關成熟的無線傳輸標準制定，這讓我們
有機會去發展一個前瞻的多媒體通訊系統，此系統需整合通訊協定、基頻數位訊號處理、SOC晶片設
計技術、影像壓縮處理及嵌入式處理器的技術。本計畫之系統方塊圖，如圖1-2所示，共分為下列三
個子計畫來進行： 
子計畫一：嵌入式壓縮核心及錯誤更正碼之相關演算法及硬體架構開發。 
子計畫二：高速無線通訊技術之相關演算法及硬體架構開發。 
子計畫三：嵌入式處理器之硬體架構及作業系統開發。 
5 
 
第1節 .   子計畫一 
1-3-1-1.    The Description of Prediction Template 
PP PN1N2 
P
N1 N2 
N1
N2 
P
Case 1 Case 2
Case 3 Case 4
N3
 
圖1-3-1-1.    Current pixel及reference pixel於prediction template中之相對位置 
 
一般影像壓縮都會藉由一個prediction template來敘述current pixel和reference pixel之間的關係，其
中current pixel所指的意思即為當下要進行編碼動作的像素，而reference pixel則是參考像素，其主要目
的為提供參考資訊給current pixel，再藉由所選定的coding tool來消除之間的redundancy，進而達到壓
縮的目的，本作品所提出的演算法中所採用的prediction template如圖1-3-1-1所示，current pixel與
reference pixel的位置關係可分為四種情形。 
 
Case 1 : 每一個frame第一列中的前兩個pixel，在這個情況下並沒有任何的reference pixel。 
Case 2：每一個frame第一列中，前兩個以外的pixel，在這個情況下其reference pixel，為在同一列
中相鄰於current pixel的左邊兩個pixel。 
Case 3：每一個frame非第一列的第一個pixel，其reference pixel是在上一列中第一個pixel及第二個
pixel。 
Case 4：每一個frame非第一列，同時也不是第一個pixel，其reference pixel分別是上一列中相同位
置的pixel，以及在同一列中相鄰於current pixel左邊的pixel。 
 
1-3-1-2.    The Description of Probability Model 
在影像壓縮領域中，若給定一個predictor，則所產生的residual可以用一個geometric distribution 
model來作為這個residual的probability model，本作品所提出的演算法也會利用此model，並且配合上
節所敘述的prediction template，更做進一步的衍生，由圖1-3-1-1中可以得知，在一張影像之中其實大
部分的情況都會隸屬於case 4，因此在case4的probability model也將成為探討的焦點，如圖1-3-1-2所
示，先假定已經有一個predictor負責產生current pixel和reference pixel之間的residual，在此可以將N1及
N2之中取較大的做為H、較小的值為L，則在一個既定的predictor之下，current pixel和H及L，分別會
可以用一個geometric distribution model來分別模擬各別residual的關係，假如把這兩個model結合，則
可以形成另一個distribution model，而此model恰好和FLEICS演算法中的probability distribution model一
致，在此以current pixel和reference pixel之間的關係來區分編碼的方式，並且在advanced adjusted binary 
code和Golomb-Rice code還會進一步的來參考N3來做 
7 
 
在原作者的論文中提到，一般而言P – L之值落在中間部份的比率較高，所以在第二個步驟中會利
用旋轉(rotation)的方式將  )(log2 range  bit的碼放在中間機率較高的區域，如此一來即可提高壓縮效
率，藉此達到更高的壓縮比率(compression ratio)。經過旋轉後的P – L之值與它所編成的code會以如圖
1-3-1-3所示的方式分佈，其中threshold與numlong分別代表使用  )(log 2 range  bit或  )(log 2 range  bit所
編碼的個數，所以我們可以由P-L-numlong / 2與P-L-numlong / 2-threshold之 sign bit來判斷要以
 )(log 2 range  bit或是以  )(log 2 range  bit來編碼。 
thresh numlong / 2numlong / 2
S0 = 1
S1 = 1
S0:sign bit of P – L – numlong / 2
S1:sign bit of P – L – numlong / 2 - thresh
S0 = 0
S1 = 1
S0 = 0
S1 = 0
┌log2(range)
┐ bit ┌log2(range)
┐ bit└log2(range)┘
 bit
P – L
thresh numlong
┌log2(range)
┐ bit└log2(range)┘
 bit
P – L
after 
rotation
 
圖1-3-1-3.    P – L旋轉後的分佈圖 
 
以下我們舉例說明adjusted binary code的編碼方式。 
以H = 104, L = 100為例： 
range = 104 – 100 + 1 = 5 
threshold = 23 – 5 = 3 
numlong = 5 – 3 = 2 
P – L之值與它所對應之碼會如表1-3-1-1所示 
 
表1-3-1-1.    range = 5的情形下，不同的P-L所編出的碼 
 P = L    P = H 
P–L  0 1 2 3 4 
rotation 4 0 1 2 3 
≥ throd? Y N N N Y 
+ threshold 7    6 
codeword 111 00 01 10 110 
 
 
若 P 為 102，則編出來的 code 為 01 
Advanced Adjusted Binary Code 演算法 
然而如表1-3-1-1所示，經過我們實際測詴後發現，P-L值是否經過旋轉對壓縮效率的影響並不十
分明顯，這在編碼與解碼的運算上等於多了許多不必要的手續，因此為了盡量提升編碼的速度，在
adjusted binary code的編碼我們不採用旋轉的步驟，如此一來編碼的流程可以簡化成如圖1-3-1-4所
示。 
9 
 
N1
N2
P
N3 If (N3 <= H) && (N3 >= L)
{
     If abs(N3 - L) < abs(N3 - H)
            residual_ABC = H – P
     else
            residual_ABC = P  -   L
}
Else
            residual_ABC = P – L
H
L
P
N3
H
L
P
N3
Horizontal-oriented 
texture
Vertical-oriented 
texture
Identification 
with N3
H = max(N1, N2)
L = min(N1, N2)
 
圖1-3-1-5.    Reference pixel N3於Advanced Adjusted Binary code中的應用 
 
假設N3也在H和L之間，則N3可以用來判斷由N1、N2、N3及P所形成的小型texture，為了要進一
步去觀察這一個texture的材質走向，先利用N3和N1及N3和N2之間的difference來區別N3是離N1較近或
是離N2較近，當然N1和N2經過比較之後，就會產生H及L，因此可以直接取N3和H及L之間的變化量
來觀察這個local texture是屬於horizontal-oriented或是vertical-oriented的屬性，由圖1-3-1-5可得知，假設
N3比較靠近H或是L，則current pixel P叫要和另一個reference取residual，加入了local texture觀察的效
應，則可以增加較有效率的codeword來提升compression ratio。 
 
 
1-3-1-4.    The Description of Content Adaptive Golomb-Rice Code 
Original Golomb-Rice code 
1. 計算 diff，diff = P – H – 1 (above range)或 L – P – 1 (below range) 
2. 選擇參數 k = log2(m) 
3. 對於我們要編碼的值 diff，必頇先找出 
a. 商數(quotient) = q = diff / m 
b. 餘數(remainder) = r = diff % m 
4. 產生編碼，編碼格式 : <商數碼(unary part)> 0 <餘數碼(binary part)> 
a. 商數碼 
使用 unary coding 編碼，若 m = 2k越接近 diff，編碼的效率越高 
b. 餘數碼 
m為 2 的 k 次方，所以需使用 k bit 編餘數的二進位碼 
 
Golomb coding是一種變動長度的編碼(variable length)，它的編碼方式與霍夫曼編碼 (Huffman  
coding)類似，然而它是採用較為簡單的機率模型來做編碼，計算的方式是利用除法原理，可根據不同
的分佈機率選用不同的除數，Golomb-Rice code是Golomb code的特殊形式，它的除數只採用2的冪次
方，這種方式對硬體編碼來說最有效率，因為對使用2的冪次方做為除數的除法運算而言，商數可以
直接以移位的方式求得，而餘數可以直接以遮罩做AND運算來取得。 
以下我們舉例說明Golomb-Rice Code的編碼方式。 
11 
 
因為在advanced adjusted binary code中已經利用reference pixel N3來增加其編碼效率，同樣的在
content adaptive Golomb-Rice code中也會再次利用N3來做判斷，如圖1-3-1-7所示，先利用diff_1及
diff_2的加總來判定current pixel是屬於smooth或是abrupt change部分，若是前者則k值應取較小值，反
之為後者，則取較大值，在此所判定的依據是將diff_1及diff_2的加總值來和所設定的threshold 
做比較，在此所設定的 threshold 值分別為 4及 16，原因是這兩個數都是屬於 2的冪次方，在硬體
實現上並不需要一個減法器來完成，只需要一組 Nand gate 即可分辨，因此在硬體的執行速度上並不
會造成太大的影響，至於 k 值分別以 1、2 及 3 來設定，主要是依據下列的分析。 
因為是採用geometric distribution model來描述residual的分佈情形，而整體geometric distribution 
model是以exponential decay rate來呈現，因此利用exponential distribution function (EDF)來描述residual
在above range及below range 
 
                                                              (1.3.1.1) 
 
其中x代表residual，u則表示不同的exponential decay rate，為了得到其probability density function 
(EDF)，則必頇將EDF除上一個factor，如(1.3.1.2)所示。 
(1.3.1.2) 
 
                                                              (1.3.1.3) 
有了(1.3.1.2)及(1.3.1.3)，就可以得到probability density function，再根據Golomb-Rice code的 編碼
方式可以得到它所需要的number of bit為多少，如(1.3.1.4)所示。 
                                                              (1.3.1.4) 
再利用計算期望值的公式就可以得到(1.3.1.5)式 
 
                                                              (1.3.1.5) 
 
從(1.3.1.5)式可以得到一個用來評估不同k值的編碼效益，並且利用parameter u來考量不同的
exponential decay rate對於編碼效益的影響，(1.3.1.5)所代表的就是bit per pixel (bpp)的意義，此單位常
用來評估影像壓縮方法的效益。根據不同的k值及exponential decay rate, u，圖1-3-1-8顯示出不同k值在
不同的exponential decay rate情況之下所產生的bpp。 
 
圖1-3-1-8.    不同k值在不同exponential decay rate所產生的expected value 
1 2 3 4 5 6 7 8 9 10
2
3
4
5
6
7
8
9
10
Parameter u
E
x
p
e
c
te
d
 v
a
lu
e
 (
b
p
p
)
k = 1
k = 2
k = 3
k = 4
k = 5
k = 6
k = 7
k = 8
1
( , ) exp( )
x
EDF x u
u u
 
1
( , ) ( , )
( )
PDF x u EDF x u
F u

254
0
( ) ( , )
x
F u EDF x u


( ) 1
2k
x
N x k
 
   
 
254
0
254
0
( , ) ( ) ( , )
1 1
( 1)( exp( ))
( ) 2
x
k
x
E k u N x PDF x u
x x
k
F u u u



  
     
  


13 
 
行效率，因此對於本計畫提出嵌入式壓縮核心技術，能夠有效提高硬體執行的帄行度，晶片佈局圖如
圖1-3-1-10所示。 
除了此EC encoder硬體晶片實現外，另外更將encoder以及decoder結合在一起，作成一encode/decode
混和功能之晶片。詳細晶片規格以及晶片圖如圖1-3-1-11所示： 
 
表1-3-1-3.    Embedded compression硬體效能以及與其他人比較 
 EC hybrid codec EC encoder [1] [2] [3] 
CLK Rate 125MHz 195MHz N/A N/A 30MHz 
Throughput  N/A 390Mbyte/sec N/A N/A 13.8Mbyte/sec 
Gate Count 45.3k 12.97k 373.86k 76.66k 26.392k 
Technology TSMC 0.18um TSMC 0.18um N/A HP 0.13um 0.18um 
voltage 1.8v 1.8v N/A N/A 1.2v 
Hardware 
Parallelism 
2 
2 1 2 1 
Target 
Application 
QFHD  
30Hz 
Full HD(1080p) 
60Hz 
N/A N/A 
VGA 
30Hz 
[1]. A. Savakis and M. Pioriun, “Benchmaking and Hardware Implementation of JPEG-LS,” 
ICIP’02, Rocherter, NY, Sept. 2002. 
[2]. M. Ferretti and M. Boffadossi, “A parallel Pipelined Implementation of LOCO-I for 
JPGE-LS,” ICPR 2004, pp. 769-772. 
[3]. C. C. Cheng, P. C. Tseng, C. T. Huang, and L. G. Chen, “Multi-Mode Embedded Compression 
Codec Engine for Power-Aware Video Coding System,” in IEEE, SIPS 2005. 
 
Ping-pong 
module
 for odd
Ping-pong 
module
for even
Bit-stream
generator
B
G
A
Control 
unit
D
E F
C
 
圖1-3-1-10.    Embedded compression硬體晶片圖 
15 
 
傳送之間可能會有資料暫停輸出的動作，因此需要這個訊號用以只是輸出資料是否有效。最後針對資
料缺乏data-valid的情況下，則由輸出或輸入原始波形判斷其輸出入的判斷依據，並由此依據分別做
出介面電路。 
在一開始設計介面電路的時候，考量到需要針對不同模組間介面制定規格外，還需考量到三個介
面電路之間的系統銜接，因此先採用SystemC做為介面開發之工具以及驗證帄台。除了能夠迅速的開發
介面電路外，另外能夠早期驗證介面正確性，並針對介面錯誤設計的地方做偵測以及修補的工作。另
外當介面之軟體規格完成後，還需要跟硬體協同驗證以確保介面的正確性。因此CoWare這套軟體被用
來做為軟硬體協同驗證之工具，並結合NC-verilog做硬體方面之模擬。以下為CoWare開發軟體以及用
以模擬系統之系統模型。 
 
圖1-3-1-12.    CoWare開發軟體以及用以模擬系統之系統模型 
 
在介面電路設計的部份，我們採用兩個memory用以同步兩邊資料並針對不同模組的時間需求接收
及傳遞相對應資料。除了兩個memory外，還有一控制電路用以控制記憶體傳入/輸出順序、切換時間控
制、以及其他相關控制訊號。最後還需要兩個輸出/輸入模組訊號轉換以及產生電路以及輸出入資料寬
度不同而設計的Register-based FIFO電路。整體介面電路方塊圖如下所示： 
 
17 
 
 
在整體軟硬體協同設計流程方面。首先經過介面以及現有硬體軟體方面設計、驗證、以及軟硬體
協同驗證後，接下來經過以硬體考量為主之軟體架構轉換以及軟硬體轉換，最後交給子計畫二做板子
合成、繞線、板子對接、驗證等程序，完成整體系統之實做。最終整體帄台架構如圖 1-3-1 之右圖所
示。 
 
圖1-3-1-16.    基於軟硬體協同模擬、驗證、以及開發之ESL系統流程規劃圖，針對總計畫之多模組
系統實行聯合驗證以及介面設計 
 
第2節 .   子計畫二 
本子計畫在實現無線傳輸模組, 並植基於多輸入輸出無線區域網路802.11n系統上來訂立一新規格
以達到傳送高畫質影像的速度需求, 同時設計為可重配置之基頻通訊收發機系統, 接收來自中央處理
器的程式化指令, 完成不同影音品質需求的無線通訊傳輸, 目前此一無線通訊基頻收發機以多輸入輸
出之正交分頻多工(MIMO-OFDM)系統來實現, 設定操作在lower UNII band, 最多使用至160MHz的頻
寬, 最少使用40MHz的頻寬, 為了提高傳輸速率, 保衛區間的比例佔FFT 週期的1/4, 天線的規畫最多
支援至四根發送天線及四根接收天線, 系統規格列如下表1-3-2-1: 
 
表1-3-2-1.    無線視訊傳輸基頻接收機規格 
System Parameters 
RF Frequency (GHz) 5.15~5.35 
Bandwidth (MHz) 40/80/160 
Modulation OFDM 
Constellations 16-QAM/64-QAM 
Subcarrier Spacing (KHz) 312.5 
FFT Period ( s) 3.2 
19 
 
佳的脈波波形可對邊界進行準確定位，延遲相關器則極易辨識出短前置符元的存在, 計數器則協助波
峰偵測, 其相對應之工作波型如圖1-3-2-2(b)所示。 
 
(a)                                    
(b) 
圖1-3-2-2.     (a)初始同步方塊圖 (b)工作波形圖 
 
1-3-2-2.    同步追蹤 
初始同步單元雖然補償了大部分的載波頻率誤差, 但是仍有殘餘的載波頻率誤差存在, 造成收到
的訊號隨著符元數的增加而有線性增長的相位旋轉, 此一線性增長的相位旋轉會造成在封包開始利用
長前置符元所取得的通道估測值失去正確性, 而漸漸無法正確補償訊號的失真, 因而位元錯誤率將隨
著符元數的增加而上升。圖1-3-2-3顯示了在不同訊雜比下殘餘載波頻率誤差的相位旋轉程度, 因為在
低訊雜比時, 初始同步估測存在著較大的誤差, 所以有明顯的相位增長, 當高訊雜比時, 初始同步單元
的估測時誤差較低, 但是我們所建置的加權最小方差殘餘同步估測演算法皆可進行正確的補償償動作
而緊緊跟隨殘餘載波誤差的偏移量。 
  
圖1-3-2-3.    殘餘載波頻率誤差與補償 
1-3-2-3.    通道估測 
為了得到4x4的多輸入輸出系統的通道估測值, 長前置符元內的訊號採取Hadamard matrix的編碼
方式,  
21 
 
vectoring mode操作下且CORDIC相當於沒有動作時，因為後面來的訊號是跟著旋轉，所以表示後面來
的訊號也不需要動作了，這顆CORDIC在任何時間就都是閒置的，故可以移除。由以上情形，我們可
將部分complex PE做CORDIC的刪減，從原本4個CORDIC拿掉其中一顆或二顆CORDIC，因此在硬體
複雜度上又可以降低。而在QHy運算時，同樣地方式只要將接收端收到的data訊號依序輸入，將所有
PE切換至rotation mode並讀取在QRD時所儲存的旋轉方向資訊去做運算。因為y只是跟著前面怎麼旋
轉就怎麼旋轉，為了減少硬體複雜度，我們只要用1位元來表示正轉或反轉的方向，即可取代好幾個
位元的角度值，具有相同效果又可以減少硬體面積。 
圖1-3-2-5(b)是實數單元的硬體架構圖，Real Stage同樣地以systolic array架構為基礎。而complex 
stage是每四個time slot輸入一個channel matrix，在演算中經過real value decomposition (RVD)轉換後，
矩陣的維度變成原來的四倍，所以在real stage變成要8個time slot去輸入資料，因此在這部分複製一套
相同硬體，將輸入的資料分成2組去運算才能夠和complex stage時間互相符合。為了有效使用硬體資
源且同時符合可QRD以及QHy運算，同樣地考慮CORDIC的使用及閒置情形將real PE做刪減，並且利
用重新排序技術將real PE個數最佳，其架構如圖1-3-2-5(b)所示。QRD運算會用到所有的real PE，在
QHy運算下則使用紫色real PE，將QHy模式下未用到的個數減至最少。 同樣是以systolic array的方式
設計，但為了能配合複數單元輸出速率，所以必需要在相同時內在完成所有資料運算。在運算期間內
會有CORDIC兩個輸入都為0而輸出值還是為0的情形，所以可以利用這些閒置的時間進行使用率最佳
化，讓硬體複雜度變小且符合QR分解和QHy都可以操作。在QR轉換操作時，為了減少硬體複雜度，
只記錄每一個CORDIC內各個stage的旋轉方向。而在QHy操作時，只要根據先前所存的旋轉資訊，就
可以使用原來的硬體將值算出。 
Complex 
PE
Complex 
PE
Complex 
PE
Complex 
PE
Complex 
PE
h34
h33
h32
h31
h24
h23
h22
h21
h14
h13
h22
h11
h44
h43
h42
h41
Complex 
PE
RU
r34 r33 00
r24 r23 r22 0
r14 r13 r22 r11
r44 000
DU
y3y2y1 y4
y1
y2
y3
y4
Channel Matrix
Received Signal
ˆ
ˆ
ˆ
ˆ
yˆ=QHy
hr23
hr24
hr33*
hr34
hr44*
-hi23-hi24
-hi34
0
0
0
0
0
0
hr22*x0
xx
0
0
hr12
hr11
hi23
hi24hi34
hi13
hi14
hr22
hr23
hr24
hr33
hr34
hr13
hr14
hi12
x
x
x
x
x
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆˆˆ
ˆ ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆˆ
ˆ
M
U
X
M
U
X
MUX
MUX
M
U
X
MUX
Real 
PE
Real 
PE
Real 
PE
Real 
PE
Real 
PE
Real 
PE
Real 
PE
Real 
PE
0
xxx
 
(a)                                       (b) 
圖1-3-2-5.    QR分解模組之(a)複數級與(b)實數級 
 
所設計的QR分解架構已由硬體實作驗證, 在每個CORDIC模組中使用9次微旋轉(micro rotation)，
為了達到高速操作支援球面解碼器，加入了3級管線級處理(pipeline stage)。使用0.18μm CMOS製
程，根據量測結果最大頻率可達到100M Hz，邏輯閘數(Gate count)為152k。此硬體只花40 ns完成8×8
通道矩陣的分解，以及每10 ns就可產生一個QHy結果，另外其產出量足夠支援高達2.4Gbps之球面解
碼器, 相較於其他作品較高的產出量。 
23 
 
表1-3-2-3.    QR分解合成結果比較 
[2-1] [2-2] [2-3] [2-4] [2-5]
 
 
最佳K值演算法，是先以單一層的搜尋為主，在單一層的搜尋結束後才進入下一層做搜尋。最佳
K值先需要設定一個合適的K值 ，愈大之K值會得到愈好的bit error rate (BER)，但若K值設定過大會
有硬體複雜度的問題，反之太小又會導致效能嚴重衰減，因此我們利用參考文獻[2-6]的作法，也就是
每一層之K值可以不相同。然後取前(相對於訊號流)四層K為8時下去搜尋，也就是將每層的每個節點
的PED都計算出來並加以排序, 只有前8小的節點才能成為有效點並向下延伸成為存活路徑(survival 
path) 。完成一層的搜尋之後, 將該層的存活路徑向下延伸, 再將每個有效點下的每個子代的PED都計
算出來並做比較(我們採用前四層為8，後面四層依序減半的方式搜尋), 然後留下最小Ki(i代表第i層的
K值)個存活路徑向下延伸, 如此反覆直到搜尋至最後一層為止, 到最後一層時是將底層的所有存活路
徑下的PED做比較, 最小的PED所屬於的存活路徑, 其路徑上所經過的節點就是此樹狀搜尋的解答。
由於Ki是個序列，我們以向量的語法表示它。以我們提議的架構，它便是一個K = [1 2 2 4 8 8 8 8]的組
態。圖1-3-2-6為整體硬體架構方塊圖，z為接收端的訊號，經過layer 8先解出8組PED的值，再傳給
layer 7的Layer Processing，其內部經過Initial stage的列舉後排序法選出最小的PED結果後，再進入7個
Reduced stages重新算出新的PED並陸續再選出其他次小的PED，最後輸出8個最小PED給layer 6，layer 
6同理layer 7輸出8個最小PED給layer 5…，如此解到layer 4之後，剩下三層只需在比較一半的PED大小
值，即搜查樣本空間減半，藉此來降低硬體複雜度。 
 
圖1-3-2-6.    所提議的硬體架構方塊圖 
25 
 
 
圖1-3-2-9.    BER效能比較圖 
 
圖1-3-2-10.    球面解碼器之布局圖 
 
表1-3-2-4.    球面解碼器硬體實作規格 
Technology UMC 90nm 
Design Kit UMC/ Faraday/CIC CBDK v2.0 
Package CQFP144 
Voltage (Core/Pad) 1.0 V/3.3 V 
Core Utilization 88.79% 
Core Size 0.96 × 0.96 mm2 
Chip Size 2.06 × 2.06 mm2 
Gate Count 232K gates 
Max Frequency 150.6 MHz 
Throughput 3.61 Gbps 
Power Consumption 267.6 mW 
 
圖1-3-2-9顯示我們所提議的球面解碼器([1 2 2 4 8 8 8 8]-best SD)與傳統K-Best SD的效能比較。我
們的球面解碼器犠牲了一些效能以有效地降低硬體複雜度，如果再使用sorted QR decomposition，則可
以改進效能。圖1-3-2-10是為球面解碼器之晶片布局圖，具有144之接腳, 採用CQFP包裝, 核心面積為
27 
 
表1-3-2-6.    接收機之FPGA內部資源使用分析 
 
在接收機端，我們整合初始同步、快速傅立葉轉換單元、通道估測、同步追蹤與補償、通道QR分
解、輸入訊號向量投影與解映射(demapper)等單元。另外整個接收機從啟動開始之工作程序則由有線狀
態機來控制，整個接收機之邏輯閘數極多，故我們使用xilinx V5-330之板子來驗證。表1-3-2-6則顯示了
接收機所使用之系統資源情形。 
最後，我們將發送機與接收機之FPGA板進行對接如圖1-3-2-11所示，並利用向量波形產生器(○1 )
來產生輸入之控制訊號，下達至發送機(○2 )，發送機送出封包資料，接收機(○3 )則進行訊號的解碼與偵
測，然後由邏輯分析儀(○4 )量測所解出之結果。經過資料的比對，發送機與解收機操作行為正確，驗證
了系統整體之性能。 
 
29 
 
法都是在規定的時間內處理完畢。以下我們探討幾項WHDVI特點，以及我們為什麼要使用Forth語言
來當作我們的處理器語言。  
Multimedia Capability 
WHDVI的應用為數位資料處理，也就是微處理器有很大的數位資料處理的吞吐量。在很嚴格的
時序壓力下，我們需要小心的用數學演算法處理，如Joint source and channel coding之等級程式。以及
之前所提到的通用型I/O和Real time時脈也都必頇要加入這個系統中，這個需求將成為決定多核心數量
的重要依據。 
Fast External Memory Interface 
WHDVI的應用對於外部記憶體資料的快速存取是非常重要的，當微控制器需要定址幾千個位元
組的資料是非常消耗時間。尤其是影像的應用，為了存取4百萬位元組的DDR隨機存取記憶體，記憶
體介面的速度將成為一個操作的瓶頸。而我們的應用則是多通道bit-stream，這代表我們更加需求對記
憶體的頻寬去切換記憶體的位址以及讀和寫。這個需求將導致探討合理的增加一個核心去處裡外部記
憶體介面。 
 Reliability 
大量運算，其損毀性高，對體積、耗能、散熱也有極嚴苛的要求。因此，系統的穩定度也是我們
考量的議題。 
 
1-3-3-3.    基於 Forth 語言的微處理器 
由以上的觀點讓我們捨棄傳統通用式處理器的辦法，而採取設計特定為WHDVI設計的微處理
器，而現在的問題變成此微處理器要運行的語言是哪種語言呢？我們的答案是Forth，Forth並不是一
個新的程式設計語言。它的商業應用歷史已經有 30 年了，Forth 語言是快速和互動式的，它能夠利
用很少的時間，產生更短、更可靠的程式，而且它屬於是一個高階語言，支援條件判斷和迴圈的功
能，可以用簡單的想法去做我們想做的事，而用Forth來說就是創造的基本單元的”字“ ，在設計循序
的狀態時，我們發現在WHDVI中我們可以用每個Forth字完整的表達。 
WHDVI系統中的微處理器採用以Forth為架構，因其簡單，不容易出錯，我們稱運行在此系統的
Forth為WHDVIForth。  
 
31 
 
1-3-3-5.    WHDVIForth System HW/SW Design Flow 
 
圖1-3-3-2.    WHDVIForth System Design Flow 
在建構Forth硬體時，我們先制定Hardware development和Software development要達到的功能。接
者尋找適合開發的環境，在硬體製作上採用Verilog硬體描述語言，帄台運行在Xilinx的FPGA上面執
行，選用的開發環境是Xilinx FPGA軟體ISE 8.2。軟體上上採用自行開發的WHDVIForth作為軟體開發
的工具。WHDVIForth是為了Windows環境所設計的Forth程式語言系統，讓我們可以用Windows的視
窗環境發展Forth程式。 
Cross Compiler：在WHDVI的系統理，我們可以很方便的定義一些新的指令，且可以往記憶體內
植入機器碼，為了組合WHDVIForth的機器碼，在20位元的字組中放入4個機器碼，並針對長指令放入
15位元的地址。我們為了針對此計畫開發了自己的組合器和編譯器，就可以為了我們的系統打造完整
的開發環境，不需要仰賴微軟或是其他公司提供工具，達到完全自主的能力。 
1-3-3-6.    WHDVIForth 微處理器核心架構 
Forth 堆疊架構處理器介紹 
Forth屬於堆疊架構的微處理器，而堆疊微處理器的複雜性比CISC要低得多，系統複雜性更是比
RISC和CISC都低很多，堆疊微處理器的高性能不需要複雜的編譯器就能夠實現，它們也提供了有競
33 
 
Return Stack
R
Parameter Stack
T
S A
L
U
Slot0
PC
Slot1 Slot2 Slot3
Instruction Register
X
D
a
ta
 B
u
s
A
d
d
re
ss B
u
s
O
p
c
o
d
e
 
D
e
c
o
d
e
S
ta
c
k
 
C
o
n
tro
l
S
ta
c
k
 
C
o
n
tro
l
C
Instruction Counter  Register
Instructionn pointer
Data pointer
 
圖1-3-3-3.    單一核心WHDVIForth資料路徑 
Set of registers: 
Name Register Function 
I Instruction latch Holding up to 5 instructions to be executed 
PC Program counter Pointing to next program word in memory 
R Top of return 
stack 
Holding return address or loop counts 
S Second item of 
data stack 
Supplying optional second argument to ALU 
T Top of data stack Accumulator for ALU 
X Address register Supplying address for memory read and write 
Two Stack 
Name Stack Function 
s_stack Data stack  Passing arguments among nested subroutines 
r_stack Return stack Saving return addresses of nested subroutines 
 
35 
 
五個位元，由於是堆疊架構的處理器，所以在20位元中可以打包4個指令，這個優點是能有更小的指
令集，更少的資料路徑和解碼控制，進而能減少程式空間。而產生最少的程式能夠最快完成所需要執
行的演算法。 
Forth是最能產生最少程式碼的語言，不像一般的C，需要繁複的編譯，組譯，最後才能得到可以
執行的程式碼，而Forth程式主要是由一連串序列的字所組成，最常用到的執行指令是Next字，在傳統
的Forth中在 inner 直譯器中執行，超過25%的執行時間是在做呼叫Next這個字，對應就是我們的
WHDVIForth硬體就是call 和return的動作，以下列出我們硬體建造的32個機器碼。 
 
Register Instructions: 
Instruction Code Function 
DUP 11010 Push T on the S stack. T remains unchanged. 
DROP 11111 Pop S stack to T. 
NOP 11110 No operation. 
OVER 11011 Push T onto S stack. Copy original contents of S to T. 
POP 11000 Push T onto S stack. Pop R stack to T. 
PUSH 11100 Push T onto R stack. Pop S stack to T. 
TX 11101 Copy T to X. Pop S stack to T. 
XT 11001 Push T onto S stack. Copy X to T. 
Memory Instructions: 
Instruction Code Function 
LDI 01010 Push T on S stack, read data word pointed by P into T. 
Increment P by 4. 
LDX 01011 Push T on S stack, read data word pointed by X into T 
LDXP 01001 Push T on S stack, read data word pointed by X into T. 
Increment X by 1. 
STX 01111 Store T into memory pointed by X. Pop S stack to T. 
STXP  01101 Store T into memory pointed by X. Increment X by 1. 
Pop S stack to T. 
Transfer Instructions: 
Instruction Code Function 
BC 00011 If Carry is 1, branch to address in address field; else 
continue. 
BRA 00000 Branch to address in address field. 
BZ 00010 If T=0, branch to address in address field; else 
continue. 
CALL 00100 Push the address in P on R stack, and branch to address 
in address field; else continue. 
NEXT 00101 If R is not 0, branch to address in address field, and 
decrement R by 1; else pop R stack and continue. 
RET 00001 Return from a subroutine to calling program. Pop return 
37 
 
1-3-3-7.    WHDVIForth 多核心微處理器 
在WHDVI系統中我們嘗詴分開不同的Tasks成為不同的Group -及時性和非及時性，對於即時性的
應用，我們採用及時性的元件與之對應，簡單的來說，即時性WHDVI應用就是在一個很小的時間周
期中(一個時間訊框到下一個時間訊框)被WHDVIForth硬體所驅動，如果不符合這個的時間限制，則
會造成音樂或是影像的延遲。 
近年來的實作這個問題傾向兩個DSP或是ARM加上一DSP的核心合作控制我們的硬體，去維持特
定硬體之上面的bit-stream的暢通，但這出現一個很重大的極限，一、如果有更多的需求被加入，運算
量是否跟得上處理速度？。二、bit-stream頻寬看似無止境的增加，往後的DSP是否能真正達到該有的
吞吐量？所以我們在這裡放棄了傳統DSP，改由分散式的架構去解決這項實作，我們需要更多的微控
制器加入，透過適當的設計，經過分散計算量，分享多個工作，才能處理更複雜的運算。當然這是需
要重寫演算法來完成的，Forth對於這方面是非常好應付的，使它成為可以分享工作的硬體。 
分散式的運算和DSP是有所不同的，如果系統是用DSP核心是有許多複雜性的，因為DSP是由許
多快速的算術運算邏輯所組成，而且DSP條件判斷的跳躍讓計算元件有更多的動態功率消耗。
WHDVIForth的多核心想法是是彈性的，這個彈性的Critical Path是不會侷限在高速的運算邏輯。 
WHDVIForth多核心的價值不是在提升高速度算術邏輯電路的速度，而是利用它的彈性，和嵌入
式分散解決問題的能力，來提高效率。這個彈性就是利用指定不同核心需要執行的工作來達成，例如
其中一個核心是用來處理外部記憶體的存取，另外一個是用來處理多媒體演算法，另外一個則是在處
理I/O的應用，在這裡可以比較和傳統單一核心處理多工作的差別，單一核心處理多工作是在時間的
許可下，切換不同副程式執行的程式，其中進入和返回副程式是非常耗費時間以及功率的，且有些工
作是要等I/O觸發，等待資料接收，主程式才能執行，這些告訴我們，程式執行的演算法將會在那裡
等待以及消耗功率。在沒有使用組合語言時，更會造成編譯更過無意義的程式。 
對於分散性的微處理器陣列來說，我們可以達到一個最佳核心數對於問題有最佳的效率，我們只
需要簡單指定一些特殊工作給特定的核心，就能增加速度，如圖1-3-3-7.所示。 
 
 
圖1-3-3-7.    多核心WHDVIForth架構圖 
 
我們在這裡指定N0為管理外部記憶體的存取，N3是操控對於通訊和多媒體ASIC輸出入的I/O，這
兩個核心通常是需要及時性的處理我們可以將這個Task用一個核心去處理，其他的Task則可以再另外
一個核心完成，N1以及N2需要跑Joint source and channel coding演算法，我們分別加入WHDVIForth的
39 
 
Core 1
write_n
data_in
full
empty
half
FIFO
read_n
data_out
         
data_i
read
write
 
data_o
addr
read_n
data_out
write_n
data_in
full
empty
half
               FIFO
S
y
st
e
m
_
a
d
d
r_
1
[1
9
:1
5
]
0000
0100
0000
0100
System_addr_1
system_addr_1[19:17]
      
          
data_i
read
write
 
 
data_o
addr
0000
1000
0000
1000
  RAM
ram_data_o
ram_addr
ram_wr
ram_oe
ram_cs
ram_clk
ram_data_i
data_o
addr
wr
oe
cs
clk
data_i
1
0
           RAM1
ram_data_o
ram_addr
ram_wr
ram_oe
ram_cs
ram_clk
ram_data_i
data_o
addr
wr
oe
cs
clk
data_i
1
0
0000
0100
0000
0100
0000
1000
0000
1000
Core 2
full empty half
full empty half
System_addr_2
system_addr_2[19:17]
S
y
st
e
m
_
a
d
d
r_
2
[1
9
:1
5
]
 
圖1-3-3-9.    多核心傳輸之資料路徑 
 
當原先儲存在ROM的資料經過Boot之後，要和鄰近的核心溝通的過程是和存取記憶體是一樣的：
都是經由地址匯流排的最高3位元判別。當核心1送出存取到FIFO的信號和資料完畢時，核心2隨時都
可以透過這個Empty信號偵測是否有資料需要讀取。 
 
1-3-3-8.    WHDVIForth 嵌入式作業系統實現 
Forth屬於一種Scripting language，它的本質是一個虛擬機，SQL、 PDF等市面上的大型軟體都有
虛擬機的身影。以堆疊基礎的虛擬機更是被廣泛利用，如Java、.NET 、的本質都是堆疊基礎的虛擬
機。由於我們的硬體是我們自行設計的，所以需要創造我們自己的虛擬機。 
此外我們發現在還沒有貫通 Forth虛擬機之前，看各家的Forth，只會越看越糊塗，那些都是外顯
的形式。基於此原因，在此計畫中我們利用ASNI C語言實作一個Forth在Visual C++ 2008 Express 
Edition上，我們稱為 WHDVIForth嵌入式作業系統。 
我們將硬體配上一個結構化的WHDVIForth嵌入式作業系統之後，副程式可以很容易的串聯起來
成為新的程式，他接受使用者輸入的一串指令系列經直譯器將指令解析後依序執行，此外
WHDVIForth還有一個編譯器允許使用者定義新指令，編成機械碼存在記憶體裡面，所以佔用記憶體
空間小，且執行速度快，這種既是虛擬機，又是程式語言的特性適合WHDVI嵌入式系統。 
 
1-3-3-9.    WHDVI Forth 與傳統編譯式的作業系統的差異 
普遍來說編譯器的結構如圖1-3-3-10.所表示： 
Source
File
Compiler Executable
 
圖1-3-3-10.    傳統編譯器 
 
為了編譯和測詴程式，比如C程式，必頇先準備一個輸入檔，再把它輸入到一個黑匣子（編譯器）
中，然後運行結果的可執行檔（通常是機器碼）。這個過程單調乏味，所以大多數使用傳統語言進行
41 
 
 
圖1-3-3-12.    作業系統 
 
1-3-3-11.    WHDVI 虛擬機的 structure 
typedef void (*WHDVIXT)(WHDVIVm *vm);  // the execution vector 
typedef struct { 
 char name[MAXNAMELEN+1];  // the name 
 WHDVIXT xt;                // the execuate vector 
 int immediate;                // execute immediately regardless of state 
} WHDVIWord ;                  // Word is the execution unit in WHDVIXT 
virtual machine 
 
首先是一個重要的程序，先宣告Function Pointer的類別，叫做WHDVIXT XT，這個叫做
ExecutionVector，它是儲存一個可以執行函數的位置。接下來定義一個Structure，裡面的內容有Name
和用Function Pointer定義的Execution Vector，和一個判斷這個字是否為立即字的Flag：Immediate。
Immediate這個Flag代表無論是在編譯模式還是在直譯模式，狀況下都會立即執行的字。 
char *tib;             //terminal input buffer, hold the text to be interpreted. 
 int ntib;           //consumed index 
 char token[1024];   //current token 
 int terminate;      // true if VM is terminating  
#define STACK_DEPTH 256 
 int datastack [STACK_DEPTH]; // the stack 
 int sp ;                     // the stack pointer 
#define MAXWORD 256 
 WHDVIWord vectors[MAXWORD]; // array to hold name and execution 
43 
 
NextToken
initial
*ptib,*ptoken
!tib return 0
Yes
ptib <-tib+ntib
*ptib && *ptib <=' '
ptib++
ptoken <- token
*ptib && *ptib>' '
*ptoken++ = *ptib++
*ptoken=0
return *token
ntib = ptib-tib
No
Yes
No
Yes
No
 
圖1-3-3-13.    處理Tokens流程圖 
1-3-3-13.    堆疊與算術操作 
堆疊式一個資料結構，越早Push進去的話它會越晚Pop出來，所以稱為First in last out(FILO) 
45 
 
Rpush和之前的Push是差不多的，只不過現在是Push到Return Stack上，他有自己的堆疊指標，
Return Stack Pointer。 
 
Pop 
Pop ( -- v )
sp  < 0
Yes
No
return 0
v= datastack[sp]
Initial v
sp--
return v
 
圖1-3-3-16.    Pop流程圖 
 
一開始他會檢查Stack pointer有沒有小於零，如果是小於零的話就要顯示這是Under Flow，代表堆
疊內沒有任何的資料，堆疊已經是空的。如果堆疊不是空的話就會抓Data stack的頂端(TOS)當作回傳
值。接下來最重要的是還要將Stack pointer減一，當堆疊的深度降一，好讓下次Pop時可以抓到正確的
數值。 
47 
 
圖1-3-3-18.    Add流程圖 
 
這個基本的語法非常簡單，將兩個原本在堆疊頂端以及在堆疊Pop出來，並令兩個相加後再放回
堆疊頂端。 
Mul 
Mul  ( -- )
Initial n1,n2 ,r 
n2=Pop(vm);
n1=Pop(vm)
r= n1 * n2
Push(vm,  r )
return
 
圖1-3-3-19.    Add流程圖 
 
乘法的運作和加法是很像的，都是先Pop出來然後做乘法，最後將它Push進去。 
Bye 
Bye ( -- )
terminate=1;
return
 
圖1-3-3-20.    Bye流程圖 
 
如果判斷到terminate為一的話，然而在主迴圈偵測到terminate非零的話整個程式的主迴圈就會離
開。意思就是當打bye時就會離開程式。 
49 
 
GetWord 
GetWord ( *name -- )
No
Initial i ,*word
i=nword;i>=0;i--
word = &(vm->vectors[i]);
Name == word-> 
name?
Return word
Yes
Yes
No
Return NULL
 
圖1-3-3-22.    GetWord流程圖 
 
    GetWord就是一個簡單的迴圈，從最後加入的字找到第一個字，所以迴圈是逆的，而這個迴
圈做的事是取出虛擬機Vector陣列的字，然後一個一個比較名稱，如果符合為真，就返回它的xt，如
不符合就返回NULL(空指標)，  
51 
 
是連續的記憶體空間，對虛擬機來說是看成是可以操作的記憶體範圍，而在本章中將會說明編譯字
(compiler word)的使用。 
Here 
Here ( -- )
Push ( here )
return
 
圖1-3-3-24.    Here流程圖 
 
Here是可以開始寫入到dictionary的位置，將他推到堆疊頂端。 
Comma 
Comma ( -- )
DictCompile( Pop(vm) )
return
 
圖1-3-3-25.    Comma流程圖 
 
Comma是將堆疊頂端的資料，寫到dictionary裡面，所以他的流程很簡單，首先先將堆疊頂端的資
料Pop出來，然後就用DictCompile編譯資料到字典裡面。 
 
53 
 
 
Semicolon ( -- )
compiling  = 0
AddWord(newword.name,newword.xt)
Return 
DictCompile(Ret); 
 
圖1-3-3-28.    Semicolon流程圖 
 
Colon為開始這個編譯模式，而Semicolon就是結束這個編譯模式。它的內容則是一開始compiling
設定為0，代表離開編譯模式，然後我們編入Ret，這個Ret是高階函式裡面一定要有的指令。最後將
新定義的字加到指令陣列裡面去。 
 
SetImmediate( -- )
Yes
No
Return 
! nword
vectors[nword-1].immediate =1
Return 
 
圖1-3-3-29.    SetImmediate 流程圖 
 
在指令集的陣列vector中，nword-1代表最後的元素，也就是剛剛加入的元素它的immediate的flag
設定成1。 
1-3-3-17.    內部迴路 
Forth的核心是Stack、Dictionary和執行指令的機制。了解內部執行的機制對複雜軟體系統在嵌入
式系統上的理解有很大的幫助。我們會設計一內部執行程序的機制，如何編寫一個高階的函式，和怎
麼去執行他。 
 
55 
 
如圖1-3-3-31. 所示，開始進入內部迴路時，我們會先對變數作出初始化的動作，然後進入迴
圈，首先先會將Instruction Pointer指到位址上的內容強迫轉型成整數，再把它設定到addr上面，所以
現在addr儲存的是instruction pointer所指向記憶體的內容之後，很重要的是要先將我們的解譯指標指到
下一個將要執行的位址上，所以我們需要前進一個CELL的空間，Instruction Pointer永遠指向下一個將
要執行指令的位置，這樣在下一次動作時才能解譯到下個字。 
接者我們再判斷是不是屬於高階字，如果是高階字，必定在字典範圍內，我們只頇判斷是否在預
設的字典範圍內就可以知道是否為高階字，如果是高階字，我們會將再次呼叫自己，形成巢狀的迴
路，當行成巢狀迴路時需要特別小心，因為我們要先將Instruction Pointer放到Return stack。 
如果不是在字典的範圍內那就是當成是基本的Code word(從硬體的角度來看Code word就是所謂的
machine code)，我們將會執行它的Execution vector，執行它時我們只需要傳如vm這個structure當作引
數，再呼叫function pointer時先將它轉型成Execution vector的型態，直接就可以用Function pointer來呼
叫。然後在最後的迴圈判斷是不是有遭遇到Return這個字，如果有就Abort，完成這次的解譯。 
再次強調Instruction Pointer是存放下次要執行指令的位址，而這次要執行的是存放到addr裡面。
如沒有編入Ret則在inner loop會每次執行，加四，做完，週而復始，所以Instruction Pointer沒有改變的
話，就是在記憶體空間解譯，加四，在解譯下個Cell空間。 
Call和Ret是一個相對的概念，一個是進入一個高階函式，一個是離開一個高階函式。一個是push 
instruction pointer到Return stack。一個是pop instruction pointer到Return stack，我們用圖 1-3-3-32.解
釋。 
 
 
圖1-3-3-32.    WHDVIForth內部執行機制 
 
高階函式 A的內容總共做三件事情，第一件事執行指令一，第二件事呼叫B，第三件事執行指令
二。再呼叫B之前，Instruction Pointer事實上已經被指到指令二，意思是它已經被保存到Return stack
上。當高階函式B返回的時候，離開之前就把Instruction Pointer從Return stack彈出，並設成Instruction 
Pointer上。Instruction Pointer就還原成進入高階函式B進入的數值。 
 
1-3-3-18.    Dual-State Compilation 
 
高階函式 A 
 
指令一 
 
呼叫 B 
(進入 B 前，IP 指到指
令二) 
 
指令二 
高階函式 B 
 
離開時，將 IP
還原為進入前之值 
57 
 
 
第4章 .    結果與討論 
第1節 .   子計畫一 
在整體計畫的規劃以及開發方面，目前 embedded compression 已經完成軟體/硬體方面的設計以
及驗證，另外也發了兩篇論文關於此硬體之論文。而除了主體演算法外，rate control 以及 bit 
truncation 等機制也加入 embedded compression 中。另外，EC 硬體也已經整合進總計畫系統中並與
其他模組作聯合驗證開發。最後，我們利用軟硬體聯合設計設計的流程(ESL)來開發不同系統間連接以
及從軟硬體混和到純硬體的功能驗證。在所開發出來的 EC中，有包含兩種模式，分別為無失真模式以
及些微失真模式。如果資料頻寬足夠，則無失真模式會被採用，以提供最高畫質視訊。如果資料頻寬
不足，則 rate control 會啟動來執行些微失真的壓縮模式。以下是原先預期的工作項目以及完成情況。 
子計畫一 
(一)、獨立植基於系統驗證帄台之嵌入式
壓縮核心之建構與驗證。 
  100% 
(二)、以低複雜度 VLSI 架構為導向之嵌入
式演算法考量。 
100% 
(三)、獨立植基於系統驗證帄台之錯誤更
正碼演算法之建構與驗證。 
100% 
(四)、與系統其它模組進行系統整合及驗
證。 
100% 
(五)、技術報告整理。 100% 
 
第2節 .   子計畫二（圖表請加上章節編號，如 1-3-2-X） 
完成開發高吞吐率之多輸入輸出多模正交分頻多工收發機，所開發之發送機支援 1/2/4 根天線, 快
速傅立葉轉換點數則可設定為 128/256/512 三種模式。多輸入輸出之多路徑通道程式模擬環境亦建立
完成, 模擬了居家環境、小型辦公室、典型辦公室等三種室內通道環境，在接收機端則包含時域之初
始同步、頻域之通道估測、殘餘載波頻率誤差與取樣時脈誤差追蹤與補償、多輸入輸出通道之 QR 分
解與多輸入輸出訊號球面偵測器。系統性能驗證完成同時硬體設計驗證完成。以下是原先預期的工作
項目以及完成情況。 
子計畫二 
(一)、應用於 2.4GHz頻段之 4x4 多輸入輸
出正交分頻多工系統之設計與性能
驗證。 
  100% 
(二)、達 2Gbps 以上之高吞吐率之多輸入
輸出解碼模組之硬體設計。 
100% 
(三 )、發送機接收機完整硬體整合並於
FPGA 帄台上進行展示與驗證 
100% 
(四)、與系統其它模組進行系統整合及驗
證。 
100% 
59 
 
第2部份.   參考文獻 
第1章 .   子計畫一 
本計畫產出: 
[1-1]. T.-H. Tsai and C.-P. Chen, “VLSI design for MPEG-4 shape coding using a contour-based binary 
motion estimation algorithm,” IET Circuits, Devices and Systems, vol. 2, no. 5, pp. 429–438, 2008.  
[1-2]. T.-H. Tsai and H.-L. Lin, “Platform-based design for the low complexity and high performance 
de-interlacing system,” IEICE Transactions, vol. 91-D, no. 12, pp. 2784–2792, 2008.  
[1-3]. T.-H. Tsai, Y.-F. Lin, and H.-Y. Lin, “Video transcoder in DCT-domain spatial resolution reduction 
using low-complexity motion vector refinement algorithm,” EURASIP Journal on Advances in Signal 
Processing, vol. 2008, pp. 177:1–177:15, January 2008. [Online]. Available: 
http://dx.doi.org/10.1155/2008/467290  
[1-4]. T.-H. Tsai and L.-T. Tsai, “An efficient design for motion-JPEG2000 system in real-time video 
encoding,” Journal of Circuits, Systems, and Computers, vol. 17, no. 4, pp. 597–610, 2008.  
[1-5]. T.-H. Tsai, C.-W. Chang, and C.-L. Fang, “Efficient bit-plane coding scheme for EBCOT algorithm 
in JPEG2000,” International Conference on Innovative Computing, Information and Control, vol. 0, pp. 
330–333, 2009.  
[1-6]. T.-H. Tsai, S.-P. Chang, and T.-L. Fang, “Highly efficient CAVLC encoder for MPEG-4 
AVC/H.264,” IET Circuits, Devices and Systems, vol. 3, no. 3, pp. 116–124, 2009.  
[1-7]. T.-H. Tsai, Y.-C. Chen, and C.-L. Fang, “2DVTE: A two-directional videotext extractor for rapid 
and elaborate design,” Pattern Recognition, vol. 42, no. 7, pp. 1496–1510, 2009. [Online]. Available: 
http://www.sciencedirect.com/science/article/B6V14-4TT31Y0-1/2/0143ef836b8a01cec0d03d81b9bf425
1  
[1-8]. T.-H. Tsai and H.-L. Lin, “Design and implementation for deinterlacing using the edge-based 
correlation adaptive method,” Journal of Electronic Imaging, vol. 18, no. 1, p. 013014, 2009. [Online]. 
Available: http://link.aip.org/link/?JEI/18/013014/1  
[1-9]. T.-H. Tsai and H.-Y. Lin, “Utilization of low-complexity and drift-reduction architecture in 
downscaling transcoder,” Journal of Electronic Imaging, vol. 18, no. 3, p. 033006, 2009. [Online]. 
Available: http://link.aip.org/link/?JEI/18/033006/1  
[1-10]. T.-H. Tsai and Y.-N. Pan, “High efficient H.264/AVC deblocking filter architecture for real-time 
QFHD,” IEEE Transactions on Consumer Electronics, vol. 55, no. 4, pp. 2248–2256, 2009.  
[1-11]. T.-H. Tsai, D.-Z Pong, C.-Y. Lin, and W.-T. Hsu, “A low cost foreground object detection 
architecture design with multi-model background maintenance algorithm,” International Journal of 
Electrical Engineering, vol. 16, no. 3, pp. 241–250, Jun 2009.  
[1-12]. T.-H. Tsai and Y.-H. Lee, “A 6.4 Gbits/s embedded compression codec for memory-efficient 
applications on advanced-HD specification,” IEEE Transactions on Circuits and Systems for Video 
Technology, vol. 20, no. 10, pp. 1277–1291, 2010.  
61 
 
[1-28]. T.-H. Tsai and C.-L. Fang, “Structural videotext regions completion with temporal-spatial 
consistency,” in IEEE International Conference on Sensor Networks, Ubiquitous and Trustworthy 
Computing, 2008, pp. 256–261.  
[1-29]. T.-H. Tsai, C.-L. Fang, and C.-W. Chang, “Design of edge and statistic based inpainting for text 
regions reconstruction,” in The 19th VLSI Design/CAD Symposium, 2008.  
[1-30]. T.-H. Tsai and D.-L. Fang, “An efficient CAVLD algorithm for H.264 decoder,” in International 
Conference on Consumer Electronics, 2008, pp. 1–2.  
[1-31]. T.-H. Tsai, S.-C. Kao and Y.-X. Lee, “The segment-based rate control algorithm in JPEG-LS for 
bandwidth-efficiency applications,” in IEEE International Conference on Multimedia and Expo, 
232008-april26 2008, pp. 793–796.  
[1-32]. T.-H. Tsai and C.-Y. Lin, “A new auto- focus method based on focal window searching and tracking 
approach for digital camera,” in International Symposium on Communications, Control and Signal 
Processing, 2008, pp. 650–653.  
[1-33]. T.-H. Tsai and Y.-N. Pan, “High efficiency architecture of fast block motion estimation with 
real-time QFHD on H.264 video coding,” in IEEE International Symposium on Multimedia, 2008, pp. 
124–129.  
[1-34]. T.-H. Tsai, Y.-N. Pan and C.-H. Lin, “An electronic system level design and performance evaluation 
for multimedia applications,” in International Conference on Embedded Software and Systems. 
Washington, DC, USA: IEEE Computer Society, 2008, pp. 621–624. [Online]. Available: 
http://portal.acm.org/citation.cfm?id=1439282.1440309  
[1-35]. T.-H. Tsai, D.-Z. Pong, C.-Y. Lin and W.-T. Hsu, “A low cost foreground object detection 
architecture design with multi-model background maintenance algorithm,” in The 19th VLSI Design/CAD 
Symposium, Aug. 2008.  
[1-36]. C.-L. Fang, R.-C. Kuo and T.-H. Tsai, “Data-aware platform realization of videotext extraction for 
content integration,” in IEEE International Symposium on Circuits and Systems, May 2009, pp. 726–729.  
[1-37]. H.-C. Lin, Y.-H. Lee, and T.-H. Tsai, “The cycle-efficient IDCT algorithm for H.264/SVC with 
DSP platform,” in IEEE International Conference on Multimedia and Expo, 282009-july3 2009, pp. 
1114–1117.  
[1-38]. D.-Z. Pong, C.-Y. Lin, W.-T. Hsu, and T.-H. Tsai, “Architecture design for a low-cost and 
low-complexity foreground object segmentation with multi-model background maintenance algorithm,” 
in IEEE International Conference on Image Processing, 2009, pp. 3241–3244.  
[1-39]. D.-Y. Shen, C.-W. Chang, Y.-N. Pan, and T.-H. Tsai, “A conflict- free multi-symbol arithmetic 
encoder for H. 264/AVC,” in Asia-Pacific Signal and Information Processing Association Annual Summit 
and Conference, 2009, pp. 290–293.  
[1-40]. D.-Y. Shen, Y.-N. Pan, Z.-H. Hou, and T.-H. Tsai, “Architecture design of conflict- free 
multi-symbol CABAC for H.264/AVC,” in The 20th VLSI Design/CAD Symposium, 2009.  
[1-41]. D.-Y. Shen and T.-H. Tsai, “A 4X4-block level pipeline and bandwidth optimized motion 
compensation hardware design for H.264/AVC decoder,” in IEEE International Conference on 
Multimedia and Expo, 282009-july3 2009, pp. 1106–1109.  
[1-42]. T.-H. Tsai, H.-G. Chen, and H.-Y. Lin, “Frame rate up-conversion using adaptive bilateral motion 
estimation,” in WSEAS International Conference on Multimedia Systems and Signal Processing, 2009.  
63 
 
 
本計畫產出: 
[2-7]. Zheng-Yu Huang and Pei-Yun Tsai, “Efficient Implementation of QR Decomposition for Gigabit 
MIMO-OFDM Systems,” accepted by IEEE Transactions on Circuits and Systems I: Regular paper. 
(SCI & EI) 
[2-8]. P. Y. Tsai, Chia-Wei Chen and Meng-Yuan Huang, “Automatic IP Generation of FFT/IFFT 
Processors with Word-Length Optimization for MIMO-OFDM Systems,” EURASIP Journal on 
Advances in Signal Processing, vol. 2011, pp.1-15. (SCI & EI) 
[2-9]. P. Y. Tsai and C. Y. Lin, “A Generalized Conflict-Free Memory Addressing Scheme for 
Continuous-Flow Parallel-Processing FFT Processors With Rescheduling,” in IEEE Transactions on 
VLSI Systems, vol. pp, 2010. (SCI & EI) 
[2-10]. P. C. Hsieh, J. S. Jhuang, P. Y. Tsai, and T. D. Chiueh, “A Low-Power Delay Buffer Using Gated 
Driver Tree,” IEEE Transactions on VLSI Systems, vol. 17, pp.1212-1219, Sep. 2009. (SCI & EI)   
[2-11]. P. Y. Tsai, Z. M. Chang, Z.Y. Huang, and W. J. Jau,” Design and Evaluation of a 4x4 MIMO-OFDM 
Transceiver for Gigabit Indoor Wireless Communications,” in Proc. of Asian Pacific Conference on 
Circuits and Systems (APCCAS), Dec. 2010, pp. 955-958. 
[2-12]. Z. Y. Huang and P. Y. Tsai, “High-Throughput QR Decomposition for MIMO Detection in OFDM 
Systems,” in Proc. of International Symposium on Circuits and Systems (ISCAS), May 2010, pp. 
1492-1495. 
[2-13]. P. Y. Tsai, W. T. Chen, X.C. Lin and M.Y. Huang, “A 4x4 64-QAM Reduced-Complexity K-Best 
MIMO Detector up to 1.5Gbps,” in Proc. of International Symposium on Circuits and Systems (ISCAS), 
May 2010, pp. 3953-3956. 
[2-14]. Z. Y. Huang, and P. Y. Tsai, “High-efficiency and high-throughput QR decomposition for MIMO 
detection,” Proc. of 20th VLSI Design/CAD Symposium, Hualien, Taiwan, Aug. 2009. 
[2-15]. P. Y. Tsai and X. C. Lin, “Improved K-best sphere decoder with a look-ahead technique for 
multiple- input multiple-output systems,” International Symposium on Intelligent Signal Processing and 
Communication Systems (ISPACS), Feb. 2009, pp. 1-4. 
[2-16]. C. W. Chen and P. Y. Tsai, "A soft IP generator for variable-size FFT/IFFT processors in 
MIMO-OFDM systems," Proc. of 19th VLSI Design/CAD Symposium, Kenting, Taiwan, Aug. 2008. 
[2-17]. P. Y. Tsai, "A fast ML sphere decoder with multi-layer multi-path search," IEEE International 
Conference on Communications, Circuits, and Systems (ICCCAS), May 2008, pp. 119-123. 
第3章 .   子計畫三 
參考他人文獻: 
[3-1]. Frank Vahid and Tony Givargis, “Embedded System Design”, J. Wiley and Sons, 2002. 
[3-2]. 行政院國家科學委員會專題研究計畫申請書:適用於數位家庭整合之無線高畫質視訊傳輸技
術. 
[3-3].  “ARCHITECTURE OF THE NOVIX 
NC4016”http://www.ece.cmu.edu/~koopman/stack_computers/sec4_4.html 
65 
 
第3部份.   附 錄 
  
 
Fig. 2.  The illustration of prediction template in Compacted-
FELICS algorithm. 
 
 
Fig. 3.  The coding procedure of adjusted binary code. (a) Flow 
chart. (b) Example with delta = 4. 
II. The Proposed compacted-FELICS Algorithm 
A. The Coding Flow of compacted-FELICS 
Algorithm 
As show in Fig. 2, the prediction template indicates 
the relationship between current and reference 
pixels. Each current pixel is assigned to one of 
three sections, including In range, Above range 
and Below range. For N1 and N2, the larger one is 
regarded as H, and L is for the other one. If the 
intensity of current pixel is between N1 and N2, In 
range is assigned to it. When being greater than H, 
the current pixel is assigned to Above range. The 
Below range is for the current pixel less than L. 
With the descriotion mentioned above, the 
compacted-FELICS coding flow is described as 
following steps 
1) The first two pixels at first row are directly 
packed into bitstream without any encoding 
procedure. 
2) According to the prediction template in Fig. 2, 
find the two reference pixels, N1 and N2. 
3) Assign L = min(N1, N2), H = max(N1, N2), 
and delta = H-L. 
4) Apply adjusted binary code for P-L in In 
range, Golomb-Rice code for L-P-1 in Below 
range, and P-H-1 in Above range.  
Except first two pixels at first row, the others 
directly start from step 2 to 4. The entire coding 
flow can be reversely performed as decoding flow. 
In original FLEICS, both adjusted binary code and 
Golomb-Rice code present serious data 
dependency and considerable computation 
requirement. In following subsections, more 
efficient adjusted binary code and Golomb-Rice 
code are provided. 
B. Adjusted binary code for compacted-FELICS 
algorithm  
In In range, the intensity of current pixel is between 
H and L. The P-L is in the range of [0, delta], where 
delta denotes H-L. If delta+1 is exactly equal to 
power of two, the P-L is encoded with log2(delta+1) 
bit. Otherwise, the required number of bit is 
represented as following equation 
 
  2 2log ( 1) # _ log ( 1)delta bit sample delta+ ≤ ≤ +⎢ ⎥ ⎡ ⎤⎣ ⎦ ⎢ ⎥      (1)            
 
where #bit_sample stands for the required number 
of bit to represent the sample of P-L. Based on (1), 
the sample of P-L could be assigned 
to 2log ( 1)delta +⎢ ⎥⎣ ⎦ or 2log ( 1)delta +⎡ ⎤⎢ ⎥ bit. The coding 
flow of adjusted binary code is illustrated in Fig. 
3(a), and an example of delta = 4 is also presented 
in Fig. 3(b). According to (1), the required number 
of bits is 2 or 3 for delta = 4. Then, the parameter 
computation of range and threshold is calculated. If 
the P-L is less than threshold, the 2log ( 1)delta +⎢ ⎥⎣ ⎦  
bit is directly assigned to P-L. Otherwise, the P-L is 
added to threshold, and encoded with 
2log ( 1)delta +⎡ ⎤⎢ ⎥  bit. The main reason is that since 
smaller P-L should be frequently occurred than 
larger one, more efficient codeword should be 
allocated to it. In Fig. 3(b), the parameter of range 
and threshold is 5 and 3, respectively. Based on 
the principle mentioned above, the P-L less than 
threshold is allocated to 2 bit, and 3 bit for the P-L 
which is greater than or equal to threshold. The 
generated codeword in Fig. 3(b) is consistent with 
the principle mentioned above. 
C. Golomb-Rice code for compacted-FELICS 
algorithm 
For both Above range and Below range, Golomb-
Rice code is adopted as the coding tool. The 
codeword of sample x, P-H-1 or L-P-1, is 
partitioned into unary and binary part. The number 
234
 
 
Fig. 6.  The layout view of prototype chip 
IV. Experiment Result and Discussion 
Our design is implemented by TSMC 0.18-um with 
Artisan cell library. The performance evaluation 
with other works is shown in Table I. Among these 
works, our proposed architecture presents higher 
operation frequency and more compacted gate 
count. The main reason can be categorized into 
two points. First, the processing speed is 
dominated by algorithm complexity. If the 
embedded compression algorithm consists of 
many multiplication operations such as 
transformation, then the operation speed is 
seriously limited. Therefore, the algorithm 
complexity should be kept as low as possible. 
Second, the embedded compression algorithm 
should be capable of parallel processing. Most 
lossless compression algorithms are with higher 
data dependency, and it seriously limits the 
performance of hardware parallelism.  
The hardware performance of [10] and [11] are 
seriously limited by the reason mentioned above. 
Although article [9] modifies JPEG-LS to be 
capable of parallel processing, its gate count is 
significantly increased and unacceptable. Based on 
proposed compacted-FELICS algorithm, the 
encoding capability of our proposed architecture 
can achieve Full-HD1080p@60Hz. The layout view 
and specification of prototype chip is illustrated in 
Fig. 6 and Table II, respectively.  
V. Conclusions 
In this paper, the embedded compression 
approach using compacted-FELICS algorithm is 
proposed to remove the data pendency and 
improve processing speed. Experiment results 
show that the proposed VLSI architecture 
demonstrates superior performance in comparison 
with other works. The prototype chip is 
implemented by TSMC 0.18-um with Artisan cell 
library, and the encoding capability can achieve 
Full-HD1080p@60Hz with two-level parallelism 
and four-stage pipelining.  
REFERENCES 
[1] The HDMI Website. [Online]. Available: 
http://www.hdmi.org. 
[2] T. Nishikawa et al., “A 60MHz 240mW MPEG-4 video-
phone LSI with 16 Mb embedded DRAM,” ISSCC, 2000. 
pp.230-231.  
[3] T. Hashimoto et al., “A 90mW MPEG4 video codec LSI 
with the capability for core profile,” ISSCC, 2001. pp.140-
141. 
[4] H. Arakida et al., “A 160mW, 80nA standby, MPEG-4 
audiovisual LSI with 16mb embedded DRAM and a 5 
GOPS adaptive post filter,” ISSCC, 2001. pp.1-11. 
[5] M. Ohashi et al., “A 27MHz 11.1mW MPEG-4 video 
decoder LSI for mobile application,” ISSCC, 2002. 
pp.366-367. 
[6] Marcelo J. et. Al, “The LOCO-I Lossless Image 
Compression Algorithm:Principles and Standardization 
into JPEG-LS”, IEEE TRANSACTIONS ON IMAGE 
PROCESSING, Aug, 2000, vol. 9, NO. 8, pp. 1309-1324. 
[7] X. Wu and N. D. Memon, “Context-based, adaptive, 
lossless image coding,” IEEE Trans. Commun., vol. 45, 
pp. 437-444, Apr. 1997. 
[8] P. G. Howard and J. S. Vitter, “Fast and efficient lossless 
image compression,” in Proc. IEEE Int. Conf. Data 
Compression, 1993, pp. 501-510. 
[9] M. Ferretti and M. Boffadossi, “A parallel Pipelined 
Implementation of LOCO-I for JPGE-LS,” ICPR 2004, pp. 
769-772. 
[10] Chih-Chi Cheng. et al. A, “Multi-Mode Embedded 
Compression Codec Engine for Power-Aware Video 
Coding System,” IEEE SIPS, 2005, pp.532-537. 
[11] Li Xiaowen et al, “A Low Power, Fully Pipelined JPEG-LS 
Encoder for Lossless Image Compression,” in Proc. IEEE 
Int. Conf. Multimedia and EXPO, 2007, pp. 1906-1909. 
TABLE II 
THE  SPECIFICATION OF PROTOTYPE CHIP 
TABLE I 
PERFORMANCE EVALUATION WITH OTHER PREVIOUS WORKS 
236
horizontal and vertical residuals are further combined to 
construct the associated geometric-based probability 
model (AGPM). The AGPM can be classified into three 
sections, including center-range, below-range and above-
range. According to the coding mode selection in Fig. 2, 
the geometric-based binary code is applied for center-
range, and content-adaptive Golomb-Rice code is for 
below-range and above-range. An exhaustive discussion 
of geometric-based binary code and content-adaptive 
Golomb-Rice code is described in following subsection, 
respectively. 
B. Content-Adaptive Golomb-Rice Code 
Golomb-Rice code is an efficient coding scheme for 
geometric distribution. However, in sophisticated lossless 
algorithm [6][7], the determination of k parameter in 
Golomb-Rice code is very complicated and consumes 
extra storage space. Therefore, content-adaptive Golomb-
Rice code is proposed to provide an efficient method for 
the determination of k parameter.  
The coding flow of content-adaptive Golomb-Rice is 
depicted in Fig. 3. The reference pixel N3 is employed to 
observe the local texture variation by the summation of 
abs(N3-N1) and abs(N3-N2). If this summation is much 
smaller, it indicates that local texture contains smooth area, 
and smaller k parameter is selected to achieve efficient 
codeword. On the other hand, the larger k parameter is 
applied. Consequently, the coding flow with the 
determination of k parameter is illustrated in Fig. 3. 
The candidate set of k parameter, 1, 2 and 3, is derived 
from our empirical principle. The threshold of summation 
is utilized to identify the degree of local texture variation, 
and three threshold values are selected as power of two, 
which is efficient for hardware implementation. For each 
degree, a proper k parameter is individually assigned. 
Consequently, the content-adaptive Golomb-Rice code 
exploits N3 to inspect the local texture variation, and the k
parameter is adaptively adjusted without any extra storage 
device. 
C. Geometric-Based Binary Code 
In center-range, the statistic of residual is represented by 
composite geometric distribution. However, the decay rate 
of center-range can not be relevantly modeled by 
exponential fashion owing to the uncertainty of spatial 
correlation between reference pixel of N1 and N2. As a 
result, Golomb-Rice code is not well-suited for center-
range. In center-range, geometric-based binary code is 
further proposed to provide an efficient coding scheme.  
    The center-range demonstrates that the current pixel 
closed to L or H has higher probability, and more efficient 
codeword should be assigned to it. It also indicates that the 
residual closed to N1 or N2 more often occurs in center-
range. Therefore, when N3 is also inside center-range, the 
difference between N3 and the other two reference pixels, 
N1 and N2, can be employed to inspect the local texture 
variation for each current pixel. Both abs(N3-N1) and 
abs(N3-N2) are applied to observe vertical and horizontal 
variation. Let N1=L and N2=H. If abs(N3-N1) is smaller 
than abs(N3-N2), it indicates that the local texture content 
belongs to vertical-oriented area, and H-P (N2-P), 
represented as vertical prediction, is selected as residual. 
On the other hand, the horizontal prediction, P-L (P-N1), 
is selected as residual. If N3 is not inside center-range, P-
L is directly applied as residual. 
Once the residual is determined, its range is between [0, 
delta], where delta denotes H-L. If its range, delta+1, is 
exactly equal to power of two, the residual is just encoded 
with log2(delta+1) bit. Otherwise, required number of bit 
is represented as following  
2 2log ( 1) # _ log ( 1)delta bit sample delta   « » ª º¬ ¼ « »   (1) 
where #bit_sample denotes how many bits is consumed to 
encoded the residual. Based on (1), the residual could be 
encoded with either 2log ( 1)delta « »¬ ¼ or 2log ( 1)delta ª º« » bit. 
Since center-range presents higher probability in both 
sides and lower probability in middle section, more 
efficient codeword should be assigned to both sides. 
Therefore, threshold, 2log ( 1)2 ( 1)delta deltaª º« »   , is exploited 
to determine the residual should be encoded with 
2log ( 1)delta « »¬ ¼ or 2log ( 1)delta ª º« » bit. If the residual is 
smaller than threshold, it also means that the residual is 
much closed to the one of both sides and should be 
encoded with 2log ( 1)delta « »¬ ¼  bit. Otherwise, the residual 
is replaced by the summation of residual and threshold.
This residual is encoded with 2log ( 1)delta ª º« »  bit. 
Probability
center
-range
below-
range
N1: reference pixel
N2: reference pixel
N3: reference pixel
P: current pixel
L H
Combination
Coding Mode Selection
Prediction 
template
above-
range
Probability
Intensity of 
current pixel P
L H
Image content
N1
N2
P
N3
Horizontal residual
AGPM
A: content-adaptive Golomb-Rice code
B: geometric-based binary code
C: content-adaptive Golomb-Rice code
A B C
Intensity of 
current pixel P
Vertical residual
Fig. 2.  The illustration of associated geometric-based probability model. 
TABLE I
PERFORMANCE EVALUATION WITH SOPHISTICATED LOSSLESS 
COMPERSSION ALGORITHMS
« »¬ ¼
NBSDUDPHWHU
NBSDUDPHWHU
8QDU\SDUW 5HVLGXDO
%LQDU\SDUW 5HVLGXDO 
Fig. 3. The coding flow of content-adaptive Golomb-Rice code. 
186
IV. EXPERIMENT RESULTS AND DISCUSSIONS
Table II shows the performance evaluation with other 
lossless compression VLSI architectures. The evaluation 
result reveals that the operation frequency of proposed 
architecture presents superior performance. The main 
reason is that our proposed algorithm demonstrates more 
compact coding flow, which mainly consists of geometric-
based binary code and content-adaptive Golomb-Rice 
code. Since each current pixel is independently encoded 
without other encoded information, parallel processing is 
compatibly applied in our VLSI architecture. On the 
contrary, JPEG-LS adopts more complex coding flow with 
serious data dependency, and makes a great limitation on 
processing speed and parallel processing capability. 
Among these articles [8]-[10], the processing speed of [9] 
can reach 183MHz; however, it consumes more hardware 
resource. Even though our work consumes more gate 
count and power consumption to achieve higher 
throughput, our work still presents superior performance 
in both parallelism and power efficiency. The maximum 
throughput of our work can achieve 776 MByte/s@194 
MHz. Moreover, the entire codec can be extended to four-
level parallelism for DFR technique in Full-HD 
1080p@120Hz . 
This work is implemented by TSMC 0.18-um 1P6M 
CMOS technology with Artisan cell library. The layout 
view and chip specification are depicted in Fig. 8, and the 
proposed VLSI architecture of entire codec is verified 
with prototyping system with LCD display, as shown in 
Fig. 9. 
V. CONCLUSIONS
In this paper, a high-speed lossless embedded compression 
algorithm with pipelining and parallel VLSI architecture is 
proposed for high-end LCD applications. Based on AGPM, 
geometric-based binary code and content-adaptive 
Golomb-Rice code construct compact coding flow and 
maintain competitive coding efficiency in comparison 
with sophisticated lossless compression algorithms. Based 
on proposed EC algorithm, parallelism and pipelining 
techniques can be compatibly incorporated to implement 
the VLSI architecture of entire codec. With regular data 
scheduling, the hardware is completely utilized without 
bubble cycle. This work is implemented by TSMC 0.18-
um 1P6M CMOS technology with Artisan cell library. The 
core size and die size is 1.62 x 1.62 mm2 and 2.12 x 
2.11mm2, respectively. Experiment results reveal that the 
processing capability achieves Full-HD 1080p@60Hz 
with RGB component by two-level parallelism, and four-
level parallelism can further support the DFR technique in 
Full-HD 1080p @120Hz.  
REFERENCES
[1] -XQ + 6RXN DQG -RQJVHR /HH ³5HFHQW 3LFWXUH 4XDOLW\
Enhancement Technology Based on Human Visual Perception in 
/&'79V´IEEE Trans. Journal of Display., vol. 3, no. 4, pp. 371-
376. Dec. 2007. 
[2] +HH*RRN/HH³6WUDWHJLF&RQVLGHUDWLRQIRU'HVLJQRI'LJLWDO79
6\VWHPLQ&KLS 3URGXFWV´ LQProc. IEEE Int. Conf. Asian Solid-
State Circuits (A-SSCC), Nov. 2007, pp. 1-4. 
[3] Sang Soo Kim, Nam Deog Kim, Brian H. Berkeley, Bong Hyun 
<RX+\RXQJVLN1DP-DH+H\XQJ3DUNDQG-XQS\R/HH³1RYHO
TFT-LCD Technology for Motion Blur Reduction Using 120Hz 
'ULQLQJZLWK0F)L´LQSID 2007, pp. 1003-1006. 
[4] T. Nishikawa,M. Takahashi, M. Hamada, T. Takayanagi, H. 
Arakida, N. Machida, H. Yamamoto, T. Fujiyoshi, Y. Maisumoto, 
O. Yamagishi, T. Samata, A. Asano, T. Terazawa, K. Ohmori, J. 
Shirakura, Y. Watanabe, H. Nakamura, S. Minami, T. Kuroda, and 
T. Furuyama, ‘‘A 60 MHz 240 mW MPEG-4 video-phone LSI with 
16 Mb embedded DRAM,’’ in Dig. Tech. Papers IEEE Int. Solid-
State Circuits Conf., 2000, pp. 230--231.
[5] Beric, A.; van Meerbergen, J.; de Haan, G.; Sethuraman, R, 
³0HPRU\&HQWULF 9LGHR 3URFHVVLQJ´ IEEE Trans.Circuit Syst. 
Video Technol., vol. 18, no. 4, pp. 439-452. Apr. 2008. 
[6] 3*+RZDUG DQG - 6 9LWWHU ³)DVW DQG HIILFLHQW ORVVOHVV LPDJH
FRPSUHVVLRQ´ LQProc. IEEE Int. Conf. Data Compression, 1993, 
pp. 501-510. 
[7] 0 - :HLQEHUJHU * 6HURXVVL DQG * 6DSLUR ³7KH /2&2,
Lossless Image Compression Algorithm: Principles and 
6WDQGDUGL]DWLRQ LQWR -3(*/6´ IEEE Trans. Image Processing,
vol. 9, no. 8, pp. 1309-1324, Aug. 2000. 
[8] ;LQNDL&KHQ+DQMXQ-LDQJ;LDR:HQ/L=KLKXD:DQJ³$1RYHO
&RPUSHVVLRQ0HWKRG IRU:LUHOHVV ,PDJH 6HQVRU1RGH´ LQProc. 
IEEE Int. Conf. Asian Solid-State Circuits (A-SSCC), Nov. 2007, 
pp. 184-187. 
[9] Markos Papadoniko Lakis, Vasilleios Pantazis, and Athanasios P. 
.DNDURXQWDV ´(IILFLHQW +LJK3HUIRUPDQFH $6,& ,PSOHPHQWDWLRQ
RI-3(*/6(QFRGHU´LQProc. Int. Design Automation and Test in 
Europe Conf & Exhibition., 2007, pp. 1-6. 
[10] Li Xiaowen et al ³$ /RZ 3RZHU )XOO\ 3LSHOLQHG -3(*/6
(QFRGHUIRU/RVVOHVV,PDJH&RPSUHVVLRQ´LQProc. IEEE Int. Conf.
Multimedia and EXPO, 2007, pp. 1906-1909. 
Fig. 8. Layout view and chip specification. 
TABLE II
PERFORMANCE EVALUATION WITH LOSSLESS COMPERSSION VLSI
ARCHITECTURE
Fig. 9. The prototyping system. 
188
40 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
Fig. 1. Illustration of prediction template in FELICS.
main techniques, including simplified adjusted binary code and
Golomb–Rice code with storage-less parameter selection,
are incorporated. Besides, the proposed color difference pre-
processing (CDP) can efficiently improve the coding efficiency
with simple arithmetic operation. Based on VLSI-oriented
FELICS algorithm, the proposed hardware architecture can be
extended to multilevel parallelism for advanced high-definition
(HD) display specifications. The rest of this paper is organized
as follows. In Section II, the background of FELICS algorithm
is introduced. Section III presents VLSI-oriented FELICS al-
gorithm. The proposed hardware architecture of VLSI-oriented
FELICS algorithm is discussed in Section IV. Experiment
results and discussions are described in Section V. Finally, the
conclusions are given in Section VI.
II. BACKGROUND OF FELICS ALGORITHM
The FELICS, proposed by P. G. Howard and J. S. Vitter in
1993, is a lossless compression algorithm with the advantage
of fast and efficient coding principle. Furthermore, FELICS
presents competitive coding efficiency in comparison with
other sophisticated lossless compression algorithms [22]. In
FELICS, three primary techniques, including intensity distri-
bution model, adjusted binary code and Golomb–Rice code,
are incorporated to construct complete coding flow. As shown
in Fig. 1, FELICS utilizes two reference pixels around current
pixel to yield the prediction template, and it can be divided into
four cases. In case 1, since surrounding reference pixels are not
available for the first two pixels, and , both current pixels
are directly packed into bitstream with original pixel intensity.
For case 2, successive pixels, and , are regarded as
reference pixels for current pixel . For non-first row, cases 3
and 4 clearly define the relationship between current pixel and
reference pixels. Between and , the smaller reference
pixel is represented as , and the other one is . As depicted
in Fig. 2, the intensity distribution model is exploited to predict
the correlation between current pixel and reference pixels. In
this model, the intensity that occurs between and is with
almost uniform distribution, and regarded as in range. The
intensity higher than or smaller than is regarded as above
range and below range, respectively. For in range, the adjusted
binary code is adopted, and Golomb–Rice code is for both
above range and below range.
Fig. 2. Probability distribution model of intensity in FELICS.
TABLE I
CODEWORD OF ADJUSTED BINARY CODE WITH THE RANGE OF [0, 4]
A. Adjusted Binary Code
Fig. 2 shows that the adjusted binary code is adopted in in
range, where the intensity of current pixel is between and .
For in range, the probability distribution is slightly higher in the
middle section and lower in both side sections. Therefore, the
feature of adjusted binary code claims that the shorter codeword
is assigned to the middle section, and longer one is assigned
to both side sections. To describe the coding flow of adjusted
binary code, the coding parameters should be first declared as
follows:
(1)
The adjusted binary code takes the sample of to be en-
coded, and range indicates that the number of possible sam-
ples should be encoded for a given delta. The upper bound and
lower bound denote the maximum and minimum number of bit
to represent the codeword for each sample, respectively. Partic-
ularly, the lower bound is identical to upper bound, while the
range is exactly equal to the power of two. The threshold and
shift number are utilized to determine which sample should be
encoded with upper bound bit or lower bound bit. If ,
the range is equal to [0, 4]. The codeword of each sample is il-
lustrated in Table I. According to (1), the required number of bit
is 2 for lower bound and 3 for upper bound. With the intensity
distribution in in range, 2 bits are allocated for the middle sec-
tion, including sample of [1, 2, 3], and 3 bits for side section,
including sample of [0, 4]. Table I reveals that the bit alloca-
tion for each sample is consistent with the probability distribu-
tion in in range. The formal principle of adjusted binary code
is more complex and an exhaustive description of it is given in
Section III.
B. Golomb–Rice Code
For both above range and below range, the probability dis-
tribution sharply varies with exponential decay rate, and the
42 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
Fig. 4. Illustration of   parameter update in cumulation table update.
additional computation load and storage requirement. In
order to maintain fast and efficient coding performance,
FELICS adopts a straightforward estimation method for
parameter selection. Since various delta values incur dif-
ferent exponential decay rate due to the diverse of image
texture, FELICS prepares a candidate set, which consists
of several parameters, and the most efficient parameter
is selected from this candidate set. An individual candidate
set is assigned to each delta value, and the parameter
that contains the minimum cumulative codeword length in
the candidate set is selected as the most efficient one for
Golomb–Rice code. The cumulation table is constructed to
store the cumulative codeword length for each delta value.
Moreover, for each delta value, the cumulative codeword
length is separately recoded for each parameter in can-
didate set. As shown in Fig. 4, the cumulation table is
constructed with and candidate set
of . If delta of current pixel is 253, the most
efficient parameter, which has the minimum cumulative
codeword length, is 1 in current cumulation table. Suppose
that for the residual of 2, Golomb–Rice code produces new
codeword with entire candidate set of ,
and the codeword length for each is 3, 3, 4, and 5 bits,
respectively. Then the cumulative codeword length of
each parameter in candidate set is immediately updated
to 20, 61, 27, and 85 bits. The updated cumulation-table
is successively referenced by next Golomb–Rice coding
procedure. Although this method is simple and efficient,
it still requires the storage capacity of bits,
where , and denote the number of bit to represent
each cumulative codeword length, each delta value and
the number of parameter in candidate set, respectively.
3) Data dependency: Even though FELICS applies a
simple and efficient method for k parameter selection
in Golomb–Rice code, it also induces serious data de-
pendency and limits hardware performance in parallel
processing. As shown in Fig. 5, both and are
encoded with Golomb–Rice code, and mapped to the
same delta value in cumulation table. In sequential pro-
cessing, since the cumulation table is completely updated
by encoding procedure, the updated cumulation-table
is available to be successively referenced by encoding
Fig. 5. Illustration of data dependency for parallel processing.
TABLE II
ILLUSTRATION OF ARITHMETIC OPERATION IN ADJUSTED BINARY CODE
procedure. However, in parallel processing, encoding
procedure cannot be simultaneously performed with
encoding procedure, since the cumulation table is currently
updated by encoding procedure and not available for
encoding procedure. Obviously, the data dependency
is a bottleneck in high-throughput applications.
B. Simplified Adjusted Binary Code With
Single-Side-Geometric Model
According to the intensity distribution model of in range, ad-
justed binary code allocates shorter codeword in middle section
and the longer one for both side sections. In order to observe
the computation complexity of adjusted binary code, the qual-
itative analysis is evaluated on the number of arithmetic op-
erations. Table II reveals that the pseudocode is demonstrated
for each coding procedure in adjusted binary code, and relative
arithmetic operation for each coding procedure is also extracted
from each line of pseudocode. Total arithmetic operation of en-
tire adjusted binary code incorporates six addition/subtraction
(add/sub), two compare (com), and two shift arithmetic oper-
ations. Since the coding procedure is successively performed,
these arithmetic operations are also sequentially executed from
hardware perspective. These sequential operations significantly
limit the processing speed and throughput rate. Among these
arithmetic operations, the circular rotation contains two add/sub
44 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
Fig. 6. Illustration of expected value with various parameter   for
Golomb–Rice code.
TABLE IV
AVERAGE EXPECTED VALUE FOR      
CR. Obviously, once the parameter is greater than 3, the ex-
pected value is greater than 4 and almost saturated. The coding
efficiency is seriously degraded while parameter is over 3. As
a result, can provide reasonable and efficient per-
formance on CR.
In order to determine which fixed parameter can present
better performance among , we apply a criterion for
further evaluation, and it is defined as
(9)
For a given fixed parameter, denotes the average
of expected value under . With (9), the corre-
sponding result is shown in Table IV. As a result, the fixed
parameter of 2 demonstrates better performance than the others.
With a fixed parameter, the data dependency is completely
removed, and the extra storage for cumulation table can be
omitted. Moreover, the competitive coding efficiency is also
maintained based on our theoretical analysis.
Indeed, while is greater than 10, the analysis result for the
determination of parameter is quite different from original
one. The main reason for the selection of is de-
scribed as follows. In original FELICS, an analysis is given to
describe the impact of parameter on codeword length based on
the parameter estimation method [19]. According to this anal-
ysis, the variance of the codeword length in Golomb–Rice code
is assigned to 1.24. However, this variance is optimized to adap-
tively adjust the parameter with cumulation-table in [19] in-
stead of storage-less parameter selection. Since the fixed value
of parameter is adopted in storage-less parameter selection,
the uncertainty of exponential decay rate should be carefully
considered. Moreover, the original FELICS indicates that the
efficient parameter, which can make more efficient codeword
for the most image content, is rarely over 3 [19]. The uncer-
tainty of exponential decay rate can be modeled by the adjust-
ment on , and the range of this adjustment should be properly
selected to be compatible with the most image content. In other
words, the reasonable range of adjustment on is equivalent to
yield the efficient parameter, which is consistent with that of
[19]. Therefore, the range of adjustment on is bounded within
, and Table IV shows that this range is reasonable for
the most image content, since the result is identical to that of
[19]. Consequently, with the extension of from 1 to 10, the
uncertainty of exponential decay rate is well modeled for the
most image content.
D. Color Difference Preprocessing
Although VLSI-oriented FELICS algorithm provides effec-
tive solution for hardware architecture design, they could also
induce the degradation in coding efficiency. The CDP, a simple
and efficient technique, is proposed to improve the coding
efficiency of VLSI-oriented FELICS algorithm. In principle,
the shorter codeword is assigned to smaller residual and longer
codeword for larger one. Therefore, the image with edge con-
tent results in much longer codeword, and presents less coding
efficiency. The CDP utilizes the difference between color com-
ponents to reduce the input residual of adjusted binary code and
Golomb–Rice code. The original color component including
and , is translated to CDP format by following formula:
(10)
(11)
(12)
where , and denote each color compo-
nent in CDP. According to (11) and (12), if both and
are directly derived from the subtraction of two color
components, the negative value will be presented, and the cor-
responding number system consumes more hardware resource.
Therefore, in order to avoid the negative value, a constant term
is added to and , respectively. With the 8 bits
for each pixel, the maximum value of 255 should be selected
as constant term to avoid the generation of negative value.
However, since the 255 consists of 8 bits of 1 in binary format,
it incurs serious ripple carry delay and additional glitch power
consumption. Hence, the number of 1 should be kept as low
as possible. Instead of 255, the 256, which only contains a
single bit of 1, is adopted as constant term. Since the CDP
applies the difference among each color component to generate
corresponding CDP color components, one of them should
be kept as original information and is exploited to covert the
other CDP color components back to original ones in decoding
procedure.
For a given image content of Coastguard sequence, color
components have similar intensity distribution in both and
components, as depicted in Fig. 7(a) and (b). Obviously, the
intensity distribution presents great amount of sharp fluctua-
tion in both and components, and it also indicates that the
46 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
TABLE VI
PERFORMANCE EVALUATION AMONG FELICS, VLSI-ORIENTED FELICS, AND
VLSI-ORIENTED FELICS WITH CDP
coding efficiency. To provide a quantitative analysis on the en-
ergy of in difference part, the effective energy of (EEY) is
defined as follows:
(14)
The difference part totally contains two CDP components,
derived from the difference between two color components in
RGB signal. Therefore, for difference part, the EEY applies the
partial differentiation to observe the energy of on both CDP
components, and the summation is subsequently performed
on both results of partial differentiation. According to (13)
and (14), the relationship between , and
is obtained as follows:
(15)
Since exhibits the most energy of , the superior
coding efficiency is achieved for . Although
is less than , the coding efficiency of both is quite
closed. With the analysis mentioned before, the CDP with
demonstrates efficient coding efficiency. Particu-
larly, if the entire image content is greatly dominated by or
component, the accuracy of this analysis result is limited.
E. Performance Evaluation in Coding Efficiency
The performance is evaluated on average CR for three
methods, FELICS, VLSI-oriented FELICS, and with CDP. Ten
most usual test sequences are applied. The evaluation result is
shown in Table VI, and FELICS presents better performance in
Akiyo, Mother and Daughter, and Hall monitor with CR over
2. The main reason is that these three test sequences contain
more smooth texture, and intensity variation is not obvious.
For FELICS, the prediction template utilizes the intensity
difference between current and neighbor pixels to generate the
residual for adjusted binary code or Golomb–Rice code. The
smooth texture presents smaller residual and makes efficient
codeword in both adjusted binary code and Golomb–Rice code.
On the contrary, Mobile calendar and Coastguard contain more
Fig. 8. Block diagram of the proposed hardware architecture of VLSI-oriented
FELICS algorithm with pipeline stage allocation.
Fig. 9. Proposed architecture with two-level parallelism. (a) Data scheduling.
(b) Line buffer with ping-pong mode operation.
complex texture and yield less efficient codeword. According
to the result of Table VI, the average CR of VLSI-oriented
FELICS is 1.70 and the degradation is only 2.3% in com-
parison with FELICS. Although the coding efficiency is less
than FELICS algorithm, the degradation is quite minor. The
simplified adjusted binary code effectively exploits SSGM to
model the intensity distribution, and greatly reduces sequen-
tial arithmetic operation. For Golomb–Rice code, the fixed
parameter of 2 is directly applied based on our theoretical
analysis. As a result, no extra storage element is required to
store the information for parameter selection and the data
dependency is completely removed. In order to further compen-
sate coding efficiency, the CDP employs the difference between
original color components to obtain more efficient codeword
as well as coding efficiency. The evaluation result reveals that
CDP greatly improves the average CR about 35% for FELICS
and 38% for VLSI-oriented FELICS, respectively. For more
complex texture, including Coastguard, Mobile calendar, and
Table tennis, CDP still demonstrates obvious improvement on
average CR.
IV. PROPOSED HARDWARE ARCHITECTURE FOR
VLSI-ORIENTED FELICS ALGORITHM
The proposed hardware architecture mainly consists of pre-
diction template module, intensity processing module, simpli-
fied adjusted binary code, Golomb–Rice code with storage-less
parameter selection, and bitstream generator. Because data de-
pendency has been completely removed by proposed algorithm,
48 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
Fig. 11. Analysis of data scheduling for each pattern of reference pixel in pre-
diction template module.
The prediction template module is responsible to make the
reference pixel available for each current pixel. In addition,
the access mechanism of reference pixels in line buffer should
follow the data scheduling mentioned before. Therefore, the
data path of prediction template module should be adaptively
adjusted to support various prediction patterns in prediction
template. In order to obtain a compact data path and superior
hardware utilization, an exhaustive analysis of data path is
necessary to be made. In Fig. 1, the prediction pattern is di-
vided into four cases, and the data scheduling for each case is
individually analyzed, as illustrated in Fig. 11. Once the regular
transposition among reference and current pixels is obtained,
it can be exploited to facilitate the design of data path in pre-
diction template module. For case 1, and are directly
packed into bitstream without any encoding procedure. For
each current pixel in case 2, both reference pixels are derived
from the latest two current pixels. For example, both reference
pixels of current pixel are derived from latest current pixels,
and . Similarly, the current pixel is also regarded
as the reference pixels for following current pixels: and
. This indicates that each current pixel can be reused as the
reference pixel for following two current pixels in case 2. For
case 2, from cycle 2 to cycle 5, the pixel of and
are derived from at the same cycle and at previous
cycle. and receive the pixel of and at
previous cycle. For case 3, both reference pixels, and ,
are read from line buffer for at cycle 1. Concurrently,
and are regarded as reference pixels for . Furthermore,
in case 4, the pixel of and are individually ob-
tained from at the same cycle and at previous cycle,
respectively. In addition, the and are read from
line buffer. According to the data scheduling in Fig. 11, the
regular transposition for each case is obtained and applied to
construct the data path of prediction template module, as shown
in Fig. 12. The multiplexer is used to switch the data path
of current or reference pixels among each case. The register
stores each current and reference pixel for following intensity
processing module.
The proposed architecture of intensity processing module is
illustrated in Fig. 13. At first, the difference between and
is derived through a subtraction, and its sign bit is used to iden-
tify which reference pixel is or . Once both and are ob-
tained, the sign bit of and , Sign H-P and Sign P-L,
Fig. 12. Proposed architecture of prediction template module.
Fig. 13. Proposed architecture of intensity processing module.
can be exploited to determine which coding mode should be
selected. Particularly, for Golomb–Rice code, the residual of
-1 and -1 can be derived by and ac-
cording to the following equations:
(16)
(17)
Therefore, and can be directly translated to
-1 and -1 through a simple inverter, and the
is also exactly identical to the residual of adjusted binary
code. With and , not only the coding mode can
be identified for each current pixel, but also the residual of
adjusted binary code and Golomb–Rice code can be efficiently
obtained without complex arithmetic operations. As a result,
the processing speed of intensity processing module can be
further improved.
C. Proposed Architecture of Simplified Adjusted Binary Code
and Golomb–Rice Code
As illustrated in Fig. 14, the proposed architecture of sim-
plified adjusted binary code module is classified into two parts:
parameter generator and code generator. In parameter generator,
the Delta, derived from intensity processing module, is passed
to CEILLOG2, and it generates upper bound bit information.
Concurrently, the range is also obtained by Add A. With range,
the threshold is derived according to (1), and the former term
50 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
Fig. 17. Three parallelisms to illustration the concept of multilevel parallelism. (a) Pipelined data scheduling. (b) Hardware architecture of VLSI-oriented FELICS
algorithm with multilevel parallelism.
assigned to each current pixel within the same group, and
it contains following primary blocks: intensity processing,
simplified adjusted binary code, and Golomb–Rice code with
storage-less parameter selection. The output codeword of
each parallel core is concatenated into the same bitstream by the
other share core, bitstream generator. Since the prediction tem-
plate module concurrently accesses individual reference pixels
for each current pixel within the same group, the line buffer is
partitioned into three segments with ping-pong operation. The
mechanism mentioned before can be further extended to higher
parallelism for more advanced high-end display specifications,
such as QHD(2560 1440) and QFHD(3840 2160).
F. CDP Function
As mentioned in Section III, the CDP mainly incorporates
original part and difference part. Since the original part is di-
rectly encoded without further manipulation, the computation
of CDP is primarily consumed on the difference part. For dif-
ference part, the computation of color difference incorporates a
subtraction and an addition according to (11) or (12). With the
8 bits for each pixel, the result of subtraction is between
and 255, and the word length of it is extended to 9 bits. Since
constant term is 256, which consists of a single bit of 1 at MSB
and 8 bits of 0, the addition is just performed on MSB instead
of entire word length. Practically, the addition for MSB is fur-
ther simplified as 1-bit inverter, and the corresponding result is
totally identical to that of 1-bit adder. For each difference part,
the subtraction and addition are implemented with 9-bit subtract
and 1-bit inverter, respectively. As a result, the hardware cost of
CDP is quite minor.
V. EXPERIMENT RESULTS AND DISCUSSION
The entire design incorporates CDP function, prediction
template, intensity processing, simplified adjusted binary code,
Golomb–Rice code with storage-less parameter selection,
and bitstream generator. Our target application is Full-HD
1080p, including complete RGB components. As mentioned
in Section IV, the design strategy of two-level parallelism
and four-stage pipelining are adopted to achieve this target
application. The performance evaluation with other existing
works, which feature high-speed performance in lossless
compression, is shown in Table VII. The author in [16] uses
dedicated hardware to implement the X-MatchPRO algorithm
and applies four-level parallelism for higher throughput. Since
the context addressable memory (CAM) is adopted to construct
its architecture, it also consumes outstanding area resource.
Therefore, for high-level parallelism, the area could be sig-
nificantly increased for [16]. In [17], the dedicated processor,
X-MatchProRli processor with two-level parallelism, is em-
ployed to construct a high-speed lossless compression system.
Although its throughput is much higher in comparison with
[16], the gate count consumption is more significant. In other
words, the processor-based implementation presents lower
area efficiency. For [26], with DWT and SPIHT, both lossless
and lossy applications are supported. However, the processing
speed is mainly limited by its complex transform and entropy
coding operations. Hence, its performance just achieves the
VGA(640 480) at 30 Hz of 4:2:0. The hardware architec-
ture of JPEG-LS is further partitioned in [25] into six parts
for pipelining. Moreover, the prediction error calculation is
proposed to remove the data dependency. Thus, the processing
speed is further improved. For [24], since the data dependency
is not removed, the processing speed is seriously limited, and
its throughput is also affected. In summary, the throughput is
primarily dominated by two issues, processing speed and the
capability of parallelism. Although more complex algorithm
can provide better coding efficiency, the processing speed
and the capability of parallelism are seriously limited by the
great amount of sequential arithmetic operations and data
dependency.
Since the parallelism and operation frequency are quite
different for each existing work, a fair comparison is provided
to make a more objective evaluation on hardware performance.
Two factors, parallelism efficiency and power efficiency, are
applied to evaluate the performance on parallelism and power
consumption, respectively. Table VII reveals the evaluation on
hardware performance between existing works and our pro-
posed architecture. The parallel-oriented architecture provides
a throughput of 200 MB/s in [16] and 275 MB/s in [17]. In
terms of parallelism efficiency, only 50 MB/s per parallelism
and 137.5 MB/s per parallelism are reached, respectively.
From [24] to [26], these works are totally implemented with
52 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS, VOL. 18, NO. 1, JANUARY 2010
4.36 Gb/s. The chip is fabricated in TSMC 0.13- m 1P8M
CMOS technology with Artisan cell library, and the core size
is 0.84 mm 0.86 mm. The experiment result shows that the
proposed architecture presents superior parallelism efficiency
and power efficiency in comparison with other existing works,
which features high-speed lossless data compression topic.
Moreover, with multilevel parallelism, the proposed archi-
tecture can be further applied in more advanced HD display
specifications such as QHD and QFHD.
ACKNOWLEDGMENT
The authors would like to thank the National Chip Implemen-
tation Center (CIC) for providing the service of chip fabrication,
measurement, and design tool support.
REFERENCES
[1] Information Technology-Digital Compression and Coding of Contin-
uous-Tone Still Image, ISO/IEC 10918-1 and ITU-T Recommendation
T.81, 1994.
[2] JPEG 2000 Part 1 Final Draft International Standard, ISO/IEC
FDIS15444-1, Dec. 2000.
[3] Information Technology-Coding of Audio-Visual Objects—Part 2: Vi-
sual, ISO/IEC 14 49602, 1999.
[4] Joint Vide Team (JVT) of ISO/IEC MPEG and ITU-T VCEG, Draft
ITU-T Recommendation and Final Draft International Standard of
Joint Video Specification, ITU-T Rec. H.263/ISO/IEC 14 496-10
AVC, 2003.
[5] P.-C. Tseng, Y.-C. Chang, Y.-W. Huang, H.-C. Fang, C.-T. Huang, and
L.-G. Chen, “Advances in hardware architectures for image and video
coding-a survey,” Proc. IEEE, vol. 93, no. 1, pp. 184–197, Jan. 2005.
[6] S.-Y. Chien, Y.-W. Huang, C.-Y. Chen, H. H. Chen, and L.-G. Chen,
“Hardware architecture design of video compression for multimedia
communication systems,” IEEE Commun. Mag., vol. 43, no. 8, pp.
123–131, Aug. 2005.
[7] T.-M. Liu, T.-A. Lin, S.-Z. Wang, and C.-Y. Lee, “A low-power dual-
mode video decoder for mobile applications,” IEEE Commun. Mag.,
vol. 44, no. 8, pp. 119–126, Aug. 2006.
[8] T.-C. Chen, S.-Y. Chien, Y.-W. Huang, C.-H. Tsai, C.-Y. Chen,
T.-W. Chen, and L.-G. Chen, “Analysis and architecture design of an
HDTV720p 30 frames/s H.264/AVC encoder,” IEEE Trans. Circuit
Syst. Video Technol., vol. 61, no. 6, pp. 673–688, Jun. 2006.
[9] D. Huffman, “A method for the construction of minimum redundancy
codes,” Proc. IRE, vol. 140, pp. 1098–1011, Sep. 1952.
[10] S. Colomb, “Run length encoding,” IEEE Trans. Inf. Theory, vol.
IT-12, no. 3, pp. 399–401, Jul. 1966.
[11] G. G. Langdon, “An introduction to arithmetic coding,” IBM J. Res.
Dev., vol. 28, no. 2, pp. 135–149, Mar. 1984.
[12] J. Ziv and A. Lempel, “A universal algorithm for sequential data com-
pression,” IEEE Trans. Inf. Theory, vol. IT-23, no. 2, pp. 399–401, May
1977.
[13] T. Welsh, “A technique for high-performance data compression,” IEEE
Comput., vol. 17, no. 6, pp. 8–10, Jun. 1984.
[14] L. Yang, R. P. Dick, H. Lekatsas, and S. Chakradhar, “CRAMES: Com-
pressed RAM for embedded systems,” in Proc. Int. Conf. Hardware/
Software Codes. Syst. Synth., Sep. 2005, pp. 93–98.
[15] L. Yang, H. Lekatsas, and R. P. Dick, “High-performance operating
system controlled memory compression,” in Proc. Int. Conf. Des.
Autom. Conf., Jul. 2006, pp. 701–704.
[16] J. Luis and S. Jones, “Gbit/s lossless data compression hardware,” IEEE
Trans. Very Large Scale Integr. (VLSI) Syst., vol. 11, no. 3, pp. 499–510,
Jun. 2003.
[17] M. Milward, J. L. Nunez, and D. Mulvaney, “Design and implementa-
tion of a lossless parallel high-speed data compression system,” IEEE
Trans. Parallel Distrib. Syst., vol. 15, no. 6, pp. 481–490, Jun. 2004.
[18] R. Mehboob, S. A. Khan, and Z. Ahmed, “High speed lossless data
compression architecture,” in Proc. IEEE Int. Conf. Multitopic, 2006,
pp. 84–88.
[19] P. G. Howard and J. S. Vitter, “Fast and efficient lossless image
compression,” in Proc. IEEE Int. Conf. Data Compression, 1993, pp.
501–510.
[20] X. Wu and N. D. Memon, “Context-based, adaptive, lossless image
coding,” IEEE Trans. Commun., vol. 45, no. 4, pp. 437–444, Apr. 1997.
[21] Lossless and Near-Lossless Coding of Continuous Still Images (JPEG-
LS), ISO/IEC JTC1/SC29 WG1 ITU-T SG8 (JPEG/JBIG), CD 14495,
1998.
[22] M. J. Weinberger, G. Seroussi, and G. Sapiro, “The LOCO-I lossless
image compression algorithm: Principles and standardization into
JPEG-LS,” IEEE Trans. Image Process., vol. 9, no. 8, pp. 1309–1324,
Aug. 2000.
[23] X. Xie, G. L. Li, X. K. Chen, C. Zhang, and Z. H. Wang, “A low com-
plexity near-lossless image compression method and its ASIC design
for wireless endoscopy system,” in Proc. Int. Conf. ASICON, 2005, pp.
37–40.
[24] L. Xiaowen, X. Chen, X. Xie, G. Li, L. Zhang, C. Zhang, and Z. Wang,
“A low power, fully pipelined JPEG-LS encoder for lossless image
compression,” in Proc. IEEE Int. Conf. Multimedia EXPO, 2007, pp.
1906–1909.
[25] M. P. Lakis, V. Pantazis, and A. P. Kakarountas, “Efficient high-perfor-
mance ASIC implementation of JPEG-LS encoder,” in Proc. Int. De-
sign Autom. Test Europe Conf. Exhib., 2007, pp. 1–6.
[26] C.-C. Cheng, P.-C. Tseng, C.-T. Huang, and L.-G. Chen, “Multi-mode
embedded compression codec engine for power-aware video coding
system,” in Proc. IEEE Workshop. Signal Process. Syst., 2005, pp.
532–537.
[27] A. Netravali and J. O. Limb, “Picture coding: A review,” Proc. IEEE,
vol. 68, pp. 366–406, 1980.
Tsung-Han Tsai (S’96–M’98) received the B.S.,
M.S., and Ph.D. degrees in electrical engineering
from the National Taiwan University, Taipei, Taiwan,
in 1990, 1994, and 1998, respectively.
From 1999 to 2000, he was an Associate Professor
of electronic engineering at Fu Jen University.
In 2000, he joined the Department of Electrical
Engineering, National Central University, Jhongli,
Taiwan, where he is currently an Associate Professor.
He has been awarded 14 patents and has authored or
coauthored more than 120 referred papers published
in international journals and conferences. His current research interests include
very large scale integration (VLSI) signal processing, video/audio coding algo-
rithms, DSP architecture design, wireless communication, and system-on-chip
design.
Dr. Tsai is a member of the Audio Engineering Society (AES) and the In-
stitute of Electronics, Information and Communication Engineers (IEICE). He
received the Industrial Cooperation Award in 2003 from the Ministry of Educa-
tion, Taiwan. He is a member of the Technical Committee of the IEEE Circuits
and Systems Society. He is also a member of the Technical Program Commit-
tees or the Session Chair of several international conferences.
Yu-Hsuan Lee was born in Taipei, Taiwan, in 1979.
He received the B.S. degree from the Department of
Electronics Engineering, Southern Taiwan Univer-
sity, Tainan, Taiwan, in 2001, and the M.S. degree in
2003 from the Department of Electrical Engineering,
National Central University, Jhungli, Taiwan, where
he is currently working toward the Ph.D. degree at
the Graduate Institute of Electrical Engineering.
His current research interests include the
video/image processing, coding, and very large scale
integration (VLSI) architecture design.
Yu-Yu Lee was born in Kaohsiung, Taiwan, in 1984.
He received the B.S. degree from the Department
of Electronics Engineering, Chang Gung University,
Taipei, Taiwan, in 2006, and the M.S degree from
the Department of Electrical Engineering, National
Central University, Jhungli, Taiwan, in 2008.
He is currently with the Department of Electrical
Engineering, National Central University. His
research interests include the image/video coding
algorithms, video signal processing, and associated
very large scale integration (VLSI) architectures.
external memory is allocated to store intermediate data of 
each 4x4 Y matrix, as shown in Fig. 1. In H.264/SVC JSVM, 
the data structure of IDCT consists of two parts: quantized 
level and de-quantized coefficient. Both are represented as lij 
and cij, respectively. The sub-index of i and j stands for the 
row and column for each coefficient and level. The 
quantized level is derived from VLC decoding, and 
subsequently de-quantized to obtain the coefficient. Then, 
each coefficient is converted to spatial residual by IDCT 
computation. It indicates that only coefficient is available 
for IDCT computation instead of level.  
The TI DSP C64+, a popular and practical platform for 
image/video processing, incorporates dual-part computing 
function: part A and part B. Each part includes 32-bit 
register file, internal memory and 4 sets of task engine (TE) 
to execute the arithmetic operation for image/video 
processing [7]. At first, the cij is fetched from the external 
memory through 128-bit system bus, and individually 
scheduled to part A and part B by 64-bit internal bus. For 
each part, 4 sets of TE are responsible to perform the 
computation of IDCT. Since both cij and lij are organized 
with column-based interleaving order, the column-based 
interleaving indicates both cij and lij are interlaced column 
by column. As illustrated in Fig. 2, at first memory access, 
c00 and c01 are allocated to part A, and part B is for c02 and 
c03. Hence, for each part, only 2 sets of TE can be utilized 
for IDCT computation, and the other 2 sets are wasted 
owing to the unavailable data of lij. It indicates that the 
hardware utilization of TE is degraded, and extra cycles are 
consumed to fetch remaining coefficients. This inefficient 
data structure seriously degrades the performance of entire 
DSP platform. 
 
3. THE PROPOSED CYCLE-EFFICIENT IDCT 
ALGORITHM  
 
In this section, the cycle-efficient IDCT algorithm in DSP 
platform is proposed for H.264/SVC applications. Three 
core techniques are incorporated: data structure reordering, 
symmetrical-based scheduling and interleaving-parallelism 
technique parallelism. The detail of each core technique is 
individually described in following sub-sections. 
 
3.1. Data Structure Reordering 
 
As mentioned previously, the TE utilization is seriously 
degraded owing to the data structure of IDCT in H.264/SVC 
JSVM. To improve TE utilization, the data structure 
reordering rearranges the data structure of IDCT to 
overcome this limitation. At first, by enhanced direct 
memory access controller (EDMA), the original data 
structure is transposed from row-based interleaving to 
column-based interleaving, as shown in Fig. 3. Subsequently, 
the EDMA transmits newly data structure to internal 
memory inside the DSP core.  
As a result, for each access of internal memory, entire 
row of cij can be concurrently transmitted to both 4 sets of 
TE in part A and part B. Since only Coeff [i] is the available 
data for IDCT, following accesses of internal memory are 
dedicated to fetch cij instead of lij. Consequently, the 
hardware utilization of both 4 sets of TE can be greatly 
improved. 
 
 
Fig. 1. The framework of DSP core. 
 
 
 
Fig. 2. The data structure of IDCT in H.264/SVC JSVM. 
 
Fig. 3. The illustration of data structure reordering for IDCT in
H.264/SVC. 
1115
symmetrical instructions should be fully allocated to each 
TE. However, 4 sets of TE own individual instruction type 
and latency, as shown in Table I [8]. Hence, some 
instructions in Fig. 4(a) are replaced to be compatible with 
TE type. For example, the “SHR” is replaced by the “SHVR” 
at TE_M. Besides, since “ADDSUB2” is dedicated for TE_L; 
therefore, it is divided into “ADD2” and “SUB2” for other 
TE. Owing to various latency of each instruction, these 
instructions are allocated for each TE in interleaving order 
to concurrently calculate z00~z30 and z01~z31. 
With interleaving-parallelism technique, both z00~z30 and 
z01~z31 can be obtained by 4 sets of TE in part A. Similarly, 
both z02~z32 and z03~z33 can be concurrently obtained by the 
4 sets of TE in part B. Once the Z is completely achieved, 
the product of ZA in (2) can be also obtained by the same 
mechanism mentioned above. As a result, a 4x4 IDCT is 
completely accomplished. 
 
4. EXPERIMENT RESULT AND DISCUSSION 
 
As depicted in Fig. 6, the cycle reduction is profiled for a 
4x4 IDCT computation. Based on TI C64+ 600MHz DSP, 
the original IDCT in H.264/SVC JSVM reference software 
[9] consumes 372 cycles. With data structure reordering, it 
can be reduced to 38.71%. Based on symmetrical-based 
scheduling and interleaving-parallelism technique, the cycle 
count is further reduced to 15.32% and 5.38%, respectively. 
With three techniques, only 20 cycles are consumed for each 
4x4 IDCT computation. The IDCT processing speed is 
accelerated as high as 18.6 times. For two layers of spatial 
scalability (4CIF and CIF) with 30 fps, the proposed 
algorithm just occupies 2.7% of total cycles in TI C64+ 
600MHz DSP. 
  The performance evaluation with existing work is 
demonstrated in Table II. The [3] applies single instruction 
and multi data (SIMD) to realize 4x4 IDCT in PC, and its 
algorithm is transplanted to the DSP for a fair comparison. 
Since it doesn’t consider the limitation memory access, the 
amount of 284 cycles is consumed, and it is much higher 
than ours. Although the article [4] presents comparable 
performance, the ASIC is additionally embedded into DSP 
core. In other words, this scheme is not totally DSP-
compliant fashion. The method of [5] adopts the built-in 
function of DSP in 8x8 IDCT, and its normalized cycle 
count for 4x4 IDCT is 33.75. As a result, Table II reveals 
that the proposed cycle-efficient IDCT algorithm presents 
superior performance in both cycle and MIPS. 
 
5. CONCLUSIONS 
 
In this paper, the cycle-efficient IDCT algorithm is proposed 
for H.264/SVC with DSP platform. Three primary 
techniques are included: data structure reordering, 
symmetrical-based scheduling and interleaving-parallelism 
technique. Based on proposed algorithm,  only 20 cycles are 
consumed. In comparison with other works, the proposed 
algorithm exhibits superior performance. With two spatial 
layers of 4CIF and CIF, the IDCT processing speed is 
accelerated as high as 18.6 times under 30 fps.  
 
6. REFERENCE 
 
[1] ITU-T Rec. H.264 and ISO/IEC 14496-10 ,”Advanced video 
coding for generic audiovisual services,”  v8, Nov 2007. 
[2] “Draft ITU-T recommendation and final draft international 
standard of joint video specification (ITU-T Rec. 
H.264/ISO/IEC 14 496-10 AVC,”in Joint Video Team (JVT) 
of ISO/IEC MPEG and ITU-T VCEG,JVTG050, 2003. 
[3] Joohyun Lee, Gwanggil Jeon, Sangjun Park, Taeyoung Jung, 
and Jechang Jeong, “SIMD Optimization of the H.264/SVC 
decoder with efficient data structure,” in Proc. ICME 2008, 
Hannover, Germany, June 2008. 
[4] Sung D.Kim, Jeong H.Lee, Jung M. Yang, Myung H. Sunwoo, 
and Sung K. Oh, “Novel instructions and their hardware 
architecture for video signal processing ,” in Proc. ISCAS 2005, 
Kobe, Japan, May 2005. 
[5]Texas Instruments,” TMS320C64x+ DSP Image/Video 
Processing Library Programmer’s Reference,” Literature 
number  SPRUEB9  March 2006 
[6] Julien Reichel, Heiko Schwarz, Mathias Wien, “Proposed 
modifications for joint Scalable Video Model 10” Doc. JVT-
W202, San Jose, California, USA, 21–27 April, 2007. 
[7] Texas Instruments, “TMS320C64x DSP Two-Level Internal 
Memory Reference Guide,” Literature number SPRU610C, 
February 2006. 
[8] Texas Instruments, “TMS320C64x/C64x+ DSP CPU and 
Instruction Set Reference Guide,” Literature number 
SPRU732G, February 2008. 
[9] Jerome Vieron, Mathias Wien, Heiko Schwarz, “JSVM 10 
software” Doc. JVT-W203, San Jose, California, USA, 21–27 
April, 2007. 
 
Fig. 6. Profiling of cycle reduction for a 4x4 IDCT. 
 
TABLE II 
 The Performance Evaluation with Existing Works 
 
 Framework Cycles /4x4 IDCT 
SIMD [3] software 286 
64x+[5] software 33.75 
ASSP[4] ASIC 24 
Proposed IDCT 
algorithm software 20 
 
1117
1278 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
Fig. 1. LCD-TV display system.
3) Both with minor order of context. This feature leads to
a compact coding flow the computation complexity of
entropy coding can be greatly saved.
4) The supported functions incorporate the lossless mode
for quality-oriented purpose and rate control mode for
memory-efficient purpose.
The characteristics of the proposed VLSI architecture are
listed as follows.
1) With pixel-based parallelism and segment-based paral-
lelism, the codec is designed with fully-parallel and
pipelining strategy.
2) The entire codec architecture can be further extended to
multi-level parallelism for QHD and QFHD with DFR
applications.
The rest of this paper is organized as follows. Section
II describes the distinction between EC and existing image
compression algorithms. Section III presents the proposed
high-speed EC algorithm. In Section IV, the VLSI architecture
of entire codec is demonstrated. Experiment results and the
discussion are given in Section V. Finally, the conclusion is
given in Section VI.
II. Distinction Between EC and Existing Image
Compression Algorithms
A. Feature of EC: Throughput-Oriented Purpose
In this paper, for advanced-HD applications, the EC puts
the effort on how to deliver an emerging compression scheme
for throughput-oriented purpose. The throughput can be further
decomposed into two factors: processing speed and parallelism
capability. These two factors are most dependent on the algo-
rithm complexity. Unfortunately, the higher coding efficiency
is usually accompanied by the intensive algorithm complexity
and serious data dependence.
As illustrated in Fig. 2, in existing image compression
algorithms, JPEG [5] adopts discrete cosine transform (DCT)
with Huffman coding to greatly remove spatial redundancy
and statistic redundancy. Furthermore, JPEG-2000 [6] even
applies discrete wavelet transform (DWT) and embedded block
coding with optimized truncation to achieve higher coding
efficiency. However, such complex algorithms also limit their
feasibility on advanced-HD applications. Instead, both JPEG-
LS [7] and CALIC [8] pursue moderate coding efficiency on
a lossless/near lossless coding scheme. Besides, FELICS [9]
Fig. 2. Categorization of existing image compression algorithms and EC on
the aspect of coding efficiency, algorithm complexity, and throughput.
more concentrates on a lossless scenario. Although JPEG-LS,
CALIC, and FELICS own more compact algorithm complex-
ity, the context-modeling technique seriously restricts their
processing speed and parallelism capability. More analysis on
this part is discussed in the next subsection.
Consequently, the existing image compression methods
suffer serious restrictions on advanced-HD applications. In
this paper, the proposed EC focuses on how to establish a
lossless/near lossless coding scheme for throughput-oriented
purpose. With a reasonable coding efficiency, how to reduce
the algorithm complexity and enable the parallelism capability
are the primary design considerations in this paper.
B. Limitation of Context-Modeling Technique on Processing
Speed and Parallelism Capability
In image coding, the geometric distribution is a well-known
probability model to estimate the distribution of prediction
residual [10]. However, it still exhibits the uncertain effect
of statistics, such as dc offset [11]. The context-modeling
technique is applied to overcome it and improve the coding
efficiency. To explain the context-modeling technique, the
prediction scheme of JPEG-LS is adopted as an example for
the following discussion. As shown in Fig. 3(a), the JPEG-LS
applies median predictor to predict the current pixel, P, and
then obtains the estimated current pixel, EP. Subsequently, the
prediction residual is achieved by taking the different between
P and EP. The context-modeling technique estimates the prob-
ability distribution of prediction residual based on the texture
complexity among reference pixels. The JPEG-LS adopts three
gradients, g1, g2 and g3, among a, b, c and d to represent
the texture complexity. This texture complexity is categorized
into 365 regions. It can be further represented as the set,
C = {ci, i = 1, . . . , 365}, and referred to as the 365th order
of context. Each ci requires the parameter set to record the
statistic data, and the parameter set is stored into the context-
table. Once the prediction residual is completely encoded,
its corresponding parameter set is subsequently updated. The
newly updated context-table is referenced by next prediction
residual.
Although the coding efficiency can be improved, the data
dependence is also induced, as shown in Fig. 3(b). The
parameter set of each ci is stored into context-table. Once
both encoding tasks of P1 and P2 are mapped to the same
parameter set, c364, the context-table should be updated by
1280 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
probability distribution for each range is given as follows:⎧⎪⎪⎨
⎪⎪⎩
PAbove(ε) = ρεAbove(1 − ρAbove) P > H
PBelow(ε) = ρεBelow(1 − ρBelow) P < L
PCenter(ε) = 1NF [ρεAbove(1 − ρAbove) + ρεBelow(1 − ρBelow)]
L ≤ P ≤ H
(2)
where ε denotes the prediction residual, and the NF is the
normalization factor. Both ρ Above and ρ Below are constants
between 0 and 1. The geometric-based binary code is estab-
lished as the entropy coding scheme for Center-range. For both
Above-range and Below-range, the entropy coding scheme is
content-adaptive Golomb-Rice code. For both entropy coding
schemes, the texture complexity determines how many orders
of context are adopted. Owing to the different probability dis-
tribution in (2), the texture complexity is separately considered
for both entropy coding schemes.
B. Content-Adaptive Golomb-Rice Code
For Above-range and Below-range, the distribution of pre-
diction residual is modeled as the geometric distribution in
(2). The Golomb code is an efficient entropy coding scheme
for geometric distribution. The Golomb code comprises two
parts: quotient and remainder
Golomb code{
quotient :
⌊
x
m
⌋
encoded with unary code
remainder : xmod m encoded with binary code (3)
where m is a positive integer, and x is the residual. The
quotient and the remainder are encoded with unary code and
binary code, respectively. To distinguish unary code and binary
code in decoding, an isolation bit, 1 bit of “0,” is inserted
between both. If m is equal to the power of two, this coding
scheme is referred to as Golomb-Rice code and listed as
follows:
Golomb Rice code{
quotient :
⌊
x
2k
⌋
encoded with unary code
remainder : xmod 2k encoded with binary code (4)
where k is a positive integer and also represents the number
of bit for remainder. For Golomb code, a divider circuit is
necessary; however, it could limit the processing speed. For
Golomb-Rice code, the divider circuit can be replaced by a
simple logic shifter. Hence, the Golomb-Rice is quite efficient
for hardware implementation.
In Golomb-Rice code, the coding efficiency is quite sen-
sitive to the k parameter. We further establish the content-
adaptive Golomb-Rice code to adaptively select the k param-
eter. Basically, for each current pixel, its texture complexity
is applied to determinate k parameter. The texture complexity
is classified into three levels: smooth-level, nature-level, and
edge-level. This is also referred to as the third order of context.
For each level, an individual k is allocated to encode the
prediction residual.
The coding flow of content-adaptive Golomb-Rice code is
depicted in Fig. 6. At first, k is determined by the texture
complexity, and it is defined as the texture−complexity = |N3
− N1|+ |N3 − N2|. By comparing it with smooth−threshold
TABLE I
Parameter Definition in Entropy Coding
Entropy Coding Parameter Definition
Content-adaptive Golomb-
Rice code
Smooth−threshold = 8
Nature−threshold = 16
Geometric-based binary
code
delta = H-L range=delta+1
upper−bit = log2(range)
lower−bit = log2,(range)
Threshold = 2upper−bit-range
Fig. 6. Coding flow of content-adaptive Golomb-Rice code.
and nature−threshold, the texture complexity can be identi-
fied, and an individual k parameter is allocated. For efficient
hardware realization of comparator, both smooth−threshold
and nature−threshold are selected from the numbers in power
of two. As shown in Table I, both smooth−threshold and
nature−threshold are set as 8 and 16 to achieve reasonable
coding efficiency. The k parameter of 1, 2, and 3 is separately
allocated to smooth-level, nature-level, and edge-level. The
reason why to choose the k = [1, 2, 3] is explained in the next
paragraph. Subsequently, the prediction residual is defined as
the P-H-1 for Above-range, and L-P-1 for Below-range. The
prediction residual is encoded with unary code and binary
code, respectively. Then, unary code and binary code are
combined with an isolation bit, 1-bit of “0,” between both.
To determine the range of k, the impact of k on entropy
is analyzed as follows. According to information theory, the
entropy is represented as follows:
H(R) =
∑
ε∈R
p(ε) 1
log2 p(ε)
(5)
where ε denotes the prediction residual, and it can be repre-
sented by the set, R = {0, . . . , 254} for 8-bit format of each
pixel. The second term of (5) is the self-information, and it can
be replaced by the number of bit in content-adaptive Golomb-
Rice code
N(ε) =
⌊ ε
2k
⌋
+ k + 1 (6)
where N(ε) denotes the number of bit in content-adaptive
Golomb-Rice for the prediction residual, ε. The first and
second terms are the number of bit for unary part and binary
part, respectively. Besides, one bit is inserted between both
parts for the identification. According to (2), the geometric
distribution is applied for both Above-range and Below-range;
1282 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
Fig. 9. Proposed high-speed EC algorithm. (a) Encoding flow chart. (b)
Execution order of encoding and the location of the check-point.
N1, N2, and N3, for each current pixel. Once the reference
pixels are prepared, the intensity processing identifies which
entropy coding scheme should be selected. If the current pixel
belongs to Center-range, the geometric-based binary code is
adopted. Otherwise, the content-adaptive Golomb-Rice code
is applied for both Above-range and Below-range.
To promise the saving ratio of memory bandwidth, the
compression ratio (CR) must reach the target CR (TCR), and
this is the primary object of rate control part. For example, if
TCR = 2, it indicates that the rate control can guarantee 50%
of memory bandwidth to be saved. To achieve this goal, two
parameters are introduced: on-line CR (OCR) and check-point.
The OCR dynamically records the present CR by accumulating
the code length of each codeword. The check-point is located
at the end of each row, as depicted in Fig. 9(b). The encoding
procedure periodically compares OCR with TCR at every
check-point. As a result, the rate control can precisely monitor
whether OCR satisfies TCR.
In rate control part, the pre-scalar quantizes both current
pixel and reference pixels from context template by 2Q. Then,
if the check-point is not reached, both quantized current pixel
and reference pixels are transmitted to intensity processing,
and the following procedure is totally identical to that of
lossless mode. Once the check-point is reached, both OCR and
TCR are compared with each other. If the OCR is greater than
TCR, the Q is decreased by 1; otherwise, the Q is increased by
1. The pre-scalar receives this newly updated Q and applies it
in the next row. Consequently, after the encoding of each row,
the OCR can be gradually approached to TCR. Note that since
the quantization factor is 2Q, the pre-scalar can be realized
with a simple logic-right shifter instead of a complex divider.
In Fig. 9(b), the Q of row 1 is unconditionally set to 0. At
the check-point of row 1, the accumulated bit length is applied
to calculate the OCR of row 1. By comparing OCR with TCR,
the Q of row 2 can be determined according to the coding flow
in Fig. 9(a). This mechanism is repeatedly performed until the
encoding procedure of the last row is accomplished. In the
decoding procedure, since the Q is directly set to 0 at row 1,
the OCR can be availably calculated by accumulating the bit
length of each codeword. By comparing OCR with TCR, the Q
of row 2 can be obtained. Hence, the Q information is derived
from the calculation, and not required to be packed into
bitstream. Both encoding and decoding follow the identical
rate control mechanism to adjust the Q at each row. As a result,
each pixel can be correctly reconstructed in rate control mode.
As mentioned in Section II, the data dependence primarily
results from the context-modeling technique instead of the
prediction scheme. In Fig. 9(b), the reason why both P1 and
P2 can be simultaneously encoded is separately discussed on
lossless mode and rate control mode. The encoding execution
order makes the reference pixels of NA, NB, NC, ND, and NE
available before the encoding of P1 and P2. In lossless mode,
since the original pixel of P1 is the same as its reconstructed
pixel, the P1 is available to be the reference pixels of P2.
As a result, both P1 and P2 can be concurrently encoded.
In rate control mode, the original pixel of P1 is not equal
to its reconstructed pixel; therefore, it is not available to
be the reference pixels of P2 until Q of row i is obtained.
Nevertheless, as mentioned previously, since the Q of row i is
already determined at the end of row i − 1, the reconstructed
pixel of P1 can be immediately obtained (de-scaling by 2Q).
In other words, the reconstructed pixels of reference pixels
and current pixel can be concurrently derived. Therefore, both
P1 and P2 are still available in parallel processing.
E. Decoding Flow Chart
As depicted in Fig. 10(a), the bitstream format consists of
two parts: Header and Encoded Symbol. The Header is set
to “0” for Center-range, “10” for Below-range , and “11” for
Above-range, respectively. The Encoded Symbol represents the
codeword of prediction residual for each pixel. The decoding
execution order is also the same as encoding execution order,
as shown in Fig. 9(b). According to this execution order, while
each current pixel, P, is under decoding, its reference pixels
are already available. With available reference pixels, the
parameters in Table I can be obtained. The decoding flow chart
is illustrated in Fig. 10(b). Based on Header information, if the
Encoded Symbol belongs to Center-range, the geometric-based
binary decoding is adopted. Otherwise, the content-adaptive
Golomb-Rice decoding is applied. Then, the reconstructed
pixel is further de-scaling by 2Q. Similarly, the rate control
also adjusts the Q by comparing with TCR and OCR at each
check-point. In lossless mode, the Q parameter is directly set
to 0.
For content-adaptive Golomb-Rice decoding, its Encoded
Symbol is composed of unary code and binary code. The
unary code can be fetched by consecutive bits of “1” until an
1284 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
TABLE IV
Performance Evaluation of Rate Control Mechanism
(a)
High- QP = 15 TCR = 2.0 TCR = 2.5 TCR = 3.0
Quality PSNR PSNR CR PSNR CR PSNR CR
(dB) Loss Loss Loss
Bluesky 48.04 0.01 2.52 0.23 2.55 5.17 2.99
Rush 47.73 0.01 2.41 0.28 2.50 5.56 3.54
Station2 47.54 0.01 2.14 2.78 2.50 9.08 3.00
Sunflower 47.85 0.01 2.58 0.11 2.61 2.49 3.00
Average 47.79 0.01 2.41 1.35 2.54 5.67 3.13
(b)
Medium- QP = 20 TCR = 2.0 TCR = 2.5 TCR = 3.0
Quailty PSNR PSNR CR PSNR CR PSNR CR
(dB) Loss Loss Loss
Bluesky 44.74 0.01 3.04 0.01 3.04 0.08 3.13
Rush 43.55 0.01 3.15 0.01 3.15 0.01 3.15
Station2 42.78 0.00 2.41 0.04 2.50 2.72 3.00
Sunflower 44.65 0.01 2.87 0.01 2.87 0.37 3.03
Average 43.93 0.01 2.80 0.02 2.89 0.98 3.02
(c)
Basic- QP = 25 TCR = 2.0 TCR = 2.5 TCR = 3.0
Quailty PSNR PSNR CR PSNR CR PSNR CR
(dB) Loss Loss Loss
Bluesky 42.25 0.01 3.15 0.01 3.15 0.28 3.28
Rush 42.18 0.01 3.54 0.01 3.54 0.01 3.54
Station2 40.40 0.01 2.69 0.01 2.69 0.38 3.00
Sunflower 42.86 0.01 2.96 0.01 2.96 0.10 3.06
Average 41.92 0.01 3.09 0.01 3.09 0.19 3.20
(a) QP = 15. (b) QP = 20. (c) QP = 25.
format in the next-generation, such as MPEG-1 for VCD and
MPEG-2 for DVD. As a result, the H.264 is applied to provide
the display source of LCD-TV. At first, the test sequence is
compressed by H.264 with QP of 15, 20, and 25. Three kinds
of QP represent the display source for high-quality, medium-
quality, and basic-quality, respectively. Then, the proposed
high-speed EC algorithm is performed at TCR of 2.0, 2.5,
and 3.0. For each QP, both CR and PSNR loss are separately
demonstrated in Table IV.
For TCR of 2.0, 50% of memory bandwidth and capacity
can be saved at least, and the PSNR loss is 0.01 dB on average
for all three kinds of visual quality. For TCR of 2.5 and
3.0, the PSNR loss is within 0.02 dB for medium-quality and
basic-quality. The high-quality preserves more high-frequency
content, and it is quite inefficient for compression. To reach
the pre-specified TCR, the high-frequency content is greatly
quantized. Hence, the visual quality is sacrificed. On the
contrary, for medium-quality and basic-quality, the high-
frequency is much less than that of high-quality. This is the
reason why the PSNR loss is minor in both medium-quality
and basic-quality. In summary, for high-quality, the TCR
should be within 2.5 to keep the PSNR loss as small as
possible. For medium-quality and basic-quality, the average
PSNR loss is less than 1 dB even at TCR of 3.0.
IV. Proposed VLSI Architecture of High-Speed
Embedded Compression Algorithm
The VLSI architecture of the proposed high-speed EC algo-
rithm is developed for entire codec. The encoder and decoder
contain the individual data path, line buffer, and separate data
scheduling strategy. Therefore, both encoding and decoding
tasks can be concurrently performed. The entire codec is
located between external memory and preceding processing
unit, such as image/video signal processing function. The
VLSI architecture is classified into two categories: coding
function and auxiliary function, as illustrated in Fig. 11. The
coding function is mainly responsible to accomplish the en-
coding/decoding task. The auxiliary function is for the control,
storage, and bitstream manipulation. The coding function
consists of context template, intensity processing, rate control,
pre-scalar/de-scalar, content-adaptive Golomb-Rice code, and
geometric-based binary code. The context template module
flexibly arranges the data path to prepare the relative reference
pixels and current pixel. Subsequently, the intensity processing
module identifies which entropy coding should be selected
and generates the prediction residual. Since only one entropy
coding is selected for each current pixel, both entropy coding
modules are interconnected in parallel with the individual
data flow. The rate control dynamically adjusts the Q, and
the pre-scalar/de-scalar applies it to make the CR gradually
approximate to TCR.
The auxiliary function comprises three major parts: control
unit, line buffer and bitstream generator/fetch. The line buffer
is applied to store the reference pixels of entire row. The
bitstream generator is responsible to pack the codeword,
derived from both entropy coding modules, into bitstream
for the transmission on dedicated bus width. The control
unit precisely schedules both coding function and auxiliary
function to accomplish complete encoding/decoding task.
A. Design Strategy Exploration
The proposed EC algorithm is designed not only for
computation-efficiency but also for parallel processing. Hence,
both parallelism and pipelining techniques are fully applied to
establish its VLSI architecture. In the following subsections,
both data scheduling and pipelining are analyzed under two-
level parallelism. Base on proposed high-speed EC algorithm,
two parallel processing techniques are developed: pixel-based
parallelism for encoder and segment-based parallelism for
decoder.
1) Data Scheduling and Pipelining Strategy for Encoder
with Pixel-Based Parallelism: Since the coding parameter of
previously-encoded prediction residual is no longer referenced
by following ones, no data dependence exists among consecu-
tive pixels. Therefore, consecutive pixels can be concurrently
encoded, and it is referred to as the pixel-based parallelism.
As depicted in Fig. 12(a), the encoding flow can be separately
performed by encoder core 1 and core 2. Consequently, with
pixel-based parallelism and pipelining technique, the encoding
capability is greatly improved as well as hardware utilization.
The encoding flow is divided into 4-stage pipelining to
provide sufficient processing speed. The pipelining strategy
and data scheduling for encoder are illustrated in Fig. 12(b).
The main consideration is described as follows. The context
template constructs a regular data path to schedule reference
pixels and current pixel. Besides, both current pixel and ref-
erence pixels are quantized by pre-scalar and de-quantized by
1286 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
Fig. 13. Pipelining strategy and data scheduling for decoder. (a) Pixel-
based parallelism in decoder. (b) 3-stage pipelining. (c) Data dependence of
bitstream.
3) Data Scheduling and Pipelining Strategy for Decoder
with Segment-Based Parallelism: To overcome the limitation,
the segment-based parallelism is proposed. As depicted in
Fig. 14(a), the entire image is divided into part 1 and part
2. The part 1 contains segment A and segment B. Both
segments C and D are allocated for part 2. The segment-based
parallelism indicates that each segment can be independently
decoded. It means the reference pixels across the segment
boundary are forbidden. As a result, the parallel processing
can be performed in the different segment, and is referred to
as the segment-based parallelism. The 4-segment partition is
adopted for two-level parallelism. The experiment result shows
that the degradation ratio on CR is as minor as 0.08% on
average at 4-segment partition. Consequently, the impact of
segment partition on CR can be reasonably ignored.
The data scheduling of segment-based parallelism is shown
in Fig. 14(b). The data scheduling for decoder core 1 is ado-
pted as the example for the following description. At cycle
1, the bitstream fetch reads partial bitstream from bitstream
buffer. At cycle 2, P1 is processed by entropy decoding, and
P4 is concurrently read by the bitstream fetch. Since the
data dependence among segments is removed, the P4 can be
independently located by bitstream fetch. At cycle 3, P2 is
availably located by the bitstream fetch, since the EB of P1 has
been yielded at cycle 2 and transmitted to bitstream fetch. The
remaining bitstream can be also decoded with the same data
scheduling until the bitstream data is completely finished. Con-
sequently, the segment-based parallelism effectively removes
Fig. 14. Illustration of segment-based parallelism. (a) Segment partition. (b)
Data scheduling for segment-based parallelism.
the data dependence of bitstream, and also exhibits the regular
and compact data flow. The 3-stage pipelining is completely
compatible with the segment-based parallelism without the
scarification of hardware utilization and throughput.
B. Proposed Hardware Architecture of Primary Function
1) Context Template and Pre-Scalar/De-Scalar: The con-
text template is responsible to prepare the reference pixels,
including N1, N2 and N3, for each current pixel. Both cur-
rent pixel and reference pixels are quantized by pre-scalar
for the following intensity processing module. Subsequently,
the current pixel is de-quantized by de-scalar to derive the
reconstructed pixel, and then it is stored into line buffer. Since
the Q is directly equal to the power of two, both pre-scalar
and de-scalar are realized with barrel shifter.
The architecture of context template is shown in Fig. 15.
The multiplexer among registers is applied to switch the data
path of each case in Fig. 4. Particularly, the line buffer is
capable of reading the reference pixels and storing current
pixel at the same time. To maintain reasonable area-efficiency,
the ping-pong mode is adopted as the design strategy for line
buffer instead of dual-port memory. Hence, the line buffer
is divided into four parts: SRAM−A, SRAM−B, SRAM−C,
and SRAM−D. Both SRAM−A and SRAM−B are allocated to
encoder core 1. Similarly, both SRAM−C and SRAM−D are
allocated to encoder core 2.
2) Rate Control: In rate control, the Q is exactly equal
to the power of two, and can be simply realized with logic
shifter. However, calculating OCR and TCR involves a divider
circuit, and it could seriously limit the processing speed.
Therefore, both OCR and TCR are directly translated to
the equivalent number of bit instead of the ratio. Since the
TCR is pre-specified, its equivalent number of bit is directly
represented as TEB−number. The EB−number indicates the
number of bit for each Encoded Symbol, and each one is
accumulated to calculate the equivalent number of bit for
1288 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
Fig. 19. Architecture of geometric-base binary decoder.
Fig. 20. Architecture of content-adaptive Golomb-Rice encoder.
Fig. 21. Architecture of content-adaptive Golomb-Rice decoder.
tor and Binary−part generator, respectively. Then, both results
are combined by the concatenation module to obtain the output
codeword. Moreover, the code length is also calculated by
the Effective−Bit−Generator module and passed to following
bitstream generator.
7) Content-Adaptive Golomb-Rice Decoder: The architec-
ture of content-adaptive Golomb-Rice decoder is depicted in
Fig. 21. The k parameter is decided by the k−determination
based on the texture−complexity. Once k parameter is deter-
mined, the Merge module extracts the partial bitstream from
the register of current symbol. In addition, it also generates
the EB to bitstream fetch unit. The ADD−A combines unary
part with binary part to yield the prediction residual. Then,
the prediction residual is passed to ADD−B and SUB−A to
generate the possible decoded pixel. Finally, the Merge module
determines which one should be adopted.
TABLE V
(a) Chip Specification and (b) Simulation Environment
(a)
Process technology TSMC 0.18 µm 1P6M
Voltage (core/PAD) 1.8-V/3.3-V
Die size 1.95 × 1.95 mm2
Core size 1.45 × 1.45 mm2
Logic gate count Total: 45.30 K
Encoder : 20.93 K
Decoder : 24.37 K
On-chip memory 3.8 kB
Maximum operation 200 MHz
Frequency
Parallel processing Two-level parallelism
Encoding/decoding QFHD (3840 × 2160) at 30 Hz
Capability Under 125 MHz
Power consumption Encoding: 58.46 mW at 125 MHz
Decoding: 74.59 mW at 125 MHz
Maximum throughput 6.4 Gbit/s at 200 MHz
(b)
Cell library Artisan cell library in typical case
Wire load model TSMC18−wl10
Clock uncertainty 0.5 ns
Input drive 3.53 µW/MHz
Output load 61fF
Power estimation tool PrimePower
Fig. 22. Layout view of proposed VLSI with two-level parallelism.
V. Experiment Result and Discussion
A. Chip Specification
The proposed high-speed EC codec chip is implemented in
TSMC 0.18-µm 1P6M CMOS technology with Artisan cell
library. The layout view is depicted in Fig. 22, and Table V
shows the chip specification and simulation environment,
respectively. The supply voltage for core and PAD is 1.8-V
and 3.3-V. The core size is 1.45 × 1.45 mm2. The entire codec
adopts two-level parallelism, and it is represented as core 1 and
core 2 for encoder and decoder. The line buffer with ping-
pong mode scheme is separately allocated for encoder and
decoder. The line buffers are realized with on-chip SRAM,
and its total capacity is 3.8 kB. The logic gate count occupies
45.30 K. The performance reaches QFHD at 30 Hz at operation
frequency of 125 MHz. The maximum throughput is as high
as 6.4 Gbit/s at 200 MHz. The power consumption is 54.46
mW at 125 MHz and 74.59 mW at 125 MHz for encoding and
decoding, respectively.
1290 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 20, NO. 10, OCTOBER 2010
TABLE VII
Illustration of Multi-Level Parallelism for Advanced HD
Display Specifications
Frame Rate of 120 Hz
Throughput Parallelism
(MB/s)
QHD 442 3
QFHD 995 5
count of the proposed VLSI architecture is quite compact in
comparison with these existing works.
In summary, the throughput is mainly dominated by two fac-
tors, processing speed and the capability of parallel processing.
Although more complex algorithm can provide better coding
efficiency, the processing speed and the parallel processing
are seriously limited. On the contrary, the proposed high-
speed EC algorithm adopts AGPM to construct the context-
modeling without context-table. Besides, both context-adaptive
Golomb-Rice code and geometric-based binary code just apply
minor order of context to greatly save computation complexity.
With pixel-based parallelism and segment-based parallelism,
the proposed VLSI architecture can be fully designed with
regular and efficient data scheduling. Moreover, the logic gate
count is quite compact.
C. Extended to Multi-Level Parallelism
In the near-future, both QHD and QFHD will become
the main stream in the display resolution of LCD-TV. To
improve the motion-picture quality, the DFR technique pushes
the frame rate from 60 Hz to 120 Hz. To support this trend,
the two-level parallelism can be further extended to multi-
level parallelism. According to the design strategy in Section
IV, both pixel-based parallelism and segment-based parallelism
can be flexibly adjusted to increase the depth of parallelism
of entire codec. For encoder with pixel-based parallelism, the
depth of parallelism is adjusted by increasing the number of
consecutive pixels for concurrent encoding. For decoder with
segment-based parallelism, increasing the number of segment
can raise the depth of parallelism. Table VII reveals that with
three-level parallelism and five-level parallelism, both QHD at
120 Hz and QFHD at 120 Hz can be fully-supported.
VI. Conclusion
In this paper, the high-speed EC algorithm was proposed.
Two coding modes were provided: lossless mode for quality-
oriented purpose and rate control mode for memory-efficient
purpose. Four primary techniques were incorporated: AGPM,
content-adaptive Golomb-Rice code, geometric-based binary
code, and rate control mechanism. The AGPM was applied
to construct context-modeling mechanism without context-
table. To achieve the compact coding flow, both content-
adaptive Golomb-Rice code and geometric-based binary code
were developed with minor order of context. The rate control
mechanism guarantees the saving ratio of memory bandwidth
and capacity. The computation-efficiency of the proposed
EC algorithm is 44% and 40% of FELICS and JPEG-
LS. Meanwhile, the competitive coding efficiency is also
achieved.
Based on pixel-based parallelism and segment-based par-
allelism, the encoding/decoding capability reaches QFHD at
30 Hz. The maximum throughput is up to 6.4 Gbit/s. The
codec chip is implemented in TSMC 0.18-µm 1P6M CMOS
technology with Artisan cell library, and the gate count
is 45.30 K. The experiment result shows that the proposed
VLSI architecture achieves superior parallelism-efficiency and
power-efficiency. Moreover, with multi-level parallelism, the
proposed VLSI architecture can be further applied in QHD at
120 Hz and QFHD at 120 Hz for DFR technique.
Acknowledgment
The authors would like to thank the National Chip Imple-
mentation Center for providing tool support in chip design.
They would also like to thank the Associate Editor and
reviewers for their valuable suggestions.
References
[1] Y. Ishii, “The world of liquid-crystal display tvs-past, present, and
future,” IEEE/OSA J. Display Technol., vol. 3, no. 4, pp. 351–360, Dec.
2007.
[2] C. T. Liu, “Revolution of the TFT LCD technology,” IEEE/OSA J.
Display Technol., vol. 3, no. 4, pp. 342–350, Dec. 2007.
[3] H.-G. Lee, “Strategic consideration for design of digital TV
system-in-chip products,” in Proc. IEEE Int. A-SSCC, Nov. 2007,
pp. 1–4.
[4] J. H. Souk and J. Lee, “Recent picture quality enhancement technology
based on human visual perception in LCD TVs,” IEEE/OSA J. Display
Technol., vol. 3, no. 4, pp. 371–376, Dec. 2007.
[5] Digital Compression and Coding of Continuous Tone Still Images:
Requirements and Guidelines, ITU Rec. T.81 ISO/IEC 10918-1, Sep.
1993.
[6] JPEG 2000 Part 1 Final Draft International Standard, ISO/IEC FDIS15
444-1, Dec. 2000.
[7] Lossless and Near-Lossless Coding of Continuous Still Images (JPEG-
LS), ISO/IEC JTC1/SC29 WG1 ITU-T SG8 (JPEG/JBIG), 1998.
[8] X. Wu and N. D. Memon, “Context-based, adaptive, lossless im-
age coding,” IEEE Trans. Commun., vol. 45, no. 4, pp. 437–444,
Apr. 1997.
[9] P. G. Howard and J. S. Vitter, “Fast and efficient lossless image
compression,” in Proc. IEEE Int. Conf. Data Compression, 1993,
pp. 501–510.
[10] A. Netravali and J. O. Limb, “Picture coding: A review,” Proc. IEEE,
vol. 68, no. 3, pp. 366–406, Mar. 1980.
[11] G. G. Langdon, Jr. and M. Manohar, “Centering of context-dependent
components of prediction error distributions,” in Proc. SPIE, vol. 2028.
Jul. 1993, pp. 26–31.
[12] M. Ferretti and M. Boffadossi, “A parallel pipelined implementation of
LOCO-I for JPEG-LS,” in Proc. IEEE Int. Conf. Pattern Recognit., 2004,
pp. 769–772.
[13] X. Chen, H. Jiang, X. Li, and Z. Wang, “A novel compression method
for wireless image sensor node,” in Proc. IEEE Int. Conf. A-SSCC, Nov.
2007, pp. 184–187.
[14] L. Xiaowen, X. Chen, X. Xie, G. Li, L. Zhang, C. Zhang, and Z. Wang,
“A low power, fully pipelined JPEG-LS encoder for lossless image
compression,” in Proc. IEEE Int. Conf. Multimedia Expo, 2007, pp.
1906–1909.
[15] M. Papadonikolakis, V. Pantazis, and A. P. Kakarountas, “Efficient high-
performance ASIC implementation of JPEG-LS encoder,” in Proc. Int.
Design Autom. Test Eur. Conf Exhibit., 2007, pp. 1–6.
[16] C.-C. Cheng, P.-C. Tseng, C.-T. Huang, and L.-G. Chen, “Multi-mode
embedded compression codec engine for power-aware video coding
system,” IEEE Trans. Circuits Syst. Video Technol., vol. 19, no. 2, pp.
141–150, Feb. 2009.
A Fast ML Sphere Decoder with Multi-Layer
Multi-Path Search
Pei-Yun Tsai
Department of Electrical Engineering
National Central University, Jhongli, Taiwan China, 32001.
Abstract-Spatial multiplexing is a key feature for upgrading
transmission rate in recent wireless communication systems.
However, the performance improvement comes at the cost of
decoding complexity. Sphere decoders have shown the distinction
to provide the maximum likelihood (ML) solution to this signal
detection problem with much lower complexity than the exhaus-
tive search. In this paper, the algorithm of a multi-layer multi-
path sphere decoder is proposed. The common information of
the same signal carried at different layers can be discovered.
Moreover, multiple nodes are visited simultaneously to speed up
the search process. Meanwhile, the completeness in the whole
search space is still retained to derive the ML solution without
conflicts among the different search paths. Simulation results
show that more than 50% decoding cycles can be reduced and
hence the average throughput can be increased.
1. INTRODUCTION
The use of multiple antennas at both transmit and receive
sides, known as the multiple-input multiple-output (MIMO)
techniques, has been shown to effectively boost capacity
and diversity in a scattering-rich multi-path environment,
which was regarded a pitfall for single-input single-output
(SISO) wireless communications in the past. In addition to
capacity gain and diversity gain, interference reduction can
also be achieved through beamforming by the antenna array.
Thus, multiple antenna configurations have been adopted in
several standards such as HSDPA, IEEE 802.11 n, and IEEE
802.16e. Among various MIMO techniques, spatial multi-
plexing provides obvious increase in spectrum efficiency,
fulfilling the requirenlents of high transmission rates in the
new wireless communication products, and hence attracts
much more attention.
For the spatial multiplexing scheme, exhaustive-search ML
detection is impractical due to its exponential growth in
complexity as the increase in the size of constellation and
the number of transmit antennas. Hence, the zero-forcing
(ZF) or vertical Bell Lab layered space-time architecture (V-
BLAST) algorithms were proposed to trade off between the
performance and complexity. On the other hand, the sphere
decoder (SD) has been introduced recently to obtain the
ML solution with reduced detection efforts. It translates an
exhaustive-search problem into a tree-search problem with
a sphere constraint. The average complexity is estimated
to be polynomial for a moderate number of antennas and
constellation points in the interested signal-to-noise ratio
(SNR) region [1].
The tree-search schemes of the sphere decoders can be
978-1-4244-2064-3/08/$25.00 ©2008 IEEE 119
classified as depth-first search [2], breadth-first search [3], and
best-first search [5]. The depth-first search SD traverses along
the branch away from the root node as far as possible before
backtracking, while the breadth-first search SD examines all
the neighboring nodes first before going down the next layer.
The best-first search SD chooses the node in the candidate
list with the least metric value as the next visiting node.
Furthermore, numerous techniques have been proposed to
further reduce the decoding cycles. The search can be applied
to the complex-valued constellation or the one after real-value
decomposition. However, the real-value decomposition of the
complex received signals doubles the depth of the tree, which
is detrimental to the throughput of the implementation visiting
one node per cycle. The Schnorr-Euchner (SE) enumeration
provides an ordering in the child nodes so that the target node
can be reached earlier. The tree pruning strategy eliminates
the node whose metric is not satisfied the sphere constraint
so that all its descendent nodes are expelled from the search
space. The antenna ordering moves the most reliable signal at
the top layer and hence the radius constraint can be decreased
quickly.
Moreover, the concept of two-layer enumeration has been
proposed in [4], which demonstrates the advantage in cycle
reduction by exploiting the signal diversity between two
layers. In [6], a lookahead search was introduced. Two nodes
are checked at one cycle and the search algorithm tends to
track the sub-tree stemming from the node with the less
metric value. With the help of the lookahead technique, the
sub-tree can be pruned earlier without computing the metrics
of all the nodes. In this paper, a novel multi-layer multi-path
search algorithm is proposed based on these two approaches.
The proposed SD can take advantage of the common symbol
information across different layers. Moreover, the metrics
along multiple paths are examined simultaneously while the
search algorithm handling multiple paths is capable of having
a full coverage of the whole search space without redundant
node visiting. Simulation results show that the ML solution
can be found and more than 50% decoding cycles can be
reduced.
This paper is organized as follows. Section II will describe
the system model and the depth-first sphere decoder. The
proposed multi-layer multi-path search SD will be introduced
in Section III. The simulation results are presented in Section
IV. Finally, Section V gives a brief conclusion.
Fig. 1. The concept of multi-layer multi-branch sphere decoder: (a) the
first visited nodes of the four paths and (b) the possible visited nodes of
four paths after horizontal leaps.
A. Iterative Sign Approach and ID EnLllneration
After QR decomposition, the diagonal elements in Rare
all real. Based on Eq. (4), the real-valued operation takes the
form of
A1-1
X= arg min L {(Re{zil - Ti iRe{xil )2
xEA i=O ' (8)
+ (lm{zil - Ti,i1m{Xi})2} ,
B. Multi-Layer Multi-Path Search
In the traditional depth-first SD, a horizontal leap at the
upper layer does not occur until the descendent nodes of the
old sub-tree satisfying the sphere constraint are all visited.
Consequently, if an improper branch is chosen at the upper
layer, a waste in decoding cycles is produced. In order to
save decoding cycles, a multi-layer multi-path search SD is
proposed. The PEDs along the four paths as indicated by the
gray arrows in Fig. 1 are calculated simultaneously. In the
subsequent search stepping into the next four layers, the sub-
trees from the bottom node with the smallest PED will be
visited first. Therefore, unlike the traditional depth-first SD,
the proposed SD can enjoy the freedom of changing the real
part and imaginary part of X2p+l and X2p simultaneously to
determine the tendency of the optimal solution.
The PEDs along the path k are computed by MCU k.
Denote the PED at the layer 4p + 2q + r of the path k as
Ti;~2q+r(x(2p+q)) with r,q E {a, I} and k == 0,1,2,3. The
PEDs can be calculated as
T(k) (x(2P+l))4p+3
==T1;t4 (x(2P+2)) + (Re{Z2p+l} - r2p+l,2p+lRe{X2p+l})2,
T(k) (x(2P+l))4p+2
==Ti;t3 (x(2P+l)) + (Im{Z2p+l} - r2p+l,2p+lIm{ X2p+l})2,
T(k) (x(2p))4p+l
==Ti;t2 (X(2p +1)) + (Re{Z2p} - r2p,2pRe{X2p} )2 ,
Tl:) (x(2p))
==Tl:~l (X(2p)) + (Im{Z2p} - r2p,2pIm{X2p})2.
Since the child nodes are ordered by the 1D enumeration,
if the PED Ti;~2q+r(x(2P+q)) is greater than the radius
constraint, its descendent nodes and right sibling nodes are
(9)
1) Initialization: direction D is set to be the same sign
as u in the last iteration. Spacing ~ is initialized to 2.
Index k is set to O.
2) Enumeration: compuate for the next candidate, ex (k +
1) == o(k) + D~.
3) Termination check: increase ~ by 2 and the direction
D is reversed. If 0 (k + 1) exceeds the boundary of
the constellation, repeat the second step. Otherwise,
increase k by 1 and repeat the second step until k ==
0-2.
Figure 1 illustrates the tree for multi-layer multi-path
search. At each layer, the child nodes are enumerated as a (k)
with increasing k from left to right. The odd-indexed layers
4p + 3 and 4p + 1 are related with the enumeration of real
part of X2p+l and X2p, while the imaginary part of X2p+l
and X2p are processed at the even-indexed layers 4p + 2 and
4p. 1D enumeration for both the real part and the imaginary
part is only required before the search process steps into the
odd-indexed layers.
Path 3
(b)
(a)
---------~--::::..~_ Layer 4p+3
----------------------
" )(}~UJYer4P+2
/\/1\... Layer 4p+ 1
r~ Layer4p
"-.---'
Path 0 Path 2
Path 1
AI
2:
Path 0 Path 2
Path 1
where Zi == Yi - L~~~l ri,jXj. Without loss of generality,
assume ri,i is positive and there are 22C points in the con-
stellation. Then, the nearest lattice point Re{ Xi} and 1m{Xi}
can be located by iterative sign approach which simply uses
additions/subtractions instead of a divider [6] and checks the
sign bit of the results for 0 times as the steps described below.
1) Initialization: Re{ Zi} or Im{Zi} is assigned to u. Index
k is set to O. 0(0) == O.
2) Sign bit check: check the sign bit of u. If u >== 0,
then u == u - 2C-k-1ri,i and 0(0) == 0(0) + 2C- k- 1.
Otherwise, u == U + 2C-k-1ri,i and 0(0) == 0(0) -
2C - k - 1 .
3) Termination check: index k is increased by 1. Repeat
the step 2 until k == 0 - 1.
The 1D enumeration can be obtained by the method similar
to the one in [6].
unit (PCU) decides the order among these paths for further
travelling onward. Finally, a stack processing unit (SPU)
must control horizontal leaps between sub-trees and store the
information of unvisited paths for backtracking. The detailed
algorithm will be illustrated in the following.
121
35302515 20
SNR (dB)
-+- SE enumeration SO, 64-QAM
-Ar- Proposed SO, 64-QAM
-ollIEI - SE enumeration SO, 16-QAM
- e - Proposed SO, 16-QAM
105
10° L--_--L-__--'-------_--'-__-'--_-------'-__--'--_----'
o
o
o 0
o
x4(0) x4(l) x4(2) x4(3) x4 (4) x4 (5) x4 (6)
x3(0) Ix4 (0) -:® 1 (!),. 2" 2
, ,.,1 "
I I. " I,
x3 (1) IX4 (0) • .t",
o
-----. Proposed SO
.......... ..... Complex SO with 20
SE Enumeration
Fig. 4. The trajectory during search. Fig. 5. The comparison of decoding cycles of SE enumeration SO and theproposed SO.
tion involves PED magnitude comparison among 9 subsets.
However, in the proposed SD, only sorting of at most four
values is required.
Fig. 4 shows the node-visiting trajectories of two algo-
rithms during search. The points along the horizontal axis are
the results of 2D SE enumeration of complex symbol X4 (n),
where n denotes the enumeration index. For each x4(n),
the corresponding 2D SE enumeration sequence of X3 (j) is
also drawn along the vertical axis, indicated by x3(j)l x4(n)'
Hence, a two-dimensional plane is formed. Assume that
the target solution is located at (x4(4),X3(O)l x 4(4))' The
conventional depth-first complex SD searches the target node
from (X4(O),X3(O)l x 4(O)) downward until the PED of the
node X3 (3) IX 4(O) greater than the radius constraint. Afterward
it switches to the second vertical line and so on. The target
node (x4(4), x3(O)l x4(4)) will be visited at the 13th test
as indicated by the dashed arrows. On the other hand, the
proposed SD can execute a horizontal leap at the high layer.
Hence, it has dispersed node-visiting trajectories. The nodes
indicated by "1" inside a circle refer to the one being first
visited by the four MCVs. The target node will be reached at
the second test extended from the node (x4(1), x3(O)l x 4(1)),
which has the least PED among the first four.
Fig. 5 illustrates the saving in decoding cycles compared
with the depth-first complex sphere decoder with 2D SE
enumeration. More than 50% decoding cycles can be saved
with the proposed SD in the most SNR region either under
64-QAM or 16-QAM and the 4 x 4 MIMO configuration to
obtain the same ML solution.
pruned and thus can find the optimal ML solution. Simulation
results demonstrate its effectiveness in cycle reduction. The
proposed SD can save more than 50% decoding complexity.
REFERENCES
[1] B. Hassibi, and H. Vikalo, "On the sphere-decoding algorithm. I.
Expected complexity," IEEE Trans. Signal Process., vol. 53, no.8, pp.
2806-2818, Aug. 2005.
[2J E. Agrell, T. Eriksson, A. Vardy, and K. Zeger, "Cloest point search in
lattices," IEEE Trans. Inf. Theory, vol. 48, no. 8, pp. 2201-2214, Aug.
2002.
[3J K. Wong, C. Tsui, R. S. Cheng, and W. Mow, "A VLSI architecture
of a K-best lattice decoding algorithm for MIMO channels," in Proc.
of IEEE ISCAS'02, vol. 3, May 2002, pp. 273-276.
[4] W. Xu, Y. Wang, Z. Zhou and J. Wang, "A fast exact ML sphere de-
coder with efficient two-layer enumeration," in Proc. of IEEE VTC'04,
vol. 2, Sep. 2004, pp.1309 - 1313.
[5] O. Pham, K. R. Pattipati, P.K. Willett, and J. Luo, "An improved
complex sphere decoder for V-BLAST systems," IEEE Signal Process.
Lett., vol. 11, no. 9, Sep. 2004, pp. 748-751.
[6J S. H. Kang, and I. C. Park, "Fast and area-efficient sphere decoding
using look-ahead search," in Proc. of IEEE VTC'07, Apr. 2007, pp.
2384-2388.
[7J A. Burg, M. Borgmann, M. Wenk, M. Zellweger, W. Fichtner, and H.
Bolcskei,"VLSI implemenation of MIMO detection using the sphere
decoding algorithm," IEEE J. Solid-State Circuits, vol. 40, pp. 1566
-1577, luI. 2005.
V. CONCLUSION
A generalized multi-layer multi-path sphere decoder is pro-
posed in this paper, which uses multiple metric computation
units simultaneously to evaluate the symbol diversity across
different layers. Moreover, the algorithm with multiple search
paths is shown to have full coverage of the nodes that are not
123
  
detection problem can also be reformulated as the real 
signal detection problem by 
{ }
{ }
{ } { }
{ } { }
{ }
{ }
{ }
{ }
kkk
k
k
k
k
kk
kk
k
k
k
nxH
y
n
n
x
x
HH
HH
y
y
~~~
~
Im
Re
Im
Re
ReIm
ImRe
Im
Re
+=
⎥⎦
⎤⎢⎣
⎡
+⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡ −
=⎥⎦
⎤⎢⎣
⎡
=
 
where  Re{．} and Im{．} denote the real part and the 
imaginary part of their arguments. The size of the real 
channel matrix kH
~ becomes 2N × 2M. 
The optimal solution to the MIMO detection is to find 
kxˆ  that can minimize the Euclidean distance between 
received signal vector ky~  and kk xH ~
~  By using QR 
decomposition of the channel matrix kH
~ , i.e. kH
~  = kk RQ
~~ , 
with a unitary matrix kQ
~  and an upper-triangular matrix 
kR
~ , the optimal solution becomes 
2
~
2
~
~~~~minarg~~~~minargˆ kkk
H
kkkkkk
kk
xRyQxRQyx
xx
−=−=
Ω∈Ω∈  
where Ω is a set of the search space. Hence, in order to 
detect kxˆ , we need to obtain kk yQ ~
~ H  and kR
~ , respectively. 
III. QR DECOMPOSITION BY GIVENS ROTATION 
A. Givens Rotation Algorithms 
The Givens rotation algorithms for complex numbers 
and real numbers are different. Consider a 2×2 complex 
matrix V represented in polar form, 
⎥⎥⎦
⎤
⎢⎢⎣
⎡
=
42
31
42
31
θθ
θθ
jj
jj
evev
evevV  
where vi denotes the magnitude of a complex number, and 
θi is its phase. The rotation matrix Gc can be given by 
 
⎥⎥⎦
⎤
⎢⎢⎣
⎡
−
=
zy
zy
j
x
j
x
j
x
j
x
c ee
ee
θθ
θθ
θθ
θθ
cossin
sincosG
  (1) 
where .,),/(tan 2112
1 θθθθθ −=−== − zyx vv  Hence 
⎥⎦
⎤⎢⎣
⎡
=⎥⎦
⎤⎢⎣
⎡
⎥⎥⎦
⎤
⎢⎢⎣
⎡
−
=
6
5
42
31
4
31
42
31
ˆ0
ˆˆ
cossin
sincos
θ
θ
θθ
θθ
θθ
θθ
θθ
θθ
j
j
jj
jj
j
x
j
x
j
x
j
x
c ev
evv
evev
evev
ee
ee
zy
zy
VG
 
          (2)
 
If only real numbers are allowed in the diagonal entries, 
GcV should be left multiplied by an additional rotation 
matrix C, which is given by
 
⎥⎦
⎤⎢⎣
⎡
=
− 60
01
θje
C
.
 
On the other hand, for a real 2×2 matrix U, define 
⎥⎦
⎤⎢⎣
⎡
−
=
θθ
θθ
cossin
sincos
rG
.
 
The rotation matrix can zero the lower left element of U as 
indicated in the following, 
⎥⎦
⎤⎢⎣
⎡
=⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡
−
=
4
31
42
31
ˆ0
ˆˆ
cossin
sincos
u
uu
uu
uu
r θθ
θθ
UG
 
with θ=tan-1(u2/u1).
 B. Proposed QR Decomposition Scheme 
For RVD MIMO signal detection approaches, such as 
RVD K-best sphere decoder [6], the real channel matrix 
kH
~ of the size 2N × 2N must be triangularized. Intuitively, 
the 2N × 2N channel matrix kH
~ is decomposed directly to 
obtain the upper triangular matrix as indicated in the top of 
Fig. 2. However, we propose an alternative approach as 
indicated in the bottom of Fig. 2. The complex matrix Hk is 
first converted into an upper triangular matrix kHˆ  by the 
complex Givens rotation algorithm. Note that its off-
diagonal terms are still complex numbers. Thereafter kHˆ  is 
expanded to a real matrix 
kS
~  of the size 2N × 2N. Finally, 
the non-zero elements in the lower-left-side of 
kS
~  are 
eliminated by the real Givens rotation algorithm.  
The complexity of these two schemes can be 
compared in terms of CORDIC operations. For a real 8×8 
matrix kH
~ , it involves 168 CORDIC operations to 
transform kH
~ to kR
~
 directly. Note that in the complex 
Givens rotation algorithm, three CORDIC operations are 
required in the generation mode for yx θθ ,  and zθ  . Four 
CORDIC operations are used in the normal rotation mode, 
and thus a total of 64 CORDIC operations are needed to 
derive kHˆ  from Hk. It takes additional 30 CORDIC 
operations to eliminate the non-zero elements in the lower-
left-hand side of 
kS
~ . To sum up, we can utilize 94 CORDIC 
operations to obtain kR
~ . It can be seen that the proposed 
scheme can save about 44% in complexity. Actually, for N×
N channel matrix, the direct triangularization method 
requires 3/)28( 3 NN −  CORDIC operations, and the proposed 
scheme needs 2/)3( 3 NN −  CORDIC operations. 
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎢
⎣
⎡
 
Figure 2. Two different schemes for QR decomposition. 
In MIMO-OFDM systems under stationary channels, 
for example, in 4×4 IEEE 802.11n systems, the channel 
matrix at the k-th subcarrier remains unchanged during the 
1493
  
V. IMPLEMENTATION RESULTS AND COMPARISONS 
  
Figure 6 BER performance of proposed QRD scheme combined with 
sphere decoding. 
TABLE I. COMPARISON OF QR DECOMPOSITION IMPLEMENTATIONS. 
 [1] [7] [5] This work 
Technology 0.18μm 0.18μm 0.13μm 0.18μm 
QRD Algorithm MGS MGS GR GR 
Processing 
Latency (cycles) 18 44 40 4 
Max Frequency 
(MHz) 277 * - 270 * 90.09 ** 
Size of QR 
Decomposition 
4×4 
real 
4×4 
real 
4×4 
complex 
4×4 
complex 
8×8 
real 
Gate Count 72 K 51 K 36 K 111  K 152 K 
N. H. E. 0.29 - 0.19 0.28 0.21 
*Synthesis result    **Layout result 
13.0
Technology
count Gate
Throughput QRD(N.H.E.) Efficiency Hardware Normalized ×=
 
We have implemented the proposed design in 0.18 μm 
CMOS technology. To achieve high-speed operation, three 
pipeline stages are inserted in each CORDIC module with 9 
micro rotation stages. The gate count is about 152k, and the 
operating frequency can be driven up to 90.09 MHz 
according to the post-layout simulation results. It takes 44 
ns to complete the QR-decomposition of an 8 × 8 RVD 
channel matrix or a 4×4 complex channel matrix and 11 ns 
to complete vector rotation of kk yQ ~
~ H , which is sufficient for 
MIMO detection up to 2.16 Gbps. Fig. 6 shows the fixed-
point performance of proposed QRD architecture together 
with the chip layout. Its core area is about 1.67× 1.63 mm2. 
It is clear that our design has little implemenation loss and 
provides good QR decomposition and vector rotation 
results to the subsequent either K-best or depth-first sphere 
decoders to achieve ML detection performance. 
In Table I, we compare the performance of our proposed 
architecture and some previous works in terms of 
normalized hardware efficiency that considers the 
technology, throughputs and gate counts. Note that we 
provide the post-layout simulation results for the maximum 
operating frequency. From the table, it can be seen that 
with those improvements, our work has fewer processing 
cycles compared to others. In addition, it also has better 
hardware efficiency than [5] to handle the same 4 × 4 
complex QR decomposition. Although similar hardware 
efficiency is shown in [1], it only supports 4×4 real QR 
decomposition, which definitely consumes less hardware 
complexity. Hence, the proposed architecture is much more 
suitable for high-throughput MIMO detection in OFDM 
systems. 
VI. CONCLUSION 
In this paper, we have implemented a high-throughput 
QR decomposition module for 4×4 MIMO-OFDM systems. 
We combine both the complex Givens rotation algorithm 
and the real Givens rotation algorithm to improve the 
required CORDIC operations. Moreover, we eliminate the 
delay buffers for skewed input sequences in the 
conventional complex Givens rotations block and improve 
hardware utilization in the real givens rotation by the time-
sharing technique. We have shown that the proposed 
architecture has good hardware efficiency to achieve the 
more complicated QR decomposition of the 8×8 real-value 
decomposed channel matrix. According to the 
implementation results, the operating frequency can 
achieve 90.09MHz in 0.18μm CMOS technology. It can 
complete the QR decomposition every 44 ns as well as 
kk yQ
~~ H  calculation every 11 ns and support MIMO detection 
up to 2.16 Gbps. 
 
ACKNOWLEDGEMENT 
 The authors greatly appreciate the chip implementation 
center (CIC) of National Science Council, Taiwan, R. O. C. 
for chip implementation. 
REFERENCES 
[1] C. K. Singh, S. H. Prasad, and P. T. Balsara, “VLSI 
Architecture for Matrix Inversion using Modified Gram-
Schmidt based QR Decomposition,” in Proc. Int. Conf. VLSI 
Design, Jan. 2007, pp. 836-841. 
[2] K.-L. Chung, W.-M. Yan, “The Complex Householder 
Transform”, IEEE Transactions on Signal Processing, vol. 45, 
no. 9, pp. 2374-2376, Sep. 1997.  
[3] A. Maltsev, V. Pestretsov, R. Maslennikov, and A. Khoryaev, 
“Triangular systolic array with reduced latency for QR-
decomposition of complex matrices,” in Proc. Int.  Symp. 
Circuits and Systems, May 2006, pp. 385-388. 
[4] Y. T. Hwang and  W. D. Chen, “A Low Complexity Complex 
QR Factorization Design for Signal Detection in MIMO 
OFDM systems”, in Proc. Int.  Symp. Circuits and Systems, 
May 2008, pp. 932-935. 
[5] D. Patel, M. Shabany, and P. Glenn Gulak, “A Low-
Complexity High-Speed QR Decomposition Implementation 
for MIMO Receivers”, in Proc. Int.  Symp. Circuits and 
Systems, May 2009,  pp.1409-1412. 
[6] M. Shabany and P. G. Gulak, “A 0.13mm CMOS 655 Mbps 
4x4 64-QAM K-best MIMO detector”, IEEE Int. Solid-State 
Circuits Conf. Dig. Tech. Papers, Feb. 2009, pp. 256-257.  
[7] K. H. Lin, C.H. Lin, C. H. Chang, C. L. Huang, and F. C. 
Chen, “Iterative QR Decomposition Architecture Using the 
Modified Gram-Schmidt Algorithm”, in Proc. Int.  Symp. 
Circuits and Systems, May 2009, pp.1409- 1412. 
 
Complex  
Stage 
Real Stages 
Buffer 
1495
2 EURASIP Journal on Advances in Signal Processing
diﬀerent from previous works, we try to analyze the finite
precision eﬀect in FFT processors and aim to oﬀer an FFT
IP generator that has the capability of automatic word-
length optimization to achieve hardware eﬃciency. The IP
generator can generate the hardware description language
of an FFT processor according to the constraints set by
users and therefore speed up the process for implementing
a new OFDM transceiver. Its features can be summarized as
follows.
(i) Parallel processing and multiple channels are taken
into consideration, either to increase throughput or
to support MIMO configurations.
(ii) The word lengths are optimized, which can be shown
to provide more eﬃcient hardware design under the
constraint of SQNR values than some conventional
works [15, 16].
(iii) Insertion of pipeline registers mainly depends on the
requirement of operating frequency to ensure the
necessity of flip-flop instantiation.
From the experimental results, we can see that these
improvements are eﬀective to generate FFT IPs that strike a
good balance between complexity and performance.
The rest of the paper is organized as follows. In Section 2,
the generic FFT architecture adopted by the proposed FFT
IP generator is illustrated. In Section 3, we discuss the finite
precision eﬀect in FFT operation. The work flow of the IP
generator and the word-length optimization procedure are
delineated in Section 4. Experimental results and compar-
isons are shown in Section 5. Finally, Section 6 gives a brief
conclusion.
2. Architecture of FFT Processors with MIMO
Configuration and Parallel Processing
In Table 1, we have listed some essential parameters in several
recent OFDM standards/drafts. Note that in UWB using
MB-OFDM modulation scheme, we show its one-channel
sampling rate. It is clear that the needed FFT processor
must support variable sizes as well as parallel processing for
either high throughput or multiple channels. In addition,
the FFT sizes mainly range from 64 points to 8192 points,
and the operating frequency covers from tens to hundreds
of mega Hz. To facilitate automatic generation of the FFT
processors fulfilling the above requirements, we resort to
exploit the mapping of its recursive nature to the pipelined
architecture. However, to accomplish parallel processing
with the high-radix algorithm, we proposed to combine two
well-known pipelined architectures, namely, the single-path
delay feedback (SDF) architecture and the multipath delay
commutator (MDC) architecture.
Figure 1 shows our adopted architecture that is able to
support the parallelism degree of two or four by utilizing
the property of the multipath delay commutator architecture
in parallel processing. If the parallelism degree of p is
desired, where p = 2 or 4, a radix-p MDC stage is
first employed. Thereafter, for the p parallel paths, we
cascade p-channel N/p-point FFT processors implemented
Radix-2
butterfly
PE6
C
om
m
u
ta
to
r
MDC SDF
2-channel
N/2
N/2-point FFT
(a)
Radix-4
butterfly
PE4
C
om
m
u
ta
to
r
4-channel
MDC SDF
3N/4
2N/4
N/4
N/4-point FFT
(b)
Figure 1: (a) Architecture of an FFT processor with parallelism
degree of two. (b) Architecture of an FFT processor with parallelism
degree of four.
by the radix-2/22/23 single-path delay feedback architecture.
If parallel processing to enhance the throughput is not
necessary, the generated FFT processor is reduced to the
conventional SDF architecture.
Table 2 compares the hardware complexity of the pro-
posed architecture and several conventional works with par-
allelism [3, 18–24]. However, those works may be designed
for specific applications such as UWB and may have special
optimization at certain stages. Here, we simply consider their
extensions to an N-point FFT processor. Note that hardware
complexity and architecture flexibility are essential concerns.
In our adopted architecture of parallelism degree of two,
one complex multiplier is required in the first radix-2 MDC
processing element and 2(log8(N/2)−1) complex multipliers
are used in the remaining two sets of radix-23 N/2-point
SDF architecture. Similarly, if the parallelism degree is four,
3 + 4(log8(N/4) − 1) complex multipliers are needed in our
architecture instead of 3(log4N − 1) complex multipliers in
the conventional radix-4 MDC architecture. Although the
higher radix-24 architecture [20, 23] can eﬀectively reduce
the number of complex multipliers, the constant multipliers
increase. Special scheduling for some specific FFT size can
help to decrease the complexity of the constant multipliers
[19]. Nevertheless it is not easily provided in an IP generator
oﬀering diverse user-specific parameters. Also the folding
scheme (SDF-kR) is not appropriate because higher and
4 EURASIP Journal on Advances in Signal Processing
M
U
X
M
U
X
M
U
X
M
U
X
M
U
X
M
U
X
M
U
X
M
U
X
M
U
X
M
U
X
Delay buﬀer Delay buﬀer Delay buﬀer
1
− j
− j
− j
− j
PE1 PE2 PE3W18
W18
W38
W38
−
−
−
−
−
+
+
+
+
+
+
+
+
+
+
1
PE4 PE5
PE6
S & A
− −
−
−−
+
+
+
+
−
−
Figure 2: Block diagram of basic arithmetic processing elements.
2048 1024 512
PE1 PE2 PE3
PE1 PE2 PE3
PE1 PE2 PE3
PE1 PE2 PE3
MUL1
M
U
X
1
Stage 1 Stage 2 Stage 3
Stage 4 Stage 5 Stage 6
Stage 7 Stage 8 Stage 9
Stage 10 Stage 11 Stage 12
ROM
ROM
ROM
256 128 64
MUL2
MUL3
M
U
X
3
M
U
X
2
32 16 8
M
U
X
4
4 2 1
Figure 3: Architecture of the generated SISO variable-length radix-23 FFT processor.
PE1 will be programmed to use only one half of its original
size, which can be done by simply using the arithmetic shift
of the counter output to the left by 1 bit without changing
the memory array. The gray vertical lines along the data path
denote the possible pipeline-register insertion positions. If
the required operating frequency is not high, then according
to the information in the timing library, only parts of these
pipeline registers are instantiated. On the contrary, all of
them will exist if the clock frequency needs to be raised to
over 100MHz.
As to automatic generation of multichannel FFT IP, it
basically can be regarded as constructing a two-dimensional
PE array. The number of columns in the PE array relates
to the number of stages. On the other hand, the number
6 EURASIP Journal on Advances in Signal Processing
x̂s(n) as well as x̂s(m), where xs(n) is the nth signal at the
sth stage, the notation (·̂) indicates the quantized version
of the signal, and m = n + N/2s. The output after complex
addition/subtraction is given by
x̂s+1(n) = x̂r,s+1(n) + jx̂i,s+1(n)
= (xr,s+1(n) + δr,s+1(n)) + j(xi,s+1(n) + δi,s+1(n))
= (xr,s(n) + xr,s(m) + δr,s(n) + δr,s(m))
+ j
(
xi,s(n) + xi,s(m) + δi,s(n) + δi,s(m)
)
,
x̂s+1(m) =
(
xr,s(n)− xr,s(m) + δr,s(n)− δr,s(m)
)
+ j
(
xi,s(n)− xi,s(m) + δi,s(n)− δi,s(m)
)
,
(1)
where xr,s(n) and xi,s(n) denote the real part and imaginary
part of xs(n) and δr,s(n) and δi,s(n) represent the real part
and the imaginary part of the quantization error, which may
have nonzero mean. Assume the mean square error at the sth
PE stage due to δr,s(n) and δi,s(n) as σ2PE,s. Note that one half
of the signals at the (s + 1)th stage is computed by addition
while the other half is computed by subtraction. Therefore,
the mean of the quantization error (x̂s+1(n) − xs+1(n)) with
n = 0, 1, . . . ,N − 1 at stage (s + 1) is given by
μPE,s+1 = E
{
δr,s(n)
}
+ jE
{
δi,s(n)
} = μPE,s. (2)
The mean squared quantization error after addition and
subtraction can be calculated respectively as
E
{∣∣[x̂r,s+1(n)− xr,s+1(n)] + j[x̂i,s+1(n)− xi,s+1(n)]∣∣2
}
= E
{∣∣δr,s(n) + δr,s(m)∣∣2
}
+ E
{∣∣δi,s(n) + δi,s(m)∣∣2
}
,
E
{∣∣[x̂r,s+1(m)− xr,s+1(m)] + j[x̂i,s+1(m)− xi,s+1(m)]∣∣2
}
= E
{∣∣δr,s(n)− δr,s(m)∣∣2
}
+ E
{∣∣δi,s(n)− δi,s(m)∣∣2
}
.
(3)
With the assumption of uncorrelated quantization errors, the
mean squared error at stage (s + 1) becomes
σ2PE,s+1 = 2σ2PE,s. (4)
Details are shown in Appendix A.
3.2. Quantization Error after Complex Multiplication.
Assume that Wr,p(m) and Wi,p(m) indicate the real part
and the imaginary part of the mth twiddle factor at the
pth complex multiplication block. The nth quantized signal
ŷp(n) after the pth complex multiplication takes the form of
ŷp(n) = ŷr,p(n) + j ŷi,p(n)
= [(xr,s(n) + δr,s(n)) + j(xi,s(n) + δi,s(n))]
·
[(
Wr,p(m) + r,p(m)
)
+ j
(
Wi,p(m) + i,p(m)
)]
≈
[(
xr,s(n)Wr,p(m)− xi,s(n)Wi,p(m)
)
+
(
Wr,p(m)δr,s(n)−Wi,p(m)δi,s(n)
)
+
(
xr,s(n)r,p(m)− xi,s(n)i,p(m)
)]
+ j
[(
xr,s(n)Wi,p(m) + xi,s(n)Wr,p(m)
)
+
(
Wi,p(m)δr,s(n) +Wr,p(m)δi,s(n)
)
+
(
xr,s(n)i,p(m) + xi,s(n)r,p(m)
)]
,
(5)
where r,p(m) and i,p(m) denote the real-part and the
imaginary-part quantization errors of the twiddle factor.
Since the twiddle factors can be predetermined by rounding
operation, they can be assumed to have zero mean. The
statistics of quantization errors after complex multiplication
can be derived as
μCM,p =
(
E
{
Wr,p(m)
}
E
{
δr,s(n)
}− E
{
Wi,p(m)
}
E
{
δi,s(n)
})
+ j
(
E
{
Wi,p(m)
}
E
{
δr,s(n)
}
+E
{
Wr,p(m)
}
E
{
δi,s(n)
})
,
σ2CM,p ≈ E
{∣∣∣[Wr,p(m)δr,s(n)−Wi,p(m)δi,s(n)
+xr,s(n)r,p(m)− xi,s(n)i,p(m)
]∣∣∣2
}
+ E
{∣∣∣[Wi,p(m)δr,s(n) +Wr,p(m)δi,s(n)
+xr,s(n)i,p(m) + xi,s(n)r,p(m)
]∣∣∣2
}
.
(6)
Similarly, by applying the assumption of uncorrelated errors
of δr,s(n), δi,s(n)r,p(m), and i,p(m), and mutually indepen-
dent random variables of the data paths and twiddle factors,
the mean squared error becomes
σ2CM,p ≈ E
{∣∣∣Wr,p(m)δr,s(n)
∣∣∣2
}
+ E
{∣∣∣Wi,p(m)δi,s(n)
∣∣∣2
}
+ E
{∣∣∣xr,s(n)r,p(m)
∣∣∣2
}
+ E
{∣∣∣xi,s(n)i,p(m)
∣∣∣2
}
− 2E
{∣∣∣Wr,p(m)Wi,p(m)δr,s(n)δi,s(n)
∣∣∣}
+ E
{∣∣∣Wi,p(m)δr,s(n)
∣∣∣2
}
+ E
{∣∣∣Wr,p(m)δi,s(n)
∣∣∣2
}
8 EURASIP Journal on Advances in Signal Processing
Unlike in [25], we introduce an extra term to account for the
possible nonzero mean after truncation. In the following, we
can see its influence on the accuracy of the analytic mean
square quantization errors.
3.4. Discussion on Word-Length Optimization. According to
the previous analyses for the finite precision eﬀect in an FFT
processor, some observations are summarized below.
(i) In the radix-23 single-path delay feedback architec-
ture, the average signal energy is increased by 2
according to Parseval’s theorem (see Appendix B),
while the mean squared quantization error also
doubles after butterfly operation as given by (4).
Define a signal-to-quantization error ratio (SQNR)
as
SQNR = 10 · log10
E
{
|xs(n)|2
}
σ2PE,s
. (16)
Hence, if the signal is not truncated after butterfly
operation, the SQNR remains the same.
(ii) The SQNR decreases after complex multiplication
because of the finite precision of twiddle factors.
The quantization errors in twiddle factors are further
scaled by the average energy of the signal to be
multiplied as indicated in (9). Consequently, the
word-length settings of twiddle factors and data paths
should be decided individually. Moreover, (9), also
reveals the reason that a shorter word-length can
always be assigned for twiddle factors than the data
path in an FFT processor since 2s/N  1.
(iii) The mean squared quantization errors increase
monotonically from the first stage to the last stage.
For those stages at which quantization errors accu-
mulate and severely pollute the least significant bits
(LSBs) of finite-precision signals, proper truncation
introduces only negligible degradation compared to
σ2PE,s as in (15) for d
2  σ2PE,s.
To verify the previous analysis, the analytic results (12)
and simulated results are compared in Figure 7. The hori-
zontal axis represents the word-length bm while the vertical
axis denotes the MSE. Twiddle factor multiplications for 64-
point and 512-point FFT operations are both evaluated. In
both cases, the twiddle factors are quantized to 10 bits in their
fractional part. The fractional part of the input data-path
signal before multiplication is represented by 11 bits and 12
bits in 64-point and 512-point FFT, respectively. Accordingly,
without truncation, the fractional parts become 21 and 22
bits. From the figure, we can see that the analytic results
approach the simulated results. Besides, the proper word-
length bm can be selected around the knee point close to the
error floor, which implies that only slight degradation occurs.
In Figure 8, the analytic results by using (4), (9), (12),
and (15) and the simulated results of the mean squared
quantization errors at each stage for 512-point FFT are
compared. In addition, we also provide the curve of the
analytic results by [25]. The eﬀect of W18 and W
3
8 in PE2
t
D
d
Ẑr,p(n) yr,p(n)
l = −1 l = 0
id
α
E{ ŷr,p(n)}
6ν
Figure 6: Quantization error distribution.
8 10 12 14 16 18
10−4
10−5
10−6
10−7
10−8
Fractional part word-length after truncation
M
SE
Analytic results (withWm64)
Simulated results (withWm64)
Analytic results (withWm512)
Simulated results (withWm512)
Figure 7: Analytic and simulated quantization mean squared error
after truncation.
is ignored temporarily. The word lengths of the output
at each stage after truncation are also indicated. It can
be seen that if there is no truncation after the PE stages,
the slope of the segment is log(2)/stage. If a proper word
length around the knee point is chosen after complex
multiplication, a nonzero slope of the segment appears but is
still less than log(2)/stage. On the other hand, if truncation
is performed after complex addition/subtraction, the slope
becomes steep. This figure demonstrates that our analytic
result that considers the bias eﬀect after truncation and uses
Gaussian distribution approximating the quantization error
10 EURASIP Journal on Advances in Signal Processing
Table 3: One example of the proposed fractional-part word-length search procedure.
2
1
Analytic
SQNR
Simulated
SQNRTwiddleStage 8Stage 7Stage 6CMul 2Stage 5Stage 4Stage 3CMul 1Stage 2
56.3556.72912131313131313131313
56.2256.53912121313131313131313
55.9856.23912121213131313131313
55.1055.48912121212121313131313
54.4854.64912121212121213131313
54.7255.04911121212121313131313
54.3854.50911111212121313131313
46.5147.061111111111111111111111
52.5353.071212121212121212121212
58.5559.081313131313131313131313
58.5159.031213131313131313131313
58.3858.911113131313131313131313
58.0358.521013131313131313131313
56.4956.86913131313131313131313
53.4353.58813131313131313131313
Stage 1Searchphase
smaller word length in processing elements, the less com-
plexity the complex adder/subtractor and the delay buﬀer. If
a smaller word length is assigned to twiddle factors, the size
of ROM tables can be scaled down linearly and the size of the
complex multiplier can also be reduced, which saves more
in silicon cost. The proposed IP generator can automatically
search for the optimal word-length setting of each stage,
which is a feature that the conventional IP generators do not
provide.
Exhaustive search for optimal word lengths is a time-
consuming work. Observing the pipeline architecture, if the
data-path at earlier stages uses a smaller word length, the
delay elements can save more and a smaller-size complex
multiplier is probably instantiated. Hence, we proposed a
procedure which includes two search phases, that is, global
search and local search, which aim to use smaller word-
length settings at the earlier stages. Initially, the same word
length of the fractional part is set at all the PE stages. In
the first phase, that is, the global search, the fractional-part
word lengths of all the PE stages are increased or decreased
together until an SQNR value of the FFT output closest to
but greater than the target value is obtained. Subsequently,
the reduction of the twiddle-factor word length is not ceased
until the SQNR value is below the target value. In fact, the
global search phase only determines the finest precision of
data paths and twiddle factors, which has also been proposed
in [16]. On the other hand, it has been pointed out in [25]
that using varying word lengths at each stage is viable when
the request of the IP that is optimized for each specific
application is eager. We then proposed a second phase to
fine tune the word length at each stage. The quantization
error accumulates and thus the LSBs may be contaminated
by quantization errors. We then truncate the LSB from the
last stage to examine if the target SQNR can be still fulfilled.
If the answer is true, then the test of LSB truncation proceeds
to the earlier stages sequentially until the SQNR value is not
satisfied. When it happens, we then restore the truncation at
that stage and initiate a new iteration of LSB truncation from
the last stage again. The procedure goes on so that the word
length at each stage can be minimized.
Table 3 gives the results of word-length optimization
procedure in the global search phase and the local search
phase for 256-point FFT with an SQNR requirement of
55dB. As mentioned earlier, in the global search phase,
one fractional part word length of all the PEs and one
fractional-part word length of all the twiddle factors are
chosen, respectively. We can see that if the LSB at stage
4 is eliminated, the SQNR value becomes unsatisfying.
12 EURASIP Journal on Advances in Signal Processing
can approach the desired SQNR value by removing those
harmless LSBs at the last stages compared to the fixed-point
representation as shown in Figure 10(a) and can employ
more flexible and adequate word lengths at each stage.
Since the word lengths at each stage may be diﬀerent, we
depict the averaged word length of the proposed scheme in
Figure 10(b). It is thus clear that our proposed word-length
algorithm can meet the requirement of any user-specified
SQNR value with reduced silicon cost.
4.3. Instantiation and Connection. Since the architecture to
be generated is very regular, in the library we have prepared
the basic submodules such as PE1 to PE6, a complex
multiplier, a memory array, shift registers, pipeline registers,
a commutator, and multiplexers. After the optimal word-
lengths are derived, we can instantiate related submodules in
the top module.
Nested FOR loops are used in the program to do the
instantiation. In the outer loop, the program will judge
which processing elements should be inserted, whether a
multiplexer is required or not to control the signal flow,
and whether a pipeline register should be included at the
current stage. The look-up tables for twiddle factors will
be automatically generated after its word length and the
table size is determined. For the multichannel configuration,
the second inner loop is used to duplicate the submodules
that can not be shared. Once the instantiations of all the
submodules are complete, the wires that connect these
input and output ports are declared and constructed. Sign
extension and LSB truncation are performed necessarily
to ensure correct signal propagation between stages with
diﬀerent representation formats.
4.4. File Output and Test Bench. Finally, our IP generator will
provide the user an FFT processor IP core and one test bench
to facilitate its verification. For a variable-size FFT processor,
the multiple test benches are oﬀered to verify the correctness
of each respective size.
5. Experimental Results and Comparisons
To verify the proposed IP generator, design examples of
diﬀerent configurations are tested as shown in Table 4. First,
we use the IP generator to generate FFT IP cores. Their
function has been examined to be correct with the auto-
matically generated test bench. The desired SQNR as well as
the resulted SQNR that adopts the simulated values in the
search procedure is also available in the table. In Table 4(a),
the performance and complexity of the FFT IP cores that
are implemented by FPGA of device xc4vsx55-12 are given.
The hardware complexity is evaluated in terms of FPGA
resources of all design examples. The number of flip-flops
is related with the pipeline registers and short delay buﬀers,
while the number of slices reflects the logic complexity
including distributed RAMs for long delay buﬀers and ROMs
for twiddle-factor lookup tables. The DSP slices correspond
to the multipliers. In our generated IP core, the DSP slices
divided by four is exactly the number of complex multipliers.
For applications in UWB systems with a sampling rate
of 528MHz and a throughput requirement more than 410
Mega samples, four parallel processing blocks are used so that
the operating frequency can be reduced by a factor of 4 [3,
24]. The throughput is calculated by themaximum operating
frequency derived after synthesis. It is clear that the generated
FFT IP meets the requirements of the UWB systems. For
two-channel FFT processors, the complexity grows almost
linearly. In addition, we can see the advantages of the
modified constant multipliers in Figure 4 by comparing the
implementation results of 802.11nwith one channel and four
channels. We can see that the DSP slices are reduced from
8 × 4 to 16, a 50% reduction in complex multipliers. And
the number of slice grows due to the complexity of shifters
and adders in constant multipliers. As to the large-size FFT
processors for 3GPP-LTE or DVB-T, the advantage of the
radix-23 algorithm is clear in that it results in small increase
of the number of complex multipliers. However, in these FFT
processors, large ROM tables with 256 entries in 2048-point
FFT and with 1024 entries in 8192-point FFT are required.
Also, the long delay buﬀers implemented by distributed
RAMs are also entailed. Both occupy large resources of the
number of slices. Although block RAMs in FPGA can be used
instead, owing to that the RAM macro is vendor specific,
we still use the memory array, thus being implemented
by distributed RAMs, to support the applications of the
generated IP cores in cell-base design flow. On the right-
hand side of the table, we compare the IPs generated
by diﬀerent generators. Due to the fact that the pipeline
registers are not inserted arbitrarily, equal throughput of the
generated FFT processors is not straightforward to come by.
However, we can normalize the hardware complexity to the
throughput and evaluate the relative complexity as the ratio
indicated in the parenthesis. With the radix-23 algorithm
and the flexible architecture, our generated IP core uses
less flip-flops and DSP slices (complex multipliers) with a
slightly increased number of slices compared to the ones
generated by Xilinx Logicore and the Spiral program. Thus,
the hardware eﬃciency is better.
In Table 4(b), the synthesis results of several generated
FFT IP cores by Design Compiler in 90 nm UMC CMOS
technology are listed. The maximum frequency is derived by
the critical path delay of typical cell library under 1-V supply
voltage. Since the word length at each stage varies in our
works, the average word length of all the butterfly stages is
shown. To get further insight into their logic components,
we indicate the equivalent gate counts of combinational
logic and noncombinational logic. The normalized area is
provided for fair comparison. The original area in their
respective CMOS technology is also given in the parenthesis.
The throughput is derived based on the maximum operating
clock frequency. The power consumption is estimated from
the synthesized results at 1-V supply voltage. Usually it is
pessimistic compared to the measurement results. Other
designs of 64-point FFT processors for 802.11a, 256-point
FFT processors for 802.16e, and an 8-channel FFT processor
are also included. Because the timing information of the
generator is mainly derived through the results in Virtex-4
FPGA, for the cell-based design flow, we push the maximum
14 EURASIP Journal on Advances in Signal Processing
low-power FFT IP by appropriate parameter settings. For
example, we can set higher operating clock frequency than
the nominal system sampling frequency to generate the
IP that has short critical path delay and then scale down
the supply voltage or synthesize it with a low-speed low-
leakage library [29]. However, there are also some limitations
that the proposed IP generator can not fully replace the
manually designed application-specific IC (ASIC), like the
use of sleep transistors or multiple-threshold transistors,
especially in nanotechnology. Besides, in large-size FFT
processors, instead of shift registers, the delay buﬀers are
usually implemented by SRAMs, which are vendor specific
and are not built-in. However, with the proposed automatic
generator, the majority of the design eﬀorts are saved.
6. Conclusion
To reduce the hardware design eﬀorts spent on diﬀerent
FFT/IFFT processors for several communication standards
and systems, an IP generator is developed. The proposed
generator uses the higher radix algorithm and thus can save
the number of complex multipliers in the generated FFT
IP cores. In addition, we analyze the finite precision eﬀect
of the radix-2/4/8 algorithm, and a more accurate analytic
result is derived. By observing the properties of the finite
precision eﬀect in FFT operation, an eﬀective word-length
searching procedure is proposed. With word-length opti-
mization, a good tradeoﬀ can be selected between complexity
and accuracy. Besides, the pipelined architecture facilitates
cutting oﬀ critical paths, and hence the generated FFT IP
cores can be driven by a suitable clock frequency specified
by users to introduce appropriate pipeline registers. The
configurations of the variable-size and multichannel modes
fulfill the needs of prosperous communication standards.
To meet the throughput requirements, parallel processing is
also incorporated. In summary, the proposed IP generator
oﬀers more flexibility and configurability than conventional
solutions for recent MIMO-OFDM systems. Experimental
results have demonstrated its capability and feasibility to
generate a hardware-eﬃcient design.
Appendices
A. Derivation of Mean Square Error after
Butterfly Operation
The mean squared error of the signal at stage (s + 1) after
complex addition takes the form of
σ2s+1,add
= E
{∣∣[x̂r,s+1(n)− xr,s+1(n)] + j[x̂i,s+1(n)− xi,s+1(n)]∣∣2
}
= E
{∣∣[δr,s(n) + δr,s(m)] + j[δi,s(n) + δi,s(m)]∣∣2
}
= E
{∣∣δr,s(n) + δr,s(m)∣∣2
}
+ E
{∣∣δi,s(n) + δi,s(m)∣∣2
}
= E
{
δ2r,s(n)
}
+ E
{
δ2r,s(m)
}
+ 2E
{
δr,s(n)
}
E
{
δr,s(m)
}
+ E
{
δ2i,s(n)
}
+ E
{
δ2i,s(m)
}
+ 2E
{
δi,s(n)
}
E
{
δi,s(m)
}
,
(A.1)
where we assume the quantization error is uncorrelated, and
hence
E
{
δr,s(n)δr,s(m)
} = E{δr,s(n)}E{δr,s(m)}. (A.2)
Similarly, the mean squared error of the signal after complex
subtraction becomes
σ2s+1,sub
= E
{∣∣[x̂r,s+1(m)−xr,s+1(m)]+ j[x̂i,s+1(m)−xi,s+1(m)]∣∣2
}
= E
{∣∣[δr,s(n)− δr,s(m)] + j[δi,s(n)− δi,s(m)]∣∣2
}
= E
{∣∣δr,s(n)− δr,s(m)∣∣2
}
+ E
{∣∣δi,s(n)− δi,s(m)∣∣2
}
= E
{
δ2r,s(n)
}
+ E
{
δ2r,s(m)
}
− 2E{δr,s(n)}E{δr,s(m)}
+ E
{
δ2i,s(n)
}
+ E
{
δ2i,s(m)
}
− 2E{δi,s(n)}E{δi,s(m)}.
(A.3)
The mean squared error of all the signals at the stage (s + 1)
can be calculated as
σ2PE,s+1 =
1
2
σ2s+1,add +
1
2
σ2s+1,sub
= 2
(
E
{
δ2r,s(n)
}
+ E
{
δ2i,s(n)
})
= 2σ2PE,s.
(A.4)
B. Derivation of Signal Energy at Each Stage
For r-point DFT, Parseval’s theorem states
r−1∑
n=0
|x(n)|2 = 1
r
r−1∑
k=0
|X(k)|2. (B.1)
Thus, for N-point FFT with N = rν, the sum of the squared
magnitude of the outputs at stage 1 can be rewritten as
N−1∑
k=0
|x1(k)|2 =
N/r−1∑
k2=0
⎛
⎝ r−1∑
k1=0
|x1(k1 + k2r)|2
⎞
⎠
=
N/r−1∑
n2=0
⎛
⎝r
r−1∑
n1=0
∣∣∣∣x
(
n1
N
r
+ n2
)∣∣∣∣
2
⎞
⎠
= r
N−1∑
n=0
|x(n)|2.
(B.2)
From the above, if an N-point FFT is decomposed into ν
stages of the r-point FFT, the sum of the squared magnitude
of the outputs at the sth stage is given by
N−1∑
n=0
|xs(n)|2 = rs
N−1∑
n=0
|x(n)|2 = 1
rν−s
N−1∑
k=0
|X(k)|2. (B.3)
In summary, for each stage of r-point FFT, the energy sum
increases by r times. Moreover, the magnitude of twiddle
factors is all 1, the existence of complexmultiplications stages
does not influence this result.
 1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：2011 年 3月 1 日 
 
                                 
 
計畫編號 NSC  98－2220－E－008－001－ 
計畫名稱 適用於數位家庭整合之無線高畫質視訊傳輸技術(3/3) 
出國人員
姓名 
蔡宗漢 
服務機構
及職稱 
中央大學電機系  
教授 
會議時間 
2010年 12月 13
日至 2010年 12
月 15日 
會議地點 
中國 深圳 
會議名稱 
(中文) 2010 國際緣起與進化計算研討會 
(英文) ICGEC 2010 (2010 The Fourth International Conference on 
Genetic and Evolutionary Computing) 
發表論文
題目 
(中文) 以 Markov 隨機場域背景相減法之前景偵測技術 
(英文) Markov Random Field Background Subtraction Method For 
Foreground Detection Under Moving Background Scene 
  
 3 
接近的中國來主辦，可以說是最能透徹瞭解中國目前的研究成果的一個重要的機會，對於我們在
台灣從事研究的老師而言，能完整的吸收此資訊是很有利的，也是國內從事多媒體系統與晶片設
計方面研究的專家學者未來應該積極參與的國際會議。 
 
四、攜回資料名稱及內容 
1. CD-ROM of Proceedings (共乙片，可利用索引快速查閱所需資料，相當方便。) 
 
 
 
 5 
研究高度相關也是競爭比較的對象。 
 
 6 月 1日, 一場是自己的學生在高速 QR分解模組的口頭報告, 安排的時間是在早
上 9:30~11:00 的場次,同場報告的還有關於 FFT, turbo decoder, clock harvesting,與
phase and frequency offset estimation 的作品, 也都與研究領域有相關, 聽眾大約有 20
人左右, 問問題的情形也相當踴躍，所幸報告的學生針對提問也都能正確地回答。下
午參加了一場 UWB的 sesseion,晚上則是令人期待的 banquet, 大會在 19世紀的古堡
搭像蒙古包之類的棚子, 只是上菜速度相當慢, 一直到 10:00 餐會才結束, 11:00 多才
回到會議會場, 午夜 12:00 方返回住宿旅社。 
 
6 月 2 日, 早上先是聆聽大會安排的第三場 Keynote, 演講主題是: “Energy-saving 
approaches for warehouse-scale computing” 主講人是來自 Google 的 Dr. Wolf-Dietrich 
Weber, 談論了 Google 在處理網頁搜尋時的節能技術, 在強調節能省電與綠能科技的
今日, 這主題顯得吸引人又有趣。接著是 poster 時間, 在 poster 會場亦遇到許多台灣
來的教授, 而該 session的 chair 是美國明尼蘇達大學的教授 Gerald Sobelman, 以前曾
見過, 此次偶遇故在會場攀談些許時候。 
 
二、與會心得 
參加國際性的會議可讓視野開闊, 也會了解國際間在學術上的競爭與交流, 每一個
與會人士都是有備而來, 對於有興趣的主題大家都不會輕易讓機會消失, 非常踴躍而主
動的討論, 因此更顯得英文的聽說讀寫都相當重要, 演講的技巧也很重要, 可以讓台下
的聽眾容易有共鳴, 親身參與會議更可以直接獲得與研究相關的訊息和交流, 以及意見
的回饋, 同時也可認識許多投入相同領域研究的學者, 收穫很多。 
 
三、考察參觀活動(無是項活動者略) 
四、建議 
參加國際會議對教師與學生來說有其必要性同時也獲益良多, 對於學術交流與研究
上的精進有不少砥礪啟發, 能獲得補助實質上有極大的幫助, 同時也讓學生能懷著榮譽
心出國報告, 增進其視野, 讓其親身體驗英文的重要。 
 
五、攜回資料名稱及內容 
ISCAS 2010 CD。內容包涵發表在此會議的所有論文之PDF檔。 
 
六、其他 
.ISCAS 2010 網站：http://www.iscas2010.org/ 
 7 
選，到對與會者的接待，都還有很多改進的地方。但因為大陸畢竟是講華語，因 
此對於第一次參加國際性研討會的研究生來說，也算是一個很好的學習機會。在面
對大部分都是華人的場合，比較能夠克服心理壓力，藉此培養報告的自信心，之後
再到比較歐美國家參加研討會，報告時的壓力就比較不會那麼大了。而且大陸在研
究能量上因為起步較慢，因此目前整體而言我認為還是台灣稍微進步一些。但同時
也可看到大陸研究生的積極與拼勁，藉此刺激我們國內研究生要更加緊努力，才不
會被大陸給迎頭 
 
三、考察參觀活動(無是項活動者略) 
四、建議 
研究生出國參加研討會，已是目前研究生必經的一個過程。但因為出國對於一般 
研究生來說還是一筆蠻沉重的經濟負擔，因此政府單位能夠給於補助我認為是一 
件很好的事情。但是因為可能某些原因，而限制某些“特定地區＂的補助上限， 
是一件非常破壞補助美意的規定，希望教育部能夠廢除這些不需要的限制。讓研 
究生能夠更踴躍的出國參加研討會，增進國內研究生的世界觀 
 
五、攜回資料名稱及內容 
會議論文集光碟與 
六、其他 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：蔡宗漢 計畫編號：98-2220-E-008-001- 
計畫名稱：適用於數位家庭整合之無線高畫質視訊傳輸技術(3/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 1 100%  
研究報告/技術報告 4 4 100%  
研討會論文 9 9 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 8 8 100%  專利 已獲得件數 2 2 100% 件  
件數 1 1 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 15 15 100%  
博士生 11 11 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 1 1 100% 
人次 
 
期刊論文 13 13 100%  
研究報告/技術報告 0 0 100%  
研討會論文 21 21 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
