hardware system. It is promising to have the 
interactivity between two persons located at remote 
sites via the network.   
This integrated project is planned to be a three-year 
proposal focusing on ’the development of multi-point 
interactive augmented reality based on 3D 
audio/video’. The goal is to develop applications on 
interactive games, tutorials, and performances via 
the network. Our system would augment virtual scenes 
into user’s real world. The 3D audio/visual 
information at each local site is first recorded by 
the local servers； then, the recorded audio and 
video data are analyzed, compressed, and transmitted 
to a cloud computing server for integration into 
virtual scenes previously prepared. Finally, the 
synthesized 3D audio/visual data for the augmented 
scene are re-transmitted back to user sites for 
interactivity purpose.  
This integrated project includes four sub-projects: 
I) video reconstruction techniques based on camera 
arrays architecture； II) the compression techniques 
for multi-viewpoint color and depth video； III) 
integration of multi-point 3D model and 3D video and 
the quality optimization techniques for arbitrary-
viewpoint 3D stereo video synthesis； IV) multi-point 
interactivity integration for real and virtual 3D 
audio.  
 
英文關鍵詞： Stereo TV, Free view-point TV, 3D audio, Multi-view 
video, network interactive multimedia, augmented 
reality 
 
 2 
行政院國家科學委員會專題研究計畫成果報告 
基於 3D 音視訊之多點互動擴增實境 
計畫編號：NSC 99-2221-E-194-002-MY3 
執行期限：99 年 8 月 1 日至 102 年 7 月 31 日 
主持人：賴文能   國立中正大學電機系 
計畫參與人員：賴文能、陳自強、林惠勇、江瑞秋、林筱婷 
 
一、 中文摘要 
 
由於多媒體科技不斷的發展，多媒體資訊的應
用已廣泛存在於日常生活中。據統計顯示 3D 電影
在美國的票房是一般電影的 3.6 倍，3D 立體不僅
帶來了前所未有的視覺衝擊，也帶來了更大的商
機，3DTV 應用技術已被網路票選為 2009、2010
年十大應用技術之一。此外，多視域 (Multi-view) 
以及自由視角 (Free View-point) 此種帶有『互動
式』的音視訊也極為熱門，國際視訊相關會議在探
討多視域視訊相關技術的投稿也與日俱增。然而，
未來『互動式』的多媒體將不再只是注重於使用者
與多媒體立體播放裝置 (例如立體視訊系統或螢
幕) 間的互動，而是讓網路兩端的使用者能夠透過
網路伺服器進行互動，注重真實性的網路多媒體立
體互動平台。 
本計畫規劃為三年期整合型計畫，目的為開發
『基於 3D 音/視訊之多點互動擴增實境』。我們
的目標係針對網路互動式的遊戲以及教學等應
用，強調以擴增實境的概念，將虛擬的場景、物體
擴增至使用者的世界，透過區域子系統進行各使用
者的現場 3D 音/視訊以及互動資料的擷取，再經
過相對資料分析、處理與壓縮，透過網路在雲端伺
服器進行整合使用者真實資料與虛擬場景之整
合，再傳送回使用者端進行立體播放，建構一個互
動式 3D 播放系統。 
本整合型計畫包含了以下四個子計畫：1) 子計
畫一：運用陣列式攝影架構進行視訊重建技術之研
究；2) 子計畫二：多視角彩色及深度視訊之壓縮
編碼技術；3) 子計畫三：多點 3D 模型與 3D 視
訊之整合與其任意視角 3D 立體影像合成之品質
最佳化技術；4) 子計畫四：真實與虛擬3D音訊之
多點互動整合技術。 
 
關鍵詞：立體電視、自由視角電視、3D 音訊、多
視域視訊、網路互動式多媒體、擴增實境 
 
Abstract： 
 
The applications of multimedia technologies have 
become an important part in our daily life. Statistics 
shows that the market of 3D movies has been 
increased to be 3.6 times of traditional 2D movies in 
America. 3D stereo technology brings not only visual 
impacts but also businesses. Moreover, interactive 
audio/video technology such as multi-viewpoint or 
free viewpoint is getting increasing attention. 
Numbers of articles about multi-view technology are 
submitted to international conferences. However, 
“interactivity” will not only be restricted to be 
between the human and the hardware system. It is 
promising to have the interactivity between two 
persons located at remote sites via the network.   
This integrated project is planned to be a 
three-year proposal focusing on “the development of 
multi-point interactive augmented reality based on 
3D audio/video”. The goal is to develop applications 
on interactive games, tutorials, and performances via 
the network. Our system would augment virtual 
scenes into user’s real world. The 3D audio/visual 
information at each local site is first recorded by the 
local servers; then, the recorded audio and video data 
are analyzed, compressed, and transmitted to a cloud 
computing server for integration into virtual scenes 
previously prepared. Finally, the synthesized 3D 
audio/visual data for the augmented scene are 
re-transmitted back to user sites for interactivity 
purpose.  
This integrated project includes four sub-projects: 
I) video reconstruction techniques based on camera 
arrays architecture; II) the compression techniques 
for multi-viewpoint color and depth video; III) 
integration of multi-point 3D model and 3D video 
and the quality optimization techniques for 
arbitrary-viewpoint 3D stereo video synthesis; IV) 
multi-point interactivity integration for real and 
virtual 3D audio.   
 
Keywords: Stereo TV, Free view-point TV, 3D 
audio, Multi-view video, network interactive 
multimedia, augmented reality 
 
 
二、 緣由與目的 
由於多媒體科技不斷的發展，多媒體資訊的
應用已廣泛存在於日常生活中，隨著各種播放裝
置的進步，人們對於多媒體資訊的需求也由傳統
收音機所提供的單純聲音資訊，進而演進為現今
的彩色影像與立體聲，乃至於立體影音效果的多
 4 
訊資料及互動資訊加以編碼壓縮，經由網路傳送至
雲端的伺服器，或是將伺服器轉換後的 3D 視訊
壓縮後傳送至各使用者端。 
子計畫三則是開發位於雲端伺服器上相關之 
3D 虛擬模型與使用者端傳送過來 3D 立體視訊
的整合，並將整合後的 3D 視訊資料加以轉換 
(重取樣)，交由子計畫二的壓縮子系統傳輸至各使
用者端，以利其進行立體視訊顯示。  
子計畫四則負責整個互動系統的所有音訊部
分，包括音訊的擷取、互動、整合、壓縮到最後呈
現之全盤技術，例如使用者端音場之錄製、使用者
聆聽相對應正確音場環境之建構、雲端伺服器中不
同音場音訊之整合、多通道音訊壓縮、及虛擬音之
建構等等。 
 
3.2  各計畫間之關聯性 
綜合上述，本群體計畫中各子計畫間的關係密
切。若以系統中各重要子系統來看，其所包含技術
如下： 
(1) 使用者端 3D 音/視訊擷取與處理子系統 (子 
計畫一的 3D 視訊與子計畫四的 3D 音訊)。  
(2) 使用者端及伺服端 3D 音/視訊壓縮/解壓縮子 
系統 (子計畫二的 3D 視訊壓縮、子計畫四的 
3D 音訊壓縮)。  
(3) 伺服器端虛擬與真實整合子系統 (子計畫三
的 3D 視訊部分、子計畫四的 3D 音訊部分)。  
(4) 使用者端 3D 音/視訊合成子系統 (子計畫三 
的 3D 視訊部分、子計畫四的 3D 音訊部
分)。  
綜合言之，四個子計畫及總計劃之間的關連可 
以如圖 3.2.1 來表示。 
 
 
 
圖 3.2.1 各子計畫關聯圖 
 
3.3  本年度各子計畫研究成果說明  
3.3.1 子計畫一：運用陣列式攝影架構進行視訊重 
     建技術之研究 
 
本計畫的目的在於利用從擷取到的影像合成
多通道高動態範圍影像對(HDR Image)先行消除
鬼影。之後用視覺顯示圖(Saliency Map)、梯度圖
(Gradient Map) 、 視 覺 顯 著 區 偵 測 (Saliency 
Detection)，將其各能量圖合併保留影像能量較大
的部分。之後使用 S.Avidan 及 A. Shamir [1.1]提出
的影像大小調整，削減能量較低的部分，可以保留
人類較為感興趣的部分，同時不會造成影像上的不
連續感，達到所擁有固定比例大小的影像，在轉換
不同比例大小的媒體顯示器之間的通用性與便利
性。我們整個系統架構，分別為影像擷取、多通道
高動態範圍影像合成、特徵擷取、特徵整合、基於
內容的影像大小調整共五大部分如圖3.3.1.1所示。 
 
 
圖 3.3.1.1. 系統流程圖 
 
A. 影像擷取（第 1年） 
影像擷取分成兩部分 
(1) 將四台相機兩兩靠近，且以平行擺設的方式架
設，並使所有相機位於同一水平面，固定相機
的光圈，各以不同的快門時間來拍攝影像，每
一對的相機拍出不同曝光度的影像作為系統的
輸入影像。 
(2) 另一部分我們使用深度攝影機，拍攝方式分為
單張影像擷取及連續影像擷取，分別擷取彩色
影像與深度影像。 
 
B. 多通道高動態範圍影像對合成（第 1年） 
這步驟會從平行架設的兩對相機 (總共四台)
拍出不同曝光度的影像去分別合成出一張高動態
範圍影像。我們提出「不同相機、不同曝光」的概
念（或稱 multi-camera, multi-exposure），藉由每
對相機中各個相機的曝光量，來達成擷取同一場景
之不同曝光度影像的目的，進而以像素之亮度值調
整與重組的方式來合成其高動態範圍影像。這樣的
做法可以達到不同曝光量之影像同時擷取的目
的，以減少影像合成時因景物移動而產生殘影
（ghost）的機會。 
 6 
 
圖 3.3.1.4 : 變量位元率之疊代運算流程圖 
 
 
此立體匹配演算法之疊代運算主要分為兩部分: 
 建立二值化地圖 (Binary map) 
 利用其地圖執行立體視覺匹配演算並疊代方式
收斂結果 
變量位元率之立體匹配在疊代過程中，每一
次的運算，都須建立二值化地圖以供每張影像，其
變量資料位元率之範圍估測使用。這二值化地圖可
操控影像中每個像素之變量位元率，以及立體視覺
匹配之運算區域。因此，二值化地圖既可控制資料
增加的區域即運算區域，亦可控制視差資訊圖結果
的品質程度需求。 
此運算流程需要使用立體匹配在兩個時間點:
初始立體影像的立體匹配，與疊代運算過程之立體
匹配。在初始立體匹配以及後續疊代立體匹配運
算，皆必須選擇一種演算法運算，得到最初以及後
續疊代運算的視差資訊圖，其中選擇很通行的全域
和區域的演算法來實作。我們系統若使用全域的立
體匹配演算法，之後疊代利用區域立體匹配(mix 
Graph Cut + Sum of Squared Differences)，如此一
來，其疊代收斂可以針對錯誤的局部小區域做更精
準的修復。因此最後結果收斂最佳化，其錯誤像素
的比例比使用原始 8 位元灰階影像的任何演算法
錯誤率低。 
 
 
圖 3.3.1.5：兩位元階層平面執行立體匹配之結果 
 
 
圖 3.3.1.6：計算後得到的二值化地圖 
C.2 邊緣偵測圖與視覺顯著圖的取得 
當線段受到裁切出現對不齊的現象時，這部
分較容易被注意到，因此定義邊緣偵測的特徵能有
一定程度地確保畫面上線條的連續性。將原始影像
I 作索貝爾運算，得到影像梯度圖 Eedge，索貝爾的
X 方向與 Y 方向的運算子。 
分析索貝爾水平方向與垂直方向的運算子，
水平方向的運算子主要為找出影像的水平方向的
梯度，在畫面上是垂直的線段，而索貝爾垂直方向
的運算子就是找影像垂直方向的梯度，在畫面上是
水平的線段，如公式 (1.1)，本計畫亦將索貝爾的
水平及垂直方向的強度分別給予權重，當影像為水
平方向縮減或延伸時，加重索貝爾水平方向的權重 
αs，將可減少垂直梯度所造成的能量分散，更容
易保存畫面上的垂直方向線段，減少垂直方向線段
的不連續現象發生。    
22
)1(E
y
l
x
l
SSedge 




   (1.1) 
 
自然影像的特徵中，最常被研究的視覺顯著
圖是延展縮減的不變量，這個特性又叫做 1⁄f 法
則。這個法則的含義是，若我們將許多的自然影像
累加起來並平均，進行富立葉轉換之後，其強度
A(f)資訊有著下列的特性：E{A(f)} 1⁄f。在頻率與
log−log 強度資訊的關係中，把單張的自然影像以
各種不同的旋轉角度合成，會發現這個關係的折線
就正是一條直線。雖然這個關係已經有許多的研究
成果，然而在實際應用上，這個關係有以下兩個限
制：1. 轉不變量在單張影像上是不存在的。2. 徵
點是分散的，大部分的低頻點分散在整張圖上，少
數的高頻點聚集在一群，且受到雜訊的影響很大
[1.3]。 
另外我們利用深度攝影機所截到的深度圖做
深度區域的分割，以場景的深度之相似性來加以分
類，這裡的實作使用影像金字塔 (Image Pyramids) 
[1.4] 進行深度區域分割，影像金字塔將圖片降低
取樣 (Down-Sample) 成數個不同比例大小，常用
於分析圖像中特徵，在色彩分割方面，若影像金字
塔中某一層的像素點與其鄰層的父親像素點集合
具有程度上的顏色相近值，則父親點集合會被合併
成一塊連通元件(Connected Component)，而同層的
各連通元件間若與鄰近者過於相似又可將它們融
合成一塊，逐層(Layer By Layer)處理過後就能得到
來源影像的深度區域分割圖如圖 3.3.1.7。 
 
  
圖 3.3.1.7：金字塔分割的結果 
 
 8 
影像的品質；所以在影像放大後，我們必須使用超
解析恢復技術(Super Resolution, SR)，參考隔壁品
質較好的高解析視角(View0) 資訊，以及同ㄧ視角
前後時刻高解析度的關鍵畫面來對失真的影像 
(View1)做修補，以提高影像的品質。 
之後搭配解壓縮的深度影像，我們可以進行自
由視角的影像合成。考量 View1 視角的影像品質
劣於 View0 的影像，我們提出新權重式的合成概
念，充分利用 View0 影像以產生品質改善的新視
角影像。 
子計畫二的成果將以三部分呈現，包含編碼架
構、影像的重建與品質提升以及自由視角影像的合
成。 
 
A. 編碼架構 (第 2,3 年) 
在子計畫二的研究中，我們使用 JMVC (Joint 
Multi-view Video coding) 作為編碼的基礎平台，並
對它的基本架構做修改，以符合我們所提出的基於
關鍵畫面的非對稱編碼架構，圖 3.3.2.1 為本計畫
中採用的非對稱編碼架構。 
 
圖 3.3.2.1 Key frame based asymmetric stereo video 
coding scheme 
 
此架構在編碼時額外傳輸了  View1 每個 
GOP 第一張的原始解析度影像，雖然在編碼時會
比非對稱編碼架構多出少許的資料量，但在解碼端
還原 View1 影像解析度的過程中，對於每張小解
析度的 View1 影像在實現影像放大時，除了有同
時間 View0 視角的影像可供參考之外，還多了大
張的 View1 影像可當參考來源。 
 
B. 影像的重建與品質提升 (第 1,2,3 年) 
    View1 影像在解碼端時必須放大還原成原始
影像以供播放，放大的過程中由於高頻資訊遺失，
造成重建影像的品質下降。我們將使用超解析影像
重建技術，提升影像的重建品質。我們提出
pixel-domain 與 wavelet-domain 的兩種做法。
pixel-domain 的做法分別利用隔壁視角的高頻資
訊搭配視差估計法，及自身視角前後時間關鍵畫面
的高頻資訊搭配運動估計法，當作補償的選擇。另
外，wavelet-domain 的做法參考文獻錯誤! 找不到
參照來源。所提到的方式，利用小波的概念，將
影像遺失的高頻資訊部份，用品質較好的高頻資訊
以 wavelet-domain 的方式進行填補與取代。最後，
也可以透過 DIBR[2.3] 的方式，由 View0 視角投
射產生 View1 虛擬視角畫面，之後與超解析重建
技術重建完成的影像相比較，最後輸出一張最佳的
重建影像。詳細的做法如下面的章節所述。 
 
B.1 Pixel-domain Super-Resolution (第 1,2 年) 
圖 3.3.2.2為我們所提出的基於關鍵畫面的非
對稱立體視訊編碼架構所使用的高頻補償流程
圖，跟一般非對稱的編碼架構相比，在提升 View1
影像品質的過程中多了前後時刻的影像可當參考
來源。 
 
 
 
圖 3.3.2.2. 基於關鍵畫面的非對稱立體視訊編碼
架構補償高頻資訊流程圖 
 
流程圖中的名稱解釋： 
D (.): down-sampling  
U (.) : up-sampling 
View0(t): t 時刻左眼視角的高解析影像 
View1(t): t 時刻右眼視角的低解析影像 
View1(t1): t1時刻 View1 視角高解析關鍵畫面的影
像 
View1(t2): t2時刻 View1 視角高解析關鍵畫面的影
像 
)(0 tView H : t 時刻 View0 視角的高頻影像 
)1(1 tView H : t1 時刻 View1 視角關鍵畫面的高頻影
像 
)2(1 tView H : t2時刻View1視角關鍵畫面的高頻影
像 
)(0 tView L : t 時刻 View0 視角的低頻影像 
)1(1 tView L : t1 時刻 View1 視角關鍵畫面的低頻影
像 
)2(1 tView L : t2時刻View1視角關鍵畫面的低頻影
像 
)(1 tView R : t 時刻 View1 視角最後完成高頻補償
後的輸出影像 
 
步驟： 
1. 將隔壁視角 View0(t) 的高解析影像經過
wavelet-based 的 down sampling 和 up sampling
 10 
2. 以 當前 View1 (t) 低解析影像放大後影像
)(1 tView L 當作基準，分別與 View1PL(t) 與 
View1R(t) 計算每個區塊之間的相似程度。 
3. 給定一區塊，計算 View1PL(t)與基準影像對應
區塊的 SAD，得到失真值 cost1。 
4. 給定一區塊，計算 View1R(t) 與基準影像對應
區塊的 SAD，得到失真值 cost2。 
5. 比較 cost1 與 cost2 大小，選擇有較小失真值
的來源，作為最佳的影像放大結果。 
 
B.5 實驗結果 (A & B 實驗結果) 
 
以下呈現基於關鍵畫面的非對稱立體視訊編碼的
效益及影像放大修補的品質。表 3.3.2.1 為編碼環
境: 
表 3.3.2.1. 編碼參數設定 
 Balloon Newspaper 
Resolution  1024768 1024768 
View number View3、5 View4、6 
GOP 16 
Frames 49 
QP set (texture) 28、32、36、40 
QP set (depth) 37, 41, 43, 45 
Coding structure Hierarchy B 
Down-sample 
and up-sample 
Wavelet-based 
 
(1) 基於關鍵畫面的非對稱立體視訊編碼 (第 2
年) 
實驗結果如圖 3.3.2.5、3.3.2.6 所示。縱軸為
修補高頻資訊後最終輸出的 View1el 高解析影像
品質，橫軸為傳輸 View1 低解析影像所需位元數 
(包含高解析關鍵畫面的位元數)。從此實驗數據
中，很明顯看出我們所追求的資料減量效果非常明
顯，最多減少幅度甚至到達 62 %。傳統的非對稱
編碼雖然最高減幅達到 69 %，但是影像的重建品
質明顯不如基於關鍵畫面的編碼方式，在 low bit 
rate 時兩者品質差距甚至到達 1.86 dB。 
 
 
圖 3.3.2.5. Balloon 的 R-D 曲線圖 
 
 
圖 3.3.2.6. Newspaper 的 R-D 曲線圖 
 
(2) 深度圖輔助基於關鍵畫面之編碼架構 (第 3
年) 
    實驗數據中，非對稱編碼的架構呈現經頻率域
或像素域修補後的影像；深度圖輔助編碼後，將經
過所提的方法決定最後輸出影像的來源，我們將呈
現傳輸與不傳輸深度影像，對 View1 視角影像重
建的影響。實驗結果如圖 3.3.2.7、3.3.2.8 所示。為
排除因為深度圖影像不準確而造成投影錯誤，特別
選用一組深度圖準確的測試影像 Mobile 來輔佐實
驗。在此組測試影像的結果，效果明顯比另一組影
像來的佳，深度圖的準確對於輔助影像品質的提升
效果較為明顯。 
圖 3.3.2.7、3.3.2.8 內縱軸為深度圖輔助關鍵
畫面編碼後的 View1 影像重建品質，橫軸為傳輸
低解析影像(含其關鍵畫面)及 View0 全解析深度
影像所需的位元數(所提的方法)，而對稱編碼與非
對稱編碼皆不包含深度圖的位元率。實驗結果顯
示，在小張影像放大的重建上，與基於關鍵畫面的
方法相比，傳輸深度影像後雖多付出了位元數，但
對於品質的提升確有正面的影響。造成編碼效率降
低，可能的原因在於深度影像與彩色影像間是獨立
編碼，並沒有利用到相對應的彩色影像資訊，未來
若將其妥善利用，相信編碼效益會有所提升。 
 
 
圖 3.3.2.7. Balloon 的 R-D 曲線圖 
 
 12 
 
 
圖 3.3.2.9. Newspaper 的 R-D 曲線圖 
 
(3) 深度圖輔助基於關鍵畫面編碼的影像合成 
從之前的實驗數據中，可以觀察出無論使用
基於關鍵畫面的編碼架構，或是加入深度圖輔助關
鍵畫面編碼架構，都有效的減少了資料量，並提升
了重建影像品質。View1 的重建影像品質將影響
FTV 合成虛擬視角影像的品質，此部分將呈現使
用此重建影像合成虛擬視角的增益。合成後的影像
品質將會使用未編碼前的兩視角及其深度圖產生
的虛擬視角當作基準 (ground truth ) 對象，計算其
影像品質。實驗結果將會與對稱編碼架構相互比
較，對稱編碼所花費的位元數為 View1 的彩色全
解析度影像及深度全解析度影像。深度圖輔助關鍵
畫面編碼花費的位元數則為 View1 的半解析度影
像加上 View0 全解析度深度影像的位元數。結果
如圖 3.3.2.10 與 3.3.2.11 所示，我們的編碼架構能
產生品質較佳的新視角合成影像。 
 
 
圖 3.3.2.10.Balloon 的 R-D 曲線圖 
 
 
圖 3.3.2.11. Mobile 的 R-D 曲線圖 
 
3.3.3 子計畫三：多點 3D 模型與 3D 視訊之整合
與其任意視角 3D 立體影像合成之品質最
佳化技術 
本子計畫所開發的核心技術包括：1) 深度影
像改善 (第 1,2 年)，2) 3D 虛擬背景模型與人體 
3D 視訊資料的整合 (第 2 年)，3) 三維人體姿態
估測與追蹤 (第 1,2 年) ，4) 3D 品質最佳化聯合
位元率控制 (第 3 年)。其中 (1) 可提供高品質的
立體影像合成與顯示，(4) 的技術開發有助於立體
視訊的合成、壓縮、與傳輸技術的最佳化，也是立
體視訊人因工程的具體實施，而 (2)(3) 的研究更
可促進異質多媒體的整合。 
A. 深度影像改善 (第 1,2 年) 
由於原始 ToF 深度影像為低解析度，經過 
3D warping 至彩色攝影機所在處後仍為低解析
度，這對 DIBR 所需的 2D+depth 格式有不良影
響，因此需轉換為高解析度深度影像。然而，若是
直接將 3D warping 後的深度影像直接以一般的 
scaleup 或 zooming 法則放大，則放大後的深度影
像除了與彩色影像在邊緣無法對齊外，其品質亦不
佳。因此，我們不採用直接放大，所謂的「深度影
像改善」是將低解析度轉換至高解析度深度影像的
一個過程，處理深度影像中不精確的部分。 
本計畫採用色彩分割 (color segmentation) 的
方法 (如圖 3.3.3.1)，先將兩邊攝影機的彩色影像
分割成好幾個色塊區域。先前把低解析度深度影像
以 depth warping (即所謂 3D warping，但被投影
的量為 depth，非 color) 投影到彩色攝影機平面
上，對投影後的深度影像做二值化 (如圖 3.3.3.2) 
以分出前景和背景。我們對每一個色塊區域依其所
對應到的深度影像的屬性 (前景或背景) 來判斷
該色塊區域屬於前景或背景，以把前景和背景的色
塊區分出來 (也就是說，色彩資訊中的前景/背景
分類係以深度影像資訊來做輔助)。因此，往後在
進行深度影像內插時，可參考色塊區域分類結果的
資訊。內插深度影像中的空點時會由每一個色塊區
域內的深度值加總取平均。內插的深度值 D(x,y) 
如下式表示( kR :為第 k 個色塊區域，ｗ為搜索範
圍)。 
       
w
wi
w
wj kk
jiDR
n
yxDR ,
1
,    (3-1) 
   
圖 3.3.3.1  (左)為攝影機拍的彩色影像 
              (右)為經過色彩分割後的影像 
 14 
  
(a)初始深度影像        (b)經三項濾波器處 
理後之深度影像 
圖 3.3.3.5 
利用修正後的深度影像，設定一門檻值，判斷
拍攝物體為前景或背景，若為前景的區域，則保留
該區域的色彩資訊，若為背景的區域，則刪除該區
域的色彩資訊，如圖 3.3.3.6 所示。 
 
 
 
 
 
 
圖 3.3.3.6  (左)原始影像 
           (右)背景刪除之彩色影像 
 
(2) 3D 視訊與 3D 虛擬場景整合與互動 
我們將建立完成的三維場景再與後處理的 
3D 視訊資料 (2D+depth) 進行整合。場景的背景
部分可以依據使用者喜好選擇背景，一般為室內照
片或是風景照，因此場景中的三維模型可以使用簡
單的立方體或是圓柱體等，虛擬場景如圖 3.3.3.7
所示。 
          
圖 3.3.3.7 虛擬背景與虛擬物體 
    接著將影像中心點做為參考座標的原點，再將
使用者的 3D 人體模型放置在虛擬場景中正確的
位置即可。這種資料並非傳統 3D 視覺的量測資
料 (x, y, z 皆以公尺為單位)，因為它的橫向單位仍
然是 pixel。本計畫根據深度攝影機的規格值，及
物體點的深度距離來推算在該距離下每一像素 
span 的橫寬大小，由此可大約推算該物體 (及該
使用者) 在虛擬空間的尺寸大小。將 3D 視訊資料
上的像素點依據在三維空間中的關係，將視訊資料
與虛擬進行整合，並且在虛擬的場景中與預設的虛
擬物體互動，如圖 3.3.3.8 所示。 
 
圖 3.3.3.8 3D 視訊資料與虛擬場景整 
在虛擬場景與 3D 視訊資料整合這一部分的
實驗，我們利用前一部分拍攝之彩色影像資訊與深
度影像資訊，藉由 3D 繪製軟體產生虛擬的三維
空間，並且依照實際比例的大小，將 3D 視訊資
料與虛擬場景結合，其結果如圖 3.3.3.9 所示。 
 
  
(a)   (b) 
圖 3.3.3.9  (a) 3D 虛擬場景 
          (b) 3D 視訊資料與虛擬場景結合 
C. 三維人體姿態估測與追蹤 (第 1,2 年) 
在本計畫中，我們為了讓使用者達到遠端視訊
會議的視覺上互動效果，因此需先對使用者的姿態
做進一步偵測與追蹤。如文獻 [3.1]，在這裡我們
是利用 Microsoft Kinect sensor 所擷取到的資料來
做三維人體上半身姿態追蹤，整個追蹤系統架構圖
如下圖 3.3.3.10 所示。我們將得到的 Microsoft 
Kinect sensor 的資料做為系統輸入，經過 Image 
preprocessing、Human pose initialization、3D model 
construction、Tracking 這些步驟後，生成相對應
於使用者姿態的人體模型並顯示。 
 
 
圖 3.3.3.10 人體三維姿態之追蹤系統架構圖 
(1) Image Pre-processing 
下圖為本計劃透過 Microsoft Kinect sensor 所得到
場景的 color map 與 depth map，如圖 3.3.3.11 所
示。 
 16 
 
圖 3.3.3.17 經由三維遮罩所挑選出的各關節候 
          選點 
(4) Tracking 
本計劃所提出的追蹤法則是基於動態規劃最
佳化演算法，動態規劃 (dynamic programming，簡
稱 DP)。動態規劃在有很多重疊子問題的情況的最
優解時有效。它將問題重新組合成子問題。為了避
免多次解決這些子問題，它們的結果都逐漸被計算
並被保存，從簡單的問題直到整個問題都被解決。
因此，動態規劃保存遞迴時的結果，因而不會在解
決同樣的問題時花費時間。 
動態模型可依時間因素劃分為幾個時期及階
段，如圖 3.3.3.18 所示，而在每個時期或階段均需
做成決策；每一個決策又連帶的影響了下一期的決
策，因此形成一連串的相關決策。 
 
圖 3.3.3.18 本計劃使用的多階段拓樸架構概念圖 
對於頭部、脖子、胸腔這個三個關節點，因為
其使用的邊緣特徵參考性過低，如果只參考距離特
徵，則穩定性又嫌不足，因此我們利用頭部、脖子、
胸腔與雙肩這四者在人體上的比例特性，在估測出
頭部、脖子、胸腔的關節點。因此在圖 3.3.3.19 中
少了脖子這一個階段使得追蹤系統分為兩部分分
別追蹤，如圖 3.3.3.19 所示。 
 
圖 3.3.3.19 本計劃使用的多階段拓樸架構 
經由姿態校正後所得到的 9 個關節點位置，利
用上一刻關節點的影像位置選取一個遮罩範圍，將
目前在這個範圍裡的點雲做為最佳關節點的候選
點。因我們定義的三維人體模型有 9 個關節點，所
以也有 9 個相對應的關節點狀態。 
為了從最佳候選點雲 Si 集合中估測到最佳的
關節點，並將人體骨架追蹤套用至動態規劃 
(DP)，我們定義了兩個特徵，分別是邊緣特徵及距
離特徵，這兩個特徵在動態規劃 (DP) 當中所代表
的物理意義為節點花費 (node cost) 與連線花費
(edge cost)，我們會找到一條最小花費(cost)的路
徑即最佳路徑。 
邊緣特徵 – 對於節點花費 (node cost)，我們
設計一個基本的三維遮罩如圖 3.3.3.20 所示。由於
我們所要追蹤的六個關節點特性略有不同，因此針
對手部與手肘設計了四種變化形式的十字型遮
罩，如圖 3.3.3.21 所示，來計算關節點的節點花費 
(node cost)。  
         
        圖 3.3.3.20  三維遮罩示意圖 
 
       圖 3.3.3.21  四種形式的十字型遮罩 
在圖 3.3.3.22 中，Di 、Wi  與圖 3.3.3.16 定義
相同。我們把三維空間以象限區分，並且統計每
一個象限區域內的點雲數目，之後把統計出來的
結果依照 (3.4) 式來判斷，其中，NumberⅠⅡⅢⅣ 代
表每一個象限所包含的點雲數目，而ζ代表比例
的門檻值，依此來選擇 Ω 的模式，如圖 3.3.3.23
所示。 
















)/( if   4_
)/( if   3_
)/( if   2_
)/( if    1_
IVII
IIII
IVII
IIII
type
NNMask
NNMask
NNMask
NNMask
    (3.4) 
    
III
III IV
 
圖 3.3.3.22 三維空間與平面象限示意圖 
 18 
 
265 335 
圖 3.3.3.26 人體姿態追蹤實驗結果 
 
表 3.3.3.1 丟球動作中各關節點的平均誤差度 
關節點 右手 右手肘 右肩 
平均誤差度(像素) 7.80 9.64 9.28 
關節點 左肩 左手肘 左手 
平均誤差度(像素) 8.20 7.44 8.62 
 
本子計畫亦將追蹤結果與  PrimeSense 的 
NITE 軟體相比較，其結果如圖 3.3.3.27 所示，
其中綠色者是 NITE 的結果，而黃色者是我們的
結果。我們可以看到效果可比擬，而本子計畫的追
蹤速度約在 30~60 Hz 之間，視每個關節點所選擇 
candidate point 集合的大小而定。 
   
 
圖 3.3.3.27 本計畫的 DP 追蹤法則與 
PrimeSense 的 NITE 軟體追蹤效果的比較 
 
D. 3D 品質最佳化聯合位元率控制 (第 3 年) 
本計畫所提出的聯合位元率控制方法架構於 
H.264/SVC 編碼標準上 [3.2]。我們將彩色序列作
為基本層 (Base Layer) 編碼，深度序列作為加強
層 (Enhancement Layer) 編碼。本計畫方法主要可
以分為三個重點部分：(1) 支持向量迴歸 (Support 
Vector Regression, SVR) 模型 [3.3]的建立。(2) 彩
色影像與深度影像的位元率分配。(3) 彩色與深度
影像序列的聯合位元率控制。在 SVR 模型建立的
過程中，需要先行建立訓練資料集。在本計畫中，
我們對每一個時刻的彩色影像與深度影像進行索
貝爾邊緣偵測 (Sobel Edge Detection)，以邊緣資訊
當作訓練資料中的輸入特徵值，接著嘗試各種不同
的彩色/深度位元分配比例來進行編碼並以參考文
獻 [3.5] 所提 3D 品質評估度量的最佳化為目標
尋找每一個時刻畫面最佳的彩色/深度位元分配比
例，當作訓練資料的正確輸出值。最後，透過收集
到的全部訓練資料建立 SVR 預測模型。在 SVR 
預測模型建立之後，編碼時就可以針對每個時刻的
彩色影像與深度影像進行特徵分析，利用所建立的 
SVR 模型預測當前時刻最佳的彩色/深度位元分
配比例，並進行位元率控制。 
(1) 3D 品質評估 
在本計畫中，我們採取 full-referene 的 3D 
品質評估方法 [3.5]。一開始將彩色影像與深度影
像分開進行評估。在彩色影像方面，以 SSIM 計
算原始彩色影像與編碼後彩色影像的相似性 
( CQ ) ；而在深度影像方面，則計算原始深度影像
與編碼後深度影像的相關係數 ( DQ )，最後再將兩
者所得到的值藉由 (3-5) 式進行線性組合，得到最
後的 3D 品質分數 ( FinalQ )，其中 ]1,0[FinalQ 。
圖 3.3.3.28 為 3D 評估方法流程圖。 
          2/1DCFinal QQQ        (3-5) 
 
圖 3.3.3.28 文獻 [3.5] 的 3D 品質評估流程 
          圖 
(2) 支持向量回歸技術 -目標輸出值 
本計畫所提出基於 3D 品質最佳化的聯合位
元率控制技術，期望透過 SVR 模型預測在每個時
刻彩色影像與深度影像間的最佳位元分配比例，以
達到最好的 3D 品質。所以在訓練資料集的建立
中，訓練資料的目標輸出值便是在每個時刻的最佳
彩色/深度位元分配比例值，其流程如圖 3.3.3.29。 
目標輸出值的求值步驟可分為五個步驟： 
步驟一：給定一總位元率，調整彩色影像序列與深
度影像序列的位元率比例分配。本論文實驗所採用
的比例組合共有九組： (10% , 90%)、 (20% , 
80%)、…、(90% , 10%)，括號中前者為彩色影像
分配比例，後者為深度影像分配比例。 
 
步驟二：將彩色影像序列與深度影像序列根據所分
配的位元率比例進行位元率控制，每一種比例組合
皆須進行。注意的是，在此我們使用 H.264/SVC 進
行編碼，而位元率係採聯合位元率控制。 
 20 
(4) 聯合位元率控制技術 
本計畫聯合位元率控制技術係架構於 
H.264/SVC 編碼標準上，圖 3.3.3.32 為本計畫聯合
位元率控制流程圖。由於在 H.264/SVC 的編碼平
台 JSVM (Joint Scalable Video Model) 中，加強層
方面並無位元率控制機制，所以為了達到聯合位元
率控制的目的，我們將基本層的位元率控制架構應
用於加強層上。並且對原始的位元率控制架構進行
修改： 
a. 每個時刻畫面目標位元的計算：在個別控制的
架構中，為各自獨立進行當前時刻畫面目標位
元的計算。而在聯合控制的架構中，我們將每
個時刻的彩色畫面與深度畫面設為一組單
位，計算當前時刻所需的總目標位元數 (記為 
ettB arg ) ，接著利用 SVR 預測位元分配比
例，計算彩色畫面與深度畫面的目標位元數 
(分別記為 c ettB arg 、
d
ettB arg )。計算公式如下式： 
d
SVRett
d
ett
c
SVRett
c
ett
pBB
pBB


argarg
argarg      (3-8)                             
其中 cSVRp  為 SVR 預測的彩色位元比例；
d
SVRp 為 SVR 預測的深度位元比例，且 
1 dSVR
c
SVR pp 。 
b. 暫存區的更新：在個別控制的架構中，每一層
皆有所屬的虛擬暫存區，每當編完一張影像之
後個別進行暫存區的更新。而在聯合控制的架
構中，只以一個虛擬暫存區進行位元控制。因
為在 H.264/SVC 中的編碼順序為先編碼基本
層，接著編碼加強層，所以，我們同樣以每個
時刻的彩色畫面與深度畫面為一組單位，當深
度影像編碼完才進行暫存區的更新，更新式子
如下式： 
    












 S
jid
ji
c
jijicjic BF
fu
fAfAfBfB ,
)(
)(,0maxmin)( ,,,,1,
 
                                      (3-9) 
其中 jif ,  為在第 i 個 GOP 內的第 j 張畫
面；  jic fB ,  為在編碼第 j 張畫面後，暫存
區所剩的空間； SB  表示為暫存區的大小；
 cjifA ,  與  djifA ,  分別代編碼第 j 張彩色畫
面與深度畫面後，實際產生的位元數；  jifu ,  
為畫面 jif ,  當時可用的通道頻寬； F  為畫
面率 (Frame Rate)。藉由以上兩點的修改以達
到聯合位元率控制的目的。 
在 H.264/SVC 中可以藉由層間預測  (Inter 
Layer Prediction) 技術增加編碼效益，然而，透過
實驗發現，在彩色影像序列與深度影像序列的
H.264/SVC 編碼中，在影像品質相當的情況下，
最多只能節省平均 2% ~ 3% 的位元率。所以，在
基於編碼效益增加有限且複雜度提升的前提下，本
計畫在 SVC 編碼中並不使用層間預測技術。也就
是說，除了某一些共用的語法位元之外，彩色與深
度幾乎是個別獨立編碼，但採取聯合位元率控制。 
彩色影像序列+深度影像序列
t 時刻總目標位元 計算
t 時刻基於 SVR 
位元分配比例計算
彩色影像位元率控制
深度影像位元率控制
暫存區更新
編碼完畢?
Yes
結束
No
ettB arg
c
ettB arg
d
ettB arg
)( ,
d
jifA
)( ,
c
jifA
)( 1, jic fB
 
圖 3.3.3.32 本論文聯合位元率控制架構圖 
在 3D 品質最佳化聯合位元率控制這一部份
的實驗，我們主要對 SVR 預測結果、位元率控制
精準度以及 3D 品質評估進行分析。 
位元率控制精準度 
我們以 MPEG 會議 [3.6] 建議的彩色影像
序列與深度影像序列量化參數組合進行編碼 (以
下彩色影像序列量化參數以 crefQP  表示，深度影
像序列量化參數以 drefQP  表示)，將彩色影像序列
與深度影像度序列個別編碼後得到的位元率 (分
別記為 dref
c
ref RR , ) 相加起來，以此當作我們的總位
元率參考值 dref
c
refref RRR   進行測試。每組測
試序列我們皆使用了任意三種 MPEG [3.6] 所建
議的量化參數組合進行實驗，因此在每組測試序列
可以得到三種總位元率。表 3.3.3.2 為平均位元率
 22 
六種情緒的歌單，並且能根據演唱者之性別、年齡
給予客製化的 KTV 歌曲表單，而 MTV 伴唱音樂
及演唱者歌聲則可以透過3D立體聲增強的方法使
得伴唱音樂及人聲的聲音更加有臨場感，而演唱者
之情緒也可以同時與 MTV 之原唱者情緒比較，依
權重計算兩者之匹配程度，進而計算歌唱情緒分
數。本子計畫目前已完成以下功能: (1)建立即時對
唱線上KTV平台 (2) MTV情緒感知 (3)線上KTV
評分系統 (4)說話者之性別及年齡辨識 (5)立體聲
增強技術。經過三年的研究開發已成功地完成真實
與虛擬音訊之多點互動整合技術，將在以下分別簡
述說明。 
 
A. 建立即時對唱線上 KTV 平台 (第 1,2 年成果) 
本子計畫提出的網路互動式 KTV (Karaoke 
Television)系統架構如圖 3.3.4.1 所示。首先針對
MTV(Music Television)之音視訊內容擷取特徵並
進行分析，定義出六種情感表現。在 KTV 歌單曲
目中提供依情緒指標挑選的服務，此外以音訊分析
技術辨識出性別與年齡資訊。最後，在歌唱時判斷
人類情感是否與歌曲匹配，算出歌唱真情分數。本
子計畫透過 Client 端以 RTMFP進行音視訊內容的
高速傳輸，並配合與 NTP Server [4.1] 的線上同步
播放技術，以達到即時對唱的目的。圖 3.3.4.2 為
本計畫建立的網路互動式 KTV 入口網站。 
 
 
圖 3.3.4.1 網路互動式 KTV 系統架構圖 
 
 
圖 3.3.4.2 網路互動式 KTV 入口網站 
網路 KTV 即時對唱功能實現，首先須考慮伴
唱 KTV 在進行互動時雙方使用者端進行同步播
放，而即時通訊主要是將雙方演唱者的歌聲傳至另
一位演唱者所在位置。藉由上述兩方面配合即可達
成線上即時對唱之目的，而根據哈斯效應 [4.2] 所
述，人類可以忍受的即時互動之延遲時間差為
100ms [4.3]，當超過 100ms 則會產生聲音重疊的情
形，因此網路 KTV 及時對唱的需求為雙方互動演
唱時的延遲時間必須在 100ms 以內。以下為建立
即時對唱 KTV系統方法 (a)同步播放伴唱 KTV音
視訊 (b)建立即時通訊系統。 
 
(1) 同步播放伴唱 KTV 音視訊 
dT ：雙方演唱者的播放時間差。 
syncT : 使用者與 NTP Server 的對時誤差。
例：  ATsync 為 A 電腦與 NTP Server 的對時
誤差。 
tT 電腦進行定時播放視訊的時間誤差。例：
 ATt 為 A 電腦播放 KTV 音視訊之誤差。 
本計劃利用參考文獻 [4.4] 之虛擬多人對唱卡拉
OK 交友系統的同步方法來實現高精準度的同步
伴唱 KTV 播放，以下將詳細說明。 
 
第一步: User1 與 User2 先與同一部 NTP Server 進
行同步對時，第二步: User1 設定未來播放 KTV 的
時間並將訊息傳送給 User2，第三步: User1 與
User2 在指定時間同步播放。 
 
播放時間差： 
         BTBTATATT tsynctsyncd     (4.1) 
同步方法架構圖如圖 3.3.4.3 所示: 
 
 
圖 3.3.4.3 同步方法架構圖 
 
而在同步播放方面，我們會將 KTV 伴唱影片
存放於 Web Server，供使用者自行下載到本地。接
著將進行互動的雙方使用者電腦與 NTP Server 進
行同步對時。然後，設定未來欲播放 KTV 的時間
後，透過 UDP Socket 來達成傳遞時間訊息的功
能。最後，呼叫播放程式完成播放 KTV 伴唱影片。 
 
(2) 建立即時通訊系統 
 24 
 
表 3.3.4.2 聲學特徵的提取和描述 
Features Description of Features 
Tempo Rhythm of music 
Duration (dur) Time distance between two high-energy frames. 
Zero Crossing Rate 
(zcr) 
Number of crossing over the 
zero point at a period 
Pitch Fundamental frequency of music 
Spectral Centroid 
(sc) 
Ratio of high-frequency versus 
low-frequency components in 
frames of a song clip 
Spectral Spread 
(ss) 
Second moment of the spectral 
centroid 
 
 
表 3.3.4.3 情緒和聲學特徵間的關係 
Acoustic Features Emotions 
tempo dur zcr pitch sc, ss 
Angry high low - high high 
Nervous low high low low low 
Excited high low - high high 
Happy high low high high low 
Sad low high low low high 
Calm low low high low low 
 
圖 3.3.4.7 MTV 情緒辨識系統方塊圖 
 
互動式 KTV 系統中也包含了 MTV，伴奏音
樂及人類歌唱聲等多媒體內容。分別從音訊和視訊
的角度判斷情緒的表現，我們可以針對刺激程度的
不同及正負面向做區分，進而使用 SVM (Support 
Vector Machine)多階層的分類架構得到辨識結
果，其流程如圖 3.3.4.8 所示。其中，在音訊特徵
擷取部分我們採用過零率 (ZCR)、持續時間
(Duration)、頻譜質心與頻譜展延、節奏等特徵；
視訊特徵擷取部分則採用動度向量(Motion Vector)
及色溫作為特徵指標。 
 
 
圖 3.3.4.8 多階層架構分類流程 
 
本子計畫在資料庫建置方面，目前有從網路
上收集了數百個 MTV 影音情緒片段，取樣頻率為
44.1KHZ，每個片段長度約 15-20 秒，而在進行音
訊特徵之前，以每 10MS 的長度切割出多張音視訊
畫面，並對音訊降頻(DOWN SAMPLING) 16KHZ 取
樣，進而進行後續音視訊特徵的擷取，如表 3.3.4.4
所示，訓練與測試資料總數共計 204 筆 MTV 片
段，而多階層辨識架構則使用了 5 個分類器，為了
瞭解音視訊特徵的組合對辨識率的影響，測試結果
如表 3.3.4.5、表 3.3.4.6 所呈現。在第一階層分別
探討刺激程度和正負面向兩種分離的分類方式，觀
察表 3.3.4.5，使用音訊特徵在區分刺激程度得到最
高準確率 89.2%，而使用色溫特徵區分正負面向程
度準確率達到 80.3%，因此，SVM_1 我們採用刺
激程度差異做區分。在第二階層下，高、低刺激程
度兩個情緒群組，分別透過 SVM_2 和 SVM_3 辨
識得到較佳準確率 91.8%和 69.4%。在最後一個階
層，我們必須區分(生氣、恐懼)、(興奮、快樂)兩
個群組，SVM_4 使用音訊和色溫特徵皆能達到
80%以上準確率，而 SVM_5 則是適用視訊特徵得
到相對較佳的辨識率。而六種情緒之多階層分類架
構實驗結果如表 3.3.4.7 所示，其平均準確率可以
達到 73.3%。 
 
表 3.3.4.4 六種情緒訓練與測試資料 
 Angry Nervous Excited Happy Sad Calm 
Training 14 6 9 13 14 12 
Testing 28 12 18 26 28 24 
 
表 3.3.4.5 第一階層分類音視訊特徵分析(A:音訊
特徵，V:視訊特徵，M:移動向量，CT:色溫) 
V First-Stage 
SVM_1 A M CT 
Arousal 
separation 89.2% 77.2% 63.5% 
Valence 
separation 65% 58.8% 80.3% 
 
 26 
angry 77.4 
nervous 92.4 
excited 82 
happy 87.8 
sad 98.5 
calm 97.6 
User-4 
(female) 
Average 89.3 
angry 55.8 
nervous 65.6 
excited 90.7 
happy 83.9 
sad 83.7 
calm 91.4 
User-5 
 
Average 78.5 
angry 67.2 
nervous 73.8 
excited 91.1 
happy 90.8 
sad 81.4 
calm 88.1 
User-6 
 
Average 82.1 
angry 65.1 
nervous 65.3 
excited 92.6 
happy 82.5 
sad 81 
calm 79.9 
User-7 
 
Average 77.7 
angry 64.2 
nervous 74 
excited 79.5 
happy 89.2 
sad 76.9 
calm 99.1 
User-8 
 
Average 80.5 
angry 62 
nervous 68.7 
excited 85.6 
happy 82 
sad 75.4 
calm 78.9 
User-9 
 
Average 75.4 
 
觀察結果顯示，具有歌唱經驗的人，在高刺
激情緒的歌曲如: angry、nervous、excited 和 happy
可以有效表達自己的情緒，故分數明顯高於不具歌
唱經驗的人，而在低刺激情緒的歌曲如:calm 和 sad
就不具有明顯差異。 
 
D. 說話者之性別及年齡辨識 (第 3 年成果) 
一般來說，人類說話特性可以反映出說話者
的性別與年齡層，唱歌亦同，差別在於唱歌聲連續
性高，且情緒起伏較大。假設處於平靜情緒下，歌
曲平均音調偏高，聲音較尖銳，我們可推測其為女
性歌聲，若處於較激動情緒下，男生會提高許多而
容易於女性聲音混淆。在年齡層部分，年輕人喜歡
節奏較重、較快而較為劇烈演唱音樂的風格，老年
人則通常喜好較為緩慢、柔和的音樂歌曲，歌唱聲
也較具滄桑的特性。而本子計畫在這裡針對音視訊
多媒體對人聲作後處理來判斷演唱者之性別與年
齡，依序可分成資料庫建置與預處理、特徵擷取與
特性分析、聲調變異之刺激程度啟發與特徵選取與
辨識等四個階段。如圖 3.3.4.10 所示。 
 
 
圖 3.3.4.10 基於語者聲調變異之年齡與性別辨識 
 
(1) 特徵擷取與特性分析 [4.14]-[4.16] 
藉 由 時 域 上 計 算 方 式 (Time Domain 
Computation) 與 頻 域 分 析 (Frequency Domain 
Analysis)，共擷取 10 組 37 項特徵參數，分別為時
間抖動(Jitter)、頻率抖動(Shimmer)、持續時間、
ZCR、過零率波峰(Zero Crossing Rate Peak)、基頻
(Fundamental Frequency)、頻譜質心、頻譜展延、
共振峰 (Formant)、梅爾倒頻譜系數 (Mel-scale 
Frequency Cepstral Coefficients, MFCC) ，如表
3.3.4.9 所示，考慮參數的物理意義與特徵間的差
異，可應用於不同年齡層與性別分群並進行識別。 
 
表 3.3.4.9 各種語音特徵 
Approaches 
of Feature 
Extraction 
Speech Features (An Abbreviation) 
Jitter 
Shimmer 
Duration (dur) 
Zero Crossing Ratio (zcr) 
Time 
Domain  
Zero Crossing Ratio Peak (zcrpk) 
Fundamental Frequency (ff) 
Spectral Spread (ss) 
Spetral Centroid (sc) 
Formant(f1’, f2’, f3’) 
Frequency 
Domain  
Mel-scale Frequency Cepstral 
Coefficients (mfcc) Dim:20 
 
(a) 在性別辨識方面，男、女生音高分別約略介
於 62~523Hz 與 110~1000Hz，或是成年人男
女基頻分別為 85 ~ 155 Hz 與 165 ~ 255Hz 
[4.17]，也就是說，基頻對於性別辨識上是一
項重要指標參數。圖 3.3.4.11 分別描述高、低
刺激程度情緒之語音音框的基頻特徵。在低
刺激程度情緒分佈較容易將資料區隔開成兩
群，換句話說，性別的識別使用低刺激程度
情緒的語音音框可能得到較佳的分類準確
率。 
 
 28 
 
圖 3.3.4.16 基於聲調變異特徵、特徵選取與辨識之
流程圖 
 
Type I: 只考慮說話者語句，分別經過特徵擷取與
選取步驟，而後進行分類及辨識時能分別得到年齡
與性別的準確率，並同時能取得區別不同年齡與不
同性別的一組最佳參數，這裡定義為特徵一群
(Feature set 1)與特徵二群(Feature set 2)，換句話
說，特徵一群與特徵二群分別對於年齡與性別有最
佳的分群及辨識效果。 
Type II: 採用聲調變異之刺激啟發策略，利用數個
高斯函數描述統計語音刺激程度，並依高、低刺激
程度進行音框分群，基於 Type I 我們使用經由特
徵選取方法最佳準確率下所挑選到的同一組參數
進行辨識，因而此種策略能夠驗證情緒刺激啟發之
可靠性。 
Type III: 可衍伸至實際應用上，只要取得一筆或
多筆語音資訊，即可採用聲調變異之刺激啟發策
略，而相較於 Type II 方式，採用 Type III 方式再
次重新進行一次特徵選取時，能分別針對年齡與性
別各再取得一組最佳參數組合，該組特徵組合不同
於 Feature set 1 與 Feature set 2，且所得到的辨識準
確率會更勝於 Type I 與 Type II 之方式。 
 
本子計畫使用了四種特徵選取的方法，分別
是循序正向選取 (Sequential Forward Selection, 
SFS)、循序反向選取(Sequential Backward Selection, 
SBS) 、循序浮動正向選取 (Sequential Floating 
Forward Selection, SFFS)與循序浮動反向選取
(Sequential Floating Backward Selection, SFBS) 。
圖 3.3.4.17 以 SFFS 方法為例，分別針對性別與年
齡辨識挑選最佳特徵參數組合，並做出正確分類比
率(Correct Classification Rate, CCR)曲線圖，在
3.3.4.17 圖(a)中挑選到第(1, 2, 3, 4, 5, 22, 23)個標
籤順序分別對應 (jitter, shimmer, f0, f0_std, sc, 
mfcc 8, mfcc 9)之 7 項參數組合，也就是說此七項
特徵對於性別辨識有較好的效果，另一方面，如圖
3.3.4.17(b)中總計挑選了 27 項特徵參數組合，可以
發現難以用少數特徵區別各年齡層，顯示年齡層在
辨識上困難。 
 
 
(a) 
 
(b) 
圖 3.3.4.17 以 SFFS 方法進行特徵參數選取 (a)針
對性別辨識 (b)針對年齡辨識 
 
(4) 實驗結果 
本子計畫在性別與年齡辨識部分，利用高斯
混合模型(GMM)描述資料分布特性，每位語者之
統計圖皆可產生不同群數的高斯分布個數，而之後
每個高斯分布皆可對應所涵蓋的音框範圍，每群語
音資料將各別對於年齡與性別辨識結果進行比
較。性別與不同年齡層語音資料個數如表 3.3.4.10
所示，共使用 193 筆語音資料，其中男性 89 位、
女性 103 位，而若以年齡層做區分，青年 65 位、
成年 91 位、老年 36 位。 
 
 
表 3.3.4.10 性別與不同年齡層語音資料個數 
Gender Age Quantity Totally 
Young 31 
Adult 41 Male 
Senior 17 
89 
Young 34 
Adult 50 Female 
Senior 19 
103 
 
本子計畫採取兩個高斯分布基於四種特徵擷
取方法來進行年齡與性別之特徵擷取，四種方法分
別為循序反向選取(Sequential Backward Selection, 
SBS) 、循序浮動正向選取 (Sequential Floating 
Forward Selection, SFFS)、循序正向選取(Sequential 
Forward Selection, SFS) 與循序浮動反向選取
(Sequential Floating Backward Selection, SFBS) 進
行機率密度函數之描述(PDF Modeling)，過程中，
挑選到的參數、個數 (Quantity)與正確分類率
 30 
zcr, 
zcr_std, 
zcrpkd, 
mfcc1-20 
Low 
jitter, 
f0_std, 
ss_std, 
sc_std, 
zcr, 
zcr_std, 
zcrpkd, 
mfcc 
1-9,13-1
9 
61.3% 
(↓6.4%) 
 
在實驗結果方面，首先藉由情緒資料庫之不
同刺激程度進行實驗，我們發現性別與年齡辨識準
確率分別隨著刺激程度的增加而降低和提升，此趨
勢顯示高刺激程度的情緒可能適用於年齡辨識，而
低刺激程度情緒有利於性別辨識。進一步考慮實際
情境下，說話者的心情不會頻繁的改變，因此考慮
聲調變異初步將語音音框分成大小各半，辨識準確
率分別在性別與年齡層方面能提升 8.5%與 9.5%。
在經過特徵選取方法後性別與準確率最佳可以達
到 97.8%，年齡準確率則可達到 67.7%。加以延伸
聲調變異之刺激啟發方法，藉由高斯機率密度函數
進行刺激程度分群，並結合特徵選取方式，兩群音
框分佈區域同時分別進行性別與年齡辨識，結果同
樣顯示性別辨識使用低刺激音框得到較佳結果
(97.9%)，而年齡辨識則使用高刺激音框有較佳準
確率(66.6%)，進一步重新使用特徵選取挑選較有
效之特徵組合，比較未經刺激程度分群前準確率，
分別在性別與年齡辨識能再次得到 1.1%與 3.9%提
升。 
 
E. 音訊聲學音場增強系統 (第 3 年成果) 
本子計畫提出立體聲增強技術，架構分為(a)
盲蔽音源分離 (b)聲學音場增強(寬度增強、深度增
強、對話增強) (c)低音增強三大部分，如圖 3.3.4.18
所示，分離出主音源：歌唱聲、對話聲，副音源：
配樂伴奏聲及環境音等，為針對不同的內容提供適
當的增強方法，配樂伴奏聲的部分，透過音場寬度
增強降低相關性方法，形成頭部音場外側化
(externalization)[4.19]，如圖 3.3.4.19 所示，減低聽
覺疲勞感[4.20],[4.21]，擴大感知音場，增加臨場
感。歌唱聲的部分則透過在主要的臨界頻帶加入延
遲脈衝、增加其空間感使其能夠和伴奏音場相匹
配，又不失其主音源清晰度，最後再將加強完後之
主音源和副音源合成，並考慮低頻衰減特性進行低
音增強。 
 
圖 3.3.4.18 聲學音場增強系統架構流程示意圖 
 
 
圖 3.3.4.19 頭外側音場定位示意圖[4.22] 
 
(1) 盲蔽音源分離 [4.23]  
分離之演算法流程如圖 3.3.4.20 所示，首先，
對左右聲道訊號切音框，音框大小為 8192 個取樣
點，約 186ms，取樣速率 44100Hz，進行短時域傅
立葉轉換，再選取所欲分離音訊的聲學特徵，利用
此特徵對每個音框之頻譜作二位元時頻遮蔽，接著
反短時域傅立葉轉換，進而形成左右聲道各音框主
副音分離後的訊號。 
 
圖 3.3.4.20 盲蔽音源分離流程圖 
 
(2) 聲學音場增強 
 
a. 寬度增強 
音場寬度增強方法，分為三個階段，如圖
3.3.4.21 所 示 ， 第一 階 段 先 透 過解 相 關 器
(De-correlator)來降低左右聲道間的相關性，以增
加聽者聆聽音場的綿密度與音源的方位性，第二階
段利用可調適性的差異訊號演算法，在不影響訊號
失真的情況下，盡可能地降低頻道間的相關性，第
 32 
窒礙難行的部分進行深入探討，提出修正的方式並
衍生相關的成果。本子計畫的相關研究成果包括了
五篇研討會論文、四篇期刊論文、一項美國專利，
以及中華民國與美國各兩項的專利申請。 
在子計畫二的三年計畫中，我們有效率且有系
統地改善多視角影像編碼系統遇到的問題。包括從
編碼端的資料減量，解碼端影像重建品質加強，以
及合成更好品質的自由視角影像。編碼端利用基於
關鍵畫面的非對稱編碼方式，有效的節省了多視角
影像傳輸時所需要的資料量。View1 低解析視角
在每個 GOP 第一張影像多傳送一張高解析度影
像，雖然因此付出較多的位元率，但在重建影像時
能夠擁有更多的參考來源。解碼端重建首先配合超
解析技術，分別利用像素域與頻率域的修補方式，
不僅參考前後時候關鍵畫面的移動向量補償，也參
考了與隔壁視角的視差向量補償，決定出最佳的高
頻資訊修補。實驗結果顯示重建影像的品質的確有
所提升。另外，在編碼端多傳送高解析度影像對應
的深度影像，能夠在解碼端配合 DIBR 技術提供一
張虛擬視角影像，達到自由視角播出的目的。與基
於關鍵畫面的方法相比，傳輸上加入深度影像後，
對於 View1 影像的重建品質提升確有正面的影
響，只是需要傳輸的位元率提高過多，未來將繼續
研究更有效的壓縮方式。 
在子計畫三中，第一年度中，我們為了讓使
用者實現遠端視訊會議的視覺互動，採用了高解度
彩色和低解析度深度攝影機 (Time-of-Flight) 來
架構我們的系統。攝影機對使用者上半身進行拍
攝，得到的資料可用於立體影像合成 (DIBR) 和
人體姿態追蹤系統上。透過 Image preprocessing、
human pose Initialization、3D model construction、
Tracking 等程序，將輸出最佳的姿態參數套入模
型達到追蹤的效果。在第二年度中，我們為了讓使
用者實現遠端視訊會議的視覺互動，採用了
Microsoft Kinect sensor 來架構我們的深度感測系
統。以深度攝影機對使用者上半身進行拍攝，得到
的資料可用於立體影像合成 (DIBR) 和人體姿態
追蹤系統上。透過 Image preprocessing、human pose 
Initialization、3D model construction、Tracking 等
程序，將輸出最佳的模型姿態參數以套入深度影像
中，達到姿態估測與追蹤的效果。而本項目所得到
的人體姿態參數將有助於將人體與虛擬背景模型
的整合 (例如把得到的人體模型置於虛擬桌子
旁，猶如視訊會議般)。在第三年度中，我們提出
一種基於 3D 品質最佳化的 3D 視訊 (彩色+深
度)編碼聯合位元率控制技術。本計畫將彩色影像
序列與深度影像序列進行整體的位元率控制，並利
用事先建立 SVR 預測模型，根據每個時刻的彩色
影像與深度影像特徵分佈，預測當前時刻的最佳彩
色/深度位元分配比例值，以達到立體合成影像的 
3D 品質最佳化。根據實驗結果可以發現在位元率
的控制效果方面，比起彩色/深度個別的位元率控
制方式可以精加 %83.2~%4.0  的精確度，並且
相對於其他編碼方法在大部分的 3D 視訊中都可
以獲得較佳的 3D 品質。綜上觀之，子計畫三已
按既定時程，達成預期的研究目標。 
在子計畫四中，我們建構出一套互動式音視訊
擴增實境系統，已完成了即時對唱的功能實現、
MTV 會依照伴唱音樂或是伴唱視訊辨識其情緒並
進行歌曲分類、歌曲的評分、性別及年齡辨識以及
可對MTV伴唱音樂和演唱者歌聲進行聲學音場增
強等功能。在即時對唱的部分，進行互動的雙方使
用者已能成功的藉由 RTMFP 進行音視訊傳遞，並
藉由與 NTP Server 對時來進行 MTV 的同步播放。
在 MTV 情緒感知部分，使用多階層的分類架構，
第一階層區分出高、低刺激情緒，主要使用到音訊
特徵；第二階層則分別以色溫和音訊為主要的特徵
區分出正負面向程度上的差異；第三階層作個別特
徵的組合得到較佳的效果。在歌曲評分機制部分，
我們可以從 MTV 之伴唱音樂、原唱歌聲及演唱者
歌聲之聲學特徵對應多維的方向及距離來計算出
演唱者與原唱者歌聲匹配的程度進而來計算其歌
唱分數。針對語者進行性別與年齡辨識的部份，性
別最高可以達到 98.9%的辨識率，而年齡辨識的部
份，最高可以達到 71.6%，且也可以依照演唱者的
性別及年齡，提供一份客製化的歌曲表單。在立體
聲增強的部分，可以為 MTV 伴唱音樂及演唱者歌
聲進行增強，使音樂及歌聲聽起來更有臨場感，也
為 KTV 互動系統更增添了真實性，雖然在歌聲的
清晰度沒有比原唱來的好，但是在臨場感以及低音
的表現都遠遠勝過了原始伴唱音樂。在未來，針對
互動式音視訊擴增實境系統，我們期望結合 3D 螢
幕，音樂伴唱提升為多聲道達到環繞、立體化的效
果，甚至在 KTV 對唱分享部分，當進行 KTV 對
唱互動時，利用擴增實境技巧打造聽眾彷彿如臨現
場的效果。 
本整合計畫之相關成果已有論文發表與投
稿、及專利申請，列示如下 (畫標線的作者為本案
共同主持人)。 
 
1. H. Y. Lin and T. W. Chen, "Augmented Reality 
with Human Body Interaction Based 
on Monocular 3D Pose Estimation," Advanced 
Concepts for Intelligent Vision Systems, Lecture 
Notes in Computer Science, Vol. 6474, pp. 
321-331, Springer, Dec., 2010. 
2. M. H. Chia, C. H. He, and H. Y. Lin, “Novel 
View Image Synthesis Based on Photo- 
Consistent 3D Model Deformation,” 
International Journal of Computational Science 
and Engineering, Vol. 8, No. 4, 2013. 
3. P. H. Lee, J. W. Huang, and H. Y. Lin, "3D 
Model Reconstruction Based on Multiple View 
Image Capture," 2012 IEEE International 
Symposium on Intelligent Signal Processing and 
Communication Systems (ISPACS 2012), Taipei, 
Taiwan, Nov. 2012. 
4. H. Y. Lin, C. C. Chang, and J. Y. Huang, "Image 
Retargeting Approach for Ranging 
Cameras," Imaging Science Journal, available 
online: Oct. 2012. 
 34 
33 – 41, 1984. 
[1.5] X. Hou and L. Zhang, “Saliency detection: A 
spectral residual approach”, in IEEE 
Conference on Computer Vision and Pattern 
Recognition, 2007, pp. 1 --8. 
[1.6] S. Goferman, L. Zelnik-Manor, and A. Tal, 
“Context-aware saliency detection” , in IEEE 
Computer Vision and Pattern Recognition, pp. 
1597--1604, 2010. 
[1.7] D. Scharstein and R. Szeliski, “A taxonomy 
and evaluation of dense two-frame stereo 
correspondence algorithms”, International 
Journal of Computer Vision, 47(1/2/3):7-42, 
April-June 2002 
[1.8] S. Sethuraman, Mel Siegel, and Angel Jordan, 
“A Multiresolution Framework for 
Stereoscopic Image Sequence Compression”, 
Proc. ICIP-94, November, 1994, pp. 361 – 
365 
[1.9] Y. Boykov, O. Veksler, R. Zabih, “Fast 
Approximate Energy Minimization via Graph 
Cuts”, IEEE Transactions on Pattern Analysis 
and Machine Intelligence, vol. 23, no. 11, pp. 
1222-1239, 2001 
[1.10] V. Kolmogorov, R. Zabih, “Computing Visual 
Correspondence with Occlusions Using Graph 
Cuts”, Computer Vision, 2001. ICCV 2001, 
pp. 508-515 vol. 2 
[1.11] E. Reinhard. High dynamic range imaging: 
acquisition, display, and image-based lighting. 
Morgan Kaufmann, 2006. 
[1.12] D. G. Lowe. “Distinctive image features from 
scale-invariant keypoints”. Int. J. Comput. 
Vision, 60(2):91–110, 2004. 
[1.13] P. E. Debevec and J. Malik. “Recovering high 
dynamic range radiance maps from 
photographs”, In SIGGRAPH ’97: 
Proceedings of the 24th annual conference on 
Computer graphics and interactive techniques, 
pages 369–378. ACM Press, 1997. 
[1.14] F. Drago, W. Martens, K. Myszkowski, and N. 
Chiba, “Design of a tone mapping operator for 
high dynamic range images based upon 
psychophysical evaluation and preference 
mapping”,  IS&T/SPIE Electronic Imaging, 
2003. 
[1.15] E. Reinhard, M. Stark, P. Shirley, and J. 
Ferwerda, “Photographic tone reproduction 
for digital images”, ACM Trans. Graph., 
21(3):267–276, 2002. 
[2.1] A. Smolic, K.Müller, P.Merkle, N.Atzpadin, 
C.Fehn, M.Müller, O.Schreer, R.Tanger, 
P.Kauff, T.Wiegand, Z.Megyesi, “Multi-view 
video plus depth (MVD)format for advanced 
3D video systems,” Joint Video Team (JVT) 
of ISO/IEC MPEG &ITU-T VCEG, 
JVT-W100, San Jose, CA,USA, April 2007. 
[2.2] H.Zheng, A.Bouzerdoum, and S.L.Phung, 
“Wavelet Based Nonlocal-Means 
Super-Resolution for Video Sequences,” Proc. 
of IEEE Int’l Conf. on Image Processing 
(ICIP), pp 2817-2820, 2010. 
[2.3] C.Fehn “A 3D-TV Approach using 
Depth-image-based Rendering (DIBR),“Proc. 
of VIIP, Sep. 2003. 
[3.1]  M. Haker, M. Bohme, T. Martinetz, and E. 
Barth. “Self-Organizing Maps for Pose 
Estimation with a Time-of-Flight Camera,” 
Dynamic 3D Imaging, pp. 142-153, 2009. 
[3.2]  H. Schwarz, D. Marpe, and T. Wiegand, 
“Overview of the Scalable Video Coding 
Extension of the H.264/AVC Standard,” IEEE 
Trans. on Circuits Systems for Video 
Technology, Vol. 17, No.9, pp. 1103~1120, 
Sep. 2007. 
[3.3]  A. J. Smola, and B. Scholkopf, “A Tutorial on 
Support Vector Regression,”  Statistics and 
Computing, pp.199~222, Aug. 2004. 
[3.4]  N. Cristianini, and J. Shawe-Taylor, “An 
Introduction to Support Vector Machines and 
Other Kernel-based Learning Methods”, 
Cambridge University Press, Cambridge, UK, 
2000. 
[3.5]  Benoit, P. Le Callet, P. Campisi, and R. 
Cousseau,“Using Disparity for Quality 
Assessment of Stereoscopic Images,” Proc. of 
IEEE Int’l Conf. on Image Processing 
(ICIP-2008), pp. 389~392, Brussels, San 
Diego, USA, Sep. 2008. 
[3.6]  Heiko Schwarz, and Dmytro Rusanovskyy, 
“Common Test  Conditions for 3DV 
Experimentation,” ISO/IEC 
JTC1/SC29/WG11, Doc. MPEG-N12745, 
Geneva, Switzerland, May 2012. 
[3.7]  Y. Morvan, D. Farin, and Peter H. N. de, 
“Joint Depth/Texture Bit-allocation for 
Multiview Video Compression,” Proc. of 
Picture Coding Symposium (PCS), Nov. 2007. 
[4.1]  NTP: The Network Time Protocol 
http://www.ntp.org/ 
[4.2]  Helmut Haas, “Über den Einfluss eines 
Einfachechos auf die Hörsamkeit von 
Sprache,” University of Gottingen, Germany, 
December 1949. 
[4.3]  Xiaoyuan Gu, Matthias Dick, Zefir Kurtisi, 
Ulf Noyer, and Lars Wolf, “Network-centric 
music performance: practice and 
experiments,” IEEE Communications 
Magazine, vol. 43, no. 6, pp 86-93, June 2005. 
[4.4]  虛擬多人對唱卡拉 OK 交友系統 馮士銓 
著 
[4.5]  O. T.-C. Chen, Jhen-Jhan Gu, Chih-Chang 
Chen, and Ping-Tsung Lu, “Affective 
perception of musical television,” Proc. of the 
4th International Conference on Awareness 
Science and Technology, Seoul, South Korea, 
Aug. 21-24, 2012. 
[4.6]  B. Pawate, “Method and system for karaoke 
scoring,” United States Patent, patent no.: 
5719344, 1998. 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/10/31
國科會補助計畫
計畫名稱: 總計畫：基於 3D 音視訊之多點互動擴增實境
計畫主持人: 賴文能
計畫編號: 99-2221-E-194-002-MY3 學門領域: 訊號處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
