中文摘要 
    機器學習或資料挖掘可以從現有的資料庫中擷取所需的知識或感興趣的樣版模式以減
輕建置專家系統或商業決策時之發展瓶頸。在 1982 年 Pawlak 首先提出約略集合理論，該
理論採用等價類別的概念並根據某種衡量標準來分割訓練例子，因此可作為一種處理資料
分類問題的方法。以往大部份使用約略集合理論之分類方法均使用離散的屬性值來推導出
資料間的關係；然而，因為模糊集合概念簡單且類似人類的推論方式，因此在智慧型系統
中常用語意名詞與隸屬函數來表示數量形態的資料。本計劃主要是整合約略集合與模糊集
合的理論來解決數量型資料應用上的問題。我們以涵蓋例子的概念提出了四種模糊約略的
知識擷取方法。第一種方法是改良約略集合論的方法並利用模糊理論以從數量形屬性資料
中推導出所有可能之最泛化涵蓋性模糊規則；第二種方法則是擴充第一種方法以處理語意
階層項目；而第三種方法則提出一整合約略集合和特殊化過程之混合型啟發式方法以節省
計算時間；最後則提出一個能推導近似涵蓋所有可能最泛化模糊規則之方法。我們所提出
的方法相當具有彈性且可用來建立模糊知識庫之雛型。 
 
關鍵字：約略集合，模糊集合，資料挖掘，數量型資料，確定性規則，可能性規則  
 
English Abstract 
Machine learning or data mining can extract desirable knowledge or interesting patterns from 
existing databases and ease the development bottleneck in building expert systems. The rough-set 
theory, proposed by Pawlak in 1982, serves as a new mathematical tool to deal with data 
classification problems. It adopts the concepts of equivalence classes to partition the training 
instances according to some criteria. Most mining algorithms based on the rough-set theory 
identify the relationships among data using crisp attribute values; however, fuzzy set concepts 
have often been used to represent quantitative data using linguistic terms and membership 
functions in intelligent systems because of its simplicity and similarity to human reasoning. 
In this project, we combine the rough set theory and the fuzzy set theory to solve the above 
problems. Four new methods are proposed based on the concept of coverage of training examples. 
The first proposed algorithm deals with the problem of producing a set of maximally general 
fuzzy rules for coverage of a set of quantitative training examples. The second one extends the 
first one to handling linguistic hierarchical items. The third learning algorithm is a hybrid 
heuristic one which integrates rough sets and rule specialization to save computation time. The 
final proposed algorithm produces an approximate coverage of training examples based on the 
variable rough set model. The proposed methods provide a flexible way in deriving rules and can 
be used to build a prototype for fuzzy knowledge bases. 
 
Keywords: rough set, fuzzy set, data mining, quantitative data, certain rule, possible rule. 
1. Introduction 
Although a wide variety of expert systems have been built, a development bottleneck occurs 
in knowledge acquisition. Recently, the rough-set theory has been used in reasoning and 
knowledge acquisition for expert systems. It was proposed by Pawlak in 1982 with the concept of 
same value of attribute Aj )(ijv, (that is, =
)(k
jv ), Obj
(i) and Obj(k) are said to have an 
indiscernibility relation (or an equivalence relation) on attribute Aj. Also, if Obj(i) and Obj(k) have 
the same values for each attribute in a subset B of A, Obj(i) and Obj(k)
The rough-set approach analyzes data according to two basic concepts, namely the lower 
and the upper approximations of a set. Let X be an arbitrary subset of the universe U, and B be an 
arbitrary subset of attribute set A. The lower and the upper approximations for B on X, denoted 
B
 are also said to have an 
indiscernibility (equivalence) relation on the attribute set B. These equivalence relations thus 
partition the object set U into disjoint set U into disjoint subsets, denoted by U/B, and the 
partition including Obj(i) is denoted by B(Obj(i)). 
*(X) and B*
B
(X) respectively, are defined as follows: 
*
B
(X) = {x | x ∈ U, B(x) ⊆ X }, and 
*
Elements in B
(X) = {x | x ∈ U and B(x) ∩ X ≠ ∅ }. 
*(x) can be classified as members of set X with full certainty using attribute set 
B, so B*(x) is called the lower approximation of X. Similarly, elements in B*(x) can be classified 
as members of the set X with only partial certainty using attribute set B, so B*
In the past, Lambert-Torres et al. found unimportant attributes from lower and upper 
approximations and deleted them from a database. Zhong et al. proposed a new incremental 
learning algorithm based on the generalization distribution table, which maintained the 
probabilistic relationships between the possible instances and the possible concepts. Two sets of 
generalizations were formed from the table based on the rough set model. One set consisted of all 
consistent generalizations and the other consisted of all contradictory generalizations, which were 
similar to the S and G sets in the version space approach. The generalizations were then gradually 
adjusted according to new instances. The examples in the database could then be merged since 
some attributes were removed. The resulting database was thus a compact database. Also, Yao 
formed a stratified granulation structure with respect to different levels of rough set 
approximations by incrementally clustering objects with the same characteristics together. Lee et 
al. simplified classification rules for data mining using rough set theory. The proposed 
classification method generated minimal classification rules and made the analysis of information 
systems easy. Tasumoto presented a knowledge discovery system based on rough sets and 
attribute-oriented generalization. It was used not only to acquire several sets of attributes 
important for classification, but also to evaluate how precisely the attributes of a database were 
able to classify data. The advantage of the rough set theory lies in its simplicity from a 
mathematical point of view since it requires only finite sets, equivalence relations, and 
cardinalities.  
(x) is called the 
upper approximation of X. After the lower and the upper approximations have been found, the 
rough-set theory can then obtain both certain and uncertain information and induce certain and 
possible rules from them. 
 
 Hierarchical Attribute Values 
Most of the previous studies on rough sets focused on finding certain rules and possible 
rules on a single concept level. However, hierarchical attribute values are usually predefined in 
real-world applications and can be represented by hierarchy trees. Terminal nodes on the trees 
represent actual attribute values appearing in training examples; internal nodes represent value 
clusters formed from their lower-level nodes. Deriving rules on multiple concept levels may lead 
to the discovery of more general and important knowledge from data. A simple example for 
attribute Transport is given in Figure 1.  
 
)(  )()( xuxuxu BABA ρ=∩ , 
wh ere ρ is an s-norm operator. That is, ρ is a function of [ ] [ ] [ ]1 ,01 ,0*1 0, →  and must 
satisfy the following conditions for each a, b, c [ ]1 ,0∈ : 
i. ;0  aa =ρ  
ii. ;    abba ρρ =  
iii. ;, if     dbcadcba ≥≥≥ ρρ  
iv. .  )  ()  (      cbacbacba ρρρρρρ ==  
Two instances of an s-norm operator for a ρ b are max(a, b) and a + b - a ∗ b. 
 
(3) The cut−α operator: 
})(|{)( αα ≥∈= xuXxxA A , 
where αA is an α-cut of a fuzzy set A. Aα
3. The First Proposed Algorithm 
 thus contains all the elements in the universal set X 
that have membership grades in A greater than or equal to the specified value of α. These 
fuzzy operators will be used in our fuzzy FP-tree mining algorithm to derive fuzzy 
association rules. 
In this project, four new methods are proposed based on the concept of coverage of training 
examples. The first proposed algorithm deals with the problem of producing a set of maximally 
general fuzzy rules for coverage of a set of quantitative training examples. The details of the 
proposed approach are shown below.  
The fuzzy rough-set algorithm for learning maximally general rules: 
INPUT: A quantitative data set with n objects belonging to c classes, each object having m 
attribute values, and a set of membership functions. 
OUTPUT: A set of maximally general rules. 
STEP 1: Partition the object set into disjoint subsets according to class labels. Denote each set of 
objects belonging to the same class Cl as Xl
STEP 2: Transform the quantitative value 
. 
)(i
jv  of each object Obj
(i), i=1 to n, for each attribute 
Aj )(ijf, j=1 to m, into a fuzzy set , represented as 
( ) ( )








+++
l
l
j
i
j
j
i
j
j
i
j
R
f
R
f
R
f
....
2
2
1
1
)(
, using the 
given membership functions, where Rjk is the k-th fuzzy region of attribute Aj f jk
i( ),  is 
)(i
jv ’s fuzzy membership value in region Rjk jA, and l (= ) is the number of fuzzy 
regions for Aj
STEP 3: Find the fuzzy elementary sets of the singleton attributes using fuzzy operations 
. 
STEP 4: Initialize l = 0, where l is used to count the number of the class being processed and k is 
A rough-set-based learning algorithm for a coverage set of maximally general multiple-level 
certain and possible rules:  
INPUT: A data set U with n objects, each of which has m decision attributes with hierarchical 
values and belongs to one of c classes.  
OUTPUT: A coverage set of maximally general multiple-level certain and possible rules. 
STEP 1: Partition the object set into disjoint subsets according to class labels. Denote each 
subset of objects belonging to class Cl as Xl
STEP 2: Find terminal-level elementary sets of single attributes; that is, if an object obj
. 
(i) has a 
terminal value vj(i) for attribute Aj, put obj(i) in the equivalence class for Aj = vj(i)
STEP 3: Link each terminal node for A
. 
j
it
jA = , 1 ≤ i ≤ |
t
jA |, in the taxonomy tree for attribute Aj 
to the equivalence class for Aj itjA = , where i
t
jA  is the i-th terminal value for Aj 
t
jA
and 
| | is the number of terminal attribute values for A
STEP 4: Set l to 1, where l is used to represent the number of the class currently being 
processed. 
j. 
STEP 5: Set q = 0, where q is used to count the number of attributes being processed. 
STEP 6: Set q = q + 1. 
STEP 7: Compute a terminal-level lower approximation of each attribute subset B with q 
attributes for class Xl
},)(,|)({)(* l
t
j
t
jl
t XxBUxxBXB ⊆∈=
 as: 
 
where )(xBtj is a terminal-level equivalence class including x and )(* l
t XB  is the 
terminal-level lower approximation for B on Xl
STEP 8: Derive the non-terminal-level lower approximations of each attribute subset 
. 
B  
STEP 9: Choose the equivalence class B'
by the 
union operation if the equivalence classes for all its underlying attribute values are all 
in lower approximations. 
j(x) among the lower approximations found and with 
the maximum value (≠0) of |B'j
STEP 10: Remove the elements in B'
(x)|; if two equivalence classes have the same   
maximum value, choose the one on a lower level; if they are still on the same level, 
choose one arbitrarily; put it in the certain rule set.   
j(x) from any Bj(x) in the lower approximations and from 
Xl
STEP 11: Repeat Steps 9 and 10 until all B
. 
j
STEP 12: If X
(x)'s in the lower approximations are empty. 
l  is empty, go to Step 20; if Xl  
STRP 13: Compute a terminal-level upper approximation of each single attribute B for class X
is not empty and q ≠ m, go to Step 6; otherwise, 
do the next step. 
l
},)(,)(,|)({)(* l
t
jl
t
j
t
jl
t XoriginalxBXxBUxxBXB ⊄≠∩∈= φ
 
as: 
 
where B tj (x) is the j-th terminal-level equivalence class in )(* lt XB , and )(* lt XB  
is the terminal-level upper approximation for B on Xl
STEP 14: Calculate the plausibility measure of each B
. 
t
j (x) as: 
p(B tj (x)) = 
)(
)(
xB
X originalxB
t
j
l
t
j ∩
, 
where original Xl
STEP 15: Choose the B
 means the object set of class l in Step 1. 
t'j(x) with the maximum plausibility value of |Bt'j
t
j
(x)| among those with 
B (x) ∩ Xl
STEP 16: Set X'
 ≠ ∅; if two equivalence classes have the same plausibility value, choose 
one arbitrarily; put it in the possible rule set. 
l = Xl and remove the elements in Bt'j(x) from Xl
STEP 17: Recursively check whether B
. 
t'j(x)'s specialization by combining the terminal-level 
possible values of another attribute can generate a set of new Bt''j(x) covering the 
elements in (Bt'j(x) ∩ X'l) with plausibility measure larger than that of Bt'j(x). Replace 
B tj (x) ∩ Xl
STEP 14: Set X'
 ≠ ∅; if two equivalence classes have the same plausibility value, choose 
one arbitrarily; put it in the possible rule set. 
l = Xl and remove the elements in Bt'j(x) from Xl
STEP 15: Recursively check whether B
. 
t'j(x)'s specialization by combining the terminal-level 
possible values of another attribute can generate a set of new Bt''j(x) covering the 
elements in (Bt'j(x) ∩ X'l) with plausibility measure larger than that of Bt'j(x). Replace 
Bt'j(x) with Bt''j(x) in the possible rule set if such a set of Bt''j(x) can be found. If the 
plausibility measure of Bt''j
STEP 16: Repeat Steps 13 to 15 until X
(x) is 1, check whether the plausibility value of its 
generalization from the taxonomy trees is still 1, and put the most general rule in the 
certain rule set. 
l
STEP 17: Repeat Steps 4 to 16 until l = c. 
 is empty. 
STEP 18: Derive the certain rules from the certain rule set. 
STEP 19: Derive the possible rules from the possible rule set. 
 
6. The Fourth Proposed Algorithm 
The final proposed algorithm produces an approximate coverage of training examples based 
on the variable rough set model. It can handle noisy quantitative training examples. 
 
The learning algorithm for a fuzzy approximate coverage: 
INPUT: A data set U with n objects belonging to c classes, each object having m attribute values, 
and a set of membership functions. 
OUTPUT: A set of maximally general fuzzy rules for an approximate coverage of the data set. 
STEP 1: Partition the object set into disjoint subsets according to the class labels. Denote each 
set of objects belonging to the same class Cl as Xl
STEP 2: Transform the quantitative value 
. 
)(i
jv  of each object Obj
(i), i=1 to n, for each attribute 
Aj )(ijf, j=1 to m, into a fuzzy set  represented as 
( ) ( )








+++
l
l
j
i
j
j
i
j
j
i
j
R
f
R
f
R
f
....
2
2
1
1
)(
 using the 
given membership functions, where Rjk is the k-th fuzzy region of attribute Aj f jk
i( ),  is 
)(i
jv ’s fuzzy membership value in region Rjk jA, and l (= ) is the number of fuzzy 
regions for Aj
STEP 3: Find the object partitions (elementary sets) for each singleton attribute. 
. 
STEP 4: Initialize l = 0, where l is used to count the number of the class being processed. 
STEP 5: Set l = l +1.  
STEP 6: Set q = 0, where q is used to count the number of attributes being processed. 
STEP 7: Set q = q +1. 
STEP 8: Calculate the relative degree of misclassification with respect to the original Xl for each 
equivalence class Bk with q attribute values and including an element x in Xl
c(B
 as: 
k(x), original Xl
∑
∑
∈
∩∈
)(
) )((
)(
)(
xBy
B
XoriginalxBy
B
k
k
lk
k
y
y
µ
µ
) = 1- , 
where original Xl
STEP 9: Compute the modified fuzzy β-lower approximation of each subset B with q attributes 
for class X
 means the original object set of class l in Step 1. 
l
B
 as: 
*β (Xl) = {(Bk(x))| x ∈ U, c(Bk(x), original Xl
where B
) ≤ β, 1≤ k≤|B(x)|},  
k(x) includes at least one element in Xl. 
Vol. 36, No. 2P2, 2009, pp. 3937-3945. (SCI)  
[2] C. H. Chen, T. P. Hong 
[3] C. H. Chen, 
and V. S. M. Tseng, "Mining fuzzy frequent trends from time series", 
Expert Systems with Applications, Vol. 36, No. 2P2, 2009, pp. 4147-4153. (SCI) 
T. P. Hong
[4] 
, V. S. M. Tseng and C. S. Lee, "A genetic-fuzzy mining approach 
for items with multiple minimum supports", Soft Computing, Vol. 13, No. 5, 2009, pp. 
521-533. (SCI) 
T. P. Hong
[5] 
, C. Y. Horng, C. H. Wu and S. L. Wang, "An improved data mining approach 
using predictive itemsets," Expert Systems with Applications, Vol. 36, No. 1, 2009, pp. 
72-80. (SCI) 
T. P. Hong
[6] C. W. Lin, 
, Y. L. Liou and S. L. Wang, “Fuzzy rough sets with hierarchical quantitative 
attributes", Expert Systems with Applications, Vol. 36, No. 3P2, 2009, pp. 6790-6799. 
(SCI) 
T. P. Hong
[7] 
 and W. H. Lu, “The Pre-FUFP algorithm for incremental mining”, 
Expert Systems with Applications, Vol. 36, No. 5, 2009, pp. 9498-9505. (SCI) 
T. P. Hong
[8] 
, C. W. Lin and Y. L. Wu, “Maintenance of fast updated frequent pattern trees for 
record deletion", Computational Statistics and Data Analysis, Vol. 53, No. 7, 2009, pp. 
2485-2499. (SCI) 
T. P. Hong
[9] C. H. Chen, 
, Y. Y. Wu and S. L. Wang, “An effective mining approach for up-to-date 
patterns", Expert Systems with Applications, Vol. 36, No. 6, 2009, pp. 9747-9752. (SCI) 
T. P. Hong
[10] C. S. Lee, M. H. Wang, G. Chaslot, J. B. Hoock, A. Rimmel, O. Teytaud, S. R. Tsai, S. C. 
Hsu and 
 and V. S. M. Tseng, "An improved approach to find membership 
functions and multiple minimum supports in fuzzy data mining", Expert Systems with 
Applications, Vol. 36, No. 6, 2009, pp. 10016-10024. (SCI) 
T. P. Hong
[11] S. L. Wang, T. Z. Lai, 
, “The computational intelligence of MoGo revealed in Taiwan’s 
computer Go tournaments”, IEEE Transactions on Computational Intelligence and AI in 
Games, Vol. 1, No. 1, 2009, pp. 73-89. (SCI) 
T. P. Hong
[12] C. M. Huang, 
 and Y. L. Wu, “Hiding predictive association rules on 
horizontally distributed data”, Lecture Notes in Artificial Intelligence, Vol. 5579, pp. 133 - 
141, 2009. (EI)  
T. P. Hong
[13] 
 and S. J. Horng, “Discovering mobile users’ moving behaviors in 
wireless networks", Expert Systems with Applications, Vol. 36, No. 8, 2009, pp. 
10809-10814. (SCI) 
T. P. Hong
[14] 
 and Y. Y. Wu, “Mining of up-to-date effective knowledge", Communication of 
IICM (Institute of Information and Computing Machinery), Vol. 12, No. 2, pp. 17-26, 2009. 
T. P. Hon
[15] V. S. Tseng, C. H. Chen, P. C. Huang and 
g, Y. F. Tung, S. L. Wang, M. T. Wu and Y. L. Wu, “An ACS-based framework for 
fuzzy data mining", Expert Systems with Applications, Vol. 36, No. 9, 2009, pp. 
11844-11852. (SCI) 
T. P. Hong
[16] G. C. Lan, 
, “Cluster-based genetic segmentation 
of time series with DWT", Pattern Recognition Letters, Vol. 30, No. 13, 2009, pp. 
1190-1197. (SCI) 
T. P. Hong
[17] 
 and V. S. M. Tseng, "A three-scan mining algorithm for high on-shelf 
utility itemsets", ICIC Express Letters, Vol. 3, No. 4(B), 2009, pp. 1327-1332. (EI) 
T. P. Hong, P. C. Wang and Y. C. Lee, "An effective attribute clustering approach for feature 
sequences on taxonomy", accepted and to appear in International Journal of Computer 
Science and Knowledge Engineering. (EI) 
[35] S. L. Wang, J. D. Chen, P. A. Stirpe and T. P. Hong
[36] C. W. Lin, 
, “Risk-neutral evaluation of information 
security investment on data centers", accepted and to appear in Journal of Intelligent 
Information Systems. (SCI) 
T. P. Hong
[37] B. C. Chien, J. H. Yang and 
 and W. H. Lu, “Efficient modification of fast updated FP-trees based 
on pre-large concepts", accepted and to appear in International Journal of Innovative 
Computing, Information and Control. (SCI) 
T. P. Hong
[38] G. C. Lan, 
, "Learning classifiers based on genetic 
programming and rough sets", accepted and to appear in Journal of Multiple-Valued Logic 
and Soft Computing. (SCI) 
T. P. Hong
[39] C. H. Chen, 
 and V. S. M. Tseng, "On-shelf utility mining with an upper-bound 
measure", accepted and to appear in ICIC Express Letters. (EI) 
T. P. Hong
[40] 
 and V. S. M. Tseng, "Genetic-fuzzy mining with multiple minimum 
supports based on fuzzy clustering", accepted and to appear in Soft Computing. (SCI) 
T. P. Hong
[41] 
, C. Y. Wang and C. W. Lin, "Providing timely updated sequential patterns in 
decision making", accepted and to appear in International Journal of Information 
Technology & Decision Making. (SCI) 
T. P. Hong, W. T. Lin, C. H. Chen and C. S. Ouyang, "Evolutionarily adjusting membership 
functions in Takagi-Sugeno fuzzy systems", accepted and to appear in International 
Journal of Intelligent Information and Database Systems. (EI) 
之學者對所演講之內容甚有興趣，並向我
們索取投影片及尋求未來之可能合作，算
是額外的收穫。 
 
（二）與會心得 
    此次會議場次主要是以智慧型的資訊
應用為主軸，與我的研究領域算是完全契
合，因此有不少的收穫。我將此次會議的
內容分為三大部份。 
 
 智慧型的資訊應用： 
    此部份探討目前常使用的機器學習方
法，包括計算與智慧型系統、多媒體擷取、
圖像及影片編碼的傳送、影像處理與分
析、知識與資訊檢索、人工智慧、機器學
習、資料庫系統及管理的應用、無線感測、
知識挖掘、資料探勘、網路、訊號、類神
經網路、基因演算法、模糊理論和影像及
聲音處理等等，均是目前相當熱門之主題。 
機器學習：包含類神經網路式學習、
資料群聚、遺傳演算法和模糊理論等文
章，主要是用來做分類法之使用和站台網
絡方面等應用。 
資料探勘和知識挖掘：資料挖掘之做
法，主要是要從大型的資料庫中去整理並
且分析資料，希望透過整理分析後之資
料，能夠藉以求出高階總結性之規則作為
決策依據。以較廣義而言，亦可將機器學
習之領域納入考量。就此次會議發表的文
章而言，均是將傳統資料挖掘技術做再更
深入的變化，以符合不同的需求。 
 智慧型節能網路系統： 
此部份主要探討智慧型節能網路系統
之技術開發及電力消耗量與品質提升等技
術的應用，包含監控設備之探討、工業上
之節能監控技術、高科技之空調管理控制
系統、動力設備運轉監控技術等，各方面
所需之電能監控技術發展。 
 
 其它相關技術 
此部份探討和智慧型工程系統間接相
關之技術，包括了智慧型網路運算及類神
經網路的技術、知識與資訊檢索、資訊安
全等議題，部份筆者較感興趣的內容介紹
如下: 
智慧型網路：智慧型網路部份主要探
討分為兩個部分，首先為如何有效地促進
網際網路之效能，並且有效地增廣其應
用，除此之外，很多文章探討如何能夠有
效地搜尋及收集網際網路的資料，雖看似
簡單的內容，其實這對於網際網路發達的
今日資訊社會相當的重要。此領域的文章
包括了使用機器學習的方法去改善並且增
進網路之效能，或是文件分類、網路服務
品質的探討、網路的流量問題及相關實際
系統應用等。  
知識和資訊檢索：此部分主要是利用
研究方法來找尋文獻、資料或數據中所需
資訊之過程，且因個人資訊需求的不同，
所以資訊檢索的對象可以多方面的，例如
圖書、期刊、報紙、專利、學位論文、期
刊論文以及會議論文等之查詢，藉由智慧
型的方式來有效地進行檢索策略，很多方
面的應用都有在此會議中被探討。 
 
此外，筆者此次也與二位專題演講的
學者 Prof. Dr. A Min Tjoa與 Prof. Dr. Leon 
Shyue-Liang Wan 分別有很好的討論互
動， 其中，Prof. Dr. A Min Tjoa的主要領
域為Web Technology相關的研究，而 Prof. 
Dr. Leon Shyue-Liang Wan則同樣為資料
探勘的相關研究，因此，筆者與二位學者
進行很多題目的討論與交流，主要探討
Web Technology、Privacy 與 Data Mining
等相關問題的討論，除此之外，亦包括關
國科會補助計畫衍生研發成果推廣資料表
日期:2010/11/01
國科會補助計畫
計畫名稱: 模糊約略集合論於資料挖掘之應用
計畫主持人: 洪宗貝
計畫編號: 98-2221-E-390-033- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 本人在 2009年獲得 TAAI 最佳碩士論文獎指導教授 
2. 本人在2009年到2010年期間, 共擔任過2個conference的general chair, 
4 個 conference 的 organized session chair, 8 個 conference 的 program 
chair, 3個conference的program track chair, 12個conference的committee 
member, 17 個期刊的 advisory board  member, 5 個 technical committee 
member, 65 個 program committee member, 33 個校外審察委員等 
3. 本人在 2009 年到 2010 年期間, 共擔任過 7 個 keynote session chair, 6
個 session chair, 並組織了 10 個 session 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
