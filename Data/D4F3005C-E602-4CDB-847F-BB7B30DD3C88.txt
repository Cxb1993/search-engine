2 
波器。但是這項假設在日常場景中是
不成立的，這會使得由這些方法所計
算的輸出影像產生諸多的瑕疵，如光
暈（halo）現象、失去顏色（grey wash）、
色調偏移、過度強化、雜訊放大等問
題。 
由前述討論及文獻回顧，顯示直
接將彩色影像分解為照明與反射兩成
分的作法有極大的困難度。相對的，
我們提出利用影像亮暗與準位的自動
調整機制將影像所含的照明成分降至
最低甚或去除，調整完的影像猶如是
在無彩度的（achromatic）標準白色光
源均勻的照明下所取得，使得色彩恆
常性與影像強化的雙重目標能夠同時
達成。演算法的流程如圖1所示，輸入
的彩色影像可以是未預先gamma 校
正的線性灰階值或是已gamma校正的
非線性灰階值，輸入灰階範圍無論是
高動態範圍（high dynamic range， 以
下簡稱HDR）或是低動態範圍（low 
dynamic range，以下簡稱LDR）皆可接
受。首先藉著自動調整輸入影像整體
亮暗的機制，將不同的輸入動態範圍
映射到標準的輸出範圍，此步驟可以
去除均勻照度的影響。接著再進行局
部的亮暗自動調整，不僅可降低非均
勻照明光源的成分，亦可增加偏暗區
與偏亮區的清晰度。從影像強化的觀
點此步驟不僅可使影像整體的明亮度
調整至適當的範圍，亦能將低曝光的
區域調亮及過度曝光的區域調暗，突
顯這些區域的細節。而從色彩恆常性
的角度檢視亮暗調整步驟可將無論是
HDR或是LDR的輸入映射至最佳的輸
出範圍，達成所謂的明度恆常性
（lightness constancy），使得物體的明
度不受照明光源強弱的影響。Land[4]
經多次的實驗證實色彩恆常性是基於
紅、綠、藍三通道的明度恆常性。所
以最後再透過準位調整機制協調三通
道的亮暗調整，使輸出影像之色彩趨
近恆常性並呈現精確的影像外貌
（image apperance）。 
 
圖 1 基於亮暗與準位自動調整色彩恆常性演算架構 
 
    以下先描述單一通道（灰階影像）
的全域與區域亮暗自動調整演算法，
並依據此方法發展出三種基於亮暗與
準位調整的色彩恆常性機制及自動選
擇的準則，最後討論如何架構實驗方
法以便客觀的評估調整後的輸出影像
在色彩恆常性與影像強化兩方面的量
化表現。 
 
2. 建構單一通道的亮暗自動調整機制 
    我們先針對輸入彩色影像的一個
通道或灰階成分說明其全域與區域的
亮暗自動調整架構。 
 
2.1全域亮暗自動調整機制 
    此機制的主要目標是無論輸入影
像是 HDR或是 LDR、整體偏亮或整體
偏暗，都能將它調整到適當的明亮準
位，猶如電子系統的自動增益控制。
我們以描述視網膜上的光感測體響應
公式，稱為Michaelis-Menten function
或 Naka-Rushton function，作為發展本
架構的基礎，其型式如下所示 
輸出 
(校正) 
影像 
4 
起始值為 1。首先定義理想的輸出影像
亮度平均值為 
nideal k
I
+
=
1
1  （6） 
而輸出影像的實際平均值為 
∑=
yx
gloreal yxIN
I
,
),(1  （7） 
其中 N為影像像素總數。設定實際平
均值與理想平均值的差異比例為 
ideal
idealreal
I
IIratio −=  （8） 
調整參數 k值的增減直到上式之差異
比例小於門檻值，或遞迴到 k值的上下
限值，k ∈ [1,10]。將求得的參數代入
公式（1）即可獲得輸出影像 
𝐼𝑔𝑙𝑜�𝑥，𝑦�，由於是先求得 n值，再以
此值計算出 k值，而兩參數是互相影響
的，因此需要將輸出影像𝐼𝑔𝑙𝑜�𝑥，𝑦�透
過下式進行微調 
𝐼𝑔(𝑥, 𝑦) = 𝐷 ∙ 𝐼𝑔𝑙𝑜(𝑥,𝑦)1 + 𝐷 − 𝐼𝑔𝑙𝑜(𝑥, 𝑦) （9） 
其中 
𝐷 = �1 − 𝐼𝑟𝑒𝑎𝑙�𝐾 （10） 
參數K在後續的色彩恆常性機制中決
定其值。最後經上述全域亮暗調整的
輸出影像𝐼𝑔(𝑥,𝑦)會比先前調整的影像
𝐼𝑔𝑙𝑜(𝑥,𝑦)略暗，因此不會產生過亮的情
形，同時也更能配合後續的區域亮暗
調整。 
 
2.2區域亮暗自動調整機制 
經過全域亮暗適應機制處理後，
影像的整體明亮度已調整到一個適當
的準位，但是仍會有一些區域細部特
徵無法凸顯出來，所以需再經過區域
性亮暗調整機制之處理，以期望達到
更佳的影像品質。此機制之基本觀念
是每一像素之灰階值會受到鄰近像素
的平均灰階之影響而調整其大小。通
常是極暗區的像素灰階會獲得提升，
相對的，極亮區的像素灰階則會遭到
下降其公式如下 
𝐼(𝑥, 𝑦) = �𝐼𝑔(𝑥,𝑦)�𝑚(𝑥,𝑦)
𝑎(𝑥, 𝑦)�𝐼𝑔(𝑥,𝑦)�𝑚(𝑥,𝑦) + 𝑏(𝑥, 𝑦) （11） 
其中𝐼𝑔�𝑥，𝑦�為經全域亮暗調整後之
影像，並將其最大值正規化為 1。圖 3
描述此公式所代表的各種曲線，橫坐
標代表輸入灰階，縱座標為輸出灰階
圖中的三條曲線分別代表 a > 0的提
升明亮度， a < 0 的降低明亮度，及 a 
= 0 的不調整明亮度。a，b，m代表 
[ ]5.0)0;,(5.05.11),( −⋅−+= yxIIyxm lpg
 
（12） 
( ) ( ) ( )( ) ( )[ ]yxIIyxI
yxIyxI
yxa
avav
avav
,,
,,
,
1max2
12
+
−
=  （13） 
[ ]
[ ]),(),(
),(),(
),(
1max2
2max1
yxIIyxI
yxIIyxI
yxb
avav
avav
+
+
=  （14） 
其中 1max =I ， 1avI 與 2avI 為曲線參數定
義為： 
( ) ( ) 01 ,, cyxIyxI plpav +=  （15） 
( ) ( )[ ] 0max2 ,, cyxIIyxI
p
lpav +−=  （16） 
其中 lpI 為影像經高斯濾波器之輸出低
頻影像，p為控制各像素點的改變亮暗
之權重， 0c 定義最小適應準位， gI 為
輸入影像之平均值。 
6 
能達到不錯的色彩恆常性表現。當然
如何判斷就是重點研究課題。 
有別於傳統色彩恆常性方法將灰
色世界假設或白區假設直接套用於估
測照明光源，我們採取將這些假設軟
性植入到三通道的全域與區域亮暗調
整機制。以下提出三種色彩恆常性機
制，分別是保持原始輸入色調的機制，
基於灰色世界的色彩恆常性機制及基
於白區的色彩恆常性機制。我們將從
全域、區域及準位三個循序調整步驟
說明每一種機制。 
 
3.1 保持原始輸入色調的機制 
    此機制主要是針對具有單一種支
配色的輸入影像進行調校，因為輸入
並非偏色影像，因此調整的目標是維
持原來的色彩比例。首先取出輸入影
像𝐼𝑖�𝑥，𝑦�, 𝑖 ∈ (𝑅,𝐺,𝐵)，的灰階成分，
令其為 
( ) ( )∑=
i
i x,yIx,yI 3
1  （20） 
依照前述的亮暗調整機制對此灰階影
像進行調整，其中在全域亮暗最後微
調過程中使用的參數 K由下式求得 
( )( )realglo
glo
II
I
K
−−
−
=
15.0
)1(5.0
max,
max,  （21） 
其中𝐼𝑔𝑙𝑜,𝑚𝑎𝑥為𝐼𝑔𝑙𝑜(𝑥,𝑦)之最大值。最
後的準位調整是依據調整前後的亮度
比例回復原始輸入影像的色度，我們
提出如下的log比例守衡原則: 
𝑙𝑜𝑔 𝐿𝑖(𝑥, 𝑦)
𝑙𝑜𝑔 𝐼𝑖(𝑥, 𝑦) = 𝑙𝑜𝑔𝐼(𝑥, 𝑦)𝑙𝑜𝑔𝐼(𝑥, 𝑦)    , 𝑖𝜖(𝑅,𝐺,𝐵) (22) 
其中𝐿𝑖(𝑥, 𝑦)為輸出校正影像， 𝐼𝑖(𝑥,𝑦)
為原始彩色影像，而𝐼(𝑥,𝑦)則是其灰階
影像， 𝐼(𝑥,𝑦)是經亮暗調整完的灰階
影像。所以最後的輸出影像𝐿𝑖(𝑥, 𝑦)可
由上式經指數 exp計算獲得。 
 
3.2基於灰色色彩恆常性機制 
本機制的主要目標是使影像三通
道的平均值互相拉近，其演算法如下: 
 
A. 全域亮暗自動調整 
影像 R、G、B三通道各自獨立調
整其輸出範圍，所以公式（1）修
正為 
𝐼𝑔𝑙𝑜𝑖 = [𝐼𝑖(𝑥,𝑦)]𝑛[𝐼𝑖(𝑥,𝑦)]𝑛 + (𝑘𝐼𝑖)𝑛 , 𝑖 ∈ (𝑅,𝐺,𝐵) (23) 
其中參數𝑛求得方式是將影像三通道
視為一體，找出其最大值與排零幾何
平均值並將兩值代入公式（4），而參
數𝑘則是 
𝑘 = min{𝑘𝑖, 𝑖 ∈ (𝑅,𝐺,𝐵)} (24) 
其中𝑘𝑖是由紅、綠、藍三通道各自求得
的𝑘參數。公式（23）代表三條不同的
轉換曲線，並可表式為 
( )
( )
( ) n
n
i
i
n
i
i
glo
k
I
x,yI
I
x,yI
x,yI
i
+











=  (25) 
由於每一通道的輸入皆被該通道的平
均值正規化，所以上式可視為灰色世
界假設的軟性植入。最後全域亮暗調
整機制的輸出由下式求出 
𝐼𝑔𝑖(𝑥, 𝑦) = 𝐷𝑖 ∙ 𝐼𝑔𝑙𝑜𝑖(𝑥, 𝑦)1 + 𝐷𝑖 − 𝐼𝑔𝑙𝑜𝑖(𝑥, 𝑦) (26) 
其中 
𝐷𝑖 =（1 − 𝐼𝑟𝑒𝑎𝑙𝑖）𝐾 (27) 
上式的𝐼𝑟𝑒𝑎𝑙𝑖為𝐼𝑔𝑙𝑜𝑖(𝑥,𝑦)的平均值。  
8 
可粗略的描述輸入影像有那些主要顏
色，接著利用已知真實照明光源成分
的影像資料庫測試每一種色彩恆常性
機制的正確性，最後依據估測誤差與
統計指標建立選擇最佳色彩恆常性機
制的選擇方法。表 1為三種色彩恆常
性機制的特性。 
 
 
 
表 1 三種色彩恆常性機制的特性 
機制 特性 
保留原始色
調 
適合有支配色，其中一或二
個通道平均值很大，機制的
目的是以不改變通道顏色比
例的方式提升影像亮度並突
顯細節特徵。  
基於灰色世
界 
適合在有很強的偏色背景，
使得一或二通道平均值很
大，但又不屬於支配色，機
制的目的是拉近三通道的平
均值。  
基於白區 
適合有白色區塊的影像，機
制的目的是將三通道最大的
反射比調整相同。  
    自動選擇機制如圖 4所示，我們
利用輸入影像各通道的幾何平均值與
標準差進行選擇。首先需判別輸入影
像是否含有單一色調的支配色，如果
有則屬該顏色的幾何平均值遠大於其
他通道，同時該通道的標準差會很小。
當輸入影像三通道的幾何平均值，其
最大值與中間值的比例大於 3，而且最
大值通道的標準差小於 1.2，則表示影
像含有紅色、綠色或藍色的支配色。
另外，當最大幾何平均值與中間幾何
平均值的比例雖然小於 3，但此兩通道
的標準差之和小於 1.2時，表示影像含
有黃色、紫色或是青色的支配色。只
要有支配色，即選擇保留輸入影像原
始色調的機制。 
  如果輸入影像未含單一支配色，此
時檢查最大幾何平均值與最小幾何平
均值的比例是否大於 3，如果是，則表
示影像具有很強的照明色偏，需使用
基於灰色世界的機制進行顏色校正，
如果該比例小於 3，則檢查最小與最大
標準差的比值是否接近 1，如果是，表
示符合灰色世界的假設，如果不是，
則使用基於白區的機制進行色彩恆常
性的校正。 
 
圖 4 三種色彩恆常性選擇流程圖 
 
3.4估測原始光源色調 
    我們稱色彩恆常性機制的輸出影
像為校正影像，表示在標準白光照明
設定下所調校的結果，通常勿需再估
10 
估測誤差分佈的平均值，而表 3則是
誤差分佈的中值。表中也顯示使用傳
統的灰色世界法與白區法結果。另外
圖 6描述本機制的自動選擇恆常性方
法與傳統方法的平均誤差比較，而圖 7
則是描述本機制的三種恆常性機制與
自動選擇的平均誤差結果的比較。從
圖表中我們有以下的觀察： 
(1) 資料庫 1的誤差相較於其它資料庫
的誤差是較大的，原因是資料庫 1
為未 gamma校正的線性影像。 
(2) 使用平均值評估實驗結果與使用
中間值之結果並不一致，這指出評
估準則仍有改善的空間。 
(3) 雖然本機制的自動選擇法比傳統
方法表現好，但是選擇法並未能在
每一資料庫都達成最小誤差，因此
我們提出的自動選擇方法仍有改
進的必要。 
(4) 圖 8（a）是有色偏的原始影像，圖
8（b）是經真實光源校正後的影像，
稱為標準影像，值得注意的是背景
照明竟然校正為偏藍的色調，主要
原因是原始影像是多照明光源影
像，而所謂的真實光源乃是專指照
射到灰球區域的光源色調，所以以
此處光源校正整張影像才會產生
前述的現象。而圖 8（c）是本機制
產生的結果，此校正影像不但在視
覺上是在白光照明的成像結果，其
明亮度與清晰度亦遠優於圖 8（b）
的標準影像。這也提示我們需要發
展以校正影像為基礎度量其與白
光照明間的差異。 
 
 
 
 
表 2 各資料庫的原始光源估測「平均」誤差 
 
 
表 3 各資料庫的原始光源估測「中值」誤差 
 
 
 
圖 6 本計畫自動選擇機制與傳統方法的「平均」
誤差比較 
 
圖 7 本計畫的三種恆常性機制與自動選擇的 
「平均」誤差比較 
 
 
 
0
5
10
15
自動選擇
機制
傳統灰色
世界法
傳統白區
法
0
5
10
15 自動選擇機制
基於灰色世界
法
基於白色世界
法
保留原始色調
法
12 
[8]K. Barnard, L. Martin, A. Coath, and Brian Funt, 
“A comparison of computational color constancy 
algorithms part II: Experiments with image data,” 
IEEE Transactions on Image Processing, vol. 11, no. 9, 
pp. 985–996, 2002. 
[9]J. van de Weijer, T. Gevers, and A. Gijsenij, 
“Edge-based color constancy,” IEEE Transactions on 
Image Processing, vol. 16, no. 9, pp. 2207-2214, 
2007. 
[10]A. Gijsenij, T. Gevers , and J. v. de Weijer, 
“Generalized gamut mapping using image derivative 
structures for color constancy,” International Journal 
of Computer Vision, vol. 86, no. 2-3, pp. 127-139, 
2008. 
[11]M. Ebner, “Color constancy based on local space 
average color,” Machine Vision and Applications, vol. 
20, no. 5, pp.283-301, 2009. 
[12]G. Arjan  and T. Gevers, "Color constancy using 
natural image statistics,” in Computer Vision and 
Pattern Recognition, CVPR ’07, pp. 1-8, June 2007. 
[13]F. Gasparini and R. Schettini, “Color balancing of 
digital photos using simple image statistics,” Pattern 
Recognition, vol. 37, no. 6, 1201–1217, 2004. 
[14]S. Bianco, G. Ciocca, C. Cusano, and R. Schettini, 
“Improving color constancy using indoor–outdoor 
image classification,” IEEE Transactions on Image 
Processing, vol. 17, no. 12, pp. 2381-2392, 2008. 
 
 
 
 
 
 
 
 
 
 
 
[15]K. Barnard, L. Martin, B. Funt, and A. Coath, “A 
data set for color research”. Color Research and 
Application, vol. 27, no. 3, pp. 148-152, 2002. 
 [16]P. V. Gehler , C. Rother, A. Blake, T. Minka , and 
T. Sharp, “Bayesian color constancy revisited,” in 
Computer Vision and Pattern Recognition, CVPR ’08, 
pp. 1-8, June 2008. 
[17]A. Chakrabarti, K. Hirakawa, and T. Zickler, 
“Computational color constancy with spatial 
correlations,” Tech. Rep. TR-09-10, Harvard Sch. of 
Eng. & App. Sc., 2010. 
[18]F. Ciurea and B. Funt, “A large image database 
for color constancy research,” in Proc. IS&T/SID 11th 
Color Imaging Conference, pp.160-164, 2003. 
[19]C. Hsin, J. W. Jang, S. J. Shin, and S. H. Chen, “A 
no-reference objective image sharpness metric based 
on a filter bank of gaussian derivative wavelets,” in 
the 2nd International Conference on Multimedia 
Technology (ICMT2011), pp. 3362-3365, Hangzhou, 
China, July 26-28, 2011. 
[20]C. Hsin, H. N. Le, and S. J. Shin, “Color to 
grayscale transform preserving natural order of hues,” 
in the 2011 International Conference on Electrical 
Engineering and Informatics, H10-2, Bandung, 
Indonesia, July17-19, 2011. 
 
 
 
 
 
 
 
 
 
 
 
餘篇文章。論文發表以口頭與壁報兩種方式進行，總計有 63場會議節次(session)。 
     大會安排五位國際知名學者進行 keynote speech。來自於意大利的 Montanari教
授首先針對智慧電網描述全球監控機制，其議題為”Global Monitoring :The Paradigm 
for Asset Management in the Smart Grid Framework”，第二位 keynote演講者是馬來西亞
學者Hamdan教授，其講題為”MYNDA: An IDSS Generator with Hyperheuristic Attribute 
Reduction”，主要說明其以網頁為基礎的MYNDA 資料探勘架構可自動的產生所需的
知識模式。接下來的 keynote演講者是來自於印尼的 Bambang教授，其講題為”Recent 
Progress in Adaptive Nonlinear Active Noise Control”，主要發表其最近在雜訊的主動控
制研究成果。第四位keynote演講者是來自於日本的Hikita教授，其講題為”Fundamental 
Principles and Application of Diagnosis for GIS using Partial Discharge Measurements”，主
要描述針對斷路器與變壓器，利用 UHF 技術量測診斷其部份放電現象。最後一位
keynote 演講者是印尼學者 Supriana 教授，其講題是”Direct skeleton Extraction using 
River-Lake Algorithm”，主要是集結過去在影像處理領域所發展的 200 餘種骨架
(skeleton)擷取技術，分析其優缺點，並提出一套新的演算法。 
本人在 ICEEEI 2011所發表的論文內容主要是發展一個兩階段的彩色影像轉換成
灰階影像的演算方法。由此方法所產生的灰階影像可維持原彩色影像的彩色對比與
明度(lightness)。演算法的第一階段是透過一個保留色度自然排序的全域映射機制將
色採影像的每一像素紅、綠、藍三值轉換為適當的灰階值，此映射機制不僅簡單快
速且能保持原有特徵的分辨性與顏色的自然排序性。演算法的第二階段則是經由調
  
 
                                 
 
 
(compressive sensing)機制的視訊編碼系統，使用物件偵測辨識方位的系統等。本
人對色彩恆常性與色彩特徵的研究課題特別感興趣，它不但可以應用到數位照相
機，亦可應用於視訊產業，可是此次 ICEEI 2011著墨於這些課題的論文非常少，
實屬可惜。不過從所聆聽與觀看的論文了解系統整合，學習機制及辨識應用是目
前電腦科學領域的熱門研究主題，此次的研討會給予本人目前正研究的主題很多
重要的啟發及未來方向的設定，收穫豐碩。 
 
三、攜回資料名稱及內容 
此次會議結束後，本人帶回 ICEEI 2011論文集光碟一片、議程手冊及個人筆
記。本人非常感謝國科會補助本人參加此次研討會。在會議中認識了來自各國的
學者，經由討論與意見交換，使我受益良多，同時也埋下日後進一步合作的種子。 
 
2011 International Conference on Electrical Engineering and Informatics 
17-19 July 2011, Bandung, Indonesia 
 
 
Color to Grayscale Transform Preserving  
Natural Order of Hues 
Chengho Hsin*, Hoai-Nam Le, and Shaw-Jyh Shin 
Department of Communications Engineering, Feng Chia University, Taichung, Taiwan 
*chhsin@fcu.edu.tw 
 
Abstract - Color-to-gray conversion in many applications re-
quires preserving both the visual appearance and feature dis-
crimination of color images. The existing methods are still need 
of improvement in these aspects. A two-stage color-to-gray trans-
form algorithm is proposed in this paper. It produces a grayscale 
image that matches the color contrast and the lightness of a given 
input color imag1e. The first stage process is a functional map-
ping scheme based on the natural order of hues. The mapping is 
simple and fast, yet it preserves feature discrimination and ap-
propriate color order of color images. The second stage of the 
proposed algorithm recovers the lightness of a color image 
through adjusting the local contrast of the first-stage output. It 
not only retains the visual appearance of the color image but also 
compensates the functional mapping for possible loss of color 
information. The proposed method could accurately convert 
various categories of color images to grayscale images, including 
natural images, computational images, and painting images.  
Experimental results show that our method produced percep-
tually accurate and preferred images when they were compared 
with other schemes. 
 
Keywords - Color-to-gray conversion, visual appearance 
I. INTRODUCTION 
Many applications require converting a color image into a 
grayscale representation, such as for reproducing on monoch-
rome devices, for subsequent image analysis, and for aesthetic 
intents. Color-to-grayscale conversion performs a reduction of 
the three-dimensional color data into a single dimension. The 
loss of information is inevitable during the conversion. Hence, 
the goal of the color-to-grayscale conversion is to retain as 
much information from the original color image as possible. 
At the same time, the aim is also to produce the grayscale im-
age with plausible visual appearance. Unfortunately, informa-
tion of isoluminant regions of a color image may be lost in its 
converted grayscale image when using the luminance channel 
(e.g., CIE L*) of a traditional color space. Fig. 1 shows an 
example of three grayscale images converted from an isolu-
minant color image. The lightness component L* of CIE 
L*a*b* color space [1] loses the color information completely. 
The luma component Y’ of NTSC color space captures partial 
color contents. In contrast with these two color spaces, the 
grayscale image obtained by our method preserves all infor-
mation of the color image. 
To retain feature discrimination in color-to-gray conver-
sion, color differences in the input image should be reflected 
 
 
Original Color Image CIE L* 
 
NTSC Y’ The Proposed Method 
Fig. 1 Our method maps isoluminant colors to unique and appropriately or-
dered grayscales. 
as much as possible onto the converted grayscale values. 
Many color-to-gray conversion methods have been proposed, 
which can be divided into two main categories: local mapping 
and global mapping [2].  In a local mapping method, the col-
or-to-gray mapping of pixel values is spatially varying, de-
pending on the local distributions of colors. Although a local 
mapping has advantages in accurately preserving local fea-
tures, but the constancy of color regions could be converted to 
inconstancy if the mapping could not produce same color val-
ues to the same grayscale values. In a global mapping method, 
the same color-to-gray mapping is used for all pixels in the 
input. It consistently maps the same colors to the same grays-
cale values over an image. However, it would be more chal-
lenging to require a global mapping that preserves local fea-
tures at different locations at the same time. 
In this paper, we present a two-stage color-to-gray conver-
sion algorithm that incorporates the merits of the global and 
local mapping schemes. The first stage of the proposed me-
thod performs a global functional mapping that preserves the 
natural hue order of the input color image. The second stage 
of our method is a local mapping scheme and its main objec-
tive is to make up for the possible loss of local feature dis-
crimination and visual appearance in the global mapped 
grayscale image. Experimental results show that the proposed 
method can successfully reproduce the visual appearance of 
color images. Even the isoluminannt color images can be con-
verted to the grayscale images with proper color order and 
H10 - 2
978-1-4577-0751-3/11/$26.00 ©2011 IEEE
  
To attain global consistency, we adopt a global mapping 
scheme for color to grayscale conversion. In order to retain 
the maximum information of a color input image, we need to 
map not only intensity values of the color image to the grays-
cale resultant image, but also include the hue and saturation of 
color components in each pixel. The goal of this scheme is to 
preserve all details of a color image in its mapped grayscale 
image. Even some details that could not be observed clearly in 
the original color images are also perceptible in our grayscale 
images. This global mapping of a gamma-corrected RGB im-
age is specified by a simple functional form: 
  ),(),(),( yxCyxIyxG ww λ+=                          (1) 
where ),( yxI w represents a weighted intensity component, 
),( yxCw denotes a weighted chrominance component, and λ is 
a parameter to control the amount of chromatic variations ap-
plied to the intensity value. The weighted intensity is given by 
            BwGwRwyxI BGRw ++=),(                             (2) 
where ,, GR ww and Bw are weighting coefficients. The weighted 
chrominance is defined by 
),(),()),((),( yxIyxSyxHyxC www θ=            (3) 
where ),( yxθ and ),( yxS are the hue angle and saturation of 
the color value at a pixel ),( yx in HSI color space respective-
ly. The hue mapping function )(⋅wH is specified by 
( ) 5.0),(cos5.0)),(( ++= βθθ yxyxH w                  (4) 
where the parameter β determines the order of hues. Let 
GR θθ , , and Bθ be the hue angles of the primary colors R, G, 
and B, respectively. Let CY θθ , , Mθ be the hue angles of the 
complementary colors of R, G, and B, which are yellow, cyan, 
and magenta. Since the warmest hue is usually a red. The 
coolest color is the complementary color to the warmest hue, 
which is a cyan. The natural order of hues is described by the 
order of warm and cool colors. In terms of the hue mapping 
function, the natural order of hues is preserved by the follow-
ing constraint 
).()()()()()( CHBHGHMHYHRH wwwwww >>>>>     (5) 
Define signed distance from hue A to hue B by
)()( BHAHD wwAB −= . The parameter β  is derived by max-
imizing the distances of the six color pairs as follows: 
( ){ }RCMGYBRBGBRG DDDDDD ,,,,,ofminimummaxβ        (6) 
We obtained the optimal D344=β . Conforming to the natural 
order of hues, the intensity weighting coefficients are assigned 
to the following values: 
                 
6
1and,
3
1,
2
1
=== BGR www                        (7) 
The third criterion of feature preservation is achieved by de-
termining an optimal λ that decides the appropriate amount of 
chrominance component added to the weighted intensity val-
ue. Hence, both dimished details and exaggerated feature dis-
criminability are avoided in the resulted grayscale image.   
Fig. 2 shows the influence of the parameter λ .  
 
 
  
Color Image 0=λ  1=λ  2=λ  3=λ  1.6=λ  
Fig. 2 Varying the parameter λ  
 
Color images include natural images, painting images, and 
computational images. A fixed value of the parameter λ  for 
all kinds of images is incapable of producing satisfied grays-
cale images. Hence, the parameter λ  should be chosen opti-
mally and automatically. 
We propose to determine an optimal λ  by maximizing 
the standard deviation (STD) of the Sobel gradient of ),( yxG
in order to preserve the maximum information of color im-
ages. The range of the parameter λ  is confined to [0, 
0.5/maxCw], where maxCw is the maximum value of Cw. The 
STD of the gradient of the grayscale image ),( yxG is able to 
represent the feature discriminability of the image.  However, 
Color images frequently contain gray regions where the red, 
green, and blue values of pixels are equal.  These gray areas 
may affect the correct measurement of feature discriminability 
by the STD of the gradient of the grayscale image ),( yxG . 
Fig. 3 shows an example of a color image that has a white 
background. The determined optimal λ is zero. The resulted 
grayscale image is shown in Fig. 3(b). If the parameter λ  is 
gradually increased in ),( yxG , the differences between the 
white background and the disks (the green and blue disks in 
the color image) are decreasing because the chrominance 
component Cw is add to these disks only. Hence, the gradient 
of the image ),( yxG is also diminishing. We are unable to 
clearly discriminate the color regions by evaluating the STD 
of the gradient of the image ),( yxG in this example. The solu-
tion to this problem will be addressed below. 
  
(a) Color Image (b) Grayscale Image 
Fig. 3 An example of a color image has a white background. The optimal λ is 
zero. 
Create a color image ''' BGR by setting the gray pixels (R, 
G, and B are equal) of the input color image RGB to zero. 
Compute a temporary weighted intensity image, ),( yxT , as 
                             '
6
1'
3
1'
2
1),( BGRyxT ++=                                (8) 
By adding the chrominance component to this temporary 
weighted intensity, we obtain a temporary grayscale image as 
follows:   
),(),(),( yxCyxTyxB wλ+=                       (9) 
  
thors surveyed nearly 20000 human responses and used them 
to evaluate the accuracy and preference of the color-to-
grayscale conversions.  
The proposed method was implemented by using MAT-
LAB 7.1 software which runs on Pentium-IV 3.0 GHz CPU 
with 2.0 GB main memory. The same twenty four color im-
ages in categories of paintings, natural images, and computa-
tional images used in [3] were applied to evaluate the perfor-
mance of the proposed method. Due to the limit of the space, 
only eight test images along with their conversion results are 
shown in Fig. 5. Test images and resultant images of other 
methods are courtesy of Čadík [3].  
The perceptual accuracy is the level of the closeness of the 
grayscale image and the original color image in appearance 
based on the criteria of the global consistency, the features 
preservation, and the ordering preservation. We compare the 
results of the proposed methods with both the best accuracy 
resultant images in Čadík’s paper [3] and the results of Kim et 
al. [4], as shown in Fig. 5. We could examine clearly in that 
the visual appearances of our results are better than the ones 
of other methods in most test images. For example, color or-
dering of Image 3 was maintained by the proposed method as 
shown in Fig. 5. The black boats are darker than the blue trees 
in Image 3. This visual appearance is reproduced faithfully by 
the proposed method. Furthermore, the sun is discriminated 
distinctively in our grayscale results. 
VI. CONCLUSIONS 
In this paper, we proposed a two-stage scheme to convert 
color images to grayscale images that can preserve maximum 
information of color images. Our algorithm not only is effec-
tive at preserving feature discrimination but also gives proper 
perceptual hue order of many kinds of image, e.g., painting 
images, computational images, and natural images, etc. These 
images may have various dynamic ranges of chrominance and 
luminance changes or they may contain isoluminant regions. 
ACKNOWLEDGEMENT 
This work has been supported by the National Science 
Council under Grant No. NSC 99-2221-E-0.35-091. 
REFERENCES 
[1] M. Fairchild, Color Appearance Models, Wiley, 2005. 
[2] K. Smith, P. Landes, J. Thollot, and K. Myszkowsky, “Apparent grey-
scale: A simple and fast conversion to perceptually accurate images and 
video,” Computer Graphics Forum (Proc. Euro graphics 2008) 27, 2, 
193–200. 
[3] M. Čadík, “Perceptual evaluation of color-to-grayscale image conver-
sions,” Computer Graphics Forum (Proc. Pacific Graphics 2008) 27, 7, 
1745–1754. 
[4] Y. Kim, C. Jang, J. Demouth, and S. Lee, “Robust color-to-gray via 
nonlinear global mapping,” ACM Transactions on Graphics (ACM SIG-
GRAPH Asia 2009), vol 28, no. 5.  
[5] R. Bala, and R. Eschbach, “Spatial color-to-grayscale transform preserv-
ing chrominance edge information,” in Proc. Color Imaging Conference 
2004, 82–86. 
[6] A. Gooch, S. Olsen, J. Tumblin, and B. Gooch, “Color2gray: salience-
preserving color removal,” ACM Trans. Graphics (Proc. SIGGRAPH 
2005) 24, 3, 634–639. 
[7] K. Rasche, R. Geist, and J. Westall, “Re-coloring images for gamuts of 
lower dimension,” Computer Graphics Forum (Proc. Euro graphics 2005) 
24, 3, 423–432. 
[8] L. Neumann, M. Čadík, and A. Nemcsics, “An efficient perception-based 
adaptive color to gray transformation,” In Proc. Computational Aesthet-
ics 2007, 73– 80. 
[9] A. Nemcsics, “Recent experiments investigating the harmony interval 
based colour space of the coloroid colour system,” in AIC 9th Congress 
Rochester, 2001 
[10] M Grundland, and N. Dodgson, “Decolorize: Fast, contrast enhancing, 
color to grayscale conversion,” Pattern Recognition 40, 11, 2891–2896, 
2007. 
[11] Y. Nayatani, “Simple estimation methods for the Helmholtz Kohlrausch 
effect,” Color Research and App. 22, 6, 385–401, 1997. 
 
 
  
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期：100年 08月 5 日 
計畫編號 NSC 99-2221-E035-091 
計畫名稱 
達成精確的彩色影像外貌：建構一個融合影像強化與色彩恆常性的演算法 
Achieving Accurate Color Image Appearance through Fusion of Image 
Enhancement and Color Constancy 
出國人員
姓名 
辛正和 
服務機構
及職稱 
逢甲大學 
   副教授 
會議時間 
100年 7月 26日
至 
100年 7月 28日 
會議地點 
中國大陸杭州市 
會議名稱 
(中文) 第二屆多媒體科技國際學術會議 
(英文) The 2nd International Conference on Multimedia Technology 
(ICMT2011) 
發表論文
題目 
(中文) 基於高斯微分小波濾波器組的非參考式客觀影像銳利度量測機制 
(英文) A No-Reference Objective Image Sharpness Metric based on a Filter Bank of 
Gaussian Derivative Wavelets 
一、參加會議經過 
ICMT 2011多媒體科技國際學術會議今年在中國大陸杭州市舉行，會議從 7月 26
日開始 7月 28日結束，為期三天。研討會主題涵蓋多媒體工具與系統、影像處理技
術、網路應用及多媒體娛樂服務等。會議由 IEEE分會主辦，協辦單位包括美國的
University of Louisville，Georgia State University、中國大陸的寧波大學、浙江理工大
學、中國電信大學、南京大學、上海交通大學及北京理工學院等。會議的宗旨是吸引
全球優秀的學者專家及工程師發表其最新研究結果，並能相互交流與建立合作平台，
研究相關的演講內容作一簡要介紹。第一位講者是來自美國 UCLA大學的
Terzopoulos教授，目前是 ACM、IEEE與加拿大皇家社團的會士，主要研究領域
為電腦圖學，電腦視覺、醫學影像、電腦輔助設計及人工智慧等領域，曾獲得多
次最佳論文引用獎。這次的演講主題是虛擬實景的電腦視覺，講者首先說明很多
的電腦視覺系統受限於實際環境的限制或實驗規模過於龐大，無法進行實際的實
驗，因此倡議在虛擬實景中測試電腦視覺系統的表現及可行性。例如在大規模的
分佈式智慧攝影機網路或大型智慧型監控環境的應用，直接測試所要發展的電腦
視覺演算架構是有困難的，而如果能透過電腦繪圖技術先建置虛擬的測試環境，
此困難便能迎刃而解。但問題是如何證明在虛擬環境中可行的演算架構能夠在真
實環境中也是成功的，便是一個爭論點也是一個重要課題，這個概念非常有趣且
富有創意，相信可在電腦視覺領域裏開創一個新的研究分支。第二位講者是來自
上海交通大學的楊教授，演講主題是三維視覺化技術與應用，主要介紹在影像分
割、影像定位及三維顯像等方面的改善技術，並將其應用於外科手術的三維視覺
化，三維人臉的重建與辨識及網際網路的電子商務。第三位講者是來自美國 
University of Louisville的 Farag教授，演講主題是拓撲定位(topological registration)
的先進方法與應用，首先針對拓撲定位方法進行基本與整體的介紹，並說明其實
驗室目前在此方面的研究結果以及其在生醫成像、生物特徵辨識及其它資料結構
的應用。第四位講者是來自於英國 Loughborough University的 Schaefer教授，其
演講主題是瀏覽大型影像資料庫的方法，核心觀念是如何自動的將相似的影像集
                                 
 
 
三、攜回資料名稱及內容 
此次會議結束後，本人攜回 ICMT2011論文集光碟一片，大會議程手冊及個人筆
記。本人非常感謝國科會與逢甲大學分別補助本人參加此次研討會。在會議中認
識了來自各國的學者，經由討論與意見交流，使我受益良多，同時也埋下日後進
一步合作的種子。 
 
 A No-Reference Objective Image Sharpness Metric 
based on a Filter Bank of Gaussian Derivative 
Wavelets 
 
Chengho Hsin*, Jr-Wei Jang, Shaw-Jyh Shin, Shin-Hsien Chen 
Department of Communications Engineering  
Feng Chia University 
Taichung, Taiwan 
chhsin@fcu.edu.tw* 
 
 
Abstract—The quality of an image depends on various attributes 
such as sharpness (or blurriness), naturalness, colorfulness, and 
contrast, etc. To develop a so-called no-reference objective image 
quality metric by incorporating all attributes of images without 
referring to the original ones is a difficult task. Hence, the 
purpose of this paper focuses on the development of a no-
reference objective image sharpness metric. A high-frequency 
component preserving the full information of a given input image 
is extracted first and then applied to a multi-channel filter bank. 
The proposed method measures image sharpness based on the 
output of this filter bank. A snug frame composed of the 
Gaussian derivative wavelets is applied to construct the filter 
bank. The output of the filter bank not only contains the 
complete information of the input but also manifests prominent 
image features. Experimental results show that the metric 
predicts well in isotropic and uniform blur case. Validated by the 
high correlation between the metric values and the subjective test 
scores, the performance of the proposed metric is comparable to 
that of human subjects. 
Keywords- sharpness metric; blurriness metric 
I.  INTRODUCTION 
Image quality assessment is indispensable to various image 
processing applications such as compression, transmission, 
enhancement, restoration, printing, display, and analysis. 
Although subjective quality assessment is considered to be the 
most accurate and reliable approach it is expensive and time-
consuming and is inappropriate for real-time implementation.  
The development of objective metrics that can precisely 
predict the perceived image quality is in great demand. 
Objective metrics can be categorized into full-reference, 
reduced reference, and no-reference. A reference such as the 
original image is needed for comparison with the processed 
image in the full-reference scheme. Reduced-reference 
objective metrics only require partial information about the 
original image. In many circumstances, the reference image is 
not available for the assessment task. Hence, objective metrics 
using the reference image impose a limitation on their 
applications. On the other hand, a no-reference metric scheme 
computes the perceived visual quality directly from a given 
image without referring to the reference image. It is much more 
useful than the other two categories. Image quality is governed 
by a variety of factors such as sharpness, naturalness, 
colorfulness, contrast, and noise etc. We shall concentrate on 
the work of the no-reference image sharpness (or blurriness) 
metric. Note that the image sharpness metric can also be 
applied to measure blurriness since they are inversely related. 
A general review of no-reference objective image 
sharpness/blurriness metrics were given in [1]. Only recent 
methods with high performance results [1, 2] are described 
here. Metrics [1-3] rely on the determination of the edge width 
to evaluate the amount of blur. These methods first find edge 
pixels using common edge detectors. The start and end 
positions of the edge for each edge pixel are found. The edge 
width is computed as the distance between the end and start 
positions. Marziliano et al. [3] have derived the overall blur 
metric as the average widths of the edges. Ferzli and Karam [1] 
pointed out that the previous method is unable to predict the 
relative amount of blurriness in images with different content. 
Hence, they proposed an image sharpness metric based on the 
notion of just noticeable blur (JNB). The JNB is the minimum 
amount of perceived blurriness around an edge given a contrast 
higher than the just noticeable difference. The probability of 
detecting a blur distortion is determined by the edge width soft 
thresholded with the corresponding JNB. The block-based 
probability summation model is adopted to compute the 
sharpness metric. It was shown in [2] that the results of the 
JNB method do not correlate well with subjective scores for 
images with non-uniform saliency content. To solve this 
problem, Narvekar and Karam [2] developed a sharpness 
metric based on a cumulative probability of blur detection 
(CPBD) to improve the JNB method. In contrast with the edge-
base approach, Vu and Chandler [4] proposed a block-based 
algorithm, named 3S metric, designed to measure the local 
perceived sharpness in an image. For each block, a spectral 
sharpness measure is determined by the slope of the magnitude 
spectrum and a spatial sharpness measure is calculated by the 
total spatial variation. A perceived sharpness map is generated 
by combining the two measures via a weighted geometric mean. 
The overall sharpness of an image is obtained by the maximum 
value of the sharpness map.  
This work has been supported by the National Science Council under Grant 
No. NSC 99-2221-E-035-091 
3362
978-1-61284-774-0/11/$26.00 ©2011 IEEE
                     ∑∑
= =
==
M
m
M
n
kk nmbnmbkB
1 1
2
2
)],([),()(             (2) 
We define the global band-pass image contrast of the kth 
channel, C(k), by 
                                      ∑
=
= L
i
iB
kBkC
1
)(
)()(                             (3) 
where Lk ≤≤0 and ∑
=
=
L
k
kC
1
1)( . Fig. 4 illustrates the 
behavior of the global band-pass image contrast for three 
different input images which were obtained from the UT 
Austin LIVE database [6]. Figs. 4 (a), (b), and (c) show the 
blurred versions of the Student Sculpture image, Lighthouse 
image, and Church and Capitol image using a circularly 
symmetric 2-D Gaussian function having a standard deviation 
of 0.53, 2.74, and 7.67, respectively. Fig. 4(d) shows the three 
curves of the global band-pass image contrast distributions for 
Figs. 4(a)-(c), where the red, blue, and green curves correspond 
to Fig. 4(a), Fig. 4(b), and Fig. 4(c), respectively. The three 
curves are monotonically decreasing with respect to the 
channel index. We are unable to differentiate different 
blurriness by examining these curves. We shall address this 
issue below. 
 
Figure 2.  Extraction of high-pass component. 
 
Figure 3.  The structure of the multi-channel filter bank. 
C. Channel Weighting 
According to the idea of the matched filtering, the most 
significant channel is determined by channel comparison and 
then it is applied to compute the sharpness metric. We define 
the weighted global band-pass image contrast as 
              Lk
nmh
kCkCW
k
,,1,0,
),(
)()(
2
L==                       (4) 
where ),( nmhk  is the equivalent impulse response of the kth 
band-pass channel. The most significant band-pass channel is 
determined by 
                    { }LkkCWk ,,1,0,)(maxargmax K==             (5)  
The sharpness metric is given by 
              
⎪⎪⎩
⎪⎪⎨
⎧
≥
=
=
=
3,)2(
2,)1(
10,)0(
max
max
max
kC
kC
orkC
metric sharp
                               (6) 
The value of sharpmetric is between 0 and 1. The sharper the 
image, the higher the metric is. Equivalently, the blurriness 
metric is defined by sharpblur metricmetric −=1  Fig. 5 shows the 
three curves that represent the distributions of the weighted 
global band-pass image contrast of Figs. 4 (a)-(c). The red 
curve corresponding to Fig. 4(a) has a peak at maxk  = 1. 
According to (6), the sharpness metric is 0.6995. The blue 
curve corresponding to Fig. 4(b) reaches maximum at maxk  = 2, 
and the sharpness metric of this image is 0.327. Finally the 
green curve corresponding to Fig. 4(c) attains the maximum at 
maxk  = 3, and the sharpness of this image is 0.16. 
               
(a)                                                         (b) 
        
(c)                                                  (d) 
Figure 4.  Three blurred images and their global band-pass image contrast 
distributions. 
 
 
3364
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/30
國科會補助計畫
計畫名稱: 達成精確的彩色影像外貌：建構一個融合影像強化與色彩恆常性的演算法
計畫主持人: 辛正和
計畫編號: 99-2221-E-035-091- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
此次發表的 ICMT 國際會議論文獲得大會最佳論文獎。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
