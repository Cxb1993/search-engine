ij
xhW  denotes the weight of link between the ith input 
node and the jth hidden node;  denotes the weight 
of link between the jth hidden node and the kth output 
node.  and  denote the biases for the jth hidden 
nodes and the kth output nodes, respectively. X
jk
hyW
j
hB
k
yB
i(t) is 
the data value of the ith input node. Yk(t) is the output 
value of the kth output node. Targetk(t) represent a real 
output value of the kth output node. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. Three-layer feedforward neural network. 
 
 
The TFNN can be used to learn the input-output 
relationship of an application by various learning 
methods. The input-output relationship of the MINO 
TFNN is described by  
 
))(()( tXgtY = ,   t = 1, …, Ntrain                  (1) 
 
, where X(t) = [ X1(t), …, XNx(t)], and Y(t) = [Y1(t), …, 
YNy(t)] are the given inputs and the outputs of an 
unknown nonlinear function g(.), respectively. Let 
Ntrain denotes the number of the training data of input-
output pairs. The function gk is the value of the kth 
output node of the function g .The function gk is 
defined as the follow: 
Yk(t) =gk(X(t))= 
⎟⎟
⎟⎟
⎟⎟
⎠
⎞
⎜⎜
⎜⎜
⎜⎜
⎝
⎛
−
+
∑ ∑
+ =
−
k
y
hN
j
Nx
i
j
HBxhW
jk
hy B
e
W
e
)
1
(
11
1
. (2) 
 
The objective function f is defined as follow: 
 
( )
ytrain
Ntrain
t
Ny
k
kk
NN
tetTtY
f
∑ ∑
= =
−
= 1 1
2)(arg)(
                        (3) 
 
Where the function represents a mean square error (MSE) 
value between computing output values and real output 
values, the objective is min the function f by tuning 
Nparam=(Nx+1)*Nh+(Nh+1)*Ny  number of TFNN 
parameters, shown as follows: 
.  
( , …., , …, , …., ,  , …, 
, ,…., , ,…., 
, , …., )                (4) 
11
xhW h
N
xhW
1 1Nx
xhW h
NxN
xhW
1
hB
hN
hB
11
hyW
Ny
hyW
1 1hN
hyW
NyN
hy
hW 1yB
Ny
yB
Hidden layer Output layerInput layer
B. Intelligent Generation Mechanism IGM 
 
Consider a parametric optimization problem of m 
parameters. Assume a current solution is P1=(S1, …, 
Sm) , where Si is a parameter value, IGM generates two 
temporary solutions P2=( , …, ) and P11S 1mS 3=( , …, 
) from perturbing P
2
1S
2
mS 1 where  and  are 1iS 2iS
iii SSS +=1  and iii SSS −=2 , i=1, …, m.         (1) 
Each disturbing value 
iS  is generated by Cauchy-
Lorentz probability distribution [10]. IGM uses the 
three-level OA [13], [14]. By using the same division 
scheme, partition all the m parameters into N non-
overlapping groups. To efficiently use all columns of 
OA, N is generally specified as N = ( )/2 
and the used OA is L
⎣ ⎦ 13 )12(log 3 −+m
2N+1(3N) excluding the study of 
intractable interaction effects. 
IGM aims at efficiently combining good groups of 
parameters from solutions P1, P2 and P3 to generate a 
good candidate solution C1 for next move. How to 
perform an IGM operation using P1 with m parameters 
is described as follows: 
Step 1: Generate two temporary solutions P2 and P3 
using P1. 
Step 2: Adaptively divide each solution of P1, P2 and 
P3 into N groups of parameters where each 
group is treated as a factor. 
Step 3: Use the first N columns of an OA Ln(3(n-1)/2), 
where . ⎡ ⎤)12(log 33 += Nn
Step 4: Let levels 1, 2 and 3 of factor j represent the 
jth groups of P1, P2 and P3, respectively. 
Step 5: Evaluate the objective function value yt of the 
combination corresponding to the experiment t, 
where t = 1, …, n. 
Step 6: Compute the main effect Sjk where j = 1, 2, …, 
N and k = 1, 2, 3. 
Step 7: Determine the best one of three levels of each 
factor based on the main effect. 
Step 8: The solution C1 is formed using the 
combination of the best groups from the 
X1(t) H1
XNx(t) HNh
Y1(t) 
YNy(t)
: 
: 
: 
: 
:
:
Bh By
 
11
xhW
Nh
xhW
1
1Nx
xhW
NxNh
xhW
1
hB
Nh
hB
11
hyW
Ny
hyW
1
1Nh
hyW
NhNy
hyW
1
yB
Ny
yB
