 
Design and Implementation of Robust Controller via Hopfield Neural Network (I) 
 
ABSTRACT 
Faults due to the aging of a controller for a control system are very common; once 
they happen, the controller is quite difficult to be repaired for some reasons. To solve 
this problem, we discuss the feasibility of replacing the existing controller with a 
Hopfield neural network (HNN) controller. The weightings of the HNN are first 
trained off line by the steepest descent algorithm to make the output of the HNN 
mimic the existing controller. After the training is completed, the HNN is applied to 
the control system as a real-time controller. An inverted pendulum system (IPS) is 
used to examine the effectiveness of the proposed HNN controller. The simulation 
results show that even with the initial condition different from that in the training data, 
the proposed HNN controller can mimic the existing controller and achieve favorable 
performance. 
 
1. Introduction 
Figure 1-1 shows the circuit model of a Hopfield neuron [1].   
∑ ( )ϕ ⋅
1x
2x
Nx
1jw
2jw
jNw
ji i
i
w x∑
jC jR
jI
jv
jx
 
Fig. 1-1. The circuit model of a Hopfield neuron 
The input of the neuron is the output voltages ),...,( 21 Nxxx  of each Hopfield neurons, 
and the input voltage ix  will multiply the weighting factors, or the conductance j iw , 
to yield the currents ji iw x  for i=1,.., N. The current summation 
1
N
ji i
i
w x
=
∑ plus the bias 
current jI  is the total current totaljI −  , and the total current is the sum of the current 
jCI  passing through capacitor and the current jRI  passing through the resistor. We 
have the following equation: 
jRjCj
N
i
ijitotalj IIIxwI +=+= ∑
=
−
1
              (1-1) 
The voltage drop between the resistor and the ground is: 
jRjj IRv =                         (1-2) 
condition bellow: 
)()( * WJWJ ≤  .                      (2-1) 
This is to minimize the error function )(WJ  with respect to the weighting vector W , 
so the necessary condition for (1-5) is  
0)( * =∇ WJ  .                       (2-2) 
So we can use the idea of local iteration descent by starting with an initial guess 
)0(W , generating the a sequence of weighting vectors )1(W , )2(W ,…, )(kW , 
)1( +kW ,…, such that the error function )(WJ  being reduced at each iteration of the 
algorithm as shown by the following inequality: 
))(())1(( kWJkWJ <+                      (2-3) 
Where )(kW is the old value of the weighting vector and )1( +kW is its updated 
value. For convenience, we define the following equation: 
)(WJg ∇=                          (2-4) 
So the steepest descent algorithm can be written as the following equation: 
)()()1( kgkWkW η−=+                    (2-5) 
Where η is the learning rate (a positive constant), and )(kg  is the gradient vector 
evaluated at the point )(kW . From (2-5), we have the following equation: 
 )()()1()( kgkWkWkW η−=−+=Δ                (2-6) 
Now, we use the first order Taylor series expansion around )(kW  to approximate 
))1(( +kWJ  as the following equation: 
)()())(())1(( kWkgkWJkWJ T Δ+≈+             (2-7) 
Substituting (2-6) into (2-7), we have the following equation: 
2)())(()()())(())1(( kgkWJkgkgkWJkWJ T ηη −=−≈+      (2-8) 
Because η is a positive learning rate parameter, so the error function will be 
decreased as the algorithm progresses from one iteration to the next.  
 
3. Control of Inverted Pendulum System (IPS) 
The following Figure 3-1 shows the mechanism of Inverted Pendulum System (IPS). 
M
m
u
l
θ
•θ
 
Figure 3-1. The inverted pendulum system (IPS) 
or 
⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛
××+=⎟
⎟
⎠
⎞
⎜⎜⎝
⎛
•
•
2
1
212
1
k1.46k1.4615.8
10
θ
θ
θ
θ                (3-6) 
The characteristic equation of (3-6) can be found as: 
01.46k15.81.46k
k1.46k1.4615.8
1-
det 12
2
21
=−−−=⎟⎟⎠
⎞
⎜⎜⎝
⎛
×−×−− λλλ
λ
    (3-7) 
Therefore 
1
2
2
1.46k15.8
k0.73
−−=
×−=×
n
n
ω
ωξ
                 (3-8) 
Let 1.8  ,  0.4 == nωξ , we can have 0.99k   ,  -13k 21 −==  from (2-8). So we have 
 
•−−=−−= 1121 0.99310.9931 θθθθu                 (3-9) 
Equation (3-9) is the linear reference controller of IPS. Let ( ) ( )0 0 20θ θ= = , the 
control of IPS using (3-9) can be seen in the following Figures 3-4 to 3-5. The 
numerical data of Figures 3-4 to 3-5 will be adopted as the training data to train the 
HNN controller to control the IPS. The architecture and training algorithm of the 
HNN controller will be presented in the following section. 
 
Figure 3-3. The control force of the 
linear reference controller 
 
Figure 3-4. The output angle (θ1) of 
IPS controlled by the linear reference 
controller 
1 1 1 11 1 12 2
2 2 2 21 1 22 2
;
;
v R i i w x w x
v R i i w x w x
= × = × + ×
= × = × + ×               (4-1) 
To update the weighting factors, i.e., { }11 12 21 22, , ,w w w w , we assume that x1 and x2 will 
be perturbed by the deviated weighting factors to yield Δ x1 and Δ x2, which can be 
defined as: 
1 1 1 2 2 2;x d x dθ θΔ = − Δ = −                  (4-2) 
Therefore we have 
1 11 1 1 12 2 2
2 21 1 1 22 2 2
( ) ( )
( ) ( )
i w x x w x x
i w x x w x x
= × + Δ + × + Δ
= × + Δ + × + Δ            (4-3) 
As shown in Figure 4-1, we also have the control output as:  
)( 21 xxcu amp +×=
∧
                    (4-4) 
We define the error J as follows: 
2)(
2
1 ∧−×= uuJ                        (4-5) 
The goal is then obvious to minimize the above J with respect to the weighting vector 
{ }11 12 21 22, , ,w w w w . The steepest descent method (or the back propagation method) in 
Section 1.3 can be applied for this purpose. For clarity purpose, we consider only 11w  
for example to show how it can be trained. First, from (2-8), we have the equation 
bellow to show how we can have a better next value of 11w : 
11
1111 )()1( w
Jkwkw ∂
∂×−=+ η ,                (4-6) 
where η  is the learning rate, and we should calculate the value of
11w
J
∂
∂ . We can use 
the chain rule to have: 
11
1
1
1
111 w
v
v
x
x
u
u
J
w
J
∂
∂×∂
∂×∂
∂×
∂
∂=∂
∂ ∧
∧                  (4-7) 
From (3-5), we have: 
)( uu
u
J −=
∂
∂ ∧
∧                        (4-8) 
From (3-4), we have: 
ampcx
u =∂
∂ ∧
1
                        (4-9) 
5. Simulation Results 
In the following simulations, we let R=1 Ohm and C=0.001 Farad in HNN controller 
with amplification constant ampc =-30, and learning rate η =0.001. Two cases are 
performed with initial weighting vectors { } { }11 12 21 22(0), (0), (0), (0) 0,0,0,0w w w w = and 
{ } { }11 12 21 22(0), (0), (0), (0) 0.1,0.2,0.3,0.4w w w w = . It is noted that the final weighting factors 
will be constant real values for real implementations. Table 1 shows the final 
weighting factors for both cases. 
 
{ }11 12 21 22(0), (0), (0), (0)w w w w  { }11 12 21 22( ), ( ), ( ), ( )w w w w∞ ∞ ∞ ∞  
( )0,0,0,0  ( )0.1755,0.01367,0.1755,0.01367  
( )0.1,0.2,0.3,0.4  ( )0.0901,-0.0743,0.2954,0.1359  
Table I. The training results for different initial weighting factors 
 
 
Figure 5-1. The training process of { }11 12 21 22, , ,w w w w  with { } { }11 12 21 22(0), (0), (0), (0) 0,0,0,0w w w w =  
After applying the final weighting factors from Figure 5-1, we have the following 
simulation results for real control actions. 
 
Figure 5-2. The control forces of IPS: linear reference 
controller (dash line) and HNN controller (solid line) with 
{ } ( )11 12 21 22( ), ( ), ( ), ( ) 0.1755,0.01367,0.1755,0.01367w w w w∞ ∞ ∞ ∞ =
 
Figure 5-3. The output angles of IPS: linear reference 
controller (dash line) and HNN controller (solid line) with 
{ } ( )11 12 21 22( ), ( ), ( ), ( ) 0.1755,0.01367,0.1755,0.01367w w w w∞ ∞ ∞ ∞ =
 
Figure 5-8. The output angular speeds of IPS: linear reference controller (dash line) and HNN 
controller (solid line) with { } ( )11 12 21 22( ), ( ), ( ), ( ) 0.0901,-0.0743,0.2954,0.1359w w w w∞ ∞ ∞ ∞ =  
6. Conclusion 
This paper has proposed a new training algorithm to train a HNN controller based on 
a well-designed control results by conventional controller. After training the 
weighting factors of HNN controller, the output (control signal) of HNN controller 
can indeed approximate the output (control signal) of the well-designed controller 
and the outputs of the plant controlled by the HNN controller can also approximate 
the outputs of the plant controlled by the well-designed controller. Different initial 
weighting factors are adopted to yield different final constant weighting factors. 
However, these different final weighting factors can still yield similar control results. 
This implies that the trained weighting factors are only to the degree of local 
minimum solutions. 
Faults due to the aging of a controller for a control system are very common; once 
they happen, the controller is quite difficult to be repaired for some reasons. We 
proposed an HNN controller for an aging control system to solve this problem. In this 
paper, we trained the weighting factors of the HNN controller to mimic the existing 
controller. Then, the trained HNN controller is used to replace the existing aging 
controller. Can we control the system by an HNN controller trained online without a 
reference controller? We will focus our research interests on exploring the potential of 
this interesting question. 
 
Reference 
[1] N. C. Kan, “Design of Hopfield Neural Network Controller with Its   
applications,” MS Thesis, Department of Electrical and Control Engineering, 
NCTU, Hsin-Chu, Taiwan, 2006. 
