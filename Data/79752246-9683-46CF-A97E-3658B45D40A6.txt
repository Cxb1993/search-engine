 - ii -
 
目錄 
 
中文摘要 ……………………………………………………………..………… v 
英文摘要 ………………………………………………………………….…… vi 
報告內容 1. 偏離車道警示與前車碰撞警示 ………………………….…….  1 
1. Introduction ………………………………………………………………………..……  1 
1.1 Motivation ……………………………………………………………………………  1 
1.2 System overview ………………………………………………………………..…..  2 
1.3 Part organization ……………………………………………………………..……  3 
2. Related Works ………………………………………………………………………..…  6 
2.1 Lane detection ………………………………………………………………………. 6 
2.2 Vehicle detection ………………………………………………………………….... 8 
2.2.1 Vehicle detection at daytime ……………………………………………..….. 8 
2.2.2 Vehicle detection in bad weather conditions ……………………………….... 11 
3. Lane Detection ……………………………………………………………………..…... 16 
3.1 The procedure of lane detection ……………………………………………………. 16 
3.2 Classification of yellow/white lane marks ……………………………………........ 20 
3.3 Classification of double/single lane marks ………………………………………… 24 
3.4 The judgment on the width of lane marks …………………………………………. 26 
3.5 The judgment on the differences of slopes and intercepts in consecutive frames … 27 
4. Vehicle Detection ……………………………………………………………………… 29 
4.1 Preceding vehicle detection in rainy day ………………………………………….. 32 
4.1.1 Template generation ……………………………………………………….… 32 
4.1.2 Define searching ranges for templates ………………………………………. 33 
4.1.3 Template matching …………………………………………………………... 34 
4.1.4 Find a detected box ………………………………………………………….. 34 
4.2 Vehicle verification ………………………………………………………………… 35 
4.3 Vehicle tracking ……………………………………………………………………. 36 
4.3.1 Predict the vehicle position …………………………………………………. 36 
4.3.2 Verification in tracking stage ……………………………………………….. 37 
5. Experiments …………………………………………………………………………… 38 
5.1 Developmental environment ………………………………………………………. 38 
5.2 Experiments ……………………….……………………………………………….. 40 
5.2.1 Lane detection ………………………………………………………………. 40 
5.2.2 Preceding vehicle detection …………………………………………………. 47 
5.3 Discussions ……………………………………………………………………....... 49 
 - iv -
6.2.3 Drowsiness discrimination by PERCLOS measurement ……………………. 96 
6.2.4 Local maxima searching of the projection chart …………………………….. 99 
6.2.5 Local thresholding ………………………………………………………......... 99 
6.2.6 Eye open/close determination ……………………………………………….. 101 
7. Conclusions and Future Works ………………………………………………………… 102 
7.1 Conclusions ……………………………………………………………………....... 102 
7.2 Future works ………………………………………………………………………. 102 
References ……………………………………………………………………………....... 103 
計畫成果自評 ………………………………………………………………… 105 
與本計畫相關之著作、專利、學生畢業論文、及得獎 …………………… 107 
附件 A: 可供推廣之研發成果資料表 ……………………………………… 109 
附件 B: 出席國際學術會議心得報告及發表之論文 .................................... 114 
 
 
 
 
 
 
 - vi -
英文摘要 
Keywords: Intelligent vehicle, advanced safety vehicle, driving assistance, monocular computer 
vision, road situation detection, driving situation detection, driver drowsiness 
detection, driving behavior detection 
In these few decades, the vehicle number is rapidly increasing because people’s incomes are 
increasing. In addition to the vehicle number, more factors of road situation, driving environment, 
and human attention result in a large amount of traffic accidents and casualties. If there is a 
mechanism to help the driver to detect the road situation and driving environment, and then to 
provide some useful information to the driver in these situations, the danger is therefore avoided. In 
this research project, we are just combining the techniques of computer vision, image processing, 
and pattern recognition, and matching the domestic and international environments to develop 
state-of-the-art techniques of vision detection to reduce the load of drivers, to reduce the traffic 
accidents, and then improve the driving safety.  
According to the 2000 report of America transportation department, more than 85.4 % of the 
light vehicle crashes are categorized into rear-end crashes, intersection crashes, road departure 
crashes, and lane change crashes. In these decades, many developed countries are urgently 
developing the mechanism of safe driving. The studies of this research project include most 
functions of advanced safe vehicles, such as road condition detection, driving situation detection, 
driver drowsiness detection, driving behavior detection, pedestrian/object detection & motion 
estimation, and others. Some ideas were generated from the discussions with transportation 
companies; thus the study contents can match the requirements of commercial applications and we 
are just aiming the goal to promote our system. We hope our partial results may attract the related 
companies to join our complete researches. In the period of the end of 2006 to the begin of 2007, 
we actually attract the CSIST, ITRI, and ARTC to works with us toward the implementation of 
advanced safe vehicles.  
The completed studying items are divided into three categories: i.lane detection; for example, 
lane mark detection, lane mark classification, multi-lane detection and classification, lane departure 
warning, ii.preceding vehicle collision avoidance; for example, preceding vehicle detection, vehicle 
tracking, distance estimation, forward collision warning, iii.drowsiness detection; for example, eye 
detection, eye open/closing decision, face orientation estimation, gaze determination, drowsiness 
recognition. Those many studying items were constructed in the styles of functional modules. It 
means the structure of the vision subsystem is reconfigurable; the subsystem can have different 
hardware modules and provides different detection function for different applications. Moreover, 
the subsystem is worked even if some hardware modules were broken down or out of work. 
Moreover, we are just install the results of lane mark detection and lane departure warning into an 
Analog Devices embedded system to enhance the practices. The studying results of this research 
project are so plenty, we have got prizes in several research contests. 
 - 2 -
 
The current visual detection system at daytime contains twelve functions: lane detection, 
multiple lane estimation, classification of solid/dashed lane marks, classification of yellow/white 
lane marks, classification of single/double lane marks, direction estimation, vehicle lateral offset 
estimation, lane departure warning, preceding vehicles detection, preceding vehicle distance 
estimation, brake light detection, and turn signal detection. The visual detection system is illustrated 
in Fig.1.2. 
Lane detection and multiple lane estimation will support lane position information for other 
detection modules. According to the classifications of solid/dashed, double/single and yellow/white 
lane marks, the road types such as urban roads or highway will be known clearly. We know what the 
direction of their vehicles is by direction estimation. Vehicle lateral offset estimation can be gained 
by calculating the distance from lane to our vehicle. According to the driving habit of every driver, 
lane departure warning function can be set to remind us that vehicle far away from our lane. The 
driver can own the information about how many vehicles in front of our vehicle and those vehicle’s 
positions preceding are by preceding vehicle detection. Preceding vehicle distance estimation let 
driver know further if our vehicle is in safe range or not. Brake light detection and turn signal 
detection support driver to understand preceding vehicle’s movement. 
 
1.2 System overview 
In this part, we present the lane detection and preceding vehicle detection. We develop efficient 
methods to make the visual detection system more robust. We briefly describe the major parts of 
lane detection, preceding vehicle detection as follows: 
(1) Lane detection: based on the study of Chen [9], we build two second differences maps and use 
lane model to fit the lane. Next, we verify the detected result by the width of lane marks and the 
differences of slope and intercept in consecutive image frames. And we utilize the information 
of lane marks to judge what kinds of road we are driving. 
(2) Vehicle detection: vehicle detection is divided into two independent parts based on daytime and 
rainy day.  
y Detecting preceding vehicle at daytime is achieved by using two features based on the 
studying results of Shih [32]: cast shadow and left/right borders. We use vertical edges, 
horizontal edges and vehicle’s ceiling to do these purposes. Using cast shadow and horizontal 
edge are to search the shadow line of a possible vehicle. Shadow line and left/right borders are 
used to confirm the size of detected box. Then we use several criteria to verify the possible 
vehicle. 
y Detecting preceding vehicle in rainy day is achieved by using template matching. After 
finding a detected box, we use the ratio of the width of the detected box to road width, 
symmetry and variances to verify the possible vehicle. 
(3) Vehicle tracking: based on the studying results of Shih [32], we predict a possible position of a 
 - 4 -
 
  
 
Fig.1.2. The system diagram of the proposed detection system. 
 
 
 - 6 -
 
2. Related Works 
Vision-based vehicle detection for driver assistance has received considerable attention over 
the last 15 years. Lane detection and Detecting preceding vehicles are playing the important roles in 
the safety vehicles and autonomous vehicle. In the last years, vision detection techniques for safety 
vehicles in various weather conditions are taken seriously and start to develop. In this chapter, the 
related works of lane detection and vehicle detection are described. 
  
2.1 Lane detection 
Many researches focus on the analysis of lane marks painted on the road surface. It is the 
objective to determine the lane information and to detect the relative position between the vehicle 
and the road. After getting the lane information, such as the offset, the orientation and the curvature, 
we can provide a robust system to improve the safe driving. 
Bertozzi and Broggi [4] proposed the GOLD (Generic Obstacle and Lane Detection) system 
which removed the perspective effect by mapping the road image into the top view and determined 
the lane marks by pattern matching which relied on the feature of the constant lane width.  
Chern et al. [12, 13] provided two algorithms individually to deal with lane detection at 
daytime and nighttime as follows:  
(1) At daytime, they divided the image field into left/right parts and extracted edge points. After 
choosing the edge points of lane marks, they linked the neighboring edge points to form several 
lists as shown in Fig.2.1. Based on quadratic polynomial curve-fitting, they fit the left/right lane 
boundaries as shown in Fig.2.2.  
(2) At nighttime, they calculated the sum of RGB in the image to extract bright pixels. Based on 
blob labeling, they confirmed the reflect spots by area. Finally, they used inverse perspective 
transform and curve approximation to fit the lane boundaries. 
 
 
Fig.2.1. The result of linking the neighboring edge points. 
 
 - 8 -
Wang et al. [37, 38] found out three properties of painted lane marks: (1) brightness, (2) 
slenderness, and (3) proximity. According to the brightness and slenderness properties, they used a 
peak-finding algorithm and Gaussian filter [22] to extract the feature points of lane marks. In order 
to represent a line segment, they utilized a LSM (Least Square Method) to characterize the line 
segment as shown in Fig.2.4. According to the proximity property, they calculated the included 
angle and two perpendicular distances between two line segments. Next they introduced the 
confidence function to combine those line segments to form several lane boundary candidates. 
Finally, they proposed a method based on master-slave strategy to select two suitable candidates as 
the lane boundaries of our driving lane as shown in Fig.2.5. 
 
     
(a)               (b) 
Fig.2.4. The bi-level images. (a) Feature point extraction. (b) Line segment image. 
 
 
Fig.2.5. Result of lane boundaries selection algorithm. 
 
2.2 Vehicle detection 
Vehicle detection methods employ several features to hypothesize vehicle locations in an 
image such as shadow, corners, symmetry, vertical edges, horizontal edges, colors, texture, and 
vehicle lights. Almost every vehicle detection system uses many features to estimate the locations 
of possible vehicles. 
 
2.2.1 Vehicle detection at daytime 
Bertozzi et al. [5] presented a method to producing a symmetry map based on the combination 
 - 10 -
Huang et al. [22] detected vehicles using three features: (i) underneath, (ii) vertical edge, and 
(iii) symmetry property. They utilized Sobel edge operator to extract underneath points and vertical 
edge points as shown in Fig.2.8.  
 
  
(a)                            (b) 
Fig.2.8. The extracted images. (a) Extracting underneath pixels. (b) Extracting vertical pixels. 
 
Based on the symmetry property, they defined a window whose size was according to the 
typical aspect ratio of vehicles and perspective constraints. The gray-level symmetry score was 
defined as 
( ) ( )
WH
h w,W/2G-h w,-W/2G
- symmetrylevelGray 
H
h
W/
w
×
∑ ∑ +
=
= =1
2
1                 (2.3) 
where G(x,y) was the gray-level value of point (x,y), and H and W denoted the height and width of 
the window, respectively. The higher symmetry value was, the more symmetric the region was. 
Kate et al. [25] focused on the detection of mid-range and distant vehicles to arrive at an 
algorithm with low computational complexity. They presented a method to detect vehicles in image 
sequences without pre-knowledge about the position of the road.  
They utilized a combination of three different data: (i) shadow, (ii) entropy, and (iii) horizontal 
symmetry. First, they select regions in the image plane that exhibit the characteristic of shadow 
projected underneath a vehicle. Next, they analyzed the entropy and horizontal symmetry of these 
selected regions. Only those regions that contain enough entropy and symmetry are identified as 
potential vehicle. The result of finding out the potential vehicles is shown in Fig.2.9. In the figure, 
numbers 1, 2, 3, and 4 are regarded as vehicles and other regions are rejected on the entropy or the 
symmetry property. Based on the results of many different experiments, they illustrate the robust 
and accurate performance of their detection method. 
 
 - 12 -
  
(a)                              (b) 
Fig.2.10. The extracted lights. (a) Nighttime road environment. (b) Bright objects image. 
 
The extracted bright objects are processed by a rule-based procedure, to identify the vehicles 
by locating and analyzing their vehicle light patterns, and estimate their distances to the 
camera-assisted car. An example is shown in Fig.2.11. 
 
  
(a)                                (b) 
Fig.2.11. Results of vehicle detection. (a) Detecting oncoming and preceding vehicles. (b) Detecting 
vehicles comprised of many other non-vehicle lights in the image. 
 
Chern and Hou [12, 13] presented a computer vision system for detecting lanes and vehicles at 
night and rainy day, they detect other vehicles relies on the rear-lights of those vehicles and lane 
boundaries detected (or estimated). They define criteria for white bright pixels as  
(R, G, B > 240) and (R > G) and (R > B)                                 (2.4) 
While the criteria for red pixels are 
(R > 100) and (R – 10 > G) and (R – 10 >B)                              (2.5) 
Using these criteria to collect the red pixel and white pixel, it can extract the nearby taillight 
candidates. After extracting the candidates, they remove the reflector spots from the bright-spot 
image and using the information of detected lane boundaries, the pairing of taillights to find 
vehicles can be simplified as the example shown in Fig.2.12.  
 
 - 14 -
 
Fig.2.13. The candidate taillight region. 
 
Sukthankar [36] provided a vision-based system “RACCOON” that tracks vehicle taillights at 
nighttime, and it can process to yield normalized red intensities at every pixel by 
rN = R/(R+G+B),                                                        (2.6) 
thresholding is then preformed to keep only the pixels which satisfy: 
a. Absolute red intensity > TA 
b. Normalized red intensity > TB, 
where TA and TB are thresholds specified at run-time. 
This selection process ensured that only pixels which are bright enough and red enough to be 
taillights. And then it built a global map in real time of the lead vehicle’s position based on the 
location and separation of the taillights in a sequence of video image. An autonomous vehicle 
detection system at night must deal with an additional set of problems as  
i. The road cannot be seen clearly at night.  
ii. Traffic looks like a pattern of bright lights on a black background.  
iii. Unlit landmarks cannot be detected so corners and intersections have to be negotiated based 
solely on the observed actions of the lead vehicle.  
RACCOON solved these problems by creating an intermediate map structure which records the 
lead vehicle’s trajectory. The path was represented by points in a global reference frame, and the 
computer controlled vehicle was steered from point to point. 
Wang et al. [37, 38] achieved the system by using an evident feature which are extracted 
through five steps: taillight standing-out process, Gaussian filter, adaptive thresholding, centroid 
detection, and taillight pairing algorithm. An example is shown in Fig.2.14. 
 
 
 
 
 
 
 
 
 - 16 -
3. Lane Detection 
In our proposed lane detection system, we utilize a consecutive RGB color image frames and a 
defined lane model to obtain some reliable cues for classifying the appearance of the lane marks, 
yellow/white lane marks, and single/double lane marks. According to the shape and reasonable 
existing range of the lane marks, moreover, we support some obvious information such as the width 
of the lane marks and the differences of the slope and the intercept in consecutive image frames. 
Based on our lane detecting methods, we can already deal with highway or urban roads and 
overcome the different variations in weather, vehicle occurrence, and broken lane marks. 
 
3.1 The procedure of lane detection 
In this study, we generate positive and negative difference maps of the acquired images by 
utilizing the strong contrast between lane marks and road surface. By the observation, the lane 
marks in our road images always tend to vertical. In order to detect the lane marks accurately, we 
build two second difference maps by a 1×3 mask as shown in Fig.3.1. In Fig.3.2, we show the 
examples about the pixels with greater horizontal difference values which is meant that they are 
located at certain edges. Based on the study of Chen [9], we define a lane model as follows: 
y = tanθ(x - b),                                                       (3.1) 
whereθis the slope of the line and b is the intercept of x-axis. 
 
1 0 -1
Fig.3.1. A mask for difference maps. 
 
  
(a)                          (b) 
Fig.3.2. The testing images. (a) Original image. (b) Second horizontal difference map. 
 
After generating second difference maps and using a defined lane model, we can find that the 
fitting lines of the maximum and minimum accumulations in the second difference maps are the 
edges of the lane marks. The result is shown in Fig.3.3. Moreover, in order to find the fitting lines 
accurately and rapidly, we fix the position of camera to know the reasonable ranges in slope and 
intercept of left/right lane marks. Based on the restricted ranges, we can avoid detecting wrong 
targets such as wire poles, roofs, and eaves. 
 - 18 -
 
On the urban roads, the driving environments are more complicated than on the highway such 
as the intersections, pedestrian crossings and a parking ban area as shown in Fig.3.5. And the cars 
usually jolt over the rough road. In order to overcome these problems, we propose a method to 
calculate the differences of the slope and the intercept in consecutive image frames. When driving 
in a rough road, we can correct the wrong detection results immediately by the differences with its 
weight. We can also judge whether there is a intersection or not when the differences change 
frequently in consecutive image frames. Based on these methods above, we can deal properly with 
all kinds of driving situations. And we show the flow chart of lane detection and tracking in Fig.3.6. 
 
  
(a)                               (b) 
 
(c) 
Fig.3.5. The illustrations of driving environments. (a) In the intersection. (b) In the parking ban area. 
(c) In the pedestrian crossings. 
 
 - 20 -
3.2 Classification of yellow/white lane marks 
We utilize the lane model to fit the edges of left/right lane marks by maximum and minimum 
accumulations. After finding these edges, we obtain the color values and position information of the 
middle lines in left/right lane marks. By our observation, the color values of the lane marks are 
vaguer if they are closer to the vanishing point. In order to avoid the false results happened, we 
select a three-fourth length of the middle line from bottom to top as the detected range. We define 
that H is the total length of the middle line. The result is shown in Fig.3.7. 
 
 
Fig.3.7. The illustration of selecting a detected range. 
 
As we define the detected range in the middle line, we can own the RGB values of all pixels 
fell in the detected range. In order to overcome the influences of different weather conditions such 
as sunny day, nightfall and cloudy day as shown in Fig.3.8, we analyze RGB values dividedly for 
yellow lane marks, white lane marks and road surfaces and we take R and B values as our key cues. 
According to the analyses of RGB values, we can choose the candidates of yellow/white lane marks 
by two cues: (i) R color value and (ii) Illumination. We give an example of the analyses of R and B 
values at dusty day as shown in Fig.3.9.  
In order to make our method adaptable and avoid the interference of road surface, we first use 
the iterative thresholding in R color value named as ThrR to extract the possible lane marks when 
their R values are larger than ThrR. And we calculate the average illumination values of possible 
lane marks named as AI_LM. We extract the pixels which close to the edge of lane marks as the 
information of road surface and then calculate the average illumination values of road surface 
named as AI_RS. We extract the most possible lane marks by the difference between AI_LM and 
AI_RS. The criteria of the difference between AI_LM and AI_RS are defined as 
  AI_LM ﹣AI_RS > 20, if there exist the lane marks. 
 AI_LM ﹣AI_RS ≤ 20, if there is the road surface.                          (3.2) 
H
3H/4 
 - 22 -
 
 
 
 
(a) 
 - 24 -
 
Table 3.1. The Analyses of R and B Values in Different Weather Conditions 
 Sunny day Nightfall Cloudy day  
R value of white lane marks 194~243 103~176 110~195 
B value of white lane marks 192~242 114~175 157~221 
R value of yellow lane marks 178~240 152~178 102~144 
B value of yellow lane marks 171~190 127~142 100~118 
R value of road surfaces 117~178 87~145 71~105 
B value of road surfaces 114~169 81~109 74~144 
 
3.3 Classification of single/double lane marks 
In the step of classifying yellow/white lane marks, we have gotten the information of the 
middle lines in left/right lane marks. Based on our observation, the double lane marks always 
converge on the vanishing point and make us classify them into single lane marks easily. In order to 
form a suitable ROI for classifying single/double lane marks, we select a second half length of the 
middle line as the detected range. We define that H is the total length of the middle line. The result 
is shown in Fig.3.10. 
 
 
Fig.3.10. The illustration of selecting a detected range. 
 
After getting detected range, we propose an algorithm to form a ROI for classifying 
single/double lane marks as follows: 
Step 1. Accumulate five consecutive lane marks from top to bottom in the detected range. 
Step 2. Get the middle point of the five consecutive lane marks as the center of ROI. 
Step 3. Based on the slope of middle line and the center position, we form a 40×5 parallelogram as 
H 
H/
2
 - 26 -
  
(a)                            (b) 
Fig.3.13. The results of classifying yellow/white and single/double lane marks. (a) On the highway. 
(b) On the urban road. 
 
3.4 The judgment on the width of lane marks 
When you drive in the night, you can observe that it often occurs dispersible situation in 
headlights, taillights, and streetlamps. In order to avoid the interferences of dispersible situation in 
the lane detection, we use the slender property and it looked like a rhombus. The proposed method 
is as follows and the illustration is shown in Fig.3.14: 
Step 1. In the section 3.2, we have obtained the edges of left/right lane mark as shown by red lines 
in Fig.3.14. 
Step 2. Fix three different height values (Y = 75, 80 and 85) as our detected heights. 
Step 3. Calculate three width values (w1, w2 and w3) of the edges when Y is 75, 80 and 85 as shown 
by green words in Fig.3.14. 
Step 4. If (w1 + w2 + w3)/3 ＞ 5 pixels, we can recognize there are the lane marks indeed. 
Otherwise, we recognize there is a dispersible situation. 
 
 
 
 
 
 
 
 
Fig.3.13. The illustration of the width judgment. 
Y
80 
75 
85 
w
w
w
Vanishing point 
(0, 0) X 
 - 28 -
difference is too large, we propose the criteria to judge and correct the wrong situation. The 
criterion is defined as 
if | Di θ1 - Di_ran_slo | > 20 or | Di b1 - Di_ran_inter | > 20, 
θi = θi-1 + Di_ran_slo and bi = bi-1 + Di_ran_inter,                   (3.6) 
Step 5. Replace the oldest (θi-3, bi-3) by new (θi+1, bi+1) of next frame and Record the values named 
as (θi-2, θi-1, θi, θi+1) and (bi-2, bi-1, bi, bi+1). If the consecutive-correction times < 6, we correct 
these wrong situations and go to step 1; otherwise, go to Step 6. 
Step 6. Reset the consecutive-correction time to zero and clean every value we recorded. In next 
frame, we restart the lane detection and go to Step 1. 
 
 
 
 
 
 
 
(a) 
 
 
 
 
 
 
(b) 
Fig.3.15. The illustration of calculating the difference values in consecutive frames. (a) 
Detected time = i. (b) Detected time = i+1. 
Frame i-3 
(θi-3, bi-3) 
Frame i-2 
(θi-2, bi-2) 
Frame i 
(θi, bi) 
Frame i-1 
(θi-1, bi-1) 
Weight:W3 
(Di θ3, Di b3) 
Weight:W2 
(Di θ2, Di b2) 
(Di θ1, Di b1) 
Detected time = i. 
….. …..
Frame i-2 
(θi-2, bi-2) 
Frame i-1 
(θi-1, bi-1) 
Frame i+1 
(θi+1, bi+1) 
Frame i 
(θi, bi) 
Weight:W3 
(Di θ3, Di b3) 
Weight:W2 
(Di θ2, Di b2) 
(Di θ1, Di b1) 
Detected time = i+1. 
….. …..
 - 30 -
 
 
(b) 
 
 
 - 32 -
4.1 Preceding vehicle detection in rainy day 
In the rainy day, the scene is obscure due to the rain as shown in Fig.4.2. Under this condition, 
it is too difficult to detect the positions of cast shadow. If we fail to detect the cast shadow at first 
step in detection stage, we can not detect the preceding vehicle. In order to overcome this condition, 
we provide the template-based method. By using predefined pattern of the tires and bottom in the 
vehicle, we perform correlation between the image and the template to detect the possible vehicle 
step by step. And then we build the suitable detected box to verify and track as shown in Fig4.1. 
  
 
Fig.4.2. Rainy image. 
 
4.1.1 Template generation 
By our observation, we find two properties for generating the template as follows: 
i. The tires and the bottom of vehicle are darker than the road surface, no matter what kinds of 
vehicle are. 
ii. The template contains a priori knowledge about vehicles: A vehicle is generally symmetric, 
characterized by a rectangular bounding box which satisfies specific aspect ratio constraints. 
Based on two properties, in the Fig.4.3, we select the bottom part of vehicle as the region for 
generating the template. 
 
 
Fig.4.3. The selected regions for generating the template. 
 
 - 34 -
 
4.1.3 Template matching 
Based on the templates and the defined searching ranges, we use template matching to 
calculate the correlation values within each searching range. Here we propose two formulas for 
calculating the maximum correlation value which indicates the best matching position of candidate 
vehicle as follows: 
(4.2)                                         .
]]),([)],(),([[
]),()][,(),([
),(
(4.1)                                                                                    ,),(),(),(
2
122∑∑ −−−∑∑ −
∑∑ −−−−
=
∑∑ −−=
x yx y
x y
x y
wnymxwyxfyxf
wnymxwyxfyxf
nmr
nymxwyxfnmR
 
Because we have already fixed the sizes of templates and searching ranges and just want to 
compare the correlation values, we utilize formula 4.1 to ignore the influences of scale changes in 
the amplitude and to reduce the computing time.  
In order to find all possible vehicles, we sort these correlation values from large to small. And 
then we choose first ten percentages of correlation values as the possible vehicle positions. The 
example is shown in Fig.4.6. 
 
  
(a)                               (b) 
Fig.4.6. The extracted images. (a) The position of maximum correlation values. (b) The 
positions of first ten percentages of correlation values. 
 
4.1.4 Find a detected box 
In above section, we obtain many correlation values. And then we provide a method to build 
the detected box for vehicle verification and tracking. The steps are as follows: 
Step 1. We choose the maximum correlation values as our starting point, and record its coordinates 
(Xm, Ym). In order to overcome the interference of rear window, we search the suitable 
position along y-coordinate based on the position of (Xm, Ym) to check if there exists the 
lowest position of correlation value or not. If there exists the lowest position (Xm, Yms) of 
correlation value, we record the position as our suitable position and form a detected box in 
 - 36 -
(ii)  Symmetry =
WH
yxWGyxWG
H
x
W
y
×
+−−
−
∑ ∑
= =1
2/
1
2)),2/(),2/((
, 
where G(x, y) is the gray-level value at (x, y). H and W are the height and width of the detected 
box. 
(iii) VarC =
WH
myxC
H
x
W
y
c
×
∑ ∑ −
= =1 1
2)),((
, 
where C = R, G, or B; VarR, VarG, and VarB are the variances of a detected box; R(x,y), G(x,y), 
and B(x,y) are the color at (x,y); mR, mG, and mB are the RGB mean values of road. 
If the detected box is met, add the detected box into tracking list, else discard the box. 
 
4.3 Vehicle tracking 
After executing vehicle detection and verification, we have already recorded the position, 
width, and color of each detected box. In order to detect the vehicles more efficient and reduce 
computing time, we divide the tracking stage into two key parts as follows and the flow chart is 
shown in Fig.4.1: 
i. Prediction: we detect the vehicle at a predicted region and check if a vehicle is found or not. 
Then we build a new detected box. 
ii. Verification: we compare the detected box with the tracking vehicle. If they are matching, we 
update the position, width, and color of the tracking vehicle. Otherwise, we record the loss times 
and discard the tracking vehicle when the loss times are over three. 
 
4.3.1 Predict the vehicle position 
For a moving object, we predict its new position as follows: 
ttVtStS Δ+=+ )()()1( ,                                                  (4.3) 
where S (t+1) is predicted position at time t+1, S(t) is current position at time t. V(t) is the velocity 
at time t. △t is the interval time between two frames. 
If the vehicle speed up immediately or the position of the vehicle is not predicted accurately, it 
will make our system failed. In order to overcome this problem, we change this method which is 
forecasting the size of the predicted region. The predicted region is where we will detect vehicle. 
When the velocity is larger, the predicted region is larger and we will track two points the upper-left 
corner point and bottom-right corner point of the predicted region. The extending searching region 
is listed as follows: 
i. We extend the size of detected box with index range that it is defined in advance for x 
coordinates and y coordinates, this index range is used to avoid the problem of the vehicle is 
speed up all at once. 
ii. According to the x coordinates for detected box at time t and t-1, we can calculate the new 
position for x coordinates. Using the same method to predict y coordinates.  
 - 38 -
5. Experiments 
We provide several experiments and comparisons in this chapter. First, we introduce our 
develop platform. Second, we demonstrate the effects of detecting lane and preceding vehicle by 
several experimental results. Finally, we compare the experimental results and discuss the 
processing time. 
 
5.1 Developmental environment 
In order to develop safe driving system, our system must be reliable, robust, and real-time for 
variant weather and driving conditions. All algorithms are implemented in C++ programming 
language and Microsoft Foundational Classes (MFC) Library, and all experiments are executed on 
a general PC with Intel® Pentium® D 3.0GHz CPU and 512MB DDR RAM and Microsoft® 
Windows XP professional operating system as our experimental platform.  
In our experiment, we use the image size of video is 320×240 pixels in RGB color space. And 
here we propose hardware architecture of our system in our experimental vehicle is shown in 
Fig.5.1. 
 
                     
(a)                           (b) 
Fig.5.1. Image grabber and processing system. (a) CCD camera. (b) IBM Notebook. 
 
In order to prove the correctness and accuracy of our system, in Fig.5.2, we have tested our 
system on our experimental vehicle which runs on the highway and urban road with the velocities 
between 50 km/hr and 120 km/hr, and we display the experimental results of real-time visual 
detection. Our experimental vehicle mounted the CCD camera is shown in Fig.5.3. 
 
USB interface 
 - 40 -
5.2 Experimental results 
We discuss these experimental results in two tasks as follows: 
i. Lane detection in eight different images containing six different weather conditions is evaluated. 
ii. Preceding vehicle detection in five different images containing four different weather conditions 
is evaluated. 
 
5.2.1 Lane detection 
The weather conditions for testing lane detection include: misty day, cloudy day, sunny day, 
dusky day, rainy day, and nighttime are shown in Fig.5.4. 
 
  
(a)         (b) 
  
(c)         (d) 
  
(e)         (f) 
 - 42 -
  
(c)         (d) 
  
(e)         (f) 
  
(g)         (h) 
Fig.5.5. The difference maps for different weather conditions. (a) Cloudy image 1. (b) Cloudy 
image 2. (c) Sunny image 1. (d) Sunny image 2. (e) Dusky image. (f) Rainy image. (g) 
Misty image. (h) Nighttime image. 
 
 - 44 -
 
 
(c) 
 
(d) 
 
 - 46 -
 
(g) 
 
(h) 
Fig.5.6. The results by calculating the maximum/minimum accumulations and the classifications of 
yellow/white and single/double lane marks. (a) Cloudy image 1. (b) Cloudy image 2. (c) 
Sunny image 1. (d) Sunny image 2. (e) Dusky image. (f) Rainy image. (g) Misty image. (h) 
Nighttime image. 
 - 48 -
weather conditions as shown in Fig.5.8 to test our vehicle detection system. 
 
  
(a)                                (b) 
  
(c)                                (d) 
  
(e)                             
Fig.5.8. Test images. (a) Cloudy image 1. (b) Cloudy image 2. (c) Dusky image. (d) Misty image. (e) 
Rainy image. 
 
Based on the features we define, we find out detected boxes which are bounded by the 
templates, left/right borders and shadow line of a vehicle and the results of a detected preceding 
vehicle is shown in Fig.5.9. 
 
 - 50 -
to the detected vehicle number. From the results, we find that the detected rate is high in different 
weather conditions. The average processing time of every function in different weather conditions is 
shown in Table 5.2. 
 
Table 5.1. The Vehicle Detected Rate in Different Weather Conditions 
 #Frame #Car Detected Type1/Type2 Error # 
Detected 
rate 
Dusky image 245 294 245 49/15 83.3% 
Rainy image 526 1778 1703 75/29 95.8% 
Sunny image 196 188 165 23/9 87.8% 
Misty image 375 132 129 3/6 97.7% 
Cloudy image 1 375 542 510 32/23 94.1% 
Cloudy image 2 466 1186 1155 31/14 97.3% 
 
Table 5.2. The Processing Time for Each Function in Second 
 Dusky image 
Rainy 
image 
Sunny 
image 
Misty 
image 
Cloudy 
image  
Lane detection 0.023279 0.027564 0.021780 0.023397 0.026230
Solid/dashed lane 
marks classification 0.000002 0.000002 0.000002 0.000002 0.000002
Yellow/white lane 
marks classification 0.000005 0.000004 0.000005 0.000005 0.000004
Single/double lane 
marks classification 0.000005 0.000005 0.000005 0.000004 0.000005
Direction estimation 0.000007 0.000007 0.000007 0.000007 0.000007
Offset estimation 0.000005 0.000005 0.000005 0.000005 0.000005
Departure warning 0.000002 0.000003 0.000003 0.000002 0.000003
Vehicle detection 0.011902 0.014677 0.010853 0.011919 0.013055
Distance estimation 0.000004 0.000004 0.000004 0.000004 0.000005
Lamp detection 0.007875 0.007996 0.013349 0.007502 0.007733
Total time 0.043086 0.050267 0.046013 0.042847 0.047049
 
 - 52 -
References 
[1] Araki, S., T. Matsuoka, H. Takemura, and N. Yokoya, “Real-time tracking of multiple moving 
objects in moving camera image sequences using robust statistics,” in Proc. IEEE Int’l Conf. 
on Pattern Recognition, Brisbane, Australia, Aug.16-20, 1998, pp.1433-1435. 
[2] Bensrhair, A., A. Bertozzi, A. Broggi, A. Fascioli, S. Mousset, and G. Toulminet, “Stereo 
vision-based feature extraction for vehicle detection,” in Proc. IEEE Intelligent Vehicle 
Sym., Versailles, France, June 7-21, 2002, pp.465-470. 
[3] Bertozzi, M., A. Broggi, and S. Castelluccio, “A real-time oriented system for vehicle 
detection,” Journal of Systems Architecture, pp.317-325, Mar. 1997.  
[4] Bertozzi, M. and A. Broggi, “GOLD: a parallel real-time stereo vision system for generic 
obstacle and lane detection,” IEEE Trans.  Image Processing, vol.7, no.1, pp.62-81, Jan. 
1998. 
[5] Bertozzi, M., A. Broggi, A. Fascioli, and S. Nichele, “Stereo vision-based vehicle detection,” 
in Proc. IEEE Intelligent Vehicles Sym., Dearborn, MI, Oct.3-5, 2000, pp.39-44. 
[6] Betke, M., E. Haritaoglu, and L. S. Davis, “Real-time multiple vehicle detection and tracking 
from a moving vehicle,” Machine Vision and Applications, vol.12, pp.69-83, Sep. 2000. 
[7] Broggi, A., M. Bertozzi, A. Fascioli, C. Guarino Lo Bianco, and A. Piazzi, “Visual perception 
of obstacles and vehicles for platooning,” IEEE Trans. Intelligent Transportation 
Systems, vol.1, pp.164-176, Sep.  2000. 
[8] Bucher, T., C. Curio, J. Edelbrunner, C. Igel, D. Kastrup, I. Leefken, G. Lorenz, A. Steinhage, 
and W. von Seelen, “Image processing and behavior planning for intelligent vehicles,” IEEE 
Trans. Industrial Electronics, vol.50, Issue.1, pp.62-75, Feb. 2003. 
[9] Chen, K.-W., Monocular Computer Vision Technigues for Road and Situation Detection, 
Master thesis, Computer Science and Information Engineering Dept., National Central Univ., 
Chung-li, Taoyuan, Taiwan, 2005. 
[10] Chen, Y.-L., Y.-H. Chen, C.-J. Chen, and B.-F. Wu, “Nighttime vehicle detection for driver 
assistance and autonomous vehicles,” in Proc. IEEE 18th Int’l Conf. on Pattern Recognition, 
Hong Kong, China, Aug.20-24, 2006, pp.687-690. 
[11] Chern, M.-Y. and B.-Y. Shyr, “Locating nearby vehicles on highway at daytime based on the 
front vision of a moving car,” in Proc. IEEE Conf. on Robotics and Automation, Taipei, 
Taiwan, Sep.14-19, 2003, pp.2085-2090. 
[12] Chern, M.-Y. and P.-C. Hou, “The lane recognition and vehicle detection at night for a 
camera-assisted car on highway,” in Proc. IEEE Conf. on Robotics and Automation, Taipei, 
Taiwan, Sep.14-19, 2003, pp.2110-2115. 
[13] Chern, M.-Y., “Development of a vehicle vision system for vehicle/lane detection on 
highway,” in Proc. of the 18th IPPR Conf. on Computer Vision, Graphics and Image 
Processing, Taipei, Taiwan, Aug.21-23, 2005, pp.803-810. 
[14] Chu J., L. Ji, L. Guo, Libibing, and R. Wang, “Study on method of detecting preceding vehicle 
based on monocular camera,” in Proc. IEEE Intelligent Vehicles Sym., Parma, Italy, June 14-17, 
 - 54 -
Vehicles Sym., Paris, France, Oct.24-26, 1994, pp.44-49. 
[30] Regensburger, U. and V. Graefe, "Visual recognition of obstacles on roads," in Proc. IEEE Int’l 
Conf. on Intelligent Robots and Systems, Munich, Germany, Sep.12-16, 1994, pp.980-987. 
[31] Rojas, J. C., and J. D. Crisman, "Vehicle detection in color images," in Proc. IEEE Conf. on 
Intelligent Transportation System, Boston, MA, Nov.9-12, 1997, pp.403-408. 
[32] Shih, H.-W. and D.-C. Tseng, "Visual detection of preceding vehicles and their brake / turn 
lights," in Proc. of the 19th IPPR Conf. on Computer Vision, Graphics and Image Processing, 
Taoyuan, Taiwan, Aug.13-15, 2006. 
[33] Shyr, B.-Y., Daytime Detection of Leading and Neighboring Vehicles on Highway: A Major 
Capability for the Driver Assistant Vision System, Master thesis, Elect. Eng. Dept., National 
Chung Cheng Univ., Chia-yi, Taiwan, 2003. 
[34] Srinivasa, N., "Vision-based vehicle detection and tracking method for forward collision 
warning in automobiles," in Proc. IEEE Intelligent Vehicle Sym., Jun.17-21, 2002, pp.626-631. 
[35] Stein, G. P., O. Mano, and A. Shashua, " Vision-based ACC with a single camera: bounds on 
range and range rate accuracy," in Proc. IEEE Intelligent Vehicle Sym., Ohio, June 9-11, 2003, 
pp.120-125. 
[36] Sukthankar, P., "RACCON: A real-time autonomous car chaser operating optimally at night," 
in Proc. SICE Conf., July 25-27, 2001, pp.38-41. 
[37] Wang, C.-C., Driver Assistance System for Lane Departure Prevention and Collision 
Avoidance with Night Vision, Master thesis, Computer Science and Information Engineering 
Dept., National Taiwan Univ., Taipei, Taiwan, 2004. 
[38] Wang, C.-C., C.-J. Chen, Y.-M. Chan, L.-C. Fu, and P.-Y. Hsiao, "Lane detection and vehicle 
recognition for driver assistance system at daytime and nighttime," in Image and Recognition 
Magazine, vol.12, no.2, 2006, pp.4-17. 
[39] Wu, B.-F., Y.-L. Chen, and C.-C. Chiu, " A discriminant analysis based recursive automatic 
thresholding approach for image segmentation," IEICE Trans. Info. System, vol.E88-D, no.7, 
pp.1716-1723, 2005. 
[40] Zehang, S., G. Bebis, and R. Miller, "On-road vehicle detection using Gabor filters and support 
vector machines," in Proc. Int’l Conf. on Digital Signal Processing, Greece, July 1-3, 2002, 
pp.1019-1022. 
[41] Zehang, S., R. Miller, G. Bebis, and D. DiMeo, "A real-time pre-crash vehicle detection 
system," in Proc. IEEE Conf. on Applications of Computer Vision, Orlando, FL, Dec.3-4, 2002, 
pp.171-176. 
[42] Zehang, S., G. Bebis, and R. Miller, "On-road vehicle detection using optical sensors: A 
review," in IEEE Int’l Conf. on Intelligent Transportation Systems, Washington DC, Oct.3-6, 
2004, pp.585-590. 
[43] Zehang, S., G. Bebis, and R. Miller, "On-Road Vehicle Detection: A Review," IEEE Trans. on 
Pattern Analysis and Machine Intelligence, vol.28, no.5, pp.694-711, 2006. 
[44] Zhihong, Z. and M. Songde, "An efficient vision system for multiple car tracking," in Proc. 
IEEE Int’l Conf. on Pattern Recognition, Quebec, Canada, Aug.11-15, 2002, pp.609-612. 
 - 56 -
報告內容 2. 駕駛昏睡狀態偵測與警示 
 
1. Introduction 
In this chapter, we will describe the motivation of our study, present the system overview of 
the drowsiness detection and warning system, and give the organization of this part. 
 
1.1 Motivation 
With the development of economics, vehicle is essential to people’s life. However, the 
increasing number of traffic accidents due to diminished driver’s vigilance level has become a 
serious problem for society. Many factors could contribute to drowsiness or fatigue, such as long 
working hours, lack of sleep, or the use of medication. Besides, another important factor of 
drowsiness is the monotonous driving on highways [17]. According to the National Highway Traffic 
Safety Administration (NHTSA), approximately 100,000 drowsy-related crashes occurs every year 
result in 1550 fatalities, 71,000 non-fatal injuries and about $12.5 billion annual money loss. Driver 
fatigue is a significant factor in a large number of vehicle accidents. In recent years, the researches 
on Intelligence Transportation System (ITS) and Advanced Safety Vehicle (ASV) are more and more 
popular in many countries of the world. 
In order to insure the safety of the driver and pedestrians on the road, we developed a real-time 
system to detect driver fatigue. We use computer vision and image processing techniques to extract 
vision cues and determine whether the driver is drowsy. Via the system the warning signals will be 
issued in time when an accident is going to happen.  
      
1.2 System overview 
The purposed driver drowsiness and warning system consists five major components as shown 
in Fig. 1.1. We briefly describe them as follows: 
(i) Active image acquisition system: In order to work under various illumination conditions such 
as day and night, we use an IR camera with a two-side infrared illuminators to acquire the 
driver’s pupil and face image.  
(ii) Eye detection: Firstly, to deal with the uneven shaded image, we divide the image into several 
regions. Secondly, we execute iterative thresholding in each region individually for forming a 
bi-level image. After obtaining a binary image, we apply connected component generation with 
geometric constrains to find candidates of eye region. We filter the candidates of eye region by 
Supported Vector Machine (SVM) then use some constrains to verify an eye pair.  
(iii) Eye tracking: When eye detection is success in three consecutive frames, the procedure is 
turned to eye tracking phase. Based on the positions of eyes, we estimate a face region. We 
detect the eye in the predicted region. If we miss tracking the eyes in three consecutive frames, 
the procedure will be returned to eye detection phase.  
(iv) Visual cues extraction: Based on the information of eyes and face position, we can extract 
 - 58 -
2. Related Works 
In these decades, many efforts had been reported in the literature for developing active safety 
systems for reducing the number of car accidents due to reduced vigilance. Driver drowsiness 
detection is one of important tasks for the safety vehicle.  
In this chapter, we will describe several related works about the driver drowsiness detection 
system. 
  
2.1 Techniques for detecting driver drowsiness 
Ueno et al. [23] presented possible techniques for detecting drowsiness of drivers can be 
generally divided into five major categories, as shown in Table 2.1. 
Among these different methods, the techniques that are best, based on accuracy are the ones 
based on physiological phenomena, which can be implemented in two ways.  
One way would be measure changes in physiological signals, such as brain waves, heart rate, 
eye blinking, pulse rate, and skin electric potential. This approach is most accurate, but not practical, 
since the sensing electrodes would have to be attached directly on the body and hence be annoying 
to drivers. It also has the disadvantage of being ill-suited to measurement over a long period of time 
owning to the large effect of perspiration on the sensors. 
 
Table 2.1. Techniques for Detecting Drowsiness 
Detection techniques Description Accuracy Practically Extendibility
Physiological 
signals 
Detection by changes in brain 
waves, blinking, heart rate, skin 
electric potential, etc. 
◎ × △ 
Sensing of 
human 
physiological 
phenomena 
Physical 
reactions 
Detection by changes in 
inclination driver’s head, 
sagging posture, frequency at 
which eyes close, gripping force 
on steering wheel, etc. 
◎ ○ ◎ 
Sensing of driving operation 
Detection by changes in driving 
operations (steering, accelerator, 
braking, shift lever, etc.) 
○ ◎ × 
Sensing of vehicle behavior 
Detection by changes in vehicle 
behavior (speed, lateral G, yaw 
rate, lateral position, etc.) 
○ ◎ × 
Response of driver Detection by periodic request for response. △ × ◎ 
Traveling conditions 
Detection by measurement of 
traveling time and conditions 
(daytime or night time, speed, 
etc.) 
× ○ ◎ 
◎ : very good  ○: good  △: average  ×: poor 
 
 - 60 -
 
Fig.2.2. FaceLAB® eye and head tracking system. 
 
Among the drowsiness detection measures and technologies evaluated in various studies, 
PERCLOS (Percent Eye Closure) methodology, a video-based method that measure eye closure, is a 
reliable and valid determination of a driver’s alertness. Copilot® as shown in Fig.2.3 (a), a drowsy 
driver monitor developed by Robotics Insitute in Carnegie Mellon University is a video-based 
system for measuring slow eyelid closure as represent by PERCLOS [10]. PERCLOS is the 
proportion of total time that the driver’s eyelids are closed 80% or more and reflects slow eyelid 
closures rather than blinks [4, 9, 10]. High PERCLOS values are strongly related to drowsiness. 
DD850 Driver Fatigue Monitor® as shown in Fig.2.3 (b) is a product of Attention Technologies. It 
is the market version of research that has been published under PERCLOS monitor and Copilot®. 
An audible alarm sounds when the unit detects that the driver is getting drowsy. A visual feedback 
shows the driver how long his eyes were closed and how far he drove. 
 
  
(a)                      (b) 
Fig.2.3. The drowsy driver monitors for monitoring eyelid closures. (a) Copilot®. (b) DD850 
Driver Fatigue Monitor®. 
 
2.2 Eye detection  
Eyes are the most important face organs for face hence eye detection is a crucial step in face 
recognition. 
Nixon [19] used the Hough transform for eye detection. The eye is modeled by a circle for the 
iris and a tailored ellipse for the sclera boundary. The Sobel gradient operator is applied to the initial 
image. Thresholding the resulting gradient image produced an edge image and then search eyes on 
the upper half of the image. The method is time-consuming, needs a high contrast eye image, and 
 - 62 -
 
Kawato and Tetsutani [16] proposed a circle-frequency filter (CF filter) to detect and track the 
point between the eyes as a face feature point. The CF filter is described as 
( ) ( ) ( )
21
0
21
0
4sin4cos ⎟⎠
⎞⎜⎝
⎛ ∑+⎟⎠
⎞⎜⎝
⎛ ∑=
==
-N
k
k
-N
k
k N / kfN / kfy x,f ππ ,                               (2.1) 
where fi, i = 0, …, N-1 are gray levels of pixels along a circle centered at (x, y). An example of fis at 
“between-eyes” is shown in Fig.2.5. 
 
  
Fig.2.5. An example of “between-eyes” and pixel values along a circle. 
 
D’Dorzio et al. [5] proposed an algorithm for eyes detection that used iris geometrical 
information for determining in the whole image. Since the iris is always darker than the sclera no 
matter what color it is, the edge of the iris is relatively easy to detect. They presented a circle 
detection operator based on the directional Circle Hough Transform. The masks are shown in 
Fig.2.6 represent in each point the direction of radial vector scaled by the distance from the center 
in a ring with minimum radius Rmin, and maximum radius Rmax. The circle detection operator is 
applied on the whole image without any constraint on plain background or limitations on eye 
regions. The maximum value M1 of the output convolution on the whole image is best candidate to 
contain an eye, then search the second maximum value M2 in the regions that are candidate to 
contain the second eye. Then verify whether M1 and M2 are eye pairs or not. 
 
   
Fig.2.6. The masks of dimension (2Rmax+1)×(2Rmax+1) that are convoluted with the gradient image. 
 
2.3 Face detection  
Smith et al. [21, 22] presented a system for analyzing driver alertness. It relies on estimation of 
 - 64 -
 
 
Fig.2.8. The whole procedure of sub-image homomorphic filtering. 
 - 66 -
 
 
Fig.3.2. The IR illuminators. 
 
 
Fig.3.3. Connection diagram of image acquisition system. 
 
3.1 Dividing the image into several regions 
By utilizing the property that the iris and pupil are always darker than the rest part of the eye 
ball and the skin, we can find the eyes in a face image via image thresholding. However, in the case 
of a moving car, the result of thresholding is affected by the changes of illumination and 
background. Some examples of uneven shaded images are shown in Fig.3.4. A single threshold will 
not work well when we have uneven illumination due to shadows or due to the direction of 
illumination. In order to avoid these interferences, we divide the image into several regions and then 
choose a threshold for each region. In this section, we will describe how to divide the image. 
 
 - 68 -
 
3.1.1.2. Finding the local maxima of the projection chart 
In the projection chart, we will determine the local maxima as the positions for division. To 
simplify the determination of local maxima, the accumulation values of horizontal difference values 
in each column are quantized into 100 discrete levels. 
100)()(
max
×=
G
iGiGa ,                                                         (3.1) 
where Ga(i) is the adjusted accumulation of horizontal differernce value in the ith column, G(i) is 
the accumulation of horizontal difference value in the ith column, and Gmax is the greatest 
accumulation of horizontal difference value. 
Before determine the local maxima, we define several feature points as follows: 
y Start position (ps): the start position in a local maximum search. 
y Start value (vs): the vertical accumulation of horizontal gradient at the start position. 
y End position (pe): the end position in a local maximum search. 
y End value (ve): the vertical accumulation of horizontal gradient at the end position. 
y Local maximum position (plmax): the position where has the largest vertical accumulation of 
horizontal gradient in a local maximum search. 
y Local maximum value (vlmax): the vertical accumulation of horizontal gradient at the local 
maximum position. 
y Left_height (hleft): the difference between vlmax and vs. 
y Right_height (hright): the difference between vlmax and ve. 
The illustration of these feature points are shown in Fig.3.7.  
 
 
Fig.3.7. The feature points of a local maximum. 
 
We search local maxima from left to right in a search region. The local maxima search 
algorithm is described as follows: 
Step 1. Set the initial values of Left_height and Right_height to 0. 
Step 2. From start position ps, we scan the projection chart from left to right. When the ripple is up, 
 - 70 -
 
   
(a)                              
 
(b) 
   
(c) 
Fig.3.10. The illustrations of dividing the image horizontally. (a) The selected regions. (b) 
Horizontal projection charts in reach region. (c) Local maxima of the horizontal 
projection charts. 
 
 - 72 -
bounding box of each blob 
Let I be a bi-level image and F and B be the foreground and background pixel subsets in I, 
respectively. A connected component of I, here referred to as C is a subset of F of maximal size 
such that all the pixels in C are connected. Two pixels, p and q are connected if there exists a path of 
pixels (p0, p1, …, pn ) such that p0 = p, pn = q and for all i which fell into 1 to n, pi-1 and pi are 
neighbors. Here, the definition of connected component relies on that of a pixel’s neighborhood: if 
all paths between pixels in C are 4 connected, then C is a 4-connected component. The classical 
sequential algorithm for labeling connected components consists of two subsequent raster-scans of I. 
In the first scan a temporary label is assigned to each pixel F based on the values of its neighbors 
already visited by the scan. For 4-neighbor connected components, the pre-visited neighbors are 
upper and left neighbor pixels. 
As a result of the scan, no temporary label is assigned to pixels belonging to different 
components, but different labels may be associated with the same component. Therefore, after 
completion of the first scan equivalent labels are sorted into equivalent classes and a unique class 
identifier is assigned to each class. And then a second scan is run over the image so as to replace 
each temporary label by the class identifier of its equivalence class. The classical sequential method 
is not efficient enough. So we implement a simple and efficient 4-connected components labeling 
algorithm.  
 
3.4 Geometric constrains 
After connected-component generation, we obtained many white candidate blobs. We can find 
the eyes somewhere in these candidates. However, it is too difficult to isolate eye blob unless we 
pick the accurate threshold value, since eyes are usually small and not bright enough compared with 
other noise blobs. Thus we will have to make use of other information to identify them. Let W and 
H be the width and height of the input image respectively. We define several geometric constraints 
based on the geometric shapes as follows: 
i. 0.007% (W×H) < blob size < 0.4% (W×H), 
ii. 1% W < width of bounding box ≤ 12.5% W, 
iii. 1% H < height of bounding box ≤ 12.5% H, and 
iv. (bounding box size – blob size) < 0.13% (W×H). 
We take the blobs that are satisfied the constraints as eye candidates. The bounding boxes of 
connected components are shown in Fig.3.13.    
 
 - 74 -
(maximum) margin are called the support vectors since they alone define the optimal separating 
hyper plane. 
The reason for mapping the input into a higher dimension space is that this mapping leads to 
better class separability. The complexity of SVM decision boundary, however, is independent of the 
feature Z space dimensionality, which can be very large (or even infinite). SVM optimization takes 
advantage of the fact that the evaluation of the inner products between the feature vectors in a high 
dimensional feature space is done indirectly via the evaluation of the kernel H between support 
vectors and vectors in the input space 
( ) ( )xxzz ′⋅=′⋅ H , (3.6) 
where the vectors z and z’ are the vectors x and x’ mapped into the feature space. In the dual form, 
the SVM decision function has the form 
( ) ( )x xx ′∑=
=
,
1
i
M
i
ii HyD β . (3.7) 
The RBFs kernels H are given by 
( )
⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧
−
−=′ 2
2
i
i σ
H
xx
xx exp, , (3.8) 
and the corresponding SVM hyper-planes are defined then as 
( ) ⎟⎟⎠
⎞
⎜⎜⎝
⎛
∑ +⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧
−
−=
=
M
i 2
2
i
i b
σ
signf
1
exp
xx
x β , (3.9) 
and can be fully specified using dual quadratic optimization in terms of the number of kernels used 
M and their width. 
The polynomial kernels H of degree q are given by 
( )[ ]qH 1),( +′⋅=′ xxxx , (3.10) 
and the corresponding SVM hyper-planes are defined as 
( ) ( )[ ] ⎟⎠
⎞⎜⎝
⎛
++′⋅= ∑
=
M
i
q
i bsignf
1
1xxx β . (3.11) 
 
3.5.2 Training data for SVM 
Training data are needed to obtain the optimal separating hyperplane. We set 30×20 pixels as 
the size of our training data. The training images are processed using histogram equalization and 
each pixel is normalized to a range between 0 and 1 before training. The eye training images are 
divided into two sets: positive set and negative set. In the positive image set, we include eye images 
of different gazes, different degrees of opening, and with/without glasses. The non-eye images are 
placed in the negative image set. Selection of proper non-eye images is very important to train SVM 
because the performance of SVM is influenced by what kinds of non-eye images are used. We use 
non-eye images similar to eye such as eyebrows, nostrils and other eye-like patches. Some 
examples of eye and non-eye images in the training sets are shown in Fig.3.14. SVM can work 
under various illumination conditions due to the intensity normalization for the training images via 
histogram equalization. The result of eye blobs verification by SVM is shown in Fig.3.15. 
 - 76 -
 
3.6 Verification of an eye-pair 
Via SVM classification, most non-eye blobs are removed and eye blobs are retained. We further 
define some constrains to verify an eye-pair as follows: 
i. The distance between two centers of eyes is larger than 10% image width. 
ii. The distance in x-coordinate between two centers of eyes is smaller than 33.3% image width. 
iii. The distance in y-coordinate between two centers of eyes is smaller than 12.5% image height. 
 
 
 - 78 -
 
4.1 Predict the eye position 
The concept of prediction and tracking is shown in Fig.4.2. in which the position at time t+1 is 
predicted from the position and velocity of eye blob at times t, t-1, and t-2. 
 
 
Fig.4.2. The concept of prediction and tracking. 
 
We used the concept of Kalman filter [28] to predict the new position of the eye. For a moving 
object, we predict its new position as follows: 
S(t+1) = S(t) + V(t)Δt,                                                 (4.1) 
where S(t+1) is the predicted position at time t+1, S(t) is the position of an eye at time t, andΔt is 
the interval time between consecutive two frames. 
We then find the eye in the new position and update the velocity. The process of updating 
velocity is shown as 
V(t) = (1−α)V(t−1) + αZ(t),                                              (4.2) 
Z(t) = (S(t) − S(t−1))/ Δ t, (4.3) 
where Z(t) is the correct velocity of the tracking eye at time t, V(t) is the weighted velocity at time t, 
V(t−1) is the weighted velocity at time t−1, and α is a weight. Sometimes, our three strategies are 
failed fail because the eye is covered by something. If our three strategies are fail in tracking phase 
at time t, equation 
S(t+1) = S(t−1) + V(t−1)Δt, (4.4) 
is used to update the predicted parameter. 
When tracking is success in the next frame, we normally update the predicted parameter; 
otherwise tracking fail update is done again. If the tracking failed in consecutive three frames, the 
procedure is returned to detection phase, and the predicted parameters will be clear and reinitialized. 
The complete diagram of our predicted operation is shown in Fig.4.3. 
 
 
 - 80 -
 
4.3 Face position estimation 
In order to make our system for eye tracking more efficiently and avoid the interference of 
background, we propose a method to estimate the face position based on the ellipse filter.  
Based on the positions of left/right eyes in detection stage, we can use connected-component 
algorithm to find the maximum blob and its center. Then we take two centers of left/right eyes to 
calculate the distance and angle. 
We design an ellipse filter based on the shape of human face. We use two methods to deal with 
the following cases individually: 
Case 1. The angle is equal to zero: we take two centers of left/right eyes named (Xpl, Ypl) and (Xpr, 
Ypr) and the distance between two eyes named Dis. We first find the center of (Xpl, Ypl) and 
(Xpr, Ypr), and add 0.2×Dis to y-coordinates to find the center (Xe, Ye) of the ellipse. We 
define the initial length of major axis is 2.4×Dis and the initial length of minor axis is 
1.6×Dis. Here we obtain an initial ellipse and then we design a method to expand the ellipse 
to find the face edges. We divide equally the length of major/minor axis into forty parts, and 
based on these divisions we obtain several ellipses. In order to reduce the computing time, 
we execute the vertical edge detection focused on the edge of our ellipses and accumulate 
the difference values. When the maximum difference value occurs, we can choose this 
ellipse position as our suitable face position. The illustrations are shown in Fig.4.4. 
Case 2. The angle is not equal to zero: we take two centers of left/right eyes named (Xpl, Ypl) and 
(Xpr, Ypr) and the distance between two eyes named Dis. We first find the center of (Xpl, 
Ypl) and (Xpr, Ypr), and add 0.2×Dis to y-coordinates to find the center (Xe, Ye) of the 
ellipse. We define the initial length of major axis is 2.4×Dis and the initial length of minor 
axis is 1.0×Dis. Here we obtain an initial ellipse and then we design a method to expand the 
ellipse to find the face edges. We divide equally the length of major/minor axis into forty 
parts, and based on these divisions we obtain several ellipses. By our observation, we add a 
condition based on the angle between two centers of left/right eyes to rotate these ellipses to 
form more ellipse candidates. In order to reduce the computing time, we execute the vertical 
edge detection focused on the edge of our ellipses and accumulate the difference values. 
When the maximum difference value occurs, we can choose this ellipse position as our 
suitable face position. The illustrations are shown in Fig.4.5. 
 
 
 
 
 
 - 82 -
 
(a) 
 
(b) 
 
(c) 
 
(d) 
Fig.4.5. An example of face position estimation for Case 2. (a) Calculating the distance and angle 
between two centers of left/right eyes. (b) The initial ellipse. (c) Rotating the ellipse to 
obtain more ellipse candidates. (d) Expanding the initial ellipse to find the maximum 
accumulation of difference values. 
(Xpr, Ypr) 
(Xpl, Ypl) 
Dis 
Angle 
(Xe, Ye) 
2.4×Dis 
1.0×Dis 
0.2×Dis 
 - 84 -
 
(a) 
 
(b) 
Fig.5.2.  Examples of training images for SVM. (a) Open eye images. (b) Closed eye images. 
 
5.1.2 Eye open/closed judgment by the ratio of eye’s height 
Here we propose a method to judge open/closed eye based on the degree of openness. First, we 
use a mask to find the center of pupil. The mask is shown in Fig.5.3.  
We take the pupil center (xp, yp) as shown in Fig.5.4 (a) as our calculating point and find the 
two largest difference values upward and downward along y-axis direction. By our observation in 
the situation of dark pupil images, the difference between iris and pupil is usually larger than the 
difference between iris and eyelid. In order to avoid the interference of dark pupil, we shift the xp 
position to set two new calculating points as: 
xpl = xp − 2, and xpr = xp + 2.  (5.1) 
After obtaining two new calculating points, we calculate the largest difference values upward 
and downward along y-axis direction for each calculating points individually. We obtain four points 
 - 86 -
5.2 Drowsiness discrimination 
Of the drowsiness-detection measures, the measure referred to as PERCLOS was found to be 
the most reliable and valid determination of a driver’s fatigue level [4]. High PERCLOS values are 
strongly related to drowsiness. PERCLOS is the percentage of eyelid closure over the pupil over 
time and reflects slow eyelid closures rather than blinks. Since our input is a sequence of video 
frames, we measure the driver’s fatigue level by calculating the percentage of closed eye frames in 
a specific time interval. The definition of PERCLOS in our proposed system is described as 
number frame total
number frame close eye
=PERCLOS .                                        (5.3) 
The system will trigger an alarm if the computed PERLOCS value exceeds the given threshold 
to alert the driver. 
 
5.3 Gaze direction estimation 
Gaze is also a useful visual cue for driver monitor system because it reveals one’s need and 
attention. Here we propose a method to estimate the gaze direction. The detail steps of gaze 
direction estimation are described as follows: 
Step 1: After obtaining two eye regions and their center points, we use the vertical difference 
operator to detect horizontal edges as the upper and lower eyelids. Then we translate it to be 
a bi-level image. 
Step 2: We extract the connected components from the bi-level image, and find the maximum white 
block. According to the block, we can find the left/right points of the eye. 
Step 3: Base on the left/right points, we calculate the distance L along x-axis. Then we divide the 
distance into tree equal parts. 
Step 4: Based on the positions of left/right eyes’ centers named (Xpl, Ypl) and (Xpr, Ypr), we can 
classify the gaze direction as following cases: 
Case 1. When (Xpl, Ypl) and (Xpr, Ypr) are fell in region A or (Xpl, Ypl) is fell in region B 
and (Xpr, Ypr) is fell in region A, we classify the gaze direction into right side. 
Case 2. When (Xpl, Ypl) and (Xpr, Ypr) are fell in region C or (Xpl, Ypl) is fell in region C 
and (Xpl, Ypl) is fell in region B, we classify the gaze direction into left side. 
Case 3. When (Xpl, Ypl) and (Xpr, Ypr) are both fell in region B, we classify the gaze 
direction into front side. 
Case 4. Here we propose a method to deal with the case when (Xpl, Ypl) is fell in region C 
and (Xpr, Ypr) is fell in region A. The method is to calculate the distances of (Xpl, 
Ypl) to left point and (Xpr, Ypr) to right point along x-axis. When the distance of (Xpl, 
Ypl) to left point is larger than (Xpr, Ypr) to right point, we consider the gaze 
direction is at right side; otherwise, we consider the gaze direction is at left side. 
Illustrations of left eye for gaze direction estimation are shown in Fig.5.5. 
 - 88 -
6. Experiments 
Several experiments and comparisons for our drowsiness detection system are reported in this 
chapter. First, we introduce our development environment. Then we demonstrate several 
experimental results of eye detection and drowsiness discrimination in different conditions. We 
compare the experimental results with some other methods. 
 
6.1 The development environment 
In order to develop safe driving system, our system must be reliable, robust, and real-time for 
various driving conditions. All algorithms are implemented in C++ programming language and 
Microsoft Foundational Classes (MFC) Library, and all experiments are executed on a general PC 
with Intel® CoreTM2 Duo 2.13GHz CPU and 1GB DDR RAM and Microsoft® Windows XP 
professional operating system as our experimental platform. In our experiment, we use the image 
size of video is 320×240 pixels with 30 fps in RGB color space. 
In order to prove the correctness and accuracy of our driver drowsiness detection system, we 
test our proposed system on the experimental vehicle which runs on the urban road with various 
illuminated environments. The IR illuminated camera with two illuminators is mounted on the 
ceiling of our experimental vehicle as shown in Fig.6.1. 
 
 
Fig.6.1. The experimental car with IR illuminated camera. 
 
6.2 Experimental results 
We discuss several experimental results in three tasks as follows: 
A. Eye detection and tracking in different images with various illumination conditions. 
B. Eye statuses estimation. 
C. Drowsiness discrimination by PERCLOS measurement. 
We give the comparison of the different methods: 
A. Local maxima searching of the projection chart. 
 - 90 -
     
(g)                                (h) 
Fig.6.2. Test images of various illuminations. (a) Normal light image. (b) Sunny image. (c) Sunny 
image with glasses. (d) Nighttime image. (e) Nighttime image with glasses. (f) Nighttime 
image with glasses and the face is illuminated by strong light. (g) Uneven illuminated 
image 1. (h) Uneven illuminated image 2. 
 
6.2.1 Eye detection and tracking 
The results of eye detection in different conditions are shown in Fig.6.3. When eye detection is 
success in three consecutive frames, we track the eyes in the predicted region. The results of eye 
tracking in consecutive frames in different conditions are shown in Figs.6.4-6.6. We can correctly 
track the eyes when the head is rotating or the eyes are temporarily covered by something. The 
detection rate of eye detection is shown in Table 6.1. The #Frame is the number of frames in a 
video. The #Detected is the number of detected eyes. The #False indicates the number that it is not 
eye but detected. The Detection rate is the ratio of #Correct to #Frame. The False rate is the ratio of 
#False to #Frame. 
 
   
(a)                                (b) 
 - 92 -
    
(a)                                 (b) 
   
(c)                                 (d) 
   
(e)                                 (f) 
   
(g)                                 (h) 
Fig.6.4. Results of eye tracking in a sequence of consecutive frames in normal light condition. 
 
 - 94 -
   
(i)                                 (j) 
Fig.6.5. Results of eye tracking in a sequence of consecutive fames. We can correctly track the eye 
even though it is temporarily covered by something.  
 
   
(a)                                 (b) 
   
(c)                                 (d) 
   
(e)                              (f) 
 - 96 -
  
(b) 
  
(c) 
  
(d) 
Fig.6.7. Test images for eye status estimation. (a) The eyes are open and gaze direction is front. (b) 
The eyes are open and gaze direction is right. (c) The eyes are open and gaze direction is 
left. (d) The eyes are closed. 
 
6.2.3 Drowsiness discrimination by PERCLOS measurement 
We calculate PERCLOS value every 0.1 second, take 3 seconds as our calculated interval, and 
set 0.3 as our threshold. If the PERCLOS value exceeds the threshold, the proposed system shows a 
red light signal and triggers an audio alarm to warn the driver. Four examples of drowsiness 
discrimination and warning by PERCLOS measurement are shown in Fig.6.8. In the PERCLOS 
graph, “D” means drowsy and “S” means sober. The green light indicates the driver is sober, the red 
light indicates the driver is drowsy. The drowsiness detection and false alarm are shown in 
 - 98 -
 
(c) 
 
(d) 
Fig.6.8. Four examples of drowsiness discrimination and warning by PERCLOS measurement. (a) 
The diver is sober in normal light. (b) The driver is drowsy in normal light. (c) The driver 
is drowsy at night. (d) The driver is drowsy in uneven illumination condition. 
Red light 
Red light 
 - 100 -
   
(a)                                (b) 
   
(c)                                 (d) 
Fig.6.10. An example of uneven illuminated image. (a) Original image. (b) Global thresholding. (c) 
Local thresholding with four partitions. (d) Local thresholding by our proposed method. 
 
   
(a)                              (b) 
   
(c)                              (d) 
Fig.6.11. An example of uneven illuminated image. (a) Original image. (b) Global thresholding. (c) 
Local thresholding with four partitions. (d) Local thresholding by our proposed method. 
 - 102 -
7. Conclusions and Future Works 
In this chapter, we make some conclusions for our proposed driver drowsiness detection 
system and give some suggestion for the future works 
 
7.1 Conclusions 
In this study, algorithms based on computer vision of eye detection and drowsiness 
discrimination are proposed for various illumination conditions on highway or urban roads. We 
measure the driver’s alertness level by the proportion of time that the driver’s eyelids are closed. 
We extract the driver’s eyes from face images by bi-level thresholding and SVM classification. To 
handle the nonuniform illumination conditions, we propose a local thresholding method which 
partition the image into several subimages and then use iterative thresholding algorithm to find a 
threshold value for each sub image. We take the vertical and horizontal positions of strong edges as 
the partition lines. We observe that our local thresholding method can achieve better result than 
global thrasholding or four-part thresholding on the uneven illuminated images. 
After finding the eyes, we detect eyes’ status including eyes open/close and gaze direction. We 
calculate PERCLOS values to determine whether the driver is drowsy and issue a warning. The 
average processing time is 0.057045 seconds，processing rate is 18 frames per seconds. 
 
7.2 Future works 
So far, we can extract the eyes from the images in various illumination conditions. To achieve 
more robust driver alertness monitoring system, we can combine the information of face orientation. 
The face orientation can be determined by the relative positions of eyes and mouth. However, 
compare with eye detection, mouth detection is more difficult because mouth color is more closed 
to skin color. Facial features are indistinct on strong light conditions because the captured images 
are overexposure. The above problems will be the next challenges as our feature works. 
 - 104 -
[15] Ji, Q. and Z. Zhu, “Robust real-time eye detection and tracking under variable lighting 
conditions,” Computer Vision and Image Understanding, vol.98, issue 1, pp.124-154, Apr. 
2005. 
[16] Kawato, S. and N. Tetsutani, "Circle frequency filter and its application," in Proc. Int. 
Workshop on Advanced Image Technology, Taejon, Korea, Feb.8-9, 2001, pp.217-222. 
[17] Lin, C.-T., R.-C. Wu, T.-P. Jung, S.-F. Liang, and T.-Y. Huang, “Estimating driver performance 
based on EEG spectrum analysis,” EURASIP Journal on Applied Signal Processing, vol.2005, 
issue 1, pp.3165-3174, Jan. 2005. 
[18] Lin, S.-M., A Real-Time Driver Drowsiness Detection and Alertness Monitor System, Master 
thesis, Computer Science and Information Engineering Dept., National Central Univ., Chungli, 
Taiwan, 2007. 
[19] Nixon, M., “Eye spacing measurement for facial recognition,” in Proc. SPIE Applications of 
Digital Image Processing VIII, San Diego, CA, Aug. 20-22, 1985, pp.279-285. 
[20] Park, I., J. H. Ahn, and H. Byun, “Efficient measurement of eye blinking under various 
illumination conditions for drowsiness detection systems,” in Proc. 18th Int. Conf. Pattern 
Recognition, Hong Kong, Aug. 20-24, 2006, vol.1, pp.383-386. 
[21] Smith, P., M. Shah, and N. da Vitoria Lobo, “Monitoring head/eye motion for driver alertness 
with one camera,” in Proc. of 15th Int. Conf. Pattern Recognition, Barcelona, Spain, Sep.3-7, 
2000, pp.636-642. 
[22] Smith, P., M. Shah, and N. da Vitoria Lobo, "Determining driver visual attention with one 
camera," IEEE Trans. on Intelligent Transportation Systems, vol.4, issue 4, pp.205-218, Dec. 
2003. 
[23] Ueno, H., M. Kaneda, and M. Tsukino, “Development of drowsiness detection system,” in Proc. 
Conf. Vehicle Navigation and Information Systems, Yokohama, Japan, Aug.31-Sep.2, 1994, 
pp.15-20. 
[24] Vapnik, V. N., The Nature of Statistical Learning Theory, 2nd Ed, Springer-Verlag, New York, 
1999. 
[25] Wang, J.-W. and W.-Y. Chen, “Eye detection based on head contour geometry and wavelet 
subband projection,” Optical Engineering, vol.45, issue 5, May 2006, pp.057001.1-057001.12. 
[26] Wang, Q. and J. Yang, “Eye location and eye state detection in facial images with 
unconstrained background,” Journal of Information and Computer Science, vol.1, no.5, 
pp.284-289, Dec. 2006. 
[27] Wang, Q., J. Yang, M. Ren, and Y. Zheng, “Driver fatigue detection: a survey,” in Proc. 6th 
World Congress on Intelligent Control and Automation, Dalian, China, June 21-23, 2006, 
pp.8587-8591. 
[28] Welch, G. and G. Bishop, An Introduction to The Kalman Filter, SIGGRAPH 2001 Course 
Notes, 2001. 
[29] Yuille, A. L., D.S. Cohen, and P. W. Hallinan, “Feature extraction from faces using deformable 
templates,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, San Diego, CA, 
Jun.4-8, 1989, pp.104-109. 
 - 106 -
法都要先知道相機架設的俯角，我們的方法可以直接估計出相機架設的俯角。 
10.車道偏離偵測與前車防撞警示的技術已獲得中華民國及美國的發明專利。 
在駕駛人的眼睛偵測、眼睛開合分析、臉部方向偵測、視線方向估計、及昏睡判定的研
究中，我們的技術具有下列多項特色： 
1. 利用不可見光的紅外線照射臉部，讓系統在任何亮度環境下都能正常運作。 
2. 可以在白天及夜晚等不同環境下都使用同一套演算法，簡化系統的負擔。  
3. 針對灰階影像所設計的臉部特徵擷取方法，適用於各種不同照度及膚色的臉部偵測及分
析，維持系統的穩定及即時運作特性。 
4. 能夠處理及分析不均勻照度下的臉部影像。這是車上駕駛環境一定會遇到的問題，但過去
的研究幾乎都不曾考慮過。 
5. 使用雙重資訊 (multi-cue)：閉眼時間及車道偏離分析，來判定是否昏睡，可增加系統的可
靠度。 
6. 同時監控駕駛人的昏睡狀態及注意力 (視線方向與臉部方向)，可提升安全防護的效果；因
為除了疲勞駕駛之外，分心也是另一項影響駕駛安全的重要因素。 
我們非常專注且長時間的投注在本研究上，已有一些成果產出，包括著作、專利、碩士
論文、及得獎等；另外，我們也吸引一些相關企業來進行產學計畫合作：中科院第二所、工
研院機械所、車輛研究測試中心、及私人公司。希望更遠的未來，能將此技術改進的更精良，
並與未來發展的技術整合成更完整的系統；例如， 
i. 改進消失點精確度，提高偵測的準確度。 
ii. 發展夜間偵測功能。 
iii. 發展多相機偵測系統。 
iv. 加入雷射雷達、毫米波雷達、或微波雷達偵測系統。 
v. 發展車隊監控/管理與車內監控系統。 
vi. 結合車上的車牌定位與辨識系統。 
vii. 配合微軟車上影音多媒體系統發展車內娛樂與學習系統。 
viii. 發展新功能；例如，側方及盲點偵測、駕駛人精神狀態偵測、.. 。 
 - 108 -
D. 得獎 
a. 2006 第一屆由田機器視覺獎 (UTMVP) 創新類第二名 
題目：先進安全車輛的多樣性視覺偵測技術 
b. 2007 第三屆車輛工程研究論文獎競賽銅質獎 
題目：先進安全車輛的道路線及前車視覺偵測技術 
c. 2008 中華民國影像處理與圖形識別研討會 (CVGIP) 佳作論文獎 
題目：A Bird-View Surrounding Monitor System For Parking Assistance 
d. 2008 第三屆由田機器視覺獎 (UTMVP) 創新類佳作獎 
題目：強健性車道偏離警示系統 
  
 - 110 -
 
技術說明 
 
先進安全車輛的視覺偵測應用不是創新，國內外都有一些相關的
研究，而且國外已有一些產品問世了；但我們專攻視覺偵測，不像某
些單位只把視覺偵測視為附屬的工具，或者只是用來爭取研究計畫而
已；因此在視覺偵測的技術發展上，我們做得廣泛，而且使用的理論、
觀念、與技術也最精專；例如，許多研究採用 Canny operator 或 Sobel 
operator 做邊界偵測，但卻不知道在什麼情形下才需要用這些運算，更
不管這些運算有多耗時；Canny operator 及 Sobel operator 是我們目前
做邊界偵測的 20 多倍及 7 倍的運算量。 
我們的研究強調準確、快速、與實用，以實際做出視覺偵測設備
為首要目標，其次才是申請專利及發表論文。我們非常專注且長時間
的投注在本研究上；而且也吸引中科院第二所、工研院機械所、車輛
研究測試中心及私人公司主動找我們合作。希望更遠的未來，能將此
技術改進的更精良，並與未來發展的技術整合成更完整的系統；例如，
(i) 改進消失點精確度，提高偵測的準確度。 
(ii) 發展夜間偵測功能。 
(iii) 發展多相機偵測系統。 
(iv) 加入雷射雷達、毫米波雷達、或微波雷達偵測系統。 
(v) 發展車隊監控/管理與車內監控系統。 
(vi) 結合車上的車牌定位與辨識系統。 
(vii) 配合微軟車上影音多媒體系統發展車內娛樂與學習系統。 
(viii) 發展新功能；例如，側方及盲點偵測、高速公路出入口偵測、駕
駛人精神狀態偵測、行人及外物動向偵測、..。 
(ix) 與國外相關產品比較或以國外政府機構所訂定的標準測試。 
 
  
技術說明 
 
英文： 
This research just aims the traffic problems to develop the 
computer-vision techniques to aid road vehicle driving for safety. The 
proposed computer vision system almost covers all functions of detection 
and surveillance for safe driving; including 1.road situation detection, 
2.driving environment detection, 3.driver drowsiness detection, 4.driving 
behavior detection, 5.pedestrian/object motion detection, 6.vehicle 
surround surveillance, and 7.others, such as stereo vision detection, 
go-stop following, in-vehicle monitoring, face recognition, traffic sign and 
signal detection and recognition, etc. 
Currently, we have completed partial functions of the monocular 
computer vision for road situation and driving situation detection, 
including: 1.lane-marking detection, 2.multiple lane estimation, 
3.solid/dashed line classification, 4.vehicle direction estimation, 5.vehicle 
lateral location estimation, 6.lane-departure detection and warning, 
7.preceding vehicle detection, 8.preceding-vehicle distance estimation, 
9.preceding-vehicle brake light detection, and 10.preceding-vehicle turning 
light detection. 
 
 - 112 -
  
技術特點 
在車道偵測、偏離車道警示、與前車防撞警示的研究中，我們提
出有效率的電腦視覺偵測技術輔助道路安全駕駛。我們的技術具有下
列多項特色： 
1. 我們的方法只要單一相機就可完成上述車道偵測、偏離車道警示、
與前車防撞警示中的所有功能，包括前車的 3D 距離估計。 
2. 在車道線偵測中，我們加入了人眼視覺的特性；所以我們的方法：
(i) 可適應各種天候狀況、(ii) 不受陰影影響、(iii) 不受雨刷與車輛
局部遮蔽車道線的影響、及 (iv) 準確且快速的方法。 
3. 我們的車道線偵測方法不需要做邊界二值化的判定。 
4. 我們的多車道線不是用偵測的，而是在本車車道線偵測出來後，用
3D 幾何關係估計出來的，快速又準確。 
5. 我們的車道偏離方法可以真正計算出己車所在位置的車道偏離，而
不是相機所看到位置的偏離，準確度可達公分等級。 
6. 我們的車道線偵測加入偵測錯誤判定及修正技術，提供駕駛更穩健
的車道線資訊及更安全的駕駛環境。 
7. 我們的車道線偵測方法可以在影像中，從單邊車道線估計另一邊車
道線；所以在路面品質不好的情況下，我們的方法仍比其他方法更
穩定。 
8. 我們的偏離車道偵測技術簡單、快速、又有效；我們比較過一些國
外相關研究的方法，還沒看到比我們更簡單、有效的方法。 
9. 在前車距離估計方面，我們只要一個相機就可以偵測前車的距離
(range)，而且我們的方法只需要相機鏡頭焦距及相機到地面的高
度，不需要相機架設的俯角 (tilt angle)。大部份方法都要先知道相
機架設的俯角，我們的方法可以直接估計出相機架設的俯角。 
10.車道偏離偵測與前車防撞警示的技術已獲得中華民國及美國的發
明專利。 
在駕駛人的眼睛偵測、眼睛開合分析、臉部方向偵測、視線方向
估計、及昏睡判定的研究中，我們的技術具有下列多項特色： 
1. 利用不可見光的紅外線照射臉部，讓系統在任何亮度環境下都能正
常運作。 
2. 可以在白天及夜晚等不同環境下都使用同一套演算法，簡化系統的
負擔。  
3. 針對灰階影像所設計的臉部特徵擷取方法，適用於各種不同照度及
膚色的臉部偵測及分析，維持系統的穩定及即時運作特性。 
4. 能夠處理及分析不均勻照度下的臉部影像。這是車上駕駛環境一定
會遇到的問題，但過去的研究幾乎都不曾考慮過。 
5. 使用雙重資訊 (multi-cue)：閉眼時間及車道偏離分析，來判定是否
昏睡，可增加系統的可靠度。 
6. 同時監控駕駛人的昏睡狀態及注意力 (視線方向與臉部方向)，可提
升安全防護的效果；因為除了疲勞駕駛之外，分心也是另一項影響
駕駛安全的重要因素。 
 
  
 - 114 -
附件：B  
出席國際學術會議心得報告及發表之論文 
 
[1] Hsiao-Ting Tseng and Din-Chang Tseng, “Automatic cloud removal from multi-temporal 
SPOT images,” in Proc. Int. Conf. Intelligent Computing (ICIC 2007), Qingdao, China, 
Aug.21-24, 2007, CD. 
 主持人出席報告 
[2] Yao-Tien Chen and Din-Chang Tseng, “Medical image segmentation based on the Bayesian 
level set method,” in Proc. Int. conf. Medical Imaging and Informatics (MIMI 2007), Beijing, 
China, Aug.14-16, 2007, pp.16-27. 
 博士班學生出席報告 
[3] Xin-Liang Shen, Din-Chang Tseng, Chun-Wei Lin, Tony Hu, and Regulus Liou, “Versatile 
visual detection techniques for advanced safety vehicles,” in Proc. Int. Conf. on Image 
Processing, Computer Vision, and Pattern Recognition, Las Vegas, Nevada, Jul.14-17, 2008, 
pp. ? 
 有報名，沒出席報告，所以沒有使用出席國際會議經費 
 
 
 
 
 - 116 -
 
 
四、建議 
參與國際學術會議的目的除了早點接觸他人新近研發的技術及觀念外，尚可增廣見
聞，與外國相關學者接觸，提昇、創新本國研發方向。另外若有機會亦可參訪會議主
辦國的一些研究機構。不過個人的管道有限，若能預先知道有同會議的國內專家學
者，則可彼此互通消息，結伴參訪。因此若有機構預先提供出席國際會議的資料，則
可能讓出席國際會議者獲得額外的參訪機會。 
 
五、攜回資料名稱及內容 
攜回資料為一片“2007 International Conference on Intelligent Computing”光碟片內含
所有會議資料及所有論文完整檔、 “2007 ICIC Conference Program” 大會手冊、及一
些相關研究的資料。 
 
六、其它 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Automatic Cloud Removal from Multi-temporal 
SPOT Images 
Hsiao-Ting Tseng and Din-Chang Tseng* 
 Department of Computer Science and Information Engineering, 
National Central University, Chung-li, Taiwan 
*tsengdc@ip.csie.ncu.edu.tw 
Abstract. Partial cloud cover is a severe problem in optical satellite remote 
sensing. The problem can be partially overcome by acquiring multiple images 
at different time over a given region and the reasonably cloud-free composite 
image can be obtained by mosaic of the cloud-free areas in the set of images. In 
this paper, a multidisciplinary operational algorithm is proposed to generate 
cloud-free mosaic images from multi-temporal images acquired by the SPOT 
satellite images. First, the original images are enhanced in the YCbCr space. 
Then we choose the base image that has the least thin cloud cover and divide 
the base image into several decision grid zones. In the cloud determination, we 
use the linear spectral unmixing method (LSU) to extract all cloud cover pixels, 
but the method cannot handle thin clouds and cloud-shadow, and often confuse 
bright land surfaces as clouds. Therefore, we utilize an opening operation to 
exclude smaller confuse bright zones and eliminate unnecessary marking like 
plain or large sized buildings by checking difference with another SPOT image. 
At last, we find thin cloud and cloud-shadow in eight-neighbor grid zones 
around thick clouds based in the location relation and sun elevation angle. The 
cloud and cloud shadow grid zones of the base image are replaced by the same 
zones of other images. Comparing the base image and replaced image, we 
create a transition zone. Finally, the multiscale wavelet pyramid method is used 
to blend images in the transition zone. 
Keywords: multitemporal remote-sensing images, cloud removal, image fusion, 
image mosaicking, linear spectral unmixing, wavelet transform 
1   Introduction 
As a tremendous amount of remote sensing data has been received and widely used 
many applications in this country since then, like numerical analysis, land-use 
classification, landscape ecological change detection, using DTM terrain data to build 
Earth model, etc. However, no matter what purpose of the Earth resources satellite 
image may be, the common target is to extract landscape information from satellite 
images. 
By the affect of weather, the images obtained from the earth resources satellite 
sensors, like Landsat and SPOT, the cloud cover and cloud-shadows interfere with 
For mixed pixels, a more sensible approach is to construct a mixture map in which a 
series of maps give the concentration per ground-cover class. In such a map, the 
contribution of each pixel is assigned in proportion to the percentage area each ground 
cover class occupies in that mixed pixel. This process of assigning more than one 
class label to an individual mixed pixel is called spectral unmixing. The map that uses 
the intensity of brightness to represent the percentage of the ground cover class in 
each pixel is called a fraction image. 
Linear mixing model is the most popular mixing model. According to the 
assumption of this model, a mixed pixel is linearly consisted of different materials. 
The unmixing process of this model is to solve a system of linear equations based on 
a least-square-error criterion. It can only be used to unmixing pixels when the number 
of bands is greater than the number of component materials. 
 
Grass
Farm
Soil
Road
Wood
 
Fig. 1. The mixed pixel phenomenon. 
 
Here we utilize the linear spectral unmixing (LSU) method [6] to solve the 
mixed pixel problem and generate fraction images of each ground cover class. The 
basic assumption of linear mixing model is the signal received from each pixel can be 
considered as a linear superposition of the spectral contributions of all pixel 
components. 
Each pixel in multispectral images is an observation vector. The observation 
vector x(i,j) is a l×1 column vector, where l is the number of spectral bands. The pixel 
x(i,j) can be described by the linear mixing model 
),(),(  ),( jiejifji += Mx ,                                       (1) 
where M is a l×p matrix containing p distinct spectral materials. The f(i,j) item is a 
p×1 column vector representing p true ground cover component proportion. The e(i,j) 
item is a l×1 vector representing random noise which is often assumed to be an 
independent and identical Gaussian distribution with zero mean and covariance 
matrix σ2I. The procedure diagram of the linear mixing model is shown in Fig.2.  
To achieve the calculation, we need to know the information of the matrix M. 
The spectral representation of those pixels corresponding exclusively to a single 
ground class is called the end-member spectra (EMS) which compose the M matrix. 
But in fact, it is hard to have a prior knowledge about what materials will exist in the 
image. Hence, a common practice is to estimate the matrix M directly from a set of 
training samples and use the least-squares (LS) method to estimate the cover class 
1)( −= MMU T .                                                   (8) 
Now substitute fc into the constraint (a) that  
1=−= U1111 TLSTcT ff λ ,                                         (9) 
we solve 
]1[]1[)( 1 −=−= − LSTLSTT ff 11U11 αλ ,                             (10) 
where 
1)( −= U11Tα .                                                   (11) 
Substituting Eq.(10) into Eq.(6) and rearranging terms, we have  
LSc ff )( UJIU1 αα −+= ,                                         (12) 
where J = 1．1T is a c×c matrix with each element being unity.  
After we get the CCP vector of the measurement vector x with sum-to-one 
constraint, the positive constraint is then add to fc and the new LM LS solution fLLS is  
c
T
c
T
T
LLS
f)fα(α
f)fα(
fff
1UJIU1
1UJI
1
<−+=
>−=
≤≤=
1　　　　　　　
0     　　　　　　　　
10     　 　　　　　　　　
c
c
cc .                          (13) 
2.2. The proposed method of cloud and cloud-shadow determination 
The details of cloud and cloud-shadow determination are described as follows: 
Step 1: Select the base image which has the least thin cloud covers on it.  
Step 2: Use the linear spectral unmixing (LSU) method to find out cloud-cover pixels 
and generate a binary image to indicate which being the cloud pixels. The 
bright pixels of open land surfaces or buildings may also be mistaken as cloud 
pixels. 
Step 3: use opening operation on the binary image to exclude smaller confuse bright 
areas like megalopolis; some representative grid zones are shown in Fig.3. 
Step 4: Partition the base image into several grid zones; then the cloud zones are 
clipped out. The size of the grid zones are 128×128 pixels; the size of the grid 
zones are based on the characteristics of the source images. 
Step 5: Check difference with another SPOT image to eliminate non-cloud bright land 
surfaces like plain or large sized buildings to avoid unnecessary marking.  
Step 6: Use eight-neighbor grid zones around cloudy grid zones to find thin-cloud and 
cloud-shadow grid zones. Because of the topographic shadow interference, it 
is found that linear mixing model method does not work well in generating 
the cloud shadow fraction image. We apply a technique based on a geometric 
model, standing on the sun elevation angle to automatically predict the 
approximate location of cloud shadow near to the cloud. The thin clouds that 
follow thick clouds are generally at neighbor to cloud-shadow as examples 
shown in Fig.4. For these reasons, thin clouds and cloud-shadows that follows 
thick clouds are selected by using eight neighboring grid zones around thick 
clouds grid zones: the left, above, and top-left grid zones for finding cloud-
shadows, others neighbors for finding thin clouds, as shown in Fig.5. 
 
3 Image mosaicking 
When the cloud and cloud-shadow grid zones in the base image are superseded by 
another image may make the seam between them. Image blending is the process to 
make the seam invisible. The basic idea is that different image features should be 
blended across a transition zone, that is to say, between the base image and replaced 
image we create a transition zone. The transition zone comprises two images 
overlapped with each other and then forming a final mixed image of two combined 
images, as smooth as possible over a transition zone. 
We use the multiscale wavelet pyramid to blend the overlapped images in 
transition zone. Since the subband images can cause different effects of human vision, 
the images are decomposed into several subband images, and the subband images are 
combined region by region. 
3.1   Statistics color correction 
When the cloudy and cloud-shadowy grid zones in the base image are masked out, 
and then filled in cloud-free blocks from other images acquired at different time. 
There is a slightly different on the intensity of pixels at the same location from two 
different scenes due to the atmospheric effects, sun angles and sensor look angles 
during acquisition, especially prominent in lowalbedo vegetated areas. Therefore 
before replacing, it is necessary to adjust the intensity to minimize the variation. 
Reinhard et al. [7] proposed a color transfer method that can be applied to 
statistics color correction. Ruderman et al. [8] developed a color space lαβ, which 
minimizes correlation between channels for many natural scenes.  
First, we transform the RGB values to lαβ color space, and then compute means 
and standard deviations of source and target images. The source colors are corrected 
by scaling and offsetting according to the mean and standard deviation of the target 
image, as follows: 
( )
( )
( )⎪⎪
⎪⎪
⎩
⎪⎪
⎪⎪
⎨
⎧
+−=′
+−=′
+−=′
  
 
       
t
s
t
sss
t
s
t
sss
t
s
t
sss llll
βσ
σβββ
ασ
σααα
σ
σ
β
β
α
α
ι
ι
,                                        (14)
 
where jc , c = l,α,β is the mean of an image for each channel and j = s, t refers to the 
source and target image, respectively. cjσ  is the standard deviations of each channel 
of source and target image.  
In this study, we use local area to correct colors. The target image is from the 
base image, that the transition zone of the cloud and cloud shadow grid zones. The 
source image is another image that the same zones of target image, but including the 
transition zone and the cloud and cloud shadow grid zones in the base image.  
 
4   Experiments and discussions 
We use the SPOT 6.25 meter resolution false-color satellite image data over Taiwan, 
merged with SPOT Pan 6.25 meter resolution and SPOT XS 12.5 meter resolution in 
our experiments. These satellite images were received by the Center for Space & 
Remote Sensing Research (CSRSR) of National Central University. We take the same 
region of two years SPOT satellite data, assuming that the land covers don’t change 
within the time interval and the location of cloud isn’t overlap. 
The processed image size is 5120 by 5120 pixels multispectral SPOT image. 
Two SPOT false-color images near to Tseng-Wen Reservoir captured in the years 
2000 and 2001 are shown in Fig.7. 
First, we select 2001 SPOT false-color image as base image, and partitioning the 
input images into several grid zones, the size of the grid zones is 128×128 pixels.  
Then we use linear spectral unmixing (LSU) to extract possibility of cloud-cover 
pixels. First, we estimate the matrix M directly from a set of training samples. Then 
we estimate ground cover class proportion (CCP) for each image pixel. We add sun-
to-one and positive constraints to make the result more reasonable when estimating 
CCPs. After the CCP of each pixel is estimated, it is easy to generate the material 
fraction images. The binary cloud fraction image is shown in Fig.8 (a). 
We can find that the bright pixels of open land surfaces or buildings may be 
mistaken as cloud pixels. If we use the strictly condition to discriminate between the 
open land surfaces or buildings and cloud, that may lead to the independent small 
cloud may be mistaken as non-cloud. For this reason, in the step we only discriminate 
the greater parts like vegetation that are not cloud. Then we use opening operation in 
the cloud fraction image to exclude smaller confuse bright areas like megalopolis in 
Fig.8 (b). Fig.9 (a) shows the original base image Fig. 7 (b) with the corresponding 
cloudy blocks in Fig 8 (b) being replaced by black blocks. Here, the basic processed 
element is a grid zone with size of 128×128 pixels. 
After above processing, the smaller confuse bright areas like megalopolis was 
removed from the cloudy grid zones. But we can find that bright land surfaces like 
plain or large sized buildings were still mistaken as the cloudy grid zones. Thus we 
check difference with another SPOT image in order to eliminate unnecessary marking. 
The result is shown in Fig.9 (b). Then we use eight-neighbor grid zones around thick 
clouds grid zones to find thin clouds and cloud-shadow grid zones. The result of 
cloud-free and cloud-shadow-free image is shown in Fig.10. 
Now, we have succeeded in marking cloudy and cloud-shadowy grid zones. 
Then the binary block image is generated to indicate which blocks being cloudy 
blocks as one example shown in Fig. 11 (a), where white blocks represent cloudy and 
cloud-shadowy grid zones. The binary block image is then used for finding all 
transition zones as one example shown in Fig. 11 (b). At last, the generated cloud-free 
and cloud-shadow-free image using multiscale wavelet pyramid method for blending 
two original images of Fig.7 in the transition zones is shown in Fig. 12, where non-
base image (Fig.7 (a)) has been adjusted by statistics color correction. 
 
  
Fig. 10. The result of cloud-free and cloud-shadow-free in base image. 
 
  
(a)                     (b) 
Fig. 11. Finding transition zones. (a) The binary block image. (b) The binary block 
image for indicate the transition zones. 
 
 
Fig. 12. The produced cloud-free and cloud-shadow-free image. 
X. Gao et al. (Eds.): MIMI 2007, LNCS 4987, pp. 25 – 34, 2008. 
© Springer-Verlag Berlin Heidelberg 2008 
Medical Image Segmentation Based on the Bayesian 
Level Set Method 
Yao-Tien Chen1 and Din-Chang Tseng2 
1 Department of Computer Science and Information Engineering, 
Yuanpei University, HsinChu, 30015, Taiwan  
ytchen@mail.ypu.edu.tw  
2 Department of Computer Science and Information Engineering, 
National Central University, Chungli, 32001, Taiwan 
tsengdc@ip.csie.ncu.edu.tw 
Abstract. A level set method based on the Bayesian risk is proposed for 
medical image segmentation. At first, the image segmentation is formulated as a 
classification of pixels. Then the Bayesian risk is formed by false-positive and 
false-negative fractions in a hypothesis test. Through minimizing the average 
risk of decision in favor of the hypotheses, the level set evolution functional is 
deduced for finding the boundaries of targets. To prevent the propagating 
curves from generating excessively irregular shapes and lots of small regions, 
curvature and gradient of edges in the image are integrated into the functional. 
Finally, the Euler-Lagrange formula is used to find the iterative level set 
equation from the derived functional. Comparing with other level-set methods, 
the proposed approach relies on the optimum decision of pixel classification; 
thus the approach has more reliability in theory and practice. Experiments show 
that the proposed approach can accurately extract the complicated shape of 
targets and is robust for various types of images including high-noisy and low-
contrast images, CT, MRI, and ultrasound images; moreover, the algorithm is 
extendable for multiphase segmentation. 
Keywords: image segmentation, level set method, Bayesian risk, hypothesis 
test. 
1   Introduction 
Nowadays a large number of various medical images [1, 2] are generated from 
hospitals or medical centers with sophisticated image acquisition devices, such as 
computed tomography (CT), magnetic resonance (MRI), ultrasound image (US), X-
ray diffraction, electrocardiogram (ECG), and positron emission tomography (PET). 
Medical image segmentation is a technique assisting doctors to process and analyze 
the medical images, so that doctors can make better diagnosis, accurately examine 
disease symptoms, and support their decisions in a variety of clinical works. For 
medical imagery, a main goal of image segmentation is to accurately extract the shape 
of targets from various types of medical images. Over these decades, many 
approaches have been developed to achieve the goal; active contour is one of the most 
powerful methods. 
 Medical Image Segmentation Based on the Bayesian Level Set Method 27 
minimization of the variational geometric framework. The surface evolution is 
performed using the fast geodesic active contour approach; numerical scheme 
combining semi-implicit additive operator splitting (AOS) [12] propagation scheme, 
level set representation, narrow band approach, and the fast marching method. 
Chenoune et al. [13] proposed a segmentation of cardiac cine-MR images and 
myocardial deformation assessment using level set methods. First, the level set 
method proposed by Osher and Fedkiw [14] is modified by introducing an additional 
region-based constraint. Then, it is applied on a 2D+t (i.e., 3-D pseudo-volume by 
stacking the 2-D images) dataset to detect endocardial contours. 
In this paper, we propose a level set method based on the Bayesian risk to segment 
various medical images. At first, by minimizing the risk of misclassification, the level 
set evolution functional is deduced. To prevent the propagating curves from 
generating excessively irregular shapes and lots of small regions, curvature and 
gradient of edges in the image are integrated into the functional. Finally, the Euler-
Lagrange formula is used to find the iterative level set equation from the derived 
functional so that the propagating curves can move towards and stop at the boundaries 
of targets. 
2   The Bayesian Risk 
In this section, the basic concept of Bayesian risk [15, 16] is introduced and which 
will be used to classify pixels into several groups based on the similar characteristics. 
Then, based on the risk we derive the level set evolution functional. 
Suppose an image comprise foreground and background pixels to be classified. 
Classification of the image can be represented by two hypotheses: a null hypothesis 
H1 where the foreground is absent, and an alternative hypothesis H2 in which the 
foreground is present. The classifier is used to determine which hypothesis is correct; 
that is, the classifier must choose one of two decisions, Θ1: the classifier declares that 
the foreground is absent, or Θ2: the foreground is present. In the hypothesis test, there 
are four conditional probabilities used for the combinations of hypothesis and 
decision. (i) P(Θ1|H1) is the probability that the classifier declares the foreground 
absent when it is actually absent. (ii) P(Θ2|H1) is the probability that the classifier 
declares the foreground present when it is actually absent. (iii) P(Θ1|H2) is the 
probability that the classifier declares the foreground absent when it is actually 
present. (iv) P(Θ2|H2) is the probability that the classifier declares the foreground 
present when it is actually present. 
P(Θ2|H1), called type I risk, is the probability of rejecting the null hypothesis H1 
when it is true; while P(Θ1|H2), called type II risk, is the probability of accepting H1 
when H1 is false. For each combination of hypothesis and decision, there exists an 
associated loss. The losses of P(Θ1|H1), P(Θ2|H1), P(Θ1|H2), and P(Θ2|H2) are denoted as 
l(1,1), l(2,1), l(1,2), and l(2,2), respectively. l(1,1) and l(2,2) are the losses of correct 
decision while l(2,1) and l(1,2) are the losses of incorrect decision. l(1,1) and l(2,2) are 
expected to be low or zero; l(2,1) and l(1,1) (also l(1,2) and l(2,2)) are mutually inverse; 
thus l(2,1) and l(1,2) are expected to be high. For images, the Bayesian risk for 
classifying a pixel into foreground or background is given by [15, 16] 
 
 Medical Image Segmentation Based on the Bayesian Level Set Method 29 
F(C, φ) = FB(C, φ) + FR(C, φ),                                   (9) 
where FB(C, φ) is the Bayesian term and FR(C, φ) is the regularity term; curve C is 
represented by zero level set (i.e., φ(x, y) = 0). The statistical decision theories are 
generally used for decision making. Here we apply minimizing the Bayesian risk to 
find the boundaries of targets in an image and the Bayesian term is defined as 
.)()()()( ),(
12
2211 ∫∫ +=
ωω
ωωωωφ dxdygPPdxdygPPCFB                  (10) 
To prevent the curve from generating excessively irregular shape and lots of small 
regions, we set the regularity term [4] as 
∫
Ω
∇= ,) ,()) ,((),( 0 dxdyyxyxCFR φφδνφ                              (11) 
where ν ≥ 0 is the constant for weighting the regularity term. 
Assuming that the gray levels of image pixels are Gaussian distribution and 
mutually independent (i.e., approximately independent and identically distributed). 
The pdf of image pixels is expressed by 
,2 ,1 ,
2
)(
exp
2
1)( 2
2
=⎥⎥⎦
⎤
⎢⎢⎣
⎡
−−
= iggP
i
i
i
i
σ
μ
σπ
ω                             (12) 
where g denotes the random variable of pixel gray levels; µi and σi are the mean and 
variance of phase ωi. To eliminate the exponential form of the Gaussian function, we 
take logarithm to make the functional of Bayesian term as 
.
)()ln(2
2
1)(ln                 
)()ln(2 
2
1)(ln ),(
1
2
2
2
2
22
22
2
1
2
12
11
∫
∫
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−
+−+
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−
+−=
ω
ω
σ
μ
πσω
σ
μ
πσωφ
dxdygP
dxdygPCFB
                  (13) 
Based on finding the minimum extremal of the functional F(C, φ), the evolving 
curve C will approach the target boundary. The functional F(C, φ) is minimized by 
solving the associated Euler-Lagrange equation. Consequently, the level set equation 
for evolution process is given as 
( ) ( )
( ) ( ) ,2ln
2
1)(ln       
2ln
2
1)(ln )(
 
2
2
2
22
22
2
1
2
12
110
⎪⎭
⎪⎬
⎫
⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−
+−+
⎪⎩
⎪⎨
⎧
⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−
+−−⎟⎟⎠
⎞
⎜⎜⎝
⎛
∇
∇
=
∂
∂
σ
μ
πσω
σ
μ
πσωφ
φ
νφδφ
gP
gPdiv
t
           (14) 
with initial condition φ (x, y, t = 0) = φ0 (x, y) in Ω and boundary condition 
Ω∂=
∂
∂
∇
on  0
 
)(0
n
Gφφ
φδ
,                                             (15) 
 Medical Image Segmentation Based on the Bayesian Level Set Method 31 
(a) (b)
(c) (d)
 
Fig. 1. A low-contrast and high-noisy image of a blurred-boundary target. (a) The original 
image. (b) The segmented result of Chan-Vese model. (c) Result of Martin’s model. (d) Result 
of the proposed approach. 
(a) (b)
(c) (d)
 
Fig. 2. Boundary extraction of CT abdominal images. (a) and (b) The original images. (c) and 
(d) The segmented results. 
 
 Medical Image Segmentation Based on the Bayesian Level Set Method 33 
Ultrasound images are obtained by emitting and receiving reflects of high-
frequency sound waves. The reflected sound wave echoes are recorded and displayed 
as an image. Segmentation on ultrasound images is useful for examining body's 
internal organs, heart, liver, spleen, pancreas, and kidneys. Two ultrasound images 
shown in Fig. 4 (a) and (b) were segmented. The boundary of a tumor was accurately 
extracted as shown in Fig. 4 (c), and the boundaries of two spurious tumors were 
extracted as shown in Fig. 4 (d). The manual-traced boundaries are shown in Fig. 4 
(e) and (f) for comparison. Due to the influence of imaging devices, such as speckles 
and random noises, the quality of images are heavily degraded; however, the 
proposed approach is still able to extract satisfactory boundaries of interested targets. 
5   Conclusions 
In this paper, level set methods were proposed for image segmentation in which the 
Bayesian risk was used to develop the segmentation algorithms. At first, the image 
segmentation was formulated as a classification of pixels. Then the Bayesian risk is 
formed by false-positive and false-negative fractions in a hypothesis test. Through 
minimizing the risk of misclassification, the level set evolution functional was 
deduced for finding the boundaries of targets. To prevent the propagating curves from 
generating excessively irregular shape and lots of small regions, curvature and 
gradient of images were integrated in the functional. Finally, we used Euler-Lagrange 
formula to minimize the functional to derive the level set evolution equations. The 
generated level set equations were useful for segmenting various kinds of medical 
images; moreover, the developed equations and algorithms are easily extended for 
multiphase segmentation. 
The proposed approach has the following advantages: (i) The statistical decision 
theories are integrated into the derivation of level set method; hence the boundaries of 
targets can be more accurately extracted. (ii) The proposed approach takes the pixel 
distribution into the segmentation, so that the local noises have less influence to the 
propagating process. (iii) Many complicated shapes, such as corners, cavities, 
convexity, and concavities can be extracted at one time; the proposed approach is 
highly compatible with the medical image segmentation. (iv) The proposed method 
can be easily extended to facilitate the multiphase and 3-D surface segmentation. 
References  
1. Armstrong, P., Wastie, M.L.: Diagnostic Imaging. Blackwell Scientific Publications, 
London (1989) 
2. Bryan, G.J.: Diagnostic Radiography: A Concise Practical Manual. Churchill Livingstone 
Inc., New York (1987) 
3. Osher, S., Sethian, J.A.: Fronts propagating with curvature-dependent speed: algorithms 
based on Hamilton-Jacobi formulations. Journal of Computational Physics 79, 12–49 
(1998) 
4. Chan, T.F., Vese, L.A.: Active contours without edges. IEEE Trans. Image Processing 10, 
266–277 (2001) 
   
Versatile Visual Detection Techniques for 
Advanced Safety Vehicles 
 
Xin-Liang Shen 1, Din-Chang Tseng 1, Chun-Wei Lin 1, Tony Hu 2, and Regulus Liou 2 
1 Department of Computer Science and Information Engineering, 
National Central University, Chungli, Taiwan 
2 Chung-Shan Institute of Science & Technology, Longtan, Taiwan 
 
Abstract - A monocular visual detection system 
including twelve detection functions is proposed to assist 
the road driving for safety. In the lane-mark detection, 
the lateral inhibition property of human vision system is 
simulated to improve the detector to satisfy all different 
weather conditions and to avoid the influence of 
windshield wiper. The preceding vehicles are detected 
based on the underneath shadow, left/right borders, and 
multiple templates of vehicles; then verified by the ratio 
of lane and vehicle widths, symmetry, and gray-level 
variance of vehicle regions. The preceding-vehicle 
distance is estimated based on a single camera with only 
known focus length and setting height. The pitch and 
yaw angles of the camera are estimated from the 
proposed method. The experimental results show that the 
proposed methods are stable and effective for safety 
detection in various weather conditions: sunny, misty, 
dusty, cloudy, rainy day, and night. The average vehicle 
detected rate is 94.5 %.  
 
Keywords: advance safety vehicle, computer vision, 
lane detection & classification, vehicle 
detection & distance estimation 
 
1. Introduction 
Recently, the researches on autonomous guided 
vehicle (AGV) and Intelligent Transportation System 
(ITS) are more and more popular [1-3, 5-7, 9-14]. The 
driver assistance systems play important roles on ITS. To 
reduce the traffic accidents, many equipments were 
utilized to develop detection techniques for safety 
vehicles such as infrared laser radar, millimeter-wave 
radar, microwave radar, camera, etc. In those equipments, 
radar is most stable, does not influenced by weather 
conditions, and can infer range data directly; but 
(millimeter-wave) radar is expensive and has only range 
function. The cameras are like human eyes; we can use 
the computer vision techniques to detect as well as to 
recognize the targets; moreover, cameras are cheap, 
flexible usage, and versatile functions. Thus vision-based 
methods are developed to provide detection for safety 
vehicles. 
The proposed visual detection system contains 
twelve modules: 1. lane detection, 2.multiple lane 
estimation, 3. solid/dashed, 4. yellow/white, 5.  
single/double lane-mark classification, 6. host vehicle 
direction estimation, 7. vehicle lateral offset estimation, 
8. lane departure warning, 9. preceding vehicles 
detection, 10. preceding-vehicle distance estimation, 11. 
brake light, and 12. turn signal detection. Due to the 
limitation of the paper length, only lane detection / 
classification and vehicle detection / distance estimation 
are described. 
Lane detection and multiple lane estimation will 
support lane position information for other detection 
modules. According to the classifications of solid/dashed, 
double/single and yellow/white lane marks, driving 
environment can be recognized clearly. The vehicle 
direction is acquired form the lane direction. Vehicle 
lateral offset can be estimated by calculating the distance 
from lane to our vehicle. According to the driving habit, 
lane departure warning function can be set to remind us 
whether the vehicle is departed or not from the current 
lane. The driver can own the information about how 
many vehicles in front of our vehicle and those vehicle’s 
positions preceding are by preceding vehicle detection. 
Preceding vehicle distance estimation let driver know 
further if our vehicle is in safe range or not. Brake light 
and turn signal detection support driver to understand 
preceding vehicle’s movement [4, 8]. 
The remaining sections of this paper are organized 
as follows: Section 2 describes our approaches on the 
lane detection and classification. Section 3 presents the 
vehicle detection and distance estimation. Section 4 
presents the experiments and the results. Section 5 is the 
conclusions. 
 
2. Lane detection and classification 
In the proposed lane detection system, we utilize a 
consecutive RGB color image frames and a defined lane 
model to obtain some reliable cues for classifying the 
appearance of the lane marks, yellow/white lane marks, 
and single/double lane marks. According to the shape 
and reasonable existing range of the lane marks, 
moreover, we support some obvious information such as 
the width of the lane marks and the differences of the 
slope and the intercept in consecutive image frames. 
Based on our lane detecting methods, we can already 
deal with highway or urban roads and overcome the 
different variations in weather, vehicle occurrence, and 
broken lane marks. 
 
   
the template matching for vehicle detection to overcome 
the interference of rainy day and cloudy day. First, we 
train the templates of the tires and bottom in the vehicle 
as our feature. Second, we build some ROI (Region of 
Interest) based on the results of template matching in the 
restricted ranges. Finally, we find the detected boxes by 
the suitable ROI to verify and track. 
 
3.1 Preceding vehicle detection in rainy day 
In the rainy day, the scene is obscure due to the rain 
as one example shown in Fig. 1 (a). Under this condition, 
it is too difficult to detect the positions of cast shadow. If 
we fail to detect the cast shadow at first step in detection 
stage, we can not detect the preceding vehicle. In order 
to overcome this condition, we provide the 
template-based method. By using predefined pattern of 
the tires and bottom in the vehicle, we perform 
correlation between the image and the template to detect 
the possible vehicle step by step. Then we build the 
suitable detected box to verify and track. 
We find two properties to generate the template as 
follows: 
i. The tires and the bottom of vehicle are darker than the 
road surface, no matter what kinds of vehicle are. 
ii. The template contains a priori knowledge about 
vehicles: A vehicle is generally symmetric, 
characterized by a rectangular bounding box which 
satisfies specific aspect ratio constraints. 
Based on the two properties, we select the bottom part of 
vehicle as the region for generating the template. Based 
on the regions we select, we translate the RGB color 
space into gray-level image and bi-level image. 
According to the analyses of gray-level image and 
bi-level image, we generate the vehicle template similar 
to the ∩ shape as shown in Fig. 4. 
 
   
   
   
(a)           (b)           (c) 
Fig. 4. The vehicle templates. (a) Original image. (b) 
Gray-level image. (c) Inversed bi-level image. 
 
After generating the vehicle templates, we search 
the possible position for each vehicle. In order to reduce 
the computing time and make the detecting results 
correctly, we define the searching ranges based on the 
lane detection and the position of vanishing point. We 
divide our detecting frame into four blocks (A, B, C, D) 
as our searching ranges and design the templates in 
different sizes for individual blocks. If the preceding 
vehicle is farther from us, the danger for us is smaller. So 
we reduce the searching ranges according to the 
y-coordinate as shown in Fig. 5. 
 
 
 
Fig. 5. The definition of searching ranges (A, B, C, D). 
 
Based on the templates and the defined searching 
ranges, we use template matching to calculate the 
correlation values within each searching range. Here we 
use the normalized correlation (NC) to find the best 
matching position of candidate vehicle: 
  .
]]),([)],(),([[
]),()][,(),([
),(
2
122∑∑ −−−∑∑ −
∑∑ −−−−
=
x yx y
x y
wnymxwyxfyxf
wnymxwyxfyxf
nmr
 
(5)
 
We have already fixed the sizes of templates and 
searching ranges and we just want to compare the 
correlation values; thus we utilize first formula to ignore 
the influences of scale changes in the amplitude and to 
reduce the computing time. To find all possible vehicles, 
we sort these correlation values from large to small. 
Then we choose first ten percentages of correlation 
values as the possible vehicle positions. One example is 
shown in Fig. 6. 
 
  
(a)                    (b) 
Fig. 6. The detected vehicles. (a) The position of 
maximum correlation values. (b) The positions of 
first ten percentages of correlation values. 
 
3.2 Vehicle verification 
To verify whether the detected box that we found is 
a vehicle, we use three criteria to check. Firstly we check 
the ratio of the width of the detected box and road width, 
secondly check the symmetry, finally we check the 
standard deviations of the detected box as follows: 
i. (Wb > Wr /3) and (Wb < Wr), 
where Wb is the width of the detected box and Wr is 
the road width. 
ii.  Symmetry  
WH
yxWGyxWG
H
x
W
y
 
)),2/(),2/((
1
2/
1
2∑ ∑ +−−
−
= = ,   (6) 
where G(x, y) is the gray level of (x, y). H and W are 
the height and width of the detected box. 
A 
B 
 
C 
 
D 
   
(xv, yv) 
(xo, yo)
direction component of the lane mark on the yz-plane is 
[0 yv q]T. Thus the acute angle of the lane mark and the 
camera optical axis (i.e., z-axis) on the yz-plane is θ, 
[ ] [ ] .cos1000cos
22
1
22 ⎟⎟⎠
⎞
⎜⎜⎝
⎛
+
=⇒
+
⋅
=
−
qy
q
qy
qy
vv
TT
v θθ  (11)
 
From Eqs.(10) and (11), the pitch angle θ and yaw angle 
φ of the lane marks with respect to the camera coordinate 
system can be found. 
 
 
 
 
 
 
 
 
 
 
 
Fig. 7. Image center and vanishing point of parallel 
lanes. 
 
After lane detection, the tilt angle of the camera can 
be inferred from the vanishing point in the image. The 
location on the road which is right under the camera is 
called as camera location. Assume the height of the 
camera is h as shown in Fig. 8. The distance d between 
the intersection point of the optical axis and the road 
plane can be calculated by the known height and tilt 
angle, 
θθ cottan hd
d
h
=⇒= .            (12) 
 
 
Fig. 8. The side view of the relationship between the 
camera and image horizontal lines. 
 
If a vehicle in the image is located below the image 
center with a vertical distance a1 as shown in Fig. 8, the 
distance between the vehicle and the camera location is 
d1, and the viewing angle with respect to the optical axis 
is δ1; that is the viewing angle with respect to the road 
plane is θ1, 
11 δθθ =− ,                 (13) 
then we can get δ1 by the relationship of right triangle as 
.tantan 1111
1 ⎟⎟⎠
⎞
⎜⎜⎝
⎛
=⇒= −
q
a
q
a δδ          (14) 
From Fig. 8, we also have 
.cot 11 θhd =                (15) 
If we substitute Eqs.(13) and (14) into Eq.(15), we have 
( ) .tancotcotcot 11111 ⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞⎜⎜⎝
⎛
+=+== −
q
ahhhd θδθθ  (16) 
By the same way, we have 
( ) .tancotcotcot 21222 ⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛
−=−==
−
q
a
hhhd θδθθ  (17) 
 
4. Experiments 
Here we provide several experiments and 
comparisons. First, we introduce our develop platform. 
Second, we demonstrate the effects of detecting lane and 
preceding vehicle by several experimental results. 
Finally, we compare the experimental results and discuss 
the processing time. 
 
4.1 Developmental environment 
In order to develop safe driving system, our system 
must be reliable, robust, and real-time for variant 
weather and driving conditions. All algorithms are 
implemented in C++ programming language and 
Microsoft Foundational Classes (MFC) Library, and all 
experiments are executed on a general PC with Intel® 
Pentium® D 3.0GHz CPU and 512MB DDR RAM and 
Microsoft® Windows XP professional operating system 
as our experimental platform. In our experiment, we use 
the image size of video is 320×240 pixels in RGB color 
space. 
In order to prove the correctness and accuracy of 
our system, we have tested our system on our 
experimental vehicle which runs on the highway and 
urban road with the velocities between 50 km/hr and 120 
km/hr, and we display the experimental results of 
real-time visual detection. Our experimental vehicle 
mounted the CCD camera is shown in Fig. 9. 
 
  
(a)                   (b) 
Fig. 9. Visual detection on the experimental vehicle. (a) 
Near-view image. (b) Connected with an IBM 
notebook. 
 
4.2 Experimental results 
We discuss these experimental results in two tasks: 
i. Lane detection in eight different images which contain 
several different weather conditions is evaluated. 
ii. Preceding vehicle detection in five different images 
which contain four different weather conditions is 
