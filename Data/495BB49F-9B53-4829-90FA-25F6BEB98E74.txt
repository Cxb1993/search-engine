（一） 計畫中文摘要 
為了要解決模糊類神經網路在大量輸入輸出時所遭遇到的限制，即為” curse of dimensionality”的問題，
在本計畫中，我們將採用階層式的模糊類神經網路的架構來解決此問題。本案提出的階層式架構除
了將可大幅度降低被調整參數的數目，並且可以針對個別受控/被學習的系統形成區域性的學習，同
時我們將利用 Kolmogorov 定理來證明所提出的階層式網路具有一般性的近似能力 (universal 
approximator)。在應用方面，我們將把此架構結合基因演算法及粒子演算法應用於股票預測及化學
反應等問題，本計劃在此真實且複雜的問題得到不錯的研究成果。另一方面，我將測試此階層式架
構在解決單輸入單輸出(SISO)及多輸入多輸出(MIMO)適應模糊類神經網路控制在設計上所照成的” 
curse of dimensionality”的問題。 
關鍵字：階層式網路架構、模糊類神經網路、系統鑑別、適應性模糊類神經控制器 
 
（二） 計畫英文摘要 
A serious problem limiting the applicability of the fuzzy neural networks is the “curse of dimensionality”. 
A way to deal with this problem is to construct a hierarchical fuzzy neural network which can substantially 
reduce the number of adjustable parameters and also form a local learning according to the approximated 
system. We also will prove this structure is a universal approximator by Kolmogorov theory. For a practical 
application, the problems of a Taiwan Stock predictor and chemical process are proposed to verify the 
effectiveness of the hierarchical -FNN. We use genetic algorithm (GA) and particle swarm optimization (PSO) 
to tune all free parameters of the hierarchical fuzzy neural network, including both control points of the 
B-spline membership functions and weights of the small fuzzy neural networks. Otherwise, in the aspect of 
the controller design, the traditional direct adaptive fuzzy-neural control scheme for nonaffine nonlinear 
systems has a vast number of free parameters when many inputs (linguistic terms) and membership functions 
of the fuzzy-neural network (FNN) are required. This, however, leads to the problem of a huge computation 
time. Spending so much computation time adjusting these parameters results in a serious controller time-delay 
problem. Therefore, we propose an observer-based adaptive fuzzy-neural controller for SISO/MIMO 
nonaffine nonlinear systems, structured by the hierarchical fuzzy neural network, to substantially reduce 
substantially the number of adjustable parameters and the computation time of the controller. 
Key words: Hierarchical structures, fuzzy neural networks, system identification, adaptive fuzzy neural 
control 
 
（三） 報告內容 
1. 前言及文獻探討 
Non-linear mappings of an input vector to one or more output signals are required in many types of 
application, e.g. system identification [1-3], predictors [4-6], controllers [7-9] etc. Besides other 
approximation methods, fuzzy systems [10-13] are one method to implement such mappings. They have been 
proven to be a universal function approximator by many authors [11-16]. Fuzzy systems are characterized by 
a rule-based specification. This has the advantage that the specification may be done without any formal 
application-specific models by exploiting human knowledge, thus offering solutions to ill-defined problems. 
Primarily artificial neural networks (ANNs) [12, 17-23] are one of approximation method to apply learning in 
order to extract the application-specific knowledge directly from some training data. They were also shown to 
be universal approximators [12, 17]. But ANNs offer no way to incorporate a priori knowledge directly to 
guide the learning process, for instance to speed convergence. Because parameters modified by learning 
withstand an intuitive representation, there is no way to get deeper insights into the acquired knowledge. On 
the other hand, many studies [24-30] combining fuzzy logic with neural networks, called fuzzy network 
chemical process. 
 
3. 研究方法 
1. Hierarchical Fuzzy Neural Networks 
Let )(1 XHy =  be a two level fuzzy neural networks from the input space 
nRU ⊂  to the output space 
RV ⊂  with the hierarchical structure shown in Fig. 1. We call this structure as the hierarchical fuzzy neural 
network (Hierarchical-FNN). Each subsystem of the hierarchical -FNN is a typical fuzzy-neural network. 
The configuration of the hierarchical-FNN is shown in Fig. 2(a) to indicate two levels of the 
hierarchical-FNN. Fig. 2(b) shows the structure of the jth subsystem in Level-2 and Fig. 2(c) shows the 
structure of the subsystem in Level-1.  
 
 
 
 
 
 
 
 
 
 
 
  
The ith fuzzy IF-THEN rule in the hierarchical -FNN is expressed as 




1
1
1
1
2
1
11,2
2
,2
,2
,2,2,2
1
,2
1
FNNfor,isthen is  and ... andisIF 
FNNfor,isthen isand... andisIF 
   :
)()(
)(
,2,2
)(
il,l 
,j
j
ij
j
m
j
m
jj
i
wyAyAy
wyAxAx
R
ii
i
jj
i
             (1) 
where ),...,2,1(FNN ,2 ljj =  represents the FNN in Level 2, 1FNN  is the FNN in Level 1, 
j
kA
,2  and 1kA  
are fuzzy sets and jiw
,2  and 1iw are singleton fuzzy sets. Next, we adopt A  to represent 
j
kA
,2 and 1kA , which 
neglects the number of levels and subsystems. That is, A  stands for the entire antecedent fuzzy set no matter 
what the level and subsystem are. By using product inference, center-averaging and singleton fuzzification, 
the output of FNN1 at Level-1 is 
   )(
)(
)(
2
11
1 1
,2
1 1
,2
1
1
1
1
Yw
y
yw
y T
h
i
l
k
kA
h
i
l
k
kAi
i
j
i
k
ϕ
µ
µ
=












=
∑ ∏
∑ ∏
= =
= =                          (2) 
where Thwwww ]...[
11
2
1
1
1
1
=  is the weighting vector for the hierarchical -FNN in Level 1 and 
T
lyyyY ]...[ ,22,21,22 = . The outputs of FNN2,j, j=1,2,…,l, at Level-2, which are the inputs of FNN1 at Level-1, 
can be expressed as 
Fig. 1 the configuration of the hierarchical -FNN  
Fig.1 
Subsystem(      )
Subsystem(      )
Subsystem(     )
1,2h
2,2h
jh ,2
1,2X
2,2X
jX ,2
…
1h
…
Subsystem(      )
lh ,2lX ,2
Subsystem(      )
1,2y
2,2y
jy ,2
ly ,2
1y…
…
Level 2 
Level 1 
12
1 1
2 2
n
s
s
n ns
y x
y x
y x
=
=
=

 
where ijx is the state, iy  is the output, and is  represents the order of i th sub-system in which 
1 2 ... ns s s s= + + + . When 1s = , the system is a single-input single-output system. 
 
 
 
 
 
 
 
 
We define the vector TTm
TT ]ˆˆ[]ˆˆˆ[ˆ 321 ζyxeeee ==  to be the estimate of the vector e . Fig. 2 shows 
the configuration of the hierarchical-FNN, which consists of a number of fuzzy-neural networks. The 
input vector of the hierarchical-FNN is eˆ  and the output of the hierarchical-FNN is fu . The output 
signal fu  is designed to approximate the control input u . To enhance its approximation capability, the 
input variables can be roughly divided into three groups, the state estimate vector xˆ , the reference signal 
vector my , and ζˆ . Then, these three groups are assigned to different FNNs in level II, and the outputs of 
level II are the inputs of the level I. The ith fuzzy IF-THEN rule in the hierarchical-FNN is expressed in 
four parts as 
1 1
2 2
2,1 2,1 2,1
11 1 1 2,1 1 2,1
2,2 2,2 2,2
21 1 2 2,2 1 2,2
2,
1 1
ˆˆIF is and ...and is then is  , For FNN
ˆˆIF is and ...and is then is  , For FNN
                                               
ˆIF is and ...an
g g
g g
n
M
A A y B
A A y B
A
ε ε
ε ε
ε

2, 2,
2, 1 2,M
1,1 1,1 1,1
2,1 2 2, 1,1 1 1,1
ˆd is then is , For FNN
IF is and ...and is then is , For FNN
M
M M
g M
Mg g M
M M
A y B
y A y A y B
ε
                 (6)
 
The notation of equation (6) is the same as them of equation (1). By using product inference, center-averaging 
and singleton fuzzification, the output of FNN1,1 at level I is 
1,1 3
1,
2,
1 1
1,1 1,1 1,1 21,1 3
2,
1 1
( )
( )
( )
h
i
i kAki k
h
i kAki k
c y
y
y
µ
ϕ
µ
= =
= =
 
 
 = =
 
 
 
∑ ∏
∑ ∏
p y                      (7) 
where ic ,1  is the center of the fuzzy set iB ,1 , and 2 21 22 2[ ... ]
T
My y y=y . The outputs of FNN2,i, i=1,2,....M, 
at level II, which are the inputs of FNN1,1 at level I, can be expressed as 
Level II Level I 
Fig. 2 
1,2FNN  
2,2FNN  
2,MFNN  
1,1FNN  F
u  1,2y
 
 
2,2y  
2,My
 
+ 
+ 
+ + 
1 2
ˆˆˆˆ [ ]
MM M M Mg
ε ε ε=ε   
22 21 22 2
ˆˆˆˆ [ ]gε ε ε=ε   
11 11 12 1
ˆˆˆˆ [ ]gε ε ε=ε   
 

 

 

 







>
><−
>≥
=
αα
ρ
αρ
αρ
|~|
~
constantpositiveaisαwhere,|~|and0~
|~|and0~
1
,1
11
11
eife
eeif
eeif
v
i
i .             (15) 
 
3. Dynamic hierarchical fuzzy neural networks 
As Fig. 4 (a) shows, we use GA to divide l  groups form the input variable of the hierarchical fuzzy 
neural network. The corresponding hierarchical -FNN is shown as Fig. 4(b). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In order to find the desired hierarchical-FNN (i.e. less complexity of the structure or computational and 
memory demands) for general continuous functions, the two-stage genetic algorithm with the randomly 
grouping method will be proposed in this project. The two-stage genetic algorithm includes the first stepGA −  
and second stepGA − . The first stepGA −  construct the hierarchical -FNN at the first and the second stepGA − optimize the 
hierarchical -FNN constructed by first stepGA −  subsequently. The flowchart of the two-stage GA and the 
overall system are shown in fig. 4(a) and fig. 4(b), respectively.  
 
4. 結果 
Example 1: we approximate the Taiwanese stock market with 12 input variables from the fundamental 
analysis and technical analysis form January 1, 1998 to April 10, 1999, a period of 100 days. The factors for 
the fundamental analysis include the leading indicator, the consumer price index, and the prime lending rate. 
Some information used for approximating the Taiwanese stock market are as follows: 
1. The training data are normalized into the range [0, 1] for convenience;  
2. Each input of the FNN has 3 B-spline membership functions (BMFs);  
3x 4x 5x 6x 7x 8x 9x10x 11x 12x1x 2x
Group 1 … Group l
Group j
…
nx…
Group 2
(a) 
Fig. 3 (a) the randomly grouping method (b) the corresponding structure 
of the hierarchical-FNN 
Subsystem(      )
Subsystem(      )
Subsystem(     )
1,2
qh
2,2
qh
j
qh
,2
T
q xxxX ][ 321
1,2 =
T
q xxX ][ 54
2,2 =
7
,2 xX jq =
… 1
qh
…
Subsystem(      )l
qh
,2T
n
l
q xxX ]...[ 12
,2 =
Subsystem(      )
1,2
qy
2,2
qy
j
qy
,2
l
qy
,2
1
qy…
…
(b) 
Fig. 6 shows error curve and the output of the hierarchical FNN when learning iteration is 30. 
 
Fig 6 Output of the HFNN trained by two-stage using PSO/GA hybrid when training data pair is 70 
Example 3: Our objective is to control the outputs 1y  and 2y  of the two-link planar manipulator to track the 
desired trajectories 1 0.8sin( )my t=  and 2 0.8cos( )my t= , respectively. The design parameters are chosen as 
2γ = , 60ρ = , and 31.36 10pm = × . The feedback and observer gain vectors are selected as 
144 24 0 0
0 0 144 24
T
c
 
=  
 
K  and 80 900 0 0
0 0 60 900o
 
=  
 
K , respectively. The filter 1( )iL s
−  is selected as 1( ) 1 ( 2)iL s s
− = + . The 
parameter values are 1 0.5m kg= , 2 0.5m kg= , 1 1l m= , 2 0.8l m=  and 2sec9.81meg = . Here, it is assumed that 
the external disturbances d1 and 2d  are values randomly in the interval [ 0.05,0.05]− . We select the initial 
states of the system are assumed as (0) [0.1,0,0.1,0]T=x . Figure 7(a) shows the tracking trajectories of the 
first output 1y  and the reference output 1my . Figure 7(b) shows the tracking trajectories of the second output 
2y  and the reference output 2my . Also, the control inputs u1 and u2  are shown in Figs. 7(c) and 7(d). 
 
 
5. 結果與討論 
Compared to the tradition fuzzy neural network, the hierarchical FNN constructed by the present GA_RG 
and GA/PSO hybrid learning algorithm can effectively approximate a unknown continuous function. In our 
algorithm, first we bring the GA_RG construct a dynamic hierarchical structure. Second, GA/PSO hybrid 
learning algorithm is used to handle the vast number of adjustable parameter in the hierarchical FNN. From 
Fig. 7. (a) the output y1; (b) the output y2; (c) the control u1; (d) the 
control u2 
299-303, 1993. 
[16] L.X. Wang and J.M. Mendel, "Fuzzy basis functions, universal approximation, and orthogonal least 
squares learning," IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 807-814, 1992. 
[17] K. Hornick, M. Stinchcombe, H. White, “ Mutlilayer feed-forward networks are universal approximators,” 
Neural Networks, vol. 2, pp. 359-366, 1989. 
[18] H. Nomura, I. Hayashi and N. Wakami, “A self-tuning method of fuzzy control by descend method,” 
Proceedings of 4th IFSA Congress, Engineering, pp. 155-158, 1991. 
[19] Y. Ito, “Representation of functions by superpositions of a step of sigmoid function and their applications 
to neural networks theory,” Neural Networks, vol. 4, pp. 385-384, 1991. 
[20] M. Karam, M.S. Fadali, and K. White, “A Fourier/Hopfield neural network for identification of nonlinear 
periodic systems,” Proceedings of the 35th Southeastern Symposium on System Theory, pp. 53 -57, 2003.  
[21] Q.Z. Chang, and M. Sami Fadali, “Nonlinear system identification using a Gabor/Hopfield network,” 
IEEE Transactions. on Systems, Man, and Cybernetics, vol. 26, no. 1, pp. 124-134, February 1996. 
[22] S.K. Park, “Bearing estimation using Hopfield neural network,” IEEE Conference on System Theory, pp. 
440-443, March 1990. 
[23] N. Kamiura, T. Isokawa, N. Matsui, “Learning based on fault injection and weight restriction for 
fault-tolerant Hopfield neural networks,” IEEE Intl. Symposium on Defect and Fault Tolerance in VLSI 
Systems, pp.339-346, Oct. 2004. 
[24] C.H. Wang, W.Y. Wang, T.T. Lee, and P.S. Tseng, “Fuzzy B-spline membership function (BMF) and its 
applications in fuzzy-neural control,” IEEE Transactions on Systems Man and Cybernetics, vol. 25, no. 5, 
pp. 841-851, May 1995. 
[25] W. Y. Wang, C. C. Hsu, C. W. Tao, and Y. H. Li, “Fuzzy-neural function approximation using a vector 
evaluation genetic algorithm,” ICONS, 2003. 
[26] W. Y. Wang and Y. H. Li, “Evolutionary learning of BMF fuzzy-neural networks using a reduced-form 
genetic algorithm,” IEEE Transactions on Systems, Man, And Cybernetics-Part B, vol. 33, no. 6, pp. 
966-976, Dec. 2003. 
[27] Y. S. Lee, C. H. Kao, and W. Y. Wang, “BMF fuzzy neural network with genetic algorithm for forecasting 
electric load,” IEEE PEDS, pp. 1662-1667, 2005. 
[28] Y. S. Lee, T. Y. Kuo, and W. Y. Wang, “Fuzzy neural network genetic approach to design the SOC 
estimator for battery powered electric scooter,” IEEE Conference on Power Electronics Specialists 
Conference, pp. 2759-2765, 2004. 
[29] I-Hsum Li, Wei-Yen Wang, Shun-Feng Su, and Yuang-Shung Lee, “A novel learning structure of fuzzy 
neural networks using reduced-form genetic algorithms,” 12th International Conference on Neural 
Information Processing, pp. 219-223, Oct., 2005. 
[30] X.J. Zeng and J.A. Keane, “Approximation capabilities of hierarchical fuzzy systems,” IEEE 
Transactions on Fuzzy Systems, vol. 13, no. 5, October, 2005. 
[31] X.J. Zeng and J.A. Keane, “Separable approximation property of hierarchical fuzzy 
systems,” IEEE Conf. on Fuzzy System, pp. 951-956, May 2005. 
[32] Z.G. Hou, M.M. Gupta, P.N. Nikiforuk, and M. Tan, and L. Cheng, “A recurrent neural network for 
hierarchical control of interconnected dynamic systems,” IEEE Transactions on Neural Networks, vol. 18, 
no. 2, pp.466-481, 2007. 
 
 
 圖一 會場 Centre de Convencions Internacional de Barcelona (CCIB) 
 
圖二 註冊櫃台 
 
圖三 發表現在現況 
 
無研發成果推廣資料 
國外 論文著作 期刊論文 3 3 100% 篇 
1. I-Hsum Li, 
Shu-Chang Li, 
and Wei-Yen 
Wang, ＇A 
Dynamic 
Hierarchical 
Fuzzy Neural 
Network with 
Hybrid PSO-GA 
for A Chemical 
Plant, ＇ 
International 
Journal of 
Intelligent 
Systems Science 
and Technology, 
vol. 1, no. 1, 
2009. 
2. I-Hsum Li, and 
LianWang 
Lee, ＇A 
Hierarchical 
Structure of 
Observer-Based 
Adaptive 
Fuzzy-Neural 
Controller for 
MIMO Systems,＇ 
submit to Fuzzy 
sets and 
systems, May,
2010. 
3. LianWang Lee 
and I-Hsum 
Li, ＇A 
H-Infinity 
Adaptive 
Sliding-Mode 
Controller for 
Nonlinear 
Pneumatic 
Actuator Systems 
via a Fourier 
Series Neural 
Network 
Approach,＇ 
submit to Expert 
Systems with 
Applications, 
September, 2010.
 
 
