like APM programming is compatible with CUDA 
computing in the slider cases, but is significantly 
worse in the case of air bearing. This study 
illustrates the workstation computing for lubrication 
analyses in which the computing devices can be multi-
core CPUs or many-core GPUs. 
英文關鍵詞： Reynolds equation, fluid-film lubrication, graphics 
processing unit, parallel computing 
 
II 
 
目錄 
摘要                   III 
Abstract                   III 
Introduction                  1 
Methods                   2 
Results                   5 
Discussion                  6 
Conclusion                  6 
Acknowledgements                7 
Nomenclature                  7 
References                  8 
Tables                   9 
Figures                   10 
國科會補助專題研究計畫成果報告自評表           14 
 
1 
 
INTRODUCTION 
The numerical solution of some form of Reynolds equation is usually required in many fluid-film 
lubrication analyses or bearing designs. Many solution methods are available for a bearing design using a 
workstation computer. The workstation may be equipped with one or more multi-core central processing units 
(CPUs) and many-core graphics processing units (GPUs). The programming scheme of a solution method can 
be based on a number of parallel computing models. For instance, the parallel computing for various 
lubrication analyses can be conducted in a shared-memory system (Wang and Tsai (1); Wang, et al. (2)) or a 
cluster of computers (Wang, et al. (2); Wang and Chen (3); Wang and Yen (4); Wang (5); Wang, et al. (6)). 
Since the sole purpose of applying parallel computing in lubrication analyses is to minimize the execution 
time, which may result a detailed bearing design via optimization and/or finer numerical grids. To avoid 
rewriting a sequential code later for a lubrication analysis, it is best to implement parallel computing scheme 
right from the start.  
The full potential of a multi-core CPU is released when all the processor cores in the system share the 
workload at the same time. Traditionally, the computational capability of a single-core CPU for a given 
program is directly proportional to its clock speed. Other than the clock speed effect, some improvement in 
single-thread performance may be obtained due to architectural sophistication in CPUs. The clock speed of 
commodity single-core CPUs reaches its peak, 3.8 GHz, in 2004. Since then, the peak clock speed is 
diminished with the advent of multiple processor cores in CPUs. As a result, the overall computing capability 
is improved by increasing the number of processor cores as well as with the increased cache size and the 
architectural sophistication of CPUs. In 2010, the number of processor cores goes up to six in Intel's powerful 
but expensive Xeon CPUs. Nevertheless, the number of processor cores in a workstation equipped with two of 
those CPUs is only a dozen. And the total number of processor cores in a similar system is not likely to 
increase dramatically in the near future. 
An emerging powerful tool for engineering and scientific modeling is the computing using 
high-performance GPUs. Recently, the applications using GPU computing for non-graphics related 
computation are abundant in scientific and engineering fields (Owens, et al. (7); Elsen, el al. (8); Ryoo, et al. 
(9)). Currently, many systems on the world’s 500 supercomputers list are configured with GPUs (10). The 
main advantage of using GPU computing is its ability to handle a large number of concurrent threads (up to 
512 in 2010). For the past several years the GPUs support only single-precision computation. The real 
numbers represented by single-precision are only accurate to six significant digits. In a preliminary study 
performed by the authors, the six-digit rounding setup is adequate for small gridworks, 256´256 or less, in 
solving the isoviscous incompressible-fluid Reynolds equation iteratively. For a larger gridwork, such as 
1,024 ´ 1,024, the iterative solution of the Reynolds equation using single-precision computing cannot 
converge when the stopping criterion specified in Ref. (11) was applied. The non-convergence is due to the 
sum of the residuals squared in iteration cannot be lowered after some number of iterations using 
single-precision operation. The high-level programming languages (C, C++, and Fortran) and the 
double-precision computation, which are essential in lubrication analyses, are now supported in GPU 
computing ((12) and (13)).  
According to Ref. (14), the CPUs are most effective for dynamic workloads marked by short sequences 
of computational operations and unpredictable control flow. At the other extreme, GPUs are designed for 
workloads that are dominated by computational work performed within a simple control flow. The GPU can 
be treated like a many-core processor. The key GPU design goal is to maximize floating-point throughput. For 
instance, some of the NVIDIA graphics cards are designed especially for high performance computing, which 
have no display output ports. In these graphics cards the drivers are optimized for GPU computing instead of 
graphics rendering. The computing for non-graphics processing on GPUs has been known as general purpose 
GPU (GPGPU) programming since 2002 (15). 
When selecting a parallel programming model for lubrication analyses one needs to consider the 
computing platform (e.g., GPU, distributed- or shared-memory system), the programming paradigm (e.g., 
GPU, MPI, or OpenMP programming) and the implementing language (e.g., Fortran or C/C++). Among the 
most important factors in considering implementing parallel computing are parallel efficiency, programming 
effort, code portability, and cost. Presently, the OpenMP is the de facto standard for parallel computing in 
today’s multi-core shared-memory computers (Chapman, et al. (16)). The use of parallel-directive format and 
the incremental parallelization of OpenMP programming make the multithreaded computing straightforward. 
3 
 
( )
( )
x
hp
y
p
hp
yx
p
hp
x
x
h
y
p
h
y
hp
yx
p
h
x
hp
x
¶
¶
¶
¶
¶
¶
¶
¶
¶
¶
¶
d¶
¶
¶
d
¶
¶d
¶
¶
¶
¶
d
¶
¶d
¶
¶
6
6
33
3333
+÷÷
ø
ö
çç
è
æ
-÷÷
ø
ö
çç
è
æ
-=
-÷÷
ø
ö
çç
è
æ
++÷÷
ø
ö
çç
è
æ
+
.       [3] 
Two inclined-surface slider bearings, with and without a central recess, are tested. The slider with a 
central recessed is used to illustrate the condition of discontinued boundary. The data of the sliders are: 
10 ££ x , 10 ££ y , and 21-=h . The recess is located at the center of the bearing where 6.04.0 ££ x  
and 6.04.0 ££ y . The boundary pressure and the recess pressure are assumed to be zero in the simulation. 
The last case is an air journal bearing governed by Eq. [2]. The data of the air bearing are: 10 ££ x , 
10 ££ y , and 9.0=e .The pressure boundary condition of the air bearing is atmospheric pressure.  
 
Computing Devices 
 Table 1 shows the key configuration of the workstations and the software used in this study. The 
eight-core workstation is used as a reference to illustrate the characteristics of multithreaded computing due to 
architecture difference. The 12-core workstation is used to conduct the major computations of GPU and 
multithreaded programming and is installed with two graphics cards. The NIVIDIA Quadro 4000 is dedicated 
for the GPU computing, while the other graphic card is attached to the monitor for display purpose. In this 
study, the market value of the GPU for computing is about one-half of a CPU used in the 12-core workstation.  
 The speedup due to parallel computing is calculated by taking the ratio of the execution (elapsed) time on 
n processor cores and the execution time of the sequential version or the time of one processor core. In a 
computation, the execution time of a program in a machine is mainly affected by the compiler and its 
compilation options. In this study, the execution time and speedup are used as the performance indicators. 
When a program is executed in a workstation the elapsed time is the summation of the execution time 
required and the time spent on the background activities during the execution. The amount of background 
activities varies in each execution. The variation of the time spent in the background activities results in the 
fluctuation of the elapsed time. Instead of averaging the execution time from several runs, the execution time 
obtained in this study is by running a case five times and record the one with shortest elapsed time. This 
should better reflect the computing performance by using the actual execution time required. 
 
Graphics Processing Unit Computing 
 The GPU computing is a relatively new computing tool. Currently, the GPU programming models used 
to release the computing power from NVIDIA GPUs is CUDA (Compute Unified Device Architecture) and 
APM (Accelerator Programming Model of Portland Group). Both Fortran and C languages are supported in 
CUDA and APM. The CUDA coding from NVIDIA is used to explicitly control and program an NVIDIA 
GPU. While CUDA is a large step towards GPU programming, it requires significant rewriting and 
restructuring the code. The key CUDA coding to apply GPU for computing can be accomplished by two steps. 
The first step is to maps the host (CPU) arrays to be used in (device) GPU and transfer the data to the device. 
Then, perform the computing in GPU and transfer the results back to the host arrays. The detail of CUDA 
operation can be found in Refs. (12) and (13). In order to ensure high efficiency in parallel computing the 
CUDA coding is not conform to Fortran or C language. This prevents the portability among different 
platforms.  
 On the other hand, the format and usage of the APM is OpenMP-like. Both OpenMP and APM 
paradigms are directive-based coding and can be incrementally applied in the bottleneck of a program. In the 
APM programming the data movement between the GPU and the system main memories is implicit and 
managed by the compiler in the form of directives (19). For instance, to perform parallel computation in a 
do-loop the directive invoked by the compiler is placed ahead of the do-loop statement. The directive in the 
OpenMP is “!$omp parallel do” while in the APM is “!$acc do parallel”. The functionality of APM or 
OpenMP directives can be invoked or ignored during program compilation stage. As a result, the coding 
development investment by lubrication engineers can be accommodated via compiler for future hardware. 
 
5 
 
equation depends on the values of the adjacent four black points. Other than the boundary points the values of 
the black points are from a previous iteration or initial guess. The computation of all red points can be 
executed in parallel among the available processor cores or GPU cores. The next step to complete the whole 
domain computation is to perform the black-point calculation. Similar operation as the red-point can then be 
carried out, but the updating is with the newly computed red points which can accelerate the convergence. The 
RBSOR method is used in both the GPU and multithreaded computing. 
In this study, the iterative process for the solution methods is terminated using a criterion based on 
truncation error analysis (Wang, et al. (11)). The stopping criterion is of vital importance in timing the 
execution time, by which the solution accuracy can be ensured and the number of iterations obtained is 
adequate. In a parallel computing, the communication latency can be counted as non-parallel portion of the 
execution. In a parallel iterative solution for lubrication models, the frequency of the communication among 
threads and data transfer between the GPU and CPU is a function of the number of iterations required. Thus, 
the effectiveness of parallel computing in solving Reynolds equations, which usually requires many iterations, 
may be reduced. The percentile of non-parallel portion of computing is larger in GPU than in multithreaded 
computing.  
 
Relaxation Factor 
 The convergence rate of the RBSOR method is highly dependent on the value of relaxation factor (w ) 
used. For a given bearing model and geometry the w  is basically a function of the grid-size used. A simple 
procedure is used in this study to determine the w  for the cases tested. In each lubrication model, the w  is 
determined from numerical tests of 256×256 gridwork. Then, the constant C in Eq. [5] can be computed using 
256/1=Dx . The values of w  for the other grid sizes can then be computed by using Eq. [5] again with 
respective xD .  
xC D+
=
1
2
w ,                  [5] 
where xD  is the reciprocal of the grid size number, and C  is a constant for a given problem. In this study, 
C = 7.6329 and 69.488 in solving Eqs. [1] and [3], respectively. 
 
RESULTS 
 Figs. 3 and 4 show the execution time and speedup respectively for various numerical grids using 
OpenMP programming for the solution of Eq. [1] modeling the inclined-surface slider. The two workstations 
listed in Table 1 are used. The large difference in execution time shown in Fig. 3 is the result of different 
number of cores, CPU clock speed, memory clock rate, and CPU cache size. In calculating Eq. [1] the size of 
double-precision real numbers required in the program is equal to 22 xN´ . For =xN 768, 1,024 and 1,280 
the required size of arrays in MB are 4.72, 8.39 and 13.11, respectively. Fig. 4 discloses the effect of CPU 
cache size (12 MB versus 4 MB per CPU) on computing performance for the fine-grain computation. It can be 
seen that the effectiveness of parallel computing (speedup) in the 12-core workstation is much better than that 
of the 8-core workstation for the cases of grid sizes = 768, 1,024 and 1,280. When the grid size is greater than 
or equal to 1,536, the speedups are significantly reduced for both the workstations. This is the result of 
running short of cache size for either workstation. 
Provided that the memory size of GPU or main system is sufficient, the execution time of GPU (can be 
treated as a whole device) or single-thread computing can scale up almost linearly in the tested cases. The 
linear scale-up means the execution time is proportional to the array size ( 2xN ) and the number of iterations. 
The scale-up capability is an important characteristic in parallel computing. A perfect multithreaded parallel 
computing should have the speedup equals the number of available processor cores. In this study, the scale-up 
for multithreaded computing is inadequate for the cases when a larger cache size is not possible (Fig. 4).  
The execution times of the GPU and multithreaded computing for the solution of Eq. [1] modeling the 
two sliders are plotted in Figs. 5 and 6 as the function of numerical grid size. The GPU computing is 
performed by either CUDA or APM programming using the PGI compiler. The multithreaded computing is 
conducted in the 12-core workstation by OpenMP programming using the Intel compiler. For all the cases 
7 
 
ensure the portability of parallel-computing-coded programs. Therefore, the computing either conducted by 
OpenMP or APM coding is suggested, of which can be invoked or ignored during the compilation stage. The 
approach presented in this study should be useful for lubrication engineers using workstations equipped with 
multi-core CPUs and many-core GPUs.  
 
ACKNOWLEDGEMENTS 
This study is supported by the National Science Council of ROC, contract number NSC 
100-2221-E-182-033.  
 
NOMENCLATURE 
a,b,c = elements in matrices A, B and C 
A  = matrix (Eq. [4]) 
B  = width of bearing or matrix (Eq. [4]) 
C  = matrix or constant in Eq. [5] 
h  = fluid film thickness 
h0  = minimum fluid film thickness 
h   = dimensionless fluid film thickness 
L, M, N = size of matrices (Eq. [4]) 
Nx, Ny = number of gridworks in x- and y-direction 
p  = fluid film pressure 
p   = dimensionless fluid film pressure 
U  = bearing velocity 
x, y  = coordinates 
x , y  = dimensionless coordinates 
xD , yD  = dimensionless grid size in x- and y-direction 
pD   = residual of p  
d   = Modification of p  (Eq. [3]) 
e   = bearing eccentricity ratio 
m  = lubricant viscosity 
w  = relaxation factor of iterative method 
Subscripts and Superscript 
i , j, k = indices for numerical grids or matrices 
n  = index of iteration 
9 
 
Table 1 The configuration of the workstations 
Softw
are 
Operation System Microsoft Windows 7 Professional 64-bit 
Language Fortran 95 
Compiler 
(multithread) Intel Fortran 12.0.4.196 64-bit 
Compiler (GPU) PGI Visual Fortran 11.6 64-bit 
Works
tation 
(reference) 
Model HP Z600 workstation 
RAM (main system) 8 GB/800 MHz 
CPU (x2) Intel Xeon E5504/2.00 GHz 8 cores (2 CPUs, 4-core/CPU), 4 MB cache/CPU 
Works
tation 
Model HP Z600 workstation 
RAM (main system) 8 GB/1,333 MHz 
CPU (x2) 
Intel Xeon X5650/2.66 GHz 
12 cores (2 CPUs, 6-core/CPU), 12 MB 
cache/CPU 
GPU (for display) NVIDIA Quadro FX580 
GPU (for computing) 
NVIDIA Quadro 4000/0.95 GHz 
256 cores (32 multiprocessors, 
8-core/multiprocessor) 
2 GB memory (dedicated video memory) 
 
 
Table 2 The computing performance of the devices in a matrix multiplication (N = 2,560, M = 5,120, and L = 
2,560).  
Compiler Compiler Options Programming Model 
Program 
Portability 
Execution 
Time (s) Speedup 
PGI ver. 11.6 
-fast GPU (CUDA) No 1.747 N/A -fast -ta=nvidia:cc20 GPU (APM) 
Yes 
1.779 
-fast -mp=allcores OpenMP 5.429 5.2 
-fast -Mconcur=allcores Auto Config. 5.507 N/A 
Intel ver. 12 /fast /Qopenmp OpenMP 0.905 9.1 /fast /Qparallel Auto Config. 0.952 N/A 
 
11 
 
0 512 1024 1536 2048 2560
0.01
0.1
1
10
100
1000
 
 
E
xe
cu
tio
n 
Ti
m
e 
(s
)
Grid Size (N
X
, N
Y
=N
X
)
Incompressible-fluid (inclined-surface slider)
OpenMP coding (Intel compiler)
 2.00 GHz (1-thread, 4 MB cache)
 2.00 GHz (8-thread, 4 MB cache)
 2.66 GHz (1-thread, 12 MB cache)
 2.66 GHz (12-thread, 12 MB cache)
 
Fig. 3 The execution times versus the grid size for the solution of the incompressible-fluid Reynolds equation 
(Eq. [1]) using OpenMP programming. 
 
0 512 1024 1536 2048 2560
0
2
4
6
8
10
12
 
 
Grid Size (NX, NY=NX)
Sp
ee
du
p
Inclined-surface slider
OpenMP computing
 2.00 GHz CPU (8-core, 4 MB cache)
 2.66 GHz CPU (12-core, 12 MB cache)
 
Fig. 4 The speedups versus the grid size for the solution of the incompressible-fluid Reynolds equation (Eq. 
[1]) using OpenMP programming. 
13 
 
12-core workstation using OpenMP programming. 
 
0 512 1024 1536 2048 2560
0.1
1
10
100
1000
(7.64)
(7.11)
(10.65)
(7.30)
 
 
Ex
ec
ut
io
n 
Ti
m
e 
(s
)
Grid Size (Nx, Ny=Nx)
Air journal bearing
 GPU CUDA computing
 GPU APM computing
 OpenMP computing (12-core)
 Sequential computing (one core)
(#): Speedup (OpenMP)
(6.02)
 
Fig. 7 The execution times versus the grid size for the solution of the journal air bearing model (Eq. [2]). The 
GPU computing is performed by CUDA or AMP programming. The multithreaded computing is conducted in 
the 12-core workstation using OpenMP coding. 
 
 
 1
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                日期：101年 11月 6日 
一  、 參加會議經過 
    2011年International Tribology Conference (ITC) 於10月30日至11月3日在日本
的廣島市（Hiroshima）舉行。前次ITC會議在2005年於日本Kobe舉行。此次會議
由廣島大學(Hiroshima University)的Nagamura教授擔任大會主席，部份發表之論
文將收集並發表於Tribology Online。本次會議在日本舉行，與會學者來自許多國
家，包括中國、韓國、印度、美國、英國、德國、法國及澳洲等，來自台灣的參
與人員只有本校二位老師（本系之吳俊仲老師及筆者）和二位筆者指導的博士
生。國內與會人數偏少，應與其它研究單位或學校的出國經費及須以英文口頭報
告有關。今年會議共有約319篇論文發表，並有多篇學生海報參加。此次會議期
間的口頭發表，每篇論文發表及討論的時間為25分鐘，大會並將所有論文摘要內
容收錄於隨身碟中，並附贈於報到時領取的資料袋中。另外，大會共安排二個半
天及一個全天的自費團體旅遊行程。 
    筆者此行（交通費、註冊費、生活費）由國科會及長庚大學計畫經費補助，
分別由指導的博士生發表研究論文：1. （詹嘉文）Application of GPU Computing 
計畫編號 NSC 100-2221-E-182-033 
計畫名稱 應用繪圖處理器計算於流體薄膜潤滑之研究 
出國人員
姓名 
王能治 
服務機構
及職稱 
長庚大學 機械系 教授 
會議時間 
100年 10月 30日至
100年 11月 3日 會議地點 Hiroshima, Japan 
會議名稱 International Tribology Conference – ITC 2011 
發表論文
題目 
(1) Application of GPU Computing in Hydrostatic Bearing Optimization  
(2) A parallel computing scheme for the analysis of air foil bearings 
 3
趣，對於未來的進一步研究有很大的助益。此次發表的二篇論文已於出國前投稿
於相關期刊，筆者並將以此為基礎，進行下一階段的研究，以期能發揮繪圖處理
器的平行計算效率。 
 另外，值得一提的是本次會議地點在廣島市於第二次世界大戰美軍投下第一
顆原子彈（1945年8月6日）的紀念館及紀念公園旁，距離當年的原爆中心的平面
距離不到2百公尺（原子彈於離地面約580公尺處被引爆），並保留一個受損的建
築物，目前列為於世界遺產，以提醒眾人核子武器的威脅及其破壞的威力。每天
都有眾多學生及團體到紀念館進行校外教學，期許二戰的慘痛經驗及代價能換得
未來的和平。 
 
三、攜回資料名稱及內容 
² Tribology Online論文集。 
² ITC2011大會中發表的的論文摘要。 
 
100年度專題研究計畫研究成果彙整表 
計畫主持人：王能治 計畫編號：100-2221-E-182-033- 
計畫名稱：應用繪圖處理器計算於流體薄膜潤滑之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
