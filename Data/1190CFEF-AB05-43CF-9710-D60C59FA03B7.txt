While this result is of tremendous theoretical 
importance, the reality of practical systems looks 
quite different: no communication system will 
tolerate an infinite delay caused by an extremely 
large blocklength, nor can it deal with the 
computational complexity of decoding such huge 
codewords. On the other hand, it is not necessary to 
have an error probability that is exactly zero 
either, a small, but finite value will suffice.  
Therefore, the question arises what can be done in a 
practical scheme. In particular, what is the maximal 
rate at which information can be transmitted over a 
communication channel for a given fixed maximum 
blocklength (i.e., a fixed maximum delay) if we allow 
a certain maximal probability of error? In this 
project, we have started to study these questions. 
Block-codes with very short blocklength over the most 
general binary channel, the binary asymmetric channel 
(BAC), are investigated. It is shown that for only 
two possible messages, flip-flop codes are optimal, 
however, depending on the blocklength and the channel 
parameters, not necessarily the linear flip-flop 
code. Further it is shown that the optimal decoding 
rule is a threshold rule. Some fundamental 
dependencies of the best code on the channel are 
given. 
 
英文關鍵詞： Channel capacity, binary asymmetric channel (BAC), 
error probability, finite blocklengths, ML, optimal 
codes, Z-channel. 
 
Abstract
In his world-famous paper of 1948, Shannon defined channel capacity as the
ultimate rate at which information can be transmitted over a communication
channel with an error probability that will vanish if we allow the blocklength to
get infinitely large. While this result is of tremendous theoretical importance,
the reality of practical systems looks quite different: no communication system
will tolerate an infinite delay caused by an extremely large blocklength, nor can
it deal with the computational complexity of decoding such huge codewords. On
the other hand, it is not necessary to have an error probability that is exactly
zero either, a small, but finite value will suffice.
Therefore, the question arises what can be done in a practical scheme. In
particular, what is the maximal rate at which information can be transmitted
over a communication channel for a given fixed maximum blocklength (i.e., a
fixed maximum delay) if we allow a certain maximal probability of error? In
this project, we have started to study these questions.
Block-codes with very short blocklength over the most general binary chan-
nel, the binary asymmetric channel (BAC), are investigated. It is shown that
for only two possible messages, flip-flop codes are optimal, however, depending
on the blocklength and the channel parameters, not necessarily the linear flip-
flop code. Further it is shown that the optimal decoding rule is a threshold rule.
Some fundamental dependencies of the best code on the channel are given.
Block-codes with a very small number of codewords are investigated for the
two special binary memoryless channels, the binary symmetric channel (BSC)
and the Z-channel (ZC). The optimal (in the sense of minimum average error
probability, using maximum likelihood decoding) code structure is derived for
the cases of two, three, and four codewords and an arbitrary blocklength. It is
shown that for two possible messages, on a BSC, the so-called flip codes of type
t are optimal for any t, while on a ZC, the flip code of type 0 is optimal. For
codes with three or four messages it is shown that the so-called weak flip codes of
some given type are optimal where the type depends on the blocklength. For all
cases an algorithm is presented that constructs an optimal code for blocklength
n recursively from an optimal code of length n− 1. In the situation of two and
four messages, the optimal code is shown to be linear. For the ZC a recursive
optimal code design is conjectured in the case of five possible messages.
The derivation of these optimal codes relies heavily on a new approach
of constructing and analyzing the code-matrix not row-wise (codewords), but
column-wise. Moreover, these results also prove that the minimum Hamming
distance might be the wrong design criterion for optimal codes even for very
symmetric channels like the BSC.
Keywords: Channel capacity, binary asymmetric channel (BAC), error prob-
ability, finite blocklengths, ML, optimal codes, Z-channel.
error probability as long as the transmission rate in bits per channel use is below
the so-called capacity of the channel. However, he did not provide a way on how to
find such schemes, in particular he did not tell us much about the design of codes
apart from the fact that good codes need to have large blocklength.
For many practical applications exactly this latter constraint is rather unfortu-
nate as often we cannot tolerate too much delay (e.g., inter-human communication,
time-critical control and communication, etc.). Moreover, the system complexity
usually will grow exponentially in the blocklength. So we see that having large
blocklength might not be an option and we have to restrict the blocklength to some
reasonable size. The question now arises what can theoretically be said about the
performance of communication systems with such restricted block size.
During the last years there has been an increased interests in the theoretical
understanding of finite-length coding, see for example [2], [3]. There are several
possible approaches on how one can approach the problem of finite-length codes.
In [3] the authors fix an acceptable error probability and a finite blocklength and
then try to find bounds on the possible transmission rates. In another approach,
one fixes the transmission rate and studies how the error probability depends on the
blocklength (i.e., one basically studies error exponents, but for relatively small n)
[2]. Both approaches are related to Shannon’s ideas in the sense that they try to
make fundamental statements of what is possible and what not. The exact manner
in which these systems have to be built is ignored on purpose.
Our approach in this work is different: based on the insight that for very short
blocklength one has no big hope of transmitting much information with acceptable
error probability, we concentrate on codes with an only very small fixed number of
codewords: so called ultra-small block-codes. For such codes we try to find a best
possible design that minimizes the average error probability. Hence, we put a big
emphasis on finding insights in how to actually design an optimal system.
For these reasons we have started to investigate the fundamental behavior of
communication in the extreme case of an ultra-short blocklength. We would like to
ask the following questions: What performance can we expect from codes of fixed,
very short blocklength? What can we say about good design for such codes?
There are interesting applications for such codes. For example, in the situation
of establishing an initial connection in a wireless link, the amount of information
that needs to be transmitted during the setup of the link is very much limited to
usually only a couple of bits. However, these bits need to be transmitted in very
short time (e.g., blocklength in the range of n = 20 to n = 30) with the highest
possible reliability [4]. Note that while the motivation of this work focuses on rather
smaller values of n, our results nevertheless hold for arbitrary finite n.
The study of ultra-small block-codes is interesting not only because of the above
mentioned direct applications, but because their analytic description is a first step
to a better fundamental understanding of optimal nonlinear coding schemes (with
ML decoding) and of their performance based on the true error probability rather
than an upper bound on the error probability derived from the union bound. To
simplify our analysis, we have restricted ourselves for the moment to binary discrete
memoryless channels.
For simplification of the exposition, in this paper we will exclusively focus on
two special cases: the binary symmetric channel (BSC) and the Z-channel (ZC). For
results on general binary channels we refer to [5]. Note that while particularly for
the BSC much is known about linear code design [6], there is basically no literature
about optimal, possibly nonlinear codes.
4
2.2 Coding for DMC
Definition 3. A (M, n) coding scheme for a DMC (X ,Y, PY |X) consists of
• the message set M = {1, . . . ,M} of M equally likely random messages M ;
• the (M, n) codebook (or simply code) consisting of M length-n channel input
sequences, called codewords;
• an encoding function f : M → X n that assigns for every message m ∈ M a
codeword x = (x1, . . . , xn); and
• a decoding function g : Yn → Mˆ that maps the received channel output n-
sequence y to a guess mˆ ∈ Mˆ. (Usually, we have Mˆ =M.)
Note that an (M, n) code consist merely of a unsorted list of M codewords of
length n, whereas an (M, n) coding scheme additionally also defines the encoding
and decoding functions. Hence, the same code can be part of many different coding
schemes.
Definition 4. A code is called linear if the sum of any two codewords again is a
codeword.
Note that a linear code always contains the all-zero codeword.
The two main parameters of interest of a code are the number of possible mes-
sages M (the larger, the more information is transmitted) and the blocklength n
(the shorter, the less time is needed to transmit the message):
• we have M equally likely messages, i.e., the entropy is H(M) = log2M bits
and we need log2M bits to describe the message in binary form;
• we need n transmissions of a channel input symbol Xk over the channel in
order to transmit the complete message.
Hence, it makes sense to give the following definition.
Definition 5. The rate1 of a (M, n) code is defined as
R ,
log2M
n
bits/transmission. (4)
It describes what amount of information (i.e., what part of the log2M bits) is
transmitted in each channel use.
However, this definition of a rate makes only sense if the message really arrives
at the receiver, i.e., if the receiver does not make a decoding error!
Definition 6. An (M, n) coding scheme for a BAC consists of a codebook C (M,n)
with M binary codewords xm of length n, an encoder that maps every message m
into its corresponding codeword xm, and a decoder that makes a decoding decision
g(y) ∈ {1, . . . ,M} for every received binary n-vector y.
We will always assume that the M possible messages are equally likely.
1We define the rate here using a logarithm of base 2. However, we can use any logarithm as long
as we adapt the units accordingly.
6
the sense that it has the exact same error probability. Similarly, since we assume
equally likely messages, any permutation of rows only changes the assignment of
codewords to messages and has no impact on the performance. Therefore, in the
remainder of this paper, we will always consider such equivalent codes as being the
same. In particular, when we speak of unique design we do not exclude the always
possible permutations of columns and rows.
The most famous relation between code rate and error probability has been
derived by Shannon in his landmark paper from 1948 [1].
Theorem 9 (The Channel Coding Theorem for a DMC). Define
C , max
PX(·)
I(X;Y ) (15)
where X and Y have to be understood as input and output of a DMC and where the
maximization is over all input distributions PX(·).
Then for every R < C there exists a sequence of (2nR, n) coding schemes with
maximum error probability λ(n) → 0 as the blocklength n gets very large.
Conversely, any sequence of (2nR, n) coding schemes with maximum error prob-
ability λ(n) → 0 must have a rate R ≤ C.
So we see that C denotes the maximum rate at which reliable communication is
possible. Therefore C is called channel capacity.
Note that this theorem considers only the situation of n tending to infinity and
thereby the error probability going to zero. However, in a practical system, we
cannot allow the blocklength n to be too large because of delay and complexity. On
the other hand it is not necessary to have zero error probability either.
So the question arises what we can say about “capacity” for finite n, i.e., if
we allow a certain maximal probability of error, what is the smallest necessary
blocklength n to achieve it? Or, vice versa, fixing a certain short blocklength n,
what is the best average error probability that can be achieved? And, what is the
optimal code structure for a given channel?
3 Channel Models
In the following we will concentrate on the special cases of binary DMCs, i.e., we
restrict our channel alphabets to be binary.
The most general binary discrete memoryless channel is the so-called binary
asymmetric channel (BAC). It has a probability ǫ0 that an input 0 will be flipped
into a 1 and a (possible different) probability ǫ1 for a flip from 1 to 0. See Figure 1.
00
11
ǫ0
ǫ1
1− ǫ0
1− ǫ1
X Y
Figure 1: Binary asymmetric channel (BAC).
8
X Y
00
11
ǫ
ǫ
1− ǫ
1− ǫ
Figure 3: The binary symmetric channel (BSC).
X Y
00 1
11
ǫ1
1− ǫ1
Figure 4: The Z-channel (ZC).
4 Preliminaries
4.1 Capacity of the BAC
As mentioned above, without loss of generality, we only consider BACs with 0 ≤
ǫ0 ≤ ǫ1 ≤ 1. The capacity of a BAC is given by
CBAC =
ǫ0
1− ǫ0 − ǫ1
·Hb(ǫ1)−
1− ǫ1
1− ǫ0 − ǫ1
·Hb(ǫ0) + log2
(
1 + 2
Hb(ǫ0)−Hb(ǫ1)
1−ǫ0−ǫ1
)
(19)
bits, where Hb(·) is the binary entropy function defined as
Hb(p) , −p log2 p− (1− p) log2(1− p).
The input distribution P ∗X(·) that achieves this capacity is given by
P ∗X(0) = 1− P
∗
X(1) =
1− ǫ1(1 + z)
(1− ǫ0 − ǫ1)(1 + z)
(20)
with
z , 2
Hb(ǫ0)−Hb(ǫ1)
1−ǫ0−ǫ1 . (21)
4.2 Error Probability of the BAC
To simplify our notation we introduce dαβ(xm,y) to be the number of positions j
where x
(j)
m = α and y(j) = β, where as usual xm, i ∈ {1, 2, . . . ,M}, is the sent
codeword and y is the received sequence.
The conditional probability of the received vector given the sent codeword can
now be written as
PY|X(y|xm) = (1− ǫ0)
d00(xm,y) · ǫ
d01(xm,y)
0 · ǫ
d10(xm,y)
1 · (1− ǫ1)
d11(xm,y). (22)
10
4.4 Error (and Success) Probability of the Z-Channel
A special case of the BAC is the Z-channel where we have ǫ0 = 0. By symmetry,
assume that ǫ1 ≤
1
2 . Note that it is often easier to maximize the success probability
instead of minimizing the error probability. For the convenience of later derivations,
we now are going to derive its error and success probabilities:
P (n)c (C ) =
1
M
M∑
i=1
∑
y
g(y)=i
(1− ǫ1)
wH(xm)
(
ǫ1
1− ǫ1
)d10(xm,y)
· I
{
if x(j)m = 0 =⇒ y
(j) = 0, ∀ j
}
. (31)
The error probability formula is accordingly
P (n)e (C ) =
1
M
M∑
i=1
∑
y
g(y) 6=i
(1− ǫ1)
wH(xm)
(
ǫ1
1− ǫ1
)d10(xm,y)
· I
{
if x(j)m = 0 =⇒ y
(j) = 0, ∀ j
}
. (32)
Note that the capacity-achieving distribution for ǫ1 =
1
2 is
Pr[X = 1] =
2
5
. (33)
4.5 Pairwise Hamming Distance
The minimum Hamming distance is a well-known and often used quality criterion
of a codebook. However, for the description of an optimal code design for a fixed
blocklength n, it turns out to be too crude. We define a slightly more general and
more concise description of a codebook: the pairwise Hamming distance vector.
Definition 10. Given a codebook C (M,n) with codewords xm we define the pairwise
Hamming distance vector d(M,n) of length (M−1)M2 as
d(M,n) ,
(
d
(n)
12 , d
(n)
13 , d
(n)
23 , d
(n)
14 , d
(n)
24 , d
(n)
34 , . . . ,
d
(n)
1M, d
(n)
2M, . . . , d
(n)
(M−1)M
)
(34)
with d
(n)
ij , dH(xi,xj), 1 ≤ i < j ≤ M. The minimum Hamming distance d
(M,n)
min is
defined as the minimum component of the vector d(M,n).
Note that we have seen in Section 4.3 that the error probability of a binary code
that is used over a BSC can be described using the Hamming distance between
the received vectors y and the different codewords xm. We introduce the following
notation for this type of Hamming distance.
Definition 11. For a given codebook C (M,n) and for some received n-vector y, the
received Hamming distance vector d(n)(y) is defined as
d(n)(y) =
(
d
(n)
1 (y), . . . , d
(n)
M
(y)
)
,
(
dH(y,x1), . . . , dH(y,xM)
)
, (35)
where d
(n)
m (y) , dH(y,xm) denotes its mth component and is called received Ham-
ming distance.
Note that an ML decoder will always decode a received vector y to that message
that results into a minimum value of the received Hamming distance:
d
(n)
min(y) , min
m=1,...,M
d(n)m (y). (36)
12
6 Main Results
6.1 An Example
To show that the search for an optimal (possibly nonlinear) code is neither trivial
nor intuitive even in the symmetric BSC case, we would like to start with a small
example before we summarize our main results.
Example 15. Assume a BSC with cross probability ǫ = 0.4, M = 4, and a block-
length n = 4. Then consider the following two weak flip codes:
C
(4,4)
1,0 ,


0 0 0 0
0 0 0 1
1 1 1 0
1 1 1 1

 , C(4,4)2,0 ,


0 0 0 0
0 0 1 1
1 1 0 0
1 1 1 1

 . (44)
We observe that while both codes are linear, the first code has a minimum Hamming
distance 1, and the second has 2. Assuming an ML decoder, the average error prob-
ability can be expressed using the Hamming distance between the received sequence
and the codewords:
P (n)e (C) =
1
M
4∑
m=1
∑
y
g(y) 6=m
PY|X(y|xm) (45)
=
(1− ǫ)4
4
4∑
m=1
∑
y
g(y) 6=m
(
ǫ
1− ǫ
)dH(xm,y)
, (46)
where dH(xm,y) is the Hamming distance between a codeword xm and a received
vector y.
If evaluated, we get an error probability P
(n)
e = 0.6112 for C
(4,4)
1,0 and 0.64 for
C
(4,4)
2,0 . Hence, even though the minimum Hamming distance of the first codebook is
smaller, its overall performance is superior to the second codebook! ♦
Our goal is to find the structure of an optimal code C (M,n)∗ that satisfies
P (n)e
(
C
(M,n)∗
)
≤ P (n)e
(
C
(M,n)
)
, (47)
for any code C (M,n).
6.2 Optimal Codes on BAC for M = 2
Due to the memorylessness of the BAC, the order of the columns of any code is
irrelevant. We therefore can restrict ourselves without loss of generality to flip-flop
codes of type t to describe all possible flip-flop codes. Also note that the only possible
linear flip-flop code is C
(2,n)
0 . All other flip-flop codes are nonlinear.
We are now ready for the following result.
Proposition 16. Consider the case M = 2, and fix the blocklength n. Then, ir-
respective of the channel parameters ǫ0 and ǫ1, on a BAC there always exists a t,
0 ≤ t ≤
⌊
n
2
⌋
, such that the flip-flop code of type t, C
(2,n)
t , is an optimal code in the
sense that it minimizes the error probability.
Proof. See Appendix A.
14
In the situation of a flip-flop code of type t, C
(2,n)
t , the LLR is given as
LLR
(n)
t (ǫ0, ǫ1, d) , (t− d) log
(
1− ǫ1
ǫ0
)
+ (n− t− d) log
(
1− ǫ0
ǫ1
)
, (49)
where d is defined to be the Hamming distance between the received vector and the
first codeword:
d , dH(x1,y). (50)
Note that 0 ≤ d ≤ n depends on the received vector, while t and n are code
parameters, and ǫ0 and ǫ1 are channel parameters.
Hence, the optimal decision rule can be expressed in terms of d.
Proposition 17. We list some properties of LLR
(n)
t (ǫ0, ǫ1, d):
1. If ǫ0 + ǫ1 = 1, then LLR
(n)
t (ǫ0, ǫ1, d) = 0 for all d.
2. LLR
(n)
t (ǫ0, ǫ1, d) is a decreasing function in d:
LLR
(n)
t (ǫ0, ǫ1, d) ≥ LLR
(n)
t (ǫ0, ǫ1, d+ 1), ∀ 0 ≤ d ≤ n− 1. (51)
3. For d ≤ t and d >
⌊
n
2
⌋
the LLR
(n)
t is always larger or smaller than zero,
respectively:
LLR
(n)
t (ǫ0, ǫ1, d)


≥ 0 for 0 ≤ d ≤ t,
≶ 0 for t < d ≤
⌊
n
2
⌋
, depending on ǫ0, ǫ1,
≤ 0 for
⌊
n
2
⌋
< d ≤ n.
(52)
4. LLR
(n)
t (ǫ0, ǫ1, d) is an increasing function in n, when we fix d, ǫ0, and ǫ1.
5. LLR
(n)
t (ǫ0, ǫ1, d) is an increasing function in t when we fix n, d, ǫ0, and ǫ1.
6. For 0 ≤ d ≤ n− 1,
LLR
(n+1)
t (ǫ0, ǫ1, d+ 1) < LLR
(n)
t (ǫ0, ǫ1, d). (53)
Proof. Omitted.
From these properties we immediately obtain an interesting result about the
optimal decision rule.
Proposition 18 (Optimal Decision Rule has a Threshold). For a fixed flip-
flop code C
(2,n)
t and a fixed BAC (ǫ0, ǫ1) ∈ Ω, there exists a threshold ℓ, t ≤ ℓ ≤⌊
n−1
2
⌋
, such that the optimal ML decision rule can be stated as
g(y) =
{
1 if 0 ≤ d ≤ ℓ,
2 if ℓ+ 1 ≤ d ≤ n.
(54)
The threshold ℓ depends on (ǫ0, ǫ1), but similar channels will usually have the same
threshold. We define the region of channel parameters with identical threshold as
follows:
Ω
(n)
ℓ,t ,
{
(ǫ0, ǫ1)
∣∣∣LLR(n)t (ǫ0, ǫ1, ℓ) ≥ 0}⋂{(ǫ0, ǫ1) ∣∣∣LLR(n)t (ǫ0, ǫ1, ℓ+ 1) ≤ 0} .
(55)
In Figure 6 an example of this threshold behavior is shown. For ǫ0 ∈ [0.136, 0.270]
we see that LLR
(7)
1 (ǫ0, 1−2ǫ0, d) ≥ 0 for d = 0, d = 1, and d = 2, while LLR
(7)
1 (ǫ0, 1−
2ǫ0, d) < 0 for d ≥ 3. Hence, ℓ = 2.
16
6.4 Optimal Codes on ZC
Theorem 19. For a ZC and for any n ≥ 1, an optimal codebook with two codewords
M = 2 is the flip codebook of type 0, C
(2,n)
0 . It has an error probability
P (n)e
(
C
(2,n)
0
)
=
1
2
ǫn1 . (56)
Proof. Let the m-th codeword be xm = (xm,1, xm,2, . . . , xm,n), m = 1, 2, and let the
received vector be y = (y1, y2, . . . , yn). Then the average error probability of a ZC
can be expressed as
P (n)e (C ) =
1
2
∑
y
2∑
m=1
m 6=g(y)
(1− ǫ1)
wH(xm)
(
ǫ1
1− ǫ1
)d10(xm,y)
· I {xm,j = 0 =⇒ yj = 0, ∀ j} (57)
=
1
2
∑
y
min
{
PY|X(y|x1), PY|X(y|x2)
}
, (58)
where wH(xm) is the Hamming weight of the codeword xm and d10(xm,y) denotes
the number of positions j where xm,j = 1 and yj = 0.
Now note that Proposition 16 shows that an optimal code should have two code-
words that are flipped to each other. The intuition behind this is that an optimal
decoder will simply ignore all those bit positions where both codewords are iden-
tical, leading to the same performance that can be achieved for a code of shorter
length. We therefore now assume that x2 = x¯ is the flipped version of x1 = x, with
x defined in (37).
For such a flip code we now observe that due to the peculiarity of the ZC that
will never flip a zero to a one, we can only make an error if the received vector is
the all-zero vector y = 0:
min
{
PY|X(y|x), PY|X(y|x¯)
}
=
{
0 if y 6= 0,
ǫ
max{wH(x),wH(x¯)}
1 if y = 0.
(59)
This error probability is minimized if one of the codewords is the all-one codeword,
i.e., we see that C
(2,n)
0 is optimal.
Lemma 20. For a ZC and for any n ≥ 2, the average success probabilities of the
weak flip code of type (t, 0), 1 ≤ t ≤ ⌊n2 ⌋, with three codewords M = 3 or four
codewords M = 4 are
3P (n)c
(
C
(3,n)
t,0
)
= 1 +
t−1∑
d=0
(
t
d
)
(1− ǫ1)
t−d ǫd1 +
(n−t)−1∑
d=0
(
n− t
d
)
(1− ǫ1)
(n−t)−d ǫd1; (60)
4P (n)c
(
C
(4,n)
t,0
)
= 1 +
t−1∑
d=0
(
t
d
)
(1− ǫ1)
t−d ǫd1 +
n−t−1∑
d=0
(
n− t
d
)
(1− ǫ1)
(n−t)−d ǫd1
+
n−1∑
d=0
[(
n
d
)
−
(
n− t
d− t
)
−
(
t
d− (n− t)
)]
(1− ǫ1)
n−d ǫd1. (61)
Moreover, these average success probabilities are increasing with t.
18
Proof. Our proof is based on induction on n. The optimal code for M = 4 and n = 2
is trivial since there are only four possible different codewords. The optimal code is
C
(4,2)
1,0 =
(
c
(4)
1 , c
(4)
2
)
. (71)
Next assume that for blocklength n, C
(4,n)
⌊n
2
⌋,0 is optimal. Then, from Claim 22 below,
this assumption still holds for blocklength (n+ 1). This will prove the theorem.
Claim 22. Let’s append one new column to the weak flip code of type (⌊n2 ⌋, 0), C
(4,n)
⌊n
2
⌋,0,
to generate a new code of length (n + 1). The optimal (in the sense of resulting in
the biggest success probability) choice among all possible 2M = 16 columns is c
(4)
2 .
In fact, this claim holds not only for t = ⌊n2 ⌋ but for any t ∈ {1, 2 . . . , ⌊
n
2 ⌋}. Note
that there are actually only 14 possible columns that we could choose as the (n+1)-
th column because the all-zero and the all-one columns clearly are suboptimal as in
this case an optimal decoder will simply ignore the (n+ 1)-th received digit.
To prove Claim 22 we append an additional bit to all four codewords as follows:

[0 x1,n+1]
[x x2,n+1]
[x¯ x3,n+1]
[1 x4,n+1]

 (72)
where xm,n+1 ∈ {0, 1} and the x is given in (37) with t ,
⌊
n
2
⌋
. Note that in the
remainder of this proof t can be read as shorthand for ⌊n2 ⌋.
We now extend the decoding regions given in (66)–(69) by one bit for m =
1, 2, 3, 4: [D
(4,n)
t,m 0] ∪ [D
(4,n)
t,m 1]. Observe that these new decoding regions retain the
same success probability ψ
(4,n+1)
t,m = ψ
(4,n)
t,m · 1, because
PY |X(0|xm,n+1) + PY |X(1|xm,n+1) = 1. (73)
However, it is quite clear that these new regions are in general not the optimal
decision regions anymore for the new code. So the question is how to fix them to
make them optimal again (and thereby also finding out how to optimally choose
xm,n+1).
Firstly note that if xm,n+1 = 0, adding a 0 to the received vector y
(n) will
not change the decision m because 0 is the success outcome anyway. Similarly, if
xm,n+1 = 1, adding a 1 to the vector y
(n) will not change the decision m.
Secondly, we claim that even if xm,n+1 = 1, all received vectors y
(n+1) ∈ [D
(4,n)
t,m 0]
still will optimally be decoded to m. To see this, let’s have a look at the four cases
separately:
• [D
(4,n)
t,1 0]: The decoding region [D
(4,n)
t,1 0] only contains one vector: the all-zero
vector. We have
PY|X
(
0(n+1)
∣∣x(n+1)1 = [0(n)1]) = ǫ1
≥ PY|X
(
0(n+1)
∣∣x(n+1)j ), ∀ j = 2, 3, 4, (74)
independent of the choices of xj,n+1, j = 2, 3, 4. Hence, we decide for m = 1.
20
• [D
(4,n)
t,2 1]: In this case only D
(4,n+1)
t,4 will yield a nonzero additional conditional
success probability:
∆ψ4 =
∑
y(n)∈D
(4,n)
t,2
PY|X
(
[y(n) 1]
∣∣ [1(n) 1])
· (x4,n+1 − x2,n+1)
+ (85)
=
t−1∑
d=0
(
t
d
)
(1− ǫ1)
t−d · ǫn−t+d1 · (1− ǫ1)
· (x4,n+1 − x2,n+1)
+ (86)
=
(
ǫn−t1 − ǫ
n
1
)
· (1− ǫ1) · (x4,n+1 − x2,n+1)
+.
(87)
• [D
(4,n)
t,3 1]: Again, only D
(4,n+1)
t,4 will yield a nonzero additional conditional
success probability:
∆ψ4 =
∑
y(n)∈D
(4,n)
t,3
PY|X
(
[y(n) 1]
∣∣ [1(n) 1])
· (x4,n+1 − x3,n+1)
+ (88)
=
(
ǫt1 − ǫ
n
1
)
· (1− ǫ1) · (x4,n+1 − x3,n+1)
+. (89)
From ǫt1 ≥ ǫ
n−t
1 > ǫ
n
1 , we can therefore now conclude that the best solution for the
choice of xm,n+1 yielding the largest increase in success probability in (82), (83),
(84), (87), and (89) is as follows:


x2,n+1 − x1,n+1 = 1,
x4,n+1 − x2,n+1 = 0,
x4,n+1 − x3,n+1 = 1
=⇒


x1,n+1 = 0,
x2,n+1 = 1,
x3,n+1 = 0,
x4,n+1 = 1.
(90)
This will lead to a total increase of success probability of
4∆Pc = ǫ
t
1(1− ǫ1) + (ǫ
t
1 − ǫ
n
1 )(1− ǫ1). (91)
Note that for n even with t = n2 , adding the column c
(4)
2 to the code C
(4,n)
n
2
,0 will
result in a code that is equivalent to C
(4,n+1)
⌊n+1
2
⌋,0
by just exchanging the roles of the
second and third codeword and re-order the columns.
For n odd with t = ⌊n2 ⌋, adding the column c
(4)
2 to the code C
(4,n)
⌊n
2
⌋,0 results
in C
(4,n+1)
n+1
2
,0
. In particular, since t = ⌊n2 ⌋ < n − t, this also proves that for even
blocklength these optimal linear codes are unique.
Finally, the case with three codewords M = 3 can be proved in a similar manner.
We observe that (
c
(3)
1 , c
(3)
3
)
≡
(
c
(3)
1 , c
(3)
2
)
(92)
are optimal codebooks for n = 2. An optimal way of extending these codes is then
to add columns c
(3)
2 or c
(3)
3 .
Similarly, we also can prove that the codebook consisting of (n− t) columns c
(3)
1
and t columns arbitrarily chosen from c
(3)
2 or c
(3)
3 is optimal on a ZC.
22
An optimal code can be constructed recursively for even n in the following way: we
start with an optimal codebook for n = 8:
C
(5,8)∗
ZC =
(
c
(5)
1 , c
(5)
2 , c
(5)
3 , c
(5)
1 , c
(5)
2 , c
(5)
3 , c
(5)
4 , c
(5)
5
)
. (97)
Then, we recursively construct an optimal codebook for n ≥ 10, n even, by using
C
(5,n−2)∗
ZC and appending 

(
c
(5)
4 , c
(5)
5
)
if n mod 10 = 0,(
c
(5)
1 , c
(5)
2
)
if n mod 10 = 2,(
c
(5)
1 , c
(5)
3
)
if n mod 10 = 4,(
c
(5)
3 , c
(5)
4
)
if n mod 10 = 6,(
c
(5)
2 , c
(5)
5
)
if n mod 10 = 8.
(98)
For n odd we have
C
(5,9)∗
ZC =
(
c
(5)
1 , c
(5)
2 , c
(5)
3 , c
(5)
4 , c
(5)
5 , c
(5)
1 , c
(5)
2 , c
(5)
1 , c
(5)
3
)
. (99)
Then, we recursively construct an optimal codebook for n ≥ 11, n odd, by using
C
(5,n−2)∗
ZC and appending 

(
c
(5)
3 , c
(5)
4
)
if n mod 10 = 1,(
c
(5)
2 , c
(5)
5
)
if n mod 10 = 3,(
c
(5)
4 , c
(5)
5
)
if n mod 10 = 5,(
c
(5)
1 , c
(5)
2
)
if n mod 10 = 7,(
c
(5)
1 , c
(5)
3
)
if n mod 10 = 9.
(100)
Note that the recursive structure in (98) and (100) is actually identical apart from
the ordering. Also note that when increasing the blocklength by 10, we add each of
the five column vectors in (96) exactly twice.
For n < 10 the optimal code structure goes through some transient states.
6.6 Optimal Codes on BSC
Theorem 24. For a BSC and for any n ≥ 1, an optimal codebook with two code-
words M = 2 is the flip code of type t for any t ∈
{
0, 1, . . . ,
⌊
n
2
⌋}
.
Proof. This proof is basically a corollary of Proposition 16. The details are omitted.
Theorem 25. For a BSC and for any n ≥ 2, an optimal codebook with three code-
words M = 3 or four codewords M = 4 is the weak flip code of type (t∗2, t
∗
3):
C
(M,n)∗
BSC = C
(M,n)
t∗2,t
∗
3
, (101)
where we define
t∗2 ,
⌊
n− 1
3
⌋
, t∗3 ,
⌊
n+ 1
3
⌋
. (102)
Proof. See Appendix B.
24
Using the shorthand k , ⌊n3 ⌋, we can write the optimal code parameters of (102)
as
[t∗1, t
∗
2, t
∗
3] =


[k + 1, k − 1, k] if n mod 3 = 0,
[k + 1, k, k] if n mod 3 = 1,
[k + 1, k, k + 1] if n mod 3 = 2.
(106)
Using (42) we can compute the pairwise Hamming distance vector of this code for
M = 3 as follows:
d(3,n) =


(2k − 1, 2k + 1, 2k) if n mod 3 = 0,
(2k, 2k + 1, 2k) if n mod 3 = 1,
(2k + 1, 2k + 2, 2k + 1) if n mod 3 = 2,
(107)
i.e.,
d
(3,n)
min =


2k − 1 if n mod 3 = 0,
2k if n mod 3 = 1,
2k + 1 if n mod 3 = 2.
(108)
For M = 4 we get accordingly:
d(4,n) =


(2k − 1, 2k + 1, 2k, 2k, 2k + 1, 2k − 1) if n mod 3 = 0,
(2k, 2k + 1, 2k, 2k, 2k + 1, 2k) if n mod 3 = 1,
(2k + 1, 2k + 2, 2k + 1, 2k + 1, 2k + 2, 2k + 1) if n mod 3 = 2,
(109)
with the same values for the minimum Hamming distance as for the M = 3.
We will compare this optimal code with the following different weak flip code
C
(M,n)
subopt:
[t1, t2, t3] =


[k, k, k] if n mod 3 = 0,
[k + 1, k − 1, k + 1] if n mod 3 = 1,
[k + 2, k, k] if n mod 3 = 2.
(110)
This code can actually be constructed from the optimal code C
(M,n−1)∗
BSC by append-
ing a corresponding column (depending on n). In fact, by adapting the proof of
Corollary 26, we can show that this second weak flip code is strictly suboptimal.
The pairwise Hamming distance vectors of this suboptimal code is given as fol-
lows. For M = 3:
d(3,n) =


(2k, 2k, 2k) if n mod 3 = 0,
(2k, 2k + 2, 2k) if n mod 3 = 1,
(2k, 2k + 2, 2k + 2) if n mod 3 = 2,
(111)
i.e., d
(3,n)
min = 2k in all cases. For M = 4 the situation is accordingly with also
d
(4,n)
min = 2k in all cases.
Hence, we see that the minimum Hamming distance of the optimal code is 2k−
1 and therefore strictly smaller than the minimum Hamming distance 2k of the
suboptimal code. By adapting the construction of the strictly suboptimal code
C
(M,n)
subopt, a similar statement can be made for the case when n mod 3 = 1.
We have shown the following proposition.
Proposition 27. On a BSC for M = 3 or M = 4 and for all n with n mod 3 = 0
or n mod 3 = 1, the codes that maximize the minimum Hamming distance d
(n)
min can
be strictly suboptimal. This is not true in the case of n mod 3 = 2.
26
A Appendix: Derivation of Proposition 16
Assume that the optimal code for blocklength n is not a flip-flop code. Then the
code has a number m of positions where both codewords have the same symbol. The
optimal decoder will ignore these m positions completely. Hence, the performance
of this code will be identical to a flip-flip code of length n−m.
We therefore only need to show that increasing n will always allow us to find
a new flip-flop code with a better performance. In other words, Proposition 16 is
proven once we have shown that
Pe
(
C
(2,n−1)
t
)
≥ max
{
Pe
(
C
(2,n)
t
)
, Pe
(
C
(2,n)
t+1
)}
. (112)
Here we have used the following notation:
C
(2,n−1)
t =
(
x(n−1)
x¯(n−1)
)
(113)
is a length-(n− 1) flip-flop code of some type t, and
C
(2,n)
t =
(
[x(n−1) 0]
[x¯(n−1) 1]
)
, C
(2,n)
t+1 =
(
[x(n−1) 1]
[x¯(n−1) 0]
)
(114)
are the two length-n flip-flop codes that can be derived from C
(2,n−1)
t .
As shown in Proposition 18, the optimal decision rule for any flip-flop code is a
threshold rule with some threshold ℓ: the decision rule for received y only depends
on d such that
g(y) =
{
x if 0 ≤ d ≤ ℓ,
x¯ if ℓ+ 1 ≤ d ≤ n,
(115)
where we use g(·) to denote the ML decoding rule.
The threshold satisfies 0 ≤ ℓ ≤ ⌊n−12 ⌋. Note that when ℓ = ⌊
n−1
2 ⌋, the decision
rule is equivalent to a majority rule. Also note that when n is even and d = n2 ,
the decisions for x and x¯ are equally likely, i.e., without loss of generality we then
always decode to x¯.
So let the threshold for C
(2,n−1)
t be ℓ
(n−1). We will now argue that the threshold
for C
(2,n)
t and C
(2,n)
t+1 according to (114) must satisfy
ℓ(n−1) ≤ ℓ(n) ≤ ℓ(n−1) + 1. (116)
Consider the code C
(2,n)
t . Note that since t is unchanged (C
(2,n−1)
t is changed to
C
(2,n)
t ), the first codeword was appended a 0, while the second codeword was ap-
pended a 1, i.e., x(n) = [x(n−1) 0] and x¯(n) = [x¯(n−1) 1], see (114).
Now firstly assume by contradiction that ℓ(n) < ℓ(n−1) and pick a received y(n−1)
with d(n−1) = ℓ(n−1) that (for the code C
(2,n−1)
t ) is decoded to x
(n−1). The received
length-n vector y(n) = [y(n−1) 0] has d(n) = ℓ(n−1) > ℓ(n), i.e., it will be now decoded
to x¯(n). This however is a contradiction to the assumption that the ML decision for
the code C
(2,n−1)
t was x
(n−1).
Then secondly assume by contradiction that ℓ(n) > ℓ(n−1) + 1. Pick a received
y(n−1) with d(n−1) = ℓ(n−1) + 1 that (for the code C
(2,n−1)
t ) is decoded to x¯
(n−1).
The received length-n vector y(n) = [y(n−1) 1] has d(n) = ℓ(n−1) + 2 < ℓ(n) + 1, i.e.,
it will be now decoded to x(n). This, however, is a contradiction to the assumption
that the ML decision for the code C
(2,n−1)
t was x¯
(n−1).
28
for all y(n−1) with d(n−1) = ℓ(n−1) the length-n received vector [y(n−1) 1] has
d(n) = ℓ(n−1) + 1 = ℓ(n) + 1 and will be decoded to x¯(n). Hence we must have
Pn
Y |X
(
[y(n−1) 1]
∣∣x(n))
Pn
Y |X
(
[y(n−1) 1]
∣∣ x¯(n)) ≤ 1, (121)
and therefore∑
y(n−1)
1≤d(n)≤ℓ(n−1)+1
PnY |X
(
[y(n−1) 1]
∣∣ x¯(n))
=
∑
y(n−1)
d(n)=ℓ(n−1)+1
PnY |X
(
[y(n−1) 1]
∣∣ x¯(n))︸ ︷︷ ︸
≥Pn
Y |X
(
[y(n−1) 1]
∣∣x(n))
+
∑
y(n−1)
1≤d(n)≤ℓ(n−1)
PnY |X
(
[y(n−1) 1]
∣∣ x¯(n))
(122)
≥
∑
y(n−1)
d(n)=ℓ(n−1)+1
PnY |X
(
[y(n−1) 1]
∣∣x(n))+ ∑
y(n−1)
1≤d(n)≤ℓ(n−1)
PnY |X
(
[y(n−1) 1]
∣∣ x¯(n)).
(123)
Hence, we get from (120):
2P (n−1)e
(
C
(2,n−1)
t
)
≥
∑
y(n−1)
ℓ(n−1)+2≤d(n)≤n
PnY |X
(
[y(n−1) 1]
∣∣x(n))
+
∑
y(n−1)
ℓ(n−1)+1≤d(n)≤n−1
PnY |X
(
[y(n−1) 0]
∣∣x(n))
+
∑
y(n−1)
d(n)=ℓ(n−1)+1
PnY |X
(
[y(n−1) 1]
∣∣x(n))
+
∑
y(n−1)
1≤d(n)≤ℓ(n−1)
PnY |X
(
[y(n−1) 1]
∣∣ x¯(n))
+
∑
y(n−1)
0≤d(n)≤ℓ(n−1)
PnY |X
(
[y(n−1) 0]
∣∣ x¯(n)) (124)
=
∑
y(n)
ℓ(n−1)+1≤d(n)≤n
PnY |X
(
y(n)
∣∣x(n))
+
∑
y(n)
0≤d(n)≤ℓ(n−1)
PnY |X
(
y(n)
∣∣ x¯(n)) (125)
= 2P (n)e
(
C
(2,n)
t
)
. (126)
(ii) If the decision rule is changed such that ℓ(n) = ℓ(n−1) + 1, we need to take
care of the second summation in (120) that contains some terms that will
now be decoded differently. Since we have assumed that ℓ(n) = ℓ(n−1) + 1, we
know that for all y(n−1) with d(n−1) = ℓ(n−1) + 1 the length-n received vector
30
Before we start our induction proof, we make an observation about a basic prop-
erty of the weak flip code given in (102).
Claim 28. For the weak flip code of (102), the largest received Hamming distance
between any y and the nearest codeword is given by the minimum Hamming distance
of the codebook:
max
y
d
(n)
min(y) = d
(3,n)
min
(
C
(3,n)∗
BSC
)
. (131)
Proof. It is not too difficult to see that a y that achieves the maximum in (131)
should have t∗1 ones, t
∗
2 ones, and t
∗
3 zeros in the positions where the optimal codebook
consists of c1, c2, and c3, respectively. I.e.,
y(n)max , (1, 1, . . . , 1︸ ︷︷ ︸
t∗1
, 1, 1, . . . , 1︸ ︷︷ ︸
t∗2
, 0, 0, . . . , 0︸ ︷︷ ︸
t∗3
). (132)
Then,
max
y
d
(n)
min(y) = maxy
min
{
d
(n)
1 (y), d
(n)
2 (y), d
(n)
3 (y)
}
(133)
= min
{
d
(n)
1 (ymax), d
(n)
2 (ymax), d
(n)
3 (ymax)
}
(134)
= min{t∗1 + t
∗
2, t
∗
1 + t
∗
3, t
∗
2 + t
∗
3} (135)
= min
{
d
(n)∗
23 , d
(n)∗
13 , d
(n)∗
12
}
(136)
= dmin
(
C
(3,n)∗
BSC
)
. (137)
Note that for other code structures, this claim is in general not true.
Case i: Step from n − 1 = 3k − 1 to n = 3k: We start with the code C
(3,n−1)
t∗2,t
∗
3
with code parameters, pairwise Hamming distance vector, and minimum Hamming
distance as follows:
code parameters: [k, k − 1, k], (138)
pairw. distance vector: d(3,n−1) = (2k − 1, 2k, 2k − 1), (139)
min. Hamming distance: d
(3,n−1)
min = 2k − 1. (140)
The corresponding success probability formula looks as follows:
3P (n−1)c
(
C
(3,n−1)
t∗2,t
∗
3
)
=
3∑
m=1
∑
y(n−1)∈D
(n−1)
m
Pn−1
Y |X
(
y(n−1)
∣∣x(n−1)m ) (141)
= (1− ǫ)n−1
3∑
m=1
∑
y(n−1)∈D
(n−1)
m
(
ǫ
1− ǫ
)dH(x(n−1)m ,y(n−1))
(142)
= (1− ǫ)n−1

 ∑
y(n−1)∈D
(n−1)
1
(
ǫ
1− ǫ
)d(n−1)1 (y(n−1))
+
∑
y(n−1)∈D
(n−1)
2
(
ǫ
1− ǫ
)d(n−1)2 (y(n−1))
+
∑
y(n−1)∈D
(n−1)
3
(
ǫ
1− ǫ
)d(n−1)3 (y(n−1)) (143)
32
+
∑
y(n)∈[D
(n−1)
3 0]
(
ǫ
1− ǫ
)d(n−1)3 (y(n−1))
+
∑
y(n)∈[D
(n−1)
3 1]
(
ǫ
1− ǫ
)d(n−1)3 (y(n−1))+1 . (147)
We compare this with the success probability of the new code:
3P (n)c
(
C(3,n)
)
= (1− ǫ)n

 ∑
y(n)∈D
(n)
1
(
ǫ
1− ǫ
)d(n)1 (y(n))
+
∑
y(n)∈D
(n)
2
(
ǫ
1− ǫ
)d(n)2 (y(n))
+
∑
y(n)∈D
(n)
3
(
ǫ
1− ǫ
)d(n)3 (y(n)) . (148)
In order to be able to compare (147) with (148), we need to be able to compare
D
(n−1)
i with D
(n)
i and d
(n−1)
i
(
y(n−1)
)
with d
(n)
i
(
y(n)
)
. Note that every y(n) can
be uniquely written as some y(n−1) plus an appended 0 or 1.
Since we have appended c
(3)
2 = (0, 1, 0)
T to the code of length n − 1, it is
obvious that
if y(n−1) ∈ D
(n−1)
1 =⇒ [y
(n−1) 0] ∈ D
(n)
1 ; d
(n)
1
(
y(n)
)
= d
(n−1)
1
(
y(n−1)
)
;
(149)
if y(n−1) ∈ D
(n−1)
2 =⇒ [y
(n−1) 1] ∈ D
(n)
2 ; d
(n)
2
(
y(n)
)
= d
(n−1)
2
(
y(n−1)
)
;
(150)
if y(n−1) ∈ D
(n−1)
3 =⇒ [y
(n−1) 0] ∈ D
(n)
3 ; d
(n)
3
(
y(n)
)
= d
(n−1)
3
(
y(n−1)
)
.
(151)
The problems are the other three cases. For example,
if y(n−1) ∈ D
(n−1)
1 =⇒ [y
(n−1) 1] ∈ D
(n)
1 or D
(n)
2 , (152)
depending on the exact value of d(n−1)
(
y(n−1)
)
. Note that [y(n−1) 1] /∈ D
(n)
3
because we have added a 0 to the third codeword. To be able to investigate the
different possible cases depending on d(n−1)
(
y(n−1)
)
, we introduce a shorthand
d , d(n−1)min
(
y(n−1)
)
= d
(n−1)
1
(
y(n−1)
)
(153)
to denote the distance to the closest codeword (which is the first codeword in
this case) and another shorthand d+ to denote any value strictly larger than
d. The received Hamming distance vector can take on one out of four possible
situations:
d(n−1)
(
y(n−1)
)
= (d, d, d) or (d, d, d+) or (d, d+, d) or (d, d+, d+). (154)
If we prolong y(n−1) by appending a 1, then the first and the third component
will be increased by 1, while the second component remains unchanged. This
34
In the fourth case [y(n−1) 0] will remain in D
(n)
2 , in all other three cases it
will change to another decision region. However all these three cases are not
possible according to (157).
Finally,
if y(n−1) ∈ D
(n−1)
3 =⇒ [y
(n−1) 1] ∈ D
(n)
2 or D
(n)
3 , (162)
depending on the exact value of d(n−1)
(
y(n−1)
)
:
d(n−1)
(
y(n−1)
)
= (d, d, d) or (d+, d, d) or (d, d+, d) or (d+, d+, d). (163)
In the first and second case [y(n−1)1] will change to D
(n)
2 , in the other two cases
it will remain in D
(n)
3 . Again, the first and the second case are not possible
according to (157).
Hence, we have shown that
if y(n−1) ∈ D
(n−1)
1 =⇒ [y
(n−1) 1] ∈ D
(n)
1 ; d
(n)
1
(
y(n)
)
= d
(n−1)
1
(
y(n−1)
)
+ 1;
(164)
if y(n−1) ∈ D
(n−1)
2 =⇒ [y
(n−1) 0] ∈ D
(n)
2 ; d
(n)
2
(
y(n)
)
= d
(n−1)
2
(
y(n−1)
)
+ 1;
(165)
if y(n−1) ∈ D
(n−1)
3 =⇒ [y
(n−1) 1] ∈ D
(n)
3 ; d
(n)
3
(
y(n)
)
= d
(n−1)
3
(
y(n−1)
)
+ 1.
(166)
But this proves that the success probability of (148) is identical to the success
probability of (147)! So in spite of increasing the length n − 1 by 1, we have
not improved our performance.
2. Appending c
(3)
1 : Next, we investigate what happens if we append c
(3)
1 =
(0, 0, 1)T. The new code has the following parameters:
[k + 1, k − 1, k]; d(3,n) = (2k − 1, 2k + 1, 2k); d
(3,n)
min = 2k − 1. (167)
One of the three problematic cases now is
if y(n−1) ∈ D
(n−1)
1 =⇒ [y
(n−1) 1] ∈ D
(n)
1 or [y
(n−1) 1] ∈ D
(n)
3 , (168)
depending on the exact value of d(n−1)
(
y(n−1)
)
given in (154). If we prolong
y(n−1) by appending a 1, now the first and the second component will be
increased by 1, while the third component remains unchanged. This means
that in the first and third case the new vector [y(n−1) 1] will belong to D
(n)
3 ,
while in the second and fourth cases it will belong to D
(n)
1 . According to
Claim 29 both the third and and the fourth cases are possible and do happen.
In the cases where [y(n−1) 1] ∈ D
(n)
3 we then have that
d
(n)
1
(
y(n)
)
= d
(n−1)
1
(
y(n−1)
)
(169)
without the additional increase by 1. This then means that the success prob-
ability of (148) is strictly larger than the success probability of C
(3,n−1)
t∗2,t
∗
3
and
the choice of c
(3)
1 is effective.
The investigation of the other two problematic cases is similar and omitted.
36
Case iii: Step from n − 1 = 3k + 1 to n = 3k + 2: In this case, we start with
the code C
(3,n−1)
t∗2,t
∗
3
with code parameters, pairwise Hamming distance vector, and
minimum Hamming distance as follows:
code parameters: [k + 1, k, k], (179)
pairw. distance vector: d(3,n−1) = (2k, 2k + 1, 2k + 1), (180)
min. Hamming distance: d
(3,n−1)
min = 2k. (181)
If we append c
(3)
1 = (0, 0, 1)
T, we get a new code with the following parameters:
[k + 2, k, k]; d(3,n) = (2k, 2k + 2, 2k + 2); d
(3,n)
min = 2k. (182)
If we append c
(3)
2 = (0, 1, 0)
T, we get a new code with the following parameters:
[k + 1, k + 1, k]; d(3,n) = (2k + 1, 2k + 1, 2k + 2); d
(3,n)
min = 2k + 1. (183)
If we append c
(3)
3 = (0, 1, 1)
T, we get a new code with the following parameters:
[k + 1, k, k + 1]; d(3,n) = (2k + 1, 2k + 2, 2k + 1); d
(3,n)
min = 2k + 1. (184)
The corresponding investigation of possible situations now reads as follows.
Claim 31. There exists no solution of ai’s, 0 ≤ a1 ≤ k, 0 ≤ a2 ≤ k−1, 0 ≤ a3 ≤ k,
that satisfies
(k + 1)− a1 k − a2 a3(k + 1)− a1 a2 k − a3
a1 k − a2 k − a3



11
1

 =

dd
d

 or

d+d
d

 or

 dd+
d

 (185)
for k ≤ d ≤ 2k − 1 and d+ > d. But there do exist solutions that satisfy
(k + 1)− a1 k − a2 a3(k + 1)− a1 a2 k − a3
a1 k − a2 k − a3



11
1

 =

 dd
d+

 . (186)
The investigation is similar and shows that appending c
(3)
1 is strictly suboptimal,
while appending c
(3)
2 and c
(3)
3 are equivalent and optimal.
B.2 M = 4
We note that the fourth codeword for M = 4 is exactly the furthest received vector
for M = 3. We can therefore adapt the computation of the received Hamming
distance vector as follows:

d
(n)
1
d
(n)
2
d
(n)
3
d
(n)
4

 =


t1 − a1 t2 − a2 a3
t1 − a1 a2 t3 − a3
a1 t2 − a2 t3 − a3
a1 a2 a3



11
1

 . (187)
The derivation follows then exactly the same lines as in Appendix B.1. The only
main difference is that we need to investigate more different columns. Actually, we
need to investigate also some columns that have not been named in Definition 13,
like, e.g., c = (0, 0, 0, 1)T and prove that they are strictly suboptimal. The details
are omitted.
38
作者是E. Tavakoli，會去聽這個演講，主要是因為我的研究裡將來也會可能討論到optimal codes 
on Binary Erasure Channel，所以想說去聽聽看是否是相似的研究，可惜的是，他們主要在討
論的東西，還是用 LDPC codes去比較其最佳化的 rates值，與我想要做在錯誤率最低下的 optimal 
codes的structure還是有所差別，不過至少對方的idea算是很有趣的，讓我覺得要上conference 
paper 也許 idea真的要好，數學不用多複雜，簡單易懂才是重點。後來一直到下午四點，就到了
換我上台演講自己要發表的 paper 了，而且我是這個 session 中第一個發表的，所以就算我私底
下或是在學校內，練習了好幾遍了，上台時還是非常緊張，尤其是之前所練習過的東西，後來上
台講時，都會漏掉很多重要的觀念，然後會念的特別快，這是我這次深深感到無奈的缺點，有好
的東西，想要與人分享的話，就必須要好好的發表，這是我相當欠缺的東西。只是在我報告完後，
很令我失望的是，不知道是台灣人真的在國際上很讓外國人看低，還是因為基本上我的做的題目
實在太過新穎，一堆國外大學的人都不太想跟我問問題似的，不然或許就是我講的東西太過不清
楚，或是畢竟是不同領域的東西，所以可能大家也不一定懂得我詳細在作的東西是什麼？更甚者
就是不知是否可能是我英文太爛？現在台灣人在國際上本來就很不知名了…真的很令人失望呀~
不過感受到這種氣氛，我們也才會發奮圖強吧！ 
下午報告完後，因為是同一個 session，就繼續待在演講場地聽別人的演講，其中較有興趣的
就是 Finite Block-Length Achievable Rates for Queuing Timing Channels，作者們是 T. Riedl, 
T. Coleman and A. Singer. 也是一個滿有趣的也不錯的 talk，只是可惜的是，因為我本身是著
重於 Finite Block-Length 的 coding，本來以為這篇論文會有所相關，但作者依然是用 bounds
去找 achievable rates，所以最後還是用漸近的逼近去大約的述說他們所發表的結果的好壞。說
真的，這篇我也覺得是 idea 很重要，東西簡短，也許說真的，一個好的論文真的是源頭的 idea
很重要，但是要再加上好的數學，兩者缺一不可，相輔相成才是根本的解決之道，數學不用特別
難，但也許 journal 就真的不一樣了，一定要有嚴謹的數學跟完整的推論，所以有一個學長才說，
要做好研究又要做得久真的要有扎實的數學與工程基礎。 
到了第二天，早上另外去聽了一場演講：A Maximum Entropy Theorem for Complex-Valued 
Random Vectors，作者是 G. Tauboc，這一場演講主要是研究一個在 information theory 裡常
常用到的用定理，Maximum Entropy Theorem，不過是在考慮當我們用的數值是複數時的情形，
然後在這個定理的應用上，他們主要是將其應用在無線通訊 MIMO的實際應用面上，值得注意的
是，他們的通道雜訊可以不用僅僅假設在高斯雜訊上是成立的，但對於一般的雜訊也是可以加
以應用的。下午幾場去聽的演講都還算滿有趣的，第一場演講主題是 On the Capacity of 
Multiplicative Multiple Access Channels with AWGN，作者是 S.R. B Pillai，在上我的指
導教授，Stefan M. Moser 的 Advanced Information Theory 時，就有提到相關的 channel，其
要有特別介紹 Multiple Access Channels，然後也有介紹一個 example，multiplicative 
channels，這篇文章主要就是講述兩者合併起來在高斯雜訊下的 capacity值，個人覺得是與我
所學的東西相當有關的一篇文章，因此在聽其演講時，比較能夠聽得上手，也有較多的思考與
複習當初的所學，希望之後研究有時間的話，也能做一下這方面的研究。到了下午，這次會議
安排的就是沒有演講活動了，反而是參加會議的人一起去看巴西的大海，所以我就與此行認識
的中國人與香港人一起行動，這裡我覺得似乎參加會議都是這樣的，總是會與華人特別的熟悉
然後相識，反而對於歐美或是日本國家的人，不管是華人對他們或是他們對華人，似乎都不一
定會有特別的感覺去相熟且認識，也許華人真的是四海之內皆兄弟，真的較易互相吸引吧！另
外在此行船上看海的活動，我也認識一位在 Information Theory 的 Network Coding 研究中相
當知名的一位教授，Raymond Yeung教授。他可以是說是 Network Coding 的始祖，不過他本人
角度想，這也許也是件不錯的事，代表我做的東西還可以有所發展，這也是參加會議的其中一個
好處，可以明白同個領域的人，做的研究是否有重疊，這樣子便可以再適時調整自己的研究方向。
另外參加完會議後，只會想要好好回台灣做研究，希望有一天自己也可以早日在台上發表並演說
自己的研究主題，並且再投稿更好的國際型會議，比如說 IEEE International Symposium on 
Information Theory (ISIT 2012) 這個更大的 A 型國際會議，我想這對於讓台灣學生多多激勵
自己，提昇自我研究產出，而事實上，參與國際會議也有所幫助，可以激厲學生的鬥志，讓台灣
學生明白國際上有多少厲害的人。最後還是覺得英文能力真的是很重要的一件事，需要長時間的
磨練才可以與一般人對答如流，還需要懂得英文語系的用法及簡短回應方式，才可以不致於誤解
對方的意思，雖然看到很多亞洲國家，如日本、韓國等的來的人，講的英文也是口音相當地重，
但他們的基本音節尚足以讓別人聽懂他們的英文是大概在講什麼，雖然還是有些人是都聽不懂
的，但是我們應該向可以把研究主題把英文表達得清楚的不同外國人學習，以達到可以將演說表
達清楚，讓別人明白我們在做什麼的研究之目的。 
 
三、考察參觀活動(無是項活動者省略) 
無 
 
四、建議 
 
五、攜回資料名稱及內容 
會議文章收錄 USB x 1、會議內容摘要手冊 x1 (包含會議主題、時間、地點及報告者)、 
紀念側背包 x1。 
 
六、其他 
無 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：莫詩台方 計畫編號：97-2221-E-009-003-MY3 
計畫名稱：有限長度區間碼通道容量 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
Po-Ning Chen, Hsuan-Yin Lin, Stefan M. Moser, 
Ultra-Small Block-Codes for Binary Discrete Memoryless Channels, in Proceedings 
2011 IEEE Information Theory Workshop (ITW'11), Paraty, Brazil, Oct. 16–20, 2011, 
pp. 175–179. 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
In his world-famous paper of 1948, Shannon defined channel capacity as the ultimate 
rate at which information can be transmitted over a communication channel with 
an error probability that will vanish if we allow the blocklength to get infinitely 
large. While this result is of tremendous theoretical importance, the reality of 
practical systems looks quite different: no communication system will tolerate 
an infinite delay caused by an extremely large blocklength, nor can it deal with 
the computational complexity of decoding such huge codewords. On the other hand, 
it is not necessary to have an error probability that is exactly zero either, a 
small, but finite value will suffice.  
Therefore, the question arises what can be done in a practical scheme. In 
particular, what is the maximal rate at which information can be transmitted over 
a communication channel for a given fixed maximum blocklength (i.e., a fixed 
maximum delay) if we allow a certain maximal probability of error? In this project, 
we have started to study these questions. 
Block-codes with very short blocklength over the most general binary channel, the 
