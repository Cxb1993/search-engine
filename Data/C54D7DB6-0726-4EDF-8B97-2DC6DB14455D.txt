 2 
The dynamic spectral shape conveys energy modulations of the sound, both in time at any frequency 
channel, and along the spectral axis at any instant. These spectro-temporal modulations critically 
determine the intelligibility of the sound [6]. Physiological studies suggest that cells in the auditory 
cortex (AI) respond to spectral features as well as to the temporal features of the input stimulus to 
the ears [7]-[9]. Therefore, examining the coding scheme of the human auditory system on the 
spectro-temporal features of the sound shall provide some strategies to mimic the human perception. 
Thus, as pointed out, such spectro-temporal features fit right into the interest of researchers who try 
to find effective cues to enhance the performance of automatic sound processing systems in a noisy 
environment. 
The focus of this research is to investigate different noise represented along the spectro-temporal 
auditory model and derive biological-motivated noise suppression strategies. We expect the 
spectro-temporal modulations of noise and speech show different characters in this multi-resolution 
representation, and hence they can be more easily separated in this domain than in the spectrum 
domain. Although temporal modulation frequencies have been used to weight the frequency channel 
in noise suppression application (e.g. [10]), the joint spectro-temporal modulation contents of noise 
have not been studied. It has also been shown that the traditional speech enhancement algorithms 
cascaded with auditory-based feature extraction as a front-end give more robust results for ASR 
systems [11]. However, it is our beliefs that human’s speech enhancement/noise suppression ability 
which resides in the auditory cortex level is the key factor to vastly improve the performance of the 
ASR systems. In other words, we do not think the traditional speech enhancement algorithms, whose 
goals are to directly increase the SNR of the input signals, describe the ways human beings suppress 
the noise. To derive biological-motivated speech enhancement strategies is the goal of this study. 
This paper is organized as follows. In Section 2, a brief review of our neuro-physiological 
evidences based spectro-temporal cortical model is given. In addition, the reconstruction procedures 
from the cortical domain outputs back to time domain signals are briefly described. In Section 3, we 
propose an enhancement strategy after observing different spectro-temporal characteristics of the 
clean and noisy speech signals. Some preliminary results of the noise reduction are demonstrated. 
We end in Section 4 with a thoroughly test of our physiology motivated enhancement strategy 
through an automatic digit recognizer which is implemented by the HTK toolkit. Results of the 
recognizer are shown and compared with the enhancement module being turned on and off. We end 
in Section 5 with a summary and discussions of still on-going works and future’s potential 
applications. 
 
2. PHYSIOLOGY BASED AUDITORY MODEL 
2.1. Cortical Model 
 
Based on the neuro-physiological findings, we have proposed a computational spectro-temporal 
cortical model. From a functional point of view, the cortex can be modeled as a bank of 
spectro-temporal modulation filters with spatial impulse response ( )IRSh x  and temporal impulse 
response ( )IRTh t . In our model, these impulse responses are defined by sinusoidally interpolating 
symmetric seed functions ( ) ( )s th h⋅ , ⋅  and their Hilbert transform  
    ˆ( ) ( ) cos ( )sinIRS s sh x h x xhφ φ φ;Ω, = ;Ω + ;Ω        (1) 
    ˆ( ) ( ) cos ( )sinIRT t th t h t thω θ ω θ ω θ; , = ; + ;        (2) 
where Ω  and ω  are called the density (scale) and velocity (rate) of these responses; φ  and θ  
are the characteristic phase; the Hilbert transform is defined as  
    ( )1ˆ ( ) h xx dzh z xπ
∞ .
. −∞= −∫  
Therefore, the spectro-temporal multi-scale, multi-rate response of neuron c for the input 
 4 
anatomical structures, and computational implementation to generate the auditory spectrogram are 
available in [13][14]. Briefly speaking, time domain signals go through the first stage of a bank of 
128 asymmetric critical overlapping bandpass cochlear filters which are equally spaced over a 5.3 
octave frequency range (with 24 filters/octave frequency resolution). Details of the filter parameters 
and implementations can be found in [14]. The output of each filter is processed by a hair cell stage 
(the second stage) which consists of a high-pass filter, followed by nonlinear compression 
(optional), and then a low-pass filter [15]. Then the final third stage mimics the action of a lateral 
inhibitory network [16] to sharpen the filter outputs, and hence the filter bank frequency selectivity. 
It is implemented by a first-difference operation across the channel array, followed by a half-wave 
rectifier, and a short-term integration to estimate the final output [13]. Examples of the auditory 
spectrograms in pertaining to various input speech/sound signals are given in [12] to demonstrate 
the information flows through these three auditory peripheral stages. 
While the nonlinear operations in the second and third stages make it impossible to have perfect 
reconstruction from auditory spectrogram back to time domain signals, perceptually acceptable 
renditions are still feasible based on the convex projection algorithm [13]. The reconstruction 
method is summarized in the following steps: 
1. Initialize a Gaussian distributed white noise with zero mean and unit variance, i.e., 
( ) ( ) (0,1)ks t ℵ ∼ , and set the iteration counter k=1. 
2. Compute ( )1 ( , )
k
sty t x  and all the way to the auditory spectrogram ( )3 ( , )krdy t x  with respect 
to ( ) ( )ks t . 
3. Find the ratio ( ) ( , )kr t x  between the desired ( , )desiredy t x and 
( )
3 ( , )
k
rdy t x . 
4. Scale the first stage cochlear filter-bank response, i.e., ( ) ( ) ( )1 1( , ) ( , ) ( , )
k k k
st sty t x y t x r t x⇐ ⋅  . 
5. Reconstruct time waveform ( 1) ( )ks t+  by inverse filtering [17] and update counter k=k+1. 
6. Go to step 2 unless certain criteria are met (e.g., the distortion rate of ( )3 ( , )
k
rdy t x  or the 
number of iteration). 
where 0log( / )x f f=  is the frequency index on the tonotopic (logarithmic) frequency axis. More 
details of the reconstruction procedures, discussions and examples of the reconstruction are given in 
[12]. Please note, the auditory spectrogram ( , )desiredy t x  is assumed roughly representing a local 
time-frequency (TF) energy distribution, and hence the estimated ( )1 ( , )
k
sty t x  can be adjusted by the 
ratio of the target ( , )desiredy t x  divided by the computed spectrogram 
( )
3 ( , )
k
rdy t x  pertaining to 
( )
1 ( , )
k
sty t x . 
 
3. SPEECH ENHANCEMENT STRATEGY 
 
Speech is a composite signal that mainly carries information about the message to be conveyed. 
Almost all researchers/scholars look for the solutions to any speech related problems in the 
frequency domain. The most widespread feature parameters used by speech researchers are cepstral 
coefficients, which capture the smoothed spectral profiles of the speech. These short-term spectral 
profiles encode the time-varying vocal tract information. However, such approaches only take the 
characteristics of speech production into consideration, but not speech perception. In contrast to the 
traditional approaches, we would examine the speech signals from our auditory model, which is 
based on physiological evidences and therefore can be employed to account for most of the 
perceptual phenomena, such as absolute hearing threshold and spectral masking. 
 
3.1. Spectro-Temporal Contents of Speech Signals 
 
 6 
 
Fig. 2. Long-term averaged rate-scale plot of 10dB SNR noisy speech spoken by male speakers 
 
3.2. Preliminary Results of the Modulation Filtering 
 
The idea of the spectro-temporal modulation filtering results from the observations of the 
degraded speech by white noise. However, real environmental noise sources might have totally 
different modulation contents, just like different noise sources have different frequency coverage. 
Therefore, we would like to investigate the effects of the modulation filtering on real noise sources 
sampled from AURORA 2.0 database set A. We generated auditory spectrograms of sampled digit 
(one to nine, zero and oh) speech from several real noise sources (recording inside a subway, babble, 
car noise and recording in an exhibition hall) with different SNRs. Table 1 shows the average 
boosted SNRs over 100 digit strings in the auditory spectrogram domain after applying the cortical 
modulation filtering. Please note, the boosted SNRs are not calculated from the reconstructed time 
domain signals, which involves the non-linear convex projection reconstruction procedures. As 
shown in Table 1, the modulation filtering provides 4~5 dB SNR gain over all noise conditions. 
This preliminary result stands out as the proof of concept of our approach. 
 
Original time domain sequence (dB) 17.42 12.42 7.42 2.42 -2.58 -7.58
Reconstructed auditory spectrogram
after modulation filtering (dB) 
22.37 16.83 11.53 6.35 1.19 -3.76
Table 1. The SNR of the original time waveform and the reconstructed auditory spectrogram 
 
4. EXPERIMENTAL RESULTS 
4.1. Experimental Setup 
 
We evaluate the idea of spectro-temporal modulation filtering in speech enhancement through 
an automatic digit recognizer which is implemented by the HTK toolkit. All digit speech signals 
using in our experiments are extracted from AURORA 2.0 database and have been pre-filtered with 
a G.712 filter. 3000 clean digit strings from the database set A and 3000 clean time-domain 
reconstructed digit strings after modulation filtering are employed to train the HTK digit models. 
Then, a total of 100 digit strings, which are not included in the training set, are used to test the 
recognizer under various SNR conditions. Those 100 digit strings are evenly extracted from four 
noise conditions of the database. The feature used by the HTK for speech recognition is the 
Mel-Frequency Cepstral Coefficients (MFCCs). Those digit strings are processed to extract 
39-dimensinal vectors composing of 12 mel-frequency cepstral coefficients (MFCCs), 12 ∆-MFCCs, 
 8 
5. DISCUSSIONS AND FUTURE WORKS 
 
A simple neuro-physiology based spectro-temporal modulation filtering is proposed for applications 
of speech enhancement. It’s been shown that applying this modulation filtering module boost the 
SNR in the auditory spectrogram domain and in the time domain provided a more satisfying inverse 
procedures from the spectrograms back to the time signals materialized. Here, we discuss further 
some observations and future works associated with this study. 
1. The long-term speech modulation contents shown in Fig. 1 well match the human 
spectro-temporal modulation transfer function in [18], which is a psycho-acoustic 
measurement of human subjects. This observation is quite similar to the fact that the 
long-term speech frequency contents fall right into the frequency range where human are 
more sensitive, i.e., with low absolute hearing threshold. Probably, these facts come from 
the evolution of human voice communication that human put much more emphasis on the 
information bearing part of the voice – speech. 
2. As pointed out in Section 4, the insertion errors come from the ripple (de)composition 
reconstruction. In the future, we intent to develop time domain operations which mimic 
the spectro-temporal modulation filtering function of human brain. This way, there won’t 
be much reconstruction errors at the silence and those time domain operations can be 
implemented in hardware for many real-time applications, such as digital hearing aids 
improvement. 
3. We only consider a simple modulation filtering here. There are many speech enhancement 
techniques, such as Wiener filtering, applied on the time domain waveforms directly. We 
intend to adapt such techniques to the spectro-temporal representations as well and 
compare their results to those from the time domain enhancement techniques. These 
studies are still on-going in our lab now. Before applying the Wiener filter, we will try a 
generalized version of the multidimensional Principal Component Analysis (PCA) to 
reduce the dimensionality of the representations. This dimension reduction algorithm is 
based on higher–order singular–value decomposition (HOSVD) [20]. We would also like 
to try the Linear Discriminant Analysis which has been used to reduce the dimensionality 
in many fields such as face recognition [21] and speech recognition [22]. 
 
6. ACKNOWLEDGEMENT 
 
This work was supported by the National Science Council, Taiwan, under the project with contract 
NSC 95-2221-E009-232. 
 
7. REFERENCES 
 
[1] Y. Ephraim and D. Malah, “Speech enhancement using a minimum mean-square error short-time 
spectral amplitude estimator,” IEEE Trans. Acoust., Speech, Signal Processing, vol. ASSP-32, pp. 
1109–1121, Dec. 1984. 
[2] J. S. Lim and A. V. Oppenheim, “Enhancement and bandwidth compression of noisy speech,” in 
Proc. IEEE, vol. 67, 1979, pp. 1586–1604. 
[3] R. J. McAulay and M. L. Malpas, “Speech enhancement using a soft-decision noise suppression 
filte,” IEEE Trans. Acoust., Speech, Signal Processing, vol. ASSP-28, pp. 137–145, Apr. 1980. 
[4] C. Kermorvant and A. Morris, “A comparison of two strategies for asr in additive noise: Missing 
data and spectral subtraction,” in Proc. Eurospeech, vol. 6, 1999, pp. 2841–2844. 
[5] H. Wilmers and H.-W. Strube, “Noise reduction for speech signals by operations on the 
modulation frequency spectrum,” J. Acoust. Soc. Am., vol. 105, no. 2, p. 1092, 1999. 
[6] T. Chi, Y. Gao, C. G. Guyton, P. Ru, and S. Shamma, “Spectro-temporal modulation transfer 
