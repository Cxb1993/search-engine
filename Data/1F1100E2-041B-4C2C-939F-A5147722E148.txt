cooperative behavior. Our results indicate that self-
aware agents that consider public self-consciousness 
utilize rational analysis in a manner that promotes 
cooperative behavior and supports faster societal 
movement toward stability. We found that a small 
number of self-aware agents are sufficient for 
improving social benefits and resolving problems 
associated with collective irrational behaviors. 
英文關鍵詞： self-aware agents； public self-consciousness； 
iterated prisoner＇s dilemma, cellular automata； 
small-world networks 
 
learning skills, managing personal interactions, and conducting one’s affairs. Self-awareness can also support internal 
knowledge of emotions, motivations, interests, and desires so as to achieve self-identification and self-realization. In 
contrast, lack of self-awareness often leads to situations where personal behaviors are driven by strong emotions that 
are disadvantageous to one’s well-being [2]. Further, a lack of understanding of others’ emotions and ideas often leads 
to exaggerations of one’s own merits and lack of restraint in social situations. 
Since self-awareness plays an important role in human emotional development, some social psychologists are 
employing agent-based modeling and simulation techniques to explore the topic [7–9]. Descriptions of specific efforts 
can be found the physics, biology, ecology, economics, management, computer science, and artificial intelligence (AI) 
literatures [10–11]. Combined, these tools represent a novel approach to analyzing behavioral patterns at all levels of 
complexity in domains ranging from social development [12–13] to Internet commerce [14–15]. In the field of social 
psychology, which addresses complex human phenomena and societal processes, a growing number of researchers are 
using agent-based modeling and simulation tools to create multi-agent systems for exploring human behavioral patterns 
on a large scale [16–18]. 
However, to date most forms of agent learning models have been restricted to what are essentially miniatures of 
external environments [7]. The emphasis has mostly been on mechanisms that connect stimulation signals and 
behavioral reactions [19]. Depending on the form of signals to which agents are exposed (primarily from physical 
environments or other agents), they use learning methods such as artificial neural networks, genetic algorithms, and 
fuzzy rule-based systems to adjust their knowledge bases or rule sets [20–21]. In some scenarios, agents are 
programmed to learn professional skills or to search for problem-solving strategies in response to pre-assigned 
demands or tasks [22–23]. 
There are at least five advantages to using self-aware agents in artificial societies and social simulations [11, 23–
24]. First, they are compatible with established artificial intelligence learning frameworks in that they support 
extensions of those frameworks, increased agent learning performance, and faster arrival at optimal problem-solving 
strategies. Second, the introduction of a public self-consciousness concept lets agents take into account shared needs 
and feedback from other agents when building connecting mechanisms between behavioral reactions and stimuli—in 
other words, standards and values can be shared. Third, the presence of both private interests and public self-
consciousness helps agents detect discrepancies between behavioral reactions and internal or external standards. They 
can therefore search for ways to decrease these discrepancies, increase learning performance, and satisfy internal or 
external standards. Fourth, an agent’s ongoing and evolving discrepancies between the self and public self-
consciousness can support researchers’ attempts to understand external dynamics, resulting in timely revisions or 
adjustments in the agent’s self-consciousness. Finally, self-aware agents support the construction of artificial societies 
Agent architectures and knowledge (rules) are major factors affecting problem-solving speed and solution 
quality (performance). The two most commonly used intelligent agent-based models emphasize knowledge acquisition 
and adaptation [7–9, 19]. The first is rule-based, with domain experts determining knowledge and rules required for 
agent problem-solving, which are integrated into the model in question so that agents can infer appropriate behaviors 
based on information from the external environment. The second is learning-based, with agents solving problems and 
collecting information from a combination of past experiences and present needs to construct new strategies or adapt 
old ones, and to transform strategies into knowledge bases and rule sets. Although rule-based agent architectures are 
easy to implement, they require large knowledge and experience data sets for use by agents, raising issues regarding 
revising, updating, customizing, and learning flexibility. 
Current intelligent agent models combine these architecture types, resulting in adaptive models with built-in 
knowledge bases or rule sets that follow three steps: sensing, planning, and acting [7]. In Figure 1 we use the extended 
classifier system (XCS) [31] to illustrate the interaction process between sense-plan-act agents and their environments. 
Interactions between XCS (which is associated with genetic algorithms and reinforcement learning [22]) and a problem 
environment occur as follows: at discrete time t , an agent senses the current state ts  and compares it with individual 
rule or classifier conditions within an agent’s internal knowledge base [ ]P . All rule sets that fulfill state ts  are stored 
in a temporary buffer [ ]M . Methods such as cost estimation and experience accumulation are used to select the most 
suitable rules in terms of fitness values, through which actions ta  are executed. Optimum learning is achieved through 
a system of rewards, punishments, and trial-and-error. Although scholars have proposed various learning algorithms for 
use with core intelligent agent architectures (e.g., artificial neural networks, genetic algorithms, Q-learning), sense-
plan-act still serves as the foundation for most agent models [7–9]. 
---Figure 1 about here--- 
Stuart Russell [7] used a compilation of agent architectures to establish a general learning-based agent model. As 
shown in Figure 2, his model contains four components in addition to detectors and effectors that interact with the 
external environment: a learning component, a performance component that monitors the external environment and 
determines responses, an assessment component that evaluates learning performance, and a problem generator for 
determining optimal directions for additional learning. In addition to containing a detailed architecture for interactions 
between an agent’s learning mechanism and external environment, Russell’s model clearly addresses all critical 
components and connections in agent model design. We used his work as our self-aware agent model foundation. 
---Figure 2 about here--- 
their opponents’ behaviors thereafter. According to Axelrod [12], the four main characteristics of the tit-for-tat strategy 
are (a) friendliness, meaning that one does not defect before an opponent defects; (b) vengefulness, so that once an 
opponent defects, it is possible to immediately “get even”; (c) tolerance, in that one stops defecting as soon as an 
opponent stops; and (d) transparency, making it easy for opponents to predict the results of a tit-for-tat strategy. By 
analyzing the evolutionary dynamics of various strategies and their predominance at different times during simulations, 
IPD researchers have found that reciprocal cooperation is an advantageous strategy that satisfies the condition of 
evolutionary stability. 
As shown in Table 2, many IPD agents adopt a memory-1 deterministic strategy [35]. In other words, they use 
short-term memory to store the strategies and behaviors of opponents in the preceding round. There are only four 
possible combinations of behaviors for two agents: both cooperate (expressed as CC ), the first cooperates and the 
other defects ( CD ), the first defects and the other cooperates ( DC ), and both defect ( DD ). This memory-1 
deterministic strategy can be expressed as ( ,  ,  ,  )cc cd dc dds s s s . For example, if an agent’s memory of the preceding 
round is CC , then the agent will choose ccS  when responding to an opponent. Since each response is limited to 
either cooperation or defection, the memory-1 deterministic strategy has a total of 16 ( 42 ) possible moves, including 
0 1 15 15( , , , ), ( , , , ), , S ( , , , ), S ( , , , )S C C C C S C C C D D D D C D D D D= = = = . Among these, 0 ( , , , )S C C C C=  is 
known as the “yes-man” (ALL-C) strategy, 5 ( , , , )S C D C D=  the tit-for-tat strategy, 6 ( , , , )S C D D C=  the win-stay, 
lose-shift strategy, and 15 ( , , , )S D D D D=  the “scoundrel” (ALL-D) strategy. 
---Table 2 about here--- 
3. SELF-AWARE AGENT MODEL 
Sigmund Freud described human personality as consisting of three parts: id, ego, and super-ego [36–37]. The id 
contains a person’s subconscious thoughts, including primitive desires originating from intuitive impulses. The purpose 
of the id is to fulfill biological needs (including sexual desires) and to avoid pain. The super-ego (also referred to as the 
moral self or self-consciousness) is a personality monitor; it operates according to a “perfection principle” that adheres 
to behavioral guidelines. A list of social rules and regulations that rely on the super-ego would include propriety, justice, 
honesty, shame, loyalty, filial piety, benevolence, love, trust, righteousness, harmony, and peace. The ego (the 
personality executor, or “rational self”) is the part of the personality structure that comes into contact with external 
environments. When a person’s id and super-ego come into conflict, the ego uses a reality principle to make 
adjustments through which the needs of the id are satisfied, while super-ego rules and regulations are adhered to based 
,i jo  represents agent iA ’s thj  opponent ( {0,1,2, , 1}j n∈ − ). 
Step 2. At the end of generation g  we arrive at the numerical sequence 0, 1, 2, 1,( , , , , )i i i i n i gC c c c c −=  , with 
,j ic  representing the number of times that opponent ,i jo  makes a cooperative move during a PD 
interaction with agent iA  in generation g . 
Step 3. Calculate the average value ( iavg ) and standard deviation ( istd ) of the numerical sequence iC . 
Step 4. Assign the reputation value ,i jr  of agent iA ’s opponent ,i jo  as ,( )j i i ic avg std −  . 
Agents become aware of their fitness values1
g
 when reputation mechanisms are introduced; at a certain point 
agents can determine their reputations based on judgments made by other group members. As shown in Figure 4, 
fitness values and reputations can be divided into high, medium, and low categories (nine cross-categories in all). Using 
the combination of high fitness value and low reputation as an example, this agent type is likely to use an always-defect 
strategy in order to improve performance. However, an always-defect strategy does not improve the public good. The 
addition of a super-ego will likely trigger a self-adjustment, resulting in a strategy that simultaneously fulfills societal 
expectations and helps the agent achieve a higher fitness value. At the end of generation , agent iA  can follow six 
additional steps to attain both a strong relative fitness value and reputation: 
Step 1. Assume that iaf  is agent iA ’s fitness value. 
Step 2. Agent iA  asks all iO  opponents about their evaluations of her reputation ,j ir . In the numerical 
sequence 0, 1, 2, 1,( , , , , )i i i i n i gAR r r r r −=  , ,j ir  represents opponent ,i jo ’s evaluation of agent iA ’s 
reputation. This information is used to calculate the average value iar  of numerical sequence iAR ; the 
resulting iar  represents agent iA ’s average reputation in group iO . 
Step 3. Agent iA  collects reputation information for opponent iO  from all other opponents, and uses it to 
create the numerical sequence 0,1 0, 1 1,0 1,2 1, 1 1,0 1, 2( , , , , , , , , , )i n n n n n gOR r r r r r r r− − − − −=      with length 
( 1)n n× − . ,j kr  represents opponent ,i jo ’s evaluation of opponent ,i ko ’s reputation. Also as part of 
this step, the average value of numerical sequence iOR  ( _ iOR avg ) and standard deviation 
                                                 
 
1 The degree of social fitness of an agent has been widely discussed in the general multi-agent system literature (see, 
for example, [28]). In the prisoner’s dilemma, an agent’s score can be used to indicate social fitness. However, we 
believe this measure is insufficient, and that self-aware agents must respond to input from both the ego (social fitness) 
and super-ego (agent reputation). 
(contact) numbers must be identical for the two network types. Every node (strategic agent) has on average eight 
adjacent nodes with which it comes into regular contact (e.g., family members, neighbors, fellow commuters, 
classmates, etc.). In cellular automata, each cell is equal to one node and edges are distributed according to a Moore 
neighborhood pattern. WS small-world network construction is identical to that of the cellular automata for the first 
stage. Once all nodes have been established and connected to their eight adjacent nodes, edge rewiring takes place 
according to a predetermined probability ρ . 
---Figure 5 about here--- 
Our IPD simulation experiment consisted of seven steps: 
Step 1. Set environmental parameters and initial values for evolutionary computations (Table 3). 
Environmental parameters include total number of time steps for each simulation experiment, strategy 
and color mapping table, agent memory capacity, and human contact and interactive social network 
values (i.e., numbers of nodes and edges, neighbor patterns, and small-world network rewiring 
probability). Evolutionary computation parameters include total number of agents/nodes, crossover rate, 
mutation rate, and total number of generations/time steps. 
Step 2. Use experimental requirements to choose and construct the most appropriate network—cellular 
automata or two-dimensional WS small-world. 
Step 3. Set time step t  to 0. 
Step 4. Based on network connection patterns, have the nodes at the ends of links iA  and jA  execute q  
IPD rounds. Using the payoff matrix shown in Table 1, calculate scores for iA  ( ias ) and jA  ( jas ), 
both ranging from q S×  to q T×  (i.e., from 0 to 5q ). Use these scores as fitness values for agents 
iA  ( iaf ) and jA  ( jaf ), with i iaf as←  and j jaf as← . 
Step 5. Agents calculate their relative fitness ( .iA Fitness ) and reputation ( .iA Reputation ) values, which 
represent the evaluations of all iA  opponents. 
Step 6. Each agent determines whether or not to make strategy adjustments. The current strategy is considered 
inappropriate when .iA Fitness LOW=  or .iA Reputation LOW= . Agents that need to adjust their 
strategies use evolutionary computation crossover operations to combine their original strategies with 
strategies used by opponents with high fitness values. They also use evolutionary mutation operations 
cluster in a manner that surrounds and restricts agents who adopt the all-defection strategy. 
During the fourth stage (generations 21 to 40), the number of tit-for-tat strategy agents declines. Due to an 
asymmetry problem involving memories of previous encounters, tit-for-tat agents start to defect and stop trusting one 
another, resulting in less clustering over large areas. However, as shown in Figure 7c, a certain number of tit-for-tat 
agents continue to surround all-defection agents to ensure that the latter group does not expand to the point of 
overwhelming tit-for-tat agents. Note also that as clusters of tit-for-tat agents start to break up and decrease in size, the 
number of agents adopting the win-stay, lose-shift strategy increases. Since win-stay, lose-shift agents do not have 
asymmetric memory problems regarding previous encounters (which increases the potential for promise-breaking), and 
since win-stay, lose-shift agents generally move toward mutual cooperation, their numbers and tendency to cooperate 
gradually increase. 
During the fifth stage (generations 41 to 100), strategy evolution enters a state of “dynamic stability.” Within 
clusters of win-stay, lose-shift agents, the number of agents who adopt an all-cooperation strategy gradually increases 
(Fig. 7d). However, in reaction to this increase, some agents take advantage of the situation by reverting to the all-
defection strategy, which reduces (or in some cases, eliminates) clusters of all-cooperation agents. The term “dynamic 
stability” refers to the idea that this process enters a long period of repetition. 
4.2. Results 
Graphical representations of our simulations are shown in Figure 8. The Figure 8a social network consists of 
cellular automata; in Figure 8b we present results for a two-dimensional WS small-world network. All parameters are 
identical. As indicated by the pink average payoff curves in Figures 8a and 8b, the full community or society clearly 
benefits when all agents possess the capacity for self-awareness, and a state of dynamic stability is achieved within a 
small number of generations. However, there can never be a situation in which all agents possess that capacity, 
therefore our focus was on determining the effects of adding a small number of self-aware agents to an otherwise 
unaltered environment. According to the green (10%) and yellow (30%) average payoff curves in the two figures, 
adding a small number of self-aware agents exerted a significant influence regardless of social network type. 
Specifically, they suppressed growth in the number of all-defection agents, prevented the initiation of a cycle in which 
all agents express betrayal and retaliation, and helped resolve conflicts between societal benefits and individual private 
interests so that cooperation could be accepted as mainstream behavior. 
---Figure 8 about here--- 
The average payoff curves in Figures 8a and 8b are similar because two-dimensional WS small-world networks 
---Figure 9 about here--- 
As shown in Figure 9d, early evolutionary growth/decline rates for the four strategies in a two-dimensional WS 
small-world network with 0% self-aware agents were similar to those shown in Figure 9a. After thirty generations, the 
number of all-defection agents reached a saturation point and remained at a fixed number that was clearly higher than 
their cellular automata counterparts. Due to the small-world network’s characteristic of low degree of separation, the 
numbers of agents adopting each of the four strategies reached a state of dynamic stability between the fiftieth and 
sixtieth generations. 
Figure 9c presents data on simulations involving cellular automata and a 10% addition of self-aware agents. 
Compared to Figure 9a (0% self-aware agents), the initial number of all-defection agents was not as great—a 150-agent 
difference. Figures 9d and 9f illustrate data for 0% and 10% additions of self-aware agents, respectively; here the 
difference in all-defection agent number was 60. As these figures show, there was a more significant delay in the 
emergence of betrayal/retaliation cycles when the percentage of self-aware agents was 10% rather than 100%. Note 
also that following the 10% addition of self-aware agents, the number of agents adopting a win-stay, lose-shift strategy 
surpassed the number of agents adopting the all-defection or tit-for-tat strategies, but after 20 generations the win-stay, 
lose-shift agents could not successfully resist the all-defection agents, even though their numbers increased. As a result, 
the number of win-stay, lose-shift agents started to decline to a stable level. 
Figure 9b shows a cellular automata consisting of 100% self-aware agents. Since all-defection agents quickly 
discovered that their strategy was inappropriate for fulfilling social expectations, during early evolutionary stages all of 
those agents used their self-adjustment mechanisms to adopt other strategies to fulfill the expectations of adjacent 
agents. Starting at the third or fourth generation, the number of all-defection agents dropped to zero, and no new all-
defection agents emerged for the rest of the simulation. The numbers of agents adopting the other three strategies also 
quickly stabilized without additional changes. Again, all parameters in Figures 9b and 9e were identical; the 
evolutionary dynamics of the four strategies in the two types of social networks were also virtually identical. The only 
significant difference was the presence of random long-distance shortcuts in the two-dimensional WS small-world 
network. Due to increased sensitivity, even small changes in a single agent’s strategy were capable of influencing the 
entire society. However, due to the low degree of separation characteristic of WS small-world networks, a new state of 
dynamic stability was quickly reestablished. 
5. CONCLUSION 
Our proposed self-aware agent model features super-ego and ego personality traits, and includes an external 
[18]  Huang CY, Sun CT, and Lin HC (2005) Influence of Local Information on Social Simulations in Small-World 
Network Models. Journal of Artificial Societies and Social Simulation, 
8(4), http://jasss.soc.surrey.ac.uk/8/4/8.html. 
[19]  Sutton RS and Barto AG (2002) Reinforcement Learning: An Introduction, Cambridge, MA: MIT Press. 
[20]  Mitchell T (1997) Machine Learning, Boston, MA: McGraw-Hill. 
[21]  Mitchell M (1998) An Introduction to Genetic Algorithms, Cambridge, MA: MIT Press. 
[22]  Huang CY and Sun CT (2004) Parameter Adaptation within Co-Adaptive learning Classifier Systems. Lecture 
Notes in Computer Sciences, 3103, 774-784. 
[23]  Dobbyn C and Stuart S (2004) The Self as an Embedded Agent. Minds and Machines, 13(2), 187-201. 
[24]  Markus H (1977) Self-Schemata and processing Information About the Self. Journal of Personality and Social 
Psychology, 35, 63-78. 
[25]  Morris D (1999) The Naked Ape: A Zoologist’s Study of the Human Animal, New York: Delta Press. 
[26]  Bem DJ (1972) Self-Perception Theory. In Berkowitz L (Ed.), Advances in Experimental Social Psychology (Vol. 
6, pp. 1-62), New York: Academic Press. 
[27]  Smith ER and Mackie DM (2000) Social Psychology, New York: Worth Publishers. 
[28]  Mead GH (1934) Mind, Self, and Society, Chicago: University of Chicago Press. 
[29]  Andersen SM and Chen S (2002) The Relational Self: An Interpersonal Social-Cognitive Theory. Psychology 
Review, 109, 619-645. 
[30]  Tice DM and Wallace HM (2003) The Reflected Self: Creating Yourself as (You Think) Others See You. In Leary 
MR and Tangney JP (Eds.), Handbook of Self and Identity (pp. 71-88), New York: Guilford Press. 
[31]  Wilson SW (2000) State of XCS Classifier System Research. In Lanzi PL, Stolzmann W, and Wilson SW (Eds.), 
Proceedings of Learning Classifier Systems, From Foundations to Applications, Berlin, Germany, 63-82. 
[32]  Rasmusen E (2006) Games and Information: An Introduction to Game Theory (4th Ed.) New York: Wiley-
Blackwell. 
[33]  Aumann RJ (2008) Game Theory. In Durlauf SN and Blume LE (Eds.), The New Palgrave Dictionary of 
Economics (2nd Ed.), London: Palgrave Macmillan. 
[34]  Nowak M and Sigmund K (1992) A Strategy of Win-Stay, Lose-Shift That Outperforms Tit-for-Tat in the 
Prisoner’s Dilemma Game. Nature, 364(6432), 56-58. 
[35]  Wu HC and Sun CT (2005) What Should We Do Before Running a Social Simulation? Social Science Computer 
Review, 23(2), 221-234. 
[36]  Strachey J (2001) The Standard Edition of the Complete Psychological Works of Sigmund Freud, Volume XIV 
(1914-1916): On the History of the Psycho-Analytic Movement, Papers on Metapsychology and Other Works, ii-
viii, London: The Hogarth Press and the Institute of Psycho-analysis. 
[37]  Myers DG and Ludwig T (2007) Psychology in Modules (Spiralbound), Visual Concept Reviews, Psych Inquiry 
and Scientific American Reader to Accompany Myers, New York: Worth Publishers. 
[38]  Bayne T and Pacherie E (2007) Narrators and Comparators: The Architecture of Agentive Self-Awareness. 
Synthese, 159(3), 475-491. 
[39]  Wheeler S, Petty R, and Bizer G (2005) Self-Schema Matching and Attitude Change: Situational and 
Dispositional Determinants of Message Elaboration. Journal of Consumer Research, 31(4), 787-797. 
[40]  Mui L, Mohtashemi M, and Halberstadt A (2002) Notions of Reputation in Multi-Agent Systems: A Review. In 
Proceedings of First International Joint Conference on Autonomous Agents and Multi-Agent Systems, Bologna, 
Italy, 280-287. 
[41]  Sabater J and Sierra C (2002) Reputation and Social Network Analysis in Multi-Agent Systems, In Proceedings 
of First International Joint Conference on Autonomous Agents and Multi-Agent Systems, Bologna, Italy, 475-482. 
[42]  Huynh T, Jennings N, and Shadbolt N (2006) An Integrated Trust and Reputation Model for Open Multi-Agent 
Systems. Autonomous Agents and Multi-Agent Systems, 13(2), 119-154. 
[43]  Watts DJ and Strogatz SH (1998) Collective Dynamics of ‘Small-World’ Networks. Nature, 393(6684), 440-442. 
Agent
En
vi
ro
nm
en
t
Critic
Learning
Algorithm
Problem Generator
Performance 
Component
Detectors
Effectors
Performance Standard
feedback
Learning goals
change
knowledge
 
Figure 2: Russell’s general learning-based agent model, adapted from [7]. 
Agent
En
vi
ro
nm
Learning Component
Cognition Component
Performance 
Component
Detectors
Effectors
messages
actions
Coordinate
changes
knowledge
knowledge
 
Figure 3: Self-aware agent model. 
 Figure 5: Our proposed simulation model. 
 
Figure 6: User interface for our iterated prisoner’s dilemma simulation system. 
  
 
(a) (b) 
Figure 8: Comparison of results triggered by adding self-aware agents to (a) cellular automata and (b) a two-dimensional WS small-world network. 
TABLES WITH CAPTIONS 
Table 1: Payoff matrix for the prisoner’s dilemma, adapted from [13]. 
Individual A’s payoffs are in blue, B’s in red. 
 
B 
Cooperation (C) Defection (D) 
A 
Cooperation (C) R = 3, R = 3 T = 5, S = 0 
Defection (D) S = 0, T = 5 P = 1, P = 1 
Table 2: Memory-1 deterministic strategies 
No. 
Strategy 
( ,  ,  ,  )cc cd dc dds s s s  
Note 
0S  ( , , , )C C C C  all-cooperation 
1S  ( , , , )C C C D   
2S  ( , , , )C C D C   
3S  ( , , , )C C D D   
4S  ( , , , )C D C C   
5S  ( , , , )C D C D  tit-for-tat 
6S  ( , , , )C D D C  win-stay, lose-shift 
7S  ( , , , )C D D D   
8S  ( , , , )D C C C   
9S  ( , , , )D C C D   
10S  ( , , , )D C D C   
11S  ( , , , )D C D D   
12S  ( , , , )D D C C   
13S  ( , , , )D D C D   
14S  ( , , , )D D D C   
15S  ( , , , )D D D D  all-defection 
Table 3: Experimental parameters. 
Parameter Default Value Description 
_ _TIME STEP LIMIT  100 Total number of generations for each simulation experiment. 
q  100 Total number of interactions between an agent and its opponent. 
Network Type  × 
If NetworkType = CA, a 2-d cellular automata is built; if NetworkType = WS-SWN, a 2-d Watts and Strogatz 
small-world network is built. 
W  50 Width of 2-d cellular automaton. 
H  50 Height of 2-d cellular automaton. 
N  2,500 Total number of nodes (agents). Default value = W × H. 
E  10,000 Total number of edges. Default value = (50 × 50 × 8) / 2 
Neighborhood  Moore Von Neumann/Moore neighborhood pattern, with periodic boundary conditions. 
ρ  0.01 
Specific parameter for a 2-d WS-SWN. Generating such a network begins with 2-d cellular automata with 
periodic boundary conditions. Each link is randomly rewired to a new node according to a rewiring 
probability. 
c  1 Agent memory capacity. 
cP  0.7 Crossover rate of the genetic algorithm used in this work. 
mP  0.01 Mutation rate of the genetic algorithm used in this work. 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/01/27
國科會補助計畫
計畫名稱: 公我意識之自我覺察代理人對於反覆囚犯困局之合作現象的影響與提升
計畫主持人: 黃崇源
計畫編號: 100-2221-E-182-058- 學門領域: 人工智慧與仿生計算
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
自我覺察是一種將注意力指向自我的心理歷程。當人們處於自我覺察狀態時，
將根據公我意識，反省自己的行為，然後執行具體的改善方案來降低自我差距，
符合重要他人設定的標準，或是社會認可的價值觀。明顯地，將注意力放在自
己身上的學習概念雖然重要卻始終被人工智慧領域的研究人員所忽視。無論如
何，我們將自我覺察機制整合至代理人的學習架構中，不僅令代理人的思考及
行為更接近人類的運作方式，也讓代理人基社會模擬更貼近真實世界。本研究
計畫已經達成下列目標：一、提出以超我及自我兩種人格特質為主體且包含外
在學習與內在認知雙重模式的自我覺察代理人模型；二、使用反覆囚犯困局來
表示代理人社會的公眾利益與個體私利兩者所造成的衝突，並分析自我覺察能
力對於代理人的個體績效與社會合作行為的影響。透過模擬實驗證實自我覺察
代理人具有三項特性：一、促進合作行為提早出現；二、促進代理人社會快速
趨向穩定狀態；三、提升代理人社會的群體利益，只要少數代理人具有自我覺
察能力就可化解社會公益與個體私利的衝突，解決個體的理性行為所導致的集
體不理性行為現象。我們期望自我覺察代理人模型與模擬結論能讓研究者重視
自我覺察能力對於代理人的學習績效與品質的重要性。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
