中文摘要 
無線感測網路環境下，如何延長感測節點長時間運作為當前感測網路研究領域之重
點。為有效借助再生能源作為輔助供電以維持感測節點運作，並且滿足感測應用之服務品
質需求，設計一具有服務品質要求之電源管理架構及方法為值得探討之問題。因傳統之動
態電源管理方法已不再適用於感測網路之環境，本研究提出了一個基於強化學習的電源管
理架構，控制感測應用之頻率維持節點之能源中立運作並於供電充足時滿足服務品質需
求，並經由實驗模擬證實，相較於不具自主與適應性的電源管理機制之下，此一具強化學
習電源管理架構可使感測網路維持永續運作，並使感測網路維持服務品質。 
關鍵詞：無線感測網路、動態電源管理、強化學習、能源收集 
 
Abstract 
In this study, a dynamic power management method based on reinforcement learning is 
proposed to improve the energy utilization for energy harvesting wireless sensor networks. 
Simulations of the proposed method on wireless sensor nodes powered by solar power are 
performed. Experimental results demonstrate that the proposed method outperforms the other 
power management method in achieving longer sustainable operations for energy harvesting 
wireless sensor network, and in maintaining better QoS at the same time. 
Keywords: Wireless Sensor Network, Dynamic Power Management, Reinforcement 
Learning, Energy Harvesting, Energy Neutrality. 
1   Introduction 
Dynamic power management (DPM) is a system-level energy utilization technique. In 
general cases, turning on the energy consuming device when there is no service request for the 
device from the system or application would induce extra energy consumption. By turning to 
energy saving or shutting down the idle devices before the service request comes and turning up 
the devices could satisfy the service demand and save energy effectively [2]. Hence, the problem 
of DPM could be considered as switching the device’s operation status according to appropriate 
occasions [2]. A power management system generally has an autonomous power manager (PMR) 
with policy of power management as the interface or middleware between operating system (OS) 
and power manageable devices (PMD). The primary task of PMR is to monitor and control the 
operation status of PMD, such that both the criteria of energy saving and service demand could 
be meet. Switching among different power consumption states would induce extra energy 
consumption for status transition delay. Energy saving by switching among different power 
consumption states is effective only when the idle time is larger than status transition delay and 
the idle power is lower than status transition power.  
Therefore, how to predict the length of idle time is a major research area in dynamic power 
management, which could be categorized into greedy [2], time-out [2], predictive [5], and 
stochastic [3, 4, 12] methods. However, recent researches on idle time prediction for DPM are 
energy harvesting sensor node, powered by battery and renewable energy source, can be 
formulated as the following. 
 
 
Fig. 1. System architecture for the sensor node 
The energy consumption of the sensor node at the ith sensing period, enode(i), can be generally 
decided by the duty cycle of the ith sensing period, d(i), and the energy consumption of each 
sensor node with full duty cycle in any sensing period, es, as the following equation,  
 
  enode(i) = d(i)es       (1) 
 
In the case of energy harvesting wireless sensor network, the energy of the sensor node is 
supplied with renewable power source. The supplied harvesting energy in the ith sensing period is 
denoted as eharvest(i), and the total supply energy of renewable power source within T sensing 
period, denoted as etotal harvest, is  
 
  etotal harvest = Σ∀i∈T eharvest(i)    (2) 
 
In [7], Kansal defines condition of energy neutrality (EN) as maintaining the energy equilibrium 
between energy consuming and energy harvesting. By maintaining the energy neutrality of a 
sensor node, sustainable operation of sensor node could be obtained such that the operation life of 
sensor is extended. The distance of energy neutrality of the sensor node,Δeneutral(i), is defined as 
the difference between eharvest(i) and enode(i) at the end of the ith sensing period as following 
 
  Δeneutral(i) = eharvest(i) － enode(i)   (3) 
 
While, the optimal energy neutrality, e*neutral, is obtained by minimizing Δeneutral(i), and is 
represented as, 
 
  e*neutral (i) = min{Δeneutral(i), ∀i∈T}  (4) 
 
Since the harvesting energy, eharvest, is practically varied time-by-time, the energy neutrality value, 
hence, would be bounded by the maximum and minimum energy neutrality values, represented as 
σ1 and σ2, respectively, as the following  
 
  σ1 = max{Δeneutral(i) － ρ, ∀i∈T}  (5) 
  σ2 = min{Δeneutral(i) － ρ, ∀i∈T}   (6) 
 
where ρ is the average harvesting energy of etotal harvest. 
Renewable 
Energy Source 
Hardware layer 
System layer 
Energy 
Consuming HW 
Main Management Unit with 
sensors and communication 
Energy Storage 
Duty-cycling 
eharvest eb enode 
eharvest enode 
3.2 Dynamic power management for energy harvesting WSN using reinforcement learning 
In this paper, a reinforcement learning based dynamic power management for energy 
harvesting wireless sensor network is proposed. The agent (learner) of the reinforcement learning 
is employed in the main management unit of Fig. 1 to learn from the environment information of 
enode, eharvest, and eb, and to adaptively decide and execute the action characterized by the desired 
operational duty-cycle. After executing the selected action, the agent will evaluate the 
performance and a reward will be calculated and granted to the agent. During the learning, the 
agent is encouraged to select the action with positive reward and a series of beneficial actions will 
be iteratively generated such that a better power management performance is gradually achieved. 
In the proposed method, state space of the environment, denoted as S, is defined as a set which 
consisted of the state of energy neutrality, SD, state of harvested energy, SH, and state of the 
current energy storage level, SB. The state of energy neutrality (SD) is defined by the distance 
function as the following equation, 
 
SD(i) = Δeneutral(i)    (8) 
 
where i is the ith sensing period. The state of harvested energy, denoted as SH could is defined as 
the eharvest(i), harvesting energy of the ith sensing period, normalized to the total harvested energy 
in percentage. The state of energy storage (SB) is defined as the percentage of the energy 
remained in the energy storage. Thus, the environment is constituted by different combination of 
states given the state space (SD, SH, SB) ∈ S. The action space, A, is defined on the controllable 
variable of duty-cycle, a(i), that is, A = a(i) ∈[Dmin, Dmax], where Dmin and Dmax, respectively, are 
the minimum and the maximum of operational duty cycle of sensor node. Hence, an action with 
higher value means a higher operational duty cycle is assigned to the sensor node at ith sensing 
period; where more energy would be consumed consequently. In reinforcement learning, the 
reward value is utilized to evaluate the performance of decided action or policy.  The distance of 
energy neutrality is a good candidate of reward by measuring the degree it close to zero. Thus, the 
immediate reward of the learning can be defined by distance of energy neutrality (rD) as  
 
rD = －|SD(i)| / (emaxharvest－ eminharvest)  (9) 
 
if the distance is a large value, the immediate reward would be small, whereas the distance is 
close to zero, a large immediate reward should be given. In the proposed system architecture, the 
harvesting energy is stored in the energy storage such that the remaining energy storage must be 
taking into consider in defining the immediate reward.  By considering the effect of the 
remained energy in the storage, the equation of immediate reward can be modified as  
 
  r'D = －rD (1－2 eb(i) / EB)   (10) 
 
equation 10 indicates that if the storage remained is lower, a large positive reward would be given 
according to the relative positive distance of energy neutrality, and vice versa. In that criteria the 
positive distance of energy neutrality would induce the positive reward, while if the storage 
remained in higher level, the positive distance would induce the negative reward. The algorithm 
for the proposed reinforcement learning based dynamic power management method for energy 
diminished according to atmosphere circumstances by the normal distribution with mean 0.5 and 
the variance 0.1. Thus, the annual sunshine profile is obtained under the given parameters before 
the simulation is conducted. According to Kansal’s energy harvesting model for deriving the 
certain sunshiny profile’s feature [8], the average harvesting power ρ1 is equal to 315.59mW, the 
deferential energies between the maxima/minima harvesting energy and the average harvesting 
power are calculated as 2702.58J (σ1) and 1136.13J (σ2), respectively. The average operational 
power ρ2 is constrained by the average harvesting power, that is, ρ2 must be less than ρ1 at the 
given voltage supply of 3.6V. By the power constrain, the average ampere at the certain 
operational voltage is 87.66mA. According to the condition of energy neutrality [8], the capacity 
of the energy storage can be estimated, which is 5944mAH, from the configuration previous 
defined. Thus, a commercial battery with capacity of 7000mAH at 3.6V is chosen for simulation. 
In the follow experiments, four levels of duty-cycle rate are used for any sensing period which 
are 100%, 65%, 30% and 0% (as the sleep mode), and the corresponding ampere consumed are 
160mA, 104mA, 48mA and 0mA respectively since the bigger duty cycle, the larger the energy is 
consumed. 
4.2 Result and Discussion 
Before the simulation is conducted, the environment states, the actions of duty-cycle and the 
degree of energy neutrality for reward should be properly specified, such that the dimensionality 
of Q table in Q-learning can be defined. The state configuration for distance of energy neutrality 
is consisted of 3 states which are negative distance state, zero distance state and positive distance 
state. States of energy harvesting is consisted of 4 states, which are the ranges of 0~900J, 
901~1800J, 1801~2700J and 2701~3600J. And the states of remained energy of battery is 
consisted of 5 states, which are in the range of 0~10%, 10~30%, 30~60%, 60~90% and 90~100%.  
The state configuration of actions of duty-cycle would have 4 actions which are 0%, 30%, 65% 
and 100%.  Table 1 shows the table for reward giving, where different reward giving strategy 
would be used at different remained storage state. For example, if the battery remained energy is 
high, higher positive distance of energy neutrality will do no good for the energy storage system; 
hence the distance going farer form energy neutrality point positively will obtain negative reward. 
However, at higher storage state, the negative distance of energy neutrality should be encouraged 
with positive reward by increasing the duty-cycle rate to consume more energy storage.  
Table 1. The rule of reward given 
Storage State 
distance of energy neutrality 
<-0.5mAH -0.5~0.5mAH >0.5mAH 
High 2 -1 -2 
Middle -2 2 -2 
Low -2 -1 2 
 
In the simulation, 50% of remained energy in battery is used as the initial condition.  The 
results of earlier and later simulation day are shown in Fig. 5 (a-c), and Fig. 5 (d-f), respectively.  
Table 2. Experiment comparison table 
Comparing items 
Method 
Percentage 
increase 
Adaptive 
Duty-Cycle 
Proposed 
Method 
Ave. energy remained 54.68% 56.88% +2.20% 
Ave. energy remained (summer) 58.78% 61.12% +2.34% 
Ave. duty-cycle 54.78% 55.16% +0.38% 
Ave. duty-cycle (summer) 63.60% 63.52% -0.08% 
5   Conclusion and Future Work 
In this study, a dynamic power management method based on reinforcement learning is proposed 
to improve the energy utilization for energy harvesting wireless sensor networks. According to 
the experiment result, the proposed method increases the energy remained in the storage, while 
maintains duty-cycle rate above 50%. By the reinforcement learning and rewarding procedure, 
the condition of energy neutrality is held. The findings of our study are three folds:  
a. at low energy situation the agent will decrease the duty-cycle rate autonomously to reduce 
the energy consumption, and at the same time will store the harvesting energy as much as 
possible to increase the remained energy;  
b. at high energy situation the agent will increase the duty-cycle rate to consume more 
energy so that the energy neutrality can be maintained;  
c. our method maintains higher remaining energy than Kansal’s adaptive duty-cycle rate 
method when the WSN is operating at the same average duty-cycle rate,   
The proposed dynamic power management can possibly be extended to embedded system 
powered by other kind of renewable power source, such as wind, tide, vibration, etc. 
References 
1.  Alpaydin, E.: Introduction to Machine Learning. MIT Press (2004) 
2.  Benini, L., Bogliolo, A., Micheli, G. D.: A Survey of Design Techniques for System-level Dynamic Power Management. 
IEEE Transactions on VLSI Systems 8(3), 299–316 (2000) 
3.  Benini, L., Bogliolo, A., Paleologo, G. A., Micheli, G. D.: Policy Optimization for Dynamic Power Management. IEEE 
Transactions on Computer-Aided Design of Integrated Circuits and Systems 18(6), 813–833 (1999) 
4.  Chung, E. Y., Benini, L., Bogliolo, A., Lu, Y. H., Micheli, G. D.: Dynamic Power Management for Non-stationary Service 
Requests. IEEE Transactions on Computers 51(11), 1345–1361 (2002) 
5.  Hwang, C. H., Wu, C. H.: A Predictive System Shutdown Method for Energy Saving of Event-driven Computation. In: Proc. 
of IEEE/ACM International Conference on Computer-Aided Design, pp. 28–32 (1997) 
6.  Jeong, K. S., Lee, W. Y., Kim, C. S.: Energy Management Strategies of a Fuel Cell/Battery Hybrid System Using Fuzzy 
Logics. Journal of Power Sources 145, 319–326 (2005) 
7.  Kansal, A., Hsu, J., Srivastava, M., Raghunathan, V.: Harvesting Aware Power Management for Sensor Networks. In: Proc. of 
ACM/IEEE Design Automation Conference, pp. 651–656 (2006) 
8.  Kansal, A., Hsu, J., Zahedi, S., Srivastava, M. B.: Power Management in Energy Harvesting Sensor Networks. ACM 
Transactions on Embedded Computing Systems 6(4),  Article 32 (2007) 
9.  Li, D., Chou, P. H.: Maximizing Efficiency of Solar-powered Systems by Load Matching. In: Proc. of ISPLED, pp. 162–167 
(2004) 
10.  Moser, C., Thiele, L., Brunelli, D., Benini, L.: Adaptive Power Management in Energy Harvesting Systems. In: Proc. of 
Design, Automation & Test in Europe Conference & Exhibition. pp. 1–6 (2007) 
11.  Pao, J. W.: The Evaluation of Operation Performance of a Photovoltaic System. Master thesis, Department of Electrical 
Engineering, National Chung Yuan University, Taiwan  (2002) 
12.  Qui, Q., Pedram, M.: Dynamic Power Management Based on Continuous-time Markov Decision Process. In: Proc. of Design 
Automation Conference, pp. 555–561 (1999) 
國科會出席國際會議發表論文報告書 
                                      日期： 2010 年 11 月 20 日 
報告人姓名 劉政廷 服務學校/系(所) 嘉義大學/資工系 
會議時間  2009/8/29-31 地    點 加拿大,溫哥華 
會議名稱 
中文： 
英文：The 7th IEEE/IFIP International Conferences on 
Embedded and Ubiquitous Computing (EUC-09) 
發表論文題目 
中文： 
英文：QoS-Aware Power Management for Energy Harvesting 
Wireless Sensor Network Utilizing Reinforcement Learning 
一、參加會議經過 
於 98 年初即開始著手本次會議論文之研究與撰述，論文初稿完
成後即於 4月份進行稿件的送交。6月中才收到論文接受通知，隨後
根據審稿委員之意見進行論文之修改定稿與論文註冊與費用之繳
交。本會議屬於由較大型的 IEEE 聯合會議，相關的議程到 7月底才
排定，同時間也預備相關的旅行文件。 
 
1、目的： 
前往於溫哥華舉行的 EUC2009 與 CSE2009 等數個 IEEE 合辦的國
際研會，公開發表計畫主持人徐超明與所指導的博碩士生合著之學術
論文作品，並且參與相關熱門主題的議程以吸收新知。所發表之作品
請參閱附件。 
2、行程： 
因加拿大距台灣甚遠約有跨換日線 8小時的時差、11 小時的飛
程，且物價水平高預算無法支應全程會議期間的生活開銷，因此無法
 
帶來諸多益處，像是良好的適應性與有效的加速計算密集的工作。具
有動態可重組能力的裝置引領出一項新的領域。一般在設計規畫階
段，細部的分析軟硬體之間的介面會產生大量的額外工作量。因此，
為了克服這個問題，演講者提出了採用自我適應及自動計算的動態可
重組系統晶片，這樣能夠減少人力在設計階段所需介入的工作量，因
為它可以有自我設定、修復錯誤區段、最佳化與自我保護的能力。這
是一個相當先進的概念。 
 
三、考察參觀活動 
受限於經費與時間，本次出國以參與研討會為主，無特定考察或
參觀活動。途經之處以攝影留念，參閱附件。 
 
四、建議 
無特別建議。 
 
五、攜回資料名稱及內容 
1、數位論文集光碟，含所有被接受的作品電子檔。 
2、議程手冊，含所有議程表及組織。 
3、註冊繳費証明， 
4、與會名牌。 
5、會議附件一組，含餐卷、手提袋等。 
Recently, more and more research works have discussed the energy harvesting 
technique for embedded system, such as wireless sensor network, light electric 
vehicle, satellite, and unmanned aerial vehicles, etc…, where energy collectors, such 
as solar panel, wind-propeller generator or water-turbine and so on are employed to 
provide the energy for embedded system [4][5].  The common drawback of these 
renewable power sources is that the supplied power are not dependable and unstable, 
thus energy harvesting techniques are only be considered as secondary power supply 
to embedded system other than battery.  Because of the unstable and uncontrollable 
environment, the survival criteria of embedded system under such environment 
should be taken into account in designing the power management mechanism for 
energy harvesting embedded system.  The ultimate energy efficient solution for 
QoS-aware wireless sensor network powered by harvesting energy should consider 
both QoS achievability and energy neutrality meanwhile.  However, these two 
factors are with highly variance and usually are hardly predictable before runtime; 
hence the online adaptive power management method is necessary and more suitable 
to overcome the unpredictable problem. 
In general, the power management system has an autonomous power manager with 
policy of power management as the interface or middleware between operating 
system and power manageable devices.  The major task of power manager is to 
control and monitor the operation status of power manageable devices, which is duty 
cycle controlling mentioned in [6] and [7], such that both the energy saving and QoS 
demand criteria could be meet.  Kansal et. al. proposed the idea of energy neutrality 
for embedded system, such as wireless sensor network.  Kansal states that the 
condition of energy neutrality is held when the energy consumption is less than or 
equal to the harvesting of energy [6], and that the sensor node can be survived and 
operated by maintaining the energy neutrality condition.  Kansal further addresses 
the adaptive duty-cycling method to maintain the energy neutrality condition for 
sensor node in wireless sensor network. 
In this study, a QoS-aware power management method based on reinforcement 
learning [8] is proposed to improve the energy utilization and QoS achievability for 
energy harvesting wireless sensor networks.   Simulation results comparing to 
Kansal’s adaptive duty-cycling method [3] are demonstrated as well.  This paper is 
organized into six sections.  Section II gives the system architecture.  In Section III, 
Reinforcement Learning Algorithm is implemented to QoS-aware power management 
for energy harvesting wireless sensor network.  In Section IV, the configurations for 
simulation are modeled.  Section V shows the result of simulation for the proposed 
method and gives the comparison with Kansal’s adaptive duty cycling method.  
Finally, in Section VI, general conclusions are drawn. 
 
II. SYSTEM ARCHITECTURE 
Study of the QoS-aware power management for energy harvesting sensor node, 
The algorithm for the proposed method is in Fig. 2.  The first line of the algorithm 
initializes all of the system parameters, such as state variables and Q-value and then 
proceeds to the agent’s learning procedure.  In this study, the solar energy is selected 
as renewable power source for an example such that a day can be defined as an 
episode, called a round hereafter.  In each daily round, as starts in line 2, the 
algorithm goes through line 3 to line 10.  The inner loop, as line 4, goes through line 
5 to 10, which represents each timeslot iteration, or step.  In line 5, the agent chooses 
an appropriate action from its power management policy according to the Q value at 
the current state, and then in line 6 the action is executed in the corresponding step.  
The agent then observes the environment’s parameters, i.e., the next state and reward 
as in line 7.  The step iterative formula could be written as line 8 according to the 
Q-learning, where the parameter gradually decreases with the number of the observed 
state.  After the Q-values are updated the next state replaces the current state (line 9).  
Once the available timeslot number for a daily round is reached, the next day (another 
round) comes up, as shown in line 10.  
 
 
Fig. 2. The Q-Learning algorithm applied to the proposed QoS-aware power management 
 
The state vector is represented in quadruple, denoted as (SD, SH, SB, SQ)∈S, for 
the environment information respectively, the action space is represented in Adc∈A 
for the operation of duty cycle, and the reward is represented in signed integer.  
According to the defined architecture, QoS-aware power management for energy 
harvesting sensor node, powered by battery and renewable energy source, can be 
explicit as the following. 
 
A. States 
In [5], Kansal et al. define criteria of energy neutrality (EN) as maintaining the 
energy equilibrium between energy consuming and energy harvesting.  By 
supporting the energy neutrality of a sensor node, sustainable operation of sensor 
node could be obtained such that the operation life of sensor is extended.  The 
distance of energy neutrality of the sensor node,ΔEneutral(i), is defined as the 
difference between Eharvest(i) and Enode(i) at the end of the ith sensing period as 
following 
1 Initialize all Q(s,a) to zeros 
2 For every day 
3 Initialize state vector si 
4 Repeat every timeslot i 
5  Choose duty-cycle action ai:=Adc(i) 
using Soft-Max 
6  Adjust duty-cycle to Adc(i) 
...wait until timeslot i ends 
would be assessed by the condition of energy neutrality instead.  The positive reward 
would be granted if the action would cause the SD transfer to be “EN” from any other 
state of SD, otherwise the negative reward would be granted. 
 
IV. CONFIGURATIONS OF SIMULATION 
A. The environment configuration for energy harvesting system 
The experiment is conducted by simulation to validate the proposed method.  The 
satisfied degree of QoS requirement is examined in the simulation way to see whether 
or how good the proposed method can be done, as well as the energy neutrality 
property is examined in the simulation to see whether the proposed method can be 
stably maintained by the reinforcement learning.  Before the simulation the 
environment for energy harvesting system and the available configuration should be 
established first.   In this study, the solar energy is employed as the harvested energy 
source; where the proportion of the solar energy to the intensity of sunshine is 
calculated by the ideal solar energy equation [10] with the altitude and azimuth of 
sunshine [11].  The solar panel is BSP-112 with reference to PowerUp Company 
[12], which can produced power up to 1W.  The energy diminished probability by 
the atmosphere circumstances is 20%, where the harvested energy is diminished 
according to atmosphere circumstances by the normal distribution with mean 0.5 and 
the variance 0.1.  Thus, the annual sunshine profile is obtained under the given 
parameters before the simulation is conducted.  Fig. 3 shows the harvested energy by 
solar panel of earlier ten days.  According to Kansal’s energy harvesting model for 
deriving the certain sunshiny profile’s feature [5], the average harvesting power is 
equal to 315.59mW, the deferential energies between the maxima/minima harvesting 
energy and the average harvesting power are calculated as 2702.58J and 1136.13J, 
respectively.  The average operational power is constrained by the average 
harvesting power at the given voltage supply of 3.6V. By the power constrain, the 
average ampere at the certain operational voltage is 87.66mA.  According to the 
condition of energy neutrality [6], the capacity of the energy storage can be estimated, 
which is 5944mAH, from the configuration previous defined.  Thus, a commercial 
battery with capacity of 7000mAH at 3.6V is chosen for simulation. In the follow 
experiments, four levels of duty-cycle rate are used for any sensing period which are 
100%, 65%, 30% and 0% (as the sleep mode), and the corresponding ampere 
consumed are 160mA, 104mA, 48mA and 0mA respectively since the longer duty 
cycle, the more the energy is consumed. 
strategy would be used at different QoS state if the QoS on demand. 
 
V. SIMULATION RESULT AND EVALUATION 
The initial battery residue energy is about 50% and the QoS request incoming rate is 
λ=6 of Poisson distribution for all above simulation.  The simulation tracings are 
shown in Fig. 4 and Fig. 5.  After numbers of simulation days, Fig. 4 (a) shows the 
profile of remained energy in battery, which is fluctuant with limitation between 55% 
and 65% for the proposed method (solid blue line).  This implies that the energy 
neutrality is always supported, thus a long-term operation for the sensor node can be 
seen.  The average daily duty cycle rate in later simulation days is shown in Fig. 4 
(b), which is always higher than 55% and almost match with Kansal’s ADC (dash red 
line). 
TABLE I 
THE RULE OF REWARD GIVEN FOR NONE OF QOS ON DEMAND 
Distance of energy 
 neutrality 
Storage State 
<-0.5mAH -0.5~0.5mAH >0.5mAH 
High 2 -1 -2 
Moderate -2 2 -2 
Low -2 -1 2 
 
TABLE II 
THE RULE OF REWARD GIVEN FOR QOS ON DEMAND 
 Duty 
cycle 
QoS State 
<5% 5~35% 35~65% >65% 
Light -1 +2 +1 -2 
Moderate -2 -1 +2 +1 
Heavy -4 -2 -1 +2 
 
  The Fig. 5 shows the Results of QoS Achievability.  The positive QoS difference 
percentage (over the baseline) means that the QoS requests have been satisfied.  We 
can see that the proposed method successfully control the QoS nearly 100%.  There 
are only a few drops under the baseline between day 437 and day 438. 
For the experiment we evaluated Kansal’s Adaptive Duty Cycling (ADC) algorithm [3] 
and the proposed method on four performance items: average residue energy of 
storage, which provided a measure of the average of energy remained in the battery 
over all simulation day; average duty cycle rate, which provided a average measure of 
service performance over all simulation day; average QoS variance, which provided 
the variance measure between service performance and QoS requirement; 
Achievability of QoS, which provided the normalized measure of the QoS satisfied 
degree.  The numerical representation is shown in Table III.  This table lists the 
evaluation and comparison result. All the measurements have been done in the 
positive way, therefore the percentage of each item indicates that the higher the 
percentage, the better the performance.  Thus, the performance of the proposed 
method to achieve QoS demand is outstanding based on nearly equivalent residue 
energy of storage which compares with Kansal’s Adaptive Duty-Cycling Control 
(ADC) method. 
 
TABLE III 
PERFORMANCE COMPARISON ON MODERATE QOS DEMANDING 
         
   Method 
 
 
Evaluation Item 
ADC Proposed 
Ave. Residue Energy of Storage 59.02% 60.28% 
Ave. Duty Cycle Rate 63.53% 62.81% 
Ave. QoS difference 48.38% 40.73% 
Ave. Achievability of QoS 90.23% 99.31% 
 
VI. CONCLUSION 
In this study, a QoS-aware power management method based on reinforcement 
learning is proposed to improve the energy utilization and satisfy the QoS demand for 
energy harvesting wireless sensor networks. According to the experiment result, the 
proposed method increases the energy remained in the storage, while maintains 
duty-cycle rate above 50%. By the reinforcement learning and rewarding mechanism, 
the condition of energy neutrality is held. The findings of this study are in below: 
1. At light QoS requirement the agent will decrease the duty-cycle rate autonomously 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/11/30
國科會補助計畫
計畫名稱: 基於強化學習之嵌入式系統動態電源管理研究及其應用於集能式無線感測網路
之FPGA實現與模擬
計畫主持人: 徐超明
計畫編號: 98-2221-E-415-011- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
