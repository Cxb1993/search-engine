should be more accurate in describing the diffusion 
tensor distribution. Using ＇Gaussian mixture＇ 
model, we will need to develop new methods to 
estimate the parameters within the probability model. 
Using this new model, we will develop a new 
segmentation algorithm which should yield better 
results than the current model. 
英文關鍵詞： diffusion-tensor imaging, diffusion-tensor 
segmentation, Gaussian probability model, Gaussian 
mixture model. 
 
The segmentation of diffusion tensor data is a relatively new field. Zhukov et. al. [2] used a 
measure called “tensor invariants” combined with level set method to segment the DTI data. The 
“tensor invariants” are combinations of tensor elements that do not change after the rotation of 
the tensor. Wiegell et. al. [3] used the weighted sum of Frobenius norm of tensor difference and 
the voxel distance as the measure. They segmented the thalamic nuclei using modified k-means 
algorithm. Wang and Vemuri used the Frobenius norm [4] and J-divergence [5] in region-based 
active contour model to segment the data. Rousson [6] and Lenglet [7] used Gaussian probability 
model together with level set method. Feddern et. al. [8] used Frobenius norm of tensor 
difference and combined mean curvature motion and self-snake into an active contour method to 
segment the data. Jonasson et. al. [9] used an approach called “geometric flow” in level set 
method. The measure they used is a normalized tensor scalar product. 
4. 研究方法 
The goal of this project is to develop a new method to segment MRI diffusion tensors. To 
put it simply, the problem we face is to segment N tensors, NT,,T,T 21 , into K classes. To 
achieve this, we need to first define a metric to measure the similarity between two tensors, i.e., 
the distance between the two tensors iT  and jT . We can express this distance as ),( ji TTD . In 
the literature, there are three distance measures currently in use: Forbenius norm, J-divergence, 
and geodesic distance. Lenglet found that both J-divergence and geodesic distance were better 
than Forbenius norm. 
The J-divergence between two tensors iT  and jT  is the symmetric form of the 
Kullback-Leibler divergence or the relative entropy of the two tensors. It is calculated as: 
 ),( ji TT
2D = )6)((
4
1 11


jijitrace TTTT  (2) 
The geodesic distance between two tensors iT  and jT  is a quantity derived from 
Riemannian geometry. Since a tensor is a 3x3 symmetric, positive-definite matrix, it has six 
independent components. Therefore, each tensor can be considered as a point on a 6-dimensional 
manifold. The distance between the two tensors on this 6-dimensional manifold is the geodesic 
distance. It can be calculated as: 
 ),( ji TT
2D = ))((log
2
1
2
1
2
1
2 
ijitrace TTT  (3) 
With the distance defined, segmentation can be performed directly. However, if we can 
express the relationship between two tensors as a conditional probability, we can put the 
The covariance of the Gaussian pdf is: 
 


N
i
T
ii
N 1
1
γγΛ  (13) 
The Gaussian pdf is then: 
 










2
exp
)2(
1
),|(
1
6
i
T
i
ip
γΛγ
Λ
ΛTT

 (14) 
Lenglet showed that using this probability with geodesic distance can obtain superior 
segmentation results. 
In this project, we propose a new model to describe the diffusion tensor. Instead of a single 
Gaussian pdf, we will use the sum of k Gaussian pdf’s. We call this model “Gaussian mixture”. It 
can be written as follows: 
 












2
exp
)2(
1
),,,,,|(
,
1
,
6
1
11
jij
T
ji
j
k
j
jkkip
γΛγ
Λ
ΛΛTTT

  (15) 
where kTT ,,1   are the empirical means as described in Equations (4)-(7), kΛΛ ,,1   are the 
covariance matrices as described in Equation (13), ji,γ  is the independent-component vector for 
the j-th Gaussian pdf as described in Equation (12), and j  is the weight for the j-th Gaussian 
pdf constrained by the following equation: 
 1
1


k
j
j  (16) 
There are two problems needed to be solved in Equation (15). The first is the estimation of 
all the parameters: jT , jΛ , and j . The second is the estimation of the order of the mixture, 
i.e., the value of k. In the following two sub-sections, we will describe the solutions to the two 
problems. 
4.1 Estimation of the Parameters: 
The estimation of the parameters can be considered as the problem to maximize the 
log-likelihood as follows: 
 


 


















N
i
jij
T
ji
j
k
j
j
N
i
kkipL
1
,
1
,
6
11
11
2
exp
)2(
1
log)),,,,,|(log(
γΛγ
Λ
ΛΛTTT

  (17) 
To simplify the above equation, we will let 
 










2
exp
)2(
1
),|(
,
1
,
6
jij
T
ji
j
jjijf
γΛγ
Λ
ΛTT

 (18) 
The estimation of the order of the mixture is done iteratively. Starting from K=1, K is 
increased by 1 in each iteration until one of the mixture components ),|( jjijf ΛTT  is less than 
a given threshold. 
4.3 Segmentation 
For segmentation, we use the “Geodesic Active Regions” (GAR) model [11] with the 
following energy function: 
 
Cdp
dpCE
kk
kk






2
1
)),,,,,|)((log(                         
)),,,,,|)((log(),,(
,21,2,21,2
,11,1,11,121
xΛΛTTxT
xΛΛTTxT


 (28) 
where C is the contour that divides two regions 1  and 2 , the p function is as defined in 
Equation (15) with ii,T  and ji,Λ  representing the mean and the covariance matrix of the i-th 
region, C  is the length of the contour, and   is a constant. 
5. 結果與討論 
The MRI images were obtained from a GE 1.5T scanner at China Medical University 
Hospital. The acquisition parameters were: b value = 1000 sec/mm2 , TE=1000 msec, TR=89 
msec, 128 x 128 pixels of 79 slices. 
 
 
Fig. 1: The segmentation results. The red region is the result from scalar Gaussian model. The 
green region is the result from Gaussian mixture model. 
Partition Problems in Computer Vision,” Journal of Visual Communication and Image 
Representation, vol. 13, pp. 249-268, 2002. 
 
Please cite this article in press as: J.-R. Liao, et al., Accelerating Fourier volume rendering by polar coordinate data representation, Comput.
Methods Programs Biomed. (2012), http://dx.doi.org/10.1016/j.cmpb.2012.05.010
ARTICLE IN PRESSCOMM-3398; No. of Pages 9
2  c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e x x x ( 2 0 1 2 ) xxx–xxx
For image-based methods, we cast a ray from each pixel on
the projection and accumulate the contributions from all the
voxels along the ray casting path. For object-based methods,
the contribution of each voxel in the volume data is accu-
mulated one by one onto the projection plane. This approach
is often called “splatting”. The computational complexity for
both methods is O(n3).
Fourier volume rendering (FVR) or frequency domain vol-
ume  rendering [5,6] is a way to reduce the computational
complexity considerably. It is based on slice projection the-
orem. The theorem states that the 2D frequency spectrum of
a projection is equal to a slice in the 3D frequency spectrum of
the original data. To implement this, we  ﬁrst Fourier transform
the 3D spatial domain data. Then, a slice is extracted by inter-
polating in the 3D frequency domain. Finally, the extracted
slice is inversely Fourier transformed to obtain the projection
of the 3D spatial domain data. The computational complexity
is O(n2 log n).
Previous FVR algorithms store frequency domain data in
rectilinear coordinate. To obtain a slice, we  need one trilin-
ear interpolation for each pixel. In this paper, we propose
a new FVR algorithm that stores and processes the fre-
quency domain data in polar coordinate. Because the data
are stored in polar coordinate, slice extraction can be done
by bilinear interpolation between adjacent angles. However,
the extracted slice is in polar coordinate and we need to con-
vert the slice to rectilinear coordinate before applying inverse
Fourier transform. This conversion can be also done by bilin-
ear interpolation and, thus, reduces the overall complexity to
O(n2).
In addition to the advantage of reduced complexity, there
are three novelties in polar coordinate FVR. The ﬁrst is to uti-
lize data regularity. When polar coordinate data are stored
along the radial direction ﬁrst, the data are stored in adjacent
memory  location when extracting a slice. The regularity of the
data storage makes memory  access much more  efﬁcient than
rectilinear coordinate. The second is to use nearest-neighbor
interpolation. Because of the high data density near the ori-
gin, we  can use nearest-neighbor interpolation to extract a
slice without sacriﬁcing image  quality. This further saves the
computational complexity. The high sampling density near
the origin also enables us to reduce the sampling density
at high spatial frequency and allows us to reduce memory
storage at a slight expense of image  quality. The third is to
shift most of the computation to the preprocessing stage.
Since converting rectilinear coordinate data into polar coor-
dinate is a one-time process, polar coordinate FVR enables us
to use a better interpolation kernel with larger support than
those that are traditionally used in spatial domain approaches.
In turn, this makes the speed in interactive visualization
independent of the size of the interpolation kernel. There-
fore, our new method has lower computational complexity
and higher data access efﬁciency than traditional rectilinear
methods.
This paper is organized as follows. Section 2 reviews the
related work in FVR. Section 3 introduces our new algo-
rithm. Section 4 shows the images generated by our algorithm
and compares the performance with other FVR algorithms.
Section 4.2 discusses the design tradeoff for our algorithm.
Section 5 concludes.
2.  Background
FVR was independently proposed by Dunne et al. [5] and
Malzbender [6].  Its application in magnetic resonance imaging
was described by Napel et al. [7].  We  can divide the later devel-
opments into three categories: (1) addition of visual effects, (2)
combining with wavelet transform, and (3) hardware acceler-
ation.
In adding visual effects, Totsuka and Levoy added shading
and depth cueing into FVR [8,9]. They found that a hemi-
spherical light source and a sinusoidal depth cueing function
can be easily applied in frequency domain [9].  Entezari et al.
[10] further improved the shading effect with higher order
approximation and spherical harmonics. To achieve an effect
similar to the transfer function in spatial domain, Nagy et al.
[11] described a method to limit data to a certain threshold
range.
In combining with wavelet transform, Grosso and Ertl [12]
used wavelet to transform spatial domain data to lower reso-
lution. Then, they rendered the low resolution data to achieve
speedup. Gross et al. [13] applied wavelet transform to decom-
pose the spatial domain data into several levels of details.
Depending on the desired resolution, the ﬁnal image  was gen-
erated by combining the results from different levels of details.
As a result, ray casting needed to be calculated from several
integrals and they used FVR to make the integration more  efﬁ-
cient. They called this method “wavelet splatting”. Westenberg
and Roerdink [14] further improved this approach and called
it “wavelet X-ray transform”. As opposed to Gross’s approach
which applied wavelet transform in spatial domain, they used
a frequency domain implementation of the wavelet transform
and signiﬁcantly reduced the complexity of the algorithm.
Later, Westenberg and Roerdink [15] extended their algorithm
by adding “view interpolation” to increase its speed during
user interaction. The so-called “view interpolation” was to
present a low resolution image  during user interaction and
reﬁned the image  when user interaction ceased.
In hardware acceleration, Lichtenbelt [16] gave a general
overview of FVR and proposed a hardware implementation.
Viola et al. [17] implemented the complete FVR algorithm in
GPU to signiﬁcantly increase its performance. Jansen et al. [18]
implemented split-stream FFT in GPU and applied it in FVR.
In all these works, frequency domain data were stored in
rectilinear coordinate and slice extraction was done by inter-
polation. In the following, we will show that storing data in
polar coordinate has many  advantages over rectilinear coor-
dinate.
For simplicity, we  will call polar coordinate FVR as polar
FVR from now on.
3. Description  of  method
Our polar FVR algorithm can be divided into the following ﬁve
steps:
1. Generate rectilinear coordinate frequency domain data by
applying 3D Hartley transform to the spatial domain data.
2. Convert the rectilinear coordinate data to polar coordinate.
Please cite this article in press as: J.-R. Liao, et al., Accelerating Fourier volume rendering by polar coordinate data representation, Comput.
Methods Programs Biomed. (2012), http://dx.doi.org/10.1016/j.cmpb.2012.05.010
ARTICLE IN PRESSCOMM-3398; No. of Pages 9
4  c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e x x x ( 2 0 1 2 ) xxx–xxx
Fig. 1 – Illustration of polar FVR algorithm: (a) frequency domain data in rectilinear coordinate, (b) frequency domain data in
polar coordinate, (c) extracted slice in polar coordinate and in rectilinear coordinate and (d) inverse Fourier transform of the
slice.
Fig. 2 – Block diagram for (a) traditional FVR and (b) polar FVR. In (b), the circled number on the upper left corner of each
block indicates the number of step in the algorithm.
Please cite this article in press as: J.-R. Liao, et al., Accelerating Fourier volume rendering by polar coordinate data representation, Comput.
Methods Programs Biomed. (2012), http://dx.doi.org/10.1016/j.cmpb.2012.05.010
ARTICLE IN PRESSCOMM-3398; No. of Pages 9
6  c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e x x x ( 2 0 1 2 ) xxx–xxx
Fig. 5 – Comparison of images without lighting effects. Top:
traditional FVR with linear interpolation, middle: traditional
FVR with Lanczos ﬁltering and bottom: polar FVR.
pre-calculate the relative memory  locations and the interpo-
lation weights for the corresponding polar coordinate data.
Then, the resampling is just a simple process of table lookup,
multiplication, and accumulation.
After the 2D coordinate conversion is done, we can apply
inverse Hartely transform to generate the ﬁnal image.
3.4.  Lighting  effects
We  now turn our attention to how to add shading effect into
polar FVR. In the shading proposed by Totsuka and Levoy
[9], the Fourier transform of the spatial domain gradient is
required as additional data. To add this effect into polar FVR
is straightforward. All we need to do is convert the Fourier
transform of the gradient data to polar coordinate.
However, depth cueing effect is more  difﬁcult to implement
in polar coordinate. The reason is that we need to generate an
additional set of data by shifting the origin of the original fre-
quency domain data. The distance and the direction of the
shift depend on the desired depth cueing effect. The depth
cueing effect is created by combining the data from the orig-
inal data and the data from the shifted data. For traditional
Fig. 6 – Comparison of images with lighting effects. Top:
traditional FVR with linear interpolation, middle: traditional
FVR with Lanczos ﬁltering and bottom: polar FVR.
FVR, this means that we  need to do two full three-dimensional
interpolations for both sets of data. For polar FVR, the interpo-
lation for the original data can be implemented as described
in our method. However, since the origin of the polar data no
longer coincide with that of the shifted data, the interpolation
for the shifted data will need a full three-dimensional interpo-
lation. In other words, the speed of polar FVR will be reduced
considerably. Therefore, depth cueing is not implemented.
4. Status  report
To compare our polar FVR algorithm with traditional rec-
tilinear coordinate FVR algorithm, we implemented both
algorithms on a desktop computer equipped with a Pentium
4 3.2 GHz processor and 1 GB of RAM. The data set used was
the CT Head dataset from Volume II of the Chapel Hill Vol-
ume  Rendering Test Dataset. The original data contained
256 × 256 × 113 samples. To obtain a 256 × 256 × 256 dataset,
we ﬁrst duplicate the 113 samples to 226 samples and then
zero ﬁlled on both sides. Without otherwise stated, all the
polar FVR data are resampled to an angular resolution of 0.5◦
Please cite this article in press as: J.-R. Liao, et al., Accelerating Fourier volume rendering by polar coordinate data representation, Comput.
Methods Programs Biomed. (2012), http://dx.doi.org/10.1016/j.cmpb.2012.05.010
ARTICLE IN PRESSCOMM-3398; No. of Pages 9
8  c o m p u t e r m e t h o d s a n d p r o g r a m s i n b i o m e d i c i n e x x x ( 2 0 1 2 ) xxx–xxx
Table 1 – Comparison of execution time in each step of real-time processing for traditional FVR and polar FVR.
Slice extraction (ms) Coordinate conversion (ms) Inverse FHT (ms) Frame rate (fps)
FVR
Lanczos 1180.00 – 19.6  0.83
Cubic spline 300.00 – 19.6 3.13
Linear 32.00 – 19.6 19.38
Polar FVR 0.64 5.1 19.6 39.46
Fig. 9 – Images of polar FVR: (a) hydrogen atom, (b) head MR
angiography and (c) engine block.
4.2.  Image  quality
Images generated from traditional FVR and polar FVR are
shown in Figs. 5 and 6. Since image  quality is a tradeoff
between quality and speed, it depends heavily on the require-
ments of the application. As an initial report, we will only
compare the images qualitatively because the conditions of
two algorithms are not exactly the same.
Image quality from polar FVR is almost the same as that
from traditional FVR with Lanczos ﬁltering and is much better
than that from traditional FVR with linear interpolation. The
superiority of Lanczos ﬁltering can be seen from the noticeable
ghost and blurring in linear interpolation. However, in terms
Fig. 10 – Polar FVR image generated from the
512 ×512 ×512 knee dataset.
of rendering speed, traditional FVR with Lanczos ﬁltering is
too slow to be useful in interactive applications. Therefore, we
can conclude that polar FVR achieves better image  quality at
a much faster speed.
A concern about polar FVR is that the four corners of the
frequency domain data are omitted because we  only sample a
disk but not a square. Since most energy in frequency domain
is concentrated in low frequency and the amount of energy in
the four corners is relatively small, the omission should have
only small impact in image  quality. The images in Figs. 5 and 6
conﬁrm this argument.
Another issue related to image  quality is the angular reso-
lution for the re-sampled polar coordinate data. Fig. 7 shows
the images generated from three angular resolutions: 0.5◦,
0.6667◦, and 1◦. These resolutions correspond to 720, 540, and
360 samples on a 360◦ circle. From naked eyes, it is difﬁcult
to perceive noticeable differences among the three images.
Fig. 8 shows the difference images of 0.6667◦ and 1◦ resolu-
tions with respect to 0.5◦ resolution. We can clearly see that
the differences have a high-frequency characteristic. This is
because the frequency domain data are very dense near the
origin and this makes the low-frequency components insen-
sitive to angular resolution. However, at high frequencies, the
insufﬁcient sampling at larger angular resolution causes high-
frequency artifacts in the difference images.
 1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：101年 7 月 31日 
                                 
一、參加會議經過 
    於 6/30 搭乘中華航空公司 CI032 班機前往加拿大溫哥華，同日晚間抵達，停留一晚。7/1 租車
前往班夫，7/2 抵達班夫。7/3 早上參加 Session「Registration, Segmentation and Visualization」，共有
五篇論文發表，分別為「Digital Mammogram segmentation based on Fisher Information Measure」、
「Pupil Detection in Infrared Images based on Active Contour Models」、「A New Image Edge Detection 
Method using Quality-based Clustering」、「An Improved Nearest-Neighborhood Algorithm for Efficient 
Super-Resolution Images Implemented on ARM-9 AT91SAM9RL」、「Rendering Particle Trajectories 
with Color-Coded Information for Atomistic Visualization」。 
    7/3 下午聽取北不列顛哥倫比亞大學 Amir Asif 教授發表 Keynote Speech：「Time Reversal Array 
Imaging Algorithms: Application to Brest Cancer Detection」，內容介紹改良以低強度電磁輻射
backscatter imaging 偵測乳房腫瘤之信號處理方法，由 Fear 與 Stoica 所提出之 backscatter imaging
方法，由於信號多重路徑的問題，影像重建易受干擾，Asif 教授則提出以反向時間陣列(Time 
Reversal Array)的信號處理提高其正確性。 
    7/3 下午聽取 Keynote Speech 之後，參加 Session「Image Coding, Compression, Learning, and 
Recognition」擔任 Session Chair，並發表論文「Ellipse Fitting by Projection for Maximum Coverage」，
Session 中另有五篇論文發表，第一篇為「Fast Dual-based Linearized Bregman Algorithm for 
計畫編號 NSC 100－2221－E－005－001－ 
計畫名稱 以高斯混合模型為基礎之磁振造影擴散張量切割 
出國人員
姓名 
廖俊睿 
服務機構
及職稱 
國立中興大學通訊工程研究所 
會議時間 
101年 7月 3日至 
101年 7月 5日 
會議地點 加拿大班夫 
會議名稱 
(中文)第十屆視覺化、造影、影像處理國際會議 
(英文)IASTED International Conference on Visualization, Imaging, and 
Image Processing (VIIP2012) 
發表論文
題目 
(中文)以投影法匹配最大覆蓋面積之橢圓形 
(英文)Ellipse Fitting by Projection for Maximum Coverage 
附件四 
 
 3 
    Backscatter imaging 在女性乳房腫瘤偵測上深具潛力，由於其非穿透性的特性，反射信號會有
多重路徑問題，因此其中仍存在著許多信號處理的問題，值得我們進一步加以探討，並考慮新的
信號處理技術，以應用到其影像重建之上。 
 
五、攜回資料名稱及內容 
    會議論文集光碟一張。 
 
六、其他 
 
ELLIPSE FITTING BY PROJECTION FOR MAXIMUM COVERAGE
Jan-Ray Liao
Dept. of Electrical Engineering
National Chung Hsing University
Taichung, Taiwan
email: jrliao@mail.nchu.edu.tw
Shye-Chorng Kuo
Dept. of Computer Science and Information Engineering
Nan Kai University of Technology
Caotun, Taiwan
email: t127@nkut.edu.tw
ABSTRACT
In this paper, we propose a new ellipse fitting method which
maximizes the area of the ellipse, i.e., finds the maximum
coverage of the ellipse. Through mathematical derivation,
we show that the multiplication of two orthogonal projec-
tions of an ellipse is maximized when the projection angle
is equal to the angle of the major or minor axes. With the
border points of an object as input, our algorithm first finds
the maximum projection length of the object at all angles.
Then, we find the maximum of the multiplication of two
orthogonal projections. The lengths and the angles of the
projections are taken as the estimation of the geometric pa-
rameters of the ellipse. Through experiments, we show that
our new approach gives better results as compared to the
popular least square approach.
KEYWORDS
Ellipse fitting, maximum coverage, orthogonal projection,
least square
1 Introduction
Converting image objects into simple geometric primitives
is an essential task in pattern recognition and computer vi-
sion. Ellipses are one of the most basic geometric primi-
tives which can be used to describe many objects. To fit
an object to an ellipse, we need to consider the following
algebraic equation:
Q(x, y) = rx2 + sxy + ty2 + ux+ vy + w (1)
with the constraint that (s2−4rt) < 0. The six parameters:
r, s, t, u, v, and w, can be called the “algebraic parameters”
of the ellipse. Since one of the six algebraic parameters is
dependent on the other five, it is common to set w to be
equal to one [3]. An alternative to algebraic parameters is
the five geometric parameters of an ellipse: the center of
the ellipse (denoted as x0, y0), the lengths of the major and
minor axes (denoted as a, b), and the angle of the major
axis with respect to the x-axis (denoted as θ).
If we let all the points on the border of an object be
represented by
(xi, yi) i = 1, · · · , n (2)
For a perfect fit, Q(xi, yi) will be equal to zero for all i.
However, it is almost certainly that there will be errors pre-
sented. One of the most popular way to fit the ellipse is
to minimize the sum of certain error measure. Of all the
error measures, the easiest one to calculate is the algebraic
distance Q(xi, yi). Therefore, the most commonly used
method to fit an ellipse is using least square method to min-
imize the sum of the squared algebraic distance [1–3]:
n∑
i=1
Q2(xi, yi) (3)
These least square fitting methods can be divided into
two categories. The first is to find the algebraic param-
eters [1–7]. Variations include normalization [4, 5] and
weighting [6] on the algebraic distance. The second is to
find the geometric parameters [8–11]. The benefit of us-
ing geometric parameters is that the fitting can be divided
into several different parts although this division may lead
to suboptimal results.
While algebraic distance is easy to find, it is known
to be biased [12]. A more appropriate error measure is the
geometric distance. To minimize the sum of geometric dis-
tance, the problem is nonlinear and the solution needs to be
found iteratively by Gauss-Newton method [13–16]. Other
approaches includes: Fourier transform [17], approxima-
tion [18, 19], semidefinite programming [20], and genetic
algorithm [21].
In addition to error minimization methods, the fitting
can also be done by optimizing moments [22–24], finding
the median of multiple solutions [25,26], and Hough trans-
form [27–31]. Among these approaches, Hough transform
is one of the most popular method for ellipse detection.
However, they are suitable for ellipse fitting only when the
search space can be mapped to either the algebraic or the
geometric parameters of an ellipse [28, 29, 31]. Another
limitation of the Hough transform is its high computational
load. This has to be overcome by either limiting the number
of points used in voting [29,31] or decomposing the search
space to lower dimension [28, 30–32]. Unfortunately, both
approaches to speed up the algorithm lead to less accurate
estimation in elliptical parameters.
In this paper, we propose a new ellipse fitting method
which maximizes the area of the ellipse. In other words,
our method finds the ellipse with the maximum coverage.
As will be derived in the following section, the projection
 Proceedings of the IASTED International Conference
July 3 - 5, 2012 Banff, Canada
Visualization, Imaging and Image Processing (VIIP 2012)
DOI: 10.2316/P.2012.782-017 103
For the third step, we can find the maximum of
the multiplication of the orthogonal projection lengths by
searching through the array generated in the second step.
After the maximum is found, the long and short projection
lengths are used as the lengths of the major and minor axes
respectively and the angle of the long projection length is
used as the angle of the major axis.
4 Experimental Results
4.1 Evaluation Criteria
We compared our algorithm with the least square algorithm
[8]. We considered two error measures as the quantitative
benchmark. One was the coverage ratio and the other was
the mismatch ratio. To define these two ratios, we needed to
define three areas which was equal to the number of pixels
satisfying certain criteria. The first was the object area, Ao.
It was defined as the number of pixels within the image
object that we tried to fit. The second was the coverage
area, Ac. It was defined as the number of pixels that were
within both the image object and the fitted ellipse. The third
was the mismatch area, Am. It was defined as the number
of pixels that were within the image object but not within
the fitted ellipse plus the number of pixels that were within
the fitted ellipse but not within the image object.
The coverage ratio, λ, was defined as:
λ =
Ac
Ao
(14)
The mismatch ratio, ρ, was defined as:
ρ =
Am
Ao
(15)
For a perfect match, the coverage ratio will be 100%
and the mismatch ratio will be 0%.
Fig. 1 is an example of the two error measures. Fig.
1(a) is the input image. Fig. 1(b) shows the results of our
algorithm. The coverage ratio is 99.46% and the mismatch
ratio is 20.95%. Fig. 1(c) shows the results of the least
square algorithm. The coverage ratio is 80.42% and the
mismatch ratio is 32.16%.
4.2 Experimental Results
We tested our algorithm on twelve image objects. These
objects can be divided into two groups: synthetic objects
and natural objects. Fig. 2 and Fig. 3 showed the synthetic
objects and the natural objects, respectively. Fig. 4 and Fig.
5 showed the fitted ellipses for both our algorithm and least
square fitting. Table 1 showed the comparison of coverage
ratio and mismatch ratio for both algorithms. As can be
seen from the results, our algorithm was better than least
square fitting for all the twelve objects shown.
The superiority of our algorithm comes from its rela-
tive insensitivity to noise inside the ellipse contour because
they are masked by the projection. This implies that we
(a)
(b)
(c)
(d)
(e)
Figure 1. An example of error measures: (a) input image,
(b) and (d) are ellipses fitted by our algorithm, (c) and (e)
are ellipses fitted by least square algorithm. In (b) (e), solid
line is the boundary of the input object, dotted line is the
fitted ellipse. In (b) and (c), gray area represents Ac. In (d)
and (e), gray area represents Am.
105
Table 1. Comparison of error measures for our algorithm and least square fitting.
Coverage Ratio Mismatch Ratio
Maximum Coverage Least Square Maximum Coverage Least Square
ellipse1 99.88% 71.03% 12.12% 36.80%
ellipse2 99.72% 84.59% 6.76% 17.65%
ellipse3 98.33% 63.60% 50.90% 81.70%
ellipse4 99.83% 71.56% 11.67% 40.13%
ellipse5 99.65% 49.35% 32.53% 86.79%
ellipse6 99.99% 72.12% 27.28% 45.57%
melon 99.49% 90.90% 12.34% 16.57%
kiwi fruit 97.37% 85.02% 10.72% 19.94%
pomelo 96.54% 71.90% 17.33% 48.10%
golf ball 99.91% 82.60% 24.26% 30.85%
coin 99.59% 88.51% 4.60% 16.14%
football 99.89% 67.98% 30.68% 47.60%
[2] T. Ellis, A. Abbood, B. Brillault, Ellipse detection and
matching with uncertainty, Image and Vision Comput-
ing, 10(5), 1992, 271-276.
[3] P.L. Rosin, A note on the least squares fitting of
ellipses, Pattern Recognition Letters, 14(10), 1993,
799-808.
[4] A. Fitzgibbon, M. Pilu, R. Fisher, Direct least square
fitting of ellipses, IEEE Trans. Pattern Analysis and
Machine Intelligence, 21(5), 1999, 476-480.
[5] E.S. Maini, Enhanced direct least square fitting of el-
lipses, International Journal of Pattern Recognition
and Articial Intelligence, 20(6), 2006, 939-953.
[6] L. Tian, S. Kamata, A Two-Stage point pattern
matching algorithm using ellipse fitting and dual
Hilbert scans, IEICE Trans. Information and Systems,
E91D(10), 2008, 2477-2484.
[7] X. Bai, C. Sun, F. Zhou, Splitting touching cells based
on concave points and ellipse fitting, Pattern Recog-
nition, 42(11), 2009, 2434-2446.
[8] W. Wu, M.J. Wang, Elliptical object detection by
using its geometric properties, Pattern Recognition,
26(10), 1993, 1499-1509.
[9] G. Watson, Least squares fitting of circles and ellipses
to measured data, BIT Numerical Mathematics, 39(1),
1999, 176-191.
[10] D. Chaudhuri, Object area-based method for elliptic
and circular fit of a two-tone image, Defence Science
Journal, 58(6), 2008, 710-714.
[11] D. Chaudhuri, A simple least squares method for fit-
ting of ellipses and circles depends on border points
of a two-tone image and their 3-D extensions, Pattern
Recognition Letters, 31(9), 2010, 818-829.
[12] P.L. Rosin, Assessing error of fit functions for el-
lipses, Graphical Models and Image Processing,
58(5), 1996, 494-502.
[13] W. Gander, G.H. Golub, R. Strebel, Least-squares fit-
ting of circles and ellipses, BIT, 34(4), 1994, 558-578.
[14] H. Spath, Least squares fitting of ellipses and hyper-
bolas, Computational Statistics, 12, 1997, 329-341.
[15] H. Spath, Orthogonal distance fitting by circles and
ellipses with given area, Computational Statistics, 12,
1997, 343-354.
[16] S.J. Ahn, W. Rauh, H. Warnecke, Least-squares or-
thogonal distances fitting of circle, sphere, ellipse, hy-
perbola, and parabola, Pattern Recognition, 34(12),
2001, 2283-2303.
[17] D. Proffitt, The measurement of circularity and ellip-
ticity on a digital grid, Pattern Recognition, 15(5),
1982, 383-387.
[18] M. Stricker, A new approach for robust ellipse fitting,
Proc. Int’l Conf. Automation, Robotics, and Com-
puter Vision, 1994, 940-945.
[19] P.L. Rosin, Ellipse fitting using orthogonal hyperbo-
lae and stirling’s oval, Graphical Models and Image
Processing, 60(3), 1998, 209-213.
[20] G. Calaore, Approximation of n-dimensional data us-
ing spherical and ellipsoidal primitives, IEEE Trans.
Systems, Man and Cybernetics, Part A: Systems and
Humans, 32(2), 2002, 269-278.
[21] L.G. de la Fraga, I.V. Silva, N. Cruz-Cortes, Eu-
clidean distance fit of ellipses with a genetic algo-
rithm, Proc. 2007 EvoWorkshops: Applications of
Evolutionary Computing, vol.4448, Springer, Berlin,
Heidelberg, 2007, 359-366.
107
[22] B.B. Chaudhuri, G.P. Samanta, Elliptic fit of objects
in two and three dimensions by moment of inertia op-
timization, Pattern Recognition Letters, 12(1), 1991,
1-7.
[23] R. Safaee-Rad, I. Tchoukanov, B. Benhabib, K.C.
Smith, Accurate parameter estimation of quadratic
curves from grey-level images, CVGIP:Image Under-
standing, 54(2), 1991, 259-274.
[24] K. Voss, H. Suesse, Invariant fitting of planar objects
by primitives, IEEE Trans. Pattern Analysis and Ma-
chine Intelligence, 19(1), 1997, 80-84.
[25] P.L. Rosin, Ellipse fitting by accumulating five-point
fits, Pattern Recognition Letters, 14(8), 1993, 661-
669.
[26] F. Mai, Y. Hung, H. Zhong, W. Sze, A hierarchical
approach for fast and robust ellipse extraction, Pattern
Recognition, 41(8), 2008, 2512-2524.
[27] R.O. Duda, P.E. Hart, Use of the hough transforma-
tion to detect lines and curves in pictures, Communi-
cations of the ACM, 15(1), 1972, 11-15.
[28] N. Guil, E.L. Zapata, Lower order circle and ellipse
hough transform, Pattern Recognition, 30(10), 1997,
1729-1744.
[29] R.A. McLaughlin, Randomized hough transform: Im-
proved ellipse detection with comparison, Pattern
Recognition Letters, 19(3-4), 1998, 299-305.
[30] K.U. Kasemir, K. Betzler, Detecting ellipses of lim-
ited eccentricity in images with high noise levels, Im-
age and Vision Computing, 21(2), 2003, 221-227.
[31] S. Zhang, Z. Liu, A robust, real-time ellipse detector,
Pattern Recognition, 38(2), 2005, 273-287.
[32] Y.C. Cheng, The distinctiveness of a curve in a param-
eterized neighborhood: extraction and applications,
IEEE Trans. Pattern Analysis and Machine Intelli-
gence, 28(8), 2006, 1215-1222.
109
100年度專題研究計畫研究成果彙整表 
計畫主持人：廖俊睿 計畫編號：100-2221-E-005-001- 
計畫名稱：以高斯混合模型為基礎之磁振造影擴散張量切割 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
