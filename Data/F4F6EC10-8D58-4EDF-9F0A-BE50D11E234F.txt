 1
 
中文摘要 
傳統之音樂光碟（Music CD）都是以 44K取樣頻率對音頻信號加以數位化，而建立其音
頻資料檔。雖然此方式可獲得較佳的音質，但也必須付出儲存大量資料的代價。相對應於音
頻資料檔，另一種MIDI（樂器數位介面）格式資料檔則是僅需要僅紀錄樂曲中的音符演奏訊
息即可，因此整首樂曲將可大量地減少資料儲存量。然而對於一般的歌曲，除有樂曲外，仍
包含演唱者之歌聲；因此並無法全由 MIDI 資料來詮釋一首歌曲。在本計劃中，我們嘗試提
出一種樂曲與歌聲分離之音頻系統，其中樂曲部分可由 MIDI 資料取代樂器聲音其數位化取
樣波型資料；而歌聲部分則僅需以 11K Hz取樣頻率加以數位化即可。因此，相較於傳統音樂
CD，整首歌曲所需的資料量將可明顯地減少，而且尚能維持令人滿意的音質。 
在許多多媒體應用中，特別是於網路上，較少的多媒體資料量將有助於改善資料傳輸的
效率，並且可獲得較好的即時效果。顯然地，此種以 MIDI 為基礎之音頻系統的確適用於網
際網路或無線網路上有效率的多媒體資料傳輸。 
 
關鍵詞：MIDI基礎音頻系統、MIDI順序器、音調產生器、波形資料、多媒體資 
 
 
 
ABSTRACT 
Conventional music CDs store the digitalized audio data files sampled by 44K Hz. They have 
high quality, but a huge amount of data is required. An alternative is MIDI (Musical Instrument 
Digital Interface) files, which have a very small data size due to only recording the musical 
performance information. However, for a song including music and vocal, MIDI data cannot 
completely replace the digitalized wave data of a song. In this project, we propose an audio system 
that separates vocal data from instrumental data and uses MIDI data to replace the wave data of the 
instruments. Because only 11K Hz sampling rate for vocal and few MIDI data for music are 
required, the total data size of a song can be reduced significantly compared with that of 
conventional music CDs, while maintaining satisfactory sound quality.  
In many multimedia applications, especially on networks, less data quantity of multimedia can 
help in improving the efficiency of data transmission and achieving the real-time performance. 
Therefore, this proposed MIDI-based audio system, using MIDI data to replace musical wave data, 
are indeed suitable for the efficient data transmission of multimedia on Internets or wireless 
networks. 
 
Keywords: MIDI-based audio system, MIDI sequencer, Tone generator, Wave data, 
Multimedia data. 
 3
bytes have their MSB set to 0, and represent the number and range corresponding to the operation 
of the status byte, such as note-on velocity. 
A standard MIDI file is composed of many MIDI messages, which includes a single header 
chunk followed by one or more track chunks, and each chunk consists of a type, a length, and data 
[2]. The data part of the header chunk specifies the format, number of tracks, and timing for the 
MIDI file. The track chunks contain multiple MIDI messages and their corresponding delta-time 
bytes, where the delta-time placed in front of each message is a variable-length quantity and 
represents the amount of time before the following message [4]. The unit of the delta-time is clock 
ticks that depend on the tempo specified in the header chunk of the MIDI file. 
 
2.2. Sequencer and Synthesizer 
For a MIDI-based audio player, the sequencer and the synthesizer are two important 
components. The sequencer is used as a MIDI file interpreter, and it can generally manage multiple 
MIDI data streams, or tracks. As MIDI data are read, the sequencer decides how long to play each 
note according to the delta-time bytes for each channel voice message in the MIDI file. It also 
knows which instrument should be used to play these notes according to the status bytes for each 
channel voice message. The synthesizer is used to actually produce the various instrument sounds, 
which means the sound quality of MIDI devices is mainly dependent of their synthesizers. Two 
widely used techniques are Frequency Modulation (FM) synthesis and Wavetable synthesis. The 
FM technique can generate various sound waves with different timbres by using a periodic 
modulator signal to modulate the frequency of another carrier signal [8]. An alternate technique, 
Wavetable synthesis [10], uses a memory table to store a large number of sampled sound segments 
(i.e. sound waveforms) that may be looked up and utilized when needed. According to amplitude 
envelope of the attack and sustain sections of various instrument sounds during playback, the 
Wavetable synthesizer will loop the samples in the corresponding segments. This approach is used 
in the majority of professional synthesizers for achieving high sound quality. 
 
3. The Proposed MIDI-based Audio System 
To achieve a small digital data size for audio, we propose an audio system using a MIDI file to 
replace the music wave data file, and the recording and playing of audio need two processing parts 
including MIDI data for instrumental tracks and wave data for vocal track, which differs from 
conventional music CD systems. In this section, the processes of recording and playing in the 
proposed MIDI-based audio system will be described in detail. 
 
3.1. Recording Process 
In the process of recording shown in Fig. 1, the vocal input must be synchronized with the 
instrumental music, but they can separately generate their own data. Firstly, we use a MIDI 
instrument to play the desired music. The corresponding MIDI data will be generated immediately. 
The generated MIDI data is significantly smaller than the conventional music wave data. While the 
music is played, the vocal input of singer also synchronously generates wave data by using the 
interprets the recorded MIDI data as a sequence of MIDI messages at the appropriate time. The tone 
generator, using FM modulation or Wavetable techniques [4], will synthesize various tones (i.e. 
music wave data) corresponding to different MIDI messages, such as piano, saxophone, trumpet, 
etc. There exists a small delay associated with the MIDI sequencer and tone generator, and thus the 
vocal wave data must be delayed to wait for the music wave data generated from the tone generator. 
To reduce this delay time, a fast DSP processor can be used to handle the processing of the MIDI 
data in the circuit implementation [6]. In the final step of the process, two D/A converters are used 
to separately convert the two sets of wave data into the actual analog signals, and the two analog 
signals will be mixed to recover the original audio song. However, the standard MIDI data do not 
specify in detail how the sound of each instrument is produced [8], which means different 
synthesizers may generate a different sound performance for the same instruments. In the future, if 
accurate definitions about sound performance are added to the MIDI data, any MIDI synthesizer 
can produce the same sound as that of the original music. 
 
 
 
MIDI 
Sequencer
MIDI  
Data
Tone 
Generator DAC 
Audio 
DAC
Vocal 
Wave  
Data 
Delay
 
Fig. 3.  Block diagram of playing process 
 
 
4. Simulation Results 
Before implementing the circuit of the proposed audio system, we used a PC (Personal 
Computer) sound effect card and a MIDI instrument to emulate this system. In the recording 
process (shown in Fig. 4), we input a MIDI music file into the MIDI instrument. The output of the 
MIDI instrument is connected to the line input of the sound effect card of PC1. The vocal input is 
connected to the microphone inputs of the sound effect cards of PC1 and PC2. The sound effect 
card of PC1 has a sampling frequency of 44 k Hz and a data resolution of 16 bits and 2-channels. 
The sound effect card of PC2 has a sampling frequency of 11 k Hz and a data resolution of 16 bits 
and 2-channels. When the MIDI instrument plays the music, the vocal is concurrently input into the 
two sound effect cards. On PC1, the complete wave file, including vocal and music, will be 
produced. At the same time, another wave file consisting only of the vocal is also produced on PC2. 
After recording, we combine the vocal wave file from PC2 with the original MIDI file into a 
“MIDI-wave” file. In the playing process, we play the complete wave file and the MIDI-wave file 
on a PC. By using the multimedia application software, “Spectrum Analyzer Pro Live”, we compare 
the spectrum of the output sound using the MIDI-wave file with that of the original sound using the 
complete wave file.  
 5
 Fig. 5  Spectrum of Song 1 using wave data file 
 
 
Fig. 6  Spectrum of Song 1 using MIDI-wave data file 
 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Song 1 Song 2 Song 3 Song 4
Error (dB)
 
Fig. 7.  Spectrum error for different songs 
 
5. Multimedia Application 
The proposed MIDI-based audio system can be applied as an on-line audio system, and the 
system architecture shown in Fig. 8 is composed of a music center and multiple subscribers. The 
music center (sever) produces and provides a huge MIDI-based song database, where these songs 
are recorded by using PCM technique with an 11 k sampling rate for the vocal, and using the MIDI 
data to replace the music wave data. By the computer-human interaction on consumer (client) node, 
user can directly select the desired songs on demand through the Internet. 
 7
 9
Feb. 1990. 
[5] K. N. Kim, U. P. Chong, and J. H. Choi, “Conversion from CD-DA format to MIDI format 
maintaining a sound quality,” Proc. 3rd Russian-Korean International Symposium on Science 
and Technology, vol. 1, pp. 300-303, June 1999. 
[6] S. Yim, Y. Ding, and E. B. George, “A real-time MIDI music synthesis system using sinusoidal 
modeling on a TI TMS320C32 digital signal processor,” IEEE 1st Workshop on Multimedia 
Signal Processing, pp. 469-474, June 1997. 
[7] N. J. Sieger and A. H. Tewfik, “Audio coding for conversion to MIDI,” IEEE 1st Workshop on 
Multimedia Signal Processing, pp. 101-106, June 1997. 
[8] A. S. Moussa and T. P. Baker, “A dynamic microtunable MIDI system: problems and solutions,” 
Proc. 5th Biannual World Automation Congress, pp. 339-344, June 2002. 
[9] T. Modegi and S-I. Iisaku, “Proposals of MIDI coding and its application for audio authoring,” 
Proc. IEEE International Conference on Multimedia Computing and Systems, pp. 305-314, 
June/July 1998. 
[10] H. C. Chen and C. C. Hsu, “MIDI-based audio recording/playing system with smaller data 
quantity,” Proc. 10th International Conference on Distributed Multimedia Systems, pp. 491-494, 
Aug. 2004. 
[11] Hu Ning, R. B.Dannenberg, G.. Tzanetakis, “Polyphonic audio matching and alignment for 
music retrieval,” 2003 IEEE Workshop on Applications of Signal Processing to Audio and 
Acoustics, pp. 185-188, Oct. 2003. 
[12] K. Iwahama, Y. Hijikata, S. Nishida, “Content-based filtering system for music data,” 2004 
International Symposium on Applications and the Internet Workshops, pp. 480-487, Jan. 2004. 
[13] M. Wang; N. Zhang; H. Zhu, “User-adaptive music emotion recognition,” 2004 7th 
International Conference on Signal Processing, vol. 2, pp.1352-1355, Sept. 2004. 
[14] Z. Yongwei, M. Kankanhalli, “Key-based melody segmentation for popular songs,” Proc. the 
17th International Conference on Pattern Recognition, vol. 3, pp. 862 -865, Aug. 2004. 
[15] Z. Yongwei, M.S. Kankanhalli, G.. Sheng, “Music key detection for musical audio,” Proc. The 
11th International Multimedia Modeling Conference, pp. 30-37, Jan. 2005. 
[16] W. A. Sethares, R. D. Morris, J. C. Sethares, “Beat tracking of musical performances using 
low-level audio features,” IEEE Transactions on Speech and Audio Processing, vol. 13, no. 2, 
pp. 275-285, March 2005. 
[17] M. Cardle, L. Barthe, S. Brooks, P. Robinson, “Music-driven motion editing: local motion 
transformations guided by music analysis,” Proc. The 20th Eurographics UK Conference, pp. 
38-44, June 2002. 
[18] T. Modegi, S. Iisaku, “Application of MIDI technique for medical audio signal coding,” Proc. 
The 19th Annual International Conference of the IEEE Engineering in Medicine and Biology 
society, vol. 4, pp. 1417-1420, Nov. 1997. 
 11
可供推廣之研發成果資料表 
5 可申請專利  □ 可技術移轉                                      日期：96年8月6日 
國科會補助計畫 
計畫名稱：用以減少多媒體資料量之MIDI為基礎的音頻系統 
計畫主持人：陳信全         
計畫編號：NSC 95-2221-E-129-018     學門領域：資訊 
技術/創作名稱 以MIDI為基礎的音頻系統 
發明人/創作人 陳信全 
中文： 
嘗試提出一種樂曲與歌聲分離之音頻系統，其中樂曲部分可由
MIDI 資料取代樂器聲音其數位化取樣波型資料；而歌聲部分則僅
需以 11K Hz取樣頻率加以數位化即可。而於播放時，則分別處理
MIDI 資料與人聲音波資料的轉換，最後再將此兩轉換後信號混音
而成。因此，相較於傳統音樂 CD，整首歌曲所需的資料量將可明
顯地減少，而且尚能維持令人滿意的音質。 
技術說明 英文： 
We propose an audio system that separates vocal data from 
instrumental data and uses MIDI data to replace the wave data of the 
instruments. Because only 11K Hz sampling rate for vocal and few 
MIDI data for music are required. In playing process, the MIDI data 
and vocal wave data are separately converted, and then two converted 
signals are finally mixed together as a song. Therefore, the total data 
size of a song can be reduced significantly compared with that of
conventional music CDs, while maintaining satisfactory sound quality.
可利用之產業 
及 
可開發之產品 
可利用於多媒體應用、影音系統等產業，並可開發如線上 KTV 系
統之產品。 
技術特點 樂曲與人聲分離處理，分別以較少的MIDI資料儲存樂曲，而人聲錄製亦可採用較低的 11K Hz取樣頻率。 
推廣及運用的價值在網路多媒體應用中，可獲致較少的多媒體資料量，將有助於改善資料傳輸的效率，並且可獲得較好的即時效果。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研
發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
