Periodic Step-Size Adaptation for Single-Pass On-line Learning 
 
中文摘要 
我們已建立此二階梯度探底的方法(2SGD)，此方法將可達成在單向的經驗中最佳的通過訓練範本。然
而，2SGD要求計算出在損失函數中，Hessian矩陣的反矩陣，這將會因成本太高而無法實現。本論文提
出了一個稱為Periodic Step-size Adaptation (PSA)的方法，PSA 能逼近映象函數的Jacobian 矩陣及探尋
Jacobian 與Hessian 間的線性關係以逼近Hessian，在廣泛而多樣的模型和任務中，獲得近似最佳化的
結果。 
 
關鍵字：隨機條件場、迴旋類神經網路、逐次學習、序列連續標註、隨機梯度探底法 
 
Abstract 
It has been established that the second-order stochastic gradient descent (2SGD) method can potentially 
achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the 
training examples. However, 2SGD requires computing the inverse of the Hessian matrix of the loss function, 
which is prohibitively expensive. This paper presents Periodic Step-size Adaptation (PSA), which 
approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian 
and Hessian to approximate the Hessian periodically and achieve near-optimal results in experiments on a 
wide variety of models and tasks. 
 
KEYWORDS: Conditional Random Fields, Convolutional Neural Networks, On-Line Learning, Sequence 
Labeling, Stochastic Gradient Descent. 
periodically. The per-iteration time-complexity of PSA is linear to the number of nonzero dimen-
sions of the data. We analyze the accuracy of the approximation and derive the asymptotic rate of
convergence for PSA. Experimental results show that for a wide variety of models and tasks, PSA is
always very close to empirical optimum in a single-pass. Experimental results also show that PSA
can run much faster compared to state-of-the-art algorithms.
2 Aitken’s Acceleration
Let w ∈ ℝ푑 be a 푑-dimensional weight vector of a model. A machine learning problem can be
formulated as a fixed-point iteration that solves the equation w = ℳ(w), where ℳ is a mapping
ℳ : ℝ푑 → ℝ푑, until w∗ = ℳ(w∗). Assume that the mapping ℳ is differentiable. Then we
can apply Aitken’s acceleration, which attempts to extrapolate to the local optimum in one step, to
accelerate the convergence of the mapping:
w∗ = w(푡) + (I− J)−1(ℳ(w(푡))−w(푡)), (1)
where J :=ℳ′(w∗) is the Jacobian of the mapping ℳ at w∗. When 휆푖 := eig(J) ∈ (−1, 1), the
mappingℳ is guaranteed to converge. That is, when 푡→∞, w(푡) → w∗.
It is usually difficult to compute J for even a simple machine learning model. To alleviate this issue,
we can approximate J with the estimates of its 푖-th eigenvalue 휆푖 by
훾
(푡)
푖 :=
ℳ(w(푡))푖 −w(푡)푖
w
(푡)
푖 −w(푡−1)푖
, ∀푖, (2)
and extrapolate at each dimension 푖 by:
w
(푡+1)
푖 = w
(푡)
푖 + (1− 훾(푡)푖 )−1(ℳ(w(푡))푖 −w(푡)푖 ) . (3)
In practice, Aitken’s acceleration alternates a step for preparing 훾(푡) and a step for the extrapolation.
That is, when 푡 is an even number,ℳ is used to obtain w(푡+1), while when 푡 is an odd number, the
extrapolation (3) is used. A benefit of the above approximation is that the cost for performing an
extrapolation is 푂(푑), linear in terms of the dimension.
3 Periodic Step-Size Adaptation
When ℳ is a gradient descent update rule, that is, ℳ(w) ← w − 휂g(w;D), where 휂 is a scalar
step size, D is the entire set of training examples, and g(w;D) is the gradient of a loss function to
be minimized, Aitken’s acceleration is equivalent to Newton’s method, because
J =ℳ′(w) = I− 휂H(w;D), (4)
(I− J)−1 = 1
휂
H(w;D)−1, and ℳ(w)−w = w − 휂g(w;D)−w = −휂g(w;D),
where H(w;D) = 푔′(w;D), the Hessian matrix of the loss function, and the extrapolation given in
(1) becomes
w = w + (I− J)−1(ℳ(w)−w) = w − 1
휂
H−1휂g = w −H−1g.
In this case, Aitken’s acceleration enjoys the same local quadratic convergence as Newton’s method.
This can also be extended to a SGD update rule: w(푡+1) ← w(푡) − 휂 ∙ g(w(푡);B(푡)), where the
mini-batch size B ⊆ D, ∣B∣ ≪ ∣D∣, is a randomly selected small subset of D. A genuine on-line
learner usually has ∣B∣ = 1. We consider a positive vector-valued step-size 휂 ∈ ℝ푑+ and “∙” denotes
component-wise (Hadamard) product of two vectors. Again, by exploiting (4), since
eig(I− diag(휂)H) = eig(ℳ′) = eig(J) ≈ 훾,
where 훾 is an estimated eigenvalue of J as given in (2), whenH is a symmetric matrix, its eigenvalue
is given by
eig(J) = 1− 휂푖eig(H)⇒ eig(H) = 1− eig(J)
휂푖
.
2
Algorithm 1 The PSA Algorithm
1: Given: 훼, 훽, 휅 < 1 and 푏
2: Initialize 휃(0) and 휂(0); 푡← 0; 푚← 훼+훽
훼−훽휅 and 푛← 2(1−훼)훼−훽 휅 ⊳ Equation (10)
3: repeat
4: Choose a small batch B(푡) uniformly at random from the set of training examples D
5: update 휃(푡+1) ← 휃(푡) − 휂 ∙ g(휃(푡);B(푡)) ⊳ SGD update
6: if (푡+ 1) mod 2푏 = 0 then ⊳ Update 휂
7: update 훾¯푏푖 ← 휃
(푡+2푏)
푖
−휃
(푡+푏)
푖
휃
(푡+푏)
푖
−휃
(푡)
푖
⊳ Equation (6)
8: For all 푖, update 푢푖 ← sgn(훾¯푏푖 )min(∣훾¯푏푖 ∣, 휅) ⊳ Equation (7)
9: For all 푖, update 푣푖 ← 푚+푢푖푚+휅+푛 ⊳ Equation (9)
10: update 휂(푡+1) ← v ∙ 휂(푡) ⊳ Equation (8)
11: else
12: 휂(푡+1) ← 휂(푡)
13: end if
14: 푡← 푡+ 1
15: until Convergence
4 Analysis of PSA
We analyze the accuracy of 훾(푡)푖 as an eigenvalue estimate as follows. Let eigen decomposition
J = QΛQ−1 and u푖 be column vectors of Q and v푇푖 be row vectors of Q−1. Then we have
J푡 =
푑∑
푗=1
휆푡푗u푗v
푇
푗 ,
where 휆푗 is the 푗-th eigenvalue of J. By applying Taylor’s expansion toℳ, we have
w(푡) −w∗ ≈ J푡(w(0) −w∗)
w(푡−1) −w∗ ≈ J푡−1(w(0) −w∗)
⇒ Δ(푡) = w(푡) −w(푡−1) ≈ J푡J−1(J− I)(w(0) −w∗)
⇒ Δ(푡+1) = w(푡+1) −w(푡) ≈
푑∑
푗=1
휆푗휆
푡
푗u푗v
푇
푗 J
−1(J− I)(w(0) −w∗)
Now let
휔푖푗 := e
푇
푖 u푗v
푇
푗 J
−1(J− I)(w(0) −w∗),
where e푖 is the 푖-th column of I. Let Δ푖 be the 푖-th element of Δ and 휆푗푖 be the largest eigenvalue
of J such that 휔푖푗 ∕= 0. Then
훾푖 ≡ Δ
(푡+1)
푖
Δ
(푡)
푖
=
∑푑
푗=1 휆
푡+1
푗 휔푖푗∑푑
푗=1 휆
푡
푗휔푖푗
=
휆푗푖 +
∑
푗 ∕=푗푖
(휆푗/휆푗푖)
푡휆푗휔푖푗/휔푖푗푖
1 +
∑
푗 ∕=푗푖
(휆푗/휆푗푖)
푡휔푖푗/휔푖푗푖
.
Therefore, we can conclude that
∙ 훾푖 → 휆푗푖 as 푡 → ∞ because ∀푖, if 휔푖푗 ∕= 0 then 휆푗/휆푗푖 ≤ 1. 휆푗푖 ≡ 푅푖 is the 푖-th
componentwise rate of convergence.
∙ 훾푖 = 휆푖 if J is a diagonal matrix. In this case, our approximation is exact. This happens
when there are high percentages of missing data for a Bayesian network model trained
by EM [8] and when features are uncorrelated for training a conditional random field
model [9].
∙ 훾푖 is the average of eigenvalues weighted by 휆푡푗휔푖푗 . Since 휔푖푗 is usually the largest when
푖 = 푗, we have 훾푖 ≈ 휆푖.
4
5 Experimental Results
Table 1 shows the tasks chosen for our comparison. The tasks for CRF have been used in compe-
titions and the performance was measured by F-score. Weight for CRF reported here is Number
of features provided by CRF++. Target provides the empirical optimal performance achieved
by batch learners. If PSA accurately approximates 2SGD, then its single-pass performance should
be very close to Target. The target F-score for BioNLP/NLPBA is not >85% as reported in [1]
because it was due to a bug that included true labels as a feature 1.
Table 1: Tasks for the experiments.
Task Model Training Test Tag/Class Weight Target
Base NP CRF 8936 2012 3 1015662 94.0% [10]
Chunking CRF 8936 2012 23 7448606 93.6% [11]
BioNLP/NLPBA CRF 18546 3856 11 5977675 70.0% [12]
BioCreative 2 CRF 15000 5000 3 10242972 86.5% [13]
LS FD LSVM 2734900 2734900 2 900 3.26%
LS OCR LSVM 1750000 1750000 2 1156 23.94%
MNIST [14] CNN 60000 10000 10 134066 0.99%
5.1 Conditional Random Field
We compared PSA with plain SGD and SMD [1] to evaluate PSA’s performance for training
conditional random fields (CRF). We implemented PSA by replacing the L-BFGS optimizer in
CRF++ [11]. For SMD, we used the implementation available in the public domain 2. Our SGD
implementation for CRF is from Bottou 3. All the above implementations are revisions of CRF++.
Finally, we ran the original CRF++ with default settings to obtain the performance results of L-
BFGS. We simply used the original parameter settings for SGD and SMD as given in the literature.
For PSA, we used 휅 = 0.9, (훼, 훽) = (0.9999, 0.99), 푏 = 10, and 휂(0)푖 = 0.1, ∀푖. The batch size
is one for all tasks. These parameters were determined by using a small subset from CoNLL 2000
baseNP and we simply used them for all tasks. All of the experiments reported here for CRF were
ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM.
Table 2 compares SGD variants in terms of the execution time and F-scores achieved after processing
the training examples for a single pass. Since the loss function in CRF training is convex, the
convergence results of L-BFGS can be considered as the empirical minimum. The results show that
single-pass F-scores achieved by PSA are about as good as the empirical minima, suggesting that
PSA has effectively approximated Hessian in CRF training.
Fig. 1 shows the learning curves in terms of the CPU time. Though as expected, plain SGD is the
fastest, it is remarkable that PSA is faster than SMD for all tasks. SMD is supposed to have an edge
here because the mini-batch size for SMD was set to 6 or 8, as specified in [1], while PSA used one
for all tasks. But PSA is still faster than SMD partly because PSA can take advantage of the sparsity
trick as plain SGD [15].
5.2 Linear SVM
We also evaluated PSA’s single-pass performance for training linear SVM. It is straightforward to
apply PSA as a primal optimizer for linear SVM. We used two very large data sets: FD (face detec-
tion) and OCR (see Table 1), from the Pascal large-scale learning challenge in 2008 and compared
the performance of PSA with the state-of-the-art linear SVM solvers: Liblinear 1.33 [16], the winner
of the challenge, and SvmSgd, from Bottou’s SGD web site. They have been shown to outperform
many well-known linear SVM solvers, such as SVM-perf [17] and Pegasos [15].
1Thanks to Shing-Kit Chan of the Chinese University of Hong Kong for pointing that out.
2Available under LGPL from the following URL: http://sml.nicta.com.au/code/crfsmd/.
3http://leon.bottou.org/projects/sgd.
6
The parameter settings for PSA are basically the same as those for CRF but with a large period
푏 = 1250 for FD and 500 for OCR. For FD, the worst accuracy by PSA is 94.66% with 푏 between
250 to 2000. For OCR, the worst is 75.20% with 푏 between 100 to 1000, suggesting that PSA is not
very sensitive to parameter settings.
5.3 Convolutional Neural Network
Approximating Hessian is particularly challenging when the loss function is non-convex. We tested
PSA in such a setting by applying PSA to train a large convolutional neural network for the original
10-class MNIST task (see Table 1). We tried to duplicate the implementation of LeNet described in
[18] in C++. Our implementation, referred to as “LeNet-S”, is a simplified variant of LeNet-5. The
differences include that the sub-sampling layers in LeNet-S picks only the upper-left value from a
2× 2 area and abandons the other three. LeNet-S used more maps (50 vs. 16) in the third layer and
less nodes (120 vs. 100) in the fifth layer, due to the difference in the previous sub-sampling layer.
Finally, we did not implement the Gaussian connections in the last layer. We trained LeNet-S by
plain SGD and PSA. The initial 휂 for SGD was 0.7 and decreased by 3 percent per pass. For PSA,
we used 휅 = 0.9, (훼, 훽) = (0.99999, 0.999), 푏 = 10, 휂(0)푖 = 0.5, ∀푖, and the mini-batch size is
one for all tasks. We also adapted a trick given in [19] which advises that step sizes in the lower
layers should be larger than in the higher layer. Following their trick, the initial step sizes for the
first and the third layers were 5 and
√
2.5 times as large as those for the other layers, respectively.
The experiments were ran on an Intel Q6600 Fedora 8 i686 PC with 4G RAM.
Table 4 shows the results. To obtain the empirical optimal error rate of our LeNet-S model, we ran
plain SGD with sufficient passes and obtained 0.99% error rate at convergence, slightly higher than
LeNet-5’s 0.95% [18]. Single-pass performance of PSA with the layer trick is within one percentage
point to the target. Starting from an initial weight closer to the optimum helped improving PSA’s
performance further. We ran SGD 100 passes with randomly selected 10K training examples then
re-started training with PSA using the rest 50K training examples for a single pass. Though PSA did
achieve a better error rate, this is infeasible because it took 4492 seconds to run SGD 100 passes.
Finally, though not directly comparable, we also report the performance of TONGA given in [20] as
a reference. TONGA is a 2SGD method based on natural gradient.
Method (pass) time error Method (pass) time error
SGD (1) 266.77 2.36 PSA w/o layer trick (1) 311.95 2.31
SGD (140) 37336.20 0.99 PSA w/ layer trick (1) 311.00 1.97
TONGA (n/a) 500.00 2.00 PSA re-start (1) 253.72 1.90
Table 4: CPU time in seconds and percentage test error rates for various neural network trainers.
6 Conclusions
It has been shown that given a sufficiently large training set, a single pass of 2SGD generalizes
as well as the empirical optimum. Our results show that PSA provides a practical solution to ac-
complish near optimal performance of 2SGD as predicted theoretically for a variety of large scale
models and tasks with a reasonably low cost per iteration compared to competing 2SGD methods.
The benefit of 2SGD with PSA over plain SGD becomes clearer when the scale of the tasks are in-
creasingly large. For non-convex neural network tasks, since the curvature of the error surface is so
complex, it is still very challenging for an eigenvalue approximation method like PSA. A complete
version of this paper will appear as [21].
References
[1] S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. Accel-
erated training of conditional random fields with stochastic gradient methods. In Proceedings
of the 23rd International Conference on Machine Learning (ICML’06), Pittsburgh, PA, USA,
June 2006.
8
Periodic Step-Size Adaptation in Second-Order Gradient Descent for 
Single-Pass On-line Structured Learning 
 
中文摘要 
在過去已證實二階隨機梯度探底法(SGD)處理完一輪訓練資料後，能夠達到該資料所能提供的最佳
解。但二階SGD 需要計算損耗函數之Hessian 矩陣的反矩陣，這對於大致上會包含非常高維度特徵空
間的結構化預測問題來說，其所需要的計算資源實是過於龐大。本論文提出了一個新的二階SGD 方
法，稱為Periodic Step-size Adaptation (PSA)。PSA 能逼近映象函數的Jacobian 矩陣及探尋Jacobian 與
Hessian 間的線性關係以逼近Hessian，在逐次學習的環境下這已被證明較直接逼近Hessian 來得更簡單
及有效率。我們於各式的問題中使用多種模型驗證PSA，包括使用隨機條件場的大尺度序列連續標註。
實驗結果顯示僅處理一輪訓練資料的PSA 其結果非常近似該資料所能提供的最佳解。 
 
關鍵字：隨機條件場、迴旋類神經網路、逐次學習、序列連續標註、隨機梯度探底法 
 
ABSTRACT 
Previously, it has been established that the second-order stochastic gradient descent (SGD) method can 
potentially achieve generalization performance as well as empirical optimum in a single pass through the 
training examples. However, second-order SGD requires computing the inverse of the Hessian matrix of the 
loss function, which is prohibitively expensive for structured prediction problems that usually involve a very 
high dimensional feature space. This paper presents a new second-order SGD method, called Periodic 
Step-size Adaptation (PSA). PSA approximates the Jacobian matrix of the mapping function and explores a 
linear relation between the Jacobian and Hessian to approximate the Hessian, which is proved to be simpler 
and more effective than directly approximating Hessian in an on-line setting. We tested PSA on a wide variety 
of models and tasks, including large scale sequence labeling tasks using conditional random fields and large 
scale classification tasks using linear support vector machines and convolutional neural networks. 
Experimental results show that single-pass performance of PSA is always very close to empirical optimum. 
 
KEYWORDS: Conditional Random Fields, Convolutional Neural Networks, On-Line Learning, Sequence 
Labeling, Stochastic Gradient Descent. 
 
Mach Learn
1 Introduction
The problem of large scale learning (Bottou and LeCun 2004, 2005; Bottou et al. 2007;
Bottou and Bousquet 2008) arises partly due to the increasing of the digital data avail-
able for data mining and partly due to the need of many unsolved artificial intelli-
gence challenges: natural language understanding, speech recognition, computer vision
and robotics. In the latter case, structured prediction (Taskar 2005; Taskar et al. 2005;
Tsochantaridis et al. 2005) is more adequate to deal with the complexity of these problems
than well-studied binary classification methods, but then either a large number of exam-
ples or a set of very high dimensional training examples is required to achieve satisfactory
generalization performance.
In this paper, we report the results of our investigation of how to apply a classical on-
line learning method to solve the large scale structured prediction problem. Given a set of
new training examples, an on-line learner can update its model without processing previ-
ously used examples while improving its performance. In contrast, off-line or batch learners
must combine previous and new data and start the learning all over again. Otherwise, the
performance may suffer.
The advantage of on-line learning for large scale structured prediction is illustrated in
Fig. 1. Assume that everyday, a fixed number of training examples become available to the
learner. Also assume that the time complexity for the off-line learner is no better than O(n),
where n is the data size. As days go by, the training examples will accumulate and the time
required for the off-line learner to complete the learning task will eventually exceed the
time required for the on-line learner to process those additional training examples. In fact,
many well-designed on-line learning algorithms converge more rapidly than their off-line
counterparts (see, for example, Collins et al. 2008; Bordes et al. 2007; Bottou et al. 2007;
LeRoux et al. 2008) given the training examples of the same size.
For on-line learning to achieve the feat illustrated in Fig. 1 and solve large scale structured
prediction problems in practical applications, an on-line learning algorithm should satisfy
some desiderata. One of them is O(d) or better per-iteration time complexity, where d is
the dimension of the weights, so that very high dimensional data can be handled. Another
is minimum batch size requirement. Some on-line learning algorithms need 100 or more
training examples in each learning-iteration to cover potentially exponentially large output
spaces. This places an unnecessary restriction to their applicability. Also, the number of
hyper-parameters required to be tuned must be minimized.
Fig. 1 Advantage of on-line
learning
Mach Learn
(a) Gradient Descent (GD)
1: Initialize θ(0)
2: repeat
3: θ(t+1) ← θ(t) − ηg(θ (t);D)
4: Update η
5: t ← t + 1
6: until Convergence
(b) Stochastic Gradient Descent (SGD)
1: Initialize θ(0)
2: repeat
3: Choose B(t) uniformly at random from D
4: θ(t+1) ← θ(t) − ηg(θ (t);B(t))
5: Update η
6: t ← t + 1
7: until Convergence
Fig. 2 (a) Batch (Off-Line) Gradient Descent Algorithm and (b) Stochastic (On-Line) Gradient Descent
Algorithm
small batches:
L(θ;D) ≈
∑
x∈D
L(θ;x) =
|D|
|B|∑
t=1
L(θ;B(t)). (2)
SGD has been an important algorithm for machine learning because it was applied to
train back-propagating neural networks. Recently, Vishwanathan et al. (2006a) showed that
by cleverly adjusting the step size in SGD, a large scale CRF model can be trained by
scanning the training examples about 10 passes, while the best performing off-line algorithm
requires several hundred passes. They also showed that their step size adjustment method,
SMD, is applicable to large margin kernel methods for structured prediction (Vishwanathan
et al. 2006b). Although SMD still takes longer CPU time to complete than the best off-
line algorithm, their work demonstrated that on-line learning is promising for large scale
structured prediction.
More recently, Bottou and Bousquet (2008) showed that well-tuned SGD can be very
efficient for both SVM and CRF. In the case of SVMs with linear kernel, they showed that
for a very large scale problem, widely used SVM packages, including LIBSVM (Chang
and Lin 2001) and SVMlight (Joachims 1998, 2006), took hours to converge, but SGD only
took less than 10 seconds. In fact, about ten years ago, LeCun et al. (1998b) and LeCun et al.
(1999) already showed that SGD is feasible for training multiple-layered hierarchical models
for large scale hand-written digit recognition and object recognition tasks. Their results lead
the machine learning community re-consider SGD as a useful optimization algorithm and
sheds new light on large scale structured prediction.
However, plain SGD and many of its variants still require scanning the training examples
many passes to achieve the same level of generalization performance as their off-line coun-
terparts. In fact, by appropriately adjusting the step size, a single pass through a large set of
training examples is sufficient for SGD to reach the empirical optimum. To see why this is
the case, consider solving (1) with Newton’s method:
θ(t+1) = θ(t) − H−1(θ (t))∇L(θ(t);D), (3)
where H−1 is the inverse of the Hessian matrix of the objective function, defined by
H(θ) := ∂
2
∂θ∂θ
L(θ;D).
Replacing η in step 3 of the algorithm in Fig. 2(a) with H−1 yields exactly the same equation.
Based on Newton’s method, Benveniste et al. (1990) showed that updating the step size with
Mach Learn
3 Approximating Jacobian
This section reviews an efficient method to estimate the eigenvalues of the Jacobian in a
fixed-point iteration mapping and an off-line learning method derived from this estimation
method.
3.1 Jacobian of fixed-point mappings
Let θ be a d-dimensional vector in space Rd and M be a mapping M : Rd → Rd . The fixed-
point iteration method solves the equation of the form θ = M(θ) by iteratively substituting
the input of M with the output of M in the previous iteration:
θ(t+1) = M(θ (t)), θ (t+2) = M(θ (t+1)), . . . .
The equation is solved when θ(t) converges to θ∗ with θ∗ = M(θ∗). Many optimization
algorithms, such as the EM algorithm (Dempster et al. 1977), can be formulated as a fixed-
point iteration method.
Suppose that we apply a fixed-point iteration method from θ(t) in the neighborhood of θ∗
and the iteration converges at θ∗. Also suppose that the mapping M is differentiable. Then
we can apply a linear Taylor expansion of M around θ∗ so that
θ(t+1) = M(θ (t)) ≈ θ∗ + J(θ (t) − θ∗), (5)
where J abbreviates M′(θ∗), the Jacobian of the mapping M at θ∗. When λi := eig(J) ∈
(−1,1), the mapping is guaranteed to converge. If λi is positive, M converges monotoni-
cally along the i-th dimension; otherwise, M converges in an oscillating manner (Burden
and Faires 1988), as illustrated in Fig. 3.
From (5), we have
θ(t+1) ≈ θ∗ + J(θ (t) − θ∗),
θ (t) ≈ θ∗ + J(θ (t−1) − θ∗)
⇒ θ(t+1) − θ(t) ≈ J(θ (t) − θ(t−1)).
Fig. 3 In a univariate fixed-point iteration, the solution of θ = M(θ) is at the intersection of y = M(θ)
(solid black curve) and y = θ (black diagonal line). Dash-dot line tracks the progress of the convergence.
When 0 < M′ < 1, the fixed-point iteration converges monotonically (left). Otherwise, when −1 < M′ < 0,
we have oscillating convergence (right)
Mach Learn
θ(t−1) − θ∗ ≈ Jt−1(θ (0) − θ∗)
⇒ Δ(t) = θ(t) − θ(t−1) ≈ JtJ−1(J − I)(θ (0) − θ∗)
=
d∑
j=1
λtjuj v
T
j J−1(J − I)(θ (0) − θ∗)
⇒ Δ(t+1) = θ(t+1) − θ(t) ≈
d∑
j=1
λjλ
t
j uj v
T
j J−1(J − I)(θ (0) − θ∗).
Now let
wij := eTi uj vTj J−1(J − I)(θ (0) − θ∗),
where ei is the i-th column of I. Let Δi be the i-th element of Δ and λji be the largest
eigenvalue of J such that wij = 0. Then
γi ≡ Δ
(t+1)
i
Δ
(t)
i
=
∑d
j=1 λ
t+1
j wij∑d
j=1 λ
t
jwij
= λji +
∑
j =ji (λj /λji )
tλjwij /wiji
1 + ∑j =ji (λj /λji )twij /wiji
.
Therefore, we can conclude that
• γi → λji as t → ∞ because ∀i, if wij = 0 then λj/λji ≤ 1. λji ≡ Ri is the i-th compo-
nentwise rate of convergence (Meng and Rubin 1994).
• γi = λi if J is a diagonal matrix. In this case, our approximation is exact. This happens
when there are high percentages of missing data for a Bayesian network model trained by
EM (Hsu et al. 2006) and when features are uncorrelated for training a CRF model (Huang
et al. 2009).
• γi is the average of eigenvalues weighted by λtjwij . Since wij is usually the largest when
i = j , we have γi ≈ λi .
3.2 Aitken’s acceleration
The estimates γ and γi have been applied in the framework of Aitken’s acceleration to
speed up the convergence of the EM algorithm (Hesterberg 2005; Huang et al. 2005; Hsu et
al. 2006), suggesting that both γ and γi can be sufficiently accurate to allow for substantial
speedup for the convergence of EM.
Let J be the Jacobian matrix of the fixed-point mapping M that we want to accelerate.
The multivariate Aitken’s acceleration is given by (McLachlan and Krishnan 1997):
θ(t+1) = θ(t) + (I − J)−1(M(θ (t)) − θ(t)), (8)
which is derived by successively applying (5) as follows:
θ∗ ≈ θ(t) +
∞∑
h=0
(θ (t+h+1) − θ(t+h))
Mach Learn
Gaussian prior (Chen and Rosenfeld 1999) to avoid overfitting. Let L(θ;D) be the log-
likelihood of D given θ . The penalized negative log-likelihood function L(θ;D) is
L(θ;D) = −L(θ;D) +
∑
i
(θi − μ)2
2σ 2
+ const. (11)
Usually, μ is assigned to zero. Clearly, the objective function of CRF training satisfies (2)
required for SGD to be applicable. The gradient of L along the direction of θi is
∇i L(θ;D) = Efi − E˜fi + θi − μ
σ 2
, (12)
where E˜fi and Efi are empirical and model expectation of fi , respectively.
To apply the triple jump extrapolation to the training of CRF, recall that the gradient for
CRF is given in (12). Solving ∇i L(θ;D) = 0 yields the penalized GIS (PGIS) mapping as
follows:
θi = θi + 1
S
log
E˜fi
Efi + θi−μσ 2
, (13)
where 1
S
is an arbitrary constant in (0,1) as the learning rate. Assigning S := max(x,y)∈D ×∑
i fi(x, y), the maximum number of feature occurrences in a training sequence, we obtain
a new update rule, which is quite similar to the original GIS given in Lafferty et al. (2001).
Assigning M(θ) to be the RHS for (13) and applying the component-wise triple jump ex-
trapolation described in (9), we have the CTJPGIS algorithm for training CRF.
We implemented CTJPGIS by replacing the L-BFGS optimization part in CRF++ (Kudo
2006).1 We also ran CRF++ with default settings to obtain the performance results of L-
BFGS. We used a tight termination condition
∣∣∣∣
L(θ (t−1)) − L(θ (t))
L(θ (t))
∣∣∣∣ < 10
−7
for all methods compared in this experiment, including L-BFGS, to ensure a fair comparison.
The experiment was run on a Fedora 7 x86-64 Linux machine with AMD Athlon 64 X2
3800+ CPU and 4 GB RAM. Table 12 shows the tasks chosen for our comparison in this
paper. The top four rows are sequence labeling tasks solved by CRF. These tasks have been
Table 1 Tasks for the experiments on sequence labeling
Task Training Test Tag Weight Target Reference
Base NP 8936 2012 3 1015662 94.0 (Sha and Pereira 2003)
Chunking 8936 2012 23 7448606 93.6 (Kudo 2006)
BioNLP/NLPBA 18546 3856 11 5977675 70.0 (Settles 2004)
BioCreative 2 15000 5000 3 10242972 86.5 (Hsu et al. 2008)
1Available under LGPL from the following URL: http://crfpp.sourceforge.net/.
2The target F-score for BioNLP/NLPBA is not >85%, as reported in Vishwanathan et al. (2006a), because it
was due to a bug that included true labels as a feature, according to the author.
Mach Learn
From (4), the optimal step size is one that asymptotically approaches to H−1, the inverse
of the Hessian matrix of L(θ∗;D). To avoid actually evaluating H−1, we can approximate
H−1 with its eigenvalues. Considering a SGD iterate M(θ) = θ − η • gB(θ), where gB is
now re-written as a stochastic function depending only on θ . Clearly, M is a fixed-point
iteration mapping, though a stochastic one, strictly speaking. Taking partial derivative of M
with respect to θ , we have
J = M′ = I − diag(η)H. (15)
By exploiting this linear relation between Jacobian and Hessian, we can approximate the
inverse of Hessian by approximating Jacobian. Since
eig(I − diag(η)H) = eig(M′) = eig(J) ≈ γ,
where γ is an estimated eigenvalue of J as given in (7). When H is a symmetric matrix, its
eigenvalue is given by
eig(J) = 1 − ηi eig(H)
⇒ eig(H) = 1 − eig(J)
ηi
.
Therefore,
eig(H−1) = ηi
1 − eig(J) ≈
ηi
1 − γi , (16)
which implies that we can update the step size component-wise by
η
(t+1)
i ∝
η
(t)
i
1 − γ (t)i
. (17)
If the mapping M converges, eig(J) ∈ (−1,1) and we can guarantee that the estimated
eig(H−1) > 0, which is required for the single pass result of SGD to hold (Bottou and LeCun
2005). A complete update rule for the step size will also involve a decay factor to ensure
that the step size will approach zero.
4.2 Stability consideration
Due to the stochastic nature of small batch mapping, it is unlikely that we can obtain a
reliable eigenvalue estimation at each single iteration. To increase stationary of the mapping,
we take advantage of the law of large numbers and aggregate consecutive SGD mappings
into a new mapping
Mb = M(M(. . . M(θ) . . .))︸ ︷︷ ︸
b
.
Assume that the step size η is sufficiently small such that
‖g(θ (t);B(t)) − g(θ (t−1);B(t))‖  ‖g(θ (t);B(t))‖.
Then at iteration t , applying SGD with a fixed step size η for b times yields
Mach Learn
Since −κ ≤ ui ≤ κ , we have vi = α when ui = κ and vi = β when ui = −κ . Solving these
equations yields:
m = α + β
α − β κ and n =
2(1 − α)
α − β κ. (22)
For example, if we want to set α = 0.9999 and β = 0.99, then m and n will be 201κ and
0.0202κ , respectively. Setting 0 < β < α ≤ 1 ensures that the step size is decreasing and
approaches zero so that its convergence can be guaranteed (Benveniste et al. 1990; Spall
2003).
We informally describe the behavior of our step size update rule from the view point of
fixed-point iteration. We note that the step size will always be reduced after every update
in our setting. But according to (16), when the estimated eigenvalue γ¯ bi is positive, SGD
converges monotonically to the optimum. In this case, vi will be large, or the reduction will
be less, to maintain a large step size along the i-th dimension. On the other hand, when γ¯ bi
is negative, the convergence is oscillating and vi will be small to reduce the step size further.
Figure 3 illustrates different types of convergence dictated by the sign of the eigenvalue. The
scale of the reduction is controlled to be proportional to the estimated eigenvalues of H−1 in
an attempt to achieve theoretical single-pass performance.
In practice, we found that assigning α, β and κ to values within the following ranges is
sufficient to produce adequate convergence performance:
• α : [0.999,0.9999],
• β : [0.96,0.99],
• κ : [0.9,0.99].
That leaves b as the only additional parameter that requires careful tuning. In Sect. 5.1.4, we
will report an empirical study of the impact of b on the convergence and provide a practical
guideline to specify these parameters.
4.3 The PSA algorithm
Algorithm 1 shows the PSA algorithm. In a nutshell, PSA applies SGD with a fixed step
size and periodically updates the step size by approximating Jacobian of the aggregated
mapping. The complexity per iteration is O(d
b
) because the cost of eigenvalue estimation
given in (18) is 2d and it is required for every 2b iterations. For models with a regularization
term, such as CRF, we can apply a technique that represents a weight vector as the product
of a scalar and a vector to further reduce the per iteration cost (see, Bottou and Bousquet
2008). We will discuss this in details in Sect. 5.1.1.
4.4 Analysis of PSA
We first compare PSA, CTJPGIS and their related methods in the literature and then present
an analysis of PSA’s asymptotic convergence property.
4.4.1 Comparison with QuickProp
PSA can be considered as a member in the family of the discretized Newton methods in
the numerical analysis literature (Ortega and Rheinboldt 1970). When the problem is to
solve a system of nonlinear equations g(θ) = 0, the first order optimality condition of an
optimization problem, the general form of these methods is
θ(t+1) = θ(t) − A[θ,h]−1g(θ (t)),
Mach Learn
Fig. 4 Iterations of discretized Newton methods. A straight line indicates an application of M, in our case
the gradient-descent. A curve line indicates applying an approximated Newton step. A dash line in (a) indi-
cates a gradient evaluation but no new weight vector is derived
The “otherwise” case turns out to be identical with the secant method. However, if we rewrite
(24) as a single mapping rule, we obtain
θ
(t+1)
i = θ(t)i −
[
ηgi (θ (t))
gi (θ (t)) − gi (θ (t) − ηg(θ (t)))
]
gi (θ (t)),
which is equivalent to the following general form:
θ(t+1) = θ(t) − A[θ(t), ηg(θ (t))]−1g(θ (t)).
Figure 4(a)–(c) illustrates the progressions of the secant and Steffensen methods. We can
see that they are indeed quite different. Both methods apply the same approximated Newton
step (i.e., (23)), which approximates the tangent hyperplane with slope H by a secant A with
respect to a pair of weight vectors. The secant method estimates H with respect to θ(t−1) and
S(θ (t−1)), while the Steffensen method is based on θ(t−1) and M(θ (t−1)). The approximation
made by the Steffensen method could be more accurate than that by the secant method
because intuitively, the secant line measured from a pair of points is close to the tangent line
if the pair are close together and it is likely that ‖M(θ (t−1))−θ(t−1)‖ < ‖S(θ (t−1))−θ(t−1)‖.
Mach Learn
Theorem 1 Let λh be the least eigenvalue of H(θ∗;D). The asymptotic rate of convergence
of PSA is bounded by
eig(S(t)) ≤ exp
{−η(0)λhb
1 − β
}
.
Proof We can show that
eig(S(t)) =
t∏
k=1
(
1 − η(0)β kb λh
)
≤ exp
{
−
t∑
k=1
η(0)λhβ
 k
b

}
= exp
{
−η(0)λh
t∑
k=1
β
k
b

}
because for any 0 ≤ aj < 1, since 1 − aj ≤ e−aj ,
0 ≤
n∏
j=1
(1 − aj ) ≤
n∏
j=1
e−aj = e−
∑n
j=1 aj .
Now, since
t∑
k=1
β
k
b
 ≈
⎛
⎝
 t
b
∑
l=0
bβl
⎞
⎠ = b
 t
b
∑
l=0
βl → b
1 − β when t → ∞,
we have
eig(S(t)) ≤ exp
{
−η(0)λh
t∑
k=1
β
k
b

}
→ exp
{−η(0)λhb
1 − β
}
when t → ∞.

Though this analysis suggests that for rapid convergence to θ∗, we should assign β ≈ 1
with a large b and η(0), it is based on a worst-case scenario and thus insufficient as a prac-
tical guideline for parameter assignment. Section 5.1.4 will provide an empirically derived
guideline for the purpose.
5 Experimental results
This section reports the experimental results of applying PSA to a variety of models and
large scale tasks, including CRF training for sequence labeling tasks, large scale training
of linear SVM for binary classification, and multi-classification by a convolutional neural
network model.
5.1 Conditional random fields
We compared PSA with plain SGD and SMD (Vishwanathan et al. 2006a) to evaluate PSA’s
performance for the four tasks as given in Table 1. Again, we implemented PSA by replac-
ing the L-BFGS optimizer in CRF++. For SMD, we used the implementation available
Mach Learn
PSA used one for all tasks, implying that PSA must perform many times more gradient-
descent iterates than SMD in a pass. But PSA is still faster than SMD partly because PSA
can take advantage of the sparsity trick as plain SGD (Shalev-Shwartz et al. 2007).
For very high dimensional models, variants of SGD could be slow due to the need to
update the entire vector for every small batch in the set of training examples. But for SGD,
the sparsity trick can be applied to drastically reduce the cost of updating. In the case of CRF
training, given a small batch B(t) in a sparse data set, it is quite likely that the expected value
of the i-th feature is zero. In that case, we have ∇i L(θ (t),B(t)) = θ
(t)
i
σ 2
and we can update θ(t)i
by
θ
(t+1)
i = θ(t)i − η(t)
θ
(t)
i
σ 2
= θ(t)i
(
1 − η
(t)
σ 2
)
.
That is, most weights can be updated by multiplying (1 − η(t)
σ 2
).
SGD can take advantage of this trick by decomposing θ to sφ, where s is a scaling factor
and φ a vector. Then, the weight update is performed based on the expected occurrences of
the features as follows:
1. s(t+1) = s(t)(1 − η(t)
σ 2
);
2. if the expected occurrence of feature i is 0, φi remains unchanged;
3. otherwise, φ(t+1)i = (s(t)φ(t)i − η(t)∇i L(θ (t),B(t)))/s(t+1).
The sparsity trick can be applied to PSA in almost the same manner during every 2b iterates
before updating the step size. However, it is not applicable to SMD because SMD performs
a local step-size updates independently at every iteration. This could be one of the reasons
why the batch size for SMD must be set to more than one to avoid intensive weight updates.
PSA can save more time if the data is very sparse, as more features will have zero expected
occurrence in B(t). The chance usually decreases if we have more labels in the model, as in
the case of the CoNLL 2000 chunking task, where 23 labels are used.
5.1.2 Convergence performance comparison
Figure 5 shows the learning curves of all methods for the four chosen tasks. The learning
curves plot the progress of test-set F-scores as a function of the number of processed exam-
ples, measured by the number of passes through the entire training data set. We reported the
learning curves for only the first 50 passes because by then all on-line methods have already
converged, though L-BFGS would still require many more passes to converge. As expected,
the learning curves show that PSA clear outperformed all methods in terms of the number
of passes, with its F-scores approaching the optimum immediately after one pass. The su-
periority of PSA becomes even more obvious for the two biological entity recognition tasks
that are more challenging. Both of the tasks require a large set of features. The fluctuation
of plain SGD is notable in all tasks, especially for BioCreative 2.
Figure 6 also shows the learning curves but they plot the progress of training set objective
(loss) values as a function of passes. Again, PSA outperformed all other methods, while
SGD is the slowest and has the widest fluctuation among all SGD variants. However, though
its progress appears to stall at a much higher objective value than other methods, it seems
that its F-scores were not suffered and remain only slightly lower than those of PSA and
SMD after 20 passes.
Mach Learn
Fig. 6 Comparing convergence performance in terms of objective (loss) values by PSA, SGD, SMD and
L-BFGS for four sequence labeling tasks with CRF
5.1.4 Guideline for parameter assignment
We provide here an empirically derived guideline for parameter assignment and discuss why
this guideline is reasonable. Initially, we investigated the impact of the update frequency b
for PSA. In Sect. 4.2, our analysis suggested that a too large or too small b should be avoided.
We ran PSA with different values of b for the base NP task to validate this analysis. Figure 9
shows the resulting learning curves. When b is too small, the curve rises rapidly but can only
converge at a poor F-score, as the case when b = 5. When b is between 7 to 15, the learning
curves are nearly the same. We only plotted the case when b = 10, which is also the number
we used for other experiments. When b = 20, the increasing of the F-score slowed down
but not very obviously so we did not plot the curve here. When b is as large as 50 or 100,
the tendency becomes clear. In those cases, PSA can still converge at a good F-score but the
convergence is slower.
The results confirmed that a moderately large b is optimal for single-pass PSA, but it
is still not precise to guide the tuning. To obtain optimal single-pass generalization perfor-
mance, b should be assigned according to the expected size of training examples |D|. We
found that when |D|  2000, a value for b in the order of 0.5|D|1000 is usually sufficent. This
setting implies that the step size will be adjusted per |D|1000 training examples.
Mach Learn
Fig. 8 Comparing objective (loss) values obtained given the same CPU time spent by PSA, SGD, SMD and
L-BFGS for four sequence labeling tasks with CRF
Table 4 Results of bootstrap tests comparing single-pass PSA with other methods given the same CPU time
spent. Cells in an italic font indicate that two methods have no significant difference, asterisked indicate that
PSA achieved lower F-scores, and in a normal font indicate that PSA performed statistically significantly
better
Base NP Chunking NLPBA BioCreative 2
PSA vs. SGD 430 66 4∗ 19∗
PSA vs. SMD 1000 1000 1000 1000
PSA vs. L-BFGS 1000 1000 1000 670
primal optimizer for linear SVM. We used the task RCV1-C2 from (Bottou 2007) (see Ta-
ble 5) and “SvmSgd,” a SGD implementation for linear SVM from the same site to obtain
the target empirical optimal error rate. Both PSA and SvmSgd used hinge loss. The para-
meter settings for PSA are κ = 0.95, (α,β) = (0.9999,0.99), and b = 10, following the
guideline given in Sect. 5.1.4. For η(0), SvmSgd has a function that uses a very small subset
of training examples to select the value of η(0). We used this function to select η(0)i = 0.1, ∀i
for PSA. Finally, the mini-batch size is one. All experiments reported here for linear SVM
Mach Learn
function of a linear SVM given training examples D = (xi , yi); i = 1,2, . . . , n, is defined by
PD(w, b) := 12‖w‖
2 + C
n∑
i=1
max(0,1 − yi(wT xi + b))2,
where w is the weight vector. Then the on-line loss function is:
P(x,y)(w, b) := 12n‖w‖
2 + C max(0,1 − y(wT x + b))2.
The gradient is given by:
∂P
∂wk
= 1
n
wk + 2C(−yxk + y2wkx2k + y2bxk), k = 1, . . . , d,
∂P
∂b
= 2C(−y + y2wT x + y2b)
when 1 − y(wT x + b) > 0. Otherwise, it is:
∂P
∂wk
= 1
n
wk.
The parameter settings for PSA for this task is the same as those for RCV1-C2 except
for η(0). Again, we used SGD to select its values and assigned the values for both SGD and
PSA. The selected values are 0.1 for all elements for L1(hinge)-loss and 0.01 for L2-loss.
The experimental results for MINST are shown in Table 6. In one pass, PSA achieves a
test error rate close to LibLinear with the CPU time comparable to SGD. PSA achieved a
much lower primal objective values than SGD. We note that for both tasks, it may take hours
to complete for a conventional dual SVM solver. We did not compare with other linear SVM
methods because our main purpose is to evaluate PSA’s single-pass performance.
We performed the same bootstrap test as described in Sect. 5.1.3 to compare single-
pass PSA and SGD given the same CPU time. The result shows that in 10,000 trials, the
accuracies of PSA L2 exceeded those of SGD L2 in 10,000 times.
5.3 Convolutional neural network
Approximating Hessian is particularly challenging when the loss function is non-convex. We
tested PSA in such a setting by applying PSA to train a large convolutional neural network
(CNN) for the original 10-class MNIST task (see Table 5).
Let xji be the ith input of the j -th unit of the output layer in a CNN, oj the computed
output of the j th unit, and tj the target output (answer) of the j th unit. The output o is a
non-linear function:
o = σ(y) = 1
1 + e−y ,
where y is a linear function of a current weight vector w. The loss function is the mean
square error of w given the training data set D:
E(w) := 1
2
∑
d∈D
∑
k∈outputs
(tkd − okd)2.
Mach Learn
with the layer trick exceeded those of without the layer trick in 9986 times, implying that
the result reported here is statistically significant.
6 Conclusions
It has been shown that given a sufficiently large training set, a single pass of 2SGD gen-
eralizes as well as the empirical optimum. Our results show that PSA provides a practical
solution to accomplish near optimal performance of 2SGD as predicted theoretically for a
variety of large scale models and tasks with a reasonably low cost per iteration compared
to competing 2SGD methods. The benefit of 2SGD with PSA over plain SGD becomes
clearer when the scale of the tasks are increasingly large. For non-convex neural network
tasks, since the curvature of the error surface is so complex, it is still very challenging for
an eigenvalue approximation method like PSA. Our future work is to extend PSA to kernel
methods.
Acknowledgements We thank the anonymous reviewers for their helpful comments. This research was
supported in part by the National Research Program in Genomic Medicine (NRPGM), National Science
Council, Taiwan, under Grant No. NSC98-3112-B-001-026 (Bioinformatics Core Service), in part by Na-
tional Science Council, Taiwan under Grant No. NSC97-2221-E-001-021-MY2, and in part by a cross-
university collaboration fund from National Taiwan University of Science and Technology.
References
Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation, 10, 251–276.
Becker, S., & LeCun, Y. (1988). Improving the convergence of back-propagation learning with second-order
methods. In Proceedings of the 1988 connectionist models summer school (pp. 29–37). San Mateo:
Morgan Kaufman.
Benveniste, A., Metivier, M., & Priouret, P. (1990). Adaptive algorithms and stochastic approximations.
Berlin: Springer.
Bordes, A., Bottou, L., Gallinari, P., & Weston, J. (2007). Solving multiclass support vector machines with
LaRank. In Proceedings of the 24th international conference on machine learning (ICML’07) (pp. 89–
96). New York: ACM.
Bottou, L. (2007). The tradeoffs of large-scale learning. Tutorial, the 21st annual conference on
neural information processing systems (NIPS 2007), Vancouver, BC, Canada. http://leon.bottou.org/
talks/largescale.
Bottou, L., & Bousquet, O. (2008). The tradeoffs of large scale learning. In Advances in neural information
processing systems, 20 (NIPS 2007). Cambridge: MIT Press.
Bottou, L., & LeCun, Y. (2004). Large scale online learning. In S. Thrun, L. Saul, & B. Schölkopf (Eds.),
Advances in neural information processing systems 16 (NIPS 2003). Cambridge: MIT Press.
Bottou, L., & LeCun, Y. (2005). On-line learning for very large data sets. Applied Stochastic Models in
Business and Industry, 21(2), 137–151.
Bottou, L., Chapelle, O., DeCoste, D., & Weston, J. (Eds.) (2007). Large scale kernel machines. Cambridge:
MIT Press.
Burden, R. L., & Faires, D. (1988). Numerical analysis. PWS-KENT Pub. Co.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: a library for support vector machines. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Chen, S. F., & Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum entropy models. Technical
Report CMU-CS-99-108, School of Computer Science, Carnegie Mellon University, PA, USA, Pitts-
burgh.
Collins, M., Globerson, A., Koo, T., Carreras, X., & Bartlett, P. L. (2008). Exponentiated gradient algo-
rithms for conditional random fields and max-margin Markov networks. Journal of Machine Learning
Research, 9, 1775–1822.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series B, 39(1), 1–38.
Mach Learn
Sha, F., & Pereira, F. (2003). Shallow parsing with conditional random fields. In Proceedings of human
language technology, the North American chapter of the association for computational linguistics
(NAACL’03) (pp. 213–220).
Shalev-Shwartz, S., Singer, Y., & Srebro, N. (2007). Pegasos: Primal Estimated sub-GrAdient SOlver for
SVM. In ICML’07: Proceedings of the 24th international conference on machine learning (pp. 807–
814). New York: ACM.
Spall, J. C. (2003). Introduction to stochastic search and optimization: estimation, simulation and control.
New York: Wiley-Interscience.
Taskar, B. (2005). Learning structured prediction models: a large margin approach. Ph.D. thesis, Stanford
University, Stanford, CA, USA. Adviser-Daphne Koller.
Taskar, B., Chatalbashev, V., Koller, D., & Guestrin, C. (2005). Learning structured prediction models: a large
margin approach. In Proceedings of the 22nd international conference on machine learning (ICML’05)
(pp. 896–903). New York: ACM.
Traub, J. (1964). Iterative methods for the solution of equations. Englewood Cliffs: Prentice Hall.
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods for structured and
interdependent output variables. Journal of Machine Learning Research, 6, 1453–1484.
Vishwanathan, S., Schraudolph, N. N., Schmidt, M. W., & Murphy, K. P. (2006a). Accelerated training of
conditional random fields with stochastic gradient methods. In Proceedings of the 23rd international
conference on machine learning (ICML’06), Pittsburgh, PA, USA.
Vishwanathan, S., Schraudolph, N. N., & Smola, A. J. (2006b). Step size adaptation in reproducing kernel
Hilbert space. Journal of Machine Learning Research, 7, 1107–1133.
Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. In 1960 IRE WESCON convention record
(pp. 96–104), New York.
 國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期：2009 年 10 月 21 日 
一、參加會議經過 
The workshop aimed at assessing the state of the art of text-mining technology applied in 
biological domain after the BioCreative (Critical Assessment for Information Extraction in Biology) II.5 
challenge, which is one of the worlds' best-known context in biological text-mining domain. Each time 
the challenge attracts the text-mining specialists with background in Mathematics, Statistics, Linguistics, 
and Biology, Informatics, Pharmaceutics to participate in and develop noval methods to different tasks. 
This time the workshop took place in Spanish National Cancer Research Centre (CNIO) located in 
Madrid, Spain. CNIO is one of the few European Cancer Centres to allocate resources to both basic and 
applied research in an integrated fashion to support molecular diagnostics and drug discovery. 
The workshop composed of 27 talks in 3 days, and covered the contents, such as the current state of 
the text-mining application in biological domain, and the text-mining needs in the pharmaceutical 
industry, and the challenge participants' talks. I gave a talk in the 3rd day of the workshop to introduce 
our article categorization system for classifying whether input articles includes protein-protein 
interaction information or not. 
二、與會心得 
Participating in the context and the workshop is a great opportunity to evaluate our text-mining 
technique and compare with other groups in the same platform. The most important thing is that I learned 
計畫編號 E731-NSC97-2221-E-001-021-MY2 
計畫名稱 以二階隨機梯度探底法實現大尺度深度機器學習 
出國人員
姓名 郭政儒 
服務機構
及職稱 
研究助理，資訊科學所，中央研究院 
會議時間 2009年 10月 07日至 2009 年 10 月 09 日 會議地點 
西班牙馬德里 
會議名稱 
(中文)2009 BioCreative II.5 研討會工作坊 
(英文) BioCreative II.5 Workshop 2009 
發表論文
題目 
(中文)應用局部學習方法在 BioCreative II.5 文件分類問題 
(英文) Applying Lazy Local Learning in BC II.5 Article Categorization Task 
附件二 
 
國科會補助專題研究計畫項下赴國外(或大陸地區)出差或研習心得報告 
                                    日期：2009 年 09 月 02 日 
一、 國外(大陸)研究過程 
The course is aimed for motivated Ph.D and Post-doctoral students, as well as young (under age 35) 
assistant professors in Public Institutions particularly from Asia, Australia, Africa and South America, 
with background in Mathematics, Statistics, Biology or Computing, who are involved in Bioinformatics 
and Genomes studies. Among 120 applicants, only twenty four (24) full time places are available for the 
course. Acceptance is subject to an evaluation process. The composition of selected students includes 
PhD students, postdoc, and assistant professor.  
The Pasteur Research Centre in Hong Kong University provided the hotel and very nice 
accommodations. The course program is very intensive and composed of 44 sections in 14 days. From 
programming languages to actual applications, the course covered many important methods and 
algorithms in the field of bioinformatics. Also, the course was set to have 50% theory and 50% practice, 
so practical sections are right after the theoretical parts. 
The speakers are very sophisticated to the responsible subjects, and students actively ask questions to 
repay the kindness from teachers who fly many hours for just a few hours of lectures. Moreover, teachers 
are eager to know how his/her lectures are to students and asking students during coffee break. 
Meanwhile, teachers are willing to share their experiences and to have discussion with students. There 
was no gap between teachers and students. 
In the last section of the course, every student is asked to have presentation of the take-home 
messages from the course. Students are grateful to the fruitful contents in the course and have no regret to 
fly all the way to Hong Kong. Some of the students have even formed research projects that they are 
going to cooperate in the future. Despite of the materials covered during the course, the organizers also 
provides a flash disk filled up with more than 1GB of bibliographic papers. Furthermore, the system 
administrator provides a copy the Linux system used during the course so that students can practices after 
they go back to their own country. 
 
二、 研究成果 
計畫編號 E731-NSC97-2221-E-001-021-MY2 
計畫名稱 以二階隨機梯度探底法實現大尺度深度機器學習 
出國人員
姓名 林冠廷 
服務機構
及職稱 
研究助理，資訊科學所，中央研究院 
出國時間 2009 年 08 月 16 日至2009 年 08 月 30 日 出國地點 
中國香港 
附件三 
 國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期：2009 年 10 月 21 日 
一、參加會議經過 
The workshop aimed at assessing the state of the art of text-mining technology applied in 
biological domain after the BioCreative (Critical Assessment for Information Extraction in Biology) II.5 
challenge, which is one of the worlds' best-known context in biological text-mining domain. Each time 
the challenge attracts the text-mining specialists with background in Mathematics, Statistics, Linguistics, 
and Biology, Informatics, Pharmaceutics to participate in and develop noval methods to different tasks. 
This time the workshop took place in Spanish National Cancer Research Centre (CNIO) located in 
Madrid, Spain. CNIO is one of the few European Cancer Centres to allocate resources to both basic and 
applied research in an integrated fashion to support molecular diagnostics and drug discovery. 
The workshop composed of 27 talks in 3 days, and covered the contents, such as the current state of 
the text-mining application in biological domain, and the text-mining needs in the pharmaceutical 
industry, and the challenge participants' talks. I gave a talk in the 3rd day of the workshop to introduce 
our article categorization system for classifying whether input articles includes protein-protein 
interaction information or not. 
二、與會心得 
Participating in the context and the workshop is a great opportunity to evaluate our text-mining 
technique and compare with other groups in the same platform. The most important thing is that I learned 
計畫編號 E731-NSC97-2221-E-001-021-MY2 
計畫名稱 以二階隨機梯度探底法實現大尺度深度機器學習 
出國人員
姓名 郭政儒 
服務機構
及職稱 
研究助理，資訊科學所，中央研究院 
會議時間 2009年 10月 07日至 2009 年 10 月 09 日 會議地點 
西班牙馬德里 
會議名稱 
(中文)2009 BioCreative II.5 研討會工作坊 
(英文) BioCreative II.5 Workshop 2009 
發表論文
題目 
(中文)應用局部學習方法在 BioCreative II.5 文件分類問題 
(英文) Applying Lazy Local Learning in BC II.5 Article Categorization Task 
附件二 
 
國科會補助專題研究計畫項下赴國外(或大陸地區)出差或研習心得報告 
                                    日期：2009 年 09 月 02 日 
一、 國外(大陸)研究過程 
The course is aimed for motivated Ph.D and Post-doctoral students, as well as young (under age 35) 
assistant professors in Public Institutions particularly from Asia, Australia, Africa and South America, 
with background in Mathematics, Statistics, Biology or Computing, who are involved in Bioinformatics 
and Genomes studies. Among 120 applicants, only twenty four (24) full time places are available for the 
course. Acceptance is subject to an evaluation process. The composition of selected students includes 
PhD students, postdoc, and assistant professor.  
The Pasteur Research Centre in Hong Kong University provided the hotel and very nice 
accommodations. The course program is very intensive and composed of 44 sections in 14 days. From 
programming languages to actual applications, the course covered many important methods and 
algorithms in the field of bioinformatics. Also, the course was set to have 50% theory and 50% practice, 
so practical sections are right after the theoretical parts. 
The speakers are very sophisticated to the responsible subjects, and students actively ask questions to 
repay the kindness from teachers who fly many hours for just a few hours of lectures. Moreover, teachers 
are eager to know how his/her lectures are to students and asking students during coffee break. 
Meanwhile, teachers are willing to share their experiences and to have discussion with students. There 
was no gap between teachers and students. 
In the last section of the course, every student is asked to have presentation of the take-home 
messages from the course. Students are grateful to the fruitful contents in the course and have no regret to 
fly all the way to Hong Kong. Some of the students have even formed research projects that they are 
going to cooperate in the future. Despite of the materials covered during the course, the organizers also 
provides a flash disk filled up with more than 1GB of bibliographic papers. Furthermore, the system 
administrator provides a copy the Linux system used during the course so that students can practices after 
they go back to their own country. 
 
二、 研究成果 
計畫編號 E731-NSC97-2221-E-001-021-MY2 
計畫名稱 以二階隨機梯度探底法實現大尺度深度機器學習 
出國人員
姓名 林冠廷 
服務機構
及職稱 
研究助理，資訊科學所，中央研究院 
出國時間 2009 年 08 月 16 日至2009 年 08 月 30 日 出國地點 
中國香港 
附件三 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本研究獲得 2008 年在芬蘭赫爾辛基舉行的國際機器學習研討會超大規模機器
學習比賽最佳新進研究獎。（The best newcomer award, Large Scale Machine 
Learning Challenge, 2008 International Conference on Machine Learning 
(ICML), Helsinki, Finland) 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
