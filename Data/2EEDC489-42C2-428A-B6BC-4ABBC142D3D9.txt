1toward the automatic design of network architectures. These methods automatically add or
remove nodes, layers and connections during the training, but such approaches only explore a
small subspace of all possible network architectures, and the result depends on the initial network
architectures. Therefore, as stated by Angeline et al. [1], "such structural hill climbing methods
are susceptible to becoming trapped at structural local optima."
In recent years, evolutionary algorithms (EAs) [2] have been applied to the ANN's
optimization. EAs are powerful search algorithms based on the mechanism of natural selection.
Unlike conventional search algorithms, they simultaneously consider many points in the search
space so as to increase the chance of global convergence. Many considerable efforts at obtaining
optimal networks based on EAs have been reported in the literature and comprehensively
reviewed by Yao [3]. However, due to over-emphasizing the ANN's biological evolution
behaviors, the EA based methods are frequently needed to tailor exclusive genetic operators, or to
modify or remove original genetic operators to avoid the production of illegal or irregular
network topologies [4-12].
We know that the weight optimization problem of an ANN belongs to the class of
real-valued optimization problems. After identifying the attribute of weight optimization
problems, it is natural to employ the handy real-valued optimization approaches to solve the
weight optimization problem, such as gradient-based methods or EA-based methods. Similar to
the weight optimization problem, the architecture optimization problem of an ANN can be
formulated as an integer optimization problem since the architecture parameters can be expressed
as integer variables. Consequently, the simultaneous architecture and weight optimization of an
ANN can be formulated as a mixed-integer optimization problem. Likewise, one can use a
suitable mixed-integer optimization method to solve this mixed-integer optimization problem. In
this study, an evolutionary algorithm, called mixed-integer hybrid differential evolution (MIHDE)
[13-15], is used to perform the optimization process. To test the capability of MIHDE for
evolving neural networks, a benchmark problem is employed in this study. It is the on-line
identification of Dynamical Systems [16]. Finally, computational results demonstrate the
effectiveness of MIHDE in evolving the neural networks.
Optimization of Artificial Neural Networks
In this study, a mixed-integer encoding scheme is used to represent the optimization
parameters of ANNs, including the number of hidden layers, the number of nodes in each hidden
layer, the types of node transfer functions and the weights of connections. The connection
weights are encoded by real numbers and the other architecture parameters encoded by integer
numbers.
In general, a better ANN has a good generalization capability. Good generalization means
that an ANN can produce a smooth approximation of the known training data to an arbitrarily
accurate degree and then be able to approximate any unknown data closely as possible. The
generalization can be categorized into global generalization and local generalization. The types
of generalization are dependent on the types of node transfer functions. As the well-known
sigmoid function, it is a nonlinear and monotonically increasing function. The sigmoid function
3where the value of hi j, is in the range (-1, 1).
4) radial basis function:
h
h w
b
H W
b
i L j n
i j
i k i j k
i jk
n
i i j
i j
i
i
,
, , ,
,
*
,
*
,
(
( )
( , , , ; , , ,
 

 

  




exp )
exp )
1
2
2
0
1
2
2
2
2
1 2 1 1 2
1
 
(7)
where W w w wi j i j i j i j ni,
*
, , , , , ,[ , , , ] 1 2 1 (8)
H h h hi i i i ni    1 1 1 1 2 1 1
*
, , ,[ , , , ]
T (9)
.
To implement the evolutionary optimization process, a mixed-integer encoding scheme is
developed to represent the genotype chromosome of the neural network. Each genotype
chromosome, corresponding to a candidate neural network, consists of three types of genes: layer
gene, node genes and weight genes.
The layer gene, in the form of an integer-valued code, denotes the number of active (or
existent) hidden layers. On the other hand, the output layer is necessarily active at any time.
Each set of node genes is composed of three integer-valued codes, respectively
corresponding to three possible hidden nodes. The meanings of node genes are defined as follows:
"0" denoting an inactive (nonexistent) node, "1" an identity function, "2" a unipolar sigmoid
function, "3" a bipolar sigmoid function, and "4" a radial basis function. In order to reduce the
number of architecture parameters and simultaneously increase the search efficiency, a fixed
output node transfer function is used in this study. The linear identity function is selected in this
study since the output can be easily obtained by summing up the weighted preceding nodes.
Finally, the weight genes, in the form of real-valued codes, are used to denote the weight and bias
parameters of each corresponding node. Therefore, the mixed-integer encoding scheme, which
combines the layer gene, the node genes and the weight genes, is able to guide the evolutionary
optimization of neural networks.
After achieving a genotype encoding, a corresponding phenotype neural network can be
obtained by pruning the inactive layers, nodes and redundant connections. And then we need to
evaluate each candidate neural network by a training process. In this study, the MIHDE algorithm
is used to conduct the training process. To evaluate an ANN, the objective function may be
defined according to the performance requirements. The error function on the training data is
chosen as the objective function to assess the ANN's performance. For simplicity, the mean
squared error function (MSE) is used and stated by the following equation:
E
mn
d k o k
L
j j
j
n
k
m L
 
 

12 1
2
11
1
( ( ) ( )) (10)
where o j is an output of the neural network, d j is the corresponding desired output, and
5Figure 1. Plant and model outputs of the dynamical control system
Figure 2. Model error of the neural network
method has the ability to optimize the weights and architectures of any neural networks. A
benchmark problem is used to test the capability of MIHDE in neural network optimization.
Computational results demonstrate that the obtained neural network model can follow the plant
Time
0 100 200 300 400 500 600 700
y(
k)
-6
-4
-2
0
2
4
6
model output (solid line)
plant output (dashed line)
Time
0 100 200 300 400 500 600 700
M
od
el
E
rr
or
-1.4
-1.0
-0.6
-0.2
0.2
0.6
1.0
