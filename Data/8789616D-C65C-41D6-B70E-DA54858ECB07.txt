可供推廣之研發成果資料表 
□ 可申請專利  ▉ 可技術移轉                                      日期：98年10月28日 
國科會補助計畫 
計畫名稱：移動平臺之前瞻性視訊監控技術 
計畫主持人：黃雅軒       
計畫編號：NSC 97-2221-E-216-040             
學門領域：資訊 
技術/創作名稱 強健性人臉辨識 
發明人/創作人 黃雅軒 
技術說明 
z 中文：本技術實現二種具高鑑別度的人臉辨識方法(CMSM 和
GDA)，並將他們有效的組合，以得到一套具強鍵性的人臉辨識
系統。CMSM (Constrained Mutual Subspace Method，限制性子空
間)使用多張影像所形成的子空間作為辨識的依據，可表示使用
者人臉特有的變化形態。GDA (Generalized Discriminant Analysis)
使用核函數之非線性區別分析，將資料映射至高維度空間使其盡
量線性可分割。這二種方法使用不同種類的特徵和比對機制，因
此他們的比對結果具有高度的互補性。針對通用的 Banca 人臉資
料庫，本研究的辨識結果在容忍 10%的錯誤接受率(False Accept 
Rate)條件下，正確的辨識率(Recognition Rate)可高達 98%。 
英文：This paper presents a robust face recognition method which two 
highly discriminating algorithms (CMSM and GDA) to recognize 
human faces. CMSM (Constraint Mutual Subspace Method) constructs 
a class subspace for each person and makes the relation between class
subspaces by projecting them onto a generalized difference subspace 
so that the canonical angles between subspaces are enlarged to 
approach to the orthogonal relation. GDA (Generalized Discriminant 
Analysis) adopts kernel function operator to make it easy to extend and 
generalize the classical Linear Discriminant Analysis to a non linear 
one. Both CMSM and GDA are effective to recognize human faces, 
however, CMSM constructs a subspace from several face images and 
GDA needs only one face image to perform recognition. Obviously, 
these two methods inherently have different properties and abilities of 
recognition so that we combine them together. Experimental results 
show that the proposed method can achieve good recognition
accuracy. 
附件二 
Face Recognition Based on Complementary Matching of Single Image and 
Sequential Images 
 
 
Yea-Shuan Huang  and  Wei-Cheng Liu 
Computer Science & Information Engineering, Chung Hua University 
No.707, Sec. WuFu Rd., Hsinchu, Taiwan, 300, R.O.C. 
 
 
Abstract 
 
This paper presents a robust face recognition method 
which two highly discriminating algorithms (CMSM and 
GDA) to recognize human faces. CMSM (Constraint 
Mutual Subspace Method) constructs a class subspace 
for each person and makes the relation between class 
subspaces by projecting them onto a generalized 
difference subspace so that the canonical angles between 
subspaces are enlarged to approach to the orthogonal 
relation. GDA (Generalized Discriminant Analysis) 
adopts kernel function operator to make it easy to extend 
and generalize the classical Linear Discriminant 
Analysis to a non linear one. Both CMSM and GDA are 
effective to recognize human faces, however, CMSM 
constructs a subspace from several face images and GDA 
needs only one face image to perform recognition. 
Obviously, these two methods inherently have different 
properties and abilities of recognition so that we 
combine them together. Experimental results show that 
the proposed method can achieve good recognition 
accuracy. 
 
1. Introduction 
Biometric identification technology is a very popular 
research field in the recent years. Various methods have 
been proposed that use different kinds of biometric data. 
Among them, face recognition consistently obtains a 
great expectation since it is contact-free and is user 
friendly. Therefore, a lot of research efforts have been 
devoted to this field, and many face recognition 
approaches based on a variety of machine learning 
theorem have been developed already. For example, 
subspace methods such as PCA [1] and LDA [2] are 
commonly used which project high dimensional features 
to low dimensional features and not only faster but also 
better recognition can be achieved. In general, LDA has 
better recognition ability than PCA which is based on an 
eigenvalue resolution and gives an exact solution of the 
maximum of the inertia. But even LDA fails for a 
nonlinear problem. A Generalized Discriminant Analysis 
(GDA) is developed to overcome this difficulty by 
mapping the input space into a high dimensional feature 
space with linear properties so that it can solve the 
problem in a LDA classical way. 
Basically, the feature derived from a single image 
denotes the location of this image in a high dimensional 
feature space. In the feature space, the locations 
corresponding to two similar images will be in general 
close to each other, and the locations of two very distinct 
images then will be quite separated apart. Therefore, 
recognition based on a single image mainly measures the 
distance (or similarity) of the features between the input 
pattern and the reference patterns. However, the feature 
derived from a set of sequential images of the same 
person can present the unique variation model of this 
person. Therefore, recognition based on sequential 
images indeed compares the specific variation pattern of 
the unknown input subject and that of each individual 
class. The two kinds of recognition seem to be 
complementary in nature. With this understanding, it will 
be very useful if both kinds of methods are combined 
together. In this paper, GDA (Generalized Discriminant 
Analysis) and CMSM (Constraint Mutual Subspace 
Method) are used together to recognize faces not only 
because they both have high recognition abilities, but 
also they probably are complementary to each other since 
GDA takes a single-image matching strategy and CMSM 
takes a sequential-image matching strategy. 
This paper is organized as follows. Section 2 describes 
two recognition models; the first is GDA and the second 
is CMSM. A linear mechanism is also proposed to 
integrate their recognition results. Section 3 presents the 
experiment results on the famous Banca face database, 
and the final conclusion is drawn in Section 4. 
 
2. Face identification method 
In this section, we describe our face recognition 
framework which integrates a single-image matching 
module and a sequence-image matching module. The 
single-image matching module uses a GDA algorithm to 
reduce feature dimension first, and then adopts a nearest 
distance classification to recognize the input pattern; the 
sequence-image matching module uses a CMSM metric 
which projects each individual subspace including the 
input and the reference subspaces onto a common 
difference subspace and their canonical angles are used 
space F. Through the kernel method, GDA in general has 
much better discrimination ability than LDA. 
The transformed feature 𝑦  now becomes 𝑦 = 𝑣𝑇𝑥 
where 𝑥  is a sample feature vector. For recognition, a 
nearest distance classification metric is applied. Let  
𝐼1 , ⋯ , 𝐼𝑚  denote the feature vectors of 𝑚 input samples,  
𝐼1
𝐺𝐷𝐴 , … , 𝐼𝑚
𝐺𝐷𝐴  are their GDA transformed vectors, and 
𝑅𝑘 ,𝑞
𝐺𝐷𝐴   𝑞 = 1, … , 𝑛  be the GDA transformed feature 
vector of the 𝑞th sample of the 𝑘th enrolled person. Then, 
the distance of the m input samples and the n reference 
data of the kth person becomes 
𝐷𝑖𝑠𝑡(𝑘) = min
𝑝=1,⋯,𝑚
 min
q=1,⋯,n
𝑑 𝐼𝑝
𝐺𝐷𝐴 , 𝑅𝑘 ,𝑞
𝐺𝐷𝐴  
 
2.2. Constrain mutual subspace method 
2.2.1. Concept of canonical angle 
In linear algebra, the similarity between two 
subspaces is calculated by the angle between them. 
Suppose  𝑅1, … , 𝑅𝑟  is a set of r reference patterns, 
 𝐼1 , … , 𝐼𝑠  is a set of s input patterns, and each pattern is 
represented by an f-dimensional feature vector. With 
PCA, an rno-dimensional reference subspace Ω can be 
constructed from  𝑅1, … , 𝑅𝑟 , and an sno-dimensional 
input subspace Λ can be constructed from  𝐼1 , … , 𝐼𝑠  
respectively. Therefore, Ω is an  rno × f  matrix and Λ is 
an  sno × f  matrix. In general, the relations of r, s, rno and 
sno are chosen to be rno≦r, sno≦s and rno≦sno. We can 
further obtain rno canonical angles   𝜃1, … , 𝜃𝑟𝑛𝑜   between 
subspace Ω and subspace Λ by the following equations: 
CXC   
       no
r
k jkkiijij
xxX
1
  , 
 
where 
i  and i  denote respectively the i-th f-
dimensional orthonormal basis vector of subspace Ω and 
Λ, 𝜆 is an eigenvalue of X and C is the eigenvectors of X, 
and X is an rno × rno matrix. The value 𝑐𝑜𝑠
2 𝜃𝑖  of the i-th 
smallest canonical angle equals to the i-th largest 
eigenvalue of Λ. The largest eigenvalue (i.e. 𝑐𝑜𝑠2 𝜃1) is 
taken to denote the similarity between subspace Ω and Λ. 
 
2.2.2. Generation of constrained subspace 
In CMSM, it is essentially important to generate a 
proper constrained subspace C which contains the 
effective matching components but eliminating the 
unnecessary ones. By projecting the input subspace and 
reference subspaces to a constrained subspace, it could 
extract discriminating features for recognizing pattern 
classes. 
Suppose there are in total Np reference subspaces. To 
generate a constrained subspace, we compute the 
projection matrix Ωk of the k-th reference subspace using 
 
Tr
j
k
j
k
jk
no
P   1   
where rno is the number of eigenvectors of a reference 
subspace, k
j  is the j-th orthonormal basis vector of the 
k-th reference subspace, and each Pk is a 𝑓 × 𝑓 matrix. 
Then, we calculate the eigenvectors of the summation 
matrix 𝑆 =  𝑃1 + 𝑃2 + ⋯ + 𝑃𝑁𝑝 , that is 𝑆𝐴 = 𝜆𝐴, where 𝜆 
and A denote the eigenvalues and the eigenvectors of S 
respectively. Finally, the t eigenvectors [A1,…,At] 
corresponding to the t smallest eigenvalues are selected 
to construct the constrained subspace CS (that is  
CS=[A1,…,At]t×f). For a more detailed description of 
CMSM, please see [4]. 
 
2.2.3. Matching on constrained subspace 
Suppose there are in total L recognition classes. Π 
denotes the input subspace derived from the input 
sequence samples, and Τk (1 ≦ k ≦ L) denotes the 
subspace derived from the training sequence samples of 
class k. Five steps need to be performed for pattern 
matching as follows: 
1. Project each Τk onto CS and generate an rno× t 
projection matrix Pk; 
2. Normalize each Pk, and with a Gram-Schmidt 
algorithm derive a reference subspace Ωk with 
basis {𝜓1
𝑘 , ⋯ , 𝜓𝑡_𝑛𝑜
𝑘 }; 
3. Project Π onto CS and generate an sno× t 
projection matrix Q; 
4. Normalize Q, and with a Gram-Schmidt algorithm 
derive the input subspace Λ with basis 
{𝜙1, ⋯ , 𝜙𝑡_𝑛𝑜 }; 
5. Compute the similarity 𝑆𝑖𝑚(𝑘) between Λ and Ωi 
by using the canonical angle computation as 
𝑆𝑖𝑚(𝑘) =    𝜓𝑖
𝑘 , 𝜙𝑗  
2
𝑟𝑛𝑜
𝑗
𝑠𝑛𝑜
𝑖
 
 
2.3. Combination scheme 
Obviously, 𝐷𝑖𝑠𝑡(𝑘)  is a distance measurement and  
𝑆𝑖𝑚(𝑘)  is a similarity measurement, they have totally 
different interpretation, and both small  𝐷𝑖𝑠𝑡(𝑘)  and 
large 𝑆𝑖𝑚(𝑘)  denote that the input patterns and the 
reference data of person k are similar to each other. In 
order to combine the matching scores of GDA and 
CMSM, the integrated value of similarity is calculated as 









)(
)()( 21
kDist
kSimksimilarity  
where 𝜔1  and 𝜔2  are the combining weights of the two 
matching scores, and 𝛼  and 𝜎  are two normalized 
parameter. All the parameters are decided by experiments. 
 
3. Experimental results 
We used the famous Banca face database to evaluate 
the performance of the proposed recognition method. The 
Banca database contains 52 individuals and each 
individual has 12 image sequences that were taken in 
different time, at different locations and by difference 
cameras. Each image sequence consists of 10 face images 
 
 2.1. The shape model 
 
Supposed there are n facial feature points and each one 
is located at obvious face contour. The position of these 
n points can be arranged into a shape vector X, that is 
 
𝑋 = [ 𝑥1 , 𝑦1 , 𝑥2 , 𝑦2 , … , 𝑥𝑘 , 𝑦𝑘 , … , 𝑥𝑛 , 𝑦𝑛  ]
𝑇  
 
where 𝑥𝑘  and 𝑦𝑘  are the X coordinate and Y coordinate 
of the k
th
 point respectively. 
All the training shapes should be aligned first 
because we want to obtain the statistic variation of 
shapes instead of the variation of locations. The ASM 
alignment procedure is an iterative process to align 
multiple face shapes which can be summarized as 
follow: 
1. All training sample are normalized according to 
two eyes positions. 
2. Rotate, scale and translate each shape to align with 
the first shape in the training set. 
3. Calculate the mean shape from the aligned shapes. 
4. Normalize the mean shape. 
5. Realign every shape with the normalized mean 
shape. 
6. If not convergence, return to step 3. 
 
When finishing the alignment procedure, by using 
the Principal Component Analysis (PCA) operation 
eigenvectors corresponding to shape variations can be 
generated. Therefore, a shape model can be represented 
as: 
𝑥 = 𝑋 + 𝑃𝑏 
 
where 𝑋  is the mean shape, 𝑃 = [𝛷1  𝛷2 …  𝛷𝑡] is the 
eigenvectors corresponding to the t largest eigenvalues 
and 𝑏  is the shape parameter which is the projection 
coefficiency that X projects onto P. Figure 1 shows the 
face models of the first three eigenvectors with varying 
𝑏𝑖  value. Obviously, 𝑏𝑖  defines shape variation. In 
general, the larger 𝑏𝑖  is, the more deviation the face 
shape will be. Usually, 𝑏𝑖  is constrained within the 
range of  ± 3 𝜆𝑖  , so that a constructed face shape will 
not degenerate too much. 
 
 
Fig. 1: The variation of the first three parameters of the 
face model, the horizontal represent the variation value 
of shape parameter, and the vertical corresponds to the 
face models derived from different eigenvectors. 
 
2.2. The feature model 
 
In general, we suppose a landmark is located on the 
strong edge. According to the normal direction of 
landmark, we can get m pixels on both sides (Fig. 2) of 
this landmark and each pixel has a gray-level value. So 
there are in total 2m+1 gray-level values which form a 
gray-level profile represented as 
𝑔𝑖 =  𝑔𝑖0, 𝑔𝑖1 , … , 𝑔𝑖(2𝑚) , where i is the landmark index. 
In order to capture the frequency information, the 
profile first derivative 𝑑𝑔𝑗  is calculated as  
 
𝑑𝑔𝑖 =  𝑔𝑖1 − 𝑔𝑖0 , 𝑔𝑖2 − 𝑔𝑖1 , … , 𝑔𝑖 2𝑚 −𝑔𝑖(2𝑚−1) . 
 
In order to lessen the effect of varying image lighting 
and contrast, the profile is normalized as  
 
𝑦𝑖 =
𝑑𝑔 𝑖
 |𝑑𝑔 𝑖𝑘 |
2𝑚−1
𝑘=0
   where 𝑑𝑔𝑖𝑘 = 𝑔𝑖 𝑘+1 − 𝑔𝑖𝑘 . 
 
The feature vector 𝑦𝑖  is called grayscale profile. 
 
 
Fig. 2: The selected feature points for constructing the 
grayscale profile. 
 
2.3. The ASM algorithm 
 
The ASM searching algorithm uses an iteration 
process to find the best landmarks which can be 
summarized as follow: 
 
1. Initial the shape parameters 𝑏  to zero (the mean 
shape). 
2. Generate the shape model point using the 𝑥 = 𝑋 +
𝑃𝑏. 
3. Find the best landmark 𝑧  by using the feature 
model. 
4. Calculate the parameters b′  by the following 
equation           
 𝑏′ = 𝑃𝑇(𝑧 − 𝑋 ). 
5. Restrict parameter b′  to be within ± 3 𝜆𝑖 . 
If |𝑏′ − 𝑏|  is less than the threshold value, then the 
matching process is completed; else 𝑏 = 𝑏′ , then return 
to step 2. 
 
considerably. In fact, different landmarks may have 
different search range for their best performance. With 
this consideration, landmarks are divided into five 
clusters according to their locations and structures. The 
five clusters are the eye cluster, the eyebrow cluster, the 
nose cluster, the mouth cluster and the facial contour 
cluster. Because the shape variations of the five clusters 
are different, we can utilize the shape variation 
information to determine more appropriate searching 
ranges so that better landmark matching can be achieved. 
Therefore, we calculate the standard deviation 𝑆𝑖  of the 
i-th landmark and then the largest standard deviation 𝑆𝑏  
of each cluster will be used to derive the search range of 
this cluster. It can be represented as 
 
𝑆𝑏 = 𝑚𝑎𝑥
𝑖∈𝑏
(𝑆𝑖)   
 
where b is 0 to 4 which corresponds to one specific 
cluster. Finally, in order to obtain better results, we take 
twice the length of 𝑆𝑏  to indicate the profile search 
range. 
 
However, some search ranges of the upper eyelid 
landmarks are overlapped with those of the lower eyelid 
landmarks. This situation will result in the possibility to 
mismatch them, i.e. an upper eyelid landmark is 
mismatched to a lower eyelid landmark or vice versa. In 
order to avoid this problem, the search range of each 
eyelid landmark must be constrained. According to the 
geometry of eye-lid landmarks, the search range of each 
upper eyelid landmark should not be lower than the 
middle line between the inner eye corner and the outer 
eye corner. Similarly, we can define the constraint for 
the lower eyelid landmark. Fig. 6 illustrates the search 
range restrictions of the upper and the lower eyelid 
landmarks. 
 
 
Fig. 6: Example of the restrictions of the eyelid. 
 
The same issue also happens in the mouth 
landmarks, but we cannot use the same constraints to 
restrict their search ranges. This is because people has 
various expressions and the lower lip may be higher 
than the connection line of the two mouth corners. 
Therefore, we intend to use a sorting method to reduce 
the impact of this issue. Firstly, the mouth landmarks 
are clustered into three groups: G1, G2 and G3. Fig. 7 
shows the clustering results. In the G1 group, when the 
best matching points of all 4 landmarks have been 
determined, we can reference the heights of the 4 
matching points to order the four landmarks. The order 
should be the same as the one defined in Fig. 7. The 
other two clusters G2 and G3 use the same method. 
Accordingly, the wrong mismatch in mouth will also be 
decreased. 
 
 
Fig. 7: The clustering results. 
 
Finally, the Mahalanobis distance is used to 
measure distance between patterns which is computed 
as  
 
𝑓 𝑌 =   𝑌 − 𝑌𝑖  𝑇𝐶𝑖
−1(𝑌 − 𝑌𝑖 ) 
 
where 𝐶𝑖  is the covariance matrix of landmark i. 
 
        We use an Adaboost algorithm to construct a 
detector for each landmark of the corner group. The 
Adaboost algorithm has been extensively used for object 
detection and it often has an outstanding performance. 
The search range of a corner landmark is 𝑙 × 𝑙 centered 
by the corresponding landmark location of the 
reconstructed shape model. If there are multiple 
detected points among the search range, the point being 
closest to the corresponding landmark of the 
reconstructed shape model is taken as the best result. 
This strategy can avoid abnormal deviation so that it 
still has a chance to obtain a good matching location in 
the next iteration. 
 
4. EXPERIMENTAL RESULTS 
 
We use the well known BioID face database as the 
training database, which contains 1508 face images. In 
order to increase the training samples, the mirror images 
will be used, too. In total, there are 3016 face images 
used in the training stage. The Cohn Kanade database 
which contains 2132 face images is used for testing. Fig. 
8 show some samples of both databases. 67 landmark 
points are manually labeled for all the images of the two 
databases. 
 
  
(a)                                       (b) 
Fig. 8: (a)Examples of the BIOID database, 
           (b) Examples of the Cohn Kanade database. 
 
In order to evaluate the accuracy of our proposed 
method, the error rate E is defined as 
REFERENCES 
 
[1] ZHANG Baizhen and RUAN Qiuqi, "Facial feature 
extraction using improved deformable templates", The 
8th International Conference on Signal Process 
Nolumn4, page : digital object identifier 
10.1109/ICOSP.2006.345927. 
[2] T. F. Coots, C. Taylor, D. Cooper, and J. Graham, 
Active shape models – their training and application, 
Computer Vision and Image Understanding, 61(1):38-
59, January 1995. 
[3] T. F. Cootes and C. J. Taylor, Statistical models of 
appearance for computer vision, Tech. Report, Oct 2001, 
University of Manchester, 
http://www.isbe.man.ac.uk/~bim/, Oct 2001. 
[4]  Kwok-Wai Wan, Kin-Man Lam, Kit-Chong Ng, "An 
accurate active shape model for facial feature 
extraction", Pattern Recognition Letters , Volume 26 , 
Issue 15,  November 2005 
[5]  T. F. Cootes, G. J. Edwards, and C. J. Taylor, Active 
Appearance Models, in Proc. European Conference on 
Computer Vision 1998. (H. Burkhardt and B. Neumann 
Ed.s). Vol. 2, pp. 484~498, Springer, 1998. 
[6]  T.F. Cootes, G.J. Edwards, C.J. Taylor, Active 
appearance models, IEEE Transactions on Pattern 
Analysis and Machine Intelligence 23 (2001) 681–685. 
[7] Fei Zuo, Peter H.N. de With, Fast facial feature 
extraction using a deformable shape model with Haar-
wavelet based local texture attributes, Proceedings of 
IEEE Conference on ICIP, pp. 1425-1428, 2004. 
[8] Chunhua Du, Qianq Wu, Jie Yang, Zhenq Wu, SVM 
based ASM for facial landmarks location, 8th IEEE 
international Conference on Computer and Information 
Technology, 2008. CIT 2008. 
[9] Feng Jiao, Stan Li, Heung-Yeung Shum, Dale 
Schuurmans, Face Alignment Using Statistical Models 
and Wavelet Feature, Proceedings of IEEE Conference 
on CVPR, pp.321-327,2003. 
 
表 Y04 
sections，題目為 
 Interaction & virtual reality 
 Motion & multiview 
 Visual surveillance 
 Feature extraction & pattern recognition 
 Human sensing 
 Industrial applications 
 Geographic information systems 
 Machine vision for transportation 
 
 
二、與會心得 
 
 MVA(Machine Vision Applications)會議除了重視電腦視覺新技術的研發以外，也
非常重視應用的開發，例如路標的辨識、腳型的測量、水質的估測和布料的檢查
等。這樣性質的研討會頗適合學校老師的參與，不但可以交換研發的心得，還可
以看到多方面的應用，刺激老師進行產學合作計畫的動機。 
 此次所頒發的 5 篇過去十年重大貢獻論文獎中大部分得獎者均是日本人，這個現
象雖然來自日本人小家子氣的特質外(不願將獎帄均分配)，也顯示我們研究的品
質需大力的加強，否則對國際社會無法造成實際的幫助。 
 
三、建議 
 
 本校老師多參與國際性會議，除了介紹研究成果，增加學校的知名度以外，也能
快速擴展視野，建立合作管道，對未來研究和教學有很大的幫助。 
 鼓勵學校的研究生參與這種研究與應用結合的國際性會議，讓他們更了解研究的
實用價值，以激發學習和研究的熱誠。 
 
四、攜回資料名稱及內容   
 
會議論文集一本和光碟片一片 
 
 
表 Y04 
 (9/12) The state of the art of 3D video technologies – accurate 3D shape and 
motion reconstruction, high fidelity visualization, and efficient coding for 3D 
video, by Prof. Takashi Matsuyama, Kyoto university; 
 (9/13) Data compression by data hiding, by Prof. Hyoung Joong Kim, Korea 
university; 
 (9/14) Multimodal information fusion in the virtual environment and its 
applications in produce design, by Prof. Jianrong Tan, Zhejiang university.. 
 此會議總共包含 39 sections，其中我參加的 sections 有 
 Multimedia Signal Processing for Intelligent Applications 
 Intelligent Surveillance and Pattern Recognition 
 Advances in Biometrics(I) 
 Advances in Biometrics(II) 
 Intelligent Image and Signal Processing 
 Behavior Analysis and Abnormal Event Detection 
 Statistical Image Processing and Application 
 Application of Intelligent Computing to Signal and Image Processing 
 
 
六、與會心得 
 
 International Conference on Intelligent Information Hiding and 
Multimedia Signal Processing (IIHMSP2009)會議包含廣泛的研究議題，都
是電腦視覺領域近年來重要的研究領域，藉由與其他學者的交談，可以擴展研究
者視野，刺激老師進行產學合作計畫的動機，頗適合學校老師的參與。 
 由於大阪市立大學名譽教授 Hiromitsu Hama 有意願來台參訪，今後將繼續聯絡，
促成此事，或許有助於學校在國際化和國際合作等事務的推廣有幫助。 
 
七、建議 
 
 本校老師多參與國際性會議，除了介紹研究成果，增加學校的知名度以外，也能
快速擴展視野，建立合作管道，對未來研究和教學有很大的幫助。 
 鼓勵學校的研究生參與這種研究與應用結合的國際性會議，讓他們更了解研究的
