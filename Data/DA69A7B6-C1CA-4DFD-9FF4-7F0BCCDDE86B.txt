第 2 頁
能。
除了簡介之外，本報告分為以下幾個部
分：2.相關研究，簡單介紹有關先前排程方法
的研究；3.研究方法及介紹實驗環境、實驗結
果；4.結論，總結本研究的成果。
2.相關研究
ㄧ般來說，system 的 resources 包含
CPU、memory、network、disk 等等，然而早
期 resource 分配的研究都以 CPUs 的分配為
主。在 multicomputer 環境下，parallel job
scheduling 通常採用 space sharing approach
[13, 16]，根據研究 parallel job scheduling 的
專家 D. Feitelson 的說法，這一類研究的基本
假設如下[1, 2, 3, 13, 14, 16]：
 當一個新的 job 到達 system 時，system
會依照這個 job 要求的 CPUs 個數，根據
本系統各 queues 包含的 jobs 的分佈及當
時 CPUs 的分配情況，執行某些策略決定
何時 assign CPUs 給此 job。
 有些方法規定 user 必須提供此 job 執行
時間的預估值。例如 backfilling scheduling
機制下，user 若提供較小的預估值，有機
會讓此 job 提前執行，但往後真正的執行
時間若大於此預估值，此 job 將會被
killed。
圖二﹕ Parallel Job scheduling 執行時 Jobs分
配圖
上述的情形可用一個 2-Dimension 的圖
來表示，如圖二所示，原點表示現在的時間，
而水平與垂直軸分別代表時間與 CPUs 個
數，每個 job 都可以視為一個較大的矩形，矩
形的長度是此 job 執行時所需的(或預估的)時
間，高度則是此 job 執行時所需的 CPUs 個數
[6, 9, 12]。當此 2D 圖裡的矩形之間出現空洞
(holes)，則表示有許多 CPUs 正在閒置。具有
backfilling 機制的系統會將 waiting queue 後
面較小的 jobs 往前移動，以設法將此漏洞補
滿。所以，有些較有效率的 job scheduling
algorithms 可以減少空洞，增進系統整體的效
能。
最簡單的 parallel job scheduling algorithm
叫 FCFS(first-come first-serve)，CPUs 的分配
完全以 jobs 到達 system 的先後次序決定，當
目前系統剩下的閒置 CPUs個數無法滿足目前
waiting queue 最前頭的 job 的需求，排在 queue
後面的 jobs 必須等待，即使後面有其他需求較
小的 jobs，也無法提早使用這些閒置的
CPUs。FCFS 的優點是簡單，容易實作，也不
用 user 提供 jobs 的預估執行時間，但是卻會
面臨很多的 holes 的碎裂(fragmentation)情形，
如果有一些需要大量 CPUs 的 jobs 擋在前
面，會嚴重影響 system 的 performance[1, 9,
12]。
為了解決 FCFS 產生的碎裂問題，
Backfilling methods 是一個好的改善方案，
backfilling methods 建議前頭的 job 所要求的
CPUs 個數無法被系統提供時，可以允許其後
面有限個數較小的 jobs 往前移，以獲得資源先
行執行，這一類方法可以改變 jobs 執行的次
序，但是 user 必須提供 job 執行時間的預估
值，給 system 做判斷[1, 2, 3, 9, 12]。Backfilling
的策略中，有兩種最常見：分別是 aggressive
backfilling 和 conservative backfilling。
EASY (Extensible Argonne Scheduling
sYstem)是 aggressive backfilling 中的代表，它
由 IBM 在 SP1 平行電腦的環境下所發展出
來，因為效能不錯，後來也應用在 SP2 的電腦
上[1, 9, 12]。為了增加 job backfilling 的機會，
只要不會延遲到位於 queue 最前頭的 job 的執
行，即可將 CPUs 要求較少的 jobs 往前搬移優
先執行，詳細的演算法在[9]有描述。
Backfilling scheduling 的另一常見的方法
叫 conservative backfilling ， 這 種 方 法 與
aggressive backfilling 類似，queue 後面較小的
job 可以往前移。當每一個 job 進入系統時，
系統都會給予一個保留，保障其最晚開始執行
的時間。但是這種方法較保守，在後方往前移
動的 job不能延遲所有在 queue 前面中任何一
個 job 的執行，其詳細的演算法在[9]有詳述。
另外一個議題是，assign 給 job 的 CPUs
的集中程度如何。有一種希望 assign 到某個
job 的 CPUs，儘量集中在一起（或稱為連續），
這一類方法主要在 mesh connected 及 Torus 的
multicomputers 上提出[4, 5, 8, 11, 17]。我們知
道，在 mesh 及 torus 上，system topology 扮演
非常重要的地位，因此，若 assigned 的 CPUs
較為分散，communication overhead 會非常
大。針對這些架構早先方法建議寧可多花一點
waiting time，讓得到 CPUs 必須集中在一起，
以降低 communication overhead。但是會有
waiting time 過長，以及 external fragmentation
的 問 題 。 為 了 平 衡 waiting time 及
communication overhead，許多研究開始著重
所謂 non-continuous allocation strategies，例如
[4, 5, 11] 建 議 在 mesh 上 ， 而 [8] 則 在
multi-dimensional Torus 上，他們共同的特色
是建立 non-continuous 但 compact 的 processor
allocation strategy，除了減少 waiting time，又
可以減低 communication cost。
前面討論的方法，比較注重如何 allocate
CPUs 給需要的 job，但忽略下面幾個問題：
 這些 jobs 執行時對 memory resource 的要
求較少提及，如果 system 剩下來的
memory 不夠，執行這些 jobs 時會產生大
量的 page faults，結果會浪費許多的時
間。解決方案在[15]有詳細的討論。
 這些 jobs 執行時對 network bandwidth 的
要求，如果分配給這些 job 的 CPUs 的位
置，是集中在少數 SMPs，communication
的機會較小，因此對 network 的 load 也較
小，但會增加 waiting time；反之，如果分
散到許多 SMPs 上，waiting time 會降低，
但 communication 較多，對 network 的
contention 也較嚴重。
3. 研究方法及實驗結果
首先舉一個簡單的例子來說明我們的想
法(為了簡單說明，本例中系統的規模、CPUs
及 memory 的需求量都減少)，假設有一 cluster
of SMPs，共有 4 個 nodes (SMPs)，每一個 node
有 4 個 CPUs，8 M 記憶體。現在有一個 job
需要 4 個 CPUs，12MB 的記憶體，為了突顯
不同策略之間的差異，假設此 job 需要相對較
第 4 頁
這裡我們建一個簡單的 function，依實際
的分配情形來計算其 cost。
Communication cost = BASE *
number_of_SMPs * Level
其中 BASE 是 communication 的基本
cost (本實驗假設 0.05)；Level (從 1 到 5)
則表示快速網路的負載程度， 值越大表
示 communication cost 越大；number_of
_SMPs 則表示 Job 執行時，system 分配給
此 job 實際分布到的 SMPs 個數。詳情請
見表二。
表二﹕本實驗假設的 communication cost
function
通訊負擔比值 (BASE=0.05)Job 分布的
SMPs 個數 Level=1 Level=2 Level=4
1 0.05 0.1 0.2
2 0.10 0.2 0.4
3 0.15 0.3 0.6
4 0.20 0.4 0.8
5 0.25 0.5 1.0
6 0.30 0.6 1.2
7 0.35 0.7 1.4
8 0.40 0.8 1.6
9 0.45 0.9 1.8
10 0.50 1.0 2.0
舉例來說，假設某個 job 需要 4 個 CPUs，
如果 4 個 CPUs 在同一個 SMP 上，
communication cost 等於 0.05 * 1 *
Level；但是如果分散在 4 個 SMPs 上，
communication cost 等於 0.05 * 4 * Level。
•系統分配給欲執行的 job 所需 CPUs，
system 允許這些 CPUs 分佈的 SMPs 總數
不 可 以 超 過 (Tight +
Minimum_number_of_SMP)；其中
Minimum_number_of _SMP 為 CPUs 分佈
的 SMPs 最小值，例如某 job 需要 24
CPUs，則 Minimum_number_of_SMP 為
3，亦即這 24 個 CPUs 必須集中在某 3 個
SMPs 上;
Tight 則是 job 所得之 CPUs 的集中程度，
Tight 值越小表示分配到的 CPUs 落在的
位置越集中，反之則較分散。例如 Tight=0
時，job 需要的 CPUs 必須集中在
Minimum_number_of _SMP 個 SMPs 中，
當Tight=10，則完全不限定所得到的CPUs
的位置。
我們在模擬器上，利用前面提及的 trace，
在不同的網路通訊負擔(即調整不同的 Level
值)的假設之下，執行下列的實驗﹕以分配到
欲執行的 job所需 CPUs的 SMP 分佈的集中程
度當參數(Tight)，實作 FCFS 及 EASY 二種 job
scheduling algorithms。至於實驗的測量值，包
含所有 jobs 的下列平均值：Turnaround time、
Waiting time 、 Slowdown 、 Communication
overhead。
圖五為 EASY 和 FCFS 在各種 Level
下，以 Tight 為參數所測出 trace 中 2 萬多個
jobs 平均 slowdown 之比較。本實驗證實，在
所有情況之下，EASY 的效能的確比 FCFS 好
很多。因此，後面的數據僅顯示以 EASY 為基
礎的結果，FCFS 的部份將省略。
圖五﹕ EASY和 FCFS 平均 slowdown之比較
圖六是 EASY 演算法在各種 Level 下，
以 Tight 為參數所測出 trace 中 2 萬多個 jobs
之平均 communication cost 之比較，由此圖可
見﹕快速網路的負載程度越大(Level 的值越
大)或/及分配之 CPUs 較不集中(Tight 值越
大)，則 communication cost 越大。
圖六﹕EASY 演算法在各種 Level 下，以 Tight
為參數所測出之平均 communication cost
圖七是 EASY 演算法在各種 Level 下，
以 Tight 為參數所測出 trace 中 2 萬多個 jobs
之平均 turnaround time 之比較，由此圖可見﹕
1. 快速網路的負載程度較小時(Level 的值越
小)，turnaround time 會隨著 Tight 增加而遞
減，例如 Level = 0 時，當 communication cost
很低時，分配之 CPUs 不用考慮是否集中，因
此 waiting time 都不大，所以 Tight 越大
turnaround time 越小。2. 快速網路的負載程度
較大時(Level 的值越大)，除了一開始 Tight=0
之外，turnaround time 會隨著 Tight 增加而遞
增 ， Level 越 大 ( 例 如 = 4 時 ) ， 因 為
communication cost 很大，分配之 CPUs 若不
集中，communication cost 則會快速增加，因
此 turnaround 也會隨著快速增大。
0
200
400
600
800
1000
1200
1400
0 1 2 3 4 5 6
分配之CPUs的集中程度(T ight值越小越集中)
M
ea
n
C
om
m
un
ic
at
io
n
C
os
t(
秒
)
Level=1
Level=2
Level=3
Level=4
0
200
400
600
800
1000
1200
1400
Tight=0 Tight=2 Tight=4 Tight=6
EASY(L=0) FCFS(L=0)
EASY(L=2) FCFS(L=2)
EASY(L=4) FCFS(L=4)
