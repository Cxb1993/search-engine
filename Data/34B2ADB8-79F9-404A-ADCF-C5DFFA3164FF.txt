  
中文摘要 
部落格的普及化已成為目前網頁內容的重要型態，部落格包含一序列文章內含用戶的
意見和觀點。而被垃圾滲透的評論日益增加，使得部落格和社交網站已經開始成為一個潛
在的威脅和騷擾。因此本計畫提出一個基於資料探勘和機器學習技術來分析垃圾網站
2.0，我們主要目標是偵測 Twitter 中散播垃圾訊息的帳號，帶給使用者一個乾淨的網路環
境。在產生能幫助判斷垃圾訊息散播者的分類器之前，必須要找出 Twitter 中能幫助分類的
特徵，本研究利用文字探勘結合了資訊檢索模型產生基於內容的特徵，並且觀察使用者發
文情形產生寫作結構行為特徵。接著使用支持向量機結合以上兩種特徵向量後產生出分類
器，幫助在 Twitter 自動偵測出散播垃圾訊息的帳號。我們也評估了我們的偵測系統，我們
的研究結果證明，在我們評估的三種資訊擷取模式中，以資訊增益產生的效果最佳，我們
的偵測系統可以達到 98.3%的精確度和 96.5%的 F1-measure。 
關鍵字：垃圾網站 2.0、資訊擷取模式、資料探勘、機器學習、文字探勘、支持向量機。 
英文摘要 
Blogs are gaining constant popularity in recent years and have now become an important genre of 
web content. A blog is a type of web content which contains a sequence of periodic (dated) articles 
with user comments and opinions. The increasing penetration of spam in the form of comments in 
blogs and social networks has started becoming a nuisance and potential threat. In this project, we 
proposed a Spam 2.0 detector based on the data mining and machine learning methodologies. Our 
works focus on Spammer detection of Twitter to bring user a clean webspace. In preparation for 
spammer detection, we need to extract the meaningful features from Tweets. We apply Text Mining 
technique with Information Retrieval Model to generate content-based features, and we also 
investigate Tweets contents to generate user behavior features. Then, we use the Support Vector 
Machine (SVM) to train classifier that can be used for detecting spammers automatically in Twitter. 
We also evaluated our detection scheme based on the suggested user and content-based features. 
Our results show that among the three Information Retrieval Models we evaluated, the Information 
Gain produces the best results. Our spam detector can achieve 98.3% precision and 96.5% 
F1-measure. 
Keywords : Spam 2.0, Information Retrieval Model, Data Mining, Machine Learning, Text Mining, 
SVM. 
 2
(1) 內容為主 (Content-based)  
根據[16]的研究指出，開放的論壇討論區是目前受 web spam 影響最嚴重的一群。在該
研究的實驗中發現，使用最受歡迎的 189 個 search keyword 進行搜尋，幾乎都有 spammed 
forum 出現在 top 20 的搜尋結果中。其研究方法以內容分析並搭配 cloaking 和 redirection
為主。[5]則是透過文字統計的方式，針對不同語言的 web spam 進行偵測。Kolari 等人
[12]採取內容分析的方式，搭配 SVM 技術，透過自動分類設法找出 blogs 上的 spam 
content。 
(2) 鏈接結構為主 (Link-based) 
Benczur 等人提出了 SpamRank 觀念[5]：一種可以透過辨識因鏈接到其他網頁而超過
power law distribution 的網頁的方式來偵測 spam 網頁。他們觀察到利用網頁鏈接的相
似性要比單純以鏈結的有無表示信任與否更能建立一個好的 spam classifier。 
另外，Cloaking 和 redirection 技術則是大量的被 spammer 使用在欺騙 browser 和 crawler，
以避免其 spam 網頁太過暴露而被輕易查覺。為此，Wu 等人[31]提出了一個可以自動
偵測 cloaking 的方法，他們是第一個利用比較有嫌疑的網頁在 crawler 回報與 browser
瀏覽內容之間的不同，並利用 machine learning 訓練一個 classifier 來協助判斷 web 
spam。而[10]則是認為由於 spammer 的目的不外乎是金錢，因此觀察那些最受歡迎和最
能夠帶來利益的搜尋關鍵字將不難發現其和 cloaking 的發生有密切的關係。 
(3) 整合並用 (Integrating) 
Lin 等人[14]以 histogram intersection similarity 來量測 blogs 的發布時間、內容與鏈接，
並做成相似矩陣，接著根據不同類型的網誌（正常 blogs 或 splogs）把這些特徵的關係
顯示在視覺化圖表上，最後從此相似矩陣中計算出有用的時間特徵值並以 SVM 來實作。 
(4) Spam 2.0 偵測方法 
Hayati和Potdar [11]分析了各種偵測Spam 2.0 的方法，其分析的web 2.0 範圍包括Blogs, 
comments, forums, opinions, online communities, wikis等，他們以偵測策略及防治策略為
考量來評估各種各反Spam 2.0 方法，他們認為目前的方法會增加使用者及系統互動的不
便，因此需要研究更自動且精緻之防治工具。 
Lee等人[13]提出一個偵測Twitter Spammer新的方法，主要是利用 “Honeypot” 吸引
Twitter上的Spammer攻擊，目的是追蹤主動發出 " Follow Honeypot" 請求的使用者，分
析其是否為 Spammer，並對其做行為分析，總共收集了 500 位使用者做為訓練集，並
使用交叉驗證。 
Benevenuto等人[7]的論文中，每位使用者有 39 個內文的特徵，例如：內文數字出現次
數、"RT @username"出現次數、Spam word出現次數…等，以及 23 個使用者行為的特徵，
例如：使用者年齡、使用者follower人數以及follow多少人、使用者帳號名稱是否出現
Spam word…等，總共 61 個特徵，並使用人工標註了 1065 個使用者，其中包含 710 個
正常使用者，355 個Spammer，最後使用SVM做訓練。 
Wang 錯誤! 找不到參照來源。所發表的論文，以圖形和內容基礎為的特徵來檢測垃圾
信息發送者。圖形的特徵包括追隨者數量，朋友的數量（你追隨的人的數量）和聲譽評
分(這是定義追隨者數量與追隨者數量的總和使用者數量之間的比例)。內容的特徵包括
內容的相似性、在最近的 20 個tweets中出現HTTP鏈接的tweets個數、在最近的 20 個
生分類器，最後利用此分類器分析未知的使用者是否為 Spammer。我們採用的內容特徵有 4
項，寫作行為特徵有 5 項。 
3.2.1 內容特徵 
(1)鏈接數量：若鏈接(URL)數越高的話，Spammer 可能的機率將會增加。使用者發文中
的鏈接位址，其正規表示式如下： 
(http:\/\/)?((\w|-|_)+\.)+\w\/?(\w|\.|\/|=|\?|-|_)*\/?                   (1) 
其中(http:\/\/)?代表此 URL 是否用 http://開頭皆可，((\w|-|_)+\.)+\w\/? 代表必須為英文
或數字開頭，由 “.” 符號結尾，並重複一次以上，接著必須接英文或數字，並且結尾是
否以 “/” 做結束皆可，所以此段正規表示式是在匹配 URL 中 Domain Name 部分。最
後一段 (\w|\.|\/|=|\?|-|_)*\/? 是在匹配 Domain Name 之後的部份，此表示式代表包含英文
字、數字、“.”符號、“/”符號、“=”符號、“?”符號、“-”符號，“_”符號，出現一次以上，
並且結果是否以 “/” 做結束皆可。 
(2) “@Replies and Mentions” 數量：Twitter 中為了方便使用者，提供了 “@” 符號增加使
用者的便利，此符號讓使用者方便在 Twitter 中回覆或提及某人，並可讓使用者利用該
符號做搜尋，並且兩位使用者若互相 Follow，則利用此符號在 Twitter 中提及某人，則
該 Twitter 會出現於該使用者的首頁的 “@Mentions”頁面。Spammer 利用此工具吸引他
人注意，利用不斷的發表類似 '@username nice to meet you'，吸引他人前往 Spammer 家
目錄觀看文章，進而點閱相關鏈接，此特徵將計算使用者 20 篇發文中的 “@” 數目，
若數目越高，則 Spammer 可能的機率將會增加。 
(3)標籤（hashtags）數量：Twitter 上的訊息可以採用添加「標籤」形式進行分類，即在
單詞、短語前面加上一個 # 構成的標籤。Spammer 會在內文上標註熱門的主題，使其
發表的新文章出現在 Twitter 的搜尋結果中，進而吸引大眾注意，此特徵將計算使用者
前 20 篇發文中的 “ # ” 數量，若 “ # ” 越多，則 Spammer 的機率越大。 
(4)文字重複比率：有別於一般使用者，Twitter 上的 Spammer 可能發表許多類似內容的文
章，若使用者重複張貼相同的內容，此使用者為 Spammer 的可能性也增加，文字重複
比率算式如下： 
                        (2) 
3.2.2 寫作行為特徵 
(1)平均發表時間：意指使用者平均多久發文一次，經由我們觀察發現，由於許多 Spammer
透過 Twitter 提供的 API，撰寫程式自動化發表文章，造成發文時間間隔非常短的現象。
由此可見若短時間內頻繁發表 Tweets 的使用者很可能為 Spammer，本研究將利用式(3)
計算特徵值： 
                     (3) 
本研究抓取各個使用者 20 篇文章，其中  代表第 i 篇發文的時間，  為計算兩篇
發文的時間間隔，單位為分鐘。 
 4
 圖 2 文字探勘流程圖 
 
文字探勘主要程序為： 
1. 使用者資料前置處理程序：對文章中的個別詞彙做 Stemming ； 
2. 詞彙向量化：採用向量空間模型（Vector Space Model）將文章轉成一個向量； 
3. 詞彙特徵選取：選取特徵的方法有很多，如詞頻（TF）、詞頻-逆向文件頻率（TF-IDF）[18]、
信息增益（Information Gain）[20]、卡方積（Chi-square）[19]等。 
4. 詞彙排名、設定 Threshold – σ：經詞彙特徵選取演算法計算後的結果進行排序，並選取
排名前σ個詞彙。 
5. 詞彙特徵擷取並導入 SVM。 
 
4. 研究結果與討論 
4.1 資料集 
本研究利用 Twitter 提供的 API 透過網路爬蟲收集資料並使用人工標記出 2364 筆
Legitimate User，並透過搜尋 “@spam” 字串標記出 636 筆 Spammer，總共蒐集了 3000 筆資
 6
 8
表 2 不同資料集大小實驗數據比較。 
資料集大小 詞彙篩選方法 σ TPR(Recall) FPR Precision F1 Measure Accuracy 
1000 TF-IDF 30 0.984375 0.016878 0.940299 0.961832 0.983389 
2000 TF-IDF 30 0.9375 0.004228 0.983607 0.96 0.983361 
3000 Information Gain 30 0.947644 0.004225 0.983696 0.965333 0.985572 
 
下表為以 3000 筆資料集做為本研究實驗數據並與其他論文的實驗結果做比較。由結果看
來，本研究優點包括：TP 較高、FP 較低、Precision 較高、F1-Measure 較高、Accuracy 較高。
整體而言，本研究實驗數據略勝於其他論文實驗數據。 
 
表 3 各個實驗使用特徵數量與結果比較 
研究文獻 特徵數量 TPR(Recall) FPR Precision F1 Measure Accuracy 
[13]   0.057  0.888 0.889 
[7] 61 0.91 0.036 0.701 0.791 0.89 
[7] 5 0.917  0.917 0.917  
[8] 25 0.898 0.0034  0.908 0.937 
本研究 39 0.947644 0.004225 0.983696 0.965333 0.985572 
 
5. 結論與建議 
本研究採用 4 項內容特徵及 5 項寫作行為特徵，並輔以文字探勘技術以萃取其他詞彙特徵
值，這些特徵所形成的向量集，藉由 SVM 分類器來訓練，經由訓練後的分類器來偵測垃圾微
網誌。由實驗結果發現，文字探勘所選取的詞彙特徵，並不一定越多越好，只取前幾名詞彙反
而能做出更精確的分類器。實驗中採用 9種內容特徵及寫作行為特徵，再加上文字探勘萃取的
前 30 名詞彙做為本研究的特徵值，實驗結果顯示了 0.9476 的偵測率，以及 0.0042 的誤報率，
皆優於目前所發表的研究成果。 
目前針對 Spam 2.0 的研究仍僅集中在英文語系的網頁，能取得網頁資料集也大多為英文內
容，針對中文語系網頁的研究則十分少見，可能原因是中文在內容語意的分析上較為困難之
故，未來我們將朝此方向努力發展。 
 
參考文獻 
[1] 维基百科, http://zh.wikipedia.org/wiki/Web_2.0. 
[2] http://royal.pingdom.com/2010/02/10/twitter-now-more-than-1-billion-tweets-per-month/. 
[3] http://search.twitter.com/api/. 
[4] http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 
[5] N. Alexandros, N. Marc, M. Mark and F. Dennis, Detecting spam web pages through content   
 10
計畫成果自評 
學術成就：本計畫已針對 Twitter 中 Spammer 的分類成功的開發出一套具有 98.3%以上偵測
率的分類器，並且在這套系統中建立出網路爬蟲，以及屬於 Twitter 的 Stammer，
並對使用者兩種不同類別的特徵個別做處理，產生出使用者寫作行為特徵以及內
容特徵，並使用機器學習理論中的 SVM 產生分類器，在這之中對分類器做最佳
參數調效，用以產生最佳的分類器，最後使用三種文字探勘的技術產生出不同的
特徵並加以比較，不僅對這三種方法加以比較，並且可產生出更佳的分類器，本
研究在偵測能力上有精確的表現，在偵測對象的廣度上，也能提供使用者多元的
保護。 
對社會影響：本系統利用 data mining 與 machine learning 的研究技術，將提供對於新型未知
的 spam 的偵測能力，能更有效的偵測出可能隨時大量爆發的 spam 2.0 攻擊。 
進一步發展之可能性：中文語系網頁的 spam 研究。 
域的專家，也是大會的議程主席。接著大會舉辦一個Panel Discussion，
主持人為IEEE-SMC的副主席Daniel Yeung教授，主談的議題為「The 
Genesis of an Innovative Research Topic」，這是個滿有趣的議題，與會來
賓也十分踴躍的提出自己的想法。12日下午還安排博士生論壇(PhD 
Colloquia)，以鼓勵優秀年輕學子從事此領域相關研究；晚上的餐會為來
自世界各地的專家學者洗塵，會中並表揚大會傑出論文獎。在會中也結
識了來自台灣義守大學的歐陽振森教授及三位中山大學的碩博士生，真
是他鄉遇同行倍感親切，會後我們一起至當地的知名景點參觀。 
本次研討會議程包含廣泛，投稿相當踴躍，共有來自許多個國家，
多篇論文投稿，此次參加會議的學者人數眾多，達到相關領域的創新技
術作充分交流的目的。相關主題如下： 
Adaptive systems            Neural net and support vector machine 
Business intelligence         Hybrid and nonlinear system 
Biometrics                 Fuzzy set theory, fuzzy control and system 
Bioinformatics              Knowledge management 
Data and web mining         Information retrieval 
Intelligent agent             Intelligent and knowledge based system 
Financial engineering         Rough and fuzzy rough set 
Inductive learning            Networking and information security 
Geoinformatics              Evolutionary computation 
Pattern Recognition           Ensemble method 
Logistics                   Information fusion 
Intelligent control            Visual information processing 
Media computing            Computational life science 
 
研討會議程分成 11 個 sessions，每個 session 分 4 組同時進行，每
個 session 安排 7 篇以上論文發表，使得在時間的掌控上不是很理想。
現場亦有壁報論文發表及發放明年度相關研討會的徵稿信息，內容相當
豐富。我們本次被接受的論文便是安排在主題 Machine Learning and 
Image Processing 中發表，論文題目為 DETECTION OF PACKED 
EXECUTABLES USING SUPPORT VECTOR MACHINES。會議投稿論
文皆經過兩名匿名學者嚴格審查，可謂質與量俱佳。其中與本研究較相
關的研究偵測電腦惡意行為的論文有 3 篇，分別為 REPEF – A SYSTEM 
FOR RESTORING PACKED EXECUTABLE FILE FOR MALWARE 
ANALYSIS, AOS: AN OPTIMIZED SANDBOX METHOD USED IN 
BEHAVIOR-BASED MALWARE DETECTION 及  THEORETICAL 
ASSESSMENT OF IMMUNOGENICITY VARIATION ON THE HA 
PROTEIN OF H2N2 INFLUENZA VIRUS BY USING FUZZY 
 2
的教育、科技、觀光上的發展，對於島國的台灣，更應開闊自己的視界
與心胸，放眼國際。會議地點桂林是廣西壯族自治區最重要的旅遊城
市，享有桂林山水甲天下之美譽。桂林的峰林地貌和秀麗山水，造就不
少經典的文學篇章和藝術作品的誕生。由著名導演張藝謀導演的《印象
劉三姐》，以漓江陽溯段作為實地場景，演出人員逾600人，全為當地漁
民。長達60分鐘的表演，將桂林山水和民族風情巧妙融合，令人眼界大
開，只是人潮多，蚊潮也不少。秀麗山水，清新的空氣，36個少數民族
純樸的鄉民悠閒地生活在這美麗大地，桂林市既有豐富的自然景觀，又
有豐富的人文景觀，整個城市宛如度假村，享有山水甲天下之美譽。此
次透過國際研討會能與各國專家學者交換意見，是一場收獲極多的學術
之旅，真是讀萬卷書行萬里路！在此12 萬分的感謝國科會給予研究經
費的支持。 
三、建議 
個人覺得中國大陸上的學術研究風氣、能力與成果，這些年進步比
臺灣快。他們努力爭取各種國際會議主辦權，在學術上之發展，令人十
分驚奇。雖然因會場以當地學者及研究生居多，所以此次發表論文的作
者資質相當良莠不齊，有些人或許因為太緊張而導致在台上吞吞吐吐，
時間控制也不理想，場地安排也不是很理想，但大陸以舉辦各種國際會
來操練其學生的學術競爭力及國際觀，假以時日其軟實力，也很快能與
西方國家並駕齊驅。在國際研討會上較少看到台灣留學生研究生的參
與，實在可惜！期盼教育部、國科會與各大學，能多加鼓勵國內研究生
參與國際研討會，以提昇我國之學術國際化的能力與培養卓越的大學。
也建議國立大學及各學門學會應結合國內學術及業界，多爭取主辦重量
級國際研討會，以增加國內研究生的競爭力，並提高國內研究風氣，增
加台灣的能見度。此行也見識桂林山水結合少數民族文化、藝術以發展
觀光，值得台灣觀光發展借鏡。 
四、攜回資料名稱及內容 
Proceedings of the International Conference on Machine Learning and 
Cybernetics 2011 (ICMLC2011) 論文集及光碟版 
五、發表的論文 
 
 4
which the proposed PDF is canvassed in section four and 
section five first clarifies some notations we used for 
evaluation and then demonstrates the experiment results of 
PDF. Finally, some concluding remarks and future works 
are made in the last section. 
2. Background Knowledge 
The PE file played a critical role in our study; however, 
we skipped the content of PE file due to the limited space in 
the paper. Materials described PE file format in details can 
be found in [9, 14].  To know how a packer work, we use 
Fig. 1 as an example to show that using different kinds of 
camouflage packers has given malware writers a great 
opportunity to ensure their newly-created malware or extend 
the life cycle of known malicious codes. Moreover, a more 
obscure form can be easily achieved by recursively apply 
various packers on a given malware. As we can see in Fig. 1, 
the packer P1 compresses or encrypts the code and data 
section of an executable E into a new section and modifies 
some entries of PE file such as the instruct pointer (IP) and 
section header to create a packed executable E1 that 
appended a reserved empty section and unpacking codes of 
P1. Similarly, reproduction of this packing process can be 
easily achieved by applying another packer P2 on E to create 
E2 or recursively on E1 to create E3. Once E1 is executed, the 
unpacking routine, as unpacking phase demonstrates, first 
decompresses and/or decrypts the packed data into the 
reserved space and then resumes the IP to the original entry 
point of E.  
The working process of packers demonstrates the fact 
that series of alterations on PE file would take place by a 
running packer. So, it is naturally to assume that the clues of 
modification left by packers could be valuable for packing 
detection. Namely, alterations which can seldom be found in 
normal executables but located in packed ones could be used 
to distinguish packed executables from non-packed ones. 
Besides, the Dynamically Linked Libraries (DLLs) and 
Application Programming Interfaces (APIs) to which an 
executable refers for managing file, configuring interface, 
accessing network, etc. could be viewed as the 
representation of a program’s behaviors. This concept has 
proven that analyzing those behavioral attributes is useful 
for detecting malware owing to the differences of function 
between malware and benign problems [20, 21]. Therefore, 
based on the same idea, we also tried to analyze the usage of 
DLLs and APIs to distinguish packed executable from non-
packed executable. 
 
Figure 1.  How a packer works 
3. Related Work 
For the sake of counterattacking the packing techniques 
which have caused some hindrance to malware detection, 
methodologies of packing detection has increasingly been 
the object of study in recent years. In [8], the authors 
exploited the property of compressing and proposed 
Bintropy—a file entropy analysis tool. Owing to the fact that 
the distribution of byte of a compressed executable is 
usually more uniform than a normal one, higher entropy 
value could be viewed as an indicator of the existence of 
compressed data. The authors have shown the efficacy of 
their approach, however, there were limitations such as false 
negatives when dealing with packed executables larger than 
500kbytes as well as false positive when facing some 
binaries with high variability. In order to save the amount of 
processing time of universal unpacking tools, [11] proposed 
an packing detection architecture which extracted nine 
features from PE file to train six classifiers by using 
different learning algorithm. McBoost [12], an advanced 
study of [11], was designed as a malware detection tool that 
also took the detection and unpacking of packers into 
account. They combined two feature extraction strategies—
heuristics and n-gram byte analysis and two classification 
algorithm included multi-layer perceptron and bagged 
decision tree to have their system’s packing detection ability 
progressed. 
4. Packing detection Framework 
Comparing to previous packing detection studies that 
used limited information from PE file as heuristic rules 
which could be easily circumvented by sophisticated 
malware writers, we tried to exploit more entries in PE files 
through a process of attribute extraction and refinement (see 
Fig. 2) to generate a set of valuable features. Using these 
features in tandem with support vector machines (SVM)—a 
classification methodology, our PDF is able to detect both 
known and unknown packers. In the following subsections, 
each step of PDF is entirely stated. 
718
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
Boolean ones beforehand. Moreover, some of the numeric 
attributes such as address of entry point, base of code or data, 
etc. were also binarized in our case because higher address 
does not necessarily reflects to higher importance. To 
binarize a numerical attribute A, after sorting its values we 
bisected its values one by one to find a best split point S* 
with its resulting two intervals are most purely distributed 
over the classes. Then the attribute can be binarized into 
A=0 if A<S* and A=1 if A≥S*. Similarly, nominal attributes 
can also be binarized by repeatedly grouping all values 
except the one of interest together as a negation of this value. 
If S* is the split value with purest intervals, then A=0 if 
A≠S* and A=1 if A=S*. To measure the purity of intervals, 
an entropy-based method: information gain (IG), proposed 
by Quinlan [15],  was leveraged in this study. Suppose that 
X be a finite set of examples, provided that there are k 
classes {C1, C2, ..., Ck} in X, then the entropy H(X) of X is 
defined in (1), where Pi is the proportion of data with class i 
in X. The higher the value of entropy, the higher the 
randomness of data distribution is. Let {A1, A2,…, An} be a 
set of attributes and Ai has q values {Ai1, Ai2 , ..., Aiq}, (2) 
implies the entropy of X conditioned by the values of Ai and 
|X ij| is the number of examples in X with attribute Ai = Aij. 
Then IG is given by (3) reflects additional information about 
X provided by Ai. The higher the IG is, the purer of the 
distribution of examples in X over k possible classes we 
have. For binarization, we modified (3) into (4) to decide the 
best splitting value for numerical and nominal attributes 
respectively. 
   (1) 
  (2) 
  (3) 
, x:num. 
or 
    , x:nom. (4) 
4.4 Attribute elimination 
Training a learning classifier with a large number of 
attributes could suffer from not only a heavy computation 
overload during the training process, but also the overfitting 
problem—classifiers have high performance on training, but 
low generalization ability on testing [6]. Elimination of 
uninformative attribute is, therefore, an essential step. In our 
attribute refinement process, we first discarded attributes 
with only one kind of value among all binaries due to their 
lack of discrimination. For example, the “kernel32.dll”, a 
basic DLL that be imported by almost every executable, 
would be neglected during this process. Also, we referred to 
the concept of “support” of association rule to filter 
attributes, all of which with support values less than 10% 
because of their lack of representativeness. 
4.5 Attribute transformation 
In order to mitigate the effect of units or scales of 
different attributes, rescaling is crucial especially when the 
relationships among attributes are calculated via distance 
function. In general, normalization and standardization are 
two kinds of linear transformation used for rescaling. Let vi 
be the value of an input variable v for the ith datum,  be the 
mean and s be the standard deviation of v. Equation (5), as 
shown below, tries to scale vi to midrange=0 and range=2 
while (6) standardizes vi to mean=0 and standard 
deviation=1.  
  (5) 
  (6) 
However, as we can see in the upper part of Fig. 3, 
variables such as number of imported APIs, size of code are 
often with wide range of value due to the effect of some 
extreme values. Also, since those attribute values were 
bounded below by zero, the situations of right-skewness and 
increasing spread with increasing level could both occur in 
the distribution of attribute values. Since linear 
transformation can only change the origin and scale of a 
distribution instead of alternating its shape, nonlinear 
transformation should be taken to deal with this heavy-tailed 
value distribution problem. The procedure of attribute 
transformation is described as follows. First of all, logarithm 
function, which tends to squeeze together the larger values 
and stretches out the smaller values, was used  based on the 
suggestion in [4] to transform attribute values if the largest 
value is more than 20 times the smallest value. When those 
attribute values are re-expressed in the log scale, the middle 
part of Fig. 3 shows not only the shrink in value range, but 
also the stabilization of the value distribution. Next, (5) and 
(6) were carried out for normalizing and standardizing 
respectively. The attribute values standardized in previous 
step were further linearly normalized by (5) or nonlinearly 
normalized by arctan function to reduce the kurtosis of value 
distribution and map attribute values in the range of (-π/2, 
π/2). The lower part of Fig. 3 demonstrates the results of 
nonlinear normalization. 
720
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
of Table 2 gives a clear indication of the merit of the usage 
PE file entries for packing detection. 
TABLE II.  EVALUATION RESULTS OF PDF 
  c  CV 
OA of 
Testing 
TP Rate of
Prediction 
L 2-5  99.97% 100% 88.40% Log + 
Normalize K 23 2-13 100% 99.93% 85.67% 
L 2-5  99.97% 100% 89.42% Log + 
Standardize K 2-1 2-7 99.97% 99.93% 94.54% 
L 2-5  99.97% 100% 88.40% Log + 
Standardize
+ Normalize K 2
3 2-13 100% 99.93% 85.67% 
L 2-5  99.97% 100% 89.08% Log + 
Standardize
+ Arctan K 2
5 2-13 99.97% 100% 85.32% 
 
  c  CV 
OA of 
Testing 
TP Rate of 
Prediction 
L 25  99.80% 14.58% 78.84% LS-Detector 
(23 heuristic 
attributes) K 215 2-11 99.87% 39.71% 56.60% 
L: Linear SVMs, K: Kernel SVMs, LS: Log + Standardize, 
6. Conclusion 
In this study, we proposed a packing detection 
framework which considers both heuristic attributes and PE 
file entries as indicators for disclosing the differences 
between packed and non-packed executables. Besides, an 
attribute refinement process was also taken place in the 
framework to filter out the uninformative attributes and 
reduces the right-skewness and high kurtosis effects among 
attribute values. Finally, linear and non-linear SVMs 
classifiers constructed the core of packing detection. Our 
PDF showed promise as an effective solution not only for 
the known packers but also for the unknown ones which are 
the most dangerous threats to computer security. Although 
the results of this study are encouraging, it is only a 
preliminary and much remains to be done. In the future 
work, we intend to integrate our PDF with the functions of 
generic unpacking and malicious code detection to present a 
total solution against malware. 
Acknowledgements 
This work was partially supported by the National Science 
Council under the contract no. NSC99-2221-E-158-005. 
References 
[1] C.-C. Chang and C.-J. Lin, “LIBSVM : a library for support vector 
machines,” 2001; Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
[2] C. Collberg, C. Thomborson and D. Low, “Manufacturing Cheap, 
Resilient, and Stealthy Opaque Constructs,” In Proceedings of the 
25th ACM SIGPLAN-SIGACT symposium on Principles of 
programming languages, 1998. 
[3] N. Cristianini and J. Shawe-Taylor, An Introduction to Support 
Vector Machines, Cambridge University Press, 2000. 
[4] J.D. Emerson and M.A. Stoto, “Transforming Data,” Understanding 
Robust and Exploratory Data Analysis, John Wiley, 1983, pp. 97-128. 
[5] F. Guo, P. Ferrie and T.-C. Chiueh, “A Study of the Packer Problem 
and Its Solutions,” In Proceedings of the 11th international 
symposium on Recent Advances in Intrusion Detection, 2008. 
[6] D.M. Hawkins, “The Problem of Overfitting,” Journal of Chemical 
Information and Computer Sciences, vol. 44, no. 1, pp. 1-12, 2003. 
[7] S. Hua and Z. Sun, “A Novel Method of Protein Secondary Structure 
Prediction with High Segment Overlap Measure: Support Vector 
Machine Approach,” Journal of Molecular Biology, vol. 308, no. 2, 
pp. 397-407, 2001. 
[8] R. Lyda and J. Hamrock, “Using Entropy Analysis to Find Encrypted 
and Packed Malware,” IEEE Security and Privacy, vol. 5, no. 2, pp. 
40-45, 2007. 
[9] Microsoft, Microsoft Portable Executable and Common Object File 
Format Specification, 2008. 
[10] E. Osuna, R. Freund and F. Girosit, “Training Support Vector 
Machines: an Application to Face Detection,” In Proceedings of the 
Computer Vision and Pattern Recognition. Proceedings., IEEE 
Computer Society Conference on, 1997, pp. 130-136. 
[11] R. Perdisci, A. Lanzi and W. Lee, “Classification of Packed 
Executables for Accurate Computer Virus Detection,” Pattern 
Recognition Letters, vol. 29, no. 14, pp. 1941-1946, 2008. 
[12] R. Perdisci, A. Lanzi and W. Lee, “McBoost: Boosting Scalability in 
Malware Collection and Analysis Using Statistical Classification of 
Executables,” Proceedings of the 2008 Annual Computer Security 
Applications Conference, 2008. 
[13] C.P. Pfleeger, “Crypto: Not Just for the Defensive Team,” Security 
& Privacy, IEEE, vol. 8, no. 2, pp. 63-66, 2010. 
[14] M. Pietrek, “Peering Inside the PE: A Tour of the Win32 Portable 
Executable File Format,” 1994; http://msdn.microsoft.com/en-
us/magazine/ms809762.aspx. 
[15] J.R. Quinlan, “Induction of Decision Trees,” Machine Learning, vol. 
1, no. 1, pp. 81-106, 1986. 
[16] A. Stepan, “Improving Proactive Detection of Packed Malware,” 
2006; 
http://www.virusbtn.com/virusbulletin/archive/2006/03/vb200603-
packed. 
[17] G. Taha, Counterattacking the Packers., McAfee Avert Labs, 
Aylesbury, UK, 2007. 
[18] S. Tong and D. Koller, “Support Vector Machine Active Learning 
with Applications to Text Classification,” J. Mach. Learn. Res., vol. 
2, pp. 45-66, 2002. 
[19] V.N. Vapnik, Statistical Learning Theory, John Wiley & Sons, 1998. 
[20] T.-Y. Wang, et al., “A Surveillance Spyware Detection System 
Based on Data Mining Methods,” In Proceedings of the IEEE 
Congress on Evolutionary Computation, 2006, pp. 3236-3241. 
[21] T.-Y. Wang, C.-H. Wu and C.-C. Hsieh, “A Virus Prevention Model 
Based on Static Analysis and Data Mining Methods,” In Proceedings 
of the IEEE 8th International Conference on Computer and 
Information Technology Workshops, 2008, pp. 288-293. 
 
 
722
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
99 年度專題研究計畫研究成果彙整表 
計畫主持人：吳金雄 計畫編號：99-2221-E-158-005- 
計畫名稱：結合內容,鏈接和寫作結構之垃圾網站 2.0 偵測技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 1 60% 
篇 
研討會論文撰寫
中 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 1 50% 
篇 
研討會論文撰寫
中 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
