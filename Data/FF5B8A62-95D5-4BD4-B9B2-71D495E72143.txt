2Contents
1 Introduction 3
2 Fault/Noise Injection-Based Learning 5
2.1 Additive input noise injection training . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Weight noise injection training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Multiplicative node noise injection . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 Multiweight fault injection training . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.5 Multinode fault injection training . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.6 Weight decay-based multinode fault injection training . . . . . . . . . . . . . . . . 7
3 Main Results 7
3.1 Additive input noise injection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 Weight noise injection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.3 Multiplicative node noise injection . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.4 Multiweight fault injection training . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.5 Weight decay-based multinode fault injection training . . . . . . . . . . . . . . . . 10
4 Noise injection-based learning versus explicit regularization 11
5 Conclusions 12
A Lemma 1 15
4case of injecting weight noise, similar result has been obtained in [1], [2], [3], [4], [51]. That is, if
weights of a well-trained network are corrupted by weight noise, its prediction error is equivalent
to mean square error plus a regularizer. Here, we should point out that the prediction error of a
neural network being injected by noise is diﬀerent from the objective function of a noise injection
learning algorithm. The former one does not need the information regarding to the learning
algorithm. The latter one requires to consider learning algorithm.
G.An in [1] presented the objective functions for the online back-propagation training with
additive weight noise injection (see Section 4 in [1]), in which the noise is deﬁned as either a mean
zero Gaussian noise or a mean zero uniform noise. However, the approach that G.An followed
is based on the derivation of the prediction error. Even G.An applied a theorem from stochastic
gradient descent [8], [9], the proof on the convergence of the on-line weight noise injection learning
has not accomplished. We will show later in this paper that a penalty term (the Equation (4.7)
in [1]) derived by G.An for weight noise injection during training is not totally correct.
As analysis on the objective functions and convergence of those on-line noise/fault injection
learning has far from complete, further investigation along this line is inevitable. Therefore, the
goals of this paper are, 1) to prove convergence of fault/noise injection-based on-line learning
algorithms for RBF networks, 2) to derive their corresponding objective functions, and 3) to
elucidate the diﬀerences and similarities amongst noise injection-based learning algorithms and
other on-line learning algorithms. The following six fault/noise injection-based on-line learning
algorithms will be investigated.
(Algorithm 1) Injecting additive input noise during training [6], [44],
(Algorithm 2) Injecting additive/multiplicative weight noise during training [35], [36],
(Algorithm 3) Injecting multiplicative node noise during training,
(Algorithm 4) Injecting multiweight fault during training [53],
(Algorithm 5) Injecting multinode fault [46], [7] during training, and
(Algorithm 6) Weight decay with injecting multinode fault [13].
Instead of considering the prediction error, this paper investigates the properties of the update
equations of these six algorithms, given by
w(푡+ 1) = w(푡) + 휇푡F(x푡, 푦푡,w(푡),b푡), (1)
where w(푡), (x푡, 푦푡), and b푡 are the estimated weight vector, a randomly selected training data,
and a random vector controlling the fault/noise during the 푡푡ℎ step, respectively. The parameter
휇푡 is the step size and the vector function F(⋅, ⋅, ⋅, ⋅) is the update function to be deﬁned later.
In this paper, the expected solution for which w(푡) converges will ﬁrst be deduced. Then, the
convergence of w(푡) with probability one is proved by applying the Gladyshev Theorem [16].
Subsequently, the corresponding objective function being minimized is devised. Moreover, we
62.1 Additive input noise injection training
For an RBF network trained by injecting additive input noise, the update equation is given by
w(푡+ 1) = w(푡) + 휇푡(푦푡 − Φ푇 (x˜푡)w(푡))Φ(x˜푡), (7)
where x˜푡 is a noise version of the input vector, given by
x˜푡 = x푡 + b푡, b ∼ 풩 (0, 푆푏I푛×푛). (8)
In Equation (7) and (8), b푡 = (푏1,푡, ⋅ ⋅ ⋅ , 푏푀,푡)푇 is a random vector, and 푏푖’s are independently
identical zero mean Gaussian noise with variance equal to 푆푏.
2.2 Weight noise injection training
While an RBF network is trained by the idea of weight noise injection, the update equation is
given by
w(푡+ 1) = w(푡) + 휇푡(푦푡 − Φ푇 (x푡)w˜(푡))Φ(x푡), (9)
where the 푖푡ℎ element 푤˜푖(푡) of w˜(푡) is given by
푤˜푖(푡) =
{
푤푖(푡) + 푏푖,푡 for additive noise injection,
푤푖(푡) + 푏푖,푡푤푖(푡) for multiplicative noise injection,
(10)
2.3 Multiplicative node noise injection
For an RBF network trained by injecting multiplicative node noise, the update equation is
given by
w(푡+ 1) = w(푡) + 휇푡(푦푡 − Φ˜푇 (x푡)w(푡))Φ˜(x푡), (11)
where Φ˜(⋅) = (휙˜1(⋅), 휙˜2(⋅), ⋅ ⋅ ⋅ , 휙˜푀(⋅))푇 . The 푖푡ℎ element is given as
휙˜푖(x푡) = (1 + 푏푖,푡)휙푖(x푡). (12)
2.4 Multiweight fault injection training
For an RBF network trained by injecting multiweight fault, the update equation is given by
w(푡+ 1) = w(푡) + 휇푡
(
푦푡 − Φ푇 (x푡)w˜(푡)
)
Φ(x푡), (13)
where w˜(푡) = (푤˜1(푡), 푤˜2(푡), ⋅ ⋅ ⋅ , 푤˜푀(푡))푇 . The 푖푡ℎ element is given as
푤˜푖(푡) = (1− 훽푖,푡)푤푖(푡), (14)
where 훽푖,푡’s are independent binary random variables with probability given by
푃 (훽푖,푡) =
{
푝 if 훽푖,푡 = 1
(1− 푝) if 훽푖,푡 = 0
(15)
for all 푖 = 1, 2, ⋅ ⋅ ⋅ ,푀 .
8Theorem 1 (Gladyshev Theorem [16]) For a recursive algorithm given by Equation (20), sup-
pose there are positive constants 휅1 and 휅2 such that for all w ∈ ℜ푚 the following conditions are
satisﬁed :
(C1) 휇푡 ≥ 0,
∑
푡 휇푡 =∞ and
∑
푡 휇
2
푡 <∞.
(C2) inf휀<∥w−w∗∥<휀−1(w −w∗)푇ℎ(w) < 0, for all 휀 > 0.
(C3)
∫ ∥푀(w, 휔)∥2푃 (휔)푑휔 ≤ 휅1 + 휅2∥w∥2.
Then for 푡→∞, w(푡) converges to w∗ with probability one.
Normally, the ﬁrst condition is satisﬁed obviously because the step size 휇푡 could be pre-deﬁned
as 푐
푡
(푐 is a constant) for all 푡 ≥ 1. Therefore, we skip the proof of Condition (C1) in the rest of
this section. To simplify the presentation, we let
H휙 =
1
푁
푁∑
푘=1
Φ(x푘)Φ
푇 (x푘), (22)
Q푔 =
1
푁
푁∑
푘=1
diag{휙21(x푘), ⋅ ⋅ ⋅ , 휙2푀(x푘)}, (23)
Γ =
1
푁
푁∑
푘=1
∇푥Φ(x푘)∇푥Φ(x푘)푇 , (24)
푌 =
1
푁
푁∑
푘=1
푦푘Φ(x푘), (25)
where 휙푖(x푘) in Equation (23) is the 푖
푡ℎ element of Φ(x푘).
3.1 Additive input noise injection
Suppose the variance of the input noise 푆푏 is small, the following theorem can be shown by
applying the Gladyshev Theorem
Theorem 2: For injecting additive input noise during training an RBF network, the weight
vector w(푡) will converge with probability one to
w∗ = (H휙 + 푆푏Γ)
−1 푌, (26)
where H휙, Γ and 푌 are given in Equation (22), (24) and (25). Besides, the corresponding
objective function ℒ(w∣풟) to be minimized is given by
ℒ(w∣풟) = 1
푁
푁∑
푘=1
(푦푘 − Φ푇 (x푘)w)2 + 푆푏w푇Γw. (27)
10
3.4 Multiweight fault injection training
Applying the Gladyshev Theorem for analyzing the properties of training an RBF network by
injecting multiweight fault (each weight is of fault rate 푝), the following theorem can be proved.
Theorem 5: For injecting multiweight fault during training an RBF network, the weight vector
w(푡) will converge with probability one to
w∗ = [(1 + 푝)H휙)]
−1 푌, (32)
where H휙 and 푌 are given in Equation (22) and (25). Besides, the corresponding objective
function to be minimized is given by
ℒ(w∣풟) = 1
푁
푁∑
푘=1
(푦푘 − Φ푇 (x푡)w(푡))2 + 푝w푇H휙w. (33)
Theorem 6: For injecting multinode fault during training an RBF network, the weight vector
w(푡) will converge with probability one to
w∗ = [H휙 + 푝(Q푔 −H휙)]−1 푌, (34)
where H휙, Q푔 and 푌 are given in Equations (22), (23) and (25). Besides, the corresponding
objective function to be minimized is given by
ℒ(w∣풟) = 1
푁
푁∑
푘=1
(푦푘 − Φ푇 (x푡)w(푡))2 + 푝w푇 (Q푔 −H휙)w. (35)
It is worthwhile to note that Equation (35) is also identical to the objective function derived
in [30]. In other words, injecting random node fault during training an RBF is able to improve
its ability to tolerate anticipated multinode random fault.
3.5 Weight decay-based multinode fault injection training
For weight decay-based multinode fault injection training, we will prove the following theorem.
Theorem 7: For injecting multinode fault during weight decay to train an RBF network, the
weight vector w(푡) will converge with probability one to
w∗ =
[
H휙 + 푝(Q푔 −H휙) + 휆
1− 푝I푀×푀
]−1
푌, (36)
12
where Q푔 is given by Equation (23).
By applying the idea of stochastic gradient descent, we can show that Equation (39) is the
objective function for the following on-line learning algorithm that is proposed by Bernier et al
in [2]1 :
w(푡+ 1) = w(푡) + 휇푡
{
(푦푡 − Φ푇 (x푡)w(푡))Φ(x푡)− 푆푏diag{휙21(x푡), ⋅ ⋅ ⋅ , 휙2푀(x푡)}w(푡)
}
. (40)
Comparing the update equation given by Equation (40) and the expectation of Equation (9),
we can see that they are corresponding to two diﬀerent learning algorithms. On the contrary,
it is worth to note that the update equation given by Equation (40) and the expectation of
Equation (11) are actually the same equation. In other words, the algorithms as given by
Equation (40) and Equation (11) are virtually the same. With the same initial conditions, both
equations will converge to the same solution.
As a result, we can conclude the following. To obtain a single output RBF that is able to
tolerate multiplicative weight noise, one could apply either injecting multiplicative node noise
during training based on Equation (11) or the regularization-based learning algorithm based on
Equation (40).
5 Conclusions
In this project, the six fault-injection-based on-line training algorithms for RBF networks have
been analyzed. Their corresponding objective functions have been deduced and their convergence
proofs have been shown. Except the case of injecting weight noise, all objective functions are of
similar form :
Mean Square Error + Regularizer.
For the case of injecting weight noise (either multiplicative or additive) during training, the
objective function is the Mean Square Error. Thus, we can conclude that on-line weight noise
injection training is not able to improve an RBF network tolerance to weight noise eﬀect. To
obtain an RBF network that is able to tolerate multiplicative weight noise, one could inject
multiplicative node noise during training, Equation (11). Owing to the similarities amongst
the objective functions derived for fault/noise injection-based algorithms a nd those based on
regularization approach, a discussion on their relations are presented.
References
[1] An G. The eﬀects of adding noise during backpropagation training on a generalization performance, Neural Compu-
tation, Vol.8, 643-674, 1996.
[2] Bernier J.L. et al, Obtaining fault tolerance multilayer perceptrons using an explicit regularization, Neural Processing
Letters, Vol.12, 107-113, 2000.
1Please refer to Equation (6) in [2].
14
[30] Leung C.S., J. Sum, A fault tolerant regularizer for RBF networks, IEEE Transactions on Neural Networks, Vol. 19
(3), pp.493-507, 2008.
[31] Leung C.S. and J. Sum Analysis on generalization error of faulty RBF networks with weight decay regularizer, in
Proceedings of ICONIP 2008, Springer LNCS, 2009.
[32] Matsuoka K., Noise injection into inputs in back-propagation learning, IEEE Transactions on Systems, Man, and
Cybernetics, Vol.22(3), 436-440, 1992.
[33] Moody J.E., Note on generalization, regularization, and architecture selection in nonlinear learning systems, First
IEEE-SP Workshop on Neural Networks for Signal Processing, 1991.
[34] Murata N., S. Yoshizawa and S. Amari. Network information criterion–Determining the number of hidden units for
an artiﬁcial neural network model, IEEE Transactions on Neural Networks, Vol.5(6), pp.865-872, 1994.
[35] Murray A.F. and P.J. Edwards, Synaptic weight noise during multilayer perceptron training: fault tolerance and
training improvements, IEEE Transactions on Neural Networks, Vol.4(4), 722-725, 1993.
[36] Murray A.F. and P.J. Edwards, Enhanced MLP performance and fault tolerance resulting from synaptic weight noise
during training, IEEE Transactions on Neural Networks, Vol.5(5), 792-802, 1994.
[37] Neti C. M.H. Schneider and E.D. Young, Maximally fault tolerance neural networks, IEEE Transactions on Neural
Networks, Vol.3(1), 14-23, 1992.
[38] Parra X. and A. Catala, Fault tolerance in the learning algorithm of radial basis function networks, Proc. IJCNN
2000, Vol.3, 527-532, 2000.
[39] Pedersen M.W., L.K. Hansen and J. Larsen. Pruning with generalization based weight saliencies: 훾OBD, 훾OBS.
Advances in Information Processing Systems 8 521-528, 1996.
[40] Phatak D.S. and I. Koren, Complete and partial fault tolerance of feedforward neural nets, IEEE Transactions on
Neural Networks, Vol.6, 446-456, 1995.
[41] Phatak D.S., Relationship between fault tolerance, generalization and the Vapnik-Cervonenkis (VC) dimension of
feedforward ANNs, IJCNN’99, Vol.1, 705-709, 1999.
[42] Phatak D.S. and E. Tcherner, Synthesis of fault tolerance neural networks, Proc. IJCNN’02, 1475-1480, 2002.
[43] Reed R., Pruning algorithms – A survey, IEEE Transactions on Neural Networks, Vol.4(5), 740-747, 1993.
[44] Reed R., R.J. Marks II & S. Oh, Similarities of error regularization, sigmoid gain scaling, target smoothing, and
training with jitter, IEEE Transactions on Neural Networks, Vol.6(3), 529-538, 1995.
[45] Antony W. Savich, Medhat Moussa and Shawki Areibi, The impact of arithmetic representation on implementing
MLP-BP on FPGAs: A study, IEEE Transactions on Neural Networks, Vol.18, 240-252, 2007.
[46] Sequin C.H. and R.D. Clay, Fault tolerance in feedforward artiﬁcial neural networks, Neural Networks, Vol.4, 111-141,
1991.
[47] Sher B.Y. and W.S. Hsieh, Fault tolerance training of feedforward neural networks. Proceedings of the National Science
Council, Republic of China (A), Vol-23, No.5, pp. 599-608, 1999.
[48] Sugiyama, M. and Ogawa, H., Optimal design of regularization term and regularization parameter by subspace
information criterion, Neural Networks, Vol.15, 349-361, 2002.
[49] Sum J., C.S. Leung, L. Hsu, Y. Huang, An objective function for single node fault RBF learning, in Proceedings of
TAAI’2007.
[50] Sum J., C.S. Leung and L. Hsu, Fault tolerant learning using Kullback-Leibler Divergence, in Proc. TENCON’2007
Taipei, 2007.
[51] Sum J., C.S. Leung and K. Ho, On objective function, regularizer and prediction error of a learning algorithm for
dealing with multiplicative weight noise, IEEE Transactions on Neural Networks Vol.20(1), Jan, 2009.
[52] Sum J. and K. Ho, SNIWD: Simultaneous weight noise injection with weight decay for MLP training, in Proceedings
of ICONIP 2009, Springer LNCS 5863, 2009.
[53] Takanami I., M. Sato and Y. P. Yang, A fault-value injection approach for multiple-weight-fault tolerance of MNNs,
Proc. of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, vol. III, pp. 515-520, 2000.
[54] Takase H., H. Kita and T. Hayashi, A study on the simple penalty term to the error function from the viewpoint of
fault tolerant training, Proc. IJCNN 2004, 1045-1050, 2004.
[55] Tchernev E.B., R.G. Mulvaney, and D.S. Phatak, Investigating the Fault Tolerance of Neural Networks, Neural
Computation, Vol.17, 1646-1664, 2005.
16
Since
퐴푢 =
푚∑
푖=1
푢푖푎푖, (43)
푤(퐴푢)푇 =
푚∑
푖=1
푢푖푤푎
푇
푖 , (44)
(퐴푢)푤푇 =
푚∑
푖=1
푢푖푎푖푤
푇 , (45)
푤푎푇푖 푤푎
푇
푖 = 푤푤
푇푎푖푎
푇
푖 , (46)
푎푖푤
푇푤푎푖 = 푤
푇푤푎푖푎
푇
푖 , (47)
푚∑
푖=1
푎푖푎
푇
푖 = 퐴퐴
푇 , (48)
the 2푛푑 term up to the 7푡ℎ terms in Equation (42) can be obtained as follows :∫
푤푤푇 (퐴푢)(퐴푢)푇푃 (푢)푑푢 = 푆푤푤푇퐴퐴푇 . (49)∫
푤(퐴푢)푇푤(퐴푢)푇푃 (푢)푑푢 = 푆푤푤푇퐴퐴푇 . (50)∫
푤(퐴푢)푇 (퐴푢)푤푇푃 (푢)푑푢 = 푆

푚∑
푖=1
푎푇푖 푎푖
)
푤푤푇 . (51)∫
(퐴푢)푤푇푤(퐴푢)푇푃 (푢)푑푢 = 푆푤푇푤퐴퐴푇 . (52)∫
(퐴푢)푤푇 (퐴푢)푤푇푃 (푢)푑푢 = 푆퐴퐴푇푤푤푇 . (53)∫
(퐴푢)(퐴푢)푇푤푤푇푃 (푢)푑푢 = 푆퐴퐴푇푤푤푇 . (54)
The 8푡ℎ term will then be given by∫
(퐴푢)(퐴푢)푇 (퐴푢)(퐴푢)푇푃 (푢)푑푢 =
∫ ∑
푖,푗,푟,푠
푢푖푢푗푢푟푢푠푎푖푎
푇
푗 푎푟푎
푇
푠 푃 (푢)푑푢,
= 3푆2
푚∑
푖=1
(푎푖푎
푇
푖 )
2 + 3푆2
∑
푖∕=푗
(푎푖푎
푇
푖 )(푎푖푎
푇
푗 )
= 3푆2
푚∑
푖=1
푚∑
푗=1
(푎푖푎
푇
푖 )(푎푗푎푗)
푇 .
Therefore, ∫
(퐴푢)(퐴푢)푇 (퐴푢)(퐴푢)푇푃 (푢)푑푢 = 3푆2퐴퐴푇퐴퐴푇 . (55)
Report on Conference Attendance
John Sum
Institute of Technology Management, National Chung Hsing University
250 Kuo Kuang Road, Taichung 40227
Taiwan R.O.C.
December 20, 2010
The conference that I attended is the International Conference on Neural Information Pro-
cessing Systems. It is the largest neural network conference in Asia. On December 2009, this
conference was held in Thailand. During the conference trip, I presented a papers entitled
SNIWD: Simultaneous weight noise injection with weight decay for MLP training.
The work presented in these papers are extended and submitted to IEEE Transactions on Neural
Networks. It has recently been conditionally accepted.
During the meeting, I was also lucky that I could meet several well-known experts in neural
networks, including Prof. Irwin King and Prof. Jun Wang from Chinese University of Hong
Kong, Prof. S.I. Amari and Prof. Cichoki from RIKEN, and others.
Since after I have met those big names, I have been assigned as one program committee
member of the ICONIP’10. Besides, I have joint the Asia Paciﬁc Neural Network Assembly as a
Governing Board Member.
1
98年度專題研究計畫研究成果彙整表 
計畫主持人：沈培輝 計畫編號：98-2221-E-005-048- 
計畫名稱：利用 Weight Decay 結合隨機 fault 或加注雜訊的在線容錯訓練之特性研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 1 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
