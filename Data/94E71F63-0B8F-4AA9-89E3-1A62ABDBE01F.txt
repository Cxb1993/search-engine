 2
分類問題的新方法。在本研究計畫的第二
年計畫中，我們提出一個新的模糊資訊增
益量測法。我們所提之模糊資訊增益量測
法比Quinlan所提之資訊增益量測法佳。
我們並提出一個根據我們所提之模糊資
訊增益量測法來處理分類問題的新方
法。首先，我們提出一個特徵與一組訓練
資料相關的模糊資訊增益量測法。然後，
根據我們所提出的模糊資訊增益量測
法，我們提出一個演算法來建構各個特徵
的各個模糊集合的歸屬函數、計算各個訓
練資料子集合屬於各個類別的類別歸屬
度及計算各個訓練資料子集合的模糊亂
度，其中各個訓練資料子集合包含某個特
定特徵的值位於某個特定的模糊集合的
定義域之內的部分訓練資料。最後，根據
所建構的各個特徵的各個模糊集合的歸
屬函數、所得到的各個訓練資料子集合屬
於各個類別的類別歸屬度及所得到的各
個訓練資料子集合的模糊亂度，我們提出
一個評估函數來分類測試資料。在本研究
計畫的第二年計畫中所提的方法可以處
理數值型及名詞型的資料，其比目前已存
在的方法有更高的平均分類正確率。  
 
三、研究方法及成果 
在本研究計畫的第二年計畫中，我們
提出一個根據模糊資訊增益量測法
(Fuzzy  Information Gain Measure)來處
理分類問題的新方法。首先，我們提出一
個特徵與一組訓練資料相關的模糊資訊
增益量測法。然後，根據我們所提出的模
糊資訊增益量測法，我們提出一個演算法
來建構各個特徵的各個模糊集合的歸屬
函數、計算各個訓練資料子集合屬於各個
類別的類別歸屬度（Class Degree）及計
算各個訓練資料子集合的模糊亂度
（Fuzzy Entropy），其中各個訓練資料子
集合包含某個特定特徵的值位於某個特
定的模糊集合的定義域之內的部分訓練
資料。最後，根據所建構的各個特徵的各
個模糊集合的歸屬函數、所得到的各個訓
練資料子集合屬於各個類別的類別歸屬
度及所得到的各個訓練資料子集合的模
糊亂度，我們提出一個評估函數來分類測
試資料。我們所提出的這個方法可以處理
數值型及名詞型的資料。這個方法比在參
考文獻[35]，[48]及[49]中被提出的方法有
更高的平均分類正確率。 
在本研究計畫的第二年計畫中，我們
應用 k-means 分群演算法[31]來產生 k 個
群心，然後以群心為模糊集合的中心來建
構數值型特徵的歸屬函數。假設 R 為一組
訓練資料，f 一個特徵。首先，我們應用
k-means 分群演算法[31]將一組訓練資料
R 的特徵 f 的值分成 k 個分群 (亦即：[x11, 
x12]、[x21, x22]、…及[xk1, xk2])以產生 k 個
群心 m1、m2、…及 mk。然後，我們以群
心 m1、m2、…及 mk 為模糊集合的中心來
建構特徵 f 的模糊集合歸屬函數如下： 
for i = 1 to k do 
{ 
 if i =1 then 
{ 
 /* 建構對應到第一個群心 m1 的
歸屬函數
1vμ ，其中 Umin 為特徵
f 的論述宇集的最小值， m1 為第
1 個群心及 m2 為第 2 個群心。*/ 
 let 
⎪⎪⎩
⎪⎪⎨
⎧
≤<−−−
≤≤×−
−−
=
otherwise
mxmifmm
mx
mxUifmU
mx
x 21
12
1
1
1
1
v
,0
,1
,5.01
)(
min
min
1
μ
 
} 
else 
{ 
 if i = k then 
 { 
    /* 建構對應到第 k 個群心 mk
的歸屬函數
kvμ ，其中 Umax 為特
徵 f 的論述宇集的最大值，mk為
第 k 個群心及 mk-1 為 mk 左邊的
群心。*/ 
  let 
 4
[36]， [38]，[42]，[61]被提出。在參考
文獻[61]中，Zadeh 定義了一個模糊集合
v 的模糊亂度量測法 (Fuzzy Entropy 
Measure)，其中有一個模糊集合 v 定義於
論述宇集 U 之內及 U = {u1, u2, …, un}，
分別對應於機率分配 P = {p1,  p2, …, 
pn}，列示如下： 
∑−= =
n
i iiiv
ppuvH
1
log)()( μ ,      (6)                                            
其中 μv 為模糊集合 v 的歸屬函數，μv(ui)
為數值 ui 歸屬於模糊集合 v 的程度，μv(ui) 
∈ [0, 1]，pi為 ui的機率，1≤ i ≤ n 及 n 為
在論述宇集 U 中所包含的元素的數量。 
在參考文獻[36]中，Kosko 根據超立
方體的幾合學(Geometry of A Hypercube)
定義了一個模糊集合 v 的模糊亂度量測
法(Fuzzy Entropy Measure)，列示如下： 
∑ ∨
∑ ∧
=
=
=
n
i
n
i
i
C
viv
i
C
viv
uu
uu
vH
1
1
))()((
))()((
)(
μμ
μμ
,    (7)                                      
其中 v 為一個定義於論述宇集 U 之內的
模糊集合，ui∈U，1 ≤ i ≤ n，n 為在論述
宇集 U 中所包含的元素的數量，μv 為模
糊集合 v 的歸屬函數，μv(ui)為數值 ui 歸
屬於模糊集合 v 的程度，μv(ui) ∈ [0, 1]，
μvc(ui)為 μv(ui)的補數，μvc(ui) ∈ [0, 1]，∧
為最小值運算元及∨為最大值運算元。 
在參考文獻[42]中，Luca 等人根據
Shannon 的 亂 度 量 測 法 (Entropy 
Measure)[53]定義了一個模糊亂度量測法
(Fuzzy Entropy Measure)。他們提出了四
個 模 糊 亂 度 量 測 法 (Fuzzy Entropy 
Measure)的原則。假設有一個定義在論述
宇集 U 中的模糊集合 v，ui∈U，1 ≤ i ≤ n，
n 為論述宇集 U 中數值的數量，μv為模糊
集合 v 的歸屬函數及 μv(ui)為數值 ui歸屬
於模糊集合 v 的程度。這些用於模糊集合
v的模糊亂度量測法H(v)的原則從參考文
獻[42]回顧如下。 
原則 1：H(v) = 0，若且唯若 v 為一個精確
集合(Crisp Set)，∀ ui ∈ v。 
原則 2：H(v)為最大值，iff μv(ui) = 1/2，∀ 
ui ∈ v。 
原則 3：若模糊集合(Fuzzy  Set)v1 比模
糊集合 v2 更為不模糊，則 H(v1) ≤ H(v2)。 
原則 4：H(v) = H(v c)，其中 v c 為 v 的補
數，v c = 1 - v。 
由 Luca 等人所提出的模糊集合 v 的
模糊亂度量測法(Fuzzy Entropy Measure) 
[42]，定義如下： 
∑=−=
n
i i
uviuvnvH 1
))(log)([(
1
)( μμ   
))](1log())(1( iuviuv μμ −−+   (8) 
其中 v 為一個定義於論述宇集 U 之內的
模糊集合，n 為在論述宇集 U 中所包含的
元素的數量，ui∈U，1 ≤ i ≤ n，μv 為模糊
集合 v 的歸屬函數，μv(ui)為數值 ui 歸屬
於模糊集合 v 的程度及 μv(ui) ∈ [0, 1]。 
在參考文獻 [38]中，Lee等人根據
Shannon 的 亂 度 量 測 法 (Entropy 
Measure)[53]及Luca的原則[42]提出一個
區間的模糊亂度量測法 (Fuzzy Entropy 
Measure)，列示如下。假設有一組元素R
被分成一組類別C，及假設一個特徵維度
被分成n個區間。在第i個區間中某個類別
C的元素歸屬於某個模糊集合v的符合程
度(Matching Degree) MDc，其中c∈C，定
義如下[38]： 
∑
∑
=
∈
∈
iRr
v
icRr
v
c r
r
vMD
)(
)(
)( μ
μ
,            (9)           
其中v為定義於特徵維度上的一個模糊集
合，μv為模糊集合v的歸屬函數，μv(r)為r
歸屬於模糊集合v的程度，μv(ui) ∈ [0, 1]，
Ri為分佈於R的第i個區間的子集合及Ric 
為Ri中屬於某個類別c的子集合及c∈C。在
第i個區間中某個類別c的元素歸屬於某
個模糊集合v的模糊亂度(Fuzzy Entropy) 
IFEc(v)，其中c∈C，定義如下： 
)(log)()( 2 vMDvMDvIFE ccc −= .   (10)             
在第i個區間中的元素歸屬於某個模糊集
合v的模糊亂度(Fuzzy Entropy) IFE(v)定
義如下： 
∑=
∈Cc c
vIFEvIFE )()( .          (11)            
在特徵維度的第 i個區間的模糊亂度
(Fuzzy Entropy) iTFE 定義如下： 
 6
 
圖2. 對應於特徵f的模糊集合 
 
表 1. 特徵 f 的值及對應的類別 
特徵f的值 類別 
1 + 
2 - 
3 + 
4 + 
5 - 
 
表 2. 特徵f的模糊集合的定義域 
模糊集合  模糊集合的定義
域 
v1 [0, 3] 
v2 [1, 5] 
v3 [3, 6] 
 
根據我們所提出的模糊資訊增益量
測 法 (Fuzzy  Information Gain 
Measure)，(亦即：公式(13)-(15))，我們可
以計算特徵 f與這組訓練資料 R相關的模
糊資訊增益 FIG(R, f)如下： 
(1) 計算特徵 f 的值位於特徵 f 的模糊集
合 v1的定義域之內的訓練資料子集合
R1 的模糊亂度(Fuzzy Entropy)：  
(i) 分別依正值類別(+)及負值類別(-)
計算特徵 f 的值位於特徵 f 的模糊集
合 v1 的定義域之內的訓練資料子集
合 R1 的歸屬度總和。令 X+ 為訓練
資料子集合 R1 之內屬於正值類別(+)
的訓練資料的特徵 f 的一組值。令 
X- 為訓練資料子集合 R1之內屬於負
值類別(-)的訓練資料的特徵 f 的一
組值。由圖 1，我們可以看出 X+ = {1}
及 X- = {2}。然後，我們可以得到 
.5.0)(
,1)(
1
1
=∑
=∑
−
+
∈
∈
Xx v
Xx v
x
x
μ
μ
 
(ii) 根據公式(13)，分別依正值類別(+)
及負值類別(-)計算特徵 f 的值位於
特徵 f 的模糊集合 v1 的定義域之內
的訓練資料子集合 R1 的類別歸屬
度(Class Degree)： 
.3333.0)(
,6667.0)(
5.1
5.0
5.01
5.0
5.1
1
5.01
1
1
1
≅==
≅==
+
+
−
+
vCD
vCD
 
(iii) 根據公式(14)，計算特徵 f 的值位
於特徵 f的模糊集合 v1的定義域之
內的訓練資料子集合 R1 的模糊亂
度 FE(v1)如下： 
)(log)(()( 1211 vCDvCDvFE ++−=  
))(log)( 121 vCDvCD −−+  
0.9182.
)3333.0log3333.0
6667.0log6667.0(
2
2
≅
×+
×−=
 
(2) 計算特徵 f 的值位於特徵 f 的模糊集
合 v2的定義域之內的訓練資料子集合
R2 的模糊亂度(Fuzzy Entropy)：  
(i) 分別依正值類別(+)及負值類別(-)
計算特徵 f 的值位於特徵 f 的模糊
集合 v2 的定義域之內的訓練資料
子集合 R2 的歸屬度總和。令 X+ 
為訓練資料子集合 R2 之內屬於正
值類別(+)的訓練資料的特徵 f 的
一組值。令 X- 為訓練資料子集合
R2 之內屬於負值類別(-)的訓練資
料的特徵 f 的一組值。由圖 1，我
們可以看出 X+ = {3, 4}及 X- = 
{2}。然後，我們可以得到 
.5.0)(
,5.15.01)(
2
2
=∑
=+=∑
−
+
∈
∈
Xx v
Xx v
x
x
μ
μ
 
(ii) 根據公式(13)，分別依正值類別(+)
及負值類別(-)計算特徵 f的值位於
特徵 f的模糊集合 v2的定義域之內
的訓練資料子集合 R2 的類別歸屬
度(Class Degree)： 
Membership 
0 1 2 3 
Feature f 
1 
0.5 
4 5 6 
v1 v2 v3 
 8
表 3. 列示特徵 f1及 f2的值及它
們對應的類別 
特徵 f1 的值 特徵 f2 的值 類別 
1 1 + 
1 3 + 
1.5 1.5 - 
2.5 3 - 
3 1 - 
3.5 3.5 + 
4.5 5 - 
5 6 + 
5.5 5 + 
 
表 4. (a)特徵f1及(b)特徵f2的模糊集合的
定義域 
(a) 
模糊集合  模糊集合的定義
域 
v11 [0, 3] 
v12 [1, 5] 
v13 [3, 6] 
(b) 
模糊集合  模糊集合的定義
域 
v21 [0, 3] 
v22 [1, 5] 
v23 [3, 6] 
 
根據 Quinlan 的資訊增益量測法
(Information Gain Measure)[50]，(亦即：
公式(3))，我們分別計算特徵 f1 與這組訓
練資料 R 相關的資訊增益 Gain(R, f1)及特
徵 f2 與這組訓練資料 R 相關的資訊增益
Gain(R, f2)如下： 
0.0728,                  
1/3)]}2log (-1/3                    
 2/3)2log [(-2/31/32/3)]2log (-2/3 1/3)2log [(-1/31/3                     
1/3)] 2log (-1/3 2/3)2log [(-2/31/3-4/9)]2log (-4/95/9)2log [(-5/9  )1,(
=
+
×++×+
+×{+=fRGain
0.0728.                    
2/3)]}2log (-2/3                      
 1/3)2log [(-1/31/31/3)]2log (-1/3 2/3)2log [(-2/31/3                      
2/3)]2log (-2/3  1/3)2log [(-1/3{1/3-4/9)]2log (-4/95/9)2log [(-5/9  )2,(
=
+
×++×+
+×+=fRGain
 
根據我們所提出的模糊資訊增益量
測法(Fuzzy Information Gain Measure)，
(亦即：公式(13)-(15))，我們分別計算特
徵 f1 與這組訓練資料 R 相關的模糊資訊
增益 FIG(R, f1)及特徵 f2與這組訓練資料
R 相關的模糊資訊增益 FIG(R, f2)如下： 
步驟 1：計算特徵 f1 與這組訓練資料 R 相
關的模糊資訊增益 FIG(R, f1)： 
(1) 計算特徵 f1 的值位於特徵 f1 的模糊集
合 v11 的定義域之內的訓練資料子集
合 R11 的模糊亂度(Fuzzy Entropy)： 
(i) 分別依正值類別(+)及負值類別(-)
計算特徵 f1 的值位於特徵 f1 的模
糊集合 v11 的定義域之內的訓練資
料子集合 R11的歸屬度總和。令 X+ 
為訓練資料子集合 R11 之內屬於正
值類別(+)的訓練資料的特徵 f1 的
一組值。令 X- 為訓練資料子集合
R11 之內屬於負值類別(-)的訓練資
料的特徵 f1 的一組值。由圖 3，我
們可以看出 X+ = {1, 1}及 X- = 
{1.5, 2.5}。然後，我們可以得到 
.125.00.75)(
,211)(
11
11
=+=∑
=+=∑
−
+
∈
∈
Xx v
Xx v
x
x
μ
μ
 
(ii) 根據公式(13)，分別依正值類別(+)
及負值類別(-)計算特徵 f1 的值位
於特徵 f1 的模糊集合 v11的定義域
之內的訓練資料子集合 R11 的類別
歸屬度(Class Degree)： 
.3333.0)(
,6667.0)(
3
1
12
1
3
2
12
2
11
11
≅==
≅==
+
+
−
+
vCD
vCD
 
(iii) 根據公式(14)，計算特徵 f1 的值
位於特徵 f1 的模糊集合 v11的定義
域之內的訓練資料子集合 R11 的模
糊亂度 FE(v11)如下： 
))(log)(
)(log)(()(
11211
1121111
vCDvCD
vCDvCDvFE
−−
++
+
−=  
0.9183.
)3333.0log3333.0
6667.0log6667.0(
2
2
≅
×+
×−=
 
(2) 計算特徵 f1 的值位於特徵 f1 的模糊集
合 v12 的定義域之內的訓練資料子集
合 R12 的模糊亂度(Fuzzy Entropy)：  
 10
一組值。令 X- 為訓練資料子集合
R21 之內屬於負值類別(-)的訓練資
料的特徵 f2 的一組值。由圖 3，我
們可以看出 X+ = {1}及 X- = {1, 
1.5}。然後，我們可以得到 
.75.175.01)(
,1)(
21
21
=+=∑
=∑
−
+
∈
∈
Xx v
Xx v
x
x
μ
μ
 
(ii) 根據公式(13)，分別依正值類別(+)
及負值類別(-)計算特徵 f2 的值位
於特徵 f2 的模糊集合 v21的定義域
之內的訓練資料子集合 R21的類別
歸屬度(Class Degree)： 
.6364.0)(
,3636.0)(
75.2
75.1
75.11
75.1
75.2
1
75.11
1
21
21
≅==
≅==
+
+
−
+
vCD
vCD
 
(iii) 根據公式(14)，計算特徵 f2 的值
位於特徵 f2的模糊集合 v21的定義
域之內的訓練資料子集合 R21的模
糊亂度 FE(v21)如下： 
))21(2log)21(
)21(2log)21(()21(
vCDvCD
vCDvCDvFE
−−+
++−=  
0.9457.
)6364.02log6364.0
3636.02log3636.0(
≅
×+
×−=
 
(2) 計算特徵 f2 的值位於特徵 f2 的模糊集
合 v22 的定義域之內的訓練資料子集
合 R22 的模糊亂度(Fuzzy Entropy)：  
(i) 分別依正值類別(+)及負值類別(-)
計算特徵 f2 的值位於特徵 f2 的模
糊集合 v22 的定義域之內的訓練資
料子集合R22的歸屬度總和。令 X+ 
為訓練資料子集合 R22之內屬於正
值類別(+)的訓練資料的特徵 f2 的
一組值。令 X- 為訓練資料子集合
R22 之內屬於負值類別(-)的訓練資
料的特徵 f2 的一組值。由圖 3，我
們可以看出 X+ = {3, 3.5}及 X- = 
{1.5, 3}。然後，我們可以得到 
..25110.25)(
,1.750.751)(
22
22
=+=∑
=+=∑
−
+
∈
∈
Xx v
Xx v
x
x
μ
μ
 
(ii) 根據公式(13)，分別依正值類別(+)
及負值類別(-)計算特徵 f2 的值位
於特徵 f2 的模糊集合 v22的定義域
之內的訓練資料子集合 R22的類別
歸屬度(Class Degree)： 
.4167.0)(
,5833.0)(
3
25.1
25.175.1
25.1
3
75.1
25.175.1
75.1
22
22
≅==
≅==
+
+
−
+
vCD
vCD
 
(iii) 根據公式(14)，計算分特徵 f2 的
值位於特徵 f2 的模糊集合 v22的定
義域之內的訓練資料子集合 R22的
模糊亂度 FE(v22)如下： 
))22(2log)22(
)22(2log)22(()22(
vCDvCD
vCDvCDvFE
−−+
++−=
 
0.9799.
)4167.02log4167.0
5833.02log5833.0(
≅
×+
×−=
 
(3) 計算特徵 f2 的值位於特徵 f2 的模糊集
合 v23 的定義域之內的訓練資料子集
合 R23 的模糊亂度(Fuzzy Entropy)：  
(i) 分別依正值類別(+)及負值類別(-)
計算特徵 f2 的值位於特徵 f2 的模
糊集合 v23 的定義域之內的訓練資
料子集合R23的歸屬度總和。令 X+ 
為訓練資料子集合 R23之內屬於正
值類別(+)的訓練資料的特徵 f2 的
一組值。令 X- 為訓練資料子集合
R23 之內屬於負值類別(-)的訓練資
料的特徵 f2 的一組值。由圖 3，我
們可以看出X+ = {3.5, 5, 6}及 X- = 
{5}。然後，我們可以得到 
.1)(
,1.750.5125.0)(
23
23
=∑
=++=∑
−
+
∈
∈
Xx v
Xx v
x
x
μ
μ
 
(ii) 根據公式(13)，分別依正值類別(+)
 12
模糊集合的歸屬函數。名
詞型特徵的論述宇集 U 的
各個值被視為個別的模糊
集合，其中 U 為特徵 fi
的論述宇集，U = {u1, 
u2, …, up}， uj為特徵 fi
的第 j 個值及 p 為特徵 fi
的論述宇集的值的數量。
名詞型的特徵 fi的各個值
uj被視為一個模糊集合，
其中
⎩⎨
⎧ ==
otherwise
uxif
x j
ju ,0
,1
)(μ 。
例如性別分為男性(Male)
及女性(Female)，則性別為
男性時，分別屬於男性及
女性的歸屬度為
1)( =malemaleμ 及
0)( =malefemaleμ 。*/ 
  for j = 1 to p do 
{ 
               let 
⎩⎨
⎧ ==
otherwise
uxif
x j
ijv ,0
,1
)(μ  
             其中 vij為第 i 個特徵 fi 的
第 j 個模糊集合，
ijv
μ 為模
糊集合 vij的歸屬函數及
)(x
ijv
μ 為數值 x 歸屬於模
糊集合 vij的程度; 
             移至步驟 4. 
步驟 3.2： /* 建構數值詞型特徵的歸
屬函數。*/  
應用 K-means 分群演算法[31]
將這組訓練資料 R 的特徵 fi
的值分成 k 個分群 (亦即：
[x11, x12]、[x21, x22]、…及[xk1, 
xk2])以產生 k 個群心 m1、
m2、…及 mk;  
for j = 1 to k do 
{ 
        /*以群心 M 為模糊集合
的中心來建構特徵 fi 的模
糊集合歸屬函數
ijv
μ 如圖
4，其中“Umin”為特徵 fi的
論述宇集的最小值，
“Umax”為特徵 fi 的論述宇
集的最大值，和 vi1 、 
vi2、…及 vik 的群心分別
為 m1、m2、…及 mk。*/ 
      if j =1 then 
         { 
          /* 建構對應到第一個
群心 m1 的模糊集合 vi1
的歸屬函數
1ivμ ，其中
Umin 為特徵 fi的論述宇
集的最小值， m1 為第
1 個群心及 m2 為第 2
個群心。*/ 
        let 
⎪⎪⎩
⎪⎪⎨
⎧
≤<−−−
≤≤×−
−−
=
otherwise
mxmifmm
mx
mxUifmU
mx
x 1
1
1
1
1
1
1iv
,0
,1
,5.01
)( 2
2
min
min
μ
 
} 
          else 
          { 
            if j = k then 
            { 
             /* 建構對應到第 k
個群心 mk 的模糊集
合 vik 的歸屬函數
ikvμ ，其中 Umax 為特
徵 fi的論述宇集的最
大值，mk為第 k 個群
心及 mk-1 為 mk 左邊
的群心。*/ 
                  Let 
⎪⎪⎩
⎪⎪⎨
⎧
≤<×−
−−
≤≤−−−
=
−−
otherwise
UxmifmU
mx
mxmifmm
mx
x k
k
k
kk
kk
k
ikv
,0
,5.01
,1
)( max
max
1
1
μ
 
               } 
              else 
 14
產生的，而且一筆測試資料的各個特徵 fi
會被給予一個動態的權 wi，其中 wi ∈ [0, 
1]。例如，假設有一組資料如圖 5 中所示，
其中符號"+"為正值的訓練資料，符號"-"
為負值的訓練資料，和 R1及 R2為兩筆測
試資料。由圖 12 我們可以看出對於測試
資料 R1而言，特徵 f1比特徵 f2重要，因
為在特徵 f1的值 u11的訓練資料（亦即：
正值訓練資料"+"及負值訓練資料"-"）的
分佈比在特徵 f2的值 u21的訓練資料的分
佈較為不複雜，不過對於測試資料 R2 而
言，特徵 f2比特徵 f1重要，因為在特徵 f2
的值 u22的訓練資料的分佈比在特徵 f1的
值 u14的訓練資料的分佈較為不複雜。在
我們的方法，測試資料 R1及 R2的特徵權
重會分別根據在測試資料 R1及 R2的各個
特徵的值的訓練資料的分佈來給予，其中
對於測試資料 R1，特徵 f2 的權重會小於
特徵 f1 的權重；對於測試資料 R2，特徵
f1的權重會小於特徵 f2的權重。 
 
 
圖 5. 有兩個特徵及兩個類別的資料分
佈 
 
在本研究計畫之第二年計畫中，我們
提出一個方法來產生各個測試資料的特
徵的動態權重，稱為模糊亂度權重量測法
(Fuzzy Entropy Weight Measure)。因為我
們所提出的模糊亂度權重量測法是根據
以一筆測試資料的各個特徵的值所估計
的此筆測試資料的類別的不確定性來計
算，所以我們用模糊亂度量測法(Fuzzy 
Entropy Measure)（亦即：公式(14)）來提
出一個估計模糊亂度量測法 (Estimated 
Fuzzy Entropy Measure)來計算以某個測
試資料的某個特定的特徵的值來估計此
測試資料的類別的不確定性如下。 
定義 4：以某個測試資料 r 的某個特
徵 f 的值 x 來估計此測試資料的估計模糊
亂度 EFEf(x)，定義如下： 
))()(()( vFE
Vv
xxEFE
f
vf ×∑∈= μ ，    (16)           
其中 Vf 為定義於特徵 f 上的一組模糊集
合，μv 為模糊集合 v 的歸屬函數，μv 是
在訓練時所建構的，μv(x)為數值 x 歸屬於
模糊集合 v 的程度，μv(x) ∈ [0, 1]及 FE(v)
為一個分佈於特徵 f 的模糊集合 v 的定義
域之內的訓練資料子集合的模糊亂度
(Fuzzy Entropy)並且是在訓練時所得到
的。 
然後，根據列示於公式(16)的我們所
提出的估計模糊亂度量測法 (Estimated 
Fuzzy Entropy Measure)，我們提出一個模
糊亂度權重量測法(Fuzzy Entropy Weight 
Measure)來產生動態的特徵權重給某個
測試資料，說明如下。假設在測試資料中
出 現 某 個 類 別 的 機 率 呈 均 勻 分 配
(Uniform Distribution) ，其中 P(x)= 1/nc，
x∈X， X 為一組測試資料的集合，P(x)
為測試資料 X 的機率密度函數及 nc 為類
別的數量。在這樣的案例中，測試資料 X
擁有最大亂度(Maximum Entropy)[33]。假
設在測試資料中出現某個類別的機率呈
均勻分配(Uniform Distribution) ，我們所
提出的模糊亂度權重量測法 (Fuzzy 
Entropy Weight Measure)的定義說明如
下。 
定義 5：某個測試資料 r 的某個特徵 f
的模糊亂度權重 FEWf(x)，定義如下： 
∑
∈
−−×
−−×
=
Fg
ggcncnc
fcncnc
f xEFE
xEFE
xFEW
n
n
)](
)(
)(
)log([
)log(
1
2
1
1
2
1
                               (17) 
其中 x 為測試資料 r 的特徵 f 的值，
EFEf(x)為以測試資料 r 的特徵 f 的值 x 來
估計此測試資料的估計模糊亂度，F 為一
組特徵，nc為類別的數量，xg為測試資料
u25 
0 u11 u1 u1 u14 u1
u21 
u22 
u23 
u24 
+ 
+ 
+ 
－ 
+ 
+ 
－
R2 + 
+ 
f1 
R1 
－ － － －
f2 
+ 
+ 
 16
有 16 筆資料的值不完整，我們使用 683
筆資料。每筆資料包含 9 個數值型的特
徵 ， 分 別 為 (1)Clump Thickness 、
(2)Uniformity of Cell Size、(3)Uniformity 
of Cell Shape、(4)Marginal Adhesion、
(5)Single Epithelial Cell Size 、 (6)Bare 
Nuclei、 (7)Bland Chromatin、(8)Normal 
Nucleoli 及(9)Mitoses。 
(iii)酒類識別(Wine Recognition)資料集：
這個資料集有 178 筆資料，被分成三個類
別。每筆資料包含 13 個數值型的特徵，
分別為(1)Alcohol Content、(2) Malic Acid 
Content、(3)Ash Content、(4)Alcalinity of 
Ash 、 (5)Magnesium Content 、 (6)Total 
Phenols、(7)Flavanoids、(8)Nonflavanoids 
Phenols、(9)Proanthocyaninsm、(13)Color 
Intensity、(14)Hue、(15)OD280/OD315 of 
Diluted Wines 及(16) Praline。 
(iv)玻璃識別 (Glass Identification)資料
集：這個資料集有 214 筆資料，被分成七
個類別。每筆資料包含 9 個數值型的特
徵 ， 分 別 為 (1)Refractive Index 、 (2) 
Sodium、(3)Magnesium、(4)Aluminum、
(5)Silicon、 (6)Potassium、 (7)Calcium、 
(8)Barium 及(9)Iron。  
(v)信用審核(Credit Approval)資料集：這
個資料集有 690 筆資料，被分成 2 個類
別。因為有 37 筆資料的值不完整，我們
使用 653 筆資料。每筆資料為信用卡能力
包含 8個數值型的特徵及 6個名詞型的特
徵。為了保護資料的機密性，原始的特徵
名稱被改為無意義的符號(A1-A14)。 
(vi)天平(Balance Scale)資料集：這個資料
集有 625 筆資料，被分成 3 個類別。每筆
資料包含 4 個數值型的特徵，分別為
(1)Left Weight、(2)Left Distance、(3)Right 
Weight 及(4)Right Distance。 
我們用我們所提出的方法與三個不
同類型的分類器（亦即：naive Bayes 
[35]、C4.5 [49]及 SMO(Sequential Minimal 
Optimization) [48]）在各個資料集分別執
行 200 次實驗。在每一次實驗，我們用資
料集的 75%來當訓練資料，其餘的當測試
資料。我們在一個自由軟體 Weka [64]的
環境上執行三個分類器（亦即：naive 
Bayes [35]、C4.5 [49]及 SMO (Sequential 
Minimal Optimization) [48]）的實驗。將
不同的分類器的分類結果比較彙整於表 
5。由表 5，我們可以看出我們在本研究
計畫的第三年計畫中所提出的方法比
naive Bayes 方法[35]、C4.5 方法[49]及
SMO (Sequential Minimal Optimization)方
法[48]有更高的平均分類正確率。 
 
表 5. 不同方法的平均分類正確率比較 
不同方法的平均分類正確率 
資料集 
NaiveBayes C4.5 SMO 
我們所提的
方法 
蝴蝶花(Iris)資
料集[63] 96.00±0.30% 95.13±0.20% 96.69±2.58% 96.88±2.40%
乳癌(Breast 
cancer)資料集
[63] 
95.90±0.20% 94.71±0.09% 97.51±0.97% 98.14±0.90%
酒類識別(Wine 
Recognition)資
料集[63] 
96.75±2.32% 91.14±5.12% 97.87±2.11% 98.36±1.26%
玻璃識別
(Glass 
identification)
資料集[63] 
42.90±1.70% 67.90±0.50% 58.85±6.58% 69.14±4.69%
信用審核
(Credit 
approval)資料
集 [63] 
74.80±0.50% 84.20±0.30% 88.10±2.21% 88.16±2.21%
天平(Balance 
scale)資料集
[63] 
89.81±1.29% 78.04±2.46% 87.61±1.38% 88.65±1.39%
（附註：所有的結果以 200 次獨立實驗所求得的平均值±標準差來表
示。） 
 
四、結果與討論 
本研究計畫為一個二年期的研究計
畫，旨在根據模糊亂度量測法及模糊資訊
增益量測法提出一些處理分類問題的新
方法。在本研究計畫的第一年計畫中，我
們提出一個用來處理分類問題的特徵子
集合挑選的新方法。特徵子集合挑選是用
來降低在分類問題或識別問題中特徵的
數量。首先，我們提出一個用來建構各特
徵之模糊集合歸屬函數的演算法，其中我
們將數值型的特徵離散化來建構它的模
糊集合歸屬函數。然後，我們根據我們所
提出的用邊界樣本的模糊亂度量測法提
出一個用來來挑選特徵子集合的演算
法。我們所提出的特徵子集合挑選的方法
可以挑出相關的特徵來比現有的方法獲
得更高的平均分類正確率。在本研究計畫
的第二年計畫中，我們提出一個新的模糊
 18
European Working Session on 
Learning, Berlin, Germany, 1991, pp. 
164-178. 
[10] N. Chaikla and Y. Qi, “Genetic 
algorithms in feature selection,” 
Proceedings of the 1999 IEEE 
International Conference on Systems, 
Man, and Cybernetics, Tokyo, Japan, 
1999, vol. 5, pp. 538-540. 
[11] C. H. Chang and S. M. Chen, 
“Constructing membership functions 
and generating weighted Fuzzy  rules 
from training data,” Proceedings of 
the 2001 Ninth National Conference 
on Fuzzy  Theory and Its 
Applications, Chungli, Taoyuan, 
Taiwan, Republic of China, pp. 
708-713, 2001. 
[12] C. H. Chang and S. M. Chen, “A new 
method to generate Fuzzy  rules from 
numerical data based on the exclusion 
of attribute terms,” Proceedings of the 
2000 International Computer 
Symposium: Workshop on Artificial 
Intelligence, Chiayi, Taiwan, Republic 
of China, pp. 57-64, 2000. 
[13] S. M. Chen, S. H. Lee, and C. H. Lee, 
“A new method for generating Fuzzy  
rules from numerical data for handling 
classification problems,” Applied 
Artificial Intelligence, vol. 15, no. 7, 
pp. 645-664, 2001. 
[14] S. M. Chen and S. Y. Lin, “A new 
method for constructing Fuzzy  
decision trees and generating Fuzzy  
classification rules from training 
examples,” Cybernetics and Systems, 
vol. 31, no. 7, pp. 763-785, 2000. 
[15] S. M. Chen, C. H. Kao, and C. H. Yu, 
“Generating Fuzzy  rules from 
training data containing noise for 
handling classification problems,” 
Cybernetics and Systems, vol. 33, no. 
7, pp. 723-748, 2002. 
[16] S. M. Chen and J. D. Shie, “A new 
method for feature subset selection for 
handling classification problems,” 
Proceedings of the 2005 IEEE 
International Conference on Fuzzy  
Systems, Reno, Nevada, 2005, pp. 
183-188.   
[17] S. M. Chen and Y. D. Fang, “A new 
approach for handling the Iris Data 
Classification Problems,” 
International Journal of Applied 
Science and Engineering, vol. 3, no. 1, 
pp. 37-50, 2005. 
[18] Y. C. Chen and S. M. Chen, “A new 
method to generate Fuzzy  rules for 
fizzy classification systems,” 
Proceedings of the 2000 Eighth 
National Conference on Fuzzy  
Theory and Its Applications, Taipei, 
Taiwan, Republic of China, 2000. 
[19] Y. C. Chen and S. M. Chen, 
“Constructing membership functions 
and generating Fuzzy  rues using 
genetic algorithms,” Proceedings of 
the 2001 Ninth National Conference 
on Fuzzy  Theory and Its 
Applications, Chungli, Taoyuan, 
Taiwan, Republic of China, pp. 
195-200, 2001. 
[20] M. R. Chmielewski and J. W. 
Grzymala-Busse, “Global 
discretization of continuous attributes 
as preprocessing for machine 
learning,” Proceedings of the Third 
International Workshop on Rough Sets 
and Soft Computing, San Jose, 
California, 1994, pp. 294-301. 
[21] T. M. Cover and P. E. Hart, “Nearest 
Neighbor Pattern Classification,” 
IEEE Transactions on Information 
Theory, vol. IT-13, no. 1, pp. 21-27, 
1967. 
[22] R. K. De, N. R. Pal, and S. K. Pal, 
“Feature analysis: Neural network and 
Fuzzy  set theoretic approaches,” 
Pattern Recognition, vol. 30, no. 10, 
pp. 1579-1590, 1997. 
[23] M. Dong and R. Kothari, “Feature 
subset selection using a new definition 
of classifiability,” Pattern Recognition 
Letters, vol. 24, no. 9, pp. 1215-1225, 
2003. 
[24] R. A. Fisher, “The use of multiple 
measurements in taxonomic 
problems,” Annals of Eugenics, vol. 7, 
 20
hypotheses,” Proceedings of the 
Seventh Annual ACM Conference on 
Computational Learning Theory, New 
Brunswick, New Jersey, 1994, pp. 
67-75. 
[44] W. S. McCulloch and W. Pitts, “A 
logical calculus of the ideas immanent 
in nervous activity,” Bulletin of 
Mathematical Biophysics, vol. 5, no. 1, 
pp. 115-133, 1943.  
[45] T. M. Mitchell, Machine Learning. 
New York: McGraw-Hill, 1997. 
[46] B. T. Ongkowijaya and Z. Xiaoyan, “A 
new weighted feature approach based 
on GA for speech recognition,” 
Proceedings of the 7th International 
Conference on Signal Processing, 
Istanbul, Turkey, 2004, pp. 663-666. 
[47] N. R. Pal, “Soft computing for feature 
analysis,” Fuzzy  Sets and Systems, 
vol. 103, no. 2, pp. 201-221, 1999. 
[48] J. C. Platt, “Using analytic QP and 
sparseness to speed training of support 
vector machines,” Proceedings of the 
Thirteenth Annual Conference on 
Neural Information Processing 
Systems,” Denver, Colorado, 1999, pp. 
557-563. 
[49] J. R. Quinlan, C4.5: Programs for 
Machine Learning, San Francisco: 
Morgan Kaufmann, 1993. 
[50] J. R. Quinlan, “Induction of decision 
trees,” Machine Learning, vol. 1, no. 1, 
pp. 81-106, 1986. 
[51] V. E. Ramesh and M. N. Murty, 
“Off-line signature verification using 
genetically optimized weighted 
features,” Pattern Recognition, vol. 32, 
no. 1, pp. 217-233, 1999. 
[52] C. Schaffer, “Overfitting avoidance as 
bias,” Machine Learning, vol. 10, no. 
2, pp. 153-178, 1993. 
[53] C. E. Shannon, “A mathematical 
theory of communication,” Bell 
System Technical Journal, vol. 27, no. 
3, pp. 379-423, 1948. 
[54] M. Stone, “Cross-validatory choice 
and assessment of statistical 
prediction,” Journal of the Royal 
Statistical Society, vol. 36, no. 2, pp. 
111-147, 1974. 
[55] F. M. Tsai and S. M. Chen, “A new 
method for constructing membership 
functions and generating Fuzzy  rules 
for Fuzzy  classification systems,” 
Proceedings of the 2002 Tenth 
National Conference on Fuzzy  
Theory and Its Applications, Hsinchu, 
Taiwan, Republic of China, 2002. 
[56] E. C. C. Tsang, D. S. Yeung, and X. Z. 
Wang, “OFFSS: Optimal Fuzzy 
-valued feature subset selection,” 
IEEE Transactions on Fuzzy  
Systems, vol. 11, no. 2, pp. 202-213, 
2003. 
[57] T. P. Wu and S. M. Chen, “A new 
method for constructing membership 
functions and Fuzzy  rules from 
training examples,” IEEE 
Transactions on Systems, Man, and 
Cybernetics-Part B: Cybernetics, vol 
29, no. 1, pp. 25-40, 1999. 
[58] C. H. Yu and S. M. Chen, “A new 
method for handling Fuzzy  
classification problems based on 
clustering techniques,” Proceedings of 
the Seventh Conference on Artificial 
Intelligence and Applications, 
Taichung, Taiwan, Republic of China, 
pp. 107-112, 2002. 
[59] C. H. Yu and S. M. Chen, “A new 
method for Fuzzy  rules generation 
for Fuzzy  classification systems,” 
Proceedings of the 6th Conference on 
Artificial Intelligence and 
Applications, Kaohsiung, Taiwan, 
Republic of China, pp. 646-651, 2001. 
[60] L. A. Zadeh, “Fuzzy  sets,” 
Information and Control, vol. 8, pp. 
338-353, 1965. 
[61] L. A. Zadeh, “Probability measures of 
Fuzzy  events,” Journal of 
mathematical Analyses and 
Applications, vol. 23, no. 2, pp 
421-427, 1965. 
[62] L. A. Zadeh, “The concept of 
linguistic variable and its application 
to approximate resoning-I,” 
Information Sciences, vol. 8, no. 3, pp. 
 1
應用模糊亂度量測法及模糊資訊增益量測法以處理分類問題 
之新方法研究 (1/2) 
Handling Classification Problems Based on Fuzzy Entropy Measures  
and Fuzzy Information Gain Measures (1/2) 
計畫編號：NSC 95-2221-E-011-116-MY2 
   執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
主持人：陳錫明   國立台灣科技大學資訊工程系 教授 
 
一、 中文摘要 
 
本研究計畫為一個兩年期的計畫，旨
在根據模糊亂度量測法及模糊資訊增益
量測法提出一些處理分類問題的新方
法。在本研究計畫的第一年計畫中，我們
提出一個用來處理分類問題的特徵子集
合挑選的新方法，其中特徵子集合挑選的
目的是降低在分類問題或識別問題中特
徵的數量。首先，我們提出一個用來建構
各特徵之模糊集合歸屬函數的演算法，其
中我們將數值型的特徵離散化來建構它
的模糊集合歸屬函數。然後，我們根據邊
界樣本提出一個新的模糊亂度量測法。根
據我們所提的模糊亂度量測法，我們提出
一個用來挑選特徵子集合的演算法。我們
所提出的特徵子集合挑選的方法可以挑
出相關的特徵來比現有的方法獲得更高
的平均分類正確率。 
關鍵詞：模糊資訊增益、模糊亂度、分類
問題、歸屬函數、特徵子集合挑
選、模糊邏輯。  
 
Abstract 
 
This project is a 2-year project. The 
purposes of this project are to present new 
methods for handling classification 
problems based on new fuzzy entropy 
measures and new fuzzy information gain 
measures. In the first year of this project, 
we propose a new method for dealing with 
feature subset selection for handling 
classification problems. The goal of feature 
subset selection is to reduce the number of 
features used in classification or 
recognition tasks. First, we “discretize” 
numeric features to construct the 
membership function of each fuzzy set of a 
feature. Then, we propose a new fuzzy 
entropy measure based on boundary 
samples. Based on the proposed fuzzy 
entropy measure, we propose a feature 
selection method to select feature subsets 
based on the proposed fuzzy entropy 
measure focusing on boundary samples. 
The proposed feature subset selection 
method can select relevant features to get 
higher average classification accuracy rates 
than the ones selected by the existing 
methods.  
Keywords: Fuzzy Information Gain, Fuzzy 
Entropy, Classification Problems, 
Membership Functions, Feature Subset 
Selection, Fuzzy Logic.  
 
二 、計畫緣由與目的 
分類技術已被廣泛的應用在多個領
域。各種類型的分類器已被提出[2], [8], 
[11], [20], [19],[15], [3], [18],  [14] 、…
等。在參考文獻[10]中，Dong 等人指出
特徵子集合的挑選是用來降低在分類問
題或識別問題中特徵的數量。很明顯的資
料中可能包含非相關及相關的特徵。如果
我們可以適切的挑選相關的特徵來處理
分類問題，我們可以提升分類的正確率。
近年來有些特徵子集合挑選 (Feature 
Subset Selection)的方法被提出，例如：相
似度量測法(Similarity Measures) [23]、亂
度改進量測法(Gain-Entropies Measures) 
[4]、特徵的相關性(Relevance of Features) 
[1] 、 決 策 表 (Decision Tables) [6] 、
OFEI(Overall Feature Evaluation Index) 
[9] 、FQI(Feature Quality Index) [7]、MIFS 
 3
“
2
minarg kmxKk
−
∈
” 會傳回能
使得數學式 2kmx − 的值最小
的一個數值k及“ • ”為歐幾里
德距離。*/ 
for all x∈X 
{ 
;minarg
2
kmxi Kk
−=
∈
 
{ }xClusterCluster ii U=  
}; 
/* 建立各個分群的新的群心 
mi，其中 ni 為屬於第i個分群
的樣本數量及1 ≤ i ≤ k。*/ 
for i = 1 to k do 
i
i
i n
m
Clusterx
x∑
= ∈ ; 
} until each cluster is not changed. 
步驟 3：根據 k 個群心建構模糊集合歸屬函
數如下： 
/* 指定第i個群心mi的鄰近群
心，其中“mL”為群心mi左
邊的“群心”，“mR”為群心
mi右邊的“群心”， “Umin”
為特徵的最小值及 “Umax” 
為特徵的最大值。*/ 
令 ⎩⎨
⎧
−
=−−=
otherwisem
iifUmU
m
i
i
L ,
1),(
1
minmin ; 
令 ⎩⎨
⎧
+
=−+=
otherwiseim
KiifimUUm R ,1
),( maxmax ; 
/*根據第i個群心mi建構模糊集合vi
的歸屬函數
iv
μ ，其中“Max”為為
最大值運算元。*/ 
令
⎪⎪⎩
⎪⎪⎨
⎧
⎭⎬
⎫
⎩⎨
⎧
⎭⎬
⎫
⎩⎨
⎧
>
−
−−
≤
−
−−
=
i
iR
i
i
Li
i
v
mxif
mm
mx
Max
mxif
mm
xm
Max
x
i
,0,1
,0,1
)(μ  
步驟 4：計算特徵f的模糊亂度如下： 
for i = 1 to k do 
;)()( ∑= ∈Cc ici vFEvFE  
.)()( ∑= ∈Vv s
s
vFEfFFE v  
步驟 5： 如果特徵f的模糊亂度的遞減比
率大於使用者定義的門閂值Tc
時，其中Tc ∈ [0, 1]，則我們令k = 
k + 1並移至步驟 2。否則，令k = 
k – 1並停止。 
在本研究計畫的第一年計畫中，我們
使用一個門閂值 Tr，其中 Tr ∈ [0, 1] ，
來省略類別歸屬度最大值大於或等於使
用者定義的門閂值 Tr 的模糊集合來挑選
特徵子集合。根據定義 1，我們可以看出
有 n 個類別的一組樣本會有 n 個對應的
“類別歸屬度”。 類別歸屬度最大值指的
就是在 n 個“類別歸屬度”中的最大者。如
果某個模糊集合的類別歸屬度最大值大
於或等於使用者定義的門閂值 Tr，其中
Tr ∈ [0, 1] ，則此模糊集合將會被省略來
降低特徵的模糊集合的數量。接著，我們
建構特徵子集合的值的歸屬度擴張矩
陣。在此之前，我們必須建構所有特徵的
值的歸屬度擴張矩陣。特徵的值歸屬於此
特徵的模糊集合的歸屬度擴張矩陣定義
如下： 
定義 5: 特徵f的值歸屬於此特徵的模糊
集合的歸屬度擴張矩陣 EMf定義為： 
mn
rvrv
rvrv
nfmnf
fmf
fEM
×
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
)()(
)()(
1
111
μμ
μμ
L
MMM
L
,  (5) 
其中 n 為樣本的數量，m為特徵 f中模糊
集合的數量， )( pfzv rμ 為樣本 rp的特徵 f
的值 rpf 歸屬於模糊集合 vz的歸屬度，1 ≤ 
p ≤ n 及 1 ≤ z ≤ m。 
令EMf [g, h]為在擴張矩陣EMf的第g
行及第h列的元素，其中1 ≤ g ≤ n，n為樣
本的數量，1 ≤ h ≤ m及m為特徵f中模糊集
合的數量。根據定義5，我們可以看出樣
本rp的特徵f的值rpf 歸屬於模糊集合vz的
歸屬度被儲存於擴張矩陣EMf的第p行及
第z列（亦即：EMf [p, z]）。然後，一組
樣本的類別歸屬度CDc(v)可以由特徵f的
模糊集合的歸屬度擴張矩陣EMf 來計
算，定義如下。 
定義 6: 某個類別c的樣本歸屬於某個模
糊集合v的類別歸屬度CDc(v)，定義如下： 
 5
的特徵集合及FS為已挑選到的特徵集
合。我們所提出的用來挑選特徵子集合
的演算法如下所示： 
步驟 1：/*分別建構各個特徵f的值歸屬於
各個特徵f的模糊集合的歸屬
度擴張矩陣EMf，及計算各個
特徵f的模糊亂度FFE(f)。*/ 
for each Ff ∈  do 
{ 
  根據公式(5)，建構特徵f的值歸
屬於特徵f的模糊集合的歸屬度
擴張矩陣EMf，列示如下： 
mn
fnrvfnrv
frvfrv
fEM
m
m
×
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
)()(
)1()1(
1
1
μμ
μμ
L
MMM
L
; 
根據公式(6)，計算各個類別c
的樣本歸屬於各個模糊集合v
的類別歸屬度CDc(v)，其中c ∈ 
C; 
根據公式(2)及(3)，計算特徵f
的各個模糊集合v的模糊亂度
FE(v); 
根據公式(4)，計算特徵f的模
糊亂度FFE(f)  
}. 
步驟 2：/*將具有最小模糊亂度的特徵放
入已挑選到的特徵集合FS，並將
此特徵從候選的特徵集合F中移
除。*/ 
           令 )(minargˆ fFFEf
Ff∈
= , 其中
符號“ )(minarg fFFE
Ff∈
” 會傳回一個
可以使得函數FFE(f)的值最小化
的某個特徵f。 
令 );ˆ( fFFEEFS =  
令 =FS { fˆ }; 
令 −= FF { fˆ }. 
步驟 3：/* 反覆地將可以降低特徵子集
合的模糊亂度的特徵放入已挑
選到的特徵集合FS，直到沒有仼
何這樣的特徵為止。 */ 
repeat 
{ 
for each  f∈F do 
{ 
   根據公式(8)，依照由使用
者所定義的類別歸屬度最
大值的門閂值Tr，其中Tr ∈ 
[0, 1] ，來建構特徵子集合
FS ∪{f}的值的歸屬度擴張
矩陣 }{ fFSEM U ，列示如下： 
),,(}{ rfFS TfFSCEMEM =U ; 
根據公式(6)，計算各個類別c
的樣本歸屬於特徵子集合FS 
∪{f}的各個複合式模糊集
合v的類別歸屬度CDc(v)，其
中c ∈ C; 
根據公式(2)及(3)，計算特徵
子集合FS ∪{f}的各個複合
式模糊集合 v的模糊亂度
FE(v); 
根據公式(9)，用邊界樣本來
計算特徵子集合FS ∪{f}的
模糊亂度BSFFE(FS, f) 
}; 
令 ),(minargˆ fFSBSFFEf
Ff∈
= ，其中
符 號 “ ),(minarg fFSBSFFE
Ff∈
” 
會傳回一個可以使得函數
),( fFSBSFFE 的值最小化的某
個特徵f。; 
令 )ˆ,( fFSBSFFEED FS −= ; 
令 ;)ˆ,( fFSBSFFEEFS =  
令 UFSFS = { fˆ }; 
令 −= FF { fˆ } 
} until )00( φ=≤= ForDorEFS ; 
令FS 為所挑選的特徵子集合. 
 
四、結果與討論 
我們將我們所提的方法做了兩個實
驗，其中這些實驗用了4種不同類型的分
類器（亦即：LMT [16]、naive Bayes[14] 、
SMO [18]及C4.5 [19]）。第一個實驗用了
4種不同類型的UCI資料集[26] （亦即：
蝴蝶花 (Iris) 資料集 [26]、乳癌 (Breast  
Cancer)資料集[26]、糖尿病(Pima Diabetes)
資料集[26]及MPG(Mile Per Gallon)資料
 7
1999, vol. 5, pp. 538-540. 
[7] S. M. Chen and J. D. Shie, “A new 
method for feature subset selection for 
handling classification problems,” 
Proceedings of the 2005 IEEE 
International Conference on Fuzzy 
Systems, Reno, Nevada, 2005, pp. 
183-188.   
 [8] T. M. Cover and P. E. Hart, “Nearest 
Neighbor Pattern Classification,” 
IEEE Transactions on Information 
Theory, vol. IT-13, no. 1, pp. 21-27, 
1967. 
[9] R. K. De, N. R. Pal, and S. K. Pal, 
“Feature analysis: Neural network and 
fuzzy set theoretic approaches,” 
Pattern Recognition, vol. 30, no. 10, 
pp. 1579-1590, 1997. 
[10] M. Dong and R. Kothari, “Feature 
subset selection using a new definition 
of classifiability,” Pattern Recognition 
Letters, vol. 24, no. 9, pp. 1215-1225, 
2003. 
[11] R. A. Fisher, “The use of multiple 
measurements in taxonomic 
problems,” Annals of Eugenics, vol. 7, 
no. 2, pp 179-188, 1936. 
[12] J. A. Hartigan and M. A. Wong, “A 
k-means clustering algorithm,” 
Journal of the Royal Statistical Society: 
Series C (Applied Statistics), vol. 28, 
no. 1, pp. 100-108, 1979. 
[13] G. H. John, R. Kohavi, and K. Pfleger, 
“Irrelevant features and the subset 
selection problem,” Proceedings of the 
Eleventh International Conference on 
Machine Learning, San Francisco, 
California, 1994, pp. 121-129. 
[14] G. H. John and P. Langley, “Estimating 
continuous distributions in Bayesian 
classifiers,” Proceedings of the 
Eleventh Conference on Uncertainty 
in Artificial Intelligence, Montreal, 
Canada, 1995, pp. 338-345. 
[15] B. Kosko, “Fuzzy entropy and 
conditioning,” Information Sciences, 
vol. 40, no. 2, pp. 165-174, 1986. 
[16] N. Landwehr, M. Hall, and E. Frank, 
“Logistic model trees,” Proceedings of 
the 14th European Conference on 
Machine Learning, Cavtat-Dubrovnik, 
Croatia, 2003, pp. 241-252. 
[17] H. M. Lee, C. M. Chen, J. M. Chen, 
and Y. L. Jou,” An efficient fuzzy 
classifier with feature selection based 
on fuzzy entropy”, IEEE Transactions 
on Systems, Man, and 
Cybernetics-Part B: Cybernetics, vol. 
31, no. 3, pp. 426-432, 2001. 
[18] J. C. Platt, “Using analytic QP and 
sparseness to speed training of support 
vector machines,” Proceedings of the 
Thirteenth Annual Conference on 
Neural Information Processing 
Systems,” Denver, Colorado, 1999, pp. 
557-563. 
[19] J. R. Quinlan, C4.5: Programs for 
Machine Learning, San Francisco: 
Morgan Kaufmann, 1993. 
[20] J. R. Quinlan, “Induction of decision 
trees,” Machine Learning, vol. 1, no. 1, 
pp. 81-106, 1986. 
[21] C. Schaffer, “Overfitting avoidance as 
bias,” Machine Learning, vol. 10, no. 
2, pp. 153-178, 1993. 
[22] M. Stone, “Cross-validatory choice 
and assessment of statistical 
prediction,” Journal of the Royal 
Statistical Society, vol. 36, no. 2, pp. 
111-147, 1974. 
[23] E. C. C. Tsang, D. S. Yeung, and X. Z. 
Wang, “OFFSS: Optimal fuzzy-valued 
feature subset selection,” IEEE 
Transactions on Fuzzy Systems, vol. 
11, no. 2, pp. 202-213, 2003. 
[24] L. A. Zadeh, “Fuzzy sets,” 
Information and Control, vol. 8, pp. 
338-353, 1965. 
[25] L. A. Zadeh, “The concept of 
linguistic variable and its application 
to approximate reasoning - I,” 
Information Sciences, vol. 8, no. 3, pp. 
199-245, 1975. 
[26] UCI Repository of Machine Learning 
Databases and Domain Theories. 
ftp://ftp.ics.uci.edu/pub/machine-learn
ing-databases/. 
[27] Weka (A Free Software), 
http://www.cs.waikato.ac.nz/ml/weka/. 
 
表 Y04 2
此研討大會的議程共分為下列幾個 Sessions： 
(1)  Applications of Intelligent Techniques for Engineering， 
(2)  Intelligent Systems (I)， 
(3)  Intelligent Systems (II)， 
(4)  Intelligent Interface Systems， 
(5)  Intelligent Systems (III)， 
(6)  Intelligent Systems (IV)， 
(7)  Intelligent and Multi Agents， 
(8)  Intelligent Computational Models， 
(9)  Service Intelligence and Service Science， 
(10) Business Intelligence，  
(11) Intelligence Applications in Natural Languages (I)， 
(12) Intelligence Applications in Natural Languages (II)， 
(13) Applied Visual Information Processing (I)，  
(14) Applied Visual Information Processing (II)， 
(15) Applied Visual Information Processing (III)， 
(16) Ontology Technology and Its Applications)， 
(17) Computational Intelligence for Image Processing (I)， 
(18) Computational Intelligence for Image Processing (I)， 
(19) Machine Learning Applications (I)， 
(20) Machine Learning Applications (II)， 
(21) Intelligent Green Production Systems， 
(22) Generalization Error and Learning Algorithm， 
(23) Neural Control， 
(24) Neural Networks， 
(25) Multiple Classifier Systems， 
(26) Support Vector Machine， 
(27) Mathematical Foundations for Machine Learning， 
(28) Intelligent Control (I)， 
(29) Intelligent Control (II)， 
(30) Fault Detection and Diagnosis， 
(31) Fuzzy Systems (I)， 
(32) Fuzzy Systems (II)， 
(33) Fuzzy Systems (III)， 
(34) Particle Swarm Optimization， 
(35) Rough Sets, Fuzzy Rough and Granular Computing， 
(36) Intelligent Control Systems Based on Fuzzy Methods & NN，  
(37) Data Mining， 
(38) Clustering & Case-Based Reasoning， 
表 Y04 1
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                        97 年 7 月 16 日 
報告人姓名  
陳  錫  明 
 
服務機構
及職稱 
國立台灣科技大學 
資訊工程系教授 
     時間 
會議 
     地點 
民國 97 年 7 月 12 日至
民國 97 年 7 月 15 日 
Kunming, China 
本會核定
補助文號
 
NSC 95-2221-E-011-116-MY2 
會議 
名稱 
 (中文) 2008 年國際機器學習與人工頭腦學研討會 
 (英文) 2008 International Conference on Machine Learning and 
Cybernetics  
發表 
論文 
題目 
1. Li-Wei Lee and Shyi-Ming Chen, “A new method for fuzzy multiple 
attributes group decision-making based on the arithmetic operations of 
interval type-2 fuzzy sets,” Proceedings of the 2008 International 
Conference on Machine Learning and Cybernetics, Kunming China, pp. 
3084-3089, July 2008. 
2. Li-Wei Lee and Shyi-Ming Chen, “Fuzzy multiple attributes group 
decision-making based on the extension of TOPSIS method and interval 
type-2 fuzzy sets,” Proceedings of the 2008 International Conference on 
Machine Learning and Cybernetics, Kunming China, pp. 3260-3265, July 
2008. 
3. Li-Wei Lee and Shyi-Ming Chen, “Fuzzy multiple attributes hierarchical 
group decision-making based on the ranking values of interval type-2 
fuzzy sets,” Proceedings of the 2008 International Conference on Machine 
Learning and Cybernetics, Kunming China, pp. 3266-3271, July 2008.  
4. Li-Wei Lee and Shyi-Ming Chen, “Weighted fuzzy interpolative reasoning 
based on interval type-2 fuzzy sets,” Proceedings of the 2008 International 
Conference on Machine Learning and Cybernetics, Kunming China, pp. 
3379-3384, July 2008.  
5. Yuan-Kai Ko, Shyi-Ming Chen and Jeng-Shyang Pan, “Weighted fuzzy 
interpolative reasoning for sparse fuzzy rule-based systems based on 
transformation techniques,” Proceedings of the 2008 International 
Conference on Machine Learning and Cybernetics, Kunming China, pp. 
3613-3618, July 2008. 
6. Shih-Ming Bai and Shyi-Ming Chen, “A new method for automatically 
constructing concept maps based on data mining techniques,” Proceedings 
of the 2008 International Conference on Machine Learning and 
Cybernetics, Kunming China, pp. 3078-3083, July 2008.  
附件三
 
表 Y04 3
(22) Intelligent Multimedia Data Analysis 
(23) Fuzzy Systems (II) 
(24) Text and Multimedia Information Retrieval (II) 
(25) Applications of Intelligent Systems (VII) 
(26) Computational Biology and Bioinformatics 
(27) Classification and Applications 
(28) Data Mining and Knowledge Discovery 
(29) Applied Intelligent Systems 
(30) Procedural Learning Methods (I) 
(31) Multisensor Information Fusion and Its Applications 
(32) Data Mining and Knowledge Discovery (II) 
(33) Procedural Learning Methods (II) 
(34) Fuzzy Set Theories and Methods 
(35) Science, Engineering and Business Intelligence (I) 
(36) Structural Learning and Feature Selection 
(37) Data Fusion and Ensemble Methods 
(38) Science, Engineering and Business Intelligence (II) 
另外，此研討會亦安排有下列10個Poster Sessions： 
(1) Visual and Audio Information Processing 
(2) Machine Learning and Applications 
(3) Data Fusion and Ensemble Methods 
(4) Data Mining and Knowledge Discovery 
(5) Systems Modeling and Analysis 
(6) Systems and Controls 
(7) Intelligent System and Web Analytics 
(8) Fuzzy Methods and Procedural Learning Methods 
(9) Science, Engineering and Business Intelligence 
(10) Multimedia Information Management 
