 行政院國家科學委員會補助專題研究計畫
■ 成 果 報 告   
□期中進度報告 
 
以FPGA為基礎之Generalized Hebbian Algorithm硬體實現及
其在可執行快速圖型辨認之單晶片學習系統之應用 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC 100－2221－E－003－018－ 
執行期間： 100年 8月 1日至  101年 8月 31日 
 
計畫主持人：黃文吉 
共同主持人： 
計畫參與人員:李偉豪、莊子昕、范哲誠、陳昊、高嘉翎  
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
■赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立台灣師範大學資訊工程學系 
 
中   華   民   國 101年 9月 30 日 
 
process. The architecture in [18] separates the weight vector updating process of GHA into a 
number of stages for data reuse. Although the architecture has fast computation time, its hardware 
resource utilization grows linearly with the dimension of data and number of principal 
components. Therefore, the architecture may not be well suited for data with high vector 
dimension and/or large number of principal components. A systolic array with low area costs is 
proposed in [19]. The systolic array is based on pixel-wise operations so that the area costs for 
weight vector updating are independent of vector dimension. Nevertheless, the latency of the 
architecture increases with the dimension of data. Moreover, similar to the architecture in [18], 
the area costs of [19] grow with the number of principal components. Therefore, the architecture 
may still have long latency and high area costs. 
In light of the facts stated above, a novel GHA implementation capable of performing fast PCA 
with low power consumption is presented in this project. The implementation is based on Field 
Programmable Gate Array (FPGA) because it consumes lower power over its multicore 
counterparts [20,21]. As compared with existing FPGA-based architectures for GHA, the 
proposed architecture has lower area cost and/or lower latency. The proposed architecture can be 
divided into three parts: the SynapticWeight Updating (SWU) unit, the Principal Components 
Computing (PCC) unit, and the memory unit. The memory unit is the on-chip memory storing 
training vectors and synaptic weight vectors. Based on the data stored in the memory unit, the 
SWU and PCC units are then used to compute the principal components and update the synaptic 
weight vectors, respectively. In the SWU and PCC units, the input training vectors and synaptic 
weight vectors are separated into a number of non-overlapping blocks for principal component 
computation and synaptic weight vector updating. Both the SWU and PCC units operate one 
block at a time. In each unit, the operations of different blocks share the same circuit for reducing 
the area costs. Moreover, in the SWU unit, the results of precedent weight vectors will be used 
for the computation of subsequent weight vectors for reducing training time. 
To demonstrate the effectiveness of the proposed architecture, a texture classification system on a 
System-On-Programmable-Chip (SOPC) platform is constructed. The system consists of the 
proposed architecture, a softcore NIOS II processor [22], a DMA controller, and a SDRAM. The 
proposed architecture is adopted for finding the PCA transform by the GHA training, where the 
training vectors are stored in the SDRAM. The DMA controller is used for the DMA delivery of 
the training vectors. 
The softcore processor is only used for coordinating the SOPC system. It does not participate the 
GHA training process. As compared with its multithreaded software counterpart running on Intel 
multicore processors, our system has lower computational time and lower power consumption for 
large training set. All these facts demonstrate the effectiveness of the proposed architecture. 
4. The Proposed Architecture 
Figure 1 shows the neural model for GHA, where x(n) = [x1(n),…,xm(n)]
T
, and y(n) = 
[y1(n),…,yp(n)]
T
 are the input and output vectors to the GHA model, respectively. In addition, m 
and p are the vector dimension and the number of Principal Components (PCs) for the GHA, 
respectively. The output vector y(n) is related to the input vector x(n) by 



m
i
ijij nxnwny
1
)()()( , j=1,…, p,           (1) 
where the wji(n) stands for the weight from the i-th synapse to the j-th neuron at iteration n.  
  
 
Figure 3. The Hardware implementation of Equations (4) and (5). 
 
Figure 4. The Architecture of Each Module in the SWU Unit. 
4.1 The SWU architecture 
We first define 



j
k
kkiiji nynwnxnz
1
)()()()( ,           (3) 
and zj(n)=[zj1(n),…,zjm(n)]
T
. It then follows that    
)()()()1( nznynwnw jijjiji  ,           (4) 
where zji(n) can be obtained from z(j-1)i(n) by 
.,...,2),()()()( )1( pjnynwnznz jjiijji            (5) 
When j=1, we can see from eqs. (3) and (5) that z0i(n)= xi(n). Figure 3 depicts the hardware 
implementation of Equations (4) and (5). As shown in the figure, the SWU unit produces one 
synaptic weight vector at a time. The computation of wj(n+1), the j-th weight vector at the 
Buffer B holds the values of zj(n) for the computation in PCC and SWU units. The data in Buffer 
B is initialized by Buffer A. That is, the initial content of Buffer B is x(n) (i.e., z0(n)), which is 
divided into b blocks. Buffer B then provides z0(n) to PCC unit for the computation of yj(n) on a 
block-by-block basis. Because z0(n) are used for the operations in PCC and SWU units, all the 
data output to PCC unit is also rotated back to Buffer B. After the PCC computation is completed, 
the Buffer B then delivers data for SWU unit. Starting from z0(n), the Buffer B provides zj(n) to 
SWU unit, and then receives zj+1(n) from SWU unit for j=0,…,p-1. The delivery of zj(n) and 
collection of zj+1(n) are also on a block-by-block basis. 
The Buffer C contains the synaptic weight vectors wj(n), j = 1,…, p. In addition to providing and 
storing data for the computation in PCC and SWU units, it also holds the final results after GHA 
training. Similar to Buffer B, each synaptic weight vectors wj(n) is divided into b blocks. They 
are delivered to PCC unit sequentially for the computation of yj(n). Moreover, since wj(n) is also 
needed for the computation of wj+1(n) in the SWU unit, the b blocks delivered to the PCC unit 
should also be rotated back to Buffer C. 
5. The Experimental Results 
Figures 6 and 7 show the Classification Success Rate (CSR) distribution of the proposed 
architecture for the textures shown in Figures 8 and 9, respectively. The CSR is defined as the 
number of test vectors which are successfully classified divided by the total number of test 
vectors. The number of principal components is p = 4. The vector dimensions are m = 16 × 16 
and 32 × 32. The distribution for each vector dimension is based on 20 independent GHA training 
processes. The CSR distribution of the architecture presented in [18] with the same p is also 
included for comparison purpose. The vector dimension for [18] is m = 4 × 4. 
The size of each texture in Figures 8 and 9 is 576×576. In the experiment, the Principal 
Component based k Nearest Neighbor (PC-kNN) rule is adopted for texture classification. Two 
steps are involved in the PC-kNN rule. In the first step, the GHA is applied to the input vectors to 
transform m dimensional data into p principal components. The synaptic weight vectors after the 
convergence of GHA training are adopted to span the linear transformation matrix. In the second 
step, the kNN method is applied to the principal subspace for texture classification. 
 
Figure 6. The CSR distributions of the proposed architecture for the texture set shown in Figure 9. 
advantages of all the cores in the processors. There are 16 threads in the codes: 8 threads for 
synaptic weight updating, and 8 threads for the principal component computation and others. An 
optimizing compiler (offered by Visual Studio) is used to further enhance the computational 
speed. It can be clearly observed from Figure 10 that the proposed architecture attains high speed 
up over its software counterparts. In particular, when the number of training iterations reaches 
1000, the CPU time of the proposed SOPC system is 733.14 ms. By contrast, the CPU time of 
Intel i7 is 1,0125.37 ms. The speedup of proposed architecture over the software counterpart is 
therefore 13.81. 
In addition to having superior computational speed, the proposed architecture consumes lower 
power. Table 1 shows the power consumption of various GHA implementations. For the power 
estimation of GHA software implementations, the tool Joulemeter (developed by Microsoft 
Research) [24] is used. The power consumed by the proposed architecture is estimated by the 
PowerPlay Power Analyzer Tool [25] provided by Altera.  
 
Figure 9. The set of textures for CSR measurements in Figure 8. 
 
Figure 10. The CPU time of the SOPC system using the proposed architecture as the hardware accelerator 
6. Concluding Remarks 
Experimental results reveal that the proposed GHA architecture has superior speed performance 
over its software counterparts and other GHA architectures. With lower clock rate and higher 
vector dimension, the proposed architecture still has faster computation speed over the 
architecture in [19]. In addition, the architecture is able to attain higher CSR for texture 
classification as compared with other GHA architectures. In fact, all the CSRs are above 90% for 
all the experiments considered in this project. The proposed architecture also has low area costs 
for fast PCA analysis with high vector dimension up to m = 32 × 32. The utilization of memory 
bits and embedded multipliers for FPGA implementation are independent of the vector dimension 
and the number of principal components. The proposed architecture therefore is an effective 
alternative for on-chip learning applications requiring low area costs, high classification success 
rate and high speed computation. 
 
References 
 
1. Jolliffe, I.T. Principal Component Analysis, 2nd ed.; Springer: Berlin, Heidelberg, Germany, 
2002. 
2. Dagher, I.; Nachar, R. Face recognition using (incremental ) IPCA-ICA algorithm. IEEE 
Trans. Pattern Anal. Mach. Intell. 2006, 28, 996–1000. 
3. Kim, K.; Franz, M.O.; Scholkopf, B. Iterative kernel principal component analysis for image 
modeling. IEEE Trans. Pattern Anal. Mach. Intell. 2005, 27, 1351–1366. 
4. Roweis, S. EM algorithms for PCA and SPCA. Adv. Neural Inf. Process. Syst. 1998, 10, 
626–632. 
5. Sajid, I.; Ahmed, M.M.; Taj, I. Design and Implementation of a Face Recognition System 
Using Fast PCA. In Proceedings of the IEEE International Symposium on Computer Science 
and its Applications, Hobart, Australia, 13–15 October 2008; pp. 126–130. 
6. Sharma, A.; Paliwal, K.K. Fast principal component analysis using fixed-point algorithm. 
Pattern Recogn. Lett. 2007, 28, 1151–1155. 
7. Boonkumklao, W.; Miyanaga, Y.; Dejhan, K. Flexible PCA Architecture Realized on FPGA. 
In Proceedings of the International Symposium on Communications and Information 
Technologies, ChiangMai, Thailand, 14–16 November 2001; pp. 590–593. 
8. Chen, T.-C.; Liu, W.; Chen, L.-G. VLSI Architecture of Leading Eigenvector Generation for 
On-Chip Principal Component Analysis Spike Sorting System. In Proceedings of the 30th 
Annual International Conference of the IEEE Engineering in Medicine and Biology Society, 
Vancouver, BC, Canada, 20–24 August 2008; pp. 3192–3195. 
9. Chen, D.; Han, J.-Q. An FPGA-based face recognition using combined 5/3 DWT with PCA 
methods. J. Commun. Comput. 2009, 6, 1–8. 
10. Ngo, H.T.; Rajkiran, G.; Asari, V.K. A Flexible and Efficient Hardware Architecture for 
Real-Time Face Recognition Based on Eigenface. In Proceedings of the IEEE Computer 
Society Annual Symposium on VLSI, Tampa, FL, USA, 11–12 May 2005; pp. 280–281. 
11. Gottumukkal, R.; Ngo, H.T.; Asari, V.K. Multi-lane architecture for eigenface based 
real-time face recognition. Microprocess. Microsyst. 2006, 30, 216–224. 
12. Pavan Kumar, A.; Kamakoti, V.; Das, S. System-on-programmable-chip implementation for 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 100－2221－E－003－018 
計畫名稱 
以 FPGA為基礎之Generalized Hebbian Algorithm硬體實現及其在可執行快
速圖型辨認之單晶片學習系統之應用 
出國人員姓名 
服務機關及職稱 
黃文吉 
國立台灣師範大學資訊工程學系教授 
會議時間地點 August 19-24, 2012, Rome, Italy 
會議名稱 
The Fifth International Conference on Advances in Circuits, Electronics and 
Micro-electronics 
發表論文題目 
Wen-Jyi Hwang, Zhe-Cheng Fan, Tsung-Mao Shen, “Unsupervised Image 
Segmentation Circuit Based on Fuzzy C-Means Clustering,” Proc. the 5-th 
International Conference on Advances in Circuits, Electronics, and 
Microelectronics, pp.23-30, 2012. 
 
一、參加會議經過 
 
International Conference on Advances in Circuits, Electronics, and Microelectronics 
(CENICS) 2012 於 8月 19至 24日在義大利羅馬舉行，此會議是電路設計的一個重要年
會，很榮幸可在此會議發表論文並與國際學者交流。在行程上於 8月 16日晚上自桃園國
際機場出發，到達住宿處已是 8月 17日中午。我的論文發表是以 Oral 的方式安排 8月
21日在名稱為 Special and Signal Processing Circuit 之 Session 中報告。參與此 Session 的
學者及學生對於我所提出的 Image Segmentation 硬體架構及效能展現高度興趣，並從他
們的 Feedback 中獲得許多啟發。在 8月 20-24 四天皆參與相關之 Session，並與相關專長
學者對於如何有效應用 Reconfigurable Computing技術等議題交換意見。我於 8月 25日
自 Rome回國，並於次日抵達桃園機場。 
 
二、與會心得 
 
CENICS 會議的主題有 Circuits, Electronics 以及 Micro-Electronics 等，與會學者多從
事於這些領域的理論的研究，因此對於如何有效以硬體實現演算法則較為陌生，我對於
此領域的介紹引起許多學者的興趣，特別是利用 FPGA以及 Reconfigurable Computing來
降低演算法則計算時間方面，對於我們目前的研究成果表示肯定，我也很高興在相關領
域的表現受到重視。 
透過出席此會議，除了與相關領域學者分享研究成果外，也獲得許多寶貴回餽。因此
對於整合 Computer Vision/Image Processing、Reconfigurable Computing與 Embedded 
Computing之技術與未來的應用發展方向等，皆有新的想法，並擬與國際的學者一起推
動完成，此次出國，實不虛此行。 
100 年度專題研究計畫研究成果彙整表 
計畫主持人：黃文吉 計畫編號：100-2221-E-003-018- 
計畫名稱：以 FPGA 為基礎之 Generalized Hebbian Algorithm 硬體實現及其在可執行快速圖型辨認之
單晶片學習系統之應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 1 1 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
The major contribution of this project is to present a novel VLSI architecture 
for the feature extraction of images. The architecture is based on Generalized 
Hebbian Algorithm (GHA) algorithm with low area costs and high computation speed. 
The utilization of memory bits and embedded multipliers for FPGA implementation 
are independent of the vector dimension and the number of principal components. 
The architecture therefore is ideal for applications requiring large input vector 
dimension and/or large number of principal components. The texture segmentation 
is considered in this project for the applications of feature extraction. 
Experimental results also reveal that the proposed architecture has high 
classification success rate. 
 
To demonstrate the superiority of our design, the comparisons of the proposed GHA 
architecture with its software counterparts and other GHA architectures have been 
made. With lower clock rate and higher vector dimension, the proposed architecture 
still has faster computation speed over existing architectures. In addition, the 
architecture is able to attain higher classification success rate for texture 
classification as compared with other GHA architectures. In fact, all the 
classification success rates are above 90% for all the experiments considered in 
