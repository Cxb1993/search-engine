is developed. It provides us with efficient tools to get better
insight into learning mechanisms. We develop the improved
optimization algorithms for the parameters identification of
chaotic systems.
II. PARTICLE SWARM OPTIMIZATION
Particle swarm optimization was introduced by Kennedy
and Eberhart in 1995 [6] as an alternative to genetic algo-
rithms. The particle swarm optimization technique has ever
since turned out to be a competitor in the field of numerical
optimization. As with the genetic algorithm, a particle swarm
optimization consists of a number of individuals refining
their knowledge of the given search space. Particle swarm
optimizations are inspired by particles moving around in the
search space. The individuals in a particle swarm optimiza-
tion thus have a position, a velocity and are denoted par-
ticles. The particle swarm optimization traditionally has no
crossover between individuals, has no mutation and particles
are never substituted by other individuals during the run.
Instead, the particle swarm optimization refines its search
by attracting the particles to positions with good solutions.
The particle swarm optimization remembers the best position
found by any particle. Additionally, each particle remembers
its own previously best found position. With particle i having
position −→xi each particle has its velocity −→vi updated each
iteration in the following way.
−→vi = χ(w−→vi +−→ϕ1i(−→pi −−→xi) +−→ϕ2i(−→pg −−→xi)) (1)
where χ is known as the constriction coefficient described
in [7], w is the inertia weight described in [8][9], −→pi the
best found position for the given particle and −→pg is the best
position known for all particles. and −→ϕ1 and −→ϕ2 are random
values different for each particle and for each dimension. The
position of each particle is updated in each iteration. This is
done by adding the velocity vector to the position vector, i.e.,
−→xi = −→xi +−→vi (2)
Setting for the velocity parameters determine the perfor-
mance of the particle swarm optimization to a large extent.
The random values −→ϕ1 and −→ϕ2 are usually set to be upper
bounded by 2.0. This is explained in [6] to give the random
values a mean of 1.0 resulting in a 50% change of over-
or undershooting when targeting the individual best points−→pi and −→pg . This results in random variation and keeps the
search flexible. Instead of using the constriction coefficient
χ one can limit the velocity by an upper threshold vmax,
which is used if the velocity is higher in a given dimension.
The inertia weight controls the search of the particle swarm
optimization balancing the ability of global and local search.
Research by Shi [10] found fixed inertia weight settings to
be too constrictive. Instead, a linear decreasing inertia weight
can explore the search space in the beginning and fine-tune
solutions towards the end.
As described previously, particle swarm optimization has
certain parameters that require tuning to work well. This is
also the case with other stochastic search algorithms e.g.
for tuning of genetic algorithm mutation rates. However,
changing a particle swarm optimization parameter can have
a proportionally large effect. Leaving out the constriction
coefficient and not constricting particle speed by a maximum
velocity will result in a buildup of speed, partially also
because of the particle inertia. Particles with such speeds
might explore the search space, but loose the ability to fine-
tune a result. Constricting the particle speed too much might
however cripple the search space exploration. Tuning the
constriction coefficient or settings for maximum velocity is
thus not a trivial task. Neither is the tuning of the settings
for the inertia weight or the random values of ϕ. These
parameters also control the abilities of the particle swarm
optimization. The higher the inertia weight, the higher the
particle speed. As with the constriction coefficient, the setting
of the inertia must balance between having good exploration
of the search space and good fine-tuning abilities. As de-
scribed previously the settings of ϕ determine how much the
particle will overshoot when targeting the best points found
previously by itself and by the whole swarm. The setting for
these parameters thus also determines the exploratory versus
the fine-tuning abilities of the particle swarm optimization.
With the tradeoffs just described, the performance of the
particle swarm optimization is problem dependent. General
rules of thumb, as described above, can be used but do not
guarantee optimal particle swarm optimization performance.
Most of the work done with particle swarm optimiza-
tion has been investigating parameter tuning and comparing
particle swarm optimizations to genetic algorithms. Not
much work has been done trying to improve the particle
swarm optimization technique, which is the basic idea in
this research project. Research by Kennedy [11][12] explore
the possibilities of restricting the particle interaction by
introducing a neighborhood topology. The idea is that the
best found position is communicated more slowly between
particles as particles only exchange information with neigh-
boring particles in the topology. The results indicate that a
topology of type star (fully connected network) is generally
good. Selecting an optimal topology is however problem
dependent.
Shi and Eberhart conclude [13][14] that the performance
of the particle swarm optimization is not sensitive to the pop-
ulation size. This means that the particle swarm optimization
will work well with a low number of particles compared to
the number of individuals needed for a genetic algorithm to
work well. As each particle has one fitness function evalua-
tion per iteration the number of fitness function evaluations
can be reduced or more iterations can be performed.
III. A NEW APPROACH TO PARTICLE MOTION IN SWARM
OPTIMIZATION
Since the origin of particle swarm optimization, many
researcher have been devoted to improving its performance,
and therefore, many revised versions of particle swarm
optimization have been proposed. These various improved
rather slow convergence rate, however, on some function
optimization problems. In this paper, by also introducing
chaotic dynamics to simulated annealing, we propose an
improved particle swarm optimization with chaotic annealing
technique. The distinctions between chaotic annealing and
simulated annealing are chaotic initialization and chaotic
sequences replacing the Gaussian distribution. The key idea
of chaotic annealing is to take full advantages of ergodic
property and stochastic property of chaotic system and
replace the Gaussian distribution by chaotic sequences in
simulated annealing.
IV. PARAMETER IDENTIFICATION OF CHAOTIC SYSTEM
Let us refer to the device generating the observed time
series as the primary system, while the secondary system is
the model with parameters to be adjusted. Both systems are
coupled in a way that ensures synchronization when they
are identical, i.e., when the parameters in the model exactly
coincide with the primary system parameters. However, since
the latter are unknown, the time evolution of the secondary
system state variables is, in principle, different from that of
the primary system. Our aim is to derive an algorithm that
adaptively adjusts the model parameters until its response is
matched to the observed time series. When this is achieved
(according to an adequate criterion), the secondary system
is synchronized and its parameters are estimates of the true
parameters in the primary system. To be specific, let
x˙ = f(x, p) (8)
represent the primary system with state variables x ∈ Rn,
and unknown parameters p ∈ Rm. Notice that x˙ represents
the temporal derivative of x. For conciseness, we usually
leave the time dependence of dynamic variables implicit.
If the functional form of (8) is known, we can build the
secondary system as
x˙ = f(x, q) (9)
where y ∈ Rn are the state variables and q ∈ Rm are
the parameters. We assume that there exists a unidirectional
coupling scheme using any signal output of the primary
system (8) which enables asymptotic synchronization of the
secondary system (9), i.e., y → x as t → ∞, as long as
q = p. Notice that techniques are now available which enable
the design of an unidirectional scheme which guarantees
synchronization. The system in (9) is fully observed and we
assume the ability to periodically change the value of the
parameter vector q. Finally, let h(x) : Rn → Rk be the time
series we observe from the primary system, which consists
of a subset of the dynamic variables in (8). It is assumed
that h(x) contains, at least, the signals needed for adequate
coupling of the systems. Our aim is to devise an algorithm to
adaptively adjust the parameters in the secondary system, q,
until the system variables, y, and the parameters themselves
converge to their counterparts in the primary system, i.e.,
both y → x and q → p. In this way, synchronization between
both systems is achieved and the parameters of the primary
system are identified.
Let x = [x1, . . . , xN ]T ∈ RN be the state vector of the
chaotic system, x˙ is the derivative of the state vector x.
Based on the measurable state vector x = [x1, . . . , xN ]T ,
for particle i, we define the following fitness function
fitness =
k∑
t=0
((x1(t)−xi1(t))2+ · · ·+(xN (t)−xiN (t))2)
(10)
where t = 0, . . . , k. Therefore, the problem of parameter
identification is transformed to that of using the particle
swarm optimization to search for the suitable value of
the parameter q such that the fitness function is globally
minimized.
V. SIMULATION RESULTS
In order to evaluate the proposed parameter identification
strategy, the Logistic map and the Lorenz system are em-
ployed in the simulation study. We begin with a study of a
one-dimensional mapping, the Logistic map, defined by
xk+1 = µxk(1− xk) (11)
where µ is a parameter. It is known [18] that this map exhibits
stable periodic as well as chaotic bahavior, depending on the
value of µ. We are interested in the case where the system is
chaotic, that is, µc < µ ≤ 4, where some periodic windows
have to be excluded. For almost any initial condition x0 in
the interval [0, 1) the iterates of equation (11) form a chaotic
sequence confined to that interval. Here µc = 3.5699456 · · ·
is the accumulation point for the periodic-doubling cascade
[18].
One of the most characteristic attributes of the chaotic
systems is sensitivity to initial conditions. To illustrate this
we depict in Figure 1 the time evolution of two nearby initial
data, whose separation ² is supposed to account for the vari-
ous sources of imprecision or error, for the logistic map at the
value µ = 4.0 of the control parameter. We observe that after
an initial stage of coherent coevolution the two curves deviate
and eventually their difference becomes comparable to the
size of the attractor itself. In other words, chaos amplifies
small errors. Experimentally indistinguishable initial states
will eventually evolve to states that are far apart in phase
space.
We next illustrate the short-time behavior of mean error
growth of small initial errors. Consider a one-dimensional
map xn+1 = f(xn). Let x0, x0 + ² (² ¿ 1) be two initial
conditions slightly displaced with respect to each other. We
follow the trajectories emanating from x0 and x0 + ² and
evaluate after n time units the instantaneous error
D²(n, x0) = |f (n)(x0 + ²)− f (n)(x0)| (12)
For any given ², as x0 runs over the attractor, the evolution
of D²(n, x0) for finite times is both x0-dependent and highly
0 10 20 30 40 50 60 70 80 90 100
0
1
2
3
4
5
6
7
8
Fig. 3. Identification of the parameter µ in Logistic map
0 10 20 30 40 50 60 70 80 90 100
0
1
2
3
4
5
6
7
Fig. 4. Identification of the parameter σ1 in Lorenz system
0 10 20 30 40 50 60 70 80 90 100
0
2
4
6
8
10
12
14
16
18
20
Fig. 5. Identification of the parameter r1 in Lorenz system
0 10 20 30 40 50 60 70 80 90 100
0
10
20
30
40
50
60
Fig. 6. Identification of the parameter b1 in Lorenz system
VI. CONCLUSION
We have proposed a new approach to particle motion
in swarm optimization for the parameters identification of
chaotic systems. The proposed method includes both effects
of chaotic dynamics and swam-based search. Simulation re-
sults of Lorenz system are provided to illustrate the effective-
ness and feasibility of the proposed algorithm. Furthermore,
one may hope that the further elaboration of optimization
theory and algorithms based on computational intelligence
for complex systems by using simple rules for complex
intelligent behaviors will contribute to an interdisciplinary
field of research.
ACKNOWLEDGMENT
This work was supported in part by the National Science
Council, Taiwan under grant no. NSC94-2218-E-214-011.
REFERENCES
[1] J.L. Hudson, M. Kube, R.A. Adomaitis, I.G. Kevrekidis, A.S. Lapedes
ans R.M. Farber, ”Nonlinear signal processing and system identification:
applications to time series from electrochemical reactions,” Chem. Eng.
Science, vol.45, pp.2075-2081, 1990.
[2] U. Parlitz, ”Estimating model parameters from time series by autosyn-
chronization,” Phys. Rev. Lett., vol.76, pp.1232-1235, 1996.
[3] A. Maybhate and R.E. Amritkar, ”Dynamic algorithm for parameter
estimation and its applications,” Phys. Rev. E, vol.61, pp.6461-6470,
2000.
[4] H. Sakaguchi, ”Parameter evaluation from time sequences using chaos
synchronization,” Phys. Rev. E, vol.65, 027201(1-4), 2002.
[5] C. Tao Y. Zhang, G. Du and J. Jiang, ”Estimating model parameters by
chaos synchronization,” Phys. Rev. E, vol.69, 036204(1-5), 2004.
[6] J. Kennedy and R. Eberhart, ”Particle swarm optimization,” In Proceed-
ings of IEEE International Conference on Neural Networks, pp.1942-
1948, 1995.
[7] M. Clerc and J. Kennedy, ”The particle swarm: explosion, stability, and
convergence in a multidimensional complex space,” IEEE Transactions
on Evolutionary Computation, vol.6, no.1 pp.68-73, Feb. 2002.
[8] R.C. Eberhart and Y. Shi, ”Particle swarm optimization: Developments,
applications and resources,” In Proceedings of IEEE International
Conference on Evolutionary Computation, vol.1, pp.81-86, 2001.
