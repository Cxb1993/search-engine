 I 
 
中文摘要 
「景物認知」是行動型機器人最重要的能力之一，也是發展機器人智慧行為的關鍵基礎，唯有
機器人能夠正確了解其周遭環境中靜態與動態物體之間的相互關係，才能夠使得機器人自主地
完成任務。「機器人同時定位、建地圖及移動物體追蹤」將機器人的高階感知推廣至高度動態
的環境，唯有正確地處理機器人與其周遭物體之關聯性，機器人才是真正自主、並且是具有智
慧的。透過「機器人同時定位、建地圖及移動物體追蹤」的演算法，機器人得以取得對周遭動
態環境的高等認知，進而在複雜的環境中達成任務，例如人數眾多的環境。本研究計畫的主要
目標包括：一、增進同時定位、建地圖及移動物體追蹤技術之強健性，進而使機器人在真實的
動態環境中智慧地自主行動；二、一般化同時定位、建地圖及移動物體追蹤技術至異質性機器
人平台；三、多機器人協作式高階感知。本報告將提出在計畫執行中所達成之具體成果與發現。 
英文摘要 
“Scene understanding” is one of the most important capabilities of autonomous robots, as well as 
the key basis for developing intelligent robot behaviors. Establishing the spatial and temporal 
relationships among the robot, stationary objects, and moving objects serves as the basis for scene 
understanding. “Simultaneous Localization, Mapping and Moving Object Tracking” can solve the 
robot perception problem in complex and dynamic environments. Without understanding the 
relationships among itself and its surrounding objects, a robot cannot be truly autonomous and 
intelligent. The study of simultaneous localization, mapping and moving object tracking 
(SLAMMOT) has a great potential of providing higher level scene understanding, and allowing robot 
to accomplish missions in complex, crowded environments. The goals of this research project 
include: (a) enhancement of the robustness of the SLAMMOT technique, based on which the robot 
can behave autonomously and intelligently in dynamic environments, (b) generalization of the 
SLAMMOT technique to heterogeneous robot platforms, and (c) cooperative high level perception in 
multiple robot scenarios. This report addresses our achievements and results during this project. 
關鍵詞 
機器人同時定位與建地圖、景物認知、行動型機器人、感測器整合、群組機器人 
Keywords: 
Simultaneous localization and mapping, scene understanding, mobile robotics, sensor fusion, 
multiple robots  
 1 
 
I. 前言 
智慧型機器人涉及許多技術，其中「景物認知（Scene Understanding）」尤為重要，唯有讓
機器人能夠認知、了解環境，才能使機器人真正自主地行動。在機器人、靜態物體以及動態物
體之間建立其時間與空間上的關係是環境感知的重要基礎。機器人定位是在機器人及靜態物體
之間建立空間上關係的過程；建地圖是在靜態物體之間建立其空間上關係的過程；然而，移動
物體追蹤則是在機器人與移動物體之間、或靜態物體與移動物體之間建立空間及時間上關係的
過程。然而，定位與建地圖是困難的，原因在於現實生活中存在於環境中的種種不確定因素，
任何感知感測器（Perception Sensor）或運動感測器（Motion Sensor）都不可能是完美的。 
可靠的「景物感知」可提供機器人在環境中進行任務的資訊。透過「機器人同時定位、建
地圖及移動物體追蹤」（Simultaneous Localization, Mapping and Moving Object Tracking 或
SLAMMOT）[6, 7]技術，我們可提供對於周遭動態環境更完善的敘述。本計畫旨在三年的研
究過程中：增加既有 SLAMMOT之強健性，提升機器人自身運動估測與移動物體偵測；擴展
SLAMMOT 之一般性，將 SLAMMOT 技術應用於異質性的機器人平台[8]，如圖表 1 所示；
以及研究在多機器人情境下，應用 SLAMMOT技術使多機器人相互合作並於真實環境中共同
完成任務。此外，我們也基於建立的 SLAMMOT系統，研究機器人於真實環境中的行動能力，
如避障、路徑規劃…等。 
 
圖表 1 各式機器人平台。左圖為本實驗室之各式機器人平台；中圖為微星科技（MSI）的輪型機器人平台；右
圖為 Aldebaran Robotics Nao人形機器人平台。 
在整個系統架構中，由於會用到各式不同的機器人平台，所以我們對不同類型的機器人平
台做了一些測試。除了測試人形機器人致動器的可能性之外，我們也同時測試裝置在平台上的
各種感知感測器，其主要包括此平台搭載之數位相機、力感測器、聲納感測器等。充分了解機
器人平台之運動模型（Motion Model）與感知模型（Sensor Model）將使得演算法更可靠。在
不同型的機器人上，我們期望能夠提出更可靠的避障與緊急應變措施，處理環境突然的變化與
機器人本體控制的誤差。 
II. 研究目的 
 3 
 
提出從立體視覺（Stereo）裝置將環境中的靜止與移動物體分離，他們使用強健的一致性隨機
取樣（RANdom SAmple Consensus或 RANSAC）方法來濾掉感知資訊中的移動物體以及雜訊，
雖然 RANSAC方法能夠容許環境中存在著至多一半的物體是動態的，然而，在許多應用環境
中的移動物體是比靜止物體還多的。我們整合了運動感測器的資訊、環境中的靜止物體以及移
動物體地圖、與精確的定位，我們的移動物體追蹤能夠在各式的環境中展示出可靠的結果。 
近來，從移動的視覺影像中還原出非固形物體的外觀以及移動模式（Recovery Non-rigid 
Shape and Motion）是在電腦視覺領域中的重要課題，他們的方法主要來自於分解環境中變因
的技術（Factorization Technique）、以及物體外觀基底表示法（Shape Basis Representation）[15-17]，
這些方法都是屬於批次作業的（Batch），理論上而言，是不適合用於線上即時的應用的；就實
際應用層面而言，這些方法對於運算能力的需求，以及處理物體間的遮蔽（Occlusion）、移動
導致的視覺模糊（Motion Blur）、以及影像的光線問題（Lighting Condition）等方面都仍是尚
待解決的難題。 
IV. 研究方法 
多機器人的高階感知平台包含了使用多個「異質性」的感測器。較大型的機器人行動能力
上通常是受到限制的，且機動性較差，但是可以搭載運算能力較強的電腦，可以輔助小型、低
成本機器人對於資訊的解析；相反地，小型的機器人在運算能力上是受限的、且續航能力較差，
但是具備極佳的機動性。 
發展多機器人的高階感知，將能大幅提升當前各式機器人應用的可靠性，而演算法的強健
性以及能將其一般化至各種異質感測器，則是發展多機器人高階感知系統的重要關鍵。因此，
本計畫的研究重點即為提升 SLAMMOT演算法的強健性、一般性、以及向上延伸至多機器人
的協作感知。 
A. 強健的自身運動估測與移動物體偵測 
機器人自身運動估測（Ego-motion Estimation）與移動物體偵測（Moving Object Detection）
是一體兩面的問題。若機器人有正確的自身運動估測，則能可靠地偵測環境中的移動物
體；同樣地，若有準確的移動物體資訊，則機器人能夠過濾掉這些資訊，並且使用環境
中的靜態物體作自身運動的估測。傳統的最小平方（Least Squares）誤差法容易遭受感
測器的雜訊與環境中的移動物體所影響，而使得估測的結果產生偏差，尤其在高度動態
的環境下更為顯著。此部份研究旨在增進基於水平雷射測距儀之 SLAMMOT演算法中
自身運動估測與移動物體偵測的強健性。 
為了考慮各個測距數據的不確定性，每次雷射資料將不僅單單代表一群確定的點，而是
一群點的「機率分佈」。我們利用基本的線段分割法，在一個雷射掃描中，找出距離差
距較大的相鄰兩個掃描點，作為兩個物體的分段點。由於我們所得到的雷射觀測為多個
點的機率分佈，因此在分段物體的時候，就必須考慮到一個物體他所包含的點的不確定
性。當不確定性越高時，就必須採納一個更寬廣的物體範圍來包含這個物體。 
 5 
 
 
圖表 2 虛擬掃描資料生成示意圖。其中青色點為所生成的虛擬掃描資料，紅色點與藍色點為原始掃描資料，其
中紅色是被分類為靜態之點，藍色為動態點。 
另一方面，一致性隨機取樣法使用幾何分布（Geometric Distribution）描述取樣的統計
特性： 
 
其中 X為隨機變數描述在第 k次白努力嘗試（Bernoulli Trial）獲得成功的機率、b為單
次取樣成功的機率、w為單一資料點為非雜訊點的機率、n為每次資料取樣所需取的資
料點。我們可以進一步整理上式如下： 
 
其中 p為期望嘗試到正確非雜訊資料的機率。上式可以解釋為：給定某個期望成功的機
率，所需的取樣次數。相對於窮舉法，一致性隨機取樣法更精確地決定取樣次數，以提
升演算法效率。針對每次的取樣，必須有相對應的計分函數以指示該次取樣表現之優劣，
我們使用如下的計分機制： 
 
即根據某次取樣下所運算之結果，統計該筆掃描資料之所有區塊中有多少雷射點能建立
合理的資料關聯。多模型的一致性隨機取樣演算法同時提升了自身運動估測與移動物體
偵測的強健性與效率，為 SLAMMOT打下重要的根基。 
 7 
 
關於地圖的建立，局部上（Locally），我們使用格點占據地圖建置（Occupancy Grid 
Mapping）[18]演算法。在已知機器人位置的前提下，演算法基於貝氏定理，根據藉由
實驗測量而得的感測器模型，在離散化的二維空間中估計每個格點為被占據與未被占據
的機率，進而達成環境地圖建置的目的。 
然而實際上，縱使我們對相鄰的感測器資訊使用迭代最近點逼近法（Iterative Closest 
Point）與一致性隨機取樣演算法使機器人有強健的自我運動估測，但在大規模環境中，
累積誤差將使得地圖全域上產生不一致的問題，如圖表 4 中左圖所示。在此處我們使
用整體一致性（Globally Consistency）最佳化調校演算法，利用機器人回到過去的地點
的資訊，搭配使用迭代最近點逼近法推測累積之誤差，並基於一致性隨機取樣演算法所
提供的不確定性估測，使用最小平方誤差法最佳化技術將過程中累積的誤差消除，以得
到整體一致的環境地圖。調整結果如圖表 4中右圖所示。 
 
圖表 4 環境地圖的建立。左圖為因局部累積誤差所造成整體不一致的地圖；右圖為執行整體一致性最佳化調校
後所得整體一致的地圖。 
C. 以視覺為基礎之機器人同時定位、建地圖及移動物體追蹤 
在近十年的機器人發展當中，雷射測距儀（Light Detection and Ranging或 LIDAR）已
占有舉足輕重的地位，然而，在多機器人的應用當中，機動性較高、較小型的機器人勢
必在重量與體積上具有較嚴苛的限制，使得 LIDAR不適合用在該平台上。因此，近年
來有愈來愈多的學者開始發展以視覺為基礎的機器人同時定位與建地圖（Visual 
Simultaneous Localization and Mapping或 vSLAM）。 
為了增進 SLAMMOT 之一般性，此部份的研究目標在使異質性機器人平台都能具備景
物認知的能力，我們分別以單眼視覺（Monocular）相機與雙眼立體視覺（Stereo）相機
作為感知感測器，發展以視覺為基礎的機器人同時定位、建地圖及移動物體追蹤系統
（ Visual Simultaneous Localization, Mapping, and Moving Object Tracking 或
vSLAMMOT）。 
首先，我們建立穩健的單一相機 vSLAM系統。利用同時定位與建地圖的演算法架構，
加上了解單一相機的移動模型和感測模型，可以建出以視覺為基礎的特徵地圖以及相機
 9 
 
 
其中機器人資訊 x、靜態地圖 M、以及動態物體 O 將同時地基於感測器資訊 Z與運動
器資訊 U 被估測；下標 k 為時間索引。實作上，我們使用擴增狀態延伸卡爾曼濾波器
（Augmented State EKF），將機器人自身、靜態地圖以及動態物體三者之狀態同時加入
延伸卡爾曼濾波器的狀態向量之中，使得三者間兩兩的相互不確定性關係能被估測，在
這樣的架構中，移動物體對於機器人自身狀態不再只是被過濾掉的雜訊，而變成能有正
面貢獻的資訊。 
關於運動預測，針對機器人我們使用等速運動（Constant Velocity或 CV）模型與等角速
度運動（Constant Angular Velocity或 CAV）模型描述自身的狀態改變，下式說明由前一
時間點之機器人狀態預測下一時間點之機器人狀態的轉換函數： 
 
其中υ與ω分別為機器人的速度與角速度；x為機器人的三維空間座標；q為機器人
的三維面向角，使用 Quaternion 參數式表達；Δt 為前後之時間差。而針對移動物體
我們則使用等速運動模型描述其運動狀態。 
感測器更新必須仰賴於穩健的移動物體偵測模組，我們使用二元貝氏濾波器（Binary 
Bayesian Filter）分析新特徵點為靜態或動態的可能性： 
 
並定義 
 
 11 
 
 
基於同樣的 vSLAMMOT 理論架構，我們也提出基於雙眼立體視覺相機的 SLAMMOT
系統，並在真實環境中實驗其有效性[5]。對於雙眼相機，我們使用如下的參數變動來
建立其感測器模型： 
 
其中二 h分別對應至左與右邊相機之參考座標；b為兩相機之距離（Baseline）。相對於
使用單一相機的 SLAMMOT，因雙眼立體視覺相機能提供深度資訊，除了效果優於單
相機之外，我們更發現使用雙眼立體視覺相機能使 SLAMMOT 演算法免於觀測度
（Observability）不足的問題。 
D. 多機器人之資訊整合 
當異質性的機器人擁有景物認知能力後，必須要有一個架構來統合、整理多個機器人自
身感測器所得到的觀測，我們以一個擴增狀態延伸卡爾曼濾波器（Augmented State EKF）
 13 
 
與追蹤的系統中經由整合移動物體的資訊，我們能夠得到一個強健的演算法，而經由整
合多個機器人的資訊，我們不但能夠得到一個更強健的演算法，也因為利用了多機器人
之間的協作感知進而得到一個更精確的估測。 
E. 避障演算法 
接近式圖表（Nearness Diagram或 ND）演算法[21, 22]的主要思想在於隨時針對機器
人周圍的環境，做出安全的行動策略。利用不同情況的歸類與反應設定，機器人可以在
遇到危險的時候，採用預先設計好的閃避策略，防止碰撞等意外的發生。在本演算法的
實作過程當中，我們發現了一些可以改進方法，可提供更周全、更有效率的避障行動。 
首先是將原本接近式圖表演算法中的固定大小安全環境，改為可根據環境情況而縮放的
動態大小安全環境。利用移動物體追蹤，我們可以判斷出各個物體遠離或接近機器人的
情況。根據移動物體追蹤所計算出的物體速度，可以適度調整機器人的安全環境。舉例
而言，如圖表 7 所示，在交叉路口上，左前方有來車（綠色實心方塊）垂直於機器人
方向經過，此時機器人可利用移動物體追蹤預測出該車體未來的動向（綠色虛線方塊），
而放棄直走的策略，改採逐漸煞車、適度繞彎而前進的安全作法（紅色線條）。 
另外一項改進則為緊急煞車情況的策略。原本接近式圖表演算法採用的是漸進式煞車方
式，慢慢根據安全環境的應對策略修正機器人的行走方向與速度。此方法固然可提供較
為流暢的移動行為，卻有可能無法即時避免緊急情況的發生。機器人移動的控制指令與
其實際結果往往有相當顯著的差異，因此，倘若遇到超過於移動指令的運動，便需要緊
急煞車的機制來防止碰撞。在多人的環境中，人員突然出現在機器人的行進方向上是相
當重要的安全考量。所以我們的避障策略，將考慮到人員突然出現在距離機器人相當近
的位置上。 
 
 
 
 
 
 
最後一項改良是穩定機器人移動的解決方案。在室內環境中，機器人往往因為避障策略
而在較狹窄的走道中，出現 S型的移動。原本計畫的避障策略乃取機器人兩端最接近的
點，算出這兩點與機器人移動中心的分角線角度，作為機器人避障的方向。在[3]中我
們試著改變避障方向的算法，改採兩端最接近的點的中點，與原點所形成的角度為新的
避障方向。圖表 8 中的藍線即為改良後的新目標方向。相較於原本目標方向（綠線），
圖表 7 判斷移動物體的未來趨勢而提早決定
較安全的路徑 
 15 
 
V. 結果與討論 
A. 強健的自身運動估測與移動物體偵測 
 
圖表 10 使用最小平方誤差法之機器人自身運動估測。其中紅點為機器人當下的感測資訊，灰色為上一個時間點
的感測資訊。 
 
圖表 11 使用多模型一致性隨機取樣的移動物體偵測。其中紅點為機器人當下的感測資訊，灰色為上一個時間點
的感測資訊，綠色為與靜態物體模型不一致的非靜態物體。 
首先，我們展示使用多模型一致性隨機採樣的移動物體偵測結果，圖表 10、圖表 11
與圖表 12展示了使用最小平方誤差法、一致性隨機取樣與多模型的一致性隨機取樣的
結果。由圖可見，最小平方誤差法容易遭致移動物體的影響，使得估測產生偏差；一致
性隨機取樣相對地能提供較正確的估測，然而，在高度動態的環境下仍然不可靠；多模
型的一致性隨機取樣則能克服一致性隨機採樣單一模型的問題，並且同時解決機器人自
身運動估測與移動物體偵測的問題。圖表 13顯示更多應用多模型的一致性隨機取樣法
的結果，在此我們使用 Navlab 同時機器人定位、建地圖、及移動物體追蹤作為測試資
料並展示結果之片段擷圖。 
 17 
 
圖表 15展示多規模多模型一致性隨機取樣演算法的不確定性估測結果，我們的方法正
確的描述了各取樣的計分結果，而相對於其他方法，我們也在大小及形狀上提供了較合
理的估測。圖表 16展示多規模多模型一致性隨機取樣演算法在自身運動估計上相對於
傳統迭代最近點逼近法之準確度提升，可以觀察到，多規模的切割方式提供了更精準的
結果。圖表 17展示多規模多模型一致性隨機取樣法之自身運動估計於水平、垂直、旋
轉的效果，相對於窮舉所有物體運動狀態之可能性，多規模多模型一致性隨機取樣法使
自身運動之估測在有限的取樣次數內收斂。 
 
圖表 14 使用多規模多模型一致性隨機取樣法的自身運動估測與移動物體偵測結果。紅點為當下的感測資料，灰
點為上一個時間點的感測資料，藍框為使用多規模切割法分析出的移動物體；下圖為對照用的數位相機資料。 
 19 
 
 
圖表 17 多規模多模型一致性隨機取樣法在自身運動估計於水平、垂直、旋轉的效果。其中紅線為演算法之估測
結果；藍線為二倍標準差之不確定性估測；綠線為內部估計系統所提供的估測，在此被當作相對精準的參考值。
橫軸為演算法執行的取樣次數。 
B. 機器人定位與路徑規畫 
解決 SLAMMOT問題的目的在於使得機器人能夠在未知且動態的環境中達成景物認知
的任務，而這個目的具體呈現則是環境地圖的建立。在章節 IV.A 與圖表 4，我們已展
示了我們的演算法可以可靠地計算出正確的環境地圖。我們進一步以機器人定位與路徑
規畫的觀點，探討機器人定位、建地圖與移動物體追蹤的可行性。 
 
圖表 18 機器人定位之結果。藍色點為表示機器人位置可能性分布之粒子，紅色為當下雷測距儀之資料，灰色為
使用 SLAMMOT演算法建立之環境地圖。 
蒙特卡羅定位（Monte-Carlo Localization）[23]演算法使用粒子過濾器，在理論上為一種
立基於貝氏過濾器的實作。演算法利用足夠多的粒子來近似機器人於環境中可能位置的
機率分布，因此可以有效的處理多可能性假設與非高斯分部不確定性的情況。 
蒙特卡羅定位演算法可分為二步驟，預測（Prediction）與更新（Update）。第一步驟為
預測階段，演算法利用從機器人控制模組得知的操作指令，配合已知的地圖資訊，對每
個粒子推測機器人可能移動到的新位置，並以此為預測階段之機器人位置分布估測；第
 21 
 
 
圖表 20 以視覺為基礎的機器人同時定位、建地圖及移動物體追蹤成果。左上圖中三角椎為移動物體，深藍色為
己加入地圖且在當下有觀測到的特徵點，深藍色為己加入地圖但在當下並未觀測到的特徵點，紅色為偵測到的移
動物體，橢圓為不確定性估測；右上、左下與右下圖中三角形為數位相機位置，黑色線為相機真實的移動軌跡，
灰色線為演算法計算出的軌跡，藍色小方塊為真實的特徵點位置，藍色小圓為估測的特微點位置，藍色橢圓為估
測的特微點分佈，紅色線為移動物體的真實移動軌跡，紅色橢圓為估測的移動物體分佈。 
  
圖表 21 單相機 SLAMMOT 於真實環境中之結果。圖（ａ）到（ｄ）為相機影像的擷圖，其中黑點為所取之特
徵點。圖（ｅ）與（ｆ）為所建立之地圖與所估測之機器人移動軌跡，分別為鳥瞰圖與側視圖。 
 23 
 
 
圖表 23 單一相機 SLAMMOT 演算法之觀測度議題。左三圖為可觀測狀況下之結果，右三圖為不可觀測狀況下
之結果。其中四角錐為相機位置，相連的線為相機移動軌跡，方框與橢圓為移動物體之估測及其不確定性。 
圖表 24 為使用雙眼立體視覺相機 SLAMMOT演算法於真實環境中實驗的結果。圖表 
25展示雙眼立體視覺相機 SLAMMOT演算法於觀測度不足情況下之結果，可以發現因
為雙眼立體視覺像機的使用，移動物體之估測仍然得以收斂。圖表 26展示雙眼立體視
覺相機 SLAMMOT演算法因相對於單一相機 SLAMMOT多一顆相機，產生較好估測的
結果。 
綜合以上，我們可以發現 SLAMMOT演算法之一般性，因其於異質型感測器上皆展現
有效性。 
 25 
 
D. 協作式多機器人同時定位與移動物體追蹤 
在這個段落，我們以協作式多機器人同時定位與移動物體追蹤（CLAT）來展示多機器
人之間資訊的整合。實驗的場景是 RoboCup標準平台聯盟（Standard Platform League）
比賽的環境，在這樣的情境中，每一隻機器人都必須要知道自己、隊友、敵人以及球的
位置才能夠進行有效率的決策。其中一個狀況如圖表 27所示中間紅框中之機器人因為
沒有足夠定位資訊而使其定位系統發散，而經由多隻機器人之間的合作得以解決此問
題。 
 
圖表 27 RoboCup標準平台聯盟比賽情境，如上圖所示紅框中之機器人因為一直盯著球而沒有足夠的定位資訊。
下方小圖為該機器人所觀測到之影像。 
結果如圖表 28所示，由左至右分別為單一機器人定位、協作式多機器人定位以及協作
式多機器人同時定位與移動物體追蹤。由結果中可以看出在單一機器人定位的情況下，
由於機器人 T2一直盯著球而沒有足夠的定位資訊而無法收斂，位置的估測也因為里程
計的誤差而漸漸偏離。在協作式多機器人定位中經由隊友之間的估測以及訊息的交換使
的估測能夠較接近真實位置。最後，藉由整合移動物體資訊，協作式多機器人同時定位
與移動物體追蹤甚至能夠提供更接近真實位置的估測。我們並且以較精確的雷射測距儀
估測當作真實資料，對於不同演算法進行詳細的分析，將位置誤差（Position Error）以
及不確定性（Uncertainty）的估測列於圖表 29。 
 
圖表 28 於 RoboCup SPL 環境之協作式多機器人同時定位與移動物體追蹤成果，場地內共有三個屬於同一隊伍
的機器人以及一顆球，機器人 T2 的任務被設定為朝球的方向移動並且緊盯著球，因此沒有足夠的定位資訊。由
左至右所使用的方法分別為單一機器人定位、協作式多機器人定位以及協作式多機器人定位與移動物體追蹤。黑
 27 
 
 
 
 
 
 
 
景物認知是行動型機器人的根基，唯有使得機器人能夠了解環境，才能使得機器人是真正
自主的。在真實環境中，移動物體的存在使得景物認知是更加困難的，我們積極發展機器
人同時定位、建地圖及移動物體追蹤的技術，期望能夠做為行動型機器人的理論與應用的
基礎，進而發展更高階的應用，例如機器人保全、服務型機器人、導覽機器人等。 
與本計畫相關之論文著作有：使用多模型一致性隨機取樣之移動物體追蹤[1]、多規模多模
型一致性隨機取樣之同時資料疊合與切割[4]、使用單一相機的同時定位、建地圖、與移動
物體追蹤[2]、使用雙眼立體視覺相機的同時定位、建地圖、與移動物體追蹤[5]、以及自我
調整的接近式圖表演算法[3]。 
 
VII. 參考文獻 
1. Yang, S.-W. and C.-C. Wang. Multiple-Model RANSAC for Ego-motion Estimation in Highly 
Dynamic Environments. in Proceedings of the IEEE International Conference on Robotics 
and Automation. 2009. Kobe, Japan. 
2. Wang, C.-C., et al., Monocular Vision-based Simultaneous Localization, Mapping and 
Moving Object Tracking. The IEEE Transactions on Robotics. 
3. Yu, C.-C., et al., Self-Tuning Nearness Diagram, in Proceedings of the International 
Conference on Service and Interactive Robotics. 2009: Taipei, Taiwan. 
4. Yang, S.-W., C.-C. Wang, and C.-H. Chang, RANSAC Matching: Simultaneous Registration 
and Segmentation, in Proceedings of the IEEE International Conference on Robotics and 
Automation. 2010: Anchorage, Alaska. 
5. Lin, K.-H. and C.-C. Wang, Stereo-based Simultaneous Localization, Mapping and Moving 
Object Tracking, in Proceedings of the IEEE/RSJ International Conference on Intelligent 
Robots and Systems. 2010: Taipei, Taiwan. 
6. Wang, C.-C., C. Thorpe, and S. Thrun. Online Simultaneous Localization And Mapping with 
基於 SLAMMOT之智慧行為 
1. 提出自我調整的接近式圖表演算法達到在真實動態環境中之避障 [3] 
2. 發展真實環境中之路徑規劃系統 
 29 
 
Monocular SLAM. in Robotics: Science and Systems Conference. 2006. 
20. Li, X.R. and V.P. Jilkov, Survey of maneuvering target tracking. Part V: multiple-model 
methods. IEEE Transactions on Aerospace and Electronic Systems, 2005. 
21. J.Minguez and L.Montano, Nearness Diagram(ND) Navigation: Collision Avoidance in 
Troublesome Scenarios. IEEE Transactions on Robotics and Automation, 2004. 
22. Yu, Y.-C., S.-W. Yang, and C.-C. Wang, Safety Distance Learning in Nearness Diagram 
Robot Navigation, in Proceedings of 2nd International Forum on Systems and Mechatronics. 
2007. Tainan, Taiwan. 
23. Thrun, S., D. Fox, and W. Burgard. Monte Carlo Localization with mixture proposal 
distribution. in the AAAI National Conference on Artificial Intelligence. 2000. Austin, TX. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 31 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
 
在本計畫中，我們提出多模型的一致性隨機取樣法完成強健的自身運動估測
與移動物體偵測，提出多規模多模型的一致性隨機取樣法處理資料分割的不
確定性，並發展總體一致的建地圖演算法，這些方法有效的增加系統的強健
性。在一般性的方面，我們提出使用單一相機的同時定位、建地圖與移動物
體追蹤演算法，接著提出使用雙眼立體視覺相機的同時定位、建地圖及移動
物體追蹤演算法，並且於異質性平台（雙足機器人 Nao以及三種不同的輪型
機器人MSN、PAL3、CCI Robot）上驗證同時定位、建地圖與移動物體追蹤
系統之有效性。基於強健性與一般性的發展我們也使用多隻 Nao機器人，發
展多機器人協作式高階感知系統。最後，我們也說明了基於同時定位、建地
圖與移動物體追蹤系統所發展之動態環境避障與路徑規劃系統。 
景物認知是行動型機器人的根基，唯有使得機器人能夠了解環境，才能使得
機器人是真正自主的。在真實環境中，移動物體的存在使得景物認知是更加
困難的，我們積極發展機器人同時定位、建地圖及移動物體追蹤的技術，期
望能夠做為行動型機器人的理論與應用的基礎，進而發展更高階的應用，例
如機器人保全、服務型機器人、導覽機器人等。 
 
 
 
 
 
 
 
 
 
 
 
 
33
Vtjoh!Spcpuȷt!Bsn!Npujpo!ȹǵȹSfbm.Ujnf!Wjtjpo.Cbtfe!Gjsf!Tnplf!
Efufdujpo!TztufnȹǵȹWjtjpo.Cbtfe!Bvupopnpvt!Spcpu!Dpouspm!gps!Qjdl!
boe!Qmbdf!PqfsbujpotȹǵȹB!Gsjfoemz!boe!Joufmmjhfou!Ivnbo.Spcpu!
Joufsgbdf!Tztufn!Cbtfe!po!Ivnbo!Gbdf!boe!Iboe!HftuvsfȹǶፕЎޑൔ֋
ޣ୷ܭჹܭԜ཮᝼ޑख़ຎǴ೿ྗഢΑߚத၁ᅰޑ׫ቹТၗ਑٠མଛᆒߍޑαᓐ
ൔ֋Ǵ܌аૈᡣӧ൑ޑ᠋౲೿ڙ੻ؼӭǶԶ᠋౲ჹܭ೭٤᝼ᚒΨ೿Ԗଯࡋޑᑫ
፪Ǵ܌аӄ൑ޑϕ୏ؼӳǴวୢ࣬྽ޑᏱ៌ǴЪ۳۳೿ૈᗺрനᜢᗖޑୢᚒǴ
ӕਔൔ֋ޣΨ೿ૈ๏ϒനమཱӣเǶջ٬᠋౲کൔ֋ޣଽᅟคݤଭ΢౛ှϕ࣬
ޕ᛽ਡЈޑཀࡘǴӢךӧܭ೭ঁሦୱύᢕࣴӭԃǴ܌аჹܭ࣬ᜢޑޕ᛽೿Ԗ࣬
྽ޑΑှǴ܌аΨ೿ૈ୼፾ਔޑԋࣁ᠋౲کൔ֋ޣϐ໔ޑྎ೯ᐏኺǴᡣ཮᝼຾
Չޑ׳٫໩ճǴ٠Ъᡣεৎӧ೭ঁ཮᝼ύԖ໩ճޑҬࢬǴ஥ӣനӭޑཥޕ᛽Ƕ!
Ӣࣁ౜ӧЋ༈ᒣ᛽ࢂ΋ߐߚத዗ߐޑࣴز᝼ᚒǴӕਔЋ༈ᒣ᛽ΨёаԋࣁჴҔ
ޑΓᆶႝတ܈ᐒᏔΓޑྎ೯ϟय़ǶӢԜӧךॺޑፕЎൔ֋ਔǴ᠋౲ॺჹܭךॺ
ޑፕЎൔ֋ϸᔈ׳ࢂ౦தޑ዗ਗ਼ǶΨӢࣁ೚ӭ࣬ᜢޑሦୱޑഗӾᏢޣΨࣣӧ
০ǴᏢޣॺ܌ගрޑ೚ӭୢᚒ೿Ԗ๱ߚதᐱډޑـှǴவ೭٤ୢᚒύךளډߚ
தӭཥޑགྷݤǴ೭٤གྷݤགྷѸёаᔅշךॺი໗ӧ೭ঁሦୱޑࣴز΢Ԗ๱όӕ
ޑ࣮ݤکཥޑБӛǶ!
ߚதޑଯᑫךॺޑࣴزԋ݀ёа೏ JFFF!Bewbodfe!Joufmmjhfou!Nfdibuspojdt
35
୯ࣽ཮ံշ஑ᚒࣴزीฝ໨Πрৢ୯ሞᏢೌ཮᝼Јளൔ֋!
! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ВයǺ!3121!!ԃ!!7!!Д!31!!В!
ीฝጓဦ! NSC96ɡ2628ɡEɡ002ɡ251ɡMY3!
ीฝӜᆀ! ӭᐒᏔΓӕਔۓՏǵࡌӦკϷ౽୏ނᡏଓᙫ!
р୯Γ঩
ۉӜ! Цണඵ! ୍ܺᐒᄬϷᙍᆀ! Ѡ᡼εᏢ!շ౛௲௤!
཮᝼ਔ໔! 3121ԃ 6Д 4ВԿ!3121ԃ 6Д 9В! ཮᝼Ӧᗺ! ߓ܎ථу!
཮᝼Ӝᆀ!
3121!JFFF!Joufsobujpobm!Dpogfsfodf!po!Spcpujdt!boe!
Bvupnbujpo!
ว߄ፕЎ
ᚒҞ!
SBOTBD!Nbudijoh;!Tjnvmubofpvt!Sfhjtusbujpo!boe!
Tfhnfoubujpo!
JDSBࢂᐒᏔΓᏢࣴزሦୱ္നख़ाޑ୯ሞࣴ૸཮ϐ΋ǴϞԃޑ཮᝼ܭϖДΟВ
ԿϖД 9Вӧߓ܎ථуᖐՉǺ!
!
Ϟԃޑ཮᝼ᆢ࡭ӵӕа۳΋ೣޑ౰ݩǴᡉҢрٰԾ୯ሞࣴزᐒᄬǵቷ୘ޑᏢޣ
܈πำৣॺჹܭᐒᏔΓࣴزሦୱޑଯࡋᜢݙᆶ዗௃Ƕ৖൑ϣ࣮ளډ౲ӭޑ៽Տ
37
!
ךॺ܌ว߄ޑፕЎǺ!SBOTBD!Nbudijoh;!Tjnvmubofpvt!Sfhjtusbujpo!boe!
Tfhnfoubujpo೏௨ӧதೕޑȨຎ᝺ٚำीȩ཮᝼ύǴ၀཮᝼ӕਔᗋԖϖጇፕЎ
ว߄ǴхࡴǺ WJDQ;!Wfmpdjuz!Vqebujoh!Jufsbujwf!Dmptftu!Qpjou!Bmhpsjuinǵ
B!Cfbsjoh.Pomz!3E04E.Ipnjoh!Nfuipe!voefs!Wjtvbm!Tfswpjoh!Gsbnfxpslǵ
Wjtjpo.Cbtfe!Qptf!Ftujnbujpo!gps!Bvupopnpvt!Joepps!Obwjhbujpo!pg!
Njdsp.Tdbmf!Vonboofe!Bjsdsbgu!TztufntǵWjtjpo.Cbtfe!Obwjhbujpo!xjui!
Qptf!Sfdpwfsz!voefs!Wjtvbm!Pddmvtjpo!boe!LjeobqqjohǵUsbotmbujpo!
Ftujnbujpo!gps!Tjohmf!Wjfxqpjou!Dbnfsbt!Vtjoh!MjoftǶୖᆶ཮᝼ޑᏢޣ
ॺࣣ߄౜Αଯࡋޑୖᆶ዗וԶൔ֋ޣΨ೿кϩӦྗഢǴ٠Ъ৖౜ځ஑཰ࡋǶຎ
᝺ٚำीࢂ྽жᐒᏔΓޑЬाᜢᗖ᝼ᚒϐ΋ǴךॺޑፕЎଞჹೀ౛Ԝୢᚒޑਡ
Ј೽ϩ଺ׯؼǴӢԜΨၟᆶ཮ޣౢғΑؼӳޑϕ୏૸ፕǶךॺΨӧԜၸำύ஥
Hand Posture Recognition Using Hidden Conditional Random Fields
Te-Cheng Liu, Ko-Chih Wang, Augustine Tsai and Chieh-Chih Wang
Abstract— Body-language understanding is essential to hu-
man robot interaction, and hand posture recognition is one of
the most important components in a body-language recognition
system. The existing hand posture recognition approaches based
on robust local features such as SIFT can be invariant to
background noise and in-plane rotation. However the ignorance
of the relationships among local features is a fundamental issue.
The part-based models argue that objects of the same category
share the same part-structure which consists of parts and
relationships among parts. In this paper, a discriminative part-
based model, Hidden Conditional Random Fields (HCRFs),
is used to recognize hand postures. Although the existing
global locations of features have been used to consider large
scale dependency among parts in the HCRFs framework, the
results are not invariant to in-plane rotation. New features by
the distance to the image center are proposed to encode the
global relationship as well as to perform in-plane rotation-
invariant recognition. The experimental results demonstrate
that the proposed approach is in-plane rotation-invariant and
outperforms the approach using AdaBoost with SIFT.
I. INTRODUCTION
Body language is an important part of communication
between people. Understanding body language could play
a key role in human robot interaction as illustrated in Figure
1. A body-language recognition system could consist of face
detection and analysis, arm gesture recognition and hand pos-
ture recognition. In our body-language recognition system,
the Viola-Jones face detector [1] and a skin color model [2]
are integrated into a robust frontal face detector. False alarms
from the Viola-Jones face detector can be rejected by the
skin color constraint. A 3D Active Appearance Model [3]
is used to align 3D faces in 2D images for estimating the
orientation of the user’s head. Figure 2(a) shows the positive
face detection result in which a person is facing toward the
robot. Given the location of the face, the upper body of the
detected person is extracted using the disparity map from the
stereo cameras as shown in Figure 2(b). Kernel Sliced Inverse
Regression [4] and Support Vector Machine [5] are used to
train a raising-arm detector. The disparity map of the upper
body is binarized as depicted in Figure 2(c) and used as the
Te-Cheng Liu was with the Department of Computer Science and
Information Engineering, National Taiwan University, Taipei, Taiwan
atwood@robotics.csie.ntu.edu.tw
Ko-Chih Wang is with the Graduate Institute of Networking
and Multimedia, National Taiwan University, Taipei, Taiwan
casey@robotics.csie.ntu.edu.tw
Augustine Tsai is with the Innovative DigiTech-Enabled Applications
and Services Institute, Institute for Information Industry, Taipei, Taiwan
atsai@iii.org.tw
Chieh-Chih Wang is with the Department of Computer Science
and Information Engineering, and the Graduate Institute of Net-
working and Multimedia, National Taiwan University, Taipei, Taiwan
bobwang@ntu.edu.tw
Fig. 1. A person uses his body language to interact with the NTU-PAL2
robot.
(a) Face detection using single
camera
(b) Upper body localization using
stereo cameras
(c) The input of the
raising-arm detector
(d) The extracted
hand posture image
Fig. 2. The results of face detection, raising-arm detection and hand posture
image extraction in our body-language recognition system.
input of the raising-arm detector. The hand posture classifier
is called only when both the face detector and the raising-
arm detector provide positive results. In addition, the face
detector and the raising-hand detector serve as the localizer
of hand posture for further classification as shown in Figure
2(d). As the current human robot interaction design of our
NTU-PAL2 robot depends on hand postures of users, a robust
hand posture classifier is critical.
Wang and Wang [6] proposed to recognize hand posture
using AdaBoost with Scale Invariant Feature Transform
(SIFT) features [7]. Their model is invariant to background
noise, illumination and in-plane rotation. In their model
an image is represented only by a set of SIFT features.
Thus, D is 131 in this case. θ nodeht , j is used to evaluate the
score for a node to be labeled as ht , and is shared by all
classes. θ
edge
yk and θ
node
yl are used to evaluate the score for
a part assignment to be of a class. Moreover, θ
edge
yk is for
the histogram of edges in the part layer. θ nodeyl is for the
histogram of nodes in the part layer. f
edge
yk and f
node
yl are
designed as binary counting features:
f nodeyl (ht) =
{
1, if ht = l ∈ H
0, otherwise
(4)
f
edge
yk (hs,ht) =
{
1, if (hs,ht) = edgek ∈ H ×H
0, otherwise
(5)
The underlying expectation is that each category has its
own distribution of parts different from other categories, and
the scores from f
edge
yk and f
node
yl will discriminate the cases in
which the scores from node features are not discriminative
enough.
B. Learning and Inference
The maximum-likelihood method is applied to estimate θ
of P(y|x). Since the result of optimization remains and the
computation is easier to deal with in the log domain , the
log-likelihood function L(θ) is to be maximized:
L(θ) = logP(θ ;y,x) = logZh(y,x)− logZy′,h(x) (6)
Zh(y,x) is the summation of P(y,h|x) over H. For n train-
ing instances, the objective function is ∑
n
k=1 log(θ ;yk,xk).
The functions of the log-sum-exp form is convex [12].
However this log-likelihood function is the difference of two
functions of this form and thus might not be concave [13].
The multiple initial cases are needed.
With learned θˆ , which class x belongs to and what
the most possible part assignment is for x are two im-
portant questions which are solved simultaneously by
argmaxy′,h P(y
′,h|x). However it is intractable and thus ap-
proximated by two tractable stages using belief propagation:
yˆ = argmax
y
P(y|x; θˆ) (7)
hˆ = argmax
h
P(h|yˆ,x; θˆ) (8)
C. Multi-class Recognition
For the multi-class recognition problems, there are two
main approaches. One is the combination of multiple binary
classifiers and the other is to solve a multi-class problem
directly. The formulation of HCRFs can solve the multi-
class recognition problem directly. In order to compare our
approaches with others, we also implement the one-against-
other approach. Let ci(x) be a classifier for class i. ci(x)1
is the probability that x belongs to class i and ci(x)0 is the
probability that x do not belong to class i. As our system is
currently implemented to recognize three hand postures, we
have a three dimensional classifier ti(x):
{
ti(x)p = cp(x)1
ti(x)q 6=p =
1
2
cp(x)0
(9)
Given the assumption that the prior probability of each
classifier is uniform, we have a combined one-against-other
classifier O(x):
O(x) =
1
3
2
∑
i=0
ti(x) (10)
Experiments on these two approaches are shown in Section
IV-C.
III. PART-STRUCTURE
In this section the cognitive meaning of HCRFs is in-
troduced. In HCRFs, a part-structure is not characterized
by a part assignment. It is expressed in terms of θ
edge
yk
and θ nodeyl . θ
edge
yk is the histogram of the edges of parts
and θ nodeyl is the histogram of the nodes of parts in the
objective function. θ
edge
yk and θ
node
yl does not directly cor-
respond to a part assignment, although the most possible
part assignment can be inferred from θ
edge
yk and θ
node
yl . Such
part-structure representations are more abstract than part
assignments. Therefore, the setting of these parameters is
obscure to human beings. One advantage of HCRFs is that
these parameters for part-structures are learned in a semi-
supervised way.
A. Semi-supervised Learning
A part-based model with supervised learning requires both
of class labels and part labels. The definition and annotation
of parts are two difficult tasks. In HCRFs, the part labels
are hidden and the selection of part-structures is guided
merely by class labels. In the parameter estimation of HCRFs
by maximum-likelihood, the optimization converges if the
gradient of L(θ) reaches ~0. Assume the derivative of L(θ)
with respect to θ nodeyl and the one with respect to θ
edge
yl are
0. Then,
∑
t
P(ht = j|y,x; θˆ) = ∑
y′,t
P(ht = j|y
′,x; θˆ)P(y′|x; θˆ)
(11)
∑
(s,t)∈E
P(hs = a,ht = b|y,x; θˆ) =
∑
y′,(s,t)∈E,a,b
P(hs = a,ht = b|y
′,x; θˆ)P(y′|x; θˆ)
(12)
,in which H is the set of part labels.
The left side of Equation 11 is the sum of the marginal
probabilities for all nodes of class y to be labeled as part
j. The left side of Equation 12 is the sum of marginal
probabilities for all edges of class y to be labeled as (a,b).
The right side of each equation is the same sum weighted
by class probabilities P(y′|x; θˆ). That is, before convergence
θ
edge
yk and θ
node
yl are adjusted according to the difference
between the sum of the marginal probabilities in data and
15
1
1
1
5
51
1
11
11
5
5
5
5
5
5
1
2
5
5
5
5
5
5
5
52
1
1
2
1
55
1
1
1
551
55
5 5
5
(a) Global location
7
7
7
3
7
7
7
77
1
77
77
1
1
1
1
7
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
11
1
1
1
11
11
1 1
1
(b) Distance to the image cen-
ter
1
1
1
1
1
1
11
1
11
11
9
1
1
1
1
1
9
9
9
1
9
1
1
1
1
19
1
1
9
1
11
9
9
1
191
99
9 1
9
(c) None
Fig. 6. The most possible part assignments from the models with and
without considering the global relationships of features.
Fig. 7. The flowchart of the body-language recognition system. The NTU-
PAL2 robot greets, head to or leave a user according to his/her hand postures.
central region. Without taking the global relationships into
account, parts could be distributed over the whole image as
shown in Figure 6(c) .
IV. EXPERIMENTS AND DISCUSSION
A. System Overview and Dataset
The body-language recognition system is developed for
the NTU-PAL2 robot to perform human robot interaction.
The NTU-PAL2 robot greets, heads to or leaves a user
according to his/her hand postures. Figure 7 is the flowchart
of our body-language recognition system. A laptop computer
with 1.66G CPU and 1.5G RAM is used in our system.
The VIDERE Design STOC stereo camera with an image
resolution of 640x480 is used to collect visual images. The
average executing time of the face detector for each frame
Fig. 8. Four sample images of the hand posture dataset. The corresponding
trees of SIFT features for HCRFs are shown.
is 230 ms, the raising-hand detector is 140 ms, and the hand
posture recognizer is 385 ms.
Figure 8 shows four sample images of our upright hand
posture dataset and the corresponding trees of SIFT features.
There are four classes: Background, Fist, Palm and Six.
For each class, there are 200 images for training and 100
images for testing. For each hand posture, the variance of
lighting condition and deformation are collected as well as
the slight variance of 3D rotation. The 90◦ cases are obtained
by flipping the upright cases. The size of each image is
around 100×100 pixels.
B. Single-class Recognition
The performances of HCRF-based recognition with differ-
ent settings are shown in Table I. The UIUC car side dataset
and our hand posture dataset were used for evaluation. All
the test images were also in-plane rotated 90◦ to test if
the approaches are in-plane rotation-invariant. The algorithm
none indicates that no global relationship of features is used.
The algorithm global indicates that the global locations of
features are used, and distance indicates that the proposed
distances to the image center of features are used.
It is shown that the accuracy of our feature form of HCRFs
has no obvious decrease in the 90◦-rotated cases. thus,
our proposal is invariant to in-plane rotation. On the other
hand, models considering no global relationship are also
invariant to in-plane rotation. However, the proposed global
relationships can enhance the accuracy to the maximum of
4.5% in the class of Six. In terms of the upright cases, a
model considering global location is better than a model
considering global distance by 1.375% in average. That is,
our feature form of HCRFs obtains invariance of in-plane
rotation with a quite small cost of accuracy.
It should be noted that the accuracy of the algorithm
Location for the classes of Fist and Palm decrease much less
than the classes of Car Side and Six in the 90◦-rotated cases.
In addition, the algorithms Location and Distance make less
enhancements in accuracy in the classes of Fist and Palm
than Car Side and Six. A possible explanation could be that
the contribution of the global relationship information is in
proportion to its distribution to the rotated case.
RANSAC Matching: Simultaneous Registration and Segmentation
Shao-Wen Yang, Chieh-Chih Wang and Chun-Hua Chang
Abstract— The iterative closest points (ICP) algorithm is
widely used for ego-motion estimation in robotics, but subject
to bias in the presence of outliers. We propose a random
sample consensus (RANSAC) based algorithm to simultaneously
achieving robust and realtime ego-motion estimation, and multi-
scale segmentation in environments with rapid changes. Instead
of directly sampling on measurements, RANSAC matching
investigates initial estimates at the object level of abstraction
for systematic sampling and computational efficiency. A soft
segmentation method using a multi-scale representation is
exploited to eliminate segmentation errors. By explicitly taking
into account the various noise sources degrading the effective-
ness of geometric alignment: sensor noise, dynamic objects and
data association uncertainty, the uncertainty of a relative pose
estimate is calculated under a theoretical investigation of scor-
ing in the RANSAC paradigm. The improved segmentation can
also be used as the basis for higher level scene understanding.
The effectiveness of our approach is demonstrated qualitatively
and quantitatively through extensive experiments using real
data.
I. INTRODUCTION
Ego-motion estimation in dynamic environments is one of
the most fundamental problems in mobile robotics, which is
the problem of determining the pose of a robot relative to
its previous location. It is not easily achievable as there are
two motions involved: the motions of moving objects and the
motion of the robot itself. A large body of work in computer
vision over the last decade has been concerned with the
extraction of ego-motion information from image sequence
[1–3]. The performance of ego-motion estimation depends
on the consistency between observations at successive time
steps, and can be degraded in the presence of outliers. The
motivation of this work is intended to provide a robust
realtime solution to the problem of relative pose estimation
in non-rigid scenes.
The iterative closest points (ICP) algorithm [4, 5], which
is based on least squares minimization, has been widely
used for aligning range images. However, conventional least
squares approaches are subject to bias in the presence of
This work was supported in part by the Taiwan National Science Council
under Grant 96-2628-E-002-251-MY3, Grant 96-2218-E-002-035, Grant 97-
2218-E-002-017, and Grant 98-2218-E-002-006, in part by the Excellent
Research Projects of the National Taiwan University, in part by Taiwan
Micro-Star International, in part by Compal Communications, Inc., and in
part by Intel.
Shao-Wen Yang is with the Department of Computer Science and Infor-
mation Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt
Road, Taipei, 10617 Taiwan any@pal.csie.ntu.edu.tw
Chieh-Chih Wang is with Faculty of the Department of Computer Science
and Information Engineering, and the Graduate Institute of Networking and
Multimedia, National Taiwan University bobwang@ntu.edu.tw
Chun-Hua Chang is with the Department of Computer Science and Infor-
mation Engineering, National Taiwan University, No. 1, Sec. 4, Roosevelt
Road, Taipei, 10617 Taiwan alan@pal.csie.ntu.edu.tw
outliers. Since the introduction of the ICP algorithm, many
variants have been proposed upon the basic ICP concept.
Rusinkiewicz and Levoy [6] proposed a combination of ICP
variants optimized for high speed with a point-to-plane error
metric. Pfister et al. [7] introduced a weighted matching
algorithm to estimate the transformation by matching succes-
sive range scans. Pfister and Burdick [8] described a multi-
scale point and line-based representation of range scans to
improve efficiency of scan matching. Minquez et al. [9] used
a new metric distance in the robot’s configuration space.
Bosse and Zlot [10] presented an iterative scan matching
technique using an extended Kalman filter maintaining all
the vehicle poses. More recently, we proposed a RANSAC-
based segment matching approach [11] to estimate a robot’s
ego-motion in dynamic environments. However, conventional
approaches cannot deal with dynamic objects. Segment-
based approaches are subject to imperfect segmentation.
As the key development in robotics has been the adop-
tion of probabilistic approaches, many recent state-of-the-art
robotic systems employ probabilistic techniques for robotic
perception. However, in developing a practical relative pose
estimation algorithm, it can be difficult to quantify the effec-
tiveness of a relative pose estimate. Gutmann and Schlegel
[12] described a comparison on the covariances from several
scan matching approaches in indoor environments. These
approaches work fine in office-like environments, particularly
with orthogonal or rectilinear walls, but are infeasible for
unstructured environments. Lu and Milios [13] have shown
how a covariance matrix for the ICP algorithm can be
estimated directly from the corresponding pair of points.
Unfortunately, the uncertainty estimates are too conservative
and do not correspond to reality. Bengtsson and Baerveldt
[14] presented to calculate the covariance matrix by esti-
mating the Hessian matrix of the error function minimized
by the iterative dual correspondence (IDC) algorithm. The
approximation mandates predefined constant offsets in trans-
lation and rotation for evaluating the minimization error.
Wang and Thorpe [15] proposed a hierarchical object based
representation for simultaneous localization and mapping
(SLAM) which estimates the uncertainty using a sampling-
based approach. The registration process is performed repet-
itively with random initial estimates. Censi [16] proposed a
Laplace approximation to calculate the covariance matrix for
scan matching algorithms.
In the computer vision literature, random sample consen-
sus (RANSAC) [17] is one of the most effective algorithms
for model fitting to data containing a significant percentage of
gross errors. It is an iterative method to estimate parameters
of a mathematical model from a set of observed data which
B. Matching
In the classical RANSAC paradigm, letting o be a feature
and h be some hypothesis, the effectiveness of each (o, h) is
examined and represented using a binary score. Specifically,
if (o, h) is an inlier pair, the score sh of the hypothesis
h is incremented. As segments might be of significantly
different sizes, a binary score is insufficient to describe the
quantity of an association between two segments. Let z be
an observation and zi be the i-th segment in z. Comparing to
the classical RANSAC process, the score sih of each segment
zi is supported on N and the effectiveness of the pair (zi, h)
is represented by a natural number.
1) Sampling: To build consensus sets, z is segmented
and represented as a collection of segments z = ∪iz
i.
First, segments are randomly selected with probabilities
proportional to their sizes. A hypothesis h is generated by
matching the selected n segments with the reference model
z¯.
2) Scoring: To obtain the score sih of a segment z
i, the
effectiveness of (zi, h) is examined by checking if (y, h) is
an inlier pair for all y ∈ zi. The score sih of a segment
zi is defined as the number of measurements y ∈ zi
which are located within neighborhoods of measurements
in the reference model z¯. Here, (y, h) is judged as an inlier
pair if and only if the measurement y transformed to the
global coordinate by the hypothesis h is located within a
neighborhood d of some measurement in the reference model
z¯. Specifically, the score sih is incremented if the pair (y, h)
is judged as an inlier pair. Therefore, we have
sh =
∑
{i|zi⊆z}
sih (5)
=
∑
zi⊆z
∑
y∈zi
1h(y) (6)
where 1h(y) is an indicator function indicating whether
or not (y, h) is an inlier pair. When the process finishes,
the hypothesis with the highest score is output as the best
transformation ψ. In this work, d is 0.3 meter.
The parameter n should be carefully determined and take
into account the tradeoff between efficiency and reliability,
and the characteristics of the data. For matching segments
with the reference model, one segment is usually sufficient
to preserve the shape information of the environment unless
an environment is composed of line segments which result
in ambiguity. It is clear that the higher the value n, the
higher the probability at least one hypotheses is an inlier, and
thus the reliability increases. Letting n = 2 and w = 0.5,
according to Equation 4, to obtain a 99% assurance of
making at least one error-free selection, the number k of
selections must be greater than or equal to 17, which is
computationally sufficient for realtime applications.
However, the present segmentation approach can fail as
the characteristics of an environment are subject to change
over time. Objects may be mis-merged with a high threshold,
while using a too low threshold results in over-segmentation.
Figure 1 gives an example illustrating the segmentation issue
and Figure 2(c) shows the visual image. On the top of these
figures, a static object and a moving object are very close to
each other. In Figures 1(b), 1(c) and 1(d), the two objects are
mis-segmented and merged together as they are not spaced
far enough apart from each other, whereas, in Figures 1(e),
1(f) and 1(g), the two objects are properly segmented but
most of the objects are split into fragmented segments.
IV. MULTI-SCALE RANSAC MATCHING
In this section, we describe the proposed RANSAC match-
ing algorithm in which segmentation issues are resolved
using a multi-scale representation. As segmentation is used
as a preprocessing step in segment-based approaches, the
segmentation errors introduced by the hard decisions bring
difficulties to the segment matching algorithm. A scale tree
representing objects of varying sizes in a range image is
proposed to eliminate the segmentation errors.
A. Multi-scale Representation
We define a scale tree comprising collections of segments
extracted from a range image at multiple scales. Edges inher-
iting parent-child relationships on the tree are established for
segments at different scales for which the child segment at
a finer scale is subsumed in the parent segment at a coarser
scale. A scale tree is constructed in a top-down manner. The
process is started by extracting segments at the coarsest scale
and then iteratively extracting finer segments by splitting
from the coarser segments. Let zj,i denote the i-th segment
extracted at the j-th scale. The vertex set of a scale tree can
then be defined as ∪j ∪i z
j,i. Figure 1 illustrates an example
of the multi-scale representation of a range image. In this
work, the segmentation thresholds are 12, 9, 6, 3, 1.5, and
0.75 meter respectively.
B. Multi-scale Matching
To avoid under-segmentation and over-segmentation, we
take advantage of the multi-scale representation wherein an
observation z is represented as collections of segments ex-
tracted at multiple scales. Instead of making hard decisions in
segmentation, we propose a soft segmentation method using
the multi-scale representation to simultaneously deal with
data association uncertainty and segmentation uncertainty.
All segments in a scale tree are taken into account in the
sampling stage of the RANSAC process. Comparing to the
segment matching approach, the consensus set is generated
from randomly selected n segments from all segments within
the scale tree.
Constructed in this manner, the uncertainty in segmenta-
tion is modeled directly in the process of building consensus
sets. It is assumed that the probability of a measurement be-
longing to a segment at some scale is uniformly distributed,
which can be expressed as
p(zj,i|y) ∼ U(ℓ), ∀zj,i ∋ y (7)
where U(ℓ) denotes the uniform distribution on Nℓ
1
, and ℓ is
the number of scales used for constructing the scale tree.
−40 −30 −20 −10 0 10 20 30 40
0
5
10
15
20
25
30
35
40
m
e
te
r
reference
observation
(a) ICP
−40 −30 −20 −10 0 10 20 30 40
0
5
10
15
20
25
30
35
40
45
m
e
te
r
reference
observation
(b) Multi-scale RANSAC matching
(c) Visual Image
−40 −30 −20 −10 0 10 20 30 40
0
5
10
15
20
25
30
35
40
m
e
te
r
reference
observation
(d) ICP
−40 −30 −20 −10 0 10 20 30 40
0
5
10
15
20
25
30
35
40
45
m
e
te
r
reference
observation
(e) Multi-scale RANSAC matching
(f) Visual Image
Fig. 2. Multi-scale RANSAC matching. The ICP results are shown in (a) and (d) while the corresponding RANSAC matching results are shown in (b)
and (e). Visual images for these two examples are depicted in (c) and (f), respectively. In (a), (b), (d) and (e), the observation is shown in red dots and the
reference model is shown in gray dots. In (b) and (e), the segments are shown in blue rectangles, and the uncertainty ellipse at around the origin is shown
in cyan. Regarding the segmentation results only the root vertices of scale trees are depicted for clarity.
occluded. Thus, if ω(zj,i) is less than some proportion φ, the
vertex corresponding to ω(zj,i) and its descendants are split
from the scale tree and forms a new tree. The split operation
is performed from the leaf vertices of a scale tree, and cuts
all edges connecting to the split vertex from the scale tree.
In our implementation, the value of φ is 50%, which is the
only parameter has to be chosen in this work, in addition to
RANSAC parameters.
Figure 2 demonstrates the effectiveness of the proposed
RANSAC matching algorithm and the multi-scale segmen-
tation. Figures 2(a) and 2(d) show the ICP results, Fig-
ures 2(b) and 2(e) depict the RANSAC matching results,
and Figures 2(c) and 2(c) present the corresponding visual
images. The most pressing issue of the conventional scan
matching algorithms is that they do not explicitly cope
with outliers. It can be seen that the RANSAC matching
algorithm outperforms the ICP algorithm. The ICP algorithm
can possibly converge to local minima in the presence of
moving objects, particularly in dynamic scenes. This is
typical for least squares approaches in which the quadratic
penalty allows a outlier far apart from the true solution to
bias the final result [17]. Specifically, the ICP algorithm uses
the whole observation to obtain a solution of the relative
pose, whereas the RANSAC matching algorithm samples
as few static objects within a range image as feasible, and
evaluates hypotheses by checking the consistency between
the sampled objects and the reference model. The multi-scale
segmentation can also supply a significant preprocessing step
for a variety of robotics applications, and be used as the basis
for higher level scene understanding.
V. UNCERTAINTY ESTIMATION
Covariance estimation is necessary to quantify the uncer-
tainty of a relative pose estimate. Most probabilistic pro-
cesses, such as extended Kalman filters (EKFs) and particle
filters (PFs), should be aware of the level of confidence in
the state estimates. A realistic covariance estimate is also
necessary for further combining the relative pose estimates
with additional odometric or inertial measurements [23]. For
example, in a Kalman filter framework, the contribution of
measurements from different sensors to the state estimate is
weighted by the Kalman gains whose values depend on the
covariances of all the sources of information contributing to
the filter.
0 64 128 256 512 1024
1
1.5
m
e
te
r
(a) Uncertainty in forward direction
0 64 128 256 512 1024
−0.5
0
0.5
m
e
te
r
(b) Uncertainty in sideward direction
0 64 128 256 512 1024
0
0.5
d
e
g
re
e
(c) Uncertainty in rotation
0 64 128 256 512 1024
−0.5
0
0.5
1
1.5
m
e
te
r
(d) Uncertainty in forward direction
0 64 128 256 512 1024
−1
−0.5
0
0.5
1
m
e
te
r
(e) Uncertainty in sideward direction
0 64 128 256 512 1024
−50
0
50
d
e
g
re
e
(f) Uncertainty in rotation
Fig. 4. Effectiveness and convergency. The x-axis denotes the number of samples used in the RANSAC process. The red line and the blue dashed line
show the mean and the 99% confidence bound, respectively, of a relative pose estimate. The green line shows the ground truth obtained from the inertial
measurement system. The uncertainty estimate of the example given in Figure 2(b) is shown in (a), (b) and (c), and the uncertainty estimate of the example
given in Figure 2(e) is shown in (d), (e) and (f). Note that the vertical scales of (c) and (f) are considerably different.
In both of the examples, there are about 100 segments at
multiple scales in total. Enumerating all combinations of
segments at multiple scales is computationally infeasible
for realtime applications. It can be seen that the significant
property of RANSAC matching is the trade-off between
increased representation power and computational overhead.
Initially, the accuracy of the method increases with the
number of samples. As increased number of samples, the
estimate tends to converge to a specific steady state within
a neighborhood of the ground truth. The property ensures
the convergency and efficiency of our algorithm. Figure 5
shows an empirical analysis of accuracy and convergency for
RANSAC matching. We assumed that an estimate is accurate
if it differs by at most 0.1 meter in forward and sideward
directions, and 1 degree in rotation from the ground truth, and
an uncertainty estimate is converged if it differs by at most
0.1 meter in forward and sideward directions, and 1 degree
in rotation from the estimate obtained from using 1024
samples. The ample experimental results in consequence
are consistent with the theoretical derivation of multi-scale
RANSAC matching given in Equation 9. Along with the
processing rate, the feasibility of RANSAC matching for
realtime estimation is also confirmed.
In summary, the experiments indicate that our algorithm
provides robust relative pose estimates in terms of ego-
motion estimation and uncertainty estimation. In comparison
with the alternative approaches [13–15], our algorithm yields
more accurate relative pose estimates and also provides more
consistent uncertainty estimates.
VII. CONCLUSION AND FUTURE WORK
We introduced a novel approach to solve the problem of
registration and segmentation of range images. RANSAC
matching solves the problem at the object level of abstraction
64 128 256 512 1024
0.6
0.7
0.8
0.9
1
ra
te
number of samples
Accuracy
Convergency
Fig. 5. Success rate. The x-axis denotes the number of samples used in
the RANSAC process.
by exploiting a multi-scale representation to model objects
of varying sizes. The main contribution of this paper is
to propose a robust realtime algorithm which takes into
account data association uncertainty and segmentation un-
certainty simultaneously in the RANSAC paradigm. Instead
of applying random initial estimates, the RANSAC process
can investigate initial estimates systematically and score
hypotheses statistically. By representing a range image as
segments at multiple scales, our approach overcomes a range
of limitations possessed by least squares approaches, such
as the inability to consistently estimate the uncertainty of
a relative pose estimate, and poor degradation to outliers.
RANSAC matching does not employ any geometric features
which are often environment dependent. It also inherits the
computational efficiency and probabilistic robustness from
the RANSAC paradigm. The feasibility and effectiveness of
the proposed approach have been demonstrated using real
data collected in urban environments without incorporating
odometry. Experimental results show that our approach out-
performs alternative approaches.
In the future, we plan to integrate spatial and temporal
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
參加國際機器人足球賽(RoboCup Standard Platform League)，打進前 8強。
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100字為限） 
與本計畫相關之論文著作有：使用多模型一致性隨機取樣之移動物體追蹤、多規模多模型
一致性隨機取樣之同時資料疊合與切割、使用單一相機的同時定位、建地圖、與移動物體追
蹤、使用雙眼立體視覺相機的同時定位、建地圖、與移動物體追蹤、以及自我調整的接近式
圖表演算法。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
本計畫的目標包括了：一、增進 SLAMMOT 演算法強健性；二、增進 SLAMMOT 演算法一般
性；三、多機器人協作式高階感知。 
我們提出多模型的一致性隨機取樣法完成強健的自身運動估測與移動物體偵測，多規模多
模型的一致性隨機取樣法處理資料分割的不確定性，並發展總體一致的建地圖演算法，這
些方法有效的增加系統的強健性。在一般性的方面，提出使用單一相機的同時定位、建地
圖與移動物體追蹤演算法，接著提出使用雙眼立體視覺相機的同時定位、建地圖及移動物
體追蹤演算法，並於異質性平台（雙足機器人 Nao 以及三種不同的輪型機器人 MSN、PAL3、
CCI Robot）上驗證同時定位、建地圖與移動物體追蹤系統之有效性。基於強健性與一般
性的發展我們也使用多隻 Nao 機器人，發展多機器人協作式高階感知系統。最後，我們也
說明了基於同時定位、建地圖與移動物體追蹤系統所發展之動態環境避障與路徑規劃系
統。 
景物認知是行動型機器人的根基，唯有使得機器人能夠了解環境，才能使得機器人是真正
自主的。在真實環境中，移動物體的存在使得景物認知是更加困難的，我們積極發展機器
人同時定位、建地圖及移動物體追蹤的技術，期望能夠做為行動型機器人的理論與應用的
基礎，進而發展更高階的應用，例如機器人保全、服務型機器人、導覽機器人等。 
