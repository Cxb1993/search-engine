膝蓋損傷之關節鏡與磁振影像融合分析與診斷 
計畫編號：NSC-94-2213-E-006-119 
執行期限：2005 年 8 月 1 日至 2006 年 7 月 31 日 
主持人：孫永年   國立成功大學 資訊工程學系 
共同主持人：林啟禎   國立成功大學 醫學系  
 
 
摘要 
我們研究膝關節鏡影像的形變校正與病灶外形
估測，以及磁振造影的對位與膝關節模型重建。
我們對廣角鏡頭的筒狀失真與透視投影的成像
形變進行關節鏡影像校正，以最接近原始病灶的
方式呈現病灶；接著，我們直接以校正後的影
像，根據影像明暗變化進行外型重建，並對膝磁
振造影進行對位、分割與三維重建，以輔助外科
醫師研判病灶，進行合適之診斷與治療。 
關鍵詞：關節鏡、磁振造影、膝關節、三維重建、
形變校正、耀光補償 
 
Abstract 
In this study, we correct arthroscopic image 
distortion, estimate shape of the target from an 
arthroscopic image, and register and reconstruct 3D 
knee joint from MR images to assist surgical 
evaluation. We correct barrel distortion and 
perspective distortion in arthroscopic images. In 
addition, glare components are frequently seen in 
arthroscopic images and afflict the shape estimation 
of the target. We solve this problem by automatic 
localization of the glare components and 
compensation based on pixel intensity of 
component’s neighborhood. Then, we present an 
improved shape-from-shading algorithm for 
estimating three-dimensional surface of the target. 
Moreover, we register MR images, segment the 
knee joint, and reconstruct 3D joint model to help 
navigating the inner structures for surgical guidance 
and evaluation. 
Keywords: arthroscopy, magnetic resonance 
imaging, knee joint, distortion correction, glare 
compensation 
 
I. Introduction 
 
Because of minimal incision and shorter recovery 
time than open surgery, endoscopic surgery has 
replaced many kinds of open surgery and has been 
popular in hospitals. Endoscopes are helpful to 
observe and diagnose inner structures and organs. 
However, some disadvantages still exist, such as the 
loss of stereo visual perception. The distance 
between organs and instrument is usually difficult 
to be judged from the endoscopic views and mostly 
rely on the surgeon’s experience. Sometimes 
surgeons move the endoscope to have better 
perception from the changes appeared in video. 
This approach works for targets whose nearby 
space is sufficiently large, such as the abdomen. In 
arthroscopy, the space around the joint is much 
smaller than that in the abdomen. In addition, 
endoscopic image distortion makes the depth 
estimation more difficult. MR images can provide 
additional information to partially alleviate the 
disadvantages. The shape from arthroscopic images 
is also useful to correct pre-operative MR data. 
Thus, this study is motivated to solve the image 
distortion problem, to develop three-dimensional 
(3D) surface estimation methods for arthroscopic 
images and MR data to assist arthroscopic surgery.  
Most of the study regarding endoscopic image 
processing focuses on endoscopic calibration [6], 
shape recovery of internal structures [1], or the 
development of virtual endoscopy systems [2]. 
Only a few of them consider the problem of glare 
component [1, 3]. For 3D surface estimation, many 
methods have been developed in the field of 
computer vision, such as shape-from-motion and 
shape-from-shading methods. In 
shape-from-motion approaches, the factorization 
methods [4] use image correspondences across 
images and rank constraints to recover 3D shape. In 
shape-from-shading, some researchers have applied 
this approach to endoscopic images [5]. In the 
reconstruction and processing on MR data, Chen et 
al. [13] reconstructs the spinal tissue by using the 
differences between CT and MR images. Zhao et al. 
[14] used an elastic 3D model to segment the femur. 
Laurendeau [15][16] used contour-based methods 
to build a 3D shoulder model from MR imags.  
In this study, we deal with the correction of 
barrel and perspective distortion. We propose a 
method to handle the glare components in 
arthroscopic images, and the object surface is 
estimated by an improved shape-from-shading 
method. The 3D reconstruction from MR data is 
based on the integration and registration of data 
acquired from three directions, rather then 
conventional approach based on a single data. 
This report is organized as follows. In Section 
II, the MR image registration method is introduced. 
vertices connected to iV    
1
i
i
t t t
i j jt
j Nj
j N
V B C
B ∈
∈
= ∑∑  (5)
where Ni is the triangles connected with Vi, Cj is the 
gravity of the triangle, and Bj is the area for the 
triangle. The 3D knee joint model after inflation is 
illustrated as Fig. 6, and the 3D knee joint model 
overlapped onto its MR slice is displayed in Fig. 7. 
 
IV. Arthroscopic distortion correction 
 
Endoscopic image distortion includes barrel 
distortion and perspective distortion. Because 
endoscopes are usually equipped with wide-angle 
lenses, the endoscopic images usually suffer from 
barrel image distortion, which makes the object 
near the image border larger the one near the image 
center. Perspective distortion resulting from 
perspective projection of the endoscopic camera 
makes object size depend on the distance between 
the camera and the object. We correct barrel 
distortion using a grid pattern to estimate relevant 
distortion parameters by [2] because this approach 
is elegant and was found useful in our experiments. 
Because the correction of barrel distortion has been 
a standard step in many endoscopic image 
processing applications, we will not discuss it in 
more details.  
After barrel distortion correction, we use an 
inverse perspective mapping [5] to eliminate 
perspective distortion. To the best of our knowledge, 
we are the first group that deals with this problem 
in endoscopic images. To establish the mapping that 
maps the original image to a corrected image, we 
define two spaces: a 3-D world space W = (x, y, z) 
∈E3, and a 2-D image space I = (u, v) ∈E2. The 
image space is located at the plane z = 0 in W. Let c 
be the position of lens and located at the origin of W. 
As shown in Fig. 8, we define the viewing direction 
as the direction of optical axis oˆ , which depends 
on the angle γ  and θ . γ  is the angle between 
the x-axis and the projection of the optical axis oˆ  
onto the x-y plane and θ  is the angle between the 
optical axis oˆ  and x-y plane. The angular aperture 
of the lens is 2α. Lines ca and cb (or oa and ob) 
represent the boundary of field of view, and p is an 
arbitrary point in the range. Assume that p(x, y) is a 
point on the corrected image plane that corresponds 
to a position q(u, v) on the original image. If the 
ratio of lengths of curves pa and ap is known, we 
can correct the image. To obtain the mapping 
between the original and the corrected images, we 
calculate the angle θ ′  as  
1
2 2
tan ( )h
x y
θ −′ = +
 .     (6) 
Then, given a point (x, y) in the original image, the 
corresponding point u(x, y, 0) and v(x, y, 0) in the 
corrected image is determined by          
1
2 2
tan ( ) ( ))
( , ,0)
2
h H
x yv x y
θ α
α
− − −+=   (7) 
1(tan ( ) ( ))
( , ,0)
2
y W
xu x y
γ α
α
− − −
=    (8) 
where the values of α and θ  can be obtained from 
the manual of endoscope.  
 
V. Handling glare components 
 
In arthroscopic images, glare components, i.e. 
regions appear over brightness, can frequently be 
seen when the light source located on the tip of an 
endoscope is close to the object. The components 
make structures unrecognizable and need to be 
detected and compensated.   
To detect the glare components, we assume that 
the glare region has two properties: 1. the intensity 
in the region is homogeneous; 2. the average 
intensity in the region is higher than the 
surrounding areas. According to the assumptions, 
we combine the approaches proposed by Althouse 
et al. [17] and Hermes et al. [10], and design an 
entropy-based cost function to estimate the 
homogeneity of the region. Let the intensity of the 
image be collected in { : }I Iω ω= ∈Ω , where ω is a 
pixel, Ω is the set of all pixels in the image, 
{ : }I Iω µ µ∈ ∈Ψ  is the intensity value of I, Iµ  is 
an intensity in the image, and Ψ is an index set. A 
region is denoted as { : }a aλ λ= ∈Λ , where aλ is an 
element belonging to a, λ is the index of the region, 
and Λ is the collection of all indexes. We define a 
likelihood function 
( | ) ( | )ap I p I aω
ω∈Ω
=∏                (9)    
and compute the entropy of the region by 
log ( | ) ( , ) log ( | )IL p a n I a p I aλ µ λ µ λ
µ∈Ψ
≡ − = ∑  (10) 
where n(I , a) represents the number of pixels with 
intensity I in the region a. Expanding (10) yields  
( , ) log ( | )L p I a p I aλ µ λ µ λ
µ∈Ψ
∝ −∑ .         (11) 
If the region is homogeneous, the entropy of the 
region is low. For example, if pixels in the region 
have the same intensity, we have p(I |a ) = 1 and the 
entropy value Lλ = 0. With different values of 
intensity, the region tends to be inhomogeneous and 
the entropy value is increased as the number of 
To evaluate the proposed methods, three test images 
were used in our experiments. We corrected image 
distortion on the images. With the 
distortion-corrected images, the results on handling 
glare components and the shape estimation 
approach can be seen in Fig. 9. In this figure, the 
first column shows the original images of the test 
data. The second column demonstrates the 
estimated shapes before handing glare components. 
It is obvious that the top of the shape is flat because 
of the large contrast between the bright (white) area 
and surrounding area. After glare component 
detection and compensation, the resultant images 
and estimated shapes can be seen in the third and 
the fourth columns, respectively. It can be seen that 
the tops have been reasonably compensated and 
estimated.  
Next, we used a 3D scanner to obtain ground 
truth range data of the test data for the evaluation of 
the accuracy of the proposed method. The ground 
truth data are shown in Fig. 10(a), which provides 
visual comparisons with the estimated shape in Fig. 
2. The Euclidean distance between two surfaces 
was used as an error measure. The red points in Fig. 
10(b) represent that the error is larger than 0.1 cm.  
 
VII. Conclusion 
 
In this study, we correct endoscopic image 
distortion and recover geometric information of the 
imaged scene. We developed an entropy-based cost 
function to segment the glare components and 
compensate for the intensity with gradient of 
surrounding pixels. Finally, we present an improved 
shape-from-shading algorithm to reconstruct 3D 
surface from corrected images. Form the 
experimental results, the proposed methods can 
provide objective 3D data to assist endoscopic 
interventions. 
 
計畫成果自評 
此研究進行關節鏡影像校正、外型估測與病
灶的磁振影像三維場景重建，其中關節鏡影像的
透視形變校正為內視鏡影像新的校正方式，目前
相關文獻尚未發現研究人員針對這類形變提出
解決之道；本研究對病灶的耀光處理與病灶外型
的估測也提出新方法，解決傳統方法的不足與缺
陷。多磁振影像之對位、整合、膝關節影像分割
與三維重建之實驗結果亦顯示其可行性。研究內
容符合原計劃內容，亦達成預期目標。本計劃之
初步成果將於今年十二月發表於 International 
Symposium on Biomedical Engineering，且部
份成果已被接受刊登在國際期刊 IEEE Trans. on 
Biomedical Engineering。我們將持續進行相關
實驗與研究內容的整理，預計在短期內將後續成
果投稿至世界一流期刊。 
 
References 
 
[1] C. H. Q. Forster and C. L. Tozzi, "Towards 3D 
reconstruction of endoscope images using shape from 
shading," in Proc. 13th Brazilian Symposium on 
Computer Graphics and Image Processing, pp. 9096, 
2000.  
[2] R. Shahidi, M. R. Bax, C. R. M. Jr., J. A. Johnson, E. 
P. Wilkinson, B. Wang, J. B. West, M. J. Citardi, K. H. 
Manwaring, and R. Khadem, "Implementation, 
calibration and accuracy testing of an image-enhanced 
endoscopy system," IEEE Trans. Med. Imaging, vol. 21, 
no. 11, pp. 1524-1535, 2002.  
[3] P. Tan, S. Lin, L. Quan, and H.-Y. Shum, "Highlight 
removal by illumination constrained inpainting," in Proc. 
IEEE ICCV, pp. 164-169, 2003.  
[4] C. J. Poelman and T. Kanade, "A paraperspective 
factorization method for shape and motion recovery," 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 3, pp. 
206-218, 1997.  
[5] P. Tsai and M. Shah, "Shape from shading using 
linear-approximation," Image and Vision Computing, vol. 
12, pp. 487-498, 1994.  
[6] J. P. Helferty, C. Zhang, G. McLennan, and W. E. 
Higgins, “Videoendoscopic distortion correction and its 
application to virtual guidance of endoscopy," IEEE 
Trans. Med. Imaging, vol. 20, no. 7, pp. 605-617, 2001. 
[7] S. M. Yamany, A. A. Farag, D. Tasman, and A. G. 
Farman, "A 3D-reconstruction system for the human jaw 
using a sequence of optical images.," IEEE Trans. Med. 
Imaging, vol. 19, no. 5, pp. 538-547, 2000.  
[8] M. T. Ahmed, S. M. Yamany, E. E. Hemayed, S. 
Ahmed, S. Roberts, and A. A. Farag, "3D reconstruction 
of the human jaw from a sequence of images.," in Proc. 
IEEE CVPR, pp. 646-653, 1997. 
[9] M. Bertozzi and A. Broggi, "GOLD: a parallel 
real-time stereo vision system for generic obstacle and 
lane detection," IEEE Trans. Image Processing. vol. 7, 
no.11, pp. 62-81, 1997. 
[10] L. Hermes and J. Buhmann, "A minimum entropy 
approach to adaptive image polygonization," IEEE Trans. 
Image Processing, vol. 12, no. 10, pp. 1243-1258, 2003. 
[11] N. Otsu, “A threshold selection method from 
gray-level histogram,” IEEE Transaction on System, 
Man, and Cybernetics, Vol. 9, No. 1, pp. 62-66, January 
1979. 
[12] W.E. Lorensen, and H.E. Cline, “Marching Cubes: a 
high resolution 3D surface reconstruction algorithm,” 
Computer Graphics (Proc. of SIGGRAPH), Vol. 21, No. 
4, pp. 163-169, 1987. 
[13] Y. T. Chen and  M. S. Wang, “Three-Dimensional 
reconstruction and fusion For multi-modality spinal 
images,” Computerized Medical Imaging and Graphics, 
vol. 28, no. 1-2, pp. 21–31, Jan. 2004. 
[14] X. Zhao, F. Qi, H. Huang, and J. Zhan, “3-D 
deformable models for femur shape recovery from 
volumetric images,” in Proc. The Fourth International 
Conference on High Performance Computing in the 
Asia-Pacific Region, vol. 2, pp. 797-798, May 2000 
[15] A. B. Albu, D. Laurendeau, L. J. Hebert, H. Moffet, 
 
Fig. 1. Flowchart for registration of MR images. 
 
 
 
     
(a)                              (b) 
Fig. 2. First stage of registration on MR images. (a) Before and (b) after registration. 
 
 
 
 
Fig. 3. Results with the second stage of registration on MR data. 
 
 
 
 
 
 
 
 
    
(a) 
 
    
(b) 
Fig. 7. Overlapping of the reconstructed model onto the original MR images. (a) Original images; (b) 
overlapped images. 
 
 
 
 
Fig. 8. Correction of perspective distortion. 
 
 
 
 
 
 
 
 
 
