行政院國家科學委員會補助專題研究計畫
■成 果 報 告
□期中進度報告
蛋白質結構之全原子座標預測
Prediction of All-atom Coordinates of Protein Structures
計畫類別：■ 個別型計畫 □ 整合型計畫
計畫編號：NSC－98－2221－E－110－062
執行期間：98年 8月 1日 至 99年 10月 31日
計畫主持人：楊昌彪 國立中山大學資訊工程學系
計畫參與人員： 黃國璽 國立中山大學資訊工程學系
安興彥 國立中山大學資訊工程學系
何秋誼 國立中山大學資訊工程學系
曾球庭 國立中山大學資訊工程學系
陳紘昕 國立中山大學資訊工程學系
顏欣偉 國立中山大學資訊工程學系
林志穎 國立中山大學資訊工程學系
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告
本成果報告包括以下應繳交之附件：
□赴國外出差或研習心得報告一份
□赴大陸地區出差或研習心得報告一份
■出席國際學術會議心得報告及發表之論文各一份
□國際合作研究計畫國外研究報告書一份
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢
執行單位：國立中山大學
中 華 民 國 1 0 0 年 1 月 1 7 日
21 前言與研究目的
在生命體中，蛋白質是細胞內最重要、特殊的大分子，所有的抗體、酵素和荷爾蒙幾
乎都是蛋白質，而結締組織、肌肉纖維等基本上也都是由蛋白質所構成。由於各種蛋白質
展現出特有的性質並發揮功能，生物體內的各種活動才有可能發生。大部分之蛋白質是由
二十種標準胺基酸組成的序列，我們稱這些胺基酸序列為蛋白質一級結構。二級結構為一
級結構上之螺旋(-Helix)與摺頁(-Sheet)等由氫鍵所造成的結構，蛋白質二級結構目錄
DSSP (Dictionary of Protein Secondary Structure) [Kabs83]將二級結構分為八種型態，而蛋白
質之二級結構會影響三級結構之摺疊方式。三級結構是指單一蛋白質之整體形狀或摺疊方
式，而四級結構用來描述由多個蛋白質所組成之蛋白質複合體之排列組合狀況。
生物學家相信，蛋白質的三級結構與功能息息相關，兩個蛋白質的結構愈相似，它們
的功能也愈類似。蛋白質為了發揮它的本身功能，它們通常必須具有特定的形狀，亦稱摺
疊(fold)。當蛋白質摺疊發生錯誤，可能導致嚴重的後果。目前已知有二十種以上的疾病是
因蛋白質錯誤摺疊而形成，包括許多知名的疾病，例如，阿茲海默症(老人癡呆症，Dementia
of the Alzheimer's Type (DAT))、狂牛病(牛海綿狀腦病，Bovine Spongiform Encephalopathy
(BSE))、人類庫賈氏病(人類海綿狀腦病，Creutzfeldt-Jakob Disease (CJD))、漸凍人(肌萎縮
性脊髓側索硬化症，Amyotrophic lateral sclerosis (ALS))、帕金森氏症(Parkinson's Disease)
及許多癌症(Cancer)及其併發症等。
欲解析蛋白質的三級結構，可以透過實驗完成，如 X-光晶體繞射(X-ray Crystallography)
和核磁共振光譜法(NMR, Nuclear Magnetic Resonance Spectroscopy)，但這是耗時且花錢的
工作。為協助生物醫學專家有效縮小研究範圍以提昇研究效率，本計畫運用資訊學上之預
測與分類方法，並提出合適的演算機制，以提昇蛋白質分類與結構預設之準確性。本計畫
提出三項研究成果，並已分別於國際與國內研討會發表[Yen10, Lin10, Hor10]，茲簡要說明
如下。
1. 利用 SVM 改進蛋白質全原子結構預測之方法：以支持向量機(support vector
machine)做為蛋白質全原子結構預測軟體之選擇器，並發展三階段特徵組合搜尋
法，以建構出一準確率更高的結構預測方法。 [Yen10]。
2. 基於蛋白質種類狀態預測之雙硫鍵鍵結狀態之預測：在雙硫鍵鍵結狀態之預測
上提出了一個新的演算法，這個演算法藉由先對蛋白質做是否包含雙硫鍵判
斷，再分別對不同種類的蛋白質進行是否有鍵結的預測，以提高預測準確度。
[Lin10]。
3. 以支持向量機作為 RNA二級結構預測工具選擇之方法：嘗試以支持向量機做為
RNA 二級結構預測軟體選擇器，並利用資訊理論基礎發展出漸進式的特徵選擇
方法，以建構出一更準確的核醣核酸二級結構整合性預測方法。[Hor10]。
4圖 1 為利用 SVM 提升蛋白質全原子結構預測之方法流程圖。首先，把數條已知骨幹
座標之蛋白質序列的重要特徵萃取出來，並分別以 SABBAC以及 Chang的方法進行預測。
把預測結果和真實骨幹座標比較，決定二者何者較優，並賦予該序列較優方法為類別名稱
(例如 SA或 CH)。關於蛋白質序列特徵方面，我們先依氨基酸七大物理化學特性分群，包
含氨基酸組成比例(Composition)、疏水性(Hydrophobicibility)、凡得瓦力(Normalized van der
Walls volume)、帶電極性(Polarity)、可極化性(Polarizability)、尺寸大小(size)、電荷(Charge)
及芳香性(Aliphaticity or Aromaticity)等。然後利用此分群資訊計算出共計 50個蛋白質特徵
組，分群方法與特徵組(子集合)如表 1所示。
表 1 蛋白質特徵
(a)依氨基酸特性分群 (b)蛋白質特徵組
為求有較佳的 SVM分類效果，必須找出一較佳的特徵組合，然由於有相當多的特徵組
合須進行嘗試。因此，本研究乃採三階段的特徵組合選擇方法，方法如圖 1所示。在第一
階段中，我們先就表 1(b)所示的 22個特徵組中找出分類效果最佳的數個特徵組，我們稱為
Winner feature sets，而剩下未被選出的特徵組則稱為 Loser feature sets。在第二階段中，則
就Winner feature sets所選出的數個特徵組，用所有的特徵組合進行分類正確率之測試。測
試完畢後，保留分類正確率超過一定程度之特徵組合所訓練出的分類器，並依照分類器選
的軟體對蛋白質進行座標預測，然後和真實座標比對，求取均方根誤差 ( Root mean square
deviation, RMSD)。在第二階段進行完畢後，僅保留最小 RMSD 所對應之特徵組合。在第
三階段中，把在第二階段所得特徵組合與第一階段之 Loser feature sets以進行漸進式(一次
加一個)組合，加入第二階段所得特徵組合中，以找出最佳 RMSD對應之特徵組合。經過此
三階段的特徵組合搜尋所創建的 SVM軟體選擇器，並以 Leave one out方式交叉驗證，其
全原子結構預測之 RMSD可降低至 0.405。在表 2裡，我們把 SVM軟體選擇器對 CASP 7
蛋白質序列之預測結果，以及 SABBAC與 Chang的方法預測結果進行比較，我們的方法分
別有 9.19%與 5.37%的進步。
表 2 不同 SVM選擇器之二級結構預測準確率比較
6在經過第一階段的蛋白質種類預測之後，我們根據不同的蛋白質種類有不同的對應方
法。對於半胱氨酸全部為還原狀態的蛋白質我們不對它作任何調整，半胱氨酸全部為氧化
狀態的蛋白質，我們採用 Chung等人[Chung09] 所提出的 Simple tune演算法對結果進行調
整，這項調整方法可以過濾掉一些不合理的鍵結狀態並且確保氧化的半胱氨酸是偶數的，
以提高預測準確度。而半胱氨酸為部分氧化部分還原狀態的蛋白質會先經過混合分類器
(Mix classification)預測每個半胱氨酸的氧化、還原狀態，最後再做 Simple tune的調整。圖 3
所示為整個演算法的流程圖。
圖 3演算法的流程圖
表 3中列出我們的方法與目前文獻上最佳的實驗結果，以我們所提出的演算法獲致的
準確度比起先前的研究都要來的高。其中 Multi-phase Approach 是先前最好的研究實驗結
果，而我們的結果在半胱氨酸狀態的準確度(QC)上提升了 3.5%，並且在整條蛋白質的半胱
氨酸狀態準確度(QP)上提升了 5.8%。
表 3 雙硫鍵鍵結狀態預測比較表
Oxidized Reduced
Method QC MCC
SpeciﬁcitySensitivity Speciﬁcity Sensitivity
QP
HNN 88.0 0.73 78.1 93.3 86.3 88.8 84.0
MultipleSVM+CSS 90 0.77 91 77 89 90 -
APTK+DISULFIND 90.3 - 82.1 89.2 - - -
Multi-phase Approach 90.7 0.79 88.4 84.4 91.8 94.1 86
Our Algorithm 94.2 0.87 91.0 92.3 95.9 95.2 91.8
本團隊在雙硫鍵鍵結狀態之預測上提出了一個新的演算法，這個演算法藉由先對整條蛋白
8最佳結果，並以軟體名稱當成該序列的類別(Label)。由於對某些序列可能會同時有兩套軟
體有相同之最佳結果，因此剔除這些序列，最終剩下 720條序列。表 4列出每一種軟體擅
長預測的序列數量，代號 PK、NU及 RN分別 pknotsRG 、NUPACK及 RNAStructure三種
軟體，每一條序列均對應一最適合的預測軟體。
表 4 各序列之類別分佈
RN NU PK 總計
212 149 359 720
在讓 SVM選擇軟體前，必須產生 SVM可接受特定型態的輸入，此種特定的輸入，我
們稱為 RNA序列的特徵。在本研究中，總共使用了 18種特徵組計 642個，各種特徵名稱
與數量如表 5所示。
表 5 RNA序列特徵
編號 特徵名稱 數量
1 The Compositional Factor 4
2 The Bi-transitional Factor 16
3 The Distributional Factor 20
4 The Tri-transitional Factor 64
5 The Spaced Bi-gram Factor 16
6 The Potential Base-pairing Factor 3
7 The Asymmetry of Direct-Complementary Triplets 3
8 The Nucleotide Proportional Factor 12
9 The Potential Single-stranded Factor 3
10 The Sequence Specific Score 1
11 The Segmental Factor 20
12 The Sequence Moment 12
13 The Spectral Properties 20
14 The Wavelet Features 20
15 The 2D-dynamic representation 19
16 The Protein Features 375
17 The Co-occurrence Factor 10
18 The 2D graphical representation 24
總計 642
由於特徵數量繁多，且並非所以特徵均有利於分類，因此我們運用了資訊理論
(Information theoretical) [Mack03]的方法，並搭配 mRMR (minimal redundancy and maximal
relevance)[Peng05]，以達到對特徵進行重要性排名(Feature ranking)目標，mRMR的特徵選
擇方法如下。
10
特徵後進行分類器合併，以及採用全部 642特徵所訓練出的 SVM選擇器，對 720條 RNA
序列進行 Leave one out之交叉驗證。結果分別為 72.9%、73.0%及 72.2%，這顯示了二級結
構正確率確實有比較高。
表 7 不同 SVM選擇器之二級結構預測準確率比較
在此部份研究中，我們嘗試以 SVM當成 RNA二級結構預測軟體選擇器，以建構出一
更強的二級結構整合性預測軟體。為訓練 SVM，我們匯集了文獻上常用的 RNA 特徵，並
利用 mRMR特徵選擇機制發展出 Incremental mRMR特徵選擇方法。實驗結果顯示，除了
分類正確率有所提升外，二級結構預測正確率也有改進。由於我們的研究是奠基在現成二
級結構預測軟體上，因此如果軟體本身的預測結果較佳，透過 SVM選擇器來整合後的整體
二級結構預測正確率也有提升。未來如果有其他二級結構預測可供整合，再配合目前的特
徵選擇與分類器合併方法，相信可以建構出準確率更高的預測工具。
3 結論
本計畫之研究目的為進行蛋白質結構全原子預測，我們運用資訊學上之預測與分類方
法，並提出適合的演算機制，以提昇蛋白質結構預測之效率與準確性。在蛋白質結構預測
方面，我們以 SVM整合了 SABBAC以及 Chang的蛋白質全原子結構預測軟體，以建構出
一功能更強大的結構預測軟體。在另一蛋白質結構預測研究中，我們將重點放在對蛋白質
結構有重大影響之雙硫鍵鍵結狀態預測。我們提出了機率回饋的訓練演算法，以提高分類
準確度。以蛋白質結構預測為基礎，本研究另外也擴展至 RNA之二級結構預測，同樣是利
用支持向量機器當成結構預測選擇器。不過在特徵選擇方面則是採用資訊理論方法，並搭
配資訊融合技術，以提升 RNA序列之二級結構預測準確率。本計畫所提出之三項主要研究
成果，已分別於國際與國內研討會發表，研討會論文請參閱附錄資料。
在蛋白質結構預測與分類研究領域中，本研究團隊於此年度計畫中已有不錯的研究成
果，以目前研究成果為基礎，未來除了考慮蛋白質一級結構之其他特性，另外亦考慮蛋白
質內原子接觸(contact)研究，來增強結構預測之準確度。
4 參考文獻
1 [Ceroni03] A. Ceroni, P. Frasconi, A. Passerini, and A. Vulo, “Predicting the disulfide 
bonding state of cysteines with combinations of kernel machines,” Journal of VLSI Signal
Processing, vol. 35, pp. 287–295, 2003.
2 [Chan01] C. C. Chang and C. J. Lin, LIBSVM: a library for support vector machines, 2001,
12
1–5, 2007.
16 [Yen10] Hsin-Wei Yen, Chang-Biau Yang and Hsing-Yen Ann, " An Effective Tool
Preference Selection Method for Protein Structure Prediction with SVM," Proc. of the 27th
Workshop on Combinatorial Mathematics and Computation Theory, pp. 62-67, Taichung,
Taiwan, April 30-May 1, 2010.
5 附錄
國內研討會發表論文[Yen10]
國外研討會發表論文[Hor10]
國外研討會發表論文[Lin10]
- 2 -
Computer Engineering in the School of Computer Science at the
University of Manchester, UK.
3. New Variants of Particle Swarm Optimization, Professor Kusum Deep,
Department of Mathematics, Indian Institute of Technology Roorkee,
India.
我發表的論文如下：
Chih-Ying Lin, Chang-Biau Yang†, Chiou-Yi Hor and Kuo-Si
Huang, " Disulfide Bonding State Prediction with SVM Based on Protein
Types," Proceedings of The IEEE 5th International Conference on
Bio-Inspired Computing: Theories and Applications (BIC-TA 2010), Volume 2,
Liverpool, UK, September 8-10, 2010, pages 1436-1442.
我發表的論文是蛋白質的雙硫鍵狀態預測問題，其輸入資料為
一條蛋白質的氨基酸序列(一級結構)，輸出為Cysteine(半胱氨酸)是否有
雙硫鍵之鍵結。先前，我們亦曾進行此題目之研究。本次論文，我們提
出一個新的訓練策略，即預測機率之回饋概念，並使用許多人常用的
SVM(Support Vector Machine)分類器，進而提升預測之準確率。我們
實驗用的資料來自世界通用且著名的PDB(Protein Data Bank)。我們預
測的準確率為94.2%，比先前最好的結果進步3.5%。
本次會議雖然仍有報告者沒出席，但情況不多，已算是出席率
不錯的國際會議。報告者報告完畢後，現場也有不少迴響，進行熱烈
的討論。以下是我印象較為深刻的論文發表(有些論文作者較多，在
此僅列出報告者)：
1. Genetic Algorithms and the Art of Zen, Martyn Amos.
2. Automatic Fish Species Classification Based on Robust Feature
Extraction Techniques and Artificial Immune Systems, Gobriela
E. Soares.
3. Accelerating the Shuffled Frog Leaping Algorithm by Parallel
Implementation in FPGAs, Doniei Munoz.
4. Collaborative Project Pursuit for Face Recognition, Eduardo
- 4 -
用夜間沒有開會的時間，前往參觀圖書館。圖書館開放時間為上午
7:00至晚上23:00，開放時間相當長。館內面積不是很大，藏書量看起
也不是非常多。倒是電腦數量相當多。由於我沒有該校證件，負責櫃
臺值班的工讀生要我在一本來賓來訪的冊子簽名，也沒看我的任何證
件，相當簡單。不過，不知是否尚未開學，整座圖書館只有三個人（我、
一個學生、值班櫃臺）。人數雖少，但所有地方均是燈火通明。此種
照明方式在台灣一定會被要求節能減碳，少開幾盞燈。
在英國看到許多地名結尾有”ton”，如倫敦市區的Kensington、
Newington、Euston火車站、倫敦郊區的Loughton、Merton、Surbiton，
心中覺得納悶。回到台灣後，趕緊上網查詢，方才知道，ton與town
意義大約相同，指的是民眾聚集居住的地方，類似於鄉鎮或聚落。有
位網友說，ton的中文翻譯為「頓」，並不恰當，音不太對，也無法顯
示其英文本意。他建議應該改譯為「屯」，發音較為正確，也正好可
以對上英文本意。
四、攜回資料名稱及內容
此次會議之後，我攜回會議論文集一冊，論文集光碟片一片，
如下：
1. Proceedings of The IEEE 5th International Conference on Bio-Inspired
Computing: Theories and Applications (BIC-TA 2010), Volume 2,
Liverpool, UK, September 8-10, 2010.
- 6 -
與本次會議主辦者 Prof. Atulya K Nagar, Liverpool Hope University, UK合影
英國 Liverpool Hope University圖書館入口大廳
Disulﬁde Bonding State Prediction with SVM
Based on Protein Types∗
Chih-Ying Lin, Chang-Biau Yang†, Chiou-Yi Hor and Kuo-Si Huang
Department of Computer Science and Engineering
National Sun Yat-sen University
Kaohsiung 80424, Taiwan
Abstract—Disulﬁde bonds play the key role for pre-
dicting the three-dimensional structure and the function
of a protein. In this paper, we propose an algorithm for
predicting the disulﬁde bonding state of each cysteine in a
protein sequence. This method is based on the multi-stage
framework and the multi-classiﬁer of the support vector
machine. We also design a new training strategy to increase
the prediction accuracy. It appends the probabilities to
the existing features and then starts a new training
procedure repeatedly to improve performance. We perform
the experiments on the data set derived from the well-
known database Protein Data Bank (PDB). We get 94.2%
accuracy for predicting disulﬁde bonding state, which gets
improvement 3.5% compared with the previous best result
90.7%.
Index Terms—disulﬁde bond; bioinfomatics; support
vector machine; cysteine state prediction;
I. INTRODUCTION
Protein is essential for a living thing. The primary
structure of a protein is a sequence composed of
twenty kinds of standard amino acids. It is usually
believed that the protein function is mainly deter-
mined by its tertiary (three-dimensional) structure
[4]. In recent years, many research topics have
drawn much attention, such as the protein functions,
protein structures and family classiﬁcation [1], [9],
[17].
In a protein, a disulﬁde bond, which is bonded by
two cysteines, is very important to form the folding
and to maintain the stability of a protein, because
the force of disulﬁde bond is much stronger than
others, such as van der Waals force or hydrogen
bond. Besides, a disulﬁde bond might maintain the
secondary structure, such as alpha helix or beta
∗This research work was partially supported by the National
Science Council of Taiwan under contract NSC98-2221-E-110-062.
†Corresponding author: cbyang@cse.nsysu.edu.tw
sheet, by resisting the attack of water molecules
on hydrogen bonds. It resists water molecules by
forming the hydrophobic core of the folded protein
through condensing hydrophobic residues around
itself. Therefore, the disulﬁde bond might have
signiﬁcant information for protein structure and
function.
Cheng et al. [8] deﬁned the disulﬁde bond pre-
diction problem as the classiﬁcation problem of four
different levels. First, a protein may have several
chains. Researchers may want to know which pro-
tein chains contain disulﬁde bond and it is called
chain classiﬁcation. Second, a chain containing
disulﬁde bond does not mean all cysteines are
oxidized in this chain. Thus the state classiﬁcation
problem is to decide whether a cysteine is oxidized
or reduced in the chain. And third, given a pair
of cysteines, the bond classiﬁcation problem is to
predict whether they are bonded together. Finally,
given all cysteines, the connectivity prediction prob-
lem is to predict the corresponding cysteines for
each bonded pair. In this paper, we focus on the
study of the cysteine state (disulﬁde bonding state)
classiﬁcation problem.
Several algorithms have been proposed for the
state classiﬁcation problem. Martelli et al. [14]
proposed the method based on the hidden Markov
model (HMM) [16] and the neural networks (NN)
[12], [19]. Chen et al. [7] applied the support vector
machine (SVM) [11], [6] and an adjustment method,
called cysteine state sequence method, to predict the
cysteine state. In addition, some studies applied the
the multi-classiﬁer to deal with this problem [5],
[18].
In this paper, we develop an algorithm to im-
prove the accuracy of cysteine state classiﬁcation.
1436
___________________________________ 
978-1-4244-6439-5/10/$26.00 ©2010 IEEE 
 
 
TABLE I
A PARTIAL PSSM OF PDBID 1AHL.
A R N D C Q E G H I L K M F P S T W Y V
1 G 0 -3 -1 -2 -3 -2 -3 6 -3 -4 -4 -2 -3 -4 -3 -1 -2 -3 -4 -4
2 V 2 -3 -3 -3 -1 -2 -2 -2 -3 3 0 -2 0 1 -2 -1 -1 -3 -1 3
3 S 2 -2 -2 -2 -2 -1 -1 -1 -2 -3 -3 -1 -2 -4 7 1 -1 -4 -3 -2
4 C -1 -4 -4 -4 10 -4 -5 -3 -4 -2 -2 -4 -2 -3 -4 -2 -2 -3 -3 -2
5 L 0 3 -2 -3 -2 -1 -1 -3 -2 0 3 1 1 -1 -3 -2 -1 -3 -2 -1
6 C -1 -4 -4 -4 10 -4 -5 -3 -4 -2 -2 -4 -2 -3 -4 -2 -2 -3 -3 -2
7 D 0 -2 1 6 -3 -1 1 -2 -2 -2 -3 0 -3 -4 -2 -1 -1 -5 -3 -1
8 S 0 -1 2 2 -2 0 0 -1 -1 -3 -3 -1 -2 -3 -1 5 1 -4 -2 -2
9 D -2 -2 1 7 -4 -1 1 -2 -2 -4 -4 -1 -4 -4 -2 1 -1 -5 -4 -4
10 G 0 -3 -1 -2 -3 -2 -3 6 -3 -5 -4 -2 -3 -4 -3 -1 -2 -3 -4 -4
11 P -1 -3 -3 -2 -4 -2 -2 -3 -3 -4 -4 -2 -3 -5 8 -1 -2 -5 -4 -3
12 S 0 0 3 3 -2 0 0 -1 2 -3 -3 0 -2 -3 -2 3 2 -4 -2 -2
13 V 0 -2 -2 -3 -1 -2 -2 -3 -3 2 0 -2 0 -1 0 0 2 -3 -2 3
),|(
,, q
k
tqtq DWYP
),,|(
,,
mixCWDYP q
k
tqqtq =
)|( qq DCP
k
tqW ,qD
Fig. 1. The two-stage system [5].
D. The Simple Tune Method
In 2009, Chung et al. [10] proposed an algorithm
using PSSM as features of SVM to predict the
cysteine state. For every cysteine in a protein, this
algorithm adopts features Qi, L, PSSMk,i. Feature
Qi is the order of the cysteine in a protein, and its
normalization is done with dividing it by the number
of cysteines in this protein. Variable L indicates the
normalized scale of protein length, which is divided
by the longest length of proteins. PSSMk,i means
the scoring submatrix of (2k + 1)-size window
centering at cysteine i, which contains (2k+1)×20
elements. In order to normalize PSSMk,i, their
algorithm uses the minimum and maximum of every
scoring matrix to guarantee that all features are
of values between zero and one. Note that every
protein has different minimum and maximum.
In addition, the algorithm uses the probability
produced by SVM to perform the effective tun-
Fig. 2. The illustrations of assumptions of infeasible bonds. (a) A
bond within a peptide-chain. (b) The mal-aligned bond between two
peptide-chains.
ing method called simple tune. This simple tune
method is based on two assumptions. First, it tends
to be infeasible if two cysteines forming a disul-
ﬁde bond in one peptide-chain locate in the same
beta sheet, as shown in Figure 2(a). Second, if
there are several disulﬁde bonds on two peptide-
chains of a beta sheet, these bonds tend to align
in parallel to preserve the structure, as shown in
Figure 2(b). The simple tune method consists of four
adjustment steps, including boundary adjustment,
oxidized inversion, reduced inversion and odd-even
revision. This adjustment can improve the accuracy
of cysteine state prediction.
1438
TABLE II
HYDROPHOBIC COEFFICIENTS [13].
R -4.5 K -3.9 N -3.5 D -3.5
Q -3.5 E -3.5 H -3.2 P -1.6
Y -1.3 W -0.9 S -0.8 T -0.7
G -0.4 A 1.8 M 1.9 C 2.5
F 2.8 L 3.8 V 4.2 I 4.5
C. Feature Extraction of Type Classiﬁcation
In the type classiﬁcation, we include eleven sets
of features. Let L be the length of protein, Fi be
the occurrence of amino acid i in a protein, H(i)
be the hydrophobic coefﬁcient of residue i (Table
II), Pi,j be the position of the jth residue i, A(j) be
the residue at position j, and Sm,n be the score of
row m and column n in PSSM.
1) Protein length: L.
2) Cysteine count: FC .
3) Cysteine odd-even index: FC mod 2.
4) Amino acid composition: The composition of
residue i is Fi
L
.
5) Average cysteine position:
PFC
j=1 PC,j
FC
.
6) Average distance of every two cysteines:
PFC
x=1
PFC
y=1 |PC,x−PC,y|
L×FC
.
7) Average hydrophobicity:
P
20
i=1 Fi×H(i)
L
.
8) Average hydrophobicity around cysteine: The
2γ values of average hydrophobicity around
all cysteines are deﬁned as
PFC
j=1 H(A(PC,j+k))
FC
for −γ ≤ k ≤ γ, but k = 0. Here we set
γ = 7.
9) Cysteine position distribution: For 1 ≤ d ≤ ρ,
the dth cysteine position distribution is |{α =
PC,j
L
| (d−1)×L
n
< α ≤ (d)×L
n
, 1 ≤ j ≤ FC}|.
We set ρ = 5.
10) Cysteine distance distribution: For 1 ≤ d ≤ δ,
the dth cysteine distance distribution is |{β =
|PC,x−PC,y|
L
| (d−1)×L
n
< β ≤ (d)×L
n
, 1 ≤ x ≤
y ≤ FC}|. We set δ = 5.
11) Average PSSM of amino acid: The average
PSSM of residue i is
PL
m=1 Sm,i
L
.
D. Feature Extraction of Mix and State Classiﬁca-
tion
In the mix and state classiﬁcation, the involved
features are similar to the algorithm of Chung et al.
[10]. We only add several extra features to the exist-
ing feature set, but we have different normalization.
The features of cysteine j in a protein are described
as follows.
1) Cysteine index: PC,j
FC
.
2) Cysteine location: PC,j
L
.
3) Hydrophobicity around cysteine: The hy-
drophobicity of 2γ residues around a cysteine
is H(A(PC,j + k)), for −γ ≤ k ≤ γ, but
k = 0. We set γ = 12.
4) PSSM around cysteine: PSSMk,j (see Sec-
tion II-D). The k is set to be 12. We do not
use the minimum and maximum to normalize
this feature set.
We also add some feature sets used in the type
classiﬁcation, including average cysteine position,
average cysteine distance, cysteine odd-even index
and average hydrophobicity, to this classiﬁer.
E. Normalization of Features
The purpose that we divide the features by their
protein lengths (such as amino acid composition) or
cysteine count (such as cysteine index) is to make
them more signiﬁcant. That is, we decrease the
effect of the variance of sequence length or cysteine
count.
The normalization is to make the distribution
of feature values more evenly, and to make SVM
separate the feature vectors more easily. Let Nj and
Mj be the average and the standard deviation of
the values in the jth dimension of all xi’s. The
normalized value of each feature is xi,j −Nj
Mj
.
IV. EXPERIMENTAL RESULTS
The data set PDB4136, used by Martelli et al.
[14], was got from the PAPIA system [15] to select
non-homologous protein chains from PDB, with
identity less than 25%. And this data set has also
been used by many researchers.
We analyze all proteins in PDB released on
March 18, 2010, which contains 61924 proteins and
149773 chains in total. As the summarization of
protein types shown in Table III, the distribution
of protein types in PDB is similar to PDB4136.
We perform 20-fold cross-validations on
PDB4136. The k-fold cross-validation means that
it splits a data set D into k pieces D = {d1, d2,
1440
TABLE IV
ACCURACY COMPARISON OF DATA SET PDB4136 WITH PREVIOUS STUDIES.
Oxidized Reduced
Method QC MCC Speciﬁcity Sensitivity Speciﬁcity Sensitivity Qp
HNN [14] 88.0 0.73 78.1 93.3 86.3 88.8 84.0
MultipleSVM+CSS [7] 90.0 0.77 91.0 77.0 89.0 90.0 -
APTK+DISULFIND [18] 90.3 - 82.1 89.2 - - -
Multi-phase Approach [10] 90.7 0.79 88.4 84.4 91.8 94.1 86.0
Our Algorithm 94.2 0.87 91.0 92.3 95.9 95.2 91.8
[4] M. K. Campbell and S. O. Farrell, Biochemistry, 4th ed.
Thomson-Brooks/Cole, 2003.
[5] A. Ceroni, P. Frasconi, A. Passerini, and A. Vullo, “Predicting
the disulﬁde bonding state of cysteines with combinations of
kernel machines,” Journal of VLSI Signal Processing, vol. 35,
pp. 287–295, 2003.
[6] C.-C. Chang and C.-J. Lin, “LIBSVM: a library for
support vector machines,” 2001, software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
[7] Y.-C. Chen, Y.-S. Lin, C.-J. Lin, and J.-K. Hwang, “Prediction
of the bonding states of cysteines using the support vector
machines based on multiple feature vectors and cysteine state
sequences,” Proteins: Structure, Function, and Bioinformatics,
vol. 55, pp. 1036–1042, 2004.
[8] J. Cheng, H. Saigo, and P. Baldi, “Large-scale prediction
of disulphide bridges using kernel methods, two-dimensional
recursive neural networks, and weighted graph matching,” Pro-
teins: Structure, Function, and Bioinformatics, vol. 62, pp. 617–
629, 2006.
[9] C.-C. Chuang, C.-Y. Chen, J.-M. Yang, P.-C. Lyu, and J.-K.
Hwang, “Relationship between protein structures and disulﬁde-
bonding patterns,” Proteins: Structure, Function, and Bioinfor-
matics, vol. 53, pp. 1–5, 2003.
[10] W.-C. Chung, C.-B. Yang, and C.-Y. Hor, “An effective tuning
method for cysteine state classiﬁcation,” in Proc. of National
Computer Symposium, Workshop on Algorithms and Bioinfor-
matics, Taipei, Taiwan, Nov. 27-28 2009.
[11] N. Cristianini and J. Shawe-Taylor, An Introduction to Support
Vector Machines and other kernel-based learning methods.
Cambridge University Press, 2000.
[12] K. Gurney and K. N. Gurney, An introduction to neural
networks. MIT Press, 1995.
[13] J. Kyte and R. F. Doolittle, “A simple method for displaying
the hydropathic character of a protein,” Journal of Molecular
Biology, vol. 157, pp. 105–132, 1982.
[14] P. L. Martelli, P. Fariselli, L. Malaguti, and R. Casadio, “Pre-
diction of the disulﬁde-bonding state of cysteines in proteins at
88% accuracy,” Protein Science, vol. 11, pp. 2735–2739, 2002.
[15] T. Noguchi, H. Matsuda, and Y. Akiyama, “PDB-REPRDB: a
database of representative protein chains from the protein data
bank (PDB),” Nucleic Acids Research, vol. 29, no. 1, pp. 219–
220, 2001.
[16] L. R. Rabiner, “A tutorial on hidden markov models and
selected applications in speech recognition,” Proceedings of the
IEEE, vol. 77, no. 2, pp. 257–286, 1989.
[17] R. Rubinstein and A. Fiser, “Predicting disulﬁde bond connec-
tivity in proteins by correlated mutations analysis,” Bioinfor-
matics, vol. 24, no. 4, pp. 498–504, 2008.
[18] M. Vincent, A. Passerini, M. Labbe, and P. Frasconi, “A
simpliﬁed approach to disulﬁde connectivity prediction from
protein sequences,” BMC Bioinformatics, vol. 9, no. 1, p. 20,
2008.
[19] J. Zurada, Introduction to artiﬁcial neural systems. St. Paul,
MN, USA: West Publishing Co., 1992.
1442
- 2 -
Using Positional Association Rules Algorithm on Protein Sequence Motifs”
2. Bhadrachalam Chitturi,“On Complexity of Transforming Strings”
3. Bhadrachalam Chitturi, Hal Sudborough,“Prefix Reversals on Strings”
4. Ankit Agrawal, Alok Choudhary, Xiaoqiu Huang,”Non-Conservative Pairwise Statistical
Significance of Local Sequence Alignment Using Position-Specific Substitution
Matrices”
5. Alvaro J. Gonzalez, Li Liao, Cathy H. Wu, “Predicting Functional Sites in Biological
Sequences Using Canonical Correlation Analysis”
6. Tai-Chun Wang, Javid Taheri, Albert Y Zomaya, “A Combinative Strategy for Higher
Reliable Tag SNPs Selection”
7. Bingru Yang, “A Novel Protein Secondary Structure Prediction System Based on
Compound Pyramid Model”
8. Umair Azfar Khan, Humaira Humayun Qureshi, “Comparative Analysis of the
Computational Methods Used in Protein Structure Prediction”
本次的第一場 keynote speech是一位美國太空總署的科學家講述有關他們如何找太空中
其他可能的生命體，或者是其他環境有可能適合人類居住的星球。由於可能的生命體的種類
太多，美國太空總署的原則是找尋可能有水的星球，並且從很久以前就有用無線電波傳送代
表地球的一些圖騰。另外一個主題是有關火星登陸的歷程，因為火星的引力比地球大很多，
為了不讓機器人摔壞，他們設計了兩種的減速裝置，第一種是為機器人配置了降落傘，第二
種是把機器人放在一個氣囊裡面，讓它在火星表面上彈跳。
第二場和第三場 keynote speech都和雲端計算有一點相關，第二場是一家軟體公司發表
了他們結合了嵌入式系統的 CPU與 Intel CPU機器的平台，他們會依照要實作的功能以及
CPU種類，把一些東西轉成組合語言、硬體描述語言或是 C語言，並且分配到各台機器上面
執行。第三場演講算是雲端計算的一些概述，我之前沒有深入的瞭解過雲端計算，只覺得就
是一堆計算資源、記憶體與儲存空間，你也不用管它是哪裡，反正就是依照需要的資源數付
錢。聽了這次演講才發現後面的資訊安全學問相當大，要怎麼讓人家能夠放心的把東西交給
你計算卻又不用擔心被你盜用，以及在各電腦之間的資料傳遞時的安全性等等。
我們發表的論文是有關對每條 RNA序列選擇一套軟體來做結構預測，本次研討會我的
論文報告完之後主持人問了兩個問題，第一個是我們為什麼是選擇一套軟體而不是取多套軟
體之間的共識，第二個是我們的準確率評分方式為什麼是只看 base pair的配對位置而沒有看
配對的 RNA。
二、與會心得
A Tool Preference Selection Method for RNA Secondary Structure
Prediction with SVM1
Chiou-Yi Hor2, Chang-Biau Yang2,3, Chiou-Ting Tseng2, and Hung-Hsin Chen2
2Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan
Abstract— Prediction of RNA secondary structures has
drawn much attention from both biologists and computer
scientists. Many useful tools have been developed for this
purpose, with or without pseudoknots. These tools have
their individual strength and weakness. As a result, we
propose a tool preference selection method which integrates
three prediction tools pknotsRG, RNAstructure and NUPACK
with support vector machines (SVM). Our method starts
with extracting features from the target RNA sequences,
and adopt the information-theoretic feature selection method
for feature ranking. We propose a method to combine
feature selection and classifier fusion, namely incremental
mRMR. The test data set contains 720 RNA sequences,
where 225 pseudoknotted RNA sequences are obtained from
PseudoBase, and 495 nested RNA sequences are obtained
from RNA SSTRAND. Our method serves as a preprocessing
way in analyzing RNA sequences before the RNA secondary
structure prediction tools are employed. Experimental results
show that our method improves not only the classification
accuracy, but also the base-pair accuracies.
Keywords: RNA, SVM, Fusion, Feature Selection, Tool Preference
1. Introduction
An RNA secondary structure is the fold of the given
sequence. The sequence is folded due to bonds between
non-adjacent nucleotides. These bonded nucleotide pairs
are called base pairs. Three possible combinations of nu-
cleotides may make a base pair: A-U, G-C, and G-U,
where A-U and G-C are called Watson-Crick pairs and G-
U is called the Wobble pair. The RNA secondary structure
prediction problem is to identify the folding configuration of
a given RNA sequence.
The methods for predicting RNA secondary structure
could be roughly categorized into two types. They are
based on thermodynamics [26], [27], [31], and comparative
approaches [5], [29], respectively. Since these tools resort
to different criteria, each of them has its own metric and
weakness. With their variety, we propose a tool preference
selection method that integrates these software in order to
improve prediction capability. Our method is based on the
machine learning approach. It includes feature extraction,
feature selection and classifier combination methods. The
features are first extracted from the given sequence and then
1This research work was partially supported by the National Science
Council of Taiwan under contract NSC-98-2221-E-110-062.
3Corresponding author: cbyang@cse.nsysu.edu.tw.
these features are input into the classifier to determine the
most suitable prediction software. In this paper, the state
of the art feature selection, mRMR [21], is employed to
identify the important features and SVM (support vector
machine) [1], [8] is used as the basis classifier. To further
improve prediction accuracies, we propose a multi-stage
classifier combination method. Instead of selecting features
independently, our classifier combination method takes the
output of classifiers in the previous stages into consideration.
Thus, the method guides the feature selector to choose
features that are most relevant to the target class label while
least dependent on what has been learned by the ensemble.
The experimental results shows that our tool preference
selection method can improve both the classification and
base-pair prediction capability.
The rest of this paper is organized as follows. In Section
2, we will give a more detailed description for the RNA
secondary prediction tools used in our paper. In addition, we
also briefly describe SVM, and some prediction software. We
introduce our feature extraction method in Section 3. Section
4 presents the feature relevance and selection. In Section
5, we focus on how to integrate multiple classifiers. Our
experimental results and conclusions are given in Sections
6 and 7, respectively.
2. Preliminaries
2.1 Support Vector Machines
Support vector machine (SVM) [6], [30] is a well-
established technique for data classification. Given a training
set of n-dimensional instances and label pairs (xi, yi), for
1 ≤ i ≤ N where xi ∈ Rn and y ∈ {−1,+1}, the SVM
solves the following optimization problem:
min
w,b,ξi
1
2w
Tw + C
∑N
i=1 ξi,
subject to yi(wTφ(xi) + b) ≥ 1− ξi, (1)
ξi ≥ 0.
The function φ maps the training vectors xi into a higher
dimensional space, namely feature space. SVM finds a linear
separating hyperplane with normal vector w and offset b
that constitutes the maximal margin in the feature space.
The symbols ξi and C represent the slack variable and the
penalty of errors in the optimization problem. To describe
the similarity between vectors in the feature space, the kernel
function is defined. In this paper, we adopt radial basis
function (RBF) as it yields best results.
FX(f) =
|S|∑
k=1
IX(k, 1)exp(−2pikfi|S| ), (6)
where 1 ≤ f ≤ |S|. The |FX(f)| represents the magnitude
of a frequency f and thus it represents the intensity for a
specific spectral. The total energy EX is defined as
EX(S) =
√√√√ |S|∑
f=1
||FX(f)||2. (7)
The spectral entropy for a given nucleotide is:
HX(S) =
|S|∑
f=1
|FX(f)|
EX(S)
log
|FX(f)|
EX(S)
. (8)
The spectral inertia for a given nucleotide is:
JX(S) =
|S|∑
f=1
f2
|FX(f)|
EX(S)
. (9)
The position at which the maximal spectral energy for
a given nucleotide occurs and its corresponding energy
percentage is:
PX(S) = arg
|S|
max
f=1
|FX(f)|/|S|. (10)
MX(S) =
|S|
max
f=1
|FX(f)|/EX(S). (11)
In addition to separate encoding of A,G,C,U nu-
cleotides, the spectral properties can also be consid-
ered simultaneously. That is, A,G,C,U are encoded into
0001, 0010, 0100, 1000 respectively. Thus, the above cal-
culation can be applicable, but the length of the binary
bit string becomes 4|S| . The spectral entropy, inertia,
maximal position and maximal energy percentage features
are calculated for A,G,C,U and ACGU encodings. Hence,
there are 20 spectral features.
3.6 The Wavelet Features
Wavelet transform [11], [22] is a technique that decom-
poses a signal into several components. It is believed that
the wavelet analysis provides the information that might be
obscured by Fourier analysis. In this paper, the maximal
overlap DWT (MODWT) is adopted and let the maximal
scale level be J . Since the MODWT is an energy-preserving
transform, the energy is unchanged after transformation.
1
N
|p|2 = 1
N
J∑
j=1
|qj|2 + |rJ+1|2, (12)
where p represents the original signal and each |qj|2/N
represents the decomposed variance at scale of j. To
obtain useful information for classification, we first con-
vert the sequence into a signal IX(k, 1), where X ∈
{A,U,G,C,ACGU}. Then we decompose the variance of
the original signal with different wavelet scales. Because
all of the sequences have length greater than 16, we select
maximal J = 4. Consequently, it totally yields 20 features.
3.7 The 2D-dynamic representation
The 2D graphical representation [2], [3] was proposed by
Bielinska-Waz et al., which adopts 2D graphical methods to
characterize nucleotide sequences. In their study, they used
nucleotides to generate a walk on the 2D graph. The walks
are made as follows: A=(-1,0), G=(1,0), C=(0,1) and T=(0,-
1). For RNA sequences, nucleotide T is replaced by U . After
finishing the walk, a 2D-dynamic graph is generated, which
can be regarded as an image. The mass of point (x,y) is
determined by how many times the walk stops there. In
this paper, the two principal moments of inertia, orientation,
eccentricity, x and y centroids, µ02, µ03, µ11, µ12, µ13, µ20,
µ21, µ22, µ23, µ30, µ31, µ32 and µ33 of the 2D-dynamic
graph descriptors are used. The number of features is 19.
3.8 The Protein Features
The genetic code [20] is the set of rules by which
nucleotides is translated into proteins. These codes define
mappings between tri-nucleotide sequences, called codons,
and amino acids. Since three nucleotides are involved for
translation, this constitutes a 43-versus-20 mapping to com-
mon amino acids. The first codon is defined by the initial
nucleotide from which the translation process starts. There
are three possible positions to start translation, each of which
yields a different protein sequence. As a result, we usually
say that every nucleotide sequence can be read in three
reading frames.
Once a nucleotide sequence is converted into a protein
sequence, its 125 PSI (protein sequence information) fea-
tures [9] can be extracted accordingly. Since there are three
possible ways for conversion, it yields 375 PSI features.
3.9 The Co-occurrence Factor
The co-occurrence factor [16] represents the distribution
that two nucleotides occur simultaneously at a given offset
within an given range in a sequence S. Let the central
nucleotide at position k be X and half window size be h.
The co-occurrence factor counts the occurrence of (X,Y )
by:
CXY (S) =
k+h∑
i=k−h,i6=k
{
1 S(i)=Y
0 Otherwise (13)
Since the minimal length is 21 among all sequences,
we set h to 10. To count for longer sequences, the co-
occurrence is calculated with a sliding window scheme.
There are 10 distinct co-occurrence patterns to count, which
are AA,AC,AG,AU,CC,CG,CU,GG,GU,UU . This is
because the pattern XY and Y X are regarded as symmetric
Input: X: all available feature set, where |X| = n.
Xs: selected feature subset.
Lc: outputs of the built classifiers.
m: the number of features to select.
Output: Selected features, a subset of X −Xs.
Step 1: Convert the outputs of the built classifiers into
numeric variables Xc. Let the number of built
classifiers be c.
Step 2: Perform mRMR feature rankings with the following
formula:
Xr = {max
r
[I(Xj , Y )−
1
r − 1 + k ∗ c
∑
Xi⊂{Xr−1∪Xc}
I(Xj , Xi)]|
Xj ⊂ {X−Xr−1 −Xs}} (17)
where k is a weighted factor to denote how much
the information of the built classifiers should be
taken into consideration.
Step 3: Output Xm={X1, X2, . . . , Xm}.
6. Experimental results
6.1 Data sets
The experimental data sets are obtained from PseudoBase
and RNA SSTRAND websites. We retrieve all PseudoBase
and RNA SSTRAND tRNA sequences and their secondary
structure information. The original numbers are 705 and
300, respectively. The sequences are then fed into pknot-
sRG, RNAstructure and NUPACK for secondary structure
prediction. To determine which software is the most suitable
for a given sequence, we adopt the base-pair accuracy for
evaluation.
Given a sequence S = a1a2 . . . aN , suppose the real
partner of a nucleic base ai is aj , where j 6= i, 1 ≤ i ≤ N
and 0 ≤ j ≤ N . If ai is unpaired, j = 0. Otherwise,
1 ≤ j ≤ N . Let the predicted partner of ai be ak. The
predicted base-pair accuracy for a single sequence is:
Accuracy = 100%× 1
N
N∑
i=1
{
1, j=k
0, Otherwise (18)
For each sequence, we calculate the base-pair accuracy
given by the three software and assign a class label, which
corresponds to the preferred software, to the sequence. The
labels are nu, pk and rn, which associate with NUPACK,
pknotsRG and RNAstructure, respectively. Since our goal
is to apply the machine learning approach to identifying
the most prominent software for prediction, we remove
the sequence that any two softwares have identical highest
accuracies in order to avoid ambiguity. Hence, the final
numbers are 495 for RNA SSTRAND tRNA and 225 for
PseudoBase database. The number of numbers in each tool
preference classes given in Table 2.
6.2 Feature selection
Before applying mRMR processes, each feature variable
is discretized into three states at the positions µ± σ, where
Table 2: Number of sequences in each tool preference class.
rn nu pk Total
212 149 359 720
Table 3: Classification Accuracies of Various Fusion Con-
figurations.
WMAJ
1 1+2 1+2+3 1+2+3+4
a. mRMR 68.3 68.8(+0.5) 68.2(-0.6) 67.8(-0.4)
b. imRMR 68.3 69.3(+1.0) 69.6(+0.3) 70.2(+0.6)
imRMR: Incremental mRMR.
µ is the mean value and σ is the standard deviation. The
discretized values takes -1 if it is less than µ − σ, 1 if
larger than µ + σ, and 0 if otherwise. We used the Leave-
One-Out (LOO) cross-validation method for testing. By
default, mRMR selects 50 most prominent features. Hence,
at each stage, we obtain 50 features to train classifiers.
We combine the classifiers incrementally. The methods for
classifier combination is the modified weighted majority vote
(WMAJ). In order to compare the difference between feature
selection with and without considering classifier outputs in
previous stages, we perform both experiments. The weighted
factor for the modified mRMR is set to 1.
In Table 3, the experiments combine mRMR feature
selection methods and WMAJ classifier fusion methods. Two
feature selection configurations are compared, which are
feature selection with and without considering outputs from
the previous stages. For the former one, the selected features
are first used to train classifiers. Then, the data are predicted
by the classifiers to produce output classes. These outputs
are served as artificial features for the subsequent stages.
Consequently, the following feature selection procedure will
encourage unselected features, which are most relevant while
least dependent on the classifier outputs. Thus, the original
mRMR are modified to learn incrementally. For feature
selection without considering outputs from the previous
stages, once features are selected, we just exclude them
and perform the original mRMR procedure in the next
stage. It is observed that if feature selection procedures
takes the classifier preference from the previous stages into
consideration, the classification rates keep elevated. This
may be due to the additional discriminant information is
involved.
Table 4 shows the classification, base-pair prediction ac-
Table 4: The classification and base-pair prediction accura-
cies of various configurations.
Configuration Features# Classification Base-pair
accuracy % accuracy %
mRMR 50 68.3 72.9(+4.1)
imRMR+WMAJ 50 × 4 70.2 73.0(+4.2)
All 642 66.3 72.2(+3.4)
imRMR: Incremental mRMR.
An Effective Tool Preference Selection Method for Protein
Structure Prediction with SVM ∗
Hsin-Wei Yen, Chang-Biau Yang1, and Hsing-Yen Ann
Department of Computer Science and Engineering
National Sun Yat-sen University, Kaohsiung, Taiwan 80424
1Corresponding author: cbyang@cse.nsysu.edu.tw
Abstract
Prediction of protein structure is a problem
of great interest in bioinformatics. Many stud-
ies have been devoted to this issue, such as Ad-
cock’s method, MaxSprout, SABBAC and Chang’s
method. In this paper, we combine the power
of two outstanding tools, SABBAC and Chang’s
method. Based on SVM, we propose a tool pref-
erence classification method for determining which
tool is potentially the better one used to predict the
structure of a target protein. We design a heuris-
tic method to select the better feature set combina-
tion for SVM. We test our method on the proteins
with standard amino acids in CASP7 dataset,
which contains 30 protein sequences. The experi-
ment results show that our method has 9.19% and
5.37% RMSD improvement against SABBAC and
Chang’s result, respectively. Our method can also
be applied to other effective prediction methods de-
veloped in the future.
1 Introduction
A protein consists of a linear chain of amino
acids which include 20 standard amino acids and
some nonstandard amino acids. Each protein has
its own functions, and its linear chain has to be
folded correctly to reveal its corresponding func-
tions. This is why the analysis of the 3D conforma-
tions of proteins is so important. People usually
use X-ray crystallography and NMR (nuclear mag-
netic resonance) [10] to determine the structures
of proteins, but both of them are time-consuming
and costly. To make structure analysis easier, re-
searchers try to determine the 3D-structures from
protein sequences with computers, and then only
∗This research work was partially supported by the Na-
tional Science Council of Taiwan under contract NSC-98-
2221-E-110-062 .
use X-ray crystallography or NMR to reveal the
details which they are really interested in.
There are two main methods to predict the po-
sitions of atoms in a protein sequence, homology
modeling and ab initio. The homology modeling
method uses the known structures to construct a
template database, and use the geometry similar-
ity of protein fragments to determine the positions
of atoms. On the other hand, ab initio does not
need to collect the existing information of proteins.
Some ab initio methods base on the force field be-
tween molecules, which consider the energy be-
tween atoms in the amino acid, such as hydrogen
bonds, van der Waals force, electrostatic force, etc.
The conformation with minimum energy is consid-
ered as the proper positions of atoms, because the
structure is the most stable when the energy is
minimized.
In this paper, we consider the all-atom protein
backbone reconstruction problem (PBRP). Given
the 3D coordinates of the α-carbons (Cα) in pro-
tein and its sequence, the PBRP is to rebuild
the 3D coordinates of all major atoms, includ-
ing N, C and O, on the backbone. Wang [11]
proposed an algorithm to reconstruct the atoms
of protein backbone with the homology model-
ing method. However, the prediction accuracy
of oxygen (O) atoms in amino acid is obviously
lower other atoms. Later, Chang [2] proposed a
method to further refine the O atoms based on
Wang’s result. SABBAC [9] is another software
tool for predicting the all-atom position. Chang’s
method and SABBAC utilize different template
knowledge, so neither one of them dominates the
other in all protein sequence prediction.
In this paper, we propose a preference classi-
fication strategy to determine which tool, either
Chang’s method or SABBAC, is a better predictor
for the given protein sequence. For a given pro-
tein sequence, if we can choose the most suitable
tool for predicting, we can reduce the RMSD (root
The 27th Workshop on Combinatorial Mathematics and Computation Theory
62
been developed, and LIBSVM [1] is one of the
packages with high classification accuracy. We use
the grid search for tuning the parameters of SVM
which achieve the best performance and avoid
over-fitting or under-fitting. And then we import
the test dataset to compute the accuracy.
Several feature sets, C,H, V, P and Z, are usu-
ally used to solve proteins classification problems
[5, 6, 7]. C represents the composition of 20 amino
acids, H represents the Hydrophobicibility , V rep-
resents the Normalized van der Walls volume, P
represents the Polarity, and Z represents the Po-
larizability. In previous studies, these feature sets
benefit the protein fold recognition. However, ob-
tained from our primitive experiments, these fea-
ture sets cannot work well in the preference clas-
sification.
Each feature in C is composition percentage of
one amino acid, so there are 20 features in C. In
each of the other feature sets (HVPZ), 20 amino
acids are divided into three groups by their prop-
erties, as shown in the first four rows of Table 1.
To improve accuracy of preference classifica-
tion, we add three feature sets E,N and A [8],
which represent size, charge, and aliphaticity or
aromaticity. Besides, in the feature selection pro-
cess, the features in one set are considered to be
independent. We separate one original feature set
into three parts with coding rules except C, be-
cause there is only one coding rule in C. The fea-
tures in one original set are regrouped according
to the rule of composition, transition, and distribu-
tion. For example, H is separated into three parts:
H, I, J; P is separated into P, Q, R. Finally, we
have 22 feature sets. Table 2 shows all new feature
sets and the number of features in each set.
Here is an example, shown in Figure 2, to ex-
plain the coding scheme of 3, 3 and 15 features in
Table 1: Three groups of 20 amino acids in each
coding scheme.
Coding Scheme Group 1 Group 2 Group 3
Hydrophobi- R, K, E, D, G, A, S, T, C, V, L, I,
city (H) Q, N P, H, Y M, F, W
Polarity L, I, F, W, P, A, T, G, H, Q, R, K,
(P ) C, M, V, Y S N, E, D
Normalized G, A, S, C, N, V, E, Q, M, H, K, F,
van der Waals T, P, D I, L R, Y, W
volume (V )
Polarizability G, A, S, D, C, P, N, V, K, M, H, F,
(Z) T E, Q, I, L R, Y, W
Size A, G, C, S P, N, D, T, the others
(E) V
Charge D, E K, R, H the others
(N)
Aliphatic or V, I, L Y, H, W, F the others
Aromatic (A)
Table 2: 22 feature sets with their sizes.
Original Set Composition Transition Distribution
C (20) C(20)
H (21) H (3) I (3) J (15)
V (21) T (3) U (3) V (15)
P (21) P (3) Q (3) R (15)
Z (21) X (3) Y (3) Z (15)
E (21) E (3) F (3) G (15)
N (21) L (3) M (3) N (15)
A (21) A (3) B (3) D (15)
                                5        10       15       20       25
Sequnce:            VSLNF KDPEA VRALT CTLLR EDFGL    
Group  :            12131 33232 13212 12113 33121
Number of Group 1:  1 2 3       4  5  6 78    9 10
Number of Group 2:   1      2 3   4 5  6       7
Number of Group 3:     1  23 4   5        6 78
                    12131 33232 13212 12113 33121
1-2/2-1 transitions:^^        ^   ^^^ ^^      ^^   
2-3/3-2 transitions:       ^^^   ^  
1-3/3-1 transitions:  ^^^       ^        ^   ^
Figure 2: A coding example of a sequence.
P, Q and R, respectively. The 3 feature values in
P are 0.4, 0.28 and 0.32. The first is 0.4 (10/25)
because 10 amino acids are contained in Group 1
and the sequence length is 25. Similarly, we get
0.28 (7/25) and 0.32 (8/25) for Group 2 and Group
3, respectively. The set Q includes 0.42, 0.16 and
0.25. 10 ”1-to-2” or ”2-to-1” transitions are repre-
sented by 0.42 (10/24) and there are 24 transitions
in a sequence consisting of 25 amino acids. So the
next 2 values are 0.17 (4/24) for Group 2 and 0.25
(6/24) for Group 3. In the set R, the first five
values are derived from the locations of the first,
1/4, 1/2, 3/4, and the last amino acid in Group
1 over the length of the protein sequence. Thus,
we get 0.04 (1/25), 0.12 (3/25), 0.56 (14/25), 0.76
(19/25), 1 (25/25) for Group 1, 0.08, 0.32, 0.52,
0.6, 0.96 for Group 2, and 0.16, 0.24, 0.36, 0.8,
0.88 for Group 3.
3.2 Our Classification Method
We propose a three-stage method to search for
the effective feature sets for solving the preference
classification problem. In Stage 1, we pick ρ win-
ner feature sets by checking the accuracy of each
combination in jackknife test, where the feature
set combinations are the elements of the power
set of the 3 sets derived from the same original
feature set. For example, we divide P into P, Q
and R, so 7 combinations P, Q, R, PQ, QR, PQ
and PQR are tested in Stage 1. Totally 50 combi-
nations, as shown in Table 3, are examined. Since
The 27th Workshop on Combinatorial Mathematics and Computation Theory
64
Table 5: The classification accuracies of the top
10 feature set combinations in Stage 1.
Combination Accuracy log(c) log(γ)
N 80.00 8 -6
LN 76.67 8 -6
AB 73.33 6 -4
I 70.00 8 -2
P 70.00 2 6
D 70.00 2 -2
H 66.67 8 -2
Q 66.67 4 2
PQ 66.67 8 -8
Z 66.67 6 -6
4 Experimental Results
We test our preference classification method on
CASP7. There are 30 protein sequences consist-
ing of only standard amino acids. Table 4 shows
the ID in CASP7, PDB and the RMSD in two
prediction tools.
In Stage 1, 50 combinations, including C, and
powers sets of HIJ, TUV, PQR, XYZ, EFG, LMN,
and ABD are tested. N, LN, AB, I, P, D, H, Q,
PQ, Z are the top 10 combinations with respect
to accuracy, as shown in Table 5. In our exper-
iments, we set ρ = 6. So we pick N, L, A, B,
I, and P as the 6 winner feature sets, and the
other 16 feature sets are the losers. The power
sets of these 6 feature sets are tested in Stage 2.
The combination with the lowest average RMSD
is NB (0.410 A˚), as listed in Table 6. And then
NB is chosen as the initial base of Stage 3. In the
first round of Stage 3, we add the 16 loser feature
sets into NB: NBC, NBH, NBJ, NBT, NBU, NBV,
NBQ, NBR, NBX, NBY, NBZ, NBE, NBF, NBG,
NBM and NBD. We get that the lowest average
RMSD appears in NBT (0.405A˚). NBT becomes
the new base for the next round, and then we
test NBTC, NBTH, NBTJ, NBTU, NBTV, · · ·.
The best RMSD of each round is shown in Ta-
ble 7. In the table, we conclude that the best
feature set combination found by our method is
NBT, which improves RMSD to 0.405A˚. For a
perfect classifier (100% classification), the average
RMSD could be reduced to 0.399A˚, which is the
upper bound of performance improvement of our
method. The conclusion of our method and perfor-
mance improvement are shown in Table 8. The av-
erage RMSD of our method is reduced to 0.405A˚,
which is 9.19% and 5.37% improvement against
SABBAC and Chang’s result, respectively.
Table 6: The average RMSD and classification ac-
curacies of the top 10 feature set combinations in
Stage 2. log(c) and log(γ) denote the best param-
eter values for SVM.
Combination Average RMSD Accuracy log(c) log(γ)
NB 0.410 83.33 8 -8
NBP 0.414 80.00 8 -8
NBI 0.415 76.67 8 -6
AB 0.415 76.67 6 -4
NAB 0.416 80.00 8 -6
NLA 0.416 76.67 8 -8
N 0.416 80.00 8 -6
NL 0.418 73.33 8 -6
NLB 0.421 73.33 8 -6
NABI 0.422 76.67 8 -6
Table 7: The best RMSD in each round of Stage
3.
Round Combination Average RMSD
Round 0 NB 0.410
Round 1 NBT 0.405
Round 2 NBTE 0.406
Round 3 NBTEU 0.409
Round 4 NBTEUX 0.410
Round 5 NBTEUXG 0.412
Round 6 NBTEUXGF 0.415
Round 7 NBTEUXGFH 0.417
Round 8 NBTEUXGFHY 0.417
Round 9 NBTEUXGFHYZ 0.421
Round 10 NBTEUXGFHYZR 0.419
Round 11 NBTEUXGFHYZRV 0.417
Round 12 NBTEUXGFHYZRVM 0.421
Round 13 NBTEUXGFHYZRVMQ 0.427
Round 14 NBTEUXGFHYZRVMQD 0.428
Round 15 NBTEUXGFHYZRVMQDJ 0.427
Round 16 NBTEUXGFHYZRVMQDJC 0.428
5 Conclusions
SVM is an effective classifier used widely. If
we want to get high classification accuracy, ex-
tracting more effective features for the classifier
is necessary. Therefore, many studies, such as fea-
ture filter [4], and feature wrapper, are in progress.
People try to find better features to improve the
accuracy, but our goal is a little different. We want
not only to improve the accuracy but also to de-
crease the average RMSD. For this reason, we pro-
pose a three-stage method to search the effective
feature set combination for decreasing the RMSD
in protein structure prediction. Our aim is to get
the lowest RMSD for each protein in the dataset.
We propose a new feature extraction scheme to
get 22 new feature sets and then build the best
feature combination with a heuristic method.
Most of time, SVM is faster than NN (Neural
Network), but sometime it is still not fast enough.
In our case, we have to check 222 − 1 combina-
The 27th Workshop on Combinatorial Mathematics and Computation Theory
66
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
