 2
 
 
1. 中英文摘要 
  
 本計劃提出了一個針對棒球節目發展以棒球運動事件為基礎的視訊偵測及檢索系統。
傳統之運動節目事件偵測系統通常以視訊畫面(frame-based)或以視訊鏡頭(shot-based)為基
礎作視訊特徵之擷取及語義之萃取，進而偵測特定之運動事件。但是，此做法較難得到較
完整運動事件之視訊畫面，同時也比較沒有結構化來分析棒球節目。棒球運動節目結構化
會讓使用者會更容易得到他想查詢的片段。一開始我們先切割出所有的鏡頭(shot)，接著針
對各個鏡頭的關鍵畫面(key-frame)及在此鏡頭(shot)內之連續視訊畫面做靜態及動態之影像
特徵擷取。靜態之影像特徵包含球場內之草地與泥土顏色的偵測、物件的切割及擷取、球
員之膚色偵測等，動態之視訊特徵主要是場景變換之位移量。接著利用事先規劃好的規則
來做帶有語義之場景(semantic scene)偵測及分類。通常一特定之棒球運動事件是由一連串且
連續之語義場景所組成，在本計劃中場景分類包含：投球場景(pitching scene)，跑壘場景
(running scene)，本壘場景(base scene)，打擊場景(player scene)，內野場景(infield scene)，外
野場景(outfield scene)，特寫場景(close-up scenes)以及其他類場景。利用這一連串分類好的
場景套入 HMM (Hidden Markov Models) 中做事件的偵測，可以準確的偵測出某一特定之
棒球運動事件。在本計劃裡，我們想要偵測之棒球運動事件有下列四種，分別為：三振、
接殺，刺殺、以及安打，實驗的數據顯示我們的系統可以正確的偵測出此四種棒球運動事
件。如此對於某一特定之棒球運動事件，使用者可以獲得完整之視訊畫面。 
關鍵字：場景，運動視訊，隱藏式馬可夫串列，關鍵畫面。 
 
Recently, many researches are addressed on the event detection of sport video. Most of these 
researches are categorized into the frame-based or the shot-based systems. By extracting the 
semantics for the successive frames or the segmented shots, the various kinds of sport events are 
detected. However, the low-degree structuralization of the frame-based or the shot-based systems 
may make the analysis of the sport video more complex and then the retrieval of the video clips 
for a sport event inefficient. Therefore, in this project, the new scene-based sport event detection 
system is proposed. Firstly, the sport video is partitioned into many video shots with the same 
semantics. Secondly, the key frame and several visual features (soil and grass color percentage, 
object number, motion vector, skin detection, player’s location) for each shot are extracted and 
analyzed to identify the semantics and then each shot will be classified into various kinds of 
semantic scenes. Here, there are total eight semantic scenes including pitching scene, running 
scene, base scene, player scene, infield scene, outfield scene, close-up scene, and other scene are 
defined. Finally, the semantic scenes are regarded as the observation symbols in the HMM 
(Hidden Markov Model) event detection system. Four kinds of events including the base hit, 
strikeout, ground outs, and air outs are detected in the proposed system. Experimental results 
show that the proposed system may detect the four kinds of sport events accurately.  
Keywords: Video shot, Video scene, Sport event detection, HMM 
 4
1. Introduction 
With the great demand for fast browsing and searching sport video contents, event-based 
video searching technology is being developed rapidly. In general, event-based searching 
technology is based on scene analysis. The scene analysis technology may be roughly categorized 
into the frame-based [1-6] and the shot-based [7-11] methods.  
In [1], the important events were modeled by the state transition diagram of “play” and 
“non-play”. The “plays” were detected by applying some low-level visual features. Based on the 
state transition model, the video content was summarized. However, the detection of some 
general events was not addressed. In [2], human running behavior in a sport video was detected 
by using a periodic motion descriptor. In [3], replay video segments in sport programs were 
automatically detected by identifying the logo in the scene transitions. Furthermore, baseball 
scenes were also classified by using the maximum entropy method [4] in which image, audio, and 
text features were utilized. The methods in [5-6] also applied audio and text features to develop 
event detection systems. The abovementioned frame-based methods are suitable for analyzing the 
scenes in sport videos but the loose structured video representation makes it difficult for them to 
identify general baseball events, e.g., base hit, strikeout, ground outs, and air outs.  
For the shot-based methods, Ekin et al. [7] applied cinematic and object-based features to 
detect the goal, referee, and penalty-box and then to summarize the soccer videos. Then, the 
soccer videos are analyzed and summarized. Lu et al. [8] proposed a structuring system to 
classify the basketball scenes using LDA and some low-level features. Especially in [9], 
shot-based baseball scene classification is proposed. Seven scenes were detected by using some 
visual features. In this study, this method was improved by considering global motion and 
object-based features for more scene classifications. Based on these classified scenes, HMM was 
applied to identify general baseball events.  
To detect video events, a state machine was often used. In [10], the state diagram of a 
hunting event was generated to detect the hunts in a wildlife video. In [11], the shot-based state 
diagram was also generated for video surveillance. Here, the concept of the state machine was 
also adopted to develop a scene-based baseball event detection system facilitating the searching 
of general baseball events.  
General baseball events, e.g., strikeout and ground outs, are hard to be detected by using the 
frame-based or shot-based methods because a general baseball event is composed of a series of 
video scenes and each scene is further composed of several video shots. Some semantic scenes 
including infield, outfield, player, and pitching may consist of several video shots. Instead of 
developing the shot-based method, scene transition was used to design the state machine and 
detect the baseball events. The scene-based baseball event detection system was developed with 
the following phases. Firstly, the video was segmented into the various video shots using the 
knowledge-based shot-boundary detecting method [15]. Secondly, the key-frame was extracted 
for each video shot and then several visual features (color distribution, motion information) were 
extracted within the key-frames and shots. Furthermore, the moving objects were extracted by 
using the multiresolution and flooding based RSST (MFRSST) video segmentation [12] such that 
the number and relative positions of players were easily acquired. Based on these visual features 
the various kinds of semantic scenes were classified. Finally, the scene-based baseball event 
detecting system was developed by applying the hidden Markov models (HMM). The baseball 
events consisted of base hits, strikeouts, ground outs, and air outs. The block diagram of the 
proposed system is shown in Fig. 1. In addition, the structure of a baseball video is shown in Fig. 
2. A baseball game is composed of many innings and each inning is composed of two half 
innings.  
 The accuracy analyses for the scene classification and event detection were using a large 
amount of video data that consisted of several hours of video frames. Experimental results show 
that the proposed system can identify the four kinds of baseball events accurately. 
 
2. Shot Boundary Detection 
 Generally, a baseball event is composed of a lot of successive scenes and each scene is 
 6
and grass regions, skin color) [9], object-based features (object number, object size, relative 
positions of players), and global motion information. In total, eight semantic scenes were 
classified, namely pitching, running, base, player, infield, outfield, close-up, and other scenes. Fig. 
3 illustrates the flowchart classifying the semantic scenes for the baseball video shots. First, the 
key-frame for each video shot was determined using the median filtering [16]. Second, based on 
the key frame, the scene for each shot was roughly classified according to the area proportion of 
soil and grass regions. Third, global camera motion was used to detect the running scenes. Finally, 
the proposed MFRSST video segmentation method [12] was applied to extract the moving 
objects and then the number and locations of the moving objects (players) were used to classify 
the other semantic scenes. 
 
3.1 Global Motion Estimation 
 The main objective of global motion extraction is to acquire a global motion compensated 
image so that the player regions can be obtained. Here, the method of indirect global motion 
estimation [18] was applied to calculate global motion. The affine parameters of global motion 
were estimated by minimizing the following energy function: 
   ∑
∈
−=
Sx
nnnfit
n
daxdwE 2|);(| ,        (6) 
where S represent the set of pixels used to calculate the global motion, wn is the weighting 
coefficient for the pixel xn, and a=[a0, a1, a2, b0, b1, b2]T is the affine parameter vector. In Eq. (6), 
the motion vector dn is obtained by using a three-step search [18]. By setting ∂Efit/∂a=0, the 
parameter vector a is obtained as:  
    )]([)]([)]([
1
⎟⎠
⎞⎜⎝
⎛⎟⎠
⎞⎜⎝
⎛= ∑∑
∈
−
∈
n
T
Nn
nnn
T
Nn
nn dxAwxAxAwa ,      (7) 
where the matrix [A(xn)] is defined as: 
   ⎥⎦
⎤⎢⎣
⎡=
nn
nn
n yx
yx
xA
1000
0001
)]([ .    
 
3.2 Extraction of Player Information 
 By careful observation, the number and locations of players may be utilized to classify 
the semantic scenes with players. For example, when the color distribution of the soil and grass 
regions is medium, the players’ locations are needed to determine whether it is a pitching scene or 
not. Hence, before detecting player location the object segmentation process is needed to extract 
the player regions. In this study, MFRSST (multiresolution and flooding based recursive shortest 
spanning tree) [12] was used to separate the object regions with the change regions between the 
key frame and its global motion compensated image. Once the objects were separated they were 
counted. MFRSST speeds up the segmentation process and with the same partition quality as 
RSST [22].  
Two image processing methods were adopted to develop the MFRSST image segmentation 
algorithm. The first one was flooding in the watershed algorithm [21]. Instead of merging the two 
vertices of the least weighted link, the neighboring vertices whose link weights closed to the 
lowest link weight were merged concurrently. Such a merging process is similar to a flooding in 
the watershed algorithm that fills up the image valleys to a specified level. The second concept is 
that of multiresolution decomposition. Performing the partitioning in the decimated image can 
significantly reduce the computation cost. The block diagram of MFRSST method is illustrated in 
Fig. 4-(a) and the method is described as follows. 
1. Perform decimation process to obtain low resolution image. 
2. Apply the FRSST [20] method to partition the decimated image into N regions. 
3. Apply fusion process to remove small spark regions. 
4. Interpolate the segmented decimated image according to the following rules: 
If the pixel is located at the region boundary (see Fig. 4-(b)), 
interpolate it with an empty block (2×2) and fill it with the pixels from original image. 
 8
3.6 Classification of Close-up, Base, Running, and Other Scenes 
When the area ratio of soil and grass regions was low or medium, global motion information, 
object size, and skin color were used to classify the close-up, base, running and other scenes. 
Firstly, if the magnitude of the global motion was larger than a specified threshold, then the shot 
was classified as a running scene. Here, the threshold value for the magnitude of global motion |a| 
was set to be 7. Secondly, skin color was used to determine whether it belonged to the other 
scenes or not. If the number of classified skin pixels was less than a predefined threshold Ts (400), 
then the scene would be classified as the other scene. On the other hand, if the number of skin 
pixels was more than Ts, the video shot was allocated to base or close-up scenes. Finally, the 
number of objects was used to differentiate between base and close-up scenes. If the key frame of 
a shot had more than two players in the field, then the scene would be categorized as a base scene. 
The block diagram for classifying the close-up, base, running, and other scenes is shown in Fig. 
8.  
 
4. General Baseball Event Detection 
 The main objective of this study was to develop a scene-based baseball detection system in 
order to detect and retrieve video clips of general baseball events more efficiently. Based on the 
classified semantic scenes that were regarded as the observation symbols in the HMM, a 4-state 
ergodic HMM was applied to detect the general baseball events including the base hit, air out, 
ground out, and strikeout.  
 
4.1 Training Process for Baseball Event Detection 
 Two kinds of hidden Markov models are frequently used for pattern classification or 
recognition applications. Fig. 9-(a) illustrates a left-right model in which the state transition 
proceeds in a left to right direction. Fig. 9-(b) illustrates the ergodic model in which the state 
transition proceeds from a state to all the other states including itself. Since the semantic scenes 
in a baseball video may change irregularly, the ergodic HMM is more suitable for identifying the 
general baseball events.  
 In HMM, vector quantization or K-means are frequently used to generate observation 
symbols by clustering the acquired features. Here, the classified semantic scenes were regarded 
as the observation symbols in the HMM to develop a baseball event detection system. Generally, 
HMM is expressed by a 3-tuple parameters λ=(A, B, π) [19], where π is the initial state 
distribution, A=[aij] is the matrix of state transition probabilities with element aij being the state 
transition probability from state i to state j, and B={bj(k)} the observation symbol probability 
distribution for symbols ok in state j. By training the parameters (A, B, π) for each baseball event, 
the four kinds of baseball events may be detected. 
 
4.2 Baseball Event Detection 
 So far, not much research [24] has addressed the detection of general baseball events; while 
a lot of research has been focused on the scene classification [1-11]. Some semantic scenes 
including infield, outfield, player, and pitching may consist of successive video shots. Instead of 
developing a shot-based method, scene transitions were used to design a state machine and to 
detect the baseball events. Here, four kinds of baseball video events including the base hit, air out, 
ground out, and strikeout were identified by using the HMM method. Based on the detected 
semantic scenes serving as observation symbol sequence O, the parameters (A, B, π) for the 
4-state ergodic HMM were estimated using the Baum-Welch algorithm [19]. Given the 
observation symbol sequence O, the observation probability P(O|λ) for each baseball event could 
be computed via the forward-backward procedure [19]. Let the observation symbol sequence 
with length T be denoted as O=[o1, o2,…, oT], a forward variable αt(i) is defined to calculate the 
probability of partial observing sequence of state i at time t for the model λ denoted as 
P(o1o2…ot,qt=i|λ). P(O|λ) may be calculated as follows. 
1) Initialization: )( 1obπ(i)α iii =     
 10
are improved significantly. 
 
5.3 General Baseball Event Detection 
In this study, four baseball events including the base hit, air out, ground out, and strikeout 
were detected. Based on the classified semantic scenes that served as the observation symbols in 
the HMM, the 4-state ergodic HMM was used to detect the four baseball events. Here, three 
baseball games in the Asia Championship 2003 were used namely the Chinese vs. Korea, Chinese 
vs. Japan, and Chinese vs. China games as the test data to analyze the accuracy of general 
baseball event detection. The HMM was trained with the baseball videos whose length is about 
19 hours 43 minutes. Figures 13-16 show the scene change for the four detected baseball events.  
When a sequence of symbols (classified semantic scenes) were observed by the HMMs of 
the four general baseball event detectors, the observation probability P(O|λ) for each baseball 
event was calculated. When the observation probability P(O|λ) for a certain general baseball 
event was higher than the others and lager than a specified probability threshold, then a general 
baseball event was detected and the event boundary was determined too. The next general 
baseball event is detected continuously. The accuracy analysis of the baseball event detection is 
given in Table 5. The number of mis-detected events denotes the number of events that were not 
detected in the test video clips. The number of false-detected events denotes the number of events 
that were detected incorrectly. It is obviously that the detection rate of the “ground out” event was 
lower than the others because there were similar scene changes in both the “ground out” and 
“base hit” events. The average precision for detecting the four general baseball events was about 
83% and the average recall was about 90%.  
Furthermore, Han’s method [24] was compared with the proposed method. In Han’s work 
several multimedia features including image, audio, and text features were integrated to detect 
several baseball events (home run, outfield hit, outfield out, infield hit, infield out, strike out, and 
walk). The accuracy analysis for Han’s work is shown in the Table 6. The average recall ratio and 
precision for Han’s method were about 71% and 79%, respectively. It is obvious that the 
proposed method for general baseball events outperforms Han’s method. However, the number of 
detected baseball events was lager than with the method proposed in this paper. Hence, in future 
works, the system would be improved to detect more baseball events.   
 
6. Conclusion 
Generally, precise event detection is complicated in video processing research. In this study, 
we tried to use as many visual features as possible to classify the semantic scenes and then 
applied HMM to identify the baseball events. Four kinds of baseball events including the base hit, 
strikeout, ground outs, and air outs were detected with the proposed system. Firstly, the sport 
video was partitioned into many video shots. Secondly, the key frame for each video shot was 
determined and then some visual features in the video shot, e.g., soil and grass color percentage, 
object number, motion vector, skin detection, player location etc., were extracted to classify the 
semantic scenes. Finally, HMM was used to detect the various kinds of baseball events. 
Experimental results show that the proposed system can detect the four kinds of baseball events 
accurately. The average PRECISION for detecting the four general baseball events was 83.25% 
and the average RECALL was 90%.  
 
Acknowledgements 
 This work had been partially funded by Chung Hua University under project no. 
CHU-94-TR-02 and National Science Council under project no. NSC 94-2213-E-216 -023. 
 
Reference 
[1] B. Li, M. I. Sezan, Event detection and summarization in sports video, Proceedings, IEEE 
Workshop on Content-Based Access of Image and Video Libraries 2001 (CBAIVL 2001), 
Dec. 2001, pp. 132 – 138. 
[2] F. Cheng, W. J. Christmas, J. Kittler, Detection and description of human running behavior 
 12
 
t
Shot-boundary 
detection
Discontinuity
z(k,k+l)
computation
)(
boundary
boundary no
),(
Detector       
kTlkkz >
<+
Frame
k
Frame
k+l Key-frame 
detection
Database
Semantic 
scene 
detection
Event 
detection
Indexing
Events ⎪
⎪⎪
⎭
⎪⎪
⎪
⎬
⎫
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
n
m
n
1
1
m
1
1
I.I
...
...
...
I.I
Key-frame
Scenes
 
Fig. 1 Block diagram of the baseball event detection system. 
 
Broadcast Baseball Video
Inning
advertisement
Inning
Half inning Half inning
batte
r
batte
r
Semantic Scenes
 
Fig. 2 The structure of a baseball video. 
 
 14
 (d)
1 21 51 01
6 5 6 711
4 52 311
229 93 011
227 86 511
2
2
23 52 411
1 20
011
201
 S e g m e n te d  lo w  
r e s o lu t io n  im a g e
th e  n e w  n o d e
la b e l  = 1
 th e  n e w  n o d e
la b e l=  2
P ix e ls  in  th e  re g io n  b o u n d a ry  a re
 r e p la c e d  b y  th e  o r ig in a l p ix e ls  L a b e l  1 L a b e l 2
R e c o n s tru c t  th e  s e g m e n te d  lo w  re s o lu tio n  im a g e  a n d  
fo rm  a  n e w  w e ig h te d  l in k s  in  th e  r e g io n  b o u n d a ry  
 
Fig. 4 (a) Block diagram of the MFRSST image segmentation system. (b) Region boundary 
detection. (c) 2-D interpolation process. (d) Each reconstructed region is regarded as a single 
node and the pixels in the region boundary are replaced by the original image pixels. After that, a 
new weighted graph is constructed.  
 
(a)  (b)  
Fig. 5 (a) Objects are segmented by using MFRSST method. (b) Objects are extracted by using 
the labeling method. 
 
 
Fig. 6 Color distributions for the soil and grass regions. 
 
  
 16
 
Fig. 11 (a) The first frame for the shot of a pitching scene. (b) The extracted key frame for this 
shot. (c) The segmented objects obtained by using the MFRSST. (d) The results after the object 
labeling process. 
 
Table 1 Accuracy analysis for the shot boundary detection 
 Video length Number of shots Number of 
detected shots 
Number of 
miss-detected 
shots 
Number of false 
detected shots 
Clip 1 11’39” 68 67 1 0 
Clip 2 6’09” 40 38 2 0 
Clip 3 8’24” 49 49 0 0 
Clip 4 9’”09 34 31 3 0 
Clip 5 13’48” 84 82 5 3 
Clip 6 19’28” 120 116 6 2 
Clip 7 12’10” 90 92 1 3 
Clip 8 20’33” 152 150 7 5 
Total 101’10” 637 625 25 13 
 
(a)  (b)  
(c)  (d)  
 18
Base 34% 42%   94% 91% 
Running 35% 34%   75% 75% 
Other     100% 75% 
Pitching 92% 89% 94% 94% 100% 98% 
Player     71% 71% 
Infield 85% 88% 90% 92% 100% 88% 
Outfield 72% 70% 85% 90% 96% 96% 
 
1 2 3 4 5
6 7 8 9 10
11 12 13
 
Fig. 13 Scene changes for the detected base hit event. 
 
1 2 3 4 5
6 7 8 9 10
11 12  
Fig. 14 Scene changes for the detected air out event. 
 
1 2 3 4 5
6
 
Fig. 15 Scene changes for the detected ground out event. 
 
