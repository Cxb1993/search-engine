 II
 
行政院國家科學委員會專題研究計畫成果報告 
子計劃三: 結合音源定位與姿態辨識之人與機器人強健型感知互動介面研發 
 計畫編號： NSC 96 － 2628 － E － 009 － 163 － MY3  
執行期限：98 年 8 月 1 日至 99 年 7 月 31 日 
主持人： 胡竹生   國立交通大學電機與控制工程學系 
計畫參與人員：張永融、翁偉庭、蕭昀軒、阮崇維、蔡沛錡 
國立交通大學電機工程學系電控工程研究所 
 
摘要 
互動介面是機器人能否實際走入人類生活的關鍵技術之一，而其中所引領出的理論問
題更需要深入探討。而在眾多的感測技術中，一般均認為影像與聲音是人與機器人最直接
的互動介面，雖然這方面的研究已經進行多年，在實際應用上仍然面臨彈性與可靠度不足
的問題。本子計劃基於以往視覺物件追蹤的研究成果，利用空間與顏色特徵的平均移動演
算法，發展能在一般環境中偵測物件大小並追蹤其方位的技術；此外，本計劃提出一個消
除穩態或是非穩態干擾聲源的語音強化之方法。這兩項技術將有助於機器人領域發展更有
效、更可靠的互動介面。 
 
關鍵詞： 人機介面, 平均移動演算法, 物件追蹤, 零波束形成 , 相對轉移函數, 非穩態
干擾聲源 
 
Abstract 
Among various sensing technologies, it is commonly recognized that image and voice are 
still the most straightforward interface when considering the human-robot interaction. Despite 
many years of research in these two interfaces, the flexibility and reliability of using voice and 
image still remains important issues for real practice. The associated theoretical problems need to 
be studied with a greater depth. This proposal plans to study the flexibility problem based on 
previous research results. Specifically, this project develops a technology for the object tracking 
with scale and orientation estimation based on spatial-color mean-shift to increase the flexibility 
of the system. Furthermore, a speech enhancement method for stationary and non-stationary 
interfering sources is established. These two technologies are helpful for a robot to recognize the 
statuses of users. 
 
Keywords: Human-machine Interface, Mean-shift, Object tracking, Nullforming, Relative 
Transfer Function Ratio, Nonstationary interfering source 
 1
行政院國家科學委員會專題研究計畫成果報告 
子計劃三: 結合音源定位與姿態辨識之人與機器人強健型感知互動介面研發 
 計畫編號： NSC 96 － 2628 － E － 009 － 163 － MY3  
執行期限：98 年 8 月 1 日至 99 年 7 月 31 日 
主持人： 胡竹生   國立交通大學電機與控制工程學系 
計畫參與人員：張永融、翁偉庭、蕭昀軒、阮崇維、蔡沛錡 
國立交通大學電機與控制工程學系 
 
Abstract(摘要) 
Among various sensing technologies, it is 
commonly recognized that image and voice are 
still the most straightforward interface when 
considering the human-robot interaction. Despite 
many years of research in these two interfaces, 
the flexibility and reliability of using voice and 
image still remains important issues for real 
practice. The associated theoretical problems 
need to be studied with a greater depth. This 
proposal plans to study the flexibility problem 
based on previous research results. Specifically, 
this project develops a technology for the object 
tracking with scale and orientation estimation 
based on spatial-color mean-shift to increase the 
flexibility of the system. Furthermore, a speech 
enhancement method for stationary and 
non-stationary interfering sources is established. 
These two technologies are helpful for a robot to 
recognize the statuses of users. 
 
Keywords: Human-machine Interface, Mean-shift, Object 
tracking, Nullforming, Relative Transfer Function Ratio, 
Nonstationary interfering source 
 
 
Introduction (前言) 
In visual tracking, object representation is an 
important issue because it can describe the 
correlation between the appearance and the state 
of the object. An appropriate object 
representation makes the target model more 
distinguishable from the background, and 
achieves a better tracking result. Comaniciu et al. 
[6] used the spatial kernels weighted by a radially 
symmetric normalized distance from the object 
center to represent blob objects. This 
representation makes mean-shift tracking more 
efficient. Radially symmetric kernel preserves 
representation of the distance of a pixel from the 
center even the object has a large set of 
transformations, but this approach only contains 
the color information of the target and the spatial 
information is discarded. Parameswaran et al. [9] 
proposed the tunable kernels for tracking, which 
simultaneously encodes appearance and 
geometry that enable the use of mean-shift 
iterations. A method was presented to modulate 
the feature histogram of the target that uses a set 
of spatial kernels with different bandwidths to 
encode the spatial information. Under certain 
conditions, this approach can solve the problem 
of similar color distribution blocks with different 
spatial configurations. 
Another problem in the visual tracking is 
how to track the scale of object. In the work by 
Comaniciu et al. [6], the mean-shift algorithm is 
run several times, and for each different window 
size, the similarity measure Bhattacharyya 
coefficient is computed for comparison. The 
window size yielding the largest Bhattacharyya 
coefficient, i.e. the most similar distribution, is 
chosen as the updated scale. Parameswaran et al. 
[9], Birchfield and Rangarajan [2] and Porikli 
and Tuzel [10] use the similar variation method 
to solve the scale problem. But this method is not 
always stable, and easily makes the tracker lose 
the target.   Collins [5] extended the mean-shift 
tracker by adapting Lindeberg’s theory 
(Lindeberg [8]) of feature scale selection based 
on local maxima of differential scale-space 
filters. It uses blob tracking and a scale kernel to 
accurately capture the target’s variation in scale. 
But the detailed iteration method was not 
described in the paper. An EM-like algorithm 
(Zivkovic and Krose [15]) was proposed to 
estimate simultaneously the position of the local 
mode and used the covariance matrix to describe 
the approximate shape of the object. However, 
implementation details such as deciding the scale 
size from the covariance matrix were not given. 
Other attempts were made to study different 
 3
Let the image of interest have M pixels and the 
associated color space can be classified into B 
bins. For example, in RGB color space, if each 
color is divided into 8 intervals, the total number 
of bins is 512. The image can be described as 
1,...{ , , }i ix i i MI b == x xx c  where ix  is the location 
of pixel i with color feature vector 
ix
c  which 
belongs to the 
i
bx -th bin. The color feature 
vector has the dimension d which is the color 
channels for the pixel (for example, in RGB 
color space, d=3 and ( , , )
i i i i
R G B=x x x xc ). To 
keep the robustness of color description of the 
spatiogram, we extend the spatiogram and define 
a new joint spatial-color model of the image Ix 
as, 
, , , ,( ) , , , , ,  1,...,xI b P b P b C b C bh b n b B= 〈 〉 =μ μ∑ ∑     (1) 
where bn , ,P bμ , and ,P b∑ are the same as the 
spatiogram proposed by Birchfield and Rangarajan (2005). 
Namely, bn  is the number of pixels, ,P bμ  the mean 
vector of pixel locations, and ,P b∑  the covariance matrix 
of pixel locations belonging to the b-th bin. In (1), we add 
two additional elements. ,C bμ  is the mean vector of the 
color feature vectors and ,C b∑  is the associated 
covariance matrix.  
1.2 SPATIAL-COLOR MEAN-SHIFT OBJECT 
TRACKING ALGORITHM 
Using the spatial-color feature and the concept 
of expectation (Yang et al., 2005), two different 
tracking algorithms are proposed as the 
following. 
A. Spatial-Color Mean-Shift Tracking Algorithm 
(Tracker 1) 
The p.d.f. of the selected pixel ( , , )bx xx c  in 
the image model ( )
xI
h b  (see (1)) can be 
estimated using kernel density function. 
, , , ,
1
1( , , ) ( , ) ( , ) ( )
B
P P b P b C C b C b
b
p b K K b b
B
δ
=
= − − −∑x x x xx c x μ c μ∑ ∑  
                                     (2) 
where PK  and CK  are multivariate Gaussian 
kernel functions and can be regarded as the 
spatially weighted and color-feature weighted 
function respectively. It is also possible to use a 
smooth kernel such as Gaussian (Yang, 2005). 
Using the concept of the expectation of the 
estimated kernel density, we can define a new 
similarity measure function between the model 
( )
xI
h b  and candidate 1,...,{ , , }j jy j j NI b == y yy c  as 
1
1( , ) ( ) ( , , )
j j
N
x y j
j
J I I J p b
N =
= = ∑ y yy y c     
, , , ,
1 1
1 ( , ) ( , ) ( )
j j
N B
P j P b P b C C b C b
j b
K K b b
NB
δ
= =
= − − −∑∑ y yy μ c μ∑ ∑  
          (3) 
The spatial-color model in (2) under measures 
like (3) might be sensitive to small spatial 
changes. This problem was discussed by 
O'Conaire et al (2007) and Birchfield and 
Rangarajan (2007). However, this model also 
gives advantages of orientation estimation. As 
shown in Fig. 2, if there is no deformation 
between candidate and target, and the distance of 
motion is not excessively large between two 
adjacent frames, we can consider the motion of 
object of two frames as a pure translation. Under 
these assumptions, the center of target model 0x  
is proportional to the center of candidate y  in 
the candidate image. As a result, we can 
normalize the pixels location and then obtain the 
new similarity measure function as the 
following. 
, 0 ,
1 1
, ,
1( ) ( ( ), )
                          ( , ) ( )
j j
N B
P j P b P b
j b
C C b C b
J K
NB
K b bδ
= =
= − − −
⋅ − −
∑∑
y y
y y y μ x
c μ
∑
∑
     (4)
 
,P bμ
0x
,P bμ
y
 
Fig. 2.  Illustration of pure translation. 
The best candidate for matching can be found by 
computing the maximum value of the similarity 
measure. Let the gradient of the similarity 
function with respect to the vector y equal to 0, 
i.e., ( )J∇ =y 0 , we can obtain the new position 
ynew of the target to be tracked, 
( )J∇ =y 0  
1
, , 0
1 1
1 ( ) ( ) ( )=
j
N B
P b j P b P C
j b
K K b b
NB
δ−
= =
⇒ − − + −∑∑ yy y μ x 0∑  
 5
1 2(2 2)(1 ) (1- ),  1
( ) 2
0,
x x if x
k x
otherwise
π π
⎧ + − = <⎪= ⎨⎪⎩
  (15) 
Assigning ( ) ( )G x k x= , the derivative of G 
becomes a constant as, 
2'( ) '( )G x k x π= = −                    (16) 
The result is simple and easy to compute. Finally, 
by substituting (16) into (13), we can obtain the 
second similarity-based mean-shift algorithm as 
follows. 
1
1
( )
( )
j
j
N
j
j
new N
j
p b
p b
=
=
=
∑
∑
y
y
y
y                     (17) 
C. Tracking Algorithm with Reduced Complexity 
(Tracker 2) 
Like most of the tracking algorithms, the 
proposed method may arrive at an incorrect 
result if the background contains similar 
information in foreground. The problem becomes 
more serious if the scale and orientation of the 
target have to be followed. One way to reduce 
the influence of background is to apply a 
weighting function to the image surrounding the 
target. The combination of the weighting 
function with the spatial-color mean-shift 
tracking algorithms proposed before is explained 
below. 
Let ,F bN  be the normalized histogram of the 
foreground of the b-th bin ( , 1F bb N =∑ ), and 
,O bN  the normalized histogram of the 
background of the b-th bin ( , 1O bb N =∑ ). The 
histogram of background is computed in the 
region around the foreground (target).The size is 
equal to two times the target size and the area is 
equal to three times the target area. Define the 
background influence factor of the b-th bin as 
,
,
F b
O b
N
N
 for , 0O bN ≠ . The maximum value of the 
factor for all bins are defined as 
,
,
1~ ,
,0
max( )
O b
F b
b B
O bN
N
N
β
= ≠
= . 
When β << 1, certain bins in background contain 
more related features than the corresponding 
foreground bins. This should results in a small 
background weighting factor for those bins. Note 
that we exclude the cases when NO,b = 0 in 
computing β. Therefore, we should also make 
the background weighting factor small for the 
cases when 1β ≈ . Based on the analysis, the 
background weighting factor can be defined as, 
0 ,
,
,
, ,
11 ,   0
1,  0    0
0,
F b
O b
O b
b F b O b
N
e if N
N
W if N and N
otherwise
β
β
β
−⎧⎛ ⎞− ≠⎪⎜ ⎟⎜ ⎟⎪⎝ ⎠⎪= ≠ =⎨⎪⎪⎪⎩
 (18) 
where β0 is a constant. Note that when 
, ,0    0F b O bN and N≠ = , the background has no 
influence to the foreground at the b-th bin. 
Therefore, it is given the largest weighting in 
(18). The weighted-background information is 
added into the mean-shift tracking algorithms 
developed in (6) and (17), and the algorithms is 
derived again as 
1
1
,
1 1
1
, , 0
1 1
( ) [ ]
( ) ( ) [ ] +
jj
jj
N B
new P b b P C
j b
N B
P b j P b b P C
j b
W K K b b
W K K b b
δ
δ
−
−
= =
−
= =
⎧ ⎫= − ×⎨ ⎬⎩ ⎭
⎧ ⎫− −⎨ ⎬⎩ ⎭
∑∑
∑∑
y
y
y
y
y
y μ x
∑
∑
(19) 
and 
j
1
N
j=1
( )
=
( )
jj
jj
N
b
j
new
b
W p b
W p b
=
∑
∑
y
y
y
y
y
y                    (20) 
D. Update of Scale and Orientation 
The characteristic values of the covariance 
matrix of the spatial-color distribution can be 
utilized to represent the scale and orientation of 
the target. A similar idea was proposed in the 
algorithm called CAMSHIFT (Bradski, 1998). 
From the experimental results shown later, this 
simple calculation provides a fairly robust scale 
and orientation tracking performance which 
greatly enhances the capability of mean-shift 
algorithm.  
We define several new elements as follows. 
Tμ  is the total mean vector of the locations of 
all pixels in the target, 'W∑  is the within-class 
covariance matrix of the B bins by adding the 
background weighting. 'B∑  is the between-class 
covariance matrix of the B bins by adding the 
background weighting. 'T∑  is the total 
 7
 
Figure 3 :  Distance error of face sequence. (*note: we only consider the distance error which is smaller than 50 pixels)
are 59×82, 50×65, and 27×98, respectively. In 
the following figures, tracker 1 means the 
algorithm of (19) and its extension while tracker 
2 the algorithm of (20) and its extension. To 
compute the tracking error, the ground-truth of 
the object location is marked visually in every 10 
frames and the error is determined by Euclidean 
distance. 
A. Spatial-Color Mean-Shift Trackers with 
Single Scale Tracking 
 
 
Figure 4 : Face sequence of frames 33, 93, 117, 126, 183, 
256, 271, 455. (red: tracker 1, blue: tracker 2, green: 
traditional mean-shift tracker) 
 
Part of the frames in the face tracking 
experiment is shown in Figure 3. Figure 4 shows 
the tracking error. For comparison purpose, 
traditional mean-shift algorithm is also 
implemented (marked as tMS in the figures). 
This example shows under simple background, 
all these methods have similar performance but 
on the average, the proposed methods 
outperform the traditional one.  
Similarly, tracking of a cup with complex 
feature in a complex background are shown in 
Figure 5 and 6. As shown in Figure 6, traditional 
mean-shift algorithm has a larger tracking error 
and sometimes loses the target. 
 
 
Figure 5 :  Cup sequence of frames 4, 45, 63, 69, 81, 105, 
166, 243. (red: tracker 1, blue: tracker 2, green: traditional 
mean-shift tracker) 
 9
 
 
Figure 9 :  Walking person tracking results of 
spatial-color mean-shift tracker2 with PCA scale method 
(frames 83, 358, 494, 598, 689, 733, 914, and 1000) 
 
 
 
Figure 10 :  Surveillance tracking results of spatial-color 
mean-shift tracker1 with PCA scale method (frames 3, 24, 
32, 51, 59, 286, 318, and 353) 
 
 
Figure 11 :  Surveillance tracking results of spatial-color 
mean-shift tracker2 with PCA scale method (frames 3, 24, 
32, 51, 59, 286, 318, and 353) 
 
C. Spatial-Color Mean-Shift Trackers with 
Scale and Orientation 
The performance of the proposed algorithms 
is analyzed in two different aspects: the 
preprocessing time of building the model and the 
computational time for each iterative step. The 
face sequence and cup sequence are used to test 
the performance of the proposed trackers. The 
models are built from the first image of these two 
sequences, and the preprocessing procedure is 
executed five times to obtain the average 
computing time. Table 1 shows the 
preprocessing time of both trackers.  
Table 1: The preprocessing time of both trackers (in 
second) 
 Tracker 1 Tracker 2 
Face sequence 0.027858 0.031299 
Cup sequence 0.017306 0.022448 
 
 11
Figure 12 and Figure 13 show the iteration time 
of the first 200 frames of face sequence and cup 
sequence for tracker 1. The average time of total 
frames (about 2300 frames) is 0.035855 second 
(about 28 frames/sec). The average time of an 
iteration of total frames (about 1900 frames) of 
cup sequence is 0.017854 (about 56 frames/sec). 
Figure 14 and Figure 15 show the results for 
tracker 2. The average time of total frames is 
0.020670 second (about 48 frames/sec). The 
average time of an iteration of total frames of cup 
sequence is 0.006608 (about 151 frames/sec). 
The tracking time of tracker 2 is smaller than that 
of tracker 1 because tracker 2 computes PK  and 
CK  at the preprocessing stage instead of at each 
iteration. Nevertheless, both trackers satisfy the 
real-time requirement of current image sampling 
rate for most cameras (30 frames per second). 
 
2. Multichannel Speech Enhancement Using 
Relative Transfer Function Based 
Nullforming  
This work focuses on eliminating multiple 
directional nonstationary signals using 
nullformer with adaptive filter. In comparison 
with beamformer, nullformer makes a null space 
to the interfering signal which could be used to 
eliminate the interfering sources. The scope of 
this work can be divided into two parts: 1. 
applying nullforming to the adaptive filter, 2. 
adaptive nullforming technique. The fixed 
nullformer constructs the null space to the 
interfering sources before executing the adaptive 
filter for target speech enhancement. In this case, 
the interfering sources are assumed unchanged 
during the adaptation. The adaptive nullformer 
updates the nullspace in a period to trace the 
change of interfering sources and corresponding 
nullspace. Nullformer is applied to generalized 
sidelobe canceler (GSC). The FBF and BM 
should be modified to satisfy the architecture 
when the nullformer is applied to GSC. For the 
fixed nullformer, The RTFs of interfering 
sources are used to find the null space by using 
singular value decomposition (SVD). For the 
adaptive nullformer, the RTFs from different 
directions are estimated before executing the 
enhancement procedure. These RTFs are used to 
find the subspace distance between previously 
known RTFs and estimated signal subspace in 
real-time. Therefore the existence of desired 
source in each frequency can be found and 
processed accordingly.  
The proposed adaptive nullformer on GSC is 
implemented and the experiment compares the 
performance between the proposed speech 
enhancement and conventional adaptive 
beamformer. 
2.1 PROBLEM FORMULATION 
  For the case with two or more interfering 
sources, the TFs of desired source and 
interfering sources are independent. Therefore 
the received signal in frequency domain with one 
desired source and N interfering sources from 
different directions can be formulated as 
1
( , ) ( ) ( , ) ( ) ( , ) ( , )
( ) ( , ) ( )
N
D D I I
i i
i
k s k s k k
k
ω ω ω ω ω ω
ω ω ω
=
= + +
= +
∑a
A
a
s n
x n
(28) 
where 
[ ]1 2( , ) ( , ) ( , ) ( , ) TMk n k n k n kω ω ω ω=n L  
1( ) ( ) ( ) ( )
D I I
Nω ω ω ω⎡ ⎤= ⎣ ⎦A a a aL
 
1( , ) ( , ) ( , ) ( , )
TD I I
Nk s k s k s kω ω ω ω⎡ ⎤= ⎣ ⎦s L  
2     1,( ) 1 ( ( ) ,)
I I TI
i Mi ia a i Nω ω ω⎡= ⎣ =⎤⎦a L K
 is the vector form of TFs between interfering 
sources and microphone array and ( , )Iis k ω  is the 
ith interfering source. 
2.2 MULTICHANNEL SPEECH 
ENHANCEMENT USING RELATIVE 
TRANSFER FUNCTION BASED 
NULLFORMING 
A. Nullforming 
The singular value decomposition (SVD) 
can be used to find the null space of the entire 
interfering signal. Assume there are N interfering 
sources in the environment and we have the 
RTFs of interfering sources, which are 
2
1
( ) 1 ( ) (
( )
    1, ,
( )
)
TI I
i iM
IT
i
I
i
I
i h h
i N
a
ω ω ω
ω
ω
⎡ ⎤= ⎣
= =
⎦h
a
L
K       (29) 
and take the complex conjugate of these RTFs in 
matrix form 
 13
1
1
†
1
1
( )
( ) (( )
( ) )
)
(
D
FN
Daωω ωω ω ω
∗
δ =a U
μ
ν
         (41) 
Therefore, the new FBF would be obtained 
( ) ( ) ( )FNFB nNF ω ω ω=w wU                (42) 
The FBF would block the interfering signal and 
make a beam to the desired signal. Let 
[ ]2 2 1( ) ( ) ( ) ( )n M Nω ω ω ω− −=P Lμ μ μ .  
The blocking matrix would be 
( ) ( ) ( )NBM FN nω ω ω=P PU                 (43) 
where 
[ ]1 2 1( , ) ( , ) ( , ) ( , )M Nk u k u k u kω ω ω ω− −=u L  
Therefore, the BM can block the desired 
signal and interfering signal and obtain the 
stationary noise. The ANC can be used to 
eliminate the residual stationary noise. 
C. Variable Nullforming 
An excessive number of reference 
microphones are not needed when there are not 
many sources in action. It’s difficult to know the 
number of active sources in each time so the 
signal subspace is estimated by order recursive 
least square estimation (ORLS). The method 
works by increasing the number of reference 
microphones when number of emitting sources 
grows. ORLS was originally used in line-fitting 
by increasing the order [30]. The linear equation 
( ) ( ) ( ) ( ), , , ,mmm i ik k k kω ω ω ω+=m eM θ  
where 
( )
( ) ( ) ( )
1 1 1
(1) (2) ( )
,
Φ , Φ , Φ ,
m m m
K
x x x x x x
m
T
k
k k k
ω
ω ω ω⎡ ⎤= ⎣ ⎦
m
L , 
( )
( ) ( ) ( )1 2 1
,
, ,1 ,
i
T
k
k
k k k
ω
ω ω ω−= ⎡ ⎤⎣ ⎦
M
m m mL , 
( )
( ) ( ) ( ) ( )
1
1 2 1
,
Φ , , , ,
m
i
u x m m
m
i
T
m
k
k h k h k h k
ω
ω ω ω ω−⎡ ⎤= ⎣ ⎦L
θ
,  
and  
 
( )
( ) ( ) ( )(1) (2) ( )
,
, , ,
m
TK
m m m
k
k k k
ω
ω ω ω⎡ ⎤= ⎣ ⎦
e
L覈 ٛ  
( ),m k ωm is PSD from the mth microphone, 
( ),i k ωM  are the PSDs from the first to the 
(k-1)th microphone and mkθ  is the mth entry of 
signal subspace under kth order. ( ),m k ωe  is the 
estimation error. For k=1, The first entry of the 
estimator is ( )
11
,=Φ
m
m
u x k ωθ . 
In each order, estimate the parameters by using 
LS estimation then the estimator would be 
( )
( ) ( ) ( ) ( )1† †
ˆ
1, , 1    ,
,
,
,
, ,
.
,
m
i
i i i m
k
k k k
i i M
k
N m
ω
ω ω ω ω−⎡ ⎤= ⎣ ⎦
= + =
M M M m
K K
θ
    (44) 
To simplify the representation, the following 
derivation excludes the representation of 
parameters- ( ),k ω . The previous equation can be 
reformulated to recursive form by [30]. The 
estimator ˆ miθ  updated in the (i+1)th order is 
( ) 1† † †1 1
†
1 1
1
†
1
†
1 1
ˆ
ˆ
i i i i i i mm
i
m i i i
i
i i m
i i i
− ⊥
+ +
⊥
+ ++ ⊥
+
⊥
+ +
⎡ ⎤⎢ ⎥−⎢ ⎥= ⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
M M M m m P m
m P m
m P m
m P m
θ
θ
       (45) 
where ( ) 1† †i i i i i−⊥ = −P I M M M M  is the projection 
matrix onto the subspace being orthogonal to 
that spanned by the columns of iM . The data 
under (i+1)th order is [ ]1 1i i i+ +=M M m . To 
enhance the efficiency, the inverse operation is 
formulated to 
† † †
1 1 1
† †
1 1 1 1
1 †
1
† †
1 1 1 1
1
i i i i i i i i i
i
i i i i i i
i
i i i
i i i i i i
+ + +
⊥ ⊥
+ + + +
+
+
⊥ ⊥
+ + + +
⎡ ⎤+ −⎢ ⎥⎢ ⎥= ⎢ ⎥⎢ ⎥−⎢ ⎥⎣ ⎦
TM m m M T TM m
T
m P m m P m
T
m M T
m P m m P m
   (46) 
where ( ) 1†i i i −=T M M  and †i i i i⊥ = −P I M TM  is the 
projection matrix. The least square error of mth 
microphone under ith order is 
( ) ( )†, ˆ ˆ     , ,m i m mLS m i i m i iJ m i M= − − =m M m M Kθ θ   (47) 
the least square error could be updated by using 
the recursive form 
( )2† 1 1, 1 ,
†
1 1
i i im i m i
LS LS
i i i
J J
⊥
+ ++
⊥
+ +
= − m P m
m P m
. 
Figure 16 shows that the least square error 
shrinks gradually as the order of recursion 
increases. The recursion should be stopped when 
the error is less than a threshold. 
Stop ORLS when ,m i thresholdLS LSJ J<  
 15
The performance of noise reduction and 
distortion is a trade-off of a beamformer. The 
better noise reduction performance may cause 
more distortion. Therefore, another quality 
measure for evaluating the performance of 
distortion is log-spectral distortion, which is 
defined as 
( ) ( )
1
2
21 2
10 10
0 02
1 1 ˆ log , log ,
1
KL
K
l k
LSD S k l S k l
L
−
= =
⎧ ⎫⎪ ⎪⎡ ⎤= Ψ − Ψ⎨ ⎬⎣ ⎦+⎪ ⎪⎩ ⎭∑ ∑
where ( ) ( ){ }2, max , ,S k l S k l δΨ ≡ is used to confine 
the log-spectrum dynamic range about 50 dB, 
i.e., ( ){ }250 10 ,10 max ,k l S k lδ −= . 
The LSD compares the original desired signal 
with the enhanced signal. LSD of different 
methods are evaluated by using the original 
desired source recorded by microphone one and 
the enhanced signal by each method. The 
condition C4 and C5 are noise-only cases so the 
first three conditions are compared. 
Table 3: Log spectrum distortion of different methods 
  C1 C2 C3 
Channel1  0.3347 0.6301 0.7425 
RSAB  0.4328 0.5522 0.5323 
GSC  0.3782 0.5938 0.5781 
NRSAB  0.3994 0.5173 0.5193 
NGSC1  0.4019 0.5761 0.5780 
NGSC2  0.4697 0.5965 0.5524 
VNGSC1  0.4141 0.5731 0.5934 
 
References(參考文獻) 
[1] Adam, Amit, Rivlin, Ehud and Shimshoni, Ilan, 2006, 
Robust Fragments-based Tracking using the Integral 
Histogram, Proceedings of the 2006 IEEE Computer 
Society Conference on Computer Vision and Pattern 
Recognition, Vol. 1, pp. 798 – 805. 
[2] Birchfield, S. and Rangarajan, S., 2005, Spatiograms 
versus histograms for region-based tracking, Proc. IEEE 
Conf. on Computer Vision and Pattern Recognition 
(CVPR’05), vol. 2, pp. 1158-1163. 
[3] Birchfield, S. and Rangarajan, S., 2007, "Spatial 
Histograms for Region-Based Tracking,"  ETRI 
Journal, vol.29, no.5, Oct. 2007, pp.697-699. 
[4] Bradski, G.R., 1998, Computer vision face tracking for 
use in a perceptual user interface, Intel. Technology 
Journal 2 (2), (1998) 1–15. 
[5] Collins, R., 2003, Mean-shift blob tracking through 
scale space, Proc. IEEE Conf. on Computer Vision and 
Pattern Recognition (CVPR '03), Vol. 2, p. 234. 
[6] Comaniciu, D., Ramesh, V., and Meer, P., 2003, 
Kernel-based object tracking, IEEE Trans. On Pattern 
Analysis and Machine Intelligence, 25(5):564-577, 
May. 
[7] Hastie, T., Tibshirani, R., and Friedman, J., 2001, The 
Elements of Statistical Learning, 1st ed. Springer. 
[8] Lindeberg, T., 1998, Feature detection with automatic 
scale selection, International Journal of Computer 
Vision, vol. 30(2), pp.79-116, November. 
[9] Parameswaran, V., Ramesh, V., and Zoghlami, I., 2006, 
Tunable kernels for tracking, Proceedings of the 2006 
IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition, Vol. 2, pp. 2179-2186. 
[10] Porikli, F. and Tuzel, O., 2005, Object tracking in 
low-frame-rate video, Proc. SPIE. Vol. SPIE-5685, pp. 
72-79. O'Conaire, C., O'Connor, N.E., and Smeaton, 
A.F., 2007, "An Improved Spatiogram Similarity 
Measure for Robust Object Localization," Proc. 
ICASSP, 2007. 
[11] Yang, C., Duraiswami, R., and Davis, L., 2005, 
Efficient mean-shift tracking via a new similarity 
measure, IEEE Conf. on Computer Vision and Pattern 
Recognition, vol. 1, pp. 176-183. 
[12] Zhang, H., Huang, Z., Huang, W., and Li, L., 2004, 
Kernel-based method for tracking objects with rotation 
and translation, Proc. the 17th International Conference 
on Pattern Recognition, (ICPR'04), vol 2, pp. 728-731. 
[13] Zhang, H., Huang, W., Huang, Z., and Li, L., 2005, 
Affine object tracking with kernel-based spatial-color 
representation, IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition (CVPR’05), 
20-25 June, vol. 1,  pp. 293- 300. 
[14] Zhao, Q. and Tao, H., 2005, Object tracking using 
color correlogram, 2nd Joint IEEE International 
Workshop on Visual Surveillance and Performance 
Evaluation of Tracking and Surveillance, 15-16 Oct., 
pp. 263- 270. 
[15] Zivkovic, Z. and Krose, B., 2004, An EM-like 
algorithm for color-histogram-based object tracking, 
Proc. IEEE Conf. on Computer Vision and Pattern 
Recognition (CVPR’04), vol. 1, pp. 798-803. 
[16] Cox, H., R. Zeskind, and M. Owen, Robust adaptive 
beamforming. IEEE Transactions on Acoustics, Speech 
and Signal Processing, 1987. 35(10): p. 1365-1376. 
[17] Dahl, M. and I. Claesson, Acoustic noise and echo 
cancelling with microphone array. Vehicular 
Technology, IEEE Transactions on, 1999. 48(5): p. 
1518-1526. 
[18] Hoshuyama, O., A. Sugiyama, and A. Hirano, A 
robust adaptive beamformer for microphone arrays 
with a blocking matrix using constrained adaptive 
filters. Signal Processing, IEEE Transactions on, 1999. 
47(10): p. 2677-2684. 
[19] Gannot, S., D. Burshtein, and E. Weinstein, Signal 
enhancement using beamforming and nonstationarity 
with applications to speech. Signal Processing, IEEE 
Transactions on, 2001. 49(8): p. 1614-1626. 
[20] Lo, Y., S. Lee, and Q. Lee, Optimization of 
directivity and signal-to-noise ratio of an arbitrary 
antenna array. Proceedings of the IEEE, 1966. 54(8): p. 
1033-1045. 
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期：2010 年 7 月 2 日 
一、參加會議經過 
2010 International Conference on Green Circuits and Systems 是上海交通大學和二個
IEEE CAS 社會的技術委员會、即數位訊號處理技術委员會和生物醫學的電路和系統
技術委员會共同贊助的一個國際論壇。這會議為學生、工程師、科學家、和教育學
者，交換消耗較少力量、較少材料，較不浪費资源和環境更加友好的電路最新的研
究結果、智慧和系統。本會議有二十七個演講會議和一個海報會議。職此次擔任 special 
session 主席，session 的主題為: Advances in Noise Reduction Techniques，議程時間為
June 21, 2010, 15:30-17:30，含職發表之論文共有六篇論文: 
(1) Active Noise Control: Open Problems and Challenges 
Sen M. Kuo, Northern Illinois University, Kevin Kuo, Northern Illinois University, 
Woon Seng Gan, Nanyang Technological University 
(2) Active Noise Control for Motorcycle Helmets 
計畫編號 NSC96－2628－E－009－163－MY3 
計畫名稱 智慧型行動輔助居家照護機器人研發-子計畫三：結合音源定位與姿態辨識之人與機器人強健型感知互動介面研發(3/3) 
出國人員
姓名 胡竹生 
服務機構
及職稱 國立交通大學電機工程系 
會議時間 99年 6月 21日至 99 年 6 月 23 日 會議地點 中國大陸上海市 
會議名稱 
(中文) 2010 International Conference on Green Circuits and Systems 
(英文) 2010 年國際綠能電子與系統會議 
發表論文
題目 
(中文) 使用雙麥克風之空間預處理之目標與干擾訊號比例為權重之
Wiener 濾波器 
(英文)  Spatially Pre-processed Target-to-Jammer Ratio Weighted 
Wiener Filter Using Two Microphones 
 3
本次大會安排了三個 keynote speech，分別為: 
Keynote I: All Emi Reduction Methods Based On Nonlinear Dynamics, Monday, June 21, 
9:20 – 10:10, Professor Gianluca Setti, University of Ferrara, Italy, President, 
IEEE Circuits and System Society 
Keynote II: Distributed On-Demand Sensing And Estimation (Dose), Tuesday, June 22, 
8:30 – 9:20, Professor Yih-Fang Huang, University of Notre Dame, USA 
Keynote III: Green Wireless Sensors Networks: An Integrated Design Paradigm, Tuesday, 
June 22, 9:20 – 10:10, Professor Magdy A. Bayoumi, University of Louisiana at 
Lafayette, USA 
 
二、與會心得 
 本次論文研討得到兩個有趣的研究方向訊息，一是北京大學深圳分院的邹月娴教
授與英國 University of Sheffield 的劉偉教授所發表的 Frequency Invariant 
Beamforming: 
Abstract—Speech enhancement is one of the key and challenging techniques for a 
real-time speech recording system working in a meeting room, where cancellation of 
the competing speech and improvement of the speech intelligibility are two major 
concerns. In this paper, the application of frequency invariant beamforming to the 
speech enhancement problem based on the generalized sidelobe cancellation (GSC) 
structure is studied. It leads to a novel hybrid broadband speech enhancement 
technique, which combines the ideas of subband processing, frequency invariant 
beamforming and voice activity detection under the GSC framework. Simulation 
results show that the proposed method outperforms two previously proposed ones in 
terms of speech distortion, background noise and competing speech reduction. 
以及陳成就教授的 QRD Recursive Least M-estimate Algorithm: 
Abstract—This paper proposes a new regularized QR decomposition based recursive 
least M-estimate (R-QRRLM) adaptive filtering algorithm and studies its mean and 
mean square convergence performance and application to acoustic echo cancellation 
(AEC). The proposed algorithm extends the conventional RLM algorithm by imposing 
a weighted L2 regularization term on the coefficients to reduce the variance of the 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數   
