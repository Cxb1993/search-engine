中文關鍵詞： 立體視訊、多視域視訊、自由視角視訊、視訊壓縮、三維動
態估測、漸進式三維網格、任意視角影像合成、錯誤隱藏 
英 文 摘 要 ： To achieve free-viewpoint presentation, the video 
data amount is increased huge. Transmission of these 
huge data via network is unavoidable. However, the 
networks are naturally subject to noise interference, 
causing that the transmitted video data will incur 
errors. Hence, how to transmit 3D video data 
correctly becomes an important issue. 
We use three years to do a complete research on 3D 
video encoding, decoding, and error concealment. In 
multi-view video transmission, we used 3D mesh to 
describe dynamic 3D video. To achieve error 
resilience, we partition the 3D mesh data into two 
groups, each has different importance and requires 
different level of channel encoding. In stereo video 
transmission, we discuss the color+depth format and 
explore error concealment when incurring channel 
errors during transmission. 
In the first year, we focus on the progressive 
decomposition of a static model. We also investigate 
the compression of dynamic inconsistent 3D mesh to 
see how it can be combined with progressive 3D model.
In the second year, in addition to focusing on the 
coding of progressive and dynamically inconsistent 3D 
meshes, we discuss how to compress the multi-view 
color video that are captured at the transmitter side 
so that arbitrary views can be synthesized at the 
receiver side. We also investigate how the receiver 
can use the decoded 3D mesh and multi-view color 
information for free viewpoint image synthesis. In 
general, the 3D mesh model contains only the 
geometrical information of objects. Hence, by 
rendering the multi-view texture information on the 
reconstructed 3D mesh model, any arbitrary views can 
be synthesized. 
In the third year, we proposed error concealment, 
focusing on the 3D video of video plus depth format. 
By exploring the correlation between color and depth, 
error concealment of one kind of information (color 
or depth) can be based on the other kind of 
 1 
行政院國家科學委員會專題研究計畫成果報告  
3D 視訊之網路傳輸編解碼技術與其抗誤設計 
計畫編號：NSC 98-2221-E-194-037-MY3 
執行期限：100 年 8 月 1 日至 101 年 7 月 31 日 
   主持人：賴文能教授   中正大學電機系 
計畫參與人員：廖有朋、陳羿霖、謝家勇、陳昶豪、佘東穎 中正大學電機所 
 
 
一、 中文摘要 
為了達到任意視角的3D視訊呈現，所需要的
視訊資料也相對大量提升，而且利用網路來傳輸
視訊資料也是現今不可或缺的一環，但是網路傳
輸的缺點就是容易受到雜訊的干擾導致資料錯誤
無法正確解回。因此，有效傳遞龐大的 3D 視訊
資料，並保護資料不受到雜訊的干擾而發生錯誤
就變成了重要的議題。 
本計畫以三年時間對立體視訊及多視域視訊
等 3D 視訊之網路傳輸編解碼技術與其抗誤設計
作一完整的研究。在多視域視訊方面，我們利用
三維網格 (3D Mesh) 的資料格式來表示動態 3D 
視訊。為了能夠達到有效的壓縮與抗誤，我們先
把 3D 網格資料區分出不同重要性來做不同程度
的通道編碼保護，如此才能同時達到最佳高壓縮
率與抗誤性。而對於立體視訊，我們主要探討 
color+depth 格式的 3D 視訊，我們探討其在遭受
通道干擾下，解碼端如何進行錯誤隱藏  (error 
concealment)，以達到較佳的 3D 視訊品質。 
在本研究的第一年中，我們先把主要焦點放
在靜態 3D 模型 (model) 的漸進式 (progressive) 
表示法上。此外，為因應 dynamic 3D video 的特
性，我們也探討 dynamic inconsistent 3D mesh 的
壓縮如何與上述的  progressive 3D model 相結
合。 
    第二年，我們除了著眼於漸進式 (progressive) 
動態 3D 模型 (model) 的壓縮外，為了能夠在接
收端合成任意視角的 3D 視訊影像，我們討論了
如何把傳輸端所擷取的多視角彩色 (材質) 視訊
加以壓縮並傳輸到接收端；本年度計畫亦討論了
接收端如何利用解碼所得到的多視域材質與 3D 
網格資訊來進行任意視角影像 (Free Viewpoint 
Image) 的合成。一般而言，3D 網格模型僅包含
物體的幾何結構資訊，因此我們將多視域材質影
像貼圖到重建的三維網格模型上即可合成任意視
角影像，而達到 3D 視訊的目的。 
第三年，本計畫針對視訊搭配深度資訊的 3D 
視訊格式，提出一個錯誤隱藏的技術。利用彩色
與深度序列之相關性，有效地利用另一序列資訊
輔助重建出錯誤畫面。對於彩色序列的錯誤隱
藏，我們對前面已正確解碼的彩色畫面移動向量
進行前處理並偵測物體的遮蔽情況，再以移動向
量外插進行錯誤隱藏，錯誤隱藏過程中針對不同
的遮蔽情況將採用不同程度的深度資訊輔助；對
於深度序列的錯誤隱藏，我們以彩色畫面移動向
量對深度進行移動補償的畫面為基礎，應用我們
提出的雙向 BMA 演算法，並以彩色畫面資訊為
輔助進行深度錯誤隱藏。 
關鍵詞：立體視訊、多視域視訊、自由視角視訊、
視訊壓縮、三維動態估測、漸進式三維網格、任
意視角影像合成、錯誤隱藏 
Abstract: 
To achieve free-viewpoint presentation, the 
requirement in video data amount is increased rather 
huge. By the way, transmission of these huge data 
via network is unavoidable. However, the networks 
are naturally subject to noise interference, causing 
that the transmitted video data will incur errors. 
Hence, how to transmit 3D video data efficiently and 
correctly becomes an important issue. 
We use three years to do a complete research 
on 3D video encoding, decoding, and error 
concealment. In multi-view video transmission, we 
used 3D mesh to describe dynamic 3D video. To 
achieve efficient compression and error resilience, 
we partition the 3D mesh data into two groups, each 
has different importance and requires different level 
of channel encoding. In stereo video transmission, 
we discuss the transmission of 3D video in 
color+depth format and explore the error 
concealment techniques when incurring channel 
errors during transmission. 
In the first year, we first focus on the 
progressive decomposition of a static model. Besides, 
we also investigate the compression of dynamic 
inconsistent 3D mesh (due to the characteristics of 
dynamic 3D video) to see how it can be combined 
with progressive 3D model. 
In the second year, in addition to focusing on 
the coding of progressive and dynamically 
inconsistent 3D meshes, we also discuss how to 
compress the multi-view color (i.e., texture 
information) video that are captured at the 
transmitter side so that arbitrary views can be 
synthesized at the receiver side (that’s the so-called 
3D video). In this second year, we also investigate 
 3 
到加強層資訊，不斷地累加至基本層，還原至原
本的解析度，以供使用者觀看。 
漸進式網格傳輸應用在抗誤傳輸的原理是：
基本模型 (解析度小) 的網格可以較重的  FEC 
來編碼，使得它在傳輸時比較能抵擋雜訊的干
擾，而加強層的網格則僅以較低的 FEC 編碼來
保護，如此形成所謂的  UEP (unequal error 
protection) 抗誤干擾編碼。 
漸進式網格除了可以應用在網路傳輸抗誤方
面，亦可應用在可調式三維模型壓縮編碼。在有
限或時變的網路頻寬條件下，為了讓使用者能夠
不停頓地觀看視訊，先傳輸資料量且解析度小的
基本層，如果頻寬還有空間，再接著傳輸較為完
整、解析度較大的加強層來加強視訊品質，提供
了使用者可以選擇不同解析程度的影像。上述可
以依據網路狀況和喜愛程度來挑選，節省一些不
需傳送之精細資訊，讓觀測者有更多的選擇可以
使用。漸進式網格示意圖，如圖2.3所示。 
 
圖2.3 漸進式網格示意圖[3] 
多視域彩色視訊編碼 
本計劃在此所謂的「多視域彩色視訊編碼」
與一般 MPEG 的 MVC (Multi-view video coding) 
是不同的。一般的 MVC 中，其不同視角間的夾
角非常小，因此，相鄰視角間影像的重疊程度很
大，有很多的 redundancy 必須要去除。然而，本
計劃所探討  3D video 中擷取的  multi-view 
video，各視角的重疊程度不大。例如，在後面所
使用的實驗數據便是由 20 支攝影機環場攝影而
來 (本計畫前期計畫亦使用 13 支攝影機進行環
場拍攝，其中位於赤道方位者僅有 8 支)，平均
相鄰兩支攝影機的夾角為 20~30 度。在這種情況
下，使用標準的 MVC 進行編碼並不適合。因
此，我們在本計劃提出另類的多視域視訊編碼：
我們先對多視域影像進行資料減量，再將減量後
的影像融合加以編碼。 
任意視角影像合成 
任意視角影像合成可参照圖2.4，假設圖中棕
色部分為 3D 網格模型，而粉紅色為鄰近的兩個
視域，藍色是我們希望合成的虛擬視角。因為在
架設攝影機時，三維空間與二維影像間的關係已
先行校正，所以彼此間轉換參數都為已知。到了
合成端，我們接收到的是棕色與粉紅色的材質資
訊，我們可以將棕色資訊投影到任意視角上，例
如投影到粉紅色視域，即可取得相對的顏色顯
示。若投影到中間藍色的新視域 (Novel view) 
時，就相當於在新視域進行影像合成的動作，合
成出該視域的顏色資訊。 
 
 
圖 2.4 任意視角影像合成示意圖 
V+D 視訊編碼架構錯誤隱藏技術 
V+D 視訊格式包含一個視角的彩色序列與
其對應的深度序列。此格式由  European IST 
project ATTEST 提出並且標準化為 MPEG-C Part 
3。接收端在解碼影像後可藉由  DIBR (Depth 
Image-based Rending) [21] 技術合成其他視角序列
而重建出多視角視訊。在 V+D 視訊格式中常見
有聯合編碼架構與獨立編碼架構。 
 
三、研究方法  
3.1 第一年度研究方法 (98 年 8 月~99 年 7 月) 
3.1.1 基本系統架構 
改良式動態 3DMC系統 
本計畫第一年的研究成果是架構在先前計畫
成果的改良式 3DMC 動態三維網格模型壓縮技
術 [4]，其中保留原本 3DMC 中對網格拓璞資訊
的壓縮，而對動態頂點座標的編碼進行改良。其
中採用 K-means分群、ICP 等前處理，再運用 
Inter-model預測的概念進行 3D 運動估測，可以
有效達到動態頂點壓縮的效果。 
關於改良式動態 3DMC系統，可用圖3.1來說
明。第一時間的 3D 網格結構係使用 MPEG-4 
AFX-3DMC [5] 進行壓縮，其後時間的 3D 網格
結構則往前一時刻的網格進行動態估測，對每一
個頂點進行 inter-model 的預測，求得頂點索引和
三維座標的殘餘值資料後將其送入MPEG-4 
AFX-3DMC 的VLC編碼器進行壓縮編碼，但其中
的三角形拓樸資訊仍然使用  MPEG-4 AFX 
-3DMC 中所使用的 Triangle Tree 的編碼方式。
另外，MPEG-4 AFX -3DMC 是使用算術編碼算法
中基於上下文 (CX) 的 MQ編碼器，其利用相鄰
位元之間具有較強相關性來進行預測編碼。至於
網 格 的 相 關 連 線 資 訊 則 仍 使 用  MPEG-4 
AFX-3DMC進行編碼。 
 5 
才不會造成重建時造成錯誤的狀況發生。白色連
線是沒有被圈選到作簡化動作的三角形網格。做
完一次簡化後會將整體的三維連線資訊當作下一
次的模型輸入，如此反覆地降低頂點連線資料量
和模型精細程度。 
選定區塊之後，計算中心點到周圍頂點的連
線距離，根據距離排序並且將已經被合併過的候
選點排除，避免產生重複合併點而造成重建時的
錯誤。確定之後再將中心點合併到已存在的周圍
頂點位置，如此可以降低計算新頂點位置複雜
度，在傳遞頂點差值方面也更加節省資料量。 
 
圖 3.5 三維模型中被選取到進行簡化的區塊 
接著要考慮的就是在眾多的頂點中，如何去
選取先後順序以及停止簡化動作的標準針。參考
[7]文獻中考慮每個區塊的中心頂點周圍所連到的
三角形，每兩相鄰面去計算其曲率，再根據曲率
值的大小，可以判斷出來這個頂點區塊的凹凸程
度。對於簡化的順序，也希望由失真程度較小，
也就是對於三維模型物理結構改變最小的區塊開
始簡化，所以找到該區塊是比較平滑的平面的
話，對於一些較有物理特色的頂點(如:角和鼻子和
尾巴)比較容易保留下來。 
                                        
                                        
                                        
                                        
                                        
                                        
                                        
圖 3.6 中心點和鄰近三角形的夾角，平面夾角
和區塊曲率的計算公式如下(3-1) 
                                                    
baba
TbTaMaxVCurvature
TbTa
TbNormalTaNormalTbTa




且6,...,2,1,
))),((cos()1(
),(0
__)),(cos(
  (3-1) 
區塊 (Patch 456) 中心頂點 V1 的曲率是經由
鄰近三角形平面所形成的夾角來計算。∠(Ta,Tb)
是指三角形 Ta 和 Tb 之間夾角 θ 值大小，Normal
為該平面的法向量。如果兩平面的夾角值近似於
Π，代表該頂點所連到的三角形區塊接近於一個平
面，因此有比較小的曲率值；相反的，如果該頂
點區塊所形成的夾角是尖銳的，代表有較大的曲
率值。取出該區塊的最大值即可看出該區域的最
大變化量，經由計算曲率由小到大排序所有選到
的頂點，就可以保留重要位置的頂點。我們針對
比較平滑的區域先進行簡化，當失真程度達到一
個門檻值即停止簡化動作，再當作下一次簡化輸
入模型。 
在 Edge collapse 簡化的過程中，可能會選到
三維空間中不恰當的邊，造成整體模型的結構發
生變異。如圖 3.7 所示。如果選要合併邊(V1、V2)
從頂點 V1 合併到頂點 V2，本來咖啡色三角形網
格法向量是朝上的方向，經過合併之後造成網格
翻轉，也可能同時產生周圍有四邊形網格的不合
理狀況發生，所以在合併的前後，可以經由計算
法向量翻轉的角度大小，如果超過 (π/4) 就有可
能是 Fold over 狀況發生。 
Sliver triangles的情形就是三角形網格的角度
很小，這些網格不適用在合併的狀況，容易造成
連線的交錯，因此可以使用(3-2)公式來計算： 
                                   
                                   
                                   (3-2) 
                         
                                       
其中 Area 為三角形網格的面積大小， l 1、
l 2、l 3 分別為三角形三個邊的長度。如果 值很
小趨近於 0 代表其邊長是幾乎是存在同一條直線
上；如果 值約等於 1 的話，代表網格接近於等
邊三角形。所以 趨近於 0 時在即判斷為無效的
網格而不考慮做簡化的動作。 
                                       
                                       
                                       
                                       
                                       
                                       
                                       
                                       
圖 3.7  Fold over 的狀況發生 
 
3.2 第二年度研究方法 (99 年 8 月~100 年 7 月) 
3.2.1 多視域彩色視訊編碼 
標準的 MVC 並不適合本計畫中多視域視
訊的編碼，因此，本計劃提出另類的多視域視訊
編碼，如圖 3.8 所示。我們先對多視域影像進行資
料減量，再將減量後的影像融合加以編碼。 
2 2 2
1 2 3
4 3 Area
l l l



 
 7 
      (3-3) 
投影幾何 
決定三角形網格的材質資訊來源後，我們需
取出該影像中對應的投影區域，並將材質資訊貼
圖至三角形網格上，如圖 3.11 所示。在攝影機校
正的步驟，我們可以得到取像系統的內部參數與
外部參數，運用這兩組參數可進一步計算投影矩
陣 Pr。因此，即可用公式(3-4)將三角形網格的頂
點投影到影像平面上，取出相對應的材質影像。
要注意的是，這個部分的處理同時也應用到前面
所說的資料減量。也就是說，在傳輸端先模擬接
收端可能應用到的材質資訊來源，對於可能應用
到的加以保留，其餘就可加以去除。 
 
圖 3.11 投影模型 
 
  (3-4) 
改良式遮蔽測試 (Occlusion Test) 
在計算攝影機的可見性 (visibility) 之前，對
於三維網格上的每個三角形網格，我們需要先做
遮蔽測試 (Occlusion Test) 來決定這個三角形網
格色彩資訊的有效來源，即在哪幾個攝影機是可
見的。然而，因重建的三維網格模型不精確，可
能造成遮蔽測試時在該攝影機視角應要被遮蔽的
網格實際上卻沒有被遮蔽，將該攝影機的顏色資
訊作為材質貼圖的來源，而造成貼圖明顯錯誤，
如圖 3.12。 
  
圖 3.12 傳統 
遮蔽測試貼圖結果 
圖 3.13 改良式 
遮蔽測試貼圖結果 
 
分析遮蔽測試的誤判結果可知：在該攝影機
視角應要被遮蔽的網格實際上卻沒有被遮蔽，而
將該攝影機的顏色資訊作為材質貼圖的來源，將
造成貼圖明顯錯誤，風險大。然而，應該沒有被
遮蔽的網格實際上有被遮蔽，而不將該攝影機的
顏色資訊作為來源，可能會失去其最佳的材質影
像來源，但對於合成影像品質的影響不明顯，風
險較小。基於兩個動作的風險不對稱，若要獲得
較佳視覺效果，必須將整體風險降到最低，因此
我們將遮蔽測試的方法加以改良，加入了「深度
緩衝局部性搜尋」與「自適性搜尋範圍調整」這
兩個演算法來減少因三維網格模型不精確造成的
遮蔽測試誤判情形。 
假設 Vi 是三維網格模型上的頂點 (Vertex)，
共有 N 個 (i=1,2,…,N)。Ij是以環場擺設攝影機所
拍攝之材質影像，共有 M 張 (j=1,2,…,M)。Bj 為
繪出各個攝影機的深度緩衝 (Depth Buffer)。利用
公式(3.4)，三維網格模型上的每個頂點 Vi 可計算
出投影到材質影像平面的座標 PjVi jViP 與相對深度
j
Vi 。一般而言，進行遮蔽測試時，我們將深度緩
衝 Bj 內 jViP 座標的值與相對深度
j
Vi 做比較，即可
判斷該頂點是否被遮蔽。 
深度緩衝局部性搜尋即是：相對深度 jVi 不直
接與深度緩衝 Bj內 jViP 座標的值做比較，而是與深
度緩衝 Bj內 jViP 座標周圍 (±R) 的最小值比較，如
圖 3.14 所示。其意義在於增加前方物體遮蔽的區
域，避免應要被遮蔽的三角形網格實際上卻沒有
被遮蔽，將該攝影機的顏色資訊作為材質貼圖的
來源，而造成貼圖明顯錯誤，雖可能會失去其最
佳的材質影像來源，但並不會使得品質明顯下
降。自適性搜尋範圍調整即搜尋半徑 (R) 會隨著
每個三角形網格的貼圖來源數目調整，在三角形
網格貼圖來源數目不多的情形下，使用較小的搜
尋半徑，避免前方物體的遮蔽區域增加而失去原
有的少數貼圖來源。 
 
圖 3.14 深度緩衝局部性搜尋示意圖 
 9 
後面區塊進行估測，如圖 3.17 (b)。 
 
圖 3.17 Intra 子區塊的移動向量估測示意圖 
針對 3D 視訊，由於多了深度資訊，為了提
高 Intra 區塊的移動向量估測正確性，我們加入
深度資訊來輔助估測，其主要流程與上述公式 
(3-15) 大致相同，差別在於計算子區塊移動向量
眾數 )( ,1MVMode
qp
t-  時，並不將搜索範圍內的所
有子區塊皆納入計數，而是利用深度資訊進行篩
選，判斷搜索範圍內每個子區塊與目標子區塊之
深度差異，如公式 (3-16)， depthSAD  必須小於預
設門檻值才納入 )( ,1MVMode
qp
t-  的計算。透過深
度資訊，可排除 Intra 移動向量估測過程中取到
不合適的移動向量，否則可能出現前景 Intra 區
塊經估測後卻得到周圍背景區塊的移動向量。 
)  : Block, Intra : (
    ) (  - ),(     SAD
 , , ,
11
,
,
的相鄰區塊mkqpmk
B(u,v)
B(i, j)
ttdepth
BBB
vu,DjiD
qp
mk




 
(3-16) 
其中， ),( 1 jiDt  表示第 t-1 張畫面中座標 (i, j) 
處之深度值， depthSAD  表示座標 (k, m) 處 Intra
區塊與搜索範圍內座標 (p, q) 處區塊之 深度差
值和。 
移動向量修正 
透過實驗發現，在物體邊緣部分的區塊表現
相較於其他區域明顯不佳，因此我們希望針對前
一時刻之移動向量進行修正，以便取得後續移動
向量外插時更好的資訊來源，移動向量的修正過
程主要分為四個步驟：(1)利用深度資訊找出可能
需修正的目標區域，(2)檢查是否有修正之必要
性，(3) 根據不同必要程度對移動向量進行修正，
(4)檢查修正後移動向量是否對重建影像有所改
善。圖 3.18 為移動向量修正流程圖。 
 
 
圖 3.18 移動向量修正流程圖 
在判斷移動向量可能修正區域時係使用深度
資訊進行邊緣與遮蔽區域判斷，邊緣區域判斷以
區塊為單位，利用 Otsu 演算法 [16] 計算出一個
區塊內的深度二值門檻，透過此門檻將區塊內像
素分為前景與背景部分，並個別計算平均值，如
公式 (3-17)。 
mk
Otsut
B(i, j)
t
mk
Otsut
B(i, j)
t
THjifor
N
ji
THjifor
M
ji
mk
mk
,
1
1
bg
avg
,
1
1
fg
avg
  ) ,( D     ,
) ,( D
  D
  ) ,( D     ,
) ,( D
  D
,
,










 
(3-17) 
其中， mkOtsuTH
,  表示畫面中座標 (k, m) 處區塊進
行 Otsu 所得到之門檻值， fgavgD  表示前景像素的
深度平均值， bgavgD  表示背景像素的深度平均值，
)(  ,D 1 jit  表示座標 (i, j) 處的深度值，M 為前
景 像素總 數， N 為背 景像素 總數。 藉由 
) ,( 1D jit  是否超過門檻值來判定為前景或背景
像素，若該區塊為邊緣區塊，其前景像素平均值
與背景像素平均值會有一定差距，因此，透過公
式 (3-18) 可計算兩平均值之差異絕對值，判斷該
值是否超過另一預設門檻即可判斷出該區塊是否
為邊緣區塊，其中， BoundaryTH  為一預設之門檻
值。 
 11 
 
圖 3.19 以深度資訊輔助移動向量外插的流程圖 
我們以 PMVE 技術為移動向量外插基礎，藉
由錯誤畫面對應之深度資訊進行輔助 (假設畫面 
t 之深度資訊被正確接收)，我們將前一時刻彩色
畫面移動向量外插至錯誤畫面 t，並且在外插後
計算每個外插像素的深度差值，利用深度差值判
斷外插後像素間深度資訊是否匹配，若差值超過
預設門檻值則該像素的移動向量便不外插，若某
一像素被複數個畫面 t-1 像素的移動向量所外插
則將所有外插移動向量取平均。估計出錯誤畫面
中各像素之移動向量後即可進行移動補償而產生
基礎錯誤隱藏畫面，此畫面將於後續錯誤隱藏過
程中被使用。 
在像素分類的部分，我們根據錯誤畫面前一
時刻與前兩時刻之深度資訊找出邊緣以及遮蔽區
域，判斷出邊緣與遮蔽區域之後，我們將進一步
分類。我們將 t-1 時刻彩色畫面上所有像素分為
三類，分別為可靠 (reliable, R)、輕微可靠 (slightly 
reliable, SR) 以及不可靠 (unreliable, UR)，像素分
類之準則如公式 (3-23)。 
otherwise
yxDyxDfor
yxDyxDfor
x, yPixelClass OcclusionBoundary
OcclusionBoundary
1-  ),(  and 1  ) ,(  
1  ),(  and 1  ) ,(  
    
R
UR
SR
  )( 







(3-23) 
(a)可靠像素的移動向量外插 
可靠像素通常位於背景或前景物體內部，這
些像素的移動向量外插於前述的基礎錯誤隱藏已
被完成並達到一定效果，因此不需再經過其它程
序。 
(b)輕微可靠像素的移動向量外插 
針對輕微可靠類別的像素，我們使用 
PMVE，但是在過程中加入聯合移動估測 (Join 
Motion Estimation) 以作修正，它與一般移動估測
不同的地方在於其成本函數 (Cost function) 包含
彩色匹配與深度匹配的總和，前者係匹配至 t-2 
畫面，而後者係匹配至 t 畫面。移動向量外插係
以像素為單位，然而單一像素的匹配無法準確追
蹤物體的移動，因此我們以每一像素為中心框出
一個區塊，利用此區塊進行匹配。聯合移動估測
的過程如圖 3.20 及圖 3.21 所示。 
 
圖 3.20 利用彩色與深度資訊進行移動向量外插
示意圖 
圖 3.20 (a) 表示相鄰之彩色影像 t-2、t-1 及 
t，t 時刻為錯誤畫面。我們利用 t-1 與 t-2 畫面
進行彩色匹配。圖 3.20 (b) 表示相鄰之深度影像 
t-2、t-1 及 t，利用 t-1 與 t 畫面進行深度匹配，
目的為估測出物體在錯誤畫面 t 中的位置。圖 
3.20 (c) 中黑色點為欲進行移動向量外插之像素
點 (x, y)，以該點為中心我們框出一個區塊視窗做
為比對視窗，即為圖中紅色框部分，黑色箭頭為 
t-1 彩色畫面座標 (x, y) 處之移動向量 MV，藍
色框則為移動估測的搜索範圍。我們在搜索範圍
內變動可能的外插移動向量以  ( -MV  來表
示， ) ,( yx  )，並計算其成本，如公式 (3-24)。 
 13 
圖 3.23 中紅色像素點為中心像素，若經判斷
後選擇以區塊進行移動補償，則區塊中每個像素 
(如圖中藍框內的黑色與灰色像素) 深度值與中心
像素深度值進行差值比較，若差值超過預設門檻
值，則該像素便不填補至錯誤畫面，填補結果如
圖中 tC  畫面產生可變形的視窗形狀。此外，若
該像素於先前已被填補過，則後續重複填補時則
取平均值。 
(c)不可靠像素的移動向量外插 
不可靠像素的移動向量外插與移動補償過程基本
上與上述輕微可靠像素的過程大致相同，唯一差異
在於：輕微可靠像素移動外插後的移動補償將取代
基礎錯誤隱藏畫面中的任何像素，而不可靠像素移
動外插後的移動補償僅可填補基礎錯誤隱藏畫面
中未曾被填補之區域。 
深度畫面的錯誤隱藏 
深度畫面錯誤隱藏係利用 BMA 技術並搭
配對應之彩色畫面進行輔助。然而，BMA 原本
主要利用畫面間與畫面內之相關性進行錯誤隱
藏，在畫面錯誤的情況下無法取得畫面內資訊，
因此我們先透過移動向量共享的方式 [19] 先產
生基礎深度錯誤隱藏畫面。由於基礎深度錯誤隱
藏畫面仍有部分錯誤，針對錯誤區塊將再進行 
BMA 加以修正，但直接應用一般 BMA 演算法
時的效果有限，所以我們參考了文獻 [17] 的作法
在過程中修改匹配準則，並且利用我們提出的雙
向 BMA 以提高錯誤隱藏效果。圖 3.24 為深度
畫面錯誤隱藏流程圖。 
深度畫面
錯誤隱藏
建立 時刻基礎
錯誤隱藏畫面
時刻不可靠彩色
移動向量偵測
發生錯誤?
解碼深度畫面
重建深度畫面
Yes
No
雙向 BMA 深度畫
面錯誤隱藏
 
圖 3.24 深度畫面錯誤隱藏流程圖 
基礎錯誤隱藏畫面產生 
我們首先對 t 時刻彩色畫面中的 Intra 區塊
產生移動向量，再利用 t 時刻的彩色畫面移動向
量對 t-1 時刻的深度畫面進行移動補償以產生 t 
時刻的基礎深度錯誤隱藏畫面。 
基礎深度錯誤隱藏畫面主要為後續之 BMA 
所使用。然而，因為彩色影像移動向量中亦存在
不可靠者，利用它們來建立的基礎深度錯誤隱藏
畫面亦可能是錯的，因此，我們必須把這些不可
靠的彩色影像移動向量予以剔除，再以 BMA 法
加以修正。 
不可靠彩色畫面移動向量偵測 
我們利用 t 時刻重建彩色畫面與 t 時刻彩
色移動向量補償間的差值 (即編碼時 t 畫面的殘
餘值 (residue))，並透過一門檻值來判斷彩色移動
向量是否可靠，如公式 (3-26)。 



 

otherwise
THx, yx, yCfor
x, yMap errorttunreliable
  )( Cˆ - )(  
    
0
1
  )(  
(3-26) 
其中， )( x, yCt  為 t 時刻彩色畫面在座標 (x, y) 
處之像素值， )( Cˆ x, yt 為 t 時刻在座標 (x, y) 處
的移動補償像素值， errorTH  為一預設門檻值。由
於 BMA 的運作皆以區塊為單位，因此需將公式 
(3-25) 中以像素為單位的不可靠圖轉換成以區塊
為單位。以 44  像素為一個區塊單位，判斷每
個區塊是否有像素 1unreliableMap ，若有，則該
區塊皆設為 1，否則該區塊的 unreliableMap  值為
0。 
雙向 BMA 深度畫面錯誤隱藏 
在 BMA 過程中有三個步驟：(1) 產生候選移
動向量，(2) 利用候選移動向量進行匹配以得到最
佳移動向量，(3) 移動向量補償。在這我們主要針
對 1unreliableMap 的不可靠區塊進行處理。 
候選移動向量產生來自於彩色畫面移動向量 
，另外，在這不以巨區塊為單位，因為邊緣巨區
塊僅以單一移動向量來表示並不合適，因此我們
以 44  區塊為單位。首先，取以該不可靠區塊
為中心的周圍八個相鄰區塊移動向量與對角線上
相鄰四個區塊移動向量做為候選移動向量集合，
如圖 3.25。 
 
圖 3.25 移動向量候選區域示意圖 
 15 
distance)來量測失真的程度。 
Hausdorff distance 
公式(4.1)代表 S 平面上點 P 到平面 S 的距離： 
         (4.1)  
公式(4.2)代表平面 S 到平面 S’的 Hausdorff 距離： 
                                     
             (4.2) 
由於 S 和 S’平面有可能距離不是對稱的大小值，
所以 Hausdorff distance 定義為公式(4.3)： 
(4.3) 
以下圖 4.5~4.22 及表 1,2 分別為 “chicken” 
及“robot”的實驗結果。其中包括兩個 model 當被
分解至第 7 層後剩下的頂點數目及網格數目、其
重建的影像、及 Hausdorff distance 失真表現。 
                                      
 Chicken模型
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
1 2 3 4 5 6 7PM   Layer
H
au
sd
or
ff
 d
is
ta
nc
e
Max
Mean
RMS
 
圖 4.5 “chicken”在 Hausdorff distance 量度下的
表現 
 
Robot 模型
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
1 2 3 4 5 6 7PM Layer
H
au
sd
or
ff
 d
is
ta
nc
e
Max
Mean
RMS
 
圖 4.6 “robot”在 Hausdorff distance 量度下的表現 
 
    
圖 4.7 “chicken” 
原始 3D 網格 
圖4.8 “chicken-1” 
簡化後的 3D 網格 
    
圖4.9 “chicken-2”  
 簡化後的 3D 網格 
圖4.10 “chicken-3”  
 簡化後的 3D 網格 
    
圖4.11 “chicken- 4”  
 簡化後的 3D 網格 
圖4.12 “chicken- 5”  
 簡化後的 3D 網格 
    
圖4.13 “chicken-6”  
 簡化後的 3D 網格 
圖4.14 “chicken-7”  
 簡化後的 3D 網格 
 
 
表 4.1  Chicken 模型簡化後資料數據 
Model chicken 
簡化的層級 頂點個數 網格面數 
原始資料量 3030 5454 
Level -1 2351 4322 
Level -2 2062 3744 
Level -3 1707 3034 
Level -4 1428 2476 
Level -5 1248 2116 
Level -6 1043 1706 
Level -7 885 1390 
 
 
    
圖 4.15 “robot” 
原始 3D 網格 
圖4.16 “robot-1” 
簡化後的 3D 網格 
 
 17 
  
圖 4.26“robot” Frame 
1 LV4 重建後三維網格 
圖 4.27“robot” Frame 
1 LV7 重建後三維網格 
 
我們除了會將重建後 3D 網格模型顯示以表
示主觀上的優劣外，亦將使用 KG 誤差值[11]及
Hausdroff distance [12]兩種評量數值來量測壓縮
簡化前後的失真度。 
圖 4.28-4.29 即為我們的實驗結果，圖表中橫
軸為平均每個模型所需的 k-byte 數，而縱軸是不
同的量度指標。紅線代表 MPEG-4 AFX-3DMC，
而藍線則代表改良式動態 3DMC 方法，其中 LV1
代表原始模型簡化一次的結果 (包括 1 個基本層
和 1 個加強層)，LV7 則為模型簡化七次後的結果 
(包括 1 個基本層和 6 個加強層)。每個圖表均為 
“robot” Frame0 - Frame5 的平均實驗值，而使
用可調式改良動態 3DMC 時，其 Frame0 仍然使
用 MPEG-4 AFX-3DMC 進行編碼。從 KG 誤差值
中我們可以看出：在簡化層級為 LV1 及 LV2 的情
況下，漸近式網格編碼的壓縮率仍優於傳統 3DMC
編碼；當簡化層級至 LV3~LV7 較精細的加強層
時，雖然可以彈性選擇較多個解析度層級，但也必
須花費較多的總位元數來壓縮基本層及所有加強
層資料。 
圖 4.29 的 Hausdorff distance 量測，是針對簡
化後模型和原始模型來計算失真值，所以可以使用
不同的頂點和網格數目來計算誤差。其中 LV7 是
簡化 7 次後的基本層級，該層使用最少的編碼位元
數，卻也同時刪除了最多的頂點和網格，所以其 
Hausdorff distance 最大；相反地，由於 LV1 只做
一次簡化動作，因此改變的模型網格結構較少，其 
Hausdorff distance 值較小。在圖 4.29 中可以看到
在相同 Hausdorff distance 值的狀況下，LV7 基本
層所花費的位元數最少而 LV1 基本層最多。因為
LV7 是把單一時刻模型簡化分成 7 個層級，並將
基本層編碼重建後做 Hausdorff  distance 量測，
因為 LV7 的基本層檔案大小遠比 LV1 的基本層檔
案小，因此 R-D 曲線中 LV1 是在 LV7 的右邊。 
Robot
0
0.2
0.4
0.6
0.8
30 40 50 60 70 80
bit rate (kbyte/model)
K
G
 e
rr
or
3DMC
Modified_3DMC
LV7
LV6
LV5
LV4
LV3
LV2
LV1
圖 4.28  ” KG 誤差”量度下之 R-D 表現 
Robot (Base Layer)
0
0.0001
0.0002
0.0003
0.0004
0.0005
0.0006
0.0007
20 30 40 50 60 70 80
bit rate  ( kbyte /model )
H
au
sd
or
ff
 d
is
ta
nc
e
LV7
LV6
LV5
LV4
LV3
LV2
LV1
3DMC
Modified_3DMC
圖 4.29  ” Hausdroff distance”量度下之 R-D 表
現 
4.2.2 多視域彩色視訊編碼實驗結果 
我們採用另一個多視域視訊來進行多視域彩
色視訊編碼與任意視角影像合成的實驗，此多視域
視訊為 20 支攝影機環場擺設，拍攝每一時刻的多
視域影像，再對每一時刻的多視域影像進行 
PCMVS [13] 演算法重建出的 3D 網格模型，攝
影機擺設如圖 4.30。 
 
圖 4.30 多架攝影機環場擺設 
 
我們使用 JM12.2 作為 H.264 的編解碼器，欲
編碼的視訊長度為 29 個畫面 (frame)。將 20 支攝
影機環場攝影的視訊分別獨立編碼所產生的材質
資訊大小為 1.54 MB，然而我們使用的分層材質資
訊表示法編碼後的大小約為 1.6 MB，效果並不理
想，如表 4.3。推測因分層材質資訊表示法的影像
含有許多高頻成分，不利於 H.264 的壓縮。因此，
 19 
向量外插後未使用可變視窗形狀的移動補償，(D) 
則為完整演算法的結果。由實驗結果可以發現，
我們的方法能提供較好的重建品質，尤其在 
Mobile 與 Newspaper 有較多增益。此外，可以
看到我們的方法使用可變視窗形狀的移動補償的
結果比未使用可提升 0.82 dB。“Balloons”、“ Book 
Arrival”、“ Kendo” 及 “ Newspaper” 測試序列的
深度為估測所得，因此我們提出的深度輔助方式
會導致部分區域重建效果未佔便宜，同時可以發
現我們方法有無使用可變視窗形狀的移動補償在
這些序列的結果大致相同，此係因為在深度不準
確的時候我們的移動補償區塊會偏向較小區塊或
像素補償，而物體的深度資訊也多有雜訊，在這
些狀況下使得效果不顯著。除此之外，對於彩色
畫面背景顏色較為單調的序列也不會有太大影
響。我們的方法除了以深度資訊輔助之外仍然使
用到相鄰彩色畫面的相關性，相較於文獻 [18] 僅
依賴深度差值比較，我們的方法可以彌補部份由
於原始深度值不精確所造成的影響。除此之外，
對於靜態影像 (如 Newspaper) 亦可以有效保持
其重建品質。圖 4.34、圖 4.35、圖 4.36 與圖 4.37 
分別為 Mobile 第 7 張畫面、Book Arrival 第 50 
張彩色畫面、 Kendo 第  33 張彩色畫面與 
Newspaper 第藉由本論文提出之方法與文獻 [18] 
之方法錯誤隱藏的重建畫面。 
 
  
(a)                   (b)    
  
(c)                  (d) 
圖4.34  Mobile 序列 7th 彩色畫面錯誤隱藏結果 
(a) 原始畫面 (b) 本論文之方法 : 33.82 dB (c) 
PMVE+Depth 文獻 [18] : 31.54 dB 
(d) HMVE+Depth 文獻 [18] : 32.46 dB 
 
  
(a)                   (b) 
  
(c)                  (d) 
圖 4.35  Book Arrival 序列 50th 彩色畫面錯誤隱
藏結果 (a) 原始畫面 (b) 本論文之方法 : 30.91 
dB (c) PMVE+Depth 文獻 [18] : 30.08 dB (d) 
HMVE+Depth 文獻 [18] : 30.12 dB 
  
(a)                   (b)    
  
(c)                   (d) 
圖 4.36  Kendo 序列 35th 彩色畫面錯誤隱藏結
果 (a) 原始畫面 (b) 本論文之方法 : 28.28 dB (c) 
PMVE+Depth 文 獻  [18] : 26.22 dB (d) 
HMVE+Depth 文獻 [18] : 26.53 dB 
  
(a)                   (b)    
 21 
一 PLRs 時係以十次不同錯誤位置進行實驗並計
算平均 PSNR，如表 4.7。 
(a)彩色序列錯誤隱藏 
表 4.7 彩色畫面錯誤隱藏平均 
 
在考慮錯誤蔓延的情況下，我們方法在相對於
文獻 [18] 的兩種方式皆有增益，在 PLR 越高的
情況下增益通常越明顯。圖 4.40 為 Mobile 彩色
序列在 PLR= 5% 時，第 17 張、第 18 張與第 19 
張解碼畫面，其中第 17 張為錯誤隱藏重建畫面。 
  
(a)                   (b) 
  
(c)                   (d) 
  
(e)                   (f) 
  
(g)                   (h) 
 
(i) 
圖 4.40 Mobile 序列在 PLR=5% 時 17th、18th 與 
19th 彩色解碼畫面，(a)(d)(g) 為原始畫面，(b) 
HMVE+Depth [18] 的 17th 錯誤隱藏重建畫面，
(e)(h) 為受到 (b) 錯誤蔓延的 18th、19th 解碼畫
面，(c) 本論文方法的 17th 錯誤隱藏畫面，(f)(i) 為
受到 (c) 錯誤蔓延的 18th、19th 解碼畫面 
(b)深度序列錯誤隱藏 
表 4.8 深度畫面錯誤隱藏平均 
 
表 4.8 為深度視訊在封包遺失率為 5%、10% 
與 15% 時，彩色畫面皆正常解碼，實驗結果計算
所有深度畫面的平均 PSNR，同一 PLRs 亦實驗
十次不同錯誤位置。 
在深度錯誤蔓延實驗中，依然可以發現與不
考慮錯誤蔓延情況時的類似結果。在深度資訊不
精確的狀況下，本文方法相較文獻 [15][19][20] 
方法的增益較少，並且在 Book Arrival PLR=5% 
時效果不如文獻 [19] 方法。不過在 Mobile 序列
上，本文方法明顯有較佳的效果。圖 4.41 為 
Mobile 深度序列在 PLR=10% 時，第 25 張、第 
26 張與第 27 張解碼畫面，其中第 25 張為深度
錯誤隱藏重建畫面。 
  
(a)                   (b) 
 
 23 
良為雙向 BMA，對於深度資訊精確的序列可以
達到 3.69 dB 的增益。 
 
本計畫至目前的研究成果投稿如下： 
1. Jui-Chiu Chiang, Chun-Hung Chen, and Wen- 
Nung Lie, “Coding of Dynamic 3D Mesh Model 
for 3D Video Transmission,” Proc. of 2011 
Pacific-Rim Symposium on Image and Video 
Technology, PSIVT’11, Korea, Nov. 2011. 
2. Jui-Chiu Chiang, Ping-He Hou, Kai-Che Liu, and 
Wen-Nung Lie, “Multiview Texture Coding and 
Free Viewpoint Image Synthesis for Mesh-based 
3D Video Transmission,” Proc. Of IEEE 
International Symposium on Circuits and Systems, 
ISCAS-2012, Seoul, Korea, May 2012. 
3. Wen-Nung Lie and Guan-Hua Lin, “Error 
Concealment for 3D Video Transmission,” 
submitted to IEEE International Symposium on 
Circuits and Systems, ISCAS-2013, Beijing, 
China, May 2013. 
 
六、參考文獻 
[1] A. Smolic, K. Mueller, P. Merkle, T. Rein, P. 
Eisert, and T. Wiegand, “Representation, 
Coding, and Rendering of 3D Video Objects 
with MPEG-4,” Proc. Of IEEE Int’l Workshop 
on Multimedia Signal Processing (MMSP), 
Siena, Italy, Sept. 29.-Oct. 1. 2004.  
[2] G. Al-Regib, and Y. Altunbasak, “An unequal 
error protection method for packet loss resilient 
3D mesh transmission,” Proc. Of Twenty-First 
Annual Joint Conference of the IEEE Computer 
and Communications Societies(INFOCOM 
2002), Vol.2,pp.743-752,June 2002.  
[3] Michael Garland, Quadric-Based Polygonal 
Surface Simplification, PhD thesis, Carnegie 
Mellon University, CS Dept., 1999. Tech. Rept. 
CMU-CS-99-105.  
[4] Chung-Hung Chen, “Dynamic 3D coding for 
multi-view video coding,” Master, July 2009, 
National Chung Cheng University.  
[5] G. Taubin and J. Rossignac, “Geometric 
Compression Through Topological Surgery,” 
ACM Transactions on Graphics, Vol. 17, No. 2, 
pp. 84 - 115, April 1998.  
[6] N. Stefanoski, P. Klie, Xiaoliang Liu, and J. 
Ostermann, “Layered Predictive Coding of 
Time-Consistent Dynamic 3D Meshes using a 
Non-Linear Predictor,” Proc. of IEEE Int’l. 
Conf. on Image Processing (ICIP-2007), pp. 
V-109-V-112, Oct. 2007  
[7] Jiang ZHU Tomohisa TANAKA and Yoshio 
SAITO, “Multi-Resolution Mesh for Sculptured 
Surface Machining,” Journal of Solid 
Mechanics and Materials Engineering, Vol 
1,No 1.2007  
[8] Chunmei Duan, Changhe Tu, Xiangxu Meng, 
"Optimization of multi-view texture mapping 
for reconstructed 3D model," Proc. Of 8th 
World Congress on Intelligent Control and 
Automation (WCICA), vol., no., pp.30-34, 7-9 
July 2010.  
[9] A. Laurentini, “The Visual Hull Concept for 
Silhouette-Based Image Understanding,” IEEE 
Trans. on Pattern Analysis and Machine 
Intelligence, Vol. 16, No. 2, pp. 150-162, Feb. 
1994.  
[10] Z. Karni and C. Gotsman, “Compression of 
Soft-body Animation Sequences,” Computers 
& Graphics, vol. 28, no. 1, pp.25–34, 2004. 
[11] Nicolas Aspert, Diego Santa-Cruz, Touradj 
Ebrahimi, “MESH: Measuring Errors Between 
Surfaces Using The Hausdorff Distance,” Proc. 
Of the IEEE International Conference in 
Multimedia and Expo(ICME 2002) ,Vol.1,pp. 
705-708,Aug.2002  
[12] Nicolas Aspert, Diego Santa-Cruz, and Touradj 
Ebrahimi, “MESH: Measuring Errors Between 
Surfaces Using The Hausdorff Distance,” Proc. 
Of IEEE Int’l Conf. on Multimedia and Expo 
(ICME 2002) ,Vol.1, pp. 705-708, Aug. 2002.  
[13] Yebin Liu, Qionghai Dai, Wenli Xu, "A 
Point-Cloud-Based Multi-view Stereo 
Algorithm for Free-Viewpoint Video," IEEE 
Trans. on Visualization and Computer Graphics, 
vol.16, no.3, pp.407-418, May-June 2010.  
[14] Q. Peng, T.W. Yang and C.Q. Zhu, 
“Block-based Temporal Error Concealment for 
Video Packet Using Motion Vector 
Extrapolation,” Proc. of IEEE Int’l Conf. on 
Communications, Circuits and Systems and 
West Sino Expositions, Vol.1, pp.10-14, 
Chengdu, China, July. 2002. 
[15] Y. Chen, K. Yu, J. Li, and S. Li, “An Error 
Concealment Algorithm for Entire Frame Loss 
in Video Transmission,” Proc. of IEEE Int’l 
Conf. on Picture Coding Symposium 
(PCS-2004), San Francisco, CA, Dec. 2004. 
[16] N. Otsu, “A Threshold Selection Method from 
Gray-Level Histograms,” IEEE Trans. on 
Systems, Man and Cybernetics, vol. 9, no. 1, 
pp.62-66, Jan. 1979. 
[17] Yunqiang Liu, Jin Wang, and Huanhuan Zhang, 
“Depth Image-Based Temporal Error 
Concealment for 3-D Video Transmission,” 
IEEE Trans. on Circuits and Systems for Video 
Technology, Vol.20, No.4, pp.600-604, Apr. 
2010. 
[18] Bo Yan and Jie Zhou, “Efficient Frame 
Concealment for Depth Image Based 3D Video 
Transmission,” IEEE Trans. on Multimedia, 
Vol.14, No.3, pp.936-941, Mar. 2012. 
[19] Chaminda T.E.R. Hewage, S. Worrall, S. 
Dogan, and A.M. Kondoz, “Frame 
1出席國際會議 (國外出差) 心得報告
2011 年 PSIVT 泛太平洋影像與視訊技術國際會議
The 5th Pacific-Rim Symposium on Image and Video Technology
(PSIVT 2011)
一、參加會議時間：民國 100年 11月 20日至 11月 23日
二、會 議 地 點 ：韓國光州市
三、行 程：11 月 19 日出發至 11 月 23 日回國，期中參加
PSIVT 2011國際會議
四、出席會議經過：
2011年 PSIVT 國際會議在韓國光州市舉行。會議時間從 11月
20日至11月23日共 4 天，它是由韓國 GIST 大學的 Yung-Sung Ho
教授所主辦。PSIVT 本年為第 5 屆，第一屆 (2006 年 12 月) 係由
本國清華大學電機系陳永昌教授與紐西蘭奧克蘭大學電腦系教授
Prof. Reinhard Klette 所發起，本人與清大張隆紋教授擔任 Program
Co-chair。傳承至今，本人除了擔任 PSIVT 的 steering committee
外，今年並擔任 General Cochair。由於投稿前負責在國內多加宣傳的
效果，今年 160 篇投稿中，我國即佔了 22 篇，居第三位 (前兩名
為韓、日)。而投稿被接受的國內學者中似乎大部分是國立大學中較
熟悉的教授，私立大學及科大的教授較少。
本次會議共有約 160 篇投稿 (2006 年第一屆有 450 篇投稿，
第二屆在智利舉辦則有172 篇投稿)，錄取約 70 篇。按照大會所公
布的數據，台灣的投稿者僅次於韓國及日本，有 22 篇之多 (被接受
之篇數未知 )。本次會議係以 single track 方式舉行，分為 3
Keynotes、7 Oral Lecture 及 2 Poster sessions，外加數個韓國當地廠
商的 live demos。本次會議雖然不大，但所邀請的 Keynote 及
Invited talk 也都非常具可看性，例如日本名古屋大學的 Prof.
Tanimoto 是浸淫於 FTV (Free viewpoint TV) 方面的專家。由於本次
的 Poster 論文與 Oral 時間不重疊，因此我們可以悠遊於一堆論文
之中，可以面對面與作者直接討論可以說是一種享受。本人共發表 2
篇 poster 論文：
3個機會與國外學者交流。下圖為聚餐時的照片，圖中右邊另ㄧ位教授
為日本 NII 的 Akihiro Sugimoto 教授，他是 PSIVT-2009 的主辦
人。
除了 Invited talk，大會亦安排了 live demo，除了廠商，也參觀
Prof. Ho 的實驗室。Ho 教授的實驗室並不如想像中的偉大，但其研
究成果卻不錯，這證明成事在人。
本人參加此次會議的另一目的是參加 PSIVT 的 steering
committee 會議。會中主要由清大陳永昌教授、我、紐西蘭的 Prof.
Reinhard Klette、日本 NII 的 Akihiro Sugimoto 教授、及韓國的
Yung-Sung Ho 教授共同討論下ㄧ屆 PSIVT 會議的細節，會議中決
定下ㄧ屆為 2013 年，在墨西哥舉辦 (跳過 2012)。由於我與陳永昌
教授是 PSIVT 國際會議的發起人之一，因此我們在這個會議中也特
別投入，藉機會認識了不少韓國朋友。
五、與會心得:
PSIVT 會議已舉辦數年。由於一開始的參與及投入，使得本人有
機會成為這個會議的核心份子之一。因此，如果有機會認識一些國際
中的重要人物，從協助參與會議，慢慢爭取主辦權，相信對本國學術
的國際化有相當幫助。
5multimedia 主題與本人研究相關。按以往的經驗，multimedia 論文約
佔 ISCAS 論文數量的 1/4~1/3，包括:
(1) Compressive sensing
(2) Video coding implementation and optimization
(3) Multiview and 3D visual signal coding
(4) 3D visual signal acquisition and rendering
(5) VLSI for image & video systems
(6) Media content analysis and recognition
(7) HEVC
(8) Image and vision sensors
(9) Visual signal coding, modeling, and representation
(10) Visual signal processing and enhancement
此外，大會亦安排了4 個 live demo session，讓參加者可以看到實際
的技術展品，非常值得。
大會今年比較特別的是安排了 SamSung 所提供產品的抽獎 (必
須投稿在三篇以上，且至少一篇發表)。本人投三中二，雖也是候選
人，但不夠幸運。此外，大會在晚宴亦安排了韓國偶像團體 4 Minute
的歌舞表演，雖引起一陣騷動，但這個團體我還是第一次聽到，離本
人年齡有點遠。
在參加會議期間，本人亦藉機會參觀了首爾市的名勝，例如韓
國中央博物館與首爾國立大學。博物館是 2005 年新建的，一樓的文
物將韓國由古朝鮮時代的演進一點一滴的紀錄下來，而首爾大學則地
大且國際化，就連大門的「案內」也會講不錯的英文呢，這一點可能
連台北的學校都還作不到呢。
以下是本人在發表 poster 論文時與相同 session 的謝君偉教授
(台灣海洋大學資工系) 合影。
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/24
國科會補助計畫
計畫名稱: 3D視訊之網路傳輸編解碼技術與其抗誤設計
計畫主持人: 賴文能
計畫編號: 98-2221-E-194-037-MY3 學門領域: 訊號處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
