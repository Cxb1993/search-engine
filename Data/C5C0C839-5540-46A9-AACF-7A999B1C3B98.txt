bird species recognition system. 
The remaining of this paper is as follows: Section 2 
proposes the developed methods. Section 3 shows the 
experimental results. Section 4 is the conclusions. 
 
2. The Proposed Birdsong Recognition 
System 
The block diagram of the proposed system as shown 
in Fig. 2.1 containing preprocessing, feature extraction 
and recognition (species decision) are described in detail 
in the following. 
 
2.1 Preprocessing 
 The procedure of preprocessing in this study is shown 
in Fig. 2.2, which contains four steps: improved syllable 
endpoint detection, normalization, pre-emphasis and 
segmentation. 
 
2.1.1 Improved syllable endpoint detection 
 As stated in Section 2, both time-domain and 
frequency-domain information were employed in the 
proposed approach. This approach, called the improved 
RS method (IRS), is described in the following. 
Step 1 Segment the input signal by using the RS method, 
and denote the result as x(t). 
Step 2 Compute the short time Fourier transform of x(t) 
with frame size N = 512, and form the 
spectrogram of the signal. 
Step 3 For each frame m, find the frequency Bin  
with the greatest magnitude. 
mbin
Step 4 Initialize the syllable index j, . 1=j
Step 5 Compute the frame t at which the maximum 
magnitude occurs 
  )][(maxarg
1 mMm
binXt
≤≤
= , (1) 
and set the amplitude of syllable j as 
  (dB)][log20 10 tj binXA ⋅= , (2) 
in which M is the number of frames of x(t), and 
X[⋅] denotes the spectrum of x(t). 
Step 6 Start from frame t and move backward and 
forward until frames  and  such that both jh jt
][log20 10 jhbinX⋅  and ][log20 10 jtbinX⋅  are 
smaller than )20( −jA (dB). 
Step 7 Start from frames  and , find frames jh jt
α−jh  and β+jt  (α, β > 0) such that both 
][log20 110 −−⋅ αjhbinX  and ][log20 110 ++⋅ βjtbinX  
are greater than )20( −jA . Then α−jh  and 
β+jt  are called the head frame and tail frame of 
syllable j. 
Step 8 Set  ,0][ =mbinX  
 ββαα +−++−−= jjjj tthhm ,1,,1, L . (3) 
Step 9 Let . 1+= jj
Step 10 Repeat Step 5 to Step 9 until 201−< AA j . 
 By using the above procedure, the boundaries of each 
syllable can be obtained for syllable segmentation. In 
Section 3, a comparison of birdsong recognition 
efficiencies for this method will be given. 
 
2.1.2 Normalization and pre-emphasis 
 In this study, the amplitudes were linearly normalized 
to the region of [-1, 1]. Besides, since the amplitude of 
high frequency part is usually much smaller than the low 
frequency part, so pre-emphasis is applied to intensify 
the signal of high frequency part. The intensification is 
accomplished by a finite impulse response (FIR) filter 
with the following form 
11)( −⋅−= zazH , (4) 
so that a signal x[n] after the filtering process has the 
following property 
],1[][][ −−= naxnxnx  (5) 
in which α is a scalar usually between 0.9 and 1, and was 
set as 0.95 in this study. 
 
2.2 Feature extraction – the MFCCs 
Birdsongs are non-stationary signals requiring short-
time analysis. The short time analysis for the signal 
obtained from Sec. 2.1 is accomplished by applying a 
Hamming window with size N equaling 512 samples. 
The equation of the applied Hamming window is 
 
⎪⎩
⎪⎨
⎧ −≤≤⎟⎠
⎞⎜⎝
⎛
−−=
otherwise ,                                   0
10 ,
1
2cos46.054.0
][
Nn
N
n
nw
π
 (6) 
For checking the proposed endpoint detection method 
and the DBNN classifier, MFCCs based feature 
extraction for each syllable was used to construct the 
feature vector. This extraction process is described in the 
following 
Step 1 Compute the fast Fourier transform (FFT) of each 
framed signal. 
 , (7) Nkenwnxkx
N
n
Nnkj <≤=∑−
=
− 0 ,][][][~
1
0
/2π
Step 2 Compute the energy of each triangular filter band 
JjkxkE
N
k
jj <≤= ∑−
=
0 ,][~][
12
0
2φ , (8) 
 where ][kjφ  denotes the amplitude(weight) of 
the jth triangular filter at frequency bin k as shown 
in Fig. 2.3,  denotes the energy of jjE th filter 
band, and J is the number of filters. 
Step 3 Compute the MFCCs by Cosine transformation 
( ) ( )∑−
=
⎟⎠
⎞⎜⎝
⎛ +=
1
0
10log5.0cos)(
J
j
ji EjJ
mmc π , 
m = 1, 2, …, 15, (9) 
 2
7% on the BPNN for both segmentation methods. 
Meanwhile, the IRS segmentation method also improved 
the RR of about 3% to 5% on the RS method. In addition, 
the combination of IRS segmentation method with 
DBNN achieved a RR of 64.93% evidently higher than 
the result of using BPNN and the RS segmentation 
method (54.27%). 
 
3.2 Recognition of a section of a birdsong 
 The second experiment is the recognition of a section 
of a birdsong containing several consecutive syllables. 
Meanwhile, both segmentation methods and both types 
of NN were also used for comparison. All the syllables in 
the birdsong section were recognized individually, and 
the species with the largest number of NN outputs was 
recognized as the species of the test birdsong. So in this 
case, the recognition rate RR was defined as 
%100
birdsongs all ofnumber 
correctly recognized birdsongs ofnumber (%) ⋅=RR
   (13) 
 The RRs of using all the four structures are shown in 
Table 3.2. It can be found that when the NN is fixed, the 
IRS segmentation method improved the RRs of 3.64% 
and 0.31% on RS method. When the segmentation 
method is fixed, the DBNN improved the RRs of 10.68% 
and 7.35% on BPNN. The above comparisons exhibit the 
efficiencies of the proposed methods. 
 
3.3 Recognition with threatened birdsongs taken into 
account 
 The territoriality instinct is innate for some bird 
species so that they vocalize threaten voices when 
infringed. The threaten voice is used to warn the 
companies and frighten the invaders. Such a type of 
vocalization is usually distinct from the regular birdsong 
requiring additional categorization. That is, two types of 
birdsongs are considered if the bird species also 
vocalizes threatened voices. In this case, feature 
extraction process is apply for both regular birdsongs and 
threatened voices so that a bird species was represented 
by two feature vectors. In the recognition process, the 
extracted feature vector of the test syllable (birdsong) 
was matched to both feature vectors of all species to 
determine the most likely species. 
 This experiment also dealt with the recognition of a 
section of a birdsong. The RRs of using all the four 
structures with the above distinction are shown in Table 
3.3, in which titles A and B represent the recognition 
without and with the distinction of threatened voices. It 
can be found that the RRs of case B exceeded case A of 
about 3.87% to 7.51%. 
 
4. Conclusion 
 Recordings in the field are usually in noise 
environment, incomplete or interrupted making the 
birdsong recognition much harder. To overcome these 
problems, the processes for syllable segmentation and 
classification were studied. In the syllable segmentation, 
an improvement of the RS method that simultaneously 
considers the time domain and frequency domain 
characteristics of the syllable was proposed. For the 
study of classifier, a DBNN with reinforcement learning 
rule was presented and compared to the using of BPNN. 
Two types of recognition problems were considered in 
the experiments, one is the recognition of a set of 
arbitrary syllables and the other is the recognition of a 
section of a birdsong. Experimental results show that the 
proposed methods evidently improved the RRs regardless 
of the consideration of threatened voices. 
 
References 
[1] C.K. Catchpole and P.J.B. Slater, Bird Song: Biological 
Themes and Variations, Cambridge University Press, 
1995. 
[2] S.E. Anderson, A.S. Dave and D. Margoliash, “Template-
based automatic recognition of birdsong syllables from 
continuous recordings,” Journal of the Acoustical Society 
of America, vol. 100, no. 2, pp. 1209-1219, 1996. 
[3] A. Härmä and P. Somervuo, “Classification of the 
harmonic structure in bird vocalization,” in Proceedings 
of the IEEE International Conference on Acoustics, 
Speech, and Signal Processing, vol. 5, pp. V-701-4, 2004. 
[4] S.A. Selouani, M. Kardouchi, E. Hervet and D. Roy, 
“Automatic birdsong recognition based on autoregressive 
time-delay neural networks,” in Proceedings of the ICSC 
Congress on Computational Intelligence Methods and 
Applications, pp. 1-6, 2005. 
[5] A. Härmä, “Automatic identification of bird species based 
on sinusoidal modeling of syllables,” in Proceedings of 
the IEEE International Conference on Acoustics, Speech, 
Signal Processing, vol. 5, pp. 545-548, 2003. 
[6] P. Somervuo, “A. Härmä, Bird song recognition based on 
syllable pair histograms,” in Proceedings of the IEEE 
International Conference on Acoustics, Speech, and 
Signal Processing, vol. 5, pp. V-825-8, 2004. 
[7] A.L. McIlraith and H.C. Card, “Bird song identification 
using artificial neural networks and statistical analysis,” 
in Proceedings of the Canadian Conference on Electrical 
and Computer Engineering, vol. 1, pp. 63-66, 1997. 
[8] A.L. McIlraith and H.C. Card, “A comparison of 
backpropagation and statistical classifiers for bird 
identification,” in Proceedings of IEEE International 
Conference on Neural Networks, vol. 1, pp. 100-104, 
1997. 
[9] A.L. McIlraith and H.C. Card, “Birdsong recognition 
using backpropagation and multivariate statistics,” IEEE 
Transactions on Signal Processing, vol. 45, no. 11, pp. 
2740-2748, 1997. 
[10] J.G. Wilpon, L.R. Rabiner and T. Martin, “An improved 
word-detection algorithm for telephone-quality speech 
incorporating both syntactic and semantic constraints,” 
AT&T Bell labs. Tech. J., vol, 63, pp. 479-498, Mar. 1984. 
[11] L.R. Rabiner and M.R. Sambur, “An algorithm for 
determining the endpoints of isolated utterances”, Bell 
Syst. Tech. J., vol.54, pp. 297-315, Feb. 1975. 
[12] J.C. Junqua, B. Reaves and B. Mark, “A study of 
endpoint detection algorithms in adverse conditions: 
Incidence on a DTW and HMM recognize,” Proceedings 
of Eurospeech, pp. 1371-1374, 1991. 
[13] S.N He, and J.B Yu, “A novel Chinese continuous speech 
endpoint detection method based on time domain features 
of the word structure,” IEEE International Conference on 
Communications, Circuits and Systems and West Sino 
Expositions, vol. 2, pp. 992-996, July 2002. 
[14] W.J. Zhang, and J.Y. Xie, “Endpoint detection based on 
MDL using subband speech satisfied auditory model,” 
IEEE International Conference on Neural Networks and 
Signal Processing, vol. 2, pp. 892-895, Dec. 2003. 
 4
計畫成果自評： 
1. 研究內容與原計畫相符程度、達成預期目標情況： 
   本計劃完成音節切割的改良技巧以及開發新型式分類器，對鳥鳴聲辨識率有所貢獻。 
 
2. 研究成果之學術或應用價值、是否適合在學術期刊發表或申請專利、主要發現或其他有
關價值等 
   本計劃研究成果之一部份投稿至APSCC2008國際研討會(EI)並被接受，預計於12/10會中
發表。另外研究成果亦已投稿至國際期刊IEICE(SCI)審查中。 
 
 
 6
