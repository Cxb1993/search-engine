具近紅外線影像處理功能的智慧型低複雜度臉部異物偵測與表情辨識技術系統晶片設計 
計畫編號：NSC  99－2221－E－005－115 
執行期間： 99 年 08 月 01 日 至 100 年 07 月 31 日 
子計畫主持人：范志鵬 副教授 
計畫參與人員： 張家瑋, 歐威良, 林哲立 
 
 
中文摘要 
此計畫開發出智慧型嬰幼兒臉部異物偵測與臉
部表情辨識視訊系統。在不限定背景複雜度，不限定
嬰幼兒膚色，以及無使用色彩資訊情況下，可正確偵
測出嬰幼兒臉部位置，然後擷取臉部特徵進行異物偵
測及表情辨識。研究內容主要包括臉部位置偵測、臉
部特徵擷取、臉部異物偵測以及表情辨識四個部分。
實驗結果顯示此提出的視訊系統有 91% 的眼/臉部偵
測正確率，87%的口部偵測正確率以及異物偵測正確
率。 
英文摘要 
This study presents a colorless facial foreign object 
detection algorithm and a facial expression recognition 
technique for baby care video surveillance systems, and 
the implementation with an embedded platform is also 
complete.  When babies encounter foreign object 
invasion, spit up in nose and mouth, occur vomit and 
other dangerous situations, the system immediately 
issues an alert to the guardian, and simultaneously acts 
facial expressions to judge whether babies are crying.  
Based on the colorless video processing algorithm, using 
a near-infrared camera only will function properly day 
and night, and then this video surveillance system will 
reduce the human burden effectively. 
 
中文關鍵字: 
   嬰幼兒監護, 異物偵測, 表情辨識, 近紅外線臉部
影像處理 
 
英文關鍵字: 
   Baby care, External object detection, Facial 
expression recognition, Near-infrared facial image 
processing 
 
1. 計畫簡介 
由於目前的台灣人口結構分佈越趨老化、少子
化，因此老人、嬰幼兒的即時照護就更受重視。對於
在家照顧嬰幼兒的母親、保母或在醫院的護士來說，
無法時時刻刻都在嬰幼兒身旁。對於突發狀況如：吐
奶、嘔吐或呼吸器官阻塞等情況，有時無法在第一時
間得到警訊並做處理。因此若能發展一套嬰幼兒的即
時視訊監護系統，便能隨時監測嬰幼兒的表情與臉部
異物偵測，倘若有任何的不適，可以馬上通知母親或
護士，讓嬰幼兒能得到最安全的照顧。 
 
2. 計畫內容 
如圖1-1所示為智慧型嬰幼兒視訊監護系統平台
架構圖，運作方式為：經由Web Camera將嬰幼兒的影
像擷取下來後，透過USB介面傳輸影像至Socle CDK開
發平台上做演算法處理，最後將結果輸出到平台上的
LCD Panel。 
 
圖 1-1 系統平台架構圖 
 
如圖1-2所示為智慧型嬰幼兒視訊監護系統的系
統流程圖，動作流程為:影像輸入後先經過我們所設計
的眼睛偵測演算法，去找出眼睛特徵點。接著再把人
臉部分擷取出來，之後進行異物偵測判斷，若有吐奶、
嘔吐或異物入侵口鼻等情況發生時則發出警告訊息，
否則我們的演算法就繼續偵測嘴巴與眉毛的特徵點，
接著把偵測出的這些特徵點所形成的特徵向量代入已
訓練完成的類神經系統，即可辨識出嬰幼兒的臉部表
情。 
 
圖 1-2 系統流程圖 
2.1 眼睛特徵點偵測 
大部分研究人臉特徵點偵測的論文或期刊，第一
步都會先將人臉擷取出來，接著利用人臉的區域把眼
睛、嘴巴、眉毛特徵點偵測出來，在此研究中，則省
略人臉擷取的部份，直接將眼睛特徵點偵測出來，如
此可減少運算量與運算時間。 
2.1.1 眼睛特徵點偵測流程 
本系統可由灰階影像，在複雜背景影像中將眼睛
特徵點偵測出來，如圖2-2 所示，為眼睛特徵點偵測
的流程圖。一開始先將輸入的灰階影像進行影像模糊
(平均濾波器)，接下來將原影像降取樣(Downscale)成4 
種比例大小的影像，把每一種降取樣的影像大小經過
眼睛濾波器(Eye filter)濾波後，取可能為眼睛的區域，
辨別是否為眼睛，將為眼睛的部份做水平Sobel運算子
及設臨界值二值化，接著由水平投影與垂值投影即可
偵測出眼睛8個特徵點。 
2.2 人臉擷取 
眼睛特徵點已偵測出來，人臉擷取的部份，只要
利用眼睛特徵點的相對位置，即可擷取出人臉的區
域。利用二個眼睛外側的特徵點距離(S)，當成人臉的
寬度與長度，即可框出人臉的區域，如圖 2-11 所示，
圖 2-12 為人臉擷取的結果。 
 
 
 
 
 
 
 
 
 
 
    圖 2-11 人臉擷取區域             圖 2-12 人
臉擷取結果 
 
2.3 嘴巴特徵點偵測 
由於眼睛特徵點已偵測出來，所以可以利用二個
特性來找出嘴巴位於人臉的大概區域。特性一為眼睛
與嘴巴的相對位置，特性二為嘴巴顏色的 R 色彩濃度
會比人臉膚色的 R 色彩濃度高，由濃度來找出嘴巴位
於人臉的大概區域。由人臉的大概區域經過水平 Sobel
運算子及設定臨界值將影像二值化後，再使用水平投
影與垂直投影來偵測出嘴巴的特徵點。圖 2-13 為嘴巴
特徵點偵測流程圖。 
 
圖 2-13 嘴巴特徵點偵測流程圖 
 
由以上步驟可偵測出嘴巴特徵點，如圖 2-14 為
偵測出嘴巴特徵點的結果。 
 
圖 2-14 嘴巴特徵點偵測結果 
 
2.4 眉毛特徵點偵測 
由於眼睛特徵點已偵測出來，所以可以利用眼睛
對眉毛的相對位置找出眉毛位於人臉的大概區域，因
為嬰幼兒的眉毛較為稀疏且邊緣不明顯，所以無法使
用 Sobel 運算子來偵測出眉毛的特徵點，為了解決眉
毛較稀疏的問題，首先將眼睛上方至額頭的區域做影
像等化，可將眉毛的對比度提高，接下來再找出二個
眉毛內側的座標，即可偵測出眉毛特徵點。圖 2-15 為
眉毛特徵點偵測流程圖。 
 
 
 
 
圖 2-15 眉毛特徵點偵測流程圖 
經由以上步驟可偵測出二個眉毛的特徵點，圖
2-16 為眉毛特徵點偵測結果。 
 
 
圖 2-16 眉毛特徵點偵測結果 
 
3. 表情辨識 
3.1 應用倒傳遞類神經網路  
倒傳遞類神經網路(Back propagation network ,BPN)
是目前類神經網路監督學習方式中最具代表性且應用
最普遍的學習方式之一。其學習步驟為執行時輸入值
經由輸入層及隱藏層的計算後，修正網路的加權值，
來降低網路輸出值與期望值的差距。 
3.2 特徵距離(Feature distance) 
嬰幼兒的人臉表情較為單純，所以我們主要辨識
三種表情，即無表情、笑與哭，由於每種表情所牽動
的臉部肌肉區域並不相同，因此由這三種表情中我們
觀察出情緒與相對應表情之間的關係，因而找出表情
線索，如表 3-1 所示。 
表 3-1 嬰幼兒的表情線索 
表情 嬰幼兒的表情線索 
無表情 以無表情當基準 
笑 嘴角往外拉 眉心往外拉 
哭 
嘴角下拉 
眉毛內側下壓且
兩眉心被拉近 
                                   
 
 
 
 
 
 
 
圖 3-2 特徵距離 
4.1.2 異物偵測的分析 
圖4-7為計算嘴巴附近區域經過二值化後之像素
平均值的流程圖，其中 W 為嘴巴上邊界到嘴巴下邊界
的高度，L1 為嘴巴左邊界向右加上十個像素單位、L2
為嘴巴右邊界向左加上十個像素單位、n 為 L1 或 L2
區域內的像素數目。當影像輸入後，我們先用前一章
4.4.3 節的方法把嘴巴的大概位置取出來，接著再去找
出用來判斷異物的區域，也就是此嘴巴範圍內的左及
右邊的黃色框框區域，最後將此黃色小框框區域內的
像素平均值記錄下來。 
 
 
圖 4-7 計算嘴巴附近區域經過二值化後之像素平均值
的流程圖 
 如圖 4-8 為二值化後差異值分析圖，我們使用上
述的方法統計差異值，並對日間與夜間的效果做比
較，而在圖檔左欄所代表的是嘴巴區域經過等化與二
值化的效果，而結果右欄所代表的是嘴巴差異值的記
錄，其中[左]mouth_avg 代表所選嘴巴區域內的左邊小
區塊之值也就是圖 4-7 的 L1、[右]mouth_avg 代表所
選嘴巴區域內的右邊區塊之值也就是圖 4-7 的 L2。 
 
圖 4-8 二值化後差異值分析圖  
 
4.1.3 異物偵測的演算法 
 得到每張 Frame 的平均值後，我們就可以用前張
影像的平均值做為參考與當下這張的平均值做減法運
算後取絕對值來得到一個差異值，此差異值是表示前
後張嘴巴附近區域的像素變化量，當此差異值大於某
一臨界值時，我們就認為可能有異物入侵或是嘔吐的
情形發生，便發出警告的訊號；若此差異值小於某一
臨界值時，表示目前處於安全狀態，所以將當下這張
的平均值取代為新的參考值，接著偵測嘴巴、眉毛特
徵點後做表情的辨識；如此一來我們就能夠藉由彈性
的去調整參考值來達到異物偵測的目的。異物偵測的
演算法如下式。 
 
 
其 中 diff 是 前 後 張 平 均 值 相 減 的 差 異 值 ；
object_detection 為計算灰階像素平均值的函式；當差
值大於臨界值(dth=25)時發出警告訊號（warning），
表示目前可能有危險情況發生；差異值小於或等於臨
界值(dth2=10)就更新參考值並找到嘴巴及眉毛特徵點
後做表情辨識；但若差異值大於臨界值(dth2=10)則不
去更新參考值。 
4.2 異物偵測結果 
 這邊我們使用幾個有異物入侵的影片來做測
試，首先是吐奶影片(一)，如圖 4-9。一開始還沒有出
現異物，所以在找出人臉特徵點後直接做表情的辨
識，並將辨識結果顯示在圖上右下角的地方。直到第
152 張 Frame 時發現有吐奶的情形發生，因此馬上發
出一警告標誌在圖右上角的地方。 
 
圖 4-9 吐奶影片(一) 
圖 4-10 為吐奶影片(二)的影片。剛開始一樣沒有異物
入侵，所以就做表情辨識，到第 246 張 Frame 時發現
有吐奶的情形發生，可能造成危險因此發出警告的訊
號；第 256 張 Frame 危險解除所以繼續做表情的分析。 
 
OS），其核心的運作方式則是繼承 WinNT 的技術，
可以適用在智慧型、具連接性、與精巧的裝置，例如
消費性電子產品、閘道器、工業控制器、手持行動裝
置、IP 機上盒、VoIP 電話與精簡型用戶端設備等。
目前 Windows CE 5.0（Technology Preview Kit for 
Windows CE 5.0），已發佈正式版本，。如圖 5-4 為
Windows CE 的架構圖。 
 
圖 5-4 Windows CE 的架構[30]  
5.2.2 撰寫應用於 Windows CE 的攝影機驅
動程式 
 因為我們實作的平台所搭載的作業系統為
Windows CE，而我們的智慧型嬰幼兒系統需要用到攝
影機，這時候就需要撰寫驅動程式來驅動攝影機，但
驅動程式在開發的過程中，必須明白作業系統層面與
硬體層面的關係，如此才能順利的依照其原理與流程
來撰寫驅動程式，所以我們在攝影機驅動程式這塊，
直接使用開放原始碼，如此在我們程式設計過程中只
需要專心在核心演算法的開發，而不需要明白整個作
業系統與硬體的關聯。 
 而我們所使用的攝影機開放原始資料庫為
ZX030C 模組，此模組是專門針對 Windows CE 而開
發的，但是使用此模組有個限制，就是必須使用該公
司所開發的攝影機，而本實作也需要用到近紅外光的
攝影機，所以我們也購買了該公司所開發的近紅外光
攝影機(型號為 ZCH-33 型)，下表 5-2 是攝影模組
ZX030C 的函式庫，因此在開發的過程中軟體(核心演
算法)、韌體(驅動程式)、硬體(近紅外光攝影機)三大
部分都已俱備。 
表 5-2 攝影機驅動程式函式庫 
函數名稱 函數說明 參數 應用說
明 
capInitCamera 初始化，並
傳回目前攝
影機數量 
 
void 
可以回
傳目前
有幾部
攝影機
int index 
unsigned char 
*pBufOut 
capGetCurrentVersion 得到驅動程
式版本與資
訊 
int lenOut 
可以回
傳目前
攝影機
版本資
訊 
capGetVideoFormat 得到畫面格 int index 回傳 0
int *pFormat 式和尺寸模
式 int 
*pSizeMode 
表示使
用成功
int index 
int *pFormat 
capSetVideoFormat 設定畫面格
式和尺寸模
式 
int 
*pSizeMode 
回傳 0
表示使
用成功
int index 
unsigned char 
*pFrameBuf 
capGrabFrame 擷取一張
Frame 畫面 
unsigned int 
bufferLen 
得到一
張畫面
的資訊
capStartCamera 啟動攝影機
開始擷取畫
面 
int index 回傳 0
表示使
用成功
capStopCamera 停止攝影機
擷取畫面 
int index 回傳 0
表示使
用成功
capCloseCamera 關閉攝影機 void 無 
備註:以上是會使用到的函數，使用時必須加入 zx030c.h、zx030c.lib
 
5.2.3近紅外光攝影機與WINCE5.0系統整合
應用與設計流程 
 嬰幼兒監護系統是建立在 Windows CE 核心
上，而周邊的這些硬體如攝影機，則需要對應的驅動
程式才能予以控制。在 BEGA220A 的開發平台上，已
附有 LCD panel、USB、SD card…等驅動程式，所以
我們實作時可直接使用。而攝影機的驅動也如上節所
提使用 ZX030C 的函式庫，而本章節將探討如何應用
上一節所提到的 ZX030C 函式庫。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   圖 5-5 Windows CE 與攝影機流程圖 
使用 capCloseCamera 
 
使用 capStopCamera 
使用 capGrabFrame 
使用 capStartCamera 
 使用 capSetVideoFormat
使用 capOpenCamera 
核心程式
结束
開始
關閉攝影機 
停止攝影機 
 
擷取視頻畫面
 
打開攝影機
 
設定輸出格式
 
啟動攝影機 
透過YUVPLAY的軟體可將處理完的影像直接顯示於 
BEGA220A 開發板上的 LCD 上。 
 
圖 5-12 實體系統呈現 
5.3.2 整體之效能分析 
 系統效能比較如下表 5-3 所示，左邊是純軟體在
BEGA220A 上執行預錄好的影像的結果；右邊是則是
加上攝影機擷取影像而得的結果。測試的影片大小為
320x240 共 100 個影格（Frames）；而從攝影機擷取
影像也是 100 個影格（Frames），使用的演算法都是
有異物偵測與表情辨識。 
表 5-3 系統效能比較 
Test sequence：test100.yuv（320x240）100 Frames 
Type Pure SW  Pure SW + 
Webcam 
Execution time
（sec） 
66 98 
FPS（Frame/sec） 1.5 1.02 
 
 
6. 結論 
本計畫所提出的智慧型嬰幼兒監護系統是使用無
色彩資訊演算法所以只要有一部近紅外線攝影機便可
不必在意日間與夜間影像，且可廣泛應用於醫院、幼
稚園…等小朋友活動的場所，並透過即時的分析臉部
表情狀態及異物偵測來提高嬰幼兒所處環境之安全
性，以減輕照護者負擔。 
本系統可在複雜背景、光線亮度較低及不同色溫
環境下正常的運作，且眼睛特徵點偵測的正確率約可
達到 91%、運算時間為 29ms，鼻子偵測準確率達 87%、
運算時間 1ms，嘴巴偵測準確率達 87%、運算時間為
2.1ms，至於表情辨識結果的正確率則為 80%。 
本 計 畫 最 後 將 此 嬰 幼 兒 監 護 系 統 實 現 於
BOLYMIN 公司的 BEGA220A 之嵌入式系統平台上。
當 ARM CPU 操作於 400MHz 並掛載在 Windows CE 系
統下時，其純軟體程式碼版本的影格率（Frame rate）
大約可達每秒 1.5 張，加上近紅外線攝影後，影格率
（Frame rate）也可達每秒 1.02 張。 
 
7. 計畫成果自評 
  本研究內容與原子計畫內容相符，達成約 90% 的
預期目標，已完成動態的嬰幼兒臉部追蹤與五官偵測
演算法開發，表情辨識與異物偵測的演算法開發與嵌
入式系統平台的軟體實作。此子計畫之研究成果可以
應用於嬰幼兒的智慧型視訊監護系統，本計畫的相關
研究成果已發表在會議論文中，而本年度的計畫成果
為共發表 4 篇期刊論文與 3 篇會議論文如下: 
 
期刊論文: 
[1] Chia-Hao Fang and Chih-Peng Fan , “Novel Low-Power 
Bus Invert Coding Methods with Crosstalk Detector,” 
Journal of Chinese Institute of Engineer (JCIE), vol.34, 
no.1, pp.123-139, January 2011. (SCI、EI)  
[2] Chih-Peng Fan and Chia-Hao Fang, “Efficient RC 
Low-Power Bus Encoding Methods for Crosstalk 
Reduction,” Integration, the VLSI Journal, vol.44, no.1, 
pp.75-86, January 2011. (SCI、EI)  
[3] Chen-Hsien Miao and Chih-Peng Fan, “Efficient Mode 
Selection with BMA Based Pre-processing Algorithms for 
H.264/AVC Fast Intra Mode Decision,” Springer’s Lecture 
Notes in Computer Science, Part I, pp.10-20, January 2011. 
(EI)  
[4] Chih-Peng Fan, Chia-Hao Fang, Hao-Jun Hu, and 
Wan-Ning Hsu, “Design and Analyses of a Fast 
Feed-forward Blind Equalizer with Two-Stage Generalized 
Multilevel Modulus and Soft Decision-Directed Scheme for 
High-Order QAM Cable Downstream Receivers,” IEEE 
Transactions on Consumer Electronics, vol.56, no.4, 
pp.2132-2140, November 2010. (SCI、EI) 
 
會議論文: 
[1] Chen-Hsien Miao and Chih-Peng Fan , “Efficient Mode 
Selection with BMA Based Pre-processing Algorithms for 
H.264/AVC Fast Intra Mode Decision,” The 17th 
International Conference on MultiMedia Modeling, Taipei, 
Taiwan, January 2011.   
[2] Chen-Hsien Miao and Chih-Peng Fan, “Extensive 
Pixel-Based Fast Direction Detection Algorithm for 
H.264/AVC Intra Mode Decision,” IEEE TENCON, 
Fukuoka, Japan, November 2010.   
[3] Zheng-Hong Hu, Guan-Xian Lu, Sheng-Min Yu, Chia-Wei 
Chang, Wei-Liang Ou, and Chih-Peng Fan, “Efficient Eye 
Feature and Facial Region Detection Methods for 
Baby-Watch-and-Care Aware Video Systems,” The 21th 
VLSI Design/CAD Symposium, Taiwan, August 2010.  
 
參考文獻 
[1] S. Birchfield, “An Elliptical Head Tracker,” 
Thirty-First Asilomar Conference on Signals, 
Systems & Computers, vol. 2, pp. 1710-1714, 1997. 
[2]  S. Birchfield, “An Elliptical Head Tracking Using 
Intensity Gradients and Color Histograms,” IEEE 
Computer Society Conference on Computer Vision 
and Pattern Recognition, pp. 232-237, 1998. 
[3] T. K. Leung, M. C. Burl, and P. Perona, “Finding 
Faces in Cluttered Scenes Using Random Labeled 
Graph Matching,” Fifth International Conference on 
Computer Vision, pp. 637-644, 1995. 
[4] C. Lin, and K. C. Fan, “Human face detection using 
geometric triangle relationship,” 15th International 
Conference on Pattern Recognition, vol. 2, pp. 
941-944, 2000. 
 1
 
出國報告 (出國類別 : 參加國際會議) 
 
 
 
 
 
 
 
 
參加 2010年 國際電機電子學會第10區(亞太區)
國際會議, 日本福岡 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
服務機關: 國立中興大學 電機工程系 
姓名職稱 : 范志鵬 副教授 
派赴國家 : 日本 
出國期間 : 2010 年 11 月 20 日 至 2010 月 11 月 25 日 
報告日期 : 2010 年 12 月 15 日 
 3
 
出席國際會議報告 
2010年 國際電機電子學會第10區會議 
日本九州大學 主辦 
 
2010年11月21日 至 2010年11月24日 
 
范志鵬 副教授 
國立中興大學 電機工程系 
 
1. 參加會議目的 
Tencon 國際會議為國際電機電子工程師學會(IEEE)第10區每年度於亞太地
區所舉辦的最大型綜合會議，發表的論文內容非常多元，包括通訊，資訊，
電力，半導體，信號處理，電路及系統設計等相關領域。此次參加會議的目
的為發表會議論文，以及觀摩前瞻的研究成果。 此 Tencon 國際會議為亞太地
區每年度電機電子設計領域最重要的會議之一，每一年於11月底固定在亞太地區
各國家及地區輪流舉辦，每年共約有百篇左右的會議論文將於四天的議程中發
表。 
 
2. 參加會議過程 
而本人所參與的是 11 月 22 日下午 3:10 至 4:10 的 Poster session 的論文
發表，論文編號為 T3-1.P1。此次論文的發表的題目為 
 
“Extensive Pixel-Based Fast Direction Detection Algorithm for H.264/AVC 
Intra Mode Decision”，主要探討為應用於 H.264/AVC 編碼器的畫面內
的模式決定快速演算法。而研究動機是為了增快畫面內模式決定的
速度及降低模式決定的誤差，應用擴展式基於像素的快速方向偵
測演算法的設計，同時達到減少決策時間且低編碼誤差的目的。 
 
 5
欲加速研究的方向，使得本人更加相信自己與研究生從事此一方面研究的重要
性。 另一方面，會議過程中有來自不同國家及地區的學者參與相關領域的報告，
在聽取各方的論文報告之後，對於此一智慧型視訊編碼/智慧型影像信號處理
領域的未來發展深感樂觀，特別是在智慧型視訊監控系統的研發上面。 
 
本次 Tencon 2010的會場為日本福岡國際會議中心，寬敞明亮，會議室數量
充足，值得國內舉辦相關大型國際會議的借鏡。但是美中不足之處為會議中心附
近無捷運車站出入口，到會場需要搭乘計程車或公車; 另外一個美中不足之處
為，晚宴無座位，僅有少許主桌，90%參與人員需全程站立用餐，且致詞及表演
時間太久，延誤用餐時間，而且無接泊車至晚宴會場，是我們今後辦理大型國際
會議需注意避免之處。 
 
 7
  
5.  在此會議發表的論文 
(共 1 篇, 原文請參考下頁內容) 
[1] Chen-Hsien Miao and Chih-Peng Fan, “Extensive Pixel-Based Fast Direction Detection 
Algorithm for H.264/AVC Intra Mode Decision,” IEEE TENCON, Fukuoka, Japan, November 
2010. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
the best mode can be found.  Thus, to reduce intra mode 
computation in the cost function, we can use some fast intra-
mode decision methods before the mode decision.  The 
RDcost equation is shown as follows. 
 
RateDistortiontRD ×+= λcos ,               (1) 
 
where “Distortion” denotes the sum of the absolute difference 
(SAD) or the sum of the square difference (SSD) between the 
current block and the predicted block, which has been 
computed by the intra prediction, “Rate” means the bit-rate, 
which is estimated by the number of bits required to encode 
the mode information and the residue, and “ λ ” is the 
Lagrangian coefficient, which is dependent on the 
quantization parameter (or QP).  
The Pixel-Based Direction Detection (PDD) [8] method is 
a good algorithm and it can be used to reduce the prediction 
modes. It computes all of the differences between two 
neighboring pixels directly, which correspond to four major 
directions (i.e. horizontal, vertical, diagonal-down-left, and 
diagonal-down-right textures), and calculates the average of 
all differences in each direction, and then it finds the smallest 
error strength as the dominant direction. Finally, it sends the 
direction modes (i.e. the dominant direction and two 
neighboring directions) and the non-directional mode (i.e. DC 
mode) to the mode decision. 
As to the other four texture directions (i.e. vertical-right, 
horizontal-down, vertical-left, and horizontal-up), the PDD 
method [8] uses the linear interpolation to compute the error 
strength. For instance, the error strength of the vertical-right 
direction is the average error strength of the vertical and 
diagonal-down-right directions, but the four directions can 
not be detected by the linear interpolation because their error 
strengths may be located between the other two major 
directions.  For instance, the error strength of the vertical 
direction is 10, and that of the diagonal-down-right direction 
is 20.  By using the interpolation method in [8], the predictive 
error strength of the vertical-right direction will be 15 (i.e. 
(10+20)/2 ). Thus, the error strength of the vertical-right 
direction is never the smallest one, and it will never be 
selected. Similarly, the horizontal-down, vertical-left and 
horizontal-up directions will never be chosen in [8]. 
Therefore, the accuracy of the direction detection will be 
reduced. 
III. PROPOSED FAST INTRA-MODE DECISION ALGORITHM 
In order to improve the precision of the direction 
detection in [8], we modify the previous PDD algorithm and 
propose an enhanced method, which is called Extensive 
Pixel-Based Direction Detection (EPDD) method.  It can 
detect the vertical-right, horizontal-down, vertical-left, and 
horizontal-up directions successfully.  By this method, the 
video quality is better than the previous PDD method with 
the similar encoding time reduction.  The details of the 
proposed algorithm are described as follows. 
 
A. Extensive Pixel-Based Direction Detection (EPDD) 
In the luma-4x4 predictive procedure, we calculate all of 
the differences between two neighboring pixels in each 4x4 
block, and obtain the corresponding eight texture directions 
(i.e. vertical (mode 0), horizontal (mode 1), diagonal-down-
left (mode 3), diagonal-down-right (mode 4), vertical-right 
(mode 5), horizontal-down (mode 6), vertical-left (mode 7), 
and horizontal-up (mode 8) directions), which are shown in 
Fig. 1.  It is noted that the mode 2 is the DC mode.   
 
 
 
 
 
 
 
 
 
 
Fig. 1  Error strengths of eight pixel directions in (a) vertical, (b) horizontal, (c) 
diagonal-down-left, (d) diagonal-down-right, (e) vertical-right, (f) horizontal-down, (g) 
vertical-left, (h) horizontal-up. 
 
The pixel direction strengths of all directional modes are 
expressed as 
di(mode0) = | f (x , y + 1) – f (x , y)|   ,             (2) 
di(mode1) = | f (x + 1 , y) – f (x , y)|   ,             (3) 
di(mode3) = | f (x – 1 , y + 1) – f (x , y)|  ,        (4) 
di(mode4) = | f (x + 1 , y + 1) – f (x , y)|  ,        (5) 
di(mode5) = | f (x + 1 , y + 2) – f (x , y)|  ,        (6) 
di(mode6) = | f (x + 2 , y + 1) – f (x , y)|  ,        (7) 
di(mode7) = | f (x – 1 , y + 2) – f (x , y)|  ,         (8) 
di(mode8) = | f (x + 2 , y - 1) – f (x , y)|   ,         (9) 
where x and y represent the horizontal and vertical positions 
of the pixel f (x , y) in each 4x4 block, respectively. Thus, the 
block direction error strengths are obtained by averaging all 
the possible pixel direction error strengths in each 4x4 block, 
and they can be formulated by 
D(mode0)  =  
12
1
(mod 0)
12
i
i
d e
=
∑    ,     (10) 
D(mode1)  =  
12
1
(mod 1)
12
i
i
d e
=
∑   ,      (11) 
D(mode3)  =  
9
1
(mod 3)
9
i
i
d e
=
∑    ,     (12) 
D(mode4)  =  
9
1
(mod 4)
9
i
i
d e
=
∑   ,      (13) 
D(mode5)  =  
6
1
(mod 5)
6
i
i
d e
=
∑   ,       (14) 
d1 d2 d3
d4 d5 d6
d7 d8 d9
d10 d11 d12
d1
d2
d3
d4
d5
d6
d7
d8
d9
d10
d11
d12
d1 d2 d3
d4 d5 d6
d7 d8 d9
d1 d2 d3
d4 d5 d6
d7 d8 d9
d1 d2 d3
d4 d5 d6
d1 d2
d3 d4
d5 d6
d1 d2 d3
d4 d5 d6 d1
d3
d5
d2
d4
d6
Mode 0 Mode 1 Mode 3 Mode 4
Mode 5 Mode 6 Mode 7 Mode 8
x
y
(a)                            (b)                           (c)  (d)
(e)                            (f)                             (g) (h)
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 4  Flowchart of the proposed SEPDD method 
 
B. Simplified Extensive Pixel-Based Direction Detection 
(SEPDD) 
In order to reduce the computational complexity of 
direction detection, we modify the luma-4x4 predictive 
procedure, and we can use sub-sampled resolution to do the 
EPDD method, which is called Simplified Extensive Pixel-
Based Direction Detection (SEPDD). We can partition the 
EPDD method into two steps. At the first step, we compute 
the major four directions by using sub-sampled patterns, and 
the pixel direction error strengths of d1, d6, d7 and d12, or 
d1, d3, d7 and d9, are calculated in mode 0 and mode 1, or 
mode 3 and mode 4, respectively. So we have four sets of 
error strengths instead of twelve in mode 0 and mode 1, and 
four sets of error strengths instead of nine in mode 3 and 
mode 4. After the pixel direction error strengths of major four 
directional modes are calculated, we average the error 
strengths to compute the block direction error strength of 
each mode, and then we can find the optimal mode for the 
major four directions. At the second step, we use the mode 
information which is decided at the first step to find the final 
optimal mode. The first optimal mode and two neighboring 
modes are computed with all differences, and then we can 
obtain block direction error strengths of the three modes. 
Finally, the final optimal mode which has the smallest error 
strength can be determined. The flowchart of the SEPDD 
method is shown in Fig. 4.  
IV. EXPERIMENTAL RESULTS AND COMPARISONS 
In order to verify the video quality with the proposed 
algorithm, we evaluate the encoding time (i.e. time reduction), 
the bit-rate variation, and the quality performance (i.e. PSNR) 
in the H.264 intra encoder. The prediction algorithm is firstly 
simulated by using the proposed EPDD and SEPDD 
algorithms. The proposed fast mode decision algorithms are 
implemented on the JM 14.2 software platform [9] provided 
by JVT.  For a fair comparison, we also implement and 
simulate all the compared algorithms on the same JM 14.2 
software and the same personal computer platform, where the 
CPU operates at 2.26GHz working frequency.  In our 
experimental environment, all the frames are encoded by 
using I-frame coding, and then each video sequence contains 
300 frames, and the high complexity RDO is enabled. To 
compare the rate distortion and the quality performance of the 
proposed algorithms with the previous method, the peak-
signal-to-noise ratio (PSNR) and the bit-rate (BR) are 
measured by using Bjontegaard’s method [10] for the 
quantization parameters (QPs) assigned at 28, 32, 36, and 40.  
As to the four QPs in the coding process, the simulation 
results of the average performance for the proposed EPDD, 
SEPDD and the compared methods are shown in Tables I and 
II, where Δ PSNR denotes the average difference of the peak 
signal-to-noise ratio, Δ BR indicates the average bit-rate 
increase, and Δ TIME indicates the average time reduction in 
the coding process. The performance indices for comparisons 
are defined as 
 
    Δ PSNR = PSNR_method – PSNR_ref           ,          (24) 
 
   Δ BR = BR BR
BR
_method _ref
_ref
−
×100%    ,              (25) 
 
  Δ TIME = TIME TIME
TIME
_method _ref
_ref
−
×
100%.         (26) 
 
In Tables I and II, the encoding time reduction of the 
proposed EPDD and SEPDD methods is similar to that of the 
Vertical (mode 0)
Horizontal (mode 1)
Find the first optimal mode
Diagonal Down-Left (mode 3)
Diagonal Down-Right (mode 4)
Vertical (mode 0) 
Vertical (mode 0)              Vertical-Right (mode 5) 
Vertical-Left (mode 7) 
Horizontal (mode 1)
Horizontal (mode 1) Horizontal-Down (mode 6) 
Horizontal-Up (mode 8) 
If the first optimal mode is                                    Find the final optimal mode
Diagonal Down-Left (mode 3)
Diagonal Down-Left (mode 3)             Vertical-Left (mode 7) 
Horizontal-Up (mode 8) 
Diagonal Down-Right (mode 4)
Diagonal Down-Right (mode 4)            Vertical-Right (mode 5) 
Horizontal-Down (mode 6) 
The First Step – Compute Sub-sampled Differences
The Second Step – Compute Full Differences
國科會補助計畫衍生研發成果推廣資料表
日期:2011/12/08
國科會補助計畫
計畫名稱: 具近紅外線影像處理功能的智慧型低複雜度臉部異物偵測與表情辨識技術系
統晶片設計(I)
計畫主持人: 范志鵬
計畫編號: 99-2221-E-005-115- 學門領域: 積體電路及系統設計
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 榮獲 98 學年度 國立中興大學 工學院傑出青年教師 
2. 榮獲 98 學年度 國立中興大學 電機工程系 優良教師評選 第三名 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
