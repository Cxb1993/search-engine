Abstract
Sliced inverse regression (SIR) is a renowned dimension reduction method
for finding an effective low-dimensional linear subspace. Like many other linear
methods, SIR can be extended to nonlinear setting via the “kernel trick”. The
main purpose of this project is two-fold. We build kernel SIR in a reproducing
kernel Hilbert space rigorously for a more intuitive model explanation and the-
oretical development. The second focus is on the implementation algorithm of
kernel SIR for fast computation and numerical stability. We adopt a low rank
approximation to approximate the huge and dense full kernel covariance matrix
and a reduced singular value decomposition technique for extracting kernel SIR
directions. We also explore kernel SIR’s ability to combine with other linear
learning algorithms for multi-classification and regression . Numerical experi-
ments show that kernel SIR is an effective kernel tool for nonlinear dimension
reduction and it can easily combine with other linear algorithms to form a
powerful toolkit for nonlinear data analysis.
1 Introduction
Dimension reduction is an important topic in machine learning and data mining. The
main demand comes from complex data analysis, data visualization and parsimonious
modeling [1, 2]. The most popular dimension reduction method is probably the
principal component analysis (PCA), which is an unsupervised method. In contrast,
the sliced inverse regression (SIR) [3] extracts the dimension reduction subspace,
called the effective dimension reduction (e.d.r.) subspace, based on the covariance
matrix of input attributes inversely regressed on the responses. SIR can be viewed
as a supervised companion of PCA for linear dimension reduction. SIR has won its
reputation to perform well in dimension reduction and related applications and has
gained great attention in statistical literature [2, 3, 4, 5, 6, 7]. In this project, we
reformulate kernel sliced inverse regression (KSIR) in a reproducing kernel Hilbert
space setting and emphasize on KSIR’s implementation techniques, its ability to
combine with other linear algorithms and its applications to classification as well as
regression including multiresponse regression. We adopt a low rank approximation
to approximate the huge and dense kernel covariance matrix and also introduce a
reduced singular value decomposition technique for estimating kernel e.d.r. directions
1
3 Kernel Extension of SIR and a Fast Implemen-
tation
3.1 Kernel Extension of SIR
The classical SIR is designed to find a linear transformation from the input space to
a low dimensional e.d.r. subspace that keeps as much information as possible for the
output variable y. However, it does not work for nonlinear feature extraction and it
fails to find linear directions being in the null space or having small angles to the null
space of ΣE(x|y). We, therefore, consider an alternative feature map Γ : X 7→ HK
given by
x 7→ Γ(x) := K(x, ·). (2)
Kernels used here are positive definite, also known as reproducing kernels. The goal
of KSIR is to estimate the feature e.d.r. directions hk’s. Similar to the classical SIR,
KSIR finds feature e.d.r. directions by solving a generalized spectrum decomposition
problem.
Note that the feature data K(xi, ·) and the feature e.d.r. directions hk’s are in
a high- or infinite-dimensional space HK . To store and compute the feature data
in a computer, a finite basis set {K(·, ui) : ui ∈ I} is used to represent (also to
approximate) the feature data, where I ⊂ X is a finite index set. We often take the
following
basis set : {K(·, xi)}ni=1, denoted by {K(·, A)}. (3)
Then the feature data become K = [K(xi, xj)]ni,j=1, the usual kernel matrix. It formu-
lates KSIR as a generalized spectrum decomposition of the between-slice covariance
operator in an RKHS framework. In other words, we solve the following generalized
eigenvalue problem:
ΣE(T |y)α = λΣTα (4)
After extracting the feature e.d.r. directions, we will map the feature data onto the
feature e.d.r. subspace spanned by these directions. The nonlinear structure of data
will be embedded in this feature e.d.r. subspace. Thus, a linear learning algorithm
can be applied on this feature e.d.r. subspace directly.
3
where 1′njK(ASj , A)/nj and 1
′
nK(A,A)/n are respectively the jth slice sample mean
and the grand mean of K(A,A). Note that thenWW ′ = ΣE(K|YJ ) is the between-slice
sample covariance. The sample covariance matrix ΣK is singular and is often having
much lower effective rank than its size. This low-effective-rank phenomenon causes
numerical instability and poor e.d.r. directions estimation [10]. An appropriate way
to deal with this problem is to find a reduced-column approximation to K, denoted
by K˜, so that K˜ has full column rank and its column space C(K˜) provides a good
approximation to C(K). This approximation will enhance the numerical stability
without much information loss. The reduced-column approximation will cut down
the problem size of the generalized spectrum decomposition and will also speed up
the computation.
Let P˜ be a projection matrix of size n × n˜, which satisfies P˜ ′P˜ = In˜. Given a
reduced-column kernel data K˜ := KP˜ , the approximation of KSIR is to solve the
following reduced generalized eigenvalue problem:
ΣE(K˜|YJ )α˜ = λΣK˜α˜, (5)
which is of much smaller size, as n˜ ¿ n. With the use of reduced kernel K˜, the
corresponding centered weighted slice means are given by W˜n˜×J with the jth column
w˜j =
√
nj/n
(
1′njK˜Sj
nj
− 1
′
nK˜
n
)′
,
where 1′njK˜Sj/nj and 1
′
nK˜/n are respectively the jth slice mean and the grand mean
of K˜. We can also apply Proposition 1 to the reduced problem (5) and the resulting
feature e.d.r. directions are given by V˜ = Σ−1
K˜
W˜ U˜D˜−1/2, where U˜ and D˜ are the
eigenvectors and eigenvalues for W˜ ′Σ−1
K˜
W˜ . Here, Σ−1
K˜
exists if a proper projection P˜
is used. Note that the most simple way to chose P˜ is a column subset of In. The
KSIR algorithm using a reduced kernel approximation is given in Algorithm 1.
4 Numerical Experiments
4.1 Data visualization
SIR and KSIR aim to extract a low dimensional feature e.d.r. subspace that contains
the information about output variable y as much as possible. The former looks for
5
−1 −0.5 0 0.5 1
−1
−0.5
0
0.5
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2 2
2
2
2
2
2
22
2
2
2 2
33
3
3333
3
3
3
3 3444 4
4
4
4
4
4
4
444 5
5
5
5
5
5 5
5
5
5
5
5
5
6
6
6 6
6
66
6
6 6
66
6
7
7
7 7
7
7
7
7
7
7
7
7 77
8 8
8 8
8
88
8
8
8 8
8
8
9
9
9
9
9
9
99
9
99
90 0
0
0
0
0
0
0 00
00
0
PCA−1
(a)  
PC
A−
2
−1 −0.5 0 0.5 1
−1
−0.5
0
0.5
1
11
1
1
1
1 11
1 1
11
1
2
2
2
2
2
2
2
22
2
2
2
33
3
3
3
3 3
3
3
3
3
3
3
4
4
4
4
4
4
4
44
4
4
4
44
5
5
5
5
5
5
5
5
5
5
5
5
5
66
66
6
6
6
6
6
6
6 6
67
7
77
7
7
77
7
7
7
7
77
8
8
88
8 8
8
8
8
8 8
8
9
9
99
9
9
9
9
9
9
9
9
9
0
0
0
0
0
0
00
0
0 0
0
00
SIR−1
(b)  
SI
R
−
2
−1 −0.5 0 0.5 1
−1
−0.5
0
0.5
1
1
111 1
1
11
1
1 1
2
222 22 222
33
3
3 3333 33
4
4
444444 44
444
555
5
55555 5
5
6 6
6
6 6
6
6
6
6
6
6
66
7 777777
7
7 88
8
8
88
8 88
88
999 9999 9
9
9
0000
0
0
0
0
0
0
00
KSIR−1
(c)   
KS
IR
−
2
−0.8 −0.6 −0.4 −0.2
−0.4
−0.3
−0.2
−0.1
0
0.1
1
11
1
11
1
1 1
1
1
1
1
1
2
2
22
2
2
2
22 22
2
33
3
3
33333
3
3
3
3
5
5
5
5
55
5
5
5
5
5
5
7
7
7777
7
7
7
7
7
7
99
9 9 999 9
9
9
9
99
KSIR−1
(d)   
KS
IR
−
2
8 8
8
8
88
8
8
8
88
8
8
Figure 1: 2D views of pendigits data by PCA, SIR, KSIR.
approximately linear structure in the feature space via kernel transformation, direct
application of linear learning algorithms on KSIR variates is often sufficient. In our
classification experiments, we particularly pick the Fisher linear discriminant analysis
(FDA) and the linear smooth support vector machine (SSVM) [13] as our baseline
linear learning algorithms. One property of SSVM is that it is solved in the primal
space and its computational complexity depends on the number of input attributes
(here is the number of KSIR variates, which is at most J − 1). A smaller number
of columns implies less computational load. Note that as data are projected along
the KSIR directions, discriminant analysis is computationally light. The FDA and
SSVM acting on top of KSIR variates are numerically compared with the standard
nonlinear SVM benchmark algorithm LIBSVM (Table 1 and Table 2).
4.3 KSIR dimension reduction for regression
Different from classification problems, KSIR for regression can be more complicated
than classification due to the lack of intuitive slices. In fact, we need to consider
more factors, like the number of slices, their positioning and the dimensionality of
7
Table 1: The error rate for FDA and linear SSVM on KSIR variates compared with
LIBSVM on classification data sets.
Data set KSIR+FDA KSIR+SSVM LIBSVM
mean (std) mean (std) mean (std)
banana 0.1176 (0.0039) 0.1183 (0.0028) 0.1229
tree 0.1225 (0.0027) 0.1156 (0.0023) 0.1283
splice 0.1182 (0.0041) 0.1175 (0.0036) 0.1048
adult 0.1702 (0.0011) 0.1492 (0.0008) 0.1491
web 0.0164 (0.0007) 0.0152 (0.0004) 0.0090
Iris 0.0227 (0.0064) 0.0273 (0.0066) 0.0373 (0.0034)
wine 0.0163 (0.0053) 0.0125 (0.0042) 0.0187 (0.0042)
vehicle 0.1460 (0.0091) 0.1499 (0.0103) 0.1460 (0.0072)
segment 0.0297 (0.0025) 0.0282 (0.0020) 0.0285 (0.0016)
dna 0.0652 (0.0035) 0.0447 (0.0013) 0.0463
satimage 0.0926 (0.0031) 0.0914 (0.0034) 0.0870
pendigits 0.0229 (0.0011) 0.0193 (0.0027) 0.0180
medline 0.1208 0.1136 0.1106
KSIR algorithm first maps the pattern space to an appropriate RKHS, and next
extracts the main linear features in this embedded feature space. It takes class la-
bels or regression response information into account and is a supervised dimension
reduction method. After the extraction of the feature e.d.r. subspace, many super-
vised linear learning algorithms, such as FDA, SVM, SVR and possible others, can
be applied to the images of input data in this feature e.d.r. subspace. This will
generate a nonlinear learning model in the original input pattern space and achieve
a very good performance for complex data analysis. We have also incorporated re-
duced kernel approximation to cut down the computational load and to resolve the
numerical instability due to singularity in large between-slice covariance matrix. The
singularity problem not only causes numerical instability but also leads to inferior
e.d.r. directions estimation.
A few leading components extracted by KSIR can carry most of the relevant
information about y in regression and in classification. It allows us to run linear
9
Table 3: R2 of RLS on 3 and 29 KSIR variates compared with LIBSVM on regression
data sets.
Data set KSIR(3)+RLS KSIR(29)+RLS LIBSVM
mean (std) mean (std) mean (std)
Housing 0.8619 (0.0398) 0.8611 (0.0440) 0.8680 (0.0314)
Kf 1000 0.6457 (0.0479) 0.6473 (0.0462) 0.6470 (0.0459)
CA 1000 0.9606 (0.0094) 0.9702 (0.0055) 0.9783 (0.0056)
Kin fh 0.6949 (0.0129) 0.6950 (0.0128) 0.7004 (0.0131)
CA 0.9733 (0.0047) 0.9770 (0.0031) 0.9821 (0.0022)
Friedman 0.9553 (0.0006) 0.9554 (0.0006) 0.9561 (0.0005)
Table 4: The training time (seconds) of RLS on 3 and 29 KSIR variates compared
with LIBSVM on regression data sets.
Data set KSIR(3)+RLS KSIR(29)+RLS LIBSVM
Housing 0.021 0.035 0.244
Kf 1000 0.015 0.024 0.371
CA 1000 0.016 0.042 0.502
Kin fh 1.029 1.121 19.57
CA 0.167 0.376 27.50
Friedman 6.064 6.433 2242.1
[4] C. Chen and K. C. Li, “Can SIR be as popular as multiple linear regression?”
Statistica Sinica, vol. 8, pp. 289–316, 1998.
[5] N. Duan and K. C. Li, “Slicing regression: a link free regression method,” Annals
of Statistics, vol. 19, pp. 505–530, 1991.
[6] P. Hall and K. C. Li, “On almost linearity of low dimensional projection from
high dimensional data,” Annals of Statistics, vol. 21, pp. 867–889, 1993.
[7] K. C. Li, “Nonlinear confounding in high-dimensional regression,” Annals of
Statistics, vol. 25, pp. 577–612, 1997.
11
Report on “First KES International Symposium on 
Intelligent Decision Technologies” 
(出席國際會議心得報告) 
 
會議地點： Himeji, Japan 
會議日期： April 23 – 24, 2009 
參與人員： 台灣科技大學資工系李育杰副教授 
 
會議簡介 
KES - Intelligent Decision Technologies 為資料探勘和決策分析 (decision making) 
領域知名的研討會之一。會議的內容主要著重在提供決策相關分析在理論與實務
方面的最新發展，涵蓋下列幾項：intelligent agents, fuzzy logic, multi-agent systems, 
artificial neural networks, genetic algorithms, expert systems, intelligent decision 
making support systems, information retrieval systems, geographic information 
systems, knowledge management systems。此次會議包含兩天的會議，並有多個
sessions 同時進行。 
 
心得報告 
本年度計畫主題 “非線性監督式降維技術的研發” (Nonlinear dimension reduction 
for supervised learning)，相關技術引發在資訊安全方面的應用與探討，也發表論
文 “Adaptive Alarm Filtering by Causal Correlation Consideration in Intrusion 
Detection”[1]，文章的摘要節錄如下： 
 
One of the main difficulties in most modern Intrusion Detection Systems is the 
problem of massive alarms generated by the systems. The alarms may either be false 
alarms which are wrongly classified by a sensitive model, or duplicated alarms which 
may be issued by various intrusion detectors or be issued at different time for the 
same attack. We focus on learning-based alarm filtering system. The system takes 
alarms as the input which may include the alarms from several intrusion detectors, or 
the alarms issued in different time such as for multistep attacks. The goal is to filter 
those alarms with high accuracy and enough representative capability so that the 
number of false alarms and duplicated alarms can be reduced and the efforts from 
alarm analysts can be significantly saved. To achieve that, we consider the causal 
correlation between relevant alarms in the temporal domain to re-label the alarm 
either to be a false alarm, a duplicated alarm, or a representative true alarm. To be 
more specific, recognizing the importance of causal correlation can also help us to 
Anomaly Detection via Over-Sampling Principal
Component Analysis
Yi-Ren Yeh, Zheng-Yi Lee, and Yuh-Jye Lee
Abstract. Outlier detection is an important issue in data mining and has been studied
in different research areas. It can be used for detecting the small amount of deviated
data. In this article, we use “Leave One Out” procedure to check each individual
point the “with or without” effect on the variation of principal directions. Based on
this idea, an over-sampling principal component analysis outlier detection method
is proposed for emphasizing the influence of an abnormal instance (or an outlier).
Except for identifying the suspicious outliers, we also design an on-line anomaly
detection to detect the new arriving anomaly. In addition, we also study the quick
updating of the principal directions for the effective computation and satisfying the
on-line detecting demand. Numerical experiments show that our proposed method
is effective in computation time and anomaly detection.
1 Introduction
Due to the reasons that only very few labeled data are available in real applications
and the events that people are interested in are extremely rare or do not happen
before, the outlier detection is getting people’s attention more and more [3, 4, 7, 8,
9, 11]. Outlier detection can be used in many application domains such as homeland
security, credit card fraud detection, intrusion and insider threat detection in cyber-
security, fault detection and malignant diagnosis etc. [8, 11, 12, 13]. Thus, the outlier
detection methods are designed for finding the rare instances or the deviated data.
In other words, an outlier detection method can be applied to deal with extremely
unbalanced data distribution problems, such as capturing the anomaly which exists
in a small proportion of network traffic.
Yi-Ren Yeh, Zheng-Yi Lee, and Yuh-Jye Lee
Computer Science and Information Engineering, National Taiwan University of Science and
Technology, No.43, Sec.4, Keelung Rd., Taipei, Taiwan 10607
e-mail: {D9515009,M9615018,yuh-jye}@mail.ntust.edu.tw
K. Nakamatsu et al. (Eds.): New Advan. in Intel. Decision Techno., SCI 199, pp. 449–458.
springerlink.com c© Springer-Verlag Berlin Heidelberg 2009
Anomaly Detection via Over-Sampling Principal Component Analysis 451
computation for computing the covariance matrix and estimating principal direc-
tions in LOO procedure is also proposed.
2.1 Principal Component Analysis
PCA is an unsupervised dimension reduction method. It can retain those charac-
teristics of the data set that contribute most to its variance by keeping lower-order
principal components. These few components often contain the “most important”
aspects of the data. Let A ∈Rp×n be the data matrix and each column, xi ∈ Rp, rep-
resents an instance. PCA involves the eigenvalue decomposition in the covariance
matrix of the data. Its formulation is solving an eigenvalue problem as follows:
ΣAΓ = λΓ , (1)
where ΣA = 1n
n
∑
i=i
(xi − μ)(xi − μ) is the covariance matrix, μ is the grand mean,
and the resulting Γ is the eigenvector set. In practical, some eigenvalues have little
contribution to variance and can be discarded. It means that we only need to keep
few components to represent the data. In addition, PCA explains variance and is
sensitive to outliers. A few points distant from the center would have a large influ-
ence on variance and its principal directions. In other words, these first few principal
directions will be influenced seriously if our data contain some outliers.
2.2 The Influence of an Outlier on Principal Directions
Based on the concept that we mentioned in the previous section, PCA is sensitive
to outliers and we only need few principal components to represent the main data
structure. That is, an outlier or a deviated instance will cause a larger effect on
these principal directions. Hence, we explore the variation of principal directions
when removing or adding an instance. This concept is illustrated in Fig. 1 where the
clustered blue circles represent the normal data, the red square represents an outlier,
and the green arrow is the first principal direction. From the right panel to the left
panel in Fig. 1, we can see that the first principal direction is affected when we
remove an outlier. The first principal direction is changed and forms a larger angle
between the old one and itself. In this case, the first principal direction will not
be affected and only form an extremely small angle between the old first principal
direction and the new one if we remove a normal instance. Via this observation, we
use LOO procedure to check each individual point the “with or without” effect. On
the other hand, we might have the pure normal data in hand. In this case, we use
the same concept in LOO setting but with incremental strategy. That is, adding an
instance to see the variation of the principal directions. Similarly, adding a normal
data point will create a smaller angle between the old one and itself while it will form
a larger angle with adding an outlier (from the left panel to the right panel in Fig. 1).
Anomaly Detection via Over-Sampling Principal Component Analysis 453
(a) The effect of over-sampling on normal data
dulpicated pointssingle point
(b) The effect of over-sampling on an outlier
single point dulpicated points
Fig. 2 The effect of over-sampling on an outlier and a normal instance
point and an outlier. Based on the over-sampling PCA, we make the idea discussed
in Section 2.2 more practical.
For computation issue, we need to recompute the principal directions many times
in the LOO scenario. In order to avoid this heavy loading, we also proposed two
strategies to accelerate the procedure in estimating principal directions. The first
one is the fast updating for the covariance matrix. The another one is the solving
the eigenvalue problem via the power method [6]. As (1) shows, the formulation of
PCA is solving an eigenvalue decomposition on the covariance matrix of the data.
However, it is unnecessary to completely re-compute the covariance matrix in the
LOO procedure. The difference of covariance matrix can be easily adjusted while
we only duplicate one instance. Hence, we consider a light updating of covariance
matrix for fast computation [5]. Let Q = AA
n
be the pre-computed scaled outer-
product matrix. We use the following updating for the adjusted mean vector μ˜ and
covariance matrix ˜Σ :
μ˜ = μ + r · xt
1 + r
(2)
and
˜Σ =
1
1 + r
Q+ r
1 + r
xtx

t − μ˜ μ˜, (3)
where A ∈ Rp×n is the data matrix, xt is the target instance and r is the parameter
of the proportion of the whole data in duplicating xt . From (3), it shows that we
Anomaly Detection via Over-Sampling Principal Component Analysis 455
Algorithm 2. Over-sampling Principal Component Analysis for On-line Anomaly
Detection
Input: the scaled outer-product matrix Q = AAn , the mean vector μ and the first principal
direction v of the normal data, the ratio r, and threshold h, and the new arriving instance x
Output: x is an outlier or not
1. Compute the updated mean vector μ˜ and covariance matrix ˜Σ :
μ˜ = μ+r·x1+r
˜Σ = 11+r Q+ r1+r xx − μ˜ μ˜
2. Extract the updated first principal direction v˜ and compute the cosine similarity of v and v˜
3. Check the cosine similarity of v and v˜ and see if it is higher than the specified threshold h
according to the ranking. The over-sampling principal component analysis outlier
detection algorithm (OPCAOD) for data cleaning is described in Algorithm 1.
After filtering the suspicious points, we can get the pure normal data and apply
the on-line anomaly detection which is not suitable for LOF and ABOD. Never-
theless, the quick updating of the principal directions in our proposed method can
satisfy the on-line detecting demand. In this phase, the goal is to identify the new ar-
riving abnormal instance. Similarly, we also apply over-sampling PCA for the new
arriving instance to check the variation of the principal directions. However, how
to determine the threshold for identifying an abnormal instance is a problem. In or-
der to overcome this problem, we use some statistics to set the threshold. The idea
is calculating the mean and standard deviation of the suspicious scores which are
computed from all normal data points. Once we have the mean and standard devia-
tion, a new arriving instance will be marked if its suspicious score is higher than the
mean plus a specified multiple of the standard deviation. The over-sampling princi-
pal component analysis for on-line anomaly detection (OPCAAD) is also described
in Algorithm 2.
4 Experimental Results
In our experiments, we evaluate our methods in three datasets. For the outlier detec-
tion, we first generate a 2-D synthetic data for testing our method. The synthetic data
is consisting of 200 normal instances (blue circle in Fig. 3) and 10 deviated instances
(red stars in Fig. 3). The normal data points are generated for the normal distribution
with zero mean and standard deviation 1. On the other hand, the deviated data points
are generated from the normal distribution with zero mean and standard deviation
15. Note that the clustering algorithm is not useful here because the outliers do not
belong to a certain cluster. In this 2-D synthetic data, we apply our over-sampling
principal component analysis outlier detection (OPCAOD) on it and filter 5% of the
whole data (10 points) as outliers (with black crosses). The result is shown in the
Fig. 3 and we can see the effectiveness of our proposed because of catching all the
outliers in this synthetic contaminated data. Except for the 2-D synthetic data, we
also evaluate our outlier detection method on pendigits dataset which can be
Anomaly Detection via Over-Sampling Principal Component Analysis 457
Table 2 The average cpu times of PCAOD, OPCAOD, LOF, and fast ABOD in pendigits
dataset
PCAOD OPCAOD (r = 0.1) LOF (k = 100) Fast ABOD (k = 40)
cpu time (sec.) 0.2671 0.2878 3.221 18.772
Table 3 The true positive (TP) rate, false positive (FP) rate, and error rate of KDD Cup 99
dataset. TP rate is the percentage of attacks detected; FP rate is the percentage of normal
connections falsely classified as attacks
Attack Testing data size TP FP Error
type normal attack Rate Rate Rate
Dos 2000 100 0.940 0.073 0.073
Probe 2000 100 0.980 0.022 0.023
R2L 2000 100 0.900 0.071 0.072
U2R 2000 49 0.816 0.038 0.038
For the on-line anomaly detection phase, we evaluate our method with KDD cup
99 dataset [10]. In our experiments, we focus on the 10% training subset under the
tcp protocol. We extract 2000 normal instances points as the training set and also
re-sample another 2000 normal instances and different size of attacks as our testing
set. The details are recorded in Table 3. In the beginning, we apply the data cleaning
phase to filter 100 points (5%) in the normal data to avoid the deviated data. After
that, we extract the normal pattern (the first principal direction) and use the on-line
anomaly detection to detect the new arriving attack. Note that we use the training
set and re-sample other attacks to determine the threshold. The results are shown in
Table 3. From Table 3, we can see the good performance of our proposed method
because of the high true positive rates and low false negative rates. On the other
hand, our proposed method also work well in detecting the rare attacks, like U2R.
It shows that an outlier detection method is suitable for the extremely unbalanced
data distribution. In summary, our proposed method and framework not only can
detect the outliers in the given data but also can be applied to predict the abnormal
behavior.
5 Conclusion and Future Work
We have explored the variation of principal directions in the leave one out scenario.
From the experimental results, we demonstrated that the variation of principal direc-
tions caused by outliers indeed can help us to detect the anomaly. We also proposed
the over-sampling PCA to enlarge the outlierness of an outlier. In addition, an ef-
fective computation for computing the covariance matrix and estimating principal
directions in LOO is also proposed for reducing the computational loading and satis-
