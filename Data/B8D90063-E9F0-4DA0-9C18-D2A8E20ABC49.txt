processor tiles. Due to the fast pace of the 
semiconductor scaling and the advances of system 
integration, the future Cloudlet Chip is expected to 
contain tens to hundreds of processor tiles. Given 
such abundant computing resources in a system, how to 
effectively and efficiently exploit the resources 
becomes a critical performance issue. 
 
In order to effectively support the different 
requirements of VM‘s as well as the diverse 
characteristics of multiple threads in a VM, the 
Cloudlet Server needs to enable the reconfiguration 
on the computation resources. The reconfigurable on-
chip interconnection can facilitate the appropriate 
distribution of system resources. However, the 
research results of this project shows that the 
benefits of the reconfigurable on-chip 
interconnection can be fully exploited when the 
characteristics of parallel applications are taken 
into account. The on-chip network needs to adjust the 
interconnection property based on the attributes of 
the applications. Therefore, the focuses of the 
project in the first year are categorized into two 
parts.  
 
(1) Evaluation and optimization of the data sharing 
of multi-threaded programs 
(2) Development and evaluation of the reconfigurable 
on-chip interconnection 
 
The proposed affinity-based optimization methodology 
can achieve up to 56.7% and 52.8% reductions of cache 
requests and misses on a many-core system. The 
proposed two-layer hybrid reconfigurable on-chip 
interconnection has also been proved to have shorter 
latencies for high locality traffic when compared 
with a pure mesh network. 
 
英文關鍵詞： Cloud Computing, Chip Multi-Processor, Network On 
Chip, Adaptive Design 
 
2 
 
 
一、 中、英文摘要 ...................................................................................................................... 3 
 中文摘要 .............................................................................................................................. 3 
 英文摘要 .............................................................................................................................. 3 
二、 研究計畫之背景、目的及重要性 ...................................................................................... 5 
三、 相關文獻 .............................................................................................................................. 7 
四、 研究方法 ............................................................................................................................ 11 
五、 結果與討論 ........................................................................................................................ 17 
 
 
4 
 
 
In order to effectively support the different requirements of VM's as well as the diverse 
characteristics of multiple threads in a VM, the Cloudlet Server needs to enable the 
reconfiguration on the computation resources. The reconfigurable on-chip 
interconnection can facilitate the appropriate distribution of system resources. However, 
the research results of this project shows that the benefits of the reconfigurable on-chip 
interconnection can be fully exploited when the characteristics of parallel applications 
are taken into account. The on-chip network needs to adjust the interconnection property 
based on the attributes of the applications. Therefore, the focuses of the project in the 
first year are categorized into two parts.  
 
(1) Evaluation and optimization of the data sharing of multi-threaded programs 
(2) Development and evaluation of the reconfigurable on-chip interconnection 
 
The proposed affinity-based optimization methodology can achieve up to 56.7% and 
52.8% reductions of cache requests and misses on a many-core system. The proposed 
two-layer hybrid reconfigurable on-chip interconnection has also been proved to have 
shorter latencies for high locality traffic when compared with a pure mesh network. 
 
Keyword: Cloud Computing, Chip Multi-Processor, Network On Chip, Adaptive Design 
 
6 
 
reconfigurable interconnect that can compose different numbers of Processor Tiles into 
cluster to support the diverse behavior of applications. This brings up the main research 
focus of this project. In this project (sub-project 5), we focuses on the evaluation and 
implementation of adaptive on-chip interconnection network and virtual machine 
co-processor for Cloudlet Server On Chip.  
 
 In the execution of the first year, we focus on the evaluation and design of a 
reconfigurable on-chip interconnection network. The on-chip interconnection network 
serves the Cloudlet Server by efficiently transmitting the required data of the parallel 
applications. The concurrent tasks (initiated as multiple threads) can also exchange the 
shared data or perform synchronization through the on-chip interconnection. Based on the 
research results of this project, we found out that the benefits of the reconfigurable the 
on-chip interconnection can be fully exploited when the characteristics of parallel 
applications are taken into account. The on-chip network needs to adjust the interconnection 
property based on the attributes of the applications. Therefore, the focuses of the project in 
the first year are categorized into two parts: 
 
(1) Evaluation and optimization of the data sharing of multi-threaded programs 
(2) Development and evaluation of the reconfigurable on-chip interconnection 
 
 In the first year, we have comprehensively analyzed the data sharing characteristics of 
multi-threaded applications. We proposed a application model and the associated 
optimization flow which significantly enhances the overall performance on a many-core 
system. The performance improvement is illustrated by using an advanced GPGPU, which is 
a real many-core system with SIMT (Single Instruction Multiple Threads) programming 
model. We also developed an accurate full system model for the reconfigurable 
interconnection. We use this system model to demonstrate that the reconfigurable 
interconnection can benefit the system performance. We also designed a prototype of the 
reconfigurable interconnection. These focuses will be discussed in more details in the 
following sections.  
 
 
 
8 
 
(2) Development and evaluation of the reconfigurable interconnection on many-core 
systems 
Due to the fast pace of the semiconductor scaling and the advances of system 
integration, it is expected that more cores or processing elements can be fit into the same 
chip, namely chip multiprocessors (CMPs). In order to support ever increasing data transfers 
on the chip, CMPs started adopting packet-switched networks with routers. However, the 
diverse behavior of parallel applications makes configurability a critical feature of on-chip 
interconnection. Recently, the idea of hybrid network that supports configurability has been 
proposed and was proved to be beneficial on performance as well as energy consumption.   
 
Das et. al [14] considered the locality of the application-induced communication in a 
CMP. Applications executed on a large CMP system would benefit from intra-cluster 
communication. In this work a hierarchical interconnect network was introduced (Fig.2(a) 
and Fig.2(b)). This two-layer topology consists a local network implemented by a fixed bus 
and a global network adopted mesh network. The hybrid topology can efficiently support a 
system situation where most communication in CMP applications is limited to the local 
network. Thus, using a fast, low-power bus to handle local communication would improve 
both packet latency and power-efﬁciency. The experimental results showed the performance 
benefit when there exists data locality with low message injection-rate. The product of 
power and latency is overwhelmed to traditional interconnect. 
 
   
Figure 2(a): Hybrid topology [14]     Figure 2(b):Transaction processing [14] 
 
In the work of [15], the system partitioned a two-dimensional mesh into several 
sub-meshes and connecting them using a global interconnect (Fig.3). This design could 
reduce the average number of hops for global traffic. This work consists of local network 
using small scaled mesh topology routers, and global ones using simple rings. The paper 
chose the hierarchical ring mainly due to its simplicity, speed and efﬁciency in embedding 
onto a circuit layout. It is also more suitable for efficient cache coherent protocols. The 
experimental results showed positive effects on hop count and latency reduction on overall 
interconnect when compared with conventional mesh networks.  
 
10 
 
 
Figure 4(c): The same network can be configured to different topologies [16] 
 
Most of the previous works on reconfigurable networks focused on systems with 
losely-coupled IPs. These IPs are working independently and have no data sharing between 
each other. This is not the assumption of this project. In this project, we focuses on Cloudlet 
Server On Chip where Processor Tiles can not only initiate individual data transactions, but 
also perform data exchange and synchronization with other Processor Tiles. In this case, the 
data access behavior and locality characteristics become important design issues that can 
significantly impact the overall performance and energy consumption.  
 
In this project, we plan to propose a two-layer hybrid network on chip, which is similar 
to [14]. The local layer adopts a single transaction bus, while the global layer uses mesh 
network routers. However, different from the design of [14], both the local and global 
network can be configured toward the behavior of the parallel applications on Cloudlet 
Server. The configurable topology and forwarding mechanism can be optimized for the 
applications. This design enables shorter latency, better performance, as well as power 
reduction.  
12 
 
sixteen threads, there require total sixteen memory accesses for the SIMT group due to the 
scattered addresses. Figure 5(b) illustrates the case when the coalesced memory access is 
applied. Once the referenced data are arranged into consecutive addresses, all the memory 
accesses will be packed into only one transaction.  
 
Cache Contention is an important design issue of Chip Multi-Processor (CMP) in 
which L2 cache and the lower memory hierarchy are shared among cores. Figure 5(c) and 
5(d) illustrate examples of how the cache contention impairs the memory system 
performance by introducing numerous cache misses. As shown in Figure 5(c), the L1 cache 
size is large enough to contain one thread. However, when multiple concurrent threads are 
using the same local cache (shown in Figure 5(d)), the size of the cache may be insufficient 
to support all the threads. These threads therefore start competing with each other for the 
insufficient cache resource. Consequently, numerous undesired capacity and conflict cache 
misses are generated.  
 
The following paragraphs use some widely adopted EDA applications, such as logic 
simulation and fault simulation, to demonstrate the construction of TCG models. This type 
of application has irregular memory access behavior due to the diverse topologies of 
different circuits.  
 
(b) Model of Parallel Applications 
In this project, the communication of concurrent threads is modeled as a directed 
acyclic graph, Thread Communication Graph, TCG=(V, E) in which V and E are the set of 
vertices and edges respectively. The left half of Figure 6 shows an example of TCG. A vertex 
in a TCG represents an individual thread and an edge stands for the data communication 
between threads. The TCG can show the execution parallelism of an application as well as 
the data communication between threads.  
 
 
Figure 5. Coalesced memory access and cache contention. (a) Non-coalesced memory 
access. (b) Coalesced memory access. (c) Single-thread execution without cache 
contention. (d) Multiple-thread execution with cache contention. 
14 
 
coalesced as a single memory transaction. From the fold (ii) of Property 1, the dependent 
memory volume of a warp can be calculated by counting the number of different dependent 
warps. This is because the dependent data can only reside in the groups which are connected 
by the in-edges.  
 
 
 
According to Property 1, the dependent memory volume of a group Gi with respect to 
Gj (denoted as vol(Gi, Gj)) can be calculated as illustrated in equation (1). In equation (1), 
vol(Gi, Gj) is equal to 1 if there exists an edge e(u, v) in the edge set E which satisfies the 
situation that vertex v belongs to group Gi and u belongs to Gj. Otherwise, vol(Gi, Gj) is 
equal to 0. 
 
As represented in equation (2), the total number of dependent memory volume of Gi 
(denoted as totvol(Gi)) can be obtained by accumulating the dependent memory volume with 
respect to all the groups. The total number of dependent memory volume of a parallel 
 
Figure 7. The example of thread affinity mapping with difference metrics. Even the 
inter-group edge of W2 is minimized in (a), the dependent memory volume can still be 
reduced by swapping the threads tA and tB as in (b). 
16 
 
a system. The third part is the multi-core cache coherence protocol, which maintains 
coherence message and coherence state in a cache.  
 
 We built the proposed reconfigurable hybrid interconnection network by modifying 
gem5’s pre-defined interconnection model. We tested our interconnection model by using 
packet generators. Packet generators generate on-chip data traffics with different 
characteristics and patterns. The overall average performance of the interconnection is 
collected and compared with the default interconnect which adopted mesh network.  
 
 
 
Figure 8(a):  Ruby memory system 
 
 
 
Figure 8(b): System model of the multi-core architecture 
18 
 
considered. The non-optimized coalescing on warp level generates enormous memory 
accesses. Due to the heavy cache traffic, the highly communicating threads can hardly fit in 
a local cache and result in severe cache contention. With the exploitation of the memory 
coalescing, the (minvol, none) mapping reduces the number of cache requests to 43.3% in 
average. At the same time, the reduced access traffic also helps to reduce the cache misses to 
47.2%. Particularly, the minvol metric reduces more cache misses in the larger test benches 
(b17 ~ b19). This is because more threads tend to compete for the same fixed size cache, and 
the affinity-agnostic mapping produces even poorer performance. Finally, the (minvol, 
mincut) mapping considers both the warp and block level thread affinity. The experimental 
results have showed that mincut metric can further mitigate the cache contention with the 
minvol metric and produces the best cache performance. Similar to the effect of memory 
coalescing, the (minvol, mincut) especially helps to the large test benches because more 
threads tend to compete for the limited cache capacity. In summary, we demonstrate 56.7% 
and 52.8% reductions of cache requests and misses by thread affinity mapping with minvol 
metric. Furthermore, the reductions can be enhanced to 61.2% and 62.4% when considering 
both the minvol and mincut metric during mapping. 
 
(2) Development and evaluation of the reconfigurable on-chip interconnection 
 We evaluated the two-layer hybrid interconnection by using by packet pattern 
generators (referred as PG's). A processor cluster is formed by connecting four PG's with a 
shared bus (Fig.9(a)). Every cluster is connected to a mesh router (Fig.9(b)). Routers are 
connected in a mesh topology. The PGs generate both local and global data traffic. In this 
hybrid network, the local communication is within the same cluster. The data 
communication is through the local shared bus. The global traffic passes between different 
clusters, and is forwarded be the router of the mesh network.  
                                 
Figure 9(a): A processor cluster    Figure 9(b): Each cluster is connected to a router (R) 
 
Figure 10: 4X4 Processor Tiles in a mesh network 
20 
 
(3) Conclusions 
 The research progress in the first year of the project can be summarized into two parts. 
First, the affinity mapping of threads can significantly enhance the locality of local cache, 
and improve the performance of a many-core system. The proposed hybrid interconnection 
can leverage the locality of the application and show better performance of the on-chip 
interconnection.  
 
 The immediate next step of this project is to integrate the idea of affinity mapping on 
the hybrid on-chip interconnection. We have also implemented a functional prototype of the 
reconfigurable interconnection switches. However, the long latencies of the prototype 
degrade the benefit of the hybrid interconnection. We are currently optimizing the design the 
prototype, and this is indeed one of the main challenges in our next step.  
 
 In the second year, this project will proceed on collaborating with other sub-projects 
and develop a prototype of Cloudlet Server on an FPGA. The main goal is to extend the 
fundamental design concept and methods to a more real system models, and perform 
comprehensive analysis on the impact of the overall system.   
 1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：101 年 2月 27 日 
                                 
一、參加會議經過 
第十七屆亞太區設計自動化國際會議(17th Asia and South Pacific Design 
Automation Conference)是於 2012/1/30~2012/2/2 日在在雪梨會議及展覽中心
(Sydney Covention and Exhibition Centre)舉辦，本屆大會主席由新南威爾斯大
學(University of New South Wales)的 Sri Parameswaran 教授出任，而技術議程
主席則是由國立台灣大學(National Taiwan University, Taiwan)的 Yao-Wen Chang
教授擔任。此會議屬於全亞太區交流的國際會議，並收錄了從 288 篇投稿中選出的
計畫編號 NSC 100-2220-E-009-039 
計畫名稱 綠色微雲伺服系統晶片上適應性網路與虛擬化加速器之硬體設計 
出國人員
姓名 
郭玹凱 
服務機構
及職稱 
國立交通大學電子工程研究所 
會議時間 
101年 1月 30日
至 
101年 2月 2日 
會議地點 澳大利亞-雪梨 
會議名稱 
(中文) 第十七屆亞太區設計自動化會議 
(英文) 17th Asia and South Pacific Design Automation Conference 
發表論文
題目 
(中文) 共用快取記憶體圖形處理器中分規律記憶體存取知執行緒關
聯性對應 
(英文) Thread Affinity Mapping for Irregular Data Access on 
Shared Cache GPGPU 
附件四 
 3 
的最新研究狀況並能致力於最新研究議題的突破。此外，亦同時提供博士研究生與
他國研究團隊跨國跨領域的合作已達成雙贏格局。 
四、攜回資料名稱及內容 
1. 17th Asia and South Pacific Design Automation Conference 議程手冊。 
2. 17th Asia and South Pacific Design Automation Conference 論文光碟。 
100 年度專題研究計畫研究成果彙整表 
計畫主持人：賴伯承 計畫編號：100-2220-E-009-039- 
計畫名稱：GreenArmy：綠色微雲伺服系統晶片平台技術--子計畫五：綠色微雲伺服系統晶片上適應性
網路與虛擬化加速器之硬體設計(1/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
兩篇國際會議論
文 分 別 發 表 於
ASP-DAC'12 和
VLSI-DAT'12, 均
為系統設計與自
動化領域中的國
際知名會議。 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：□已發表 □未發表之文稿 □撰寫中 ■無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
本計畫在此年度預計發表 1~2 篇國際會議論文，今已有 2篇國際重要的研討會論文被接
受，其中一篇已於 2012 年 2 月於澳洲雪梨發表。而在技術創新方面，也與計畫書中的規劃進
度相同，完成初步的適應性網路原型設計。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
一、學術成就 
   (一)本研究計畫之初步成果已證明，在晶片多核心系統中，可適應性的晶內網路可以
有效的增加多執行緒的系統效能。對於多執行緒的平行應用，本研究計畫也提出了一套軟
硬體整合的最佳化方式，目前正在撰寫報告，預計近期投搞至國際知名期刊。 
   (二)本計畫依據研究需求，組成晶片多核心系統的研究團隊。此團隊對於晶片多核心
系統的模型設計，已有一定的熟悉度，並具備建立不同晶片多核心系統關鍵模組的模型之
能力，並能對此模型進行軟硬體整合的系統效能驗證。 
 
二、技術創新 
   (一) 本研究利用多核心系統模型證明了可適應性網路的優勢，已完成初步的可適應性
網路原型的設計，並在邏輯閘（Gate Level）階層實現。目前正在進行此原型的改良，並
試著與多核心系統進行整合。 
 
三、檢討與展望 
   本年度目前研究執行，大致與計畫書中規劃的進度吻合。展望未來，希望能夠配合總
計畫，與其他子計畫充分合作，並以以下項目為主要目標。 
