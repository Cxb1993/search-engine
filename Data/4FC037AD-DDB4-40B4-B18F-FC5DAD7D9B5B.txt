 1
???? 
  Lempel-Ziv??????????????????????????????? GIF
? TIFF?Lempel-Ziv???(dictionary)??????????????????????
???????????????????????????????????????
????????????????????????Lempel-Ziv??????????
???????????????????????????????Lempel-Ziv???
???????????????????????????????????????
??????????????????????????????????? 
  Storer? Reif????????? Lempel-Ziv??????????????????
??????????????? k-error ??????????(hash function)????
???????????????????????????????????????
?????(collision)??????????????????????????(perfect 
hashing)? k-error ???????????????????????????????
??????????????????????????? Lempel-Ziv???????
???????????????????????????????????????
??? 
????Lempel-Ziv???????k-error??????? 
 
Abstract 
Lempel-Ziv scheme is a lossless data compression algorithm. It is found in several image file 
formats, such as GIF and TIFF. Lempel-Ziv is a dictionary-based compression. The key insight 
of the method is that it builds a dictionary of previously seen phrases in the text being 
compressed. When decoding, the dictionary can be built in the same way the encoder does. 
Lempel-Ziv goes beyond most dictionary-based compressors in that it is not necessary to 
preserve the dictionary to decode the data stream. However, the Lempel-Ziv code is very 
vulnerable on an error-prone communication channel. Even a single error will cause 
synchronization loss at the decoder’s dictionary, and possibly all decoded symbols after the error 
turn out faulty. 
Storer and Reif have studied the error resilient Lempel-Ziv algorithm and showed that it gives 
protection against error propagation. The algorithm uses hash function to eliminate the 
dependence between addresses in the dictionary. But they left an open problem in the error 
resilient Lempel-Ziv algorithm with k-error protocol. The hash conflicts affect the compression 
performance. In this research, we will implement a k-error protocol Lempel-Ziv algorithm with 
perfect hashing. By using a simple perfect hashing scheme, the hash conflicts can be easily 
removed and the compression performance can be improved. Besides, the error resilient 
Lempel-Ziv data compression scheme can be conducted for alleviating the error propagation 
problem in transmitting data or images over noisy channels, such as wireless applications and 
mobile systems.  
Keywords: Lempel-Ziv code, error resilient, k-error protocol, perfect hashing 
 3
  In this research, a perfect hashing scheme is employed to avoid hash conflicts. By combining 
the perfect hashing scheme and the k-error protocol skillfully, an efficient error resilient LZ 
algorithm is studied. 
 
 
2. The Proposed Algorithm 
  In 1991, Chang et al. [7] proposed an approach to be used for assigning keywords to addresses 
in the way that no two different keywords are assigned in the same address. This perfect hashing 
scheme can be used for searching reserved words in compilers, function names in operating 
systems, and so on. Assume that we have a set of keys and two characters ki1 and ki2 are extracted 
from each key ki such that all extracted pairs (ki1, ki2)s are distinct. Then the assigned address of 
each key is determined as following form: 
h(ki) = f1(ki1) + f2(ki2), 
where f1 and f2 are two integer-valued functions defined on the set of distinct ki1s and the set of 
distinct ki2s, respectively. This scheme showed that if all the extracted pairs (ki1, ki2)s are distinct 
then h(ki) guarantees perfect. 
  Now the k-error protocol and the perfect hashing scheme are combined to yield an 
error-resilient and efficient LZ algorithm. When encoding, the pointer pairs are output as Storer 
and Reif’s algorithm. Then the distinct pairs are extracted and the hash functions f1 and f2 are 
established. In addition, the mapping function g between the dictionary and the hash table is 
constructed. When decoding, the hash functions and the mapping function are used to reconstruct 
the right hash table. 
The details of our encoding and decoding algorithms are described as follows. 
------------------------------------------------------------------------------------------------------- 
I. Encoding algorithm 
1. At the start, dictionary D is empty and pointer P is 0. 
2. Read next character C from the input sequence. 
3. If the string S = D[P] + C presents in dictionary D and count(P, C) ≥ b 
then assign the index of S to P; 
else { 
  if count(P, C) = 0  
  then { 
    Add the string S to dictionary D; 
    Output pointer pair (P, C); 
    count(P, C) = 1; 
    P = 0; 
  }  
  else { 
    Output pointer pair (P, C); 
 5
 
Table 2 
Dictionary D 
Index Phrase 
1 A 
2 B 
3 C 
4 BB 
5 BBC 
 
  Once the pointer pairs are found, the distinct pairs (0, A), (0, B), (0, C), (2, B), (4,C) are 
extracted for constructing the hashing functions f1 and f2. Firstly, the EP-matrix E is constructed 
as follows. 
  CBA
E =
0
2
4 







100
010
111
.
Then the compressed matrix C of E is found. 
  CBA
C =
0
24 


110
111
.
We eventually obtain f1(0) = 0, f1(2) = f1(4) = 1, and f2(A) = 1, f2(B) = 2, f2(C) = 4. After the hash 
functions are established, hash table H as shown in Table 3 can be constructed using the 
following algorithm. 
For each distinct pointer pair (P, C) 
{ 
 Index = f1(P) + f2(C); 
 H[Index] = D[P] + C; 
} 
 
 
 7
Table 4 
Decoding process 
Step P C Output 
1 0 A A 
2 0 B B 
3 0 B B 
4 0 A A 
5 0 B B 
6 0 C C 
7 0 A A 
8 2 B BB 
9 2 B BB 
10 2 B BB 
11 4 C BBC 
 
 
 
3. Experimental Results 
  In this section, we will try to study the hash-conflict effects on compression performance with 
a few experiments. A file set from Calgary Corpus (http://corpus.canterbury.ac.nz/) is tested. It 
has been developed specially for testing some new compression algorithms. The files were 
selected based on their ability to provide representative performance results. 
  In order to compare the compression performance produced by these LZ algorithms, data 
compression ratio is used to quantify the reduction in data quantity. The data compression ratio 
(CR) is determined by  
CR = 
data compressed  theof size
data original  theof size . 
For example, a 10MB file that compresses to 2MB would have a 10/2=5 compression ratio. It 
also means that the data occupies 20 percent of its original size after compression. In addition, we 
have implemented three algorithms for compression performance comparisons, including generic 
LZ algorithm (LZ), k-error protocol LZ algorithm (k-LZ), and k-error protocol LZ algorithm with 
perfect hashing (k-LZ-perfect). The hash function employed in k-LZ algorithm is a 2-universal 
hash function. It is defined as ((31891*C+P)%139663)%TableSize. In this hash function, (P, C) 
denotes a pointer pair, both 31891 and 139663 are prime numbers, and TableSize indicates the 
hash table size. 
 9
ratio, especially on the large test files. For example, the compressed data is about 43 percent of 
the original size of the “bible.txt” file using k-LZ algorithm. But the compressed size will be 32 
percent of the original data using k-LZ-perfect algorithm. The k-LZ-perfect compression has 
resulted in a saving of 11 percent. In addition, the compressed data is about 51 percent of the 
original size of the “world192.txt” file using k-LZ algorithm and the compressed size will be 37 
percent of the original data using k-LZ-perfect algorithm. The k-LZ-perfect compression has 
resulted in a saving of 14 percent. 
  In Table 6, we present the performance comparison for b = 3 and TableSize = 32768. In 
addition, the performance comparison for b = 3 and TableSize = 16384 is shown in Table 7. We 
can expect that the compression ratio of k-LZ algorithm to be reduced due to the small table size, 
especially on the large test files (bible.txt and world192.txt). As for the small test files, the 
reduction of the compression ratio is not obvious. On the contrary, the compression ratios of LZ 
and k-LZ-perfect algorithms increase with the decreasing table size. The reason is that the number 
of bits in some pointers is reduced. 
Table 7 
Performance comparison for b = 3 and TableSize = 16384 
Files Original LZ k-LZ k-LZ-perfect 
 Size Compressed CR Compressed CR Compressed CR 
bib 111261 47006 2.37 67196 1.66 60333 1.84 
book1 768771 269897 2.85 420070 1.83 323270 2.38 
book2 610856 211197 2.89 341796 1.79 261273 2.34 
paper1 53161 26897 1.98 36886 1.44 34837 1.53 
paper2 82199 37331 2.20 50522 1.63 46931 1.75 
news 377109 159083 2.37 243285 1.55 198453 1.90 
bible.txt 4047392 960771 4.21 1807298 2.24 1141085 3.57 
world192.txt 2473400 633949 3.90 1342188 1.84 800283 3.09 
 
From these experiments, we observed that hash conflicts affect the compression performance 
in k-LZ algorithm indeed. This effect is particularly obvious when the hash table size is small. 
Furthermore, the compression ratio of these LZ algorithms tends to grow consistently with the 
size of test file. In practice, these LZ algorithms work well only when the input data is 
sufficiently large. These results verify Reznik and Szpankowski’s analysis [8] for the expected 
redundancy rate of the k-error protocol LZ code. The average redundancy rate decreases with the 
size of input data, and the dependence upon the constant b is much less obvious. 
 
 
 11
?????? 
  ?????????????????Lempel-Ziv??????????? Lempel-Ziv
??????(dictionary)??????????????? Lempel-Ziv ????????
???????????????????????????????????????
???????????????????????????????????????
???????(perfect hashing)? k-error ?????????????????????
???????????????????????????????????? 
  ??????????????? Lempel-Ziv????????? Lempel-Ziv????
???????????????????????????????????????
?????????(? GIF? TIFF)???????????????????????
??? 
