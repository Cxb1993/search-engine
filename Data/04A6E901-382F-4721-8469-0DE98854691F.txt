 2
1 中文摘要 
隨著程式語言的蓬勃發展，Java相關的應用越來越廣泛，其具備有簡單、安全、
跨平台、物件導向、多執行緒...等優點，其中跨平台的特性（“Write once, run 
anywhere.＂）更是廣受歡迎的最主要原因，透過Java虛擬機器（Java Virtual Machine, 
JVM）的技術，使得相同的程式可執行於不同的平台上，達到高可攜性；然而實
現跨平台所換來的代價則是效能降低，其原因在於Java被執行前會被轉譯為中間語
言（Bytecode），再透過各平台上的JVM執行，而為了達到較高的相容性、容易移
植，Bytecode指令集採堆疊型式運作（Stack-Based Operation），因此指令集複雜度
較低，可輕易地轉譯於各平台上，但效能卻也隨之下降。近來Java在各種硬體平台
或電子裝置上的應用越來越普及，已有不少研究針對JVM的效能進行改善，讓使
用者能夠真正的感受到Java所帶來的便利性。 
 
 
ABSTRACT 
Along with rising implementation of programming language , Java applications 
with simple ,safe, cross-platform , object-oriented and multithread and so on are rapidly 
growing in popularity . Especially , “Write once, run anywhere” depicts the reason 
popular used. The technology of Java Virtual Machine(JVM) making the same 
programs allowed to be executed on the different platform are highly portable but make 
the performance worse instead. In order to achieve better compatibility and colonise , 
Java will be translated into intermediate code (Bytecode) then performed via JVM of 
each platform. Because the instruction of Bytecode is Stack-Based Operation , it is easy 
to translate on each platform and complexity of instruction set is low but have 
performance worse .Therefore researcher improve the performance of JVM to facilitate 
Java for user.  
 
 
 
 
 4
describe the thread mechanisms in the section III. Section IV shows the experimental 
results, and finally conclusions and the future work are given in section V. 
 
 
3. SW/HW Co-Designed MTJVM 
The architecture of our MTJVM is shown in Fig. 1, including the software part 
(multi-thread programs and the class loader) and hardware part (PEs and the thread/ 
memory managers), which will be described in details in the following subsections. 
IQ
ue
ue
H
ar
dw
ar
e T
hr
ea
d 
C
on
tro
l*P
C
*
In
st
ru
ct
io
n*
M
em
or
y 
ac
ce
ss
*
 
Fig. 1. MTJVM system architecture. 
 
 6
In step 3, the set_stack_frame instruction contains an immediate value in two bytes, 
which is used to record the usage of arguments and local variables needed by a method. 
This information will be used to set a new stack frame (to update some stack pointers) 
when invoking a method at runtime.  
Another issue of method invocations or field accesses is the addresses resolution. 
As we known, a class file keeps all its symbolic references in one place, the constant 
pool. Without resolving, these indirect symbolic references lead to poor performance. 
Due to this reason, some tables are built in step 5 and used for resolving. The indirect 
symbolic references of method invocations or field accesses will be replaced with their 
direct references and offsets by the loader.  
According to many similarities, bytecodes of JVM can be classified as many types 
to speed up instruction decoding and to reduce the MTJVM hardware area. So that we 
re-encode the bytecodes OP codes in step 6 to transfer the design complexity of the 
decoding unit from hardware to software, ie., the loader, and make the hardware virtual 
machine executing fast.   
Finally, another important step in the proposed class loader is to map the 
thread-related primitives described above to newly implemented thread hardware 
instructions, shown in Table 1., for hardware support. 
 
 
Table 1. Thread Instructions 
Thread Primitive Thread Instruction Byte(s) Notation 
New Thread 
Thread.init() 
Thread_init {ID N, Offset [0]} 2 
Thread.start() Thread_start {Priority P, ID N } 2 
yield() Thread_yield 1 
wait (long 
milliseconds) 
Thread_wait {Time Info} 3 
sleep (long Thread_sleep {Time Info} 3 
 
 
P: 3-bit 
(0~7) 
 
N: 4-bit 
 8
the slow stack, for getting the higher performance. A half of the register file is used 
as a run-time stack, and the other part is taken as the context switch buffer. Stack 
data accesses are handled by a stack pointer (Top of Stack, TOS) with auto 
increment and decrement. Local variable accesses are handled by an indirect register 
addressing mode, using a fixed pointer (Bottom of LocalVariable, BOL) and an 
offset to get proper values.  
‧ Execution:  Uses fetched operands to do integer arithmetic. Floating point 
calculations are emulated by software. 
‧ Memory Access: 
This stage contains a shared 32-bit memory (composed of four 8-bit memories, 
4-bank * 8-bit) and a memory manager, shown in Fig. 2, that has following features: 
1. Handles the memory access for each PE. 
2. Manages the heap usage and the method’s stack. 
3. When accessing a thread’s memory space, an address mapping unit handles the 
address resolution with a thread ID and an offset. 
4. A memory enabling mechanism is bounded to each PE for supporting non-word data 
accesses. 
 
 
 
 
 
 
 
 
Fig. 2. Memory Manager 
Heap
Arbiter
Heap_Ask*
Heap_Grant*
Thread’s Stack、
Heap Manage
Address
Mapping Unit
O
ffs
et
*
H
ea
p_
Po
in
te
r
A
dd
re
ss
*
Memory
Enable
m
em
_w
rit
e*
m
em
_t
yp
e*
ad
ju
st
m
en
t*
Memory Manager
M
ap
pe
d_
Ad
dr
es
s*
M
em
or
y_
En
ab
le
*
ID
*
Method_Stack_Pointer*
lo
ad
_p
c*
st
or
e_
pc
*
pc
_f
ro
m
_c
on
te
xt
*
Extended Hardware 
for TLP Support 
D
at
a 
A
dj
us
tm
en
t
M
em
or
y_
D
at
a*
D
at
a*
8 4 5 32 32
32432
Memory
PE.MA Stage
 10
requests from each thread. At the same time, only the highest priority thread can access 
the shared resource (set). 
3. State Sets: Several state sets as shown in Table 2, formed by registers, record the 
blocked threads’ information. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 4. Thread manager. 
 
 
Wait Set
Wait Set
Arbiter
Sleep Set
Sleep Set
Arbiter
Ready Set
R
ea
dy
 S
et
 
P
ha
se
1 
A
rb
ite
r
R
ea
dy
 S
et
 
P
ha
se
1 
A
rb
ite
r
R
ea
dy
 S
et
 
P
ha
se
2 
A
rb
ite
r
Synchronization
Set
Monitor
Exit
Arbiter
Enter
Arbiter
Thread Control Unit
Thread Manager PE(s)
Thread-Dispatch Bus
Thread 
Control
State Set
Arbitration
 12
 
 
 
 
 
 
Fig. 5. Thread frame structure. 
 
 
 
 
 
 
 
 
 
 
Fig. 6. Thread life cycle. 
 
Always 00
Reference1
2
3
…
31
…63
…
33
32
Thread’s Status0
Thread’s Reference1
2
3
…
31
Heap
Thread N
Frame
…
Thread 0
Frame
Th
re
ad
’s
Sp
ac
e
Memory
PE Register File
(Run-time Stack)
Th
re
ad
’s
 
C
on
te
xt
Th
re
ad
’s
 
St
ac
k
…
Ready
Running
Sw
itc
h
Sc
he
du
le
End
Dead
Thread
initializing
Start
start ( )
implement runnable
Sleeping
Waiting
Synchronizing
sleep( )
wait( )、IO
monitorenter( )
Locked
‧monitorexit ( )
‧lock failed
‧time up
‧interrupt
‧time up
‧interrupt
‧notifyall( )
N
ew
R
un
na
bl
e
D
ea
d
Blocked
 14
another thread. This mechanism is handled by the context-switch controller, CSController, and 
run in background when the memory is idle. 
 
 
 
 
 
Fig. 7. Pre-load mechanism with CSController. 
B.3 Blocked Threads 
A blocked state can be at one of 3 sub-states: sleeping, waiting, and synchronizing. 
When a thread is asked to sleep, it sleeps in the sleep set until time up or interrupts 
occur. When a long-term access or wait( ) occurs, the thread waits in the wait set until 
the event finishes. Then, unblocked threads will go back to the ready set and are cleared 
from the sleep/wait set. 
The synchronization mechanism in Java is handled by a monitor, used as a 
mutually exclusive lock. When a thread enters the synchronized code section, it starts to 
acquire a lock. If the synchronization set returns a matched reference, then the thread 
fails to lock and goes back to the ready set. Otherwise, thread locks successfully and 
keeps going. 
 
B.4 Dead Threads 
When executing the stop()/destroy() methods, all atomic-operations within 
synchronization will be forced to terminate. This can cause an unsafe behaviour. Hence, 
we prefer that threads die naturally in our system. 
 
 16
 
 
 
 
 
 
Fig. 8. The speedups for 2 design examples 
 
The first example is the DCT processing. A main process generates threads to do 
parallel processing. Each thread owns an independent read/write buffer to keep the 
parallelism. The second is an accumulated addition of numbers from 1 to 10000, 
computed by cooperative threads. These threads will synchronize to each other and their 
computed results are finally summed together. From the Fig.8 we can find that each of 
speedups about the performance is almost linear (i.e., scalable) with the numbers of PEs 
used. 
Fig. 9 shows the scalability of MTJVM’s area and performance. When the number 
of PEs is over 6 that performance starts to drop and the system critical path is located on 
arbiters of the thread-manager. To solve this problem, the arbiters can be designed with 
more than one cycle to maintain the performance scalability of MTJVM. 
Scalability of Performance
0
50
100
150
0 1 2 3 4 5 6 7 8 9 PE(s)
M
ax
 C
lo
ck
Scalability of Area
0
5000
10000
15000
20000
25000
30000
35000
40000
1 2 3 4 5 6 7 8 PE(s)
ALUTs
 
Fig. 9. Scalabilities of MTJVM’s area and performance. 
DCT
0
0.5
1
1.5
2
2.5
3
3.5
4
1 PE 2 PE 4PE
Sp
ee
dU
p 
10
0%
Accumulation
0
0.5
1
1.5
2
2.5
3
3.5
4
1 PE 2 PE 4 PE
1 Thread
2 Thread
4 Thread
8 Thread
 18
6. Conclusions 
We have designed a MTJVM and its preprocessing class loader for the scalable 
platform. It supports TLP at hardware level without any software/OS support and 
provides scalability of the hardware area and performance. The pre-processing class 
loader loads classes statically and handles some optimizations such as addresses 
resolution for reducing hardware complexity and for improving performance. Thread 
processing operations are transferred from software in API into hardware instructions of 
MTJVM for efficiently by the loader. Mostly, the minimum cost of each thread-context 
switch only needs 2 cycles. With this system, applications can be run in higher 
performance and with more flexibility. 
In this architecture, threads execution environment will be restricted by hardware, 
such as the max number of threads support and the capacity of synchronization sets in 
MTJVM. In the future we shall apply additional software approaches to eliminate these 
limitations and to enhance thread managing mechanisms. For the hardware part, the 
ALUs of it can be improved with reconfigurable units [6] to get better hardware 
performance and utilization. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 20
 
[11] Shih-Hsun Hsu, Yu-Xuan Lin and Jer-Min Jou, “Design of a Dual-Mode NoC 
Router Integrated with Network Interface for AMBA-based IPs”, Asian Solid-State 
Circuits Conference (A-SSCC) 2006, 13-15, November, 2006. 
 
[12] Shih-Hsun Hsu, Jer-Min Jou, Ming-Chao Lee, Chien-Ming Sun, “An Effieient NoC 
Pipelined Router Design”, International Conference on Computer & Communication 
Engineering (ICCCE '06), 9-11, May, 2006. 
 
[13] Shih-Hsun Hsu, Zi-Lun Wang, Jer-Min Jou, Ming-Chao Lee, Chien-Ming Sun, “A 
Low Power Routing ASIP for NoC”, International Conference on Computer & 
Communication Engineering (ICCCE '06), 9-11, May, 2006. 
 
[14] Jer Min Jou, Chien-Ming Sun, Yuan-Chin Wu, Ming-Chao Lee, Ye-Xuan Yan, 
Hong-Yi Su, and Haoi Yang, “A Multi-tile Reconfigurable Platform Design for DSP 
Applications,” ACIT-SIP 2005, Novosibirsk, Russia, June 20-24, 2005. 
 
[15] Shih-Lun Chen, Jer-Min Jou, Chien-Ming Sun, Yuan-Chin Wu, Haoi Yang, 
Hong-Yi Su,"Reconfigurable Processor Core Design for Network-on-a-Chip," 
International Computer Symposium, 2004. 
 
 
8. 計畫成果自評 
經過一年的研究，我們研讀了上述的參考資料提出了一個適用於低功率可重置嵌
入式處理器矽核研製，並且發表於 VLSI Design/CAD Symposium 2007, 另外也以
投稿至 International Journal of Electrical Engineering(IJEE)的期刊中，我們研讀了參
考資料的同時，也研究國內外可重置平台、系統設計方法。除此之外，我們所發
表的 Java processor(也就是 Scalable Multi-Threaded JVM)架構，達成了研究設計可
重置嵌入式處理器矽核的目標。另外，我們這幾年的相關研究成果[7] ~ [15]，有許
多的論文發表，針對相關的 issues 做不同的研究，令有專利申請中。整體來說，我
們已達成本計劃這一年的研究進度，有很好的研究成果。 
 
 
 
