1 
 
行政院國家科學委員會專題研究計畫成果報告 
多媒体雕塑工作室－智慧型音視訊編修處理系統 
基於機器學習之視訊內容擬真修補與強化 
 
一、中文摘要 
本 計 畫 報 告 簡 述 本 三 年 期 計 畫
(2009/8~2012/7)所發展之創新技術成果。
在這個三年期的計畫中，第一年我們提出
一種兩階段的人臉仿真強化技術。本論文
提出的方法主要分為兩步驟：全域的高解
析度人臉估測與區域的人臉五官仿真，可
有效的增強放大後的人臉細節。在第二年
的研究中，我們提出一種廣義化的影像超
解析度技術，可應用在許多不同種類的超
解析度技術上去提升重建影像的品質。。
在第三年度的研究重點為視訊超解析度
技術。為了有效解決超解析度的視訊會有
潛在的播放不連續性問題，我們引入了動
態紋理合成以重繪視訊的每一個禎幅。透
過這樣的技術，我們有效解決了播放連續
性的問題，並且達到了較高的視覺品質。 
關鍵詞： 影像放大、超解析度、機器學
習、動態紋理合成。 
 
二、英文摘要 
 In this three-year project, we have 
developed several efficient image. In the 
first year, we proposed a generalized DCT 
decimation scheme for DCT-domain spatial 
downscaling which performs two-fold 
decimation on sub-frames of a flexible size 
larger than the traditional 8×8 block size to 
improve the visual quality. Efficient 
sparse-matrix representations are then 
derived to reduce the computation of the 
proposed DCT decimation method.  In the 
second year, proposed a generic framework 
for enriching the dataset for search-based 
super-resolution schemes with reasonable 
computation and memory cost.. In the third 
year, proposes a high-quality video 
super-resolution scheme via dynamic 
texture synthesis (DTS). We render the 
super-resolution frames by using DTS to 
maintain the temporal consistency, leading 
to a high-quality reconstruction. In the 
proposed method, to further reduce the 
computational complexity, we only 
super-resolve key-frames and the remaining 
non-key-frames will be predicted using 
bi-directional overlapped block motion 
compensation. 
Keywords: Image upcaling, super-resolution, 
machine learning, dynamic texture 
synthesis. 
 
二、第一年之研究簡介 
本子計畫在第一年度提出一種 Two-step
的人臉仿真強化 (Face hallucination) 技
術，可有效的增強放大後的人臉細節。如
圖一所示，本方法主要分為兩步驟：全域
的高解析度人臉估測與區域的人臉五官
仿真。一般而言，人臉仿真可視為訓練出
的 原 型 臉 之 線 性 組 合  (Linear 
combination)，由於低解析度資訊不足造成
計算出來的組合係數不正確，導致一些瑕
疵的發生。本論文透過最大化事後機率 
(Maximum-a-posteriori) 的方法重新估測
係數可有效的減少重建人臉的瑕疵發
生。 
3 
 
會進一步提升其重建影像的品質。圖三顯
示美方法使用之架構圖。令搜尋的來源與
目標取出的特徵向量為 f，我們將搜尋相
似貼片的過程轉化為 
*=argmax ( | )L L L
TL
j
T T I
j j ip
f
f f f  
(a) (b) (c)
(d) (e) (f)
圖四、重建高解析度影像 (a) 原始影像 
(b) 內插影像 (c) [2]’s 方法 (d) [4]’s 方法 
(e) [1]’s 方法 (f) [1]結合提出的方法。 
透過搜尋的策略，找到最相似的特徵後，
進一步利用仿射轉換 (Affine Transform) 
去描述兩個貼片之間的相似關係，使得下
列式子成立： 
  opt
2
argmin ( )L L L LI I T Ti i j jw 
H
H s c s Hc
 
 
其中，H 是我們要估計的 Homograph 矩陣，
透過 Minimizing 上述式子，我們就能在已
知的集合中找到更多相似的貼片，提升重
建品質。圖四顯示本方法重建之畫質。 
四、第三年之研究簡介 
近幾年來，視訊/影像超解析度技術發展已
臻成熟，許多相關的研究報告指出，以機
器學習(Machine Learning) 為基礎的單張
影像超解析度技術可產生優於傳統超解
析度技術的成果[4]。然而，以視訊超解析
度技術來說，大多數還是採用傳統的
Multi-frame方式的超解析度技術。原因有
二，其一是由於視訊自然已經包含了多張
禎幅 (Frame) 的資訊，自然容易拿傳統超
解析度技術處理。其二，由於學習式的超
解析度演算法往往都面臨計算複雜度較
高的問題，因此直接應用學習式超解析度
技術到每一禎幅上面未免不切實際。然而，
學習式超解析度技術有著高品質重建的
優勢，因此，我們試圖尋找目前已知最佳
的到的超解析度技術演算法到視訊演算
法上面。 
考慮到近期的學習式超解析度演算法，雖
然都能重建出較佳的超解析度版本，然而，
最近提出了一種新穎的以紋理合成 
(Texture synthesis)為基礎的超解析度技術
[6]。此演算法可針對紋理部分重建出具有
細微細節的高頻成分，換句話說，在視覺
品質上面，紋理合成的超解析度技術相當
HR features
Input LR 
image
Feature 
extraction
Training set
(HR-LR pairwise)
Find the matched 
points in LR 
domain
*Affine Patch 
Representation
LR features
arg max( | )
LT L
f f arg max( | )
HT L
P P
Homograph 
estimation
Fine searching in 
spatial domain
Super-resolution 
method
Output HR 
image
 
圖三、基於仿形轉換之超解析度方法架構圖。 
5 
 
其中
; ;
T
p t p t
y C x  且 C 是一個 Orthogonal
的投影矩陣，通常此投影矩陣會是主成分
分析的投影矩陣。因此，我們將每一個貼
片投影到此子空間之後，其維度可大幅下
降，因此，這樣 一來便可以避免
Underdetermine 的問題，此外，由於維度
的降低，計算時間複雜度也跟著降低。在
這個計畫中，我們同時也考慮了各種不同
的降維後的維度，並比較分析其性能，在
接下來的實驗中，我們是挑選降維後的維
度與輸入的貼片數量(時間軸上)相同，這
麼一來，其投影之後的貼片矩陣將會是一
個方陣，相當有利於後續求解的過程。最
後，此線性系統的解可以透過最小平方法
(Least-squares) 來求解[10]: 
1
1: 1 1: 1 1: 1 2:
( )
T T
T T T T

  
A Y Y Y Y , (2)  
其中矩陣  
1: 1 2
  ... 
T T
Y y y y  代表了從第一
個楨幅到第 t 個楨幅，其他參數的求解都
可以參考文獻 [10] 得知。至此，我們已
經可以有一個好的線性系統來描述一個
貼片在時間軸上的變化。然而，在超解析
度技術的這個問題點上，我們所擁有的資
訊只有低解析度的視訊，與其超解析度後
的視訊。為了順利解決這個問題，我們觀
察原始高解析度視訊與低解析度視訊的
相似性，發現兩者雖然視訊內容的清晰度
並不一致，然而，在主要動態紋理的展示
上面確是相當一致。如圖五所示，即使視
訊經過縮小，然而在其動態紋理的路徑上
面，還是存在著相似性，因此，我們假設
兩者在高解析度與低解析度的視訊中，共
享同一個動態紋理路徑。 
令一個在第 t 時間超解析度後的貼片表示
為 TS
t
x ，而其動態紋理的轉換矩陣為 TSA ，
則利用前面的式子，我們可以輕易求得這
個轉換矩陣。再假設高解析度與低解析度
視訊中，共享相似的動態紋理路徑，我們
簡單的將低解析度的動態紋理路徑，取代
到超解析度視訊的動態紋理路徑，來達到
重繪視訊的目的、克服時間軸上不一致性
的問題。因此，由於動態紋理的路徑與 LRA
是正相關關係，因此我們可以進一步假設
LR
A 與 HRA 的相似性，故以動態紋理為基礎
的視訊超解析度問題可表示為： 
  
; 1 ;
N(0, )
SR LR TS
p t p t t
 y A y Σ  
其中  ; ;
T
TS TS TS
p t p t
y C x 。 相同地，我們
可以利用一樣的程序直接求者得到轉換
矩陣 LRA 。在求出新的動態紋理路徑後，
我們直接透過投影矩陣投影回到原本貼
片的影像域，即
; ;
SR TS SR
p t p t
x C y ，便可求得這
個貼片對第 t 個時間點的動態紋理。比較
起傳統的時間軸限制，利用動態紋理合成
進行視訊的重繪在速度上有明顯的優勢，
此外，由於動態紋理合成是在子空間下進
行，在整體記憶體的使用量上面會明顯低
於時間軸限制的使用。 
 
圖六、某個貼片之動態紋理路徑圖。紅色
線表示低解析度貼片的紋理路徑，藍色線
表示超解析度的貼片動態紋理，而桃紅色
線表示原始視訊的動態紋理路徑。 
-0.4-0.200.20.40.6
-1
-0.5
0
0.5
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
 
10
98
7
6
4
3
5
 
LR patch
SR patch
HR patch
7 
 
圖八、  透過  (b) sparse coding [1], (c) 
ASDS SR [8], (d) NLBP SR [8], (f) TS SR 
錯誤! 找不到參照來源。, 以及 (g) 提出
方法的超解析度技術之視覺品質比較。. 
圖八展示了其中一個貼片透過布圖超解
析度放大的結果，其中我們提出來的方法 
(f) 明顯優於題差人的方法，如 sparse 
coding SR [1], ASDS SR [8], and NLBP SR 
[8] 等的方法。圖八展示了動態紋理合成
所扮演的角色，我們可以發現，如果單純
只用紋理合成的超解析度技術，雖然第畫
面的視覺品質良好，然而我們可以注意到，
其與第二、第三楨幅的差異影像顯示了許
多不同的差異資訊，這就是因為此類方法
無法保證時間軸連續性的證明。反觀我們
提出來的方法，不僅在視覺品質上有著與
紋理合成超解析度技術相似的效果，且在
播放時的連續性也相對高出許多，因此驗
證了提出方法的有效性。 
為了驗證系統在時間軸播放時的連續性，
我們利用MOVIE 的評估系統來衡量我們
超解析度視訊的品質。表一與表二明顯看
出，我們提出的方法 (DTS SR) 可以優於
別人的方法，此外，即使我們採用的
BOBMC 移動補償技術，我們仍然可得到
一定的重建品質。其中我們可以發現，即
使 TSS 的方法在每一個楨幅有最好的視
覺品質，然而整體上受到時間軸播放不連
續的影響，整體的MOVIE誤差指標上升，
而 Sparse coding SR 這類學習的方法則受
限於內容無法有效的超解析度放大，因此
也無法有效降低整體MOVIE 誤差指標。 
(j) (k) (l)
(b)(a) (c)
(d) (e) (f)
(g) (h) (i)
 
圖九、 由上而下分別是原始高解析度楨
幅、利用Sparse coding SR放大後的楨幅、
利用提出 TSS 方法放大的楨幅以及最後
提出來的方法放大後的楨幅。由左至右分
別為原始楨幅，以及其與第二張、第三張
楨幅的差異影像。. 
 
圖十、在 BOBMC 不同間隔的效能比較。 
 
(a)
(f)
(b)
(e)
(c)
(d)
6 8 10 12 14
0.15
0.16
0.17
0.18
0.19
0.17
0.19
0.21
0.23
0.24
0.25
Interval between KFs
M
O
V
IE
 i
n
c
re
a
s
in
g
 r
a
ti
o
 (
%
)
 
 
Video 1
Video 3
Video 2
出席國際學術會議心得報告 
                                                             
計畫編號 98B3084J63 
計畫名稱 
多媒体雕塑工作室-智慧型音視訊編修處理系統-基於機器學習之視訊內容
擬真修補與強化(3/3) 
出國人員姓名 
服務機關及職稱 
林嘉文 國立清華大學電機工程學系 副教授 
會議時間地點 
2011 年 10 月 14 日至 2011 年 10 月 16 日 上海交通大學 (姊妹校拜訪) 
2011 年 10 月 16 日至 2011 年 10 月 20 日 浙江杭州 
會議名稱 IEEE Workshop on Multimedia Signal Processing (MMSP) 2011 
發表論文題目 
Chih-Chung Hsu and Chia-Wen Lin, “Image super-resolution via feature-based affine 
transform,” in Proc. IEEE Workshop Multimedia Signal Processing (MMSP), Oct. 
2011, Hang-Zhou, China. 
 
一、 經過 
A. 至上海交大參訪及演講 
在參加 MMSP 會議前，本人和資工系許秋婷教授在 10 月 14 日至 10 月 16 日
間拜訪上海交通大學。由於本人和上海交大電子工程系的圖像通信與訊息處理研
究所楊小康副所長及林巍嶢教授已認識多年，相當熟識，此外上海交大也是本校
之姊妹校，因此趁著到杭州參加會議的機會，順道拜訪上海交大。 
我們在 10 月 14 日中午 12:05 抵達上海虹橋機場，隨即搭計程車前往上海交大
閔行校區的電子工程系。閔行校區為上海交大的行政中心所在，校園十分遼闊。
在花了一些時間找到電子系所在位置後，我們即先拜會林巍嶢教授，先由他陪同
參觀校園模型並介紹校園之環境，然後就帶領我和許秋婷教授到會議室進行演講。 
 
上海交大閔行校區模型 
我們在 10 月 14~15 日晚上住宿於上海交大徐匯校區內的旅館。在周末，我們
繼續和林巍嶢教授討論可能的合作項目及作法，由於楊小康副所長需先行前往杭
州，所以跟他約好在 MMSP 再會面討論。我們主要討論的重點是找出可以互相合
作之項目，我們的共識為視訊內容分析和其在行為分析及視訊編輯的應用是雙方
都有興趣且在進行之研究項目，因此是可行的合作目標，並決定在未來繼續以
email 討論，以找出明確的合作研究項目。此外我們也討論研究生互訪擔任 intern
的可行性，不過由於目前由大陸申請來台的程序仍是相當曠日費時，在對大陸學
生研究津貼的給付也有不少困難，因此我們仍在尋找可行的方案。由於我和林教
授目前共同為 Journal of  Visual Communication and Image Representation 編
輯一個 “Recent Advances on Analysis and Processing for Distributed Video Systems” 
特刊，所以也討論了論文審查及錄取標準之基本原則及遇到的問題和對應的解決
方案等。 
 
B. 至杭州參加 MMSP 
 
國際多媒體信號處理研討會 (MMSP) 是由 IEEE 信號處理分會的多媒體信號
處理技術委會所主辦的年度會議，於 2011 年 10 月 17 日至 2011 年 10 月 19 日在
中國大陸浙江省杭州舉行。本次出國參加 IEEE MMSP 2011 會議最主要之工作為
發表一篇論文“Image super-resolution via feature-based affine transform”，並和參
加會議的學者及工業界專家進行意見交流。 
MMSP 今年分成 regular sessions、keynote talks、overview talks、demo sessions、
和 tutorials。本年度繼續維持去年的投稿及錄取規定，除了論文長度上限為 6 頁外，
regular sessions 論文錄取率也維持 30%之低錄取率。今年總共有 163 篇論文投稿，
共錄取 82篇論文，包含 27篇 oral papers及 55篇 poster papers。此外邀請了Dr. Susie 
Wee (Cisco CTO), Prof. Mauro Barni (University of Sienna), 及 Harry Shum 
(Corporate VP, Search Product Development, Microsoft) 分 別 針 對 Could 
Computing、Data Privacy and Protection 及進行九場精彩之 Keynote/plenary 
Speeches，反應非常熱烈。議程委員會還安排了三個 overview talks，分別由 Dr. 
Thomas Stockhammer (Qualcomm)，Prof. Helen Meng (Chinese University of Hong 
Kong)和 Prof. Pascal Frossard (EPFL)針對 video streaming，speech-based interaction
和 Dictionary perspective on multimedia learning 三個主題進行 overview talks，講員
及演講內容之水準皆相當高。 
除了參加MMSP議程進行外，本人也參加了 IEEE Signal Processing Society 之
MultiMedia Signal Processing (MMSP) Technical Committee TC meeting，討論 TC 之
專業事務與未來規劃事項。由於 MMSP TC 為 MMSP 會議的指導委員會，我們除
了檢討此次 MMSP 會議投稿及稿件審查事務外，並在會議中並討論兩個由義大利
團隊和英國團隊所提之 MMSP 2013 提案，並決定在會後以通訊投票方式決定要錄
取哪一個提案。事後投票結果，由義大利團隊獲得 2013 年會議主辦權。 
由於 MMSP 在西湖所在之杭州舉行，所以會議宴會後半段為驅車前往西湖觀
Image Super-Resolution via Feature-Based Affine 
Transform 
Chih-Chung Hsu 1 and Chia-Wen Lin 2 
# Department of Electrical Engineering, National Tsing Hua University 
Hsinchu, Taiwan 
1 
m121754@gmail.com 
3 
cwlin@ee.nthu.edu.tw 
 
Abstract—State-of-the-art image super-resolution methods 
usually rely on search in a comprehensive dataset for 
appropriate high-resolution patch candidates to achieve good 
visual quality of reconstructed image. Exploiting different scales 
and orientations in images can effectively enrich a dataset. A 
large dataset, however, usually leads to high computational 
complexity and memory requirement, which makes the 
implementation impractical. This paper proposes a universal 
framework for enriching the dataset for search-based super-
resolution schemes with reasonable computation and memory 
cost. Toward this end, the proposed method first extracts 
important features with multiple scales and orientations of 
patches based on the SIFT (Scale-invariant feature transform) 
descriptors and then use the extracted features to search in the 
dataset for the best-match HR patch(es). Once the matched 
features of patches are found, the found HR patch will be aligned 
with LR patch using homography estimation. Experimental 
results demonstrate that the proposed method achieves 
significant subjective and objective improvement when 
integrated with several state-of-the-art image super-resolution 
methods without significantly increasing the cost. 
I. INTRODUCTION 
Image/video super-resolution (SR) has become an attractive 
technique in enhancing the resolution of  low-resolution (LR) 
images because it has many applications such as security in 
surveillance video, enhancement of aged photos and captured 
images from low-power devices. For example, a LR image 
can be taken from a mobile device. If we want to obtain the 
high-resolution (HR) image from LR image, the image super-
resolution can be used to solve such problem.  
In general, there are two kinds of the image super-
resolution techniques which are multi-frame and single-frame 
methods. The first-type methods need to obtain multiple input 
LR images and align these LR images to calculate the missing 
sub-pixel values within pixels. Since the multi-frame SR 
methods require accurate alignment, the scaling-up factor of 
the LR image has its limit practically. On the other hand, 
single-frame image SR schemes do not suffer from this 
problem. The single-frame based methods collect a 
candidate/training pool which may contain a large number of 
the LR and HR pairs. In the reconstruction stage, each LR 
patch is replaced with it corresponding HR patch(es) obtained 
from the candidate/training pool. Therefore, the performance 
of the single-frame SR methods heavily depends on the 
comprehensive of the candidate/training pool. This work 
focuses on enhancing the performance of single frame image 
SR.  
The most representative image SR methods include 
example-based super-resolution (ES) [2], sparse coding (SC) 
[4], nonlocal means filter (NLM) [5], and texture synthesis 
super-resolution (TSS) [6]. All of these methods require a 
comprehensive dataset as the training set to obtain good visual 
quality of the reconstructed image. For example, ES requires 
to collect a comprehensive training set containing HR patches 
and their LR counterparts and the relationship between LR 
and HR patches is modeled as Markov Random Fields (MRF). 
Similarly, TSS uses the texture synthesis technique to 
hallucinate the HR patch from training set. SC also collects a 
large number of training samples and learn a set of 
overcomplete bases. The super-resolved patches can be 
replaced with a linear combination of the overcomplete basis 
under an L1 norm constraint.  
The basic concept of NLM [5] is to exploit self-similarity 
in an image. Unlike ES, the pool of candidate patches is 
obtained from the input LR image itself. Similarly, once the 
candidate patches for the input LR patch are found, the LR 
patch is replaced with a linear combination of its 
corresponding HR candidate patches. Note that, when the 
candidate poll for NLM is not comprehensive enough, we may 
not find sufficiently similar candidate patches for an LR patch. 
As a result patches result in blurring effect due to dissimilar 
patches [4].  
 
Fig. 1 Illustration of search-based SR methods in [2][4][6][7]. 
Most of the above state-of-the-art SR schemes can be 
represented as search-based schemes as illustrated in Fig. 1.  
The search-based schemes search for each LR input patch in 
the candidate/training pool a set of best-match HR patches. 
Consequently, the LR patch is replaced with the HR patch 
As shown in Fig. 4, the proposed APR method is composed 
of four major steps. In the training stage, the SIFT feature 
vectors with various scales and orientations are calculated and 
stored for the training images. In the super-resolution stage, 
the first step is to calculate the SIFT feature vector of each 
patch of the input LR image and find the best match in the 
training set. Second, the homography matrix between the 
matched pair is established. Then, the matched HR patch is 
aligned using the calculated homography matrix. Finally, the 
best match of each input patch is obtained from the aligned 
patches of the training set in the spatial domain. 
The feature extraction step extracts scale- and orientation-
invariant features based on the SIFT descriptors. Let the ith 
SIFT feature vector be denoted as 1 128i
×
∈f \ , and the patch of 
an image as p. Finding the most similar patch LTjp  in the 
training set for input LR patch LIjp can be formulated as the 
following maximum likelihood (ML) problem: 
* = arg max ( | )L L L
TL
j
T T I
j j ip
p
p p p                        (1) 
In general, search-based approaches are used to solve the 
above problem [2], [5], [7]. However, to achieve good visual 
quality of a super-resolved image, a large-size training set is 
usually required, thereby leading to high computational 
complexity as shown in Fig. 3. To reduce computation while 
maintaining comparable visual quality, we propose to convert 
this problem to a feature matching problem by extracting the 
SIFT features of training set and input LR image so that it is 
possible to find good candidate patches  under different scales 
and orientations without performing full-search in the spatial 
domain. Besides, the feature extraction and matching is only 
performed for visually important patches, whereas the 
remaining unimportant patches can be super-resolved using a 
baseline method without sacrificing the visual quality of the 
reconstructed image. 
On the other hand, using the original SIFT features would 
suppress the edge and corner pixels. Instead, we retain these 
feature points because the visually important regions in an 
image usually exist around the edge or corner pixels. 
After extracting the SIFT features, the solution to ML 
problem in (1) can be approximated by maximizing the 
likelihood between two feature vectors as follows: 
* = arg max ( | )L L L
TL
j
T T I
j j ip
f
f f f                         (2) 
However, since the matched feature vectors ( and L LT Ij if f ) 
may be obtained from different scales and orientations, the 
corresponding patch pair (  and L LT Ij ip p ) may also be with 
different scales and orientations. One problem is how to 
determine these two parameters. Estimating the orientation 
and scale information directly from the SIFT features is 
usually not accurate enough, since the values of these 
parameters have been quantized prior to calculating SIFT 
features. To obtain an accurate estimate, a homography matrix 
can be established for estimating the two parameters for the 
patch pair. Toward this end, given a source patch ps, we can 
obtain an encoded patch by 
( ( ))e ss R= +p p d                             (3) 
where R represents a rotation function, s is a scaling function, 
d is the spatial translation. This process can be represented as 
an affine transformation of coordinates of patches defined 
below: 
1 2 1
3 4 2
e s s
e s s
x m x m y t
y m x m y t
= + +
= + +
­®¯                             (4) 
where m1, m2, m3, and m4 are the parameters for controlling 
the scaling ratio and rotation angle, (t1, t2) denotes the 
translation vector, and (xe, ye) and (xs, ys) indicate the 
coordinates of the encoded patch and source patch, 
respectively. As a result, there are six parameters to be 
determined. To estimate these parameters, we formulate (4) as 
the homography estimation problem [9] expressed by 
N N
1 2 1 11 12 13
4 5 2 21 22 23
31 32 330 0 1 1 1
e s s
e s s
e
x m m t x h h h x
y m m t y h h h y
w h h h
=
ª º ª º ª º ª º ª º« » « » « » « » « »« » « » « » « » « »« » « » « » « » « »¬ ¼ ¬ ¼ ¬ ¼ ¬ ¼ ¬ ¼
sc cH

	

   (5) 
where ce and cs denote the coordinates. This problem can be 
solved by linear least-squares approximation via rearranging 
(5) in a simple form as follows: 
1 0 0 0
0 0 0 1
s s e s e s e
s s e s e s e
x y x x x y x
x y y x y y y
=
− − −ª º
=« »
− − −¬ ¼
h Ah 0  
    (6) 
where [ ]11 12 13 21 22 23 31 32 33 Th h h h h h h h h=h . 
The form cannot be directly solved by using conventional 
least-squares solution for Ah = b. Instead, such problem can 
be solved using singular-value decomposition (SVD). Let Ah 
= 0, we have 
            ( )
1 1
( ) ( ) ( )
2 2
1
0 ( )
2
T T T
T T T T
f
f
= =
∂
= = + =
∂
h Ah Ah h A Ah
A A A A h A Ah
h
               (7) 
Consequently, the solution of h should be equivalent to the 
eigenvector of TA A  that has an eigenvalue of zero. Then, the 
calculated h vector is converted to matrix form H. Once the 
homography matrix for an input patch is obtained, the matrix 
can be used to align the matched patch from the training set 
with the input patch. With this correspondence, the pixel 
values in the encoded patch can be determined using a bilinear 
function. Consequently, the encoded patch can be obtained 
using the following homography and warping functions: 
optaffine
se =c H c                                     (8) 
and 
( )( )affine( )e e s ew=p c p c                         (9) 
where w is a bilinear interpolation function. 
For each feature, we can find the best-match patch and then 
align it to the input. To increase the matching accuracy, we 
SIFT extraction is 5. More simulation results can be found in 
[13]. 
The proposed APR framework exploits the multi-scale and 
multi-orientation self-similarity of patches to enrich the 
candidate pool. As shown in Fig. 2, when the search pool is 
enriched, the performance of the search-based super-
resolution techniques will be improved.  
 
Fig. 6 Comparison of visual quality of reconstructed images: (a) ground-
truth, (b) bicubic interpolation, (c) the method in [7] (SSIM: 0.72/MSE:179), 
(d) SC [4] (SSIM: 0.71/MSE:211), (e) ES [2] (SSIM: 0.72/MSE:198), and (f) 
ES + APR (SSIM: 0.76/MSE:161). 
Fig. 6 compare the visual qualities of super-resolved images 
using the proposed APR method on top of ES (APR + ES) 
with bicubic interpolation, Glasner et al.’s method [7], SC, 
and ES. As illustrated, the proposed APR framework 
significantly improves the subjective visual quality of the 
reconstructed image, especially on edge regions. Besides, the 
proposed method also introduces fewer noisy artifacts in the 
smoothing regions compared to the other methods. Table I 
shows the objective quality comparison among three baseline 
super-resolution schemes  and the proposed APR method on 
top of the baseline schemes for four test images. Our method 
significantly improves the objective quality of reconstructed 
images when it is integrated with the search-based SR 
methods. For these search-based SR methods, the quality of 
reconstructed image is dependent on the richness of 
candidate/training set.  By exploiting the multi-scale and 
multi-orientation self-similarity of an image, the improvement 
comes from the enriched candidate/training pool which 
usually makes it easier to find a better match for a patch to be 
super-revolved so that the artifacts in the reconstructed image 
can be reduced. According to our experiments, the proposed 
method leads to 10−20% increase in execution time for 
different SR methods. 
TABLE I 
COMPARISON OF OBJECTIVE VISUAL QUALITIES OF RECONSTRUCTED IMAGES 
USING FOUR STATE-OF-THE-ART SR SCHEMES WITH & WITHOUT THE 
PROPOSED APR IMPROVEMENT 
Methods Image 1 Image 2 Image 3 Image 4 
ES [2] 21.3 dB/0.75 19.3 dB/0.71 23.2 dB/0.74 22.8 dB/0.73 
ES + APR 22.0 dB/0.79 20.0dB/0.72 23.5 dB/0.81 23.7 dB/0.79 
Gain 0.7 dB/0.04 0.7 dB/0.01 0.3 dB/0.07 0.9 dB/0.06 
NLM [5] 22.1 dB/0.73 19.9 dB/0.72 22.6 dB/0.71 23.3 dB/0.77 
NLM + APR 22.7 dB/0.74 20.8 dB/0.81 23.0 dB/0.76 24.1 dB/0.83 
Gain 0.6 dB/0.01 0.9 dB/0.09 0.4 dB/0.05 0.8 dB/0.06 
SC [4] 22.8 dB/0.75 19.2 dB/0.72 23.3 dB/0.76 21.9 dB/0.78 
SC + APR 23.7 dB/0.76 19.8 dB/0.75 23.8 dB/0.81 22.3 dB/0.82 
Gain 0.9 dB/0.01 0.6 dB/0.03 0.5 dB/0.05 0.4 dB/0.04 
Note, the performance of the proposed method relies on 
extracting gradient features for patch matching across 
different scales and orientations. Therefore, if an input LR 
image contains too few gradient features, the improvement 
with the proposed method would become marginal.  
V. CONCLUSION 
We have proposed an efficient APR framework to enrich 
the candidate/training pool of search-based SR schemes by 
exploiting the multi-scale and multi-orientation self similarity 
existing in an image. By exploiting the self-similarity of an 
image, our method significantly enriches the 
candidate/training pool, making it easier to find a better match 
for a patch to be super-revolved with the search-based SR 
schemes. One main contribution of the proposed method is to 
apply feature-based matching to significantly reduce the high 
computational cost for searching in a large candidate/training 
pool. Besides, the proposed APR framework can be easily 
integrated with state-of-the-art search-based SR algorithms. 
Experimental results demonstrate that the proposed method 
effectively improves the visual qualities of super-resolved 
images both subjectively and objectively. 
REFERENCES 
[1] B. Baker and T. Kanade, “Limits on superresolution and how to break 
them,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 9, pp. 
1167–1183, Sept. 2002. 
[2] W. T. Freeman, T. R. Jones, and E. C. Pasztor, “Example-based super-
resolution,” IEEE Comput. Graphics & App., vol. 22, no. 2, pp. 56–65, 
Mar. 2002. 
[3] Q. Shan, Z. Li, J. Jia, and C.-K. Tang, “Fast image/video upsampling,” 
ACM Trans. Graphics, vol. 27, no. 5, 2008. 
[4] J. Yang, J. Wright, T. Huang, and Y. Ma, “Image super-resolution as 
sparse representation of raw image patches,” in Proc. IEEE Int. Conf. 
Comput. Vis. Pattern Recognit., pp. 1-8, 2008. 
[5] M. Protter, M. Elad, H. Takeda, and P. Milanfar, “Generalizing the 
nonlocal-means to super-resolution reconstruction,” IEEE Trans. 
Image Process., vol.18, no.1, pp.36-51, Jane 2009. 
[6] Y. HaCohen, R. Fattal, and D. Lischinski, “Image upsampling via 
texture hallucination,” in Proc. IEEE Int. Conf. Comput. Photography, 
Cambridge MA USA, pp. 20-30, Mar. 2010. 
[7] D. Glasner, S. Bagon, and M. Irani, “Super-resolution from a single 
image,” in Proc. IEEE Int. Conf. Comput. Vis., Oct. 2009. 
[8] H. Chang, D. Yeung, and Y. Xiong, “Super-resolution through 
neighbor embedding,” in Proc. IEEE Int. Conf. Comput. Vis. Pattern 
Recognit.,pp.275-282,WashingtonDC,2004. 
[9] Dubrofsky and Elan Nathan, “Homography Estimation,” MS. Thesis, 
Department of Computer Science, UBC University. 
[10] H. Bay, A. Ess, T. Tuytelaars, and L.V. Gool, “SURF: Speeded up 
robust features,” Comput. Vis. Image Understanding, vol. 110, no. 3, 
pp. 346--359, 2008. 
[11] D. G. Lowe, “Distinctive image features from scale-invariant 
keypoints,” Int. J. Comput. Vis., vol. 60, pp. 91-110, February 2004. 
[12] Y. Zhi, Y. Peimin, and L. Sheng, “Super resolution based on scale 
invariant feature transform,” in Proc. IEEE Int. Conf. Audi., Langu. 
and Image Process., pp.1550-1554, 7-9 July 2008. 
[13] NTHU image super-resolution project. [Online]. Available: 
http://www.ee.nthu.edu.tw/nvlab/SR/index.htm. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/12/10
國科會補助計畫
計畫名稱: 基於機器學習之視訊內容擬真修補與強化
計畫主持人: 林嘉文
計畫編號: 98-2221-E-007-080-MY3 學門領域: 視訊與影像分析
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 計 畫 主 持 人 獲 邀 擔 任 IEEE Transactions on Multimedia, IEEE 
Transactions on Circuits and Systems for Video Technology, IEEE 
Multimedia Magazine, EURASIP Signal Processing: Image Communication, 
Elsevier Journal of Visual Communication & Image Representation 等國
際知名期刊之 Associate Editor 
 
2. 計畫主持人論文獲得中華民國影像處理與圖型識別學會 2010 年及 2012 年
論文優等獎共兩次，及 2012年論文佳作獎一次。 
 
3. 此計畫部分成果已移轉給國內最大手機廠商 HTC 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
