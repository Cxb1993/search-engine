1. Introduction
The rapid spread of computer usage by the human being has stimulated the
need for automatic speech recognition (ASR). The ultimate goal of research on
ASR is the construction of machines that are indistinguishable from humans in
their ability to communicate in natural spoken language. The speech recognition
has been investigated about forty years. The early research in ASR can be found in
[1-3]. In the recent years, a lot of speech recognition devices with limited capabilities
are now available commercially. These devices are usually able to deal only with a
small number of acoustically distinct words. The ability to converse freely with a
machine still represents the most challenging topic in speech recognition research.
The difficulties involved in ASR are: (1). to extract linguistic information from
an acoustic signal and discard extralinguistic information such as the identity of
the speaker, his or her physiological and psychological states, and the acoustic
environment (noise) [4], (2). to normalize an utterance which is characterized by
a sequence of feature vectors that is considered to be a time-varying, nonlinear
response system, and (3). to meet real-time requirement since prevailing recognition
techniques need a large amount of computation. Even if the same speaker utters
the same syllable, the duration changes every time with nonlinear expansion and
contraction and hence the duration of the whole sequence of feature vectors and the
durations of stable parts are different every time. Therefore, it is very difficult to
find a simple model to represent the speech waveform.
A speech recognition system basically contains extraction of features and clas-
sification of an utterance. The measurements made on the speech waveform include
energy, zero crossings, extrema count, formants, LPC cepstrum [5-11] and the Mel
frequency cepstrum coefficient (MFCC) [12]. The LPC method provides a robust,
reliable and accurate method for estimating the parameters that characterize the
2
by applying an information-theoretic iterative clustering technique [28] to a train-
ing sequence containing several repetitions of the vocabulary word. The clustering
process removes all time-sequence information from the training sequence and rep-
resents each vocabulary word as a set of independent spectra. An input utterance is
classified by encoding it with every codebook and finding the codebook that yields
the smallest average distortion. After the speech data are compressed by VQ, the
compressed speech data are easy for classification and the recognition rate can be
improved. For classification, the most common methods are the distortion measures
[8,28,32,34] used in the DP process and the HMM [36]. In general, the selection
of a perceptually meaningful distortion measure in clustering and the construction
of an optimal codebook are difficult. It is also difficult to apply the VQ to a large
vocabulary because the computational cost is still high in clustering. The theory of
HMMs and its applications to speech recognition is nothing new. The theory was
published by Baum et al [35], but widespread understanding and applications of
the theory of HMMs to speech processing have occurred only within the past ten
years. The HMM technique has significantly reduced the computational cost and
has been used for large vocabulary connected and continuous speech recognition
applications. The mandarin syllable recognition was recently studied by Wagner
et al [50] and Chen et al [51] and the digit recognition can be found in [1,46-49,56].
In this paper, we present a simple, fast and accurate speech recognition method.
We use five simple techniques for speech data compression, from which we obtain
five different types of features for each syllable. For speech recognition, we simply
use a simplified Bayes decision rule with a weighted variance. The recognition
results are excellent and the computational cost is very low.
4
independent. The conditional density can be represented as
f(x1, ..., xk|ci) =
[
k∏
l=1
1√
2piσil
]
e
− 12
∑k
l=1
(
xl−µil
σil
)2 (2.4)
where i = 1, ..., 408 for syllable recognition. To use the Bayes rule for decision
making, among 408 categaries, we only compare the logarithmic value of (2.4), i.e.,
l(ci) =
k∑
l=1
log σil +
1
2
k∑
l=1
(
xl − µil
σil
)2, i = 1, .., 408. (2.5)
The Bayes rule decides the category with the least l(ci) to which the input x =
(x1, ..., xk) belongs. A weighted variance was found to improve the recognition rate
for a Bayes decision rule [56]. Using a weighted variance, (2.5) becomes
l′(ci) =
k∑
l=1
log (cσil) +
1
2
k∑
l=1
(
xl − µil
cσil
)2, i = 1, .., 408. (2.6)
where c is a weighted factor for the variance. In experiments, c = 1.5 is used.
Therefore, the values in both (2.5) and (2.6) will later be used for classification.
6
i.e.,
E =
N−1∑
n=0
[s(n)− sˆ(n)]2. (3.2)
The unknown ak, k = 1, ..., p, are called the LPC coefficients and can be solved by
the least square method. The most efficient method known for obtaining the LPC
coefficients is Durbin’s recursive procedure [9]. Here in our experiments, p = 16
and N = 200 (the frame size of the Hamming window). In order to apply dis-
tance measures in classification, LPC coefficients are transformed into LPC cepstra
(LPCC), which for each mandarin syllable are represented in a sequence of vectors,
each having 16 LPC cepstra. The following recursive equations [53] are used to
transform the LPC coefficients ak, k = 1, ..., p, to the LPCC a′k, k = 1, ..., p:
a′1 = −a1 (3.3)
a′i = −ai −
i−1∑
j=1
(1− j/i)aja′i−j 1 < i ≤ p (3.4)
a′i = −
i−1∑
j=1
(1− j/i)aja′i−j p < i (3.5)
The sequence of LPCC vectors is a time-varying, nonlinear response system. Even
if the same speaker utters the same syllable, the duration changes every time with
nonlinear expansion and contraction and hence the length of the whole sequence
and the durations of stable parts are different every time. In this paper, we present
five ways to compress and normalize a sequence of LPCC vectors into a standard
pattern such that the speech waves for the same syllable uttered by the same or
different speakers has the same patterns. In extracting features, we only use the
8
Feature 1 (F1) – Compression according to the total absolute value of LPCC vectors.
Let y(k) = (y(k)1, ..., y(k)p), k = 1, ..., n, be the LPCC vector for the kth frame
of a speech waveform in the new sequence. Let
S =
30∑
k=1
p∑
i=1
|y(k)i| (3.7)
be the sum of total absolute values of the elements of LPCC vectors in the main
feature subsequence. Partition the 30 vectors in the main subsequence into 10 seg-
ments such that each segment has the absolute value S/10, i.e, the part of the
speech waveform with large absolute value of the LPCC elements is divided into
more segments. Since the second half of the new sequence only contains the tail
sound of a syllable utterance, we compress the second half into two segments ac-
cording to the absolute values of LPCC vectors. The average value of the LPCC
in each segment is used as one feature value of the speech wave. Since the first 10
elements of a LPCC vector are used, we obtain 12x10 feature values.
Feature 2 (F2) – Compression according to the differences between two consecutive
LPCC vectors.
Let the sum of absolute differences of two consecutive LPCC vectors be denoted
by
S =
30∑
k=2
p∑
i=1
|y(k)i − y(k − 1)i|. (3.8)
Partition the first 30 vectors of LPCC into 10 segments of different lengths such
that in each segment, the total absolute value of the differences between consecutive
LPCC vectors is S/10, i.e., the part of a speech waveform with large absolute
value of the differences has more segments than the part with small absolute value.
Similarly, the rest of the sequence containing a tail sound is partitioned into two
segments according to the same absolute value of the differences of LPCC vectors.
10
Let y(k) = (y(k)1, ..., y(k)p), k = 1, ..., n, be the LPCC vector for the kth frame
of a speech waveform in the LPCC sequence. Let
S =
n∑
k=1
p∑
i=1
|y(k)i|1/2 (3.10)
be the sum of total square roots of absolute values of the elements of LPCC vectors.
Partition the whole sequence of LPCC vectors into 10 segments of different lengths
such that each segment has an equal sum of square roots of absolute values S/10,
i.e, the part of the speech waveform with large square roots of absolute values of
the LPCC elements is divided into more segments. The average value of the LPCC
in each segment is used as one feature value of the speech wave. Since the first 10
elements of a LPCC vector are used, we obtain 10x10 feature values.
Feature 5 (F5) – Compression by deleting the stable portion of LPCC vectors using
the criterion of the square roots of the absolute values of the LPCC vectors.
Let the difference of two consecutive LPCC vectors be denoted by
D(k) =
p∑
i=1
|y(k)i − y(k − 1)i|1/2 (3.11)
for k = 2, 3, ..., n. Note that the difference from Feature 3 is that here we use a
square root of the absolute difference of two consecutive vectors. As in feature 3,
the LPCC vector y(k) is removed if the square root of the difference D(k) from
the previous vector y(k − 1) is too small. Let y1(k), k = 1, ...m (< n), be the new
sequence of LPCC vectors after deletion. Then partition the new sequence of LPCC
vectors into 10 equal segments. The average value of the LPCC in each segment is
used as a feature value. This compression produces 10x10 feature values.
For a syllable, our compression methods produce five different patterns of fea-
ture values, i.e., five matrices of feature values for each syllable.
12
monosyllables. Among 10 samples of each mandarin syllable, pick up any 2 samples
for recognition and the rest of 8 samples is used for training, i.e., the rest of 8 sam-
ples is used to estimate µil and σil, i = 1, ..., 408, l = 1, ..., k (k=10x10 or 12x10) in
(2.6). Diagram 4.1 shows a block diagram of our approach for speech recognition.
Diagram 4.1. Flowchart of a syllable recognition.
4.2. Experiment with Features F1, F2, F3, using Separation of a LPCC Sequence
into Main Feature Subsequence and Tail Sound Subsequence.
In order to test the effect of separation of a LPCC sequence into two subse-
quences, the same feature (F1, F2, F3) are extracted from the same LPCC sequence
without separation of LPCC sequence into two subsequences. The classifier is the
simplified Bayes decision rule (2.5). The recognition rates for the the three features
with and without separation into the main feature and tail sound subsequences are
in Table 4.1. On the average, more than 1.3% of recognition rate is improved by
using the technique of the separation of LPCC sequence into the main feature and
the tail sound subsequences.
Features Recognition Rates
with separation without separation
14
4.4. the Best Combinations of Features for Recognition
From the above experiments, the separation of a LPCC sequence into main
feature and tail sound subsequences and the weighted varaiance can add additional
recognition rates. In this experiment, we use these techniques to find the best
combinations among five features in order to achieve the highest recognition rates.
The best combinations from one feature to five features are given in Table 4.3.
The highest recognition rate is 94.36% which is achieved by the combination of five
features and which is much higher than the recognition rate 79.78% obtained by
using the HMM method with poly-observation techniques at Telecommunication
Laboratory. Note that the feature 3 (F3) alone is able to provide a recognition rate
92.40%.
Combinations Recognition Rates
(F3) 92.40%
(F3,F5) 93.39%
(F1,F3,F5) 93.87%
(F1,F2,F3,F5) 94.12%
(F1,F2,F3,F4,F5) 94.36%
Table 4.3. The best combinations of features.
4.5. Experiment for Top 3 Candidates.
From the above experiment, we obtain the best combinations of features which
are used to obtain the recognition rates for top 3 candidates. The results are given
in Table 4.4.
Combinations Recognition Rates
(F3) 97.67%
16
means and sample variances. To represent a syllable, the HMM system uses the
parameters, which include the initial distribution, the transition probabilities and
the probability of the observation Ot in each state, denoted by (pi, piij , bj(Ot)). The
number of parameters in the HMM is much more than that of our system. The
estimation of these parameters is not easy. First, one has to choose initial estimates
of the HMM parameters so that the forward-backward reestimation algorithm [35,
57-58], which is special case of the EM algorithm [58-59], modifies the parameters
to increase the probability of the training data until a local maximum has been
reached. The key question is that the local maximum may not be the global max-
imum of the likelihood function [58-59]. Furthermore, the probability bj(Ot) of an
observation Ot in each state is difficult to estimate, since it does not have a simple
normal distribution like a sample mean. For a syllable, the HMM method uses
various probability distributions including Gaussian mixtures to find the probabil-
ity bj(Ot) of the observation Ot. The computational cost for the convergence of
the iterative reestimation algorithm and the clustering of training data into states
for Gaussian mixtures is extremely high. In our method, we only estimate the
individual normal mean and variance for the syllable. In this study, we use a sim-
plified Bayes rule for classification, which is the best classifier with the minimum
probability of misclassification and which is fast in computation. Every step in the
simplified Bayes decision rule is a simple calculation except the logarithm of sample
variance (2.6), which is part of training time, not recognition time. If each of 408
mandarin syllables has an equal probability to occur, then the Bayes rule becomes
a ML classifier. However, each syllable does not occur equally likely. The classi-
fication in the HMM is to compute the probability of a sequence of observations
(O1, ..., OT ) which is the sum of the probabilities of the sequence of observations in
all possible state sequences. For the large vocabulary (408 syllables) classification,
18
References
[1] K. H. Davis, R. Biddulph, and S. Balashek, ”Automatic recognition of spoken
digits”, J. Acoust. Soc. Amer., vol. 24, pp. 637-642, 1952.
[2] H. Dudley and S. Balashek, ”Automatic recognition of phonetic patterns in
speech”, J. Acoust. Soc. Amer., vol. 30, pp. 721-739, 1958.
[3] P. B. Denes and M. V. Mathews, ”Spoken digit recognition using time frequency
pattern matching”, J. Acoust. Soc. Amer., vol. 32, pp. 1450-1455, 1960.
[4] V. W. Zue, ”The use of speech knowledge in automatic speech recognition”,
Proc. IEEE, vol. 73, no. 11, pp. 1602-1615, Nov. 1985.
[5] S. S. McCandless, ”An algorithm for automatic formant extraction using lin-
ear prediction spectra”, IEEE Trans. Acoust., Speech, Signal Processing, vol.
ASSP-22, no. 2, pp. 135-141, Apr. 1974.
[6] B. S. Atal and S. L. Hanauer, ”Speech analysis and synthesis by linear pre-
diction of the speech wave”, J. Acoust. Soc. Amer., vol. 50, pp. 637-655,
1971.
[7] J. Makhoul and J. Wolf, Linear Prediction and the Spectral Analysis of Speech,
Bolt, Baranek, and Newman, Inc., Cambridge, Mass., Rep. 2304, 1972.
[8] F. Itakura, ”Minimum prediction residual principle applied to speech recogni-
tion”, IEEE Trans. Acoust., Speech, Signal Processing, vol. ASSP-23, no. 1,
pp. 67-72, Feb. 1975.
[9] J. Makhoul, ”Linear prediction: A tutorial review”, Proc. IEEE, vol. 63, no.
4, pp. 561-580, Apr. 1975
[10] J. Tierney, ”A study of LPC analysis of speech in additive noise”, IEEE Trans.
Acoust. Speech, Signal Processing, vol. ASSP-28, no. 4, pp. 389-397, Aug.
1980.
[11] M. R. Sambur and L. R. Rabiner, ”A speaker-independent digit recognition
20
tion system”, IEEE Trans. Acoust., Speech, Signal Processing, vol. ASSP-34,
no.6, pp. 1465–1472, Dec. 1986.
[22] S. K. Das, ”Some experiments in discrete utterance recognition”, IEEE Trans.
Acoust., Speech, Signal Processing, vol. ASSP-30, no. 5, pp. 766-770, Oct.
1982.
[23] A. Aktas, B. Kammerer, W. Kupper, and H. Lagger, ”Large -vocabulary iso-
lated word recognition with past coarse time alignment”, IEEE ICASSP 86,
Tokyo, pp. 709-712, 1986.
[24] S. L. Banner, ”Simulating an acoustic recognizer”, IEEE ICASSP 86, pp. 725-
728, 1986.
[25] L. Rabiner and J. Wilpon, ”Speaker-independent isolated word recognition for
a moderate size (54 word) vocabulary”, IEEE Trans. Acoust., Speech and
Signal Processing, vol. ASSP-27, no.6, pp. 583-587, Dec. 1979.
[26] L. R. Rabiner, S. E. Levinson, A. E. Rosenberg, and J. G. Wilson, ”Speaker-
independent recognition of isolated words using clustering techniques”, IEEE
Trans. Acoust. Speech, Signal Processing, vol. ASSP-27, pp. 336-349, Aug.
1979.
[27] C. E. Shannon, ”Coding theorems for a discrete source with a fidelity criterion”,
in Information and Decision Processes, R. E. Machol, ed. New York: McGraw-
Hill, pp. 93-126, 1960.
[28] Y. Linde, A. Buzo, and R. M. Gray, ”An algorithm for vector quantizer design”,
IEEE Trans. Communications, vol. COM-28, no. 1, pp. 84-95, Jan. 1980.
[29] R. Gray, ”Vecter quantization”, IEEE ASSP Magazine, pp. 4-29, Apr. 1984.
[30] J. Makhoul, S. Roucos, and H. Gish, ”Vector quantization in speech coding”,
Proc. IEEE, vol. 73, no. 11, pp. 1551-1588, Nov. 1985.
[31] A. Buzo, A. Gray, R. Gray, and J. Markel, ”Speech coding based upon vector
22
[41] Y. Kamp, ”State reduction in hidden Markov chain used for speech recogni-
tion”, IEEE Trans. Acoust., Speech and Signal Processing, vol. ASS-33, no.4,
pp. 1138-1145, Oct. 1985.
[42] B. H. Juang, ”On the hidden Markov model and dynamic time warping for
speech recognition-a unified view”, AT&T Lab. Tech. J., vol. 63, no. 7, pp.
1213-1243, Sep. 1984.
[43] L. R. Bahl, F. Jelinek, and R. L. Mercer, ”A maximum likelihood approach
to continuous speech recognition”, IEEE Trans. Pattern Anal. and Machine
Intelligence, vol. PAMI-5, no.2, pp. 179-190, Mar. 1983.
[44] L. R. Rabiner and S. Levinson, ”A speaker-independent, syntax-directed, con-
nected word recognition system based on hidden Markov models and level
building”, IEEE Trans. Acoust., Speech and Signal Processing, vol. ASSP-33,
no. 3, pp.561-672, Jun. 1985.
[45] K. F. Lee, ”Context-dependent phonetic hidden Markov models for speaker-
independent continuous speech recognition”, IEEE Trans. Acoust., Speech and
Signal Processing, vol. ASSP-38, no. 4, pp. 599-609, Apr. 1990.
[46] M. R. Sambur and L. R. Rabiner, ”A speaker-independent digit-recognition
system”, B.S.T.J., vol. 54, no. 1, pp. 84-102, Jan. 1975.
[47] M. Elghonemy, M. Fikri, M. Hashish, and E. Talkhan, ”Speaker independent
isolated Arabic word recognition system”, IEEE ICASSP 86, Tokyo, pp. 697-
699, 1986.
[48] L. R. Rabiner, J. G. Wilpon, and F. K. Soong, ”High performance connected
digit recognition using hidden Markov models”, IEEE Trans. Acoust., Speech
and Signal Processing, vol. ASSP-37, no. 8, pp. 1214-1225, Aug. 1989.
[49] L. R. Rabiner, R. H. Juang, S. E. Levinson, and M. M. Sondhi, ”Recognition of
isolated digit using hidden Markov models with continuous mixture densities”,
24
