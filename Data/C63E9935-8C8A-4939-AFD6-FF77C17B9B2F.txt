 
 
1
1、計畫簡介 
1.1 前言 
 
 在目前的高資訊時代裡，多媒體的應用1層面已普及至一般家庭中。為了讓消費者接收到
的視聽效果有所提升，並使播放內容更逼真的展現真實環境，多媒體的內容製作與呈現方式
不斷地在更新中。在多媒體的內容表現上，使用者現今所扮演的角色不再只是被動的接受一
些固定範圍的資訊，其需求已演變至播放內容可以根據其所特別喜好而有所調整，即所謂的
互動性呈現（interactive display），此已成為一種不可不重視的趨勢。在以往的2D電視上，
其所提供的平面顯示無法提供觀看者真實環境的深度資訊，也因此人眼無法觀測到立體感的
呈現；此外，其所播放的影像內容完全是由製作者所決定，使用者無法根據自己想看的角度
與深遠去調整播放的內容，而讓視訊效果大打折扣。有鑑於此，在此計畫裡，如何建構出與
人互動的介面，並且如何依據人的需求呈現出相對的視訊資料將是我們的研究目標，我們希
望能根據使用者的不同觀察角度，估算並呈現其觀看視角所應接收到的視訊內容，並藉由立
體 (stereo) 及自由視角 (free viewpoint) 的影像播放，讓使用者可以在2D的螢幕上感受到3D
的真實視訊效果。 
虛擬影像合成技術在學術界中已發展了許多年，過去主要的演算法分為兩大類：影像式
合成 (Image-based Rendering) 技術以及模型式合成 (Model-based Rendering) 技術。影像式合
成是單純利用影像的資訊來進行合成，首先找出兩張(或以上)影像之間彼此的相關性，再根
據這些相關性進行線性內插以合成虛擬影像。而模型式合成 (Model-based Rendering) 則參考
了攝影機擺設與空間中物體的關係，利用這些關係建構出一個模型，最後反投射產生虛擬影
像。近幾年的研究則提出了一個新的虛擬影像合成技術，稱為深度影像式合成  (Depth 
Image-based Rendering) 技術。深度影像式合成是利用彩色影像和所屬的深度影像，搭配攝影
機參數投射產生虛擬影像。深度影像式合成在影像品質上優於影像式合成，加上複雜度遠低
於模型式合成，已成為近幾年研究的一項技術。 
 
1.2 研究目的 
圖一為3DAV的系統架構，本計畫在整體計畫裡所負責的部分即是 rendering，其中因為
考量 interactivity 的關係，我們所呈現的資料不是完全被動的由影像擷取端所決定，而是根
據使用者的喜好，利用已擷取到的些許固定視角視訊資料，描繪並呈現使用者所選取之自由
視角 (free viewpoint) 影像。自由式角影像合成方面，一方案將採用DIBR (Depth Image Based 
Rendering) 的方式，使用影像加深度資訊產生新影像，另一方案則採用3D model的形式，使
用已校正攝影機產生模型和紋理的對應來產生新影像。 
 
 
 
3
2、相關文獻探討 
 
為了達到互動性播放的需求，MPEG委員會特別制訂 3DAV (3D Audio Visual) 的標準，與
其有關的應用包括：omni-directional video、interactive stereo video和free viewpoint等等。其中
自由視角的播放為其中最基本但也最具有挑戰性的應用。針對此項需求，已有許多相對應的
系統因而產生，如表一所示，其中image-based 及 model-based的方法最廣為人知。 
 
表一. Methods for free view image generation and their features [1] 
method data acquisition 
data 
conversion 
view generation quality 
Image-based direct no 
warping  
projection 
not precise 
Integral 
photography 
direct optical optical precise 
Ray-space 
calibration/  
registration 
coordinate 
transform 
memory access 
Interpolation 
precise 
Model-based 
calibration  
registration 
3D model 
texture 
texture mapping not precise 
DIBR direct/depth no 
warping  
projection 
precise 
 
Image-based法產生中間影像的方式通常不是直接採用內差，而是採取彎曲 (warping) 或
是投射 (projection) 的方式而得。因為攝影機的數量有限，其相對應擷取到的影像數量也不
多，因此在產生多視域影像時，我們會利用視差的資訊來幫助描繪出不屬於原擷取視角的新
視角影像，此種方式屬於幾何補償描繪法 (rendering with geometry compensation)。 
Model-based的方法必須先偵測場景中的物體，再對此物體建立其3D模型，最後利用投射
的方式，即可產生自由視角影像。由於在3D立體模型的建立上，其困難度會隨著所建構物體
形狀結構的複雜而升高，時至今日，在實做上仍有許多困難之處有待克服，因此，我們知道
一個準確的3D模型建立，對後級自由視角影像的產生有決定性的影響。 
 
2.1 影像式新視角影像合成 (Image-based Rendering)  
影像式合成 (Image-based Rendering) 中最重要的問題在於如何精確求得兩張或以上的
攝影機影像之間的視差 (disparity)，之後利用此視差資訊搭配像素內插求得新視角影像。這
種方式不需要攝影機的內外部參數即可合成一組立體影像對，原因是它只需要影像中前景與
背景在三度空間中相對的距離差異，然後根據這樣的資訊來合成另一眼的影像。合成的影像
 
 
5
2.2 深度影像式新視角影像合成 (Depth Image Based Rendering) 
深度影像式合成法 (Depth Image Based Rendering, DIBR) 的任意視角影像可由攝影機影
像搭配深度影像產生，示意圖如圖2.2。接下來我們先說明許多與 DIBR [4]相關的基本理論，
說明二維攝影機影像與三維空間物體之間的關係。包含針孔攝影機模型、三維投射原理以及
虛擬攝影機的影像合成。 
 
       
圖2.2 2D+depth method 
 
2.2.1 針孔攝影機模型 (Pinhole Camera Model) 
 
 
圖2.3 針孔攝影機模型 (Pinhole Camera Model) 
 
如圖2.3所示，在三維空間中的一點 M，其齊次座標 (homogenous coordinate) 表示法為
M = [ X Y Z 1 ]T。當 M 在空間中投射於一個二維影像平面上，所交於平面中的一點 m，其
齊次座標表示法為m = [ x y 1 ]T。其中三維點 M 與二維點 m 之間的關係如下： 
                         s m P M⋅ = ⋅                                 (2.1) 
其中 (2.1) 式中的 s 為一個非零的參數，P為一個3×4的投射矩陣 (projection matrix)。而P3×4
又可分解成以下的式子： 
 [ ]|P A R T= ⋅                                  (2.2) 
+ = 
 
 
7
2.2.3 虛擬攝影機影像合成 
在三維投射中我們得知兩張攝影機影像中的內容有著彼此對應的關係存在，假設已知一
架攝影機 α 的影像和攝影機 α 的投射相關參數，欲合成目標攝影機 β 的影像，只需要攝影
機 β 的投射矩陣，即可合成與目標攝影機 β 影像相似的虛擬影像。這樣的推論可由(2.3)、
(2.4)式說明，數學表示式如下： 
      ( )1 1M R A D m A Tα α α α α α− −= ⋅ ⋅ ⋅ − ⋅                           (2.5) 
       D m A R M A Tβ β β β β β⋅ = ⋅ ⋅ + ⋅                                  (2.6) 
其中 (2.5) 式代表攝影機 α 影像資訊投射至空間中的數學表示式，(2.6) 式表示空間中的一
點反投射至攝影機 β 影像平面上的數學表示式。結合 (2.5)、(2.6) 式我們可以得到一個虛擬
影像合成的公式如 (2.7) 式所示： 
        ( )1 1D m A R R A D m A T A Tβ β β β α α α α α α β β− −⋅ = ⋅ ⋅ ⋅ ⋅ ⋅ − ⋅ + ⋅                  (2.7) 
(2.7) 式中的深度值 D 等同於 (2.4) 式中的非零參數 s。每個影像點都有其對應的深度值，
這些深度值參數紀錄了二維影像點與對應的三維空間點之間距離的關係。深度值通常會經過
轉換公式儲存為二維的深度影像 (depth image)。圖2.5和圖2.6分別為一架攝影機所顯示的影像
以及所屬的深度影像。 
 
      
圖2.5 彩色影像                         圖2.6 對應的深度影像 
 
(2.5) 式中的Aα、Rα 和 Tα、為攝影機 α 的內外部參數矩陣，Aβ、Rβ 和 Tβ 為攝影機 β 的
內外部參數矩陣。mα 為攝影機 α 影像的點座標位置，mβ 為攝影機 β 影像的點座標位置。
將攝影機 α 影像點座標 mα 代入 (2.7) 式，搭配深度值 Dα 計算出對應的攝影機 β 影像點
座標 mβ，再將攝影機 α 影像點的紋理填入點座標 mβ 的位置中，由攝影機 α 影像所投射出
的虛擬攝影機 β 影像就完成了。 
若要合成任意虛擬攝影機視角的所應看到影像，只要有此虛擬攝影機的內外部參數資
訊，就可以合成在空間中無攝影機擺設之位置所欲見到的虛擬攝影機影像。由於攝影機的內
外部參數資訊能夠推算出攝影機在空間中的位置，所以不同的攝影機內外部參數資訊可視為
 
 
9
          
圖2.10 有空洞的左眼合成影像                  圖2.11 內插填補後完整的左眼合成影像 
 
2.2.5 深度影像平滑化 (Depth Image Smoothing) 
雙眼影像對的合成影像中由於遮蔽效應而產生的空白區域，利用內插方式填補可能會造
成影像模糊的現象，如圖2.11中女軍官的頭髮處。在文獻[6]中提出深度影像平滑化 (depth 
image smoothing) 的方法來填補空白區域。作者將深度影像經過方向性的高斯濾波處裡，得
到模糊的深度影像，如圖2.12所示。利用模糊的深度影像來投射左眼影像，能夠有效的縮小
空白區域，又能使左眼合成影像邊緣模糊的問題得到改善。影像結果如圖2.13所示。還有許
多相關文獻[7][8]致力於如何縮小合成時所造成的空白區域，並保持好的影像品質。 
 
          
圖2.12 方向性高斯模糊深度影像               圖2.13 採用模糊深度影像合成的左眼影像 
 
2.2.6 深度影像的壓縮 (Depth Image Compression) 
當使用者利用DIBR產生任意視角影像時，並在真實攝影機影像與虛擬攝影機影像之間移
動觀看時，不同視角影像之間的切換必須非常平滑。文獻 [9-10] 中提到利用DIBR合成虛擬
影像時所遭遇的問題以及解決的方法。另外，深度影像的壓縮 [11] 對於合成影像的視覺品
質也是值得探討的一個項目。許多文獻提到如何有效的壓縮深度影像，而沒有考慮到深度影
像的壓縮對於自由視角合成影像視覺品質的影響。文獻 [12] 中提出一個能夠有效壓縮深度
影像的方式，以此壓縮後的深度資訊所合成的影像仍能夠保持不錯的視覺品質。壓縮的方式
是先將深度影像根據灰階的差異分割成不同的區塊，如圖2.14和圖2.15所示，在對每個區塊進
行壓縮。這樣的方式能夠保留深度影像中的邊緣資訊，免於因壓縮而造成的高頻失真。圖2.16
為此文獻中合成影像的結果。 
 
 
11
三維模型，由這個三維模型可以求得任兩個鄰近攝影機影像的對應關係 (correspondence)。利
用任意兩張鄰近攝影機影像的資訊，配合對應的資訊，作者運用影像內插 (view interpolation)
及影像形變 (view morphing)的概念合成虛擬影像。此外，利用三維模型的資訊，兩張鄰近攝
影機影像間遮蔽區域 (occlusion)的問題也將獲得解決。此方式雖改善了影像式合成中遮蔽區
域的問題，但建構三維模型卻是非常費時，因此，增加合成影像的品質及同時降低合成影像
所需的時間是未來所須著重的重要課題。 
 
圖2.17 3D Room 攝影機配置 
     
2.3.2 獨立視角紋理對應 (View-independent Texture Mapping) 
 經由visual hull之重建 [14] ，可以取得相當許多空間上三維點資訊。然而，這些離散的
三維點資訊不足以讓模型完整。使用Power Crust方法將空間中三維點連接成連續的多邊形，
並會包覆住整個3D模型，再將多邊形化為多個三角形。如此一來，就建立完成網格 (mesh) 資
訊。藉由這些網格，便能透過不同的計算獲得更多資訊，如網格中的三角形與攝影機之間的
法向量、平面方程式等。而獨立視角紋理對應，此方法是以多台相機的顏色依權重混合後繪
製在3D模型表面上，也就是網格。而權重根據網格之法向量與各來源相機 (reference camera) 
視線間的夾角而定。若夾角越接近180度，也就是此網格正面有較大的範圍被該相機擷取到，
且其權重值越大，將混合後取得的顏色把一塊三角形填為同一顏色。然而，此方法產出的影
像品質卻不盡理想。由於網格與網格之間的顏色並不一定平滑連續，造成呈現投射到影像上
會有不連續的狀況發生。 
文獻 [15] (View-dependent Vertex-based Texture Mapping) 提出解決方法。該作者取代以網
格法向量與各相機視線間的夾角而定的權重值，改以虛擬攝影機新視角視線與各來源相機視
線的夾角作為權重wc，c為來源相機編號。夾角越小表示該來源相機視線越靠近新視角視線，
所佔權重值越大。如此一來，影像上不連續的狀況便可改善不少。然而，該文獻是將空間中
的三維點填上顏色。若網格不夠細緻的話，會讓新視角輸出影像較為模糊。最後，在文獻 [16] 
中提到，即使像機再多，顏色混合時只需要最靠近的2~3台相機即可，過多反而造成錯誤的顏
色被混入其中。 
 
 
13
解決方法： 
此問題可以參考深度影像的資訊來判斷此點是否為前景。根據每個點相對應的深度值來
判斷的方式，我們稱為以深度資訊為考量的投射 (depth priority-based projection) [17]，當三維
空間中的多點投射在二維影像平面上的同一點時，這些三維空間點中深度值最小的點，表示
距離攝影機最近，可將此點視為前景，而將之投射到目標影像平面上，解決後結果如圖3.4。 
 
          
 圖3.3 未參考深度資訊投射之結果                 圖3.4 參考深度資訊投射之結果 
 
3.1.3 遮蔽區域的填補 (Oocclusion Filling)  
 
問題： 
 兩架視角不同的攝影機在拍攝同一物體時，其中一架攝影機會因為視角不同的關係而看
不見物體的某些部分，而此區域稱為遮蔽區域，以圖3.5左邊攝影機為例，其遮蔽區域為紅色
區塊。 
 
圖3.5 攝影機遮蔽區 
 
解決方法： 
我們可以利用鄰近攝影機影像，來修補因遮蔽而無法填補的區域，假設左邊攝影機和右
邊攝影機的共同視角資訊為白色，如圖3.5，由於攝影機視角有限，左邊攝影機會沒有紅色區
域的資訊，可藉由右邊攝影機補助來消除遮蔽區域，在實際上合成時，圖3.6顯示的是有遮蔽
狀況下的合成影像，考慮另外一側的攝影機影像後的結果如圖3.7所示。 
 
 
 
15
題，如[19]所提供的深度資訊，這些深度資訊於估測時並未考慮到時間上一致性的問題，造
成相連時間點上的物體同為不動的背景，但彼此的深度資訊卻不盡相同，如圖3.11和圖3.12
紅色圈起的部份，兩者為不動的背景，但對應的深度資訊在不同的時間點上卻不同，如圖3.13
和圖3.14所示，因此在合成自由視角的視訊序列時，將產生連續畫面的不一致。 
 
    
圖3.11 Cam4彩色影像 (T=0)                    圖3.12 Cam4彩色影像 (T=1) 
    
圖3.13 Cam4深度影像 (T=0)                     圖3.14 Cam4深度影像 (T=1) 
 
解決方法： 
 為了改善此現象，必須將背景深度做同步統一的步驟，我們使用背景相減法找出背景不
動區域，圖3.15為相減後產生之結果，可設定一閥值（threshold）判定為是否為背景，大於閥
值則歸類為前景，二位元圖顏色為白色，小於閥值歸類為背景區域，二位元圖顏色為黑色，
產生二位元圖遮罩。 
 
 
圖3.15 背景相減法 
 
 
17
       
圖3.20 深度未修正後投影到Cam4                     圖3.21 深度修正後投影到Cam4 
 
3.2 模型式新視角影像合成所遭遇的問題與解決方法 
 
問題： 
2.4.2節中提到文獻 [16] 是將空間中的三維點填上顏色。若網格不夠細緻的話，在空間3D
模型投影至二維影像平面時，會讓許多影像平面上的像素點都對應到同一塊 mesh，使的影像
平面上該區域都是相同的顏色，使新視角影像輸出時影像較為模糊，讓觀看影像品質不佳。 
 
解決方法： 
這裡提出使用一種以像素 (pixel) 為基底的方法做新視角影像輸出。如圖3.22，假設已知
新視角相機的相機參數，那麼藉此便可以定出相機中心與每個像素在空間中的座標。利用光
追蹤 (ray tracing) 的方式，空間中的相機中心與像素一點 P 的射線便會與3D模型交於一點
或多點 (有可能交於模型背部點)，取最近的點稱之點 V。將點V投影回各來源相機的影像平
面上抓取顏色資訊，並乘以權重值wc混合形成像素點 P 的顏色。這樣便同時解決影像上不連
續的問題且提高了影像品質。 
 
圖3.22 投射模型 
 
 
 
19
z 將三角形內之每一點都必須找到該對應網格索引值。位於此三角形內，每一點深度值
除了 a、b、c 三點，剩餘的皆是內插出來的深度值，由於是共面所以該三角形區塊內
的深度值不能差異過大，這一些內插出來的深度值可以幫助判斷是否為距離最近的
mesh，當從模型依射線投影至影像平面像素點時會有一個深度值，前面已有提過射線
不只交於一個 mesh，當交於多個 mesh，就會有多個深度值，為了判斷是否離最近的
mesh，若該像素點所內插出深度值接近於 mesh 射線投影回至影像平面的深度，我們
就認定該像素點相交於該 mesh。假設離最近得 mesh 編號為 5，則將網格索引值 (mesh 
index) 5 填入網格對照表，這樣就可以快速得知影像平面上對應那一塊 mesh。 
 
 
圖 3.24 深度值內插 
 
 
圖3.25 網格對照表 
 
3. 取得顏色 
z 每一像素點以網格對照表找此射線對應的網格索引值 
z 反投影回各相交於此點的來源射影機之顏色 
z 依照每隻攝影機距離計算權重值 wc  
z 將權重值正規化 
4. 依照每隻攝影機提供的顏色乘上權重後，將提供之顏色混合填上新視角影像上 
a 
b 
c 
P
 
 
21
         
     圖4.3 “Breakdancers”虛擬影像 (k = 4)               圖4.4 “Breakdancers”虛擬影像 (k = 10) 
 
由實驗結果得知，當 k = 2時，消除空洞的成果就已經相當不錯。k = 4和 k = 10的實驗結
果與k =2差異不大，但卻需要花更多時間。“Ballet”實驗結果如下。 
 
         
圖4.5 “Ballet”虛擬3號攝影機彩色影像               圖4.6 “Ballet”虛擬影像 (k=2) 
 
         
圖4.7 “Ballet”虛擬影像 (k = 4)                 圖4.8 “Ballet”虛擬影像  (k = 10) 
 
4.1.2 深度資訊為考量的投射實驗結果 
在三維投射 (3D warping)過程中，若有多對一投射的情形發生，這時候很容易造成因前
景資訊被背景資訊覆蓋而產生瑕疵。以深度資訊為考量的投射方式能夠避免因前景資訊被背
景資訊覆蓋的問題。圖4.9為沒有參考深度順序所投射的實驗結果，可以看見前景中舞者的左
手及頭部被背景遮蓋。圖4.10為參考深度順序後所投射而得的實驗結果。 
 
 
 
23
 
 
圖4.15 遮蔽區域填補後的“Breakdancers”4號攝影機影像 
 
              
圖4.16 由3號攝影機影像所合成之虛擬4號攝影機影像    4.17 由5號攝影機影像所合成之虛擬4號攝影機影像 
 
 
圖4.18 遮蔽區域填補後的“Ballet”4號攝影機影像 
 
4.1.4 邊緣遮蔽擴張與深度影像膨脹實驗結果 
 在仔細觀察合成的影像後可以發現，由於深度影像估測的誤差造成物體邊緣附近有瑕疵
產生，如圖4.19中人臉右側的曲線。我們提出邊緣遮蔽擴張 (boundary occlusion dilation) 與深
度影像膨脹 (depth image dilation) 兩種方法來改善。圖4.20和圖4.21分別為這兩種方法的結
果。 
討論過特定視角的虛擬攝影機影像後，我們進一步討論任意視角虛擬攝影機影像的合
成。我們利用虛擬攝影機參數來合成任意視角的影像，如之前所描述的方法。實驗之設定為3
 
 
25
4.2 子計劃一提供影像 
 此實驗使用子計劃一中環場攝影機所真實拍攝的資訊，並且使用3D模型所產生的深度資
訊提供DIBR合成，探討攝影機之間距離較大且彎曲角度較大的實驗狀況，以下說明攝影機的
擺設情況。 
 
4.2.1 實驗環境攝影機架設位置 
相機為環形擺設如圖4.23，分三層，共13架，每架相機影像解析度皆為640×480。第一層
8架 (藍點)，每45度1架，第二層4架 (紅點)，俯視角60度，每90度1架，第三層1架 (綠點)，
垂直往下看。每個時間點皆產生一3D模型，以T=0時刻為例，其3D模型共包含13303個 vertex，
30543個 mesh。 
  
圖4.23 攝影機擺設位置 
以下分別為攝影機補捉之原始前景畫面圖與經過3D重建後所得到的深度圖： 
 
    
cam 0    cam 1    cam 2     cam 3 
    
cam4   cam 5    cam 6     cam 7 
Cam 0 
Cam 1 
Cam 2 
Cam 3 
Cam 4 
Cam 8 
Cam9 
Cam 5 
Cam7 
Cam 11 
Cam 10 
Cam 6 
Cam 12 
 
 
27
4.2.2 DIBR 新視角影像的合成 
假設新視角影像的攝影機位於攝影機5 和攝影機2 之間，其旋轉矩陣與位移矩陣下，在
空間的位置如圖4.26所示，紅點位置為虛擬攝影機，原本影像大小為640×480，為了能使圖較
清楚呈現在報告上，將圖不必要的藍色背景去除，影像大小變成260×200，結果如圖4.27所示。
此合成影像使用2張彩色影像加上深度資訊，並且套用本計畫提出DIBR之問題改善方式，包
括 Scatter Filtering、Depth Priority Based、Projection Occlusion Filling。 
       novel view 1的R|T矩陣
-0.028835 -0.998987 -0.034551 229.820187
0.439763 -0.043719 0.897049 -230.717278 
-0.897651 0.010672 0.440578 1648.765533
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
       (4.1) 
       novel view 2的R|T矩陣
-0.028835 -0.998987 -0.034551 35.324682
0.439763 -0.043719 0.897049 -306.342127 
-0.897651 0.010672 0.440578 1689.124543
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
       (4.2) 
   
圖4.26 虛擬攝影機位置1         圖4.27 以DIBR產生novel view 1 
        
  圖4.28 虛擬攝影機位置2         圖4.29 以DIBR產生novel view 2 
 
Cam 2 Cam 5 
Novel view 2 
Cam 2 Cam 5 
Novel view 1 
 
 
29
5、成果自評 
    
   實驗結果顯示DIBR的影像合成效果優於IBR合成法，但DIBR於合成時仍有許多問題需要
解決，以下為本計畫針對各種問題所提出的解決方法。 
1. Hole Problem 
Solution:以散射填補 (Scatter Filling)的方式，將三維空間與二維影像平面之間點的一
對一投射改為一對多投射，如此就能夠減少投射的合成影像中空洞的現象。 
2. Occlusion Problem 
Solution: 利用兩張或多於兩張的影像來填補用單一影像於DIBR合成時出現的遮蔽區
域。 
3. Dis-occlusion Problem 
Solution: 以深度資訊為考量的投射 (Projection Based on Depth Priority)。當三維空間
中的多點投射在二維影像平面上的同一點時，這些三維空間點中深度值最小的點，表
示距離攝影機最近，可將此點視為前景，而投射到目標的影像平面上。 
4. Erroneous Depth-image Problem 
Solution：以邊緣遮蔽擴張 (Boundary Occlusion Dilation) 以及深度影像膨脹 (Depth 
Image Dilation) 而獲得合成影像品質的改善。 
5. Depth inconsistency in synthesized video 
 Solution：跟據前一張畫面，可判斷目前畫面中的背景部分，若屬於背景，則此背景部
分的深度將參照前一張影像之深度，使影像前後張之不動物體部份的深度一致，合成
的視訊畫面將不會有閃爍的現象。  
 
   實驗顯示，本計畫裡所提出的方法可以讓合成的虛擬視角攝影機影像擁有好的視覺品質，
並且在不同視角轉換的狀況下也非常平滑。 
此外，在DIBR與模型式合成法於自由視角影像上之合成可看出，當擺設攝影機的彎曲角
度過大和距離過遠時，使用DIBR會使得遮蔽區域過大，無法有效合成出較好的結果，而使用
3D model-based合成法則會有較佳的效果呈現。 
 
 
31
[16] P. V. Goswami, A. Narayanan, P.J. Dwivedi, S. Penta, S.K., “Depth Image: Representations 
and Real-time Rendering,” 3DPVT, June 2006 
[17] 許家榮, “Novel View Synthesis For Free View-point Video System,” 國立中正大學電機工
程研究所碩士論文 
[18] Chia Long Hsu, Jui-chiu Chiang, “Novel View Synthesis using Depth Information for 
3DTV,” Proc. of The 21th IPPR Conference on Computer Vision, Graphics, and Image 
Processing, Taiwan, Aug. 2008. 
[19] http://www.research.microsoft.com/vision/ImageBasedRealities/3DVideoDownload 
[20] C. L. Zitnick, S. B. Kang, M. Uyttendaele, S. A. J. Winder, and R. Szeliski, “High-quality 
Video View Interpolation Using a Layered Representation”, In ACM SIGGRAPH and ACM 
Trans. on Graphics, pp. 600-608, Aug. 2004. 
