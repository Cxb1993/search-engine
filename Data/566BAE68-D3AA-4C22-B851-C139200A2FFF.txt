ANGEL: A New Effective and Efficient Hybrid Clustering Method for Large Databases 
 
Cheng-Fa Tsai1 and Chia-Chen Yen1 
Department of Management Information Systems 
National Pingtung University of Science and Technology, 91201 Pingtung, Taiwan 
{ cftsai, m9556001}@mail.npust.edu.tw 
 
Abstract. This paper presents a new clustering 
algorithm named ANGEL, capable of 
satisfying various clustering requirements in 
data mining applications. As a hybrid method 
that employs discrete-degree and 
density-attractor, the proposed algorithm 
identifies the main structure of clusters 
without including the edge of clusters and, 
then, implements the DBSCAN algorithm to 
detect the arbitrary edge of the main structure 
of clusters. Experiment results indicate that 
the new algorithm accurately recognizes the 
entire cluster, and efficiently solves the 
problem of indentation for cluster. Simulation 
results reveal that the proposed new clustering 
algorithm performs better than some existing 
well-known approaches such as the K-means, 
DBSCAN, CLIQUE and GDH methods. 
Additionally, the proposed algorithm performs 
very fast and produces much smaller errors 
than the K-means, DBSCAN, CLIQUE and 
GDH approaches in most the cases examined 
herein.  
Keywords: data clustering, data mining, 
hybrid clustering algorithm 
 
1 Introduction 
Clustering in data mining is essential for 
various business applications. Clustering 
involves recognizing useful patterns in large 
databases, and using them to accurately 
predict feature behavior. Numerous data 
clustering schemes have been proposed in 
recent years, subsequently attracting strong 
attention. Data mining applications place 
specific requirements on data clustering 
approaches involving the capability to obtain 
fast clusters, end-user comprehensibility of 
the results, scalability and insensitivity to the 
order of input records. Many existing 
clustering methods have high computational 
time, or may have pattern recognition 
problems when using large databases. 
Therefore, an efficient and effective clustering 
algorithm is important. Clustering approaches 
can be categorized as partitioning, hierarchical, 
density-based, grid-based and mixed. 
Partitioning methods like K-means [3] attempt 
to identify K partitions containing similar 
objects. K-means algorithm is easily and 
quickly implemented, but does not accurately 
recognizing the shapes of patterns that are 
non-spherical or not the same size. 
 2
This section describes the merits and 
limitations of various clustering algorithms. 
2.1 K-means 
K-means [3], which was presented in 1967, 
was the first clustering algorithm. It includes 
the following steps. (1) Select K partition 
centers randomly from data sets. (2) Assign 
each object to its closest center. (3) 
Recalculate K partition centers and repeat step 
2 until the centers convergence. The 
performance of K-means depends on step 1. 
Hence, K-means always converges to a local 
optimum. Moreover, K-means can not filter 
noise, and does not cluster non-spherical 
patterns correctly. 
 
2.2 CLIQUE 
CLIQUE [12] integrates grid-based and 
density-based clustering techniques. Its 
special ability is in identifying clusters in 
high-dimensional data. CLIQUE initially 
generates a grid map from feature space. For 
each dimension, the algorithm identifies the 
high-density units by using the priori method: 
if a k-dimension region is dense, then 
k-1-dimension region must also be dense. 
Therefore, CLIQUE significantly shrinks the 
search space by first examining combinations 
of low-dimension regions. Finally, the dense 
regions are grouped into clusters by 
examining their connectivity. Although 
CLIQUE has a fast clustering time, but its 
cluster boundaries are either horizontal or 
vertical, owing to the nature of the rectangular 
grid. 
 
2.3 DBSCAN 
DBSCAN is a first-density-detecting 
method, which depends on two arguments, 
namely Eps and MinPts. Eps denotes the 
radius of the search circle, and MinPts 
represents a number of minimal neighbors in 
the search circle. These arguments are adopted 
to examine the ε -neighbors contained in 
each object. In the database, object P can be 
termed the core point if point P has at least 
MinPts neighbors and has a maximum 
distance from any object of Eps. The search 
process is then expanded form point P to its 
neighbors. If point Q, which denotes a 
neighbor of point P, has a number of ε
-neighbors below MinPts, then point Q is 
called a border point. DBSCAN can 
accurately recognize any arbitrary pattern by 
applying this expansion. Since each expansion 
must examine all objects, the time complexity 
of DBSCAN is also high when the database 
size is large. 
 
2.4 GDH 
GDH is a hybrid grid-based, density-based 
and hierarchical clustering technique, 
presented by Wang [17]. GDH refers the idea 
of density function [7] and gradient decrease 
[14] from the study by Hinneburg and concept 
of sliding window [14]. It can significantly 
 4
called populated cube. 
 
 
Influence function is defined as a 
mathematical description that has the 
influence of an object has within its 
neighborhood. The density function is defined 
as the sum of influence function of all objects 
in the region, and can be any arbitrary 
function. For simplicity, this work applies the 
Euclidean density function and Gaussian 
representation. The Gaussian density function 
is given by: 
, 
where N represents the number of objects of 
the region; d(x,xi)2 denotes the distance 
between x and xi, and σ is the smooth-index. 
A populated cube is called a density-attractor 
if it has the highest Gaussian density function 
among all cubes. The density-attractor is the 
initial point of search space, as shown in 
Fig.2. 
 
Fig. 2. The density-attractor diagram. 
 
3.1.2 Identifying the main structure 
This investigation employs the 
discrete-degree as a measure of grid-density 
detecting preprocesses to identify the main 
structure of cluster excluding the cluster edge. 
All populated cubes are split up into nine 
sub-hypercubes. ANGEL computes the 
number of objects within each sub-hypercube 
according to the location of the objects. The 
range of discrete-degree is derived and 
defined as follows: 
UL = ( n / 9 ) * ( 1 + α ) 
LL = ( n / 9 ) * ( 1 - α ) 
  UL and LL represent the upper and limits 
of discrete-degree, respectively. n denotes the 
number of objects of populated cube, and α 
is the percentage of the tolerance value. If all 
of the density of sub-hypercubes in the 
hypercube are between UL and LL, then the 
density in the hypercube is equally distributed, 
and the hypercube is the main structure of the 
cluster, and can be assigned to cluster directly. 
Otherwise, the edge detection method has to 
be utilized, as displayed in the left diagram on 
hypercube B, E, I and L of Fig. 3. 
 
 6
19    SearchNeighbors(C); 
20   END IF-ELSE 
21   ClusterId++; 
22 END WHILE 
END ANGEL 
 
DataSets is an entire database or a partial 
dataset. ξ represents the length of a hypercube; 
α denotes the percentage tolerance value, and 
τ is the threshold of the hypercube's density. 
Eps represents a search radius, and MinPts 
denotes the smallest number of objects in the 
region. The algorithm is presented step by 
step below. 
 
Step 1.Initialization of all arguments. 
Step 2.CreatGridStructure() function generates the 
structure of the hypercube map, and assigns all 
objects to the appropriate hypercube. 
Step 3.CacluateGridsInfo() function computes the 
range of discrete-degree for each hypercube, 
filters it, and returns the populated-cube-set 
PopulCubes. 
Step 4.Repeat the process by while loop. 
Step 5.SelectDensityAttractor() function obtains the 
density-attractor, and returns to cube C. 
Step 6.If cube C is null, then stop the algorithm. 
Step 7.If the discrete-degree of cube C is equally 
distributed, then assign cube C directly to the 
cluster, and continue searching neighbors by 
the SearchNeighbors() function. 
Step 8.Otherwise, ANGEL applies DBSCAN for the 
edge detecting and returns a sub-cluster set to 
Cs. 
Step 9.Assign a sub-cluster of Cs resulting from a 
DBSCAN run if its border points are close to 
cube C using the ChangeClusterId() 
function. 
Step 10.ANGEL then searches the neighbors of the 
cube C with the SearchNeighbors() function. 
 
The neighbor searching process 
SearchNeighbors(Cube) is as follows: 
 
Step 11.The SearchNeighbors(Cube) function returns 
a set of neighbors NeighborCubes located on 
upside, downside, left side, right side, left up 
side, left down, right up side and right down 
side of the cube Cube. 
Step 12.Continue the process until the neighbors of the 
Cube is empty. 
Step 13.HighDensity() function returns a search 
subject to CurrCube with the highest density 
function. 
Step 14.As stated above, if the discrete-degree of cube 
CurrCube is equally distributed, then it is 
assigned directly to the cluster by 
ChangeClusterId() function, and the 
neighbors searching continues by the 
SearchNeighbors() function. 
Step 15.Otherwise, ANGEL applies DBSCAN for 
edge detection, and returns a sub-cluster set to 
NCs. 
Step 16.Each sub-cluster of NCs is assigned to a 
cluster if its border points are close to cube the 
CurrCube. 
 8
 
 
4.1 Clustering correctness experiments on 
ANGEL 
The seven datasets were adopted to 
examine the clustering accuracy. The datasets 
not only include simple shape of the datasets 
used in traditional methods’ experiments in 
which each cluster is separated far from the 
others, but also includes the many arbitrary 
shaped clusters that are close to each other. 
These datasets can help reveal clearly how 
ANGEL identifies arbitrary shapes. Fig. 5 
depicts the experimental results of ANGEL. 
 
 
Fig. 5. The experimental results (clustering correctness rate) of 
ANGEL; Data Set 1: 98.584%, Data Set 2: 99.688%, Data Set 3: 
98.534%, Data Set 4: 99.744%, Data Set 5: 99.505%, Data Set 6: 
99.458%, Data Set 7: 99.6%. 
The experimental results demonstrate that 
ANGEL can handle arbitrary shapes for 
clustering. However, K-means cannot 
recognize arbitrary shapes. Although CLIQUE 
and GDH could handle the arbitrary shapes in 
Dataset 4 to 7, CLIQUE could not smoothly 
detect the edges of clusters, or remove the 
indentations of neighboring clusters, due to 
the nature of the rectangular grid. 
Additionally, the gradient decrease function in 
GDH placed some clusters in the wrong 
position if the hypercubes were neighbors but 
the gradient decrease between the hypercubes 
was too high. Table 1 shows the clustering 
experimental results with ANGEL, K-means, 
 10
pp. 281-297, 1967. 
4 Zhang, T., Ramakrishnan, R., Livny, M., 
“BIRCH: An Efficient Data Clustering 
Method for Very Large Databases,” 
Proceedings of the ACM SIGMOD 
International Conference on Management 
of Data, Vol. 25, No. 2, pp. 103-114, 
1996. 
5 Ester, M., Kriegel, H. P., Sander, J., Xu, 
X., “A Density-Based Algorithm for 
Discovering Clusters in Large Spatial 
Databases with Noise,” Proceedings of 
the 2nd International Conference on 
Knowledge Discovery and Data Mining, 
pp. 226-231, 1996. 
6 Wang, W., Yang, J., Muntz, R., “STING: 
A Statistical Information Grid Approach 
to Spatial Data Mining,” Proceedings of 
23rd International Conference on Very 
Large Data Bases, pp. 186-195, 1997. 
7 Hinneburg, A., Keim, D. A., “An 
Efficient Approach to Clustering in Large 
Multimedia Databases with Noise,” 
Proceedings of the 4th International 
Conference on Knowledge Discovery and 
Data Mining, pp. 58-65, 1998. 
8 Guha, S., Rastogi, R., Shim, K., “CURE: 
An Efficient Clustering Algorithm for 
Large Databases,” Proceedings of the 
ACM SIGMOD International Conference 
on Management of Data, pp. 73-84, 1998. 
9 Agrawal, R., Gehrke, J., Gunopulos, D., 
Raghavan, P., “Automatic Subspace 
Clustering of High Dimensional Data for 
Data Mining Applications,” Proceedings 
of the ACM SIGMOD International 
Conference on Management of Data. 
Seattle, Washington, D C: ACM Press, pp. 
94-105, 1998. 
10 Karypis, G., Han, E.-H., Kumar, V., 
“Chameleon: A Hierarchical Clustering 
Algorithm Using Dynamic Modeling,” 
IEEE Computer, pp. 68-75, 1999. 
11 Guha, S., Rastogi, R., Shim, K., “ROCK: 
A Robust Clustering Algorithm for 
Categorical Attributes,” Proceedings of 
the 15th International Conference on Data 
Engineering, pp. 512-521, 1999. 
12 Ng, R. T., Han, J., “CLARANS: A 
Method for Clustering Objects for Spatial 
Data Mining,” IEEE Transactions on 
Knowledge and Data Engineering, Vol.14, 
No. 5, pp. 1003-1016, 2002. 
13 El-Sonbaty, Y., Ismail, M. A., Farouk, M., 
“An Efficient Density Based Clustering 
Algorithm for Large Databases,” 
Proceedings of the 16th IEEE ICTAI 
2004, pp. 673-677, 2004. 
14 Tsai, C. F., Wang, T. P., “GDH: An 
Effective and Efficient Approach to 
Detect Arbitrary Patterns in Clusters with 
Noises in Very Large Databases,” Degree 
of master at National Pingtung University 
of Science and Technology, Taiwan, 
2006. 
 
 12
計畫成果自評 
 
本研究計畫就研究內容而言，與原計畫可謂
相符，並已達成部分預期目標。本研究計劃
於原計畫書內提出以ANGEL群集及MHSOM群
集分析之方法，此研究之成果已發表於
Lecture Notes in Artificial Intelligence, vol. 
4426, pp. 817-824, 2007, 接受率約 12％；將
本方法成功運用於 Unsupervised Anomaly 
Detection 方面亦發表於 Lecture Notes in 
Computer Science, Vol. 4985, pp. 356-365, 
2008 接受率約 16％；改善 ANGEL 群集方法
之 G-TREACLE 群集分析方法亦 發 表 於
Lecture Notes in Computer Science, Vol. 
5012, pp. 739-748, 2008,接受率約 30％；
MHSOM 群集方法應用於影像壓縮已發表於
2008 ICICIC 國際會議，並已投稿至國際期
刊中。此外，與本方法相關之分群技術及相
關應用計有 5個，業已通過屏東科技大學之
全額經費補助獎勵，委請五洲專利事務所向
經濟部專利標準局申請中華民國（四個）與
美國發明專利（三個）中。 
與本研究計畫相關之論文已發表於如下之
會議與期刊中： 
 
1. Cheng-Fa Tsai and Chia-Chen Yen,    
“ANGEL: A New Effective and Efficient 
Hybrid Clustering Technique for Large 
Databases, Lecture Notes in Artificial 
Intelligence, vol. 4426, pp. 817-824, 2007.                                                       
2. Cheng-Fa Tsai and Chia-Chen Yen, 
“Unsupervised Anomaly Detection Using 
HDG-Clustering Algorithm,” Lecture Notes in 
Computer Science, Vol. 4985, pp. 356-365, 
2008.             
3. Cheng-Fa Tsai and Chia-Chen Yen, 
“G-TREACLE: A New Grid-based and 
Tree-alike Pattern Clustering Technique for 
Large Databases,” Lecture Notes in Computer 
Science, Vol. 5012, pp. 739-748, 2008. 
4.  Cheng-Fa Tsai, Chen-An Jhuang, 
Chih-Wei Liu, ‘Gray Image Compression 
Using New Hierarchical Self-Organizing Map 
Technique,’ 2008 The Third International 
Conference on Innovative Computing, 
Information and Control （ICICIC）, June 18 
- 20, 2008, in CD-paper no. 575, Dalian, 
China （EI）.          
 
 
 
 
 
 
 
 
 
 
 
 
 
 14
 2 
Forecasting、Unification of Neural and Wavelet Networks and Fuzzy Systems
及 Independent Component Analysis 等多項熱門議題，發表論文甚多。 
而筆者之論文被安排於 7月 28 日 16:00-17:45 之 regular session 中第一篇口
頭論文發表(Oral Presentation)，Session Number 為 Session C: Artificial 
Intelligence, Session Chair 是 Dr.Byoung-Tak Zhang，來自 Seoul National 
University。其中主要探討有關於一種運用智慧型螞蟻演算法(Ant System)混
合節省啟發法(Saving Heuristic)於護理人員之排班系統中，根據醫療院所之
護理人員的休假及班別偏好及不違反硬性限制的原則下而設計，進而解決其排
班過程當中所需要花費更多時間及人力的問題。提供之功能包括有：維持排班
之公平性原則、讓護理主管製作班表的時間從人工的 2~3 個工作天縮減到 1 秒
內即可完成、可排除人為的疏失而造成的錯誤班表、讓班表可更加地彈性化、
讓休假日不違反勞基法規定的最低休假天數，以上多樣化的功能可以讓護理人
員在使用本軟體系統時可獲得快速且合理地的排班表，對護理人力作最恰當地
分配並減輕護理主管的負擔，並對護理的資訊化作出貢獻。 
會議中，在眾多的議題內，筆者特別注意的是與 Neural and Mental 
Development、Neural Networks for Diagnostics, Prediction and Control: 
Capabilities and Myths 有關的議題：其中有許多議題之內容其研究結果給予
筆者許多寶貴的經驗與啟發。特別是 Artificial Intelligence 受到了廣泛的
討論，有許多公司與研究單位提出不同的解決方案，預期將對知識探勘與知識
管理的技術產生重大的變革。 
 
二、 與會心得 
資訊技術之進步可謂瞬息萬變，一日千里。它是現代科技的整合，也是
一種計算系統，包括軟體與硬體。智慧型系統是最近非常熱門的研究領域，
許多學者投入了此方面的鑽研，所以最近與此方面相關的國內或國際學術會
議層出不窮，與人工智慧及資料庫相關所涵蓋的範圍極廣且複雜。由於硬體
不斷的革新，也引發了需多在整體設計架構上的變革需求。是以如何獲取最
新的資訊科技並加以實作應用，同時調整產業的方向與步調，將是日後國內
