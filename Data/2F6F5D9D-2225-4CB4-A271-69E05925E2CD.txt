 行政院國家科學委員會補助專題研究計畫 
█ 成 果 報 告   
□期中進度報告 
 
 
應用於格網與可擴充平行系統之通訊最佳化廣播與品質服務排程技
術之研究(3/3) 
 
計畫類別：個別型計畫  □整合型計畫 
計畫編號：NSC 97-2221-E-216-011-MY3 
執行期間：99 年 08 月 01 日至 100 年 07 月 31 日 
 
計畫主持人：許慶賢   中華大學資訊工程學系教授 
共同主持人： 
計畫參與人員： 陳世璋、陳泰龍 (中華大學工程科學研究所博士生) 
     徐一中、陳柏宇、張弘裕、蔡宗輝、朱元佑 
(中華大學資訊工程學系研究生) 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年二年後可公開查詢 
          
執行單位：中華大學資訊工程學系 
 
中 華 民 國    100   年  10    月   28   日 
 2 
行政院國家科學委員會專題研究計畫成果報告 
 
應用於格網與可擴充平行系統之通訊最佳化廣播與品
質服務排程技術之研究 
 
計畫編號：NSC 97-2221-E-216-011-MY3 
執行期限：99 年 8 月 1 日至 100 年 7 月 31 日 
主持人：許慶賢   中華大學資訊工程學系教授 
 
計畫參與人員： 陳世璋、陳泰龍 (中華大學工程科學研究所博士生) 
     徐一中、陳柏宇、張弘裕、蔡宗輝、朱元佑 
(中華大學資訊工程學系研究生) 
 
 
一、中文摘要 
 
本計劃是有關應用在格網環境中可擴充平行系統的通訊、廣播與品質服務
排程技術之最佳化。本計畫預計執行期限為三年，第一年，我們開發適用於相
同叢集網格架構的特殊型通訊局部化資料分割技術以及邏輯處理器與資料之間
對應的技術。並且提出適用於不同叢集網格架構的通用型通訊局部化資料分割
技術。利用 Hungarian method 根據已建立的動態評估外部通訊效能的模式在不
同數量的處理器組合下，找出最好的資料分割方式。第二年，在廣播演算法的
設計中，我們分為以圖為基礎與樹狀結構為基礎的演算法，在樹為基礎的演算
法當中，我們規劃運用不同的演算法來改良，為了避免圖形中產生迴圈造成重
複資料的傳遞，我們設計一個預先排程系統來解決。在高品質服務工作排程系
統下，基於傳統的 Min-Min 品質需求排程的基礎，我們提出有效的重新排程演
算法來改善減少工作時間或減少資源的使用量，為資源最佳化重新排程。第三
年，結合服務計算的概念，我們提出以品質服務為基礎的資源與工作排程技術，
包括資源成本(Economic Guided)與時間成本(Performance Guided)最佳化的技術。
基於傳統的 Min-Min、Max-Min 排程，我們也提出二階段、或多階段的重新排程
方法，藉以改善資源的使用效率以及滿足使用者的不同需求。結合格網經濟模
型，這一個部份的研發成果，也導入實際的格網系統進行實驗與部屬。本計畫
預計完成的研究項目，皆已經實作出來，並在相關期刊與研討會發表。 
 
關鍵詞：格網計算、可擴充平行系統、服務式計算、可擴充計算、通訊最佳化、廣播
演算法、P2P 技術、品質服務排程、異質性計算 
 4 
了無法滿足有特殊需求的工作內容，其整體系統效能也會因此大打折扣。有鑑於此，
因 應 新 世 代 的 服 務 式 計 算 (Service Computing) 與 服 務 導 向 架 構 (Service Oriented 
Architecture)，在計算格網系統上的工作排程技術，勢必需要加入可以滿足這樣目標的
技術，這也是目前及待解決與改良的問題。因為這些研究的方向使得格網的高效能運
算技術有更多的發展空間與更多的研究方向。 
如同前面所提到的，雖然有許多過去的研究都在探討這些問題，但是我們所要解
決與提出的是，適應於動態格網架構，並且以服務導向架構(SOA)為基礎，提出滿足使
用者服務品質(QoS)的解決方案。 
鑒於可擴充計算(Scalable Computing)已經成為新的計算模式。不論是多叢集系統，
格網系統，異質性網路系統，網路通訊則是這些系統共同倚賴的基本要素。因此，在
可擴充式計算系統，降低網路通訊成本往往是許多研究的第一考量。從演算法的角度
來說，最基本且重要的通訊機制就是訊息的廣播；而從資源管理與服務導向計算的角
度而言，有效運用系統資源與工作排程的相關技術，最受到重視。因應新世代的服務
式計算(Service Computing)與服務導向架構(Service Oriented Architecture)，在
本計畫中，我們將研發多個應用於格網與可擴充平行系統之通訊最佳化、廣播與以品
質服務為基礎的資源與工作排程技術。本計畫有三個主要的研究課題：一、發展適應
於多叢集系統下之通訊最佳化技術；利用多叢集系統下之處理器重新排序技術改善排
程工作的結果，此項成果可以直接應用在異質性格網系統。二、發展應用於異質性網
路的訊息廣播技術；根據不同的網路拓樸，採用適當的排程策略，並且開發最佳化的
評估模組，以實際的 work-load 分析效能。三、發產以品質服務為基礎的資源與工作
排程技術，結合格網經濟模型，將研發成果導入實際的格網系統。 
 
三、研究方法與成果 
 
在發展應用於格網與可擴充平行系統之通訊最佳化、廣播與品質服務排程技術之
研究，我們的工作主要包含以下研究課題： 
 
a. 多叢集系統通訊最佳化與動態資料配置 
b. 最佳化叢集式格網拓墣研究 
c. 適應於動態異質網路之訊息廣播技術 
 6 
 
 
圖二  多叢集之異質性網路頻寬 
 
  
(a) 
 
(b) 
 
圖三 (a) 不同叢集格網傳輸成本數值 (b) 傳輸成本最佳化數值 
 
我們所提出的資料分配技術，目的是為了降低通訊的成本並加速平行程式在處理
資料時所花費的時間，這個理論是運用在不同通訊成本所組成的環境上。由於我們的
理論分析與程式模擬在不同通訊成本下所執行的結果都比原先未經過處理器重排的配
置方法明顯有顯著的改善，並且優於在未考慮通訊成本情況下的配置方法。顯示這兩
 8 
於訊息廣播上較為複雜的排程模式，此架構主要由 Switch、Workstation、Bidirectional Link 
組成。分為以下三種情形： 
一、 所有的傳送端與接收端皆連接於同一個橋接器(Switch)，如圖五(a) 
二、 所有的傳送端連接於相同的橋接器，但是接收端連接相對於傳送端在不
同的橋接器上，如圖五(b) 
三、 每個傳送端與相對應的接收端接連接於不同的橋接器上，如圖五(c) 
對於資訊廣播的排程系統下，在程式執行的各個階段中，所有節點皆同時可廣播
與接收訊息。這種方法有時候會產生訊息碰撞所耗費的時間。 
 
 
圖五 異質性網路工作站(HNOW)架構傳送端與接收端相關位置 
 
針對 HNOW 的網路架構下，我們開發新的方法 Location Aware Broadcast Scheme 
(LABS)，提昇網路通訊時的頻頻壅塞問題與降低檔案廣播至錯誤路徑的機率。針對不
同的網路結構，我們考量節點數量 s，限制樹狀結構的寬度與高度，避免訊息廣播時的
廣播風爆與衝撞問題的產生，在各個節點建立不同的廣播路徑樹狀結構 Location 
Oriented Spanning Tree (LST)，與其它相關方法比較其執行上的效能，並且執行效能的評
估與測試，利用不同的工具偵測網路當時的效能，再根據所測得的數據給予各個網域
不同的權重值，決定工作與資料廣播的方式。對於分散式訊息廣播的技術，整合出一
套完整的資料配置技術。 
 10 
圖八為資料量訊息較小(2048 flits)時所產生的數據。圖九為資料量訊息較大(10240 
flits) 時所產生的數據比較。由數據可以得知在不同的網路速度下(2~8 Speed Types)，
LABS 都比相關的其他方法有更少的網路訊息傳送延遲，提升網路使用效率。 
0
2000
4000
6000
8000
10000
12000
16 32 64 128 256
A
v
e
r
a
g
e
 L
a
te
n
c
y
 (
μ
se
c
)
Number of workstations
2 speed types (LABS)
2 speed types (TWO-VBBS)
4 speed types (LABS)
4 speed types (TWO-VBBS)
6 speed types (LABS)
6 speed types (TWO-VBBS)
8 speed types (LABS)
8 speed types (TWO-VBBS)
 
圖八 HNOW 網路資料傳送數據比較(2048 flits) 
 
0
6000
12000
18000
24000
30000
36000
42000
48000
16 32 64 128 256
A
v
er
a
g
e 
L
a
te
n
cy
 (
μ
se
c)
Number of workstations
2 speed types (LABS)
2 speed types (TWO-VBBS)
4 speed types (LABS)
4 speed types (TWO-VBBS)
6 speed types (LABS)
6 speed types (TWO-VBBS)
8 speed types (LABS)
8 speed types (TWO-VBBS)
 
圖九 HNOW 網路資料傳送數據比較(10240 flits) 
 
3.3 以品質服務為基礎的資源與工作排程 
 
傳統的 Greedy、Min-min、Max-min、STA、STP、MTP 工作排程演算法，無法套用
在服務導向架構(Service Oriented Architecture)的系統中。我們開發以品質服務為基礎之時
間成本最佳化排程技術(Makespan Optimization Rescheduling)與品質服務為基礎之資源成
本最佳化排程技術(Resource Optimization Rescheduling)。 
實施的方法上，我們首先發展兩種基本類策略，一是工作時間最佳化重新排程(MOR)
 12 
(d) (NT=300, NR=50, QR=40%, HT=1, HQ=1) 
 
QT% 15% 30% 45% 60% 75% 
Min-Min 879.9 1380.2 1801.8 2217.0 2610.1 
QoS Guided Min-Min 558.4 915.9 1245.2 1580.3 1900.6 
MOR 474.2 817.1 1145.1 1478.5 1800.1 
Improved Ratio 15.07% 10.79% 8.04% 6.44% 5.29% 
  
(e) (NT=500, NR=50, QR=30%, QT=20%, HQ=1) 
 
HT 1 3 5 7 9 
Min-Min 1891.9 1945.1 1944.6 1926.1 1940.1 
QoS Guided Min-Min 1356.0 1346.4 1346.4 1354.9 1357.3 
MOR 1251.7 1241.4 1244.3 1252.0 1254.2 
Improved Ratio 7.69% 7.80% 7.58% 7.59% 7.59% 
  
(f) (NT=500, NR=50, QR=30%, QT=20%, HT=1) 
 
HQ 3 5 7 9 11 
Min-Min 1392.4 1553.9 1724.9 1871.7 2037.8 
QoS Guided Min-Min 867.5 1007.8 1148.2 1273.2 1423.1 
MOR 822.4 936.2 1056.7 1174.3 1316.7 
Improved Ratio 5.20% 7.11% 7.97% 7.77% 7.48% 
  
使用 ROR 演算法的條件時必須當某計算節點只分配到微小的工作而整體工作時間
與 MOR 演算法相等時，幫助排程系統快速得到一組重配置的方法，所須使用的運算節
點需搜尋該區域網格環境中所使用到的處理器個數，當所有的處理器分配到該工作執
行後不影響整體工作時間時，即完成所有動作。表二為 ROR 演算法之模擬效能對照表。 
 
表二 ROR 演算法之效能比較表 
(a) (NR=100, QR=30%, QT=20%, HT=1, HQ=1) 
 
Task Number (NT) 200 300 400 500 600 
QoS Guided Min-Min 100 100 100 100 100 
ROR 39.81 44.18 46.97 49.59 51.17 
Improved Ratio 60.19% 55.82% 53.03% 50.41% 48.83% 
  
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
 
Resource Number (NR) 50 70 90 110 130 
QoS Guided Min-Min 50 70 90 110 130 
ROR 26.04 35.21 43.65 50.79 58.15 
Improved Ratio 47.92% 49.70% 51.50% 53.83% 55.27% 
  
 14 
find the earliest completion time and the corresponding host 
    end for 
find the task tk with the minimum earliest completion time 
assign task tk to the host ml that gives it the earliest completion time 
delete task tk from Mv 
update dtl 
 update CTil for all i 
end do 
其中，CT : Completion time … …….(系統完成時間) 
 dt : delay time ………………..(網路延遲時間) 
 et : execute time ……………...(單一工作執行時間) 
 i : the job ID ………………….(單位工作編號) 
 j : the machine ID …………….(處理器編號) 
 
以工作時間最佳化重新排程 Makespan Optimization Rescheduling (MOR)的工作排程演
算法如下： 
         QOS guided scheduling algorithm ….. 
for CTj in all machines 
    find out the machine with maximum makespan CTmax and set it to be the 
standard 
end for 
do until no job can be rescheduled 
for job i in the found machine with CTmax  
        for all machine j 
           according to the job‟s QOS demand, find the adaptive machine j  
if the execute time of job i in machine j + the CTj < makespan 
       rescheduling the job i to machine j   
       update the CTj and CTmax 
   exit for 
end if 
        next for 
        if the job i can be reschedule 
find out the new machine with maximum CTmax 
        exit for 
end if 
next for 
end do  
 
以運算節點資源最佳化重新排程的 Rseource Optimization Rescheduling (ROR)的工作排
程演算法如下： 
         QOS guided scheduling algorithm ….. 
for m in all machines 
    find out the machine m with minimum count of jobs 
end for 
 16 
數量，檔案數量成長時，也有容錯的特性。本計畫預計完成的研究項目，皆已經實
作出來，並在相關期刊與研討會發表。 
本子計畫所開發的資源與工作排程系統，相較於現有的系統，採用以服務導向
架構為基礎的概念，具有設計上的彈性、與系統的輕量化，可以滿足不同使用者的
需求(Quality of Service)、以及高執行效率的特性。此架構具創新性；開發的元件都
符合可擴充式計算(Scalable Computing)的架構，加上 Grid 技術日漸成熟，未來將我
們所開發的技術與系統與既有的格網平台結合，亦是相當可行的作法。 
在軟體開發上，我們使用軟體工程較新的敏捷(Agile)開發模式，並與開放原始
碼社群整合。未來可以提供使用者一個安全、便利的格網平台，提供管理者一個集
中式的管理介面，與提供開發者一個具高度擴充性及相容性的系統架構，使得未來
管理的人力及時間成本大幅降低。從格網系統管理的角度來看，這也是本計劃的另
一個創新與貢獻。 
因應未來以服務為導向的資訊服務，我們結合格網經濟模型與 QoS 最佳化計算
於 Web 服務中，讓格網系統有更實際的應用價值；在未來格網系統的普及與推廣工
作上，也有很大的幫助。 
服務計算結合 Grid 的研究還在起步階段。未來，我們將把研究重點放這一個部
份，預期對格網技術會有重要的影響。下表整理出本計劃主要的貢獻。 
 過去技術 完成計畫後狀況 
技術面  通訊最佳化的研究集
中在平行與分散式記
憶體系統。 
 訊息廣播技術、資源
與工作排程技術，未
考 量 格 網 動 態 的 特
性，經濟模型、服務
與品質導向機制。 
 針對叢集式格網的動態特性，所提出來
的解決方案更符合未來可擴充式計算
的 系統 (Scalable Computing System) 架
構。可以容易導入不同的可擴充式系
統。 
 考量以服務導向架構與格網經濟模
型，提出滿足使用者需求、與系統最佳
化的資源與工作排程策略。就核心技術
何言，此架構具創新性與可性的。由於
Grid 中介軟體技術日趨成熟，未來整合
這些核心技術在既有的格網系統是指
日可待的。 
使用面  只 有 從 事 高 性 能 計
算、或相關應用領域
的科學家會考慮使用
格網系統。 
 讓格網系統平民化，滿足不同的使用者
的使用情境。 
 一般使用者 (未接觸過格網的使用者) 
- 能 夠 直 接 進 入 我 們 所 設 計 好 的
Portal，快速取得需要的服務。 
 18 
 Chia-Wei Chu, Ching-Hsien Hsu, Hsi-Ya Chang, Shuen-Tai Wang and Kuan-Ching Li, "Parallel File Transfer 
for Grid Economic" Proceedings of the 4th ICST International Conference on Scalable Information Systems 
(InfoScale 2009), Hong Kong, June, 2009, Lecture Notes of the Institute for Computer Science, Social 
Informatics and Telecommunications Engineering, (ISBN: 978-3-642-10484-8) Vol. 18, pp. 76-89, (DOI: 
10.1007/978-3-642-10485-5_6) (EI) 
 Yun-Chiu Ching, Ching-Hsien Hsu and Kuan-Ching Li, "On Improving Network Locality in BitTorrent-Like 
Systems" Proceedings of the 4th ICST International Conference on Scalable Information Systems (InfoScale 
2009), Hong Kong, June, 2009, Lecture Notes of the Institute for Computer Science, Social Informatics and 
Telecommunications Engineering, (ISBN: 978-3-642-10484-8) Vol. 18, pp. 58-75, (DOI: 
10.1007/978-3-642-10485-5_5) (EI) 
 Ching-Hsien Hsu, Yen-Jun Chen, Kuan-Ching Li, Hsi-Ya Chang and Shuen-Tai 
Wang, "Power Consumption Optimization of MPI Programs on Multi-Core Clusters" 
Proceedings of the 4th ICST International Conference on Scalable Information 
Systems (InfoScale 2009), Hong Kong, June, 2009, Lecture Notes of the Institute 
for Computer Science, Social Informatics and Telecommunications Engineering, 
(ISBN: 978-3-642-10484-8) Vol. 18, pp. 108-120, (DOI: 
10.1007/978-3-642-10485-5_8) (EI) 
 Shih-Chang Chen, Ching-Hsien Hsu, Tai-Lung Chen, Kun-Ming Yu, Hsi-Ya Chang 
and Chih-Hsun Chou, “A Compound Scheduling Strategy for Irregular Array 
Redistribution in Cluster Based Parallel System,” Proceedings of the 2nd 
Russia-Taiwan Symposium on Methods and Tools for Parallel Programming (MTPP 
2010), LNCS 6083, pp. 68-77, 2010. (EI) 
 Ching-Hsien Hsuand Tai-Lung Chen, “Adaptive Scheduling based on Quality of 
Services in Heterogeneous Environments”, IEEE Proceedings of the 
4th International Conference on Multimedia and Ubiquitous Engineering (MUE), 
Cebu, Philippines, Aug. 2010. 
 Ching-Hsien Hsu, Tai-Lung Chen and Kun-Ho Lee, "QoS Based Parallel File 
Transfer for Grid Economics" IEEE Proceedings of the 2009 International 
Conference on Multimedia Information Networking and Security (MINES 2009), pp. 
653-657, Wuhan, China, November, 2009. (EI) 
 
 
五、計畫成果自評 
 
本計畫之研究成果達到計畫預期之目標。研究團隊共計發表了四篇國際期刊與
六篇國際研討會論文，其中，[45] Performance and Economization Oriented Scheduling 
Techniques for Managing Applications with QoS Demands in Grids 是改善 QoS-Guided 的
排程。[46] Scheduling of Job Combination and Dispatching Strategy for Grid and Cloud 
System 是有關在異質性系統或格網環境下將工作排程最佳化，而[47] QoS Based 
Parallel File Transfer for Grid Economics 是改善 QoS-Guided 的排程。[48] Message 
Transmission Techniques for Low Traffic P2P Services 則是訊息傳送最佳化技術研究。 
本計畫有目前研究成果，感謝國科會給予機會、也感謝許多合作的學校、教授、
同學協助軟硬體的架設、測試、與協助機器的管理。另外，對於參與研究計畫執行
同學的認真，本人亦表達肯定與感謝。 
 
六、參考文獻 
 
[1] [1] Jesper Andersson, Morgan Ericsson, Welf Löwe, and Wolf Zimmermann, “Lookahead Scheduling for 
Reconfigurable GRID Systems,” 10th International Europar '04: Parallel Processing, vol. 3149, pp. 263-270, 
2004. 
[2] [2] Luiz Angelo, Barchet-Steffenel and Grégory Mounié, “Scheduling Heuristics for Efficient Broadcast 
Operations on Grid Environments,” Proceedings of Parallel and Distributed Processing Symposium, pp. 8, 
2006. 
[3] [3] Shah Asaduzzaman and Muthucumaru Maheswaran, “Heuristics for Scheduling Virtual Machines for 
 20 
[26] [26] Jens Koonp and Eduard Mehofer, “Distribution assignment placement: Effective optimization of 
redistribution costs,” IEEE TPDS, vol. 13, no. 6, June 2002. 
[27] [27] Chao Lin, “Efficient broadcast in a heterogeneous network of workstations using two sub-networks,” 
Proceedings of 7th International Symposium on Parallel Architectures, Algorithms and Networks, pp. 273 - 279, 
May 2004. 
[28] [28] Chao Lin, “Efficient contention-free broadcast in heterogeneous network of workstation with multiple 
send and receive speeds,” Proceedings Eighth IEEE International Symposium on Computers and 
Communication, 2003 (ISCC 2003), pp. 1277 – 1284, vol.2, 2003. 
[29] [29] Chao Lin, Yu-Chee Tseng and Jang-Ping Sheu, “Efficient Single-node Broadcast in Switched-based 
Network of Workstations with Network Partitioning,” Proceedings of Tenth International Conference on 
Computer Communications and Networks, pp. 68-74, 2001. 
[30] [30] Jong Sik Lee, “Data Distribution Management Modeling and Implementation on Computational Grid,” 
Proceedings of the 4th GCC, Beijing, China, 2005. 
[31] [31] Victor E. Mendia and Dilip Sarkar, “Optimal Broadcasting on the Star Graph,” IEEE Trans. Parallel and 
Distributed Systems, vol. 3, no. 4, pp. 389 - 396, July 1992. 
[32] [32] M.A. Moges and T.G. Robertazzi, “Grid scheduling divisible loads from multiple sources via linear 
programming,” 16th IASTED International Conference on Parallel and Distributed Computing and Systems 
(PDCS), pp. 423-428, 2004. 
[33] [33] J. Moore and M. Quinn, “Generating an Efficient Broadcast Sequence Using Reflected Gray 
Codes, ”IEEE Trans. Parallel and Distributed Systems, vol. 8, no. 11, pp. 1117-1122, Nov. 1997. 
[34] [34] Aske Plaat, Henri E. Bal, and Rutger F.H. Hofman, “Sensitivity of Parallel Applications to Large 
Differences in Bandwidth and Latency in Two-Layer Interconnects,” Proceedings of the 5th IEEE High 
Performance Computer Architecture HPCA'99, pp. 244-253, 1999. 
[35] [35] Sriram Ramanujam, Mitchell D. Theys, “Adaptive Scheduling based on Quality of Service in Distributed 
Environments,” International Conference on Parallel and Distributed Processing Techniques and Applications 
(PDPTA), pp. 671-677, 2005. 
[36] [36] Adele A. Rescigno, “Optimal Polling in Communication Networks,” IEEE Trans. on Parallel and 
Distributed System, vol. 8, no. 5, pp. 449 - 461 May 1997. 
[37] [37] Gerald Sabin, Rajkumar Kettimuthu, Arun Rajan and P Sadayappan, “Scheduling of Parallel Jobs in a 
Heterogeneous Multi-Site Environment,” in the Proc. of the 9th International Workshop on Job Scheduling 
Strategies for Parallel Processing, Lecture Notes In Computer Science; Vol. 2862, pp. 87-104 , June 2003. 
[38] [38] Fernando G. Tinetti and Andrés Barbieri, “An Efficient Implementation for Broadcasting Data in Parallel 
Applications over Ethernet Clusters,” Proceeding of 17th International Conference on Advanced Information 
Networking and Applications, pp. 593 - 596 March 2003. 
[39] [39] Fernando G. Tinetti and E. Luque, “Efficient Broadcasts and Simple Algorithms for Parallel Linear 
Algebra Computing in Clusters,” International Proceeding of Parallel and Distributed Processing Symposium, 
pp. 8, 2003. 
[40] [40] Weizhe Zhang, Hongli Zhang, Hui He, Mingzeng Hu, “Multisite Task Scheduling on Distributed 
Computing Grid,” Lecture Notes in Computer Science, vol. 3033, pp. 57–64, 2004. 
[41] [41] Ching-Hsien Hsu, Bing-Ru Tsai, Tai-Lung Chen and Shih-Chang Chen, “Scheduling for Atomic 
Broadcast Operation in Heterogeneous Networks with One Port Model,” Accepted, The Journal of 
Supercomputing (SCI, EI), Kluwer Academic Publisher, 2009. 
[42] [42] Ching-Hsien Hsu, Tai-Lung Chen and Jong-Hyuk Park,  “On improving resource utilization and system 
throughput of master slave jobs scheduling in heterogeneous systems,”  Journal of Supercomputing, Springer, 
Vol. 45, No. 1, pp. 129-150, July 2008. (SCI, EI). 
[43] [43] Ching-Hsien Hsu and Yung-Chneg Chang,  “Job Rescheduling Techniques for Optimizing Resource 
Utilization and Makespan in Grid”, Proceedings of the 5th Workshop on Grid Technology and Applications 
(WoGTA’08), Dec. 2008. 
[44] [44] Ching-Hsien Hsu, Justin Zhan, Wai-Chi Fang and Jianhua Ma, “Towards Improving QoS-Guided 
Scheduling in Grids,” IEEE Proceedings of the third ChinaGrid Annual Conference (ChinaGrid 2008), 
Dunhunag, Gansu, China. 
[45] [45] Ching-Hsien Hsu and Tai-Lung Chen, “Performance and Economization Oriented Scheduling Techniques 
for Managing Applications with QoS Demands in Grids”, International Journal of Ad-Hoc and Ubiquitous 
Computing, Vol. 5, No. 4, pp. 219-226, 2010. 
[46] [46]  Tai-Lung Chen, Ching-Hsien Hsu and Shih-Chang Chen, “Scheduling of Job Combination and 
 22 
出席國際學術會議心得報告 
 
計 畫 名 稱 應用 P2P 與 Web 技術發展以 SOA 為基礎的格網中介軟體與經濟模型 
計 畫 編 號 NSC 97-2628-E-216-006-MY3 
報 告 人 姓 名 許慶賢 
服 務 機 構 
及 職 稱 
中華大學資訊工程學系教授 
會 議 名 稱 
The 2nd Russia-Taiwan Symposium on Methods and Tools of Parallel 
Programming Multicomputers (MTPP 2010) 
會議 /訪問時間地點 海參威, 俄羅斯 / 2010.05.16-19 
發 表 論 文 題 目 
A Compound Scheduling Strategy for Irregular Array 
Redistribution in Cluster Based Parallel System 
 
參加會議經過 
 
會議時間 行程敘述 
2010/05/16 (上午) 
10:00 會場報到 
11:00  參訪研究中心 (半日) 
 
(下午) 
6:00  committee meeting 
(晚上) 
7:00 參加歡迎茶會 
2010/05/17 (上午) 
9:00  開幕致詞 
9:10  聽取 Parallel Algorithm 相關論文發表 
11:00 聽取 Models and Tools 相關論文發表 
 
(下午) 
2:00 聽取 Parallel Programming 相關論文發表 
 
 24 
 
 
 26 
The rest of this paper is organized as follows: Section 2 gives a survey of existing works related to array 
redistribution.  Section 3 gives notations, terminology and examples to explain each parts of scheduling 
heuristics.  The proposed techniques are described in section 4.  Section 5 presents the results of the 
comparative evaluation, while section 6 concludes the paper. 
2   Related Work 
Array redistribution techniques have been developed for regular array redistribution and GEN_BLOCK 
redistribution in many papers.  Both kinds of redistribution issues require at least two sorts of techniques.  
One is communication sets identification which decomposes array for nodes; the other one is communication 
scheduling method which derives schedules to shorten the overall transmission cost for redistributions. 
ScaLAPACK [9] was proposed to identify  communication sets for regular array redistribution.  Guo et al. 
[2] proposed a symbolic analysis method to help generate messages for GEN_BLOCK redistribution.  Hsu et 
al. [3] proposed the Generalized Basic-Cycle Calculation method to shorten the communication for 
generalized cases.  The research on prototype framework for distributed memory platforms is proposed by 
Sundarsan et al. [11] who developed a method to distribute multidimensional block-cyclic arrays on 
processor grids.  Karwande et al. [8] presented CC-MPI with the compiled communication technique to 
optimize collective communication routines.  Huang et al. [6] proposed a flexible processor mapping 
technique to reduce the number of data element exchanging among processors and enhance the data locality.  
To reduce indexing cost, a processor replacement scheme was proposed [4].  With local matrix and 
compressed CRS vectors transposition schemes the communication cost can be reduced significantly.  
Combining the advantages of relocation scheduling algorithm and divide-and-conquer scheduling algorithm, 
Wang et al. [12] proposed a method with two phases for GEN_BLOCK redistribution.  The first phase acts 
like relocation algorithm, but the contentions avoidance mechanism of second phase will not be proceeded 
immediately while contentions happened.  To minimize the total communication time, Cohen et al. [1] 
supposed that at most k communication can be performed at the same time and proposed two algorithms 
with low complexity and fast heuristics.  A study [7] focusing on the cases of local redistributions and 
inter-cluster redistribution was given by Jeannot and Wagner.  It compared existing scheduling methods and 
described the difference among them.  Rauber and Runger [10] presented a data-re-distribution library to 
deal with composed data structures which are distributed to one or more processor groups for  executing 
multiprocessor task on distributed memory machines or cluster platforms.  Hsu et al. [5] proposed a 
two-phase degree-reduction scheduling heuristic to minimize the overall communication cost.  The 
proposed method derives each time step of a complete schedule by performing degree reduction technique 
while the number of messages of each node representing the degree of each vertex in algorithm level. 
3   Preliminary 
Following are notations, terminology and examples to explain each parts of scheduling heuristics for 
GEN_BLOCK redistribution.  To improve data locality, multi-phase scientific problems require appropriate 
data distribution schemes for specific phases.  For example, to distribute array for two different phases on 
six nodes, which are indexed from 0 to 5, two strings, {13, 20, 17, 17, 12, 21} and {16, 18, 13, 16, 29, 8}, 
 28 
 
A result of scheduling heuristics 
No. of step No. of message Cost of step 
Step 1 m3(17), m5(13), m7(13) , m10(13) 17 
Step 2 m1(13), m6(3), m9(12), m11(8) 13 
Step 3 m2(3), m4(1), m8(4) 4 
Total cost 34 
  
Fig. 2. A result of scheduling messages with low communication cost and minimal steps 
The result in Fig. 2 schedules messages in three steps, which is the number of minimal steps or CTmax.  The 
total cost is small which representing low communication cost due to messages with larger cost and 
messages with smaller cost are in separate steps.  However, the schedule can still be better by providing a 
cost normalization method and a new scheduling technique to avoid synchronization delay among nodes 
during message transmissions in next section. 
4   The Proposed Method 
In this paper, a two-step communication cost modification (T2CM) and a synchronization delay-aware 
scheduling heuristic (SDSH) are proposed to normalize the communication cost of messages and reduce 
transmission delay in algorithm level.  The first step of T2CM is a local reduction operation, which deal 
with the message happened in local memory.  In other words, candidates are transmissions whose source 
node and destination node are the same node.  For example, m1, m3, m5, m7, m9 and m11 are such kind of 
transmissions which happened inside nodes.  The second step is a inter amplification method, which is 
responsible for transmissions happened across clusters.  Assumed there are two clusters, and node 0~2 are 
in cluster 1, other nodes are in cluster 2.  Then m6 is such message which is transmitted from cluster 1 to 
cluster 2.  Both operations are responsible for different kind of transmissions due to the heterogeneity of 
network bandwidth.  The local reduction operation reduces simulated cost of messages to 1/8 which is 
evaluated from PC clusters that connected with 100Mbps layer-2 switch.  On same argument, inter 
amplification operation increases cost of messages five times.  The cost then becomes more practical for 
real machines when scheduling heuristics try to give a perfect schedule with low communication cost.  For 
previous research, the difference does not exist in algorithm level of scheduling heuristics in and could result 
in erroneous judgments and high communication cost. 
Fig. 3 gives the results of local reduction and inter amplification operations modifying data size for 
messages m1~11.  The given schedule in Fig. 2 becomes the results in Fig. 4.  Difference of Fig. 2 and Fig. 
4 shows the schedule could be improved and explains the explain the erroneous judgments.  First, the 
dominators in step 1 and 2 are changed to others whose estimated cost is larger in Fig. 4.  For example, the 
m3 and m1 are replaced by m10 and m6 for both steps, respectively.  Second, the cost of step 1 and step 2 are 
changed due to new dominators are chosen in both steps.  Furthermore, the synchronization delay is small 
in algorithm level but results in more node idle time in practical.  For instance, the cost of m3, m5, m7 and 
m10 are 17, 13, 13 and 13 are close to each other in step 1 in Fig. 2.  But it is quite different in practical in 
 30 
m5 and m7.  The message m5 owns node 2 as source node and so does m6.  Both messages cannot be 
scheduled in the same step.  Similarly, m6 and m7 cannot be scheduled together due to destination node.  
On same argument, it is impossible to move m10 to step 2 due to m9 and m11.  If m5, m7, m9 and m11 can be 
placed in other step, it would be possible to place m6 and m10 together to minimize the communication cost 
of the results.  SDSH successfully places them in step 3 and then schedules m6 and m10 in step 1 to shorten 
the cost of other steps.  This operation also successfully avoids node contentions that happened in Fig. 4. 
 
A result of the proposed method 
No. of  
step 
No. of  
message 
Cost of  
step 
Step 1 m2(3), m6(15), m10(13) 15 
Step 2 m4(1), m8(4) 4 
Step 3 m1(1.625), m3(2.125), m5(1.625), 
m7(1.625), m9(1.5), m11(1) 
2.125 
Total cost 21.125  
Fig. 5. A result of proposed method with low synchronization delay and contention free 
5   Performance Evaluation 
To evaluate the proposed method, it is compared with a scheduling method, TPDR [5].  The simulator 
generates schemes (strings) for 8, 16, 32, 64 and 128 nodes, and there are three nodes in a cluster.  To 
constrain the data size of each node, the lower bound and upper bound of each value in the strings are 1 and 
the value that array size divided by the number of nodes, where the array size is 10,000.  If the array is 
distributed on eight nodes, the lower bound and the upper bound of data size are 1 and 1250 for each node, 
respectively. 
Fig. 6 shows the results of comparisons between SDSH and TPDR.  For each set of node, the number on 
the right side represents the cases that SDSH performs better, TPDR performs better or tie cases.  In the 
simulation results for 8 nodes, the proposed method wins 813 cases which is less than 90% because it is easy 
for both methods to find the same results when performing GEN_BLOCK redistribution on few number of 
nodes.  Therefore, the number of tie cases is over than 10%, and is much more than the results of other sets.  
When performing GEN_BLOCK redistribution with more nodes, SDSH outperforms TPDR, and TPDR loses 
over 92% cases in the rest of the comparisons.  Note that the proposed method always find the best results 
in over 93% cases including the tie cases in all comparisons.  It also shows the contribution of SDSH for 
shortening transmission cost and avoiding synchronization delay. 
 32 
provides local reduction and inter amplification operations to enhance the importance of messages.  The 
SDHC deal with messages separately to avoid synchronization delay and reduce the cost.  The performance 
evaluation shows that the proposed methods outperforms its competitor in 92% cases and improves about 
15% on overall communication cost. 
References 
[1] Cohen, J., Jeannot, E., Padoy, N., Wagner, F.: Messages Scheduling for Parallel Data Redistribution 
between Clusters. IEEE Transactions on Parallel and Distributed Systems 17(10), 1163-1175 (2006) 
[2] Guo, M., Pan, Y., Liu, Z.: Symbolic Communication Set Generation for Irregular Parallel Applications. 
The Journal of Supercomputing 25(3), 199-214 (2003) 
[3] Hsu, C.-H., Bai, S.-W., Chung, Y.-C., Yang, C.-S.: A Generalized Basic-Cycle Calculation Method for Efficient 
Array Redistribution. IEEE Transactions on Parallel and Distributed Systems 11(12), 1201-1216 (2000) 
[4] Hsu, C.-H., Chen, M.-H., Yang, C.-T., Li, K.-C.: Optimizing Communications of Dynamic Data 
Redistribution on Symmetrical Matrices in Parallelizing Compilers. IEEE Transactions on Parallel and 
Distributed Systems 17(11), (2006) 
[5] Hsu, C.-H., Chen, S.-C., Lan, C.-Y.: Scheduling Contention-Free Irregular Redistribution in 
Parallelizing Compilers. The Journal of Supercomputing 40(3), 229-247 (2007) 
[6] Huang, J.-W., Chu, C.-P.: A flexible processor mapping technique toward data localization for 
block-cyclic data redistribution. The Journal of Supercomputing 45(2), 151-172 (2008) 
[7] Jeannot, E., Wagner, F.: Scheduling Messages For Data Redistribution: An Experimental Study. The International Journal of 
High Performance Computing Applications 20(4), 443-454 (2006) 
[8] Karwande, A., Yuan, X., Lowenthal, D. K.: An MPI prototype for compiled communication on ethernet 
switched clusters. Journal of Parallel and Distributed Computing 65(10), 1123-1133 (2005) 
[9] Prylli, L., Touranchean, B.: Fast runtime block cyclic data redistribution on multiprocessors. Journal of 
Parallel and Distributed Computing, 45(1), 63-72 (1997) 
[10] Rauber, T., Rünger G.: A Data Re-Distribution Library for Multi-Processor Task Programming. International Journal of 
Foundations of Computer Science 17(2), 251-270 (2006) 
[11] Sudarsan, R., Ribbens, C. J.: Efficient Multidimensional Data Redistribution for Resizable Parallel 
Computations. In: Fifth International Symposium on Parallel and Distributed Processing and 
Applications, 182-194 (2007) 
[12] Wang, H., Guo, M., Wei, D.: Message Scheduling for Irregular Data Redistribution in Parallelizing 
Compilers. IEICE Transactions on Information and Sysmtes E89-D(2), 418-424 (2006) 
 
 34 
2009/08/30 (上午) 
9:00 聆聽 Keynote Speech 
Cache-Aware Scheduling and Analysis for Multicores by Wang Yi 
10:30 聽取 P2P 相關論文發表 
 
(下午) 
1:00 聆聽 Keynote Speech 
     Network Analysis and Visualization for Understanding Social  
Computing by Ben Shneiderman 
2:15 參加Panel discussion  
3:45 主持 Session 
 
(晚上) 
7:30 參加晚宴 
2009/08/31 (上午) 
8:00 聆聽 Keynote Speech 
White Space Networking - Is it Wi-Fi on Steroids? by Prof. Victor 
Bahl 
10:30 聽取 Network Management 相關論文發表 
 
(下午) 
1:30 聆聽 Keynote Speech 
     Computational Science and Engineering in Emerging  
Cyber-Ecosystems by Prof. Manish Parashar 
2:00 主持 Session 
 
 
這一次在 Vancouver, Canada 所舉行的國際學術研討會議共計三天。這三天每天早上下午
都各有穿插專題演講，共邀請了七個不同領域的專家學者給予專題演講，第一天邀請了 Dr. 
Stephen S. Yau (Arizona State University, USA)與 David Chaum 分別針對 Privacy, Security, 
Risk and Trust in Service-Oriented Environments 和 Elections with Practical Privacy and 
Transparent Integrity 給予專題演講揭開研討會的序幕，接下來兩天也分別有 Wang Yi (North 
Eastern University, China) 、Ben Shneiderman、Dr. Fei-Yue Wang、Prof. Victor Bahl 和 Prof. 
Manish Parashar (Rutgers University)五位專家學者針對 Cache-Aware Scheduling and Analysis 
for Multicores 、Network Analysis and Visualization for Understanding Social Computing、Social 
 36 
Data Distribution Methods for Communication Localization in Multi-Clusters with 
Heterogeneous Network 
Shih-Chang Chen
1
, Ching-Hsien Hsu
2
 and Chun-Te Chiu
2
 
1
 Institute of Engineering and Science 
2
 Department of Computer Science and Information Engineering 
Chung Hua University, Hsinchu, Taiwan 300, R.O.C. 
chh@chu.edu.tw 
Abstract 
Grid computing integrates scattered clusters, servers, storages and networks in different geographic 
locations to form a virtual super-computer.  Along with the development of grid computing, dealing with 
the data distribution requires a method which is faster and more effective for parallel applications in order 
to reduce data exchange between clusters.  In this paper, we present two methods to reduce inter-cluster 
communication cost based on the consideration to different kinds of communication cost and a simple logic 
mapping technology.  Our theoretical analyses and simulation results show the proposed methods are 
better than the methods without reordering processor and considering the communication cost.  The 
performance evaluation shows that the proposed methods not only reduce communication cost successfully 
but also achieve a great improvement. 
1. Introduction 
Computing grid system [5] integrates geographically distributed computing resources to establish a 
virtual and high expandable parallel environment.  Cluster grid is a typical paradigm which is connected by 
software of computational grids through the Internet.  In cluster grid, computers might exchange data 
through network to other computers to run job completion.  This consequently incurs two kinds of 
communication between grid nodes.  If the two grid nodes are geographically belong to different clusters, 
the messaging should be accomplished through the Internet.  We refer this kind of data transmission as 
external communication.  If the two grid nodes are geographically in the same space domain, the 
communications take place within a cluster; we refer this kind of data transmission as interior 
communication.  Intuitionally, the external communication is usually with higher communication latency 
than that of the interior communication.  Therefore, to efficiently execute parallel programs on cluster grid, 
it is extremely critical to avoid large amount of external communications. 
Array redistribution is usually required for efficiently redistributing method to execute a data-parallel 
program on distributed memory multi-computers.  Some efficient communication scheduling methods for 
the Block-Cyclic redistribution had been proposed which can help reduce the data transmission cost.  The 
previous work [9, 10] presents a generalized processor reordering technique for minimizing external 
 38 
[9] presented an efficient method for optimizing localities of data distribution when executing data parallel 
applications.  The data to logical grid nodes mapping technique is employed to enhance the performance of 
parallel programs on cluster grid. 
For a global grid of clusters, these techniques become inapplicable due to various factors of Internet 
hierarchical and its communication latency.  More and more multi-clusters under heterogeneous network 
environment in which the performance issue was of primary importance on.  Bahman Javadi et al. [12, 13] 
proposed an analytical model for studying the capabilities and potential performance of interconnection 
networks for multi-cluster systems.  In this following discussion, our emphasis is on minimizing the 
communication costs for data parallel programs on cluster grid and on enhancing data distribution of 
communication localities with heterogeneous network. 
3. Research Model 
3.1 Identical Cluster Grid 
To explicitly define the problem, upon the number of clusters (C), number of computing nodes in each 
cluster (ni), 1≦i≦C, the number of sub-blocks (K) and <G(C):{n1, n2, n3, …, nc}> presents the cluster grid 
model with ni computing nodes in each cluster.  The definition of symbols is shown in Table 1. 
 
Table 1 The definition of symbols. 
C The number of 
clusters. 
K The degree of 
refinement 
ni The number of 
computing nodes in 
each cluster. 
G(C):{n1, n2, n3, …, 
nc} 
The cluster grid model 
 
We consider two models of cluster grid when performing data reallocation.  Figure 1 shows an example 
of localization technique for explanation. The degree of data refinement is set to three (K = 3).  This 
example also assumes an identical cluster grid that consists of three clusters and each cluster provides three 
nodes to join the computation.  In algorithm phase, in order to accomplish the fine-grained data distribution, 
processors partition its own block into K sub-blocks and distribute them to corresponding destination 
processors in ascending order of processors‟ id that specified in most data parallel programming languages.  
For example, processor P0 divides its data block A into a1, a2, and a3; it then distributes these three 
sub-blocks to processors P0, P1 and P2, respectively.  Because processors P0, P1 and P2 belong to the same 
cluster with P0; therefore, these three communications are interior.  However, the same situation on 
processor P1 generates three external communications.  Because processor P1 divides its local data block B 
into b1, b2, and b3. It then distributes these three sub-blocks to processors P3, P4 and P5, respectively.  As 
processor P1 belongs to Cluster-1 and processors P3, P4 and P5 belong to Cluster-2, there are three external 
communications.  Figure 1(a) summarizes all messaging patterns of this example into the communication 
 40 
 
(a) 
 
 
(b) 
 
Figure 2. Communication tables of data reallocation on the identical cluster grid. (C = 4, n = 3, K = 4) (a) Without data mapping. 
(b) With data mapping. 
If we change the distribution of block B to processors reside in cluster-2 (P3, P4 or P5) or cluster-3 (P6, P7 
or P8) in the source distribution, we find that the communications could be centralized in the local cluster for 
some parts of sub-blocks.  Because cluster-2 and cluster-3 will be allocated the same number of sub-blocks 
in the target distribution, therefore processors belong to these two clusters have the same priority for node 
replacement.  In this way, P3 is first assigned to replace P1.  For block C, most sub-blocks will be 
reallocated to processors in cluster-4, therefore the first available node P9 is assigned to replace P2.  Similar 
determination is made to block D and results P1 replace P3.  For block E, cluster-2 and cluster-3 have the 
same amount of sub-blocks.  Processors belong to these two clusters are candidates for node replacement.  
However, according to the load balance policy among clusters, cluster-2 remains two available processors 
for the node replacement while cluster-3 has three; our algorithm will select P6 to replace P4.  Figure 2(b) 
gives the communication tables when applying data to logical grid nodes mapping technique. We obtain |I| = 
28 and |E| = 20. 
3.2 Non-identical Cluster Grid 
Let‟s consider a more complex example in non-identical cluster grid, the number of nodes in each 
cluster is different.  It needs to add global information of cluster grid into algorithm for estimating the best 
target cluster as candidate for node replacement.  Figure 3 shows a non-identical cluster grid composed by 
four clusters.  The number of processors provided by these clusters is 2, 3, 4 and 5, respectively.  We also 
set the degree of refinement as K = 5.  Figure 3(a) presents the table of original communication patterns that 
consists of 19 interior communications and 51 external communications.  Applying our node replacement 
 42 
distribution scheme in Figure 3(a) and 3(b), respectively.  But the proposed processor mapping methods 
provide new sequences of logical grid node which are <P4, P5, P11, P2, P9, P0, P6, P10, P1, P7, P12, P3, P8, 
P13> and < P3, P5, P9, P2, P10, P1, P6, P11, P0, P7 P12, P4, P8, P13 > in next section.  Consequently, the 
necessary costs of both sequences are 740 units.  The result reflects the effectiveness of this sequence which 
has the less communications cost.  In next section, we will to explain the research model and calculation of 
communication cost. 
 
(a) 
 
 
(b) 
 
Figure 4. Communication model of Multi-Clusters with Heterogeneous Network. (a) Example of four clusters 
with various inter-cluster communication costs. (b) The communication matrix table. 
3.4 Communication Model of Data Distribution in Multi-Clusters 
To set the communication cost of inter-cluster as V(i,j).  The communication cost of distribute data block 
from C1 to C3 is denoted V(1,3).  Assume there is block A (β=A) from node P of Ci, total cost formula 
denoted W(β)i..  W(β)i = (β1*V(i,1) + β2*V(i,2)+…+ βj*V(i,j)).  (1≦i, j≦C).  β1, β2, …, βj-1 andβj represent 
number of sub-blocks that Pi has to send from C1 to C1, C2, …, Cj-1, Cj. Figure 5 shows the communication 
cost of data distribution from each node according to distribution scheme in Figure 4(b).  There is the data 
block A on logic nodes P0 within a grid model C = 4, K = 5, <G(4):{2, 3, 4, 5}>.  Assume the sub-blocks a1, 
a2 of block A on P0 needs to be redistributed from C1 to C1, the a3, a4, a5 needs to be  redistributed from C1 
to C2, no data is redistributed from C1 to C3, C4.  The communication cost of redistributing block A from P0 
and P2 are W(A)1 = (2*0 + 3*20 + 0*30 + 0*30) = 60 and W(A)2 = (2*20 + 3*0 + 0*50 + 0*20) = 40, 
respectively.  Accordingly, W(A)3 = 125, W(A)4 = 105. 
 44 
next global minimum cost. 
To a select processor id for redistributing a data blocks according to the communication cost in Figure 5, 
GR will first select P13 for N P¸10 for K, P13 for N, P8 for M,…, P0 for F and P5 for B.  A new sequence of 
logical grid node is provided which is <P4, P5, P11, P2, P9, P0, P6, P10, P1, P7, P12, P3, P8, P13> and the 
necessary communication cost is 740 units, accordingly. 
 
According to the method described above, the code of algorithm is shown as follows: 
 
For P=0 to n-1 
Determine how many cost matrix t in 
every cluster  
EndFor 
Order by t 
While (Replacement s not complete) 
If (cluster have processor) 
select target cluster processor id P 
that has minimum cost from t  
    EndIf 
EndWhile 
 
Figure 6. Processor Mapping using Global Reordering Algorithm. 
4.2 Divide and Conquer Reordering Algorithm 
The proposed method is introduced in this section called Processor Mapping using Divide and Conquer 
Reordering Algorithm (DCR).  The GR method employs the greedy algorithm to choose the minimum cost 
for processor mapping.  The DCR method uses the Greedy algorithm to choose processor id for data block 
with minimum cost for each cluster first.  The number of selected data block is equal to the number of 
processors provide in each cluster.  Due to select several data blocks for each cluster with minimum cost 
and a processor id, cross match is not under consideration.  Certainly, the results of processor mapping will 
not be perfect.  Namely, conflict selections will possibly happen.  To resolve the conflict situations, the 
conflict part can be regarded as a sub-grid model of the original grid model.  Data blocks without conflict 
situation and selected processor id are excluded.  DCR employs GR method to select processor id for rest of 
data blocks for a complete result. 
To select data blocks with minimum cost for each cluster according to the communication cost in Figure 5, 
DCR will select A and L for C1, A, D and L for C2, B, G, J and M for C3, and C, E, H, K and N for C4.  After 
select processor id for B, C, D, E, G, H, J, K, M and N, DCR employs GR to select processor id for A, F, I 
and L again.  Then, a new sequence of logical grid node is provided which is < P3, P5, P9, P2, P10, P1, P6, 
 46 
Figure 9 shows the results of the grid model with C = 16, K = 64 and <G(16):{5, 6, 7, 8, 9, 10, 11, 12, 13, 
14, 15, 16, 17, 18, 19, 20}> while comparing four different methods.  Obviously, GR and DCR have less 
communication cost comparing with the other two models.  GR and DCR can reduce about 26% to 29% 
cost while comparing with the traditional one which is without processor reordering.  Both of them also 
reduce 11% communications cost while comparing with the Original one.  Above simulation results show 
the proposed reordering technologies not only outperform previous processor reordering method but also 
successfully reduce communication cost on the heterogeneous network and improve the communication cost 
6. Conclusions 
In this paper, we have presented a generalized processor reordering method for communication localization 
with heterogeneous network.  The methods of processor mapping technique are employed to enhance the 
performance of parallel programs on a cluster grid.  Contribution of the proposed technique is to reduce 
inter-cluster communication overheads and to speed up the execution of data parallel programs in the 
underlying distributed cluster grid.  The theoretical analysis and results show improvement of 
communication costs and scalable of the proposed techniques on multi-clusters with heterogeneous network 
environment. 
 
Figure 8. Communication costs comparison with C = 8, K = 16, <G(8):{4, 4, 4, 6, 6, 6, 8, 8}>. 
 
Figure 9. Communication costs comparison with C = 16, K = 64 and <G(16):{5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 
15, 16, 17, 18, 19, 20}> 
 48 
出席國際學術會議心得報告 
 
計 畫 名 稱 應用P2P與Web技術發展以SOA為基礎的格網中介軟體與經濟模型 
計 畫 編 號 NSC 97-2628-E-216-006-MY3 
報 告 人 姓 名 許慶賢 
服 務 機 構 
及 職 稱 
中華大學資訊工程學系教授 
會 議 名 稱 
The 4th International ICST Conference on Scalable Information  
Systems (INFOSCALE 2009) 
會議 /訪問時間地點 香港 / 2009.06.09-11 
發 表 論 文 題 目 
Power Consumption Optimization of MPI Programs on Multi-Core  
Clusters 
 
參加會議經過 
 
會議時間 行程敘述 
2009/06/10 (上午) 
8:30 會場報到 
9:00 聆聽Keynote Speech 
Reevaluating Amdahl's Law in the Multicore Era 
Xian-He Sun, Illinois Institute of Technology, Chicago, USA 
10:30 發表論文、聽取其它論文發表 
(下午) 
1:30 聆聽Keynote Speech 
     Metropolitan VANET: Services on the Road 
     Minglu Li, Shanghai Jiao Tong University, China 
2:00 聽取 Resource Allocation and Application相關論文發表 
3:45 主持 Session 
(晚上) 
6:30 參加晚宴 
 50 
Power Consumption Optimization of MPI Programs on 
Multi-Core Clusters 
 
Ching-Hsien Hsu and Yen-Jun Chen 
 
 
 
Abstract 
 
 
While the energy crisis and the environmental pollution become important global issues, the power 
consumption researching brings to computer sciences world. In this generation, high speed CPU structures 
include multi-core CPU have been provided to bring more computational cycles yet efficiently managing 
power the system needs. Cluster of SMPs and Multi-core CPUs are designed to bring more computational 
cycles in a sole computing platform, unavoidable extra energy consumption in loading jobs is incurred. 
Data exchange among nodes is essential and needed during the execution of parallel applications in 
cluster environments. Popular networking technologies used are Fast Ethernet or Gigabit Ethernet, which are 
cheaper and much slower when compared to Infiniband or 10G Ethernet. Two questions on data exchange 
among nodes arise in multi-core CPU cluster environments. The former one is, if data are sent between two 
nodes, the network latency takes longer than system bus inside of a multi-core CPU, and thus, 
wait-for-sending data are blocked in cache. And the latter is, if a core keeps in waiting state, the unpredicted 
waiting time brings to cores higher load. These two situations consume extra power and no additional 
contribution for increasing overall speed. In this paper, we present a novel approach to tackle the congestion 
problem and taking into consideration energy in general network environments, by combining hardware 
power saving function, maintaining the transmission unchanged while saving more energy than any general 
and previous cases. 
 
 52 
 
Figure 2: AMD Quad-Core CPU system structure [12] 
 
AMD quad-core CPU, as shown in Figure 2, has individual L2 cache in each core and share L3 cache, (a 
special design), and then integrated to DDR2 memory controller into CPU, helping to increase memory 
access speed. Each core has individual channel to access system bus, and L3 cache and peripheral chips from 
crossbar switch. AMD provides “PowerNow!” [4] technology to adjust each core‟s working frequency / 
voltage.  
A cluster platform is built up by interconnecting a number of single-core CPU, and a message passing 
library, such as MPI is needed for data exchange among computing nodes in this distribution computing 
environment. In addition, high speed network as Infiniband is needed to interconnect the computing nodes. 
As multi-core CPUs are introduced and built in cluster environments, the architecture of this newly proposed 
cluster is as presented in Figure 3. The main advantages of data exchanges between cores inside of a CPU is 
much faster than passing by a network and South / North bridge chip. 
 
 54 
send data to a core that is inside of a different host will be needed to consume extra energy when waiting for 
data. 
“SpeedStep” and “PowerNow!” technologies are good solutions to reduce power consumption, since they 
adjust CPU‟s frequency and voltage dynamically to save energy. The power consumption can be calculated 
by the function: 
 
P=IV=V
2
f =J/s. (1) 
 
where P is Watt, V is voltage, I is current, f is working frequency of CPU, J is joule and s is time in 
seconds. It means that lower voltage in the same current condition saves more energy. How and when to 
reduce voltage and frequency become an important issue, since one of main targets of clustering computing 
computers is to increase the performance, while slowing down CPU‟s frequency is conflict with performance. 
Considering data latency of network, and CPU load in current CPU technologies, we would like to create a 
low energy cost cluster platform based on general network architecture, that keeps almost the same data 
transmission time though lower in energy consumption when CPU in full speed. 
To address the above questions, we use OpenMPI and multi-core CPU to build up a Linux based a low 
energy cost cluster, and implement three solutions on this environment. 
 CPU power consumption reduction 
Drive CPU power saving technology to reduce working frequency when low working loading, the 
method reduces unavailable power consumption. 
 CPU internal bus congestion reduction 
Add waiting time between each data frame before send out, the method slows down data 
transmission speed and reduces core working loading. 
 Loading-Aware Dispatching (LAD) Algorithm 
 56 
Base on a simulation board, researchers have designed routing path algorithm that tries to find a shortest 
path to transmit data in Networks-on-Chip [15], in order to reduce data transmission time between CPUs, as 
also to have opportunities to realistically port and implement it to a cluster environment. 
Others, researches have applied Genetic Algorithms to make a dynamically and continuous improvement 
on power saving methodology [9]. Through a software based methodology, routing paths are modified, link 
speed and working voltage are monitored and modified at the same time to reduce power consumption of 
whole simulation board, while the voltage detection information required hardware support. 
Consider higher power density and thermal hotspots happened in NoC, the paper [18] provided a 
compiler-based approach to balances the processor workload, these researchers partitions a NoC system to 
several area and dispathes jobs to them by node remapping, the strategy reduces the chances of thermal 
concentration at runtime situation, and brings benefit about a bit of performance increasing. The paper [20] 
and [24] studied the same point about thermal control. 
Modern Operating Systems as Linux and Windows provides hardware power saving function as 
introduced in [1] and [2], where they can drive “SpeedStep” [3] and “PowerNow!” [4] utilizing special 
driver to control CPU voltage and frequency. Of course hardware support is necessary, since depending on 
the CPU loading, CPU is automatically selected with lower frequency and voltage automatically. Besides, 
someone add management system into OS kernel to control energy consumption directly [22]. 
The peripheral devices of computer, as disk subsystem is a high energy consumption hardware, the paper 
[17] studied how to implement disk energy optimization in compiler, these researcheers considered disk start 
time, idle time, spindle speed, the disk accessing frequency of program and CPU / core number of each host, 
made up a benchmark system and real test environment to verify physical result. 
Some groups study the power saving strategy implementation in data center, as database or search engine 
server. Huge energy is consumed by this kind of application when they have no work. The nearer research 
[19] provides a hardware based solution to detect idle time power waste and designs a novel power supplier 
 58 
As description of Figure 1, Intel‟s CPU architecture shares L2 cache to cores using individual hub, all 
packets between core and cache needs to pass through by it. The architecture has 2 advantages and 2 
problems: 
Advantages 
 Flexible cache allocation 
Every core was allowed to use whole L2 cache from cache hub, the hub provides single memory access 
channel for each core, and hub assigns cache memory space to requested core. The method simplifies 
internal cache access structure. 
 Decrease cache missing rate 
When each core has massive cache request all of a sudden, flexible cache memory allocation provides 
larger space to save data frame, and also decreases page swapping from main memory at the same time. 
Problems 
 Cache Hub Congestion 
If huge amount of data request or sending commands happen suddenly, individual cache hub blocks data 
frames in cache memory or stops commands in queue. All cores and hub keep in busy state and thus 
consume extra energy. 
 Network Bandwidth Condition 
Lower network bandwidth makes previous situation more seriously in many nodes' cluster, since network 
speed cannot be as fast as internal CPU bus, if cross-node data frames appear, the delivering time is longer 
than intra-node data switch. 
Compared with Intel, while data frame flood sends to CPU, AMD structure has no enough cache to save 
them, yet individual bus / memory access channel of each core provides isolated bandwidth, L2 cache built 
 60 
less than 0.5s. These experiment results means the core work loading brings up by massive data frame, not 
by CPU bound process. This method reduces core work loading and helps below method to operate. 
Although the challenge presented in section 3.1 exists, as for power saving issue, we use AMD system 
and “PowerNow!” to slow down lowering loading core frequency. The given CPU supports two steps 
frequency, and therefore they work in different voltage and current. Thus we focus on frequency adjustment, 
and calculating power consumption of each core as below: 
P = Vmax × Imax × T (3) 
where Vmax and Imax are found from AMD CPU technology specification [6], and T is program execution 
time. Since “Time” joins the function, the unit of P is Joule. 
There is a CPU frequency controlling software: CPUFreq. It provides simple commands to change CPU 
work state and 4 default operation modes: 
 Performance mode: CPU works in highest frequency always 
 Powersave mode: CPU works in lowest frequency always 
 OnDemand mode: CPU frequency is adjusted following CPU work loading 
 UserSpace mode: User is permitted to change CPU frequency manually follow CPU specification 
We have used UserSpace mode and got the best CPU work loading threshold range to change CPU 
frequency: 75%~80%, if CPU work loading lower than this, we reduce frequency; if higher, we increase 
frequency. But actually, the default threshold of OnDemand mode is 80%, so we use OnDemand mode to 
control CPU frequency when our data dispatching method is executed. 
Following the previous results, working with OnDemand mode of CPUFreq, we provide a 
Loading-Aware Dispatching method (LAD). Based on the AMD “PowerNow!” hardware structure, and 
keeping the same load on all cores is necessary for efficient energy consumption, thus sending data from 
central node to lowest loading node makes sense. If the load can be reduced on a core, then reducing CPU 
frequency is permitted for saving energy. 
 62 
10.       //sort TargetNode from low to high 
11.       CPULoadingSorting; 
12.       //send 1000 data frame 
13.       for(i=1; i<1000; i++) 
14.          SendData(TargetNode[i]);    
15.       if(whole data transmitted) 
16.          DataSendingFinish=true; 
17.    } 
18.    //send finish message to receiving nodes 
19.    for(i=1; i<NodeNumber; i++) 
20.       SendData(i); 
21. } 
22. if (other nodes) 
23. { 
24.    //receive data from node 0 
25.    ReceiveData(0); 
26.    usleep(); 
27. } 
5. Performance Evaluation and Analysis 
In this chapter, experimental environment and results of LAD algorithm is presented. The cluster platform 
includes two computing nodes and connected via Gigabit Ethernet, and each node is installed with Ubuntu 
Linux 8.10 / kernel 2.6.27-9, OpenMPI message passing library is selected for thread execution affinity 
function, the hardware specification is listed as next. 
 
Table 3: Host specification 
CPU 
AMD Phenom X4 9650 
Quad-Core 2.3GHz 
Layer 1 Cache 
64K Instruction Cache 
and 64K Data Cache Per Core 
Layer 2 Cache 512K Per Core 
Layer 3 Cache Share 2M for 4 Cores 
Main Memory DDR2-800 4GB 
 
 64 
third one is OnDemand Mode (OD, slows down frequency while CPU loading lower than 80%), and last one 
is LAD algorithm that works with OnDemand Mode. 
Besides, each block has four delay time configurations, the first one contains no delay between each data 
frame, the second delays 5µs, the third one delays 10µs, and last one delay 20µs. Still in figures that follows 
next, TD stands for Transmission Delay, Transmission Time as TT, and PC for Power Consumption. 
Rank number 
The “Rank Number” in each figures and tables mean the number of nodes / cores join data dispatching. 
For example, rank 2 means rank 0 dispatchs data to rank one, and rank 4 means rank 0 dispatchs data to rank 
one, 2, and 3. Since each host has four cores, the rank number 2~4 are internal node data transmission, and 
rank 5~8 are cross node data transmission. 
Although only four cores join work in rank number 2~4, other cores consume energy at the same time, 
and we still need to add the energy consumed. 
One byte frame 
Table 3 and Figure 5 show the TT for one byte frame, and Figure 6 the PC. Comparing PM, PS and OD 
mode, we find that TD increases the TT over 3 seconds in rank 2~4 in every frequency level, but increases 
less than one second in 5~8. Table 4 and Figure 6 displayed one byte frame PC. Clearly, the PS mode spends 
the longest time to transmit data, though consumes the lowest energy. OD mode has none remarkable 
performance in power saving in rank 7~8, but it uses average 100J less than PM mode in rank 2~6, and 
keeps TT increasing less than 0.4s in cross-node situation. LAD algorithm displays advantage in no delay 
situation, less than 1s TT increasing yet consumes almost the same energy in rank 7~8. In other situations, 
LAD spends maximum 4s longer than OD mode, and saves 400J. 
 
Table 4: Detail results of time effect of TD on TT (Frame = 1 Byte) 
 66 
 
Table 5: Detail results of power effect of TD on PC (Frame = one Byte) 
       Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 25.400 52.864 79.535 994.105 1690.074 2392.710 2957.550 
5 147.322 182.882 204.155 1081.577 1790.088 2518.918 3077.249 
10 363.860 360.526 281.785 1056.495 1758.337 2447.797 3054.071 
20 516.103 512.610 510.546 1099.199 1759.448 2477.007 3021.368 
PS 
mode 
0 20.349 41.126 64.903 712.858 1180.742 1652.981 2068.886 
5 94.676 120.595 138.159 756.769 1236.005 1690.681 2107.585 
10 188.282 155.224 173.431 755.412 1274.347 1724.810 2113.297 
20 257.897 474.881 274.890 768.764 1247.358 1725.381 2117.081 
OD 
mode 
0 15.422 26.561 43.711 807.412 1516.140 2220.892 2926.660 
5 105.881 133.768 161.069 767.735 1733.671 2433.350 3063.280 
10 216.499 175.010 182.916 869.106 1578.218 2407.817 3072.742 
20 232.068 220.862 300.934 799.179 1557.660 2429.356 3029.503 
LAD 
0 20.563 41.198 95.615 776.907 1475.156 2291.333 2901.684 
5 112.530 106.221 126.801 957.703 1557.115 2203.301 2980.904 
10 207.967 161.345 166.637 870.518 1631.205 2207.031 2976.349 
20 213.865 302.963 294.978 676.686 1370.538 2073.595 2787.763 
 
 68 
Table 6: Detail results of time effect of TD on TT (Frame = 1460 Byte) 
       Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 0.353 0.525 0.721 8.969 12.421 17.518 25.286 
5 0.996 1.188 1.321 11.687 13.115 18.323 24.394 
10 2.481 2.267 1.818 9.811 12.892 17.752 23.960 
20 3.330 3.281 3.245 14.031 12.760 17.835 24.511 
PS 
mode 
0 0.621 0.913 1.254 11.391 18.925 25.933 31.738 
5 1.448 1.825 2.004 10.599 19.379 26.427 32.430 
10 2.947 2.252 2.442 12.100 19.803 26.545 32.802 
20 3.708 3.749 3.941 11.890 19.405 26.641 32.827 
OD 
mode 
0 0.408 0.548 0.738 7.769 13.033 18.427 24.356 
5 1.394 1.707 1.931 8.435 13.749 19.017 25.097 
10 2.818 2.221 2.285 8.329 13.512 18.971 24.542 
20 3.723 3.720 3.874 8.352 13.584 18.841 24.547 
LAD 
0 0.630 0.940 1.271 10.855 16.063 21.985 24.732 
5 1.403 1.500 1.646 10.192 16.104 21.200 24.741 
10 2.861 1.993 2.080 9.852 16.307 21.228 25.182 
20 3.742 3.871 3.611 10.482 17.143 21.356 25.566 
 
 
 
 70 
 
Figure 10: Power effect of TD on PC (Frame = 1460 Byte) 
 
8000 byte frame 
Although 8000 byte frame is the longest one, PS mode TT keeps 6s longer than other frames‟ size, as in 
Figure 9. Comparing OD and PM Mode, OD mode spends less than 1s longer than PM Mode, yet saves 
200~400J in other cases. Comparing LAD algorithm and OD mode, LAD algorithm still keeps its advantages 
in the longest frame size, spends almost the same TT in 8 ranks and average 2~3s longer in other cross-node 
situations, consuming 100~ 400J less than OD mode. 
 
Table 8: Detail results of time effect of TD on TT (Frame = 8000 Byte) 
       Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 1.220 1.409 1.597 11.158 20.343 26.952 31.241 
5 1.484 1.710 1.783 13.364 21.986 27.664 34.053 
10 1.993 2.171 2.260 11.857 21.455 27.397 33.398 
20 3.824 3.753 3.732 11.247 21.604 27.513 33.178 
 72 
Table 9: Detail results of power effect of TD on PC (Frame = 8000 Byte) 
       Rank 
Number 
 
Mode & TD 
2 3 4 5 6 7 8 
PM 
mode 
0 193.677 223.682 253.527 1771.355 3229.492 4278.684 4959.571 
5 235.588 271.466 283.055 2121.562 3490.321 4391.715 5405.982 
10 316.393 344.651 358.780 1882.322 3406.024 4349.329 5301.999 
20 607.068 595.796 592.462 1785.484 3429.678 4367.744 5267.074 
PS 
mode 
0 166.576 186.997 200.777 1176.672 1958.431 2437.525 2767.107 
5 159.936 191.638 211.630 1193.522 1979.779 2514.637 2847.718 
10 198.064 220.483 231.693 1237.790 1413.934 2526.632 2936.468 
20 334.509 334.009 303.022 1192.380 1971.568 2514.637 2851.002 
OD 
mode 
0 107.866 147.418 185.271 1320.356 2877.695 4069.769 4903.488 
5 156.932 193.698 202.611 1652.663 2925.864 4059.867 5447.829 
10 203.622 231.316 238.859 1706.812 2862.416 4076.114 5432.837 
20 353.408 367.326 361.262 1664.418 3014.893 4030.163 5487.617 
LAD 
0 167.818 201.826 224.777 1355.342 2522.337 3977.022 4695.212 
5 183.581 197.163 205.979 1436.942 2499.579 3962.008 4708.029 
10 212.619 220.259 225.554 1301.254 2938.202 4316.622 5198.511 
20 284.493 376.613 329.994 1265.924 2629.309 4060.896 5061.468 
 
 
 74 
 Operation command sending in any application 
Small data frame helps to reduce transmission time and energy consumption, more core calculation cycles 
can be released to do CPU bound jobs. 
Besides, there are two kinds of application suitable to use large data frame. 
 Database server that is sending data back 
 Distributed file transmission 
Larger data frame reduces frame generated time and transmits more data in single frame because larger 
content space. 
There are many directions to continue this investigation, to develop methods to save energy. If hardware 
and software provides functions about voltage or speed control, motherboard or any other type of peripheral 
device, then a hardware driver, power-aware job scheduling and data distribution algorithms can be 
combined and implemented, targeting in the construction of a low energy cost cluster computing platform in 
future. 
 
Reference 
1. “Power Management Guide”, http://www.gentoo.com/doc/en/power-management-guide.xml 
2. “Enabling CPU Frequency Scaling”, http://ubuntu.wordpress.com/2005/11/04/enabling-cpu-freq uency
-scaling/ 
3. “Enhanced Intel SpeedStep Technology for the Intel Pentium M Processor”, ftp://download.intel.co
m/design/network/papers/30117401.pdf 
4. “AMD PowerNow! Technology Platform Design Guide for Embedded Processors”, http://www.am
d.com/epd/processors/6.32bitproc/8.amdk6fami/x24267/24267a.pdf 
5. “AMD / Intel CPU voltage control driver down load”, http://www.linux-phc.org/viewtopic.php? f=
13 &t= 2 
 76 
16. “InfiniBand Introduction”, http://en.wikipedia.org/wiki/InfiniBand 
17. Seung Woo Son, Guangyu Chen, Ozcan Ozturk Mahmut Kandemir and Alok Choudhary, “Com
piler-Directed Energy Optimization for Parallel-Disk-Based Systems” IEEE Transactions on Parall
el and Distributed Systems, September 2007, Volume. 18, No. 9, Pages: 1241-1257 
18. Sri Hari Krishna Narayanan, Mahmut Kandemir and Ozcan Ozturk, “Compiler-Directed Power D
ensity Reduction in NoC-Based Multi-Core Designs”, Proceedings of the 7th International Synpo
sium on Quality Electronic Desing, 2006, Pages: 570-575 
19. David Meisner, Brian T. Gold and Thomas F. Wenisch, “PowerNap: eliminating server idle po
wer”, Proceeding of the 14th International Conference on Architectural Support for Programming
 Languages and Operation Systems, 2009, Pages: 205-216 
20. Michael B. Healy, Hsien-Hsin S. Lee, Gabriel H. Loh and Sung Kyu Lim, “Thermal Optimizati
on in Multi-Granularity Multi-core Floorplanning” Proceedings of the 2009 Conference on Asia 
and South Pacific Design Automation, 2009, Pages: 43-48 
21. M. Aater Suleman, Onur Mutlu, moinuddin K. Qureshi and Yale N. Patt, “Accelerating Critical
 Section Execution with Asymmetric Multi-core Architecture”, Proceeding of the 14th Inteernati
onal Conferenceon Architectural Support for Programming Languages and Operating Systems, 20
09, Pages: 253-264 
22. David C. Snowdon, Etienne Le Sueur, Stefan M. Petters and Gemot Heiser, “Koala: A Platform
 for OS-Level Power Management”, Proceedings of the Fourth ACM European Conference on 
Computer Systems, 2009, Pages: 289-302 
23. Alexander S. van Amesfoort, Ana lucia Varbanescu, Henk J. Sips and Rob V. van Nieuwpoort,
 “Evaluating Multi-core Platforms for HPC Data-Intensive kernels”, Proceedings of the 6th AC
M Conference on Computing Frontiers, 2009, Pages: 207-216 
24. XianGrong Zhou, ChenJie Yu and Peter Petrov, “Temperature-Aware Register Reallocation for 
Register File Power-Density Minimization”, ACM Transactions on Design Automation of Electro
nic Systems, March 2009, Volume 14, Issue 2, No. 26. 
 78 
 
 
行政院所屬各機關人員出國報告書提要                  
                                                 撰寫時間： 98 年  5 月  30  日           
姓 名 許慶賢 服 務 機 關 名 稱 
 
中華大學 
資工系 
連絡電話、 
電子信箱 
03-5186410 
chh@chu.edu.tw 
出 生 日 期  62 年 2  月 23  日 職 稱 副教授 
出席國際會議 
名 稱 
The 3rd ChinaGrid Annual Conference (ChinaGrid) 
到 達 國 家 
及 地 點 
Kunming, China 出 國 
期 間 
自 98 年 05 月 24 日 
迄 98 年 05 月 29 日 
內 容 提 要 
這一次在昆明所舉行的國際學術研討會議共計四天。第一天上午本人抵達會
場辦理報到，第一天，除了主持一場 invited session 的論文發表。同時，自
己也在上午的場次發表了這次被大會接受的論文。第二天，聽取了 Prof. Kai 
Hwang 有關於 Massively Distributed Systems: From Grids and P2P to Clouds 精
闢的演說。第二天許多重要的研究進行論文發表。本人餐與 Architecture and 
Infrastructure、Grid computing、以及 P2P computing 相關場次聽取報告。晚
上本人亦參加酒會，並且與幾位國外學者及中國、香港教授交換意見，合影
留念。第三天本人在上午聽取了 Data and Information Management 相關研
究，同時獲悉許多新興起的研究主題，並了解目前國外大多數學者主要的研
究方向，並且把握機會與國外的教授認識，希望能夠讓他們加深對台灣研究
的印象。第四天，本人則是選擇 Service Oriented Computing 以及 Network 
Storage 相關研究聆聽論文發表。四天下來，本人聽了許多優秀的論文發表。
這些研究所涵蓋的主題包含有：網格系統技術、工作排程、網格計算、網格
資料庫以及無線網路等等熱門的研究課題。此次的國際學術研討會議有許多
知名學者的參與，讓每一位參加這個會議的人士都能夠得到國際上最新的技
術與資訊。是一次非常成功的學術研討會。參加本次的國際學術研討會議，
感受良多。讓本人見識到許多國際知名的研究學者以及專業人才，得以與之
交流。讓本人與其他教授面對面暢談所學領域的種種問題。看了眾多研究成
果以及聽了數篇專題演講，最後，本人認為，會議所安排的會場以及邀請的
講席等，都相當的不錯，會議舉辦得很成功，值得我們學習。 
出 席 人 所 屬 機 
關 審 核 意 見 
 
層 轉 機 關 
審 核 意 見 
 
研 考 會 
處 理 意 見 
 
 80 
compared with the traditional QoS guided scheduling 
method. To evaluate the performance of the proposed 
techniques, we have implemented our rescheduling 
approaches along with the QoS Min-Min scheduling 
algorithm [9] and the non-QoS based Min-Min scheduling 
algorithm. The experimental results show that the 
proposed techniques are effective in heterogeneous 
systems under different circumstances. The improvement 
is also significant in economic grid model [3]. 
The rest of this paper is organized as follows. 
Section 2 briefly describes related research in grid 
computing and job scheduling.  Section 3 clarifies our 
research model by illustrating the traditional Min-min 
model and the QoS guided Min-min model.  In Section 4, 
two optimization schemes for reducing the total execution 
time of an application and reducing resource need are 
presented, where two rescheduling approaches are 
illustrated in detail. We conduct performance evaluation 
and discuss experiment results in Section 5. Finally, 
concluding remarks and future work are given in Section 
6. 
2. Related Work 
Grid scheduling can be classified into traditional grid 
scheduling and QoS guided scheduling or economic based 
grid scheduling.  The former emphasizes the 
performance of systems of applications, such as system 
throughput, jobs‟ completion time or response time.  
Swany et al. provides an approach to improving 
throughput for grid applications with network logistics by 
building a tree of “best” paths through the graph and has 
running time of O(NlogN) for implementations that keep 
the edges sorted [15].  Such approach is referred as the 
Minimax Path (MMP) and employs a greedy, 
tree-building algorithm that produces optimal results [20].  
Besides data-parallel applications requiring high 
performance in grid systems, there is a Dynamic Service 
Architecture (DSA) based on static compositions and 
optimizations, but also allows for high performance and 
flexibility, by use of a lookahead scheduling mechanism 
[4]. To minimizing the processing time of extensive 
processing loads originating from various sources, the 
approaches divisible load model [5] and single level tree 
network with two root processors with divisible load are 
proposed [12]. In addition to the job matching algorithm, 
the resource selection algorithm is at the core of the job 
scheduling decision module and must have the ability to 
integrate multi-site computation power.  The CGRS 
algorithm based on the distributed computing grid model 
and the grid scheduling model integrates a new 
density-based internet clustering algorithm into the 
decoupled scheduling approach of the GrADS and 
decreases its time complexity [24].  The scheduling of 
parallel jobs in a heterogeneous multi-site environment, 
where each site has a homogeneous cluster of processors, 
but processors at different sites has different speeds, is 
presented in [18]. Scheduling strategy is not only in batch 
but also can be in real-time.  The SAREG approach 
paves the way to the design of security-aware real-time 
scheduling algorithms for Grid computing environments 
[21].  
For QoS guided grid scheduling, apparently, 
applications in grids need various resources to run its 
completion.  In  [17], an architecture named public 
computing utility (PCU) is proposed uses virtual machine 
(VMs) to implement “time-sharing” over the resources 
and augments finite number of private resources to public 
resources to obtain higher level of quality of services.  
However, the QoS demands maybe include various 
packet-type and class in executing job. As a result, a 
scheduling algorithm that can support multiple QoS 
classes is needed.  Based on this demand, a multi-QoS 
scheduling algorithm is proposed to improve the 
scheduling fairness and users‟ demand [11].  He et al. [7] 
also presented a hybrid approach for scheduling moldable 
jobs with QoS demands.  In [9], a novel framework for 
policy based scheduling in resource allocation of grid 
computing is also presented.  The scheduling strategy 
can control the request assignment to grid resources by 
adjusting usage accounts or request priorities. Resource 
management is achieved by assigning usage quotas to 
intended users. The scheduling method also supports 
reservation based grid resource allocation and quality of 
service feature.  Sometimes the scheduler is not only to 
match the job to which resource, but also needs to find the 
optimized transfer path based on the cost in network. In 
[19], a distributed QoS network scheduler (DQNS) is 
presented to adapt to the ever-changing network 
conditions and aims to serve the path requests based on a 
cost function. 
3. Research Architecture 
  
Our research model considers the static scheduling 
of batch jobs in grids.  As this work is an extension and 
optimization of the QoS guided scheduling that is based 
on Min-Min scheduling algorithm [9], we briefly describe 
the Min-Min scheduling model and the QoS guided 
Min-Min algorithm.  To simplify the presentation, we 
first clarify the following terminologies and assumptions. 
 QoS Machine (MQ) – machines can provide special 
services. 
 QoS Task (TQ) – tasks can be run completion only on 
QoS machine. 
 Normal Machine (MN) – machines can only run 
normal tasks. 
 Normal Task (TN) – tasks can be run completion on 
both QoS machine and normal machine. 
 82 
4.1 Makespan Optimization Rescheduling (MOR) 
The first one is Makespan Optimization Rescheduling 
(MOR), which focuses on improving the makespan to 
achieve better performance than the QoS guided 
scheduling algorithm. Assume the makespan achieved by 
the QoS guided approach in different machines are CT1, 
CT2, …, CTm, with CTk = max { CT1, CT2, …, CTm }, 
where m is the number of machines and 1  k  m.  By 
subtracting CTk – CTi, where 1  i  m and i  k, we can 
have m-1 available time fragments.  According to the 
size of these available time fragments and the size of tasks 
in machine Mk, the MOR dispatches suitable tasks from 
machine Mk to any other machine that has available and 
large enough time fragments.  Such optimization is 
repeated until there is no task can be moved.   
 
 
 *M1 
 
M2 
T1 7 4 
T2 3 3 
T3 9 5 
*T4 5 X 
Machine 
Makespan 
A. The Min-Min algorithm B. The QOS guided scheduling algorithm  
M3 
7 
5 
7 
X 
T5 9 8 6 
*T6 5 X X 
Machine 
0 
M1 M2 
*T4 
*T6 
T1 
3 
8 
12 
M3 
T3 
T5 
Makespan 
T2 
0 
M1 M2 
T2 
*T4 
*T6 
T1 3 
8 
13 
M3 
T3 
T5 
 
Figure 3. Min-Min and QoS Guided Min-Min 
 
 
 12  
 *M1 M2 
T1 7 4 
T2 3 3 
T3 9 5 
*T4 5 X 
B. The Makespan Optimization 
Rescheduling (MOR) algorithm  
M3 
7 
5 
7 
X 
T5 9 8 6 
*T6 5 X X 
Machine 
0 
*M1 M2 
*T4 
*T6 
T1 3 
8 
11 
M3 
T3 
T5 
Makespan 
T2 
Machine 
T1 
T2 
T3 
M2 
T5 
M3 
A. The QOS guided scheduling 
algorithm 
*T6 
*T4 
*M1 
Makespan 
12 
8 
3 
 
Figure 4. Example of MOR 
 
Recall the example given in Figure 3, Figure 4 
shows the optimization of the MOR approach.  The left 
side of Figure 4 demonstrates that the QoS guided scheme 
gives a schedule with makespan = 12, wheremachine M2 
presents maximum CT (completion time), which is 
assembled by tasks T2, T1 and T3.  Since the CT of 
machine „M3‟ is 6, so „M3‟ has an available time 
fragment (6).  Checking all tasks in machine M2, only 
T2 is small enough to be allocated in the available time 
fragment in M3.  Therefore, task M2 is moved to M3, 
resulting machine „M3‟ has completion time CT=11, 
which is better than the QoS guided scheme. 
As mentioned above, the MOR is based on the QoS 
guided scheduling algorithm.  If there are m tasks to be 
scheduled in n machines, the time complexity of MOR is 
O(m
2
n).  Figure 5 outlines a pseudo of the MOR scheme.   
 
 84 
scheme. The experiment model consists of heterogeneous 
machines and tasks.  Both of the Machines and tasks are 
classified into QoS type and non-QoS type.  Table 1 
summarizes six parameters and two comparison metrics 
used in the experiments.  The number of tasks is ranged 
from 200 to 600. The number of machines is ranged from 
50 to 130. The percentage of QoS machines and tasks are 
set between 15% and 75%.  Heterogeneity of tasks are 
defined as Ht (for non-QoS task) and HQ (for QoS task), 
which is used in generating random tasks.  For example, 
the execution time of a non-QoS task is randomly 
generated from the interval [10, Ht10
2
] and execution 
time of a QoS task is randomly generated from the 
interval [10
2
, HQ10
3
] to reflect the real application world.  
All of the parameters used in the experiments are 
generated randomly with a uniform distribution.  The 
results demonstrated in this section are the average values 
of running 100 random test samples.  
 
Table 1: Parameters and Comparison Metrics 
  
Task number (NT) {200, 300, 400, 500, 600} 
Resource number (NR) {50, 70, 90, 110, 130} 
Percentage of QOS resources (QR %) {15%, 30%, 45%, 60%, 75%} 
Percentage of QOS tasks (QT %) {15%, 30%, 45%, 60%, 75%} 
Heterogeneity of non-QOS tasks (HT) {1, 3, 5, 7, 9} 
Heterogeneity of QOS tasks (HQ) {3, 5, 7, 9, 11} 
Makespan 
The completion time of a set of 
tasks 
Resource Used (RU) 
Number of machines used for 
executing a set of tasks 
 
 
5.2 Experimental Results of MOR 
 
Table 2 compares the performance of the MOR, Min-Min 
algorithm and the QoS guided Min-Min scheme in term of 
makespan.  There are six tests that are conducted with 
different parameters.  In each test, the configurations are 
outlined beside the table caption from (a) to (f).  Table (a) 
changes the number of tasks to analyze the performance 
results.  Increasing the number of tasks, improvement of 
MOR is limited. An average improvement ratio is from 
6% to 14%.  Table (b) changes the number of machines.  
It is obvious that the MOR has significant improvement in 
larger grid systems, i.e., large amount of machines.  The 
average improvement rate is 7% to 15%.  Table (c) 
discusses the influence of changing percentages of QoS 
machines.  Intuitionally, the MOR performs best with 
45% QoS machines.  However, this observation is not 
always true.  By analyzing the four best ones in (a) to (d), 
we observe that the four tests (a) NT=200 (NR=50, QR=30%, 
QT=20%) (b) NR=130 (NT=500, QR=30%, QT=20%) (c) 
QR=45% (NT=300, NR=50, QT=20%) and (d) QT=15% 
(NT=300, NR=50, QR=40%) have best improvements.  All of 
the four configurations conform to the following relation, 
 
0.4  (NT  QT) = NR  QR          (3) 
 
This observation indicates that the improvement of MOR 
is significant when the number of QoS tasks is 2.5 times 
to the number of QoS machines.  Tables (e) and (f) 
change heterogeneity of tasks.  We observed that 
heterogeneity of tasks is not critical to the improvement 
rate of the MOR technique, which achieves 7% 
improvements under different heterogeneity of tasks. 
 
Table 2: Comparison of Makespan 
 
(a) (NR=50, QR=30%, QT=20%, HT=1, HQ=1) 
 
Task Number (NT) 200 300 400 500 600 
Min-Min 978.2 1299.7 1631.8 1954.6 2287.8 
QOS Guided Min-Min 694.6 917.8 1119.4 1359.9 1560.1 
MOR 597.3 815.5 1017.7 1254.8 1458.3 
Improved Ratio 14.01% 11.15% 9.08% 7.73% 6.53% 
  
(b) (NT=500, QR=30%, QT=20%, HT=1, HQ=1) 
 
Resource Number (NR) 50 70 90 110 130 
Min-Min 1931.5 1432.2 1102.1 985.3 874.2 
QOS Guided Min-Min 1355.7 938.6 724.4 590.6 508.7 
MOR 1252.6 840.8 633.7 506.2 429.4 
Improved Ratio 7.60% 10.42% 12.52% 14.30% 15.58% 
  
(c) (NT=300, NR=50, QT=20%, HT=1, HQ=1) 
 
QR% 15% 30% 45% 60% 75% 
Min-Min 2470.8 1319.4 888.2 777.6 650.1 
QOS Guided Min-Min 1875.9 913.6 596.1 463.8 376.4 
MOR 1767.3 810.4 503.5 394.3 339.0 
Improved Ratio 5.79% 11.30% 15.54% 14.99% 9.94% 
  
(d) (NT=300, NR=50, QR=40%, HT=1, HQ=1) 
 
QT% 15% 30% 45% 60% 75% 
Min-Min 879.9 1380.2 1801.8 2217.0 2610.1 
QOS Guided Min-Min 558.4 915.9 1245.2 1580.3 1900.6 
MOR 474.2 817.1 1145.1 1478.5 1800.1 
Improved Ratio 15.07% 10.79% 8.04% 6.44% 5.29% 
  
(e) (NT=500, NR=50, QR=30%, QT=20%, HQ=1) 
 
HT 1 3 5 7 9 
Min-Min 1891.9 1945.1 1944.6 1926.1 1940.1 
QOS Guided Min-Min 1356.0 1346.4 1346.4 1354.9 1357.3 
MOR 1251.7 1241.4 1244.3 1252.0 1254.2 
Improved Ratio 7.69% 7.80% 7.58% 7.59% 7.59% 
  
(f) (NT=500, NR=50, QR=30%, QT=20%, HT=1) 
 86 
in Grid Computing”, Journal of Concurrency: Practice and 
Experience, vol. 14, pp. 13-15, 2002. 
[4] Jesper Andersson, Morgan Ericsson, Welf Löwe, and Wolf 
Zimmermann, "Lookahead Scheduling for Reconfigurable 
GRID Systems", 10th International Europar'04: Parallel 
Processing, vol. 3149, pp. 263-270, 2004. 
[5] D Yu, Th G Robertazzi, "Divisible Load Scheduling for Grid 
Computing", 15th IASTED Int‟l. Conference on Parallel and 
Distributed Computing and Systems, Vol. 1, pp. 1-6, 2003 
[6] Fangpeng Dong and Selim G. Akl, "Scheduling Algorithms for 
Grid Computing: State of the Art and Open Problems", 
Technical Report No. 2006-504, 2006. 
[7] Ligang He, Stephen A. Jarvis, Daniel P. Spooner, Xinuo Chen, 
Graham R. Nudd, "Hybrid Performance-oriented Scheduling of 
Moldable Jobs with QoS Demands in Multiclusters and Grids", 
Grid and Cooperative Computing (GCC 2004), vol. 3251, pp. 
217–224, 2004.  
[8] Xiaoshan He, Xian-He Sun, Gregor Von Laszewski, "A QoS 
Guided Scheduling Algorithm for Grid Computing", Journal of 
Computer Science and Technology, vol.18, pp.442-451, 2003. 
[9] Jang-uk In, Paul Avery, Richard Cavanaugh, Sanjay Ranka, 
"Policy Based Scheduling for Simple Quality of Service in Grid 
Computing", IPDPS 2004, pp. 23, 2004. 
[10] J. Schopf. "Ten Actions when Superscheduling: A Grid 
Scheduling Architecture", Scheduling Architecture Workshop, 
7th Global Grid Forum, 2003. 
[11] Junsu Kim, Sung Ho Moon, and Dan Keun Sung, "Multi-QoS 
Scheduling Algorithm for Class Fairness in High Speed 
Downlink Packet Access", Proceedings of IEEE Personal, 
Indoor and Mobile Radio Communications Conference (PIMRC 
2005), vol. 3, pp. 1813-1817, 2005 
[12] M.A. Moges and T.G. Robertazzi, "Grid Scheduling Divisible 
Loads from Multiple Sources via Linear Programming", 16th 
IASTED International Conference on Parallel and Distributed 
Computing and Systems (PDCS), pp. 423-428, 2004. 
[13] M. Baker, R. Buyya and D. Laforenza, "Grids and Grid 
Technologies for Wide-area Distributed Computing", in Journal 
of Software-Practice & Experience, Vol. 32, No.15, pp. 
1437-1466, 2002. 
[14] Jennifer M. Schopf, "A General Architecture for Scheduling on 
the Grid", Technical Report ANL/MCS, pp. 1000-1002, 2002. 
[15] M. Swany, "Improving Throughput for Grid Applications with 
Network Logistics", Proc. IEEE/ACM Conference on High 
Performance Computing and Networking, 2004. 
[16] R. Moreno and A.B. Alonso, "Job Scheduling and Resource 
Management Techniques in Economic Grid Environments", 
LNCS 2970, pp. 25-32, 2004. 
[17] Shah Asaduzzaman and Muthucumaru Maheswaran, 
"Heuristics for Scheduling Virtual Machines for Improving QoS 
in Public Computing Utilities", Proc. 9th International 
Conference on Computer and Information Technology 
(ICCIT‟06), 2006. 
[18] Gerald Sabin, Rajkumar Kettimuthu, Arun Rajan and P 
Sadayappan, "Scheduling of Parallel Jobs in a Heterogeneous 
Multi-Site Environment", in the Proc. of the 9th International 
Workshop on Job Scheduling Strategies for Parallel Processing, 
LNCS 2862, pp. 87-104 , June 2003. 
[19] Sriram Ramanujam, Mitchell D. Theys, "Adaptive Scheduling 
based on Quality of Service in Distributed Environments", 
PDPTA’05, pp. 671-677, 2005. 
[20] T. H. Cormen, C. L. Leiserson, and R. L. Rivest, "Introduction 
to Algorithms", First edition, MIT Press and McGraw-Hill, 
ISBN 0-262-03141-8, 1990. 
[21] Tao Xie and Xiao Qin, "Enhancing Security of Real-Time 
Applications on Grids through Dynamic Scheduling", Proc. the 
11th Workshop on Job Scheduling Strategies for Parallel 
Processing (JSSPP'05), pp. 146-158, 2005.  
[22] Haobo Yu, Andreas Gerstlauer, Daniel Gajski, "RTOS 
Scheduling in Transaction Level Models", in Proc. of the 1st 
IEEE/ACM/IFIP international conference on Hardware/software 
Codesign & System Synpaper, pp. 31-36, 2003.  
[23] Y. Zhu, "A Survey on Grid Scheduling Systems", LNCS 4505, 
pp. 419-427, 2007. 
[24] Weizhe Zhang, Hongli Zhang, Hui He, Mingzeng Hu, 
"Multisite Task Scheduling on Distributed Computing Grid", 
LNCS 3033, pp. 57–64, 2004. 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/29
國科會補助計畫
計畫名稱: 應用於格網與可擴充平行系統之通訊最佳化, 廣播與品質服務排程技術之研究
計畫主持人: 許慶賢
計畫編號: 97-2221-E-216-011-MY3 學門領域: 平行與分散處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
- 主辦 2009 台俄雙邊研討會 - 平行多核心計算工具與應用 
- 獲邀 ICEGT 2011 給予專題演講 
- Best Paper Award, Computer Society of the Republic of China, 2010 
- Best Paper Award, 2009 National Computer Symposium, 2009 
- 協助建構 Taiwan UniGrid 平台。發展 UniBox 協助近 20 所國內大學加入
UniGrid 平台。成功開發 Data Replication and Management 子系統。 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
