 
研究計畫成果報告中英文摘要：  
（一） 計畫中文摘要 
由馬偕‧叡理博士（Rev. Dr. George Leslie Mackay）規劃，於 1882 年建成，有「台
灣北部最早的教育學府」之稱的「牛津學堂」是典型中西合璧的建築，擁有豐富的史
蹟文化內涵，現為國家二級古蹟，亦為本校之校史館。在 95 年國科會專題計畫「NSC 
95-2221-E-156 -005：支援時空情境轉換之多人多點暨互動式虛擬展示系統」及在 96 年
國科會產學專題計畫「NSC 96-2622-E-156-001-CC3：支援室內外定位功能之虛擬導覽
系統的設計與實作」的支持下，我們完成了虛擬牛津學堂暨馬偕文物館之建構、馬偕
歷史文件之修復等研究，亦完成了一套支援室內外定位功能之虛擬導覽系統。 
在獲得原計畫第一、二年寶貴的執行經驗與成果後，我們延續原計畫內容，進一
步的結合主持人 3D 動作分析的研究經驗與成果，提出第三年之研究計畫：「支援直覺
化 3D 人機介面之虛擬導覽系統的設計與實作」。預計完成能讓使用者在不受羈絆的融
入式虛擬實境中，直接以聲音及動作控制虛擬導覽系統的直覺化 3D 人機介面。藉由此
延續計畫的執行，期望能完整地完成原有三年期國科會計畫之所有研究內容與目標。 
關鍵詞：馬偕、牛津學堂、虛擬實境、虛擬導覽、動作分析、3D 人機介面 
（二） 計畫英文摘要 
Planned by Rev. Dr. George Leslie Mackay, Oxford College, the first educational 
institute in northern Taiwan, was established in 1882. It is a typical Chinese and Western 
style architecture with rich historical and cultural content, now is a Class 2 national 
monument, and also the school history hall of Aletheia University. Under the support of first 
two years National Science Council Project (NSC95-2221-E-156-005 and 
NSC96-2622-E-156-001-CC3), we have completed the researches of image inpainting, 
virtual tour system of Oxford College, and a indoor/outdoor positioning supported virtual 
tour system. 
After gaining results and experiences from the first two years, and for the completeness 
of the original project, now we integrate the outcomes and experiences of 3D motion 
analysis research, the third-year project is proposed as “Intuitional 3D Human Computer 
Interface Supported Virtual Tour System”. It is expected to allow users controlling intuitional 
3D human computer interface of virtual tour system by voices and motions under 
restriction-free virtual environment. This project is aimed to complete all the research 
contents and goals of the original three-year National Science Council project. 
Keywords:  Mackay, Oxford College, Virtual Reality, Virtual Tour, Motion Analysis,  
3D Human Computer Interface 
 
在國科會經費的支援下，我們成功地建置了如圖 1 所示的直覺化虛擬實
境人機介面。此環境包含用以投射出虛擬實境場景之投影機、螢幕前方的二
台視訊攝影機及收音麥克風，用擷取使用者的動作及聲音資料，再傳至電腦
系統，作為分析比對動作及聲音指令之用。此外；我們還另外加入了日漸普
及的 WiiRemote 設備，讓使用者也能使用 WiiRemote 直覺且有效率的改變導
覽視角及操作虛擬導覽系統。(詳細內容請見附錄[1]之國際會議論文) 
 
   
   圖 1：融入式虛擬實境之直覺化人機介面示意圖 
2. 語音指令之分析與實作：由於語音之比對技術已臻成熟，故此部分研究以結
合現有之語音比對技術為主，我們採用了免費的 Microsoft Speech API，實
作出了適合虛擬實境系統之語音指令辨識組件。我們所完成之語音指令辨識
組件可分成四部分：Microphone、語音辨識引擎(Voice Recognition 
Engine)、語音指令辨識核心(Voice Command Recognition Core)及語音指令
輸出模組(Voice Command Output Module)。 
3. 3D 動作即時追蹤之實作及 3D 動作指令分析與比對之研究：此部分計畫是以二
部一般的視訊攝影機，將使用者之動作指令即時轉換成三維的動作資料，並
進行 3D 動作指令之分析比對，進而能直覺的以動作指令與虛擬實境場景互動
(3D 動作即時追蹤及動作指令分析流程圖如圖 4 所示)。在此部分研究中，我
們成功地完成了一套即時的 3D 動作追蹤系統，使用者可以直接利用雙手，自
然地做出各種動作指令，直覺化地與虛擬實境場景互動(如圖 5所示)。 
本系統只需要兩台一般的 CCD webcam 當作追蹤設備，透過即時的 Stereo 
vision 影像處理技術，即可追蹤、分析使用者的 3D 互動指令。搭配我們所
定義的數種互動指令，更可讓使用者不必花太多的時間去學習就可以上手，
達成了低成本且直覺化的 3D 人機互動功能。由實驗結果可以看出本系統不僅
有令人滿意的準確度，也有效地增進了與虛擬實境互動的樂趣(如表 1所示)。 
(詳細內容請見附錄[2]之國際會議論文) 
 
圖 4：3D 動作即時追蹤及動作指令分析流程圖 
 
 
計畫成果自評 
在 97 年國科會專題研究計畫的支持下，我們順利地完成了計畫預計之所有主要工
作：1.直覺化虛擬實境人機介面之建構、2.語音指令之分析與實作、3. 3D 動作之即時
追蹤及 3D 動作指令之分析與比對。此外；我們還另外加入了日漸普及的 WiiRemote，
讓使用者也能使用 WiiRemote 直覺且有效率的改變導覽視角及操作虛擬導覽系統，進
一步地提高了與虛擬實境互動的樂趣。而參與計畫之研究生也均能從計畫中找到適合
的研究方向及論文題目，並都已順利的完成碩士學業。 
本計畫成功地結合虛擬導覽系統及直覺化的語音導覽指令、3D 動作導覽指令等功
能，實作出一套支援直覺化 3D 人機介面之虛擬導覽系統，並發表了二篇 IASTED 國際
會議論文：“An Intuitional 3D Virtual Navigation System Supporting Motion and Voice 
Commands”,於 The IASTED International Conference on Internet and Multimedia Systems 
and Applications 2008 (IMSA-2008)，及“A Real-Time 3d Motion Tracking and Interaction 
System for Virtual Reality”於 The IASTED International Conference on Internet and 
Multimedia Systems and Applications 2009 (IMSA-2009) ，最新之成果亦已投稿至 SCI/EI
等級之期刊，成果可謂豐碩。 
常見之虛擬實境展示系統大多缺乏與使用者之互動，即使少數系統可與使用者互
動，但往往需要搭配一些昂貴、不易維護且容易損壞之虛擬實境專用設備；或搭配一
般使用者不易馬上學會之操作方法。本計畫所完成之直覺化 3D 人機介面大幅地降低了
互動設備之經費需求，達成了讓使用者能在不受羈絆的融入式虛擬實境中，直接以聲
音及動作指令與虛擬實境場景作互動。由實驗結果可以看出本系統不僅有令人滿意的
準確度，也有效地提高了虛擬導覽系統的實用性與使用樂趣。且本計畫所發展之互動
式 3D 人機介面不單適用於虛擬導覽系統，亦可廣泛的應用於虛擬實境系統或娛樂業，
以提供使用者更生動、更多元化的互動方式。國內專營虛擬實境系統之主要廠商例如：
愛迪斯科技及艾伯特電通均對本計畫所研發之技術深感興趣，有意合作，足證本計畫
之成功與價值。 
 
 
 
AN INTUITIONAL 3D VIRTUAL NAVIGATION SYSTEM SUPPORTING 
MOTION AND VOICE COMMANDS 
 
Ching-Sheng Wang, Shu-Shien Kao 
Department of Computer Science and Information Engineering 
 Aletheia University  
Taiwan 
cswang@email.au.edu.tw 
 
 
ABSTRACT 
In recent years following a steady progression of the 
image representation techniques and graphic hardware, it 
has afforded the development of a virtual navigation 
system to be more achievable than before.  Yet to gauge 
from the common virtual navigation systems to date, they 
either lack a direct interaction with the user, or they 
require using complex GUI interface, or supplying 
expensive and difficult-to-maintain interactive equipment, 
such as a sensor glove, 6-way optical mouse, human 
motion capturing system and so forth.  This paper has 
adopted the common and economic equipment to devise 
an intuitional 3D virtual navigation system supporting 
voice and motion commands.  The proposed system 
adopts the WiiRemote to capture the use’s motion 
information, analyze and convert them into the navigation 
commands, such as walking, running, turning and the like, 
integrating also the voice commands, allowing the user to 
operate the virtual navigation system intuitively and 
effective in an unhindered environment. 
 
KEY WORDS 
Virtual navigation, motion, voice, WiiRemote 
 
 
1. Introduction 
 
In recent year following leaps and bounds in 
software/firmware development, it has brought a 
significant improvement to the personal computer’s image 
quality, compression texture and optical expression 
technologies, and renders the need for a high-end virtual 
reality navigation system no longer unreachable.  The 
production of a virtual reality system is no longer 
confined to requiring a professional laboratory or a 
research center but that most PC users or researchers in 
the multimedia domain are steadily capable of coming 
into contact with the domain of virtual navigations. 
 
Yet as the common virtual navigation systems presently 
still lack a user interactive functions, but their interface 
for providing the interactive functions are also largely 
confined to the conventional input equipment, such as a 
keyboard, mouse, joystick and so forth.  Nevertheless, 
under a 3D environment, a majority of the conventional 
2D input equipment that tends to have a poor efficiency 
often require pairing with certain non-intuitional 
commands or operating modes. Such types of 
conventional equipment can utilize graphic interface 
commands or more complex keyboard commands to 
allow the user to interact with a virtual scene; however, it 
tends to require the user spending a considerable amount 
of time to familiarize himself/herself to all such non-
intuitional interactive commands.  Consequently, many 
researchers have been dedicating in the research and study 
of virtual reality interactive equipment [1, 2, 3, 4, 5, 6]. 
 
In terms of studies conducted on user control and 
interaction in a 3D virtual reality, Alexander Kulik  et al. 
had designed a control method resembling the computer 
mouse supporting six-dimensional axes to control the 
browsing and interaction in a 3D virtual reality that 
allowed the user to an intuitional operation in a 3D virtual 
reality setting[1].  Yet as the equipment differs from 
common mice, it does take a little time to get familiar to 
for it is somewhat difficult to get started.  One other 
interaction system adopted intuitional interactive 
HAPTIC device’s, presented by Seungzoo Jeong et al., 
had integrated the HAPTIC device in an immersive 
virtual environment to allow the user to intuitionally 
interact with virtual avatar [2].  While Hrvoje Benko , 
Steven Feiner had adopted an exclusive glove that 
allowed the user to grab and shift spheres interactively in 
virtual environment [3].  Frank Seiniche, Klaus Hinrichs 
[4] had also presented a control method for accurately 
selecting objects in a 3D virtual environment; however, it 
still required the exclusive glove, paired with complex 
hand gesturing commands, requiring the user to spend 
time learning it to greatly reduce the intuition, and besides 
that the above methods are also quite costly. 
 
In light of which, this paper has selected the common and 
economic equipment, including the WiiRemote and 
microphone as the premise of developing the computer 
interactive commands using the most intuitional voice and 
motions to humans.  With the WiiRemote becoming more 
prevalent and affordable, and as the voice recognition 
systems are developing maturely, hence we are able to 
rapidly and effectively develop a computer interactive 
virtual navigation system utilizing very little funding. 
624-066 75
2.3 The Intuitional Interactive Equipment 
 
In research, some of the common intuitional motion 
interactive equipment includes: the VICON, 5DT Glove, 
six-way optical mouse and so forth. But, those 
equipments tend to be rather expensive and difficult to 
maintain, and as they also differ somewhat from the 
conventional interface in usage, a user needs time to 
familiarize the operating and manipulating commands.  
With that, this paper has chosen the increasingly 
prevalent, and economical and practical WiiRemote as the 
system’s intuitional interactive device that allows the user 
to operate the virtual navigation system and to interact 
with it intuitionally and effectively without being 
hindered by the setting. 
 
The WiiRemote consists of two parts:  the CMOS infrared 
sensor and the Micro Electro Mechanical Systems 
(MEMS) 3-axial accelerator sensor chipset.  Of which, the 
CMOS infrared sensor receives and tabulate the infrared 
signals via the infrared sensor located at the front end of 
the remote control.  When paired with five infrared LED 
original manufacturer’s sensor bar, and with the 
WiiRemote pointing to the TV screen, it is feasible to 
calculate, to a degree, the direction and distance of the 
While the MEMS 3-axial accelerator sensor chipset is 
loaded with the ADXL330 accelerator sensor.  This is a 
three-dimensional accelerator sensor model, which is 
capable of detecting the speed of acceleration along axis 
X (left-right), Y (front-rear) and Z (up-down), and 
prompting the amount of speed acceleration with 
simulated current voltage.  
 
Because of the WiiRemote’s steadily gaining popularity, 
its comprehensive functions, economical affordability, 
sturdiness and resiliency, we have thus adopted it as the 
system’s motion interactive equipment, while the voice 
interactive equipment can be filled simply with a common 
microphone. 
 
 
3. Overview of System Framework 
 
Based on the foresaid software/hardware comparison, the 
system has adopted the WiiRemote as the intuitional 
motion input device for recognizing the motion navigation 
commands, and has adopted the microphone coordinated 
with Microsoft Speech Application Program Interface 
(Speech API)[7] for recognizing the voice navigation 
commands, which are integrated to devise a virtual 
navigation system supporting motion and voice 
commands.  
 
3.1 System Framework 
 
The system framework, as shown in Fig. 1, is composes 
of three major components, which are the navigation 
control component, the voice command recognition 
component, and the motion command recognition 
component.  Among them, the virtual navigation control 
component is consisting of the virtual scene data, the 
motion command control interface, and the voice 
command control interface, which are responsible for 
receiving the intuitional motion and voice navigation 
commands, and in turn execute and show the 
corresponding virtual scenes. 
 
In addition, the voice command recognition component is 
divided into four parts:  the microphone input device, 
Microsoft Speech API recognition engine, voice 
command recognition core and voice command output 
module. Among them, the recognition engine, upon being 
tested and adjusted, is capable of achieving a nearly 90% 
accuracy under the circumstances of without severe 
interrupting noise around.  Lastly the information the 
recognition core derives is converted into voice 
navigation commands, which are sent to the virtual 
navigation control module for execution. 
 
While the motion command recognition component 
utilizes the WiiRemote’s CMOS infrared sensor to scan 
the LED location shifting information, and adopts an 
micro electromechanical systems (MEMS) 3-axial 
accelerator sensor chipset to retrieve the walking and 
running speed, and the information derived is analyzed 
and recognized by the motion command recognition core 
before the information is converted into navigation 
commands of the virtual navigation system. 
 
 
Fig. 1: Diagram of system framework 
 
 
77
4. System Environment and Experiment 
 
The proposed system incorporates the common hardware, 
including the microphone and the prevalent WiiRemote 
that allows the user to operate a virtual navigation system 
intuitionally and effective in an unhindered virtual 
environment. This section will describe in detail the 
system’s construction, testing environment and 
experimental results. 
 
4.1 System Construction 
 
 
Fig. 3:  Environment of virtual navigation system  
 
The system integrates dual voice and motion navigation 
command systems, with its environment have been 
constructed as shown in Fig. 3.  In terms of motion 
commands, when placed at 5m in front, the WiiRemote 
captures a user’s the location shifting information through 
the LED lights fitted on the glasses by utilizing the 
CMOS infrared sensor’s positioning functions, deriving 
from two infrared LED lamps strapped to the two sides of 
a pair of glasses (as shown in Fig. 4), allowing the user to 
complete promptly the view angle converting commands 
through head motions.  
 
 
Fig. 4: Illustration of glasses + infrared LED 
 
In addition, one other WiiRemote is placed on a user to 
capture the speed acceleration information when the user 
runs and walks by utilizing the built-in MEMS 3-axial 
accelerator censor chipset, which analyzes the user’s 
running and walking motions to detect the speed change 
and to pair with corresponding motions and speed, 
allowing the user to experience a realistic speed change.  
While the microphone is also strapped onto the user 
acting as an auxiliary command input device for walking 
and running to further enhance the intuition and 
practicality of the interactive command. 
 
4.2 Testing Environment and Findings 
 
With the system environment development comprising of 
the hardware and software, the hardware is comprised of a 
Sony VAIO SZ36 notebook computer (1.83GHz CPU, 
2MB RAM), infrared LED’s, two WiiRemotes and a 
microphone.  The software is comprised of Quest3D 4.0, 
Microsoft Speech API.  The virtual scene for testing 
encompass Aletheia University virtual campus outdoors 
and Aletheia University Mackay Museum indoors, and 
ten individuals, consisting of five men and five women, 
have been chosen to participate in the tests, where three 
pairing methods of using the voice command only, or 
using the motion command only, or using a combination 
of voice and motion commands, to operate the 3D virtual 
navigation system together with comparing the 
coordination of auxiliary equipment such as a keyboard 
and mouse, to analyze the pros and cons of the three 
methods, with testing findings enlisted in Table-3. 
 
Table-3: Comparison table of operation methods of guidance 
 
 Keyboard & 
mouse 
Voice 
commands 
Motion 
commands 
Voice & 
motion 
commands 
Running 
and 
walking 
Non-
intuitional, 
difficult to 
adjust the 
speed, 
lower 
immersion, 
and requiring 
memorizing 
the  buttons 
of different 
commands 
Intuitional, 
but difficult 
to adjust the 
speed, 
with limited 
immersion, 
Intuitional 
and is able 
to adjust 
the speed, 
with high 
immersion, 
but prone 
to cause 
fatigue 
Intuitional 
and is able 
to adjust 
the speed, 
with high 
immersion, 
and can 
resolve the 
fatigue 
problem. 
Turning 
Convenient 
to operate but 
with low 
Intuition 
Difficult to 
control the 
angle 
High 
Intuition 
High 
Intuition 
View 
angle 
shifting 
Easy to 
operate, but 
with low 
Intuition 
Difficult to 
control the 
angle 
High 
Intuition 
High 
Intuition 
 
• Using the keyboard and mouse to operate the 
virtual navigation: When using the conventional 
keyboard and mouse to operate the virtual 
navigation, it is found with Non-intuitional 
operation, and difficult to adjust the speed, while it 
79
  
 
A REAL-TIME 3D MOTION TRACKING AND INTERACTION SYSTEM  
FOR VIRTUAL REALITY 
 
Ching-Sheng Wang, En-Yih Jean Jie-Ting Hou 
Department of Computer Science and Information 
Engineering, Aletheia University 
Department of Computer Science and Information 
Engineering, Tamkang University 
32 Chen-Li Street, Tamsui 151 Ying-chuan Road, Tamsui 
Taiwan Taiwan 
cswang@email.au.edu.tw, jean@email.au.edu.tw 696410348@s96.tku.edu.tw 
 
 
ABSTRACT 
Based on the development of computer’s capability and 
the technology of human-computer interactions, the 3D 
human-computer interaction is no longer a dream. In this 
paper, we propose a real-time 3D motion tracking and 
interaction system which makes the user could interact 
with virtual reality systems without any equipment in 
hand. The proposed system adopts only two general CCD 
cameras to capture user’s 3D motion information based 
on the theory of stereo-vision, according to our interaction 
mechanism, user can interact with the virtual reality 
intuitively. The experiment shows that our low cost 
system performs real-time 3D human-computer 
interaction with virtual reality satisfied. 
 
KEY WORDS 
Motion Tracking, Interaction, HCI, Stereo Vision, Virtual 
Reality 
 
 
1.  Introduction 
 
For the past few years, the human computer interaction is 
one of the most popular issues in the academic and the 
business societies, especially in the entertainment field, 
such as EyeToy of Sony PS2 and Wii of Nintendo. After 
the entering of these two gaming equipments in the 
market, it not only makes people interact with machines 
more intuitively, but also makes the games more 
interesting.  
 
Although it is possible to perform 3D interaction with 
virtual reality by using some famous motion tracking 
systems such as VICON, 5DT Glove and six-way 
optional mouse, but those equipments tend to be rather 
expensive and difficult to maintain. In the end, these are 
not suitable for use by the general users. 
 
Some researchers have been dedicating in the research 
of virtual reality interaction [1, 2, 3, 4]. Alexander Kulik 
has designed a control way which is familiar with the 
mouse of computer. It uses six axial of degrees to control 
the interaction and guidance in the virtual reality. This 
system does make the user work in the virtual reality 
much more intuitional. However, the equipment is not as 
same as the general mouse, so the user is not easy to 
acquire mastery of it [1]. Seungzoo Jeong designed 
another system called HAPTIC input system [2]. Hrvoje 
Benko and Steven Feiner produced one with a special 
glove which can grab or move a sphere in virtual reality 
[3]. Frank Steinicke and Klaus Hinrichs also brought up a 
method to select an object accurately in virtual reality, but 
it still needs a particular glove with complicated gestures 
of instructions [4]. Although the above researches worked 
well mostly, they are still quite costly and need lots of 
time on training user. In this paper, we performed a real-
time 3D motion tracking system by two common low cost 
CCD cameras to support the interaction of virtual reality.  
 
There are many motion tracking algorithms. For 
example, Kalman Filter, Mean Shift, Particle Filter and so 
on [5, 6]. Although some of these algorithms can track 
objects precisely, massive amount of computations are 
required. Takala proposed an object tracking system using 
colour, texture and motion to perform real-time multi-
object tracking [7], as well. However, most of those 
renowned tracking systems are designed for videos, not 
for 3D motion tracking. In our system, we can capture 
user’s 3D motion information based on the theory of 
stereo-vision. 
 
Stereo vision, which is most dominantly used on 
mobile robots [8], is an important technology in computer 
visions. Like humans can use two eyes to estimate depths 
and locations, robots equipped with the 3D vision system 
can achieve the same task. There are also equipments like 
Point Grey Bumblebee camera [9], which can capture 
depth images in real-time and can calculate depths of 
objects quickly. Besides, Jeongseok Ki proposed a 3D 
gaze tracking and analysis system [10], using the vision 
theory of human eyes, can track the centre of pupil and 
the reflection on the cornea to determine the 3D data of 
the object.  
 
Vision-based real-time tracking systems have 
numerous applications, such as surveillance systems, 
games, virtual reality, augmented reality, and so on [11, 
12, 13, 14]. Nevertheless, some of the systems analyze 
 Left camera 
 
Right camera 
  
 
Figure 4. Tracking result 
 
2.2 3DMotion Tracking Based on Stereo-vision 
 
In this paper, we use the theory of stereo-vision to set up 
CCD cameras in following steps [17]: 
1. Set up two same cameras in parallel on the same 
plain. 
2. Put the objects in the field, and is captured in both 
cameras 
3. Record the distance between the two cameras, and 
the focus. 
d
f f
YL e f t
XL e f t
YR i g h t
XR i g h t
Y
X
Z
(X,Y,Z)
 
(a)                                            (b) 
Figure 5. Camera set up diagram (a) Vertical view  
(b) Stereography 
 
In the Figure 5, d: is the distance between the two 
cameras. f: is the focus of the two cameras z: is the 
distance between the camera and the object 
 
The Z-axis calculation is shown below: 
Z
Y
f
Y
and
Z
Y
f
Y
Z
dX
f
x
and
Z
X
f
x
RightLeft
RightLeft
==
−
==
(2) 
 
In that two cameras are parallel to each other, therefore 
LeftY  and RightY of the two images are the same. As a 
result, the equation becomes: 
X
fdZ
∆
=                                      (3) 
 
Consequently, all we need are the distance (d) between 
the two cameras, the focus (f) and the difference of x 
coordinates of two images (∆x), then we can have the Z 
coordinate and completed 3D coordinates. 
 
 
3.  Motion Commands and Experimental 
Result 
 
 
 
 
Figure 6. System architecture 
 
The architecture of the proposed system is shown in 
Figure 6. Users will only need to stand in front of the 
cameras, and shake their hands, which need to be distinct 
from the background to initial the hand data. Then our 
system will track user’s 3D motion, analyze and send the 
motion commands to virtual reality system automatically 
and real-timed for interaction. 
 
3.1 Motion Commands 
 
The 3D coordinates of the motion acquired by tracking 
system can be used with absolute, relative coordinates and 
motion vectors to determine specific motion commands. 
The navigation/interaction commands are determined as 
shown in Figure 7. The blue area of Figure 7(a) is the 
trigger zone of commands which command will be 
triggered only when tracing object reach the blue area. 
And, the green area is the buffer zone for unknowing 
motions of user which the command will not be triggered. 
 
 
 
Figure 7.  Schematic diagram of the determination of 
navigation/interaction commands 
[6] T. Kobayashi, K. Nakagawa, J. Imae & G. Zhai, Real 
time object tracking on video image sequence using 
particle swarm optimization,  International Conference on 
Control, Automation and Systems, 2007, 1773-1778  
 
[7] V. Takala, & M. Pietikainen, Multi-Object Tracking 
Using Color, Texture and Motion, Computer Vision and 
Pattern Recognition, 2007, 1-7 
 
[8] I.H. Kim, D.E. Kim, Y.S. Cha, K. Lee,  & T.Y. Kuc, 
An embodiment of stereo vision system for mobile robot 
for real-time measuring distance and object tracking, 
Control, Automation and Systems, 2007, 1029-1033  
 
[9] http://www.ptgrey.com/ 
 
[10] J. Ki, Y.M. Kwon, & K.Sohn, 3D Gaze Tracking and 
Analysis for Attentive Human Computer Interaction, 
Frontiers in the Convergence of Bioscience and 
Information Technologies, 2007, 617-621 
 
[11] J. Chung & K. Shim, Color Object Tracking System 
for Interactive Entertainment Applications, International 
Conference on Acoustics, Speech and Signal Processing, 
2006 
 
[12] Y. Wang, T. Yu, L. Shi, & Z. Li; Using human body 
gestures as inputs for gaming via depth analysis, IEEE 
International Conference on Multimedia and Expo, 2008, 
993-996  
 
[13] O. Portillo-Rodriguez, O.O. Sandoval-Gonzalez, C.A.  
Avizzano, E. Ruffaldi, D. Vercelli, & M. Bergamasco, 
Development of a 3D real time gesture recognition 
methodology for virtual environment control, Robot and 
Human Interactive Communication, 2008, 279-284 
 
[14] X. Marichal, & T. Umeda, Real-Time Segmentation 
of Video Objects for Mixed-Reality Interactive 
Applications, Proc. SPIE, vol. 5150, Visual 
Communications and Image Processing, 2003, 41-50 
 
[15] M.K. Bhuyan, D. Ghosh, & P.K. Bora, Feature 
Extraction from 2D Gesture Trajectory in Dynamic Hand 
Gesture Recognition, IEEE Conference on Cybernetics 
and Intelligent Systems, 2006, 1-6  
 
[16] T. Morimoto, O. Kiriyama, Y. Harada, H. Adachi, T. 
Koide, & H.J. Mattausch, Object tracking in video 
pictures based on image segmentation and pattern 
matching, International Symposium on Circuits and 
Systems, 2005, 3215-3218 
 
[17] C.H. Huang, N.C. Tang, W.S Yeh, C.M. Hsieh, &  
Y.C. Liao, Using multiple cameras to construct 3D avatar 
from 2D video based on thinning and tracking algorithm, 
Ubi-Media Computing, 2008, 249-254 
此次會議本人發表之論文: ” A Location and Interest Based Peer to Peer Virtual 
Navigation System” 被安排在8月5日11:15~13:15的Signal Processing, pattern recognition 
and applications Section 中報告，報告及討論時間約 15 分鐘，此 Section 之報告議程如下:
August 05, 2009 
11:15~13:15 
Session Title: Signal Processing, pattern recognition and applications 
 
715- Quantitative Evaluation of Pneumoconiosis in Chest Radiographs Obtained with a 
CCD Scanner(Japan - Munehiro Nakamura) 
 
714- Adaptive System Identification Using Time-Varying Fourier Transform  
(Israel - Hadas Benisty) 
 
743- Improving the Performance of Machine Learning based Face Recognition 
Algorithm with Multiple Weighted Facial Attribute Sets (India - Sakthivel Subramaniam) 
 
583- Exploration of VLIW Space Processor for Adaboost-Based Applications  
( Algeria -Moad benkiniouar)  
 
572- Inverse Filter Order Reduction for Room Acoustics Equalization 
 (Algeria - Maamar AHFIR) 
 
790- A Location and Interest Based Peer to Peer Virtual Navigation System 
(Taiwan - CHING-SHENG WANG) 
 
765- Markerless Gesture Based Interaction for Design Review Scenarios 
 (Germany - Daniel Wickeroth) 
 
243 - Buck Regulators With or without Magnetically Coupled Filters  
( Jordan -  Igrid Khawaldeh) 
 
二、與會心得 
本人此次發表之論文提出了一套以使用者的興趣及位置為依據的同儕式(Peer to 
Peer,簡稱 P2P)網路傳輸虛擬導覽系統。由於日異龐大的場景及多媒體資料一直以來都
是建構虛擬導覽系統時的一大問題，虛擬導覽系統的設計者常常須考慮龐大的資料是否
會影響到使用者導覽時的順暢度，以及在眾多使用者同時上線使用的情況下，是否會嚴
重地降低伺服器的效能。而要解決這問題，現今大多數的系統都是使用增加伺服器或增
加網路頻寬等方式，然而這卻會增加許多的建置成本。在本論文中我們則設計出了支援
定位功能的同儕式虛擬導覽系統，本系統以使用者的位置及興趣為基礎，讓使用者能從
身旁有共同導覽興趣的同儕間，更有效率的 P2P 下載相關的虛擬場景及多媒體資料，有
效地提高導覽資料的傳輸效能。 
 
在這次的會議中，世界各地的學者在研討會中共發表了逾百篇的研究論文及成果，
會議內容相當豐富，本人與與會的學者充分的討論本人及他人之論文，獲益良多。此次
有賴國科會補助機票費、生活費及註冊費，才得以順利參加在物價昂貴的英國倫敦所舉
行之國際學術會議，受益匪淺、深表感謝。 
 
A Location and Interest Based Peer to Peer Virtual Navigation System 
 
 
Ching-Sheng Wang                     Ching-Yang Hong 
Department of Computer Science and Information Engineering 
 Aletheia University 
cswang@email.au.edu.tw          fm963851@smail.au.edu.tw 
 
 
Abstract 
 
The increasingly amount of virtual scenes and 
multimedia data have been the major problems in 
developing a virtual navigation system. Designers of 
virtual navigation system need to consider whether 
the massive data would influence the smoothness of 
navigation to users, and whether the efficiency of 
server would be decreased sharply when multiple 
users are online at the same time. In order to solve 
this problem, most of existing systems increase the 
number of servers or the network bandwidth, but the 
construction cost is also increased as a result. This 
paper designed a peer to peer (P2P) virtual 
navigation system with positioning function, this 
system is based on location and interest of users, so 
that when users carry mobile equipments such as 
notebook computer, UMPC, Mini notebook,…etc., 
walking around the actual scenes of indoor and 
outdoor areas of campus, users can peer-to-peer 
download/share virtual scenes and multimedia data 
from surrounding users or the users have the same 
navigation interest. Therefore, the transmission 
efficiency of navigation system would be improved 
obviously.  
Keywords: Virtual Reality, Navigation, Peer to 
Peer, P2P, Positioning, Location 
 
1. Introduction 
 
Virtual Reality (VR) is a virtual environment for 
simulating real scenes on computers. In VR, scenes 
can be environments familiar to users, or virtual 
spaces that not easy to experience. With a helmet-
mounted display or gloves, users can feel like as if 
they are in a real setting. In recent years, with the 
advancement of the efficiency of computers and the 
network transmission speed, the applications of VR 
have become wider. The common applications 
include navigations in computer games, virtual 
campuses, virtual museums, training simulations, 
etc.  
However, as virtual scenes become more 
exquisite and complicated, the file size also becomes 
larger and larger. For computer games, users can 
install all scenes from the CD to their computers 
previously, without the need to download massive 
data. On the contrary, the virtual navigation system 
mostly requires real-time download, so the long 
download time would affect the smoothness of 
navigation to users. Since users need to be connected 
with the server for a long time to download data, 
when multiple users are downloading files at the 
same time, more users would be influenced, and the 
server load would be increased as well.  
Aiming at the above problems, this proposed 
system applies the Voronoi Diagram Segmentation 
[1, 2, 3], and properly segments all scenes into 
independent download areas according to the 
distance of data points, and develops a mechanism 
that allows pre-download of scenes and multimedia 
data based on our outdoor GPS and indoor RFID 
positioning functions [4, 5]. This system uses the 
active RFID equipments, sets proper sensing 
thresholds by analyzing the signal intensity of active 
RFID, and judges the coordinate position of user on 
the navigation site more accurately with reference to 
the traveling path of user. With the assistance of 3D 
virtual navigation system, users can locate 
corresponding virtual scene position accurately when 
walking in a real scene on-site. Thus, besides 
downloading scenes in advance, it can remind users 
actively, so that they can receive detailed 
information of cultural relics nearby them.  
In addition, due to the rapid development of Peer 
to peer transmission in recent years, some P2P 
applications related to VR have emerged. Anthony 
Steed and Cameron Angus used the potentially 
visible set (PVS) as the basis to judge whether 
communication between peers is needed [6]. BjÖrn 
Knutsson et al. proposed a large-scale multi-player 
online game architecture for data exchange and state 
maintenance based on P2P network [7]. Shun-Yun 
Hu et al. used the Voronoi Segmentation to help 
managing users and objects state in P2P VR system 
[8]. The well-known World of Warcraft, a large-
scale multi-player online game also downloads game 
update files through P2P transmission [9]. The 
above-mentioned methods have proved the 
feasibility of combination of P2P and VR as well as 
the improvement of efficiency. This paper further 
978-1-4244-4457-1/09/$25.00 ©2009 IEEE 703
Diagram (VD) to segment the scenes, because of 
square division and hexagonal division are all equal 
segmentations, and will have a problem of cutting 
buildings if the segmentation areas of virtual scenes 
are too small, and may have too many buildings/data 
in the same area if the segmentation area is too big.  
The Voronoi Diagram is a tool for analyzing 
geometry problems, it can make segmentation of all 
objects in the Euclidean space according to their 
distances, so that each object can get an area, any 
point in the area is closer to this object than other 
objects.  
By segmenting the campus navigation scene by 
the Voronoi Diagram, the images of the overall 
buildings can remain intact, while dividing proper 
area size. Figure 1 shows the schematic diagram of 
segmenting campus scene by Voronoi Diagram. The 
red dots represent buildings and their multimedia 
information, and the black line represents the virtual 
segmentation borderline. According to this method, 
relevant campus scenes and multimedia information 
can be allocated to different areas in the figure, so as 
to solve the problem of oversize or undersize scene 
segmentation.  
 
Figure 1. Voronoi segmentation diagram of 
virtual campus scene 
 
3.2. P2P download mechanism  
 
Figure 2 shows the precedence of P2P 
transmission of the system. When user requests the 
download of virtual scene and multimedia data, 
system will check the availability of P2P 
transmission with the users in the same scene at first, 
and download the data if the request is permitted. 
Otherwise, system will check the availability of P2P 
transmission with adjacent users, users having the 
same interest, and server sequentially, or wait a 
moment to request the P2P transmission again, until 
complete the download task. 
 
 
Figure 2. Precedence of P2P transmission 
 
The virtual scene and multimedia data download 
mechanism of this system are shown in Figure 3~5. 
As shown in Figure 3, when a user, U1, wants to 
download the data of nearby scene, user U1 will 
obtain the information of users in the same area from 
the server at first, and then ask other users of the 
same scene for requested data to download. If user 
U2 has the data requested by U1, U1 and U2 can 
perform P2P data transmission directly. If there is 
data, but no bandwidth for download, user U1 will 
ask other users of the same scene for downloading 
data. Otherwise, if other users in the same scene do 
not have the data requested by U1, the system will 
ask users of different scenes or server for requested 
files. 
 
705
position indoors), and relevant multimedia 
information will be indicated at top right corner of 
the user interface. Figure 9 is the management 
interface of P2P download system, the left side is the 
area grouping information of users, and the top side 
shows the virtual scene and multimedia data being 
downloaded. 
 
 
Figure 7. Diagram of user navigation interface 
(indoors) 
 
 
Figure 8. Diagram of user navigation interface 
(outdoors)  
 
 
Figure 9. Diagram of P2P download management 
interface 
 
5. Experimental Results 
 
The equipment used in this system included: 
desktop computer (CPU: 3.0GHz, graphic card: ATI 
4870, RAM: 2G), notebook computer (CPU: 
1.83GHz, graphic card: Nvidia 8600GT, RAM: 2G), 
GPS receiver and RFID Tag/Reader etc. Two 
experiments were tested for 10 users and 20 users, 
respectively. The bandwidth of server was 
100Mbps/10Mbps, and the bandwidth of users 
included 10Mbps/2Mbps, 8Mbps/1Mbps, and 100 
Mbps /10 Mbps.  
 
Table 1.  Table of performance comparison 
(8M/1Mbps-10 users download data at the same 
time)  
 
Server only Proposed P2P mechanism 
Overall upload 
bandwidth 10Mbps 20Mbps 
Average 
download speed 1Mbps 2.3Mbps 
Average 
waiting time 329sec 133sec 
 
Table 2.  Table of performance comparison 
(8M/1Mbps-20 users download data at the same 
time)  
 
Server only Proposed P2P mechanism 
Overall upload 
bandwidth 10Mbps 30Mbps 
Average 
download speed 512Kbps 1.8Mbps 
Average 
waiting time 667sec 170sec 
 
 
 
Figure 10. Illustration of performance analysis  
 
Table 1 and 2 show the download performance 
comparisons between the proposed P2P system and 
707
