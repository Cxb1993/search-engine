computation load of processing covariance matrices is 
consequently reduced due to the smaller size of sub-
areas. Beside, image features can be easier extracted 
due to relatively larger portion in a smaller area 
for features. Further, weights have been 
appropriately assigned to features to enhance face 
recognition result. The popular ORL database will be 
used when conducting experiments. Various kinds of 
performance comparisons against related existing 
approaches will be done to show the improved 
computation load and face recognition rate of 
SA2DPCA. The better performance of SA2DPCA will make 
the approach more suitable to be applied to practical 
applications 
英文關鍵詞： 2-D principle component analysis, feature extraction, 
computation cost, recognition rate, face recognition 
alogorithm 
 
 2
 
計畫編號：NSC 100-2221-E-011-137 
執行期限：100 年 8 月 1 日至 101 年 7 月 31 日 
主持人：楊英魁   國立台灣科技大學 電機工程研究所 
 
 
一、中文摘要 
 
本研究計畫為改善人臉辨識演算法之
成效，提出了一種降低二維主成份分析法
(Two-Dimensional Principal Component 
Analysis, 2DPCA)運算量的演算法，此方法
名為次區二維主成份分析法 (Sub-Area 
Two-Dimensional Principal Component 
Analysis, SA 2DPCA)。目前基於傳統二維
主成份分析法之各種改良方法中，由於大
多數方法在特徵分解(eigen-decomposition)
的部份會需要很大的運算量(computation 
load)，因此本計畫提出改良傳統二維主成
份分析法，使用合適的分割數目，將圖片
資料分割為數區以減少運算量，並證明分
割數目的重要性，過高的分割數目會使得
特徵擷取的成效反而變差，同時因為分區
處理的關係，每一區的特徵更容易被取
得，進而獲得良好的特徵擷取成效。此法
也使用權重值(weight)的觀念，將經二維主
成份分析法處理後所得的投影特徵向量，
賦與權重值，並使用最小平均平方法來將
此權重值最佳化，因此可以得到一組最適
合用來辨識的投影特徵向量。本計畫將以
ORL 資料集為測試對象，實施各種實驗，
並與其它性質相同的現存幾種方法做比
較，期以實驗結果證實，本計畫所提之次 
區二維主成份分析法的運算量比大部份基
於傳統二維主成份分析法的方法明顯的降
低，且提升辨識的成效，這將讓此方法更
好應用在人臉辨識相關各領域上。 
 
 
關鍵詞：二維主成份分析法，特徵分解，
運算量，辨識率，人臉辨識演算法 
 
Abstract 
 
To improve the performance of face 
recognition algorithm, this project proposes 
an approach named Sub-Area 
Two-Dimensional Principal Component 
Analysis (SA2DPCA) in order to not only 
reduce the computation load but also increase 
the face recognition rate of 
Two-Dimensional Principal Component 
Analysis (2DPCA). The 2DPCA has been a 
popular approach on face recognition and 
therefore there are number of approaches 
being proposed to improve its performance. 
However, most of those approaches require 
heavy computation load to raise recognition 
rate during the process of 
eigen-decomposition that involves with 
covariance matrices computation. The 
SA2DPCA is therefore proposed to divide a 
face image into appropriate number of 
smaller areas (sub-areas) and process these 
smaller areas individually. The computation 
load of processing covariance matrices is 
consequently reduced due to the smaller size 
of sub-areas. Beside, image features can be 
 4
    在主成份分析法(PCA)用於人臉辨識
領域的多年之後，Yang 等人[15]提出了一
種名為二維主成份分析法(two-dimensional 
principal component, 2DPCA)的演算法，此
法主要的目地是應用於人臉資料的特徵擷
取(feature extraction)，其二維的特性，可以
得到降低運算量及提升辨識率的好處，此
法在辨識率和運算量這二個關鍵議題中取
得到全面性的提升[15]。因為二維主成份分
析法在人臉辨識上優異的表現，進而衍生
出各類型不同的演算法，由 Zhang 等人提
出的二維平方主成份分析法 ((2D)2PCA) 
[25]，使用的方法為將二維主成份分析法從
橫軸方向及縱軸方向進行處理，取出最短
的維度，再將所得的結果來做辨識，優點
是可降低運算量，但在一般的情況下，當
可做為訓練的資料量較多時，理論上辨識
率要比訓練資料量少的時候有所提升，在
如此的條件之下，此法與其他演算法相
比 ， 提 升 的 幅 度 卻 不 顯 著 [22] 。 由
Sanguansat 等人[26]所提出的二維主成份
分 析 法 結 合 二 維 線 性 鑑 別 分 析 法
(two-dimensional Linear discriminant 
analysis, 2DLDA)的方式來對人臉進行辨
識，優點是採取二維線性鑑別分析法可以
避免傳統線性鑑別分析法所帶來的小樣本
尺度問題[22]，以及辨識率的提升，但缺點
是因為多了一道二維線性鑑別分析法的處
理程序，所以整體的運算時間會比原先只
使用二維主成份分析法時要長。Meng 等人
[27]提出了以量化量測法(volume measure)
結合二維主成份分析法的方法，其做法為
使用二維主成份分析法進行特徵擷取，後
端的分類使用計算距離的方式，此為由自
身所定義方法來計算矩陣的量(volume)，這
個方式的好處是較適合計算高維度的資
料。由 Wang 等人[28]提出的統計二維主成
份分析法(Probabilistic 2DPCA)，使用高斯
分佈(Gaussian distribution)的概念結合二維
主成份分析法，此方式的優點是對於圖片
中的雜訊，有很好的抵制效果。Kim 等人
[29]提出了混合式雙向二維主成份分析法
(fusion method based on bidirectional 
2DPCA)，此法不只對行向量做降維，也對
列向量做降維，混合這兩部份處理的結
果，來進行後端的運算，好處是辨識率會
提升，缺點是運算量較高[30]。 
    上述所提到的人臉辨識演算法，各有
其優缺點。當二維主成份分析法被提出
後，更是強化了主成份分析法系列演算法
在人臉辨識領域裡的地位，因為二維主成
份分析法是針對人臉資料而設計的，所以
表現更勝於傳統主成份分析法。 
在基於二維主成份分析法的基礎下，
本計畫提出一套針對人臉資料特性處理的
演算法，由於矩陣維度對運算量的影響很
大，此法的精神在於對二維的人臉資料進
行分割區塊的處理，再分別將每個被分割
的小區塊進行二維主成份分析法的運算，
如此可以達到降低運算量及提升辨識率的
效果，我們將此法稱為次區二維主成份分
析法 (sub-area two dimensional principal 
component, SA2DPCA)，預計此法將不只可
以降低在二維主成份分析法的運算量，並
在辨識率上比大部份基於傳統二維主成份
分析法的方法有更好的表現。 
 
  
三、研究結果與討論 
   
 在本研究中，提出了一種降低二
維主成份分析法運算複雜度的演算法，名
為分部特徵分解之二維主成份分析法。二
維主成份分析法 是一個專為人臉辨識設
計的特徵擷取演算法，其特色為計算量不
高，但能夠擁有良好的特徵擷取成效，而
讓系統後端在進行辨識時有高辨識率的表
現，而使得此法廣泛應用於人臉辨識的領
 6
編號 3 的方法為使用二維主成份分析
法後加上二維線性鑑別分析法，所以複雜
度會比單純只使用二維主成份分析法來的
高。編號 4 為本研究所提出之分部特徵分
解之二維主成份分析法，其複雜度比單純
使用二維主成份分析法還低，因此，若有
任何一種方法，其複雜度比二維主成份分
析法的還高的話，就表示此法的複雜度比
本研究所提出之分部特徵分解之二維主成
份分析法還高。編號 5 的方法為核(Kernel)
之二維主成份分析法，此法使用將資料投
影到高維度空間處理後再轉回來低維度空
間的概念，結合二維主成份分析法使用，
但因高低維度空間轉換的過程相當耗費時
間，所以此法的運算複雜度很高，因此，
雖然編號 5 的方法比本研究所提的分部特
徵分解之二維主成份分析法的辨識率高，
但運算複雜度的表現，卻遠不如本研究所
提之法。編號 6 之法為使用二維主成份分
析 法 結 合 特 徵 混 合 法 (Feature fusion 
approach)，此法的辨識率雖高，但其運算
處理時間比二維主成份分析法高了十倍左
右之多[39]，所以此法的運算複雜度也會比
本研究所提出之分部特徵分解之二維主成
份分析法還高出許多。綜觀表 11 所列的方
法，本研究所提出之分部特徵分解之二維
主成份分析法在辨識率表現的部份，比編
號 1、編號 2 與編號 3 的方法都來的好。雖
然編號 5 與編號 6 的辨識率表現比本研究
所提之法高，但因此二種方法的運算複雜
度相當高，所以在這個部份，本研究所提
出之分部特徵分解之二維主成份分析法是
占有較佳的優勢。本研究所提之法能夠兼
顧辨識率表現及較低的運算複雜度，以整
體性的表現來看，是優於表 11 中其它五種
方法。 
 
四、未來應用方向 
 
人臉辨識被廣泛的應用在各種領域
中，特別是在應用端的產品，現今有許多
的電子產品都使用了人臉辨識的功能，因
此，本計畫的目的是希望能降低人臉辨識
的運算量，而使得產品的性能表現更佳。
在研究端的部份，則是希望藉由降低複雜
度來提升二維主成份分析法的效能，並且
期望此法不只是用在人臉辨識的領域，而
是只要在使用二維主成份分析法的情況
下，都能獲得降低運算量的好處，也就是
說從基礎理論端來提升二維主成份分析法
的效能。 
 
五、重要參考文獻 
 
 
[1] A. Z. Kouzani, F. He, and K. Sammut, 
“Towards invariant face recognition,” Inf. 
Sci., vol. 123, pp. 75–101, 2000. 
[2] K. Lam and H. Yan, “An 
analytic-to-holistic approach for face 
recognition based on a single frontal view,” 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 
20, no. 7, pp. 673–686, Jul. 1998. 
[3] M. A. Turk and A. P. Pentland, 
“Eigenfaces for recognition,” J. Cognitive 
 8
Intelligence., vol. 12, no. 1, pp. 103-108, Jan. 
1990. 
[19] A. Pujol, J. Vitria, F. Lumbreras and J. J. 
Villanueva, “Topological principal 
component analysis for face encoding and 
recognition,” Pattern Recognition Letters., 
pp. 769-776, 2001. 
[20] K. I. Kim, K. Jung, and H. J. Kim, “Face 
Recognition Using Kernel Principal 
Component Analysis,” IEEE Signal 
Processing Letters., vol. 9, no. 2, pp. 40-42, 
Feb. 2002. 
[21] H. Wang, Z. Wang, Y. Leng, X. Wu and 
Q. Li, “PCA  Plus  F-LDA: A new 
approach to face recognition,” International 
Journal of Pattern Recognition and Artificial 
Intelligence., Vol. 21, No. 6, pp. 1059–1068, 
2007. 
[22] W. H. Yang and D. Q. Dai, 
“Two-Dimensional Maximum Margin 
Feature Extraction for Face Recognition,” 
IEEE Transactions on Systems, Man and 
Cybernetics., vol. 39, no. 4, pp. 1002-1012, 
Aug, 2009. 
[23] H. Wang, Y. Leng, Z. Wang and X. Wu, 
“Application of image correction and 
bit-plane fusion in generalized PCA based 
face recognition,” Pattern Recognition 
Letters 28, pp 2352-2358, Aug. 2007. 
[24] P. C. Hsieh and P. C. Tung, “A novel 
hybrid approach based on sub-pattern 
technique and whitened PCA for face 
recognition,” Pattern Recognition 42, pp. 
978-984, 2009. 
[25] D. Zhang and Z. H. Zhoua, “(2D)^2PCA: 
Two-directional two-dimensional PCA for 
efficient face representation and 
recognition,” Neurocomputing 69, pp. 
224–231, Jun. 2005. 
[26] P. Sanguansat, W. Asdornwised, S. 
Jitapunkul and S. Marukatat, 
“Tow-dimensional linear discriminant 
analysis of principle component vectors for 
face recognition,” IEEE., pp. 345-348, 2006. 
[27] J. Meng and W. Zhang, “Volume 
measure in 2DPCA-based face recognition,” 
Pattern Recognition Letters 28, pp. 
1203–1208, Jan. 2007. 
[28] H. Wang, S. Chen, Z. Hu and B. Luo, 
“Probabilistic two-dimensional principal 
component analysis and its mixture model 
for face recognition,” Springer Neural 
Comput & Applic 17, pp. 541–547, 2008. 
[29] Y. G. Kim, Y. J. Song, U. D. Chang, D. 
W. Kim, T. S. Yun and J. H. Ahn, “Face 
recognition using a fusion method based on 
bidirectional 2DPCA,” Applied Mathematics 
and Computation., vol. 205, pp. 601–607, 
2008. 
[30] Y. Qi and J. Zhang, “(2D)2PCALDA: 
An efficient approach for face recognition,” 
Applied Mathematics and Computation., vol. 
213(1), pp. 1-7, Jul. 2009. 
[31] K. L. Yeh, Principal Component 
Analysis with Missing Data [Online], pp. 
1-12, Jun. 2006. Available:   
http://www.cmlab.csie.ntu.edu.tw/~cyy/learn
ing/tutorials/PCAMissingData.pdf 
[32] 
http://episte.math.ntu.edu.tw/entries/en_lagra
nge_mul/index.html 
[33] 
http://zh.wikipedia.org/zh-tw/%E6%8B%89
%E6%A0%BC%E6%9C%97%E6%97%A5
%E4%B9%98%E6%95%B0 
[34] S. J. Leon, Linear algebra with 
applications, seventh ed. Prentice Hall, 2006.  
[35] A. M. Martinez, and A. C. Kak, “PCA 
 10
 
 2
IPCV 2012 Conference - decision on your paper (IPC2842) 
Tuesday, April 24, 2012 3:23 PM 
From: 
"Steering Committee" <sc@world-comp.org> 
Add sender to Contacts 
To: 
yingkyang@yahoo.com 
Cc: 
yingkyang@yahoo.com 
Dear Dr. Ying-Kuei Yang:  
 
I am pleased to inform you that the following paper which you submitted to: 
The 16th International Conference on Image Processing, Computer Vision, & Pattern 
Recognition 
(IPCV'12: July 16-19, 2012, USA) has been accepted as a Regular Research 
Paper (RRP) - ie, accepted for both, publication in the proceedings and oral 
formal presentation.  Please see below for the categories of accepted papers. 
 
Paper ID #: IPC2842 
Title: SI2DPCA: A Low-Computation Face Recognition Approach 
Author(s): Ying-Kuei Yang, Wei-Li Fang and Jung-Kuei Pan 
 
Note: The "paper ID #" shown above is composed of three letters 
(conference prefix) followed by four numeral/digits. You will need to 
have this "Paper ID #" at the time of registration and final paper 
submission (for publication). 
 
(The evaluation of this paper is arranged by Track Chair: 247) 
 4
Wang et al. [16] proposed “probabilistic two-dimensional principal component analysis” that 
combines 2DPCA with Gaussian distribution concept to mitigate the noise influence in face image 
recognition. Kim et al. [17] proposed “fusion method based on bidirectional 2DPCA” that reduces 
dimensions of both row and column vectors before performing face recognition procedure. It does 
increase recognition rate, but at the expense of high computation cost [18]. 
    Aforesaid face recognition algorithms based on 2DPCA have tried to either increasing 
recognition rate or reducing computation cost, but not both. In this paper, an approach named 
sub-image 2-dimensional principal component analysis (SI2DPCA) is proposed hoping to achieve not 
only reducing the computation but also increasing the recognition rate in face image recognition 
applications. Unlike conventional 2DPCA, the SI2DPCA divides a whole face image into smaller 
sub-images so that features can be better recognized and extracted. At the same time, computation cost 
can also be reduced due to smaller size of sub-images.  
 
2. The sub-image 2-dimensional principal component analysis 
 
2.1 Two-dimensional principal component analysis (2DPCA) 
 
    The 2DPCA approach by Yang et al. [11] in 2004 is proposed particularly for two dimensional 
image data. Suppose there is an image data set Z={A1, A2,…, AN } with N images, and the dimension 
of every image is n×n. The covariance matrix of the image data set is computed by Eq. (1) and the 
average value of the data set is computed by Eq. (2). 
1
1 ( )( )
N
T
i i
iN =
= − −∑R A A A A                   (1) 
1
1 N
i
iN =
= ∑A A                       (2) 
where Ai is an image in the data set, R is covariance matrix, and A  is data average. 
After eigen-decomposition is performed for covariance matrix, k eigenvectors corresponding to 
the k biggest eigenvalues are selected. These eigenvectors are the projection vectors of the original 
image data set and the features of the image can therefore be extracted from those projection vectors as 
shown in Eq. (3). 
i i=Y AX       i=1,2,…,k              (3) 
where Yi are projected feature vectors, Xi means eigenvectors. Suppose there are k biggest eigenvalues 
being selected, then a feature vector set B=[Y1 ,Y2 ,…,Yk ] in descending order of eigenvalues can be 
obtained and these projected feature vectors are the resultant principal components of an original 
image data A by 2DPCA. 
 
2.2 Sub-image 2-dimensional principal component (SI2DPCA) 
 6
m m
2 2
m m m m m m
2 2 2 2 2 2
m m m m m m
2 2 2 2 2 2
m m
2 2
11 12 1m1 1( 1)
21 22
1 ( 1) m
(( 1)1 ( 1) ( 1)( 1) 1) m
m1 mmm m( 1)
0
a a a a a
a a
a a a a
a a a a
a a a a
λ
λ
λ
λ
λ
+
+
+ + + + +
+
−
−
−
=−
−
L L L
O M M O M
M M O M M O M
L L L L
L L L L
M O M M O M
M O M M O M
L L L L
     (6) 
Because the time complexity of computing a determinant is O(n!), the total computation cost of 
computing the determinants for each of smaller square matrices in Eq. (6) is much less than the 
computation cost for Eq. (5). From the final equation that is set to be zero, several λ values can be 
obtained. The eigenvectors can consequently be calculated by substituting λ values into its 
corresponding matrix in Eq. (6). The eigenvector that is based on the largest λ value is the most 
important feature. The eigenvector based on the second largest λ value is the second important feature, 
and so on.  
    Because current computers are mostly binary-based systems, ill condition problem that gives 
incorrect result could be caused when computing eigen-decomposition [19]. To avoid such problem, 
many studies use singular value decomposition (SVD) to replace the process of eigen-decomposition. 
For a matrix with dimension m×n, the computation cost of SVD can be described as Eq. (7) [20]. 
4m2n + 8mn2 + 9n3                    (7) 
The big-order of (15) is O(n3) when m < n, meaning the dimension variation causes significant 
difference in terms of computation cost. This infers that the idea of working on several smaller 
matrices rather than one original larger matrix by SI2DPCA can lower computation cost when 
performing eigen-decomposition process. 
The comparison of computation cost between the conventional 2DPCA and the proposed 
SI2DPCA is shown in Table 1 that details the formulas of computation cost. In Table 1, the formula (a) 
that comes from Eq. (2) is to compute the data average by 2DPCA. The N means computation 
summation of N images. The computation amount of m×n in Formula (a) is for matrix addition. 
Adding one in formula (a) is for the division operation in Eq. (2). Formula (b) is from Eq. (1) to 
compute the covariance matrix. The N and the plus-one have same meaning as in formula (a). There 
are three operations in Eq. (1). The first one is subtracting the data average obtained by formula (a) 
from the original image data, causing m×n computation. There are two such operations in Eq. (1), so 
the computation cost is 2×m×n. The second operation is the multiplication of the two matrices shown 
in Eq. (1). The computation cost of such matrix multiplication is n3. The third operation is the 
computation of transposing a matrix, causing m×n computation. Formula (c) comes from Eq. (7) and 
is the computation cost of eigen-decomposition for 2DPCA. 
The rest of formulas in Table 1 are for SI2DPCA. Suppose an original image with dimension 
m×n is equally divided into k smaller images, where k can be square-rooted and m and n are times of 
 8
goal is to perform face image recognition. That is, hoping the proposed SI2DPCA does not improve its 
computation cost at the expense of recognition performance. 
In SI2DPCA, after an original face image is equally divided into several smaller sub-images, each 
of the sub-images is processed individually for feature extraction. Because the size of a sub-image is 
smaller, any important features existing in this sub-image can be easier to be found and therefore to be 
extracted. For example, a sub-image may include only features of eyes and hair, and these features and 
their detailed textures would then be so obvious to be recognized and extracted in this relatively small 
image. On the other hand, a whole image includes not only eyes and hair but also many other features. 
In this situation, the features of eyes and hair may not be so outstanding in such immense image data 
and therefore can not be easily recognized. Even these two features have been recognized, their 
weights in its image may not be as great as those extracted from smaller sub-images because of the 
co-existence of other features in the whole bigger image.  
 
3. Experiments and analysis 
3.1 The ORL database 
 
The ORL database [21] is a well-known face image database and is used in this paper for 
experiments. There are 40 individual faces in ORL database. Each individual face has 10 different 
images making totally 400 face images in the database. The images were taken with a tolerance of 
some tilting and rotation of the face for up to 20 degrees [11][21]. In ORL database, all images are 
grayscale with dimension of 112×92. The pixel value range is 0~255.    
 
3.2 Experiments and analysis of SI2DPCA 
    According to Table 1, the computation cost of SI2DPCA and 2DPCA can be calculated for the 
images in ORL database. Every image has dimension of 112×92, meaning m and n in Table 1 are 112 
and 92 respectively. And each image is divided into 4 smaller sub-images, meaning the value of k in 
Table 1 is 4. Suppose 200 images are taken as training data, meaning N in Table 1 is 200, and 8 
features are selected and extracted. Putting these values into Table 1, the result is shown in Table 3. 
    Table 3 shows that the computation cost for SI2DPCA is only half of 2DPCA. When calculating 
covariance matrix and eigen-decomposition, there are many quadratic or cubic power computations. 
Smaller image dimensions operated in SI2DPCA can greatly reduce the computation cost, as discussed 
previously 
       
Table 3: Analysis of computation cost for ORL database 
Computation type 2DPCA SI2DPCA 
 10
SI2DPCA. This is because SI2DPCA operates against matrices in smaller dimensions. For methods (4), 
(5) and (6) in Table 5, they even put additional processes to 2DPCA. Method 5 combines 2DPCA with 
Kernel algorithm. This approach projects image data to high dimensional space, causing high 
computation cost. Although its recognition rate is slightly better than the proposed SI2DPCA, the 
much higher computation cost makes it difficult for practical applications. Method 6 combines feature 
fusion with 2DPCA in order to increase recognition rate. The resultant recognition rate is very good at 
98.1% that is better than the rate of 93.5% by SI2DPCA in the experiment. Unfortunately, the 
computation cost of this approach is so high, at least 10 times higher than 2DPCA, that it is impossible 
to be applied to any practical applications.  
 
 
Table 5: Comparison among other methods and SI2DPCA 
Metho
d 
number 
Method Recognitio
n 
rate 
Computation 
cost 
1 (2D)2PCA [12] 90.5% high 
2 2DPCA+Fusion method based on 
bidirectional [17] 
92.5% high 
3 2DPCA+2DLDA [14] 93.5% high 
4 SI2DPCA (proposed) 93.5% low 
5 2DPCA+Kernel [22] 94.58% very high 
6 2DPCA+Feature fusion approach [23] 98.1% very very high 
 
4. Conclusion 
The feature extraction algorithm 2DPCA is specially developed for face recognition. Its 
characteristics are low computation cost and good feature extraction, making 2DPCA a popular 
approach for face recognition. In this paper, an enhanced approach “SI2DPCA” is proposed to operate 
at even lower computation cost without compromising its good recognition performance. Both of the 
two goals of reducing computation cost and maintaining good recognition rate have been shown in the 
results of the conducted experiments in this paper.  
References 
[1] A. Z. Kouzani, F. He, and K. Sammut, “Towards invariant face recognition,” Inf. Sci., vol. 123, pp. 
75–101, 2000. 
[2] M. J. Er, S. Wu, J. Lu, and H. L. Toh, “Face recognition with radial basis function RBF neural 
networks,” IEEE Trans. Neural Netw., vol. 13, no. 3, pp. 697–710, May 2002. 
[3] F. Yang and M. Paindavoine, “Implementation of an RBF neural network on embedded systems: 
 12
205, pp. 601–607, 2008. 
[18] Y. Qi and J. Zhang, “(2D)2PCALDA: An efficient approach for face recognition,” Applied 
Mathematics and Computation., vol. 213(1), pp. 1-7, Jul. 2009. 
[19] S. J. Leon, Linear algebra with applications, seventh ed. Prentice Hall, 2006.  
[20] 3D Computer Vision Winter Term 2005/06, Nassir Navab 
[21]“The ORL face database”, http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.htm 
[22] N. Sun, H. X. Wang, Z. H. Ji, C. R. Zou and L. Zhao, “An efficient algorithm for Kernel 
two-dimensional principal component analysis,” Neural Comput & Applic., 17, pp. 59-64, 2008. 
[23] Y. Xu, D. Zhang, J. Yang and J. Y. Yang, “An approach for directly extracting features from 
matrix data and its application in face recognition,” Neurocomputing, 71, pp. 1857-1865, Feb, 2008. 
 
 
 
 
 14
 
A Fuzzy-Reasoning Radial Basis Function Neural Network with Reinforcement 
Learning Method 
Ying-Kuei Yang, Jin-Yu Lin, Wei-Li Fang and Jung-Kuei Pan 
Dept. of Electrical Engineering, National Taiwan University of Science & Technology,  
Taipei, TAIWAN 
e-mail: yingkyang@yahoo.com 
 
Abstract 
   
The performance of rule-based fuzzy systems primarily relies on its rules and membership functions. 
Some researchers have proposed a self-construct rule-based fuzzy system based on reinforcement 
learning, which often results in a 5-layer neural network. The Fuzzy-reasoning Radial Basis Function 
Neural Network (FRBFN) proposed in this paper has only 3 layers reduce its forwarding calculation 
time. The reasoning time is reduced by eliminating fuzzification and defuzzification in the learning 
system. The membership functions of a rule are finely tuned through learning procedure. The Radial 
Basis Function Neural Network (RBFN) is employed in FRBFN to offer the generality and 
smoothness of the network. The result of experiment has shown that our proposed network has the 
ability to well train a rule-based fuzzy system through reinforcement learning with god performance.  
 
Keywords: fuzzy systems, radial basis function neural network, reinforcement learning, supervised 
learning 
 
1. Motivation 
 The rule-based fuzzy systems have been successfully used in many real-world control 
applications [1]. Many researchers have devoted research on constructing rules and membership 
functions by system itself through learning [2][3][4][5]. Supervised learning requires sample data 
sufficiently large enough to learn. The use of reinforcement learning is to eliminate this requirement. 
Reinforcement learning method is based on a trial-and-error method and therefore no precise training 
pair is required [6][7][8][9][10]. 
 Most researchers have considered a fuzzy system as a five layers neural network. This selection is 
to map input variables, input membership functions, rule base, output membership functions and 
output variables to each layer of neural network respectively. The use of five layers has created a 
 16
reinforcement signals are used as a criteria to update learning parameters in ASN. The values of these 
parameters are learned and consequently generated through the signals of state variables s(t) and 
internal reinforcement signal )(ˆ tr .  
 
Figure 2.   Block diagram of Action Selection Network (ASN) 
 
The output of ASN does not directly apply to the controlled plant. Instead, the output y(t) of ASN 
is first sent to the Stochastic Action Modifier (SAM). The SAM then tries to optimize y(t) to generate 
a modified output )(ˆ ty  based on the predicted reinforcement signal )(tp  at that time and its 
stochastic algorithm.     
 ))(),(()(ˆ ttyNty σ=         (1) 
where N is a normal or Gaussian distribution function with mean )(ty  and variance )(tσ . A small 
value of )(tσ  indicates the system is closer to a stable situation. For our learning algorithm, we 
choose probability function )(tpσ  as 
)(21
1)( tpp e
t +=σ         (2) 
where )(tp  is generated by ACN and is the signal used to predict reinforcement signal )(tr .  
 
2.3  Function of ACN  
 
The function of an ACN shown in Figure 1 is to generate signals of )(tp  and )(ˆ tr  by receiving 
state variables and external reinforcement signals. The output signal )(tp  is a prediction signal and 
)(ˆ tr  is an internal reinforcement signal. The purpose of )(tp  is to predict the infinite discounted 
prediction signal )(tz and )(ˆ tr  is the difference between )(tp  and )(tz . The objective of ACN is to 
model the environment such that it can perform a multi-step prediction of reinforcement signal for the 
current action chosen by ASN. Using a Radial Basis Function Network (RBFN) is proposed to 
implement an ACN, because a RBFN has the ability to approximate the real-valued mapping of 
continuous or piecewisely continuous function. Another reason to use RBFN is for total network 
architecture of FRBFN. If both the ASN and ACN use the same neural network architecture, some 
 18
2.4  Learning in Action Selection Network 
 
The parameters that need to be adjusted by learning mechanism are centers and widths in nodes 
of hidden layer and the weights between hidden and output layers. One of the goals of the ASN is to 
have output that maximizes the reinforcement signal )(tr . The learning relation can be written as: 
i
i m
rm ∂
∂∝Δ        (3) 
where im  denotes a parameter to be adjusted and imΔ  is the amount of adjustment applied to the 
parameter im  stands for in ASN. The relation indicates that the amount of adjustment to a parameter 
is proportional to the strength of its associated reinforcement signal. Using chaining rule to expand the 
right part of equation (3), we have 
ii m
y
y
r
m
r
∂
∂⋅∂
∂=∂
∂         (4)  
where y is the output of the ASN and 
im
y
∂
∂  denotes the amount y is changed when adjustment 
parameter im  changes. The equation (4) shows y
r
∂
∂  and 
im
y
∂
∂  are required in order to determine 
im
r
∂
∂ . The term 
im
y
∂
∂  is the gradient information for output regarding to its operational parameter and 
can be derived easily. But the reinforcement signal is not generated in every time step. The gradient 
value of 
y
r
∂
∂ can only be estimated. In our learning algorithm, we use stochastic exploration method 
[13] to estimate gradient information
y
r
∂
∂ . For multi-step prediction, the gradient information 
y
r
∂
∂  is 
estimated as 
  [ ]
11
11
ˆ
)(ˆ
ˆ
−−
−− ⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅=⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅−≈∂
∂
tptp
tt
yytryypz
y
r
σσ          (5) 
 The estimation of 
y
r
∂
∂  is defined in equation (5), where zt-1 is infinite discounted cumulated 
outcome. 
A general network architecture of ASN based on RBFN is shown in Figure 5.  
 20
2.6  Learning for centers in hidden layer of ASN  
 
The kernel function of hidden nodes in FRBFN is Gaussian function. There are two parameters, 
width and center, should be adjusted in hidden nodes. The interested variable is center v in equation 
(6). Because a hidden node in FRBFN is a multi-dimensional node, the output of a hidden node is a 
non-linear transformation based on Euclidean distance. Therefore, only the calculation of the 
Euclidean norm between input state and its center is required for multi-linguistic variable cases to 
perform Gaussian transformation.  
 Since the interested variable is v, by equation (4), the center parameters can be adjusted with 
respect to each individual dimension. 
jiji
y
y
rr
,, ν∂
∂⋅∂
∂=ν∂
∂              (11) 
where i stands for the i-th rule node and j stands for the j-th dimension. Referring to equation (5) and 
(6),  
∑∑
==
⋅−−−−−−−==
Q
i
ij
ni
nini
i
ii
i
ii
Q
i
ijij w
vxvxvx
wy
1
2
,
2
,,
2
2,
2
2,2,
2
1,
2
1,1,
1
)
2
)(
2
)(
2
)(
exp( σσσϕ L     
the center parameter can be adjusted in every dimension by 
)1(
)(
)1(
ˆ
)(ˆ
0
)1(
)(
0)
2
)(
2
)(
exp(
2
)(
2
)(
2
)(
)
2
)(
2
)(
exp(
)
2
)(
2
)(
exp(
2
,
1,,
1
2
,
1,,
2
,
2
,,
2
1,
2
1,1,
2
,
2
,,
2
,
2
,,
2
1,
2
1,1,
,
2
,
2
,,
2
1,
2
1,1,
2
,
2
,,
2
1,
2
1,1,
,,,
−
−⋅−⋅⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅=
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −−
−+−⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛ ⋅−−−−−⋅∂
∂=
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −−−−−−−∂
∂⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛ ⋅−−−−−⋅∂
∂=
⋅−−−−−∂
∂⋅∂
∂=∂
∂⋅∂
∂=∂
∂
−
−
−∑
∑
∑
t
vx
tyytr
t
vx
w
vxvx
y
r
vxvxvx
w
vxvx
y
r
w
vxvx
y
ry
y
rr
ji
tjiji
i
tp
ji
tjiji
i
ij
ni
nini
i
ii
ni
nini
ji
jiji
i
ii
jii
ij
ni
nini
i
ii
i
ij
ni
nini
i
ii
jijiji
σϕσ
σσσ
σσσνσσ
σσννν
LLL
LLL
L
 (12) 
  
      a      b       c 
    a: estimated gradient  
b: to decide effectiveness 
c: to decide direction & amount 
Part a is an estimated gradient information, as discussed in equation (5). Part b is one of the factors 
affects the adjustment amount. In part c, the nominator decides the direction and amount of adjustment. 
According to equation (3) and equation (12), the learning rule for center in j-th dimension of i-th 
output node is 
)1(
)1(
)(ˆ
)(ˆ 2
,
1,,
1,
, −⋅−
−⋅⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅⋅=∂
∂⋅=Δ −
−
t
t
vxyytr
v
rv i
ji
tjiji
tp
ASN
ji
ASNji vv
ϕσσαα      (13) 
where 
wASN
α  is a positive learning rate for weight in ASN. So, the center vij(t+1) needs to be adjusted 
 22
adjusted by. 
 
( )
3
,
2
3
,
2
2
,
2
,,
2
1,
2
1,1,
2
,
2
,,
2
,
2
,,
2
1,
2
1,1,
,
2
,
2
,,
2
1,
2
1,1,
2
,
2
,,
2
1,
2
1,1,
,,,
00)
2
)(
2
)(
exp(
2
)(
2
)(
2
)(
)
2
)(
2
)(
exp(
)
2
)(
2
)(
exp(
−
−
⋅−⋅⋅⋅∂
∂=
−−⋅−+−−⋅⋅−−−−−⋅∂
∂=
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −−−−−−−∂
∂⋅⋅−−−−−⋅∂
∂=
⋅−−−−−∂
∂⋅∂
∂=∂
∂⋅∂
∂=∂
∂
∑
∑
∑
jiiiiji
jiii
i
ij
ni
nini
i
ii
ni
nini
ji
jiji
i
ii
jii
ij
ni
nini
i
ii
i
ij
ni
nini
i
ii
jijiji
vxw
y
r
vxw
vxvx
y
r
vxvxvx
w
vxvx
y
r
w
vxvx
y
ry
y
rr
σϕ
σσσ
σσσσσσ
σσσσσ
vv
LvvLL
LLL
L
   
( )
1
3
,
2
1
ˆ
)(ˆ −
−
−
⋅−⋅⋅⋅⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅=
tjiiiii
tp
vxwyytr σϕσ
vv            (19) 
This is the amount need to be adjusted for the i-th node in dimension j. According to equation (3) 
and equation (19), the learning rule for width at i-th hidden node is  
( )
1
3
,
2
1,
,
ˆ
)(ˆ −
−
−
⋅−⋅⋅⋅⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅⋅=∂
∂⋅=Δ
tjiiiii
tp
ASN
ji
ASNji vxw
yytrr σϕσασασ σσ
vv    (20) 
where σα ASN  is a positive learning rate for a width in ASN. The width in each dimension )1(, +tjiσ  
needs to be adjusted by ji,σΔ , that is 
( )
1
3
,
2
1
,,,,
ˆ
)(ˆ)()()1( −
−
−
⋅−⋅⋅⋅⎥⎥⎦
⎤
⎢⎢⎣
⎡ −⋅⋅+=Δ+=+
tjiiiii
tp
ASNjijijiji vxw
yytrttt σϕσασσσσ σ
vv  (21) 
 
3.  Conclusion 
 The proposed Fuzzy-reasoning Radial Basis Function Neural Network has been applied to 
control a cart-pole balancing system and the result has shown our proposed system works better. The 
experiment is not discussed detailed here due to space limit. 
 The contribution of this paper can be summarized as: 
1. The Fuzzy-Reasoning Radial Basis Function Neural Network (FRBFN) is proposed to 
incorporate the reinforcement learning method with a neural network. 
2. Because of the corresponding equivalence between a fuzzy system and a radial basis function 
neural network, the proposed FRBFN has only three layers due to the elimination of 
fuzzification and defuzzification functions. This feature significantly reduces the computation 
cost. 
3. The suggestion of combining together the input and hidden layers of ASN and ACN, which 
simplifies the proposed FRBFN architecture. 
4. The formula of adjusting parameters in ASN and ACN are derived for the proposed 
architecture. 
 
References 
1. C. T. Lin and C.S. Lee, “Neural Fuzzy Systems: a Neuro-Fuzzy Synergism  to Intelligent 
Systems”, Prentice Hall , 1996. 
2. T. Watanabe, “A study on multi-agent reinforcement learning problem based on hierarchical 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/12/08
國科會補助計畫
計畫名稱: 人臉辨識法之運算量與辨識率之研究
計畫主持人: 楊英魁
計畫編號: 100-2221-E-011-137- 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
