 2
background noise. Consequently, a robust 
binocular tracking system with auditory 
perception has been developed to enhance 
target tracking. This control architecture not 
only can be used in security systems and used 
in home robots to improve the way in which 
they interact with humans. 
Keywords: omni-directional wheeled robot, 
stereo vision, auditory perception, robotic 
binocular head, cepstrum lifter 
????????? 
With advances in robotic vision, visual 
target tracking remains constrained by 
difficulties in tracking invisible targets: 
designing an image tracking algorithm to 
search for a target that is not in the image 
plane is difficult. If, when the tracking system 
starts, the target is not in the image plane, or it 
is covered by objects in the working space, 
“target disappearance” may occur and much 
time will be spent searching for the target in 
the work space, causing the visual tracking to 
become unstable. This study develops an 
auditory perception technique to improve 
overall tracking performance. When the target 
vanishes in the image plane, the auditory 
perception system is used to track the target’s 
sound source and cause the robotic head to 
face the target. When the head faces the target, 
its image appears in the image plane to ensure 
that the visual tracking system is working. 
Some recent studies about visual tracking, 
such as those of Chen [1] and Wang [4], have 
combined pattern matching and contour 
matching to increase the robustness of visual 
tracking systems. 
In 1995, Koichi Yanagisawa [5] et al. 
constructed an equilateral triangle microphone 
array to track a whistle. In 2002, Chen [2] 
realized such a sound tracking system by 
designing IC circuits and using digital signal 
processors. He placed this sound source 
tracking system on an autonomous guided 
vehicle to enable it to track a whistle. 
While combining auditory perception 
with visual tracking, Hu and Su [3] developed 
a self-calibrated speaker tracking system based 
on both audio and video information. They set 
an array of equilateral triangle microphones on 
a pan-tile active camera to track a speaker. The 
Japanese company, NEC, researched and 
designed a small, intelligent robot-R100 [6] 
with a height of 44cm and a weight of 7.9Kg. 
It has two cameras, three microphones, many 
force sensors and ultrasonic sensors. NEC 
began to plan the R100 in 1997. The 
robot-R100 combines pronunciation 
recognition, image recognition and an 
electromechanical system to track and 
recognize specific people. R100 can 
understand hundred Japanese characters. This 
robot responds appropriately, enabling simple 
interactions with humans. 
In this study, a simple but fast 
image-tracking algorithm – Edge-Blobs 
pattern and contour matching is proposed for 
visual tracking. A color space target model is 
constructed and color space patterns and 
contours matched to filter out background 
noise. In auditory perception, an equilateral 
triangular array of microphones is used to 
track the sound source and perform simple 
voice recognition. The sound source-tracking 
 4
times. The following equations are used.   
360( )cos( )
 ; : 0,1,2... ,  : 0,1,2...        (1)
360( )sin( )
where
( , ) :  the coordinates of the points of the ellipse 
:  the shift pixels of the ellipse conv
m
n c
m
n c
m m
n n
u u a sm n
N n N m M
v v b sm n
N
u v m
s
 
= + −    
= + −  
ergent
:  the number of the pixels of a ellipse
:  the time of the ellipse convergent
N
M
 Equation (1) is used to implement the 
algorithm as Fig. 4, when the points 
( , )m mn nu v of ellipse m do not touch the edge of 
the objects in the image, make it convergent 
continuously to the next ellipse m+1 and 
calculate its’ points as 1 1( , )m mn nu v
+ + and so as that 
until the point of ellipse’s points contact with 
the edge of objects, then the initial contour of 
the target could be got, the ( , )contour contourn nu v are 
the positions of the initial contour points. The 
contour points are stored and the COG of the 
ellipse is used to establish the contour model 
as required for contour matching: 
; : 0,1... , : [0, ]                                             (2)
( , ) :  the distances from the contors' coordinate to the ellipse's center 
d contour
n n c
d contour
n n c
d d
n n
u u u
n N m M
v v v
u v
 = −  
= −  
 For an image that has been binarized and 
undergone edge detection, contour matching 
must be performed for each position of the 
blobs’ COG and find the number of the 
contour points with a value of 255 (white edge) 
is determined. More matched points 
correspond to great similarity between the 
objects and the target. The shape of a target 
changes as time passes, so the tracking 
algorithm is enhanced by refreshing the 
contour model. 
When an object is chosen as the target 
after contour matching, the smallest rectangle 
as shown in Fig. 2, that can contain all of this 
object’s blob pixels is identified, and long and 
short axes of the outer rectangle are shifted, as 
displayed in Fig. 3, such that they become the 
new ellipse’s major and minor axis. The ellipse 
converges until is comes into with the object’s 
edge; it then forms the new contour model. 
This method is called the contour 
matching method using blob information. The 
procedure is just like the edge-blobs pattern 
matching method. Contour model refresh is 
applied to the target to yield the basis of the 
subsequent contour matching. Accordingly, 
contour image tracking is performed. 
3.2.3 YCbCr Space Image Tracking 
 The algorithm combines Edge-Blobs 
Pattern with Edge-Blobs Contour, such that it 
combines the inner information of the target 
with the outer information. This algorithm 
incorporates the YCbCr color space into this 
algorithm, as shown in Fig. 5. This cue method 
outperforms pattern matching alone, contour 
matching or gray level space image because it 
finds not only the degree of matching between 
the inner information of the target but also 
finds them between the outer information, and 
the YCbCr space is used to filter the objects 
whose colors differ from that of the target. 
This improves performance when the 
background is complex, and reduces the 
processing time because edge-blobs pattern 
matching and edge-blobs contour matching 
have few blobs than gray level matching. 
 6
source tracking is thus completed. 
3.4 Binocular Tracking with  
Auditory Perception 
3.4.1 Hardware and Control Architecture 
The target-tracking system, presented in 
Fig. 7, is a five-axes robotic binocular head 
with an equilateral triangular array of 
microphones, which has sides of 28.5cm. The 
overall target-tracking system displayed in Fig. 
8 combines the sound source tracking control 
system with the visual tracking control system. 
PC1, as presented in Fig. 8, tracks the sound 
source and obtains the motors’ encoder signals 
and the image tracking commands; finally, it 
sends suitable motor commands to keep the 
target in the centers of image planes. 
When the tracking system starts, the first 
step of the tracking algorithm searches the 
image target in both of the CCD image planes. 
If the image target is present, then the second 
to fifth axes of the motors maintain the image 
target in the center of the image plane; 
otherwise, the sound source is tracked until the 
target appears in one or both of the CCD 
image planes. Fig. 9 shows the overall tracking 
algorithm. 
3.4.2 Experiments 
The experiment concerns binocular 
tracking with the auditory perception system 
under complicated and semi-covered 
circumstances. Fig. 10 depicts the 
experimental setup. A cover plate is placed to 
cover the left-half path of the target. The goal 
of this experiment is to test the robustness of 
the overall tracking system, as the speed and 
the direction of the covered target changes. All 
of the dashed and dotted-solid lines in Figs. 11 
to 15 represent the target’s trajectory, and the 
position of the sound source as determined by 
the auditory perception system. The target is 
covered from 0 cm to 20 cm along the sliding 
rail, so a tracking error of around 10cm exists 
from time 0sec to 15sec. From 15sec to 20sec, 
the target accelerates. Figs. 11 to 13 (b) show 
the detail tracking performance. The 
experimental results in Figs. 11 to 13 display 
the robotic binocular head tracking 
performance. Figs. 14 and 15 plot the image 
tracking performance. 
??????? 
An omni-directional wheeled mobile 
robot has been constructed and a robust 
binocular tracking system with an auditory 
perception system mounted on the top of the 
robot has also been developed. The advantages 
of this tracking system are as follows. 
1. An auditory perception system is formed 
from an equilateral triangular array of 
microphones. These simple and cheap sensors 
constitute an economic but practical sound 
source tracking system. (Binocular target 
searching and tracking can be added when 
vision is inactive.) 
2. A more efficient image tracking algorithm is 
proposed, based on blob determination. The 
edge-blobs pattern matching method 
distinguishes the internal characteristics of the 
target from the other background objects, and 
edge-blobs contour matching distinguishes the 
target’s external characteristics. These two 
new methods reduce the number of matching 
below that associated with full-range searching 
 8
Max rectangular long axis
M
ax
 re
ct
an
gu
la
r s
ho
rt 
ax
is
Blob
Shift u
Sh
ift
 v
Max ellipse long axis
M
ax
 e
lli
ps
e 
sh
or
t a
xi
s
The contact point
Fig. 3 The convergent ellipse 
start
Target Model
Binary
&
Edge Detection
Morphological
Processing
Find the Cog
of the Blob
Get the 
Initial Contour
360( )cos( )
360( )sin( )
m
n c
m
n c
u u a sm n
N
v v b sm n
N
= + −
= + −
( , ) 255m mn nIf u v =
contour m
n n
contour m
n n
u u
V v
=
=
No
Yes
m=m+1
 
Fig. 4 Procedure of making initial contour 
 
Start
Grab Image
&
YCbCr Conversion
Image
Processing
(Binary, Edge, Mor. )
COG
of 
Each Blob
Pattern
Match
Contour
Match
PMErr CMErr
MErr = w(PMErr)+(1-w)CMErr 
If MErr < 
threshold
Pattern 
& 
Contour 
Model Refresh
Target 
Detected
Pattern
&
Contour
Model
Initial 
Model
Making
No Target
YES
NO
 
Fig. 5 Image tracking algorithm 
Mic.0
Mic.2
Mic.1
VII
II V
III IV
0o
60o
120o
180o
-120o
-60o
 
Fig. 6 Array of Microphones 
Table 1 Pan-direction rules 
 
 10
 
Fig. 12 Right eye optical axis trajectory 
 
Fig. 13 Left eye optical axis trajectory 
 
Fig. 14 Right Image tracking error 
 
Fig. 15 Left Image tracking error 
 
 
 
 
表 Y04 2
?????? 
??????????????????????????????????
????????Fusion of Human and Robot Intelligence in our Cyber Society???
????????????????????????????????????
???????????????????????????????????
??????????????????????????????????????
????????????????????????????????????
?????? 
 
????????(????????) 
? 
 
???? 
????????????????????? 
1. ???????????????????????? IROS 2006?????
???????????? 
2. ??????????????????????????? 
 
??????????? 
?????????? 
 
???? 
???????????????????????????????????
 
 
 
 
 
 
 
 
 
 
 
 
gearheads through timing belts, and the last degree of freedom 
is driven directly by a DC servo motor with a planetary 
gearhead. 
B. Design of Robot Arms and Hands 
Each developed robot arm consists of three joints 
(shoulder, elbow, wrist). The shoulder and wrist are spherical 
joints with roll-pitch-yaw equivalents. The shoulder provides 
gross singularity-free movement ( 270D  pitch, 120D  yaw and 
180D  roll). The elbow provides a finer 90D  of pitch motion that 
complements the shoulder’s movement. The wrist provides 
produces small singularity-free movements that complement 
the shoulder and elbow with an additional 180D  roll, 180D  pitch 
and 120D  yaw capability to the hand. Therefore, each robot 
arms has 7 DOF of motion. 
Connected to the wrist by the palm, the robot hand has a 
total of 7 DOF. Since the developed humanoid robot requires 
the ability to grasp a target object by wrapping, the middle, 
ring and little fingers of each hand are combined to one finger 
with an appropriate width. The index finger and the combined 
finger employ 2 DOF (MP 1×  and PIP 1× ) since DIP joints are 
ossified with 15D inward in the consideration of low utility rate 
[5][7]. Both MP and PIP joints provide 120D  of pitch motion. 
The thumb employs 3 DOF (CMC 1× , MP 1×  and IP 1× ) [2] for 
the robot to achieve the abilities to push a platform by its palm 
with a maximum 20N force or to grasp a target object by 
wrapping with a maximum 5N force. The CMC joint provides 
an 90D  of yaw motion to move the thumb from the palm side 
for pushing to the opposite side for grasping. Both MP and IP 
joints of the thumb provide a 120D  of pitch motion. Except the 
CMC joint is driven by a DC servo motor with a planetary 
gearhead through a spur gear, all other six joints are driven by 
DC servo motors with planetary gearheads through bevel 
gears. 
C. Design of Wheeled Mobile Base 
The wheeled mobile base can drive the robot to move 
around. The base with the appearance of a rectangular 
parallelepiped is balanced using four passive caster wheels at 
the four corners of the base and driven by two powered 
wheels that provide motion and steering. Steering is 
accomplished by differential velocity control of the powered 
wheels. The mobile base can support a maximum payload of 
150kg. Each drive wheel has one DC servomotor and a pair of 
gears mounted integrally in the wheel hub. Each drive wheel 
module is independently mounted on an L shape suspension 
linkage to allow the base to be driven on non-planar surfaces 
without loss of control. One point of the linkage is a revolute 
joint fixed to the mobile base frame, and the other point is 
affixed to the mobile base frame by a spring. The force 
exerted by the spring can be adjusted according to the 
payload. 
D. Hardware Architecture of the Humanoid Robot 
Fig. 2 depicts the hardware architecture of the developed 
humanoid robot. Two frame grabbers and one DualC6x PCI 
card were installed in the host PC with PENTIUM IV 2.4 GHz 
CPU and 256 MB RAM. Two types of boards, the driver 
board and the signal processing board, were developed. The 
driver board, providing amplification capability with 6 
channels for each, was used to drive servomotors. The signal 
processing board, providing 6 D/A channels, 8 A/D channels, 
6 decoder channels and 8bits DI/DO capability, was used to 
receive home signals, read the position information of the 
servomotors, and output voltage signals to the driver board. 
Consequently, six signal processing boards and six driver 
boards were employed in the control system of the robot. The 
Dual C6x PCI Board can communicate with the six signal 
processing  boards through the DSP~LINK3 I/O interface. 
The two cameras, synchronized with a synchronous 
signal, simultaneously generate two NTSC video signals 
without aliasing. Then, the frame grabber cards convert the 
signals into digital signals, and store the signals in their 
memory. These data are reformatted in real time, then 
transferred to the PC, and processed in the PC. The Dual C6x 
PCI Board is responsible for the computation of the control 
strategies of the humanoid robot. By using the Visual C++ 
programming language and CCS (code composer studio), this 
research constructs the windows interface which is available 
for user using and observing. Therefore, the control of the 
frame grabber and the Dual C6X PCI Board can be entirely 
implemented in the GUI program. 
III.  IMAGE PROCESSING AND OBJECT RECOGNITION 
Unnecessary background must be removed to reduce the 
image processing load. The proposed approach to removing 
unnecessary background is to preprocess quickly the entire 
image to determine the approximate location of each object in 
the image plane. Then, more sophisticated image processing is 
applied to the regions around all the rough locations to 
determine the correct positions of all the objects and recognize 
the desired target object. 
The main idea behind the fast detection of the object’s 
approximate location is as follows. First, features (such as 
color, length, width, shape, and others) of the object are 
selected. Second, the entire image is subdivided by a square. 
Finally, each square of the grid is matched to determine the 
possible location of each object, according to the information 
concerning the selected features of that object. 
Once all approximate locations of objects are determined, 
the squares in which the objects appear are marked. The 
marked square window must expand to a suitable size to cover 
the whole object before this local region is processed. A 
uniform dilation for each side of the marked square window 
can be chosen according to the shape of an object and the 
working space. In this work, the selected dilation ratio for 
each side is three. Thus, the marked square window will 
expand to nine times the size of the original. Finally, an 
“attention window” is created after the centroid of the object 
in every expansion window is obtained. The size of the 
attention window equals that of the expansion window, but 
the center of the attention window lies on the centroid of the 
object in the image plane. 
An object-recognition method is proposed to determine 
whether the object in the attention window is the desired 
0r r r
r r
x u X
Zα
− =   ,  0r r r
r r
y v Y
Zβ
− =  (8) 
Substituting (5) and (6) into (7) and (8), yields a system of 
four linear equations. If the two corresponding image points 
( ),l lx y  and ( ),r rx y  are accurately found in the left image and 
in the right image, respectively, then the three-dimensional 
position of the point ( , , )g g gP X Y Z=  can be determined by 
solving the system of linear equations. The number of 
equations exceeds the number of unknowns, and so 
determining ( , , )g g gP X Y Z=  to satisfying all four equations 
exactly is impossible. The alternative, then, is to determine 
( , , )g g gP X Y Z=  using least-error-squares. The result is referred 
to as the least-squares estimator (LSE) of P , which is the 
estimated position of an arbitrary point in three-dimensional 
scene space with reference to the gaze coordinate system. 
In some applications of static binocular stereo geometry, 
the position of a corresponding point in the gaze coordinate 
system, projected onto the optical centers of both image 
planes, is desired. Then, the solution to the triangulation 
problem can be attained more straightforwardly using the 
following equations. 
1 2
1 2
tan tan
2 tan tang
DX θ θθ θ
−= +   or  
( )
( )1 21 2
sin
2 sing
DX
θ θ
θ θ
−= +       
( )1 2 31 2
cos cos sin
sing
DY θ θ θθ θ= + , ( )1 2 31 2
cos cos cos
sing
DZ θ θ θθ θ= +   (9) 
The action of the robotic binocular head can be divided 
into two stages of movement. In the first, the robotic head 
saccades the surroundings for the target object. Once the 
target object is found, the head enters the second stage of 
movement, in which the eyes of the head are driven to fixate 
the target object. The saccade behavior is seeking the target 
object in three-dimensional space, so that the target object is 
visible in the binocular image planes. An object recognition 
method, proposed in Section 3, is used to identify the target 
object. Only the 3th and 5th joints of the robotic head, which 
provide the tilt and pan movements respectively, are activated 
in the saccade stage to simplify the saccade behavior. 
Once the target object is detected by the saccade 
behavior, the robotic head will drive its two eyes to gaze at the 
target object. The objective of gaze control is to control the 
robotic head so that the centroid of the target object is 
maintained at the optical centers of both image planes of the 
two CCD cameras. The gaze behavior is achieved by moving 
joints 1, 2 and 3, following the proposed control strategy. The 
required joint displacements associated with the movement of 
the left camera, required for the left camera to fixate the target 
object, can be derived from (5) and (7) by ignoring the depth 
information. 
1
3 tan il
l
yθ β
− ⎛ ⎞= ⎜ ⎟⎝ ⎠
 (10) 
1 3
1
costan il
l
x θθ α
− ⎛ ⎞= ⎜ ⎟⎝ ⎠
 (11) 
where 0il l lx x u= − , 0il l ly y v= − . Similarly, the required joint 
displacements associated with the motion of the right camera, 
required for the right camera to fixate the target object, can be 
derived from (6) and (8) by ignoring the depth information. 
1
3 tan ir
r
yθ β
− ⎛ ⎞= ⎜ ⎟⎝ ⎠
             (12) 
1 3
2
costan ir
r
x θθ α
− ⎛ ⎞= −⎜ ⎟⎝ ⎠
         (13) 
where 0ir r rx x x= − , 0ir r ry y y= − . The tilt movement 3θ  is 
redefined as the average of (10) and (12), since both cameras 
share a common tilt joint in the robotic head. 
During the eye/arm coordination of the humanoid robot, 
the hand picks up the target object and places it in a known 
location relative to the arm base coordinate system. In 
developing the control strategy for approaching the target 
object, the following definitions of feature points, the 
associated coordinate systems of all three possible objects, and 
the coordinate transformations between pairs of coordinate 
systems, are used. 
As shown in Fig. 4, the feature points and the associated 
coordinate systems of all three possible objects (a rectangular 
parallelepiped, a cup with a handle and a bottle-shaded 
container with square bottom) are defined as follows. Three 
corners 1tP , 2tP , 3tP  of each object are extracted by the vision 
system as the feature points of the object. The right bottom 
corner is labeled 3tP , the left bottom corner 1tP , and the right 
top corner 2tP . The origin of the object coordinate system is at 
3tP , and the x  and y  axes are specified to be in the 1 3t tP P
JJJJJG
 and 
2 3t tP P
JJJJJJG
 directions, respectively. As shown in Fig. 5, the 
transformations between pairs of coordinate systems are 
defined as follows. 
pDesired
oT defines a desired coordinate transformation from the 
target object coordinate system to the palm coordinate system. 
g
oT  represents a coordinate transformation from the target 
object coordinate system to the gaze coordinate system. 
g
abT  represents a coordinate transformation from the arm base 
coordinate system to the gaze coordinate system 
ab
wT  defines a coordinate transformation from the wrist 
coordinate system to the arm base coordinate system. 
w
pT  defines a coordinate transformation from the palm 
coordinate system to the wrist coordinate system. 
p
pDesiredT  defines a coordinate transformation from the desired 
palm coordinate system to the current palm coordinate system. 
A proposed control law is derived by the following 
procedure. 
(1) Selecting pDesiredoT  
pDesired
oT  can also be described as the desired object pose 
relative to the palm coordinate system in the approaching 
stage. pDesiredoT  is selected as follows. 
0 1
offsetpDesired
o
I P
T ⎡ ⎤= ⎢ ⎥⎣ ⎦  
where offsetP  is chosen based on the grasped object. 
