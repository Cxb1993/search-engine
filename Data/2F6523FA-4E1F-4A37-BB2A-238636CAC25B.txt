 I
行政院國家科學委員會補助專題研究計畫 成果報告   □期中進度報告 
 
應用於視訊鑑識之視訊濃縮及超解析技術 
 
 
計畫類別：個別型計畫   □整合型計畫 
計畫編號：NSC 99－2218－E－156－004－ 
執行期間： 99 年 11 月 01 日至 100 年 10 月 31 日 
 
執行機構及系所：真理大學資訊工程系 
 
計畫主持人：梁祐銘 
共同主持人： 
計畫參與人員：吳昇哲、曾銘濱、金孟良、曾郁文、李啟豪、黃偉倫、徐千祐、
翁銘萱 
 
 
 
成果報告類型(依經費核定清單規定繳交)：精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國 101 年 1 月 31 日 
 1
1. 前言 
隨著科技的進步及攝影器材的普及，視訊監控已成為我們生活中最重要的安全監控工具之一。尤
其在美國 911 事件發生後，世界各國紛紛廣泛地在重要街道、機場、建築物架設視訊監控器材。以英
國為例，在 2007 年時，英國監視攝影機的數量已高達兩千五百萬台。而在國內，政府近幾年也一直編
列預算要在全台灣重要的路段及路口設置監視攝影機。 
由於監視攝影器材被廣泛地架設，攝影器材所儲存的視訊紀錄(video record)變成警方辦案時重要的
協助資源，因此視訊鑑視變成一門很先進的學問。近年來國內各種大、小刑案，許多都是由警方利用
調閱視訊紀錄來提供破案之重要協助，例如在民國 98 年間，高雄市因調閱監視系統而查破的刑案計有
278 件、查獲嫌疑犯 397 人。 
然而目前警方辦案的方式為調閱案發現場周遭的視訊紀錄，利用人眼觀看影片的方式，從影片中
找出可疑事件的片段。倘若警方不知案發的確切時間，加上周遭所取得的視訊紀錄量過於龐大，便需
耗費相當大的人力與時間才能從中找出可疑事件的片段。一般而言，在某些場合，如停車場，有大部
份的時間可能都是沒有任何的動靜，但警方仍得看完全部內容才能避免遺漏重要的訊息。因此若能先
對取得的視訊做視訊濃縮(video condensation)處理，將一段 24 小時的視訊資料濃縮成幾分鐘的視訊片
段，且又把所有的重要訊息(如移動物件及物件出現的時間順序關係)都保留下來，如此對警方辦案的效
率便可以大大地提昇。 
2. 研究目的 
本研究主要目的是發展一個視訊濃縮技術來濃縮一段很長的監控視訊資料。此濃縮的視訊片段不
但可以將所有重要的訊息都保留下來，為了讓濃縮的視訊片段更短，我們希望讓原本在不同時間出現
的物體，經過濃縮後，可能變成在同樣時間出現，但不改變其出現的時間先後順序。此外，若警方可
以事先知道嫌疑犯在犯案現場行進的路徑，我們可以透過一種運動過濾(motion filter)來將其他方向運動
的物體過濾掉，如此又可大大地濃縮視訊資料。 
3. 文獻探討 
視訊濃縮的目的是希望將一段很長的視訊影片，濃縮成一小段的影片來代表原來的視訊資料。目
前市面上所販售的數位監視錄影器(DVR)大部份都有提供「物體移動偵測錄影」，亦即只針對有物體在
場景中移動的片段做儲存，以節省硬碟儲存空間。然而假如進入監控場景的移動物體是依序進出場景
的話，即一物體離開場景後另一物體才進入場景，如此能濃縮的程度便有限，此方式雖可以保留最完
整的資訊，但濃縮效果極為不好，而且容易受到運動雜訊的影響。 
近年來也有不少學者從事視訊濃縮技術相關的研究，最早是利用快轉(fast forwarding)方式來進
行視訊濃縮。快轉技術主要是利用一個固定的時間間隔(regular time interval)來抽取(sampling)
保留個別單張或一群的視訊框(frame)，其餘部份皆刪除[1]。此方法可能造成一些重要的片段卻沒有
任何代表的視訊框，或一些較長的片段可能選取過多重覆而不重要的代表視訊框。由於本計畫之應用
需要有較高的視訊濃縮率(video condensation ratio)，濃縮率在此定義為處理後的視訊長度與尚未
處理前的視訊長度之比，因此若要利用此方法達成高濃縮率的目的，必須刪除更多連續的視訊框，如
此可能會導致一些重要的資訊會被排除掉。雖然後來有些學者針對此問題提出了內容動態刪除法
(content-adaptive skipping)或動態取樣法(adaptive and dynamic sampling)[2][3][4]來改善效
果，這些方法只利用少數的影像來代表整段視訊資料，且每張代表的影像都概述了視訊中重要事件發
 3
原始影像中不同的 y 座標值得到一個 XT 影像序列與不同的 x 座標值得到一個 YT 影像序列。接著，我
們利用垂直方向邊界偵測(edge detection)的方法對 XT 與 YT 影像序列做移動物體偵測，得到 XT 與 YT
邊界影像序列。假如我們只要找尋某個運動方向的移動物體時，可利用運動方向過濾將 XT 與 YT 邊界
影像中不同方向的移動物體過濾掉，只留下我們要找尋方向的移動物體。最後，我們再對 XT 與 YT 邊
界影像序列做時間軸上的影像細縫裁減，來達到視訊濃縮的目的。以下子章節我們將針對這四個步驟
分別作詳細的描述。 
 
 
圖 1、系統架構圖 
 
4.1 XYT 圖的轉換 
一個視訊的組成是在時間 t 內由許多張二維圖片所構成的(如圖 2 所示)，因此我們可以用三維空間
(x-y-t)來表示一視訊，如圖 3(a)所示。在 XYT 圖的轉換中，我們對 y 軸上每個 y 值都可以擷取出一個
x-t 二維關係的 XT 圖，同樣地在 x 軸上也可以從每個 x 值取得一個 y-t 二維關係的 YT 圖，分別如圖
3(b)與 3(c)所示。從 XT 圖可得知視訊中目標物體移動的先後順序與方向，從 YT 圖可得知物體大致的
型態。 
 
         
(a)原始視訊框#60                     (b)原始視訊框#294 
         
(c)原始視訊框#546                   (d)原始視訊框#780 
圖 5.影像序列 
 
 
原始
視訊 
XYT 圖
轉換 
移動物
體偵測
影像細
縫裁減 
濃縮
視訊
運動方
向過濾
 5
   
(a)                                     (b) 
圖 5.去除雜訊後的結果;(a)XT 圖(b)YT 圖 
 
4.3 運動方向過濾 
在經過移動物體偵測後，我們可以將原始的 XT 與 YT 影像序列轉換成邊界 XT 與邊界 YT 影像序
列，接著我們將所有的邊界 XT 影像合併成一張新的 EXT 影像，所有的邊界 YT 影像合併成一張新的
EYT 影像，如圖 6 所示，黑色區域則表示移動物體在整個視訊的移動情形。 
 
   
(a) EXT 圖                                (b) EYT 圖 
圖 6.合併後的 EXT 與 EYT 影像 
 
EXT 圖與 EYT 圖中相連的黑色區域即代表同一個物體。因此我們首先利用連通物件標記分別找出
EXT 與 EYT 中所有的物體，接著利用物體在時間軸的關係找出 EXT 圖與 EYT 圖中物體的對應關係，
最後再利用斜率來判斷物體移動的方向，並選擇所要過濾與刪除移動的方向。圖 7 代表刪除由右至左
方向的移動物體。 
 
   
(a) EXT 圖                                (b) EYT 圖 
圖 7.過濾方向後 EXT 與 EYT 影像 
 
 7
5. 結果與討論 
本節以二個視訊資料的實驗來驗證我們所提的視訊濃縮技術。第一個視訊資料長度共有
500 個視訊框，視訊內容為兩個移動目標物以同方向進行移動，且分別出現在第 40~150 個視訊
框與第 260~370 個視訊框，其中間隔有 110 個視訊框是沒有移動目標物。經過濃縮後的影片長
度減為 170 個視訊框，結果如圖 10 所示。在濃縮後的影片中，完整保留了兩個移動目標物，
並且將不同時間出現的移動目標物同時顯示在同一視訊框中，但又不改變其原始之時間順序關
係。 
   
(1)原始視訊框#55                      (2)原始視訊框#270 
   
(3)濃縮後視訊框#60                      (4)濃縮後視訊框#120 
圖 10.濃縮前後的比較 
 
第二個視訊資料長度共有 450 個視訊框，在第 30~150 個視訊框與第 300~420 個視訊框分
別兩個有以反方向移動的移動目標物。濃縮後的影像剩下 300 個視訊框，實驗結果如圖 11 所
示。在此實驗中，由於為了保留移動目標物出現的時間順序關係，因此反向移動的兩個移動目
標物無法如第一個實驗一般，同時顯示在同一視訊框中，因此濃縮率較不高，但同樣可以將中
間沒有移動目標物的視訊框全部移除。 
 9
Proceedings of the IEEE International Conference on Computer Vision, 2003, pp. 104- 109. 
[7] H. W. Kang, Y. Matsushita, X. Tang, and X. Q. Chen, “Space-Time Video Montage,” Proceedings of the 
IEEE Conference on Computer Vision and Pattern Recognition, 2006, pp. 1331- 1338. 
[8] Y. Pritch, A. Rav-Acha, and S. Peleg, “Nonchronological Video Synopsis and Indexing,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 11, pp. 1971- 1984, Nov. 2008. 
[9] Z. Li, P. Ishwar, and J. Konrad, “Video Condensation by Ribbon Carving,” IEEE Transactions on Image 
Processing, vol. 18, no. 11, Nov. 2009. 
[10] S. Avidan and A. Shamir, “Seam Carving for Content-Aware Image resizing,” ACM Transactions on 
Graphics, vol. 26, no. 3, 2007. 
[11] 梁祐銘、金孟良、曾銘濱、吳昇哲，2011，「應用於視訊鑑識之視訊濃縮技術」，2011 年資訊技術
與產業應用研討會，台北：北台灣科技學院主辦。 
[12] 吳昇哲、金孟良、曾銘濱，2011，視訊濃縮技術，真理大學資訊工程學系實務專題技術報告。 
 
 
7. 計畫成果自評 
本研究針對影像視覺在視訊鑑識上的應用，提出視訊濃縮的技術。本研究所發展的視訊濃縮技術
可以將一段很長的監控視訊資料濃縮成一小段影片，又可以將所有重要的訊息都保留下來。此外，我
們可以透過一種運動過濾的方式來將其他方向運動的物體過濾掉，如此又可大大地濃縮視訊資料。以
下針對學術成就、技術創新與社會影響等方面來評估研究成果之價值。 
在學術成就上，我們針對視訊鑑識提出了視訊濃縮的技術，目前已發表了研討會論文，也計畫將
研究成果投稿於期刊論文。 
在技術創新部份，我們相較於目前現有的技術中，我們可以完全在 XYT 上偵測物體並進行濃縮的
技術，而不需進行背景模組化與前景物擷取的部份。另外，我們加入運動過濾來刪除我們不要的運動
方向的物體，如此可以得到更好的濃縮結果。 
在社會影響上，利用我們視訊濃縮的技術，可以將一段 24 小時的視訊資料濃縮成幾分鐘的視訊片
段，且又把所有的重要訊息都保留下來，如此對警方辦案的效率便可以大大地提昇。 
 2
Identification and Search (VCDIS)、Multimedia Services and Technologies for E-health 
(MUST-EH)、Multimedia-Aware Networking (WoMAN)、With special session on "Hot 
Topics in Multimedia Delivery (HotMD)、Interactive Ambient Intelligent Multimedia 
Environments (AIME)、Acoustic and Video Coding and Communication (AVCC)等六
場。第一天的 tutorial 則包含 Yo-Sung Ho 主講的「Technical Challenges of 3D Video 
Processing」與 Cha Zhang& Zhengyou Zhang 主講的「Audio, Video, and Their Joint 
Processing with Applications to Teleconferencing」兩場。第五天的workshop包含Content 
Protection & Forensics (CPAF)、Advances in Automated Multimedia Surveillance for 
Public Safety (AAMS-PS)、Advances in Music Information Research (AdMIRe)、Hot 
Topics in Multimedia、與 Hot Topics in 3D Multimedia (Hot3D)等五場。最後一天的
tutorial 則包含 W.A.C. Fernando 主講的「Quality of Experience (QoE) in Multimedia 
Communications」與 Julius Mueller& Thomas Magedanz 主講的「Packet Core Network 
Evolution in regard to Future Internet Research」兩場。 
第二至四天包含三場 keynote speech 與 oral presentation session、poster&demo 
session 則是這次會議主要的重點。第一場的 keynote speech，speaker 為 Computer 
Architecture Department at UPC 的 Mateo Valero 教授，其主講題目為「Towards Exaflop 
Supercomputers」。首先簡單地介紹 Barcelona Supercomputing Center，接著討論
supercomputer 相關的議題。第二場的 keynote speech，speaker 為 Electrical Engineering 
and Computer Sciences at UC Berkeley 的 Avideh Zakhor 教授，其主講題目為「Fast, 
automatic, photo-realistic, 3D modeling of building interiors」，主要介紹他們所發展的
 4
演講外，也能與參加會議的學者們互相交流，討論研究心得，對於自己的研究工作，
更是獲得了許多無形的幫助。 
在三場 keynote speech 中讓我印像最深的是微軟的學者所介紹 kinect 相關的應
用。由於本人博士論文是從車人類動作相關的研究，其中最困難的是前景物擷取的
問題，若用簡單的 2D 方式會有 view-invariant 問題，若用 3D motion capture 的方式
其代價又很高。而微軟推出了平價的 kinect 後便可以解決這樣的問題，因此這場
keynote speech 讓我收獲良多。 
在本次大會的 oral 中，有來自各地的學者，陸續發表各實驗室的研究成果，內
容十分精彩，雖然每位學者在短短的幾分鐘內簡介其成果，但在後來的 poster&demo 
session 中可以與作者深入的討論，因此讓我得到許多新的知識，對自己的研究也提
供了不少新的想法。 
三、考察參觀活動(無是項活動者略) 
四、建議 
此次非常感謝國科會給我這個機會能讓我出國參加本次的國際研討會，讓我可
以了解目前國際間在多媒體領域最新的研究議題，並且可以與各國的教授學者進行
討論與交流。本次的會議是相當成功與圓滿，且本次的 oral 與 poster 的安排也相當
的特別，相信未來國內若有同樣性質的會議也可朝向此一方向努力。 
五、攜回資料名稱及內容 
會議議程與投稿者論文的隨身碟。 
六、其他 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：梁祐銘 計畫編號：99-2218-E-156-004- 
計畫名稱：應用於視訊鑑識之視訊濃縮及超解析技術 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 0%  
研究報告/技術報告 0 0 0%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 0%   
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 0 0 0%  
博士生 0 0 0%  
博士後研究員 0 0 0%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 0%  
研討會論文 0 0 0% 
篇 
 
論文著作 
專書 0 0 0% 章/本  
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 0 0 0%  
博士生 0 0 0%  
博士後研究員 0 0 0%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 0% 
人次 
 
