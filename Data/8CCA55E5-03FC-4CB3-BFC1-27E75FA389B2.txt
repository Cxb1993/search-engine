2 
壹、前言 
樣式辨認中，由於維度過高且樣本點不足的情況下造成 Hughes phenomenon [1]與做特徵 
值分解時會有奇異問題現象產生，所以我們採取降低資料維度的步驟，藉著挑選出有助於辨 
識的特徵，而其最主要的好處是降低其維度後對資料分群或辨識所需的運算量有可能提升圖 
形辨別的辨識率。特徵選取的目標是要從原有的特徵中挑選出最佳的部分特徵；而特徵萃取 
依據不同的準則將原始特徵空間映至另一特徵空間達到減少維度的目的。 
本研究首先加入叢集分析後的算法與原始演算法各萃取 1到 15個維度數，並比較最高的 
辨識率。另外則是將原始地圖當作測試樣本，利用不同演算法判別出類別，並將地圖真實情 
形與類別呈現出。 
貳、研究目的 
近年來，高維度資料(high dimensional data)如高光譜影像資料、基因微陣列(microarray)、 
手寫辨識、人臉辨識等資訊，逐漸廣泛的被應用於日常生活中。而傳統的分類技術在統計樣 
式辨認時的假設大多是基於有足夠的訓練樣本可以供研究者使用。但是，高維度資料分類時 
所需要的訓練樣本數通常比傳統的資料要多出許多，所以更容易出現訓練樣本數不足的情況 
發生。高維度資料的主要問題就是當訓練樣本數過少時，傳統的分類器容易會出現 Hughes 
phenomenon  [1]。也就是說當資料的維度增加時，在不增加訓練樣本的數量情況下，常會因 
為 Hughes phenomenon  [1]而造成的辨識率下降。 
在現實中的情況，訓練樣本是較難以取得的，所以在處理高維度資料時，經常會遭遇到 
小樣本(small  sample  size)的問題。通常會利用到特徵萃取(feature  extraction)或是特徵選取 
(feature selection)來降低原始資料訓練樣本數過少所造成的影響。 
在進行資料處理的時候，就如同圖1所表示，資料的維度數增加可以增加資料的分散度 
(separability)。資料的分散度增加相對於辨識率提升是有幫助的，但是在資料維度數增加時， 
分類器中所需估計的參數同時也會相對的增加。 
圖 1 資料維度數與分散度的 
相關性 
圖 3 資料維度數與辨識率相 
關性 
圖 2 資料維度數與參數估計 
精確度相關性 
圖 4 Hughes henomenon 
(Hughes,1968) 
圖2中，分別表示不同大小的樣本點數，在資料維度數增加，而不改變訓練樣本點數的情 
況下，參數估計的精確度就會隨著維度 p 增加而下降。圖3顯示當資料的維度數變大時分散程 
度就會變大並且有助於辨識；可是當樣本的維度數過大時，參數估計精確度不良的效果就會 
大於分散程度所提供的幫助，最終造成辨識率不良的情況。所以，如何在資料的維度數較多， 
也就是分散程度較高的情況下，而不增加訓練樣本的數目，可以達到提升辨識率的目的，這 
是一個兩難的問題。 
接著由圖4中的圖形可以看出，平均辨識率會伴隨著測量複雜度增加而減少，只有在當訓 
練樣本點數 n在無限大的時候，平均辨識率才不會因為測量複雜度的增加而受到影響。在現 
實的情況下，要得到“足夠“的訓練樣本，是件不容易的事情。因此在本研究中希望在有限 
的樣本下，減輕Hughes phenomenon [1]對實驗所造成的影響，進而提升辨識率。 
高維度資料分類時，訓練樣本裡所有的類別是已知的，但是實際上在同一類別中有可能 
有些的樣本是非常態分布或是多峰混合分布(multi­modal  mixture  distribution)，這些樣本有可
4 
( )( ) å 
= 
- - = 
L
i 
T 
i i i b  m m m m P S 
1 
0 0 
LDA  (6) 
i m 代表類別 i的平均數。LDA的用意在取得最大的分散量，所以要將組內的分散量拉到最小； 
組間的分散量則是越大越好。因此最佳化以下的準則： 
( ) ÷ 
ø 
ö ç 
è 
æ = 
-  LDA 1 LDA 
LDA  trace  b w  S S J  (7) 
1.3 無參數加權特徵萃取 
在本研究中還使用了另一種無參數萃取方法，無參數加權特徵萃取(Nonparametric 
Weighted Feature Extraction, NWFE) [4]，來與LDA進行比較。LDA和NWFE都是用來尋找最大 
化組內分散度矩陣的反函數乘上組間分散度矩陣的跡數(trace)之特徵空間。 
NWFE的組間分散度矩陣(  NW b S  )以及組內分散度矩陣(  NW w S  )，其定義如下： 
T i 
k j 
i 
k 
i 
k j 
i 
k 
L
i 
L 
i j 
j 
n 
k  i 
j i 
k 
i b  x M x x M x n 
P S 
i 
)) ( ))( ( (  ) ( ) ( ) ( ) ( 
1  1  1 
) , ( 
NW - - = å åå 
= 
¹ 
= = 
l 
(8) 
å å 
= = 
- - = 
L 
i 
n 
k 
T i 
k i 
i 
k 
i 
k i 
i 
k 
i 
i i 
k 
i w 
i 
x M x x M x 
n 
P S 
1  1 
) ( ) ( ) ( ) ( 
) , ( 
NW  )) ( ))( ( ( 
l  (9) 
在上列式子中  ) (i k x  表示類別 i的第 k 個樣本點，  i n 表示類別 i的樣本點數，  i P表示類別 i的 
事前機率。  (i,j) k λ  為第 i類的第 k 個樣本點對第  j 類分散矩陣的權重，是由  ) (i k x  以及  ) (  ) (i k j  x M  所構成 
的方程式，其定義如下： 
å 
= 
- 
- 
= 
i n 
l 
(i)
l j 
(i)
l 
(i)
k j 
(i)
k (i,j) 
k 
)) (x ,M (x 
)) (x ,M (x 
λ 
1 
1 
1 
dist 
dist 
(10) 
式子中  ) (  ) (i k j  x M  代表樣本點  ) (i k x  在類別  j 中的權重，其定義如下： 
å 
= 
= 
j n 
l 
j
l 
j i 
kl 
i 
k j  x w x M 
1 
) ( ) , ( ) (  ) (  (11) 
(i,j) 
kl w  則是用以計算加權平均數的權重，其定義如下： 
å 
= 
- 
- 
= 
i n 
l 
j
l 
(i)
k 
j
l 
(i)
k (i,j) 
kl 
) ,x (x 
) ,x (x 
w 
1 
1 ) ( 
1 ) ( 
dist 
dist  (12) 
) ,x (x  jl 
(i)
k 
) ( dist  代表第 i類的第 k 個樣本點與第  j 類的第 l個樣本點的距離。 
另外 Kuo & Landgrebe (2002) [5]提出共變異數矩陣的對角線部分對組內分散度矩陣有改 
善的效果，於是基於這樣的觀點本研究也將組內分散度矩陣改成以下的形式： 
NW NW NW R 
2 
1 
) ( diag 
2 
1 
w w w  S S S + = 
-  (13) 
最佳的特徵可以由最佳化以下的準則得到： 
) ) (( trace  NW 1 NW R NWFE  b w  S S J 
- - =  (14) 
二.分類器 
2.1 最大概似分類器
6 
接下來根據矩陣U 即可定義出所需要的目標函數 J ： 
( ) åå å 
= = = 
= = 
C
i 
n 
j 
j i 
m 
ij 
C
i 
i k  x m u J c c c U J 
1  1 
2 
1 
2 1  ) , ( dist ) ( ,..., , ,  (20) 
在這方程式之中m是一個介於  ) , 1 [ ¥ 的權重係數，而  ) , ( dist  j i  x m  則是  i m 與  j x  的距離函數  (  i m 代 
表叢集 i的中心；  j x  為樣本點  j )，本研究中所採用的為歐幾里得距離。為了要滿足方程式 
(19)，可以利用方程式(20)得到一個新的目標函數  new J  ： 
å å å å 
= = = = 
- + = 
C 
i 
n 
j 
C 
i 
ij j 
n 
j 
j i 
m 
ij n C  u x m u c c c U J 
1  1  1 1 
2 
2 1 2 1 new  ) 1 ( ) , ( dist ) ( ) ,..., , , ,..., , , ( l l l l  (21) 
方程式(21)之中的  j l 是對應於方程式(19)的n組的 Lagrange multipliers限制。接著為了要最佳 
化目標函數  new J  ，所以針對所傳入的參數分別進行微分，得到以下方程式： 
å 
å 
= 
= = 
n 
j 
m 
ij 
n 
j 
j 
m 
ij 
i 
u 
x u 
m 
1 
1 
) ( 
) ( 
(22) 
å
= 
- 
÷ 
÷ 
ø 
ö 
ç 
ç 
è 
æ 
= 
k
k 
m 
j k 
j i 
ij 
x m 
x m 
u 
1 
1 
2 
) , ( dist 
) , ( dist 
1  (23) 
整個 fuzzy c­mean叢集分析法的演算流程如下： 
步驟 1：隨機填入矩陣U 中所有數值但是需要滿足方程式(19)。 
步驟 2：利用方程式(22)來計算出所有叢集的中 
心點  i m 。 
步驟 3：計算目標函數  1 new J  。 
步驟 4：利用方程式(23)計算出新的矩陣U ， 
步驟 5：重複步驟 2到步驟 4，直到  n J new 小於一個臨界值或是當(  1 new + n J  -  n J new )已經相 當趨近於 
0時，則停止演算法。 
肆、研究方法 
1 叢集分析特徵萃取法 
在高維度資料中常會遇到非常態分布或是多峰型態分部的情形，此時同個類別的樣本點 
存在些許差異性，如圖  6，這種情形容易造成分類器在分類時難以分類，因此若能將叢集分 
析演算法將 class1 與 class2 看成四個不同的類別如圖 7，再融入特徵萃取裡，這樣有助於 
特徵萃取所萃取出的空間以利分類器的運作。 
圖 6 同類別樣本中 
存在差異性問題 
圖 7 以叢集分析處理同類別中 
存在差異性問題 
圖 8 不同類別 
交疊情形 
圖 9 以叢集分析處理不同類別 
交疊情形 
此外在樣式辨認中也常常會遇到兩個甚至多個類別有相互交疊的情形，如圖  8，在此圖 
中發現到 class 1與 class 2大部分是相互交疊的，這種情形往往在分類時是非常地難以處理， 
這也是造成辨識率下降的一個原因。因此，若能使用叢集分析的方法將兩個類別各自分成兩
8 
Indian Pine Site為森林和農作物地區是 1992年 6月所收集的資料，取 Indiana州西北 100 
平方公里區域，共 9個類別，分別為玉米田己耕地(Corn­clean)、玉米田未耕地(Corn­notill)、 
玉米略耕地(Corn­min)、牧草地(Grass/Pasture)、林地(Woods)、乾草地(Hay­windrowed)、大豆 
未耕地(Soybean­notill)、大豆略耕地(Soybean­min)和大豆已己耕地(Soybean­clean)。 
伍、研究成果 
本章中的實驗結果分二個部份來看，第一個部份比較本研究所提出的演算法與原始演算 
法各萃取 1到 15個維度數，並比較最高的辨識率。第二部份是將原始地圖當作測試樣本，利 
用不同演算法判別出類別，並將地圖真實情形與類別呈現出。 
表 1是使用 ML分類器資料集則為 Indian Pine Site，從此表中可以看出使用ML分類器分 
類，特徵萃取法使用 LDA 時，辨識率會提升最多，可由原本的 0.8181 提升至 0.8747；若特 
徵萃取法是使用 NWFE 時也能提升一些效能由原本的 0.8763 提升至 0.8958；若使用 PCA時 
則融入叢集分析法的效能也能相當接近原本的方法。 
表 2是使用 1NN資料集為 Indian Pine Site，表中顯示當特徵萃取法使用 LDA時效能可提 
升許多由原本的 0.8445提升至 0.8945，使用 NWFE 及 PCA也能趨近至原本的辨識率。 
表 3是分類器使用 ML分類器，資料集為 Washington DC Mall的辨識率，由此表可看出 
在不加入叢集分析的情況下已經能有相當高的辨識率，而融入叢集分析的方法後的效能也能 
相當趨近於原本的辨識率甚至稍為高一點。 
表 4代表分類器使用 1NN分類器對 Washington DC Mall資料集所產生的辨識率，由表中 
發現原本的方法也是同樣就能達到相當高的效能，使用 LDA以及 NWFE皆能提升一些效能， 
至於使用 PCA時也能相當趨近原本的效果。 
接下來是將原始的地圖當作測試樣本，並使用不同演算法將分類結果後地圖的類 
別以最直觀的方式呈現出，並比較新演算法與原始演算法的優劣。因為本研究在 Indian 
Pine Site 資料上的表現效果是比較顯著的，於是將 Indian Pine  Site 的圖形呈現出，在 
此部份每個圖形皆為以特徵萃取法萃取類別數減一個維度數來判別 ，圖 7為 Indian Pine 
Site 地形的 9 個類別的原始類別分別散布的位置；圖 8 是以 ML分類器利用 LDA萃取 
8 個維度數後分類的情形；圖 9 與圖 10 則為以 ML 分類器利用融入 FCM 之 LDA 萃取出 8 
個維度數後分類的效果。 
表 1  Indian Pine Site 
使用 ML分類器的辨識率比較 
表 2  Indian Pine Site 
使用 1NN分類器的辨識率比較 
特徵萃取 叢集分析方法 叢集個數 最高辨識率(特徵數) 
無  0.8725(14) 
2  0.8708(13) 
k­mean 
3  0.8640(13) 
2  0.8723(14) 
PCA 
Fuzzy c­mean 
3  0.8668(13) 
無  0.8181(7) 
2  0.8729(9) 
k­means 
3  0.8746(8) 
2  0.8747(9) 
LDA 
fuzzy c­mean 
3  0.8732(9) 
無  0.8763(10) 
2  0.8941(9) 
k­mean 
3  0.8965(9) 
2  0.8958(9) 
NWFE 
Fuzzy c­mean 
3  0.8954(9) 
特徵萃取 叢集分析方法 叢集個數 最高辨識率(特徵數) 
無  0.7920(15) 
2  0.7920(15) 
k­mean 
3  0.7919(15) 
2  0.7919(15) 
PCA 
Fuzzy c­mean 
3  0.7919(15) 
無  0.8445(7) 
2  0.8908(10) 
k­means 
3  0.8940(15) 
2  0.8906(10) 
LDA 
fuzzy c­mean 
3  0.8945(12) 
無  0.8993(14) 
2  0.8797(12) 
k­mean 
3  0.8848(12) 
2  0.8810(9) 
NWFE 
Fuzzy c­mean 
3  0.8861(11)
10 
上來看可以發現在 PCA中因為沒有考慮到組間分散度矩陣以及組內分散度矩陣的問題，因此 
融入叢集分析的方法並無法獲得相當地改善。對於 LDA 以及 NWFE 兩種特徵萃取法，因為 
兩種方法都是要最佳化組間分散度矩陣及組內分散度矩陣的比率，所以採用融入叢集分析的 
方法通常能獲得效能上的增加以及辨識率的提升。 
柒、計畫成果自評 
本研究計畫主要依據計畫書所規劃事項進行，研究成果皆符合預期，其內容如下表： 
原計畫書內容 參見 
主題一 高維度叢集分析暨融入高維度叢集分析之特徵萃取探究 下列發表論文[2][5] 
主題二 核化特徵萃取與選取之研究 下列發表論文[7] 
主題三 核化辨識器之研究 下列發表文[3] 
研究期間工作進行相當順利，相關成果與發表論文亦相當豐碩，參與計畫研究人員也得 
到完整且良好研究之訓練，並且發表數篇論文如下所列： 
[1]  Bor­Chen  Kuo,  Chun­Hsiang  Chuang,  Chih­Cheng  Hung,  and  Szu­Wei  Yang,  (2008).  A  Novel  Random 
Subspace Method Using Spectral and Spatial Information for Hyperspectral Image Classification. Proceedings of 
International Geoscience and Remote Sensing Symposium, July. 6­11, 2008. (EI) 
[2]  Bor­Chen Kuo, Wen­Chun Huang, Hsiang­Chuan Liu, and Shiau­Chian Tseng, (2008). A Novel Fuzzy C­Means 
Method  for  Hyperspectral  Image  Classification.  Proceedings  of  International  Geoscience  and  Remote  Sensing 
Symposium, July. 6­11, 2008. (EI) 
[3]  Bor­Chen Kuo,  Jinn­Min Yang, Tian­Wei Sheu, and Szu­Wei Yang,  (2008). Kernel­Based k­NN and Gaussian 
Classifiers for Hyperspectral Image Classification. Proceedings of International Geoscience and Remote Sensing 
Symposium, July. 6­11, 2008. (EI) 
[4]  Hsiao­Yun  Huang,  Bor­Chen  Kuo,  and  Yu­Ling  Lee,  (2008).  Adaboost­NWFE  Classification  Scheme  for 
Hyperspectral Image. Proceedings of International Geoscience and Remote Sensing Symposium, July. 6­11, 2008. 
(EI) 
[5]  Chih­Cheng Hung, Wenping Liu, and Bor­Chen Kuo, (2008). A New Adaptive Fuzzy Clustering Algorithm for 
Remotely Sensed Images. Proceedings of International Geoscience and Remote Sensing Symposium, July. 6­11, 
2008. (EI) 
[6]  Hsin­Hua Ho, Bor­Chen Kuo, Jin­Shiuh Taur, and Cheng­Hsuan Li, (2008). A Flexible Metric Nearest­Neighbor 
Classification Based on the Decision Boundaries of svms for Hyperspectral Image. Proceedings of International 
Geoscience and Remote Sensing Symposium, July. 6­11, 2008. (EI) 
[7]  Cheng­Hsuan  Li,  Bor­Chen  Kuo,  Chin­Teng  Lin,  and  Chih­Cheng  Hung,  (2008).  Dimension  Reduction  for 
Hyperspectral  Image  Classification  via  Support  Vector  based  Feature  Extraction.  Proceedings  of  International 
Geoscience and Remote Sensing Symposium, July. 6­11, 2008. (EI) 
參考文獻 
[1]  Hughes, G. F. (1968). On the mean accuracy of statistical pattern recognition. IEEE Trans. Inform. Theory, vol. 
14, pp. 55 ­ 63, Jan. 
[2]  Jolliffe, I. T. (1986). Principal Components Analysis. Springer­Verlag, New York. 
[3]  Fukunaga, K. (1990). Introduction to Statistical Pattern Recognition. San Diego, CA:Academic Press. 
[4]  Kuo, B­C.  and Landgrebe, D. A.  (2004). Nonparametric Weighted Feature Extraction  for Classification.  IEEE 
Trans. on Geoscience and Remote Sensing, vol. 42, no. 5, pp. 1096­1105, May. 
[5]  Kuo B.­C. and Landgrebe D. A. (2002). A covariance estimator for small sample size classification problems and 
its application to feature extraction. IEEE Trans. Geosci. Remote Sens., vol. 40, no. 4, pp. 814–819, Apr. 
[6]  Cover, T. M. and Hart, P. E. (1967). Nearest Neighbor Pattern Classification. IEEE Transactions on Information 
Theory, vol. 13, no. 1, pp.21­27. 
[7]  K. Alsabti, S. Ranka, and V. Singh. An Efficient K­Means Clustering Algorithm. PPS/SPDP Workshop on High 
performance Data Mining, pp. 34­39,March 1998. 
[8]  J.S. Roger Jang, C.T. Sun, and E. Mizutani, Neuro­Fuzzy and Soft Computing: A Computa­tional Approach  to 
Learning and Machine Intelli­gence. Prentice Hall, 1997.
