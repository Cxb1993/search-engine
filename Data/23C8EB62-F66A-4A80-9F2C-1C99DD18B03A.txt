 2
multiplier 最 佳 化 之 視 訊 次 取 樣 （ non-linear 
sampling）法則，在時間限制下濃縮出包含最多
preference 之摘要視訊，且同時考量相鄰 shot 播放
速度的平滑性。 
從實驗結果顯示，除了兩人畫面偵測與背景分
類之外，其它各項語意特徵之偵測均能得到良好的
結果。此外，由使用者主觀測試所完成之摘要視訊
結果可以看出，本系統提出之摘要法則除了能提供
觀賞摘要視訊上的舒適性，也能使得摘要內容具有
連慣性讓使用者容易瞭解影片內容。 
 
關鍵詞：MPEG-4、視訊摘要、視訊搜尋、使用者
喜好、合適性反饋、語意式分類、音/視訊資料融
合、音/視訊內容特徵分析、語意特徵分析 
  
 
Abstract 
This project is to develop functions of 
intelligent multimedia content analysis and 
applications for the Media Server/Gateway in Home 
network. In the future, it will be a trend to do 
multimedia search and streaming services, that 
incurs a need of audio/video server (for multimedia 
provision, search and transcoding) and networked 
multimedia interface (for connection between the 
server and the A/V terminal devices). With the 
increasing amount of multimedia data, not only the 
transmission and presentation of multimedia are 
focused, but also the creation, digitalization, storage, 
search, processing, management, and distribution of 
multimedia contents will form a broaden content 
engineering in the near future. 
This subproject would like to focus on the 
research of multimedia (audio/video) content 
analysis and its associated applications. In recent 
years, definition and description of audio/video 
content features are always the main concern of 
MPEG-7 standard. However, MPEG-7 will not 
define how these features are captured and computed 
since they vary with the presentation form, source, 
application domain, and application goal of 
audio/video signals. By integrating these extracted 
audio/video content features, we can make economic 
management and efficient exchange of digital 
information.  
Besides, we will integrate the extracted A/V 
features in applications including image/video/audio 
filtering, audio/video retrieval, audio/video 
summarization, image/video/ audio classification, 
audio/visual cryptosystem, and multimedia 
watermarking. 
In the first year, we developed the MPEG-4 
video feature analysis techniques and their associated 
applications, e.g., news video summarization system 
and sport video retrieval system. These two systems 
include the same video pre-processing, including: 1) 
motion vector correction and refinement, 2) I-frame 
motion estimation (3) P-frame DCT coefficient 
reconstruction. In news video summarization system, 
the strategy of “presenting the anchor video with 
news audio” is adopted. We first detect the anchor 
shots and keep the anchor audio therein. Then we 
classify all the news shots, and the ‘Lagrangian 
multiplier’ method is applied to adjust the video 
playback speed of each news shot. The adjusted 
playback speeds can maintain the equilibrium 
between the human’s perception and the consumed 
time for the goal of video summarization. Finally, the 
summarized news video can be constructed through 
the combination of condensed news video and the 
anchor shot audio. About the sport video retrieval 
agent, we use a two stage searching method based on 
query by example. At the first stage, we build an 
indexing structure by means of k-means clustering, 
and then the system uses this indexing structure to 
coarsely determine the “class” that the query video 
belongs to. At the second stage, we use dynamic 
programming method to find the most similar videos 
in that class. We also bring out two auxiliary agents, 
“user preference” and “relevance feedback”, to help 
users to find more suitable videos they want. In sum 
up, these two systems both go through the entire 
processing in the compressed domain so that the 
processing time can be shortened significantly. 
In the second yerar, we developed efficient 
mechanisms to identify specific events from videos and 
classify them semantically. Here we deal with news and 
baseball sports videos. A multi-modal information fusion 
technique integrating the closed caption, anchor’s speech, 
and visual information for TV news video classification is 
presented. By recognizing closed-caption characters from a 
video, phrases of single- and double-character are found 
for classification. On the other hand, content of the 
anchor’s speech signal is not recognized, but instead, 
labeled with pre-trained cluster means by using a 
level-building DP (Dynamic Programming) algorithm. 
Visual information, including the color and motion features, 
is extracted from the news footage part for classification. 
The above three information is individually classified by 
using statistical Relevance Factor (RF) or SVM (Support 
Vector Machine) technique, amounting to 7 different 
classifiers. Results of multiple classifiers are then 
combined to get fused outputs by using a modified 
Bayesian technique. Experiments show that the proposed 
fusion system is capable of increasing the classification 
rate by 14% with respect to the best single-modal system. 
Our Bayesian fusion rule also outperforms the best Product 
rule presented in [12] by 3%. 
In baseball game, an event is defined as the portion 
of video clip between two pitches, and a play is defined as 
a batter finishing his plate appearance. A play is a 
concatenation of many events, and a baseball game is 
formed by a series of plays. In this project, only the event 
happened in the last pitch of a plate appearance is detected. 
It is then semantically classified to represent the 
corresponding play by using an algorithm integrating 
caption rule-inference and visual feature analysis. Our 
proposed system is capable of classifying each baseball 
play into eleven semantic categories, which are popular 
and familiar to most of the audiences. In an experiment of 
260 testing plays, the classification rate achieves up to 
87%.  
 4
 
我們在音/視訊分析處理及應用方面共計發
表了 9 篇會議論文[19-27]，部分成果則仍在整理
中。 
 
四、第一年計畫成果 (92 年 8 月~ 93 年 7 月)  
新聞視訊摘要系統與運動視訊搜尋系統 (本部分
節錄自 NSC 92-2213-E-194-037 成果報告) 
   
4.1 視訊之一致化前處理 
   
    視訊的一致化前處理，乃是由於壓縮領域下的
畫面特性不同-- I-frame 缺乏動態資訊，而 P-frame
缺乏完整的空間資訊 (DCT 係數)。因此透濄一致
化的前處理，使不同壓縮型態的畫面都能擁有一致
化的資料內容，將可增進後續的視訊內容特徵分析
的方便與完整性。 
 
4.1.1 I-frame 移動向量估測 
 
根據圖 3，B1 的 FMV 是對 P1 所形成的移
動向量，而 B2 的 FMV 為 B2 對 P1 的移動向
量。利用線性外插，即可推得 I2 對 P1 的 FMV 的
可 
B1 B2 I2 
B1 
B2 
B1、B2 之向
量差 D1 
估測 I2
的 FMV 
B2 
D1 
 P1    B1     B2    I2 
 
圖 3 利用 B1與 B2的 FMVs 來估測 I2的 FMV 
 
能移動向量。如圖 3 所示。需注意的是，在此 B1、
B2、及 P1 的 FMVs 均屬於不同畫面但相同位置
的 MBs 所有。 
 
4.1.2 Motion vector 修正 
 
基本上，基於局部剛性物體的假設，除了是屬
於物體邊緣的  boundary MB 外，每個  MB 的 
MV 應該是與其周圍相鄰 MB 的 MV 相似。而
即使是 boundary MB，其 MV 亦會與其中一半相
鄰 MB 者 (即物體或背景)相似。在此前提下，比
較每個 MB 與周遭 MBs 其 MV 的差異性。其
差異性大者，則我們可以判定這個 MB 的 MV 
是有問題的。 
我們修正的方法是採多數決的方式。當發現某
一 MB 的 MV 出現問題時，我們將企圖找出周
遭正確的 MB 移動向量，取這些向量的平均值來
修正替代其原始的移動向量。詳細技術請參考相關
產出之碩士論文[4,5]。 
4.1.3 P-frame 的 Intra-coded DCT 係數重建 
 
我們的技術直接分析 MPEG 視訊內容的運
動向量 (MV) 與 DCT 係數，在此，我們將不進
行 image domain 的移動補償，因為此舉會導致大
量 的 IDCT 運 算 。 本 計 畫 引 用 Chang and 
Messerschmitt [1]提出在 DCT domain 進行移動補
償的方法，可直接有效的求得 P frame Intra-coded 
DCT，因此有較傳統分析原始影像資料方式更快速
的優點。 
在此需特別注意的是，由於 MPEG 標準中，
存在著非整數像素（Half-pixel）的移動向量，因
此當我們透過上述的流程獲取「整數像素」之移動
補償的 DCT 係數後，需接著修正 Half-pixel 移
動向量的影響。在 Spatial domain 下，就單一像
素而言，Half-pixel 移動補償乃是利用周圍的 2 個
或 4 個點來計算影像內差；因此在 DCT domain 
下，勢必得利用參考畫面中四個整數像素移動向量
所重建的 Blocks 來進行運算，如此的運算流程將
耗費大量的運算代價。因此本計畫參照  M. 
Ghanbari [2] 所提出的「Linear Filtering」方法，簡
化如此繁複的計算過程。 
 
4.2 應用一: 新聞視訊摘要系統 
 
經由前述前處理將資料轉換後，本計畫的新
聞視訊摘要法則將以「時間分配」為主要原則。由
於觀眾各有不同的喜好，因此本計畫傾向保留完整
的影片(外景)內容，僅透過特徵計算的方式，達成
不同景間的時間分配；並搭配「聽主播、看新聞」
的原則讓使用者在無損內容理解的情況下，短時間
來瀏覽整段自動摘要的影片。圖 4 為新聞摘要系統
架構示意圖，而在 4.2.1~4.2.3 節則對此摘要系統
架構有較詳盡的介紹。 
anchor
k
i
i TT =∑=1
 
圖 4 新聞摘要系統架構示意圖 
 6
的時間長度，來檢驗本摘要系統在調整播放速度，
而不減損影片內容的原則下，能夠節省多少的影片
觀賞時間，藉由如此比較的結果，當可分辨出本計
畫所設計的時間調整流程之良窳，分析 12 則新聞
後的實驗結果如表 3 所示。摘要比率為實際摘要長
度除以原始影片長度。 
 
表 3 原始影片與摘要影片之時間比較(摘要比率) 
新聞編號 1 2 3 4 
摘要比率 0.267 0.207 0.201 0.261
新聞編號 5 6 7 8 
摘要比率 0.261 0.242 0.258 0.115
新聞編號 9 10 11 12 
摘要比率 0.295 0.246 0.336 0.212
 
4.3 應用二 : 運動視訊搜尋系統 
 
4.3.1 視訊資料庫收集與索引結構建立 
 
Feature 
computation 
Motion feature 
Color feature 
Shape feature 
Texture feature 
Video database
Supervised labeling
Unsupervised clustering 
(k-means) 
Save cluster 
center info. 
Video database training
Link file info. 
in one cluster 
 
圖 5 視訊資料庫訓練與建立流程圖 
 
在這個應用中，我們的視訊屬於 7 種不同的
運動類視訊 (例如高爾夫球、撞球、相撲或籃球等
等)，每一種類則包含個數不一的視訊片段。當使
用者以某一類運動視訊範例進行查詢時，本系統能
夠將資料庫中與該視訊類似的影片搜尋出來 (必
定屬於同一大類運動影片中的類似場景)。由於資
料庫中的運動視訊種類繁多(包含不同運動種類，
或相同運動種類而不同場景)，所以我們在進行視
訊資料分類方面先以人工方式將視訊標記上不同
的大類記號(例如高爾夫球、或籃球等等)。然而我
們可以發現，即使屬於相同一大類的視訊資料，其 
“空間/時間” 特徵亦有所不同，因此容許我們再分
成許多小類。以相撲類型為例，可分為兩人對決、
場外觀眾席、相撲選手特寫、以及正決鬥中等等不
同小類。因此在此，我們是利用 k-means clustering 
的方法，再將各大類型的樣本 shot 各自區分為數
種小類 [3]，算出各小類的叢集中心點並儲存。 
 
4.3.2 搜尋機制 
 
 
圖 6 視訊搜尋流程圖 
 
圖 6 是搜尋機制的流程圖。使用者載入的視
訊先經過壓縮視訊轉換及視訊特徵擷取 (共包含 
shape、motion、texture、color 等十數個特徵，均
為 compressed domain 計算出來，細節請見 [6])
後 ， 便 進 行 將  Frame-based 特 徵 轉 換 到 
Shot-based 特徵的計算。接著，便進入第一階段，
將參考視訊的特徵向量與上述計算叢集中心點數
值計算距離，我們採用的方式是 L2 norm 距離分
析的方式，算出的距離越小，表示欲搜尋的參考視
訊資料與該叢集越相似。 
第二階段便要針對排序在前的次類別  (叢
集)，逐一比對其所包含的資料庫 shot 片段。由於
一個大類別內視訊資料仍分為多個次類別，所以我
們利用第一階段所求得的結果，找出與排名第一相
同類別的所有次類別，作為第二階段搜尋的依據。
在第二階段搜尋後，再根據距離的大小值來排定相
似度順序，找出相似的視訊片段。 
在視訊資料庫中，每段視訊資料其畫面張數
的不同可能造成擷取出的特徵數不同，便會遇上兩
段視訊資料無法比對的窘境。除了畫面張數不同的
考量外，當兩段視訊資料內容類似，只是其畫面播
放速率不同時，我們亦希望仍然可以將其比對出
來。因此我們使用動態規劃法進行比對 [4]。 
我們建立一個 multi-stage 架構來進行動態
規劃比對，橫向每一個 stage 代表參考視訊資料
的畫面、縱向每一個 node 則為資料庫中待測視訊
資料的畫面。動態規劃法中計算三種 cost，包括 
node cost、edge cost 及 path cost。node cost 如圖
7 中為橫向畫面其特徵向量值與縱向畫面特徵向
量值之差異，如左上角斜條紋 node cost 為參考視
訊資料中第一張畫面的特徵向量值減去資料庫中
待測視訊資料中第一張畫面的特徵向量值再取絕
對值。Edge cost 則是指連結兩節點間的畫面距離
花費。在動態規劃法比對中，每個節點皆須記錄從
第一張 frame 到該點本身所累積的最小  path 
cost，以及此最小 path cost 是由前面哪一個節點
連接過來。對每個 stage 的每個節點都作如此的
計算到最後的 frame N，我們可以得到一個從 frame 
1 到 N 最小的 path cost。然後，經由反推，即可
 8
5.1.1 字幕資訊新聞分類 
新聞字幕 (caption) 資訊對新聞分類是相當
重要的，因為字幕帶有人類主觀意識的文字描述。
要取出視訊中的文字資訊必先找出文字影像的正
確位置，本計畫中以  color 特徵找出  caption 
frame，以 edge histogram 來偵測出文字影像的正
確位置，其方法流程如圖 8 所示。對於文字影像的
特徵擷取，本計畫採用 OCR 常用的三大特徵以
表示一文字影像的特性。在文字識別上，引用 
subspace 辨識方法 [7,8]，主要是利用 test sample 
到 subspace 基底的投影量來完成辨識工作。辨識
取得文字代碼後，即可利用此資訊來做文字新聞分
類。我們引用關聯因數 (relevance factor) 分類法 
[13] 和 SVM 分類器來完成文字新聞分類。除此
之外，以往文字新聞分類的資訊統計都以 single 
character 的方式為基本統計單位，對於較有鑑別能
力的字組，卻沒辦方發揮其區分效果。因此，本計
畫開發 double characters 的文字資訊統計方式。
在字幕新聞的分類中，我們可取得四個 mode (例
如， single character/double character 分別配合 
RF/SVM 分類方法) 的分類結果，彼此之間的分
類能力能達互補功效，以利新聞節目分類的後續整
合。 
 
 
圖 8 文字切割方法 
 
5.1.2 語音資訊新聞分類 
 
在新聞內容呈現時，語音內容是另一個重要
的資訊。由於外景語音通常夾雜一些音訊，所以本
計畫以棚內主播之語音作為新聞分類的資訊之
一。近幾年來大部分語音辨識學術研究都以 
HMM 為主，但是主播話句種類繁多，完善的 
HMM mode 不易獲得，所以我們以動態時間校準
（Dynamic Time Warping）來做語音比對。而且為
了克服語音連續問題，我們採用 Level building 
DTW 比對方式[9]，此種比對方法只需要單音資訊
就可以完成連音比對，並且告知使用者音節斷點。
我們分析語音訊號最主要的目的並不是想把主播
語音轉換成文字詞句，主要目的是想要利用主播的
聲音資訊來做新聞分類。因此，我們配合 k-means 
clustering，以 labeling 的方式來分析主播語音。
利用此方法，我們不需把收集的語音依字詞分開，
只需 training 語音辨識系統以得到音節比對樣版
（template）。 
對於語音新聞分類，我們仍用關聯係因數 
(relevance factor) 分類法 [13] 和 SVM 分類器來
完成語音新聞分類。在語音新聞的分類中，我們可
取得 2 個 mode (分別採用 RF/SVM) 的分類結
果。圖 9 顯示語音端點偵測及特徵擷取流程圖。 
 
 
圖 9、語音端點偵測及特徵擷取流程圖 
 
5.1.3 視覺資訊新聞分類 
 
本計畫取用棚外視訊，雖然視覺資訊對新聞
分類助益不大，但是某些類別的特殊畫面是可以藉
由視訊特徵來區分的。本計畫參考文獻 [10,11] 使
用  color histogram，motion of activity，camera 
motion 及 object motion 來表現一張畫面的視覺特
徵。在視覺新聞分類上，本計畫使用 SVM 分類
器。由於分類的基本單位為一張 frame ，因此以
所有畫面多數決的方式來決定視訊的新聞類別。 
 
5.1.4 多模式資訊整合 
 
目前大部分的多媒體分類研究都以單一資訊
為主，因此本計畫中整合多媒體各方面的資訊，以
提高最後的分類效果。整合新聞字幕、主播語音、
外景視訊的分類資訊是本計畫的重點，如圖 10 所
顯示。我們利用這些多媒體資訊、配合 SVM 分
類器和關聯因數 (relevance factor) 分類法可得到 
7 個 modes 的分類資訊。在整合的技巧上，我們
引用一種 multiple sensors fusion 的整合方法 [14] 
來完成這 7 個 modes 的資訊整合。此種整合方
式主要原理，主要是根據 Bayesian criterion 求出
每一分類器對分類結果的信心度，利用此信心指數
來評估每一分類器的分類效果，以作為資訊整合依
據。此種整合方法的特色是，每種資訊在整合過程
中並不會相互影響 (如:特徵一致化)，並且 7 個 
modes 的分類結果可以獨立存在。這樣的特色很
適合多媒體資訊整合，因為字幕、語音、視訊資訊
性質差異大，所以我們才引用此種 sensors fusion 
Zero-crossing 
rate
 
 
Speech 
segmentation 
Raw data 
Frame energy
MFCC
Feature extraction 
Emphasis 
function 
Hamming 
window 
FFT  
(512 points) 
Triangular 
filter banks 
DCT
 10
當然不同電視台須採用不同的字幕 template 以便
進行偵測。得到這些字幕的資訊後，若判定為一個
打者的最後一個事件，則利用棒球比賽的規則，可
以由這些字幕找出可能發生的 play，如圖 13 所
示。舉例來說，若前一狀態為兩好球，而這一狀態
好壞球數被重新設定 (reset)，且三個壘包的資訊
都沒有變化，則我們可以判定可能的 play 有{三
振、內野封殺、外野接殺}。 
 
5.2.3 利用視覺特徵分類 
 
關鍵鏡頭要具有「對同一類別而言，該鏡頭
擁有最大的相似性以及一致性；而對不同的類別而
言，該鏡頭又具有明顯的可區分性」的特性，所以
我們訂定投捕鏡頭的下一個鏡頭為關鍵鏡頭。在本
計畫中，我們針對關鍵鏡頭進行視覺特徵的抽取，
並利用二階段分類法進一步將 play 分成內野型、
外野型、未擊出型三類。其中，第一階段先判斷為
擊出型和未擊出型；第二階再將同屬於擊出型者分
為內野型與外野型，如圖 14 示。我門們在兩個階
段都是採用 SVM 分類器，輸入的視覺特徵由於
報告空間有限，請參考 [15]。當然，既使用 SVM 
為分類器，我們必須準備足量的 training set。 
 
 
圖 11、字幕處理之流程圖 
 
 
 
圖 12、棒球視訊中字幕資訊的一個例子 
 
5.2.4 結合字幕與視覺特徵來判斷確定之語意 
 
由字幕資訊可以推敲出 1~3 個可能的語意
play。若可能的 play 只有一個，那麼該 play 即為
確定的 play。若可能的 play 有兩個，經由觀察發
現，必分屬於三大型態 (未擊出、內野及外野) 中
的兩種；若可能的 play 有三個，則必分屬於此三
大型態。因此，字幕資訊再搭配視覺特徵必可判斷
出最後確定的 play。舉例來說，若由字幕判定可能
的 play 有{三振、內野封殺、外野接殺}，而由視
覺特徵分類出的型態為未擊出型，則我們可以判定
確定的 play 為{三振}。 
 
 
圖 13、利用字幕資訊判斷「可能 play」的樹狀圖 
 
 
 
 
 
 
圖 14、兩階段視覺特徵分類示意圖。第一階段判
斷為擊出型和未擊出型；第二階再將同屬於擊出型
者分為內野型與外野型。 
 
 
5.2.5 實驗結果 
 
視訊畫面解析度為 720×480 pixels，frame rate
為 30fps。本計畫中，我們總共收集了 305 則棒球
視訊，其中 45 則是訓練分類器時使用。 
至於 11 種 play 的分類正確率，我們將結果
列舉如表 7。表 7 中，只要有被拿去當訓練用的棒
 12
6.1.3 使用者介面（User Interface） 
 
使用者與系統互動時須要有一個橋樑作為溝
通，其主要目的在於讓使用者容易地設定其對摘要
影片的要求及喜好。如何提供一個方便指定喜好的
介面，是系統設計上的一項考量，故本文不考慮讓
使 用 者 自 行 設 定 low-level 特 徵 ， 而 採 用
semantic-level 特徵挑選的方式，作為使用者與系
統溝通的橋樑，如圖 16 所示。 
 
 
 
圖 16 使用者介面示意圖 
 
使用者與系統互動時，經由系統提供的介面，
使用者可以挑選想要的語意特徵並指定播放上的
設定。本系統提供予使用者可選擇的喜好項目，如
表 9。 
 
y 語意特徵： 
1. 人物的部份，可提供數量上的選擇、是否有
說話交談、及是否為人物特寫鏡頭。 
2. 指定的特殊事件，例如，有是否含有移動的
物體、是否有爆破場景、是否為 Zoom in（因
為一般 Zoom in 可能為影片中重要部份）。 
3. 背景上的指定，分為室內與室外。 
y 播放上的設定： 
4. 摘要時間的限制。為配合使用者需求，可讓
使用者自己指定摘要影片的播放時間長度
（或張數），此項目幾乎為一般摘要系統中
基本的選項。 
5. 相鄰 shot 間播放速度平滑性之指標。由於本
系統採用影片次取樣方式作為摘要呈現的方
式，為了避免 shot 間播放速度變化過大，本
系統提供使用者設定播放速度平滑性的重要
性。使用者可以給定 0 至 3 之間的整數值，
其值愈高，表示愈加強調播放速度之平滑
性，意即 shot 間播放倍數相差越小。 
 
表 9 使用者可挑選及設定的喜好項目表 
人物 數量: □1 人   □2 人    
□說話   □特寫 
特殊事件 □移動物體  □爆破  □Zoom in
背景 □室內  □室外 
時間限制：         張畫面 
相鄰 shot 間播放速度平滑性之指標（ρ）：     (0 
~ 3) 
 
6.2 視 訊 語 意 特 徵 分 析 （ Semantic Feature 
Analysis） 
 
在場景分割後，我們得到一序列的 shot，然後
再分析其語意特徵，以此語意特徵分析結果作為提
供使用者對喜好的選擇。以一般影片來說，人物的
出現很容易就吸引了人的目光，所以我們能以人臉
的偵測來判斷畫面中是否有人物的出現。也可以再
考慮臉部所占畫面的比例或配合 Zoom in 偵測來
判斷其是否為特寫景頭，或以人臉偵測配合影片字
幕偵測來判斷人物是否在交談。除人物外，移動的
物體（如動物、交通工作或特殊的物體等），往往
也是影片中吸引觀看者目光的地方。除上述人物、
特寫、交談、移動物體外，對於緊張剌激類影片，
爆破場面則是重要且吸引人的畫面。最後我們再配
合背景的偵測來判斷影片畫面是屬於在室內或戶
外。 
根據上述說明，本系統的語意特徵分析包含以
下六項： 
1. 攝影機變焦偵測（Zoom Detection） 
2. 影片字幕偵測（Text Detection） 
3. 爆破偵測（Explosion Detection） 
4. 人臉偵測（Human Face Detection） 
5. 移動物體偵測（Moving Object Detection） 
6. 背景分類（Background Classification） 
在語意特徵分析上，攝影機變焦偵測、影片字
幕偵測、背景分類等項目是以每張畫面偵測為基
礎。人臉偵測，除了在每張畫面執行外，並且考慮
時間軸上的修正。移動物體偵測、爆破偵測，則以
考慮一段時間內的變化為偵測基礎。最後以所偵測
事件佔一個 shot 內張數的比例來作為該 shot 中包
含某一事件重要性的指標，以應用於後續的摘要處
理。 
 
6.3 基於使用者喜好之視訊摘要機制 
 
對於使用者而言，low-level 特徵（例如，color、
motion、texture、shape 等）只是量測數值的大小，
使用者無法具體的指定 low-level 特徵的範圍來表
達個人特殊的喜好。本報告前面已經介紹視訊之語
意特徵分析，其目的在於讓使用者透過語意式特徵
的選取，方便且有意義地指定個人的喜好項目。例
如，本系統可以提供使用者指定「人物」和「對話」
兩個喜好項目，藉以取出視訊中包含人物對話之
shot，進而濃縮得到摘要視訊。在實際應用層面
上，基於低傳輸頻寛、短播放時間和低功率運算平
台等因素，如何在有限的時間下，產生使用者接受
度最高的摘要視訊是有必要的。 
在 文 獻  [16] 中 ， 首 先 基 於 使 用 者 的
preference 來訂出每個 shot 的重要性（relevance）
或 weight。其次，為了產生高 relevance 和緊湊的
摘要視訊，可以依據 weight 的高低依來序選取
 14
少珍貴的 shot。假設第 j 項 preference 在整部影片
中出現的機率為 Pj，其資訊量則為 log2(1/Pj)。圖
18 所示為 Pj相對於 log2(1/Pj)的曲線圖，由圖可知
當出現機率越少，表示其資訊量越高。 
P 
log2(1/P) 
 
圖 18 不同機率 P 相對於其資訊量 log2(1/P)。 
 
對於每個 preference 項目而言，我們以 “具代
表性” shot 的數量來計算其出現的機率，如式（6.1）
所示，其中 ijW 為第 i 個 shot 第 j 個 preference 項目
的 confidence factor。當 confidence factor 值大於 0.5
時，我們認定其具代表性。求得各項 preference 的
出現機率後，相對也可求得各項 preference 所含的
資訊量。  
 
S
N
i
i
j
j N
WT
P
S∑
== 1
)(
       （6.1） 
其中，
⎩⎨
⎧ ≥=
   otherwise  ,  0
0.5 if  ,  1
)(
i
ji
j
W
WT ， SN 為整個視訊影片
中 shot 的總數。 
對於使用者而言，他所挑選的 preference 才是
真正代表個人的偏好，因此一個 shot 內包含所指
定 preference 項目的多寡也是決定 shot relevance
的因素之一。當使用者挑選了多項 preference 時，
其中某一項 preference 所含的資訊量越高，表示對
使用者更形重要。因此包含此項 preference 的
shot，對使用者而言勢必為重要的 shot。綜合上述
考量，我們採用（6.2）式來衡量一個 shot 的
relevance。 
 
shot relevance : ⎟⎟⎠
⎞
⎜⎜⎝
⎛= ∑
∈Gj j
i
ji P
WSR )1(log2 （6.2） 
其中，i 為 shot 的索引，j 為 preference 的索引，G
為使用者挑選的 preference 項目的集合。 ijW 為第 i
個 shot 具備第 j 個 preference 特性的 confidence 
factor，而 )1(log2
jP
為該 preference 的資訊量（可視
為一種 weighting），因此（6.2）式可說是一個 shot
擁有的平均 preference。 
Shot relevance 為在次取樣倍數分配時的考量
之一。Shot relevance 越高的 shot 應分配到較低的
次取樣倍數，也就是比較接近正常播放速度。 
 
6.3.2 摘要視訊的連貫性 
 
考量視訊連貫性的原因有兩點。其一，在產生
摘要影片時，如果只單純按 shot 的 relevance 高低
來決定是否納入或排除在摘要影片中，則在播放合
成出的摘要影片時觀看者可能會感到內容上的不
連貫性。在此考量下，本系統有限度地將時間軸上
與 relevance shot 較為相鄰的 shot 亦納入考量範圍
內。如圖 19 所示，雖然 shot 2 與 shot 4 為 relevance
較高的 shot，但在摘要處理上我們亦考慮將 shot 1、 
shot 3、shot 5 一併納入次取樣倍數分配上的考量。
這些 relevance 較低的 shot，則可以利用較快的播
放倍數讓使用者快速地瀏覽。 
其二，本系統以改變次取樣倍數的方式做為摘
要呈現，不同的 shot 分配不同的播放倍數。然而
我們也要避免觀賞者在觀看時覺得忽快忽慢，對播
放上感到不適，所以我們在最後摘要處理時，同時
考量相鄰 shot 前後間播放倍數差異的關係。如圖
19 所示，shot 9 ~ shot 13 為時間上相鄰且具
relevance 的 shot，故在次取樣的倍數給定上，須考
量到其播放速度的差異。  
 
 
圖 19  時間軸上各個 shot 所對應的 relevance 
 
6.3.3 Shot Distortion  
 
依據各個 shot 內容性質上的差異，各個 shot
所能容忍的次取樣倍數也不相同。此外，本系統利
用 preference 來評量一幅畫面的 relevance，因此次
取樣後對於 preference 的影響也是必需加以考慮
的。 
當一個 shot 次取樣 k 倍後，其所含的內容只
剩下原來的 1/k，故在次取樣前後，視訊內容所產
生的差異性，我們稱之為失真。可預期地，摘要視
訊整体失真量要愈小愈好。在以 keyframe 為呈現
方式的摘要方法中，其目標為挑選出一組
keyframes，使其相對於原視訊而言的整體失真是
最小的。而在衡量失真上，是採用重複 keyframe
的方式來插補畫面於被挑選的 keyframe 間（如此
可得到與原視訊相同的長度），再衡量其失真。本
系統衡量次取樣前後的失真時，也是採用文獻 [18] 
類似的方法，將次取樣後的空白畫面以插補相同畫
面方式來衡量其失真度的大小。 
 16
temporal cost。在一組給定的次取樣倍數{ ik }情形
下，temporal cost 如下： 
∑
=
− −=
S
S
N
i
iiNT kkTCFkkkC
2
121 )(),...,( （6.7） 
其中 zxxTCF κ−= 25.1)( ， zκ 為一常數，在 zκ =12
情況下，不同次取樣倍數差與其相對應值 )(⋅TCF
如圖 21 所示。 
 
TCF(.) 
| ki-1 - ki |  
圖 21 不同次取樣倍數差| ki-1 - ki |的 TCF(.)函數值
曲線圖（ zκ =12） 
 
綜合上述，同時考慮 shot relevance、摘要視訊
連貫性和 shot distortion 三個因素可以得到一個視
訊整體的 cost function VC 如下（6.8）式。在考慮
content cost 與 temporal cost 合併計算時，基本上兩
者計算衡量的單位並不一致，故我們利用γ 來調整
temporal cost 的權重，如此在做最佳化處理時才能
同時考量到兩者，而不會偏向某一方。γ 值亦可以
作為內容連貫（CT）相對於 CC的權重。γ 值的設
定將在下一小節作說明。 
),...,(),...,(),...,( 212121 SSS NTNCNV kkkCkkkCkkkC ⋅γ+=
         （6.8） 
故本文視訊摘要之最佳化，即在所給予的播放
時間限制下，找出一組{
SN
kkk ,...,, 21 }的次取樣參
數使得 cost function CV達到最小。此最佳化問題可
以表示如下： 
( )⎟⎟⎠
⎞⎜⎜⎝
⎛ −×γ+×∑
=
−==
S
Si
N
i
iiiiiNik
kkTCFkDSR
1
1}~1|{
)()(min
K
（6.9） 
subject to  T
k
NSN
i i
i =∑
=1
      
其中，i 為 shot 的索引，Ni為第 i 個 shot 所包含的
frame 數，T 為畫面張數的限制（可換算為時間限
制）。 
 
6.3.4.3 次取樣參數最佳化 
 
本系統採用 Lagrangian multiplier 最佳化方法
將有條件限制的最佳化問題轉換為無條件限制的
最佳化問題，即： 
 
( ) ⎟⎟⎠
⎞⎜⎜⎝
⎛ λ+×γ+×∑ ∑
= =
−==
S S
Si
N
i
N
i i
i
iiiiiNik k
fkkTCFkDSR
1 1
1}~1|{
)(),()(min
K
      （6.10） 
在求解（6.10）式時，首先給定一個 λ 值，對
所有的 shot 找尋一個符合（6.10）式的最佳次取樣
倍數 )(* λK ，再將 )(* λK 代入∑
=
SN
i i
i
k
N
1
檢查是否等於
目標畫面張數 T。若相等或極為相近則目前這組解
即為符合條件式的最佳解 )( ** λK 。反之，則改變 λ
值，重新搜尋直到找到合於條件式之解。給定一 λ
值後，我們採用 Dynamic programming（DP）的技
術來搜尋最佳的次取樣倍數。 
 
6.4 實驗結果 
 
為了評估本系統提出之視訊摘要系統，目前我
們收集 7 部電影（movie）和 3 部影集（drama）進
行相關實驗。這些測試視訊的相關資訊如下： 
1. 壓縮格式：MPEG-4 
2. GOP：12 
3. GOP 結構：IBPBPBPBPBPB 
4. 畫面大小：352×288 pixels 
實驗平台為以 Windows XP 為作業系統之個
人電腦，CPU 為 Pentium-Ⅳ 3.0GHz，記憶體為
512MB，以 Microsoft Visual C++所撰寫的程式碼。 
在下列實驗中，系統的播放倍數設定上，我們
採用以下六種播放倍數 1、2、4、8、12 及∞（∞
倍表示此 shot 直接跳過），也因此將視訊壓縮成
上述 GOP 結構。在播放上依據視訊影片壓縮的結
構，選擇性跳過 frame，或者是解碼部份 frame，
但不播放，以此方法進行不同次取樣倍率的播放
（如圖 22）。如果視訊的 GOP 結構為一般常見
IBBPBBPBBPBB，可將播放倍數設定為 1、3、6、
12 及∞。如此設定在 6 倍快播時需解碼 I、P1、P2，
但只播放 I、P2 畫面即可，其它倍數（1、3 及 12）
快播時只解碼需要播放的畫面即可。如此可有五種
的快播模式，又可達到即時轉碼傳輸。 
本章的實驗測試主要分成 3 個部份：（1）視
訊分割，（2）preference 偵測，（3）摘要影片效
果評量。透過這些實驗測試來分析評估本系統摘要
系統的優劣。在進行測試前，我們首先定義評估標
準： 
false alarms = 無事件發生，但被誤判為事件發生 
missing = 有事件發生，但無法偵測出事件發生 
 
6.4.1 摘要影片效果評量 
 
  本系統提出基於使用者喜好的摘要機制，同一
部影片依據使用者的喜好不同，可產生出不同的摘
要影片。而對於摘要影片內容的品質而言，無法有
效的以公式來衡量，故仿效廣用於音訊品質的評斷
的 MOS（Mean Opinion Score）評分方式，以檢驗
本系統影片摘要的效果。本文並訂定以下的主觀式
 18
 
(a)
 
(b)
 
(c) 
 
(d) 
 
(e) 
圖 23 不同 ρ之下各個 shot 所對應的次取樣倍數關
係圖，(a)各個 shot 所對應的 shot relevance，(b) 
ρ=0 之下所對應的次取樣倍數關係圖，(c) ρ=1 之
下所對應的次取樣倍數關係圖，(d) ρ=2 之下所對
應的次取樣倍數關係圖，(e) ρ=3 之下所對應的次
取樣倍數關係圖。 
 
 
 
 
 
( a ) shot 7 
 
( b ) shot 8 ( c ) shot 9 
( d ) shot 10 
 
( e ) shot 11 ( f ) shot 12 
( g ) shot 13 
 
( h ) shot 14 ( i ) shot 15 
 
圖 24 視訊影片內容畫面，（a）shot 7，（b）shot 8，
（c）shot 9，（d）shot 10，（e）shot 11，（f）
shot 12，（g）shot 13，（h）shot 14，（i）shot 15。
當 ρ= 0 時，播放順序為（b）→（d）→（g）→
（i）。當 ρ= 1 時，播放順序為（b）→（c）→
（d）→（e）→（g）→（i）。當 ρ= 2 或 ρ= 3
時，播放順序為（a）→（b）→（c）→（d）→
（e）→（f）→（g）→（i）。 
 
 
七、參考文獻 
 
[1] Shih-Fu Chang, Member and David G. 
messerschmitt, “Manipulation and Comositing of 
MC-DCT Compressed Video” IEEE Journal on 
selected areas in communications, Vol.13, No.1, 
January 1995. 
[2] P. A. A. Assuncao and M. Ghanbari, “A 
frequency-domain video transcoder for dynamic 
bit-rate reduction of MPEG-2 bit streams,” IEEE 
Transactions on Circuits and Systems for Video 
Technology, Vol. 8, No. 8 , pp.953 -967, 1998. 
[3] D. A. Adjeroh, M. C. Lee and I. King “A 
distance measure for video sequence similarity 
matching,” Proceedings of International 
Workshop on Multi-Media Database 
Management Systems, pp.72-79, 1998 
[4] R. Mohan, “Video sequence matching,”
Proceedings of IEEE International Conference 
on Acoustics, Speech, and Signal Processing 
1998, Vol. 6 , pp.3697-3700, 1998 
[5] 賴駿銘，結合空間與動量特徵分析之 MPEG-4 
新聞視訊摘要系統，國立中正大學電機所碩士
論文 
[6] 陳信宇，基於壓縮領域特徵之 MPEG-4 視訊
檢索系統，國立中正大學電機所碩士論文 
[7] Dr. E. Oja, Subspace Method of Pattern 
Recognition, John Wiley & SONS INC., New 
York, 1983. 
ρ= 0 
Shot number 
次取樣倍數 
ρ= 1 
Shot number 
次取樣倍數 
A
Sh t 49
ρ= 2 
Shot number 
次取樣倍數 
A B
ρ= 3 
Shot number 
次取樣倍數 
BA
Shot 
relevance 
Shot number 
Explosion 
