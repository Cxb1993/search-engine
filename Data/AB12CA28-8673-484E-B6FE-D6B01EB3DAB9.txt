精簡報告 共 8 頁 第 2 頁
流暢度導向之統計式機器翻譯系統解碼模式
Fluency-Based Decoding Models for Statistical Machine Translation
計畫編號：NSC 97-2221-E-260-023
執行期限：2008 年 08 月 01 日至 2009 年 12 月 31 日
主持人：張景新 (jshin@csie.ncnu.edu.tw)
執行機構及單位名稱: 國立暨南國際大學資訊工程學系
計畫參與人員：張景新，歐懷文、秦祥智、邱世杰、范睿凱、張志正、陳右宗
(國立暨南國際大學資訊工程學系)
摘 要
目前雖有許多和 SMT 相關的研究，翻譯的
流暢度卻一直不是很理想。以 IBM 所提的
BLEU Score (Papineni, 2002) 來評估，
在英翻中的這個特定領域，分數僅在
0.21~0.29 之間。這樣不流暢的翻譯品質
是很難讓使用者接受的。其中一個關鍵就
在於, 解碼步驟以隨機的方式來產生目
標句, 而在產生過程中, 僅參考局部有限
的詞彙, 因此常無法產生符合目標語文
法的候選句. 本研究的主要目的, 即希望
能以搜尋大量語料及網頁的方式, 尋找
翻譯的候選句, 以取代傳統 SMT 用隨機
方式產生候選句的 decoding 問題. 這些
搜尋到的候選句基本上都是流暢的, 因
此可望有效改善翻譯句子的流暢度。
長久以來，SMT 的改善方式皆是在
Translation Model 中做改善。然而，由
於 SMT 模型的表達能力 (expressive
power) 非常有限，透過 word-to-word
或 phrase-to-phrase 轉換，再加上局部
位置的 re-ordering 並不能有效產生流
暢的目標語。尤其是許多目標語特有的詞
彙或詞素，並無法有效地透過這樣的機制
產生。因此，我們認為應該跳出 SMT 固有
的限制，尋找其他增進流暢度的方法。尤
其是 Decoding (Searching) 這個較少被
人注意的部分，應該更受重視。初步的實
驗顯示，這種以搜尋取代隨機產生候選句
的模式，可以當成翻譯系統跟自動譯後修
繕系統的前端，有效改善翻譯的流暢度。
關鍵詞: 流暢度, 解碼模式, 搜尋誤差, 機
器翻譯
Abstract
While there are many SMT researches for
the past tens of years, the performances are
far from satisfactory. In translating English
to Chinese, for instance, the BLEU Scores
(Papineni, 2002) range only between 0.21
and 0.29. Such influent translation quality
is unacceptable for most human readers.
One key issue that causes disfluency is the
decoding process. Most decoding process
regard target sentence generation as a
stochastic process, and only local context is
consulted while decoding. Therefore, the
target sentences generated in this way are
usually not fluent. The current work
proposes to search fluent translation
candidates from large corpora or from the
web, instead of using traditional decoding
methods to generate the translation
candidates. Since the large corpus and the
Web are produced by native speakers, the
target sentences thus searched are most
likely fluent.
The main approaches for improving
classical SMT had put much energy on the
Translation Model (TM). Unfortunately,
the classical SMT models have very low
expressive power. The application of
word-for-word or phrase-to-phrase
translation and a little bit local re-ordering
might not generate fluent target language
sentences. In particular, many target
精簡報告 共 8 頁 第 4 頁
Unfortunately, to make the computation
feasible, the classical SMT models have very
low expressive power in the Translation Model
(TM) component. It models the translation
process in terms of the fertility probability,
lexical translation probability and distortion
probability. In comparison with traditional view
that the translation process involves
complicated lexical transfer, structure transfer,
and target morphological generation, the three
kinds of probabilities in the TM have very
limited expressive power for describing such a
translation process. The assumption to generate
good candidates with such a TM is therefore
not quite true.
To take into account the target language
grammar, and thus the fluency of the candidate
target sentences, a word-based 3-gram model is
usually used as the language model (LM), so
that target sentences with frequently observed
3-gram patterns are scored higher. Again, to
make computation feasible, longer n-grams are
rarely used. Under such LM constraint, the
candidate generation process can only foresee a
limited window. The assumption to generate
good candidates with such a LM is therefore
not quite true either.
In fact, the candidates of the target sentence,
which are hidden in the arg max
E
operator, are
generated as a stochastic process. Starting from
a particular state, the next word is predicted
based on a local n-gram window within a
distance allowed by the distortion criterion; the
possible paths are exploited using the stack
decoding algorithm, beam search algorithm or
other searching algorithms. The candidates
generated in this way thus may be only
“piecewise”consistent with the target language
grammar, but may not be really globally
grammatical. It is then not likely to be fluent.
This decoding process therefore sometimes
falls into the “garbage-in and garbage-out”
situation. No matter how well-formulated the
TM and LM may be, if the stochastically
generated candidates do not include the correct
and fluent translation, the system will
eventually deliver a garbage output, that is a
disfluent sentence as the best one. This kind of
error is known as a kind of searching error.
Because the TM and LM have limited
expressive power to describe the real criteria
that carry the transfer and generation process,
noisy sentence segments and whole sentences
are generated for scoring. This could lead to
bad performance in terms of BLEU score or
human judgments.
Phrase-based SMT had partially resolved the
expressive power issue of TM and LM by using
longer word sequences. However, the
acquisition of phrases has its own problems. In
particular, most phrase-based SMT acquires the
phrase pairs by conducting bilingual word
alignment first (Och 99, 00a, 00b, 04).
Adjacent words are then connected in some
heuristic ways, which do not have direct link
with the source or target grammar, to form the
“phrases”. The quality of such phrases is
therefore greatly affected by the word
alignment accuracy; and, the phrases for the
target language side may not really respect the
target grammar. Under such circumstances, a
huge number of noisy “phrases” will be
introduced and significantly enlarge the
searching space. The stochastically generated
phrase sequences thus may not correspond to
good candidate sentences either.
To summarize, the application of
word-for-word or phrase-to-phrase translation
plus a little bit local word/phrase re-ordering
might not generate fluent target sentences that
respect the target grammar. In particular, many
target specific lexical items and morphemes
cannot be generated through this kind of
models. If they do, they may be generated in
very special ways. This could be a significant
reason why the SMT models do not work well
after the long period of research.
The implication is that we might have to
examine the arg max
E
operation, that is, the
decoding or searching process, in the classical
SMT models more carefully. We should try
decoding method that respect target grammar
more, instead of following the criteria set forth
by the TM and LM of the SMT model, which
精簡報告 共 8 頁 第 6 頁
junk sentences that are not related with the
translation of the source sentences.
2.1 Searching Strategy
To acquire fluent translations from the web,
the following searching strategy is proposed.
Firstly, the lexical translation of the source
words is acquired by lexicon lookup. This can
be conducted by using a bilingual lexicon or the
translation pairs acquired from an SMT.
Initially, we can assume that the word order
is preserved in the lexical translation sequence.
Under this simple assumption, we can force the
whole lexical sequence as a single token, by
quoting the whole string, and then submit the
single quoted query string to a search engine,
like Google. The returned snippets are then
likely to include fluent translation of the source
sentence at the top few search results. The best
candidates can be selected from the search
results according to the BLEU scores between
the submitted and the returned sentences.
Actually, any other criteria that indicate fluency
and adequacy of lexical translation between the
two can be used for selecting the candidate
translations.
However, it is likely that the lexical
sequence also includes some lexically
translated source-specific words, which should
not be translated into the target anyway; it may
also include lexically translated words that
appear on the web only in their synonymous
forms. If this is the case, a simple solution to
such 1-to-0 matches is to delete parts of the
lexically translated words, and then resubmit
them to the search engine for matching.
To have as many exact matches as possible,
adjacent lexical words will be quoted as a
single token before they are submitted to the
search engine. In general, from zero to L words
can be deleted from the original lexical
sequence, with adjacent words quoted as a
single query string before they are submitted to
the search engine.
To see how this simple method performs, we
simulate the proposed method by using the
Academia Sinica Word Segmentation Corpus,
ASWSC-2001 (CKIP 01), as the lexically
translated sequences. By sampling 106
sentences, of 7 words on average, from the
whole corpus, we have the following average
matching scores (in terms of “matching
percentage”and BLEU scores) for the search
results of the 106 sentences. The performance
is acquired by deleting at most two tokens (i.e.,
L=2) from the input sequence.
Top-1 Top-1x Top-5x
%Match 0.941 0.758 0.740
BLEU 0.847 0.548 0.526
Table 1. Average matching scores of the
search results of 106 sample sentences.
The row labeled ‘%Match’uses the number
of matched words normalized by the average
length (in terms of words) of the submitted and
returned sentences as its matching score. The
column labeled “Top-1” refers to the case
where the search results with the highest score
is selected as the only candidate translation. On
the other hand, “Top-1x”means to exclude
search results that are exactly the same as the
submitted sentence. In other words, it simulates
the case where no search result is a perfect
translation. The “Top-5x” experiment also
excludes exact matches and selects the best 5
candidates from the remaining search results.
It can be seen that the searched candidates
are much more fluent and adequate than
stochastically generated SMT outputs
according to their much higher scores, even
though exact matches may not be found.
Searching based decoding is thus a potentially
encouraging solution to improve translation
fluency and adequacy.
Of course, there are other translation
problems which may make this strategy
insufficient. For instance, the word order may
be significant in some searching instances.
Fortunately, most search engines are based on
an order-free indexing mechanism; the word
order problem is thus alleviated automatically
to some extents during searching. To be more
robust, in our future works, we will apply
techniques like (Shih 07) to automatically learn
effective query formulation patterns, by
augmenting input words with appropriate
精簡報告 共 8 頁 第 8 頁
[Och 04] Och, Franz Josef and Hermann Ney.
2004. “The alignmenttemplate approach to
statistical machine translation.”
Computational Linguistics, 30:417–449.
[Papineni 02] Papineni, K., Roukos, S., Ward,
T., and Zhu, W. J. (2002). "BLEU: a method
for automatic evaluation of machine
translation" in Proceedings of ACL-2002,
40th Annual Meeting of the Association for
Computational Linguistics pp. 311--318.
[Shen 06] Shen, Wade, Brian Delaney and Tim
Anderson, 2006. “The MIT-LL/AFRL
IWSLT-2006 MT System,” Proc. of the
International Workshop on Spoken Language
Translation (IWSLT) 2006, pp. 71-76, Kyoto,
Japan, 27 November 2006.
[Shih 07] Shih, Shu-Fan, 2007. "A Query
Augmentation Model for Answering
Well-Defined Questions, Master Thesis,
Department of Computer Science and
Information Engineering, National Chi Nan
University, Taiwan, ROC, July, 2007.
[Zhou 04] Zhou, Yu, Chengqing Zong, and Bo
Xu, 2004. “Bilingual Chunk Alignment in 
Statistical Machine Translation,” In 
Proceedings of IEEE International
Conference on Systems, Man & Cybernetics
(SMCC2004), Hague, Netherlands, 2004.
