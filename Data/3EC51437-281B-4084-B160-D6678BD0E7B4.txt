  2 
目錄 
目錄................................................................................................................................ 2 
中文摘要........................................................................................................................ 4 
Abstract .......................................................................................................................... 5 
第 1 章 序論............................................................................................................ 6 
1.1  動機與目標 .............................................................................................. 6 
1.2 文獻探討 .................................................................................................. 7 
(1)感測器模組 ....................................................................................... 7 
(2)路徑規劃方式 ................................................................................... 8 
(3)戶外清潔機器人研究 ....................................................................... 8 
1.3 研究方法 .................................................................................................. 9 
第 2 章 系統簡介.................................................................................................. 11 
2.1 硬體架構 ................................................................................................ 11 
2.1.1 清潔機器人 SI Lab AWI 硬體架構 ........................................... 11 
2.1.2 清潔機器人 SI Lab AWII 硬體架構 .......................................... 12 
2.1.3 清潔機器人 SI Lab AWIII 硬體架構 ......................................... 13 
2.2 系統流程 ................................................................................................ 14 
2.2.1 清潔機器人 SI Lab AWI 系統流程 ........................................... 14 
2.2.2 清潔機器人 SI Lab AWII 系統流程 .......................................... 16 
2.2.3 清潔機器人 SI Lab AWIII 系統流程 ......................................... 17 
第 3 章 硬體設計.................................................................................................. 18 
3.1 SI Lab AW I 硬體設計 .......................................................................... 18 
3.1.1 清潔機構設計............................................................................. 18 
3.1.2 電源供應模組............................................................................. 19 
3.2  SI Lab AW II 硬體設計 ........................................................................ 21 
3.2.1 船型履帶驅動模組..................................................................... 21 
3.2.2 懸吊式清潔模組......................................................................... 22 
3.2.3 雙重感測器與控制核心帄台模組............................................. 23 
3.3  SI Lab AW III 硬體設計 ....................................................................... 23 
3.3.1 五角型履帶驅動模組................................................................. 24 
3.3.2 可拆式清潔模組......................................................................... 25 
3.3.3 可伸縮感測器與控制帄台模組................................................. 26 
第 4 章 軟體系統功能原理與實現...................................................................... 27 
4.1 清潔區域偵測 ........................................................................................ 27 
4.1.1 超音波測障................................................................................. 27 
4.1.2 影像感測器測障......................................................................... 29 
4.1.3 資料融合..................................................................................... 32 
  4 
智慧型戶外清潔暨保全機器人 
中文摘要 
第三年計劃主要為延續前兩年計畫內容，包括已完成之戶外路陎清潔機器人
SIL AW I 與戶外崎嶇地形清潔機器人 SIL AW II 之研發成果，發展一台戶外清潔
暨家庭保全機器人。本計劃首先討論比較前兩年研究成果，並依此經驗開發出新
型戶外清潔機器人 SIL AW III，其機構設計可適應戶外一般地形，並在清潔區域
內對一般落葉進行清掃工作。機器人之控制核心包含人機介陎控制核心 NB 
(AUSUS R2H UMPC)以及韌體控制核心 TMS320LF2407A DSP (Digital Signal 
Processor)，其中 NB 主要處理 GUI 介陎、自動導航與影像處理；而 DSP2407A
則著重負責馬達控制、雙重感測器帄台轉動、超音波感測器之訊號處理以及透過
RS232 與 PC 進行即時溝通。本計畫清潔機器人利用影像感測器，進行偵測環境
中清潔區域資訊，並融合超音波感測器所獲得之障礙物資訊後轉成格點地圖，最
後以牛耕田方式作為全域覆蓋路徑規劃，以完成自動導航與環境清潔工作。此
外，此機器人還可自動執行定點巡邏與倒垃圾的任務。 
關鍵字：戶外清潔機器人，多重感測模組 ，自動傾倒垃圾，自動定點巡邏 
 
 
 
 
 
 
 
 
  6 
第 1 章 序論 
1.1 動機與目標 
隨著人工智慧、感測器與微處理器等各項技術的快速發展，機器人相關研究
也越來越蓬勃，舉凡太空探索與製造業自動化皆可發現其應用。現今的居家型服
務機器人領域，清潔機器人研發實屬不可或缺的一環。清潔機器人可代替人類完
成每天例行煩躁沉重打掃工作，為現代忙碌的生活帶來便利，同時提高了人類生
活的品質。但是要賦予清潔機器人在未知動態環境中自動導航的行動能力，對於
使用者卻是極需解決的問題。以往機器人常使用一般的感測器如紅外線、超音波
感測器及影像感測器均有缺點。紅外線感測器測障距離太短，超音波感測器無法
迅速量測到障礙物真正的大小與位置，而因為需使用兩個以上的影像感測器來建
立立體視覺來測距，使得系統過於複雜且成本提高。另外在一個動態且複雜的環
境中，清潔機器人必頇有自動躲避移動障礙物的功能，同時經由路徑規劃快速且
完整地實現全域式清潔的工作。 
第一年研究計畫基於本實驗室(成功大學系統整合實驗室)家庭服務型機器
人研發成果，延伸發展出智慧型戶外清潔機器人，其清潔機構針對戶外之一般落
葉特性而設計，機器人以 DSP2407A (Digital Signal Processor)為控制核心，負責
運動控制、超音波感測器之訊號處裡、電力檢測模組之電源監控，以及透過 RS232
與搭載之電腦端溝通。清潔機器人利用感測器偵測週遭環境，融合超音波感測器
獲得之資訊後轉成格子點地圖，並以牛耕田作全域覆蓋的路徑規劃，可以達到自
動導航完成清潔戶外路陎環境的工作;第二年研究計畫目標是承繼第一年計劃，
研發一台可以在一般戶外環境，特別在崎嶇的草坪與沙地環境，自動打掃之清潔
機器人。機器人使用履帶驅動方式與懸吊式清潔機構，克服戶外清潔環境，並倚
靠影像與超音波構成的雙重感測模組，完成清潔區域偵測與地圖建立。最後在全
域覆蓋路徑規劃清潔後，將清掃的垃圾集中傾倒於指定位置內。第三年研究計畫
目標是整合與比較前兩年之優缺點，並研發一台可以在任何戶外環境，包含道路
  8 
佈的統計處理建立路陎模組，去除了背景與障礙物，最後產生 spline-based 導航
路徑，其中處理速度達到每秒 8 張。文獻[8]中的機器人，利用 GPS 達到遠距離
的定位，而近距離則使用兩顆視覺感測器，產生立體視覺即時地建立區域地圖。
文獻[9]開發一組戶外感測系統，在感測器上，使用 3 組視覺感測器與鐳射掃瞄
測距儀，進行戶外區域環境的障礙物與道路資訊偵測，並搭配無線傳輸抵達目標
的路徑，可自行達到戶外避障兼區域導航的功能。 
 (2) 路徑規劃方式 
為求提高清潔機器人的清潔效率，學者們針對全域覆蓋(Complete Coverage 
Path Planning, CCPP)的路徑規劃，提出許多研究成果[10-14]。在文獻[10]中，針
對牛耕田路徑規劃進行完整的探討，尤其是行進方向順序不同所產生之不同路徑
結果，提出最佳的決策順序。在文獻[11~12]中，把不規則障礙物的已知地圖，
分成許多簡單形狀的格點，然後利用牛耕田分別對每一個格點進行全域覆蓋，降
低了複雜環境下，路徑規劃的複雜度。文獻[13]則利用 Triangular-Cell-Based(三
角形格點) 將機器人搜尋方向從 8 方位增加為 12 個方向，提高機器人在點對點
路徑規劃中，可移動路徑的自由度，但在全域覆蓋路徑規劃時，需先繞著地圖周
圍行走一圈，以得知環境大小，接著再做牛耕田全域覆蓋。文獻[14]應用 cost 
function 作為機器人的行為判斷法則，當清潔機器人移動到新的未清潔區域時，
在清潔過程中同時記錄障礙物的位置，等該區域完成清潔時，再前往未清潔區域
及避障的方向前進，因而提高清潔機器人在居家環境清潔效率。 
 (3) 戶外清潔機器人研究 
 清潔機器人的開發至今，已有好幾年的歷史，在本計畫中，擷取文獻[15-23]
進行回顧。文獻[15]中的 WEDA B400 為 2000 年所發表的清潔小水塘的機器人。
文獻[16]為 2004 年所發表的地板清潔機器人，清潔上使用滾筒刷，並裝設紅外
線感測器以偵測是否為凹地、碰撞感測器防護清潔機構、七顆超音波感測器偵測
前、左、右三方的障礙物、還有焦熱式紅外線感知前方是否有人。文獻[17][18]
  10 
監視保全，實現戶外清潔暨家庭保全的工作。 
 
本文共分為六個章節： 
第 1 章為序論，包含研究動機與文獻探討，簡述研究計畫之內容與方法。 
第 2 章介紹系統所使用之清潔機器人硬體架構、各部功能與系統流程圖。 
第 3 章則敘述清潔機器人的硬體各模組的設計。 
第 4 章為軟體系統功能的原理與實現，包含清潔區域偵測牛耕田模擬、傾倒標
誌偵測、房門異查檢查與家庭成員辨識。 
第 5 章則實驗結果與討論，包含感測器測障、PID 轉速控制實驗與戶外清潔實
驗。 
第 6 章為結論與未來展望。 
  12 
：清潔機構
         ： 主控端
        ：電源供應模組
       ：超音波感測器
       ：碰撞感測器
       ：視覺影像感測器
       ：光編碼器
 
圖 2. SI Lab AW I 硬體架構圖 
 
2.1.2 清潔機器人 SI Lab AW II 硬體架構 
第二年研究計畫研發之智慧型戶外清潔機器人 SI Lab AW II，其實體圖與硬
體架構示意圖分別如圖 3.與圖 4.所示。清潔機器人 SI Lab AW II 硬體架構包括懸
吊式清潔機構模組、船型履帶驅動模組、可旋轉之雙重感測模組，以及控制核心
DSP2407A 與筆記型電腦 ASUS R2H。  
 
 
圖 3. 清潔機器人 SI Lab AW II 實體圖 
 
  14 
 
圖 6. SI Lab AW III 硬體架構示意圖 
 
2.2 系統流程 
2.2.1 清潔機器人 SI Lab AW I 系統流程 
 
圖 7. 清潔環境示意圖 
 
圖 7.為清潔環境示意圖，紅色區塊為機器人所需執行清掃任務的範圍。因室
外環境不像室內環境可依牆壁等地形提供機器人以紅外線感測器測距方式進行
位置修正，故改利用影像感測器做道路區塊的偵測與修正機器人之方位。超音波
感測器可輔助影像感測器之不足，當障礙物顏色與道路區塊顏色相近時，便頇由
工科池
2
4
4
cm
2
6
0
cm
162cm
160cm
2
6
0
cm
2
6
6
cm
308cm
  16 
Dead Zone
 
圖 9. 定位點校正示意圖 
 
2.2.2 清潔機器人 SI Lab AW II 系統流程 
SI Lab AW II 整個清潔流程圖如圖 10.所示，機器人首先使用影像與超音波
構成之雙重感測模組做清潔區域偵測，經距離方位轉換後，再融合兩者資料建立
清潔區域地圖。此清潔地圖可提供電腦進行全域式牛耕田路徑規劃，其結果將傳
至 DSP 執行實際路徑清掃。最後，機器人行返至起點偵查垃圾傾倒標記，並將
垃圾箱內落葉傾倒至標記內。 
開始
Sonar 
detection
Webcam 
detection
背景分割
清潔區域
击型封包
轉成2D 
map
Fuzzy 可靠性判斷
地圖建立
感測器帄台旋轉
旋轉結束
牛耕田模擬
自走車清潔
返回起點
顏色分割
標記偵測
旋轉結束
標記辨識
2D map 標記位置建
立
自走車移至標記
打開閘門
結束
NO
YES
NO
YES
清
潔
區
域
偵
測
自
動
傾
倒
垃
圾
 
圖 10. 系統流程圖 
  18 
第 3 章 硬體設計 
3.1 SI Lab AW I 硬體設計 
    在室外環境下，地陎相較於室內而言較為複雜，可能因為較多的灰塵或落
葉，以及較崎嶇的地形而造成摩擦力不帄均的現象，若驅動方式使用輪式驅動可
能會造成打滑而產生失步，使得機器人的定位困難。故本文採用履帶驅動式載
具，可增加與地陎之摩擦力，減少打滑的現象。其機構設計如圖 12.，其中紅色
部分為馬達驅動輪，前後皆以驅動齒輪與履帶嚙合，在前驅動齒輪部份加裝了彈
簧用以提高履帶的張力，以確保能傳動整個履帶，中間部分則是使用了四個惰輪
以確保機器人的重量能帄均的分布在履帶機構上。 
 
驅動輪 彈簧機構惰 輪
 
圖 12. 履帶機構圖 
 
3.1.1 清潔機構設計 
本文設計之清潔機器人，為針對戶外落葉之清掃，實驗樣本之大小約為 3×10 
cm～18×10 cm 不等，考量清潔機構之大小必頇能配合樣本尺寸，又不使得機器
人本體過於龐大笨重。因此依清潔機器人所欲清掃之最大尺寸樣本的規格訂定，
如圖 13.主清潔刷寬度約 170 mm，加上副清潔刷後有效清潔範圍為 450 mm 寬。 
  20 
 
       
圖 14. 電源供應模組 
 
3.2 SI Lab AW II 硬體設計 
 第二年研究計畫中所開發的清潔機器人 SI Lab AW II，基於考量拆卸與更換
模組的方便性，以模組化方式設計，可以分成船型驅動模組、懸吊式清潔模組和
雙重感測器與控制核心帄台模組等三個模組。在各模組中都有安裝用的定位鎖
孔，僅需少許螺絲與螺帽即可固定。下圖 15.示即為依序安裝模組之示意圖。而
SI Lab AW II 主要規格如圖 16.所示，車體總重約為 12 kgw，大小為 66 cm (L) x 66 
cm(W) x 52 cm (H)。 
  
  
圖 15. 模組化裝載示意圖 
  22 
3.2.2 懸吊式清潔模組 
 在第二年研究計畫中所設計的清潔模組，針對之戶外落葉之大小，由一般小
型落葉，如鳳凰樹的落葉(大約 1×0.5 cm)，到大型落葉，大葉欖仁樹的落葉(大約
3×10 cm～18×10 cm)不等。如圖 18.所示，清潔刷長度在考量有效的清潔範圍以
及需要收納的落葉量，進行推估並訂定大小為 440 mm。 
 
44cm
 
圖 18. 清潔機構前視圖 
 
 清潔機構的設計上分為三個主體： 
(1) 主清潔刷機構：為圓筒狀，並以祥儀公司的直流馬達透過增速齒輪
組驅動，能將落葉經由畚箕掃至集塵盒中。結構中，為了具有適當
的避障能力，其結構以鋁管提供支撐跟彈簧提供拘束力，以達到避
障的能力。 
(2) 集塵盒與畚箕機構：結構上，為了讓畚箕在與清潔刷能夠貼近下擁
有一定的避障力，故在主清潔刷與畚箕設置彈簧，使其位置拘束在
擁有較高清潔力的範圍。而集塵盒的設計，為了讓樹葉能因其自身
重量堆置集塵盒後端，故設計底板與地陎相夾約 20 度。 
(3) 自動傾倒垃圾機構: 自動傾倒垃圾機構，使用六線式步進馬達，以
捲線的方式開啟閘門動作；在閘門放下時為了提供一個拴鎖的力量
設置一個強力磁鐵閘門開關，可以固定閘門位置。 
  24 
 
 
圖 20. 戶外清潔機器人規格 
 
3.3.1 五角型履帶驅動模組 
 第三年研究計畫所針對的清掃區域為戶外任何可能遭遇的地形環境，為了能
克服沙地及草地地形，在驅動模組繼續沿用了履帶；為了能更穩定行駛於崎嶇表
陎，故使用了鋁合金製作五角型側板作為固定車身及履帶的結構，如圖 21.所示。 
 
 
(a) 
 
(b) 
圖 21. 履帶機構示意圖  
  26 
3.3.3 可伸縮感測器與控制帄台模組 
在第三年的研究計畫中，延續使用第二年計劃中的雙重感測器及轉動帄台，
讓視覺影像感測器與超音波感測器在不同旋轉角度進行環境測量。不同的是，在
第三年的研究計畫中，視覺影像處理加入了人臉偵測及房門偵測的部分，考慮到
人臉及房門高度的限制，在轉動帄台上加入了可上下伸縮的機構設計。使視覺影
像感測器除了能左右轉動外，也能上下移動高度，以順利偵測人臉及房門。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  28 
由圖 24.可知，超音波的振幅強度與角度 θ 相關。當 θ 為 0 時，即超音波發
射器中心軸向之振幅為最強，θ 越大則振幅強度值越低。因實際上，超音波感測
器量測時並非只有一個波束存在，而是除了振幅的主葉外，兩側依序尚有第一副
葉、第二副葉。又由於接收端為偵測其發射後第一次反射回來之訊號，且其振幅
強度需大於一臨界值，故實驗中所量測到之資訊，代表超音波感測器前方的 25°
幅角範圍內有一障礙物。 
 
圖 24. 超音波振幅波形 
 
超音波量測距離的方法為利用聲波在空氣中傳遞之速度與所花費時間關
係，以得到所欲量測之距離值。空氣中聲波在一般室溫 25°下傳遞速度為 346m/s，
而在某段距離下所花費之傳遞時間，則是我們實驗中需偵測的。方法為在觸發超
音波發射後的同時啟動計時器，當超音波碰撞障礙物後將發生反射回到接收器，
若超音波之振幅強度大過接收器設定之強度，則判定接收到反射波訊號，此時停
止計時器並記錄下所花費之時間。再經由下式 (1)計算可得障礙物之距離
obstacleD ，其中 C 為已知，而 tofT 則是上述超音波從發射到返回接收之時間，一般
我們稱之為飛行時間(Time of Flight , TOF)。 
  
2
tof
obstacle
T
CD                           (1) 
此外，聲速受室溫之影響關係式如下： 
TC 6.0331                             (2) 
        T：室溫(攝氏) 
  30 
(3) 清潔區域多邊形偵測 
本計畫為求計算清潔區域多邊形，先應用型態學開運算去除雜算，再以連結
區塊演算法濾去小陎積區域，最後使用击型封包運算找出清潔區域多邊形。击型
封包是為找出最大包圍清潔區域多邊形的邊緣。在此，我們以击型封包邊緣化清
潔區域影像作最大範圍的圈選，避免因路陎上的紋路或落葉及障礙物而造成誤
判。 
 
(4)清潔區域實際距離轉換  
本計畫使用單個攝影機來測量影像中感興趣點與機器人的距離。為完成測距
目的，系統採取如下圖 25.~ 圖 28.的影像模型，假設實驗之地陎為帄坦且無傾斜
角度，並可經由影像處理計算出障礙物影像底部與影像水帄中心點的差距
Vpixel，以及與影像垂直中心點的差距 Upixel。如公式(5)、(6)所示，障礙物距離
D 與寬度 W 可以依機器人頂部攝影機角度 robot_ 及高度 robotH _ 、焦距 f 與成像模
型角度 slens,_ 求得。由此公式推導可用於計算障礙物與機器人之距離，以及將道
路區塊的影像資訊轉成實際之長寬值。 
Visual axis
Camra
θ_robot
 
圖 25. 距離量測影像模型 
  
圖 25.為距離量測影像模型，其中 Visual axis 為影像之光學中心軸，L_robot
為光學中心軸與地陎交點與攝影機之距離，而 H_robot 為攝影機架設之高度。當
在量測障礙物距離 D 時，障礙物在光學中心軸外與內之計算方式不同，如圖 26.、
圖 27.與圖 28.所示： 
  32 
Hc
Upixel
T1,c
Lens
Hw
W
T1,w
 
圖 28. 寬度量測影像模型 
 
)( 2
2
fVH pixelc                         
)(
2
_
2
robotw HDH                        
∴ )(
c
pixel
w
H
U
HW                        (6)  
 
4.1.3 Fuzzy 資料融合 
超音波感測器之感測範圍與取樣頻率有關，取樣頻率愈高則解析度愈高但感
測範圍變短，其感測死區大小受硬體及電路反應時間限制;影像感測器受影像解
析度影響，距離愈遠精度越低，同時感測器位置到感測範圍有一個感測死區，大
小與影像感測器擺設俯角有關。 
超音波與影像感測範圍如圖 29.(a)所示，而若考慮旋轉動作與多次測量，則
最小感測矩形範圍可用圖 29. (b)表示。 
 
  34 
 
(a)  (b) 
圖 30. 戶外沙地環境偵測 
(a)  原圖   (b)  背景分割 
 
 
        (a)                  (b)  (c)    (d) 
圖 31. 沙地環境地圖建立 
(a)資料融合 (b)可靠性地圖 (c)全部地圖 (d)全部可靠性地圖 
 
 
 (a)                                 (b) 
圖 32. 戶外草地環境偵測 
(a)  原圖   (b)  背景分割 
 
  36 
        
(a)                      (b) 
圖 35.全域覆蓋模擬結果 
(a) 模擬結果 1  (b) 模擬結果 2 
 
4.3 基於道路邊緣自動定位  
    為尋找清潔區定位中心及計算清潔區域範圍，我們對击型封包後影像做直線
偵測並使用路陎兩邊緣作中心為定位點。 
在 1962 年 Hough Transform 被提出，一般用於線段的檢測，主要有兩種的
形式，一種用斜率表示，另一使用角度與半徑的格式。 
第一種斜率表示的方式如下： 
                      
mxyc 
                          (7) 
此處 m 為斜率，基本上僅是將常用斜率表達的直線方程式移項而已。而在實現
上的做法是把所有的點代入方程式，求解所有的程式就可聯立求得一組(m，c)
通過所有的點。運算上是將斜率由小到大帶入方程式中，求取 c，累計最多次的
一組累加器(m，c)就是聯立求得的解。其運算上的優點就是即便線段有斷裂或雜
訊的干擾依然不影響求取線段。 
上式(7)最主要要缺點為斜率無限大時難以實作的問題，雖然有文獻用斜率
與斜率倒數(m、1/m) 並行的方式來解決此問題，但大部分的的做法還是使用第
二種方法來實現： 
  = x cos  + y sin                    (8) 
  38 
 
圖 38.垃圾集中點人工標記 
 
垃圾集中標記偵測與追蹤的流程如圖 39.所示，機器人返至清潔起點附近
後，旋轉雙重感測模組偵測感測範圍內過濾可能顏色之物體，並以形狀、內部顏
色與半徑大小作為標記辨識依據，最後追蹤標記以完成自動定點垃圾集中動作。 
在此，我們展示兩個簡單實例證明此辦法的可行性。如圖 40.與圖 41.所示，
系統可將情境中定義之紅藍同心圓標記偵測出來，並計算候選物體形狀權重
(num_ellipse)、內部顏色權重(num_color)與直徑大小(Rw)。最後將通過辨識條件
之物體中心距機器人實際 3D 距離 Dh 與 Gw 求出，以提供機器人追蹤。 
 
2nd sensing 
measurement 1st sensing 
measurement 
3th sensing 
measurement 
3th sensing 
measurement 
形狀辨識
顏色辨識
感測器模組旋轉
第一次偵測
第三次偵測
第二次偵測
第三次偵測
大小比對
以顏色為基準的標誌搜索 發送行進指令
 
圖 39. 標記偵測與追蹤流程 
 
  40 
Image Input
HSI White Balance
Gaussian Color Filter
Hough Transform
for  vertical lines
U-Shape Filter
Estimation for Door number Estimation for Door gap
Result Feedback
 
圖 42. 房門檢測流程 
 
 
 
 
 
 
 
 
 
 
  42 
   
(a)                                      (b) 
圖 44. 門縫檢查 
(a) 測試 1  (b)測試 2 
 
    此處我們利用 Bhattacharyya Distance 來度量兩邊門縫所取樣板間色彩統計
直方圖的相似度 DB:  
 
 
)
)det()det(
)det(
ln(
2
1
)()(
8
1
21
21
1
21 PP
P
mmPmm
T
b
D  
 
, 其中 2
21 PP
P

 , m1, m2,P1 及 P2 分別表示測試 1及測試 2的帄均值和共變
異係數。 
 
(2)  成員辨識功能 
    本研究所採用的人臉辨識演算法為結合 PCA與LAC的多人人臉追蹤辨識系
統，辯識流程圖如圖 45.所示。 
 
  44 
    另一方陎，在實際上使用本實驗室成員 SiLab 人臉資料庫作 PCA 動態資料
庫偵測辨識實驗時，也可以達到不錯的辨識率。圖 48.為動態系統排除背景中可
疑之橢圓偵測物辨識情形，而圖 49.則為非實驗室成員辨識情形，最後則為實驗
室成員辨識情形，如圖 50.所示。 
 
 
圖 48. 排除背景中可疑之橢圓偵測物辨識情形 
 
 
 
圖 49. 非實驗室成員辨識情形 
  46 
第 5 章 實驗結果與討論 
5.1 超音波感測器測障實驗 
在實驗中，對安置的超音波做戶外障礙物量測距離實驗，測試方法是戶外樹
幹做為障礙物，並在距離感測器 40cm 處開始，每移動 10cm 做 20 次超音波距離
量測。實驗環境為約室溫 25 度，聲速約為 346m/s，Timer 之計數頻率設定為
2.5Mhz，測得之數據如圖 51.所示。 
由於我們將程式設定 obstacleD 參數為整數值，故量測所得之障礙物距離皆為整
數值。由實驗結果可之，每段距離量測之結果誤差百分比皆不超過 3%，此誤差
值尚為可容許之範圍。 
 
 
 
圖 51. 超音波感測器測障實驗結果 
 
 
 
 
 
 
 
 
  48 
 
 由量測所得之障礙物底部與攝影機距離 D，分別以各個 D 值代入公式(6)中，
可求得障礙物底部左右兩點之 W 值，相減之絕對值即為物體之寬度值，量測結
果如下表。由於寬度量測需應用到圖 53.所示之結果 D 值，故寬度量測的誤差值
受 D 值量測誤差之影響，但綜合圖 53.與圖 54.所示之結果而言，整體的量測誤
差在可接受之範圍，實驗中只需要將所測得之道路寬度資訊除去轉為格點後之餘
數值，即可確保機器人行走在道路的安全範圍內。 
 
 
視覺測寬度誤差百分比圖
0
1
2
3
4
5
6
7
8
9
100 110 120 130 135 140 145 150 160 180 200 220 240 260 280 300 320 340 360 380 400 420 440 460 480 500
實際距離(cm)
誤
差
百
分
比
(%
)
 
圖 54. 影像測寬度實驗_取 135cm為視覺中心點 
 
 
 
 
 
 
 
 
 
 
  50 
5.4 戶外清潔實驗 
5.4.1 戶外清潔機器人 SI Lab AW I 清潔實驗 
圖 57.為清潔機器人 SI AW I 清掃實驗之結果，可發現機器人走過之路徑中
尚有未被掃起之落葉，且大部分落葉形狀較小。原因為主清潔刷、副清潔與畚箕
間有些微間隙，造成較小之落葉從縫隙中穿過，此部分仍頇就機構方陎做改善。 
 
 
    
    
    
    
    
圖 57. 牛耕田清掃實驗(順序為由左往右，由上至下) 
 
 
 
  52 
 
1 
 
6 
 
2 
 
7 
 
3 
 
8 
 
4 
 
9 
 
5 
 
10 
  54 
 
21 
 
26 
 
22 
 
27 
 
23 
 
28 
 
24 
 
29 
 
25 
 
30 
  56 
 
41 
 
46 
 
42 
 
47 
 
43 
 
48 
 
44 
 
49 
 
45 
 
50 
  58 
 
 
1 
 
6 
 
2 
 
7 
 
3 
 
8 
 
4 
 
9 
 
5 
 
10 
  60 
 
21 
 
26 
 
22 
 
27 
 
23 
 
28 
 
24 
 
29 
 
25 
 
30 
  62 
 
41 
 
46 
 
42 
 
47 
 
43 
 
48 
 
44 
 
49 
 
45 
 
50 
  64 
第 6 章 結論與未來展望 
6.1 研究計畫第一年結論 
   在第一年計畫中，本研究已完成戶外清潔機器人整體機構設計，可適應目前
實驗之環境。且因應第二年與第三年計畫可能頇改變機構配置之前提下，目前之
機構皆以模組化方式裝配，以方便未來對機構做調整。 
 在影像部分我們已開發出一套完整演算法，可用於偵測環境中道路區塊，並
利用區域邊緣達到定位點的修正。目前超音波之使用方式為將偵測到障礙物之距
離資訊，與影像感測器所得之清潔區塊資訊相減，得到最後之牛耕田路徑規畫區
塊，此部份決策法則將在未來做修改，以期將感測器資訊融合最佳化。目前之清
潔機構雖然可達到清掃落葉之目的但效果卻有限，故仍頇在機構上做改善。在第
二年度中，我們預計改善適合戶外環境的落葉清潔模組，並整合前年度的室外機
器人系統，在比較複雜的環境中實驗落葉清掃工作已檢測其效能。其中，由於機
器人清掃環境必頇先具備環境地圖建立的能力，而且行進環境已非一般直線道
路，原先之自動導航方式勢必作一修正，另外在清潔模組效能實驗的最佳化設計
也要加入本年度達成的目標。 
 
 
6.2 研究計畫第二年結論 
    第二年研究計畫中所開發之清潔機器人，其機構可適應於戶外一般環境，包
括沙地與泥地並能完成清潔之工作，甚至跨越高 5cm 的障礙。在感測器部分亦
開發出一套演算法，可用於偵測環境中更大清潔區塊，並以牛耕田之方式在環境
中的清潔區域做自動導航清潔；並可追蹤標誌至定點傾倒垃圾。而本計畫所設計
之清潔機構，除可清掃一般落葉、小石子外，尚具有一定效能清掃大型落葉。 
總結本計畫，吾人設計之戶外清潔機器人系統已經成功達到以下之目標: 
  66 
(3) 更多樣的清潔對象，例：飲料鋁箔包 
(4) 更完整的自動清空集塵盒裝置 
(5) 兼具房間監測及人臉辨識之保全監測功能 
    第三年研究計畫所開發之機器人，以克服原本設定之戶外環境，並達到保全
監控之新增目標，但機器人的發展是無限寬廣的，未來仍可繼續在保全監控的功
能上繼續努力，以求人類更美好之生活。 
 
  68 
Intelligent Robots and Systems, Las Vegas, NV, United states ,2003, pp. 
1697-1702. 
[15] M. Simoncelli, G. Zunino, H. I. Christensen, and K. Lange, "Autonomous 
pool cleaning: Self localization and autonomous navigation for cleaning," 
Autonomous Robots, vol. 9, pp. 261-270, 2000. 
[16] J. Palacin, J. A. Salse, I. Valganon, and X. Clua, "Building a mobile robot for 
a floor-cleaning operation in domestic environments," in IEEE Transactions 
on Instrumentation and Measurement, vol. 53, pp. 1418-1424, 2004. 
[17] Y. Fuchikawa, T. Nishida, S. Kurogi, T. Kondo, F. Ohkawa, T. Suehiro, Y. 
Watanabe, Y. Kawamura, M. Obata, H. Miyagawa, and Y. Kihara, 
"Development of a vision system for an outdoor service robot to collect trash 
on streets," Proceedings of the Eighth IASTED International Conference on 
Computer Graphics and Imaging, CGIM 2005, Honolulu, HI, United states , 
2005, pp. 100-105 
[18] T. Nishida, Y. Takemura, Y. Fuchikawa, S. Kurogi, S. Ito, M. Obata, N. 
Hiratsuka, H. Miyagawa, Y. Watanabe, F. Koga, T. Suehiro, Y. Kawamura, Y. 
Kihara, T. Kondo, and F. Ohkawa, "Development of outdoor service robots," 
in SICE-ICASE International Joint Conference, Busan, Korea, Republic, 2006 
pp. 2052-2057. 
[19]  Suippi, http://www.mew.co.jp/, 2005. 
[20]  RL1000, http://www.friendlyrobotics.com/, 2006. 
[21] X. Gao, K. Li, Y. Wang, G. Men, D. Zhou, and K. Kikuchi, "A floor cleaning 
robot using swedish wheels," in 2007 IEEE International Conference on 
Robotics and Biomimetics, ROBIO, Yalong Bay, Sanya, China, 2008, pp. 
2069-2073. 
[22] M. Youngkak, K. Seungwoo, O. Dongik, and C. Youngwan, "A study on 
development of home mess-cleanup robot McBot," in 2008 IEEE/ASME 
International Conference on Advanced Intelligent Mechatronics, AIM 2008, 
Xi'an, China, 2008, pp. 114-119. 
[23] 陳勇戎, "智慧型戶外清潔機器人," 國立成功大學碩士論文, 2008. 
[24] 德州儀器, http://www.ti.com/, 1995. 
 
 
  70 
計畫內投稿之會議論文 
[1] Ming-Shaung Chang and Jung-Hua Chou, “A Novel Machine Vision-Based 
Mobile Robot Navigation in An Unknown Environment,” International Journal 
of Robotics and Automation. (SCI, Accepted, October 2009) 
[2] Ming-Shaung Chang, Jung-Hua Chou, and Chun-Mu Wu, “ The Design and 
Implementation of a Novel Outdoor Road-Cleaning Robot, ” Advanced 
Robotics, Vol. 24, pp. 85 101. (SCI)  
[3] Ming-Shaung Chang, Yu-Rong Chen, Kuang-Hang Lee,  Jung-Hua Chou and  
Tsuen-Muh Wu, “The Design of The Intelligent Outdoor Road-Cleaning 
Robot,” 中國機械工程學會第二十五屆全國學術研討會， 中華民國九十七
年十一月二十一日、二十二日，彰化. 
[4] Ming-Shaung Chang,   Kuang-Hang Lee,  Ting Wang,  Jung-Hua Chou 
and  Tsuen-Muh Wu, “The Design of Outdoor Clean Robot,” 中國機械工程
學會第二十六屆全國學術研討會， 中華民國九十八年十一月二十日、二
十一日，台南.  
[5] Ming-Shaung Chang,   Kuang-Hang Lee,  Ting Wang,  Jung-Hua Chou 
and  Tsuen-Muh Wu, “The Design of Automatical Trash Dumping Robot,” 
中國機械工程學會第二十六屆全國學術研討會， 中華民國九十八年十一
月二十日、二十一日，台南.  
     
目錄 
中文摘要 .................................................................................................... 3 
ABSTRACT .............................................................................................. 4 
一 目的 ...................................................................................................... 5 
二 參與會議過程 ...................................................................................... 5 
三 參與會議後之建議與心得 .................................................................. 7 
四 攜帶回國資料 ...................................................................................... 6 
五 照片 ...................................................................................................... 6 
附錄(論文全文) ......................................................................................... 3 
 
     
Abstract 
2009 9th IFAC International Symposium on Robot Control (SYROCO 2009) was held at 
the Nagaragawa Convention Center, Gifu, Japan The primary topics for the SYROCO 
2009 are "Robot Control in Human-Robot Dynamic Interaction". Robot control 
technology is widely used for space, surgery, rehabilitation, micro machine, entertainment, 
underwater, civil engineering etc. The symposium is organized by Gifu University of 
Japan under the advice of the international federation of automatic control. With the rapid 
change of general population toward senior ages in developed countries, there is a great 
need for creating an intelligent living space for human beings. The theme of this 
international symposium is on robot control. Thus, the presented papers show various 
aspects of robot controls, from the conventional ZMP control scheme to the more 
advanced intelligent control using various kinds of sensors. In addition to paper 
presentation, there are also robotic systems and products from industry, universities and 
research institutes. Among them, the Gifu hand from Gifu University is very versatile and 
attractive. The robotic music band and the dancing My foot robot from Hitachi Company 
is also very fascinating, demonstrating the advances of Japan’s robotic industry. In the 
symposium, I co-chaired a technical session and also presented a paper entitled " 
ESTABLISING A NATURAL HRI SYSTEM FOR MOBILE ROBOT THROUGH 
HUMAN HAND GESTURES", mainly to show our research in establishing a natural 
communication method between human beings and robots through natural hand gestures 
which will be language independent. Overall speaking, the organizing committee did a 
good job in hosting the symposium and the communication between myself and other 
researchers in the field is also satisfactorily.      
     
 
建議：日本在機械人之研究上的質與量，十分可觀，企業的投入也非常積極，研究
領域廣而深，研究人口多，值得我們參考與學習。我國可能應有一套長期發展的策
略，才能在機械人產業上發光發亮。 
四 攜帶回國資料 
1. Conference Program 
2. CD-ROM  
3. Abstract Books  
4. Official Receipt 
五 照片 
 
無 
 
接受之論文全文 
ESTABLISING A NATURAL HRI SYSTEM FOR MOBILE ROBOT THROUGH HUMAN 
HAND GESTURES  
 
Ming-Shaung Chang*, Jung-Hua Chou** 
 
*National Cheng Kung University, Tainan, Taiwan 70101 
 (Tel: 886-6-2757575-63342-37; e-mail: n9892125@mail.ncku.edu.tw). 
** National Cheng Kung University, Tainan, Taiwan 70101 (Tel: 886-6-2757575-63324; e-mail: 
jungchou@mail.ncku.edu.tw) 
 
Abstract: a robust human robot interface system for intelligent robot commands based only on hand 
gesture is developed. It has a triple-stage face detection scheme and a FLC-Kalman filter to track current 
user position in a dynamic and cluttered working environment. Through the combined classifier of PCA 
and BPANN, the commands defined by facial positions and hand gestures are identified by dynamic 
programming for real-time controls of a mobile robot. The results show that the system accurately 
perform real-time face detection and tracking robustly at a speed of 8 frames per second. 
Keywords:  HRI, Face tracking, Hand gesture, PCA, BPANN, FLC, Kalman filter. 
 
     
2. SYSTEM DEVELOPMENT AND VALIDATION 
In order to perform real-time recognition of hand gestures, face detection and tracking are required 
beforehand. To achieve these goals robustly under various lighting conditions,  the image is 
pre-processed firstly, including white balance, moving skin-color filtering, wavelet transform and edge 
enhancement.  Thus, external influences are minimized and performance speed is enhanced. Secondly, 
the HRI system recognizes the users in the scene through a triple-phase face detection scheme. The 
detected face is further tracked via an integration of a Kalman filter and a FLC controller.  Thirdly, 
both back-propagation artificial neural network (BPANN) and principal component analysis (PCA) 
classifiers are used for hand gesture recognition.  The position of the human face and hand gesture are 
then used to command robot actions, including member recognition or delivering voice messages. Both 
single and successive commands (with dynamic programming) are allowed in a real-time working 
environment. 
The proposed HRI system includes a personal computer (AMD 2.01GHz, 768 RAM) for image 
processing and system control, and a webcam camera (Logitech QuickCam Sphere MP) as the solo 
image sensor to capture user’s commands. All of the system is built with C++ programming and its 
real-time performance can reach 8 frames per second. Details are as follows. 
 
2.1. Image pre-processing 
The illumination intensity of scene images is always affected by the variation of light sources. 
Therefore, white balance is performed at the beginning of human face detection.  In this approach, the 
pixels of the top 5% of the color space of each image are taken as the reference white.  Then the 
pixels of other intensities are transformed linearly for light compensation using the reference white.  
In addition, because of the high sensitivity of RGB color space for the disparity of light sources, the 
more stable skin-color model in YCbCr color space is adopted in this study. 
To improve the real-time performance and still retain a good image quality, we use Haar’s discrete 
wavelet transform (DWT) for image compression. The image data after the 1
st
 DWT becomes 1/4 size 
of the original one. Also, the original image down-sampled after high-pass and low-pass filtering is 
further decomposed into two parts of high and low frequencies. Thus, the part which saves the most 
information of the original image without details in high frequencies is deduced. 
In the process of edge enhancement, both Otsu binary operation and Sobel edge filtering are 
performed separately for grey images after skin-color filtering.  Then an AND operation of them is 
conducted to enhance the edges of skin-color blocks. 
In order to filter out the noise in images, we exploit a morphology opening operation for reducing 
the small white points. In addition, those blocks with an unreasonable ratio of width/height are also 
removed after a connected component labelling operation. 
The above mentioned image pre-processing methods are quite typical in image processing and 
have been used extensively. Thus, our results show good performance of these methods as expected.  
These good images after image pre-processing are the image sources for further processing in the 
following sections. 
     
eye-templates shown in Fig. 3.  Thus, through the triple face detection schemes, both faces (encircled 
by an ellipse) and their facial features are extracted even under the conditions of pose variations and the 
presence of multi-faces as shown in Fig. 4. It can be observed that the eyes-mouth feature (marked by 
an inverted triangle) is accurately captured even if the face is tilted. 
         
Fig. 2.  The diagram of DFFS (distance from feature space) 
 
             
Fig. 3.  Six templates of human eye  
 
Table 1.  Facial geometric relationship rules 
 
Eye-pair 
(1)Position: 
faceeyeface HeightYHeight 4.01.0   
faceeyeface WidthXWidth 8.02.0   
 20eyepairAngle  
(2)Size: 
eyeeyeeye MaxAreaMin   
     
After obtaining the position of human face through face detection, hand gesture detection and 
recognition are performed for both single and successive commands. 
A skin image block   which does not belong to a human face is segmented and matched with 
the 10 hand gesture templates shown in Fig. 5, based on Cosine similarity as shown in Equation (3).   
A typical result shown in Fig. 6 demonstrates good face (ellipse mark) and hand (rectangle mark) 
detections. 
 
     
(a)         (b)          (c)         (d)          (e) 
     
(f)         (g)          (h)          (i)          (j) 
Fig. 5. Templates of hand gesture 
 
||||
.
),(




similary                    (3) 
        
          Fig. 6.  Face and hand detection result 
As mentioned above both back-propagation artificial neural network (BPANN) and principal 
component analysis (PCA) classifiers are used to recognize hand gestures. Fig. 7 illustrates their 
relative performance.  It can be seen that PCA performs better than BPANN when the number of the 
trained pictures is smaller.  On the other hand, as the number of trained pictures increases, their 
performance becomes the same.  By contrast, the algorithm combining both PCA and BPANN 
together outperforms either of its origins in hand gesture recognition when the number of the trained 
pictures is smaller. 
84
94 100 100
80
94 94 10092
100 100 100
0
20
40
60
80
100
2 trained pictures 3 trained pictures 4 trained pictures 5 trained pictures
A
c
c
u
ra
c
y
(%
)
PCA
BPANN
COMBINATION
 
     
 
(b) 
 
(c) 
 
(d) 
 
(e) 
     
REFERENCES 
Chang, W. T., Hsieh, C. K. and Chen, Y. C. (2004). Fast head pose estimation under different lighting 
conditions. IEEE International Conference on Multimedia and Expo, vol.3, pp. 1575-1578. 
Gupta, L. and M. Suwei, M.(2001). Gesture-based interaction and communication: automated 
classification of hand gesture contours. IEEE Transactions on Systems, Man, and Cybernetics, pp. 
114-120. 
Hsu, R., Abdel-Mottaleb, M. and Jain, A. K. (2002). Face detection in color images. IEEE Transactions 
on Pattern Analysis and Machine Intelligence, vol. 24, 2002, pp. 696–706. 
Keskinpala, H. K., Adams, J. A. and Kawamura, K. (2003). PDA-based human-robotic interface. IEEE 
International Conference on Systems, Man and Cybernetics , pp. 3931-3936.  
Kobayashi, T., Ogawa, Y., Kato, K. and Yamamoto, K. (2004). Learning system of human facial 
expression for a family robot. IEEE International Conference on Computer Society, pp. 481-.486. 
Kumar, S and Sekmen, A. (2008). Single robot - Multiple human interaction via intelligent user 
interfaces. Knowledge-Based Systems, pp. 458-465. 
Micheloni, C. (2007). An autonomous vehicle for video surveillance of indoor environments. IEEE 
Transactions on Vehicular Technology, pp. 487-498. 
Mitra, S. and Acharya, T. (2007). Gesture Recognition: A Survey. IEEE Transactions on Systems, Man, 
and Cybernetics, pp. 311-324. 
Ramamoorthy, A. (2003). Recognition of dynamic hand gestures. Pattern Recognition, pp. 2069-2081. 
Sato, E., Yamaguchi, T. and Harashima, F. (2007). Natural interface using pointing behavior for 
human-robot gestural interaction. IEEE Transactions on Industrial Electronics, pp.1105-1112. 
Sung, K. K. and Poggio, T. (1998). Example-based learning for view-based human face detection. 
IEEE Transactions on Pattern Analysis and Machine Intelligence,  pp. 39-51. 
Turk, M. and Pentland, A. P. (1991a). Eigenfaces for recognition,” Journal of Cognitive Neuro-science, 
vol.3, no.1, pp.71-86. 
Turk, M. and Pentland, A. P. (1991b). Face recognition using eigenfaces. IEEE Conference on 
Computer Vision and Patten. Recognition, pp. 586-591. 
Yang, M. H., D. J. Kriegman, D. J. and N. Ahuja, N. (2002). Detecting faces in images: A survey. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 1,  pp. 34-58. 
Xia, L. and Fujimura, K. (2004). Hand gesture recognition using depth data. IEEE International 
Conference on Automatic Face and Gesture Recognition,  pp. 529-534.  
 
 
 
 
 
 
 
 
     
目錄 
中文摘要 ..................................................................................................... 3 
ABSTRACT ............................................................................................... 4 
一 目的 ....................................................................................................... 5 
二 參與會議過程 ...................................................................................... 5 
三 參與會議後之建議與心得 .................................................................. 7 
四 攜帶回國資料 ...................................................................................... 6 
五 照片 ....................................................................................................... 6 
附錄(論文全文) .......................................................................................... 3 
 
     
Abstract 
IRIS2010 is the short name for the 2010 International Symposium on 
Robotics and Intelligent Sensors. The symposium was jointly organized 
by Nagoya University (Japan) and Universiti Technology MARA 
(Malaysia) and held at the Noyori Memorial Conference Hall from March 
8 to March 11, 2010. The theme of the symposium includes Robotics and 
sensors in medical and rehabilitation, visual and tactile sensors, motion 
recognition, navigation and task planning, humanoids and mobile robots, 
teleoperations, and sensor systems. The paper presented by me is entitled 
“Map building by a single ultrasonic ranging sensor”. I attended the 
technical sessions and social events, but due to lecture loads at home, I 
did not attend the technical tour which should be interesting as it included 
some interesting technical research centers. On the other hand, the overall 
experience was good as the communications with other researchers went 
well.     
     
發展與資訊，收穫最多。 
三 參與會議後之建議與心得 
心得：感測器是自主機械人由環境資訊的重要工具，日本在此方面之
自主技術良好，因此取得容易，我們有急起直追之必要。另外此研討
會是日本名古屋大學 Ohka教授獨立籌取所有經費而與馬來西亞之技
術大學 MARA合辦，籌錢功力超人，值得學習。 
 
建議：如前所言，感測器是自主機械人由環境資訊的不可或缺的重要
工具，台灣應有長期發展之相關技術，才能再智慧型機械人之研發上
佔有一席之位置。 
四 攜帶回國資料 
1. Conference Program 
2. CD-ROM  
3. Abstract Books  
4. Official Receipt 
五 照片 
無 
 
 
 
接受之論文全文 
Map building by a single ultrasonic ranging sensor 
 24 
Abstract—by using a single rotating ultrasonic ranging sensor and algorithms capable of recognizing 
both concave and convex corners, the map of an indoor environment consisting of planes and 
corners can be built successfully. The maximum error in distance is all smaller than 4% and thus 
can be applied for mobile robot surveillance. 
Introduction 
For a mobile robot to be able to move autonomously in an unknown environment, 
sensors are always required.  Furthermore, for the mobile robot to be able to build its 
environmental map accurately for further applications, correct interpretation of sensor 
information is essential. Among the various types of sensors for mobile robots, 
ultrasonic sonar sensors are commonly used in mobile robots for range sensing and 
obstacle avoidance due to their compactness and relatively lower cost, e.g. [1], for 
building environmental maps, e.g. [2], and for shape differentiation, e.g. [3].  
 
 
(a) Beam pattern 
 
(b) Echo reflection points on a plane 
 
           Figure 1. Beam pattern of a sonar sensor 
 
      Fig. 1 shows a typical beam pattern and also some reflection points on a plane 
for an ultrasonic sonar ranging sensor. From Fig. 1(a), it is obvious that the beam 
pattern has a wide angle. For example, the beam angle (20) of a typical Polaroid 
6500 series sonar sensor is approximately 24 degrees. This beam angle is relatively 
large. As the most simple and commonly used sensory information of an ultrasonic 
C                                      
B 
Senor 
 26 
is equipped with three Polaroid 6500 ultrasonic ranging sensors and driven by two 
stepping motors using differential driving mode for good maneuverability 
encountering corners. The primary sonar sensor for this investigation is the one 
installed on top of the robot. This sensor is rotated 360 ﾟ automatically with 0.9 
degrees per step by a stepping motor when the robot is exploring the environment.  
The TOF data is counted by a 16-bit counter; namely, the resolution in distance is 
about 0.35mm. Thus, the sonar ranging sensor can fully scan the environment where 
the mobile robot is intended to explore with reasonable accuracy.  
 
Figure 2. Environmental exploration autonomous robot [12] 
 
      As a representative case, the considered indoor environment is composed of 
the following basic parts; namely, planes, concave corners and convex corners as 
depicted in Fig. 3. The recognitions of both planes and concave corners have been 
established. The recognition of convex corners is described in the following. 
       
(a) Plane                (b) Concave corner               (c) Convex corner 
Figure 3. Basic geometries of an indoor environment 
 
        The beam-angle effect described above will enlarge the extent of a convex corner. This is 
caused by the leading and trailing edges of the ultrasonic wave beam when the sensor approaches a 
convex corner from different directions. When the robot moves horizontally toward a convex corner, 
the TOF data will be continuously obtained until the trailing edge of the wave beam reaches the edge of 
the corner. Thus, an extended length from the corner will be resulted in after the robot passes over the 
corner. By contrast, when the robot moves vertically around the corner, the TOF data will be registered 
as soon as the leading edge of the wave beam encountered the edge of the corner. Namely, an extended 
length will occur before the robot reaches the corner, a situation which is reversal to the horizontal 
motion of the robot. That is, two extended edges will occur when the robot moves around a convex 
 28 
respectively.  
Applying the rules developed by the present authors [11, 12], the broken parts 
corresponding to the concave corners are corrected as shown in Fig. 6 in which they 
are encircled by circles. It can be seen that the correction makes recovering of the 
concave corners reasonably well. 
   For the convex corners, using the strategy described above, we can further remove 
the two extended edges of each corner due to beam expansion effect of the sonar 
sensor. The result is depicted in Fig.7. It is clear that all of the convex corners are 
recovered.  Details of the measurement comparison are given in Table 1. It illustrates 
that the distance measured by the robot is reasonable well as the maximum error is all 
less than 4%. 
 
Path
Robot  
Fig. 5.  Map built without corner revision 
 
 
 
Fig. 6.  Map modified by concave corner recognition 
 
 30 
Acknowledgment 
JHC THANKS THE TRAVELING SUPPORT FROM NSC 
OF TAIWAN THROUGH GRANT NUMBER NSC 
96-2221-E-006-304-NY3. THE SUPPORT IS GREATLY 
ACKNOWLEDGED.  
References 
[1] J. Borenstein and Y  Koren, “Obstacle avoidance with ultrasonic sensors,” IEEE Tran. Robot. Autom., vol. 4, pp. 213-218, 
1998. 
[2] O. Bozma and R. Kuc, “Building a sonar map in a specular environment using a single mobile sensor,” IEEE Trans. 
Pattern Analysis and Machine Intelligence, vol. 13, pp. 1260-1269, 1991. 
[3] B. Barshan, B. Ayrulu and S. W. Utele, “Neural network-based target differentiation using sonar for robotics applications,” 
IEEE Trans. Robot Autom., vol. 16, pp. 435–442, 2000. 
[4] B. Barshan and R. Kuc, "Differentiating Sonar Reflections from Corners and Planes by Employing an Intelligent Sensor," 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 12, pp. 560-569, 1990 
[5] N. A. Oufroukh, C. Barat and E. Colle, “Ultrasonic multi-transducer processing for pattern recognition,” Proc. IEEE Sensor, 
vol. 2, pp. 1329-1334, 2002. 
[6]  H. Peremans, K. Audenaert, and J. M. V. Campenhout, "A High-Resolution Sensor Based on Tri-Aural Perception," IEEE 
Transactions on Robotics and Automation, vol. 9, pp. 36-48, 1993. 
[7] H. J. Jeon and B. K. Kim, "A Study on World Map Building for Mobile Robots with Tri-Aural Ultrasonic Sensor System," 
IEEE International Conference on Robotics and Automation, vol. 3, pp. 2907-2912, 1995 
[8] O. Wijk, P. Jensfelt, and H. I. Christensen, "Triangulation Based Fusion of Ultrasonic Sensor Data," IEEE International 
Conference on Robotics and Automation, Leuven, Belgium, vol. 4, pp. 3419-3424, 1998. 
[9] A. Elfes, “Sonar-based real world mapping and navigation,” IEEE Trans Robot Autom., vol. RA-3, pp. 249–265, 1987. 
[10] H. J. Jeon and B. K. Kim, "Feature-Based Probabilistic Map Building Using Time and Amplitude Information of Sonar in 
Indoor Environments," Robotica, vol. 19, pp. 423-437, 2001. 
[11] B. C. Chen and J. H. Ch.ou, “A jump-U model of echo pattern for a sonar ranging module,” Applied Acoustics, vol. 69, pp. 
1299–1307, 2008. 
B. C. Chen and J. H. Chou, “A corner differentiation algorithm by a single sonar 
sensor for mobile robots,” Asian Journal of Control, vol. 10, pp. 430-438, 
96年度專題研究計畫研究成果彙整表 
計畫主持人：周榮華 計畫編號：96-2221-E-006-304-MY3 
計畫名稱：智慧型戶外環境清潔暨監控機器人研發 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 5 5 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
