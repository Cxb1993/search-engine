information in an MRF framework. Some experimental 
results are shown to demonstrate improved depth 
estimation results of the proposed algorithm. 
英文關鍵詞： Depth estimation, structure from motion, Markov 
random field. 
 
行政院國家科學委員會專題研究計畫成果報告 
以統計學習對單一影像或視訊之三維深度估測 
Three-dimension Depth Estimation Using Statistical Learning from Single 
Image or Video 
 
計畫編號：NSC 98-2221-E-007-089-MY3 
執行期限：98年 8月 1日至 101年 7月 31日 
主持人：賴尚宏   國立清華大學資訊工程學系 
    Email: lai@cs.nthu.edu.tw 
計畫參與人員： 
            陳曉薇        國立清華大學資訊工程學系 
                        李東穎        國立清華大學資訊工程學系 
                        蘇德峰        國立清華大學資訊工程學系 
                        嚴 智           國立清華大學資訊工程學系 
                        魏震豪        國立清華大學資訊工程學系 
                        孫煜洧        國立清華大學資訊工程學系 
                        王芳瑜        國立清華大學資訊工程學系 
                        李郁慈        國立清華大學資訊工程學系 
                      鄭雅勻       國立清華大學資訊工程學系 
                      李奕賢       國立清華大學資訊工程學系 
                      張慈珊       國立清華大學資訊工程學系 
 
 
中文摘要 
在本篇報告中，我們提出了一個從影片中重
建深度圖的新方法，不僅考慮了幾何空間上
的一致性，同時也加入時間上的資訊。大部
分先前的重建方法假設影片中的景物為靜態
物體，也因此這些方法無法預估影片中有移
動物體的深度資訊。在我們提出的方法裡，
能夠偵測移動物體的區域並且利用馬可夫隨
機架構來幫助我們從影片中得到更好的深度
圖。首先我們採用尺度不變特徵轉換找到影
片中每個影格相對應的特徵點，並且利用它
來實作基於運動的三維重建得到每個影格的
相機參數和所相對應的3D點座標。接著利
用這些3D點和影像分割的資訊將3D資訊擴
張到整張影像，此為初始的深度圖。利用重
新投影的方法找出錯誤較大的深度區塊進行
校正，找出使重新投影錯誤值最小的深度當
成校正後的深度值。另外我們以重新投影錯
誤值較大的部份當作初始位置，幫助找出移
動物體所在的完整區塊。最後利用馬可夫隨
機架構來得到最佳化深度圖。我們提出的方
法會在此篇論文實驗結果部分展示改善後的
估測深度圖。 
 
Abstract—In this paper, we propose a novel approach to 
reconstructing depth map from a video sequence, which not 
only considers geometry coherence but also temporal 
coherence. Most of the previous methods of reconstructing 
depth map from video are based on the assumption of rigid 
motion, thus they cannot provide satisfactory depth 
estimation for regions with moving objects. In this work, we 
develop a depth estimation algorithm that detects regions of 
moving objects and recover the depth map in a Markov 
Random Field (MRF) framework. We first apply SIFT 
matching across frames in the video sequence and compute 
the camera parameters for all frames and the 3D positions 
of the SIFT feature points via structure from motion. Then, 
the 3D depths at these SIFT points are propagated to the 
whole image based on image over-segmentation to construct 
an initial depth map. Then the depths for the segments with 
large re-projection errors are refined by minimizing the 
corresponding re-projection errors. In addition, we detect 
(SfM) to compute the camera poses first, computed the 
optical flow across frames, and used the triangulation 
method iteratively to optimize the 3D surface. 
3. Proposed Method 
Fig. 1 is the flow chart of the proposed depth map 
estimation algorithm. The first four steps is to collect 
information for recovering depth map; and the latter 
stage of the system, i.e. from 5th to 8th steps, is to 
reconstruct and refine the depth map. First, we apply the 
SIFT feature matching [12] to extract the corresponding 
points in the neighboring frames and use these points to 
compute the camera parameters by the SfM algorithm 
[13][14]. Then, we apply the mean-shift segmentation 
algorithm [7] and determine the occlusion boundaries 
based on [9] to classify the sky, ground, vertical regions. 
By using the 3D coordinate points from SfM, we 
reconstruct a rough depth map based on the above over-
segmentation. Next, we refine the depth map in the 
segments with large re-projection errors. However, the 
re-projection errors are still large after the depth 
refinement process in the regions containing moving 
objects. Thus, we use these regions as seeds to detect the 
regions of moving objects. In the final step, we optimize 
the depth map in the MRF framework to obtain accurate 
depth estimation. 
3.1 SIFT Matching 
To reconstruct the depth map from video, we would 
like to establish the relationship between the neighboring 
frames. The first step is to find reliable corresponding 
points in the video. SIFT is an algorithm to detect and 
describe interest points in the images. The SIFT feature 
description is robust against changes due to image scale, 
noise, illumination and rotation. 
Since there are a lot of features to be compared in 
SIFT algorithm, it must be efficient in matching the 
feature descriptors or it would cost a lot of time. Pele and 
Werman [12] presented a new metric between 
histograms of image features, such as SIFT histogram, 
which is called the Earth Mover’s Distance (EMD) 
variant. They also proposed a linear time algorithm for 
computing the EMD variant with a robust ground 
distance for oriented gradients. By using this approach, 
SIFT matching no longer costs a lot of time and it can be 
completed in real time. 
For the above advantages, we apply the SIFT 
matching to compute the corresponding points across six 
neighboring frames and the corresponding points are the 
input to the Structure from Motion (SfM) algorithm in 
the second step. In order to obtain accurate results in 
SfM algorithm, we referenced six neighboring frames for 
the SIFT feature matching among the selected frames. 
Fig. 2 depicts the idea of the SIFT matching step. 
Assuming the camera in the video moved toward the 
same direction, the more the number of reference frames 
used in the system, the less the corresponding points will 
be extracted for all the reference frames especially in the 
region close to the boundary of the image. The 
corresponding SIFT matching points will be used to 
estimate the camera parameters and the corresponding 
3D coordinates in the second step. To obtain more 3D 
information, we reference only two neighboring frames 
for a selected frame. The details will be given in section 
3.5. 
Fig. 3 depicts the SIFT matching results with 
different numbers of reference frames in the video 
sequence “Poznan_Street”. The red points in the images 
are the matching points. In Fig. 3(a), the total number of 
SIFT matching points is 400; and in Fig. 3(b), the total 
number of SIFT matching points is 768. 
3.2 Structure from Motion 
In this step, we apply the SfM algorithm [13][14] to 
the SIFT correspondence points to estimate the camera 
parameters and 3D structures of these SIFT feature 
points. There are a lot of SfM algorithms proposed in the 
past, and we employ the SfM method [13] proposed by 
Martinec et al. in this step. The idea of their approach, 
which reconstructs 3D structure from 2D image 
coordinates at many different views, is based on fitting a 
low rank matrix to a matrix with missing data. The 
method can deal with higher-level noise data than the 
previous works, and it can work well even with more 
than 90% missing data. We also use the method in [14] 
proposed by Pollefeys et al. for the metric upgrade. 
In the proposed algorithm, we assume there is 
camera motion in the period of video under 
consideration. To estimate the depth map at a specific 
Fig. 1. The flowchart of the proposed depth estimation algorithm. 
also important to us, especially the sky label. Most of the 
previous methods of constructing depth map from 
images cannot provide good depth estimation in the sky. 
The main reason for this is because there is usually no 
texture in the sky region, so there is no geometric clue 
for depth estimation. Once we know where the region of 
sky is, this problem can be resolved easily by assigning 
sky region to a very large depth value. 
An example of applying the algorithm in [9] is 
depicted in Fig. 4. Blue lines in Fig. 4(a) are the 
occlusion boundaries estimating by the method in [9]. 
The arrow on the blue lines denotes the separation of 
background and foreground, and the left side of the 
arrow is in front of the right side of the arrow. Fig. 4(c-e) 
shows the ground, sky and vertical masks, respectively. 
3.5 Initial Depth Map 
We have collected a lot of useful information for 
reconstructing depth map in the previous steps, including 
camera parameters, some reliable 3D points, mean-shift 
over-segmentation, the ordering of the over-
segmentations, and the region of each label. Therefore, 
we use all the information to construct an initial depth 
map. 
The main idea is that the neighboring pixels which 
have similar colors may have similar depth. In the SfM 
step, in addition to the camera parameters for all frames, 
we also obtain a sparse set of 3D feature points. These 
points are the only 3D information so far. Thus, these 3D 
points are used to propagate the depths to the entire 
image. The flowchart of this step is shown in Fig. 5. 
Because the number of 3D points is sparse, it is hard 
to expand to the entire image. Therefore, in the first, we 
added some points from SIFT matching only referring 
two neighboring frames in section 3.1. Giving camera 
parameters and those corresponding points in each frame, 
we can compute the 3D coordinate for each 
corresponding point. Next, we scan all the 3D points to 
decide the range of the depth in the target frame, and 
record which pixels are already assigned with depth 
values. Then, we started to recover the depth map. For 
each segment, we counted how many pixels are already 
assigned with depth values. If the number is less than a 
threshold (5 in our implementation), we do nothing 
because there is not enough information to assign the 
depth for this segment. Otherwise, we consider each 
segment as a plane, and construct a smooth depth map 
for the segment. It is reasonable that depth is related to 
coordinate for each pixel in the segment. We set a linear 
plane, as in eq. (1) for each segment. 
 
           (1) 
 
Note that x and y are the pixel coordinates, a, b, c are the 
plane parameters, and d is the corresponding depth value. 
 
 
[
     
   
     
] [
 
 
 
]  [
  
  
  
] (2) 
 
By using the least-square fitting, we calculate parameter 
a, b, c from some known points, (x1, y1) … (xn, yn) with 
the relation given in eg. (2). 
Not all of the segments have enough information to 
construct the depth. Hence, we propagate the depth from 
the known segments to unknown segments. In the edge 
of known segment and unknown segment, we check the 
similar degree of color. If the color is similar, then set the 
same depth value to the neighboring pixels. After 
propagating the depth, then we repeat constructing depth 
for each segment, which describe above. The steps of 
constructing depth and propagating depth are iterative. 
Fig. 6 depicts some results in this step. The red regions 
represent no depth information. 
 
3.6 Depth Map Refinement 
In the previous step, the initial depth map is rough 
and there is no depth information in some segments. 
Hence, in this step we would correct those segments 
which cause big error or have no depth information in 
the depth map. 
First, we compute the error for each pixel in the 
whole image. For the pixel which has unknown depth 
value, we set a big error; and for those pixels which have 
set the depth value, we compute the projection error. The 
1. Add more 3D information 
 1.1 By using camera parameters and SIFT matching, 
which 
 only refers two neighboring frames, we can get more 3D  
points 
2. Set depth to each segments 
 2.1 For each segment, construct a smooth depth map 
by 
             eg. (2) if it has enough information 
 2.2 Propagate the depth value to neighboring 
Fig. 5.Flowchart of computing the initial depth map. 
Fig. 6. The computed initial depth map. The red regions denote the 
regions in lack of depth information. 
Because we assume the moving object in the neighboring 
frame and current frame would be the same shape and 
not be deformed. Then, we shift the neighboring frame 
by the displacement and subtraction by the current frame. 
We call the result as an error map. Fig. 10 shows the 
processing of making error map. In our experiment, we 
use refer two neighboring frames, so we have two error 
maps. These two error maps are summed to form a 
combined error map. The final error map in Fig. 11(a) 
shows the probability of moving object. The pixels with 
darker intensities are more likely to be moving object. 
The process of finding the moving object is also 
based on segments. Therefore, the combined error map 
based on segment computation is depicted in Fig. 11(b). 
We use the seeded segments as the center, and visit the 
neighboring segments to merge the neighboring 
segments if the associated error is small. We repeat the 
segment merging process iteratively until convergence. 
Fig. 11(d) shows the region of detected moving object in 
this step. 
 
3.8 MRF Optimization 
The previous steps of constructing and refining 
depth map are all segment-based. Because these steps do 
not account for the relationship between neighboring 
segments, the computed depth map usually contains 
obvious errors. To improve the depth map estimation, we 
integrate the information in a Markov Random Field 
(MRF) framework to optimize the depth map. 
Let the depth map of current frame be represented 
by D. The image of the current frame is denoted by I and 
  ̂                is the set of all the reference 
frames. Then, we define the following energy function 
for the MRF formulation: 
 
  ( ̂     ̂)    ( ̂      ̂)      ( ̂   ) (6) 
 
where the data term ED measures the projection error, the 
smoothness term ES represents the smoothness on the 
depth map between neighboring pixels, and α is the 
weight used to balance these two terms.  ̂ is the refined 
depth map by MRF. 
The definition of the data term   ( ̂    ) is given in 
eq. (7). It computes the color difference between the 
corresponding points in the neighboring frames based on 
the current depth estimate. 
 
   ( ̂      ̂)
 ∑∑    )(pI-I(p) tt    
 
      
 
(7) 
where         ( 
    ̂          )      (8) 
 
pt is the corresponding point of a pixel p in the t
th
 
reference frame, and δ is the threshold for the color 
difference. The symbols Kt, Rt, tt denote the camera 
parameters at the t
th
 reference frame. K, R, t are the 
Fig. 10. The flow of image displacement and subtraction between 
neighboring frames. 
Fig. 11. (a) The final error map based on pixel computation. (b) 
The final error map based on segment computation. (c) The seeded 
segments. (d) The detected moving object region. 
Fig. 12. The depth map after the MRF optimization. 
 
6. Reference  
[1] A. Saxena, M. Sun, and A. Y. Ng. Make3D: 
Learning 3D Scene Structure from a Single Still 
Image. In IEEE Trans. on Pattern Analysis and 
Machine Intelligence, 2008. 
[2] B. Liu, S. Gould, D. Koller. Single Image Depth 
Estimation From Predicted Semantic Labels. In 
Proc. International Conference on Computer 
Vision and Pattern Recognition, 2010. 
[3] G. Zhang, J. Jia, T. Wong, H. Bao, Recovering 
Consistent Video Depth Maps via Bundle 
Optimization. In Proc. International Conference on 
Computer Vision and Pattern Recognition, 2008. 
[4] G. Zhang, J. Jia, T. Wong and H. Bao. Consistent 
Depth Maps Recovery from a Video Sequence. In 
IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 2009. 
[5] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein 
and R. Szeliski. A Comparison and Evaluation of 
Multi-View Stereo Reconstruction Algorithms. In 
Proc. International Conference on Computer 
Vision and Pattern Recognition, 2006. 
[6] R. A. Newcombe and A. J. Davison. Live Dense 
Reconstruction with a Single Moving Camera. In 
Proc. International Conference on Computer 
Vision and Pattern Recognition, 2010. 
[7] D. Comanicu and P. Meer. Mean shift: A robust 
approach toward feature space analysis. In IEEE 
Trans. on Pattern Analysis and Machine 
Intelligence, May 2002. 
[8] P. F. Felzenszwalb and D. P. Huttenlocher. 
Efficient Graph-Based Image Segmentation. In 
International Journal of Computer Vision, 2004. 
[9] D. Hoiem, A. A. Efros and M. Hebert. Recovering 
Occlusion Boundaries from an Image, In 
International Journal of Computer Vision, 2010. 
[10] J. Sun, H. Y. Shum, and N. N. Zheng. Stereo 
matching using belief propagation. In Proc. 
European Conference on Computer Vision, 2002. 
[11]  P. Felzenszwalb and D. Huttenlocher. Efficient 
belief propagation for early vision. In International 
Journal of Computer Vision, 2007. 
[12] O. Pele, M. Werman. A Linear Time Histogram 
Metric for Improved SIFT Matching. In Proc. 
European Conference on Computer Vision, 2008. 
[13] D. Martinec, T. Pajdla. 3D Reconstruction by 
Fitting Low-Rank Matrices with Missing Data. In 
Proc. International Conference on Computer 
Vision and Pattern Recognition, 2005. 
[14] M. Pollefeys, L. Van Gool, M. Vergauwen, F. 
Verbiest, K. Cornelis, J. Tops, and R. Koch. Visual 
modeling with a hand-held camera. In International 
Journal of Computer Vision, 2004. 
[15] K. Alsabti, S. Ranka and V. Singh. An Efficient k-
means Clustering Algorithm. In Pattern Recognit. 
Lett., vol. 14, no. 10, pp. 763 - 769, 1993. 
[16] V. Hedau, D. Hoiem, and D. Forsyth. Recovering 
the Spatial Layout of Cluttered Rooms”, In Proc. 
International Conference on Computer Vision, 
2009. 
[17] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. 
Kolmogorov, A. Agarwala, M. Tappen, and C. 
Rother. A Comparative Study of Energy 
Minimization Methods for Markov Random Fields. 
In Proc. European Conference on Computer 
Vision, 2006. 
[18] Y. Boykov, O. Veksler, and R. Zabih. Fast 
Approximate Energy Minimization via Graph 
Cuts. In IEEE Trans. on Pattern Analysis and 
Machine Intelligence, 2001. 
[19] V. Kolmogorov and R. Zabih. What Energy 
Functions can be Minimized via Graph Cuts?. In 
IEEE Trans. on Pattern Analysis and Machine 
Intelligence, 2004. 
[20] G. Um, G. Bang, N. Hur, J. Kim and Y.-S. Ho. 
Test Sequence “Lovebird1&2.  
[21] M. Domański, T. Grajek, K. Klimaszewski, M. 
Kurc, O. Stankiewicz, J. Stankowski and K. 
Wegner. Poznań Multiview Video Test Sequences 
and Camera Parameters. ISO/IEC 
JTC1/SC29/WG11 MPEG 2009/M17050, Xian, 
China, October 2009. 
 
 
 
 
 Fig. 15. The 4th frame in the lovebird1 data set. (a) The current frame, (b) the initial depth map, and (c) the depth map after MRF optimization. 
Fig. 16. The 4th frame in the lovebird2 data set. (a) The current frame, (b) the initial depth map, and (c) the depth map after MRF optimization. 
Fig. 17. (a) The 50th frame in the Road sequence. (b) The depth map estimated by our method. (c) The depth map estimated by [4]. 
Fig. 18. (a) The 53th frame in the Stair sequence. (b) The depth map estimated by our method. (c) The depth map estimated by [4]. 
 
(a) (b) (c) 
(a) (b) (c) 
(a) (b) (c) 
(a) (b) (c) 
國立清華大學補助員生出國參加國際學術會議報告 
                                    序號： 
姓    名 蘇德峰 單  位  資工系博士班 職  稱 博士班研究生 
會議期間 2012/03/27-2012/03/30 會議地點 
日本京都  
Kyoto, Japan 
會議名稱 
2012 IEEE International Conference on Acoustics , Speech, and Signal Processing  
(ICASSP 2012) [2012 國際音學、語音與訊號處理研討會] 
論文名稱 Parallelized random walk algorithm for background substitution on a multi-core embedded system 
報告內容 
 
很榮幸的在歷經投稿、審查委員的意見，最後一致同意接受此篇投稿。2012 IEEE 
International Conference on Acoustics , Speech, and Signal Processing 是 IEEE Signal Processing 
Society 主辦，針對音訊、影像與訊號處理為主題的國際性重要會議，今年在日本京都國際
會議中心舉行為期四天的會議。而在主會議開會期間，由於各種議題發表的論文非常多，因
此我主要挑選與我研究主題相關的 oral session 為主，再聽完 oral paper之後，在把握時間去
相關的 poster paper 與發表者討論內容。 
 
我所發表的paper被安排在主會議的最後一天(3/30)以poster的型式發表。我們主要提出
一個快速的Random Walk演算法，平行化實現在一個多核心嵌入式平台上以提升速度。並將
此技術用於影像背景替換的應用上，並獲得不錯的效率與結果。在我的poster session，有許
多人對於我們的研究感到興趣並提出問題互相討論，與不同國家的研究人進行討論，例如針
對不同的嵌入式平台的比較，或是演算法的討論都讓我收穫豐富。從27日至30日參與會議議
程，也認識了許多不同國家同領域的研究學者，在彼此交流的過程也獲益良多。  
 
而我在這次的會議中，也能感覺得到，能夠口頭報告的論文都有很好的研究價值，而即
使是以海報呈現的論文也有相當不錯的水準。參與國際會議能迅速掌握目前研究者的研究方
向，對自己熟悉領域能更確切了解最新發展，對不熟悉的部份，可能藉由交流進一步激盪出
不同火花，確實讓我增廣了不少見聞與國際觀，並且從與各國研究人員的討論交流中，更是
獲得了寶貴的經驗與建議，讓我覺得此行有很大的收穫。 
 
最後在此除了感謝指導教授賴尚宏老師的指導與鼓勵外，也相當感激中研院所設置的補
助，讓我能夠參加此次盛會。除了增廣見聞外，也看到國外學者的創意與研究熱忱，讓我相
當激動與興奮，期許自己也能跟他們一樣，能夠更深入研究，也讓國內的研究人員能夠與國
際人才交流。今年我在會場好幾篇台灣的論文發表，台灣近年來研究從注重研究發表數量轉
往研究發表品質，我相信藉由鼓勵台灣研究生出國出席一流會議從而提升研究水準是很值得
的投資。 
 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/12/04
國科會補助計畫
計畫名稱: 以統計學習對單一影像或視訊之三維深度估測
計畫主持人: 賴尚宏
計畫編號: 98-2221-E-007-089-MY3 學門領域: 視訊與影像分析
無研發成果推廣資料
Proc.Visual 
Communication 
and Image 
Processing 
Conference 
(VCIP), Tainan, 
Taiwan, Nov. 
2011 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 9 3 100% 
演算法設計，撰寫
程式及實驗之進
行 
1 位未領薪水、6
位實領 3 個月、2
位實領 7 個月 
博士生 2 3 100% 
演算法設計，撰寫
程式及實驗之進
行 
 
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
我們已發展出整合多項技術的視訊深度圖估計方法，其中不只使用到單一影像
的統計學習資訊，也利用到了視訊中時間與空間的關係，由時間與空間的關係
進一步找出移動物體的區域並整合到一個圖模型中。 
該方法可以提供出較過去準確的深度圖，在未來也仍具有發展性，有許多項目
值得繼續進行研究，包含：在馬可夫隨機場模型中加入更多約束使得深度估測
更為精準，另一個方向是將所發展系統進行加速而達成即時的運算。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
