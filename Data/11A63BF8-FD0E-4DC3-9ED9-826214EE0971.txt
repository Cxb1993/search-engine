 2 
本研究發展出能在未知與大範圍環境中合作巡航
的異質機器人系統。針對不同運動形式的異質機器
人，選擇適當的SLAM方法。感測模型將使用具尺度
與方位不變的區域特徵偵測與追蹤方法，以有效辨識
特徵與維持適量的特徵數目。異質機器人在巡航過程
中將利用視覺式感測器與網路通訊合作完成訊息傳遞
任務，例如傳遞特徵資訊、傳遞SLAM所需的位置資
訊、協助修正地圖資訊、協助重置SLAM失敗的機器
人、以及輔助機器人在不同面向辨識物件等。另外，
也將探討異質機器人在未知與大範圍環境中即時實現
SLAM與合作巡航等問題。本研究將針對人形與兩輪
驅動兩種異質機器人進行探討與測試。 
整合異質機器人執行合作任務一直是多行動機器
人控制問題中非常具有挑戰性的議題。因為異質機器
人的運動模型各不相同，要有效地整合為相互合作的
系統，必須選擇適當的SLAM方法，以便機器人系統
準確地自我定位與合作執行任務。另外，受限於機器
人的感測能力，研究人員一直挑戰更好更快的感測模
型，結合不同的SLAM方法，以便應用在不同形式的
異質機器人系統。目前影像處理領域已發展出很多適
合機器人即時運作所需的特徵偵測與追蹤演算法，修
改後可做為機器人的視覺式感測模組。針對這些目
標，本研究直接切入困難度較高的異質機器人視覺式
合作巡航之研究議題，整體探討SLAM的運動模型與
感測模型搭配異質機器人的整合問題，期望與國外相
關的研究進度縮小差距甚至超越。我們以適合異質機
器人的自我定位與地圖建立方法，整合人形與輪式驅
動兩種機器人，在未知且大範圍的環境中進行SLAM
訊息傳遞與合作巡航任務，就本人所知仍未有文獻如
此嘗試。 
 
2. 機器人 EKF SLAM 規劃 
本計畫只通過執行一年，本年度的研究成果
(2009.8~2010.10)除了完成相關文獻的蒐集與整理之
外，使用具尺度與方位不變的區域特徵偵測方法，搭
配擴張型卡爾曼過濾器 (EKF) ，針對自由移動
(free-moving)的單眼攝影機進行系統狀態的估測。本研
究提出有效率的特徵初始化程序，並重新定義好的特
徵(good features)，以及發展配合 EKF 的影像深度計算
方法。最後，將發展完成的自由移動攝影機系統的狀
態估測方法，應用於人形機器人，提供人形機器人系
統於未知環境中進行 SLAM 與巡航等任務。具體的研
究項目與採用方法分別說明如后。 
 
2.1. 區域特徵偵測與追蹤 
Bay et al. [2008]針對尺度與旋轉之不變量，提出
加速強健特徵 (SURF)，建立區域特徵的偵測器
(detector)與描述器(descriptor)。偵測器用以偵測區域特
徵的點座標，結合 Hessian 矩陣概念與積分影像方法，
縮短影像與高斯濾波器進行迴積運算所花費的時間。
而描述器則利用特徵點鄰近區域內的 Haar 小波響應
(Haar wavelet responses)分佈，建立特徵點的描述向
量。在不犧牲特徵偵測與描述效果的情況下，SURF
計算的速度優於 SIFT 方法。 
SURF 根據 Hessian 矩陣偵測特徵點，找尋 Hessian
矩陣行列式(determinant)的最大值位置即是特徵點的
座標。相對於影像 I 上的點 x=(x,y)，Hessian 矩陣 H(x,σ)
在點 x 與尺度 σ的定義為： 
⎥⎦
⎤⎢⎣
⎡=
),x(),x(
),x(),x(
),x( σσ
σσσ
yyxy
xyxx
LL
LL
Η  
其中 Lxx(x,σ)為影像 I 上的點 x 與高斯二階導數
(Gaussian second order derivative) 22 /))(( xg ∂∂ σ 的迴
積，同理定義 Lxy(x,σ)與 Lyy(x,σ)。SURF 使用盒子過濾
器(box filter)搭配影像積分(image integral)方法，改善
Hessian 矩陣在計算上費時的缺點。高斯二階導數搭配
尺度 σ=1.2，其效果與使用如圖 1 所示的 9×9 盒子過
濾器相同。盒子過濾器以 Dxx, Dyy與 Dxy取代 Hessian
矩陣的元素，則 Hessian 矩陣的行列式可以近似求算
為 
2
approx )()det( xyyyxx wDDDΗ −=  
其中 w 為使方程式平衡的權重係數，w 設為 0.9。
Hessian 矩陣使用尺度不變的概念偵測特徵，亦即包含
尺度空間變數σ，可以偵測到不同尺度的區域特徵
[Lindeberg 1998]。例如同一特徵位於不同距離位置也
可以有效地偵測到，相較固定影像區塊(image patch)
的特徵偵測更具強健性。依據盒子過濾器所偵測出的
特徵點數量眾多，必須加以過濾，稱為特徵點在影像
尺度空間的定位。SURF 首先在鄰近 3×3×3 的空間中
使用非最大值抑制(non-maximum suppression)，接著將
Hessian 矩陣行列式的最大值在影像尺度空間中做內
插，以便更精準地定位特徵點。 
描述器的建立之前，先訂出特徵點的方位
(orientation)，假若特徵描述向量建立在此方位上，特
徵比對時則不必再考慮特徵旋轉的問題。因此，特徵
點方位的建立是為了達到影像旋轉的不變性
(orientation invariant)，方法是給予特徵點一個可重現
的方位。區域影像特徵描述向量定義每個子區域中將
有 4 維的特徵向量，表示為 v=(Σdx,Σdy,Σ|dx|,Σ|dy|)，其
中 Σdx與 Σdy為 Harr 小波反應值的總和值。總計整個
特徵描述向量的維度為 4×4×4=64，如圖 2 所示。影像
特徵偵測後所建立的特徵描述向量將應用於影像特徵
的追蹤程序。 
影像特徵的追蹤程序方面，本研究使用集合對集
合(set-to-set)的特徵比對追蹤方法，與影像區塊特徵的
集合對區域 (set-to-region) 的差異平方和 (sum of 
squared differences, SSD)追蹤方法不同，優點是集合對
集合的電腦計算負擔較小，可以提高特徵追蹤效率。
本研究的區域特徵比對程序為：首先，計算特徵描述
向量與參考影像的特徵描述向量之間的尤基里德距離
(Euclidean distance)。其次，使用最鄰近比例比對策略
(the nearest neighbor ratio matching strategy)[Baumberg 
2000, Mikolajczyk and Schmid 2003, Lowe 2004]進行
比對。最後，利用限定比對比例(matching ratio)的方式
找到正確的對應特徵。比對比例定義為最小尤基里德
距離相對於第二小尤基里德距離的比值。假如比對比
例接近 0.7 倍時，亦即 
7.0
2
1 ≈=
nd
st
d
dr  
則可稱兩個特徵點比對成功。以圖 3(a)與(b)說明特徵
偵測與追蹤範例，圖 3(a)左上角小圖為參考影像，右
 4 
   mifor
h
h
kfv
h
hkfu
I
I
C
iz
C
iy
vC
C
iz
C
ix
uC
iy
ix
ik  , ,2 ,1          
0
0
L=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
+
+
=⎥⎦
⎤⎢⎣
⎡=z  (2.6)        
其中 Iix與 Iiy為第 i 個特徵點經由透視投影至影像平面
的座標值，單位為像素(pixel)； m 表示 k 時刻量測向
量的特徵點個數；fC為攝影機焦距，( 0u , 0v )為影像平
面中心點的像素值， uk 及 vk 分別為影像平面水平方
向和垂直方向的校正參數，假設影像平面無扭曲現
象，可以令 uk 與 vk 為 1； TCizCiyCixCi hhhh ][= 稱為視
線(ray)向量，是特徵點相對於攝影機中心的三維座標。 
 
2.3 特徵點初始化 
進行 SLAM 之前須先規劃特徵初始化程序，包括
選取好的特徵、計算影像深度、計算特徵三維座標、
以及處理特徵追蹤錯誤問題等。影像追蹤過程中，有
三種情況會造成影像特徵在追蹤過程中遺失或誤
判：第一種是當特徵落在影像的外面，可以使用影像
特徵點的 3 維座標判斷特徵是否落在影像外面，在運
算過程中可以預期此種特徵遺失現象的發生；第二種
情況則是特徵與其附近區域的影像發生混淆的現
象。此狀況表示特徵點與其附近區域的影像並非同在
三維空間中相近位置，導致追蹤過程中發生特徵遮蔽
或遺失。第三種情況是觀測位置變動時，導致影像特
徵變形，因而追蹤到的影像特徵並非所要追蹤的特
徵。第二與第三種情況的特徵都不是好的特徵，必須
在追蹤過程當中過濾掉。第二種情況追蹤的影像特徵
會自然遺失，再補充特徵之外不必做額外的處理。針
對第三種情況，可以使用追蹤視窗概念檢測特徵是否
為所要追蹤的影像特徵。本研究規劃以下程序，以便
在追蹤過程中選取好的特徵： 
a. 利用 SURF 影像特徵偵測與追蹤方法，追蹤 C1
個強度(strength)最強的特徵，稱為 C1特徵集合； 
b. C1特徵集合中部分特徵會因為以上所提第一或第
二情況導致特徵遺失，再補足總數為 C1個追蹤特
徵的集合； 
c. C1特徵集合中部分特徵會因為以上所提第三情況
導致特徵誤判，以追蹤視窗概念過濾掉錯誤特
徵，再補足總數為 C1個追蹤特徵的集合。 
d. 由 C1 個追蹤特徵中，選取 C2 個特徵計算其影像
深度，稱為 C2特徵集合。同時以高斯分佈的均值
(mean)與標準差(standard deviation)描述影像深度
的分佈與收斂情況。 
e. 由 C2 特徵集合中，選取 C3 個影像深度收斂的特
徵，稱為 C3特徵集合，提供機器人進行 SLAM 程
序。 
因此，本研究定義好的特徵(good features)為：影像追
蹤過程中不發生非正常遺失(第二與第三種情況)，且
其影像深度已收斂者(例如落在 3 個標準差之內)。另
外，為了避免特徵點在追蹤時，位於遠與近的位置之
特徵點相互干擾，造成影像深度計算的誤差過大。可
以限定特徵集合的影像深度範圍。 
圖 4 為追蹤特徵點與影像深度收斂的範例，攝影機
位於(0,0)，Y 軸朝向影像深度方向。設定 C1為 30 個
特徵的集合，而 C2與 C3分別為 20 與 10 個特徵的集
合。追蹤 10 個點特徵點並持續追蹤 10 步距，特徵點
的位置分佈結果如圖 4(a)所示，圖中橢圓顯示一個標
準差的分佈範圍。圖 4(b)與(c)分別顯示 3 個步距與 5
個步距的計算後，特徵點位置的分佈，其均值皆已落
在 10 步距所計算的一個標準差分佈範圍內。因此，以
3~5 步距所計算特徵點位置均值，可以當作特徵點已
收斂的三維位置座標值。 
 
圖 4(a) 追蹤 10 個點特徵點 10 步距時特徵分佈圖(Y
軸深度方向) 
 
圖 4(b) 追蹤 3 個步距的特徵分佈  圖 4(c) 追蹤 5 個
步距的特徵分佈 
 
2.4 影像深度計算 
本研究使用的單眼視覺不具有自動對焦功能，無
法測得特徵點的影像深度(image depth) d，必須使用 2
張影像求算影像深度，並假設過程中特徵點不曾移
動，而且視角不能太小。本研究修改 Montiel et al. 
[2006]提出的概念求算影像深度。圖 5 中時間 k 時特
徵點的位置向量可以表示為 ( ) )()( khkrkY ii +=  
使用球體座標將特徵點修改為以 6 個變數描述空間中
的位置， 
)( iiizyxi rrrfY φθρ=  
其中 rx、ry、rz為機器人攝影機在空間中的位置，將此
點當做中心設定球體座標；ρi 代表特徵點的影像深度
的倒數， id ρ/1= ，以避開影像深度在計算上的影響；
θi 是相對於球體座標的經度(longitude)；φi 則是緯度
(latitude)。視線向量表示為 
( ) ( ) ),(
1
ii
i
i mk
kh φθρ=  
其中 m(θi,φi)為視線向量的單位方向向量，起點在攝影
機中心 r；當時間 k 新的特徵點被偵測到時，利用擷
取的特徵點影像像素值[Iix(k) Iiy(k)]T，以及攝影機的位
置 r(k)與角度θ(k)、φ(k)，可以初始化特徵點的位置。
由於特徵點影像訊息是標示在攝影機座標，直接在攝
影機座標求算角度θ(k)與φ(k)以及影像深度，計算較為
 6 
設 定 機 器人 上 的 攝影 機 初 始地 圖 座 標為
(17,-70,13)，此座標約為機器人蹲下時，實際攝影機
中心距離雙圓形特徵的位置；x 軸方向以機器人向右
為正，y 軸為攝影機與特徵位置的距離。取樣時間為
機器人行走一步並進行 EKF SLAM 程序一次的間隔
時間，約為 13sec。啟動後機器人靜止，執行系統啟
動程序以確定 EKF 狀態估測收斂，接著機器人向正 x
方向緩慢移動，每步約移動 2~3cm。圖 9 顯示實測中
所拍攝的機器人連續移動影像，EKF SLAM 所得地圖
如圖 10 所示為 3D 環境立體圖。圖中圓圈表示被偵測
到的區域特徵，經特徵初始化後成為 SLAM 的地
標。”*”的連線表示機器人移動路徑，估測機器人向正
x 方向移動約 65cm。 
 
圖 8 實測環境 
  
  
  
圖 9 機器人連續移動截圖 
 
圖 10 估測 3D 立體圖 
 
4.2 自由移動攝影機的 EKF SLAM 
在本範例自由移動攝影機移動一個方形路徑，進
行 SLAM。攝影機向右移動 90cm、向後移動 90cm、
向左移動 90cm、向前移動 90cm，最後再向右移動
90cm。此實測結果共拍攝 947 張影像，實驗影像大小
為 320×240。圖 10 為描繪攝影機估測三維路徑圖。圖
11 為此範例的八張截圖，各時刻左圖為當時所擷取的
環境影像，圖右邊為 Matlab 所繪製的上視圖。圖 11(a)
為系統啟動，已知 4 個地圖特徵標示如圖左邊；圖 11(b)
為影像特徵初始化；圖 11(c)右邊圖的橢圓代表特徵點
的高斯分佈狀態，可看出地圖特徵都有收斂的情況。
而且感測器視線離開已知地圖特徵時，EKF SLAM 依
然成功執行。圖 11(d)至(h)分別為攝影機向後移動、
向左移動、向前移動、在向右移動。攝影機回程時依
舊可以穩定資料關聯成功之前的地圖特徵，而且 EKF 
SLAM 也穩定地運作。 
 
圖 11 MonoSLAM 三維立體圖 
 
(a) 91th影像：系統啟動 
 
(b) 140th影像：影像特徵初始化 
 
(c) 420th影像
 
(d) 496th影像 
 8 
Problem, IEEE Transactions on Robotics and Automation, 
vol.17, no. 3, pp. 229–241. 
[13] Durrant-Whyte, H., and T. Bailey, 2006, Simultaneous 
Localization and Mapping: Part 1, IEEE Robotics and 
Automation Magazine. 
[14] Durrant-Whyte, H., and T. Bailey, 2006, Simultaneous 
Localization and Mapping: Part 2, IEEE Robotics and 
Automation Magazine. 
[15] Ferguson, D., and A. Stentz, 2006, Using Interpolation to 
Improve Path Planning: The Field D* algorithm, Journal of 
Field Robotics, pp.79-101. 
[16] Grisetti, G., C. Stachniss, and W. Burgard, 2007, Improved 
Techniques for Grid Mapping With Rao-Blackwellized 
Particle Filters, IEEE Transactions on Robotics, Vol.23, 
No.1, pp.34-46. 
[17] Hähnel, D., 2005, Mapping with mobile robots, PhD 
Dissertation, University of Freiburg. 
[18] Ho, K.L. and P. Newman, 2007, Detecting Loop Closure 
with Scene Sequences, International Journal of Computer 
Vision, vol. 74, no.3, pp.261-286. 
[19] Kalman, R.E., 1960, A New Approach to Linear Filtering 
and Prediction Problems, ASME. 
[20] Karlsson, N., E. Di Bernardo, J. Ostrowski, L. Goncalves, 
P. Pirjanian, and M.E. Munich, 2005, The vSLAM 
Algorithm for Robust Localization and Mapping, IEEE 
International Conference on Robotics and Automation. 
[21] Lindeberg, T., 1998, Feature detection with automatic scale 
selection, International Journal of Computer Vision, vol.30, 
no.2, pp79-116. 
[22] Liu, J.S., R. Chen, and T. Logvinenko, 2001, A theoretical 
framework for sequential importance sampling and 
resampling, in Sequential Monte Carlo Methods in Practice, 
A. Doucet, N. de Freitas, and N. Gordon, Eds., pp. 225-246, 
Springer, New York, NY, USA, January 2001. 
[23] Lowe, D.G., 1999, Object recognition from local 
scale-invariant features, Proceedings of International 
Conference on Computer Vision, pp.1150-1157. 
[24] Lowe, D.G., 2004, Distinctive Image Features from 
Scale-Invariant Keypoints, International Journal of 
Computer Vision, 60, 2, pp. 91-110. 
[25] Mikolajczyk, K., and C. Schmid, 2001, Indexing based on 
scale invariant interest points, Proceedings of International 
Conference on Computer Vision, vol.1, pp.525-531. 
[26] Mikolajczyk, K., and C. Schmid, 2003, A performance 
evaluation of local descriptors, Proceedings Computer 
Vision and Pattern Recognition, pp.257-263. 
[27] Montemerlo, M., 2003, FastSLAM: a factored solution to 
the simultaneous localization and mapping problem with 
unknown data association, Ph.D. dissertation, Carnegie 
Mellon University, Pittsburgh, PA. 
[28] Montiel, J.M.M., J. Civera, and A.J. Davison, 2006, 
Unified Inverse Depth Parametrization for Monocular 
SLAM, Proc. Robotics: Science and Systems. 
[29] Murillo, A.C., J.J. Guerrero, amd C. Sagues, 2007, SURF 
features for efficient robot localization with 
omnidirectional images, Proceedings IEEE International 
Conference on Robotics and Automation, pp.3901-3907. 
[30] Murphy, K., and S. Russell, 2001, Rao-Blackwellised 
Particle Filtering for Dynamic Bayesian Networks, in 
Sequential Monte Carlo Methods in Practice, Springer 
Verlag. 
[31] Pfaff, P., R. Triebel, and W. Burgard, 2007, An efficient 
extension to elevation maps for outdoor terrain mapping 
and loop closing, The International Journal of Robotics 
Research, vol.26, no.2, pp.217-230. 
[32] Rekleitis, 2004, A Particle Filter Tutorial for Mobile Robot 
Localization, Technical Report. TR-CIM-04-02, Centre for 
Intelligent Machines, McGill University, Montreal, Quebec, 
Canada. 
[33] Smith, R., M. Self, and P. Cheeseman, 1990, “Estimating 
Uncertain Spatial Relationships in Robotics,” In 
Autonomous Robot Vehicles, I.J. Cox and G.T. Wilfong, 
Eds., Springer-Verlog, pp.167-193. 
[34] Stentz, A., 1995, The Focussed D* Algorithm for 
Real-time Replanning, Proceedings of the International 
Joint Conference on Artificial Intelligence. 
[35] Sturm, P., and S. Maybank, 1999, On Plane based Camera 
Calibration A General Algorithm, Singularities, 
Applications, In Proceedings of the IEEE Conference on 
Computer vision and Pattern Recognitions, pp.432-437. 
[36] Thrun, S., 2002, Robotic mapping: A survey, in Exploring 
Artificial Intelligence in the New Millennium, G. 
Lakemeyer and B. Nebel, Eds.. San Mateo, CA: Morgan 
Kaufmann, ch. 1. 
[37] Thrun, S., 2003, Learning Occupancy Grid Maps With 
Forward Sensor Models, Autonomous Robots, vol.15, 
pp.111-127. 
[38] Tsai, R.Y., 1987, A versatile camera calibration technique 
for high accuracy 3D machine vision metrology using off 
the sheff TV cameras and lenses, IEEE J. of Robotics and 
Automation, 3(4), pp.323-344. 
[39] Welch, G., and G. Bishop, 2006, An Introduction to the 
Kalman Filter, Department of Computer Science, 
University of North Carolina. 
[40] Wu, C.Y., and L.C. Fu, 2007, An integrated robotic 
vSLAM system to realize exploration in large indoor 
environment, Proceedings CACS International Automatic 
Control Conference. 
[41] Zhang, Z., 2000, A Flexible New Technique for Camera 
Calibration, IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 22, no. 11. 
 
 
Y.-T. WANG, M.-C. LIN, R.-C. JU AND Y.-W. HUANG 
478 
we develop a robust and efficient feature initialization algorithm for MonoSLAM. Second, 
the MonoSLAM is modified and applied to detect and track a moving object. 
 
2. Feature Initialization for SLAM. During the procedure of SLAM, the robot locates its 
global coordinate by consulting the positions of a group of fixed features or beacons in the 
environments. In the research, the MonoSLAM developed by Davison et al. [2] is utilized to 
implement SLAM of a small-size humanoid robot. MonoSLAM utilizes an extended 
Kalman filter (EKF) to update the estimation of the robot state and the map of beacons in 
the environments, recursively. An image feature initialization algorithm is proposed in the 
paper for SLAM and moving object detection. The procedures of the algorithm include 
detection of image features, selection of good features, calculation of image depths, and 
update of feature locations. The algorithm is described as follows. 
(a) Capture an image and smooth the image by using a Gaussian smoothing operator; 
(b) Utilize a detecting and tracking method to track a set of image features, called C1; 
(c) In the image set C1, select good features which are successively detected and tracked 
in a series of images, and discard those bad features which are lost during tracking; 
(d) Choose c2 features from the remaining features in set C1 and compute the image depth 
of these features, named image set C2. Abandon the features whose image depths are 
not convergent after several iterations of calculation; 
(e) Choose c3 features from image set C2 whose image depths are convergent. Compute 
the 3D coordinates for these features, called image set C3. Therefore, these features are 
initialized and ready for SLAM and moving object detection. 
In Step (b), the features are extracted from the image by using the SURF method 
proposed by Bay et al. [7]. SURF is a scale-invariant method for detection of image 
features. It detects region features from an image and obtains the location and the descriptor 
vector of each interest point. The basic concept of a scale-invariant method is to detect 
image features by investigating the determinant of Hessian matrix H [8]. Bay et al. [7] 
utilize a box filter to process on the image instead of calculating the Hessian matrix, and 
then the determinant of Hessian matrix is approximated by 
2
approx. )()det( xyyyxx wDDDΗ −=  
where Dij are the images filtered by the corresponding box filters; w is a weight constant. 
The interest points or features are extracted by examining the extreme value of determinant 
of Hessian matrix. Furthermore, the unique properties of the extracted features are 
described by using a 64 dimensional descriptor vector as depicted in Fig. 1. An example of 
feature extraction is depicted in Fig. 2. Many regional features are detected in this image 
and illustrated as white circles. 
The extracted features from two successive images can be matched by checking the 
Euclidean distance d between the corresponding descriptor vectors, and using the nearest 
neighbor ratio matching strategy [9]. Therefore, the region features of two images can be 
tracked efficiently. The matching ratio r is defined as the ratio of the smallest distance d1st 
to the second smallest distance d2nd. If the ratio r closes to 0.7, we say that these two 
features are matched. 
Y.-T. WANG, M.-C. LIN, R.-C. JU AND Y.-W. HUANG 
480 
( ) ( ) tkvkRkhkh cWcici Δ++−=+ )1(1)(1  
v is the velocity vector of the robot. Therefore, the image depth l(k) of the feature can be 
determined with known robot states which are provided by EKF updates. That is, 
( ) ( ) tkvkRkhmkl cWcicici Δ++++= )1(11),()( φθ  
During the camera motion, the depth or 3D coordinates of the image features will converge 
into a probabilistic distribution area, as shown in Fig. 4. When the image depth converges, 
the corresponding feature is selected to be a beacon for the calculation of EKF SLAM. 
 
cz
iZ
p
cy
cx
h
z
y
x
beacon
}{C
}{W
  
 Fig. 3 Robot coordinate systems Fig. 4 Probabilistic distribution of 3D features 
 
3. Moving Object Detection. The moving object can also be detected by investigating the 
moving features, as shown in Fig. 5. The moving object map-based detector is developed 
for detecting the moving objects, and the details of this detector can be found in [4]. 
 
B
A C
0B
x y
zcx
cy
cz
object moving
∗ iY
∗
∗
∗ ∗ ∗
beacons fixed
jm 1+jm
2+jm
camera
oxoy
oz
 
B
A
C
kB
x y
zcx
cy
cz
∗ iY
∗
∗
∗ ∗ ∗
beacons fixed
jm
1+jm
2+jm
camera
∗ ∗
∗
 
 (a) (b) 
Fig. 5 Initial state t=0 (a) and current state t=k (b) of fixed beacons and moving object 
 
4. Experimental results. A small-size humanoid robot is designed and fabricated for 
demonstration, which is equipped with a Windows-based industrial PC. Fig. 6 depicts the 
appearance of the designed robot. The control system comprises three subsystems, 
including a vision sensor system, a PC-based control system, and a motor drive system. 
Each subsystem is able to work independently and also in coordination with each other. The 
camera system is the only sensor to provide the robot position information in the 
environments. In this research, we develop the control system by utilizing a Microsoft 
Windows-based industrial PC, Wafer-LUKE533, provided by a local vendor. The fabricated 
Y.-T. WANG, M.-C. LIN, R.-C. JU AND Y.-W. HUANG 
482 
Table 1 Displacement and orientation of the moving object 
 
Object Displacement (cm) Object Orientation (degree)  
Location x y z yaw pitch roll 
B0 -18.5  90.0 23.0   0˚    0˚    0˚ 
B1 -10.0 102.5  6.7 5.8˚ -15.1˚ -35.0˚ 
B2 -12.1  77.5 21.4  -8.6˚  15.3˚  52.6˚ 
B3 -22.1 101.7 26.5  4.7˚  38.9˚ -21.3˚ 
 
5. Conclusions. In this research, we developed an image feature initialization algorithm for 
MonoSLAM and moving object detection. The tasks are implemented on a PC-based 
controller for a small-size humanoid robot. The contribution of this research is in two 
aspects: firstly, an improved algorithm for image feature initialization is developed for the 
EKF-based SLAM by using SURF, a scale-invariant feature extraction method. Secondly, 
the moving object detection is performed by using the same feature initialization algorithm, 
concurrently. Experimental works are performed in this paper and the results show that the 
SLAM with the proposed feature initialization algorithm has the capability to support the 
humanoid robot simultaneously navigating and detecting moving objects in the 
environments. The developed detector can determine rigid moving objects at this time. In 
the future, we will extend the method to detecting multiple objects with different moving 
velocities, or non-rigid moving objects like human body with articulated arms and legs. 
 
Acknowledgment. This work was supported in part by the National Science Council in 
Taiwan under grant no. NSC95-2221-E-032- 055-MY2. 
 
REFERENCES 
 
[1] M.W.M.G. Dissanayake, P. Newman, S. Clark, H. Durrant-Whyte, and M. Csorba, A solution to the 
Simultaneous Localization and Map Building (SLAM) Problem, IEEE Transactions on Robotics and 
Automation, vol.17, no. 3, pp. 229–241, 2001. 
[2] A.J. Davison, I.D. Reid, N.D. Molton, and O. Stasse, Mono SLAM: Real-time single camera SLAM, 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.29, no.6, pp.1052-1067, 2007. 
[3] M. Montemerlo and S. Thrun, FastSLAM, Springer-Verlag, 2007. 
[4] C.C. Wang, C. Thorpe, S. Thrun, M. Hebert, and H. Durrant-Whyte, Simultaneous localization, mapping 
and moving object tracking, International Journal of Robotics Research, Vol.26, no.9, pp.889-916, 2007. 
[5] M. Tanaka, Reformation of particle filters in simultaneous localization and mapping problems, 
International Journal of Innovative Computing, Information and Control, vol.5, no.1, pp.119-128, 2009. 
[6] J. Shi and C. Tomasi, Good features to track, in Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition, pp.593–600, 1994. 
[7] H. Bay, A. Ess, T. Tuytelaars, L. Van Gool, SURF: speeded up robust features, Computer Vision and 
Image Understanding, vol.110, pp.346-359, 2008. 
[8] T. Lindeberg, Feature detection with automatic scale selection, International Journal of Computer Vision, 
vol.30, no.2, pp79-116, 1998. 
[9] D.G. Lowe, Distinctive image features from scale-invariant keypoints, International Journal of Computer 
Vision, 60, 2, pp. 91-110, 2004. 
[10] S. Hutchinson, G.D. Hager, and P.I. Corke, A tutorial on visual servo control, IEEE Transactions on 
Robotics and Automation, vol.12, no.5, pp.651-670, 1996. 
[11] J.M.M. Montiel, J. Civera, and A.J. Davison, Unified inverse depth parametrization for monocular 
SLAM, in Robotics Science and Systems Conference, 2006. 
本計畫分兩年執行：在第一年，將改良影像特徵偵測與追蹤方法，以及規劃特徵初
始化程序與影像深度演算法，並且整合應用於人形機器人，以便在未知環境中執行同時
定位與建圖的任務。本年度將使用擴張型卡爾曼過濾器進行人形機器人系統狀態的估
測，因此，也將進行擴張型卡爾曼過濾器的共變異數矩陣之推導與特性分析。第二年將
利用機率式 SLAM 的分解概念來設計輪式驅動機器人的同時定位與建圖演算法，將使用
粒子過濾器估測機器人的狀態，以擴張型卡爾曼過濾器估測地標的位置。所建立的機率
式 SLAM 演算法將可以應用於輪式驅動機器人，以便在未知環境中執行任務。最後將人
形與輪式驅動等異質機器人整合，在未知環境中進行 SLAM 訊息傳遞與合作巡航等任
務。本計畫所發展的異質機器人合作巡航系統，將以實地測試方式評估所提相關演算法
的實用性。 
 
二、與會心得與建議 
Innovative Computing, Information and Control (ICIC) International 舉辦的國際智慧型資
訊學會議(International Symposium on Intelligent Informatics, ISII)是相當具水準的智慧型資
訊方面的國際會議。參加此次會議，對於各國不遺餘力在智慧型資訊方面投注的心力與
財力印象深刻。筆者也利用機會參觀主辦會場附近的城市，體會到他們在科技應用領域
的研究相當活躍。北京附近屬於傳統與摩登結合的城市群，當地政府醉心於觀光產業發
展的精神，也同樣表現在科技的發展上。智慧型資訊領域的研討會不多，且主要仍以偏
重理論探討的文章居多。像 ICIC ISII 注重產業實務與學術交流的會議較少，筆者的研究
主要以技術研發與實作為主，相當需要參加此類會議以吸取國際化經驗，在此感謝國科
會對此研究計畫的支持。希望國科會也能持續支持實作研究，並給予國際研討會之論文
發表者，在名額與經費增加補助，以期吸收國際研究經驗與交換研究心得。另外，在此
次會議也吸收了不少新知，也獲得許多研究方向、方法與實驗設計方面的重要觀念，更
認識一些同樣做控制實務的他國研究人員，對往後的研究工作將有相當大的助益。 
 
三、攜回資料名稱及內容 
Electronic Proceedings: International Symposium on Intelligent Informatics, ISII. 
 
 
本計畫分兩年執行：在第一年，將改良影像特徵偵測與追蹤方法，以及規劃特徵初
始化程序與影像深度演算法，並且整合應用於人形機器人，以便在未知環境中執行同時
定位與建圖的任務。本年度將使用擴張型卡爾曼過濾器進行人形機器人系統狀態的估
測，因此，也將進行擴張型卡爾曼過濾器的共變異數矩陣之推導與特性分析。第二年將
利用機率式 SLAM 的分解概念來設計輪式驅動機器人的同時定位與建圖演算法，將使用
粒子過濾器估測機器人的狀態，以擴張型卡爾曼過濾器估測地標的位置。所建立的機率
式 SLAM 演算法將可以應用於輪式驅動機器人，以便在未知環境中執行任務。最後將人
形與輪式驅動等異質機器人整合，在未知環境中進行 SLAM 訊息傳遞與合作巡航等任
務。本計畫所發展的異質機器人合作巡航系統，將以實地測試方式評估所提相關演算法
的實用性。 
 
二、與會心得與建議 
Innovative Computing, Information and Control (ICIC) International 舉辦的國際智慧型資
訊學會議(International Symposium on Intelligent Informatics, ISII)是相當具水準的智慧型資
訊方面的國際會議。參加此次會議，對於各國不遺餘力在智慧型資訊方面投注的心力與
財力印象深刻。筆者也利用機會參觀主辦會場附近的城市，體會到他們在科技應用領域
的研究相當活躍。北京附近屬於傳統與摩登結合的城市群，當地政府醉心於觀光產業發
展的精神，也同樣表現在科技的發展上。智慧型資訊領域的研討會不多，且主要仍以偏
重理論探討的文章居多。像 ICIC ISII 注重產業實務與學術交流的會議較少，筆者的研究
主要以技術研發與實作為主，相當需要參加此類會議以吸取國際化經驗，在此感謝國科
會對此研究計畫的支持。希望國科會也能持續支持實作研究，並給予國際研討會之論文
發表者，在名額與經費增加補助，以期吸收國際研究經驗與交換研究心得。另外，在此
次會議也吸收了不少新知，也獲得許多研究方向、方法與實驗設計方面的重要觀念，更
認識一些同樣做控制實務的他國研究人員，對往後的研究工作將有相當大的助益。 
 
三、攜回資料名稱及內容 
Electronic Proceedings: International Symposium on Intelligent Informatics, ISII. 
 
 
478 Y.-T. WANG, M.-C. LIN, R.-C. J U  AND Y.-W. HUANG 
2.  Feature Initialization for SLAM. During the procedure of SLAM, the robot lo- 
cates its global coordinate by consulting the positions of a group of fixed features or 
beacons in tlie environments. In the research, the MonoSLAM developed by Davison 
et al. [2] is utilized to implement SLAM of a small-size humanoid robot. MonoSLAM 
utilizes an extcndcd Kalrnan filter (EKF) to updatc the estirnation of the robot state and 
the map of beacons in the environments, recursively. An image feature initialization algo- 
rithm is proposed in the paper for SLAM and moving object detection. The procedures of 
the algorithm include detection of image features, selection of good features, calculation 
of image depths, and update of feature locations. The algorithm is described as follows. 
(a) Ca.pture an image and smooth the image by using a Ganssian smoothing operator; 
(b) Utilize a detecting and tracking method to track a set of image features, called C1; 
(c) In the image set C1, select good features which are successively detected and tracked 
in a series of images, and discard those bad features which are lost during tracking; 
(d) Choose c2 features from the remaining features in set C1 and compute the image 
depth of these features, named image set C2. Abandon the features whose image depths 
are not convergent after several iterations of calcula~tion; 
(e) Choose c3 fea,tilres from image set C2 whose image depths are convergent. Compute 
the 3D coordinates for these features, called image set C3. Therefore, these features are 
initialized and ready for SLAM and moving object detection. 
In Step (b), the features are extracted from the image by using the SURF method 
proposed by Bay et al. [7]. SURF is a scale-invariant method for detection of image 
features. It detects region features from an image and obtains the location and the 
descriptor vector of each interest point. The basic concept of a scale-invariant method is 
to detect image features by investigating the determinant of Hessian matrix H [8]. Bay 
et al. [7] iitilizc a box filt,er to process on thc imagc instead of calculating the Hcssian 
matrix, and then the determinant of Hessian matrix is approximated by 
where Dij are the images filtered by the corresponding box filters; lu is a weight constant. 
The interest points or features are extracted by examining tlie extreme value of deter- 
minant of Hessian matrix. Furthermore, the unique properties of the extracted features 
are described by using a 64 dimensional descriptor vector as depicted in Figure 1. An 
example of feature extraction is depicted in Figure 2. Many regional features are detected 
in this ima,ge and illustrated as white circles. 
The extracted features from two successive images can be matched by checking the 
Euclidean distance d between the corresponding descriptor vectors, and using the nearest 
neighbor ratio matching strategy [9]. Therefore, the region features of two images can be 
tracked efficiently. The matching ratio r is defined as the ratio of the smallest distance 
dlst to the second smallest distance dand. If the ratio I- closes to 0.7, we say that these 
two features are matched. 
In Step (d) of image initialization algorithm, the depths of image features are calculated 
using two successive images. Assume that there are 7n image features with position 
vectors, Zi, which can be described as 
where i = 1 , .  . . , m; the vectors h and 1%' denote the ray vectors represented in the world 
frame and camera frame, respectively, as shown in Figure 3; Rr is the rotation matrix of 
the camera fra,me with respect to the world frame. In this research, the position vectors 
of features are determined lrom the monocular image by using the perspective projection 
method [lo]. By the perspective projection method, the coordinates of the feature i in 
Y.-T. WANG, M.-C. LIN, R.-C. JU AND Y.-W. HUANG 
FIGURE 3. b b o t  coordi- 
nate systems 
omj+2 r, Yo ---a 
moving object 
fixed 
* Y;: 
beacons 
* * *  
FIGURE 4. Probabilistic 
distribution of 3D 
FIGURE 5. Initial state 1 = 0 (a) and current state % = tk (b) of fixed 
beacons and moving object 
4. Experimental Results. A small-size humanoid robot is designed and fabricated for 
demonstration, which is equipped with a Windows-based industrial PC. Figure 6 depicts 
the appearance of the designed robot. The control system comprises three subsystems, 
including a vision sensor system, a PC-based control system, and a motor drive system. 
Each subsystem is able to work independently and also in coordination with each other. 
The camera system is the only sensor to provide the robot position information in the 
environments. In this research, we develop the control system by utilizing a Microsoft 
Windows-based industrial PC, Wafer-LUKE533, provided by a local vendor. The fabri- 
cated robot is 51cm in height and 3kg in weight, including two 12V/2.5AH Li batteries. 
One battery provides the power for 18 servo motors and the other provides for the con- 
troller. The motor drive system is composed of a SSC-32 circuit board. 
The experimental set-up is depicted in Figure 7. The humanoid robot implements robot 
SLAM and moving object detection in a corner of the laboratory. Four object locations 
are tested and shown in Figure 8. By using the proposed initialization algorithm, fixed 
features (yellou~ circles) in the environments are selected as beacons for robot SLAM, while 
the moving features (black circles) are classified as a moving object. The displacement 
and orientation of the detected object are listed in Table 1. In the table, the object 
displacement xyz indicates the translational displacement from the origin of the world 
coordinate (set on the camera) to the origin of the coordinate on the box, which is located 
on top-right corner of the box. The object orientation represents the yaw-pitch-roll angles 
Y.-T. WANG, M.-C. LIN, R.-C. JU AND Y.-W. HUANG 
TABLE 1. Displacement and orientation of the moving object 
5. Conclusions. In this research, we developed an image feature initializa,tion algorithm 
for MonoSLAM and moving object detection. The tasks are implemented on a PC- 
based controller for a small-size humanoid robot. The contribution of this research is in 
two aspects: firstly, an improvcd alg~ri t~hm for image fe,zt,~lre init,ializat,ion is developed 
for the EKF-based SLAM by using SURF, a scale-invariant feature extraction method. 
Secondly, the moving object detection is performed by using the same feature initialization 
algorithm, concurrently. Experimental works are performed in this paper and the results 
show that the SLAM with the proposed feature init,ializa.tion algoritlzm has the capability 
to support the humanoid robot simultaneously navigating and detecting moving objects 
in the environments. The developed detector can determine rigid moving objects at  this 
time. In the future, we will extend the method to detecting multiple objects with different 
moving velocities, or non-rigid moving objects like human body with articulated arms and 
legs. 
Acknowledgment. This work was supported in part by the National Science Council 
in Taiwan under grant no. NSC95-2221-E-032-055-MY2. 
REFERENCES 
[I] M. W.  M. G. Dissa.nayalte, P. Newma.n, S. Clark, H. Durrant-Whyte and M. Csorba, A solution to 
the simultaneous localization and map building (SLAM) problem, IEEE Tkansactions on Robotics 
and Automation, vo1.17, no.3, pp.229-241, 2001. 
[2] A. J. Davison, I. D. Reid, N. D. Molton and 0. Stasse, Mono SLAM: Real-time single camera SLAM, 
IEEE Transactions on Pattern Anul?jsis and Machine Intelligence, vo1.29, no.6, pp.1052-1067, 2007. 
[3] M. Montemerlo and S. Thrun, FastSLAM, Springer-Verlag, 2007. 
[4] C. C. Wang, C. Thorpe, S. Thrun, M. Hebert and H. ~ u r r a n t - ~ h y t e ,  Simultaneous localization, 
mapping and moving object tracking, International Journal of Robotics Research, vo1.26, no.9, 
pp.889-916, 2007. 
[5] M. Tanaka, ref or ma ti or^ of particle filtcrs i r ~  sirnultancous localizatio~i and rnapping prol,lcrr~s, Inter- 
national Journal of Innovative Computing, Information and Control, vo1.5, no. 1, pp. 119-128, 2009. 
161 J. Shi and C. Tomasi, Good features to track, Proc. o,f the IEEE Conference on Computer v2Si071 
and Pattern Reco,gnition, pp.593-600, 1994. 
[7] H. Bay, A. Ess, T. Tuytelaars and L. Van Gool, SURF: Speeded up robust features, Computer Vision 
and Image Understandin,q, vol. 110, pp.346-359, 2008. 
[8] T. Lindeberg, Feature detection with automatic sca.le selection, International Jo7jrnal of Computer 
Vision, vo1.30, 110.2, pp79-116, 1998. 
[9] D. G. Lowe, Distinctive image features from scale-invariant keypoints, International Journal of 
Computer Vision, vo1.60, no.2, pp.91-110, 2004. 
[lo] S. Hutchinson, G. D. Ha.ger and P. I. Corlte, A tutorial on visual servo control, IEEE Transactions 
on Robotics and Automation, vo1.12, no.5, pp.651-670, 1996. 
[ll] J. M. M. Montiel, J. Civera and A. J. Davison, Unified inverse depth parametrization for monocular 
SLAM, Robotics Science and Systems Conference, 2006. 
98年度專題研究計畫研究成果彙整表 
計畫主持人：王銀添 計畫編號：98-2221-E-032-012- 
計畫名稱：異質機器人在未知環境中的視覺式合作巡航 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 4 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
