II 
 
目錄 
 
第一年結案報告中文摘要及關鍵詞 …………………………………………… III 
第一年結案報告英文摘要及關鍵詞 …………………………………………… IV 
第一年結案報告內容 …………………………………………………………… V 
第二年結案報告中文摘要及關鍵詞 …………………………………………… XVII 
第二年結案報告英文摘要及關鍵詞 …………………………………………… XVIII 
第二年結案報告內容 …………………………………………………………… XVIV 
第三年結案報告中文摘要及關鍵詞 …………………………………………… XXVII 
第三年結案報告英文摘要及關鍵詞 …………………………………………… XXVIII 
第三年結案報告內容 …………………………………………………………… 1 
參考文獻 ………………………………………………………………………… 11 
計畫成果自評 …………………………………………………………………… 13 
IV 
Abstract 
 
Our system detects persons under hall-way environments.  Object segmentation is first 
applied to separate the foreground objects from the background scene in the video sequence.  To 
handle partial occlusion, detection and identification are accomplished by Generalized Hough 
Transform (GHT).  Our face detection subsystem consists of 6 steps (or modules): System setup, 
camera calibration and camera control, lighting compensation, background subtraction, noise and 
shadow elimination, and real-time contour-based face detection. First we set up our equipment: 
PTZ cameras, WFV cameras, control PCs, network, and storage media onto our monitored 
hall-way scene and make sure that they can communicate and cooperate with each other.  Then 
Camera calibration is applied to eliminate the geometric distortion of all cameras. The third step 
is to normalize the lighting affect cause by different weather and time.  For the fourth step, 
comparing to existing methods, the background model can be constructed no matter whether 
there exist moving foreground objects or not. In addition, the background model is capable of 
handling the illumination changes and intrusive but motionless targets by using the short-term 
approach and long-term approach, respectively, to keep updating the background model.  For the 
fifth step, the noises and shadows are eliminated and the holes are filled in order to reduce the 
false and the missing foreground detection components, respectively. Furthermore, one particular 
function in this module is the automatic digital matting, which can be applied to have visually 
accurate segmentation result for the foreground objects.  For the sixth step, in order to apply 
statistical contour analysis, which requires training samples consist of identical dimensions, we 
first find presentations in same dimensions for all head and facial feature contours in training 
database (refer as DB henceforth) by B-spline based approximation.  Then by using the 
combination mechanism of GHT, facial features are integrated into one single face identifier to 
reduce false alarm of detection. 
 
Keywords: Background Subtraction, Active Contours, Generalized Hough Transform, Head 
Detection, and Video Surveillance.
VI 
the moving foreground objects in image sequences. For recent works of background subtraction, 
a Gaussian model [1] or mixture of Gaussian models (MOGS) [12], [13] are used. But they may 
not get the complete foreground regions and shadow effects may still exist. Especially, the 
approaches of these papers need a period of time without any moving objects in the beginning of 
video sequence to model the background. The work in [14] proposed a predictive watershed 
method to improve the segmentation result of using background subtraction method.  But the 
boundaries of foreground objects do not segment precisely. 
Frontal-view face detection [15], [16], [17], has long been a subject of research interest. For 
example, Sung and Poggio [18] proposed a face detection system comprising distribution-based 
models for face and non-face patterns (or patches) and a multilayer preceptor. Rowley, Baluja and 
Kanade [17] demonstrated a neural network-based face detection system with high detection 
accuracy and a reliable performance. In [15], Liu presented a Bayesian discriminating features 
method for face detection, in which the likelihood density was estimated by considering both the 
projection weights and the residual components in eigenspace. Papageorgiou, Oren and Poggio 
[16] developed a detection technique based on the use of an over-complete wavelet model to 
present an object class. Finally, Viola and Jones [19] presented a real-time frontal-view face 
detection system featuring a cascade of boosting classifiers based on an over-complete set of 
Haar-like features.  
To solve above existing and expected problems and to optimize detection performance, we 
are going to develop a real-time system, which can automatically detect the uncooperative subject 
and his/her face with a closer view at a distance up to 50 meters in indoor lighting environment.  
Subject’s location will be estimated through two computer (notebook) control cameras, which 
have overlapped wide field of views (WFVs). 
 
3. System Setup and a Real-Time Contour-Based Face Detection Subsystem 
 
3.1 Lighting Compensation 
 
Variation in illumination conditions caused by weather, time of day, etc., makes the task 
difficult when building video surveillance systems of real world scenes.  Especially, cast 
shadows produce troublesome effects, typically for object tracking from a fixed viewpoint, since 
it yields appearance variations of objects depending on whether they are inside or outside the 
shadow.  In this work, we will handle such appearance variations by removing shadows in the 
image sequence.  This can be considered as a preprocessing stage which leads to robust video 
surveillance. 
We will disable the AGC function at the camera, and use image processing technique and 
software program to control the focus, iris and shutter speed of the camera in order to have the 
best image or video quality.  Our goal is to ―normalize‖ the input image sequence in terms of the 
distribution of incident lighting to remove illumination effects including shadow effects by using 
),,(/),,(),,( tyxLyyxItyxN   
 
VIII 
 
 
 
 
 
 
 
 
 
 
 
 
 
image over the M consecutive images, the background model can be constructed for each video 
sequence, as shown in Figure 2.c.  
The reason why the median value is chosen instead of the mean value for the initial process 
is because median value is not sensitive to noises, such as the appearances of the moving 
foreground objects or sudden illumination change, as shown in Figure 2.b.  In addition, our 
assumption is adjustable. That is, if the background scene is stable, then the appearance 
frequency of the pixel belonging to the background region can be higher. On the contrary, if the 
background scene is unstable, such as affected by slowly moving foreground objects or unstable 
lightings, then the appearance frequency of the pixel needs to be lower. 
 
3.2.2   Background Updating 
The initial background model cannot be expected to work for long periods of time.  
Without updating the background model, it will cause the false extraction (or detection) results by 
illumination changes, such that the sun is blocked by clouds and then appears again, or physical 
changes, such that an object is deposited or a car is parked.  To overcome above problems, two 
different approaches are applied to update the background model: the short-term updating 
approach and the long-term updating approach. 
For the short-term updating approach, this approach will quickly update the background 
pixels in order to maintain the sensibility of extracting or detecting the moving foreground pixels. 
The intensity of the background pixel μt(x, y) at location (x, y) and time t is updated by following 
equation: 








 ),(                                            ),(
),(        ),(*),()1(
),(
1
1
Foregroundyxifyx
Backgroundyxifyxyx
yx
tt
ttt
t


  
 
where ρ is the updating rate. If the ρ value is higher, then the background pixel is easier to adapt 
to the new environment. For example, it is necessary to have the higher ρ value when the  
Figure 2. Background model construction.  (a) The first M (M=60) consecutive images in the 
video sequence. (b) The intensity distribution of M consecutive images for a pixel 
located at (x,y). (c) The background image (or model) is built based on the average 
(x,y) of v
z
(x,y) located at location (x,y). 
X 
 
 
 
 
 
 
 
 
 
 
3.3 Noise and Shadow Region Elimination 
 
This module will eliminate the shadow and noise from the output of previous step.  This 
step refines the background subtraction results. We will apply the morphological operations 
including the erosion, dilation and connected-component labeling (Connected Component 
Labeling) [22] operations to eliminate those false detection foreground pixels excepting those 
pixels belonging to the shadows and to fill the holes inside the foreground region, as shown in 
Figure 3.c. The shadow region is easily detected as the foreground region and it always follows 
the movement of the foreground object, so it is a challenging work to discriminate between the 
foreground object region and the background shadow region. Here, this work can successfully 
reduce the shadow effects by using the method of the vector model proposed in [23].  That is, 
each detected foreground pixel P is the center pixel of one 3×3-pixel window, and there are 8 
pixels in its 8-connected neighboring area. Those 9 pixels located in the two-dimensional (2D) 
window are then regarded as one intensity column vector.  Because shadow is usually caused by 
illumination factor, so if the pixel locates in the shadow region, then the intensity vectors of this 
pixel exist a linear dependent relation between the shadow region and the corresponding 
background region. Conversely, the linear dependent relation does not exist for any foreground 
pixels. By using this method, the shadow region is successfully removed, as shown in Figure. 4.c 
 
3.4 Real-Time Contour-Based Face Detection 
 
After we got the foreground object information, we will retrieve precise face location.  
Only contour information is used to achieve the system efficiency.  However, not only head 
contour information but also facial feature contours are utilized to detect human faces. 
We proposed an automatic contour extraction process for those databases shot under simple 
background. The process of extracting head and facial feature contour for the GHT module is 
illustrated in Figure 5. Each facial image is manually labeled with three canonical points, i.e. a 
single point at the inner corner of each eye (a rigid point), and a point at the tip of the nose. The 
image is normalized by using rotation and scaling by locating the inner corners of the two eyes at 
predefined positions. The location of the tip of the nose varies widely between training samples, 
and hence the third point is obtained from the average location of the nose tip of all normalized 
facial images. Affine transformation is then used to normalize all the cropped facial images into  
(a) (b) (c) 
Figure 4. Shadow region elimination.  (a) The original image.  (b) The result of the 
background subtraction with shadow effects.  (c) The result after eliminating the 
shadow region. 
XII 
 
 
thickness of 1~3 pixels. In order to remove the redundant point groups and to ensure that the 
thickness of the head contour consists of a single pixel only, the Dijkstra Shortest Path Algorithm 
[20] is employed with the assumption that each of the 8-connected edge points has a weight of 1. 
However, the number of contour points is different and thus further analysis which required data 
vector to be the same dimension are impossible to apply. We deploy data fitting by B-Spline 
which using the least square approximation for curve representation. This approach is widely 
used in computer vision, and especially, computer graphics [21], [22], [23]. Such an 
approximation problem might involve N control point c = [c1,…,cN]
T
 through a set of data points 
d = [d1,…,dM]
T
 at given parameter values u1,…,um and leads to an over determined system of a 
linear equations: 
 


































MN
MNM
N
N
d
d
c
c
uBuB
uBuB
uBuB





11
1
221
111
)()(
)()(
)()(
 
where Bi(u) is the i-th B-Spline basis function. Here an opened B-Spline curve of degree 3 
defined by 13 control points and a uniform knot vector is used to achieve this purpose. Some 
Edge image 
Binary 
image 
Head contour 
Normalized 41* 
41-pixel image 
Image grayvalue 
histogram 
Erosion Dilation 
Edge detection 
AND 
Contour mask 
Dijkstra’s 
Shortest 
Path 
Head contour + other edges 
Below 
chin is 
cut 
Figure 7.  Head-contour extraction process for each 
normalized facial image. 
Beginning point Ending point 
Figure 7.  Head-contour extraction process for each normalized facial image. 
 
14 
 Figure 9. A cluttered-background sequence. 
 
 XVI 
[14] Chien, S.-Y., Ma S.-Y., Chen, L.-G.: Predictive Watershed: A Fast Watershed Algorithm for 
Video Segmentation. IEEE Transactions on Circuits and Systems for Video Technology, Vol. 
13, No. 5, May (2003) 453-461. 
[15] C. Liu, ―A Bayesian Discriminating Features Method for Face Detection,‖ IEEE Trans. 
Pattern Analysis and Machine Intelligence, vol. 25, no. 6, pp. 725-740, 2003. 
[16] C.P. Papageorgiou, M. Oren, and T. Poggio, ―A General Framework for Object Detection,‖ 
in Proc. IEEE Int. Conf. Computer Vision, pp. 555-562, 1998. 
[17] H.A. Rowley, S. Baluja, and T. Kanade, ―Neural Network-Based Face Detection,‖ IEEE 
Trans. Pattern Analysis and Machine Intelligence, vol. 20, no.1, pp. 22–38, 1998. 
[18] K.K. Sung and T. Poggio, ―Example-Based Learning for View-Based Human Face 
Detection,‖ IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 20, no. 1, pp. 39-51, 
1998. 
[19] P. Viola and M. Jones, ―Rapid Object Detection Using a Boosted Cascade of Simple 
Features,‖ in Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 
511-518, 2001. 
[20] R.P. Grimaldi, Discrete and Combinatorial Mathematics, 4th edition, Addison Wesley 
Longman, Inc., 1999. 
[21] A. A. Goshtasby, ―Grouping and Parameterized Irregularly Spaced Points For Curve 
Fitting,‖ ACM Trans. On Graphics, 19: 185-203, 2000. 
[22] YingLiang MA, F. Pollick, and W.T. Hewitt, ―Using B-Spline for Hand Recognition,‖ ICPR, 
pp.274– 277, Aug. 2004. 
[23] H. Pottmann, S. Leopoldseder, M.Hofer, ―Approximation with active B-Spline Curves and 
Surfaces,‖ IEEE Proceeding of Pacific Graphics, 2002, pp. 8-25. 
[24] M. Isard, and A. Blake, ―CONDENSATION—Conditional Density Propagation for Visual 
Tracking,‖ IJCV, 11(2), pp. 127-145, 1993. 
[25] D. Maio and D. Maltoni, ―Real-Time Face Location On Gray-Scale static Images,‖ Pattern 
Recognition 33(2000) 1525-1539. 
 XVIII 
Abstract 
Tracking moving objects is a key issue in computer vision.  It is important in a wide variety 
of applications, like surveillance, human motion analysis, traffic monitoring, and man machine 
interfaces.  The appearance of face which is tracked may vary due to the changes in 
illuminations, the orientation and position in 3D space relative to the camera, and the changes in 
its motion.  Thus, reliable and efficiency tracking in complex environment is still a difficult 
problem.  
Based on a combination of particle filter and mean shift method, an algorithm for face 
tracking based skin color and motion cue is developed.  This method uses YCrCb space for skin 
color modeling since in this space the range of the skin color and the non-skin color can be 
separated easily.  Also, to distinguish a moving face from the skin-like and complex background, 
we add another motion cue for determining the moving region and use an additional color 
transformation for enhancing the skin region.  In addition, this method not only can solve the 
problems above but the problem of the dynamic background by using color and motion cue 
together.  
Particle filter and mean shift, two key methods in this system are two successful approaches 
taken in the pursuit of robust tracking.  Using particle filter method, this system can run in 
real-time since instead of searching face in the entire image, particle filtering samples the face 
image by generating particles as the face candidates.  Also, the mean shift method can result in 
low computational cost for one of its properties is grouping all face candidates together to move 
to the local maxima actively.  The idea of combing them together is not only the advantage of 
each above, but the major reason is using mean shift method can solve the degeneracy problem of 
particle filtering, and at the same time using particle filter method can avoid getting stuck at a 
local minimum.   
Finally, the face appearance may vary due to changes of face orientation, translation, scaling, 
and rotational variations.  To deal with variations above, this system adopts a dynamic face 
region for tracking.  Hence, this proposed system can be expected to track face reliably and 
efficiently in the real environment.  
 
Keywords: Face Tracking, Skin Color Modeling, Particle Filter, Mean Shift, Monte Carlo 
method (SMC). 
  
XX 
 
system, we applied re-sampling and scaling approaches after mean shift. 
2. PROPAGATION BASED ON BAYESIAN FILTERING 
Particle filter, also known as Sequential Monte Carlo method (SMC) [4], is a Bayesian tracking 
filter that is not constrained to the assumptions of Gaussian statistics and linearity. In Bayesian 
terms, the posterior distribution of the state can be expressed as: 
 
1( | ) ( | ) ( | )t t t t t tp x Z kp z x p x Z                     (1) 
where k is a normalization constant, xt represents the state, zt represents the current observation of 
the system at time t, and Zt-1 represents all the information collected until time t.  
The main idea of particle filter is to approximate the probability distribution by a weighted 
sample set. It provides an estimation of the posterior in Eq. (1) in three steps: sampling, 
measurement and re-sampling, as illustrated in Figure 1. The sampling step consists of taking 
 (a) Initial system state 
 (c) Measurement  (b) Sampling 
 Particle Filtering 
  Update state 
 (d) Mean Shift 
 Mean Shift 
Figure 2. Workflow diagram of the proposed system.  
 
 
 
 
 
tT 
Selective re-sampling 
  
XXII 
probability pixel value (probability 1.0). In the color probability distribution image, the larger the 
pixel value is, the color of that pixel is more similar to skin color. 
We further apply the transformation to the color probability distribution image by   
( , ) ' ( , )p x y p x y         (4) 
where γ is an exponential element with positive value larger than 1. This transformation will map 
wild range of dark input values into a narrower range of output values, with the opposite being 
true for higher values of input levels. So after Eq. (1), we can increase the contrast between skin 
region and non-skin region to achieve the purpose of enhancing the real skin pixels. Figure 3. (a). 
illustrates an image with complex environment. As can be seen in (c), after the transformation, 
the skin region will be obviously separated from the background compared with (b). The skin 
color model is adopted frame-by-frame to cope with the possible changes of skin color due to 
variable illuminations.  
While single color cue has little efficiency to distinguish target object from background, we 
further adopt motion cue to overcome the limitation of the individual cue. We differentiate the 
current frame with the previous frame to generate the difference image using the motion analysis 
method in [5]. The method is to compute the absolute value of the differences in each pixel. 
When the accumulated difference is above a predetermined threshold, the pixel is assigned to the 
moving region. For each sample, the likelihood is estimated by color and motion cues of the 
rectangle window. The color cue is the zeroth moment in the rectangle in the color probability 
distribution image: 
(b) (c) 
(a) 
Figure 3. The effect of applying transformation in color probability distribution image. (a) Frame 17 in 
a test video sequence. (b) Surface plot of original color probability distribution image. (c) 
Surface plot of transformed color probability distribution image. As illustrated above, the 
original distribution is too ―noisy‖ compared with the transformed distribution. 
  
XXIV 
                                             
( ) ( )
1
[ ]
N
n n
t t t
n
E S w s

        (11) 
where E[St] means the mean state of the sample set S at time t. 
3.2 Mean shift under different scales 
To overcome the degeneracy problem of particle filtering, we apply the mean shift to all the 
samples after they are measured. The mean shift algorithm is described in [2] and it can shift the 
samples toward a close local maximum. The algorithm is as follows. 
 
Given the sample ( )ns  with position 0 0 0( , )p x y : 
1. Computer the observation in the rectangle window. 
00 (1 ) ( , ) ( , )c m
x y
M I x y I x y             (12) 
10 [(1 ) ( , ) ( , )]c m
x y
M x I x y I x y                  (13) 
01 [(1 ) ( , ) ( , )]c m
x y
M y I x y I x y             (14) 
where 00M  is the zeroth moment, 10M  is the first moment for x and 01M  is the first 
moment for y. 
2. Then the new mean position 1 1 1( , )p x y  will be 
10
1
00
M
x
M
                                 (15) 
00
01
1
M
M
y 
                                (16) 
3. If  1 0p p   , stop. Otherwise, set 0 1P P  and go to Step 1. 
While the iteration stops, the sample will move to the close local maximum. The threshold 
ε should be set properly, so there won’t be too many repeated points. 
 
In order to increase the efficiency and accuracy of tracking, we change the window size of 
the rectangle dynamically. The window size could be adjusted by  
2*z M             (17)   
where M is the zeroth moment of the rectangle window of  the mean state in the color 
probability distribution image (the expression is the same with Eq. (4)). To convert the resulting 
2D region to a 1D length, we need to take the square root. In practice, for tracking faces, we set  
  
XXVI 
5. REFERENCES 
[1] A.S. Arulampalam, S. Maskell, N.J. Gordon, and T. Clapp, ―A tutorial on particle filters for 
on-line nonlinear/non-Gaussian Bayesian tracking,‖ IEEE Transactions on Signal Processing, 
Vol.50, No.2, pp. 174, 2002.  
[2] G.R. Bradski, ―Computer vision face tracking for use in a perceptual user interface,‖ Intel 
Technology Journal, Vol.2, No.2, pp. 12-21, 1998.   
[3] J.C. Chen and J.J. Lien, ‖A View-Based Statistical Approach for Multi-View Face Detection 
and Pose Estimation,‖  Master thesis, NCKU, 2005. 
[4] A. Doucet, N. de Freitas, and N. Gordon, ―Sequential Monte Carlo Methods in Practice,‖ 
Springer, Vol.7, No.3, pp.301-319, 2001. 
[5] H. P. Graf, E. Cosatto, D. Gibbon, M. Kocheisen and E. Petajan, "Multi-Modal System for 
Locating Heads and Faces," International Conference on Automatic Face and Gesture 
Recognition, pp. 88-93, 1996.   
[6] M. Isard and A. Blake, ―Condensation-Conditional Density Propagation for Visual Tracking,‖ 
Int'l J. Computer Vision, Vol. 29, No.1, pp. 5–28, 1998.  
[7] M. Isard and A. Blake, ―ICONDENSATION: Unifying low-level and high-level tracking in a 
stochastic framework,‖ ECCV, Vol.1, No.1, pp. 893-908, 1998. 
[8] P. Perez, C. Hue, J. Vermaak and M. Gangnet, ―Color-Based Probabilistic Tracking,‖ ECCV, 
Vol.1, pp. 661-675, 2002.
Figure 5. Results of tracking face containing occlusion. 
 
(d) (c) 
(b) (a) 
Figure 6. Results of tracking the non-face object containing rotation. 
 
  
XXVIII 
Abstract 
 
A novel kernel discriminant transformation (KDT) algorithm based on the concept of 
canonical differences is presented for automatic face recognition applications.  For each 
individual, the face recognition system compiles a multi-view facial image set comprising images 
with different facial expressions, poses and illumination conditions.  Since the multi-view facial 
images are non-linearly distributed, each image set is mapped into a high-dimensional feature 
space using a nonlinear mapping function.  The corresponding linear subspace, i.e. the kernel 
subspace, is then constructed via a process of kernel principal component analysis (KPCA). The 
similarity of two kernel subspaces is assessed by evaluating the canonical difference between 
them based on the angle between their respective canonical vectors.  Utilizing the kernel Fisher 
discriminant (KFD), a KDT algorithm is derived to establish the correlation between kernel 
subspaces based on the ratio of the canonical differences of the between-classes to those of the 
within-classes. The experimental results demonstrate that the proposed classification system 
outperforms existing subspace comparison schemes and has a promising potential for use in 
automatic face recognition applications. 
 
Keywords: Face recognition, canonical angles, kernel method, kernel Fisher discriminant (KFD), 
kernel discriminant transformation (KDT), kernel PCA.
  
2 
kernel subspaces, in a high-dimensional feature space. Yang [18] showed that kernel subspaces 
provides an efficient representation of non-linear distributed data for object recognition purposes. 
Accordingly, Fukui et al. [5] developed a kernel version of CMSM, designated as KCMSM, 
designed to carry out 3D object recognition by matching kernel subspaces. However, although 
the authors reported that KCMSM provided an efficient means of classifying non-linearly 
distributed data, the problem of the reliance of the classification performance upon the choice of 
an appropriate constraint subspace dimensionality was not resolved.  
In an attempt to address the problems outlined above, this study proposes a novel scheme for 
comparing kernel subspaces using a kernel discriminant transformation (KDT) algorithm. The 
feasibility of the proposed approach is explored in the context of an automatic face recognition 
system. To increase the volume of information available for the recognition process, a multi-view 
facial image set is created for each individual showing the face with a range of facial expressions, 
poses and illumination conditions. To make the non-linearly distributed facial images more easily 
separable, each image is mapped into a high-dimensional feature space using a non-linear 
mapping function. The KPCA process is then applied to each mapped image set to generate the 
corresponding kernel subspace. To render the kernel subspaces more robust to variances in facial 
images, canonical vectors [7] are derived for each pair of kernel subspaces. The subspace 
spanned by these canonical vectors is defined as the canonical subspace. The difference between 
the vectors of different canonical subspaces (defined as the canonical difference) is used as a 
similarity measure to evaluate the relative closeness (i.e. similarity) of different kernel subspaces. 
Finally, exploiting the proven classification ability of utilizing the kernel Fisher discriminant 
(KFD) [2], [11], a kernel discriminant transformation (KDT) algorithm is developed for 
establishing the correlation between kernel subspaces. In the training process, KDT algorithm is 
proceeded to find a kernel transformation matrix by maximizing the ratio of the canonical 
differences of the between-classes to those of the within-classes. Then in the testing process, the 
kernel transformation matrix is applied to establish the inter-correlation between kernel 
subspaces. 
 
 
 
 
 
 
 
Fig. 1. (a) and (b) show the first five eigenvectors (or principal components) and corresponding 
canonical vectors, respectively, of a facial image set. Note that the first and second rows 
correspond to the same individual. Comparing the images in (a) and (b), it is observed that 
each pair of canonical vectors, i.e. each column in (b), contains more common factors 
(a) 
(c) 
(d) 
(b) 
(e) 
u 
v Θ 
d=u-v 
d: Canonical 
difference  
  
4 
    
d
r rr
vujiiffCanonicalD
1
2
,   
                           jiTji CCCCtrace  . (3) 
Clearly, the closer the two subspaces are to one another, the smaller the value given by 
 jiiffCanonicalD ,  in its summation of the diagonal terms. As shown in Figs. 1.(c) and (d), the 
canonical differences between two facial images contain more discriminative information than 
the eigenvector differences and are less affected by variances in the facial images. 
 
6. Kernel Discriminant Transformation (KDT) Using Canonical Differences 
 
This section commences by discussing the use of KPCA to generate kernel subspaces and 
then applies the canonical difference concept proposed in Section 2.2 to develop a kernel 
discriminant transformation (KDT) algorithm designed to determine the correlation between two 
subspaces. Finally, a kernel Fisher discriminant (KFD) scheme is used to provide a solution to the 
proposed KDT algorithm. Note that the KDT algorithm is generated during the training process 
and then applied during the testing process. 
 
3.1 Kernel Subspace Generation 
 
To generate kernel subspaces, each image set in the input space is mapped into a 
high-dimensional feature space  using the following nonlinear mapping function:  
   )X(,),(XX,,X: 11 mm    . (4) 
In practice, the dimensionality of , which is defined as h, can be huge, or even infinite. 
Thus, performing calculations in  is highly complex and computationally expensive. This 
problem can be resolved by applying a ―kernel trick‖, in which the dot products    yx    are 
replaced by a kernel function k(x,y) which allows the dot products to be computed without 
actually mapping the image sets. Generally, k(x,y) is specified as the Gaussian kernel function, 
i.e.  







 

2
2
yx
exp)y,x(

k . (5) 
  Let m image sets be denoted as  mX,,X1  , where the i-th set, i.e. ],,[X 1 ini xx  , contains ni 
images in its columns. Note that the images of each facial image set belong to the same class. The 
―kernel trick‖ can then be applied to compute the kernel matrix Kij of image sets i and j 
( mji ,...,1,  ). Matrix Kij is an ni × nj matrix, in which each element has the form 
  
6 
To obtain the canonical difference between two subspaces, it is first necessary to calculate the d × 
d projection matrices ij  and ji , i.e.  
T
jiijj
T
i QQ  . (10) 
The canonical difference between two transformed kernel subspaces i and j can then be computed 
from  
    jijiji
T
jijiji QQQQtracejiiffCanonicalD ),(   
                   
   TPPPPT TjijijijijijiTtrace  ,  
(11) 
where ijiij R 
1
 and jijji R 
1
. 
The transformation matrix T is derived by maximizing the ratio of the canonical differences of 
the between-classes to those of the within-classes. This problem can be formulated by optimizing 
the Fisher discriminant, i.e.  
               
 
  
 



kiiffCanonicalD
iiffCanonicalD
i
i
Wk
m
i
B
m
i
,
,
maxargT
1
1
T

 
 
 
   
 
 TT
TT
maxarg
T
W
T
B
T
Strace
Strace
 , 
(12) 
where    
m
i B
T
iiiiiiB
i
S
1
)PP)(PP(   denotes the between-scatter given that 
Bi is the set of class labels of the between classes and 
   
m
i Wk
T
kikikikikikiW
i
S
1
)PP)(PP( is the within-scatter given that Wi is the set of 
class labels of the within classes. 
 
3.3 Kernel Discriminant Transformation Optimization 
 
In this section, we describe an optimization process for solving the Fisher’s discriminant 
given by Eq. (12). First, the number of all the training images is assumed to be M, i.e. 
 
m
i i
nM
1
. Using the theory of reproducing kernels as shown in Eq. (7), vectors   T1
w
qq
t  
can be represented as the span of all mapped training images in the form 
  
8 
eigenvectors of VUμ
1  are computed as the solutions of α . 
  
10 
Fig. 3. Workflow of proposed face recognition system. 
  
12 
 
. 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 4. (a) illustrates the convergence of the KDT solution procedure for different 
experimental initializations. It can be seen that as the number of iteration increases, the Jacobian 
value given in Eq. (18) converges to the same point irrespective of the initialization conditions. 
However, it is observed that the KMSM scheme achieves a better performance than the proposed 
method for random initializations. Fig. 4.(b) and (c) demonstrate the improvement obtained in the 
similarity matrix following 10 iterations. Fig.5.(a) illustrates the relationship between the 
identification rate and the dimensionality, w, and demonstrates that the identification rate is 
degraded if w is not assigned a sufficiently large value. From inspection, it is determined that w = 
2,200 represents an appropriate value. Adopting this value of w, Fig. 5(b) compares the 
identification rate of the KDT scheme with that of the MSM, CMSM, KMSM and KCMSM 
methods, respectively. Note that the data represent the results obtained using eight different 
training/testing combinations. Overall, the results show that KDT consistently outperforms the 
other classification methods 
Fig. 5. (a) Relationship between dimensionality w and identification rate. (b) Comparison of identification 
rate of KDT and various subspace methods for eight training/testing combinations. 
(b) (a) 
Fig. 4. (a) Convergence of Jacobian value α)(J  under different initialization conditions. (b) and (c) 
similarity matrices following 1
st
 and 10
th
 iterations, respectively. 
(a) (b) (c) 
  
14 
10. Satoh, S.: Comparative Evaluation of Face Sequence Matching for Content-Based Video 
Access. IEEE Conference on Automatic Face and Gesture Recognition (FG), (2000) 163-168 
11. Schölkopf, B., Smola, A., and Müller, K.-R.: Nonlinear Component Analysis as A Kernel 
Eigenvalue Problem. Neural Computation, (1998) 10(5): 1299-1319  
12. Shakhnarovich, G., Fisher, J.W. and Darrel, T.: Face Recognition from Long-Term 
Observations. European Conference on Computer Vision, (2000) 851-868  
13. Shakhnarovich, G., and Moghaddam, B.: Face Recognition in Subspaces. Handbook of Face 
Recognition, 2004. 
14. Turk, M., and Pentland, A.: Face Recognition Using Eigenfaces. CVPR, (1993) 453-458 
15. Viola, P. and Jones, M.: Robust Real-Time Face Detection. International Journal of 
Computer Vision, (2004) 57(2): 137-154   
16. Wolf, L. and Shashua, A.: Kernel Principal Angles for Classification Machines with 
Applications to Image Sequence Interpretation. CVPR, (2003) 635-642   
17. Yamaguchi, O., Fukui, K., and Maeda, K.: Face Recognition Using Temporal Image 
Sequence. FG, (1998) (10):318-323  
18. Yang, M.-H.: Kernel Eigenfaces vs. Kernel Fisherfaces: Face Recognition Using Kernel 
Methods. FG, (2002) 215-220  
19. http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                         ２００９年    １０月   22日 
報告人姓名  
張峻豪 
 
就讀校院 
（科系所） 
                     □博士班研究生 
國立成功大學資工所 
                     碩士班研究生 
     時間 
會議 
     地點 
2009/9/22~2009/9/29 
 
中國大陸-西安市 
本會核定 
補助文號 
 
NSC 96-2628-E-006-201-MY3 
會議 
名稱 
 (中文)亞洲電腦視覺會議 2009 
 (英文)The Asian Conference on Computer Vision, 2009 
發表 
論文 
題目 
 無 （幫忙同實驗室同學的論文發表） 
報告內容應包括下列各項： 
一、參加會議經過 
  第一天(9/22)，下午 3點從高雄小港機場搭乘港龍航空出發，經由香港轉機東方航空抵達西
安時約晚上 9點。 
  第二天(9/23)，會議註冊，發送論文資料、紀念品等，這天沒有會議活動。 
  第三天(9/24)，會議開始，Workshop的演講，會議有邀請許多來自不同國家的專家講解關於
現在的技術跟一些影片 Demo，最令人印象深刻的是一個來自日本的技術團隊，他們將使用者的一
些特徵擷取下來(例如多角度的臉部資訊、聲音、走路姿勢等)，然後將這些資訊套用到一個設定
好的劇本裡面，接下來使用者就可以看到自己的成為了動畫中的一員，聽著相似於自己的聲音、
相似於自己的走路姿勢，這樣能令使用者更有參與感，例如將使用者聲音與電腦聲音做某種程度
上的合成使其能更加符合動畫中的情境。是個滿有趣的主題，能令使用者有身臨其境的感覺。 
  第四天(9/25)，投稿者的演講及展示，開始接連三天的 presentation，早上 8:30~下午 16:30
為 oral presentation 的部份，下午 16:50~18:30 為 poster 部份；並且另設有 Demo room 讓一
些與會者展出她們的實作成品，其中有類似 Wii的遊樂器、有手寫字跡辨認跟三 D斷層掃描重建
等許多創意；此外在會場還有廠商帶來他們的最新產品，例如全方位攝影機等等。 
  第五天(9/26)，25、26、27三天活動內容皆是挑選與本身研究有關的主題去聽演講或是下午
的 poster解說。 
  第六天(9/27)，會議正式結束。 
  第七天(9/28)，自由活動。 
  第八天(9/29)，由於適逢大陸國慶假期的班機問題，延至 9/29回台。 
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                         ２００９年    １０月  ２２日 
報告人姓名  
陳郁麒 
 
就讀校院 
（科系所） 
                     □博士班研究生 
國立成功大學資工所 
                     碩士班研究生 
     時間 
會議 
     地點 
2009/9/23~2009/9/27 
 
中國大陸-西安市 
本會核定 
補助文號 
 
NSC-98-2922-I-006-178 
會議 
名稱 
 (中文)亞洲電腦視覺會議 2009 
 (英文)The Asian Conference on Computer Vision, 2009 
發表 
論文 
題目 
無 （幫忙同實驗室同學的論文發表） 
報告內容應包括下列各項： 
一、參加會議經過 
 
本次 2009ACCV 研討會是在大陸西安古都新世界大飯店舉行，為期五天，會議目的主要為交
流這兩年影像領域相關的研究與技術應用，總共有千餘位國內外研究學者及學生參加，會議主要
以 oral presentation及 poster的方式進行。 
22日學生於高雄小港機場搭乘港龍航空班機直達香港，接著轉乘東方航空抵達西安，於晚上
十一點左右入住下榻飯店。 
  會議第一天主要為報到之用，註冊時領取會議資料及贈送背包一只。 
  第二天首先由 Workshop開場演講，日本學者介紹將 3D動畫裡的人物面孔代換為使用者的相
關技術，先使用各種設備擷取使用者本身的相關資訊，包括臉部角度、聲音、行走姿勢…等，接
著電腦藉由這些資料開始製作 3D動畫。下午的演講則是著重於 3D場景重建的相關技術。 
  第三天的 oral presentation 總共有十一場，四十五篇 poster 展出，參加了幾場與自己領
域相關的主題，並拍照蒐集其相關領域的海報資料，直到下午六點半，將 poster 全數參觀完畢。  
  第四天的十二場 oral presentation，四十二篇 poster 展出中，學生參加了關於 shadow 
removal、image enhancement以及 detection of vehicle logos 方面的主題。其中一篇 poster
主題為 Detection of Vehicle Manufacture Logos Using Contextual Information 是一篇藉由
車牌加上車燈位置進而定位出車輛標誌的技術，這使得除了以車子的顏色、車型、車牌之外，又
多了一向追蹤車輛的相關資訊。 
  會議最後一天總共有十二場 oral presentation，四十七篇 poster展出，下午六點半，ACCV
會議活動正式在這天結束。而學生由於機位的問題，無奈延至 9/29回台。 
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                           ２００９年    １０月    ６日 
報告人姓名  
方競賢 
 
就讀校院 
（科系所） 
                     □博士班研究生 
國立成功大學資工所 
                     碩士班研究生 
     時間 
會議 
     地點 
2009/9/23~2009/9/27 
 
中國大陸-西安市 
本會核定 
補助文號 
 
NSC-98-2922-I-006-178 
會議 
名稱 
 (中文)亞洲電腦視覺會議 2009 
 (英文)The Asian Conference on Computer Vision, 2009 
發表 
論文 
題目 
 (中文) 時空概念上的人體行為辨識 
 (英文) Human Action Recognition using Spatio-Temporal Classification 
一、參加會議經過 
 
    在大陸西安舉行的電腦視覺會議(ACCV),從 9/22-9/27,期間有各種有關電腦視覺的相關論文
發表以及呈現,不管是人機互動.影像處理.圖片切割等等,都有很多令人耳目一新的呈現,期間聽
了很多oral section,見識到來自很多國家的人以及他們做的研究到了甚麼樣的地步,國際性的會
議真的是場面浩大,有來自各國的教授以及學生,發表著他們的成果以及發現創新,當然也有去參
觀 poster section,一邊看還可以跟作者直接問答互動,認識了來自美國的朋友,還有大陸的研究
生,日本跟韓國人的論文也頗為有趣,而且國際性的會議場面果然就是不一樣,而在 24號的主題報
告中,我去聽了一個很有趣的演講,他在發表如何製作一部由使用者當主角的電影,也就是說他可
以拍下參予者的影像,錄下其聲音的特色,然後紀錄其走路的特徵,經由這些系統的整合,把所有
參予者 assign適當的電影角色,然後電影中那個角色就會擁有相似造型的五官,音色相似的聲音,
以及動作類似的行為模式,衝擊到的是影像的應用與現實間可以那麼靠近有趣,而日本此發表也
在他們當地有產品發表的影片,我個人覺得這個應用非常的有趣也很實際。之後 25,26,27號我就
是選了我感興趣的主題去聽,然後去看了一些產品的 demo,其中有看到一個信封處理系統,他可以
很快速的掃瞄過信封表面,紀錄住址姓名等資訊,雖然說其對信封上的字(電腦打字)有比較死的
規定,不過可以看出影像處理跟現實之間是息息相關的,可見有關影像的電腦應用層面日趨重要。 
 
 
二、與會心得 
    心得的話,因為我有參加過比較小型的 conference,這次參加這種國際型的大型會議是第一
次,果然一進去就是來自各國的人,美國印度日本韓國大陸英國法國台灣等等,就感受到那種衝
擊,很多來自各國的有名教授,還有很多有實力的學生,還有看到一些研究團隊,很重要的是國際
會議,就需要國際共通語言,英文的重要性就很有份量的在我眼前砸下,在台灣從國中就學英文,
一路到研究所,也學了很多年,學的都是比較中規的英文,發音標準,但是去完會議後,發現現實上
口音百百種,那多元化簡直聽得我東缺西漏,還以為自己英文程度已經算可以,才發現天差地遠,
語言能力不到強者的境界,那也沒太多用處,然後就是看了一些 oral section的演講者,專業程度
以及他對此領域的熟悉程度,都讓我知道自己學到的是那麼的淺薄而不夠,而研究的態度真的需
要再改進,看到有些演講者在講台上揮灑自如,下面再多問題都難不了他的時候,我就覺得了不
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                         ２００９年    １０月    ６日 
報告人姓名  
謝松憲 
 
就讀校院 
（科系所） 
                     □博士班研究生 
國立成功大學資工所 
                     碩士班研究生 
     時間 
會議 
     地點 
2009/9/23~2009/9/27 
 
中國大陸-西安市 
本會核定 
補助文號 
 
NSC-98-2922-I-006-178 
會議 
名稱 
 (中文)亞洲電腦視覺會議 2009 
 (英文)The Asian Conference on Computer Vision, 2009 
發表 
論文 
題目 
 (中文)使用單張影像及權重圖分離 Reflectance 和 Shading 
 (英文)Weighted Map for Reflectance and Shading Separation Using a 
Single Image 
報告內容應包括下列各項： 
一、參加會議經過 
  我們在 9/22 從高雄小港機場，搭港龍航班到香港，再轉搭東方航空到西安，抵達住宿飯店
時已晚上十一點。9/23，即會議開始的第一天，我們到 ACCV 會場註冊，並且獲得背包，和一些
ACCV的資料。由於第一天沒有正式的會議活動，因此註冊完後自由行動。9/24，會議活動正式展
開，首先為 Workshop的演講，開場的是日本的四個人，展覽關於如何將 3D動畫中的主角，代換
成自己。前置工作則必須花大約十五分鐘來使用各種擷取設備（包含臉部多角度的拍攝、走路姿
勢的拍攝、聲音、頭髮等），來讓電腦獲得足夠的資訊。接著由兩位韓國人來報導有關人臉表情
辨識的方法。他們使用 PCA＋LDA 和 AAM。下午則是以 3D 重建為主題的演講，我參觀了兩場，第
一場著重在如何重建大場景的 3D 模型，第二場則是注重細微的紋路如何重建。聽完後第三天的
活動也在此結束。9/25，這天開始 oral presentation和 poster的活動(9/25~9/27)。一個早上
約有 10場左右，下午亦同。我選取了與我領域有相關的來聽。下午四點半，oral presentation
結束，poster正式展開，約有四十多篇同時展出，我一篇篇參觀直到展覽時間結束。9/26，今天
是我 poster展出的時間。早上將海報張貼完後我就去聽對我研究有幫助的演講。到了 poster正
式展開的時間，我站在我海報面前，並解說我這次投稿的論文。期間有許多大陸人來詢問問題，
我也盡可能的回答他們，並彼此留下連絡的方式。9/27，我參加了早上的演講和下午的 poster，
ACCV的正式會議活動也在此結束。 
  原本預定在 9/28號返回台灣，但是由於機票座位訂不到，因此延至 9/29回台。 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/01/03
國科會補助計畫
計畫名稱: 基於輪廓、顏色及五官特徵的即時人臉偵測、追蹤及辨識系統
計畫主持人: 連震杰
計畫編號: 96-2628-E-006-201-MY3 學門領域: 圖形辨識
無研發成果推廣資料
國內 論文著作 期刊論文 4 4 100% 篇 
本研究的成果已發表
在 SCI &amp； EI，
其 Impact Factor 個
別如下： 
[1]. W.S. Chu, J.C. 
Chen, and J.J. 
Lien, ＇Kernel 
Discriminant 
Transformation for 
Image Set-Based 
Face 
Recognition,＇ 
Pattern 
Recognition, 
accepted and will 
be published in 
2011. Impact Factor 
= 2.554, Ranking 
=9.3% (23/246) 
[2]. T.H. Wang and 
J.J. Lien, ＇Facial
Expression 
Recognition System 
Based on Rigid and 
Non-Rigid Motion 
Separation and 3D 
Pose Estimation,＇ 
Pattern 
Recognition, Vol. 
42, No. 5, pp. 
962-977, 2009. 
Impact Factor = 
3.279, Ranking = 7% 
(17/229) 
[3]. C.T. Tu and 
J.J. 
Lien, ＇Automatic 
Location of Facial 
Feature Points and 
Synthesis of Facial 
Sketches Using 
Direct Combined 
Model,＇ IEEE 
Transactions on 
Systems, Man and 
Cybernetics: Part 
B, 2009. Impact 
Factor = 2.361, 
Ranking = 11.8% 
(2/17) 
[4]. C.C. Wang, J.Y 
Wu, and J.J 
Li ＇P d t i
碩士生 12 12 100% 
參與的碩士生人員
有：黃子魁、黃彥淇、
賴宗亨、林柏均、施
尚佑、唐瑞鴻、謝松
憲、張峻豪、劉子揚、
陳郁麒、吳沛勳、洪
晉宗。 
博士生 1 1 100% 參與的博士生人員為陳洳瑾。 
博士後研究員 0 0 100%  
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重要
國際合作、研究成果
國際影響力及其他協
助產業技術發展之具
體效益事項等，請以
文字敘述填列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
