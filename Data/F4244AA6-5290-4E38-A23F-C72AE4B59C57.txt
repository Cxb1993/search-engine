行政院國家科學委員會補助專題研究計畫■ 成 果 報 告   □期中進度報告 
 
設計與實作一個最佳演化樹之分散式建構環境 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號： NSC 94-2213-E-216 -028 
執行期間： 94年8月1日至95年7月31日 
 
計畫主持人：游坤明 
共同主持人：唐傳義 
計畫參與人員：周嘉奕、黃立明、蔡宜霖 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
□ 涉及專利或其他智慧財產權，□一年□二年後可公開查詢  
         
 
 
 
 
執行單位：中華大學資訊工程系 
 
中   華   民   國  95  年  8  月  30  日 
一、前言 
由於分子生物學的迅速發展，DNA 已經能從各個物種取出來。物種之間的比較，也進入定
量的計算。而當生物學家得到 DNA 序列後，如何計算序列之間的親疏程度是一個相當重
要的議題。在計算序列之間的親疏程度上，我們通常先求得一個距離矩陣 (distance matrix) 
代表兩兩序列之間的距離，爾後再用此 matrix 來建構演化樹 (evolutionary tree)。為了計算
序列之間的親疏程度，生物學家希望將序列並排，相同的部份儘量對齊。這個問題稱 (多
重序列排比) multiple sequence alignment，在得到  distance matrix 後，有許多不同的數學模
型可以來建立演化樹，大多數的模型也都是難題，而這裡我們所指的難題、指的是問題難
度在 NP-hard 以上。 
目前在建立演化樹上有許多不同的方法，這些方法中、依據不同的假設、包含了許多的 
heuristic algorithm；而根據不同的數學模型，有許多不同的 optimum algorithm。這些選擇、
對生物學家而言沒有絕對的好壞、也沒有絕對的意義，生物學家往往在許多的測試、嘗試
以後得到他們想要的答案。所以，當我們建構完演化樹後、我們也試著評估樹的好壞。 
在這些建立演化樹的方法中，我們可以從距離短陣建立，也可以從 DNA 序列來建立，在
建立一個最佳解的演化樹的問題上，大部份都已被證明為 NP-Complete 問題。在此計劃中
我們主要選擇是從距離矩陣來建立演化樹；用距離矩陣來建構演化樹的 heuristic algorithm 
常用有下列數種方法  (1) Unweighted Pair-Group Method with Arithmetic Mean (UPGMA), 
(2) Transformed Distance Method, (3) Neighbors-Relation Method, (4) Neighbor-Join Method，
而我們想要求的是最佳解的樹，所以我們選擇了方法是 Minimum Size Ultrametric Tree。
Ultrametric Tree 亦是由距離矩陣所建構出來的，它是一棵有根樹 (Rooted Tree)，其樹葉 
(Leaf) 代表了某一個物種，內部節點  (Internal Node) 代表在其下面物種的共同祖先 
(Ancestor)，並且假設每個物種的演化速率相等。如此，於 Ultrametric Tree 的假設下，所建
立出的樹由其各內部節點至其所屬的 leaf 距離為等距。但同樣地，給一群物種間的距離矩
陣建立最小的 Ultrametric Tree (Minimum Size Ultrametric Tree, MUT) 已被證明是 NP 的問
題。 
目前來建構演化樹所使用的方法大部份為 “分枝與界限＂ (branch-and-bound) 的方式求
最佳解。當處理的資料量不大時，單一處理器尚能負擔其計算量，但當資料量增多時，單
一處理器便會出現記憶體不足或者無法在合理的時間內求出答案。所以在目前的技術下，
如果要在合理的時間內得到答案的話，必需考慮使用多處理器或平行系統來處理所需的資
料。前人曾提出了一個以分枝與界限的方法來建構 MUT (Minimum Size Ultrametric Tree)，
在些方法中，我們可以有效的在單一機器上以分枝與界限來建構 MUT，在我們之前申請
通過的二年計劃中，在此議題的研究上，我們有下述的心得及成果，在研究中，我們依據 [24] 
作為發展的根據，依此為依據，發展了一個有效率的平行化演算法，在我們的平行化系統
中，所有計算節點同時對他們所擁有的候選樹做分枝 (branch) 的動作，當計算節點發現候
選樹符合 bound 的條件時便不再對此候選樹做分枝的動作。而當計算節點得到更好的 
upper bound 值時便會將此值傳遞給其他所有計算節點，其他所有計算節點得到新的 upper 
bound 值便可以 bound 掉更多的候選樹。基於這個理由，在叢集系統的解集合會少於單一
處理器的系統，所以我們提出的平行化分枝與限界演算法在  speedup 上可能會達到 
super-linear 的速度。我們使用了 global pool 及 local pool 做為一種負載平衡的機制，讓
計算節點不至於有閒置。在我們的系統架構中，我們用的是 master / slave 的架構，並且資
料是在執行期間由 master 指派。在我們的研究成果中可以知道，在單一處理器時，23 個
物種數已經是在我們能接受的時間內能得到最佳解的物種數目了，如果要計算更多的物種
而利用分支與界定演算法進行解決問題的重點在於分支（Branch）與界定（Bound）的方法
選擇決定上。於平行分散式系統中，往往會將原始問題分成數個可行解（Feasible solution）
的子問題（Subproblem），而分支後的一個或數個可行解分配給數個運算處理單元計算，因
此分支的動作將會影響負載平衡問題。因此，我們在研究的進行中使用了 global pool 及 
local pool 做為一種負載平衡的機制，讓計算節點不至於有閒置。而在我們的系統架構中，
我們用的是 master / slave 的架構，並且資料是在執行期間由 master 來指派，以提高系統
整體運算效率。 
 
四、計畫成果自評 
本計畫之執行成果將能於較短的時間內建構出 MUT，不但能夠增進執行效率，還能夠大幅
提昇 ultrametric tree 的正確性與結果的可讀性，本年度的計畫執行不但已順利完成預
期的計畫目標，亦已將所獲得的研究成果整理成論文並且發表了三篇國際研討
會論文 (二篇為 SCI)，一篇國內研討會論文 (EGBS 2006) ，並且亦已將所得之最
佳演化樹的建構成果撰寫工具程式並以網站的形式提供給相關領域之專家學者
研究之用 (UTCE: Ultrametric Tree Construction and Evaluation platform)，有效分享本計畫
之執行成果，UTCE 之操作畫面如圖一至五所示，本年度的計畫執行成果可說是相當
完整且豐碩。  
 
 
圖一：Ultrametric Tree Construction and Evaluation platform 首頁畫面
 
 
 
圖四：3PR 關係限制之建構使用者輸入介面 
 
 
 
 
圖五：ＭＳＡ使用者輸入介面 
G. Min et al. (Eds.): ISPA 2006 Ws, LNCS 4331, pp. 215 – 220, 2006. 
© Springer-Verlag Berlin Heidelberg 2006 
An Efficient Parallel Algorithm for Ultrametric Tree 
Construction Based on 3PR* 
Kun-Ming Yu1, Jiayi Zhou2,** , Chun-Yuan Lin3, and Chuan Yi Tang4 
1
 Department of of Computer Science and Information Engineering, Chung Hua University 
2
 Institute of Engineering Science, Chung Hua University 
3
 Institute of Molecular and Cellular Biology, National Tsing Hua University 
4
 Department of Computer Science, National Tsing Hua University 
300, Hsinchu, Taiwan, R.O.C 
1
 yu@chu.edu.tw, 2 jyzhou@pdlab.csie.chu.edu.tw,  
3
 cyulin@mx.nthu.edu.tw, 4 cytang@cs.nthu.edu.tw 
Abstract. In the computational biology and taxonomy, to construct phylogenetic 
tree is an important problem. A phylogenetic tree can represent the relationship 
and histories for a set of species and helpful for biologists to observe existent 
species. One of popular model is ultrametric tree, and it assumed the evolution 
rate is constant. UPGMA is one of well-known ultrametric tree algorithm. 
However, UPGMA is a heuristic algorithm, and it can not guarantee the 
constructed tree is minimum size. To construct minimum ultrametric tree (MUT) 
has been shown to be an NP-hard problem. In this paper, we propose an efficient 
parallel branch-and-bound algorithm with 3-Point Relationship (3PR) to reduce 
the construction time dramatically. 3PR is a relationship between a distance 
matrix and the constructed phylogenetic tree. The main concept is for any two 
species closed to each other in a distance matrix should be also closed to each 
other in the constructed phylogenetic tree. We use this property to mark the 
branching path with lower priority or higher, then we move the lower ranked 
branching path to delay bound pool instead of remove it to ensure the optimal 
solution can be found. The experimental results show that our parallel algorithm 
can save the computing time and it also shows that parallel algorithm with 3PR 
can save about 25% of computing time in average. 
Keywords: phylogenetic tree, minimum ultrametric tree, parallel branch-and-
bound algorithm, 3-point relationship, 4-point relationship. 
1   Introduction 
To construct phylogenetic trees is an important problem in the computational biology 
and in taxonomy, the phylogenetic tree can represent the histories for a set of species 
and helpful for biologists to observe existent species or evaluate the relationship of 
them. However, the real evolutionary histories are unknown in practice. Therefore, 
many methods had been proposed and tried to construct a meaningful phylogenetic 
tree, which is closing to the real one. 
                                                          
*
 The work is partially supported by National Science Council. (NSC 94-2213-E-216 -028). 
**
 The corresponding author. 
 An Efficient Parallel Algorithm for Ultrametric Tree Construction Based on 3PR 217 
Definition 4: Let P be a topology, and )(, PLba ∈ . ),( baLCA  denotes the lowest 
common ancestor of a and b. If x and y are two nodes of P, we write yx →  if and 
only if x is an ancestor of y. 
Definition 5: The distance between distance matrix and rooted topology of 
phylogenetic trees is consistent if ]),[],,[min(],[ kjMkiMjiM <  if and only if 
),(),(),( kjLCAkiLCAjiLCA =<  for any nkji ≤≤ ,,1 . Otherwise is contradictory. 
2.1   Sequential Branch-and-Bound Algorithm for MUTs 
In the MUT construction problem, the branch-and-bound is a tree search algorithm 
and repeatedly searches the branch-and-bound tree (BBT) [8,14] to find a better 
solution until optimal one is found. The BBT is a tree which can represent a topology 
of UTs. Assume that the root of BBT has depth 0, hence each node with depth i in 
BBT represents a topology with a leaf set {1,...,i+2}. 
2.2   3-Point Relationship (3PR) 
3PR is a logical method to check the LCA relation for any triplet of species (a, b, c) in 
a distance matrix, which is preserved or not in the constructed phylogenetic trees. For 
any two species (a, b), LCA(a, b) denotes the least common ancestor of (a, b). If (x, y) 
are two nodes in a phylogenetic tree, x → y is written if x is an ancestor of y. For a 
triplet of species (a, b, c) in the distance matrix M, if the distance M[a, b] of species a 
and b is less than M[a, c] and M[b, c], LCA(a, c)=LCA(b, c) → LCA(a, b)  (as ((a, b), 
c); in Newick tree format). For a triplet of species (a, b, c), it is contradictive if the 
least common ancestor relation in a distance matrix is not preserved in the constructed 
phylogenetic tree. 3PR can be used to evaluate the qualities of constructed 
phylogenetic trees. A phylogenetic tree is considered unreliable if the number of 
contradictive triplets is large. The evaluated result may be useful for biologists to 
choose a feasible phylogenetic tree construction tool. 
3   Parallel Branch-and-Bound Algorithm with 3PR 
Parallel Branch-and-Bound Algorithm with 3PR (PBBU with 3PR) is designed on 
distributed memory multiprocessors and the master-slave architecture. The PBBU 
uses a branch-and-bound technique to avoid exhaustive search of possible trees. For 
load-balance purpose, the master processor (MP) contains a Global Pool and each 
slave processor (SP) has Local Pool, moreover we use new data structure instead of 
the link list to store BBT. 
In [5], 3PR is applied as a tree evaluation method. We use this property to put 
lower rank branching path to Delay Bound Pool (DBP) when selecting branch path in 
the branch-and-bound algorithm. For example, Table 1 is the distance matrix and 
Figure 1 shows two candidates when inserting the third species c. In PBBU without 
3PR, both (a) and (b) candidates need to be added to the pool when branching. 
However, topology of (b) is closing to distance matrix, it obtained higher rank, and (a) 
has lower rank. In PBBU with 3PR, only (b) (with higher rank) candidate will be 
 An Efficient Parallel Algorithm for Ultrametric Tree Construction Based on 3PR 219 
1 2 4 8 16
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
5500
Without 3PR vs. With 3PR (HMDNA)
Without 3PR
With 3PR
Number of processors
T
im
e 
(s
ec
.)
 
Fig. 2. 3PR vs. Without 3PR (HMDNA) 
1 2 4 8 16
0
2500
5000
7500
10000
12500
15000
17500
20000
22500
25000
27500
30000
32500
35000
37500
40000
Without 3PR vs. With 3PR (Random)
Without 3PR
With 3PR
Number of processors
T
im
e 
(s
ec
.)
 
Fig. 3. 3PR vs. Without 3PR (Random) 
1 2 4 8 16
0
1
2
3
4
5
6
7
8
9
10
11
Speed-up (HMDNA)
Without 3PR
With 3PR
Number of processors
S
pe
ed
-u
p 
ra
tio
 
Fig. 4. Speed-up ratio (HMDNA) 
14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
11000
12000
13000
Computing time (16 processors)
Without 3PR
With 3PR
Number of species
T
im
e 
(s
ec
.)
 
Fig. 5. Computing time (16 processors) 
5   Conclusions 
In this paper, we have designed PBBU with 3PR for constructing MUTs problem. The 
3PR is the relationship between distance matrix and constructed evolutionary tree. It 
moves candidates which do not fit 3PR to delay bound pool in branch-and-bound 
algorithm. After that, we can obtain the tighter bounding value quickly and uses it to 
bound more candidates. In order to evaluate the performance of our proposed 
algorithm, a random data set and a practical data set of HMDNA are used. The 
experimental results show that PBBU with 3PR can find optimal solution for 36 
species within a reasonable time on 16 PCs. Furthermore, the speed-up ratio shows 
the performance of our algorithm is good in our PC cluster environment. Moreover, 
the results also show that PBBU with 3PR can save about 25% in average of 
computing time than PBBU without 3PR, and it assured the results are optimal with 
the delay bound technique. 
An Efficient Scheduling Algorithm for Irregular Data Redistribution 
Kun-Ming Yu and Yi-Lin Tsai  
Department of Computer Science and Information Engineering 
Chung Hua University, Hsinchu, Taiwan 300, ROC. 
Tel : 886-3-5186412 
Fax: 886-3-5329701 
Email: yu@chu.edu.tw 
 
Abstract. Dynamic data redistribution is used to 
enhance the performance of an algorithm and to 
achieve data locality in parallel programs on 
distributed memory multi-computers. Therefore, 
the data redistribution problem has been 
extensively studied. Previous results focus on 
reducing index computational cost, schedule 
computational cost, and message 
packing/unpacking cost. However, irregular data 
redistribution is more flexible than regular data 
redistribution; it can distribute different sizes of 
data segments of each processor to those 
processors according to their own computation 
capability. High Performance Fortran 2 (HPF-2), 
the current version of HPF, provides an irregular 
distribution functionality, such as GEN_BLOCK 
which addresses some requirements of irregular 
applications for the distribution of data in an 
irregular manner and explicit control of load 
balancing. In this paper, we present a 
degree-reduction-and-coloring (DRC) algorithm 
for scheduling HPF2 irregular array redistribution. 
We devoted to obtain the minimal number of 
transmission steps as well as to reduce the overall 
redistribution time. The proposed algorithm 
intends to reduce the number of maximum 
transmission messages in the first phase and then 
applies graph-coloring mechanism to obtain the 
final schedule. The proposed method not only 
avoids node contention, but also shortens the 
overall redistribution time. To evaluate the 
performance of DRC algorithm, we have 
implemented DRC algorithms along with the 
Divide-and-Conquer algorithm. The simulation 
results show that DRC algorithm has significant 
improvement on communication costs compared 
with the Divide-and-Conquer algorithm. 
Keywords: Irregular redistribution, communication 
scheduling, GEN_BLOCK, degree-reduction 
 
1. Introduction 
Parallel computing systems have been widely 
adopted to solve complex scientific and engineering 
problems. To efficiently execute a data-parallel 
program on distributed memory multi-computers, an 
appropriate data distribution is critical to the 
performance.  Appropriate distribution can balance 
the computational load, increase data locality, and 
reduce inter-processor communication. Array 
redistribution is crucial for system performance 
because a specific array distribution may be 
appropriate for the current phase, but incompatible for 
the subsequent one. Many parallel programming 
languages thus support run-time primitives for 
rearranging the array distribution of a program. The 
data redistribution problem has been widely studied in 
the literature. In general, data redistribution can be 
classified into two categories: the regular data 
redistribution [1,5,6,7,9,11,13,15,18] and the irregular 
data redistribution [4,8,22-24]. The regular data 
redistribution decomposes data of equal sizes into 
processors. There are three types of this data 
redistribution, called BLOCK, CYCLIC, and 
BLOCK-CYCLIC(n). The irregular data distribution 
employs user-defined functions to specify data 
distribution unevenly. High Performance FORTRAN 
2 (HPF-2) provides GEN_BLOCK functionality and 
makes it possible to handle different processors 
dealing with appropriate data size according to their 
computation capability. Previous works emphasized 
the minimal steps of data redistribution and scheduled 
the ordering of messages with minimal total 
transmission size. In the regular array redistribution, 
[15] proposed an Optimal Processor Mapping (OPM) 
scheme to minimize the data transmission cost for 
general BLOCK-CYCLIC regular data realignment. 
Optimal Processor Mapping (OPM) utilized the 
maximum matching of realignment logical processors 
to achieve the maximum data hits for reducing the 
amount of data exchange transmission cost. In the 
irregular array redistribution problem, [22, 23] 
proposed a greedy algorithm to utilize the 
Divide-and-Conquer technique to obtain near optimal 
scheduling while attempting to minimize the size of 
total communication messages as well as the number 
of steps. 
In this paper, we bring up the 
Degree-Reduction-and-Coloring (DRC) algorithm to 
efficiently perform GEN_BLOCK array redistribution. 
In section 2, we define the data communication model 
of irregular data redistribution and give an example of 
GEN_BLOCK data redistribution as the preliminary. 
Section 3 describes the DRC algorithm for the 
irregular redistribution problem. The performance 
analysis, simulation results and practical transmission 
with MPI on SMP/Linux cluster are presented in 
section 4. Finally, the conclusions are given in section 
5. 
 
2. Data communication models 
 In this section, we present some properties of 
irregular data redistribution with GEN_BLOCK 
functionality. There are no repetitive communication 
3.1 Degree-Reduction Step 
  The goal in this step is to reduce Max-degree 
repeatedly in each iteration, until Max-degree is equal 
to 2. An example of 4-degree communication 
redistribution has taken as shown in Fig 2. In the first 
phase (phase-1) of degree-reduction step, the 
messages are sorted by the non-increasing order of 
|Eij|, and the result is shown in Table 2. Then, DRC 
selects the messages into step1 of the schedule 
according to non-increasing order of message size 
except those messages causing the conflict. After 
phase-1, the Max-degree will be decreased by 1 (from 
4 to 3). Fig 3(a) and Table 3(b) show this scenario. 
DRC repeat the procedure until the Max-degree 
reaches 2, which is depicted in Fig 4. 
 
Figure 2. A data redistribution example with 
Max-degree = 4  
Table 2. The messages are sorted by non-increasing 
order of message size 
Msg no. e34 e45 e22 e65 e21 e33 e77 e11 e35 e66 e32 e76 e55
Msg size 17 15 12 10 9 8 8 7 7 7 6 6 5
 
(a) 
 
(b) 
Figure 3. The messages communication (a) before 
phase-1 of the degree-reduction step; (b) after phase-1 
of the degree-reduction step. 
 
Table 4. The schedule after phase-1 
Schedule Table 
Step1: e34, e45, e22, e77, e11, e66 
Step2:  
Step3:  
Step4:  
 
(a) 
 
(b) 
Figure 4. The messages communication (a) before 
phase-2 of the degree-reduction step; (b) after phase-2 
of the degree-reduction step. 
 
Table 5. The schedule after the procedure of phase-2 
Message no. e34 e45 e22 e65 e21 e33 e77 e11 e35 e66 e32 e76 e55
Message size 17 15 12 10 9 8 8 7 7 7 6 6 5
 
Schedule Table 
Step1: e34, e45, e22, e77, e11, e66 
Step2: e65, e21, e33, e76 
Step3:  
Step4:  
 
Message number e34 e45 e22 e65 e21 e33 e77 e11 e35 e66 e32 e76 e55
Message size 17 15 12 10 9 8 8 7 7 7 6 6 5
with different number of processors and total message 
size. The number of processors is from 8 to 24. We 
can observe that the DRC algorithm has better 
performance in the uneven data redistribution 
compared with Divide-and-Conquer algorithm. Since 
the data is concentrated in the even case, from Fig 7(a) 
and 7(b), we can observe that DRC has better 
performance compared with the uneven case. In both 
even and uneven cases, DRC performs better than the 
Divide-and-Conquer algorithm.  
Figure 6. The events percentage of computing time is plotted (a) with different number of processors and (b) with 
different number of total message sizes in 24 processors, on the uneven data set. 
Figure 7. The events percentage of computing time is plotted (a) with different number of processors and (b) with 
different number of total message sizes in 24 processors, on the even data set. 
 
5.Conclusion 
 In this paper, we have presented a 
Degree-Reduction-Coloring (DRC) scheduling 
algorithm to efficiently perform HPF2 irregular array 
redistribution on a distributed memory multi-computer. 
The DRC algorithm is a simple method with low 
algorithmic complexity to perform GEN_BLOCK 
array redistribution. The DRC algorithm is an optimal 
algorithm in terms of minimal number of steps. In the 
same time, DRC algorithm is also a near optimal 
algorithm satisfying the condition of minimal message 
size of total steps. Effectiveness of the proposed 
methods not only avoids node contention, but also 
shortens the overall communication length.  
 For verifying the performance of our proposed 
algorithm, we have implemented DRC as well as the 
Divide-and-Conquer redistribution algorithm. The 
experimental results show improvement in 
communication costs and high practicability on 
different processor hierarchies. Also, the experimental 
results indicate that both of them have good 
performance on GEN_BLOCK redistribution. In 
many situations, DRC is better than the 
Divide-and-Conquer redistribution algorithm. 
 
Reference 
[1] G. Bandera and E.L. Zapata, “Sparse Matrix 
Block-Cyclic Redistribution,” Proceeding of IEEE 
Int'l. Parallel Processing Symposium (IPPS'99), San 
Juan, Puerto Rico, 355 - 359 ,April 1999 
[2] J.A. Bondy and U.S.R. Murty, Graph Theory with 
Applications, Macmillan, London, 1976. 
[3] Frederic Desprez, Jack Dongarra and Antoine Petitet, 
“Scheduling Block-Cyclic Data redistribution,” IEEE 
Trans. on PDS, vol. 9, no. 2, pp. 192-205, Feb. 1998. 
[4] Minyi Guo, “Communication Generation for 
Irregular Codes,” The Journal of Supercomputing, 
vol. 25, no. 3, pp. 199-214, 2003. 
[5] Minyi Guo and I. Nakata, “A Framework for 
Efficient Array Redistribution on Distributed 
應用網格建立一個高效能演化樹平行建構環境 *
 
游坤明 1,  徐蓓芳 1,  賴威廷 1,  謝一功 1,  周嘉奕 1,  林俊淵 2,  唐傳義 3  
 
1  中華大學資訊工程學系  
2  國立清華大學分子與細胞生物研究所  
3  國立清華大學資訊工程學系  
 
1  yu@chu.edu.tw, {b9102042, b9004060, b9102004}@cc.chu.edu.tw, 
jyzhou@pdlab.csie.chu.edu.tw 
2 cyulin@mx.nthu.edu.tw 
3 cytang@cs.nthu.edu.tw 
 
摘要 
以平行處理方式來計算龐大的資料運算是近
年來一個非常重要的應用觀念。有許多不同的環境
架構伴隨著不同的應用。網格 (Grid) 是一種建立
在網際網路上的架構，網格可透過網際網路與其他
網格互相分享資源，因此可以視為在使用龐大的且
容易增減的資源來運算；與傳統的叢集式系統相
比，傳統的叢集式系統 (Cluster) 若要增加運算能
力，則必需花費比網格多的費用，因此運算能力有
限。在一般所見的網格中，必須要有相同的協定、
彼此認同的認證、安全性的考量以及合理的資源存
取，才能讓網格在網路上互相溝通。使用網格運算
我們所要處理的資料及程式，並且在合理的時間內
得到正確的結果。本論文使用平行化演算法並以人
類粒腺體為例，在單機、網格與叢集電腦環境中建
構演化樹，並比較其效能差異。 
關鍵詞：等距演化樹 , 叢集電腦計算, 網格計算, 
Globus Toolkit 
1. 簡介 
生物資訊研究領域中，科學家常常需要從演
化樹的結果以了解物種間的親疏關係。從距離矩陣
中建造演化樹在生物學和分類法方面是一個重要
的議題，因此也產生許多不同的模型及相對應的演
算法。而大部份的最佳解問題都已被証明為 
NP-hard。 
 
* This word was supported in part by the NSC of 
ROC, under grant NSC-93-2213-E-216-037 and 
NSC-94-2213-E-216-028 
其中在許多不同的模型中有一個重要的模型
便是假定演化的速度是一致的 [5, 17]。在這種前
提下，利用距離矩陣算出的演化樹將會是一個等距
演化樹(ultrametric tree)。 
本論文使用一種高效能的平行化分枝界限演
算法(branch-and-bound) 建立最小距離演化樹 。這
個平行演算法是建立在 master-slave centralize 的
架構上，並且加入了有效的負載平衡、節點與節點
間通訊的策略，以解決最小權值等距演化樹建構的
問題，使得時間在可容忍的範圍內完成。 
近年來，對於許多以電腦輔助來求解的問題
越來越多，且個人電腦的計算能力已無法滿足在合
理的時間內得到結果。於是分散式的計算技術便是
下一個發展的層次。本論文以人類粒腺體為例建構
出演化樹，建構演化樹是一種非常複雜且耗時的計
算過程，使用一般的個人電腦，將耗費大量的時間
以求得結果，有時還會因資源不足造成等待許久的
運作中斷，因此，要在合理的時間內得到滿意的結
果，必須具有高效能的電腦，如超級電腦，但在經
濟的考量下，我們可使用叢集電腦或網格來達到近
似的效能。 
叢集電腦可有大小不同規模，此做法的最大
優點是「可擴充性」 (scalability) ：只要增加新的
個人電腦，就可以提高叢集電腦的效能。在某些情
況下資料是分布在不同的地區中需要互相存取，而
網格是透過網路連線將好幾個在不同地區的叢集
電腦串聯成的，更可以有效的利用這樣的優點來保
持最新的訊息，所以在使用資源效率方面更遠勝於
叢集電腦 [19]。 
在網格上發展的技術為中介軟體，是用來整
合網格分散的計算資源，主要角色是擔任機器間協
調功能的任務。在網格的使用者和資源提供者之
 1
花較長的時間，或者沒有回傳資料，都沒有關係，
因為有此狀況時，它會在暫停一段時間後，自動把
工作分派給其他電腦做處理。 
2.4 Globus Toolkit 
Globus [8, 14, 20]對訊息安全、資源管理、訊
息服務、數據蒐集管理以及應用開發環境等網格關
鍵理論和技術進行廣泛的研究，並且開發出可以在
多種平台上執行的 GlobusToolkit，用來幫助規劃和
建造大型網格試驗和應用平台，開發大型網格系統
可以執行的應用程式。Globus Toolkit 同時提供了
好幾種語言模式給程式設計師選擇，就類似像物件
導向的方式。程式開發者更可以由 Globus Toolkit 
中所提供的服務任意選取最符合需求的工具去與
現存的軟體作整合。例如: GRAM 提供資源管理的
協定、MDS 提供資訊服務的協定、GridFTP 提供
了資料傳輸的協定…等，這些全部都有使用 GSI 
安全協定在他們的連接層 [20]。
 
表 1. GlobusToolkit 所提供的服務 
 
2.5 MPICH-G2 
MPI 是 訊 息 傳 送 介 面 (Message Passing 
Interface)用來撰寫  message-passing programs 和
可以廣泛的使用於平行運算的一種基礎 API。在
網格應用程式上 message-passing 的優點是它提供
比通訊協定 TCP/IP sockets 更高層的介面，讓我們
可以直接使用通訊結果而不必知道中間是如何溝
通。Globus 服務已被用來發展成 Grid-enable MPI 
以 MPICH library 為基礎，Nexus 為通訊基礎， 
GRAM 服務為資源分配和 GSI 來做安全認證。 
MPICH-G2 是 Grid-enable 以 MPI v.1.1 為基
礎在網格上的實作。它使用了 Globus Toolkik(像是
資源分配、安全性)的服務。MPICH-G2 准許以連
接不同平台的機器來執行 MPI 的程式。MPICH-G2
會自動作資料轉換當在兩個不同平台時的傳輸和
自動的選擇 TCP 以提供多重協定通訊的訊息給網
路上機器及傳出有MPI提供的訊息給區域內機器。 
2.6 UniGrid 
網格計算的目的是用來整合大型網路環境下
的各種資源。UniGrid 是連結國內七所大學及國家
高速網路中心之電腦網格系統，建置一「國家計算
網格實驗平台」，以協助推廣網格計算的觀念到各
產學領域。UniGrid 將著重在使用網格計算領域最
常用之程式集及工具套件 Globus。並且有提供隨
時每個節點的 CPU、RAM 狀況監看。 
Globus[8]提供了網格中使用的協定，可以讓
使用者充分利用分散於各處的資源中建出網格計
算的架構。 
 
圖 2. UniGrid 上的程式流程 
 
Service Name 功能 
Resource managrment GRAM 資源分配與工作管理 
Communication Nexus 單一或多重溝通服務 
Security GSI 認證與聯繫上的安全服務 
Information MDS 分散式存取和狀態的資訊 
Health and status HBM 監測系統零件健康狀況 
Remote data access GASS 遠程存取資料經由連續及平行的連繫裝置 
Executable management GEM 結構、讀取技術與狀態執行管理 
Information GRIS 查詢計算資源現有的設定、能力及狀態 
GridFTP GridFTP 提供高效能、安全，以及健全的資料傳輸機制 
                                                                             3
衡，並且我們為了減低不必要的計算，在算出一個
比原來標準用的上限還低時，就會一直把資訊傳給
全部的節點以達到提升計算效率。而平行化架構採
用的是主從式架構，起始化時分配的資料與計算過
程中所需動態分配的資料都是由 master 來做分配。 
在負載平衡的問題上，一般來說可以分為靜
態與動態的負載平衡 [3]。靜態的負載平衡指的是
在資料的分配只在程式一開始的期間做分配，而程
式執行期間不做任何的資料牽移；相對的動態負載
平衡指的是會依需求而在節點間搬動及牽移資
料。動態負載平衡可以分為集中式的 (centralized) 
與分散式的 (decentralized) [3]。集中式的負載平衡
是由一台管理主機 (管理節點) 來做調控，每個 
節點藉由把資料送至管理節點後再由管理節點來
決定資料要如何分配。相對的分散式負載平衡則是
由節點彼此間互相溝通後再彼此間一同決定的機
制。一般來說，集中式的架構能夠有更好的負載平
衡，因為管理節點可以知道所有節點的狀態並決定
一個更好的分配，但在一個大型的平行系統下，集
中式的負載平衡會因為管理節點的瓶頸而效能不
佳。 
 
Input: A n * n distance matrix M 
Output: The minimum ultrametric trees 
 
Step 1: Master computing node re-label the species 
such that feasible maxmin permutation. 
Step 2: Master computing node creates the root of the 
BBT. 
Step 3: Master computing node run UPGMA and 
using the result as the initial UB (upper bound). 
Step 4: Master computing node branches the BBT 
until the branched BBT reach 2 times of total nodes 
in the computing environment. 
Step 5: Master computing node broadcasts the global 
UB and send the sorted matrix the nodes cyclically. 
 
Step 6:  
while number of UTs in LP (Local Pools) > 0 or 
number of UTs in GP (Global Pools) > 0 do 
 if number of UTs in LP = 0 then 
  if number of UTs in GP <> 0 then 
   receive UTs from GP 
  end if 
 end if 
 v = get the tree for branch using DFS 
 if LowerBound(v) > UB then 
  continue 
 end if 
 insert next species to v and branch it 
 if v branched completed then 
  if Cost(v) < UB then 
   update the GUB (Global Upper 
Bound) to every nodes  
   add the v to results set 
  end if 
 end if 
 if number of UTs in GP = 0 then 
  send the last UT in sorted LP to GP 
 end if 
end while 
 
Step 7: Gather all solutions from each node and 
output it. 
4. 實驗結果 
4.1 實驗環境及結果 
在實驗的環境中，我們使用了單機、以及叢
集電腦與網格的系統。單機及叢集電腦的系統如表
2。網格實驗環境使用的是 UniGrid 系統。 
在實驗數據中，我們挑選人類粒腺體做為實
驗數據，並以物種數目 12、14、16、18、20、22 一
一執行，每一物種數目有 10 組測試資料。我們從
10 組資料中分別取中位數、平均數、最差情況來
做實驗結果比較，以期消除資料相依所產生執行時
間的差異。 
 
表 2. 實驗環境 
單機 
處理器數目 1 
環境 中華大學平行分散實驗室 
硬體設備 AMD 2000+、2GB DDR RAM 
叢集電腦 
處理器數目 16 
環境 中華大學平行分散實驗室 
硬體設備 AMD 2000+、1GB DDR RAM 
網格 
處理器數目 12 
環境 國家網格計算實驗平台 
硬體設備 AMD 1.3G、2GB DDR RAM 
處理器數目 4 
環境 東海大學高效能計算實驗室 
硬體設備 
AMD MP 2000+ ‘2、512MB DDR 
RAM’2 
 
如表 3 及圖 4 分別為執行時間中位數的結
果，我們可以發現，當物種數目增加時，計算時間
也相對增加，而在圖中也可以了解，不論是叢集電
腦或者是網格系統，都能夠有效的降低執行時間。 
表 3. 中位數時間比較表 
物種數目 單機 叢集電腦 網格 
12 0.113313 0.146947 0.130881 
14 2.936615 0.889956 1.180245 
                                                                             5
適合，且叢集電腦資源有限，如果遇到一個龐大的
問題也可能需要計算很久的時間，所以即使資料分
布在世界各地也可以輕鬆的應付並保持資料的最
新狀況。 
 
表 6. 叢集電腦與網格使用不同節點數比較 
 叢集電腦 16 節點 網格 16 節點 網格 24 節點
1 1629 1652.96 885.517 
2 10273 10691.6 6838.49 
3 750 764.104 750.752 
4 561 566.25 458.426 
5 249 258.197 256.616 
6 199 199.96 148.454 
7 54 57.693 83.55 
8 126 128.596 110.922 
 
0
2000
4000
6000
8000
10000
12000
1 2 3 4 5 6 7 8
物種組別
時
間
(秒
)
叢集電腦16節點
網格16節點
網格24節點
 
圖 7. 叢集電腦與網格使用不同節點數比較圖 
 
5. 結論 
實驗中測試的網格所使用的處理器 16 顆，與
叢集電腦相比，數據中發現，網格與叢集電腦使用
處理器個數相同，網格並無任何優勢，網格效能較
叢集電腦差，因為叢集電腦內部溝通速度遠大於網
格間所使用的網際網路溝通。未來我們可以考慮建
立更有效率的網格溝通機制，相信可以大幅改善網
格的效能。 
 我們的目標將是不只侷限於計算人類
粒腺體的資料，推廣至以網格來運算蛋白質的樣
本，甚至其他的資料在正規化之後皆可用網格來運
算以得到結果。 
目前是用網格來運算人類粒腺體的樣本，雖
然花費的時間非常的多，因為剛開始時需要找出在 
網格上的最佳效率，但是，未來中，我們將更有效
率的平行演算法及網格 API，目標在使用方面，只
要將要處理的資料整理成我們目前資料輸入的形
式，就可以得到我們要求的數據，所以，未來可能
運用此種方法來運算類似的龐大資料，像蛋白質等
等的樣本，應該跟目前的運作方式相同，在網格上
運算即可得到結果。 
參考文獻 : 
[1] 全球網格(World Wide Grid)發展趨勢 
[2] 格網計算平台架設實例簡介格網計算平架設
實 例 簡 介  Introduction to Constructing 
Introduction to Constructing Computational 
Computational Grid Grid 
王順泰 
[3]  Barry Wilkinson, Michael Allen, “Parallel 
Programming”, P.H. 
[4]  B.Y. Wu, K.M. Chao, C.Y. Tang, 
“Approximation and Exact Algorithms for 
Constructing Minimum Ultrametric Tree from 
Distance Matrices,” Journal of Combinatorial 
Optimization 3, pp. 199-211 
[5] D. Gusfield, “Algorithms on Strings, Trees, and 
Sequences, computer science and computational 
biology,” Cambridge University Press, 1997 
[6]  E. Dahlhaus, “Fast parallel recognition of 
ultrametrics and tree metrics,” SIAM Journal on 
Discrete Mathematics, 6(4):523-532, 1993 
[7] H.J. Bandelt, “Recognition of tree metrics,” 
SiAM Journal on Discrete Mathematics., 
3(1):1-6, 1990 
[8] The Globus Project: A Status Report. I. Foster, 
C. Kesselman. Proc. IPPS/SPDP '98 
Heterogeneous Computing Workshop, pp. 4-18, 
1998. 
[9] The Anatomy of the Grid: Enabling Scalable 
Virtual Organizations. I. Foster, C. Kesselman, S. 
Tuecke. International J. Supercomputer Applications, 
15(3), 2001. 
[10]  The Nexus Approach to Integrating 
Multithreading and Communication. I. Foster, C. 
Kesselman, S. Tuecke, J. Journal of Parallel and 
Distributed Computing, 37:70--82, 1996. 
[11] A Grid-Enabled MPI: Message Passing in 
Heterogeneous Distributed Computing Systems. I. 
Foster, N. Karonis. Proc. 1998 SC Conference, 
November, 1998. 
[12] A Secure Communications Infrastructure for 
High-Performance Distributed Computing. I. Foster, 
N. Karonis, C. Kesselman, G. Koenig, S. Tuecke. 6th 
IEEE Symp. on High-Performance Distributed 
Computing, pp. 125-136, 1997. 
[13]  M.D. Hendy and D. Penny, 
“Branch-and-bound algorithms to determine 
minimal evolutionary trees,” Mathematical 
Biosciences, 59:277-290, 1982. 
[14] M. Frach, S. Kannan, and T. Warnow, “A 
robust model for finding optimal evolutionary 
trees,” Algorithmica, 13:155-179, 1995. 
[15] M. Krivanek, “The complexity of ultrametric 
partitions on graphs,” Information Processing 
Letter, 27(5):265-270, 1988. 
[16] Chuan Yi Tang, Solomon K.C. Wu, "Chee 
Kane Chang, “A scalable Fully Distributed 
                                                                             7
UTCE: Ult rametric Tree Const ruct ion and Evaluat ion plat form
Kun-Ming Yu, Jiayi Zhou, Chun-Yuan Lin, Chuan Yi Tang
Int roduct ion
UTCE is a platform  with tools for ult rametric t ree construction 
and t ree evaluat ion. Phylogenet ic t rees can be used by 
biologists to describe the evolut ionary relat ionship among of 
living species. Many models or tools have been proposed to 
construct  phylogenet ic t rees. One of the popular models is an 
ult rametric t ree with the assumption of a constant  rate. 
UPGMA is one of well-known ult rametric t ree building 
algorithms.
Although UPGMA often fails to reconst ruct  an evolut ionary 
t ree when a molecular clock does not  apply, it  is suitable for 
some cases of clocklike data. Moreover, the assumption of a 
constant  rate can be used by biologists to est imate or 
disprove init ial observat ions or hypotheses for their research 
work.
However, UPGMA is a heurist ic algorithm and can not 
guarantee the constructed phylogenet ic t ree with m inimum 
size. UTCE provides an efficient  m inimum ultrametric tree 
construct ion tool, PBBU, with a parallel branch-and-bound 
algorithm. The input  of PBBU is a met ric distance matrix or 
original/pre-aligned mult iple DNA/protein sequences of 
species.
It  should be noted that  the constructed phylogenet ic t ree by 
PBBU could not  be used to reject  other phylogenet ic t rees by 
UPGMA and other tools. The goal of PBBU is to give another 
v iewpoint  for biologists to observe the evolut ion relat ionship 
of species under the assumpt ion of a molecular clock 
hypothesis and the minimum evolut ion principle. In addit ion, 
in UTCE, two logical methods, 3PR and 4PR, are designed to 
evaluate a phylogenet ic t ree and/or the corresponding metric 
distance matrix. 
Para lle l Branch-and-Bound
It  is a parallel branch-and-bound algorithm to construct an 
m inimum ultrametric t ree from a distance matrix with the 
number of species and t ime constraints. User can specify the 
number of processors for comput ing and t ime constrain. The 
results will send by e-mail within given t ime.
3 -Point  Relat ionship ( 3 PR)
3PR is a logical method to check the relat ion for any t riple of 
species (a, b, c) in distance matrix, which is preserved or not  
in the constructed phylogenet ic t rees. Moreover, it  can 
illust rate the consistent  between distance matrix  and 
evolut ion for various tree const ruct ion methods.
4 -Point  Relat ionship ( 4 PR)
4PR is another logical method to find cont radict ive species in 
distance matrix by checking the LCA relat ions in any set  of 4 
species.
For any set  of 4 species (a, b, c, d), it  has 4 t r iplets of species 
(a, b, c), (a, b, d), (a, c, d) and (b, c, d) and each tr iplet  has its 
LCA relat ion. For any set  of 4 species (a, b, c, d), no 
phylogenet ic tree can conform  to all of four LCA relat ions 
when they are ((a,b),c);, ((a,b),d);, ((a,c),d);, ((b,d),c); or 
((a,b),c);, ((a,b),d);, ((a,c),d);, ((c,d),b); and this set is 
cont radict ive. 
Acknow ledgem ent
The work is part ially supported by Nat ional Science Council. 
NSC 94-2213-E-216 -028
Sequence Alignm ent
A simple dynamic programming algorithm is used to compute 
the edit  distance amount  any two species (a,b) with DNA or 
protein sequences. 
UTCE is freely available at :
ht tp://pdclust er1 .csie .chu.edu.tw /t ree2
17
Science, 1991, m tDNA in D-loop, 
using PAUP*  3.0
The results of PBBU. Here we can observe the 
Chimpanzee already separated with others.
Science, 1992, T7 bacteriophage,
NJ, UPGMA also can construct
this t ree. 
PNAS, 2006, NJ, non-coding 
region EnM001
We also obtain this t ree by using UTCE with the sam e 
distance m atrix from paper. 
Nature, 2000, m tDNA not in D-
loop, using PAUP*4.0
d[0,1]  =  506 >  d[0,7]  =  381, however in the NJ 
t ree, 0 and 1 are closer to each other than 0 
and 7
----- contradiction sets ----- 
283 
W.Pygmy_1  A.American !Kung_7    European_8  
W.Pygmy_1  A.American !Kung_10   European_8  
W.Pygmy_1  E.Pygmy_6  !Kung_7    European_8  
W.Pygmy_1  E.Pygmy_6  !Kung_10   European_8  
W.Pygmy_2  W.Pygmy_1  E.Pygmy_4  A.American  
W.Pygmy_2  W.Pygmy_1  E.Pygmy_5  A.American  
W.Pygmy_2  W.Pygmy_1  E.Pygmy_6  A.American  
W.Pygmy_2  European_8 !Kung_8    P.NewG_80   
A.American European_8 !Kung_8    P.NewG_80   
. 
. 
.
The sam ple output  contradict ion set .
 
專題報告的時間控制上不良，超出預定之時程太多，這些都是整個會議美中不足
的地方。 
 
四、攜回資料 
1. 大會議程 
2. IMECS 2006研討會論文集 
3. 優秀論文獎(The Certificate of Merit)獎狀 
專題報告的時間控制上不良，超出預定之時程太多，這些都是整個會議美中不足
的地方。 
 
四、攜回資料 
1. 大會議程 
2. IMECS 2006研討會論文集 
3. 優秀論文獎(The Certificate of Merit)獎狀 
216 K.-M. Yu et al. 
In the input of distance matrix, a phylogenetic tree is constructed according to the 
distance matrix [10,11]. In general, these values are edit distances between two 
sequences of any two species. There are many different models and motivated 
algorithmic problems were proposed [1,9]. However, most of optimization problems 
for phylogenetic tree construction have been show to be NP-hard [2-4,6,7]. An 
important and commonly used model is assumed that the rate of evolution is constant. 
Based on this assumption, the phylogenetic tree will be an ultrametric tree (UT), 
which is rooted, leaf labeled, and edge weighted binary tree. Because many of these 
problems are intractable and NP-hard, biologists usually construct the trees by using 
heuristic algorithm. The Unweighted Pair Group Method with Arithmetic mean 
(UPGMA, [1]) is one of the popular heuristic algorithms to construct UTs. 
Although construct MUTs is an NP-hard problem, it is still worthy to construct for 
middle-size of species. Thus, it seems possible to find an optimal tree using 
exhaustive search. Nevertheless, for n species, the number of rooted and leaf label 
tree is, it grows very rapidly. For example, 710)10( >A , 2110)20( >A , 3710)30( >A . 
Hence, it is impossible to exhaustively search for all possible trees even n are middle-
size. Wu et al. [13] proposed a branch-and-bound algorithm for constructing MUTs to 
avoid exhaustive search. The branch-and-bound strategy is a general technique to 
solve combinatorial search problems.  
In this paper, 3-Point Relationship (3PR) is used to construct MUTs more 
efficiently. 3PR is the relationship between a distance matrix and the constructed 
phylogenetic tree. The concept is that in triplet of species (a, b, c), any of two species 
which is closed to each other in the distance matrix should aslo be closed to each 
other in the constructed phylogenetic tree in a distance matrix. The experimental 
results show that PBBU with 3PR can reduce about 25% computation time both in 
sequential and parallel algorithms. 
The paper is organized as follows. In section 2, some preliminaries for sequential 
branch-and-bound algorithm and 3PR are given. Parallel algorithm is described in 
section 3. Section 4 shows our experimental results, and final section is our conclusions. 
2   Preliminaries 
In this paper, we present PBBU with 3PR for construct minimum ultrametric tree. In 
the following, we denote an unweighted graph G=(V,E,w) with a vertex set V, an edge 
set E, and an edge weight function w. Some definitions are given as follows: 
Definition 1: A distance matrix of n species is a symmetric nn×  matrix M such that 
0],[ ≥jiM  for all 0],[ =iiM , and for all nji ≤≤ ,0 . 
Definition 2: Let ),,( wEVT =  be an edge weighted tree and Vvu ∈, . The path length 
from u to v is denoted by ),( vudT . The weight of T is defined by ∑
∈
=
Ee
ewTw )()( . 
Definition 3: For any M (not necessarily a metric), MUT for M is T with minimum 
)(Tw  such that },...,1{)( nTL =  and ],[),( jiMjidT ≥  for all nji ≤≤ ,1 . The problem 
of finding MUT for M is called MUT problem. 
218 K.-M. Yu et al. 
selected due to the distance of a and c is greater than the distance of b and c. This 
result is based on the conception that in a triplet of species (a, b, c), any of two 
species which is closed to each other in the distance matrix should also be closed to 
each other in the corresponding phylogenetic tree in a distance matrix. However, it 
cannot be directly used to bound another branching path, and PBBU with 3PR put 
others candidates to the DBP to ensure the optimal solution can be found.  
 
Table 1. Distance matrix 
 a b c 
a 0 25 20 
b 25 0 15 
c 20 15 0 
a c b a c b
(a) (b)
 
Fig. 1. Candidate BBT 
4   Experimental Results 
In the experimental results, we implement PBBU and PBBU with 3PR on a Linux 
based PC cluster. Each computing node is an AMD Athlon PC with a clock rate of 2.0 
GHz and 1GB memory. Each node is connected with each other by 100Mbps 
network. There are two data sets used to test our algorithms. One is a random data set, 
which is generated randomly. The distance matrix in the random data set is metric and 
the range of distances is between 1 and 100. Another is a data set composed of 136 
Human Mitochondrial DNAs (HMDNA), which is obtained from [12]. Its distance 
matrix is metric and the range of distances is between 1 and 200. In order to eliminate 
the problems of data dependence, for each testing data, we run 10 instances. Then we 
compare the average, median, and worst cases.  
Figure 2 and 3 show that PBBU with 3PR and delay bound technique can find the 
optimal solution and save about 25% of computation time than PBBU without 3PR. 
Because 3PR technique move lower ranking candidates which disaccording to 3PR to 
delay bound pool, after that, the better bounding value can be found early. Afterward 
it can bound more candidates to decreasing computation time. 
Figure 4 is the speed-up ratio of HMDNA data set. We observed that the speed-up 
ratio of 3PR is better than it without 3PR. Furthermore, the difference between 3PR 
and without 3PR is larger when the number of processors increasing. Because of the 
tighter bounding value can be found quickly with more processors. It also shows that 
our algorithm is scalable in large number of computing resources. Figure 5 shows the 
computation time of 16 processors of PBBU with 3PR for different number of 
species. We can observe that the computation time grow rapidly when the number of 
species increasing. Moreover, the reduced proportion between PBBU and PBBU with 
3PR is increasing with larger number of species. We consider that large number of 
species contains more candidates that a tighter bounding value which can be obtained 
from 3PR technique can also bound grater number of candidates; it can decreasing the 
computation time. 
220 K.-M. Yu et al. 
References 
1. T.H. Cormen, C.E. Leiserson, R.L. Rivest and C. Stein “Introduction to Algorithm,” MIT 
Press, 1990. 
2. W.H.E. Day “Computationally difficult parsimony problems in phylogenetic systematics,” 
J. Theoretic Biol., 103, 1983, pp.429-438. 
3. W.H.E. Day “Computational complexity of inferring phylogenies from dissimilarity 
matrices,” Bulletin of Math. Biol., 49, 1987, pp.461-467. 
4. W.H.E. Day, D.S. Johnson, and D. Sankoff “The computational complexity of inferring 
rooted phylogenies by parsimony,” Math. Biosci., 81, 1986, pp.33-42. 
5. C.T. Fan, “The evaluation model of evolutionary tree,” Master Thesis, Nationa Tsing Hua 
University, 2000. 
6. L.R. Foulds “Maximum savings in the Steiner problemin phylogeny,” J. Theoretic Biol., 
107, 1984, pp.471-474. 
7. D. Gusfield “Algorithms on Strings, Trees, and Sequences, computer science and 
computational biology,” Cambridge University Press, 1997. 
8. M.D. Hendy and D. Penny “Branch and bound algorithms to determine minimal 
evolutionary trees,” Math. Biol., 59, 1982, pp.277-290. 
9. S. Kumer, K. Tamura, M. Nei “MEGA: Molecular Evolutionary Genetics Analysis 
software for miceocomputers,” Comput. Appl. Biosci., 10, 1994, pp.189-191. 
10. W.H. Li “Molecular Evolution,” Sinauer Associates, 1997. 
11. R.D.M. Page “TreeView: An application to display phylogenetic trees on personal 
computers,” Comput. Appl. Biosci., 12, 1996, pp.357-358. 
12. L. Vigilant, M. Stoneking, H. Harpending, K. Hawkes and A.C. Wilson “African 
Populations and the Evolution of Human Mitochondrial DNA,” Science, 253, 1991, 
pp.1503-1507. 
13. B.Y. Wu, K.M. Chao, and C.Y. Tang “Approximation and Exact Algorithms for 
Constructing Minimum Ultrametric Trees from Distance Matrices,” J. Combinatorial 
Optimization, 3, 1999, pp.199-211. 
14. C.F. Yu and B.W Wah “Efficient Branch-and-Bound Algorithms on a Two-Level Memory 
System,” IEEE Trans. Parallel and Distributed Systems, 14, 1988, pp.1342-1356. 
patterns in the irregular GEN_BLOCK array 
redistribution. A data redistribution can be represented 
by a bipartite graph, called a redistribution graph. To 
simplify the presentation, notations and terminologies 
used in this paper are defined in the following. 
Definition 1: Given an irregular GEN_BLOCK 
redistribution on array A[SPi] and A[DPi] over P 
processors, the source processors of array data 
elements A[SPi] are denoted as SPi; the destination 
processors of array elements A[DPi] are denoted as 
DPi where 1 ≤ i ≤ P. 
Definition 2: A bipartite graph G = (V, E) is 
used to represent the communications of an irregular 
data redistribution between source and destination 
processors.  Vertices of G are used to represent the 
processors.  Edge eij in G denotes the message sent 
from SPi to DPj, where eij ∈ E.  |Eij| denotes the 
transmission message size through the redistribution. 
Definition 3: Every message transmission link 
in irregular data redistribution is not overlapped. 
Hence, the total number of message transmission link 
E is P ≤ E ≤ 2 × P - 1.  
Definition 4: Each processor has more than one 
eij to send data to destination processors or receive 
data from other source processors. The number D of 
eij owned by one processor is denoted as D-degree and 
the maximum D-degree of all processors is denoted as 
Max-degree. We denote that the processors have the 
Max-degree number of messages as Pmax. 
Definition 5: If SPi sends messages to DPj-1 
and DPj+1, the transmission between SPi and DPj 
must exist, where 1 ≤ i, j ≤ P.  This result was 
mentioned as the consecutive communication property 
[12]. 
Fig.1 shows an example of redistributing two 
GEN_BLOCK distributions on A[SPi] and A[DPi].  
Table 1(a) shows mapped communication message 
size to source processors and destination processors, 
respectively.  The communications between source 
and destination processor sets are depicted in Fig 1.  
There are 13 transmission messages, e11, e21, e22, …e77 
among the processors involved in the redistribution. 
Due to the considerable influence of node contention, 
a processor can only send at most one message to 
another processor in each communication step and the 
same is true for the receiving message. The messages, 
which cannot be scheduled in the same 
communication step, are called conflict tuple. For 
instance, {e11,e21} is a conflict tuple since they have a 
common destination processor DP1; {e21,e22} is also a 
conflict tuple because of the common source 
processor SP2.  Table 1(b) shows a simple schedule 
result for this example.  
 
 Figure 1. An example of data redistribution  
Table 1(a). The total message size of redistribution 
data for each processor in Fig. 1. 
SP1 SP2 SP3 SP4 SP5 SP6 SP7
7 27 32 15 15 7 14 
DP1 DP2 DP3 DP4 DP5 DP6 DP7
16 12 14 17 27 23 8 
Table 1(b). A simple schedule 
Schedule Table 
Step1: e34, e45, e22, e77, e11, e66 
Step2: e56, e23, e35 
Step3: e21, e33, e55, e76  
 
3. Proposed Algorithm 
 The performance of a data redistribution 
procedure is determined by four costs: index 
computational cost Ti, schedule computational cost Ts, 
message packing/unpacking cost Tp, and data transfer 
cost. The data transfer cost for each communication 
step consists of start-up cost Tsu and transmission cost 
Tt. Let the unit transmission time τ denote the cost of 
transferring a message of unit length. In general, the 
message startup cost is directly proportional to the 
number of communication steps. The total number of 
communication steps is denoted by N. The total 
redistribution time equals Ti+Ts+ ∑=
=
++
Ni
i
isup mTT
1
)( τ , where mi = Max{e1, e2, e3, .., 
ek} and ej represents the size of message scheduled in 
the ith communication step for j = 1 to k. In irregular 
redistribution, messages of varying sizes are 
scheduled in the same communication step. Therefore, 
the largest size of message in the same 
communication step dominates the data transfer time 
required for this communication step.  
The main idea of the 
Degree-Reduction-and-Coloring (DRC) algorithm is 
to diminish the degree of Pmax repeatedly by 
scheduling the message in the first step of data 
redistribution process until Max-degree is equal to 2. 
The remaining messages are then scheduled into the 
communication steps by utilizing the concept of 
bipartite graph coloring mechanism. The details of the 
steps will be described in the following subsections. 
3.2 Message-Coloring Step 
 After completing the degree-reduction step, we 
can obtain a redistribution graph with Max-degree of 
2 and the resulting redistribution graph is 2-edge 
colorable [2], since it is a bipartite graph and its 
maximum degree is equal to 2. In the 
Message-Coloring Step, DRC schedules the left 
messages into the same step in a non-increasing order 
to accomplish an optimal scheduling unless a conflict 
occurs. Figure 5 shows the outcome of 
message-coloring and Table 6 shows the final 
schedule obtained from DRC algorithm. 
 
Figure 5. The outcome of redistribution graph after 
applying the message coloring mechanism 
 
Table 6. The final schedule obtained from DRC 
Msg no. e34 e45 e22 e65 e21 e33 e77 e11 e35 e66 e32 e76 e55
Msg size 17 15 12 10 9 8 8 7 7 7 6 6 5
 
Schedule Table 
Step1: e34, e45, e22, e77, e11, e66 
Step2: e65, e21, e33, e76 
Step3: e35 
Step4: e32, e55  
 
The algorithm of the Degree-Reduction-Coloring is 
given as follows. 
====================================== 
Algorithm DRC 
generating messages; 
// generate messages from AS[Pi] to AD[Pi] 
step = maximum degree; 
sort_msgSize(); 
// sorting in decreasing order by message size 
while (step > 2 ) 
   { 
      choose_msg(step); 
      // selecting message without conflict tuple, set 
into (maximal degree - step + 1) schedule 
step 
      step-- 
   }  // degree-reduction iteration 
while ( remaining_messages != null ) 
   { 
      selecting_msg(maximal degree-1); 
      // selecting message set into maximal degree-1 
schedule step 
      check_msg_continue_set(); 
      // check remaining message set 
      coloring_maximal_msg(maximal degree); 
      // color the maximal message with degree 
maximal degree -1 and the neighbor 
message with maximal degree    
   }  // message coloring mechanism 
end of  DRCM 
====================================== 
 
4. Performance Evaluation 
 To evaluate the performance of the proposed 
methods, we have implemented the DRC along with 
the Divide-and-Conquer algorithm [23]. The 
performance simulation is discussed in two categories, 
even GEN_BLOCK and uneven GEN_BLOCK 
distributions. In even GEN_BLOCK distribution, each 
processor owns similar size of data. In contrast to 
even distribution, few processors might be allocated 
by grand volumes of data with uneven distribution. 
Since data elements could be centralized to some 
specific processors, it is also possible for those 
processors to have the maximum degree of 
communications.  
 The simulation program generates a set of 
random integer number and the size of message as 
A[SPi] and A[DPi]. Moreover, the total message size 
sending from SPi equals to the total size receiving to 
DPi keeping the balance between source processors 
and destination processors. 
 We assume that the data computation 
(communication) time in the simulation is represented 
by the transmission size |Eij|. In the following figures, 
the percentage of events is plotted as a function of the 
message size and the number of processors. Also, in 
the figures, “DRC Better” represents the percentage of 
the number of events that the DRC algorithm has 
lower total computation (communication) time than 
the Divide-and-Conquer algorithm, while “DC Better” 
gives the reverse situation. If both algorithms have the 
same total computation (communication) time, “The 
Same Results” represents the number of that event.  
 In the uneven distribution, the size of message’s 
up-bound is set to be B*1.7 and that of low-bound is 
set to be B*0.3, where B is equal to the sum of total 
transmission message size / total number of 
processors. In the even distribution, the size of 
message’s up-bound is set to be B*1.3 and that of 
low-bound is set to be B*0.7. The total message-size 
is 10M.  
 Fig 6(a) and 6(b) show the simulation results of 
both the DRC and the Divide-and-Conquer algorithm 
Memory Multicomputers,” The Journal of 
Supercomputing, vol. 20, no. 3, pp. 243-265, 2001. 
[6] Minyi Guo, I. Nakata and Y. Yamashita, 
“Contention-Free Communication Scheduling for 
Array Redistribution,” Parallel Computing, vol. 26, 
no.8, pp. 1325-1343, 2000. 
[7] Minyi Guo, I. Nakata and Y. Yamashita, “An Efficient 
Data Distribution Technique for Distributed Memory 
Parallel Computers,” Joint Symp. on Parallel 
Processing  (JSPP'97), pp.189-196, 1997. 
[8] Minyi Guo, Yi Pan and Zhen Liu, “Symbolic 
Communication Set Generation for Irregular Parallel 
Applications,” The Journal of Supercomputing, vol. 
25, pp. 199-214, 2003. 
[9] Edgar T. Kalns, and Lionel M. Ni, “Processor 
Mapping Technique Toward Efficient Data 
Redistribution,” IEEE Trans. on PDS, vol. 6, no. 12, 
pp. 1234-1247, December 1995. 
[10] S. D. Kaushik, C. H. Huang, J. Ramanujam and P. 
Sadayappan, “Multiphase data redistribution: 
Modeling and evaluation,” International Parallel 
Processing Symposium (IPPS’95), pp. 441-445, 
1995. 
[11] Peizong Lee, Academia Sinica, and Zvi Meir Kedem, 
“Automatic Data and Computation Decomposition on 
Distributed Memory Parallel Computers,” ACM 
Transactions on Programming Languages and 
systems, Vol 24, No. 1, pp. 1-50, January 2002. 
[12] S. Lee, H. Yook, M. Koo and M. Park, “Processor 
reordering algorithms toward efficient GEN_BLOCK 
redistribution,” Proceedings of the ACM symposium 
on Applied computing, pp . 539-543, 2001. 
[13] Y. W. Lim, Prashanth B. Bhat and Viktor and K. 
Prasanna, “Efficient Algorithms for Block-Cyclic 
Redistribution of Arrays,” Algorithmica, vol. 24, no. 
3-4, pp. 298-330, 1999. 
[14] C.-H Hsu, S.-W Bai, Y.-C Chung and C.-S Yang, “A 
Generalized Basic-Cycle Calculation Method for 
Efficient Array Redistribution,” IEEE Transactions 
on Parallel and Distributed Systems, vol. 11, no. 12, 
pp. 1201-1216, Dec. 2000. 
[15] Ching-Hsien Hsu, Kun-Ming Yu, “An Optimal 
Processor Replacement Scheme for Efficient 
Communication of Runtime Data Realignment,” 
Parallel and Distributed and Processing and 
Applications, - Lecture Notes in Computer Science, 
Vol. 3358, pp. 268-273, 2004. 
[16] C.-H Hsu, Dong-Lin Yang, Yeh-Ching Chung and 
Chyi-Ren Dow, “A Generalized Processor Mapping 
Technique for Array Redistribution,” IEEE 
Transactions on Parallel and Distributed Systems, vol. 
12, vol. 7, pp. 743-757, July 2001. 
[17] Antoine P. Petitet and Jack J. Dongarra, “Algorithmic 
Redistribution Methods for Block-Cyclic 
Decompositions,” IEEE Transactions on Parallel and 
Distributed Systems, vol. 10, no. 12, pp. 1201-1216, 
Dec. 1999 
[18] Neungsoo Park, Viktor K. Prasanna and Cauligi S. 
Raghavendra, “Efficient Algorithms for Block-Cyclic 
Data redistribution Between Processor Sets,” IEEE 
Transactions on Parallel and Distributed Systems, vol. 
10, No. 12, pp.1217-1240, Dec. 1999. 
[19] .L. Prylli and B. Touranchean, “Fast runtime block 
cyclic data redistribution on multiprocessors,” 
Journal of Parallel and Distributed Computing, vol. 
45, pp. 63-72, Aug. 1997. 
[20] S. Ramaswamy, B. Simons, and P. Banerjee, 
“Optimization for Efficient Data redistribution on 
Distributed Memory Multicomputers,” Journal of 
Parallel and Distributed Computing, vol. 38, pp. 
217-228, 1996. 
[21] Akiyoshi Wakatani and Michael Wolfe, 
“Optimization of Data redistribution for Distributed 
Memory Multicomputers,” short communication, 
Parallel Computing, vol. 21, no. 9, pp. 1485-1490, 
September 1995. 
[22] Hui Wang, Minyi Guo and Wenxi Chen, “An 
Efficient Algorithm for Irregular Redistribution in 
Parallelizing Compilers,” Proceedings of 2003 
International Symposium on Parallel and Distributed 
Processing with Applications, LNCS 2745, 2003.  
[23] Hui Wang, Minyi Guo and Daming Wei, 
"Divide-and-conquer Algorithm for Irregular 
Redistributions in Parallelizing Compilers”, The 
Journal of Supercomputing, vol. 29, no. 2, pp. 
157-170, 2004. 
[24] H.-G. Yook and Myung-Soon Park, “Scheduling 
GEN_BLOCK Array Redistribution,” Proceedings of 
the IASTED International Conference Parallel and 
Distributed Computing and Systems, November, 
1999. 
 
 
間，擔任資源分配的協調工作，幫助使用者找到適
合其使用的機器，並完成資料存取的交易 [19]。
其中一個重要的組成要素，就是後設資料。 
網格的優點之一，是有效率的使用閒置中的
電腦，若是再長時間運算比較下，網格可以更有效
率的使用資源。使用平行處理的環境，像是叢集計
算或網格計算，必須用平行化的演算法以及使用平
行化的溝通工具，例如 MPI，以幫助程式在該平臺
上順利運作。 
目前我們已成功的在網格的環境上執行平行
化演算法，並且建構出演化樹，從網格與叢集電腦
的實驗數據可看出，網格擁有與叢集電腦相似的效
能。在本論文中，比較使用單機、叢集電腦及網格
三種環境下的效能，在實驗結果中可顯示出，單機
運算能力遠不如叢集電腦及網格；叢集電腦與網格
之間的比較，若在相同節點數計算下，兩種環境效
能是差不多的。 
2. 背景 
2.1 等距演化樹 
在建立演化樹上有許多模型，其中一種為等
距演化樹。等距演化樹為假設各物種的演化速率一
致 [5, 13]，而等距演化樹的特性為有共同的父節
點，物種存在葉節點而且在邊上有權重值的一個二
元樹，在每個內節點的子樹中有同樣的路徑長到每
一個葉節點上 [4]。對於一個 n * n 的距離矩陣 M 
來說，定義最小的等距演化樹指的是兩兩葉節點的
邊上權重總合為最小的。因為等距演化樹可以很容
易的轉換為二元樹且不需要改變葉節點的距離 
[13]，所以，等距演化樹是一個非常適合給電腦計
算的模型。 
 
  圖1. 建立分支界限樹 (BBT) [3] 
 
如圖 1，我們可知，等距演化樹的數目 A(n)，
隨著 n 的增加，演化樹的數量也快速的增加。有一
些有關等距演化樹的研究先前已被提出  [6, 7, 
15]。由於這些問題往往是不易解的，所以這些研
究大都是基於 heuristic 演算法。舉例來說，像 
UPGMA(Unweighted Pair Group Method with 
Arithmetic mean) [17]就是一個很常被用來建立等
距演化樹的演算法。 
在本論文中，我們使用 Exact Algorithms for 
Constructing Minimum branch-and-bound’s from 
Distance Matrices [4]的演算法為基礎，並將之平行
化。在上述方法中，使用分支界限法的策略作為找
尋最小距離演化樹的方法。為了求得最小距離演化
樹我們會將所有可能的樹型都找出並一一求值，但
隨著物種數的增加，等距演化樹 A(n)的增加是非常
快的，例如：A(20) > 1021 ，A(25) > 1029 ，A(30) 
> 1037 ，於是上述方法中使用了分支界限法的策
略來避免完全的搜尋。在本論文中，使用有效率平
行化的分支界限演算法建立最小距離演化樹，在我
們提出的方法中，是一個主從且集中式的平行化架
構，並在此架構中加上了 loading-balancing, 
bounded 和 communication strategies 等機制，以增
加程式的效率。 
2.2 叢集計算 
叢集計算(cluster computing)在隨著目前的科
技下，處理器和周邊設備的普及，我們可以用低成
本連接出高效能的叢集計算機。叢集計算機是以高
速網路連接個人電腦或工作站而成的，可提供高效
能的計算能力而且降低原來達到此效能的成本。在
運作上，既然是由許多台電腦連接的，所以普通的
應用程式也無法在上面發揮作用，必須設計適合在
平行及分散式環境中的演算法，而且同時配合像是
MPI 這種專門用來做平行溝通的軟體，來設計應用
程式。 
現今在電腦和網路普及下，幾乎是可以看成
所有電腦都與網際網路相連，如果把叢集電腦更廣
義的角度來看，每台電腦就好像被網際網路連接的
大型區網，全球就是一個大型的叢集電腦，但是事
實並非如此，因為無法做到資源互相分享、計算互
相分擔，所以為了達到更廣義的資源活化運算，於
是網格計算的理念被提出。 
2.3 網格計算 
網格計算(Grid Computing)可讓分散於各地
的虛擬組織，協調彼此的資源分享，同時滿足大量
運算的需求。而集合分散的運算資源之外，網格計
算能夠經由網路管理組織內任何一個可使用的運
算資源，進而降低伺服器的閒置時間。 
網格計算可以解決在同一時間內使用網路上
很多資源去解決一個問題或者當一個問題需要大
量處理器計算或是需要存取大量分佈不同地方的
資料。耳熟能詳的例子像是  SETI (Search For 
Extraterrestrial Intelligence )@home 它讓上千人的
電腦在閒置時的處理器中去幫助計算資料。而且這
些電腦都是獨立性工作，指的是說無論有些工作需
                                                                             2
 User Applications
Message Passing Library 
(MPICH-G2)
Grid Middleware 
(Globus Toolkit)
Queuing System
(Condor)
Operating System
(Unix)
Network Fabric
* MPI Applications Can be execute in Grid environment.
* Support job submission, scheduling, resource allocation in Grid environment.
* Provide secure access to remote resource.
* Provide message passing among user MPI Programs.
* Provide priority-based job queuing and scheduling.
* MPICH-G2 & Globus compatible with many platforms include Linux, FreeBSD, Solaris etc...
* We use GT 3.2.1 and MPICH-G2 1.2.6 on Linux system (Debian).
 
圖 3 .UniGrid 的架構圖[2] 
 
3. 系統架構 
單機上的程式處理方式與分散式系統的處理
方式不同，所以當平台由單機發展為分散式系統
時，若程式想要發揮平行處理的效能，就必須改變
原來的程式演算法，在程式中加入訊息傳送的程式
觀念。 
3.1 單機演算法 
在建構演化樹的問題上，一般用的是 
UPGMA 這一類的啟發式演算法，所得到的解並不
是最佳解。[4] 中提出了利用 branch-and-bound 來
建構最佳解演化樹。雖然 branch-and-bound 的解
空間會非常大，但中型的演化樹在生物學家的實際
用上仍然非常有實用價值。 
在 [4] 中所提出的演算法中，首先，執行
UPGMM 得到一個起始解的 upper bound (UB)，接
著開始建立 branch-and-bound tree (BBT) 如果建
立時 lower bound (LB) 大於目前的 UB 時就刪除
此節點，選擇下一個位置繼續建立，當計算到 UB
比目前的 UB 低時就更新。直到所有物種都建立完
畢，最後，權值最小的樹即是我們所要求的解。其
演算法如下： 
 
Algorithm BBU 
Input: An n £n distance matrix M. 
Output: The minimum ultrametric tree for M. 
 
Step 1: Relabel the species such that (1, 2….. n) is a 
maxmin permutation. 
Step 2: Create the root v of the BBT such that v 
represents the only topology with leaves 1 and 2. 
Step 3: Run UPGMM to find a feasible solution and 
store its weight in UB. 
Step 4:  
while there is a node in BBT do 
Delete all nodes v from BBT if LB(v) ¸ UB or all 
the children of v have been deleted. 
Select a node s in BBT, whose children has not 
been generated. 
Generate the children of s by using the branching 
rule. 
If a better solution is obtained, then update UB. 
End while 
 
3.2 平行化分支界限演算法 
雖然利用 branch-and-bound 的技巧可以利用
bound 值來避免將每個可能做搜尋，但是隨著物種
數目的增加，所需的計算時間也成指數成長。所
以，我們便利用平行計算的方法來加速演化樹的建
構。 
考慮在平行計算的環境上的特性，所以針對
資料結構和演算法做了些改變和增加。在資料結構
上，為了減少節點與節點之間的溝通，因此所定義
的資料結構包含了每個內結點的左子節點、右子節
點、父節點，與子結點的路徑。在演算法上，為了
更能發揮平行處理的環境，必須讓每個節點的計算
量平衡，故必須加上如 Global Pools、Local Pools、
等機制讓節點與節點間可以達到動態的負載平
                                                                             4
16 36.1053 29.2515 11.6873 
18 1003.268 324.5231 332.807 
20 138.684 157.6269 99.2526 
22 9873.82 5911.42 2625.637 
 
中位數時間比較圖
0
2000
4000
6000
8000
10000
12000
12 14 16 18 20 22
物種數目
時
間
(秒
) 單機
叢集電腦
網格
 
圖 4. 中位數時間比較圖 
 
表 4 以及圖 5 為平均計算時間；表 5 以及
圖 6 為最差計時間，兩個相比較，我們可以了解、
平均計算時間可能被最差計算時間所影響，因為建
構演化樹的問題有資料相依的情形，所以我們會選
擇中位數計算時間做為我們主要的比較依據。而從
圖中也可以觀察到，計算時間隨著數種數目的成長
有相當快速的增加。 
表 4. 平均數時間比較表 
物種數目 單機 叢集電腦 網格 
12 0.344878 0.166051 0.236581 
14 41.99103 7.537349 7.390265 
16 390.8962 207.8525 58.34718 
18 2598.467 983.583 1031.805 
20 1114.028 4705.797 249.0695 
22 9873.82 5911.42 2625.637 
 
平均值時間比較圖
0
2000
4000
6000
8000
10000
12000
12 14 16 18 20 22
物種數目
時
間
 (秒
) 單機
叢集電腦
網格
 
圖 5. 平均數時間比較圖 
 
表 5. 最差狀況時間比較表 
物種數目 單機 叢集電腦 網格 
12 0.785476 0.395494 0.927229 
14 303.738 40.4603 35.1285 
16 1387.6 911.781 203.611 
18 9339.67 4327.96 4606.76 
20 5009.17 25064.1 1028.86 
22 9873.82 5911.42 5068.7 
 
最差狀況時間比較圖
0
10000
20000
30000
12 14 16 18 20 22
物種數目
時
間
 (秒
) 單機
叢集電腦
網格
 
圖 6. 最差狀況時間比較圖 
 
4.2 結果討論 
我們從數據中取出中位數、平均數、最差情
況來做比較。由圖表明顯看出隨著物種數目增加、
計算相同物種時，單機效能最差，叢集電腦次之，
網格效能最佳。正常情況下，叢集電腦的效能應比
網格好，因為使用內部溝通為高速網路的叢集電
腦，其效能遠高於使用網際網路溝通的網格。但是
實驗結果與理論不符，這是因為實驗中所使用的叢
集電腦設備較網格所使用的電腦設備差。 
最初使用單機運算樣本以繪出演化樹，雖然
成功建出演化樹，但是花費時間非常驚人，且運算
樣本的大小有限，因此進度緩慢、效率不佳，之後
採用叢集電腦。叢集電腦環境為 16 顆處理器，因
此效率提高許多。但因為考慮到經濟成本以及為了
應付更巨大的計算，我們考慮了更有效率的平行處
理環境，網格。 
所以我們開始把平行化建立演化樹的程式以
網格平台來做實驗。我們以 10 組物種數目 20 的資
料去實驗，實驗結果見表 6 和圖 7。實驗結果發現
網格計算效能，如果在相同的節點數目，計算效能
似乎較叢集電腦差了一點，但是如果網格使用 24
節點，則效能遠超過叢集電腦 16 節點。 
而現今已有許多網格平台的建立，像是
UniGrid 就是聯合國內七所大學以及國家高速網路
中心的叢集實驗室所成的網格實驗平台，我們可以
使用更多的資源去執行程式，並且透過網格運算的
技術，他會到網路中尋找閒置的電腦，並將工作依
據適當的比例分配，送到這些電腦上執行，然後將
結果送回，這樣做可以更有效率。 
而且我們考慮了未來萬一資料是放置在世界
各處或者是隨時都會更動的，那叢集電腦就顯得不
                                                                             6
Parallel Branch & Bound Algorithm on PVM 
cluster” 
[17] W.H. Li and D. Graur, “Foundomentals of 
Molecular Evolution,” Sinauer Associates, 
1991. 
[18] Yuji Shinano, “Kenichi Harada and Ryuichi 
Hirabayashi,” Control Schemes in a 
Generalized Utility for Parallel 
Branch-and-Bound Algorithms, Parallel 
Processing Symposium, 1997. Proceedings., 
11th International , 1-5 Apr 1997, Page(s): 621 
-627 
[19] GridCafé (http://www2.twgrid.org/gridcafe) 
[20] The Globus Project (http://www.globus.org/) 
 
                                                                             8
