 1
目  錄 
 
一、中文摘要……………………………………………………………… 2 
二、英文摘要……………………………………………………………… 3 
三、前言…………………………………………………………………… 4 
四、研究目的 …………………………………………………………….. 5 
五、文獻探討 …………………………………………………………….. 6 
六、研究方法與結果 ……………………………………………………. 11 
七、總結與計畫成果自評 ……………………………………………… 22 
參考文獻 ………………………………………………………………… 23 
附件一 可供推廣之研發成果資料表…………………………………… 25 
 3
二、研究計畫英文摘要 
As the development of Internet and database technology progresses rapidly, we can acquire 
a lot of information from various information systems and the Web. Then, it is important to 
know how to select suitable algorithms to implement on a fast platform in order to find out 
implied information and how to set up knowledge database based on existing experiences and 
rules so that everyone can learn better. The purpose of this project is to explore how to present 
knowledge under knowledge exploration and handling process and to discovered knowledge 
using an efficient algorithm and platform. One way is to use suitable parallel algorithms and the 
other way is the grid computation platform. Our main research topics focus on association rule, 
data classification and data clustering. We translate these traditional data mining algorithms into 
the parallel algorithms in order to execute them on the grid computing environment. Next, we 
will develop a knowledge base for parallel mechanism of algorithm and job allocation 
mechanism on the grid computing environment. 
 This plan has three phases. The first phase is to develop many different parallel algorithms 
of association rules for different applications and to implement them on the grid computing 
environment. The second phase is to develop many different parallel algorithms of data 
classification and data clustering for different applications and to implement them on the grid 
computing environment.  The third phase is to build data mining engine by incorporating 
intelligent mechanisms for parameter setting and knowledge databases to provide users with 
better mining results in an optimal process. We will select agent mechanism to distribute the 
load among the nodes. The expected result of this project will make significant contribution to 
academic world and industries on research for grid computing. This will be considerably 
helpful to promote the development of our information technology and value-added knowledge 
systems. 
 
Keywords: grid computing, association rule, knowledge exploration, parallel algorithm, data 
classification, data clustering 
 
 5
在眾多的資源裡找出符合使用者需求的計算資源。以下就本階段的幾個重要環節說明我
們所採行的方法，以及可能遇到的問題和我們提出的解決方法。 
一、 資料量大小比較：將一個現有不同的平行演算法放在格網環境中，並用不同大小的資
料，去比較該演算法在格網計算中的效能性及變化。 
二、 同質、異質節點比較：同質節點執行和混合節點間執行效率之比較。 
三、 網格資源：節點之 CPU、磁碟空間、記憶體當作評估節點資源因子。 
四、 資源代理人機制：每個節點安裝資源代理人，即時回報節點目前資源狀態給工作分配
節點，動態按照資源大小重新分配工作至各節點。 
五、 網格節點的加入、移除：透過資源代理人回報機制，動態偵測節點加入、移除，重新
按照資源大小，分配工作至各節點。 
六、 線上即時資料探勘：透過網格強大節點數量的計算能力，探討不同資料探勘演算法的
工作分配機制，讓探勘演算法可以線上即時查詢，不再是耗時的計算。 
 
四、研究目的 
本計畫目的在格網計算環境下，發展多種平行化資料挖掘技術，和探討知識探索處
理過程的最佳化，以作為管理和決策支援之應用。接下來將這些技術模組化並整合，
建立一個網站讓使用者來使用這些技術，透過格網的快速分散、平行計算能力，將加
速知識探索的時效性。本計畫第三年是藉由第一、二年所累積之平行化演算法及網格
環境的經驗，接下來要導入代理人機制的設計。藉由資源代理人機制，計算分散在世
界各個角落的資源，在眾多的資源裡找出符合使用者需求的計算資源。 
本次計畫針對產岀之 BIT 演算法為基礎，改良兩種不同的平行演算法，分別是針對
「資料切割」和「子樹切割」兩種分散計算的方式，並探討在代理人機制下如何達到
最佳之負載平衡。本計畫是架構在 Globus 4.0 的 MPI 執行環境下實作，分析在不同
容量大小的資料集與不同數量的處理器下，評估所發展之演算法之效能，為資料探勘
技術在格網應用上引進代理人機制，讓資料探勘技術在格網應用的深度進一步提升。 
 
 
 
 7
 
Figure 5.1 The proposed BIT tree 
For simplicity, we use the numeric number to represent the item. The root is an empty node 
to hold the pointer of the first tree node where it stores the smallest 1-itemset encountered so far. 
Additional left or right child is added according to the items contained in a transaction. The 
construction of the BIT tree is complete after the last transaction has been processed. The left and 
right children are defined as follows. 
Definition 1: Left Child of BIT. Given a binary tree structure BIT, the left child of an n-itemset 
node is its n+1-itemset. Assume that an n-itemset is represented by (i1, i2,…, in). Then, the left 
child of that itemset is the n+1-itemset (i1, i2,…, in, in+1). For example, the itemsets (1, 2, 3) and 
(1, 2, 4) are the left children of (1, 2) as shown in Figure 5.1. 
Definition 2: Right child of BIT. Given a binary tree structure BIT, the right child of an 
n-itemset node is its sibling n-itemset. To simplify the construction process, we arrange the 
sibling nodes in the increasing order of the item numbers. For example, Figure 5.1 shows that (1, 
3) and (1, 4) itemsets are the right children of the itemset (1, 2). 
Definition 3: Representation of a BIT node. In a BIT tree, each node represents one of the all 
possible combinations of items, which we call the candidate itemsets. At the end of the tree 
construction, we have generated all candidates for the processed transactions. Using our BIT 
structure we only need to record the last item of the itemset in the tree as shown in Figure 5.2. 
This simplified representation can save a lot of memory space. For example, in Figure 5.2, the 
nodes A, B, C and D represent itemsets (1, 2, 3), (1, 3), (2, 3) and (3) respectively. 
 
 9
all the 2i-1 itemsets {(x1), (x2),…,(x1, x2, …, xi)} to be added to a lexicographical tree. When add 
any one of the n-itemset to the lexicographical tree, it has to search the tree according to the 
prefix of the itemset to add a new node or update the support count of the itemset. Therefore, it 
has to search the lexicographical tree at least 2i-1 times. While in the node representation and 
binary tree of our method, the itemset nodes are added to the BIT without searching the tree. 
According to the items contained in the transaction, each item can be considered a child node (in 
the simplified format) to directly added to the previous item node and regulate the BIT tree until 
the end has been reached. The number of the itemsets we generate is also 2i-1. For instance, there 
is a transaction T = (1, 2, 3), which generates 7 candidate itemsets {(1), (2), (3), (1, 2), (1, 3), (2, 
3), (1, 2, 3)} to be added to an empty lexicographical tree, shown in Figure 5.3. Take the itemset 
(1, 2, 3) as an example, when it is added to the lexicographical tree, we have to search the 
lexicographical tree by its prefix to find the right place to be added. On the contrast, when we 
construct the BIT for the transaction (1, 2, 3), we only need to add two children to the previous 
item without searching the tree structure, shown in Figure 5.4. By the definition given in Section 
3.1, the simplified BIT for constructing the transaction (1, 2, 3) also refers to Figure 5.5. The 
itemset nodes of our method are the same as the nodes in the lexicographical tree. Therefore, the 
itemsets, which are generated by our method, cannot be missing or duplicate. 
 
 
Figure 5.3 The lexicographical tree for constructing the transaction (1, 2, 3) 
 11
else if there exists the 1-itemset in the BIT tree 
     then increase the support count of the 1-itemset node 
else add the 1-itemset to the root's left child and regulate the BIT tree 
else  
generate 2i-1 candidate itemset nodes 
    if there exists corresponding itemset nodes in the BIT tree 
     then increase the support count of the itemset node 
else  
1. add the n-itemset node to the right child of its sibling n-itemset 
node 
2. adjust the sub-tree of the added node 
3. add the n-itemset node to the left child of the n-1-itemset node 
4. adjust the sub-tree of the added node 
 
整個 BIT 演算法的效能瓶頸為建立二元樹，代理人機制須具有平均分配建樹成本給
處理器的能力，以增進資料探勘的效率，這也是本次計畫的主要工作。 
 
 
六、研究方法與結果 
本次計畫重點係以關聯法則演算法 BIT 為基礎，進一步將 BIT 擴展成平行化 BIT 演算
法，並探討在格網環境下(圖 6.1)代理人機制的各種環境差異比較。 
實驗環境如下： 
Client Computer： 
Master（Node 1）:Pentium 4 3.0GHz、3.5G RAM 
Slave（Node 2）：Pentium 4 2.0 GHz、256MB RAM 
Network： 
高速乙太網路 10/100Mbps 
Grid OS: Globus 4.0 
 13
(1). Interface Agent 
 提供檔案上傳和結果下載機制。 
 提供人機介面: 強化使用者方便性之操作介面。 
 依照其他 Agent 的需要，詢問使用者的需求，如參數設定(Mining Agent)、評分的
好壞(Evaluation Agent)、視覺化的互動(Visualization Agent)。 
 記錄使用者及 Agent 之間的互動(Monitor Agent)。 
 提供登入、登出機制。 
(2). Inference Agent 
 依照使用者需求的不同，輔助其設定演算法的參數。 
 提供使用者個人化的服務。 
(3). Data Agent 
 瞭解使用者上傳的資料。 
 管理使用者上傳的資料檔案。 
(4). Mining Agent 
 依照使用者所設定的演算法、參數、及資料，進行資料挖掘的工作。 
 將挖掘的結果傳給 Evaluation Agent。 
(5). Evaluation Agent 
 接受 Mining Agent 傳來的結果進行評估，並將評估結果呈現給使用者。 
 接受使用者對結果及參數的回饋(Feedback Agent)。 
 提供過濾(Filter)的機制。 
(6). Visualization Agent 
 接受 Mining Agent 及 Evaluation Agent 傳來的資料，並以視覺化的方式呈現。如
長條圖、直方圖、圓餅圖、AR Graph 等。 
 提供與使用者互動的機制，如 Zoom in、Zoom out、Distortion 等。 
 
(二)、 BIT － 資料切割法 
1. 方法： 
於 MPI 提供平行輸入功能之前，先行切割輸入資料，由於先行切割資料，所以每
個 Node 讀入的資料量相差無幾，有利於各個 Node 的負載平衡（Load Balance）。
 15
 
Root Root 
Root
RootRoot
4:1 4:1 
5:1 5:1 
4:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
0:1 
4:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
Root 
0:1 
4:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
Root 
0:1 
4:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
6:1 
Root
0:1 
4:1 
5:1 5:1 
7:1 7:1 7:1 6:1 
7:1 
 17
Node 2： 
Input： 
 
Tid ItemSet 
1 0, 5, 7 
2 1, 2, 5 
 
 
Root Root RootRoot
0:1 0:1 
5:1 5:1 
0:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
Root 
0:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
1:1 1:1 
Root 
0:1 
5:1 5:1 
7:1 7:1 7:1 7:1 
1:1 1:1 
2:1 2:1 2:1 2:1 
 19
平均分給 Node2 及 Node3，這兩個 Slave nodes 再根據所分配到的樹來做 Mining，
找出 Frequent Patterns。 
2. 流程圖： 
 
 
3. 二元樹圖形： 
Node 1： 
Input： 
Tid ItemSet 
1 4, 5, 7 
2 0, 6, 7, 8 
3 0, 5, 7 
4 1, 2, 5 
 
sub_tree info:處理 prefix 是 0 
 
Write Sub-Tree Info to Node1 
Gather Result 
Scan Database
Write Sub-Tree Info to Node2 
Agent divides Tree 
Mining Mining 
Construct Sub-Tree on Node 1 Construct Sub-Tree on Node 2 
 21
 
4. 代理人機制：Agent 的主要目的是要達到各 Node 間的負載平衡，在切割子樹法中，
Agent 位於 Master Node 中，會根據 Node 的數量，平均切割子樹，讓每個 Node 中
之 Sub-tree 大小一致，資料分配到每個 Node 上，每個 Node 依照 Sub-tree 資訊，
建構專屬子樹，以達到分散建立子樹計算量之負載平衡。 
5. Algorithm 的 Pseudo Code（含 MPI） 
Input：Transactions 
Output：Frequent ItemSet 
Steps： 
1. Divide Tree into N sub-trees 
2. Agent distributes N sub-trees to nodes  
3. Scan data from transactions 
4. Distribute every transaction to every node 
5. Execute BIT Algorithm on each nod to construct its dedicated sub-tree 
6. Gather the results from each Sub-Tree 
7. Output the results 
Root 
5:2 
7:1 7:1 
1:1 
2:1 2:1 
4:1 
5:1 
7:1 7:1 
7:2 6:1 
7:3 7:1 
8:1 8:1 8:1 8:1 
5:3 
 23
 
參考文獻： 
1. Ming-Chuan Hung, Jungpin Wu, Jin-Hua Chang, and Don-Lin Yang, "An Efficient K-Means 
Clustering Algorithm Using Simple Partitioning," Journal of Information Science and 
Engineering, Vol.21, No.6, pp. 1157-1177, Nov. 2005. 
2. Kun-Che Lu, Don-Lin Yang and Jungpin Wu, "Fast Adaptive Clustering for Very Large 
Datasets," Proceedings of the 9th WSEAS International Conference on Computers, Jul. 2005. 
Athens, Greece. 
3. Tapas Kanungo,David M. Mount,Nathan S. Netanyahu,Christine D. Piatko,Ruth Silverman 
and Angela Y. Wu, “An Efficient k-Means Clustering Algorithm: Analysis and 
Implementation,” IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE 
INTELLIGENCE, 2002 
4. Alan P. Reynolds, Graeme Richards and Vic J. Rayward-Smith, “The Application of 
K-Medoids and PAM to the Clustering of Rules,” IDEAL 2004, LNCS 3177, pp. 173–178, 
2004. 
5. T. Zhang, R. Ramakrishnan and M. Livny, “BIRCH: an efficient data clustering method for 
very large datasets,” ACM SIGMOD conference 1996. 
6. M. Ester, H.P. Kriegel, J. Sander and X. Xu, “A density-based algorithm for discovering 
clusters in large spatial databases,” ACM KDD Conference, 1996. 
7. Don-Lin Yang, Ching-Ting Pan, and Yeh-Ching Chung, “An Efficient Hash-Based Method 
for Discovering the Maximal Frequent Set,” Proceedings of IEEE International Computer 
Software and Applications Conference (COMPSAC 2001), pp. 511~516, Oct. 2001, Chicago, 
USA.. 
8. D. L. Yang, H. P. Lee, and M. C. Hung, “An efficient support-oriented algorithm for 
interactive mining of association rules,” Proceedings of the 5th World Multiconference on 
Systemics, Cybernetics and Informatics, pp. 455-460, Jul. 2001, Florida, U.S.A.. 
9. Don-Lin Yang and Chia-Ching Wu, “A Framework for Mining Multiple-level Association 
Rules,” Proceedings of the Fifth Conference on Artificial Intelligence and Applications, Nov. 
2000, Taipei, Taiwan. 
10. D.L. Yang and J.C. Wang, “A Study on Mining Association Rules with Long Itemset 
Patterns,” Proceedings of the 11th National Conference on Information Management, May 
2000, Kaohsiung, Taiwan. 
11. Mingjun, S. and Sanguthevar, R., 2006, “A Transaction Mapping Algorithm for Frequent 
Itemsets Mining,” IEEE Transaction on Knowledge and Data Engineering, Vol. 18, No 4, pp. 
472-481. 
 25
可供推廣之研發成果資料表 
□可申請專利 ■可技術移轉                                    日期：2008 年 5 月 31 日 
國科會補助計畫 
計畫名稱：研究和發展以網格運算為基礎之平行化智慧型資料挖掘
系統 
計畫主持人：楊東麟 
計畫編號：NSC 95-2221-E-035-068-MY3 學門領域：資訊學門一 
技術/創作名稱 以網格運算為基礎之平行化智慧型資料挖掘系統 
發明人/創作人 楊東麟 
中文： 
在網格運算環境上，協助使用者選擇適合的演算法和參數設定，
從大量的資料中，能夠快速的利用資料挖掘技術找到隱含的知識，
做為決策支援與規劃的參考。我們使用的資料挖掘演算法，包括關
聯法則、資料分類、資料分群。本系統可以提昇我國資訊技術、網
格運算的發展與知識加值的產業效益。 
 
技術說明 
英文： 
In the Grid-based computing environment, we help users select 
suitable data mining algorithms and their corresponding parameters 
with the implementation on a fast platform in order to find the hidden
information from a large amount of data. The results can be used to 
support decision making and planning. Our system includes association 
rule mining, classification, and clustering algorithms. The expected 
result can make significant contributions to academic field and 
industries on grid computing research. This will be helpful to promote 
the advanced development of Taiwan’s information technology and 
value-added applications in the knowledge-based industry. 
 
可利用之產業 
及 
可開發之產品 
 
使用知識管理進行決策支援的產業都可以應用本系統。 
技術特點 
 
在 Grid 的平台上，使用平行計算的技術，對大量的資料能夠快速
的挖掘到隱含知識做為決策支援之用。 
推廣及運用的價值 
 
對學術界與產業界在資料挖掘和網格運算的領域很有價值，對於提
昇我國資訊技術的發展與知識加值將有相當的助益。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
附件一 
問題，進行的很順利圓滿達成任務，我們也聽了其它優秀的論文發表，整體而言可以說是一次
成功的學術研討會。 
本會議的 5個Sessions包括 "Clustering Analysis and Semi-Supervised Learning "、" Theorectic 
Aspects of Data Mining"、"Classification and Supervised Learning"、"Sequential Mining and Text 
Mining" and "Novel Approaches"。3個 Keynote speeches 分別是： 
1. Serge Abiteboul, INRIA Saclay and University Paris-Sud.  
Title: P2P Content Sharing 
2. Ravi Kumar,Yahoo! Research.  
Title: Online Social Networks: Modeling and Mining 
3. Harvey J. Miller,Department of GeographyUniversity of Utah, USA.  
Title: Geographic Theory and Geospatial Knowledge Discovery 
另外，還有 4 個 Tutorials： 
1. Integration of Classification and Pattern Mining: A Discriminative and Frequent Pattern-based 
Approach  
2. Privacy-Preserving Location Services  
3. Sample Selection Bias – Covariate Shift: Problems, Solutions, and Applications  
4. Mining Ubiquitous Data Streams: From Theory to Applications  
 
二、與會心得 
感謝國科會對於參加本次國際學術研討會議的補助，使得個人收獲良多。除了學術專業上
能和其他國家的學者、專家共同探討相關研究課題以外，還可以有機會在其他領域上交流與學
習。更從優秀的論文和高水準的專題演講中發現一些值得進一步探討的題目。不僅如此，也從
幾天的活動和行程中的接觸，瞭解到其他國家在資訊科技方面的發展和推廣應用。尤其歐洲國
家在電腦與通訊的普及化，帶動其他科技和國家經濟的進步。出國參加國際會議，可以看到很
多我們可以學習的地方。其中包括西方文化和社會風俗的認識，透過會議主辦單位的安排參觀
活動，從音樂演奏會和古堡古蹟的介紹，進一步了解當地的歷史和生活背景，也體驗資訊科技
在他們社會的應用情形。 
An Efficient Sequential Pattern Mining Algorithm Based on the 2-Sequence 
Matrix 
 
 
Chia-Ying Hsieh 
Dept. of Information 
Engineering and Computer 
Science, Feng Chia 
University, Taiwan 
sgimoon@soft.iecs.fcu.edu.tw 
Don-Lin Yang 
Dept. of Information 
Engineering and Computer 
Science, Feng Chia 
University, Taiwan 
dlyang@fcu.edu.tw 
 Jungpin Wu 
Dept. of Statistics, Feng 
Chia University, Taiwan 
cwu@fcu.edu.tw
 
 
Abstract 
 
Sequential pattern mining has become more and 
more popular in recent years due to its wide 
applications and the fact that it can find more 
information than association rules. Two famous 
algorithms in sequential pattern mining are AprioriAll 
and PrefixSpan. These two algorithms not only need to 
scan a database or projected-databases many times, 
but also require setting a minimal support threshold to 
prune infrequent data to obtain useful sequential 
patterns efficiently. In addition, they must rescan the 
database if new items or sequences are added. In this 
paper, we propose a novel algorithm called Efficient 
Sequential Pattern Enumeration (ESPE) to solve the 
above problems. In addition, our method can be 
applied in many applications, such as for the itemsets 
appearing at the same time in a sequence. In our 
experiments, we show that the performance of ESPE is 
better than the other two methods using various 
datasets. 
 
Keyword: Sequential pattern, data mining, 
association rule, candidate enumeration, minimum 
support 
 
1. Introduction 
 
Data mining techniques have been developed for 
many researches with wide applications. In recent 
years, sequential pattern mining has become more 
important because it has been used extensively in 
business, bioinformatics, Web mining, computer 
intrusion detection and so on.  
Sequential patterns represent sequential 
relationships between items. They can provide more 
information than association rules. The existing 
methods of sequential pattern mining can be divided 
into two categories: (1) candidate generation-and-test 
approaches, such as AprioriAll [1] and GSP [2], and (2) 
divide-and-conquer approaches, such as PrefixSpan 
[3]. These methods are used to find frequent sequences 
for time-interval [4], fuzzy patterns [5], partial orders 
[6], and so on. As a result of parallel technique 
advances, many researchers have parallelized existing 
methods [7]. However, these approaches have some 
problems. For example, AprioriAll and GSP must scan 
the original databases many times. While PrefixSpan 
only scans a database once, it needs to generate and 
scan multiple projected databases which might be 
larger than the original database. In addition, they all 
need to set a minimal support threshold at the 
beginning of mining processes to prune infrequent 
candidates for the execution time speed-up. If the 
minimal support is changed, they must start afresh. 
Their variations all suffer similar problems. 
Hence, we propose a more efficient sequential 
pattern mining algorithm called Efficient Sequential 
Pattern Enumeration (ESPE) which is based on a 
2-sequence matrix. It has the following characteristics: 
1. Scan the database only once. 
2. Reduce the number of candidate sequences 
being generated that will not be frequent.  
3. Require less memory space. 
4. Can deal with long sequences. 
5. Do not need to set the minimal support in 
advance. 
6. Can effectively find rules based on users’ 
demand. 
7. Can do incremental mining for new items and 
sequences. 
In our approach, ESPE provides complete 
information and finds interesting rules more easily. 
The remainder of the paper is organized as follows: 
Section 2 describes related work and Section 3 gives 
2008 IEEE International Conference on Data Mining Workshops
978-0-7695-3503-6/08 $25.00 © 2008 IEEE
DOI 10.1109/ICDM.Workshops.2008.111
583
.2008.82
again to encode PY. 
(4) Repeat the above three steps until the last 
transaction is processed. 
(5) When users are ready, they can define a 
minimum support, a minimum sequence length or 
any specific sequences they desire. ESPE can easily 
find frequent sequences and generate corresponding 
rules accordingly. 
These five steps involve four techniques: enumerate 
all 2-sequences, store remaining sequences, encode the 
sequences, and specify the user’s demand. We will 
explain them in the next subsections. 
 
4.1. Enumerate all 2-sequences 
 
At the first step, the algorithm reads a transaction 
and enumerates all of its 2-sequences. This procedure 
is similar to FSE’s candidate enumeration [8], but 
ESPE does not need to generate all candidate 
n-sequences at once. It only enumerates all 
2-sequences and that can avoid generating too many 
candidates and wasting memory space. ESPE can 
represent all information necessary for sequential 
pattern mining by simply generating 2-sequences with 
the format of Definition 1. For example, Table 2 shows 
all the 2-sequences generated from Table 1 with our 
enumeration method. By Definition 1, our method only 
generates eight sequences in SID=1: <3 1:2 1 3><3 2:1 
3><3 3><1 2:1 3><1 1:3><1 3><2 1:3><2 3>, and these 
sequences could replace the original sequences. 
However, FSE generates twenty-three sequences: <3 
1><3 2><3 3><1 2><1 1><1 3><2 1><2 3><3 1 2><3 1 
1><3 1 3><3 2 1><3 2 3><1 2 1><1 2 3><1 1 3><2 1 3><3 
1 2 1><3 1 2 3><3 1 1 3><3 2 1 3><1 2 1 3><3 1 2 1 3>. 
Hence, using Definition 1 ESPE can enumerate 
sequences more efficiently and save memory space. 
 
Table 2. Enumeration of all the 2-sequences 
SID sequenceAll of the 2-sequences 
1 3 1 2 1 3 <3 1:2 1 3><3 2:1 3><3 3><1 2:1 3><1 
1:3><1 3><2 1:3><2 3> 
2 1 2 3 2 <1 2:3 2><1 3:2><2 3:2><2 2><3 2> 
3 3 1 2 3 <3 1:2 3><3 2:3><3 3><1 2:3><1 3><2 3>
4 1 2 1 3 1 <1 2:1 3 1><1 1:3 1><1 3:1><2 1:3 1> 
<2 3:1><3 1> 
 
It is obvious that if we enumerate all candidates in a 
sequence, its runtime will grow exponentially. If the 
length of sequence gets longer, it will generate many 
more candidates. Hence, a long sequence may need a 
lot of memory space and cause a short of memory 
space. Moreover, the generated sequences of candidate 
form the shape like an inverted triangle. More 
candidates are generated when they are close to the 
middle of the triangle as shown in Figure 1. In Figure 
1, the length of sequence is 6 and it generates 52 
candidates. The number of 3-sequence candidates is 
the largest and the numbers of other candidates get 
smaller toward both ends at 1-sequence and 
6-sequence. 
 
Figure 1. A list of all candidate sequences for 1 to 
6-sequences 
 
Next, we explain why we enumerate 2-sequence 
instead of 3-sequence or longer sequences. For a 
3-sequence needs to generate ll CC 23 +  (l is the 
length of sequence) candidate sequences, it spends 
more time and memory space than 2-sequence does. 
For example, in Figure 1, there are 15 candidates in the 
2-sequence while 3-sequence generates 32 (15+17) 
candidates. The more candidates being generated, the 
more space will be required. If we choose 3-sequence, 
it needs 32 NN +  (N is the number of items) units 
of memory space to store indexes. When the sequence 
is very long or contains many items, it will generate 
more candidates and require more space too. For 
example, in a sequence database with 6 items, 36 units 
of memory space are allocated for the 2-sequence 
approach while the 3-sequence needs 252 (36+216) 
units of memory space. Since the number of candidates 
and required memory space are accumulative when the 
sequence length is greater than 3, we use 2-sequence in 
our algorithm to save time and space. 
 
4.2. Store Sequences Efficiently 
 
The goal of our method is to store and retrieve data 
quickly and efficiently. Additionally, it needs to do 
incremental mining. Therefore, we use the index to put 
585
distinguish from the item of length-2. For example, for 
the sequence <11113>, we can encode this sequence 
into <1, 1, -3> with two length-2 and one length-1 
where -3 indicates that it is a length-1. In this method 
we don’t need to record all sequences and can save the 
memory space. 
After computing the index, recoding, and encoding 
the sequences for each transaction in Table 1, we 
recode the support value in a similar matrix. The result 
is shown in Table 5. They are represented in the form 
of <X>:S and <Y>:S where X and Y are defined in 
Definition 1 and S is the support of X or Y. Take the 
first entry for example: 11:2 means the support of <1 
1> is 2, <-1>:1 means the support of <1> is 1, <-3>:1 
means the support of <3> is 1, and <7>:1 means the 
support of <3 1> is 1. Here the index of 7 is used to 
stand for <3 1> and save the space as described before. 
Using this matrix, we can provide all information 
specified by user sequence on-demand. 
 
Table 5. The structure of support counters 
 2-Sequence Remaining sequences (PY) 
1 <11>:2 Length=1 <-1>:1  <-3>:1 
Length=2 <7>:1 
2 <12>:4 Length=1 <-1>:2  <-2>:1  
 <-3>:4 
Length=2 <1>:1  <5>:2  
<7>:1  <8>:1 
Length=3 <5,-1>:1 
3 <21>:2 Length=1 <-1>:1  <-3>:1 
Length=2 <7>:1 
4 <22>:1 Length=1  
5 <13>:4 Length=1 <-1>:1  <-2>:1 
6 <23>:4 Length=1 <-1>:1  <-2>:1 
7 <31>:3 Length=1 <-1>:1  <-2>:2  
<-3>:2 
Length=2 <2>:1  <6>:2  
<5>:1 
Length=3 <2,-3>:1 
8 <32>:3 Length=1 <-1>:1  <-3>:2 
Length=2 <5>:1 
9 <33>:2 Length=1  
 
4.4. User sequence on-demand 
 
One major feature of ESPE algorithm is that it does 
not need to define any threshold. Therefore, ESPE 
stores all possible rules and a user can set parameters 
based on their own demands to find interesting rules. 
ESPE allows users to make specific queries on the 
support threshold, the length of sequence or some 
specific sequences. Then, ESPE can find the 
information efficiently. For example, if a support 
threshold is given, we only need to check the support 
counter of 2-sequence. If the support counter of 
2-sequence satisfies with the minimum support 
threshold, then we check the encoded PY.  
Moreover, if the user sets the minimum length of 
sequence or asks for specific sequences, our method 
can show the result immediately. For example, in 
Table 5, if the minimum support is 3, then the 
algorithm only needs to check the entries 2, 5, 6, 7, and 
8 in the encoded PY. At the entry 2, there is only <-3>:4 
satisfying the minimum support threshold. Hence, we 
do not need to check the sequence whose length is 
longer than 1. If the minimum length of sequence is 4, 
we only query encoded PY with length-2. Besides, if a 
user wants to focus on any specific sequence such as 
<1 2> then we only need to check the second entry. All 
parameters can be set at the same time. 
 
5. Extension to our method  
 
In general, the sequences in a database are not only 
ordering items but also items that may appear together. 
Our method can deal with this kind of application as 
well. We take the itemsets which appear at the same 
time as an element, and encode each element. 
To extend our method, most of the previous 
processes are the same except some differences in 
index computation and storage structure. The first step 
transforms the data. The next step is the same as ESPE, 
but it needs another method to compute the index when 
items appear at the same time. Then, it must build a 
different table to store the sequence in the final step. 
 
5.1. Transform Sequences 
 
We use a different form to represent the items 
which appear at the same time. In a transaction, if the 
sequence is <2 (3 2) 1>, then (3 2) and (2 3) can be 
considered as the same because 3 and 2 appear at the 
same time regardless of their order. Hence, we can not 
use Lemma 1 to calculate the index any more. The 
characteristics of these data have the relationship like a 
right triangle as shown in Figure 3. We use Lemma 2 to 
compute their indexes and use a prefix of “*” to 
distinguish them from the indexes of Lemma 1. 
 
 
Figure 3. A list of 2-itemsets 
587
many projected-databases. This means our algorithm is 
not limited by different minimal support thresholds 
especially when they are small. 
 
C100S1.5T10N0.05
0
2
4
6
8
10
12
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Support(%)
Ru
nti
me
(se
c)
ESPE
PrefixSpan
 
Figure 4. Scalability test for various min_sup 
thresholds 
 
The next experiment displays the scalability test of 
different length of customer transactions. We use 
C10T1I0.02 datasets with the minimum support=5% 
and the average number of transactions per customer (S) 
ranges from 10 to 40. Figure 5 shows that ESPE is 
better than PrefixSpan when S increases. As the length 
of customer transaction increases PrefixSpan needs to 
generate a lot of projected-databases and scan them 
many times. Therefore, PrefixSpan is unfavorable to 
longer length of customer transaction. In addition, it 
shows the performance of our algorithm grows linearly 
when the length of S increases. 
 
C10S10-40T1N0.02 Support 5%
0
5
10
15
20
25
30
35
40
45
50
10 15 20 25 30 35 40
Average transaction per customer
Ru
nti
me
(se
c)
ESPE
PrefixSpan
 
Figure 5. Scalability test for various transactions per 
customer 
 
Our algorithm is not restricted by the min_sup, 
the sequence length, and the number of items. We use 
C100S5T1 datasets with a support of 1% and set the 
number of items N between 10 and 100 to run the 
scalability test. From the result in Figure 6, when the 
number of items increases, the runtime of ESPE is still 
better than PrefixSpan. 
 
C100S5T1N0.01-0.1
0
20
40
60
80
100
120
140
160
10 25 50 75 100
Number of items
Ru
nti
me
(se
c
ESPE
PrefixSpan
Figure 6. Scalability test for various numbers of items 
 
T10I4D0.1KN25
0
1
2
3
4
5
6
7
8
9
10
11
1 2 3 4 5Support(%)
Ru
nti
me
(se
c)
FSE
ESPE
 
Figure 7. Compare ESPE with FSE for various 
supports  
 
Our method can not only deal with sequential 
patterns but also association rules. Hence, ESPE can 
compete with other algorithms of association rule. We 
use the dataset of association rule T10I4D0.1KN25 
with varying supports to compare ESPE with FSE in 
Figure 7. Our algorithm is better than FSE 9% to 10%. 
This is because FSE needs to generate all candidates 
and wastes a lot of time. 
Figure 8 is the result of the scalability test for 
various transaction numbers between ESPE and FSE. 
Using T10I4DN25 datasets with D ranging from 0.1K 
to 1K, we show that ESPE is 10% to 43% better than 
FSE. 
589
