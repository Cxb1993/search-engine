hand, the head craftsmanship of advanced stage confronts the
research bottleneck of the fabrication difficulties. This is the
primary motivation and scenario of the research work of the
proposed sub-project. Figure 1 shows the history of the
industrial robot and the intelligent robot. In figure 1(a)
describes a simplified survey of the most advanced robot
heads around the world. Figure 1(b) illustrates the annual
reports on the robot development of the output-value, annual
predication, and market occupation in the years of
2005~2015 from Ministry of Economic Affair, R.O.C.
Figure 1(a) shows annual achievement on the self-fabricated
robot head named by “Hwa-Hsai Robot “of the sub-project 6
in 2008, which include the side-view, front-view of the
self-fabricated robot head and the virtual-reality simulation
of the proposed humanoid robot [10,11].
The sub-project 6 is dedicated to self-fabricate a head skull
on the viewpoint of ryodoraku craftsmanship instead of
traditionally engineering viewpoint. Many institutes have
developed robot heads using the RC-servomotor and
single-chip controllers. However, they are still obscure in the
craftsmanship of the dynamic expressions with realistic
scenarios. Knowing that the proposed sub-project 6 is
focused on the technique integration of the research
teamwork, the anticipate technique development will
consists of mechatronic analysis, communication techniques
of facial emotions, and mechanical designs of skull
mechanism. In Fig. 1, the most advanced robot heads around
the world are shown and will be investigated in the next page:
While, a simplified classification on the different stages of
the humanoid-head technology is illustrated in Table 1.
Detailed investigation of the humanoid communications on
the verbal and nonverbal expressions has found that nearly
60% of them are nonverbal.
How to describe the facial expressions of the robot head
closed to the biological features of human becomes a
challenging work in the development of the humanoid robots.
Generally speaking, the topics of the robot head can be
categorized into four different realms, i.e., actuator drivers,
facial/skull mechanics, the artificial skin, and facial
dynamics. In this sub-project, most head architecture will be
developed by itself independently. While, the facial theories
of the artificial skin, the facial/skull mechanics, and actuator
drivers will be technique integrated with the 1st, the 2nd, and
the 3rd sub-projects, respectively. In other words, the issue of
facial dynamics will be enacted independently in the sixth
sub-project. Thus, this sub-project will be focused on the
theoretical derivation, computer simulations, experiments,
and realistic implementation of the facial dynamics on a
self-fabricated android robot head in the laboratory. What
makes and how to make a robot head appear human-like
scenario? It is mentioned in researches that the presence of
features are the total number of features, and the dimensions
of the head. The infrastructure of the robot skull includes
three divisions, i.e., the front skull, the rear skull, and the jaw
mechanism. The front skull consists of the facial skeleton and
artificial organs, such as eyeballs, eyebrows, and eyelids.
The degrees of freedom on the robot head are prearranged by
a series of artificial intelligent, AI-class servomotors and a
8-bit single-chip controller with feedback signals, i.e.,
CM-5/12V DC，ATMega128, 16Mhz with torque 12kgf.cm
/7V and 17.1 kgf.cm /9.6V. As illustrated in Table 1, the
eyeballs and the robot jaw are actuated using the mini-sized
pneumatic cylinders with 2.5mm outside diameter and
5.2mm piston stokes.
Table 1 Actuators’ degrees of freedom on the robot skull
Organs Actuators Controller DOF Model
Eyeballs AI-servomotor Single-chip 4 120 degrees
Eyelids AI-servomotor Single-chip 2 3 sections
Eyebrows Pneumatic Single-chip 4 4 sections
Robot jaw Pneumatic Single-chip 1 6 sections
Robot Neck DC-servomotor Single-chip 2 3 sections
The well-developed module of the artificial eyes consists of a
tunable lens, a tunable pupil, and a CMOS image sensor
array, which can execute signal processing, image
recognition, and image tracking on an embedded computer as
a feedback controller. This system is more advanced than
state-of-the-art camera sensors on humanoid robots.
Fig. 1. The infrastructure of the robot skull. (a) global space of the robot
eyeballs, (b) robot facial mechanisms, (c) self-handcrafted FRP skull and
silicon-gel skin, (d) robot skull framework.
Table 2. Electrical conductivity of the ryodoraku meridians
Ryodoraku
meridians
Facial
Position
Current
（mA）
Voltage
(volts)
Torque
(kgf-cm)
Lung & vas Inner face 10-50 2.0-4.5 1.5-4.0
Intestine Outer face 5-20 0.5-2.1 1.5-4.0
Lymph & liver Forehead 40-80 2.5-4.2 2.0-5.0
Kidneys Eye coners 40-80 2.5-4.2 2.0-5.0
Stomach Eyebrow ends 5.0-10 3.0-4.6 3.0-6.0
Bladders Mouth coners 5.0-10 3.0-4.6 3.0-6.0
The electrical conductivity arranged according to the
ryodoraku meridians beneath the artificial skin are illustrated
in Table 2, which shows six ryodoraku meridians with
different electrical and mechanical features. The fuzzy rules
of the facial dynamics along the ryodoraku meridians are
defined as follows：
Rule #1：Robot skull is automatically transfer to trigger
facial actuator if BA enters the upper red region
Rule #2：Electrode is negative high if BA is in the upper
yellow region
Rule #3：Eectrode is negative low if BA is in the upper
green region
Rule #4：Electrode is off if BA is in the white region
Rule #5：Electrode is positive low if BA is in the lower
green region
Rule #6：Electrode is negative high if BA is in the lower
yellow region
Rule #7：Robot skull is automatically turn off the facial
actuator if BA enters the lower red region.
The total degrees of freedom of the proposed scheme are
12 DOF, which include symmetric actuators along lung &
vas, Intestines, lymph & liver, kidneys, stomach, and
bladders. More delicate expressions can be achieved if the
number of the ryodoraku meridians beneath the artificial skin
is doubled to the suggested number listed in the Table 2. It is
learned that the thickness and the electrical conductivity of
facial films are different and schemed according to the
ryodoraku analysis discussed in this section. In Fig. 3,
topological coordinates of three basic emotions are
constructed using the teachable DC servo-actuators and the
proposed single-chip controller, which include readable and
analytical emotional modules of happy, angry and sadness
emotions. The proposed AI single-chip would record the
real-time positions of the emotional module step by step.
Consequently, seven basic emotional modules are well
established and recorded in the PC-based computer on
WLAN and Ethernet, as shown in figure 4. The emotional
modules are constructed and recorded in the host computer
on the topological space. Note that there are only a few
emotional modules schemed in the example, i.e., happy
module, sad module, and angry module owing to their simple
characteristics. Thus, the skull fabrication is not only an
engineering problem but also a craftsmanship problem. In the
next section, figure 4 is shown with an example of the tensor
analysis and the emotional problem on the topological
viewpoint.
IV. TENSOR INVARIANCE OF THE FACIAL DYNAMICS
The proposed seven basic artificial emotions on a
self-handcrafted robot head are prearranged by a series of
artificial intelligent, AI-class motors and controller with
feedback signals, i.e., CM-5/12V DC，ATMega128, 8-bit
16Mhz with torque 12kgf.cm /7V and 17.1 kgf.cm /9.6V .
The proposed artificial skin will be schemed to include
additional 10-DOF on the ryodoraku meridians under the
artificial skin.
Reasonable gateways are routes to imitate human existing
facial behaviors by assumption of elastic materials with
elasticity and poisson’s ratio. Implementation of the 
techniques of the emotional interpolation and the
comforming mapping, enomous artificial emotions can be
created in the long run. In Table 3, the initial states of facial
emotions can be transformd or mapped into an artificially
created emotion by different ways of emotion combinations
on the Euclidean space. For instance, a 2D tensor invariant
case is shown in Fig. 4, which illustrates the principle strains
of the facial dynamics rotate according to the equilibrium
directions of Mohr’s circle.
Fig. 3. Facial topology of the dynamic emotions. (a) three basic
modules of the composite emotions, (b) Neural fuzzy network of the
robot facial emotions on the viewpoint of topology.
V. SIMULATIONS AND EXPERIMENTS
A series of emotional photographs of the head model
are then taken and analyzed using the POSER-6.0 software.
While, the well-setting 3D camera with DAVID freeware
software and laser-range scanning are used to grip the
scenario of the different facial emotions of the head model,
which will be applied to make comparison with POSER-6.0
images. The coding sequences of the proposed AI CM-5
ATMega single-chip controller with AX-12 servo motors are
also well-arranged in Table 4, which include the
arrangements and binary codes of the eyeball, the eyelids, the
mouth, and the cheek of the robot head.
Applying of the EMG detector with the detection of
energy indices (mA), the emotional trend curves can be
plotted with respect to time in seconds. The module
procedure of the facial dynamics is carefully illustrated in
figure 5(a). In this example, happy sequence module and
angry sequence module include 4 sets of basic emotion
elements, respectively. Then, a 40-picture continuous video
with 20piece/sec speed can be reconstructed and named
happy module, which will be produced and played in two
seconds if necessary. Similarly, the angry module can also be
produced and replayed in 2 seconds if necessary.
In such ways, the prescribed movie with delicately facial
dynamics can be produced according to the emotional
interpolation between two different modules. In this example,
the simulation result of happy module and angry module are
plotted using blue lines and green lines along the upper side
and down side, respectively. Consequently, the composite
events are mixed from 65% happy module and 35% angry
module, which is illustrated in figure 5(b). Finally, it can be
concluded that the ryodoraku craftsmanship approach has
successfully created the emotional scenarios to satisfy most
of the circumstances we may encounter in our daily life.
VI. CONCLUSION
In this subproject, the tensor invariant concept of
ryodoraku craftsmanship is successfully applied to simplify
the force exerting conditions along the trajectories with good
conductivity. Besides, advanced artificial emotions can be
achieved using the self-fabricated head skull and ryodoraku
craftsmanship with the silicon-based material. The
traditional engineering viewpoint of the facial emotions is
also replaced by the ryodoraku craftsmanship viewpoint.
Simulations and the ryodoraku experiments with the data
acquisition are made and successfully demonstrated the
advantages of the proposed innovation. However, there still
exists much work to do owing to unsatisfactory mini-sized
actuators beneath the facial skin on robot skulls. Future
works of this research will be continued and focused on the
improvement of facial materials and the 3D experiments of
more advanced ryodoraku craftsmanship.
ACKNOWLEDGMENT
Financial support for this work is provided by the
Ministry of Economic Affairs, Taiwan, R.O.C., under
the contract NSC99-2221-E-146-007.
REFERENCES
1. Y. Aoki and S. Hashimoto, “Adaptive Head Modeling
System and Its Applications,” Proceedings of ICSP 2000,
pp.1237-1244 , 2000.
2. K. Bern and T. Beaum, “Desig Concept of Human-like
Robot Head,” Proceedings of 2005h IEEE-RAS
International Conference on Humanoid Robots, , 2005.
3. H, Bie, Q. Huang, and W. Zhang, etc., “Visual Tracking 
of a Moving Object of a Robot Head with 3 DOF,” 
Proceedings of 2003h IEEE International Conference on
Robotics, Intelligent Systems and Signal Processing,
Changsha, China, Oct., pp.686-691, 2003.
4. F. Bourel, C Chibelushi, and A. Low, “Robust Facial 
Expression Recognition Using a State-based Model of
Spatial-localized Facial Dynamics,” Proceedings of Fifth
IEEE International Conference on Face and Gesture
Recognition, Changsha, China, Oct., pp.686-691, 2002.
5. L. Gralewski, N. Campbell, and I. Penton-Voak, “Using a 
Tensor Framework for the Analysis of Facial Dynamics,” 
Proceedings of the 7th IEEE International Conference on
Automatic Face and Gesture Recognition, , 2006.
Fig. 5. Dynamic simulations of the facial emotions.on the viewpoint of
topology (a) basic emotional expressions, (b) the composite emotional
expressions.
國科會補助計畫衍生研發成果推廣資料表
日期:2011/07/06
國科會補助計畫
計畫名稱: 子計畫六：人形機器人顱顏仿生工藝之技術發展
計畫主持人: 汪清國
計畫編號: 99-2221-E-146-007- 學門領域: 機器人學及應用 
研發成果名稱
(中文) 機器人情緒之中醫經絡仿生技術
(英文) Chinese-medicine-based Meridian Biomimic Technique of the Robot Emotion 
成果歸屬機構
華夏技術學院 發明人
(創作人)
汪清國,黃漢邦
技術說明
(中文) 本發明係為一種機器人情緒之中醫經絡仿生技術，其包含一機械頭部模組、一擬
真臉部模組、複數個致動器及一控制模組，係將該些致動器仿照人的臉部之經絡
穴位設於機械頭部模組，並將該些致動器連接控制模組，再將擬真臉部模組覆蓋
於機械頭部模組，以及與該些致動器連接。本發明之目的係利用控制模組驅動該
些致動器於機械頭部模組作動，而使擬真臉部產生一臉部變化，以達到機器人臉
部可模仿人的臉部之情緒表情之功效。其中，臉部變化包含人的喜、怒、哀及樂
之情緒表情。
(英文) Delicate variations of the facial emotions can dramatically spiritualize the humanoid 
robot with passionate scenarios. While, the robot head and facial craftsmanship can be 
categorized as three stages i.e., the preliminary stage, the secondary stage, and the 
advanced stage. Most of developing head issue are dedicated to the basic facial stages, 
which aimed to facial control and inventive mechanism. However, some of the advanced 
facial mechanisms may face the craftsmanship bottleneck of the facial emotions. The 
proposed robot head includes 24DOF, which are artificial eyeball (2DOF), artificial 
eyelid (2DOF), artificial eyebrow (4DOF), artificial mouth (4DOF), artificial neck 
(2DOF), artificial jaw (2DOF), and artificial facial expression (8DOF). The topological 
analysis are also simulated by applying the ADAMS software and silicon-gel artificial 
skin. 
產業別 醫療器材製造業
技術/產品應用範圍 機器人頭部模組之中醫經絡情緒仿生技術
技術移轉可行性及
預期效益
機器人頭部模組之中醫經絡情緒仿生技術
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
2011 年度應邀擔任 ICMSE 2011 (International Conference on Manufacturing 
Science and Engineering) 國際研討會，Advanced Manufacturing Systems
分項會議主席(Session Chair)。(2011 年 4 月 9-11 日，中國桂林)。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
