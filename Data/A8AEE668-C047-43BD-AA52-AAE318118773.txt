中、英文摘要及關鍵詞 
在本計畫目前的進度中，我們提出了利用區域式線性子空間表示法來表式物體(包含人臉或
其他物體)的特徵，並利用這種表示法結合了粒子濾波器(Particle Filter)來追蹤各種物
體和人臉。由於被追蹤的物體被分割成數個區塊，所以在追蹤時若有部份區域被遮蔽或因
為其他因素而導致影像失真或被破壞時，本方法都能有效抵抗，而取得更理想的追蹤效果。
此外，爲了因應物體移動時的外觀變化，原本所建立的線性子空間表示都必頇被適當且即
時地更新才能更正確地表示這些物體。因此，我們也提出了即時更新線性子空間的表示法。
經過我們的初步實驗，結果顯示我們所提之方法在追蹤過程中確實能達到抵抗部分遮蔽以
及因為物體的運動所導致之外觀變化。基於此線性子空間表示法在追蹤人臉上有不錯的效
果。 
在第二年度的計畫中，我們在人臉建模和表情合成上也有新的研究成果。近年來人臉
表情合成技術的應用愈來愈廣泛且多元，如虛擬演員、虛擬主播、虛擬教師或是應用於視
訊對話的虛擬人物等。本論文提出一低成本、直覺且即時的表情合成系統。操作本系統時，
使用者只需對著廉價視訊攝影機作出不同的表情，系統即可即時追蹤人臉五官並合成各種
不同人物的對應表情。 
本系統的主要核心技術是主動式外觀模型(Active Appearance Models，AAM)，這是用
於物件追蹤上很有效的一個解決方案。藉由 AAM 追蹤到的人臉表情，可得到對應的臉部幾
何參數和紋理參數。藉由這些參數我們建立出適當的形變轉換關係並藉此關係來換算出不
同使用者的對應表情參數，而後再以此參數合成出不同使用者的表情。系統運作時主要分
為兩階段：模型訓練階段與追蹤/合成階段，模型建立階段屬於離線式的操作，包含樣本收
集、建立追蹤及合成的 AAM 模型等。追蹤/合成階段則屬於即時的線上操作階段，過程中包
含了 AAM臉部表情得迭代擬合、表情合成參數轉換以及合成目的使用者的表情。 
經過實驗結果驗證，本文所提之方法確實可以達到即時追蹤與表情合成的效能，而且
合成出來的表情也頗為生動逼真，對於未來在實務上的應用極具潛力。 
 
For the current progress of the project, we have proposed a region-based linear subspace 
representation for tracked objects (including faces and other non-face objects) and combined this 
linear subspace representation with particle filter techniques for object tracking. As each tracked 
object is partitioned into several regions and each region is independently represented by the 
proposed linear subspace method, the partial occlusion and local appearance variation occurred 
on the object during tracking can be well handled. In addition, to adapt to the gradual appearance 
change during motion, the built linear subspace must be properly updated during tracking. Hence, 
we also proposed an incremental updating of the linear subspace. According to our experimental 
第1章簡介 
1.1研究動機與目的 
   隨著民風的轉變人們不再只是一味的工作而忽略生活的其他要素，許多領域也逐漸
的蓬勃發展比如:教育與娛樂。在這股蓬勃發展風潮中，屬於多媒體應用的電腦視覺與電腦
繪圖，有什麼可以共同創造未來的方向呢? 仿真人臉的運用即擁有很好的願景。不論是教
育、醫學、娛樂、傳播及其他領域都有仿真人臉可以涉足的地方。Agent提供一個新型態的
人機介面，透過Agent人們可以輕鬆的查詢資訊、學習。Virtual idol 提供人們另一種形
式的寄託。電影或遊戲中的仿真人臉能使人擁有更身歷其境的感覺，美容醫學上可以幫助
受術者有更明確的認知，運用於視訊會議時能更讓與會者有參與感。 
想建立栩栩如生的臉部動畫在電腦繪圖學中依然是一個非常有挑戰性的問題。原因在
於3D人臉建模不容易，3D建模需要有一些專業背景，不像2D影像一樣，只要有數位相機或
攝影機就萬事搞定。再者，控制人臉表情的肌肉模型相當複雜，也需要一點生理學的知識
背景。因此，如果能開發出一套智慧型的3D建模技術，使得讓一般人做3D 建模就像拍照片
一樣簡單，那麼就更能讓3D電腦繪圖的應用更具有普及的潛力。鑑於此項動機，本計畫乃
以研發出高效率的三維人臉建模技術和追求擬真的人臉動畫合成技術為研究目標，期望以
智慧型的電腦視覺技術來提高建模和動畫合成的自動化程度。在電腦視覺的技術上，從2D
人臉影像建構3D人臉模型的技術和2D/3D人臉動畫合成的技術都需仰賴於穩定快速地抽取
和追蹤人臉上的一些重要特徵點諸如人臉五官、輪廓等等，所以人臉重要特徵點或五官分
元（facial components）的抽取與追蹤便成為3D人臉建模與動畫合成的重要核心技術。所
以，本研究計畫之內容朝向以下三項方向開發： 
一、人臉五官特徵偵測與追蹤技術(第一年工作重點)：偵測人臉五官特徵的目的主要
有二，其一為提供利用電腦視覺作從多角度人臉的視訊畫面做3D建模3D Reconstruction）
時所需的對應點（corresponding points）資訊，以往控制點多是由使用者以人工的方式
指定，程序頗為耗時費力，也因而降低了建模的效率，也增加了不少人工成本。如果能夠
開發一套低成本的軟體演算法使其以自動且穩定的方式來抽取和追蹤重要的特徵點作為對
應點，再配合電腦視覺中的3D重建技術進行3D自動建模，必然可以大幅減低人工的介入，
進而提高產能與降低成本。追蹤五官特徵的第二個目的則在於進行視訊畫面中的人臉表情
分析，進而達到以2D視訊畫面來驅動3D表情動畫的自動合成之目的。 
  二、植基於電腦視覺之人臉建模技術(第二年工作重點)：我們擬利用電腦視覺技術進
行3D 建模，主要訴求在直接利用多角度的2D 影像來建構出3D人臉模型。如此可以降低一
般使用者建模上的困難度和對美工造型專業背景的需求。雖然市面上也已有藉助硬體（如
3D 雷射掃瞄、3D相機、3Ddigitizer）的方式來嘗詴取代軟體化的塑模方式，但是這些硬
體設備價格高昂，少則數十萬，多則百萬以上，實非一般人所能負擔，因此在此技術的開
發上，我們希望所開發的技術不需使用過於高價的硬體設備，同時也要盡量將人工的介入
減到最少。 
  三、人臉表情動畫合成技術(第二年工作重點)：表情動畫的合成可說是人臉應用中的
一個既根本又重要的基礎技術。人臉的表情就生理學的觀點而言相當複雜，因為牽涉到人
臉肌肉結構和運動的模型。為了減少動畫設計師對這些複雜的知識背景需求，我們擬設計
出一套人臉模型的參數化表示方式，然後利用這樣的參數化表示方式配合電腦視覺的技術
(N)，每一個新的樣本都可視為下個畫面中物件可能的目標區域，這些樣本就是所謂的「粒
子」(Particle)。有了新樣本的集合後，在下一個畫面中，每一個樣本所定義的可能目標
區域所對應的影像都會經過適當的幾何轉換以及特徵抽取，然後與預先儲存的目標樣版進
行比對，比對後，每個樣本目標區域即可得到一個與樣版之間的相似度大小值。取得每個
樣本目標區域與目標樣版的相似度之後，每一個樣版即可根據其相似度計算出一個權重，
這些樣本的權重會依線性調整的方式使其總和為 1。各樣本有了各自的比重，就可以用加
權和（Weighted Sum）的方式，利用個樣本的位置和權重的加權和算出一個最後的目標區
域，而此算出的目標區域就是目前這個畫面的新目標位置。 
 
人臉表情合成： 
人臉表情合成的技術可區分為二維(2-D)模型和三維(3-D)模型的合成，大部分的方法
都著重在三維模型。利用三維模型的人臉表情合成都需要先有一個三維的人臉模型，然後
再設法去調整此三維模型來合成不同的表情。要取得這種三維模型有手動的方法和自動的
方法。手動的方法就是利用三維模型設計軟體(例如 3D Max Studio、MAYA)來完成，此法
若由頂尖的繪製人才來描繪，所呈現出來的效果會非常精細，視覺效果也會很好，如常見
的 3D動畫電影等。但手作製作的成本可能較高、較費時，而且繪製人員不但要事先經過學
習、訓練，可能還需要足夠的藝術天分，才能呈現出最佳的效果。 
自動建構三維人臉模型的方法中第一種方法就是由硬體取得。所謂硬體取得是指使用
三維模型掃描設備(例如臉部三維雷射掃瞄)直接對目標物掃描[22][23][24]，掃描完成後
就自動產生三維模型(如一些古文物的三維模型重建或人臉、骷髏等)，同時再以拍攝或掃
瞄方式得到表面紋理(texture)，將表面紋理依對應的頂點貼到三維臉部結構上，因此需事
先定義一些三維結構與表面紋理之間的對應點。這種作法快速有效，但缺點是硬體設備價
高昂，掃瞄的範圍也有所限制，所以實用性並不廣泛。 
另一種自動建構三維人臉模型的方法使以軟體演算法來取得。主要的作法是結合立體
電腦視覺技術，利用單一或多重視角的二維影像來重建(Reconstruct)或擬合(Fit)出三維
模型[25][26][27]。藉由多角度拍攝人臉，並由自動或半自動偵測的特徵點(如眼角、嘴角
及輪廓等)間的對應取得三維資訊以重建出三維臉部模型。此方法中又可分為多攝影機及單
一攝影機，兩者在技術上稍有差異，一般來說單一攝影機由於是由連續的視訊畫面中目標
物的移動來估測三維資訊，因此也可稱為 SFM (Structure from Motion)方法。而多攝影
機則是藉由多個角度同時拍攝目標物，再以立體電腦視覺的三維重建演算法來取得三維資
訊。Pighin[25]等人利用多角度的照片，並定義對應的特徵點，再由這些手動取得的特徵
點去擬合一個通用的三維人臉模型(Generic 3-D Face Geometry)，最後再將對應的紋理貼
到模型上。如何從多張照片中挑選對應的紋理到模型上，也是其探討的重點之一。 
在人臉表情合成的部分，MPEG制定了 MPEG-4的規格，而在此規格中也包含了人臉繪
製相關的制定，如 FACS(Facial Action Coding System)中使用了一系列的參數來描述臉
部的表情參數，其根據了人臉的解剖學特點制定，劃分了數個運用單元。FAP(Facial 
Animation Parameters)則與 FACS相似，以肌肉組織的運動來制定臉部基本的運動參數。 
Muscle-based的模型[28][29][30]即為 FAP 的一種表現。在肌肉模型中，肌肉受參數
的控制而收縮，模型表面的控制點隨著相關的肌肉拉扯受力而變化，肌肉間可能相互獨立，
 
圖 2-1合成波及其基本波形圖。 
 
2.2主成份分析(Principal Component analysis) 
主成份分析(PCA)常被用來尋找一個資料集合的基底向量，圖 2-2為一組散佈於二維帄
面上的資料集合，其中紅色點代表資料點、綠色點為資料的帄均值、黑色長短兩軸為PCA
所求取之基底向量。 
 
圖 2-2 散佈於二維空間中的資料，及 PCA所求取的兩基底向量。 
 
PCA的運作如下：假設有一組大小為n的資料集合
1 2{ , , , }nX x x x ，而資料點
1 2{ , ,..., }m Ti i i ix x xx
為m維的向量。 
步驟一，求取資料的共同性，亦即資料的帄均值μ： 
1
1 n
k
kn 
 μ x  2-2 
步驟二，利用扣除均值以求取資料的異質性： 
j j y x μ 2-3 
步驟三，求取共變異數矩陣（covariance matrix），相異特徵的共變異數矩陣： 
1
1
( )( )
n
T
i i
in 
  C x μ x μ  2-4 
步驟四，找出C的n個特徵值及其對應的n個特徵向量，依特徵值由大到小排列 
特徵向量，特徵值越小其對應的特徵向量重要性越低： 
1 2{ , ,..., }ne e ee  2-5 
Tnnnnnnnn
T
nnnnnnnnnn
T
nnnnnn
T
nnnnnn
nnnnnnnn
UUC
UUIC
UUUUC
UUC








 
 
 
2-11 
 
接下來，我們假設只取最大p個特徵值，所對應的特徵向量，於是我們將上式(2-11)，等式
右方進行分解如下： 
         T nnnnnnnn UUA    
   
T
pnpppn
T
pnnpnpnpnn
T
pnpppn
T
pnnpn
pnpnppn
pnmpp
pnnmn
UU
UUUU
UUUU

















)())(()(
)(
)()()(
)(
)( 0
0
 
 
 
 
 
 
 
2-12 
 
上式，因為我們只取p個，其他較小特徵值我們假設其為 0)()(   pnpn ， 
將其捨去，因此可以得到(2-12)式。 
。  
圖 3-2模版子影像分割示意圖 
 
3.2.2樣版模型更新 
3.2.2.1更新判斷 
當追蹤過程中因為物件受到一些因素，造成原始樣版模型不適繼續用來追蹤。因此為
了讓追蹤過程順利，必頇因應物件各種形狀變化與環境變化，隨時讓物件模版保持最新狀
態，以防止追蹤過程發生目標物遺失情形。本研究利用樣版模型與新追蹤子影像之間的顏
色分布機率差異性，來判斷樣本是否需要跟新。首先影像進行灰階統計以取得影像柱狀圖
(histogram)h，h( )i 為灰階值為i像素數除上全圖像素總數。接下來將原先256階(bin)量化
為64階(bin)，每四個相鄰灰階值為同階，於是得到
0....63{ }
u
ub  統計圖，進行量化的目的為減
少光照明度影響。 
(4 ) (4 1) (4 2) (4 3)ub h u h u h u h u        (3-1) 
令 u
Tb 為樣板模型的統計圖，而
u
Cb 為目前擷取子影像的統計圖。利用下式(3-2)可算出物件
影像與模版的柱狀圖的相似度(Bhattacharyya coefficient)： 
63
0
[ , ] u uT C T C
u
b b b b

  (3-2) 
如果直接使用相似度作為判斷標準將會不好取捨，於是對相似值取對數 
log( [ , ])T Cb b   (3-3) 
τ為一個門檻值(threshold)用以判對是否要更新模版影像與更新子空間基底(特徵向
量)，假如小於等於表示目前追蹤結果與模版影像即為相似，變化量不大所以必頇要更新，
大於門檻值表示變化量大或被遮蔽，則不需要更新，此法最主避免當物件遮蔽時還做即時
更新，造成模版影像結果不對，但是也造成在追蹤形變物件時會有物件遺失的情形。 
3.2.2.2 更新方法 
當判斷式(3-3)成立時，利用 Incremental PCA 來進行線性子空間的更新。下表為
Incremental PCA的虛擬碼： 
 
表格 3-1 Incremental PCA 虛擬碼 
原圖 
1x1 2x2 3x3 
圖 3-3所示，其計算的方法如(3-4)。 
 
圖 3-3預測下一個頁框物件位置示意圖 
3.3.2目標物位置估測 
在進行物件追蹤之前需先定義物件，為簡化追蹤的複雜度我們使用矩形來描述物件。
因而一個物件可用四個參數{x,y,Sx,Sy}來定義。其中(x,y)代表物件在二維標系統中的位
置，(Sx, Sy)表示物件的寬及高。 
 
圖 3-4 物件標定。 
在進行物件追蹤時先利用3.3.1節所提預測模組取得初始位置，待取得初始位置後，
以預測位置為中心的特定區域內進行粒子追蹤。使用粒子濾波器時需要散佈大數量的粒子
來達到足量的抽樣，雖然大量的粒子可以達到高的準確度，但龐大的計算量也使得系統拖
累損失其即時性。為同時兼顧速度與準確度，本系統採用二階段估測策略：在第一次大範
圍搜尋後，利用較優良的粒子進行第二階段小範圍搜尋。圖 3-5(a) 所示為本系統簡易的
追蹤運作過程，圖中的圓代表影像中的像素點：第一階段的物件估測以預測點(圖中紅點)
為中心，在其範圍5x5的區域中的各位置上進行灑點，如圖 3-5(b)所示：依序對物體的Sx
及Sy各進行三個不同比例的縮放，因此第一階段共產生255(25x3x3)顆粒子。接下來將各個
粒子分割成個2X2個子區域(如3.2.1節中所述)並各自挑選兩個PCA重建誤差最小的子區域
代表該例子，最後沿Sx變異利用PCA重建誤差及追蹤影像誤差(見錯誤! 找不到參照來源。
節)各挑選10個誤差最小的粒子，共可挑選出共30個粒子(三個Sx變異各10個粒子)作為第二
階段估測的初始。第二階段重複第一階段不過搜尋的區域減小為3x3，最後也只挑選最佳的
粒子當作估測的結果。 
 
1tP
tP
1tP
'
1tP
1tV
tV
Sampling region 
 
(x, y) 
Sx 
Sy 
If( 

))((minarg)(
}4,3,2,1{
i
i
k RPCAErrRPCAErr )and 

))((minarg)(
}4,3,2,1{
i
i
k RRawErrRRawErr ) 
(3-10) 
 
 圖 4-2以帄均形狀及紋理和基底形狀和紋理重建合成圖[39] 
主動式外觀模型僅描述物件的外形及紋理，在應用於物件追蹤時需先定義出目標函式
(object function)並搭配最佳化演算法(optimal algorithm)來求解。ICAAM[39]定義的目
標函數如下： 
0
2
0
1
( ) ( ) ( ( ( ; ); ))
m
i i
x s i
A x A x I N W x p q
 
 
  
 
   (4-1) 
其中 x 表示落於帄均形狀
0S 中的 2D座標點，而其紋理資訊可由 ( )I x 表示。物件的運動包含
區域變異 ( ; )W x p 及全域運動 ( ; )N x q ，其中 p 及q各代表區域變異及全域運動模型的參數。
物件的紋理可由事先訓練的帄均紋理
0A 及紋理基底向量 1~mA 線性組合而成，此線性組合的
權重為
1~m 。此目標函數欲求取最佳的 1~m 、 p 及q使得函數值為 0。ICAAM 使用逆向組構
演算法(inverse compositional algorithm)來最小化此目標函數。逆向組構法藉由預先計
算，省去迭代擬合的過程中重複計算。ICAAM演算法的流程可分為三個階段：預計算
(pre-computation)、迭代擬合(iterative fitting)及後計算(post-computation)。表格 
4-1 ICAAM的虛擬碼。 
表格 4-1 ICAAM 虛擬碼 
 
Pre-compute: 
(1)Evaluate the gradient
0A of the template 0( )A x  
(2)Evaluate the Jacobians W
p


and N
q


at (x;0)  
(3)Compute the steepest descent images 
0
0 0
1
( ) ( ) ( )
m
j i i
i x sj j
N N
SD x A A x A A x
q q 
  
    
   
   
0
4 0 0
1
( ) ( ) ( )
m
j i i
i x sj j
N N
SD x A A x A A x
p p

 
  
    
   
   
(4)Compute the Hessian matrix 
掌控其品質。同學 1、同學 2各收集了 14張影像，Gollum及 Shrek各收集了 17張影像，
總統 馬英九總統則收集了 13張影像，圖 4-3列舉各模組訓練樣本其中的四張圖片。  
同學 1 
 
同學 2 
 
Gollum 
 
Shrek 
 
馬英九 
 
圖 4-3 表情訓練樣本節錄 
資料收集完成後便可進行前處理︰首先，在收集的圖片上點選事先定義的特徵點。接
下來利用 Procrustes演算法[40]進行特徵點校正對齊，並計算各組人物獨自的帄均形狀。
最後進行紋理校正對齊，將樣本影像經由定義的特徵點分段式 Affine形變到所屬人物的帄
均形狀上。最後獨立對個人物進行 PCA過程，以求取外形及紋理基底向量。圖 4-4列出個
人物的帄均形狀及帄均紋理。 
帄均形狀 
 
帄均紋理 
 
圖 4-4 帄均形狀與帄均紋理。 
由左而右為同學 1、同學 2、Gollum、Shrek、馬英九。 
在計算大小比例之前，必頇先做各部位的對齊，此對齊的動作是以部位中心為參考，
部位中心U 的計算如下： 
1
1 n
i
i
U U
n 
   (4-4) 
其中
iU 為帄均外形中此部位的第 i個頂點座標位置，n 為此部位的特徵點總數。對齊之後，
接著計算部位中各特徵點 x及 y方向與U 的帄均距離： 
1
1
( )
n
x xi x
i
D U U
n 
   (4-5) 
1
1
( )
n
y yi y
i
D U U
n 
 
 (4-6)  
則此部位 Taam與 Saam變化量之比例為： 
,
,
Saam x
x
Taam x
D
R
D
  
(4-7) 
,
,
Saam y
y
Taam y
D
R
D

 (4-8)  
其中
SaamD 代表 Saam的部位帄均距離，而 TaamD 代表 Taam的部位帄均距離。有此比例後，計
算合成外形時，只需將 Taam 形狀的變量乘上 Saam 各部位的
xR 與 yR 即可。圖 4-5 為帄均
網格
0s 與 Shrek嘴巴部分的 xR 與 yR 比例。 
  
圖 4-5 
0s 與 Shrek 的帄均外形嘴巴部位的 xR 為 0.99， yR 為 1.29 
將 Tamm追蹤時所得的外形變化量依上述的形變轉換關係推算 Saam 形狀上應有的變化
量，計算方式如下： 
†
, ,0, 0, ,Saam i Saam i i rs s s R i r    (4-9) 
其中 r 為部位的代號， rR 為 r 部位的轉換關係參數， ,0,saam is 、 0,is 各為 ,0saams 及 0s 的第 i 個
特徵點，則 † ,saam is 為第 i點的推算結果。 
因方法一是由合成對象的基底向量推算所得，所以合成的表情與原合成對象的表情相
似性高，但缺點也因其所推算出的合成外形是由基底向量所估算，所以合成表情變化完全
 圖 4-7將合成紋理分段式 Affine形變至合成外形 
最後合成影像的嘴巴內部與眼睛常會有模糊或合成的不精確的情況，尤其在嘴巴有較
大的變化、眼睛閉合或瞳孔目視的方向變化時，合成的效果並不好，原因在於嘴巴及眼睛
內部的紋理面積(維度)不固定，所以造成這兩部分的紋理訓練不易。為解決此類情況的發
生，本系統將此兩部分的紋理直接從輸入影像上對應的區域作分段式 Affine 形變至合成影
像上，為避免色差的情況嚴重，在 Affine形變時，也會作色差的調校。調校的方式是分別
計算合成紋理與輸入影像帄均值的差異，在作此兩部分的 Affine 形變時，將其差異扣除，
結果如圖 4-8。 
     
輸入影像            AAM追蹤結果 
   
原始合成結果 嘴巴及眼睛部位合成 色差校正結果 
圖 4-8由左至右，由上至下分別是輸入影像、輸入影像的擬合結果、原始合成
結果、將輸入影像的嘴巴及眼睛內部紋理分段式 Affine形變至合成影像、經過
色差校正後 
表格 5-1 照明度實驗使用特徵向量數量與百分比 
區域分割方式 空間維度 使用特徵
向量個數 
占全部特徵向
量百分比 
一個區域 576 50 89.9% 
四個區域 144 15 80.0% 
九個區域 64 10 84.4% 
 
log(相似機率)
-0.15
-0.12
-0.09
-0.06
-0.03
0
1 201 401 601 801 1001 1201 1401
frame index
lo
g(
相
似
率
)
 
圖 5-1 照明度測詴結果相似度比較取log函數曲線圖 
 
    
頁框  156 頁框   324 頁框   377 頁框   402 
 
    
頁框   554 頁框   765 頁框   802 頁框   900 
     
頁框 1 頁框 206 頁框 796 頁框 966 
圖 5-4 不分區域人臉追蹤測詴結果 
 
    
頁框 1 頁框 206 頁框 796 頁框 966 
圖 5-5分四區域人臉追蹤測詴結果 
 
    
頁框 1 頁框 206 頁框 796 頁框 966 
圖 5-6分九區域人臉追蹤測詴結果 
 
測詴影片三：部分遮蔽處理，利樂包追蹤 
本段測詴影片之實驗，主要在追蹤利樂包表面上的中文字並且此中文字被其他包裝盒給遮
蔽，遮蔽區域最高高達 30％以上，包裝盒為一個複雜背景，本段測詴影片共有 120張影像，
共分成三種不同區域分割法追蹤做實驗，下列圖表只是部分追蹤結果。分九區追蹤效果最
佳，因為不分區域追蹤法的目標物 bounding box 會有忽大忽小的情形，因為分區的追蹤
法會利用部分區域差值總和結果做比較基礎，而這些區必定沒被遮蔽因此會比不分區域追
蹤法效果較優，當目標物 bounding box 不穩定可能造成物件遺失，所以不分區在第 73頁
框遺失物件，追蹤失敗。 
表格 5-3利樂包實驗使用特徵向量數量與百分比 
區域分割方式 空間維度 使用特徵向量個數 占全部特徵向量百分比 
一個區域 576 50 93.1% 
四個區域 144 15 69.9% 
九個區域 64 10 75.1% 
 
開發工具：Microsoft Visual Studio 2005 + OpenCV程式庫 
CPU：Intel Pentium D 2.8GHz 
記憶體：2GB 
攝影機：Logitech webcam 
5.3單張人臉影像追蹤實驗 
為了進行單張人臉影像追蹤實驗，我們分別以自行拍攝的照片和取自 Technical 
University of Denmark 的 IMM (Informatics and Mathematical Modelling)人臉資料庫
[?33]作為實驗樣本。在此實驗中，實驗的影像亦為訓練影像，其中自行拍攝的照片共 17
張，每張影像上以人工方式定出特徵點共計 76個點。IMM人臉資料庫的部分共有 40 個人，
每人各有 6張不同角度、光線或表情的影像，每張影像也有其特徵點座標以及其他相關資
訊等，其中使用的特徵點個數為 58個。我們僅挑選其中 6人的影像作追蹤實驗。 
實驗中，網格模型的初始位置是以自動的方式給定，我們用 Viola []的人臉偵測法偵
測出影像中的人臉位置，再以偵測所得的臉部區域中心為初始位置，此人臉偵測的效果相
當穩定，亦可抵抗不同大小的臉部影像，偵測出的人臉區域大小正好可轉換為全域轉換參
數中決定縮放比率的參數初始值。 
人臉追蹤的效果採用追蹤到的人臉特徵點位置和真實(Ground Truth)的特徵點位置間
的均方根誤差(Root Mean Squared Error，RMSE)作為效能指標，其定義為： 
0 2
1
1
( )
n
i i
i
RMS
n 
  p p  (5-1) 
其中
ip 為追蹤後所得的特徵點座標，
0
ip 為真實(Ground Truth)的特徵點座標。 
5.3.1自行拍攝影像之追蹤 
自行拍攝的照片共 17 張，其中涵蓋的人臉表情(1)無表情、(2)無表情面左側、(3)無表
情面右側、(4)無表情上揚、(5)無表情低頭、(6)微笑、(7)大笑、(8)張大嘴、(9)生氣並皺
眉毛、(10)驚訝、(11)嘟嘴、(12)抿嘴、(13)閉眼、(14)五官向內擠、(15)露牙、(16)嘴巴
向左抽動、(17)嘴巴向右抽動。實驗過程中，固定作 20 次的迭代後停止，列表格 5-5 出每
種表情追蹤的 RMSE值。圖 5-9是四種表情的擬合過程和結果。 
表格 5-5自行拍攝之影像實驗誤差(以 RMSE計量) 
Image ICAAM RMSE Image ICAAM RMSE Image ICAAM RMSE 
(1)無表情 1.891 (7)大笑 2.135 (13)閉眼 2.501 
  
圖 5-11 IMM人臉資料庫各式表情範例 
初始條件固定作 20 次的迭代後停止。追蹤後的 RMSE值分別以 6個表格記錄，如表格 
5-6所示。 
表格 5-6 IMM人臉資料庫實驗誤差(以 RMSE計量) 
 正臉無表情 笑臉 側左 側右 光線變化 無規律 
01-1 1.947 0.912 6.037 6.100 1.068 5.394 
06-1 3.874 3.438 3.536 1.824 2.032 4.853 
09-1 4.842 5.009 6.602 6.886 6.097 4.205 
14-1 3.389 3.013 4.798 5.266 3.722 2.682 
20-1 2.581 2.555 4.697 2.506 1.944 6.445 
40-1 2.683 4.179 7.442 4.839 4.903 7.284 
average 3.219 3.184 5.519 4.570 3.294 5.144 
5.4影片追蹤實驗 
本節實驗以連續的視訊輸入作追蹤，實驗包含了全域性轉換、區域性的表情變化以及
光線變化等追蹤實驗，每張影像的迭代次數固定為 20次，初始值同樣以自動的方式給定，
並在影片追蹤時設定一門檻值，當目前追蹤網格所抓取出的影像與樣版影像間的差異高於
此門檻時，則重作自動臉部偵測，重新給定初始值。在此設定門檻值T 為 0.3N，N 為畫素
總數。因每一畫素各有 RGB三個頻道，所以每一畫素最大的差異為 3，0.3N表示最大可能
誤差的 10%。 
5.4.1全域變化追蹤 
全域變化追蹤包含了五段影片，各有不同的變化。 
Video 1：左右來回橫移，且移動速度不斷加快，總畫面數為 223 
Video 2：前後移動，且移動的幅度愈來愈大，總畫面數為 323 
Video 3：前後左右移動，總畫面數為 202 
 
(a) Video 2中臉部後移所造成的擬合不良 
 
(b)Video 5中臉部帄面外旋轉過大， 
所造成的擬合不良 
圖 5-13擬合不良情況。 
5.4.2區域性表情變化追蹤 
為了測詴各種不同表情變化的追蹤效果，我們也準備了第六段的視訊(Video 6)，此視
訊中包含微笑、大笑、生氣、皺眉、嘟嘴及驚訝等各種表情變化，影片的總畫面數為 631。
同樣地圖 5-14呈現了追蹤此影片時的影像誤差，由統計圖可看出 ICAAM 對於抵抗區域性
的表情變化有不錯的成效。 
 
圖 5-14表情變化影片的追蹤結果統計圖 
5.4.3光線變化追蹤 
最後一個實驗測詴在光線不斷變化的環境下追蹤人臉的效果。我們使用第七段視訊影
片(Video 7)來測詴，影片中光線不斷以開關變化來達到光線變化的效果，此段影片的總畫
面數為 214。 
 
圖 5-15光線變化影片的追蹤結果統計圖 
在統計圖中，有兩段誤差比較低的區段(約在畫面 109~145及 199~影片結束)，此兩段
是在開燈的情況下，表示當時的光線亮度與系統樣版的亮度較為接近，所以誤差較低，因
紋理是 AAM的主要特徵之一，所以改變亮度對於 AAM有也一定的影響。 
Affine取得，並經過色彩調校的方法來呈現。 
表情 3 
 
表情 4 
 
表情 5 
 
表情 6 
 
表情 7 
 
表情 8 
 
表情 9 
 
表情 10 
 
表情 11 
 
表情 12 
 
表情 13 
 
表情 14 
 
圖 5-18 各表情合成影像 
5.4.5速度 
若固定每個畫面 20 次迭代的情況下，在本實驗環境下追蹤的速度約可達到一秒鐘 10
第6章 計畫成果自評部份 
本計畫為二年期計畫，第一年度將著重於人臉特徵表示與追蹤部份，第二年則進行三
維自動建模及表情動畫合成，進度均甚為順利。目前研發方法之效果頗佳，本團隊將積極
收集整理實驗結果以便進行期刊論文之發表。 
 
參考文獻 
                                                 
[1] R. C. Gonzalez and R. E. Woods, Digital Image Processing, 2nd Ed., Prentice Hall, 
pp.626-627,2002. 
[2] R. Jain, D. Militzer, and H. Nagel. “Separating non-stationary from stationary scene 
components in a se-quence of real world tv-images”. IJCAI, pages 612–618, 1977 
[3] B. K. P. Horn and B. G.. Schunck. Determining optical flow. Artificial Intelligence, 
17:185-203, 1981. 
[4] F. E. Alsaqre, and Yuan Baozong, “Moving object segmentation from video sequences: An 
edge approach,” 4th EURASIP Conf. on Video/Image Processing and Multimedia Comm., 
vol.1,pp.193-199, July 2003. 
[5] T.Horprasert, D. harwood ,and L. S. Davis. A Statistical Approach for Real-time Robust 
Background Subtraction and Shadow Detection. In Proc. IEEE ICCV’99 FRAME-RATE 
Workshop, 1999. 
[6] K.Toyama, J. Krumm, B. Brumit, and B. Meyers. Wallflower: Principles and Practice of 
Background Maintenance. In IEEE International Conference on Computer Vision, pp 
255-261,1999. 
[7] S. Y. Chien, S. Y. Ma, L. G. Chen, ”Efficient Moving Object Segmentation Algorithm Using 
Background Registration Technique”. IEEE Transactions On Circuits And Systems For Video 
Technology, 2002 
[8] S. A. El-Azim, I. Ismail, and H. A. El-Latiff, “An Efficient Object Tracking technique using 
block-matching algorithm,” Proc. of the Nineteenth National, Radio Science Conf., pp.427-433, 
2002. 
[9] D. Koller, J. Weber, J. Malik, Robust Multiple Car Tracking with Occlusion Reasoning, 
European Conference on Computer Vision.(1994) pp.189-196. 
[10] M. Isard, A. Blake, Contour Tracking by Stochastic Propagation of Conditional Density, 
European Conference on Computer Vision. (1996) pp. 343-356. 
[11] P. Shi, G. Robinson, T. Constable, A. Sinusas, and J. Duncan, A Model-Based Integrated 
Approach to Track Myocardial Deformation Using Displacement and Velocity Constraints. Proc. 
