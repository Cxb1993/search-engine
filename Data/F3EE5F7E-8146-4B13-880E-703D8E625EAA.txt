  2 
( ) / 2
                                 (6)
l r
l rr
l r
y y
y yy
y z bf x x
∴ =
+
= ⋅ =
−
 
由(4)、(6)，以及(3)式，可算得 P 點的 x,y,z 座標
值。 
● 特徵點偵測 
   特徵點(feature)為一影像中，具局部可區別性，且
具有某些強烈獨特性的特殊點(salient point)。影像特
徵點的偵測，可視為辨認影像中特殊點的問題。 在一
灰階影像中，特殊點具有局部最大(local maximum)之
灰階空間導數 (gray level spacial derivatives)。 若一影
像點在所有方向皆具有最大的空間導數，該點可能為
一角落點 (corner)；而假如僅在某一方向具有較大的
空間導數，則該點可能為一邊緣(edge)上的點[16]。 
Shi 和 Tomasi [16]提出一能辨識優良特徵點的
方法，首先選定適當大小的影像補丁(patch)W，如
11x11 pixels影像區域，由其中各個像素(pixel)影像強
度(image intensity)的水平梯度 xg 和垂直梯度 yg 建構
一2×2 G 矩陣： 
2
2
( , )
( , ) ( , ) ( , )
( , ) ( , ) ( , )
x y x
i j W y x y
g i j g i j g i j
g i j g i j g i j∈
 
=  
  
∑G        (7) 
再計算G矩陣的特徵值 1λ , 2λ ，以及最小特徵值
1 2min( , )λ λ λ= 。 若一影像補丁具有較大的最小特徵
值 λ，則該補丁所在位置可視為一不錯的特徵點位置。 
     將上述運算子 (operator)掃描運用在整個影像
(frame)中的所有patches，具有最大λ 值的patch，即可
選為影像中最好特徵點的位置。 圖三紅色補丁為使用
Shi和Tomasi方法所偵測得的特徵點。 
 
圖三 使用Shi和Tomasi方法偵測到的特徵點(紅色補 
     丁) 
 
由獲得的特徵點，可建構行走機器人週遭環境的
簡略地圖，供機器人定位和路徑導引使用。為了降低
mismatch程度和提升計算速率，patch大小須適當選
定，本研究選用patch大小為11x11 pixels。  
● 以關聯度為基礎的特徵匹配(Correlation-based 
feature matching)  
在二個不同影像中找尋相同特殊點的問題稱為
特徵匹配。特徵匹配的方法，有依據幾何特性
(geometry based)、外觀(appearance based)、物體辨認
(object-recognition based)，以及其他原理等。根據外
觀的特徵匹配法，因具有高強健性、容易定義，以及
計算較快等優點，本研究採用以關聯度為基礎的特徵
匹配法。物體特徵點的獨特性，由該點的鄰近像素，
如一小的矩型影像補丁的影像特性所決定。所以要找
出兩影像中對應的特徵點配對(conjugate pair)，可由掃
描方式，逐一計算另一影像中所選定搜尋區域內所有
特徵點附近影像補丁與第一影像中該特徵點附近影像
補丁的相似度，來決定最佳配對特徵點的所在[12]。 
定義如下的正規化差異平方和  (normalized 
sum-of-square-difference)量度指標C，以表示兩影像補
丁的相似度(similarity)： 
 
2
0 01 1
( , ) 1 0
( , )( , )
i j W
I i j II i j I
C
N
σ σ∈
 
−−
− 
 
=
∑
          (8) 
其中W 為定義影像補丁的視窗(window)(中心點為基
底像素)， 0I 和 1I 為該二影像補丁的像素強度， 0I 和 1I
為該二影像補丁的像素強度平均值， 0σ 和 1σ 則為該二
影像補丁像素強度的標準差， N 為影像補丁內所含
的像素個數。 
( , )
1 ( , )
i j W
i j
N ∈
 
=  
 
∑I I ； 21 1 1
( , )
1 ( ( , ) )
i j W
I i j I
N
σ
∈
= −∑  ；   
2
0 0 0
( , )
1 ( ( , ) )
i j W
I i j I
N
σ
∈
= −∑                      (9) 
若差異量度值C為零，表示該二影像補丁完全匹
配。當C值愈大，表示該二影像補丁不匹配(mismatch) 
程 度 愈 高 。 在 搜 尋 區 域 內 低 於 門 檻 值 ( 如
0.95thresholdC = )且具有最小C值的影像補丁，可視為所
欲尋找的匹配特徵點。 
 
 
 
圖四 不同影像的特徵匹配結果 
  4 
1 2
1 1 1 1 2
2 2 1 2 2
aug
( | ) ( | ) ( | )
( | ) ( | ) ( | )( | )
( | ) ( | ) ( | )
xx xy xy
y x y y y y
y x y y y y
k k k k k k
k k k k k k
k k
k k k k k k
 
 
 
=  
 
  
P P P
P P P
P
P P P



   
 (29) 
其中 ( | )xx k kP 和 i i ( | )y y k kP 分別為機器人和第 i 個特徵
點的 covariance 矩陣； ( | ) ( | )
i i
T
xy y xk k k k=P P 為機器人
和第 i 個特 徵 點 間 的 cross-covariance 矩 陣 ； 
( | ) ( | )
i j j i
T
y y y yk k k k=P P 則為第 i 個和第 j 個特徵點間的
cross-covariance 矩陣。 
擴增狀態之預測方程式為： 
( )
( )
aug aug aug
aug
1
2
( 1 ) ( ) ( ) 0
( ) ( ) 0
( )
                    
( )
k k k k k k
k k k k
k k
k k
+ =
 
 
 
=  
 
  
ˆ ˆx | f x | ,u , ,
ˆf x | ,u , ,
yˆ |
yˆ |

      (30) 
擴增狀態預測向量之 covariance 矩陣為： 
( ) ( )
( ) ( )
aug aug aug aug
aug aug
( +1 ) ( )
                        ( )
T
x x
T
n
n n
k k k k
k k
= ∇ ∇
+ ∇ ∇
P f P f
f P f
    (31) (31) 
其中  
0 0  0
0 0  0
0 0  0
 
  0      0   0     
aug
x
x
∇ 
 
 
 ∇ =
 
 
 
 
f
I
f I
I



    

              (32) (32) 
( )aug diag[ ( ) 0 ]nxn∇ = ∇f f  .  
所以 
1 2
1 1 1 1 2
2 2 1 2 2
aug ( 1 )
( ) ( ) ( ) ( )
( ) ( ) ( )
( ) ( ) ( )
T
x xx x x xy x xy
T
y x x y y y y
T
y x x y y y y
k k
k k k k k k k
k k k k k k
k k k k k k
+ =
 ∇ ∇ + ∇ ∇
 
∇ 
 ∇ 
 
 
P |
f P | f Q f P | f P |
P | f P | P |
P | f P | P |



   
 (33) 
假設 t=kT 時，全部特徵點的量測模型為： 
   ( )( ) ( ), ( ), ( ),k k k k k=z h x y w                 (34) 
則其中第 i 個特徵點的輸出預測值，可計算如下： 
  ( ) ( )1 ( 1 ), ( 1 ),0,i i ik k k k k k k+ = + +ˆ ˆzˆ h x y      (31) 
    假設 ( 1)i k +z 為機器人感測系統對第 i 個特徵點
的量測值，則該量測所獲得的創新( innovation) 可定
義如下： 
  ( )ˆ( 1) ( 1) 1i i ik k k k+ = + − +ν z z          (32) 
定義 當 時 整體 N 個 特 徵 點 之 創 新向 量 為 ：
1 2( 1) ( 1)  ( 1)  ( 1)
TT T T
Nk k k k + = + + + ν ν ν ν ， 則 其
covariance 矩陣為： 
 
( 1 ) ( 1 )
                      ( 1)
aug aug
T
x x
T
w w
k k k k
k
νν + = ∇ + ∇
+ ∇ + ∇
S | h P | h
h R h
        (37) 
其中 
( 1 ) ( 1 )
( 1 )
          0   0 0 
aug
aug
k k k kaug aug
i
i x
aug k k
i i
i
+ +
+
∂∇ =
∂
 ∂ ∂ 
=  ∂ ∂
  xˆ xˆ
,
xˆ
hh
x
h h
x y
 
   (38) 
令 
aug,i i x
∇H h , 
,R i x∇H h , , iy i∇ yH h , 
n n
∇H h ，(38) 式可寫為： 
  
( 1) [ 0 0 0  0]i R yk + =H H H… …      (39) 
其中非零的部分對應於機器人方位估測值和第 i 個特
徵點估測值 ˆ ( 1 )k ki +y 的所在位置。 第 i 個特徵點
估測值 ( )1i k k+zˆ 的 covariance 矩陣，可計算如下： 
   
aug( 1 | ) ( 1) ( 1| ) ( 1)
                      ( 1) ( 1) ( 1)
T
i i i
T
w w
k k k k k k
k k k
+ = + + +
+ + + +
S H P H
H R H
 (40) 
在狀態向量進行更新步驟之前，須先進行量測數據
結合的檢驗步驟 (data association step)。若第 i 個特徵
點新量測所獲得的創新 ( 1)ki +ν ，滿足下式條件： 
    
1 2( 1) ( 1 ) ( 1)Ti i ik k k k χ−+ + + ≤ν S ν          (41) 
其中 2χ 為依照 chi-square 分布[3]所選定之閘值(gate 
value)，則新量測到的該特徵點視為與前一取樣時刻的
第 i 個特徵點為同一點，稱為特徵具有匹配性。若(41)
式不滿足，則新量測到的該特徵點不與前一取樣時刻
的第 i 個特徵點為同一點，可在下一遞迴步驟時，將
該點加在擴增狀態向量的尾端變成新的一特徵點。此
時前一取樣時刻的第 i 個特徵點的匹配點，必須繼續
尋找，若找不到,可能該點於此時已不見了，於更新前
可將該點從擴增狀態向量中去除掉。 
   最後，擴增狀態向量之更新，以及其 covariance 矩
陣，可分別計算如下： 
aug augˆ ˆ( 1 | 1) ( 1 | ) ( 1) ( 1)i ik k k k k v k+ + = + + + +x x K (42) 
 
aug aug( 1| 1) ( 1 | )
                           ( 1) ( 1| ) ( 1)T Ti i i
k k k k
k k k k
+ + = +
− + + +
P P
K S K
 (43) 
其中卡門增益矩陣可計算如下： 
  
1
aug( 1) ( 1| ) ( 1) ( 1 | )Ti i ik k k k k k−+ = + + +K P H S   (44) 
     
● 行動機器人之局部路徑規劃研究 
     智慧型機器人的行動導引(navigation)技術，可分
為下列兩項問題：(一)避開障礙物(obstacle avoidance); 
以及(二)到達目標方位(goal achieving)。 
     使用 SLAM 技術所提供的即時局部資訊，通常無
法立即規劃出到達最終目標方位的最短行進路徑，所
以機器人可以利用 SLAM 每一遞迴步驟所獲得的機器
人當時所在方位，以及最靠近機器人前方的特徵點資
訊，建構一可以避開障礙物且能往最終目標靠近一點
的局部路徑，轉換為各關節軸(低階致動伺服系統)的局
部運動命令軌跡，再下達給機器人的伺服控制器加以
實現。 
      本研究參考 Kim 和 Cho [10]所提的模糊避障和局
部路徑規劃法，與前述基於特徵點的 EKF/SLAM 演算
  6 
max 運算，則候選經過點 ix 同時滿足三項目標的程度
可以下列模糊交集合表示： 
       1 2 3D G G G= ∩ ∩                     (53) 
 { }
1 2 3
( ) min ( ), ( ), ( )i i i iD G G Gx x x xµ µ µ µ=       (54) 
0 0.5 1 1.5 2 2.5 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Membership functions of fuzzy goals G1, G2, G3
 
 
mG1
mG2
mG3
 
圖十 目標 G1 ,G2 , G3 之隸屬函數 
0 0.5 1 1.5 2 2.5 3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Fuzzy membership function of fuzzy decision
  
 
圖十一 供選擇最佳經過點之隸屬函數 ( )iD xµ   
 
在所有候選經過點中，具有最高 Dµ  的候選經過點可選
為當時之最佳經過點 optx : 
{ }{ }1 2 3arg max min ( ), ( ), ( )iopt i i iG G Gx Xx x x xµ µ µ∈=     (55) 
 
● 避免落入陷穽(get trapped)之方法 
機器人有可能走進由障礙物所形成的死巷中
(如 U 形障礙物中)，在裡面遊蕩而無法往目標前進。
為了避免落入遊蕩狀況，在演算法中需偵測該狀況是
否發生。如圖十二所示，若 00 90tθ θ− > ，則可視為機
器 人 將 陷 入 遊 蕩 狀 況 ， 其 中 tθ  
( tθ pi θ= − , ( ) ( )( )target Robot Robot targetatan2 ,y y x xθ = − − ) 
為從目前機器人位置至目標點的方向角， 0θ 為目前機
器人前進的方向角，此時可以設定虛擬目標點暫時取
代原目標點的方法來規劃，以避免遊蕩。若
0
0 90tθ θ− < ，或並沒有偵測到障礙物，則不設定虛
擬目標點，而仍舊依原訂的目標點來規劃。 
如圖十二所示，虛擬目標點( virtual targetx , virtual targety )
的設定，可以選在與最近障礙物夾 45oVTβ = 角度方
向上，距離機器人適當遠(如 =1.5 m)的地方： 
     
 
virtual target sin Robotx xα= +                    (56) 
 
virtual target cos Roboty yα= +                    (57) 
其中 
 ( ) ( )( )
min min
0
U Robot Robot U
90
atan2 ,
u VT
u
y y x x
α θ β
θ
= + −
= − −
       (58) 
 
圖十二 避免落入遊蕩狀況之設定虛擬目標點方法 
 
本研究整合 EKF/SLAM，根據如圖十三所示之模
糊局部路徑規劃演算法流程圖，進行機器人之模糊導
航技術模擬研究。 
 
研究結果與討論 
   本研究於模糊局部路徑規劃的模擬時，對模糊目標
函數選擇不同的 is 和中心值 iC , i=1, 2, 3, 了解機器人
導航的特性。  選定目標位置如下：
target target
T
x y    
[ ]14.50 14.00 T= (m)；目標臨限(機器人停止)距離則
選為 targetτ =0.3 (m)；危險範圍選擇： mind =0.1(m)， 
maxd =0.5(m)；正常化所用的參數距離則選 maxR =2.5 
(m)。候選經過點共選 21 點，障礙物則選用由 SLAM
所得的特徵點。 
    圖十四至十七為代表性的模擬結果。由結果可
知， 1s 愈小所得的性能愈好， 2s 改變則看不出對性能
有所影響， 3s 較小所得的性能也會較好。 1C 值會影響
機器人與最近障礙物之避障距離，當 1C 愈小避障距離
愈短。當有擁擠的障礙物時，改變 2C 對行走路徑將有
明顯的影響， 2C 至少要大於 0.4。當 3C 較大，機器人
可以較短路徑到達目標點。 
     因使用太多時間於三輪機器人低階伺服控制硬
體設計製作與控制實現的研究。該控制器的設計採用
主、從微控制器架構(如圖十八所示)，製作上發生一
些問題，所以有關立體視覺系統辨識特徵點和機器人
內部感測器獲得速度數據之整合運用，並無法在期限
內完成。因此，未來將再繼續探討可以即時實現的特
徵點匹配和視差估測的有效方法的研究，並以即時實
驗的方式瞭解該策略的性能。 
 
maxx
  8 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖十八 全向輪機器人整體系統架構 
 
參考文獻 
1. S. Se, D. Lowe, and J. Little, “Mobile Robot 
Localization and Mapping With Uncertainty Using 
Scale-Invariant Visual Landmarks,” Int. J. of 
Robotics Research, Vol. 21, No. 8, pp. 735-758, 
August 2002. 
2. M. Montemerlo, S. Thrun, D. Koller, and B. 
Wegbreit, “FastSLAM: A Factored Solution to 
Simultaneous Mapping And Localization Problem,” 
Proc. 18th Natl. Conf. on Artificial Intelligence, pp. 
593-598, 2002. 
3. A. J. Davison, I. D. Reid, N. D. Molton and O. 
Stasse, “MonoSLAM: Real-Time Single Camera 
SLAM,” IEEE Trans. on Pattern Analysis and 
Machine Intelligence, Vol. 29, No. 6, pp.1052-1067, 
June 2007. 
4. J. Borenstein and Y. Koren, “Real-Time Obstacle 
Avoidance For Fast Mobile Robots,” IEEE Trans. on 
Sys., Man, and Cybernetics, Vol. 1, No. 5, pp. 
1179–1187, 1989. 
5. K.H. Kim and H.S. Cho, “Mobile Robot Navigation 
Based on Optimal Via-Point Selection Method,” 
Proc. IEEE/RSJ Int. Conf. Intelligent Robots and 
Sys., pp. 1242–1247, 1998. 
6. A. Fatmi, A. A. Yahmadi, L. Khriji, and N. 
Masmoudi, “A Fuzzy Logic Based Navigation of a 
Mobile Robot,” Proc. of World Academic of Science, 
Engineering and Technology, Vol. 15, October 2006. 
7. K.H. Kim and H.S. Cho, “An Obstacle Avoidance 
Method for Mobile Robots Based On Fuzzy 
Decision-Making,” Robotica, Vol. 24, pp. 567-578, 
Cambridge University, 2006. 
8. M. Xie, Fundamentals of Robotics: Linking 
Perception to Action, World Scientific, 2003. 
9. R. Jain, R. Kasturi, and B. G. Schunck, Machine 
Vision, McGraw-Hill, 1995. 
10. Z. Zhang, “Flexible Camera Calibration by Viewing 
A Plane from Unknown Orientation,” in 7th IEEE 
Int. Conf. Computer Vision, pp. 666–673, 1999. 
11. J. Heikkilä and O. Silvén, “A Four-step Camera 
Calibration Procedure with Implicit Image 
Correction,” In Proc. IEEE Int. Conf. on Computer 
Vision and Pattern Recognition, San Juan, Puerto 
Rico, pp. 1106-1112, 1997. 
12. J. Sola, A. Monin, M. Devy and T. 
Lemaire,”Undelayed Initialization in Bearing Only 
SLAM,” in Proc. IEEE/RSJ Conference on 
Intelligent Robots and Systems, 2005. 
13. J. Shi, and C. Tomasi, “Good Features to Track,” In 
Proc. IEEE Int. Conf. on Computer Vision and 
Pattern Recognition, Seattle, USA, pp. 593–600, 
1994. 
14. A. J. Davison and D. W. Murray,“ Simultaneous 
Localisation and Map-Building Using Active 
Vision,” IEEE Trans. on Pattern Analysis and 
Machine Intelligence, pp. 865-880, July 2002. 
15. P. M. Newman, “On the Structure and Solution of 
the Simultaneous Localisation and Map Building 
Problem,” PhD Thesis, The University of Sydney, 
Sydney, March1999.  
16. M.I.Ribeiro, “Kalman and Extended Kalman Filters : 
Concept, Derivation and Properties,” Technical 
Report, Institute for Systems and Robotics, Portugal, 
2004. 
17. R. G. Brown and P.  Y . C. Hwang, Introduction to 
Random Signals and Applied Kalman Filtering, 
John Willey&Sons, 3rd Ed., 1997. 
18. J. M. M Montiel, J. Civera, and A. J. Davison, 
“Unified Inverse Depth Parametrization for 
Monocular SLAM,” In Robotics Science and 
Systems, 2006. 
19. K. S. Fu, R. C. Gonzalez, and C. S. G. Lee, Robotics, 
Control, Sensing, Vision, and Intelligence, 
McGraw-Hill, 1987. 
20. H. Choset, K. M. Lynch, S. Hutchinson, G. Kantor, 
W. Burgard, L. E. Kavraki and S. Thrun, Principles 
of Robot Motion : Theory, Algorithms and 
Implementations, The MIT Press Cambrige, England, 
2005. 
21. Lowe, “Distinctive Image Features From 
Scale-Invariant Keypoints,” Int. Journal of 
Computer Vision, pp. 91–110, 2004. 
22. C. Harris, and M. Stephens, “A Combined Corner 
and Edge Detector,” In Fourth Alvey Vision 
Conference, pp. 147-151, Manchester, UK, 1988. 
23. P. P. Smith, Active Sensors For Local Planning in 
Mobile Robotics, World Scientific, 2001. 
24. H. J. Zimmermann, Fuzzy Set Theory and Its 
Applications, Second Edition, Kluwer Academic 
Publishers, 1991.  
25. D. Shreiner, M. Woo, J. Neider, and T. Davis, 
OpenGL Programing Guide, Fifth Edition, 
Addison-Wesley, 2006. 
26. J. Smart, K. Hock and S. Csomor, Cross-Platform 
GUI Programming with wxWidgets, Prentice Hall, 
2006. 
 
ARM7  
dsPIC30F4011 
(Slave) 
dsPIC30F4011 
(Slave) 
dsPIC30F4011 
(Master) 
DAC 
DAC 
AC 
CSBL900 
CSBL900 
CSBL900 
直流馬達 
直流馬達 
直流馬達 
UART 
I2C 
I2C 
98年度專題研究計畫研究成果彙整表 
計畫主持人：林麗章 計畫編號：98-2221-E-005-044- 
計畫名稱：基於特徵點之全向輪機器人同時定位建圖與模糊局部路徑規劃研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
