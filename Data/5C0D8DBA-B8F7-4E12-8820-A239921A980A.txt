行政院國家科學委員會專題研究計畫成果報告 
 
計畫名稱：子計畫三：仿人形運動娛樂機器人動作學習與合作行為之研製 
執行期限：民國97年8月1日至100年7月31日 
主持人：黃國勝國立中正大學電機工程學系 
計畫編號： 97-2221-E-194-041-MY3 
 
摘要 
本計畫利用加強式學習(reinforcement learning)解決
動態行走與平衡控制的問題。本計畫無任何關於雙足機
器人動態模型的相關知識，而是採用表格的方式泛化連
續的輸入資訊，使用此表格的優勢在於可以減少連續值
狀態空間的大小，並提出一手部平衡演算法，此演算法
將輸出連續行為來控制雙足機器人的手部行為，進行機
器人的平衡控制。最後，建構類似二足機器人的模型來
進行模擬，模擬結果顯示，兩足機器人在開始行走時，
無法穩定行走，經由演算法學習後，將可改善其行走的
穩定性。 
 
I. 簡介 
要使機器人保持行走時的姿態平衡，機器人必須結
合相關系統。其中一種方法是在計算機器人的中心值，
另一些研究使用軌跡規劃(trajectory planning)[1]，也有一
些研究使用數學模型建立二足機器人的動態方程式
[2]，這些方法的計算量都相當複雜。有部分學者提出使
用壓力感測器的技術來幫助機器人平衡，例如參考文獻
[3]，此計畫即在機器人的足部安裝感測器，用感測器來
觀察足部與地面的情況。另一個方面使用機器學習的方
法讓機器人學習如何保持平衡行走[4]。在參考文獻[5]
中，透過設立在機器人雙腳上的感應器，來計算零力矩
點(Zero Moment Point)，然後將計算出來的 ZMP值作為
模糊控制器的輸入值，並且調整機器人平衡的姿勢。此
外，一些研究方法使用加強式學習的方法[6]，調整兩足
機器人行走的參數。 
 
II. 背景 
A.增強式學習 
加強式學習是使能夠感知環境狀態的代理人經由
與真實環境互動，利用錯誤嘗試的方式，從「無」到「有」
訓練出控制策略，訓練過的控制策略使代理人能在任意
的初始狀態下，選擇一連串到達目標的最佳行動序列。
代理人與環境互動過程，將以報酬方式進行評估，當環
境回傳好的報酬，代理人將強化此決策，若回傳差的報
酬，代理人將弱此決策，隨著一段時間的策略調整，使
代理人獲得最大的累積報酬。 
 
B. Q-Learning 
Q-learning為一種不需使用模型且建立於隨機動態
規畫的增強式學習法。在學習過程中，Q-Learning 為受
教者而真實環境為施教者，利用互動的過程取得互動經
驗，取得互動經驗的同時，Q-Learning自我調整策略，
互動過程將持續一段時間，以便取得相當數量的經驗， 
進行策略調整的程序。最後，即可學習到一組最佳的策
略。以下為 Q-learning演算法。 
1. 將所有狀態及動作的 Q(s, a)值初始化為 0。 
2. 感知目前所處之狀態(s)。 
3. 根據動作價值函數選擇一個動作(a)。 
4. 在環境中執行動作(a)。 取得下一個狀態(s')，r為
立即獲得報酬。 
5. 更新動作價值函數 
 )]([),()1(),(1 sVrasQasQ tt ′++−←+ γαα   
 ),(max)( asQsV a A∈←   
α是學習率參數以及 γ為一固定且值介於 0和 1之間的折
扣因數 
回至程序 2. 
 
III. 提出的方法 
本研究嘗試以 Q演算法執行策略學習並提出手部平
衡演算法建構一智慧型系統架構，如圖 1，此系統架設
在機器人上，Q學習從環境中感知狀態值，經過手部平
衡演算法評估一行動，並輸出到環境中，接著感知下個
狀態值和獎勵值，經由互動過程逐步調整策略，最後使
機器人達成步態行走平衡。 
 3 



<∆
>∆
=
0
0
Q
Q
n
p
m β
β
β
 
其中 0>> np ββ . 
行動偏權變異 
行動偏權變異進行探索。由於較大的變異具有較高
的探索，變異應隨著學習減少。行動偏權變異更新如下： 
)( BvQBvBv v −∆+← β  (4) 
簡要概述該算法所提出的方法如圖 3。 
1. 將 Q-table中，所有狀態 s和行動 a所對應的 Q(s, a)
歸零。 
2. 從環境觀察目前狀態 s。 
3. 根據評估函數求得行動 a，並利用手部平衡演算法計
算偏權值： 
Output = a + N(Bma, Bva)。 
4. 輸出此連續行動 a，讀取下個狀態 s'與產生的加強訊
號 r。 
5. 以公式(1)更新狀態-行動值。 
6. 以公式(3)與(5)分別更新 action bias mean 與 action 
bias variance。 
7. s  s'，a  a，重覆步驟 2，直到完成要執行的次數
或已到達要求的狀態。 
圖 3，智慧型系統演算法 
 
IV. 模擬 
模擬的 Bioloid人形機器人如圖 4所示。圖 4 (a) 為
機器人的三維架構。機器人是以 18個 RC伺服馬達所驅
動。馬達總質量為 1.31公斤。每個臂膀有三個自由度
(DOF)，而每條腿有六個活動自由度；髖關節有三個可
用的自由度，膝蓋有一個，而其餘的自由度位於踝關節。
另外，為得知機器人腳和地面間的接觸力，在機器人的
每隻腳各裝有四個壓力感應器(其中兩個於腳尖，兩個於
腳跟)，如圖 4 (b)所示。 
 在 Q-learning中，利用 ZMP的方法以達到動態平
衡，因此，此演算法能學習控制其兩個臂膀的動作，藉
此可盡量使腳的 ZMP保持在中心點。 
  
圖 4(a) 圖 4(b) 
圖 4，機器人模擬平台. 
A. 狀態空間(state space) 
如圖 5，狀態值的表示為將腳的區塊作切割得出的號
碼位置，以十進位的形式表示，右手邊的狀態值為個位數，
左手邊的狀態值為十位數。表示式如下:  
)10( ×= Lt SS + RS  
將圖 5公式化可知，其中 St 是一個狀態的數字表示
式;SR表右手邊腳的數值，SL是左手邊腳的數值。 
(1) (2) (3)
(7)
(6)(5)(4)
(9)(8)
(1) (2) (3)
(7)
(6)(5)(4)
(9)(8)
 
LS
 
RSRLt S)10S(S +×=
 
圖 5，狀態值的計算 
B. 獎懲值 
在學習走路任務中，機器人平穩前進時獎懲公式會給
予機器人一個正的報酬；反之，當機器人移動不穩或跌倒，
則給一個負的報酬。此學習演算法的任務便是由選擇動作
的過程中獲得最大報酬值。 
獎懲值的計算方法如下: 
 如機器人有穩定的行走時，只要瞬間 ZMP處於
狀態 55就將報酬設為 0。 
 如現行狀態超過這個區塊時，便有一個懲罰報酬
為-1。 
 當在最壞的情況下，機器人跌倒，則該回合結
束，並有一個懲罰報酬-10。 
C. 動作空間 
在此部分，須優先設計出 36個連續的基礎動作。此
參數的動作更新設定如下: 
If  ∆ Q> 0 , mβ = 0.5 . If  ∆ Q< 0, mβ = 0.2， vβ 是學習
率，行動趨勢變化必須小於行動趨勢均值。在不斷地獲得
mβ 、 vβ 之後，使用一般分配的方法獲得動作如下： 
Output = a + N(Bma , Bva ). 
D. 策略更新 
策略會根據目前的狀態計算出在此環境中最合適的
  出席國際會議報告 
2010 IEEE International Conference on Systems, Man, and Cybernetics 
 
Istanbul, Turkey 
中正大學電機系 黃國勝  
一、 參加會議經過  
2010年IEEE 國際電機電子工程師協會在土耳其伊斯坦堡所舉辦的SMC 2010 系統、
人與控制論國際會議，其提供了一個國際論壇，邀集所有全世界對國際電機電子工
程師協會向下系統、人與控制論有著濃厚興趣領域並積極參與的所有學術界及業界
先進，讓其可以透過演說、發表及刊登，展現其各在各方面的最新的研究、貢獻及
實際應用，並提供所有研究著一個交換意見與各方面合作進展的會議場所。換言之，
IEEE SMC是一年一度全世界所有電機電子系統、人機與控制領域學術界及業界研究
者的一大盛事，亦為此一領域最具規模的研討會議。每年IEEE/SMC會議不但吸引了
來自世界各地學術、產業、研究界超過七佰人與會，而且IEEE/SMC會議發表論文之
審查號稱是各種學術研討會中最為嚴謹的，以今年為例大會共收到超過1000篇的投
稿論文，經過審查通過在會議中發表之論文共有來自43個國家不到618篇論文(接受
率大約62%左右)，而且部份論文還屬於壁報型式。筆者與中興大同共同籌組special 
session on “Intelligent Learning in Control  Systems” ，據了解special session
受邀之論文共270篇左右，同樣地也要經過相同的審查程序，結果只有少 數能被接受。
由於IEEE/SMC會議論文審查程序嚴謹，因此在這個國際學術研討會中發表的論文都
是屬於EI論文，這也是對本項國際學術研討會之另一種肯定。大會在10月11日(星期
天)便安排了Tutorials and Workshop課程，正式研討會會議則是從10月12日(星期一)
上午開始論文口頭發表，所有論文則是集中在10月12~14日三天，由於本次會議發表
論文很多，因此每天會議從上午八點開始安排到晚上六點十分，在每天的活動中係
分為六個時段，每一時段同時在十一個不同房間進行不同主題之論文發表，同時大
會在每天上午9:50~11:00時段都安排了專家論譠(Keynote speech session),本次會議三
場keynote speech之主題為Oscillatory Dynamic and Brain Machine Interface Systems、
On Large Systems of Interacting Robots, Agents and Humans、Biology Inspired 
Techniques for Soft Computing, Robot Control and Perception in the Machine and 
Human-Machine Symbiotic Systems等。而本年度論文發表主題分類包括了決策理論、
機器人及智慧型感測、模糊控制系統、製造系統、協合控制機器人、Petri-Net理論與
應用、網路/電子商務系統、品質及系統工程、類神經網路、運輸 /交通控制系統、
虛擬實境技術在工程系統應用、智慧型軟體運算、醫療機電整合、分散式智慧系統
等。本次會議我國與會人士包括台北科技大學李祖添校長、海洋大學鄭慕德教授
(IEEE/SMC Fellow)、台北科技大學主任秘書(前大同大學電資學院院長)黃有評教授、
中興大學理學院院長黄博惠教授等四十餘位。。此次研討會筆者所發表之論文係被
安排在第三天(10/14) Intelligent Learning in Control  Systems session發表。
該session共有六篇論文發表，每人口頭報告二十分鐘，筆者論文口頭發表之兩篇題
目各為” Real-Valued Q-learning in Multi-agent Cooperation (pp. 401-406)”
及” Behavioral-Fusion Control Based on Reinforcement Learning (pp. 409-411)”。
與會期間大會也在第二天(10/13)晚間安排了歡迎酒會(conference banquet)活動，
 三、 攜回資料名稱以及內容 
1. 會議論文集隨身碟。  
2. 大會議程集本。 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：黃國勝 計畫編號：97-2221-E-194-041-MY3 
計畫名稱：仿人形運動與娛樂機器人之設計、研製及應用--子計畫三：仿人形運動娛樂機器人動作學
習與合作行為之研製 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 1 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
