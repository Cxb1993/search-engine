 2 
1. Introduction 
For a new production process, or for new production equipment or materials, 
manufacturing systems usually need to execute a number of pilot runs to ensure 
stability and quality prior to mass production. However, because the data obtained 
from pilot runs is often incomplete and insufficient, using limited data (small data sets) 
to build a robust management model for mass production is difficult. Furthermore, as 
the fast progress of technology makes a product’s life cycle shorter and shorter, the 
ability to transfer the experience from pilot runs to mass production rapidly and 
robustly has become a core competence of enterprises. 
A comprehensive manufacturing problem is generally known to be complicated 
and non-linear. The newly developed Artificial Neural Networks (ANN) have been 
widely utilized to extract management knowledge from acquired data, but having 
sufficient training samples is a basic assumption. Researchers believe that without 
sufficient training samples, neural networks will suffer non-negligible errors. 
Regarding sample sufficiency, computational learning theory [1] looks for some 
answers to machine learning problems concerning sample size, such as: how many 
training examples are needed to lead to successful learning, how many reasonable 
computations are needed for successful learning, and what the estimated 
misclassifying rate is in learning. Anthony and Biggs [1] also developed a probably 
approximately correct (PAC) concept to identify classes of hypotheses that can/cannot 
be learned from a polynomial number of training examples with a reasonable amount 
of computations. Furthermore, Vapnik [2] considered a sample size small if the ratio 
of the number of training samples to the Vapnik-Chervonenkis dimensions (VC 
dimensions) of a learning machine function is less than 20. 
However, these theories focus on general machine learning with a large number 
of training samples and cannot be applied to practical cases with a small data set 
 4 
Ivănescu et al. [7] proposed a procedure to solve the limited data problem in 
batch process industries. They assumed that the job arrival moments obey a Poisson 
arrival process and utilized a bootstrap procedure to generate another 250 bootstrap 
jobs. According to their results, the procedure they proposed improved the regression 
modeling performance. 
Lanouette et al. [8] utilized neural networks to evaluate various modeling 
strategies where only a small experimental data set is available. This research mainly 
focused on selecting the optimal process model for a wood pulp factory. Because the 
experiments are expensive and time consuming, very few samples can be obtained. 
They concluded that stacked neural networks, proposed by Wolpert in 1992 [9], are 
more appealing. 
Oniśko et al. [10] proposed a method using Noisy-OR gates to learn Bayesian 
network parameters from small data sets. They tested their method on a model for 
diagnosing liver disorders, called HEPAR II. The parameters of HEPAR II were 
extracted from a valid small data set of patient records. They successfully improved 
the diagnostic accuracy. 
Rhodes and Keefe [11] analyzed the Revolutionary Organization November 17 
(N17) data, a terrorist organization, to weigh the strengths and weaknesses of the 
Bayesian approach in inferring network structure. They proved that the Bayesian 
approach can infer the topology of terrorist networks. 
Huang and Moraga [12] combined the principle of information diffusion [13] 
with a traditional neural network, called a diffusion-neural-network (DNN), for 
function learning. According to the results of their numerical experiments, DNN 
improved the accuracy of the Backpropagation Neural Network (BPN). The 
information diffusion approach partially fills the information gaps caused by data 
incompleteness via the application of fuzzy theories used to derive new samples, but 
 6 
The research is organized as follows: Section 2 presents the methods, including 
the modified extreme value theory and the area membership value; Section 3 briefly 
states the physical meanings of the real data and input factors of the ANN model; and 
Section 4 offers a numerical example of the proposed method. Computational results 
are provided in Section 5, and conclusions are mentioned in Section 6. 
 8 
Step 10. Use the synthetic samples obtained in Step 9 and the real samples 
obtained in Step 1 together ( totally, this example set has 105 samples) also translated 
to area membership values, to train the ANN. 
Step 11. Obtain another sample set and calculate the corresponding area 
membership values as the testing samples to validate the trained ANN. 
Step 12. Repeat Steps 1 to 11 several times (say 10 times) and calculate the scale 
average error rate. 
Step 13. Repeat Steps 1 to 12 with different training data set scales. 
All the methods needed in the proposed procedure are expressed as follows: 
2.1 The Pearson product-moment correlation coefficient 
The Pearson product-moment correlation coefficient, or correlation coefficient, 
indicates the strength and direction of a linear relationship between two random 
variables x  and y . In other words, it refers to the departure of two variables from 
independence. It is obtained by dividing the covariance of the two variables by the 
product of their standard deviations and is calculated as follows: 
 
( , )
ˆ ˆx y
Cov x y
r
s s
 , (1) 
where ( , )Cov x y  is the covariance of x  and y  and ˆxs  and ˆys  are the 
standard deviations of  x  and y , respectively. 
2.2 The modified extreme value theory 
Traditionally, ANN has been commonly used to derive management knowledge 
from real empirical data. However, a considerable quantity of samples is needed in the 
learning process to establish a robust network. In practice, collecting data from pilot 
runs is expensive and time-consuming, and consequently, the available data is often 
insufficient for training an ANN. Insufficient data usually produces imprecise system 
information and presents two major problems: the nature of the population domain 
 10 
where X  is the sample mean, and s  is the sample standard deviation. 
When equations (2), (3) and (4) are combined, the GEV can be shown as 
follows: 
 
6
0.450041
, , exp
E s
X sH e

  
 
   
 
 
 
 
  
  
 
. (5) 
Accordingly, the extreme value will be: 
  6 0.450041 ln( ln( ))E s X s H

       . (6) 
In this research, since we use the extreme value as the domain limits of 
population, that is, the two data range limits being the left end a  and the right end 
b , the H  value should approach zero. Consequently, in equation (6), a very small 
number, 2010 , is set in order for H  to be the probability of the extreme value. 
2.3 The estimation of extreme values 
The procedure is described in detailed steps as follows: 
Step 1. Let min  and max  be the minimum and maximum values of the data 
set, respectively. Separate the samples into two parts: one contains 
samples smaller than   2maxmin  ; the other one contains samples 
greater than   2maxmin  . 
Step 2. Transfer the sample values into iR  and iL  for the samples greater or 
smaller than   2maxmin  , respectively. 
Using the following equation: 
 2 2
2 2
i i i
i i i
min max min max
R X for X
min max min max
L X for X
 
  
 
  
. (7) 
Let R  and Rs  be the average and standard deviation of iR , and let 
 12 
  
2
ˆ
2
i ix y
n




 , (12) 
where iy  is the predicted value of linear regression with ixX ; that is, 
i iy x   , 
1
n
i
i
x
x
n


 is the sample mean, ˆ  is the root mean square error of 
linear regression and n  is the sample size. 
2.5 Production of synthetic samples 
After the domain range, a  and b , of the input factors and prediction intervals 
of other factors have been estimated, sn  synthetic samples are randomly produced 
within these ranges. These randomly produced samples are defined as synthetic 
samples. Though it requires further research, according to our limited experience, 100 
artificial samples are sufficient for training most of the ANNs. 
These synthetic samples, however, cannot be used directly to train the ANN, 
because the samples are picked up randomly (uniformly distributed) and cannot 
precisely present the complete information of the data distribution. Therefore, the 
value of the diffusion function is calculated and added to each of the samples in order 
to reflect the importance level of each chosen sample. 
The traditional ways to produce samples are based on producing them according 
to the data distribution. These methods usually produce more samples within the area 
of highest probability, as Li et al. [5] did. Nevertheless, in this research, there is no 
distribution information acquired, so we rate the importance level of the samples 
using fuzzy membership technique. We assume the data set is in a common 
membership function,  , as shown in Figure 1. 
 14 
 
Figure 2:  The area membership values 
Eventually, 100 area membership values are obtained and taken as the training 
data for the ANN. 
a  
2
min max
 16 
size along with gas and measuring the surface area through the amount it 
absorbs. 
(b) Particle size distribution-90th percentile (PSD-90): In a unit of time, the 
ninetieth percentile of the size of the diameter of the flow through ceramic 
powder. 
(c) Particle size distribution-50th percentile (PSD-50): In a unit of time, the 
fiftieth percentile of the size of the diameter of the flow through ceramic 
powder. 
(d) Particle size distribution-10th percentile (PSD-10): In a unit of time, the 
tenth percentile of the size of the diameter of the flow through ceramic 
powder. 
(e) Moisture content (Mois): Measure the moisture content quality of the 
ceramic powder after toasting at 130oC for an hour. 
(f) Sintering temperature (Sinter Temp): The temperature in the stove while 
sintering in the laboratory. 
(g) The K  value (K): The K  value of samples acquired from the laboratory 
at 20℃. 
(h) Dissipation factor (DF): Denotes the portion of the total energy in the 
capacitor that is lost as internal heat or the ratio of energy dissipated to the 
energy stored. A large DF will cause damage to the MLCC, and the DF of 
the ideal capacitor is equal to 0. 
(i) Temperature characteristic (TC-min): The minimum change of capacitance 
over the specified temperature range, governed by the specific dielectric 
material. 
 18 
4. The Numerical Example 
In this paper, a problem is solved using the data sheet provided by a 
manufacturer of MLCC. There are 44 samples in total, only 5 samples are used to 
simulate the company in the beginning stage of production for training the neural 
network and the rest of the samples for validation. The reasons for doing this are 
mentioned in the beginning of this case study. It is not known whether or not the 44 
samples are enough to build a robust model. In fact, 5 samples will be added to the 
training set each time to represent each of the manufacturing stages. The 5 samples fit 
our definition of an insufficient number of training samples, and thus it is difficult to 
build up a robust forecast neural network. 
To explain the procedure in detail, a total of 44 samples were obtained from the 
manufacturer, shown in Table 1. Among them, a specific number (5 to 30, in 
increments of 5) of samples were randomly selected as the training samples, and the 
rest were used as the testing samples for evaluating the average learning error rates of 
the ANN. Thus the experimental scales in this study are 5, 10, 15, 20, 25 and 30 
training samples. The following is an example of the process with 10 training samples, 
and the procedure is depicted in steps: 
 20 
Step 1. Select 10 samples randomly from Table 1 as the training data for training 
the ANN. The selected data set is listed in Table 2. 
Table 2:  The 10 training samples selected randomly from Table 1 
No. SA 
PSD 
-90 
PSD 
-50 
PSD 
-10 
Mois 
Sinter 
temp 
K DF×10-4 
TC 
-min 
TC 
-max 
TC 
peak 
D-50 RK 
5 3 1.97 0.94 0.45 0.06 1287 10286 28 -73.5 49.3 -9 0.94 16479 
7 3 1.97 0.92 0.43 0.06 1286 10268 29 -74.3 49.3 -7.4 0.92 17153 
12 3.2 2.15 0.95 0.41 0.02 1283 10797 30 -74 43.5 -6.5 0.95 17292 
19 3 1.87 0.94 0.46 0.02 1300 10508 26 -74 47.1 -7.7 0.94 17537 
21 3.1 2.01 0.94 0.43 0.08 1270 11117 39 -75 43.8 -5.7 0.94 18218 
22 3.1 1.93 0.94 0.46 0.01 1292 10479 35 -74 43.9 -6.8 0.94 18181 
34 2.9 1.83 0.95 0.49 0.01 1281 11384 25 -75.6 54.1 -6.4 0.95 18623 
35 2.6 1.93 0.98 0.49 0.04 1270 11513 24 -75.6 55.2 -6.2 0.98 20156 
38 2.9 1.91 0.96 0.48 0.03 1283 11388 26 -76.4 56.1 -6.3 0.96 20230 
39 2.9 1.94 0.96 0.47 0.03 1273 12242 30 -75.4 55.3 -6.3 0.96 19044 
Step 2. Calculate the absolute correlation coefficients ( r ) of input factors with 
output. The results are shown in Table 3. 
Table 3:  The r  of the input factors and the output factor 
 SA 
PSD 
-90 
PSD 
-50 
PSD 
-10 
Mois 
Sinter 
temp 
K DF×10-4 
TC 
-min 
TC 
-max 
TC 
peak 
D-50 
r  0.6875 0.3718 0.7928 0.7102 0.2269 0.5163 0.7632 0.2661 0.9098 0.6724 0.6990 0.7928 
Step 3. Use equations (7), (8) and (9) to calculate the estimated a  and b  for 
the input factor with the biggest r . In this example, TC-min has the biggest r , and 
the min  and max  are -76.4 and -73.5, respectively. Since,  
 76.4 73.5
74.95
2 2
min max   
   , thus, 
 75, 75.6, 75.6, 76.4, 75.4iL       , 
 73.5, 74.3, 74, 74, 74iR       , and 
77.9206a    and 71.4305b   . 
Step 4. Use the data of Table 2 to calculate linear regressions between other 
factors and the input factor with the biggest r .  
The results are shown in Table 4.  
 22 
Table 7:  The random numbers selected between the lower and upper bounds for 
each factor 
 SA 
PSD 
-90 
PSD 
-50 
PSD 
-10 
Mois 
Sinter 
temp 
K DF×10-4 
TC 
-max 
TC 
peak 
D-50 RK 
random 
number 
3.1228 2.1966 0.9483 0.386 0.0754 1322.108 10074.06 31.9356 45.0738 -9.8586 0.9531 16894.02 
Step 8. Use the data in Table 2 and 6 and a  and b  obtained in Step 3 and use 
Equations (13) and (14) to calculate the corresponding area membership values. The 
RK factor is used as an output factor in the ANN, and there is no need to calculate its 
area membership value. 
The results of the area membership value for each factor are given in Table 8. 
Table 8:  The area membership value for each factor 
 
SA 
PSD 
-90 
PSD 
-50 
PSD 
-10 
Mois 
Sinter 
temp 
K DF×10-4 
TC 
-min 
TC 
-max 
TC 
peak 
D-50 RK 
area 
member 
–ship 
value 
0.1922 0.0190 -0.0337 -0.0090 0.0135 0.0210 -508.61 7.6614 0.1051 -6.5110 -0.1502 0.0037 16894.0 
Step 9. By using the rest data of Table 5 and repeating Steps 6 to 8, we can 
acquire a total of 100 synthetic samples (the determination of the optimal number of 
synthetic samples needs further study). 
Step 10. Use the 100 synthetic samples obtained in Step 9 and the 10 real 
samples listed in Table 2, with the translated area membership values, to train the 
ANN. 
The RK value in Table 8 is the value assigned to the output node of the ANN; the 
others are the inputs of the ANN. Although other researchers, such as Amirakian and 
Nishimura [17] and Wang et al. [18], provided algorithms to determine the number of 
hidden nodes and hidden layers, the number of hidden nodes and hidden layers are 
believed to differ case by case. Furthermore, their algorithms did not focus on 
insufficient training sample cases. In this study, the optimal structure of the ANN is 
determined by the Evolutionary Optimizer tool from Pythia software. Note that any 
 24 
 
1
RK
RK
m
i i
i i
output of network
m



 (15) 
where m  is the number of the samples in validation set, and 1, 2, ,i m  . m  
equals 34 in this example. 
The RK and output of network  values are shown in Table 9, with an average 
error rate of 0.0519. 
Table 9:  The 34 RK and output of network  values 
RK 18624 17427 16935 16662 17577 18230 18901 17781 
output 18106.5028 19393.1353 17878.1707 16080.9939 17813.8864 17979.6814 17865.5855 17757.5944 
RK 15201 16066 16640 16603 17187 17388 17747 19840 
output 17115.8807 18177.7634 17485.0886 17388.6315 18945.2143 18583.0101 18518.0704 19835.6455 
RK 17286 16606 17052 18092 17112 18527 18030 17145 
output 18352.9060 18245.6267 17597.2259 19025.7240 15516.1374 18687.0558 17961.7023 17137.3799 
RK 20164 16642 17047 17308 17635 16681 16188 18302 
output 17459.5878 17575.3539 17048.9697 17864.4626 18036.5806 18383.6151 16483.6360 17819.4739 
RK 17287 17687       
output 16412.5591 19258.5133       
Step 12. Repeat Steps 1 to 11 ten times and calculate the scale average error rate. 
The number of repetitions was determined based on our limited experience. The 
analysis time required 3-5 minutes to calculate the average error rate for one 
repetition from Step 1 to 11 with 30 training samples using the proposed procedure on 
a AMD Athlon 64 X2 5000+ 2.6GHz computer with 2GB Ram. This analysis time is 
acceptable for most MLCC manufacturers. 
Step 13. Repeat Steps 1 to 12 with different training data set scales (from 5 to 
30). 
 26 
 
Figure 3:  The comparisons of average error rates from the computational results 
 28 
Obviously, no matter what the average or standard deviation of error is rate on 
each training data set scale, the proposed procedure results in a significant 
improvement of the ANN. Additionally, according to the p-value in Table 10, the 
average and standard deviation of the error rates are significantly different where the 
significance level equals 0.05. That is, the proposed procedure provides a better 
solution for small sample set problems. 
 30 
References 
[1] M. Anthony, N. Biggs, Computational Learning Theory, Cambridge University 
Press, UK, 1997. 
[2] N.V. Vapnik, The Nature of Statistical Learning Theory, Springer, New York, 
2000. 
[3] P. Niyogi, F. Girosi, P. Tomaso, Incorporating prior information in machine 
learning by creating virtual examples, Proceeding of the IEEE, 1988, pp. 
275-298. 
[4] D.C. Li, Y.S. Lin, Using synthetic sample generation to build up management 
knowledge in the early manufacturing stages, European Journal of Operational 
Research. 175 (2006) 413-434. 
[5] D.C. Li, C.S. Wu, T.I. Tsai, F.M. Chang, Using Mega-Fuzzification and Data 
Trend Estimation in Small Data Set Learning for Early FMS Scheduling 
Knowledge, Computers & Operations Research. 33 (2006) 1857-1869. 
[6] J.S.R. Jang, ANFIS: Adaptive-Network-based Fuzzy Inference Systems, IEEE 
Transactions on Systems, Man and Cybernetics. 23 (1993) 665-685. 
[7] V.C. Ivănescu, J.W.M. Bertrand, J.C. Fransoo, J.P.C. Kleijnen, Bootstrapping to 
solve the limited data problem in production control: an application in batch 
process industries, Journal of the Operational Research Society. 57 (2006) 2-9. 
[8] R. Lanouette, J. Thibault, J.L. Valade, Process modeling with neural networks 
using small experimental datasets, Computers & Chemical Engineering. 23 
(1999) 1167-1176. 
[9] D. Wolpert, Stacked generalization, Neural networks. 5 (1992) 241-259. 
[10] A. Oniśko, M.J. Druzdzel, H. Wasyluk, Learning Bayesian network parameters 
from small data sets: application of Noisy-OR gates, International Journal of 
Approximate Reasoning, 27 (2001) 165-182. 
