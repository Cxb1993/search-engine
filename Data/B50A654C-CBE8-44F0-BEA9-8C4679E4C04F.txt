2 
 
Adopting CUDA can link CPU and GPU together. The program will be switched 
between two parts, host and device. In the host part, CPU will handle all the work and the 
program will run as a normal C/C++ application. While a kernel function is called, the 
program will be switched to the device part, and GPU will be in charge of data processing. All 
data needed should be transferred into GPU’s memory before calling a kernel function, and 
those essential data should be transferred back to the main memory of CPU later. A sketch of 
the processing flow of CUDA is shown in Fig. 1[3]. 
 
Fig. 1. Processing Flow of CUDA. 
3. Synchronization Errors 
Every wireless communication system has synchronization issues, and it is especially 
critical for an OFDM system. In theory, the frequencies of transmitter and receiver should 
remain exactly the same, but since transmitter and receiver have independent oscillator 
sources, the real frequencies generated are highly possibly not equal. Therefore 
synchronizations are needed, or data received cannot be interpreted correctly. 
Since transmitter and receiver have independent oscillator sources, the frequencies used 
to upsample and downsample subcarriers are not the same. Such frequency offsets result in 
ICI, which heavily damages the orthogonality between subcarriers, and the communication 
4 
 
methods we adopt can be divided into two phases, as shown in Fig. 3. 
 
 
Fig. 3. Parallelizing Fractional and Integer CFO Estimations. 
The next part is the compensations of CFO and SCO. Though CFO and SCO are 
compensated in time-domain and frequency-domain, respectively, but their mechanisms are 
similar, which use Derotators. A Derotator is simply a complex multiplier, rotating the 
complex-valued input by a phase [5], which is the negative of the offsets. But there is 
something that makes the compensations complex, which is the phase shift. The values 
compensated to each of the data signals are different, so they need to be calculated 
individually before the compensations. Again we divide this process into a two-phase 
parallelization, which is shown in Fig. 4. 
6 
 
There are data dependences between symbols because the offsets keep on changing 
during the transmission. So at the end of processing a symbol, the residual CFO and SCO for 
the next symbol have to be estimated correctly. Both Residual CFO and SCO Estimations 
need a coefficient, ߦ, which can be obtained through JWLS Estimator (Joint Weighted Least 
Squares Estimator) [7]. Fig. 6 illustrates how to parallelize the calculations of JWLS 
Estimator. 
 
 
Fig. 6. Parallelizing JWLS Estimator. 
4.3 Optimizations 
By analyzing the initial results after all works in 4.2 are done, we find out that there is a 
bottleneck about I/O traffic which will strictly set a limit to the system performance. Referring 
to Fig. 1-3, before a CUDA program switches the activity to the device (GPU), all data 
required for the calculations of the kernel functions have to be copied to the memory of the 
GPU. And the results are copied back after all the calculation processes are done. Such 
procedure is followed by the CUDA program writing in our work, which is illustrated in Fig. 
7, but a problem occurs when the data being transferred exceed a certain level. 
8 
 
5. Experiment Results and Analysis 
5.1 Initial Results after Parallelizing 
Here we give the results after the parallelizing, which are the results at the end of 
Section 4.2. Fig. 9 shows the relationship between the runtime and the number of symbols, 
with different numbers of users. Fig. 10 shows the corresponding symbol time. 
 
 
Fig. 9. Runtimes of Different Numbers of Users, Parallelized. 
 
 
Fig. 10. Symbol Times of Different Numbers of Users, Parallelized. 
10 
 
5.2 Results after Pipelining 
We come up with a pipelining method to improve the performance of the system, as 
described in Subsection 4.3. It can nearly remove the saturation condition, and significantly 
improve the performance at the same time. Fig. 12 shows the relationship between the 
runtimes and the number of symbols, with different numbers of users. Fig. 13 shows the 
corresponding symbol times. 
 
 
Fig. 12. Runtimes of Different Numbers of Users, Pipelined. 
 
 
Fig. 13. Symbol Times of Different Numbers of Users, Pipelined. 
12 
 
 
 
Fig. 14. (c) Synchronizer Runtime Comparison between CPU and GPU, 100 Symbols. 
Table 1. Synchronizer Runtime Speedup Ratio between CPU and GPU. 
Users 1 5 10 25 50 60 
Speedup 4.076 20.811 39.707 78.109 109.798 125.009 
 
Table 2. Comparison between Requirements and Final Results. [8][9] 
Parameters Maximum Users Symbol Time 
Requirements 56 128μs 
Final Results 60 122.464μs 
 
Since the synchronizer on GPU is mainly to parallelize users and subcarriers, the 
speedup ratios between the performance of the CPU and the GPU increase greatly as the 
number of users increase. When there are 60 users at a time, the synchronizer on GPU is 
faster than the one on CPU by 120 times. Even if there is only one user, a speedup ratio of 4 
14 
 
Reference 
[1] Ming-Che Hsieh, “DSP Software Implementation of an IEEE 802.16-2004 Baseband 
Receiver,” Master Thesis, Dept. of Electrical Engineering, National Taiwan University, 
Taipei, Taiwan, Jun. 2010. 
[2] Javier Gozalvez, “The European Union Backs the DVB-H Standard,” IEEE Vehicular 
Technology Magazine, Jun. 2008. 
[3] nVidia. 2010. CUDA Programming Guide, version 3.1. 
[4] J. J. van de Beek, P. O. Borjesson, “A Time and Frequency Synchronization Scheme for 
Multiuser OFDM,” IEEE Journal on Selected Areas in Communications, pp. 1900-1914, 
1999. 
[5] T.-D. Chiueh, and P.-Y. Tsai, OFDM Baseband Receiver Design for Wireless 
Communications. Taiwan, John Wiley & Sons, Inc., 2007. 
[6] nVidia, “CUDA CUFFT Library,” NVIDIA Developer Technology, 2007. 
[7] P.-Y. Tsai, H.-Y. Kang, and T.-D. Chiueh, “Joint Weighted Least Squares Estimation of 
Frequency and Timing Offset for OFDM Systems over Fading Channels,” in Proc. of 
IEEE Vehicular Technology Conference, vol. 4, pp.2543-2547, April 22-25, 2003. 
[8] Clint Smith and John Meyer, 3G Wireless with WiMAX and Wi-Fi: 802.16 AND 802.11, 
McGraw-Hill Professional Engineering, 2004. 
[9] IEEE Computer Society and IEEE Microwave Theory and Techniques Society, “IEEE 
Std 802.16-2004,” IEEE Standard for Local and Metropolitan area networks–Part 16: 
Air Interface for Fixed Broadband Wireless Access Systems. 
 
 
Publication List: 
[1] Y. P. Chen, S. H. Hsieh, P. H. Cheng, T. N. Chien, H. S. Chen, J. J. Luh, J. S. Lai, F. Lai, 
and S. J. Chen, “An Agile Enterprise Regulation Architecture for Health Information 
Security Management,” Telemedicine and e-Health, Vol. 16, No. 7, pp. 807-817, 
September 2010. 
[2] T. N. Chien, S. H. Hsieh, P. H. Cheng, Y. P. Chen, S. J. Chen, J. J. Luh, H. S. Chen, and 
J. S. Lai, “Usability Evaluation of Mobile Medical Treatment Carts: Another Explanation 
by Information Engineers,” Journal of Medical Systems, pp. 1-8, September 2010. 
[3] J. C. Lin, M. J. Fan-Chiang, S. J. Chen, M. Schulte, and Y. H. Hu, “Complexity Analysis 
of Systematic Spectrum Sensing for Cognitive Radio,” The 10th Software Defined Radio 
Technical Conference (SDR’10), Washington DC, USA, Nov-Dec 2010. 
[4] C. Yu, C. J. Chen, M. H. Yen, P. A. Hsiung, and S. J. Chen, “A Memoryless Viterbi 
Decoder for OFDM Systems,” 2010 VLSI Design/CAD Symposium, KaoHsiung, Taiwan, 
ROC, August 2010, pp 45-48. 
[5] C.-S. Lin, W.-L. Liu, W.-T. Yeh, L.-W. Chang, W.-M. Hwu, S.-J. Chen, and P.-A. 
Hsiung, “A Tiling-Scheme Viterbi Decoder in Software-Defined Radio for GPUs,” The 
7th International Conference on Wireless Communications, Networking, and Mobile 
Computing, Sep. 2011 (Accepted). 
 
 2
design of SDR in our previous NSC project).  
 
 
Figure 1: Diagram of Rigel processor. 
 
 
Figure 2: SDR MAC layer and Rigel chip in the CR platform. 
 
Simultaneously, we would like to perform research on the VLSI design of a Parallel Multi-Core and 
Low-Power Processor. After the tape-out, we will run some benchmarks to verify the performance and the 
power consumption. Since Rigel is a scalable architecture, the performance can be improved by using more 
cores in parallel computation. There are statistics on the scalability simulation of Rigel in TSMC 45nm 
 4
parallel computations. Out of the clusters, there are PLL and JTAG for boundary scan. And the memory 
controller is used for SRAM access. 
 
 
Figure 3: Function diagram of our Rigel design. 
 
We will put our chip design on a platform which is called MorPack (Morphing Package) as shown in 
Figure 4. This MorPack platform is developed by CIC, which contains six bare dies connected by ball grid 
array. The accelerator will be available from our Rigel design. And we can use it as a computation platform 
for the baseband signal processing. With so many benefits of our Rigel chip design, it will be an outstanding 
accelerator design for the computation platform. 
 
 
Figure 4: MorPack with our design on the top layer. 
 
The Morphing package reserves 16 I/O pins for the accelerator. So, we can have the pins for VLSI 
testing and other flexible purposes on our accelerator. The ARM CPU, Northbridge, Southbridge, and 
Accelerator are connected together with a tri-state AHB bus. On the south bridge, there are functions of 
general-purpose I/O port, universal asynchronous receiver/transmitter, watch dog, pause/remap, timers, and 
interrupt controller. As for the north bridge, there are arbiter/decoder, VGA, ROM, SRAM, NOR Flash and 
 6
 
Table 3: Elapsed time of operations per mobile WIMAX frame. 
 
Reference: Kim, J., Seungheon Hyeon, Seungwon Choi, “Implementation of an SDR System Using Graphics 
Processing Unit,” IEEE Communications Magazine, Vol.48, No.3, March 2010. 
 
2. VLSI Testing for Multi-Core Chip. 
 Now, the Verilog codes of the pipeline, clusters, and cores are almost finished. But it might have some 
bugs in the source codes. We will have to generate some benchmarks to verify the source codes, or it could 
have the possibility to lead the tape-out to fail. After the verification of the source codes, we will use some 
tools to compile and synthesize our design. Simultaneously, we have to check the conformance to the timing 
constraints before and after Place and Route, and we need to verify the timing formality and integrity. Besides, 
we are going to build a boundary scan system, JTAG, in our chip. This way, we could detect problems in the 
chip after the tape-out. Especially since we use a multi-core architecture, we have to consider the parallel 
computation among the cores, and scan the value in the L1 cache, cluster cache, and memory outside the 
clusters, respectively. Verification and testing on a multi-core chip will take more effort than on an ordinary 
design. 
 
3. A New Structure for Data Streaming. 
  In the present structure, data is streaming from Input/Output, and passing from the Southbridge with I/O 
port to the main memory in the Northbridge. If the CPU makes an indication to move the tasks from memory 
to compute in the accelerator, the data would be streamed to the accelerator. After the computation, the results 
would be streamed back to the main memory. Figure 6 shows an example. 
There is an issue if we want to build a high-speed and good-performance computation platform. It will 
waste time on data streaming between the main memory and the accelerator. For a data rate of 54 Mbps in 
wireless LAN 802.11a/g, the time taken to transfer data from main memory to GPU is about 0.6 seconds. This 
will significantly lower the performance. Our computation is applied for baseband data processing. So it 
would better be a real-time structure, no need to first stream the data to the main memory before streaming it 
to the accelerator (Figure 7). During the signal processing in an SDR receiver, the data rate of the first block 
in the PHY layer is 2.88Gbps (= 350Mbytes/sec). If our accelerator could exceed the performance of 350 
Mbytes/sec, it is possible to use the new structure for our design to improve the performance. 
 
 
5.4x 
3.9x 
 8
 
REFERENCE 
 
[1] J. H. Kelm, D. R. Johnson, M. R. Johnson, N. C. Crago, W. Tuohy, A. Mahesri, S. S. Lumetta, M. I. Frank, 
and S. J. Patel, “Rigel: An Architecture and Scalable Programming Interface for a 1000-Core Accelerator,” 
in Proc. of International Symposium on Computer Architecture, pp. 140-151, June 2009. 
[2]. Kim, J., Seungheon Hyeon, Seungwon Choi, “Implementation of an SDR System Using Graphics 
Processing Unit,” IEEE Communications Magazine, Vol. 48, No.3, PP. 156-162, March 2010. 
 
 
 10
 
軟體定義無線電之維特比解碼器在圖形處理器的切割方案 
A Tiling-Scheme Viterbi Decoder in Software-Defined Radio for GPUs 
 
摘要 
在這份報告中，我們提出一種針對於軟體定義無線電中維特比解碼器的平行設計。我們的方法分
割-處理的原則藉由切割解碼序列、執行各自獨立推斷的解碼、並且合併部分候選路徑成為最後的路
徑。對於各自獨立的維特比解碼之最佳路徑是由平行地在格子結構間計算其漢彌距離進而選出。 
我們的方法在nVidia 8800 GTX的平台上展現出比在 Intel Core 2 2.4Ghz中央處理器平台上的一般
循序 C 語言實作有著 14.6 倍的加速。而且，比較於之前已存在根據圖形處理器的實做，我們的方法勝
出 2.5 倍。 
 
關鍵字：軟體定義無線電，維特比解碼器，圖形處理器，統一計算裝置架構 
 
 
Abstract 
In this report, we propose a parallel design of Viterbi decoder for Software-Defined Radio (SDR). Our 
method implements a divide-and-conquer approach by tiling decoding sequences, performing independent 
speculated Viterbi decoding, and merging partial candidate paths into the final path. For each independent 
Viterbi decoding, the best path is selected by calculating Hamming distances trellis-by-trellis in parallel. 
Our method shows up to 14.6x speedup on an nVidia 8800 GTX over a sequential C implementation 
on a 2.4GHz Intel Core 2 CPU. Also, compared with the existing GPU-based implementation, our method 
outperforms up to 2.5x. 
 
Keywords: Software-Defined Radio (SDR), Viterbi Decoder, Graphics Processing Units (GPUs), Compute 
Unified Device Architecture (CUDA) 
 
 
 
 
 
 
 
 
 
 
 
 12
CUDA provides a programming interface [4] to utilize the highly-parallel nature of GPUs and hide the 
complexity of controlling GPUs. A programmer specifies a kernel as a series of instructions and a data set, 
then the kernel is executed by thread blocks which consists of a number of threads with unique IDs. For 
independent executions among different thread blocks, synchronization is required to avoid the race 
condition. Note that it is expensive for synchronizing thread blocks by comparing with synchronizing 
threads in a thread block. 
 
 
Fig. 1. CUDA Architecture. 
 
At runtime, thread blocks are distributed to streaming multiprocessors (SMs) as shown in Fig. 1. 
Precisely, a thread block is divided into warps of 32 threads. Each warp is executed by an SM within 4 
cycles if the input data is cached for computing. For utilizing an SM efficiently, the threads of each warp 
should execute the same instructions. Otherwise, it will be executed sequentially with overhead for stalling 
if threads execute different instructions. This scenario is called warp divergence. 
 
Table I. Comparisons of Memory Type. 
 
 
As to the type of memory as listed in Table I, each kind of memory is designed from different read or 
write speeds, hardware, and programming scopes. Global memory, as a bridge between host and device, is 
shared among all thread blocks. Constant and Texture memory are read-only memories for faster memory 
accesses. In a thread block, the most essential component is shared memory. Being different from registers 
 14
FECs, after partitioning, we have NF-by-NC independent chunks in total. Each chunk will be traversed 
independently for storing candidate paths by Hamming distance with corresponding input sequences, and 
then candidate paths between neighboring chunks are selected for merging. The decoding sequence will be 
obtained after merging all NC chunks of a FEC block in parallel. 
 
 
Fig. 3. Concept of TVDA. 
 
TVDA consists of two phases: First, as shown in Algorithm 1, Viterbi decoding will process every 
chunk for local distance by each thread block in parallel. Second, as shown in Algorithm 2, chunks will be 
merged for obtaining global distance. 
The following four components are for calculating the Hamming distance with corresponding input: 
 State transition matrix (ST): stp,q captures the connection of state transition between State p in 
previous trellis and State q in current trellis. stp,q = 1 represents the existence of a state 
transition, stp,q = 0 otherwise. 
 Hamming distance matrix (HD): hdp,q,c represents the Hamming distance between State p in 
previous trellis and State q in current trellis with a corresponding input c. 
 Local distance:  represents the accumulated Hamming distance from p in Trellis (i-1) and 
State q in Trellis i. 
 Global distance:  represents the accumulated Hamming distance of State p in last trellis of 
Chunk i. 
Every state in the first trellis of the current chunk is a candidate connector from the previous chunk. To 
achieve the global MLE when merging chunks, TVDA takes every state in the first trellis as a start state 
except the first chunk of every FEC block. Note that when NC is equal to 1, our method is reduced into [3]. 
In that case, the merging phase does not need to be executed. 
 
 16
 
Fig. 4. Testing Scenario. 
  
 
Fig 5.  BER Performance. 
 
Figure 6 illustrates the speedups of Kim’s parallel VDA [3] on a GPU and our TVDA on a GPU over a 
sequential VDA on a CPU. Our method is 1.4x to 14.6x faster than sequential CPU implementation. 
Compared with Kim’s work, ours shows up to 2.5x speedup on a GPU. Particularly, when NF is small, our 
method performs much well over Kim’s. That is because our method can extract more parallelism within a 
FEC. While Kim’s method only gets parallelism from independent FECs, our method has parallelism from 
both independent FECs and chunks in FECs. 
Note that the number of received FECs which are to be processed in a time depends on the type of 
wireless application. For one kind of applications such as voice call service, the latency must be short to 
maintain Quality-of-Service (QoS) for a user. That is, small amount of FECs need to be processed in a 
 18
[5]  A. Viterbi, “Convolutional Codes and their Performance in Communication Systems,” IEEE 
Transactions on Communication Technology, Vol. 19, No. 5, pp.751– 772, 1971. 
[6]  A. Viterbi, “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding 
Algorithm,” IEEE Transactions on Information Theory, Vol. 13, No. 2, pp. 260–269, 2002. 
[7]  M. Wu, Y. Sun, and J.R. Cavallaro, “Reconfigurable Real-time MIMO Detector on GPU,” in 
Proc. of the 43rd Asilomar Conference on Signals, Systems and Computers, pp. 690–694, 2010. 
[8] M. Wu, Y. Sun, S. Gupta, and J.R. Cavallaro,”Implementation of a High Throughput Soft MIMO 
Detector on GPU,” Journal of Signal Processing Systems, Vol. 64, No. 1, pp. 123-136, 2010. 
 
 
99年度專題研究計畫研究成果彙整表 
計畫主持人：陳少傑 計畫編號：99-2220-E-002-041- 
計畫名稱：行動醫療感知無線電平台--子計畫三：感知無線電之雲端可調式推論與學習框架(1/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 0 100%  
博士生 3 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
