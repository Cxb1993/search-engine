characterizing features of the observed data. Second, 
they can solve problems which are stalled by high 
dimensionality. In this paper, the first advantage is 
applied to the selection of relevant features and the 
second is employed to generate the multivariate 
classifier. Experimental results show that our model 
can significantly improve classification training 
time by combining a compact subset of relevant 
features without the loss of accuracy in multi-class 
classification problems. In addition, the 
discrimination degree of our classifier outperforms 
other conventional classifiers (C4.5, CART, SVM, 
NaiveBayes). 
英文關鍵詞： Multivariate analysis, multi-class problems, feature 
evaluation, feature selection, feature extraction, 
inductive learning 
 
1. 前言 
藉由一組獨立變數(a subset of independent variables)對某一範疇的相關變數(categorical 
dependent variable)進行資料間相關性探討與分析，運用探勘出來的資訊於資料分類處理，甚
至利用獨立變數預測可能的相關變數結果，我們稱之為分類處理(classification processing)，
日常生活許多常見的應用問題均需要透過分類處理；舉例來說：網頁分類(web page 
classification)有助於維護網頁目錄(web directories)與進行焦點內容延伸(focused crawling)；垃
圾信偵測(web spam detection)可以過濾未經請求的訊息(unsolicited messages)與有效提高搜尋
引擎搜尋品質；異常偵測 (anomaly detection) 可以從資料中發現不被批准的模式
(non-conforming patterns)，這些 pattern 包含偏離現象(outliers)、不協調的觀察資料(discordant 
observations)、例外狀況(exceptions)、畸變現象(aberrations)等；醫療診斷(medical diagnosis)
可以從他人歷史病例與個人病歷中歸納出值得注意的徵狀；另外，行動商業行為(mobile 
commerce behavior)，詐欺偵測(fraud detection)，破產預測(bankruptcy prediction)[38]，與犯罪
行為分析(crime activity analysis)等都需要仰賴許多各種形式的資料分類處理。如何有效地管
理不斷快速增加的電子資料，並且利用它們找到解決問題的方法，甚至進一步將成功經驗轉
換為企業或商業智慧(business intelligence)，已經成為現代各行各業關心的議題。由於行動雲
端運算的蓬勃發展，資訊科技使用者將隨時隨地隨處(anytime anywhere any-devices)輕易取得
所需的各種資料與應用程式，我們可以預期未來個人或企業組織對即時知識與智慧
(immediate knowledge & intelligence)的需求亦將獲得相當的重視。 
 
2. 研究目的 
分類問題依照分類目標數量(the number of objectives)可區分為「單目標分類問題」與「多目
標分類問題」，單目標分類問題關切資料分類結果是否隸屬於一種歸納結果，舉例來說：判
定個人是否罹患某種特定疾病(乳癌,心臟病,肺癌)，判定收到的資料訊息是否有害，判斷銀行
是否可能導致破產，甚或判定個人的政治傾向都是單目標分類問題；若資料經過分類處理之
後，產生多種歸納結果(或多個類別)，即為多目標分類問題，舉例來說：人口普查與分類，
消費者行為與消費能力的區分，動植物的分類，音訊與聽力的識別等。另外，分類問題依照
分類器使用的變數方式又可區分為「單變量分類法」與「多變量分類法」，單變量分類法運
用單因子產生單一維度準則(single dimensional criteria)，僅專注於特定角度觀察資料的差異
性，若使用不恰當的因子，將造成不良的分類品質，所以事先的因子評估(variable evaluation)
對分類品質有決定性的影響；再者，大型資料集(large scale dataset)通常需要進行許多處理回
合才能完成所有分類工作，因此在不同的分類處理過程中，因子評估必須重新進行以確保分
類準則的適當性，模式簡單性(model simplicity)是單變量分類法的最大優點。多變量分類法
則整合多個變數以產生複維度準則(multi-dimensional criteria)，它可以整合不同的角度觀察資
料的差異性，挑選有效的單因子並統整起來，其產生的加乘效果(synergistic effect)將快速提
升分類品質，雖然多變量分析會花費許多運算時間與成本，但卻可以有效減少許多處理程
序，如何在多變量分析與處理程序之間取得一個平衡點，是處理多目標分類問題的重要課
題，也是本研究的目的。運用單變量分類法於處理多目標分類問題容易造成許多弊端，我們
將它們整理如下： 
1. 隨著目標數量(the number of objectives)的增加，分類正確率(classification accuracy)難以提
資料所屬類別有 1, 2, 8, 以及 9，B群組則包含類別 1, 2, 3, 以及 4，類似的類別有相近的類
別編號，群組 A 與 B 使用相同的資料出現機率值 (2/6, 1/6, 1/6, 2/6) 進行 entropy 或 Gini 
index 的量測，因此 A 與 B 均獲得相同的 entropy 測量值與 Gini index。然而，群組 B 中的
資料差異性明顯低於群組 A，所以群組 B 較群組 A 擁有較好的分類品質，此一現象可由統
計變異數輕易偵測出來(variance(A)>variance(B))。 
一般而言，整合多變數時，變數之間的相關性可能對資料分類的處理產生加乘效果；加
乘效果可以提高資料分類的準確度，加乘效果同時加速資料分類的效能；表 1 以 10 筆個人
通勤資料集為例，x1 為性別，x2 為車輛的擁有情況(x2=0表沒有車輛, x2=1表一輛, x2=2表兩
輛)，x3 為旅遊成本，x4為收入的等級，x5 為相依變數(or 目標屬性)分為三個交通類別：bus, car 
以及 train。屬性 x1, x2, x3, x4 的 Gini indexes 分別為 0.6, 0.45, 0.16, 0.36。由於屬性 x3 擁有
最低測量值，使用收入等級對 10 筆資料進行分類，分類結果與目標屬性 x5比較後，獲得最
佳的一致性(除第八筆資料外)，故我們以屬性 x3 為出發點，搜尋下一個適當特徵屬性有效協
助屬性 x3，強化分類處理以提升分類品質。 
 
表1、個人通勤資料集。 
輸入屬性 目標屬性 
x1 x2 x3 x4 x5  
male 0 cheap low bus 
male 1 cheap medium bus 
female 0 cheap low bus 
male 1 cheap medium bus 
female 1 expensive high car 
male 2 expensive medium car 
female 2 expensive high car 
female 1 cheap medium train 
male 0 standard medium train 
female 1 standard medium train 
 
表 2列出整合變數 xi 與變數 xj 的相關係數|rij|、Gini indexes乘積、|rij|×Gini(xi)×Gini(xj)、
以及 Gini(xi∪xj)。如表所示，即便屬性 x1 與 x2擁有最低的相關程度(i.e. |r12|=0.1429)，我們
仍以屬性 x3 為搜尋下一個適當特徵屬性的依據，在包含屬性 x3 的三個組合之中(x1∪x3)、
(x2∪x3)、以及(x3∪x4)，由於變數 x1 與變數 x3擁有較低的相關係數，若整合變數 x1 與變數 x3
之後，再對 10 筆資料進行分類，可獲得最低的 Gini(x1∪x3)值，表示變數 x1 可有效輔助變
數 x3，此現象可利用|rij|×Gini(xi)×Gini(xj) 觀測而得。 
 
表2、Gini index 相關測量值。 
(1) 
(xi∪xj) 
(2) 
|rij| 
(3) 
Gini(xi)×Gini(xj) 
(4) 
(2)×(3) 
(5) 
Gini(xi∪xj) 
然後，兩種常見的資料前置處理特徵選擇與特徵提取同時整合於我們的研究方法中，我
們預定發展一套啟發式選擇演算法(heuristic selection algorithm)，此演算法基於 New_Gini(或 
New_Entropy)逐步地篩選適當的特徵屬性以提供後續的多變量分析，篩選演算法的初步構想
如下： 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
在上述演算法中，相較於集合 B1，集合 B2的形成是源於將新挑選的獨立變數加入集合 
B1中(如 step 4 所示)，利用主成份分析(PCA)將集合 B1與 B2的屬性資料(attribute values)分別
進行統計分析，比較集合 B1與 B2產生的第一主成份之變異數(i.e. eigenvalue)，若 PCA(B2)－
PCA(B1)呈現明顯差異(大於門檻值 k，如 step 5 所示)，則集合 B2較集合 B1擁有較多而且較
完整的資訊，提供更有效用的分類處理，迴圈 step 6 至 step 7 的目的在挑選下一個適當的獨
立變數用以持續增強分類效果。 
當一組適合的獨立變數選定後，本研究計畫將運用主成份分析整合這些獨立變數，並且
提取第一主成份作為多變量分類器的準備，相關統計運算背景如下：假設最初資料集內有 p
個原始變數，分別記為 x1, x2, …, xp，經過主成份分析後，得到 p 個主成份 y1, y2, …, yp，其
對應特徵值分別為 λ1, λ1, …, λp，以矩陣運算表達如下： 
Y=AX 
其中 














=














=














=
ppppp
p
p
p x
x
x
X
aaa
aaa
aaa
A
y
y
y
Y
M
L
MOMM
L
L
M
2
1
21
22221
11211
2
1
                      
輸入資料: 將訓練資料庫T中的所有獨立變數收集至集合A中。 
輸出資料: 從集合A中挑選一組獨立變數，這組獨立變數擁有較好的資料分類能力。  
1. B1←φ; B2←φ.  
2. 從集合A中挑選出具有最小Gini index 值的獨立屬性α，將此屬性加入B1中。 
3. 再從集合A中挑選出另外一個獨立屬性β使得 |),(|)(Gini βαβ r× 獲得最小化，將屬性β從集合A中刪除，然後再
加入B1中。 
4. 基於相同的設計原理，再從集合A中挑選出獨立屬性γ使得 |),(),(|)(Gini γβγαγ rr ⋅× 獲得最小化，將屬性γ從
集合A中刪除，然後 B2 ← B1∪{γ}。 
5. 若 PCA(B2) －PCA(B1) > k 執行以下迴圈 
6. Begin 
        從集合A中挑選出下一個獨立屬性x 使得 |),(|)(Gini
2
yxrx
By∈
∏× 獲得最小化。 
        B1 ← B2; B2 ← B2∪{x}. 
7. End 
8. 傳回集合 B2 
集
 svmguide4 擁有最顯著的改善成果，svmguide4 使用的三種屬性數量分別為 N-=4, N=5, 以
及
 N+=6，最佳的分類準確度發生在 N=5。至於其他三個資料集 segment, places, 以及
satimage，群集分析對分類準確度帶來的效益似乎較變異性分析明顯，變異性分析的效益在
segment 與 svmguide4 呈現些微地明顯性。檢視圖 3 中所有實驗結果，多數情況下除了資料
集 wine 以外，變異性分析可以增強群集分析的效果。在資料集 wine 之中，實驗結果顯示三
種選擇策略擁有類似的準確度，因而顯示變異性分析與群集分析對分類工作沒有明顯的助
益，這種情況可能源於屬性間原本已經低度相關，所以原始資料在經過群集分析與變異性分
析處理之後沒有太多的變動。至於屬性數量的因素方面，資料集 segment 與 satimage 在R 與 
RV 情況下，使用較多的屬性可以明顯改善分類準確度。 
 
 
(a) NaiveBayes 
 
(b) C4.5 
 
(c) Simple CART 
 
(d) SVM 
圖 3. 準確度效能分析 
 
分類器的辨別能力(discrimination capability)通常採用 ROC 面積來進行量測，ROC 面積
是利用真陽率(TPR = true positive rate)與偽陽率(FPR = false positive rate)的座標圖形繪製而
形成，當曲線愈靠近座標圖形左上方時，ROC 面積愈大，這表示辨別能力愈高，ROC 面積
的數值範圍介於 0 與 1 之間。如圖 4 所示，RVC 策略挑選出來的屬性成功地將分類器的辨
別能力推進至優良程度，從資料集 places 與 svmguide4 的辨別效能看來，變異性分析與群
集分析對分類工作的助益再次獲得驗證。另一方面，從實驗結果中得知屬性數量對辨別能力
參考文獻 
[1] Föllmer, H. (1973). On entropy and information gain in random fields. Probability Theory 
and Related Fields, 26(3), pp. 207-217. 
[2] Gini, C. (1912). “Variabilità e mutabilità,” Reprinted in Memorie di metodologica statistica 
(Ed. Pizetti E, Salvemini, T), Rome: Libreria Eredi Virgilio Veschi. 
[3] Gorban, A. N., & Zinovyev, A. (2010). Principal manifolds and graphs in practice: from 
molecular biology to dynamical systems, International Journal of Neural Systems, 20(3), pp. 
219-232. 
[4] Kabira, M. M., Islamb, M. M., & Murase, K. (2010). A new wrapper feature selection 
approach using neural network. Neurocomputing, 73, pp. 3273-3283. 
[5] Kwak, N., & Lee, J.-W. (2010). Feature extraction based on subspace methods for regression 
problems. Neurocomputing, 73, pp. 1740-1751. 
[6] Lee, J. A., & Verleysen, M. (2007). Nonlinear Dimensionality Reduction, Springer. 
[7] Luh, G.-C., & Chun-YiLin. (2010). PCA based immune networks for human face recognition. 
Applied Soft Computing, in press. 
[8] Maldonado, S., Weber, R., & Basak, J. (2011). Simultaneous feature selection and 
classification using kernel-penalized support vector machines. Information Science. 
Information Sciences, 18, pp. 115-128. 
[9] Šušteršič, M., Mramor, D., & Zupan, J. (2009). Consumer credit scoring models with limited 
data. Expert Systems with Applications, 36(3), pp. 4736-4744. 
[10] Tsai, C.-F., & Hsiao, Y.-C. (2010). Combining multiple feature selection methods for stock 
prediction: Union, intersection, and multi-intersection approaches. Decision Support Systems, 
50, pp. 258-269. 
[11] Wang, C.-M., & Huang, Y.-F. (2009). Evolutionary-based feature selection approaches with 
new criteria for data mining: A case study of credit approval data. Expert Systems with 
Applications, 36, pp. 5900-5908. 
[12] X. Wu, V. Kumar, J.R. Quinlan, J.Y. Ghosh, M.H. Q., G. McLachlan, et al., Top 10 algorithms 
in data mining. Knowledge and information systems 14(1) (2008) 1－37. 
 
計畫成果自評 
研究執行成果與計畫預定目標完全相符，也順利達成預期的效果，本研究已在國際學術研討
會發表(Hung-Yi Lin, "A Robust Feature Selection Scheme for Multi-class Classification 
Problems", 2012 International Conference on Business and Information, July 3-5, 2012, Sapporo, 
Japan 請參酌研討會論文)。 
 
 
BACKGROUND AND RELAYED WORK 
To precisely identify discriminative features from others, proper data preprocessing is 
important before constructing a machine learning model. Well data preprocessing can refine 
features’ discrimination while improper data preprocessing can likely cause pivotal features fail to 
be discovered. Generally, variety makes continuous data more controversial than discrete (nominal 
or categorical) data. Variety simplification for continuous data is commonly required before 
feature evaluation. Discretization is the process of transferring continuous data into discrete 
counterparts. Discretization can preserve the data distribution similar to the raw data. However, for 
classification problems, data’s discrimination is more attractive than their distribution. 
The multiplicity of target classification variable is another issue worthy of discussion. Data 
with multiple categories burden the classifier and in turn bring more complex patterns to the 
classification task. Classifiers are thus requested to maintain powerful discriminative capability 
when confronting multi-class problems. The construction of classification models for a large 
number of competing classes [1, 3, 12, 16] has drawn many researchers’ attentions. These tasks are 
not handled well by general purpose learning methods and are usually addressed in an ad-hoc [5, 
11, 18] fashion. For example, to sift through and analyze the massive and increasingly available 
data on proteins, researchers need new machine-learning methods for protein structure prediction 
[2, 19]. There are two principal approaches for applying traditional two-class learning algorithms 
to multi-class problems. Both of them adopt the policy of separate-and-conquer. Pairwise 
classification [7, 8] turns a single k-class learning problem into k(k－1)/2 two-class problems. 
One-against-all class binarization [6, 9] transforms a k-class problem into k two-class problems 
which are constructed by using the instances of class i as the positive instances and the instances of 
classes j (j=1…k, ij ≠ ) as the negative instances. Decomposition of a single multi-class problem 
into several two-class sub-problems is problematic especially in the case when the target classes 
are ordinal. The improper handles such as separating the similar classes into distinct categories or 
merging the distinct classes into the same category can severely confuse two-class sub-problems 
and in turn largely deteriorate classification performance and accuracy rate. Multi-class 
classification problems have their own characteristics and they should have their own strategies 
toward the solution. 
 
FEATURE SELECTION SCHEME 
Figure 1 shows the entire scheme of our feature selection. It is assumed that the readers are 
familiar with the cluster analysis validated by Huang-index. Four steps including first cluster 
processing, feature evaluation, feature selection, and second cluster processing are respectively 
highlighted as follows. 
 
( )∑
=
−−
−−=
C
k
m
kijiij cxcxu
1
)1/(2
/
                                      (2) 
∑∑
==
×=
N
i
m
ij
N
i
i
m
ijj uxuc
11
/)(                                           (3) 
                                                 
This iteration will stop when ε<−+ }{max )()1( kijkijij uu , where ε is a termination criterion between 
0 and 1, whereas k is the iteration steps. This procedure converges to a local minimum or a saddle 
point of Jm. The main advantages of this algorithm are its simplicity and speed which allows it to 
run on large datasets. It minimizes intra-cluster variance, but does not ensure that the result has a 
global minimum of variance.  
Huang-index [10] provides the measure of goodness of cluster for complicated information 
systems based on PBMF-index function and rough set theory. The maximum value of Huang-index 
function not only provides the best partition, but also obtains the optimal accuracy of classification 
for the approximation sets. All features of a dataset are first clustered into groups using fuzzy 
c-means. The clustered data are then used to identify approximate regions and calculate cluster 
centers based on rough set theory. Finally, all calculated data are put into the index method to find 
the cluster validity index 
'
'
11),( C
C
C DF
E
C
CH ××=α
, where C is the designated number of clusters, 
1E is constant for a given dataset and is used only to prevent the value of the second term from 
disappearing. Finally, the value of CD′  is equal to the maximum separation distance amongst the 
centroids of all the lower approximate sets associated with the different clusters of the decision 
attribute, i.e.
ji
C
jiC
zzD ′−′=′
=1,
max
. The detailed process in determining an optimal cluster number for 
features is outlined as Fig. 2. 
 
 
Fig. 2.  Data preprocessing with cluster analysis. 
 
B.
 
Feature evaluation 
The designs of Gini impurity, information gain, and information gain ratio pay the common 
attention to data distribution. Namely, when one feature is tested, the probability distribution of 
Apply FCM on the designated 
Initialize the cluster number as 2 
Calculate the cluster validity 
Maximizing 
Start 
End 
Increase the cluster number with 1 
Yes 
No 
reduction in dimensionality with usually lower noise than the original patterns. In practice, 
the correlation matrix of the data is constructed and the eigenvectors on this matrix are computed. 
Correlation matrix can overcome the problem attributed to different measure scales. The 
eigenvectors that correspond to the largest eigenvalues (the principal components) can now be 
utilized to reconstruct a large fraction of the variance of the original data. A simple classifier 
integrating multiple feature aspects without suffering from redundancy can be achieved by PCA. 
The mathematical background of PCA is explained as follows:  
Suppose there are N features in a feature set S (i.e., S={f1, f2, …, fN}) and it consists of m 
measurements on these N features. A subset of n features is taken from S and the correlation matrix 
∑of these n features is made for PCA. Let the eigenvalues of ∑be 0)()()( 21 ≥≥≥≥ SSS nλλλ L . The 
i-th component is the linear combination which is given by niniii fafafay +++= L2211 , where 
[ inii aaa ,,, 21 L ] is the eigenvector corresponding to )(Siλ , ni ,,2,1 L= . Consequently, the 
proportion of total vaiance (i.e., explained variance) due to the i-th principal component is 
∑
=
n
j
ji SS
1
)(/)( λλ . Generally, most (for instance, more than 70%) of the total population variance can 
be featured to one or two components. In this paper, the eigenvalue corresponding to the first 
principal component is used to approximate the data variability derived from a subset of features. 
In the case that one feature has redundant information to the existing selected features, it brings 
none or only few additional data variability to the existing system and this situation is easy to be 
detected via PCA. 
In our design, the change of first eigenvalues ( 1λ  and '1λ ) is investigated between two 
successive selection rounds. If the significant difference between 1λ  and '1λ  is detected, the new 
adding feature has the tolerant redundancy to the current data system and it is identified as a useful 
feature. Otherwise, it has so little variability contribution to the current data system that it is 
classified as a serviceable feature. Users can designate the maximum round number for the 
selection process. This number is used to limit the maximum execution round of selection. If the 
variability analysis can acquire the satisfactory result before executing so many rounds, the 
selection process will cease and output the selected features. A small quantity of   1+N  features 
is suggested to be sufficient for our selection process. So, it also takes a time complexity of 
  )())(( 2 NONO ≈ in PCA as well as that in feature evaluation step. The heuristic selection algorithm 
is given in the appendix.  
 
EXPERIMENTAL RESULTS AND ANALYSES 
The five multi-class datasets used in this paper are named as segment, places, wine, 
svmguide4, and satimage. They were respectively downloaded from Statlog [15], UCI [17], or 
StatLib [4, 14] and depicted as Table 1. The number of features excluding the target class varies 
from 9 to 36 and the number of target classes varies from 3 to 7. Dataset segment has the highest 
number of classes and dataset satimage has the largest instance size and feature number of all. The 
raw data values corresponding to all features are either real or integer. The implementation of the 
proposed scheme was executed in C and Matlab programming languages performed on a 
workstation with an Intel Core 2 dual 2.4 GHz processor. 
 
TABLE I Abstract of five datasets. 
Datasets segment places wine svmguide4 satimage 
# of instances 2310 987 178 612 4435 
# of features 19 9 13 10 36 
# of classes 7 4 3 6 6 
 
 (d) SVM 
Fig. 3 Performance of accuracy. 
 
The discrimination capability of classifiers is typically measured by ROC area. The ROC area 
is directly represented by plotting the fraction of true positives out of the positives (TPR = true 
positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate). It is 
a comparison of two operating characteristics (TPR & FPR) as the criterion changes and therefore 
measures the discrimination capability of classifier. The closer the curve is to the upper left-hand 
corner of the graph, the greater the area and the higher the discrimination capability. The range of 
the ROC area is 0 to 1.  
As shown in Fig. 4, the features selected by RVC have successfully pushed classifiers’ 
discrimination capability to “Good level” or even higher. In view of the performance obtained 
from places and svmguide4, the effect brought by cluster analyses and variability analyses is 
verified once again. On the other hand, selected feature quantity has no remarkable influence on 
the experimental results. This phenomenon is consistent with the well-granted fact that only a 
limited number of relevant features are sufficient to complete the entire classification task. 
 
 
(a) NaiveBayes 
 
(b) C4.5 
 
(c) Simple CART 
8. If ψλ
λλ
≥
−
)(
)()(
1
1121
S
AA , then 2AA ← . 
9. Else 
1AA ←  and break the while loop. 
10. End While 
11. Return A 
 
Initially, the feature with the best AG evaluation serves as the first basis for next selections. 
As line 3 shows, the algorithm may keep selecting before reaching the maximum selection round 
number ϖ . A feasible ϖ  is suggested as   1+N according to the experience rule of statistical 
sampling, where N is the size of the feature space S. By means of variability analyses, lines 4-7 
select two features which have better complementary variability effect to set A. Line 8 measures 
the incremental variability ratio gained from the new adding feature. The measurement greater than 
the threshold ψ means the enhancement is so significant that the feature should be collected. 
Under such situation, another round of selection will be triggered. Oppositely, insignificant 
enhancement will cause the selection process to be terminated as shown in line 9. Line 11 outputs 
the final results. The proceeding of this heuristic selection is controlled by ϖ  and ψ . Lower ϖ  
and higher ψ  will sift out a compact feature subset while higher ϖ  and lower ψ  will output a 
plentiful subset. 
  
 
[19] Yu, L. & Liu, H. 2004. Efficient feature selection via analysis of relevance and redundancy. 
Journal of Machine Learning Research, 5, 1205-1224. 
[20] Zhang, B., Chen, Z. & Murphey, Y. L. 2005. Protein secondary structure prediction using 
machine learning. IEEE International Joint Conference on Neural Networks, 1, 532-537. 
 
A ROBUST FEATURE SELECTION SCHEME FOR MULTI-CLASS 
CLASSIFICATION PROBLEMS 
 
Hung-Yi Lin 
Department of Distribution Management, National Taichung University of Science and 
Technology, 
129 Sec. 3, Sanmin Road, Taichung City 404, Taiwan, 
linhy@nutc.edu.tw 
 
 
ABSTRACT  
This paper integrates cluster and variability analyses to advance a novel feature selection 
scheme. Huang-index method using fuzzy c-means is employed to enhance cluster validity and 
optimizes a consistent cluster number among the features. A new entropy-based feature evaluation 
method is formulated for the identication of discriminative features. Multivariate statistical 
analyses are utilized to solve the redundancy between features. Experimental results show that our 
new feature selection scheme successfully sifts a compact subset of characterizing features for 
classification problems with multiple classes. 
 
Keyword: Cluster analysis, variability analysis, multivariate analysis, feature selection 
 
INTRODUCTION 
Classification problems have become more complex in modern digital applications when 
facing incessant data explosion. Present databases are capable of enduring various data types and 
accommodating a great number of fields (attributes, features) to one record. Meanwhile, data 
scales become larger and larger. In addition to these great changes, modern classification problems 
also hesitate to make decisions among a variety of classes. The ever-increasing growths in data 
complexity and class multiplicity have severely deteriorated classification performance and 
accuracy rate. 
Our work in this paper focuses on three issues that have not been touched in earlier work. 
Firstly, although cluster analyses have been widely used for many applications, no cluster analyses 
with validation study are applied in advance of processing multi-class problems. The first goal in 
this paper is to present an appropriate data clustering scheme for accurately refining the raw data. 
Secondly, we investigate data variation inside subsets and include such information into our new 
feature evaluation criterion. By means of such design, we can identify the most discriminative 
features for the task of multi-class classification. Thirdly, a novel variability analysis relied on 
statistical multivariate analysis is proposed and a new heuristic algorithm facilitates the selection of 
a compact subset of minimal-redundancy and maximal-relevance features. The final goal of this 
paper is to provide the necessary and sufficient classification factors to the classifiers. 
This paper is organized as follows. Section 2 reviews the problems derived from continuous 
data and multi-class classification. Section 3 presents the new feature selection scheme. The design 
of variability analysis using statistical multivariate analysis and heuristic algorithm are included in 
this section. The experimental and analytical results are presented in Section 4. Finally, concluding 
remarks are given in the last section. 
 
 Fig. 1.  Entire scheme of feature selection. 
 
I. First cluster processing: raw data in continuous values or ordinal number are first 
preprocessed by cluster analysis. Nominal data can skip this processing. Our clustering 
method can group raw data into a non-uniform categorization and assign every cluster a 
sequencing number. 
II. Feature evaluation: all input features with or without preprocessed values are evaluated by our 
new criterion. All data used in this individual evaluation method are only involved once. 
III. Feature selection: the using of correlation matrices in variability analyses consumes higher 
computational costs. However, only a few features are involved in this step. 
IV. Second cluster processing: only the features selected by Step III are processed in this step. 
The raw data corresponding to these features are recalled and processed by the cluster 
analysis again. The difference between the first and second cluster processing is the resulting 
cluster number. Because only a smaller subset of features is processed by the second cluster 
analysis, the larger cluster number is assigned to the selected features so that the classification 
task can be accomplished using this compact subset of features 
 
A.
 
Cluster analysis and Huang-index 
Unsupervised learning algorithm FCM (fuzzy c-means) has been applied successfully to a 
number of problems involving feature analysis and classifier design. The FCM minimizes the 
following objective function: 
∑∑
= =
∞<<−=
N
i
C
j
ji
m
ijm mvxuJ
1 1
2
1  ,                                       (1) 
, where m is any real number greater than 1, uij is the degree of membership of xi in the cluster j, xi 
is the i-th of d-dimensional measured data, cj is the d-dimension center of the cluster, and ||*|| is 
any norm expressing the similarity between any measured data and the center. Fuzzy partitioning is 
carried out through an iterative optimization of the objective function shown above, with the 
update of membership uij and the cluster centers cj by: 
Step I. First cluster processing 
Training data 
Step II. Feature evaluation 
Cluster analysis Fuzzy c-mean 
Step III. Feature selection 
Variability Characterizing 
Step IV. Second cluster processing 
Output selected features with 
different classes is calculated and used to measure the relevance with respect to the class concept. 
Gini impurity is computed by summing the probability of each item being chosen times the 
probability of a mistake in categorizing that item, while information gain and information gain 
ratio are calculated using Shannon’s entropy. Shannon’s entropy is a board and general concept 
which studies the amount of information in a transmitted message. The probabilities that a 
particular message actually transmitted was a measure of how much information was in the 
message. Data distributions in multi-class problems are looser and in turn have more diverse 
results than those in two-class problems. Therefore, features evaluated according to the 
conventional entropy criterion could lead to similar measurements so that it hard to explicitly 
distinguish their discriminative powers from the similar evaluation results. To remedy such 
situation, we simply include the information of data variation inside every subset to the 
conventional entropy criterion. The purpose of this design is to simultaneously take the 
dissimilarity around subsets (via data distribution) and the aggregation inside subsets (via data 
variation) into account. The measure of data variation is simply conducted by statistical variance. 
One feature with good discrimination should be capable of aggregating those data belonging to the 
same group. Let T be a training dataset, data variation measured from every subset iT using data 
variance )(2 iTσ is used to enhance the traditional Shannon’s entropy. Intuitively, one subset iT  
comprising totally consistent data both zeros entropy )( iTH  and variance )(2 iTσ . We formulate 
the enhanced entropy (denoted as EH) to be a new feature evaluation criterion as following: 
∑
=
××=
n
i
iii TTxHTTTxEH
1
2 )(),(||/||) ,( σ                                  (4) 
, where feature x takes on values in {1, 2, …, n} and it splits the train set T into subset Ti, 
},,2,1{ ni L∈ . In this design, the success of approving one relevant feature relies on low entropy 
and low data variance. This is why the summation of the product of ),( iTxH and )(2 iTσ  for all 
subsets becomes a better criterion for the feature evaluation of multi-class problems. 
In [13], it is revealed that there is no significant difference between Gini index and 
information gain. The behavior of these two split functions is so similar that the probability in 
selecting the same splits is as high as 98%. Therefore, when numeric attribute x is tested for 
multi-class problems, Gini index can be modified in a similar way as Shannon’s entropy, i.e., 
∑
=
××=
n
i
iii TTGiniTTTxGiniEnhanced
1
2 )()(||/||) ,(_ σ                        (5) 
Similar to the design of classic information gain, ) ,( TxEH is compared with the initial status 
before splitting. Such improvement is called Aggregation Gain and defined as 
) ,()()( 2 TxEHTTI −×σ  in this paper, where I(T) is the total entropy and )(2 Tσ  is the total 
variance before splitting. AG(x, T) is taken to represent the aggregation gain obtained by adopting 
feature x. Consequently, the higher AG(x, T) verifies that feature x possesses the higher relevance 
to the class concept. 
Raw feature values preprocessed by cluster analyses and then evaluated by our new criterion 
constitute the first stage of our selection scheme. The goal of this stage is to efficiently investigate 
all original features and rank them according to a sounder criterion. The output of this stage is a 
proper subset of candidate features for the subsequent handle of multi-class classification. We 
remind that such preprocessing and evaluation can be accomplished within an execution 
complexity of O(N). 
 
C.
 
Variability analysis using statistical multivariate analysis 
Redundancies among variables signify that variable values correlate closely with each other. 
Two highly correlated features have the class-discriminative power which is similar to that derived 
from only one of them. Principal component analysis (PCA) is a multivariate statistical tool used 
for analyzing data variability. It can reproduce the total system variability and achieves high 
To verify the effectiveness of the selected features, four classification methods including 
NaiveBayes, C4.5, simple CART, and SVM selected from the 10 most influential algorithms [20] 
were tested. Different feature subsets were generated according to three strategies. The first 
strategy only adopts the feature evaluation criterion of information gain which is the classic 
relevance analysis and marked as “R” in the following figures. In addition to the classic relevance 
analysis, the second strategy also applies the variability analysis via PCA to reduce the redundancy 
among the selected features, which is denoted as “RV”in the following figures. The third strategy 
adopts the relevance analysis of AG criterion, the variability analysis of PCA, and the cluster 
analysis validated by Huang-index, which is marked as “RVC”in the following figures. To 
investigate the classification performance affected by feature number as well, three feature 
quantities of   12/ +N ,   1+N , and   12 +N were tested. For simplicity, these three 
quantities are respectively denoted as N-, N, and N+ in the following figures. 
Figures 3(a) to 3(d) respectively show the accuracy performance using four classifiers under 
different cases. All experimental results in this study were assessed using 10-fold cross validation. 
The features selected by RVC generally outperform those by R or RV. Dataset svmguide4 has the 
most significant improvement for all four classifiers. Three feature quantities used in svmguide4 
were N-=4, N=5, and N+=6, respectively. The best performance all happened at N=5. As to datasets 
segment, places, and satimage, the benefits brought by cluster analyses seem to be larger than 
those brought by variability analyses. The effect of variability analysis is slightly apparent only 
when segment and svmguide4 are used. In view of all experimental results in Fig. 3, variability 
analysis can enhance the effect of cluster analyses in most cases except for wine. In dataset wine, 
the similar performance for three strategies reveals that variability and cluster analyses bring trivial 
help in the classification tasks. This may rise from the situation that the selected features are 
inherently uncorrelated and the raw data are invariant under the processing of cluster analyses. 
With respect to the factor of selected feature quantity, segment and satimage get the improvement 
when imposing more features on the cases using R and RV. 
 
 
(a) NaiveBayes 
 
(b) C4.5 
 
(c) Simple CART 
 (d) SVM 
Fig. 4 Performance of ROC. 
 
CONCLUSION 
The feature selection scheme proposed in this paper has three main contributions. Firstly, the 
first attempt to promote the discriminative power of raw data by means of cluster analyses. 
Although FCM and Huang-index method yields the superior effect when evaluating features, many 
other clustering algorithms can realize this objective as well. Secondly, the more precise evaluation 
criterion is proposed for multi-class problems. Only the limited computational costs are required to 
this design. The whole evaluation complexity retains at the same level. Thirdly, the compact subset 
comprising discriminative features is acquired by the heuristic selection scheme. Cluster, relevance, 
and variability analyses are successfully integrated on an efficient and effective framework. The 
future researches direct to the following aspects. The first is to apply different cluster analyses on 
different features and use distinct cluster number to assess every single feature. We are motivated 
to raise features’ discrimination power to an even higher level and this success can make the 
classification model involved by an even fewer amount of features. In addition, creation of a new 
classifier which can fully advance the superiority of the new feature selection scheme over many 
traditional methods. 
 
APPENDIX  
Heuristic selection algorithm 
The notations used in the algorithm are as follows. 

 S: The feature space comprising all original features whose size exceeds 2. 

 
ϖ : The designated maximum selection round number. 

 
A , 1A , 2A : The sets comprising selected features. 

 α, β: The features selected in every selection round. 

 1λ (S): The largest eigenvalue of the correlation matrix when applying PCA on S. 

 
ψ : The threshold between 0 and 1. 
 
Feature Selection 
Input: feature set S with training data 
Output: set of selected features 
1. Sort all features of S in a decreasing order according to their individual AGs. If two or more 
features have the same evaluation, the feature with higher priority ranks first. The feature with 
the highest AG is denoted as f1. 
2. A← { f1}.  
3. While ϖ≤A  
4. Select the next feature α  in AS − which maximizes )(1 αλ ∪A . 
5.      }{1 α∪← AA . 
6. Similarly, select the next feature β  in 1AS −  which maximizes )( 11 βλ ∪A . 
7.      }{12 β∪← AA . 
REFERENCES 
[1] Aran, O. & Akarun, L. 2010. A multi-class classification strategy for Fisher scores: application 
to signer independent sign language recognition. Pattern Recognition, 43(5), 1776-1788. 
[2] Baldi, P. & Pollastri, G. 2002. A machine learning strategy for protein analysis. IEEE Intelligent 
Systems, 17(2), 28-35. 
[3] Caballero, J. C. F., Martínez, F. J., Hervás, C. & Gutiérrez, P. A. 2010. Sensitivity versus 
accuracy in multiclass problems using memetic pareto evolutionary neural networks. IEEE 
Transactions on Neural Networks, 21(5), 750-770. 
[4] Chang, C.-C. & Lin, C.-J. 2001. Software available: a library for support vector machines 
(LIBSVM). <http://www.csie.ntu.edu.tw/~cjlin/libsvm>. 
[5] Chen, J., Wang, C. & Wang, R. 2009. Adaptive binary tree for fast SVM multiclass 
classification. Neurocomputing, 72(13–15), 3370-3375. 
[6] El-Yaniv, R., Pechyony, D. & Yom-Tov, E. 2008. Better multiclass classification via a 
margin-optimized single binary problem. Pattern Recognition Letters, 29(14), 1954-1959. 
[7] Fernández, A., Calderón, M., Barrenechea, E., Bustince, H. & Herrera, F. 2010. Solving 
multi-class problems with linguistic fuzzy rule based classification systems based on pairwise 
learning and preference relations. Fuzzy Sets and Systems, 161(23), 3064.3080. 
[8] Fürnkranz, J. 2002. Round robin classification. Journal of Machine Learning Research, 2, 
721-747. 
[9] Galar, M., Fernández, A., Barrenechea, E., Bustince, H. & Herrera, F. 2011. An overview of 
ensemble methods for binary classifiers in multi-class problems: experimental study on 
one-vs-one and one-vs-all schemes. Pattern Recognition, 44(8), 1761-1776. 
[10] Huang, K.-Y. 2010. Applications of an enhanced cluster validity index method based on the 
fuzzy c-means and rough set theories to partition and classification. Expert Systems with 
Applications, 37(12), 8757-8769. 
[11] Li, R., Lu, J., Zhang, Y. & Zhao, T. 2010. Dynamic Adaboost learning with feature selection 
based on parallel genetic algorithm for image annotation. Knowledge-Based Systems, 23(3), 
195-201. 
[12] Ou, G. & Murphey, Y. L. 2007. Multi-class pattern classification using neural networks. 
Pattern Recognition, 40(1), 4-18. 
[13] Raileanu, L. E. & Stoffel, K. 2004. Theoretical comparison between the gini index and 
information gain criteria. Annals of Mathematics and Artificial Intelligence, 41(1), 77-93. 
[14] StatLib Databsets Archive <http://lib.stat.cmu.edu/datasets/>. 
[15] Statlog Datasets <http://www.is.umk.pl/project/datasets-stat.html>. 
[16] Su, C.-T. & Hsiao, Y.-H. 2009. Multiclass MTS for simultaneous feature selection and 
classification. IEEE Transactions on Knowledge and Data Engineering, 21(2), 192-205. 
[17] UCI machine learning repository <http://archive.ics.uci.edu/ml/>. 
[18] Wu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., et al. 2008. Top 10 algorithms in data 
mining. Knowledge and Information Systems, 14(1), 1-37. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/18
國科會補助計畫
計畫名稱: 運用多變量分析於加速分類多目標問題之研究
計畫主持人: 林泓毅
計畫編號: 100-2221-E-025-015- 學門領域: 人工智慧
無研發成果推廣資料
Issue 1, January 2012, 
pp. 167-177. (SCI, 
IF=1.293, Rank in 
Computer Science, 
Software Engineering: 
34/99). 
研究報告 /技術報
告 0 0 100%  
研討會論文 1 1 100% 
Hung-Yi Lin, ＇＇＇＇A 
Robust Feature 
Selection Scheme for 
Multi-class 
Classification 
Problems＇＇＇＇, 2012 
International 
Conference on Business 
and Information, July 
3-5, 2012, Sapporo, 
Japan. 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重要
國際合作、研究成果
國際影響力及其他
協助產業技術發展
之 具 體 效 益 事 項
等，請以文字敘述填
列。) 
1. 日本京都大學訪問學者 101年六月~九月 
2. 獲國科會 100年度【補助大專校院獎勵特殊優秀人才】 
3. 2011 台灣電子商務學會 TOPCO 崇越論文大賞資管組最優論文獎(100年度) 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
科 
教 
處 
計 
畫 
舉辦之活動/競賽 0  
