行政院國家科學委員會補助專題研究計畫 成果報告   □期中進度報告 
 
應用於遠距協同作業之多模態人機智慧型互動關鍵技術研究－ 
子計畫三：以視覺為基礎之協同作業互動技術研發(I) 
 
計畫類別：□個別型計畫   整合型計畫 
計畫編號：NSC 99-2219-E-155 -004 
執行期間： 99 年 8 月 1 日至 100 年 7 月 31 日 
 
執行機構及系所：元智大學 通訊工程系 
 
計畫主持人：李建誠 助理教授 
共同主持人： 
計畫參與人員：酈頤芳、辜士哲、衛仲軒 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            涉及專利或其他智慧財產權，□一年  二年後可公開查詢 
 
中   華   民   國  100  年 10 月 31 日 
2 
 
character recognition. Computer vision techniques detect fingertip and track the fingertip trajectories to 
recognize the finger gesture. The finger gesture can be used to implement the editing functions, such as 
copy, paste, delete, undo, redo, insert, etc. Additionally, vision-based handwriting character recognition 
provides a convenient input device. Based on these vision-based HCI interfaces, the interfaces of 
tele-collaboration can be enhanced, and more close to the natural communications. 
Keywords: tele-collaboration, human-computer interaction (HCI), fingertip detection, gesture recognition 
一.  前言與目的 
本計畫為「應用於遠距協同作業之多模態人機智慧型互動關鍵技術研究」的子計畫之一，
本子計畫第一年之研究重點在研究開發以攝影機為輸入裝置之指尖筆勢辨識(vision-based 
fingertip gesture recognition)，以提供協同作業時所需的智慧型人機互動介面。協同作業環境在
遠距應用上越來越受到重視，像是遠距會議與遠距居家照護上。人與人之間可以透過聲音、肢
體語言以及表情來做溝通，但人與機器，往往只能夠透過鍵盤、滑鼠等人機介面來傳遞訊息，
雖然有許多使用者對於透過鍵盤輸入訊息並不陌生，但對於剛接觸鍵盤的使用者來說，他必須
開始學習透過一個又一個按鍵來輸入訊息，這並不是人類平時熟悉的表達方式。於是，有人試
著透過更自然的方式，像是透過語音辨識系統[1]，[2]，或者是手勢辨識系統[3]-[5]來將訊息傳
遞給機器。 
在手勢辨識的部份，由於使用者可以在固定的位置上傳遞指令，對於行動不便的人士，
或是臥病在床的老人，都可以利用手勢辨識系統來傳遞訊息，讓家人或者是看護人員了解使用
者的需求。如果搭配上感測系統，還能夠調整室內的空調或是調整燈的開關等等。在手寫筆勢
辨識方面，目前也有許多市面上的手機透過觸控式螢幕來接收手寫筆勢傳遞的訊息，一方面減
少輸入裝置的配置空間，另一方面又可以略過用鍵盤敲打所花費的時間；然而，透過鍵盤來傳
遞訊息需要花時間學習，也不符合人類平常所習慣的表達方式，但在過去長時間地普及之下，
使用者也漸漸地習慣，雖然如此，還是有許多企業試圖打破這個傳統，像近期蘋果電腦推出的 
i-phone 以及 i-pad，捨棄了傳統鍵盤，人機介面皆透過觸控式螢幕來操作，由於使用上方便，
而且軟體功能強大，已經吸引不少年輕族群加入，並引起了一陣風潮，相信未來會有愈來愈多
的年輕族群，願意捨棄傳統的人機介面，學習以手勢或手寫筆勢等自然方式來傳遞訊息。 
在手寫筆勢方面，大部分的研究是透過手寫板等手寫輸入裝置來傳遞訊息[6]，[7]，而手
寫板會透過指壓來記錄手寫筆勢的軌跡、筆壓以及筆速等等，透過這些資訊使用者可以輕易地
得到許多筆勢的訊息，雖然手寫板能讓使用者透過書寫的方式來傳遞訊息，但我們仍希望能以
更簡易的人機介面擷取手寫筆勢的訊息，甚至是讓使用者能直接在桌面上書寫，於是本計畫提
出以攝影機為輸入裝置之指尖筆勢辨識，以網路攝影機 (webcam) 做為人機介面，讓使用者能
直接在任何平面上書寫，透過攝影機擷取指尖，辨識所寫之筆勢或文字，如圖 1-1 所示。 
本研究計畫以網路攝影做為人機介面的目的有兩個，第一個是網路攝影機取得容易而且
價格低廉，二來是網路攝影機可以任意配置在電腦週邊，但以網路攝影機為人機界面所要克服
的問題，在於機器無法直接從一段手寫筆勢影像中取得筆勢的相關資訊，我們必須經過一連串
的前處理，從一開始手指膚色辨識，到指尖位置偵測，接著才進入手寫筆勢的軌跡擷取，但擷
取下來的筆勢又包含了初始的落筆軌跡已及書寫結束時的提筆軌跡，要如何從影像中判別書寫
者的指尖何時碰觸到了桌面，何時又離開了桌面，這些都是要克服的問題。 
4 
 
示，Lin 等人以二十六個英文字母以及十個阿拉伯數字作為測試資料，以四種手勢作為筆勢開
始、結束以及字元種類的切換，如圖 2-2 所示，最後，手勢的平均辨識率到達八成，而筆勢
辨識率方面，阿拉伯數字的筆勢辨識率為 98.64 %，英文字母的筆勢辨識率為 97.31 %。 
2007 年，Jin 與 Yang [29]建立了一個以視覺為基礎的手寫符號辨識系統，以改良式的前
景擷取系統，偵測出手掌的範圍，經過一連串的影像處理，讓使用者在複雜的背景下書寫，仍
然可以偵測到指尖的位置，實驗以二十六個英文字母作為測試資料，實驗結果，大寫字母的筆
勢辨識率有 95.3 %，小寫字母的筆勢辨識率有 98.7 %。 
2008 年，Hsieh 等人[30]從手寫筆勢影片中，前後兩張影像差異的區域，與影像中的膚
色區域重疊，接著從重疊的區域中找出手的區塊，而手指指尖通常位於手部上方的位置， Hsieh 
等人利用手指指尖與手的相對位置，從影像中找到了手指指尖的位置，接著從 847 張手勢影
像中判斷指尖的位置，辨識率為 98.58 %；Hsieh 等人以二十六個英文字母以及十個阿拉伯數
字作為手寫筆勢樣本，利用自己建立的筆勢模組，從 900 部手寫筆勢影像中作辨識，實驗結
果得到 91.11 %的辨識率。 
 
 
圖 2-1 從影像中擷取指尖的軌跡[28] 
 
 
圖 2-2 做為筆勢開始、結束以及字元種類切換的四種手勢[28] 
 
在特徵擷取方面，有些人直接從完整的手寫筆勢中擷取特徵序列，有些人則會將筆勢的
軌跡分成數個子筆劃，再從子筆劃中擷取特徵序列，而筆勢之中會有一些可作為分割的依據，
例如直角、線段之間不連續的斷點等等，而分割出來的軌跡會有空間跟時間上的關連，可作為
子筆劃的特徵。 
2008 年，Kerallah 等人[31]結合了幾何 (geometric) 與動態的特徵，以筆劃速度的區域極
端值作為分割筆勢的依據。Kerallah 等人以橢圓來取代每一個子筆劃 (sub-stroke)，如圖 2-3 所
示。每一個橢圓包含了橢圓長軸、短軸以及長軸與水平線的夾角等幾何特徵，如圖 2-4 所示。
而動態的特徵則涵蓋子筆劃的初始時間、結束時間、最大速度以及 Beta 模組[31]的參數，
Kerallah 一共收集了 30,000 筆阿拉伯數字的筆勢資料，其中 20,000 作為訓練資料，10,000 筆
作為測試資料，實驗結果得到 95.08 %的辨識率。 
6 
 
 
圖 2-6 繪圖軟體中常見的九種圖形[32]  
 
筆畫中包含了許多的特徵，常見的特徵有筆畫的長度、方向與曲率等等。然而，不同的
符號必須以不同型態的特徵取代，像中文字的符號包含許多的直線以及銳角，如圖 2-7 所示，
阿拉伯的文字符號則包含許多的曲線，如圖 2-8 所示；所以，我們在挑選特徵之前，必須要
先了解符號的特性，以下介紹幾種較為常見的筆勢特徵。  
其中最常見的就是長度特徵以及方向特徵，長度特徵代表了軌跡的長短，常用的長度特
徵有軌跡的長度、軌跡前後端點的距離[33];而方向的特徵則代表了筆劃的方向，常用的方向特
徵有軌跡的平均方向、軌跡起始的方向以及軌跡起點與終點連線的方向[33]。 
但筆劃除了在方向與長度可以跟其他的筆劃區別之外，還有筆劃的彎曲程度可做為特
徵，所以我們會用曲率 (Curvature) 特徵來代表一個筆劃彎曲的程度[33]，也可用線段與線段
之間的夾角來表示線段之間的彎曲程度；除此之外，區域特徵也是一個常見的特徵，它被用來
表達筆劃的長度與寬度，或是長度與寬度之間的比值 (aspect) [34]。 
 除了上述的特徵之外，還有跟時間有關的動態特徵 (Dynamic Feature) ，動態特徵除了
表示筆劃的持續時間 (duration) 之外，還能表示筆劃的速度、加速度等特徵[33]；也是非常重
要的特徵之ㄧ。 
 
 
圖 2-7 中文手寫字體[9]  
 
 
圖 2-8 阿拉伯文手寫字體[23]  
 
 2005 年，Pastor 等人[35]利用座標點的前後關係取得相關特徵，並將座標點與座標點
8 
 
   
1. Insert    
   
 7. Capitalize
   
 
   
 13. Paragraph 14. Font 15. Style 
   
16.  
Single  
Space 
17. 
Double 
Space 
18. 
Number 
List 
19. Cut 20. Delete 
  
 
21. Transpose 
22.  
Select 
Word 
23. Copy 24. Paste  
圖 3-1 筆勢辨識研究所採用的二十四種筆勢  
 
 
3.1 手寫筆勢辨識流程 
視訊影片在正式進入手寫筆勢辨識前，需先經過一段前處理過程，包括影片中的指尖偵
測、移除原始軌跡中不必要的筆跡，接著進行軌跡前處理，最後得到用於特徵擷取的軌跡，如
圖 3-2 所示；取得用於特徵擷取的軌跡之後，再將軌跡中的轉折點找出來，藉此將筆勢分割
成數個子筆劃 (sub-stroke)，而每一筆子筆劃分別能夠擷取出一組用於手寫筆勢辨識的特徵序
10 
 
將軌跡做前處理
找軌跡的轉折點
將筆勢辨識子系統內的每
一種筆勢建立 HMM 模組
將軌跡分為數個 sub-
stroke
將子系統內的所有特徵序
列分群
開始
結束
取得指尖的軌跡
紀錄指尖軌跡的座標點
移除筆勢兩端提筆與落
筆的軌跡
從 sub-stroke 擷取特徵
序列
將子系統內的所有特徵序
列交給 PBM-Index + K-
means 選取最佳的群組數
目
將子系統內所有的特徵序
列量化為一個值
將筆勢依子筆劃的數目分
配到對應的子系統
依筆勢辨識子系統內的筆
勢選取適合的特徵序列
 
圖 3-3 手寫筆勢辨識系統流程圖  
 
3.2 前處理 
在取得影片中指尖的軌跡前，我們需要先經過幾個步驟，才能夠找出影片中指尖的位置。
首先，我們先選取影片中的膚色區域，從膚色區域中選取最大的輪廓 (contour)，將此輪廓定義
為手掌的輪廓，找出手掌的輪廓之後，還必須依照指尖的特性，從輪廓中找出符合指尖特性的
位置。 
12 
 
 
圖 3-5 手掌輪廓的邊框  
 
3.2.3 軌跡擷取 
指尖經過的座標點即是軌跡的位置，所以，我們將指尖經過的所有座標點紀錄下來，做為
筆勢的路徑，如圖 3-6 所示。 
 
 
(a)    (b) 
圖 3-6 軌跡紀錄；(a) 筆勢範例；(b) 紀錄下的軌跡。  
 
從軌跡的路徑我們可以發現，除了符合手寫筆勢的路徑之外，軌跡的前後兩端還包含了落
筆跟提筆的路徑，由於每一個筆勢的軌跡都包含了前後兩端提筆與落筆的路徑，這在筆勢辨識
上無法提供有效的分類資訊，於是我們把軌跡前後兩端，落筆與提筆的路徑濾除。 
由於書寫者在落筆及提筆的瞬間，指尖會停留在原來的位置，我們利用這個特性來記錄筆
勢的停頓點，而初始的落筆以及結束時的提筆分別會產生一個停頓點，停頓點是指現在的指尖
位置與上一個時間的指尖位置的距離小於某個門檻值 (threshold)，則判定為停頓點，我們將這
兩個停頓點之間的軌跡保留，也就是符合筆勢範例的軌跡，結果如圖 3-7 所示，移除落筆前
的軌跡演算法如下： 
 
for i = 1 to N  
if Si ≠P1 then  remove Si;  
  else break; 
end 
  
14 
 
   
          (a)                 (b) 
圖 3-8 使用者的筆勢；(a) 較小的筆勢；(b) 較大的筆勢。  
 
 
2. 重新取樣 
在做完正規化處理後，有些筆勢的座標點會出現在同樣的位置，成為多餘的座標，而重
新取樣則會移除這些多餘的座標點。 
 
3. 內插法 
在筆勢做完正規化處理後，本研究用內插法將資料點之間的空隙補齊，如圖 3-9 所示，
其公式如下： 
   
   
01
010
0 xx
yyxxyy
n
ni
i 


                    (3-13) 
  其中 x0 及 xn+1 為斷點兩端點在水平方向的位置，y0 及 yn+1 為斷點的兩端點在垂直方
向的位置，而 n 為斷點在筆勢上的編號。 
 
4. 平滑化處理 
當書寫者在書寫時常會出現一些不穩定的軌跡，如圖 3-10 所示，這會影響夾角計算的
過程，於是本研究將筆勢做平滑化處理；首先，本研究將每個資料點與高斯 (Gaussian) 函數
卷積 (convolution)，得到一個新的值，接著，再將各個值做正規化，作為平滑處理後資料點，
其公式如下： 
           iSS   
~   * G (i) / sum.                   (3-10) 
其中， 
                     G(i)  
 
2
2
2
2/)1(

 wi
e ,                     (3-11) 
sum =

1
0
)(
w
i
iG .                          (3-12) 
16 
 
   θi+l = arcos(
21
21
ll
ll

 ); 
if θi+l < θthreshold then 
    Si+l = bending point; 
  end 
end 
end 
其中，hmin 指的是 mean-shift 所用的最小頻寬，本計畫將 hmin 定義為兩個資料點的距離，
h 指的是 mean-shift 的初始頻寬，如公式 3-17 所示，本計畫將初始頻寬定義為十個資料點的
距離，l 代表取夾角線段的長度，n 代表筆勢前處理後的資料點 (data point) 數， x 代表座標
點在水平方向的位置，y 代表座標點在垂直方向的位置，本計畫設定的 θthreshold 為 135°。 
本計畫依照此方法取出的轉折點如圖 3-11 所示；之後，將所有位置中轉折點出現的次數
累加取直方圖 (histogram) , 如圖 3-12 所示，接著，將 histogram 的資料點套進 mean-shift [42]
演算法，即可找出轉折點的位置，為了標示轉折點的位置，我們將 histogram 的各個資料點與
核函數 (kernel function)做 convolution，也就是核密度估計 (Kernel Density Estimation, KDE)，
求得一個機率密度函數，相較原本的直方圖，此機率密度函數的機率分佈更加地平滑，函數內
區域極大值的位置會被突顯出來，如圖 3-13 所示。  
做完核密度估計後的機率密度函數，跟原始的直方圖比起來，更能表達資料點與鄰近資料
之間的關係，函數的機率分佈也更加的平滑，常見的 Kernel 有： 
1. Epanechnikov Kernel: 
KE(S) =


 
otherwise                         0
1)1(
2
w
cS
w
cSk .            (3-14) 
2. Uniform Kernel: 
KU(S) = 

 
otherwise0
1
w
cSk
.                    (3-15) 
3. Normal Kernel: 
KN(S) = k．exp（
2
2
1
w
cS  ）.              (3-16) 
 而本計畫採用的 kernel 為 Normal Kernel，其中，S 代表資料點的位置，k 為一個常數值，
w 為函數的頻寬，c 為函數的中心點。 
 而 mean-shift 的演算法如下[42]： 
18 
 
      Pthreshold  = Pmax * 0.25.                     (3-25) 
其中 Pmax 指的是機率密度函數中的機率最大值。 
 
 
l = 4 l = 5 l = 6 l = 7 l = 8 
    
l = 9 l = 10 l = 11 l = 12 l = 13 
  
l = 14 l = 15 l = 16 l = 17 l = 18 
 
   
l = 19 l = 20    
圖 3-11 利用不同長度的線段從筆勢中取出的轉折點 
20 
 
 
3.4 特徵擷取 
依據筆勢中 sub-stroke 的特性，我們一共擷取了八種幾何相關的特徵。在此做一個詳細介
紹。 
1. 跟筆劃長度有關的特徵 
筆劃長度算是在筆勢辨識中最容易辨識的特徵之ㄧ，但單一取子筆劃的長度又可能跟其
它筆勢的子筆劃長度相同，於是我們取子筆劃與筆勢長度的比值做為長度特徵，取名為 
sub-stroke ratio，其公式如下： 
R = l / L.                                (3-26) 
L 指的是筆勢的總長度，l 指的是子筆劃的長度。 
  
2. 跟區域有關的特徵 
區域特徵能夠看的出筆勢的密集程度、長寬的關係以及對角線的長度等特徵，在本計畫
的筆勢樣本裡有許多的直線與斜線筆劃，如圖 3-1 所示，為了分辨不同的傾斜角度，本計畫
選取強調子筆劃長寬關係的特徵，aspect [36]，公式如下： 
A(S) = 
)()(
)()(
SxSy
SxSy

 .                     (3-27) 
其中，△x 指的是子筆劃的投影在水平方向的寬度，△y 指的是子筆劃投影在垂直方向
的高度，當子筆劃兩端點的連線為垂直直線時，A 的值為 1，當子筆劃兩端點的連線為水平
直線時，A 的值為 -1。 
另外，在第一個子系統中，有些筆勢的閉合程度會特別明顯，例如 gesture Redo 以及 
Undo，可以跟同方向閉合程度較低的 gesture Underline 以及 Backspace 做一個區別，本計畫
取了與筆劃閉合程度有關的特徵，closure [33]： 
           C = d / l.                             (3-28) 
其中，d 指的是子筆劃兩端點連線的距離，l 指的是子筆劃線段的長度，當筆劃的閉合
程度愈高，closure 的值愈低。 
 
3. 跟角度有關的特徵 
角度的特徵能代表筆劃的方向，本計畫取了兩個角度特徵，分別為子筆劃兩端連線與水
平線夾角的正弦 (sine) 跟餘弦 (cosine)，其公式如下： 
Sinθ = 
22 yx
y

.                         (3-29) 
Cosθ = 
22 yx
x

.                         (3-30) 
其中， x 指的是子筆劃在水平方向的向量，y 指的是子筆劃在垂直方向的向量。 
 
4. 跟曲率有關的特徵 
曲率特徵能呈現出筆劃的彎曲程度，在此我們取了兩個曲率的相關特徵，第一個特徵
22 
 
當具有相同筆勢的子系統，系統內筆勢的子筆劃數目比原筆勢的子筆劃數目高時，則系統
會不斷的降低 Pthreshold ，直到子筆劃的數目提升為止，其演算法如下： 
While Pthreshold  !=  0 
   Pthreshold  = Pthreshold  * 0.9; 
   nl = 0; 
   for i = 1 to nm 
    if mpi > Pthreshold  then 
     nl++;   
end 
   end 
   if  nl  = n  then 
    break;   
end 
  end. 
 其中，nm 指的是 mean-shift 找到的頂點總數，mpi 指的是第 i 個 mean-shift 找到的頂點位置
所對應到機率密度函數的值，n 指的是其他筆勢辨識子系統中具有相同筆勢種類的子筆劃數目。 
 當具有相同種類筆勢的子系統，系統內筆勢的子筆劃數目比原筆勢的子筆劃數目低時，則調高 
Pthreshold ，直到筆勢子筆劃的數目降低為止。 
 
2. mean-shift 的頻寬修正 
當調整 Pthreshold 無法將筆勢歸類到其他子系統時，則調整 mean-shift 的頻寬以及 mthreshold，
其演算法如下： 
nl  = 0; 
for i = 1 to nm 
   if mpi > Pthreshold  then 
    nl++; 
   end 
end 
if( n > nl)  then 
 h = h * 0.7; 
end 
else then 
 h = h / 0.7; 
end 
mthreshold  = hmin * h; 
 其中，nm 指的是 mean-shift 找到的頂點的數目，mpi 指的是第 i 個 mean-shift 找到的頂點位
置所對應到機率密度函數的值， n 指的是其他筆勢辨識子系統中具有相同筆勢的子筆劃數目。 
 
3.6  K-means 分群演算法 
 本計畫利用 K-means 演算法替手寫筆勢子系統內的所有特徵序列分群，在決定群組的數目 
24 
 
由式子 3-16 來看，PBM-index 主要有三個部份 1/K、E1/EK 以及 DK，從第一個部份來看，
資料被分為愈多個群組，K 的值愈大，1/K 愈小，將 K 放在分母的目的是希望能夠用較少的群
組數來代表資料分佈狀況；而第二的部份 E1 是常數，EK 表現的是群組內部的密集程度，當密集
程度愈高，第二個部份的值就愈小，E1/EK 的值會提升；而第三個部份 DK 則代表了群組之間的
離散程度。 
綜合這三個部份來看，當群組內部的關連性提高、群組之間的離散距離提升或是群組的數目
下降，都會使得 PBM-index 的值提升，這為分群的最佳化提供了一個準則；而 Pakhira 等人[40]
發現，當群組的數目在沒有設定上限的情況，資料被盲目的分群，到最後演變成每一個資料點就
是一個群組，這時候 index 的值很大，卻顯得毫無意義，於是 Pakhira 等人便替分群的數目設立
了一個上限： 
Kmax ≦ n .                          (3-41) 
其中，n 代表資料的數目；如此便可避免 index 的值很大，但分群的結果毫無意義的情況發生。 
 
3.8 HMM 分類系統 
將手寫筆勢子系統內的特徵序列分群之後，我們能夠將各個特徵序列量化成一個值，接著，
本計畫選取 HMM 作為分類系統，為手寫筆勢子系統內的每一種筆勢建立一組 HMM 模組，而
建立 HMM 分類系統的流程如下，系統流程圖如圖 3-16 所示： 
1. 依筆勢中子筆劃的數目，將筆勢資料分配到對應的筆勢辨識子系統。 
2. 將 HMM 的內部參數初始化。 
3. 將子系統內的各個特徵序列量化。 
4. 從每一筆手寫筆勢中取出一組量化過的觀察序列。 
5. 以波氏演算法更新 HMM 的參數。 
6. 以 forward algorithm 找出最符合觀察序列的筆勢模組。 
 
26 
 
   
      (a)      (b) 
圖 3-17 因手寫習慣不同而取得不同位置及數目的轉折點； (a) 筆勢中有二個轉折點；(b) 筆
勢中有四個轉折點。 
3.8.2 HMM 分類器 
由於手寫筆勢有時間概念，而子筆劃與子筆劃之間有筆順的關連，於是，本計畫採用帶狀左
右連通的 HMM 模型，如圖 3-18 所示。 
 
 
圖 3-18 帶狀左右連通型 HMM 模組  
 
一個 HMM 模組裡面有三個參數，分別是轉移機率矩陣 A，觀察機率矩陣 B，以及代表初
始狀態的機率分佈矩陣 Π，其定義如下： 
A = {aij|1 ≦ i, j ≦ N}.                 (3-42) 
                   B = {bj(o)|1 ≦ j ≦ N, 1 ≦ o ≦ M}.    (3-43) 
Π = {pi|1 ≦ i ≦ M}.                   (3-44) 
其中 N 代表狀態數目，M 代表觀察元數目。aij 指狀態 i 轉移到狀態 j 的機率，bj(o) 指狀
態 j 對應到觀察元 o 的機率，pi 指初始狀態為狀態 i 的機率。 
一開始建立 HMM 模組時，必須要給轉移機率矩陣 A，觀察機率矩陣 B，以及代表初始狀
態的機率分佈矩陣 Π 初始值，在轉移矩陣方面，由於本研究採用的是帶狀左右連通型模組，現
在的狀態只與上一個狀態有關，在模組初始化的時候，停留在現在的狀態以及轉移到下一個狀態
的機率是一樣的；在觀察機率矩陣方面，初始狀態下每一個觀察元出現的機率是一樣的；在分佈
矩陣方面，由於第一個狀態代表筆勢的第一個子筆劃，而每個筆勢一定都是從第一個子筆劃開
始，於是，初始狀態為第一個狀態的機率為百分之百。 
每個子筆劃的特徵序列在分群時被歸類到 K 個群組裡，而我們再依特徵序列歸屬的群組給
予這個特徵序列一個編號 k 的值，這樣我們就能將所有的特徵序列從原本多個維度的向量，量化
成為一個值，這麼做能夠大幅的降低資料的維度，讓離散型的 HMM 分類器得以進行分類。示
意圖如圖 3-19 所示；由於一個子筆劃的特徵序列會對應到所屬群組的編號，而一個筆勢是由數
個子筆劃組成，所以一個筆勢則會取出一組編號序列，這組編號序列即是 HMM 的觀察序列，
而 HMM 的第 t 個狀態 (state) 則代表了第 t 個子筆劃；而第 t 個觀察元，則代表第 t 個狀態
下所對應到的特徵序列編號。 
28 
 
2. 從各個 sub-stroke 擷取一組特徵序列。 
3. 將筆勢依子筆劃的數目分類到對應的筆勢辨識子系統。 
4. 針對子系統內的筆勢選取不同的特徵序列。 
5. 以 PBM-index 搭配 K-means 演算法對所有的特徵序列做最佳化分群。 
6. 將所有的特徵序列量化為對應於所屬群組的編號。 
7. HMM 的內部參數初始化。 
8. 以波氏演算法訓練 HMM 內部參數。 
9. 將測試用的觀察序列分配到 forward algorithm 計算出最大機率路徑的 HMM 筆勢模組。 
 
表 4-1 每一種筆勢的資料數目 
Pen gesture  
number 
Number of pen 
gestures 
Pen gesture 
 number 
Number of pen  
gestures 
1 147 13 129 
2 151 14 131 
3 150 15 141 
4 115 16 142 
5 147 17 145 
6 141 18 146 
7 143 19 149 
8 147 20 145 
9 143 21 144 
10 144 22 133 
11 149 23 127 
12 137 24 149 
 
表 4-2 筆勢辨識子系統所擁有的筆勢種類 
  
One-stroke 
sub-system 
Two-strokes 
sub-system 
Three-strokes 
sub-system 
Four-strokes 
sub-system
Five-strokes 
sub-system
Six-strokes 
sub-system 
Seven-strokes 
sub-system
classes 
gesture 02 gesture01 gesture06 gesture11 gesture04 gesture04 gesture04 
gesture03 gesture02 gesture08 gesture12 gesture07  gesture18 
gesture05 gesture03 gesture09 gesture13 gesture21   
gesture19 gesture10 gesture13 gesture14    
gesture22 gesture15 gesture14 gesture15    
 gesture17 gesture15 gesture16    
 gesture22 gesture17 gesture20    
 gesture24 gesture20     
  gesture22     
    gesture23         
30 
 
 
表 4-5 PBM-index 的值，子系統二  
Number of clustering PBM-index Number of clustering PBM-index 
2 0.680321 14 0.555342 
3 0.613735 15 0.640199 
4 0.732878 16 0.647923 
5 0.882945 17 0.650735 
6 0.586204 18 0.423192 
7 0.72857 19 0.413174 
8 0.606542 20 0.558577 
9 0.918919 21 0.521066 
10 0.921804 22 0.443925 
11 0.369896 23 0.422895 
12 0.905659 24 0.489077 
13 0.359515 25 0.439081 
the number of best cluster : 10 
 
 
表 4-6 PBM-index 的值，子系統三 
Number of clustering PBM-index Number of clustering PBM-index 
2 0.097212 14 0.045311 
3 0.120345 15 0.075999 
4 0.15348 16 0.064767 
5 0.115091 17 0.030725 
6 0.08869 18 0.029309 
7 0.194449 19 0.050142 
8 0.241137 20 0.02441 
9 0.107277 21 0.032014 
10 0.088893 22 0.041833 
11 0.072095 23 0.022104 
12 0.107269 24 0.032238 
13 0.053657 25 0.043231 
the number of best cluster : 8 
 
32 
 
 
表 4-9 PBM-index 的值，子系統六 
Number of clustering PBM-index Number of clustering PBM-index 
2 0.615816 14 0.180053 
3 0.51249 15 0.176755 
4 0.58725 16 0.170794 
5 0.343709 17 0.127267 
6 0.221037 18 0.129338 
7 0.511277 19 0.129212 
8 0.396203 20 0.122123 
9 0.348743 21 0.144294 
10 0.274649 22 0.122032 
11 0.266933 23 0.139186 
12 0.261097 24 0.110113 
13 0.190866 25 0.133695 
the number of best cluster  : 2 
 
表 4-10 PBM-index 的值，子系統七 
Number of clustering PBM-index Number of clustering PBM-index 
2 0.39735 14 0.215318 
3 0.473591 15 0.135337 
4 0.363369 16 0.210268 
5 0.265659 17 0.199215 
6 0.222697 18 0.254876 
7 0.314737 19 0.205538 
8 0.553151 20 0.184473 
9 0.255413 21 0.238817 
10 0.138343 22 0.206855 
11 0.354089 23 0.208944 
12 0.210327 24 0.196273 
13 0.340476 25 0.169868 
the number of best cluster  : 8 
 
4.3 HMM 模組的測試結果 
在決定最佳的分群數目之後，我們利用 K-means 演算法找出的資料中心點，依照特徵序列對
應到各個中心點的距離將資料分群；接下來，將分配到各個子系統的每一種筆勢以 Baum-Welch [43]
演算法建立 HMM 模組。接著我們開始測試，以 forward algorithm [43]計算觀察序列對應到子系統
34 
 
取指尖的軌跡。本研究一共取了二十四種手寫筆勢做為樣本，以轉折點為基準，將筆勢資料分為數
個子筆劃，並將筆勢依照子筆劃的數目分類到對應的手寫筆勢子系統。本研究從各個子筆劃中擷取
八種特徵，依照子系統內的筆勢種類選取不同的特徵序列，接著以 PBM-index 搭配 K-means 演
算法將各個子系統內的特徵序列分群，並將各個特徵序列量化成一個值；本研究選用隱藏式馬可夫
模型作為分類器，實驗結果得到 99 %以上的平均辨識率。而未來的研究方向仍會朝著以手寫筆勢
辨識系統為基礎，透過網路攝影機等人機介面，將訊息傳遞給機器，讓使用者能以更自然的方式傳
遞訊息。 
 
五. 計畫成果自評 
本計畫研究成果與進度皆符合本計劃原先之設定。預期成果及目標均已達成，本研究成果計
有一篇碩士論文、一篇期刊論文[44]、三篇已發表的會議論文[45]-[47]及。其他相關研究成果正陸
續整理中，相信所有的成果不但可供學術界參考，其對資訊相關產業技術的提升亦有相當之助益。 
 
 
參考文獻 
[1] D. A. Reynolds, "Speaker identification and verification using Gaussian mixture speaker models," 
Speech Communication, vol. 17, pp. 91-108, 1995. 
[2] D. A. Reynolds et al., "Speaker Verification Using Adapted Gaussian Mixture Models," Digital 
Signal Processing, vol. 23, pp. 19-41, 2000. 
[3] P. R. G. Harding, T. J. Ellis, "Recognizing Hand Gesture using Fourier Descriptors," in Proceedings 
of the 17th International Conference on Pattern Recognition, Cambridge UK, August 23-26, 2004, pp. 
286-289. 
[4] J. H. Kim et al., "Hand Gesture Recognition System Using Fuzzy Algorithm and RDBMS for Post 
PC," Springer Berlin, Heidelberg, 2005, Pattern Recognition and Trend Analysis, pp. 170-175. 
[5] D. Xu, "A Neural Network Approach for Hand Gesture Recognition in Virtual Reality Driving 
Training System of SPG," in The 18th International Conference on Pattern Recognition, Hong Kong, 
August 20-24, 2006, pp. 519-522. 
[6] C. L. Liu, S. Jaeger, and M. Nakagawa, "Online Recognition of Chinese Characters: The 
State-of-the-Art," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 2, pp. 
198-213, February 2004. 
[7] M. Talebinejad, A. Miril, and A. D. C. Chan, "A Computationally Efficient HMM-Based 
Handwriting Verification System," in IEEE International Instrumentation and Measurement 
Technology Conference Victoria, Vancouver Island, Canada, May 12-15, 2008, pp. 1868-1872. 
[8] T. Steinherz, E. Rivlin, and N. Intrator, "Offline cursive script word recognition - a survey," 
International Journal on Document Analysis and Recognition, vol. 2, pp. 90-110, 1999. 
[9] T. H. Su et al., "Off-line recognition of realistic Chinese hand writing using segmentation-free 
strategy," Pattern Recognition, vol. 42, pp. 167-182, 2009. 
[10] R. Bertolami, M. Zimmermann, and H. Bunke, "Rejection strategies for offline handwritten text line 
recognition," Pattern Recognition Letters, vol. 27, pp.  2005-2012, 2006. 
[11] R. Bertolami and H. Bunke, "Ensemble Methods for Handwritten Text Line Recognition Systems," 
36 
 
[26] ACECAD DigiMemo A502, http://www.acecad.com.tw/dma502.html. 
[27] F. Lin and X. Tang, "Dynamic Stroke Information Analysis for Video-Based Handwritten Chinese 
Character Recognition," in Proceedings of the Ninth IEEE International Conference on Computer 
Vision, Nice, France, October 13-16, 2003, pp. 695-700. 
[28] Y. Liu, X. Liu and Y. Jia, "Hand-Gesture Based Text Input for Wearable Computers," in Proceedings 
of the Fourth IEEE International Conference on Computer Vision Systems, New York, January 04-07, 
2006, pp. 8-13. 
[29] L. JIN and D. YANG, "A Novel Vision-Based Finger-Writing Character Recognition System," 
Journal of Circuits, Systems, and Computers, vol. 16, no.  3, pp. 421-436, 2007. 
[30] C. C. Hsieh , M. R. Tsai , and M. C. Su, "A Fingertip Extraction Method and its Application to 
Handwritten Alphanumeric Characters Recognition," in 2008 IEEE International Conference on 
Signal Image Technology and Internet Based Systems, November 30-December 03, 2008, pp. 
293-300.   
[31] M. Kherallah, "On-line handwritten digit recognition based on trajectory and velocity modeling," 
Pattern Recognition Letters, vol. 29, 99. 580-594, 2008. 
[32] W. JIANG, Z. X. SUN, "Hmm-Based On-Line Multi-Stroke Sketch Recognition," in Proceedings of 
the Fourth International Conference on Machine Learning and Cybernetics, Guangzhou, August 
18-21, 2005, pp. 4564-4570. 
[33] D. Willems, R. Niels, Definitions for features used in online pen gesture recognition, Technical 
Report, NICI, Radboud University Nijmegen, 2008 
[34] Y. Zhang, G. Shi, J. Yang, "Hmm-Based On-Line Multi-Stroke Sketch Recognition," in 2009 10th 
International Conference on Document Analysis and Recognition, Barcelona, Spain, July 26-29, 
2009, pp. 1255-1259. 
[35] M. Pastor, A. Toselli and E. Vidal, "Writing Speed Normalization for On-Line Handwritten Text 
Recognition," in Eighth International Conference on Document Analysis and Recognition, Seoul, 
Korea, August 31-September 01, 2005, pp. 1131-1135. 
[36] Y. Zhang, G. Shi, J. Yang, "HMM-based Online Recognition of Handwritten Chemical Symbols," in 
10th International Conference on Document Analysis and Recognition, Barcelona, Spain, July 26-29, 
2009, pp. 1255-1259. 
[37] L. R. Rabiner, "A tutorial on Hidden Markov Models and selected applications in speech 
recognition," Proceedings of the IEEE, vol. 77, no. 2, pp. 257-286, February 1989. 
[38] C. J. C. Burges, "A Tutorial on Support Vector Machines for Pattern Recognition," Data Mining and 
Knowledge Discovery, vol. 2, pp. 121-167, 1998. 
[39] T. Kohonen, "The Self-Organizing Map," Proceedings of the IEEE, vol. 78, no. 9, pp. 1464-1480, 
1990. 
[40] S. Han, "Systematic Multi-Path HMM Topology Design for Online Handwriting Recognition of East 
Asian Characters," in Ninth International Conference on Document Analysis and Recognition, 
Curitiba, Parana, Brazil, September 23-26, pp. 604-608. 
[41] Pluskid, 漫 谈  Clustering (1): k-means, Machine Learning, December 29, 2008, 
http://blog.pluskid.org/?p=17. 
[42] D. Comaniciu and P. Meer, "Mean shift: A robust approach toward feature space analysis," IEEE 
 1
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 99  年 9 月 20 日 
一、參加會議經過 
此報告為本人參加 7th IEEE International Conference on Signals and Electronic Systems (ICSES 
2010)會議的出國報告書，參加 ICSES 2010 會議的主要目的，是希望自己能夠從與會期間，與許
多優秀學者進行學術交流，以從他們的研究貢獻獲得自己研究上的靈感。此次會議有許多的專家
學者與會，也邀請了多位國際專家學者進行專題演講。 
ICSES 2010，為期四天，於 2010 年 9 月 7 日到 9 月 10 日，在波蘭/Gliwice 舉行。會議第一
天是報到及歡迎茶會，第二天早上為開幕式及演講，接著便是一連串的 session。這次會議的主題為
訊號處理與電子系統，剛好與本人所學與研究領域相符，會中也邀請了多位專家進行演講，如：瑞
典 Linköping University, Dept. of Computer and Information Science 的 Zebo Peng 教授，演講有關
"Building Reliable Embedded Systems with Unreliable Components" 題 目 ； 德 國
Otto-von-Guericke-Universität Magdeburg 的 Prof. dr hab. Rudolf Kruse，演講有關"Temporal Pattern 
Mining"等；這些演講主題均是目前資訊領域裡面最熱門也最受關注的題目，透過大師的介紹，也
使本人對這些領域有了更深一層的了解。 
另外，本人也聽了不少的 session ，了解世界一些學者對相關領域的研究現況，而在會議晚宴
計畫編號 NSC 99-2219-E-155 -004 
計畫名稱 應用於遠距協同作業之多模態人機智慧型互動關鍵技術研究－子計畫
三：以視覺為基礎之協同作業互動技術研發(I) 
出國人員
姓名 李建誠 
服務機構
及職稱 元智大學/通訊系/助理教授 
會議時間 99 年 9 月 7 日至 99 年 9 月 10 日 會議地點 
波蘭/Gliwice 
會議名稱 
(中文) 第七屆 IEEE 訊號與電子系統國際會議 
(英文)7th IEEE International Conference on Signals and Electronic Systems (ICSES 
2010) 
發表論文
題目 
(中文)基於 Gabor 特徵選取之臉部表情識別 
(英文)Gabor Feature Selection for Facial Expression Recognition 
附件四 
 3
 
圖一，ICSES 2010 會議會場 
 
 
 
圖二，發表論文之海報 
六、其他 
無 
Reviewer 1
Global decision   Accept
  (from 10-Excelent to 1-Poor)
Rating the relevance of this paper   5
Rating the originality of this paper   6
Rating the technical correctness of this paper   6
Rating the clarity of this paper   6
Rating the references for this paper   7
Does this paper need to be edited?   Yes
Suggestion for presentation format   poster
Comments to   
the author   
Some abbreviations need explanation (like RBF).
Please explain the meaning of symbols used in formula 
(1), log with the base 2. 
What does "g" mean in equation (4)?
The indexes m, n should be explained just after the 
formula (2).
Give the functions hR and hI for the Gabor filter.
   (from 10-Excelent to 1-Poor)
Ranking this paper for a best-paper award 4
Ranking this paper for inclusion in the special issue  
of the "Metrology and Measurement Systems"  
and "Applied Mathematics and Computer Science" Journals   
5
Reviewer 2
Global decision   Marginal Accept
  (from 10-Excelent to 1-Poor)
Rating the relevance of this paper   8
Rating the originality of this paper   5
Rating the technical correctness of this paper   7
Rating the clarity of this paper   6
Rating the references for this paper   8
Does this paper need to be edited?   No
b第 2 頁 (共 4 頁)(B)
2011/10/31(B)b
   (from 10-Excelent to 1-Poor)
Ranking this paper for a best-paper award 3
Ranking this paper for inclusion in the special issue  
of the "Metrology and Measurement Systems"  
and "Applied Mathematics and Computer Science" Journals   
2
b第 4 頁 (共 4 頁)(B)
2011/10/31(B)b
 onstrate that the proposed method achieves a high recogni-
tion rate and outperforms other facial expression recognition 
systems. 
The rest of this paper is organized as follows. Section 2 
introduces the proposed effective Gabor feature extraction 
method. Section 3 presents experiment results. Finally, 
conclusions are summarized in Section 4.  
II. EFFECTIVE GABOR FEATURE EXTRACTION BASED ON 
ENTROPY 
The overall process of the proposed facial expression rec-
ognition method is displayed in Fig. 1. First, the preprocess-
ing step is applied to input images to produce standardized 
facial images for subsequent processing. Facial images are 
detected using the Viola and Jones face detector [15]. This 
preprocessing is an important step because the input images 
usually have some slight differences, such as head tilt and 
head size. The original images are cropped into face images 
and resized to 128 × 96. Then, a histogram equalization 
method is applied to eliminate variations in illumination. 
The following feature extraction step uses facial images that 
have been down-sampled by a factor of two. 
This study uses Gabor features to recognize facial expres-
sions from static images. In practice, the dimension of a 
Gabor feature vector is so high that the computation and 
memory requirements are very large. For this reason, several 
sampling methods have been proposed to determine the 
“optimal” subset for extracting Gabor features. This study 
proposes an effective Gabor feature selection to extract the 
informative Gabor features representing the facial character-
istics. Entropy is used as a criterion to measure the impor-
tance of the feature. This approach reduces the feature di-
mension without losing much information and also de-
creases computation and storage requirements. 
 
 Fig. 1. Overall process of the proposed method. 
Entropy is a measure of the uncertainty associated with a 
random variable X in information theory, defined as 
  
x
xXpxXpXH )(log)()(                   (1) 
The less uncertainty there is, the less entropy there is. 
Conversely, more uncertainty produces more entropy. The 
objective of feature selection is to select a subset of features 
that gives as much information as possible. Thus, this study 
formulates an effective feature selection scheme based on 
the feature position probability distribution to select the 
informative Gabor features. 
For a given input image I(x, y), the magnitude of a filtered 
image Fmn(x, y) can be obtained as 
    22 ),(),(),(),(),( yxIyxhIyxIyxhRyxF mnmnmn    (2) 
where * indicates 2-D convolution, and hRmn(x, y) and 
hImn(x, y) represent the real and imaginary parts of the Ga-
bor filters. Let Om,n,x,y(r) denote the occurrence of Gabor 
magnitude response r in Fmn(x, y) for all training images. 
The feature position probability is defined as 
L
yxnm
yxnm n
rO
rp
)(
)( ,,,,,,                               (3) 
where 
  Lr yxnmL rgn 0 ,,, )(  ,       0   r  L.           (4) 
The entropy of the feature position probability distribu-
tion is defined as 
  
r
yxnmyxnmyxnm rRprRpRH )(log)()( ,,,,,,,,,     (5) 
where R is a random variable of the occurrence of Gabor 
magnitude response. The value of the entropy Hm,n,x,y(R) 
indicates the uncertainty of the feature at the pixel position 
(x, y) of the Gabor features with mth scale and nth orienta-
tion for all training images. A larger value of Hm,n,x,y(R) 
means the feature magnitudes vary from different images. 
Thus, features along this axis of the feature space can im-
prove the discriminating power between different expres-
sion classes. On the other hand, a smaller value of the en-
tropy Hm,n,x,y(R) indicates the corresponding features tend 
toward the same magnitude. That is, features along this axis 
contribute less to discrimination. To reduce the feature 
space, these features should not be considered in the classi-
fication phase. Accordingly, sort Hm,n,x,y(R)in descending 
order and use the first M  N Gabor features as the feature 
vector to form a lower M-dimensional subspace, where N is 
the dimension of the original feature space and M is that of 
the effective feature space. Fig. 2 shows an example of 
effective Gabor filters of a face image with three scales and 
eight orientations. Fig. 2(a) shows the magnitudes of the 
Gabor features. Fig. 2(b) shows the points of the top ten 
percent of the Gabor features. 
III. EXPERIMENT RESULTS 
The current study evaluates the proposed algorithm using 
the JAFFE database (Fig. 3) which is commonly used for 
facial expression recognition tasks. The database includes 
210 facial expression images of ten people. Each person has 
seven expressions (anger, disgust, fear, happiness, sadness, 
surprise, and the neutral state) and there are three examples 
of each expression. These images are grayscale and have a 
resolution of 256 × 256. In the preprocessing step, face 
images are detected by the Viola and Jones face detector, 
cropped, and resized to 128 × 96. Histogram equalization is 
then applied to eliminate the illumination variation. After-
wards, the resolution of the face images is down-sampled to 
64 × 48. 
The current study uses the leave-one-out strategy in the 
training procedure, as in [13]. The database is divided ran-
domly into thirty segments for each expression. Then, 
twenty-nine segments per class are used to train and the 
remaining segment is used to test. The procedure of training 
and testing is repeated thirty times until each segment has 
been used in test. Finally, all the recognition rates are aver-
aged to obtain an overall recognition rate for the proposed 
method. 
Table 1 compares different levels of the effective Gabor 
features. Results show that the top ten percent of the se-
lected Gabor features with three scales and eight orienta-
tions achieves the highest recognition rate of 96.73%. This 
means that there was a 10-fold reduction in the number of 
Gabor filters used. The results also clearly show that the 
Gabor features with three scales and eight orientations out-
perform those with five scales and eight orientations. 
140
 ACKNOWLEDGMNENTS 
 We would like to thank the National Science Council 
(Grant number: NSC 97-3114-E-006-001) for supporting 
this work. 
REFERENCES 
[1] A. Samal and P. A. Iyengar, “Automatic recognition and analysis of 
human faces and facial expressions: a survey,” Pattern Recognition, 
vol. 25, no. 1, pp. 65-77, 1992. 
[2] M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial 
expressions: the state of the art,” IEEE Trans. Pattern Anal. Mach. 
Intell., vol. 22, no. 12, pp. 1424-1445, 2000. 
[3] B. Fasel and J. Luettin, “Automatic facial expression analysis: a 
survey,” Pattern Recognition, vol. 36, no. 1, pp. 259-275, 2003. 
[4] C. Darwin, The Expression of Emotions in Man and Animals, John 
Murray, reprinted by Univ. of Chicago Press, 1965. 
[5] D. Keltner and P. Ekman, “Facial expression of emotion,” Handbook 
of Emotions, M. Lewis and J. M. Haviland-Jones, Eds. New York: 
Guilford, pp. 236-249, 2000. 
[6] P. Ekman and W. V. Friesen, The Facial Action Coding System: A 
Technique for The Measurement of Facial Movement, Francisco, 
Consulting Psychologists Press, 1978. 
[7] Y. Yacoob, and L. Davis, “Recognizing faces showing expressions,” 
in Proc. Int'l Workshop Automatic Face and Gesture Recognition, pp. 
278-283, 1995. 
[8] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski, “Measuring facial 
expressions by computer image analysis,” Psychophysiology, vol. 36, 
pp. 253-264, 1999. 
[9] I. A. Essa and A. P. Pentland, “Coding, Analysis, Interpretation, and 
Recognition of Facial Expressions,” IEEE Trans. Pattern Analysis 
and Machine Intelligence, vol. 19, no. 7, pp. 757-763, 1997. 
[10] T. Xiang, M.K.H. Leung, and S.Y. Cho, “Expression recognition 
using fuzzy spatio-temporal modeling,” Pattern Recognition, vol. 41, 
no. 1, pp. 204-216, 2008. 
[11] X. Chen and T. Huang, “Facial expression recognition: A clustering-
based approach,” Pattern Recognition Letters, vol. 24, pp. 1295-1302, 
2003. 
[12] R. Zhi and Q. Ruan, “Facial expression recognition based on two-
dimensional discriminant locality preserving projections,” 
Neurocomputing, vol. 71, pp. 1730-1734, 2008. 
[13] F. Y. Shin, C. F. Chuang, and P. S. P. Wang, “Performance 
Comparisons of Facial Expression Recognition in JAFFE database,” 
International Journal of Pattern Recognition and Artificial 
Intelligence, vol. 22, no. 3, pp. 445-459, 2008. 
[14] X. Feng, M. Pietikäinen, and A. Hadid, “Facial expression recognition 
based on local binary patterns,” Pattern Recognition and Image 
Analysis, vol. 17, no. 4, pp. 592-598, 2007. 
[15] P. Viola and M. Jones, “Rapid object detection using a boosted 
cascade of simple features,” in Proc. IEEE Conference on Computer 
Vision and Pattern Recognition, pp. 511-518, 2001. 
[16] Y. Wang, G. B. Huang, P. Saratchandran, and N. Sundararajan, “Time 
series study of GGAP-RBF network: predictions of Nasdaq stock and 
nitrate contamination of drinking water,” in Proc. the IEEE 
international joint conference on neural networks, vol. 5, pp. 3127–
3132, 2005. 
[17] G. B. Huang, P. Saratchandran, and N. Sundararajan, “An efficient 
sequential learning algorithm for growing and pruning RBF (GAP-
RBF) networks,” IEEE Trans Syst, Man, Cybern B, vol. 34, no. 6, pp. 
2284-2292, 2004. 
[18] H. Yu and J. Yang, “A direct LDA algorithm for high-dimensional 
data with application to face recognition,” Pattern Recognition, vol. 
34, pp. 2067-2070, 2001. 
[19] L. Zhao, G. Zhuang, and X. Xu, “Facial Expression Recognition 
Based on PCA and NMF,” in Proc. the 7th World Congress on 
Intelligent Control and Automation, pp. 6822-6825, 2008. 
[20] X. X. Qi and W. Jiang, “Application of Wavelet Energy Feature in 
Facial Expression Recognition,” in Proc. IEEE international 
Workshop on Anti-counterfeiting, Security, Identification, pp. 169-
174, 2007. 
[21] W. Liejun, Q. Xizhong, and Z. Taiyi, “Facial expression recognition 
using improved support vector machine by modifying kernels,” 
Information Technology Journal, vol. 8, no. 4, pp. 595-599, 2009. 
[22] Chien-Cheng Lee, Yu-Chun Chiang, Cheng-Yuan Shih, and Chun-Li 
Tsai, “Noisy Time Series Prediction Using M-estimator based Robust 
Radial Basis Function Neural Networks with Growing and Pruning 
Techniques,” Expert Systems with Applications, vol. 36, pp. 4717-
4724, 2009. 
142
99 年度專題研究計畫研究成果彙整表 
計畫主持人：李建誠 計畫編號：99-2219-E-155-004- 
計畫名稱：應用於遠距協同作業之多模態人機智慧型互動關鍵技術研究--子計畫三：以視覺為基礎之
協同作業互動技術研發(I) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
