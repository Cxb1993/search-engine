公開查詢
執行單位：台大資訊網路與多媒體研究所
中 華 民 國 2008 年 7 月
日
可供推廣之研發成果資料表
V□ 可申請專利 V□ 可技術移轉 日期： 2008
年 7 月 日
國科會補助計畫
計畫名稱：以漸進式晶格六面演算法為基礎之體積顯像法及其加速與應用（第
2 年）
計畫主持人： 歐陽明
計畫編號： NSC 95-2221-E-002-299-MY2 學門領域：資訊二
技術/創作名稱 顯示三維體積資料之方法
發明人/創作人 歐陽明, 何建璋, 莊永裕, 陳炳宇
附件二
可利用之產業
及
可開發之產品
動畫及遊戲娛樂業. 軟體產品.
技術特點
原始的 marching cubes algorithm 無法妥善處理結構一致的問
題、在某些解析度下會出現裂縫，且難以保留 sharp feature。我們
的方法較簡單且易實作，並可同時解決上述問題，解決方法乃將
3D marching cubes 轉為 2D cubical marching squares，利用 sharp
feature 來決定岐義之處，再利用 sampling face sharp feature 來消
除 inter-cell dependency。。我們運用有這許多優點的 CMS 進行雕
刻，可得到極為精細的模型。
推廣及運用的價值
註: 已提出中華民國專利申請，申請案號為 95102936。
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送
貴單位研發成果推廣單位（如技術移轉中心）。
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
※ 3.本表若不敷使用，請自行影印使用。
計畫成果自評
計畫執行兩年內除以下 8 篇論文發表外, 還有下列成果, 算是成果豐碩:
1. 已於 2006 年十月成功的在台北(台大)舉辦 Pacific Graphics2006, 擔任
International Program Co-chair。所有的會議論文直接刊在 The Visual Computer 期
刊中。
2. 電視(中視, 6/30/2007)與三家報紙(聯合, 自由, 蘋果) 報導: 繽紛魔鏡
(報導如下).
3. 在 國際一流會議 SIGGRAPH2006 展示 "Hand Shadow Illusions and 3D DDR
Based on Efficient Model Retrieval", demo in Emerging Technology, video demo (1)
3D_DDR :http://3d.csie.ntu.edu.tw/s2006etech/ddr.mpg (2) Hand Shadow Illusions:
http://3d.csie.ntu.edu.tw/s2006etech/shadow.mpg , Boston, July, USA, 2006. (台灣的
第一次在此項目被接受).
三生黃庭豪說，Wii 只能做手的動作、還須手拿遙控器，這台跳舞機
不用遙控器，站在網路攝影機前，跟著螢幕動作全身舞動即可。這台
跳舞機去年獲得美國電腦圖學界最大會議 Siggraph 邀請參展、今年更
獲法國 Laval Vir tu l 虛擬實境會議邀請參展。
黃庭豪說，跳舞機抓住影像之後，能將人與背景分離，並比對人的
動作是否正確，未來可應用在醫學復健、舞蹈學習與減肥運動，他跳
5 天瘦了 2 公斤。
論文發表:
1. Wai Seng Ng, Chien-Chang Ho, Ming-Yang Yu, Kai-Hsuan Chen, Bo-Ruei
Chen, Kuan-Ting Liu, Ming Ouhyoung , Practical Issues in a Virtual Sculptor
System, paper accepted to publish in CVGIP2008, The 21th IPPR Conference on
Computer Vision, Graphics , and Image Processing, 2008.
2. Wai Seng Ng, Chien-Chang Ho, Ming Ouhyoung , Practical Issues in a Virtual
Sculptor System for Rapid Modeling, submitted to SIGGRAPH ASIA 2008
Emerging technology & Sketches, December 2008.
3. Chung-Lin Wen, Chang-His Hsieh, Bing-Yu Chen, and Ming Ouhyoung.
Example-based Multiple Local Color Transfer by Strokes.
Computer Graphics Forum, (Pacific Graphics 2008 (PG08) Conference
Proceedings, Tokyo, Japan), 2008
4. Chieh-Ju Tu, Shuen-Huei Guan, Yung-Yu Chuang, Jiann-Rong Wu, Bing-Yu
Chen, and Ming Ouhyoung. Digital Restoration of Moldy Aged Films. ACM
SIGGRAPH 2007 Conference Abstracts and Applications (Sketches Program), San
Diego, California, USA., 2007.
5. Fu-Che Wu, Wan-Chun Ma, Rung-Huei Liang, Bing-Yu Chen, and Ming
Ouhyoung. Domain Connected Graph: the Skeleton of a Closed 3D Shape for
Animation. The Visual Computer, Vol. 22, No. 2, p.117 - p.135, 2006.
6. Ming-Yang Yu, Po-Kuang Chen, Yu-Jen Chen, Meng-Chieh Yu, Jen-Yuan Chiang,
Chien-Lin Tang, Jack Hsieh, Bing-Yu Chen and Ming Ouhyoung. Eco-Echo. ACM
SIGGRAPH 2007 Conference Abstracts and Applications (Art Gallery Program), San
Diego, California, USA., 2007.
7. J.S. Yeh, J.Y Chiang, T.H. Huan, L.F. Lin, Y.C Tsai, L.K Wang, D.Y.Chen, Y.Y
Chuang, B.Y. Chen, M. Ouhyoung, "Hand Shadow Illusions and 3D DDR Based
Abstract
We present a real-time interactive modeling system for users to create sculpture in a
virtual world. We found several practical issues for virtual sculpting, and proposed
solutions to overcome these issues in this paper. Our previous work, Cubical
Marching Squares (CMS), provides a relatively accurate method to generate surface
from volume data. Also, another previous work provides a volumetric data structure
which preserves information of sharp features after boolean operation is applied. By
combining the data structure for boolean operation and CMS to display the data, it is
possible to create sculpture in a natural way in a virtual world. However, an
artist friendly system for sculpting requires continuous operation to mimic the real
world sculpting experiences. Also, the system must provide an intuitive way for artists
to operate, and so these requirements raise four issues to be addressed. First, a motion
graver is created to overcome the zigzag artifacts as shown in Figure 1 , which is
caused by discretely sampled sculpting movements. Second, to perform continuous
boolean operation at interactive rate is another second issue. Third, a force feedback
device is used to provide touchable hints during operations. Finally, some basic
Computer Numerical Control (CNC) like operations are provided for more precise
control over the sculpting operation.
Keywords: Marching squares, Marching cubes, Boolean operations, volume
sculpting, virtual sculpting
（二）研究計畫之背景及目的
1. 動機及重要性
在電腦圖學發明之前，人們在沒有電腦的輔助之下，其實是用一些真實中存的物
體做雕刻的動作，相反的在現今，使用電腦製作一個三維模型，是需要很多特殊
的技巧和專業的商用軟體才能產生，同樣地，若果要對模型做修改，同樣的專業
技術是很需要的。所以說，對於廣羅大眾而言，產生或者修改一個三維模型並不
是一件簡單的事，特別是對於一些希望能快速產生模型的人來說，就更加需要
單一觀看，不做動作是可以接受，但是對於虛擬的雕刻系統來說，連續性的操作
是會出現一些延緩的現象。現在，我們結合現有的多核心系統來加速整個運算過
程。
第三個問題是一個人性化的介面。解決方法是利用一個輸入的控制器---
PHANToM，這個控制器如同之前的一個研究[Lu et al.2002]，該控制器最大的一個
特色提供了6個degree of freedom的力回饋裝置，允許使用者在一個虛擬的環境
下，能夠準確和快速地操作雕刻刀。所以，通過這個力回饋的控制器，可以更簡
單的來雕刻建立模型。至於那個力回饋的功能，能夠提供真實的力回饋，並且快
速的反應給使用者。
最後的問題是便利性和自動化。一些基本的computer numerical control能夠輔
助使用者，讓他們的操作更準確，這些輔助操作如控制雕刻刀貼近物體的表面，
這個功能夠幫助使用者做一些特別的事，例如雕刻文字或者是挖洞。
總結來說，即使是那些能夠保留 sharp feature 的演算法，面對雕刻這種特別應
用，之前的造法會有很多問題，例如是 Z字形的缺點，連續性操作並沒有即時反
應給使用者，效率太差也是一個重要的問題。另外，力回饋和輔助操作能夠提供
一個更方便和更精準的雕刻系統。在這篇報告裏，我們提出了 4個實際的問題和
相對應的解決方法，並且把這些結合一個可以實際使用的虛擬的雕刻系統。
主要有2種方法實作虛擬雕刻系統，第一種是volumetric，Marching Cubes演算
法[Lorensen and Cline1987]是最初被提出來，利用了volume資料結構表現出來。
Galyean et al. [Galyean and Hughes 1991] 利用了Lorensen的方法發展出能用來
modeling的技術。後來Ferley et al. [Ferely et al. 2001] 在傳統的volume結構下
加上adaptive的技巧，還是不能保持模型原有的sharp feature，是因為傳統的
Marching Cubes演算法並沒有保留sharp feature和拓撲學的資訊。所以後來有
人利用更多的資訊，發展出Extended Marching Cubes(EMC) [Kobbelt et al. 2001] 和
Dual Contouring [Ju et al. 2002] ，這兩種都能夠保留物體sharp feature的部份。
直到最近，Perng et al. [Perng et al. 2001] 利用了EMC演算法實做出一個虛擬雕
刻系統。但是，topology consistent在模型mesh裏面是一個很重要的東西，所
以Ho et al. [Ho et al. 2005b] 提出了Cubical Marching Squares (CMS) 演算法，
能夠保留模型中的topology consistent，並且在這個研究 [Ho et al. 2005a] 提出
了一個全新的資料結構，能大大減少記憶體的使用。至於第二種傳統的雕刻系統
的做法是Adaptively Sampled Distance Fields(ADF)，Frisken et al. [Frisken
et al. 2000] 利用ADF並且保留模型中細緻的部份，其後更在2001年提出了一個雕
刻的系統 [Perry and Frisken 2001]。
最近幾年有很多關於雕刻的架構和應用的例子被發展出來[Correa and Silver 2007;
Correa 2007; Pixologic 2007] ，這些或多或少已經整合到一些日常看到的知名商業建
模軟體，如3Ds Studio Max, Maya和ZBrush。這些專業的軟體裡面通常有很多很
強大的功能，如果是受過專業訓練的建模師會覺得使用這些軟體是一件很容易的
事，但是這類軟體的學習通常是花一段時間，對初學者是不容易上手的。所以我
們現在的目標是，希望能夠提供一個簡單的系統給任何人使用，特別是那些對三
維模型不熟的人來使用。
圖二
4.2 效率問題
當使用者操作雕刻刀的時候，可以利用當時雕刻刀的所在位置，建立一個
bounding box，這個box是可以完全包含雕刻刀，接下來，只要檢查那些cell跟
bounding box有重疊的部份，運算的時候只需要重新運算重疊的部份，沒有重疊
所以在雕刻刀貼近物體之後，雕刻刀便能在物體上面做平滑的移動，這個概念能
夠去除一些深度問題。
實作的方法大概如下，第一，因為產生出來的結果是由很多三角形所組成的，所
以先計算雕刻刀和物體的表面的最短距離，然後在該方向射出一條ray，這樣打
到的三角形作為將要貼近的表面，最後雕刻刀便能在貼近物體之後，再利用這個
三角形的normal和tangent的資訊，在那個平面上做平移的動作，這樣，雕刻刀
便能在這個平面上做自由的雕刻。
5. 實驗結果
我們實驗的機器是Intel Core Quad Q6600, 4GB RAM 和 nVidia 8800GT顯卡處
理rendering的部份，至於程式實作的部份，主要是用了C++和Direct3D。我們系
統的架構主要是建立在multi-thread的架構上，因為Cubical Marching Squares
裏面的每一個cell都可以獨立處理的，這樣，便可以分別把cell丟給不同的CPU
core作處理，我們用的multi-thread的函式庫是OpenMP，這樣的一個API能幫助
我們建立起一個即時的雕刻系統，所以，最終的顯示速率可以達到差不多30幀/
秒。圖三可以看到3個主要的加速技巧，它們分別是Bounding Box，Efficient Rays
和Multi-Threading。
我們邀請了一位建模師來使用我們的系統，她一開始只需要15分鐘便已經熟識怎
麼用這個系統，之後，建模師開始真正使用這個系統來建立一些3D模型，如圖四，
相比較起來，如果要用一些專業的商業軟件，可能要花更長的時間來產生。
表一
6. 總結
在這篇報告裏，我們展示了一個即時的雕刻系統，並把它結合動態雕刻刀做應
用。我們利用了convex hull這個概念來產生一些比較平滑的結果，同時克服了
sampling rate不足的情形。這個方法能夠去除大量的Z字形的缺點，但是，從結
果還可以看出一些破碎的邊角，使用者可能會對這種情況不太滿意。所以，在研
究過之前的一些成果後，將來要做的可能是由更複雜的模型去產生動態雕刻刀
[Loop 1994]。
即使sharp feature能夠確實地保留起來，但是演算法用到的octree的資料結構
裏面的深度值其實是有限制的，因為我們的考慮到的是即時性，跟深度的大小會
有很明顯的關係，所以會有這種矛盾的關係。當使用者希望做更細緻調整的時
候，往往會被深度不夠所限制，從另外一個觀點來看，快速的產生模型是我們所
需要的。最後，建模師有提供一個寶貴的意見是，他們通常會希望可以在使用商
業軟體前，先有一個比較粗糙的模型來開始會比較好，所以，我們的系統正好能
夠提供一個很方便的建模介面，再加上有力回饋的功能，讓使用者能夠在短時間
快速的建立起自己的三維模型。
LORENSEN, W. E., AND CLINE, H. E. 1987. Marching cubes: A high resolution 3d
surface construction algorithm. In Proc. SIGGRAPH 1987, 163–169. 1, 2
LU, Y.-H., WANG, W.-T., HUEI LIANG, R., AND OUHYOUNG, M. 2002. Virtual
sculptor: A feature preserving haptic modeling system. In Proc. of ACM International
Workshop on Immersive Telepresence. 1
PERNG, K.-L., WANG, W.-T., FLANAGAN, M., AND OUHYOUNG, M. 2001. A
real-time 3d virtual sculpting tool based on modified marching cubes. In Proceedings
of International Conference on Artificial Reality and Tele-existence, 64–72. 2
PERRY, R. N., AND FRISKEN, S. F. 2001. Kizamu: a system for sculpting digital
characters. In Profroceedings of ACM SIGGRAPH, 47–56. 2
PIXOLOGIC, 2007. Zbrush: Software from pixologic. http://www.pixologic.com. 2
PREPARATA, F. P., AND HONG, S. J. 1977. Convex hulls of finite sets of points in
two and three dimensions. Communication of the ACM 20, 2, 87–93. 3.1
is the motion artifacts. We assume our system is able to perform
continuous sculpting operation. Although, CMS preserves sharp
feature, however, sharp features cause another problem, motion ar-
tifacts. As shown in Figure 1(a-c), since the graver’s motion is dis-
cretely sampled during operation, in combination of sharp features,
the zigzag artifacts become more serious.
The second issue is performance. The extension of our work is to
speed up our previous system [Ho et al. 2005a]. One of the contri-
butions of our system is to make it faster and the user can use the
system interactively. The previous system is recalculated for all the
cells each time when performing the Boolean operation. It is fast
enough in single shot basis. However, while performing continuous
carving, it causes a delay time for the user. We also take advantage
of multi-threads system on a multi-core PC to speed up the whole
computation.
The third issue is a friendly user interface for sculpting. A primary
input device, PHANToM is integrated into this system as our pre-
vious work [Lu et al. 2002]. The main feature of the controller is
provided six degree of freedom haptic device, which allows user to
move the graver quickly and accurately in three-dimensional space.
Thus, sculpturing the model can be more easily through the hap-
tic controller. Another feature of PHANToM is force reactions, it
provides a realistic and quickly force feedback to the user.
The last issue is convenience and automation. Some basic Com-
puter Numerical Control (CNC) like operation is provided to assist
the user to perform sculpting operation more precisely. The assisted
operations related to controlling the graver include ”attach to sur-
face” and ”attach to axis”. Attach operations help user to perform
particular tasks, such as engraving character or drilling holes. An-
other assisted operation spins the object to create a pottery wheel
like environment.
To sum up, although sharp feature preserving algorithms preserve
the characteristic of sculptures, previous solutions still suffer from
zigzag artifacts, and low performance while performing continu-
ous operations. Also, force feedback and assisted operations help
a user to perform sculpting more easily and more precisely. In this
paper, we address these four practical issues and provide solutions
to create a practical virtual sculptor system.
2 Related work
There are two main approaches for sculpturing. The first one is vol-
umetric approach. Marching cubes algorithm [Lorensen and Cline
1987] is first proposed to provide a volume data structure to rep-
resent object and Galyean et al. [Galyean and Hughes 1991] ap-
plies it to be a volumetric modeling technique. Although Ferley
et al. [Ferley et al. 2001] proposed an adaptive volume sculpt-
ing system, the original marching cubes algorithm will not pre-
serve sharp features and will not maintain topology consistency.
By using more information for the volume data structure, extended
marching cubes(EMC) [Kobbelt et al. 2001] algorithm and Dual
contouring(DC) [Ju et al. 2002] method is proposed to preserved
more details. Perng et al. [Perng et al. 2001] proposed a real-
time volume sculpting using EMC algorithm. However, topology
consistency is an essential issue of a mesh. Ho et al. [Ho et al.
2005b] proposed a cubical marching squares(CMS) algorithm to
deal with topology consistency and a novel data structure which
can greatly reduce memory consumption [Ho et al. 2005a]. The
second approach is adaptively sampled distance fields. Frisken et
al. [Frisken et al. 2000] propose a system with adaptively sam-
pled distance fields which can preserved fine details and Perry et al.
[Perry and Frisken 2001] proposed a system for sculpting digital
characters.
There are several frameworks and application systems for sculpting
that have been proposed in recent years [Correa and Silver 2007]
[Sculpting 2007]. They are integrated to numerous famous 3D soft-
ware packages such as 3D Studio Max, Maya, and ZBrush. These
professional packages have many powerful features. Professionals
may feel intuitive to operate these professional software, but it may
not easy for beginners. Our goal is to provide an easy to use mod-
eling environment for everyone, especially for those who are not
familiar with three-dimensional mesh modeling software.
3 The Virtual Sculptor System
Our proposed system pipeline is as follows. First of all, user can
import his/her own model or creates model directly with objects
provided from our system. Secondly, our system captures the infor-
mation from the PHANToM device, while a Wii remote controller
is held in another hand for motion parallax display. After all, the
system can create a motion graver and directly use it as a input to
the Cubical Marching Squares framework. In this process, our sys-
tem calculates the force at the same time. Finally, the output is
shown in front of the user. The solutions of the four issues men-
tioned previously are described in the rest part of this section.
3.1 Motion Artifacts
The artifacts created from traditional virtual sculptor is addressed
in this section. The zigzag edges shows in Figure 1 and Figure 2(b)
can be a big problem when an artist uses a box as the graver to
perform carving. We restricted the graver to be convex in three
dimensions. And this restriction is not a serious problem because
most of the traditional gravers in real world are point, flat bottom,
round, knife and liner shapes.
The solution we found is that the sampling problem can be trans-
lated to a convex hull problem. The three-dimensional convex hull
is a minimal convex set containing a set of points in three dimen-
sions. We use two samples of the graver operation, carving or stuff-
ing, while sample is a set of points that forms a set and two sets are
on different position. Two sets of points are directly processed by
the convex hull algorithm at the same time [Preparata and Hong
1977]. The output of the set of points is the convex set of the two
samples. Then we use this convex set to perform graver operation.
The temporal result seems to be smooth at the edge between the
graver and the sculpted object since a smooth convex hull is created
and the sampling problem thus has disappeared. The algorithm for
creating a motion graver is given in Algorithm 1.
Algorithm 1 CreateMotionGraver. This procedure creates a lin-
ear approximated motion model of two successive samples (model
A and B) of a graver.
1: procedure CREATEMOTIONGRAVER(Model A, Model B)
2: for each vertex v in A.vertex
3: vertexlist← vertexlist ∪ v;
4: end for
5: for each vertex v in B.vertex
6: vertexlist← vertexlist ∪ v;
7: end for
8: return CONVEXHULL(vertexlist);
9: end procedure
3.2 Performance issues
The user holds a virtual graver which has position and rotation. We
created a bounding box that contains the graver and only recalcu-
lates those cells which are overlapping with the bounding box. The
Graver Object Average time(before acceleration)
Average time
(after acceleration)
Cube Cube 262.0ms 39.2ms
Sphere Cube 298.8ms 39.5ms
Tetrahedron Cube 281.5ms 42.1ms
Cylinder Cube 318.9ms 59.8ms
Table 1 This table shows the time average of a boolean operation on the target
object. Columns from left to right: graver shape, sculpted object, time average
of the old system and time average of our proposed system.
(a) (b)
(c) (d)
Figure 4 Some results created from our system.
may want a more smoothing result. One possible solution of the
problem is to translate to a more complicated model of the motion
graver such as smooth spline surfaces [Loop 1994].
The depth problem is mostly addressed in this system. Like many
other system, our system provides an additional camera view port to
help the artist to feel the space perception. Two views can provide
an additional feature to an artist when the artist wants to get more
depth information. Second issue the artist provides is that she wants
a floor such as a ground attached to an object. Also, she wants a
fixed rotation to the object and performs rotation along the normal
of the ground, where we propose to use a Wii remote controller in
a separate hand for motion parallax effect.
Although the sharp features are mostly preserved by adaptive reso-
lution, the problem is that the depth of the octree is limited. Thus,
the user cannot perform a detailed manipulation after generating a
rough model at first. It is a dilemma between quality and processing
time. On the other hand, our artist provided a useful comment that
many artist want to have a rough model before editing in profes-
sional three-dimensional modeling software such as 3ds Max, Maya
or ZBrush. Therefore, our system provides a convenient model-
ing interface with haptic interface and the artist can create a rough
three-dimensional model in a few minutes.
6 Acknowledgments
This research is sponsored by the National Science Council, Taiwan
(R.O.C.) under contract numbers NSC95-2221-E002-299, NSC96-
2752-E002-006-PAE, and NSC96-2622-E002-002. We would like
to thank our artist ”Kelsy Tang” for her valuable feedback.
References
CORREA, C. D., AND SILVER, D. 2007. Visualizing collision effects
between penetrating and non-penetrating objects. In SIGGRAPH ’07:
ACM SIGGRAPH 2007 sketches, ACM, New York, NY, USA, 47. 2
FERLEY, E., CANI, M.-P., AND GASCUEL, J.-D. 2001. Resolution adap-
tive volume sculpting. Graphical Models 63, 6, 459–478. 2
FRISKEN, S. F., PERRY, R. N., ROCKWOOD, A. P., AND JONES, T. R.
2000. Adaptively sampled distance fields: A general representation of
shape for computer graphics. In Proceedings of ACM SIGGRAPH, 249–
254. 2
GALYEAN, T. A., AND HUGHES, J. F. 1991. Sculpting: an interactive
volumetric modeling technique. In Proceedings of ACM SIGGRAPH,
267–274. 2
HO, C.-C., TU, C.-H., AND OUHYOUNG, M. 2005. Detail sculpting
using cubical marching squares. In ICAT ’05: Proceedings of the 2005
international conference on Augmented tele-existence, ACM, New York,
NY, USA, 10–15. 1, 2, 3.3
HO, C.-C., WU, F.-C., CHEN, B.-Y., CHUANG, Y.-Y., AND OUHY-
OUNG, M. 2005. Cubical marching squares: Adaptive feature preserving
surface extraction from volume data. Computer Graphics Forum 24, 2.
1, 2
JU, T., LOSASSO, F., SCHAEFER, S., AND WARREN, J. 2002. Dual con-
touring of hermite data. In SIGGRAPH ’02: Proceedings of the 29th
annual conference on Computer graphics and interactive techniques,
ACM, New York, NY, USA, 339–346. 1, 2
KOBBELT, L. P., BOTSCH, M., SCHWANECKE, U., AND SEIDEL, H.-P.
2001. Feature sensitive surface extraction from volume data. In Proc.
SIGGRAPH 2001, 57–66. 1, 2
LOOP, C. 1994. Smooth spline surfaces over irregular meshes. In SIG-
GRAPH ’94: Proceedings of the 21st annual conference on Computer
graphics and interactive techniques, ACM, New York, NY, USA, 303–
310. 5
LORENSEN, W. E., AND CLINE, H. E. 1987. Marching cubes: A high
resolution 3d surface construction algorithm. In Proc. SIGGRAPH 1987,
163–169. 1, 2
LU, Y.-H., WANG, W.-T., HUEI LIANG, R., AND OUHYOUNG, M. 2002.
Virtual sculptor: A feature preserving haptic modeling system. In Proc.
of ACM International Workshop on Immersive Telepresence. 1
PERNG, K.-L., WANG, W.-T., FLANAGAN, M., AND OUHYOUNG, M.
2001. A real-time 3d virtual sculpting tool based on modified marching
cubes. In Proceedings of International Conference on Artificial Reality
and Tele-existence, 64–72. 2
PERRY, R. N., AND FRISKEN, S. F. 2001. Kizamu: a system for sculpting
digital characters. In Profroceedings of ACM SIGGRAPH, 47–56. 2
PREPARATA, F. P., AND HONG, S. J. 1977. Convex hulls of finite sets of
points in two and three dimensions. Commun. ACM 20, 2, 87–93. 3.1
SCULPTING, V. 2007. Virtual sculpting youtube video keywords: Virtual
sculpting siggraph 2007 sketch. In SIGGRAPH ’07: ACM SIGGRAPH
2007 sketches, ACM, New York, NY, USA. 2
Pacific Graphics 2008
T. Igarashi, N. Max, and F. Sillion
(Guest Editors)
Volume 27 (2008), Number 7
Example-based Multiple Local Color Transfer by Strokes
Chung-Lin Wen† Chang-Hsi Hsieh† Bing-Yu Chen‡ Ming Ouhyoung§
National Taiwan University
Abstract
This paper investigates a new approach for color transfer. Rather than transferring color from one image to
another globally, we propose a system with a stroke-based user interface to provide a direct indication mechanism.
We further present a multiple local color transfer method. Through our system the user can easily enhance a defect
(source) photo by referring to some other good quality (target) images by simply drawing some strokes. Then, the
system will perform the multiple local color transfer automatically. The system consists of two major steps. First,
the user draws some strokes on the source and target images to indicate corresponding regions and also the
regions he or she wants to preserve. The regions to be preserved which will be masked out based on an improved
graph cuts algorithm. Second, a multiple local color transfer method is presented to transfer the color from the
target image(s) to the source image through gradient-guided pixel-wise color transfer functions. Finally, the defect
(source) image can be enhanced seamlessly by multiple local color transfer based on some good quality (target)
examples through an interactive and intuitive stroke-based user interface.
Categories and Subject Descriptors (according to ACM CCS): I.4.3 [Image Processing and Computer Vision]: En-
hancement
1. Introduction
Nowadays, digital cameras are very popular, and most peo-
ple take many photos on their trips, reunions, etc. However,
since most end users are not professional photographers, a
user usually takes a lot of photos but eventually finds out
that only a small portion of them are satisfactory. Indeed,
with powerful commercial post-processing software, the ex-
perts could enhance these defect photos, but this task can
be time-consuming. For those who are not skillful in edit-
ing photos, it might even be unachievable. Fortunately, due
to the ease of taking photos, people may dispose of several
photos of the same object or similar scene captured by using
one or more cameras. Thus, it is easy to acquire some good
quality photos of the same object or similar scene, and it is
possible to use them for enhancing the defect photos.
The direct enhancement of the photos by using some post-
processing tools may not be an easy task. For example, in the
† e-mail:{jonathan, isaddo}@cmlab.csie.ntu.edu.tw
‡ e-mail:robin@ntu.edu.tw
§ e-mail:ming@csie.ntu.edu.tw
case of a backlighted photo, one may think that the user can
use the brightness/hue adjustment tools as contained in or-
dinary image processing software. However, such tools usu-
ally can only be used to adjust the brightness/hue globally. If
such tools are applied to achieve a brighter foreground, the
background might be overexposed. Even if the foreground
region in interest is carefully specified, the process is not
only time-consuming but also may result in artifacts at the
foreground and background border. On the other hand, if we
can refer to the same object in other good quality photos, it
would be much easier. Figure 1 shows three cases and our
result along with their reference example(s).
In this paper, we propose an easy-to-learn and easy-to-use
system for photo enhancement. To use our system, no extra
photographic equipment and knowledge is required and the
users also do not have to change their habits in taking photos.
What they only have to is to acquire some other good quality
photos as targets and draw some strokes on the source and
target photos to indicate the corresponding regions and the
regions they want to preserve. Then, a multiple local color
transfer operation is applied to transfer the color from the
targets to the source photo through gradient-guided pixel-
c© 2008 The Author(s)
Journal compilation c© 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.
C.-L. Wen, C.-H. Hsieh, B.-Y. Chen, & M. Ouhyoung / Example-based Multiple Local Color Transfer by Strokes
(a) (b)
(c) (d)
Figure 2: (a) Strokes on the source photo. The light blue
stroke was drawn for preserving the background, since there
is no such stroke on the target photo (b). (b) Corresponding
strokes on the target photo. (c) The source photo. (d) Our re-
sult. The background is preserved completely, and the scarf
is successfully recovered.
some strokes drawn by the user. The color of the strokes indi-
cates correspondence between the source and target photos.
After the user has drawn some strokes, we first analyze
them. We first collect the pixels under the strokes that specify
the regions to be preserved (background) and to be edited
(foreground) on the source photo to build the pixel sets PsB
and PsF , respectively, where B and F are the labels used to
denote the background and foreground regions, respectively.
In addition to this, we use p to denote a pixel. cp and lp are
the color and label of pixel p and Is and It are the source and
target photos.
4. Background Preservation
Before applying the color transfer, in order to avoid unex-
pected modification of the background regions, we conduct a
background preservation process to segment the source im-
age into foreground and background regions based on the
strokes PsF and P
s
B as described in Section 3. The back-
ground preservation process is performed by an improved
version of the graph cuts algorithm [BVZ01]. It is used to
minimize the following energy function:
E(lp) = ∑
p∈Is
Ec(p)Ep(p)+α ∑
p,q∈Is,q∈Np
Es(p,q), (1)
where lp is a possible labeling of pixel p and Np denotes the
neighboring pixels of p, i.e., q ∈ Np means that p and q are
neighboring pixels.
Ec(p) is the color term which is used to measure the con-
formity of the color of pixel p and is defined in the same way
as in most of the previous work [WYC∗06,RKB04,LSS05].
The foreground strokes PsF and the background ones P
s
B are
used to build 3D GMMs (Gaussian Mixture Models) to de-
scribe their color distributions. These are used to estimate
whether the color of a pixel p is closer to the foreground or
the background region.
Es(p,q) is the smoothness term which is intended to
maintain the edge in the image. It uses the L2−norm distance
of the neighboring pixels p and q in L∗a∗b∗ color space to
measure the smoothness of the two pixels. If the color dis-
tance of the two pixels is small, it aims for making the labels
of two pixels as same as possible. Otherwise, two different
labels can be assigned to the two pixels.
Besides the color and smoothness terms as used in the tra-
ditional graph cuts algorithm, we introduce a new position
term Ep(p) in Eq. (1), which utilizes the spatial informa-
tion specified by the strokes in order to avoid discontinu-
ous segmentation due to similar colors in different regions
of the image as shown in Figure 3 (a). If a pixel is closer
to the foreground/background strokes, it is more likely that
it should belong to the foreground/background regions. Fur-
thermore, if a pixel lies between the foreground and back-
ground strokes, it means that the user expects the system to
decide where to place the border. We use the following equa-
tion to model this:
Ep(p) =
1
2
sign(r)×|r|n +0.5, (2)
where r ∈ [−1,1] indicates the normalized distance differ-
ence from pixel p to the foreground or background strokes
Psx , x ∈ {F ,B} and the opposite ones Psx¯ which is defined as:
r =
Dist(p,Psx)−Dist(p,Psx¯)
Dist(p,PsF )+Dist(p,P
s
B)
,
and the distance function Dist(p,Psx) is defined as:
Dist(p,Psx) = min
p′
‖p− p′‖,∀p′ ∈ Psx .
Moreover,
n = aexp(−b Dist(p,P
s
F )+Dist(p,P
s
B)
max(ws,hs)
)
is used to decide on the importance of the spatial informa-
tion. ws and hs denote the width and height of the source
image Is, and are used to normalize the distance functions.
With the new position term Ep(p)∈ [0,1]∝ r, we achieve
c© 2008 The Author(s)
Journal compilation c© 2008 The Eurographics Association and Blackwell Publishing Ltd.
C.-L. Wen, C.-H. Hsieh, B.-Y. Chen, & M. Ouhyoung / Example-based Multiple Local Color Transfer by Strokes
(a) (b)
Figure 4: (a) The result of directly applying the pixel-wise
local color transfer functions for transferring color. (b) The
close-up view of the red rectangle in image (a).
(a) (b)
Figure 5: (a) The weight of the gradient-guided constraints.
(b) The difference between Figure 2 (c) and (d).
in Figure 4. Thus, before performing the color transfer,
we need to use the gradient of the original source image
Is to improve the pixel-wise local color transfer functions
u(p), f (p) and v(p) and obtain the gradient-guided color
transfer functions uˆ(p), ˆf (p), and vˆ(p) based on the the
following quadratic energy function proposed by Lischin-
ski et al. [LFUS06]:
ˆf = argmin
ˆf
{
∑
p∈Is
w(p)( ˆf (p)− f (p))2 +λ ∑
p∈Is
h(∇ ˆf (p),∇L)
}
,
(7)
where the first term constraints the solution ˆf (p) to be as
close to the original f (p) as possible and w(p) is defined as:
w(p) = max
j∈x
C(cp, j), ∀lp = x ∈ {F ,B}.
The second term in Eq. (7) is the smoothness term which is
defined as:
h(∇ ˆf (p),∇L(p)) = | ˆfx(p)|
2
|Lx(p)|α + ε
+
| ˆfy(p)|2
|Ly(p)|α + ε
,
where L is the log-luminance channel of the source image Is,
and the subscripts x and y denote spatial differentiation. This
ensures consistency between the neighbor values in ˆf (p),
but allows for rapid changes across significant edges. Al-
though Eq. (7) is used for enhanced the scaling parameter
f (p), the shifting parameters u(p) and v(p) are improved by
applying the same process. Figure 5 (a) shows the weight of
the gradient-guided constraints.
5.3. Color transfer
Finally, we conduct the color transfer via Eq. (3). Hence,
each pixel is altered by different and suitable transfer pa-
rameters. Furthermore, the color transferring process does
not influence the preservation (background) regions. Figure
5 (b) shows the difference between the source image and the
final result. The background is preserved completely and no
detail is lost in the photo enhancement process.
6. Results
Figure 2 (c) shows a common case where the defect photo
is taken against the light source. We used another photo (the
left upper photo of Figure 1 (a)) taken on the same day to
enhance it. The source defect photo is successfully enhanced
by our color transfer method and both the clothing and scarf
in the source photo are successfully recovered as shown in
Figure 2 (d). Figure 2 (a) and (b) show the strokes drawn
on the source and target photos respectively.
Figure 1 (c) shows a situation where is difficult to take
a satisfying photo by a specific exposure. The source photo
(the left lower photo of Figure 1 (c)) is taken by the exposure
which is suitable to capture the sea of clouds. For the over-
exposed sky, we chose another photo (the right lower photo
of Figure 1 (c)) taken on an airplane at sunrise and transfer
the color of the sky to enhance it. The result is shown in the
upper photo of Figure 1 (c).
Our multiple local color transfer method can also be used
as for global color transfer. Figure 6 (a) shows an overex-
posed photo and Figure 6 (b) shows another photo taken
by a different person on the same trip, which served as tar-
get photo. Our result (Figure 6 (c)) is compared with Rein-
hard et al.’s [RAGS01] one (Figure 6 (d)).
Figure 7 shows the comparison of our result and that of
Tai et al.’s local color transfer method [TJT05]. The source
photo is taken with the red tree, so the surface of the river
reflects a little bit of red light. Tai et al.’s result as shown
in Figure 7 (c) successfully separates the trees and the river
when transferring the green color, however, the river still in-
cludes a little bit of red. Our river as shown in Figure 7 (d)
correctly reflects the green light on the river.
Sometimes it is hard to figure out how to achieve a com-
bination of effects in a single step. In this case, it is better
to have a progressive editing feature in the system, so that
we can concentrate on one part first and then processing the
others. Figure 8 demonstrates that our system is capable to
c© 2008 The Author(s)
Journal compilation c© 2008 The Eurographics Association and Blackwell Publishing Ltd.
C.-L. Wen, C.-H. Hsieh, B.-Y. Chen, & M. Ouhyoung / Example-based Multiple Local Color Transfer by Strokes
(a) (b) (c)
(d) (e) (f)
Figure 9: An editing example of using multiple target pho-
tos. (a) The source photo. (b) Our result. (c) The strokes on
the source photo (a). (d)∼(f) The target photos and their cor-
responding strokes, including the target photo for the (d) sky,
(e) mountain, and (f) forest.
(a) (b) (c)
Figure 10: (a) The source photo taken by long exposure for
recording the track of the cars’ lights. (b) The target photo
record the mood in the evening. (c) Our result is created by
combining the two subjects.
roads, and houses, but the sea and sky become too bright
because of overexposure. However, the user takes another
photo (Figure 10 (b)) to capture the mood of the sea in the
evening. Hence, we can use our system to combine these two
subjects to create a better photograph.
Figure 2 (c) and Figure 11 show a result of enhancing the
foreground which is mixed with the background, therefore
it requires tedious segmentation in traditional approaches
if partial editing is preferred. Using our stroke-based user
interface, the underexposed foreground is successfully en-
hanced, but the small background regions surrounded by
the foreground are not influenced. In addition, although the
bookshelf in the source and target photos are not specified
by the user, since its color is similar to the tablets, it can also
be enhanced by the tablets in the target photo.
(a) (b) (c)
Figure 11: (a) The strokes on the source photo. (b) The
strokes on the target photo. (c) Our result.
Table 1: Processing time in each stage. (in seconds)
Scarf Coast Sunrise
(Figure 2) (Figure 8) (Figure 1 (c))
Image dimension 480 × 640 428 × 640 640 × 424
Color space 1.133 1.133 0.978
conversion
Graph cuts 2.984 3.094 3.454
Gradient-guided 3.969 2.046 3.781
improvement
Upsampling 3.453 3.579 2.891
Other processes 1.148 0.992 0.830
Total 12.687 10.844 11.953
7. Implementation
In our method, the biggest problem with obtaining the
gradient-guided color transfer function is performance. Al-
though downsampling is quite useful for speeding up
the process, upsampling is another problem. We use
Kopf et al. [KCLU07]’s joint bilateral upsampling method
to solve this problem. In their method, they combine region
information of the full resolution source photo in the up-
sampling process, and the result is smooth and edges are
preserved. Therefore, we can achieve an interactive photo
enhancement system with a stroke-based user interface and
still have good results.
In our system, the solution calculated on one eighth size
in width and height is enough to get acceptable results, and
the upsampling takes about 4 seconds for a 0.3 mega-pixel
image in our implementation. Table 1 lists the processing
times at each stage of some results in this paper. All results
in this paper were produced by following parameter settings:
α = 16 in Eq. (1); a = 5 and b = 5 in Eq. (2); λ = 0.2, α = 1
and ε = 0.0001 in Eq. (7).
8. Conclusion and Future Work
In this paper, we have proposed an example-based photo en-
hancement system with an interactive and intuitive stroke-
based user interface. Furthermore, a multiple local color
transfer method is presented and applied to transfer color
from the target examples to the source defect photo through
gradient-guided local (pixel-wise) color transfer functions.
Our method can produce accurate results that match the
c© 2008 The Author(s)
Journal compilation c© 2008 The Eurographics Association and Blackwell Publishing Ltd.
