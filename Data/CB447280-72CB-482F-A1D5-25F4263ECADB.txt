集中心之間的距離，不僅減少反覆運算
時間以提升探勘效率，且又不失探勘精
確度。 
關鍵詞：資料探勘、資料串流探勘、隱
私保護資料探勘、分類、頻繁項目集探
勘、分群 
二、 緣由與目的 
資料探勘(Data Mining)是一種從
大量資料中萃取出知識的技術。近年
來，隨著網路頻寬的提升與設備的發
展，導致資料串流(Data Streams)的興
起。資料串流與傳統的靜態資料不同，
其特點為[1]: (1)資料具有時間性；(2)
資料的模式會隨著時間不斷改變；(3)
資料量龐大；(4)資料流入的速度極快；
(5)需要即時回應。由於上述的特性，資
料串流的管理與分析具有很高的困難
度。因此，如何快速且精確地探勘資料
串流已成為重要的研究議題[2]。 
然而，現行的資料串流探勘(Data 
Streams Mining)的研究並沒有考量到
資料隱私的問題，機密資料容易被有心
人士進行攻擊或破壞。Verykios 等學者
[3] 針 對 隱 私 保 護 資 料 探 勘
(Privacy-Preserving Data Mining)的研究
現況作整理及分類，將現行的隱私保護
資料探勘技術分為五類，分別是資料分
佈、資料修改、資料探勘演算法、資料
或規則隱藏、以及隱私防護。 
在第ㄧ類研究中[4]，現行的方法分
別針對集中式與分散式的資料庫來設
計。在分散式資料庫的部分，來源資料
分散於不同的儲存體，並且在此環境下
從事資料探勘的操作。而分散式資料探
勘又可區分為水平式資料切割和垂直
式資料切割兩種型式。水平式資料切割
表示資料分佈在多個單位，每個單位的
欄位相同且各自擁有部分資料紀錄；而
垂直式資料切割則表示，每個單位除了
聯結鍵(Join key)外，其餘的欄位相異，
但通常擁有相同的資料紀錄。 
在第二類研究中[5]，公開的資料先
行透過擾亂、阻擋等機制對原始資料作
保護，避免原始資料中隱密性資料被揭
露[6, 7, 8, 9, 10, 11, 12]。此類型的研究
不外乎使用加密技巧將原始資料加密
或加入亂數之擾亂，但是必須確保能夠
由加密後的資料中萃取出資料探勘過
程所需要的各項資訊。 
在第三類研究中，現行的研究大都
集中於探勘技術之分類[13]、分群[14]
和關聯規則[15]等三類方法。這些研究
利用不同的探勘需求設計出不同的演
算法來達到隱私保護的目的。在第四類
研究中，經由修改機制將相關的資料內
容進行隱藏或修改來達到保護的目的
[16]，透過減少公開的資訊量，使探勘
單位不易從中推論得知隱密性的資料
內容。 
在第五類研究中[17]，隱私保護的
方式乃選擇性的修改資料內容，使修改
後資料保有較高的品質及可用性。隱私
保護的技術可分為：啟發式基礎
(Heuristic-based) 技術，密碼學基礎
(Cryptography-based)技術，以及重建基
礎 (Reconstruction-based) 技術等三大
類。第一類技術採用選擇性的修改部分
特定資料，將修改資料而損失的資料利
用率減至最低。第二類技術是以安全多
單位計算協定(Secure multiparty proto-
col)來達到隱私保護的目的[18, 19, 20, 
21]。各單位於計算完成時，除了各自
 2
保護資料隱私的作法；第三小節說明
WASW演算法如何探勘資料串流；第四
小節利用實驗數據說明本研究所提出
之方法與過去的方法在安全性以及效
能上的比較。 
3.1.1 PCDS 研究架構 
圖 1 為隱私保護資料串流探勘流
程圖，本流程主要分為資料串流預處理
以及資料串流探勘兩個階段。在第一個
階段中，當資料串流抵達資料串流預處
理 系 統 (Data Streams Preprocessing 
System, DSPS)後將資料進行批次處
理，因此能夠有效地利用系統資源並且
將資料擾亂。在第二個階段中，即時資
料探勘系統(Online Data Mining System, 
ODMS)使用移動框架 (Sliding Window)
的模式來儲存每次接收到的資料串
流，因此能夠較有彈性地儲存和探勘資
料。最後，當資料分佈產生變化時，
ODMS 能立即更新分類模型。 
 
 
 
圖 1 隱私保護資料串流探勘流程 
 
3.1.2 資料串流預處理 
由於資料串流無法在一個固定時
間內被收集齊全，因此過去的技術不適
合應用於資料串流。除此之外，資料串
流在不同時間內的分佈狀況也會不
同，使用過去的方法將會使資料的誤差
增加，導致錯誤的探勘結果。因此，在
資料串流預處理階段，我們利用圖 2 資
料切割擾亂演算法(Data Splitting and 
Perturbation, DSP)來解決隱私保護的問
題。DSP 主要為利用資料中的非機密屬
性作為資料切割的條件，以對資料進行
隱私保護。首先，針對輸入值進行設定
(步驟 1)，當原始資料輸入後，計算每
個屬性內的變化量，以找出具最大變化
量之屬性 j* (步驟 1)。接著，以 j*作為
第一個切割節點，計算屬性的中間值，
並利用中間值將資料切割成兩個子集
合(葉節點)(步驟 3)。之後，重覆步驟 2、
3 選取與切割的動作，直到資料集中沒
有可分切割的屬性(步驟 4)。最後，將
每個葉節點 t 內的 nt筆機密資料利用其
平均值來作替換，以達到隱私保護效
果，並且將各個葉節點已被擾亂的資料
集合輸出到 ODMS(步驟 5、6)。由上述
DSP 演算法步驟可看出，當一筆資料具
有越多非機密屬性時，DSP 的隱私保護
效果越好，且經過隱私保護的資料值，
在進行資料探勘後也保持著一定水準
的探勘精確度。 
3.1.3 資料串流探勘 
在資料串流探勘階段，我們利用加
權平均移動框架演算法改良傳統的
VFDT 演算法來探勘經過擾亂後的資
料串流。圖 3 為 WASW 演算法，當
ODMS 接收到資料串流後，使用移動框
架 W 來儲存每一批進入的資料串流，
並且依照進入的先後順序給予不同的
時間權重。由於新的資料之獲益價值會
比舊的資料高，因此給新的資料較高的
權重能夠反應資料的變化。接著，利用
Hoeffding bounds 抽樣方法所形成的分
裂評估函數 Hb()，將資料分類以建立分
 4
記錄聯繫(Distance-Based Record Link-
age, DBRL)來評估 DSP 演算法的安全
性： 
   


N
i
ii xyN
ASD
1
2)(1      (1) 
                            
 
  


n
i i
i
i
i
y
yy
x
xxDBRL
1
2)
)()(
(      (2) 
 ASD 利用空間距離計算公式來測
量資料點之間的差異性。而 DBRL 除了
計算兩個資料之間的距離之外，還將資
料的變異性也納入計算，因此能夠找出
經過擾亂後的資料與原始資料的變異
程度。其中 為原始機密值； 為擾亂
後的值；N 為資料紀錄的數量；
ix iy
x為 的
平均數；
ix
y 為 的平均數；iy )i(x 為 的
標準差；
ix
)( iy 為 的標準差。ASD 實
驗的結果如圖 4 所示，在 5 個不同的資
料集中，DSP 演算法都比其他演算法有
較高的 ASD 值，因此有較高的安全
性。圖 4 中可以看出在第 5 個資料集
內，各個演算法產生的 ASD 值比其他
資料集產生的 ASD 值低，這是因為資
料集內能用來擾亂資料的數值屬性較
少。由此得知，在擾亂的過程中，數值
屬性的多寡是決定洩漏風險程度的標
準。當數值屬性越多，資料將會被嚴重
擾亂，因此會有更低的資料洩漏風險。
DBRL 實驗的結果如圖 5 所示，在 5 個
不同的資料集中，DSP 演算法的 DBRL
值都是最小的，代表原始資料與經過擾
亂後的資料之間的關聯較低。由此得
知，經過 DSP 演算法擾亂的資料值能
夠被推測出的機率不高，故能有較高的
安全性。 
iy
圖 4 安全性之 ASD 比較 
 
 
 
圖 5 安全性之 DBRL 比較 
 
3.1.4.2 資料誤差測量 
在原始資料值與經過擾亂的值之
間，我們使用偏誤平均差 (Bias in Mean, 
BIM)和偏誤標準差 (Bias in Standard 
Deviation, BISD)來評估DSP演算法的
誤差：  
)(
X
XYBIM              (3) 
 
)(
X
XY
S
SSBISD           (4) 
BIM 計算資料擾亂前與擾亂後，平
均值產生多大變化，來判斷資料的誤差
 
 6
 3.2.1 PFSM 研究架構 
3.2.1.1 轉換程序 PFSM 主要分為轉換、探勘與回推
等三個程序。如圖 9 所示。傳送方在進
行資料傳送前，先經由轉換程序將原始
交易資料內容保護之後，再交由分析單
位進行頻繁項目集探勘。最後，將探勘
結果傳回後，經由回推程序對其探勘結
果進行轉換，即可獲得正確的頻繁項目
集。程序之詳細說明如下： 
此程序採用資料匯總[26]的概念設
計符合資料串流環境之需求的保護機
制。由於是在資料串流環境下，資料流
入後先進行將資料內容轉為純文字檔
的預先處理，才經由轉換機制將原始資
料進行處理，再交由分析單位進行探
勘。為了因應資料串流環境的特性，本
論文之轉換機制採用匯總資料的概
念，原始交易資料透過公式的轉換，將
多個欄位數合併成一個欄位值，以改變
原始資料的內容，進而達到資料隱私保
護之目的。這樣的方式不但可以達到保
護資料之目的，同時可以減少資料之項
目欄位，縮短探勘所需之時間。分析單
位仍然可以從轉換後的資料探勘出符
合條件的頻繁項目集。轉換程序的運作
為資料傳送方先決定兩個參數值 p 及
c，其中 p 為欲合併的固定欄位數
(p≧2)，而ｃ為一常數(c≧2)。接著進
行原始交易資料之轉換。此程序利用公
式(5)循序合併每 p 個項目成為一個新
的項目欄位 ，合併後交易紀錄中的
欄位值 為公式(6)記算求得，轉換之
示意圖如圖 10 所示。其中
rA
xyA



2
p  為每次
位移的項目個數，由於在資料串流的環
境下除了探勘精確度之考量外，其探勘
效能亦是重要的考量之一。因此在本研
究中我們將每次的項目位移個數設為



2
p ，讓探勘之效能與精確度得到平
衡 。
 pz 1
(1) 轉換程序：由於原始交易資料可能
含有隱密性資料內容及未知的隱密性
資訊，所以在資料傳送至分析單位探勘
之前，必須先透過資料匯總之轉換程序
對原始資料內容進行項目匯總的處理
進行保護。 
(2) 探勘程序：此探勘演算法的目的主
要是將新增的資料加入資料庫，並將過
期的資料刪除後，進行快速的漸進式探
勘找出頻繁項目集。 
 (3) 回推程序：由於分析單位是對轉換
後的資料進行頻繁項目集之探勘，因此
所探勘出之結果亦受到保護。所以，資
料方必須將分析單位之探勘結果藉由
回推的程序，找出其正確之探勘結果。 
 
 




zpr
z
r acA
2
)1(
1
 , 
圖 9 PFSM 流程圖 
 8
3.2.1.2 探勘程序 
此探勘演算法為漸進式頻繁項目
集探勘之演算法，採用移動框架的概念
所設計，利用框架位移更新資料內容，
進行即時性資料之快速探勘。演算法步
驟如圖 11。 
演算法步驟說明: 
步驟1. 設定框架大小(W)，最小支持
度(s)。 
步驟2. 當資料( iD )進入以後，首先依
據框架資料找出所有 2-itemset
候選項目集 kC ,12 ，若 FII  且
符 合 門 檻 值 之 條 件
( iDs  )，則將該候
選項目集加入頻繁項目集之
集合中(FI)，若 F
numI . 
II  且不符
合 門 檻 值 之 條 件
(  snumI *  posIz . k zD,
，則由頻繁項目集之集合中移
除。 
.  )
步驟3. 利用 2-itemset 之頻繁項目集
之 集 合 找 出 3-itemset 、
4-itemset…等等更高之後選項
目集( khk )，並計
算該些項目集之出現次數。判
斷該些項目集是否為頻繁項
目集( 
h
k
h CCC
,1,1,1
1 
 kdbsnumI ,1.   )，並
將該些頻繁項目集加入頻繁
項目集之集合中。 
步驟4. 將最後之頻繁項目集結果輸
出。 
步驟5. 重複步驟 2 至步驟 4，直到框
架已滿( k = n )。 
步驟6. 在 新 的 串 流 資 料
( )進入後，由
於框架已滿，首先刪除過期的
串流資料(  1,imk kD )，
並重新檢視該些頻繁項目集
是否仍為頻繁項目集，並移除
條 件 不 符
( 
   jnk kD,1
 
   nposIk kDsnumI ,.*.  )
之項目集。 
步驟7. 將新增資料(  )加入框架後，
再重新搜尋框架資料中所有
2-itemset 候選項目集，並判斷
該些候選項目集是否符合門
檻值之條件。如符合條件
( kDsnumI .  )則將該候
選項目集加入頻繁項目集之
集 合 中 ， 若 不 符 條 件
(    kposIz zDsnumI ,.*.  )
則由原頻繁項目集之集合中
移除。 
步驟8. 利用 2-itemset 之頻繁項目集
之 集 合 找 出 3-itemset 、
4-itemset…等等更高之後選項
目集( jihj  )，並
計算該些項目集之出現次
數。判斷該些項目集是否為頻
繁 項 目 集
( 
i
h
ji
h CCC
,,,
1 
 jidbsnumI ,.   )，並將該
些頻繁項目集加入頻繁項目
集之集合中。 
步驟9. 將最後之頻繁項目集結果
輸出。 
hL
步驟10. 每當新的資料串流進入後，重
複執行步驟 6 至步驟 9。 
 
 
 
 10
  
 12
 
圖 11 FPSM 之探勘程序演算法 
  
  
  
  
  
  
 14
 其中， 
 : 轉換後欄位數w (












2
1
i
inw ); 
n : 原始資料表的欄位數; 
由於轉換過程中合併項目欄位之
因素，可能無法找出合併項目間彼此
連性。轉換後之量 資料在探勘
失問題。
此項目彼此間之特性與合併的欄位
數，
 
之關 化
後可能導致探勘結果之遺 因
皆會影響其探勘之結果。舉例來
說，合併項目 321 ,, aaa 為 1A 之後，可能
無法得知 321 ,, aaa 彼此間之關連性。因
此項目合併的欄位數越大，雖然可以
減少越多的資料處理量，但是也越容
易出現規則遺失之情況。因此本研究
在項目位移個數為 
2 的設定下,可以
有效提升探勘之精確度，減少頻繁項
目集遺失之問題。 
Almaden Research Center 所開發的人
工資料集生成器
erator)產生虛擬交易資料庫，本實驗利
用將資料庫內容分割成多個相同大小
 i
本實驗採用的資料產生器為 IBM 
(Synthetic data gen-
的 b
料庫
產生
ucket 來模擬資料串流的環境。本
實驗之實作部份為了模擬串流環境，
在資料庫內容產生後先進行將資料內
容轉換為純文字檔的預先處理，接著
進行演算法之轉換步驟。再依序讀入
檔案交由資料分析方進行頻繁項目集
探勘，探勘之結果也是以純文字檔案
輸出並傳回。最後進行回推的步驟求
得實驗資料之實際探勘結果。 
實驗部分為了評估方法的效能及
其適用的資料特性，本實驗主要分成
兩個部份，分別用來測試執行之效能
與項目集遺失程度。針對虛擬資
之設定參數(D, N, T, P, AP, R)、轉
換程序中之合併欄位項目數(i)、最小
支持度(Sup.)以及框架位移數(s= 


2
i )
進行測試評估，詳細實驗設計內容如
下。 
(一) PFSM, FP-Growth 與 Aprio
首先，Apriori 演算法以循序的方
式逐步
ri 演
算法之效能比較 
找出所有的高頻項目集，每次
來
行探
次探勘，直到
所有資料讀取完成。總運算時間為各
演算
的項目產生都需要重新掃描資料庫
搜尋頻繁項目，因此每當有資料新增
的時候，就必須重新對整個資料庫進
勘，龐大的計算量導致探勘時間
大幅度的增加，以至於在探勘效能方
面不理想。FP-Growth 演算法在探勘的
過程中，由於不需要產生後選項目
集，因此比 Apriori-based 演算法達到
更好的效能。但是在資料串流的環境
中，該演算法面臨與前者相同的問
題，在每次新增資料的時候皆需進行
整個資料庫的掃描，因而仍然不適用
於資料串流的環境。 
圖 13(a) 和 (b) 為 PFSM 、 
FP-Growth 以及 Apriori 演算法的效能
比較。在本實驗中每一次資料進入後
各探勘演算法就進行一
法每次探勘所花費的時間總和。
即表示若有 y 筆資料，則各演算法分
別進行ｙ次探勘所需之時間總和。圖
中橫軸為項目集的最小支持度門檻
值，縱軸為總運算之時間。設定框架
位移量 d=10k，合併項目欄位數 i = 3，
D100k_N1k_T10_P2k_AP4_R.25_i3 和
D100_N1k_T10_P2k_AP8_R.25_i3 之
參數設定下(項目集平均長度為 4 和
8)，各演算法所花費的總計算時間。由
圖 13 中可清楚看出在支持度較小的時
候 PFSM 演算法可以大幅節省總計算
 16
D100k_T10_P10k_AP4_R.25_Sup.1%_i3
4.2
2.1 1.8 1.4
0.2
0
1
2
3
4
5(%
)
1k　 2k 3k 4k 5k
項目總個數(N)
高
頻
項
目
集
遺
失
程
度
PFSM
D100k_N1k_T10_P10k_AP4_R.25_i3
57
15
5 3.5 1
0
10
20
30
40
50
60
0.25% 0.50% 1% 2% 4%
最小支持度 (Sup.)
高
頻
項
目
集
遺
失
程
度
(%
)
PFSM
 
 
 
圖 15 項目總個數與項目集遺失之影
響 
 
 
圖 17 最小支持度與項目集遺失之影
 
) 項目欄位之合併個數(i)的影響： 
 
_P10
 
響 
(5
D100k_N1k_P10k_AP4_R.25_Sup.1%_i3
3.8 4.2 5.5
7.2
20.6
0
5
10
15
20
25
10 15 20 25 30
交易紀錄平均長度(T)
高
頻
項
目
集
遺
失
程
(%
)
度
PFSM
 
 
圖 16 : 交易紀錄平均長度與項目集遺
失之影響 
 
(4) 最小支持度(Sup.)的影響： 
_P10K_AP4_R 數設定下，
持度愈低愈容易造成項目遺失的問
件之
    如圖 17 所示，在 D100k_N1k_T10 
.25_i3 之參
支
題。因為當支持度愈低，符合條
如圖 18 所示，在 D100k_N1k_T10
K_AP4_R.25_ Sup.1% 之參數設
定下，合併的項目欄位數愈大，項目
集遺失的程度愈嚴重。因為合併的項
目欄位數愈多，愈容易造成項目彼此
間互相影響。所以合併的項目欄位數
愈少，愈容易找出趨於完整之高頻項
目集。 
D100k_N1k_T10_P10k_AP4_R.25_Sup.1%
2 4
6.5
12
18
0
5
10
15
20
2 3 4 5 6
項目欄位之合併個數(i)
高
頻
項
目
集
遺
失
程
度
(%
)
PFSM
 
 
圖 18 項目欄位合併個數與項目集遺
 
.3 隱私保護資料串流分群探勘 
說
項目集愈多，就愈容易出現項目集遺
失的情況。所以支持度愈高，此方法
愈能找出完整的高頻項目集。 
 
失之影響 
3
在這一小節中，我們將細部地
 18
 
表2 30筆3個屬性的資料 
年齡 薪資 購買金額
 
 
31 30000 2000 
40 33000 3500 
38 31000 1500 
24 28000 1800 
28 27000 2200 
26 22000 1500 
25 23000 3000 
23 25000 1600 
34 32000 2500 
42 28000 4100 
20 18000 1600 
21 16000 2000 
29 30000 1400 
36 27000 2200 
22 23000 3000 
32 29000 4600 
25 24000 3200 
34 31000 3400 
23 25000 2800 
42 38000 4500 
37 41000 2100 
41 43000 1700 
22 24000 3000 
29 31000 2800 
30 29000 2300 
26 34000 1400 
19 21000 3300 
36 45000 2900 
23 23000 2100 
36 41000 2900 
 
假設所處理的資料串流包含多筆
多維度數值型資料 1 ...X K ，每筆資
料 都 有 其 時 間 戳 記
... ...，多維度資料以
X ...
(timestamp) 1T KT
iX = (
1
ix ...
d
(圖 )的形式表示，每一列代表一
筆資料，而一欄代表資料的一個屬
再配合圖 21 的旋轉轉換矩陣 R(θ)
對資料進行擾亂，其作法是將座標軸
上的資料以一個 θ 角度順時針方向旋
轉以擾亂資料。圖 22 顯示資料旋轉擾
亂程序，參數定義如下： 
mn 為擾亂前的資料， '
ix
後的資料。  
)表示。當串流資料進
m × n 的資料矩陣
為擾亂
來，將資料以一個
20mnD
性。
 D mnD
 T 為未擾亂屬性個數。 
 為使用者所設定的擾亂角度。 
 R )( 為使用者所設定擾亂角度的
, ≤ 為從
中選取所要擾亂的屬性。 
為所選取屬性擾亂前的
為所選取屬性經
過
旋轉轉換矩陣。 
(1 
('V
 jA kA k n , kj 與
mn
資料，
≤ j
,j AA
)
D
),( kj AA V
)k
擾亂後的資料。 
 
 
 
資料矩陣 圖 20 
 
 
 
 20
 
表3 擾亂前的資料 
 
年齡 額 薪資 購買金
25 23,000 3,000 
23 25,000 1,600 
34 32,000 2,500 
42 28,000 4,100 
 
 
 
 
 
表4 擾亂後的資料 
 
年齡 金額 薪資 購買
13,532.424 14,141.400 -12,446.741
13,398.809 15,374.376 -14,549.639
17,508.380 19,675.608 -18,347.454
16,831.526 17,206.504 -14,882.439
 
3.3
微群集生成 
對於旋轉擾亂後的資料，首先依
個數，利用微
群集
    所 微群集為群集特徵向量的
延伸，主要是用來記錄關於旋轉擾亂
n
.1.2 分群探勘 
3.3.1.2.1 
據使用者設定的微群集
生成程序產生微群集。以微群集
中各個資料點的屬性值來計算微群集
的統計資訊，之後以此統計資訊代表
該微群集。對於微群集生成程序所產
生的每一個微群集，都附予一個獨特
的 id。由於微群集經過優化，使得微
群集在更新的過程中，提升微群集吸
收(absorb)資料點的正確性，進而得到
較好的分群結果。 
謂的
後資料點的統計資訊。假設一個微群
集代表 個多維度資料 1 ...X nX ，每一
個多維度資料以 iX = ( .. 表示，
記 。每個微群集總共有 d + 3)
個資料項，d 為屬性個數，表示方式為
{
1
ix .
d
ix )
(2
並且每一個多維度資料都有其時間戳
1T ... nT
SS , TS , SST, ST, n} 。 其 中
SS =  dPSS
d
SS,...,SSSS ,..,, 21
2
)( pix ， p
, 
PSS = 1ni 1 ，為第 p 個
屬 性 資 料 值 的 平 方 總 和 ；
TS =  dTSTS ...,, , 
n p
PTS
d
TS ,,..,21
ix ， pPTS =i1 1 ，為第 p 個
為屬性資料值的總和；SST 2)
1T ... nT
 = ni 1
的平方總和；
iT(
時間戳記 ST 
= ni iT1 為時間戳記 1T ... T n
某
2
和
4
{{ 22 + 23 , 24 + 22 , 23 + 25 },{ 2 + 3, 4 + 2,
3 + 5}, 23 + 24 , 3 + 4, 2}也就是{{13, 
20, 34}, {5, 6, 8}, 25, 7, 2}。 
 
 
n 的
明，
總
假
和
的數量。舉 設
筆 3 A(2, 4, 3)
3
， 則 該 微 群 集 的 表 示 方 式 為
；
為資料點
個微群集代表
B(3, 2, 5)
例說
維度資料
，其時間戳記分別為 和
 
 22
詳細步驟說明如下： 
：對於每一對資料點 和 ，計
算它們之間的歐基里德平方
Step 2：找出距離最大的兩個資料點，
儲存到群 中心集合 
S，並且將群集中心個數q加2。 
：判斷是否目前的群集中心個數
，若是則執行Step 4，反
Step3.1：對於群集中心以外的每一個
資料點
Step 3.1.2
：對於每一個群
 
Step 3.1.2：找出每一個資料點 與各
集中心 的歐基里
德平方距離的最小值。 
D 中找出最大值，並將
該資料點 設為新增的群
集中心。接著將群集中心個
Step 4：以 Q 個群集中心為初始群集中
心，利用 K-means
Q 個微群集 M。 
 
圖23 微群集生成程序 
 
舉例說明，假設使用者設定的微
群集個數 為 4，
A(20, 20), B(23, 24), 
C(21
計算每一對資料點之間的歐
點，選取
前的群集中心個數為 2
Input：P,  Q
Output： M   
1. For each xi, xj do 
DD∪{ };  ),(2 ji xxd
2. SS∪{x, y} s.t. = Max[D]; ),(2 yxd
q q+2; 
3. While ( q ＜ Q) do 
{    
3.1 For each ix   do S
        {   
3.1.1 For each   do ks  S
  DiDi∪{ };),(2 ki sxd
3.1.2 DminDmin∪{Min[Di]}; 
         } 
3.2 S  S∪{ } s.t. Min[Di] = 
Max[Dmin];  
ix
q q+1; 
}  
4. Use K-means to generate M ; 
Return ;  
ix jx
距離 ),(2 ji xxd 。 
將其 集
Step 1
Step 3
q等於使用者設定的微群集個
數Q
之，則執行Step 3.1與Step 3.2。 
ix ，執行Step 3.1.1與
。 
Step 3.1.1 集中心 ks ，
計算資料點 與群集中心
s 之間的歐基里德平方距
ix
) 。
k
離 ,( sxd 2 ki
個群 之間
ix
Step 3.2：從 min
ix
Q為 3，資料點個數 n
4 個資料點分別為
, 26), D(23, 25)，時間戳記分別為
1, 2, 3, 4。
基里德平方距離，分別為 ),(2 BAd = 25, 
),(2 CAd = 37, ),(2 DAd = 34, 
),(2 CBd = 8, ),(2 DBd = 1, ),(2 DCd = 
5。由於 A 與 C 為距離最大的兩個資料
A 與 C 為群集中心。由於目
，小於使用者所
設定的微群集個數，重新計算資料點
B, D 與群集中心 A, C 之間的歐基里德
數q加1，返回Step 3。 
演算法產生
 
 
 
 24
除該微群集。若所有新舊時戳皆大於
門檻值 δ，則合併兩個最相鄰的微群
集。假設微群集 A 與微群集 B 合併為
微群集 AB，更新微群集的統計資訊的
作法如下： 
ABn
i
p
ix ))((
2
1 = Ani pix ))(( 21 + Bni pix ))(( 21 ; 
 ni ABpix1 )( =  ni Apix1 )( +  ni Bpix1 )( ; 
ni ABiT1 2 ))(( = A ; ni iT1 2))(( +
 =  + ; 
合併後的微群集 id為雙方 id的聯集
(union)。 
3.3.1.2.2 幾何時間結構配置 
結構的時間配置來儲存，
相 較 於 傳 統 的 金 字 塔 時 間 結 構
何時間
照所在的框架編號
ni BiT1 2))((
 ni ABiT1 )( = + ( ; ni AiT1 )( ni BiT1 )
ABn An  Bn
    更新後的微群集以快照的形式利
用幾何時間
(pyramidal time frame) [28]，幾
結構解決了金字塔時間結構所產生的
多餘性(redundancy)問題，使記憶體的
使用更有效率。 
幾何時間結構是將快照分配到不同的
框架編號(frame number)上，編號值介
於0到 )(log2 T 之間，T為資料串流的最
大時間長度，而快
指的是快照被儲存當時所在的粒度
(granularity)層級。儲存在框架編號為i
的快照，其時刻要滿足可被 i2 整除的
條件，因此，儲存在框架編號為0的快
照，其時刻皆為奇數。另外，假設
max_capacity為各層級最多可儲存的
快照數目，且由上述可知框架數量的
上限不超過 )(log2 T ，因此可知自資料
串流開始到T時間單位所儲存的快照
個 數 最 多 為 (max_capacity)   
)(log2 T 。 
 
表6 產生的5個微群集 
 
 資料點 微群集 
D11, D12 {{12287146.088996, 
68, 
1.36}, 
{15657.506, 27036.804, 
366375131.113
9726691
-13882.4}, 265, 23, 2} 
D5, D6, D7, 
D8, D14, 
D15, D17, 
D19, D23, 
{{1237220221.088996, 
4025306040.209392, 
D27, D29 
1072082446.12}, 
{121522, 209933.084, 
-107838.6}, 3344, 170, 
11} 
D
D16, D18, 
{{2093163461.370428, 
{151545.358, 
1, D2, D3, 
D4, D9, 
D10, D13, 
D24, D25 
6244391409.073056, 
1800249195.12}, 
261750.592, -140106.4}, 
2161, 125, 11} 
D20, D21, 
D26 
{{873403382.014404, 
2607534094.997216, 
{51019.93, 88156.528, 
826343628.72}, 
-49572}, 1517, 67, 3} 
D2 , 
D30 
{  
3333704191.95048, 
{57829.858, 99935.324, 
2, D28 {1116322351.265988,
1124225046.8}, 
-58004.4}, 2168, 80, 3} 
 
以下為幾何時間結構配置快照的
原則：假設s為
幾何時間結構
整除，如果s能被 整除，而不被 12 i
新生成的快照，當s進入
時，首先判斷s是否能被
i2 i2
 26
Input：W, C, K 
utput：C, Gj  O
1. For each mi, sj do 
}; 
td Min[Di]; 
j s.t. = Min[Di]; 
3. F
DiDi∪{ d ),(2 ji sm
2. For each mi do 
Poin is[i]
   CenterM[i] s ),(2 ji smd
or each sj do 
   sj   
i m nterM[i] = sj
i
i
n
n  s.t. Ce ; 
 ea  mi, sj do 
D Di∪ }; 
ter j s.t. sj = Min[Di]; 
＞ Pointdis[i] ] do 
{
ch mi s.t. 
4. For ch
i { ),(2 ji smd
5. For each mi do 
Cen M[i] (2 ms ), ji sd
6.While  i [ 2d  ) , ( iCenterMmi  
 
6.1 For ea  ) iterM  ＞Pointdis, (2 Cenmd i [i] do 
{ 
h mi, sj do 
D
6.1.1 For eac
iDi∪ }; 
      int Min[Di]; 
j s.t. = Min[Di]; 
o 
{ ),(2 ji smd
6.1.2 For each mi do 
Po dis[i]
      CenterM[i] s ),(2 ji smd
6.1.3 For each sj d
      sj   
i m nterM[i] = sj
i
i
n
n  s.t. Ce ; 
r ea h mi, sj do 
D
6.1.4 Fo c
iDi∪ }; 
          6
nt j s.t. sj = Min[Di]; 
}  
7. For eac mi do 
  Gj ∪{ mi } s.t. CenterM[i] = sj; 
{ ),(2 ji smd
.1.5 For each mi do 
Ce erM[i] (2s ), ji smd
} 
h 
Gj
Return; 
 
圖24 聚合群集生成程
  
 28
),(2 ji smd  。
Step 6.1.2： ，找出它
sj之間的歐基
里德平方距離的最小值，將其
Step 6.1.3：對於每一個目前的群集中心點
sj。 
，重新計
s 之間的
歐 基 里 德 平 方 距 離
。
Step 6.1.5： ，將其目
sj 儲 存 到
。 
儲存到所歸屬
。
25 顯示聚合群集生成過程，用來
圖 25(a)顯示所
有的聚合點 k1, k2, k3 為群集中心。
圖 2
對於每一個聚合點
與各個群集中心
對於每一個聚合點
對於每一個聚合點
前 的 群 集 中 心
CenterM[i]
的聚合群集 G
，其中
im
儲存到Pointdis[i]，並且將其
目前的群集中心 sj 儲存到
CenterM[i]。 
sj ，計算群集內的加權平均
數，並將計算結果儲存到
Step 6.1.4： im
算它與各個群集中心 j
),(2 ji smd  
im
Step 7：將每一個聚合點 im
j  
圖
說明聚合群集生成的概念。
5(b)顯示經過聚合群集生成程序的步
驟 1 至步驟 2 後形成的三個群集，並且更
新每一個群集中心。經過步驟 6 的判斷與
計算後，從圖 25(b)可看出更新群集中心
後，聚合點 m1 和 m2 與原來的群集關係
產生變化。有別於傳統 K-means 演算法需
對全部聚合點進行計算，聚合群集生成只
需針對與原來的群集關係產生變化的 m1
和 m2 進行計算。圖 25(c)顯示最後的分群
結果，原本屬於群集 3 的聚合點 m1 變為
屬於群集 1，而原本屬於群集 2 的聚合點
m2 變為屬於群集 1。 
 
 
 
(a) 
 
 
 
(b) 
 
 
 
(c) 
 
圖25 聚合群集生成過程 
 
 30
已證明 CluStream 具有良好的探勘精確度
[28]。雖然 PPCDS 分為隱私保護和資料串
流探勘兩階段，但是在精確度的比較上，
我們只針對探勘結果進行分析，因此並無
比較不適當的問題。接著，我們藉由控制
資料點數量、維度數量及群集數量所產生
出的人造資料集來試驗 PPCDS 的延展性
(scalability)，所謂延展性是指隨著資料數
量與參數變化時，系統所能處理的能力。
最後利用微群集率對探勘精確度所造成
的影響來進行 PPCDS 的敏感性(sensitivity)
分析，所謂敏感性分析是指分析一個系統
周圍條件發生改變時，引起結果變化的敏
感程度。 
 
表9 實驗執行環境 
 
硬體部分 
CPU  
2.8GHz 
Intel Pentium D 
記憶體 1GB 
硬碟 160GB 
軟體部分 
作業系統 ws XP Windo
程式語言 ++ 6.0 Visual C
 
料 使用美國計
算 oc ng Ma-
hinery, ACM) KDD-CUP’98 器官慈善捐
贈資
S的精確度評估 
的實驗中，我們使用平
均距離變異(average of the sum of square 
]來衡量精
，因此目前時刻 前的期間 的
其群集中心之間的歐基里德平方距離總
和除以聚合群集個數K，如公式(12)所示。
針對
PPCDS 的串流資料處理能力進行試驗，資
’98，參數
在真實資 集方面，我們
機協會(Ass iation for Computi
c
料集，這個資料集包含了 95412 筆捐
贈者的個人資料，每筆資料包含 481 個屬
性，本實驗從每筆資料中擷取 56 個屬性
來進行實驗。在人造資料集方面，為了試
驗 PPCDS 的延展性，我們藉由控制資料
點數量、維度數量及群集數量的改變以產
生人造資料來進行分析。所產生的人造資
料呈高斯分佈，並且為了反映資料串流隨
著時間的演變，我們在人造資料集每產生
10K 個資料點就改變當前高斯分佈的平均
數與變異數。 
3.3.2.2 實驗結果 
3.3.2.2.1 PPCD
    在精確度評估
distance, Average SSQ) [28, 29
確度，Average SSQ的值越小表示精確度
越 高 。 資 料 來 源 為 真 實 資 料 集
KDD-CUP’98，實驗參數設定為n = 2000
和t = 2。所謂平均距離變異(Average SSQ)
定義如下：假設在目前時刻Tc前的期間h
中，總共有W個聚合點，對於每一個聚合
點mi找出距離最近的群集中心sj，並計算
出 mi 與 sj 之間的歐基里德平方距離
),(2 ji smd Tc h
平均距離變異等於h中所有W個聚合點與
圖26(a) (b)顯示在不同期間h和資料串流
速率SP下，探勘精確度變化的情形，其中
SP = 200表示資料串流以每單位時間200
個資料點的速率匯入。圖26的橫軸為不同
的單位時間數量，而縱軸為平均距離變異
(Average SSQ)。從圖中可看出，雖然資料
在探勘的過程中經過旋轉擾亂的隱私保
護處理，但因為旋轉擾亂具有等量轉換的
特性，因此對探勘結果的精確度並不會造
成太大的影響。另外，在微群集生成的過
程中，透過群集中心的優化產生高品質的
微群集，進而提升探勘的精確度。 
3.3.2.2.2 PPCDS的延展性評估 
在延展性評估的實驗中，首先
料來源為真實資料集 KDD-CUP
 32
不同維度數量和群集數量的設定，我們可
看出 PPCDS 的執行時間非常穩定，除了
當維度數量劇增時，因為資料旋轉擾亂是
利用維度來進行隱私保護處理，會造成執
行效率降低，但大致上還是具一定程度的
延展性。 
 
 
 
圖28 維度數量變化的影響 
 
 
 
圖29 群集數量變化的影響 
 
3.3.2.2.3 PPCDS的敏感性評估 
為了要得到精確度高的探勘結果，微
集的數量要遠大於聚合群集的數量，然
行效率和
    
群
而過於大量的微群集會降低執
記憶體使用效益，因此如何在探勘精確度
與儲存效益之間取得平衡是非常重要
的。在這個實驗中，我們以 KDD-CUP’98
資料集為資料來源，藉由控制微群集的數
量，使用微群集率(micro-cluster ratio)和平
均距離變異來評估微群集的數量對探勘
精確度的影響。所謂的微群集率為微群集
數量除以聚合群集數量。圖 30 顯示微群
集率對精確度的影響，橫軸為不同的微群
集率，而縱軸為平均距離變異。這裡我們
將單位時間數量固定為 200，SP = 200 和
h=16。從圖中可看出，如果使用的微群集
數量與聚合群集數量相同時，得到的探勘
精確度會較差，主要是因為使用的微群集
數量太少所造成的結果，但當微群集率增
加時，探勘精確度會隨之提升。在微群集
率到達大約 15 之後，可發現探勘精確度
轉為穩定。這個結果顯示，要得到好的探
勘精確度並不一定需要把微群集數量設
太大，只要微群集數量和聚合群集數量之
間達到一定比例即可。 
 
 
 
圖30 微群集之敏感性分析 
 
四、 計畫成果自評 
1. 成果與創建 
護資料串流探勘
的問題，並且分別針對分類、頻繁項目集
進行研究與改良。在分類
探勘
本研究探討隱私保
與分群探勘技術
的研究上，我們提出一個隱私保護資
料串流分類的方法，包含資料串流預處理
與資料串流探勘兩個階段。在資料串流預
 34
拓展其應用層面。 
3. 研究內容與計畫相似程度 
研究方法及架構與計畫大致相同。 
，「資料串流關聯規
則探勘之隱私保護研究」，2007
趙景明、黃炳勳，
保 年資訊技
，台北
趙景明、郭芳甄，
中，
趙景明、陳威廷，
（光
趙景明、沈志欽，「資料串流分群探
勘之隱私保 化
企業經營管理理論暨實務研討會論
趙景明、郭芳甄，「
勘相似性頻繁項目集」
， ，第一期，民國九十七年
(7). C. M. Chao, “Privacy-Preserving C
si  s,” in Pro-
ceedings on-
 
第十卷，第二期，民國九十
Streams,” in st 
Management,” ACM 
ol. 32, No. 2, pp. 
 Proceedings of 
4. 會議或期刊發表 
本計劃所發表的論文情形如下： 
(1). 趙景明、吳文群
資訊 七
科技管理與資訊人才培育研討會論
文集，台灣‧台北，2007年6月，第
131-136頁。 
(2). 「資料串流分類探
勘之隱私 護研究」，2007
術與產業應用研討會論文集
市，2007年6月，第393-399頁。 
(3). 「從雜訊資料中探
勘相似性頻繁項目集」，2007全國計
算機會議論文集，台灣‧台 2007
年12月，第366-379頁。 
(4). 「在多重資料串流
環境中之序列樣式探勘」，第十九屆
國際資訊管理學術研討會論文集
碟），台灣‧南投，2008年5月。 
(5). 
護研究」，第九屆電子
文集（光碟），台灣‧彰化，2008年5
月。 
(6). 從雜訊資料中探
，資訊管理展
望 第十卷
六月，第89-110頁。 
fication of Data Stream
 of the 20th International C
las- ACM SIGMOD international confer-
ence on Management of Data, pp. 
ference on Software Engineering and 
Knowledge Engineering, San Francisco,
California, U.S.A., July 2008, pp. 
603-606. 
(8). 趙景明、吳文群，「資料串流頻繁項
目集探勘之隱私保護研究」，資訊管
理展望，
年十二月，第45-66頁。 
(9). C. M. Chao and C. C. Shen, “Pri-
vacy-Preserving Clustering of Data 
 Proceedings of the 21
International Conference on Software 
Engineering and Knowledge Engi-
neering, Boston, Massachusetts, 
U.S.A., July 2009, pp. 530-535. 
(10). C. M. Chao, P. Z. Chen, and C. H. Sun, 
“Privacy-Preserving Classification of 
Data Streams,” Tamkang Journal of 
Science and Engineering, Vol. 12, No. 
3, September 2009, pp. 321-330. 
(11). C. M. Chao, P. Z. Chen, and C. H. Sun, 
“Privacy-Preserving Clustering of Data 
Streams,” Tamkang Journal of Science 
and Engineering. (Submitted) 
五、 參考文獻 
1. Golab, L. and Ozsu, M., “Issues in 
Data Stream 
SIGMOD Record, V
5-14 (2003). 
2. Garofalakis, M., Gehrke, J. and Rastogi, 
R., “Querying and mining data stream: 
you only get one look,”
 36
s of 
ntally parti-
f the 
artitioned Data,” Pro-
ng Distributed D
Mining,” ACM SIGKDD Explorations 
ems, Vol. 9, No. 
 
 International 
，2004 年。 
tabases,  
the 28th IEEE International Conference 
on Computer Software and Applica-
tions, pp. 424-429 (2004). 
17. Kantarcioglu, M. and Clifton, C., “Pri-
vacy-preserving distributed mining of 
association rules on horizo
tioned data,” IEEE Transactions on 
Knowledge and Data Engineering, Vol. 
16, No. 9, pp. 1026–1037 (2004). 
18. Vaidya, J. and Clifton, C. “Privacy 
Preserving Association in Vertically 
Partitioned Data,” Proceedings o
8th ACM SIGKDD 118 International 
Conference on Knowledge Discovery 
and Data Mining, pp. 639-644 (2002). 
19. Vaidya, J. and Clifton, C. “Pri-
vacy-Preserving K-Means Clustering 
over Vertically Partitioned Data,” Pro-
ceedings of the 9th ACM SIGKDD In-
ternational Conference on Knowledge 
Discovery and Data Mining, pp. 
206-215 (2003). 
20. Vaidya, J. and Clifton, C., “Pri-
vacy-Preserving K-Means Clustering 
over Vertically P
ceedings of the 9th ACM SIGKDD In-
ternational Conference on Knowledge 
Discovery and Data Mining, pp. 
206-215 (2003). 
21. Clifton, C., Kantarcioglu, M., Vaidya, 
J., Lin, X. and Zhu, M. Y.., “Tools for 
Privacy Preservi ata 
ACM SIGMOD International Confer-
ence on Management of Data, pp. 
Newsletter, Vol. 4, No. 2, pp. 28-34 
(2002). 
22. Traub, J.F., Yemini, Y. and Woznia-
kowski, H., “The Statistical Security of 
a Statistical Database,” ACM Transac-
tions on Database Syst
4, pp. 672-679 (1984). 
23. Adam, N.R. and Wortmann, J.C., “Se-
curity-Control Methods for Statistical 
Databases: A Comparative Study,” 
ACM Computing Surveys, Vol. 21, No. 
4, pp. 515-556 (1989). 
24. Aggarwal, C.C. and Yu, P.S., “A Con-
densation Approach to Privacy Pre-
serving Data Mining,” Proceedings of 
the 9th International Conference on
Extending Database, pp. 183-199 
(2004). 
25. Domingo-Ferrer, J. and Torra, V., “Or-
dinal, Continuous and Heterogeneous 
k-Anonymity through Microaggrega-
tion,” Proceedings of
Conference on Data Mining and 
Knowledge Discovery, pp. 195-212 
(2005). 
26. 薛夙珍、梁嘉鴻、林明言， "一個以
匯總資料防護隱私之關聯規則探勘
方法"，第九屆人工智慧與應用研討
會，台北
27. Zhang, T., Ramakrishnan, R. and 
Livny, M., “BIRCH: an Efficient Data 
Clustering Method for Very Large Da-
” Proceedings of the 1996
 38
出席國際學術會議心得報告 
 
教 師 姓 名 趙 景 明 (Ching-Ming Chao) 服 務 單 位 東吳大學資訊管理學系 
會議時間地點 98 年 7 月 1 日 ~ 7 月 3 日 
美國、波士頓 
獲補助年度 97 年度 
會 議 名 稱 
The Twenty-First International Conference on Software Engineering and Knowledge 
Engineering (SEKE 2009) 
發表論文題目 
 Privacy-Preserving Clustering of Data Streams 
與會心得(含參加會議經過、收穫及攜回資料名稱與內容)： 
 
一、參加會議經過 
本會議係由美國 Knowledge Systems Institute 主辦之第二十一屆國際軟體工程暨知識工程學術
研討會，會議為期三天，總共有來自世界各地 36 個國家的一百五十多位專家學者參與，合計
發表了 130 篇論文。在為期三天的論文發表中，130 篇的論文分別於 15 個 session 發表，而每
一個 session 有三個會場同時進行，每一篇論文之發表與討論時間約為 25 分鐘。本人於 7 月 3
日上午 Session 10 (8:10-9:00 a.m.)中發表了一篇論文，並且全程參與三天的會議，每一個 session
均選擇一個會場聆聽論文發表並且參與討論。 
 
二、收穫 
本會議之主軸為軟體工程與知識工程，其目的在研討軟體工程與知識工程之研究成果，並探
討這兩個領域之整合。本人藉由參與本次會議學習了軟體工程與知識工程的最新觀念與知
識，對於我往後在教學及研究工作上將有莫大的助益。 
 
三、攜回資料名稱及內容 
1. Proceedings of the Twenty-First International Conference on Software Engineering and Knowl-
edge Engineering (ISBN: 1-891706-24-1) 
2.Final program of the Twenty-First International Conference on Software Engineering and 
Knowledge Engineering 
 
randomly select an already perturbed attribute Aj 
and the remaining last attribute Ak to perform per-
turbation, and reduce T value by 1. 
Input： , mnD   
Output：   'mnD
1. T n; 
2. While (T > 0) do 
{  2.1 If (T > 1) 
{ Randomly select Aj and Ak from ; mnD
),(' kj AAV  )(R  ; ),( kj AAV
TT-2;} 
2.2 Else  // T = 1 
{ Randomly select an already distorted attrib-
ute Aj with the last attribute Ak from ; mnD
),(' kj AAV  )(R  ; ),( kj AAV
TT-1;}} 
Figure 1. Rotation-based perturbation process. 
2.2 Cluster Mining 
2.2.1 Micro-Cluster Generation 
A micro cluster is an extension of cluster feature vector, 
with a main purpose of recording statistical information of 
data points after rotation perturbation. Assume one mi-
cro-cluster represents n number of multidimensional 
data 1X ... nX , each multidimensional data is represented 
by iX = ( ... ), and each multidimensional data has 
its proprietary timestamp T1...Tn. Each micro-cluster has 
(2d + 3) numbers of data items, with d as the attribute 
number and expressed as {
1
ix
d
ix
SS ,TS , SST, ST, n}. Among 
which, SS ={SS1,SS2,…,SSp,…,SSd}, SSp = , 
, which is the total square sum for data value of 
p-th attribute;
2
1
)( ni pix
dp 1
TS
p
= {TS1,TS2,…,TSp,…,TSd}, TSp 
= , , which is the sum of data values 
of p-th attribute; SST = is the sum of the 
squares of timestamp T1...Tn; ST = is the sum of 
timestamp T1...Tn; while n is the number of data points. 
 ni pix1 d1
ni iT1 2)(
n
i iT1
Figure 2 shows the micro-cluster generation process with 
parameters defined below: 
 P = { ix | 1 ≤ i ≤ n } represents the data after pertur-
bation. 
 Q  represents the micro-cluster number set by user. 
 M represents the input micro-cluster. 
 q represents the current number of cluster centers, 0 ≤ 
q ≤ Q . 
 ix and jx (1 ≤ i ≤ n , 1 ≤ j ≤ n, ji  ) represent two 
randomly selected data points, n is the data point 
number. 
 ),(2 ji xx  represents the squared Euclidean dis-
tance between ix and x . 
d
d
j
 D = { ),(2 ji xx | 1 ≤ i ≤ n, 1 ≤ j ≤ n, ji  } repre-
sents the set of the squared Euclidean distance be-
tween any two arbitrary data points, with an initial 
value of ψ. 
 s  represents a cluster center, 1 ≤ k ≤ q. k
 S = { ks | 1 ≤ k ≤ q} represents the set of current clus-
ter center, with an initial value ofψ. 
2 ),( ki sx  represents the squared Euclidean dis-
tance between ix  and s . 
d
d
k
 Di = { ),(2 ki sx | 1 ≤ i ≤ n, 1 ≤ k ≤ q} represents the 
set of the squared Euclidean distance between data 
point ix  and each of the cluster center, with an ini-
tial value ofψ. 
Input：P,  Q
Output： M  
1. For each xi,  do D D {∪ }; jx  ),(2 ji xxd
2. S S {x, y} s.t.∪ = Max[D]; q),(2 yxd  q+2;
3. While ( ＜ Q) do q
{3.1 For each ix   do S
k     { 3.1.1 For each   do s S
DiDi {∪ d }; ), s(2 x ki
3.1.2 DminDmin∪{Min[Di]};} 
3.2 SS {∪ x } s.t. Min[Di] = Max[Dmin]; i
q q+1;} 
4. Use K-means to generate M ; 
Figure2. Micro-cluster generation process. 
Detailed steps are described below: 
Step 1: Calculate the squared Euclidean distance 
 between each data point xi and xj. ),(2 ji xxd
Step 2: Find the two data points that have the longest dis-
tance, store the distance to the cluster center set S and 
add the cluster center number q by 2. 
Step 3: Determine if the current cluster center number q 
equals to the micro-cluster number Q set by the user, 
if affirmative then execute Step 4, if not execute Step 
3.1 and Step 3.2. 
Step3.1: Execute Step 3.1.1 and Step 3.1.2 on each data 
point xi outside of the cluster center. 
Step 3.1.1: Calculate the squared Euclidean distance 
 between the data point xi and the cluster 
center sk for each cluster center sk. 
),(2 ki sxd
Step 3.1.2: Find the minimal value of the squared Euclidean 
distance between each data point  and each clus-
ter center. 
ix
Step 3.2: Find the maximal value of Dmin , set the data point 
xi as the new cluster center. Then add the cluster cen-
ter number q by 1, return to Step 3. 
information since the start of data streams, therefore the 
subtractive characteristic of feature vector is used according 
to the micro-cluster id to find out the time scope of mi-
cro-clusters set by user. Assume the current time is tc, users 
would like to mine on the data during the period h from 
current to period of past experiences in order to obtain K 
clustering result. Under the condition given, we will need to 
find the snapshots stored before time tc-h. We take S(tc-h’) 
to represent the micro-cluster set for time tc-h’, take S(tc) to 
represent the micro-cluster set for time tc , whereas h’ refers 
to the tolerance for error for time tc-h previously set by user. 
For each micro-cluster in S(tc), find out the micro-cluster 
that conform to S(tc-h’) according to its individual id, and 
reduce the cluster feature vector what conforms to the mi-
cro-cluster of S(tc-h’). This approach will ensure the mi-
cro-cluster generated during the period h set by user will 
not influence the mining result. Then, use the micro-cluster 
center as the macro-point in conformity with the period h 
for user observation, then take the data point quantity con-
tained in the macro-point as weight to select K number of 
the data points as the cluster center for macro-clustering, 
using macro-cluster generation process shown in Figure 3 
to cluster for generation of K number of macro-clusters, 
with parameters definitions described below: 
 W = { im | 1 ≤ i ≤ Q} represents the macro-point set 
for the observation period h for user.  
 C = { js | 1 ≤ j ≤ K } represents the set of current 
cluster centers, with the initial value selected from the 
K number with the largest value according to the data 
number n of each macro-point im . 
 K represents the macro- cluster number set by the 
user. 
 Gj represents a macro-cluster, 1 ≤ j ≤ K. 
 mi represents a macro-point, 1 ≤ i ≤ Q. 
 ni represents the data number in macro-point mi, 1 ≤ i 
≤ Q. 
 sj represents a cluster center, 1 ≤ j ≤ K. 
2 ),( ji sm  represents the squared Euclidean dis-
tance between im  and js . 
d
({ 2 mdD  }Kjjii   represents the set 
of the squared Euclidean distance between 
macro-point m  and each cluster center sj . 
1  |), s 
i
 Pointdis[i] is used for storing the Squared Euclidean 
Distance between macro-point im  and the nearest 
cluster center, 1 ≤ i ≤ l. 
 CenterM[i] is used for storing the current cluster cen-
ter for macro-point m , 1 ≤ i ≤ l. i
 imd  (2 , CenterM[i] ) represents the squared 
Euclidean distance between the macro-point im  and 
the current cluster center CenterM[i]. 
Detailed steps are described below: 
Step 1: Calculate the squared Euclidean distance 
 between each macro-point  and 
each cluster center sj. 
),(2 ji smd im
Step 2: Find the minimal value of the squared Euclidean 
distance between each macro-point i  with each 
cluster center sj, store the value to Pointdis[i] while 
store the current cluster center sj to CenterM[i]. 
m
Input：W, C, K 
Output：C, Gj  
1. For each mi, sj do DiDi {∪ }; ),(2 ji smd
2. For each mi do Pointdis[i]Min[Di]; 
   CenterM[i] sj s.t. = Min[Di]; ),(2 ji smd
3. For each sj do 
   sj   

i
ii
n
mn  s.t. CenterM[i] = sj; 
4. For each mi, sj do DiDi {∪ }; ),(2 ji smd
5. For each mi do 
CenterM[i] sj s.t. sj = Min[Di]; ),(2 ji smd
6. While  i [  ＞  Point-
dis[i] ] do 
 , iCenterM )  (2 mid
{6.1 For each mi s.t. , CenterM[i] ) ＞ 
Pointdis[i] do 
imd  (
2
{6.1.1 For each mi, sj do Di Di {∪ };  ),(2 ji smd
6.1.2 For each mi do Pointdis[i] Min[Di]; 
CenterM[i] sj s.t. = Min[Di]; ),(2 ji smd
6.1.3 For each sj do 
sj   

i
ii
n
mn  s.t. CenterM[i] = sj;
6.1.4 For each mi, sj do Di Di {∪ };  ),(2 ji smd
 6.1.5 For each mi do 
CenterM[i] sj s.t. sj = Min[Di];}} ),(2 ji smd
7. For each mi do 
  GjGj {∪  mi } s.t. CenterM[i] = sj; 
Figure 3. Marco-cluster generation process. 
Step 3: For each current cluster center sj, calculate the 
weighted mean inside the cluster and store the re-
sult to sj. 
Step 4: For each macro-point ，recalculate the squared 
Euclidean distance ji between the 
macro-point and each cluster center sj. 
im 2d ),( sm
Step 5: For each macro-point , store the current cluster 
center sj to CenterM[i]. 
im
Step 6: Determine if the distance d2(mi, CenterM[i]) be-
tween any arbitrary macro-point i  and the cur-
rent cluster center is greater than the distance stores 
for mi stored in Pointdis[i], if affirmative then exe-
cute Step 6.1, or else execute Step 7. 
m
Step 6.1: For the squared Euclidean distance d2(mi, Cen-
terM[i] )  between the current clusters is greater 
than the distance Pointdis[i] stored at each 
macro-point , execute Step 6.1.1 to Step 6.1.5. i
Step 6.1.1: For each macro-point i , recalculate the 
squared Euclidean distance  
between the macro-point  and each cluster 
m
m
im
),(2 ji smd
