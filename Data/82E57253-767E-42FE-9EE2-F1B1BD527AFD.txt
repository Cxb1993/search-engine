1 
摘要 
k鄰居搜尋方法是用來在一群資料點中找尋一給定查詢點 k個最近鄰居的方法，k鄰居
搜尋方法非常耗時，為了加快 k 鄰居的搜尋速度，許多快速 k 鄰居搜尋方法已被提出。現
有快速 k鄰居搜尋方法的效能很容易受到資料集的維度、資料量、以及資料分佈情形影響，
在極端的情況下，快速 k 鄰居搜尋方法的效能可能比完全搜尋方法還差。為了解決這個問
題，本計劃使用多種不同的真實資料集，測試 5 種快速 k 鄰居搜尋方法。實驗結果顯示，
我們所設計的方法在不同維度、不同資料量、以及不同資料分佈情形的資料集下，大部份
情況下，表現均比現有快速 k 鄰居搜尋方法更好。本計劃的結果非常適合用來為未知資料
集挑選快速 k鄰居搜尋方法。 
 
關鍵字：快速 k鄰居搜尋方法，正交搜尋樹，效能評估. 
 
Abstract 
The problem of k-nearest neighbors (kNN) search is to find nearest k neighbors from a given 
data set for a query point. The finding process of kNN is very time comsuming. To speed up the 
finding process of nearest k neighbors, many fast kNN search algorithms were proposed. The 
performance of fast kNN search algorithms is highly influenced by the number of dimensions, 
number of data points, and data distribution of a data set. In the extreme case, the performance of 
a fast kNN search algorithm may be poorer than the full search algorithm. To overcome this 
problem, five fast algorithms were tested using multiple real data sets. Experimental results show 
that the proposed methods have better performance than others under various numbers of 
dimensions, various numbers of data points, and different types of data sets in most cases. The 
result of the project will be very useful in choosing a fast kNN search algorithm for an unknown 
data set. 
 
Keywords: Fast kNN Search Algorithm, Orthogonal Search Tree, Performance Evaluation. 
3 
作法是使用分群方法[23-25]將資料點分成幾個不同的群組，針對每個群組的資料，再
細分為多個群組，同樣的過程重覆進行直到每個群組內的資料點距離小於容許值或是
只剩下一個資料點；針對每個群組，須記錄群組中心、群組半徑、以及群組內每個資
料點到群組中心的距離；搜尋資料時，依群組中心與查詢點距離由近至遠依序尋找，
然後再以目前找到之第 k個最近距離 Dk與群組的半徑做為檢查依據，過濾群組。一般
而言，Ball Tree方法對高維度資料有良好的表現[11]，但需要計算查詢點與群組中心的
距離以及將群組依與查詢點的距離排序，因此容易受到所使用之分群方法、群組數量、
以及資料點數量的影響。Friedman [12]等人使用空間分解方法將資料點分類，這個方
法產生一個平衡的 k維度二元樹，又稱為 k-d tree；k-d tree的建構方式是先找出各個資
料維度中使得資料集分離度最大的維度，然後將資料集依該維度將資料均勻的分成兩
個群組，同樣的過程可以對每個群組再次進行分群動作，直到群組中只剩一個資料點
或群組中資料點的距離小於容許值為止；查詢資料時，則依 k-d tree的分類方式找到查
詢點所屬的樹葉群組，由該群組找起，然後再找兄弟群組，搜尋一個群組前，先檢查
該群組是否可被過濾掉。由於 k-d tree 只需計算查詢點到群組邊界的距離，因此額外負
擔較小，當資料集的維度不大時 k-d tree通常比 Ball Tree有更好的表現，資料點數量
對 k-d tree的影響也會比較小，相反的，因分群方法較簡略，當維度大時，效果較差。
其它方法還有 Kim與 Park [13]等人使用有序分割方法建立一個排序好的多分枝搜尋
樹，方便依與查詢點距離的遠近來搜尋群組；Mico [14]等人則事先建立一個距離表及
二元樹，然後使用不同的群組過濾方法來過濾不可能的群組；McNames [15]使用主軸
分析方法找出資料集的主軸，然後使用資料點映射到主軸的映射值將資料點分成 nc個
子群組，每個子群組也使用相同的方式再分成 nc個子群組，直到每個群組的數量小於
nc為止。為了方便依與查詢點的距離遠近搜尋群組，所產生的子群組必須依映射值排
序，方便依與查詢點的距離遠近搜尋群組，所建立出來的樹稱為主軸搜尋樹(PAT)；
Chen[16]等人則使用金字塔結構建構低限制樹(LBT)，金字塔最下層是個別資料點，往
上一層則儲存下層所有資料點所成群組的中心點(維度大小減半)以及群組半徑，最上層
則是所有資料點的平均值且維度為 1；搜尋資料時則使用贏者更新(winner update)搜尋
方法來加速 kNN的搜尋速度。 
 非樹狀結構之快速 kNN搜尋方法 
這類搜尋方法通常使用排序方式以及資料集的部份特徵來過濾資料點。這類方法中，
Cheng[17]等人提出一個 min-max方法，用來降低運算數量，另外，他們也提出一個部
份搜尋方法(PDS)，該方法在距離計算過程中，一旦發現此距離不可能是 kNN 時，可
提早結束距離計算。Bei and Gray [18]稍後也提出相同於 PDS的詳細演算法以及實驗
結果，證明 PDS 確實可有效降低計算量，PDS 方法通常也會被使用在其它快速 kNN
搜尋方法中。Ra and Kim [19]將資料點事先依資料點的平均值作排序，在 kNN搜尋過
程中，使用二元搜尋法找到資料集中平均值與查詢點平均值最相似的 k個資料點，然
後依平均值差異由小而大，逐次搜尋資料點，搜尋過程中，則利用資料點的平均值與
查詢點的平均值差異程度以及目前第 k個最近鄰居的距離來決定是否提早結束搜尋過
程。Tai [20]等人應用資料點的多個映射值（projection values）來過濾不可能的資料點。
Lu [21]等人應用資料點各維度的平均值、變異數、以及 norm來過濾不可能的資料點；
Lai [22] 等人則應用資料點的多個映射值與三角不等式來過濾不可能的資料點。 
5 
為了評估現有 kNN方法在不同資料集下的效能表現，本計劃使用兩組資料集，這兩組
資料集分別說明如下： 
 真實影像資料集 
為了測試資料維度與資料量對 kNN搜尋方法的影響，本資料集使用 8 張 USC-SIPI影
像資料庫[30]所提供的影像作為實驗資料，每張影像大小為 512 × 512 像素。本實驗所
使用的影像如表一所示：  
表一：真實影像 
Lena Milk Peppers F16 
Tiffany Baboon Bridge Lake 
本實驗共產生 9個資料集，其中包括 5個具有 16個維度的資料集，其資料量分別
是 1024, 2048, 4096, 8192, 與 16384，以及 4個具有 1024個資料點的資料集，維度分
別是 4, 64, 256, 與 1024。每個資料集都是一個碼書(codebook)，碼書是使用 8 張影像
中所有不重疊的影像區塊透過 LBG演算法[23]產生而成。以一個 16個維度且資料量為
1024的資料集為例，要產生這個資料集，8 張影像中所有不重疊的 4 × 4影像區塊
(131072個影像區塊)均被用來做為資料點，所產生出來的碼書內含 1024個碼字。為了
測試搜尋速度，本實驗產生 10000個查詢點，每個查詢點悉由 8 張影像中的影像區塊
(131072個影像區塊)隨機選取而來。 
 真實資料集 
為了測試不同維度、不同資料量、以及不同種類之真實資料集對 kNN搜尋方法的影響，
我們由美國加州大學 Irvine分校所建立的機器學習資料庫[31]中過濾挑選出沒有資料
完整、純數值、且含有群組資訊的真實資料集，所挑選的資料集及其統計資訊如表二
所列，其中資料分散程度的計算方式是取各群組"標準差/(最大值－最小值)"的平均
值。針對每個資料集，分別產生 10000個查詢點作為測試資料，每個查詢點則是隨機
由資料集中挑選產生。 
7 
表五與表六分別列出 5個快速 kNN搜尋方法針對不同資料量之資料集與不同維度
之資料集搜尋 10000個查詢點 3個最近鄰居的搜尋時間(單位：ms)。由表五的結果，
我們可以發現，當資料量不大時，LAI方法效果最好，當資料量大時，OST方法可得
到最佳的效果。由表六的結果，我們可以發現，當維度小時，LAI方法效果最好，當
維度大時，LBT方法可得到最佳的效果。 
表五：針對 16個維度不同資料量之資料集的搜尋時間 
資料量 FSA PAT LAI LBT MPAT OST 
1024 590 71 45 110 71 55 
2048 1,175 98 73 172 98 78 
4096 2,338 149 119 281 147 120 
8192 4,738 288 196 476 290 182 
16384 9,311 372 331 811 367 252 
表六：針對 1024個資料點不同維度之資料集的搜尋時間 
維度 FSA PAT LAI LBT MPAT OST 
4 174 30 20 47 30 23 
16 590 71 45 110 71 55 
64 2,430 204 136 265 200 158 
256 9,943 846 568 890 836 655 
1024 41,355 3,987 2,757 2,442 3,931 3,076 
 
由表五與表六發現，當資料集來自真實影像時，我們可以依據資料集的資料量與
維度挑選適用的快速 kNN搜尋方法。挑選建議如表七所示： 
表七：針對真實影像資料集的快速 kNN搜尋方法挑選建議 
資料量 
維度 
小於 8192 大於 8192 
小於 1024 選擇 LAI方法 選擇 OST方法 
大於 1024 選擇 LBT方法 選擇 OST方法 
 真實資料集 
表八與表九分別列出快速 kNN搜尋方法針對不同真實資料集的前處理時間與搜尋
10000個查詢點 3個最近鄰居的資料搜尋時間(單位：ms)。 
表八：針對不同真實資料集各種快速 kNN搜尋方法的前處理時間 
資料集 PAT LAI LBT MPAT OST 
Iris 0 0 0 0 0 
Abalone 16 16 1,906 31 47 
Magic 343 234 1,013,609 343 546 
Letter 421 265 159,140 468 1,154 
Ionosphere 0 0 15 0 63 
Statlog 94 47 3,468 124 1,482 
Waveform 78 16 2,109 93 1,950 
Optical 140 31 2,110 156 3,682 
  
 
 
9 
參考文獻 
[1]  S. Theodoridis and K. Koutroumbas, Pattern Recognition, Academic Press, 2nd edition, 
2003. 
[2] H. Murase and S. K. Nayar, “Visual learning and recognition of 3D objects from 
appearance,” International Journal of Computer Vision, vol. 14, no. 1, pp. 5-24, January 
1995. 
[3] V. Roth, J. Laub, M. Kawanabe, and J. M. Buhmann, “Optimal cluster preserving 
embedding of nonmetric proximity data,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 25, no. 12, pp. 1540-1551, December 2003. 
[4] Y. C. Liaw, “Improvement of the fast exact pairwise-nearest-neighbor algorithm,” Pattern 
Recognition, vol. 42, no. 5, pp. 867-870, May 2009. 
[5] C. Y. Chen, C. C. Chang, and R. C. T. Lee, “A near pattern-matching scheme based on 
principal component analysis,” Pattern Recognition Letters, vol. 16, pp. 339-345, April 
1995. 
[6] Y. C. Liaw, J. Z. C. Lai, and Winston Lo, “Image restoration of compressed image using 
classified vector quantization,” Pattern Recognition, vol. 35, no. 2, pp. 329-340, February 
2002. 
[7] A. Gersho and R. M. Gray, Vector Quantization and Signal Compression, Kluwer Academic 
Publishers, Boston MA., 1991. 
[8] T. M. Cover, P. E. Hart, “Nearest neighbor pattern classification,” IEEE Transactions on 
Information Theory, vol. 13, no. 1, pp. 21-27, January 1967. 
[9] Nicolás García-Pedrajas and Domingo Ortiz-Boyer, "Boosting k-nearest neighbor classifier 
by means of input space projection," Expert Systems with Applications, vol. 36, issue 7, pp. 
10570-10582, March, 2009. 
[10] Keinosuke Fukunaga and Patrenahalli M. Narendra, "A branch and bound algorithm for 
computing k-nearest neighbors," vol. C-24, issue 7, IEEE Transactions on Computers, pp. 
750-753, July 1975. 
[11] Ting Liu, Andrew W. Moore, and Alexander Gray, "New algorithms for efficient 
high-dimensional nonparametric classification," Journal of Machine Learning Research, vol. 
7, pp. 1135-1158, 2006. 
[12] J. H. Friedman, J. L. Bentley, and R. A. Finkel, "An algorithm for finding best matches in 
logarithmic expected time," ACM Transaction on Mathematical Software, vol. 3, no. 3, 
pp.209−226, 1977. 
[13]  B. S. kim and S. B. Park, “A fast k nearest neighboring finding algorithm based on the 
ordered partition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 8, 
no. 6, pp. 761-766, November 1986. 
[14] L. Mico, J. Oncina, and R. C. Carrasco, “A fast branch and bound nearest neighbor 
classifier in metric spaces,” Pattern Recognition Letters, vol. 17, no. 7, pp. 731-739, June 
1996. 
[15] J. McNames, “A fast nearest-neighbor algorithm based on a principal axis search tree,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 9, pp. 
964-976, September 2001. 
[16] Yong-Sheng Chen, Yi-Ping Hung, Ting-Fang Ten, Chiou-Shann Fuh, “Fast and versatile 
algorithm for nearest neighbor search based on a lower bound tree,” Pattern Recognition, 
vol. 40, no. 2, pp. 360-375, February 2007. 
[17] D.-Y. Cheng, A. Gersho, B. Ramamurthi, and Y. Shoham, “Fast Search Algorithms for 
Vector Quantization and Pattern Matching,” IEEE International Conference on Acoustics, 
Speech, and Signal Processing, vol. 1, pp. 9.11.1-9.11.4, March 1984. 
[18] C. D. Bei and R. M. Gray, “An improvement of the minimum distortion encoding algorithm 
for vector quantization,” IEEE Transactions on Communications, vol. 33, no. 10, pp. 
1132-1133, October 1985. 
[19] S. W. Ra and J. K. Kim, “Fast mean-distance-ordered partial codebook search algorithm for 
image vector quantization,” IEEE Trans. on Circuits and Systems II: Analog and Digital 
Signal Processing, vol. 40, no. 9, pp. 576–579, September 1993. 
11 
計畫成果自評 
 原計畫相符程度與達成預期目標情況 
本計劃預計完成一快速 kNN搜尋方法的效能檢測資料庫並設計受資料量數目，資料維
度，以及資料分佈情況影響較小的快速 kNN搜尋方法。 
計劃執行完畢，已完成開發兩個受資料量數目、資料維度、以及資料分佈情況影響較
小的快速 kNN搜尋方法以及整理出兩組可作為效能檢測的真實資料集，實驗結果顯示，本
計劃所提方法在不同維度、不同資料量、以及不同資料分佈情形的資料集下，大部份情況
下，表現均比現有快速 k鄰居搜尋方法更好。本計劃的結果也非常適合用來為未知資料集
挑選快速 k鄰居搜尋方法。 
 研究成果之學術或應用價值 
學術價值：本計劃已開發兩個快速 kNN搜尋方法，成果發表在 SCI 國際期刊 Digital 
Signal Processing[28]與 Pattern Recognition [29]，實驗分析結果發表在國際
研討會[32]中。 
應用價值：本計劃所開發出來的快速 kNN搜尋方法是目前已知受資料量、維度、與資
料種類影響最小的方法，此方法可應用在樣型識別[1]、物件識別[2]、資料
分群[3, 4]、函式近似[5]、向量量化[6, 7]、與樣型分類[8, 9]等各種領域。 
 程式開發人才培育 
本計劃執行人員，包括多位大學部學生，協助閱讀整理文獻、搜集實驗資料、轉換
實驗資料格式、分析實驗資料特性、以及實作快速搜尋方法等，參與本計劃，對學生英
文文件閱讀能力、報告能力、程式實作能力的提昇、做事態度的養成、與實驗方法的熟
悉均有很大的幫助。 
 
 出席國際學術會議心得報告 
                                                             
計畫編號 NSC 99-2221-E-343-006 
計畫名稱 快速 k 個最近鄰居搜尋之效能檢測資料庫與演算法開發 
出國人員姓名 
服務機關及職稱 
廖怡欽 
南華大學副教授 
會議時間地點 民國 100 年 7 月 18 日至民國 100 年 7 月 21 日 
會議名稱 
The 2011 International Conference on Image Processing, Computer Vision, and 
Pattern Recognition 
發表論文題目 Evaluation of Fast K-Nearest Neighbors Search Methods using Real Data Sets 
 
一、參加會議經過 
此次出國主要目的是到美國拉斯維加斯參加 IPCV2011 會議發表論文(附件一)，該會
議屬於 WORLDCOMP 2011 的一個分會，WORLDCOMP 2011 共包括 12 個分會，是一個非常大
的會議，共有 88 個國家超過一千人參加。會議時間 7 月 18 日至 7 月 21 日共四天，開會
場地是 Monte Carlo 飯店。 
此行搭乘華航 CI0006 班機於 7/17 下午 4 點 40 分起飛到美國洛杉機轉機，然後再搭
乘美國航空到拉斯維加斯，到達拉斯維加斯 Excalibor 飯店時已是美國時間下午 6:00。
我住的飯店離會議地點不遠，走路大約 20 分鐘，飯店 Check In 後，先到會議地點看看，
順便參觀拉斯維加斯。 
7 月 18 日早上 8:30 在 Monte Carlo 飯店的戲院開幕，接下來是一系列的演講及論
文發表，7 月 18 日晚上 9:10 大會晚宴，晚宴期間認識了一些與會學者，有從台灣過去
目前在美國北德州大學(UNT)Dallas 分校教書的楊逢仁教授，阿肯色中央大學(UCA)教書
的陳啟天教授，有新加坡國立大學的黃建民教授，馬來西亞多媒體大學的 Sim Kok Swee
教授，馬來西亞 Universiti Kebangsaan 的 Nasharuddin Zainal 教授，...等。 
7/19~7/20 挑選了幾篇有興趣的論文聽聽順便跟一些與會學者談談，發現美國許多
學校也有招生壓力，而且也必須到高中宣導，也發現美國電腦相關科系招生不好，多數
美國人不念電腦相關科系，東南亞狀況則較穩定。7/20 搭乘下午 6:35的飛機到洛杉機，
然後在洛杉機停留幾天，順道到本校的姊妹校西來大學與西來寺，洽談交換學生事宜，
7/26凌晨 1:35分搭乘華航 CI0007回國，7/27 早上 6:00回到台灣。 
 
                                        
 
                                
 
 
April 14, 2011 
 
 
 
 
 
 
 
 
 
 
Dear Prof. Yi-Ching Liaw, 
 
This notification letter is to inform you that the paper entitled “EVALUATION OF FAST K-NEAREST 
NEIGHBORS SEARCH METHODS USING REAL DATA SETS” which was submitted to The 2011 
International Conference on Image Processing, Computer Vision, & Pattern Recognition (IPCV'11) has 
been accepted for publication. 
 
You have been invited by the members of the Organizing Committee of The 2011 World Congress in 
Computer Science, Computer Engineering, and Applied Computing (WORLDCOMP’11) to attend the 
annual summer conference series to be held in Las Vegas, Nevada, USA (July 18-21, 2011). Universal 
Conference Management Systems & Support (UCMSS), who is managing the operation of these 
conferences, wishes to congratulate you on behalf of the Conference Committee and extend this invitation 
to you for presentation of the above paper at this event. Enclosed, please find a letter addressed to the 
United States Consulate General for US VISA purposes. 
 
We hope that you will take advantage of this exceptional opportunity to join us. We are confident that you 
will enjoy the scientific program that is being offered at this year’s event.  
 
Congratulations once again, and thank you for your contribution to the conference.  We look forward to 
welcoming you at the conference in Las Vegas. 
 
 
 
 
 
 
 
 
 
 
 
 
Prof. Yi-Ching Liaw 
No.55, Sec.1, Nanhua Rd., Zhongkeng 
Dalin Township, 
Chiayi County 62248, 
Taiwan 
 
 
Sincerely, 
 
Kaveh D. Arbtan 
UCMSS 
San Diego, California 
U.S.A. 
 
Administered by: UCMSS 
13223-1 Black Mountain Road, 
Suite 135      
San Diego, CA 92129-2658 
 
Official Contact: Prof. H. R. Arabnia  
Phone: (706) 542-3480                                                
Fax:     (706) 542-2966 
Email: hra@cs.uga.edu 
www.world-academy-of-science.org 
WORLDCOMP’11                                                
 July 18-21, 2011 
Monte Carlo Resort 
Las Vegas Nevada, USA 
EVALUATION OF FAST K-NEAREST NEIGHBORS 
SEARCH METHODS USING REAL DATA SETS 
 
Yi-Ching Liaw 
Computer Science and Information Engineering, Nanhua University, Chiayi, Taiwan 
 
 
Abstract - The problem of k-nearest neighbors (kNN) search 
is to find nearest k neighbors from a given data set for a query 
point. To speed up the finding process of nearest k neighbors, 
many fast kNN search algorithms were proposed. The 
performance of fast kNN search algorithms is highly 
influenced by the number of dimensions, number of data 
points, and data distribution of a data set. In the extreme case, 
the performance of a fast kNN search algorithm may be 
poorer than the full search algorithm. To help understand the 
performance of fast kNN search algorithms on data sets of 
different types, five fast algorithms were tested in this paper 
using multiple real data sets. The experimental results of the 
paper will be very useful in choosing a fast kNN search 
algorithm for an unknown data set. 
Keywords: Fast kNN search algorithm, performance 
evaluation.  
 
1 Introduction 
  The problem of k-nearest neighbors (kNN) search is to 
find nearest k neighbors for a query point from a given data 
set. This problem occurs in many types of applications [1-3].  
 The intuitive method of finding nearest k neighbors for a 
query point Q from a data set S={X1, X2, ...,  Xn} of n data 
points is to compute n distances between the query point Q 
and all data points in the data set S. This method is called the 
full search algorithm (FSA). Generally, the squared Euclidean 
distance is applied to measure the distance between two points, 
for a query point Q=[q1, q2, ..., qd]T with dimension d and a 
data point Xi=[xi1, xi2, ..., xid]T from data set S, the distance 
between these two points is defined as below : 
∑
=
−=
d
j
jiji qxQXd
1
2)(),(
 (1) 
 The full search algorithm is easy to be implemented, but 
always takes a lot of computation time. To reduce the 
computation time of the kNN finding process, many fast kNN 
search algorithms were proposed [4-8]. Among available fast 
kNN search methods, algorithms proposed by McNames [4], 
Lai et al. [5], Chen et al. [6], and Liaw et al. [7][8] usually 
have better performance than others. In this paper, we refer to 
algorithms presented in [4], [5], [6], [7], and [8] as PAT, LAI, 
LBT, MPAT, and OST, respectively. 
 The performance of fast kNN search algorithms is highly 
influenced by the number of data dimensions, the number of 
data points, and the type of data distribution. For data sets of 
different types, different search algorithms should be chosen 
to achieve the best performance. For the case of a wrong fast 
algorithm is chosen for a data set, the performance of finding 
nearest k neighbors may be poor. In the extreme case, the 
performance of a fast kNN search algorithm may poorer than 
the FSA.  
 To help finding the best kNN search algorithm for an 
unknown data set, this paper evaluates the performance of 
several new and high performance fast kNN search algorithms 
[4-8] using real data sets of various dimensions, various 
numbers of data points, and various types of applications. 
 The rest of this paper is organized as follows. In section 
II, five fast kNN search algorithms [4-8] are briefly reviewed. 
The real data sets selected in this paper are described in 
section III. Experimental results and conclusions are given in 
section IV and section V, respectively. 
2 Fast kNN Search Algorithms 
2.1 The PAT Algorithm 
 The PAT algorithm [4] consists of two phases : the 
principal axis search tree construction phase and the nearest 
neighbors finding phase.  
 The principal axis search tree construction phase builts a 
principal axis search tree for a data set. In the beginning, there 
is only one node (the root node) is created for the tree and all 
data points in the data set are assigned to the root node. Next, 
data points in the root node are partitioned into nc child nodes. 
The partition process of a node is started by evaluating the 
principal axis of data points in the node and followed by 
partitioning data points in the node into nc child nodes 
according to projection values of data points onto the 
principal axis. The partition process is repeatedly applied to 
each child node until the number of data points in a node is 
less than  nc. After the principal axis search tree is built, every 
3 Real Data Sets 
 To evaluate the performance of fast kNN search 
algorithms, two sorts of real data sets were selected and 
described in the following sections. 
3.1 Data Sets From Real Images 
 In this paper, 8 real images with size of 512 × 512 pixels 
from USC-SIPI Image Database [9] (see Table I) were 
selected to generate 9 data sets and 10000 query points.  
Table I : Images from USC-SIPI Image Database 
    
Lena Milk Peppers F16 
    
Tiffany Baboon Bridge Lake 
 Each data set is a codebook generated using the LBG 
[10] algorithm and all non-overlapping 4×4 image blocks 
obtained from 8 images. Here, 9 data sets were generated. 
Five of them are with 16 dimensions and various numbers of 
data points (1024, 2048, 4096, 8192, and 16384) and the 
others are with various dimensions (4, 64, 256, 1024) and 
1024 data points. 
 Each query point is a non-overlapping 4×4 image block 
randomly selected from 8 images. 
3.2 Data Sets From UC Irvine Machine 
Learning Repository 
 UC Irvine machine learning repository [11] provides 
many useful real data sets. In this paper, 10 real data sets with 
numeric data and various numbers of dimensions as listed in 
Table II were selected in the paper. For each data set, 10000 
query points were generated by randomly selecting data points 
from the data set.  
 In convinience, data sets listed in Table II are referred to 
as "Iris", "DL_Sensor", "Abalone", "Magic", "Letter", 
"Ionosphere", "Statlog", "Waveform", "Optical", and 
"SECOM", respectively.  
 
 
Table II : Data Sets from UC Irvine machine learning 
repository 
Name Dimensions Data Points 
Iris 4 150 
Dodgers Loop Sensor  6 50400 
Abalone 8 4177 
MAGIC Gamma Telescope 10 19020 
Letter Recognition 16 20000 
Ionosphere 34 351 
Statlog (Landsat Satellite) 36 6435 
Waveform Database Generator 40 5000 
Optical Recognition  64 5620 
SECOM 590 1567 
4 Experimental Results 
 To evaluate the performance of fast kNN search 
algorithms, data sets and query points obtained in section 3 
were used for testing the performance of six kNN search 
algorithms (FSA, PAT, LAI, LBT, MPAT, and OST).  
 In the experiment, all search trees were constructed with 
nc=20 for the PAT, MPAT, and OST algorithms. For the LBT 
algorithm, source code downloaded from the author's web site 
[12] is used. All programs were implemented using Microsoft 
Visual C++ 2008 Express under Windows Vista Home 
Premium. All programs were executed on a personal 
computer with Intel Core 2 Due P8600 2.4G Hz and memory 
of 4 GB. 
4.1 Experiment Using Data Sets From Real 
Images 
 Tables III and IV list the preprocessing time (in 
milliseconds) for five fast kNN search algorithms. From 
Tables III and IV, we can see that the LBT and OST 
algorithms take the most preprocessing time in different cases. 
When a data set with a large number of data points, the LBT 
algorithm will take a lot of preprocessing time while for a data 
set with a large number of dimensions, the OST algorithm has 
the most preprocessing time.  
Table III : Preprocessing time for five kNN search algorithms 
were applied on data sets with 16 dimensions and various 
numbers of data points. 
Number of 
Data Points PAT LAI LBT MPAT OST 
1024 16 0 39 16 31 
2048 16 0 164 16 78 
4096 39 8 1,014 47 164 
8192 117 47 6,755 133 375 
16384 312 179 59,826 343 905 
the OST algorithm performs well for all data sets and the 
MPAT ranks second.   
 From this experiment, we can conclude that the OST 
algorithm is a good choice for most data sets. However, in the 
case of a data set with a huge number of dimensions  and the 
preprocessing time is important, the MPAT algorithm is a 
better choice than the OST algorithm. From [7], we know that 
the performance of the MPAT algorithm is highly influenced 
by the number of data points in leaf nodes. In the case of the 
MPAT algorithm is applied, a suitable nc must be chosen to 
ensure that the number of data points in leaf nodes is large to 
achieve a good performance. 
5 Conclusions 
 The performance of fast kNN search algorithms is highly 
influenced by the number of dimensions, number of data 
points, and data distribution of a data set. To help understand 
the performance of fast kNN search algorithms on data sets of 
different types, five fast kNN search algorithms were tested in 
this paper using data sets from real images and UC Irvine 
machine learning repository. From the experimental results, 
we can find that the OST algorithm performs well for most 
cases. For a data set from real images, the LAI algorithm is 
also a good choice. If a data set with a huge number of 
dimensions and the preprocessing time is important, the 
MPAT method becomes the best choice. 
6 Acknowledgement 
This work is supported by a grant from the National Science 
Council (NSC-99-2221-E-343-006).  
7 References 
[1] S. Theodoridis and K. Koutroumbas, Pattern 
Recognition, Academic Press, 2nd edition, 2003. 
[2] Yi-Ching Liaw, "Improvement of the fast exact pairwise-
nearest-neighbor algorithm,"      Pattern Recognition, vol. 42, 
no. 5, pp. 867-870, May 2009. 
[3] Chih-Tang Chang, Jim Z. C. Lai, and M. D. Jeng, "Fast 
agglomerative clustering using information of k-nearest 
neighbors," Pattern Recognition, vol.43, no. 12, pp. 3958-
3968, December 2010. 
[4] J. McNames, "A fast nearest-neighbor algorithm based 
on a principal axis search tree," IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 23, no. 9, pp. 964-
976, September 2001. 
[5] J. Z. C. Lai, Y. C. Liaw, and J. Liu, "Fast k-nearest-
neighbor search based on projection and triangular 
Inequality," Pattern Recognition, vol. 40, no.1, pp. 351-359, 
January 2007. 
[6] Yong-Sheng Chen, Yi-Ping Hung, Ting-Fang Ten, 
Chiou-Shann Fuh, "Fast and versatile algorithm for nearest 
neighbor search based on a lower bound tree," Pattern 
Recognition, vol. 40, no. 2, pp. 360-375, February 2007. 
[7] Yi-Ching Liaw, Chien-Min Wu, and Maw-Lin Leou, 
"Fast K-Nearest Neighbors Search Using Modified Principal 
Axis Search Tree," Digital Signal Processing, Vol. 20, Issue 5, 
pp. 1494-1501, September 2010. 
[8] Yi-Ching Liaw, Maw-Lin Leou, and Chien-Min Wu, 
"Fast exact k nearest neighbors search using an orthogonal 
search tree," Pattern Recognition, Vol. 43, No. 6, pp. 2351-
2358, June 2010.   
[9] http://sipi.usc.edu/database. 
[10] Y. Linde, A. Buzo, and R. M. Gray, “An algorithm for 
vector quantizer design,” IEEE Trans. on Communications, 
vol. 28, no. 1, pp. 84-95, January 1980. 
[11] http://archive.ics.uci.edu/ml/index.html. 
[12] http://www.cs.nctu.edu.tw/~yschen/software.html. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：廖怡欽 計畫編號：99-2221-E-343-006- 
計畫名稱：快速 k個最近鄰居搜尋之效能檢測資料庫與演算法開發 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 2 0 50% 
本計劃相關結果
發 表 在 Digital 
Siginal 
Processing and 
Pattern 
Recognition 期
刊. 
 
研究報告/技術報告 0 0 100%  
研討會論文 1 0 50% 
篇 
kNN 快速搜尋方法
的相關應用, 發
表在 2011CVGIP. 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 9 0 100% 
人次 
本系沒有研究生, 
因此找大學部學
生幫忙搜集整理
及分析真實資料
集,以及幫忙寫計
劃相關程式. 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 kNN 快速搜尋方法
效能分析結果發
表在美國舉辦的
IPCV 2011研討會.
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  
國外 
專利 已獲得件數 0 0 100% 件  
