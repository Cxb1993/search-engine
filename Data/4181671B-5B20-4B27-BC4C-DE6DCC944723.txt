 
中 文 摘 要 ： 為提供個人居家健康管理及保健服務，促進個人健康服務的
提升，此本研究計畫以三年期為目標，建置「普及健康服務
格網：以格網為基礎之個人健康服務系統」。透過結合格網
系統之基礎建設及點對點系統之具容錯、自我調適的資源分
享機制，形成標準化之平台。並於此平台上記錄及追蹤個人
健康與診療紀錄，透過資料挖掘技術，及疾病預測模型分析
個人遺傳資訊，開發智慧型個人化身心健康諮詢與管理系
統。並透過感測元件與 RFID 技術建置個人化行動照護系統，
以建立情境感知、人物感知、與環境感知之行動式健康服務
平台，提供醫療資訊、自動服藥提醒、環境安全與個人安全
提醒等功能。本計畫亦利用纖維材料開發可監測心電訊號、
體溫、呼吸及肢體動作之織品感測元件，讓使用者於居家環
境中，舒適的接受全天候生理監測。藉由本計畫所建置之普
及健康服務格網，建立個人居家健康照顧服務，達到疾病預
防、提高健康照顧品質和降低健保成本之目標。 
本計畫於第一年度之進度中已完成：(a) 設計連結 Home 
Server 之 Overlay 以及開發多個服務在格網上藉由 SOA 架構
入口網站存取格網資源，(b) 完成「個人化身心健康管理系
統」的雛型設計，以及相關資料庫的建置規畫，同時也完成
心血管疾病預警系統功能之規畫與 API 參數的設定，(c) 開
發一個行動化的 RFID 平台，建立 RFID 讀取標籤與遠端主機
連接之通訊實驗；規劃與建置以 SOA 為基礎之室內外定位系
統，(d) 完成居家環境互動式人機介面裝置開發，Widget 概
念製作健康服務 GUI 與健康服務功能 API 建置，以及可偵測
肢體動作變化之織品感測元件研製。並於第二年度接續完成
下列研究成果，(a) 架構一個具有負載決策機制與服務組合
的服務平台並強化使用者介面達到結合多個不同計算與儲存
能力之系統平台，(b) 完成「個人化身心健康管理系統」的
實作以及和其它子計畫的初步整合，開發整體系統彙總的資
料倉儲與溝通介面，並且建置心血管疾病預警系統之功能，
(c) 利用第一年所開發完成的整合型主動式 RFID 讀取標籤，
研究如何利用行動通訊網路，設計與其他子計畫的連接介
面；完成 Zigbee 室內定位系統之建置，(d) 完成穿戴式運動
生理感測智慧平台開發，及低功率短距離生理感測無線傳輸
器電路設計製作。於此第三年度中，本計畫進行各項子計畫
之整合，並完成下列成果：(a) 利用點對點技術並且與其他
子計畫進行整合，系統內使用者可以互相交談或是傳輸檔
案，並使用 VPN 以及虛擬機器提供異地備援模式，(b) 彙整
「個人化身心健康管理系統」與 「心血管疾病預警系統」，
並且完成心血管疾病資料的建立以及其他子系統的整合，提
供心血管疾病相關新聞到個人健康助理系統。此外，對於分
Virtual machine technology that can provide remote 
backup. (b) Completing the integration of the PHM 
system and cardiovascular disease early warning 
system along with the systems of other subprojects, 
and built the dataset for the establishment of 
cardiovascular diseases, then provide information 
about Cardiovascular disease-related into personal 
health assistant system.  In addition, we 
synchronized the data on the central database and the 
database of each home server, and performed data 
backup and maintenance processes regularly. (c) 
Developing the mobile Medicare protection system to 
provide the services for elders at home, and 
completing outdoor positioning system, and 
integration of Google Latitude. (d) APP to the 
concept of smart phones, using HTML5 cross-platform 
production and health management system. Manufacture 
a wearable smart cloth for sensing posture and 
evaluate the sense stability. 
英文關鍵詞： Grid, P2P, data mining, RFID, mobile Medicare 
protection system, motion measurement textile sensor, 
interactive multimedia interface, wearable motion 
measurement smart clothing, low voltage and lower 
power consumption wireless conditioning technology. 
 
  I 
中文摘要 
 為提供個人居家健康管理及保健服務，促進個人健康服務的提升，此本研究計畫以三
年期為目標，建置「普及健康服務格網：以格網為基礎之個人健康服務系統」。透過結合格
網系統之基礎建設及點對點系統之具容錯、自我調適的資源分享機制，形成標準化之平台。
並於此平台上記錄及追蹤個人健康與診療紀錄，透過資料挖掘技術，及疾病預測模型分析
個人遺傳資訊，開發智慧型個人化身心健康諮詢與管理系統。並透過感測元件與 RFID 技
術建置個人化行動照護系統，以建立情境感知、人物感知、與環境感知之行動式健康服務
平台，提供醫療資訊、自動服藥提醒、環境安全與個人安全提醒等功能。本計畫亦利用纖
維材料開發可監測心電訊號、體溫、呼吸及肢體動作之織品感測元件，讓使用者於居家環
境中，舒適的接受全天候生理監測。藉由本計畫所建置之普及健康服務格網，建立個人居
家健康照顧服務，達到疾病預防、提高健康照顧品質和降低健保成本之目標。 
本計畫於第一年度之進度中已完成：(a) 設計連結Home Server 之Overlay以及開發多個
服務在格網上藉由SOA架構入口網站存取格網資源，(b) 完成「個人化身心健康管理系統」
的雛型設計，以及相關資料庫的建置規畫，同時也完成心血管疾病預警系統功能之規畫與
API參數的設定，(c) 開發一個行動化的RFID平台，建立RFID讀取標籤與遠端主機連接之
通訊實驗；規劃與建置以SOA為基礎之室內外定位系統，(d) 完成居家環境互動式人機介面
裝置開發，Widget概念製作健康服務GUI與健康服務功能API建置，以及可偵測肢體動作變
化之織品感測元件研製。並於第二年度接續完成下列研究成果，(a) 架構一個具有負載決策
機制與服務組合的服務平台並強化使用者介面達到結合多個不同計算與儲存能力之系統平
台，(b) 完成「個人化身心健康管理系統」的實作以及和其它子計畫的初步整合，開發整體
系統彙總的資料倉儲與溝通介面，並且建置心血管疾病預警系統之功能，(c) 利用第一年所
開發完成的整合型主動式RFID讀取標籤，研究如何利用行動通訊網路，設計與其他子計畫
的連接介面；完成Zigbee室內定位系統之建置，(d) 完成穿戴式運動生理感測智慧平台開
發，及低功率短距離生理感測無線傳輸器電路設計製作。於此第三年度中，本計畫進行各
項子計畫之整合，並完成下列成果：(a) 利用點對點技術並且與其他子計畫進行整合，系統
內使用者可以互相交談或是傳輸檔案，並使用VPN以及虛擬機器提供異地備援模式，(b) 彙
整「個人化身心健康管理系統」與 「心血管疾病預警系統」，並且完成心血管疾病資料的
建立以及其他子系統的整合，提供心血管疾病相關新聞到個人健康助理系統。此外，對於
分別放置Centralized database跟Home server上的資料庫的資料進行同步化處理及定期備份
維護，(c) 基於行動化的RFID平台之基礎上，完成開發一個RFID隨身照護系統，提供個人
在居家醫療照護上所需之服藥確認與資訊提醒功能；完成戶外定位系統，並整合 Google 
Latitude，(d) 穿戴式運動生理感測智慧衣研製，與穩定性評估，並使用HTML5製作跨平台
健康管理系統。 
 
關鍵字：格網、點對點、資料挖掘、RFID、遠距醫療支援服務、穿戴式運動感測智慧衣、
互動式多媒體人機介面、定位系統、行動照護、低功率無線傳輸器、肢體動作織品感測元
件。 
 
  III 
other sub-projects to make users in the system can chat or transmit files with friends, and using 
VPN and Virtual machine technology that can provide remote backup. (b) Completing the 
integration of the PHM system and cardiovascular disease early warning system along with the 
systems of other subprojects, and built the dataset for the establishment of cardiovascular diseases, 
then provide information about Cardiovascular disease-related into personal health assistant 
system.  In addition, we synchronized the data on the central database and the database of 
each home server, and performed data backup and maintenance processes regularly. (c) 
Developing the mobile Medicare protection system to provide the services for elders at home, 
and completing outdoor positioning system, and integration of Google Latitude. (d) APP to the 
concept of smart phones, using HTML5 cross-platform production and health management 
system. Manufacture a wearable smart cloth for sensing posture and evaluate the sense stability. 
 
 
 
 
 
 
Keywords: Grid, P2P, data mining, RFID, mobile Medicare protection system, motion 
measurement textile sensor, interactive multimedia interface, wearable motion measurement 
smart clothing, low voltage and lower power consumption wireless conditioning technology. 
 
  1 
一、 前言 
為提供居家健康管理及保健服務，促進個人健康服務，本計畫建置一套基於格網
之健康服務系統，利用格網技術結合點對點技術，建立標準化平台。提供個人、家庭、
醫師到醫院完整之健康管理服務，並透過感測元件與 RFID 技術建置個人化行動照護
系統，及利用纖維材料開發可監測心電訊號、體溫、呼吸及肢體動作之織品感測元件，
讓使用者於居家環境中，舒適的接受全天候生理監測。藉由本計畫所建置之普及健康
服務格網，建立個人居家健康照顧服務，達到疾病預防、提高健康照顧品質和降低健
保成本之目標。 
二、 研究目的 
本計畫建置之醫療格網提供個人、家庭、醫師到醫院之完整健康服務，以達成智
慧型健康管理之目標，本計畫包含四個子計畫：A.健康服務格網系統，B.個人化身心
健康諮詢與管理系統，C.個人化之行動照護系統，D.個人化身心健康服務人機介面系
統。各子計畫之研究目的如下： 
子計畫A：健康服務格網系統 
透過使用點對點 (peer-to-peer)以及格網(Grid)技術來發展一健康服務格網系統
(Health Service Grid)，其中主要包含兩個部分，一是居家健康格網系統(Home Health 
Grid)，主要提供使用者在家庭之中的健康服務；二是與醫療格網(Medicare Grid)系統整
合，提供醫院端的追蹤以及諮詢服務，希望能將原本只有病患本人到醫院才能做的問
診或檢查部分能在家庭之中完成。我們將提供標準化的API來整合其他子計畫提供的各
種醫療或是健康服務，有效的管理個人的健康資訊，並且能夠與其他居家健康格網系
統連線，讓民眾能夠不論走到哪，只要有安裝居家健康格網系統的地方，都能夠享受
到同樣的服務。此外，我們將透過點對點的技術整合格網內各節點，以改善各項資料
的分享效能；並且利用雲端的虛擬化技術，提供系統的容錯以及備份效益。 
子計畫B：個人化身心健康諮詢與管理系統 
預計開發個人化身心健康諮詢與管理的健康服務系統，來達到居家健康服務的目
的，其中服務的項目包括：個人健康紀錄追蹤(就診紀錄、後續複診/復健紀錄)、病症
控制、以及健康諮詢系統等。主要為疾病預防及醫療照護資料的追蹤管理，希望藉由
個人健康資料的分析來提供一套完整的健康服務。本子計畫將透過資料挖掘的技術和
基因分析的方法，以符合CRISP-DM程序的研究方法來進行標準的發展步驟，針對復
健和運動治療、慢性病患追蹤、用藥、飲食和營養管理、行動安全偵測等等，來開發
各種相關的諮詢輔助功能，以提供智慧型的個人化身心健康諮詢與管理為目標。 
子計畫C：個人化之行動照護系統 
透過RFID技術的使用，並結合行動通訊機制與居家健康格網系統連結，建立一個
人化的行動照護系統。本子計畫將與產業界合作，研究開發可方便攜帶的行動化RFID
手持設備，讓讀取器具備行動通訊與可程式化功能，本身亦具備主動式RFID標籤的功
能，讓RFID辨識機制得以在日常生活中運作。另一方面，我們亦將發展RFID系統與開
放式格網系統的資料交換介面，讓行動照護系統得以藉由格網系統上龐大的儲存與運
算能力，普及於個人的日常生活。因應未來以服務為導向的資訊與健康服務產業，本
  3 
 
子計畫 B： 
個人化的健康管理，讓醫療資源可以針對病患所需有效使用，對於醫療照護與資
源運用都更為合適。在國內研究方面，文獻 [5]從需求與供給兩面去探討老年人的長期
照顧與照顧者間的關係，並分析研究這些資料力求提升居家照顧的品質，讓長期受照
顧者受惠。此外，為提升老年人跟慢性病者的居家安全與生活品質，即時監控系統的
開發於文獻 [6]則是另一重點。營養師在居家照護中扮演極重要的角色，營養師需要根
據實際狀況制定個人化的營養評估 [7]，將此數據再透過資料挖掘分析居家照顧的效
益，文獻 [8]將資料挖掘運用於居家照護的分析上，建立分析模型，以及文獻 [9]針對
病患特質與醫院屬性進行分析。 
國際上如歐盟有制定出一連串的國家策略來發展e-Health [10~12]，其e-Health的相
關政策、方針與目前我們發展的系統架構相似，除了整合醫療資源以期給予病患個人
化的居家健康照護能有最佳的諮詢與管理外，更能節省醫療資源降低能源的消耗。 
資料倉儲研究上，文獻 [13]研究資料倉儲在SOA架構的格網平台上的應用，文獻 
[14]建立可以動態維護資料的資料倉儲。資料挖掘技術方面，文獻 [15]利用關聯法則
在醫療資料上做分析來讓醫生做為診療的參考，並為病患做更適當的治療，文獻 [16]
利用關聯法則的分析來預測疾病以便預防病人罹患可能的潛在疾病。 
基因方面的研究，Welcome Trust Case Control Consortium (WTCCC)在新英格蘭醫
學期刊(NEJM)發表使用 Affymetrix Human Mapping 500K Array Set基因晶片進行冠狀
動脈疾病單一核干酸多型性之研究 [17]。病人樣本來自英國公衛系統以及自願者捐
獻，一共收集到英國六十歲以下的 1988個心肌梗塞病人進行實驗，並將實驗結果與德
國於 Augusburg 所進行的一項心肌梗塞研究計畫 (German MI)的成果交岔比對 
[18~21]，並找到一個與冠狀動脈疾病高度關聯(>90%)的 loci : 9p21.3(SNP, rs1333049)，
以及數個關聯性機率高於 80%的基因座上有與心肌梗塞高度相關的 SNP︰1p13.3 
(rs599839)，1q41 (rs17465637)，10q11.21 (rs501120)與 15q22.33 (rs17228212)。Copy 
Number Variation (CNV) [22]方法進行基因的預測。另外 CNV是一種偵測人類遺傳基
因體變異區塊的方法，我們藉由比較高血脂症病人間共同的 CNV區塊，得到七個位於
基因上具有統計顯著性的拷貝數區域，這些區域可能會與心血管疾病有相關。文獻 [23, 
24]為近年來關於心臟疾病基因表現研究的文獻，說明了如心血管疾病這種長期的慢性
疾病在現今都可藉由遺傳資訊的收集與分析進行預測以及預警。 
 
子計畫 C： 
健康照護對象範圍廣泛，可區分為 3 大類，分別為在家生活、慢性病患者以及重
症病患。在家生活指的是可以獨立自主生活的健康人，但其本身希望得到保健需求；
慢性病患者包括身體功能退化、中風、憂鬱症、失智者之慢性病等；重症患者則為需
要臥床的病人 [25]。而本子計劃將可滿足健康醫療格網所需的自動辨識機制。 
針對居家環境，為因應健康照護或醫療求助的需求，蒐集及傳輸需求者的血壓、
血糖及其它生物訊號的資訊內容，將有助於醫護人員掌握與追蹤其生理狀況，降低診
斷處置的錯誤率，進而提升醫療品質 [26]。由於 RFID 具有非接觸性、無方向性、可
永久使用等特點 [27]，可對居家環境的個人健康照護狀態，進行自動化的物件辨識與
  5 
面設計上就有些基本訴求(徐瑜卿，行動多媒體通訊產品使用者介面設計之研究) [48]： 
(1) 介面目錄以圖像方式呈現，讓使用者在辨識上速度較快。 
(2) 螢幕範圍內所顯示的資訊量不可過多，以適量圖文搭配為主。  
(3) 圖像是功能選單以動態形式呈現會加強整體操作互動的感受。  
(4) 選單結構的設定影響使用者對操作功能選單路徑上的掌握，呈現方式  
   以九宮格的配置較好。  
(5) 選單的寬度太大會導致使用者迷失，功能選單只在一個層級的畫面全  
   部顯示出來，才能幫助使用者達到快速且正確的搜尋導引效果。  
3. 應變感測智慧型紡織品 
在應變感測智慧型紡織品中存在著些許問題，如應力-應變曲線與應變-電阻曲線的
不穩定，而造成再現性不佳 [49-52]。因此感測材料的選擇與感測元件之設計為應變感
測織品的重要考量因素。並且許多研究僅能顯示肢體運動過程織品的電阻變化，無法
獲得相對應的角度變化;或者可量測肢體動作角度變化的儀器設備過於複雜與龐大，造
成量測程序繁多、費時、和地點侷限等缺點 [53, 54]。因此，期望利用本計畫所研製的
穿戴式運動生理感測智慧衣隨時隨地量測人體姿態角度的變化。 
四、 研究方法與執行進度 
子計畫 A 
本子計畫在本年度主要是在完成各種 API以便與其他子計畫進行整合。此外，近
年來由於雲端運算的發展，故本計畫亦將雲端運算之技術以及概念運用於本計畫中。
我們使用雲端運算的技術，來備份 Home Server中的資料；因此即使家中的Home Server
的機器或是資料突然毀壞了，我們也不必擔心資料消失的問題，我們仍舊可以從雲端
中將資料抓回。另外，我們還使用了雲端運算中的虛擬機器，來代替未開機的 Home 
Server；因此即使家中的 Home Server未開機，我們仍舊可以開啟一台虛擬機器代替此
home server，並且從雲端備份的資料中抓取此 Home Server之資料。因此，對於使用
者而言，無論何時何地都可以順利的抓取他們想要查詢的資料。 
使用 VPN以及虛擬機器提供異地備援模式： 
l 虛擬化（Virtualization） 
雲端服務的提供，不論是提供平台（Platform）、軟體（Software）、設備
（Infrastructure）或是網路（Network）的服務都需要大量的主機作為後盾。以主
機管理的角度來說，若全部以實體主機提供服務，則硬體設施的費用將讓人望之
卻步，再加上主機空間、電力使用及管理人力上的成本，更是一筆龐大的支出。
因此，我們需要採用虛擬化技術來因應，虛擬化技術可以在單一介面下管理不同
的作業系統，並進行資源分配達到充分利用系統資源的目的。過去虛擬化技術總
讓人有效能不彰的印象，近年來在各家軟硬體廠商的努力，包含虛擬化平臺架構
的改良及硬體支援的大力加持下，已有不少廠商在營運環境中使用虛擬化技術的
成功案例，例如微軟的MSDN網站以及 Amazon的 EC2等等。 
l 虛擬機器 (Virtual Machine) 
虛擬機器的目的是希望能夠在一部實體主機上面『同時運作多個作業系統』，
目前常見的虛擬機器軟體有：VMWare，KVM，Xen，VirtualBox等。 
l KVM（Kernel-based Virtual Machine） 
  7 
本子計畫建置一套個人化身心健康諮詢與管理系統，藉由健康服務格網平台，隨
時追蹤、記錄使用者的健康資訊，達到有效率的管理，讓使用者可以無時無刻掌握、
檢測自己的身體健康情況。醫師也能藉由格網上的資料庫快速掌握病患的健康問題，
給予病患一些諮詢建議或要求到醫院就診，讓健康服務不會受限於醫院，可以在系統
服務範圍內的任何地方都能得到好的服務。 
表2為第三年的執行進度表。第三年進行與各子計畫整合，完成「個人化身心健康
管理系統」與「心血管疾病預警系統」的介面實作與測試，提供多樣的遺傳預測方法，
像是單一核甘酸多型性(SNP)與心血管遺傳疾病檢測，還有拷貝數變異(CNV)與心血管
遺傳疾病檢測，建立相關的測試資料，並且進行兩個系統的整合，透過「心血管疾病
預警系統」的基因比對分析，將判定「是否有心血管疾病」的結果存入「個人化身心
健康管理系統」中的檢驗資料表，根據判定結果，提供有心血管疾病基因者飲食相關
衛教飲食的建議及相關的新聞，建立後端新聞知識庫，提供諮詢者有關心血管疾病相
關的新聞訊息，以建立更完整的各種個人化的模組。除了系統的部份，同時透過個人
健康助理手機介面與醫院連結。 
在資料庫整合的部分，除了定期備份資料，並將資料分別放置Central database跟
Home server上的database。由於，資料分散在各個Home server的資料庫中，因此我們使
用SQL SERVER 2008中的同步化機制，對所有儲存在Central database及Home server的
資料進行同步化的處理。另外，我們提出SQL-HBase Mapping Model，目的是讓
Web-based Application Program可以用SQL指令存取HBase。首先將Web Program內的
SQL指令，透過Web Service這個XML格式的服務將指令包裝好並傳送給HBase所在的
雲端平台。藉由Web Service與Socket的合作，雲端平台會呼叫SQL-HBase program把包
裝好的SQL指令拆開、分析、並翻譯成HBase API適用的格式，來執行HBase。最後再
透過Web Service把執行結果回覆給Web Program，即完成SQL-HBase Mapping的動作。 
與子計畫A進行系統登入流程的修改、測試，並且與子計畫A的好友清單整合連
線，透過權限的設定，觀看好友相關資料。與子計畫D的手機介面進行整合，提供使
用者飲食記錄的輸入介面，輸入餐點的方式使用「食物樣版」，例如：排骨飯、雞排飯
（內容物可能約有3菜及2碗飯）等的形式讓使用者選擇，使用者也可以自行建立個人
化的「食物樣版」。另外系統中還建立卡路里簡易的計算，並且建立手機版及網頁版介
面皆可以使用的「卡路里運算類別」。 
表 2. 子計畫 B第三年執行進度的規畫表 
時程 工作項目 進度評估 
2010/11~ 
2011/01 
l 「個人化身心健康管理系統」： 
- 介面修改、測試 
- 建立測試資料 
心血管疾病預警系統新增資料 
已完成 
2011/02~ 
2011/04 
l 與「心血管疾病預警系統」整合： 
- 將判定「是否有心血管疾病」的結果存入「個人化身
心健康管理系統」中的檢驗資料表。 
- 建立飲食衛教的頁面。 
l 系統的登入流程： 
- 修改為使用 Cookies紀錄登入資料 
已完成 
  9 
Ø 開發智慧型手機定位系統介面。 
Ø 整合 Google Latitude於 SOA定位系統中。 
Ø 定位系統與格網系統之資料交換介面。 
Ø 子計畫間的系統整合。 
表 3. 子計畫 C第三年執行進度的規畫表 
時程 工作項目 進度評估 
2010/11~ 
2011/01 
l 發展可攜式RFID整合式手持設備ヽ與實際環境的部署。 
l 建立單一入口網頁 SOA Portal Web，將室內外定位系統
統整在單一使用介面上。 
已完成 
2011/02~ 
2011/04 
l 服藥提性ヽ確認ヽ回報與長期用藥續購之功能。 
l 將 Google Latitude整合至 SOA定位系統中，讓戶外定位
的方式由原本的 GPS方法外，可以有更多的定位資訊來
源。 
已完成 
2011/05~ 
2011/07 
l 大範圍公共區域的監控與安全性提示。 
l 移植 SOA平台的定位系統介面至智慧型手機上，讓在戶
外活動的老人可以簡單的使用，並獲得健康上的保障。 
已完成 
2011/08~ 
2011/10 
l 完成長期照護的雲端資訊服務機制。 
l 提供各子計畫需要的 API功能，並製作簡易的呈現資訊
介面，整合至計畫中的入口網頁。 
已完成 
 
子計畫 D 
本子計畫D主要是開發一套人性化身心健康服務人機介面系統，內容包含子計畫
D1所發展的互動式人機介面裝置，與子計畫D2開發的個人化穿戴式運動生理感測智慧
衣平台。由於行動裝置介面解析度皆不相同，在介面顯示的部分須考慮到各裝置顯示
的大小，以及使用觸控螢幕觸控的容易度。研究一般智慧型手機APP的顯示方式，發
現多以圖案為主，文字為輔。每個功能的按鈕幾乎都是大圖示且好按，再以簡單的幾
個文字來說明。並規畫其他子計畫需顯示的畫面，整合進行動裝置版的健康管理系統
中。第一版的介面以九宮格顯示為主，以便在小尺寸的螢幕能夠一次看到所有功能。 第
二版的介面考慮到使用者操作可能會將裝置橫放操作，九宮格顯示方式並不能滿足此
需求。所以參照一些製作行動裝置版的網頁，使用JQuery+CSS3做版面設計。目前行
動裝置APP的開發程式語言都不相同，為了能夠使系統能跨平台操作，所以採用Web 
Application的方式，使用最新的HTML5來製作。不論是Apple的iOS、Google的Android、
或是Microsoft的Windows Phone，HTML5都有支援顯示。 
在個人化穿戴式運動生理感測智慧衣平台方面，研究結果顯示，生理量測系統必
須在適當壓力下與肢體動作感測元件接觸，同時不阻礙肢體動作感測元件之伸長與回
復，才能獲得穩定並且準確的肢體動作角度變化。因此，本計劃於第三年度著重於兩
個方向的研究:1. 肢體感測元件之穩定化設計，和 2. 固定感測元件於測試部位的織品
結構設計。最後，在計畫整合階段進行個人化穿戴式運動生理感測智慧衣的測試。表
4為子計畫 D第三年執行進度的規畫表。 
表 4. 子計畫 D第三年執行進度的規畫表 
時程 工作項目 進度評估 
2010/11~ l 近距離無線傳輸端與遠距離傳輸平台整合。 已完成 
  11 
l 完成與子計畫 D1整合的「健康食」的介面，提供個人化輸入的服務，讓使用者方
便操作。另外系統中的卡路里簡易的計算，可以約略估計使用者所攝取營養素(熱
量、醣類、蛋白質、脂肪)的量。 
 
子計畫 C 
l 掌握 RFID 元件設計技術及智慧型手機的整合方法，並熟悉健康醫療格網所需的
系統界面，做為本研究計劃整合發展的基礎。 
l 確認 CHF做為整體居家醫療格網的服務架構，並發展各層級所應提供的服務功能
模組，完成 RFID讀取器中介軟體、XML訊息轉換機制及行動裝置的通訊傳輸介面，
做為實踐居家醫療格網「無所不在 Ubiquitous」應用的關鍵。 
l 將 ZigBee室內定位與 GPS 戶外定位的介面統一建置在 SOA Portal Web上，並了
解相關系統的發展技術，做為往後系統發展與研究計劃的前置作業。 
l 以 Google定位 API為基礎，進行戶外定位系統的雛型開發。 
l 將 SOA定位系統之介面移植至智慧型手機中，進行定位系統介面的開發。 
l 撰寫定位系統相關的 API，並同時替定位資訊開發簡易的資訊呈現介面，整合至
計畫中的入口網頁。 
 
子計畫 D 
子計畫 D在計畫執行成果，已經完成開發人性化身心健康服務人機介面系統，內
容包含子計畫 D1所發展的互動式人機介面裝置，與子計畫 D2開發的個人化穿戴式運
動生理感測智慧衣平台。本子計畫結合現今的生醫訊號量測技術，纖維紡織科技與人
性化互動式多媒體設計技術，開發成為一個人化穿戴式運動生理感測智慧衣與互動式
人機介面裝置， 
關於實際應用時的相關問題，由於本系統之設計導向與實做為建立可融入居家環
境中，並能與個人體貼互動及介面簡單友善的健康服務裝置為應用目的。對於欲提供
健康服務者與醫療院所來說，此裝置與系統可提供健康服務為目的導向所需的開放式
架構軟體(包含健康服務 GUI 與健康服務功能 API)服務應用介面與互動介面，可簡化
醫療院所實際導入服務者需在使用者端的開發時間與部署困難度。對於使用者來說，
可在居家環境裡裝置上的熟悉之互動式介面中，獲得更多樣化的健康服務。 
本計畫第一年已針對五種導電彈性織帶結構探討其應變-電阻行為，第二年研製可
偵測肢體動作變化之織品感測元件。第三年主要針對「穿戴式運動生理感測智慧衣研
製，與舒適性、穩定性評估」進行探討，而可細分為三項，結果與討論如下： 
l 織品感測元件的穩定化設計 
    織品感測元件的開發已有了雛型的設計，但存在著電阻值的不穩定，而訊號可用
程度有限等缺點。因此調整電極間距以及對織帶施加適當壓力，增加織帶與電極間的
接觸壓力，穩定使用過程中織帶與電極間的接觸面積。結果顯示，織品感測元件經穩
定化設計後，顯著提升數據的精準性與穩定性。 
l 紡織品固定元件的研製 
    人體在活動時，服飾總是會受到四肢或軀幹擺動的牽扯，帶動服飾其他部位的 
移動，這可能嚴重造成感測元件偵測的誤判。因此肢體動作織品感測元件必須固定在
正確的位置上，不能有太大的左右或上下之位移。穿戴式運動生理感測智慧衣的設計
  13 
2006. 
[8] 郭惠敏、徐雅瑛，居家照護品質預測因子之探討-以居家照護資料庫之資料探勘為例，
長庚大學，2004. 
[9] 鄒淑英、詹前隆，利用資料探勘技術探討超長天住院病人之相關屬性，元智大學，2005. 
[10] http://stn.nsc.gov.tw/view_detail.asp?doc_uid=0970531001&kind_no=A01) 
[11] http://www.ehealthnews.eu/content/view/1064/62/ 
[12] http://www.sweden.gov.se/content/1/c6/06/43/24/f6405a1c.pdf 
[13] Pascal Wehrle, Maryvonne Miquel, Anne Tchounikine, “A Grid Services-Oriented 
Architecture for Efficient Operation of Distributed Data Warehouses on Globus,” 21st 
International Conference on Advanced Networking and Applications (AINA'07), 2007.  
[14] A. Halim Elamy, Reda S. Alhajj, Behrouz H. Far, “Building Data Warehouses with 
Incremental Maintenance for Decision Support,” CCECE/CCGEI, Saskatoon, May 2005. 
[15] Ribeiro, M.X., Traina, A.J.M., Traina, C., Azevedo-Marques, P.M., “An Association 
Rule-Based Method to Support Medical Image Diagnosis with Efficiency,” IEEE 
TRANSACTIONS ON MULTIMEDIA, VOL. 10, NO. 2, FEBRUARY 2008. 
[16] Ordonez, C., “Association rule discovery with the train and test approach for heart disease 
prediction,” IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE, 
VOL. 10, NO. 2, APRIL 2006. 
[17] Wellcome Trust Case Control Consortium. Genome-wide association study of 14,000 cases 
of seven common diseases and 3,000 shared controls. Nature 2007; 447:661-678. 
[18] Lopez AD, Mathers CD, Ezzati M, Jamison DT, Murray CJ. Global and regional burden of 
disease and risk factors, 2001: systematic analysis of population health data. Lancet 2006; 
367:1747-1757. 
[19] Yusuf S, Hawken S, Ounpuu S, et al. Effect of potentially modifiable risk factors associated 
with myocardial infarction in 52 countries (the INTERHEART study): case-control study. 
Lancet 2004; 364:937-952.  
[20] Marenberg ME, Risch N, Berkman LF, Floderus B, de Faire U. Genetic susceptibility to 
death from coronary heart disease in a study of twins. N Engl J Med 1994; 330:1041-1046.  
[21] Christensen K, Murray JC. What genome-wide association studies can do for medicine. N 
Engl J Med 2007; 356:1094-1097. 
[22]Personal genome sequencing: current approaches and challenges, Michael Snyder, Jiang Du 
and Mark Gerstein, 2010 24: 423-431 Genes Dev. 
[23]Samani NJ, Burton P, Mangino M, et al. A genomewide linkage study of 1,933 families 
affected by premature coronary artery disease: The British Heart Foundation (BHF) Family 
Heart Study. Am J Hum Genet 2005; 77:1011-1020. 
[24] Alfakih K, Brown B, Lawrance RA, et al. Effect of a common X-linked angiotensin II type 
2-receptor gene polymorphism (-1332 G/A) on the occurrence of premature myocardial 
infarction and stenotic atherosclerosis requiring revascularization. Atherosclerosis (in press). 
[25] Kenneth H. Doerr, William R. Gatesa & John E. Mutty (October 2006), “A hybrid approach 
to the valuation of RFID/MEMS technology applied to ordnance inventory”, International 
  15 
2003. 
[43]Dominique Paret, RFID and Contactless Smart Cards, Wiley, 2002. 
[44] Kenneth H. Doerr, William R. Gatesa and John E. Mutty (October 2006), “A hybrid 
approach to the valuation of RFID/MEMS technology applied to ordnance inventory”, 
International Journal of Production Economics, Pages 726-741, Volume 103, Issue 2. 
[45] http://www.microsoft.com/health/ww/ict/Pages/Connected-Health -Framework.aspx 
[46] https://health.google.com/health/ 
[47] http://www.microsoft.com/taiwan/business/peopleready/newsletter/03/industry_focus_1.msp
x  
[48] 王復中、周宣光，e-Health 2.0  現象研究，第五屆國際健康資訊管理研討會， 民國99
年3月。 
[49] C. T. Huang, C. F. Tang, M. C. Lee, S. H. Chang, A wearable yarn-based piezo-resistive 
sensor, Sensors and Actuators A: Physical, Vol. 141, pp. 396-403, 2008. 
[50] C. T. Huanga, C. F. Tang, M. C. Lee, S. H. Changa, Parametric design of yarn-based 
piezoresistive sensors for smart textiles, Sensors and Actuators A: Physical, Vol 148, pp. 10-15, 
2008. 
[51] E. Scilingo, F. Lorussi, A. Mazzoldi, D. Rossi, Strain-Sensing Fabrics for Wearable 
Kinaesthetic-Like Systems, IEEE, Vol. 3, No. 4, pp. 460-467, 2003. 
[52] S. Coyle, D. Morris, K.T. Lau, D. Diamond, N. Moyna, Textile-based wearable sensors for 
assisting sports performance, IEEE, pp. 307-311, 2009. 
[53] M. Pacelli, G. Loriga., N. Taccini, R. Paradiso, Sensing Fabrics for Monitoring Physiological 
and Biomechanical Variables: E-textile solutions, IEEE, pp. 1-4, 2006 . 
[54] F. Lorussi, S. Galatolo, D. Rossi, Textile-Based Electrogoniometers for Wearable Posture 
and Gesture Capture Systems, IEEE, Vol. 9, No. 9, pp.1014-1024, 2009. 
[55] Ssu-Hsuan Lu, Kuan-Chou Lai, Kuan-Ching Li, and Yeh-Ching Chung, "Design and 
Analysis of Arrangement Graph-based Overlay Systems for Information 
Sharing," Proceedings of the 3rd IEEE International Workshop on Management of Emerging 
Networks and Services (IEEE MENS 2011) in conjunction with IEEE GLOBECOM 2011, 5-9 
December, Houston, Texas, USA, pp. 694-698. 
[56] Tian-Liang Huang, Po-Jung Huang, Quan-Jie Chen, You-Fu Yu, Kuan-Chou Lai, 
“Neighboring-based Dynamic Distributed Load Balance Policies,” TANET 2010, Oct. 27-29, 
2010. 
[57] Quan-Jie Chen, You-Fu Yu, Tian-Liang Huang, Po-Jung Huang, Kuan-Chou Lai, “A 
Semi-Structured P2P Network for Multi-Attribute Range Query in Cloud Computing,” 
TANET 2010, Oct. 27-29, 2010. 
[58]  Chin-Jung Hsu, Wu-Chun Chung, Kuan-Chou Lai, Kuan-Ching Li, Yeh-Ching Chung, “A 
Novel Approach for Cooperative Overlay-Maintenance in Multi-Overlay Environments,” The 
2nd IEEE International Conference on Cloud Computing Technology and Science (CloudCom 
2010), Nov. 30 – Dec. 3, 2010. 
[59] Kuan-Chou Lai, and You-Fu Yu, “A Semi-Structured Overlay for Multi-Attribute Range 
  17 
Fang-Rong Hsu, Don-Lin Yang, Chia-Hsien Wen, and Chuang-Chien Chiu, "Medicare-Grid: 
New Trends on the Development of E-Health System Based on Grid Technology", E-Health 
2010, IFIP Advances in Information and Communication Technology, Volume 335, H. 
Takeda (Ed.), pp. 148–159, 2010. (ISSN: 0302-9743 (Print) 1611-3349 (Online), EI). 
[72] Chao-Tung Yang, Wen-Jen Hu, and Bo-Han Chen, “A Multiple Grid Resource Broker with 
Monitoring and Information Services,” Algorithms and Architectures for Parallel Processing - 
9th International Conference, ICA3PP 2010, Lecture Notes in Computer 
Science, Editors: , vol., 6083, pp. 90–99, Springer, Korea, May 21-23, 2010. 
(ISSN: 0302-9743 (Print) 1611-3349 (Online), EI). 
[73] Chao-Tung Yang, Chih-Hao Lin, and Wen-Jen Hu, “Implementation of a Heuristic QoS 
Measurement for Network Bandwidth on Grid Computing Environments,” Algorithms and 
Architectures for Parallel Processing - 9th International Conference,ICA3PP 2010, Lecture 
Notes in Computer Science,Editors: , vol., 6082, pp. 121–130, Springer, Korea, May 21-23, 
2010. (ISSN: 0302-9743 (Print) 1611-3349 (Online), EI). 
[74] Chao-Tung Yang, Keng-Yi Chou, and Kuan-Chou Lai, “An Adaptive Job Allocation 
Strategy for Heterogeneous Multi-Cluster Systems,” Advances inGrid and Pervasive 
Computing - 5th International Conference, GPC 2010, Lecture Notes in Computer Science, 
Editors: RS Chang, vol. 6104, pp. 562–572, Springer, Taiwan, 11-13 May 2010. 
(ISSN: 0302-9743 (Print) 1611-3349 (Online), EI). 
[75] Chao-Tung Yang, Hung-Yen Chen, Chih-Lin Huang, and Shyh-Chang Tsaur, 
"Implementation of a Distributed File Storage with Replica Management in Peer-to-Peer 
Environments", International Journal of Ad Hoc and Ubiquitous Computing (IJAHUC), Vol. 7, 
No. 3, pp. 202 – 210, May 2011. (ISSN Online: 1743-8233 - ISSN Print: 1743-8225, SCI JCR 
IF=0.435, 0.865, 56/78, EI). 
[76] Po-Chi Shih, Yeh-Ching Chung, Kuan-Ching Li, Chao-Tung Yang, Ching-Hsien Hsu, 
Fang-Rong Hsu, Dong-Lin Yang and Chia-Hsien Wen, “Medicare-Grid: A Grid Based 
E-Health System”, Information Journal(SCI), 2011. 
[77] Chao-Tung Yang, Keng-Yi Chou, and Kuan-Chou Lai, “Design and Implementation of an 
Adaptive Job Allocation Strategy for Heterogeneous Multi-Cluster Systems,” Concurrency 
and Computation: Practice and Experience, Volume 23, Issue 15, pages 1701–1722, October 
2011. (ISSN: 1532-0626, SCI JCR IF=0.907,1.004, 1.791, EI). 
[78] Wei Chen, Kuo-Cheng Yin, Don-Lin Yang, Ming-Chuan Hung, "Data Migration from Grid 
to Cloud Computing," The First International Conference on Engineering and Technology 
Innovation (ICETI2011), 2011-11, Kenting, Taiwan. 
[79] Ming-Hung Hsieh, Don-Lin Yang, Jungpin Wu, "Effective Facial Expression Recognition 
on Melancholia Analysis," 2011 Symposium on Engineering, Medicine, and Biology 
Applications, Session 13-1, pp.1-4 , 2011-07. Kaohsiung, Taiwan. 
[80] Wei Chen, Don-Lin Yang, Ming-Chuan Hung, Jungpin Wu, "An Omnipresent Personal 
Health Management System," The 2011 International Conference on Computing and Security , 
C-15 , 2011-07. Ulaanbaatar, Mongolia. 
  19 
Journal on Wireless Communications and Networking (JWCN), vol. 2008, Article ID 604747, 
9 pages, 2008. doi:10.1155/2008/604747 (SCI, IF=0.976) 
[93] Tien-Wei Shyr, Shih-Ju Huang, Variation of Microstrain of 316L Stainless Steel Fiber 
during a cold drawing process, 2011 中國紡織學術年會, Oct. 21-22, Shanghai, China, 2011. 
[94] Shih-Ju Huang, Tien-Wei Shyr, Formation of σ-phase in 316L Stainless Steel Fiber Using a 
Multi-pass Cold Drawing Process, Conference, 2011 The World Congress on Engineering and 
Technology, Oct. 28 - Nov. 2, Shanghai, China, 2011. 
[95] Huang Shih-Ju and Shyr Tien-Wei, Crystalline Phases of 316L Stainless Steel from Wire to 
Fiber, The 10th Asia Textile Conference, September 7-9, Ueda, Nagano, Japan, 2009. 
[96] 許家瑜、林庭安、韓修齊、王修羚、江長翰、謝靜玟、石天威，不鏽鋼與鍍碳導電織
帶之結構參數對接觸電阻之探討，第二十七屆纖維紡織科技研討會，南亞技術學院，中
壢，2011年。 
[97] 石天威、謝靜玟、李忠真、江長翰，彈性織帶力學遲滯之探討，第三十四屆高分子學
術研討會暨前瞻功能性高分子材料國際學術研討會，逢甲大學，台中，2011年。 
[98] 石天威、謝靜玟、黃詩茹，不鏽鋼纖維晶相結構對電磁遮蔽機制之影響，2011年第四
屆兩岸紡織科技研討會，台南應用科技大學，台南，2011年。 
[99] 謝靜玟、石天威，316L不鏽鋼纖維晶體結構與矯頑磁力之關係，2010年中國材料科學
學會年會。義守大學，高雄，2010年。 
[100] Shyr Tien-Wei and Huang Shih-Ju，Quantitative analysis of Textural 316L Stainless 
Steel Fiber by Rietveld Method，2010年中國材料科學學會年會。義守大學，高雄，2010
年。 
[101] 江長翰、陳敬中、江明哲、李忠真、謝靜玟、石天威，不同量測條件下鍍碳導電
彈性織帶之彎曲-電阻關係的探討，第二十六屆纖維紡織科技研討會，嶺東科技大學，台
中，2010。 
[102] 江長翰、沈宗毅、商明德、李忠真、謝靜玟、石天威，濕潤條件對鍍碳導電彈性
織帶之拉伸-電阻關係的探討，第二十六屆纖維紡織科技研討會，嶺東科技大學，台中，
2010。 
[103] 石天威、謝靜玟、黃詩如、李忠真、江長翰、林佑澤，不銹鋼紗/Lycra®織帶對伸
長率和應變速率於拉伸-電阻值之探討，第二十五屆纖維紡織科技研討會，中國文化大
學，台北，OB-01，P.25，2009。 
[104] 石天威、謝靜玟、黃詩如、李忠真、江長翰、林佑澤，不銹鋼紗/Lycra®織帶對伸
長率和應變速率於拉伸-電阻值之探討，第二十五屆纖維紡織科技研討會，中國文化大
學，台北，OB-01，P.25，2009。 
[105] Tien-Wei Shyr*, Jing-Wen Shie and Yan-Er Jhuang, The Effect of Tensile Hysteresis 
and Contact Resistance on the Performance of Strain-Resistant Elastic-Conductive Webbing, 
INSTRUMENTS & INSTRUMENTATION Sensors 2011, 11, 1693-1705. 
[106] Tien-Wei Shyr*, Jing-Wen Shie, Shih-Ju Huang, Shun-Tung Yang, Weng-Sing Hwang, 
Phase transformation of 316L stainless steel from wire to fiber, Materials Chemistry and 
Physics, Volume 122, No. 1, pp. 273-277, 2010. 
[107] 石天威、謝靜玟、莊宴爾，黏彈與摩擦性質對Lycra®彈性織帶力學遲滯現象之影
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
■ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱  計 畫 編 號  
報 告 人 
姓 名 
史伯其 服 務 機 構 
及 職 稱 
清華大學資工系博士生 
會議/訪問時間 
 地點 
2010.12.9~2010.12.11 
韓國光州 
會 議 名 稱 The 2010 FTRA World Convergence Conference (FTRA WCC 2010) 
發表論文題目 Medicare-Grid: A Grid Based E-Health System（檢附論文檔案） 
一、主要任務摘要（五十字以內） 
The 2010 FTRA World Convergence Conference (FTRA WCC 2010) is aimed at 
addressing key themes on "Technologies and Services for Converged environments". 
My mission objective is to present our paper entitled “Medicare-Grid: A Grid Based 
E-Health System” as well as sharing experiments with many international researchers in 
the E-Health field. 
二、經過 
This 2010 FTRA World Convergence Conference is hold in Gwangju, Korea from 
December 9 to 11. I present our paper entitled “Medicare-Grid: A Grid Based E-Health 
System” at U-healthcare session 1-E on December 9. Besides, I also attend many other 
session includes other U-healthcare session and keynote speaking session. In this 
conference I meet with many domestic and international researchers in the E-health 
field and learn many new idea and technologies by experience sharing. Following 
pictures are taken from the conference. 
 三、心得 
The rapid evolution of information technology has brought new opportunities to 
improvements in the state of art of medical services. One major scenario is that patient’s 
digital health record can easily be shared among hospitals and medical centers via 
internet, enabling the examination performed in one location while clinical diagnosis by 
physicians in another location. How to sharing the digital health record among hospital 
with less interfere to original procedure when taken medical treatment is an important 
issue. Standard format of digital health record is the key solution. There exist an 
international standard named HL7 for the digital health record, but, however, it is not 
fully compatible with the standard format (Taiwan electronic Medical record Template) 
used in Taiwan. It will be an important future work on how to make Taiwan standard 
compatible with international standard. 
四、建議與結語 
International conference provides a very good opportunity to make a connection 
from domestic researchers with international researchers. It is recommended to hold as 
more international conference as possible in Taiwan, to extend the research perspectives 
Medicare-Grid: new trends on the development of  
E-Health System based on Grid Technology 
Yeh-Ching Chung
1
, Po-Chi Shih
1
, Kuan-Ching Li
2
, Chao-Tung Yang
3
, Ching-
Hsien Hsu
4
, Fang-Rong Hsu
5
, Don-Lin Yang
5
, Chia-Hsien Wen
2
, Chuang-Chien 
Chiu
5 
 
1National Tsing Hua University, 2Providence University, 3Tunghai University, 4Chung Hua 
University, 5Feng Chia University 
ychung@cs.nthu.edu.tw1 
Abstract. The evolution of information technology over the last decade has 
brought opportunities to improvements in the state of art of medical services. 
One scenario is that a patient’s digital health record can easily be shared among 
hospitals and medical centers via internet, enabling the examination performed 
in one location while clinical diagnosis be done by physicians in another. In this 
paper, we propose a Medicare-Grid — a novel grid-based E-health system 
proposed to ease the process of retrieving and exchanging personal health data 
among hospitals and medical centers. Grid and peer-to-peer technologies have 
been used to integrate computing and storage resources provided by hospitals, 
as also to develop an Electronic Health Record (EHR) center to store and share 
EHRs among these locations. We have also developed a system prototype using 
ultimate hardware resources and open source software systems to simulate a 
real scenario as described above. We demonstrate that the idea proposed in the 
project is feasible, possible to be implemented and applicable to real world. 
Keywords: Grid, E-health, P2P, data mining, RFID, wearable measuring 
system. 
1   Introduction 
During the last century, the development of medical services has greatly improved 
regarding to the quality of medical treatments, results that successfully prolong 
human lives. One of major evolutions on software is the digitalization process of 
personal health record. The digitalized record, which is formally named Electronic 
Health Record (EHR), can thereby be shared easier among hospitals via internet. In 
order to make the EHR sharing mechanism feasible, major issues to be considered are 
twofold. The former one is how to digitalize personal health record to a standard form 
which must be recognizable by all hospitals, while the latter is regarding on how to 
share them among hospitals. For such a goal, we propose Medicare-Grid — a grid 
based E-health system addressing the above issues, facilitating the sharing and 
exchange of digitalized personal health record among hospitals. 
using grid technology; currently, over one thousand hospitals have joined to this 
project. IBM cooperates with University of Pennsylvania to integrate computing 
resources provided by each hospital and form a computational grid environment, so 
that all participating hospitals can easily utilize remote resources and share medical 
data. 
3   Medicare-Grid Platform 
Medicare-Grid platform can be divided by three components, namely computing grid, 
data grid and EHR management system. Computing grid platform provide the 
computational cycles needed to the execution of Medicare related applications, while 
Data grid provides a virtual data storage system to support EHR sharing, which 
management system handles the EHR format translation among specialized HIS and 
standard TMT format and to provide user friendly web interface for users to operate 
the system. In subsections that follow next, we will describe detailed system design of 
these components. 
3.1   Computing Grid 
A grid platform is an aggregation of geographically distributed resources that working 
all together over the Internet as a vast virtual computing environment [6, 7, 8]. The 
main task of computing grid is resource brokering to optimize a global schedule for 
requested grid jobs matching and selecting suitable and available requested resources. 
With a resource broker, users are insulated from the grid middleware, thus avoiding 
communicative burdens between users and resources.  
A workflow-based grid resource broker is presented whose main function is to 
match available resources with user requests. Also, the broker solves the job 
dependency problem by sorting topologically and then execution of workflows. In 
order to deal with communication-intensive applications, the broker considers 
network information statuses during matchmaking and allocates the appropriate 
resources, thus speeding up execution and raising throughputs. In Fig. 1, the 
architecture of Resource Broker system and component relationships are presented, 
including also functions of each component are listed in the relation link. As Grid 
Portals allow easy access to the system [9, 10], a schematic diagram of the complete 
Workflow System is shown in Fig. 2. 
The achievements are described as follows. First, we construct a computational 
grid platform using Globus toolkit, distributed in 5 different locations (universities). 
Second, we have designed and implemented a resource broker which main function is 
to match available resources according to users’ needs. Finally, we provide a uniform 
graphic user interface (GUI) to use Medicare-Grid platform, to achieve automatic 
resource discovery and efficient available resource usage. Indeed, this supports grid 
users to submit their jobs to the suitable grid resources without knowing in advance 
any information on available resources. 
1. File management module: responsible for file operations, such as file insertion, 
file retrieve, file recovery, replicate and cache from a storage peer, 
2. Intra-overlay module: to provide functions to locate peer in local group using 
Chord architecture, 
3. Inter-overlay module: to provide API for communication need among groups. 
A novel file replication mechanism is proposed, different from existing replication 
mechanisms such as PAST [11], OceanStore [12], and Freenet [13] that rely on global 
information of system. We made use of levels to control the degree of replication in 
our system. Peers that originally hold a file is skipped by replication level will hold a 
simple indicator to the peer which really holds such file. 
In order to evaluate performance of the proposed system and its potential, we have 
implemented and deployed such proposal on Taiwan Unigrid [14] to perform large 
scale experiments. These experiments have been executed on the storage system 
located in 9 geographically distributed sites with total of 42 servers, as shown in Fig. 
3(a). During the experimentation process, we selected top 10 download files from 
SourceForge.net as the source of testing data, and three different group bound 
network bandwidth {1000kbps, 100kbps, 10kbps} to cluster storage nodes. 
Fig. 3(b), 3(c) and 3(d) show that we have successfully cluster some closer located 
peers under specific group bound. In the experiment, we noticed that the average 
measured bandwidth time between the newly coming peer and the measured target is 
less than 10 ms, and the average locate operation is less than one second. 
 
Fig. 3. (a) Server location in Taiwan Unigrid testbed, (b), (c) and (d) Server group result with 
different group bound 1000, 100, 10kbps. 
physicians do not needed to learn how to obtain EHR from other hospitals; instead, in 
their perspective, all the patient’s health records can just be read from its own hospital. 
As listed in early this subsection, we address the third issue by developing a web 
portal interface, in order to demonstrate our prototype. This web-based portal 
integrates all enabled functions provided by computing grid, data grid, and EHR 
sharing mechanism and able to connect between hospital and Medicare-Grid platform 
to exchange the EHR data, as shown in Fig. 5. 
 
Fig. 4. The EHR sharing mechanism. 
 
Fig. 5. The web portal interface with enabled functions of Medicare-Grid platform. 
4   Medicare Applications 
In this section, we present medical related applications developed in this project, that 
include (1) a data warehouse for medical decision support system, (2) a RFID based 
mobile monitoring system to precisely identify people or items, and (3) a wearable 
physiological signal measurement system that periodically monitor the health 
condition of patients. In subsections that follow next, we describe details on the 
design of these applications. 
research the relation of a specific SNP in a specific race between the cardiovascular 
diseases, and these data can also help us design the microarray experiment. 
By now, the prototype system has 34 cardiovascular diseases and their genetic data. 
Each disease has alternative splicing form graph, protein-protein interaction graph and 
related gene list, and haplotype data. In addition, the number of all CVD related genes 
is 480 and the number of CVD related tag SNPs is 79621. 
Table 1.  The list of cardiovascular diseases we provided and their number of related genes. 
Disease Name Gene Number 
Aortic aneurysm 48 
Arrhythmogenic right ventricular cardiomyopathy 22 
Arterial thromboembolic disease 13 
Ascending aortic disease 28 
Atherosclerotic vascular disease 48 
Brugada syndrome 6 
Cardiac amyloidosis 9 
Cardiomyopathy familial restrictive 26 
Carney complex 21 
Carnitine palmitoyltransferase II deficiency, late-onset form 2 
Cerebral amyloid angiopathy 26 
Congenital sick sinus syndrome 6 
Coronary disease 212 
Digeorge syndrome 79 
Dilated cardiomyopathy 122 
Familial hypercholesterolemia 76 
Familial hypertrophic cardiomyopathy 78 
Infantile dilated cardiomyopathy 18 
Insulin resistance-related hypertension 21 
Jervell and Lange-Nielsen syndrome 3 
Myocardial infarction 146 
Naxos disease 4 
Orthostatic hypotension 31 
Polymorphic ventricular tachycardia 21 
Venous thrombosis 40 
Ventricular tachycardia 46 
Watson syndrome 156 
Williams syndrome 514  
4.2   Mobile Intelligence System 
The Mobile Intelligence System (MIS) establishes an active RFID environment 
comprising various components and approaches for context acquisition of individuals, 
environment’s variables and their associated values. In MIS, RFID-based localization, 
tracking and monitoring techniques were developed for enhancing context acquisition 
in medical-care environments. Among these functions, localization is the most 
important component in MIS and serves as the key technology for developing mobile 
intelligence services. The localization system termed as Real-Time Location System 
(RTLS), employs active RFID technologies and has three major components as shown 
process vital signals such as ECG, body temperature, etc. Measured bio-signal data 
are transmitted via Bluetooth technology to the “Mobile Medical Information 
Processing Module”, like PDA or Notebook, for further processing and analysis. All 
physiological measurement results can be sent to “Remote Mobile Medical 
Information Processing Module”, which are e-health PC workstations in the health 
care centers through GSM or internet/wireless networks when connection is possible. 
The e-Texcare system contains electrocardiogram (ECG/EKG), heart rate (HR), 
respiration rate, body temperature, and falling detection unit. Therefore, functions as 
the lethal arrhythmias monitoring, continuous examination of cardiovascular and 
cardiopulmonary functions, respiration activities, and falling detection are achieved.  
 
Fig. 7. A prototype of e-Texcare®  wearable multi-functional physiological measurement 
system. 
5   Conclusion Remarks 
In this paper, we integrate grid and peer-to-peer technologies to build up a high-
performance computing and storage environment as underlying backbone and 
proposed an EHR sharing mechanism based on this backbone to form a Medicare-
Grid platform. As prototype, we closely collaborated with Taichung Veterans General 
Hospital, Taiwan who kindly provides us EHR sampling data for experiment purposes. 
These sampling data are then translated to standard TMT format and stored on 
Medicare-Grid platform. 
In this platform, we have developed three Medicare applications: Medical Decision 
Support system, which provides analysis of cardiovascular diseases and its genetic 
data. Each disease has the alternative splicing form graph, protein-protein interaction 
graph and related gene list and haplotype data; RFID-based localization, tracking and 
monitoring techniques were developed for enhancing context acquisition in medical-
care environments, and finally, the development of e-Texcare health care system and 
a wearable multi-functional physiological measurement system, to demonstrate that 
the lethal arrhythmias monitoring, continuous examination of cardiovascular and 
cardiopulmonary functions, respiration activities, and falling detection are achievable. 
 
Acknowledgment. This paper is based upon work supported by National Science 
Council (NSC), Taiwan, under grants no. NSC97-3114-E-007-001- and NSC98-2218-
E-007-005-.  
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
▓ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱 普及健康服務格網：以格網
為基礎之個人健康服務系統 
計 畫 編 號 
99-2218-E-007-001 
報 告 人 
姓 名 
許慶賢 服 務 機 構 
及 職 稱 
中華大學資工系 教授 
會議/訪問時間 
 地點 
26-29, May, 2011 
Busan, Korea 
會 議 名 稱 The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications 
發表論文題目 Dynamic Voltage Adjustment for Energy Efficient Scheduling on Multi-Core Systems 
 
一、主要任務摘要（五十字以內） 
    ISPA 是平行分散計算研究領域一個大型的研討會。這一次參與ISPA 2011
的工作包括，主持一場專題演講，以及發表最新的研究成果。此外，在會場上
也看到許多新的研究成果與方向。值得一提的是，這一次開會認識多位國外的
教授，並談及未來有可能合作的項目。個人認為收獲頗多。 
 
二、對計畫之效益（一百字以內） 
    這一次參與 ISPA 2011除了發表我們在此一計劃最新的研究成果以外，
也在會場中，向多位國內外學者解釋我們的研究內容，彼此交換研究心得。
除了讓別的團隊知道我們的研究方向與成果，我們也可以學習他人的研究經
驗。藉此，加強國際合作，提升我們的研究質量。 
 
三、經過 
    這一次在 Busan所舉行的國際學術研討會議共計三天。第一天本人參與
大多數的論文發表場次，並且聽取由 Prof. Kai Hwang (University of Southern 
California)的演講- Research Frontiers in The Clouds, Many-Core and Internet of 
Things。當天晚上參與大會安排的接待會。第二天，本人除了參與多個論文
發表的場次，並在下午主持 Prof. Jianhua Ma (Hosei University)的專題演講- 
Cyber-Individual: Visions and Challenges。當天也見到許多位台灣的教授，彼
此互相認識。晚上參加晚宴，認識近十位國外教授，並且與幾位國外學者及
中國、香港教授交換意見，合影留念。第三天，本人親自發表我們這一次的
論文。也參與第三天全部的大會議程。本人主要聽取 Grid/Cloud相關研究，
同時獲悉許多新興起的研究主題，並了解目前國外大多數學者主要的研究方
向，並且把握最後一天的機會與國外的教授認識，希望能夠讓他們加深對台
灣研究的印象。三天下來，本人聽了許多優秀的論文發表。這些研究所涵蓋
的主題包含有：Cloud Computing, Parallel and Distributed Computing, Pervasive 
Computing, Wireless Sensor Network, Internet of Things, Smart Environments, 
Dynamic Voltage Adjustment for Energy Efficient Scheduling on 
Multi-Core Systems 
Ching-Hsien Hsu
1
, Shih-Chang Chen
2
 and Chao-Tung Yang
3
 
1
Department of Computer Science and Information Engineering, Chung Hua University, 
Hsinchu, Taiwan 30012, R.O.C. 
chh@chu.edu.tw 
2
Department of Computer Science, National Tsing Hua University, 
Hsinchu, Taiwan 30013, R.O.C. 
cszthomas@gmail.com 
 
3
Department of Computer Science and Information Engineering, Tunghai University, 
Taichung, Taiwan 40704, R.O.C. 
ctyang@thu.edu.tw 
 
Abstract 
Nowadays, energy saving has become one of the 
most critical issues in IT.  In this paper, we propose an  
energy-efficient method, message classifying and 
voltage control (MCVC), for scheduling inter-
processor communications on multi-core processors.  
The proposed technique has two new elements in 
scheduling and energy saving technologies.  Firstly, the 
MCVC suggests a contention-free scheduling method 
that considers network transmission latency to avoid 
synchronization delay.  For energy saving, according to 
message sizes, the MCVC suggests different voltage 
level for dynamically adjusting CPU speed to reduce 
energy consumption of idle processors.  The simulation 
results show that MCVC has noticeable improvements 
on communication cost and energy usage.  
 
Keyword: energy saving, communication scheduling, 
data redistribution, multi-core. 
1 Introduction 
The advance of CPU technologies brings multi-
core architectures to realize higher density and 
heterogeneous cluster and grid computing.  Instead of 
raising the frequency of CPU, multi-core architecture 
provides range of solutions for parallel applications.  
One of the most common operations on multi-core 
systems is data redistribution.  Data redistribution is a 
problem of redistributing data with irregular segments 
between nodes to decrease the penalty of 
communication cost during runtime.   There are many 
research for efficient scheduling of data redistribution 
on distributed memory multi-computers.  However, 
multi-core CPU based computing systems is different 
from the single-core based clusters / grids.   
Another issue barely discussed for data 
redistribution is power saving technique.  The pursuit 
of high performance for scientific programs was 
always the goal for researchers to develop algorithms 
to find the perfect schedules.  This idea, however, 
accompanies with large power consumption.  To 
decrease the usage of power and not drop the 
performance was not under consideration of present 
papers.  Therefore, an efficient scheduling algorithm is 
proposed with a power saving technique for data 
redistribution to keep an eye on both performance 
efficiency and energy consumption on multi-core 
based systems in this paper. 
The basic idea of the proposed scheduling 
algorithm is to classify the different types of messages 
by identifying the different bandwidth in the whole 
system.  For performing data redistribution, a node 
represented by a core may be a member of Quad-Core 
or Dual-Core CPU in a physical machine in the multi-
core based systems.  The irregular data segments sent 
from node to node can be with different transmitting 
rate.  Therefore, messages which represent the 
transmitted uneven data segments should be classified 
in different categories to amplify the fact.  Another 
idea to improve the schedules with less contention 
while performing redistribution is to isolate a specific 
or more clusters, which are interconnected through 
infrastructure networks such as local area network or 
Internet. Each cluster has different number of multi-
core machines.  In such a system, applications running 
on multiple processors might incur communication 
messages which may be classified into four types.   
 Type 1: data copying within a core processor.   
 Type 2: A message is sent between different 
cores in the same machine. 
 Type 3: A message is sent between different 
machines in the same cluster. 
 Type 4: A message is sent between different 
clusters within the grid system. 
 
4.2 The Scheduling Problem  
 
An example of the scheduling problem is given in 
Figure 2.  There are 13 messages to be transmitted 
among six processors.  The numbers associated with 
each message represents message size.  To avoid node 
contention, it is assumed that messages from the same 
sending processor cannot be scheduled in the same 
scheduling step.  Similarly, a receiving processor 
cannot receive two or more messages at the same 
scheduling step.  To deal with the communication 
scheduling, we propose a message classifying with 
voltage control (MCVC) method.  The MCVC has two 
phases.  The first phase deals with type 1 and type 2 
messages and scheduling these messages in one step.  
Since type 1 and type 2 messages are local data copy 
operation, there is no conflict among these messages.  
One thing worthy to mention is that local data copy is 
much faster than inter-processor communication. Thus 
the actual cost of this step could be reduced according 
to different hardware architecture.  Such adaption will 
be explained with an example in next section.  The 
second phase of the MCVC is to schedule the rest of 
messages using a two-phase degree reduction 
scheduling algorithm (TPDR) which is our previous 
work [1].  Before applying TPDR, messages of type 3 
and type 4 should be enlarged by multiplying a 
constant according to the underlying network 
architecture.  This is because inter-processor or inter-
cluster communication are with higher transmission 
latency.  An example will be demonstrated in the 
following section. 
Let Degreemax represents the maximal number of 
messages to be sent from a sending processor or 
received by a receiving processor.  An abstracted 
description of the TPDR method is given below.   
1. If Degreemax > 2, scheduling smallest cost of 
messages which are to be sent from or received 
by the nodes with Degreemax. 
2. Degreemax = Degreemax  1.  If Degreemax > 2, 
repeat (1). 
3. If Degreemax  2, using coloring theory to 
schedule the remaining messages into two steps. 
 
 
SP0 
SP3 
SP1 
SP2 
SP4 
SP5 
DP0 
DP3 
DP1 
DP2 
DP4 
DP5 
8 
28 
29 
2 
21 
30 
20 
25 
8 
22 
26 
8 
m11 
m10 
m9 
m8 
m7 
m6 
m5 
m4 
m3 
m2 
m1 
(12) 
(14) 
(8) 
(8) 
(2) 
(12) 
(8) 
(9) 
(16) 
(12) 
(8) 
Cluster  1 
 
Cluster  2 
 
 
Quad-Core PC1 
Dual-Core PC2 
Single-Core PC3 SP6 DP6 16 25 
m12 
(16) 
(9) 
m13 
 
 
Figure 2: The computing environment of multi-core 
machines in two clusters in gird. 
 
4.3 Voltage Classification 
 
As mentioned earlier, Quad-Core or Dual-Core 
architectures allow CPUs change voltage of cores for 
various purposes including power saving.  A 
consequent optimization of the MCVC is to provide a 
set of given values to suggest better voltage of each 
core for different communication steps.  In a 
scheduling step, data size of messages are often 
different from each other.  The communication cost of 
a scheduling step is usually dominated by the message 
with largest data size.  Although some nodes may 
complete their transmissions, the dominator is still on 
the way.  This results idle time of most processors.   
Another observation is that voltage of the core 
playing as a dominator and the other cores are always 
the same.  This means that once a node finished its 
transmission before the dominator completion, it still 
works with the same voltage during idle time.  
Considering about the waste of idle time and work 
voltages for the cores, the MCVC suggests lower 
voltage values by giving a set of parameters.  This 
operation reduces the voltage and extends the 
transmission time of the non-dominators.   However, 
the extended transmission time will not be larger than 
5 Evaluation of Performance  
To evaluate the performance of MCVC, we implement 
MCVC along with TPDR.  Following is the setting of 
the evaluation: 
 
 The total data size is 10,000 data units. 
 The numbers of nodes are 16, 32, 64 and 128. 
 There are eight and 16 nodes in each cluster for 
two sets of evaluations. 
 The range of data size for nodes is from 1 unit to 
8*(10,000/number of nodes) units. 
 1000 test samples for each set of comparisons. 
 
Figure 5 shows the results of comparison on 
various numbers of nodes while there are eight nodes 
in each cluster.  There are 4,000 cases examined over 
16, 32, 64 and 128 nodes respectively.  With high 
irregularity of data distribution among nodes, MCVC 
wins about 75% cases on 16 nodes, and over 95% 
cases on 32, 64, 128 nodes.  The results show that 
MCVC outperforms its competitor in the evaluation 
and proofs the importance to isolate the type 1 
messages.  This first phrase of first operation arranges 
type 1 messages together in a scheduling step.  The 
motivation is to avoid node contention while gathering 
messages with large data size together if possible. 
Figure 6 shows the results of comparison between 
MCVC and TPDR while there are 16 nodes in each 
cluster.  The results are similar to Figure 5, MCVC win 
most cases on various set of nodes.  Even there is no 
type 4 message among nodes in evaluation on 16 nodes, 
MCVC can still wins about 70 % cases. 
 
Comparison of MCVC and TPDR
with eight nodes in each cluster
0
100
200
300
400
500
600
700
800
900
1000
16 32 64 128
Number of Nodes
N
u
m
b
er
 o
f 
C
as
es
MCVC
TPDR
Same
 
Figure 5: Comparisons of MCVC and TPDR with 
eight nodes in each cluster. 
 
Figures 7 and 8 illustrate the improvement on the 
scheduling results of MCVC to show the performance 
of second operation.  Figures 7 and 8 give the results of 
improvement on voltage usages while performing the 
schedules derived by MCVC in Figures 5 and 6.  The 
voltage control mechanism of MCVC helps reduce over 
50% energy usage for various numbers of nodes.   
 
Comparison of MCVC and TPDR
 with 16 nodes in each cluster
0
100
200
300
400
500
600
700
800
900
1000
16 32 64 128
Number of Nodes
N
u
m
b
er
 o
f 
C
as
es
MCVC
TPDR
Same
 
Figure 6: Comparisons of MCVC and TPDR with 16 
nodes in each cluster. 
 
Improvement on power consumption with
eight nodes in cluster
0
10
20
30
40
50
60
16 32 64 128
Number of Nodes
E
n
e
rg
y
 (
1
0
^
6
 U
n
it
s)
Original
Improved
 
Figure 7: Improvement of MCVC on power 
consumption with eight nodes in each cluster. 
 
Improvement on power consumption with
16 nodes in cluster
0
10
20
30
40
50
1 2 3 4
Number of Nodes
E
n
er
g
y
 (
1
0
^
6
 U
n
it
s)
Original
Improved
 
Figure 8: Improvement of MCVC on power 
consumption with 16 nodes in each cluster. 
6. Conclusions 
This paper presents a message classifying and voltage 
control scheduling (MCVC) technique to reduce 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
□ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱  計 畫 編 號  
報 告 人 
姓 名 
 周嘉政 服 務 機 構 
及 職 稱 
 清華大學資工系 
 博士生 
會議/訪問時間 
 地點 
 May 16-20, 2011 
 Anchorage, Alaska, USA 
會 議 名 稱  IPDPS 2011 (The 25th IEEE International Parallel and Distributed Processing Symposium) 
發表論文題目  
 
一、主要任務摘要（五十字以內） 
 
二、經過 
 
三、心得 
 
四、建議與結語 
 
五、攜回資料 
 
 
   
由於電腦硬體不斷的快速發展，以及新興高速網路架構的出現，平行與分散處理一
直是很熱門的研究主題，而相關的研究應用更是不勝枚舉。今年在此研討會中出現的相
關主題包含了資源管理，傳輸與 I/O最佳化，GPU加速運算，排程，分散式系統，數值
分析演算法，編譯器，分散式演算法與模型，平行演算法，儲存系統與記憶體，容錯，
資源利用率，雲端計算，作業系統與資源管理，無線感測網路，多重與平行處理，動態
執行系統，繞徑與傳輸等。其中值得注意的是，隨著 GPU平行處理能力大幅提升，相關
研究與應用也益發蓬勃。 
在電腦及網路技術日益進步之際，也連帶著引發出許多新的研究議題，如程式的平
行化，利用 GPU運算資料以加速程式執行，雲端計算與資源利用等等。由此可見，平行
與分散處在未來的數年之內還是有許多研究題目有待探索。 
四、建議與結語 
參與國際會議可以讓國內的研究人員了解目前國際上相關研究的發展現況與未來趨
勢，也可以拓展其國際視野，對於國內的研究人員幫助相當大，因此國內研究人員應該
積極參與大型的國際會議。 
另外，若是國內學術單位有爭取到在國內舉辦大型國際會議的機會，不僅能促進國
際學術交流與合作，還能夠開拓國內的研究生的視野。 
六、攜回資料 
 無 
graph based on heuristics. The guest graphs may be
“squeezed” or “twisted” to fit the host graphs. Figure
1 gives an intuitive example to map from the 3 × 48
ring to the 12 × 12 grid. As shown in the figure, the
dashed blocks show the areas where the ring is folded.
With careful 90 and 180 degree “turns” (as shown in
the dashed blocks), the dilation cost can be controlled
under 2 in the host graph.
Figure 1. The embedding of the 3× 48 ring to the 12× 12 grid
The idea behind those folding heuristics assumes
the programming model solving the problem requires
frequent communications in the local region. Therefore
the subproblems(tasks) that are close to each other in
the problem domain should be mapped to compute
nodes close to each other on the host machine so the
communication will occur in the local neighborhood.
In order to apply the folding method, the decompo-
sition of the problem and the topology of the host
machine need to be known in advance prior to the
mapping. In addition, this method does not consider
the run-time communication situation and only static
communication information from the problem domain
is used as the basis for mapping. The communication
traffic pattern and the amount of messages being trans-
mitted among tasks are not considered to decide the
mapping. Finally, the computation cost of the folding
method is of O(n3) for n tasks to be mapped, which
is too expensive for large scale applications.
In this paper, we present a hierarchical mapping
algorithm (HMA), which can work for arbitrary com-
munication patterns of guest applications on regular
or irregular host topologies, and has better scalability
than the folding method. The idea is to partition tasks
into “supernodes” such that the tasks within the same
supernode communicate to each other more frequently
than the tasks from different supernodes. The com-
munication size between tasks is obtained from the
MPI trace collected during the run-time. After that,
the algorithm optimizes the mappings “inter supern-
odes” and “intra supernodes” separately. If the amount
of “intra supernode” communication is much more
than that of the “inter supernode”, the hierarchical
mapping algorithm is optimal. Furthermore, since we
can optimize mappings in different levels of hierarchy
separately, the computational cost can be significantly
reduced. For even larger applications, this approach can
be applied recursively to achieve good scalability.
The hierarchical mapping algorithm consists of three
steps: task partitioning, initial mapping, and fine tun-
ing. Task partitioning, based on the communication
information, utilizes the normalized cut algorithm in
the spectral graph theory, which partitions a graph ac-
cording to the “connectivity” defined by edge weights,
and can handle any irregular communication pattern
of tasks. The initial mapping assigns the supernodes
onto the processor blocks on the host machine. If the
topology of the host machine is also irregular, then
the normalized cut algorithm is used again to partition
the processing nodes based on their communication
latency. Otherwise, for host machines with a regular
topology, the heuristics (e.g., [23]) can be used to
construct the initial mapping of supernodes. The fine
tuning step employs optimization methods to improve
initial mapping with the desired objective function,
such as the dilation or the hop-byte.
We verified this idea on the IBM Blue Gene/P
system using the PDGEMM, a ScaLAPACK[12]-
based matrix multiplication computation kernel and
the AMG2006 [1] which is a tier 1 application of
the Sequoia benchmark [3]. The compute nodes on
Blue Gene/P are interconnected through five networks,
one of which connects the nearest neighbors into a
three-dimensional torus. The torus network has the
bandwidth of 5.1GB/s with 100 ns (32-byte packet) or
800 ns (256-byte packet) hardware latency for nearest
neighbor. The experiment results show the mapping
algorithm helps to reduce the point-to-point commu-
nication time for the PDGEMM up to 20% and the
AMG2006 up to 7%
The rest of the paper is organized as follows: Section
II reviews related work of mapping. Section III intro-
duces the hierarchical mapping algorithm with detailed
discussions on task partitioning, initial mapping and
ing clusters V into k groups, {A1, A2, . . . , Ak}. Each
Ai is called a supernode.
There are two objectives to optimize for the desired
partitioning. The first objective is the cohesion cri-
terion, which requires that the communication traffic
within a supernode is heavier than that between su-
pernodes. The purpose is to guarantee the quality of
the partitioning, since in the later steps, tasks in differ-
ent supernodes cannot be exchanged. Mathematically,
the coherence criterion is equivalent to maximize the
function
p({A1, A2, . . . , Ak}) =
k∑
i=1
ρ(Ai, Ai), (2)
where
ρ(A,B) =
∑
u∈A,v∈B
w(u, v). (3)
We call ρ(Ai, Ai) the cohesion of Ai, and
p({A1, A2, . . . , Ak}) the cohesion function of
the partition.
It can be shown that to maximize (2) is equivalent
to minimize
q({A1, A2, . . . , Ak}) =
k∑
i=1
ρ(Ai, V −Ai), (4)
since ρ(Ai, Ai) = ρ(Ai, V ) − ρ(Ai, V − Ai) and∑k
i=1 ρ(Ai, V ) = ρ(V, V ) is a constant. Many algo-
rithms were designed to minimize (4), such as the
network flow algorithm[13]. We define ρ(Ai, V −Ai)
as the coupling of Ai.
The second objective is called the equality criterion,
which means the size of each Ai should be as equal as
possible such that computational cost in the following
steps can be minimized. We can integrate this criterion
to the first one by adding constrains |Ai| = n/k to (4).
But this constrained optimization problem will be very
difficult to solve [22]. Alternatively, we can employ a
different objective function, such as
faverage({A1, A2, . . . , Ak}) =
k∑
i=1
ρ(Ai, V −Ai)
|Ai| ,
(5)
or
fnormalized({A1, A2, . . . , Ak}) =
k∑
i=1
ρ(Ai, V −Ai)
ρ(Ai, V )
.
(6)
The function faverage is called the average cut [15],
which measures the average coupling of Ai. Similarly,
the function fnormalized is called the normalized cut,
which computes the fraction of the coupling of Ai
of all the edges adjacent to the nodes in Ai. Both
objective functions are designed to balance the size of
supernodes. Thus, if one of the supernodes is too small,
the ratio
ρ(Ai,V−Ai)
|Ai|
or
ρ(Ai,V−Ai)
ρ(Ai,V )
will be magnified,
unless its coupling is small.
Both the minimum average cut problem and the min-
imum normalized cut problem are NP-hard. However,
the spectral graph theory provides descent approxima-
tions. Let W be the matrix whose element (i, j) is
w(i, j), and D be a diagonal matrix where
D(i, i) =
n∑
k=1
w(i, k). (7)
The Laplacian matrix is defined as
L = (D −W ); (8)
and the normalized Laplacian matrix is
LN = D
− 1
2 (D −W )D− 12 . (9)
If W is symmetric, it can be shown that L and LN
are symmetric positive semidefinite. Generally, the
spectral algorithm for the normalized cut performs
better than that of the average cut. Thus, we will use
the normalized cut in the task partitioning step.
The minimum normalized cut for k = 2 can be
solved as follows. Let λ2 be the smallest nonzero
eigenvalue of LN and z2 be the corresponding eigen-
vector, LNz2 = λ2z2. The vertices in V can be
partitioned according to a threshold δ and the elements
in z2, {
vi ∈ A1, if z2(i) > δ;
vi ∈ A2, if z2(i) ≤ δ. (10)
The threshold δ can be selected by observing the gaps
of elements in z2, or using the medium of z2 if no
obvious gaps exist.
For the partitioning of k > 2, the partitioning
algorithm can be applied recursively and a hierarchical
partitioning is generated. The partitions form a dendro-
gram, i.e. a tree diagram whose nodes are the partitions
produced by the hierarchical partitioning, which can be
used in the initial mapping to partition compute nodes
into processor blocks accordingly.
The major cost of the spectral graph based algo-
rithm is the computation of z2. When the matrix in
Various optimization techniques, such as the simplex
method, the local search method, or the simulated
annealing [17], can be used to improve the initial
mapping. Here we propose a new optimization scheme,
called two-phase method, which is suitable for the
hierarchical mapping structure.
The two-phase method requires the supernodes to
be equally partitioned. It consists of two phases: the
inter supernode optimization and intra supernode opti-
mization. The optimization algorithm in each phase is
arbitrary and independent. Here we use the local search
method as an example to illustrate the idea, since it is
simple and effective if a good initial guess is provided.
The local search algorithm is described in Algorithm
3. For n nodes, there are n(n−1)/2 pairs. If each node
has at most k links, the update step can be done in
O(k) time. Thus, the overall algorithm takes O(kn2)
time.
Algorithm 3 The local search algorithm for mapping
1) For each pair of nodes (u, v),
2) Exchange the placement of u and v.
3) Update the hop-byte metric for the new
placement.
4) If the new placement is better than the old
one,
5) Accept the new placement.
6) End If
7) End For
In the inter supernode optimization phase, each
supernode is the basic unit in the exchange step. In the
intra supernode optimization phase, only tasks within
a supernode can be exchanged. Suppose there are N
tasks in total, and m supernodes, so each supernode
has N/m tasks. We also assume that each task and
each node in the supernode graph has a bounded
degree. Then the inter supernode optimization takes
O(m2N
m
) time; and the intra supernode optimization
takes O((N
m
)2m) time. Therefore, the time complexity
for the two-phase method, with the local search algo-
rithm, is O(N(m+N/m)). When m =
√
N , the time
complexity is minimized O(N
√
N).
IV. PERFORMANCE EVALUATION
In this section, we describe the performance eval-
uation for the hierarchical mapping algorithm. Two
applications, PDGEMM and AMG2006, were exper-
imented on 4,096 nodes of the Blue Gene/P system
using SMP mode3. We discuss the impacts of different
configurations in the hierarchical mapping: supernodes
with different sizes, supernodes with multiple levels
and supernode with different geometries. The details
will be given in the following subsections.
Since the application can run with different simula-
tion length/iterations, we choose to show the results
using improvement percentage (as to the time mea-
surement) compared with the system default ordinal
mapping method. The formula of the improvement
(time reduction) percentage is
ρ =
Tdefault − THMA
Tdefault
, (12)
where Tdefault is the communication time by using the
system default mapping, and THMA is the communi-
cation time by using the mapping generated by the
hierarchical mapping algorithm.
A. Applications
1) PDGEMM: The first application is a driver pro-
gram of the subroutine PDGEMM in ScaLAPACK,
which implements the matrix-matrix multiplication. To
match the nature of the matrix algorithms, ScaLA-
PACK partitions a matrix A into blocks and and
distributes them to tasks arranged as a two-dimensional
rectangular grid in the cyclic order [12] ( according to
the row-major order or the column-major order). In the
test cases, a square grid is used. The communication
mechanisms used in ScaLAPACK are primarily point-
to-point MPI function calls. In PDGEMM, messages
are only sent to the neighbor tasks in the right and
in the bottom. For tasks at the boundary of right
and bottom, messages are sent to the left most tasks
in the same row, and to the top most tasks in the
same column respectively. This is so called periodic
boundary. The communication pattern of PDGEMM
using 1,024 nodes is shown in Figure 2, in which a
blue dot at (i, j) means there are messages sent from
node i to node j. The number nz = 2048 counts the
total number of blue dots.
When applying the normalized cut algorithm, as
shown in Algorithm 1, the 1,024 nodes, arranged in a
32×32 grid with row-major order, are partitioned into
two clusters, as displayed in Figure 3(a). The nodes
3One MPI task runs on one compute node
0 2 0 4 0 6 0 8 0 1 0 0 1 2 0
0
2 0
4 0
6 0
8 0
1 0 0
1 2 0
n z =
1 0 6 4
(a) 128 nodes
0 5 0 1 0 0 1 5 0 2 0 0 2 5 0
0
5 0
1 0 0
1 5 0
2 0 0
2 5 0
n z =
2 2 3 8
(b) 256 nodes
Figure 4. The Communication pattern of AMG2006
the effect of the fine tuning step is limited in theory,
since only nodes within a supernode can be exchanged.
Learning the impact of the supernode size to the
mapping quality is the purpose of the experiments.
For each application, we partitioned tasks into three
different supernode sizes: 8, 64, and 512, and used the
mappings obtained by HMA in real executions. The ex-
periment results are shown in Figure 6. For PDGEMM,
the correlation of supernode size and the quality of
mapping is clear, that is, the larger supernode, the
better performance result. For the size as small as 8,
0 100 200 300 400 500 600
−0.08
−0.06
−0.04
−0.02
0
0.02
0.04
0.06
0.08
 
 
cluster 1
cluster 2
Figure 5. The 512 elements of z2 of the normalized Laplacian
matrix for AMG2006.
the mapping by HMA increases the communication
time. This phenomenon can be caused by that the task
partitioning disrupts the task locality, which means task
nodes that should be placed in a neighborhood cannot
be mapped nearby.
For AMG2006, the size-quality correlation breaks
down for the supernode size 512. The possible explana-
tion is when we do the initial mapping, the default ordi-
nal mapping is used to arrange the tasks in a supernode.
However, for AMG2006, the default mapping for 512
nodes may not provide a good starting point for opti-
mization. When the fine tuning is applied to optimize
the mapping, the resulted mapping is trapped to a local
minimum and cannot be improved. This problem does
not occur to PDGEMM, because the default mapping
matches naturally to the regular communication pattern
of PDGEMM. Thus, the optimization in fine tuning has
a good starting point to further improve the quality of
the mapping.
C. Hierarchy
Here we want to study how the depth of the supern-
ode hierarchy affects the performance. As discussed
in Section III-C, deeper hierarchy may lower time
complexity of the fine tuning. However, it is also a
trade-off to the mapping quality, since more levels of
partitioning may restrict the optimization that can be
done in the fine tuning stage.
For each application, we constructed three hier-
archies: level 1, level 2, and level 3. The level 1
hierarchy partitions 4,096 tasks into 64 supernodes
with 64 tasks in each supernode. The level 2 hierarchy
links and the number of inter-(processor)block links.
For a supernode Ai, the accumulated message size
between intra/inter supernode (AMSIIi) is
AMSIIi =
ρ(Ai, Ai)
ρ(Ai, V −Ai) , (13)
where function ρ is defined in (3), and V denotes the
set of all tasks. The AAMSII is
AAMSII =
∑
Ai∈V AMSIIi
k
, (14)
where k is the number of supernodes.
IILR is defined on the processor block mapped by
a supernode. Let P be a processor block assigned to
a supernode, whose shape matches the supernode’s
geometry, and let eu,v represent a direct link connecting
node u and node v. Here we assume that all the
supernodes are assigned to homogeneous processor
blocks to V . The intra supernode links SLintra is
defined as
SLintra = |{ev,u|u ∈ P, v ∈ P}|; (15)
and the inter supernode links LSinter is
SLinter = |{ev,u|u ∈ P, v /∈ P or v ∈ P, u /∈ P}|.
(16)
Then the IILR is defined as
IILR =
SLintra
SLinter
. (17)
The AAMSII for AMG2006 is 2.06 and the IILR
for each geometry is drawn in blue dots (linked by
lines) in Figure 8. The y-axis on the left represents the
improvement percentage and the y-axis on the right
represents IILR. It can be clearly seen that geometries
with larger IILR (closer to 2.06) outperforms the
geometries with smaller IILR. In this case, geometry
4×4×4 is better than geometry 16×2×2 and others.
This result shows the supernode geometry does
affect the communication performance, and the effect
can be estimated by using AAMSII and IILR.
V. DISCUSSION
For the two applications in this study, if we apply
mapping method based on static information only such
as [23], the mapping layout will simply be the same
as the system default ordinal mapping. Since the run-
time information is not considered, the point-to-point
communication performance improvement with those
methods will be limited.
-3%
-2%
-1%
0%
1%
2%
3%
4%
5%
6%
7%
8%
16x4x1 16x2x2 8x8x1 4x4x4
Supernode Geometry
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Improvement
Intra/Inter Link Ratio
Figure 8. Different supernode geometries for partitioning
The limitation of our proposed hierarchical mapping
method is when the communication traffic on the
interconnection links cannot be identified with the
sender/receiver task information, the algorithm may not
be able to optimize the mapping. For example, when
the collective communication dominates the applica-
tion communication time and the collective communi-
cation may either use a separate physical interconnec-
tion or use communication protocols like copy-then-
forward mechanism, the communication traffic cannot
be identified with the sender/receiver task information.
The scenarios like above currently would prevent the
algorithm optimize the mapping. To solve this problem,
further information from the system software and the
MPI implementation will be needed.
The overhead when applying our proposed mapping
algorithm includes the communication pattern collec-
tion and the calculation based on the algorithm. The
overhead from the communication pattern collection
depends on the performance tool used. Typically it
requires additional one layer of function call to wrap
each MPI function call and the bookkeeping for times-
tamps. The algorithm complexity is given in Section
III which is currently done offline. If the mapping
is done during the run-time (e.g., remapping MPI
tasks between simulation iterations/steps), there will
be overhead to migrate the MPI tasks.
To further reduce the mapping overhead, parallel
eigen-solvers [12], [8] can be used in task partitioning
to reduce the partitioning cost. In addition, the fine
tuning may be done in parallel if supernodes can
be divided in to “independent sets” (i.e., there is no
or neglectable communication traffic between the two
sets). However, this may introduce extra overhead to
the fine tuning step.
[18] Rami G. Melhem and Ghil-Young Hwang. Embedding
rectangular grids into square grids with dilation two.
IEEE Trans. Comput., 39(12):1446–1455, 1990.
[19] Sangman Moh, Chansu Yu, Dongsoo Han, Hee Yong
Youn, and Ben Lee. Mapping strategies for switch-
based cluster systems of irregular topology. In 8th
IEEE International Conference on Parallel and Dis-
tributed Systems, Kyongju City, Korea, June 2001.
[20] Jesper Larsson Tra¨ff. Implementing the mpi process
topology mechanism. In Supercomputing, pages 1–14,
2002.
[21] H. Wen, S. Sbaraglia, S. Seelam, I. Chung, G. Cong,
and D. Klepacki. A productivity centered tools frame-
work for application performance tuning. In QEST ’07:
Proceedings of the Fourth International Conference on
the Quantitative Evaluation of Systems (QEST 2007),
pages 273–274, Washington, DC, USA, 2007. IEEE
Computer Society.
[22] R. Wu, Z.; Leahy. An optimal graph theoretic approach
to data clustering: theory and its application to image
segmentation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 15(11):1101–1113, Nov
1993.
[23] Hao Yu, I-Hsin Chung, and Jose Moreira. Topology
mapping for blue gene/l supercomputer. In SC ’06:
Proceedings of the 2006 ACM/IEEE conference on
Supercomputing, page 116, New York, NY, USA, 2006.
ACM.
 三、心得 
Cloud computing has become a hot research topic recently and are promised to keep hot 
for at least three to five years. Currently, my research direction is on parallel job scheduling, 
which has been widely studied over last ten years. Relatively, it is hard to find a new 
breakthrough on this topic. During this conference, I have discussed with many international 
researchers and learned many valuable information on the issue of cloud computing. This 
could serve as a good chance for me to think how to smoothly extend my research topic from 
traditional parallel job scheduling to cloud computing. 
四、建議與結語 
International conference provides a very good opportunity to make a connection from 
domestic researchers with international researchers. It is recommended to hold as more 
international conference as possible in Taiwan, to extend the research perspectives and 
dimensions for domestic researchers. 
五、攜回資料 
Conference Proceeding CD. 
 
The TLPA technique works as follows. For the job j to be 
executed, all allocable clusters need be evaluated by the 
scoring function, and the cluster with the best score is chosen 
to allocate the job for execution. 
TLPA_BJS is designed to cooperate with the basic job 
scheduling approaches. The scoring function of TLPA_BJS 
is shown in Fig. 1. 
 
 
Figure 1.  Scoring function of TLPA_BJS 
III. EXPERIMENTS AND DISCUSSIONS 
To show the effectiveness of TLPA, we compare 
TLPA_BJS with BF and FF under cooperation with FCFS 
and SJF. A series of simulations has been conducted using 
publicly downloadable workload trace named SDSC SP2 log 
[6]. Two variables, system loading (SL) and system 
heterogeneity (SH), were added to simulation parameters to 
increase the dimensions of comparison basis. SL changes the 
heaviness of the input workload while SH controls the 
variance of the computing speed among the clusters. All 
parameter settings used in the simulations are summarized in 
Table I. 
Table II shows the average performance improvement of 
TLPA_BJS with respect to BF and FF respectively. Each 
result is the average of all the combinations of three SL and 
three SH settings. The simulation depth with the best 
performance in each set of experiments is shown in red color 
and boldface. There are two observations. First, the results 
show that TLPA_BJS outperforms BF and FF for all 
simulation combinations in terms of ATT. Second, the 
results reveal a clear correlation between simulation depth 
and performance improvement, that is, the deeper simulate 
depth, the better performance improvement. 
IV. CONCLUSIONS 
This paper investigates the issues of processor allocation 
in CHMC and proposes the TLPA technique to improve 
system performance. Experimental results show that system 
performance can be improved up to 32.75% by using TLPA 
into processor allocation decision. 
TLPA provides a brand-new viewpoint to processor 
allocation. First, the allocation decision can be based on a 
performance metric other than simple policies. Second, the 
allocation decision is made based on simulation, not just 
some static rules. We anticipate further improvement can be 
made by utilizing those concepts in the design of new 
processor allocation algorithms. 
TABLE I.  ALL PARAMETER SETTINGS USED IN THE SIMULATIONS 
Number of clusters in CHMC 5 
Processors in each cluster 8, 128, 128, 128, 50 
Job scheduling algorithm FCFS, SJF 
Workload source SDSC’s SP2 log 
System loading (SL) Low, Medium, High 
Speed heterogeneity (SH) 0, 0.1, 0.2 
Simulation depth d 2, 4, 8, 16, 32, 64 
TABLE II.  AVERAGE PERFORMANCE IMPROVEMENT OF TLPA_BJS 
WITH RESPECT TO BF AND FF RESPECTIVELY 
Workload 
source 
Job 
scheduling 
algorithm 
Compared 
processor 
allocation 
algorithm 
Simulation 
depth 
Performance 
improvement of 
TLPA_BJS 
SDSC’s 
SP2 log 
FCFS 
BF 
2 1.84% 
4 3.59% 
8 11.79% 
16 20.41% 
32 25.21% 
64 30.97% 
FF 
2 10.28% 
4 12.73% 
8 18.20% 
16 24.20% 
32 27.89% 
64 32.75% 
SJF 
BF 
2 10.32% 
4 10.31% 
8 10.88% 
16 10.77% 
32 10.21% 
64 11.06% 
FF 
2 2.97% 
4 2.98% 
8 3.57% 
16 3.46% 
32 2.96% 
64 3.79% 
REFERENCES 
[1] D. England and J. B. Weissman, "Costs and Benefits of Load Sharing 
in the Computational Grid," Job Scheduling Strategies for Parallel 
Processing, ed, 2005, pp. 160-175. 
[2] O. Sonmez, H. Mohamed, and D. Epema, "On the Benefit of 
Processor Coallocation in Multicluster Grid Systems," IEEE 
Transactions on Parallel and Distributed Systems, vol. PP, pp. 1-1, 
2010. 
[3] K.-C. Huang, P.-C. Shih, and Y.-C. Chung, "Using Moldability to 
Improve Scheduling Performance of Parallel Jobs on Computational 
Grid," Advances in Grid and Pervasive Computing, ed, 2008, pp. 116-
127. 
[4] K.-C. Huang and H.-Y. Chang, "An Integrated Processor Allocation 
and Job Scheduling Approach to Workload Management on 
Computing Grid," Proceedings of the 2006 International Conference 
on Parallel and Distributed Processing Techniques and Applications 
(PDPTA'06), Las Vegas, USA, 2006, pp. 703-709. 
[5] K.-C. Huang, P.-C. Shih, and Y.-C. Chung, "Towards Feasible and 
Effective Load Sharing in a Heterogeneous Computational Grid," 
Advances in Grid and Pervasive Computing, ed, 2007, pp. 229-240. 
[6] Parallel Workloads Archive, 
http://www.cs.huji.ac.il/labs/parallel/workload/. 
Scoring Function of TLPA_BJS (j, c, d, p) 
I. Simulate allocating job j to cluster c, estimate the runtime of j, and 
calculate the score of j by using p. 
II. For i = 1 to d or until no jobs in the waiting queue. 
(a). Pick up a job i from the waiting queue using the job 
scheduling algorithm. 
(b). Find the earliest time that some cluster(s) C’ is able to 
accommodate job i 
(c). For each cluster k in C’, calculate the temporary score if 
allocating job i to cluster k using p. 
(d). Simulate allocating job i to the cluster with the best 
temporary score, and set the score of job i to the best 
temporary score. 
III. Output the average scores of all simulated jobs. 
 
四、心得 
此次大會共安排了在論文發表方面共分15 個Session 主題進行150篇論文發
表，綜合此次會議之重點共有: 
1.Computer Software Engineering and 
Information System Design  
Software Architectures  
Software Design and Development  
Software Testing  
Software Agents  
Web-based Software Engineering  
Project Management  
Software Performance Engineering  
Service Engineering  
Model-Driven Development  
Applications of DB Systems and Information Systems
 
  
2. Computer Network and Communication 
Technology 
Attacks and Prevention of Online Fraud  
Cryptographic Protocols and Functions  
Economics of Security and Privacy  
Identity and Trust Management  
Information Hiding and Watermarking  
Infrastructure Security  
Intrusion Detection, Tolerance and Prevention 
Network and Wireless Network Security  
Trusted Computing  
Adaptive Modulation and Coding  
Channel Capacity and Channel Coding  
CDMA and Spread Spectrum  
3. Computer Simulation and Modeling  
Simulation Tools and Languages  
Discrete Event Simulation  
Object-Oriented Implementation  
Web-based Simulation  
Monte Carlo Simulation  
Distributed Simulation  
Simulation Optimization  
Numerical Methods  
Mathematical Modelling  
Agent-based Modelling  
Dynamic Modelling  
Continuous and Discrete Methodologies  
Time Series Analysis  
Complex Systems Modeling and Simulation  
Economics and Finance Modeling  
4. Computer Automation Control  
Micro-computer Embedded Control 
Applications  
Process Control and Automation  
Sensors and Applications  
Industrial Process Control  
Decision Support Systems  
Fuzzy Control and Its Applications  
Cybernetics for Informatics  
Industrial Bus Control Applications  
Measurement and Diagnosis Systems  
Digital System and Logic Design  
Circuits and Systems  
 
 
 
 
 
 1
Using Data Mining Technique to Improve the Manufacturing Yield of LCD Industry 
Ruey‐Shun Chen     
Department of Information 
Management, China 
University of Technology,  
Hsinchu, Taiwan 
rschen@cute.edu.tw 
 
 
Y.C. Chen 
Department of Information 
Enginerring, National cheng 
Kung University ,  
Tainan, Taiwan 
Dvd000001@gmail.com 
 
 
C.C. Chen   
Department of Information 
Management, Tung Hai 
University ,  
Taichung, Taiwan 
emily@thu.edu.tw 
Abstract‐This paper applies the data mining 
technique to the TFT-LCD industry. First, the 
Information Gain of each attribute data was 
computed. Then, the data mining engine is used to 
further analyze the largest value. The engine was 
built based on Association Model with a 
multi-dimension Cube to store information such as 
machines and product quantity that pass through 
each station, Once the data is uploaded to the Data 
Warehouse, the next step is to find the association 
rule, generated by the algorithm method, between 
different stations and machines and compute its 
confidence level.  
This paper proposed an effective data mining 
methodology to be used in the Array 
manufacturing process. The result shows the yield 
has increased, the process time has decreased. The 
stability of the manufacturing process has vastly 
been improved.  
 
Keywords-Data mining, LCD, manufacturing 
process, Association rule. 
 
I. Introduction 
The LCD industry has been regarded as 
the “second semiconductor industry" in Taiwan. 
Originally, it was Jin Ye Electron Inc. who had 
introduced the TN- LCD technology from the 
United States and started the first TN-LCD 
production line in Taiwan. Subsequently in 1997, 
CPT  had  attained  the  LCD production 
know-how from ADI, a company invested by 
Japanese Mitsubishi Dynamo group. Two years 
later, seven production lines were set up. The 
TFT-LCD will succeed semiconductors 
industries become the next gem in Taiwan. 
Improving yield by statistics and 
experimental method such as yield rate models 
or yield rate simulation all requires very precise 
statistics techniques. Unfortunately, when it 
comes to complex interactions and non-linear 
attributes, the function of those traditional 
analyzing methods is limited. Moreover, it is 
impractical to manually retrieving useful data for 
decision making from the manufacturing 
database. Usually hundreds of attributes are 
needed to establish system behavior patterns. 
Thus, the data search technique can effectively 
analyze original and huge data from database as 
the basis for improving the yield rate.  
The purpose of the present investigation is 
to put forward a valid data mining methodology 
for Array production. By identifying the fault 
and improving the yield, the enterprises can 
reduce the production cost, achieve higher 
order-meeting rates and increase their 
competitiveness.  
 
II. LCD and Data Mining 
A. LCD 
LCD manufacturing requires processes 
similar to semiconductors, such as lithography, 
etching, ashing and ion doping. It also requires a 
class 1 to class 1,000 clean room environments 
similar to that for semi- conductors. The 
difference would bethe number of steps that it 
takes to manufacture LCDs, which is usually 
below 40 compared to around 230 steps for 64M 
DRAMs.  
The TFT array substrate is transformed from a 
bare sheet of glass into the nervous system of the 
display in a rigorous, intricate, highly sensitive 
process that includes a series of application and 
reduction steps involving chemicals, gases and 
heat. These steps can be repeated five to ten 
times, depending upon the design of the process. 
Frequently, the machines performing these steps 
are arranged in a cluster-tool configuration to 
improve efficiency between process steps due to 
varying process times and maintenance 
schedules for each step. Another popular 
configuration of equipment is known as in-line. 
 
B. Data Mining Task  
   Most data mining goals fall into the 
following main categories [2], [4], [6]: 
a. Data Processing: Depending on the goals and 
requirements of the KDD process, analysts 
may select, filter, aggregate, sample, clean 
and/or transform data. 
b. Prediction: Given a data item and a predictive 
model, predict the value for a specific 
attribute of the data item. 
c. Regression: Given a set of data items, analyze 
the dependency of some attribute values upon 
the values of other attributes in the same item, 
and the automatic production of a model that 
can predict these attribute values for new data.  
d. Associations: Given a set of data items, 
identify relationships between attributes and 
items such as the presence of one pattern 
 3
group, mask type, or serial number as the 
variables and the rest are kept as constants to 
find the combination with higher defect rate. 
(i, j, k) represents three variables. At this 
stage, under the (i, 0, 0), (0, j, 0), (0, 0, k), the 
combination creating the highest defect rate is 
identified along with its Support and Confidence 
values. 
Step 4: Using machine group, mask type, and 
serial number as variables to find the 
combination with higher defect rate. 
Step 5: The Support and Confidence values from  
 
4. Application and Analysis 
    A real case data is applied to the 
developed algorithm for discussion purpose. 
The encoded data was provided by the  LCD 
manufactories. 
A. Application of Algorithm: Using LCD as 
an example 
    The sources of data are CIM system 
information and defect analysis of LCD 
manufactories. The information includes 
production information of each station, for 
instance, the yield, defect rate, serial numbers, 
defect type, defect causes, production start and 
ending time …etc.  
Step 1: Original data is stored according to data 
model defined by Cube in data warehouse  
The original data from Data Warehouse 
after transformation and calculation. Data below 
90% yield is selected and analyzed with the data 
of S-Open cause in line defect group. 
Step 2: One item is selected from machine group, 
mask type, or serial number as the variable and 
the rest are kept as constants to find the 
combination with a higher defect rate. 
    Combinations of yield below 90% are 
identified. Among all, the mask type 3 with No.1 
machine in the first machine group shows the 
highest possibility to cause high defect rate. 
Step 3: Two items are selected from machine 
group, mask type, or serial number as the 
variables and the rest are kept as constants to 
find the combination with higher defect rate. 
Support (machine group 3, mask type 3 with 
low yield) =198/784=25.26% 
Confidence (machine group 3, mask type 3 
with low yield) = 198/215 = 92.09%  
Step 4: Machine group, mask type, and serial 
number are selected as the variables to find the 
combination with higher defect rate. 
Step 5: The Support and Confidence from  
The Support and Confidence listed above 
surpass other combinations. However, each 
machine group is indispensable in the five 
manufacturing procedures.  
Step 6: Reports 
With analysis on each defect, Support and 
Confidence are computed with the proposed 
Data Mining technique to obtain valid 
information. 
                           
B.  Benefit Analysis 
(1) Improve the yield of Array manufacturing 
process 
Taking an example, the test number is the 
sum of all panels tested during that week. The 
data is automatically recorded by the testing 
facility. The number of good items is the number 
of qualified panels. By dividing the number of 
qualified panels  by the total number of tested 
panels, the yield should be 90.7%， 
After improvement has been carried out, 
while yield increases from 90.7% to 95.3%. 
Based on those facts, it can be concluded that 
CIM data mining system can effectively improve 
the yield, where the improvement margin is 
4.6%  
5. Conclusion 
This paper proposed that by recording the 
defects occurred during the manufacturing 
process and combining it with Array data, The 
research conclusions are as follows: 
a. A data mining system is established for LCD 
industry. 
b. The Information Gain is calculated by 
using attribute data. Further analysis is 
conducted using the data mining engine 
on the highest value.  
c. The data mining system improve the yield by 
4.6%.  
This paper has applied actual data to the 
data mining technique and conduct experiments 
to prove that the logarithm proposed is feasible. 
The yield has indeed increased 4.6%,. The 
overall manufacturing stability has substantially 
improved. The system can actively monitor the 
manufacturing process and notify engineers via 
Notes system or beepers to effectively improve 
the process and yield. 
 
Reference 
(1) Jiawei Han, Micheline Kamber, “Data Mining 
Concepts and Techniques”, 2001. 
(2) Ming-Syan Chen, Jiawei Han, and Philip S. Yu, 
“Data Mining: An Overview from a Database 
Perspective,” IEEE Transactions on Knowledge 
and Data Engineering, Volume 8 No. 6, 
pp.866-883,1996 
(3) S.S. Chen, P.Y Hsu, Y. L. Chen, “Mining 
Association Rules in Sequence Data”, Journal of 
Information Management, vol. 6, no.2, 
pp.167-182, 1999. 
(4) R. Srikant and R. Agrawal, “Mining Quantitative 
Association Rules in Large Relational Tables”, 
ACM SIGMOD, pp.1~12, 1996. 
(5) Cabena and Peter, “Discovering Data Mining: 
From Concept to Implementation”, Prentice Hill, 
1998. 
 
 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
■ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱 普及健康服務格網：以格網
為基礎之個人健康服務系統 
計 畫 編 號 
NSC 98-2218-E-007-005 
報 告 人 
姓 名 
楊東麟 
服 務 機 構 
及 職 稱 
逢甲大學 資訊工程學系 
教授 
會議/訪問時間 
 地點 
2011年 7月 8-11日 
Ulaanbaatar, Mongolia  
會 議 名 稱 2011 International Conference on Computing and Security  
2011年計算與安全國際研討會議 
發表論文題目 An Omnipresent Personal Health Management System 
一個無所不在的個人健康管理系統 
 
一、主要任務摘要（五十字以內） 
發表和觀摩與本計畫相關的研究成果，探討在電腦網路和通訊普及的環境，
應用資訊和格網技術，提升都會和偏遠地區的個人健康管理。 
 
二、對計畫之效益（一百字以內） 
參加國際會議與學者、科學家、工程師和研究人員，一起探討資訊、格網應
用的相關議題，除了提升研究水準和擴散成果的應用，還可以增加國際合作機會
和強化研究成果的實務價值，對於我們計畫的發展有很大的助益。 
 
三、經過 
2011年計算與安全國際研討會議(International Conference on Computing and 
Security, ICCS2011)於2011年7月8日到11日在蒙古國的首都烏蘭巴托舉行，由蒙
古大學和台灣科技大學共同主辦，IEEE Taipei Section等單位協辦，包括兩天四個
sessions的議程，主要議題是探討計算與安全技術，內容涵蓋the dependability, fault 
tolerance, grid and cloud computing以及biometrics identification, verification and 
security 等技術和相關的fundamental theoretical approaches, practical experimental 
projects, and commercial components and systems方面的討論。透過蒙古、臺灣、新
可以有機會在其他領域上交流與學習。尤其其他學者也有探討健康醫療的論文發
表，讓我有機會從優秀的論文和高水準的講解中發現一些值得進一步探討的題
目，包括健康管理的數學模式的探討和其他 Cloud Computing和 Security的議題。
不僅如此，也從幾天的活動和行程中的接觸，瞭解到蒙古國各方面的發展和科技
的推廣應用，雖然地廣人稀(面積 1,564,115 平方公里，人口 2,754,685)，但是在
東戈壁的沙漠還是可以收到手機訊號。出國可以找到很多我們可以學習的地方，
尤其是各國競爭激烈的資訊科技進步快速，我們如果不積極主動的參與國際活動
則會導致不進則退的問題。 
 
五、建議與結語 
要增進國內的學術研究水準和國際化，一方面除了獎勵大學院校教師和研究
人員出國參加各項學術會議，另一方面多爭取在國內舉辦國際性會議，或者和國
外機構合作進行相關的研究計畫和聯合舉辦大型的研討會。像這次的會議能夠由
蒙古大學和台灣科技大學共同主辦，就是一個很好的國際合作的例證。這樣和國
外有較多和較深入的交流，不但增強我們學習的層面，還能夠提昇我們學術領域
在國際上的能見度和知名度。另外，鼓勵和獎助研究生(包括碩士生)出國參加國
際性學術研討會議，也是有助於培養更多專業的優秀人才。雖然政府和學校在這
些方面都有很努力的在推動，但是在經費來源逐漸缺少的情況下，很難擴大規模
產生更大的效益，建議能夠結合產業的研發需求和能量，利用節稅優惠鼓勵研發
投資的手段達到永續發展的長期目標，來鼓勵台灣學術界踴躍參與國際會議和增
加國際交流的機會。 
 
六、攜回資料 
1. Proceedings of the 2011 International Conference on Computing and Security。 
論文集沒有書面資料，議程資料和 Full paper內容是錄在 CD的電子檔案。 
 
  
data including patient profile, medical history, daily intake of 
food, exercise, health examination, treatment, and recovery 
procedures. Second, we use data mining to analyze and 
produce resultant data via the platform to provide doctors with 
reference for personal health consultation and management. 
The goal is to improve the traditional medical service with 
better information sharing and personal health management. 
Figure 1.   A prototype of e-Texcare® wearable physiological measurement 
system 
The rest of this paper is organized as follows. Section II 
describes related work. Section III and Section IV present our 
research methods and system implementation. In Section V we 
describe the discussion of our system functions. Finally, we 
make a conclusion and talk about future work in Section VI. 
II. RELATED WORK 
Grid is a technology that integrates distributed computing 
resources on the Internet using more loosely coupled, 
heterogeneous, and geographically dispersed computers. Since 
a grid can be dedicated to a specialized application, we 
construct the platform of Health Grid for personal or home 
healthcare. This platform performs data exchange, storage and 
sharing on the Grid using P2P in the bottom layer, where data 
access interfaces are provided to integrate all the services.  
Figure 2.   The EHR sharing mechanism 
The Health Grid is based on the architecture of the 
Medicare Grid, which is composed of Computing Grid, Data 
Grid and Electronic Health Record (EHR) Management 
System. Computing Grid is responsible for service operations, 
Data Grid is used to store data, and EHR Management System 
deals with all kinds of EHR shared between all the connected 
hospitals using the mechanism in Fig. 2. 
Data Mining is a process of analyzing data after performing 
data pre-process [5] and construction of data warehouse [6]. 
Some applications like [7] analyzes the health information of 
patients and gives the analyzed result to doctors or patients for 
reference; [8] focuses on mining cancer data; [9] processes 
electronic case history and [10] deals with the medical database. 
We hope to find out information or rules that have referential 
importance to doctors or patients after sifting through different 
databases. Other applications are like data mining agent, for 
example, [11] focuses on the behavior of Internet shopping and 
[12] builds an agent system for medical data mining. 
The purpose of data mining is digging out useful 
knowledge or rules that no one has discovered from a large 
database. The common data mining methods include 
Association Rule [13], Classification [14], Clustering [15] and 
Time Series. 
Association Rule is a way that filters out infrequent 
itemsets by support threshold from databases, then using 
confidence threshold to obtain high accuracy rules. Generalized, 
numerical and negative associations are the common 
applications in data mining domain [16]. Others include 
parallel, cyclic [17] and multi-level [18] association rule 
mining. 
The most famous classification method is decision tree. 
SLIQ algorithm [19] is proposed for classifying numerical data 
from large-scale databases and [9] uses parallel computing to 
speed up the construction of decision trees. 
Clustering aims to set similar group for differentiating 
different data. There’re two ways for clustering, one is 
distance-based and the other is hierarchical approaches. Two 
clustering methods work on different data types: K-prototype 
for mixed type datasets and K-means for numerical datasets.  
While the Internet becomes more important and the concept 
of globalization arises, there are many hospitals using decision-
support systems in a global scale. eGlobalHealth Research 
Group in American developed MedProtege IHE/KBS system 
for cross-platform, medical knowledge management and 
resource integration to global hospital management. This 
system is separated into three levels, Communication Level, 
Operation Level and Knowledge Base Level. Similarly, we 
want to build a health consulting and data management system 
based on Grid which has powerful computing capability and 
distributed data management system. Using integrated data, we 
can do analysis with data mining to improve the quality of 
decision support. 
III. RESEARCH METHODS 
Our system uses patient’s EHR data for the basis of 
personal health consultation and self-management system. The 
EHR data include individual health records captured by the 
system and the medical records in the hospital’s Health 
Information Systems (HIS). In our prototype, the basic 
physiological data, food intake and exercise are kept in the 
individual records, and the examination, diagnosis, treatment 
and consulting contents are extracted from the hospital records.  
9. Consulting area lets the patient make 
consultation request to the doctor and review 
the result from the doctor using this interface. 
We design most of the interfaces with easy of 
use by ticking off the items on a list to avoid 
typing text. 
10. Outpatient appointment can be requested by the 
patient. If a doctor is fully booked, the system 
will notify the patient other options. 
(2)  Doctor 
1. Patient personal data can be viewed by the 
doctor, but no update can be made.  
2. Family medical history can be viewed by the 
doctor, but no update can be made. 
3. Physiology and examining data are entered by 
the hospital. They can be viewed by the doctor, 
but no update can be made.  
4. Diagnosis data are recorded by the doctor. 
5. Treatment data are recorded by the doctor. 
6. Medicine data are prescribed by the doctor. The 
details of medicine description are provided by 
the pharmaceutical factory and can not be 
updated.  
7. Food data are entered by the patient for doctors 
to examine and make advises. No update is 
allowed. 
8. Exercise data are entered by the patient for 
doctors to examine and make advises. No 
update is allowed. 
9. Consulting area is for doctor and patient to 
communicate. The doctor provides advice for 
the patient. 
10. Outpatient appointment is used for patient to 
setup time for a doctor visit. 
C. Database Design 
Preventive care of the cardiovascular disease is one of the 
main purposes of our system. Therefore we need to collect 
physiology data, family medical history, exercise data, and 
food intake data, in addition to the data from the hospital, like 
examination, diagnosis, treatment, and medicine data. To help 
patients manage their health, the system analyzes the integrated 
data to provide doctors references for the diagnosis and 
consultation. The patients can check the progress of following 
doctors’ instruction and get consultation as needed no matter if 
they are inpatient, outpatient, recovering at home, or going out 
in the public places.  
Cardiovascular disease is a common disease that involves 
the heart and blood vessels (arteries/veins). Related diseases 
include high blood pressure, arteriosclerosis, heart disease, 
apoplexy, deep vein thrombosis and pulmonary embolism. In 
addition to eating less vegetable and fruits, most people like 
fried food and high-sugar drinks. Few people like to do 
exercise regularly. These are some of the reasons people may 
get cardiovascular disease. The age of the patient with the 
cardiovascular problem is getting younger. To better monitor 
our health condition, our system records the data of personal 
profile, physiological data, examination, diagnosis, treatment, 
medicine, food, exercise and family medical history. Analyzing 
the latent factors of cardiovascular disease through these data, 
we hope the system can provide useful data for doctor to make 
effective consultation and help patients understand and prevent 
from getting cardiovascular disease. The result of our schema 
design is shown in the ER diagram of Fig. 4 and Tables I to IV 
in the Appendix. They are only part of the complete design. 
Figure 4.  ER Diagram 
D. Security 
(1) Home Server IP (HSIP) 
The Home Server IP (HSIP) is used to access any 
HS. Since dynamic IPs are assigned for most of the 
internet services in Taiwan, the IP of an HS will be 
different every time when the HS is connected to the 
Health Grid.  Boot Strap can find the HSIP for a user 
of the HS who passes the login validation from Login 
Server (LS). There’s no database in Boot Strap. The 
ID of the HS is sent to its Region Server (RS) by Boot 
Strap.  The RS uses this HSID to ask all the HSs 
inside the region. If an HS connecting to the Health 
Grid has the same HSID, it will send its IP to RS, and 
RS will return the IP to Boot Strap. This way we can 
ensure the correctness and security of accessing HS. 
(2) Data Access Authorization 
In addition to keeping a copy of the login ID and 
password in LS, all of the personal data like EHR 
records are stored in the database of the patient’s HS. 
Only the patient and his/her friends can access the 
  
 
V. DISCUSSION 
The patients can use their computers to enter the Personal 
Health Management System through the Health Grid platform 
at any time. They can obtain useful medical data, consult with 
the doctor, share and monitor their health information with 
family members and authorized care takers. It is especially 
convenient for those old or disabled people requiring much 
attention from family members or nursing providers.  
It’s a complicate work to deal with large-scale health and 
medical data, especially tracking and recording the patients’ 
physiology data on demand. The format, usage and data source 
of each table from the Health Grid and Medical Grid are 
different. To provide the personal service and consultation, it 
needs the data from the hospital and integrates them to a 
unified view. To decide what data to include in and exclude 
from the system is a hard job even after discussing with doctors 
and the medical staffs in the hospitals. There are too many 
different views from the perspectives of the doctors as well as 
the patients regarding what the meaning and useful data are. 
After all, we can eat what meals we can cook. The solution is 
to construct the system incrementally. With this prototype, we 
can get feedback from the doctors and patients participating in 
our project. Accordingly, improvements based on the 
suggestions from the users can be made to strengthen the 
functions of the system as well as the system interfaces and the 
Health Grid as a whole. 
In addition, the integration and analysis of all collected data 
to help doctor perform consultation are even harder. Currently 
we only use simple methods of data mining to find common 
rules. Since a lot of domain expertise in medical knowledge is 
required to make right decisions, our system is far from 
complete at this stage. We believe it is a long process and more 
insights are still needed from various specialists in the medical 
area as well as the information and communication technology 
areas. 
VI.  CONCLUSION AND FUTURE WORK 
We provide an omnipresent personal health management 
system based on the Health Grid, where patients can record, 
track their health condition, and obtain personal consultation 
and healthcare away from the hospital. This service is 
especially beneficial to the people in the remote areas and the 
places lacking health resources. If smart shirts are available, 
more accurate data can be collected more easily and 
conveniently. If the patient does not wear the smart shirt, one 
can still use computer or other devices to get the same service, 
and manage their health properly. 
While Cloud Computing is getting more and more popular, 
in the future, we will move the Grid platform to the cloud 
environment, providing more efficient and stable services. The 
database will be changed from the relational model to others 
suitable for big datasets in a distributed and parallel 
environment. Currently, we are already working on a version 
on Apache Hadoop’s HBase to get more efficient access ability 
and extendibility. We will also extend our system to work on 
the problem of other diseases. 
APPENDIX 
 
TABLE I.  CONSULTING AREA 
 
TABLE II.  PHYSIOLOGICAL DATA 
TABLE III.  EXAMINATION DATA 
 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
■ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計畫名稱 普及健康服務格網：以格網為基礎之個人健康服務系統 計畫編號 NSC 99-2218-E-007-001 
報告人 
姓名 石天威 
服務機構 
及職稱 
逢甲大學纖維與複合材料學系
教授 
會議/訪問 
時間地點 2011/10/20-22 與 2011/10/28-31：中國，上海 
會議名稱 
(1) 2011 中國紡織學術年會 
2011 China Textile Academic Conference (CTAC2011) 
(2) 2011 國際工程與技術大會 
2011 World Congress on Engineering and Technology (CET) 
發表論文題目 
(檢附論文檔案) 
(1) 316L不銹鋼纖維製程中纖維之晶格微應變的探討 
(2) Formation of σ-phase in 316L Stainless Steel Fiber Using a Multi-pass 
Cold Drawing Process 
一、主要任務摘要（五十字以內） 
    參與「2011中國紡織學術年會(CTAC2011)」，發表「316L不銹鋼纖維製程中纖維之
晶格微應變的探討」論文；參與「2011國際工程與技術大會(CET)」，發表「Formation of 
σ-phase in 316L Stainless Steel Fiber Using a Multi-pass Cold Drawing Process」論文。 
 
二、對計畫之效益（一百字以內） 
  此次參與之CTAC2011與CET會議，分別對材料科學基礎研究、技術紡織品、高性
能與功能性材料等議題進行學術交流與分享，透過工程科技做為科學發現與產業發展
之橋樑，對於人體軀幹姿態感測紡織品材料設計，以及織品感測平台之呈現與推廣有
相當大的幫助。 
 
三、經過 
  本人於10/20搭機前往上海，參與10/21~22在中國上海舉辦之「2011中國紡織學術
年會(CTAC2011)」，發表「316L不銹鋼纖維製程中纖維之晶格微應變的探討」論文。於
10/28~10/31參與10/28~11/2在中國上海舉辦之「2011國際工程與技術大會(CET)」，發表
「Formation of σ-phase in 316L Stainless Steel Fiber Using a Multi-pass Cold Drawing 
Process」論文，和與會學者相互討論交流。於10/31離開上海返回台灣。 
 
四、心得 
  CTAC2011會議以材料科學與現代紡織為主題，將新型材料技術、電子信息技術、
生命科學與技術、節能與環保技術與現代紡織生產相結合，搭建跨紡織領域、跨學科
的學術交流。並於10/21發表「316L不銹鋼纖維製程中纖維之晶格微應變的探討」論文，
探討不鏽鋼纖維經冷抽加工細化纖維直徑時發生應變-誘導麻田散鐵相轉變，且由於麻
田散鐵晶格擴張導致麻田散鐵晶相之均方根微應變大於沃斯田鐵晶相，相應於麻田散
鐵相轉變機理。而CET會議以工程科技做為科學發現與產業發展之間的橋樑，提供亞
316L 不銹鋼纖維製程中纖維之晶格微應變的探討 
石天威, 黃詩茹 
(逢甲大學 纖維與複合材料學系) 
 
摘要  直徑 20 μm 的 316L 不銹鋼纖維經冷抽加工分別製成 12、8、與 6 μm 等不同直徑的纖維。利用 X-ray
繞射法對 316L 不銹鋼纖維進行相鑑定發現，不銹鋼纖維存在沃斯田鐵相、麻田散鐵相、及 sigma 相。以 Rietveld
法之 Materials Analysis Using Diffraction Software 分析微結構發現，不銹鋼纖維經冷抽加工細化纖維直徑
時發生應變-誘導麻田散鐵相轉變，且由於麻田散鐵(體心立方)晶格擴張，導致麻田散鐵晶相之均方根微應變大
於沃斯田鐵晶相，相應於麻田散鐵相轉變機理。 
關鍵詞  應變誘導相變; 均方根微應變 
 
Variation of Microstrain of 316L Stainless Steel Fiber during a cold drawing process 
SHYR Tien-Wei, HUANG Shih-Ju 
(Department of Fiber and Composite Materials, Feng Chia University, Taiwan) 
 
Abstract  316L austenite stainless steel fiber with a diameter of 20 µm was reduced to 12, 8, and 6 µm, respectively, by  
cold drawing process. Phase identification and microstructural analysis of 316L austenite stainless steel fiber have been 
investigated by X-ray diffraction method and Rietveld method with a Materials Analysis Using Diffraction software. 
Three crystalline phases such as austenite, α'-martensite, and sigma phase of the fibers were identified by an X-ray 
diffraction method. The strain-induced martensite transformation was occurred during cold drawing process, which was 
analyzed by using Rietveld method. The root mean square microstrain of martensite phase was higher than that of 
austenite phases due to the expansion of a martensite (bcc) lattice, which is corresponding to the martensite phase 
transformation mechanism. 
Keywords  strain-induced phase transformation, root mean square microstrain 
 
 
 
 
 
 
 
 
 
 
 
基金項目：行政院國家科學委員會（NSC 97-2221-E-035-026-MY3) 
作者简介：石天威，男，教授，英國里茲大學 (University of Leeds, UK) 紡織工業系博士。主要研究方向为纖維結構，
E-mail: twshyr@fcu.edu.tw。 
pseudo-Voigt (pV) functions to describe the profiles 
whose intensity is directly related to the structure of 
each studied phase[6, 7]. 
3  Results and discussion 
3.1  Phase identification 
X-ray diffraction pattern of 316L fibers were shown in 
Fig. 1.The austenite and martensite phases with four 
small peaks of sigma (σ) phase were found in all 
stainless steel fibers. The sigma phase formed in 
austenite stainless steel is a hard and brittle 
intermetallic phase. The crystal structure is a 
tetragonal unit cell, which space group is P42/mnm. 
Each unit cell contains 30 atoms. The lattice 
parameters are a=8.7-9.2 Å and c=4.554-4.8 Å. Cold 
drawing resulted in an increased rate of nucleation of 
particles and in an accelerated growth of the 
precipitates [8, 9]. 
 
圖 1 直徑 為 20、12、8、與 6 µm 之 316L 不銹鋼
纖維的 XRD 圖譜 
Fig.1 XRD patterns of 316L stainless steel fibers with 
diameters of 20, 12, 8, and 6 µm. 
The XRD profiles of stainless steel fibers were refined 
by MAUD software. The diffraction patterns were 
simulated by fitting a series of background parameters, 
crystal structural parameters, microstructural 
parameters, and texture parameters. After 
simultaneous refinement and correction, XRD profiles 
were improved. Typical refinement of the XRD 
patterns of stainless steel fiber in diameter of 8 µm is 
shown in Fig. 2. Before microstrain and crystallite 
anisotropic corrections, the refined profile of 
α'-martensite phase was narrow, which did not overlap 
the experimental profile. The refined profile is 
coincided with the experimental profile after 
microstrain and crystallite anisotropic corrections. It 
means that crystallite anisotropy, distortion and/or 
microstrain existed in the lattice of crystalline 
materials which are the effective factors of diffraction 
broadening. 
 
 
圖 2 直徑 8 µm 之 316L 不銹鋼纖維 XRD 圖譜的精
煉，微應變與晶粒異向性:(a)校正前與(b)校正後 (虛
線表示實驗曲線，實線表示擬合曲線) 
Fig.2 Refinement of the XRD pattern of 316L 
stainless steel fiber in diameter of 8 µm (a) before and 
(b) after the microstrain and the crystallite anisotropic 
corrections, respectively. The dotted line represents an 
experimental profile and the solid line represents a 
refined profile. 
[3] PAULA A S, CANEJO J H P G, MAHESH K K, et al. Study of 
the textural evolution in Ti-rich NiTi using synchrotron 
radiation [J]. Nucl Instrum Methods B 2006, 246: 
206–210. 
[4] Juho Talonen. Effect of strain-induced α’-martensite 
transformation on mechanical properties of metastable 
austenitic stainless steels [D]. Helsinki University of 
Technology, Finland. 2007:45–48. 
[5] LANGFORD J I, LOUËR D. Powder diffraction[J]. Rep Prog 
Phys, 1996, 59: 131-234. 
[6] SAHU P, DE M, KAJIWARA S. Microstructural 
characterization of Fe–Mn–C martensites athermally 
transformed at low temperature by Rietveld method [J]. 
Mater Sci Eng A, 2002, 333: 10–23. 
[7] BOSCO Enrica, ENZO Stefano, BARICCO Marcello, X-ray 
analysis of microstructure in Au–Fe melt spun alloys [J], 
J Magn Magn Mater, 2003, 262: 136–141. 
[8] PADILHA A F, RIOS P R. Decomposition of austenite in 
austenitic stainless steels [J]. ISIJ Int, 2002, 42: 325–337. 
[9] CHEN T H, YANG J R. Effect of solution treatment and 
continuous cooling on σ-phase precipitation in a 2205 
duplex stainless steel [J]. Mater Sci Eng A-Struct., 2001, 
311: 28–41. 
[10] TATEYAMA Shinji, SHIBUTA Yasushi, SUZUKI Toshio. A 
molecular dynamics study of the fcc–bcc phase 
transformation kinetics of iron [J]. Scripta Materialia, 
2008, 59: 971–974. 
[11] PORTER D A, EASTERLING K E. Phase Transformation in 
Metals and Alloys [M]. 2nd. London: Chapman & Hall, 
1992: 391. 
B. Analysis of instruments and methods  
The phase transformation studies in multiple-step of cold 
drawing ASS 316L wire and fibers were invested by X-ray 
diffraction using a Cu Kα radiation operating at 40 kV and 300 
mA on Rigaku D/max 255pc. The phase identification and 
quantification using MDI Jade 5.0 and Rietveld method with 
Materials Analysis Using Diffraction (MAUD) software, 
respectively. The surface morphology of the stainless steel 
wire and the fibers were examined using a HITACHI S3000 
field emission scanning electron microscope (SEM) and an 
energy dispersive spectrometer (EDS) of HORIBA 
EMAX400 was used to determine the element compositions 
of the precipitates. 
 
III.  RESULT AND DISSCUSSION 
A. Identification of Crystalline Phases 
X-ray diffraction pattern of 34 µm ASS 316L fiber was 
fitted using the Rietveld method, which is a full pattern of 
refinement, as showed in Fig. 1(a). Four σ-peaks are apparent 
close to (111) of the γ-austenitic phase ( Fig. 1(b)). The 
σ-phase formed in ASS is a hard and brittle intermetallic phase. 
The crystal structure is a tetragonal unit cell, which space 
group is P42/mnm. Each unit cell contain 30 atoms. The lattice 
parameters are a=8.7-9.2 Å and c=4.554-4.8 Å[4, 10]. 
Quantification of the σ-phase was done and the XRD 
profile was refined using the Rietveld method employing the 
MAUD software[11]. A FIZ-FindIt software for inorganic 
crystal structures database, ICSD, was used for the selection 
of the crystal structure model. Each volume fraction of the 
crystalline phases was defined, see Table 2.  
Trace of σ-phase precipitated during the short term of heat 
treatment, which is different from previous studies on aging, 
heat treatment, creep test, and solution treatment in long term 
of time[5-7, 10]. Cold working resulted in an increased rate of 
nucleation of particles and in an accelerated growth of the 
precipitates[4]. Grains of stainless steel fibers were refined 
during cold working, which increased grain boundaries, thus 
providing a large numbers of sites for nucleation of the 
σ-phase.  
 
 
 
Figure 1. X-ray diffraction pattern of 34 µm stainless steel fiber 
(a) full pattern, (b) enlarged pattern. (The dot line represents 
experimental profile and the continue line represented refined 
profile.) 
 
TABLE 2. Volume fractions (%) of each crystalline phase and 
lattice parameters of σ-phase 
Diameter of 
stainless 
steel fibers 
Vγ  Vα' Vσ 
Lattice 
parameters of 
σ-phase (Å) 
50 µm 91.34 7.24 1.42 
a=8.799 
c=4.541 
34 µm 90.82 5.01 4.17 
a=8.820 
c=4.591 
20 µm 82.53 8.19 9.28 
a=8.814 
c=4.597 
 
REFERENCES 
[1] Vishnuvardhanan Vijayakumar, Aurelien Du Pasquier, Dunbar P. Birnie 
III, “Electrical and optical studies of flexible stainless steel mesh 
electrodes for dye sensitized solar cells”, Sol. Energy Mater. Sol. Cells, 
2011. 
[2] C. C. Su, S. H. Chang, “Radial growth of carbon nanocoils on stainless 
steel wires coated with tin particles using chemical vapor deposition 
from acetylene”, Mater. Letter, Vol. 65, pp. 1114-1116, 2011. 
[3] C. Kaya, A. R. Boccaccini and P. A. Trusty, “Processing and 
characterisation of 2-D woven metal fibre-reinforced multilayer Silica 
matrix composites using electrophoretic deposition and pressure 
filtration”, J. Eur. Ceram. Soc., Vol. 19, pp. 2859-2866, 1999. 
[4] A. F. Padilha and P. R. Rios, “Decomposition of austenite in austenitic 
stainless steels”, ISIJ International, Vol. 42, pp. 325–337, 2002. 
[5] M. SCHWIND, J. KӒ LLQVIST, J.-O. NILSSON, J. ÅGREN and H.-O. 
ANDRÉN, “σ-phase precipitation in stabilized austenitic stainless 
steels”, Acta mater., Vol. 48, pp. 2473–2481, 2000. 
[6] Jorge L. Garin, Rodolfo L. Mannheim, “Precipitation in AISI 316L(N) 
during creep tests at 550 and 600 °C up to 10 years”, J. Mater. Process. 
Technol., Vol. 209, pp. 3143–3148, 2009. 
[7] A. F. Padilha, D. M. Escriba, E. Materna-Morris, M. Rieth, M. 
Klimenkov, “Precipitation in AISI 316L(N) during creep tests at 550 
and 600 °C up to 10 years”, J. Nucl. Mater., Vol. 362, pp. 132–138, 
2007. 
[8] D. N. Wasnik, G. K. Dey, V. Kain, I. Samajdar, “Precipitation stages in 
a 316L austenitic stainless steel”, Scripta Mater., Vol. 49, pp. 135–141, 
2003. 
[9] C. H. Shek, D. J. Li, K.W. Wong, J. K. L. Lai, “Creep properties of 
aged duplex stainless steels containing σ phase”, Mater. Sci. Eng. 
A-Struct., Vol. 266, pp. 30–36, 1999. 
[10] T. H. Chen, J. R. Yang, “Effect of solution treatment and continuous 
cooling on σ-phase precipitation in a 2205 duplex stainless steel”, Mater. 
Sci. Eng. A-Struct., Vol. 311, pp. 28–41, 2001. 
[11] P. Sahu, “Lattice imperfections in intermetallic Ti–Al alloys: an x-ray 
diffraction study of the microstructure by the Rietveld method", 
Intermetallics, Vol. 14, pp. 180–188, 2006. 
[12] F. Long, Y. S. Yoo, C. Y. Jo, S. M. Seo, H. W. Jeong, Y.S. Song, T. Jin, 
Z.Q. Hu, “Phase transformation of η and σ phases in an experimental 
nickel-based superalloy”, J. Alloy. Compd., Vol. 478, pp. 181–187, 
2009. 
[13] B. Weiss and R. Stickler, “Phase instabilities during high temperature 
exposure of 316 austenitic stainless steel”, Met. Trans., Vol. 3, pp. 
851-866, 1972. 
[14] T. P. S. Gill, M. Vijaylakshmi, P. Rodriguez and K. A. Padmanabhan, 
“On microstructure-property correlation of thermally aged type 316L 
stainless steel weld metal”, Metall. Trans. A, Vol. 20, pp. 1115–1124, 
1989. 
  
and dimensions for domestic researchers. 
五、攜回資料 
Conference Proceeding (Topic: U-healthcare). 
 
As for the first issue, we adopted Taiwan electronic Medical record Template 
(TMT) [1], which is a standard EHR format established by Department of Health, 
Executive Yuan, Taiwan. Based on this format, we developed an application that 
translates specific EHR format used in each hospital or medical center into a standard 
TMT format. As part in this project, we developed an application for Taichung 
Veterans General Hospital, Taiwan which translated more than 500,000 EHR samples 
as testing data. 
As for the second issue, we developed a Medicare-Grid platform to address the 
issue of exchanging EHRs. First, grid and peer-to-peer technologies were used to 
develop an Electronic Health Record (EHR) center as a decentralized database to 
store and share EHRs among participating hospitals and medical centers. For each site, 
we developed a client application that permits them to connect to the EHR center, to 
upload or download EHRs. Although this mechanism is actually a centralized 
approach, which has potential drawback on scalability and single point of failure, 
peer-to-peer is considered to decentralize this “single” server and makes it scalable, 
fault-tolerant and robust. 
In addition to the data sharing mechanism mentioned, we also integrate computing 
resources provided by hospitals, to form a computational grid for medical related 
applications. We use the de facto standard grid middleware Globus [2] to build up a 
computing grid platform and implement a workflow-based resource broker to 
efficiently match and select available resources in reply to user’s requests. 
Additionally, a web portal is also developed which supports users to utilize 
underlying grid resources with ease. 
Based on our computing and data grid platform, we developed medical related 
applications to improve the in-hospital medical services. Applications include (1) a 
data warehouse for medical decision support system, (2) a RFID-based mobile 
monitoring system to precisely identify people or items, and (3) a wearable 
physiological signal measurement system that monitors the health condition of a 
patient. 
The remainder of this paper is organized as follows. Section 2 describes similar 
projects with respect to domestic and international perspectives, while in Section 3 
issues regarding to the development of grid platform and EHR sharing mechanism are 
discussed. Later, three medical related applications are presented in Section 4, and 
finally, conclusion remarks are presented in Section 5. 
2   Related Works 
As far as we know, the insights presented in this project are novel not only in Taiwan, 
since it is focused on the use of grid technology to enable EHR sharing among 
hospitals and to integrate various Medicare applications. Nevertheless, there are some 
medical related projects that utilize the computing grid platforms as underlying 
platform such as Knowledge Innovation National Grid (KING) project [3] and 
BioGrid related project [4]. 
There exists a similar project called National Grid [5], which is focused on 
enabling medical related documents such as EHR or X-ray image to be shared by 
 Fig. 1. The architecture of Resource Broker system and the relationships of each component. 
 
Fig. 2. A schematic diagram of the complete Workflow System. 
3.2   Data Grid 
Data grid system is designed as distributed two-layer hierarchical peer-to-peer 
architecture based on the principle of locality. The bottom layer called intra-group 
overlay is constructed with Chord system, which the overlay clusters neighboring 
peers provide services within local regions; the upper layer called inter-group overlay 
is constructed to connect local groups together with consideration to locality. 
Specifically, the data grid system consists of three modules: 
2.3   EHR Management System 
EHR management system is the core service in this paper, addressing the issues of 
sharing standardized EHR among each participating hospital, and they are threefold. 
The former one is how to standardize EHR, followed by how to share them, and the 
latter is regarded to the user interface. For the first issue, we adopted TMT, which is a 
standard EHR format established by Department of Health, Executive Yuan, Taiwan, 
and developed a translation application to translate the specific EHR format used in 
each hospital into standard TMT format. Since the HIS (Hospital Information System) 
used in each hospital is developed by difference software company, each of them 
have a specific format and database schema. To make standard TMT format 
practicable, it requires understanding both TMT and specific hospital format in order 
to develop translation application for that hospital. Throughout the development of 
this paper, we focused our attention to the development on the transformation 
application between TMT and the format used in Taichung Veterans General Hospital, 
organization which made available more than 500,000 EHR sampling data, and 
translated with success to standard TMT format and stored next on data grid. 
As for the second issue, we exploit the data grid as the fundamental EHR storage 
space to store and share EHR among hospitals, as illustrated in Fig. 4 the proposed 
EHR sharing mechanism. The text in black color represents the HIS used in each 
hospital. Physicians in each hospital make use a desktop computer to read and record 
patient’s health record in the local database. The text in red color is the server and 
application developed in this paper to facilitate EHR sharing. The EHR Center is 
constructed using data grid as described in subsection 3.2. A client application is then 
implemented with the functionality of search, upload, and download data from EHR 
Center. In order to connect hospital with data grid, the TMT Translator is responsible 
for the translation of specific EHR format to stand TMT format. 
A case study is used to demonstrate the entire sharing operation. For instance, a 
patient X has appointment with a doctor in hospital A, his/her health record is then 
stored in local HIS database according to operation procedure of hospital A. Patient 
X’s EHR is then translated to TMT format (by TMT Translator) and uploaded to EHR 
Center (by P2P Storage Client). As patient X register in another hospital for a 
diagnosing session, say hospital C, the EHR of X will be downloaded partial or 
entirely (depending on the purpose of the appointment patient X has in this medical 
center) from EHR center and translated to the format used in hospital C. This 
mechanism enables the inspection done by one hospital to be diagnosed by doctor 
served in another hospital. 
Advantages of this sharing mechanism are twofold. First, it is easy to deploy in 
hospital. Only two applications (TMT Translator and P2P Storage Client) are required 
to install in each hospital. P2P Storage Client is a universal application that is 
developed only once. Although each hospital must develop and own a personalized 
version of TMT Translator, this application may have been developed or under 
development since Taiwan government is promoting the TMT as standard and 
requests that all hospitals to follow this standardization process. Second, this sharing 
mechanism will not interfere with the procedure of taking medical treatment. All EHR 
translation and exchange are performed in background and on-demand. Moreover, 
4.1   Medical Decision Support System 
With the rapid growth on the amount of medical data, to find useful information 
among such a large dataset in an efficient way is desired. The medical decision 
support system, which is used to help physicians or medical professionals to make 
better decisions for treatment, is built based on data mining techniques. In this system, 
we focused on the cardiovascular disease (CVD) and made use of patients’ EHR data 
as the source data. To build an EHR medical decision support system, we collected 
patients’ EHRs on the data grid system and then stored them in a data warehouse, in 
which data mining techniques are utilized to analyze the EHR information. 
There are a number of previous works on mining medical data, such as mining 
approaches to analyze medical database to find useful patterns, and mining personal 
health information and providing the results as references for doctors [15]. Data 
warehousing has been extensively investigated, including data preprocess and data 
warehouse maintenance over changing information sources [16]. On the technology 
of data mining, it consists of many approaches such as association rule [17] which 
discovers hidden and interesting rules in database, clustering which divides a data set 
into several groups by their characteristics, and decision tree which is used to predict 
the class of the new input data. 
Based on related works, we considered these approaches and then developed a 
medical decision support system, to analyze the EHR data warehouse and provide 
useful results. Results obtained relate to the data we collected from some specific 
hospitals, since they represent the analysis of the heart disease EHR in a particular 
population, where the results must be confirmed by medicine experts or physicians. 
Additionally, we establish a genetic database of cardiovascular diseases. In this 
system, we integrate 34 cardiovascular diseases and its related gene expression data, 
SNP, protein-protein interaction, alternative splicing and protein-protein interaction 
(PPI) information into a web-based interface. Through the analysis of this data model, 
we obtain significant result of rules for future research and tracking. 
The source of cardiovascular disease related gene data is from NCBI OMIM 
database [18]. OMIM disease data is collected first, and then the text-mining 
technique is used to generate a dataset of cardiovascular disease. Next, analysis on 
this dataset is performed to obtain a list of cardiovascular diseases and their related 
genes. Lastly, we parse these annotations and store them in a database, and then filter 
out those incorrect results from the dataset, as in Table 1. 
We use the list of cardiovascular disease related genes to search the STRING 
database [19], and get the PPI network graph. Due to the cardiovascular disease is 
multi-complex disease, these graphs can help to understand the interactions involved 
in these related genes. The source of alternative splicing data is AVATAR [20], which 
is a value-added alternative splicing database. Alternative splicing is an important 
event of gene transcript, and it causes the polymorphism of the gene expression. We 
link this database and obtain the alternative splicing result of these cardiovascular 
disease related genes to help us observe the form of the specific gene. The source of 
SNP data is HAPMAP [21], which provides plentiful SNP information, like the 
Linkage Disequilibrium (LD) Maps, tagSNPs and the race classification data. We 
performed analysis on these data and reserved the SNP data that show a high LD 
value that is related to the cardiovascular disease gene. These SNP data can help us 
in Fig. 6. The monitoring and tracking system can reflect the position of individuals 
that with active RFID Tag through web interface. In addition, for some areas that are 
dangerous or private, they can be marked as off limits from the system or restrict the 
time-duration for stay. For example, 30 minutes is set for a bathroom to avoid 
accident such as tumble of elder people who is not able to move or unconscious. Once 
an abnormal event occurred, such as illegal entrance to a limited area or over stay-
duration in a specific zone, an alarm can be dispatched and email or short message is 
sent to the system administrator. 
 
 
Fig. 6. The Real-Time Location System (RTLS) architecture. 
4.3   E-Texcare Health Care System 
A number of research topics listed next have been investigated to achieve the e-
Texcare health care system requirements for good functionality, portability, 
comfortable, endurance, and ease of use. The system configuration established and 
developed is listed as follow: 
1. Wearable research (design, fabricate and integrate) 
 Physiological measurement  
 Electronic circuits 
2. Physiological measurement research 
 Microprocessor, and micro sensor circuits for physiological measurement 
 Signal processing 
3. Mobile and wireless communication research 
 RF and wireless communications 
 Information transmission and reception 
 
A wearable and portable health care system is available for measurements on 
humans, as shown in Fig. 7. The wearable platform contains sensors that acquire and 
References 
1. Taiwan electronic Medical record Template (TMT), http://emr.doh.gov.tw/old/index.html 
2. Foster I., Kesselman C.: Globus: A Metacomputing Infrastructure Toolkit. J. 
Supercomput. Appl. 11(2), 115--128 (1997) 
3. Knowledge Innovation National Grid (KING) Project, 
http://www.nchc.org.tw/tw/about/publication/king.php 
4. National Bioinformatics Applied Grid, http://biogrid.genomics.org.cn/index.jsp 
5. NationalGrid, http://www.nationalgrid.com/corporate 
6. Czajkowski, K., Fitzgerald, S., Foster, I., Kesselman, C.: Grid Information Services for 
Distributed Resource Sharing. In: 10th IEEE International Symposium on High 
Performance Distributed Computing, pp. 181--184. IEEE Press, New York (2001) 
7. Foster, I., Kesselman, C.: The Grid: Blueprint for a New Computing Infrastructure. 
Morgan Kaufmann, San Francisco (1999) 
8. Foster I.: The Grid: A New Infrastructure for 21st Century Science. Physics Today, 55(2), 
42--47 (2002) 
9. Yang C.T., Lai K.C., Shih P.C.: Design and implementation of a workflow-based 
resource broker with information system on computational grid. J. Supercomput. 1--34 
(2009) 
10. Yang C.T., Shih P.C., Lin C.F., Chen S.Y.: A resource broker with an efficient network 
information model on grid environments. J. Supercomput. 40(3), 76--109 (2007) 
11. Druschel P., Rowstron A.: PAST: A large-scale, persistent peer-to-peer storage utility. In 
HotOS VIII, Germany (2001) 
12. Kubiatowicz J., Bindel D., Chen Y., Czerwinski S., Eaton P., Geels D., Gummadi R., 
Rhea S., Weatherspoon H., Weimer W., Wells C., Zhao A.B.: OceanStore: An 
Architecture for Global-Scale Persistent Storage. In: 9th international Conference on 
Architectural Support for Programming Languages and Operating Systems (2000) 
13. Clarke I., Sandberg O., Wiley B., Hong T.W.: Freenet: A Distributed Anonymous 
Information Storage and Retrieval System. In: Proceedings of the ICSI Workshop on 
Design Issues in Anonymity and Unobservability, USA (2000) 
14. Taiwan Unigrid, http://www.unigrid.org.tw 
15. Lloyd-Williams M.: Case Studies in the Data Mining Approach to Health Information 
Analysis. In: IEE Colloquium on Knowledge Discovery and Data Mining, vol. 1, pp. 1--4 
(1998) 
16. Rundensteiner E.A., Koeller A., Zhang X.: Maintaining Data Warehouses over Changing 
Information Sources. Communications of the ACM, 43(6) (2000) 
17. Dai P.T., Chu J.L., Lin S.J., Chang H.B., Yang D.L.: Construction and Analysis of a Data 
Warehouse System for Customer Relationship Management. In: Proceedings of EC2003, 
Taiwan (2003) 
18. Online Mendelian Inheritance in Man, OMIM. McKusick-Nathans Institute of Genetic 
Medicine, Johns Hopkins University and National Center for Biotechnology Information, 
National Library of Medicine, http://www.ncbi.nlm.nih.gov/omim/ 
19. Mering C., Jensen L.J., Kuhn M., Chaffron S., Doerks T., Krüger B., Snel B., Bork P.: 
STRING 7 -- recent developments in the integration and prediction of protein interactions. 
Nucleic Acids Res., vol. 35, pp. 358--362 (2007) 
20. Chang H.C., Yu P.S., Huang T.W., Hsu F.R., Lin Y.L.: The Application of Alternative 
Splicing Graphs in Quantitative Analysis of Alternative Splicing Form from EST 
Database. J. Comput. Appl. Technol., 22(1), 14--22 (2005) 
21. The International HapMap Consortium. The International HapMap Project. Nature 426, 
pp. 789--796 (2003) 
RFID, Security等熱門的研究課題。此次的國際學術研討會議有許多知名學者
的參與，讓每一位參加這個會議的人士都能夠得到國際上最新的技術與資
訊。是一次非常成功的學術研討會。 
 
四、心得 
 
    參加本次的國際學術研討會議，感受良多。讓本人見識到許多國際知名
的研究學者以及專業人才，得以與之交流。讓本人與其他教授面對面暢談所
學領域的種種問題。看了眾多研究成果以及聽了數篇專題演講，最後，本人
認為，會議所安排的會場以及邀請的講席等，都相當的不錯，覺得會議舉辦
得很成功，值得我們學習。 
 
五、建議與結語 
大會安排的會場以及邀請的講席等，都相當的不錯，覺得會議舉辦得很
成功，值得我們學習。 
 
六、攜回資料  
     論文集光碟片   
 
出國行程表 
 
5/26  前往 Busan、研討會報到，參與 ISPA 2011研討會，參與歡迎晚會 
5/27  主持專題演說，全日參與研討會、參與晚宴 
5/28  發表論文、全日參與研討會 
5/29  返回台灣 
 
 
type of message in single scheduling step.  The 
attribute of this kind of messages is with high 
bandwidth and the senders are also the receivers. 
The rest of this paper is organized as follows.  
Related research is briefly described in section 2.  
Section 3 introduces the architecture of Quad-Core 
CPU.  Section 4 presents research architecture, the 
proposed scheduling algorithm and power saving 
technique. In Section 5, the simulation results and 
performance analysis are given to weigh the pros and 
cons.  Finally, the conclusions presented in Section 6. 
2 Related Work 
There are many research for optimizing 
communication costs at runtime in performing data 
redistribution on distributed multi-computer systems. 
Wang et al. [5] proposed the Divide-and-Conquer 
scheduling method to minimize the communication 
cost.  The messages are separated in several groups.  
Messages that belong to different groups are called link 
messages.  They tried to combine two groups in each 
merging phase.  While combining two groups, 
messages will be scheduled and the contention will be 
avoided.  After all groups are merged, the scheduling 
result is given.  Yook and Park [7] provided the 
relocation scheduling algorithm to deliver low cost 
schedules.  The relocation scheduling algorithm is a 
two-phase algorithm.  The first phase sorts messages in 
the order according to the data size and then schedules 
messages according to the sorted order.  When 
contention happens and the minimal scheduling steps 
can not be obtained, the second phase will reallocate 
the scheduled messages.  Wang et al. [6] proposed a 
scheduling method which is the combined version of 
Divide-and-Conquer scheduling method and relocation 
scheduling algorithm. 
To decrease the power consumption, Meisner et al. 
[2] provided methods for power supplier operation and 
detecting the power waste. Shin and Kim provided 
power saving methodology by improving CPU and 
energy control [3].  A benchmark system considering 
about disk start time, idle time and disk access 
frequency was developed to improve disk energy 
utilization [4].  A temperature-aware register 
reallocation method was proposed to remap node for 
thermal control [8]. 
3 Quad-Core CPU System Structure 
With the advance of CPU technologies, multi-core 
architectures are rapidly used for high performance 
computing.  Figure 1 shows the architecture of 
AMD/Intel Quad-Core CPU.   
AMD Quad-Core CPU is with four cores and 
each owns a 512KB L2 cache.  Cores share the 2MB 
L3 cache and the integrated DDR2 memory controller 
helps increase the memory access speed.  AMD 
provides “PowerNow!” technology to control the 
frequency and voltage of each core.   
Intel Quad-Core CPU showing four sets of 
architectural state, execution engine and local APIC.  
Every two sets share a L2 cache and bus interface and 
connect to a system bus, separately.  The Intel Quad-
Core CPU system structure looks just like a 
combination of two Dual-Core CPUs.  This 
architecture provides each core flexible space to use L2 
cache.  Intel CPU provides “SpeedStep”  technology to 
control CPU frequency and voltage.   
 
  
512KB L2 Cache 
 
Core 1 
512KB L2 Cache 
2MB L3 Cache 
Core 2 Core 3 
512KB L2 Cache 
 
Core 4 
512KB L2 Cache 
 
System Request Interface 
Crossbar Switch 
DDR2 HyperTransport technology 
 
(a) 
  
Execution Engine 
Architectural State 
Execution Engine 
Local APIC 
Architectural State 
Local APIC 
Architectural State 
Execution Engine 
Local APIC 
Architectural State 
Execution Engine 
Local APIC 
Second Level Cache 
Bus Interface 
System Bus 
Second Level Cache 
Bus Interface 
System Bus 
 
(b) 
Figure 1: (a) AMD Quad-Core CPU structure; (b) 
Intel Quad-Core CPU structure. 
 
The hardware instructions of both architecture 
brings the idea of message classifying and voltage/ 
frequency control for energy saving.  To this end, a 
scheduling method and voltage control technique will 
be presented for optimizing communications and keep 
an eye on power consumption. 
4 Energy Efficient Scheduling Techniques 
4.1 Research Architecture 
 
First of all, we describe the research architecture of this 
study as following.  A grid system is composed by one 
the transmission time of that step.  Following equation 
shows the suggest value, Vs, for adjusting voltage: 
 
Vs =(The cost of messages)/ (The cost of dominator) (1) 
 
The Vs could be normalized in four levels after the 
value of Vs is given by above equation.  The details of 
four levels are list as follows: 
 Level 1: 1, where 1 ≤ Vs < 0.75 
 Level 2: 0.75, where 0.75 ≤ Vs < 0.5 
 Level 3: 0.5, where 0.5 ≤ Vs < 0.25 
 Level 4: 0.25, where 0.25 ≤ Vs 
 
We use Figure 2 as an example to explain the 
above idea.  Figure 2 shows the computing 
environment with two clusters and multi-core 
machines in each cluster.  There are one Quad-Core 
CPU machine as PC 1 in Cluster 1, one Dual-Core 
machine as PC 2 and one Single-Core machine as PC 3 
in Cluster 2.  In other words, there are seven nodes 
separated in three multi-core machines in two clusters.  
Four types of messages are also illustrated in this 
example such as m1 for type 1; m2 for type 2; m12 for 
type 3; and m8 for type 4.  The given data distribution 
schemes are {8, 28, 29, 2, 30, 21, 16} and {20, 25, 8, 
22, 8, 26, 25}, where number represents the data size 
for each nodes.  The data size is often used to represent 
the communication cost between nodes while 
exchanging data in grids.  The SP0~6 and DP0~6 are 
source and destination nodes for two schemes, 
respectively.   Arrows which represent the 
communications between source nodes and destination 
nodes are marked as m1~13.  The communication 
scheduling problems of data redistribution are 
transformed to edge coloring problems in bipartite 
graphs.  Therefore, to schedule communications is to 
process those arrows by MCVC.  
The first phase of the scheduling operation 
isolates messages of type 1 and assumed the cost of 
selected messages is divided by 8 according to the 
difference of transmitting rate.  The selected messages 
are m1, m3, m5, m7, m9, m11 and m13 and the costs 
become 1, 2, 1, 0.25, 1, 1.5 and 2.  The second phase of 
first operation separates the rest of messages in two 
steps.  Before messages are scheduled, costs of m2, m4, 
m6, m8 and m10 are changed to 1.5, 1.125, 1.5, 40 and 
1.75 according to the difference of transmitting rate, 
respectively.  Three pairs of messages m4 and m6, m6 
and m8, and m8 and m10 have to be scheduled separately 
according to the scheduling policies.  For instance, SP2 
is the sender of m4 and m6 then both messages must be 
scheduled in different steps.  The complete schedule 
given by the first operation of MCVC is shown in 
Figure 3.  The first step is the result of first phase, and 
the rest steps are the results of second phase of the first 
operation.  The cost dominated by messages with 
maximal data size is used to represent the cost of each 
step. Thus m3 and m13 are the dominators of step 1, m8 
is the dominator of step 2, and m10 is the dominator of 
step3.  The costs of each step are 2, 40 and 1.75.  The 
cost of the derived schedule is 43.75 which is the 
summation of costs of all steps. 
  
A schedule of MCVC 
No. of step No. of message Cost of step 
Step 1 m1(1), m3(2), m5(1), m7(0.25), m9(1), 
m11(1.5), m13(2) 
2 
Step 2 m2(1. 5), m4(1.125), m8(40), m12(9) 40 
Step 3 m6(1.5), m10(1.75) 1.75 
Total cost 43.75 
 
Figure 3: A schedule of MCVC. 
 
4.4 Voltage Adjustment 
 
The MCVC suggests a set of values to adjust the 
voltage of cores to reduce the usage of energy.  
Equation 1 provides VS for messages in each step.  For 
step 1, VS of m1, m3, m5, m7, m9, m11 and m13 are 0.5, 1, 
0.5, 0.125, 0.5, 0.75 and 1.  The normalized values of 
VS become 0.5, 1, 0.5, 0.25, 0.5, 0.75, and 1, 
respectively.  For step 2, the normalized values of VS 
for m2, m4, m8 and m12 are 0.25, 0.25, 1 and 0.25.  For 
step 3, the normalized values of m6 and m10 become 1 
and 1.  The complete list of VS is given in Figure 4.  
Assumed the transmission time of step 1, 2 and 3 are 2, 
40 and 1.75 time units, the original energy usage are 14, 
160 and 3.5 units for each step, respectively.  To adjust 
the voltage with the suggested VS, the energy 
consumption becomes 9, 70 and 3.5, respectively.  The 
total energy consumption is 82.5 units instead of 
consuming 177.5 units and provides 53.5% 
improvement.   
  
Value of VS in each step 
Step1 Step 2 Step 3 
Message Rm Message Rm Message Rm 
m1 0.5 m2 0.25 m6 1 
m3 1 m4 0.25 m10 1 
m5 0.5 m8 1 
m7 0.25 m12 0.25 
m9 0.5 
m11 0.75 
m13 1 
  
  
Total 4.5 Total 1.75 Total 2 
  
Figure 4: Values of VS in each step. 
communication cost and decrease the usage of energy 
for applications on multi-core systems.  The MCVC 
suggests a contention-free scheduling technique that 
takes consideration of network latency in its scheduling 
strategy.  For energy saving, it suggests different 
voltage level for dynamically adjusting CPU speed to 
reduce energy consumption.  The simulation results 
show MCVC has noticeable improvement on 
communication cost and energy usage.  The scheduling 
comparison shows MCVC outperforms other 
scheduling approach in most cases and improves the 
energy usage about 50%.  
REFERENCES 
[1] Ching-Hsien Hsu, Shih-Chang Chen and Chao-
Yang Lan, “Scheduling Contention-Free 
Irregular Redistribution in Parallelizing 
Compilers, ”  The Journal of Supercomputing, 
Vol. 40, No. 3, pp. 229-247, June 2007.  
[2] David Meisner, Brian T. Gold and Thomas F. 
Wenisch, “PowerNap: eliminating server idle 
power,” Proceeding of the 14th International 
Conference on Architectural Support for 
Programming Languages and Operation Systems, 
pp. 205-216, 2009. 
[3] Dongkun Shin and Jihong Kim, “Power-Aware 
Communication Optimization for Networks-on-
Chips with Voltage Scalable Links,” Proceeding 
of the International Conference on 
Hardware/Software Codesign and System 
Synthesis, pp. 170-175, 2004. 
[4] Seung Woo Son, Guangyu Chen, Ozcan Ozturk 
Mahmut Kandemir and Alok Choudhary, 
“Compiler-Directed Energy Optimization for 
Parallel-Disk-Based Systems,” IEEE 
Transactions on Parallel and Distributed 
Systems, Vol. 18, No. 9, pp. 1241-1257, 
September 2007. 
[5] Hui Wang, Minyi Guo and Daming Wei, “A 
Divide-and-conquer Algorithm for Irregular 
Redistributions in Parallelizing Compilers,” The 
Journal of Supercomputing, Vol. 29, No. 2, 
pp.157-170, August 2004. 
[6] Hui Wang, Minyi Guo and Daming Wei, 
“Message Scheduling for Irregular Data 
Redistribution in Parallelizing Compilers,” 
IEICE Transactions on Information and Sysmtes, 
Vol. E89-D, No. 2, pp. 418-424, February 2006. 
[7] Hyun-Gyoo Yook and Myung-Soon Park, 
“Scheduling GEN_BLOCK Array 
Redistribution,” The Journal of Supercomputing, 
Vol. 22, No. 3, pp. 251-267, July 2002. 
[8] XianGrong Zhou, ChenJie Yu and Peter Petrov, 
“Temperature-Aware Register Reallocation for 
Register File Power-Density Minimization,” 
ACM Transactions on Design Automation of 
Electronic Systems, Vol. 14, Issue 2, No. 26, 
March 2009. 
[9] “Key Architectural Features of AMD Phenom 
X4 Quad-Core Processors,” 
http://www.amd.com/us-en/Processors/ 
ProductInformation/0,,30_118_15331_15332%5
E15334,00.html 
[10] Ching-Hsien Hsuand Shih Chang Chen, “A Two-
Level Scheduling Strategy for Optimizing 
Communications of Data Parallel Programs in 
Clusters”, International Journal of Ad-Hoc and 
Ubiquitous Computing (IJAHUC), pp. 263-269, 
Vol 6, No. 4, 2010.  
 
出席國際學術會議報告 
出席人員：周嘉政 
國立清華大學資訊工程學系  博士生 
會議名稱：國際平行與分散處理會議 
一、主要任務摘要 
出席參加第二十五屆國際平行與分散處理會議。 
二、經過 
國際平行與分散處理會議 IPDPS 是本會議舉行以來的第 25 屆，於 2011 年 5 月 20
日至 26日在美國安克拉治舉辦。 
除了 keynote之外，也參與了幾個會議 session。 IPDPS 2011會議 session有以下幾
個：Resource Management，Communication & I/O Optimization，Hardware-Software 
Interaction，Runtime Systems，Routing and Communication，Self Stabilization and Security，
Numerical Algorithms，Reliability and Security，Wireless and Sensor Networks，GPU 
Acceleration，Multiprocessing and Concurrency，Compilers，Distributed Algorithms and 
Models，Parallel Graph and Particle Algorithms，Distributed Systems and Networks，
Programming Environments and Tools，Parallel Algorithms，Distributed Systems，Storage 
Systems and Memory， Operating Systems and Resource Management ， Numerical 
Algorithms，Fault Tolerance，Resource Utilization，Parallel Programming Models and 
Languages，Algorithms for Distributed Computing，Scheduling，Computational Biology and 
Simulations，Cloud Computing，而其中的許多 session，都展示了目前世界上各個研究單
位在平行與分散處理領域最新的研究方向以及成果。 
三、心得 
國際平行與分散處理會議的規模年年增長，這顯示出了平行與分散相關研究的重要
性，目前世界各地學者在這些方面的研究仍然持續地在創新以及進展中。 
Hierarchical Mapping for HPC Applications
I-Hsin Chung
IBM T.J. Watson Research Center
Yorktown Heights, NY USA
ihchung@us.ibm.com
Che-Rung Lee Jiazheng Zhou Yeh-Ching Chung
National Tsing-Hua University
Hsin-Chu, Taiwan
{cherung,jzzhou,ychung}@cs.nthu.edu.tw
Abstract—As the high performance computing systems
scale up, mapping the tasks of a parallel application
onto physical processors to allow efficient communication
becomes one of the critical performance issues. Existing
algorithms were usually designed to map applications
with regular communication patterns. Their mapping
criterion usually overlooks the size of communicated
messages, which is the primary factor of communication
time. In addition, most of their time complexities are too
high to process large scale problems.
In this paper, we present a hierarchical mapping
algorithm (HMA), which is capable of mapping appli-
cations with irregular communication patterns. It first
partitions tasks according to their run-time communi-
cation information. The tasks that communicate with
each others more frequently are regarded as strongly
connected. Based on their connectivity strength, the tasks
are partitioned into supernodes based on the algorithms
in spectral graph theory. The hierarchical partitioning
reduces the mapping algorithm complexity to achieve
scalability. Finally, the run-time communication infor-
mation will be used again in fine tuning to explore
better mappings. With the experiments, we show how
the mapping algorithm helps to reduce the point-to-point
communication time for the PDGEMM, a ScaLAPACK
matrix multiplication computation kernel, up to 20%
and the AMG2006, a tier 1 application of the Sequoia
benchmark, up to 7%.
I. INTRODUCTION
Mapping tasks of a parallel application onto phys-
ical processors of a parallel system to minimize the
communication cost is one of the classical problems
in the high performance computing. Two architectural
trends had brought this problem back to attention
again. First, with the increasing scale of the system,
the interconnection among processors is not scaled
accordingly, which makes communication expensive.
Second, the performance gap between computation
and communication grows exponentially [16]. Thus,
minimizing the communication cost has significant
impacts to the overall application performance.
The problem of mapping application tasks onto a
targeted hardware physical topology has been consid-
ered as a graph embedding problem [6]. In general,
an embedding of a guest graph G = (VG, EG) into a
host graph H = (VH , EH) is a one-to-one mapping
φ from VG to VH . The quality of the embedding
is usually measured by two cost functions: dilation
and expansion. The dilation of an edge (u, v) ∈ EG
is the shortest path in H that connects φ(u) and
φ(v). The dilation of an embedding is the maximum
dilation over all edges in EG. The expansion of an
embedding is |VH |/|VG|. In other words, the dilation
of an embedding measures the worst stretched edge
and the expansion measures the relative size of the
guest graph. Another measurement of the mapping is
based on the hop-byte (i.e., latency ×message size
[23]). Let w(u, v) be the size of messages transmitted
on an edge (u, v) ∈ EG, and d(φ(u), φ(v)) be the
distance, usually measured by the number of hops1, of
the shortest path in H that connects φ(u) and φ(v).
The hop-byte of an embedding is computed as∑
(u,v)∈EG
w(u, v)d(φ(u), φ(v)). (1)
Finding the optimal mapping φ that minimizes di-
lation or hop-byte is NP-hard. Previous general map-
ping solutions to this problem model “fold” the guest
1It is assumed that the latency is proportional to the number of
hops(distance).
fine tuning. Section IV presents the experiment and
the evaluation results. Section V discusses the technical
details and the limitations of the method. Section VI
concludes the paper.
II. RELATED WORK
Mapping parallel programs onto parallel computing
systems is a challenging problem. Graph embedding
has been studied and applied to optimize VLSI circuits
[14], [18]. The graph embedding for VLSI circuits tries
to minimize the longest path where our mapping tries
to reduce the point-to-point communication time.
Space filling curves [4] are applied to map parallel
programs onto parallel computing systems. The use of
space filling curves to improve proximity for mapping
is well studied and has been found useful in parallel
computing. The paper [5] extends the concept of space
filling curves to space filling surfaces. It describes three
different classes of space filling surfaces and calculate
the distance between facets. Unlike the space filling
curve that is based on static information, our hierar-
chical mapping utilizes the run-time communication
information.
Recently, many mapping techniques are developed
to improve the application communication performance
[10]. Our method is trying to further extend the efforts
so the mapping can be handled efficiently on large
scale systems while run-time communication perfor-
mance data is taken into consideration.
There are methods using graph-partitioning and
search-based optimization to solve the mapping prob-
lem. For example, [9] uses an off-line simulated an-
nealing to explore different mappings on Blue Gene/L.
Our approach tries to use heuristics with better initial
mapping and the exploration of different mappings
has the potential to be done in parallel in different
supernodes for fine tunings.
The work in [23] developed topology mapping li-
braries. The mapping techniques are based on folding
heuristics. Our method tries to integrate the run-time
measurement into mapping consideration. The methods
based on folding heuristics require the topologies for
guest and host known in advance where methods based
on run-time measurement allow mapping done more
dynamically.
In terms of supporting MPI topology functions, there
are works done for specific systems: [20] uses graph-
partitioning based for embedding and [19] describes
embedding techniques for switch-based network. Our
work proposes a general solution that helps mapping
on high performance computing systems.
III. HIERARCHICAL MAPPING ALGORITHM
As compared to mapping algorithms based on static
information, the motivations of hierarchical mapping
are to handle 1) the applications with irregular or
complex communication patterns and 2) the computing
systems with high dimensional interconnection, and
3) the mapping efficiency in large scale computing
systems.
Applications with irregular spacial discretization in
the problem domain often use irregular meshes with
irregular communication patterns. Irregular communi-
cation patterns are also from MIMD applications such
as CCSM [2], in which tasks are divided into groups
to compute different subproblems. On the other hand,
the interconnection of the high performance computing
systems is getting more and more complicated. Each
compute node may have 3 or higher dimensional
communication interconnection. Irregular communica-
tion patterns and the high dimensional interconnection
make the mapping more difficult if only based on
the static information. Another issue is to maintain
the efficiency as the applications and the computing
systems scale up. When the number of tasks increases,
finding the optimal mapping for those two kinds of
problems becomes a challenge since tuning cannot be
done manually. To address these issues, we develop the
hierarchical mapping algorithm (HMA). The algorithm
consists of three parts: the task partitioning, the initial
mapping, and the fine tuning. In the task partitioning,
the algorithm groups tasks that have strong relations
into “supernodes”. In the initial mapping, those “su-
pernodes” are mapped onto the host machine. Finally,
the mapping is fine tuned locally by optimization
methods.
A. Task Partitioning
Let G(V,E,w) be a weighted graph constructed
based on the profile information collected during the
run-time using the MPI tracing tool (e.g.,[21]). The
V (vertices) represents the set of tasks,the E (edges)
represents the communication relation and the w(u, v)
equals to the message size communicated between the
task u and the task v. For a given k, the task partition-
problem is large and sparse and the number of desired
eigenvalues and eigenvectors is few, subspace methods,
such as Lanczos method, are more efficient [7]. More
precisely, the time complexity of subspace methods for
computing z2 is O(|E|).
Our task partitioning process is sketched in Algo-
rithm 1.
Algorithm 1 Algorithm for task partitioning
1) Form LN = D
− 1
2 (D − W )D− 12 , where D is
defined in (7).
2) Compute the eigenvector z2 of LN associated to
the smallest nonzero eigenvalue.
3) Use (10) to partition vertices with a threshold δ.
4) Repeat 2 to 4 for each partition until the desired
number of partitions is generated.
B. Initial Mapping
The initial mapping places supernodes onto the tar-
get host machine. The criterion is to minimize the com-
munication cost, such as latency, among supernodes.
For that purpose, one needs to generate the supernode
graph, in which each vertex is a supernode, and an
edge (i, j) exists for supernode Ai and supernode Aj
if ρ(Ai, Aj) is larger than some threshold ∆. Since the
number of supernodes is much less than the number
of tasks, more complicated algorithms are allowed to
use in this step.
Here we use different mapping algorithms for regu-
lar and irregular supernode graphs, since for regular
graphs, such as meshes or rings, better mappings
usually can be obtained (e.g., [11]). The differentiation
of regular and irregular graphs can be done effectively,
as described in [11]. However, we note that existing
algorithms for mapping tasks with regular communi-
cation pattern also assume the target host machine has a
regular connection topology, such as 3D torus. Without
this assumption, even mapping a ring to a host machine
with an irregular connection topology is NP-complete.2
Thus, we classify the initial mapping into two types
of problems based on the application communication
pattern and the host machine topology.
• Type I problem: the supernode graph is of some
regular pattern and the topology of the target host
machine is also regular.
2It can be reduced to the traveling salesman problem.
• Type II problem: Problems other than the type I
problems.
For the type I problems, there exist many efficient
algorithms [11], [23]. Here we only consider the al-
gorithm for the type II problems. The goal is to find
the same dendrogram for the computation nodes as the
task partitioning.
Here we apply the spectral graph algorithm again
to partition the compute nodes (processing units) into
(processor) blocks. First is to define the connection
matrix W of the computation nodes on the target
machine. Based on the latency t(i, j) between node
i and node j, we define the (i, j) element of W as
w(i, j) = e−αt(i,j) (11)
for some parameter α that normalizes the latency. Thus,
if the communication latency t(i, j) is large, we say
node i and j are less connected.
With matrix W , we can compute matrix D and
matrix LN , as defined in (7) and (9) respectively.
Similarly, we use the eigenvector z2 of LN to guide
the partition. The difference between Algorithm 1 and
Algorithm 2 is each partition needs to match the
associated dendrogram of tasks. To do that, we sort
the elements of z2, and select a threshold δ that can
partition the elements of z2 into two partitions with the
desired sizes.
The algorithm for type II problems is sketched in
Algorithm 2. For n supernodes, the time complexity is
O(n2), since W is a dense matrix.
Algorithm 2 Algorithm for initial mapping
1) Compute the connection matrix W using (11)
2) Form LN = D
− 1
2 (D − Wˆ )D− 12 , where D is
defined in (7).
3) Compute the eigenvector z2 of LN associated to
the smallest nonzero eigenvalue.
4) Select a threshold δ such that nodes can be
partitioned into specified size.
5) Repeat 2 to 4 until the partitions match the
dendrogram of task partitions.
C. Fine Tuning
The hop-byte metric does not only provide a mea-
surement to the quality of a mapping, but also can be
used as the objective function in the fine tuning step.
0 100 200 300 400 500 600 700 800 900 1000
0
100
200
300
400
500
600
700
800
900
1000
nz = 2048
Figure 2. The communication pattern of PDGEMM.
in the first eight columns and the nodes in the last
eight columns form a cluster; and the nodes in the
middle 16 columns are in a cluster. Since this grid is
with periodic boundary, nodes in the first cluster are
indeed neighbors. This example shows the normalized
cut algorithm works for regular communication pattern.
The element values of z2, the eigenvector corre-
sponding to the second smallest eigenvalues in the
normalized Laplacian matrix, are plotted in Figure
3(b). The ith element of z2 with value yi are marked at
(i, yi). The partitioning threshold δ is 0. Thus, nodes
with the corresponding elements positive are parti-
tioned into a group; and so are nodes corresponding
to negative elements.
2) AMG2006: The second application is
AMG2006, which is one of the tier 1 codes in
the ASC Sequoia Benchmark. It is a parallel algebraic
multigrid solver for linear systems arising from
problems on unstructured grids. The parallelization is
done by using MPI. The MPI communication can take
most of the time as the size of the problem increases.
Figure 4(a) and Figure 4(b) display the communication
pattern of the test program for 128 nodes and 256
nodes, in which only messages with size larger than
500,000 bytes are shown. Each blue dot at (i, j)
represents the communication from node i to node j,
and the number nz means the total number of blue
dots. It can be seen that although the communication
occurs mostly in the neighborhood, the communication
pattern is not regular nor symmetric. In addition, the
degree of each node is not small, which increases
the difficulty of mapping the application to a host
0 10 20 30
0
5
10
15
20
25
30
 
 
cluster 1
cluster 2
(a) The partitions given by Algorithm 1
0 200 400 600 800 1000 1200
−0.05
0
0.05
 
 
cluster 1
cluster 2
(b) The 1,024 elements of z2 of the normalized Laplacian
matrix for PDGEMM.
Figure 3. The task partitioning by Algorithm 1 for PDGEMM.
machine whose compute nodes have interconnection
with three or fewer dimensions.
The power of the normalized cut algorithm is ex-
hibited particularly to the irregular communication
pattern, like the ones generated by AMG2006. Figure 5
displays the elements of z2 for a 512 nodes AMG2006
application, and the partitioning made by Algorithm 1.
A clear gap between element 256 and 257 can be seen
in the figure. Although we do not need to partition the
clusters equally, for this application, the normalized cut
algorithm happens to partition tasks into equally sized
groups.
B. Supernode Size
The experiments in this subsection are concerned
with the relation of the supernode size and the quality
of mapping. In Section III-C, we had pointed out that
if the size of a supernode is large, the optimization for
the intra supernode will be expensive. On the contrary,
if the number of nodes in a supernode is too small,
-15%
-10%
-5%
0%
5%
10%
15%
20%
25%
8 64 512
Supernode Size
Im
p
ro
v
e
m
e
n
t
(a) PDGEMM
-8%
-6%
-4%
-2%
0%
2%
4%
6%
8%
8 64 512
Supernode Size
Im
p
ro
v
e
m
e
n
t
(b) AMG2006
Figure 6. Different supernode sizes for partitioning
further partitions the supernodes in level 1 into 8 small
supernodes with 8 tasks in each small supernode. The
level 3 hierarchy partitions 4,096 tasks into 8 large
supernodes with 512 tasks in each large supernode.
Then each 512-task large supernode is partitioned into
8 supernodes with 64 tasks in each supernode. Finally
each 64-task supernode is partitioned into 8 small
supernodes with 8 tasks each in each small supernode.
The fine tuning for the hierarchy level 2 and 3 applies
the optimization method recursively, from the top level
to the individual tasks. For instance, for the level 3
hierarchy, we optimize the mapping by exchanging the
top level large supernodes of size 512, then the second
level supernodes of size 64, then the third level small
supernodes of size 8, and finally the tasks within the
third level small supernodes.
The experiment results are shown in Figure 7. For
AMG2006, the quality of mapping drops with the
depth of hierarchy. Only the mapping produced by the
level 1 hierarchy improves the communication perfor-
mance. For PDGEMM, the trade-off relation between
mapping quality and depth of hierarchy breaks down at
level 3. Since the communication pattern of PDGEMM
0%
5%
10%
15%
20%
25%
1-level (512) 2-level (64-8) 3-level(512-64-8)
Supernode Hierarchy
Im
p
ro
v
e
m
e
n
t
(a) PDGEMM
-4%
-2%
0%
2%
4%
6%
8%
1-level (64) 2-level (64-8) 3-level(512-64-8)
Supernode Hierarchy
Im
p
ro
v
e
m
e
n
t
(b) AMG2006
Figure 7. Different levels for partitioning
is regular, it might be the restriction of deeper hierarchy
limits the possible disruption made by the fine tuning.
D. Supernode Geometry
In this part of experiments, we want to explore
the relations between the supernode geometry and the
quality of mapping. The supernode geometry means
how a supernode is shaped on the target host machine.
For instance, if a supernode is of size 16, and will
be put onto a block of compute nodes with a 3D torus
interconnection, its geometry can be 1×1×16, 1×2×8,
1×4×4, or 2×2×4 (assuming the rotation invariance).
In the experiments, we fixed the supernode size to 64
and selected four geometries of supernodes for initial
mapping: namely 16×4×1, 16×2×2, 8×8×1, and 4×
4×4. The results for AMG2006 are presented in Figure
8. We observed that the results may not be totally
random, but can be reasoned by the relation between
the two numbers: AAMSII and IILR. The AAMSII
is the ratio between the average accumulated message
size of the intra-supernode communication and that
of the inter-supernode communication. The IILR is
the ratio between the number of intra-(processor)block
VI. CONCLUSION
In this paper we present a hierarchical mapping
algorithm that maps the tasks of a parallel application
onto processors. The hierarchical mapping algorithm
first partitions the tasks based on the communication
profile into supernodes. Then the algorithm partitions
the compute nodes on the host machine into processor
blocks and performs the initial mapping. After the
initial mapping, each supernode is further fine tuned
by using the local search method to explore possi-
ble improvement. The results are verified by using
the PDGEMM, a matrix computation kernel, and the
AMG2006, a tier 1 application from the Sequoia
benchmark.
As discussed in the paper, the mapping problem
is getting more complicated as the scale of the high
performance computer systems goes up. An efficient
and dynamic mapping method is needed to assist
users utilizing the interconnection of the system. The
mapping algorithm proposed provides a viable solution
that is scalable. It is dynamic in such a way that
run-time point-to-point communication information is
taken into consideration for mapping.
For the future work, we plan to improve the HMA
by trying other graph partitioning algorithms for task
partitioning and more optimization methods for the
fine tuning. In addition, we plan to evaluate the HMA
using more applications with different communication
patterns. Furthermore, we plan to apply the HMA
to computing systems with heterogeneous intercon-
nections. (e.g., clusters where the link latencies are
different).
REFERENCES
[1] The AMG Benchmark.
https://asc.llnl.gov/sequoia/benchmarks/#amg.
[2] The Community Climate System Model (CCSM).
http://www.cesm.ucar.edu/models/ccsm4.0/.
[3] The Sequoia Benchmark.
https://asc.llnl.gov/sequoia/benchmarks/.
[4] Space-Filling Curves. Springer-Verlag, 1994.
[5] Masood Ahmed and Shahid Bokhari. Mapping with
space filling surfaces. IEEE Trans. Parallel Distrib.
Syst., 18:1258–1269, September 2007.
[6] Romas Aleliunas and Arnold L. Rosenberg. On
embedding rectangular grids in square grids. IEEE
Transactions on Computers, 31(9):907–913, September
1982.
[7] Zhaojun Bai, James Demmel, Jack Dongarra, Axel
Ruhe, and Henk van der Vorst. Templates for the So-
lution of Algebraic Eigenvalue Problems: a Practical
Guide. SIAM, 1987.
[8] Satish Balay, Kris Buschelman, William D. Gropp,
Dinesh Kaushik, Matthew G. Knepley, Lois Curfman
McInnes, Barry F. Smith, and Hong Zhang. PETSc
Web page, 2001.
[9] G. Bhanot, A. Gara, P. Heidelberger, E. Lawless, J. C .
Sexton, and R. Walkup. Optimizing task layout on the
blue gene/l supercomputer. IBM Journal of Research
and Development, 49(2):489–500, March 2005.
[10] Abhinav Bhatele´, Eric Bohm, and Laxmikant V. Kale´.
A case study of communication optimizations on 3d
mesh interconnects. In Euro-Par ’09: Proceedings of
the 15th International Euro-Par Conference on Paral-
lel Processing, pages 1015–1028, Berlin, Heidelberg,
2009. Springer-Verlag.
[11] Abhinav Bhatele, I-Hsin Chung, and Laxmikant V.
Kale. Automated mapping of structured communica-
tion graphs onto mesh interconnects. 2010.
[12] L. S. Blackford, J. Choi, A. Cleary, E. D’Azevedo,
J. Demmel, I. Dhillon, J. Dongarra, S. Hammarling,
G. Henry, A. Petitet, K. Stanley, D. Walker, and R. C.
Whaley. ScaLAPACK Users’ Guide. Society for
Industrial and Applied Mathematics, Philadelphia, PA,
1997.
[13] Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. Introduction to Algorithms.
McGraw-Hill, 1990.
[14] John A. Ellis. Embedding rectangular grids into square
grids. IEEE Trans. Comput., 40(1):46–52, 1991.
[15] M. Fiedler. Algebraic connectivity of graphs.
Czechoslovak Mathematical Journal, 23(98), 1973.
[16] S. L. Graham, M. Snir, and C. A. Patterson, editors.
Getting Up To Speed: The Future Of Supercomputing.
National Academies Press, Washington, D.C., 2005.
[17] J. Kleinberg and E. Tardos. Algorithm Design. Addison
Wesley, 2005.
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
■ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱  計 畫 編 號  
報 告 人 
姓 名 
史伯其 服 務 機 構 
及 職 稱 
清華大學資工系博士生 
會議/訪問時間 
 地點 
2011.5.23~2011.5.26 
Newport Beach, CA, USA 
會 議 名 稱 The 11
th IEEE/ACM International Symposium on Cluster, Cloud and Grid 
Computing (CCGrid2011) 
發表論文題目 A performance goal oriented processor allocation technique for centralized heterogeneous multi-cluster environments 
一、主要任務摘要（五十字以內） 
The CCGrid conference is aimed on the issue from traditional cluster computing, grid 
computing to nowadays cloud computing. My mission is to present our paper entitled “A 
performance goal oriented processor allocation technique for centralized heterogeneous 
multi-cluster environments” as well as sharing research experience with many international 
researchers in this filed. 
二、經過 
The 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing is 
hold at Newport Beach, CA, USA from May 23 to 26. I present my paper as a poster at the 
“Demos and Posters Reception” session on May 24. More than ten persons are interested in 
my work and we have many discussions. In addition, I attend many other sessions include 
the GPU-based Computing, Grid and Cloud Computing, Autonomic Cloud Computing 
Panel, Resource Scheduling on the Cloud, and all the keynote speaking. Following pictures 
are taken from the conference. 
 
A performance goal oriented processor allocation technique for centralized 
heterogeneous multi-cluster environments 
 
Po-Chi Shih 
Dept. of Computer 
Science 
NTHU 
Hsinchu, Taiwan 
shedoh@sslab.cs.nthu.e
du.tw 
Kuo-Chan Huang 
Dept. of Computer and 
Information Science 
NTCU 
Taichung, Taiwan 
kchuang@ntcu.edu.tw 
Che-Rung Lee 
Dept. of Computer 
Science 
NTHU 
Hsinchu, Taiwan 
cherung@cs.nthu.edu.t
w 
I-Hsin Chung 
IBM T.J. Watson 
Research Center 
Yorktown Heights 
NY 10598 
ihchung@us.ibm.com 
Yeh-Ching Chung 
Dept. of Computer 
Science 
NTHU 
Hsinchu, Taiwan 
ychung@cs.nthu.edu.tw 
 
 
Abstract—This paper proposes a processor allocation 
technique named temporal look-ahead processor allocation 
(TLPA) that makes allocation decision by evaluating the 
allocation effects on subsequent jobs in the waiting queue. 
TLPA has two strengths. First, it takes multiple performance 
factors into account when making allocation decision. Second, 
it can be used to optimize different performance metrics. To 
evaluate the performance of TLPA, we compare TLPA with 
best-fit and fastest-first algorithms. Simulation results show 
that TLPA has up to 32.75% performance improvement over 
conventional processor allocation algorithms in terms of 
average turnaround time in various system configurations. 
Keywords-multi-cluster; look-ahead; processor allocation 
I.  INTRODUCTION 
This paper focuses on the processor allocation issues in 
centralized heterogeneous multi-cluster (CHMC) system. A 
CHMC system consists of a collection of interconnected 
clusters and a central job manager. Each cluster has 
homogeneous processors while the number and the speed of 
processors in different clusters may be different. The central 
job manager entails two tasks: job scheduling and processor 
allocation. Job scheduling determines the execution order of 
the submitted jobs, while processor allocation assigns the job 
to a set of available processors for execution. Job submission 
is in an on-line manner, which means the job manager has no 
information of future job submissions. Each job can be 
sequential (runs on single processors) or parallel (executed 
on multiple processors simultaneously) and there is no 
dependency among the jobs. Each submitted job needs to 
specify the number of required processors and estimated job 
runtime. 
Processor allocation methods in CHMC can be classified 
into three categories, which are single site allocation [1], 
multi-site co-allocation [2], and adaptive allocation [3]. This 
paper focuses on proposing a new single site allocation 
algorithm in CHMC. In such environments, spatial 
fragmentation of available processors and speed 
heterogeneity among clusters are two major performance 
issues. Conventional Best-Fit (BF) [4] and Fastest-First (FF) 
[5] algorithms are designed to cope with spatial 
fragmentation and speed heterogeneity respectively. Their 
performance is unstable and largely depends on the workload 
and system configurations. In this paper, we propose a 
processor allocation technique, called temporal look-ahead 
processor allocation (TLPA), to take both spatial 
fragmentation and speed heterogeneity into consideration. 
Given a target waiting job to be allocated, the design 
philosophy of TLPA is to find an allocation for the target job 
such that this allocation will result in the best overall 
performance for all waiting jobs(include the target job). 
In the CHMC system, processor allocation algorithms 
need to work together with job scheduling algorithms. With 
different job scheduling approaches, it requires some 
adaptions to utilize TLPA into processor allocation decision. 
To demonstrate the capability of TLPA, we propose 
TLPA_BJS, which is a TLPA-based processor allocation 
algorithm designed to work with basic job scheduling 
algorithms such as First-Come-First-Served (FCFS) or 
Shortest-Job-First (SJF). The proposed TLPA technique and 
TLPA_BJS processor allocation algorithm will be covered in 
next section. 
II. TLPA TECHNIQUE AND TLPA_BJS ALGORITHM 
Every algorithm that utilizes TLPA into processor 
allocation decision needs to specify a scoring function. The 
scoring function takes four inputs: 
 j: the job to be executed. 
 c: the cluster to be simulated for allocation. 
 d: the simulation depth, which is a positive integer 
indicating the maximum number of subsequent 
waiting jobs to be simulated when calculating score. 
 p: the performance metric to optimize. 
and outputs a numerical value, called score. This score 
represents the expected performance in terms of p for those 
d+1 jobs (job j and d subsequent waiting jobs) if job j is 
allocated to cluster c. This paper focuses on the performance 
metric p=average turnaround time (ATT) which is defined as 
 
jobs ofnumber  total
 job  

i ii
submitTimeendTime
ATT  (1) 
The score is calculated by averaging the expected turnaround 
time of those d+1 jobs in the simulation procedure. 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
□ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱 普及健康格網: 以格網為基
礎之個人健康服務系統(3/3)
計 畫 編 號
NSC2218-E-007-001 
報 告 人 
姓 名 陳瑞順 
服 務 機 構
及 職 稱中國科技大學/教授 
會議/時間 
 /地點 
IEEE 2011 電腦設計與應用國際會議/2011/5/27-5/29/中國.西安 
會 議 名 稱 
IEEE 2011 電腦設計與應用國際會議(ICCDA 2011) 
IEEE 2011 3rd International Conference on Computer Design and 
Applications 
發表論文題目 Using Data Mining Technique to Improve the Manufacturing Yield of LCD Industry 
 
一、 主要任務摘要（五十字以內） 
這次參加 IEEE ICCDA 2011 會議之中，不僅能夠快速了解目前世界上各個實
驗室的研究趨勢，也看到了不同領域的研究成果，並藉此吸取他人的經驗。同時
在全體的報告過程中，激發出許多新的研究想法，從中也了解到自己研究長處以
及一些研究的盲點，在此除了感到獲益良多之外，也特別要感謝國科會在經費上
的補助。 
 
二、對計畫之效益（一百字以內） 
參加此類國際知名大型研討會，不僅可以了解世界上相同領域的研究方向以
及研究深度之外，並且能提升研究水準與拓展國際視野，因此有志於長期研究的
學者更應鼓勵多參與國際性的研討會，藉由和與會的學者互相討論也可以激發出
不錯的研究想法對計畫之效益。所以，若能夠持續在經費上得到國科會的適當補
助，對於研究學者將會有極大的幫助。 
 
三、經過 
今年IEEE ICCDA 2011研討會議時間是在2011 年5 月27 日至5 月29 日期間
展開，其地點為中國.西安，因此我們在會議當天(即5 月27 日)搭乘班機於下午1
點30 分由桃園國際機場起飛，經過廣州轉機5個半小時左右，即抵達中國.西安機
場，接著我們搭乘往飯店休息。隔天我們即前往會場報到註冊，並展開為期3 天的
研討會。 
 
會議主題涵蓋15 主題。此次會議有歐、美、亞、非、等地區國專家學者出席
發表文章，其中來自亞洲地區的學者包括日本、韓國、大陸、台灣。 
    大會內容主要分15 部分發表論文，每一天有論文發表，其中 5月28 日開始 
連續2天, 早上與下午各有平行發表論文。 
    本次會議中，與會學者對台灣一般研究成果亦相當肯定，論文發表亦因此獲得
許多寶貴的意見。 
5. Artificial Intelligence  
Machine Learning  
Pattern Recognition  
Knowledge Discovery  
Intelligent Data Analysis  
Neural Networks  
Genetic Algorithms  
Medical Diagnostics  
Data Mining 
Support Vector Machines  
Machine Vision  
Intelligent Systems and Language  
 
6. Computer Science  
Numerical Algorithms and Analysis  
Computational Simulation and Analysis  
Data Visualization and Virtual Reality  
Computational Mathematics  
Computational Graphics  
Computational Statistics 
Scientific and Engineering Computing  
Parallel and Distributed Computing  
Grid Computing and Cluster Computing  
Embedded and Network Computing  
Signal and Image Processing  
CAD/CAE/CAM/CIMS  
 
本人發表Using Data Mining Technique to Improve the Manufacturing Yield of 
LCD Industry方面的paper 屬於computer science，有各種不同的方法，問題探討, 並
帶回 one CD-disk及一本論文集. 
 
五、建議與結語 
    此次在中國.西安，參加此次國際會議後，深覺國內的研究方向和技術不輸國
外，如何達成人才永續培育，資源充分利用，有待大家共同努力。 
 
六、攜回資料 
由於今年ICCDA 2011，所以當天註冊完只有一片CD的光碟，及一本論文集其
內容是收錄所有論文集。 
 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
      
 
 2
implies the presence of another pattern.  
e. Classification: Given a set of predefined 
categorical classes, determine to which of 
these classes a data item belongs. 
f. Clustering: Given a set of data items, partition 
this set into a set of classes such that items 
with similar characteristics are grouped 
together. Clustering is the best used for 
finding groups of items that are similar. 
  
C. Data Mining Methodology 
A variety of techniques are available to enable 
the above goals. The most commonly used 
techniques can be categorized in the following 
groups[2], [3], [4], [5], [6]: 
a. Statistical Methods: Historically, statistical 
work has focused mainly on testing of 
preconceived hypotheses and on fitting 
models to data. 
b. Decision Trees: A decision tree is a tree 
where each non-terminal node represents a 
test or decision on the considered data 
item. Depending on the outcome of the test, 
one chooses a certain branch. 
c. Case-Based Reasoning: Case-based 
reasoning (CBR) is a technology that tries 
to solve a given problem by making direct 
use of past experiences and solutions. 
d. Rough Sets: A rough set is defined by a 
lower and upper of a set. Every member of 
the lower bound is a certain member of the 
set. Every non-member of the upper bound 
is a certain non-member of the set. 
e. Neural Networks: Neural networks (NN) 
are a class of systems modeled after the 
human brain. Like in the human brain, the 
strength of neuron interconnections may 
change in response to a presented stimulus 
or an obtained output, which enables the 
network to “learn”. 
f. Bayesian Networks: Bayesian belief 
network (BBN) are graphical 
representations of probability distributions, 
derived from co-occurrence counts in the 
set of data items. Specifically, a BBN is a 
directed, acyclic graph, where the nodes 
represent attribute variables and the edges 
represent probabilistic dependencies 
between the attribute variables.. 
 
3. Algorithm 
Firstly, the model of the data should be 
defined. The first step is to abstract the data. In 
the Array manufacturing process is divided into 
five masks. The machines are divided into seven 
groups: PVD, CVD, PHO, Wet Etch, Dry Etch, 
Strip, and Clean, along with other 
supplementaries. Panels in the line defect group 
are unacceptable to customers and would impact 
profoundly on yield. Dot defect group affects 
product rank is conditionally acceptable by 
customers and has a lighter impact on yield. 
Machines in each manufacturing step have great 
influence on the cause of defect. For example, 
two line defects and one dot defect are found in 
one substrate after 5 manufacturing steps. The 
causes of defect are from 3 different steps and 
are associated with 3 machine groups. Since 
manufacturing machines in the same group can 
be used in different mask process at the same 
time, all the numbering of cause of defect could 
be simplified as 1.n to represent each machine 
group; to further distinguish machine in which 
process, value n can be re-defined by masks. As 
defect type is the value generated by 
examinations, all defective types and causes can 
be combined to obtain a 3 dimensional structure. 
The machine ID from history data then becomes 
the attribute value of multidimensional structure. 
  Association rule describes the 
combinations of data properties and relationships 
in statistics. Its general form: X1, X2… Xn =>Y, 
it means using combinations of X1, X2…Xn to 
predicting Y. The question to be discussed would 
be the relationship between the combination of 
machines in array substrate production line in 
the TFT-LCD panel factory and its yield. This is 
an application of Association rule.The definition 
of support and confidence can be explained as 
following: assuming there is an A, which can be 
an item, Ai, or a combination of items, Ai∪…∪
Aj, while Ai… Aj are all items from the data (or 
column). Support of A is the sum of data 
transaction divided by the number of data 
including A. The Confidence of A→B is the 
probability of B occurred at the same time while 
A occurred.  
 Support(A→B)=P(A ∩ B) 
 Confidence(A→B)=P(A|B)= P(A ∩ B)/P(A)  
Association algorithm procedures 
Data mining engine was built based on 
Association model, while Association rule uses a 
multidimensional Cube to store serial number of 
machines and the quantity of array substrate 
passing each manufacturing station. The 
procedures of data mining process in Association 
algorithm are as followed.  
Step 1: Original data is centrally stored 
according to data model defined by Cube in data 
warehouse  
Step 2: One item is selected from machine group, 
mask type, or serial number as the variable and 
the rest are kept as constants to find the 
combination with higher defect rate. 
(i, j, k) represent three variables. At this 
stage, under the (0, j, k), (k, 0, k), (i, j, 0), the 
combination creating the highest defect rate is 
identified along with its Support and Confidence 
values. 
Step 3: Two items are selected from machine 
 4
 
Figure 1. Yield Improvement Passage 
 
 
Yield rate improvemet trend
95.3%
90.7%
9.2
8.5
7.4
6.1 6.5
7.5
6.8
6.0 5.5 5.8 5.5
4.9 4.9 4.7
0.0
2.5
5.0
7.5
10.0
12.5
15.0
6/16 6/23 6/30 7/7 7/14 7/21 7/28 8/4 8/11 8/18 8/25 9/1 9/8 9/15
fa
ilu
re
 in
de
x
70.0%
75.0%
80.0%
85.0%
90.0%
95.0%
100.0%
yi
el
d 
ra
te
PRO
WET
PHO
CVD
PVD
良率
指數
加坡、芬蘭和中國等國家的學者和相關學術、研究人員的國際平台，互相交換、
分享各種研究構想、方法和實驗結果。而且是兼顧「理論」和「實務」的層面來
探討相關的議題，可以促成在學術上和專業上的貢獻有更具體、實際應用的研究
成果，所以有學術上的地位和重要性。 
ICCS2011 國際研討會的規模不大，但是有來自數十個國家的 60 篇投稿論
文，接受的篇數是 23 篇，接受率為 26%。會議在國立蒙古大學舉行，一共有四
個 Sessions分在兩天進行論文發表，涵蓋的領域包括:  
· 1: Cloud Computing 
· 2: Fault Tolerant Computing  
· 3: Biometrics Technologies  
· 4: Biometrics Performance Evaluation   
· 5: Biometrics Applications   
· 6: Information Security in Internet   
我們的論文”一個無所不在的個人健康管理系統(An Omnipresent Personal 
Health Management System)”在 7月 10日下午 Session C發表，由 Session Chair, Dr. 
Ching-Shoei Chiang 主持，除了報告我們的研究成果以外，也回答一些與會者提
出的問題，包括實驗上和未來實際運用上可以繼續延伸的疾病種類，也就是除了
心血管疾病以外，能夠提供糖尿病等慢性病人的健康管理系統，都有在我們未來
發展的考慮範圍之內。所以口頭報告的部分，很順利圓滿的達成任務。此外，也
聽了其它優秀的論文發表和問題討論，整體而言，可以說是一次成功的學術研討
會。 
 
四、心得 
非常感謝國科會對於參加本次國際學術研討會議的補助，使得個人收獲良
多。除了學術專業上能和其他國家的學者、專家共同探討相關研究課題以外，還
An Omnipresent Personal Health Management 
System 
 
Wei Chen, Don-Lin Yang 
Dept. of Information Engineering and 
Computer Science 
Feng Chia University, Taichung, 
Taiwan 
 
Ming-Chuan Hung 
Dept. of Industrial Engineering and 
Systems Management 
Feng Chia University, Taichung, 
Taiwan 
Jungpin Wu 
Dept. of Statistics 
Feng Chia University, Taichung, 
Taiwan 
 
Abstract—Despite many technological advances made in modern 
society, people in remote areas are still facing severe problems with 
lack of resources, especially in the area of healthcare. Those that live 
long distances from large cities or in areas where proper healthcare 
is not available often receive inadequate care. To improve the quality 
of healthcare services, we propose an omnipresent personal health 
management system based on ICT technology and Grid. This system 
is also beneficial to people who live in metropolitan areas as well as 
home-care patients. Using our system, patients can record and track 
their health information, consult doctors and receive advice at any 
time via the Internet. Doctors can monitor the progress of their 
patients' recovery from an illness and help the patients learn how to 
manage their own health. Based on the overall evaluation, doctors 
can determine if a patient should return to the hospital or go to a 
nearby facility for additional care.  After examining the patient's data, 
the system can provide users with healthcare advice and disease 
prevention measures as prescribed by doctors. Emergency alerts will 
be issued if abnormal symptoms are detected by monitoring the 
patient's physiological data. Our system also tries to reduce the time 
and cost of hospital visits and improve the quality of healthcare 
overall. 
Keywords-component; Telemedicine, Grid, Healthcare, Data 
mining, Omnipresent 
I.  INTRODUCTION  
With the advancement of the economy and technology 
development, the need for better and more personal services 
like quality healthcare becomes apparent. For instance, 
HealthVaulty from Microsoft can let users’ PC, health or 
fitness device connect to the HealthVault database and get the 
personal health report, analytic result, and some specific 
information through online health tools. It’s the typical 
characteristic of personal health service or the customized 
functions of medical management systems. 
In addition to providing preferred personal service, tele-
medical service is one of the discussed issues regarding to the 
service of medical organization for remote patients. The 
conference of eHealth 2009 [1] considered advances in 
telemedicine. The 2010 conference of Taiwan’s application of 
the cloud computing in the medical and health industry said 
that, one of the three targeted applications was to strengthen the 
telemedicine industry. Taking Taiwan for example, most off-
shore islands, mountain areas and country sides fall short of 
medical resources. Once the local residents get a serious illness, 
they need a lot of effort to reach the hospital in the big cities at 
Main Island and may miss the time for the best treatment. 
Telemedicine may not solve this problem completely, but some 
time and cost could be saved. Better yet, proper preventive 
measures provided by tele-medical services may reduce 
medical emergency while more information are available for 
care takers in case of emergency. 
To reduce the disadvantage of the remote area residents due 
to the long distance and short of resources, we want to build a 
personal health recording and resource sharing platform based 
on the Grid technology to provide the personal health 
management like those in any large city. The platform is called 
Health Grid [2] evolved from the Medicare Grid [3]. We 
collect medical histories, trace personal health records, and 
integrate various activity data to build the databases or data 
warehouse of Health Grid. Then we use data mining [5-12] to 
analyze the data from the database of Health Grid and produce 
reference data for the doctors in their consultation. More 
importantly, except the physical medical facilities, we want to 
provide the same medical service no matter if the patient is in 
the big city or not. In the same vein, patients and their families 
can retrieve the data for decision making at any time and any 
place. It is expected to reduce the high medical cost and 
increase the quality of healthcare.  
The Health Grid platform provides recording and tracing 
health information with any devices including e-TexcareR 
smart shirt [4] as shown in Fig. 1, which can record the 
physiological data of a person wearing the shirt. With e-
TexcareR, doctors can find out the problem of patients 
immediately and instruct them what to do through the Health 
Grid platform. Such that the patient may get medical service 
without visiting the hospital as long as they have the access to 
the Health Grid platform. However, if a hospital visit is needed, 
timely and well-prepared medical treatment can be arranged at 
the medical center or emergency care will be dispatched in a 
more efficient manner. 
To better promote the use of the Health Grid platform, we 
surveyed the 10 top causes of death released by the Department 
of Health in Taiwan, and decided to focus on the health issues 
of the cardiovascular acute disease, which is related to 3 of 
these 10 causes of death. First, we need to find the necessary 
Cardiovascular disease is our main topic of research, so we 
collect, track and integrate the data regarding latent factors, 
preventing methods, care approaches and medical treatments. 
After analyzing a patient’s EHR, our system can provide some 
references for a doctor to make consultation. This process 
works for inpatient, outpatient, or home patient where people 
are not confined to the hospital. Instead, the patients can use 
the Health Grid platform to manage their health status while 
they stay at home or go outdoors at any communication-
reachable places. For example, people wearing smart shirts can 
go anywhere as long as their physiological data can be 
monitored. The system can be accessed using mobile phones, 
PCs or any devices connected to the Internet no matter where 
one’s physical location or environment is. 
Figure 3.  System Architecture 
Before the system design, we collect formats of health 
records and various medical forms including the reference data 
from the doctor on the consultation session. We design the 
database schemas based on HL7 [20] and discuss with doctors 
on the functional and non-functional requirements. Based on 
the architecture of the Health Grid, we construct SQL database 
on the platform and develop user interfaces to provide the 
services as discussed. In the following we introduce the system 
architecture, system design and database design in details. 
A.  System Architecture 
As shown in the system architecture of Fig. 3, the PC in 
every patient’s home is a node of the Grid, called a Home 
Server (HS). A set of Personal Health Management System and 
a database is installed at each HS. Patients’ EHR are stored in 
the database of HS. When an HS enters the Grid, the 
environment distributes the HS to a Home Grid. The Region 
Server (RS) and Boot Strap manage the Home Grid and its HS 
set. Initially, four region servers are built to cover the four 
major areas of Taiwan. After passing login verification on the 
Login Server, patients can access their HS through RS and 
Boot Strap to get their EHR or other services. Doctor to patient 
and patient to patient communications can be established easily 
in our platform. Family members or care takers are also given 
the access to the system with proper authorization for 
monitoring or reminding patients of their health condition and 
the importance of following the instruction from the doctor. 
For the security reason, users cannot access any HS if they 
don’t pass the verification by LS and Boot Strap. If a user 
account has multiple logins at different nodes at the same time, 
the system will inform the user right away. If a user idles over 
30 minutes, the system will ask the user to re-login. If an HS is 
disconnected or gets no response, the system activates the 
backup and restore mechanism. The backup process makes a 
copy for every HS periodically, and the restore simulates the 
previous HS according to the backup process. 
B. System Design 
 From the user perspective, we have three types of users: 
administrator, doctor and patient. The definition of a patient 
includes his/her family members and authorized care takers. 
An administrator is responsible for system execution and 
maintenance. A doctor needs to examine patients’ health 
records and provide consultation for the requests from patients 
via the system. If any patient requires medical attention in or 
out of hospital, doctor can get all necessary information 
immediately. The patients can wear smart suit to monitor their 
health condition or key in the physiological data. 
The following describes the interfaces for the patient and 
doctor: 
(1) Patient 
1. To login the first time, the system will ask the 
patient to create new account and enter the 
personal data. The data can be edited later. 
2. Family medical history data are entered during 
the new account creation. Family medical 
history can be inquired and updated as needed. 
3. Physiology and examination data are entered 
by the hospital. The patient can inquire the data 
only. The data will be updated after a new 
examination is done. 
4. Diagnosis data are entered by the hospital. The 
patient can inquire the data only. The system 
stores every diagnosis record.  
5. Treatment data are entered by the hospital. The 
patient can inquire the data only. The system 
keeps every treatment record.  
6. Medicine data are entered by the hospital or 
drug company. The patient can inquire the data 
only. The system stores every medicine record. 
7. Food data are entered by the patient. Food data 
can be inquired and updated. The record is 
stored in a daily basis. 
8. Exercise data are entered by the patient. 
Exercise data can be inquired and updated. The 
record is stored by period and the patient can 
decide the length of a period. 
  
 
database. A Friend List is used to record these friends 
including family members, friends, care takers, and 
doctors.  They are authorized to view the patient’s 
data. 
(3) Backup and  restore 
The Health Grid will backup the data in each HS 
periodically. The backup file is saved as an ISO image 
file. This file is used not only to recover the HS 
system during a restore, but also to become a 
simulated HS when the original HS is crashed. We 
use KVM [20] to simulate a virtual machine for the 
HS when no response is received.  
IV. SYSTEM IMPLEMENTATION 
The construction of Personal Health Management System 
includes software, hardware, smart suit, PC, server, mobile 
phone and the Internet to form a Health Grid. We use PHP, C# 
and SQL server to implement the system. At this time it is a 
prototype with part of the interfaces still under development. 
With the limit of space, we only show the main page and two 
of the nine functions in the following from the patient’s view.  
Figure 5.  The front page of Personal Health Management System  
As show in Fig. 3, every user needs to use their PC (or 
other device) to connect to the Portal of the Health Grid. After 
passing the login-verification with the HS access authorization, 
the user will enter the front page of Personal Health 
Management System as displayed in Fig. 5. 
For the registered users, they can use all the nine services 
through the link buttons on the main page. Examination Data 
will show the list of recent examinations and other detailed 
contents. From examination data, the user can check what 
treatments and medicines being provided, where these three 
parts can be displayed immediately. To know more about the 
treatments, the system can show medicine prescription and 
provide reminding messages for taking drugs at proper times. 
The consulting area is the place of communication for the 
doctor and patient. First, the system will show the recent 
consulting list for viewing. If the user wants to request a new 
consultation, the system provides a user friendly interface as 
shown in Fig. 6. The user simply makes the selection from the 
provided lists very easily and quickly. Text can also be entered 
if necessary. The doctor also uses the Consulting Area to return 
responses. Fig. 7 is the picture of food data area. The user can 
enter what he/she eats every day, and the search function can 
help the user or doctor to view the status of taking food during 
a certain period. Since entering the type of 5 nutrition groups is 
not convenient, we are trying another way of input so that the 
users just choose what they eat (ex: hamburger, sandwich, 
etc…) and do not need to know what kind of nutrition group 
the food belongs to. 
 
Figure 6.  Input Data in New Consulting Area  
 
Figure 7.  To Edit the Food Data  
 TABLE IV.   FAMILY MEDICAL HISTORY 
ACKNOWLEDGMENT 
This research was supported by the National Science 
Council, Taiwan, under grant NSC 98-2218-E-007-005. 
REFERENCES 
[1] eHealth 2009, http://www.ehealthnews.eu/content/view/1510/37/ 
[2] Ssu-Hsuan Lu, Kuan-Chou Lai, Don-Lin Yang, Ming-Hsin Tsai, Kuan-
Ching Li, Yeh-Ching Chung, “Pervasive Health Service System: 
insights on the development of a Grid-based personal health service 
system,” The 12th International Conference on e-Health Networking, 
Application & Services (IEEE Healthcom2010), pp. 61-67 , 2010, Lyon, 
France 
[3] Chia-Ying Hsieh, Kun-Che Lu, Kuo-Cheng Yin, Jie-Ru Lin, Don-Lin 
Yang, “A Study on the Grid-based Medical Decision Support System,” 
The Journal of Taiwan Association for Medical Informatics, Vol.17(3), 
pp. 1-16, Sep. 2008. 
[4] The Development of Multi-functional Physiological Measurement Smart 
Shirt at Feng Chia University, Taiwan, 
http://www.ord.fcu.edu.tw/DistinguishedResearch/SmartShirt.html, 
2008. 
[5] J. Yang, X. Yao, and C. Zhang, “Evolving Materialized Views in Data 
Warehouse,” Proc. of IEEE Congress on Evolutionary Computation 
(CEC’99), Chicago, USA, 1999. 
[6] H.B. Chang, J.L. Chu, P.T. Dai, S.J. Lin, and D.L. Yang, “Construction 
and Analysis of a Data Warehouse System for Customer Relationship 
Management,” The Fourth Conference on Electronic Business Theory 
and practice, Taiwan, 2003. 
[7] Lloyd-Williams M., “Case Studies in the Data Mining Approach to 
Health Information Analysis,” IEE Colloquium on Knowledge 
Discovery and Data Mining, Vol. 1, pp.1-4, 1998. 
[8] N. Benner, N. Herman,P.C. Pendharkar, J.A. Rodger, and G.J. 
Yaverbaum, “Association, Statistical, Mathematical and Neural 
Approaches for Mining Breast Cancer Patterns,” Expert Systems with 
Application, Vol. 17, pp. 223-232, 1999. 
[9] T. Lincoln, W. Panko, and J. Silverstein, “Technologies for Extracting 
Full Value from the Electronic Patient Record,” Proc. of the 32nd 
Hawaii Int’l Conf. on System Sciences, pp. 1-9, 1999. 
[10] Nada Lavrac, “Selected Techniques for Data Mining in Medicine,” 
Artificial Intelligence in Medicine, Vol. 16, pp. 3-23, 1999. 
[11] J.N.K. Liu and R.S.T. Lee, “iJADE eMiner - A Web-based Mining 
Agent based on Intelligent Java Agent Development Environment 
(iJADE) on Internet Shopping,” Advances in Knowledge Discovery and 
Data Mining, pp. 28-40, 2001. 
[12] Ilker Hamzaoglu, Hillol Kargupta, and Brian Stafford, “Web Based 
Parallel/Distributed Medical Data Mining Using Software Agents,” 
American Medical Informatics Association Fall Symposium, 1997. 
[13] R. Agrawal and R. Srikant, “Fast Algorithms for Mining Association 
Rules”, VLDB, pp. 487-499, 1994. 
[14] K. Ali, S. Manganaris, and R. Srikant, “Partial Classification using 
Association Rules,” Proc. of the 3rd Int’l Conference on Knowledge 
Discovery in Databases and Data Mining, Newport Beach, California, 
USA, 1997. 
[15] R.C. Dubes and A.K. Jain, Algorithms for Clustering Data, Prentice Hall, 
1988. 
[16] His-Chiang Li and Don-Lin Yang, “ Effective Preprocess for 
Incremental Mining of Association Rules with Dynamic User-Specified 
Supports,” The Second Taiwan Conference on Intelligent Technology 
and Applied Statistics, Taiwan, 2004. 
[17] B. Ozden, S. Ramaswamy, and A. Silberschatz, “Cyclic Association 
Rules,” Proc. 1998 Int’l Conf. on Data Eng. (ICDE ’98), pp. 412-421, 
1998. 
[18] J.R. Quinlan, “Induction of Decision Trees,” Machine Learning, vol. 1, 
no.1, p. 81-106, 1986. 
[19] R. Agrawal, M. Mehta, and J. Rissanen, “SLIQ: A fast scalable classifier 
for data mining,” Proc. of 1996 Intl. Conf. on Extending Database 
Technology (EDBT’96), 1996. 
[20] KVM, http://www.linux-kvm.org/page/Main_Page
 
太地區等各國專家學術交流的平台，集中展現當前的科學沿革、發展動向與研究成果。
並於10/31發表「Formation of σ-phase in 316L Stainless Steel Fiber Using a Multi-pass Cold 
Drawing Process」論文，探討不鏽鋼纖維表面形成的σ-phase特殊形態。 
 在場與會學者提出許多看法，進一步於會後與各地學者相互討論交流研究經驗與交
換心得意見，並對相關研究領域提出許多寶貴建議與想法，可做為後續研究之參考，
對於日後研究方向有相當大的助益。 
 
 
石天威教授於CTAC2011會場 石天威教授於CET會場 
 
五、建議與結語 
  感謝國科會補助計畫案。 
 
六、攜回資料 
 
中國紡織學術年會論文集與會議指南 國際工程與技術大會議程手冊 與全文電子檔光碟 
 
 
 
1  Introduction 
Strain-induce martensite transformation is a known 
non-diffusive phase transformation in stainless steel of 
austenitic series, which leads the variation of lattice 
and change the atomic coordinate positions. When a 
316L stainless steel wire is deformed plastically, it 
results to form the microstrain of the crystalline 
phases in metal[1]. The increment of microstrain 
between the crystalline phases results the broadening 
of the corresponding diffraction profile[2]. It is 
associated with the lattice plane spacing variation 
from one part of a grain to another[3]. X-ray diffraction 
(XRD) analysis is a well-established technique for 
identifying and analyzing microstructure of materials. 
According to the previous study [4], strain-induced 
martensite was formed during cold working process, 
and high dislocation density was substantially 
increased in austenite simultaneously. The dislocation 
density can be calculated from microstructural 
parameters. 
In this work the X-ray diffraction patterns of the fibers 
were analyzed by using Rietveld method. The 
crystalline phases of the fibers were identified and the 
root mean square microstrain (r.m.s. microstrain) of 
crystalline phases were measured.  
2  Experimental 
2.1  Materials 
A 316L as-received wire with the diameter of 20 µm 
was used in this study. The chemical compositions of 
316L stainless steel is shown in Table 1. The deviation 
of the compositions was ±5 wt%. As-received fiber 
was used and diminished the diameter to 12, 8, and 6 
µm, respectively, by cold drawing process.
 
表 1 316L 不銹鋼化學成分組成(wt%) 
Tab. 1 Chemical compositions of 316L stainless steel (wt%). 
Element 
C Si Cr Ni Mn Mo P S N Fe 
0.01 0.75 17.1 11.97 0.53 1.99 0.006 0.001 0.046 Balance 
 
2.2  Methods 
The microstructure of as-received fiber was 
characterized by Rigaku D/max 2550 PC X-ray 
diffractometer with Cu-Kα radiation, operating at 40 
kV and 300 mA. The angular range was from 20° to 
125° with 0.02° of step per second. The phase 
identification and microstructural analysis was done 
by using MDI Jade 5.0 and Rietveld method with 
Materials Analysis Using Diffraction (MAUD) 
software, respectively. The Rietveld analysis of X-ray 
diffraction patterns is allowed to measure the 
crystalline phase percentage and root mean square 
microstrain of crystalline phases. If the crystallite 
sizes and microstrains are isotropic, the slope of the 
linear Eq. 1 provides the microstrain state while the 
ordinate at origin gives the inverse of the mean size. 
For anisotropic crystallite sizes and/or microstrains, 
one has to plot one linear equation for each crystal 
direction (h) in order to reconstruct the anisotropy[5]. 
1) (           sin1cos λ
θ
ελ
θβ
h
h
h
T
+=  
in which βh is the sample contribution to the peak 
width in 2θ, Th is the mean crystallite size and εh the 
mean microstrain, all in crystal direction. 
The Rietveld method basically consists of modeling a 
large portion of the experimental XRD pattern with 
3.2  Root mean square microstrain of 
crystallite phases 
The r.m.s. microstrain of both austenite and martensite 
phases of samples was measured. Due to the small 
quantity of sigma phase, it has been neglected from 
this study. The r.m.s. strain of [111] and [220] in 
austenite phase were increased with a decrease of 
diameter of stainless steel fiber (see Fig. 3). Compare 
with austenite phase, [110], [200], and [211] in 
martensite phase have higher r.m.s. microstrain (see 
Fig. 4). The r.m.s. microstrain of [200] in martensite 
rose substantially when the diameter of 20 µm 
stainless steel fiber decreased during cold drawing 
process. The variation of microstrain in [200] 
martensite phase was attributed to strain-induced 
martensitic transformation. According to the previous 
study[10], the Bain model of γ → α' deformation is a 
continuous expansion of a bcc lattice along the [100] 
axis, the results suggested a possible path for the 
fcc-bcc martensitic transformation. The carbon atoms 
fit into the bcc cell at positions causing the lattice to 
elongate in this direction[11]. 
 
 
圖 3 不銹鋼纖維中沃斯田鐵相之殘留微應變 
Fig.3 Root mean square microstrain of austenite phase 
in stainless steel fibers. 
 
圖 4 不銹鋼纖維中麻田散鐵相之殘留微應變 
Fig.4 Root mean square microstrain of martensite 
phase in stainless steel fibers. 
4  Conclusions 
The strain-induced martensite transformation of 316L 
austenite stainless fiber occurred during a cold 
drawing process. Cold drawing led to accelerate 
precipitation of the sigma phase. The root mean 
square microstrain of martensite phase was higher 
than that of austenite phases due to expansion of a 
martensite (bcc) lattice. The results agreed with the 
Bain model which is a continuous expansion of a bcc 
lattice along the [200] axis. 
Acknowledgment 
This research was supported by the National Science 
Council of Taiwan, grant number NSC 
97-2221-E-035-026-MY3. 
References 
[1] SHYR Tien Wei, SHIE Jing Wen, HUANG Shih Ju, et al. 
Phase transformation of 316L stainless steel from wire 
to fiber[J]. Materials Chemistry and Physics, 2010, 122: 
273–277. 
[2] CULLITY B D, STOCK S R, Elements of X-ray Diffraction 
[M]. 3rd ed., Prentice Hall, New Jersey, 2001. 
Formation of σ-phase in 316L Stainless Steel Fiber 
Using a Multi-pass Cold Drawing Process 
 
Shih-Ju Huang 
Department of fiber and composite materials, Feng Chia University 
Taichung, Taiwan, R.O.C. 
P9887908@fcu.edu.tw 
Tien-Wei Shyr 
Department of fiber and composite materials, Feng Chia University 
Taichung, Taiwan, R.O.C. 
twshyr@fcu.edu.tw
 
 
Abstract—316L austenitic stainless steel fibers with 50, 34, and 
20 µm diameter were produced from 190 µm diameter by 
multiple-step of cold drawing and heat treatment processes. 
Crystalline phases were identified and quantified using wide 
angle x-ray diffraction. The morphology of the crystalline phases 
was observed using a scanning electron microscope with an 
energy dispersive spectroscope. It was found that the 
intermetallic phase is spherical σ-phase, which was precipitated 
on the surface of the stainless steel fibers. Cold drawing induced 
large amount of grain boundaries and then provided the sites for 
the σ-phase nucleation. 
Keywords-austenitic stainless steel; cold drawing; σ-phase; 
precipitate 
I. INTRODUCTION 
Austenitic stainless steel (ASS) fiber is wieldy used in the 
electrodes and the chemistry field due to its conductivity, high 
thermal stability, and corrosion resistance[1-3]. The stainless 
steel fiber is produced using a multiple-step of cold drawing 
and heat treatment processes. It is well-known that 
strain-induced martensite occurs during cold working 
condition. Furthermore, intermetallic compounds such as the 
σ-phase, χ phase, and Laves phase form during high 
temperature for long term[4-7]. Formation of an intermetallic 
phase such as the σ-phase is a problem when applied in a 
scorching environment. The existence of a σ-phase not only 
reduces corrosion resistance but also affects many properties 
such as ductility, toughness, and conductivity[8]. The 
morphology of a σ-phase plays an important role and causes 
reverse effects of mechanical properties[9]. 
The aim of this study was to identify the crystalline phases 
and the morphology of the intermetallic phases when 
producing fiber from ASS wire using multi-step cold drawing 
and heat treatment processes. 
II.  EXPERIMENTAL 
A. Materials 
A fully annealed state of 316L as-received wire with a 
diameter of 190 µm was used in this study. The as-received 
wire was subjected to a multi-pass bundle drawing process at 
room temperature. A following heat treatment was carried out 
in the range of 800 ◦C for 10 min. The diameters of the drawn 
fibers were 50, 34, and 20 µm with respect to each pass of 
cold drawing. The chemical composition is shown in Table 1. 
The deviation of the composition was ±5 wt%. 
TABLE 1. Chemical Compositions of 316L stainless steel 
(wt%) 
Element 
C Si Cr Ni Mn Mo P S N Fe 
0.01 0.75 17.1 11.97 0.53 1.99 0.006 0.001 0.046 Balance 
 
B. Morphology of σ-phase 
The SEM observation showed that the stainless steel fiber 
possessed a mottled texture and partial indentation when 
applying a multiple-step of cold drawing process (Fig. 2(a)). 
Large amounts of salient spheroids (less than 1 µm) appeared 
on the surface of the stainless steel fiber (Fig. 2 (b)). The 
surface of the stainless steel wire was smoother than that of the 
fiber. No particle was found on the wire surface (Fig. 3). The 
element composition of the spheroids was analyzed by EDS 
(Table 3). The deviation of the element composition was ±1 
wt%. The composition of spheroid on ASS can be defined as 
(Fe, Ni)x(Cr, Mo)y, i.e. σ-phase, where x and y vary from 1 to 7 
[12]
.  
As the martensite grains formed preferentially at the 
intersection points of deformation twins. Most of the σ-phase 
was found at grain boundaries[6, 8], incoherent twin boundaries, 
high-energy interfaces of oxide inclusions[13], and the 
dislocation array piled up[10]. Gill et al. (1989) pointed out that 
the dendrite-like σ-phase has an unstable shape, but the 
globular σ-phase has a stable one [14]. 
 
 
Figure 2. SEM images of 34 µm diameter stainless steel fiber 
with(a) 2.50 k and (b) 10.0 k magnification.  
 
Figure 3. SEM image of 190 µm diameter stainless steel
 wire. 
 
TABLE 3. Weight Fractions (wt%) of Element Compositions 
of The Surface of 34 µm Fiber 
Position 
Element 
Fe Cr Ni Mo 
A 67.73 18.04 9.96 2.92 
B 62.08 26.82 4.40 6.00 
 
IV. CONCLUSIONS 
Phase transformation of ASS from wire to fiber occurred 
during a multiple-step of cold drawing process. A spherical 
σ-phase precipitated on the surface of the stainless steel fiber 
can be observed. It could be explained that austenitic grain 
boundary was enhanced by a cold drawing process, which 
provided a large numbers of sites for σ-phase nucleation. 
The work suggests further experiments, in which the 
observation and analysis of σ-phase in austenitic matrix. It 
would be interesting to study the influence of conductivity 
with formation of σ-phase. 
ACKNOWLEDGMENT 
This research was supported by the National Science Council 
of Taiwan, ROC, grant number NSC 97-2221-E-035-026-
MY3. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/01/31
國科會補助計畫
計畫名稱: 普及健康服務格網：以格網為基礎之個人健康服務系統(3/3)
計畫主持人: 鍾葉青
計畫編號: 99-2218-E-007-001- 學門領域: 前瞻優質生活環境計畫
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
