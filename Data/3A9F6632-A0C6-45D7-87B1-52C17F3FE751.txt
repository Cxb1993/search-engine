 2
 
ABSTRACT 
As the success of the World-Wide Web people change some behaviors from local scales to Web scales. Today, 
more and more web pages are created on the Web in incredible speed because it is easy to spread information to 
overall Web user and it is economically low-priced. As a result of the well and high speed growing of the Web, 
the quantity of Web pages are too numerous to count today. Due to the reason, users of Web need some tools to 
help them effectively access Web data. One of the solutions is to get help from search engines to reduce the pages 
to a small set. And recent investigate shows that search engines work well in this job. 
Due to the reason of extensive usage of search engines, this project will focus on two important issues of 
nowadays search engines, that is, the reputation ranking and search result clustering. We hope we can build an 
adapted and improved search engine equipped with the new ranking algorithm and the result clustering interface. 
The technology of clustering search results with different topics has been developed in these years. For 
traditional clustering methods, some researchers clustered the document sets using the similarity between two or 
more documents, or exploited machine learning clustering manner training some documents to get the cluster 
rules. Each of the document clustering method described before has its advantages or disadvantages. However, 
the structure between web pages and general documents are not always the same. It can not confirm that the 
technologies with good performance on general documents clustering always perform well on the web pages 
clustering. Besides, the efficiency issue is also crucial for the clustering of web pages. In web pages clustering it 
can not use the same technology of analyzing all the contents to calculate its cluster as general document 
clustering. Supposing that we apply the method of document clustering on web pages clustering, it will waste a 
lot of time to get the clustered results. For this reason, we propose some more efficient methods that will conquer 
these issues. Generally the search engines can return information of several hundred to thousand of the pages’ 
titles, snippets and URLs. Almost all of the technologies about search result clustering have to attain further 
information from the contents of the returned lists. 
 
Keyword: Information Extraction, WWW, Search Engine 
 
 
 4
Larry Page and Sergey Brin propose a new algorithm called PageRank [5][6] to measure the importance of web 
pages in 1998 and Google became a well-known search engine today. Generally speaking, the spirit of PageRank 
is that pages are voted by hyperlinks of pages. Hyperlinks are embedded in web pages, a hyperlink which directs 
to another page means that this page is worth referred. A page has high PageRank score if it owns quite large 
number of in-links or if there are other high PageRank score pages link to it. Actually, PageRank considers not 
only the number of in-links of a page but also a random walk model that similar to web use behavior. 
Intuitively, PageRank defines the importance of web pages and helps search engine to select high quality pages 
more efficiently. The key idea of PageRank is that if page p1 has a link to page p2, the author of p1 is interested in 
page p2. Thus, a high PageRank score page means that many people are interested in it. Naturally those 
interesting pages (high PageRank score pages) will be ranked at the top of search results. However, the inherent 
character of PageRank is that it reflects the link relationship between pages. There exists a potential problem 
when measuring the importance of a page. In a word, since search engines repeatedly return the currently popular 
pages at the top of search results, a hidden bias exists on the metric which calculating the real importance of 
pages will affect the accuracy of search results of search engines. Popular pages are always at the top of search 
results while unpopular pages often get ignored by search engine users. Because of search engine users are used 
to click on the top search results [2] causing that popular pages tend to get more popular. This “richer-get-richer” 
phenomenon is really a problematic issue for those unpopular but high quality pages. Although Google is 
outstanding in web search engines today, PageRank algorithm still suffers two problems: biased ranking for 
newly created pages and assigning equal weight to all out-links of a page. Since newly created pages often do not 
receive enough in-links to show its real importance in the initial time stage. [7] shows that the newly created 
pages will prove its real importance after taking a long time to evolve. The long bias made few web users to 
notice new pages via search engines and thus the search results will be undependable. When calculating the 
importance of web pages, PageRank treats each out-link as an equivalent value. We will discuss the impact of 
giving different weight to links later. In fact, some investigations [8][9] have shown that search results could be 
more accurately by distributing different importance to links. 
For search result categorization, VIVISIMO (http://www.vivisimo.com) is one of the examples of cluster search 
engines on Internet. It is also the online cluster search engine with the most mature technology. In [21], we know 
that the AOL portal adopted VIVISIMO on top of the search results provided by Google. We can receive the 
clustered result after submitting our query to VIVISIMO. VIVISIMO then will return a frame of search result. 
The left frame denotes the structure of clustered categories of the top 200 to 300 search results. Users can 
navigate through the hierarchy by expanding or collapsing the category names. After choosing one category name, 
the results that are relevant to the category users select would be displayed at the right frame. As illustrated in 
Figure 2, the search result after submitting a query “Jaguar” to VIVISIMO. We can easily identify that it gets the 
final clustered result by analyzing the top 202 snippets of search results. The quantity of the search result is large. 
The total number of all search results VIVISIMO gets from the query “Jaguar” is more than 52 millions pages. 
However users can only surf the top 202 pages that VIVISIMO consider appropriate for user queries. 
 
Figure 2: An example of VIVISIMO search result after submitting a query “Jaguar”. 
Some of the other well-known clustered search engines like Clusty and Kartoo display their search results by 
applying relevant pictures or graphic user interface. By offering these relevant figures, users could attain what 
information the query relates to more easily. Search results returned by Clusty accompany some pictures related 
to the query users submit. These attached pictures are extracted from the search results it attained. Figure 3 is an 
example of search result of Clusty after submitting a query “Jaguar”. The pictures Clusty extracted are the wiki 
 6
and hosts have. Then we come back to our initial goal to alleviate the bias of PageRank algorithm on newly 
created pages and anticipate the PageRank score of those pages in next time stage. Toward this goal, we propose 
an effective algorithm to anticipate the PageRank value of web pages in next time stage and to accelerate these 
newly created pages to be discovered more rapidly. 
Since the intrinsic feature of the web, we can divide the web graph into a three-level hierarchical structure. We 
first take a look at the relations between directory and page. Table 1 shows the total number of directories and the 
total number of pages. We can see that the number of directories is about 40% of the number of pages. It also 
tells us that a directory contains less than three pages on average. We are interested in this information and we 
make a statistics for the distribution about the number of pages within a directory in our dataset. The result is 
shown in Table 2. 
Table 1: Total number of directories and pages 
 Total number 
Directory 1802945 
Page 4613707 
We can see in Table 2, the most part of directories (about 93%) contain less and equal than three pages. About 
97.5% of directories contain less and equal than ten pages in their directory. There are only 2.5% of directories 
can have more than ten pages in their directory. 
Table 2: A statistics of number of pages in a directory 
Number of pages within a directory Number of directories Ratio  
less and equal than 3 1676089 92.964% 
less and equal than 5 1718938 95.3406% 
less and equal than 7 1739713 96.4928% 
less and equal than 10 1757547 97.482% 
greater than 10 45398 2.518% 
We define “ladder-stage” as the cluster of pages which have similar link structure and thus their PageRank value 
is close to or even equal to each other. We are interested in the distribution of pages’ PageRank in a directory. 
Thus we randomly select some directories and observe the PageRank values of pages in a directory. 
Figure 6 to Figure 9 shows charts the ranking distribution of pages in different directories. In a small directory, 
such as Figure 6 , there is often a 2-layer graph and the lower layer is long and straight. In a medium-sized 
directory, such as Figure 7, we can see a 3-layer (or 4-layer) graph. There could be a multi-layer graph in a 
directory which has plenty of pages. Figure 8 and Figure 9 shows the view of multi-graph. In fact, the lowest 
layer of Figure 9 is already shown a multi-layer graph. It looks like a straight line due to the limit of our drawing 
program. We can see a more clearly multi-layer graph of Figure 9 in Figure 10. It is expectable that the layer 
graph in a directory since some pages within the same directory may have similar contents thus the in-link 
number of them are alike because of the set of pages refer to the pages with similar contents. 
3.00E-08
5.00E-08
7.00E-08
9.00E-08
1.10E-07
1.30E-07
1.50E-07
0 1 2 3 4 5 6 7 8 9 10 11
Rank ID of pages in Directory
Pa
ge
R
an
k 
va
lu
e
Rank in Directory
 
Figure 6: Ranking distribution of pages in a directory 
 8
anticipate the possible convergence value of a new page in the next time stage. The algorithm of DRank is stated 
below. 
Assume that ip  is a newly created page located within a full-grown directory. In order to anticipate the DRank 
value of ip  in next time stage, we divide the process of prediction into two steps: 
1. firstly, we calculate Page Quality of ip at time it . 
2. secondly, check if there exists any ladder-stage closing to Page Quality of ip  
A. If true, set DRank value of ip  to average (L), where L means the set of rank values of pages 
which belong to the ladder-stage. 
B. else, set DRank value of ip  for Page Quality of ip . 
In DRank algorithm, there are two parameters T and C for limiting the selected PageRank values to a threshold 
and helping to find a closely ladder-stage. 
Figure 11 and Figure 12 shows two examples of DRank algorithm. In Figure 11, there are about three ladder-
stages of pages and the target page which we want to observe is moving close to the second ladder-stage in time 
stage T2. The dot of PageRank value in T2 is very close to Q2 since we assign a very slightly increasing on the 
part of increasing ratio of Page Quality formula. Figure 12 is an obvious example. We can see that the target page 
which we want to observe is moving close to the first ladder-stage and thus we can anticipate the PageRank of the 
target page in time stage T3 would approach to the average PageRank value of pages in first ladder-stage. 
T1
Q2
6.0E-08
7.0E-08
8.0E-08
9.0E-08
1.0E-07
1.1E-07
1.2E-07
1.3E-07
1.4E-07
1.5E-07
1.6E-07
0 2 4 6 8 10 12 14 16 18 20 22 24
Serial number of pages in Directory
Pa
ge
R
an
k 
va
lu
e
 
Figure 11: An example of DRank algorithm – a directory within www.epa.gov 
T1
Q2
4.00E-08
1.40E-07
2.40E-07
3.40E-07
4.40E-07
5.40E-07
6.40E-07
7.40E-07
8.40E-07
9.40E-07
1.04E-06
1.14E-06
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85
Serial number of pages in Directory
Pa
ge
R
an
k 
va
lu
e
 
Figure 12: An example of DRank algorithm – a directory within www.nba.com/timberwolves 
 
we found that a lot number of methods about search results clustering only used the information of 
snippets of web documents. All terms and noun phrases are extracted from snippets. Most of those methods did 
not use the other information such as terms in title or URL words etc. Although few of the other methods use title 
information, it is like to concatenate title words and snippet words together to get longer snippets. Terms 
 10
In      (Eq.1) we still attempt to obtain the optimal permutation of all terms. In our rewriting equation      
(Eq.1) we divide the operations into two segments. One is for the terms extracted from title words and the other is 
for the terms extracted from snippet information. The first half of      (Eq.1) describes the calculation of terms in 
title and the later half denotes the operation of terms in snippet. The constants 1w , 2w , 3w  and 4w  are different 
weight numbers for the coverage of terms in title word, the distinctiveness of terms in title, the coverage of terms 
in snippet and  the distinctiveness of terms in snippet respectively. Because our new proposal is the terms 
extracted from title words are more significant than terms extracted from snippet words, we give the higher 
weight to functions titlegc  and titlegd . For our rewriting equation we decided that the values of title terms’ 
coverage and title terms’ distinctiveness are more crucial than snippet terms’ coverage and snippet terms’ 
distinctiveness. In other words, we have to calculate the coverage and distinctiveness values of a concept in both 
title words domain and in snippet words domain. We adjust the weight values dynamically based on the coverage 
of all terms extracted from titles. That means, the weight values are decided according to the ratio of the 
document numbers which are covered by the entire chosen terms extracted from title words. Supposing that the 
coverage ratio of terms extracted from title words is higher, the weight 1w  and 2w  for functions titlegc  and 
titlegd  is higher, too. The higher coverage ratio means that there would be more documents we would get by 
the title terms. The more information we would get, the more important the title terms are. We have chosen the 
four weight values dynamically and empirically as shown in Table 3. The weights  1w  and 2w  for functions 
titlegc  and titlegd begin from 0.5. If weight values for functions titlegc  and titlegd  is greater, weight 
values for functions snippetgc  and snippetgd  smaller. We still reserve the original weight values for 
functions cg  and dg  as 0.8 and 0.2 respectively. 
Ratio R Title 
Weight 
Snippet 
Weight 1
w  2w  3w  4w  
R＜0.3 0.5 0.5 0.5×0.8 = 0.40 0.5×0.2 = 0.10 0.5×0.8 = 0.40 0.5×0.2 = 0.10 
0.3≦R＜0.4 0.6 0.4 0.6×0.8 = 0.48 0.6×0.2 = 0.12 0.4×0.8 = 0.32 0.4×0.2 = 0.08 
0.4≦R＜0.5 0.7 0.3 0.7×0.8 = 0.56 0.7×0.2 = 0.14 0.3×0.8 = 0.24 0.3×0.2 = 0.06 
0.5≦R＜0.6 0.8 0.2 0.8×0.8 = 0.64 0.8×0.2 = 0.16 0.2×0.8 = 0.16 0.2×0.2 = 0.04 
R≧0.6 0.9 0.1 0.9×0.8 = 0.72 0.9×0.2 = 0.18 0.1×0.8 = 0.08 0.1×0.2 = 0.02 
Table 3. Different Weight for Terms Extracted from Title and Snippet 
The other method we proposed is using the URL information of each web page to attain the more 
precise clustered search results. We modified the calculation of distinctiveness by applying URL information. 
From our investigation, we observed that URL is always composed of several meaningful terms. For example, an 
URL like “http://sports.yahoo.com/” is composed of three terms - “sports”, “yahoo” and “com”. We proposed 
that we could exploit these meaningful terms in URL to decide the difference between two pair of web 
documents. However a URL such as 
“http://sports.yahoo.com/nba/teams/det;_ylt=Aj8Sla9g5Wgtmz7SahVzOHi8vLYF” we only extract the URL 
terms from “http://sports.yahoo.com”. In other words, we extract the URL terms from the host address only. 
Because the terms after the host address are not always the meaningful terms in many URLs. Sometimes there are 
several directory terms that without meanings appearing after host address. Sometimes there are several 
parameters in the URL after host address. We consider that those terms such as directory names and parameters 
would not provide more useful information. Consequently in our method we extract the terms from the URL host 
addresses only. 
Supposing that several URLs of web documents are composed of similar set of terms, it is very possible 
that these web pages should be clustered into the same category. By this way it could simplify the calculation of 
distinctiveness. We rewrote the original equation into (Eq.2) as follows: ( ) ( ) ( )jkdjkcjk cSURLgwcSgwcSg ,,, 1,21,11, −−− ×+×≡ ααα (Eq.2) 
The detail of URLgd  is described as follows: 
 12
(Eq.  2) 
Where )( jcu  and )( 1, −kSu α  mean the total URL terms under the concept jc  and under the concepts set 
1, −kSα  respectively. We modified the function dg  into function URLgd . The function URLgd  attempt to 
compute the subtraction of URL terms under jc  and under 1, −kSα  . Because the total number of URL terms is 
much less than the total number of terms in web document snippets, the execution time compared with the 
original “DisCover” algorithm would be reduced greatly. 
We also cite one of the useful features called Cluster Entropy. In function cg  and dg  of DisCover, it only 
applies the conception of subtraction to determine the difference between a concept and a concept set. However, 
the intersection of a concept and a concept set should be considered too. Hence we add a cluster entropy feature 
into original DisCover algorithm to adjust the clustered result. The formula for integration of the method is 
shown as follows: 
)1(),(
)1(),(),('
1,2
1,11,
ptsUnderConcejkd
Docjkcjk
EntropycSgw
EntropycSgwcSg
+××+
+××≡
−
−−
α
αα
 
(Eq.  3) 
The definition of DocEntropy  and ptsUnderConceEntropy  is shown as follows: 
 
∑∑ ∑
−
−
− ∈
∈
∈
∩
∩
∩
∩−
=
1,
1,
1,
)()(
)()(
log
)()(
)()(
k
k
k Sw
j
j
Sw
Sw
j
j
Doc
wdcd
wdcd
wdcd
wdcd
Entropy
α
α
α
 
(Eq.  4) 
 
∑∑ ∑
−
−
− ∈
∈
∈
∩
∩
∩
∩−
=
1,
1,
1,
)()(
)()(
log
)()(
)()(
k
k
k Sw
j
j
Sw
Sw
j
j
eptUndercConc
wtct
wtct
wtct
wtct
Entropy
α
α
α
 
(Eq.  5) 
Where it is defined 0log0 ⋅ . DocEntropy  means the condition of the intersection of )( jcd  distribute in 
)( 1, −kSd α  while ptUnderConceEntropy  means the condition of the intersection of )( jct  distribute in )( 1, −kSt α . 
The value of DocEntropy  indicates the value of documents covered by jc  distribute in documents covered 
by  1, −kSα  . The value of ptsUnderConceEntropy  denotes the entropy value of the under concepts of a concept jc  
distribute in the under concepts of a concept set 1, −kSα  . It indicates the value of concepts under a concept jc  
distribute in concepts under a concept set 1, −kSα  . The value of ptsUnderConceEntropy  indicates the value of 
under concepts of jc  distribute in under concepts of 1, −kSα . 
 14
concepts 1c  and 2c  are equal. In the DisCover method we would choose 2c  as the next concept because the 
subtraction of )( 2cd  and )( 1, −kSd α  is greater than the subtraction of )( 1cd  and )( 1, −kSd α . However we 
consider that 1c  should be the more appropriate one than 2c  to be the next selected concept. We apply our new 
method to re-select for the 1c  and 2c . Supposing that )( 1, −kSd α =40. For our enhanced formula, the 
comparative value for 1c  is (9/40+1) ≒ 0.2195. For 2c  the value is (10/40+10) = 0.2. We would select the 
concept 1c  with the greater comparative value. The concept 1c  is also the adequate one that would maximize the 
distinctness between  1, −kSα . 
9)()(
1)()(
1,1
1,1
=−
=∩
−
−
k
k
Sdcd
Sdcd
α
α
d(Sα,k-1)=40 d(Sα,k-1)=40
d(c1)=10
d(c2)=20
10)()(
10)()(
1,1
1,1
=−
=∩
−
−
k
k
Sdcd
Sdcd
α
α
(A.) (B.)
1
9
10
10
 
Figure 15: An Example for Modified Cluster Entropy 
 
Concept Hierarchy Modification 
The method to construct the concept hierarchy in the CAARD is to calculate each pair of concepts and clusters 
to distribute each rest concept that is not top level cluster into the most appreciate category. It utilize vector 
model to attain the relation value of each concept and cluster. In our new hierarchy method, we still apply that 
each concepts is represented by a binary vector of dimension N where N is the number of the search result 
documents. However we do not utilize the traditional method to calculate the relation for each pair of concepts by 
using vector model. For our method, we simplify the calculation for relation of pairs. We focus on the mapping of 
each pair of element in the binary vectors. For example, we give ),...,( 21 kNkkk VVVV =  as a binary vector of a 
cluster k, ),...,( 21 cNccc VVVV = as a binary vector of a concept c. Each mapping elements in the same position 
of kV  and cV  would determine the relation value of cluster k and concept c. 
 The entire process for computing the relation of cluster k and concept c is described in the pseudo code as 
shown as follows: 
vector_k  Å vector space of cluster k of N dimension 
vector_c Å  vector space of concept c of N dimension 
for i=1 to N 
if ( ( vector_k[i], vector_c[i]) == (1,1) ) 
Rvalue++; 
else if( ( vector_k[i], vector_c[i]) == (0,1) || ( vector_k[i],_ vector_c[i]) == (1,0) ) 
 16
and noun phrases are also included in the index into our database. Next the improved “DisCover” algorithm 
calculates for the optimal permutation of terms. After our improved “DisCover” algorithm calculation, our 
system continuously operates for the concepts hierarchy. Finally we represent the clustered result using web tree 
view style and return to users. 
 
Figure 16. The System Flow Chart 
Figure 17 shows the complete result interface of our clustered system. The left frame is the hierarchical 
structure of all concepts. The right frame denotes the documents correlated with the concept selected. Users can 
select one concept then there would be the document result appears in the right frame. 
 
Figure 17. Our System Interface after Sending the Chinese Query “蘋果” 
 
In our experiments, for each query we collect the top 100 search results returned by Google. We choose five 
Chinese ambiguous queries and five English ambiguous queries as shown in TABLE II to be our testing queries.  
TABLE II 
 THE AMBIGUOUS QUERIES USED IN EXPERIMENTS 
Chinese Testing Queries 蘋果(apple), 天堂(paradise), 爪哇(java), 飛碟(flying saucer), 地中海
(the Mediterranean sea) 
English Testing Queries Jaguar, Latex, Java, Panther, Lotus 
TABLE III 
THE SPECIFIC QUERIES USED IN EXPERIMENTS 
Chinese Testing Queries 蘋果電腦(Apple Computer), 天堂線上遊戲(lineage online game), 爪哇咖啡
(java coffee), 飛碟電台(uforadio), 地中海型貧血(Thalassaemia) 
English Testing Queries Jaguar car, Latex editor, Java Program, Panther OS, Lotus IBM 
 
DATA BASE 
Web 
Interface 
Query 
Clusters 
Index after parsing snippets and 
titles respectively 
Improved DisCover 
Algorithm 
Hierarchy of Concepts 
Building 
Web Tree View 
Construction 
User 
Google Top 100 
Search Results 
Title Parsing Snippet Parsing 
Stemming, stop word deletion, 
noun phrases extraction 
 18
Coverage Improvement Degree Compare to DisCover (English Specific Queries)
-0.06
-0.04
-0.02
0
0.02
0.04
1 2 3 4 5 6 7 8 9 10
Cluster Number
DisCover DisCover' M1 M2 M3 M4 M5
 
Coverage Improvement Extent Compare to DisCover (English Specific Queries) 
 
C/O value for Top2 to Top10 Clusters (Chinese Ambiguous Queries)
0
5
10
15
20
25
30
35
40
2 3 4 5 6 7 8 9 10
Cluster Number
Co
ve
rag
e/O
ve
rla
p
DisCover DisCover' M1 M2 M3 M4 M5
 
The C/O value for Top2 to Top10 Clusters (Chinese Ambiguous Queries) 
C/O value for Top2 to Top10 Clusters (Chinese Specific Queries)
0
2
4
6
8
10
12
2 3 4 5 6 7 8 9 10
Cluster Number
Co
ve
rag
e/O
ve
rla
p
DisCover DisCover' M1 M2 M3 M4 M5
 
The C/O value for Top2 to Top10 Clusters (Chinese Specific Queries) 
 
 20
We also perform our methods to six volunteers on examining our methods. Figure 19 represents the 
normalized ranking score with the average of them at the right-most column for the top10 cluster concepts of 
Chinese ambiguous and specific queries respectively. In the average our methods would attain better clustered 
score than DisCover especially for M5. For Chinese ambiguous and specific queries our methods could attain the 
better clustered score than DisCover. 
 
 
Average Execution Time of Each Method (Chinese Queries)
25.3283
36.7735 36.8049
6.4893
168.8516
34.8546 33.0532
0
20
40
60
80
100
120
140
160
180
200
Ex
ec
uti
on
 T
im
e (
se
c)
DisCover DisCover' M1 M2 M3 M4 M5
 
Average Execution Time of Each Method (Chinese Queries) 
Average Execution Time of Each Method (English Queries)
28.7588
37.3125
42.0707
4.9233
122.3205
20.0468 17.8
0
20
40
60
80
100
120
140
160
Ex
ecu
tio
n T
im
e (
sec
)
DisCover DisCover' M1 M2 M3 M4 M5
 
Average Execution Time of Each Method (English Queries) 
Normalized Average Ranking Score of Top10 Clustered Concepts for Each Tester (Chinese Ambiguous Queries)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Tester1 Tester2 Tester3 Tester4 Tester5 Tester6 Total Average
No
rm
ali
ze
d S
co
re
DisCover DisCover' M1 M2 M3 M4 M5
 
Normalized Ranking Score of Top10 Clustered Concepts (Chinese Ambiguous Queries) 
Normalized Average Ranking Score of Top10 Clustered Concepts for Each Tester (Chinese Specific Queries)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Tester1 Tester2 Tester3 Tester4 Tester5 Tester6 Total Average
No
rm
ali
ze
d S
co
re
DisCover DisCover' M1 M2 M3 M4 M5
 
Normalized Ranking Score of Top10 Clustered Concepts (Chinese Specific Queries) 
Figure 19: Ranking Results 
 
 
 
 
 
 22
 
 
 
Compare to Other Clustered Search Engines 
We compare the clustered result of our M5 method to the other online clustered real systems. VIVISIMO and 
Clusty  are chosen. We compare the top 10 clustered concepts of VIVISIMO and Clusty to top 10 clustered 
concepts of M5.  
TABLE IV and TABLE V show the Top 10 clustered concepts of VIVISIMO, Clusty and our M5 method. In 
these two tables we find that our M5 method would also attain the most key subjects of each query as VIVISIMO 
and Clusty return. The clustered concepts in our method are always simple and meaningful. It would not happen 
that there are some complicated and meaningless concepts as “Aguar Jguar Jauar Jagar Jagur Jagua” which 
VIVISIMO returns for the query “jaguar”. Besides our method could also return the Chinese search result 
clustered result while VIVISIMO and Clusty could not serve. 
TABLE VII 
COMPARE OF CAARD AND OUR NEW CONCEPT HIERARCHY IN LEVEL 2 CONCEPT (ENGLISH QUERIES) 
CAARD New Concept Hierarchy Method: 
M2 Level 2 
concept 
number 
(p) 
Coverage ratio 
of Level 2 
concepts 
(q) 
Average 
coverage for 
each concept in 
Level 2 (%)
(q×100/p) 
Level 2 
concept 
number 
(p) 
Coverage ratio 
of Level 2 
concepts 
(q) 
Average 
coverage for 
each concept in 
Level 2 (%)
(q×100/p) 
Jaguar 53 0.49 0.9245 24 0.45 1.875
Jaguar Car 83 0.72 0.8675 31 0.66 2.219
Latex 57 0.5 0.8872 25 0.48 1.92
Latex Editor 111 0.77 0.6937 19 0.61 3.2105
Java 84 0.61 0.7262 27 0.57 2.1111
Java Program 50 0.44 0.88 21 0.38 1.8095
Panther 45 0.54 1.2 28 0.53 1.8929
Panther OS 100 0.82 0.82 8 0.58 7.25
Lotus 38 0.43 1.1316 21 0.39 1.8571
Lotus IBM 77 0.57 0.7402 17 0.45 2.6471
 
TABLE VI 
COMPARE OF CAARD AND OUR NEW CONCEPT HIERARCHY IN LEVEL 2 CONCEPT (CHINESE QUERIES) 
CAARD New Concept Hierarchy Method: 
M2 Level 2 
concept 
number 
(p) 
Coverage ratio 
of Level 2 
concepts 
(q) 
Average 
coverage for 
each concept in 
Level 2 (%)
(q×100/p) 
Level 2 
concept 
number 
(p) 
Coverage ratio 
of Level 2 
concepts 
(q) 
Average 
coverage for 
each concept in 
Level 2 (%)
(q×100/p) 
蘋果(Apple) 43 0.5 1.1628 15 0.44 2.9333
蘋果電腦(Apple computer) 103 0.76 0.7379 22 0.49 2.2273
天堂(paradise) 21 0.45 0.8824 23 0.41 1.7862
天堂線上遊戲(lineage online game) 113 0.65 0.5752 30 0.61 2.0333
爪哇(java) 70 0.65 0.9286 26 0.61 2.3462
爪哇咖啡(java coffee) 125 0.84 0.672 32 0.7 2.1875
飛碟(flying saucer) 55 0.6 1.0909 23 0.52 2.2609
飛碟電台(uforadio) 81 0.72 0.8889 31 0.62 2
地中海(the Mediterranean sea) 87 0.72 0.8276 28 0.58 2.0714
地中海型貧血(Thalassaemia) 117 0.85 0.7265 29 0.76 2.6207
 24
Reference 
[1] D. Fallows. Search Engine Users: Internet searchers are confident, satisfied and trusting – but they are 
also unaware and naïve. Embargoed for publication until 4pm, January 23, 2005, 
http://www.pewinternet.org/. 
[2] iProspect Search Engine User Behavior Study. April, 2006, http://www.iprospect.com/. 
[3] G. Salton, A. Wong, and C. Yang. A vector space model for information retrieval. Journal for the 
American Society for Information Retrieval, 1975. 
[4] Search Engine Watch: http://searchenginewatch.com/. 
[5] L. Page, S. Brin, R. Motwani, and T. Winogrand. The PageRank Citation Ranking: Bringing Order to the 
Web. Technical report, Stanford University, Stanford, CA, 1998. 
[6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proceedings of 
WWW Conference, April 1998. 
[7] J. Cho and S. Roy. Impact of Search Engines on Page Popularity. In Proceedings of the International 
World-Wide Web Conference, May 2004. 
[8] X. M. Jiang, G. R. Xue, H. J. Zeng, Z. Chen, W.-G. Song and W.-Y. Ma. Exploiting PageRank Analysis 
at Different Block Level. In Proceedings of Conference of WISE 2004. 
[9] W. Xing and A. Ghorbani. Weighted PageRank Algorithm. In Proceedings of the 2nd Annual Conference 
on Communication Networks and Services Research. 305-314, 2004. 
[10] G. Salton and M. J. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983 
[11] S. Abiteboul, M. Preda, and G. Cobna. Adaptive on-line page importance computation. In Proceedings of 
the International World-Wide Web Conference, May 2003. 
[12] T. H. Haveliwala. Topic-sensitive pagerank. In Proceedings of the International World-Wide Web 
Conference, May 2002. 
[13] S. Kamvar, T. Haveliwala, C. Manning, and G. Golub. Extrapolation methods for accelerating pagerank 
computations. In Proceedings of the International World-Wide Web Conference, May 2003. 
[14] G.-R Xue, Q. Yang, H.-J. Zeng, Y. Yu, Z. Chen. Exploiting the Hierarchical Structure for Link Analysis. 
In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development 
in Information Retrieval, August 2005. 
[15] A. Ntoulas, J. Cho and C. Olston. What’s New on the Web? The Evolution of the Web from a Search 
Engine Perspective. In Proceedings of the International World-Wide Web Conference, May 2004. 
[16] D. Fetterly, M. Manasse, M. Najork and J. Wiener. A Large-Scale of the Evolution of Web Pages. In 
Proceedings of the International World-Wide Web Conference, May 2003. 
[17] J. Cho, S. Roy and R. E. Adams. Page Quality: In Search of an Unbiased Web Ranking. In Proceedings 
of the SIGMOD Conference, June 2005. 
[18] K. Bharat, B.-W. Chang, M. Henzinger, M. Ruhl. Who Links to Whom: Mining Linkage between Web 
Sites. In Proceedings of the International Conference on Data Mining, November 2001. 
[19] D. Cai, X. F. He, J. R. Wen and W. Y. Ma. Block-level Link Analysis. In Proceedings of the 27th Annual 
International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2004. 
[20] S.-F. Lin and H.-Y. Kao. Toward the Impact Analysis of the Ranking and Relationship between the 
Hierarchy and Clustering in Web Sites on the Page Popularity. A Master Thesis in Department of 
Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, 
R.O.C., July 2006. 
[21] Google Directory: http://directory.google.com/. 
[22] Paolo Ferragina and Antonio Gulli. A Personalized Search Engine Based on Web-Snippet Hierarchical 
Clustering. In WWW 2005, pages 801-810. 
