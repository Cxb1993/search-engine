 2
一、中文摘要 
交通事故日益增加，十字路口往往是重大交通事故發生的場所，賦予路口監視系統擁有
完備之智慧型保全能力，進而建立智慧型開放性空間道路監視系統實是刻不容緩。本研究中，
將針對十字路口設計一套智慧型戶外交通安全監視系統，本研究建構出一多台攝影機組之視
訊監控系統，能夠對整個路口場景進行完備之監控任務，並對特定目標物進行特寫鏡、追蹤、
放大(zoom in)與縮小(zoom out)等工作，主要研發車輛與行人之偵測、追蹤、與記錄等相關影
像及視訊處理技術。本研究利用多組擺設不同方位之攝影機組，每組攝影機組皆包含兩隻不
同功能、性質攝影機，分別為「固定式全場攝影機」與「移動式特寫攝影機」，其中，固定式
全場攝影機主要功能用於監視整個監控場景，提供追蹤目標物之位置資訊；而移動式特寫攝
影機則追蹤、放大與縮小監視目標物，取得目標物之細部資料，進而對行人與車輛做行為分
析。本研究計畫的成果不僅能透過雙攝影機組自動偵測、追蹤與紀錄物件，更進一步可以自
動做物件判斷與行為分析，並使此系統於物件查詢、檢索、行為或特定事件偵測皆能透過這
些步驟而得到完美結果。 
關鍵詞：智慧型保全、全場攝影機、特寫攝影機、行為分析、物件偵測、查詢、檢索 
二、英文摘要 
Recently, the accident events on the crossroad are increasing in Taiwan according to the reports 
of governments. Most of the events occur at crossroad on the unsupervised ones. The main goal of 
this project is to give the intelligence of video surveillance and monitoring (VSAM) system. It is 
necessary and urgent to build up the intelligent systems for the unsupervised crossroads. We have 
constructed a prototype system for the opening space. A smart devise with two calibrated cameras is 
designed to grab the entire sense and the clearer, larger, and detail images of moving objects. The 
first one camera grabbing the entire sense detects and tracks the moving objects. The position and 
trajectory information is passed to the second one by the calibrated parameters such as the pan, title, 
and zoom data. It is unnecessary for the second camera to track the objects with the video 
processing or matching processes. Vehicles and pedestrians are the two elementary objects at 
crossroads. Based on the calibrated two-camera device, the high-level behaviors of objects are 
extracted and analyzed. In addition, the event detection techniques based on the object behavior are 
also developed. In this project, several devices will be used to obtain the image from different 
positions. Our system, not only the moving objects can be detected, tracked, and recorded, but also 
their detail behaviors will be identified. 
Keywords: surveillance, intelligent systems, event detection, two calibrated camera device 
 
 4
我們則提出了以Fuzzy 結合SOM的方式，將軌跡當作輸入判別異常的軌跡。在車輛碰撞預測
中，我們則是提出了以場景中的深度資訊搭配車輛速度資訊來預測車輛位置的方式來預測車
輛是否碰撞。而交通違規偵測方面，則是利用特定區域中不合法出現的人或車輛之偵測。在
第二年關鍵性資料記錄主題中，我們完成了四個模組，分別為(5)人臉偵測、(6)人臉索引、(7)
車牌偵測、(8)車牌辨識等技術。我們在本計畫中提出了一種結合adaboost 與Neural Networks
的偵測技術，在偵測技術中，主要的考量在於如何減少scanning windows 的數目與如何減少
scanning window的處理時間，因此我們提出結合adaboost與Neural Networks的方式來達成上述
兩項目標，並用於人臉偵測、人形偵測與車牌偵測中。在人臉辨識中，目前最困難的部分為
訓練影像過少的問題，本計畫提出利用最近特徵空間轉換法進行人臉索引。在車牌辨識中，
本計畫則是提出一種使用LBP方法作為特徵的新OCR系統，由於LBP 方法是一種描述pixel 
與pixel 之間大小關係的紋理資訊，因此對於雜訊的敏感度較低，因此本計畫利用LBP的此項
特性提出一新的車牌辨識模型，希望在辨識率上有所提昇。 
 
圖一：安裝於路口之監控攝影機。 
 
 6
斯分佈適合被利用於模型化因上述改變所造成像素值在原始數值附近所做的小幅變
動；但是在大部份的情況下，像素值的變動並不只局限於一個值，而有可能在某些
值做變動，如濺起漣漪的湖面、反光的電腦螢幕亦或者是因光線移動所造成的陰影
改變…等，因此，採用多個高斯的分佈來模型化背景是較適合的方式。高斯混合模
型以數學的角度而言，對於擁有多類別的樣本(Pattern) 與傳統的單一高斯分佈(Single 
Gaussian Model)和向量量化(Vector Quantization)兩種模型比較，的確具有極佳的近似
能力，向量量化的模型，是選取幾個重要的位置來代表整個向量空間，但是模型本
身沒有把樣本在空間中的分佈大小與形狀描述出來，因此，此方法並不理想。而單
一高斯分佈模型使用一個平均值向量代表多個樣本在向量空間的中心位置，並且利
用共變異矩陣近似樣本群在向量空間中所分佈的形狀，其效果有限。而高斯混合模
型使用多個單一高斯模型來代表特徵向量的分佈，如此可以較精準的紀錄樣本的各
種類別和向量空間中的位置，也能描述樣本群在空間中的大小及形狀，因此，高斯
混合模型十分適合描述特徵向量在色彩空間的分佈。一個高斯混合模型具有以下三
個參數，分別為混合加權值(mixture weights)、平均值向量(mean vector)以及共變異矩
陣(covariance matrix)，而對於每一個像素而言，都可以用式 (1)與 (2)所述的參數來
描述：其中 iW 表示混合加權值， iμ  表示平均向量， i∑  表示共變異矩陣，而M 則
是高斯分佈的個數，對每一個影像像素而言，都可以用λ來表示像素的模型。若所
使用的資料 { }nN XXXX ,....,2,1= 分佈於 D 維空間中，其高斯混合模型的相似度表示如
(2)，其中 ( )ix θη ;  為第  i 個高斯分佈的密度函數，而混合加權值也必須滿足
1
1 1
=∑ =Mi W 的條件。高斯混合模型的架構可用圖三來表示。最後，高斯混合模型將
先取N 個資料點來訓練模型，並經過K 平均值分類法得到初始化後的參數，再由期
望值最大演算法得到的以下(3)至(5)三個方程式，分別為其中 iW 表示混合加權值， 
iμ  表示平均向量， i∑  表示共變異矩陣，而公式中的 jβ 則為事後機率，定義為
( ) ( )xjpxj |=β ，接著進行參數的更新，並計算新的相似函數的值，如此不斷的疊
代，不斷地更新模型的參數，直到相似函數的值已經沒什麼變動，或是疊代的次數
超過某個門檻值，才停止疊代。經過以上方法所得到的參數，即為高斯混合模型所
判斷前景的依據，當像素代入判斷式所得到屬於前景之機率大於背景機率，則此像
素為前景。 
 8
大而無法表示，而加一的目的是為了使背景影像能根據環境的變化而統計出最佳的
結果。而此加減後的結果也符合環境所產生變化，假設背景像素值與輸入像素值相
同，則histogram的值維持不變，表示背景沒有變化；如果背景像素值與輸入像素值
不同，則會遞減背景像素值所累計的次數值，遞增輸入像素值所累計的次數值，而
呈現出較佳的背景，最後再去判斷背景像素與輸入像素出現次數最高的像素值。此
種方法的主要精神，是去找出像素間變化量較大者，來當作背景像素值，所以當背
景像素值遞減時，有可能其他的像素值所累計的次數大於背景像素值所累計的次
數，但並不將此像素值更新為背景像素值，原因是此像素值並沒有再出現過，而只
是背景建立時所統計的數值，除非此像素值再度出現，才會被更新為背景像素值。 
 
圖四：RGB色彩空間直方圖 
 
(b)軌跡分析與行為判斷： 
對於物體的行為可分為正常與異常兩種情況，由於異常資料較不易取得，因此大部份的
研究會針對正常行為來討論。過去關於物體正常行為的研究，可分為監督式與非監督式兩種，
所謂的監督式即需事先定義判斷的準則；反之，非監督式即自動地學習資料的分布，找出資
料的主要特徵。 
1. 監督式學習： 
(1.1) Dynamic time warping(DTW) : DTW 是一種以 template 為基礎的 dynamic 
t 
Image Sequence 
0   Max_G     255 
Frequency
0       Max_R  255 
Frequency
0     Max_B    255 
Frequency
 10
等差異性擷取活動式範圍，通常利用動態輪廓(Active contour)或曲線(Snake)方式實
作，找出人臉的邊界，而後續的改良方式除了提升了型態在邊緣的地標點位置
(Landmark point)準確性，也加入了分群法，避免型態差異過大，另外在搜尋的機制
上有較佳的延展性，然而此一類型偵測器必須限制特定偵測範圍，且無法滿足及時
偵測之要求。 
2. 低階分析：此一類型偵測器著重於資訊融合，色彩，動態位移量，五官特徵與幾何
資訊等等，皆是資訊融合的最佳資訊，因此適用於多專家系統，然後針對個個結果
進行投票，以決定其最後結果。換句話說，低階分析是以影像處理為主要的方法，
如以 Sobel 濾鏡，搭配邊緣為基礎的 SNoW 分類器，架構一個快速的人臉特徵分類
方法。又或者以邊緣偵測使用 Canny 濾鏡，再將轉換後影像輸入到多階層 B 木條曲
線(Multilevel B-Splines)取得特徵值，最後搭配支援向量機來分類人臉；Canny 濾鏡為
John F. Canny 於 1986 發展出來而名，使用了多階的演算法來偵測一個大範圍的邊
緣。另外許多研究在 RGB 色彩空間進行膚色偵測，RGB 色彩空間是以光的三原色
紅、綠、藍為刺激值混合出某一色彩的顯像方式，三原色中每一種色彩的值都介於
0-255 的範圍，透過紅、綠、藍等值的大小組合來決定像素的顔色。然而 RGB 存在
著亮度因素的問題，使用 RGB 色彩空間無法有效萃取出人臉膚色，故提出將 RGB
先行轉換至 YCbCr 色彩空間之後，再進行偵測膚色的方式，最後經由眼睛及嘴巴兩
項特徵比對特徵，確認是否為人臉。 
3.  主成份分析：主成份分析之主要目的在「轉換」原始變項使之成為一些互相獨立的
線性組合變數，而且經由線性組合之後，得到的主成分仍保有原變數最多的資訊，
故可直接比對主成份分析後，求得特徵值(Eigenvalue)及特徵向量(Eigenvector)的相似
程度來作分類；又或者進一步使用線性鑑別式分析，則能夠將原始資料投射到最具
鑑別性的向量空間中，找出最大化及最小化分散(Class scatter)之間的轉換矩陣，由於
加入類別資訊，因此比主成份分析來得更為穩定。 
4. 類神經網路：此一類型分類器收集大量人臉樣本，建立類神經網路架構，加以訓練，
獲得每個結點之權重值，而檢驗時，直接將視窗資料經由一連串之權重加總(weighted 
summation)得到該視窗之判斷值，藉此決定是否為人臉影像視窗，如 Garcia 等人採用
迴旋類神經網路(Convolutional neural network)來判別是否為人臉，人臉影像輸入至類
神經網路前，可以依人臉膚色找出特徵定位點，並進行兩次重新取樣以降低維度，
再將資訊傳輸至類神經網路神經元，使用雙彎曲函數(Sigmoid neurons)計算，判定是
否為人臉。Chang 認為傳統的輻射基底類神經網路(Radio basis function neural network; 
 12
1. 演算法式：近來，NFL(Nearest feature line)方法是一種最熱門而且有效的人臉辨識技術，
它認為在同一個人臉的不同樣本影像中任取兩個樣本所連成的特徵線(Feature line)，可以
虛擬出這兩張樣本間的所有線性變化關係，進而虛擬出無數的樣本出來，因此運用這條
線當作比對基礎，將一測試影像與該線段之間的距離當作衡量標準，此方法可大幅提升
傳統最短距離法的辨識率，因此不論是何種特徵，只需在比對階段採用 NFL 比較，大多
可以提升辨識率。 
2. 特徵空間轉換法：在特徵空間轉換法中，最重要的即為主成份分析(PCA)，主成份分
析之主要目的在「轉換」原始變項使之成為一些互相獨立的線性組合變數，而且經
由線性組合之後，得到的主成分仍保有原變數最多的資訊，故可直接比對主成份分
析後，求得特徵值(Eigenvalue)及特徵向量(Eigenvector)的相似程度來作分類；又或者
進一步使用線性鑑別式分析，則能夠將原始資料投射到最具鑑別性的向量空間中，
找出最大化及最小化分散(Class scatter)之間的轉換矩陣，由於加入類別資訊，因此比
主成份分析來得更為穩定。由於以 PCA 為基礎的演算法多著重於全域觀點而忽略了
區域局部的資訊，因此，近來有另一個與 PCA 觀點不同方法受到研究的重視，即使
用拉普拉司臉進行人臉辨識(face recognition using laplacianfaces)。該方法提出以特徵
空間方法建立拉普拉斯矩陣(laplacian Matrix)為基礎的人臉辨識系統，來保留訓練樣
本之局部結構，其作法是在臉特徵空間中流型結構的區域性透過區域保存投影方法
(locality preserving projection, LPP)在最近相鄰關係圖(nearest-neighbor graph)而保留
下來。此外，研究者更進一步將拉普拉司臉方法加入費雪條件(Fisher criterion)來最佳
化組內緊密度和組間可分性，基於區域保存投影方法來計算修改後的組內與組間變
異，以保留局部結構，並且，使用正交基鄰近保留判別分析 (orthogonal neighborhood 
preserving discriminant analysis, ONPDA)或邊境費雪分析法(marginal Fisher analysis, 
MFA)等方法描述樣本之間鄰近關係的相似矩陣，此類方法皆能大幅提升人臉辨識時
對人臉表情、姿勢以及光線變化的容忍力。 
3. 類神經網路：類神經網路在分類辨識問題上已行之有年，一般而言，類神經網路相較
於線性方法的優點有：(a)具有容錯的能力，(b)具有適應學習的能力，(c) 具有非線性
特徵空間轉換的能力。因此在人臉辨識中，仍有許多重要的方法，例如 Convolutional 
Neural Network、Radial based function neural network、probability neural network 等等
皆在人臉辨識領域中有不錯的表現。 
 
 14
之則設為 0。 
(C)代表鄰近六點依左上那一點按照順時針順序由二元值轉十進位的結果。 
(D)若原圖在(B)為 1 則不變，若為 0 則設 0。 
在本模組中，我們在原 LBP code 加入了 Hue 的 H 資訊，對上述做一個改良，將 H quantize
到 32 bin，然後，因此我們可以得到式(7)： 
 
圖五：多邊形 LBP 編碼 
 
⎩⎨
⎧
<−
≥−=−= ∑−
=
   
   0
   1
)(   ,2)(),(
1
0
, Tdgg
Tdgg
xsggsyxLBP
cp
cp
P
p
p
cpccRP (7) 
由於加入了顏色資訊，因此此一新的LBP編碼不僅具有紋理資訊也加入了顏色資訊，如此，
有機會會得到比原先的方法有更好的效果。圖六為本模組的實驗結果，可以發現透過加入顏
色資訊的LBP可以將前景切割地更完整。 
 16
connected component algorithm)，以及 size filter 將較小的區塊去除後，即可得到場景中主要
的移動物體。接下來的目標，即是利用 bounding box distance 的方法來決定此目標物與之前
場景中物體的對應關係。bounding box distance 的方法，計算中心點與之前物體的最小距離，
如果中心點位於之前物體內部；與一般 Euclidean distance 的不同點是，Euclidean distance 會
在物體發生合併或是分裂時，產生較大的距離，造成比對錯誤。首先定義一組 track，用來記
錄場景中曾經出現過的物體，而物體使用下列的特徵來表示:(1) 中心點 C(x,y)，(2)連通元件
的 left、right、top、bottom，(3)速度 M(x,y)，而物體速度的資訊，是用來預測下一個時間點
的位置。接著使用 bounding box distance 所產生的 distance matrix，利用閥值二值化，得到
correspondence matrix，即可利用此 correspondence matrix 決定與目標物的對應關係。其對應
關係有五種情況: 
1. entering :目標物與 track 無對應關係，表示有新的物體進入場景中，即產生一個新的 track 
記錄此目標物。 
2. correspondence : 目標物與 track 一對一的對應，及更新 track 中的所記錄的特徵。 
3. merging :目標物與多個 track 對應，表示物體間互相重疊，此時即利用物體之前的速度資  
訊，來預測下個時間點的位置，直到目標物與 track 一對一的對應。 
4. splitting : 多個目標物與 track 對應，表示物體間發生分裂，此時即對分裂後的物體，產
生一個新的 track。 
5. leaving : track 與目標物無對應關係，表示此物體離開場景、或是前景物偵測產生錯誤。
如果此 track 經過一段時間後，都沒有跟任何目標物對應，即刪除此 track。得到目標物
與 track 的對應關係後，即更新 track 中物體的特徵。 
接下來，我們利用 snake 方法來對偵測物進行分割，圖七(a)為原影像，而圖七 (b)則延續
圖七(a)之結果，對單一行人、多人或車輛進行動態輪廓模型分割擷取出最右一欄的輪廓資料。 
其次，我們利用  Convolutional Neural Network(CNN) 對切出目標物影像分類，
Convolutional Neural Networks (CNN)成功地將影像前處理與訓練過程結合在一起，不僅可以
應用在文字辨識上亦可應用在物件偵測上，它在 feature map 層中將訓練影像以 convolution 的
方式將特徵融合並且搭配訓練所得的參數進行空間轉換，而此一轉換空間又與後端的分類器
轉換空間相配合形成一完整的倒傳遞網路的參數調整。CNN 中最重要的即為 feature map 中的
區域感受野(local receptive field)以及加權值共享(weight sharing)，這些加權值共享的神經元被
稱為處在同一特徵映像(feature map)中，此種作法的好處在於可大幅降低網路的自由度，使得
預測正確率得以提高。我們將它應用於行人、多人與車輛分類上，獲得不錯之效果，而此方
 18
當訓練完成後，由於先前做過特徵向量的正規化，每個特徵向量所加上的長度不同，因
此需再調整鍵結值向量的長度使其還原成原本特徵向量的長度，調整的方法如下:輸入所有的
特徵向量至網路中，找出每個特徵向量的得勝類神經元，接著對每個類神經元相對應得勝的
特徵向量中，找出所加上長度的最大值，即為所需調整的長度大小。舉例來說，假設第j 個
類神經元相對應得勝的特徵向量中，正規化後所加上的最大長度為L，即將第j 個類神經元的
長度調整回來，並將所有的類神經元還原成不同的特徵向量長度。 
假設輸入的軌跡為[(x1,y1), (x2,y2)…, (xm,ym)]，而所形成的特徵向量為Tg =[x1,y1,s1,d1…, 
xm,ym,sm,dm]，利用Euclidean distance 找出得勝的類神經元 j，其距離表示成Dj，如果Dj/m>qj，
則表示此輸入軌跡為異常，而qj的值為利用所有訓練的資料，其得勝類神經元為 j 之距離所
計算出來的值，也就表示當輸入軌跡與得勝類神經元 j 的距離，大於先前所有訓練軌跡之得
勝類神經元 j 所計算出來的距離，表示此輸入軌跡不屬於先前訓練軌跡的分布範圍，即表示
此軌跡為異常。 
此外，此異常閥值 jq 取決於所有訓練軌跡之得勝類神經元為 j 的最大距離，即計算所有
輸入軌跡 iT 之得勝類神經元為 j 的距離 ijD ， mDQ ijij /= (m 為軌跡 iT 的長度)，接著取所有 ijQ 中
的最大值來當作異常閥值 jq 的值，簡單來說，此閥值可看作群聚 (類神經元)的大小，也就是
屬於此群的所有資料中，與此群聚中心的最大距離 ijij Qq max= ，而 ijD 除以 m 的原因是因為
輸入軌跡的長度並不相同，為避免較長軌跡會得到較大的值，因此將 ijD 除以 m 以得到較平均
距離即可用於異常軌跡的判斷。 
 
(3)車輛碰撞預測模組： 
在模組(2)中，我們已取得了車輛的軌跡與車速的資訊，在未注意前車狀態與未保持安全
車距此兩類事件中，通常是指前車車速減慢或是靜止而後車車速未相對減緩與保持距離，若
能在除了車輛的異常行為偵測外，在交通監視系統中建立一個即時的碰撞預測系統不僅可以
早期提出警告外，也可以提早啟動關鍵性資料的偵測與記錄以利於記錄從事件發生前到結束
的完整過程，因此在本模組中，我們完成了一個基於視覺的車輛碰撞預測系統。由於碰撞預
測對於時間有嚴格的要求，例如車輛在不同深度時有不同的大小，若要以不同大小的模板來
偵測將會花費大量的時間，使預測的可行性降低，因此要提高車輛偵測的效率才能有效地達
成預測的機制。我們在本模組中利用影像中的深度資訊來決定車輛偵測模板的大小以提高偵
測的效率，並利用深度資訊來預測車輛的未來位置以達成碰撞預測。 
在本模組中首先要進行消失線與消失點偵測才能取得深度資訊圖以獲得深度資訊，並建
 20
 
(4)交通違規偵測模組： 
本模組發展智慧型系統取締交通違規事件，分為兩部分，一個為靜態違規，另一為動態
違規，例如：越線受罰、佔用機車停車格等屬於靜態違規，而令一種機車非兩段式左轉，則
屬於動態違規，分別描述如下： 
 
4.1：靜態違規 
在十字路口有許多禁止車輛停止的區域，如車輛停止於紅線上與紅燈時機車停等區有汽
車停止佔用等違規，利用路口的攝影機進行舉證可節省人力，以及提高駕駛人的警覺性，藉
以減少違規的發生。由於我們在模組(3)中具有車速的資訊，在車輛停止的事件中，車速為 0
則是靜止，而車速大於 0 為移動，當目標車輛的位置在目標區域內，根據車輛種類以及其狀
態為停止或是移動，汽車停止於機車停等區為非法，而機車與汽車停止於紅線皆為非法，決
定是否合法靜止於該目標區域，並利用 PTZ 攝影機，針對非法車輛進行拍照舉發的動作。建
立偵測車輛違規停止模組之流程如下： 
1. 定義出欲偵測之目標區域(機車停等區或紅線區域)，採用 95 年度之研究成果，偵測出目
標區域中的車輛以及車輛的速度，速度為 0 則車輛為靜止，速度大於 0 則為移動中。 
2. 針對靜止之目標車輛的種類判別以及根據目標車輛停止的區域，汽車與機車不得停止於紅
線，汽車不得停止於機車等停區，根據條件決定目標之合法性。 
3. 針對非法之目標車輛，以 PTZ 攝影機進行拍攝紀錄，並標釋出車牌位置，供違規舉證。 
 
4.2：動態違規 
在許多設置有人行天橋與地方道之路口，有許多行人依舊直接穿越馬路導致事故發生，
利用路口的攝影機偵測行人進入馬路，並進行攝影，如果有事故發生便可有紀錄還原真相。
根據行人的移動軌跡以及速度，可以得知行人是否有可能進入馬路，當有行人進入到馬路時，
便針對進入馬路的行人進行錄影，如此有紀錄可在事故發生時，有紀錄供調閱，還原真相，
釐清責任，並可根據當時車輛的軌跡與速度得知車輛是否有禮讓行人，。建立偵測行人穿越
馬路之模組流程如下： 
1. 採用 95 年度之研究成果，畫定行人或車輛之禁止區域，偵測出在馬路上的移動目標為車
輛或是行人，若於禁止區域出現非法車輛或行人即為違規。 
2. 當偵測出有移動目標為行人時，利用第二年之研究偵測出行人的軌跡與速度，驅動攝影
機，紀錄行人穿越馬路的過程，如有事故發生，可供還原事情真相。 
 22
 
圖十：物件偵測器系統架構區塊圖。 
本研究整合統計式、類神經網路與 adaboost 偵測器之技術，於偵測器架構進行改善，採
用由粗糙到細緻的策略，提出一整合型的物件偵測架構，如圖十一所示細節架構，主要是從
兩方面改善偵測速度，第一：降低每一個檢查視窗的時間，利用類神經網路架構，快速地檢
查每一個視窗，判斷是否為一物件視窗？第二：減少檢查視窗的數目，透過簡單的特徵預先
過濾大部分背景區域，在利用複雜的判斷器(verifier)決定目標物的位置與大小。 
在類神經網路訓練過程中，利用統計方式，決定自動類神經網路的架構，並僅需 224 與
88 分鐘，便可以完成粗糙層級與細緻層級的類神經網路訓練過程，再利用已訓練好之資訊，
重複利用，建構特徵影像基礎(featuremap-based)之預先過濾的機制，僅需 24 小時內便可完成
訓練。其細部偵測器建制程序如下： 
 
 
圖十一：物件偵測器細部架構 
 
 24
 
圖十二：物件偵測器應用於人臉偵測之結果 
 
(6)人臉辨識 
 
6.1 人臉模辨識組基本概念： 
人臉辨識一直是一個相當熱門的研究領域，它的困難之處在於人臉影像具有高度的非線
性分佈，因此難以線性的方式將不同類別的資料分割，此外，本計畫中的人臉資料預設只能
收集到少量樣本，因此如何以少量樣本達到良好的辨識率亦是一個熱門的研究議題，本模組
的目的便在於提出一新的空間轉換方法稱之為 Nearest Neighbor Line (NNL)，此方法能夠以少
量樣本虛擬出人臉的線性變化，並且能將資料盡量以線性且保留區域拓樸結構的方式呈現，
可以在轉換空間中使不同類別的資料盡量呈現原空間中線性分佈的關係，並藉此達到提高分
類能力的目的。在實驗結果中我們發現，本模組所提出的方法，在人臉辨識率中有相當好的
效果。 
假設目前我們有 N 筆 D 維的資料，而目標函是衡量任三筆資料間的線性程度，如式(8)
與圖十三所示： 
 
 26
 
( )
WSW
WSW
WJ
W
T
B
T
W
optFLD maxarg= . (12) 
 
此處， BS 是組間的變異， WS 是組內的變異，而W 則是具有類別資訊的最佳化轉換矩陣，
以下則分別介紹本模組中所使用的組內變異與組間變異。 
 
6.2 區域化組內變異學習： 
假設 [ ] LNDLNN xxxxx ×= KK ,,,,X 2111211 為訓練樣本，每一個樣本皆歸屬於某一 L類別{ }LppC 1= ，
主要精神在於求得每一個樣本點到同一類別中其它任兩樣本點上的向量，並使其最小化，因
此，Local within-class 的最佳化如式(13)： 
 
( ) ( )
( )( ) ( )
T
ww
p
ijk
L
p
N
i
N
j
N
k
p
ijk
p
ij
p
ijk
p
j
p
i
p
ij
pkji
T
p
ijk
p
ij
p
ijk
p
ij
p
ijk
L
p
N
i
N
j
N
k
p
ijk
p
ijkw
FFS
xFxxx
NkNjNikji
Cxxx
xxS
FSJ
2
1
sinθ,and
,1,,1,,1and
,,where
sinθsinθ
2
1
2
1w
1 1 1 1
1 1 1 1
2
=
⎪⎪⎩
⎪⎪⎨
⎧
=−=
===≠≠
∈
=
=
∑∑∑∑
∑∑∑∑
= = = =
= = = =
KKK  
(13) 
其中， pijkS 為組內相似度權重，定義如下： 
 
⎪⎩
⎪⎨
⎧=
otherwise,0
 ofector shortest v  among is ,1 i
p
ijkp
ijk
xqFS   (14) 
 
p
ijkS 可以決定每一個樣本點所要取的點到線的最短向量的個數，因此，當 pijkS 中的 q 很小時，
我們可以保留下區域的拓樸結構。 
 
6.3 區域化組間變異學習： 
在上述過程中，我們得到了 within scatter matrix，接下來我們要求的是 between scatter 
matrix，主要精神在於求得每一個樣本點到其它類別中其它任兩樣本點上的向量，並使其最大
 28
 
最後，將組內變異與組間變異兩個矩陣代入 fisher’s criterion，即可求出所要的空間轉換
矩陣。 
最後我們進行本模組的實驗，我們以 CMU 資料庫來驗證我們人臉辨識模組的強健性，
第一個實驗我們用以驗證本人臉辨識模組的空間轉換能力，我們取資料庫中的任三個人的所
有影像資料來表示，從圖十五可以發現，我們的人臉辨識模在降到二維空間後，不僅能將三
個類別的資料予以切割，亦能在資料分佈上保留區域變化，例如人臉的光線、表情以及姿勢
等，此外還能將這些變化的線性關係保留下來。而圖十六則將本模組(PCA+NNL)的方法與其
它相關的人臉辨識方法作比較，此一比較是在每一類別的 170 張影像中取 5 張作為訓練樣本，
而其它影像作為測試樣本的比較結果，可以發現在本計畫中所提出的人臉辨識模組表現較其
它方法在辨識率上有較優異的表現。 
 
圖十五：人臉辨識模組低維空間轉換結果 
 
 30
訊，可能有車牌的區域，對最單純的車牌文字11-1111，至少要有12的水平投影量，而車
牌的高度，再找出連續大於12 水平投量的部分，其高度介於15 個像素到30 個像素之間
的水平區間，便是可能的車牌區域。 
5. 找到影像上可能有車牌區域存在的區間後，我們再對這樣區間進行垂直投影的計算，這
裡提到的垂直投影，是指以垂直方向進行累加。藉此找到再精確的可能區域。 
6. 車牌區域的確認，因為在對可能的水平區域進行垂直投影的計算，很容易有雜訊的出現，
即非車牌區域亦會有累加值的出現，這些雜訊可能會導致錯誤的偵測發生，所以我們要
先對這些垂直投影量進行過濾的動作，於是我們把投影量小於2 的部分去除。接著發現
其他的垂直投影會有離散的現象，所以必須對這些投影量進行合併的動作，投影值接近
的部分，我們將兩者合併在一起。而合併的區間也必須去考慮到合併的大小，太小會有
遺失資訊的問題，而太大則會把不是車牌的區域也併入車牌區域。所以根據實驗的結果
發現，參考水平區間的高度，間距小於高度三分之一者，便將兩者合併。我們便能得到
可能的車牌區域位於水平區間兩倍到四倍的寬度中。 
7. 對影像進行驗證是否確定有無車牌的出現，這個部分分為兩種情況，首先為找不到車牌
區域出現在影像上時，當這樣情形發生時，我們改變第一次進行邊緣偵測的閥值，以一
定的比例逐漸縮小，重新對影像進行可能的車牌區域尋找，以避免影像上車牌區域的出
現。這是為了避免因為車牌區域的亮度對比過低而導致被系統給忽略掉的可能。而當有
車牌區域的存在時，亦必須進行驗證的動作，避免把相似的車牌區域的部分誤認為車牌
區域。驗證車牌區域首先重新對找到的車牌區域進行水平投影，重新計算水平投影值是
否有大於12，以避免錯把非車牌區域的邊緣資訊也納入計算。當車牌影像的清晰度不足
時，可能找到過小的車牌區域，所以我們在找到的車牌區域附近上下左右各擴張車牌高
度的五分之一的高度，藉此避免不完整的車牌區域導致車牌文字切割時發生錯誤。 
8. 對車牌區域的影像進行正規化的處理，因為偵測出來的影像可能不符合我們的作文字辨
識時，欲輸入類神經網路的格式，所以我們必須對影像進行正規化的動作。而車牌區域
被切割出來時，可能包含其他的不必要的部分，所以我們對影像作二值化的處理，從車
牌區域的中間往上與往下找到水平投影為零的投影量，以此為文字的上下邊界，至此為
最完整的車牌區域偵測結果。 
 
(8)車牌辨識模組： 
當我們得到車牌區域的影像後，我們便要針對影像進行文字的辨識，而文字的辨識，首
先要將車牌文字從車牌影像獨立切割出來，以便利用辨識時，將文字的影像輸入到類神經網
 32
8.2文字辨識模組： 
在切割完文字之後即進行文字辨識，本模組進行車牌文字辨識的部分是利用CNN類神經
網路進行文字的判斷，以倒傳遞演算法為基礎，加上一些限制條件，加速類神經網路的計算
時間與收斂速度。以下將介紹本論文使用的倒傳遞類神經網路，加上區域性感受野與加權值
共享等限制。此模組所使用的CNN架構如圖十七所示： 
 
圖十七： 車牌辨識CNN架構 
 
從圖十七中，第一隱藏層與第二隱藏層分別有四個特徵映像，所以也共享四個加權值，
偏移值也以共享的方法進行運用，就是一個特徵映像中，僅有一個自由偏移值，所以第一隱
藏層中的神經元有5x5 的感受野，自由加權值共有25 個以及一個自由偏移值。接著第二層的
隱藏層中則有3x3 的感受野，而第一隱藏層有4 個特徵映像，第二層的神經元都有四個獨立
的感受野與第一隱藏不同的特徵映像連結，因此在第二隱藏層中有著36 個自由加權值與4 個
自由偏移值。輸入層的資料為11x21 的影像資料，在第一隱藏層中神經元的感受野為5x5 的
大小，因為在第一隱藏層要有17x7 個神經元才能夠涵蓋所有的輸入層。為了降低運算量，所
以我們在第一隱藏層中的特徵映像垂直方向利用稀疏取樣，我們將稀疏取樣的係數為2，這表
示垂直方向相鄰的神經元感受野相距兩個輸入連結，所以我們只用了9 列的神經元，就可以
 34
 
圖十八：雙攝機組影像 
 
在目標物攝影機可以透過場景攝影機的驅動取得目標物的清晰影像後，接著便是要計算
目標物攝影機的影像，進行車牌偵測與辨識的動作。因為PTZ 攝影機在拍攝目標物的影像
時，會連續取得多張影像，所以在進行車牌辨識時，直到車牌消失前，會累計車牌辨識的結
果，當車牌消失於畫面上時，輸出辨識結果，以提高辨識結果的成功率，如圖十九所示。 
 
 
 36
 
根據所有偵測到的結果，我們統計所有字元出現的次數，最後輸出次數最高的結果為此
次車牌的辨識結果。上述的辨識過程中，我們可以得到6H7227 的為最終結果，而過程中有
一次的辨識結果為687227，因為我們利用統計的方式，少數的錯誤將不影響最終的輸出結果。 
五、研究成果自評 
本研究建構出一多台攝影機組之視訊監控系統，能夠對整個十字路口場景進行完備之監
控任務，並對特定目標物進行特寫鏡、追蹤、放大(zoom in)與縮小(zoom out)等工作，不僅在
實務上能夠對車輛與行人進行偵測、追蹤、與記錄等相關行為影像及視訊處理，而且提出多
個偵測技術與辨識技術作為新理論，在理論與實務上皆有所貢獻。未來將此一技術整合至
embedded system上，增加其實用性。 
參考文獻 
[1] C. C. Han, H. Y. Liao, G. J. Yu, and L. H. Chen, “Fast face detection via morphology-based 
pre-processing,” Pattern Recognition, vol. 33, pp. 1701–1712, 2000. 
[2] M. Yang, D. Kriegman, and N. Ahuja, “Detecting faces in images: A survey,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 24, pp. 34–58, 2002. 
[3] E. Hjelmas and B. K. Low, “Face detection: A survey,” Computer Vision and Image 
Understanding, vol. 83, pp. 236–274, 2001. 
[4] B. Moghaddam and A. Pentland, “Probabilistic visual learning for object detection,” in Proc. 
5th International Conference on Computer Vision, (Cambridge, Mass.), pp. 786–793, Jun. 1995. 
[5] K. K. Sung and T. Poggio, “Example-based learning for view-based human face detection,” 
IEEE Transactions Pattern Analysis and Machine Intelligence, vol. 20, pp. 39–51, January 
1998. 
[6] A. N. Rajagopalan, C. Chellappa, and N. T. Koterba, “Background learning for robust face 
recognition with PCA in the presence of clutter,” IEEE Transactions on Image Processing, vol. 
14, pp. 832–843, 2005. 
[7] J. T. Chien and C. C. Wu, “Discriminant waveletfaces and nearest feature classifiers for face 
recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, pp. 
1644–1649, 2002. 
[8] C. Liu, “A Bayesian discriminating features method for face detection,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 25, pp. 725–740, 2003. 
[9] P. Shih and C. Liu, “Face detection using discriminating feature analysis and support vector 
machine,” Pattern Recognition, pp. 260–276, 2006. 
 38
[28] N. Sumpter and A. Bulpitt, ＂Learning spatio-temporal patterns for predicting object 
behavior,” Image Vision. Computing, 2000 
[29] W. M. Hu, D. Xie, and T. N. Tan, “A hierarchical self-organizing approach for learning the 
patterns of motion trajectories, IEEE Trans. on Neural Networks, 2004 
[30] J. Owsens and A. Huter, “Application of the self-organizing map to trajectory classification,” 
in Proc. IEEE Int. Workshop Visual Surveillance, 2000. 
[31] Stauffer and W. Grimson, “Adaptive background mixture models for real-time tracking,” in 
Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol.2, 1999, pp. 246-252. 
[32] Toyama, J. Krumm, B. Brumitt, and B. Meyers, “Wallflower: principles and practice of 
background maintenance,” in Proc. Int. conf. Computer Vision, 1999,pp. 255-261. 
 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
2
 
poses and expressions and, furthermore, human faces are not evenly lit. Pose, illumina-
tion, and expression (PIE) usually create most critical problems in face recognition. 
Three stages, face representation, discriminative feature analysis, and data classification, 
comprise the identification process. In the first stage, effective features are extracted 
representing face images of one person with various expressions, illuminations, and pos-
es. In stage II, the best discriminative bases for dimensionality reduction are found by 
minimizing the scatter of data points. Metric distances between templates and an input 
sample are calculated for data classification in stage III. 
Principal component (PC) is the most popular representation in an eigenspace [1]. 
The most representative and global features of face images are extracted by principal 
component analysis (PCA). Its main goal is to find the best representation of the original 
images. Sufficient information could be kept in low dimensional feature spaces. In addi-
tion, high dimensional face images are reduced into low dimensional feature vectors. 
Yang et al.[2] extended the 1D-PCA to 2D-PCA. Moreover, face images can be 
represented in vector forms using the kernel PCA[3], Gabor-based PCA[4], wavelet 
transform(WT)[5], and discrete cosine transform(DCT)[6] methods. All of them 
represent a whole face appearance as a feature vector, globally. Recently, a new face 
representation method is proposed based on the Laplacian matrix transform [7, 8]. He et 
al.[7] proposed a manifold learning method called Laplacianface which can preserve the 
local topology and information for recognition. Furthermore, Yan et al.[8] and Hu[9] 
used the priori class information in the discriminant analysis for recognition improve-
ment. In their approaches, a single feature vector is ‘globally’ represented by the whole 
face image. They preserve the local structure in the eigenspaces. In contrast, a descriptor 
extracted from local facial regions is another representation using local features. In [10], 
facial components are first represented in modular eigenspaces. Moreover, facial land-
marks are extracted for describing the facial components using the Gabor filter. Ahonen 
et al.[11] describe the appearance of local facial regions based on local binary pattern 
texture features. Ten facial components are detected and combined into a feature vector 
by ignoring the cheeks [12]. According to their conclusions, the component-based face 
recognition system outperforms the global approach. However, two critical problems 
should be solved in their approach: accurately locating the facial components (i.e. region 
of interested, ROI) and effectively fusing the features of ROIs. 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
4
 
 
Figure 1: The proposed recognition/authentication scheme. 
2. THREE-STAGE FACE-BASED SCHEME 
In this study, three stages, face representation, discriminative feature analysis, and 
data classification, comprised the identification process. In addition, multiple classifica-
tion results were fused to obtain the final results. 
 
2.1 Face representation 
 
The performance of face recognition highly depends on PIE variation. Many ap-
proaches have been proposed for reducing the effects of the changes, such as the Gabor 
filter function, the derivatives of distribution, the logarithmic transformation, the WT, 
the discrete cosine transformation on an edge map, and so on. According to the conse-
quence in [5], low frequency components extracted by WT are less sensitive to varying 
images and containing the higher discriminating power. Therefore, the first step in our 
scheme is to extract the low-frequency features of face images via WT for reducing PIE 
changes. Basically, lighting on a local face region is assumed to be in a linear change or 
uniform. As shown in Fig. 2, our simple approach is to partition the face images into four 
parts, i.e. parts L, R, T, and B. An assumption is made in this study: The light comes from 
one direction, and the illumination on each half face is uniform or linearly changed. 
Without losing the global face features, a new small face image F is generated in a 
coarse scale via the WT. The advantage of our face representation is that: Local features 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
6
 
generalized the projection into the null space and found the discriminative common vec-
tors (DCVs). Unlike the PCA-based or LDA-based approaches, DCVs were operated in 
a null space of the original feature space. The DCV-based method tries to find a trans-
formation matrix which is subject to the following equation: 
             WSWWSWWJ T
T
WWS
TW
B
T
WWS
TW
optFLD
00
maxargmaxarg)(
==
== ,           (4) 
where BWT SSS +=  is a total scatter matrix. In addition, the common vectors form 
a subspace in which the scatter between classes equals the total scatter. The scatter of the 
training samples is only generated from the between-class scatter, and the within-class 
scatter nearly equals zero. 
 
2.3 Data classification 
 
In the recognition process, MK  training samples of K  persons were collected. 
Due to the S3 problem, the DCVs were found, and the transformed prototypes were gen-
erated for matching. The most popular classification rule is the nearest neighbor match-
ing of the prototypes using the Euclidean-based distance. The nearest-feature-space 
(NFS)[5] strategy was adopted in this study. A new testing sample was transformed by 
the DCVs. Three kinds of distance-based computation in the transformed feature space 
were designed as follows: (1) the nearest point-to-point (i.e. the nearest neighbor, 1NN), 
(2) the nearest point-to-line, and (3) the nearest point-to-space. The last two approaches 
generated the pseudo prototypes of a class when the training samples were few. In this 
study, the smallest distance of these three kind distances )),(()( ii xfdxd Φ=  was cho-
sen, and the ID of the testing sample was thus determined. 
In the authentication process, M training samples of an individual X in the 
enrollment stage were considered. Similarly, M randomly selected samples from the 
other persons were collected to be the negative samples. This is considered to be a 
two-class classification problem. Personal DCV-based feature space )(xΦ for individual 
X  was thus constructed. If a sample x belongs to class X , the distance value should be 
smaller than a threshold value Xt . Otherwise, it is not a member of class X , i.e., a 
forged sample. 
In this study, five modular feature spaces iΦ  were constructed and five distances 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
8
 
e.g., image size, the training and testing samples for each data set were summarized in 
Table 1. Moreover, all of them were aligned with the eyes and mouths. 
The experimental results were divided into two groups: one for recognition and the 
other for authentication. The rates of the proposed scheme are listed in Table 1. In addi-
tion to the proposed scheme, eight alternative schemes were implemented and compared. 
In general, images were first reduced by the WT for dimension reduction as shown in 
Fig. 3. On the other hand, the WT process was skipped in evaluating the feature pre-
serving power of WT. Four possible combinations were compared. First, the DCV-based 
transformation was replaced with the conventional LDA transformation in Fig. 3(a). 
Second, the NFS-based rule was replaced by the nearest neighbor (1NN) in each clas-
sifier (Fig. 3(b)). The first two schemes were classified into the multiple-classifier-based 
approach. Third, images were only transformed by the DCVs and classified by the NFS 
rule as shown in Fig. 3(c). These schemes are similar to the approach of Cevikalp et 
al.[16]. Lastly, the original LDA-based schemes were implemented as shown in Fig. 3(d). 
These combinations were repeated again by skipping the WT process, e.g. row R4 means 
the scheme in Fig. 3(a) skips the WT process.  
The recognition rates for the proposed approach and the alternative schemes are 
shown in Table 1. Each recognition rate was obtained by running the process for 10 
times. The training samples were randomly selected from the data sets and the other 
samples were used for testing as listed in Table 1. Images were transformed by WT for 
dimension reduction as shown in rows R1, R3, R5, R7 and R9. WT preserved the dis-
criminant power and reduced the noises in a high dimension because the recognition 
performances were almost similar, e.g. the data pairs: (R1, R2), (R3, R4), (R5, R6), (R7, 
R8), and (R9, R10). The DCV-based scheme had more discriminating power than the 
LDA approach when the S3 problem occurred as shown in the compared results listed in 
the row pairs (R1, R3) and (R2, R4) expect for data sets CMU and IIS, (R7, R8) and (R7, 
R9) for all five data sets. Features were projected to and matched on the null spaces. The 
inverse of within-class scatter matrix was calculated by an SVD-based approach when 
the S3 problem occurs. However, the recognition performance was almost the same be-
cause the S3 problem did not occur, e.g. (R1, R3) and (R2, R4) for CMU and IIS data 
sets. In order to show the performance of DCVs on data sets CMU and IIS, a new 
process was run by setting the image size as 64 by 64. The recognition rates on data set 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
10
 
sifier. For each classifier, the DCVs and the NFS rules are adopted for solving the S3 
problem and increasing the matching prototypes. Finally, the weighted summation fuses 
the multiple classifiers by automatically calculating the weighting values. 
REFERENCES 
1. M. Turk and A. Pentland, “Eigenfaces for recognition,” Journal of Cognitive Neu-
roscience, vol. 3, no. 1, pp. 71–86, 1991. 
2. J. Yang, D. Zhang, A. Frangi, and J. Yang, “Two-dimensional PCA: A new ap-
proach to appearance-based face representation and recognition,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, vol. 26, pp. 131–137, 2005. 
3. J. Yang, A. F. Frangi, J. Y. Yang, D. Zhang, and Z. Jin, “KPCA plus LDA: A com-
plete kernel Fisher discriminant framework for feature extraction and recognition,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, pp. 
230–244, 2005. 
4. C. Liu, “Gabor-based kernel PCA with fractional power polynomial models for 
face recognition,” IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, vol. 26, pp. 572-581, 2004. 
5. J. T. Chien and C. C. Wu, “Discriminant waveletfaces and nearest feature classifi-
ers for face recognition,” IEEE Transactions on Pattern Analysis and Machine In-
telligence, vol. 24, pp. 1644–1649, 2002. 
6. M. J. Er, W. Chen, and S. Wu, “High-speed face recognition based on discrete co-
sine transform and RBF neural networks,” IEEE Transactions on Neural Networks, 
vol. 16, pp. 3679–691, 2005. 
7. X. He, S. Yan, Y. Hu, P. Niyogi, and H. J. Zhang, “Face recognition using Lapla-
cianfaces,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 
27, pp. 328–340, 2005. 
8. S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, and S. Lin, “Graph embedding and 
extensions: A general framework for dimensionality reduction,” IEEE Transactions 
on Pattern Analysis and Machine Intelligence, vol. 29, pp. 40–51, 2007. 
9. H. Hu, “Orthogonal neighborhood preserving discriminant analysis for face recog-
nition,” Pattern Recognition, vol. 41, pp. 2045–2054, 2008 
10. A. Pentland, B. Moghaddam, and T. Starner, “View-based and modular eigenspac-
es for face recognition,” in IEEE Conference on Computer Vision and Pattern 
Recognition, (Seattle, Washington), pp. 84–91, Jun. 1994.  
11. T. Ahonen, A. Hadid, and M. Pietikainen, “Face descriptor with local binary pat-
terns: Application to face recognition,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, pp. 2037–2041, 2006.  
12. B. Heisele, P. Ho, J. Wu, and T. Poggio, “Face recognition: Component-based 
versus global approaches,” Computer Vision and Image Understanding, pp. 6–21, 
2003. 
13. R. Lotlikar and R. Kothari, “Fractional-step dimensionality reduction,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 22, pp. 623–627, 
2000. 
14. K. Etemad and R. Chellappa, “Discriminant analysis for recognition of human face 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
12
 
Table 1. The rates for various recognition schemes for four benchmarks(%). 
Data Sets CMU YALE ORL IIS 
Image size 32x32 32x32 48x48 48x48 
Training samples 1020 60 200 768 
Testing samples 10540 105 200 3072 
R
at
es
 
R1 Fig. 2 84.60 81.91 95.50 82.97 
R2 Fig. 2 - WT 86.45 87.59 96.50 85.61 
R3 Fig. 3(a) 86.20 80.95 93.25 83.69 
R4 Fig. 3(a) - WT 86.47 81.91 94.50 84.47 
R5 Fig. 3(b) 69.22 77.14 90.25 78.02 
R6 Fig. 3(b) - WT 69.61 78.09 90.75 78.12 
R7 Fig. 3(c) 77.14 79.05 94.5 79.89 
R8 Fig. 3(c) - WT 74.43 78.10 93.25 79.39 
R9 Fig. 3(d) 80.13 76.19 91.5 83.57 
R10 Fig. 3(d) – WT 78.30 75.24 90.5 83.08 
 
Table 2. The rates of modular-based and vector-based approaches for CMU and Yale 
benchmarks(%). 
Data Sets CMU YALE
LPPFace[7] 80.48 81.90
MFA[8] 86.35 84.76
ONPDA[9] 85.96 83.80
OLPPFace[18] 83.15 82.85
Fig. 2 84.60 81.91
Fig. 2 - WT 86.45 87.59
 
Table 3. The parameters for various authentication schemes for four benchmarks(%). 
Data Sets CMU YALE ORL IIS 
Image size 32x32 32x32 48x48 48x48 
Enrollment 
for a person 
PS 30 4 5 6 
NS 30 4 5 6 
Testing for a 
person 
PS 140 7 5 24 
NS 9380 98 195 3048 
Total testing 
samples 
PS 9520 105 200 3072 
NS 637840 1470 7800 390144 
 
 
YING-NONG CHEN, CHIN-CHUAN HAN, CHENG-TZU WANG, AND KUO-CHIN FAN 
 
14
 
Ying-Nong Chen (陳映濃) was born in Taipei, Taiwan, in 1976. He 
received the B.S and M.S degrees in information management and 
informatics from the Nan Hua University, Taiwan and the Fo Guang 
University, Taiwan, in 2000 and 2003, respectively. He is currently 
pursuing the Ph.D degree in computer science and information engi-
neering at the National Central University, Taiwan. His research 
interests include pattern recognition, and computer vision.  
 
 
Chin-Chuan Han (韓欽銓) received the B.S. degree in computer 
engineering from National Chiao-Tung University in 1989, and an 
M.S. and a Ph.D. degree in computer science and electronic engi-
neering from National Central University in 1991 and 1994, respec-
tively. From 1995 to 1998, he was a postdoctoral fellow in the Insti-
tute of Information Science, Academia Sinica, Taipei, Taiwan. From 
2000 to 2004, he worked with the department of computer science 
and information engineering, Chunghua University, Taiwan. In 2004, 
he joined the department of computer science and information engineering, National 
United University, Taiwan, where he became a professor in 2007. Prof. Han is a member 
of IEEE, SPIE, and IPPR in Taiwan. His research interests are in the areas of face recog-
nition, biometrics authentication, video surveillance, and pattern recognition. 
 
Cheng-Tzu Wang(王鄭慈) is currently an associate professor in 
the Department of Computer Science at National Taipei University 
of Education, Taiwan. He received his M.S. and Ph.D. degrees in 
the Center for Advanced Computer Studies from the University of 
Louisiana in 1991 and 1994, respectively. His current interests in-
clude image processing, hybrid soft computing models, and soft-
ware engineering. 
 
 
Kuo-Chin Fan(范國清) was born in Hsinchu, Taiwan, R.O.C., 
on June 21, 1959. He received the B.S. degree in electrical en-
gineering from the National Tsing-Hua University, Hsinchu, in 
1981, and the M.S. and Ph.D. degrees from the University of 
Florida (UF), Gainesville, in 1985 and 1989, respectively. In 
1983, he joined the Electronic Research and Service Organiza-
tion (ERSO), Taiwan, as a Computer Engineer. From 1984 to 
1989, he was a Research Assistant with the Center for Informa-
tion Research, UF. In 1989, he joined the Institute of Computer 
Science and Information Engineering, National Central Univer-
sity, Chung-Li, Taiwan, where he became a Professor in 1994. From 1994 to 1997, he 
was Chairman of the Department. Currently, he is the Director of the Computer Center. 
His current research interest includes image analysis, optical character recognition, and 
document analysis. Prof. Fan is a member of SPIE. 
Process，演講者分別為 Jaromír Štolc, Ministry of Transport of the CR，Peter Jusko, 
Czech Airlines，Jan Klas, Air Navigation Services of the Czech Republic，Josef Rada, 
Civil Aviation Authority，他們都是捷克相關安全單位的重要主管，對於我們只著重
於技術研發的人員而言，是另一種經驗。針對 User Experience of Security Systems
與 Port, Perimeter and Airport Securit： 
z User Experience of Security Systems 
1. Millimeterwave Doppler Surveillance Radar adds Biorhythm Signature to Reduce 
False Alarm Rate : 本論文探討如何減少監控用雷達上的雜訊。 
2. Aspects of Image Quality Enhancement in Security Technology :本論文提出幾種
影像品質強化的方式，主要是探討如何取得適的的threshold的方法。 
3. User Experiences with an Unobtrusive Decision aid for Deception Detection : 本
論文提出一個新系統，可以檢視動作和語意的行為。 
4. Securing a Health Information System with a Government Issued Digital 
Identification Card : 本論文探討IC card中該儲存哪些個人醫療資訊以利醫療
資訊系統的建利。 
5. Potential Noncontact Tools for Rapid Credibility Assessment from Physiological 
and Behavioral Cues : 本論文探討如何以非接觸式的儀器以心理和行為模式快
速判斷信賴程度。 
z Port, Perimeter and Airport Security 
1. Relationship Between Level Of Detection Performance and Amount of Recurrent 
Computer-Based Training : 本論文探討如何以X光線影像為特徵，訓練機場影
像特徵作為安全監控用。 
2. Opportunities for Increased Use of Standards in the Integration of Perimeter 
Intrusion Detection Systems : 本論文探討如何整何不同的入侵者偵測系統。 
3. Security and Crisis Management for Air Transport : 本論文探討如何管理航空機
場的安全及危機管理。 
 
三、考察參觀活動(無是項活動者省略) 
A - Czech Airlines Training Centre 
B - National Integrated Air Traffic Control Centre - IATCC Prague 
C - Air Navigation Services of the CR at Prague Airport 
 
四、建議 
無 
 
五、攜回資料名稱及內容 
Proceeding of 42th IEEE International Carnahan Conference on Security Technology會議論
文一本，包括此次所有發表論文內容。 
to reduce the brightness effects. Two disadvantages should be 
noticed here. First, the DC-free operation was used in a case 
of linear brightness variation, not in a contrast case. Second, 
the approximated discrete coefficients highly depended on the 
training samples. They were hardly determined in various 
environments. 
 Linear discriminant analysis (LDA) is a popular technique 
for discriminant feature analysis. Different from the PCA-based 
representation, the class information was considered in the 
analysis process. The most discriminant bases were found to 
project the original feature vectors into the discriminant spaces 
for identification. Some variants of LDA, such as F-LDA[16], 
D-LDA[5], K-DDA[17], and FD-LDA[18], were proposed for 
increasing the discriminant powers. All their approaches find 
the optimal subspaces by minimizing a criterion, and a sample 
vector is projected into the subspaces using the found 
eigenvectors with nonzero eigenvalues. Moreover, Cevikalp 
[19] proposed another variant of LDA, called discriminative 
common vectors (DCV), for data discriminant analysis. A null 
subspace was constructed in which the within-class scatter 
was set as zero in the training stage. The DCV-based method 
achieves the better performance than the others, especially in 
the case of few training samples. Therefore, the small sample 
size problem is also solved in this approach. However, the 
computational complexity of these statistical approaches 
strongly depends on the dimensionality of original data and the 
number of training samples. Therefore, image transformation 
techniques, such as discrete cosine transform (DCT) or 
discrete wavelet transform (DWT), reduce the dimensionality 
and extract the intrinsic features for face recognition. Er[8] 
applied the DCT, for examples, for dimensionality reduction 
and selected the DCT coefficients to be the input vectors of a 
neural network. Chien[6] used DWT in the similar manner for 
the same purpose. The feature vectors generated by DWT 
contain more discriminant powers than those by DCT. DWT 
could be treated as a pre-process for LDA enhancement. A 
clustering technique is another enhancement of LDA-based 
approach. Er[8] proposed an un-supervised clustering method 
to obtain the LDA-based sequential operations efficiently. 
Kim[20] also used an un-supervised clustering method to 
hybrid the different poses' structures of the LDA to reduce the 
effect of pose variations. 
The novel scheme as shown in Fig. 1 is designed for 
effective recognition and authentication in illumination and 
expression changes. At the first stage, high dimensional face 
images are transformed into small images of low 
dimensionality using discrete wavelet transform (DWT). 
According to the consequences in [8], the sub-mages of 
sub-band LL possess the higher discriminant powers. In order 
to reduce the illumination effect, local features with uniformly 
illumination are extracted. The transformed image is easily 
partitioned into four half-a-face parts: a left face (L), a right 
face(R), a top face (T), and a bottom face (B). Each of them 
represents the local features and a single modal feature space. 
In addition, the sub-image of sub-band LL is transformed 
again to obtain a full face (F) in a smaller scale for globally 
representing the whole face features. The Energy-based 
features of these five sub-images are extracted to enhance the 
discriminant powers. At stage II, discriminative common 
vectors (DCVs) of each modal feature space are obtained from 
the training samples. A nearest matching strategy is adopted 
to output the five modal classification results at stage III. Three 
possible combinations, point-to-point, point-to-line, and 
point-to-space, are designed for increasing the performance. 
Multiple classifiers are thus generated and fused by a 
weighted summation. Similarly, a new test image is 
transformed, partitioned, convoluted, and projected to obtain 
five scores. They are fused to determine the final result. 
II. THREE-STAGE FACE RECOGNITION AND 
AUTHENTICATION 
In this study, three stages, face representation, 
discriminative feature analysis, and data classification, 
comprise the identification process. At stage I, multimodal 
features were extracted for effectively representing face 
images. At stage II, the most discriminant vectors were found 
by minimizing the scatter of data points. Metric distances 
between templates and an input sample were calculated at 
stage III. In addition, multiple classification results are fused to 
obtain the final decision. 
A. Stage I: Face representation 
The performance of face recognition highly depends on the 
variation of pose, illumination, and expression. Many 
approaches have been proposed for reducing the effects of 
their changes, such as Gabor filter function, the derivatives of 
distribution, the logarithmic transformation, the wavelet 
transformation, the discrete cosine transformation on an edge 
map, and so on. They represented the face images using the 
invariant features against the illumination and expression 
problems. Basically, the effects of lighting on local face regions 
are assumed to be in linear change or uniform. One simple 
approach is to directly partition the face images into four 
sub-images, i.e. sub-images L, R, T, and B. An assumption is 
made in this study: The light comes from one direction, and 
the illumination on each half face is uniform or in linear change. 
Without losing the global face features, a new small face 
image F in a coarse scale is generated via the wavelet 
transform at the next scale. The element values in each 
sub-image are convoluted by summarizing the corresponding 
5 by 5 neighboring values. Each face image is represented by 
a modal feature space. 
B. Stage II: Discriminative feature analysis 
Face images are represented in various forms. The most 
salient and invariant features should be analyzed and 
extracted for face recognition effectively. In the last two 
decades, eigenspace-based transformation is the most 
popular linear projection approach in a feature space. 
Samples are projected into a subspace in which the 
within-class scatter is minimized and the between-class 
scatter is maximized. This popular projecting transformation, 
called Fisher linear discriminant (FLD), or LDA, satisfies the 
Fisher's criterion as follows.  
,
||
||maxarg)(
WSW
WSWWJ
W
T
B
T
WoptFLD
 (1)
260
Authorized licensed use limited to: National United University. Downloaded on October 28, 2009 at 22:10 from IEEE Xplore.  Restrictions apply. 
VI. REFERENCES 
[1] D. Swets and J. Weng, “Using discriminant 
eigenfeatures for image retrieval,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 18, pp. 
831--836, August 1996. 
[2] M. Turk and A. Pentland, “Eigenfaces for recognition,” 
Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 
71--86, 1991. 
[3] J. Yang, D. Zhang, A. Frangi, and J. Yangn, 
“Two-dimensional PCA: A new approach to 
appearance-based face representation and recognition,” 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 26,   pp. 131--137, 2005. 
[4] J. Yang, A. F. Frangi, J. Y. Yang, D. Zhang, and Z. Jin, 
“KPCA plus LDA: A complete kernel Fisher discriminant 
framework for feature extraction and recognition,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 27, pp. 230--244, 2005. 
[5] C. Liu, “Gabor-based kernel PCA with fractional power 
polynomial models for face recognition,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 26, pp. 572--581, 2004. 
[6] J. T. Chien and C. C. Wu, “Discriminant waveletfaces 
and nearest feature classifiers for face recognition,” 
IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 24, pp. 1644--1649, 2002. 
[7] X. He, S. Yan, Y. Hu, P. Niyogi, and H. J. Zhang, “Face 
recognition using Laplacianfaces,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 27, pp. 
328--340, 2005. 
[8] M. J. Er, W. Chen, and S. Wu, “ High-speed face 
recognition based on discrete cosine transform and rbf 
neural networks,” IEEE Transactions on Neural 
Networks, vol. 16, pp. 3679--691, 2005. 
[9] A. Pentland, B. Moghaddam, and T. Starner, 
“View-based and modular eigenspaces for face 
recognition,” in  IEEE Conference on Computer Vision 
and Pattern Recognition, (Seattle, Washington), pp. 
84--91, Jun. 1994. 
[10] T. Ahonen, A. Hadid, and M. Pietikainen, “ Face 
descriptor with local binary patterns: Application to face 
recognition,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, pp. 2037--2041, 2006. 
[11] B. Heisele, P. Ho, J. Wu, and T. Poggio, “Face 
recognition: Component-based versus global 
approaches,” Computer Vision and Image 
Understanding, pp. 6--21, 2003. 
[12] Y. Adini, Y. Moses, and S. Ullman, “Face recognition: 
The problem of compensating for changes in illumination 
direction,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 17, pp. 721--732, 1997. 
[13] A. Shashua and T. Riklin-Raviv, “The quotient image: 
Class-based re-rendering and recognition with varying 
illumination conditions,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 23, 2001. 
[14] Y.-J. Song, Y.-G. Kim, U.-D. Chang, and H. B. Kwon, 
“Face recognition robust to left/right shadows: Facial 
symmetry,” Pattern Recognition, vol. 39, pp. 1542--1545, 
2006.
[15] A. N. Rajagopalan, C. Chellappa, and N. T. Koterba, 
“Background learning for robust face recognition with 
PCA in the presence of clutter,” IEEE Transactions on 
Image Processing, vol. 14, pp. 832--843, 2005. 
[16] R. Lotlikar and R. Kothari, “ Fractional-step 
dimensionality reduction,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 22, pp. 623--627, 
2002. 
[17] K. Etemad and R. Chellappa, “Discriminant analysis for 
recognition of human face recognition,” Journal of 
Optical Society America, vol. 14, pp. 1724--1733, August 
1997. 
[18] J. Fortuna and D. Capson, “Improved support vector 
classification using PCA and ICA feature space 
modification,” Pattern Recognition, vol. 37, pp. 
1117--1129, 2004. 
[19] H. Cevikalp, M. Wilkes, and A. Barkana, “Discriminative 
common vectors for face recognition,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 27, pp. 4--13, 2005. 
[20] T.-K. Kim and J. Kittler, “Local linear discriminant 
analysis for multimodally distributed classes for face 
recognition with a single model image,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 27, pp. 318--327, 2005. 
VII. VITA 
Chin-Chuan Han received the B.S. degree in computer 
engineering from National Chiao-Tung University in 1989, and 
an M.S. and a Ph.D. degree in computer science and 
electronic engineering from National Central University in 
1991 and 1994, respectively. In 2004, he joined the 
department of computer science and information engineering, 
National United University, Taiwan, where he became a 
professor in 2007. Prof. Han is a member of IEEE, SPIE, and 
IPPR in Taiwan. His research interests are in the areas of face 
recognition, biometrics authentication, video surveillance, 
image analysis, computer vision, and pattern recognition. 
FIGURE 1: THE PROPOSED 
RECOGNITION/AUTHENTICATION SCHEME. 
262
Authorized licensed use limited to: National United University. Downloaded on October 28, 2009 at 22:10 from IEEE Xplore.  Restrictions apply. 
