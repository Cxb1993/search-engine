 1 
 
行政院國家科學委員會補助專題研究計畫 
■成果報告   
□期中進度報告 
 
3-D MOD：網路立體視訊播放系統－子計畫一：多視角立體視訊編
碼方法最佳化設計(I) 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC 99-2221-E-224 -076 
執行期間： 99 年 8 月 1 日至 100 年 8 月 31 日 
 
執行機構及系所：國立雲林科技大學電子工程系 
 
計畫主持人：蘇慶龍 
共同主持人： 
計畫參與人員：李曜言、房煥偉、賴秉洋、吳國暄、彭康寧 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另頇繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告發表之論文乙份 
□國際合作研究計畫國外研究報告 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            ■涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
 
中   華   民   國  100 年  9 月  29 日 
 
 3 
傳輸上往往都有其傳輸的限制，因此我們需要將多視角影像進行壓縮以降低傳輸頻寬或儲存時之檔案
容量。 
 
三、研究目的 
立體來源影像是包含許多各種不同角度同一時間及地點所拍攝的影片所結合而成，因此多視角的
影像資料量相較於單一視角的資料量更大，而在多媒體的應用和傳輸上往往都有其傳輸的限制，因此
我們需要將多視角影像進行壓縮以降低傳輸頻寬或儲存時之檔案容量。在壓縮系統中多視角視訊影像
壓縮系統(Multi-View Video Coding System，簡稱 MVC)是目前正在發展中針對多視角的視訊壓縮標
準，此系統又稱 H.264/MVC 是目前 H.264/MPEG-4 AVC 的一種延伸標準，且此系統可應用在自由視
角的電視系統或是立體電視系統上，而立體電視系統被預期會是第一個進到大眾消費市場的產品，因
此這標準預計將會成為未來 3D 立體多媒體應用的關鍵標準。由於多視角視訊影像壓縮系統本身具有高
複雜度及高資料量的特性，如何能有效低針對這些多視角影像做有效率的編碼動作以及使資料經壓縮
編碼後能降低儲存空間將是值得研究探討的題目。此外依目前技術顯示，立體螢幕必頇有「深度」資
訊以此建立立體場景。而計算深度時我們藉由壓縮過程中計算得到之視角差來推算深度，並將推算出
的深度資訊結合於壓縮訊號中，如此一來可大量降低顯示器端計算的複雜度，進而降低困難度使立體
螢幕更加普及。 
 
四、研究方法與文獻探討 
(一) 文獻探討 
1. 多視角視訊編碼簡介 
圖一為多視角視訊編碼架構，不同於傳統 H.264/MPEG-4 AVC 編碼架構，多視角視訊編碼器多了
不同視角間的畫面預測。因此，當編碼器依序編碼不同視角畫面時，被參考的不同視角畫面也將會被
預先儲存。由於需要做不同視角間的畫面預測。以下將針對計畫做部分原始架構的介紹： 
 
+
+
QuantizationTransform Entropy
ITransform
Deblocking
Mode Dicision
Motion 
Estimation
Motion 
Compensation
Intra Prediction
Reference view i
Disparity 
Compensation
Reference Other View
IQuantization
Disparity 
Estimation
View i 
picture
Bitstream
 
圖一、多視角視訊編碼架構圖 
 
A. 畫面內預測(Intra Prediction)： 
同一張畫面中利用各個區塊對區塊周邊的像素為單位，搜尋區塊向量預測有八種預測模式，如圖
二畫面內的八種預測模式。 
 5 
 
圖四、從同一畫面不同時間預測宏塊(Macroblock) 
 
D. 視角與時間軸估測結構(View-Temporal Prediction Structure)： 
多視角視訊編解碼標準除了支援時間上的畫面預測，同時提供不同視角畫面的預測，架構上與可
擴展視訊編碼(scalable video coding，SVC)雷同，使用階層式 B 畫面。圖五說明階層式 B 畫面(Hierarchical 
B Pictures)在視角和時間上的預測架構，其中水帄代表時間軸預測而垂直則表示不同的視角預測。個別
的視角影片在時間序列上採用階層式 B 畫面(Hierarchical B Pictures)，而不同視角間則採用傳統
"IBPBPBP...."之預測結構(Prediction Structure)，也就是第奇數個視角的影片序列會利用相鄰兩個視角畫
面作視角的預測。如圖五中 S1，S3 及 S5 三個視角影片序列除了時間上的預測之外，還分別利用「S0，
S2」、「S2，S4」及「S4，S6」做為視角間預測的預測視角。因不同預測架構會帶來不同的編碼效率，
所以預測架構(Prediction Structure)頇配合影片拍攝時相機擺設做更改，才可達到最高效能。圖六為 HHI
針對相機十字擺設的預測架構；圖七為 HHI 針對二維相機陣列擺設的預測架構；圖八為 KDDI 
Corporation 針對[1]一維的相機擺設所提的視角預測架構，若選擇位於中央的相機為主視角的 I 畫面，
則可得到較佳的編碼效率。 
 
 
圖五、基於 MPEG-4 AVC 之階層式 B 畫面視角與時間預測架構[2] 
 
 7 
2. MVC 編碼器快速演算法目前研究現況 
由於科技的發達，電視生產各大廠商正著手研發立體電視，且深信立體電視必將成為家家戶戶必
備的下一代家電產品，連帶許多三維影像的壓縮解壓縮等相關研究也逐漸受到重視，以下將介紹目前
已提出減少壓縮運算量的方式： 
 
A. 多視角群(Group of Multi-View)相對於 NTT 的多畫面群[3]： 
Yonsei University 和 KBS 提出多視角群(Group of Multi-View)的概念[4][5]。其想法雷同於 NTT 的
多畫面群，也就是希望以一個 GOP(Group of Pictures)為最小單位讓不同型態 GOP 可以有不同的預測架
構。但不同於 NTT 的多畫面群定義，一個多視角群代表的是在同一時間點多個不同視角畫面的集合。
如圖九所示，多畫面群的型態可以分為 I-GOMV，P-GOMV 和 B-GOMV 這三類。其 I-GOMV 中的 I0
及 Bn 估測方式如同傳統 H.264 中的 I-Frame 及 B-Frame 作法，而 P-GOMV 畫面群中不同視角畫面間的
預測則允許從不同時間的 GOMV 畫面群來預測。B-GOMV 作法亦同於 B-frame 也就是可以允許兩個不
同時間的 GOMV 畫面群來預測，而多視角群概念(Group of Multi-View)則為 b4 畫面可參考 I0、B1、
B2 和 B3 等的可跨斜角方向預測，參考畫面可為不同時間且不同畫面。但在最近的研究中[6]顯示，在
基本架構(Base GOP)時使用 P-GOMV 圖十會較 B-GOMV 參雜 P-GOMV 圖九好。
 
0 1               2 3              4               5               6               7                    8 
0
1
2
3
4
5
6
7
I0
P0
P0
P0
P0
I0
P0
P0
P0
P0
B1
B1
B1
B1
B1
B1
B1
B1
B1
B1
B1
b4
b4
b4
b4
b4
b4
b4
b4
b4
b4
b4
b4
B2
B2
B2
B2
B2
B2
B2
B2
B2
B2
B2
B2
B2
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
B3
View
Time
I_GOMV
I_GOMV
B_GOMV
B_GOMV
B_GOMV
P_GOMV
P_GOMV
P_GOMV
 
圖九、多畫面群的型態可分 I、B、P 三種型態 
 
 9 
block 或 sub-blocks 被預測的最佳模式，其中黑色、黃色、白色、灰色的 Block 邊線分別代表 inter_ME、
inter_DE (inter-view)、intra、skip mode。如圖十二(a)所呈現，當區域的影像屬於靜態區域時如背景，
會有較高的機率傾向被預測為 skip mode。而在攝影時，因為背景通常佔據較多的畫面比例，所以可以
發現，在所有模式中 skip mode 佔據較大的數量。另外，在圖十二(b)的 Race1 中可以觀察到，即使因
為攝影鏡頭的移動，導致背景也隨之移動，相較於圖十二(a)中的背景固定畫面，在 skip mode 的選擇
上也能有相當接近的結果。此外，從圖十二中也可以看出，inter mode 則有較大的機率出現在移動的物
體上。 
 
Skip Mode
Inter_ME Mode
Inter_DE Mode
Intra Mode
 
Skip Mode
Inter_ME Mode
Inter_DE Mode
Intra Mode
 
圖十二、Ballroom and Race1 sequences 在 FMD 模式下每個 macroblocks 的最好模式. (a)Ballroom sequence. 
(b)Race1 sequence. 
 
根據以上描述做進一步分析，圖十三(a)呈現不同 sequences 的 non-anchor frames mode 分布百分比，
圖中的三個 sequences 為 Ballroom, Exit, Vassar，各自分別代表動態區域的多寡從高到低。從統計可以
看到，在每一個 sequences 的 mode 分布中，skip mode 和 inter_16x16 mode 佔有極大的比例，兩者相加
的比例最高可達到 97%，其中 skip mode 的比例也可從 66%到 92%，並且 skip mode 的分布比例跟動態
區域的多寡成反比。因此，當動態區域越少時如 Vassar sequence，skip mode 的分布比例也隨之增加，
反之則減少。另一方面，如圖十三(a)呈現在同一個 sequence 中，當 basic QP (bQP)越大時，在 mode 的
選擇上 skip mode 比例也隨之增加，而其他 mode 的比例則逐漸降低。根據以上分析說明 mode 的比例
會隨 QP 的改變而成正比變動。而每一個 mode 的運算時間如圖十三(b)的圓形圖所呈現，圖中為
Ballroom、Exit 及 Vassar sequence (search range = ±32)中每一個 MB 的各個 mode 帄均消耗時間比例，
可以看到 inter modes 為 mode decision 中最花費運算時間的部份，所消耗的時間高達約 99%，其中
inter_8x8 (include inter_8x4、inter_4x8 及 inter_4x4)較其他 inter mode 有更高的比例。而 skip mode 和 intra 
mode 僅消耗微小的運算時間。 
Mode distribution ratio
0 10 20 30 40 50 60 70 80 90 100
S
e
q
u
e
n
c
e
s
Vassar(bQP=37)
Vassar(bQP=32)
Vassar(bQP=27)
Exit(bQP=37)
Exit(bQP=32)
Exit(bQP=27)
Ballroom(bQP=37)
Ballroom(bQP=32)
Ballroom(bQP=27)
SKIP_ratio
INTER16x16_ratio
INTER16x8(8x16)_ratio
INTER8x8_ratio
INTRA_ratio
  
圖十三、(a) 不同 sequences 及不同 bQP 的 mode distribution ratio. (b) JMVC software profiling. 
 
 11 
%100%,100 x
T
MrateMatchingx
T
IrateOccurrence 
Case a Case b Case c Case d
O
c
c
u
rr
e
n
c
e
 r
a
te
66
67
68
69
70
71
72
73
74
75
M
a
tc
h
in
g
 r
a
te
96.2
96.4
96.6
96.8
97.0
97.2
97.4
97.6
97.8
Occurrence rate
Matching rate
 
(a) 
MBA
MBB
MBCur MBA
MBD MBB
MBCur MBA
MBB
MBCur
MBC
MBA
MBD MBB
MBCur
MBC
 
   (b)       (c)         (d)           (e) 
圖十五、統計不同 sequences (Ballroom, Exit, Vassar)的四種不同 case (a, b, c, d)的 occurrence rate and 
matching rate. (a) Occurrence rate and Matching rate. (b)Case a. (c)Case b. (d)Case c. (e)Case d. 
 
以上述統計為基礎，我們進一步分析 RDcost 的空間相關性，圖十六呈現 Ballet sequence 畫面中第
7 到 9 列的所有 MB 其 inter mode 和 skip mode 的 RDcost 折線圖，X 軸表示每列的所有 MB number，
Y 軸則代表每一個 MB inter mode 的 RDcost (RDcostinter)和 skip mode 的 RDcost (RDcostskip)。從圖中的
折線趨勢可以發現，每一個 MB 和其鄰近 MBs 的 RDcost 大小都有極高的相關性，尤其是在畫面中的
靜態區域，如紅色線所框貣的部份，當 MB 屬於動態區域時，可以觀察到 skip mode 會有極大的 RDcost，
因為錯誤預測的結果。 
 13 
 4
mod
),(),(
),,(),(
min







otherseothercalculate
yxSKIPyxINTER
yxSKIPyxINTER
ifationterearly
AvgAvg
CurAvg
 
 
當不符合以上提前結束條件時，表示 current MB 屬於較複雜區域，暫時判斷為 non-skip 模式以防
止 PSNR loss。此外，當鄰近 MBs (MBA and MBB)其中一個或兩個 MB 被提前結束而沒有 inter mode 
RDcost 時，會使 current MB 無參考依據，所以如圖十四流程圖必頇選擇 path2 路徑。因此在畫面中的
背景區域有許多 MB 會無法被完全的減少運算複雜度。接下來介紹我們提出的 Prediction RDcostinter 演
算法，用來改善上述演算法的缺點。同樣利用空間的相關性，在圖十六中可看到，當 MB 被選擇為 skip 
mode 時，inter mode RDcost 傾向於接近 skip mode RDcost，並且與鄰近 MBs 也有極高的相關性。因此，
利用上述特性，當MB被提前結束時，inter mode RDcost由鄰近MBs預測，inter mode的Prediction RDcost
計算方式如下: 
 5









Avg
Avg
CurCur
SKIP
INTER
xSKIPINTER
 
 
根據上述的演算法，我們針對 ESMD Using Spatial Correlation 演算法分類為 Prediction RDcostinter 
enable 和 disable 兩個部分做準確性的分析。如圖十四的流程圖所示，在 early skip mode decision 演算法
結束後包含兩個路徑：path1 和 path2。其中 path1 路徑代表在公式(4)中被判斷只有執行 skip mode 的部
份而忽略 inter mode 的運算，另外 path2 路徑則表示沒有提前結束，準備做 inter_16x16 mode 的 RDcost
運算。因為要針對 early skip mode decision 部份做分析，因此我們先將 Prediction RDcostinter 部分
disable。圖十七(a)呈現 Ballroom、Exit 及 Vassar sequences 所統計出來數據的總帄均值，其中圓形圖代
表演算法選擇各路徑的比例，並且包含與 FMD 運算比較的結果，在各自路徑下的 hit rate 和 miss rate
的比例。從圖中可以看到，path1 路徑的 hit rate 高達 99% (30.1 / 30.4 %)，表示在 early skip mode decision
部分有高的準確度，而僅有 0.3%的 miss rate，其中 path1 路徑的 miss 會因提前結束而造成畫面 PSNR
的 loss。雖然 path1 有高的 hit rate，但是 path1 僅預測出一張畫面所有 MB 中約 30% (淺綠色部份)的
SKIP_MB (在 FMD 中選擇 skip mode 為最佳模式的 MBs)。在 path2 路徑的 miss 部分代表還有約 51% (淺
黃色部份)的 SKIP_MB 未被提前結束，而作了多餘的運算。第二部分，我們將 Prediction RDcostinter 部
分 enable。同樣進行上面的分析，從圖十七(b)呈現的結果顯示，在 path1 的比例從原本的 30%增加到
57%，並且能維持高的 hit rate，而整體方面，在全部的 MB 中僅剩下約 24% (淺黃色部份)的 SKIP_MB
沒有被準確的預測。從上面的結果說明，ESMD Using Spatial Correlation 演算法能達到好的預測結果，
以減少 inter mode 運算複雜度，並且能維持好的品質。 
   
Path2 ( Miss )
51.8%
Path1 ( Hit )
30.1%
Path1 ( Miss )
0.3%
Path2 ( Hit )
17.8%
Path1
Path2
              
Path1 ( Miss )
0.5%
Path1 ( Hit )
57.8%
Path2 ( Miss )
24.1%
Path2 ( Hit )
17.6%
Path1
Path2
 
(a)                                    (b) 
圖十七、ESMD Using Spatial Correlation 演算法的 hit rate and miss rate 分析. (a) Prediction RDcostinter is 
disable. (b) Prediction RDcostinter is enable. 
 15 
2. 編碼器快速演算法效能分析 
最後將針對我們提出的演算法執行模擬分析以了解演算法的效能，並且比較原始 JMVC 架構。所
有數據分析及統計實驗結果使用的硬體帄台包含 Intel Quad-Core 2.33GHz 的 central processing unit，
2GB memory，及 Microsoft Windows XP，並且實現在 MVC reference software JMVC 8.2。其測試的設
定如下，group of pictures length 設定為 8，number of reference pictures 設定為 2，在可變大小的 ME 及
DE 使用 full search algorithm，另外 search range 設定為 64，max iterations for bi-prediction search 次數
設定為 4，search range for iterations 設定為  8。在 test sequences 方面，我們使用七種 sequences 包含三
個 frame size為1024x768的 sequences，如“Breakdancers”, “Ballet” and “Uli”，和四個 frame size為640x480
的 sequences，如“Exit”, “Ballroom”，“Vassar”及“Race1”，而這些 sequences 包含不同的 camera arrangement
和影像特性，例如：“Breakdancers”的影像擁有 fast motion，“Ballroom”具有旋轉 motion 影像及複雜的
背景紋理，而“Ballet”具有忽快忽慢的旋轉 motion 影像，“Vassar”具有較多的靜態和 small motion 影像，
“Race1”包含全部畫面影像的 motion，因為攝影機的移動。為了瞭解演算法的 RD performance 和節省的
編碼時間，下面的 Table.II 中，依據“ΔPSNR(dB)”, “ΔBitrate(%)”and “ΔTime(%)”三種比較方式呈現並
且包含不同的 bQP，其中“ΔPSNR(dB)”代表 peak signal-to-noise ratio (PSNR) change，“ΔBitrate(%)”表
示 bitrate change in percentage，“ΔTime(%)”則意指時間節省的比例，各自計算方式如下： 
OriginalAfter PSNRPSNRPSNR   
%100x
bitrate
bitratebitrate
bitrate
Original
OriginalAfter







 

 
%100x
time
timetime
time
Original
OriginalAfter







 

 
 
統計提出的Early Skip Mode Decision for Multiview Video Coding演算法與原始 JMVC的 FMD效能
比較，並且增加多種不同 QP 值，其呈現在 Table.II 中。我們提出的演算法能節省約 76.9%的運算複雜
度，並且僅微不足道的 R-D performance 下降。 
 
TABLE.II Performance Comparison of Proposed Algorithm. 
Sequences Basis QP 
Proposed Algorithm vs JMVC 
ΔPSNR(dB) ΔBitrate(%) ΔTime(%) 
Breakdancers 
24 -0.032 -0.403 58.9 
28 -0.068 -0.359 65.6 
32 -0.051 -0.516 73.1 
36 -0.075 -0.845 78.8 
40 -0.093 -0.992 83.4 
Ballet 
24 -0.022 -0.357 78.5 
28 -0.022 -0.216 82.7 
32 -0.028 -0.311 85.5 
36 -0.039 -0.361 87.9 
40 -0.041 -0.372 89.8 
Uli 
24 -0.055 0.951 55.9 
28 -0.071 0.831 62.0 
32 -0.093 0.620 68.2 
 17 
五、計畫成果自評 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價值（簡要敘述成果所
代表之意義、價值、影響或進一步發展之可能性）、是否適合在學術期刊發表或申請專利、主要發現或
其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
一、 本計劃預計完成之工作項目 
1、開發 MVC Early Skip Mode Decision (ESMD)快速演算法。 
2、開發 MVC Inter Frame Prediction 快速演算法。 
3、開發 MVC Inter View Prediction 快速演算法。 
4、開發 MVC Intra Frame Prediction 快速演算法。 
 
二、 達成之目標 
上述 4 項預計完成之目標均已經順利達成，成功開發了多視訊流之 MVC 快速編碼系統。 
 
三、 綜合評估 
整體而言，本計劃提出之快速演算法相較於傳統 MVC 做法節省約 76.9%的運算複雜度，且
PSNR 值僅降低至可接受範圍 0.1dB 內。 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：■已獲得 □申請中 □撰寫中□無 
技轉：■已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
一、 論文部分 
已經發表 5th FTRA International Conference on Multimedia and Ubiquitous 
Engineering (MUE 2011)會議論文。 
 
二、 專利部分 
多媒體編碼系統之可調式分數型移動估測法/ (Method of Scalable Fractional Motion Estimation 
for Multimedia Coding System)（美國專利通過，領證中） 
 
三、 技轉部分 
計畫成果「適用於視訊壓縮硬體設計之移動向量快速運算技術」技轉股票上市公司「倚強科
 19 
5. 2010 教育部主辦 98 學年度大學校院積體電路(IC)設計競賽 大學組可程式邏輯設計 優
等 吳國暄、房煥偉  
6. 2010 教育部主辦 98 學年度大學校院積體電路(IC)設計競賽 大學組可程式邏輯設計 佳
作 蘇緯翔、林耘生  
7. 2010 參加 98 學年度 教育部主辦全國大學校院積體電路(IC)設計競賽得獎對伍獲得全國
前十強實驗室 (主辦單位統計發佈消息)  
8. 2010 98 學年度大學校院嵌入式系統設計競賽 嵌入式系統軟體組 設計完成獎 黃瑞竣、
朱諺汝、華智豪 (比賽日期：2010/05/22 比賽地點：成功大學) 
 
六、參考文獻 
[1] A. Ishikawa,“Results of Core Experiments 1-D on Multiview Video Coding,” ISO/IEC 
JTC1/SC29/WG11, m13228, 2006. 
[2] K. Mueller, P. Merkle, A. Smolic, and T. Wiegand,“Multiview Coding Using AVC,” ISO/IEC 
JTC1/SC29/WG11,m12945, 2006. 
[3] “Submissions Received in Cfp on Multiview Video Coding,” ISO/IEC JTC1/SC29/WG11, m12969, 
2006. 
[4] A. Ishikawa, ``Results of Core Experiments 1-D on Multiview Video Coding,'' ISO/IEC 
JTC1/SC29/WG11, m13228, 2006. 
[5] K. Sohn, Y. Kim, J. Seo, J. Yoon, J. Kim, C. Park, and J. Lee, “H.264/AVC- compatible Multi-view 
Video Coding,” ISO/IEC JTC1/SC29/WG11, m12874, 2006. 
[6] P. Merkle,A. Smolic,K. Muller,and T. Wiegand, “Efficient Prediction Structures for Multiview Video 
Coding,” IEEE Transactions on Circuits and Systems for Video Technology,17.(11),pp.1461-1473, 2007 
[7] L.-F. Ding, P.-K. Tsung, S.-Y. Chien, W.-Y. Chen,and L.-G. Chen, “Content-Aware Prediction 
AlgorithmWith Inter-View Mode Decision for Multiview Video Coding,” IEEE Transactions On 
Multimedia, vol. 10, no. 8, Dec. 2008 
[8] L.-F. Ding, S.-Y. Chien, and L.-G. Chen, “Joint prediction algorithm and architecture for stereo video 
hybrid coding systems”, IEEE Trans.Circuits Syst. Video Technol., vol. 16, no. 11, pp. 1324–1337, Nov. 
2006. 
[9] G. Zhu, X. Xu, P. Yang, Y. He, X. Meng, and J. Zheng, ”CE4 report,” ISO/IEC JTC1/SC29/WG11 and 
ITU-T SG16 Q.6, JVT-T126, 2006. 
 21 
Sik Jeong) 
 
Room:TULIP 
Kwak) 
 
Room:GERANIUM 
Hyung Lee) 
 
Room:Carnation I 
Karopoulos) 
 
Room:Carnation II 
Tsinaraki) 
 
Room:LILIUM 
Ghil Kim) 
 
Room:JASMINE 
12:00 
13:40 
Lunch 
13:40 
14:40 
Keynote Speech 3, (Chair: Sang-Soo Yeo ) 
Title: Distributed Intelligence - Convergence, divergence and ubiquitous access for future technological 
developments 
by Prof. Habib F. Rashvand, School of Engineering, University of Warwick, UK 
Room: TULIP 
14:40 
15:00 
Break 
15:00 
16:00 
Keynote Speech 4, (Chair: Changhoon Lee ) 
Title: Coordination in Agile Product Line Development 
by Prof. J.C. (Hans) van Vliet, Department of Information Management and Software Engineering (IMSE), 
Vrije Universiteit, The Netherlands 
Room: TULIP 
16:00 
16:20 
Coffee Break 
16:20 
18:00 
Session 6-A 
(Social.-11) 
(Chair: Isaac 
Wiafe) 
 
Room:TULIP 
Session 6-B 
(ISCC-11) 
(Chair: BongHwa 
Hong) 
 
Room:GERANIUM 
Session 6-C 
(EAPIC-IST-11) 
(Chair: Heuiseok 
Lim) 
 
Room:Carnation I 
Session 6-D 
(IWCS-11) 
(Chair: Seung-Ho 
Lim) 
 
Room:Carnation II 
Session 6-E 
(MUE-11) 
(Chair: Marcelo 
S. Alencar) 
 
Room:LILIUM 
Session 6-F 
(MCC-11) 
(Chair: Kilhung 
Lee) 
 
Room:JASMINE 
19:00 
20:30 
Banquet (Room: TULIP) 
 
在參與會議當中，可發現這個會議將Ubiquitous Computing and Beyond、Multimedia 
Modeling and Processing、Ubiquitous Services and Applications、Multimedia Services and 
Applications、Multimedia and Ubiquitous Security、、等多媒體相關主題皆納在內，因此
不管是哪一個研究領域，都可以在這個會議當中看到最新的成果。 
 
二、與會心得 
本次會議，參與者來自各國，尤其以韓國(主辦國家)專家學者居多，其所能錄取的
論文為其中的精華，因此從中可以獲得各國研究人員與業界技術的心得，並且有助於
擴展視野，並能增加與各國人士交流之能力。 
在會場中，常見到許多國外的學者、研究生、業界人士積極參與會議，但國內參加
此會議的專家學者並不多，研究生更寥寥可數，就我三天觀察下來從台灣來的學生大
約只有兩位。可能是此會議之接受率不高，且註冊費相當昂貴。造成國內學者與會不
多之原因。然而該會議發表多為相當先進之成果，因此本國學者參與的不多，實為可
惜。 
藉著此次國際會議參與，不僅認識及接觸一些多媒體影像處理等領域的一流研究人
員，跟他們交換彼此設計技術與研究心得，並探討未來多媒體影像之研究方向，誠為
一不可多得之自我學習機會，與會人士也對我們論文提出之 3D 立體影像處理技術上
的成就和發展贊賞有加，可見此項研究具有高度前瞻性及實用性。參加此次國際會議
研討後，筆者個人心得與收穫可歸納如下： 
1、 與相同領域學者專家交換研究成果及心得，並修正未來研究方向； 
2、 廣泛接觸其他研究領域之學者專家在，增廣見聞，並將相關資料帶回給系上同
 23 
    圖四、報告會場外景       圖五、與台灣學生合影     圖六、與大陸留學生交流 
 25 
aforementioned depth images were combined through 
determine active object depth on background and produced a 
complete depth image. To avoid possible mistakes during 
object search, we modified places where there were rigorous 
changes in depth value, and refined the depth image to its 
perfection. Finally, through DIBR, we were able to produce 
multi-view images through single images, which allow 
spectators to watch 3D videos on monitors through 3D 
Red-Cyan glasses or naked eyes. 
 
 
 
A. Active Object Detection Algorithm 
The traits of motion from active object and background 
of a frame is that if the background within a frame remains 
still, its motion vector (MV) most be around zero, vice versa. 
Since the amount of MV from background is permanently 
larger than those from object in the aforementioned two 
situations, we can filter out the background, and the blank 
block left is the active object. Nevertheless, this form of 
active object detection is not an ideal one, therefore these 
blocks have to undergo a more refined cut and a depth 
construction before it completes object detection. 
 
1) Frame differencing MV statistic and analysis: 
Motion estimation (ME) [7] is a usually used process in 
video compression that yields the motion vector (MV) to 
indicate the motion displacement for each macroblock. We 
analyze the distribution of MV on 2D video source, as 
shown in Fig. 2 The MV dissimilarity of background and 
active object provides the possibility to differentiate both. In 
real 2D videos, the front objects cover the rear objects. 
 
 
(a)                                       (b) 
Figure 2. (a) X vector image, (b) Y vector image 
 
2) Object Compensation: 
Since the object found by moving vector was physically 
incomplete, it still had to be patched. There are three ways 
in patching an object: color patch, contrast ratio patch and 
morphological patch. 
We can utilize the overall conformity among colors of an 
object to patch the color of the object. If an object was with 
a clear foreground but a blurred background, high-pass filter 
and morphological close process could be applied to find the 
pattern to an object, which is the so called contrast ratio 
patch. When the aforementioned methods were combined to 
patch an object, there may still be voids and splits on the 
object. But if the object was patched with an additional close 
process from morphological patch, one could achieve a 
precise detection of the object. 
 
B. Background Determination Algorithm 
After the evaluation and statistical analysis of common 
video frames, their backgrounds were categorized into 
atmospheric perspective, horizontal perspective and linear 
perspective according to human visual property. Among 
these three categorizations, the distinction between 
foreground and background was most obvious. Accordingly, 
the original frame was analyzed and processed through these 
three conditions, and we can find depth information to this 
frame so as to build depth image of the background. 
 
1) Atmospheric Perspective: 
We began with applying Laplacian filter to extract high 
frequency part of a frame before adjusting and analyzing the 
extracted part through morphological patch to see whether it 
complies with atmospheric perspective (with the lower part 
of the frame as clearer while the upper part blurs ). Finally 
we designed the depth image through gradation, as shown in 
Fig. 3 (a) and (b). 
 
2) Horizon: 
Bellow the horizontal line of the frame: the lower part is 
closer while the upper part is far apart from the observer, 
and the depth image was constructed accordingly. The 
production of this kind of depth image started with detecting 
the whole frame with horizontal line: if the bright dot on y 
axis was larger than the width of the image to a certain 
degree, the coordination of y axis which matches up with the 
horizontal line would be recorded, without the object being 
involved in this process of distinction, as shown in Fig. 3 (c) 
and (d).  
 
3) Linear Perspective: 
Using Hough Transform to detect binary images with 
lines, then distinguished high frequency lines through 
bubble sort, which became the so called “vanishing line”, or 
VL. Fig. 4 (a) and (b) are vanishing lines which were 
produced under different circumstances. 
After the vanishing line was distinguished, we searched 
once again for the place where VL assembled, which was 
the farthest point, or the vanishing point (VP), of the frame. 
According to the number of vanishing points, different 
procedures should be adapted: If VP equals 1, the point 
undergoes nonlinear gradational diffusion; if VP is larger 
than 1, the average from each coordination are to be the new 
VP, which then undergoes nonlinear diffusion, as shown in 
Fig. 4 (c) and (d). 
 
 
    
(a)                      (b)                      (c)                        
(d) 
Figure 3. (a) Before analyzing atmospheric perspective, (b) The 
produced depth image, (c) Detecting the horizontal line, (d) The 
produced depth image 
 
 
    
(a)                      (b)                      (c)                        
(d) 
Figure 4. (a) VL I, (b) VL II, (c) VP >1, (d) VP =1 
 
 
C. Determine Object Depth on Background 
After the object and background were processed, we 
have to ensure the object of a proper depth value, which was 
achieved through finding relative position of the object in 
 27 
through naked-eye monitors or 3D Red-Cyan glasses. Since 
naked-eye 3D image is effective only when being 
demonstrated on naked-eye monitor, we presented our 
research results in the form of Red-Cyan image instead, as 
shown in Fig. 10 and Fig. 11. Meanwhile, we invited 10 
audiences to evaluate the effect of depth video and 
Red-Cyan video (with the highest mark as 100), as shown in 
Fig. 12. The average mark was 83% versus 86% concerning 
each of their recognition rate. When in Full HD (1920x1088 
pixel), the parallel technique of CUDA can reach 32.52 fps, 
which is 48.73 times faster than that of traditional platform , 
as presented in Table 1. 
 
 
Figure 10. 2D-to-3D physical system 
 
 
 
(a) Soccer 
 
(b) Tangled 
 
(c) Wall-E 
 
(d) Seabirds 
 
(e) Toy Story 
 
Figure 11. Image from left to right are original image, depth image 
and Red-Cyan image. 
 
0
20
40
60
80
100
Depth quality
Visual comfort
TangledSoccer SeabirdsWall-E Toy Story
 
Figure 12. Evaluation results 
TABLE 1. PERFORMANCE COMPARISON 
 
Video name 
x86 
(frame/sec) 
CUDA 
(frame/sec) 
Speed Ratio 
(CUDA/x86) 
Soccer 0.67 26.51 39.57 
Tangled 0.66 34.38 52.09 
Wall-E 0.64 33.79 52.80 
Seabirds 0.66 33.30 50.45 
Toy Story 0.71 34.61 48.75 
Total Avg. 0.67 32.52 48.73 
V、 CONCLUSION 
According to the findings of our research, the 2D-to-3D 
video algorithm combined the extracted objects with depth 
information from the backgrounds, and constructed 3D 
images through complete depth images and their original 2D 
images, which were then placed on a 3D monitor and 
achieved the effect of real-time play. After a series of 
examination and verification with various forms of short 
video clips, we would like to move on to real-time 
processing of longer videos, and finally integrating our 
research results into monitors and players so as to normalize 
stereo technique. 
ACKNOWLEDGMENT 
The authors are grateful to the National Science Council, 
Taiwan for the sponsorship under grant: 
NSC99-2220-E-027-002. 
REFERENCES 
[1] G. Ge, N. Zhang, L. Huo, and W. Gao,“2D to 3D conversion 
based on edge defocus and segmentation," IEEE 
International Conference on Acoustics, Speech and Signal 
Processing, pp. 2181-2184, 2008. 
[2] C.-C. Cheng, C.-T. Li, P.-S. Huang, T.-K. Lin, Y.-M. Tsai, and 
L.-G. Chen,” A Novel 2D-to-3D Conversion System Using 
Edge Information,” in Proc. IEEE Intl. Conf. Consumer 
Electronics (ICCE), vol. 56, Issue 3, pp. 1739-1745, 2010. 
[3] Yu-Lin Chang, Chih-Ying Fang, Li-Fu Ding, Shao-Yi Chen, 
and Liang-Gee Chen,“ Depth Map Generation for 2D-to-3D 
Conversion by Short-Term Motion Assisted Color 
Segmentation,” in IEEE International Conference on 
Multimedia and Expo 2007 (ICME 2007), pp.1958-1961, 
2007. 
[4] Y.-L. Chang, C.-Y. Fang, L.-F. Ding, S.-Y. Chen, and L.-G. 
Chen,” Depth map generation for 2D-to-3D conversion by 
short-term motion assisted color segmentation,” in Proc. of 
ICME, pp. 1958-1961, 2007. 
[5] Y.-M. Tsai, Y.-L. Chang, and L.-G. Chen,“ Block-based 
vanishing line and vanishing point detection for 3d scene 
reconstruction,” in Proc. Intl. Symp. Intelligent Signal Proc. 
and Comm. Syst., pp. 586-589, 2006. 
[6] NVIDIA Corporation, “NVIDIA CUDA Compute Unified 
Device Architecture Programming Guide”, Version 2.3, 2009. 
[7] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, 
“Overview of the H.264/AVC video coding standard,” IEEE 
Trans. Circuits Syst.Video Technol., vol. 13, no. 7, pp. 
560–576, Jul. 2003. 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：蘇慶龍 計畫編號：99-2221-E-224-076- 
計畫名稱：3-D MOD：網路立體視訊播放系統--子計畫一：多視角立體視訊編碼方法最佳化設計(I) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 2 0 100%  專利 已獲得件數 0 0 100% 件  
件數 1 0 100% 件  
技術移轉 
權利金 1 0 100% 千元  
碩士生 4 3 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 2 0 100%  專利 已獲得件數 1 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
邏輯設計 優等 吳國暄、房煥偉  
6. 2010 教育部主辦 98 學年度大學校院積體電路(IC)設計競賽 大學組可程式
邏輯設計 佳作 蘇緯翔、林耘生  
7. 2010 參加 98 學年度 教育部主辦全國大學校院積體電路(IC)設計競賽得獎
對伍獲得全國前十強實驗室 (主辦單位統計發佈消息)  
8. 2010 98 學年度大學校院嵌入式系統設計競賽 嵌入式系統軟體組 設計完成
獎 黃瑞竣、朱諺汝、華智豪 (比賽日期：2010/05/22 比賽地點：成功大學) 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
