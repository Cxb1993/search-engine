實際上有著視差 (disparity) 的存在，視差的大小
變化讓觀測者感覺到物體與背景的深度變化。3D
視覺系統的呈現，便是基於這個概念。雙眼立體視
訊 (Binocular Stereoscopic Video) 呈現系統中需
要使用到兩組視訊資料，一組視訊資料為左眼使
用，另一組視訊資料為右眼使用，這兩組資料皆為
一般三維 (二維空間加上第三維時間) 的視訊資
料，可藉由兩個不同的攝影機來擷取。當人們的雙
眼分別接收到相對應的視訊資料，大腦便會將這兩
組有著些微差異的視訊結合，使得人們感覺到場景
的深度。 
立體視訊相較於傳統視訊需要雙倍的資料量
和頻寬，壓縮的編碼複雜度亦倍增。因此過去的研
究者也企思如何以較佳的壓縮效率來編碼立體視
訊。演變至今的立體視訊壓縮架構，都是採用混合
視差預測與運動預測  (Disparity Compensation 
Prediction and Motion Compensation Prediction) 的
聯合預測立體視訊壓縮編碼系統 (Joint Prediction 
Stereoscopic Video Coding，參考圖 1)。 
 
應用最新的 H.264 視訊標準來開發立體視訊
壓縮系統，更是目前的主流。視差資訊的輔助雖可
以增進壓縮效率，但也帶來了更多計算上的負擔，
對 H.264 這種最新視訊標準而言，存在許多繁雜
的最佳化編碼程序，對雙眼立體視訊而言尤其更行
明顯。 
H.264 視訊標準的編碼中，可變區塊的尺寸 
(variable block size)與多張參考影像  (multiple 
reference) 的選擇，並使用  RDO (rate-distortion 
optimization) 技術，來選擇最佳的區塊預測的模式 
(mode) 與運動向量，此最佳化程序稱作模式選擇 
(mode decision，參考圖 2)。H.264 立體視訊壓縮系
統由於需要針對每一模式進行 RDO 檢驗，加上對
多張影像的運動估測與視差估測搜尋，計算量更是
倍增。 
本計劃的立體視訊編碼系統，主要是利用基於
影像特徵  (feature-based) 的類神經網路 (Back- 
Propagation Neural Network ， BPNN) 分 類 器 
(classifier) 在 off-line 的訓練，應用至 on-line 的編
碼模式選擇加速。我們為立體視訊編碼系統設計的
類神經網路分類器是兩級式設計，第一級類神經網
路用在輔助區塊分割方式的選擇 (16x16 Direct、
16x16、16x8、8x16、8x8、Intra)，接著第二級類
神經網路則是輔助區塊預測之參考來源選擇 (時
間軸上的運動預測；或是不同視域方向的視差預
測)。 
 
三、研究方法  
3.1 
3.2 
系統架構 
有關本計劃的立體視訊編碼系統架構圖，可以
參考下圖 3。 
本計畫係以 H.264-based 的 JMVM 平台來開
發立體視訊編碼/解碼系統，對於每一個階層式 B 
frame，BPNN 演算法分為兩個步驟：off-line 的訓
練、on-line 的分類。步驟過程分別如下： 
1. off-line 訓練： 
依據所計算之影像特徵作訓練，類神經網路的
收斂到穩定狀態後，可以得到神經元間鍵結的權重
值。 
 
 
2. on-line 執行： 
當壓縮器在編碼時，將 off-line 訓練得到的權重
值應用在兩級式 BPNN 順向處理  (forward 
processing) 分類器。第一級的作用為區塊分割方
式選擇，第二級的作用則為區塊預測之參考來源選
擇。 
 
影像特徵擷取 
L e ft 
v ie w
R ig h t 
v ie w
‧ ‧ ‧
‧ ‧ ‧
I B B P
I B B P
 
圖 1 立體視訊壓縮編碼系統示意圖 
8x16 8x8Direct / skip 16x16 16x8
4x8 4x48x8
Direct / skip
8x8 8x4
圖 2  H.264 的模式選擇流程 
圖 3  本計畫立體視訊編碼系統架構圖 
視差估測前，預先選出較可能的預測來源，去除較
不可能者，而達到加速的目的。第一級類神經網路
所選出的 K1種候選分割方式，除了 Direct 及 Intra
之外，都會繼續進入第二級類神經網路為進行區塊
預測來源的分類，以減少運動或視差估測的運算花
費。 
第二級類神經網路輸出的類別，依據右眼視訊
B frame 進行估測時的參考來源，共有三種：
Forward motion (F)、Backward motion (B)、Disparity 
(D)。 
在這一級的類神經順向分類，我們對參考來源
的方式設計了兩種方法： 
1. 3 取 K2： 
取類神經網路輸出值最高的前 K2個， N = 2
是比較好的選擇。 
 
2. 條件判斷式選擇： 
觀察第二級輸出的分數類神經網路輸出值 
(F , B , D) 分布，設計一個 threshold-based 的條
件。我們給 F、B、D 各一個 threshold：α、β、γ，
以及差值的 threshold：δ1、δ2。第二級條件判斷如
圖 4 的流程圖所示。 
 
 
圖 4  第二級類神經網路的條件判斷流程 
四、實驗結果 
本計劃所提出利用兩級類神經網路輔助
H.264 立體視訊編碼模式選擇加速的效能，類神經
網路分類之主要作用於區塊分割方式選擇及子區
塊預測來源選擇。我們分別單用第一級，以及第一
級加上第二級來測試效能，而效能評估則是觀察
PSNR、Bit-rate 以及時間加速相對於全模式選擇比
較其得到的編碼效益。 
4.1 立體視訊編碼器實驗條件 
本 計 劃 使 用 的 測 試 影 像 序 列 分 別 為 
ballroom，race，exit，vassar，puppy，soccer，train 
and tunnel 六組，如圖 5 所示。Soccer 與 race 都
是屬於高動量的影像序列，而且動的區域不只於人
物主體的移動，還包括攝影機的移動，而 puppy
與 vassar 則是屬於較靜態的影像，其餘的算是固定
camera 拍攝情況下，中運動量與高運動量的影
像。其餘相關的編碼參數如表 2 所示。 
 
圖 5 測試影像序列 
表 2 編碼器相關編碼參數 
 
 
  
puppy soccer 
  
exit ballroom 
  
race vassar 
 
train and tunnel 
視訊格式 4:2:0 
視訊畫面大小 640x480，720x480，720x288
畫面率 30 frames/sec，25 frames/sec 
視訊長度 37 frames 
GOP Size 12 frames 
B 參考畫面數 3 frames 
Disparity vector 搜尋範圍 X∈ [-48,48] ,Y∈ [-2,2] 
Motion vector 搜尋範圍 ±16 pixels 
RDO On 
FME (Fast Motion Estimation) Off 
Deblocking filter On 
1/4 Sub-pixel search On 
Quantization Parameters 28 
puppy 39.279 
(-0.04) 
671.312 
(+1.16 %) 83.96 
soccer 39.814 
(-0.16) 
1217.170 
(+7.54%) 85.22 
網
路 
train and 
tunnel 
37.11 
(-0.12) 
1525.189 
(+16.83 
%) 
84.22 
ballroom 38.31 
(-0.02) 
1178.079 
(+2.57 %) 77.90 
exit 40.15 
(-0.04) 
504.838 
(+4.07 %) 77.51 
puppy 39.31 
(-0.01) 
663.730 
(+0.01 %) 77.24 
soccer 39.92 
(-0.06) 
1155.853 
(+2.13 %) 77.60 
第
一
級 
類
神
經 
網
路 
train and 
tunnel 
37.15 
(-0.066) 
1376.903 
(+5.47 %) 
77.59 
 
表 5 兩級式 (第二級分類器為條件判斷流程) 類
神經網路輔助模式選擇加速與全模式選
擇、單用第一級類神經網路輔助模式選擇加
速的編碼效益比較表 
 Sequence 
PSNR 
(dB) 
Bit-rate 
(kbps) 
Avg. B 
(sec) 
ballroom 38.33  1148.568 13.210 
puppy 39.32  663.645 14.150 
soccer 39.97  1131.795 14.820 
exit 40.19  485.103 12.760 
全
模
式 
選
擇 train and 
tunnel 
37.22  1305.514 9.060  
Sequence 
PSNR 
(dB) 
Bit-rate 
(kbps) 
Speed up 
% 
ballroom 38.24 
(-0.09) 
1225.416 
(+6.69%) 
81.45 
exit 40.11 
(-0.08) 
509.092 
(+4.95%) 82.68 
puppy 39.27 
(-0.04) 
664.541 
(+0.13 %) 82.26 
soccer 39.842 
(-0.16) 
1195.284 
(+5.61%) 83.67 
兩
級
式 
類
神
經 
網
路 
train and 
tunnel 
37.142 
(-0.08) 
1387.070 
(+6.25 %) 
80.13 
ballroom 38.31 
(-0.02) 
1178.079 
(+2.57 %) 77.90 
exit 40.15 
(-0.04) 
504.838 
(+4.07 %) 77.51 
puppy 39.31 
(-0.01) 
663.730 
(+0.01 %) 77.24 
soccer 39.92 
(-0.06) 
1155.853 
(+2.13 %) 77.60 
第
一
級 
類
神
經 
網
路 
train and 
tunnel 
37.15 
(-0.066) 
1376.903 
(+5.47 %) 
77.59 
 
五、結論 
本年度計劃利用兩級式類神經網路來輔助立
體視訊 H.264 編碼時的模式選擇加速。第一級用來
決定區塊分割方式，第二級則決定估測的參考來
源。 
在訓練過程中，如果能適度調整訓練樣本群的
的加權，也許能有效改善分類的正確性，進而增進
編碼效益。如果第一級的分類能更準確，亦表示其
用來作為估測的這些來源目標能對第二級所取的
影像特徵更有相關性，更進一步改善第二級分類的
辨識率。在兩級式類神經網路應用的結果中，也許
可以再針對第二級的影像特徵進行改善。 
以條件判斷為基礎的第二級加上第一級的結
果，整體編碼時間效益約可以加速 80% ~ 84%，而
Bit-rate 增加，中高運動量的測試影像大約增加 4% 
~ 6%的 Bit-rate。實驗結果顯示類神經網路經由訓
練學習而進一步推廣的優點，不論在第一級分類器
還是兩級式分類器同時使用，影像特徵對於我們預
測區塊的分割、估測來源的選擇，類神經網路的確
發揮其特點幫助我們在編碼過程的模式選擇加速。 
如何選擇適合編碼區塊分割的影像特徵，以利
用類神經網路學習特點的推廣，幫助我們在編碼的
時候省去不必要的模式評估，也許還能有更好的影
像特徵能幫助在本論文所提的類神經網路系統輔
助的效能提升。如果將我們提的類神經輔助系統應
用在未來立體視訊壓縮系統中，只需要少部分的記
憶空間，儲存經有效訓練後的神經元鍵結的權重
值，便能有效的達到加速的效果，且 Bit-rate 也不
會增加過高，是提供了未來立體視訊壓縮系統也許
能採用的一種選擇。也提供未來若有人需要應用類
神經網路來輔助壓縮系統的話，一些實驗上的經驗
與小技巧。 
 
六、參考文獻 
 
[1] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini 
and G.J. Sullivan, “Rate-constrained coder 
control and comparison of video coding 
standards,” IEEE Trans. on Circuits and 
Systems for Video Technology, Vol. 13 , no. 7, 
pp. 688~703, July 2003. 
[2] ThomasWiegand, Gray J. Sullivan, Gisle 
Bjontegaard, and Ajay Luthra, “Overview of 
the H.264/AVC video coding standard,” IEEE 
Trans. on Circuits and Systems for Video 
Technology, Vol. 13, no. 7, pp.560~576, July 
2003. 
[3] Richard O. Duda, Peter E. Hart, David G. Stork, 
“Pattern Classification,” Second Edition, 
WILEY-INTERSCIENCE, 2000. 
[4] J. Reichel, H. Schwarz, and M. Wien. Joint 
Scalable Video Model JSVM-6. 2006. 
