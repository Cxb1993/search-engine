 2
 行政院國家科學委員會專題研究計畫成果報告 
題目：非參數化與半參數化 Wilcoxon 學習機之研究(I) 
Research on nonparametric and semiparametric Wilcoxon learning 
machines 
計畫編號：NSC 96-2221-E-110-093 
執行期限：96 年 8 月 1 日至 97 年 7 月 31 日 
主持人：謝哲光   國立中山大學電機工程學系 
計畫參與人員：吳旭焜、吳宗翰、鄭文欽、廖時慧 
 
中文摘要 
 
摘要-在傳統機器學習領域中，模糊神經網路長久以
來就被認為是一個有效率且功能強大的學習機。近來，為
了提高抑制離群值之韌性，學者提出了 Wilcoxon 模糊神經
網路，它是將線性參數化回歸問題中 rank-based Wilcoxon
的方法推廣至非參數化神經網路。一般而言，模糊神經網
路和 Wilcoxon 模糊神經網路是非參數化模型，它們在回歸
函數的形式上是不受限制的。然而，它們不但很難去解釋，
更糟的是，當我們的預測模型中自變數維度很大的時候，
我們將付出高額的計算代價。在統計回歸理論中，過去的
學者已經提出了半參數化模型去克服這個缺點。在半參數
化模型中，它的參數化部份會保有容易解釋的部份，而它
的非參數化部分也保有彈性。基於這個理由，在本研究中
我們將提出半參數化模糊神經網路和半參數化 Wilcoxon
模糊神經網路。我們所使用的學習法則是 backfitting 的方
法，而這方法經常被使用在半參數化回歸問題中。從模擬
結果中可以看出半參數化模型表現得比非參數化的模型來
得好。 
 
關鍵字：模糊神經網路(FNN)，Wilcoxon 模糊神經網
路(WFNN)，非參數化模型，半參數化模型。 
 
英文摘要 
 
Abstract—Fuzzy neural network (FNN) has long been 
recognized as an efficient and powerful learning machine for 
general machine learning problems.  Recently, Wilcoxon 
fuzzy neural network (WFNN), which generalizes the 
rank-based Wilcoxon approach for linear parametric 
regression problems to nonparametric neural network, was 
proposed aiming at improving robustness against outliers.  
FNN and WFNN are nonparametric models in the sense that 
they put no restrictions, except possibly smoothness, on the 
functional form of the regression function.  However, they 
may be difficult to interpret and, even worse, yield poor 
estimates with high computational cost when the number of 
predictor variables is large.  To overcome this drawback, 
semiparametric models have been proposed in statistical 
regression theory.  A semiparametric model keeps the easy 
interpretability of its parametric part and retains the flexibility 
of its nonparametric part.  Based on this, semiparametric 
FNN and semiparametric WFNN will be proposed in this 
study.  The learning rules are based on the backfitting 
procedure frequently used in semiparametric regression.  
Simulation results show that the semiparametric models 
perform better than their nonparametric counterparts. 
 
Keywords—Fuzzy neural network (FNN), Wilcoxon 
fuzzy neural network (WFNN), nonparametric models, 
semiparametric models.. 
 
一、前言 
 
 The field of machine learning is currently an active 
research area.  The goal of machine learning is to find a 
general rule that explains experimental data given only a 
sample of limited size.  Popular learning machines 
developed in the past include artificial neural networks [1], 
[2], radial basis function networks [3], [4], fuzzy neural 
networks (FNNs) (or neuro-fuzzy networks) [5], [6], and 
support vector machines [7], [8].  They are different in their 
origins, network configurations, and objective functions.  In 
statistical terms, the aforementioned learning machines are 
nonparametric in the sense that they do not make any 
assumptions on the functional form of the discriminant or 
predictive functions, e.g., linearity.  Apart from their 
fascinating origins, we are particularly interested in fuzzy 
neural networks, namely fuzzy systems treated as neural 
networks, because their machine parameters usually have 
clear physical meanings. 
 
 Wilcoxon fuzzy neural network was recently proposed 
aiming at improving robustness against outliers [9].  As is 
well known in statistics, the resulting regressors by using the 
rank-based Wilcoxon approach to linear parametric regression 
problems are usually robust against (or insensitive to) outliers 
  4
 ∑
=
=
m
j
jjkk rws
1
, ∑
=
=
m
j
jrg
1
,  (2b) 
then 
 gsy kk = . (2c) 
The fuzzy system (1) or (2) can be represented as a 
feedforward network, namely, the fuzzy neural network (FNN) 
[17], [18].   
 
 Let nX ℜ⊆  and pY ℜ⊆ .  Suppose we are given 
the training data set 
 ( ){ } YXdS l
qqq
×⊆= =1,: x . (3) 
In the following, we will use the subscript q to denote the qth 
example.  For instance, qix  denotes the ith component of 
the qth pattern nq ℜ∈x , lq∈ , ni∈ .   
The aim is to find optimal weights jkw , ijc , and ijv  
that minimize the total sum of squared errors  
 ( )∑∑
= =
−=
p
k
l
q
qkqktotal ydE
1 1
2
2
1: . (4) 
Appropriate updating rules, e.g., back propagation (BP) rules, 
can be used to approximate the optimal weights. 
 
II. NONPARAMETRIC WFNN  
 
 In this section, we will briefly review the WFNN [9].  
The WFNN used here is the same as the standard FNN, except 
the p bias terms at each output node.  The predictive function 
of WFNN is a nonlinear map h given by  
 ( ) kkkk bthy +== x , pk ∈ , (5) 
where ( ) ( )xx kk ft =  and ( )xkf  is given by (1).  From (2), 
we have  
 ggst Tkkk rw== , (6) 
where 
 [ ] mTmkkk ww ℜ∈= ...: 1w , [ ] mTmrr ℜ∈= ...: 1r . 
 
 Given the training data set (3), the approach in a WFNN 
is to choose network weights that minimize the Wilcoxon 
norm of the total residuals 
 ∑
=
Ψ=Ψ
p
k
ktotal
1
, (7a) 
 ( )( ) ( ) ( )∑∑
==
==Ψ
l
q
kq
l
q
qkqkk qaRa
11
ρρρ ,  (7b) 
 qkqkqk td −=:ρ , lq∈ , pk ∈ . (7c) 
Here, ( )qkR ρ  denotes the rank of the residual qkρ  among 
k1ρ , …, lkρ , ( ) ( )klk ρρ ≤≤K1  are the ordered values of 
k1ρ , …, lkρ , and ( ) ( )( )1: += liia ϕ  is a score function with 
( ) ( )5.012: −= uuϕ .  The incremental gradient descent 
algorithm requires that kΨ s be minimized in sequence.  
Using (2), (5), (6), and (7), the updating rules for jkw , ijc , 
and ijv  are given by 
 
jk
k
jkjk w
ww ∂
Ψ∂−← η  
 ( )( )∑
=
⋅+=
l
q q
qj
qkjk g
r
Raw
1
ρη , mj∈ , (8a) 
 
ij
k
ijij c
cc ∂
Ψ∂−← η  
    ( )( ) ( ) ( )∑
=
−−⋅+=
l
q ij
ijqi
qkjk
q
qj
qkij v
cx
tw
g
r
Rac
1
2ρη , (8b) 
 
ij
k
ijij v
vv ∂
Ψ∂−← η  
    ( )( ) ( ) ( )∑
=
−−⋅+=
l
q ij
ijqi
qkjk
q
qj
qkij v
cx
tw
g
r
Rav
1
2
2
ρη , (8c) 
  mj∈ , ni∈ , 
where 0>η  is the learning rate [9].  The bias term kb , 
pk ∈ , is given by the median of the residuals at the kth 
output node, i.e., 
 { }qkqklqk tdmedb −= ≤≤1 . (9) 
 
III. SEMIPARAMETRIC FNN AND WFNN 
 
 Consider the following semiparametric additive model 
 ( ) ( ) ε++= xu hgy , mℜ∈u , nℜ∈x , ℜ∈y , (10) 
where ( )ug  is a parametric regressor, ( )xh  is the 
nonparametric component of the predictive function, and ε  
is the error term.  The semiparametric additive model (10) is 
shown schematically in Fig. 2.  In the following, we will 
introduce the popular backfitting procedure frequently used in 
semiparametric regression problems [19], [20]. 
 
 Suppose we are given the training data set 
 ( ){ } ℜ×ℜ×ℜ⊆= = nmlqqqq dS 1,, xu . (11) 
Let the partial residuals be defined by 
 ( )qqq hdy x−=ˆ , ( )qqq gdy u−=~ , lq∈ . (12) 
In the backfitting procedure, the training data set for the 
parametric regressor is given by 
 ( ){ } ℜ×ℜ⊆= = mlqqqg yS 1ˆ,u , 
whilst that of the nonparametric regressor is given by 
 ( ){ } ℜ×ℜ⊆= = nlqqqh yS 1~,x . 
 
 First, we propose FNN-based semiparametric models.  
For simplicity of illustration, we consider a special 
semiparametric FNN, which is called a partially linear model 
in semiparametric regression theory, where the parametric 
component ( )ug  is a linear regressor of the form 
 ( ) uβu Tg += 0β , [ ] mTm ℜ∈= ββ ...: 1β , ℜ∈0β , (13) 
and the nonparametric part ( )xh  is an FNN (1) with single 
output ( 1=p ). 
 
  6
(MAD), and the mean absolute percentage error (MAPE).  
We then average those testing error rates to give estimated 
testing error rates.  The simulation results are shown in Table 
I, where we use 20 hidden nodes for all learning machines.  
It is observed that semiparametric models outperform the 
nonparametric models. 
 
 Suppose we use the final fitted models corresponding to 
the first run.  Partial residual plots with vertical alignment 
[12] of the partially linear semiparametric WFNN fit for 
dewpoint data are shown in Fig. 4.  As can be observed, the 
scatter about the curves does not show any systematic patterns, 
so we have a positive diagnosis of model adequacy for the 
partially linear semiparametric model. 
 
 
五、結果與討論 
 In this study, we have proposed the semiparametric 
fuzzy neural network and semiparametric Wilcoxon fuzzy 
neural network models to solve nonlinear learning problems.  
These models combine components of parametric and 
nonparametric models, keeping the easy interpretability of the 
former and retaining some of the flexibility of the latter.  The 
learning rules are based on the backfitting procedure 
frequently used in semiparametric regression.  A numerical 
example has been provided to illustrate the use of 
semiparametric FNN and WFNN.  Simulation results have 
shown that the semiparametric models perform better than 
their nonparametric counterparts.  It is straightforward to 
extend our approach to other semiparametric models such as 
additive models, generalized additive models, generalized 
partially linear models, additive partially linear models, and 
generalized partially linear partially additive models. 
 
六、參考文獻 
[1] K. Hornik, M. Stinchcombe, and H. White, “Multilayer 
feedforward networks are universal approximators,” 
Neural Netw., vol. 2, no. 5, pp. 359-366, 1989. 
[2] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, 
“Learning internal representations by error 
propagation,” in Parallel Distributed Processing: 
Explorations in the Microstructure of Cognition, D. E. 
Rumelhart and J. L. McClelland, Eds. Cambridge, MA: 
MIT Press, 1986, vol. 1, Foundations, pp. 318-362. 
[3] E. J. Hartman, J. D. Keeler, and J. M. Kowalski, 
“Layered neural networks with Gaussian hidden units as 
universal approximations,” Neural Comput., vol. 2, no. 
2, pp. 210-215, 1990. 
[4] J. Park and I. W. Sandberg, “Universal approximation 
using radial basis function networks,” Neural Comput., 
vol. 3, no. 2, pp. 246-257, 1991. 
[5] B. Kosko, “Fuzzy systems as universal approximators,” 
IEEE Trans. Computers, vol. 43, no. 11, pp. 1329-1333, 
Nov. 1994. 
[6] L. X. Wang and J. M. Mendel, “Fuzzy basis functions, 
universal approximation, and orthogonal least squares 
learning,” IEEE Trans. Neural Netw., vol. 3, no. 5, pp. 
807-814, Sep. 1992. 
[7] B. E. Boser, I. M. Guyon, and V. N. Vapnik, “A training 
algorithm for optimal margin classifiers,” in Proc. 5th 
Annu. ACM Workshop on Comput. Learn. Theory, D. 
Haussler, Ed., Pittsburgh, PA, 1992, pp. 144-152. 
[8] C. Cortes and V. N. Vapnik, “Support vector networks,” 
Mach. Learn., vol. 20, pp. 273-297, 1995. 
[9] J. G. Hsieh, Y. L. Lin, and J. H. Jeng, “Preliminary study 
on Wilcoxon learning machines,” IEEE Trans. Neural 
Netw., vol. 19, no. 2, pp. 201-211, Feb. 2008. 
[10] R. V. Hogg, J. W. McKean, and A. T. Craig, Introduction 
to Mathematical Statistics, 6th ed. Englewood Cliffs, NJ: 
Prentice-Hall, 2005. 
[11] W. Härdle, M. Müller, S. Sperlich, and A. Werwatz, 
Nonparametric and Semiparametric Models.  Germany, 
Berlin: Springer, 2004. 
[12] D. Ruppert, M. P. Wand, and R. J. Carroll, 
Semiparametric Regression.  NY: Cambridge 
University Press, 2003. 
[13] W. L. Hung and M. S. Yang, “An omission approach for 
detecting outliers in fuzzy regression models,” Fuzzy 
Sets Syst., vol. 157, no. 23, pp. 3109-3122, 2006. 
[14] H. H. Tsai and P. T. Yu, “On the optimal design of fuzzy 
neural networks with robust learning for function 
approximation,” IEEE Trans. Syst., Man, Cybern. B, 
Cybern., vol. 30, no. 1, pp. 217-223, Feb. 2000. 
[15] C. C. Chuang, S. F. Su, and S. S. Chen, “Robust TSK 
fuzzy modeling for function approximation with 
outliers,” IEEE Trans. Fuzzy Syst., vol. 9, no. 6, pp. 
810-821, Dec. 2001. 
[16] W. Y. Wang, T. T. Lee, C. L. Liu, and C. H. Wang, 
“Function approximation using fuzzy neural networks 
with robust learning algorithm,” IEEE Trans.  Syst., 
Man, Cybern. B, Cybern., vol. 27, no. 4, pp. 740-747, 
Aug. 1997. 
[17] L. X. Wang, A Course in Fuzzy Systems and Control. 
Englewood Cliffs, NJ: Prentice-Hall, 1997. 
[18] V. Kecman, Learning and Soft Computing.  Cambridge, 
MA: MIT Press, 2001. 
[19] L. Breiman and J. H. Friedman, “Estimating optimal 
transformations for multiple regression and 
correlations,” J. American Stat. Assoc., vol. 80, no. 391, 
pp. 580-619, 1985. 
[20] A. Buja, T. J. Hastie, and R. J. Tibshirani, “Linear 
smoothers and additive models,” Annals of Stat., vol. 17, 
pp. 453-555, 1989. 
[21] J. Maindonald and J. Braun, Data Analysis and 
Graphics Using R, 2nd ed.  NY: Cambridge University 
Press, 2007. 
 
七、計畫成果自評 
本研究計畫內容大致上與原計畫相符且已達到原計
畫內容的目標。主要完成半參數化 Wilcoxon 學習機相關
問題之研究及可行性之探討。機器學習理論一直以來都是
計算智慧與統計學兩大領域的巧妙結合。本研究計畫不但
  8
maxtemp
10 15 20 25
20
25
30
35
40
10
15
20
25
mintemp
20 25 30 35 40 5 10 15 20 25
5
10
15
20
25
dewpt
 
Fig. 3.  Scatterplot matrix for the dewpoint data. 
 
 
10 15 20 25
0
5
10
15
20
25
30
mintemp
co
nt
rib
ut
io
n 
to
 d
ew
pt
20 25 30 35 40
0
5
10
15
20
25
30
maxtemp
co
nt
rib
ut
io
n 
to
 d
ew
pt
 
Fig. 4.  Partial residual plots of the partially linear semiparametric WFNN fit for 
dewpoint data. 
 
 
