word’s position. Because it is very likely that more than one path 
includes the same word in the same position, we need to 
aggregate the probabilities for a given word at a given position 
over all possible paths in a lattice [2]. In this way, many possible 
hypotheses in the ASR output for a spoken segment, obtained in 
the form of a lattice, can be expressed in a structure efficient for 
retrieval [2]. An example word-based PSPL (W-PSPL) is in Fig. 
1, which can be further extended to Subword-based PSPL 
(S-PSPL) for better indexing of OOV words or rare words. 
S-PSPL is very similar to W-PSPL, except all words wi in Fig. 1 
is replaced by subword units and so on [8]. 
 
Lattice: 
 
    All paths: 
w1w2, w3w4w5, w6w8w9w10, w7w8w9w10 
(b) 
PSPL structure: 
w1: P(w1, 1|Lj)   w2: P(w2, 2|Lj)   w5: P(w5, 3|Lj)   w10: P(w10, 4|Lj) 
w3: P(w3, 1|Lj)   w4: P(w4, 2|Lj)   w9: P(w9, 3|Lj) 
w6: P(w6, 1|Lj)   w8: P(w8, 2|Lj) 
w7: P(w7, 1|Lj) 
cluster 1        cluster 2        cluster 3         cluster 4 
(c) 
 
Fig. 1. (a) An ASR lattice, (b) all paths in (a), (c) the constructed PSPL structure, 
where w1,w2 . . . are word hypotheses in the lattice. 
 
3. PROPOSED APPROACH 
The approach proposed in this paper is presented in this section. 
 
3.1. Overall System 
The overall system diagram for the proposed approach is shown 
in Fig. 2. In the upper middle of the figure, the PLSA model is 
constructed based on a large scale text corpus. Reliable 
probability distributions can thus be estimated. Next, in the 
upper left of the figure, PSPLs are constructed for all spoken 
segments in the spoken archive, from which the possible terms 
wi in each segment can be easily obtained. The core of the 
proposed approach, Latent Semantic Retrieval over PSPLs, is 
then in the lower part of the figure. We first use the probability 
distributions obtained from PLSA to derive a more precise 
semantic relevance measure between terms, as will be presented 
in Sec. 3.2. Meanwhile, topically representative or semantically 
important terms are also extracted as key terms using the 
probability distributions from the PLSA model, as will be 
presented in Sec. 3.3. Finally, the relevance score between the 
query Q and each spoken document dj can then be computed 
based on the semantic relevance measure R(wi, ql) between all 
possible terms wi in each spoken segment and all query terms ql 
in the query Q, the posterior probabilities P(wi|Lj) of all 
possible terms from W-PSPL/S-PSPL, and the key term weight 
λil, from which the relevant spoken segments are then retrieved. 
This will be discussed in Sec. 3.4. 
3.2. Semantic Relevance between Terms 
With reliable estimation of probabilities P(wi|zk) for all words 
wi and all topics zk from a well-trained PLSA model, by Bayes’ 
Theorem, the probability P(zk|wi) can be obtained as follows: 
 
 
 
In this way, for each term wi a latent topic vector can be 
constructed with dimension K, whose components are simply the 
K probabilities P(zk|wi) in Equ. (3) for the k latent topics {z1, 
z2, . . . , zK}. 
With the latent topic vectors constructed for all terms wi, 
the semantic relevance R(wi, wi’) between two terms wi and wi’ 
can be easily derived from the distance between their 
corresponding latent topic vectors or probability distributions. 
 
 
Fig. 2. Overall System Diagram for the Proposed Approach. 
 
In this research, a total of 5 different distance measures were 
used, including the cosine similarity, the correlation coefficient, 
Euclidean distance, Kullback-Leibler distance and 
Bhattacharyya distance. 
 
3.3. Key Term Extraction 
The purpose here is to de-emphasize semantically less 
meaningful terms, and select topically more clear and 
representative terms and weight them more in retrieval. In 
addition to such features as occurrence counts in training corpus, 
document frequencies and so on, we preliminarily use two 
entropy measures, the segment entropy and the latent topic 
entropy. The segment entropy Eseg(wi) for a term wi is defined 
as 
 
 
 
where cij is the count of term wi appearing in spoken document 
dj ,N is the total number of segments in the spoken document 
is the total count of term wi in the archive, and 
spoken document archive. Although this entropy measure is 
useful to identify the key terms, another entropy measure turns 
out to be even more useful, the latent topic entropy defined by 
 
(3)
(4)
