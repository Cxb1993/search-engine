 I 
行政院國家科學委員會專題研究計畫結案報告 
 
 
平行編譯器中不規則資料重新分佈之訊息產生與通訊最
佳化研究 
 
 
中文摘要 
本報告是有關於在分散式記憶體環境下對 GEN_BLOCK動態式資料重新配置技術
之描述。在這一個計畫中，我們研發了對 GEN_BLOCK資料重新配置的通訊排程演算
法，Two-Phase Degree-Reduction (TPDR)，以及對估算通訊成本做較佳化的演算法，Local 
Message Reduction(LMR)。TPDR是由兩個排程方法所組成的演算法：第一個是本計畫
所發展的排程演算法，優先處理需傳送最多訊息或接收最多訊息的處理器；第二個演
算法運用著色理論和修正著色排程機制，找出最後兩個步驟中，訊息適合擺放的位置，
以此協助完成第一個排程演算法未完成的排程步驟。LMR 針對網路環境中的傳輸速度
與本機傳送速度的差異，給予每一筆通訊較佳通訊成本，幫助排程演算法降低資料重
新配置的時間，是一個適用於任何通訊排程演算法的方法。在本計畫中，由於 TPDR
有效降低整體排程步驟的通訊時間並且避免處理器互相競爭傳送訊息的優先順序，且
LMR 提供比以往更能反應出真實傳輸時間的通訊成本理論模組，因此有助於提升平行
編譯器運算平行程式的整體效能。 
 
關鍵詞：GEN_BLOCK、動態式資料重新配置、分散式記憶體、排程演算法、通訊成本
理論模組、平行編譯器。 
 1 
一、緣由與目的 
在分散式記憶體環境裡，大部分的科學運算程式在執行的過程中，需要經常改變資
料配置方式，以便配合不同運算階段中對處理器的資料需求。在平行程式重新配置資料
的過程中，處理器需要經常移動大量的資料已達到平行程式的資料配置需求。然而花費
的通訊成本過高時會影響程式整體的執行效能，因此，動態的資料配置技術在分散式記
憶統環境中，是一個重要的研究方向。過去，由於分散式記憶體環境都是由相同能力的
電腦組成，因此，資料重新配置研究主要著重在規則的資料重新配置上。隨著網路的發
達，利用網際網路上的電腦進行平行運算的研究越來越受到重視，因此需要針對不同能
力的電腦組成的分散式記憶體環境開發不規則的資料重新配置通訊演算法，但目前仍然
少有針對不規則的資料重新配置型態所發展的演算法。在本計畫中，我們將在分散式記
憶體環境下，針對不規則的資料分配情形提出高效率的通訊排程演算法。主要方向為研
發最少排程步驟的排程方法，並尋找和可以利用著色理論縮短整體排程時間的排程方
式。接著，我們針對環境中不同的訊息傳輸速度，研發出通訊成本理論模組，給予不同
類型的訊息不同的通訊成本。在設計排程演算法的同時我們也將設計一套理論分析模組
和資料配置參數產生器，以便比較理論上和實際上的排程結果是否互相對應。 
二、研究方法與成果 
由於 GEN_BLOCK運算程式要求分配不同大小的連續資料區塊給各個處理器，不
容易有效率地執行訊息封裝和資料重新配置(Data Redistribution)。為了降低資料重新配
置所花費的通訊時間，發展有效率的資料排程演算法是必需的。 
資料重新配置時，處理器與資料通訊的關係是由兩個資料配置參數 (Data 
Distribution scheme)所形成。將這個關係表示成二分圖就如圖一所示，SP0到 SP3為二分
圖中的一個節點集合(Vertex Set)，代表資料未重新分配時的處理器集合；DP0 到 DP3
則是二分圖中的另一個節點集合，代表資料重新分配後的處理器集合；m1到 m7是二分
圖中的邊(Edge)，代表處理器之間傳送訊息的關係。 
本計畫提出的資料重新配置之排程演算法為 Two-Phase Degree Reduction 
(TPDR)，主要分成兩個部份。第一個部分先對需要傳送或接受最多通訊訊息的處理器
做通訊排程，如圖一中的 SP1和 DP2，也就是先處理二分圖中與擁有最大 Degree 的節
點(處理器)有關係的邊(通訊訊息)。 
 3 
因為最大 Degree 值為 2，排程演算法進入利用著色理論來排通訊訊息的第二部份。
將 m1、m3、m5和 m7 塗上紅色，m4 塗上藍色。紅色和藍色分別代表排程步驟 1和 2，
因此可得到排程結果，如圖三。 
在著色的過程中，假如顏色修正機制發現 m4放置在第一個排程步驟且 m3和 m5放
再第二個排程步驟後，會使得整體的傳送時間減少的話，則 m4應塗成紅色，m3和 m5
應塗成藍色，其排程結果便如圖四所示。 
 
Schedule Table 
Step 1 m1、m3、m5、m7 
Step 2 m4 
Step 3 m2、m6 
 
 
圖三、經由 TPDR排程演算法排程後所得到的結果。 
     
 
Schedule Table 
Step 1 m1、m4、m7 
Step 2 m3、m5 
Step 3 m2、m6 
 
 
圖四、顏色修正機制更換了 m4以及 m3和 m5在 Step 1 和 Step 2的位置。 
 
我們針對不同的資料配置情形和各個排程演算法進行分析，發現排程演算法是否
容易找到最佳或花費最少排程時間的排程結果與資料配置情形有關聯。當我們設定每個
處理器取得的資料為平均值的正負百分之三十時，也就是當平行程式要求資料分配的情
形為資料較平均地分配在各個處理器時，通訊排程演算法較不易找到接近最佳解的通訊
排程結果。然而當資料明顯地被集中在某個或是某些少數處理器時，通訊排程演算法大
部分都可以找到花費較少的排程結果。然而在比較這些結果後，本計畫中提出的排程演
算法明顯地勝出其他演算法許多。 
LMR 的通訊成本理論模組提供傳輸速率比來改善資料重新分配的效率，是獨立於
排程演算法的技術。其主要精神在去除節點傳送訊息給自己時被過於考慮的通訊成本，
以反應出實際在機器上執行的時間。因此，LMR 先測量節點傳送訊息給其他節點以及
給自己的速率，將其差異表現在估算的通訊成本中，譬如，節點傳送訊息給自己的，其
通訊成本應比傳給其他節點的低，不能視為同一通訊類型。LMR 提供兩者的速率比，
用來降低本地端傳送的成本或增加外地傳送的成本，以突顯兩者不同。讓排程演算法先
得知每個通訊其合理的通訊成本後在進行排程，會有較佳的排程結果。圖五結合 TPDR
 5 
息索引計算涵式用來產生訊息，並完成封裝技術以便快速傳送訊息。 
 完成Interval graph 之演算法實作 - 我們完成Interval graph之演算法實作，可用來
判斷通訊訊息的最少排程步驟。 
 完成GEN_BLOCK資料重新分配之排程演算法實作 - 針對最小排程步驟分析可行
的排程演算法，找出最大Degree與最小步驟的值是相等的。因此以循序降低最大
Degree進行通訊排程，並完成演算法與實作。 
 完成著色理論嵌入不規則排程的模組 - 利用著色理論得到的排程雖然是最小排程
步驟，但無法得到最低通訊量的排程結果，因此嵌入TPDR排程演算法中，利用其
特性排程不需要在乎通訊量的通訊訊息。 
 完成List scheduling algorithm[22]以及Divide-and-conquer scheduling algorithm [20]
之實作 - 為了證實本計畫開發的排程演算法為有效率的排程演算法，需實做其他
排程演算法以便進行實驗。 
 完成用於分析通訊排程步驟與訊息長度成本的理論模組 - 為了比較排程演算法的
優缺，我們完成了通訊成本的理論模組，用以判斷排程結果的好壞。 
 完成解決訊息傳送可能引起競爭之研究 - 通訊訊息再被傳送或接收的過程中易引
起處理器互相競爭，增加通訊成本。我們的排程演算法成功地避免了傳送訊息時可
能引起的通訊競爭情形。 
 完成資料配置參數產生器 - 為了模擬實際的資料配置參數，我們實做了一個資料
配置產生器，產生資料集中於某些處理器和平均分散在各個處理器上的資料配置
參數。 
 完成傳輸速率比的理論模組 - 為了讓通訊成本能確實反應出傳送資料花費的時
間，我們完成了傳輸速率比的理論模組，避免排程演算法處理到不正確的訊息。 
 完成網路傳輸速率測試程式之實作 - 未提供傳輸速率比的理論模組測試數據，需
要測試網路環境中的真實傳輸速度。 
 完成實驗數據分析模組 - 為了比較 LMR改善排程演算法的程度，我們提出分析模
組，分析改善前後的優劣。 
 執行本計畫所發表之相關論文列舉如下 
1. Ching-Hsien Hsu, Shih-Chang Chen and Chao-Yang Lan, "Scheduling Contention-Free Irregular 
Redistribution in Parallelizing Compilers," Accepted, The Journal of Supercomputing, Kluwer 
Academic Publisher, 2006.  // TPDR 演算法 
2. Ching-Hsien Hsu, Min-Hao Chen, Chao-Tung Yang and Kuan-Ching Li, “ Optimizing 
 7 
1. G. Bandera and E.L. Zapata, “Sparse Matrix Block-Cyclic Redistribution,” Proceeding of 
IEEE Int'l. Parallel Processing Symposium (IPPS'99), San Juan, Puerto Rico, April 
1999. 
2. Frederic Desprez, Jack Dongarra and Antoine Petitet, “Scheduling Block-Cyclic Data 
redistribution,” IEEE Trans. on PDS, vol. 9, no. 2, pp. 192-205, Feb. 1998. 
3. Minyi Guo, “Communication Generation for Irregular Codes,” The Journal of 
Supercomputing, vol. 25, no. 3, pp. 199-214, 2003. 
4. Minyi Guo and I. Nakata, “A Framework for Efficient Array Redistribution on 
Distributed Memory Multicomputers,” The Journal of Supercomputing, vol. 20, no. 3, pp. 
243-265, 2001. 
5. Minyi Guo, I. Nakata and Y. Yamashita, “Contention-Free Communication Scheduling 
for Array Redistribution,” Parallel Computing, vol. 26, no.8, pp. 1325-1343, 2000. 
6. Minyi Guo, I. Nakata and Y. Yamashita, “An Efficient Data Distribution Technique for 
Distributed Memory Parallel Computers,” JSPP'97, pp.189-196, 1997. 
7. Minyi Guo, Yi Pan and Zhen Liu, “Symbolic Communication Set Generation for 
Irregular Parallel Applications,” The Journal of Supercomputing, vol. 25, pp. 199-214, 
2003. 
8. C.-H Hsu, S.-W Bai, Y.-C Chung and C.-S Yang, “A Generalized Basic-Cycle 
Calculation Method for Efficient Array Redistribution,” IEEE TPDS, vol. 11, no. 12, pp. 
1201-1216, Dec. 2000. 
9. Ching-Hsien Hsu, Chao-Yang Lan and Shih-Chang Chen, “Optimizing Scheduling 
Stability for Runtime Data Alignment,” Embedded System Optimization - Lecture 
Notes in Computer Science, Vol. 4097, pp. 825-835, Springer-Verlag, Aug. 2006. 
(ESO’06) (SCI Expanded, NSC92-2213-E-216-029) 
10. Ching-Hsien Hsu, Chao-Yang Lan and Shih-Chang Chen, "Scheduling Contention-Free 
Irregular Redistribution in Parallelizing Compilers," Accepted, The Journal of 
Supercomputing, Kluwer Academic Publisher. (SCI, EI, NSC93-2213-E-216-028, 
NCHC-KING-010200) 
11. C.-H Hsu, Dong-Lin Yang, Yeh-Ching Chung and Chyi-Ren Dow, “A Generalized 
Processor Mapping Technique for Array Redistribution,” IEEE Transactions on Parallel 
and Distributed Systems, vol. 12, vol. 7, pp. 743-757, July 2001. 
12. Edgar T. Kalns, and Lionel M. Ni, “Processor Mapping Technique Toward Efficient 
Data Redistribution,” IEEE Trans. on PDS, vol. 6, no. 12, December 1995. 
 1 
 
行政院所屬各機關人員出國報告書提要                  
                                                 撰寫時間： 95 年  10 月  24  日   
姓 名 許慶賢 服 務 機 關 名 稱 
 
中華大學 
資工系 
連絡電話、 
電子信箱 
03-5186410 
chh@chu.edu.tw 
出 生 日 期  62 年 2  月 23  日 職 稱 副教授 
出席國際會議 
名 稱 
Eighth International Conference on Parallel Computing Technologies (PaCT’05), 
September 5 -9 2005. 
到 達 國 家 
及 地 點 
Krasnoyarsk, Russia 出 國
期 間
自 95 年 09 月 03 日
迄 95 年 09 月 11 日
內 容 提 要 
這一次在俄羅斯所舉行的國際學術研討會議共計五天。第一天上午由
Thomas L. Casavant博士針對 Genomics and Biomedical Applications for Parallel 
Computing以及 Fine Grained Parallelism in Spatial Dynamics Simulation主題發表精
闢的演說作為研討會的開始。第二天許多重要的研究成果分為兩個平行的場
次進行論文發表。本人選擇了 Architecture and Infrastructure 場次聽取報告。
晚上本人亦參加酒會，並且與幾位國外學者及中國、香港教授交換意見。第
三天本人在上午聽取了 Data and Information Management相關研究，同時獲
悉許多新興起的研究主題，並了解目前國外大多數學者主要的研究方向。當
天下午發表我們的論文，本人亦參與大會所舉辦的晚宴。並且與幾位外國學
者認識，交流，合影留念。會議第四天是一天的 Social Program。會議最後
一天，本人選擇與這一次論文較為相近的 Scheduling, Fault Tolerance and 
Mapping以及分散式計算研究聽取論文發表，並且把握最後一天的機會與國
外的教授認識，希望能夠讓他們加深對台灣研究的印象。五天下來，本人聽
了許多優秀的論文發表。這些研究所涵蓋的主題包含有：網格系統技術、工
作排程、網格計算、網格資料庫以及無線網路等等熱門的研究課題。此次的
國際學術研討會議有許多知名學者的參與，讓每一位參加這個會議的人士都
能夠得到國際上最新的技術與資訊。是一次非常成功的學術研討會。參加本
次的國際學術研討會議，感受良多。讓本人見識到許多國際知名的研究學者
以及專業人才，得以與之交流。讓本人與其他教授面對面暢談所學領域的種
種問題。看了眾多研究成果以及聽了數篇專題演講，最後，本人認為，會議
所安排的會場以及邀請的講席等，都相當的不錯，覺得會議舉辦得很成功，
值得我們學習。 
出 席 人 所 屬 機 
關 審 核 意 見 
 
 
層 轉 機 關 
審 核 意 見 
 
 
研 考 會 
處 理 意 見 
 
 
 
 3 
Every communication step will be scheduled after a degree-reduction iteration.  The second phase (coloring 
phase) schedules all messages of processors that with degree-2 and degree-1 using an adjustable coloring 
mechanism.  Based on the TPDR method, we also present an extended TPDR algorithm (E-TPDR).  The 
proposed techniques have the following characteristics:  
  It is a simple method with low algorithmic complexity to perform GEN_BLOCK array redistribution.   
  The two-phase degree reduction technique can avoid node contentions while performing irregular array 
redistribution. 
  The two-phase degree reduction method is a single pass scheduling technique.  It does not need to 
re-schedule / re-allocate messages.  Therefore, it is applicable to different processor groups without 
increasing the scheduling complexity. 
The rest of this paper is organized as follows.  In Section 2, a brief survey of related work will be presented.  
In section 3, we will introduce an example of GEN_BLOCK array redistribution as preliminary.  Section 4 
presents two communication scheduling algorithms for irregular redistribution problem.  The performance 
analysis and simulation results will be presented in section 5.  Finally, the conclusions will be given in section 6. 
2. Related Work 
Many methods for performing array redistribution have been presented in the literature.  These researches are 
usually developed for regular or irregular problems [5] in multi-computer compiler techniques or runtime support 
techniques.  We briefly describe the related works in these two aspects. 
Techniques for regular array redistribution, in general, can be classified into two approaches: the 
communication sets identification techniques and communication optimizations.  The former includes the 
PITFALLS [17] and the ScaLAPACK [16] methods for index sets generation; Park et al. [14] devised algorithms 
for BLOCK-CYCLIC Data redistribution between processor sets; Dongarra et al. [15] proposed algorithmic 
redistribution methods for BLOCK-CYCLIC decompositions; Zapata et al. [1] proposed parallel sparse 
redistribution code for BLOCK-CYCLIC data redistribution based on CRS structure.  The Generalized 
Basic-Cycle Calculation method was presented in [3]. 
Techniques for communication optimizations category, in general, provide different approaches to reduce 
the communication overheads in a redistribution operation.  Examples are the processor mapping techniques [10, 
12, 4] for minimizing data transmission overheads, the multiphase redistribution strategy [11] for reducing 
message startup cost, the communication scheduling approaches [2, 7, 13, 21] for avoiding node contention and 
the strip mining approach [18] for overlapping communication and computational overheads. 
On irregular array redistribution, some work has concentrated on the indexing and message generation while 
some has addressed on the communication efficiency.  Guo et al. [9] presented a symbolic analysis method for 
communication set generation and to reduce communication cost of irregular array redistribution.  On 
communication efficiency, Lee et al. [12] presented a logical processor reordering algorithm on irregular array 
redistribution.  Four algorithms were discussed in this work for reducing communication cost.  Guo et al. [19, 
20] proposed a divide-and-conquer algorithm for performing irregular array redistribution.  In this method, 
communication messages are first divided into groups using Neighbor Message Set (NMS), messages have the 
same sender or receiver; the communication steps will be scheduled after those NMSs are merged according to 
the relationship of contention.  In [21], a relocation algorithm was proposed by Yook and Park.  The relocation 
algorithm consists of two scheduling phases, the list scheduling phase and the relocation phase.  The list 
scheduling phase sorts global messages and allocates them into communication steps in decreasing order.  
Because of conventional sorting operation, list scheduling indeed performs well in term of algorithmic 
complexity.  If a contention happened, the relocation phase will perform a serial of re-schedule operations.  
While algorithm flow goes to the relocation phase, it has to allocate an appropriate location for the messages that 
can’t be scheduled at that moment.  This leads to high scheduling overheads and degrades the performance of a 
redistribution algorithm. 
 
3. GEN_BLOCK Array Redistribution 
In this section, we will present some properties of irregular array redistribution.  To simplify the presentation, 
notations and terminologies used in this paper are prior defined as follows. 
Definition 1: Given an irregular GEN_BLOCK redistribution on a one-dimension array A[1:N] over P 
processors, the source processors of array elements A[1:N] is denoted as SPi; the destination processors of array 
elements A[1:N] is denoted as DPi where 0 ≤ i ≤ P−1. 
Definition 2: A bipartite graph G = (V, E) is used to represent the communications of an irregular array 
redistribution on A[1:N] over P processors.  Vertices of G are used to represent the source and destination 
processors.  Edge eij in G denotes the message sent from SPi to DPj, where eij ∈ E.  |E| is given as the total 
 5 
will present two low complexity and high availability scheduling methods.   
 
4.1 The Two-Phase Degree Reduction Method 
The Two-Phase Degree Reduction (TPDR) method consists of two parts.  The first part schedules 
communications of processors that with degree greater than two.  In a bipartite graph representation, the TPDR 
reduces the degree of maximum degree nodes by one in each reduction iteration.  The second part schedules all 
messages of processors that with degree-2 and degree-1 using an adjustable coloring mechanism.  The degree 
reduction is performed as follows. 
Step1: Sort the vertices that with maximum degree d by total size of messages in decreasing order.  Assume 
there are k nodes with degree d.  The sorted vertices would be <Vi1, Vi2, …, Vik>. 
Step2: Schedule the minimum message mj = min{m1, m2, …, md} into step d for vertices Vi1, Vi2, …, Vik, 
where 1 ≤ j ≤ d. 
Step3: Maximum degree d = d-1.  Repeat Steps 1 and 2. 
We first use an example to demonstrate the degree reduction technique without considering the coloring 
scheme.  Figure 2(a) shows the communication patterns initially.  The source GEN_BLOCK distribution is of (7, 
10, 4, 18, 7, 18, 36).  The destination GEN_BLOCK distribution is of (10, 14, 18, 14, 14, 12, 18).  The 
redistribution is carried out over seven processors with maximum degree 3.  Therefore, the communications can 
be scheduled in three steps.  According to the above description in step 1, there are two nodes with degree 3, 
SP6 and DP1.   
The total message size of SP6 (36) is greater than DP1 (14).  Thus, SP6 is the first candidate to select a 
minimum message (m11) of it into step 3.  A similar selection is then performed on DP1.  Since m5 is the 
minimum message of DP1 at present, therefore, m5 is scheduled into step 3 as well.  As messages m11 and m5 are 
removed from the bipartite graph, adjacent nodes of edges m11 and m5, i.e., SP6, DP4, DP1 and SP3 should update 
their total message size.  After the degree reduction iteration, the maximum degree of the bipartite graph will 
become 2.  Figures 2(b) to 2(d) show this scenario.  Figures 2(e) and 2(f) show the similar process of above on 
degree = 2 bipartite graph.  In Figure 2(e), vertices SP6, SP5, SP4, SP1, DP3, DP2, DP1 and DP0 have the 
maximum degree 2 and are candidates to schedule their messages into step 2.  According to the degree 
reduction method, m12, m10 and m7 are scheduled in order.  The next message to be selected is m8.  However, 
both messages of DP3 will result node contention (one with SP4 and one with SP5) if we are going to schedule 
one of DP3’s messages.  This means that the degree reduction method might not reduce degree-2 edges 
completely when the degree is 2. 
 7 
To avoid the above situation, an adjustable coloring mechanism to schedule degree-2 and degree-1 
communications in bipartite graph can be applied.   
Since the consecutive edges must be scheduled into two steps, there is no need to care about the size of 
messages.  That means we don’t have to schedule the large messages together on purpose.   
 
 
Figure 3: Adjustable coloring mechanism for scheduling degree-2 and degree 1 communications.  The 
bipartite graph has three CSs.  Row 1 is the schedule tables of CS1 and CS2.  Row 2 is the schedule table 
consisted of CS1 and CS2, and a schedule table of CS3.  Row 3 is the same as Row 2 but the schedule 
table of CS3.  The messages m12 and m13 are exchanged.  Row 4 is the schedule table consisted of CS1、
CS2 and CS3. 
Let’s consider again the example in Figure 2(e).  Figure 3 demonstrates scheduling of the coloring 
phase for communication steps 1 and 2.  To facilitate our illustration, we denote each connected 
component in G’ as a Consecutive Section (CS).  In Figure 3, there are three Consecutive Sections, the 
CS1 is consisted of four messages m1, m2, m3 and m4; the CS2 is consisted of five messages m6, m7, m8, m9 
and m10; the CS3 is consisted of two messages m12 and m13.  A simple coloring scheme is to use two 
colors on adjacency edges alternatively.  For example, we first color m1, m6 and m12 red; then, color m2, 
m7 and m13 blue; and so on.  The scheduling results for CS1 and CS2 are given as shown in Row 1.  Row 
2 shows the merging result of CS1 with CS2 and the schedule of CS3.  In Row 2, messages m6 (15) and m13 
(18) dominate the communication time at steps 1 and 2, respectively.  This results total communication 
cost = 33.  If we change the order of steps 1 and 2 in CS3, it becomes m13 dominates the communication 
time in step 1 and m12 dominates the communication time in step 2.  This will result total communication 
cost = 30.  Therefore, the colors of two steps in CS3 are exchanged in Row 3 for less communication cost.  
Row 4 shows communication scheduling of the adjustable coloring phase for degree-2 and degree-1 
communications.  The complete scheduling for this example is shown in Figure 4. 
 9 
// schedule the minimum message (assume e
ij
) of v into step d 
13.          mark_red(); 
// mark adjacent vertices of edge e
ij
 as red to avoid conflict communication 
14.          update schedule(step, message ); 
15.       } 
16.       step--; 
17.    } 
18.    coloring_message(); 
// color the remaining messages with maximal degree is 2 
19.    merge_Schedule(); 
// merge schedules of consecutive sections into communication steps 1 and 2 
20.    update schedule(step, message); 
21. } 
end_of_two_phase_degree_reduction 
_______________________________________________________________________ 
4.2 Extended TPDR 
Based on TPDR, we present an extended two-phase degree reduction (E-TPDR) algorithm.  An 
edge-complement operation is added in the degree-reduction phase.  As the TPDR algorithm stated, the original 
degree-reduction operation only schedules degree-k nodes’ messages into communication step k.  This might not 
fully utilize the available space in step k and remains heavy communications in the previous steps (less than k).  
Therefore, a principle for adding extra messages into these steps is to select the maximum message that is smaller 
than the length of current step and with un-marked adjacent vertices.   
The key concept of this modification is to schedule messages into communication steps during reduction 
phase as many as possible.  Because the additional scheduled messages are with smaller message size than the 
current step length, the edge-complement operation will not influence the cost of original scheduling from TPDR.  
Figure 6 shows the communication schedule of the example given in Figure 2 using E-TPDR.  Although this 
example does not reflect lower total cost of E-TPDR, section 5 will demonstrate the improvement of E-TPDR 
method from the simulation results. 
 S1: m1(7), m3(7), m6(15), m9(10), m13(18) 
S2: m4(4), m7(3), m10(8), m12(12) 
S3: m11(6), m5(3), m8(4), m2(3) 
 
Figure 6: The E-TPDR scheduling of communications for the example in Figure 2. 
 
The algorithm of the extended two-phase degree reduction method is given as follows. 
_______________________________________________________________________ 
Algorithm E_TPDR (P, GEN_BLOCK) 
1. generating messages; 
2. while ( messages != null) 
3. { 
4.    step = d = maximal degree; 
5.    while (step > 2 ) 
6.    { 
7.       find_mark_blue(d); 
// marking degree-d nodes blue 
8.       sort_marked_blue_node(); 
// sorting blue nodes in decreasing order by message size 
9.       while(marked node !=null) 
10.       { 
11.          choose_marked_node(); 
// selecting a marked node, assume v 
12.          pick_minimal_message(); 
// schedule the minimum message (assume e
ij
) of v into step d 
13.          mark_red(); 
// mark adjacent vertices of edge e
ij
 as red to avoid conflict communication 
14.          update schedule(step, message ); 
15.          step_length(d); 
               // determine the length L of current step 
16.          sort_small_message(); 
               // sort all messages with unmarked adjacent vertices and message size is smaller than L 
17.          pick_message(); 
               // schedule the selected messages into current step if there is no contention 
18.          mark_red(); 
               // mark adjacent vertices of the scheduled edges as red 
19.          update schedule( step, message ); 
 11 
Figure 9 gives the comparisons of the E-TPDR algorithm and the divide-and-conquer algorithm.  When the 
number of processors is 4, there are about 60% cases has the same result by both algorithms.  Similar to the 
previous observations, the E-TPDR performs well when number of processors is numerous. 
5.2 Simulation B 
Simulation B is carried out to examine the performance of TPDR and E-TPDR algorithms on even cases.  We 
also use the random generator to produce 10,000 data sets for this test. 
Figures 10 and 11 show the performance comparisons of TPDR and DC, E-TPDR and DC, respectively.  
Overall speaking, we have similar observations as those described in Figures 7 and 9.  The E-TPDR performs 
better than TPDR.  When number of processors is large, the TPDR and E-TPDR both provide significant 
improvements.  Compare to the results in uneven cases (simulation A), the ratio of our algorithms outperform 
the DC algorithm become lower.  We move our focus to Figure 12 to explain this phenomenon.   Figure 12 
gives the statistics of different degree nodes for even distribution test data.  We observed that there is no nodes 
with degree higher than 4.  In other words, the maximum degree of nodes of these 10,000 test samples is 3.  On 
this aspect, the DC algorithm and the TPDR methods have more cases that are the same.  This is also why the 
TPDR and E-TPDR have better ratio from 99% to 93%. 
From the above performance analysis and simulation results, we have the following remarks. 
Remark 1: The TPDR and E-TPDR algorithms perform better in uneven cases than in even cases.   
Remark 2: The TPDR and E-TPDR algorithms perform well when number of processors is large. 
 
TPDR vs DC with uneven data distribution scheme
0
2000
4000
6000
8000
10000
number of processors
n
u
m
b
e
r
 
o
f
 
s
a
m
p
l
e
s
TPDR better than DC
3134 6724 8253 8926 9288 9481 9662 9766 9811 9843 9908
DC better than TPDR
511 901 884 698 555 420 293 204 168 139 76
the same
6355 2375 863 376 157 99 45 30 21 18 16
4 6 8 10 12 14 16 18 20 22 24
 
Figure 7: The E-TPDR scheduling of communications for the example in Figure 2. 
Statistics of degree with uneven data distribution scheme
0
50000
100000
150000
200000
250000
300000
4 6 8 10 12 14 16 18 20 22 24
number of processors
n
u
m
b
e
r
 
o
f
 
n
o
d
e
s
degree1
degree2
degree3
degree4
degree5
degree6
 
Figure 8: Statistic of degree-2 nodes in 10,000 uneven GEN_BLOCK test samples. 
 13 
Statistics of degree with even data distribution scheme
0
50000
100000
150000
200000
250000
300000
350000
400000
4 6 8 10 12 14 16 18 20 22 24
number of processors
n
u
m
b
e
r
 
o
f
 
n
o
d
e
s
degree1
degree2
degree3
degree4
degree5
degree6
 
Figure 12: Statistic of degree-2 nodes in 10,000 even GEN_BLOCK test samples. 
6. Conclusions 
In this paper, we have presented a two-phase degree-reduction (TPDR) scheduling technique to efficiently 
perform HPF2 irregular array redistribution on distributed memory multi-computer.  The TPDR is a simple 
method with low algorithmic complexity to perform GEN_BLOCK array redistribution.  An extended algorithm 
based on TPDR is also presented.  Effectiveness of the proposed methods not only avoids node contention but 
also shortens the overall communication length.  The simulation results show improvement of communication 
costs and high practicability on different processor hierarchy.  
In HPF, it supports array redistribution with arbitrary source and destination processor sets.  The 
technique developed in this paper assumes that the source and the destination processor sets are the same.  In 
the future, we will study efficient methods for array redistribution with arbitrary source and destination processor 
sets.  Besides, the issues of scheduling irregular problems on grid system and considering network 
communication latency in heterogeneous environments are also interesting and will be investigated.  Also, we 
will also study realistic applications and analyze their performance. 
References 
[1] G. Bandera and E.L. Zapata, “Sparse Matrix Block-Cyclic Redistribution,” Proceeding of IEEE Int'l. Parallel 
Processing Symposium (IPPS'99), San Juan, Puerto Rico, April 1999. 
[2] Frederic Desprez, Jack Dongarra and Antoine Petitet, “Scheduling Block-Cyclic Data redistribution,” IEEE 
Trans. on PDS, vol. 9, no. 2, pp. 192-205, Feb. 1998. 
[3] C.-H Hsu, S.-W Bai, Y.-C Chung and C.-S Yang, “A Generalized Basic-Cycle Calculation Method for 
Efficient Array Redistribution,” IEEE TPDS, vol. 11, no. 12, pp. 1201-1216, Dec. 2000. 
[4] C.-H Hsu, Dong-Lin Yang, Yeh-Ching Chung and Chyi-Ren Dow, “A Generalized Processor Mapping 
Technique for Array Redistribution,” IEEE Transactions on Parallel and Distributed Systems, vol. 12, vol. 7, pp. 
743-757, July 2001. 
[5] Minyi Guo, “Communication Generation for Irregular Codes,” The Journal of Supercomputing, vol. 25, no. 3, 
pp. 199-214, 2003. 
[6] Minyi Guo and I. Nakata, “A Framework for Efficient Array Redistribution on Distributed Memory 
Multicomputers,” The Journal of Supercomputing, vol. 20, no. 3, pp. 243-265, 2001. 
[7] Minyi Guo, I. Nakata and Y. Yamashita, “Contention-Free Communication Scheduling for Array 
Redistribution,” Parallel Computing, vol. 26, no.8, pp. 1325-1343, 2000. 
[8] Minyi Guo, I. Nakata and Y. Yamashita, “An Efficient Data Distribution Technique for Distributed Memory 
Parallel Computers,” JSPP'97, pp.189-196, 1997. 
[9] Minyi Guo, Yi Pan and Zhen Liu, “Symbolic Communication Set Generation for Irregular Parallel 
Applications,” The Journal of Supercomputing, vol. 25, pp. 199-214, 2003. 
[10] Edgar T. Kalns, and Lionel M. Ni, “Processor Mapping Technique Toward Efficient Data Redistribution,” IEEE 
Trans. on PDS, vol. 6, no. 12, December 1995. 
[11] S. D. Kaushik, C. H. Huang, J. Ramanujam and P. Sadayappan, “Multiphase data redistribution: Modeling and 
evaluation,” Proceeding of IPPS’95, pp. 441-445, 1995. 
[12] S. Lee, H. Yook, M. Koo and M. Park, “Processor reordering algorithms toward efficient GEN_BLOCK 
redistribution,” Proceedings of the ACM symposium on Applied computing, 2001. 
[13] Y. W. Lim, Prashanth B. Bhat and Viktor and K. Prasanna, “Efficient Algorithms for Block-Cyclic 
Redistribution of Arrays,” Algorithmica, vol. 24, no. 3-4, pp. 298-330, 1999. 
[14] Neungsoo Park, Viktor K. Prasanna and Cauligi S. Raghavendra, “Efficient Algorithms for Block-Cyclic Data 
