 
中英文摘要 
 
快取記憶體的使用可以用來大幅提升處理器的效能，然而，由於存取的次
數非常頻繁及所需的電晶體數量龐大，使得處理器的整體功率及面積消耗有相
當大的部分是消耗在快取記憶體上。在本計劃中，我們提出了二種低功率的技
術，可有效的降低快取記憶體的功率消耗。首先，第一種低功率設計，稱為「分
頁式快取記憶體」，將一個完整的快取記憶體切割成較小分割所構成的集合，
每一個分割分別對應到存放在 TLB 中的一個分頁，藉由限制一個區塊可以存
放的範圍，而且一次只存取一個較小的分割，標籤面積、功率消耗與存取時間
皆可大幅的降低與改善。第二種低功率設計，稱為「常用樣式再編碼」技術，
利用常用樣式局部性的現象，將常用樣式再重新編碼成較短的格式，同時降低
位元線切換所造成的動態功率消耗及漏電流所造成的靜態功率消耗。與傳統的
32K 2-way 的指令快取記憶體比較，實驗結果顯示，在分割尺寸為 2K 的架構
下，同時應用這二種設計可減少 5%的快取記憶體面積，57%的功率消耗與 39%
的存取時間。 
關鍵詞: 低功率設計、快取記憶體、分頁式、TLB、再編碼。 
 
The on-chip caches are widely used to improve the system performance. Due to the 
frequent access and the need of large amount of transistors, however, the caches 
contribute significantly to both the total power consumption and area cost of the chip. In 
this project, we propose two low-power schemes to minimize the cache power 
consumption. In the first scheme, called paged cache, the entire cache is divided into a 
set of partitions, and each partition is dedicated to only one page cached in the TLB. By 
restricting the range in which the cached block can be placed, and accessing only a single 
partition, both the tag area and power consumption per cache access can be reduced 
largely. Then, the second design is called frequent pattern recoding (FPR). Based on the 
observation that most programs exhibit the frequent pattern locality, in FPR a few 
frequent patterns are recoded into the short format to reduce the bitline switching 
activities and leakage current, and thus both the dynamic and static power consumption 
of the cache can be reduced. Compared to the baseline case of a 32KB 2-way instruction 
cache, the experimental results show that our design with 2KB partition size can result in 
roughly 5%, 57% and 39% reduction in cache area, power consumption and access time, 
respectively. 
Keywords: Low power design, cache, paged, TLB, recoding.  
 
                                                 
 
called FOE, in which the frequent opcode with 8-bit width would 
be remapped into a short code whose width is much shorter than 
8-bit. The width of remapped code depends on the number of 
frequent opcodes that we choose. It is given by log2N, where N is 
the number of frequent opcodes. For example, if the number of 
frequent opcodes that we choose is 4, the remapped code width is 
log24=2 bits. Thus, when a frequent opcode is encountered during 
the program execution, we can deactivate the remainder 6 bits to 
reduce the power consumption of instruction cache. By exploiting 
the frequent opcode locality exists in most programs, the 
proposed FOE scheme can effectively reduce the power 
consumption of instruction cache. 
2.2 Related Work 
A low-power opcode encoding method was proposed in [1]. They 
reassign opcodes so that more frequently consecutive instruction 
pairs have a smaller Hamming distance between their opcodes. 
By reducing the number of bit switches that occur in the 
instruction register and instruction bus, their method can reduce 
the overall chip power consumption. A similar deign presented in 
[2], where Pouladi et al. proposed two schemes to improve the 
energy efficiency of instruction fetch in the embedded systems 
with 16-bit and 8-bit external bus interface architecture. By 
recoding and swapping the bits in the individual fields of 
instruction code to minimize the Hamming distance between the 
consecutive instructions, their designs can largely reduce the 
amount of switching activity on the bus lines and thus the overall 
power consumption. Because each program exhibits the unique 
behavior that is different from other programs, the reassigned 
method in [1] and recoding technique in [2] are individually 
optimal for each program, not for all programs. Thus, an off-line 
analysis must be performed before starting the program execution. 
Due to the lack of flexibility, they are usually applied to the 
embedded systems with a specific application. 
In [3], a run-time reconfiguration mechanism was proposed for 
both code size and power reduction. They use an instruction 
remap table to map multiple instructions to a single compressed 
bit pattern. A similar deign presented in [4], where two selective 
instruction compression methods are proposed to reduce 
instruction memory and instruction bus energy consumption. 
There are two major differences between our design and [3][4]. 
First, the FOE scheme is software independent. Without any 
modification of source code, our design is compatible with the 
existing applications. Second, the FOE scheme is tableless, i.e., 
no tables are required to store the compression information. 
3. FREQUENT OPCODE ENCODING 
The key idea behind our design is to reduce the read power 
consumption of the instruction cache by encoding the frequent 
opcode into a short remapped code. As revealed in Section 2, 
because the instructions LW, SW, ADDU and ADDIU account 
for over 60% of the executed instructions, we choose these four 
instructions as the frequent instruction set throughout this paper. 
The entire operation flow of our design is illustrated in Fig. 2 and 
described as follows: 
Write Flow: When the instruction is loaded from the lower level 
memory, we first identify whether the loaded instruction is a 
frequent instruction or not. We need an extra 1-bit signal to 
indicate the class of instruction, i.e., flag shown in Fig. 2. 
Depending on the value of flag, there are two possible paths in the 
write flow. (1) If the loaded instruction is not a frequent 
instruction, then it and flag=0 would be written into the 
instruction cache directly without opcode recoding. (2) If the 
loaded instruction is a frequent instruction, then its opcode must 
be recoded into a short format. This work can be performed by the 
remapped opcode encoder. Finally, the remapped opcode has to 
be written into the cache together with flag=1. 
Read Flow: When the instruction is fetched from the cache, by 
examining the flag value we can easily identify if the fetched 
instruction is a frequent instruction. (1) flag=0 implies that the 
fetched instruction is a non-frequent instruction. It would be 
delivered to the CPU core without decoding. (2) In the other case, 
flag=1 means that the fetch instruction is a frequent instruction. 
Its opcode is recoded before it is written into cache. 
Consequently, we have to decode the remapped opcode to the 
original opcode before delivering it to the CPU core. This work 
can be performed by the remapped opcode decoder. 
3.1 Remapped Opcode Encoder 
Fig. 3(a) shows the remapped opcode encoder which has 8 input 
and 3 output signals. The function of the remapped opcode 
encoder is to identify the frequent instruction. If the input opcode 
is a frequent opcode, then it will generate 2-bit remapped code 
and set flag signal to 1. Otherwise, it resets flag signal to 0 and 
output D7, D6 of the original opcode directly. The function is 
described by the truth table shown in Fig. 3(b), and we can use the 
following Boolean expression to synthesize the circuit. 
flagDflagDDDDDDDR
flagDflagDDDDDDDDDDDDDR
DDDDDDDDDDDDDDDDDDflag
⋅+⋅=
⋅+⋅+=
++=
612345670
712456124560371
123456723423401567
)(
)(
 
Frequent 
instructions?
Instruction loaded
from lower memory
Y
N
Instruction
Cache
flag = 0
flag = 1
flag
flag = 1?
Remapped opcode
decoder
Y
N
Instruction
Cache
flag = 0
flag = 1
flag
CPU core
Remapped opcode
encoder
 
 (a) (b)
 
Fig. 2. The operation flow of the frequent opcode encoding 
scheme. (a) Write to cache. (b) Read from cache. 
Table 1. Summary of frequent instructions. 
Function Opcode Format
LW Load word,displaced addressing 0x28 LW rt,offset(rs) inc_dec
SW Store word,displaced addressing 0x34 SW rt,offset(rs) inc_dec
ADDU Add unsigned(no overflow check) 0x42 ADDU rd,rs,rt
ADDIU Add immediate unsigned(no overflow check) 0x43 ADDIU rd,rs,rt  
400
instructions, i.e., N = NFI + NNFI. In reading frequent opcode, 
beside flag bit, only 2 (out of 8) bit cells are active, so the number 
of active bitlines is 1+2=3. In the other case, the number of active 
bitlines is 1+8=9 in reading non-frequent opcode. 24 is the 
number of bit cells that do not belong to opcode field. Combining 
Equation (1) and the SimpleScalar simulation results, we can 
obtain the ASPI value for each benchmark, as shown in Table 2. 
On average, the FOE scheme reduces the ASPI value to 29.21. 
Compared to ASPIConv=32, our design can reduce 8.7% of bitline 
switching activities. 
A cache consists of tag-array and data-array. Note that our design 
only reduces the bitline, sense amplifier and output driver power 
of the data-array, which contribute only a part of total cache 
power consumption. By using CACTI tool [5], we obtain the 
considered partial components in our FOE scheme contribute 
roughly 48.2% to the total cache power consumption for the 
baseline instruction cache. Consequently, our design can reduce 
the overall cache read power by 8.7%*0.482=4.2%. Although the 
encoder and decoder are simple combinational circuit whose 
power consumption is negligible compared to the cache, for more 
accurate estimation, taking into account their power consumption, 
the overall power reduction is roughly 4%. 
4.2 Performance Impact 
The remapped opcode encoder is located on the instruction 
refilling path. Compared to the conventional cache without any 
encoding scheme, our design will result in a write performance 
loss. For instruction cache, because the write operation only 
occurs in cache miss which is not the critical path in determining 
the system performance, the write performance loss induced by 
FOE would not affect the system performance. 
In contrast to write, because the instruction fetch determines the 
length of pipeline stage, thus the system cycle time, the 
instruction read performance is particularly critical to the system 
performance. In the FOE scheme, the remapped opcode decoder 
is located on the critical path in fetching instruction. Compared to 
the conventional cache, our design will result in an extra delay in 
opcode decoding. From Section 3.3, the remapped opcode 
decoder is a very simple combinational circuit. It costs only one 
2-input AND gate delay, which is about 0.09 ns measured from 
the HSPICE simulation. 
By using CACTI tool [5], as shown in Table 3 we can obtain the 
time results for all components in the baseline cache. From this 
table, it is clear that there are two paths in a cache read, i.e., data 
side and tag side. Because the cache access time is determined by 
the longest path, the access time of the baseline cache is 1.4432 
ns. Note that due to the additional decoder our design will 
increase the time length in data side, i.e., 1.3070+0.09=1.3970 ns. 
Because the increased data path (i.e., 1.3970 ns) is still less than 
tag path (i.e., 1.4432 ns), we conclude that the FOE scheme does 
not incur any read performance penalty. 
5. CONCLUSIONS 
By exploiting the frequent opcode locality, in this paper we 
propose a simple frequent opcode encoding scheme to reduce the 
cache power during a frequent instruction read. Based on over 
60% of the executed instructions belong to the frequent 
instruction set, i.e., LW, SW, ADDU and ADDIU, our design can 
reduce the overall read power consumption of a 32KB 2-way 
instruction cache by 4% without any performance loss. 
6. REFERENCES 
[1] Kim, S. and Kim, J. 1999. Opcode encoding for low power 
instruction fetch. IEE Electronics Letters. 35, 13, 1064-1065. 
[2] Pouladi, A. and Nooshabadi, S. 2005. Opcode encoding for 
low power embedded systems. in Proceedings of Symposium 
on Circuits and Systems. 23-26. 
[3] Chandar, S. G., Mehendale, M. and Govindarajan, G. 2001. 
Area and power reduction of embedded DSP systems using 
instruction compression and re-configurable encoding. in 
Proceedings of International Conference on Computer Aided 
Design (ICCAD),  631-634. 
[4] Kadayif, I. and Kandemir, M. T. 2002. Instruction 
compression and encoding for low-power systems. in 
Proceedings of International ASIC/SOC Conference, 301-
305. 
[5] Shivakumar, P. and Jouppi, N. P. 2001. CACTI 3.0: An 
integrated cache timing, power and area model. COMPAQ 
WRL Research Report. 
 
 
Table 2. The ASPI value for each benchmark. 
LW SW ADDU ADDIU N FI % ASPIFOE
164.gzip 17.35% 7.37% 21.42% 12.03% 58.17% 29.51
175.vpr 28.28% 12.87% 21.66% 9.21% 72.02% 28.68
176.gcc 24.93% 15.16% 17.03% 11.71% 68.83% 28.87
181.mcf 36.32% 12.59% 10.40% 8.85% 68.16% 28.91
197.parser 33.76% 13.83% 13.43% 8.02% 69.04% 28.86
253.perlbmk 19.91% 9.45% 11.07% 17.36% 57.79% 29.53
255.vortex 32.48% 21.48% 16.08% 9.19% 79.23% 28.25
256.bzip2 25.22% 6.02% 25.87% 5.79% 62.90% 29.23
Ave. CINT 27.28% 12.35% 17.12% 10.27% 67.02% 28.98
177.mesa 22.18% 8.79% 13.44% 7.66% 52.07% 29.88
179.art 28.26% 3.15% 28.53% 3.31% 63.25% 29.21
183.equake 19.04% 6.85% 19.48% 11.11% 56.48% 29.61
188.ammp 41.97% 8.35% 7.45% 7.94% 65.71% 29.06
Ave. CFP 27.86% 6.79% 17.23% 7.51% 59.38% 29.44
Table 3. Time components of the baseline 32KB, 
2-way cache (ns). 
decode WL+BL sense_amp compare
Mux
driver
sel
inverter
output
driver total
Data side 0.6027 0.3350 0.1868 0.1825 1.3070
Tag side 0.2562 0.1445 0.1148 0.3138 0.3858 0.0456 0.1825 1.4432
402
