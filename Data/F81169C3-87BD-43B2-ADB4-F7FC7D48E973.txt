sentiment aware opinion leader and influence mining. 
They are both key issues in the Web2.0 era. 
In this project, we will discuss characteristics of 
signed social networks and the relationship with real 
social network. We will also propose to utilize the 
knowledge social networks constructed from Delicious, 
Yahoo Answer, and Wikipedia to discuss and analysis 
the impact and influence of opinion leaders in these 
networks. Moreover, we will discuss the mining and 
the sentiment orientation of blog articles and news. 
These researches will help us to understand the 
operation and information dissemination in a 
knowledge social network. Therefore, we focus the 
following three topics: (1) Signed Social Network 
Mining, (2) Opinion Leader Mining and Knowledge 
Recommendation, and (3) Influence and Sentiment 
Mining of Blog Articles and News. According to the 
developed mining systems and sentiment aware 
techniques in the proposed project, we prospect to 
clearly understand the effects of opinion leaders and 
the information dissemination flow. 
 
英文關鍵詞： Web2.0, Social Network, Signed Network, Opinion 
Leader, Opinion Sentiment Mining, Question 
Recommendation 
 
 definition of well-known state is brief. We can identify a 
Web page as well-known page (ex: URLs of Yahoo, Google) 
when the page is annotated by large part of users. 3) The 
pages in burgeoning state are like isolated page but more 
useful and interesting for lots of users after sharing in Web. 
These pages will become a focus of attention after a span of 
time. 
Table 1 : Variations of the annotation counts and PageRanks of different 
pages 
URL 9/24 9/27 PageRank
A 
http://www.smashingmagaz
ine.com/2009/09/24/10-usef
ul-usability-findings-and-gu
idelines/ 
135 2249 0 
B http://www.joelonsoftware.com/items/2009/09/23.html 96 479 0 
C http://msdn.microsoft.com/en-us/data/bb931106.aspx 135 136 5 
 
 
 
 
Figure 1 : The states and evolution of a page. 
Figure 2 shows an example of a page with a large 
number of annotations in Del.ici.ous. The red region of the 
page count chart represents a bursty event period for the 
page. There are several reasons for a page to enter into the 
burgeoning state, such as users who are involved by experts. 
When an expert annotates a new page to his bookmarks, 
there is a higher probability than a common user for others 
to follow the tagging behavior. In this work, a user has a 
good insight means that the user can see or lead the bursty 
phenomenon before others do. 
The annotation behavior of users for these pages will 
involve different situations. Suppose that a page is not 
interesting or useful for people, the page will be ignored by 
other users to annotate it, we called them isolated for these 
pages. Consider another condition, if a page is useful or 
interesting for attracting user to tag, we define this page is in 
the state of burgeoning. We list the page counts evolution of 
a well-known Web 2.0 service site Twitter.com in Figure 2, 
we use red region and blue region to demarcate the 
burgeoning state and well-known state. It did not catch the 
attention of user to tag before August 2006. User gradually 
noticed the site by sharing and communicating in internet, 
and a bursty event happened around in February 2007 as 
shown in the first red region. After a period of time, this 
page had regular annotation counts from users. Thus, the 
state of this page changed to “well-known”. The gap 
between the well-known state and isolated states 
corresponds to the quantity of page counts. A page in the 
well-known state must receive a large and regular number of 
page counts. 
 
 
Figure 2 : The page count chart of Twitter.com. 
II. RELATED WORK 
In the analysis of the Web linking graph, PageRank [21] 
is a common link analysis algorithm that represents the 
importance score of which people who randomly click on 
links will arrive at a group of hyperlinked documents. The 
Hyperlink-Induced Topic Search (HITS) algorithm [15] is 
another prominent algorithm for ranking Web pages, and 
two critical concepts are defined in a mutual recursion. A 
new algorithm based on eigenvector calculations for ranking 
blog called EigenRumor[5], this approach scores each blog 
entry by weighting the hub and authority scores of the 
bloggers and the reputation of posts. 
There are many research topics about the folksonomy 
systems. Semantics[25, 26] research in user tagging is 
derived from the social annotations. Social tag prediction[10] 
and recommendation[23, 24] problem determine whether a 
given tag should be applied to associated Web page. Tag 
importance score [22] regard about how to rate the goodness 
of tags for the particular documents. Browsing social 
annotations [17] show how to effectively find desired 
resources from a large amount of annotation data .  
An increasing number of studies has investigated 
folksonomy systems between users and their behaviors, 
 (d, W) = days in interval W/days in interval t୉. 
We describe the different estimations of entropy in Figure 3, 
the periods of bursty in Figure 3(a) is more dispersal than 
Figure 3(b). This represents that there is a higher probability 
for the event in Figure 3(b) to compose these periods of 
events to be a bursty event. 
Consider the example in Figure 4. Users A, B and C tag 
a page (http://www.pinvoke.com/) in different time stamps. 
The red regions represents the bursty periods. User A will 
get the higher annotation insight benefited from the three 
periods of bursty phenomenon. Users B and C attain their 
annotation insight score from the last period of bursty 
phenomenon. The annotation insight of user C is higher than 
user B because user C is more close to the bursty 
phenomenon for predicting the bursty event. In our 
measurement, we finally attain the annotation insights for 
user A as 0.03195, for user B as 0.00035, and for user C as 
0.02345. These values also fit our observations. 
 
     
(a)                          (b) 
Figure 3: Different distributions of bursty events. 
 
Figure 4: An example of estimating annotation insight 
B. CAIS: Community based Annotation Insight Search  
In a folksonomy system, the interactions and 
relationships among users strongly impact the information 
propagation. In Del.ici.ous, a fan’s network can be regarded 
as a way for users to share and distribute their bookmarks. 
Based on the fans network, we propose a Community based 
Annotation Insight Search algorithm (CAIS) that utilizes the 
leverage of user communities following the evaluation of 
local user annotation insight. In this algorithm, we use local 
annotation insight as a factor to form the matrix A. This 
matrix represents the annotation relationships between users 
and pages. Moreover, we use the fans information to 
construct the matrix F to represent the fans network among 
users. As shown in Figure 5, we then build a linking based 
reinforcement model to evaluate the insight score of users (I) 
and burgeoning score of pages (P) by evolving the effect of 
the community score of users (C).  
If a page is annotated by a user with good insight who 
also has a large number of following fans, then this page 
will attract more attention after it has been annotated, and, 
therefore, it should have a high burgeoning score in our 
system. Moreover, when a user annotates many pages with 
high burgeoning scores and has many following fans, we 
will consider the user has a high annotation insight. The 
reinforcement algorithm is described as follows: 
 
 
 
 
Figure 5: Link model of CAIS (Community based Annotation Insight 
Search). 
 Table 2 : Setting of page distribution 
 Well-known 
pages 
Burgeoning 
pages 
Isolated pages
number ratio number ratio number ratio
A 100 33% 100 33% 100 33%
B 20 10% 100 33% 180 57%
C 100 33% 20 10% 180 57%
 
A. Ranking results 
The ranks of the simulated experts are shown in Figure 
6. For expert detection, we can see that the CAIS can detect 
the differences between the three types of experts. Geeks 
have earlier taggings for most of pages, so we can consider 
that the geeks will be ranked in the top. CAIS will rank 
pioneers higher than veterans, because of that pioneers 
mostly concentrate on tagging the pages which will become 
popular. Note that some overlapping between them is 
expected due to the PMF simulation setup. For the spammer 
detection, CAIS will effectively reduce the ranking of 
imitators and flooders by using the information of user 
community. However, group imitators are ranked after the 
experts. We think the group imitators are not easy to 
identify when their community network have been 
completely constructed. ER+IS have a similar performance 
as CAIS, but it ranks veterans higher than pioneers in 
average, and the rank of imitators and flooders are higher 
than in CAIS. ER tends to find a user who has more 
bookmarks and fans. Thus, other spammers also have been 
ranked in the top of ranking. The performance of HITS+IS, 
HITS, and HITS+BWI are affected by the spammers, 
because these spammers mimic the behavior of experts, so 
they will get the similar ranking for these approaches. 
 
B. Different Page distribution analysis in dataset Sh 
The experimental results of the different page 
distributions are shown in Table 3. There are numerous of 
page counts and related user for well-known pages. This 
largely affects the baseline approaches, CV, FREQ and 
FFREQ to detect the answer set. In set A, baseline 
approaches always make well-known pages on the top. The 
MAP value of baseline approaches in set A is thus affected 
by the 100 well-known pages. Note that CAIS, ER+IS and 
HITS+IS with the scheme of IS produces better MAP value 
than others in set A, B and C. CAIS also have higher MAP 
values than ER+IS and HITS+IS. The ER+IS uses the 
EigenRumor linking model, which is not appropriate at all 
for the folksonomy system. It is also affected by the 
distribution of well-known pages more. In the total average 
MAP values of three sets, CAIS still have the best 
performance among all methods. A similar conclusion is 
also observed in real dataset. CAIS attained the best 
performance with MAP 0.047 and MAP values of other 
methods range from 0.014 to 0.024. 
 
(a) Simulated experts 
 
 
(b) Simulated spammers 
Figure 6: Ranks of real-world users and simulated users 
 
 international conference on Web search and web data 
mining,2008 
[10] P. Heymann, D. Ramage, and H. Garcia-Molina. Social tag 
prediction. In Proceedings of the 31st annual international 
ACM SIGIR conference on Research and development in 
information retrieval,2008 
[11] K. Jarvelin and J. Kekalainen. IR evaluation methods for 
retrieving highly relevant documents. In Proceedings of the 
23rd annual international ACM SIGIR conference on Research 
and development in information retrieval,2000 
[12] L. J. Jensen, J. Saric, and P. Bork. Literature mining for the 
biologist: from information retrieval to biological discovery. 
Nature Reviews Genetics,2006 
[13] S. Ji, K. Zhou, C. Liao, Z. Zheng, G.-R. Xue, O. Chapelle, G. 
Sun, and H. Zha. Global ranking by exploiting user clicks. In 
Proceedings of the 32nd international ACM SIGIR conference 
on Research and development in information retrieval,2009 
[14] T. Joachims. Optimizing search engines using clickthrough data. 
In Proceedings of the eighth ACM SIGKDD international 
conference on Knowledge discovery and data mining,2002 
[15] J. M. Kleinberg. Authoritative sources in a hyperlinked 
environment. J. ACM,1999 
[16] R. Kumar, J. Novak, P. Raghavan, and A. Tomkins. On the 
Bursty Evolution of Blogspace. World Wide Web,2005 
[17] R. Li, S. Bao, Y. Yu, B. Fei, and Z. Su. Towards effective 
browsing of large scale social annotations. In Proceedings of the 
16th international conference on World Wide Web,2007 
[18] C.-H. Lo, W.-C. Peng, and M.-F. Chiang. Ranking Web Pages 
from User Perspectives of Social Bookmarking Sites. In 
Proceedings of the 2008 IEEE/WIC/ACM International 
Conference on Web Intelligence and Intelligent Agent 
Technology - Volume 01,2008 
[19] Y. Matsuo and H. Yamamoto. Community gravity: measuring 
bidirectional effects by trust and rating on online social 
networks. In Proceedings of the 18th international conference 
on World wide web,2009 
[20] M. G. Noll, C.-m. A. Yeung, N. Gibbins, C. Meinel, and N. 
Shadbolt. Telling experts from spammers: expertise ranking in 
folksonomies. In Proceedings of the 32nd international ACM 
SIGIR conference on Research and development in information 
retrieval,2009 
[21] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank 
Citation Ranking: Bringing Order to the Web. In 1999. 
[22] A. Penev and R. K. Wong. TagScore: Approximate Similarity 
Using Tag Synopses. In Proceedings of the 2008 
IEEE/WIC/ACM International Conference on Web Intelligence 
and Intelligent Agent Technology - Volume 01,2008 
[23] S. Sen, J. Vig, and J. Riedl. Tagommenders: connecting users to 
items through tags. In Proceedings of the 18th international 
conference on World wide web,2009 
[24] Y. Song, Z. Zhuang, H. Li, Q. Zhao, J. Li, W.-C. Lee, and C. L. 
Giles. Real-time automatic tag recommendation. In Proceedings 
of the 31st annual international ACM SIGIR conference on 
Research and development in information retrieval,2008 
[25] F. M. Suchanek, M. Vojnovic, and D. Gunawardena. Social tags: 
meaning and suggestions. In Proceeding of the 17th ACM 
conference on Information and knowledge management,2008 
[26] X. Wu, L. Zhang, and Y. Yu. Exploring social annotations for 
the semantic web. In Proceedings of the 15th international 
conference on World Wide Web,2006 
[27] G.-R. Xue, H.-J. Zeng, Z. Chen, Y. Yu, W.-Y. Ma, W. Xi, and 
W. Fan. Optimizing web search using web click-through data. 
In Proceedings of the thirteenth ACM international conference 
on Information and knowledge management,2004 
 
 
The 26th Annual Conference of the Japanese Society for Artificial Intelligence, 2012 
- 2 - 
content based search algorithm such as Okapi BM25 algorithm 
or language model [Zhai 2004] may not work well because of the 
short length of contents of questions. For example, we observed 
the answering questions of the user u405015 in Stack Overflow. 
This user has answered six questions as shown in Table 1 (See 
Appendix for contents of the answering questions). We can see 
from this table that the titles and the contents of these questions 
have few same words even if they are topically related. If we 
adopt traditional content-based methods to determine whether 
new questions are suited to users or not, it may not work in this 
example. 
 
Table 1: Titles of answering questions of User u405015 in Stack 
Overflow dataset. 
Questio
n id Title of question 
q6618485 CSS: Width 100% is not 100% of Screen 
q6614999 
Fixed inline-block div with negative margin-
right and shifting float: what's special about 
-4px? 
q6614886 
A span can be a div, but a div can't be a 
span 
q6600523 Internet Explorer Specific CSS Glitch 
q6598083 Table not resizing properly in IE7 
 
The QA-relation based methods utilize a special structure in 
cQA services, i.e., the question-reply relationship. Figure 2(a) 
shows the relation between users and questions while Figure 
2(b) shows the relation between users and users. Considering the 
problem of question routing, new questions are needed to route 
to people with high expertise. Relying on the content has proven 
to be limited to rank users’ expertise levels [Littlepage 1997], 
previous works [Campbell 2003, Dom 2003] have shown that 
using graph-based algorithms can be more effective than using 
content-based methods alone. There have some work adopt the 
graph-based ranking algorithms such as PageRank [Brin 1998] or 
HITS [Kleinberg 1999] algorithm to rank users [Jurczyk 2007, 
Zhang 2007] by their expertise. However, contents of questions 
are not considered in these algorithms. It is not appropriate to 
route questions of different topics to the same users with high 
authority values.  
Considering the example of question-reply graph illustrated in 
Figure 3, the labels of edges represent the topics of questions. If 
now we have one new question about “perl”, this question would 
be routed to User 4 according to the result by graph-based 
algorithms because of its higher in-degree. However, we should 
route this question to User 3 because the answering questions of 
User 3 are more similar to the topics of “perl” in the past. 
 
 
Figure 2: The QA relationship in cQA. 
 
 
Figure 3: An example of question-reply graph. 
 
Because of the shortcomings of directly applying the concepts 
of content and QA-relations, we cannot just apply these two 
basic methods to solve our problem of question routing. 
Previous works [Li 2010, Zhou 2009] of Question Routing 
(QR) in cQA services focus on the contents of questions that 
users answered in their answering history. Zhou et al.’s [Zhou 
2009] apply the three models to compute the user’s expertise, 
namely a profile-based, a thread-based, and a cluster-based 
model. The profile-based model considers the user’s answering 
questions as a group and the thread-based model treat user’s 
answering questions individually while the cluster-based model 
clusters the questions into groups. In Li’s work [Li 2010], 
questions are routed to the users with high expertise and 
availability. This work combines content-based approach with 
the work of answer quality issue in cQA services. Answering 
more questions similar to the new question with good quality, 
users are more likely to answer this new question. These two 
works do not consider the activity of users. If we route a question 
to a user who is inactive even though this user is able to answer 
this question, we cannot get new questions solved or get the 
answer efficiently. Hence, we should route new questions to the 
users not only with great expertise, but with high activity. The 
information for a specific user we have is less and the data we 
can use is only the past QA information of this user. Using these 
data to predict user’s activity is difficult because of irregularity 
and insufficiency of information. 
To address the question routing problem, we propose a 
framework to find the proper users for a given question to get 
new questions solved effectively and efficiently in cQA services. 
Based on the properties we mentioned in previous sections, we 
address the shortcomings of two baseline approaches by 
combining content-based approaches and QA-relation based 
approaches. Furthermore, we utilize the time characteristic of 
each user since user’s activity is also indispensable in the 
question routing problem. 
First, we compute the expertise score of users according to the 
contents of user’s answering questions and social network 
characteristic as illustrated in Figure 4. We then estimate the 
expertise score of each candidate user for a given question based 
on the contents of questions in answering history of this user. We 
also utilize the question-reply graph to enhance the expertise 
score of each user by peer-expertise dependency. We define the 
peer-expertise dependency as a mutual reinforcement 
relationship between the asking and the answering of users. It 
assumes that expert users of a question q are those users who 
have answered many other questions (denoted as Q) that are 
related to q, and these questions Q are asked by those users who 
have great authority values. Finally, we combine the content 
based approach and peer-expertise based approach to measure 
the overall expertise quantity of each user. 
The 26th Annual Conference of the Japanese Society for Artificial Intelligence, 2012 
- 4 - 
authority should be a popular hub. The scores of hubs and 
authorities are defined as follows: 
 
ܽݑݐ݄݋ሺ݌ሻ ൌ ෍ ݄ݑܾሺݍሻ
௤∈ூሺ௣ሻ
 ( 1 ) 
݄ݑܾሺ݌ሻ ൌ ෍ ܽݑݐ݄݋ሺݍሻ
௤∈ைሺ௣ሻ
 ( 2 ) 
 
where ܽݑݐ݄݋ሺ݌ሻ and ݄ݑܾሺ݌ሻ indicate the hub score and the 
authority score of page ݌ . ܫሺ݌ሻ  represents the set of pages 
pointing to page ݌; ܱሺ݌ሻ represents the set of pages pointed be 
page ݌.  
Such link-based algorithms are applied in a bipartite network 
where an asker is linked to an answerer when a question posted 
by the former has been answered by the later.  In another work 
[Jurczyk 2007], a similar approach is applied on the dataset 
crawled from Yahoo! Answers. From these proposals’ results, we 
can get a conclusion that there is a high correlation between link-
based metrics and user’s expertise. However, in our problem of 
question routing, if we return a user ranking based on the 
authority score, we always get the same user ranking. As will be 
shown in experimental results, we use PageRank as one of the 
baselines to compare. In our work, we consider the content of 
test question to get the user ranking more precisely. 
In other social media, Balog et al. propose generative 
probabilistic models to address the expert finding problem in 
enterprise corpora [Balog 2006]. Two general strategies to expert 
searching given a document collection are presented. The first 
one models an expert’s knowledge based on the documents that 
they are associated with while the second one finds experts by 
locating the topics of documents. From 2005, Text REtrieval 
Conference (TREC) has provided a platform with the Enterprise 
Search Track for researchers to empirically assess their methods 
for expert finding [Craswell 2005]. In addition, there are some 
researches for finding experts over the e-mail corpus such as 
[Campbell 2003, Dom 2003]. Graph-based ranking algorithms 
are applied to rank email correspondents according to their 
expertise on subjects of interest. In the graph, nodes represent the 
correspondents and edges mean the relation of email 
correspondence between nodes respectively. 
Language models have strong foundations in statistical theory 
and have performed quite well in many information retrieval 
tasks [Ponte 1998, Zhai 2004]. Typically language model 
approach can be divided into two types: profile-based and 
document-based methods. The profile-based approach (e.g., 
[Balog 2006]) estimates the probability of a candidate being an 
expert given the query topic by modeling the knowledge of an 
expert from associated documents. While in document-based 
methods, [Balog 2006] finds relevant documents for a given 
topic first. Then it ranks the candidates based on these documents. 
In our work, we use the language models to estimate the strength 
between users and questions. 
We describe other related work in this sub-section. The push 
mechanism can not only help improve the performance of cQA 
services, but fulfill user’s satisfaction and information need. Liu 
et al. [Liu 2008] also want to improve user’s satisfaction 
(especially askers) by solving the problem of predicting 
information seekers satisfaction in cQA services. A general 
prediction model is presented and they develop a variety of 
content, structure, and QA based features. 
There are lots of research studies about the analysis of cQA 
system. Adamic et al. [Adamic 2008] analyze the forum 
categories and cluster them based on content characteristics and 
patterns of user interaction to understand the knowledge sharing 
activity in Yahoo! Answers. The findings showed that different 
categories may have different characteristics. For example, 
interactions in users in some categories resemble expertise 
sharing forums while in other categories may represent the 
incorporate discussion, advice or support etc. Gyongyi et al. 
[Gyongyi 2007] analyzed 10 months worthy of Yahoo! Answers 
data to discuss the user behavior and impact in cQA portals.  
3. Method 
In this section, we present our proposed framework to address 
the problem of question routing to appropriate users in cQA 
services. Figure 6 shows the framework of our approach. Our 
framework consists of two components: the expertise model and 
the activity model. The expertise model ranks users according to 
their expertise calculated by content-based model and peer-
expertise model. The content-based model calculates the 
similarity between user’s answering questions and test questions 
as user’s expertise of test questions, and the peer-expertise model 
calculates user’s authority as user’s expertise through the 
question-reply relation in cQA services. The activity model 
analyzes user’s answering distribution and estimates the activity 
score of each user at the time of the routed question. The Linear 
Regression model combines the expertise index and activity 
index to be the user’s final score of each test question. We define 
the combined score as Expertivity score, donated as  
ܧݔ݌݁ݎݐ݅ݒ݅ݐݕሺݍ, ݑሻ.  
 
 
Figure 6: The proposed framework. 
3.1 Expertise model 
Given a test question ݍ, we determine the expertise score of a 
user ݑ by content-based model and peer-expertise model. In the 
content based model, we utilize the same approach as illustrated 
in [Li 2010]. We use this method to model user’s expertise 
according to the content similarity between the answering 
questions of users and test questions. If the test question is 
similar to the questions answered by the user in the past, he/her 
would have more probability and ability to answer this question. 
Hence we use content-based model to model the strength 
between users and questions. The other component of expertise 
model is peer-expertise model. We employ and adjust the 
traditional PageRank algorithm and run the modified algorithm 
on the weighted question-reply graph. Then we get the authority 
value of each user for each test question. 
 
QLL: Query Likelihood Language 
We use the query likelihood language (QLL) model as a 
similarity measure to weigh the strength of users and questions. 
For a new question ݍ to be routed, we define the score of user ݑ 
as equation ( 3 ).  
 
ࡽࡸࡸሺࢗ, ࢛ሻ ൌ ࡼሺࢗ|࢛ࢗሻ ( 3 ) 
ࡼሺࢗ|࢛ࢗሻ ൌෑࡼሺ࣓|࢛ࢗሻ
࢝∈ࢗ
 ( 4 ) 
The 26th Annual Conference of the Japanese Society for Artificial Intelligence, 2012 
- 6 - 
in the morning to this user. Since questions cannot be solved 
efficiently, we should also consider the hour activity of users.  
A real example of considering user’s activity is illustrated in 
Figure 8. As shown in this figure, there are two users and their 
answering records with respect to time. One point represents one 
question is answered by the user at the time. In this case, we use 
the total answering count to represent the expertise score of these 
two users. User u9567 have answered 207 questions in the past and 
u78845 have answered 166 respectively. Thus the expertise of u9567 
is higher than u78845. Now we have a new question q4572362 at time 
t1. We route this new question to u9567 rather than u78845 
according to the expertise score. However, we investigate the 
time series to these two users. We can easily get a conclusion 
that the activity of u78845 is higher than that of u9567 at time t1 
although u9567 have answered more questions. Therefore, we 
route the question q4572362 to the user u78845 instead of the user 
u9567. From this example, we consider user’s activity is also an 
important criterion in question routing.  
 
 
Figure 8: A real example of activity model. 
 
PA: Period activity 
In this sub-section, we describe the details of calculating the 
period activity. We consider the period activity of a user in two 
manners, i.e., query independent manner and query dependent 
manner. The former is denoted as ܲܣ଴ and the other is denoted as 
ܲܣ௤ . Query independent means user’s period activity is 
independent of the topic of the test question while question 
dependent assumes otherwise.  
We predict user’s activity in the testing region as the following 
steps. First, we do the discretization for the training region in 
advance. We assume that the length of the testing region is m 
days and the length of training region is n days. We split the 
training region into n/m bins with length m for each bin. In our 
experiment, we also set the length of bin as 4 days in ܲܣ଴ and 
ܲܣ௤ since our testing region is 4 days. Since our problem is to 
solve the problem of question routing, we only focus on the 
questions users answered and use the answering count of each 
user to represent their activity value. After the steps of 
discretization, we can get the first seven values of each bin for 
every test user. We use SVM [Chang 2001], to predict the value 
and the kernel type we used is the sigmoid function. We train and 
predict user’s activity by the regression model. We define this 
predicated score as ܲܣ଴. Users are ranked according to this score 
for each test question. 
Since the method ܲܣ଴  does not consider the information of 
test question, we adjust the method ܲܣ଴  to develop another 
method ܲܣ௤ . Instead of assuming each user having the same 
level of activity for different topics of questions, we consider 
question dependent to calculate user’s activity. Users have 
different levels of activity for different topics of questions. This 
assumption makes sense since users usually have diverse 
background knowledge and experience. 
When we count the answering questions for each bin, we only 
focus on the questions related to the test question. We calculate 
the similarity between the test question and the questions 
answered by candidate users. We scale the scale of similarity 
values into [0, 1]. 
 
DA: Daily activity 
As the same definition of period activity, we also treat daily 
activity in two manners: ܦܣ଴ as the daily activity independent of 
the topic of test question and ܦܣ௤  otherwise. Consider the 
example shown in Figure 9. The curve in the figure means the 
answering count versus hour of the user. In other words, a data 
point (x, y) means the user answered y questions in the hour x. we 
can see obvious difference between these two users in the figure. 
User u1 is active in the night or morning while user u2 is active at 
midday. Now a new question is posted at hour h1. Assume these 
two users having the same expertise on this question. Since the 
activity of u2 is higher than that of u1 at hour h1 according to the 
curves. We route the question q1 to the user u2. 
 
 
Figure 9: An example of considering hour activity. 
 
In order to develop our method, ܦܣ଴ and ܦܣ௤, we collect data 
from 2010/10/28 ~ 2010/10/31 from the website, Stack Overflow, 
to investigate the response time of each question. There are 6,160 
questions and 11,272 answers during this period. Most answer’s 
response time is usually short. On average, the time interval of 
answer and question is 2.26 hours. 
We calculate ܦܣ଴ and ܦܣ௤ as follows: For a given question q, 
supposed q is posted at hour h1, we calculate the score of the 
user’s hour activity score by counting the answering questions 
the user answered from hour h1 to h1+average response time in 
the past as ܦܣ଴ . Average response time means the average 
response time of all answers of all users in our training set. We 
also consider questions dependent with the hour activity. We 
only count the relevant questions to the test question to be the 
score ܦܣ௤.  
We then use a linear regression model to combine the 
expertise model and activity model. Hence, we rank users for 
each test question by the combined score ܧݔ݌݁ݎݐ݅ݒ݅ݐݕሺݍ, ݑሻ. 
 
4. Experiments 
In this section, we present experimental evaluation results to 
access the effectiveness of our system. In particular, we conduct 
experiments on the Stack Overflow (http://stackoverflow.com) 
cQA archive with over tens of thousands questions in our work. 
This dataset is public and can be downloaded from the Stack 
Overflow Blog (http://blog.stackoverflow.com/). The dataset we 
downloaded covers the duration from July 31, 2008 to December 
31, 2010 and what we used for our experiment is the snapshot 
from October 1, 2010 to October 31, 2010. These questions are 
limited in the “programming” domain. The dataset statistics are 
shown in Table 2. There are totally 71,000 questions and 
The 26th Annual Conference of the Japanese Society for Artificial Intelligence, 2012 
- 8 - 
more effective than content-based method in determining the 
expertise of users. According to the result of combination of 
PeER and QLL, we can get a better performance than using these 
two methods separately since the MRR value of QLL is 0.0999 
and that of PeER is 0.1222 while that of combination of these 
two methods is 0.1295.  
In our activity model, we examine of effect of question 
dependent in advance. PA୯  is better than PA଴  and DA୯  is better 
than DA଴. The results revealed that question dependent manner is 
important than question independent. We should consider the 
information when we calculate the activity of users. PA୯ is the 
most effective method. It uses the period activity of users by 
modeling the answering curve according to time. DA୯ is slightly 
worse than PA୯. Since PA୯ do not consider the user’s active time 
per day, we combine this method with DA୯. We combine these 
two methods to enhance the activity model. The MRR value of 
activity model is 0.1245, while that of PA୯ is 0.1176 and that of 
DA୯ is 0.1023 respectively. 
We use three methods to compare with our activity model, 
namely Average Response time, Entropy, and Inverse Entropy. 
We compare these three baselines with PA୯. We can see from 
Table 3 that PA୯ outperforms the other three baselines no matter 
what evaluation metric is used. The performance of Entropy 
outperforms Inverse Entropy. The results revealed that there are 
few phenomenon of busty event in user’s answering record 
versus time. 
Despite considering the test questions, we compare the 
methods RC, PR, PA଴. Among these methods, the performance 
of PA଴ is the best. We compare with RC and PA଴. RC uses the 
answering count scores to rank users while PA଴  models the 
answering count versus time. We can get a conclusion that some 
users are not always active on the website. We cannot use the 
total answering count to determine the score of users. We should 
consider the trend in user’s answering curve instead of just 
counting the total number of answering questions. 
As mentioned in introduction, we should consider the user’s 
expertise and activity simultaneously in the problem of question 
routing. In the combination part, expertise model is more 
effective than activity model. We compare our expertise model 
and activity model. The performance of expertise model 
outperforms the activity model. If we want to route a new 
question to users, user’s expertise is more important than user’s 
activity. Moreover, according to the result of combination of 
expertise model and activity model, we get the best system 
performance when the weights of these two models equal 0.5. 
These two models are equivalently important in our problem. 
Our final system performance yields the best performance 
comparing the other methods. The best MRR value is 0.1372.  
 
Table 3 : Overall comparison of all methods in GD1 
Method MAP MRR R-Precision P@1 P@3 
RC 0.0219 0.0331 0.0108 0.0122 0.0094 
PR 0.0166 0.0252 0.0059 0.0081 0.0053 
ART 0.0018 0.0026 0.0003 0.0003 0.0002 
Entropy 0.0160 0.0236 0.0078 0.0088 0.0056 
Inverse 
Entropy 0.0018 0.0024 0.0004 0.0002 0.0003 
QLL 0.0731 0.0999 0.0457 0.0497 0.037 
PeER 0.0891 0.1222 0.0633 0.0732 0.0449 
PA଴ 0.0223 0.0333 0.0107 0.0122 0.0096 
PA୯ 0.0828 0.1176 0.0582 0.0708 0.0426 
DA଴ 0.0223 0.034 0.0108 0.0101 0.0108 
DA୯ 0.0706 0.1023 0.0494 0.0596 0.0361 
Expertise 
Model 0.0955 0.1295 0.0693 0.0810 0.0463 
Activity 
Model 0.0874 0.1245 0.0629 0.0755 0.0451 
Expertivity୐ 0.0997 0.1372 0.0734 0.0857 0.0494 
5. Conclusion 
In this paper, we address the problem of question routing over 
a community QA portal, Stack Overflow. We show that not only 
user’s expertise but user’s activity is also an important criteria for 
question routing. Based on our proposed framework, we have 
introduced several methods to model user’s expertise and activity. 
Expertise model includes QLL and PeER; QLL uses content-
based method while PeER uses peer-expertise model. Activity 
model contains PA୯  and DA୯; PA୯  uses the answering curve of 
each user while DA୯  uses daily active time of users to model 
user’s activity.  
Our experiments on a collection of Stack Overflow cQA portal 
have shown that our proposed methods outperform the baseline 
methods and enjoy a better performance. Our proposed method 
has the best performance in several measurements. Moreover, 
our method combined by expertise model and activity model is 
also better than the method using training technique. The best 
MRR value in our experiments is 0.1367. It means that on 
average each test question will get at least one answer if we route 
the test question to the top 7 ranked users. Considering the total 
users to be ranked, our system demonstrates that our proposed 
framework is able to route new questions to users. New questions 
can be solved effectively and efficiently. 
 
References 
[Adamic 2008] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. 
Ackerman, "Knowledge sharing and yahoo answers: everyone 
knows something," presented at the Proceeding of the 17th 
international conference on World Wide Web, Beijing, China, 
2008. 
[Agichtein 2008] E. Agichtein, C. Castillo, D. Donato, A. Gionis, 
and G. Mishne, "Finding high-quality content in social media," 
presented at the Proceedings of the international conference on 
Web search and web data mining, Palo Alto, California, USA, 
2008. 
[Balog 2006] K. Balog, L. Azzopardi, and M. d. Rijke, "Formal 
models for expert finding in enterprise corpora," presented at 
the Proceedings of the 29th annual international ACM SIGIR 
conference on Research and development in information 
retrieval, Seattle, Washington, USA, 2006. 
[Brin 1998] S. Brin and L. Page, "The anatomy of a large-scale 
hypertextual Web search engine," presented at the Proceedings 
of the seventh international conference on World Wide Web 7, 
Brisbane, Australia, 1998. 
[Campbell 2003] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. 
Dom, "Expertise identification using email communications," 
presented at the Proceedings of the twelfth international 
conference on Information and knowledge management, New 
Orleans, LA, USA, 2003. 
[Cao 2008] Y. Cao, H. Duan, C.-Y. Lin, Y. Yu, and H.-W. Hon, 
"Recommending questions using the mdl-based tree cut 
model," presented at the Proceeding of the 17th international 
conference on World Wide Web, Beijing, China, 2008. 
[Chang 2001] C.-C. Chang and C.-J. Lin, "LIBSVM : a library 
for support vector machines, Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm," 2001. 
[Craswell 2005] N. Craswell, A. P. d. Vries, and I. Soboroff, 
"Overview of the TREC-2005 Enterprise Track," presented at 
the In Proceedings of TREC 2005, 2005. 
The 26th Annual Conference of the Japanese Society for Artificial Intelligence, 2012 
- 10 - 
APPENDIX 
Table 4 : Contents of Answering questions of User u405015 
in Stack Overflow dataset 
Question id Content of question 
q6618485 
q6614999 
q6614886 
q6600523 
q6598083 
 
參與 The 26th Annual Conference of the Japanese Society for Artificial 
Intelligence, 2012 報告 
高宏宇副教授 
成功大學資訊工程學系 
 
(一) 參加會議經過 
2012 日本人工智慧學會年會(JSAI-2012)於六月十二日在日本山口縣(Yamaguchi city, Japan)
舉辦，此會議是由日本人工智慧學會(JSAI: Japanese Society for Artificial Intelligence)每年舉
辦的學會年會。日本人工智慧學會擁有三千多名會員，此次參與年會的人數也接近千人，是
個相當大的學會年會。年會中發表的論文主要以日文為主，今年也是日本人工智慧學會第一
次有國際會議場次，收錄以英語撰寫的國際論文，藉此可增加日本學會與國外學者的交流。
此次會議中與多位在日本的人工智慧專家做經驗交流，彼此分享在人工智慧與網際智慧之研
究心得，實為一難得之機會。並藉由此次交流，加強了台灣人工智慧學會與日本人工智慧學
會的合作關係，並為今年在台舉辦的 TAAI 2012 開啟了與國際學者合作之路。 
(二) 與會心得 
此次會議議題相當廣泛，大家參與會議時討論
亦非常熱烈，大會安排多場海報式發表，使得
大家可以針對自己興趣之主題充分與發表者深
入討論，因此收獲相當多。經由這次會議的參
與，不但得以認識一些相關領域之學者，互相
交換研究心得，而且可吸收最新資訊，對日後
將原本領域之技術應用在資料探勘和不同領域
之研究將有所助益。筆者非常感謝國科會的補
助，才能夠參加此次國際性會議，藉由此次之
參與將對未來之研究之領域擴展有莫大之助
益。 
 
(三) 參觀活動 
1Hung-Yu Kao
寄件者: Ken-ichi Fukui [fukui-jsai2012@ai.sanken.osaka-u.ac.jp]
寄件日期: 2012年3月18日星期日 下午 1:48
收件者: danushka@iba.t.u-tokyo.ac.jp
主旨: JSAI2012: NOTIFICATION OF ACCEPTANCE IOS-3
郵件標幟: 待處理
標幟狀態: 已完成
Dear Authors, 
(This e‐mail is sent by Bcc) 
 
We are delighted to inform you that your abstract submitted to the Special Session on Web Intelligence 
and Data Mining (IOS‐3) has been accepted for oral presentation. 
 
The next step will be to submit your paper (not exceeding 10 pages in JSAI 2012 format) during the 
submission period from 6th of April to 18th of April, 2012 via the online submission site. 
 
Author guidelines and style files are provided here. 
http://www.ai‐gakkai.or.jp/conf/2012/?page_id=110 
 
If you have any questions please feel free to contact me. 
 
Looking forward to seeing you at JSAI 2012! 
 
Best regards, 
 
Danushka Bollegala 
danushka@iba.t.u‐tokyo.ac.jp 
IOS‐3 organizing committee. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/31
國科會補助計畫
計畫名稱: 有號社群網路中具情緒感知之意見領袖與影響力探勘研究
計畫主持人: 高宏宇
計畫編號: 100-2221-E-006-262- 學門領域: WEB 技術
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
受邀至日本人工智能協會(JSAI)發表論文 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
