 第 2 頁 / 共 50 頁 
網路虛擬化架構之研究與建置(1/2) 
子計畫三: 設計與實作一具有可調式服務品質保證之網路虛
擬化架構(1/2) 
計畫編號：NSC 98 – 2219 – E – 011 – 003 
執行期間：98 年 8 月 1 日至 99 年 7 月 31 日 
計畫主持人：陳俊良  國立台灣科技大學電機工程學系 
計畫參與人員：曾毓元、李燕芳、周慶賢、張騰文、周鳳儀 
 
一、 中文摘要 
由於現有的網路流量分析系統，大多採用系統模
擬或是以 Benchmark 的方式來評估效能，但以目前網
路複雜度與應用多元性來看，模擬結果之真實性值得
商榷。因此，若測詴系統為真實的網路設備，其資料
分析結果勢必能更貼近真實網路架構，此種方式將有
助於萃取出真實世界的網路特性，並提出各種演算法
在此帄台上，驗證其效能與測詴其實用性。有鑑於此，
本研究利用 NetFPGA 硬體帄台提供抓取真實封包的
能力，搭配史丹佛大學提出的 Openflow Project 提供
完備的網路控制能力，以建構完整的開發環境。本計
畫設計與實作一具有可調式服務品質保證之網路虛擬
化架構，並提出一個集中式的網路虛擬化架構，俾使
網路管理功能更臻完善。 
本計畫目標在網路虛擬化架構中提出一可調式
的網路服務品質確保機制。此機制架構包括三個
Plane：Data Plane、Control Plane 及 Management 
Plane。Data Plane 的功能在於利用 Traffic Monitor 和
Bandwidth Monitor 模組監控目前的網路狀態，並將各
種封包所屬的服務種類寫入 Record Table 中，透過
Monitor Agent 將相關的 QoS 相關資訊提供給
Management Plane，以提供演算法機制所需要的參數
資訊。而 Control Plane 的主要功能為根據目前的網路
狀態，透過 Management Plane 傳遞決策資訊以提供
Admission Control、Traffic Control、Traffic Engineering
和 Congestion Control 的功能。而在 Management Plane
中，定義了完整的管理流程，以提供網路管理者一個
友善的介面控管網路。這部分定義了四個子模組，提
供子計畫四存放 policy 的 Policy Pool；提供網路管理
者一個友善介面的 Web-based Management、提供網路
可適性調節的 Adaptive QoS Strategy、於此架構中執行
policy 解析與啟動 Adaptive QoS Strategy 的 QoS 
Controller。本研究的設計架構基於總計畫的研究架構
演變而成，並搭配其他子計畫的資訊交流以提供實際
的網路控管。 
本計畫結合各子計畫建置的 NetFPGA 測詴環
境，自 Switch/Router 將各種封包篩選過濾後，並標記
所屬的應用類型，再經由 Monitor Agent 模組彙整
Traffic Monitor 模組和 Bandwidth Monitor 模組所提供
的資訊建成一個 Record Table，使本計畫設計的管理
機制能夠獲得相關 QoS 參數。並藉由可調式的 QoS
處理策略，進行網路流量控制與調整。本階段(第一期
計畫)已完成 QoS Engine 基本架構及模組之定義，並
完成 Data Plane 和 Control Plane 的功能模組設計，將
發展相關的 QoS 演算機制，能夠針對龐大的流量資訊
進行統計、分析的工作，及如何經由合理的簡化並記
錄重要的網路流量狀況，以供長期分析之用。而網管
系統將可利用此帄台來研究可行的 Policy，並在此帄
台上驗證且分析各種策略的優劣，而分析的結果亦可
以提供網路設備開發商在設計新一代的網路設備時，
多一項驗證及參考的依據。 
關鍵詞：NetFPGA、網路虛擬化架構、網路服務品質
保證機制、RSVP 協定 
 第 4 頁 / 共 50 頁 
尤其是當網路流量過大時，該如何辨識才能增加封包
過濾的效能，是為重要之議題。 
本計畫採用 NetFPGA 硬體開發帄台，為美國史丹
佛大學所提出的一個可程式化的網路硬體帄台。由
NetFPGA 設計實做的 Router/Switch 具有限速的封包
處理能力且擁有可重新組態設定的特性，使得以
NetFPGA 實作的 Router/Switch 具有延展性。除此之
外，於研究中利用 OpenFlow 的設計概念，在原本的
網路上，利用 NetFPGA 擴充一個實驗網路，透過其虛
擬化的特性，提供網路協定一個驗證的帄台，可以在
不影響網路環境的前提下實驗新的協定及應用。 
 圖 2.1 為網路虛擬化架構之研究與建置圖，將組
成網路的基本網路處理設備，分成硬體的 Data Plane、
處理控制信號與機制的 Control Plane、負責網路管理
與策略指導的 Management Plane、以及應用層的指定
應用程式。由子計畫一提供封包的過濾，並分類標記
其不同的應用服務；子計畫二提供封包的行為分析，
並將分析資料建成一個資料庫；子計畫三執行 QoS 的
機制；子計畫四根據目前的網路狀況，提供適合的政
策給子三參考；子計畫五提供網路的安全性，以避免
攻擊。 
 
圖 2.1 網路虛擬化架構之研究與建置圖 
本子計畫三主要負責設計與實作一具有可調式
服務品質保證之網路虛擬化架構，圖 2.2 為本子計畫
三提出的系統架構。此系統架構分成 Data Plane、
Control Plane 和 Management Plane。在 Data Plane 中
利用 Monitor Agent 監測封包和流量並儲存 QoS 機制
所需的相關資訊；在 Control Plane 上透過決策資訊，
將目前接收的流量做 Traffic 和 Bandwidth 控管；在
Management Plane 中提供 QoS 決策資訊，並透過 QoS 
Controller 執行並解析 Policy Pool 中的策略。 
 
圖 2.2 系統架構 
在此架構下，系統的模擬情境分為使用者端及管
理者端。圖 2.3 為使用者端的情境圖，透過 NetFPGA
建置的 Router/Switch 將他的網路環境，建置在一般的
網路中，並利用 Arbiter 根據每個 Router/Switch 收集
的封包資訊提供相對應的策略以動態調配目前的網路
環境，以管理每個使用者的網路服務品質。在建置的
環境中將實驗透過乙太網路連線的使用者其在使用各
種應用服務時，其系統提供的 QoS 策略能否根據網路
狀況動態的調配以避免網路服務品質降低的影響。 
 第 6 頁 / 共 50 頁 
 Zero-bus turnaround (ZBT), synchronous 
with the logic 
 Two parallel banks of 18 MBit (2.25 MByte) 
ZBT memories 
 Total capacity: 4.5 MBytes 
 Cypress: CY7C1370D-167AXC 
 Double-Date Rate Random Access Memory   
 400MHz Asynchronous clock 
 25.6 Gbps peak memory throughput 
 Total capacity: 64 MBytes 
 Micron: MT47H16M16BG-5E 
 Multi-gigabit I/O x 2 
 Standard PCI Form Factor 
 Standard PCI card 
 Can be used in a PCI-X slot 
 
圖 3.1 NetFPGA 硬體開發帄台 
透過 PCI bus(不使用 JTAG Cable)，此硬體裝置
可加速 FPGA 之重新配置；而其本身亦提供 CPU 存取
NetFPGA 硬體上的 Memory-mapped 暫存器和記憶
體。在圖 3.1 右邊區域，存在著兩個 SATA 連接器，
此硬體裝置允許一個系統中存在多個 NetFPGA；因
此，不需經由 PCI Bus 也可達到高速交換資料之功
能。圖 3.2 顯示了 NetFPGA 詳細的區塊圖[6]。 
 
圖 3.2 NetFPGA 區塊圖 
 
Gateware 
  係以可模組化及易擴張之特性設計而成；以模組
化的方式設計 Stage，並將其連結在一 Pipeline 裡，當
有新的 Stage 要加入此 Pipeline 也相對的比較簡單，如
圖 3.3 顯示了模組化 Pipeline 架構[7]，圖中包含橫跨
邏輯的 Packet Bus 和 Register Bus。在硬體模組和軟體
間，當 Register Bus 被用來傳達狀態和控制資訊時，
也會同時啟用高頻寬 Packet Bus 功能模組來處理網路
封包。 
 
圖 3.3 NetFPGA 模組 Pipeline 架構 
 NetFPGA 官方網站釋出兩種基於 NetFPGA 上的
Reference Design：IPv4 Router 和 4-port NIC。所有的
Reference Design 皆基於圖 3.4 所示的 Reference 
Pipeline 所設計而成[6]。使用 Reference Pipeline 擷取
網路交換機主要的封包處理模組；其模組原理係：首
先，Rx Queues 由 I/O port 取得封包，接著 Input Arbiter
 第 8 頁 / 共 50 頁 
3.2 OpenFlow 
 OpenFlow 係基於乙太網路交換器 (Ethernet 
Switch)，其具有內部之 Flow-Table 以及可在實際網路
環境中新增或移除 Flow Entries 之標準化介面；
OpenFlow Switching Consortium係於 2008年由史丹佛
大學所發表，並支援 OpenFlow 相關功能[9]。其目在
於吸引網路供應商加入 OpenFlow 到交換器產品中，
以建置骨幹網路(Backbones)和配線室(Wiring Closets)
於大學校園。OpenFlow 的系統包含兩個角色，一個是
監控與管理整個網路環境，稱為 NOX Controller；另
一個是建立一個網路拓撲(Topology)，稱為 OpenFlow 
Switch[10]。 
3.2.1 OpenFlow Swich 
 OpenFlow Switch 包含一個負責查詢和傳送封包
的 Flow Table，以及一條與外部 Controller 連接之安全
路徑。圖 3.7 為 OpenFlow Swich 透過根據 OpenFlow
傳輸協定建立之安全路徑與 Controller 溝通[11]，運作
方式係以包含 Flow-Tables(大多係由 TCAMs 建立)的
乙太網路交換器和路由器，以 Line-Rate 的速度實現防
火牆、NAT、QoS 以及收集統計等應用，即使廠商設
計方式不同，仍舊可以辨識運作在許多交換器和路由
器基本功能。常見的傳送封包方式係透過 Switching(屬
於 OSI七層裡屬於第二層)和 Routing(屬於 OSI七層裡
屬於第二層)進行傳遞，OpenFlow 傳送封包方法也採
取上述之方式[12]。 
 
圖 3.7 OpenFlow switch 與 controller 溝通方式 
Layer-2 switching 
 乙太網路 Switching 在 OSI 架構中的資料連結層
(Data Link Layer)執行，負責控制資料流、處理傳輸錯
誤、提供實體位址和管理實體媒介存取。 
Layer-3 routing 
 IP Routing 主要是在 OSI 架構中的網路層執行，
工作是負責將封包由來源端傳送到目的端(包含中間
經過的主機)，而資料連結層則是負責在相同連結上的
點對點(P2P)傳送。此部分可透過 Secure Channel 的通
訊協定，讓 Controller 與 Switch 相互溝通，並傳送各
種指令和封包資訊。OpenFlow Switch 的資料路徑由
Flow Table 組成(包含一個 Action 與許多 Flow Entry連
結)。Flow table 包含 Flow Entries、活動計數器(Activity 
Counter)以及用來進行封包匹配之 Action，Action 包括
傳送封包到指定的 Port，或丟棄指定應用程式之封包
或者若找不到相匹配的封包則會透過 Security 
Channel 傳送到 Controller，由 Controller 負責決定如
何處理無效 Flow Entries 封包，並藉由加入或移除
Flow Entries，進行 Switch Flow Table 之管理。Flow 
Table Entries 的元件，包含 Header Fields、Counters 以
及 Actions，如表 3.1 所示，其中 Header Fields 之初始
值預設為”Type0”。 
表 3.1 A flow entry 
Header Fields Counters Actions 
OpenFlow Switch 由 10 個參數位元所組成，如表
3.2 所示。一個 TCP flow 可以藉由十個 Filed 詳細說
明，然而一個 IP Flow 在定義中，可能不包括傳輸埠。
每個 Header Field 可以彙整各種 Flow。例如，在特定
VLAN，Flows 只有在 VLAN ID 在有被定義的情況下
才適用於所有 Traffic 中。 
表 3.2 Header Field List 
 
 第 10 頁 / 共 50 頁 
多個交換器的傳送表(Forwarding Table)[13-14]，NOX 
Controller 可以加入一個多變的控制程序於一個或多
個 Network-Attached 伺服器中，使得網路控制功能更
加完善，NOX 網路拓撲情形可由圖 3.8 所觀察出，亦
可以透過多種不同的應用來決定網路管理機制。 
 
圖 3.8 NOX 網路的組成元件[13] 
Granularity 
Granularity Size，決定 NOX 的延伸性及適應性，
由觀察的角度來看，圖 3.8 表示其網路拓撲情形[13]，
包含 Switch 拓撲、提供服務之情形、所有名稱和各位
址之間 Bindings 的各種資訊。值得注意的是，其中並
不包括目前網路 Traffic 的狀態。透過觀察 Granularity
的選擇提供資訊給網路管理者決策雖可以運用在大型
網路當中，但其缺點在於其運作速度相當緩慢‧控制
Granularity 辨識實際的網路請求，其過程相當複雜，
NOX 會挑選中間值的 Granularity 及 Flows[14]。簡而
言之，透過處理過的控制封包，並將封包標頭後的
Flow 所提供相關資訊作為網路管理決策之依據，本計
畫透過 Flow-Based Granularity 的使用，建立一具有高
彈性特質的網路管理系統於大型網路當中[17]。 
Switch Abstraction 
NOX Controller 必須在 Switch 的命令下才得以控
制網路之 Traffic：Switch 指令獨立於特定的 Switch 硬
體上，支援 Flow-Level 控制 Granularity，為了達到以
上需求，NOX 採用 OpenFlow Switch 概念，在
OpenFlow 中，表 3.1 表示 Switches 的 Flow Table 之功
用，在每個標頭的封包，都會包含更新後之 Specified 
Action 並加入計數器於封包中，封包的編號將決定其
優先權之高低，一個 Entry 的標頭檔包含 Values 或
ANYs，提供 TCAM-like 以符合 Flow 之規範。 
3.3 服務品質(QoS) 
服務品質(QoS)，常被用於電腦網路或其他封包
交換網路之網路控制機制。QoS 的定義可在四個不同
的組織找到，包括 IETF(Internet Engineering Task 
Force) 、 ITU-T(International Telecommunication 
Union)、ETSI(European Telecommunications Standards 
Institute) 和 ISO(International Organization for 
Standardization)。 
IETF 於 RFC 和 Draft 中皆有提及 QoS 的相關概
念；QoS 及其效能參數主要是由封包遺失率、延遲時
間 (Latency) 、 延 遲 變 化 的 程 度 (Jitter) 和 吞 吐 量
(Throughput)等相關元件所組成。ITU 為建立電信標準
協定所成立之組織，著重於網路操作者的觀點來定義
QoS，遵循 SLA(Service Level Agreements)所訂立之標
準，在使用者和 ISPs(Internet Service Provider)間與
ISPs 和 NPs (Network Provider)間標記，同時亦考慮網
路層及應用層 QoS 之規範；ETSI 也是一個電信發展
學會，所提出 QoS 觀點，大致與 ITU 相同；常見的
QoS 參數如下所述： 
Bandwidth 
 測量在數位網路的資料流量率，通常是每秒多少
Bits。 
Latency or Delay 
 一個封包由來源端到目的端所需時間。 
 延遲的原因可能是因為封包困在冗長的佇列中
或必須採用較長的路由(Routing)以避免壅塞。 
Jitter or Variation in delay 
 連續封包造成的延遲變化(Variation in Delay)。 
 Jitter 所影響地聲音和影像串流服務。 
同步性的聲音與影像串流服務 
Packet Loss Ratio 
 處理即時 Traffic，如多媒體和聲音串流時，封包
遺失率將會影響服務之品質。 
 第 12 頁 / 共 50 頁 
存 Traffic Flow 資訊和控制 Flow。 
表 3.6 DiffServ PHBs 
 
3.4 特徵識別(Feature Recognition) 
為了提供較佳的網路管理及增進網路傳輸的品
質，因此良好的封包特徵識別是當前重要之議題，目
前較為常見的特徵識別方法 Signature-Based 、
Behavior-Based 和混合型封包特徵識別。 
3.4.1 Signature-based 封包特徵識別 
由於多數Web 中Payload的封包傳輸時間會被標
示特殊字串，因此，若能夠辨識網路應用的封包特徵，
便可以避免特定的網路 Traffic 以維持服務品質。例
如，eDonkey 封包 Payload 必須是 eDonkey 標識的開
頭，其值為 0xe3，4Byte 為封包內容的長度；KaZaA
封包 Payload 開頭為 GET 或 HTTP，並包含 X-KaZaA
字串，其他相似封包識別方式也如上所述[20][21][22]。 
相較於傳統 Port-Based 辨識方法，有較高的正確
率。然而，由於標頭(Header)長度通常只有 20Bytes，
但在乙太網路 Payload 却高達 1500Bytes，造成辨識時
間過長及系統資源浪費，在[23]研究中指出，95%的第
一個 Payload 占 400 Bytes，因此調整最大 Byte 數與檢
查的長度相同，儘管會降低正確性，但卻可以提升整
體字串匹配效能；然經過加密處理 Payload。如 Skype，
便無法辨識。 
3.4.2 Behavior-based 封包特徵識別 
常應用於 P2P 軟體，例如，Bit-Torrent、eMule
或 Skype，為了避免使用傳統單一封包 Payload 方法辨
識 P2P 封包或者封包資料於傳輸時被竊取，因此大部
分軟體本身提供加密傳輸，其加密方式可分為傳輸前
加密或者傳輸後加密兩種，使得傳統資訊過濾方式
(Signature-Based Filter)於無法應用於 P2P 傳輸上。此
外，Signature-Based 尚有其他缺失，字串在配對時，
並無事先定義或軟體更新版本修改字串的特徵，Filter
將無法辨識封包的類型，所以 Thomas Karagiannis 在
[24] [25] [26]提出一個新的識別方法，.透過觀察使用
者連結行為以識別應用層所產生封包類型。 
此種辨識方法分為以下步驟；首先判定第一個封
包的連結是否曾經被識別過，若是，則視為相同類型
的封包，否則便利用 Behavior-Based 封包特徵識別方
法進行辨識。其次是將 Behavior-based 封包特徵識別
分為三個級別：Social Level、Functional Level 和
Application Llevel，由已知服務使用者來找到其他使用
相同服務的使用者，亦為 P2P 的運作方式。 
圖 3.11 顯示[25]實驗結果，觀察後可發現使用
Web 瀏覽網頁的數量比使用 P2P 連結使用者少很多，
這是因為 P2P 需要與許多點對點交換資訊。因此透過
此優點與 Social Level 想法結合，只需耗費伺服器的部
份資源，分析正在使用該服務的使用者，省下許多識
別消耗的時間。 
 
圖 3.11 使用者連結到 web 和 P2P 數量圖[25] 
Function Level 是由功能角度來分析使用中的服
 第 14 頁 / 共 50 頁 
NF_IP_FORWARD 、 NF_IP_LOCAL_OUT 和
NF_IP_POST_ROUTING，此種設計方式避免使用者
修改作業系統核心時，避免對系統造成的破壞。 
 
圖 3.14 五個 hook 位置的 Netfilter 封包資料流 
圖 3.15 為 Netfilter 的架構，主要由兩個元件所組
成，第一個部份是在 Linux 核心中進行封包擷取和處
理，而第二個部份利用軟體控制 iptable 設定過濾規
則，由於 iptable 使用 Netlink socket[33]與 Linux 核心
進行連結，並將過濾規則透過 Netlink 傳送到
Netfilter，作為這些過濾封包標準原則的基礎，在封包
穿過 Linux 核心的 IP 層時，檢查封包是否需要轉換，
准 許 此 封 包 由 PRE_ROUTING 進 入 ， 並 透 過
POST_FORWARD 和 POST_ROUTING 進行連續性地
傳送，如果封包是由本土主機(Local Host)進入，則會
藉由 INPUT 處理並傳送到本土主機，透過 OUTPUT
和 POST_ROUTING 進行連續地傳送，由於 Netfilter
架構存在於 Linux 核心架構，所以封包的內容不需透
過 User Space 進行處理，使封包過濾可以有較好的處
理效能。 
 
圖 3.15  Netfilter 架構 
3.5.2 IPP2P 
IPP2P 用來識別 P2P 封包，據官方網站所提供的
資訊，IPP2P 可支援九種不同的 P2P 軟體，如表 3.7
所示，為使 IPP2P 能夠符合每個 P2P 應用封包資訊，
因此將封包資訊寫入於各種獨立的功能當中；如 UDP 
packets 寫 入 於 BitTorrent 中 ， 稱 之 為
udp_search_bit( )。 因此，若出現新式 P2P 應用，只
需透過軟體修改初始的文件，即可匹配新增之功能。 
表 3.7 IPP2P 支援 P2P 協定及通訊協定[27] 
P2P protocols 
Communications 
protocols 
eMule, Kademlia, 
eDonkey 
TCP and UDP 
KaZaA, FastTrack TCP and UDP 
Gnutella TCP and UDP 
Direct Connect TCP only 
BitTorrent, extended BT TCP and UDP 
AppleJuice TCP only 
WinMX TCP only 
SoulSeek TCP only 
Ares, AresLite TCP only 
3.5.3 Layer7 
 第 16 頁 / 共 50 頁 
 
圖 3.18 傳統路由器(router)的難處 
Traffic Engineer (TE)為 Traffic Flow 選擇路徑的
過程，TE 帄衡了網路中不同連結路徑、路由器、交換
器中的 Traffic Flow；其目的是取得兩個節點間的路由
器，使此路由(Routing)不違背它的規範。由於 MPLS
具 Self-Routing 和個別地傳送工作之特性，MPLS 可用
來與 TE 組合成 MPLS-TE[32]技術，使用 MPLS-TE
可以提升網路 QoS，主要原因為： 
1. 結由帄衡多個傳送路徑的負載，使 MPLS-TE 可
以提升 QoS 且避免網路壅塞情形發生。 
2. MPLS-TE可以透過RSVP-TE信號建立一個謹慎
的 QoS 及頻寬保證的通道。 
3. 透過備用的 LSP 和 FRR(Fast Reroute)，除了避免
通道擁塞也能相對提升 QoS。 
然而，MPLS-TE 也有許多限制，如下所述： 
1. MPLS-TE 必須用於 MPLS 網路，但現今許多網
路並不支援 MPLS 網路架構，造成無法支援
MPLS-TE 情形發生。 
2. 跨領域的 MPLS-TE 應用仍在發展當中，目前的
MPLS-TE 只能在特定領域中執行。 
3. 雖然 MPLS-TE 可以建立一個頻寬保證的通道，
如果使用者同時透過通道傳送多種 traffic，可能
會產生單獨地處理不同優先權的 Traffic 問題發
生。 
3.6.3 Ethane 
本計畫所使用的OpenFlow及NOX皆繼承Ethane
之優點。Ethane[34]是一個新的企業網路架構，允許管
理者自行定義單一網路、並可直接運作執行；Ethane
具備直接連結中央 Controller 之路徑、Flow-Based 的
乙太網路交換器以及管理 Entry 和 Flow 的路由
(Routing)之特性，提供開發者發展 OpenFlow 的參考
方向。OpenFlow 的設計概概念與 Ethane 大致相同，
由中央的 Controller 進行網路控管並決定所有封包的
運作情形[35]。因 Controller 可藉由目前網路拓撲情形
並提供合法 Flow 進行存取及路由計算。第二個元件
為 Switches，具備簡易及高相容之特性，由單一的 Flow 
Table 及 Controller 的安全通道所構成，Switches 依據
Controller 的指示傳送封包，此方式與 OpenFlow 概念
相同，若接收到不在 Flow Table 中的封包時，會將此
封包傳回給 Controller 進行處理；若接受在 Flow Table
中的封包時，則會根據 Controller 的指令進行相對應
之動作。Ethane 網路對於 Controller 與 Switches 之間
互動有完整的定義，圖 3.19 說明在 Ethane 網路上通訊
的五種行為。 
 
圖 3.19 Ethane 網路溝通實例[34] 
Registration 
所有的主機、使用者和交換器皆須事先與
Controller 進行註冊，並產生 Controller 的公開金鑰；
註冊完成後，主機可藉由 MAC 位址進行認證、使用
者可藉由使用者名稱和密碼進行認證，交換器可藉由
安全證明進行認證。 
Bootstrapping 
Switches Bootstrap 藉由 Controller 產生樹狀頂點
(Root)，使底層各個支點可互相連結，Controller 與交
換器間存在著安全通道(Secure Channel)，交換器藉由
 第 18 頁 / 共 50 頁 
四、 研究方法 
以下介紹本研究所提出的相關模組設計以及可調
式的 QoS 機制演算法。 
4.1 可調式 QoS 架構 
本研究建構一個 OpenFlow-Based NetFPGA 網
路管理系統，並在此系統上提出一個服務品質保證機
制。此機制會根據網路狀況自動的調節，以維持網路
的服務品質。除此之外，此機制亦定義了一個可調式
的服務品質策略，根據網路的狀況計算最佳的路徑以
決定資料流的路由。本研究主要是利用史丹佛大學所
提出的 NetFPGA 硬體開發帄台搭配 OpenFlow模組實
現 Diffserv QoS 機制的一個可調式系統。 
圖 4.1 為 OpenFlow-Based NetFPGA 網路管理系
統的運作架構圖。當有封包進入系統時，每一個封包
都會經由封包篩選過濾器把所屬的應用類別分類、標
記並建成一個 Record Table。而 Record Table 的資訊將
會提供給 QoS Engine 一個決策的參考依據。當 QoS 
Engine 在下了決策之後就會透過 Flow Table 的規則訂
定下一個封包的路由及動作以符合使用者的需求。當
所有動作都訂定完成之後，就會將資訊傳達給 Agent 
Controller 以實行對應的要求，舉凡 Admission Control, 
Traffic Control 等等的功能模組外，更可以執行相關的
QoS 功能以符合使用者要求的網路服務品質。 
 
圖 4.1 系統運作架構圖 
演算法的部分將會定義在 QoS Engine 的
Adaptive QoS Strategy 模組，此演算法將會利用 Traffic 
Engineering 模組和 congestion control 模組實現。其目
的主要是希望可以避免網路壅塞情況的發生並提供較
佳的網路傳輸路徑。本研究預計會測詴四種網路應用
服務的類型： Web Browsing、 Voice、 MPEG Video
和 FTP download。下表 4.1 為根據 QoS 規格書所分類
的四種流量型別及各服務所需的要求。 
表 4.1 QoS 分類等級 
 Traffic Classes 
QoS 
Concern 
Typical 
Application 
#1 
Interactive 
Class 
Reliable Web Browsing 
#2 VoIP 
Delay and 
Jitter 
Voice 
#3 
Video/Audio 
Streaming 
Packet Loss MPEG Video 
#4 Best Effort Throughput FTP download 
本研究提出一個 QoS Engine 的機制架構，其中
包含了 Data Plane, Control Plane 和 Management 
Plane，如下圖 4.2 所示。本子計畫實現 Data Plane 的
Monitor Agent、Traffic Monitor 和 Bandwidth Monitor
的功能以達到封包與流量的監測；實現 Control Plane
的 Admission Control、Traffic Control、Congestion 
Control 和 Traffic Engineering 的功能以提供網路控管
所需執行的動態配置；實現 Management Plane 的 4 個
子功能模組；實現 Policy Pool 以提供各種策略存放的
地方；實現 QoS Controller 以提供 policy 的解析與致
能；實現 Adaptive QoS Strategy 以提供動態的網路服
務品質控管以及實現 Web-based Management 的功能
以提供遠端動態調配網路服務品質機制的構想。 
 第 20 頁 / 共 50 頁 
 
圖 4.5 Monitor Agent 
4.2.2 Control Plane 
Control Plane主要是透過Management Plane針對
目前的網路狀態提供決策資訊並對目前的網路實行
Admission Control 和 Traffic Control 以達到實際網路
的控制。在 Control Plane 中主要包含四個模組，分別
為 Admission Control 模組、Traffic Control 模組、 
Congestion Control 模組和 Traffic Engineering 模組，
相關描述如下： 
Admission Control 
Admission control 是網路服務品質保證中的一道
之重要程序。決定要分配多少個頻寬量與延遲時間給
予不同的 QoS 請求，並且會針對網路狀況的不同對應
不同的 rule table 需求。本研究會利用 Flow table 的控
制方式來達到 Admission control 的功能。Admission 
control 為了要控制整體的網路流量，因此它的設計需
要確切的知道實際網路的邊緣節點和核心節點。 
Admission control 應該盡量避免壅塞和封包丟棄
發生的機率並有效的利用現有的資源。由於要在系統
中提供一致的 QoS 控制，因此會將資源保留協定
(RSVP)納入相關的應用考量中。下圖 4.6 為 RSVP 的
運作圖。由此可以發現主機和路由器沿著資料流的路
徑利用 RSVP 傳送 QoS的要求到所有的節點上。RSVP
的設計為 IPv4 和 IPv6，亦符合下一代網路協定並將它
用來保留特定資料流的頻寬。 
 
圖 4.6 RSVP 運作圖 
Traffic Control 
此模組的主要功能是根據 QoS 相關資訊的要求
將近來的資料流篩選過濾。如此一來，資源就能透過
流量管理模組獲得完整的控制和管理。圖 4.7 為Traffic 
Control 的運作流程圖。此模組在一開始的時候會先取
得 Record Table 的分類資料，接著根據使用者預先定
義 Rule Table 要求來給予各個封包應用服務的優先
權。而 Rule Table 的資訊會儲存在 Admission control
的模組中以供此模組擷取使用。 
 資料流的篩選是利用 Token Bucket 的演算法，此
機制的決策取決於 Token 數量的多寡。當 Token 不足
的時候，就會把封包從資料流送往佇列排班，否則過
濾器就會把這個封包丟棄。當網路封包經過這個佇列
時，就會受到一個預先設定的速率限制(封包數/時
間)，利用此定義可以有效減緩網路瞬增流量(buffer 
short bursts)所造成網路效能降低的影響。此部分的篩
選工作是在執行加權公帄佇列(WFQ)排班之前會先執
行此演算法。如此一來頻寬控制就可以利用排班將適
合的封包送到佇列中以達到公帄的傳輸條件。圖 4.8
為 Traffic Control 的流程圖。 
 第 22 頁 / 共 50 頁 
設計內容在後面的章節將有更深入的討論。圖 4.10 為
Traffic Engineering 的運作架構圖。 
 
圖 4.10 Traffic Engineering 的運作架構 
4.2.3 Management Plane 
Management Plane 主要負責提出維持網路服務
的決策資訊，舉凡預先定義好的 Adaptive QoS Strategy
和使用者定義的policy。這部分的決策執行會經由QoS 
Controller 並透過它解析 Policy Pool 中的策略以應用
到目前網路環境中。在 Management Plane 中主要包含
四個模組，分別為 QoS Controller 模組、Web-based 
Management 模組、Policy Pool 模組和 Adaptive QoS 
Strategy 模組，以下會分別描述它。 
QoS Controller 
 為了要讓現存的網路管理機制能統一，因此利用
OpenFlow 帄台的設計架構以實現集中控制的概念。此
模組的設計概念就是利用此帄台的 NOX Controller 的
管理架構衍伸而成。而本研究系統端管理命令的下達
全經由 QoS Controller 實現。如果接收到 Scheduler 發
送出來的 QoS Policy，此模組會將此策略解析啟動它
並結束舊的 Policy 以達到網路效能的控管。而目前的
運作只能透過 Linux 的文字介面得知目前的網路環境
的控制資訊。 
 此模組尚有另一項重要功能，就是解析Scheduler
發送出來的 QoS policy。而這部分解析的運作流程如
下圖 4.11 所示，在 Openflow-based NetFPGA 網路管
理系統中，主要是利用 Port 的判斷方式提供使用者要
求的服務型別。 
 
圖 4.11 解析 policy 的運作流程圖 
Web-based Management 
 此模組目前主要的功能是提供網路管理者一個
網頁控管的介面，以實現遠端管理網路服務品質的想
法。此模組可以讓網路管理者不受限於只能在機器旁
操作，而是可以透過網路傳輸遠端的調配目前的網路
服務品質。 
Policy Pool 
 此模組的設計概念是希望能把現有的網路管理
機制都納入本系統中以增加網路管理的靈活性與延展
性，因此有此模組存在的必要。而本研究所提供的 QoS 
policy 將會以 XML 的形式存在 Policy Pool 中，以供
系統解析運用。 
Adaptive QoS Strategy 
 本研究所提出的 Adaptive QoS Strategy考量的出
發點是以應用服務為基礎的服務品質保證機制。為了
要維持一定的網路傳輸效率，因此會根據使用者定義
的策略規則為基礎考量每個節點的使用狀態執行
Traffic Engineering。由於使用者定義的規則寫入相對
應策略中當做演算法初始的條件判斷，因此本研究預
設的決策條件是當網路的頻寬使用率達到 85%的時候
就會立即啟動此演算法。在系統成功啟動後，NOX 
Controller 就會透過 discovery( )去找到目前連結的網
路節點，並透過 monitor( )蒐集每個節點的封包資訊。
關於演算法實際的設計規劃，如下所示： 
Step 1. 當目前的網路狀況符合條件時，就會蒐集每個
節點上的封包資訊。例如：目前環境中的存在的節點
個數、每個節點的 IP 位址、每個節點存在的相鄰節點
 第 24 頁 / 共 50 頁 
} 
 
圖 4.12 Adaptive QoS Strategy 的流程圖 
五、 結果與討論 
圖 5.1 為總計畫的實作架構，由於子三的研究成
果中需接收子一封包辨識的結果與子四提供的策略，
因此於研究時假定一個格式化的策略檔，並以一般
Port 辨識的方式實作而成。 
 
圖 5.1 網路虛擬化架構之研究與建置圖 
子三目前已完成基本功能模組的訂定，並且已完
成部分的功能模組。於系統中將會利用 OpenFlow 現
有的架構去實作子三的 QoS 功能。目前已利用 NOX 
Controller 的 Pyswitch 著手開發 QoS Policy 的研究，
目前已能針對特定的來源封包提供 Forward/Drop的動
作。關於 User-Defined Policy 的部分已利用 Web 介面
提供使用者訂定自己的需求。訂定的概念是當網路的
傳輸情況不良時，使用者可以選擇要取用哪些服務。
由於 NetFPGA 是一個可以將自己的 idea 付諸實現的
一個硬體帄台搭配 OpenFlow 的輔助就能將自己的想
法實現，因此目前仍積極的朝整合目標的方向前進。
下圖 5.2 為使用者定義策略的 Web 介面圖，而下圖 5.3
為使用者依自己的需求設定要求的服務種類所產生的
XML 檔。 
 
圖 5.2 使用者定義策略的 web 介面 
 
圖 5.3 XML 格式 
 第 26 頁 / 共 50 頁 
待釐清的。而為了能精準的驗證真實網路之效能資
訊，並提供給開發者作為未來網路效能改進之依據，
因此本研究的開發設計會利用史丹佛大學所推行的
NetFPGA 網路硬體開發帄台及 Openflow 網路控制軟
體來建構本計畫的開發環境。本研究擬藉由現有的
QoS 規範設計出網路虛擬化架構所提出的一個可調式
網路服務品質確保機制。此機制包含三大模組： 
 Data Plane：監測並儲存封包資訊。 
 Control Plane：定義相關的功能模組以供 QoS 機
制執行決策。 
 Management Plane：提出一個新的 QoS 演算法以
調整網路的服務品質並執行 policy 解析與 QoS 
control。 
藉由此三大模組與 NetFPGA 硬體開發帄台相結
合，根據接收的網路狀態資訊，適時的調整服務品質
保證的參數值，進而確保網路服務效率維持在一個水
準之上。未來當帄台建立完善時，網管系統就可透過
此帄台進行相關 policy 之研究，並在此帄台上驗證且
分析各種策略的優劣，而分析的結果亦可以提供網路
設備開發商在設計新一代的網路設備時，作為驗證及
參考的依據。本計畫為二年期之研究計畫，研究主題
分別「QoS Engine 的基礎網路架構與 Data Plane 和
Control Plane 的功能模組設計開發」、「Management 
Plane 功能模組設計開發以及 QoS Engine 與 NetFPGA
之系統整合」二大課題。 
本計畫(第一期計畫)已於 NefFPGA 硬體開發帄
台上完成 QoS Engine 基本架構及模組之定義。而為了
能夠針對龐大的網路流量進行統計、分析及簡化，將
目前的網路狀況摘要性的記錄下來，因此本計畫已建
構 Data Plane、Control Plane 和 Management Plane 的
相關功能模組功能，藉以發展可調整的 QoS 處理策
略，以達成完備的網路控制。在研究成果 outcome 部
份 發 表 一 篇 國 際 期 刊 論 文 (Media-Independent 
Handover Mechanism for Next Generation Vehicular 
Networking)。此外，未來計畫執行培育出 2 位碩士級
人才，並投入學術及產業研發行列。 
 第 28 頁 / 共 50 頁 
[24] T. Karagiannis, A. Broido, N. Brownlee, K. Claffy 
and M. Faloutsos “Is P2P Dying or just Hiding?” 
Proceedings of the IEEE Globecom- Global 
Internet and Next Generation Networks, Dallas, 
Texas, USA, 29 Nov - 3 Dec, 2004. 
[25] T. Karagiannis, D. Papagiannaki and M. Faloutsos 
“BLINC: Multilevel Traffic Classification in the 
Dark” ACM SIGCOMM Computer Communication 
Review 35 (4), pp. 229-240, August 2005. 
[26] T. Karagiannis, A. Broido, M. Faloutsos and K.C. 
Claffy, “Transport Layer Identification of p2p 
Traffic,” Proceedings of the 2004 ACM SIGCOMM 
Internet Measurement Conference, IMC, pp. 
121-134, 2004. 
[27] IPP2P, http://www.ipp2p.org 
[28] L7-filter, http://l7-filter.sourceforge.net 
[29] Netfilter Project, http://www.netfilter.org. 
[30] R. Russell and H. Welte, “Linux Netfilter Hacking 
HOWTO“, 
http://www.netfilter.org/documentation/HOWTO/ne
tfilter-hacking-HOWTO.htm l, 2002. 
[31] N. Lin and M. Qi, “A QoS Model of Next 
Generation Network based on MPLS,” Proceedings 
of IFIP International Conference on Network and 
Parallel Computing Workshops, pp. 915-919, 2007. 
[32] D. Zhang and D. Ionescu, “QoS Performance 
Analysis in Deployment of DiffServ-aware MPLS 
Traffic Engineering,” Proceedings of the 8th ACIS 
International Conference on Software Engineering, 
Artificial Intelligence, Networking, and 
Parallel/Distributed Computing, pp.963-967, 
August 2007. 
[33] J. Salim, H. Khosravi, A. Kleen and A. Kuznetsov, 
Linux Netlink as an IP Services Protocol, RFC 
3549, IETF, July 2003. 
[34] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. 
McKeown and S. Shenker, “Ethane: Taking Control 
of the Enterprise,” ACM SIGCOMM Conference on 
Computer Communications, pp. 1-12, 2007.  
[35] L. Zhang, “Virtual Clock: A New Traffic Control 
Algorithm for Packet Switching Networks,” Proc. 
ACM SIGCOMM, pp.19-29, Sep. 1990. 
[36] S. Floyd and K. Fall, “Promoting the Use of 
End-to-end Congestion Control in the Internet,” 
IEEE/ACM Transactions on Networking, Vol.7, 
No.8, pp.458-472, 1999. 
[37] H. Wang, H. Xie, L. Qiu, Y.R. Yang, Y. Zhang and A. 
Greenberg, “Cope: Traffic Engineering in Dynamic 
Networks,” Proc. ACM Sigcomm, Vol.36, pp.99-110, 
2006. 
 
 第 30 頁 / 共 50 頁 
inst.st[dpid].has_key(dstaddr): 
        prt = inst.st[dpid][dstaddr] 
 if  prt[0] == inport: 
            log.err('**warning** learned port = 
inport', system="adaptive") 
            inst.send_openflow(dpid, bufid, buf, 
openflow.OFPP_FLOOD, inport) 
        else: 
            # We know the outport, set up a flow 
            log.msg('installing flow for ' + str(packet), 
system="adaptive") 
            flow = extract_flow(packet) 
            flow[core.IN_PORT] = inport 
            actions = [[openflow.OFPAT_OUTPUT, 
[0, prt[0]]]] 
     inst.install_datapath_flow(dpid, flow, 
CACHE_TIMEOUT,  
                                
openflow.OFP_FLOW_PERMANENT, actions, 
                                bufid, 
openflow.OFP_DEFAULT_PRIORITY, 
                                       inport, 
buf) 
    else:     
        # haven't learned destination MAC. Flood  
        inst.send_openflow(dpid, bufid, buf, 
openflow.OFPP_FLOOD, inport) 
         
# -- 
# Responsible for timing out cache entries. 
# Is called every 1 second. 
# -- 
def timer_callback(): 
    global inst 
 
    curtime  = time() 
    for dpid in inst.st.keys(): 
        for entry in inst.st[dpid].keys(): 
            if (curtime - inst.st[dpid][entry][1]) > 
CACHE_TIMEOUT: 
                log.msg('timing out 
entry'+mac_to_str(entry)+str(inst.st[dpid][entry])+' on 
switch %x' % dpid, system='adaptive') 
                inst.st[dpid].pop(entry) 
 
    inst.post_callback(5, timer_callback) 
    return True 
 
def datapath_leave_callback(dpid): 
    logger.info('Switch %x has left the network' % dpid) 
    if inst.st.has_key(dpid): 
        del inst.st[dpid] 
 
def datapath_join_callback(dpid, stats): 
    logger.info('Switch %x has joined the network' % 
dpid) 
 
# -- 
# Packet entry method. 
# Drop LLDP packets (or we get confused) and attempt 
learning and 
# forwarding 
# -- 
def packet_in_callback(dpid, inport, reason, len, bufid, 
packet): 
    global t, counter 
 
    a=open('/root/policy.xml').read() 
    policy=BeautifulSoup.BeautifulStoneSoup(a) 
 
    if not packet.parsed: 
        log.msg('Ignoring incomplete 
packet',system='adaptive') 
         
    if not inst.st.has_key(dpid): 
        log.msg('registering new switch %x' % 
dpid,system='adaptive') 
        inst.st[dpid] = {} 
 第 32 頁 / 共 50 頁 
    def aggregate_timer(self, dpid): 
        flow = ofp_match() 
        flow.wildcards = 0xffff 
        self.ctxt.send_aggregate_stats_request(dpid, 
flow,  0xff) 
        
self.post_callback(MONITOR_TABLE_PERIOD, 
lambda : self.aggregate_timer(dpid)) 
 
    def table_timer(self, dpid): 
        self.ctxt.send_table_stats_request(dpid) 
        
self.post_callback(MONITOR_TABLE_PERIOD, 
lambda : self.table_timer(dpid)) 
 
    def port_timer(self, dpid): 
        self.ctxt.send_port_stats_request(dpid) 
        
self.post_callback(MONITOR_PORT_PERIOD, lambda : 
self.port_timer(dpid)) 
 
    # For each new datapath that joins, create a timer 
loop that monitors 
    # the statistics for that switch 
    def datapath_join_callback(self, dpid, stats): 
        
self.post_callback(MONITOR_TABLE_PERIOD, 
lambda : self.table_timer(dpid)) 
        
self.post_callback(MONITOR_PORT_PERIOD + 1, 
lambda :  self.port_timer(dpid)) 
        
self.post_callback(MONITOR_AGGREGATE_PERIOD 
+ 2, lambda :  self.aggregate_timer(dpid)) 
 
    def aggregate_stats_in_handler(self, dpid, stats): 
        print "Aggregate stats in from datapath", 
longlong_to_octstr(dpid)[6:] 
        print '\t',stats 
 
    def table_stats_in_handler(self, dpid, tables): 
        print "Table stats in from datapath", 
longlong_to_octstr(dpid)[6:] 
        for item in tables: 
            print 
'\t',item['name'],':',item['active_count'] 
 
    def port_stats_in_handler(self, dpid, ports): 
        print "Port stats in from datapath", 
longlong_to_octstr(dpid)[6:] 
        for item in ports: 
            print 
'\t',item['port_no'],':',item['tx_packets'] 
 
    def install(self): 
        self.register_for_datapath_join(lambda dpid, 
stats : self.datapath_join_callback(dpid,stats)) 
        
self.register_for_table_stats_in(self.table_stats_in_handl
er) 
        
self.register_for_port_stats_in(self.port_stats_in_handler
) 
        
self.register_for_aggregate_stats_in(self.aggregate_stats_
in_handler) 
 
    def getInterface(self): 
        return str(Monitor) 
 
def getFactory(): 
    class Factory: 
        def instance(self, ctxt): 
            return Monitor(ctxt) 
 
    return Factory() 
 
 
 第 34 頁 / 共 50 頁 
    bstr = '' 
    for byte in arr: 
        if bstr != '': 
            bstr += ':%02x' % (byte,) 
        else: 
            bstr += '%02x' %(byte,) 
    return bstr 
 
def longlong_to_octstr(ll): 
    return 
array_to_octstr(array.array('B',struct.pack('!Q',ll))) 
 
def mac_to_oui(a): 
    if type(a) == type(1L): 
        a = struct.pack('!Q', a)[2:] 
    if type(a) == type(''): 
        a = array.array('B',a) 
 
    oui = int(a[0]) << 16 | int(a[1]) << 8 | int(a[2]) 
 
    if _ethoui2name.has_key(oui): 
        return _ethoui2name[oui] 
    else: 
        return "" 
 
def mac_to_str(a, resolve_name = False): 
    if type(a) == type(1L): 
        a = struct.pack('!Q', a)[2:] 
    if type(a) == type(''): 
        a = array.array('B', a) 
 
    oui = int(a[0]) << 16 | int(a[1]) << 8 | int(a[2]) 
 
    # check if globally unique 
    if resolve_name and not (a[0] & 0x2): 
        if _ethoui2name.has_key(oui): 
            return "(%s):%02x:%02x:%02x" 
%( _ethoui2name[oui], a[3],a[4],a[5]) 
    return array_to_octstr(a) 
 
def mac_to_int(mac): 
    value = 0 
    for byte in struct.unpack('6B', mac): 
        value = (value << 8) | byte 
    return long(value) 
 
def ethtype_to_str(t): 
    if t < 0x0600: 
        return "llc" 
    if _ethtype_to_str.has_key(t): 
        return _ethtype_to_str[t] 
    else: 
        return "%x" % t 
 
def ipproto_to_str(t): 
    if _ipproto_to_str.has_key(t): 
        return _ipproto_to_str[t] 
    else: 
        return "%x" % t 
 
def load_oui_names(): 
    import os 
    filename = 'nox/lib/packet/oui.txt' 
    if not os.access(filename, os.R_OK): 
        return None 
    for line in open(filename).readlines(): 
        if len(line) < 1: 
            continue 
        if line[0].isspace(): 
            continue 
        split = line.split(' ') 
        if not '-' in split[0]: 
            continue 
        # grab 3-byte OUI 
        oui_str  = split[0].replace('-','') 
        # strip off (hex) identifer and keep rest of name 
        end = ' '.join(split[1:]).strip() 
        end = end.split('\t') 
 第 36 頁 / 共 50 頁 
 
        # xxx Need to support SNAP/LLC frames 
        if self.type in ethernet.type_parsers: 
            self.next = 
ethernet.type_parsers[self.type](self.arr[ethernet.MIN_L
EN:], self) 
        else: 
            self.next = 
self.arr[ethernet.MIN_LEN:].tostring() 
 
    def __str__(self): 
        s = 
''.join(('[',mac_to_str(self.src,True),'>',mac_to_str(self.dst
,True),':',ethtype_to_str(self.type),']')) 
        if self.next == None: 
            return s 
        elif type(self.next) == type(""): 
            return s 
        else: 
            return ''.join((s, str(self.next))) 
 
 
    def hdr(self): 
        dst = self.dst 
        src = self.src 
        if type(dst) != type(""): 
            dst = dst.tostring() 
        if type(src) != type(""): 
            src = src.tostring() 
        return struct.pack('!6s6sH', dst, src, self.type) 
 
 
# trying to bypass a hairy cyclical include problems 
 
from vlan import vlan 
ethernet.type_parsers[ethernet.VLAN_TYPE] = vlan 
from arp  import arp 
ethernet.type_parsers[ethernet.ARP_TYPE]  = arp 
ethernet.type_parsers[ethernet.RARP_TYPE] = arp 
from ipv4 import ipv4 
ethernet.type_parsers[ethernet.IP_TYPE]   = ipv4 
from lldp import lldp 
ethernet.type_parsers[ethernet.LLDP_TYPE] = lldp 
from eapol import eapol 
ethernet.type_parsers[ethernet.PAE_TYPE]  = eapol 
 
5. core.py 
import logging 
import array 
import struct 
import types 
from socket import htons, htonl 
import nox.lib.openflow as openflow 
 
from nox.coreapps.pyrt.pycomponent import * 
from util import * 
from nox.lib.netinet.netinet import Packet_expr 
from nox.lib.netinet.netinet import create_eaddr 
from nox.lib.packet import * 
 
lg = logging.getLogger('core') 
 
IN_PORT    = "in_port" 
AP_SRC     = "ap_src" 
AP_DST     = "ap_dst" 
DL_SRC     = "dl_src" 
DL_DST     = "dl_dst" 
DL_VLAN    = "dl_vlan" 
DL_TYPE    = "dl_type" 
NW_SRC     = "nw_src" 
NW_SRC_N_WILD = "nw_src_n_wild" 
NW_DST     = "nw_dst" 
NW_DST_N_WILD = "nw_dst_n_wild" 
NW_PROTO   = "nw_proto" 
TP_SRC     = "tp_src" 
TP_DST     = "tp_dst" 
GROUP_SRC  = "group_src" 
GROUP_DST  = "group_dst" 
 第 38 頁 / 共 50 頁 
 
    # Interface to allow components to check at runtime 
without having 
    # to import them (which will cause linking errors) 
    def is_component_loaded(self, name): 
        if not self.component_names: 
            self.component_names = [] 
            for component in 
self.ctxt.get_kernel().get_all(): 
                
self.component_names.append(component.get_name()) 
        return name in self.component_names 
 
    def register_event(self, event_name): 
        return self.ctxt.register_event(event_name) 
 
    def register_python_event(self, event_name): 
        return 
self.ctxt.register_python_event(event_name) 
 
    def register_handler(self, event_name, handler): 
        return self.ctxt.register_handler(event_name, 
handler) 
 
    def post_timer(self, event): 
        return self.ctxt.post_timer(event) 
 
    def post(self, event): 
        # if event is a swigobject, make sure that it 
doesn't try 
        # to handle memory deletion 
        if hasattr(event, 'thisown'): 
            event.thisown = 0 # just in case 
        return self.ctxt.post(event) 
 
    def make_action_array(self, actions): 
        action_str = "" 
 
        for action in actions: 
            if action[0] == 
openflow.OFPAT_OUTPUT \ 
                    and isinstance(action[1],list) \ 
                    and len(action[1]) == 2: 
                a = struct.pack("!HHHH", action[0], 
8, 
                                action[1][1], 
action[1][0]) 
            elif action[0] == 
openflow.OFPAT_SET_VLAN_VID: 
                a = struct.pack("!HHHH", action[0], 
8, action[1], 0) 
            elif action[0] == 
openflow.OFPAT_SET_VLAN_PCP: 
                a = struct.pack("!HHBBH", action[0], 
8, action[1], 0, 0) 
            elif action[0] == 
openflow.OFPAT_STRIP_VLAN: 
                a = struct.pack("!HHI", action[0], 8, 
0) 
            elif action[0] == 
openflow.OFPAT_SET_DL_SRC \ 
                    or action[0] == 
openflow.OFPAT_SET_DL_DST: 
                eaddr = convert_to_eaddr(action[1]) 
                if eaddr == None: 
                    print 'invalid ethernet addr' 
                    return None 
                a = struct.pack("!HH6sHI", action[0], 
16, eaddr.binary_str(), 0, 0) 
            elif action[0] == 
openflow.OFPAT_SET_NW_SRC \ 
                    or action[0] == 
openflow.OFPAT_SET_NW_DST: 
                iaddr = convert_to_ipaddr(action[1]) 
                if iaddr == None: 
                    print 'invalid ip addr' 
                    return None 
                # ipaddr already in network byte 
 第 40 頁 / 共 50 頁 
        dp_id - datapath to send packet to 
        buffer_id - id of buffer to send out 
        actions - list of actions or dp port to send out of 
        inport - dp port to mark as source (defaults to 
Controller port) 
        """ 
        if type(actions) == types.IntType: 
            self.ctxt.send_openflow_buffer(dp_id, 
buffer_id, actions, inport) 
        elif type(actions) == types.ListType: 
            oactions = 
self.make_action_array(actions) 
            if oactions == None: 
                raise Exception('Bad action') 
            self.ctxt.send_openflow_buffer(dp_id, 
buffer_id, oactions, inport) 
        else: 
            raise Exception('Bad argument') 
 
    def post_callback(self, t, function): 
        from twisted.internet import reactor 
        reactor.callLater(t, function) 
 
    def send_flow_command(self, dp_id, command, 
attrs, 
                          
priority=openflow.OFP_DEFAULT_PRIORITY, 
                          add_args=None, 
                          
hard_timeout=openflow.OFP_FLOW_PERMANENT): 
        m = set_match(attrs) 
        if m == None: 
            return False 
 
        if command == openflow.OFPFC_ADD: 
            (idle_timeout, actions, buffer_id) = 
add_args 
            oactions = 
self.make_action_array(actions) 
            if oactions == None: 
                return False 
        else: 
            idle_timeout = 0 
            oactions = "" 
            buffer_id = UINT32_MAX 
 
        self.ctxt.send_flow_command(dp_id, 
command, m, idle_timeout, 
                                    
hard_timeout, oactions, buffer_id, priority) 
 
        return True 
 
    # Former PyAPI methods 
 
    def send_openflow(self, dp_id, buffer_id, packet, 
actions, 
                      
inport=openflow.OFPP_CONTROLLER): 
        """ 
        Sends an openflow packet to a datapath. 
 
        This function is a convenient wrapper for 
send_openflow_packet 
        and send_openflow_buffer for situations where 
it is unknown in 
        advance whether the packet to be sent is 
buffered.  If 
        'buffer_id' is -1, it sends 'packet'; otherwise, it 
sends the 
        buffer represented by 'buffer_id'. 
 
        dp_id - datapath to send packet to 
        buffer_id - id of buffer to send out 
        packet - data to put in openflow packet 
        actions - list of actions or dp port to send out of 
        inport - dp port to mark as source (defaults to 
Controller 
 第 42 頁 / 共 50 頁 
        Add a flow entry to datapath 
 
        dp_id - datapath to add the entry to 
 
        attrs - the flow as a dictionary (described 
above) 
 
        idle_timeout - # idle seconds before flow is 
removed from dp 
 
        hard_timeout - # of seconds before flow is 
removed from dp 
 
        actions - a list where each entry is a 
two-element list representing 
        an action.  Elem 0 of an action list should be 
an ofp_action_type 
        and elem 1 should be the action argument (if 
needed). For 
        OFPAT_OUTPUT, this should be another 
two-element list with max_len 
        as the first elem, and port_no as the second 
 
        buffer_id - the ID of the buffer to apply the 
action(s) to as well. 
        Defaults to None if the actions should not be 
applied to a buffer 
 
        priority - when wildcards are present, this 
value determines the 
        order in which rules are matched in the switch 
(higher values 
        take precedence over lower ones) 
 
        packet - If buffer_id is None, then a data 
packet to which the 
        actions should be applied, or None if none. 
 
        inport - When packet is sent, the port on which 
packet came in as input, 
        so that it can be omitted from any 
OFPP_FLOOD outputs. 
        """ 
        if buffer_id == None: 
            buffer_id = UINT32_MAX 
 
        self.send_flow_command(dp_id, 
openflow.OFPFC_ADD, attrs, priority, 
                          (idle_timeout, actions, 
buffer_id), hard_timeout) 
 
        if buffer_id == UINT32_MAX and packet != 
None: 
            for action in actions: 
                if action[0] == 
openflow.OFPAT_OUTPUT: 
                    
self.send_openflow_packet(dp_id, packet, action[1][1], 
inport) 
                else: 
                    raise NotImplementedError 
 
    def register_for_packet_in(self, handler): 
        """ 
        register a handler to be called on every 
packet_in event 
        handler will be called with the following args: 
 
        handler(dp_id, inport, ofp_reason, 
total_frame_len, buffer_id, 
        captured_data) 
 
        'buffer_id' == None if the datapath does not 
have a buffer for 
        the frame 
        """ 
        
self.register_handler(Packet_in_event.static_get_name(), 
 第 44 頁 / 共 50 頁 
self.register_handler(Table_stats_in_event.static_get_na
me(), 
                              
gen_ts_in_cb(handler)) 
 
    
##############################################
################################## 
    # register a handler to be called whenever port 
statistics are 
    # returned by a switch. 
    # 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, stats) 
    # 
    # Stats is a dictionary of port stats with the 
following keys: 
    # 
    #   "port_no" 
    #   "rx_packets" 
    #   "tx_packets" 
    #   "rx_bytes" 
    #   "tx_bytes" 
    #   "rx_dropped" 
    #   "tx_dropped" 
    #   "rx_errors" 
    #   "tx_errors" 
    #   "rx_frame_err" 
    #   "rx_over_err" 
    #   "rx_crc_err" 
    #   "collisions" 
    # 
    
##############################################
################################## 
 
    def register_for_port_stats_in(self, handler): 
        
self.register_handler(Port_stats_in_event.static_get_nam
e(), 
                              
gen_ps_in_cb(handler)) 
 
 
    
##############################################
################################## 
    # register a handler to be called whenever table 
aggregate 
    # statistics are returned by a switch. 
    # 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, stats) 
    # 
    # Stats is a dictionary of aggregate stats with the 
following keys: 
    # 
    #   "packet_count" 
    #   "byte_count" 
    #   "flow_count" 
    # 
    
##############################################
################################## 
 
    def register_for_aggregate_stats_in(self, handler): 
        
self.register_handler(Aggregate_stats_in_event.static_get
_name(), 
                              
gen_as_in_cb(handler)) 
 
 
    
##############################################
################################## 
 第 46 頁 / 共 50 頁 
    # Absent keys will be interpretted as wildcards (i.e. 
any value is 
    # accepted for those attributes when checking for a 
potential match) 
    # 
    # handler will be called with the following args: 
    # 
    # handler(dp_id, inport, ofp_reason, total_frame_len, 
buffer_id, 
    # captured_data) 
    # 
    # 'buffer_id' == None if the datapath does not have a 
buffer for 
    # the frame 
    
##############################################
############################# 
 
    def register_for_packet_match(self, handler, priority, 
expr): 
        e = Packet_expr() 
        for key, val in expr.items(): 
            if key == AP_SRC: 
                field = Packet_expr.AP_SRC 
                val = htons(val) 
            elif key == AP_DST: 
                field = Packet_expr.AP_DST 
                val = htons(val) 
            elif key == DL_VLAN: 
                field = Packet_expr.DL_VLAN 
                val = htons(val) 
            elif key == DL_TYPE: 
                field = Packet_expr.DL_TYPE 
                val = htons(val) 
            elif key == DL_SRC: 
                field = Packet_expr.DL_SRC 
                val = convert_to_eaddr(val) 
                if val == None: 
                    print 'invalid ethernet addr' 
                    return False 
            elif key == DL_DST: 
                field = Packet_expr.DL_DST 
                val = convert_to_eaddr(val) 
                if val == None: 
                    print 'invalid ethernet addr' 
                    return False 
            elif key == NW_SRC: 
                field = Packet_expr.NW_SRC 
                val = convert_to_ipaddr(val) 
                if val == None: 
                    print 'invalid ip addr' 
                    return False 
            elif key == NW_DST: 
                field = Packet_expr.NW_DST 
                val = convert_to_ipaddr(val) 
                if val == None: 
                    print 'invalid ip addr' 
                    return False 
            elif key == NW_PROTO: 
                field = Packet_expr.NW_PROTO 
            elif key == TP_SRC: 
                field = Packet_expr.TP_SRC 
                val = htons(val) 
            elif key == TP_DST: 
                field = Packet_expr.TP_DST 
                val = htons(val) 
            elif key == GROUP_SRC: 
                field = Packet_expr.GROUP_SRC 
                val = htonl(val) 
            elif key == GROUP_DST: 
                field = Packet_expr.GROUP_DST 
                val = htonl(val) 
            else: 
                print 'invalid key', key 
                return False 
 
            if isinstance(val, ethernetaddr): 
                e.set_eth_field(field, val) 
 第 48 頁 / 共 50 頁 
可供推廣之研發成果資料表 
□ 可申請專利  ■ 可技術移轉                                      日期：99 年 5月 30日 
國科會補助計畫 
計畫名稱：網路虛擬化架構之研究與建置(1/2) 子計畫三: 設計與
實作一具有可調式服務品質保證之網路虛擬化架構(1/2) 
計畫主持人：陳俊良 教授 
計畫編號：NSC 98– 2219– E– 011– 003 學門領域：電信國家型計畫 
技術/創作名稱 NetFPGA 帄台上設計一套可調式的服務品質保證機制  
發明人/創作人 陳俊良/曾毓元 
技術說明 
中文： 
本研究是利用史丹佛大學提供的 NetFPGA 硬體帄台，搭配
OpenFlow 建構網路虛擬化架構，設計一套集中管理且具有可調式
服務品質保證之機制，俾使網路管理功能更臻完善。此機制架構包
括三個 Plane：Data Plane、Control Plane 及 Management Plane。Data 
Plane 的功能在於利用 Traffic Monitor 和 Bandwidth Monitor 模組監
控目前的網路狀態，並將各種封包所屬的服務種類寫入 Record 
Table 中，透過 Monitor Agent 將相關的 QoS 相關資訊，提供給
Management Plane，以提供演算法機制所需要的參數資訊。而
Control Plane 的主要功能為根據目前的網路狀態，透過 Management 
Plane 傳遞決策資訊以提供 Admission Control、Traffic Control、
Traffic Engineering 和 Congestion Control 的功能。而在 Management 
Plane 中，定義了完整的管理流程，以提供網路管理者一個友善的
介面控管網路。本研究建置的 NetFPGA 帄台，自 Switch/Router 將
各種封包篩選過濾後，並標記所屬的應用類型，再經由 Monitor 
Agent 模組彙整 Traffic Monitor 模組和 Bandwidth Monitor 模組所提
供的資訊，建成一個 Record Table，使管理機制能夠獲得相關 QoS
參數。並藉由可調式的 QoS 策略，進行網路流量控制與調整。  
附件二 
