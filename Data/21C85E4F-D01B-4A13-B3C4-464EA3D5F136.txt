the caption/symbol templates In the 1st phase, the system locates 
the SCB, extracts SCB color histogram, and SCB sub-image 
extraction. In the 2nd phase, if the key-frame is selected, the SCB 
templates can be used to understand the succeeding video. In the 
last phase, we interpret the semantic meanings for the succeeding 
video by using the generated caption/symbol templates. 
II. SCB EXTRACTION AND MODELING 
A. SCB Extraction 
It is obvious that the SCB sub-image region is either 
stationary globally or varying locally. We combine color-based 
local dynamic and temporal motion consistency to locate the SCB 
from a group of frames (GoF), because there is high color 
correlation and low motion activities within the SCB sub-image 
region.  
First, the color distance between two consecutive frames fk 
and fk+1 is computed pixel-wise. The HSI color distance [8] for 
pixel i in fk and the same pixel i in fk+1 is denotes as DCi. Then, we 
define the motion activity MAi for every pixel i of each frame fk 
using standard block-based motion estimation. We classify pixel i 
by using the thresholding algorithm based on the two histograms 
hist(DC) and hist(MA) to determine whether pixel i belongs to the 
SCB or not. Here, we propose a fine-tuning to find the best 
thresholds. By analyzing hist(DC) and hist(MA), we may select the 
thresholds θDC and θMA and obtain the potential SCB mask Mscb in 
ith frame 
 
B. SCB Color Modeling 
To generate the SCB color model, the HSI color components 
of each pixel within SCB mask are coded into 4 bits, 2 bits and 2 
bits, respectively. The color model for the SCB in frame k is 
,255))(()( ≤≤−= ∑ j0 for  jI,S,HLjh
i
k
i
k
i
k
iHSI
k δ (1)
where  are the three color components of pixel i in 
frame k, δ(i - j)=1 for i = j, δ(i - j)=0 otherwise. LHSI (‧) represents 
the color mapping function which converts a 3-D color component 
(H, S, I) to a 1-D color index. The representative SCB 1-D color 
histogram hR is created by averaging the histograms of the 
segmented SCBs in GoF as 
k
i
k
i
k
i ISH ,,
,)(1)(
1
∑
=
= N
k
k
R jhN
jh  (2)
where N is the number of frames in GoF. 
 
C. SCB Appearance Detection 
Since the SCB template does not change much during the 
entire video, we may use the SCB color model to identify its 
presence even if the SCB may be transparent. The similarity 
measure between the pre-stored SCB model hR and the potential 
SCB is formulated by Mahalanobis distance [14] d(hiMscb, 
hR)=(hiMscb－hR)TCm-1(hiMscb－hR), where hiMscb denotes the color 
distribution of the potential SCB mask Mscb in ith frame. Based on 
the distance, d(hiMscb, hR), we may find a set of frames with small 
content change which called content-change keyframe. The show-
up keyframe denotes the 1st frame which detected SCB presence. 
III. CAPTION TEMPLATE GENERATION 
A. Character Extraction 
Once the SCB sub-image region is identified, we develop the 
character block extraction. We apply the Connected Component 
Analysis (CCA) algorithm to generate the character blocks in the 
SCB. The character block is the outmost bounding box of a given 
binary component from CCA, called the minimum bounding 
rectangle (MBR). Sometimes, the highlight text is shown with high 
intensity pixels located in low intensity background, whereas the 
normal text is displayed with low intensity pixels in high intensity 
background. Hence, we need to perform the same operations in the 
contrast reversed key-frame. Since characters are thin objects, non-
character regions can be removed by using morphological opening 
operation. Then the MBR can be spitted by the segmentation 
method [12]. 
B. Character Recognition and Caption Template 
Due to the limited image resolution, it is a nontrivial problem 
to achieve good character recognition rate. This report uses a pre-
trained SVM classifier as the recognition kernel to identify whether 
the MBR is a text or not. The font-independent features are used 
and the feature vector of character block consisting of Area, 
Centroid, Convex Area, Eccentricity, Equivalent Diameter, Euler 
Number, Orientation, and Solidity. The size of character block is 
normalized to 30×40. Once a new character is recognized, we may 
generate a specific caption template defined as 
                           (3) 
,][  ,,text/digit ,
i CBSVPosCT =
where Pos indicates the relative location in the SCB, text/digit bit 
indicates the caption is text or digit, SV indicates a set of support 
vectors for this character, CB is the corresponds character block. 
This caption template can be used to identify the characters in the 
succeeding videos. 
C. Digit Object Interpretation 
The SCB has the following regularities: (1) the digit and 
annotative objects are related based on their distance; (2) the digit 
objects and its associated annotative objects are always on the same 
horizontal line or vertical line; (3) two related digit objects may 
sandwich the same annotative object, but two independent digit 
objects cannot be related to the same annotative object. Here, we 
transform the semantic interpretation problem to the object labeling 
problem by using Relaxation Labeling [13] to assign each digit 
object a label (or an annotative object). 
Given (1) a set of objects {Oi, i=1, 2,…, n} including digit 
objects Λd and annotative objects Λa, (2) a set of labels {λj, j=1, 
2,…, m} with priorities, and (3) a neighboring relation over a pair 
of objects and a constraint relation over a pair of labels, our goal is 
to assign one label to one digit object. Here, we use the relaxation 
approaches to maximize (or minimize) the probabilities by iteration 
and taking into account the probabilities associated with 
neighboring features. We define the variable pi(λj) to represent the 
probability of the object Oi∈λj, with 0≤ pi(λ) ≤1 and ∑j pi(λj)=1. 
We define the compatibility of object i labeled by λ with object j 
labeled by λ΄ as c(i, λ, j, λ΄) which it represents the inter-
dependency between object j labeled as λ΄ and object i labeled as λ. 
It is insufficient that if the compatibility is only determined by the 
distance between digit object and the label (or the annotative 
object), and it may introduce the false interpretation. Therefore, we 
define the compatibility as follows 
confidence of five games converge after 35 and 5 iterations 
respectively. It means that the quarter annotation of the SCBs as 
shown in Figs. 3(a)~(e) is the easiest to label. Consider the MBR of 
the quarter number is the nearest digit object to the quarter 
annotation, therefore the influences from the other annotative 
object can be clearly reduced. Fig. 4(a) shows that the labeling 
confidence of Game-A is the slowest. This is primarily due to the 
increased influence of another digit object. By comparing the 
curves of Game-B and Game-D we find that the fast convergence is 
due to the score number of home team is distinctly parallel and 
close to the annotation “HT”. 
 
 
(a) score of home team             
 
Fig. 4. The confidence curve of iterative labeling. 
 
B. Sy
rule to verify the BP variation is correct or not, we apply the spatial 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(b) quarter 
mbol Understanding 
Based on the symbol template generation, we may identify 
and generate the three individual base templates. Initially, we find 
the variation in the SCB and evaluate the hue of symbol to acquire 
the presence/absence situation. Then, we verify the corresponding 
Base Pattern (BP): {(b3, b2, b1)} as (001), (010) or (100), where bi= 
1 or 0 indicates whether the ith base is taken. Once the content-
change keyframe is detected, we check whether the BP change is 
(001)→(010), (001)→(011), (010)→ (100),… ,etc. We define the 
BP change as (i j k)→(l m n) where │i−l│≤1, │j−m│≤1,│k−n│≤1, 
and 0<│i−l│+│j −m│+│k−n│≤3. Besides applying the temporal 
rule to identify the base template. For instance, if │i−l│≤1, 
│j−m│≤1, and │k−n│=0, and then we may identify the 1st and the 
2nd base templates by applying the spatial rules. For each content-
change keyframe, we may identify the base template in terms of its 
location, shape type, and hu / hu .  
V. CONCLUSIONS 
This report proposes a robust caption extraction, localization, 
recognition, and interpretation method for sports videos 
understanding. Our goal is to construct SCB template automatically 
and understand the on-going game stats descriptions from video 
caption. Firstly, the system can construct the SCB template 
automatically. Secondly, an effective SCB interpretation algorithm 
is proposed. Finally, the contextual description can be extracted by 
using the SCB template for the succeeding videos. It is a robust 
caption interpretation method for sports videos understanding. 
Experimental results demonstrate that the algorithm performs well 
and can be applied to different sports videos. 
五 參考文獻 
[1] S. Dasiopoulou, V. Mezaris, I. Kompatsiaris, V.-K. 
Papastathis, and M. G. Strintzis, “Knowledge-Assisted 
Semantic Video Object Detection,” IEEE Trans. CAS for VT, 
vol. 15, no. 10, pp. 1210–1224, Oct. 2005. 
[2] M. R. Naphade, I. V. Kozintsev, and T. S. Huang, “A Factor 
Graph Framework for Semantic Video Indexing,” IEEE Trans. 
CAS for VT., vol. 12, no. 1, pp. 40–52, Jan. 2002. 
[3] M. R. Naphade, S. Basu, J. R. Smith, C.-Y. Lin, and B. Tseng, 
“Modeling Semantic Concepts to Support Query by Keywords 
in Video,” in Proc. IEEE ICIP 2002, Rochester, Sep., 2002. 
[4] N. Babaguchi, Y. Kawai, T. Ogura and T. Kitahashi, 
“Personalized Abstraction of Broadcasted American Football 
Video by Highlight Selection,” IEEE Trans. Multimedia, vol. 
6, no. 4, pp. 575–586, Aug. 2004. 
[5] C.-H. Liang, W.-T. Chu, J.-H. Kuo, J.-Ling Wu, and W.-
Huang Cheng, “Baseball Event Detection using Game-specific 
Feature Sets and Rules,” in Proc. IEEE ISCAS 2005, Kobe, 
Japan, May. 23–26, 2005. 
[6] C G.M. Snoek and M. Worring, “Multimodal Video Indexing: 
A Review of the State-of-the-art,” Multimedia Tools and 
Applications, vol. 25, no. 1, pp. 5–35, 2005. 
[7] D. A. Sadlier and N.E. O’Connor, “Event Detection in Field 
Sports Video using Audio-visual Features and a Support 
Vector Machine,” IEEE Trans. Circuits Syst. Video Technol., 
vol. 15, no. 10, pp. 1225–1233, Oct. 2005. 
[8] H. C. Shih and C. L. Huang, “MSN: Statistical Understanding 
of Broadcasted Sports Video Using Multilevel Semantic 
Network,” IEEE Trans. Broadcasting, vol. 15, no. 4, pp. 449–
459, Dec. 2005. 
[9] G. Xu, Y.-F. Ma, H.-J. Zhang, and S.-Q. Yang, “An HMM-
based framework for video semantic analysis,” IEEE Trans. 
CAS for VT., vol. 15, no. 11, pp. 1422–1433, Nov. 2005. 
[10] C. Y. Chao, H. C. Shih, and C. L. Huang, “Semantics-based 
highlight extraction of soccer program using DBN,” in Proc. 
IEEE ICASSP 2005, Philadelphia, PA, Mar. 18–23, 2005. 
[11] D. Zhang, R. K. Rajendran, and S.-F. Chang, “General and 
Domain-specific Techniques for Detecting and Recognizing 
Superimposed Text in Video,” in Proc. IEEE ICIP 2002, Sep. 
22–25, 2002. 
