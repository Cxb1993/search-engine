 2
一、中文摘要 
近年來，自動監控系統研究已應用於特定區域
或場所，例如地鐵站，飛機場等。傳統偵測通常都
以人的視覺來判斷。這種監視方法是非常吃力，而
且不能夠同時監看每個攝影機。因此，使用智慧型
工具來協助偵測異常的情形是相當適合的。異常的
群眾移動情形如：打架、破壞公物、恐慌的移動、
以及在單行道中相反方向的移動等。人群的移動，
在開放空間中相對是較複雜的，且往往會被一些環
境的因素所影響。人群偵測是一個極具挑戰性的問
題，在許多影像中，行人通常不是太小，不然就是
在一群人中移動，或者複雜遮蔽的情形造成不能準
確的偵測。而且因為行人有不同的體型、顏色，同
時也必須考慮各種的背景，光線變化等影響偵測的
各種的情況。 
本計劃的研究動機就在於研發智慧型的數位影
像監控之群眾偵測及分析系統。本計畫內容可以分
成五大部分：（1）背景建立與移動物體偵測；（2）
移動物體切割、標記與比對；（3）群眾移動偵測；（4）
群眾移動路徑與方向估測；與（5）系統整合與測試。
我們結合了背景建構及動態自動調適性的更新方式
來分割出視訊物件，再依照各個視訊物件的各個區
域位置預測其移動範圍，建立出各個物件的完整移
動路徑。人形計數的部份，利用一人型資料庫，辨
識視訊物件是否為人形，之後再加以統計以便達到
群眾移動偵測之目的。我們發展的系統非常適合即
時運算，並在實際監視系統視訊影片測試，人形辨
識的正確辨識率(ture-positive)目前達 89%，誤判
率(false-positive)低於 8% 。 
 
關鍵詞：智慧型監控系統、影像處理、群眾偵測、
移動物體偵測、人形辨識。 
 
 
Abstract 
Automatic Surveillance System is an enabling 
technology that is in strong demand by large 
application sections, such as subway station or airport. 
Conventional approach of monitoring is due to visual 
observation from human operators. By using pure 
visual observation is rather difficult for the operators to 
collect information from different cameras and wide 
range of video sequences. Thus, an intelligent 
surveillance system is essential to assist for abnormal 
crowd behavior monitoring. Examples of abnormal 
crowd motion may include fights, vandalism, panic 
movement and opposing flow in one way corridors. 
Crowd motion is very complicated in the open area and 
is usually affected by various factors. This is a 
challenging problem as pedestrians come with small 
size, different shape and color, against cluttered 
background, occlusion with each other, and varying 
illumination conditions. This research is motivated by 
the above mentioned issues. We proposed to develop a 
crowd detection and analysis system for intelligent 
surveillance. The proposed project includes five parts: 
(1) background modeling and object motion detection, 
(2) motion object segmentation, labeling, and matching, 
(3) crowd motion detection, (4) motion direction 
estimation, and (5) system implementation, integration, 
and validation. We combine a background construction 
method and automatic adaptation updating capability to 
achieve proper segmentation of multiple video objects. 
Then the moving range of each object is predicted 
based on its position and motion. The recognition of 
pedestrian can be fulfilled by comparing a database of 
human silhouette features. The resulting system is very 
suitable for real-time application of surveillance. We 
have tested on the video sequence of DVR, the 
true-positive human identification rate is 89%, while 
t h e  f a l s e - p o s i t i v e  r a t e  i s  a s  l o w  a s  8 % .             
 
Keyword: Intelligent Surveillance System, Image 
Processing, Crowd Detection, Motion Object Detection, 
H u m a n  D e t e c t i o n ,  P e o p l e  C o u n t i n g .             
 
 
 4
景是否已取得較佳的背景影像等，設定等動態之閥
值來判定現階段該像素是否須被更新，其後視訊物
件之取得利用 Kim 和 Hwang 等改良之邊影像應用
[6]，用以提升物件的輪廓取得，再根據取得的前
景物件加以追蹤與紀錄路徑資料，以便於事後的人
型辨識、群眾移動偵測、群眾移動路徑與方向估
測，最後我們對多個影片進行測試且紀錄實驗結果
以利以後的分析。 
 
三、計劃執行結果與討論 
本研究內容分成五大部分：（1）背景建立與移
動物體偵測、（2）移動物體切割、標記與比對、（3）
群眾移動偵測、（4）群眾移動路徑與方向估測、（5）
系統整合與測試。研究方法與結果詳述如下： 
 
( 1 ) 背景建立與移動物體偵測 
移動物體偵測與切割(motion detection and 
segmentation)是電腦視覺應用的基本步驟，一般有
關移動物體的處理及辨識的各類探討，常見的不外
乎以背景影像建構或是連續畫面相減等方式為基
礎，做初步的影像物件萃取。我們利用背景影像建
構為基礎，輔以影像邊緣相減等方式來擷取影像物
件；利用兩者各自不同的特性來提升所擷取出來的
影像物件的完整性。整個基本架構如圖一所示。 
 
圖一、移動物件擷取流程圖 
 
所謂的背景，通常是指擁有相對於前景較為穩
定的畫素，短時間內變化不很劇烈，且至多在一定
的小範圍內變化的像素稱之為背景。本文中背景建
構的方式，是基於此一假定。如圖二所示，連續四
個 Frame((a)到(d))的影像差異主要是緣起於人的
移動，圖二(e)顯示(a)至(e)之間差值的累計。故整
個演算法在這個假設下所展開。 
  
(a)                 (b) 
  
(c)                 (d) 
 
(e) 
圖二、 (a) Frame12；(b) Frame13；(c) Frame14； 
(d) Frame15；(e) (a)-(e)之間差值的累計 
  
基礎背景建置的方式是取從輸入視訊畫面資
料一開始的連續 n 個 Frame 當作背景之初始集合，
如 SetBackground(x, y)={I0(x, y), I1(x, y), … , In(x, y)}，令
IBackground(x, y)為建構出來之背景，並取 IBackground(x, y)
等於 SetBackground(x, y)之平均值，同時計算每個背景
像素的變異數 vBackground(x, y)： 
 
( ) ( )),(, yxSetmeanyxI BackgoundBackground =             (1) 
( )
( ) ( )( )
n
yxIyxI
yx
n
i
Backgoundi
Background
∑
=
−
= 1
2,,
,ν
        (2) 
因此 IBackground(x, y)為建構出來之初始背景，
vBackground(x, y)為該背景每個點之變異數，我們也設
定變異數閥值 vThreshold(x, y)，一開始我們設定初始
值 vThreshold(x,y)=0，該數值留做更新背景之判定用。
在真實的環境下，要取得一個完全乾淨的背景是十
分困難的，系統在建置時絕少情況下是可以要求畫
面上一切物件清場的，即使能達成這樣的要求，隨
著時間的推移，日照的變化等因素，背景將不可避
免的由細微的差異逐漸累積成過大的差值，因此，
勢必背景有更新的必要。我們提出一個新的方法，
 6
( ) ( ) ( )( ) ( )( ){ }0 0 1 1 0 0 1 1, , , , , , ,in n nVO x y x y TLabel x y i TLabel x y j i j= ∀ = ∪ = =  (7) 
物件追蹤目前常用的技術大致可分為兩類，第
一類使用決策式法則，此類方法通常是一最佳化問
題，成本函數的選擇為其中重要的環節，但這個方
法的問題在於，若掉入一區域性最小值，將造成結
果發生錯誤。第二類是隨機統計式方法，使用隨時
間而變的序列資料，預測下個時間的狀態，常見的
方法為 Kalman filter，但此方法對於物體的模型有
一定的限制。我們將以 Particle filter 進行物件追
蹤。Particle filter 屬於第二類，採用貝氏機率的概
念，估測下一個時間的狀態，我們提出的物件追蹤
則利用最小質量差異(區塊面積)和最小運動向量
(區塊質心)差異為判定標準來比對: 
( )( , )  , , 11,
, , 11
1 min 1
kx y Object in framen
k n k nn n
k n
x y k nn
M MMV MVM
MMV
α α
∈
−−
−−
⎡ ⎤⎛ ⎞⎛ ⎞ −−= ⋅ + − ⋅⎢ ⎥⎜ ⎟⎜ ⎟ ⎜ ⎟⎢ ⎥⎝ ⎠ ⎝ ⎠⎣ ⎦∑
 (8) 
（3）群眾移動偵測 
人形辨識一直以來都是十分令人關注的問
題，各式各樣的辨識方式紛紛被提出，例如特徵抽
取、外形比對、運動規律性等。但在個別的辨識方
法中皆有其特定的條件限制，例如拍攝角度、距
離、特定的行徑方向、是否需正面人臉、或無法即
時運算等。然而，過於嚴謹的限制會侷限住演算法
的應用範圍，我們提出一種以輪廓比對為基礎且利
用資料庫比對的演算法，依靠更新資料庫的方式而
不須更動演算法主體，可應用於較廣的外在條件環
境的變化。 
在實際運用的時候，整個系統在初始化時將整
個資料庫讀取出，找出物件邊緣，針對畫面上各
VOni與人型資料庫中比對取得最相似之距離值，若
該值小於一定之閥值則將該將 VOni 定義為人形，
否則就判斷為非人；同時為了可以簡單分辨出一個
影像物件內是否有多個人形，我們再對影像物件進
行外型辨識時，使用了反覆性的辨識方式，如果此
物件有偵測出人形時系統會將此人形邊緣區塊移
除，接著在對同一影像物件進行辨識直到此影像物
件已沒有人形偵測出來，才會換到下一個影像物
件，如此我們便可多個人形辨識出來，當然此演算
法必須在建立在人形不會重疊太大面積的情況
下，流程如圖五所示。 
 
圖五、人形辨識流程圖 
伴隨著資料庫的運用，在不同的環境設定下直
接更替相對應之資料庫，則可達到不修改原始演算
法的情況下達到更新之效益，然而對於辨識率低弱
之特殊情形，亦可靠更新特定族群資料的方式達到
提升辨識率之效果。反之，若資料庫遠不敷測試情
況所使用時，誤判機率將會隨之明顯大幅提升，而
過大之資料庫則耗損過多系統資源與效能，然而資
料與資料之間亦不存在時間之關聯性，勢必會將侷
限後期之發展。 
在辨識出人形的同時，我們可針對畫面中所有
人形進行統計，如此便可達到所謂的群眾移動偵
測，亦指在此畫面中屬於人形的影像物件我們可以
計算與記錄以便於之後的群眾移動路徑與方向估
測。 
（4）群眾移動路徑與方向估測 
人群的偵測主要的需求是在人口密集的城市
裡，行人往往是在人群中移動。這些情形做分析是
非常困難的，因為沒有個別的行人可以準確的被分
割出來，路上行人是非常混亂的，背景也不是單一
個物體，因此從多個物體中分辨人類是很困難的。
群眾監看通常分為正常及不正常行為監看，例如正
常群眾移動的相關情形，或不規則的人群相關動
作。不規則的群眾移動情形如：打架，爭執，破壞
公物，恐慌的移動以及在單行道中相反方向的移
動。 
我們將本畫面中所有的影像物件 VOni進行人
形辨識，同時也經由計算前一張影像的影像物件
VOn-1i與目前的影像物件VOni之差值以求出物件移
動向量；將判定為人形的影像物件不斷地記錄此物
件之質心位置，最後則統計出目前影像 n 中所有的
人形影像物件之移動向量與移動路徑，流程如六所
示： 
 8
所提之背景建構之物件切割方式是較為優異的。 
Test Data 1
0
0.2
0.4
0.6
0.8
1
1.2
1 16 31 46 61 76 91 106 121 136
Frame
本文方式
比對方式
 
表一、測試影片一之分析統計結果與比較 
 
Test Data 2
0
0.2
0.4
0.6
0.8
1
1 14 27 40 53 66 79 92 105 118 131 144
Frame
本文方式
比對方式
 
表二、測試影片二之分析統計結果與比較 
 
人形辨識之實驗，我們利用自行拍攝之校園某
一道路之影片，將測試影片中逐頁所抽取出之視訊
物件，先剔除過小(15x20)或是溢出銀幕範圍之物
件，之後將由人工計數之正確人數及物件，跟系統
所判定之人數及物件作比較計算其正對率 89% 
(true positive TP)、正錯率 11% (true negative TN)、
負對率 8% (false positive FP)、負錯率 92% (false 
negative FN)之各數值作評估。 
 
表三、人形辨識之正確率 
1000 Frame T F 
P 996 122 
N 73 446 
 
我們也進行了這四大項的整合如圖十所示，九
宮格左上角為移動物體偵測結果，並且標示 ID 及
追蹤，左上角為人形偵測的數量與移動物件偵測的
數量，其他畫面從左到右，從上到下依序為 :
“2:DR＂與背景相減的結果；“3:CSE＂輸入畫面
之邊緣影像；“4:NB＂背景相減結果與輸入畫面
做 AND 運算之結果；“5:BGMask＂背景更新之遮
罩；“6:BY＂背景影像；“7:CGT＂目前移動物體
的邊緣；“8:TMask＂  前景影像偵測結果；
“9:CEstS＂人型資料庫比對結果。利用偵測出來
的人形影像物件可實現所謂的：群眾移動偵測、群
眾移動路徑與方向估測。 
左上角的畫面中移動物體若被標示為 P1 時則
表示此移動物體為人形，後面為此物體的 ID。 
 
圖十、移動行人偵測系統 
 
四、 參考文獻 
 
[1] J.W. kim,Kang-Sun Choi,Byeong-Doo 
Choi,and Sun-Jea Ko. Real-time vision-based 
people counting system for the security door. 
Proc .of 2002 International Technical 
Conference On Circuits Systems Computers 
and Communication, July 2002. 
[2] Elena Stringa and Carlo S. Regaz zoni. 
Real-time video-shot detection for scene 
surveillance applications. IEEE Transactions on 
Image Processing, 9(1):69-79, January 2000.                 
[3] Ismail Haritaoglu, David Harwood, and Larry S. 
Davis. W4:real-time surveillance of people and 
their activities. IEEE Transactions on Pattern 
Analysis and Machine Intelligence ，
22(8):809-830，August 2000. 
 10
video object segmentation and tracking for 
content-based applications, IEEE Transactions 
Circuits and Systems for Video Technology, 
12(2):122 - 129, Feb. 2002. 
[20] Deepak Turaga, Mohamed Alkanhal. An 
Efficient Three-Step Search Algorithm for 
Block Motion Estimation, MidTerm project 
18-899, Spring, 1998. 
[21] Tao Yang, Quan Pan, Stan Z. Li, Jin Li. 
Multiple layer based background maintenance 
in complex environment. mage and Graphics, 
2004. Proceedings. Third International 
Conference on 18-20 Dec. 2004 Page(s):112 - 
115  
[22] http://archer.ee.nctu.edu.tw/contest/ 
五、計畫成果自評  
    本研究內容與原計畫相符，達成預期
五大項目，研究成果具前瞻之學術及應用
價值，適合在學術期刊發表或申請專利。 
表 Y04
行政院國家科學委員會補助國內專家學者出席國際學術會議報告
95 年 9 月 20 日
報告人姓名 林道通 服務機構
及職稱
國立台北大學資訊工程系
副教授
時間
會議
地點
9/10—9/14, 2006
希臘雅典
本會核定
補助文號
NSC 95-2221-E-305 -006
會議
名稱
(中文) 2006 年國際類神經網路研討會
(英文)International Conference on Artificial Neural Network
發表論文
題目
Human Facial Expression Recognition Using Hybrid Network of PCA and
RBFN
報告內容應包括下列各項：
一、參加會議經過
今年ICANN2006於九月十日至十四日在希臘雅典舉行，共有來自三十四個國家共208
篇論文發表。ICANN 自 1991 年舉辦至今已歷史十五年，此會議已經成為歐洲地區非
常重要的盛事之一。今年大會由希臘國立雅典科技大學之 Intelligent Systems
Laboratory 及 Image, Video and Multimedia System Laboratory 主辦。包括七場
tutorial,八場 keynote speech, 二十一場 regular session, 十場 special
session.
二、與會心得
今年 ICANN2006 有兩大特色:
(一)論文接受率僅 43.7:今年共有 475 篇投稿，接受 208 篇,因此水準在一般以上
(二)主辦單位特別安排一場有關歐盟 European Commission 的共同研究計畫 EU
Cognitive System Initiative 主持人暢談歐盟如何整合十數個大國共同在
Cognitive System 進行研究，鎖定四大目標 Robust, Adaptive, Effective,
Natural 並整合各種領域整合認知科學和人機系統。會後更安排了一系列的相
關 workshop: Blending Intelligent Multimedia, Semantic Web Technologies
and Artificial Neural Networks; Application Field: E-Culture,
Intelligent Affective Interaction: Technologies and Applications,
Cognitive Systems
三、建議
目前歐盟正積極籌備一些跨國的大型研究計畫，企圖發揮歐盟的力量，讓歐洲國家
的科技水準超越美國及日本，值得我們注意。
四、攜回資料名稱及內容
論文集兩冊,S. Kollias et al. (Eds.): ICANN 2006, Part I&II, LNCS 4131, 4132
附
件
三
Human Facial Expression Recognition 625
Fig. 1. Examples of seven principal facial expressions in JAFFE [20]: joy, disgust,
anger, surprise, fear, neutral, and sadness (from left to right)
Facial expression recognition from still images is a more diﬃcult problem than
from video sequences due to the fact that less information during expression
actions is available [9]. Cottrell and Metcalfe [10] applied PCA and backpropa-
gation neural network to recognize facial expression, gender, and identity from
static images. Chen and Huang [9] modiﬁed linear discriminate analysis (LDA)
algorithm and presented a new clustering based feature extraction method for
facial expression recognition. A constructive feed-forward neural network was
further proposed for facial expression recognition with pruning technique by
Ma and Khorasani [11]. Other approaches such as multiple discriminate analy-
sis, ICA, Adaboost, Fisher Weight Maps, Appearance model etc. can also been
found in the recent literatures [12,13,14,15,16,17,18,19].
In this paper, we are concerned with automatic classiﬁcation of facial ex-
pressions from still images. The psychologists have indicated that as least six
emotions are universally associated with distinct facial expressions. Examples
of Japanese Females Facial Expression (JAFFE) databases are shown in Fig. 1.
The rest of this paper is arranged as follows. Section 2 illustraes the main com-
ponents of the proposed system. We utilized PCA in the pre-processing stage to
extract features from face imagery. We further proposed a hybrid model of radial
basis function network and PCA re-construction network to fulﬁll the facial ex-
pression diﬀerentiation task. In section 3, the experimental results are presented.
Finally, conclusion remarks are drawn in section 4.
2 Hybrid Network Model
2.1 Radial Basis Function Network
The radial basis function neural network (RBFN) theoretically provides a suf-
ﬁcient large network structure such that any continuous function can be ap-
proximated to within an arbitrary degree of accuracy by appropriately choosing
radial basis function centers [21]. The RBFN is trained using sample data to
approximate a function in multidimensional space. The RBFN is a three-layered
Human Facial Expression Recognition 627
Input
Images
Expression a
Eigen Space
Expression b
Eigen Space
Expression f
Eigen Space
Expression g
Eigen Space
.
.
.
Expression a
Reconstruction
Expression b
Reconstruction
Expression f
Reconstruction
Expression g
Reconstruction
.
.
.
Similarity
Measure Output
Fig. 2. Classiﬁcation procedure of PCA reconstruction
Input
Images RBFN
.
.
.
Class 1
Class k
PCA
Projection
Expression 1 (1)
Re-construction
Expression 1 (n 1)
Re-construction
Expression k (1)
Re-construction
Expression k (n k)
Re-construction
.
.
.
.
Fig. 3. Schematic block diagram of the proposed hybrid model
from other eigen vectors of training expressions. Based on this episode, we di-
vide the training set into seven classes according to diﬀerent expression and
compute the eigen space of each class. For a test face image, we ﬁrst project it
onto the eigen space of each class independently and then derive reconstructed
image from each eigen space. By measuring the similarity (mean-square error)
between input image and the reconstructed image of each class, we can identify
the class of input image whose reconstructed image is most similar to the input
one. The procedure of the developed PCA reconstruction method is delineate
in Fig. 2.
2.3 Hybrid Architecture
The cognitive and emotional states of a person can be correlated with visual
features derived from images of the mouth and eye regions [23]. Many researches
Human Facial Expression Recognition 629
(a) Type A – face images.
(b) Type B – eyes and lips images.
Fig. 4. Sample images extracted from JAFFE database: (a) Type A – face images, and
(b) Type B – eyes and lips images
Table 1. Classiﬁcation rate (%) of PCA reconstruction method
Data type top one match top two matches top three matches
Type A - face 84.83 90.95 93.81
Type B - lips 78.57 88.10 91.90
Type B - eyes 83.33 90.95 93.33
Table 2. Classiﬁcation rate (%) of single stage RBFN with PCA pre-processing method
Data type top one match top two matches top three matches
Type A - face 73.81 87.14 93.33
Type B - lips 64.29 84.29 90.95
Type B - eyes 76.19 89.52 92.86
preliminary clustering task. The confusion matrices of the single stage RBFN
top one match test results are recorded in Table 3. As we can see from the con-
fusion matrices, not all expressions were equally well recognized by the system.
Expressions ”fear” and ”neutral” were often mis-classiﬁed to other classes. Be-
sides, expressions ”sadness”, ”disgust”, ”anger” were often mis-classiﬁed with
each other. ”Joy” is seldom mis-classiﬁed by other expressions. Thus, we intend
to divide the ﬁrst stage classiﬁcation into various categories by grouping several
Human Facial Expression Recognition 631
Programming method and achieved 93.8% recognition rate based on 21 two-class
classiﬁers [16]. The evaluation scheme is diﬀerent than ours. As we can observe
from Table 4, our method is very competitive and outperforms the previous
methods.
4 Conclusion
In this paper, we proposed a hybrid model combining PCA feature re-construc-
tion and RBFN method to tackle facial expressions recognition problems. From
our experimental results, this cascade strategy works properly. As we can see
from the records, our method is very competitive. We learn that it is possible to
re-organize the training scenarios and construct a multi-layer hybrid network for
further investigation and improvement. Thus, we arrange the ﬁrst stage classi-
ﬁcation by dividing into several groups of categories using local features of eyes
images. Later, train the face image individually for each classiﬁer to perform
less intensive identiﬁcation task in the second stage. Various arrangements have
been veriﬁed. From these results, we conclude the local face features is useful for
diﬀerentiating expressions when a more appropriate classiﬁer is arranged. In the
future, we can adopt a more sophisticate learning algorithm of neural network
and fuzzy theory to induce the best combination of sub-category in the ﬁrst
stage. Leverage of the recognition performance can be expected. Furthermore,
more work will be conducted to validate the technique on other databases such
as CMU AMP Face Expression Database [26], POFA image set of Ekman and
Friesen [27], and other emotion research databases [28].
Acknowledgement
This work was supported in part by National Science Council. Taiwan, R.O.C. un-
der grants NSC 88-2213-E216-010 and NSC 89-2213-E216-016.The author would
like to thank Mr. De-Cheng Pan for his help in simulations and experiments.
References
1. S. Ioannou, A. Raouzaiou, V. Tzouvaras, T. Mailis, K. Karpouzis, and S. Kol-
lias. Emotion recognition through facial expression analysis based on a neurofuzzy
network. Neural Networks, 18:423–435, 2005.
2. N. Fragopanagos and J. Taylor. Emotion recognition in human-computer interac-
tion. Neural Networks, 18:389–405, 2005.
3. Y. Yacoob, H.M. Lam, and L.S. Davis. Recognizing faces showing expressions. In
International Workshop on Automatic Face and Gesture Recognition, Zurich, 1995.
4. M. Bartlett, P. Viola, T. Sejnowski, L. Larsen, J Hager, and P. Ekman. Classifying
facial action. In M. Mozer D.S. Touretzky and M. Hasselmo, editors, Advances in
Neural Information Processing Systems, volume 8. The MIP Press, 1996.
5. P. Ekman and W. Friesen. The facial action coding system. Consulting Psycholo-
gists Press, 1965.
