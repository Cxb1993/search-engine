II 
Abstract 
 
With the advance of technologies, the demand of audio and video presentations migrates from 
mono channel and black/white display into multiple channels and 3 Dimensional (3D) 
high-definition color display, respectively. Recently, many research groups and technical 
committees extensively develop and propose 3D audio and video technologies that are realized in 
various entertainment applications to allow users dip in an interactive and augmented world. 
 
This integrated project is to investigate an interactive free-view 3D audio and video displaying 
system which includes five sub-projects: (1) 3D audio acquisition, processing and interactive 
presentation, (2) multi-channel video acquisition and processing, (3) multi-channel video codec, (4) 
multi-channel video rendering and interactive presentation and (5) 2D/3D video conversion. First, 
3D audio-visual information of a singer at the stage is recorded under 16 microphones and 13 
cameras. Second, the recorded audio and video data are efficiently pre-processed, modeled and 
compressed with consideration of multi-view presentation. Based on the multi-channel compressed 
data, an effective interactive system is developed to provide 3D audio and video free-view 
presentation. By employing the proposed system, high-quality audio-visual singing performance 
can be listened and watched more transparency and reality at multiple view points. 
 
Keywords: MPEG surround, head related transfer function, blind source separation, adaptive 
whitening, 3D reconstruction, computer vision, 3D TV, stereoscopic video, multi-view video, free 
viewpoint video, video compression, depth image, 3D motion compensation, 3D mesh, 2D/3D 
video converter, depth estimation、stereo image synthesis 
 
 
 
 
 
 
 
 
 
 
IV 
3.3.3 物體之三維模型重建 ......................................................................................................29 
3.4 結語 ..........................................................................................................................................36 
第四章 子計畫三：多視域視訊整合表示法之資料壓縮技術......................................................37 
4.1 前言 .........................................................................................................................................37 
4.2 本期第一年度 (96/8~97/7) 研究方法 ...................................................................................39 
4.2.1 系統架構 ..........................................................................................................................39 
4.2.2 DIBR BASED INTER-VIEW PREDICTION ...............................................................................39 
4.2.2.1 GLOBAL COMPENSATION MODEL .................................................................................41 
4.2.2.2 LOCAL COMPENSATION MODEL ...................................................................................41 
4.3 第二年度 (97/8~98/9) 研究方法: 動態 3D 網格結構壓縮技術..........................................42 
4.3.1 應用 ICP 技術之 INTER-MODEL 預測............................................................................43 
4.3.1.1 三維動態估測 (3D MOTION ESTIMATION)................................................................43 
4.3.1.2 ICP (ITERATIVE CLOSEST POINT) 法則 .......................................................................43 
4.3.2 改良式動態 3DMC 系統.................................................................................................45 
4.3.2.1 改良式動態 3DMC 系統架構 .................................................................................45 
4.3.2.2  K-MEANS 分群 .........................................................................................................46 
4.3.2.3 空間-時間搜尋(SPATIO-TEMPORAL SEARCH) ............................................................46 
4.3.2.4 局部編號搜尋 (LOCAL INDEX SEARCH) ....................................................................47 
4.3.2.5 改良式動態 3DMC 系統綜整 .................................................................................47 
4.4 實驗結果 .................................................................................................................................48 
4.4.1 本期第一年計畫成果 .......................................................................................................48 
4.4.1.1 立體視訊編碼器實驗條件 ........................................................................................48 
4.4.1.2 MODE SELECTION OF DIBR-BASED INTER-VIEW PREDICTION CODING.........................48 
4.4.2 本期第二年計畫成果 ......................................................................................................50 
4.4.2.1 動態三維網格模型的取得 .......................................................................................50 
4.4.2.2 傳統 3DMC 技術與改良式動態 3DMC 技術之比較 ...........................................50 
4.5 結語 .........................................................................................................................................54 
第五章 子計畫四：多視域視訊之影像內差技術與自由視角立體播放應用 .............................55 
5.1 前言 ..........................................................................................................................................55 
5.2 研究目的 ..................................................................................................................................55 
5.3 相關文獻探討 ..........................................................................................................................56 
5.3.1 影像式新視角影像合成 (IMAGE-BASED RENDERING) ....................................................57 
5.3.1.1 影像形變 (VIEW MORPHING) ....................................................................................57 
5.3.2 深度影像式新視角影像合成 (DEPTH IMAGE BASED RENDERING) ................................58 
5.3.2.1 二維到三維轉換 (2D TO 3D CONVERSION) ..............................................................58 
5.3.2.2 深度影像平滑化 (DEPTH IMAGE SMOOTHING) .........................................................59 
5.3.3 深度影像的壓縮 (DEPTH IMAGE COMPRESSION).............................................................59 
5.4 研究方法 .................................................................................................................................60 
5.4.1 DIBR 所遭遇的問題與解決方法.....................................................................................60 
5.4.2 散射填補 (SCATTER FILTERING) ......................................................................................60 
5.4.3 考量深度資訊的投射 ( DEPTH PRIORITY-BASED PROJECTION).......................................60 
5.4.4 遮蔽區域的填補 (OOCCLUSION FILLING) .......................................................................61 
5.4.5 邊緣遮蔽擴散 (BOUNDARY OCCLUSION DILATION)........................................................61 
5.4.6 合成視訊的深度不一致性 (DEPTH CONSISTENCY PROBLEM IN SYNTHESIS VIDEO) ......62 
5.5 實驗結果 ..................................................................................................................................63 
5.5.1 MICROSOFT 提供影像 .......................................................................................................63 
VI 
圖目錄 
 
圖 2.1 3D AV 音訊處理、儲存與音場重建流程圖 ..........................................................................3 
圖 2.2 多輸入多輸出盲蔽訊號分離的架構圖..................................................................................5 
圖 2.3 GSO 正交集概念 .....................................................................................................................6 
圖 2.4 語音(上)與音樂(下)的原始訊號時域波形 ............................................................................6 
圖 2.5 混合訊號 1X (上)與 2X (下)的訊號時域波形 .........................................................................7 
圖 2.6 經過 GSO 且單位化之 1U (上)與 2U (下)的訊號時域波形 ...................................................7 
圖 2.7 以麥克風陣列進行音源定位的組態示意圖..........................................................................9 
圖 2.8 MPEG SURROUND 架構圖 ................................................................................................10 
圖 2.9 3 維空間音源插補示意圖......................................................................................................10 
圖 2.10 不同視角的音場對應圖......................................................................................................11 
圖 2.11 5.1 聲道音場示意圖.............................................................................................................12 
圖 2.12 自由視角 5.1 聲道音場重建示意圖...................................................................................12 
圖 2.13 自由視角 7.1 聲道音場重建示意圖(前置環場增強)........................................................13 
圖 2.14 自由視角 7.1 聲道音場重建示意圖(後置環場增強)........................................................13 
圖 2.15 錄音環境之三維空間示意圖..............................................................................................13 
圖 2.16 錄得混合訊號及經過前處理的混合訊號與模擬合成的混合訊號頻譜 .........................14 
圖 2.17 二支麥克風擺放的示意圖..................................................................................................15 
圖 2.18 四支麥克風擺放的示意圖..................................................................................................15 
圖 2.19 影音同步錄製之攝影棚環境..............................................................................................17 
圖 2.20 16+1 音軌應用於 MPEG SURROUND 架構示意圖........................................................18 
圖 2.21 實際錄音資訊與插補資訊之間的比較..............................................................................18 
圖 2.22 自由視角選擇音場示意圖..................................................................................................19 
圖 2.23 自由視角立體聲音場重建示意圖......................................................................................19 
圖 2.24 重建立體聲音場之三種實施例..........................................................................................20 
圖 2.25 聲波同相重疊示意圖..........................................................................................................20 
圖 2.26 示範影片四種視角之相應音場示意圖..............................................................................20 
圖 2.27 重建後的立體聲音場..........................................................................................................21 
圖 3.1 將各個 VOXEL 投影回 2D 剪影影像 ................................................................................23 
圖 3.2 使用八張「貓的虛擬影像」重建之結果，右側為頭部的特寫。 ...................................23 
圖 3.3 RAY TRACING 示意圖 .......................................................................................................24 
圖 3.4 M’上的 DEPTH MAP 示意圖..............................................................................................25 
圖 3.5 DATA MAP 示意圖 ..............................................................................................................26 
圖 3.6 三維模型(左圖)與重新取樣之模型(右圖) ..........................................................................26 
圖 3.7 真實模型與三維座標資訊與色彩資訊重新取樣比較........................................................26 
圖 3.8 原始影像與重建模型比較....................................................................................................27 
圖 3.9 RGB MAP..............................................................................................................................27 
圖 3.10 重新取樣後模型距離 R 的 HISTOGRAM 統計結果 .....................................................28 
圖 3.11 攝影機環場擺設示意圖。..................................................................................................28 
圖 3.12 用於小物體的攝影機環場擺設..........................................................................................28 
圖 3.13 各攝影機視角拍攝影像及濾除背景的影像......................................................................29 
圖 3.14 一個模擬攝影機擺設系統及參數配置的設定介面..........................................................29 
圖 3.15 CAT 虛擬相機拍攝圖 .........................................................................................................30 
VIII 
圖 4.26 ROBOT -FRAME 2 壓縮重建後 3D 網格 .......................................................................52 
圖 4.27 ROBOT -FRAME 3 原始 3D 網格 ....................................................................................52 
圖 4.28 ROBOT -FRAME 3 壓縮重建後 3D 網格 ........................................................................52 
圖 4.29 ROBOT -FRAME 4 原始 3D 網格 ....................................................................................52 
圖 4.30 ROBOT -FRAME 4 壓縮重建後 3D 網格 ........................................................................52 
圖 4.31 ROBOT -FRAME 5 原始 3D 網格 ....................................................................................52 
圖 4.32 ROBOT -FRAME 5 壓縮重建後 3D 網格 ........................................................................52 
圖 4.33 在 E1 量度下兩種方法之 R-D 表現 (CHICKEN) ...........................................................53 
圖 4.34 在 E2 量度下兩種方法之 R-D 表現 (CHICKEN) ...........................................................53 
圖 4.35 “CHICKEN” FRAME 2 改良式 3DMC 壓縮重建後之 3D 網格 ................................53 
圖 4.36 “CHICKEN” FRAME 2 之 MPEG-4 3DMC 壓縮重建後之 3D 網格 ........................53 
圖 5.1 BLOCK DIAGRAM OF 3DAV SYSTEM ARCHITECTURE........................................56 
圖 5.2 VIEW MORPHING 示意圖................................................................................................57 
圖 5.3 2D+DEPTH METHOD ........................................................................................................58 
圖 5.4 景物影像................................................................................................................................58 
圖 5.5 對應的深度影像....................................................................................................................58 
圖 5.6 有空洞的左眼合成影像........................................................................................................58 
圖 5.7 內插填補後完整的左眼合成影像........................................................................................58 
圖 5.8 方向性高斯模糊深度影像....................................................................................................59 
圖 5.9 採用模糊深度影像合成的左眼影像....................................................................................59 
圖 5.10 原始深度影像......................................................................................................................59 
圖 5.11 深度影像分割之結果..........................................................................................................59 
圖 5.12 文獻[53]中合成影像之結果 ...............................................................................................59 
圖 5.13 未散射填補之結果..............................................................................................................60 
圖 5.14 散射填補後之結果..............................................................................................................60 
圖 5.15 未參考深度資訊投射之結果..............................................................................................60 
圖 5.16 參考深度資訊投射之結果..................................................................................................60 
圖 5.16 攝影機遮蔽區......................................................................................................................61 
圖 5.17 填補前的合成影像..............................................................................................................61 
圖 5.18 填補後的合成影像..............................................................................................................61 
圖 5.19 物體邊緣瑕疵......................................................................................................................61 
圖 5.20 邊緣遮蔽擴張......................................................................................................................61 
圖 5.21 空白填補之結果..................................................................................................................61 
圖 5.22 CAM4 彩色影像 (T=0).......................................................................................................62 
圖 5.23 CAM4 彩色影像 (T=1).......................................................................................................62 
圖 5.24 CAM4 深度影像 (T=0).......................................................................................................62 
圖 5.25 CAM4 深度影像 (T=1).......................................................................................................62 
圖 5.26 背景相減法..........................................................................................................................63 
圖 5.27 閉合二次結果.......................................................................................................................63 
圖 5.28 CAM4 T=0 ...........................................................................................................................63 
圖 5.29 CAM4 修正 T=1 ..................................................................................................................63 
圖 5.30 深度未修正後投影到 CAM4.............................................................................................63 
圖 5.31 深度修正後投影到 CAM4.................................................................................................63 
圖 5.32 靜態的“BREAKDANCERS”視角移動序列影像 ............................................................64 
圖 5.33 虛擬攝影機位置 1...............................................................................................................65 
X 
表目錄 
表 2.1 訊號源、混合訊號以及正交混合訊號之相關係數比較......................................................7 
表 2.2 COMPARISON OF THE SEPARATED RESULTS MEASURED BY SIR ..................14 
表 2.3 不同改善方案與它種分離演算法分離效能 SIR 值比較 ...................................................15 
表 2.4 不同改善方案與它種分離演算法分離效能 MSE 值比較 .................................................16 
表 2.5 不同改善方案與 FAST ICA 的分離效能 SIR 值比較 ......................................................16 
表 2.6 不同改善方案與它種分離演算法效能 MSE 值比較 .........................................................16 
表 5.1 METHODS FOR FREE VIEW IMAGE GENERATION AND THEIR FEATURES [42]
............................................................................................................................................................56 
表 5.2 多視角影像相關參數............................................................................................................64 
2 
用 MPEG Surround 做為 16 加 1 音軌之多通道音訊壓縮來以利傳輸。最後發展出 3D 音視訊
多媒體系統中自由視角之音場重建技術，除了利用 HRTF 3D sound 對立體聲加強其空間感與
音源定位外，也對 5.1 聲道播放系統定義出重建相應音場之方式。 
 
子計畫二的研究以三維視訊內容的建構為主，探討利用多視域(multiple viewpoint)的影
像擷取方式，開發適當的景物或是影像的表示法，以利於包括視訊資料的壓縮、處理與製作，
以及自由視角播放(free-viewpoint playback)或立體顯示(stereoscopic display)等用途。因此除了
負責視訊資料的即時擷取之外，子計畫二於群體計畫中所扮演的角色還包括兩大部分，以電
腦視覺的角度進行多視域視覺系統設定與校正以及利用電腦圖學的角度來思考影片播放時
所需具備的各項功能。 
 
子計畫三研究開發雙眼立體視訊壓縮技術 (作為多視域視訊壓縮技術的初步研究，並可
應用於 3D TV 上)、多視域視訊壓縮技術 (用以提供自由視角視訊呈現所需之原始視訊內容) 
以及多視域視訊整合 Omni-Directional Video(ODV)表示法的視訊壓縮 (包括了時間與空間上
隨機存取機制的設計)，並以本子計畫為橋樑，結合其餘子計畫，串連出一個立體多視域視
訊互動式呈現系統。 
 
子計畫四研究由多視域影像中建立出任意視角影像，以達到自由視角的播放需求，在子
計畫四中，因應使用者所要求的觀視角度，其中的關鍵就在於虛擬影像合成的技術。而近幾
年的研究則提出了一個新的虛擬影像合成技術，稱為深度影像式合成 (Depth Image-based 
Rendering)技術。深度影像式合成是利用彩色影像和所屬的深度影像來合成，搭配攝影機參數
投射產生虛擬影像。深度影像式合成在影像品質上優於影像式合成，加上複雜度遠低於比模
型式合成低，已成為近幾年研究的一項主要技術。計劃中也針對 DIBR 幾項問題提出解決方
案，如空洞、遮蔽區域、深度的優先順序、邊緣問題，在實驗中也得到不錯的影像結果。 
 
子計畫五之研究目標為開發 2D 視訊轉換成 3D 立體視訊之技術研發，可以將舊有的單通
道 (Mono) 視訊轉換為立體 (Stereo) 通道視訊，或較便宜的設備來擷取立體視訊，讓使用者
可在 3D TV 上進行立體觀賞。到目前為止，已經分析探討四種不同類型的特徵用於深度資
訊之估計。為了整合這四種特徵，使用最小平方誤差法求出相對應之權重。透過這些權重可
以整合這四種不同類型的特徵，估計出深度資訊。配合背景深度指派，則可以獲得完整之深
度資訊，進而轉換獲得視差圖。此外，主觀實驗評估顯示，基於 DIBR 技術下，本計畫產生
3D 視訊可以具有接近真實 3D 視訊的視覺品質。 
 
 
 
 
 
 
4 
2.2 研究方法 
2.2.1 3D AV 音訊錄製之研究 
建構環繞音場的第一步就是要對音訊的錄製，在此節針對錄音的設備、環境及方法給予
詳盡的介紹，以利後續的音源分離、插補、定位、壓縮及傳輸。 
2.2.1.1 低音之錄製 
關於低音錄製的部分，由於其與一般的音源比較不同，主要是取其 20Hz~200Hz 頻段的
音源做為低頻成份，也是將來呈現 5.1 或 7.1 聲道的 LFE(Low Frequency Enhancement)聲道輸
出部份。所以本子計畫在進行 3D 音訊錄製的實驗時，另外使用了在低頻時具有較良好頻譜
響應的麥克風放置於其中一處位置；其低頻時具有較良好頻譜響應的麥克風除了與其他 15 支
麥克風進行同步擷取全頻域音訊以外，我們將利用人耳聽覺對於低頻方向性較為不敏銳，且
人類聽覺模型僅能接收能量較高的低頻的特點，在後處理時將其上述低頻時具有較良好頻譜
響應麥克風之音軌的低頻成分取出並加以放大，成為含有 16 軌全頻域、以及一軌重低音輸
出，共計 17 軌的 3D 音訊源擷取環境。 
2.2.1.2 後方音場之錄製 
通常在聆聽環繞音效時，除了前面的聲道(Left、Right、Center)能將音源及定位資訊傳達
給使用者，然而主要提供進一步的聆聽效果則是依靠前一段所述的重低音聲道以及後方聲場
(包含 Left Surround、Right Surround)。而後方音場的來源有不同的產生方式，有依靠後製作
或是人為合成而產生者，也有在進行錄音時即擺設專為接收後方音場之反射、繞射或後方音
源成分的麥克風[1]。本子計畫採用後製作的方式，由於表演者位於 16 支麥克風環場架設之
中心點，所以我們將後方音場經前場錄製到的音訊做合理的計算與推估，再合成為後方音場，
希冀藉此一方式收錄 3D 音訊中音場的其它成分，必能增加後方音場的臨場感。而後音場之
製作於錄音時亦使用有限數量麥克風接收音訊，對於自由視角(free-view point)所需但麥克風
錄音角度沒接收到的方向，我們將對音源利用合成與插補的方式產生。 
2.2.2 音源的分類與分離 
當音源擷取的工作完成後，則進行音源的分離與音源的分類。音源分離的理論基礎主要
為獨立成份分析(independent component analysis, ICA)，以找出隨機變數或訊號中隱藏因子的
統計及分析方法。假設模型裡的資料變數是由潛在變數以線性或非線性的方式組合而成，但
組合方式未知，這些變數也假設為非高斯分佈且互相獨立，我們的目標就是找出這些獨立成
份[2][3]，並且應用在調適性盲敝訊號分離演算法(Adaptive blind source separation )以對麥克風
所錄製的音源成份進行分離。 
2.2.2.1 音訊的分類 
由於直接對聲音信號進行分離的效果並不是很好，因此需針對音源訊號的種類做分類處
6 
2.2.2.4 改善分離效能之方案 
混合信號的前處理佔了極為重要的角色，根據先前分離演算法的實驗證明，使用不同的
前處理方法可以使分離效能得到不同的結果。為了提升盲蔽訊號分離結果的可辨識度，本計
畫除了 2.2.2.2 節伸展脈波的前處理，再提出了數種前處理的方案，希望能對分離效能更有所
改善。 
• Gram-Schmidt 正交化 
Gram-Schmidt 正交化(Gram-Schmidt Orthogonalization)亦稱 GSO，其目標為將多維的隨機
向量透過逐步削減彼此投影分量的方式，使得隨機向量間彼此互為正交成為正交集；經 GSO
處理後的多維隨機向量再經單位化(Normalization)成為正交集，且經白化處理之後，就能夠提
高各訊號彼此之間統計上的獨立性。GSO 單位正交的概念為圖 2.3： 
},...,,,{ 321 DXXXX },...,,,{ 321 DUUUU },...,,,{ 321 DEEEE
 
   
圖 2.3 GSO 正交集概念 
 
如果訊號源 S 彼此間的獨立性夠高，有機會將某些訊號源較完整的分量提早求出，也
就是某些 iU 的波形會與原訊號 jS 具有相當高的相關性。下面將示範一組混合訊號，分別是
音樂與語音的混合訊號，驗證 GSO 使得混合訊號在未經過白化與分離演算法前有機會提早
接近信號源。 
 
 
圖 2.4 語音(上)與音樂(下)的原始訊號時域波形 
 
8 
其中 2
1−Λ 由分量方式運算(Componentwise operation)得到，如式(2.2)，除了特徵值分解法之外，
奇異值分解法(Singular Value Decomposition, SVD)也成為求出其它種的白化矩陣的方法，我們
首先對 XXR 進行奇異值分解，將會得到 U，S，T 三個矩陣如式(2.3)；其中 U，S，T DR∈ ，S
是對角成分呈遞減之對角矩陣，U 和 T 為單式矩陣(Unitary Matrix)，白化矩陣可寫為式(2.4)： 
                          )(],,[ TXXRSVDTSU =                          (2.3) 
                            TUTSV 2
1−=                                (2.4) 
另外也可以透過對混合訊號做奇異值分解而求出白化矩陣，如式(2.5)和(2.6)： 
                             )(],,[ TXSVDTSU =                            (2.5) 
 
                          TVSV 1−=                                  (2.6) 
 
白化的目的除了使混合訊號彼此間不相關之外，其另一層背後的意義就是要確保分離演算法
不論在什麼時候都能使得 *Y 之間互為正交，即彼此不相關，而最後又要使得輸出信號的自相
關函數 YYR 等於訊號源的自相關函數 IRSS = 。由於演算法到最後求出的 *W 還要再乘上白化矩
陣才真正成為 W，而 *W 又是一個正交矩陣，盲蔽訊號分離系統獨立性驗證的推導： 
                         
SS
T
TTT
TT
T
XX
T
YY
R
I
IWW
WVXXVEW
WXXWE
WWR
YYER
=
=
=
=
=
=
=
      
      
      
][      
][      
      
][
**
**                             (2.7) 
實際上，白化矩陣很可能沒有絕對的好壞之分，不同白化矩陣對於不同的混合信號實際上會
產生各異的去相關效果，因此，本計畫將上述之白化方式全部參與在演算法中，目前使用上
的判定標準為：若某一白化後的 V 其彼此間的相關係數最小，則取用之。 
2.2.3 音場中各音源的定位 
在音源位置量測研究上，我們曾經使用四個麥克風同步錄製音訊以及 TDCC(Time 
Domain Cross Correlation)演算法來算出麥克風間的延遲時間，只要利用四個麥克風所形成之
陣列並且搭配 TDOA(Time Delay of Arrival)的數學模型，便可以知道單一音源的距離( r )、高
度角(φ )與方向角(θ )，如式(2.8)~式(2.10)所示。用於定位的麥克風陣列之示意圖如圖 2.7 所
示，其中 12d 、 13d 、 14d  表示音源 S 與其他三個麥克風間的延遲距離， 34d 表示麥克風 m3 與
麥克風 m4 之間的延遲距離，而 σ 表示麥克風間的間距。 
10 
 
圖 2.8 MPEG SURROUND 架構圖[10] 
 
如圖 2.8 所示，我們將所需傳輸之音軌做為 MPEG-D 的輸入，其音軌數量為 N(N<=17,
包含 16 軌全音域及 1 軌重低音域)，進入空間音訊編碼器(SAC Encoder)做有效的降混
(Downmix)及空間參數(Spatial Parameter)估測與編碼，而音訊壓縮部份依使用者端設備的需求
即可用 MP3 或 AAC 等多種雙(單)聲道壓縮技術傳輸至接收端，空間參數亦隨著音訊壓縮技
術中的位元串(Bitstream)傳輸，接收端再經由空間音訊解碼器(SAC Decoder)做多通道音軌之
重建。 
MPEG-D 因應使用者需求可提供多種音訊播放之環境，倘若使用者只有傳統的雙聲道播
放系統，其音訊經壓縮技術解碼後，不必經過 MPEG-D 空間音訊解碼器，即可聆聽優美旋律；
若使用者具有家庭劇院(Home Theater)，經過 MPEG-D 空間音訊解碼器亦可解出相對的 5.1 聲
道或 7.1 聲道；MPEG-D 可有效的降低傳輸位元率(Bit rate)，所以適用於可攜式設備上之耳機
收聽，並提供雙聲道音訊及具有 3D 效果的聆聽環境(Binaural Rendering)。而有關於 MPEG-D
針對自由聆聽角的實做，將會再後續章節說明。 
2.2.5 音源在 3 維空間中的插補 
欲完整蒐集整個三維空間音場的資訊，理想上應以遍佈音場的麥克風來錄製空間中的音
源資訊。然而實際應用中，可以運用插補的方式來取代。在把所接收到的音源定位後，我們
即可得知離此音源最接近的麥克風為哪些，並可利用其周圍的麥克風所接收到的音源資料來
插補出我們所要的所在位置之音源。接下來將推導3維空間音源插補之通式，如圖2.9所示，
圖中音源Sound Source的位置(a,b,c)為已知，而其與麥克風 1M 、 2M 與 3M 的間的距離 1d 、 2d 與
3d 也是已知的，其中 ),,(4 tsrM  是空間上的任意一點，而現在我們要用 1M 、 2M 或者是 3M 來
插 補 出 4M 的 訊 號 ， 首 先 必 須 在 三 維 座 標 中 求 出 音 源 與 4M 間 的 距 離 4d ， 而
222
4 )()()( tcsbrad −+−+−= 。 
 
圖 2.9 3 維空間音源插補示意圖 
12 
 
圖 2.11 5.1 聲道音場示意圖 
 
由圖 2.11 可以知道 5.1 聲道當初在建立音場時其左聲道(L)、右聲道(R)、中置聲道(C)、
左環繞聲道(Ls)以及右環繞聲道(Rs)的放置點，而低音喇叭(LFE)則是因為人類的聽覺系統對
於低頻訊號之方位感知較不靈敏，所以不用考慮其擺放位置，只要放置點接近音場環境即可，
一般來說皆放置於環繞音場的前方。而在知道當初標準的 5.1 聲道其音場是如何建立以後，
接著我們便可以使用所錄製到的多通道麥克風音源訊號來重建 5.1 聲道的音場，在此本計畫
所採用的方式與在重建立體聲音場的技巧類似。首先同樣地我們必須先知道使用者選擇自由
視角的方向角以及高度角，接著以該視角為中心點便可以重建出各聲道之音源訊號，圖 2.12
為針對某一視角所重建出的 5.1 聲道音場。 
 
圖 2.12 自由視角 5.1 聲道音場重建示意圖 
 
在重建過程中，中置聲道、左聲道及右聲道之重建方式與立體聲音場重建時相似，皆是
經由錄製到的麥克風音訊合成處理後再把音訊送入喇叭予以實現，而關於左環繞聲道及右環
繞聲道之重建，基於表演者位於 16 支麥克風環場架設之中心點的前提下，我們必需考慮音訊
主體所給予後音場之聲學環境(Acoustic environment)所存在的條件。對於後音場之重建，其最
重要需考慮的條件為回響(Reverberation)，因為在一空間中，其後音場所存在的眾多聲波能量
中，除了音訊主體之衰減外，亦有多重路徑所造成的反射，對於人類聽覺系統來說，最明顯
的感受即為回音(Echo)，所以重建後音場時，我們根據該使用者所選擇之自由視角其相鄰的
麥克風，再經由合理推導出後音場該處回響的響應，最後合成其殘響及回音的效果予以後音
場之左環繞聲道及右環繞聲道播放。另外，當初本計畫在進行 3D 音訊錄製實驗時曾放置一
個低音麥克風去擷取低頻的訊號，因此在這邊除了重建左聲道、右聲道、左環繞聲道以及右
環繞聲道以外，還必須將低音域實現才是完整的 5.1 聲道音場。關於多聲道之音場重建，利
用後製作合成的方式，亦可以推廣至 7.1 聲道，如圖 2.13 與 2.14 皆是針對某一視角所重建出
14 
ICA[8]。在本段中，所有量測的結果都是以 SIR(signal to interference ratio)為基準，SIR 的定
義如式(2.12)： 
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
−
=
∑
∑
=
=
N
n
mm
N
n
m
SS
S
dBSIR
1
22
1
2
)ˆ(
10log10)(
       (2.12) 
 
其中 mSˆ 表示重建後的訊號， mS 表示訊號源，SIR 值越高代表分離效能越好。原始未經處理與
經過前處理的分離結果見表 2.2。 
表 2.2 Comparison of the separated results measured by SIR 
      Separated sources 
Schemes 
Speech (S1) 
(dB) 
Music (S2) 
(dB) Improvement 
Original -0.38279 -0.72886 
Fast ICA Preprocessed -0.3166 -0.61241 + 0.1827(dB) 
Original -0.34667 -0.70897 JADE 
Preprocessed -0.38432 -0.63593 
+ 0.0334(dB) 
Original -0.34395 -0.85996 NP ICA 
Preprocessed -0.36556 -0.59154 
+ 0.2468(dB) 
Original -0.3472 -0.82107 Radical 
ICA Preprocessed -0.34953 -0.60509 
+ 0.2137(dB) 
雜訊消除及非線形性補償現象可由各種 ICA 演算法的前處理獲得改善，麥克風 M1 為所
錄得的訊號如圖 2.16(a)所示，圖 2.16(b)為所錄得的訊號經過前處理，比較圖 2.16(a) 與圖
2.16(b)，雜訊較前處理前減少了很多，以及經過前處理的頻譜信號上非常類似於模擬合成的
訊號(無回音及非線性的情況) 如圖 2.16(c)所示，從表 2.2 得知經過前處理的過程有效地提高
音源分離的效能，前處理也使分離後的音訊檔聽起來更有質感。 
 
(a) (b) (c) 
圖 2.16 錄得混合訊號及經過前處理的混合訊號與模擬合成的混合訊號頻譜 
(a) M1 麥克風所錄得的訊號頻譜 
(b) 錄音訊號經過前處裡後的頻譜 
(c) 模擬合成的訊號頻譜 
 
 
16 
表 2.4 不同改善方案與它種分離演算法分離效能 MSE 值比較 
音源類型 音源位置 
Propose 
Method (A) 
(dB) 
Propose 
Method (B) 
(dB) 
Fast ICA 
(dB) 
NP ICA 
(dB) 
S1(150,0,-85) 0.0051603 0.005165 0.0099406 0.005165 2 speech 
(male & female) S2(-140,90,40) 0.0078189 0.0078227 0.0089639 0.0078227 
S1(140,-80,40) 0.019343 0.019363 0.026927 0.019363 2 music 
(violin & piano) S2(230,-25,15) 0.020068 0.020097 0.027352 0.020097 
S1(300,-90,60) 0.0096029 0.0096095 0.023374 0.0096095 2 music 
(saxophone & 
piano) S2(180,60,40) 0.011654 0.01166 0.01415 0.01166 
S1(300,-90,60) 0.0046864 0.0046739 0.0049562 0.0046739 1 speech + music 
(female & piano) S2(180,60,40) 0.0063224 0.006138 0.0090493 0.006138 
 
表 2.5 不同改善方案與 Fast ICA 的分離效能 SIR 值比較 
音源類型 音源位置 
Propose 
Method (A) 
(dB) 
Propose 
Method (B) 
(dB) 
Fast ICA 
(dB) 
NP ICA 
(dB) 
2.7888 2.8659 3.0058 2.6891 S1(150,0,-85) 
0.68389 -0.050336 0.76756 -1.637 
3.2957 3.1431 -0.00519 2.3184 
2 speech 
(male & female) 
S2(-140,90,40) 
0.80164 0.12029 -1.8699 1.0501 
2.2933 2.2228 2.7343 0.71949 S1(140,-80,40) 0.49393 0.4266 -0.79017 0.32635 
1.1842 1.1301 0.41982 1.2974 
2 music 
(violin & piano) S2(230,-25,15) -0.21898 -0.28547 -1.5711 -0.32004 
3.5704 3.6055 4.7154 3.4329 S1(300,-90,60) 2.0925 2.0921 3.3635 1.7523 
3.0227 3.0016 2.4425 2.9411 
2 music 
(saxophone & 
piano) S2(180,60,40) 
0.70243 0.65734 1.3194 0.77489 
5.2874 5.2493 5.5234 5.0576 S1(300,-90,60) 
3.03572 3.5537 4.4641 3.729 
5.3167 5.4064 4.9833 5.3417 
1 speech + music 
(female & piano) 
S2(180,60,40) 
3.1565 3.2463 3.2452 3.19119 
表 2.6 不同改善方案與它種分離演算法效能 MSE 值比較 
音源類型 音源位置 
Propose 
Method (A) 
 (dB) 
Propose 
Method (B) 
(dB) 
Fast ICA 
(dB) NP ICA (dB) 
0.0041216 0.004049 0.0039207 0.0042173 S1(150,0,-85) 
0.0066914 0.0079239 0.0065626 0.011416 
0.0064339 0.0040552 0.012188 0.0080576 
2 speech 
(male & female) 
S2(-140,90,40) 
0.011426 0.0067408 0.021137 0.01079 
0.013879 0.014106 0.012539 0.01994 S1(140,-80,40) 0.021003 0.021331 0.028224 0.021827 
0.01689 0.017102 0.20142 0.016456 
2 music 
(violin & piano) S2(230,-25,15) 0.023334 0.023694 0.031854 0.023883 
18 
… … …
…
 
圖 2.20 16+1 音軌應用於 MPEG SURROUND 架構示意圖 
2.3.5 音源的插補 
為了驗證插補出的麥克風資訊的正確性，本子計劃在實驗中參照節 2.2.5 的圖 2.9 所述之
麥克風陣列的配置下預先擺放了 M4，並且與 M1、M2、M3 等麥克風一同進行同步錄音，其
後依據音源與麥克風陣列間的距離關係，由上述之推導以 M1 為參考資訊求出 M4 的預測信
號，其插補結果與實際信號時域波形間的比較如圖 2.21 所示，兩信號樣本間的均方誤差為
0.012。至於兩信號振幅的差異，則是因為麥克風與音源間隨著距離增加而接收能力呈指數衰
減的緣故；由於 M1 距離音源較遠，其接收之衰減較 M4 為大，因此 M1 的信號在延遲後即
使依據相對於 M4 與音源間的關係放大，但所插補出的資訊其振幅仍較真正的 M4 小。不過
兩訊號以人耳實際收聽時的判定而言音量並不會相差太多。 
 
0 2 4 6 8 10 12 14 16 18
x 104
-0.5
0
0.5
0 2 4 6 8 10 12 14 16 18
x 104
-1
-0.5
0
0.5
1
 
圖 2.21 實際錄音資訊與插補資訊之間的比較 
實際上，由於 3D 音訊錄製環境的因素，僅使用一個參考訊號對任意位置進行音訊的內
插有時只適用於理想狀態下，比如說點狀音源、或是無方向性且均勻擴散的音源。除了麥克
風間的時間差與能量差以外，未來我們也必須考慮到聲音的繞射、反射以及麥克風接收的衰
減等因素，進而研究出一個適合用於任意聆聽角度的音訊合成方式。 
2.3.6 自由聆聽角立體聲音場重建 
在自由視角立體聲音場重建方面，本子計畫之做法是考慮到利用使用者對互動式介面所
下達之視角資訊(如方向角 θ、高度角 ψ等)來進行立體聲之左聲道與右聲道的音場重建，而一
般我們使用立體聲的播放系統時，會把一對立體聲揚聲器放在人的前面；而當揚聲器朝向人
面，並與鼻子中心點形成一個正三角形時為最佳的聆聽位置。因此在重建音場時我們主要根
據聆聽角的位置來重建其立體聲的音場，以下我們將以圖 2.22 來開始說明如何重建空間上任
一點的音場訊號。 
20 
的重疊原理，造成建設性的重疊，讓兩聲道間聲音的臨場感更明顯，而 2.24(b)所造成建設性
的重疊卻不是我們所要的資訊，因為它會降低左右聲道間的 ITD(Inter-channel Time 
Difference)，相較於圖 2.24(c)來說，較難以判斷出表演者的方位。針對自由視角立體聲音場
重建，本子計劃已完成與影像部份同步之音場，並有環場每 90 度間隔之四種視角音場示範影
片如圖 2.26 示，詳情請參照網址http://www.dsp.ee.ccu.edu.tw/ochen/demo.rar [74]。 
 
 
 (a) (b)  (c) 
圖 2.24 重建立體聲音場之三種實施例 
(a)左：B，右：D 
(b)左：B + C*0.5，右：D + C*0.5 
(c)左：A + B，右：D + E 
 
 
圖 2.25 聲波同相重疊示意圖 
 
 
圖 2.26 示範影片四種視角之相應音場示意圖 
 
因此本子計畫所使用的音場重建方式即利用與其鄰近的兩個麥克風音源訊號來做合成，
其鄰近的兩個麥克風訊號於前章節所述，利用多音源位置的偵測與分離成多個音源，最後便
建立起經使用者選擇之自由視角相對應的立體聲音場，如圖 2.27 示。 
22 
第三章 子計畫二：多視域視訊整合表示法之資料壓縮技
術 
3.1 前言 
實體三維電腦模型的重建為電腦視覺研究領域的重要課題之一，它在傳統上的應用範圍
包含了逆向工程、圖形識別以及工業上的檢驗等。由於電視為人類的重要日常消遣之一，因
此許多電視相關之影音設備不斷的更新，這個社會的現象使得許多研究人員投入多媒體的相
關研究。人類所生活的地方是一個三維立體的世界，而目前的電視皆以二維平面的連續影像
所結合產生的影片，來作為影音多媒體的播放資訊，所以研究人員投入相當多的努力在三維
的影音設備與模型建構上。例如在聲音上Rendu[18] 提出一個即時的音訊擷取與擬真之方法
在一個真實的三維環境上即時擷取一般麥克風的音訊。而在三維物體模型的取得與建構上，
則有Ming[19]等人利用多視角剪影資訊得到一個以多面體(polyhedral)表示的可見外殼(visual 
hull)，並以此可見外殼為初始模型去限制立體視覺(stereo)演算法的搜尋範圍，改善立體視覺
演算法的深度圖，並能使得重建物的凹陷區域獲得改善。 
 Esteban[20]等人提出先以多相機視角之間的體積交集產生以八分樹(octree-based)的方式
來表示粗略幾何模型，再用匹配立方體網格三角化轉為網格(mesh)資訊，然後利用多相機之
間的色彩資訊，針對網格資訊中的頂點(vertex)進行改善的動作。此方式並無法使用在紋理不
明顯的物體上，並且必須額外執行立方體網格三角化法才能建立出標準的多邊形模型。 
 本計劃所提出來的方式是在多架相機環場的架設場景下進行影像的擷取，我們先透過相
機校正藉以獲得多架相機之間的內外部參數資訊，接著利用影像色階降維之擷取與分割方
法，擷取出前景與背景影像與前景輪廓線。然後對前景物體以視覺外殼方式建立物體之三維
模型。 
一般而言，在我們取得一個物體之三維模型後，我們可以將此三維模型進行資料之減量
(壓縮)，然後經過資料重新取得與觀賞位置之給定，進行物體的形體視覺化，也就是我們可
以在任何一個視角下，將觀賞者所應見到物體之形體重繪於三維顯示器上。此子計畫之主要
目的為多相機校正取得相對關係與相機參數，進行物體的前景偵測，然後在可視外殼重建之
方法上延伸研究，利用立體視覺修正三維模型使之精細化與利用絞接式動態物體重建使三維
模型精細化，使得物體三維模型更加的準確。 
3.2 研究方法 
三維模型的取得我們依照執行的順序分為以下的幾個部分。 
3.2.1 多相機的校正 
為了使各相機能正確得知拍攝物體的大小比例，我們必須先利用相機校正來得知各相機
的內部參數如焦距長、影像中心、筒狀失真參數、長寬比，以及外部參數如相機與物件的相
對位置──旋轉矩陣、平移矩陣。針對不同的環境有各種適合的校正方法，在此我們採用Tsai[21]
24 
使用Cheung[24]的visual hull方法，取得三維點資訊，然而，光有這些離散的資訊是不夠
的。Power Crust[25]可用於將空間中一群三維點連接成連續的多邊形，並會包覆住整個3D模
型，在將多邊形化為多個三角形。如此一來，這樣便有了網格(mesh)資訊了。藉由網格，便
能透過不同的計算獲得更多資訊，如法向量、平面方程式等。 
 
首先，我們先實作 view-independent texture mapping。此方法是以多台相機的顏色依權重
混合後繪製在 3D 模型表面上，也就是網格。而權重根據網格之法向量與各來源相機(reference 
camera)視線間的夾角而定。若夾角越接近 180 度，也就是說此網格越近似以正面被該相機擷
取到，其權重值越大。然而，此方法產出的影像品質卻不盡理想。由於網格與網格之間並不
一定平滑，造成影像上會有不連續的狀況發生。 
 
View-dependent rendering 便可以解決這問題。除了多相機影像、相機參數與網格資訊外，
更將新視角(Novel View)相機的相機參數加入計算。以 View-dependent vertex-based texture 
mapping[26]為例。該作者取代以網格法向量與各相機視線間的夾角而定的權重值，改以新視
角視線與各來源相機視線的夾角作為權重 Wc，c 為來源相機編號。夾角越小表示該來源相機
視線越靠近新視角視線，所佔權重值越大。如此一來，影像上不連續的狀況便改善不少。 
 
然而，該篇是將空間中的三維點填上顏色。若網格不夠細緻的話，會讓新視角輸出影像
較為模糊。因此，這裡我們用一種以像素(pixel)為基底的方法做新視角影像輸出。如圖 3.3，
假設已知新視角相機的相機參數，那麼我們便可以定出相機中心與每個像素在空間中的座
標。利用類似光追蹤(ray tracing)的方式，空間中的相機中心與像素 Pij的射線便會與 3D 模型
交於一點或多點，取最近的點即是該像素看到的點 V。將點 V 投影回各來源相機的影像平面
上抓取顏色資訊，並乘以前述之權重值 Wc混合形成 Pij 的顏色。這樣便同時解決影像上不連
續的問題且提高了影像品質。 
 
圖 3.3 RAY TRACING 示意圖 
上述有個很大的問題在於速度。因為每條射線都必須與每個網格計算是否有相交，這是
非常耗時的。因此我們利用深度圖(depth map)來製作網格對照表(mesh lookup table)，用意在
迅速知道每條射線會相交於哪一最近的網格。由於我們已有 3D 模型，可輕易建立新視線的
深度圖。接著，要建立網格對照表：對於各個網格，先投影回新視角影像平面，如此可以知
道哪幾條射線會相交於該網格。接著判定該網格相對於新視線的深度是否等同深度圖。是的
26 
 
圖 3.5 DATA MAP 示意圖 
 
  
圖 3.6 三維模型(左圖)與重新取樣之模型(右圖) 
 
除了針對模型的 3 維點資訊重新取樣，此程式同樣也對模型的色彩資訊(RGB)做重新取樣，
如下圖所示。 
 
(a)     (b) 
圖 3.7 真實模型與三維座標資訊與色彩資訊重新取樣比較 
(a)真實模型經過三維重建的正面部份 
(b)以三維座標資訊與色彩資訊重新取樣的結果 
28 
 
圖 3.10 重新取樣後模型距離 R 的 HISTOGRAM 統計結果 
3.3 實驗結果 
3DAV 實驗計劃實行期間，多個實做階段皆已略有不錯的成果，各項目介紹重點如下： 
3.3.1 同步攝影系統架設 
目前我們已順利架設一套同步攝影系統，以個人電腦搭載 4 張影像擷取卡予以並外接 13
台類比擷取攝影機；在這個攝影機環場擺設的環境下，我們的系統能夠在各相機滿足同步擷
取的情況下達到每秒 6 張影像的儲存(減少攝影機數量時可再提高每秒所能儲存影像的數目)。 
 
圖 3.11 攝影機環場擺設示意圖。 
 
攝影機為環形擺設，分三層，共 13 架。第一層 8 架，每 45 度 1 架。第二層 4 架，俯視
角 60 度，每 90 度 1 架。第三層 1 架，垂直往下看。 
 
圖 3.12 用於小物體的攝影機環場擺設 
3.3.2 攝錄影像前景擷取 
在取得多個相機攝錄的影像之後，為銜接後續各階段所需的圖像資訊，我們必須濾除隨
著物體一併被錄製下的背景畫面；我們從預先拍攝的背景影像作為參考依據，透過一些已發
展成熟的相關演算法來達成這個目的。 
30 
以 cat 作為虛擬 model 並以簡單圖像當 model 的外皮。353 個 point，672 個 mesh。再藉
由虛擬相機拍攝成為 13 張 reference image，如圖 3.15。 
 
圖 3.15 CAT 虛擬相機拍攝圖 
其結果如下: 
a.將 novel view 設置在同於 view0 的位置 
  
(a)  (b) 
圖 3.16 CAT 虛擬三維模型重建在 VIEW0 位置  
(a) Reference view 
(b) Novel view 
 
b.將 novel view 設置在 view0 與 view1 之間 
   
(a)  (b)  (c) 
圖 3.17 CAT 虛擬三維模型重建在 VIEW0 與 VIEW1 間 
(a)Reference view 0 
(b)Novel view 
(c)Reference view 1。 
 
從虛擬實驗看來，我們提出的方法可讓產出的新視線影像幾乎等同於原始拍攝影像。依
新視線視角與各相機視角間的角度作為權重比例讓輸出影像不會有邊緣不連續的情況。此
外，以像素為基底更可讓影像品質更精確，邊緣相當清晰。然而，從 view0 與 view1 之間的
32 
   
(a)   (b)   (c) 
圖 3.21 木頭人虛擬三維模型重建在位置:VIEW0 與 VIEW1 間 
(a)Reference view 0 
(b)Novel view 
(c)Reference view 1。 
以實際拍攝機器人的影像當做實驗輸入，相機架設方式與虛擬實驗相同。環形擺設分三
層，下層八架、中層四架、上層一架，共十三架。拍攝到之影像依下中上順序如圖 3.22 所示。
13303 個 point，30543 個 mesh。 
 
圖 3.22 機器人實際拍攝圖 
其結果如下: 
a.將 novel view 設置在同於 view0 的位置 
  
(a)     (b) 
圖 3.23 機器人實際拍攝重建位置:VIEW0 
(a) Reference view 0 
(b)Novel view 
 
b.將 novel view 設置在 view0 與 view1 之間，且恰好在 view8 的斜右下方 
34 
資料表示法之建構 
將真實物體重建的 3D 模型的三維點資訊，重新取樣，使得該模型的三維資訊，能夠以
方位角(θ)360 度與仰角(ψ)90 度的半球面為座標位置；使沿著通過其球心的射線，取得和模型
相交的交點資訊，藉此重新表示模型。 
 
圖 3.27 史賓機器人第 10 架攝影機所拍攝之影像 
 
圖 3.28 經過三維重建後之完整模型 
 
圖3.29 經過三維點資訊與色彩資訊重新
取樣後之模型 
 
圖 3.30 方向角 360 度(X 軸方向)與仰角 90 度(Y 軸方向)的交點取樣 
 
如圖 3.30 將 map 的取樣原點移動至史賓機器人模型的正中心，然後做方向角 360 度(x
軸方向)與仰角 90 度(y 軸方向)的交點取樣。在取得交點後，計算所有交點距離射線原點的距
離(r)，再將所有距離(r)加以量化，使得距離(r)可用 0~255 之間的灰階值加以表現。圖中所表
示之灰階值，距離射線原點越近之距離(r)值，其代表之灰階值越明亮，反之離射線原點越遠
之交點的 r 值，代表的灰階值越暗。 
36 
 
圖 3.36 依照其方向角與仰角的相對交點位置，將色彩資訊顯示在二維 MAP 上 
 
如圖 3.36 將 map 的取樣原點移動至真人的模型正中心，然後做方向角 360 度(x 軸方向)
與仰角 90 度(y 軸方向)的色彩資訊(RGB)取樣，再直接依照其方向角與仰角的相對交點位置，
將色彩資訊顯示在此二維 map 上。 
 
3.4 結語 
本計畫完成了一個環場實體三維重建系統，利用場景架設、物體表面資訊和物體輪廓資
訊來作為重建的依據。其研究成果包含下列幾項： 
1. 本研究設計了一套新的環場重建方式；利用可見外殼上的幾何特性，有效地估測出物體
的深度範圍，減少了計算量並增加了準確性。  
2. 在改善的步驟中將可見外殼視為一堆約束邊線的集合，彼此可分成獨立的個體，因此將
來可以叢集電腦的方式，把可見外殼視為多個獨立個體所組成，然後分別丟至各個節點
去做運算，加快整體系統執行的速度。  
3. 在相同的相機架設下，所估測出的曲線可比單純由多視角剪影演算法的約束邊線更好的
結果。 
 
 
 
 
 
 
 
 
 
 
 
38 
(a) (b) 
(c) (d) 
 
圖 4.1 多攝影機擺設方式 
(a). 平行擺設  
(b). 弧狀擺設 
(c). 環狀擺設 (朝向中心) 
(d). 環狀擺設 (朝向外) 
 
由 ITU-T Video Coding Experts Group (VCEG) 以及 ISO/IEC Moving Picture Experts 
Group (MPEG) 所組成的 JVT 組織，已經釋出了多視域視訊編碼平台 JMVM。JMVM 是以
H.264 編碼標準為基礎發展的多視域視訊編碼平台，採用混合視差預測與運動預測 (Disparity 
Compensation Prediction and Motion Compensation Prediction) 的聯合預測多視域視訊壓縮編
碼系統 (Joint Prediction Stereoscopic Video Coding)，圖 4.2 為 JMVM 所採用的 Hierarchical B 
picture 的預測編碼架構。 
 
圖 4.2  HIERARCHICAL B PICTURES IN JMVM 
 
本期計劃第一年所提出的多視域編碼架構，將參考 JMVM 的 Hierarchical B picture 的參
考架構，使用 depth based image render (DIBR) 的方式，用深度資訊來輔助編碼，以增進壓縮
效率。我們將 inter-view prediction 的方式做修改，使用攝影機參數與深度資訊的 view synthesis 
prediction，進一步減少 inter-view redundancy。 
由於多視域 3D 視訊的資料量遠大於以往 2D 的彩色視訊，在有限的硬體空間之下，多
視域視訊的壓縮編碼是勢在必行的。因此在本期第一年度的計劃中，我們發展一種應用於多
視域視訊之動態三維網格模型壓縮技術，結合 MPEG-4 AFX-3DMC 原有的優點，能夠將一
序列動態立體視訊的位元率再減少、提高壓縮率，達到節省更多的位元空間的目的，以利傳
輸。 
40 
攝影機的內外部參數如式(4.1)： 
},{]|[ BAkTRK kkk ∈、  
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
=
100
0
0
y
y
x
x
os
f
os
f
K
                                (4.1) 
f 為 focal distance，ox 與 oy 為影像中心座標(pixel)，sx 與 sy 為 dimensions of the camera 
photosensor，K與攝影機的內部參數有關，R為攝影機的 rotation matrix，T為攝影機的 translation 
matrix，假如三維空間中一點 M(u, v, w)，其成像在 camera A 上是一個像素點 PA(x, y)，則 
PA(x, y) 可以表示為： 
MTRKyxP AAAA ]|[),( =                           (4.2) 
 
同樣的三維空間點 M(u, v, w)，其成像在 camera B 上所對應的像素點 PB(x, y) 也可以寫成： 
 
MTRKyxP BBBB ]|[),( =                           (4.3) 
 
經由 3D warping 的推導[30]，我們可以得出 PA(x, y)和 PB(x, y) 的關系，下式 (4.4)、(4.5) 
[31] 則是描述一個三維空間點 (u, v, w) 與其投影在兩個不同的攝影機c和c’上的兩個像素點 
(x, y)、(x’, y’) 間的關係，R(c)與 R(c’)是攝影機 c 和 c’的 rotation matrix，T(c)與 T(c’)是攝影
機 c 和 c’的 translation matrix，A(c)與 A(c’)是攝影機 c 和 c’的內部參數，D(c, t, x, y)是攝影機
c 的第 t 個時間點所拍攝到的畫面上，座標為 (x, y) 的像素點其對應的深度資訊。 
 
( ) )(,,,
1
)( 1 cTyxtcDy
x
AcR
w
v
u
+⋅
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
⋅⋅=
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
−
                           (4.4) 
 
⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧
−
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
⋅⋅=
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
− )'()()'(
'
'
'
1 cT
w
v
u
cRcA
z
y
x                           (4.5) 
 
Camera A 
Camera B 
 
圖 4.5 不同攝影機間之幾何關係 
圖 4.6 是我們以微軟所釋出的 break dancer 影像序列的第三架和第四架攝影機模擬 DIBR 
based inter view prediction。我們將 break dancer 第四架攝影機的影像，以 DIBR 的方式投影至
第三架攝影機，做為第三架攝影機的預測影像，如圖 4.6 所示。 
42 
差異約有 20 pixel 左右，若是搜查範圍夠大，應該可以逼近 affine model 的效果，但是由於位
移值也必須一併編碼傳送至解碼端，若是位移值太大，則編碼效率將會下降。我們採用的 DIBR 
based inter view prediction 對於每一個 view 皆須傳送對應的深度資訊與攝影機參數，再加上額
外的位移資訊，則整體的編碼效果並不會優於使用 JMVM 的 disparity search 方式，因此，我
們暫不考慮使用 local motion search 的方式進行補償。 
Depth search 的補償方式是假設 DIBR 的投影誤差為深度資訊不準確所造成，因此我們以
一個 Δd 值做為深度誤差之補償值，藉由變更不同的 Δd 值進行 DIBR 投影，找到一個使投影
過後的區塊與原始的區塊 SAD 最小的 Δd 值，即為我們欲求得的深度補償值。實驗結果如圖
4.7(b)，可以看出使用深度補償的確能夠改善 DIBR 的投影誤差，由於深度值 d 和 disparity
是一體兩面的，因此改變 d 值幾乎等同於在 x 方向上變動，因此對於 y 方向的投影誤差，使
用 depth search 的方式無法有效改善。 
 
(a) (b) 
 
(c) 
圖 4.7  DIBR 投影誤差補償 
(a) affine model compensation 
(b) motion compensation 
(c) depth compensation 
4.3 第二年度 (97/8~98/9) 研究方法: 動態 3D 網格結構壓縮技術 
本年度計畫中我們考慮 3D 網格結構的壓縮 (網格結構係承接另一子計畫的成果)，在
MPEG-4 Part16 AFX - 3DMC 中的壓縮演算法 TS (Topological Surgery) 已有不錯的壓縮效
果。針對單一的 3D 網格模型可以充分考慮其點線面間的相關性，對於一般的單一網格壓縮
可達到 50 倍的壓縮率。不過要是處理動態網格時，兩兩時間點間模型之冗餘性在 MPEG-4 
Part16 AFX-3DMC 中是沒有被考慮，因為 MPEG-4 Part16 AFX-3DMC 處理的僅是對於單一
模型，因此發起了我們對於 Inter-model 壓縮探討的動機。為了進一步壓縮位元率，我們將仿
效二維視訊壓縮 (MPEG 或 H.26X) 採用的 Inter-frame 預測 (如圖 4.8)， 使用 “Inter-model” 
預測以利探討不同時間點的兩個網格結構間頂點位置的相關性，除去兩者間的冗餘性。 
 
 
圖 4.8 INTER-FRAME 預測 
44 
陣與平移矩陣。最後則是利用該轉換矩陣處理欲進行對齊的圖形，並計算它與上一次疊代計
算結果之間的誤差值變化，若變化小於給定的門檻值 (threshold) 則視為收斂完成。 
 
圖 4.10 ICP 演算法示意圖 [38] 
在實際進行處理之前，使用者必須先在兩組欲進行比較的模型裡定義一組稱為 data set，
也就是欲對準的模型。另外一組模型則稱作 model set，這是用來當做對齊基準的資料。ICP 在
經過多次重複計算之後會找出一組最佳的幾何轉換矩陣，把 model set (以 M 代表)對齊在 
data set (以 D 代表) 上面。可分為： 
1. 設定資料格式與停止門檻值 
2. 尋找對應點 
3. 計算幾何轉換資料 
4. 更新座標位置 
5. 計算誤差值與檢測是否停止 
等五個步驟，以下就五個步驟做細部說明。 
1. 設定資料格式與停止門檻值 
在使用 ICP 演算法之前，必須先對 data set 與 model set 的資料型態進行設定。資料格
式上 data set 特別規定必須為點集合，在 ICP 演算法啟動前，還需設定一個條件門檻值，停
止條件門檻值就是指每次執行完一個循環後，必須計算兩組資料間的對正誤差值，並且計算
其變化比例，如果比條件門檻值大的話則重複執行”計算幾何轉換資料”(步驟 3)，反之則為停
止 ICP 演算法，跳出迴圈。 
在程式迴圈開始執行的時候要先設定第 0 次疊代使用的 data set Dk (k=0) =D0，D0 即為
欲進行校準的 data set，而 model set 則是設定為 M。 
2. 尋找對應點 
設定完前置作業後，接著進行第二個步驟，尋找對應點組合。所謂套合就是將兩組資料
做整體拉近的動作，讓兩組資料的誤差達到整體最小的目標。首次找尋兩組間的對應點組時，
如果 model set 的資料格式也是點集合的話，那對於進行過 k 次疊代的 data set Dk 裡的每一
個點 p 來說，找到的都是在 model set 裡距離 p 最近的點 s，是以 p 與 s 兩點的距離 
(Euclidean distance) 來決定的，如 (4.8) 式。 
),(min),(
, iMs
spdMpd
i ∈∀
=                            (4.8) 
總歸來說，ICP 演算法會對於 data set Dk 裡的每一點 pi 搜尋 model set M 裡與其距離
最短的對應點 s*，並與其形成一組對應關係 ( pi , s*)，並紀錄 pi 與 s* 的座標位置。 
3 計算幾何轉換資料 
前一步驟提到關於尋找對應組點，ICP 演算法後續將會利用這些點對計算 data set 與 
model set 之間的幾何轉換關係。因為 ICP 演算法的幾何轉換屬於剛體轉換 (rigid body 
transformation)，裡面包含了旋轉矩陣與平移矩陣兩個部分，只要透過前面的點對應關係就能
找出幾何轉換的資訊。 
不過在此必須特別注意的是每次疊代前都得重新尋找對應點組，因為 data set 的點每次
經過不同的幾何轉換都將改變其對應點。但無論疊代多少次，每次計算幾何轉換矩陣時所使
用的都是最原始的 data set 資料，而非經過上一回幾何轉換後的 data set 資料。每次計算幾何
46 
 
圖 4.11 改良式動態 3DMC 系統架構圖 
4.3.2.2  k-means 分群 
利用 k-means 對模型頂點分群起因於我們認為 ICP 演算法的確可以使全域的頂點經過
疊代處理後達到套合對齊的效果。然而，一旦處理的模型是非剛體時，直接使用 ICP 演算法
是不合宜的。我們認為在這部分需先經過分群，將模型從非剛體分類為數區塊的剛體，如圖
4.12 所示，各群頂點再進行 ICP 演算法。 
 
圖 4.12 3D 網格模型分為 5 群 (以色彩區分) 
4.3.2.3 空間-時間搜尋(Spatio-temporal search) 
Spatio-temporal search 的概念是我們除了可以往重建後 
1n-Fˆ  中去尋找預測源，我們亦可
在 Fn 中尋找，只是就 Fn 而言並非能全部尋找。因為在解碼時，會有先後解碼順序的問題存
在，所以在 Fn 的尋找部份，我們只尋找 causal 部份的頂點 (即排序在前，已編碼完畢的頂點)，
而對於尚未處理的頂點則不做為預測源尋找的範圍。 
Fn 與 1ˆn-F  先進行 Temporal 方向之動態估計，假設可搜尋到一個頂點 (v_index_T , 
x_residue_T , y_ residue_T, z_ residue_T)，接著，在目前時間模型中已編碼完之頂點集合中進
行搜尋，這就是 Spatial 方向的動態估計。實際上，在 Spatial 方向上的估測並非前面全部的頂
點，而僅是往該頂點前面一段編號的頂點進行搜尋，以減少搜尋時間 (當然這是假設頂點標
48 
 
圖 4.14 改良式 3DMC 系統架構圖 
4.4 實驗結果 
4.4.1 本期第一年計畫成果 
4.4.1.1 立體視訊編碼器實驗條件 
本期第一年計劃中，立體視訊所使用的測試影像序列是微軟所提供的 break dancer，break 
dancer 的視訊總共有 8 個不同的 view，攝影機使用弧型架設的方式，每一個 view 都有一組視
訊資料以及其對應的深度影像 (如圖 4.15)。 
 
4.4.1.2 Mode selection of DIBR-based inter-view prediction coding  
我們先以 SAD 為判斷條件，來模擬實際編碼時，使用 motion estimation mode 與 DIBR 
mode 預測的結果，使用的是 break dancer camera 3 序列中第 2 和第 3 張畫面，如圖 4.16，皆
是以 4x4 的 block 為一個預測單位。Motion estimation mode 與 DIBR mode 的選擇 (mode 
selection) 是以何者所造成 predicted block 的 SAD 較小為選擇標準。圖 4.16 中，白色的區
域是表示選擇 motion estimation mode 做預測，黑色的區域是表示選擇 DIBR 的方式做預測。
我們可以看出，使用 DIBR 的地方大多是在影像內容變動大的區域，如中央舞者的右手，因
為前後兩張畫面的差異太大，使得用 motion estimation 的方式無法找到一個很相似的區域做
預測，而使用 inter view 方向較能夠預測。 
由於解碼端在解碼時，也需要深度資訊，因此深度影像也需要進行編碼而傳送至解碼端，
而深度資訊經由壓縮過後產生的失真，會造成 DIBR 的投影誤差，也可能造成使用 DIBR mode
來做預測的失真變大，降低編碼效率。在此，我們模擬將 depth image 以 block based 的方式
表示，以 16x16、8x8、4x4 的 block 為單位，取每一個 block 的平均值來表示這個 block 的深
度值，並進行 DIBR 的投影，計算造出的預測影像和原始影像的能量差異，如圖 4.17。 
50 
由圖 4.17 中可我們看出，全部使用 16x16，或是 8x8 block size 的 depth 表示法，其 DIBR
的結果比起使用 per-pixel depth 的方式還要差，這是很直觀的結果，但是使用 variable block size
的 depth 表示法可以逼近 per-pixel depth 的效果。由於在自然界中，同一個物體的表面，其深
度變化並不大，因此在深度平滑的區域使用較大的 block size depth representation，而在深度
變化大的區域使用較小的 block size，可以減少 depth 的資料量，同時也可以降低因為 depth
的失真，造成 DIBR 的投影誤差的現象，提升編碼效率。 
4.4.2 本期第二年計畫成果 
4.4.2.1 動態三維網格模型的取得 
我們所使用的 3D 網格模型是利用架設環場攝影機擷取多視域視訊，再對每一時刻的多
視域影像進行 Visual hull [36] 演算法後重建出的 3D 網格模型，如圖 4.18、4.19。使用的 3D 
網格模型為“robot”之 Frame 0 - Frame 5。 
 
圖 4.18 “ROBOT”FRAME 1 
 
圖 4.19 “ROBOT”FRAME 2 
4.4.2.2 傳統 3DMC 技術與改良式動態 3DMC 技術之比較 
我們除了會將重建後 3D 網格模型投影顯示以表示主觀上的優劣外，另外將計算三種評
量數值：平均誤差距離、KG 誤差及投影深度圖 SNR (Signal to Noise Ratio) 等來表示壓縮前
後的失真度。 
• 平均誤差距離 
因為我們的壓縮屬於有損壓縮，因此原始模型與重建後模型兩者的頂點座標上難免造成
誤差。在此我們直接計算兩模型頂點與頂點間三維座標的誤差值 (如 4.12 式) 
)(1
1
1 iiii
n
i
ii czbyaxn
E −+−+−= ∑
=
                     (4.12) 
(4.12) 式中假設重建後的模型頂點座標為 (x,y,z)，對應於原始模型中的頂點座標為 (a,b,c)。n
則為模型中頂點的總數。 
• KG 誤差 
KG 誤差是在電腦繪圖領域中常見的一種量度方式，它可以用來量度兩物體之間的誤
差，並且經過正規化後呈現的一個數值。文獻 [40] 中也提到 KG 誤差用來量度 3D 網格模
型誤差。 
nA
AA
KGE
××−
−
=
113
~
2
]1,.....,1,1[
*100)( μ
                        (4.13) 
上式中 A 表示一個 n×3  大小的矩陣，放置原始模型的頂點座標，n 為模型中頂點的總數，
~
A
表示重建後的對應頂點座標，大小亦為 n×3  的矩陣。特別在此還要計算網格頂點座標的平
均值 13×μ 。 
 
52 
  
圖 4.23 ROBOT-FRAME 1 原始 3D 網格 圖 4.24 ROBOT-FRAME 1 壓縮重建後 3D 網格 
  
圖 4.25 ROBOT -FRAME 2 原始 3D 網格 圖 4.26 ROBOT -FRAME 2 壓縮重建後 3D 網格 
  
圖 4.27 ROBOT -FRAME 3 原始 3D 網格 圖 4.28 ROBOT -FRAME 3 壓縮重建後 3D 網格 
  
圖 4.29 ROBOT -FRAME 4 原始 3D 網格 圖 4.30 ROBOT -FRAME 4 壓縮重建後 3D 網格 
  
圖 4.31 ROBOT -FRAME 5 原始 3D 網格 圖 4.32 ROBOT -FRAME 5 壓縮重建後 3D 網格 
 
另外我們除了“robot” 這個 3D 網格模型外，我們也另外實驗了“chicken” 這個動態模
型。縱使我們開發的演算法是設計給 3D 視訊所重建的 3D 模型，但仍可使用於一般電腦繪
圖 3D 的模型來進行實驗。“chicken” 這個模型在許多討論 3D 網格壓縮的論文中都會使
用，因此我們也對“chicken” 使用我們的改良式 3DMC 進行壓縮，但壓縮過程中並沒有使用
到電腦繪圖 3D 模型中具有的對應頂點關係，我們仍將其當作時間變化性的 3D 模型來處
理。實驗結果圖如圖 4.33 與圖 4.34。我們使用了平均距離誤差 E1 及 KG-誤差 E2 來量度品
質。 
54 
4.5 結語 
本計畫第一年度提出了 DIBR-based inter-view  prediction 的方式，移除多視域視訊的
inter-view redundancy，相對於 disparity estimation 的方式，DIBR 投影能更準確的預測，由於
DIBR 的方式需要深度資訊與攝影機參數，因此準確的攝影機較正還有深度資訊的計算是必要
的。為了補償由於攝影機參數或是深度資訊的錯誤造成 DIBR 的錯誤投影，我們討論了 affine 
compensation model、local motion search，以及 depth search 三種補償方式。 
在第二年度中，本計畫對 3D 視訊採用動態三維網格的表示法，並探討其壓縮法則，本
演算法採用了三維動態估測的方式，先加入了 K-means 與 ICP 的作法，將兩個時間的 3D 
模型先行對齊，再進行動態估測，估測時不僅往 Temporal 的方向進行搜尋，也考慮了 Spatial
的方向，增加可搜尋的頂點數。實驗結果顯示整體下來相較於 MPEG-4 AFX-3DMC 可以達
到減少 20%的資料量。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
56 
  
AV Aquisition 
Data processing 
(Integration AV with CG, 
Modeling, etc.) 
Interaction 
Rendering 
Delivery 
Backchannel 
• Viewpoint selection 
• Display type selection 
• Listening point & sound 
direction selection 
• Play type selection  
Raw Video Data 
+ 
Camera parameters 
Uncompressed   
Format  
Format Conversion 
 
Application Specific 
data 
(In Uncompressed 
Format) 
Compressed  
Format  
Decoding  
Encoding 
(Compression) 
 
 
圖 5.1 BLOCK DIAGRAM OF 3DAV SYSTEM ARCHITECTURE 
5.3 相關文獻探討 
為了達到互動性播放的需求，MPEG 委員會特別制訂 3DAV (3D Audio Visual) 的標準，
與其有關的應用包括：omni-directional video、interactive stereo video 和 free viewpoint 等等。
其中自由視角的播放為其中最基本但也最具有挑戰性的應用。針對此項需求，已有許多相對
應的系統因而產生，如表 5.1 所示，其中 image-based 及 model-based 的方法最廣為人知。 
表 5.1 Methods for free view image generation and their features [42] 
method data acquisition data conversion view generation quality 
Image-based direct no warping  projection not precise 
Integral 
photography direct optical optical precise 
Ray-space calibration/  registration 
coordinate 
transform 
memory access 
Interpolation precise 
Model-based calibration  registration 
3D model 
texture texture mapping not precise 
DIBR direct/depth no warping  projection precise 
 
58 
此方法稱為預先扭曲 (prewarp) 或是矯正 (rectification)。平行的兩張影像其對應點關係會變
成只有一維方向(橫軸方向)的維度，這樣不僅縮短搜尋對應點的時間，也增加了對應點的精
確性。搜尋對應點的方式類似壓縮技術中移動估測的搜尋方式。得到兩張影像之間的視差後，
以線性內插方式合成虛擬影像，再根據 sˆI 所需的視角將虛擬影像 sˆI 後扭曲 (postwarp) 之後得
到欲觀賞之虛擬影像 Is。 
5.3.2 深度影像式新視角影像合成 (Depth Image Based Rendering) 
深度影像式合成法 (Depth Image Based Rendering, DIBR) 的任意視角影像可由攝影機影
像搭配深度影像產生，示意圖如圖 5.3。在[45] 中有詳細 DIBR 相關的基本理論，說明二維攝
影機影像與三維空間物體之間的關係。藉由針孔攝影機模型與三維投射原理就能產生虛擬攝
影機的影像合成。 
       
圖 5.3 2D+DEPTH METHOD 
5.3.2.1 二維到三維轉換 (2D to 3D Conversion) 
接下來介紹深度式合成影像的應用，文獻[46]中提出如何利用一張影像配合深度影像產
生立體影像對。圖 5.4 和圖 5.5 分別為一張景物影像以及對應的深度影像，可假設這張影像為
右眼所看到的影像。利用二維到三維轉換的方式我們可以得到一張左眼影像如圖 5.6 所示。
但由於遮蔽的關係，圖 5.6 中會產生空白的區域。這些區域可以利用鄰近的像素資訊以線性
內插方法填補，得到完整的左眼合成影像，如圖 5.12 所示。如此，以原始的右眼影像，圖 5.4，
搭配左眼合成影像，圖 5.7，便能組成立體影像對。 
 
圖 5.4 景物影像 
 
圖 5.5 對應的深度影像 
 
圖 5.6 有空洞的左眼合成影像 
 
圖 5.7 內插填補後完整的左眼合成影像 
+ = 
60 
5.4 研究方法 
5.4.1 DIBR 所遭遇的問題與解決方法 
 理論上，給予正確的相關資訊後，有機會合成完美的虛擬影像，而不需經過前後級額外
的處理。但在現實環境中，由於三維空間座標單位與二維影像座標單位精確度不同，如浮點
數轉為整數，在投射的過程中就會產生誤差，使合成的虛擬影像中會有一些問題產生。 
5.4.2 散射填補 (Scatter Filtering) 
問題： 
 空洞 (hole) 的問題，在投射時，由於精確度不同，三維空間中的多點可能都會投射在二
維影像平面上同一點的位置。而將二維影像平面上的一點反投射至三維空間時，重建的三維
物體就會產生空洞而不完整。 
解決方法： 
針對合成影像中空洞的問題，我們提出散射填補 (Scatter Filling) [54]的方式來進行改
善。散射填補的原理是將三維空間與二維影像平面之間點的一對一投射改為一對多投射，如
此就能夠減少投射的合成影像中空洞的現象如圖 5.13，解決後結果如圖 5.14。 
 
圖 5.13 未散射填補之結果 
 
圖 5.14 散射填補後之結果 
                          
5.4.3 考量深度資訊的投射 ( Depth Priority-based Projection) 
問題： 
 當三維空間中的多點投射到二維影像平面上同一點的位置時，這些點也許是前景資訊，
也可能是背景資訊。若將前景的位置貼上背景資訊，影像就會出現錯誤的內容，如圖 5.15。 
解決方法： 
此問題可以參考深度影像的資訊來判斷此點是否為前景。根據每個點相對應的深度值來
判斷的方式，我們稱為以深度資訊為考量的投射 (depth priority-based projection) [54]，當三維
空間中的多點投射在二維影像平面上的同一點時，這些三維空間點中深度值最小的點，表示
距離攝影機最近，可將此點視為前景，而將之投射到目標影像平面上，解決後結果如圖 5.16。 
 
圖 5.15 未參考深度資訊投射之結果 
 
圖 5.16 參考深度資訊投射之結果 
62 
5.4.6 合成視訊的深度不一致性 (Depth Consistency Problem in Synthesis 
Video) 
問題： 
 DIBR 利用已擷取的兩張影像，和估測的深度影像，再利用反投射的方式找到其在三度空
間中所對應的點，最後將此點反射到虛擬攝影機的成像平面上，亦即新視角所應觀測到的 2D
平面。由此可知，深度影像的估計在此顯的格外重要，而深度影像的估測上一直存在一個問
題，如[55]所提供的深度資訊，這些深度資訊於估測時並未考慮到時間上一致性的問題，造
成相連時間點上的物體同為不動的背景，但彼此的深度資訊卻不盡相同，如圖 5.22 和圖 5.23
紅色圈起的部份，兩者為不動的背景，但對應的深度資訊在不同的時間點上卻不同，如圖 5.24
和圖 5.25 所示，因此在合成自由視角的視訊序列時，將產生連續畫面的不一致。 
 
圖 5.22 CAM4 彩色影像 (T=0) 
 
圖 5.23 CAM4 彩色影像 (T=1) 
 
圖 5.24 CAM4 深度影像 (T=0) 
 
圖 5.25 CAM4 深度影像 (T=1) 
解決方法： 
 為了改善此現象，必須將背景深度做同步統一的步驟，我們使用背景相減法找出背景不
動區域，圖 5.26 為相減後產生之結果，可設定一閥值（threshold）判定為是否為背景，大於
閥值則歸類為前景，二位元圖顏色為白色，小於閥值歸類為背景區域，二位元圖顏色為黑色，
產生二位元圖遮罩。根據圖 5.3.16 二位元圖遮罩，由於光源和雜訊關係產生許多錯誤的白點，
必須將此雜訊去除，改善此部份使用的方法為形態學上的閉合（closing），即先將其膨脹
（dilation）再侵蝕（erosion），閉合一次的結果，雖然大部份的雜訊去除，但是還是有些許
雜訊部份。因此再做一次閉合，圖 5.27 為最後背景二位元遮罩。 
64 
右排列，相關的參數如表一所示。八架攝影機中，4 號攝影機設為世界座標中心，其餘的七
架攝影機分別對 4 號攝影機進行校正，得到精確的攝影機參數。每架攝影機各有 100 張彩色
影像和其對應的深度影像，其中深度影像是由[56]所提方式求得，由彩色影像搭配深度影像
使用 DIBR 的方式產生虛擬視角影像，圖 5.32 為攝影機 3 到攝影機 5 的移動序列影像。 
表 5.2 多視角影像相關參數 
Test Sequence resolution features camera 
space (cm)
frame rate 
(fps) 
Property of 
camera array
Breakdancers 1024×768 violent 
motion 
20 15 1D/arc 
Ballet 1024×768 violent 
motion 
20 15 1D/arc 
 
 
圖 5.32 靜態的“BREAKDANCERS”視角移動序列影像 
5.5.2 子計劃一提供影像 
子計劃一中環場攝影機所真實拍攝的資訊，並且使用 3D 模型所產生的深度資訊提供
DIBR 合成，探討攝影機之間距離較大且彎曲角度較大的實驗狀況，以下使用環場攝影機的實
際拍攝彩色圖像做實驗，並探討 DIBR 與模型式影像合成的產生影像結果。 
5.5.2.1 DIBR 新視角影像的合成 
假設新視角影像的攝影機位於攝影機 5 和射影機 2 之間，其旋轉矩陣與位移矩陣下，在
空間的位置如圖 5.33 所示，紅點位置為虛擬攝影機，原本影像大小為 640×480，為了能使圖
較清楚呈現在報告上，將圖不必要的藍色背景去除，影像大小變成 260×200，結果如圖 5.34
所示。此合成影像使用 2 張彩色影像加上深度資訊，並且套用本計畫提出 DIBR 之問題改善
方式，包括 Scatter Filtering、Depth Priority Based、Projection Occlusion Filling。 
novel view 1 的 R|T 矩陣
-0.028835 -0.998987 -0.034551 229.820187
0.439763 -0.043719 0.897049 -230.717278 
-0.897651 0.010672 0.440578 1648.765533
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
 
novel view 2 的 R|T 矩陣
-0.028835 -0.998987 -0.034551 35.324682
0.439763 -0.043719 0.897049 -306.342127 
-0.897651 0.010672 0.440578 1689.124543
⎡ ⎤⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
 
66 
 
圖 5.39 以 DIBR 產生 NOVEL VIEW 1 
 
圖 5.40 模型式產生 NOVEL VIEW 1 
 
圖 5.41 以 DIBR 產生 NOVEL VIEW 2 
 
圖 5.42 模型式產生 NOVEL VIEW 2 
5.6 結語 
實驗結果顯示 DIBR 的影像合成效果優於 IBR 合成法，但 DIBR 於合成時仍有許多問題
需要解決，以下為本計畫針對各種問題所提出的解決方法。 
1. Hole Problem 
Solution:以散射填補 (Scatter Filling)的方式，將三維空間與二維影像平面之間點的一
對一投射改為一對多投射，如此就能夠減少投射的合成影像中空洞的現象。 
2. Occlusion Problem 
Solution: 利用兩張或多於兩張的影像來填補用單一影像於 DIBR 合成時出現的遮蔽
區域。 
3. Dis-occlusion Problem 
Solution: 以深度資訊為考量的投射 (Projection Based on Depth Priority)。當三維空間
中的多點投射在二維影像平面上的同一點時，這些三維空間點中深度值最小的點，表
示距離攝影機最近，可將此點視為前景，而投射到目標的影像平面上。 
4. Erroneous Depth-image Problem 
Solution：以邊緣遮蔽擴張 (Boundary Occlusion Dilation) 以及深度影像膨脹 (Depth 
Image Dilation) 而獲得合成影像品質的改善。 
5. Depth inconsistency in synthesized video 
 Solution：跟據前一張畫面，可判斷目前畫面中的背景部分，若屬於背景，則此背景部
分的深度將參照前一張影像之深度，使影像前後張之不動物體部份的深度一致，合成
的視訊畫面將不會有閃爍的現象。 
68 
6.2.1 攝影機運動參數之估計 
攝影機運動的產生是由於人為操作所引起，基本上攝影機的操作可以圖 6.1 所表示。摒
除畫面內的物體運動，單純的攝影機操作將造成畫面內的移動向量產生特定方向的分佈，又
可以稱為 global motion。圖 6.2(a) 所示，即為單純「Zoom」所造成的移動向量場。圖 6.2(b) 所
示，該張影像除了攝影機的「Zoom」操作外，也具有輕微的「Pan Left」。 
由於攝影機運動的關係，除了會影響實際物體移動之方向和移動量的估計外，也會影響
移動向量之分析與分類，故本計畫進行攝影機運動參數之估計。估計求得之攝影機參數將可
以補償實際物體的移動向量，其參數也將有助於移動向量類型之分類。 
 
 
圖 6.1 攝影機操作圖 (摘自文獻[67]) 
 
    
(a)                                   (b) 
圖 6.2 攝影機變焦（ZOOM）所形成的移動向量分佈： 
(a) 單純的攝影機變焦 
(b) 攝影機變焦加上輕微的左移（Pan Left） 
 
如同上述，在實際的 video sequence 中，由於拍攝影片時攝影機的運動，使得大多數的畫
面都會隨著 global motion，因此本計畫藉由攝影機參數的估計來消除影像中 global motion，
以提升視差估計 (disparity estimation) 的正確性。由於攝影機移動會影響整張畫面，本計畫基
於下列的假設進行 global motion 參數的估計： 
(a) 4-parameter camera motion model 
(b) 相鄰二影像之控制點配對已經成功 
(c) 控制點皆屬影像背景部分，或僅摻雜少許的 object motion 
本計畫採用文獻 [68] 的方法，使用簡單之 4-parameter 模擬影像的 global motion，故此處簡
單介紹 4-parameter camera motion model，其模型如下： 
⎥⎦
⎤⎢⎣
⎡+⎥⎦
⎤⎢⎣
⎡⎥⎦
⎤⎢⎣
⎡
−=⎥⎦
⎤⎢⎣
⎡
′
′
tilt
pan
y
x
zoomrotate
rotatezoom
y
x
 
其中， [ ]tyx 、 [ ]tyx ′′ 分別代表攝影機運動前後的影像點座標(對應到相同物體點)。pan 與
tilt 則代表小角度 panning 或 tilting 時，在影像平面上產生的位移量 (分別於 x 及 y 方向上)。
根據上面的假設，本計畫選取多數之移動向量視為攝影機運動產生之影響，另外，此
4-parameter model 與 affine model 非常相似，因此可以採用類似 least square estimation 的方法
70 
現問題時，我們將企圖找出周遭正確的 Macroblock 移動向量，取這些向量的平均值來修正替
代其原始的移動向量。為什麼說是周遭正確的  Macroblock 移動向量呢？因為周遭的 
Macroblock 移動向量也有可能是錯的，錯誤的 Macroblock 移動向量會導致我們的修正產生
誤差，所以我們要找出周遭正確的 Macroblock 移動向量。 
   
 
  
   
   
 
  
   
中央 Macroblock 有錯誤 中央 Macroblock 正確 
 
圖 6.5 MACROBLOCK 移動向量出現錯誤的例子 
 
例如圖 6.6 中，MB #5 的移動向量有錯誤，所以利用週遭 MBs 的移動向量做修正，但
是 MB #7 所擁有的移動向量是錯誤的，所以我們只取 MBs #1, #2, #3, #4, #6, #8, #9 的移動
向量作平均，來取代之前錯誤的 MB 移動向量值。當然也可能 MB #5 周遭 MB 移動向量
的關係很雜亂而無法估測，在此情況下，我們不針對 MB #5 進行移動向量估測。 
 
1 2 3 
4 5 6 
7 8 9 
取1.2.3.4.6.8.9的移
動向量之平均當作為
MB #5 的新移動向
量 
7
1 2 3 
4 5 6 
7 8 9 
 
圖 6.6 錯誤移動向量之重新估測 
6.2.3 平滑區域移動向量限制 
對於前述的移動向量是由一般視訊編碼常用的 exhaustive block-matching algorithm 方法
來估測，其演算法只考慮到編碼效益而並未考慮到影像中真正的移動向量為何。為了更接近
真實的移動向量情況，在此參照文獻[70]提出之方法，加入 MAD (mean absolute difference) 的
限制，以防止因為天空或牆壁等平滑區域所造成的移動向量估測錯誤，公式如下式(6.1)： 
 
(6.1)          
 
其中，i 為跟目前 Macroblock 相鄰 8 個方向的 Macroblock 序號，M 為 8。當此值小於一個門
檻值則判定為平滑區域，移動向量定為 0，例如在天空或牆壁等平滑區域都應將滿足此式。 
1
1 ( )
M
i
MAD i Th
M =
<∑
72 
  
(a)             (b) 
  
(c)             (d) 
  
(e)             (f) 
圖 6.7 多解析度深度線索特徵值擷取 
(a)原始影像  
(b)motion  
(c)edge density 
(d)variance 
(e)contrast  
(f)initial depth map 
 
為了整合不同性質之特徵，本計畫中將上述 4 種不同特徵值進行線性加乘融合成最終的
深度圖，如圖 7(f)。此外，為了使特徵的擷取能更為強健，本計畫將在不同解析度下去擷取
特徵，其流程示意圖如下圖 6.11 此計算三種解析度，包括原影像解析度，原來的 1/2 解析度
和原來的 1/4 解析度下的特徵值，最後經過適當的線性加乘得到最終的深度圖如圖 6.12 所示。 
 
 
圖 6.8 材質漸層示意圖  圖 6.9 LAW’S MASKS 
 
 
圖 6.10 大氣透視示意圖 (摘自文獻[72]) 
74 
motion Variance Edge density contrast
Initail depth map
*? 1 *? 2 *? 3 *? 4
 
圖 6.13 特徵融合流程圖 
 
 
 
                                       (6.5) 
 
 
 
對於影像中任一位置，我們可以求得 a、b、c、d 四種特徵值，而圖 6.7(a)所用的測試影像為
Microsoft 所提供的 break dancer 影像，並提供相對應的深度圖，我們以此深度值做為真實深
度值 (ground truth depth) ，來估測出我們的線性解 ω1 到 ω4。實驗結果顯示，以真實的深度
圖為依據去估測線性解，可以找出較為適當的參數，使得結果較趨近於真實深度值。 
6.4 前景深度修正與背景深度指派 
6.4.1 視訊內容分析與分類 
根據視訊內容與深度線索特徵之性質，我們將視訊分成下列三種類型： 
(1) 移動量大且背景單純 
(2) 移動量小 
(3) 移動量大且背景複雜 
針對上述三種類型之視訊，本計畫分別設計出合適之前景物體切割法則，用以輔助深度
值之修正與指派。為了分類此三種視訊，我們以一個 video shot 為處理單元 (假設已進行 shot 
segmentation 處理)，配合移動量和背景複雜度分析來達成。後續，我們將分別針對移動量和
背景複雜度分析進行說明。 
(1) 移動量分析 
對於一個 video shot 之移動量分析，先計算此 shot 內之平均動量。每張畫面之平均動
量之定義如下： 
∑∑ −−−
x y
d
WH
TtyxItyxIU
NN
))1,,(),,((1                        (6.6) 
其中，NH 為畫面高度，NW 為畫面寬度，I(x,y,t)表示第 t 張影像中在(x,y)處之灰階值，Td 表
示前後影像間絕對差值之門檻值， )(⋅U 代表 unit step function。 
整個 video shot 之平均動量之定義如下： 
∑∑∑ −−−
t x y
d
tWH
TtyxItyxIU
NNN
))1,,(),,((1                       (6.7) 
 
其中，Nt 表示此 shot 內畫面之張數。若此平均動量小於一個門檻值，則判斷為第二種類型
且採用前景切割法則 2。例如，Akiyo 即屬於第二種類型之視訊。 
1 1 1 1   1
1
1 1 1 1   2
2
                     
3
                        
4
     n n n n n
a b c d ground truth depth
a b c d ground truth depth
a b c d ground truth depth
ω
ω
ω
ω
⎡ ⎤ ⎡ ⎤⎡ ⎤⎢ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢ ⎥⋅ = ⋅⎢ ⎥⎢ ⎥ ⎢ ⎥⋅ ⋅⎢ ⎥⎢ ⎥ ⎢ ⎥⎣ ⎦⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦
76 
• 形態學 (Morphology) 處理 
對於二值化後的初始深度圖，因為雜訊或深度估測錯誤的影響，可能造成切割出的前景
輪廓與實際物體相差太遠，或是背景誤判為前景。對此我們將使用形態學影像處理中的斷開 
(Morphological opening) 來消除雜訊造成的影響，並且讓前景輪廓趨於平滑。 
•相鄰單元標記法 (Connected Component Labeling) 
相連單元標記 (Connected Component Labeling) 或稱編號演算法 (Labeling)，此方法的目
的是將二值化影像中的每一個區塊標記成相同且唯一的整數，經由相連區域標記後，我們可
以得到各個區域的標記編號和面積，這些資訊可以作為後續步驟之用。 
 
• 區域移除 (Region removal) 
對於一般視訊畫面而言，前景物體通常佔畫面的面積較大，因此我們以每個標記區域的
面積為依據，面積小於一個門檻值 (此值跟畫面大小有關) 則予以刪除，設為 0，藉此消除背
景誤判為前景或雜訊造成的小錯誤。除此之外，在一般視訊畫面中，前景物體的底部線通常
位於影像的下半部，因此我們將區域底部線位於影像上半部的區域予以刪除。 
 
• 空洞填補 (Hole filling) 
由於物件中可能包含平滑區域，造成估計出之深度初始值較小。經過二值化處理後容易
被誤判為背景區域。此情況會造成物件內部產生空洞現象。為了消除物件內空洞現象，本計
畫將針對任一個標記之背景區域被前景區域包圍和任一個標記之背景區域只與影像其中一個
外框邊緣相鄰者做處理，將該背景區域修正為前景區域。 
 
• Delaunay 三角化修正 
經過以上步驟，我們可以得到 block-based 的前景物體區域圖，雖然切割出的前景位置
與實際前景位置大致上符合，但因為此前景圖是以區塊為單位，切割出的輪廓無法與實際物
體輪廓吻合。在此步驟中，我們將使用 Delaunay 三角化做前景廓輪修正。建構 Delaunay 三
角化最簡易的演算法為 Bowyer-Watson [73]。藉由 Delaunay 三角化圖和 block-based 前景圖
做比對，得到較符合實際物體輪廓的前景圖。其步驟如下： 
(1) 對於 Delaunay 三角化圖，我們對每個三角形區域做相鄰單元標記，得到各個區域的面積。 
(2) 統計各區域中， block-based 前景區域佔任一區域的比例，若大於一個門檻值，則此區域
設為前景區域。 
 
• 前景物體深度值指派 
我們將對上述切割出的前景物體指派深度值。其方法為對前景區域，給定初步深度值，
再對前景區域中，深度值過小的位置，以左、右鄰近的深度峰值進行內插。 
6.4.2.2 法二：累積畫面差值+ Delaunay 三角化修正之前景切割技術與深度值指
派 
對於移動量較小的視訊內容而言，例如新聞畫面、面談畫面等，不論背景複雜度大或小，
要切割出前景移動物體相較於移動量大的視訊容易的多。在此，我們將利用畫面差值做為前
景移動物體偵測的方式，但由於移動量較小，單單取前後張畫面的差值並不足夠，因此以下
將使用累積多張畫面的差值來做為偵測的依據，選取的畫面張數將跟移動量分析有關，移動
量愈小，取的畫面張數要愈多，資訊才足夠，最後再配合前述的 Delaunay 三角化做修正，
即可將前景移動物體切割出。流程如圖 6.15 所示。 
78 
如同方法一，由於前景物體佔畫面的比例較高，因此我們將消除標記後面積小於一個門
檻值 (此值跟畫面大小有關) 的區域，只留下面積較大的前景。 
 
• 靜止狀態判斷與修正 
由於此方法處理的是移動量較小的視訊類型，經常會因為某一段畫面中前景幾乎不動而
造成偵測上的困難。因此我們將每個時間點的移動像素個數佔畫面比例 (做完區域移除步驟
後的結果) 記錄起來，以此作為靜止狀態的判斷準則。若前後張畫面的比例相差太大，代表
靜止狀態發生，則將保留前一張的偵測結果到後一張來，如此可防止前景短期物體不移動所
造成的偵測錯誤。 
 
• Delaunay 三角化修正 
此步驟如同法一的 Delaunay 三角化修正一般，在此不多加贅述。但不一樣的地方是，
由於方法二處理的視訊內容背景複雜度可能很高，因此必須把區塊大小縮小為 4×4，修正後
的前景才能夠精細而不被背景干擾。 
 
• 輪廓平滑化 
經過 Delaunay 三角化修正後的前景物體輪廓，因為是以 mesh-based 方式做切割，造成
輪廓呈現鋸齒現像，對此我們將使用形態學中的閉合 (closing) 處理，使用接近於圓形的基本
運算子 (structuring element)，如此可以將鋸齒狀的輪廓抹平。 
 
• 前景移動物體深度值指派 
如同方法一，在此也將以初步深度值來做為前景物體深度值指派的依據，並將前景區域
中，深度值過小的區域做內插。 
 
6.4.2.3 法三：移動向量之前景深度值指派 
在移動量大且背景複雜度高之情況下，要正確地切割出前景是非常困難的。此外，一個 
video shot 內，移動量大意即存在較大之移動向量。由於此種類型視訊中，移動向量存在極高
的重要性，所以，我們不再進行前景物體切割處理，而直接採用初始深度值做為最終的估計
深度值，即初始深度值只含有移動向量特徵資訊。 
6.4.3 背景深度指派 
背景深度指派包括以下幾個步驟。 
1.將所有切割出的前景區域進行標記，計算出各個物體的底部線。根據所有物體之底部
線，找出底部線最高的位置，藉此找出最遠物體。 
2.以最高底部線的位置為基準，高出影像長度十分之一的位置作為消失點(線)。 
3.對於消失點以上，則給定 0 的深度值。 
4.對於消失點以下，則給定深度梯度值，公式如下： 
)/()(255),( vpHvp
B yNyyyxD −−×=  
其中 y 為影像坐標 (以左上角為原點) 上任一點位置的高度，yvp 為消失點在影像坐標上的
高度，NH 為影像高度。最後將前景、背景深度圖取最大值運算，即可得到完整的深度圖。 
6.5 基於深度影像繪圖法之立體影像對合成技術 
所謂 DIBR 技術是為了模擬人眼看真實世界的情況而發展出來。當人們眼睛注視著前方一
80 
的左、右眼影像相似的立體效果和品質。目前測試影像包括 Microsoft 提供的 break dancer 和
ballet 影像，如圖 6.16、6.17，與測試沒有深度圖的影像如 weather 影像。 
本計畫使用 SHARP 立體液晶顯示器做為觀看立體影像的方式，使用裸眼立體顯示器的好
處是可以避免傳統使用偏光式立體眼鏡可能造成鬼影產生，使用交錯式立體眼鏡的成本又太
高，容易損壞，且在觀賞時容易產生閃爍的情況。 
 在估測出場景深度後，本計畫將以 DIBR 技術合成左、右眼影像。接著以主觀人眼評估
來分析各種立體影像的立體效果。我們對 10 至 12 個人進行測試，對立體視訊打 0 至 10 的分
數，並且不事先告知立體影像的產生方式，以維持公平性。深度影像與合成左、右眼影像如
圖 6.18、圖 6.19、圖 6.20 和圖 6.21 所示。 
 
  
(a)                (b) 
圖 6.16 實驗結果(I) 
(a) break dancer 測試影像 
(b)相對應的真實深度圖 
 
  
(a)                (b) 
圖 6.17 實驗結果(II) 
(a) ballet 測試影像 
(b)相對應的真實深度 
82 
  
(a)                 (b) 
  
(c)                 (d) 
圖 6.20 實驗結果(V) 
(a) Salesman 測試影像，  
(b) 估測深度圖  
(c) 合成出的左眼影像， 
(d) 合成出的右眼影像 
 
    
(a)           (b) 
 
    
(c)          (d) 
圖 6.21 實驗結果(VI) 
(a) Manege 測試影像 
(b) 估測深度圖 
(c) 合成出的左眼影像 
(d) 合成出的右眼影像 
 
84 
第七章 結論 
7.1 計畫整合結果 
在音視訊的錄製擷取上，子計畫一與子計畫二同步進行多視角影像進行同步錄製，錄製
過程使用 16 支麥克風陣列搭配 13 之攝影機來建立一個 3D 的音場環境，以進行同步環場音
視訊擷取。而在音訊儲存方面，本計畫使用 MPEG Surround 進行多通道音訊之壓縮，並利
用通道間的相關性做最佳壓縮，及配合子計畫三發展出音視訊的系統多工與同步方式。接著
在互動式播放呈現的部份，子計畫四根據使用者回饋之視角資訊利用視角內差方式實現自由
空間場中任意視角立體影像合成。子計畫一實作出自由聆聽角 5.1 聲道與提出自由視角 7.1
聲道重建方法，以及提出盲蔽訊號分離改善方案，有效地提升分離音源的效能，並提出外加
刺激訊號伸展派波，更加地提升分離演算法的效益。如此，加上多角度音訊內插方法，搭配
多聲道、HRTF 3D 音源定位與音效處理，來提供綿密的音場讓使用者達身歷其境的效果。
另外，子計畫一配合子計畫五經網路傳輸的媒介，利用有限的頻寬通道，進行多通道的音訊
壓縮，以同步重建使用者的需求。子計畫二設計了一套新的環場重建方式，利用可見外殼上
的幾何特性，有效地估測出物體的深度範圍，減少了計算量以及增加了準確性。 
在子計畫三中，多視域視訊整合表示法之壓縮的設計，除了考慮與音訊進行多工整合
外，同時依據視訊擷取(子計畫二) 所獲得資料間的相互關係，改良出 3D 視訊採用動態三維
網格的表示法，並探討其壓縮法則，此方法相較於 MPEG-4 AFX-3DMC 可以達到減少 20%
的資料量。此外，子計畫三提出了 DIBR-based inter-view prediction 的方法，具有隨機存取的
能力，以提供子計畫四在多視域視訊自由視角播放，影像內差合成可隨時取得所需之視角與
空間資料 (不需從頭解壓縮)。子計畫四的自由視角立體播放與子計畫一 3D 音訊播放進行整
合以完整同步呈現自由視角的影音表現。而在子計畫五中，在 2D/3D video conversion，提出
一個全自動的基於單視域來源之立體視訊轉換技術，藉由各種深度線索得到初步深度估測
圖，接著以 Delaunay 三角化為基礎做前景物體的切割，並指派前景與背景適當的深度值，
最後再經由深度影像繪圖產生出我們所要的立體效果。這些相關資訊提供給子計畫三 3D 視
訊壓縮所使用，此外為了使 2D 視訊轉成 3D 視訊後壓縮傳送，子計畫五與子計畫三之立體
視訊壓縮技術相互合作。另一方面，在有限頻寬之限制下，僅能接收到單一視域視訊且無視
差資料，則 2D/3D video converter 利用此接收之單一視域視訊資料產生視差資訊，提供子計
畫四進行多視域之影像內差處理所使用。 
綜觀上述結果，本總計畫更進一步落實 3D-TV、3D video、audio streaming 等視聽娛樂與
遠距醫療等技術上的應用。使人們更滿足虛擬實境中的全方位任意視角之感受，以達到身入
其境、重建原音視訊的視聽體驗。 
 
 
7.2 成果自評 
在子計畫一的部份，對於音源分離的前處理部份提出了以外加刺激訊號“伸展脈波”，經
86 
stereo video encoding apparatus for implementing the same”[81]。第二年計畫成果分別發表了論
文"A  Fast H.264-based Stereo Video Encoding Algorithm Based on Hierarchical Two-stage 
Neural Classification,"[82] 以 及 “Coding of Dynamic 3D Mesh Model for 3D Video 
Transmission,”[83]。 
 
子計畫四方面，提出了可讓合成的虛擬視角攝影機影像擁有好的視覺品質，並且在不同
視角轉換的狀況下也非常平滑，將影像問題如 Hole Problem、Occlusion Problem、Dis-occlusion 
Problem、Erroneous Depth-image Problem、Depth inconsistency in synthesized video 提出解決方
式。此外，在 DIBR 與模型式合成法於自由視角影像上之合成可看出，當擺設攝影機的彎曲
角度過大和距離過遠時，使用 DIBR 會使得遮蔽區域過大，無法有效合成出較好的結果，而
使用 3D model-based 合成法則會有較佳的效果呈現。相關成果已發表於，The 21st IPPR 
Conference on Computer Vision, Graphics and Image Processing [84]。 
 
子計畫五方面，根據目前之研究結果而言，與原計畫內容之符合度很高，已經達成本計
畫預期的進度與成果。由於 2D/3D video conversion 之技術，將有助於數位內容 3D 影像/視訊
相關產業技術層面的豐富與擴展，也有助於智慧生活空間科技的厚實，因此，本計畫的研究
成果已準備申請專利。此外，本計畫之研究成果具有相當之學術價值，目前已經有部分之成
果已經發表，其他部分也將陸續發表於國際會議與國際期刊中。透過本子計畫之執行，對於
學生在英文論文閱讀能力、思考與解決問題、程式撰寫與實作、系統整合之能力之等各方面
的訓練，均有著非常大的助益。子計畫五之相關成果已發表於 IDMC/3DSA/Asia Display 2009 
[85]。 
 
 
 
 
 
 
 
 
 
88 
and Systems, pp. 669-672, Aug. 2009.  
[17] Yi-Hsiang Cheng, Chun-Hung Chiu, Wen-Chih Wu and Oscal T.-C. Chen, “Pre-processing 
Scheme to effectively compensate environment and equipment factors for sound source 
separation,” submitted to IEEE international Symposium on Circuits and Systems, Paris, 
France, May 2010. 
[18] Emmanuel Gallo, Nicolas Tsingos and Guillaume Lemaitre, “3D-AudioMatting, postediting, 
and rerendering from field Recordings,” EURASIP Journal on Advances in Signal Processing, 
article ID 47970, 16 pages, 2007. 
[19] M. Li, H. Schirmacher, M. Magnor and H.-P. Seidel, “Combining stereo and visual hull 
information for on-line reconstruction and rendering of dynamic Scenes,” Proc. of IEEE 
Workshop on Multimedia Signal Processing, pp. 9-12, 2002. 
[20] C. Hernandez Esteban and F. Schmitt., “Multi-Stereo 3D object reconstruction,” Proceedings 
of International Symposium on 3D Data Processing, Visualization and Transmission, pp. 
159-166, 2002. 
[21] R. Tsai. “A versatile camera calibration technique for high-accuracy 3dmachine 
visionmetrology using off-the-shelf TV cameras and lenses,” IEEE Trans. Robotics and 
Automation, pp. 323–344, 1987. 
[22] E. Trucco and A. Verri, Introductory techniques for 3-D computer vision, Chapter 10, Upper 
Saddle River N.J. : Prentice Hall, 1998. 
[23] R. Hartley and A. Zisserman, Multiple view geometry in computer vision, 2nd Ed. London: 
Cambridge University Press, 2003. 
[24] German K. M. Cheung, Simon Baker and Takeo Kanade, “Visual hull alignment and 
refinement across time: a 3D reconstruction algorithm combining shape-from-silhouette with 
stereo,” Computer Vision and Pattern Recognition, pp. 375-382, 2003. 
[25] Nina Amenta, Sunghee Choi and Ravi Krishna Kolluri, “The power crust,” ACM Symposium 
on Solid and Physical Modeling, pp. 249-266, 2001. 
[26] T Matsuyama, XJ Wu, T Takai and T Wada, “Real-time dynamic 3-D object shape 
reconstruction and high-fidelity texture mapping for 3-D video,” IEEE Transactions on 
Circuits and Systems for Video Technology, pp. 357-369, 2004 
[27] Pooja Verlani, Aditi Goswami, P. J. Narayanan, Shekhar Dwivedi and Sashi Kumar Penta, 
“Depth image: representations and real-time rendering,” 3D Data Processing, Visualization, 
and Transmission, pp. 962-969, 2006. 
[28] Yo-Sung Ho and Kwan-Jung Oh, “Overview of multi-view video coding,” Proc. of 
International Workshop on Systems, Signals and Image Processing, pp. 5-12, 2007. 
[29] Yushan Chen, Canhui Cai and Jilin Liu, “YUV correction for multi-view video compression,” 
Proc. of Int’l Conf. on Pattern Recognition, vol. 3, pp. 734-737, 2006. 
[30] Oliver Schreer, Peter Kauff, Thomas Sikora, 3D Video communication, New York: John Wiley 
& Sons. Ltd., 2005. 
[31] Emin Martinian, Alexander Behrens, Jun Xin and Anthony Vetro, “View synthesis for 
multi-view video compression,” Proc. of Picture Coding Symposium, 2006. 
[32] S. Shimizu, M. Kitahara, H. Kimata, K. Kamikura, Y. Yashima, “View scalable multiview 
video coding using 3-D warping with depth Map”, IEEE Trans. on Circuits and Systems, vol. 
17, pp. 1485-1495, 2007. 
[33] P. Merkle, A. Smoli, K. Muller and T. Wiegand, “Multi-view video plus depth representation 
and coding,” Proc. of IEEE Int’l Conf. on Image Process, vol. 1, pp. 201-204, 2007. 
[34] Xing San, Hua Cai, Jian-Guang Lou and Jiang Li, “Multi-view image coding based on 
geometric prediction,” IEEE Trans. on Circuits and Systems, vol. 17, pp. 1536-1548, 2007. 
[35] P. Merkle, A. Smolic, K. Muller and T. Wiegand, “Multi-view video plus depth representation 
and coding,” Proc. of IEEE Int’l Conf. on Image Process, vol. 1, pp. 201-204, 2007. 
[36] A. Laurentini, “The visual hull concept for silhouette-based image understanding,” IEEE 
90 
based on depth maps created from blur and edge information,” Proc.of SPIE, vol. 5664, pp. 
104-115, 2005. 
[60] 林峻永，應用於立體顯示器之二維轉立體靜態影像技術，國立交通大學碩士論文，2005
年。 
[61] Joohwan Kim, Yunhee Kim, Junghyun Park, Jin-mo Kang and Byoungho Lee, “Stereoscopic 
conversion of two-dimensional movie encoded in MPEG-2,” Proc. of SPIE, vol. 6311, 2006. 
[62] Yu-Lin Chang, Chih-Ying Fang, Li-Fu Ding, Shao-Yi Chen and Liang-Gee Chen, “Depth map 
generation for 2D-to-3D conversion by short-term motion assisted color segmentation,” Proc. 
of IEEE International Conference on Multimedia and Expo, pp.1958 – 1961, 2007. 
[63] S. Battiato, A. capra, S. Curti and M. L. Cascia, “3D stereoscopic pairs by depth-map 
generation,” Proceeding of the 2nd International Symposium on 3D Data Processing, 
Visualization, and Transmission, pp. 124-131, Jun. 2004. 
[64] S. Battiato, S. Curti, M. La Cascia, E. Scordato and M. Tortora, “Depth map generation Bny 
image classification,” Proc. of SPIE IS&T/SPIE's 16th Annual Symposium on Electronic 
Imaging, 2004. 
[65] Yu-Lin Chang, Wei-Yin Chen, Jing-Ying Chang, Yi-Min Tsai, Chia-Lin Lee and Liang-Gee 
Chen, “Priority depth fusion for the 2D to 3D conversion system,” Proc. of SPIE, vol. 6805, 
2008. 
[66] Bela Julesz, Foundations of Cyclopean Perception, Cambridge Massachusetts: MIT Press, 
2006. 
[67] Y. Wang, J. Ostermann and Y.-Q. Zhang, Video Processing and Communication, New Jersey: 
Prentice Hall, 2002. 
[68] Y.-P.Vtn, S. R. Kulkarni and P. J. Ramadge, “A new method for camera motion parameter 
estimation,” Proc. of IEEE Int’l Conf. Image Processing, vol. 1, pp. 406-409, 1995. 
[69] Y. Su, M.-T. Sun and V. Hsu, “Global motion estimation from coarsely sampled motion vector 
field and the applications,” IEEE Trans. on Circuits and Systems for Video Technology, vol. 15, 
no. 2, Feb. 2005. 
[70] Shih-Hsuan Yang and Fu-Min Jheng, “An adaptive image stabilization technique,” Proc. of 
IEEE International Conference on Systems, Man and Cybernetics, vol. 3, pp. 1968-1973, 
2006. 
[71] E. R. Davies, Laws’ Texture Energy in TEXTURE. In Machine Vision: Theory, Algorithms, 
Practicalities, 2nd Edition. San Diego, Academic Press, 1997. 
[72] 陳明民，數位立體影像之理論探討與創作實驗之研究，國立嘉義大學碩士論文，2005 年。 
[73] 吳振君, 施斌, 祁長青, 任意多介質區域變尺寸有限元網格自動剖分, 工程地質電腦應
用，2002 年 4 期 
[74] http://www.dsp.ee.ccu.edu.tw/ochen/demo.rar 
[75] H. Y. Lin and W. Z. Chang, “High dynamic range imaging for stereoscopic scene 
representation,” Proc. of IEEE International Conference on Image Processing, pp. 4305-4308,   
Nov. 2009. 
[76] H. Y. Lin, T. W. Chen, C. C. Chen, C. H. Hsieh and W. N. Lie, “Human pose estimation from 
monocular image captures,” Proceedings of the IEEE International Conference on Multimedia 
& Expo, pp. 994-997, June. 2009. 
[77] H. Y. Lin and J. R. Wu, “3D reconstruction by combining shape from silhouette with stereo,” 
Proceedings of the 19th International Conference on Pattern Recognition, Tampa, Florida, 
USA, Dec. 2008. 
[78] H. Y. Lin and K. D. Gu, “Depth recovery using defocus blur at infinity,” Proceedings of the 
19th International Conference on Pattern Recognition, Tampa, Florida, USA, Dec. 2008. 
[79] Jui-Chiu Chiang, Lien-Ming Liu, and Wen- Nung  Lie, “A hierarchical two-stage 
neural-classifier for mode decision of H.264/AVC stereo video encoding,” Proc. of the 2 nd 
