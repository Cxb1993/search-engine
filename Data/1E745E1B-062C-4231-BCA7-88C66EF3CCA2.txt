 
中 文 摘 要 ： 最長共同子序列問題在近 30年內被大量的討論與研究，它的
應用層面廣泛，包含語音辨識、資料搜尋、生物資訊等。後
來有學者開始對限制型最長共同子序列進行研究，亦即對答
案做一些限制：必須包含給定的序列為子序列、不能包含給
定的序列為子序列、必須包含給定的字串為子字串、不能包
含給定的字串子字串。本計劃報告包含兩個與限制型共同子
序列相關的問題之成果，(1)結合要包含的字串與要包含的序
列兩種限制而擴展出更一般化的「序列性子字串限制型
LCS」，此問題是給定兩條字串 A 和 B，長度分別為 m和 n，
以及一組 k條有順序的限制子字串(C1, C2, ..., Ck)所構成
的限制序列 C，希望找出一條 A和 B的共同子序列 D 滿足以
下條件(a). C裡面的所有限制子字串都是 D的子字串 (b). 
(C1, C2, ..., Ck)在 D裡面依序出現 (c). D 是最長的。
(2)當限制條件(字串、序列)有很多條的時候，對於包含、不
包含的變形，我們也設計良好的演算法，另外，我們也針對
生物資訊方面提出了一些延伸應用。 
中文關鍵詞： 限制型最長共同子序列、動態規劃、序列型子字串 
英 文 摘 要 ： The longest common subsequence (LCS) problem has been 
widely discussed and studies in the past thirty 
years. Applications of LCS problem spread in voice 
recognition, data mining and bioinformatics. Later 
on, some researchers study the constrained LCS 
problem that the solution is restricted: should 
contain a given sequence as its subsequence, should 
not contain a given sequence as its subsequence, 
should contain a given string as its substring, and 
should not contain a given string as its substring. 
In this report, we have two main results on the 
constrained LCS problem. (1) Combine the string-
inclusion and sequence-inclusion constrained LCS 
problems to form a more generalized model 
called ＇sequential substring constrained LCS.＇ In 
this problem, we are given two strings A and B with 
lengths m and n, respectively and a constraint 
sequence C formed by k constraint substrings (C1, 
C2, ..., Ck). Our aim is to find a common subsequence 
D of A and B satisfying the following conditions: 
(a). D contains all constraint substrings C as its 
substrings. (b). (C1, C2, ..., Ck) appears in D 
sequentially. (c). D is the longest. (2) When several 
 
 
-1- 
 
一、 前言與研究目的 
最長共同子序列(Longest Common Subsequence, LCS)問題最主要是找出兩個
字串或檔案的相似度，正式定義為：給定兩條字串 A = {a1, a2, ..., am}和 B = {b1, 
b2, ..., bn}，我們希望找出從 A 和 B 各刪除一些字之後剩下的這兩條字串最長的
共通部份，這個問題最早是由 Wagner 和 Fisher 在 1974 年提出的[Wagn74]。三十
多年來，有很多論文討論這個問題以及它相關的變形。利用 Google 學術搜尋，
於 2012 年 10 月，用”longest common subsequence”當作關鍵字搜尋，可以會找到
16000 筆結果，由此可見受重視之程度。LCS 問題可以應用於許多領域，包含檔
案比對、資料庫模糊搜尋、語音辨識，還有最近被大量使用的生物資訊領域。多
條序列的 LCS 可以被用來當作這些序列的身份證，假如這些序列是屬於同一個
物種，LCS 就是這個物種的序列中都包含的部份，就有可能是生物上有意義的區
域，例如 motif、promoter、reception sites 或是 conserved regions。之後可再把每
個物種的共同區域拿來做比對，以做為物種之間親緣關係的參考。 
LCS 的一種變形叫做限制刑 LCS，在這個問題中，除了給予兩條字串 A 和 B
之外，另外還有一條限制字串 C = {c1, c2, ..., cr}，要找的是 A 和 B 共同子序列中
包含 C 為子序列的最長一條。此問題最主要的貢獻，在於生物的序列比對。在
生物的意義上，C 可視為生物學家所特別指定需要對齊的地方。因此，相較於原
本沒有任何預設立場的序列排列，此種方法將更有生物上的意義。所以，也有許
多學者進一步把此問題改成 constrained alignment，並且實作出一些可供生物學
家使用的工具軟體[Lu05, Tang03, Tsai04]。 
台灣大學陳怡靜博士在 2009 年在 Journal of Combinatorial Optimization 發表
了”On the generalized constrained longest common subsequence problems”論文
[Chen11]，該篇論文也啟發了本研究計畫的動機。在該篇論文中，陳博士提出了
與傳統限制型共同子序列類似的另外三種變形，演算法都是使用動態規劃：第一
種是包含C為子字串的限制型 LCS，第二種是不包含C為子序列的限制型 LCS，
第三種是不包含 C 為子字串的限制型 LCS，Gotthilf 等人[Gott10]也在 2010 年提
出了針對不包含C為子序列的限制型LCS的演算法。2010年，陳怡靜博士[Chen10]
又提出了整合原始與第一種變形的限制型 LCS 的問題，也就是給定四條字串 A、
B、P 和 Q，要找出 A 和 B 的共同子序列中包含 P 為子序列但是不包含 Q 為子
序列中最長的，並提出了兩個演算法。 
本計畫成果報告為延伸其他學者所提出的限制型共同子序列問題，整合不同
變形、增加限制字串的條數以及尋求於生物資訊方面之應用，本報告提出各項研
究成果已分別於期刊及國內外研討會發表，茲簡要說明如下。 
1. 尋找包含序列性子字串限制型最長共同子序列之演算法：給定兩條字串
以及由一群有序之子字串構成之限制，尋找這兩條字串間依序包含這些
子字串之限制型最長共同子序列[Tsen12]。 
2. 使用支援向量機預測必須蛋白質：給定一堆蛋白質，我們要從中找出哪
一些是必要蛋白質[Yang11]。 
 
 
-3- 
 
我們使用的動態規劃式如下，Mሾi, j, 0ሿ紀錄不考慮限制條件的 LCS 長度，可由傳
統的 LCS 動態規劃式取得，而Mሾi, j, 1ሿ則是記錄包含限制字串為子字串的限制型
LCS 長度，其中஺ሾ݅ሿ (஻ሾ݆ሿ)代表字串 A (B)中假設 C 的結尾和ܽ௜ ( ௝ܾ)配對則 C
的開頭會對應到的位置。 
Mሾi, j, 1ሿ ൌ max
ۖە
۔
ۖۓ െ∞ if	݅ ൑ 0	or	݆ ൑ 0Mሾi െ 1, j െ 1, 1ሿ ൅ 1 if	ܽ௜ ൌ ௝ܾ
ܯൣ஺ሾ݅ሿ െ 1, ஻ሾ݆ሿ െ 1,0൧ ൅ ݎ if	ܽ௜ ൌ ௝ܾ ൌ ܿ௥
Mሾi, j െ 1, 1ሿ
Mሾi െ 1, j, 1ሿ otherwise
 
對於限制字串有多條，且在限制型 LCS 出現的地方是不可重疊的版本，我
們提出的動態規劃如下，其中஺௞ሾ݅ሿ (஻௞ሾ݆ሿ)代表字串 A (B)中假設 Ck 的結尾和ܽ௜ 
( ௝ܾ)配對則 Ck的開頭會對應到的位置，時間複雜度為 O(mnl+(m+n)(|Σ|+r))， 
Mሾi, j, kሿ
ൌ max
ە
ۖۖ
ۖ
۔
ۖۖ
ۖ
ۓ െ∞ if ݇ = 0 and (i < 0 or j < 0);0 if	݇	 ൌ 	0	and	݅	 ൌ 	0	and	݆	 ൒ 	0;
0 if	݇	 ൌ 	0	and	݅	 ൒ 	0	and	݆	 ൌ 	0;
െ∞ if	݇	 ൒ 	1	and	ሺ݅	 ൑ 	0	or	݆	 ൑ 	0ሻ;
Mሾi െ 1, j െ 1, kሿ ൅ 1 if	ܽ௜ ൌ ௝ܾ
ܯൣ஺௞ሾ݅ሿ െ 1, ஻௞ሾ݆ሿ െ 1, ݇ െ 1൧ ൅ ݎ if k ≥ 1 and	ܽ௜ ൌ ௝ܾ ൌ ܿ௟ೖ௞
Mሾi, j െ 1, kሿ
Mሾi െ 1, j, kሿ otherwise
 
對於限制字串是多條，而且在限制型 LCS 出現的地方是可重疊，但是後面
的限制字串不能出現在前面的限制字串之前，我們亦提出一個動態規劃演算法，
時間複雜度為(mnr + (m + n)|Σ|)。對於只有兩條限制字串的情形，我們的演算法
也比前人的方法快了 l 倍[Chen11]。 
 
2.2 排除子字串之限制型共同子序列之演算法 
在此報告中，我們針對給定兩條字串 A、B 以及多條總長度為 R 的限制字串
C 1, C 2, … C k，要找出 A 和 B 的共同子序列中不包含任一條限制字串為子字串
的最長一條，我們同樣是使用動態規畫來取得限制型 LCS 的長度，並參考了
KMP[Knut77]及 Aho-Corasick[Aho75]的方法，來判斷當有字元對應到的時候，我
們的狀態要如何變化以確保不會包含任一條限制字串。例如當限制字串為{he, 
she, his, hers}的時候，我們的狀態轉換如下圖，一旦走到限制字串所代表的狀態
則這個新的字元就不能被加入到限制型共同子序列中，我們的時間複雜度為
O(mnR) 
 
 
-5- 
 
6 Neighbors' intra-degree  T 1 0.6181
7 Essential index O 1 0.6171
8 Clique level  T 1 0.6141
9 Degree related to all interactions  T 1 0.6067
10 Common function degree  O 1 0.6058
11 Clustering coefficient  T 1 0.5981
12 Betweenness centrality related to all interactions T 1 0.5981
13 Other process  P 1 0.598 
14 Density of maximum neighborhood component  T 1 0.5859
15 Maximum neighborhood component   T 1 0.5841
16 Closeness centrality  T 1 0.5689
17 Degree related to physical interactions  T 1 0.5686
18 Edge percolated component  T 1 0.5584
19 Other localization  P 1 0.5582
20 Open reading frames length  O 1 0.5449
21 Cytoplasm  P 1 0.534 
22 Average amino acid PSSM  S 20 0.5325
23 Cell cycle  P 1 0.5313
24 Transcription  P 1 0.5298
25 Mitochondrion  P 1 0.517 
26 Metabolic process  P 1 0.5119
27 Bottleneck  T 1 0.5107
28 Cysteine count  S 1 0.5094
29 Endoplasmic reticulum  P 1 0.5093
30 Cysteine odd-even index  S 1 0.5088
31 Average hydrophobic  S 1 0.506 
32 Signal transduction  P 1 0.505 
33 Average cysteine position  S 1 0.5032
34 Outdegree related to metabolic interaction  T 1 0.5032
35 Outdegree related to transcriptional regulation 
interaction  
T 1 0.4993
36 Indegree related to metabolic interaction T 1 0.499 
37 Average distance of every two cysteines S 1 0.4973
38 Transport P 1 0.4971
39 Betweenness centrality related to metabolic 
interactions  
T 1 0.4964
40 Protein length  S 1 0.4958
 
 
-7- 
 
 
我們針對 Yen[Yen10]的方法的進一步延伸，Yen 的方法是對於每條蛋白質使
用一種預測方法，但仔細分析該方法之數據後發現對於同一條蛋白質中 N、C、
O 位置的預測，兩個預測方法的準確率是有所不同的。因此，我們又將分類器細
分為針對 N、C 和 O 三種。我們決定要使用哪一種方法來作預測的 SVM，使用
以下九種特徵(1)親疏水性(2)凡得瓦力(3)極性(4)極化性(5)體積(6)帶電性(7)原子
重量(8)等電位點(9)溶劑可接觸區域，分別對 N, C, O 預測位置之後，再將的三維
座標整合起來成為最後的預測結果。我們的流程圖如下： 
 
Protein sequence 
and its Cα
coordinates
C-atom 
classifier
N-atom 
classifier
O-atom 
classifier
Choose the tool by 
SVM
Choose the tool by 
SVM
Choose the tool by 
SVM
BBQ Chang BBQ Chang BBQ Chang
Combine the classifiers´ results
3D coordinates 
of all N, C and O 
atoms
Prediction of N-atom Prediction of C-atom Prediction of C-atom
 
我們進行實驗的資料集為 CASP7,CASP8, CASP9, 分別取其只含有標準氨
基酸的蛋白質，各包含 29, 24, 55 條蛋白質。我們以 RMSD 來衡量效能，改進的
幅度如下：CASP7 從 BBQ 的 0.3955 改進為 0.3835, CASP8 從 BBQ 的 0.4437
改進為 0.4313, CASP9從BBQ的0.4133改進為0.3691。完整的實驗數據如下表： 
 
 
-9- 
 
Mathematics and Computation Theory, Taichung, Taiwan, pp. 32-37, 2010. 
6. [Chen11] Y. C. Chen, K. M. Chao, “On the generalized constrained longest 
common subsequence problems,” Journal of Combinatorial Optimization, Vol. 21, 
No. 3, pp. 383–392, 2011. 
7. [Chen12] Kai-Yu Chen, Chang-Biau Yang and Kuo-Si Huang, “Prediction of 
Protein Backbone Structure by Preference Classification with SVM," Proc. of the 
9th International Conference on Information Systems and Technology 
Management, Sao Paulo, Brazil, May 30-June 1, 2012. 
8. [Chun09] Wei-Chun Chung, Chang-Biau Yang and Chiou-Yi Hor, “An Effective 
Tuning Method for Cysteine State Classification,” Proc. of National Computer 
Symposium, Workshop on Algorithms and Bioinformatics, Taipei, Taiwan, Nov. 
27-28, 2009. 
9. [Fari01] P. Fariselli and R. Casadio, “Prediction of contact maps with neural 
networks and correlated mutation,” Protein Engineering, Design and Selection, 
Vol. 14, No. 11, pp. 835–843, 2001. 
10. [Gott10] Z. Gotthilf, D. Hermelin, G. M. Landau, and M. Lewenstein, “Restricted 
LCS," 2010. 
11. [Gron07] D. Gront, S. Kmiecik, and A. Kolinski, “Backbone building from 
quadrilaterals: A fast and accurate algorithm for protein backbone reconstruction 
from alpha carbon coordinates,” Journal of Computational Chemistry, Vol. 28, No. 
9, pp. 1593–1597, 2007. 
12. [Knut77] Donald Knuth; James H. Morris, Jr, Vaughan Pratt, "Fast pattern 
matching in strings," SIAM Journal on Computing, Vol. 6 No. 2, pp. 323–350, 
1977. 
13. [Lin12] Dong-Jian Lin, Chang-Biau Yang and Yung-Hsing Peng, “Protein Contact 
Prediction Based on Protein Sequences," Proc. of the 11th Conference on 
Information Technology and Applications in Outlying Islands, Taitung, Taiwan, 
May 25-26, 2012 
14. [Lu05] C. L. Lu and Y. P. Huang, “A memory-efficient algorithm for multiple 
sequence alignment with constraints”, Bioinformatics, Vol. 21, No. 1, pp. 20-30, 
2005. 
15. [Maup06] J. Maupetit, R. Gautier, and P. Tuffery, “SABBAC: online structural 
alphabet-based protein backbone reconstruction from alpha-carbon trace,” Nucleic 
Acids Research, Vol. 34, pp. W147–W151, 2006. 
16. [Song07] J. Song, Z. Yuan, H. Tan, T. Huber, and K. Burrage,“Predicting disulfide 
connectivity from protein sequence using multiple sequence feature vectors and 
secondary structure,” Bioinformatics, Vol. 23, No. 23, pp. 3147–3154, 2007. 
17. [Tang03] C. Y. Tang, C. L. Lu, M. D. T. Chang, Y. T. Tsai, Y. J. Sun, K. M. Chao, J. 
M. Chang, Y. H. Chiou, C. M. Wu, H. T. Chang and W. I. Chou, “Constrained 
multiple sequence alignment tool development and its application to RNAse 
Journal of Complexity ( ) –
Contents lists available at SciVerse ScienceDirect
Journal of Complexity
journal homepage: www.elsevier.com/locate/jco
Efficient algorithms for the longest common subsequence
problem with sequential substring constraints✩
Chiou-Ting Tseng a, Chang-Biau Yang a,∗, Hsing-Yen Ann b
a Department of Computer Science and Engineering, National Sun Yat-Sen University, Kaohsiung 80424, Taiwan
b National Center for High-Performance Computing, Tainan 74147, Taiwan
a r t i c l e i n f o
Article history:
Received 10 June 2011
Accepted 18 July 2012
Available online xxxx
Keywords:
Algorithm design
Bioinformatics
Longest common subsequence
Constrained LCS
Sequential substring
a b s t r a c t
In this paper, we generalize the inclusion constrained longest
common subsequence (CLCS) problem to the hybrid CLCS problem
which is the combination of the sequence inclusion CLCS and the
string inclusion CLCS, called the sequential substring constrained
longest common subsequence (SSCLCS) problem. In the SSCLCS
problem, we are given two strings A and B of lengths m and n,
respectively, formed by alphabet Σ and a constraint sequence C
formed by ordered strings (C1, C2, C3, . . . , C l) with total length r .
The problem is that of finding the longest common subsequence D
of A and B containing C1, C2, C3, . . . , C l as substrings and with the
order of the C ’s retained. This problem has two variants, depending
on whether the strings in C cannot overlap or may overlap. We
propose algorithms with O(mnl+ (m+ n)(|Σ | + r)) and O(mnr +
(m+n)|Σ |) time for the two variants. For the special case with one
or two constraints, our algorithm runs in O(mn+(m+n)(|Σ |+ r))
or O(mnr + (m + n)|Σ |) time, respectively—an order faster than
the algorithm proposed by Chen and Chao.
© 2012 Elsevier Inc. All rights reserved.
1. Introduction
Given two strings A = a1a2a3 · · · am and B = b1b2b3 · · · bn, the longest common subsequence (LCS)
problem is that of finding the longest common part of A and B by deleting zero or more characters
from A and B. It was first proposed in 1974 by Wagner and Fischer [23]. Much ink has been expended
✩ This research workwas partially supported by the National Science Council of Taiwan under contract NSC100-2221-E-110-
050.∗ Corresponding author.
E-mail address: cbyang@cse.nsysu.edu.tw (C.-B. Yang).
0885-064X/$ – see front matter© 2012 Elsevier Inc. All rights reserved.
doi:10.1016/j.jco.2012.08.002
C.-T. Tseng et al. / Journal of Complexity ( ) – 3
Table 1
The PrevMatch table for A = atcatatgag .
1 2 3 4 5 6 7 8 9 10
a t c a t a t g a g
a −1 1 1 1 4 4 6 6 6 9
c −1 −1 −1 3 3 3 3 3 3 3
g −1 −1 −1 −1 −1 −1 −1 −1 8 8
t −1 −1 2 2 2 5 5 7 7 7
programming technique applying to A, B and C . As noted above, the string inclusion CLCS problem is
a special case of the SSCLCS problem with only one partition. Because many cells in their lattice are
not used, we can compact the 3D lattice into a 2D lattice. Since the characters of the constraint C need
to be consecutive in SSCLCS, after the first character of C is matched, the next character in SSCLCS
must be the second character of C . With this fact, we can find the possible match of the constraint
by continuously finding the next occurrence of the next character of the constraint in A and B. For
example, consider A = atcatatgag, B = atcatctagg and C = tag . We have a2 = b5 = c1, so we can
find the best SSCLCS containing C starting at (2, 5) by jumping through (4, 8), (8, 9). On the other hand,
we can wait until the last character of C is matched, and then we find the nearest occurrence of the
previous character reversely. Since we perform the dynamic programming approach, we should not
refer to cells that have not yet been calculated. Thus, we will perform the matching process in the
backward (reverse) way.
We use LCS(S1, S2) to denote the LCS between S1 and S2 and |LCS(S1, S2)| to denote its length. Ai..j
is also used to denote the substring of a string A starting at position i and ending at position j. It is easy
to obtain the following fact.
Proposition 1. Suppose that ai = bj = cr . If Aiˆ..i and Bjˆ..j contain C as their subsequences, then
LCS(A1..ˆi−1, B1..jˆ−1) ⊕ C ⊕ LCS(Ai+1..m, Bj+1..n) forms a feasible solution of the SSCLCS problem, where⊕ denotes the string concatenation operation.
Furthermore, if there is another i′, iˆ ≤ i′, and Ai′..i also contains C as its subsequence, then the
solution derived from Ai′..i is not worse than the above solution obtained in Proposition 1. Thus, we
can conclude the following theorem.
Theorem 1. Let T = {(i′, j′, i, j)|ai = bj = cr , i′ and j′ are the largest indices such that Ai′..i and
Bj′..j contain C as their subsequences}. The SSCLCS solution can be obtained by finding the maximum of
LCS(A1..i′−1, B1..j′−1)⊕ C ⊕ LCS(Ai+1..m, Bj+1..n), where (i′, j′, i, j) ∈ T .
To find the previous occurrence of a certain character, we reverse the NextMatch table proposed
by Landau et al. [17] into the PrevMatch table which records the previous occurrence position of
each symbol in every position. An example of a PrevMatch table for A = atcatatgag is shown in
Table 1, where −1 means that the character never appears. The PrevMatch table can be constructed
in O(|S| |Σ |) time and space, where S denotes the input string andΣ denotes the alphabet set of S.
We call the index i′ (j′) in Theorem 1 the starting position corresponding to ending position
i (j). For each ending position, the corresponding starting position can be calculated with at most
r lookups in the PrevMatch table. We name the starting position tables for A and B as StartPos tables
ζA and ζB, respectively. For the position whose corresponding starting position does not exist, we
fill −1 in the StartPos table. For example, suppose A = atcatatgag and C = acat . Then, we have
ζA = [−1,−1,−1,−1, 1,−1, 1,−1,−1,−1]. For the same A, suppose C = tag; we have ζA =
[−1,−1,−1,−1,−1,−1,−1, 5,−1, 7]. The time required for constructing the two StartPos tables
is O((m+ n)r), since each position requires at most r lookups in the PrevMatch table and each lookup
takes only constant time.
We find the string inclusion CLCS with a two-layer dynamic programming lattice. Let M[i, j, k]
denote the length of SSCLCS between A1..i and B1..j with k partitions (strings) satisfied. When k = 0,
it is layer 0 that represents the lattice of the ordinary LCS, in which no constraint is considered. And,
when k = 1, it is layer 1 that represents the lattice of CLCS length, containing the given constraint
C.-T. Tseng et al. / Journal of Complexity ( ) – 5
and space. With ζA and ζB, each cell in the M table can be obtained in constant time. So the time
complexity of our algorithm is O(mn+ (m+ n)(|Σ | + r)), which improves a lot on that of Chen and
Chao’s method [9] with O(mnr) time. Our space complexity is O(mn+ (m+ n)|Σ |).
3. Algorithms for non-overlapping partitions
In Section 2, we presented an algorithm for the case where there is a single partition in the
constraint sequence. In this section, we are going to extend it to two or more partitions which do
not overlap in the SSCLCS answer.
For ease of understanding,wewill first discuss the casewhere exactly twopartitions are involved in
the constraint sequence. We extend the idea used in the previous section to solve this problem. Layer
0 stores the ordinary LCS length, in which no constraint is considered. Layers 1 and 2 correspond to
the matching of the first partition and both partitions, respectively. We also construct the PrevMatch
tables of A and B first. Since the StartPos table depends on the constraint, the StartPos tables for the two
partitions are different. We denote them as ζ 1 and ζ 2. Layers 0 and 1 are constructed as the previous
section. For layer 2, because these two partitions cannot overlap, we can apply a DP similar to that for
layer 1 to it. Note that the value in layer 1 will become positive only after the end of the first matching
to C1. If the corresponding starting position of C2 is in the middle of the first matching to C1 in layer
1, the SSCLCS length will still be −∞. For example, if we add a second partition tag to the example
in Table 2, the values of layer 2 are all−∞ except that the values of (10, 9) and (10, 10) are 7. When
M[8, 9, 2] refers toM[4, 6, 1], there is no LCS between atca and atcatc containing acat , so the SSCLCS
should still be−∞.
Now, we propose the algorithm for an arbitrary number of partitions. Let ζ kA and ζ
k
B denote the
StartPos tables for Ck on A and B, respectively. The dynamic programming formula is given in Eq. (2):
M[i, j, k] = max

−∞ if k = 0 and (i < 0 or j < 0);
0 if k = 0 and i = 0 and j ≥ 0;
0 if k = 0 and i ≥ 0 and j = 0;
−∞ if k ≥ 1 and (i ≤ 0 or j ≤ 0);
M[i− 1, j− 1, k] + 1 if ai = bj;
M[ζ kA [i] − 1, ζ kB [j] − 1, k− 1] + lk if k ≥ 1 and ai = bj = cklk;
M[i− 1, j, k]
M[i, j− 1, k] otherwise.
(2)
Theorem 3. Eq. (2) solves the SSCLCS problem with l partitions that the partitions cannot overlap.
Proof. The correctness of each M[i, j, k] is shown as follows. For k = 0, the dynamic programming
formula is almost the same as the ordinary dynamic programming formula for computing the
traditional LCS because there is no constraint in layer 0. The only difference is that some extra pseudo-
cells have value −∞ when i < 0 or j < 0. For k ≥ 1, it is separated into four cases. First, before the
partition of this layer is contained in the SSCLCS, its length should be−∞, so we set the initial value of
the boundary condition to−∞. Second, when ai ≠ bj, the LCS length cannot be increased, sowe adopt
the ordinary dynamic programming formula. Third, when ai = bj, it can be added into the SSCLCS of
A1..i−1 and B1..j−1. In this case, if the partition of this layer is not contained in SSCLCS(A1..i−1, B1..j−1),
then M[i − 1, j − 1, k] is −∞, so the M[i, j, k] obtained will still be −∞. Otherwise, the constraint
cannot stop us from adding it. Fourth, when ai = bj = Cklk , it possibly satisfies the partition of this
layer. In this case, we can jump directly to the corresponding starting position, with the help of the
StartPos table, and add length lk to the answer. But if Ck is not a subsequence of A1..i or B1..j, the StartPos
table will return−1, so we set the virtual boundary condition with i < 0 or j < 0 to return−∞. 
The time and space complexities of the preprocessing for constructing the StartPos tables are both
O((m+n)(|Σ |+r)). The time and space complexities of Eq. (2) are bothO(mnl), since the required time
for each cell is constant. So the total time and space complexities are both O(mnl+ (m+n)(|Σ |+ r)).
C.-T. Tseng et al. / Journal of Complexity ( ) – 7
Consider our previous example, A = atcatatgag, B = atcatctagg and C = (acat, tag), whose valid
overlapping lengths are 0 and 1. The earliest matching to C2 occurs atM[8, 9, 2]; referring to Table 2,
M[8, 9, 2] = max(M[4, 6, 1] + 3,M[5, 7, 1] + 2) = 6.
The required time and space are analyzed as follows. Let |W2| be the total number of valid overlaps
between the two partitions where |W2| ≤ min(l1, l2). For the preprocessing, we spend O((m+n)|Σ |)
time and space in constructing the PrevMatch tables of A and B. O((m+ n)l1)time and O(m+ n) space
are required to construct the ζ 1 tables. We take O((m+ n)l2) time and space to construct the new ζ 2
tables. It takes O(l1l2) time and O(|W2|) space to calculate the valid overlapping lengths. For the DP
lattice, layers 0 and 1 are constructed in O(mn) time and space. Layer 2 is constructed in O(mn|W2|)
time and space because there are atmost |W2| cases in each cell. So the total time and space complexity
is O(mn|W2| + (m+ n)(|Σ | + r)+ l1l2) = O(mnr + (m+ n)|Σ |), where r = l1 + l2.
Chen and Chao [9] proposed an algorithm for the case where two constraints of lengths ρ1 and
ρ2 are given and the order is arbitrary in the CLCS. Their algorithm requires O(mnρ1ρ2) time and
O(mn(ρ1+ρ2)) space. To solve this problem,we can performour algorithm in this subsection twice by
setting the two partitions differently. So our algorithm is an order faster than the algorithm proposed
by Chen and Chao.
4.2. An algorithm for an arbitrary number of partitions
In this section, we extend the algorithm for two partitions to one for an arbitrary number of
partitions. M[i, j, k] still denotes the SSCLCS length between a1,...,i and b1,...,j containing C1, . . . , Ck
as substrings. In the preprocessing phase, we first construct the PrevMatch tables for A and B. Second,
we use the PrevMatch tables to construct the StartPos tables for all partitions with all overlapping
lengths. Third, for every two consecutive partitions Ck−1 and Ck, we find all valid lengths of overlap,
to form the setWk.
The DP formula for layers 0 and 1 is the same as Eq. (2). The DP formula for the remaining layers,
where k ≥ 2, is given as follows:
M[i, j, k] = max

−∞ if i ≤ 0 or j ≤ 0;
M[i− 1, j− 1, k] + 1 if ai = bj;
M[ζ kA [i, w], ζ kB [j, w], k− 1] + lk − w,
wherew is a valid overlapping length inWk if ai = bj = cklk;
M[i− 1, j, k]
M[i, j− 1, k] otherwise.
(4)
Theorem 4. The combination of Eqs. (2) and (4) solves the l-partition SSCLCS problemwhere the partitions
may overlap.
Proof. The SSCLCS answer D corresponds to characters in positions pa,1, pa,2, . . . , pa,|D| from A
and pb,1, pb,2, . . . , pb,|D| from B. Suppose Ck1 matches pa,νk and pb,υk for all 1 ≤ k ≤ l. If two
adjacent layers do not overlap, then the correctness follows from Theorem 3. If Ck−1 overlaps
with Ck in SSCLCS with length wk, it follows that Ck−1lk−1−wk+1,...,lk−1 = Ck1,...,wk . We always find the
nearest match in both A and B, so when we match for Ck1,...,wk from M[pa,νk+wk−1, pb,υk+wk−1, k]
and M[pa,νk+wk−1, pb,υk+wk−1, k − 1], we will trace both back to M[pa,νk , pb,υk , k − 1]. Thus,
M[pa,νk+lk−1, pb,υk+lk−1, k] = M[pa,νk−1 , pb,υk−1 , k − 1] + lk−1 + lk − wk − 1 which matches with
our assumption. 
The complexity of the above algorithm is analyzed as follows. In the preprocessing phase, we need
O((m + n)|Σ |) time and space to construct the PrevMatch table of A and B. For each partition Ck, we
require O((m + n)lk) time and space to construct the StarPos table, so the total time required for all
StarPos tables is O((m + n)r). For every two consecutive partitions Ck−1 and Ck, we take O(lk−1lk)
time to find the valid overlapping lengths and use O(|Wk|) = O(lk) space to store them, where
|Wk| ≤ min(lk−1, lk). So the total time and space required for finding all valid overlapping lengths
C.-T. Tseng et al. / Journal of Complexity ( ) – 9
[22] Y.T. Tsai, The constrained longest common subsequence problem, Information Processing Letters 88 (2003) 173–176.
[23] R.A. Wagner, M.J. Fischer, The string-to-string correction problem, Journal of the Association for Computing Machinery 21
(1) (1974) 168–173.
[24] C.B. Yang, R.C.T. Lee, Systolic algorithm for the longest common subsequence problem, Journal of the Chinese Institute of
Engineers 10 (6) (1987) 691–699.
both imbalanced and balanced datasets. The best
values of F-measure, MCC, AIC and BIC of our
experiments on the imbalanced dataset are 0.5197,
0.4671, 0.2428 and 0.2543, respectively. For the
balanced dataset, we get 0.7742, 0.5484, 0.3603
and 0.3828, respectively. The best of our models
outperforms other previous methods with various
performance measurements.
The rest of this paper is organized as follows.
In Section II, we will introduce some preliminary
knowledge. In Section III, we will present our
method. Section IV shows the experimental results
and compares them with some previous results.
Finally, Section V gives a conclusion and some
possible future works.
II. Preliminaries
In this section, we will introduce some related
works. We will introduce the support vector ma-
chine (SVM), the position specific scoring matrix
(PSSM), and the performance measurements we
use.
A. Support Vector Machine
The support vector machine (SVM) [9] is a
method that can classify a set of samples into two
classes. Let {(x1, y1), (x2, y2), . . . , (xn, yn)} denote
the set of samples, where xi is the feature vector of
sample i, yi is its label, n denotes the number of
samples and d denotes the dimension of xi. SVM is
to find a hyperplane which has the maximal margin
to separate the samples with labels +1 and -1. The
margin is defined as the shortest distance from each
feature vector to the support hyperplane. The SVM
provides some various kernel functions to transform
the original feature space into higher dimensional
space. Then, the cases will become nearly linearly
separable. Four kernel functions are usually used,
including linear, polynomial, radial basis function
and sigmoid.
B. Position Specific Scoring Matrix
Altschul et al. [3] proposed the Position Specific
Scoring Matrix (PSSM). The basic local align-
ment search tool (BLAST) [2] and position spe-
cific iterated-BLAST (PSI-BLAST) are two popular
tools related to PSSM, both implemented by Na-
tional Center for Biotechnology Information (NCBI)
(http://www.ncbi.nlm.nih.gov/). Querying primary
biological sequences is the main task of the BLAST.
PSI-BLAST is based on BLAST. It first obtains the
list of similar sequences produced by BLAST. Then,
PSI-BLAST produces a profile of the list, and uses
it as an input to perform the next iteration in PSI-
BLAST. The PSSM is calculated by PSI-BLAST
in each iteration. The scores of PSSM represent
the occurrence probabilities over the background
probability.
C. Performance Evaluation
We use receiver operating characteristic (ROC)
curve and area under curve (AUC) value to evalu-
ate the classifiers. And we measure the prediction
result generated by a classifier by various kinds of
accuracy measurements, including precision, recall,
F-measure, Matthews correlation coefficient (MCC)
and percentage of essential proteins. The formulas
are given as follows.
1. Precision: TP
TP+FP
2. Recall: TP
TP+FN
3. F-measure: 2×precision×recall
precision+recall
4. MCC: TP×TN−FP×FN√
(TP+FN)(TP+FP )(TN+FP )(TN+FN)
5. Percentage of essential protein:
Number of essential proteins in top n
n
.
In the above, TP (true positive) denotes the
number of true objects (essential proteins) correctly
predicted, FP (false positive) denotes the number
of false objects (nonessential proteins) wrongly pre-
dicted, FN (false negative) denotes the number of
true objects (essential proteins) wrongly predicted,
TN (true negative) denotes the number of false
objects (nonessential proteins) correctly predicted
and n is the number of proteins.
With Akaike’s information criterion (AIC) and
Bayesian information criterion (BIC) [12], if a
classifier involves more features, it will get more
penalty. The formulas of AIC and BIC are given as
follows.
1. AIC:
∑N
i=1 err(i)
N
+ 2×(k+1)
N
2. BIC:
∑N
i=1 err(i)
N
+ (logN)×(k+1)
N
In the above, err(i) denotes the error of data el-
ement i, N denotes the number of samples in the
dataset and k denotes the number of features. By the
definitions, BIC gets more penalty on the number
of features than AIC.
27
2011 National Computer Symposium
Copyright©2011. National Chiayi University. All rights reserved.
III. OUR METHOD
In this section, we will introduce our features and
our prediction method with SVM.
A. Feature Extraction
The feature set includes topological properties
(T), such as bit string of double screening scheme
and betweenness centrality related to physical inter-
actions; protein properties (P), such as cell cycle and
metabolic process; sequence properties (S), such
as amino acid occurrence and average amino acid
PSSM; and other properties (O), such as phyletic
retention and essential index. There are totally 45
properties, including 90 features, as shown in Table
I.
Lin et al. [18] and Chin et al. [8] proposed
the double screening scheme. They use a ranking
scheme to find the essential proteins. But each
protein does not have a unique score. Thus, we
propose a bit string implementation to transform
the ranking to a score of protein. We select two
topological properties for ranking, A and B. For
the n target proteins, we rank them by A and B
individually. Then, n
2
iterations are performed to
construct the bit vector for each protein. A 2D-bit
array M is built, where each bit M [i, j] corresponds
to a protein j in iteration i. In the ith iteration, we
find the top i ranked proteins, described as follows.
We select the top 2i proteins by ranking method A,
then we select the top i proteins by ranking method
B among the 2i proteins. If a protein j is selected,
we set its M [i, j] to 1; otherwise, M [i, j] is set to 0.
Hence, each protein will have n
2
bits and its score
is the sum of its bits.
An example of our bit string implementation is
shown in Tables II and III. In the first iteration, our
goal is to find the top 1 essential protein by double
screening scheme. We first select top 2 essential
proteins by ranking method A, which are W and
X . Next, W and X are 2nd and 1st ranked by
B, respectively. Hence, in the first iteration, X is
finally selected. Then, we set the bit M [1, X] for
the first iteration to 1, and the others are set to 0.
In the second iteration, the top 2 essential proteins
are desired to be found. First, four proteins W , X ,
Y and Z are selected, because they are the top 4 by
ranking method A. By ranking B, we select the top
2 proteins from them, which are X and Y . Hence,
TABLE II
RANKING BY TWO DIFFERENT METHODS.
Ranking method
Protein name A (DMNC) B (MNC)
W 1st 4th
X 2nd 2nd
Y 3rd 1st
Z 4th 3rd
TABLE III
BIT STRING BY THE DOUBLE SCREENING SCHEME.
ith iteration
Protein name 1st 2nd Sum of bit string n− r Sum
W 0 0 0 0 0
X 1 1 2 2 4
Y 0 1 1 3 4
Z 0 0 0 1 1
the bits M [2, X] and M [2, Y ] are 1, and the others
are 0 in the second iteration. Finally, we sum up the
bits of each protein, as shown in the fourth column
of Table III.
There is still a problem in our bit string imple-
mentation, the number of proteins scarcely being
selected is around n
2
. Thus, about n
2
sums are close
to 0. It would cause SVM being difficult to classify
these n
2
proteins. In order to overcome this problem,
for each protein, we add another score n− r to the
sum of the bit vector, where r is the rank of the
protein by ranking method B. In this paper, we use
DMNC as ranking A and MNC as ranking B. In
this example, n = 4, so the values n− r of W , X ,
Y and Z are 0, 2, 3 and 1, respectively. We sum it
up with the bit string, hence the final scores are 0,
4, 4 and 1.
B. Our Method with SVM
This paper uses the SVM program developed
by Chang and Lin [6], called LIBSVM [13]. We
apply 10-fold cross-validation for 10 times on the
dataset. The procedure of our method is described
as follows.
Step 1.Retrieve the protein-protein interaction
dataset from the DIP database.
Step 2.Extract features described in Section III-A.
Step 3.Measure the performance of each property.
Step 4.Build 45 feature sets (models) by adding
29
2011 National Computer Symposium
Copyright©2011. National Chiayi University. All rights reserved.
TABLE IV
THE PERFORMANCE COMPARISON OF THE IMBALANCED DATASET.
ith model AUC Precision Recall F-measure MCC AIC BIC
(number of features)
Hwang et al. (10) 0.7781 0.7382 0.3299 0.4525 0.4179 0.2608 0.2646
Marcio et al. (23) 0.7245 0.6574 0.1714 0.2716 0.267 0.3016 0.31
1st (1) 0.7046 0.6049 0.2513 0.3551 0.3035 0.276 0.2767
2nd (2) 0.7338 0.6494 0.3143 0.4236 0.3666 0.2626 0.2636
3rd (22) 0.7616 0.6495 0.3202 0.4289 0.3704 0.2685 0.2765
4th (23) 0.7531 0.6709 0.2987 0.4131 0.3668 0.269 0.2774
5th (24) 0.7829 0.6706 0.2968 0.4111 0.3653 0.2653 0.274
6th (25) 0.7539 0.6739 0.3024 0.4172 0.3707 0.2698 0.2789
7th (26) 0.7851 0.748 0.3416 0.4686 0.4331 0.2509 0.2603
8th (27) 0.7849 0.7504 0.3481 0.4754 0.4389 0.2519 0.2617
9th (28) 0.7765 0.7401 0.3662 0.4898 0.4463 0.2517 0.2618
10th (29) 0.8123 0.7333 0.3783 0.4977 0.4502 0.246 0.2565
11th (30) 0.7814 0.7368 0.3685 0.4911 0.4462 0.2533 0.2641
12th (31) 0.8122 0.7373 0.3703 0.4917 0.4469 0.247 0.2582
13th (32) 0.8219 0.7436 0.3711 0.4948 0.4513 0.2428 0.2543
34th (72) 0.8371 0.7237 0.4031 0.5162 0.4614 0.2556 0.2812
36th (74) 0.8376 0.7333 0.4026 0.5197 0.4671 0.2547 0.2809
40th (78) 0.8413 0.727 0.3938 0.51 0.4576 0.2591 0.2867
TABLE V
THE PERFORMANCE COMPARISON OF THE BALANCED DATASET.
ith model AUC Precision Recall F-measure MCC AIC BIC
(number of features)
Hwang et al. (10) 0.7781 0.7737 0.7019 0.7356 0.4985 0.3839 0.3912
Marcio et al. (23) 0.7245 0.6951 0.7125 0.7036 0.4004 0.4319 0.4478
1st (1) 0.7046 0.7306 0.6903 0.7098 0.4365 0.3921 0.3934
2nd (2) 0.7338 0.7701 0.6674 0.715 0.4725 0.3879 0.3899
3rd (22) 0.7616 0.7409 0.7137 0.7268 0.4643 0.396 0.4113
4th (23) 0.7531 0.7426 0.7593 0.7506 0.4964 0.382 0.3979
5th (24) 0.7829 0.7494 0.723 0.7357 0.4813 0.3872 0.4037
6th (25) 0.7539 0.7384 0.7478 0.743 0.4833 0.3896 0.4068
7th (26) 0.7851 0.7625 0.7325 0.747 0.5049 0.375 0.3929
8th (27) 0.7849 0.7646 0.7358 0.7499 0.5099 0.3747 0.3932
9th (28) 0.7765 0.7495 0.7425 0.7459 0.4947 0.3833 0.4024
10th (29) 0.8123 0.7872 0.7119 0.7474 0.5219 0.3627 0.3826
13th (32) 0.8219 0.7854 0.7299 0.7565 0.5321 0.361 0.3828
16th (35) 0.8194 0.781 0.7538 0.767 0.5429 0.3603 0.3841
32nd (70) 0.8502 0.7741 0.7744 0.7742 0.5484 0.3883 0.4352
0.2428 and 0.2543, respectively, which occur at the
13th model. For the balanced dataset, the lowest
AIC is 0.3603 in the 16th model and the lowest
BIC is 0.3828 in the 13th model.
The prediction method proposed by Chin et al.
[8] gives a score to each target protein. Thus,
the protein ranks can be obtained by the scores.
LIBSVM also gives us a probability in the two-class
classification, which represents the confidence of
the classification. We use these probabilities to rank
the proteins which are predicted as essential ones.
Table VI show the percentages of proteins which
are predicted as essential in the top ranked proteins
in the imbalanced experiment. The value with an
underline is the highest. In the balanced dataset,
since nearly half of the proteins are not put into
the dataset, we cannot get the probabilities for all
proteins. Hence, we do not this ranking experiment
31
2011 National Computer Symposium
Copyright©2011. National Chiayi University. All rights reserved.
TABLE VI
THE PERCENTAGE OF ESSENTIAL PROTEINS IN THE IMBALANCED DATASET.
Percentages of essential proteins in predicted n proteins
ith model TOP 100 TOP 30% TOP 50% TOP 80% TOP 100%
(number of features) (10.25%)
Chin et al. (1) 0.64 0.5324 0.498 0.4897 0.4215
Hwang et al. (10) 0.82 0.7843 0.7223 0.6047 0.5586
Marcio et al. (23) 0.693 0.6478 0.5654 0.4858 0.46
1st (1) 0.694 0.6471 0.6008 0.5477 0.5349
2nd (2) 0.757 0.6945 0.6451 0.5722 0.5319
3rd (22) 0.799 0.6843 0.6459 0.5719 0.5318
4th (23) 0.798 0.7222 0.6609 0.5822 0.535
5th (24) 0.819 0.7092 0.6621 0.5832 0.5393
6th (25) 0.804 0.7222 0.6658 0.5844 0.5357
7th (26) 0.922 0.8259 0.734 0.6283 0.5733
8th (27) 0.915 0.8198 0.7342 0.6272 0.5723
9th (28) 0.9 0.8116 0.7373 0.6306 0.5677
10th (29) 0.905 0.8171 0.7387 0.6308 0.5795
29th (67) 0.895 0.8468 0.749 0.6403 0.5887
32nd (70) 0.898 0.8406 0.7492 0.6463 0.5931
37th (75) 0.891 0.8382 0.7512 0.6399 0.5907
[8] C.-H. Chin, C.-W. Ho, and M.-T. Ko, “Prediction of essential
proteins and functional modules from protein-protein interac-
tion networks,” Ph.D. dissertation, National Central University,
Chung-Li, Taiwan, 2010.
[9] C. Cortes and V. Vapnik, “Support-vector networks,” Machine
Learning, vol. 20, no. 2, pp. 273–297, 1995.
[10] N. Cristianini and J. Shawe-Taylor, An Introduction to Support
Vector Machines and other kernel-based learning methods.
Cambridge University Press, 2000.
[11] L. M. Cullen and G. M. Arndt, “Genome-wide screening for
gene function using rnai in mammalian cells,” Immunology and
Cell Biology, vol. 83, no. 3, pp. 217–223, 2003.
[12] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of
Statistical Learning: Data Mining, Inference, and Prediction.,
2nd ed., 2009.
[13] C.-W. Hsu, C.-C. Chang, and C.-J. Lin, “A
practical guide to support vector classification,”
http://www.csie.ntu.edu.tw/∼cjlin/papers/guide/guide.pdf.
[14] Y.-C. Hwang, C.-C. Lin, J.-Y. Chang, H. Mori, H.-F. Juan, and
H.-C. Huang, “Predicting essential genes based on network and
sequence analysis,” Molecular BioSystems, vol. 5, no. 12, pp.
1672–1678, 2009.
[15] T. Ito, T. Chiba, R. Ozawa, M. Yoshida, M. Hattori, and
Y. Sakaki, “A comprehensive two-hybrid analysis to explore
the yeast protein interactome,” Proceedings of the National
Academy of Sciences of the United States of America, vol. 98,
no. 8, pp. 4569–4574, 2001.
[16] H. Jeong, S. P. Mason, A.-L. Barabsi, and Z. N. Oltvai,
“Lethality and centrality in protein networks,” Nature, vol. 411,,
pp. 41–42, 2001.
[17] C.-Y. Lin, C.-B. Yang, C.-Y. Hor, and K.-S. Huang, “Disulfide
bonding state prediction with svm based on protein types,”
Bio-Inspired Computing: Theories and Applications, pp. 1436–
1442, 2010.
[18] C.-Y. Lin, C.-H. Chin, H.-H. Wu, S.-H. Chen, C.-W. Ho,
and M.-T. Ko, “Hubba: hub objects analyzer- framework of
interactome hubs identification for network biology,” Nucleic
Acids Research, vol. 36, pp. W438–W443, 2008.
[19] N. Przˇulj, D. Wigle, and I. Jurisica, “Functional topology in
a network of protein interactions,” Bioinformatics, vol. 20, pp.
340–348, 1998.
[20] L. Salwinski, C. S. Miller, A. J. Smith, F. K. Pettit, J. U.
Bowie, and D. Eisenberg, “The database of interacting proteins:
2004 update.” Nucleic Acids Research, vol. 32, pp. D449–D451,
2004.
[21] S. Wasserman and K. Faust, Social Network Analysis: Methods
and Applications. Cambridge University Press, 1994.
[22] I. H. Witten and E. Frank, Data Mining:Practical Machine
Learning Tools and Techniques with Java Implementations.
Morgan Kaufmann, 2000.
[23] S. Wuchty and P. F. Stadle, “Centers of complex networks,”
Journal of Theoretical Biology, pp. 45–53, 2003.
[24] H. Yu, P. M. Kim, E. Sprecher, V. Trifonov, and M. Gerstein,
“The importance of bottlenecks in protein networks: Correlation
with gene essentiality and expression dynamics,” PLoS Com-
putational Biology, vol. 3, pp. 713–720, 2007.
33
2011 National Computer Symposium
Copyright©2011. National Chiayi University. All rights reserved.
- 2 - 
成8個 parallel  session 同時進行。 
在準備前往巴西聖保羅參加會議時，心理相當猶豫掙扎。最後
決定，也許一生只有這麼一次，因此毅然決然前往參加。要取得巴西
簽證相當麻煩，是我碰過最複雜的一次，必須提出存款證明、工作保
信等資料，並且需要14天才可辦妥。 
我的飛機是從高雄至香港轉機(1.5小時)，搭卡達航空，至杜哈(8
小時)，再轉機一次(仍為卡達航空)，然後至聖保羅(14.5小時)下機。
不算等待時間，光是飛行時間總計為24小時。這也是我今生以來飛行
最久的一次。 
 
二、 與會心得 
本次會議有三天，每天的第一場都是邀請演講 (Keynote 
Speech)。演講者與演講題目如下：  
1. Association for Information Systems, Prof. Dr. Dov Te´eni, President 
of AIS, Tel Aviv University, Israel. 
2. E-Government, Prof. Dr. Walter Castelnovo, Universitá de Insubria, 
Italy. 
3. Internet for Retrieval of Information, Prof. Dr. Armando Malheiro, 
Univ. Porto, Portugal. 
我發表的論文如下： 
Kai-Yu Chen, Chang-Biau Yang and Kuo-Si Huang, " Prediction of 
Protein Backbone Structure by Preference Classification with 
SVM," Proceedings of the 9th International Conference on Information Systems 
and Technology Management (9thCONTECSI), São Paulo, Brazil, May 30-June 
1, 2012. 
我發表的論文是蛋白質骨幹結構的預測問題(all-atom protein 
backbone reconstruction problem, PBRP)，其輸入資料為一條蛋白質的
氨基酸序列(一級結構)，以及每個氨基酸的中央碳之三維座標，輸出
- 4 - 
 
三、 參觀活動與建議 
這是我第一次去巴西。訂定飛機航班時，才發現大部分航班都
是往西飛行，在中途站(歐洲、南非、中東)再轉機，飛行時間大約都
需要超過24小時(不算等待時間)。台灣至巴西真的非常遙遠。 
巴西(Brazil，葡萄牙文Brazil)位於南美洲，國土面積約850萬平
方公里(世界排名第五)，約為台灣面積的236倍，人口約1億9千萬2百
萬(世界排名第五)，約為台灣人口的8.3倍。2011年的人均GDP約為
13,000美元(2011年台灣的人均GDP約為20,000美元)。巴西原是葡萄牙
殖民地，故官方語言為葡萄牙語。巴西於1822自葡萄牙手中獨立建
國。目前共分26個州和1個聯邦區(巴西利亞)，巴西最著名的運動為
足球，是唯一曾五次奪取世界盃冠軍的國家（1958年、1962年、1970
年、1994年和2002年）。 
本次會議所在地是聖保羅大學（葡萄牙語：Universidade de São 
Paulo，簡稱USP），為一所州立大學，是巴西規模最大的大學，創建
於1827年。目前有11個校區，其中4個位於聖保羅市區，現有各類學
生7.5萬人。在QS World University Rankings排名中，2011年排名169，
是南美洲排名最好的大學。在Academic Ranking of World Universities 
(ARWU，上海交大排名)排名，2010年排名是 101~150。在Google地
圖上，或在聖保羅的公車上，「USP」與「Cidade Universitania」（大
學城）指的都是同一個地方，即聖保羅大學。 
聖保羅(São Paulo)人口1100萬，是巴西，也是南美洲第一大的都
市。聖保羅自1974年就開通第一條地鐵，至今共有六條路線，若加上
快捷火車，則應該有十條以上的路線。由於市區人口多，靠地鐵行動
的人口也多，上下班尖峰時間，擠得水洩不通。有一次，我於上班時
間搭地鐵，明明旅客已經上車(當然月台上還有許多擠不上車的旅
客)，可能列車門無法正常關閉，又足足空等待五分鐘。在繁忙的車
站，一條軌道的兩側都是月台，停車時兩側車門均打開，某一側車門
供旅客下車離開，另一側車門則供旅客上車。 
聖保羅的市區公車也是四通八達，班次相當密集。每一部公車
- 6 - 
附錄：相關相片 
 
楊昌彪教授攝於會場大樓外 
 
 
楊昌彪教授攝於會場外 
- 8 - 
 
會場外矗立與會者國旗之直立海報(包含台灣) 
 
 
楊昌彪教授進行論文報告 
 
Nowadays, the 3D coordinates of a protein can be built by X-ray crystallographic or nu-
clear magnetic resonance (NMR), but plenty of time and cost are required. Therefore, two
computational methods, ab initio and homology modeling, were proposed for predicting
the 3D protein structure (Zhang, 2008).
The ab initio method considers the interactions between the amino acids and the
liquors to determine the stable status of each molecule. The homology method builds the
templates with known protein structures and then produces a model based on the align-
ments between the target protein and the templates. The proper templates can be deter-
mined by the similarities of their conformations (Holm & Sander, 1991; Kazmierkiewicz,
Liwo, & Scheraga, 2002; Simons, Kooperberg, Huang, & Baker, 1997; Samudrala & Moult,
1998).
The all-atom protein backbone reconstruction problem (PBRP) is to reconstruct the
3D coordinates of all atoms (N, C, and O) on the backbone for a protein whose amino
acid sequence and α-carbon coordinates are given. Wang et al. built a 4-residue fragment
library to reconstruct all atoms on the backbone (Wang, Yang, & Tseng, 2007). Chang et
al. proposed a method to refine the positions of oxygen atoms on the backbone of a protein
(Chang, Yang, & Ann, 2009). Yen et al. further proposed a tool preference classification
by choosing either SABBAC (Maupetit, Gautier, & Tuffery, 2006) or Chang’s method
(Chang et al., 2009) to predict the protein structure for PBRP (Yen, Yang, & Ann,
2010). The method proposed by Yen et al. tries to find a way to select possibly better
predicting method between SABBAC and Chang’s method for improving the accuracy.
Another method for PBRP, named BBQ (Backbone Building from Quadrilaterals) (Gront,
Kmiecik, & Kolinski, 2007), considers a fragment with four contiguous amino acids as
quadrilateral. Then, it calculates Cα (α-carbon) trace to choose proper quadrilaterals as
the prediction results.
Our goal is to improve the accuracy of structure prediction for PBRP. Since the
performance of BBQ is better than that of SABBAC, we consider BBQ and Chang’s
method as our candidate prediction tool, instead of SABBAC. We apply the tool prefer-
ence classification individually to determine the preferred prediction tool, either BBQ or
Chang’s method, for each atom. Based on the preferred tools, we get better accuracy for
predicting the 3D coordinates of N, C, and O atoms. The preference classification tool we
use is the support vector machine (SVM), with which the backbone structure of a protein
can be reconstructed.
For evaluating the performance of our proposed method for PBRP, the data sets
of CASP (Critical Assessment of Protein Structure Prediction) are used in this paper.
CASP is a competition of protein structure prediction which takes place every two years
from 1994. The data sets we use were extracted from the target proteins of 7th (CASP7),
8th (CASP8) and 9th (CASP9) competitions of CASP. In our experiments, we test 29,
24, and 55 proteins, consisting of only standard amino acids, in CASP7, CASP8, and
CASP9, respectively. We improve the average RMSDs of BBQ results from 0.3955 to
0.3835 in CASP7, from 0.4437 to 0.4313 in CASP8, and from 0.4133 to 0.3691 in CASP9.
The experimental results show that the accuracy of our method is superior to SABBAC,
Chang’s method, Yen’s method and BBQ.
The rest of this paper is organized as follows. Section 2 introduces proteins and
their properties, a method for measuring the similarity of two formations of a protein,
2
Table 1: The win frequencies of various methods in each data set.
Data set SABBAC BBQ PULCHRA Chang Total
CASP7 6 12 2 9 29
CASP8 4 13 2 5 24
CASP9 7 32 1 15 55
Chang et al. (2009) found that some results of Wang et al. (2007) are not so good.
They discovered that the average RMSD of O atom is usually worse than N and C atoms.
They measured the lengths and angles of peptide bonds so that the energy function can
be adjusted to a simpler one. According to the energy function, the position of each O
atom can be found independently. For improving the accuracy, they also proposed the
two-phase refinement strategy.
Yen et al. (2010) observed the results from Chang et al. (2009) and SABBAC
(Maupetit et al., 2006), and they discovered that the predictions of Chang’s method are
not always better than SABBAC, and vice versa. If one can exactly select the better
predicting method between SABBAC and Chang’s method for each protein, the result of
prediction would be improved. Hence Yen et al. proposed a method which employs the
SVM as the classifier to build a decision model, which can guide us to determine which
tool is better for predicting the structure of the target protein.
3 Our Method
In the method proposed by Yen et al. (2010), Chang’s method and SABBAC are employed
as the candidate prediction tools. There are some other methods (PULCHRA and BBQ)
for predicting protein structure. And, we did some primitive experiments on the data
sets of CASP7, CASP8 and CASP9 for evaluating these methods. Table 1 shows that
BBQ and Chang’s method can be thought as better prediction tools among these four
methods, because that their frequencies of winning predictions are greater than the other
two. It can be easily seen that none of these two selected tools is always the best method
for predicting 3D structures of proteins.
In order to predict the structure of protein more accurately, we further observe
the results from BBQ and Chang’s predictions. Comparing the predictions of N-atoms,
C-atoms and O-atoms, we realize that each of these two prediction tools has its own
superiority on prediction. Hence, we modify Yen’s method by considering N, C, and O
atoms individually. We come out with a new prediction model, named atom classifier.
We apply the atom classifiers on a given protein sequence whose α-carbon coordinates are
known. After assembling the results from different atom classifiers, we can rebuild the 3D
coordinates of all atoms (N, C, and O) on the backbone. The flowcharts of our method
and the atom classifier are shown in Figures 1 and 2, respectively.
4
Protein 
sequences
Feature set selection by leave-
one-out test
Training set 
filter
Training set 
weighting
Feature set 
selection by 
leave-one-out 
Tests
The 
preference 
of atom
Figure 2: The flow chart of the atom classifier.
6
Sequence
Index
Property
(Hydrophobicity)
P
N
H
S M V P G
1           5
K V T L Q
6           10
K D A Q N
11           15
L I G I S I G G G A
16          20 21           25
N H H N N P H N H P P P N P P H H N H N H N N N N 
1           2 3 4     5 6
1         2  3 4 5 6     7 8  9 10 11
1   2 3     4 5 6       7 8
P-N transitions
N-H transitions
H-P transitions
| |
| | | | | | | ||
| |
Figure 3: An example for illustrating the extraction of the feature sets.
Feature vector H
0.24, 0.44, 0.32, 0.08, 0.375, 0.08, 0.24, 0.4,0.44, 0.56, 0.6, 0.04, …
0.24, 0.44, 0.32, 0.08, 0.375, 0.08 0.24, 0.4,0.44, 0.56, 0.6, 0.04, …
Feature vector H
F
Feature vector H
R
Figure 4: An example for illustrating that the feature vector H is divided into two parts,
HF and HR.
group N is 0.04 (1/25), 0.2 (5/25), 0.72 (18/25), 0.92 (23/25), and 1 (25/25). For group
H, the sequence of D-descriptor is 0.08 (2/25), 0.12 (3/25), 0.36 (9/25), 0.68 (17/25),
and 0.84 (21/25). Finally, we combine above elements and get the feature vector of 21
elements: {0.24, 0.44, 0.32, 0.08, 0.375, 0.08, 0.24, 0.4, 0.44, 0.56, 0.6, 0.04, 0.2, 0.72,
0.92, 1, 0.08, 0.12, 0.36, 0.68, 0.84}. Then, the vector is divided into two parts. The first
part includes the former 6 elements and the other one contains the rest 15 elements. For
instance, H is the feature vector of 21 elements. The first (former) part and the other
(rear) part are denoted as HF and HR, where |HF | = 6 and |HR| = 15. Figure 4 shows
an example that the feature vector is divided into the former and rear parts.
3.2 Feature Set Reorganization
First, all feature sets are initialized by self-tests for selecting better feature subsets. Then,
these feature sets are reorganized by using crossover and extension operations. In the
crossover operation, given two feature vectors X and Y , we exchange XF and YF . Then,
we get two new feature vectors XFYR and YFXR. The extension operation is to add XF or
XR to Y, and to add YF or YR to X, we get XFYFYR, XRYFYR, XFXRYF and XFXRYR.
8
Table 4: The RMSDs of our twelve experiments and the comparison of various methods.
Test Training Test Our method Modified Yen Yen BBQ Chang PAA PIA
I CASP7 CASP7 0.3684 0.3762 0.4019
0.3955 0.4264 0.3624 0.3547
II CASP8 CASP7 0.3882 0.3902 0.4191
III CASP9 CASP7 0.3849 0.3956 0.4265
IV CASP8+9 CASP7 0.3835 0.3956 0.4291
V CASP8 CASP8 0.4203 0.4358 0.4543
0.4437 0.5010 0.4321 0.4192
VI CASP7 CASP8 0.4288 0.4437 0.4219
VII CASP9 CASP8 0.4260 0.4437 0.4852
VIII CASP7+9 CASP8 0.4313 0.4419 0.4866
IX CASP9 CASP9 0.3609 0.3837 0.4104
0.4133 0.4320 0.3768 0.3585
X CASP7 CASP9 0.3839 0.4133 0.4384
XI CASP8 CASP9 0.3763 0.4133 0.4415
XII CASP7+8 CASP9 0.3691 0.4106 0.4302
other 41 proteins are in the human part. Considering the 66 proteins in the server part,
37 sequences contain at least one nonstandard amino acid. Hence, we have 29 sequences
in our training set, which consists of only standard amino acids. Similarly, there are 24
and 55 sequences in the server part containing only standard amino acids in CASP8 and
CASP9, respectively. We perform twelve experiments, including three self-tests and nine
independent tests. Table 4 describes the training set and testing set of each experiment.
It also summarizes the average RMSD of each method. The term “Modified Yen” means
that this paper re-performs Yen’s method by replacing prediction tools with BBQ and
Chang’s method rather than SABBAC and Chang’s method originally.
The term “PAA” (perfect for all atoms) in Table 4 means the proper tool can be
exactly chosen for predicting all atoms, i.e., the best tool is applied to the prediction
of all atoms in the whole target protein. One may note that the RMSD of PAA for a
certain protein is the better RMSD between BBQ and Chang’s method. Another term
“PIA” (perfect for individual atoms) means the RMSD of each atom on the backbone
(including N, C and O atoms) which is correctly predicted by either BBQ or Chang’s
method individually. Note again that if one atom (say, N) decides to employ a method
as its classifier, then this classifier is invoked for the same kind of atom (N) in the whole
target protein. It is clear that “PIA” forms the result bound of our method for PBRP.
Table 5 shows the average RMSD of each atom on the backbone. One may observe
that the average RMSD of O-atom is always maximal. Our method is excellent on selecting
the proper method for predicting O-atom. Therefore, RMSDs of our prediction are better
than the previous methods.
5 Conclusion
For the all-atom protein backbone reconstruction problem (PBRP), Yen et al. (2010)
recently proposed a tool preference classification method to predict the coordinates of
all atoms on the backbone of a target protein by choosing either Chang’s method or
SABBAC. According to the preference, a better prediction tool is employed to do the
coordinate prediction. This paper improves Yen’s method by performing tool preference
for N, C and O atoms separately. The candidate tools for us are Chang’s method and
10
Gront, D., Kmiecik, S., & Kolinski, A. (2007). Backbone building from quadrilaterals: A
fast and accurate algorithm for protein backbone reconstruction from alpha carbon
coordinates. Journal of Computational Chemistry , 28 , 1593-1597.
Holm, L., & Sander, C. (1991). Database algorithm for generating protein backbone
and side-chain coordinates from a c alpha trace application to model building and
detection of coordinate errors. Journal of Molecular Biology , 21 (1), 183-194.
Huang, C.-D., Lin, C.-T., & Pal, N. R. (2003). Hierarchical learning architecture with
automatic feature selection for multiclass protein fold classification. IEEE Transac-
tions on NanoBioscience, 2 , 221-232.
James, L. K. (1993). Nobel laureates in chemistry, 1901-1992. Chemical Heritage Foun-
dation (June 1, 1993).
Kazmierkiewicz, R., Liwo, A., & Scheraga, H. A. (2002). Energy-based reconstruction of
a protein backbone from its α-carbon trace by a Monte-Carlo method. Journal of
Computational Chemistry , 23 , 715-723.
Li, Z. R., Lin, H. H., Han, L. Y., Jiang, L., Chen, X., & Chen, Y. Z. (2006). PROFEAT:
a web server for computing structural and physicochemical features of proteins and
peptides from amino acid sequence. Nucleic Acids Research, 34 , W32-W37.
Maupetit, J., Gautier, R., & Tuffery, P. (n.d.). SABBAC v1.2: Structural alphabet based
protein backbone builder from alpha carbon trace. http://bioserv.rpbs.jussieu.fr/cgi-
bin/SABBAC.
Maupetit, J., Gautier, R., & Tuffery, P. (2006). SABBAC: online structural alphabet-
based protein backbone reconstruction from alpha-carbon trace. Nucleic Acids Re-
search, 34 , W147-W151.
Moult, J., Fidelis, K., Kryshtafovych, A., Rost, B., & Tramontano, A. (2009). Critical
assessment of methods of protein structure prediction - Round VIII. Proteins , 77 ,
1-4.
Muni, D. P., Pal, N. R., & Das, J. (2006). Genetic programming for simultaneous
feature selection and classifier design. IEEE Transactions on Systems, Man, and
Cybernetics, Part B: Cybernetics , 36 , 106-117.
Rotkiewicz, P., & Skolnick, J. (2008). Fast procedure for reconstruction of full-atom
protein models from reduced representations. Journal of Computational Chemistry ,
29 , 1460-1465.
Samudrala, R., & Moult, J. (1998). An all-atom distance-dependent conditional proba-
bility discriminatory function for protein structure prediction. Journal of Molecular
Biology , 275 , 895-916.
Santini, S., Wei, G., Mousseau, N., & Derreumaux, P. (2003). Exploring the folding path-
ways of proteins through energy landscape sampling: Application to Alzheimer’s β-
amyloid peptide. Internet Electronic Journal of Molecular Design, 2 (9), 564-577.
Simons, K. T., Kooperberg, C., Huang, E., & Baker, D. (1997). Assembly of protein
tertiary structures from fragments with similar local sequences using simulated an-
nealing and Bayesian scoring functions. Journal of Molecular Biology , 268 , 209-225.
Sotoca, J. M., & Pla, F. (2010). Supervised feature selection by clustering using condi-
tional mutual information-based distances. Pattern Recognition, 43 , 2068-2081.
Wang, J. H., Yang, C. B., & Tseng, C. T. (2007). Reconstruction of protein backbone
with the α-carbon coordinates. In Proceedings of 2007 national computer symposium
12
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/19
國科會補助計畫
計畫名稱: 限制型最長共同子序列之研究
計畫主持人: 楊昌彪
計畫編號: 100-2221-E-110-050- 學門領域: 計算機理論與演算法
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
