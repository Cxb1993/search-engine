 2
行政院國家科學委員會專題研究計畫年度報告 
仿生人形機器人之發展 總計畫(3/3) 
(Development of Biomimetic Humanoid Robots) 
計畫編號：NSC-97-2221-E-002-161-MY3 
執行期限：99 年 8 月 1 日至 100 年 7 月 31 日 
主持人：黃漢邦 台灣大學機械工程學系 
Email: hanpang@ntu.edu.tw 
研究人員：嚴舉樓、紀孟谷、淩品洋、陳彥霖、李泓逸、楊瑋軒
一、中文摘要 
本計畫『仿生人形機器人之發展』目標
在發展一個類人形的智慧型機器人系統，並
期望能夠建構出完善的智慧型人形機器人軟
硬體平台，並且期望在過程中的各項產出技
術可以被利用在相關的技術領域之中。 
本計畫將要發展之仿生人型機器人的身
高約為 135cm 至 145cm，屬於大型人型機器
人，此類機器人需要強大的馬達扭力輸出，
強健的機構設計以及有效的電源管理，才能
對於如此龐大的系統做有效的控制，對於這
些要點，我們必須做深入的討論。 
第三年本計畫發展動量控制系統，藉由
動力學中轉動慣量與角動量之概念，我們可
以生成對於機器人支撐腳角動量為零之運動
軌跡，藉以增加機器人之步行穩定度，並且
由於此動量控制方法所生成之運動軌跡會使
得腰部及手臂自然擺動，如此一來也可以使
得機器人的步態更像人類。 
本計畫第三年全力著重在機器人之軟韌
硬體之整合，機器人硬體之整合，包含頭部、
腰部、手臂與腳之機構設計、接合處之安裝
以及電路配線。關於韌體之整合，包含從 PC
端經由 USB 發送傳到 PIC C32 轉換成
CAN-Bus 送到各軸C30之命令與通信協定定
義以及馬達端或是感應器之回傳資料解碼。
關於軟體之整合，包含了機器人視覺、軌跡
與運動規劃以及人機互動等等部分。以上這
些部分都是本計畫第三年整合之重點。 
 
關鍵詞：智慧型機器人、人形機器人、零力
矩點、動量控制，視覺系統，人工皮膚，人
工眼，感應器融合。 
Abstract 
The main goal of this three-year project 
“Development of Biomimetic Humanoid 
Robots” is to develop a human-like intelligent 
robot system and we expect to develop a sound 
software-hardware platform for humanoid 
robots. We also expect the technology we 
developed in this project can be applied in 
related domains. 
The humanoid robot which is developed in 
this project is about 135~145cm tall. It is a 
human-sized robot. Human-sized robots need 
large motor torques, strong and robust 
mechanism and effective power management 
for good controlling the robot well. We aimed at 
these points in this three-year project. 
In the third year, we developed a 
momentum control system. By using the 
concepts of moment of inertia and angular 
momentum in dynamics, we can generate 
zero-moment motions toward the support leg in 
order to improve the stability of walking. With 
this method, the robot can walk more 
human-like with smooth arm and torso motions. 
We also focused on the integration of the 
hardware, the firmware and the software in the 
third year. About the integration of the hardware, 
mechanism design of the linkage and the 
installation among the head, torso, arms, hands 
and legs, the circuit board design and the wiring 
of the robot are included. About the integration 
of the firmware, the command transmission and 
 4
 
Figure 2 The Jacobian Matrix with Momentum Jacobian
where m  is the total mass of the robot, E 
is the 3×3 identity matrix (distinct from the I  
inertial matrix), Br τ→  is the 3×1 vector from 
the base link to the total center of gravity (COG) 
and I  is the 3×3 inertia matrix with respect to 
the COG. Mθ  and Hθ  are the 3× n inertia 
matrices that indicate how the joint speeds 
affect the linear and angular momentums, 
respectively. ( ^ ) is an operator that translating a 
vector of 3 × 1 into a skew symmetric 3× 3 
matrix, which is equivalent to a cross product. 
With the n×1 column vector, θ , which contains 
velocities of all joints as its elements, and where 
n is the total number of joints, we could derive 
the P and L represented in the coordinateΣB 
about the pelvis. In addition, all vectors of 
position, velocity, and angular velocity are 
represented in the Cartesian frameΣ0 fixed on 
the ground. 
2 Recursive Inertia Calculation 
 Because the robot is a multi-linkage 
mechanism, we must re-calculate the robot 
inertia under each configuration. The only 
information we can get in advance is the inertia 
and mass of each linkage of the robot. And then 
we can get the whole robot inertia with the 
recursive inertia formula. To derive the mass 
jm , the COG position jc , and the inertia jI of 
the linkages that are next to the linkages whose 
1jm − , 1jc − , and 1jI −  are known, we adopted the 
recursive calculation as shown below: 
1 1j j jm m m− −′= +                            (2) 
1 1 1 1( ) / ( )j j j j j j jc m c m c m m− − − −′ ′= + +                (3) 
1 1 1 1 1 1 1 1( ) ( )
T
j j j j j j j j j j jI I m D c c R I R m D c c− − − − − − − −′ ′= + − + + −       (4) 
( ) ( )T TD r r r I rr≡ −                         (5) 
 
 
Eq. (2) combines the jth and the j-1th mass. Eq. 
(3) calculates the weighted average of the COG 
position.  Eq. (4) calculates the inertia matrix 
with parallel-axis theory. In Eq. (4), Rj-1 is the 
3×3 orientation matrix (or twist matrix) of the 
j-1th joint from its coordinate to the world 
coordinate. Recursively calculating the jm , 
the jc , and the jI  from the tips of swing limbs 
to the fixed foot as the ground part,  
 6
 
 
 
Figure 4 The motion control architecture 
5 Integration of the Whole Robot 
The robot arms, torso, legs and the battery 
are integrated successfully, as shown in Figure 
5. 
 
Figure 5 The integrated robot 
With the integrated robot, a complete humanoid 
robot system is done. The robot can perform 
many different works with motion planning 
algorithms. 
 
Sub-Project 1 
(Force Sensing Array) 
  This work presents the development of a 
reliable capacitive tactile sensing array which 
will be tailored as the artificial skins for a 
humanoid robot.  The proposed capacitive 
sensing element include of two sensing 
electrodes and a common floating electrode.  
The sensing electrodes as well as the metal 
interconnect for signal scanning are 
implemented on the FPCB, while the floating 
electrode is patterned on one of the PDMS 
structures.  This special design can effectively 
reduce the complexity of device structure and 
thus makes the device highly manufacturable.  
The measured maximum sensitivity is about 
14.5%/kPa.  Pressure distribution is captured 
using a scanning circuit.  The deployment of 
the skin on a humanoid robot is also 
demonstrated. 
 
DESIGN 
 The sensing array consists of a 
micromachined PDMS structures and a flexible 
 8
structure layer.  (b) The FPCB fabricated by a 
foundry.  (c) The assembled device  
 The solder resist on the FPCB layer serves 
as the insulator layer between the floating 
electrode and the sensing electrodes.  The row 
and column interconnects are implemented on 
both sides of the FPCB layer with a total 
thickness of 100μm.  Layer-II and PDMS 
structure are aligned each other using a 
microscope.  Layer-II (FPCB) is glued to the 
PDMS structure by applying PDMS 
prepolymer (mixed with curing agent) on the 
perimeters of the layers using a syringe.  And 
the device is cured at room temperature for 24 
hours.  The PDMS structure is firmly bonded 
on Layer-II under normal operation.  The 
assembled schematic of the device is shown in 
Figure 8(c). 
 The fabricated tactile sensing array is 
shown in Figure 9.  The sizes of the sensing 
electrodes of arm skin are shown in Figure 9(a). 
The deposited metal patterns on this layer are 
large square shapes without connecting to each 
other.  Figure 9(b) shows the pictures of the 
fabricated tactile sensing array for robot arms.  
The size of device is 200×100 mm2 including 
interconnects for scanning circuitry.  The 
fabricated sensor arrays for robot face are 
shown in Figure 10.  The special tailoring 
shapes are designed for to cover the face. 
 
Figure 9 The fabricated tactile sensing array. (a) 
the size of the sensing electrode (b) the 
fabricated tactile sensing array on the robot 
arm. 
 
 
Figure 10 The fabricated artificial skin for robot face: (a) 
the left cheek skin (b) the right cheek skin (c) the forehead 
skin (d) the chin skin. 
 
MEASUREMENT AND DISCUSSION 
Figure 11 is the schematics of the experimental 
setup for measuring normal force and shear 
force, respectively.  A force gauge (HF-1, 
ALGOL Engineering Co.), whose maximum 
resolution is 1mN, is used to measure the 
applied force.  The force gauge is fixed on a 
z-axis (vertical) translational stage whose 
displacement resolution is 1 μ m.  The 
capacitances of the sensing element are 
measured by a CV analyzer (Keithley 590, 
Keithley Instruments Inc.).  For normal force 
measurement, a straight PMMA rod is 
connected to the force gauge.  As the z-axis 
stage table moves down, the bump of the 
sensing element is pushed by the PMMA rod.   
Figure 12 shows the measured relationships of 
capacitance (sensor response) vs. applied 
pressure for the sensing elements of three 
different sizes: 18×10 mm2, 15×10 mm2 and 
26×5.7 mm2.  During the measurement of 
each data point, the displacement of the z-axis 
translation stage is slowly increased until the 
force (measured by the force gauge) applied on 
the sensing element reaches to certain 
predefined value, and then capacitance is 
measured by the CA analyzer.  Each data point 
in the figure is the average result by measuring 
a sensing element five times.   
(a) (b) 
15×10 mm2 
18×10 mm2 
(b) 
Tactile sensor 26×5.7 mm2 
(a) 
(c) (d) 
 10
 
 
Figure 15 Pictures of the artificial skins 
installed on the head and arm of a humanoid 
robot. 
 
Figure 16 The measured pressure distributions 
by touching. 
The development of tactile sensing arrays, 
which are tailored as the skins for a humanoid 
robot, was presented.  The proposed sensing 
array, which employs a novel capacitive 
sensing mechanism, consists of a 
micromachined PDMS structure and a flexible 
printed circuit board (FPCB). The 
corresponding scanning circuit was designed 
and implemented.  The characteristics of the 
devices with different dimensions were 
measured and discussed.  The measured 
maximum sensitivity is 14.5%/kPa.  By using 
the fabricated 8x8 sensing array, tactile images 
induced different shapes were also 
demonstrated. 
 
Sub-Project 2 
(Robot Hands, Arms, and Trunk) 
1 Motion planning algorithm for humanoid 
robot arms 
Some of the most significant challenges 
confronting autonomous robotics lie in the area 
of automatic motion planning. The goal is to be 
able to specify a task in a high-level language 
and have robot automatically compile this 
specification into a set of low-level motion 
primitives, or feedback controllers, to 
accomplish the task. Motion planning problems 
arise in such diverse fields as robotics, 
assembly analysis, virtual prototyping, 
pharmaceutical drug design, manufacturing, a 
magically free-flying piano (Figure 17), and 
computer animation. Such problems involve 
searching the system configuration space of 
one or more complicated geometric bodies for a 
collision-free path that connects a given start 
and goal configuration, while satisfying 
constraints imposed by complicated obstacles. 
 
 
 
 
 
Figure 17 Moving a piano 
 
In highly structured spaces, fixed-base 
robot arms perform a variety of task, including 
assembly, in highly structured spaces, 
fixed-base robot arms perform a variety of task, 
including assembly, welding, and painting. In 
painting, for example, the robot must deposit a 
uniform coating over all points on a target 
surface (Figure 18). This coverage problem 
presents new challenges because (1) ensuring 
Robot forehead 
Robot chin 
Robot arm 
 12
 
 
Figure 20 A path planning problem that 
involves finding and using a hammer in a 
virtual world. 
 
Simulation 
Trajectory 
Goal 
Inverse 
kinematics Trajectory 
Cartesian Space Joint Space
ADAMS
OK
MATLAB
 
Figure 21 A path planning problem that 
involves finding and using a hammer in a 
virtual world. 
2 Grasping trajectory planning algorithm 
for humanoid robot arms 
A mobile manipulator working in office or 
home environments is an important issue in 
robotics. Carrying an object is a fundamental 
and common task for mobile manipulators. To 
manipulate an object, the robot needs to sense 
the environment and to acquire the object 
information. Many issues which deal with 
mobile manipulator utilize predefined object 
models, or ID tags which makes it easy to 
identify the object by the mobile manipulator or 
the robot. Selecting a good g rasp of an object 
using an articulated Robot Hand is a difficult  
 
Figure 22 A path planning problem that 
involves finding and using a hammer in a 
virtual world. 
 
problem because of the huge number of 
possibilities. More complex bands have even 
more possibilities. Of course, large portions of 
this 10 dimensional space are worthless 
because the fingers are not in contact with the 
object, but even if the problem were 
reparameterized, a brute force search would 
still be intractable. 
Grasp planning in this study has two major 
issues: (1) how to plan a grasp pose efficiently 
from the 3D model, and (2) how to find a stable 
grasp from the 3D model, even though the 3D 
model has redundant data and errors in shape. 
So we will execute project base on the vision of 
the other project to do grasping planning. And, 
we hope grasp planning to answer the above 
two issues. 
Bin-picking task was achieved based on an 
object model such as CAD model. To find a 
target object, the object recognition process 
matches the object with its model and estimates 
the object pose. Because of a limited variety of 
objects for a task in industry, this technology to 
provide the object model in advance is often 
utilized for industrial situation. In other 
applications, there are grasp planning methods 
which are based on various shape models. In 
these approaches, grasp planning necessary 
takes accurate and precise object models which 
are given in advance. But Stanford scientists 
 14
  
Figure 24 Hand mechanism 
 
 
Figure 25 Arm mechanism 
 
Figure 26 Trunk mechanism 
 
 
Figure 27 The NTU torso humanoid robot 
Mechanism 
 
Sub-Project 3 
(Artificial Eyes) 
Recently biomimetic and humanoid 
mechanisms are paid attention for enhancing 
the functionalities of intelligent robots. The 
ability of spatial sense plays an important role 
in robot control. The functions of visual system 
can be combined with not only robots but also 
other intelligent systems or applications.  
For the robotic vision, the three-dimensional 
image reconstruction based on points-points 
correspondence and multiple view geometry 
has attained remarkable achievements [3].  
The complex process could be even shared 
through internet connections [4]. But it is still 
difficult to be applied to embedded robotic 
system. Accordingly, this subproject intended 
to design a useful visual system based on 
robotic applicative structures. It was also 
attempted to develop one direct and useful 
algorithm which supported real-time operation 
without the help from programmable graphics 
hardware. 
The humanoid visual system developed in 
this subproject included an eye mechanism and 
an image processing algorithm, respectively. 
The eye mechanism consisted of one 
movement module, one vehicle, two eye units, 
and a controller. Each eye unit contained one 
miniature CMOS image sensor. The controller 
was based on personal computer but still 
maintained the compatibility with high-level 
embedded systems such as microprocessor 
single boards. The control algorithm in this 
subproject was aimed to provide the functions 
of real-time obstacle avoidance and distance 
measurement for humanoid robots. It was 
developed based on the concept of computer 
vision and by using the region separation to 
facilitate the comparison between two raw 
image data.  
To enhance the functionality and to reduce 
the image noise of the humanoid visual system, 
a tunable lens and an artificial iris have been 
developed, respectively. In this subproject, a 
tunable lens driven by dielectric elastomer 
elastomer for small form factor and high 
 16
holder, as shown in Figure 29. The gauge shaft 
was connected and aligned to the rotation axis 
of the gear A. Therefore, the gear A rotated 
synchronously with the eyeball and the shaft of 
the angle gauge. The gauge holder was 
designed demountable and was fixed with the 
baseplate by screws. Hence the angle gauges 
could be replaced if needed. The angle gauge 
used in this subproject is MTS-360 with the 
low linearity error of ±0.5%. MTS-360 is 
produced by Piher International 
Corporation, and it was connected to the host 
PC through Arduino-Nano surface mount 
board integrated with an USB interface. 
Miniature cameras with CCD chips were 
attached on the eyeballs and connected to the 
host PC through the USB interface. The 
miniature camera used in this subproject is 
CM-51 from Mega System Technologies Inc. 
It has 1.3 mega-pixels. 
 
 
Figure 30 System control flowchart. 
 
 
Figure 31 Demonstration mechanism and 
presentation.  
The larger window in the left hand side of the 
monitor shows the result of the distance 
measurement to the nearest object. It also 
shows the boundaries of the nearest object 
projected on the left camera. The six windows 
in the right hand side of the monitor shows the 
original images, segmented images and 
corresponded images, respectively from top to 
bottom. The three images on the left column of 
these six images express the images of the left 
eye. The yellow circles in the bottom windows 
indicate the nearest object. 
 
Figure 30 expresses the whole flowchart of 
the system control. This design purposed to use 
regions as corresponded units, which 
substituted for points-points correspondence for 
increasing the operational speed and reducing 
the deficiency of view data. The shapes and 
colors of those regions which were separated 
from camera images are useful information for 
the system to extract suitable features. Those 
features were considered as the basis of image 
correspondence. Then the result and current 
viewpoint of the eyeballs could help to 
calculate all 3D distances of corresponding 
region pairs.  
To demonstrate the system functionalities, 
the driving program rotated the eyeballs in 
parallel to the direction of the nearest detected 
object, and the calculated 3D-distance of this 
object was displayed in a monitor. The neck 
rotations were also helpful when the eye 
mechanism reached its rotation limit. The 
planar projection of the vision boundaries was 
also displayed in the monitor. The monitor 
displayed the processed images from both eyes, 
as shown in Figure 31. 
Micromachined Tunable Lens 
The schematic of the tunable lens is shown in 
Figure 32. The tunable lens was simply made 
of the proposed dielectric elatomer actuator 
which covered an acrylic holder. The liquid 
lens was contained by the acrylic holder. NaCl 
solution was used as the liquid lens. 
Accordingly, the fabrication process was 
simple and low cost. Polydimethylsiloxane 
(PDMS) was used as the elastomer layer in the 
actuator. PDMS has high dielectric constant 
and is transparent to visible light. A thin gold 
layer was deposited on top of the PDMS to 
 18
 
The second method to determine the focal 
length was to measure the lateral profile of the 
tunable lens under different bias voltages. The 
radius of curvature of the lens was then 
extracted through image processing. By 
applying the lens maker’s formula, the focal 
length was obtained. The lens height increased 
by 51 µm when the bias voltage increased to 
900 V, implying the shortening of the focal 
length. The extracted focal length under 
different bias voltages is also shown in Fig. 8. 
For the bias voltage increasing from 0 V to 900 
V at 100 V increment, the focal length 
decreased from 16.1 mm to 13.1 mm. The error 
bars in Figure 35 represent the standard 
deviation of five tests. The focal length of the 
fabricated lens can be tuned by 3 mm with high 
accuracy. 
 
Figure 35 Measured focal length under 
different bias voltages. 
 
Micromachined Tunable Iris 
The proposed artificial iris is depicted in 
Figure 36(a). The device consisted of a PDLC 
film sandwiched by indium tin oxide (ITO) 
electrodes. The bottom electrode served as a 
common ground, and the top electrodes were 
patterned so that the driving voltage could be 
applied on to different areas of the PDLC film, 
as shown in Figure 36(b). When the driving 
voltage was applied across a top electrode and 
the bottom electrode, the director of the liquid 
crystal underneath the top electrode aligned to 
the electric field. Therefore, the PDLC 
underneath the top electrode became 
transparent. The artificial iris became totally 
opaque if all the top electrodes were turned off. 
The fabricated artificial iris is shown in Fig. 37. 
 
 
Figure 36 Schematics of the artificial iris. (a) 
Top view. (b) Side view along AA’. 
 
Figure 37 Fabricated artificial iris with one 
electrode turned on. 
 
 
Figure 38 Schematic of the control system. 
 
The control circuit to drive the artificial iris 
is illustrated in Figure 38. It was composed of a 
photodiode, an operational amplifier, an 
analog-to-digital converter (ADC), a 
microcontroller, and a series of bipolar junction 
transistors (BJTs). The photodiode S1133 from 
Hamamatsu Photonics was used to detect the 
ambient light intensity. Its output voltage was 
linearly proportional to the light intensity. The 
amplifier LM324 from National Semiconductor 
was used to magnify the output voltage of the 
photodiode to 0~3 V. The amplified voltage 
 20
radius of curvature of the NaCl lens was 
consequently changed. The variation of the 
focal length under different bias voltages was 
characterized separately using two different 
approaches, and consistent results were 
obtained. The lens diameter was 4 mm. The 
focus lens can be tuned from 16.1 mm to 13.1 
mm when the bias voltage increased from 0 V 
to 900 V. 
We have designed and fabricated an artificial 
iris which consists of eight concentric PDLC 
rings for modulating ambient light intensity. 
The inner PDLC rings can be sequentially 
turned transparent as the ambient light intensity 
increases. A control circuit has been integrated 
with the artificial iris for driving the PDLC 
rings in response to the ambient light. The 
function of the integrated has been successfully 
demonstrated and characterized. The 
transmittance of the fabricated artificial iris was 
tuned from 94% to 20.4% as the ambient light 
intensity increased from 0 lx to 2500 lx. The 
presented system can be integrated with a lens 
diaphragm for more sophisticated optical 
modulation for assisting patients of aniridia or 
enhancing the humanoid robot vision. The 
complete system will be fully characterized and 
implemented on a humanoid robot in the future. 
 
 
Sub-Project 4 
(Robot Legs) 
I. Mechatronic fabrication and homing 
for the robot 
A biped robot have been designed and 
fabricated in our laboratory since the project 
started, and the mechanism model of the biped 
robot is constructed in SolidWorks shown in 
Figure 40, and Figure 41 is the snapshot of the 
robot. The robot has 12 degree of freedoms 
(DOFs), 6 on each leg. The model includes the 
aluminum structure, motors, Harmonic Drives, 
pulleys, force sensors and batteries. 
II. Walking Gait generation & trajectory 
planning 
The steps of adjusting posture include Squat, 
Shift, Pause, Initial, Final, Return, Stand and 
Buffer. Because we use different dynamic 
model, the time of each step is different, too. 
 
 
The second step is to generate the 
trajectory of COM (Center Of Mass). The third 
step is to use the method called Inverse 
Kinematics to produce the whole trajectory for 
each motor. However, the third step is not the 
last one because the final step is to confirm 
stability of generated trajectory. As the 
beginning of this section is mentioned, the 
ZMP of generated trajectory for each motor 
must be in the support areas, otherwise the 
designed trajectory will let the robot fall down 
when robot move on the ground. All of the 
process of trajectory planning is shown in 
Figure 42.  
 
 
Figure 41 Picture of the biped robot 
 
Figure. 40 Dimensions of the biped robot 
 22
means that when the swing leg starts to leave 
the ground at the beginning of Single Support 
phase, the position of CG projection is out of 
the foot which is support on the ground. The 
ZMP of LIPM is in the foot, but it means the 
acceleration of CG is great enough to keep the 
ZMP in the foot. However, the robot is a 
multi-linkage system, and each linkage has a 
mass. If the robot leg move faster, the dynamic 
response is different from the LIPM. If the ratio 
is smaller than 0.5, 
it means the acceleration of CG is small. 
Therefore, the smaller the ratio, the more stable 
the robot. Figure 46 illustrate CG trajectory and 
ZMP of LIPM on foot in single support phase 
with different distance ratio of single support 
phase and double support phase.  
 
 
 
In order to reduce the inertia force, the walking 
speed must decrease. The second solution to 
stabilize the walking robot is to reduce step size 
because under the same walking period smaller 
step size means smaller speed and acceleration. 
This concept is also used in design the height of 
swing foot, and it also decreases the power 
consumption in small height of foot on the 
swing leg. 
The third method is to fix the clearance of robot, 
the clearance affect the stability when the robot 
landing. Figure 47 show the effect of clearance. 
The main clearance occurs at the hip joint due 
to the maximum torque which is applying to 
hip roll direction. The best way for robot and 
control is to replace the mechanism with a new 
and high accurately manufacture one, but this 
may cost a lot of time in manufacture and 
assembling robot. Hence, another way to deal 
with this problem is clearance compensation, 
which is adding the an angle at hip joint to 
compensate clearance, so the problem becomes 
to know how many degrees change due to the 
clearance. The inclinometer is utilized to detect 
the upper body inclination, so the clearance 
problem is clear. As a result, we reduce the 
compact force because of abnormal landing.  
The forth problem is control error in motor PD 
control, if the error is too large, the robot can 
not walk stably. In the motor control, two 
control strategy, one is control voltage and the 
other is control current, has been compared, 
and there are less control error in control 
current strategy. Besides, tuning the PD control 
parameter within control theorem is a way to 
reduce control error. So far, there are some 
solution to deal with the model difference, 
loading difference, clearance, and control error. 
 
V. Turning Trajectory 
Except for the walking trajectory, we also 
designed turning trajectory for our biped robot. 
Therefore, there are three turning trajectories 
that we designed. The difference between those 
modes is the order of body turn and leg turn. 
Initially, the COM and foot trajectories were 
generated first. Then use Inverse Kinematics to 
Figure 47 The landing effect of clearance. 
 
Figure 46 The illustration of CG trajectory 
and ZMP of LIPM on foot in single support 
phase with different distance ratio of single 
support phase and double support phase. 
 
Figure 45 The problem and the Solution 
was caused by the uncertainty between 
model and robot.  
 24
 
Figure 51 The trajectories of two motors in 
Hip    
Yaw in the second mode. 
If the biped robot takes the first turning 
mode, the body will move back in the initial   
walking direction. This situation will restrict 
the area of turning motion. So, when the robot 
turns in the second turning mode, it can avoid 
this kind of limitation to make a turn.   
    
The third mode：Simultaneity 
In the design of the biped robot, there are 
some restrictions in mechanism that limit the 
work angle of the motor in hip yaw from -40 
degrees to 40 degrees. Because of the 
restriction of mechanism, the maximum angle 
that robot can turn is under 40 degrees. 
Therefore, the third turning mode is used to 
make the robot turn more. The flowchart of the 
third mode is showed in Figure 52.  
In the third turning mode, if the robot takes a 
θ  turning motion, one of the motors in hip 
yaw takes a 2/θ turn and the other takes a 
Yaw in the third mode. 
 
The third turning mode could overcome the 
restriction of mechanism and let the angle that 
the biped robot is able to turn become twice 
than the first mode and the second mode. The 
trajectories of two motors in hip yaw in the 
third turning mode as the robot takes a 30 
degree turn are shown in Figure 53.   
Squat
Simultaneity
Turn 1st
Return
Stand Up
Shift
Return and 
Shift Buffer
Simultaneity
Turn 2nd
 
Figure 52 The flowchart of the third mode 
 
Figure 53 The trajectories of two motors in Hip    
2/θ− turn in Simultaneity Turn 1st, and the 
body takes a 2/θ  turn. And in the 
Simultaneity Turn 2nd, each of those two 
motors turns back. After Simultaneity Turn 2nd, 
the robot finishes a whole θ  turn.  
 
VI. Experiments 
There are one walking in Figure 54 and three 
turning experiments to show the performance 
of the robot when it takes a 10 degree left turn. 
The first turning mode is shown in Figure 55. 
The results of the second mode and the third 
mode are shown in Figure 56 and Figure 57.  
 26
 
Figure 56 The experiment of the second mode    
Figure 57 The experiment of the third mode  
 
 28
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
This sub-project is dedicated to self-fabricate a 
head skull on the viewpoint of ryodoraku 
craftsmanship instead of traditionally 
engineering viewpoint. Many institutes have 
developed robot heads using the 
RC-servomotor and single-chip controllers. 
However, they are still obscure in the 
craftsmanship of the dynamic expressions with 
realistic scenarios. Knowing that the proposed 
of this sub-project is focused on the technique 
integration of the research teamwork, the 
anticipate technique development will consists 
of mechatronic analysis, communication 
techniques of facial emotions, and mechanical 
designs of skull mechanism. In Figure 58, the 
most advanced robot heads around the world 
are shown and will be investigated in the next 
page: While, a simplified classification on the 
different stages of the humanoid-head 
technology is illustrated in Table 1. 
Detailed investigation of the humanoid 
communications on the verbal and nonverbal 
expressions has found that nearly 60% of them 
are nonverbal. 
How to describe the facial expressions of the 
robot head closed to the biological features of 
human becomes a challenging work in the 
development of the humanoid robots. Generally 
speaking, the topics of the robot head can be 
categorized into four different realms, i.e., 
actuator drivers, facial/skull mechanics, the 
artificial skin, and facial dynamics. In this 
sub-project, most head architecture will be 
developed by itself independently. While, the 
facial theories of the artificial skin, the 
facial/skull mechanics, and actuator drivers will 
be technique integrated with the 1st, the 2nd, 
and the 3rd sub-projects, respectively. In other 
words, the issue of facial dynamics will be 
enacted independently in the sixth sub-project. 
Thus, this sub-project will be focused on the 
theoretical derivation, computer simulations, 
experiments, and realistic implementation of 
the facial dynamics on a self-fabricated android 
robot head in the laboratory. What makes and 
how to make a robot head appear human-like 
scenario? It is mentioned in researches that the 
presence of features are the total number of 
features, and the dimensions of the head. The 
infrastructure of the robot skull includes three 
divisions, i.e., the front skull, the rear skull, and 
the jaw mechanism. The front skull consists of 
the facial skeleton and artificial organs, such as 
eyeballs, eyebrows, and eyelids. The degrees of 
freedom on the robot head are prearranged by a 
series of artificial intelligent, AI-class 
servomotors and a 8-bit single-chip controller 
with feedback signals, i.e., CM-5/12V DC，
ATMega128, 16Mhz with torque 12kgf.cm /7V 
and 17.1 kgf.cm /9.6V. As illustrated in Table 1, 
the eyeballs and the robot jaw are actuated 
using the mini-sized pneumatic cylinders with 
2.5mm outside diameter and 5.2mm piston 
stokes.  
 
Figure 58. The infrastructure of the robot skull. 
(a) global space of the robot eyeballs, (b) robot 
facial mechanisms, (c) self-handcrafted FRP 
skull and silicon-gel skin, (d) robot skull 
framework. 
 30
0RLR bbb += ρ                        (7) 
where bρ  and bL  are the conductivity and the 
length of the bladder meridians along the 
artificial skin, respective. Besides, aI  is the 
average current, such that 
pmpmav IItdtffIII 318.02sin
2
0 00
+=+= ∫ π π  (8) 
where mI , pI  are the mean and peak current, 
0f  is the frequency. Considering of the 
mechatronic design and the ergonomic 
evaluation, the output voltage of the electric 
acupuncture bV  is speculated as 
gcbab EVRIV −+= , where cV  is the counter 
voltage and gE  is the ergonomically 
compensated voltage of the patients. Then, the 
gage current measured from system GI  can be 
denoted as 
0)( IrRVI ijbG ++= , where 0r , 0I  are 
the facial resistance and the activating current. 
The diagnosis and activation will be followed 
by the fuzzy inferences of the inference engine 
kC   
i
n
k
n
k
kjkijiiji
n
k wfwfwfC ∑ ∑−
−=
−
−=
++=
+=⊕=
2
1
2
1
,1,11
1
1
2
2
21
1
)( D  (9) 
where ⊕  and D  are both fuzzy logic operators. 
iw  is the weighting factor of each fuzzy rule. 
According to Baye’s theorem, conditional 
probability of electric acupuncture can be 
formulated as 
)(
)()(
)(
,
,
ji
kkk
jik fP
cPccP
fcY =       (10) 
where )( , jik fcY , )( , jifP  is defined as the 
output and the probability of 
jif , . According 
to the ergonomic parameters, the ryodoraku 
signals detected from the spinal cord can be 
divided into three categories, i.e., red, yellow, 
and green.  
The electrical conductivity arranged 
according to the ryodoraku meridians beneath 
the artificial skin are illustrated in Table 2, 
which shows six ryodoraku meridians with 
different electrical and mechanical features. 
The fuzzy rules of the facial dynamics along 
the ryodoraku meridians are defined as 
follows： 
 
Table 2. Electrical conductivity of the 
ryodoraku meridians 
Ryodoraku 
meridians
Facial 
Position 
Current 
（mA） 
Voltage 
(volts) 
 Torque
(kgf-cm)
Lung & vas  Inner face 10-50 2.0-4.5 1.5-4.0
Intestine Outer face 5-20 0.5-2.1 1.5-4.0
Lymph & 
liver 
Forehead  40-80 2.5-4.2 2.0-5.0
Kidneys Eye coners 40-80 2.5-4.2 2.0-5.0
Stomach Eyebrow 
ends 
5.0-10 3.0-4.6 3.0-6.0
Bladders Mouth 
coners 
5.0-10 3.0-4.6 3.0-6.0
 
Rule #1：Robot skull is automatically transfer 
to trigger facial actuator if BA enters the upper 
red region 
Rule #2：Electrode is negative high if BA is in 
the upper yellow region 
Rule #3：Eectrode is negative low if BA is in 
the upper green region 
Rule #4：Electrode is off if BA is in the white 
region 
Rule #5：Electrode is positive low if BA is in 
the lower green region 
Rule #6：Electrode is negative high if BA is in 
the lower yellow region 
Rule #7：Robot skull is automatically turn off 
the facial actuator if BA enters the lower red 
region.  
 
The total degrees of freedom of the proposed 
scheme are 12 DOF, which include symmetric 
actuators along lung & vas, Intestines, lymph & 
liver, kidneys, stomach, and bladders. More 
delicate expressions can be achieved if the 
number of the ryodoraku meridians beneath the 
artificial skin is doubled to the suggested 
number listed in the Table 2. The thickness and 
the electrical conductivity of facial films are 
different and schemed according to the 
ryodoraku analysis discussed.  
In this subproject, advanced artificial 
emotions can be achieved using the 
self-fabricated head skull and ryodoraku 
craftsmanship with the silicon-based material. 
The traditional engineering viewpoint of the 
facial emotions is also replaced by the 
ryodoraku craftsmanship viewpoint. 
Simulations and the ryodoraku experiments 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/07/23
國科會補助計畫
計畫名稱: 總計畫：仿生人形機器人之發展
計畫主持人: 黃漢邦
計畫編號: 97-2221-E-002-161-MY3 學門領域: 機器人學及應用 
研發成果名稱
(中文) 仿生人形機器人之發展--總計畫：仿生人形機器人之發展
(英文) Development of Biomimetic Humanoid Robots
成果歸屬機構
國立臺灣大學 發明人
(創作人)
黃漢邦,楊燿州,施文彬,汪清國,
林沛群
技術說明
(中文) 本計畫『仿生人形機器人之發展』目標在發展一個類人形的智慧型機器人系統，
並期望能夠建構出完善的智慧型人形機器人軟硬體平台，並且期望在過程中的各
項產出技術可以被利用在相關的技術領域之中。 
本計畫將要發展之仿生人型機器人的身高約為135cm至145cm，屬於大型人型機器
人，此類機器人需要強大的馬達扭力輸出，強健的機構設計以及有效的電源管理，
才能對於如此龐大的系統做有效的控制，對於這些要點，我們必須做深入的討論。
 
第二年的主要目標在於機器人之零力矩點/重心軌跡規劃，機器人之視覺，聲音，
語音系統之整合，以及感應器網路之建設以及感應器融合理論。 
本計劃將設計一套軌跡生成器，用以生成穩定步行所需要的重心軌跡。並且我們
著重於跟各子計畫之整合，包含我們將視覺系統與模組整合進入機器人之頭部，
人工皮膚與力感應模組整合到機器人之手臂，軀幹與臉部，並且我們也整合了陀
螺儀，加速規模組用以增進機器人之穩定性。 
(英文) The main goal of this three-year project “Development of Biomimetic Humanoid Robots” 
is to develop a human-like intelligent robot system and we expect to develop a sound 
software-hardware platform for humanoid robots. We also expect the technology we 
developed in this project can be applied in related domains. 
The humanoid robot which will be developed in this project is about 135~145cm tall. It is 
a human-sized robot. Human-sized robots need large motor torques, strong and robust 
mechanism and effective power management for good controlling the robot well. We 
developed a walking pattern generator for generating the COG trajectory for stable 
walking. We also focused on the integration among all sub-projects. The vision system 
has been integrated into the robot head, the artificial skin and force sensing modules are 
also integrated into the robot arms, torso and the face. We also integrated gyroscope and 
accelerometer for enhancing the walking stability.
產業別 研究發展服務業；其他專業、科學及技術服務業
技術/產品應用範圍
機械手臂/工業自動化，人型機器人，教育機器人 
機械腳/人型機器人，教育機器人 
人工皮膚/醫學工程，製程設計，仿生機器人 
人工眼/醫學工程，仿生機器人
技術移轉可行性及
預期效益
機械手臂控制可應用於工業機械手臂，或是可應用於人型機器人之上 
人工皮膚與人工眼可以應用於仿生機器人或是醫學工程領域之上
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
參加 2009 上銀智慧機械手臂比賽獲得競賽冠軍 
「美商國家儀器 GSD 圖形化系統設計競賽：第九屆 NI 應用徵文比賽」榮獲學
術組第二名 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
