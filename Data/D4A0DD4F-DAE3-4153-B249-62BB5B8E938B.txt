 2
the scene, and the multiple blurred images are 
then combined in the brain to form a complete 
image. By contrast, in the superposition eye, 
each ommatidium forms a blurred image of the 
entire vision field. Since each ommatidium 
occupies a slightly different position within the 
eye, minute differences exist between the 
multiple images, and thus a neural pooling 
process is required to make sense of the scene 
[10]. 
In the machine vision field, superposition 
eyes appear to be the most practical way 
forward since existing image acquisition 
technologies (e.g. CCD cameras and the alike) 
are invariably designed to capture the whole 
scene. According to Aloimonos [11], the 
artificial compound eyes used in modern 
automatic visioning systems can be broadly 
classified as either spherical or planar. In this 
paper, emphasis is placed on the planar-type of 
eye. As shown in Fig. 1(a), this type of 
compound eye comprises a rectangular array of 
CCD cameras arranged on a flat plane. In this 
arrangement, each CCD camera essentially 
performs the function of a single ommatidium 
within a biological compound eye. This paper 
considers the simplest case of a planar-type eye, 
namely the single-row Superposition-type 
Planar Compound-like Eye (SPCE) shown in 
Fig. 1(b). 
 
 
(a) 
 
(b) 
Figure 1. (a) Planar compound-like eye, (b) 
Single-row planar compound-like eye 
 
3. Parallel binocular and translational 
motion 
3.1 Basic concept 
Given O is the origin of a 3-D space 
coordinate system, assume two identical CCD 
cameras, perpendicular to the Z-axis at a point 
(0, 0, f), are arranged parallel in the X, Y planes. 
f is focal length. The distances between adjacent 
cameras is h in horizontal direction as illustrated 
in Fig. 2. 
For a single CCD camera to a point (X, Y, Z) 
in 3-D space, its projections on the image planes 
of a CCD cameras is (x, y), based on image 
projection ZfXx /=  and ZfYy /= . If the point 
translates from ),,( ZYX  to ),,( ''' ZYX  with 
XXX Δ+=' , YYY Δ+=' , and ZZZ Δ+=' . 
where ),,( ZYX ΔΔΔ  is the 3D translational 
displacement, the images will move from ),( yx  
to ),( '' yx  respectively in left and right CCD 
camera. 
 
 
Figure 2. Schematic parallel Binocular 
 
3.2 Recovering Translational Motion from 
Parallel Binocular  
Consider two CCD cameras are shown as Fig 
2. And, assume the object ( ) },...,1:,,{S niZYX iii == of 3D points in the 
world, which translates rigidly along the vector 
),,( ZYX ΔΔΔ  to form a new set 'S . Let the 
projections of the set S on the two image planes 
be ( ){ }rljniyx jiji ,.,...,1:, == . Select the left and 
right images in horizontal directions, we can 
obtain easily the relationship based on image 
projection between left and right images.  
Basically, if the left CCD camera is selected 
with the relationship of after motion for the 
object 'S  of 3D points, a pair of image flow 
equations can be established. Sum up 
 4
1×7, 1×9, 1×11, 1×13, 1×15 and 1×25, 
respectively. In every case, the CCD cameras 
were assumed to be spaced at an interval of 90 
mm from their neighbor(s). To ensure the 
reliability of the results, a total of 300 trials 
were conducted for each simulation. The 
investigations commenced by considering one 
translational displacement: (200, 150, 100), 
units in mm. An assumption was made that the 
image data was noise free. Utilizing all above 
SPCE configurations and any motion recovery 
model (i.e. Models A or B or C or D), the 
corresponding ego-motion estimates were found 
to be identical (200, 150, 100) [mm]. The image 
data were then corrupted by noise with a 
variance ranging from 1 to 100 and the motion 
estimation procedure was repeated.  
 
5.1 Validations of four Models 
5.1.1. Accuracy 
Figure 4 shows the relative errors of the 
displacement parameters recovered using 
various single-row SPCE configurations for 3D 
sensing fields containing 5 points, 50 points and 
200 points, respectively. In the recovery process, 
the image points are corrupted by noise with a 
variance of 100 in every case and the 
translational displacement parameters are 
recovered using Models A, B, C, and D, 
respectively.  
 
 
Figure 4. Relative errors of recovered motion 
parameters using four estimation models 
 
Based on the experimental results presented 
in Fig. 4, the following observations can be 
made: 
1) The errors in the translational parameters 
recovered using four Models converge to a 
low value as the number of CCD cameras in 
the single-row SPCE increases. Overall, the 
results indicate that the four models can be 
ranked in order of reducing accuracy as 
follows: Model A > Model B > Model C > 
Model D.  
2) The estimation errors associated with the 
Models C and D recovery scheme remain 
consistently higher than that Models A and B, 
Since Models A and B includes more image 
flow equations than Models C and D. 
However, it is known that the estimation error 
reduces significantly as the number of image 
points increases [12]. In the current case, 
when the number of image points is increased 
to 200, it can be seen that Models C and D 
can provide the best motion estimation 
performance near to Models A and B. From 
inspection of data, for the 1x115 SPCE, the 
relative error of the motion parameters 
estimated using Models C and D are 0.09%, 
0.09%. Models A and B are 0.08%, 0.08%. In 
other words, Models C, D and Models A, B 
can have an identical motion estimation 
performance given a sufficient number of 
CCD cameras in the SPCE configuration. 
 
5.1.2. Computational efficiency 
Since the orders of the stereo disparity 
matrices and image flow vectors are directly 
related to the number of CCD cameras 
configured within the single-row SPCE, the 
execution times of the four motion estimation 
models clearly depend on the scale of the SPCE. 
Table 1 summarizes the total elapsed times of 
the four models when applied to estimate the 
motion parameters of two hundred 3D test 
points using eight different single-row SPCE 
configurations. As expected, the elapsed time 
can be ranked as follows: Model A > Model B > 
Model C > Model D. Model D has the minimum 
execution time. 
 
Table 1. Elapsed time of ego-translation in 
single-row SPCE configurations with four estimation 
models. 
Single-row SPCE configurations (unit: sec) 
Model
1×5 1×15 1×35 1×55 1×75 1×95 1×105 1×115
A 0.06 0.39 1.97 4.87 9.42 18.85 19.53 25.40
B 0.05 0.37 1.84 4.51 8.55 14.18 17.89 22.53
C 0.03 0.09 0.22 0.35 0.47 0.60 0.66 0.72 
D 0.03 0.09 0.21 0.34 0.47 0.60 0.64 0.70 
 6
 
 References 
[1] J. W. Kimball, The Compound Eye, 
Kimball’s Biology Pages. Author website: 
http://users.rcn.com/jkimball.ma.ultranet/Biolog
yPages/C/CompoundEye.html .  
[2] E. C. Sobel, “The locust’s use of motion 
parallax to estimate distance”, J. Comp. Physiol. 
A, vol. 167, 1990, pp. 579–588. 
[3] T. Collett, “Animal behaviour: Survey 
flights in honeybees”, Nature, vol. 403, 2000, 
pp. 488–489. 
[4] J. Tanida, T. Kumagai, K. Yamada, S. 
Miyatake, K. Ishida, T. Morimoto, N. Kondou, 
D. Miyazaki, Y. Ichioka, “Thin observation 
module by bound optics (TOMBO): Concept 
and experimental verification”, Applied Optics, 
vol. 40, no. 11, 2001, pp. 1806–1813.  
[5] R. Ng, M. Levoy, M. Bredif, G. Duval, M. 
Horowitz, P. Hanrahan, “Light field 
photography with a hand-held plenoptic 
camera”, Computer Science Tech. Rep. CSTR 
2005-02, Stanford University, Stanford, CA., 
2005.  
[6] J. Neumann, C. Fermuller, Y. Aloimonos, V. 
Brajovic, “Compound eye sensor for 3D ego 
motion estimation”, in: Proc. IEEE/RSJ Int. 
Conf. Intelligent Robots and Systems, vol. 4, 
2004, pp. 3712–3717. 
[7] C. L. Tisse, “Low-cost miniature wide-angle 
imaging for self-motion estimation”, Optics 
Express, vol. 13, no. 16, 2005, pp. 6061–6072. 
[8] G. L. Lin, “Determining 3-D translational 
motion by the parallel binocular”, to appear in 
ICICIC 2009.  
[9] W. S. Romoser, The Science of Entomology, 
New York: Macmillian Publishing, 1973. 
[10] M. F. Land, D.-E. Nilsson, Animal Eyes, 
New York: Oxford University Press, 2002. 
[11] Y. Aloimonos, New camera technology: 
Eyes from eyes, Author website: 
http://www.cfar.umd.edu/~yiannis/resaerch.html. 
[12] J. Weng, T. S. Huang, N. Ahuja, “Motion 
and structure from two perspective views: 
algorithms, error analysis, and error estimation”, 
IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 11, 1989, pp. 451-476. 
[13] M. Elwell, L. Wen, “The power of 
compound eyes”, Optics & Photonics News, 
1991, pp. 58–59. 
 
