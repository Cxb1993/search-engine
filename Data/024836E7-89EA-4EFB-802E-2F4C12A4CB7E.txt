 2
vision” and “robotic palm and finger control-
ling mouse” are the main keys making 
CU-Robot operated stably, and have been 
achieved preliminarily. The CU-Robot de-
veloped in this project can interact with PC’s 
screen and operate the PC. As long as things 
relating to PC, they will become possibly its 
applications. Accordingly, it is expectable 
that the extended researches and applications 
from this project will be unlimited. 
 
Keywords: Computer Vision, Computer Us-
er, Robot, Robotic Palm and Finger, 
Eye-Hand Coordination Mechanism 
 
二、緣由與目的 
 
 基於科技與電腦技術的突飛猛進，智
慧型機器人方面的研究，近年來備受世界各
國所重視並持續快速發展中。在現今的機器
人研究中，機器人的視覺多半用來分析外界
運動的物體，或分析機器人本身所處環境的
狀況以產生適當的反應機制。在這些熱門研
究領域中，電腦視覺（Computer vision）最
常用來分析我們所處真實世界裡各種物體
外在所表現的「有形」資訊。然而，在我們
所提出的研究範疇裡，機器視覺用來分析電
腦世界中的「無形」資料，即為有關攝影機
對著電腦螢幕進行內容物認知與操控的研
究。現今工業檢測領域中常以「顯示器缺陷
檢查」為主[1][2]，鮮少有針對電腦螢幕內
容做前瞻性機器人視覺反應之研究。 
 現今人類社會裡，電腦的使用已經非
常普及，且人在電腦前面操作的時間也越來
越長。網際網路的發展更提供人類許多便
利，然而也因資訊量愈來愈大，造成人類的
另一種勞累。於此機器人科技蓬勃發展之
際，如何設計一個「能像人操作電腦」的機
器人（其操作「電腦」，除了透過視覺與機
器手外，並無任何有線或無線遂行資料之通
訊），具有幫助我們透過電腦處理網路資訊
的能力，將是非常有用也極具挑戰性的研
究。據前所言，既然現今電腦科技極其蓬勃
發展且電腦幾乎與所有人類的生活作息密
不可分，本研究進行之「會使用電腦的機器
人」可算是新興的研究課題。 
 
CU‐Robot PC
Vision
Action
Mouse Hand
Eye‐Hand 
Coordinate 
Mechanism
Screen 
CCD 
Mind 
Map
Perception
 
圖一 CU-Robot 架構示意圖。 
 
本研究現階段藉著電腦視覺與微電腦
控制的相關技術發展一個獨立的機器人原
型，其具有模仿「電腦使用者」操作電腦的
能力，我們稱之為 Computer User Robot，
簡稱 CU-Robot。換句話說，此 CU-Robot
可以像一般人（電腦使用者）操控電腦的真
實滑鼠，以完成所賦予的任務（Mission）。
此項研究算是相當創新的嘗試，因為在「被
使用之電腦」與 CU-Robot 之間，除了透過
電腦視覺與操作滑鼠的機器手外，並無任何
有線或無線遂行資料之通訊。圖一顯示本研
究計畫計的 CU-Robot 示意圖，圖中右上角
為一般的電腦螢幕（ Display）及滑鼠
（Mouse）。下方 Mouse hand 為 CU-Robot
操作滑鼠的機器手機構；左邊藍色框線內為
CU-Robot 的軟體核心架構，其基本上具有
幾 項 基 本 能 力 ， 一 方 面 分 析 認 知
（ Perception ）從 CCD 來的視覺能力
（Vision），並將認知內容存入心智地圖
（Mind map）中；另一方面則是根據所賦
予的任務（Mission）與 Perception 分析的
 4
值連續的特性（少數部分會有較劇烈
的變動），而螢幕和邊框之間常會有
較大的灰階值落差，藉此可以讓顯示
器的螢幕和邊框之間留下連續的輪
廓，以便後續封閉輪廓追蹤方法能夠
找出螢幕範圍。因此我們使用 Mean－
C 適應性二值化方法，每個像素便會
依其對應的閥值被歸類為背景或輪
廓。灰階影像中的某些像素（例如邊
框）其值和鄰近範圍的像素接近，便
會被歸類為背景；反之若某像素是在
螢幕與邊框的邊界附近，則會被歸類
為輪廓。該結果能使螢幕範圍留下一
圈粗的邊界。 
2. 平行細線化處理（Parallel thinning）：
二值化影像留下的輪廓通常會有數個
像素寬，這是根據前一步驟適應性二
值化的參數（window size n 和常數 C）
所決定。要能使封閉輪廓追蹤方法正
確地保持單一方向（順時鐘或逆時鐘）
追蹤，必須將整個二值化影像進行細
線化處理，而且使單一像素寬的輪廓
二值化影像保有原來的連續性和拓樸
性。 
3. 封閉輪廓追蹤（Closed-contour track-
ing）：在適應性二值化中所留下的二
值化輪廓影像會包含其他並非螢幕邊
界的輪廓，經由細線化處理後依然會
存在。為了加速追蹤螢幕輪廓，我們
不針對二值化影像中所有輪廓線進行
追蹤，而只沿著影像原點斜 45 度角找
尋有交會的輪廓線。一旦找到有交會
的輪廓線，便沿順時鐘方向跟隨連續
的像素追蹤該輪廓是否為封閉。基於
螢幕的特性（在[4]提及），螢幕輪廓
線為封閉的機會較其他區域所留下的
輪廓還大，因此在追蹤一輪後確定為
封閉即標定為螢幕區域。假使沿影像
原點斜45度交會的所有輪廓線都不具
有封閉的情形，即拋棄此張影像並對
下一張影像再重複前述步驟。這是由
於邊框在某些時間四個角落中的某些
陰影影響到螢幕輪廓的封閉性，且基
本上在一段影像串流中螢幕輪廓為封
閉的影像居多，因此不以單一時間點
來觀察，此切割方法依然有效。 
4. 標定螢幕區域：封閉輪廓所留下的像
素點即為螢幕區域，此時 CU-Robot
即可知道針對後續影像在同一區域皆
為螢幕範圍。圖四顯示幾種不同顯示
器和不同的複雜顯示內容，皆可以此
螢幕切割方法標定出螢幕區域。 
 
Grayscale 
Image 
Adaptive 
Thresholding 
Parallel 
Thinning
Specify the 
Display 
Region 
Closed-Contour 
Tracking 
 
圖三 螢幕區域切割方法。 
 
 
圖四 此方法所執行的結果，黃色外框為標
定切割範圍。 
 
在另一方面，由於 webcam 是經由人
工擺設，僅概略放置於螢幕前，多數狀況
下會有透視形變或影像旋轉的問題，所提
切割方法並不會因為形變狀況而影響到螢
幕切割的效能，圖五顯示一些具有影像形
變情形經由此螢幕切割方法的標定狀況。
雖然形變影像中的螢幕區域能被標定出
 6
 
       
圖八 樣板模組舉例。 
  
1. 動態游標偵測（Moving Cursor Detec-
tion）：當 CU-Robot 啟動初始後，螢幕
區域已經標定時，會先令機器手讓滑鼠
進行短距離移動帶動游標移動，並同時
偵測攝影機捕捉到的前後兩張影像（已
經過校正）螢幕上動態物件。接著在影
像中偵測到的動態物件的範圍內以樣
板資料庫裡的滑鼠游標樣板（圖八最後
一個）比對該動態物件是否為滑鼠游
標，進而在心智地圖中記錄游標位置
C 與大小 C 。此方法需要配合機器手
機構之動作。 
2. 靜態區域追蹤（Static Region Track-
ing）：以存在 CU-Robot 的心智地圖中
的滑鼠游標位置為基準，在其附近擴大
一個範圍γ，在γ內以游標樣板一次移
動一像素比對 C ，計算γ內與樣板距
離最短的點為新的游標位置。 
CU-Robot 在常態下都以靜態區域追蹤
方式持續標定游標位置，只有在最初始時
及無法確定心智地圖中所記錄之游標位置
正確性時，才以動態游標偵測法重新標定
游標位置。 
 
3.5 心智地圖建立 
 CU-Robot在進行螢幕視覺認知後必須
將視覺物件（Vision object）的幾項重要資
訊存放起來，以供後續眼手協調機制使
用。此存放的空間稱之為心智地圖（Mind 
map），其資訊主要包含某視覺物件為何及
該視覺物件在相對於螢幕區域原點的座
標 。 心 智 地 圖 為 一 個 階 層 性 架 構
（Hierarchical architecture），參考圖九，
在最上層稱為根源物件（Root object）；每
個物件以字串「名稱」為其索引，其下可
以個別加入或刪除某個被認知到的子物件
(Child object)，而子物件內又可能包含其他
子物件。 
需要取得某個子物件資訊或探索該物 
件是否存在於心智地圖時，CU-Robot 可以
從最根源的階層或其中某一層以「名稱」
向同階層或向下層搜索該物件，進而取得
該物件的資訊。CU-Robot 所使用的心智地
圖根源物件為原始校正後的影像，圖十顯
示目前應用實際例子，其下層的視覺物件
關係可能如圖十一所示。 
 
 Root Object 
 Child Object 1 
 Child Object 2 
 Child Object 3 
 Child Object 1-1 
 Child Object 1-2 …
 
 Child Object 2-1 …
 
…
  
圖九 心智地圖的階層性架構。 
 
Original Corrected Image 
(Root Object) 
Screen
Cursor
Txt1
Txt2 
Txt3 
A
B
C
 
圖十 心智地圖以圖示表示。 
 
 Original Corrected Image 
 Screen 
 Cursor 
 Txt1 
 Txt2 
 Txt3  
圖十一 以階層性架構表示圖十之情形。 
 8
3.7 眼手協調機制 
 一般來說，游標在螢幕上移動的距離
與滑鼠實際移動的距離並不一定具有特定
的關係式，某些情況下也與滑鼠移動的加
速度有關，甚至不同的滑鼠與電腦又有不
同的設定。再者，經由螢幕視覺所認知的
目標物與游標的距離（通常單位為像素），
和實際機器手機構移動的距離(單位為公
厘)之間無法產生一個絕對的轉換公式。因
此 CU-Robot 無法決定移動滑鼠多少可以
讓游標移到目標物上，必須以螢幕視覺 SV
和認知區塊（Perception）做為控制 MH 的
迴授機制，這個機制即稱為 CU-Robot 的
「眼手協調機制」。它是一個結合認知區
塊結果和人所給予的任務（Mission）做出
適當的滑鼠操作動作（Action），此機制的
好壞直接影響 CU-Robot 能否有效地執行
各項任務。 
 此機制的執行過程配合圖十四說明： 
1. MH_Move：透過 Mouse Hand 移動滑
鼠朝向目標移動。 
2. MH_Click：透過 Mouse Hand 在目標上
單擊滑鼠鍵。 
3. P_Check：透過 Perception 檢查、認知
螢幕上的游標位置。 
 
MH_MoveStart a task step P_Check
MH_ClickEnd of this task step
Not Yet
OK
 
圖十四 眼手協調機制。 
 
若將每次「滑鼠移動到目標位置，並按下
滑鼠按鍵」視為一個基本工作（Task），
CU-Robot 之機器手掌指 MH 會開始移動
滑鼠（MH_Move），控制 XY 移動平台以
游標朝向目標方向移動；此同時認知區塊
會持續追蹤滑鼠游標位置是否移動到目標
（P_Check），如果尚未移到目標處，則繼
續執行 MH_Move，直到 P_Check 認定滑
鼠游標已經在目標物上或合理可點擊的範
圍內將 XY 移動平台停下。最後，CU-Robot 
之 機 器 手 掌 指 MH 單 擊 滑 鼠
（MH_Click），如同我們手指單擊滑鼠一
樣，告訴「被使用之電腦」已完成這個部
分工作（Task）。當中若 XY 平台已經超
過預先訂定的移動界限，MH 還必須將滑
鼠抬起返回 XY 移動平台原點，繼續執行
MH_Move 往目標物移動。 
 
Action
Initial 
Vision
Mind Maps
Mission 
complete
Maps 
Generation
Analysis
Live Vision
Mission
 
圖十五 執行完整任務之概要流程。 
 
將多次的Task合併為單一任務操作流
程，可以用圖十五做說明。假設一個任務
（Mission）已經賦予 CU-Robot，同時「被
操作之電腦」也已經運行該任務的場景。
螢幕視覺 SV 對「被使用之電腦」螢幕進
行基本視覺定位（Initial Vision），並由認
知區塊分析螢幕上所顯示的物件產生心智
地圖（Mind Maps）。在分析階段中，
CU-Robot 就所接受的任務與現有之相關
心智地圖進行比對分析（Analysis），同時
開始進行活動視覺（Live Vision），此部分
的機制至為重要，如同人類視覺中的選擇
 10
及螢幕上其他物件的認知已經建立相關基
礎。目前 CU-Robot 對於螢幕上的滑鼠游標
位置辨識和兩個區塊的定位不會有太大的
偏差，即使 CU-Robot 在第一次認定誤判，
還是可以在第二次嘗試時達成，這與機器
手的控制速度和螢幕視覺的游標位置更新
速度有關。在機器手機構控制上，目前以
穩定與可行性評估為主，暫不求快速，未
來在 XY 移動平台的控制方法進行調整及
滑鼠游標位置識別改進後，應可增進滑鼠
移動速度與準確度。整體上而言，計畫所
預定的基礎核心─眼手協調機制已經成
形，是本計畫執行的一大突破。 
 此計畫研究內容項目其實非常多，在
有限的時間內必須完成包含硬體機構設計
建置、微電腦程式控制、控制電路設計、
電腦視覺演算法開發及 CU-Robot 核心程
式撰寫，涵蓋軟、硬體之間功能的相互匹
配。再者，機器人的核心程式架構撰寫並
不容易，並非單一執行緒即可完成單一任
務，從本文開始之圖一的架構即可顯示各
部分的功能區塊都是獨立的執行緒，在軟
體設計的架構上不是一件容易的事。例如
螢幕視覺 Vision 持續在切割螢幕區域和校
正的同時，認知區塊 Perception 也在持續追
蹤滑鼠座標記錄於心智地圖中。當我們下
達執行任務的指令時，認知區塊還必須找
出螢幕上目標物的位置和大小，建立於心
智地圖上，進而讓眼手協調機制通知動作
區塊控制機器手移動滑鼠。在經過一段時
間的程式運行後，各區塊的功能也都活動
並相互配合，如同人類可以在同一時間執
行各種感知和動作。 
計畫執行至今，從機器手硬體到基本
的 CU-Robot 核心架構已經掌握，整體系統
已經建置。CU-Robot 本身的核心認知能力
也已經擁有，CU-Robot 的執行程式必須配
合任務內容撰寫相應的程式，但基本的結
構都已經在目前確定下來。接下來進行的
研究必須再精進後二項展示任務的環境，
使本階段的研究計畫更為完備。未來還會
再加入鍵盤使用的功能，即可讓 CU-Robot
在操作電腦上如同人類電腦使用者一般的
靈活。本計畫之研究成果可啟發未來機器
人發展的走向，「此類機器人」不僅行為
像人，其思維創造力也可能加速進化其為
「可學習人類所發展之電腦網際網路知識
累積平台」者。由於本計畫所產出的研究
成果 CU-Robot，其可以與電腦螢幕信息互
動並操控電腦，將來舉凡與電腦有關者，
均可成為可應用的範疇。故而，預期所延
伸的研究與應用，無可限量！ 
 
 
五、參考文獻 
[1] S.H. Kim, J.H. Kim, and S.W. Kang, 
“Nondestructive defect inspection for 
LCDs using optical coherence tomo-
graphy,” Displays, In Press, Corrected 
Proof, Available online 21 Apr. 2011. 
[2] L.F. Chen, C.T. Su, and M.H. Chen, “A 
neural-network approach for defect 
recognition in TFT-LCD photolithogra-
phy process,” IEEE Trans. Electron. 
Packag. Manuf., vol. 32, no. 1, pp. 1-8, 
Jan. 2009. 
[3] Multithreading with C++ and MFC, 
http://msdn.microsoft.com/en-us/library/
975t8ks0(v=VS.100).aspx 
[4] Y.S. Chen and K.L. Lin, “Display re-
gion segmentation from a computer 
screen image using closed-contour 
tracking,” Proc. of International Con-
ference on Machine Learning and Cy-
bernetics, Guilin, Guangxi, China, (July 
10-13, 2011), Vol. 4, 1739-1745, 2011. 
[5] M. Sonka, V. Hlavac, and R. Boyle, 
Image Processing, Analysis, and Ma-
chine Vision, ISBN 0-534-95393-X, 
PWS, Chapter 4, 63-68, 1998. 
[6] C.S. Green and D. Bavelier, “Action 
video game modifies visual selective at-
tention,” Nature, Vol. 423, 29 May 
2003. 
 
 
Page 2-2 
圖 4 
  
                   圖 2                                   圖 3 
 
本年的 ICMLC 研討會與 ICWAPR (International Conference on Wavelet Analysis and Pattern 
Recognition) 同時舉行 (圖 2)，大會論文集由 IEEE 出版，有光碟 (圖 3)以及紙本論文集。由
於論文篇數多，所安排的 sessions 也多 (在此不贅述，參見次頁之 Program Schedule)，然其
中令人印象深刻的有兩點： 
1. 捷克 Czech Technical University in 
Prague的Prof. Vladimir Marik介紹
世界上少有的 Department of 
Cybernetics (圖 4)。 
2. 此次大會安排了所謂的博士生論
文發表討論會 PhD Colloquia，由
學者專家給予博士生的一些研究
建議，本人也受邀參加其中一場給
予博士生指導。本場次中有三位分
別來自澳洲、香港、大陸的博士生
發表文章，我們手中也有他們的博
士論文研究摘要，除了給予他們實
質建議外，亦分別發給他們證書，
頗為特別，值得借鏡！ 
 
大會的晚宴在此桂林大宇喜來登酒店的一樓舉行，本人因擔任 Session Chair，被大會安排在
前面的主要桌次，本人與本系林志民講座教授同桌，並認識一些重要人士，特別是 International 
Journal Machine Leaning and Cybernetics (IJMLC)的 Editor-in-Chief, Prof. Xizhao Wang(本人已
於 2010/10 受邀開始擔任此期刊的 Associative Editor)。總體來說，本次會議的參與收穫頗豐。 
 
論文集： 
紙本論文集 (Vol. 4)、CD 論文集、研討會摘錄手冊 
 
次頁開始為 Program Schedule 以及所發表的兩篇論文影本。 
3
3
 
 1
2
-J
u
ly
 (
T
u
e
s
d
a
y
) 
0
9
:0
0
 –
 1
0
:0
0
 
K
e
y
n
o
te
 S
p
e
e
c
h
 C
 (
R
m
 S
) 
S
p
e
a
k
e
r:
 
S
h
y
i-
M
in
g
 C
h
e
n
 
T
o
p
ic
: 
F
u
z
z
y
 F
o
re
c
a
s
ti
n
g
 B
a
s
e
d
 o
n
 H
ig
h
-O
rd
e
r 
F
u
z
z
y
 T
im
e
 S
e
ri
e
s
 a
n
d
 G
e
n
e
ti
c
 A
lg
o
ri
th
m
s
 
 
1
0
:0
0
 –
 1
0
:2
5
 
C
o
ff
e
e
 B
re
a
k
 (
F
o
y
e
r)
  
1
0
:2
5
 –
 1
2
:2
5
 
P
a
n
e
l 
D
is
c
u
s
s
io
n
 (
R
m
 S
) 
S
p
e
a
k
e
r:
 
D
a
n
ie
l 
Y
e
u
n
g
, 
T
in
 K
a
m
 H
o
, 
H
o
n
g
 Y
a
n
, 
S
a
m
 K
w
o
n
g
 a
n
d
 V
la
d
im
ír
 M
a
ří
k
 
T
o
p
ic
: 
T
h
e
 G
e
n
e
s
is
 o
f 
a
n
 I
n
n
o
v
a
ti
v
e
 R
e
s
e
a
rc
h
 T
o
p
ic
 
 
1
2
:3
0
 –
 1
3
:5
0
 
O
T
A
1
 (
R
m
 A
) 
In
te
lli
g
e
n
t 
In
fo
rm
a
ti
o
n
 M
in
in
g
 
O
T
B
1
 (
R
m
 B
) 
In
te
lli
g
e
n
t 
S
y
s
te
m
s
 V
I 
O
T
C
1
 (
R
m
 C
) 
F
u
z
z
y
 S
y
s
te
m
s
 
P
T
1
 (
R
m
 S
) 
(1
3
:0
0
-1
4
:3
0
) 
Im
a
g
e
 P
ro
c
e
s
s
in
g
 
1
3
:5
0
 –
 1
5
:1
0
 
O
T
A
2
 (
R
m
 A
) 
T
e
x
t 
a
n
d
 M
u
lt
im
e
d
ia
 
In
fo
rm
a
ti
o
n
 R
e
tr
ie
v
a
l 
O
T
B
2
 (
R
m
 B
) 
F
u
z
z
y
 S
y
s
te
m
s
 a
n
d
 I
n
te
lli
g
e
n
t 
S
y
s
te
m
s
 
1
5
:1
0
 –
 1
5
:3
0
 
C
o
ff
e
e
 B
re
a
k
 (
F
o
y
e
r)
 
P
T
2
 (
R
m
 S
) 
(1
4
:3
0
-1
6
:0
0
) 
F
u
z
z
y
, 
W
a
v
e
le
ts
 a
n
d
 N
u
m
e
ri
a
l 
 
O
p
ti
m
iz
a
ti
o
n
s
 
1
5
:3
0
 –
 1
6
:5
0
 
O
T
A
3
 (
R
m
 A
) 
In
te
lli
g
e
n
t 
S
y
s
te
m
s
: 
M
e
th
o
d
o
lo
g
ie
s
 a
n
d
 
A
p
p
lic
a
ti
o
n
s
 I
 
P
h
D
 C
o
ll
o
q
u
ia
 A
 (
R
m
 B
) 
 
1
6
:5
0
 –
 1
8
:1
5
 
O
T
A
4
 (
R
m
 A
) 
In
te
lli
g
e
n
t 
S
y
s
te
m
s
: 
M
e
th
o
d
o
lo
g
ie
s
 a
n
d
 
A
p
p
lic
a
ti
o
n
s
 I
I 
P
h
D
 C
o
ll
o
q
u
ia
 B
 (
R
m
 B
) 
IW
W
IP
 (
R
m
 C
) 
In
te
rn
a
ti
o
n
a
l 
W
o
rk
s
h
o
p
 o
n
 
W
e
b
 I
n
fo
rm
a
ti
o
n
 P
ro
c
e
s
s
in
g
 
P
T
3
 (
R
m
 S
) 
(1
6
:0
0
-1
7
:3
0
) 
S
y
s
te
m
 C
o
n
tr
o
l 
a
n
d
 M
o
d
e
lin
g
 
1
9
:3
0
 –
 2
2
:0
0
 
B
a
n
q
u
e
t 
(R
m
 S
) 
 1
3
-J
u
ly
 (
W
e
d
n
e
s
d
a
y
) 
0
9
:0
0
 –
 1
0
:0
5
 
O
W
A
1
 (
R
m
 A
) 
S
y
s
te
m
s
 a
n
d
 C
o
n
tr
o
ls
 
O
W
B
1
 (
R
m
 B
) 
In
n
o
v
a
ti
v
e
 W
a
v
e
le
t 
th
e
o
ri
e
s
 
a
n
d
 t
e
c
h
n
o
lo
g
ie
s
 
O
W
C
1
 (
R
m
 C
) 
M
a
c
h
in
e
 L
e
a
rn
in
g
 a
n
d
 I
ts
 
A
p
p
lic
a
ti
o
n
 
P
W
1
 (
R
m
 S
) 
(9
:0
0
-1
0
:0
0
) 
In
te
lli
g
e
n
t 
S
y
s
te
m
s
 w
it
h
 
A
p
p
lic
a
ti
o
n
s
 
1
0
:0
5
 –
 1
0
:2
0
 
C
o
ff
e
e
 B
re
a
k
 (
F
o
y
e
r)
 
1
0
:2
0
 –
 1
1
:2
5
 
O
W
A
2
 (
R
m
 A
) 
S
ig
n
a
l 
P
ro
c
e
s
s
in
g
, 
W
a
v
e
le
ts
 
a
n
d
 A
p
p
lic
a
ti
o
n
s
 
O
W
B
2
 (
R
m
 B
) 
N
e
u
ra
l 
N
e
tw
o
rk
 a
n
d
 L
e
a
rn
in
g
 
O
W
C
2
 (
R
m
 C
) 
S
y
s
te
m
s
 M
o
d
e
lin
g
 a
n
d
 
C
o
n
tr
o
l 
1
1
:2
5
 –
 1
2
:3
0
 
O
W
A
3
 (
R
m
 A
) 
A
d
v
a
n
c
e
d
 C
o
m
p
u
ta
ti
o
n
a
l 
T
e
c
h
n
iq
u
e
s
 i
n
 M
e
d
ic
a
l 
Im
a
g
in
g
 
O
W
B
3
 (
R
m
 B
) 
M
u
lt
im
e
d
ia
 P
ro
c
e
s
s
in
g
 
O
W
C
3
 (
R
m
 C
) 
C
o
g
n
it
iv
e
 C
o
m
p
u
ti
n
g
 a
n
d
 
D
a
ta
 M
in
in
g
 
P
W
2
 (
R
m
 S
) 
(1
0
:0
0
-1
1
:0
0
) 
S
ta
ti
s
ti
c
a
l 
L
e
a
rn
in
g
 
 
  
3) Both of frmP  and dispP  are consecutive regions, 
respectively. 
4) If ( )yxPgray ,  and some of its neighbors belong to 
dispP  but the others belong to frmP , there exists an 
intense change of gray level among the frame 
neighbors and the display region neighbors. 
5) All of the pixels that satisfy 4) can be gathered to 
consist of a closed contour that is the display region. 
Taking advantage of the characteristics above, we can 
separate the frame and the display region of a computer 
screen by using such edge-like feature. However common 
edge detectors like canny edge detector [5] are not easy to 
find a simple closed contour outlining the display region 
which preserves connectivity. Moreover, objects in an 
image will usually suffer from the distortion of an optical 
imaging system. Thus the display region of a screen in an 
image may not seem so rectangular. To segment the display 
region of a computer screen for further analysis, it is 
necessary to track the region’s contour in object-distorted 
images. 
In our study, a mean-C adaptive thresholding 
algorithm [6] is adopted. After performing this algorithm, 
the resultant image will contain thick closed contours 
including the one of a display region, and other thick 
unclosed ones. Though the pixels belonging to neither frmP  
nor dispP  may also remain edge-like contours, they cannot 
become closed since they are inconsecutive. The mean-C 
adaptive thresholding results in thick contours in the binary 
image thresP . 
To track along the contours pixel by pixel with a good 
topology to find out the specific closed contour of the 
display region, all of the contours are needed to be thinned. 
In [7], the parallel thinning algorithm provides a fast 
thinning process which can assist our methodology to be in 
real-time. All of the contours will be reduced to 1-pixel 
wide. The topologies of those contours should be connected, 
especially the closed contour of display region are all 
preserved in thinP . 
The last step presented in section 3 is to apply the 
proposed closed-contour tracking algorithm to thinP , the 
display region will be then found. Though the contours 
within the display region may remain to be closed, only the 
most exterior closed contour will be considered as the one 
belonging to the display region. 
 
Grayscale 
Image 
Adaptive 
Thresholding 
Parallel 
Thinning
Specify the 
Display 
Region 
Closed-Contour 
Tracking 
 
Figure 1. Steps for the proposed segmentation approach. 
Figure 1 shows the proposed segmentation approach. 
Following subsections takes a brief review of the applied 
mean-C adaptive thresholding algorithm and the parallel 
thinning algorithm, which are regarded as the preprocessing 
in our approach. 
2.1. Mean-C Adaptive Thresholding 
As mentioned above, only those pixels that some of 
neighbors belong to frmP  and the others belong to dispP  
will exist intense changes of gray level and remain 
edge-like contours. In [6], local thresholds over the entire 
image are used for adaptive thresholding. For a grayscale 
image grayP , the corresponding local threshold ( )yxTL ,  
for each pixel ( )yxPgray ,  is the mean, which is calculated 
from its local neighborhood, minus a constant C. The local 
threshold is defined as 
( ) ( )( ) CjiW
n
yxT
nx
nxi
ny
nyj
yxL −⎥⎥⎦
⎤
⎢⎢⎣
⎡
= ∑ ∑−−
−+=
−−
−+=
2/)1(
2/)1(
2/)1(
2/)1(
,2 ,
1,
 
 
(1) 
where ( ) ( )jiW yx ,,  is a n by n window and 
{ }… ,9 ,7 ,5 ,3∈n . Such window contains ( )yxPgray ,  at the 
center and its 1−× nn  neighboring pixels. The resultant 
binary image thresP  is decided as 
( ) ( ) ( ).,, if    ,0 otherwise   ,1, ⎩⎨
⎧ >
=
yxTyxPyxP Lgraythres       (2) 
Based on the characteristic 1), those pixels in grayP  having 
gray level similar to their own neighboring pixels will result 
in local thresholds less than themselves. By 2) these pixels 
will become background (white pixels). And by 4), others 
having intense changes of gray level within the windows 
will result in local thresholds greater than themselves. 
Thick contours (black pixels) will be preserved in the image 
thresP . 
1740
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
  
At 0S , the result of ( )yxPM ,0  is ( ) ( )lhTyxPM ,, 11 = . 
And according to the algorithm, 2T  is the transpose of 1T . 
Hence the result of ( )yxPM ,1  at 1S  is ( ) ( )hlTyxPM ,, 10 = . 
For example, if it is at 1S  and the 8 neighboring pixels of 
( )yxPM ,1  formed ((1101)BIN, (1000)BIN) = (DHEX, 8HEX), 
then ( ) ( ) 0D,8, 10 == TyxPM . Thus ( )yxPM ,1  should be 
eliminated. Figure 4 shows the thinning result of Figure 
2(b). 
3. Closed-Contour Tracking Algorithm 
The basic idea of the algorithm is to use a mask for 
tracking the consecutively connected pixels in ( )yxPthin , . 
Before presenting the proposed closed-contour tracking 
algorithm, some definitions will be introduced first. 
 
Figure 4. Thinning result of Figure 2(b). 
3.1. Definitions 
Definition 1. The indices of row (x) and column (y) 
of an image are defined to start from 0. 
Definition 2. Given a 33×  mask M centered at 
( )yxPthin ,  is denoted as ( )yxM , , the 8 orientations in 
clockwise order from O1 to O8 that the mask can move are 
specified in Figure 5(a). For instance, in Figure 5(b) the 
solid blue mask ( )AA yxM ,  centered at pixel A is the current 
position, and the dotted red mask ( )BB yxM ,  is at the 
position after moving toward O1 to pixel B. 
 
 
 
O1 O2 O3 
O4 
O5 O6 O7 
O8 
 
    
    
    
    
B 
A 
(a) (b) 
Figure 5. (a) A 33×  mask and the order of its 8 orientations. (b) An 
example for the mask moving toward its next connected pixel. 
Definition 3. Searching in clockwise order from a 
starting orientation { }7531 ,,, OOOOOS ∈ , while a 
connected pixel is found at OF, where [ ]8,1∈F , a variable 
( )SF OOd ,  is used to represent the numbers of orientations 
that have passed. In Figure 5(b), assume that the search is 
starting from O7, then ( ) 3, 71 =OOd . 
Definition 4. A set thinPL ⊂  includes the southeast 
pixel of the top left-hand corner pixel ( )1,1thinP  and all the 
pixels along such orientation of ( )1,1thinP  till reaching the 
boundary of the thinned image: 
( ) ( )( ){ }+∈<≤== ΖiMiiiPiipL thinL  and ;1  , , ,    (3) 
where M is the height of thinP . 
Definition 5. L is crossing a contour at ( )iipL  ,  
(4a)( )
( ) ( )⎩⎨
⎧
=−=
=
1 ,1 and 0 ,
or ,1 ,
 if
iiPiip
iip
thinL
L
   (4b)
3.2. Tracking Process 
The tracking process starts from ( )1 ,1Lp  along L. If 
(4a) or (4b) is satisfied, such pixel is called a starting pixel 
SPp  of the current finding contour. M is then set to ( )iiM ,  
and tries searching if there is any connected black pixel 
around ( )iiPthin ,  starts from O1 in clockwise order. Figure 
6 gives an example thinP  that illustrates the closed-contour 
tracking process. The elements in the set L including pixel 
A are indicated by the crossness of thick diagonal green 
line. 
 
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
0 O1 O2 O3       
1 O8 B O4  O1 O2 O3   
2 O7 O6 O5  O8 C O4   O1 O2 O3
3 O1 O2 O3  O7 O6 O5   O8 D O4
4 O8 A O4       O7 O6 O5
5 O7 O6 O5       
6 I       
7       E
8 H O1 O2 O3    
9 O8 G O4    
10 O7 O6 O5   F 
11        
Figure 6. An example 1216×  binary image thinP  that illustrating the 
closed contour. 
1742
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
  
-
(a) (b) 
Figure 8. The display region segmentation result (the thicker colored 
line) is shown in (a) the thinned binary image and in (b) the original 
gray image. 
Some other results of different kinds of computer 
screens and different displaying contents are given in 
Figure 9 to demonstrate the feasibility of the proposed 
approach. Furthermore, our method also performs well even 
if the computer screen is captured with different viewpoints 
or rotated, as the results shown in Figure 10. 
 
Figure 9. Some other examples obtained by applying the proposed 
approach. The display region is segmented and represented by thick 
colored contours. 
 
(a) (b) 
(c) (d) 
Figure 10. Results of images that are suffering from distortions. (a) A 
trapezoidal distortion. (b) Another viewpoint and trapezoidal 
distortion. (c) A counter-clockwise rotation. (d) A clockwise rotation. 
To examine the real-time characteristic of the proposed 
approach, we calculate time elapse t∆  that all the steps 
stated in Figure 1 are taken for processing a single frame. 
The instantaneous frame rate φ that indicates the processing 
speed for a single frame is the reciprocal of t∆ . The 
experimental results show that φ can be up to 
approximately 27 FPS, which is very close to the input 
frame rate of the webcam. Examples of φ are also shown at 
the bottom right-hand corner of images in Figure 9 and 10. 
Although during one second period there might exists some 
frames that display region is not able to be segmented 
which may be due to the lighting adjustment function of the 
capturing device or the bad illumination causing the noise 
branch at the corners of the contour, it will not affect the 
normal process. 
5. Conclusions 
So far we have brought up a novel research of 
computer vision that perceiving information from another 
computer screen. The proposed algorithms to segment the 
display region is indeed a necessary preprocessing step 
before perceiving information through computer screens 
and can be executed in real-time. Even though the captured 
image may suffer from distortion due to optical imaging 
component, rotation, or viewpoints that are not straight 
before the screen, the method performs still well. Along this 
research, we will consider other effective algorithms to 
resolve distortion issues in the near future. 
Acknowledgments 
This work was supported in part by the National 
Science Council, Taiwan, Republic of China, under Grant 
Nos. NSC99-2221-E-155-080. 
 
 
1744
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
International Conference on
Machine Learning and Cybernetics 2011
Dear Author,
Congratulations. Your paper has been accepted for publication in the proceedings of the
International Conference on Machine Learning and Cybernetics (ICMLC) 2011.
Please verify the following items to ensure their accuracies:
(1) Please confirm the following:
Paper ID: 3832
Title: DISPLAY REGION SEGMENTATION FROM A
COMPUTER SCREEN IMAGE USING CLOSED-CONTOUR
TRACKING
Authors: YUNG-SHENG CHEN, Yuan Ze University
Please be reminded your paper will be rejected if the title, names of author(s) or
the order of author list of your final version is different from the original version.
(2) If your paper exceeds six pages, you must pay HKD 540 (USD 70 #) per extra
page.
(3) Your paper will NOT be published in the conference proceedings unless you
COMPLETE EVERY STEP of the following tasks by 11-May-2011
a. Register on or before 11-May-2011
The registration fee:
Non-IEEE Member: USD 550 (equivalent to HKD 4270 #)
IEEE Member*: USD 500 (equivalent to HKD 3890 #)
Student*: USD 450 (equivalent to HKD 3500 #)
Please pay in Hong Kong Currency.
* Identification(s) may be required. Please bring your identification(s) to
the conference.
# As of 20-April-2011, the exchange rate is HKD 7.78 for one USD.
The registration fee of each participant should be settled by credit card
payment. For detail, please see the other attachment (creditcardpayment.
doc)
Please be reminded when you settle the registration fee, attach the Paper
ID, Paper Title and the Registrant Name as a remark.
Page 1/2
  
VIDEO-BASED INTELLIGENT VEHICLE CONTEXTUAL INFORMATION 
EXTRACTION FOR NIGHT CONDITIONS 
DUAN-YU CHEN, JUN-JHE WANG, CHIA-HSUN CHEN, YUNG-SHENG CHEN 
Department of Electrical Engineering, Yuan Ze University, Taiwan 
E-MAIL: dychen@saturn.yzu.edu.tw, s974642@mail.yzu.edu.tw, s970509@mail.yzu.edu.tw, eeyschen@saturn.yzu.edu.tw 
Abstract: 
Advanced warning system for vehicles is a critical issue 
in recent years for automobiles, especially when the number 
of vehicles is growing rapidly world wide. The cost down of 
general cameras makes it feasible to have an intelligent 
system of visual-based event detection in front for forward 
collision avoidance and mitigation. When driving at 
nighttime, vehicles in front are generally visible by their 
taillights. Therefore, in this paper, a computational system, 
which is referred to as the dynamic visual system, is 
proposed to detect and analyze the taillights of the vehicles 
in front in spatiotemporal domain, and then extract 
corresponding contextual information. Predefined critical 
contextual information of nearby vehicles can be used for 
driver-assistance systems to convey a warning. Experiment 
from extensive dataset shows that our proposed system can 
effectively extract critical contextual information under 
different lighting and traffic conditions, and thus prove its 
feasibility in real-world environments. 
Keywords: 
Spatiotemporal analysis; Contextual information 
1. Introduction 
Advanced warning system for vehicles is a critical 
issue in recent years for automobiles, especially when the 
number of vehicles is growing rapidly world wide. The 
cost down of general cameras makes it feasible to have an 
intelligent system of visual-based event detection in front 
for forward collision avoidance and mitigation. Intelligent 
systems can be classified into two categories, passive and 
active. Passive systems intend to reduce the damage in an 
accident, and the active ones try to prevent the unexpected 
events [4-7].  
In the literature, some researchers focus on detecting 
vehicles at nighttime [8], traffic lights [9] and tail lights of 
vehicles in front [10]. Most related works are proposed in 
spatial domain. In [8-10], color features are first 
computed and bright regions are detected for further 
consideration of candidate objects. Chen et al. [8] first 
perform bright object segmentation and verify the 
segmented regions if they are the targets by conducting 
the process of spatial clustering using the features of 
shape, texture, and the relative positions, i.e., the 
symmetric property.   
Park and Jeong [9] use color features to extract 
candidate regions and then select candidate pixels that 
may come from signal light from grabbed image based on 
statistical features. Regions remain are examined to see if 
each region were of a shape of circle since there are many 
light sources that have color like signal light. Finally, they 
remove noise to increase reliability by robust filtering. In 
[10], tail lights of vehicles are their concerns. Initially, a 
color filter in HSV color model is first applied. The 
parameters of the red HSV filter and white HSV filter are 
investigated to automatically extracted candidate regions. 
Finally, the aspect ratio of candidate regions is computed 
for filter out noises further. This ensures that similar 
objects with a large distance apart, such as lights from 
vehicles in different lanes, are filtered out of the pairing 
process. In [1], Chen and Lin proposed a taillight analysis 
based system that can detect braking events at nighttime 
in frequency domain. However, extracting contextual 
information from vehicles in front in spatiotemporal 
domain is still a challenging problem due to the cluttered 
dynamic background. Therefore, in this paper, a 
video-based intelligent system that can effectively extract 
critical contextual information is proposed.  
The remainder of this paper is organized as follows. 
Section 2 details the proposed approach of contextual 
information extraction. Section 3 shows the preliminary 
experiment results, and Section 4 concludes this work. 
2. Context Information Extraction 
In this section, we shall describe the proposed 
approach of extracting contextual information of vehicles 
in front. The approach consists of three major steps. In 
Section 2.1, the color space of original video frames is 
1550
2011 IEEE978-1-4577-0308-9/11/$26.00 ©
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
  
The input jv  in the first hidden layer is defined as 
∑
=
=
n
i
ijij xwv
0
                      (6) 
where ix  the input of the ith net and jiw  is its 
corresponding weight. Using the activation function f, we 
can have the neuron output jϕ of the first hidden layer by 
)( jj vf=ϕ                      (7) 
jϕ  is then used as input to the 2nd hidden layer by  
  ∑
=
=
n
i
jkjk wv
1
ϕ                         (8) 
Using f we thus have the output of 2nd layer by 
)( kk vf=ϕ                             (9) 
Accordingly, the output of the 2nd hidden layer is obtained 
by 
∑
=
=
n
k
klkl wO
1
ϕ                           (10) 
The error function in this work is defined by 
∑ −=
k
mOmdE 2)()(
2
1                    (11) 
where )(md  is the expected value of the mth training, 
O(m) is the output of the kth neuron in the mth training. To 
adapt the weighting w, similar to [11] we perform a direct 
adaptation of the weight step based on local gradient 
information. To achieve this, we introduce for each 
weight its individual update-value ∆ , which solely 
determines the size of the weight-update. This adaptive 
update-value evolves during the learning process based on 
its local sight on the error function E according to the 
following learning-rule: 
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
−∆
<
∂
∂
∗
∂
−∂
−∆∗
>
∂
∂
∗
∂
−∂
−∆∗
=∆ −
+
elset
w
tE
w
tEift
w
tE
w
tEift
t
),1(
0)()1(),1(
0)()1(),1(
)( η
η
          (12) 
where +− <<< ηη 10 . Every time the partial derivative 
of the corresponding weight w  changes its sign, which 
indicates that the last update was too big and the 
algorithm has jumped over a local minimum, the value 
∆  is decreased by the factor −η . If the sign of the 
derivative retains, the update-value is slightly increased in 
order to accelerate convergence in shallow regions. If the 
derivative is positive (increasing error), the weight is 
decreased by its update-value, if the derivative is negative, 
the update-value is added by 
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
<
∂
∂∆+
>
∂
∂∆−
=∆
else
w
tEift
w
tEift
tw
,0
0)(),(
0)(),(
)(
                (13) 
where )()()1( twtwtw ∆+=+ . 
However, if the partial derivative changes its sign, the 
previous weight-update is reverted by 
0)()1(),1()( <
∂
∂
∗
∂
−∂
−∆−=∆
w
tE
w
tEiftwtw       (14) 
The update-values and the weights are changed every 
time the whole pattern set has been presented once to the 
network. 
3. Experimental Results 
The extensive test dataset is captured under different 
road environments, and traffic conditions and its 
resolution is 320×240 captured using a CCD camera. The 
setting of the parameters used in our proposed approach is 
illustrated in Table 1.  
Table 1.  The setting of the parameters used in our proposed 
approach. 
Parameters Value 
α 8 
β 2 
γ 8 
wt 10 
For candidate taillight detection, some examples are 
shown in Fig.3. We can see that most noises annotated by 
red bounding boxes can be removed. The regions labeled 
by blue bounding boxes mean that these regions are the 
tracked tail lights. 
1552
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
  
[9] J. H. Park, C. S. Jeong, “Real-time Signal Light 
Detection,” Proc. Future Generation Communication 
and Networking Symposia, 2008. 
[10] R. O’Malley, M. Glavin, E. Jones, “Vehicle 
Detection at Night Based on Tail-Light Detection,” 
Proc. International Symposium on Vehicular 
Computing Systems, 2008. 
[11] M. Riedmiller and H. Braun, “A direct adaptive 
method for faster backpropagation learning: the 
RPROP algorithm,” Proc. IEEE International 
Conference on Neural Networks, San Francisco, 
1993. 
[12] R. E. Kalman, "A New Approach to Linear Filtering 
and Prediction Problems," Journal of Basic 
Engineering, vol. 82, no. 1, pp. 35–45, 1960.
 
1554
Proceedings of the 2011 International Conference on Machine Learning and Cybernetics, Guilin, 10-13 July, 2011
99 年度專題研究計畫研究成果彙整表 
計畫主持人：陳永盛 計畫編號：99-2221-E-155-080- 
計畫名稱：植基於電腦視覺「仿電腦使用者」機器人之原型設計 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
此篇會議論文為
本計畫初階成果
之一。目前此項部
分，已接近完成投
搞國際期刊的版
本。至於所有本計
畫之研究成果，預
計可以經過精進
整理後，再投稿二
至三篇之國際會
議論文與國際期
刊論文。 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
博士後研究員 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
已發表一篇國際會議論文【Y.S. Chen and K.L. Lin, ’Display re-gion segmentation 
from a computer screen image using closed-contour tracking,’ Proc. of International 
Con-ference on Machine Learning and Cy-bernetics, Guilin, Guangxi, China, (July 
10-13, 2011), Vol. 4, 1739-1745, 2011.】此篇會議論文為本計畫初階成果之一。目前此
項部分，已接近完成投搞國際期刊的版本。至於所有本計畫之研究成果，預計可以經過精進
整理後，再投稿二至三篇之國際會議論文與國際期刊論文。 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本人對於這個研究計畫的研究成果持非常高的評價，最主要的是藉著現今科技的技術，
如：電腦硬體、程式平台、攝影設備、機構製造、積體電路、以及網際網路等，所延伸的
發展給現今的人類帶來非常多的便利。 
 
  由於電腦的使用極為普及，造成人在電腦前面操作的時間也越來越長。網際網路的發
展更引領人類凡事藉著電腦網路來幫忙，使得資訊量愈來愈大，造成人類的另一種勞累。
於此機器人科技蓬勃發展之際，如何設計一個「能像人操作電腦」的機器人（其操作「電
腦」，除了透過視覺與機器手外，並無任何有線或無線遂行資料之通訊），具有幫助我們透
過電腦處理網路資訊的能力，是項非常有用也極具挑戰性的研究。 
 
  於焉，本計畫藉由電腦視覺與微電腦控制的相關技術發展一獨立的機器人原型
CU-Robot，具有模仿「電腦使用者」操作電腦的能力。它可以像一般的電腦使用者看著電
腦螢幕，操作電腦的真實滑鼠，以完成所賦予的任務。CU-Robot「操作」電腦的過程除了
