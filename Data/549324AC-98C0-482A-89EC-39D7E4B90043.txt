2 
 
中文摘要： 
 
在第一年年的研究成果中，我們提出一套新的以圖找圖之檢索索技術，不不經過特徵訓
練練，直接從局部的輪輪廓廓線段(edge)特徵比對找出指定物件的位置，並可進一步結
合色彩等其他不不同局部特徵進行行檢索索。我們設計了了與過去不不同的檢索索特徵，透過
轉角位置建構出影像上局部特徵的比對”單位圓”，在此單位圓的基礎下，透過已
被證實具有足夠強健性的 log-polar histogram來來進行行特徵比對，以達到過去 shape 
context所無法實現的物件”偵測”效果。目前我們正對所提出之方法進行行實驗與改
良良，以進一步加強其對複雜背景的容忍程度度與及降降低計算複雜度度。另一方面，我
們也計畫結合色彩等異異質特徵，以提供更更詳盡的檢索索功能。 
 
在第二年年的研究成果中，我們提出透過產生一致前景物件樣板 (Consensus 
Foreground Object Template, CFOT)進而在影片中比對出前景物件區域，以達成在
快速移動攝影機中進行行前景物件偵測目的。一旦前景物件獲得後，相對應的時間
區間聲音軌訊號可從視訊影片檔案中擷取出來來。在本研究中，我們會針對該對應
區間聲音訊號，進行行 MFCC 特徵萃取，對於屬於相同類類別視訊影片進行行聲音模
型訓練練，透過高斯混合模型 Gaussian Mixture Model (GMM)，對於相同物件類類別
的不不同影片，產生分類類器 (classifier)。一旦聲音軌與視訊軌各自獲得對於待測視
訊影片註解後，對於異異質性特徵進行行結合(Multimodal fusion)，以獲得最後的視
訊註解。 
 
在第三年年的研究成果中，我們提出了了一個整合自異異質特徵值域的強健式移動前景
物體偵測方法。我們利利用 SIFT特徵比對並且提出了了一個機率率率式的架構來來建構共
同前景物件樣板(consensus foreground object templates, CFOT)。我們所提出的
CFOT能夠從有興趣的視訊訊框中偵測到移動前景物體，並且從前景區域中萃取
出視訊特徵。此外，我們也利利用了了音訊特徵來來增加視訊註解的正確率率率。在實驗結
果中，我們基於從 Youtube網站上下載的影片資料料庫進行行測試，並且從測試中獲
得了了可靠的實驗結果。 
 
 
 
 
 
 
 
 
 
 
4 
 
第一年年成果 單一特徵影像檢索索 
 
前言 
 
 近年年來來隨著 youtube等視訊分享平台的興起，視訊檢索索越加成為了了研究上的
熱門領領域，紛紛尋求有別於文字外，更更加方便便有效的視訊搜尋方法。另一方面，
對於擁有大量量數數位內容的典藏單位來來說說說，人工的文字註解方式不不僅造成人力力上的
負擔，同時也增添了了閱覽者在搜尋上的限制。如何從資訊龐大的視訊資料料中尋找
出特定的內容，一直都都是相當重要的研究課題。 
 
研究目的 
 
 有鑑於此，本研究計畫希望能開發出具實用性的策略略方法，提供數數位視訊內
容新的檢索索工具，以便便更更有效方便便地從大量量視訊內容中，自由搜尋出想要的內
容。我們期望能透過以圖找圖的方式，偵測並比對出資料料庫中與目標相符的視訊
畫面，免去文字註解的麻煩與缺漏漏。 
 
文獻探討 
 
目前以單一特徵對影像進行行檢索索的技術，已在某些特定領領域取得了了不不錯的成
果。例例如 Shape Context[1,2]、CSS(Curvature Scale Space)[3]常被應用在以物體輪輪
廓廓為主的比對上(如相似商標檢索索、魚類類資料料庫檢索索、人體姿勢判定等)。而 Lowe
於 1999年年提出的 SIFT(Scale-Invariant Feature Transform)[4]特徵，更更能以極高準
確率率率在影像上偵測出與指定物相同的物件。SIFT先在影像上找出特徵點，再透
過高維度度的特徵空間來來對特徵點本身進行行描述，使其能容忍影像本身的縮放與旋
轉變化，並透過一對一的點對應來來找出不不同影像中相同物件的位置。然而，也正
由於 SIFT本身具有的高鑑別率率率，而使得大多數數”相近”的物件容易易遭到濾濾除。舉
例例而言，當檢索索者想在視訊資料料庫中尋找腳踏車車時，SIFT往往只能找出同一部
腳踏車車，而無法檢索索出其他不不同品牌型號的腳踏車車，因此使得檢索索結果的範圍受
到嚴重限制。 
 
相對於以單一特徵對影像進行行檢索索，早期結合多種特徵的視訊檢索索研究，主
要是對不不同的簡單特徵賦予不不同之權重分配[5,6]，再根據累累計所得的特徵總量量來來
決定檢索索結果的近似程度度。由於權重部份需仰賴專家或檢索索者自身決定，而且僅
能以直覺方式衡量量適當權重，導致其結果往往與檢索索者期望之結果相去甚遠。因
此，近年年來來陸陸續有學者提出以人工智慧學習的方式[7,8]，針對單一類類物件進行行特
徵訓練練，以二分法的方式判定影像上是否包含該類類物件。然而，此類類方法雖能以
高準確率率率偵測出特定物件，但卻無法提供檢索索者對各種不不同檢索索標的自由進行行檢
6 
 
   
(a)              (b)  
圖二、轉角位置偵測。(a)原圖；(b)將(a)左旋 22度度、縮小 65% 
 
由於任取兩兩兩兩轉角所形成的圓數數量量會隨轉角數數成平方倍數數上升，因此可以適
當刪除鄰近轉角與及限定 Ri範圍來來降降低 descriptor數數量量。更更可依 Ri大小進行行階
層式的剔選比對，當大範圍的 descriptor不不符比對時，便便無須比對其範圍內的較
小 descriptor，達到提昇比對效率率率的結果。假設標的物在某一對轉角位置的
histogram為 HT，資料料庫中待比對的 histogram為 HD，則我們將 HT與 HD間之相
似度度定義為 Σ(HT∩HD)/ΣHT。圖三顯示的是以圖二(a)中的一個 descriptor對圖二(b)
進行行比對。綠綠線為由兩兩轉角所形成的直線，藍藍線為以該直線為直徑之外接圓，紅
線則為比對最接近之兩兩 descriptor中心點連連線。 
 
圖三、圖二中的一組相似 descriptor對應情況。 
 
在找出具有局部不不變性的描述範圍 Ci後，同樣的概念念亦可用於色彩等其他
不不同局部特徵，對影像進行行異異質描述。但由於前述的線段輪輪廓廓本身是一維的性
質，其他色彩等特徵則是作用於二維的區域，當影像縮放時，圓內的特徵統計總
量量也會隨直徑 Ri成平方倍成長，因此需以 Ri2作為量量化基準。 
 
結果與討論論 
 
 我們設計了了與過去不不同的檢索索特徵，透過轉角位置建構出影像上局部特徵的
比對”單位圓”，在此單位圓的基礎下，透過已被證實具有足夠強健性的 log-polar 
histogram來來進行行特徵比對，以達到過去 shape context所無法實現的物件”偵測”
效果。目前我們正對所提出之方法進行行實驗與改良良，以進一步加強其對複雜背景
的容忍程度度與及降降低計算複雜度度。另一方面，我們也計畫結合色彩等異異質特徵，
8 
 
 
[10] X.  C.  He,  N.  H.  C.  Yung,  “Curvature  Scale  Space  Corner  Detector  with  Adaptive 
Threshold  and  Dynamic  Region  of  Support,  ”  17th  International  Conference  on  
Pattern Recognition, vol. 2, pp.791-794, 2004. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
10 
 
後的影像找出對應的特徵點，對於相同物件的偵測，提供高鑑別率率率結果。 
 
而在異異質性特徵整合(multimodal fusion)部分，1998年年，Shim等人[3]提出將
視訊影片中的文字進行行光學辨識識(Optical Character Recognition, OCR)，以取得對
於該影片的文字註解，1999年年，Brunelli等人[4]提出透過語音文字辨識識提供影片
文字註解，而整合的方式，可以透過時間軸上的同步，在不不同 modalities 
(multimodal)中各自取得的文字註解整合，達到更更有效的視訊註解。 
 
在電視影片分類類當中， Wang[5]等人在 2000 年年提出了了利利用不不同視訊片段在
不不同 modal的註解狀狀態轉移，利利用隱藏馬可夫模型(Hidden Markov Model, HMM)
的訓練練，產生不不同種類類視訊片段分類類器(Classifier)，最終決定視訊註解時，假設
各自 modal為獨立立事件，將各自產生的事件機率率率相乘，得到整合後的視訊註解機
率率率，如此可以相當公平的將聲音軌與視訊軌所得到視訊註解同時考慮，得到最後
的註解結果。 
 
有鑑於此，我們計畫提出一套自動化的視訊註解技術，首先不不經特徵訓練練，
先對視訊內容進行行前景偵測，接著將偵測到的視訊片段進行行聲音特徵萃取，以及
前景物件特徵萃取，最後訓練練出聲音與視訊的物件類類別分類類器，一旦測試影片輸
入時，可以分辨出該影片是屬於那一物件類類別的影片，進而對該影片進行行註解，
以利利於後續的視訊檢索索動作。 
 
研究方法 
 
本研究的系統架構如圖一所示。圖一左側為視訊影片輸入，首要之務是進行行
視訊前景物件偵測，如圖一左側黃色圈選範圍所示，此例例子中飛機為前景物件。
一旦前景物件得以從視訊檔案中偵測得知，其相對應視訊片段中包含前景物件的
時間區間，空間位置與包含區域，皆可獲得。在對應的時間區間中，聲音軌的特
徵可以根據聲音訊號分析得知，而視訊軌的訊號亦可以進行行特徵萃取，如圖一中
間紅色方塊所示。當異異質性特徵從不不同軌中獲得後，可以將獲得特徵進行行整合
(fusion)，整合過後的特徵或者分類類標記會成為最後的視訊文字註解。 
 
12 
 
進而對於不不同物件類類別影片，產生各自分類類器。如圖三所示， 一旦一段測試影
片輸入，經由各種物件類類別分類類器運算，取其最大值，該影片之聲音物件分類類註
解即可得知。 
 
圖三、聲音軌特徵訓練練架構圖 
 
類類似於聲音軌的特徵萃取，視訊軌的特徵萃取，我們採用了了 Yang 等人 [6] 
所提出的方法，以及本研究所提出的前景偵測方法，透過對每張影像中前景物件
的的 SIFT 特徵點萃取，經由 Sparse coding，並經由 linear SVM訓練練，取得各
種不不同類類別影片的特徵向量量，產生物件類類別分類類器。當一段測試影片輸入時，可
以獲得對於不不同物件類類別影片的評估分數數，取具有最大值的分類類器類類別，成為視
訊軌的視訊註解。 
 
一旦聲音軌與視訊軌各自獲得對於待測視訊影片註解後，對於異異質性特徵進
行行結合(Multimodal fusion)，以獲得最後的視訊註解。在本研究當中，針對獲得
的聲音軌與視訊軌特徵，分別會進行行 feature level (圖一下方藍藍色方塊) 以及
classifier level fusion(圖一下方橘色方塊)，而形成最後視訊文字註解。 
 
結果與討論論 
在本計劃中，我們提出了了一嶄新作法產生前景物件樣板，並在原本視訊影片
中偵測出前景物件，除了了在短時間內前景物件改變不不大狀狀況下得以精精確產生前景
樣板外，當觀測時間增長後，前景物件所可能產生的姿態改變，將造成前景物件
樣板模糊化，產生錯誤累累積(error propagation)，進而造成前景物件偵測失敗。目
前我們正針對此問題進行行改良良與實驗測試，此外，我們也進行行程式最佳化，以降降
低運算複雜度度，以及複雜背景測試，使得提出方法得以更更廣泛使用於更更多的測試
14 
 
第三年年成果 網路路視訊的自動化註解 
 
前言 
由於網路路視訊的應用近年年來來的蓬勃發展，舉例例來來說說說，線上視訊分享和和搜索索
的規模越來來越大，網路路視訊的註解議題越來來越值得注意。傳統上，這些應用是基
於相關的標籤註解資訊(tag information)，但是由使用者所提供的錯誤和雜亂亂的註
解標籤會降降低註解，擷取(retrieval)，或者高階應用(像是活動和行行為分析)的系統
表現。因此，發展一套對於網路路視訊資料料的自動化註解技術是相當必要的，而該
技術的成功能夠讓前述所提的應用提升系統表現。 
 
研究目的 
為了了要解決這樣的問題，我們提出了了一個整合自異異質特徵值域的強健式移
動前景物體偵測方法。我們利利用 SIFT特徵比對並且提出了了一個機率率率式的架構來來
建構共同前景物件樣板(consensus foreground object templates, CFOT)。我們所提
出的 CFOT能夠從有興趣的視訊訊框中偵測到移動前景物體，並且從前景區域中
萃取出視訊特徵。此外，我們也利利用了了音訊特徵來來增加視訊註解的正確率率率。 
 
文獻探討 
從非靜態攝影機所拍攝或者低解析度度的視訊中偵測移動物件是一項具有挑
戰性的工作。在[1]中，Sheikh和 Shah提出了了利利用像素顏色和空間結構的綜合表
現以建構前景和背景模型。Patwardhan 等人[2]將場景分層解構並且利利用最大化
可能性技術將像素指定至不不同的分層以達到前景預測。雖然在此論論文中具有吸引
人的結果，然而，該方法只能在少量量的相機位移狀狀況下偵測前景物件。 
 
為了了在非控制的視訊資料料中偵測前景物件，一個傳統的方法是先預估出相機
的全域位移(global motion)，而與全域位移不不相關的區塊，則可被認為是前景物
體所在區塊。Meng和 Chang[3]利利用了了移動向量量場以產生整張視訊訊框的全域位
移。Irani和 Anandan[4]提出了了利利用 2D和 3D參參數數來來偵測移動物件。Wang等人[5]
使用 MPEG 視訊中的移動向量量以預估放大縮小和移動的幾何變換參參數數(affine 
parameter)。然而，在場景中具有雜亂亂背景情況下，或者當物件姿態(appearance)
和尺度度(scale)變化時，預估相機位移仍然是一個具有挑戰性的議題。另一個種類類
的偵測物件演算法則是利利用參參考背景影像模型(reference background  modeling)
進而獲得前景物件。Felip[6]等人從取樣後的運動向量量中預估主要向量量(dominant 
motion)，此外還利利用此主要向量量達成影像間的校正。Zhao等人[7]提出了了在室內
場景中利利用學習場景模型，從非靜態相機中偵測前景物體。此方法基於 SIFT特
徵比對和幾何關係計算而獲得場景模型，然而，這樣的設定對於戶外場景或者複
雜背景情況中，並不不適用。 
 
16 
 
 
圖三，建構建構共同前景物件樣板(consensus foreground object templates, CFOT) 
 
為了了偵測移動前景物件，我們使用 CFOT當作受測影像(query image)對於整
張視訊訊框進行行偵測，而 CFOT會在一段時間(在兩兩兩兩個 CFOT重置區間內)的每
張訊框中搜尋類類似的影像樣型(image pattern)。我們使用的類類似度度比對是基於
SAD對於每張訊框的區域進行行窮舉搜尋(exhaustive search)得到最佳比對結果。依
但該前景區域確定後，前景偵測機制旋即結束，如圖二所示，我們使用方框將前
景區域顯示於訊框中。 
 
一旦前景物兼備偵測到後，對於相對應的時間區間，我們會萃取出聲音特
徵，我們將視訊資料料中的聲音訊號轉換為 19 維度度的美爾頻率率率倒譜係數數
(melfrequency ceptrum coefficients, MFCCs)，基於 32 毫秒(ms)的漢明視窗框架
(Hamming-windowed frame)以及 10 毫秒的位移 (shift)。因為高斯混和模型
(Gaussian mixture model, GMM)廣泛的使用在聲音分類類中[15, 16]，我們對於每個
種類類訓練練一個 GMM，而一段視訊資料料的輸出則是 GMM的根據最大可能性來來決
定該視訊屬於那一個種類類。此外，我們使用基於貝式資訊條件的 SGML 演算法
[17]以決定每個 GMM的個數數。 
 
此外，在視訊特徵方面，在本計畫中，我們利利用 SFIT[12]和方向梯度度直方圖
(histogram of oriented gradients, HOG)描述子(descriptor)[18]以獲取視訊訊框中的
姿態和形狀狀資訊。我們採用高密度度的(dense)SIFT描述子。而針對 HOG描述子，
我們考慮了了均勻分布(uniformly spaced)的高密度度的網格(dense grid)用以萃取梯度度
直方圖。我們並沒有考慮時間空間(space-time)特徵，因為在我們資料料庫裡裡的都都是
移動物件，而向量量資訊並無法提供額外的辨識識能力力。 
 
而在學習演算法中，我們採用了了稀疏編碼(Sparse coding, SC) [19]。為了了針對
SIFT 和 HOG 描述子產生稀疏表示法，我們使用 Mariral 等人[20]所發展的軟體
進行行字典學習(dictionary learning)，並且將相關的特徵描述子進行行編碼。在得到
這兩兩個特徵的編碼稀疏係數數向量量後，我們使用最大池水技術 (max pooling 
technique)以將 SIFT(HOG)描述子轉換為 K維度度的特徵向量量。 
 
結果與討論論 
我們提出了了一個強健性的影像註解方法，能夠自動化決定前景物件區域，並
且預估其類類別。前者是使用我們所提出的共同前景物件樣板(consensus foreground 
object templates, CFOT)以偵測移動物件，而後者這是利利用從不不同值域整合異異質性
18 
 
applications,”  in  IEEE  PAMI  ,  2011. 
[14]  A.  Senior,  “Tracking  People  with  Probabilistic  Appearance  Models,”  in  PETS,  
2002. 
[15]   J.   Shirazi   et   al.,   “Improvements   in   audio   classification   based   on   sinusoidal  
modeling,”  in  IEEE  ICME,  2008. 
[16]   P.   Dhanalakshmi   et   al.,   “Classification   of   audio   signals   using   AANN   and  
GMM,”  in  Applied  Soft  Computing,  2010. 
[17] S.-S. Cheng, H.-M. Wang, and H.-C.  Fu,  “A  model-selectionbased self-splitting 
Gaussian mixture learning with application to speaker identification,”   in  
EURASIP JASP, 2004. 
[18]   N.   Dalal   and   B.   Triggs,   “Histograms   of   oriented   gradients   for   human  
detection,”  in  CVPR,  2005. 
[19]  M.  Elad  et  al.,  “Image  denoising  via  sparse  and  redundant  representations  over  
learned  dictionaries,”  in  IEEE  Trans. Image Processing, 2006. 
[20]   J.   Mairal,   F.   Bach,   J.   Ponce,   and   G.   Sapiro,   “Online   dictionary   learning   for  
sparse  coding,”  in  ICML,  2009. 
 
 
 
 
 
 
into a number of different scales. However, there is no 
systematic way to determine how many scales should 
be chosen to derive a best solution. 
 
3. Proposed method 
 
3.1. The proposed descriptor 
 
In this paper, we propose a new method to deal with 
the object detection problem. First of all, we use Canny 
edge detector [5] to extract the edge map of an image. 
Similar to other contour-based methods [7-9], the 
redundant edges in an image may significantly affect 
the accuracy and computation cost. Hence, in the first 
step we try to eliminate isolated short edges and link 
those edges if the distance between the end points of 
two neighboring edges is short.  
Once the edge map of an image is extracted, we 
adopt the algorithm proposed by He and Yung [10] to 
detect the HCPs from the extracted 2D edges. It has 
been pointed out in [11] that an HCP plays an 
important role in the recognition process of a human 
visual system. According to the theories reported in 
cognitive psychology, human beings can easily 
recognize an object by taking only the HCPs and 
joining them with straight line segments [11]. 
 
Figure 1. Green dashed line is the diameter 21PP . 
Red edge segments show the edge pixels within 
the descriptor H12. 
 
To describe the related spatial distribution of 
edge(s), we use HCPs to determine the range and 
orientation of different descriptors. For two different 
HCPs, P1 and P2, we define a circular range C12 whose 
diameter is 21PP . Figure 1 shows a polar coordinate 
mask that can be used to calculate the spatial 
distribution of edge pixels within C12. For comparing  
the descriptors obtained at different scales, the number 
of pixels contained in each bin needs to be normalized 
by a corresponding diameter.  
Considering the requirement of rotation invariant 
for some applications, one can simply use the 
corresponding diameter 21PP  to adjust the orientation 
of the first bin of the corresponding mask towards P1 or 
P2. Figure 2 shows the two different masks (180ш
apart) that will result in two different polar-histograms, 
H12 and H21, respectively. If the orientation is not an 
issue, we do not need to rotate any descriptor to make 
distinction between top and bottom, or left and right. 
 
Figure 2. Rotation invariant can be achieved by 
using two 180шapart polar coordinate masks. 
 
Since the number of potential descriptors is 
proportional to the square of the number of HCPs, 
redundant HCPs have to be removed to save 
computation time. We consider those HCPs that reside 
on the same edge and stay within a neighborhood 
distance TL as potential redundant HCPs. For each 
neighborhood we choose the HCP that has the 
maximum curvature magnitude as a representative 
HCP. Through this screening process, we can remove 
redundant HCPs. To restrict the size of a circular mask, 
we set the maximum allowable diameter to be TH.  
Since the value of TH will influence the maximum 
detectable size of objects in a real image, we need a 
strategy to deal with the situation when the distance 
between two HCPs is larger than TH , we apply multi-
scale sampling to convert an original image into 
different sizes. When at a specific resolution the largest 
distance between two HCPs is larger than TH, one can 
take an image with smaller size to execute the 
algorithm. One thing to be noted is that the purpose of 
our multi-scale sampling process is to enhance the 
efficiency of the annotation and detection task. This is 
quite different from multi-scale sampling approaches 
that are designed to deal with the scaling problem. 
To perform object detection in a cluttered image is 
very difficult due to several reasons. First, the cluttered 
background may severely interfere with the detection 
task. Second, in many cases the target objects to be 
detected may not look homogenous from their 
appearance. Their constituent components may have 
very different textures on the same object. Under these 
circumstances, for a descriptor formed by two chosen 
HCPs, only the edge pixels residing on the edges that 
cover the two HCPs will be considered valid and 
counted into the corresponding polar histogram. Since 
our method is very general, a large number of 
descriptors may be involved. To save computation time, 
some descriptors whose entropies are lower than a 
threshold, TE, should be discarded. For example, when 
the two HCPs of a descriptor belong to two objects at a 
distance or they share only a straight edge inside the 
77167
variability, and scale changes. In the experiments, we 
only chose the models of three object categories out of 
the five since there are only very few HCPs in the 
models of the other two object categories. Figure 4 
shows the comparison of the performances among 
chamfer matching [1], Ferrari et al. [7] and our 
method. It is clear that our method received higher 
detection rate with fewer false positives. In other 
words, our method has less false detections in higher 
ranked results. This characteristic can help people 
search more accurate returned instances from the top 
tens of responding detection results. Figure 5 shows 
part of the successfully detected instances through 
searching the ETHZ dataset. In these cases, the 
positions and the ranges of the target objects were 
precisely located. Note that some of the detected 
shapes were deformed and some of them were 
interfered with a cluttered background. 
Figure 6 shows some unsuccessful cases. 
According to our observation, most of the missing 
detections were caused by the undetected HCPs and 
poor edge detection result. Sometimes over smoothed 
edge boundaries may reduce the number of detectable 
HCPs, such as the left column of Figure 6. In the 
middle column and the right column of Figure 6, 
fragmental edges and missing edges are also the causes 
of miss detection.  
 
Figure 5. Some successfully detected results based 
on the ETHZ dataset. 
 
5. Conclusion 
 
In this paper, we propose a new object detection 
method that does not need to introduce a learning 
mechanism. To ensure the invariance with respect to 
RST, we utilize HCPs on edges to determine the region 
of a polar histogram and counting the related edge 
pixels inside. Since we only consider one or two edges 
in each local descriptor, partial matching can be 
efficiently performed in a cluttered image. Finally, we 
locate the target objects in real images by a voting 
process. The experiment results show that our 
proposed method outperforms the existing state-of-the-
art work. 
 
Figure 6. Some unsuccessfully cases. The upper 
row shows three undetected cases and the bottom 
row shows their corresponding Canny edge maps. 
 
Acknowledgement 
 
This research was supported in part by Taiwan E-
learning and Digital Archives Programs (TELDAP) 
sponsored by the National Science Council of Taiwan 
under NSC Grant: NSC99-2631-H-001-020. 
 
References 
 
[1] D. M. Gavrila and V. Philomin, “Real-time object 
detection for smart vehicles,” in IEEE International 
Conference on Computer Vision, 1999. 
[2] H. J. Wolfson, and I. Rigoutsos, “Geometric Hashing: An 
Overview,” IEEE Computational Science and 
Engineering, vol. 4, no. 4, pp. 10-21, 1997. 
[3] D. H. Ballard. “Generalizing the hough transform to 
detect arbitrary shapes,” Pattern Recognition, vol. 13, no. 
2, pp. 111–122, 1981. 
[4] D. G. Lowe, “Object recognition from local scale-
invariant features”. Proceedings of the ICCV, vol. 2. pp. 
1150–1157, 1999. 
[5] J. Canny, "A Computational Approach To Edge 
Detection," IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 8, pp. 679-714, 1986. 
[6] T. Hastie, R. Tibshirani and J. Friedman, “14.3.12 
Hierarchical clustering,” The Elements of Statistical 
Learning (2nd ed.). Springer, pp. 520–528. 2009. 
[7] V. Ferrari, T. Tuytelaars, and L.J. Van Gool. “Object 
detection by contour segment networks.” In Proceedings 
of the European Conference on Computer Vision 
(ECCV), vol. 3, pp. 14–28, 2006. 
[8] P. Felzenszwalb and J. Schwartz, “Hierarchical matching 
of deformable shapes.” In IEEE onference on Computer 
Vision and Pattern Recognition, pp. 1–8, 2007. 
[9] Q. Zhu, L. Wang, Y. Wu and J. Shi, “Contour Context 
Selection for Object Detection: A Set-to-Set Contour 
Matching Approach,” ECCV, vol. 2, pp. 774-787, 2008. 
[10] X. C. He and N. H. C. Yung, “Curvature Scale Space 
Corner Detector with Adaptive Threshold and Dynamic 
Region of Support,” in Proceedings of the 17th 
International Conference on Pattern Recognition, vol. 2, 
pp. 791-794, Aug. 2004. 
[11] J. Feldman, M. Singh, “Information along contours and 
object boundaries”, Psychological Review, vol. 112, no. 
1, pp. 243-252, Jan. 2005. 
77369
Data-Driven 
 Consensus Foreground 
Object Generation 
Fine-Tuned 
Foreground 
Object Detection 
Figure 1: The framework of the data-driven foreground object
detection method.
Feature Extraction by 
SIFT 
Foreground 
Feature Point 
Decision 
Foreground Region 
Estimation 
Consensus Foreground 
Object Template 
Generation 
Foreground Object 
Probability and 
Averaged Foreground 
Image Derivations 
Figure 2: Detailed ﬂow chart of the proposed data-driven con-
sensus foreground object generation process.
2 Data-Driven Foreground Object Detec-
tion
Fig. 1 shows the framework of the proposed data-
driven foreground object detection scheme: consensus
foreground object template generation and foreground
object detection. The image sequence shown on the left
of Fig. 1 represents the input video data. The image
at the middle represents the generated consensus fore-
ground object template (CFOT) which is a compact rep-
resentation of the foreground object. Once a CFOT is
obtained, it can be used to guide the foreground object
detection and tracking in the subsequent frames. Fig.
2 shows a detailed ﬂow chart of the generation of con-
sensus foreground object template. The implementation
details of each step shown in Fig. 2 will be discussed in
the subsequent sections.
2.1 Consensus Foreground Object Template
Generation
2.1.1 Feature Point Extraction by SIFT
Scale-invariant feature transform (SIFT) feature detec-
tor is adopted to extract the feature points in each frame,
due to its powerful capabilities in computer vision ﬁeld.
For each pair of corresponding SIFT feature points, a
motion vector can be obtained. Some motion vector ex-
amples are depicted by red arrows in Fig. 3 (a).
2.1.2 Foreground Feature Point Decision
We assume that a set of feature points can be detected
from each frame. Let Nk represent the total number of
detected feature points in frame k. When a set of Nk
motion vectors is obtained from two adjacent frames k
(a) (b) 
q 
hist  (q) k 
Quantization 
Index 
Figure 3: An foreground detection example: (a) the motion
vectors are in red arrows, the detected foreground SIFT fea-
ture points are depicted by green circles, and the determined
foreground region is shown by a green rectangle, and (b) the
histogram obtained corresponding to this image frame.
and k−1, the next task is to identify foregroundmotion
vectors. Here, a vector clustering algorithm is intro-
duced to perform foreground motion vector extraction.
One thing to be noted is that every motion vector has its
own direction and magnitude. According to our obser-
vation, if a camera aims to trace and to obtain a close-up
shot of the target object, magnitudes of the motion vec-
tors to distinguish the foreground object from the back-
ground scene is more robust than using the directions of
the vectors. Next, a quantization process applied on the
magnitudes of the motion vectors is used to quantize all
vectors into a small number of classes.
After the quantization process, for each frame k, a
histogram histk should be determined, as shown in Fig.
3(b). Next, in our observation, the most probable fore-
ground object in a frame should have motion vectors
corresponding to the bin containing the maximum value
in the histogram. Therefore, the foregroundmotion vec-
tors can be determined. In addition, an average fore-
ground motion vector, denoted as vk = ("xk,"yk),
for each frame k is calculated for later position calibra-
tion process.
2.1.3 Foreground Region Estimation
When the foreground feature points can be determined,
we shall describe how to use the positions of the fore-
ground feature points to determine the foreground re-
gion in a frame. Let p = (x, y) denote the centroid of
feature points in Pk (positions of the feature points at
frame k). The difference values calculated by (x− xn)
(xn is the x component of the n-th point in Pk) for all
feature points in Pk can be assumed to form a Gaussian
distribution with zero mean and a standard deviationσx.
Similarly, for the y components, the associated standard
deviation σy can be obtained.
According to the property of Gaussian distribution,
about 95% of the feature points should locate within the
rectangle deﬁned by two corner positions: the upper left
corner (x − 2σx, y − 2σy) and the bottom-right corner
3046584
(a) (b) (c) (d) 
FOP 
SAD 
Detected 
Foreground 
Object 
k 
I k 
v , n P , k 
R k and 
(e) 
CFOT 
Figure 4: Dark Car Sequence: (a) frame 1672, and (b) frame
1682 (c) frame 1692, (d) frame 1702, and (e) generated CFOT.
(a) (b) (c) (d) 
FOP 
SAD 
Detected 
Foreground 
Object 
v , n 
k 
I k 
P , k 
R k and 
(e) 
CFOT 
Figure 5: Bright Car Sequence: (a) frame 12435, and (b)
frame 12440 (c) frame 12445, (d) frame 12550, and (e) gen-
erated CFOT.
Therefore, the camera, foreground object, and back-
ground are all moving with relatively high-speed mo-
tions. Fig. 4(a)-(d) show the experimental results of
Dark Car sequence at different time instants with in-
creasing order. The detected motion vectors (in red ar-
rows), foregroundSIFT feature points (in green circles),
and estimated foreground regions (enclosed by green
rectangles) are shown in the ﬁrst row. Next, the inter-
mediate foreground object probability and the averaged
foreground image are illustrated in the second and third
rows, respectively. After the consensus foreground ob-
ject generation process, the CFOT can be obtained, as
illustrated in Fig. 4(e). Subsequently, the CFOT is used
for precise foreground object detection. The SAD cal-
culated results are shown in the forth row of Fig. 4. Fi-
nally, a more precise foreground object region in each
frame were identiﬁed, as shown by the green rectangles
in the last row. Similarly, the detection results of Bright
Car sequence are shown in Fig. 5.
Finally, we may notice that although the estimated
foreground regions at the initial stage were imperfect
(as shown as the green rectangles in the ﬁrst rows of Fig.
4 and Fig. 5), the ﬁnal results (i.e., the green rectangles
in the last rows of Fig. 4 and Fig. 5) are very precise.
4 Conclusions
In this paper, a novel data-driven foreground object
detection technique was proposed. The developed tech-
nique equipped with the following capabilities: (1) it is
able to detect the foreground object in a video captured
from a fast moving camera ; (2) it can detect a fore-
ground object from a low contrast (spatially/temporally)
area; and (3) it can detect a foreground object from a
dynamic background. The trifold contributions of our
method are: (1) a new foreground region estimation for
identifying the foreground SIFT feature points was pro-
posed; (2) a novel foreground object probability genera-
tion along with a calibration method for dealing the im-
perfect foreground region estimation problem; and (3) a
novel consensus foreground object template for detect-
ing the foreground object in every video frame.
References
[1] C. Wren, A. Azarbayejani, T. Darell, and A. Pent-
land, “Pﬁnder: Real-Time Tracking of Human
Body,” IEEE Trans. Patt. Analy. Mach. Intell., Vol.
19, No. 7, pp. 780-785, 1997.
[2] C. Stauffer and W. Grimson, “Learning Patterns of
Activity Using Real Time Tracking,” IEEE Trans.
Patt. Analy. Mach. Intell., Vol. 22, No. 8, pp. 747-
767, 2000.
[3] J. Meng and S. F. Chang, “CVEPS - A Compressed
Video Editing and Parsing System,” Proc. ACM
Intl. Conf. Multimedia, pp. 43-53, 1997.
[4] R. L. Felip, L. Barcelo, X. Binefa, and J. R.
Kender, “Robust Dominant Motion Estimation Us-
ing MPEG Information Sports Sequences,” IEEE
Trans. Circuit System for Video Technology, Vol.
18, No. 1, pp. 12-22, 2008.
[5] M. Oshima and Y. Shirai, “Object Recognition Us-
ing Three-Dimensional Information,” IEEE Trans.
Patt. Analy. Mach. Intell., Vol. 5, No.4, pp. 353-
361, 1983.
[6] D.G. Lowe, “Object recognition from local scale-
invariant features,” Proc. IEEE International Con-
ference on Computer Vision, pp. 1150-1157, 1999.
[7] A. Senior, “Tracking People with Probabilistic Ap-
pearance Models,” Proc. IEEE Performance Eval-
uation of Tracking and Surveillance, pp. 48-55,
2002.
[8] Y.Q. Shi and H. Sun, “Image and Video Compres-
sion for Multimedia Engineering: Fundamentals,
Algorithms, and Standards,” CRC Press LLC, pp.
407-408, 1999.
[9] http://www.youtube.com
[10] http://www.topgear.com/uk/
30486056
Fig. 2. Our framework for foreground object detection.
each image frame. Irani and Anandan [4] proposed to detect
moving objects using 2D and 3D parameters. Wang et al. [5]
used motion vectors in MPEG videos to estimate affine pa-
rameters for zooming and translation. Nevertheless, estima-
tion of camera motion is still a challenging task due to back-
ground clutter present in a scene, or the appearance and scale
variations of the foreground objects, etc. Another category
of object detection algorithms is to model a reference back-
ground image. Felip et al. [6] estimated the dominant motion
from the sampled motion vectors, and the alignment based
on inter-image homography can be achieved by the dominant
motion. Zhao et al. [7] proposed to detect objects from videos
captured by a non-static camera in an indoor scene with an in-
crementally learned scene model. Their method is based on a
matching scheme using SIFT features and homography calcu-
lation. However, this setting may not be practical for videos
captured in outdoor scenes or with complex backgrounds.
Due to limited quality of Web video data, recent work on
Web video classification typically considers the use of mul-
tiple types of features. Most prior methods utilized various
static features such as appearance, color, etc. of each frame,
while some also considered space-time features to extract mo-
tion information [8, 9]. Methods which combine features
from different domains (e.g. text in [8, 10] and audio in [10])
also exist. However, as pointed out in [11], most existing
approaches on video classification cannot be easily general-
ized to applications of web video clips. Due to low qual-
ity and diversity of Web videos, the direct use of web-based
data could dramatically degrade the performance of the de-
signed algorithm. Moreover, they cannot be easily extended
to address video annotation problems. As a result, it usu-
ally requires some preprocessing techniques to obtain refined
data/features to improve the performance of Web video clas-
sification/annotation. For example, a hierarchical taxonomy
structure was proposed in [10] to alleviate noisy data, and in
[8], the authors pruned the motion features using spatial and
temporal statistics. In our work, in order to reduce the effect
of camera motion and cluttered background information, we
present a unique way to extract the region of interest for the
foreground object (Section 2), followed by the integration of
features from multiple domains (Section 3).
3. FOREGROUND OBJECT DETECTION
Our method for foreground object detection consists of two
major steps: construction of the consensus foreground object
template (CFOT) and its use for object detection. Figure 2
illustrates the framework, and Figure 3 shows a detailed flow
chart of the construction of CFOT.
3.1. Consensus Foreground Object Template (CFOT)
3.1.1. Foreground region estimation
Scale-invariant feature transform (SIFT) [12] is a popular
computer vision algorithm, which is used to detect local in-
terest points in an image, and to describe the associated ap-
pearance information. As an initial stage of our foreground
feature point extraction, we apply the SIFT feature detector
in each frame of a video sequence.
For each pair of corresponding SIFT feature points in ad-
jacent frames, a motion vector can be calculated. Assuming
that the motion vectors extracted frommoving foreground ob-
jects are significantly different from those from background
clutter, we apply a vector clustering algorithm to perform ini-
tial foreground region extraction. A quantization process ap-
plied on the magnitude of the motion vector is used to catego-
rize the motion vectors into a small number of classes. Each
motion vector is assigned with a corresponding quantization
index, and a histogram of these indices is calculated. For this
histogram, the bin with the maximum value is identified as
foreground, and the associated SIFT interest points are con-
sidered as initial foreground points. More specifically, let qˆt
denote the quantization index of the histogram histt at frame
t with the maximum value, and thus qˆt satisfies:
qˆt = arg max
q∈{1,2,··· ,Q}
(histt(q)) , (1)
whereQ represents the number of quantization categories (we
chose Q = 6, and did not observe the results will be sensitive
to this choice). Finally, a set of foreground interest points at
frame t , denoted by Ft, can be defined as follows:
Ft = {fi : g(vi) = qˆi and i ∈ {1, 2, . . . , Nt}}, (2)
where vi is the motion vector of feature point fi, g(·) is the
quantization procedure for vi, andNt is the number of motion
vectors obtained from two adjacent frames.
Once the foreground interest points are obtained, we de-
fine a candidate foreground region Rt according to the spatial
distribution of Ft with a Gaussian distribution assumption.
More specifically, let (x, y) denote the centroid of the fore-
ground SIFT points, and σx and σy represent the correspond-
ing standard deviation, we thus use the upper left corner (x−
2σx, y−2σy) and the bottom-right corner (x+2σx, y+2σy)
to set the boundary of Rt. We note that the recently proposed
SIFT flow [13] also advocates SIFT matching for determining
We do not use the salient SIFT descriptors (with local interest
points detected at different image scales), since they cannot
sufficiently describe the objects which are relatively small or
with low contrast in an image. We also found that such a
small percentage of local descriptors cannot provide adequate
descriptive information for multi-class object categorization
problems. Therefore, we choose to extract dense SIFT de-
scriptors. Although the color rgSIFT descriptors which add
appearance features from R and G color channels have been
shown to work well in several applications, we did not find
them useful in recognizing artificial objects with large vari-
ations of color (as the objects in our dataset do). Therefore,
we do not use color salient descriptors. As for the HOG de-
scriptors, we consider a dense grid of uniformly spaced cells
and extract gradient histograms. We did not consider space-
time features, since the foreground objects in our dataset are
all moving objects, and thus motion information does not pro-
vide any additional discriminating ability.
4.2.2. Learning sparse feature representation
Sparse coding (SC) has been shown to be an effective tech-
nique in many vision tasks [19]. To produce sparse represen-
tation for both SIFT and HOG descriptors, we use the soft-
ware package developed by Mairal et al. [20] to learn the
dictionaries (one for each feature), and to encode the associ-
ated feature descriptor. The parameter λ, which controls the
sparsity of the encoded coefficient vector, is set to 0.2 in our
experiments. The average and the maximum number of non-
zero elements in the encoded coefficient vectors are 4.87 and
18 for SIFT, and 5.43 and 16 for HOG (both out ofK = 225,
which is the size of the dictionary). After obtaining the en-
coded sparse coefficient vectors for both features, we use the
max pooling technique to covert the SIFT (HOG) descriptors
from each frame into aK-dimensional feature vector.
5. EXPERIMENTAL RESULTS
5.1. Web Video Dataset
We collect a complex, uncontrolled, and challenging Web
video dataset from YouTube for our experiments. The video
data are all captured by moving or shaky cameras, and the
moving object of interest are present in cluttered background.
Significant scale and viewpoint variations of the objects can
be observed, and the resolution of a large portion of videos
in our dataset is low. We consider six different moving ob-
ject categories: Airplane, Ambulance, Car, Fire Engine, He-
licopter, and Motorbike. Each object category has 25 to 30
video sequences, and each sequence has one moving fore-
ground object present in it. We randomly select 10 from
each class for training, and the remaining for testing. Fig-
ure 5 shows examples video frames of each object category
in our dataset. We note that, for audio classification, in or-
der to achieve comparable audio classification results as prior
Airplane
Ambulance
Car
Fire Engine
Helicopter
Motorbike
Fig. 5. Example videos in our dataset.
Table 1. Results of audio classification. MAP = 59.40%
Airplane Ambulance Car FireEngine Helicopter Motorbike
Airplane 50.00% 0% 22.22% 5.56% 16.67% 5.56%
Ambulance 0% 80.00% 10.00% 10.00% 0% 0%
Car 18.75% 0% 43.75% 6.25% 18.75% 12.50%
FireEngine 0% 5.56% 11.11% 83.33% 0% 0%
Helicopter 10.53% 0% 21.05% 0% 52.63% 15.79%
Motorbike 0% 0% 26.67% 6.67% 20.00% 46.67%
work using MFCC features, we further consider the use of
auxiliary training audio data, which are also collected from
YouTube. This additional video training data for audio clas-
sification does not necessarily have objects of interest visi-
ble, and we do not use this set of data for visual classifica-
tion either. Nevertheless, none of the above training data is
present in our test set, and we only use extracted audio and
visual features to train the associated classifiers and to per-
form feature/classifier-level fusion.
For our visual features, SIFT descriptors are extracted
from 16 × 16 pixel patches of an image, and the spacing
between adjacent patches is 6 pixels (horizontally and ver-
tically). HOG descriptors are extracted from each 8× 8 pixel
grid, and only one scale in an octave of the pyramid is used.
We note that we resize the longer side of the image to 300
pixels if its width or height exceeds 300 pixels for both de-
scriptors; this is to preserve the aspect ratio of each image.
5.2. Results of audio and visual classification
Table 1 shows the confusion matrix and the mean average
precision (MAP)for audio classification. The extraction of
MFCC audio features and the use of GMM classifiers were
detailed in Section 4.1. As shown in Table 1, we achieved
MAP = 59.4%, while objects Ambulance and Fire Engine
were with better classification results (> 80%).
To classify video data using either SIFT or HOG features,
we sub-sample 20 frames from each of the test video sequence
Fig. 6. Example annotation results. The convex hulls shown
in different colors are determined by our CFOT, and each
color corresponds to the associated object class label.
Audio+SIFT+HOG) were not the best in Table 4, while the
fusion of two visual features generally produced the small-
est improvements. This is expected, since rather than adding
features from the same domain or increasing the number of
features for fusion, integration of heterogeneous features from
multiple domains is expected to provide complementary infor-
mation for improved performance, as supported by our empir-
ical results (e.g. Audio+HOG or Audio+SIFT).
Finally, we show some of our video annotation examples
in Figure 6. Figure 6(a) illustrates an excellent annotation
result, which predicted the correct class label with a perfect
CFOT determined. We note that the helicopter in Figure 6(b)
cannot be successfully recognized by either visual feature;
this is probably because we do not have a large number of
front-view helicopter videos in our dataset). However, with
both feature and decision-level strategies, a correct annotation
result was obtained. Figure 6(c) is a challenging video, since
both the foreground object (aircraft) and background clutter
(e.g. sky, cloud, etc.) are present, and the contrast between
them is nominal. Similarly, its class label was successfully
predicted using the combination of both visual and audio fea-
tures, while the use of either feature did not produce a correct
output. These examples again verify the effectiveness of our
approach, which utilizes the significance of integrating het-
erogeneous features for improved video annotation.
6. CONCLUSION
We proposed a robust video annotation method which auto-
matically determines the region of the foreground object and
predicts its class label. The former was done using our con-
sensus foreground object template (CFOT) for moving object
detection, and the later was achieved by the integration of het-
erogeneous features from different domains. In this work, we
especially focused on the challenging task ofWeb video anno-
tation, in which most existing Web videos are captured under
uncontrolled environments, with insufficient quality or lim-
ited tag information available. Unlike prior sliding window
or object detector based methods, we do not require pixel-
level ground truth data for training; instead, only the label of
each video is utilized, which is especially practical for Web
video applications. In our experiments, we collected a Web
video dataset with only label information as ground truth. We
verified that our CFOT is able to identify the foreground re-
gion of interest, while our proposed framework provides tag
information (class label) using feature and decision-level fu-
sion techniques. We observed a significant improvement in
recognition (annotation) accuracy using our method. In future
work, we expect to extend our CFOT framework for further
high-level vision tasks such as activity or event recognition
using Web videos.
Acknowledgements This work is supported in part by
National Science Council of Taiwan via NSC 99-2221-E-001-
020 and NSC 100-2631-H-001-013.
7. REFERENCES
[1] Y. Sheikh and M. Shah, “Bayesian modeling of dynamic scenes
for object detection,” in IEEE PAMI, 2005.
[2] K. A. Patwardhan et al., “Robust foreground detection in video
using pixel layers,” in IEEE PAMI, 2008.
[3] J. Meng and S. Chang, “CVEPS - a compressed video editing
and parsing system,” in ACM Multimedia, 1996.
[4] M. Irani and P. Anandan, “A unified approach to moving object
detection in 2D and 3D scenes,” in IEEE PAMI, 1998.
[5] R. Wang, H.-J. Zhang, and Y. Zhang, “A confidence measure
based moving object extraction system built for compressed do-
main,” in IEEE ISCAS, 2000.
[6] R. L. Felip, L. Barcelo, X. Binefa, and J. R. Kender, “Robust
dominant motion estimation using mpeg information in sport
sequences,” in IEEE CSVT, 2008.
[7] Y. Zhao, M. Casares, and S. Velipasalar, “Continuous back-
ground update and object detection with non-static cameras,” in
IEEE AVSS, 2008.
[8] J. Liu, J. Luo, and M. Shah, “Recognizing realistic actions from
videos “in the wild”,” in CVPR, 2009.
[9] L. Duan, D. Xu, I. W. Tsang, and J. Luo, “Visual event recog-
nition in videos by learning from web data,” in CVPR, 2010.
[10] Z. Wang, M. Zhao, Y. Song, S. Kumar, and B. Li, “Youtubecat:
Learning to categorize wild web videos,” in CVPR, 2010.
[11] S. Zanetti, L. Zelnik-Manor, and P. Perona, “A walk through
the web’s video clips,” in CVPR Workshop, 2008.
[12] D. Lowe, “Object recognition from local scale-invariant fea-
tures,” in PETS, 1999.
[13] C. Liu et al., “SIFT flow: dense correspondence across differ-
ent scenes and its applications,” in IEEE PAMI , 2011.
[14] A. Senior, “Tracking People with Probabilistic Appearance
Models,” in PETS, 2002.
[15] J. Shirazi et al., “Improvements in audio classification based
on sinusoidal modeling,” in IEEE ICME, 2008.
[16] P. Dhanalakshmi et al., “Classification of audio signals using
AANN and GMM,” in Applied Soft Computing, 2010.
[17] S.-S. Cheng, H.-M. Wang, and H.-C. Fu, “A model-selection-
based self-splitting Gaussian mixture learning with application
to speaker identification,” in EURASIP JASP, 2004.
[18] N. Dalal and B. Triggs, “Histograms of oriented gradients for
human detection,” in CVPR, 2005.
[19] M. Elad et al., “Image denoising via sparse and redundant rep-
resentations over learned dictionaries,” in IEEE Trans. Image
Processing, 2006.
[20] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary
learning for sparse coding,” in ICML, 2009.
 2
三、攜回資料名稱及內容 
 ICIP2010 會議論文集光碟片一片 
 
conventional model-based action prediction methods [7] need a 
training process to achieve the same goal.  
 
2. OBJECT INPAINTING USING ACTION 
PREDICTION METHOD 
 
A. Posture synthesis 
The problem of insufficient posture number would affect the 
visual quality of any video sequence generated by an action 
prediction-based approach. To solve the short-of-postures problem, 
we use our previous posture synthesis method [5] to enrich the 
number of postures. The main concept of a posture creation 
process is to combine the constituent parts of different available 
postures to enrich the contents of the posture database. Therefore, 
the first process is to perform appropriate segmentation on the 
postures in the database. To do a better posture segmentation job, 
we need to know the amount and speed that each component of a 
posture moves. For a component that moves significantly and 
faster, we need to take more intermediate postures to interpolate 
the gap generated by missing frames. Taking any two postures 
from the posture database, we use a bounding rectangle to bound 
each posture first. Then, we align these two bounding rectangles 
(including orientation and scale) as indicated in the middle part of 
Fig. 1. Then, we take the difference between these two postures 
and project these differences onto the y-axis as indicated at the 
right side of Fig. 1. To detect which parts of a human body move 
significantly and speed, one has to calculate the differences 
between a posture and all other database postures. These posture 
differences are all projected onto the y-axis and the accumulated y-
axis component will be like the distribution shown at the right 
hand side of Fig. 2. From the peaks and valleys of the projected 
distribution, one can segment properly a posture as indicated by 
the posture sequence shown in Fig. 3. From the segmented 
components of a posture, new postures can be synthesized by 
combining constituent components as shown in Fig. 4. 
 
Fig. 1. Project posture differences onto the y-axis. 
 
Fig. 2. Project all the differences between any two postures onto 
the y-axis. 
 
Fig. 3. The constituent components of a posture are partitioned 
based on local variance extraction.  
 
B. Graphical model construction 
After synthetic posture creation, the posture database will have 
much more number of postures. These postures can be used to 
build the graphical model (as shown in Fig. 5) of an object action. 
A graphical model provides a simple representation of an object 
action. To obtain the graphical model of an object action, we 
project all postures (including synthetic and existing postures) onto 
a feature space. Then, we link those postures that appear in 
adjacent frames in the constructed feature space. After applying 
the above procedure, we can obtain a graphical representation of 
an object action. To model the distribution of postures in the 
feature space, we need to know the distances between distinct 
postures. We use the shape context descriptor [9] to make a 
detailed description of a posture. We calculate the value of shape 
context along the silhouette of a posture. Later these shape 
contexts will be used to compare the degree of similarity between 
two distinct postures.  
 
Fig. 4. A new posture is composed of three components (head, 
body, and legs). 
 
Fig. 5. The graphical model of an object action in low 
dimensional manifold.  
 
(a) (b) (c) 
Fig. 6. Extracting the local context of a posture: (a) the object’s 
original posture; (b) the object’s silhouette described by a set of 
feature points; and (c) a shape context mask on a feature point. 
To calculate the shape context, the silhouette of a posture 
needs to be represented as a set of sampled points 1 2{ , ... }nP p p p=  
(as indicated in Fig. 6(b)). For each sampled point ip P∈ , a 
corresponding local histogram is computed in a log-polar space (as 
indicated in Fig. 6(c)) to represent the local shape context of pi. 
The cost of matching two different sampled points which belong to 
two different postures can be defined as follows 
2
1
( ) ( )1( , )
2 ( ) ( )
i j
i j
K p q
i j
k p q
h k h k
D p q
h k h k=
⎡ ⎤−⎣ ⎦= +∑ ,                    (1) 
where ( )
ip
h k and ( )
jq
h k  denote the k-th bin of the two sampled 
points pi and qj, respectively. The best match between two 
different postures can be accomplished by minimizing the 
following total matching cost: 
2 
i
n 
j 
1
computing the probability during inference. The data point 
estimated at node j is 
1 1arg  max ( )
j
j j
j j j j
c
c p c M M− +=) ,                        (5) 
where jc  is the candidate point associated with node j, ( )jp c  
is the self probability of candidate point jc , and 
1j
jM
+  is the 
message from node j-1 to node j. 1jjM
+  can be calculated as 
follows: 
1 2
1 2 1 1 1[ ]
max ( , , ) ( )
k
j j j
j j j j j j jc
M c c c p c M M+ ++ + + + += Ψ % % ,               (6) 
where 1jjM +%  is the previous message that can be used to 
generate 1jjM +  through executing Eg.(7). 1jjM +  includes the 
probability information of all candidate data points of node k. 
The initial 1`jjM +%  is set as a column vector with all 1s. The 
function 1 2( , , )j j jc c c+ +Ψ  is defined as follows:  
 
2
1 2 2
1 ( )( , , ) exp( )
22j j j
uc c c θ σσ π+ +
−Ψ = −                      (8) 
where θ  is the angle between line 1j jc c +  and 1 2j jc c+ + , u  and σ  
are the mean and variance of all angles in a complete trajectory 
of an object action 
 
3. EXPERIMENT RESULT 
 
To test the effectiveness of the proposed action prediction 
method, we used several test sequences to evaluate the efficacy of 
the proposed method. However, we only use one sequence to 
demonstrate the power of our approach. The sequence was 
captured by a commercial digital camcorder with a frame rate of 
30 fps, and a resolution of 352×240 (SIF). In the experiments, we 
first removed several consecutive frames to simulate a real-world 
situation in which objects in a number of consecutive frames are 
damaged due to packet loss during transmission of the video or due 
to a damaged hardware component. We applied the proposed 
action prediction method to reconstruct object actions. Besides, we 
also made a comparison between Xu et al.’s approach [8] and ours. 
For test sequence, the proposed method could keep the motion 
continuity of a reconstructed action and provided better result than 
Xu et al.’s approach. Fig. 9(a) shows some snapshots of the test 
sequence #1 and the experiment results of Xu et al.’s approach [8] 
and ours are shown in Fig. 9(c) and Fig. 9(d), respectively. 
According to the experiment result, it could be observed that the 
proposed method can maintain continuity on an action and 
provided better result than the result generated by applying Xu et 
al.’s approach. Compared with original the video, the 
reconstructed object action using our method is close to each other. 
Therefore, the proposed action prediction method is suitable for 
object inpainting which can better recover an object action and 
maintain motion continuity simultaneously. 
     
(a) 
 
(b) 
  
(c) 
  
(d) 
Fig. 9. The experiments on test sequence #1; (a) original video 
frames; (b) remove several consecutive frames (c) the result of [8]; 
and (d) the result obtained by applying the proposed method 
 
4. CONCLUSION 
 
In this paper, we proposed a novel framework for object inpainting. 
The proposed method consists of three steps: posture synthesis, 
graphical model construction, and action prediction. The 
advantage of this action prediction strategy is that it can handle the 
cases such as non-periodic motion or complete occlusion. Our 
experimental results also show that the proposed method can keep 
the reconstructed motion look continuous.  
 
ACKNOWLEDGEMENT 
 
This work was supported in part by Taiwan E-learning and Digital 
Archives Programs (TELDAP) sponsored by the National Science 
Council of Taiwan under NSC Grants: NSC99-2631-H-001-020. 
 
5. REFERENCES 
 
[1] K. A. Patwardhan, G. Sapiro, and M. Bertalmío, “Video 
inpainting under constrained camera motion,” IEEE Trans. 
Image Process., vol. 16, no. 2, pp. 545–553, Feb. 2007. 
[2] Y. Wexler, E. Shechtman, and M. Irani, “Space-time 
completion of video,” IEEE Trans. Pattern Anal. Match. Intell., 
vol. 29, no. 3, pp. 1–14, Mar. 2007. 
[3] S.-C. S. Cheung, J. Zhao and M. V. Venkatesh, “Efficient 
object-based video inpainting,” in Proc. IEEE Conf. Image 
Process., Atlanta, GA, pp. 705–708, Oct. 2006. 
[4] J. Jia, Y.-W. Tai, T.-P. Wu, and C.-K. Tang, “Video repairing 
under variable illumination using cyclic motions,” IEEE Trans. 
Pattern Anal. Match. Intell., vol. 28, no. 5, pp. 832–839, May 
2006. 
[5] C.-H. Ling, C.-W. Lin, C.-W. Su, H.-Y. Mark Liao, and Y.-S. 
Chen, “Video object inpainting using posture mapping,” IEEE 
Conf. Image Process., Cairo, Egypt, Nov. 2009. 
[6] T. Ding, M. Sznaier, and O. I. Camps, “A rank minimization 
approach to video inpainting,” in Proc. IEEE Conf. Comput. 
Vis., Rio de Janeiro, Brazil, pp. 1–8, Oct. 2007. 
[7] L. Wang, W. Hu, and T. Tan, “Recent developments in human 
motion analysis,” Pattern Recognit., vol. 36, no. 3, pp. 585–
601, Mar. 2003. 
[8] X. Xu, L. Wan, X. Liu, T.-T. Wong, L. S. Wang, C.-S. Leung, 
“Animating animal motion from still,” ACM Trans. Graphics, 
vol. 27, no. 5, Dec. 2008. 
[9] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and 
object recognition using shape contexts,” IEEE Trans. Pattern 
Anal. Mach. Intell., vol. 24, no. 4, pp. 509–522, Apr. 2002. 
 2
文作者進行面對面的研究討論。 
 
二、與會心得 
在參與此次的研討會中，有機會和多媒體研究領域的研究學者以及來自世界各知名大學研究機
構的研究生有面對面接觸的交流與討論，對於多媒體領域得以有更深一層的領悟與了解，在此
國際研討會中除了拓展國內與會者的國際視野外，也提供了國外研究團隊交流的機會，進一步
拓展研究視野，對於將來研究議題的發展具有正面影響。 
 
三、考察參觀活動(無是項活動者略) 
四、建議 
在本次國際多媒體展覽會議當中，本人發表壁報論文 Automatic Annotation of Web Videos 一
篇，充分和多位與會學者進行面對面的深入討論，對於該論文之後的發展，有著更深層的體
會。在本次會議當中，與國內來自台灣大學，清華大學的研究團隊充分交流，也交換彼此的
研究心得與經驗。 
 
五、攜回資料名稱及內容 
會議論文集隨身碟一個。 
 
六、其他 
Fig. 2. Our framework for foreground object detection.
each image frame. Irani and Anandan [4] proposed to detect
moving objects using 2D and 3D parameters. Wang et al. [5]
used motion vectors in MPEG videos to estimate affine pa-
rameters for zooming and translation. Nevertheless, estima-
tion of camera motion is still a challenging task due to back-
ground clutter present in a scene, or the appearance and scale
variations of the foreground objects, etc. Another category
of object detection algorithms is to model a reference back-
ground image. Felip et al. [6] estimated the dominant motion
from the sampled motion vectors, and the alignment based
on inter-image homography can be achieved by the dominant
motion. Zhao et al. [7] proposed to detect objects from videos
captured by a non-static camera in an indoor scene with an in-
crementally learned scene model. Their method is based on a
matching scheme using SIFT features and homography calcu-
lation. However, this setting may not be practical for videos
captured in outdoor scenes or with complex backgrounds.
Due to limited quality of Web video data, recent work on
Web video classification typically considers the use of mul-
tiple types of features. Most prior methods utilized various
static features such as appearance, color, etc. of each frame,
while some also considered space-time features to extract mo-
tion information [8, 9]. Methods which combine features
from different domains (e.g. text in [8, 10] and audio in [10])
also exist. However, as pointed out in [11], most existing
approaches on video classification cannot be easily general-
ized to applications of web video clips. Due to low qual-
ity and diversity of Web videos, the direct use of web-based
data could dramatically degrade the performance of the de-
signed algorithm. Moreover, they cannot be easily extended
to address video annotation problems. As a result, it usu-
ally requires some preprocessing techniques to obtain refined
data/features to improve the performance of Web video clas-
sification/annotation. For example, a hierarchical taxonomy
structure was proposed in [10] to alleviate noisy data, and in
[8], the authors pruned the motion features using spatial and
temporal statistics. In our work, in order to reduce the effect
of camera motion and cluttered background information, we
present a unique way to extract the region of interest for the
foreground object (Section 2), followed by the integration of
features from multiple domains (Section 3).
3. FOREGROUND OBJECT DETECTION
Our method for foreground object detection consists of two
major steps: construction of the consensus foreground object
template (CFOT) and its use for object detection. Figure 2
illustrates the framework, and Figure 3 shows a detailed flow
chart of the construction of CFOT.
3.1. Consensus Foreground Object Template (CFOT)
3.1.1. Foreground region estimation
Scale-invariant feature transform (SIFT) [12] is a popular
computer vision algorithm, which is used to detect local in-
terest points in an image, and to describe the associated ap-
pearance information. As an initial stage of our foreground
feature point extraction, we apply the SIFT feature detector
in each frame of a video sequence.
For each pair of corresponding SIFT feature points in ad-
jacent frames, a motion vector can be calculated. Assuming
that the motion vectors extracted frommoving foreground ob-
jects are significantly different from those from background
clutter, we apply a vector clustering algorithm to perform ini-
tial foreground region extraction. A quantization process ap-
plied on the magnitude of the motion vector is used to catego-
rize the motion vectors into a small number of classes. Each
motion vector is assigned with a corresponding quantization
index, and a histogram of these indices is calculated. For this
histogram, the bin with the maximum value is identified as
foreground, and the associated SIFT interest points are con-
sidered as initial foreground points. More specifically, let qˆt
denote the quantization index of the histogram histt at frame
t with the maximum value, and thus qˆt satisfies:
qˆt = arg max
q∈{1,2,··· ,Q}
(histt(q)) , (1)
whereQ represents the number of quantization categories (we
chose Q = 6, and did not observe the results will be sensitive
to this choice). Finally, a set of foreground interest points at
frame t , denoted by Ft, can be defined as follows:
Ft = {fi : g(vi) = qˆi and i ∈ {1, 2, . . . , Nt}}, (2)
where vi is the motion vector of feature point fi, g(·) is the
quantization procedure for vi, andNt is the number of motion
vectors obtained from two adjacent frames.
Once the foreground interest points are obtained, we de-
fine a candidate foreground region Rt according to the spatial
distribution of Ft with a Gaussian distribution assumption.
More specifically, let (x, y) denote the centroid of the fore-
ground SIFT points, and σx and σy represent the correspond-
ing standard deviation, we thus use the upper left corner (x−
2σx, y−2σy) and the bottom-right corner (x+2σx, y+2σy)
to set the boundary of Rt. We note that the recently proposed
SIFT flow [13] also advocates SIFT matching for determining
We do not use the salient SIFT descriptors (with local interest
points detected at different image scales), since they cannot
sufficiently describe the objects which are relatively small or
with low contrast in an image. We also found that such a
small percentage of local descriptors cannot provide adequate
descriptive information for multi-class object categorization
problems. Therefore, we choose to extract dense SIFT de-
scriptors. Although the color rgSIFT descriptors which add
appearance features from R and G color channels have been
shown to work well in several applications, we did not find
them useful in recognizing artificial objects with large vari-
ations of color (as the objects in our dataset do). Therefore,
we do not use color salient descriptors. As for the HOG de-
scriptors, we consider a dense grid of uniformly spaced cells
and extract gradient histograms. We did not consider space-
time features, since the foreground objects in our dataset are
all moving objects, and thus motion information does not pro-
vide any additional discriminating ability.
4.2.2. Learning sparse feature representation
Sparse coding (SC) has been shown to be an effective tech-
nique in many vision tasks [19]. To produce sparse represen-
tation for both SIFT and HOG descriptors, we use the soft-
ware package developed by Mairal et al. [20] to learn the
dictionaries (one for each feature), and to encode the associ-
ated feature descriptor. The parameter λ, which controls the
sparsity of the encoded coefficient vector, is set to 0.2 in our
experiments. The average and the maximum number of non-
zero elements in the encoded coefficient vectors are 4.87 and
18 for SIFT, and 5.43 and 16 for HOG (both out ofK = 225,
which is the size of the dictionary). After obtaining the en-
coded sparse coefficient vectors for both features, we use the
max pooling technique to covert the SIFT (HOG) descriptors
from each frame into aK-dimensional feature vector.
5. EXPERIMENTAL RESULTS
5.1. Web Video Dataset
We collect a complex, uncontrolled, and challenging Web
video dataset from YouTube for our experiments. The video
data are all captured by moving or shaky cameras, and the
moving object of interest are present in cluttered background.
Significant scale and viewpoint variations of the objects can
be observed, and the resolution of a large portion of videos
in our dataset is low. We consider six different moving ob-
ject categories: Airplane, Ambulance, Car, Fire Engine, He-
licopter, and Motorbike. Each object category has 25 to 30
video sequences, and each sequence has one moving fore-
ground object present in it. We randomly select 10 from
each class for training, and the remaining for testing. Fig-
ure 5 shows examples video frames of each object category
in our dataset. We note that, for audio classification, in or-
der to achieve comparable audio classification results as prior
Airplane
Ambulance
Car
Fire Engine
Helicopter
Motorbike
Fig. 5. Example videos in our dataset.
Table 1. Results of audio classification. MAP = 59.40%
Airplane Ambulance Car FireEngine Helicopter Motorbike
Airplane 50.00% 0% 22.22% 5.56% 16.67% 5.56%
Ambulance 0% 80.00% 10.00% 10.00% 0% 0%
Car 18.75% 0% 43.75% 6.25% 18.75% 12.50%
FireEngine 0% 5.56% 11.11% 83.33% 0% 0%
Helicopter 10.53% 0% 21.05% 0% 52.63% 15.79%
Motorbike 0% 0% 26.67% 6.67% 20.00% 46.67%
work using MFCC features, we further consider the use of
auxiliary training audio data, which are also collected from
YouTube. This additional video training data for audio clas-
sification does not necessarily have objects of interest visi-
ble, and we do not use this set of data for visual classifica-
tion either. Nevertheless, none of the above training data is
present in our test set, and we only use extracted audio and
visual features to train the associated classifiers and to per-
form feature/classifier-level fusion.
For our visual features, SIFT descriptors are extracted
from 16 × 16 pixel patches of an image, and the spacing
between adjacent patches is 6 pixels (horizontally and ver-
tically). HOG descriptors are extracted from each 8× 8 pixel
grid, and only one scale in an octave of the pyramid is used.
We note that we resize the longer side of the image to 300
pixels if its width or height exceeds 300 pixels for both de-
scriptors; this is to preserve the aspect ratio of each image.
5.2. Results of audio and visual classification
Table 1 shows the confusion matrix and the mean average
precision (MAP)for audio classification. The extraction of
MFCC audio features and the use of GMM classifiers were
detailed in Section 4.1. As shown in Table 1, we achieved
MAP = 59.4%, while objects Ambulance and Fire Engine
were with better classification results (> 80%).
To classify video data using either SIFT or HOG features,
we sub-sample 20 frames from each of the test video sequence
Fig. 6. Example annotation results. The convex hulls shown
in different colors are determined by our CFOT, and each
color corresponds to the associated object class label.
Audio+SIFT+HOG) were not the best in Table 4, while the
fusion of two visual features generally produced the small-
est improvements. This is expected, since rather than adding
features from the same domain or increasing the number of
features for fusion, integration of heterogeneous features from
multiple domains is expected to provide complementary infor-
mation for improved performance, as supported by our empir-
ical results (e.g. Audio+HOG or Audio+SIFT).
Finally, we show some of our video annotation examples
in Figure 6. Figure 6(a) illustrates an excellent annotation
result, which predicted the correct class label with a perfect
CFOT determined. We note that the helicopter in Figure 6(b)
cannot be successfully recognized by either visual feature;
this is probably because we do not have a large number of
front-view helicopter videos in our dataset). However, with
both feature and decision-level strategies, a correct annotation
result was obtained. Figure 6(c) is a challenging video, since
both the foreground object (aircraft) and background clutter
(e.g. sky, cloud, etc.) are present, and the contrast between
them is nominal. Similarly, its class label was successfully
predicted using the combination of both visual and audio fea-
tures, while the use of either feature did not produce a correct
output. These examples again verify the effectiveness of our
approach, which utilizes the significance of integrating het-
erogeneous features for improved video annotation.
6. CONCLUSION
We proposed a robust video annotation method which auto-
matically determines the region of the foreground object and
predicts its class label. The former was done using our con-
sensus foreground object template (CFOT) for moving object
detection, and the later was achieved by the integration of het-
erogeneous features from different domains. In this work, we
especially focused on the challenging task ofWeb video anno-
tation, in which most existing Web videos are captured under
uncontrolled environments, with insufficient quality or lim-
ited tag information available. Unlike prior sliding window
or object detector based methods, we do not require pixel-
level ground truth data for training; instead, only the label of
each video is utilized, which is especially practical for Web
video applications. In our experiments, we collected a Web
video dataset with only label information as ground truth. We
verified that our CFOT is able to identify the foreground re-
gion of interest, while our proposed framework provides tag
information (class label) using feature and decision-level fu-
sion techniques. We observed a significant improvement in
recognition (annotation) accuracy using our method. In future
work, we expect to extend our CFOT framework for further
high-level vision tasks such as activity or event recognition
using Web videos.
Acknowledgements This work is supported in part by
National Science Council of Taiwan via NSC 99-2221-E-001-
020 and NSC 100-2631-H-001-013.
7. REFERENCES
[1] Y. Sheikh and M. Shah, “Bayesian modeling of dynamic scenes
for object detection,” in IEEE PAMI, 2005.
[2] K. A. Patwardhan et al., “Robust foreground detection in video
using pixel layers,” in IEEE PAMI, 2008.
[3] J. Meng and S. Chang, “CVEPS - a compressed video editing
and parsing system,” in ACM Multimedia, 1996.
[4] M. Irani and P. Anandan, “A unified approach to moving object
detection in 2D and 3D scenes,” in IEEE PAMI, 1998.
[5] R. Wang, H.-J. Zhang, and Y. Zhang, “A confidence measure
based moving object extraction system built for compressed do-
main,” in IEEE ISCAS, 2000.
[6] R. L. Felip, L. Barcelo, X. Binefa, and J. R. Kender, “Robust
dominant motion estimation using mpeg information in sport
sequences,” in IEEE CSVT, 2008.
[7] Y. Zhao, M. Casares, and S. Velipasalar, “Continuous back-
ground update and object detection with non-static cameras,” in
IEEE AVSS, 2008.
[8] J. Liu, J. Luo, and M. Shah, “Recognizing realistic actions from
videos “in the wild”,” in CVPR, 2009.
[9] L. Duan, D. Xu, I. W. Tsang, and J. Luo, “Visual event recog-
nition in videos by learning from web data,” in CVPR, 2010.
[10] Z. Wang, M. Zhao, Y. Song, S. Kumar, and B. Li, “Youtubecat:
Learning to categorize wild web videos,” in CVPR, 2010.
[11] S. Zanetti, L. Zelnik-Manor, and P. Perona, “A walk through
the web’s video clips,” in CVPR Workshop, 2008.
[12] D. Lowe, “Object recognition from local scale-invariant fea-
tures,” in PETS, 1999.
[13] C. Liu et al., “SIFT flow: dense correspondence across differ-
ent scenes and its applications,” in IEEE PAMI , 2011.
[14] A. Senior, “Tracking People with Probabilistic Appearance
Models,” in PETS, 2002.
[15] J. Shirazi et al., “Improvements in audio classification based
on sinusoidal modeling,” in IEEE ICME, 2008.
[16] P. Dhanalakshmi et al., “Classification of audio signals using
AANN and GMM,” in Applied Soft Computing, 2010.
[17] S.-S. Cheng, H.-M. Wang, and H.-C. Fu, “A model-selection-
based self-splitting Gaussian mixture learning with application
to speaker identification,” in EURASIP JASP, 2004.
[18] N. Dalal and B. Triggs, “Histograms of oriented gradients for
human detection,” in CVPR, 2005.
[19] M. Elad et al., “Image denoising via sparse and redundant rep-
resentations over learned dictionaries,” in IEEE Trans. Image
Processing, 2006.
[20] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary
learning for sparse coding,” in ICML, 2009.
97 年度專題研究計畫研究成果彙整表 
計畫主持人：廖弘源 計畫編號：97-2221-E-001-020-MY3 
計畫名稱：結合異質特徵的視訊註解及擷取 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 4 80% 
本計畫主要是利
用異質特徵包括
運動向量、色彩等
去註解並擷取視
訊。三篇已刊登的
期 刊 都 是 針 對
video object去做
描述（註解），並
最終之目的是方
便有效的擷取。論
文均登在多媒體
領域最頂級期刊
IEEE Trans. on 
Multimedia 及
Image 
Processing. 
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
國外 
技術移轉 
權利金 0 0 100% 千元  
