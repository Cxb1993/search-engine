The research project had be investigated thoroughly by the PI and his students and rendered a 
publication, “Keyword-based Concept Search on Consumer Photos by Web-based Kernel 
Function” in the prestigious conference in multimedia community, ACM Multimedia 2008. 
With the novel idea and addressing keyword-based photo search for large-scale social media (i.e., 
Flickr, YouTube), the work attracts strong interests from major search companies such as Yahoo!, 
Google, and Microsoft, etc. 
 
Please see the details in the paper and the abstract for the paper is as follows: 
In light of the strong demands for semantic search over large-scale consumer photos, which 
generally lack reliable user-provided annotations, we investigate the feasibility and challenges 
entailed by the new paradigm, concept search – retrieving visual objects by large-scale automatic 
concept detectors with keywords. We investigate the problem in three folds: (1) the effective 
concept mapping and selection methods over large-scale concept ontology; (2) the quality and 
feasibility of the pre-trained concept detectors applying on cross-domain consumer data (i.e., 
Flickr photos); (3) the search quality by fusing automatic concepts and user-annotated data (tags). 
Through experiments over large-scale benchmarks, TRECVID and Flickr550, we confirm the 
effectiveness of concept search in the proposed framework, where the semantic mapping by 
web-based kernel function over Google snippets significantly outperforms conventional 
WordNet-like methods both in accuracy and efficiency. 
with highest lexical mapping scores to resolve the noisy concept 
mapping in an ad-hoc way. Through our preliminary experiments 
and other discoveries such as [2], the effective number of concepts 
is actually query-dependent.  
Authors in [3] conducted concept mapping over 25 manually 
picked concepts. Besides lexical similarity, the mapping process 
considered detector performances, mutual information between 
query terms and concept names, and time-sensitive information; 
though promising, such work requires gathering time-sensitive data 
for each query and is not feasible for generic visual search. 
In [2], authors conducted concept search over 374 LSCOM 
concepts. However, the representative synonyms for each concept 
are manually and subjectively selected. The concept mapping is 
simply the intersection of query terms and the manually selected 
concept synonyms. The problem arises when no direct matching 
between query terms and synonym sets. The issue occurs often 
since LSCOM merely expands a very small semantic space.  
It is worthy noting that above methods heavily rely on WordNet-
based lexical mapping (cf. Table 1) and are mostly unstable due to 
short query terms or concept descriptions. The same problem is also 
observed in the information retrieval community when determining 
the similarity between two short terms [5]. In this work, we address 
this problem by investigating effective mapping functions and 
utilize rich auxiliary knowledge sources including Google and 
Wikipedia. 
3. CONCEPT DETECTION 
Semantic concept detection aims to detect the presence of a 
specific concept, such as “rain,” “car,” “meeting,” etc. Effective 
methods for training and detecting semantic concepts are still open 
issues and require intensive research efforts. In this work, we adopt 
the 374 pre-trained LSCOM detectors by Columbia University 
(Columbia374) [7]. The Columbia374 uses three types of features 
(i.e., edge direction histogram, Gabor texture, and grid color 
moments) and then trains classifiers (i.e., support vector machines) 
individually.  In the detection phase, the features are extracted 
accordingly in each photo; for each concept detector, we apply the 
pre-trained support vectors (on broadcast news videos) to detect the 
existence of the concept within each consumer photo. The detection 
scores are normalized to [0,1] by sigmoid function to indicate the 
probabilistic existence of the concepts. Thereby each photo is 
associated with a 374-dimensional (real-valued) concept vector. 
Though the detectors are trained by broadcast news video data, we 
are keen to their effectiveness and usability in the new domain of 
consumer photos. 
4. AUTOMATIC CONCEPT MAPPING 
To map user queries to pre-defined concept lexicons, there are 
mainly two components, query-concept mapping and concept 
selection; the former measures the semantic relatedness to the 
specific concepts in the lexicon and implies the weights for multi-
concept fusion; the latter is to prune the irrelevant concepts since 
concept detection results are generally imperfect. More concepts do 
not necessarily promote the search performance [2][4]. In this work, 
we consider and compare variant concept search methods 
parameterized by mapping functions, auxiliary knowledge, and 
concept selection approaches as shown in Figure 1. 
4.1 Mapping by WordNet 
We first compare the query-concept mapping by WordNet, 
adopted by most prior works [1][3][4]. Observing their limitations, 
we further propose to enhance the mapping with variant 
considerations. We adopt the concept descriptions rather than short 
concept names only [9]. For stemming, Porter Stemmer is not 
applicable here since it would generate non-qualified words in some 
cases (e.g. ‘studies’ to ‘studi’); instead, we seek to WordNet 
Stemmer since it can generate qualified words by consulting the 
WordNet dictionary. Manually selecting synonyms such as that in 
[1] and [2] would be very time-consuming and labor-costly as the 
scale of the concept lexicon becomes large. Therefore, we 
automatically expand synonyms by using WordNet to expand each 
word into a synonym set. 
After automatic synonym expansion, we select adapted Lesk 
semantic relatedness algorithm [6], one of the effective pair-wise 
semantic measures based on WordNet; an alternative is Resnik [3], 
which shows poor performance in our early evaluation. Besides, 
there could be lots of replicate words in the description of each 
concept. For instance, the definitions of the two sports-related 
concepts Basketball and Tennis are “One or more people playing 
basketball” and “One or more people playing tennis” respectively; 
thus “people” is an unimportant word because it appears more 
frequently than “basketball” or “tennis.” However, they are equally 
weighted in prior query-concept mapping algorithms. From this 
observation, we adjust each word with the TFIDF (term frequency 
and inverse document frequency) weight, which is widely used in 
text mining and information retrieval, and we find out the MAP 
improved from 0.022 to 0.0521. 
4.2 Mapping by web-based kernel function  
By investigation, user queries are usually short (about 2.7 query 
words in Google search engine [5]); this phenomenon leads to the 
difficult measurement between user queries and concepts. Under 
this consideration, expansion is an intuitive way to enrich the 
semantic meanings of short terms. To address this problem, we 
adopted web-based kernel function (WKF) [5] and use Google and 
Wikipedia retrieved snippets as expansion resources. A snippet is a 
small piece of information generated by search engine or 
contributed by human2. Since snippets are concise summary of 
retrieved documents, we can handle them in an efficient way. Let 
S(q) = d1,d2,....,dn{ } denoted the retrieved snippets when issuing 
query q; and for each retrieved snippet di , we compute the term 
vector ti  by indexing around 40k terms (ignoring stop-list words).  
Next we can represent the semantic meaning of each query or 
concept as the centroid of these snippets S(•), called C(S(•)) and  
C(S(⋅)) = 1
n
ti
i=1
n∑ . 
                                                                 
1 In this work, we measured concept search on 8 conspicuous search topics 
(156, 158, 161, 164, 165, 168, 169, and 171) in TRECVID 2005. 
2  For example, the Google snippet returned first by the keyword 
“mountain” is “A mountain is a landform that extends above the 
surrounding terrain in a limited area, with a peak. ...” 
Table 1: Summary of query-concept mapping methods in 
related work. See Section 2 for more details. 
Groups Concept 
number 
Mapping  
methods 
Auxiliary 
knowledge 
Search over 
consumer media? 
IBM [1] 39 Lesk WordNet no 
CU [2]  374 Intersection - no 
NUS [3] 25  Resnik, 
Info. measures 
WordNet, 
News articles 
no 
NTU (this 
work) 
374 Web-based 
kernel function 
Google, 
Wikipedia 
Flickr photos 
652
vs. mean+std vs. top-N), mainly described in Section 4. The result is 
summarized in Figure 2. We can see concept selection is required as 
explicit improvements for all mapping methods and external 
knowledge; “mean+std,” setting the threshold mean plus one 
deviation, almost outperforms other two concept selection methods. 
The combination of Google mapping and “mean+std” concept 
selection towers above all others due to effective mapping by WKF 
over contextually rich, though unstructured, web contents. We later 
exploit this principle for experimenting concept search over 
consumer photos. Another interesting phenomenon is that concept 
selection on Google has larger promotion than that on Wikipedia; 
showing Wikipedia retrieved snippets are more organized than those 
Google retrieved. It is natural since Wikipedia involves more human 
editing.  
Mapping by WKF is more effective than that by WordNet 
methods. For example in query topic 165 “Find shots of basketball 
players on the court,” the most informative concept selected by 
WKF is Basketball; while concept Courthouse by using WordNet-
based mapping. This phenomenon shows dictionary-based mapping 
heavily relies on lexicon information and ignores the rich contextual 
knowledge in the web. 
5.3 Experiments in Flickr550 by WKF 
For the text-based baseline (“tag-only”), we employ MySQL full 
text search over associated tags from the consumer photos. To 
observe how concepts can help text-based search, we exploit late 
fusion for text-search baseline and concept search result (with equal 
fusion weight 0.5) and generate the search results (“tag+concept”). 
The impact from concepts in each category is summarized in Table 
2. It is obvious that queries in categories “Scene,” “Sport,” and 
“General object” derive benefits from concepts, while “Landmarks” 
and “Named objects” obtain limited accounts. Figure 3 shows 
detailed relative improvement for three benefited categories. The 
most explicit promotion is “football,” with the selected top 3 
concepts including “football,” “sports” and “sky;” query “beer” is 
degraded since the top 2 selected concepts are Bar_Pub and Store, 
though semantically relevant, but with poor detection performance. 
User-contributed tags are often inaccurate or ambiguous. In 
particular, due to the complex motivations behind tag usage, tags do 
not necessarily describe the content of the photos. Concept search in 
this work can actually augment text-based (tags) search baselines. 
The top row in Figure 4 is five photos, not tagged with “car,” but 
still correctly retrieved by selected concepts mapped by keyword 
“car;” while the bottom row is tagged with “car” but the content is 
not related to car and then rectified by the selected concepts (i.e., 
Car, Car_Racing, Vehicle).  
6. CONCLUSIONS  
The explosive growth of community-sharing photos and videos 
has created the need for semantic retrieval. In this paper, we explore 
how to leverage the large-scale semantic concepts for text-based 
concept search over consumer photos, which generally lack reliable 
textual metadata. Experimenting over two benchmarks, TRECVID 
and Flickr550, we had confirmed the effectiveness of concept search 
via the semantic mapping by web-based kernel function over 
Google snippets and demonstrated its significance against 
conventional WordNet-like methods both in accuracy and efficiency. 
We also illustrate the potential of pre-trained concept detectors 
applying on cross-domain consumer photos. We point that the user-
contributed tags are somehow inaccurate or ambiguous and can be 
improved by semantic concepts in the applications of keyword-
based search.  
For the future work, we are extending the work for searching 
community-sharing videos and investigate the adaptability issues for 
these pre-trained concept detectors for cross-domain usage. 
7. REFERENCES 
[1] A. Haubold et al, “Semantic multimedia retrieval using lexical query 
expansion and model-based reranking.” Proc. ICME, 2006. 
[2] L. Kennedy, S.-F. Chang, “A reranking approach for context-based 
concept fusion in video indexing and retrieval.” Proc. CIVR, 2007. 
[3] S.-Y. Neo et al, “Video retrieval using high level features: exploiting 
query matching and confidence-based weighting.” Proc. CIVR, 2006. 
[4] A. Natsev et al, “Semantic concept-based query expansion and re-
ranking for multimedia retrieval.” Proc. ACM Multimedia, 2007. 
[5] M. Sahami, T.D. Heilman, “A web-based kernel function for measuring 
the similarity of short text snippets.” Proc. WWW, 2006. 
[6] S. Banerjee, T. Pedersen, “Extended gloss overlaps as a measure of 
semantic relatedness.” Proc. IJCAI, 2003. 
[7] A.Yanagawa et al, “Columbia University's baseline detectors for 374 
LSCOM semantic visual concepts.” Columbia University ADVENT 
Technical Report #222-2006-8, 2007 
[8] X. Li et al, “Video search in concept subspace: a text-like paradigm.” 
Proc. CIVR, 2007. 
[9] Milind Naphade et al, “Large-scale concept ontology for multimedia,” 
IEEE Multimedia Magazine, Vol. 13, No.3, July-September, 2006. 
 
Figure 3: The relative AP improvements (%) over 17 queries, 
belonging to three query categories “scene,” “sports,” and 
“general objects” in Flickr550. 
     
     
 
Figure 4: The top row is five photos not tagged with “car” but 
still retrieved by selected concepts queried by keyword “car;” 
The bottom is randomly chosen from those wrongly tagged 
“car,” which are later corrected by selected concepts. This is 
evident that user-contributed tags are somewhat inaccurate and 
might be mended by automatic concepts. 
Table 2: twenty-nine Flickr550 queries in 5 categories; the 
relative improvements by late fusion (averaging normalized 
search scores) over tag-only and concept search (WKF and 
mean+std) baselines (in MAP within each category).  
category (#) tag-only tag+concept relatve improv. (%) 
Landmarks (7) 0.315 0.316 0.48 
Scene (5) 0.242 0.281 15.71 
Sports (3) 0.668 0.727 8.85 
General objects(9) 0.379 0.434 14.42 
Named objects (5) 0.318 0.311 -2.31 
654
Kuan-Ting Chen, D96944008 
 
Kuan-Ting Chen, D96944008 
About this report 
About computer vision conference 
CVPR  
Part1: Agenda 
Part2: Main Conference 
Part3: Workshops + Tutorials 
Aftermath 
About computer vision conference 
The goal of computer vision is to develop an 
intelligence environment which a robotics or machine 
can response as a person. The history of computer 
vision is as Figure 1 and related background 
knowledge is as Figure 2. Three important conferences 
in computer vision are ICCV (IEEE International 
Conference on Computer Vision), CVPR (IEEE 
International Conference on Computer Vision and Pattern Recognition) and 
ECCV (European Conference on Computer Vision) and you can find latest 
trends in computer vision from these three conferences.  
ICCV and ECCV are hold every each two years and CVPR is hold every year 
that you can choose attend two computer vision every year. The average of 
accept rate is about 20% and about 1000 paper submitted. CVPR does not 
only include computer vision topic but also pattern recognition issues.  
 
Figure 1 Techniques’ development in computer vision. 
Kuan-Ting Chen, D96944008 
and image and video retrieval, and so on. The applications are surveillance, 
camera, robot, search, and medical. 
For face, gesture, and action analysis, topics are face alignment, face 
recognition, hand-tracking and action with scripts. The Figure 3 is an example 
of action analysis by scripts. For camera calibration, some people use mesh, 
mirror, and depth to calibrate different cameras, such as large field-of-view 
camera, Passive stereo time-of-Flight sensor as Figure 4. For Motion and 
Tracking and segmentation, a lot of people use graph cut to minimize energy 
function. Figure 5 is an example about detection and tracking pedestrian. 
 
Figure 3 Action analyses in a movie. 
 
Figure 4 Using mirror to analysis environment by multi-camera robotics. 
 
Figure 5 An example of pedestrian detection and tracking. 
