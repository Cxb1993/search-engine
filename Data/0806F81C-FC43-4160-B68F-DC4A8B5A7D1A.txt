 
 
Abstract 
Nowadays, the 3D image processing has become a trend in the related visual 
processing field. Many automatic 2D to 3D conversion algorithms have been 
proposed to solve the lack of 3D content. But there is still no fast algorithm that 
converts single monocular images well. 
In this thesis, we propose a fast conversion algorithm that includes the image 
segmentation, image classification, object boundary tracing method, and 3D image 
generation. The image segmentation adopts the watershed method to easily collect 
the information of depth cue. Then, the image classification recovers the geometry 
of scene in the image. With the depth cue and geometry information, the object 
boundary tracing method is proposed to detect objects in image efficiently. Finally, 
the object result is used to generate depth map and 3D anaglyph image.  
To evaluate the results, we compare the stereo images with other 2D to 3D 
conversion systems. Experiment result shows that the proposed 2D to 3D conversion 
algorithm could perform better than the associated ones in the depth accuracy and 
processing speed for converting monocular images. 
  
1 
 
1. Report 
1-1 Introduction 
Video is a widely used multimedia nowadays. After the resolution of video reach 
a full HD(1920*1080) level, human starts to seek a more realistic way to enjoy it. 
Therefore, the research in 3D video has become more and more popular. However, 
the traditional contents are all 2D contents and they cannot fit a 3D display device. 
Because of the reason, many automatic 2D to 3D conversion algorithms have been 
proposed to solve the lack of 3D content. But there is still no fast algorithm that 
converts monocular still images well.  
 
1-2 Target 
    Our goal is to develop an algorithm that can convert a traditional 2D content 
into a 3D one. In order to make our algorithm have a wide application, the target 
speed of the algorithm is 30fps for a HD1080(1920*1080) sequence. Furthermore, it 
can combine with other sub-project and build a transmission system. By the system, a 
3D display technology will become a much easier task and it can also improve the 
application in many regions. 
 
1-3 Previous work 
Recovering 3D information from a 2D video is a basic problem in computer 
vision. Many depth cues can be used to extract the 3D information from a 2D video, 
but each cue has its own advantages and disadvantages for different conditions. 
Iinuma et al. [1] used the defocus cue to evaluate depth information by a single frame 
and the motion cue to convert the video. Cheng et al. [2] used the geometry cue and 
motion cue to evaluate the depth information. The simple concept and low 
computational complexity of those methods have enabled it to be adopted in real-time 
applications. However, those methods cannot perform well for a monocular still 
image. 
Another approach is the pattern recognition-based method. In which, an image is 
first partitioned into many regions, and each region is categorized into several classes 
to be assigned depth. Based on this concept, Battiato et al. [3] classify regions into 
indoor, outdoor with geometric elements, and outdoor without geometric elements. 
Then, it uses the information collected in the classification step to estimate the depth. 
Even through this method could generate the high-quality result for the monocular 
still image, this method cannot perform well for many types of scenes. Hoiem et al. [4] 
also classify regions into several classes first. Then, they extract the boundary 
information of regions to merge small regions into objects, and further assign a 
3 
 
the constraint segmentation to merge segments by well-defined conditions. Finally, 
we assign the depths to the objects, and use the DIBR algorithm [6] to generate the 
images for left and right eyes in the final stage. 
 
1-4-a. Initial Segmentation 
In the proposed 2D to 3D conversion algorithm, the accuracy of object boundary 
detection is important. Thus a proper choice of image segmentation algorithm is also 
important in our approach. We adopt the watershed image segmentation [7] because it 
can preserve edge in the object boundary, and it is suitable for fast application. 
Since the number of segments is related to the computational complexity in our 
algorithm, we propose a neighbor merge method to reduce the segments. In this 
method, we refer to the color and texture information of each small segment to further 
merge segments into meaningful ones.  
For the color information, we consider the color distance between segments by 
their average color. For the color space, we apply the Hue-Saturation-Value (HSV) 
color space, and its color difference        is computed by the formula [8]. For the 
texture information, we apply a subset of the filter bank in [9] to compute the texture 
responses of each pixel. The filter bank consists of 6 edges filters, 6 bars filters, 1 
Gaussian filter, and 2 Laplacian of Gaussian filters. With the texture responses, the 
histogram of maximum responses is computed for every segment, and then the 
symmetrized Kullback-Leibler divergence        is computed for every neighboring 
segment. 
Finally, we compute the edge cost to combine the above color and texture 
information for every neighbor segments by the formula, 
                                           (1) 
where  ,   are the weighting factors to control the amount of color difference 
       and divergence       . 
With the edge cost, two small segments could be merged if their edge cost is 
lower than the threshold T. The threshold is automatically and iteratively refined until 
the number of segment is smaller than a constant. 
 
1-4-b Surface layout 
After initial segmentation, we apply the surface layout algorithm [5] to estimate 
the geometry for each segment. The surface layout algorithm can label the image into 
geometry classes, which coarsely describe the 3D scene orientation of each image 
region. Every region in the image is categorized into one of three main classes: 
“support”, “vertical”, and “sky”. In addition, the “vertical” class is further categorized 
into one of five subclasses: “left”, “center”, “right”, “porous”, and “solid”. In the 
5 
 
1-4-d Constraint segmentation 
With the proposed object boundary tracing method, some segments in the image 
are not complete objects. They could be further merged by the event constraints as 
listed in Table 1. We could merge the segments if the following conditions are 
satisfied. 
Condition 1:                 
Condition 2:                         
Condition 3:                        
Condition 4:                
We seriatim check these conditions, and merge the segments. After the constraint 
segmentation process, the object-based segmentation is done.  
 
Table 1. Events of constraint segmentation 
Event 1: the color of the segment is similar to the other. 
Event 2: the label confidence of segment is similar. 
Event 3: the shape of the segment is similar to the other. 
Event 4: the y axis position of the segment is similar 
Event 5: the segment is inside of the other segment. 
Event 6: the segment is small enough. 
 
1-4-e Depth assignment and 3D image construction 
Finally, we assign the depth to the objects according to the object segmentation 
result and the geometry information in Section 2.3. Our model in the 3-dimensional 
space consists of a ground plane and objects are orthogonal to the ground and sky.  
At first, for each region, we fit a set of line segments to the ground-vertical 
boundary by using the Hough transform. Those line segments are used to determine 
that the “vertical” segments are planar or not. If a “vertical” segment contains the line 
segments, it is a planar. Otherwise a “vertical” segment is a non-planar. 
Then, we assign different depth for segment according to their conditions. For the 
“ground” segment and the planar “vertical” segment, we assign gradient depth. Then, 
we assign corresponding depth according to the position of horizontal line and the 
behavior of ground-vertical boundary. For the “sky” segment and the non-planar 
“vertical” segment, we assign constant depth according to its position in the image 
coordinate. 
After the depth assignment, we have the disparity map and further generate an 
anaglyph image for left and right eyes by the depth-based image rendering (DIBR) 
algorithm [6]. 
 
7 
 
 
 
 
 
 
 
Fig. 2. Flower garden sequence                 Fig. 3 . hall monitor sequence 
 
 
 
 
Fig. 4. Scenery15 sequence                      Fig. 4. Scenery15 sequence 
 
 
 
 
 
 
Fig. 6. Outdoor21 sequence                       Fig. 7 . Structure10 sequence 
 
1-7. Conclusion 
In this paper, we proposed an efficient 2D to 3D conversion algorithm which 
automatically converts a still 2D image into a 3D one. With the proposed object 
boundary tracing method, the computation time is much reduced to 2.25%. The 
9 
 
3. Self-assessment 
    We successfully proposed a high speed algorithm compared to other 
object-based ones. Under object-based algorithm, we can make a complete object in 
the same depth. That is, we will feel that every part in the object is at the same 
distance with us. It is very important for a comfortable 3D experience. 
    This algorithm was also published in CVGIP, 2011 as an oral paper. It will be a 
good reference for someone who expects to do further research in this area. 
    However, this algorithm cannot reach our final goal – the real time application. 
Although we believe that the parallel computation can speed up the algorithm a lot, it 
may not reach 30fps when handling a full HD sequence. For a realistic application, we 
should try to simplify and speed up this algorithm. 
    The other question is that there is no standard for evaluating the qualities of 2D 
to 3D conversion algorithm since there is no ground truth for a real 2D sequence. 
Furthermore, the confortable 3D content is a complex issue because it will depend on 
human’s feeling. The judgment way should be different from the traditional 2D 
sequence. We believe that to establish a fair computation method to judge which 
algorithm is better will be an important issue in the related research. 
 
 Published paper 
Yi-Chun Chen et. al. “Efficient 2D to 3D conversion with Object-Based 
Segmentation”, Computer Vision, Graphics, and Image Processing (CVGIP), 2011 
 
 2 
二、與會心得 
  此會議為訊號處理領域的主要會議，其中包含了通訊、語音編碼、視訊編碼、影
像處理以及與數位訊號相關之議題。在此會議中，參加了數場發表場次，也聽了不
少篇國外學者所做的研究。雖然說這個會議選擇在希臘的一個離島舉辦，原本預期
心理會覺得應該論文數量不多且論文品質可能不高，但是，聽了這幾場之後，我發
現此會議所接受的論文品質似乎具有一定的水準。所以，參加此會議讓我又多了解
了許多數位訊號處理研究議題。 
 
三、考察參觀活動(無是項活動者略) 
 略 
 
四、建議 
 此會議雖然規模小，但卻是五臟俱全。然而，由於參加會議那段時間剛好是希臘
罷工潮，所以遇到罷工事件導致科孚島上所有計程車在我們抵達那天皆沒有營運。
更慘的事是，主辦單位也沒有查覺這件事，最後導致我們拖著行李徒步至開會飯店。
因此，建議如果以後遇到國內有罷工事件的話，那麼主辦單位最好是安排公車讓參
與者搭乘。 
 
五、攜回資料名稱及內容 
 研討會會議手冊(內含會議行程以及詳細各場次所欲發表之論文題目) 
會議光碟(內含所有完整會議資訊以及所有論文全文) 
 
六、其他 
 無 
 
 
 
It should be further discussed how the effectiveness of the proposed technique depends on the 
employed motion estimation strategy. 
 
I would further suggest to add rate distortion curves (instead of the PSNR and bit rate difference 
curves) in order to illustrate the impact on the rate distortion efficiency. 
 
 
----------------------- REVIEW 2 --------------------- 
PAPER: 115 
TITLE: AN EFFICIENT FRACTIONAL MOTION ESTIMATION MODE SELECTION ALGORITHM FOR 
H.264/AVC SCALABLE VIDEO EXTENSION 
AUTHORS: Gwo-Long Li and Tian-Sheuan Chang 
 
OVERALL RATING: 1 (weak accept) 
NOVELTY AND ORIGINALITY: 3 (fair) 
TECHNICAL CONTENT AND CORRECTNESS: 3 (fair) 
CLARITY OF PRESENTATION: 4 (good) 
RELEVANCE TO THE CONFERENCE: 5 (excellent) 
 
Fast mode decision algorithms have been in use for H.264. What is new in this paper for SVC is a 
way to avoid the fractional pel motion estimation for the new SVC modes - motion prediction, 
residual prediction. 
 
The proposed method looks at integer pel cost to avoid some of the fractional pel cost 
computations. While the basic concept is not new, it may be new when applied in the SVC concept. 
Hence the rating for "innovation" and "technical content" are given as average. 
 
Some of the experimental results are surely useful for the SVC comminity - hence the relevance is 
given as "excellent" - and seing from the CFP fo the conference, teh contents seem to fit in. 
 
Points for potential improvement: 
1. Some gramatical mistakes could have been avoided. 
2. The introduction directly jumps to an existing fast ME / mode decision algorithm without even a 
paragraph of background to SVC, and what is currently available for SVC in public implementations. 
 
Gwo-Long Li <glli@dragons.ee.nctu.edu.tw> 2011年3月28日上午11:13 
收件者: tschang@dragons.ee.nctu.edu.tw 
[隱藏引用文字] 
Page 2 of 2Gmail - DSP2011 notification for paper 115
2011-08-02https://mail.google.com/mail/?ui=2&ik=b51218734b&view=pt&cat=Paper%20Submissi...
16x16 16x8 8x16 8x8
8x44x8 4x4
IME for 
Inter mode
FME for 
Inter mode
IME for Inter 
mode + Residual
FME for Inter 
mode + Residual
IME for ILM
FME for ILM
IME for ILM + 
Residual
FME for ILM + 
Residual
Best mode selection
Best mode  
Fig.2 Illustration of mode selection process of SVC 
16x16 16x8 8x16 Sub-mode
IME for Inter mode
FME for Inter mode
Best mode selection
Best mode
Fig.3(a) Illustration of mode 
selection process for H.264 
16x16 16x8 8x16 Sub-mode
IME for Inter mode
Mode filtering for FME
Best mode selection
Best mode
FME
 
Fig.3(b) Illustration of mode 
pre-selection concept for 
H.264 
IME for 
Inter mode
FME for 
Inter mode
IME for Inter 
mode + Residual
FME for Inter 
mode + Residual
IME for ILM
FME for ILM
IME for ILM + 
Residual
FME for ILM + 
Residual
Best mode selection
Best mode
16x16 16x8 8x16 Sub-mode
 
Fig.4 Illustration of mode selection process for SVC 
 
2. PROPOSED MODE PRE-SELCTION ALGORITHM 
In this section, we conduct several analyses to observe the 
relationship between the rate distortion costs (RDCosts) of 
IME and FME of different prediction modes.  
 
2.1. Analysis for Inter-layer and Inter prediction modes 
Fig. 5 shows the relationship between RDCosts of IME and 
FME of different prediction modes. In this figure, the 
vertical axis indicates the RDCosts and the horizontal axis is 
the index of macroblocks. The InterI and ILMI individually 
stand for the IME RDCosts of Inter and ILM mode; the 
InterF and ILMF are the FME RDCosts of Inter and ILM 
mode, respectively. From this figure, we can derive a 
property that the RDCosts of IME and FME are very close 
to each other for the same prediction mode. For example, 
the RDCost of IME is very close to the RDCost of FME for 
Inter mode and the same situation can be seen from ILM 
prediction mode. Therefore, if the IME RDCost of Inter 
mode is sufficiently larger than that of IME RDCost of ILM 
mode, the FME RDCost of Inter mode will be larger than  
FME RDCost of ILM mode and vice versa. As a result, we 
conduct several simulations to confirm the property that we 
observed and the statistical results are shown in Table 1. In 
this table, the conditional probability of P(A|E) is defined as 
follows.  
       
      
    
                              (1) 
where 
P(E) = P(I                        
                                    (2) 
P(A)=P(In                     
                                         (3) 
                   
                                  
                                       (4) 
 
From the statistical results shown in Table 1, we can 
observe that the conditional probability of Eq.(1) could 
achieve up to 85.74% on average. In summary, we conclude 
that if IME RDCost of Inter (ILM) mode is sufficiently 
larger than that of IME RDCost of ILM (Inter) mode, the 
FME of Inter (ILM) mode can be skipped. 
 
 
(a) 
 
(b) 
Fig.5 Relationship between RDCosts of IME and FME of 
different prediction modes (a) Football, (b) Foreman 
sequence 
 
Table 4. Bitrate comparison of proposed algorithm 
 JSVM Proposed Increasing (%) 
Akiyo 29.23  29.72  1.69 % 
Dancer 117.11  118.58  1.26 % 
Coastguard 264.93  267.03  0.79 % 
Table 181.62  184.12  1.38 % 
Tempete 225.52  227.77  1.00 % 
Football 332.05  335.79  1.13 % 
Foreman 111.12  113.16  1.84 % 
MD 45.89  46.79  1.96 % 
Mobile 338.48  341.41  0.86 % 
News 68.92  69.66  1.07 % 
Soccer 186.04  187.56  0.82 % 
Stefan 284.97  287.91  1.03 % 
 
Table 5. PSNR comparison of proposed algorithm 
 JSVM Proposed Difference (dB) 
Akiyo 39.94  39.92  -0.02  
Dancer 41.00  41.00  -0.00  
Coastguard 33.97  33.96  -0.01  
Table 35.33  35.31  -0.02  
Tempete 34.37  34.35  -0.02  
Football 35.69  35.68  -0.01  
Foreman 36.69  36.68  -0.01  
MD 38.95  38.91  -0.04  
Mobile 33.51  33.49  -0.02  
News 38.24  38.22  -0.01  
Soccer 35.77  35.74  -0.03  
Stefan 34.85  34.84  -0.01  
 
Table 6 Averaged mode reduction of our proposal (Unit: %) 
Akiyo Dancer Coastguard Table Tempete MD 
62.50 68.75 75.00 75.00 75.00 68.75 
Football Mobile Foreman News Stefan Soccer 
81.25 75.00 75.00 68.75 75.00 75.00 
 
4. CONCLUSION 
In this paper, an efficient mode pre-selection algorithm for 
fraction motion estimation is proposed to reduce the 
computational complexity of fraction motion estimation in 
SVC. By observing the relationship between IME RDCosts 
and FME RDCosts of different prediction modes, several 
mode pre-selection rules are proposed to reject some 
potentially skippable modes before FME operation. In 
addition, since our proposed mode pre-selection algorithm is 
only composed by several simple additions, subtractions, 
and comparators, it can be easily realized in hardware form. 
Simulation results demonstrate that our proposed algorithm 
can reduce 72.92% prediction modes on average before 
entering FME operation. In addition, the bitrate increasing 
and PSNR degradation of our proposed algorithm is only 
1.24% and 0.02dB, respectively. 
 
 
5. ACKNOWLEDGE 
This work was supported by National Science Council of 
Taiwan, under Grant NSC98-2220-E-009-001. 
 
 Initialize ∀Φij=True|Φij∈Φ
InterI16× 16+ω ≤ 
ILMI16× 16
ILMI16× 16+ω≤ 
InterI16× 16
InterI16× 8+ω≤ 
ILMI16× 8
ILMI16× 8+ω≤ 
InterI16× 8
InterI8× 16+ω ≤ 
ILMI8× 16
ILMI8× 16+ω ≤ 
InterI8× 16
InterISub-mode+ω≤ 
ILMISub-mode
ILMISub-mode+ω≤ 
InterISub-mode
ΦILM16× 16=False
ΦInter16× 16=False
ΦInter16× 8=False
ΦILM16× 8=False
ΦILM8× 16=False
ΦInter8× 16=False
ΦILMSub-mode=False
YES
YES
YES
YES
YES
YES
YES
NO
NO
NO NO
NO
NO
NO
InterI16× 16 ≤ 
InterI16× 8
InterI16× 16 ≤ 
InterI8× 16
ILMI16× 16 ≤ ILMI16× 8
ILMI16× 16 ≤ ILMI8× 16
ΦInter16× 8=False
ΦInter8× 16=False
ΦILM16×8=False
YES
YES
NO
NO
ΦInterSub-mode=False
YES
InterI16× 16 ≤ InterISub-
mode
ILMI16× 16 ≤ ILMISub-
mode
ΦInterSub-mode=False
YES
NO
NO
ΦILMSub-mode=False
YES
NO
ΦILM8× 16=False
YES
NO
 Execute FME for Φ
NO
Mode pre-selection for Inter-layer and Inter prediction 
modes
Mode pre-selection for partition size
YES
 
Fig.7 Flowchart of our proposed FME mode pre-selection 
algorithm 
 
6. REFERENCES 
[1] Draft ITU-T Recommendation and Final Draft International 
Standard of Joint Video Specification (ITU-T Rec. 
H.264/ISO/IEC14496-10 AVC), March 2003. 
[2] T.-C. Wang, Y.-W. Huang, H.-C. Fang, L.-G. Chen, 
“Performance analysis of hardware oriented algorithm 
modifications in H.264,” in proceeding of IEEE International 
Conference on Acoustics, Speech, and Signal Processing, vol.2, 
pp.493-496, April 2003. 
[3]  Y.-K. Lin, D.-W. Li, C.-C. Lin, T.-Y. Kuo, S.-J. Wu, W.-C. 
Tai, W.-C. Chang, and T.-S. Chang, ” A 242mW, 10mm2 1080p 
H.264/AVC High Profile Encoder Chip,” in proceeding of 
International Solid-State Circuits Conference (ISSCC), pp. 314-
315, Feb. 2008. 
[4]  H. Schwarz, D. Marpe, and T. Wiegand, “Overview of the 
scalable video coding extension of the H.264/AVC standard,” 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/15
國科會補助計畫
計畫名稱: 子計畫五：高畫質多視角立體視訊核心技術研究(I)
計畫主持人: 張添烜
計畫編號: 99-2221-E-009-185- 學門領域: 積體電路及系統設計
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
