 1
 
行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   □期中進度報告 
解決神經網路最佳學習之研究--近似等效類神經網路模式 
The Research in Solving Neural Network's Optimal Learning 
-- Nearly Equivalent Neural Network Model 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC100-2221-E-230 -021 
執行期間：100 年 8 月 1 日至 101 年 7 月 31 日 
 
執行機構及系所：正修科技大學資訊管理學系 
 
計畫主持人：陳玉珠 
共同主持人：黃瑞初 
計畫參與人員：許普騰、林昱安、黃致樫、楊仁賓、黃督周、鄭傑 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告 □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
        □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中 華 民 國  100 年 7 月 31 日 
 
附件一 
 3
methods usually need many trials. They do 
have the problem of the time-consuming. 
 
In this research, a new technique, called 
“nearly equivalent NN model” is proposed and 
employed. This technique is expected to have 
the global searching for system’s behavior. The 
mathematical model of the system analyzed 
also can be expressed a clear nonlinear 
function. In Section II, we simply introduce the 
supervised NN model and its learning 
algorithm. The nearly equivalent NN model is 
reported in Section III. Some studies and 
simulations for real system’s modeling are 
presented in Section IV and then a conclusion 
is made in Section V. 
 
II.  SUPERVISED NN MODEL AND ITS 
LEARNING ALGORITHM. 
 
In our studies, a well-known three-layered 
feed-forward fully connected supervised NN is 
shown in Fig.1. The sigmoid function is used 
to be the activation function of each node in 
network. The major steps of BP algorithm can 
be summarized as follows [2, 3]. 
Step 1: Initialize all weights, ij s, to small 
random values. 
Step 2: Present an input pattern and specify the 
desired output. Calculate output using the 
present value of ij s. 
Step 3: Find the error term, j  for all nodes. If 
dj, Oj and Hj denote the desired value of the jth 
output node, the computed value of the jth 
output node, and the computed value of the jth 
hidden layer node, then error terms are 
calculated as; 
for the output layer node j: 
)O1(O)Od( jjjjj      (1) 
for the hidden layer node j: 

k
kjkjjj  )H1(H ,           (2) 
where k is over all nodes in the layer above 
node j. 
Step 4: Adjust weights by 
1))-n(-n)((
Xn)(1)n(
ijij
ijijij




         (3) 
where (n+1), (n) and (n-1) index next, 
present, and previous iteration numbers, 
respectively. Xi is the ith input connected with 
node j.ηis the learning rate. ζ is a momentum 
that effectively filters out high-frequency 
variations of the error surface. 
Step 5: Present another input and go back to 
step 2. All training inputs are presented 
cyclically until all weights stabilize. 
 
Fig. 1. A three-layered feed-forward  
neural network. 
 
III. THE NEARLY EQUIVALENT MODEL 
 
In our study, the polynomial network is 
used as the basic tool for constructing the 
nearly equivalent model. The polynomial NN 
and the idea of equivalent model are described 
as follows, respectively. 
 
A. The polynomial network 
 
Fig. 2. shows a three-layered polynomial 
NN. It is a Sigma-Pi network. The output of 
network can be expressed as a polynomial in 
the elements of the input vector [14]. 
 
 
 
Fig. 2. A three-layered polynomial network. 
 
Suppose we denote ],...,,,[ 210 nkkkkk xxxxX  ; 
10 kx as the kth input pattern, then the output 
of the jth sigma unit is given by 
 5
has achieved the optimal learning already. 
 
The major steps of the algorithm are 
described as follows [15]. 
Step 1 Initialize all weights k s to small 
random values (typically between -0.5 and 
+0.5). 
Step 2 Calculate output using the present 
values of k s. 
Step 3 Find the error ke . If dk, and yk 
denote the desired value and the computed 
value of the output for input pattern k, then the 
error term can be calculated as; 
  kkk yde  .                  (10) 
Step 4 Adjust weights by         nxnenn kkkk  1 ;    (11) 
where,  1n  and  n  index the next and 
present iterations. ηis the learning rate. 
Step 5 Present another input and go back to 
Step 2. 
All training inputs are presented cyclically 
until the weights are stable. 
 
IV. SIMULATIONS 
 
In this section, several signal problems 
were studied by using NN model with BP 
learning algorithm and the nearly equivalent 
NN model. The comparison of the 
performances by two methods is reported. The 
polynomial NN is used as the tool for 
constructing the equivalent model. The original 
supervised NN could be replaced by the 
high-order polynomial function developed by 
the polynomial NN. It makes the output of NN 
model could be expressed by a high-order 
polynomial function of input variables. This 
function could be treated as a multi-variable 
linear function. Thus, the least-mean-square 
(LMS) method can be used to find the nearly 
optimal solution. The mean absolute 
percentage error (MAPE) and accuracy are 
used as the performance measurement.  
 
Signal recognition 
 
In this study, the signal recognition for a 
non-convex system (NC2) in two dimensions 
shown in Fig. 4 is simulated. 1038 points are 
generated. The first set with 519 points was 
used for NN’s training, and the second set with 
519 points was used for NN’s testing. A size of 
2-4-1 sigmoid NN with BP learning algorithm 
was used in this study. Table 1 lists the 
accuracy statistics of the signal recognition 
performed by NN model with different initial 
weights and different learning rates. 
 
We used the adequate numbers of 
polynomial NN to substitute the neurons of the 
trained NN model. Here, the initial weights 1 
with the learning rate 0.9 in the Table 1 was 
taken as the NN model approximated by the 
polynomial NN. Table 2 shows the absolute 
mean errors of each neuron in the trained NN 
approximated by individual polynomial NN. 
 
According to approximation results, the 
order of the equivalent polynomial function 
was determined to be 6. LMS algorithm was 
used to find the coefficients of the equivalent 
model. In our simulation, the accuracies of 
training and test are 93.4% and 92.7% by using 
the equivalent polynomial function. Clearly, 
the performance of this polynomial network is 
better than the original NN model. It 
demonstrates the trained network was 
encountered the local minima. 
0.0 0.2 0.4 0.6 0.8 1.0 1.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0.1
0.9
 
Fig. 4.  The samples of non-convex system. 
 
Table 1. The accuracy statistics performed by 
NN Model with size 2-4-1. 
 
Initial Weights 1 Initial Weights 2 Initial Weights 3
Accuracy (%) Accuracy (%) Accuracy (%) 
 
Learnin
g rate 
Training Test Training Test Training Test
0.1 79.3 77.9 80.8 79.4 80.2 80.1
0.3 77.2 77.3 75.8 77.5 84.4 84.9
0.5 83.9 82.8 78.4 76.7 79.3 78.1
0.7 80.5 81.1 80.2 81.3 76.8 77.1
0.9 54.1 52.8 53.5 54.6 74.3 73.4
 7
[1] A. Khotanzad, R. C. Hwang, A. Abaye, 
and D. Maratukulam, “An adaptive 
modular artificial neural network: Hourly 
load forecaster and its implementation at 
electric utilities”, IEEE Transactions on 
Power Systems, Vol. 10, pp. 1716-1722, 
1995. 
[2] C. Wohler and J. K. Anlauf, “An adaptive 
time-delay neural-network algorithm for 
image sequence analysis”, IEEE 
Transactions on Neural Networks, Vol. 10, 
pp. 1531-1536, 1999. 
[3] C. C. Huang, H. C. Huang, Y. J. Chen, R. 
C. Hwang, "An AI system for the decision 
to control parameters of TP film printing", 
Expert Systems With Applications, Vol. 36, 
No. 5, pp. 9580-9583, 2009.07 
[4] D. E. Rumelhart, G. E. Hinton and R. J. 
Williams, “Learning internal 
representations by error propagation”, 
Parallel Distributed Processing, Vol. 1, 
MIT Press, 1986. 
[5] B. Cetin, J. Burdick, and J. Barhen, 
“Global descent replaces gradient descent 
to avoid local minima problem in learning 
with ANN,” Proceedings of IEEE 
International Conference on Neural 
Network, Vol. 2, pp. 836-842, 1993. 
[6] X. H. Yu and G. A. Chen, “On the local 
minima free condition of backpropagation 
learning,” IEEE Transactions on Neural 
Networks, Vol. 6, pp. 1300-1303, 1995. 
[7] Lawrence, Steve, Tsoi, Ah Chung; Giles, 
C.Lee, “Local minima and generalization”, 
IEEE International Conference on Neural 
Networks, Vol. 1, pp. 371-376, 1996. 
[8] D. S. Huang, “The local minima-free 
condition of feedforward neural network 
for outer-supervised learning,” IEEE 
Transactions on System, Man, and 
Cybernetics-Part B: Cybernetics, Vol. 28, 
No. 3, pp. 447-480, June 1998. 
[9] Kryzhanovsky, Boris, Magomedov, Bashir; 
Fonarev, Anatoly, “On the probability of 
finding local minima in optimization 
problems”, IEEE International Conference 
on Neural Networks-International Joint 
Conference on Neural Networks 2006, p 
3243-3248, 2006. 
[10] Li, Hong-Ru, Li, Hai-Long, “A global 
optimization algorithm based on 
filled-function for neural networks”, 
Dongbei Daxue Xuebao/Journal of 
Northeastern University, Vol. 28, No. 9, 
pp.1247-1249, September 2007. 
[11] Cho, Yong-Hyun, Hong, Seong-Jun, “An 
efficient global optimization of neural 
networks by using hybrid method”, 
Proceedings of the Frontiers in the 
Convergence of Bioscience and 
Information Technologies, FBIT 2007, pp. 
807-812, 2007. 
[12] Kryzhanovsky, Boris, Kryzhanovsky, 
Vladimir, “Binary optimization: On the 
probability of a local minimum detection 
in random search”, Artificial Intelligence 
and Soft Computing - ICAISC 2008 -9th 
International Conference, Lecture Notes in 
Computer Science, Vol. 5097 LNAI, pp. 
89-100,2008. 
[13] Ninomiya, H., “A hybrid global/local 
optimization technique for robust training 
of microwave neural network models”, 
IEEE Congress on Evolutionary 
Computation, 2009, pp. 2956 – 2962, 
18-21 May 2009. 
[14] A. J. Patrikar, Neural Network Paradigms 
for Adaptive Signal Processing and 
Control, Ph.D. Thesis, Southern Method 
University, Texas, U.S.A, 1992. 
[15] S. Haykin, “Adaptive filter theory, fourth 
edition,” Prentice Hall, 2002. 
 
 
 
 
 2
提問者的問題。於場次結束後，特地與參加學者討論交換了許多探討人工智慧應
用的問題，也針對神經網路應用於此研究提出了許多看法，對個人的研究方面頗
有助益。 
 
2012年6月份，本人又參加了2012 International Conference on Mechanical and 
Electronic Engineering (2012-ICMEE)，此次2012 ICMEE為ISER Association所主
辦，會議在中國大陸安徽省合肥市銀瑞林大飯店國際會議廳舉行，共計有來約五
個國家數百餘位學者及專業人員參加。依據大會敘述，本次會議共約有600篇論文
投稿，經審查後，共接受約300篇論文得以發表於此次大會，會議方式採口頭報告
(Oral)與海報張貼(Poster)兩部份。本人在此次會議中發表了兩篇論文，題目分
別為; 1. The QNN Transmittance Estimator of TP Film with Two Layers Coating for 
Electronic Appliance, 2. The Chromatic Aberration Estimation of TP Film with Two 
Layers Coating for Electronic Appliance。 
 
藉由參與此次會議，本人得再次接觸不同國家的學者專家並認識交談，除對其他
國家於同一領域相關研究有進一步之了解外，並同時對他國的研究情況有更進一
步之認識。經由參與本次會議，由相關的論文發表中，無論是在研究的應用價值
與對問題的探討深度，個人深深以為應多鼓勵國內各大專以上之教師與研究生參
加此類型國際會議並發表論文，藉由吸取相關知識外，並可將國內的研究情形與
水準介紹予他國學者了解，經由與世界各國學者的交談，適時的將國內教育與研
究概況介紹予他們認識。 
 
二、與會心得 
 
兩次會議分別為國際性質會議，規模龐大，發表之論文數量與質量也十分可觀。
藉由參與兩次會議，本人得與接觸不同國家的學者專家並認識交談，除對其他國
家於同一領域相關研究有進一步之了解外，並同時對他國的研究情況有更進一步
之認識。經由參與會議，由相關的論文發表中，無論是在研究的應用價值與對問
題的探討深度，個人深深以為應多鼓勵國內各大專以上之教師與研究生參加此類
型國際會議並發表論文，藉由吸取相關知識外，並可將國內的研究情形與水準介
紹予他國學者了解，經由與世界各國學者的交談，適時的將國內教育與研究概況
介紹予他們認識。 
 
兩次會議中，相關論文的研究應用範圍極為廣泛，大多為電機、資訊相關領域及
其於工業技術的應用。諸多論文的探討均為實際應用取向，相當具有參考價值。
經由會議的參與，個人吸取了與本人研究方向極為相關的知識，對個人未來之研
究幫助甚大，可謂獲益良多。因此，需要處理之工作要項包含整理及參考相關研
究論文，找出具有價值性的文章仔細研讀，並開始研究。 
 
 4
 
 
 6
 
 
 8
 
 
 10
 
 
 12
 
 
 14
 
 
 16
 
 
 18
 
 
 20
 
 
 22
 
 
 24
 
 
 26
 
 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
