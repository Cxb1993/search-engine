II
中文摘要
隨著網路資訊量的大量增加，檢索、擷取、學習和組織網路資訊和知識已經成為智慧型
網際網路資訊系統 (Intelligent Internet Information System，簡稱 I3S) 所要達成的主要目
標。I3S 是整合資料探勘、智慧型代理人系統、資訊擷取、知識表達和資料庫系統等研
究的跨研究領域。我們在第一年度計畫中，研讀 I3S相關研究領域，找出探討文字內容
語意相關的重要研究問題，並提出方法，以完成 I3S 底層子系統 “I3 Web Analyzer
(I3WA)”的系統雛形，並抓取Web data做實驗分析。在第二年計畫中，我們研究並開發
I3S其他系統：I3 Metadata Extractor (I3ME) 和 I3 Knowledge Learner (I3KL)。在今年度
計畫，同時探討 I3ME和 I3KL的相關研究，並完成 I3S系統雛形和介面，讓使用者可
以用於建立領域入口網站。
關鍵字：資料探勘、超連結分析、資訊擷取、資訊檢索、文件分類、語意式網際網路
Abstract
As the Internet information grows, retrieving, extracting, learning, and organizing its
information and knowledge is the main goal of this project, Intelligent Internet Information
System (I3S). I3S system is an integration of data mining, intelligent agents, information
extraction, knowledge representation, and database studies. In the first-year project, we study
related research areas and identify important research issues on exploring content semantics;
propose effective and efficient methods for these issues. We also develop I3 Web Analyzer
(I3WA), the bottom layer of I3S. Then, I3S components, I3 Metadata Extractor (I3ME) and I3
Knowledge Learner (I3KL), are designed and developed in the second year. Experiments
about I3ME and I3KL are also performed to evaluate the effectiveness of I3S. In this year, we
integrate these subsystems to carry out the prototype of I3S and develop manual interface for
the user to interact with I3S while constructing domain portals.
Keywords: Data Mining, Hyperlink Analysis, Information Extraction, Information Retrieval,
Document Categorization, Semantic Web.
2search engines and automatic programs are hard-pressed to extract structured information
from popular HTML pages. Although extensible markup language (XML) [54] is designed to
deliver structured information of a Web page, it’s not yet in popular use. Therefore, research
of Information Extraction (IE) [55][10][18][28][23] is developed to automatically extract
structured information from unstructured or semi-structured Web pages. Machine Learning
(ML) [35] and Data Mining (DM) [38][46][19][24][1][2][43][20][36][11] are then applied to
obtain knowledge from the extracted information and store the knowledge in databases or
knowledge bases. In the project report, we omit related works as the detail is available from
our recent publication [29].
4. Research Methods
In this section, we first describe the issues, idea and concept of I3S as the overview. Then,
each kernel component of I3S is illustrated in following subsections: I3WA, I3ME and I3KL.
4.1 Overview of I3S
In this project, we will develop an Intelligent Internet Information system (I3S). The system
architecture of I3S is shown in Figure 1. I3S consists of three-layer horizontal components
that cooperate with the vertical component, domain knowledge ontology.
Domain
Ontology
I3 Web Analyzer (I3WA)
I3 Metadata Extractor (I3ME)
I3 Knowledge Learner (I3KL)
DB
Web
Documents
KB
Domain
Expert
Figure 1: The architecture of I3 system.
 I3 Web Analyzer (I3WA). This analyzer consists of two basic components: Web focused
crawler (I3Crawler for short) and Web hyperlink and content analyzer (I3Analyzer).
I3Crawler is responsible for gathering various domain-related documents from the Internet.
I3Analyzer analyzes collected documents with corresponding hyperlinks to extract
informative structures and contents.
 I3 Metadata Extractor (I3ME, the 2nd year project). After I3WA collects documents on
certain topics, I3ME is employed to automatically extract metadata (structured
information) from semi-structured or unstructured Web documents. Extracted metadata
consists of rich structured information to facilitate the following learning process, I3KL.
 I3 Knowledge Learner (I3KL, the 3rd year project) [30]. Given initial domain knowledge
(ontology), an intelligent learning system try to discover new and useful knowledge for
enhancing domain knowledge. I3KL will apply several data mining and machine learning
algorithms to obtain knowledge from Web documents preprocessed by I3WA and I3ME.
44.2.4. Stop Words and Stemming Words
Stop words (or negative dictionaries) are commonly found in almost every document.
However, stop words are also highly domain dependent. For example, computer is probably a
stop word in computer literature databases. Word stemming maps multiple representations of
a word into a single stemmed term to provide significant compression and improve the recall
rate of retrieval. However, the precision rate, based on minimizing non-relevant information,
may be reduced. For example, memorial and memorize can be stemmed to memory.
Consequently, word stemming influences recall and precision measures and should be
carefully processed when designing information systems. Generally, the stemming algorithm
removes suffixes and prefixes to derive the final stem. The Porter algorithm [37] is based on a
set of conditions of the stem, suffix and prefix and associated actions. Some stemming
methods are based on dictionaries. Studies of various stemming methods are summarized in
[17][5]. The interpretation of term semantics is dependent on the domain that is defined as the
concept hierarchy (CH) of categorization in this project. CH prototype can be automatically
by I3WA consisting of [13] and [44]. CH is also related to learning classifications of I3KL.
Based on the CH, we first focus on crawling domain-related web pages and exploring term
semantics based on mining term associations from these data [33]. Then, terms without
enough association supports and confidences are processed through stemming and
stopword-filtering.
4.3 I3WA
In first year, we focus on implementing the bottom layer subsystem, I3 Web Analyzer (I3WA).
By interacting with the domain knowledge, I3WA focuses on abilities of crawling and
understanding Web documents and hyperlinks. I3WA preprocesses the Web and extracts
useful information for its following extracting and learning subsystems, respectively I3ME
and I3KL. Basically, I3WA consists of a Web Crawler and a Web Content and Hyperlink
Analyzer. According to the semantic issues introduced in section 4.2, the analyzer must
cooperate with Document Parser and Linguistic Detector to identify content semantics and
linguistic information of a document. I3WA integrates these components to discover
informative structures and contents from Web sites.
 Given a Web site, the crawler first gathers and parses all pages from the site. Content
Analyzer is then used to extract informative contents of each page. Structure Analyzer is
applied to discover the informative structure of the site that contains a set of article pages
linked by a set of hierarchical TOC pages.
Web
Documents
Web
Crawler
Document
Parser
Structure
Analyzer
Linguistic
Detector
Content
Analyzer
DB
Figure 2: I3WA components.
6Figure 4: Linguistic constraints for English and Chinese BIG5 documents.
Figure 5: Structure constraints to focus on crawling Yahoo! directory.
Figure 6: Topic constraints for crawling“bioinformatics”papers.
4.3.2. Document Parser
The implementation of the document parser is based on Microsoft Windows systems.
Windows provides COM (or DCOM) for invoking software components. The document
8Figure 7: Linguistic Detector Data flow diagram.
Experiment and evaluation are performed on five thousands of pages that are randomly
selected from our past search engine caches. The linguistic detector determines the result
encoding of each page for measuring the accuracy, as shown in Figure 8.
Figure 8: Experiment of testing linguistic detector flow chart
We assume that documents which were crawled from Taiwan use BIG5 encoding and
documents which were crawled from mainland China use GB encoding. As shown in Table 1,
the accuracy is the weighted sum of those correct results based on the assumption: “All
Taiwan pages are BIG documents and all China pages are GB.”
FEDCBA
FED
FED
E
FEDCBA
CBA
CBA
A
precision







 \
Input a test
count
Test count > 0?
Doc
caches
DB
Randomly select a
document
Remove tags
Linguistic Detector
Determines the
encoding
Record the result
Output the
results
Big5 and Gb
Class
Character-
probability
tables
Calculate precision
Test count--
Y
N
EOF?
Input a byte
Compare the accumulated
document’s BIG5- and
GB-probability values
Set the input byte
to BIG5 and GB
classes as first byte
Conform to
BIG5 or GB
first byte?
Input next byte
Set the byte to
BIG5 and GB
classes as second
Combine the 2
bytes as a
2-byte
character
Look up probabilities
in both BIG5 and GB
tables
Accumulate document’s
BIG5- and GB-probability
values
Text stream
Y
N
YN
10
Based on the definition, a site is represented as several sub-trees that denote specific types of
pages. Web masters usually use CGI programs to generate different types of pages. Based on
the observation, CGI patterns or URL addresses can be applied to identify templates for these
sub-trees. For example, directory pages of Yahoo! can be represented by
“http://dir.yahoo.com/*/”, where “*” indicates the directory name. 
“http://directory.google.com/Top/*/”is the URL pattern of the Open Directory service
provided by Google. Therefore, Structure Analyzer should be able to discover URL patterns
as sub-trees. In this project, we first use regular expression to represent the generalization and
specialization of URL patterns. Based on these patterns, we will categorize pages of a site to
denote several sub-trees. Then, Structure Analyzer applies HITS algorithm to calculate
authority and hub values of each page. Based on predefined sub-trees information and
hyperlinks, TOC and article pages in each sub-tree can be determined.
HITS algorithm with entropy measure on links is applied to identify TOC pages and article
pages as our past studies [25][26]. The advantage of this method is anti-noisy while pages
contain noisy content blocks like menus, advertisements, shortcuts, etc. We expect that article
pages hold high authority values and TOC pages hold high hub values. However,
identification of sub-directories pages is ambiguous since sub-directories are TOC pages
linked by one or few TOC pages. Should sub-directory-TOC pages hold high hub and
authority values? The answer is not always true since some pages are belonged to both
sub-directory-TOC and article pages. But, these pages are often separated TOC and article
contents into different blocks. In a systematic Web site, a page is always composed of a set of
tables (HTML <TABLE> tag) for presentation purpose. We assume that each <TABLE>
represents to a content block of a page. Therefore, each content block can be individually
calculated hub value, but not authority value. Furthermore, considering the content block is
not isolated by <TABLE>, the block may be any node of the page’s DOM tree. That is the
generalization-specialization process on DOM trees must be considered in our works [13].
However, some pages are not generated by the systematic web site. We propose a novel
approach to automatically construct Concept Hierarchy so that the manual Taxonomy used
Focused Crawler can be replaced by our Concept Hierarchy Generator (CHG). By classifying
types of the Web information into: static and dynamic contents, CHG is able to extract a
website’s concept hierarchy based on analyzing the Entropy distribution of URLs. Folowing 
links of the website root, top-down and bottom-up hyperlink analysis approaches are applied
to extract concept hierarchies of static and dynamic contents, respectively. Experiments show
that our system is able to correctly extract concept hierarchies of both types of websites [44].
4.3.5. Content Analyzer
Given a Web site, after crawling and parsing pages of the site, Content Analyzer tries to
automatically extract informative contents from each page for providing significant
information. Based on our past studies, Content Analyzer will enhance InfoDiscoverer
[31][25] by performing generalization and specialization on the document DOM-tree to get
better results.
Based on the W3C Document Object Model (DOM) [48], an HTML page can be parsed and
represented by a tree structure in which internal nodes indicate HTML tags and leaf nodes
indicate embedded texts. With DOM, programmers can build documents, navigate their
structure, and add, modify, or delete elements and content. Accordingly, Content Analyzer
navigates the DOM-tree to merge (generalize) or split (specialize) texts top-down reachable
from a node. Since there are probably too many leaves in a DOM-tree, finding informative
elements is complex and tedious. According to the statistics of InfoDiscoverer [31], about
70% of Web pages use <TABLE> in presentation. Therefore, we use <TABLE> nodes as the
12
Content Blocks
Structure Blocks
Redundant Blocks
URL: http://edition.cnn.com/
Figure 9: The homepage of CNN International Edition.
In this project, we first propose I3 Block Extractor (I3BE) to partition a page into blocks,
analyze these blocks to identify attributes (CB, SB or RB), and cluster similar blocks into
groups.
4.4.3. Block Partitioner (BP)
Given a page, Document Parser7 parses the HTML source into a DOM-tree and store tree
nodes and structures into the database for easily accessing. BP is applied to determine block
boundaries based on the DOM-tree. The result must be as close as the decision made by
human experts. Processes of BP are as the followings:
 Translate tags and texts into protein sequences. By regarding the DOM-tree nodes as
protein’s amino acid, an HTML page is translated into a protein sequence. We map
HTML tags into amino acids based on the presentation style of each tag. Text length is
also fuzzified into five symbols of amino acids.
 Apply SEG8 [56] to analyze the sequence complexity. SEG is a tool that segments
sequence(s) by local complexity. An HTML sequence can be divided into several low-
and high-complexity segments by SEG.
 Analyze the block boundary based on the sequence complexity. Based on the
DOM-tree of a page, we top-down analyze the sequence complexity as the criterion to find
the optimal block-partition.
We took experiment on pages linked by Mozilla9 that contains 5,242 pages fetched by the
focused crawler of I3WA. In these pages, there are 25 pages linked by the Mozilla Site Map10
page. We manually extract 213 meaningful blocks from these 25 pages. By applying BP to
automatically extract blocks from these pages, we compare blocks partitioned by I3BE and
human experts. Based on overlapped text length between blocks extracted by human and
system, the precision of BP is 90%.
4.4.4. Block Analyzer (BA)
After partitioning a page into blocks, BA tries to analyze the information measure of each
block and identifies the block’s attribute: CB, SB, or RB. By extracting content and anchor
7 The detail is in section 5.2 of the 1st-year project report.
8 ftp://ftp.ncbi.nlm.nih.gov/pub/seg/seg.
9 http://www.mozilla.org/.
10 http://www.mozilla.org/sitemap.html.
14
possible concepts since they are uniformly distributed in each CB. Then, substring filter is
applied to remove concept candidates that are substrings of other concepts. The whole process
is shown in Figure 10.
0where,log)(
1


ij
n
j
ijniji wwwtermEN
, n = |D|， D is the set of pages.
wij is the normalized term frequency in the page j.
Table 8: Mappings of concepts to Dublin Core Metadata.
Dublin Core Metadata Elements Concepts extracted and selected by the user
Title 名稱、景點名稱、
Subject 行程名稱、旅行名稱、關鍵字
Description 資訊 、指南、描述、事項、特色
Type 類別
Date 日期、時間
Publisher 網站名稱，旅行社
Creator 作者、製作
Contributor 電話、傳真、郵件、Email
Format 圖片
Identifier URL、網址
Source Object ID
Coverage 地址、位於、路線、位置、地點言
Language 語言
Relation 交通、費用、門票、住宿、飯店、餐廳
Rights 版權
Figure 10: Attribute extraction process and examples.
To extract values of corresponding attributes from CB, CB texts are translated to a long
sequence based on mappings defined in Table 9. Then, BLAST is applied to extract metadata
pattern so that corresponding texts can be identified and extracted into attribute-value pairs.
Table 9: Mappings of HTML tags and DC metadata fields to symbols.
Tag Name Symbol Tag Name Symbol
TABLE E FONT/I/U/B… Q
CAPTION C P/HR P
16
portal directories are similar to the “category tree”structure. More complex portals even
organize their directories as acyclic graphs. In the project, we focus on dealing with the
hierarchical classification of“category tree”.
In HMC, we hierarchically apply one-against-one SVM (LIBSVM13) to train hierarchical
metadata objects of NMNS. During the training process, manually constructed domain
concept hierarchies are employed to extend feature semantics. Also, DC metadata are
considered in the metadata classification. In other words, the same keyword appearing in
different DC fields is regarded as different features. As for the testing process, based on each
SVM classifier, Hierarchical Classifier routes a new document to the adequate class through
the hierarchical paths as shown in Figure 12. During the hierarchical assignment of class, two
decision strategies are widely used in SVM:
 Probability is default strategy of SVM that outputs the probabilities value for making
decision.
 Voting strategy makes decisions based on the major votes of each one-against-one SVM
classifier.
In this project, we propose an alternative approach:
 Scoring strategy estimates the average of decision value of each classifier to a class. The
class with the maximal value is selected.
Virtual category tree Virtual directed acyclic category
Category tree Directed acyclic category graph
Figure 11: Types of taxonomy.
Doc Doc
Flat Classification Hierarchical Classification
Figure 12: Flat Classification vs. Hierarchical Classification.
13 http://www.csie.ntu.edu.tw/~cjlin/libsvm/.
18
5.1 Using I3S to Construct Travel Domain Portal
I3S is divided into following subsystems: Crawler and Parser, Page Partition, Attribute
Extractor, Metadata Block Extractor, and Result. Users can interact with each subsystem
through interface as shown in Figure 14.
Figure 14: Main page of I3S.
 Crawler and Parser. This step defines domain-related pages in the focused crawler via
specifying related keywords and site URLs. Then, users can manually select pages from
TopN Google’s search results and pages of these sites, as shown in Figure 15. These
selected page groups are parsed for further analysis.
Figure 15: Specify domain keywords and URLs and select crawled page groups.
 Page Partition System. As shown in Figure 16, this process partition each selected page so
that users can verify extracted blocks and identified properties: CB, SB, or RB.
20
Figure 18: Manually selected attributes.
Figure 19: Mappings to DC fields.
 Metadata Block Extractor. The system estimates the score of each block based on
attributes scores. Then, similar blocks with higher scores are grouped to denote one
metadata block template. Attribute-value pairs of the template are also extracted and
marked as shown in Figure 20. Users can remove or add new attributes, or directly define
A-V pair, as shown in Figure 21, Figure 22, and Figure 23, respectively. Finally, metadata
extracted from the block is shown in Figure 24 and Figure 25.
22
Figure 22: Press right-button of mouse to add attributes.
Figure 23: User-define attributes and values.
Figure 24: Attributes and values extracted by the system.
Figure 25: Select DC fields.
 Result. The interface is used to evaluate the result of the whole system as shown in Figure
26.
24
Figure 27: Applying I3S to interesting domains.
Figure 28: Integrate I3S application with research results of other fields.
6. References
[1] R. Agrawal, J. Gehrke, D. Gunopulos and P. Raghavan, “Automatic Subspace Clustering of High 
Dimensional Data for Data Mining Applications,” Proceedings of the ACM SIGMOD International 
Conference, pages 94-105, 1998.
[2] R. Agrawal and R Srikant, “Fast Algorithms for Mining Association Rules,” Proceedings of the 20th 
International Conference on VLDB, September 1994.
[3] Altschul, S.F., Gish, W., Miler, W., Myers, E.W. and Lipman, D.J., “Basic Local Alignment
Search Tool,”Journal of Molecular Biology, 215:403-410, 1990.
[4] Altschul, S.F., Madden, T.L., Schäffer, A.A., Zhang, J., Zhang, Z., Miller, W. and Lipman, D.J.,
“Gapped BLASTand PSI-BLAST: A New Generation of Protein Database Search Programs,”
Nucleic Acids Research, 25(17): 3389-3402, 1997.
[5] R. Baeza-Yates, “Modern Information Retrieval,” Addison Wesley, 1999.
[6] T. Berners-Lee, T. R. Cailiau, etc., “The World-Wide Web,” Communications of the ACM, 37(8):76-82,
August 1994.
[7] T. Berners-Lee, “Semantic Web Road Map,” http://www.w3.org/DesignIssues/Semantic.html.
[8] K. Bharat and M. R. Henzinger, “Improved Algorithms for Topic Distillation in a Hyperlinked
Environment,” Proceedings of the ACM SIGIR International Conference on Information Retrieval, 1998.
[9] S. Brin and L. Page, “The Anatomy of a Large-scale Hypertextual Web Search Engine,” Proceedings of the 
7th International World Wide Web Conference, 1998.
[10]C. Cardie, “Empirical Methods in Information Extraction,” AI Magazine, 18(4):5-79, 1997.
