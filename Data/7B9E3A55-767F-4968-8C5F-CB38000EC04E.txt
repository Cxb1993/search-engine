Abstract
Algebraic-geometry (AG) codes open new and rather exciting possibility in coding theory
for finding a sequence of asymptotically good linear codes that meet or exceed the Gilbert-
Varshamov bound. To make AG codes competitive in real world applications, the theory of
AG codes must be made easier to be understood and the encoding and decoding complexity of
AG codes be reduced to rival that of Reed-Solomon codes. The decoding of AG codes follows
the same four-step procedure as that in the decoding of Reed-Solomon codes: syndrome gen-
eration, error-locator polynomial finding, error-location search, and error-value evaluation.
The main goal of this project is to develop efficient algorithms and hardware architectures
for the encoding and decoding of existent AG codes, such as Hermitian codes, and by ex-
ploiting a special tower of function fields, to construct a series of new AG codes with good
asymptotical behaviors. In this project, a series-in-series-out hardware architecture for a sys-
tematic encoding scheme of Hermitian codes is developed with complexity similar to that for
a systematic encoder of classical cyclic codes. The upper bounds of the numbers of memory
elements and constant multipliers in the proposed architecture are shown both proportional
to O(n), where n is the codeword length. And to encode a codeword of length n, this archi-
tecture takes n clock cycles without any latency. A parallel early stopped Berlekamp-Massey
(PESBM) algorithm is developed in this project for finding error-locator polynomials in the
decoding of one-point AG codes. The computation complexity of the PESBM algorithm is
O(γn2), where n is the codeword length and γ is the smallest nonzero nongap of the point
on the algebraic curve over which the code is defined. Due to the early stopped property, the
PESBM algorithm is the most efficient algorithm compared with those in the literature. A
pipeline hardware architecture of the PESBM algorithm via one-dimensional systolic array
is also developed in this project. This implementation requires only a series of t+ ⌊g−1
2
⌋+1
processing elements, called PE cells, g delay units, called D cells, and γ operation elements,
Contents
Abstract 1
Contents i
List of Figures vi
List of Tables ix
1 Introduction 1
1.1 Algebraic-Geometry Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Systematic Encoding of Hermitian Codes . . . . . . . . . . . . . . . . . . . . 5
1.3 Determination of Error-Locator Polynomials . . . . . . . . . . . . . . . . . . 8
1.4 Implementation of Algorithms for Finding Error-Locator Polynomials . . . . 10
1.5 Implementation of Error-Value Evaluator for Hermitian Codes . . . . . . . . 11
1.6 Construction of New Classes of Algebraic-Geometry Codes . . . . . . . . . . 13
2 A Serial-In-Serial-Out Systematic Encoder for Hermitian Codes 14
i
4.1.3 Description of the PESBM Algorithm . . . . . . . . . . . . . . . . . . 67
4.2 A Systolic Array Implementation . . . . . . . . . . . . . . . . . . . . . . . . 70
4.2.1 An Implementation of Left-column Replacement Algorithm via Sys-
tolic Array . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.2.2 Reduction of Hardware Complexity . . . . . . . . . . . . . . . . . . . 74
4.3 Cell Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.4 A Design Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.5 Analysis of Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5 An Efficient Architecture of Error-Value Evaluator for Hermitian Codes 95
5.1 A Generalized Forney Formula for Error-Value Evaluation of Hermitian Codes 95
5.1.1 Theories for the Calculation of the Separating Functions . . . . . . . 97
5.2 An Algorithm for the Error-value Evaluation of Hermitian Codes with One
Error-Locator Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
5.3 An Efficient Hardware Architecture for the Error-Value Evaluation of Hermi-
tian Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5.3.1 Functions of the Error-Value Solver . . . . . . . . . . . . . . . . . . . 100
5.3.2 Main Circuits of the Error-Value Evaluator . . . . . . . . . . . . . . . 103
5.3.3 The Division Circuit in the Error-Value Evaluator . . . . . . . . . . . 110
5.4 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
iii
.2 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Bibliography 164
v
4.1 The extended syndrome matrix M of the Hermitian code. . . . . . . . . . . 72
4.2 Data dependency graph of Gaussian elimination without column exchange:
(a) single elimination subgraph with k = 0 of the given example; (b) unrolled
space-time solid. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.3 The data flows of two-dimensional array and one-dimensional array of Gaus-
sian elimination without column exchange. . . . . . . . . . . . . . . . . . . . 92
4.4 The input and output definitions of a PE cell and a D cell. . . . . . . . . . . 93
4.5 The input and output definitions of a OE cell. . . . . . . . . . . . . . . . . . 93
4.6 The circuit for computing data discrepancies and available candidates in the
cell OE(i). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.7 The circuit for updating coefficient vectors in the cell OE(i). . . . . . . . . . 94
4.8 The proposed implementation of the PESBM algorithm via systolic array for
the Hermitian code CΩ(23Q). . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.1 Inputs and Outputs of the error-value solver. . . . . . . . . . . . . . . . . . . 101
5.2 A Horner’s loop. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.3 A circuit for calculating the coefficients φ
(NI)
NP,a,b and φ
(NI)
NP,a+q+1,b of the polyno-
mial φ(x, y) for a fixed a, 0 ≤ a ≤ a(NI−1)NP . . . . . . . . . . . . . . . . . . . . 107
5.4 A circuit for calculating the error value eNP . . . . . . . . . . . . . . . . . . . 110
5.5 A three-dimensional representation of data dependencies of computations in
the division algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
vii
List of Tables
2.1 A list of the values of di, δij and δ
′
ij in the (64,48) Hermitian code. . . . . . . 27
4.1 The truth table of E(IR)j and k(IR)j for the cell PE(j), for a given Hermitian
code CΩ(23Q) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.2 The transition tables of the state machine in OE cells, for a given Hermitian
code CΩ(23Q). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.1 Hardware and time complexities of each step in Function I. . . . . . . . . . . 122
ix
Generation and Error Locator Search in the decoding of Hermitian codes” (with project
code NSC 89-2213-E-007-105), we have proposed the architectures for syndrome generation
and error location search in the decoding of Hermitian codes, which is published in [6].
The remaining works in the decoding of Hermitian codes are determining the error-locator
polynomial(s) and evaluating the error values.
1.1 Algebraic-Geometry Codes
In this section, we will give a trimmer presentation of AG codes on plane curves to introduce
basic concepts related to our work. For more details on the theory of AG codes, please
refer to the articles and the references therein in the special issue on AG codes of the IEEE
Transactions on Information Theory in November, 1995 or the books [5, 7].
Let GF (q) be a finite field with q elements. On the set GF (q)3 \ {(0, 0, 0)} of nonzero
3-tuples over GF (q), an equivalent relation “≡” is defined as (a0, a1, a2) ≡ (b0, b1, b2) if and
only if there exists a nonzero λ in GF (q) such that λai = bi for i = 0, 1, 2. The equivalent
class of (a0, a1, a2) is denoted by (a0 : a1 : a2). Let P
2 be the set of all such equivalent
classes, usually called a projective plane over GF (q). And let A2 be the set GF (q)2 of all
2-tuples over GF (q), usually called an affine plane over GF (q). Each affine point Pa in A2
with Pa = (a¯1, a¯2) can be identified with a unique equivalent class (or called projective point)
P = (a0 : a1 : a2) in P
2 with a0 6= 0 by letting a¯i = ai/a0 for i = 1, 2. The set of all the other
points P = (a0 : a1 : a2) with a0 = 0 forms the so-called line at the infinity in the projective
plane P2.
Let F (x0, x1, x2) be a homogeneous polynomial over GF (q) in three indeterminants. A
projective curve is defined by the following equation: F (x0, x1, x2) = 0. We say that F (P ) =
0, i.e., P is a zero of F , if F (a0, a1, a2) = 0, where P = (a0 : a1 : a2). And such a point
P = (a0 : a1 : a2) in the projective plane P
2 over GF (q) is called a rational point of the
projective curve defined by F . Dividing each term of F by the monomial xl0 with l the
2
Let L(mQ) denote the vector space over GF (q) consisting of all polynomials f(x, y) in
x and y over GF (q) such that the pole order of f(x, y) at Q is less than or equal to m.
Hereafter, we shall assume that m ≥ 2g − 1. And by the Riemann-Roch Theorem, the
dimension of L(mQ) is m−g+1. Consider a set B(m) = {foi(x, y)}1≤i≤m−g+1 of monomials
foi(x, y) with pole order oi at Q such that o1 < o2 < · · · < om−g+1. It is clear that for each
nongap j, j ≤ m, there is exactly one monomial foi in B(m) such that oi = j. A monomial
xayb of the same pole order oi at Q as foi is called a consistent term of foi if x
ayb 6= foi .
Such a consistent term xayb can be expressed as a linear combination of fok ’s, 1 ≤ k ≤ i [9].
Thus B(m) is a basis of the vector space L(mQ) over GF (q). Let γ be the smallest nonzero
nongap at Q. The basis B(m) = {foi}1≤i≤m−g+1 of L(mQ) is called a standard basis if ℓ and
ℓ− γ are nongaps at Q, then [10]
fℓ = fγfℓ−γ. (1.2)
Notice that for any nonnegative integers m and m′ with m < m′, we have L(mQ) ⊆ L(m′Q),
and thus we can construct standard bases B(m) and B(m′) for L(mQ) and L(m′Q) such
that B(m) ⊆ B(m′). Thus a tower B(2g − 1) ⊆ B(2g) ⊆ B(2g + 1) ⊆ · · · of standard bases
can be constructed for L(mQ)’s, m ≥ 2g − 1. For the Hermitian curve of degree 5 in (1.1),
the vector space L(mQ) with m = 23 has a standard basis B(23) consisting of m−g+1 = 18
monomials:
B(23) =
{
1, x, y, x2, xy, y2, x3, x2y, xy2, y3, x4, x3y, x2y2, xy3, x5, x4y, x3y2, x2y3
}
. (1.3)
Note that the monomial y4 with pole order 20 is absent and is a linear combination y + x5
of the two monomials y and x5.
Suppose that there are n finite rational points Pi over the symbol field GF (q), 1 ≤ i ≤ n,
on the curve. With m ≥ 2g − 1 assumed, the m− g + 1 monomials foi, 1 ≤ i ≤ m− g + 1,
of the basis B(m) of the vector space L(mQ) can be used to construct an (m− g + 1)× n
4
over the same symbol field. For example, a (64, 32, 27) Hermitian code and a (16, 8, 9) ex-
tended Reed-Solomon code, both over the same symbol field GF (16), have the same rate
1/2. But over practical channels, the (64, 32, 27) Hermitian code has far better performance
than the (16, 8, 9) extended Reed-Solomon code [11]. In general, Hermitian codes are q times
longer than Reed-Solomon codes over the same symbol field GF (q2) [13]. Thus the goal of
this paper is to develop a serial-in-serial-out architecture for systematic encoding of Hermi-
tian codes that is about q times larger than for Reed-Solomon codes and is able to target a
low area of implementation.
Let Hq denote the Hermitian curve yq + y = xq+1 over the field GF (q2) with genus g =
q(q − 1)/2. It is well known that Hq has a single rational point Q at infinity and q3 finite
rational points Pi = (αi, βi), i = 0, · · · , q
3 − 1, with βqi + βi = α
q+1
i [5, 7]. Let GF (q
2)(x, y)
be the algebraic function field associated with the Hermitian curve Hq : yq + y = xq+1 over
GF (q2), called the Hermitian function field over GF (q2). For any non-negative integer m,
let L(mQ) denote the vector space over GF (q2) consisting of all rational functions in the
Hermitian function field GF (q2)(x, y) whose pole divisors are upper bounded by mQ. For
m > 2g − 2, the dimension of L(mQ) over GF (q2) is equal to m − g + 1. Since the pole
orders ox and oy of the rational functions x and y at Q are q and (q + 1) respectively, the
elements foi = x
aiybi with 0 ≤ ai ≤ q, 0 ≤ bi and oi = aiq + bi(q + 1) ≤ m form a basis of
L(mQ). The Hermitian code Cm of length n = q3 is defined to be
Cm = CL(D,mQ) ≡ {(f(P0), · · · , f(Pn−1)) ∈ GF (q
2)n|f ∈ L(mQ)},
where D =
∑q3−1
i=0 Pi is the sum of all places of degree one (except Q) of the Hermitian
function field GF (q2)(x, y). For 2g − 2 < m ≤ q3, the dimension k(m) of Cm is equal to
dimL(mQ) = m− g + 1, i.e.,
k(m) = m− g + 1. (1.5)
The minimum distance dmin(m) of Cm satisfies
dmin(m) ≥ q
3 −m. (1.6)
6
Unlike the brute-force matrix multiplication or the encoding schemes in [13,14], a novel and
elegant approach for the systematic encoding of Hermitian codes is developed in [15] by em-
ploying a GF (q2)[t]-module structure of Hermitian codes. However, to our best knowledge,
there is no serious discussion on the hardware implementation complexity of this systematic
encoding scheme in the literature. Therefore in Chapter 2, we will develop a serial-in-serial-
out hardware architecture, similar to a classical cyclic encoder, for the systematic encoding
scheme proposed in [15] and to demonstrate that the hardware complexity of this archi-
tecture is much less than that of the brute-force serial-in-serial-out systematic encoding by
matrix multiplication.
1.3 Determination of Error-Locator Polynomials
The algebraic decoding procedure by searching the error-locator polynomial(s) and then
the error locations is the common principle of the available decoding algorithms for AG
codes. Justesen et al. [16] first contributed to the simplified understanding of AG codes and
presented an algorithm for decoding codes constructed from nonsingular irreducible algebraic
plane curves [5, 7]. This algorithm can correct up to d∗/2 − k2/4 errors with complexity of
order O(n3), where n is the code length, d∗ is the designed minimum distance, and k is the
degree of the curve.
Based on the Gaussian elimination and by iteratively determining unknown syndromes
from the known syndromes, Feng and Rao proposed an efficient decoding algorithm in [9],
usually called the Feng-Rao algorithm, which can correct up to ⌊(d∗−1)/2⌋ errors with com-
plexity of order O(n3). The Feng-Rao algorithm applies the modified fundamental iterative
algorithm (FIA) [17] followed by the majority voting scheme (MVS) [9]. The execution of
the modified FIA on a syndrome matrix enclosing unknown entries is similar to the opera-
tion of Gaussian elimination on columns without column exchange [18]. After applying the
modified FIA, there are (at least one) candidate values for the (first) confronted unknown
8
where γ < k is the order of the first nonzero nongap in the function space associated with
the code. The Ko¨tter’s algorithm has a regular and simple structure and is suitable for VLSI
implementation [10].
Among these algorithms of determining the error-locator polynomial(s), one can distin-
guish them into two types: the Gaussian-elimination-type algorithm, such as the Feng-
Rao algorithm, and the Berlekamp-Massey-type algorithm, such as the Sakata et al.’s algo-
rithm. The Berlekamp-Massey-type algorithm seems to be more efficient than the Gaussian-
elimination-type algorithm, but the latter is more easily understanding than the former.
Recently, we introduced an excursion from a restricted Gaussian elimination to several
aspects of the Berlekamp-Massey algorithm [24], where it can be seen that the prominent
features of the Berlekamp-Massey algorithm can be fully reached naturally by operating a
restricted Gaussian elimination on an appropriate syndrome matrix which has structural
properties. The formulation taken in [24] makes the deep insights of the Berlekamp-Massey
algorithm crystal clear. By applying and extending the formulation in [24] for the decoding
of BCH codes, a simple and efficient decoding algorithm for AG codes, called parallel early
stopped Berlekamp-Massey (PESBM) algorithm, will be developed in Chapter 3 of this
report.
1.4 Implementation of Algorithms for Finding Error-
Locator Polynomials
An implementation of the Sakata et al.’s algorithm has been reported in [25,26]. Based on the
Sakata et al.’s algorithm, an algorithmic and architectural result for a decoder of Hermitian
codes has been presented in [27]. Moreover, by applying a set of parallel Berlekamp-Massey
algorithms, an implementation of the parallel Sakata et al.’s algorithm has been designed by
Ko¨tter in [10].
10
the decoding of BCH codes and Reed-Solomon codes. For each potential error point Q, a
separating function σQ is constructed with the property that it vanishes at all potential error
points except the point Q. Then the error value eQ at the point Q is equal to S(σQ)/σQ(Q),
where S(σQ) can be calculated from coefficients in the function σQ and some syndromes.
Several ways to construct the separating function σQ from a Gro¨bner basis of the error
locator ideal are firstly proposed in [29]. Then an efficient algorithm for the calculation
of error values using the separating functions derived in [29] are developed in [30]. Since
the above approach involves changing the Gro¨bner bases of the error-locator ideal with
respect to different monomial orderings in a multi-variate polynomial ring, the process for
calculating the separating functions is messy and may not have efficient hardware architecture
for practical implementations.
Instead of changing the Gro¨bner bases, two simple methods which can either construct
the separating function σQ for the (potential) error point Q or generate a new error-locator
polynomial with a small degree of vanishing in Q is derived in [31]. Also, an algorithm for
evaluating the error values based on the above method is proposed in [31]. Although the
algorithm still requires the knowledge of a reduced Gro¨bner basis for the error-locator ideal,
it can be easily modified to the case when only one error locator polynomial is given. Thus
the process for calculating the error-locator ideal can be stopped immediately once an error
locator polynomial is found, and the time and hardware complexities for searching the error
locator-ideal and solving the error locations will be greatly reduced. Therefore, in Chapter
5, we will modify the algorithm in [31] and propose an efficient architecture via systolic
arrays for the determination of error values in the decoding of Hermitian codes when only
one error-locator polynomial is given.
12
Chapter 2
A Serial-In-Serial-Out Systematic
Encoder for Hermitian Codes
In this chapter, we first review the basic properties of Gro¨bner bases for ideals and modules
and present the systematic encoding scheme of Hermitian codes as developed in [15]. We
then develop a serial-in-serial-out hardware architecture, similar to a classical cyclic encoder,
for this systematic encoding scheme. The complexity of our architecture will be analyzed.
2.1 Gro¨bner bases and Systematic Encoding of Hermi-
tian Codes
In this section, we first describe a module structure of Hermitian codes Cm under a given per-
mutation [12,15]. Then we review some basics of Gro¨bner bases of modules over polynomial
rings [15, 36]. Finally we present a systematic encoding scheme for Cm by the construction
of a reduced Gro¨bner basis for the module structure of Cm as developed in [15].
2.1.1 Module Structures of Hermitian Codes
Let PGF (q2)(x,y) and DGF (q2)(x,y) be the set of all places and the divisor group of the Hermi-
tian function field GF (q2)(x, y) over GF (q2) respectively. Let Sn be the symmetric group
14
arbitrary place in Oi as Pi,0, and set the places Pi,j in orbit Oi as Pi,j = σ
j(Pi,0), 1 ≤ j ≤ ni.
Then we have Pi,ni = Pi,0 and in general, we define Pi,j = Pi,j(mod ni)
, ∀j ∈ Z. It is clear
that Pi,j−1 = σ
−1(Pi,j). Then the components of codewords corresponding to the places Pi,j
in the same Oi can be permuted cyclically by σ.
To construct a module structure of the Hermitian code Cm, we first rearrange the compo-
nents of a codeword according to the new labeling of places Pi,j. Then a codeword v in Cm
can be represented as an ℓ-tuple of polynomials in GF (q2)[t]:
v = (v1(t), v2(t), . . . , vℓ(t)), (2.3)
where vi(t) =
∑ni−1
j=0 f(Pi,j)t
j and f ∈ L(mQ). Since
tvi(t)(mod t
ni − 1) =
∑ni−1
j=0 f(Pi,j)t
j+1(mod tni − 1)
=
∑ni−1
j=0 f(Pi,j−1)t
j =
∑ni−1
j=0 f(σ
−1(Pi,j))t
j (2.4)
for each i, we can see from (2.1) that multiplying a codeword v = (v1(t), v2(t), . . . , vℓ(t))
in Cm by t is equivalent to permuting the codeword locally cyclically by σ. Thus Cm is
closed under multiplication by t modulo tni − 1 at each ith coordinate in (2.3). Since Cm is
GF (q2)-linear, Cm can be represented as a submodule of the GF (q
2)[t]-module
M = ⊕ℓi=1(GF (q
2)[t]/(tni − 1)), (2.5)
which is the direct sum of the quotient rings GF (q2)[t]/(tni − 1).
For convenience to compute the Gro¨bner bases, we consider the following canonical epi-
morphism
π : GF (q2)[t]ℓ → ⊕ℓi=1(GF (q
2)[t]/(tni − 1)).
Let ei be the ith standard basis vector in the GF (q
2)[t]-module GF (q2)[t]ℓ and X i =
(tni − 1)ei, for i = 1, · · · , ℓ. Define C¯m ≡ π−1(Cm) which is a submodule of the free
GF (q2)[t]-module GF (q2)[t]ℓ and is generated by all codewords in Cm (regarded as vec-
tors in GF (q2)[t]ℓ) and all X i’s. Note that Cm is isomorphic to the quotient module
C¯m/(X1, . . . ,Xℓ).
16
are called the nonstandard monomials forW and the monomials not in LT (W) the standard
monomials. As can be seen in [36], every non-zero submodule W of M¯ has a Gro¨bner basis
G, which can be computed by Buchberger’s algorithm, and is generated by G, i.e., W = (G).
A Gro¨bner basis G = {g1, . . . , gs} is called reduced if lc(gi) = 1 for all i and for each i,
no term in gi is divisible by any lm(gj) for j 6= i. With the POT ordering (in fact, any
monomial ordering), every Gro¨bner basis can be transformed to a reduced Gro¨bner basis,
which is unique for every non-zero submodule W of M¯ [36]. Such a reduced Gro¨bner basis
G = {g1, . . . , gs} cannot have two i 6= j such that ek divides both lm(gi) and lm(gj) for
some k. Otherwise, lm(gi) = lt(gi) = t
nek and lm(gj) = lt(gj) = t
mek, and the leading
monomial of gi will divide the leading term of gj if n ≤ m or the leading monomial of gj
will divide the leading term of gi if n ≥ m, a contradiction. By the pigeon-hole principle,
the size of the reduced Gro¨bner basis of a submodule of M¯ is no more than ℓ [15]. From
now on, we assume all the Gro¨bner bases we use are reduced.
Now consider the submodule C¯m of M¯ associated with the Hermitian code Cm. Since the
generators of the submodule C¯m include X i = (t
ni − 1)ei, the (reduced) Gro¨bner basis G
(with respective to the POT ordering) of C¯m must contain exactly ℓ vectors of the form
g1 = (g11(t), g12(t), . . . , g1ℓ(t)),
g2 = (0, g22(t), . . . , g2ℓ(t)),
...
gℓ = (0, 0, · · · , 0, gℓℓ(t)),
(2.7)
where deg gii ≤ ni for all i and deg gij < deg gjj for all 1 ≤ i < j. Once the (reduced)
Gro¨bner basis G = {g1, . . . , gℓ} of C¯m is computed, a division algorithm with respect to G
can be applied to a vector v = (v1(t), . . . , vℓ(t)) ∈ M¯ to obtain the following representation
v = q1(t)g1 + · · ·+ qℓgℓ + v¯, (2.8)
where qi(t) ∈ GF (q2)[t] for all i and v¯ is a unique liner combination of standard monomials
for C¯m. The vector v¯ in (2.8) is called the remainder of v with respect to G. The division
algorithm is described as follows. At the first step, we let v = v1 = (v11(t), . . . , v1ℓ(t)) and
divide v11(t) by g11(t) to obtain a quotient q1(t) and a remainder r1(t) with r1(t) = 0 or
18
in C¯m, a contradiction. Conversely, given a coset leader u = (u1(t), . . . , uℓ(t)) in M¯ with
ui(t) =
∑ni−1
j=ri
uijt
j , we can compute its remainder u¯ = (u¯1(t), . . . , u¯ℓ(t)), u¯i(t) =
∑ri−1
j=0 u¯ijt
j ,
with respective to G, such that v = u− u¯ is a coset leader which is in C¯m and corresponds to
a codeword in Cm. Note that v and u have the same nonstandard terms uijt
jei, 1 ≤ i ≤ ℓ,
ri ≤ j ≤ ni−1. In summary, each codeword in Cm corresponds uniquely to a specification of
nonstandard terms to a coset leader in M¯. Since there are ki = ni−ri nonstandard terms in
the ith coordinate, we have the GF (q2)-dimension of Cm to be k(m) =
∑ℓ
i=1 ki = n−
∑ℓ
i=1 ri.
Thus we can regard the coefficients and the monomials of the nonstandard terms in a
codeword of Cm as the information digits and the information positions, respectively. And
we regard the coefficients and the monomials of the standard terms in a codeword of Cm as
the parity-check digits and parity-check positions, respectively, where the number of parity-
check digits for Cm is r(m) =
∑ℓ
i=1 ri.
With u = (u1(t), . . . , uℓ(t)), where ui(t) =
∑ni−1
j=ri
uijt
j , and v = u − u¯, where u¯ =
(u¯1(t), . . . , u¯ℓ(t)), u¯i(t) =
∑ri−1
j=0 u¯ijt
j , is the remainder of u with respective to the (reduced)
Gro¨bner basis G, the mapping u 7→ v is a systematic encoding of the Hermitian code Cm
since v is a codeword in Cm (as a submodule ofM = ⊕ℓi=1(GF (q
2)[t]/(tni − 1))) and u and
v have the same nonstandard terms.
Example 1. Consider the Hermitian code C53 defined by the Hermitian curve H4 = x5 −
y4− y over GF (42). We have n = 64, k(53) = 48, r(53) = 16 and dmin(53) = 11 in this case.
Let σ be the GF (42)-automorphism of the Hermitian function field GF (42)(x, y)
σ =
{
x 7→ αx
y 7→ α5y,
(2.9)
where α is a primitive element of GF (42) and then α5 is a primitive element of the subfield
GF (4) [37]. Under the action of σ, supp(D) can be decomposed into six orbits
O1 = {σj((1, α1))|j = 0, . . . , 14}, O2 = {σj((1, α2))|j = 0, . . . , 14},
O3 = {σ
j((1, α4))|j = 0, . . . , 14}, O4 = {σ
j((1, α8))|j = 0, . . . , 14},
O5 = {σj((0, α0))|j = 0, 1, 2}, O6 = {(0, 0)}.
20
cyclic encoder, for the systematic encoding scheme of Hermitian codes, we introduce here a
serial representation for the information block u and the codeword v as follows. We define
the information polynomial and the codeword polynomial as
u(t) =
ℓ∑
i=1
ui(t) t
∑ℓ
j=i+1 nj and v(t) =
ℓ∑
i=1
vi(t) t
∑ℓ
j=i+1 nj ,
which correspond to the vectors in GF (q2)n
u = (0, . . . , 0, uℓ,rℓ, . . . , uℓ,nℓ−1, . . . , 0, . . . , 0, u2,r2, . . . , u2,n2−1,
0, . . . , 0, u1,r1, . . . , u1,n1−1) and
v = (vℓ,0, . . . , vℓ,nℓ−1, . . . , v2,0, . . . , v2,n2−1, v1,0, . . . , v1,n1−1),
(2.11)
in the sense that we will serial in the coefficients of the information polynomial u(t) (with
appropriately padded zero symbols) to the encoder and serial out the coefficients of the
codeword polynomial v(t) from the encoder both in the descendent degree order.
The systematic encoding scheme amounts to compute the remainder u¯ of the information
vector u in M with respective to G by the division algorithm described in Subsection 2.1.2.
To have the serial-in-serial-out feature, when the first miniblock u1 = (0, . . . , 0, u1,r1, . . . , u1,n1−1)
of the information block u in (2.11) is shifted into the encoder from u1,n1−1 to u1,0 = 0, the
first miniblock v1 = (v1,0, . . . , v1,n1−1) of the codeword v should be shifted out of the encoder
simultaneously from v1,n1−1 to v1,0. During this first period [1, n1] of times, we need to calcu-
late the parity-check symbols v1j = −u¯1j for 0 ≤ j ≤ r1−1, where u¯1j is the coefficient of the
first component u¯1(t) of the remainder vector u¯ = (u¯1(t), . . . , u¯ℓ(t)) of u = (u1(t), . . . , uℓ(t))
(as a vector in M¯) with respective to the (reduced) Gro¨bner basis G. By the division algo-
rithm described in Subsection 2.1.2, u¯1(t) is the remainder of u1(t) divided by g11(t) with
quotient q1(t) whose degree deg q1(t) ≤ k1 − 1. This is similar to the function of the sys-
tematic encoder for a classical cyclic code [38] and can be implemented by a divide-by-g11(t)
circuit as shown in Figure 2.1 with i = 1.
In general, during the ith period [n1+· · ·+ni−1+1, n1+· · ·+ni] of times, 2 ≤ i ≤ ℓ, we need
to calculate the parity-check symbols vij = −u¯ij for 0 ≤ j ≤ ri−1, where u¯ij is the coefficient
of the ith component u¯i(t) of the remainder vector u¯. And by the division algorithm described
22
≤ max{k1 − 2, k2 − 2, . . . , ki−1 − 2, ki − 1}+ (nj − kj − 1), (2.13)
since deg gij(t) < deg gjj(t) = nj − kj for all i < j. Now consider the case of i = m. Since
qm(t) is the quotient of u
′
m(t) = um(t)−
∑m−1
i=1 qi(t)gim(t) divided by gmm(t), we have
deg qm(t) = deg u
′
m(t)− deg gmm(t)
≤ max{deg um(t), max
1≤i≤m−1
{deg(qi(t)gim(t))}} − (nm − km)
≤ max{nm − 1, max
1≤i≤m−1
{max{k1 − 2, k2 − 2, . . . , ki−1 − 2, ki − 1}
+(nm − km − 1)}} − (nm − km)
= max{k1 − 2, k2 − 2, . . . , km−1 − 2, km − 1},
where the second inequality is from (2.13). This completes the proof. 2
Now, we let di = max{k1 − 2, k2 − 2, . . . , ki−1 − 2, ki − 1}, the upper bound of deg qi(t)
in (2.12). Note that di ≥ ki − 1. Then the degree of u′i(t) will be upper bounded by
deg u′i(t) = deg qi(t) + deg gii(t) ≤ di + ri.
Now the second difficulty can be easily removed by closing the switch, which is controlled
by the signal sci and is initially open, in the divide-by-gii(t) circuit shown in Figure 2.1
at the time t = n1 + · · · + ni−1 − (di − ki), just (di − ki + 1) clocks before the ith period
[n1 + · · · + ni−1 + 1, n1 + · · · + ni] of times. The coefficients of the quotient qi(t) will be
generated during the period [n1 + · · ·+ ni−1 − (di − ki), n1 + · · ·+ ni−1 + ki]. The switch of
the divide-by-gii(t) circuit will be open again at the time t = n1+ · · ·+ni−1+ki+1 in order
to produce the remainder u¯i(t), i.e., the parity-check symbols.
Next, we deal with the first difficulty, i.e., the synchronization problem. We note that
the coefficient of the term tkj−1 of the quotient qj(t) is generated when the information
symbol uj,nj−1 is input to the encoder at the time t = n1 + · · · + nj−1 + 1. If we were
starting to multiply qj(t) by gji(t), j < i, at the same time as the coefficients of qj(t) begin
to be generated, the coefficient of the term tkj−1+deg gji(t) of the product qj(t)gji(t) would
24
as the quotient output of the division of u′i(t) by gii(t), by a cyclic shift register of length
di + 1 as shown in Figure 2.3. The multiplexer, controlled by the signal mci, in the cyclic
shift register will select its input from the quotient output of the divide-by-gii(t) circuit while
the coefficients of the quotient qi(t) are being generated. After the computation of qi(t) is
completed, the multiplexer will select its input from the cyclic feedback of the shift register.
Now to put δij delay elements for the multiply-by-gij circuit, i < j, to delay its input qi(t) is
equivalent to connect the input of the multiply-by-gij circuit to the signal xδ′ij of the cyclic
shift register in Figure 2.3, where δ′ij = δij mod (di+1). The serial-in-serial-out architecture
for the systematic encoding of the ith information miniblock ui = (0, . . . , 0, ui,ri, . . . , ui,ni−1)
is shown in Figure 2.4, together with the multiplication circuits for later use. The overall
circuit in Figure 2.4 is called the ith level of the serial-in-serial-out architecture for the
systematic encoding. Since the output vij = uij for ri ≤ j ≤ ni − 1 and vij = −u¯ij for
0 ≤ j ≤ ri − 1, the switch in front of the output vij can be controlled by the same signal sci
used in the divide-by-gii(t) circuit.
idii
qq
,0,
 
1,0, −irii
uu 
isc
imc
1,, −ii niri
uu 
1,0, −−− irii
uu 
1,, −ii niri
uu 
1−


)()( 1 tgtq iii + )()( tgtq ii 
isc

−
=
+
1
1
1 (t) )(
i
j
ijj gtq )()(
1
1
tgtq
i
j
jj

−
= 
	
−
=
1
1
)()(
i
j
jij tgtq

 

)()(
1
 1 tgtq
i
j
ijj

=
+ 
=
i
j
jj tgtq
1
)()( 

)( of storage tqi
)(-by-Divide tgii 1,,1,0, −− iii niririi
vvvv 
)(-by-multiply 1 tgii+ )(-by-multiply tgi
Figure 2.4: The ith level of the serial-in-serial-out architecture for the systematic encoding.
Example 2. Consider again the (64,48) Hermitian code C53 over GF (16) in Example 1.
Since supp(D) is decomposed into six orbits under the action of the σ given in (2.9), the
systematic encoder has six levels. The values of di, δij and δ
′
ij are given in Table 2.1. A
mark × in Table 2.1 represents a value that is not used. Since g56(t) in the Gro¨bner basis G
26
Consider the ith level of the proposed serial-in-serial-out architecture. As can be seen from
Figure 2.4, the numbers of memory elements needed in the divide-by-gii(t) circuit, in the
(ℓ− i) multiply-by-gij(t) circuits, i < j ≤ ℓ, and in the (cyclic) shift-register storage for qi(t)
are ri,
∑ℓ
j=i+1 deg gij(t), and at most (di + 1) respectively. Thus, the total number Dm of
memory elements for the systematic encoding of Hermitian code Cm is
Dm ≤
ℓ∑
i=1
ri +
ℓ−1∑
i=1
ℓ∑
j=i+1
deg gij(t) +
ℓ−1∑
i=1
(di + 1)
≤ (n− k(m)) +
ℓ∑
j=2
(j − 1)max(nj − kj − 1, 0)
+
ℓ−1∑
i=1
(max{k1 − 2, k2 − 2, · · · , ki−1 − 2, ki − 1}+ 1), (2.15)
since
∑ℓ
i=1 ri = r(m) = n − k(m), deg gij(t) < deg gjj(t) = rj = nj − kj, i < j, and from
Theorem 1. Similarly, the total number Mm of constant multipliers needed in the proposed
architecture for Hermitian code Cm is
Mm ≤
ℓ∑
i=1
ri +
ℓ−1∑
i=1
ℓ∑
j=i+1
(deg gij(t) + 1)
≤ (n− k(m)) +
ℓ∑
j=2
(j − 1)(nj − kj). (2.16)
From (2.15) and (2.16), it can be seen that Dm and Mm increase as ℓ increases in general.
Thus to construct a systematic encoder with less complexity, we tend to use a permutation
which results in the minimum number of orbits in the decomposition of supp(D) among all
possible permutations.
Note that for any Hermitian code Cm over GF (q
2), there always exists a permutation σ
σ :
{
x 7→ αx
y 7→ αq+1y,
(2.17)
where α is a primitive element in GF (q2) and αq+1 is a primitive element in the subfield
GF (q), by taking δ = µ = 0 and ǫ = α in (2.2). The permutation σ defined in (2.17)
28
by
Dm ≤ (n− k(m)) +
∑
j=2,··· ,q+2; nj−kj>0
(j − 1)(nj − kj − 1)
+
∑q+1
i=1 max{k1 − 2, ki − 1}+ (q + 1)
≤ (n− k(m)) +
∑q+2
j=2(j − 1)(nj − kj) + (q + 1)(k1 − 1) + (q + 1)
≤ (n− k(m)) + (q + 1)(n− k(m)− r1) + (q + 1)k1
= (q + 2)(n− k(m)) + (q + 1)(k1 − r1),
(2.19)
where we have used Theorem 2 in the first two inequalities, and from (2.16), the total number
Mm of finite-field constant multipliers can be further upper-bounded by
Mm ≤ (n− k(m)) +
∑q+2
j=2(j − 1)(nj − kj)
≤ (n− k(m)) + (q + 1)(n− k(m)− r1)
= (q + 2)(n− k(m))− (q + 1)r1.
(2.20)
In general, with the permutation in (2.17) and for high rate Hermitian codes Cm with n = q
3
and m = q3 − O(q2) (and then k(m) = m + 1 − q(q+1)
2
= q3 − O(q2)), the upper bounds of
Dm, Mm, D
′
m, M
′
m in (2.19), (2.20) and (2.18) are of the order
Dm = O(q
3) = D′m, and Mm = O(q
3)≪M ′m = O(q
5).
Hence the longer high rate Hermitian codes are, the greater the reduction of the space
complexity is in the proposed hardware architecture for a systematic encoder.
30
10 20 30 40 50 60 70
0
200
400
600
800
1000
1200
m = 12 to 63 
co
m
pl
ex
ity
M
m
,
M
m
D
m
,
D
m
Figure 2.6: Comparison of hardware complexity between the proposed architecture and the
brute-force matrix multiplication for a serial-in-serial-out systematic encoder of a Hermitian
code Cm over GF (4
2) with 12 ≤ m ≤ 63.
32
3.1 Left-column Replacement Algorithm
In this section, we will present a basic type of column operations performed in the funda-
mental iterative algorithm (FIA) [17] or in [24] for determining the linear dependency of a
column on its previous (i.e. left) columns in a matrix. Related concepts and useful results
will be further developed.
To determine the linear dependency of a column on its previous columns in a given matrix
A, a restricted Gaussian elimination on columns of the matrixA can be performed as follows.
For each non-zero column aj of the matrix A, we subtract a multiple of a previous column
av, v < j, of the matrix A from column aj to eliminate the leading (i.e. the topmost)
non-zero entry of column aj to lower down the position of the leading non-zero entry of the
newly resulted column, if possible. More precisely, if the leading non-zero entry of the jth
column aj is the ith entry ai,j and there is a previous column av, v < j, which has a leading
non-zero entry ai,v also at the ith position, then we replace column aj by aj − (ai,j/ai,v)av.
The replaced column aj − (ai,j/ai,v)av is either a zero vector or has a leading non-zero
entry below the ith position. For convenience, we will call the above operation a left-column
replacement operation and denote it by
aj ← aj + βav with v < j, (3.1)
where β is a constant.
A matrix A′ is called a left-reduced matrix of A if it is obtained by applying a series of
left-column replacement operations in (3.1) on the matrix A and the leading non-zero entry
of each column of the matrix A′, if exists, cannot be further eliminated by any left-column
replacement operation. The leading non-zero entry of a column in a left-reduced matrixA′ is
called a pivot. Note that we have restricted ourselves to left-column replacement operations
in applying Gaussian elimination on columns of the matrix A, i.e. without any column
exchange and without any column replacement by a multiple of a right column.
34
defined by
Soi =
n∑
k=1
ekfoi(Pk), ∀ i. (3.4)
Since
∑n
k=1 ckfoi(Pk) = 0 for all 1 ≤ i ≤ m − g + 1 by the parity-check matrix H in (1.4),
the first m − g + 1 (one-dimensional) syndromes can be calculated from the received word
r as follows: Soi =
∑n
k=1 rkfoi(Pk), ∀ 1 ≤ i ≤ m − g + 1, which will be called known (one-
dimensional) syndromes with respect to the received vector r and the parity check matrix
H . Similarly, for each ordered pair {oi, oj} of nongaps at Q, a (two-dimensional) syndrome
Soi,oj with order oi + oj can be defined by
Soi,oj =
n∑
k=1
ekfoi(Pk)foj (Pk) =
n∑
k=1
ekfoifoj (Pk). (3.5)
As discussed in [9], a (two-dimensional) syndrome Soi,oj may not be equal to the (one-
dimensional) syndrome Soi+oj of the same order. If Soi,oj 6= Soi+oj , Soi,oj is called a consistent
term of Soi+oj and, in this case, Soi,oj can be obtained by a linear combination of Sk’s with
0 ≤ k ≤ oi + oj. Consequently, with r and H , Soi,oj ’s are known only when oi + oj ≤ m.
Consider the (m− g+1)× (m− g+1) syndrome matrix S = [Soi,oj ]1≤i,j≤m−g+1. A column
SCoj of the syndrome matrix S is said to be a partial linear combination of its previous
columns up to the first ℓ components if there exists a linear combination coefficient vector
σ
(ℓ)
j = (σ
(ℓ)
j,1, σ
(ℓ)
j,2, . . . , σ
(ℓ)
j,j ) with σ
(ℓ)
j,j 6= 0 for the column S
C
oj
such that
j∑
k=1
σ
(ℓ)
j,kSoi,ok = 0, ∀1 ≤ i ≤ ℓ. (3.6)
Suppose that all entries of the matrix S are known. The following theorem, whose proof can
be found in [40, Theorem 1], provides the kernel of the algebraic decoding of the AG code
Cm.
Theorem 4. Suppose that the Hamming weight of the error vector e is no greater than
⌊d
∗−1
2
⌋, where d∗ = m − 2g + 2 is the designed minimum distance. If a column SCoj of
the syndrome matrix S is a partial linear combination of its previous columns up to the
36
need to know the exact values of Soi,oj in the matrix S of order less than or equal to m+ g.
Unfortunately, not all of them in the syndrome matrix S are known. As mentioned before,
with the received vector r and the parity check matrix H , the value of the syndrome Soi,oj
in the matrix S is known only if oi + oj ≤ m.
3.2.1 An extended syndrome matrix
Define an (m + 1)× n matrix X = [Xk,i]0≤k≤m,1≤i≤n with Xk,i = foj (Pi), 1 ≤ i ≤ n, if k is
a nongap oj , less than or equal to m, at Q and Xk,i := △, 1 ≤ i ≤ n, if k is a gap at Q.
The symbol “△” stands for an unspecified or blank entry in the matrix X. Let Y be the
diagonal matrix
Y =

e1 0 · · · 0
0 e2 · · · 0
...
...
. . .
...
0 0 · · · en

with error values ei’s on the diagonal. Then an (m+1)× (m+1) matrixM = [Mk,l]0≤k,l≤m
can be defined as
Mk,l =
{ ∑n
i=1 eifk(Pi)fl(Pi), if both k and l are nongaps at Q,
△, otherwise,
by regarding the matrix M as the product XYXt. Note that if k = oi and l = oj are
nongaps at Q, then we have Moi,oj = Soi,oj . We thus call the matrix M as an extended
syndrome matrix. Each non-blank entry Mi,j , i.e. Mi,j 6= △, will be called an effective entry
of the matrixM . And if Mi,j is effective, then Mi,j is either a consistent term of Si+j or just
the Si+j. The non-effective data, △’s, are not able and not necessary to be determined. Note
that, by deleting all its non-effective entries, the extended syndrome matrixM reduces to the
syndrome matrix S. For example, consider the Hermitian code C23 of length 64 over GF (4
2)
defined by taking B(23) in (1.3) as a standard basis of L(23Q), the extended syndrome
38
first b + 1 columns of the matrix M . Since the extended syndrome matrix M is near in a
Hankel form, the following result is useful.
Lemma 5. Suppose that j and j′ are two nongaps at Q with j′ < j and j′ ≡ j(mod γ),
where γ is the smallest nonzero nongap at Q. If the (j′ + 1)th column MCj′, as a whole, is
a linear combination of its previous columns, then the (j + 1)th column MCj is also a linear
combination of its previous columns.
Proof. Since the column MCj′ is a linear combination of its previous columns by the as-
sumption, there exists a vector σ
(m)
j′ for the column M
C
j′ such that
j′∑
v=0
Mu,vσ
(m)
j′,v = 0, for all nongap u, 0 ≤ u ≤ m, at Q. (3.10)
We note here that if the entryMu,v is equal to△ for some 0 ≤ v ≤ j′, then, in our convention,
we have σ
(m)
j′,v = 0. Thus, we shall formally define the multiplication 0 · △ of 0 and ∆ to be 0
and then the formula in (3.10) is well defined. In fact, with this coefficient vector σ
(m)
j′ , the
range of u, 0 ≤ u ≤ m, in (3.10) can be extended to all u ≥ 0 by the Remark after Theorem
4. Because the basis B(m) is standard and then satisfies (1.2), we have
Mu,v =
n∑
i=1
eifu(Pi)fv(Pi) =
n∑
i=1
ei(fu−γfγ)(Pi)fv(Pi)
=
n∑
i=1
eifu−γ(Pi)(fvfγ)(Pi) =
n∑
i=1
eifu−γ(Pi)fv+γ(Pi) =Mu−γ,v+γ ,
where u − γ, v are nongaps. Then for column MCj′+γ, we have
∑j′
v=0Mu−γ,v+γσ
(m)
j′,v =∑j′
v=0Mu,vσ
(m)
j′,v = 0, for all nongaps u − γ, 0 ≤ u − γ ≤ m, by (3.10). This implies that
the column MCj′+γ is a linear combination of its previous columns in the matrix M if the
column MCj′ is. Since j = j
′ + ℓγ for some ℓ ≥ 1 and by induction, the column MCj is a
linear combination of its previous columns in the matrix M .
Suppose that there exists a coefficient vector σ
(a−1)
b for the non-blank column M
C
b such
that
b∑
j=0
Mi,jσ
(a−1)
b,j = 0, if 0 ≤ i ≤ a− 1; but
b∑
j=0
Ma,jσ
(a−1)
b,j = Da,b 6= 0. (3.11)
40
of Da,b and Da−γ,b+γ as well as their corresponding coefficient vector σ
(a−1)
b and σ
(a−γ−1)
b+γ .
If a − γ is a gap at Q, then the (a − γ + 1)th row MRa−γ of the matrix M is a blank row.
Hence, we only know that with the coefficient vector σ
(a−γ−1)
b+γ = (0
γ ,σ
(a−1)
b ), the (b+γ+1)th
column MCb+γ of the matrix M is a partial linear combination of its previous columns at
least up to the first a− γ + 1 components. Hence, we have the following result.
Lemma 6. Suppose that a−γ is a nongap at Q and there exists a nonzero discrepancy Da,b
at the position (a, b) of the matrix M with respect to the coefficient vector σ
(a−1)
b . Then
the coefficient vector σ
(a−γ−1)
b+γ = (0
γ,σ
(a−1)
b ) gives a nonzero discrepancy Da−γ,b+γ at the
position (a− γ, b+ γ) of the matrix M and, moreover, Da−γ,b+γ = Da,b.
If the nonzero discrepancy Da,b cannot be reduced to zero by the left-column replacement
algorithm, then we say that there is a pivot position, at the (a, b)-th position of the matrix
M , on the column MCb . And, by Lemma 3, if the column M
C
b has a pivot position, then it
is linearly independent of its previous columns. Note that for each r, m + 1 ≤ r ≤ m + g,
the Sr-slanted diagonal does not cross the first r −m columns in the matrix M , however,
we shall regard every entry in these r −m columns as being above the Sr-slanted diagonal.
Lemma 7. Given an r, 0 ≤ r ≤ m+ g, and if the non-blank column MCj of the matrix M
has a pivot position on or above the Sr-slanted diagonal, then every non-blank column M
C
j′
of the matrixM to the left of the column MCj with j
′ ≡ j(mod γ) also has a pivot position
on or above the Sr-slanted diagonal.
Proof. If j = 0, then we have done. Suppose that j > 1 and that there is a non-blank
column MCj′ , where j
′ < j and j′ ≡ j(mod γ), which does not have any pivot position on
or above the Sr-slanted diagonal. If r ≥ (⌊
m+1
2
⌋ + g) + j′, i.e. the first ⌊m+1
2
⌋ + g entries
of the column MCj′ are above the Sr-slanted diagonal and (by the assumption) among these
positions there is no pivot at all. Then, by Theorem 4, the column MCj′, as a whole, is a
linear combination of its previous columns. Hence, by Lemma 5, the column MCj is also
a linear combination of its previous columns and then does not have any pivot position,
42
have γ = 4, ℓ0 = 0, ℓ1 = 5, ℓ2 = 10, ℓ3 = 15. Then, without loss of generality, we shall assign
Lr(i) = ℓi if r ≤ ℓi for the initial condition.
Since the column MCLr−1(i) of the matrix M does not have a pivot position on or above
the Sr−1-slanted diagonal which is trivially above the Sr-slanted diagonal, we have
Lr−1(i) ≤ Lr(i), ∀ r, and 0 ≤ i ≤ γ − 1. (3.17)
By the definition of Lr(i) and with the Lr(i) > ℓi assumed, the column M
C
Lr(i)−γ is the
rightmost column such that its column index satisfies (Lr(i) − γ) ≡ i(mod γ) and it has
a pivot position on or above the Sr-slanted diagonal. Then, by Lemma 7, each non-blank
column MCj , where j < Lr(i) − γ and j ≡ i(mod γ), of the matrix M also has a pivot
position on or above the Sr-slanted diagonal. Consequently, we have the following result.
Lemma 8. For an r, 0 ≤ r ≤ m+ g, an i, 0 ≤ i ≤ γ−1, and with Lr(i) > ℓi assumed, there
are exactly (Lr(i) − ℓi)/γ pivot positions of the matrix M within the different non-blank
columns MCj ’s, where j ≤ Lr(i) − γ and j ≡ i(mod γ). And, in particular, there are in
different non-blank rows.
3.4 The Decoding Algorithm
Recall that, by the definition of Lr−1(i), the columnM
C
Lr−1(i) of the matrixM , 0 ≤ i ≤ γ−1,
does not have a pivot position on or above the Sr−1-slanted diagonal. Thus, there is a
coefficient vector σ
(r−1−Lr−1(i))
Lr−1(i)
such that the columnMCLr−1(i) is a partial linear combination
of its previous columns up to the entry Mr−1−Lr−1(i),Lr−1(i). To find Lr(i)’s from Lr−1(i)’s,
we shall pay our attention to the following columns of the matrix M : MCLr−1(0), M
C
Lr−1(1),
. . ., and MCLr−1(γ−1).
If r − Lr−1(i) < 0 for some r and i, then we say that the column M
C
Lr−1(i) of the matrix
M is still in the waiting state at the iteration (or step) of r, and we let Lr(i) = Lr−1(i) and
44


 ED
'
γγ OEOD' +− 


ED
'

( )



−D
E
h
( )( )




−D
E
O
h
γ

( )−D
E
h
( ) ( ) ( ) ( )( )











EDEDD
E
O
ED
ED
D
E
D
E
'
' +−+−−
−=  hhh
γ
( )γPRG
DQG


 DDEDEDDD ≡+<+≥
GLDJRQDO
VODQWHG


 ED
6 +
GLDJRQDO
VODQWHG
ED
6 +
0PDWUL[V\QGURPHH[WHQGHG7KH
γO
Figure 3.2: A general case of how to reduce the nonzero discrepancy Da,b to zero.
We summarize what we have done in the following lemma.
Lemma 9. Suppose that there are coefficient vectors σ
(a−1)
b and σ
(a′−1)
b′ for two effective
columns MCb and M
C
b′ of the matrix M , respectively, and in addition, both the next dis-
crepancy Da,b and the next discrepancy Da′,b′ are nonzeros. Suppose moreover that the
indices a, b, a′, and b′ satisfy a′ ≥ a, a′ + b′ < a+ b, and a′ ≡ a( mod γ). Then the nonzero
discrepancy Da,b can be reduced to zero by the left-column replacement algorithm and a
coefficient vector σ
(a)
b for the column M
C
b of the matrix M can be obtained by updating
the coefficient vector σ
(a−1)
b as shown in (3.19).
Let ×Rr (i), initially set to ∞, denote the row-index of the pivot position of the column
MCLr−1(i) of the matrix M for each r, 0 ≤ r ≤ m+ g. Note that if, within the iteration of r
in the left-column replacement algorithm, the column MCLr−1(i) of the matrix M does have
a pivot position, then
×Rr (i) = r − Lr−1(i). (3.20)
46
×Rs (u) and then we have ℓ
×
i =
×Rr (i)−×
R
s (u)
γ
. (Note that ×Rs (u) must be less than
×Rr (i), otherwise the nonzero discrepancy at the pivot position (×
R
r (i), Lr−1(i)) would
be reduced to zero by the nonzero discrepancy at the pivot position (×Rs (u), Ls−1(u)),
a contradiction.)
2. If there is no s, u with s < r and 0 ≤ u ≤ γ − 1 such that ×Rs (u) 6= ∞ and ×
R
s (u) ≡
×Rr (i)(mod γ), then ℓ
×
i is the first integer such that ×
R
r (i) − ℓ
×
i γ is a gap at Q or
×Rr (i)− ℓ
×
i γ < 0.
Then, we have Lr(i) = Lr−1(i) + ℓ
×
i γ. Moreover, by Lemma 6 and Lemma 9, a coefficient
vector σ
(r−Lr(i))
Lr(i)
for the column MCLr(i) can also be obtain by updating the coefficient vector
σ
(r−1−Lr−1(i))
Lr−1(i)
by
σ
(r−Lr(i))
Lr(i)
= (0ℓ
×
i γ ,σ
(r−1−Lr−1(i))
Lr−1(i)
)−
Dr−Lr−1(i),Lr−1(i)
Ds−Ls−1(u),Ls−1(u)
(σ
(s−1−Ls−1(u))
Ls−1(u)
, 0r−s) (3.21)
for the first case, as summarized in Figure 3.3 where ×Rs (u) = s − Ls−1(u) and ×
R
r (i) =
r − Lr−1(i), and
σ
(r−Lr(i))
Lr(i)
=
{
(0ℓ
×
i γ ,σ
(r−1−Lr−1(i))
Lr−1(i)
), if ×Rr (i)− ℓ
×
i γ is a gap at Q,
(0Lr(i), 1), if ×Rr (i)− ℓ
×
i γ < 0,
(3.22)
for the second case.
With the obtain coefficient vector σ
(r−Lr(i))
Lr(i)
, the left-column replacement algorithm can
go on one step further from r to r + 1 to examine the next discrepancy at the position
(r + 1− Lr(i), Lr(i)) of the matrix M . If there is one coefficient vector σ
(r−Lr(i))
Lr(i)
such that
r − Lr(i) = ⌊
m+1
2
⌋ + g − 1, then the algorithm stops. Since the column MCLr(i) is a partial
linear combination of its previous columns up to the first ⌊m+1
2
⌋+ g entries and by Theorem
4, the column MCLr(i), as a whole, is a linear combination of its previous columns in the
matrix M and then an error-locator polynomial is determined.
We summarize our decoding algorithm in the following, called Algorithm I.
Algorithm I:
48
Update ×Rr (i) = r − Lr−1(i),
store D×Rr (i),Lr−1(i) and σ
(r−1−Lr−1(i))
Lr−1(i)
.
If there is at least one pair of s, u with s < r and 0 ≤ u ≤ γ − 1
such that ×Rs (u) ≥ 0 and ×
R
s (u) ≡ ×
R
r (i)(mod γ), then we select
such a pair of s, u with the largest ×Rs (u) and then
ℓ×i =
×Rr (i)−×
R
s (u)
γ
,
Lr(i) = Lr−1(i) + ℓ
×
i γ,
σ
(r−Lr(i))
Lr(i)
= (0ℓ
×
i γ ,σ
(r−1−Lr−1(i))
Lr−1(i)
)−
Dr−Lr−1(i),Lr−1(i)
D
×
R
s (u),Ls−1(u)
(σ
(s−1−Ls−1(u))
Ls−1(u)
, 0r−s).
Else if ℓ×i is the smallest positive integer such that ×
R
r (i)− ℓ
×
i γ
is a gap at Q, then
Lr(i) = Lr−1(i) + ℓ
×
i γ,
σ
(r−Lr(i))
Lr(i)
= (0ℓ
×
i γ ,σ
(r−1−Lr−1(i))
Lr−1(i)
).
Else if ℓ×i is the smallest positive integer such that ×
R
r (i)−ℓ
×
i γ <
0, then
Lr(i) = Lr−1(i) + ℓ
×
i γ,
σ
(r−Lr(i))
Lr(i)
= (0Lr(i), 1).
Before describing how to treat the unknown syndromes Sm+1, Sm+2, . . . , we consider the
following two examples for the demonstration of the proposed algorithm.
Example 3. Consider a BCH code over GF (24) with the designed error-correcting capability
t = 3, which is the same example taken in [17, Example 3]. Suppose a vector r is received
and the following (one-dimensional) syndromes Sr, 0 ≤ r ≤ 5, are known:
S0 = α
7, S1 = α
12, S2 = α
6, S3 = α
12, S4 = α
14, S5 = α
14.
And now we have g = 0, and there is no gap at all. Since γ = 1, the index i in Algorithm I
can be omitted. The execution of the proposed algorithm is briefly described in the following.
For r = 0 , the discrepancy Dr−Lr−1,Lr−1 = D0,0 = 1 ·M0,0 = S0 6= 0. This discrepancy
cannot be reduced to zero, hence, we update ×R0 = r − Lr−1 = 0 and store D0,0 and
σ
(−1)
0 = (1). The integer ℓ
× = 1 then L0 = L−1 + 1 = 1 and we obtain σ
(r−Lr)
Lr
=
σ
(−1)
1 = (0, 1). There is a pivot at the position (0, 0) of the matrix M .
50
S9 = 0, S10 = α
5, S11 = α
2, S12 = α
4, and S13 = α
5.
Then, the extended syndrome matrix M is
M =

α4 △ △ α △ α3 α6 1 α6 0 α5 α2 α4 α5
△ △ △ △ △ △ △ △ △ △ △ △ △ △
△ △ △ △ △ △ △ △ △ △ △ △ △ △
α △ △ α6 △ α6 0 α5 α2 α4 α5 # # #
△ △ △ △ △ △ △ △ △ △ △ △ △ △
α3 △ △ α6 △ α6 α2 α4 α # # # # #
α6 △ △ 0 △ α2 α4 α5 # # # # # #
1 △ △ α5 △ α4 α5 # # # # # # #
α6 △ △ α2 △ α # # # # # # # #
0 △ △ α4 △ # # # # # # # # #
α5 △ △ α5 △ # # # # # # # # #
α2 △ △ # △ # # # # # # # # #
α4 △ △ # △ # # # # # # # # #
α5 △ △ # △ # # # # # # # # #

,
where the M5,5 = α
6 6= S10 is a consistent term of S10 as well as M8,5 and M5,8 are two
consistent terms of S13, and # denotes an unknown syndrome.
After executing the proposed algorithm to the iteration r = 13, we have the following
results:
L13(0) = 9, σ
(4)
9 = (α
4, 0, 0, α3, 0, 0, 1, 0, α5, 1).
L13(1) = 7, σ
(6)
7 = (1, 0, 0, α
2, 0, α6, 0, 1).
L13(2) = 8, σ
(5)
8 = (α
6, 0, 0, α2, 0, α5, α, 0, 1).
Moreover, we have stored the following pivots:
(s, u) = (0, 0) with ×R0 (0) = 0, D0,0 = α
4, and σ
(−1)
0 = (1).
(s, u) = (6, 0) with ×R6 (0) = 3, D3,3 = α, and σ
(2)
3 = (α
4, 0, 0, 1).
(s, u) = (10, 2) with ×R10(2) = 5, D5,5 = α, and σ
(4)
5 = (α
1, 0, 0, α1, 0, 1).
(s, u) = (12, 0) with ×R12(0) = 6, D6,6 = α
6, and σ
(5)
6 = (α
2, 0, 0, 1, 0, α5, 1).
52
The idea of the proposed decoding algorithm is to iteratively find a coefficient vector σ
(a)
b
that solves the submatrixM (a,b) of the extended syndrome matrixM by increasing the order
r, where r = a+b and 0 ≤ r ≤ m+g. If there is no gap, the proposed algorithm is reduced to
the early stopped Berlekamp-Massey algorithm (ESBM) [24], since the entries of the matrix
M treated in this paper will have the back-shifted property in this case. The structural
properties of such a Hankel-type matrix have been exploited by Liu and Lu in [24]. With
these properties, for any an r, among those Sr’s lying on the Sr-slanted diagonal, there is
only one Sr being examined by the algorithm. With gaps, however, the back-shifted property
of the elements in the extended syndrome matrix M is ruined by blank columns and blank
rows. However, with the γ-shifted structural properties of the matrix M , our proposed
decoding algorithm, which generalizes the Feng-Rao algorithm, interprets the whole process
of applying the left-column replacement algorithm on the matrix M into a set of γ parallel
ESBM algorithms. While, the Ko¨tter’s algorithm, which is a parallel version of the Sakata
et al.’s algorithm, is a set of γ parallel conventional Berlekamp-Massey algorithm.
The proposed decoding algorithm can be regarded as a Gaussian-elimination-type algo-
rithm (and, hence, is simple) and as a Berlekamp-Massey-type algorithm (and, hence, is
efficient) simuiltaneously. We note here that the operations performed in each component
ESBM algorithm are independent of but related with each other. And, for any an r, among
those Sr’s (or their consistent terms) lying on the Sr-slanted diagonal, there are at most γ
Sr’s being examined by the algorithm.
3.4.1 Majority Voting Scheme
As mentioned by Feng and Rao in [9], the key problem in decoding AG codes is to find
the real values of the (one-dimensional) syndrome Si for i = m + 1, m + 2, . . . , m + g and
a coefficient vector σ
(⌊(m+1)/2⌋+g−1)
j which satisfies Theorem 4. Suppose that Algorithm I
has run up to the iteration of r = m. And now, consider the following confronted unknown
entry Mm+1−Lm(i),Lm(i) for some i, 0 ≤ i ≤ γ − 1. As mentioned before, if m + 1 − Lm(i)
54
@r−Lr−1(i),Lr−1(i) = @5,9 calculated by (3.23). Since we have a pair (s, u) = (10, 2) such
that ×R10(2) = 5 = r − Lr−1(i). Hence, the candidate @5,9 is not available.
If i = 1, since r − Lr−1(i) = 14− 7 = 7 is a nongap at Q, then we may have the candidate
@r−Lr−1(i),Lr−1(i) = @7,7 calculated by (3.23). Since r − Lr−1(i) = 7 ≡ 1(mod 3) and
there is no a pair (s, u) such that ×Rs (u) ≡ 1(mod 3). Hence the candidate @7,7 is
indeed available. By (3.23), with the coefficient vector σ
(6)
7 we have the available
candidate @7,7 = α
3. And by the γ-shifted properties of the matrix M , with the
coefficient vector (0ℓγ,σ
(6)
7 ) we may have the available candidate @7−ℓγ,7+ℓγ. Since
both r − Lr−1(i)− γ = 7− 3 = 4 and r − Lr−1(i)− 2γ = 7− 6 = 1 are gaps at Q, we
concludes that there is only one available candidate, which is related to @7,7, for the
confronted unknown syndrome. We note here that S7,7 6= S14 is a consistent term of
S14.
If i = 2, since r − Lr−1(i) = 14− 8 = 6 is a nongap at Q, then we may have the candidate
@r−Lr−1(i),Lr−1(i) = @6,8 calculated by (3.23). Since we have a pair (s, u) = (12, 0) such
that ×R12(0) = 6 = r − Lr−1(i). Hence, the candidate @6,8 is not available.
The total number of the available candidates with the same values plays an important role
when the majority voting scheme is proceeded. In [9], Feng and Rao have shown that the total
number of correct candidates for the confronted unknown syndrome is greater than the total
number of the incorrect ones. Now, suppose that the candidate @m+1−Lm(i),Lm(i) is available
and there is a pair (s, u) such that ×Rs (u) ≡ m+1−Lm(i)(mod γ) and ×
R
s (u) < m+1−Lm(i),
then we will have the total number (m+ 1−Lm(i)−×Rs (u))/γ of available candidates with
the same value @m+1−Lm(i),Lm(i). On the other hand, if the candidate @m+1−Lm(i),Lm(i) is
available but there is no a pair (s, u) such that ×Rs (u) ≡ m + 1 − Lm(i)(mod γ), then we
will have the total number 1 + (m + 1 − Lm(i) − ℓj)/γ of available candidates with the
same value @m+1−Lm(i),Lm(i), where we assumed that m + 1 − Lm(i) ≡ j(mod γ). With the
consistent term being considered, where we note that there always exists a procedure, for
56
as the total number of nongaps ALr−1(i) at Q in [0, Lr−1(i) − 1]. The similar consideration
occurs in the calculation of updating coefficient vector as well as in the calculation of available
candidate. For updating coefficient vector, as shown in the Algorithm I, we must first take
one multiplication to calculate the Dr−Lr−1(i),Lr−1(i)/D×Rs (u),Ls−1(u) and then multiply it to the
stored vector σ
(s−1−Ls−1(u))
Ls−1(u)
. Hence, it requires totally ALs−1(u) + 1 multiplications. Thus,
for the case of a specified i in the iteration of r, 0 ≤ r ≤ m, it requires totally at most
ALr−1(i) + ALs−1(u) + 1 ≤ 2ALr−1(i) multiplications.
As described in Section 3.4, the proposed algorithm can be considered as γ independent (or
parallel) ESBM algorithms. In [24], Liu and Lu have shown that the worst case of the ESBM
algorithm for the decoding t-error-correcting BCH codes is that all the t pivot positions are
lying on the diagonal of the considered syndrome matrix. Since the proposed algorithm, the
Algorithm I, is just the extension of the ESBM algorithm to examine the extended syndrome
matrixM , consequently, the worst case of the Algorithm I is also that all the pivot positions
are lying on the diagonal line of the matrix M .
In the following, we will first consider the computation complexity of each separated ESBM
algorithm for a specified i, 0 ≤ i ≤ γ − 1, and then the total complexity of the proposed
algorithm. Recall that, if r − Lr−1(i) < 0, then the algorithm is still in the waiting state.
Hence, we shall consider the range of r, rather than 0 ≤ r ≤ m + g, to be ℓi ≤ r ≤ m + g.
For the simplicity, we shall suppose that m ≥ 4g − 1.
Now, let’s consider the case of i = 0. Since ℓ0 = 0 obviously, hence there are totallym+g+1
iterations for examining the first m+ 1 known entries and the following g unknown entries.
For the worst case of the proposed algorithm, there are the following ℓγ0 + 1 pivot positions
occurred: (0, 0), (γ, γ), . . . , (ℓγ0γ, ℓ
γ
0γ), which are on the diagonal, where ℓ
γ
0 = ⌊(t+g)/γ⌋ with
t = ⌊(m − 2g + 1)/2⌋. Since there is a pivot at the position (0, 0), then we have L0(0) = γ.
By the Algorithm I, for the iterations from r = 1 through r = γ−1, we have γ−1 steps with
r−Lr−1(i) < 0, i.e. there are γ−1 steps in the waiting state. Suppose that there are Bℓγ0γ gaps
58
then he must determine the real values of the unknown syndromes from Sm+1 to Sm+2g+2.
A rough bound of the ALr(i), m + 1 ≤ r ≤ m + 2g + 2, is that ALr(i) ≤ m − g. Then, the
total complexity of the proposed algorithm for examining unknown syndromes is at most
γ(2g + 2)2(m− g)(≤ O(γm2)).
Altogether, the complexity of the proposed algorithm is in the order O(γm2) ≃ O(γn2),
since m ≃ n for a long code, where n is the code length. From the upper bound, we can show
that our algorithm as efficient as the Ko¨tter’s algorithm. Moreover, by storing the nonzero
discrepancy D×Rs (u),Ls−1(u) together with the coefficient vector σ
(s−1−Ls−1(u))
Ls−1(u)
, our algorithm is
superior for saving computation complexity. We note here that, how good the complexity is
depends on the code construction. For example, in the case of one-point Hermitian code, we
have γ = n1/3. Then the complexity of the proposed algorithm for the decoding of one-point
Hermitian code is O(n7/3). On the other hand, let’s consider the curve [42] in the affine
space GF (q)3, where q = k2, defined by yk+1 = xk + x and zk+1 = −xyk − yxk − 1.
If k ≡ 1(mod 3), then the curve has (k2 − 1)2 rational points over GF (q) and the genus of
the curve g = k3 − k2 − k. The pole order of the functions x, y, and z at Q (the point at
the infinity) are (k + 1)2, k(k + 1), and k(k + 2) respectively. Then, we have γ ≃ n1/2. And
the complexity of the proposed algorithm is O(n5/2).
60
























∆∆∆
∆∆∆
∆∆∆
∆∆∆
∆∆∆
∆∆∆
∆∆∆
∆∆∆
∆∆∆
∆∆∆∆∆∆∆∆∆∆∆∆∆∆
∆∆∆
∆∆∆∆∆∆∆∆∆∆∆∆∆∆
∆∆∆∆∆∆∆∆∆∆∆∆∆∆
∆∆∆











#
#
#
#
#
#
#

α
α
α
αα
α
ααα
ααα
αααα
αααααα
ααααααα
ααααααααα
GLDJRQDO
VODQWHG

−+P6
( ) ZW +H
 == gP
FDQGLGDWHDYDLODEOHQRQ#
Ó
FDQGLGDWHDYDLODEOH
g+

 +

P
Figure 3.4: An illustrative example to compare the proposed algorithm with the Ko¨tter’s
algorithm.
62
4.1.1 Left-column Replacement Algorithm
Given a nonzero column aCj of a matrixA, the idea of the left-column replacement algorithm
is to subtract a multiple of a previous (i.e. left) column aCv , v < j, to eliminate the leading
(i.e. the topmost) non-zero entry of column aCj and thus to lower down the position of the
leading non-zero entry of the newly resulted column, if possible. If the leading nonzero entry
of aCj is the ith entry ai,j and there exits a previous column a
C
v with v < j and nonzero
leading entry ai,v, then we replace column a
C
j by a
C
j − (ai,j/ai,v)a
C
v . For convenience, we
denote the above operation by
aCj ← a
C
j + βa
C
v with v < j (4.1)
,where β is a constant, and call it a left-column replacement operation. After applying a
series of left-column operations on a matrix A, a matrix A′ is called a left-reduced matrix
of A if the leading nonzero entry of each column of A′, if exists, cannot be further zeroed
out by any left-column operation. The leading nonzero entry of a column is called a pivot.
Note that all pivots must in different rows of the matrix A′.
Let a′Cj be the jth column of a left-reduced matrix A
′. Then a′Cj is resulted from a linear
combination of the first j columns aCv , 1 ≤ v ≤ j, of the original matrix A, i.e. there exists
a coefficient vector βj = (βj,1, · · · , βj,j−1, 1) satisfying
j−1∑
v=1
βj,va
C
v + a
C
j = a
′C
j . (4.2)
From (4.2), we can see that if a′Cj is a zero vector, then a
C
j is a linear combination of its
previous columns in A. Otherwise, a′Cj must has a pivot at the ith coordinate for some i.
Since a′u,j = 0 for all 1 ≤ u ≤ i, we have
au,j = −
j−1∑
v=1
βj,vau,v, 1 ≤ u ≤ i− 1
from (4.2). Thus the column aCj is a partial linear combination of its previous columns up
64
(0, · · · , 0︸ ︷︷ ︸
γ
), gives a nonzero discrepancy Da−γ,b+γ at the position (a− γ, b+ γ) of the matrix
M . Moreover, Da−γ,b+γ = Da,b.
Proof. Please see the paragraph before Lemma 6.
We say that there is a pivot at the position (a, b) in the column MCb of matrix M if the
nonzero discrepancy Da,b cannot be reduced to zero by the left-column replacement algo-
rithm. By Lemma 11, if the columnMCb has a pivot position, then it is linearly independent
of its previous columns. Let Sr-slanted diagonal in the matrix M be the slanted diago-
nal where all non-blank entries Sr’s or its consistent terms reside. Notice that for each r,
m + 1 ≤ r ≤ m + g, the Sr-slanted diagonal does not across the first r − m columns in
the matrix M . However, we regard every entry in these r −m columns as being above the
Sr-slanted diagonal. The following lemmas are bases of the PESBM algorithm.
Lemma 13. Given an r, 0 ≤ r ≤ m+ g, and if the effective column MCb of the matrix M
has a pivot position on or above the Sr-slanted diagonal, then every non-blank column M
C
b′
of the matrixM to the left of the column MCb with b
′ ≡ b(mod γ) also has a pivot position
on or above the Sr-slanted diagonal.
Proof. Please see the proof of Lemma 7.
Lemma 14. Suppose there are coefficient vectors σ
(a−1)
b and σ
(a′−1)
b′ for the tow effective
columns MCb and M
C
b′ of the matrix M respectively, and both the discrepancies Da,b and
Da′,b′ are not zeros. Suppose that the indices a, b, a
′, and b′ satisfy a′ ≥ a, a′+ b′ < a+ b and
a′ = a+ ℓγ for some ℓ ≥ 0. Then the nonzero discrepancy Da,b can be reduced to zero by the
left-column replacement algorithm and the coefficient vector σ
(a)
b for the column M
C
b can
be obtained by updating the coefficient vector σ
(a−1)
b as shown in the following equation:
σ
(a)
b = σ
(a−1)
b −
Da,b
Da′,b′
(0ℓγ ,σ
(a′−1)
b′ , 0
(a+b)−(a′+b′)) (4.4)
Proof. Please see the paragraph before Lemma 9.
66
With the obtained coefficient vector σ
(r−Lr(i))
Lr(i)
, the algorithm goes one step further from
r to r + 1 to examine the discrepancy of th entry Mr+1−Lr(i),Lr(i). If there is one coefficient
vector σ
(r−Lr(i))
Lr(i)
such that r − Lr(i) = ⌊
m+1
2
⌋ + g − 1, then by Theorem 4, the algorithm
terminates. The PESBM algorithm is summarized in Algorithm I. We shall assume that
every entry in the extended syndrome matrix M is known in Algorithm I.
Algorithm I:
Initial conditions :
For each i, 0 ≤ i ≤ γ − 1, set ×R−1(i) =∞, L−1(i) = ℓi, and σ
(−1)
L−1(i)
= (0L−1(i), 1).
For r from 0 to m+ g
For i from 0 to γ − 1
If r − Lr−1(i) < 0 (i.e. the column MCr−Lr−1(i) is still in waiting state),
Set Lr(i) = Lr−1(i) and σ
(r−Lr(i))
Lr(i)
= σ
(r−1−Lr−1(i))
Lr−1(i)
.
Else (i.e. r − Lr−1(i) ≥ 0),
If r − Lr−1(i) is a gap at Q,
Set Lr(i) = Lr−1(i) and σ
(r−Lr(i))
Lr(i)
= σ
(r−1−Lr−1(i))
Lr−1(i)
.
Else (i.e. r − Lr−1(i) is a nongap at Q),
With σ
(r−1−Lr−1(i))
Lr−1(i)
, calculate the discrepancy Dr−Lr−1(i),Lr−1(i) by (4.3), i.e.
Dr−Lr−1(i),Lr−1(i) = σ
r−1−Lr−1(i)
Lr−1(i)
· (Mr−Lr−1(i),0, . . . ,Mr−Lr−1(i),Lr−1(i)),
where the operation ”·” denote the inner product of two vectors.
If Dr−Lr−1(i),Lr−1(i) = 0 and r − Lr−1(i) = ⌊
m+1
2
⌋+ g − 1,
the algorithm stops.
Else if Dr−Lr−1(i),Lr−1(i) = 0,
Set Lr(i) = Lr−1(i) and σ
(r−Lr(i))
Lr(i)
= σ
(r−1−Lr−1(i))
Lr−1(i)
.
Else (i.e. Dr−Lr−1(i),Lr−1(i) 6= 0),
68
algorithm, to the matrix M , there is at least one available candidate value. Thus at least
one @m+1−Lm(i),Lm(i) calculated by (4.7), where 0 ≤ i ≤ γ − 1, is available.
Supposed that, for a specified i, the @m+1−Lm(i),Lm(i) is calculated and it is available.
By Lemma 12, we may have an available candidate @m+1−Lm(i)−γ,Lm(i)+γ=@m+1−Lm(i),Lm(i),
which resides at the position (m+1−Lm(i)− γ, Lm(i)+ γ) of the matrixM , if (1) m+1−
Lm(i)−γ is a nongap at Q, and (2) there is no pair (s, u) such that ×Rs (u) ≥ m+1−Lm(i)−γ
and ×Rs (u) ≡ m+1−Lm(i)(mod γ). In [9], Feng and Rao have proved that the total number
of correct candidates for the confronted unknown syndrome is greater than the total number
of incorrect ones. Thus with the consistent term being considered, the real value of the
confronted unknown syndrome can be determined by the majority voting scheme(MVS).
Once the value of Sm+1 or its consistent term is obtained, Algorithm I can then go one step
further until a coefficient σ
(⌊m+1
2
⌋+g−1)
j for some j is found. With these data, an error-locator
polynomial can then be found.
4.2 A Systolic Array Implementation
As discussed in Section 3.2, to determine an error-locator polynomial, it is necessary to find
a set of linearly dependent columns in the extended syndrome matrix M . If the Hamming
weight wt(e) of an error pattern is not greater then the error capacity t, the rank of M is
less or equal to t. It seems that only t+ 1 columns of M should be examined. By Theorem
4, for finding an error-locator polynomial, it is necessary to know the value of each syndrome
Sr, 0 ≤ r ≤ m + g. However, the syndromes of order m + 1 to m + g are unknown. Thus
we need consider all columns in M for obtaining the exact value of the syndrome Sr with
0 ≤ r ≤ m+ g through majority voting.
70
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 






































































																	
																	
																	
																

															

															
															
														

													

												

												
											

									

								

							

				

			






































Figure 4.1: The extended syndrome matrix M of the Hermitian code.
S0 = 0 but S4 6= 0, we have ×(0) = 4. The non-local connections can be removed by
adding transmittent variables m
(0)
oi,oj , where m
(0)
oi,j = m
(0)
oi−1,oj for all oi, oj with 5 ≤ oi ≤ 17
and 4 ≤ oj ≤ 23, and m
(0)
0,oj
= m
(0)
4,oj
for all oj with 4 ≤ oj ≤ 23. Projecting out the i-axis
of the graph will produce a one-dimensional array, which corresponds to the first row of the
two-dimensional array in Figure 4.3, for the first elimination step.
Observe that at step k, where k is a nongap, we have a
(k)
i,j = a
(k−1)
i,j if i < ×(k), since all the
entries a
(k−1)
i,j are zero for all i < ×(k). Thus at step k, a graph identical to that in Figure
4.2(a), but of size ( ⌊m+1⌋
2
+1−k)×(m+1−g−k), can be developed. A three-dimensional data
dependency graph is then produced by replicating and connecting subgraphs of appropriate
size in the k direction. The shape of the space is shown in Figure 4.2(b). By projecting out
the i-axis in the three-dimensional data dependency graph, a two-dimensional systolic array
is then established in Figure 4.3. The two-dimensional systolic array in Figure 4.3 is almost
72
Two kinds of processing elements, named PE cells and D cells, are required. Each D cell,
which corresponds to a blank column in M , contains only delay units. A PE cell, which
corresponds to an effective column inM , deals with the left-column replacement operations
related to the corresponding column. The functions of a PE cell will be discussed later. No-
tice that all syndromes lying on the same slanted diagonal in M , i.e. with the same order,
will be examined at the same step in the proposed one-dimensional array.
Now consider the data dependencies associated with the unknown syndrome Sr, for r =
m+ 1, · · · , m+ g. Denote the first unknown syndrome at the position (r−Lr−1(i), Lr−1(i))
of M by the symbol @. We first assume that the value Sr is known and equal to @. As we
can see in Algorithm I, if the column MCLr−1(i) is a partial linear combination of its previous
columns up to its first r − 1 − Lr−1(i) components, the PESBM algorithm intends to zero
out the data discrepancy Dr−Lr−1(i),Lr−1(i) by the left-column operation with the calculated
value @r−Lr−1(i),Lr−1(i). If the candidate @ is available, then we have @ = @r−Lr−1(i),Lr−1(i).
The value @r−Lr−1(i),Lr−1(i) will then be passed to the majority voting scheme(MVS) for
determining the real value of the first confronted syndrome. Thus the data dependencies
of computations of an available candidate is the same as the Gaussian elimination without
column exchange. Therefore, the one-dimensional systolic array in Figure 4.3 can be utilized
for the implementation of the PESBM algorithm. Notice that by inserting the blank entries
in M and D cells in the one-dimensional array, syndromes with the same order will be
examined simultaneously by the array. Thus the available candidate values for the first
confronted syndrome will be calculated and passed to the MVS at the same time.
4.2.2 Reduction of Hardware Complexity
Since the syndrome matrix S and the extended syndrome matrix M are symmetric and
the left-column replacement algorithm is a type of restricted Gaussian elimination, the sym-
metry property described in the last chapter can be used in our implementation for deter-
mining the unknown syndromes and thus reducing hardware complexity. Moreover, as we
74
instead of examining each column where the syndrome Sr (or its consistent term) lies on,
the PESBM algorithm (Algorithm I) only inspects the column MCLr(i) for each i = 0 to γ−1.
Consequently, only γ discrepancies and at most γ coefficient vectors should be calculated
and updated at the each step. Thus those PE cells which will never operate at the same
time can shall all operation functions.
To explain the share of cell functions more precisely, we first give some notations. For each
i, 0 ≤ i ≤ γ − 1, define hi to be the largest integer such that i + hiγ ≤ t + ⌊
g−1
2
⌋ + g. Let
C(i) to be class consisting of the columns in M whose column indices modulo by γ will be
i, i.e. C(i) = {MCj |j ≡ i(mod γ)}. Observe that in Algorithm I, for each i, 0 ≤ i ≤ γ − 1,
only one column in the set C(i) will be examined at each step. Thus those PE cells , which
corresponds to the columns in the same class C(i), can shall all operation functions, such
as the calculations of discrepancies and the updates of coefficient vectors. For convenience,
for each i, we extract all operation functions from each PE corresponding to the columns in
C(i) and put them into an cell, named operation element (OE(i)). Then there are totally at
most γ OE cells and each OE cell can be reused at different steps. The functions of all PE
cells, D cells and OE cells will be further discussed in the next subsection.
4.3 Cell Functions
As described in Subsection 4.2.2, all operation functions in a PE cell are extracted to the
corresponding OE cell. Thus the only remained function in a PE cell is data transmission.
Figure 4.4(a) shows the input and output definitions of the PE cell, named PE(j), which is
the (j + 1)th element in the one-dimensional array and corresponds to the effective column
MCj in M .
As we can see in Figure 4.4(a), two vectors aj and bj , received from the left-hand side, will
be passed immediately to the corresponding OE cell below, say OE(i), where j ≡ i(mod γ),
for calculating discrepancies and updating of coefficients, and then be pipelined to the next
76
Pass a˜j and b˜j to the right-hand side cell.
Since a D cell corresponds to a non-effective column in M , the cell function of a D cell
becomes quite simple. The input and output definitions of a D cell are given in Figure 4.4(b).
When the cell D(j) receives the input vectors a and b in the beginning of a step, it passes
theses vectors immediately to the corresponding OE cell, say OE(i), where j ≡ i(mod γ),
and then transmit them to its righthand side cell at the end of the step.
Now consider the functions of an OE cell. The input and output definitions of the ith OE
cell, named OE(i), are presented in Figure 4.5. Recall that in Subsection 4.2.2, we define hi
to be the largest integer satisfying i + hiγ ≤ t + ⌊
g−1
2
⌋ + g for each i, 0 ≤ i ≤ γ − 1. Thus
the cell OE(i) receives the input vectors ci, ci+γ, · · · , ci+hiγ from the above one-dimensional
array in the beginning of a step and returns the modified data di,di+γ, · · · ,di+hiγ to the
corresponding PE cells. Observe that if i + hγ is a gap for some h, OE(i) receives ci+hγ =
{ai+hγ, bi+hγ} from the cell D(i + hγ). For convenience, we define k(IR)i+hγ = 1 in OE(i)
and OE(i) does not need to output di+hγ, if i+ hγ is a gap at Q. To deal with the unknown
syndromes, a set of signals, say @ andAV , is sent from OE(i) to the MVS below. The signal
@ represents the value of an available candidate for the first confronted syndrome (if there
is any) and the vector AV indicates which candidates in C(i) are available. If the candidate
of the first confronted syndrome in the column MCi+hγ is available, the value of AVi+hγ is set
to be one; otherwise, zero. After the exact value @MV S of the first confronted syndrome is
determined by MVS, @MV S will be returned to OE(i) from MVS for further processing.
Recall that in Subsection 4.1.3, for each i, we define ℓi to be the smallest nongap at Q such
that ℓi ≡ i(mod γ) . Let h′i be the integer satisfying ℓi = i+ h
′
iγ. Then the column M
C
i+hγ is
effective if and only if h ≥ h′i. As mentioned in Subsection 4.2.2, the cell OE(i), 0 ≤ i ≤ γ−1,
deals with all operation functions associated with all columns in the class C(i). Thus a set of
internal control signals, denoted by {{wsi+hγ}h′i≤i≤hi, {Li+hγ}h′i≤i≤hi, {IMi+hγ}h′i≤i≤hi, {chki+hγ}0≤i≤hi
, EnableL,EnableD,Enableσ, SelectD, Selectσ,σ}, are required in each OE(i).
78
Enableσ to enable the updating of the coefficient vector and use Selectσ to select the input
coefficient vector which should be used in the updating process.
The set of signals {chki+hγ}0≤h≤hi marks whether a column in M has been checked.
chki+hγ = 0 means that the column M
C
i+hγ is not checked yet, while chki+hγ = 1 means
MCi+hγ has been checked. Thus the initial value of chki+hγ is one if 0 ≤ h ≤ h
′
i, and zero if
h′i < h ≤ hi. Notice that in Algorithm I, if a pivot is found in the column M
C
i+q′γ at step
r, where q′ satisfies i + q′γ = Lr−1(i), a × is marked and the coefficient vector is modified
only on that column and a new value Lr(i) is generated by (4.4) or (4.5). Suppose that
Lr−1(i) = i + h
′γ and Lr(i) = i + h”γ. To simplify the control circuits in our design, the
modified values of ×, ×−1 and coefficient vectors will be sent back to all PE(i+hγ)’s, where
h′ ≤ h < h”. Thus the signal IMi+hγ (Index of Modification), h
′
i ≤ h ≤ hi, is required
for indicating whether the modified data is necessary to be sent back to PE(i + hγ). If
IMi+hγ = 1, then the modified values of ×, ×−1 and coefficient vectors will be sent to
PE(i+ hγ); otherwise, IMi+hγ = 0 and OE(i) bypass the original values to PE(i+ hγ).
As mentioned in Section 3.2, the calculation of consistent terms of syndromes Soℓ in S (or
inM) may occur. Once a one-point AG code is constructed, the relation of consistent terms
with syndromes Soi and the locations of consistent terms in S (or inM) can be determined
by the curve and the selected basis of the vector space L(mQ). Let S ′oℓ denote a consistent
term of Soℓ , then we have
S ′oℓ = Soℓ +
ℓ−1∑
k=1
Sok . (4.9)
From (4.9), it can be seen that the consistent term S ′oℓ can be obtained by adding the original
syndrome Soℓ with some previously appeared syndromes. Suppose that the calculation of
consistent term S ′oℓ occurs in the column M
C
i+hγ for some h. Then the corresponding previ-
ously appeared syndromes will either arrive from the the top-end of PE(i+ hγ) of pipelined
through its left PE cells. Thus PE(i+ hγ) passes a previously appeared syndrome, say Sok ,
for the calculation of S ′oℓ to OE(i) at the step ok, and Sok will be registered in OE(i) for
80
Else (i.e. ain(Lr−1(i)) is an effective and known syndrome),
For all h, h′i ≤ h ≤ hi, set
a˜in(i+hγ) =
{
check − consistent(ain(i+hγ)), if wsi+hγ = 1 and E(IR)i+hγ = 1,
0, otherwise.
Set EnableD = 1.
Calculate the discrepancy D by D = σ · (ai+h′γ, a˜in(i+h′γ)), where ”·”
means the inner product of tow vectors and (ai+h′γ , a˜in(i+h′γ)) is selected
by SelectD.
If D = 0 and IRi+h′γ = ⌊
m+1
2
⌋+ g − 1,
the algorithm stops.
Else if D = 0,
Set Enableσ = 0.
Go to Bypass.
Else
If ×i+h′γ = 1 (i.e. D can be zeroed out),
Set Enableσ = 1.
Update σ ← σ − D((×−1)i+h′γ)(σ×i+h′γ , 0), where (×
−1)i+h′γ and
σ×i+h′γ are selected by Selectσ.
Go to Bypass.
Else (i.e. D can not be zeroed out),
Go to Modify.
Else (i.e. k(IR)j+h′γ = 0 and ain(j+h′γ) is an unknown syndrome),
If AVi+h′γ = 1 (i.e. the candidate in M
C
Lr−1(i)
is available),
Set EnableD = 1.
Calculate @ by @ = −σ·(ai+h′γ , a˜in(i+h′γ)), where (ai+h′γ , a˜in(i+h′γ)) is selected
by SelectD.
Send @ to MVS → @MV S.
82
are selected by Selectσ.
Go to Bypass.
Else (i.e Li+hγ = 0 for every h, hi ≤ h ≤ hi, and every effective column in the class C(i) has
a pivot),
If k(IR)j = 1 (i.e. ain(Lr−1(i)) is a known syndrome),
For all h, h′i ≤ h ≤ hi, set
a˜in(i+hγ) =
{
check − consistent(ain(i+hγ)), if E(IR)i+hγ = 1,
0, otherwise.
Set EnableD = 0 and Enableσ = 0.
Go to Bypass.
Else (i.e. ain(Lr−1(i)) is an unknown syndrome),
Hold and wait for the output @MV S from MVS.
For all h, h′i ≤ h ≤ hi, set
a˜in(i+hγ) =
{
check − consistent(@MV S), if E(IR)i+hγ = 1,
0, otherwise.
Set EnableD = 0.
Go to Bypass.
Bypass:
Set EnableL = 0.
For each h, h′i ≤ h
′ ≤ hi, set ×˜i+hγ = ×i+hγ, (×˜
−1
)i+hγ = (×−1)i+hγ and σ˜i+hγ =
(σi+hγ, 0).
Go to End.
Modify:
Set EnableL = 1.
For each h, h′i ≤ h ≤ hi,
84
where h′ is an integer such that i+ h′γ = Lr−1(i). With (4.10), we can see that the enable
signals EnableD, EnableL and Enableσ are simple functions of control signals {wsi+hγ},
{E(IR)i+hγ}, {×i+hγ}, {AVi+hγ} and {Li+hγ}. Therefore, all these enable signals can be
implemented by combinational logic circuits of control signals. Moreover, to determine the
value of AVi+hγ for some h, we first notice that
AVi+hγ =
{
1, if h ≥ h′ and ×i+hγ = 0,
0, otherwise.
(4.11)
Thus the signal AVi+hγ can be implemented by simple combinational logic of {chki+hγ} and
{×i+hγ}. The implementation of other control signals, such as {Li+hγ}, {IMi+hγ} and
{chki+hγ} can be achieved by a finite state machine with states {Li+hγ}. We will discuss
the finite state machine in detail by an example in the later section.
Figure 4.6 and Figure 4.7 demonstrate the two main computation functions with OE(i).
Suppose that the currently used coefficient vector is σj . If EnableD = 1, a discrepancy D
or an available candidate value for the first confronted syndrome is calculated by using the
rightmost j − 1 finite-field multipliers in the circuit in Figure 4.6. If D is nonzero, then the
control signal Enableσ is examined to decide whether the currently used coefficient σj
needs to be updated by the circuit in Figure 4.7. Observe that when the operation of
updating occurs, then the currently used coefficient σj will be substituted according to
(4.4) or (4.5). That is, there exists some h” such that the content in σ should be replaced
by σ −D(×−1)i+h”γ(σi+h”γ, 0). The corresponding (×−1)i+h”γ and (σi+h”γ, 0) are selected
by two multiplexers via the signal Selectσ. We note that the rightmost component of the
coefficient vector stored in σ of an OE cell is designed to be always one. Consequently,
only k − 1 finite-field multipliers are necessary for the cell OE(i) in Figure 4.6, where k is
an integer such that i+ hiγ = ok. Also note that the rightmost component of σ×i+hγ , for all
h, 0 ≤ h ≤ hi, is either 0 or 1. Therefore, the corresponding multiplication with
D(×−1)i+hγ in the operation of modifying coefficient vector can be implemented by a
multiplexer, as shown in Figure 4.7. Hence only k − 1 finite-field multipliers are necessary
for the cell OE(i) in Figure 4.7, too. Since the time for the computation of discrepancy (or
86
Table 4.1: The truth table of E(IR)j and k(IR)j for the cell PE(j), for a given Hermitian
code CΩ(23Q)
k(IR)j
IRj E(IR)j j = 8 j = 9 j = 10 j = 12 j = 13 j = 14
0 1 1 1 1 1 1 1
1 0 1 1 1 1 1 1
2 0 1 1 1 1 1 1
3 0 1 1 1 1 1 1
4 1 1 1 1 1 1 1
5 1 1 1 1 1 1 1
6 0 1 1 1 1 1 1
7 0 1 1 1 1 1 1
8 1 1 1 1 1 1 1
9 1 1 1 1 1 1 1
10 1 1 1 1 1 1 0
11 0 1 1 1 1 0 0
12 1 1 1 1 0 0 0
13 1 1 1 1 0 0 0
14 1 1 1 0 0 0 0
15 1 1 0 0 0 0 0
16 1 0 0 0 0 0 0
17 1 0 0 0 0 0 0
Since the Hermitian code CΩ(23Q) over GF (4
2) has γ = 4, we can divide the columns in
M into four classes: C(0) = {MC0 ,M
C
4 ,M
C
8 ,M
C
12}, C
(1) = {MC1 ,M
C
5 ,M
C
9 ,M
C
13},
C(2) = {MC2 ,M
C
6 ,M
C
10,M
C
14}, and C
(3) = {MC3 ,M
C
7 ,M
C
11}. For each i, 0 ≤ i ≤ 3, all
columns in the ith class C(i) can share the same operation functions in the cell OE(i).
Notice that the columns in C(3) are all non-effective. Therefore, OE(3) is empty. Since we
have h0 = 12 = o7, h1 = 13 = o8, and h2 = 14 = o9, thus totally 6 + 7 + 8 = 21 finite-field
multipliers and 4 finite-field inverters are required in our design.
Recall that as mentioned in Section 4.3, each cell OE(i) needs a finite state machine with
states {Li+hγ}h′i≤h≤hi to implement the function of updating Lr(i) (if necessary) at a
specific step r. To explain the state transition table of the finite state machine in OE(i)
concisely, we introduce a set of intermediary signals {Iℓi+hγ}0≤h≤hi (Index of ℓ×). The
88
Table 4.2: The transition tables of the state machine in OE cells, for a given Hermitian code
CΩ(23Q).
L0 L4 L8 L12 Iℓ0 Iℓ4 Iℓ8 Iℓ12 L˜0 L˜4 L˜8 L˜12 ˜chk0 ˜chk4 ˜chk8 ˜chk12 IM0 IM4 IM8 IM12
1000 01∗∗ 0100 1100 1000
001∗ 0010 1110 1100
0001 0001 1111 1110
0000 0000 1111 1111
0100 001∗ 0010 1110 0100
0001 0001 1111 0110
0000 0000 1111 0111
0010 0001 0001 1111 0010
0000 0000 1111 0011
0001 0000 0000 1111 0001
0000 0000 0000 1111 0000
(a) The transition table of the state machine in OE(0).
L5 L9 L13 Iℓ1 Iℓ5 Iℓ9 Iℓ13 L˜5 L˜9 L˜13 ˜chk1 ˜chk5 ˜chk9 ˜chk13 IM5 IM9 IM13
100 001∗ 010 1110 100
0001 001 1111 110
0000 000 1111 111
010 0001 001 1111 010
0000 000 1111 011
001 0000 000 1111 001
000 0000 000 1111 000
(b) The transition table of the state machine in OE(1).
L10L14 Iℓ2 Iℓ6 Iℓ10 Iℓ14 L˜10 L˜14 ˜chk2 ˜chk6 ˜chk10 ˜chk14 IM10 IM14
10 0001 01 1111 10
0000 00 1111 11
01 0000 00 1111 01
00 0000 00 1111 00
(c) The transition table of the state machine in OE(2).
90
              
     
  
     
  
 
  

  
  
  






	















 

 

 

 

 

 

  









	
ﬀ



ﬁ
ﬂ

	



ﬁ
ﬂ
ﬃ



ﬁ
ﬃ
ﬃ





ﬃ
ﬃ
ﬃ






ﬃ
ﬃ
ﬃ


 
!






  



ﬃ
ﬃ
ﬃ







ﬃ
ﬃ
ﬃ
"
#$
Figure 4.3: The data flows of two-dimensional array and one-dimensional array of Gaussian
elimination without column exchange.
92
     


1, −jjσ


1,jσ





2, −jjσ



     




	


1−
′′
×Rq











i)( 1−×
γihi+
−× )( 1


)
,0,...,0(
γ″+
×
ihi














),0,...,0(
i×

γihi+×

R
ihi

1,
γ″+
×
R
j
ihi

2, −×
″+ γ
R
j
ihi

1, −×
″+ γ

ﬀﬁ




)1,,...,,0,...,0(),0,...,0( 1,1, −= jjjj σσﬂ
ﬃ


	

 
!

ﬁ


"
#
	


#
	



$$#

#
	
 %





Figure 4.7: The circuit for updating coefficient vectors in the cell OE(i).
151617181920212223000000 SSSSSSSSS
& & &&&&&&&&&&&&&
'(
)
*+ ,
)-
+ ,
).
+ ,
)
/+ ,
)0
+ ,
)
1+
,
2334
'(
)5
+
'(
)
6+
'(
)7
+
'(
)
8+
'(
)-
*+
'(
)-.
+
'(
)-
/+
'(
)-5
+
0S 4S 5S 8S 9S 10S 12S 13S 14S∆ ∆ ∆ ∆ ∆ ∆
9 :
; : <
: = : >? : >>: >@ : >A :
B
(
CDE
B
(
CFE
B
(
C
G
E
B
(
CHE
IJ
K L
MNO
P Q
L
ONRS TU
VW
X
W
Y
Z
[
NO U
L
\
RO
W
M
]
^
U
L
ROM
L_ _L
SNU
F
G
H `
Y a b c d
Figure 4.8: The proposed implementation of the PESBM algorithm via systolic array for the
Hermitian code CΩ(23Q).
94
fa,b = x
ayb, i.e.,
Sa,b =
n−1∑
i=0
eifa,b(Pi). (5.1)
The order of Sa,b is defined to be the pole order of fa,b at the infinity point P∞, which is
equal to aq + b(q + 1). Since cHTm = 0, the syndromes with order no greater than m can be
generated from the received word, i.e., Sa,b =
∑n−1
i=0 rifa,b(Pi), and those syndromes are
called known syndromes.
An polynomial σ(x, y) in GF (q2)[x, y] is called an error-locator polynomial if it satisfies
σ(Pij ) = 0, ∀j = 1, . . . , t. For an error pattern e, the set of all error locator polynomials
forms an error locator ideal. With the knowledge of known syndromes and by using the the
Feng-Rao majority voting principle, an error-locator polynomial with least pole order at
P∞ or a Gro¨bner basis of the error locator ideal can be calculated. Also, all syndromes of
any order can be determined by using the Feng-Rao majority voting principle.
Under the assumption t ≤ t∗(m), the error-locator polynomial with least pole order at P∞
can be expressed of the form σ(x, y) =
∑t∗(m)+1
ℓ=1 σℓfoℓ , where foℓ is the ℓth elements in the
basis of L(mP∞). In the rest of this paper, we only consider the error-locator polynomial
σ(x, y) with least pole order at P∞ and let Q1, · · · , Qs be the zeros of σ(x, y). For
convenience, we call Q1, · · · , Qs potential error points. Notice that the set of error points
{Pi1, · · · , Pit} is contained in {Q1, · · · , Qs} and some values ei corresponding the potential
error point Qi in the error pattern e are zeros. Thus the syndrome Sa,b in (5.1) can be
rewritten as
Sa,b =
s∑
i=1
eifa,b(Qi). (5.2)
For a fixed j, 1 ≤ j ≤ s, a separating function σj(x, y) for the potential error point
Qj = (αj, βj) is a polynomial in GF (q
2)[x, y] with the property
σj(Qi) = 0, for all 1 ≤ i ≤ s and i 6= j,
σj(Qj) 6= 0.
(5.3)
96
Theorem 16. Let σ(x, y) =
∑t∗(m)+1
ℓ=1 σℓfoℓ be an error-locator polynomial and Qi,
i = 1, . . . , s, be the potential error points on σ(x, y). For a fixed potential error point
Qj = (αj, βj), the quotient polynomial q
(1)
j (x, y) in GF (q
2)[x, y] constructed as
q
(1)
j (x, y) =
σ(x, y)Hq(x, βj)− σ(x, βj)Hq(x, y)
(x− αj)(y − βj)
(5.6)
has the property {
νQj(q
(1)
j ) = νQj(σ)− 1,
νQi(q
(1)
j ) ≥ νQi(σ), 1 ≤ i ≤ s, i 6= j.
(5.7)
5.2 An Algorithm for the Error-value Evaluation of
Hermitian Codes with One Error-Locator
Polynomial
Let σ(x, y) =
∑t∗(m)+1
ℓ=1 σℓfoℓ be an error-locator polynomial with least pole order at P∞ and
Qi, i = 1, . . . , s, be the potential error points on σ(x, y). Consider the potential error point
Qi and the local ring OQi(Hq). Since Qi is a zero of σ(x, y), σ(x, y) is in the maximal ideal
MQi(Hq), i.e., νQi(σ) > 0. Thus from (5.3), we can see that σj is a separating function for
the point Qj if and only if
νQi(σj) > 0, for all 1 ≤ i ≤ s and i 6= j,
νQj(σj) = 0.
(5.8)
Consider the polynomial q
(1)
j (x, y) defined in Theorem 16. Let σ
(1)
j (x, y) be the remainder
of q
(1)
j (x, y) divided by Hq(x, y), i.e.,
σ
(1)
j (x, y) = q
(1)
j (x, y)−Hq(x, y)q(x, y), (5.9)
for some q(x, y) ∈ GF (q2)[x, y]. Then we have
νQi(σ
(1)
j ) ≥ min{νQi(q
(1)
j (x, y)), νQi(Hq(x, y)q(x, y))}
= min{νQi(q
(1)
j (x, y), νQi(Hq(x, y)) + νQi(q(x, y))},
for all i = 1, . . . , s. Since Hq(x, y) = xq+1 − yq − y ≡ 0 in the function field GF (q2)(x, y),
we have νQi(Hq(x, y)) =∞ for all i = 1, . . . , s. From (5.7), we can see that
νQj(q
(1)
j (x, y)) = νQj(σ)− 1 <∞,
98
(b) Calculate
q
(k+1)
j (x, y) =
σ
(k)
j (x, y)Hq(x, βj)− σ
(k)
j (x, βj)Hq(x, y)
(x− αj)(y − βj)
and
σ
(k+1)
j (x, y) = q
(k+1)
j (x, y) mod (x
q+1 − yq − y)
=
∑
a,b σ
(k+1)
j,a,b x
ayb.
(c) If σ
(k+1)
j (αj, βj) = 0, let k ← k + 1 and go to step (b).
(d) ej =
∑
a,b σ
(k+1)
j,a,b
Sa,b∑
a,b σ
(k+1)
j,a,b
αaj β
b
j
.
5.3 An Efficient Hardware Architecture for the
Error-Value Evaluation of Hermitian Codes
In this section, we firstly describe the functions of the error-value solver of Hermitian codes
according to Algorithm I. Then we present the main circuits in the error-value solver by
utilizing the Horner’s loop in the decoding of Reed-Solomon codes. Finally, we propose a
division circuit for calculating σ
(k+1)
j (x, y) in Algorithm I via systolic arrays.
5.3.1 Functions of the Error-Value Solver
Assume that all the syndromes Sa,b involved in (5.5) are known by Feng-Rao majority
voting scheme. From Algorithm I, we can see that there are four tables needed for the data
storage in the error-value solver when computing the polynomial σ
(k)
j (x, y) =
∑
a,b σ
(k)
j,a,bx
ayb
in the kth iteration at the potential error position Qj . The first table, called σ-table, is
used to store the two-dimensional coefficients σ
(k)
j,a,b in the polynomial σ
(k)
j (x, y). Since we
need to calculate the polynomials Hq(x, βj) and σ
(k)
j (x, βj) in step (b) of Algorithm I, two
tables, named Hx-table and σx-table, are required to save the coefficients of Hq(x, βj) and
σ
(k)
j (x, βj). Moreover, we use an additional table, called φ-table, to store the temporary
coefficients.
100
Step I : ( set the initial values for a new potential error point QNP )
Load σa,b’s to σ-table ( i.e., set the initial polynomial σ
(0)
NP (x, y) to be σ(x, y)).
Calculate Hq(x, βNP ) and store the coefficients hNP,a of Hq(x, βNP ) =
∑q+1
a=0 hNP,ax
a
into Hx-table.
Calculate σ
(0)
NP (x, βNP ) and store the coefficients σ
(0)
NP,a of σ
(0)
NP (x, βNP ) =
∑
a σ
(0)
NP,ax
a
into σx-table.
Trigger SNI and set NI = 1.
Step II : ( the NI-th iteration when calculating the separation function of the point QNP )
(1) Calculate
φ
(NI)
NP (x, y) , σ
(NI−1)
NP (x, y)Hq(x, βNP )− σ
(NI−1)
NP (x, βNP )Hq(x, y)
=
∑
a,b φ
(NI)
NP,a,bx
ayb,
(5.11)
and store the coefficients φ
(NI)
NP,a,b of φ
(NI)
NP (x, y) into φ-table.
(2) Calculate
q
(NI)
NP (x, y) ,
φ
(NI)
NP
(x,y)
(x−αNP )(y−βNP )
(5.12)
and
σ
(NI)
NP (x, y) , q
(NI)
NP (x, y) mod (x
q+1 − yq − y)
=
∑
a,b σ
(NP )
NI,a,bx
ayb,
(5.13)
and update σ-table by the coefficients σ
(NI)
NP,a,b of σ
(NI)
NP (x, y).
(3) Calculate
σ
(NI)
NP (x, βNP ) (5.14)
and update σx-table by the coefficients σ
(NI)
NP,a of σ
(NI)
NP (x, βNP ) =
∑
a σ
(NI)
NP,ax
a.
(4) Calculate σ
(NI)
NP (αNP , βNP ) = σ
(NI)
NP (x, βNP )|x=αNP .
Step III: ( check whether the polynomial σ(NI)(x, y) is a seperating function)
If σ
(NI)
NP (αNP , βNP ) = 0 (i.e., σ
(NI)
NP (x, y) is still not a separation function)
Trigger SNI and set NI = NI + 1.
Go to Step II.
102
  )(βf
β
nn
fff ,,, 10 −
)(α   	
 	 			  α
Figure 5.2: A Horner’s loop.
initialized as zeros and the final result is stored in the registers, too. Detailed operations
are described as follows. The value of the registers is fn when the first clock comes. And
the next incoming coefficient fn−1 will update the result as (fnβ + fn−1). When the third
clock comes, the result becomes to ((fnβ + fn−1)β + fn−2), and so forth. Until the last
coefficient f0 enters, the computation is completed. Thus totally (n + 1) clocks are
required. As we can see, this circuit needs only one finite-field adder, one finite-field
multiplier and a buffer, which is simple. We call this circuit a Horner’s loop and the
multiplication factor β the feedback gain.
By writing hNP,0 = −(β
q
NP + βNP ) as f(βNP ), where f(y) = −y
q − y, the Horner’s loop in
Figure 5.2 can be used to evaluate hNP,0. In our utility, the feedback gain β and the input
coefficients f0, . . . , fn in Figure 5.2 must be replaced by βNP and 0,−1, 0 . . . 0︸ ︷︷ ︸
q−2
,−1,
respectively. We can see that only one finite field multiplier and (q + 1) clocks are needed
to generate the coefficients hNP,0. Since the values of hNP,a, a = 1, . . . , q + 1, are
immediately stored into hx-table once the signal SNP is triggered, totally one finite field
multiplier and (q + 1) clocks are needed to generate the coefficients hNP,a of Hq(x, βNP ).
For convenience to describe the circuits in the following steps and derive the complexity of
our error-value solver, we need the following definition:
Definition 17. Let F (x, y) =
∑
a,b Fa,bx
ayb be a bi-variate polynomial in GF (q2)[x, y]. For
104
Notice that the calculation of Hq(x, βNP ) and σ
(0)
NP (x, βNP ) can be started at the same
time. Hence to complete Step I, totally (q + 2) multipliers and max{q + 1, b(0)NP + 1} clock
periods are needed.
Now, we consider the calculation of the polynomial φ
(NI)
NP (x, y) in Step II(1). Since
Hq(x, βNP ) = xq+1 + hNP,0 and σ
(NI−1)
NP (x, βNP ) =
∑a(NI−1)
NP
a=0 σ
(NI−1)
NP,a x
a, we have
φ
(NI)
NP (x, y) , σ
(NI−1)
NP (x, y)Hq(x, βNP )− σ
(NI−1)
NP (x, βNP )Hq(x, y)
= σ
(NI−1)
NP (x, y)(x
q+1 + hNP,0)− (
∑a(NI−1)
a=0 σ
(NI−1)
NP,a x
a)(xq+1 − yq − y)
= φ
(NI)′
NP (x, y) + φ
(NI)′′
NP (x, y),
(5.18)
where φ
(NI)′
NP (x, y) = σ
(NI−1)
NP (x, y)(x
q+1 + hNP,0) and
φ
(NI)′′
NP (x, y) = −(
∑a(NI−1)
NP
a=0 σ
(NI−1)
NP,a x
a)(xq+1 − yq − y).
From (5.18), we can write φ
(NI)′′
NP (x, y) as
φ
(NI)′′
NP (x, y) = −(
∑a(NI−1)
NP
a=0 σ
(NI−1)
NP,a x
a)(xq+1 − yq − y)
= −
∑a(NI−1)
NP
a=0 σ
(NI−1)
NP,a x
a+q+1 + (
∑a(NI−1)
NP
a=0 σ
(NI−1)
NP,a x
a)y
+(
∑a(NI−1)
NP
a=0 σ
(NI−1)
NP,a x
a)yq
=
∑
a,b φ
(NI)′′
NP,a,bx
ayb.
Then the coefficients φ
(NI)′′
NP,a,b of φ
(NI)′′
NP (x, y) can be determined as
φ
(NI)′′
NP,a+q+1,0 = −σ
(NI−1)
NP,a , 0 ≤ a ≤ a
(NI−1)
NP
φ
(NI)′′
NP,a,1 = σ
(NI−1)
NP,a , 0 ≤ a ≤ a
(NI−1)
NP
φ
(NI)′′
NP,a,q = σ
(NI−1)
NP,a , 0 ≤ a ≤ a
(NI−1)
NP
φ
(NI)′′
NP,a,b = 0, otherwise.
(5.19)
Let φ[a, b] denote the (a, b)-position in the content of φ-table, which stores the coefficient
φ
(NI)
NP,a,b of φ
(NI)
NP (x, y). To save the complexity for calculating φ
(NI)
NP (x, y), we immediately
store the coefficients of φ
(NI)′′
NP (x, y) once the signal SNI is triggered. From (5.19), we can
see the initial values of φ[a, b] = φ
(NI)′′
NP,a,b in φ-table can be loaded directly from the σx-table .
Now, we consider the first term φ
(NI)′
NP (x, y) in (5.18). From (5.13) and (5.15), we can see
106
that the coefficient φ
(NI)′
NP,a+q+1,b is just the input φ
(NI−1)
NP,a,b and the coefficient φ
(NI)′
NP,a,b can be
generated by multiplying the input σ
(NI−1)
NP,a,b with hNP,0. Since
φ
(NI)
NP (x, y) = φ
(NI)′
NP (x, y) + φ
(NI)′′
NP (x, y), to obtain the coefficients φ
(NI)
NP,a,b (or φ
(NI)
NP,a+q+1,b) of
φ
(NI)
NP (x, y), we need to add the coefficients φ
(NI)′
NP,a,b (or φ
(NI)′
NP,a+q+1,b) with the value
φ[a, b] = φ
(NI)′′
NP,a,b (or φ[a+ q + 1, b] = φ
(NI)′′
NP,a+q+1,b) pre-stored in the φ-table. As we can see in
Figure 5.3, the circuit needs only one multiplier. Since 0 ≤ b ≤ b(NI−1)NP , the circuit requires
(b
(NI−1)
NP + 1) clocks to sequentially generate the coefficients φ
(NI)
NP,a,b and φ
(NI)
NP,a+q+1,b from
b = b
(NI−1)
NP to b = 0. Since a ranges from 0 to a
(NI−1)
NP , (a
(NI−1)
NP + 1) copies of Figure 5.3 are
needed if we want to simultaneously generate the coefficients φ
(NI)
NP,a,b for all a. From (5.13),
we can see that a
(NI−1)
NP ≤ q for all NP and NI. Thus to complete the calculation of
φ
(NI)
NP (x, y) in Step II(1), totally (q + 1) finite-field multipliers and (b
(NI−1) + 1) clocks are
needed.
Since the computation in Step II(2) involves a division process for bivariate polynomials,
which is complicated, we leave it to the next subsection. Now, we consider the calculation
of the coefficients σ
(NI)
NP,a of the polynomial σ
(NI)
NP (x, βNP ) in Step II(3). From (5.16) and
(5.17), we can see that for a fixed a, the Horner’s loop in Figure 5.2 can be used to
calculate the coefficient σ
(NI)
NP,a with the feedback gain β and the input coefficients f0, . . . , fn
replaced by βNP and σ
(NI)
NP,a,0, . . . , σ
(NI)
NP,a,b
(NI)
NP
, respectively. Since σ
(NI)
NP,a,b = 0 for all b > b
(NI)
NP ,
it needs at most (b
(NI)
NP + 1) clock periods to generate the coefficient σ
(NI)
NP,a for a given a.
Since deg σ
(NI)
NP (x, βNP ) ≤ a
(NI)
NP , (a
(NI)
NP + 1) Horner’s loops are needed if we want to
simultaneously generate all coefficients σ
(NI)
NP,a, 0 ≤ a ≤ a
(NI)
NP . From (5.13), we can see that
a
(NI)
NP ≤ q for all NP and NI. Thus at most (q + 1) Horner’s loops are needed for this step.
Notice that the calculations of σ
(NI)
NP,a’s will not take place at the same time for different NI,
thus the (q + 1) Horner’s loops for generating the coefficients σ
(NI)
NP,a in Step I(1) can be
reused in this step for efficiency. Therefore, no additional multiplier and (b
(NI)
NP + 1) clocks
are required to calculate all coefficients σ
(NI)
NP,a of σ
(NI)
NP (x, βNP ) in Step II(3).
108
1) ( −
 
ba
ba
NI
baNP S
,
,
)(
,,
σ
),()( NPNPNINP βασ
NPe
Figure 5.4: A circuit for calculating the error value eNP .
5.3.3 The Division Circuit in the Error-Value Evaluator
In this subsection, we focus on designing an architecture for calculating the polynomial
σ
(NI)
NP (x, y) in Step II(2) of Function I. As we can see in the last subsection, all operations
related to the polynomial σ
(NI−1)
NP (x, y) have completed and σ-table contains the coefficients
σ
(NI−1)
NP,a,b of σ
(NI−1)
NP (x, y) in the end of Step II(1) for the NI-th iteration. Thus for
convenience to update the contents of σ-table we can reset σ-table to be zero at the
beginning of Step II(2).
To calculate the polynomial σ
(NI)
NP (x, y), we firstly consider the polynomial φ
(NI)
NP (x, y) in
(5.11). Let A
(NI)
NP and B
(NI)
NP be the (1, 0) and (0, 1)-weighted degree of φ
(NI)
NP (x, y)
respectively, as defined in Definition 17. Then φ
(NI)
NP (x, y) can be expressed as
φ
(NI)
NP (x, y) =
∑A(NI)
NP
a=0
∑B(NI)
NP
b=0 φ
(NI)
NP,a,bx
ayb
=
∑A(NI)
NP
+B
(NI)
NP
d=0
∑B(NI)
NP
b=0 φ
(NI)
NP,d−b,bx
d−byb,
(5.21)
where φ
(NI)
NP,d−b,b = 0 if d− b < 0 or d− b > A
(NI)
NP . (Note that the coefficient φ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
may be equal to zero.) Since deg1,0 σ
(NI−1)
NP (x, y) = a
(NI−1)
NP and deg1,0Hq(x, y) = q + 1, we
have deg σ
(NI−1)
NP (x, βNP ) ≤ a
(NI−1)
NP and degHq(x, βNP ) ≤ q + 1. Since
deg0,1 σ
(NI−1)
NP (x, y) = b
(NI−1)
NP , deg0,1Hq(x, y) = q and from (5.11), we have
A
(NI)
NP , deg1,0 φ
(NI)
NP (x, y) ≤ a
(NI−1)
NP + q + 1
B
(NI)
NP , deg0,1 φ
(NI)
NP (x, y) ≤ max{b
(NI−1)
NP , q}.
(5.22)
Now, we consider the quotient polynomial q
(NI)
NP (x, y) =
φ
(NI)
NP
(x,y)
(x−αNP )(y−βNP )
defined in (5.13).
110
set {xayb|0 ≤ a ≤ A(NI)NP , 0 ≤ b ≤ B
(NI)
NP }, where N = (A
(NI)
NP + 1)(B
(NI)
NP + 1). Then
φ
(NI)
NP (x, y) can be expressed as
φ
(NI)
NP (x, y) = φ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP yB
(NI)
NP
+φ
(NI)
NP,A
(NI)
NP
−1,B
(NI)
NP
xA
(NI)
NP
−1yB
(NI)
NP + φ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
−1
xA
(NI)
NP yB
(NI)
NP
−1
+ · · ·+ φ(NI)NP,0,0
=
∑N−1
i=0 φ
(NI)
NP,iψi,
where φ
(NI)
NP,i , φ
(NI)
NP,ai,bi
if ψi = x
aiybi. To calculate q
(NI)
NP (x, y), we can divide φ
(NI)
NP (x, y)
from the greatest term φ
(NI)
NP,N−1ψN−1 = φ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP yB
(NI)
NP to the smallest term
φ
(NI)
NP,0ψ0 = φ
(NI)
NP,0,0 by the denominator polynomial (x− αNP )(y − βNP ) according to the
(1, 1)-revlex monomial ordering.
The detailed division procedure for calculating q
(NI)
NP (x, y) is described as follows. Let
Q(j)(x, y) and R(j)(x, y) be the quotient term and remainder polynomial at the jth step of
the division procedure. Initially, we let q
(NI)
NP (x, y) = 0, Q
(0)(x, y) = 0 and
R(0)(x, y) = φ
(NI)
NP (x, y). At the first step, we divide the greatest term
R
(0)
N−1ψN−1 = φ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP yB
(NI)
NP in R(0)(x, y) by the leading term xy of D(x, y) to
obtain the first quotient term
Q(1)(x, y) = φ
(NI)
NP,A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP
−1yB
(NI)
NP
−1 = Q
(1)
A
(NI)
NP
−1,B
(NI)
NP
−1
xA
(NI)
NP
−1yB
(NI)
NP
−1. Then we
update the quotient polynomial q
(NI)
NP (x, y) by q
(NI)
NP (x, y) +Q
(1)(x, y) and the remainder
polynomial by R(1)(x, y) , R(0)(x, y)−Q(1)(x, y)D(x, y) =
∑N−1
i=0 R
(1)
i ψi with R
(1)
N−1 = 0.
Notice that when calculating R(1)(x, y), except the the divided monomial ψN−1, only the
coefficients of the monomials xA
(NI)
NP
−1yB
(NI)
NP , xA
(NI)
NP yB
(NI)
NP
−1 and xA
(NI)
NP
−1yB
(NI)
NP
−1, whose
(1, 1)-weights are all less than the (1, 1)-weight of ψN−1 = x
A
(NI)
NP yB
(NI)
NP , in the product
Q(1)(x, y)(x− αNP )(y − βNP ) are subtracted from R(0)(x, y). At the jth step, the (j − 1)th
remainder polynomial R(j−1)(x, y) can be written as R(j−1)(x, y) =
∑N−1
i=0 R
(j−1)
i ψi with
R
(j−1)
i = 0, ∀i ≥ N − (j − 1). Suppose that ψN−j = x
aN−jybN−j . We divide the term
R
(j−1)
N−j ψN−j in R
(j−1)(x, y) by the leading term xy of D(x, y) to obtain the jth quotient
term Q(j)(x, y) = Q
(j)
aN−j−1,bN−j−1
xaN−j−1ybN−j−1. Then we update the quotient polynomial
q
(NI)
NP (x, y) by q
(NI)
NP (x, y) +Q
(j)(x, y) and the remainder polynomial by
112
no greater than A
(NI)
NP +B
(NI)
NP − 1. Since
Q(1)(x, y)D(x, y) = (
∑B(NI)
NP
b=0 Q
(1)
d−b−1,b−1x
d−b−1yb−1)(xy − αNP y − βNPx+ αNPβNP )
=
∑B(NI)
NP
b=0 Q
(1)
d−b−1,b−1(x
d−byb − αNPxd−b−1yb − βNPxd−byb−1
+αNPβNPx
d−b−1yb−1),
with d = A
(NI)
NP +B
(NI)
NP , we can see that except the divided monomials x
d−byb with
d = A
(NI)
NP +B
(NI)
NP , the product Q
(1)(x, y)D(x, y) only influences the coefficients of the
monomials with (1, 1)-weights equal to A
(NI)
NP +B
(NI)
NP − 1 or A
(NI)
NP +B
(NI)
NP − 2 when
calculating R(1)(x, y) from R(0)(x, y). At the (j + 1)th step, the jth remainder polynomial
R(j)(x, y) can be written as R(j)(x, y) =
∑A(NI)
NP
+B
(NI)
NP
d=0
∑B(NI)
NP
b=0 R
(j)
d−b,bx
d−byb with R
(j)
d−b,b = 0,
for all 0 ≤ b ≤ B(NI)NP and d ≥ (A
(NI)
NP +B
(NI)
NP )− (j − 1), i.e., the (1, 1)-weight of lm(R
(j)) is
no greater than A
(NI)
NP +B
(NI)
NP − j. We divide all terms R
(j)
d−b,bx
d−byb, b = 0, . . . , B
(NI)
NP , in
R(j)(x, y) with d = A
(NI)
NP +B
(NI)
NP − j by the leading term xy of D(x, y) to obtain the
(j + 1)th homogeneous quotient polynomial
Q(j+1)(x, y) =
∑B(NI)
NP
b=0 R
(j)
d−b,bx
d−b−1yb−1
=
∑B(NI)
NP
b=0 Q
(j+1)
d−b−1,b−1x
d−b−1yb−1
(5.25)
with d = A
(NI)
NP +B
(NI)
NP − j. Then we update the quotient polynomial q
(NI)
NP (x, y) by
q
(NI)
NP (x, y) +Q
(j+1)(x, y) and calculate the (j + 1)th remainder polynomial
R(j+1)(x, y) , R(j)(x, y)−Q(j+1)(x, y)D(x, y) =
∑A(NI)
NP
+B
(NI)
NP
d=0
∑B(NI)
NP
b=0 R
(j+1)
d−b,bx
d−byb with
R
(j+1)
d−b,b = 0 for all 0 ≤ b ≤ B
(NI)
NP and d ≥ A
(NI)
NP +B
(NI)
NP − j, i.e., the (1, 1)-weight of
lm(R(j+1)) is no greater than A
(NI)
NP +B
(NI)
NP − (j + 1). Since
Q(j+1)(x, y)D(x, y) = (
∑B(NI)
NP
b=0 Q
(j+1)
d−b−1,b−1x
d−b−1yb−1)(xy − αNPy − βNPx+ αNPβNP ),
where d = A
(NI)
NP +B
(NI)
NP − j,
=
∑B(NI)
NP
b=0 Q
(j+1)
d−b−1,b−1(x
d−byb − αNPx
d−b−1yb − βNPx
d−byb−1
+αNPβNPx
d−b−1yb−1),
we can see that except the divided monomials xd−byb with d = A
(NI)
NP +B
(NI)
NP − j, the
product Q(j+1)(x, y)D(x, y) only influences the coefficients of the monomials with
(1, 1)-weights A
(NI)
NP +B
(NI)
NP − j − 1 and A
(NI)
NP +B
(NI)
NP − j − 2 when calculating R
(j+1)(x, y)
from R(j)(x, y). After the (A
(NI)
NP +B
(NI)
NP + 1)-th step is completed, we have
114
the node (A
(NI)
NP , B
(NI)
NP ) on the slanted diagonal with d = A
(NI)
NP +B
(NI)
NP in the DG with
k = 1, of R(0)(x, y) by lt(D) = xy. Then the first homogeneous quotient polynomial
Q(1)(x, y) = R
(0)
A
(NI)
NP
,B
(NI)
NP
xA
(NI)
NP
−1yB
(NI)
NP
−1 = Q
(1)
A
(NI)
NP
−1,B
(NI)
NP
−1
xA
(NI)
NP
−1yB
(NI)
NP
−1
is generated. By propagating the products Q
(1)
A
(NI)
NP
−1,B
(NI)
NP
−1
αNP , Q
(1)
A
(NI)
NP
−1,B
(NI)
NP
−1
βNP and
−Q(1)
A
(NI)
NP
−1,B
(NI)
NP
−1
αNPβNP to the neighbor nodes (A
(NI)
NP − 1, B
(NI)
NP ), (A
(NI)
NP , B
(NI)
NP − 1) and
(A
(NI)
NP − 1, B
(NI)
NP − 1) of the node (A
(NI)
NP , B
(NI)
NP ) respectively, the coefficients of the next
remainder polynomial R(1)(x, y) can be calculated. At the second iteration, we divide the
terms R
(1)
A
(NI)
NP
−1,B
(NI)
NP
xA
(NI)
NP
−1yB
(NI)
NP and R
(1)
A
(NI)
NP
,B
(NI)
NP
−1
xA
(NI)
NP yB
(NI)
NP
−1 with (1, 1)-weight
d = A
(NI)
NP +B
(NI)
NP − 1, which correspond to the nodes (A
(NI)
NP − 1, B
(NI)
NP ) and
(A
(NI)
NP , B
(NI)
NP − 1) on the slanted diagonal with d = A
(NI)
NP +B
(NI)
NP − 1 in the DG with k = 2,
of R(1)(x, y) by lt(D) = xy. Then we get the second homogeneous quotient polynomial
Q(2)(x, y) = R
(1)
A
(NI)
NP
−1,B
(NI)
NP
xA
(NI)
NP
−2yB
(NI)
NP
−1 +R
(1)
A
(NI)
NP
,B
(NI)
NP
−1
xA
(NI)
NP
−1yB
(NI)
NP
−2
= Q
(2)
A
(NI)
NP
−2,B
(NI)
NP
−1
xA
(NI)
NP
−2yB
(NI)
NP
−1 +Q
(2)
A
(NI)
NP
−1,B
(NI)
NP
−2
xA
(NI)
NP
−1yB
(NI)
NP
−2.
By propagating the products
Q
(2)
A
(NI)
NP
−2,B
(NI)
NP
−1
αNP , Q
(2)
A
(NI)
NP
−2,B
(NI)
NP
−1
βNP ,−Q
(2)
A
(NI)
NP
−2,B
(NI)
NP
−1
αNPβNP ,
Q
(2)
A
(NI)
NP
−1,B
(NI)
NP
−2
αNP , Q
(2)
A
(NI)
NP
−1,B
(NI)
NP
−2
βNP ,−Q
(2)
A
(NI)
NP
−1,B
(NI)
NP
−2
αNPβNP
to the neighbor nodes of the nodes (A
(NI)
NP − 1, B
(NI)
NP ) and (A
(NI)
NP , B
(NI)
NP − 1), the
coefficients of the next remainder polynomial R(2)(x, y) can be calculated. At the (j + 1)th
iteration, we divide all terms R
(j)
d−b,bx
d−byb, b = 0, . . . , B
(NI)
NP , with (1, 1)-weight
d = A
(NI)
NP +B
(NI)
NP − j, which correspond to the nodes (d− b, b), b = 0, . . . , B
(NI)
NP , in the
slanted diagonal with d = A
(NI)
NP +B
(NI)
NP − j in the DG with k = j + 1, of R
(j)(x, y) by
lt(D) = xy. Then the (j + 1)th homogeneous quotient polynomial
Q(j+1)(x, y) =
∑B(NI)
NP
b=0 R
(j)
d−b,bx
d−b−1yb−1 =
∑B(NI)
NP
b=0 Q
(j+1)
d−b−1,b−1x
d−b−1yb−1 with
d = A
(NI)
NP +B
(NI)
NP − j is generated. By propagating the products Q
(j+1)
d−b−1,b−1αNP ,
Q
(j+1)
d−b−1,b−1βNP and −Q
(j+1)
d−b−1,b−1αNPβNP with d = A
(NI)
NP +B
(NI)
NP − j and b = 0, . . . , B
(NI)
NP to
the nodes on the slanted diagonals with d = A
(NI)
NP +B
(NI)
NP − j − 1 and
116
the same time, all coefficients φ
(NI)
NP,d−b,b, b = 0, . . . , B
(NI)
NP , of φ
(NI)
NP (x, y) must be input into
the PE’s simultaneously. Then totally (A
(NI)
NP +B
(NI)
NP + 1) steps are required to complete
the division process. Since φ
(NI)
NP (x, y) can be divided by D(x, y) and lt(D) = xy with the
(1, 1)-revlex monomial ordering, all coefficients of the monomials xa, a = 0, . . . , A
(NI)
NP , or
yb, b = 0, . . . , B
(NI)
NP , in R
(A
(NI)
NP
+B
(NI)
NP
−a)(x, y) or R(A
(NI)
NP
+B
(NI)
NP
−b)(x, y) must be zero. Thus
the leftmost PE corresponding to a = 0 in the one-dimensional array can be omitted and
the number of PEs in the one-dimensional array becomes A
(NI)
NP . Also, the number of steps
can be reduced to (A
(NI)
NP +B
(NI)
NP − 1), since only the terms lying the slanted diagonal with
d = A
(NI)
NP +B
(NI)
NP to d = 2 are required to be computed. From (5.22), we can see that for
the NI-th iteration of Step II(2) in Function I, at most (a
(NI−1)
NP + q + 1) PEs and
(a
(NI−1)
NP + q +max{b
(NI−1)
NP , q}) steps are needed to completed the division process for
calculating q
(NI)
NP (x, y) in (5.13).
Now, we derive the cell functions of a PE cell. From (5.25), we can see that at the (j +1)th
step, the (j + 1)th homogeneous quotient polynomial Q(j+1)(x, y) is a linear combination of
monomials with the same (1, 1)-weight A
(NI)
NP +B
(NI)
NP − j − 2. Thus we have
q
(NI)
NP,d−b−1,b−1 = Q
(j+1)
d−b−1,b−1 = R
(j)
d−b,b, (5.26)
where d = A
(NI)
NP +B
(NI)
NP − j, b = 1, . . . , B
(NI)
NP and 1 ≤ d− b ≤ A
(NI)
NP . From Figure 5.6 and
(5.26), the coefficients q
(NI)
NP,a,b in the quotient polynomial q
(NI)
NP (x, y) =
∑
a,b q
(NI)
NP,a,bx
ayb can
be determined recursively as follows:
For d = A
(NI)
NP +B
(NI)
NP down to d = 2
(the (j + 1)th iteration, where j = A
(NI)
NP +B
(NI)
NP − d)
118
a PE cell is then derived as follows:
qout = φin +R + C,
C˜ = D1[qout]× βNP ,
R˜ = (D1[qout]−D1[D1[qout]× βNP ])× αNP
= (D1[qout]−D1[C˜])× αNP ,
(5.29)
where C˜ and R˜ are the updated variables for C and R after one step.
 
 
NPα
R~
outq
inφ
R
NPβ
C~
][1 outqD
Figure 5.8: The circuits inside a PE cell.
Figure 5.8 demonstrates the circuit inside a PE cell. From Figure 5.8, we can see that there
are two finite-field multipliers in a PE cell. Since the leftmost PE cell with a = 1 needs not
to pass any signal to its left-hand side, only the circuits inside the dash box in Figure 5.8
are necessary for the first PE cell. Since there are A
(NI)
NP PEs, totally (2(A
(NI)
NP − 1) + 1)
multipliers are needed for calculating q
(NI)
NP (x, y). From (5.22), we can see that for the
NI-th iteration in Step II of Function I, at most (2a
(NI−1)
NP + 2q + 1) multipliers are
necessary. Since each step in the division process spends only one clock cycle,
(a
(NI−1)
NP + q +max{b
(NI−1)
NP , q}) clock cycles are required to completed the division process.
Since a
(NI−1)
NP ≤ q by (5.13), the numbers of multipliers and consumed clock cycles can be
further upper bounded by (4q + 1) and (2q +max{b(NI−1)NP , q}) respectively when
calculating q
(NI)
NP (x, y) in (5.13).
Finally, we consider the calculation of σ
(NI)
NP (x, y) in Step II(2). From (5.13) and (5.24),
120
(a, b)th position of σ-table. Thus to calculate σ
(NI)
NP (x, y) from q
(NI)
NP (x, y), no additional
finite-field multipliers and clock cycles are needed. Therefore, totally (4q + 1) multipliers
and (2q +max{b(NI−1)NP , q}) clock cycles are required to finish Step II(2).
Note that to save the time complexity, the calculation of q
(NI)
NP (x, y) in Step II(2) does not
need to wait until the computations of φ
(NI)
NP (x, y) in Step II(1) are completed. Actually,
the calculation in Step II(2) can be started after one clock of the first results φ
(NI)
NP,a,b
(NI−1)
NP
,
0 ≤ a ≤ 2q + 1, coming out from the circuit shown in Figure 5.3. Consider the coefficient
φ
(NI)
NP,a,b, which is generated from the circuit in Figure 5.3 at the (b
(NI−1)
NP − b+ 1)th clock
and will be input to the one-dimensional systolic array at the
((a
(NI−1)
NP + q + 1) + (max{b
(NI−1)
NP , q})− (a+ b) + 1) th clock. Since
(a
(NI−1)
NP + q + 1) + (max{b
(NI−1)
NP , q})− (a+ b) + 1 ≥ b
(NI−1)
NP − b+ 1, the coefficient φ
(NI)
NP,a,b
can be registered in the φ-table once it is generated and then input to the one-dimensional
systolic array after ((a
(NI−1)
NP + q+1)+ (max{b
(NI−1)
NP , q})− (a+ b)+ 1)− (b
(NI−1)
NP − b+1)+1
clocks. Thus the number of consumed clock cycles to complete Step II(1) and Step II(2)
is upper bound by (2q +max{b(NI−1)NP , q}+ 1).
5.4 Complexity Analysis
Table 5.1: Hardware and time complexities of each step in Function I.
number of multipliers number of consumed clocks
Step I q+2 max{q + 1, b(0)NP + 1}
Step II(1) q+1
2q +max{b(NI−1)NP , q}+ 1Step II(2) 4q+1
Step II(3) 0 b
(NI)
NP + 1
Step II(4) 0 a
(NI)
NP + 1
Step III(1) 0 b
(νQNP (σ))
NP + 1
Step III(2) 1 0
Since multiplication is the most complicated operation in a finite field when considering the
hardware circuitry, we express the hardware complexity in terms of the number of
122
b
(0)
NP = b
(0) ≤ ⌊
ot∗(m)
q+1
⌋ for all NP . Now, we consider the upper bounds of a(NI)NP and b
(NI)
NP .
From (5.13), we have
a
(NI)
NP , deg1,0(σ
(NI)
NP )
= deg1,0(q
(NI)
NP mod (x
q+1 − yq − y))
≤ q,
(5.35)
for any NP and NI. The following lemma describes an upper bound of b
(NI)
NP :
Lemma 20. Consider the potential error point QNP on an error-locator polynomial
σ(x, y) =
∑t∗(m)+1
ℓ=1 σℓfoℓ . Let σ
(NI)
NP (x, y) be the polynomial defined in (5.13) and b
(NI)
NP be
the (0, 1)-weighted degree of σ
(NI)
NP (x, y). Then for any NI ≥ 1, we have
b
(NI)
NP ≤ b˜
(0) +NI(q − 1), (5.36)
where b˜(0) , max{⌊
ot∗(m)
q+1
⌋, q}.
Proof. We prove this lemma by induction on NI. Consider the case NI = 1. From (5.22)
and (5.23), we have
deg1,0 q
(1)
NP (x, y) ≤ A
(1)
NP − 1 ≤ a
(0) + q
deg0,1 q
(1)
NP (x, y) ≤ B
(1)
NP − 1 ≤ max{b
(0)
NP , q} − 1 ≤ max{⌊
ot∗(m)
q+1
⌋, q} − 1 = b˜(0) − 1.
Then q
(1)
NP (x, y) can be written as
q
(1)
NP (x, y) =
a(0)+q∑
a=0
b˜(0)−1∑
b=0
q
(1)
NP,a,bx
ayb.
From (5.13), we have
σ
(1)
NP (x, y) , q
(1)
NP (x, y)mod (x
q+1 − yq − y).
Then
σ
(1)
NP (x, y) =
∑a(0)+q
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a,bx
ayb mod (xq+1 − yq − y)
= (
∑q
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a,bx
ayb +
∑a(0)+q
a=q+1
∑b˜(0)−1
b=0 q
(1)
NP,a,bx
ayb) mod (xq+1 − yq − y)
= (
∑q
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a,bx
ayb +
∑a(0)−1
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a+q+1,bx
a+q+1yb) mod (xq+1 − yq − y)
=
∑q
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a,bx
ayb +
∑a(0)−1
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a+q+1,bx
a(yq + y)yb
=
∑q
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a,bx
ayb +
∑a(0)−1
a=0
∑b˜(0)−1
b=0 q
(1)
NP,a+q+1,bx
a(yq+b + y1+b)
=
∑q
a=0
∑b˜(0)+(q−1)
b=0 (q
(1)
NP,a,b + q
(1)
NP,a+q+1,b−q + q
(1)
NP,a+q+1,b−1)x
ayb.
124
From (5.35) and (5.36), tNP can be further upper bounded by
tNP ≤ b˜(0) + 1
+
∑νQNP (σ)
NI=1 ((2q +max{b˜
(0) + (NI − 1)(q − 1), q}+ 1) + (b˜(0) +NI(q − 1) + 1) + (q + 1))
+(b˜(0) + νQNP (σ)(q − 1) + 1)
= b˜(0) + 1 +
∑νQNP (σ)
NI=1 (2q + 2b˜
(0) + 2NI(q − 1) + 4) + (b˜(0) + νQNP (σ)(q − 1) + 1)
= b˜(0) + 1 + ((q − 1)νQNP (σ)
2 + (3q + 3 + 2b˜(0))νQNP (σ)) + (b˜
(0) + νQNP (σ)(q − 1) + 1)
= (q − 1)νQNP (σ)
2 + (2b˜(0) + 4q + 2)νQNP (σ) + 2b˜
(0) + 2.
(5.37)
Since QNP is a simple point on the Hermitian curve Hq(x, y) = xq+1 − yq − y, we have
νQNP (σ) = I(QNP ,Hq ∩ σ),
where I(QNP ,Hq ∩ σ) is the intersection number of Hq(x, y) and σ(x, y) at QNP [7]. Thus
(5.37) can be further written as
tNP ≤ (q − 1)I(QNP ,Hq ∩ σ)2 + (2b˜(0) + 4q + 2)I(QNP ,Hq ∩ σ) + 2b˜(0) + 2. (5.38)
Suppose QNP , NP = 1, . . . , s are all potential error points on the error-locator polynomial
σ(x, y). Let Φ(m) be the total number of consumed clocks in our error-value solver for the
Hermitian code Cm over GF (q
2). From (5.38), Φ(m) can be expressed as
Φ(m) =
∑s
NP=1 tNP
≤
∑s
NP=1((q − 1)I(QNP ,Hq ∩ σ)
2 + (2b˜(0) + 4q + 2)I(QNP ,Hq ∩ σ) + 2b˜(0) + 2)
= (q − 1)
∑s
NP=1 I(QNP ,Hq ∩ σ)
2 + (2b˜(0) + 4q + 2)
∑s
NP=1 I(QNP ,Hq ∩ σ)
+2(b˜(0) + 1)s.
(5.39)
To derive a more explicit upper bound of Φ(m), we need the well-known Bezout’s Theorem
in the following:
Theorem 21. (Bezout’s Theorem) Let F1 and F2 be projective plane curves of degrees m1
and m2 respectively. Assume that F1 and F2 have no common component. Let P
∗ is a
point in the projective plane P2 and I(P ∗, F1 ∩ F2) is the intersection number of F1 and F2
at P . Then ∑
P ∗
I(P ∗, F1 ∩ F2) = m1m2.
126
Equation (5.43) is true since∑s
NP=1 I(QNP ,Hq ∩ σ)
2 ≤ (
∑s
NP=1 I(QNP ,Hq ∩ σ))
2
≤ ((q + 1)η)2.
Moreover, since I(QNP ,Hq ∩ σ) ≥ 1 for all NP = 1, . . . , s, and from (5.42), we have
s ≤ (q + 1)η. (5.44)
From Lemma 22 and (5.44), the total number Φ(m) of consumed clocks in (5.39) can be
further upper bounded as
Φ(m) ≤ (q − 1)(q + 1)2η2 + (2b˜(0) + 4q + 2)(q + 1)η + 2(b˜(0) + 1)(q + 1)η. (5.45)
Take the Hermitian code C21 over GF (4
2) for example. Since the the designed
error-correcting capacity t∗(21) for the Hermitian code C21 is equal to 5, the error-locator
polynomial with least pole order at P∞ can be expressed as
σ(x, y) = σ1 + σ2x+ σ3y + σ4x
2 + σ5xy + σ6y
2. Then we have η = 2 and
b˜(0) = max{⌊o6
5
⌋, 4} = max{2, 4} = 4. Thus from (5.45), we have Φ(21) ≤ 660.
128
are interesting and important in coding theory. All the results in Section 2.1, 2.2 and 2.3
are given in [5] and the readers familiar with this topic can skip them.
6.1.1 Places and Discrete Valuation Rings
As discussed in [7, Chapter 7]), any projective curve X (singular or nonsingular) has a
nonsingular (smooth) model X˜ of X and K(X˜) ∼= K(X), where K(X) and K(X˜) are
function fields of X and X˜ respectively. This relation is called a birationally equivalence
between X˜ and X. For a projective curve X and K is algebraically closed, there is a
one-to-one correspondence between the places of X and the discrete valuation rings of
K(X), see [7]. From the viewpoint of function fields, the reasonable definition of places is
put in this section. From now on, we concentrate on the function fields K(X) over K for
some projective curve X and the special case that K (not necessary algebraically closed) is
algebraically closed in K(X).
Definition 23. An (algebraic) function field F over K (F/K) is a field extension of K
such that for some x ∈ F\K which is transcendental over K, [F : K(x)] is finite. 
Definition 24. A valuation ring O of a function field F/K is a subring of F satisfying
1. K ( O ( F .
2. For any x ∈ F , x ∈ O or x−1 ∈ O.

Definition 25. Let R be a Noetherian (every ideal of R is finitely generated) domain
which is not a field. If R has only one maximal ideal and the maximal ideal is generated by
one element, then R is called a discrete valuation ring. 
The definition of places in a function field is based on the following theorem.
130
Proof. Given any x 6= 0 in F . First assume that x ∈ OP . Suppose that for some x ∈ OP , x
is not of the form utm, where u is a unit in OP and m ∈ N∪ {0}. Then consider the infinite
sequence {xi} defined as x1 = x and for i > 1, xi ∈ OP satisfies xi−1 = xit. Then we have
an infinite ascending chain of proper ideals of OP as (x1) ( (x2) ( (x3) ( · · · which
contradicts to the fact that OP is Noetherian. So any x ∈ OP has the form. For x /∈ OP ,
since x−1 ∈ OP , x−1 = wts for some unit w ∈ OP and s ∈ N ∪ {0}. Hence x = w−1t−s. If
x = u1t
m1 = u2t
m2 , where ui are units. Assume that m2 ≥ m1. Then u
−1
2 u1 = t
m2−m1 . So
m1 = m2 and u1 = u2. This shows that this representation is unique.
Check that vP (·) is independent of the choice of t. If t0 is another prime element for P . Let
t = u1t
k1
0 and t0 = u2t
k2 , where u1, u2 are units and k1, k2 > 0. Then t = (u1u
k1
2 )t
k1k2
implies that k1 = k2 = 1. So t0 = vt for some unit v.
Next show that vP (·) is a discrete valuation. Obviously, vP (·) satisfies the conditions
(1),(2),(4) in Definition 28. Let xi = uit
mi (i = 1, 2), where ui are units and mi ∈ Z,
assume that m1 ≥ m2. Then x1 + x2 = u2tm2(u
−1
2 u1t
m1−m2 + 1). Since
u−12 u1t
m1−m2 + 1 ∈ OP , vP (u
−1
2 u1t
m1−m2 + 1) ≥ 0. Hence vP (x1 + x2) ≥ m2. This shows
that vP (·) satisfies condition (3).
To check condition (5), it only needs to show that K ∩ P = {0}. If x 6= 0 and x ∈ K ∩ P ,
then x−1 ∈ K but x−1 /∈ OP which contradicts to the fact that K ⊆ OP . 
Definition 30. Assume F/K is a function field and x ∈ F , P ∈ PF . If vP (x) > 0, then we
say that x has a zero at P . If vP (x) < 0, then we say that x has a pole at P . 
Given a place P , consider the quotient field FP := OP/P , since K ∩ P = {0}, K can be
embedded into FP . Moreover, FP is a finite extension of K ( [5, Proposition I.1.14]).
The degree of P is defined to be [FP : K] and denoted as deg P . A place of degree 1 is
called a rational place. For P ∈ PF and x ∈ OP , the coset of x in OP module P is denoted
by x(P ). If x 6∈ OP , x(P ) is defined to be ∞. So for any x ∈ F , we can consider x as a
132
Lemma 31. For any x ∈ F , deg(x) = 0. So deg(x)0 = deg(x)∞, where
(x)0 :=
∑
P∈PF : vP (x)>0
vP (x)P,
(x)∞ :=
∑
P∈PF : vP (x)<0
−vP (x)P.
Moreover, deg(x)0 = [F : K(x)] if x /∈ K. 
Definition 32. For x ∈ F , the order of x (ord(x)) is defined to be ord(x) = deg(x)0. 
By an easy inductive proof, we can get that dimD ≤ degD + 1 and degD − dimD ≤ c for
some constant c (independent of D). Define the genus of F (g(F )) to be
g(F ) := max{degD − dimD + 1 | D ∈ DF}.
It is obvious that g(F ) ≥ 0. The next is the Riemann’s theorem [5, Theorem I.4.17].
Theorem 33. (Riemann’s Theorem) Let F/K be a function field of genus g.
(a) For any divisor D ∈ DF ,
dimD ≥ degD + 1− g.
(b) There is an integer c, depending on F/K such that
dimD = degD + 1− g.
whenever degD ≥ c.

Weil Differentials
In fact, Weil differentials and differentials are equivalent (for differentials, see [7, Chapter
8]). In the viewpoint of Weil differentials, any differential can be regarded as a K-linear
map from the set of adeles to the function field. It is easier to prove the Riemann-Roch
theorem by using Weil differentials than differentials.
134
Definition 38. The divisor (ω) of a Weil differential ω 6= 0 is the uniquely determined
divisor of F/K satisfying
1. ω vanishes on AF ((ω))+ F .
2. If ω vanishes on AF (D) + F then D ≤ (ω).
A divisor W is called a canonical divisor of F/K if W = (ω) for some ω ∈ ΩF . 
In the Riemann’s Theorem, we have the inequality dimD ≥ degD + 1− g for D ∈ DF . Let
i(D) = dimD − degD − 1 + g, the index of speciality of D. It is clear that i(D) ≥ 0 and
i(D) = 0 if degD is sufficiently large by Riemann’s theorem. The Riemann-Roch
theorem [5, Theorem I.5.15] gives the exact value of i(D).
Theorem 39. (Riemann-Roch Theorem) Let W be a canonical divisor, then for any
D ∈ DF ,
dimD = degD + 1− g + dim(W −D),
where g is the genus of F/K. 
6.1.3 Algebraic Geometric Codes
Geometric Goppa (AG) codes were first proposed in [2]. They are a generalization of the
classical Goppa codes [48]. In [11], there is an equivalence definition of AG codes, and from
this viewpoint, AG codes can be considered as a generalization of Reed-Solomon codes.
Here we just introduce the later definition, for the first, it needs some knowledge about the
residues of Weil differentials.
Definition 40. Let F/Fq be a function field, D = P1 + P2 + · · ·+ Pn, where P1, P2, . . . , Pn
are rational places in PF . If G is a divisor of F and suppD ∩ suppG = ∅, then define the
geometric Goppa Code CL(D,G) as
CL(D,G) := {(x(P1), . . . , x(Pn)) | x ∈ L(G)} ⊆ F
n
q .
136
Consider the special case G = rQ and D = P1 + · · ·+ Pn for some distinct rational places
P1, P2, . . . , Pn, Q in PF and some positive integer r > 0. The geometric Goppa code
Cr := CL(D, rQ)
is determined by the space L(rQ). If r > 0 and L(rQ)\L((r − 1)Q) = ∅, then the integer r
is called a gap at Q, otherwise r is called a nongap at Q. There are g gaps at
Q [5, Theorem I.6.7], where g is the genus of F/Fq. The set of nongaps at Q forms a
numerical subsemigroup of N and is called the Weierstrass semigroup with respect to Q,
denoted as SQ.
Definition 42. A set {x1, . . . , xl} ∈ F (F being a function field) is called a Weierstrass
generating set with respect to a place Q if (xi)∞ = viQ for vi distinct and {v1, . . . , vl}
generates the Weierstrass semigroup SQ with respect to Q. In particular, if V = {v1, . . . , vl}
is a minimal generating set for SQ (this means that any subset of V can’t generate SQ),
then {x1, . . . , xl} is called a reduced Weierstrass generating set (with respect to Q). 
6.1.4 Long Codes
In the consideration for the construction of long block codes, the asymptotic problem is
interesting. In this section, we state one of the asymptotic problem, for the others, see [49].
AG codes play an important role in this asymptotic problem in coding theory.
Asymptotic Problem
For an [n, k, d] code C, the parameters R(C) = k/n and δ(C) = d/n are important. For long
code construction, we want to know the behavior of the parameters δ(C) and R(C) when
the length n of the code becomes large. This is one of the asymptotic problems. Fix q (a
power of a prime), consider the set Vq containing all the point (δ, R) such that there exists
a code C over Fq (the finite field of q elements) with the parameters (δ(C), R(C)) = (δ, R).
138
Proof. Consider a sequence of function fields Fi/Fq of genus gi and with Ni rational places
such that (such a sequence exists by the definition of A(q)) gi →∞ as i→∞ and
lim
i→∞
Ni/gi = A(q).
For 0 ≤ δ ≤ 1− A(q)−1, choose a sequence ri > 0 such that
lim
i→∞
ri/Ni = 1− δ and ri < Ni − 1.
For each i, consider the code Ci = CL(Di, riQi), where Qi is an arbitrarily chosen rational
place of Fi/Fq and Di is the formal sum of all rational places of Fi/Fq except for Qi. Then
deg(riQi −Di) < 0 and so dim(riQi −Di) = 0. The parameters ki and di then satisfies
ki = dim(riQi)− dim(riQi −Di) = dim(riQi) ≥ ri + 1− g
by the theorem of Riemann-Roch and
di ≥ Ni − 1− deg(riQi) = Ni − ri − 1.
Then
R(Ci) ≥
ri + 1
Ni
−
gi
Ni
and δ(Ci) ≥ 1−
ri − 1
Ni
.
Assume that R(Ci)→ R and δ(Ci)→ δ′ (otherwise choose a convergent subsequence). Thus
R ≥ 1− δ −A(q)−1 and δ′ ≥ δ. Since the function αq is decreasing,
αq(δ) ≥ αq(δ
′) ≥ 1− δ −A(q)−1.

The Tsfasman-Vlaˇdut-Zink Bound
The Drinfeld-Vlaˇdut Bound says that A(q) ≤ q1/2 − 1 ( [5, Theorem V.3.6]). Moreover, if
q is a square, A(q) = q1/2 − 1 [4]. Apply Theorem 44, we get the Tsfasman-Vlaˇdut-Zink
bound for a square q as
αq(δ) ≥ 1− δ −
1
q1/2 − 1
for 0 ≤ δ ≤ 1−
1
q1/2 − 1
.
140
Note that the limit exists and so this definition is well-defined [34]. If λ(F) > 0, then F is
called to be asymptotically good. F is called asymptotically optimal if λ(F) = q1/2 − 1.
We say that F is asymptotically bad if λ(F) = 0.
6.2.1 Finite Extensions of Function Fields
In this section we consider the finite extensions F ′/K of function fields F/K, this means
that F ′ is a finite dimensional field extension of the field F .
Ramification index and difference exponent
Let P be a place in PF . A place P ′ ∈ PF ′ is called an extension of P and denoted by P ′|P
if P ⊂ P ′. If t is a prime element for P , define e(P ′|P ) := vP ′(t), the ramification index of
P ′ over P . By the definition of e(P ′|P ), vP ′(x) = e(P
′|P ) · vP (x) for any x ∈ F . The
extension P ′|P is called ramified if e(P ′|P ) > 1 and unramified if e(P ′|P ) = 1. In fact,
e(P ′|P ) ≤ [F ′ : F ] [5, Corollary III.1.12]. P ′|P is called totally ramified if
e(P ′|P ) = [F ′ : F ]. Let O′P := {x ∈ F
′ | x is integral over OP}, the integral closure of OP
in F ′. Consider the set
CP := {x ∈ F
′ | TrF ′/F (x · O
′
P ) ⊆ OP},
where TrF ′/F is the trace map of the field extension F
′/F [51, pp. 289]. Then CP is an
O′P -module and CP = t · O′P for some t ∈ F ′ [5, Proposition III.4.2]. Define the different
exponent of P ′ over P by
d(P ′|P ) := −vP ′(t).
By [5, Proposition III.4.2], d(P ′|P ) is well-defined and d(P ′|P ) ≥ 0. Moreover,
d(P ′|P ) = 0 for almost all P ∈ PF and P
′|P . So we can define the divisor
Diff(F ′/F ) :=
∑
P∈PF
∑
P ′|P
d(P ′|P ) · P ′,
142
polynomial a(T ) ∈ K[T ], the formal derivative a′(T ) = a0, so a(T ) is separable if and only
if a0 6= 0.
Theorem 46. Let F/K be a function field with characteristic p > 0 and a(T ) ∈ K[T ] is a
separable additive polynomial of degree pn. Assume that all the roots of a(T ) = 0 lie in K.
Consider the field extension F ′ = F (x) with a(x) = u for some u ∈ F . If for some place
Q ∈ PF , vQ(u) = −m < 0 and gcd(m, p) = 1, then
(a) Q is totally ramified in F ′/F and the field extension F ′/F is Galois with
[F ′ : F ] = pn. The Galois group of F ′/F is isomorphic to (Z/pZ)n. If Q′|Q is the
extension of Q in F ′, then
d(Q′|Q) = (pn − 1)(m+ 1).
(b) If P ∈ PF and vP (u) ≥ 0, then dF ′/F (P ) = 0 and hence P is unramified in F ′/F .
Proof. (a) Assume Q′|Q is an extension of Q in F ′, then a(x) = u implies that vQ′(x) < 0
and
pnvQ′(x) = e(Q
′|Q)vQ(u) = e(Q
′|Q)(−m).
Then e(Q′|Q) = pn and vQ′(x) = −m since gcd(p,m) = 1 and e(Q′|Q) ≤ pn. Therefore,
[F ′ : F ] = pn and so a(T )− u is irreducible over K. It is then obvious that F ′/F is a
splitting (by the assumption that all the roots of a(T ) = 0 lie in K) and separable
extension and so is Galois.
Next claim that AutFF
′ ∼= (Z/pZ)n, where AutFF ′ is the Galois group for F ′ over F . Let
Γ = {α ∈ F | a(α) = 0}, note that Γ ⊂ K by hypothesis. For 0 6= α ∈ Γ, l · α ∈ Γ for any
l ∈ N, so α, 2α, . . . , (p− 1)α are distinct in Γ since p is prime and α 6= 0. If iα = jβ for
α, β ∈ Γ and 1 ≤ i, j ≤ p− 1, then choose s, t such that is+ pt = 1, then
α = (is+ pt)α = isα = jsβ.
So there exists α1, . . . , αm in Γ such that each cyclic group Gi generated by αi satisfies
Gi ∩Gj = {0} whenever j 6= i and Γ = ⊕Gi, i.e., m = n and Γ ∼= (Z/pZ)n. It is obvious
144
6.3 The Tower FH
For q > 0 a power of a prime, consider the tower of function fields FH = {Hi | i ≥ 1}
defined as follows:
H1 = Fq2(x1), where x1 is transcendental over Fq2 .
For i ≥ 2, Hi = Hi−1(xi), where x
q
i + xi = x
q+1
i−1 .
(6.1)
In this section, we will show that this tower FH is asymptotically bad. However, the
construction of the AG codes corresponding to these function fields is simple since it is easy
to find a Weierstrass generating set.
Theorem 47. Let FH = {Hi | i ≥ 1} be defined as in above and n ≥ 2.
(a) If P∞ ∈ PH1 is the simple pole of x1, then it is totally ramified in Hn/H1 and (also
denote the extension of P∞ in PHn as P∞) for all 1 ≤ i ≤ n,
vP∞(xi) = −q
n−i(q + 1)i−1. (6.2)
(b) Hn is Galois over H1 such that [Hn : H1] = q
n and the Galois group of Hi/Hi−1 for
each 2 ≤ i ≤ n is isomorphic to Z/pZ each of which is given by xi 7−→ xi + β for some
β ∈ Fq2 with β
q + β = 0.
(c) P∞ is the only one place with nonzero different exponent and the only one place
ramified in Hn/H1. The different exponent of P∞ in Hn/H1 is
dHn/H1(P∞) = (q − 1)
n−1∑
i=1
qn−1−i(q + 1)i + qn−1 − 1.
Proof.
(a) Let eHi/Hj (P∞) be the ramification index of P∞ in Hi/Hj. When n = 2,
q · vP∞(x2) = (q + 1)(−1) · eH2/H1(P∞).
146
Hn
qn−2 qn−3(q + 1) qn−i(q + 1)i−2 qn−i+1(q + 1)i−1
Fq2(x1, x2) Fq2(x2, x3)· · · Fq2(xi−1, xi) Fq2(xi, xi+1)
q q + 1 q q + 1 q q + 1 q
Fq2(x1) Fq2(x2) Fq2(x3) Fq2(xi−1) Fq2(xi)
By Theorem 46, the only place ramified in H2/H1 is P∞. And for 2 < i ≤ n, the
place ramified (and has nonzero difference exponent) in Hi/Hi−1 is a pole of xi. But
−vP∞(xi) = q
n−i(q + 1)i−1 = ord(xi), so there is only one pole P∞ of xi. Hence the
only one place ramified (has nonzero different exponent) in Hn/H1 is P∞ which has
degree 1.
Let di = dHi/H1(P∞). For i ≥ 3,
di = eHi/Hi−1(P∞) · dHi−1/H1(P∞) + dHi/Hi−1(P∞)
by [5, Corollary III.4.11]. By Theorem 46, d2(P∞) = (q − 1)(q + 2) and
dHi/Hi−1(P∞) = (q + 1)
i−1. So the assertion follows immediately as follows:
dn = q · dn−1 + (q − 1)((q + 1)
n−1 + 1)
= qn−2d2 + q
n−3(q − 1)((q + 1)2 + 1) + · · ·+ (q − 1)((q + 1)n−1 + 1)
= qn−2(q − 1)((q + 1) + 1) + (q − 1)
n−1∑
i=2
qn−1−i((q + 1)i + 1).
= (q − 1)
n−1∑
i=1
qn−1−i((q + 1)i + 1).
= (q − 1)
n−1∑
i=1
qn−1−i(q + 1)i + (q − 1)
n−1∑
i=1
qn−1−i.
= (q − 1)
n−1∑
i=1
qn−1−i(q + 1)i + qn−1 − 1.

148
The number of rational points in the projective curve Xn defined in (6.1) is q
n+1 + 1, this
follows by counting the solution (x0 : x1 : . . . : xn) ∈ P n(Fq2) of the homogeneous equations
xq2x0 + x2x
q
0 = x
q+1
1
xq3x0 + x3x
q
0 = x
q+1
2
...
xqnx0 + xnx
q
0 = x
q+1
n−1
.
Lemma 50. The projective curve Xn ⊂ P n(Fq2) defined in (6.1) has q
n+1 + 1 rational
points. 
Naturally, we then have a question: Is N(Hn) equal to the number of rational points at Xn,
i.e., N(Hn) = q
n+1 + 1? If so, then FH is asymptotically bad since
λ(FH) ≤ lim
n→∞
qn+1 + 1
1
2
(n− 1)(q − 1)qn−1
=
q2
q − 1
lim
n→∞
2
n− 1
= 0.
Theorem 51. N(Hn) = q
n+1 + 1.
Proof. The place P∞ is totally ramified (by Theorem 46). Claim that all the other
rational places in H1 split completely in Hn/H1. Prove this by induction on n:
n = 2, consider the equation ∏
β∈Ω0
(x2 − β) = x
q+1
1 .
If P0 ∈ PH1 is the zero of x1, let P
′ ∈ PH2 be an extension of P0, then P
′|P0 is unramified
by Theorem 46 and ∑
β∈Ω0
vP ′(x2 − β) = (q + 1).
Hence the place P ′ ∩ Fq2(x2) is a zero of
∏
β∈Ω0
(x2 − β) and so is the zero of x2 − β0 for
some β0 ∈ Ω0. Since each x2 − β has no common zero in Fq2(x2),
q + 1 = vP ′(x2 − β0).
150
we get the decompositions ∏
β∈Ω0
(xn − β) = x
q+1
n−1,∏
β∈Ωi
(xn − β) =
∏
α∈Γi
(xn−1 − α).
Since any rational place in Hn−1 except for P∞ is a zero of xn−1 − α for some α ∈ Fq2, the
same argument shows that they splits completely in Hn/Hn−1. This completes the proof. 
6.3.2 The Weierstrass semigroup
Fix n, the order of each xi is q
n−i(q + 1)i−1 and P∞ is the only pole of xi. When we
consider the Weierstrass semigroup S
(n)
P∞
with respect to P∞,
qn−1, qn−2(q + 1), . . . , (q + 1)n−1 are nongaps and forms a subsemigroup S ′ of S
(n)
P∞
. The
sequence (a1, . . . , an) := (q
n−1, qn−2(q + 1), . . . , (q + 1)n−1) is in fact a telescopic sequence,
the semigroup S ′ is therefore a telescopic semigroup [53]. The genus g(S ′) of
S ′ [53, Lemma 6.5] satisfies
2g(S ′)− 1 = −a1 + (q − 1)
n∑
i=2
ai
= −qn−1 + (q − 1)
n∑
i=2
qn−i(q + 1)i−1
= −qn−1 + (q − 1)
n−1∑
i=1
qn−i−1(q + 1)i
= 2g(Hn)− 1.
So g(S ′) = g(Hn) and hence S
′ = S
(n)
P∞
.
6.3.3 An explicit construction
Fix a function field F/Fq and a rational place Q. Let {φ1, . . . , φl} ⊂ F be a Weierstrass
generating set with respect to Q, then it is easy to find a basis for each corresponding AG
codes CL(D, rQ) (D = P1 + . . .+ PN is the divisor of the summation of all rational places
152
0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 0.1
10−8
10−7
10−6
10−5
10−4
10−3
10−2
10−1
Symbol error probability
B
lo
ck
 e
rro
r r
at
e
RS code
H−code 
H3−code
Figure 6.1: Block Error Rate versus Symbol Error Probability.
Choose r = 187, consider the (256, 128, d′) code CL(D, rP∞) over F16, where d
′ ≥ 69.
Compare this code with the (16, 8, 9) extended Reed-Solomon code and the (64, 32, 27)
Hermitian code [54]. They all have rate 1/2, and as seen in Figure 6.1, the (256, 128, d′)
code CL(D, rP∞) is better than the others two when the channel symbol error rate is no
more than 0.07.
154
algorithm is simple and efficien. The computation complexity of the algorithm is in the
order of O(γn2), which is the same as that of the Ko¨tter’s algorithm, where n is the code
length. Yet the PESBM algorithm is superior, comparing with the Ko¨tter’s algorithm, in
the following aspects:
1. With the early stopped property, the PESBM algorithm can save both processing
time and computation complexity.
2. With storing both nonzero discrepancy as well as the corresponding coefficient vector,
the proposed algorithm prevents from the additional multiplicative operations for the
normalization of the saved coefficient vector. The saved coefficient vector needs to be
normalized only when it is being used to update the currently used coefficient vector.
3. An accurate method of counting the available candidates is developed in the
algorithm. Based on the point of view from the Feng-Rao algorithm, the method to
count the total number of the available candidates is not correct in that of the
Ko¨tter’s algorithm.
In this project, we also developed an efficient implementation of the PESBM algorithm via
systolic array by adopting the γ-shift properties on the extended syndrome matrix M . The
time complexity of the proposed design is m+ g + 1 by using (t+ ⌊g−1
2
⌋+ 1) PE cells, g D
cells and γ OE cells. Totally, at most γ finite-field inverters,
γ(2t+2⌊ g−1
2
⌋−γ+1)
2
finite field
multipliers and γ finite state machines are required in the proposed architecture.
Therefore, the proposed design achieves a huge reduction on the hardware complexity by
only adding simple control circuits.
In this project, we have proposed an algorithm of error-value evaluation of Hermitian codes
by modifying the algorithm proposed in [31], when only one error-locator polynomial is
given instead of an ideal of error-locator polynomials. We also developed a hardware
architecture of the proposed algorithm and implemented via systolic array.
156
(a.2) If m is in the range (i− 1)(q2 − 1) ≤ m < (i− 1)(q2 − 1) + q2 + (q − 2)(q + 1),
then we have 0 < ki < ni and the set of roots of the leading monomial gii(t) of gi is
GF (q2)∗\ {α−(r+s(q+1)) ∈ GF (q2)∗ : 0 ≤ r ≤ q, 0 ≤ s ≤ q − 2,
rq + s(q + 1) + (i− 1)(q2 − 1) ≤ m},
where GF (q2)∗ is the multiplicative group GF (q2) \ {0}.
(a.3) If m < (i− 1)(q2 − 1), then we have ki = 0.
(b) Let i = q + 1.
(b.1) If m ≥ q(q2 − 1) + (q − 2)(q + 1), then kq+1 = nq+1.
(b.2) If m is in the range q(q2 − 1) ≤ m < q(q2 − 1) + (q − 2)(q + 1), then we have
0 < kq+1 < nq+1 and the set of roots of the leading monomial gq+1q+1(t) of gq+1 is
GF (q)∗ \ {α−s(q+1) ∈ GF (q)∗ : 0 ≤ s ≤ q − 2, s(q + 1) + q(q2 − 1) ≤ m},
where GF (q)∗ is the multiplicative group GF (q) \ {0}.
(b.3) If m < q(q2 − 1), then we have kq+1 = 0.
Proof. Since a root of the gii(t) corresponds to a marked box in the ith row of the root
diagram, items (a.1), (a.2) and (b.1) follow immediately from Theorem 3.3(2), Theorem 3.4
and Theorem 3.3(3) in [12] respectively. By substituting q + 1, (
∏q
k=1Mk(y))y
s,
Pq+1,j = (0, α
λq+1+j(q+1)) and vq+1(t) =
∑q−2
j=0(α
s(q+1)t)j for i, (
∏i−1
k=1Mk(y))x
rys,
Pi,j = (α
j, αλi+j(q+1)) and vi(t) =
∑q2−2
j=0 (α
r+s(q+1)t)j respectively and following the
derivation in the proof of Theorem 3.4 in [12], the result of item (b.2) can be obtained.
(Here, we have changed the notations m, ℓm+1, hm+1(t), ℓi and hi(t) used in the proof of
Theorem 3.4 in [12] to our notations q, λq+1, vq+1(t), λi and vi(t).) Thus only items (a.3)
and (b.3) are left to be proved.
We first prove item (b.3). Since the marked boxes in the root diagram for Cm form a
subset of the marked box in the root diagram for Cm′ for any m
′ < m, it suffices to show
that the (q+ 1)th row of the root diagram for Cm is full if m = q(q
2− 1)− 1. Now consider
158
Thus the (q + 1)th row of the the root diagram for Cm is full.
We finally prove item (a.3) by induction. Since the marked boxes in the root diagram for
Cm form a subset of the marked box in the root diagram for Cm′ for any m
′ < m, it suffices
to show that for each 1 ≤ i ≤ q, if m = (i− 1)(q2 − 1)− 1, then the ith row of the root
diagram for Cm is full. We fist consider i = q and let m = (q − 1)(q2 − 1)− 1. As discussed
in the proof of item (b.3), we can see that the (q + 2)th rows of the two root diagrams for
Cm and Cm+1 are full and the jth rows of the two root diagrams are empty for all
1 ≤ j ≤ q − 2. Note that (q + 1)th rows in the root diagrams for Cm and Cm+1 are both
full from (b.3). By following the same argument as in the proof of item (b.3), we can show
that the empty boxes in the (q− 1)th row of the root diagram for Cm are the same as those
in the (q − 1)th row of the root diagram for Cm+1. Thus the root diagrams for Cm and
Cm+1 differ only in the qth row. Since the qth row of the root diagram for Cm+1 contains
only one empty box and k(m+ 1) = k(m) + 1 again by (1.5), the qth row of the root
diagram for Cm must be full. With similar argument, we can show that if the ith row of
the root diagram for Cm, m = (i− 1)(q2 − 1)− 1, is full, then the (i− 1)th row of the root
diagram for Cm′ , m
′ = (i− 2)(q2 − 1)− 1, is also full. Hence by induction, for each
1 ≤ i ≤ q, if m = (i− 1)(q2 − 1)− 1, then the ith row of the root diagram for Cm is full. 2
Now we proceed to prove Theorem 2.
Proof of Theorem 2
We first proof that k1 ≥ k2 ≥ · · · ≥ kq. Let i(m) be the smallest positive integer such that
m < (i(m)− 1)(q2 − 1) + q2 + (q − 2)(q + 1). Consider the following cases:
• i(m) > q : By Proposition 52 (a.1), we have kj = nj for all j ≤ q. Thus k1 = · · · = kq.
• i(m) = q : Again by Proposition 52 (a.1), we have kj = nj for all j ≤ q − 1. Thus
k1 = · · · = kq−1 > kq by Proposition 52 (a.2).
160
• m < q(q2 − 1) : From Proposition 52 (b.3), we have kq+1 = 0 ≤ kq.
• q(q2 − 1) ≤ m < (q − 1)(q2 − 1) + q2 + (q − 2)(q + 1) : In this case, we have
0 < kq < nq, 0 < kq+1 < nq+1, and
kq = |{(r, s)|0 ≤ r ≤ q, 0 ≤ s ≤ q − 2, rq + s(q + 1) + (q − 1)(q
2 − 1) ≤ m}|
kq+1 = |{s|0 ≤ s ≤ q − 2, s(q + 1) + q(q
2 − 1) ≤ m}|
from Proposition 52 (a.2) and (b.2). Thus kq ≥ kq+1.
To finish our proof, consider kq+1 and kq+2. Since nq+2 = 1, we have kq+2 ≤ 1. If
kq+1 < kq+2, then we have kq+2 = 1 and kq+1 = 0. Since kq+2 = 1 implies that (0, 0, . . . , 0, 1)
is a codeword of Cm, the minimum distance dmin(m) of Cm must be equal to 1. However,
kq+1 = 0 implies that m < q(q
2 − 1) by Proposition 52 (b.3). From (1.6), the minimum
distance satisfies
dmin(m) ≥ q
3 −m > q,
which is a contradiction. Hence we conclude that k1 ≥ k2 ≥ · · · ≥ kq ≥ kq+1 ≥ kq+2. 2
.2 Proof of Lemma 3
With (3.2) and by induction, the kth column ak, 1 ≤ k ≤ n, of the original matrix A is a
linear combination of the first k columns a′v, 1 ≤ v ≤ k, of the left-reduced matrix A
′,
ak = γk−1a
′
1 + · · ·+ γ1a
′
k−1 + a
′
k (1)
where γv, 1 ≤ v ≤ k − 1, are constants and dependent on the column index k.
Now assume that the left-reduced matrix A′ of A has a pivot at the (i, j)th position. Then
as stated in (3.3), column aj of A is a partial linear combination of its previous columns
up to the (i− 1)th entry. Suppose that column aj of A is also a partial linear combination
of its previous columns up to the ith entry, i.e. there exist β1, . . . , βi−1 such that
au,j = −βj−1au,1 − · · · − β1au,j−1, ∀ 1 ≤ u ≤ i. (2)
162
Bibliography
[1] V. D. Goppa, “Codes associated with divisors,” Probl. Peredachi Inform., vol. 13,
no. 1, pp. 33–39, 1977.
[2] ——, “Codes on algebraic curves,” Dokl. Akad. Nauk SSSR, vol. 24, pp. 170–172, 1981.
[3] ——, “Algebraic-geometric codes,” Izv. Akad. Nauk SSSR, vol. 21, pp. 75–91, 1983.
[4] M. A. Tsfasman, S. G. Vlaˇdut, and T. Zink, “Modular curves, Shimura curves, and
Goppa codes better than Varshamove-Gilbert bound,” Math. Nachr., vol. 104, pp.
13–28, 1982.
[5] H. Stichtenoth, Algebraic Function Fields and Codes. New York: Springer-Verlag,
1993.
[6] C.-C. Lu, H.-S. Wang, and J.-P. Chen, “Efficient architectures for syndrome
generation and error location search in the decoding of hermitian codes,” IEEE Trans.
Communications, vol. 52, no. 2, pp. 176–179, Feb. 2004.
[7] W. Fulton, Algebraic Curves: An Intorduction to Algebraic Geometry. Rewood City,
CA: Addson-Wesley, 1989.
[8] K. Saints and C. Heegard, “Algebraic-geometric codes and multidimensional cyclic
codes : A unified theory and algorithms for decoding using gro¨bner bases,” IEEE
Trans. Inform. Theory, vol. 41, pp. 1733–1751, Nov. 1995.
164
[19] I. M. Duursma93, “Majority coset decoding,” IEEE Trans. Inform. Theory, vol. 39,
pp. 1067–1070, May 1993.
[20] G. L. Feng, V. K. Wei, T. R. N. Rao, and K. K. Tzeng, “Simplified understanding and
effect decoding of a class of algebraic-geometric codes,” IEEE Trans. Inform. Theory,
vol. 40, pp. 981–1002, July 1994.
[21] S. Sakata, “Finding a minimal set of linear recurring relations capable of generating a
giving finite two-dimensional array,” I. Symbolic Comput., vol. 5, pp. 321–337, May
1988.
[22] J. Justensen, K. J. Larsen, H. E. Jensen, and T. Høholdt, “Fast decoding of codes from
algebraic plane curves,” IEEE Trans. Inform. Theory, vol. 38, pp. 111–119, Jan. 1992.
[23] S. Sakata, J. Justesen, Y. Madelung, H. E. Jensen, and T. Høholdt, “A fast decoding
method of AG codes from Miura-Kamiya curves cab up to half the Feng-Rao bound,”
Finite Fields and Their Applications, vol. 1, pp. 83–101, 1995.
[24] C.-W. Liu and C.-C. Lu, “A view of gaussian elimination applied to early stopped
Berlekamp-Massey algorithm,” IEEE Trans. Communications, vol. 55, no. 6, pp.
1131–1143, June 2007.
[25] M. Kurihara and S. Sakata, “A fast parallel decoding algorithm for general one-point
AG codes with a systolic array architecture,” Proceedings of IEEEE ISIT’95, p. 99.
[26] S. Sakata and M. Kurihara, “A systolic array architecture for implementation a fast
parallel decoding algoritm of one-point AG codes,” Proceedings of ISIT’97, p. 378.
[27] M. E. O’Sullivan, “Vlsi architecture for a decoder for Hermitian codes,” Proceedings of
ISIT’97, p. 376.
[28] C.-W. Liu, K.-T. Huang, and C.-C. Lu, “A systolic array implementation of the
Fang-Rao algorithm,” IEEE Trans. Computers, vol. 48, no. 7, pp. 690–706, July 1999.
166
[40] A. Skorobogatov and S. Vlaˇdut, “On the deocding of algebraic-geometric codes,”
IEEE Trans. Inform. Theory, vol. 36, pp. 1051–1060, Sept. 1990.
[41] D. A. Leonard, “Error-locator ideals for algebraic-goemetric codes,” IEEE Trans.
Inform. Theory, vol. 41, pp. 819–824, May 1995.
[42] S. Sakata, H. E. Jensen, and T. Høholdt, “Generalized Berlekamp-Massey decoding of
algebraic-geometric codes up to half the Feng-Rao bound,” IEEE Trans. Inform.
Theory, vol. 41, pp. 1762–1768, Nov. 1995.
[43] P. Q. B. Hochet and Y. Robert, “Systolic gaussian elimination over gf(p) with partial
pivoting,” IEEE Trans. Computers, vol. 38, no. 9, pp. 1321–1324, Sep. 1989.
[44] S.-Y. Kung, VLSI Array Processors. NJ: Prentice Hall, 1988.
[45] P. S. Tseng, A Systolic Array Parallelizing Compiler. Boston, MA: Kluwer Academic,
1990.
[46] M. F. Atiyah and I. G. Macdonald, Introduction to Commutative Algebra. Reading,
MA: Addison-Wesley, 1969.
[47] H. Matsumura, Commutative Algebra, 2nd ed. Reading, MA: Benjamin/Cummings,
1992.
[48] F. J. MacWilliams and N. J. A. Sloane, The Theory of Error-Correcting codes. New
York: North-Holland, 1978.
[49] M. A. Tsfasman and S. G. Vlaˇdut, Algebraic-Geometric Codes. Amsterdam,
Netherlands: Kluwer Academic, 1991.
[50] J. H. van Lint, Introduction to Coding Theory, 3rd ed. Berlin, Germany:
Springer-Verlag, 1999.
[51] T. W. Hungerford, Algebra. New York: Springer-Verlag, 1974.
168
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            2007 年 7 月 27 日 
報告人姓名  呂忠津 
 
服務機構
及職稱 
 
國立清華大學電機工程系教授 
 
 會議時間 
     地點 
2007 年 6 月 24 日至 2007 年 6 月 29 日 
Nice, France 
會議 
名稱 
 (中文) 二００七年 IEEE 國際訊息理論會議 
 (英文) 2007 IEEE International Symposium on Information Theory 
發表 
論文 
題目 
 (中文) 以廣義克羅內克積建構的碼 
 (英文) On Codes Constructed by Generalized Kronecker Product 
 
表 Y04 
