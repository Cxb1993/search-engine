capacity is B =
∑
v∈V Lv∑
i∈N ci
.
In this study, our proposal is based on the concept of
virtual servers. We present a novel load balancing algo-
rithm that intends to minimize (1) as much as possible. The
intuition behind our proposal is simple in the sense that
our algorithm aligns the distributions of the loads of virtual
servers and the capacities of peers participating in the sys-
tem. Specifically, consider an ordered node set N such that
nodes ni and nj inN satisfy ci ≥ cj if i < j, where i and j
are the indices of the two nodes ni and nj , respectively. Let
an ordered virtual server set V in which vi, vj ∈ V have
their load values satisfying Li ≥ Lj if i < j. Typically,
|V | ≥ |N |. Then, our proposal conceptually reassigns the
virtual servers with the first k-th largest load values in V to
themost capable node (i.e., n1) such that
∣∣∣∑ki=1 Li−c1×B∣∣∣
is minimized. We perform this iteratively by letting N =
N − {n1} and V = V − {v1, v2, · · · , vk}.
We summarize our contributions as follows. First, our
proposed scheme estimates the probability distributions of
the capacities of peers and the loads of virtual servers.
Nodes in parallel push their virtual servers in a way as men-
tioned above to evenly balance their loads. Clearly, our al-
gorithm highly depends on the accuracy of the estimation
for the probability distributions of capacities of peers and
loads of virtual servers. Our proposal guarantees that when
the number, n, of nodes is ≥ 105 (or the number of vir-
tual servers, m, is ≥ 105), and if we sample ln2 n nodes
and ln2 m virtual servers, then the approximation error 1 is
≤ 11.6% with high probability 2. In contrast, if we sample
ln3 n nodes and ln3 m virtual servers, the approximation
error becomes ≤ 1%. Fortunately, our performance study
shows that our proposal performs well using only ln2 n and
ln2 m samples for nodes and virtual servers, respectively.
Second, our design is easy to understand and imple-
ment. Although we discuss our proposal in the context of
Chord [11], our proposal can be easily integrated into most
DHT networks if the DHTs (e.g., Pastry [9]) embed a lin-
ear interconnect geometry (i.e., nodes with the numerically
closest IDs have an overlay connection).
Finally, our design is guided by rigorous performance
analysis. In addition, we investigate the performance of our
design in extensive simulations, and show that our proposal
significantly outperforms the coordinate-based hierarchical
matching algorithm (e.g., the work in [12, 10]) in terms of
the load a node perceives.
1Let p be a parameter to be estimated. Let pˆ be the estimation for
p. The approximation error, ε, for p with high probability is such that
Pr (|pˆ− p| ≤ ε) ≥ 1− O(n−Ω(1)).
2“With high probability” in this paper denotes the probability no less
than 1−O(n−Ω(1)).
2 Related Work
Consider an object set O and the peer set N . As we
discussed in Section 1, typical DHTs assume that the loads
of objects in O are equal (e.g., lo = 1). The load of a peer
can thus be estimated as the number of objects hosted by
the peer. The earlier studies in [11, 3] show that the load
imbalance factor can be O(log n), where n = |N | is the
total number of nodes participating in the system. That is,
the maximum load of a peer can be up-toO(log n) times the
average. Then, several proposals (e.g., [7] and [4]) reduce
the load imbalance factor to a constant. In addition to evenly
partitioning loads among peers, the proposal by Godfrey et
al. exploits the heterogeneity of peers [1].
Instead of evenly partitioning the key subspace into each
peer, Chord [11] suggests the notation of virtual servers.
Different virtual servers in the system manage disjoint key
subspaces, and a virtual server serves as an elementary en-
tity for balancing loads among peers. Particularly, let V
be the set of virtual servers in the system, and each virtual
server v ∈ V has a load value Li. Then, a perfect load-
balanced DHT allocates virtual servers (denoted by V i) to a
peer v such that the resultant load (i.e.,
∑
v∈Vi Lv) of v is
proportional to v’s capacity [11, 8].
Rao et al. propose three general schemes, namely one-
to-one, one-to-many and many-to-many, to cope with the
load imbalance in a DHT [8]. In the one-to-one scheme,
each light node (i.e., the load of the peer is relatively light)
randomly probes a node in the system. If the probed node is
heavy, then a virtual server transfer from the heavy node to
the light node occurs. In contrast to the one-to-one scheme,
the one-to-many scheme introduces the concept of directo-
ries. Some nodes in the system are selected as directory
nodes so that any light node can register its load value with
a picked directory. Any heavy node can then query a picked
directory node, and the directory node may provide a par-
tial set of light nodes to the heavy peer such that a heavy
peer can transfer some of its loads to the light nodes. In the
many-to-many scheme, light and heavy nodes register their
loads with directories. The directories compute matches
between heavy and light nodes, and then respectively re-
quest the heavy and light nodes to transfer and to receive
the loads.
In [12], Zhu et al. suggest to designate some nodes in
a DHT network as directories and to organize these direc-
tories into a tree-shaped overlay. The tree-shaped overlay
helps each node in the system to determine whether a peer
is heavy or not. It is also used to match the heavy and light
nodes. The matches are performed in a bottom-up fashion
towards the root of the tree. Consider an internal node, i, of
the tree. i aligns the heavy and light nodes that cannot be
matched in i’s children nodes. Possibly, i cannot help match
some of the heavy and light nodes. If so, i reports the un-
418
Authorized licensed use limited to: IEEE Xplore. Downloaded on January 22, 2009 at 21:32 from IEEE Xplore.  Restrictions apply.
that the loads among participating peers can be balanced. In
our proposal, the peer i queries a peer j that is the successor
of r via the DHT GET operation, and receives a reply from
j indicating the r-th most capable peer, k. i then pushes
v(r) to k. Each peer i ∈ N performs this for each of its
locally hosted virtual servers. For instance in Fig. 1, peer 3
queries the peer with ID of 98 for allocating its local virtual
server r(96), and then migrates v(96) to peer 10 since peer
10 is the 96-th most capable peer. The peer n− 2 in Fig. 1
performs the similar operations and pushes a virtual server
of the 96-th rank to peer 10.
Theorem 1. Let S be a peer set that includes top-k most
capable nodes, where k = 1, 2, 3, · · · , n. Assume the least
capable peer in S is of the t-th rank. If a DHT implements
our load balancing scheme, then peers in S host the per-
centage,
∑
v∈V Lv·
∑
i∈S ci∑
i∈N ci
, of workloads in the DHT.
We give the details of the proof for Theorem 1 in our
technical report [2]. In summary, the basic load balanc-
ing scheme we mentioned above guarantees that any subset
containing the top-k most capable peers with the total ca-
pacity of c˜ receives loads of virtual servers proportional to
c˜∑
i∈N ci
(see Theorem 1), where k = 1, 2, 3, · · · , n. This
leads to a perfect load-balanced DHT network.
In the following, we detail 1) how our load balancing
scheme estimates the probability distributions of capacities
of peers and loads of virtual servers in Section 3.2. Par-
ticularly, we will present a sampling scheme with a low
overhead. 2) Since each node pushes its local virtual server
based on the two probability distributions, Section 3.3 dis-
cusses how the estimated distributions are disseminated to
nodes in the network.
In this study, we assume that the values of cmax and
Lmax are available to each of participating peers. Each peer
also has the knowledge for the minimal capacity, cmin, of
a peer and the minimal load value, Lmin, of a virtual sever.
We note that these values may be predefined before operat-
ing the system. Moreover, we assume that our system can
rely on a third-party service (e.g., the proposal in [ 5]) to
sample participating nodes in the network uniformly at ran-
dom and to approximate the system size (i.e., n = |N |).
However, if we define the DHT ring ID space of the range
[0, 1], then our proposal does not need to know the value of
n in advance. Similarly, we also assume that the number of
virtual servers (i.e.,m = |V |) can be known in advance.
3.2 Sampling
3.2.1 Approximation of Probability Distribution for
Capacities of Peers
Our design randomly picks a node s ∈ N to samples ln2 n
nodes in the network. Based on the samples, s then approxi-
mates the probability distribution of capacities of peers. We
first state the procedures performed by s as follows to esti-
mate the distribution, and then conclude the quality of our
sampling procedures in Theorem 2.
For approximating the probability distribution of capac-
ities of nodes in N , our idea is to have a randomly picked
node s sample ln2 n nodes, denoted by the peer set S, inde-
pendently and uniformly at random in the network. s then
specifies a “range segment”, [cl, cu], within the capacity val-
ues of peers in S such that the probability of a node having
the capacity value in [cl, cu] is not less than 13 lnn . s removes
nodes in S having capacities in [cl, cu], and it iterates this
process until S becomes empty. The resultant disjoint range
segments [cl, cu]’s, each defining the probability (i.e., 13 lnn )
for the event that a peer ni ∈ N is with the capacity value
cni ∈ [cl, cu], together approximate the probability distri-
bution for the capacities of nodes in N .
Prob. Dist.
Approximation

Figure 2. An illustration of our sampling algorithm
Fig. 2 illustrates our sampling scheme. Our sampling
algorithm finds a capacity range [cl, cu] such that the proba-
bility of a node having ci ∈ [cl, cu] is at least 13 lnn , starting
from cl = cmin to cu = cmax.
Assume that the resultant range segments our al-
gorithm determines are
⋃k
i=1
{[
c
(i)
l , c
(i)
u
]}
such that∑k
i=1 Pr
(c)
(
c
(i)
l ≤ X ≤ c(i)u
)
= 1, where k = 3 lnn,
c
(1)
l = cmin, c
(i)
l = c
(i−1)
u and c(k)u = cmax. Consider a
peer κ with the capacity value of cκ, and cκ ∈ [c(j)l , c(j)u ].
In our implementation, we estimate the capacity rank for κ
as⎛
⎜⎜⎜⎜⎝
∑k
i=j+1
(
nPr(c)
(
c
(i)
l ≤ X ≤ c
(i)
u
)
cˆ
(i)
1
)
+
ncˆ
(j)
2
(
c
(j)
u −cκ
)
(3 lnn)
(
c
(j)
u −c(j)l
)
∑k
h=1
(
nPr(c)
(
c
(h)
l ≤ X ≤ c
(h)
u
)
c
(h)
l
+c
(h)
u
2
)
⎞
⎟⎟⎟⎟⎠·n,
(4)
where cˆ(i)1 =
1
2
(
c
(i)
l + c
(i)
u
)
and cˆ(j)2 =
1
2
(
c
(j)
l + c
(j)
u
)
.
Note that (4) approximates (2) given in Section 3.1. Our
sampling scheme guarantees the result as follows.
420
Authorized licensed use limited to: IEEE Xplore. Downloaded on January 22, 2009 at 21:32 from IEEE Xplore.  Restrictions apply.
4 Simulations
4.1 Experimental Setting
We have implemented an event-driven simulator to as-
sess the performance of our proposal. Our simulator is
based on the Chord protocol [11]. Our simulator also im-
plements the coordinate-based hierarchical matching algo-
rithm (CHMA), discussed in our technical report [2], to
mimic the proposal presented by Zhu et al. in [12].
In our simulations, the capacity of a simulated peer
has the power-law distribution, i.e., the Pareto distribution
Pr(X > x) =
(
x
xm
)−β
with the shape parameter of
β = 1.5 and the location parameter of xm = 1. In our
study, we simulate 10, 000 Chord peers.
In the simulation study, the load value of a simulated
virtual server also has the Pareto distribution Pr(Y > y) =(
y
ym
)−α
. The default shape parameter α is 1.5, and the
default location parameter, ym, is 1. In the simulations, we
vary the values of α from 1.5 to 12. The number of virtual
servers created over our simulated Chord network is from
10, 000 to 100, 000, and 100, 000 is the default.
In our simulations, each simulated peer has a lifetime
with a mean of 150 minutes. The lifetime follows the expo-
nential distribution.
The major performance metrics we measure is the Load
imbalance factor. LetN be the set of nodes participating in
the system, Lv be the load value of a virtual server v, and
ci be the capacity of a peer i ∈ N . The load imbalance
factor is defined as
∣∣∣∑v∈Vi Lv−ci×B
∣∣∣
ci×B , where Vi represents
the set of virtual servers allocated to the peer i ∈ N , V =⋃
i∈N Vi, and the expected load value per unit capacity is
B =
∑
v∈V Lv∑
i∈N ci
. Clearly, a perfect load balancing algorithm
is to allocate virtual servers to a peer i such that the load
imbalance factor of i is 0.
We note that in default the number of samples that our
proposal opts for the estimation of capacities of peers and
load values of virtual servers are respectively ln2 n and
ln2 m, where n = |N | andm = |V |.
4.2 Comparing CHMA and Our Proposal
Fig. 3 depicts the simulation results regarding the load
imbalance factor for CHMA (Fig. 3(a)) and our proposal
(Fig. 3(b)). Given the Pareto distributions for the capacities
of peers and loads of virtual servers, we vary the shape pa-
rameter (i.e., α) for the probability distribution of the loads
of virtual servers. The coordinate, (x, y), in Fig. 3 denotes
the percentage x of nodes having the “averaged” load im-
balance factor of y.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
alpha=1.5
alpha=3
alpha=4.5
alpha=6
alpha=12
(a) CHMA
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
alpha=1.5
alpha=3
alpha=4.5
alpha=6
alpha=12
(b) Our proposal
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
10000
20000
50000
100000
(c) CHMA
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
10000
20000
50000
100000
(d) Our proposal
Figure 3. The load imbalance factor for (a) the power-
law distributions of capacities of peers and loads of virtual
servers, and (b) the uniform distributions of capacities of
peers and loads of virtual servers (note that the scale of y-
axis in (a) and (b) is different from that in (c) and (d))
We first observe that our proposal performs better than
CHMA (see Fig. 3(a) and (b)). In most configurations, the
load imbalance factor introduced by our proposal is nomore
than 0.01. This indicates that all peers in our system ap-
proximate their ideal load values of≥ 99% and of≤ 101%.
Specially, in contrast to CHMA, our proposal is less sensi-
tive to the probability distribution of loads of virtual servers.
The experimental results depicted in Fig. 3(a) and (b)
illustrate that when α becomes larger (i.e., the number of
lightly loaded virtual servers increases), CHMA tends to
perform worse in terms of the load imbalance factor. We
thus investigate the uniform distribution of loads of virtual
servers. In particular, the capacities of peers also follow the
uniform distribution. Fig. 3(c) and (d) provide the simu-
lation results. As we can see, CHMA introduces nodes of
10% having the load imbalance factor of up-to 0.16 (not
shown in Fig. 3(c)). CHMA performs better when the num-
ber of virtual servers increases (in this experiment we also
vary the number of virtual servers from 10, 000 to 100, 000).
On the contrary, our proposal performs very well. The per-
formance of our proposal is clearly irrelevant to the prob-
ability distribution for the loads of virtual servers. Addi-
tionally, our proposal’s performance is not sensitive to the
number of virtual servers.
422
Authorized licensed use limited to: IEEE Xplore. Downloaded on January 22, 2009 at 21:32 from IEEE Xplore.  Restrictions apply.
出席國際學術會議心得報告 
                                                             
計畫編號 97-2221-E-006-132 
計畫名稱 低成本且負載平衡的同儕網路之設計與實作 
出國人員姓名 
服務機關及職稱 
蕭宏章  
國立成功大學資訊工程學系 
會議時間地點 97 年 12 月 17 日至 12 月 20 日, 中國上海 
會議名稱 The 2008 International Conference On Embedded and Ubiquitous Computing(EUC 2008) 
發表論文題目 Load Balancing in Peer-to-Peer Networks Based on Sampling System States 
 
 
 
 
一、參加會議經過 
 
EUC 2008 國際會議於 12 月 17 日至 20 日在中國上海舉辦，此一會議探討的主題之一為
普及計算。本人於此會議中發表論文的題目為「Load Balancing in Peer-to-Peer Networks Based 
on Sampling System States」，會議期間的 12 月 19 日上午進行口頭報告。另外，本人也擔任會
議其中一個 session 的 chair。此次會議中本人與多位學者或專家做經驗交流，實為一難得之機
會。本人除了主持 session 之外，也參與了兩場 Keynote Speeches。 
 
二、與會心得 
 
此次會議議題相當廣泛，大家參與會議時討論亦非常熱烈。此次會議中華民國參加之人
員多為台灣大學及交通資訊工程系的學者，而成功大學僅有本人參與。 
筆者非常感謝國科會的補助，才能夠參加此次國際性會議，藉由此次之參與將對未來之
研究有莫大之助益，因此像這種傳統、有代表性國際會議應多鼓勵國內學者多參與，以提昇
國際地位，並提高國內之研究水準。 
 
 
capacity is B =
∑
v∈V Lv∑
i∈N ci
.
In this study, our proposal is based on the concept of
virtual servers. We present a novel load balancing algo-
rithm that intends to minimize (1) as much as possible. The
intuition behind our proposal is simple in the sense that
our algorithm aligns the distributions of the loads of virtual
servers and the capacities of peers participating in the sys-
tem. Specifically, consider an ordered node set N such that
nodes ni and nj inN satisfy ci ≥ cj if i < j, where i and j
are the indices of the two nodes ni and nj , respectively. Let
an ordered virtual server set V in which vi, vj ∈ V have
their load values satisfying Li ≥ Lj if i < j. Typically,
|V | ≥ |N |. Then, our proposal conceptually reassigns the
virtual servers with the first k-th largest load values in V to
themost capable node (i.e., n1) such that
∣∣∣∑ki=1 Li−c1×B∣∣∣
is minimized. We perform this iteratively by letting N =
N − {n1} and V = V − {v1, v2, · · · , vk}.
We summarize our contributions as follows. First, our
proposed scheme estimates the probability distributions of
the capacities of peers and the loads of virtual servers.
Nodes in parallel push their virtual servers in a way as men-
tioned above to evenly balance their loads. Clearly, our al-
gorithm highly depends on the accuracy of the estimation
for the probability distributions of capacities of peers and
loads of virtual servers. Our proposal guarantees that when
the number, n, of nodes is ≥ 105 (or the number of vir-
tual servers, m, is ≥ 105), and if we sample ln2 n nodes
and ln2 m virtual servers, then the approximation error 1 is
≤ 11.6% with high probability 2. In contrast, if we sample
ln3 n nodes and ln3 m virtual servers, the approximation
error becomes ≤ 1%. Fortunately, our performance study
shows that our proposal performs well using only ln2 n and
ln2 m samples for nodes and virtual servers, respectively.
Second, our design is easy to understand and imple-
ment. Although we discuss our proposal in the context of
Chord [11], our proposal can be easily integrated into most
DHT networks if the DHTs (e.g., Pastry [9]) embed a lin-
ear interconnect geometry (i.e., nodes with the numerically
closest IDs have an overlay connection).
Finally, our design is guided by rigorous performance
analysis. In addition, we investigate the performance of our
design in extensive simulations, and show that our proposal
significantly outperforms the coordinate-based hierarchical
matching algorithm (e.g., the work in [12, 10]) in terms of
the load a node perceives.
1Let p be a parameter to be estimated. Let pˆ be the estimation for
p. The approximation error, ε, for p with high probability is such that
Pr (|pˆ− p| ≤ ε) ≥ 1− O(n−Ω(1)).
2“With high probability” in this paper denotes the probability no less
than 1−O(n−Ω(1)).
2 Related Work
Consider an object set O and the peer set N . As we
discussed in Section 1, typical DHTs assume that the loads
of objects in O are equal (e.g., lo = 1). The load of a peer
can thus be estimated as the number of objects hosted by
the peer. The earlier studies in [11, 3] show that the load
imbalance factor can be O(log n), where n = |N | is the
total number of nodes participating in the system. That is,
the maximum load of a peer can be up-toO(log n) times the
average. Then, several proposals (e.g., [7] and [4]) reduce
the load imbalance factor to a constant. In addition to evenly
partitioning loads among peers, the proposal by Godfrey et
al. exploits the heterogeneity of peers [1].
Instead of evenly partitioning the key subspace into each
peer, Chord [11] suggests the notation of virtual servers.
Different virtual servers in the system manage disjoint key
subspaces, and a virtual server serves as an elementary en-
tity for balancing loads among peers. Particularly, let V
be the set of virtual servers in the system, and each virtual
server v ∈ V has a load value Li. Then, a perfect load-
balanced DHT allocates virtual servers (denoted by V i) to a
peer v such that the resultant load (i.e.,
∑
v∈Vi Lv) of v is
proportional to v’s capacity [11, 8].
Rao et al. propose three general schemes, namely one-
to-one, one-to-many and many-to-many, to cope with the
load imbalance in a DHT [8]. In the one-to-one scheme,
each light node (i.e., the load of the peer is relatively light)
randomly probes a node in the system. If the probed node is
heavy, then a virtual server transfer from the heavy node to
the light node occurs. In contrast to the one-to-one scheme,
the one-to-many scheme introduces the concept of directo-
ries. Some nodes in the system are selected as directory
nodes so that any light node can register its load value with
a picked directory. Any heavy node can then query a picked
directory node, and the directory node may provide a par-
tial set of light nodes to the heavy peer such that a heavy
peer can transfer some of its loads to the light nodes. In the
many-to-many scheme, light and heavy nodes register their
loads with directories. The directories compute matches
between heavy and light nodes, and then respectively re-
quest the heavy and light nodes to transfer and to receive
the loads.
In [12], Zhu et al. suggest to designate some nodes in
a DHT network as directories and to organize these direc-
tories into a tree-shaped overlay. The tree-shaped overlay
helps each node in the system to determine whether a peer
is heavy or not. It is also used to match the heavy and light
nodes. The matches are performed in a bottom-up fashion
towards the root of the tree. Consider an internal node, i, of
the tree. i aligns the heavy and light nodes that cannot be
matched in i’s children nodes. Possibly, i cannot help match
some of the heavy and light nodes. If so, i reports the un-
418
Authorized licensed use limited to: IEEE Xplore. Downloaded on January 22, 2009 at 21:32 from IEEE Xplore.  Restrictions apply.
that the loads among participating peers can be balanced. In
our proposal, the peer i queries a peer j that is the successor
of r via the DHT GET operation, and receives a reply from
j indicating the r-th most capable peer, k. i then pushes
v(r) to k. Each peer i ∈ N performs this for each of its
locally hosted virtual servers. For instance in Fig. 1, peer 3
queries the peer with ID of 98 for allocating its local virtual
server r(96), and then migrates v(96) to peer 10 since peer
10 is the 96-th most capable peer. The peer n− 2 in Fig. 1
performs the similar operations and pushes a virtual server
of the 96-th rank to peer 10.
Theorem 1. Let S be a peer set that includes top-k most
capable nodes, where k = 1, 2, 3, · · · , n. Assume the least
capable peer in S is of the t-th rank. If a DHT implements
our load balancing scheme, then peers in S host the per-
centage,
∑
v∈V Lv·
∑
i∈S ci∑
i∈N ci
, of workloads in the DHT.
We give the details of the proof for Theorem 1 in our
technical report [2]. In summary, the basic load balanc-
ing scheme we mentioned above guarantees that any subset
containing the top-k most capable peers with the total ca-
pacity of c˜ receives loads of virtual servers proportional to
c˜∑
i∈N ci
(see Theorem 1), where k = 1, 2, 3, · · · , n. This
leads to a perfect load-balanced DHT network.
In the following, we detail 1) how our load balancing
scheme estimates the probability distributions of capacities
of peers and loads of virtual servers in Section 3.2. Par-
ticularly, we will present a sampling scheme with a low
overhead. 2) Since each node pushes its local virtual server
based on the two probability distributions, Section 3.3 dis-
cusses how the estimated distributions are disseminated to
nodes in the network.
In this study, we assume that the values of cmax and
Lmax are available to each of participating peers. Each peer
also has the knowledge for the minimal capacity, cmin, of
a peer and the minimal load value, Lmin, of a virtual sever.
We note that these values may be predefined before operat-
ing the system. Moreover, we assume that our system can
rely on a third-party service (e.g., the proposal in [ 5]) to
sample participating nodes in the network uniformly at ran-
dom and to approximate the system size (i.e., n = |N |).
However, if we define the DHT ring ID space of the range
[0, 1], then our proposal does not need to know the value of
n in advance. Similarly, we also assume that the number of
virtual servers (i.e.,m = |V |) can be known in advance.
3.2 Sampling
3.2.1 Approximation of Probability Distribution for
Capacities of Peers
Our design randomly picks a node s ∈ N to samples ln2 n
nodes in the network. Based on the samples, s then approxi-
mates the probability distribution of capacities of peers. We
first state the procedures performed by s as follows to esti-
mate the distribution, and then conclude the quality of our
sampling procedures in Theorem 2.
For approximating the probability distribution of capac-
ities of nodes in N , our idea is to have a randomly picked
node s sample ln2 n nodes, denoted by the peer set S, inde-
pendently and uniformly at random in the network. s then
specifies a “range segment”, [cl, cu], within the capacity val-
ues of peers in S such that the probability of a node having
the capacity value in [cl, cu] is not less than 13 lnn . s removes
nodes in S having capacities in [cl, cu], and it iterates this
process until S becomes empty. The resultant disjoint range
segments [cl, cu]’s, each defining the probability (i.e., 13 lnn )
for the event that a peer ni ∈ N is with the capacity value
cni ∈ [cl, cu], together approximate the probability distri-
bution for the capacities of nodes in N .
Prob. Dist.
Approximation

Figure 2. An illustration of our sampling algorithm
Fig. 2 illustrates our sampling scheme. Our sampling
algorithm finds a capacity range [cl, cu] such that the proba-
bility of a node having ci ∈ [cl, cu] is at least 13 lnn , starting
from cl = cmin to cu = cmax.
Assume that the resultant range segments our al-
gorithm determines are
⋃k
i=1
{[
c
(i)
l , c
(i)
u
]}
such that∑k
i=1 Pr
(c)
(
c
(i)
l ≤ X ≤ c(i)u
)
= 1, where k = 3 lnn,
c
(1)
l = cmin, c
(i)
l = c
(i−1)
u and c(k)u = cmax. Consider a
peer κ with the capacity value of cκ, and cκ ∈ [c(j)l , c(j)u ].
In our implementation, we estimate the capacity rank for κ
as⎛
⎜⎜⎜⎜⎝
∑k
i=j+1
(
nPr(c)
(
c
(i)
l ≤ X ≤ c
(i)
u
)
cˆ
(i)
1
)
+
ncˆ
(j)
2
(
c
(j)
u −cκ
)
(3 lnn)
(
c
(j)
u −c(j)l
)
∑k
h=1
(
nPr(c)
(
c
(h)
l ≤ X ≤ c
(h)
u
)
c
(h)
l
+c
(h)
u
2
)
⎞
⎟⎟⎟⎟⎠·n,
(4)
where cˆ(i)1 =
1
2
(
c
(i)
l + c
(i)
u
)
and cˆ(j)2 =
1
2
(
c
(j)
l + c
(j)
u
)
.
Note that (4) approximates (2) given in Section 3.1. Our
sampling scheme guarantees the result as follows.
420
Authorized licensed use limited to: IEEE Xplore. Downloaded on January 22, 2009 at 21:32 from IEEE Xplore.  Restrictions apply.
4 Simulations
4.1 Experimental Setting
We have implemented an event-driven simulator to as-
sess the performance of our proposal. Our simulator is
based on the Chord protocol [11]. Our simulator also im-
plements the coordinate-based hierarchical matching algo-
rithm (CHMA), discussed in our technical report [2], to
mimic the proposal presented by Zhu et al. in [12].
In our simulations, the capacity of a simulated peer
has the power-law distribution, i.e., the Pareto distribution
Pr(X > x) =
(
x
xm
)−β
with the shape parameter of
β = 1.5 and the location parameter of xm = 1. In our
study, we simulate 10, 000 Chord peers.
In the simulation study, the load value of a simulated
virtual server also has the Pareto distribution Pr(Y > y) =(
y
ym
)−α
. The default shape parameter α is 1.5, and the
default location parameter, ym, is 1. In the simulations, we
vary the values of α from 1.5 to 12. The number of virtual
servers created over our simulated Chord network is from
10, 000 to 100, 000, and 100, 000 is the default.
In our simulations, each simulated peer has a lifetime
with a mean of 150 minutes. The lifetime follows the expo-
nential distribution.
The major performance metrics we measure is the Load
imbalance factor. LetN be the set of nodes participating in
the system, Lv be the load value of a virtual server v, and
ci be the capacity of a peer i ∈ N . The load imbalance
factor is defined as
∣∣∣∑v∈Vi Lv−ci×B
∣∣∣
ci×B , where Vi represents
the set of virtual servers allocated to the peer i ∈ N , V =⋃
i∈N Vi, and the expected load value per unit capacity is
B =
∑
v∈V Lv∑
i∈N ci
. Clearly, a perfect load balancing algorithm
is to allocate virtual servers to a peer i such that the load
imbalance factor of i is 0.
We note that in default the number of samples that our
proposal opts for the estimation of capacities of peers and
load values of virtual servers are respectively ln2 n and
ln2 m, where n = |N | andm = |V |.
4.2 Comparing CHMA and Our Proposal
Fig. 3 depicts the simulation results regarding the load
imbalance factor for CHMA (Fig. 3(a)) and our proposal
(Fig. 3(b)). Given the Pareto distributions for the capacities
of peers and loads of virtual servers, we vary the shape pa-
rameter (i.e., α) for the probability distribution of the loads
of virtual servers. The coordinate, (x, y), in Fig. 3 denotes
the percentage x of nodes having the “averaged” load im-
balance factor of y.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
alpha=1.5
alpha=3
alpha=4.5
alpha=6
alpha=12
(a) CHMA
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
alpha=1.5
alpha=3
alpha=4.5
alpha=6
alpha=12
(b) Our proposal
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
10000
20000
50000
100000
(c) CHMA
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Lo
ad
 Im
ba
la
nc
e 
Fa
ct
or
Percentage of Nodes
10000
20000
50000
100000
(d) Our proposal
Figure 3. The load imbalance factor for (a) the power-
law distributions of capacities of peers and loads of virtual
servers, and (b) the uniform distributions of capacities of
peers and loads of virtual servers (note that the scale of y-
axis in (a) and (b) is different from that in (c) and (d))
We first observe that our proposal performs better than
CHMA (see Fig. 3(a) and (b)). In most configurations, the
load imbalance factor introduced by our proposal is nomore
than 0.01. This indicates that all peers in our system ap-
proximate their ideal load values of≥ 99% and of≤ 101%.
Specially, in contrast to CHMA, our proposal is less sensi-
tive to the probability distribution of loads of virtual servers.
The experimental results depicted in Fig. 3(a) and (b)
illustrate that when α becomes larger (i.e., the number of
lightly loaded virtual servers increases), CHMA tends to
perform worse in terms of the load imbalance factor. We
thus investigate the uniform distribution of loads of virtual
servers. In particular, the capacities of peers also follow the
uniform distribution. Fig. 3(c) and (d) provide the simu-
lation results. As we can see, CHMA introduces nodes of
10% having the load imbalance factor of up-to 0.16 (not
shown in Fig. 3(c)). CHMA performs better when the num-
ber of virtual servers increases (in this experiment we also
vary the number of virtual servers from 10, 000 to 100, 000).
On the contrary, our proposal performs very well. The per-
formance of our proposal is clearly irrelevant to the prob-
ability distribution for the loads of virtual servers. Addi-
tionally, our proposal’s performance is not sensitive to the
number of virtual servers.
422
Authorized licensed use limited to: IEEE Xplore. Downloaded on January 22, 2009 at 21:32 from IEEE Xplore.  Restrictions apply.
