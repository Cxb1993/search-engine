計劃名稱：一個植基於分析人類行為的居家照護系統  
計畫編號：NSC  94(95,96)-2213-E-001-018 
計畫期間：94 年 8 月 1 日至 97 年 7 月 31 日 
主持人  ：廖弘源 中央研究院資訊所研究員 
 
 
中文摘要 
  
在第一年的研究成果中，我們提出一個能
自動分割並辨識人類連續運動的新機制。
吾人提出利用星狀圖來有效描述人類輪廓
的變動情形。由此圖的描述方式，人類的
連續運動行為可由一連串的參數來表示，
即所謂動作模組化。為了使此種模組化過
程能十分精簡，同時又能描述跨越時間空
間的特徵，吾人使用高斯混合模式來達到
此目的。另外，為了快速捕捉在時間軸上
的行為變化，吾人將時間軸上的星狀圖利
用 DCT 轉換到頻率軸，並使用幾個係數來
最有效表達原始輪廓資料。實驗顯示吾人
所提出的方法可以有效辨識人類連續的動
作。 
 
在第二年的研究成果中，本研究提出一個
新的機制其利用跨越時空的特徵在即時視
訊監控系統中，能夠偵測移動物體同時給
予辨識其類別。此機制中，利用計算連續
畫面之差異以及兩個成對畫面差之相似性
快速有效地偵測移動物體。並利用所提出
之新的機器學習方法來辨識五種移動物體:
行人、腳踏車、機車、汽車以及撐傘的行
人等。提出的學習機制能夠有效地利用相
鄰畫面之資訊提供具有強健性之辨識效
果，其準確度可達 96%。 
 
第三年，著眼於視訊資料中，偵測人類視
覺聚焦區域並且能夠計算出適當的區域範
圍是相當具有挑戰性的研究議題。本研究
提出新穎的方法，其以跨越時空之視訊分
析為基礎，建立動態視覺聚焦模型。在我
們提出的方法中，首先在三度空間中偵測
重要的點，並以此為起點，在所建立的動
態視覺地圖中向外擴張。為尋找適當的範
圍，以滳為基礎之法則作為區域擴張之依
據。實驗結果顯示，所提出的方法能夠有
效地計算連續畫面中之視覺聚焦區域。 
 
 
英文摘要： 
 
 In the first year of this project, a 
framework of automatic human action 
segmentation and recognition in continuous 
action sequences is proposed. A star-like 
figure is proposed to effectively represent the 
extremities in the silhouette of human body. 
The human action, thus, is recorded as a 
sequence of the star-like figure parameters, 
which is used for action modeling. To model 
human actions in a compact manner while 
characterizing their spatio-temporal 
distributions, star-like figure parameters are 
represented by Gaussian mixture models 
(GMM). In addition, to address the intrinsic 
nature of temporal variations in a continuous 
action sequence, we transform the time 
sequence of star-like figured parameters into 
frequency domain by discrete cosine 
transform (DCT) and use only the first few 
coefficients to represent different temporal 
 II
1. 簡介 (第一年研究成果) 
人類的行為分析是一個非常困難
的問題，尤其是當吾人想利用視訊資
料來探討此議題時難度更高。一段完
整的人類行為可能包括一連串的動
作；包括伸手、開窗子、走進房間、
坐下然後再站起來，繼而走開。為了
分析這些連續的行為模式，吾人必須
加以描述。但是，描述一長段連續動
作是非常困難的。吾人最直覺能想到
的方法是針對此長段行為做片段分
割，然後再針對切割後的片段加以完
整描述。如此一來，藉著對整段連續
動作加以適度分割，就如同將一較大
型且複雜的問題加以分段，使其簡
化；另外，加上對各小段用有效率的
方法加以詳細描述，日後即可進行分
段比對的工作。本研究的目的即在如
何有效且正確的切割一長段人類行
為，並設計完整且正確的描述方法。
人類運動行為分析長久以來一直都是
電腦視覺領域內一個非常重要的課
題。由視訊中分析人類運動模式有兩
個最大的困難處。首先，因為人類具
有許多的關節，因此在分析其運動模
式時具有高度之複雜性。其次，處理
人類的運動必須有效的將運動加以描
述。通常，此種描述的過程必須跨越
時間與空間，在文獻上雖然有許多報
告，但找到一個適切且有效的方法仍
然是各方努力的重點。本研究的目的
是找到一個能系統化描述人類運動模
式並加以辨識的方法。 
現存運動模式辨識方法可分為兩
種（如果以所使用的模型做標準），一
種為二維而另一種為三維的方法[1]。
在[2]中，Haritaoglu 等人提出一個 W4
系統，此系統計算人類輪廓在水平及
鉛直軸的投影，並以此為基礎來決定
標的物是站立、坐姿、彎腰，或者躺
姿。另外，Bobick 及 Davis[3]提出一
個以能描述時間軸變化的堆疊描述
法。他們利用運動的能量及運動的歷
史來描述並辨識不同的運動方式。在
[6-9]的四個研究工作中，研究者利用
受測者的輪廓來執行人類行為分析。
除了上述二維模式為主的辨識方式，
利用三維模型執行人類行為分析及辨
識也是可能的。在[4]中，Boulay 等人
利用 PCA 學習人類的二維姿態。之
後，他們再利用三維方法去重建三維
模式。另外，Zhao 等人[5]則利用三維
模型去判斷所偵測到的物體是否為
人。但是，通常三維模型耗費較多的
時間，因此，吾人大都利用二維模型
執行人類行為辨識工作。唯有如此，
系統的耗費才會合算。 
在本研究中，我們提出一個利用
時空描述人類運動的機率模式。吾人
利用星狀圖來代表人類輪廓中的極端
點，達到唯一描述輪廓的目的。在這
種狀況下，一連串的人類輪廓變化過
程（即運動過程）可以被視為一連串
的參數變化過程。這些連串的參數可
以用高斯混合模式來代表(Gaussian 
Mixture Model)。有了這些混合模式一
連串的不同動作可利用參數的變化是
否平滑來找出基本動作的邊界。找出
邊界後，吾人利用離散餘弦轉換並取
前幾項係數作為代表該運動片段的參
數。如此，辨識過程的效率即可大大
的提升。 
2. 前處理 
因為我們所要建構的是一個即時
的行為偵測系統，因此，建立背景模
型是必須的。唯有如此，吾人才有即
 3
Fig.2. Human actions can be discriminated by using 
spatio-temporal distribution of star-stick orientation 
and length modeled by a two-dimensional GMM 
(K=5). A walking sequence and a raising-hands 
sequence are demonstrated in (a) and (b), respectively. 
4. 運動邊界的切割 
由前段吾人已知一連串的運動框
架都是由高斯混合模式所表示。為了
有效分割運動的邊界，吾人使用
KL-divergence 作為距離量測工具來
偵測較可靠的滑動窗戶(sliding 
window)的邊界。基本上，
KL-divergence 是一個可用來量測兩
個高斯分佈的最自然工具。這部分我
們使用蒙地卡羅模擬來實現。因為使
用蒙地卡羅模擬計算需花費大量計算
時間，因此吾人使用逼近
KL-divergence 的方式來加快計算速
度。 
5. 運動片段的辨識 
雖然能跨越時間及空間的星狀圖
表示法可用高斯混合模式來表達，但
是時間的順序上卻不易由此表法掌
握。例如：用高斯混合模式就無法適
切表示並分別出舉起手及放下手的兩
個連續動作。對於此問題，吾人以一
權重的方式加以解決。 
6. 實驗結果 
吾人使用的實驗資料是包含許多
人類運動片段的視訊。此段視訊共涵
蓋 1651 個框架。其中所包含的運動有
舉手、彎腰、轉身、走路，及體操動
作等等。這些運動的真正分界點是用
肉眼觀察獲得。圖 3 所示為用人為方
式偵測得到的運動類別。圖 4(a)所示
為用吾人所提方法所偵測到的運動分
界點。圖 4(b)則是用人為方式所偵測
的分界點。由圖 4 觀察，吾人所提出
的演算法則可偵測到大多數正確邊界
點。由本實驗結果計算，吾人的正確
偵測率達到 90%以上。圖 5 所示的連
續動作為吾人所偵測到的打拳動作。
在動作辨識方面，吾人使用與
modeling 相同的方式計算未知動作的
特徵值，並以此組特徵值與資料庫中
儲存的視訊片段特徵值加以比對。吾
人使用的視訊動作資料係由六個人所
表演產生。實驗資料分為測試及訓練
資料。吾人訓練的動作資料有七種，
分別為：行走、舉手、彎腰、蹲下、
起身、躺下，及坐下等。在測試資料
庫中，共有 126 種動作（可重覆），每
一個動作所包含的框架數為 20 到 150
張之間。表 1 所列為吾人所提出的方
法在實驗時獲致的結果。從表 1 的結
果，吾人可看出「躺下」這個動作得
到 100%的辨識結果。獲致這個結果的
主要原因乃因「躺下」這個動作與其
他六個動作均有極大的差別。至於其
他動作的辨識率，例如：行走、舉手、
彎腰、坐下等等，辨識成功率都高於
80％。至於辨識率最低的「起身」（73
％）則由於其動作與「舉手」非常類
似，因此辨識相對較差。 
 
Fig.3. Results of action boundary detection. 
 5
[6] H. Fujiyoshi and A. Lipton, “Real-Time Human Motion 
Analysis by Image Skeletonization,” in Proc. IEEE 
Workshop on Application Computer Vision, pp.15-21, 
1998. 
[7] I. C. Chang and C. L. Huang, “The Model-based Human 
Body Motion Analysis System,” Image Vision 
Computation, Vol.18, no.14, pp.1067-1083, 2000. 
[8] R. Cucchiara, c. Grana, A. Prati, and R. Vezzani, 
“Probabilistic Posture Classification for Huamn-Bahavior 
Analysis,” IEEE Trans. on Systems, Man, and 
Cybernetics-Part A: Systems and Humans, Vol.35, no.1, 
pp.42-54, Jan. 2005.
[9] A. Ali and J. K. Aggarwal, “Segmentation and 
recognition of continuous human activity,” Proc. IEEE 
Workshop on Detection and Recognition of Events in 
Video, pp. 28-35, Vancouver, Canada, July, 8 2001. 
[10] Y. Rui and P. Anandan, “Segmenting Visual Actions 
Based on Spatio-Temporal Motion Patterns,” Proc. CVPR 
2000.  
[11] J. Goldberger, H. Aronowitz, “A Distance Measure 
Between GMMs based on Unscented Transform and tis 
Applications to Speaker Recognition,” Proc. Eurospeech, 
2005. 
[12] D.A. Reynolds, "Speaker Identification and Verification 
Using Gaussian Mixture Models", Speech Communication, 
vol. 17, pp. 91-108, Elsevier, 1995. 
[13] D. Reynold and R.C. Rose, “Robust text independent 
speaker identification using gaussian mixture speaker 
models,” IEEE Tran. Speech and Audio Processing, 
3:72–83, Jan. 1995. 
[14] R. O. Duda and P. E. Hart. Pattern Classification and 
Scene Analysis. John Wiley and Sons Inc., 1973. 
[15] P. Correa, J. Czyz, T. Umeda, F. Marques, X. Marichal, 
B. Macq, “Silhouette-based Probabilistic 2D Human 
Motion Estimation for Real-Time Applications,” IEEE 
ICIP 2005. 
[16] N. P. Cuntoor and R. Chellappa, “Key Frame-Based 
Activity Representation Using Antieigenvalues,” pp. 
409-598, ACCV 2006.  
[17] C. Stauffer and W.E.L. Grimson, Adaptive background 
mixture models for real-time tracking, Proc. IEEE 
International Conference on Computer Vision and Pattern 
Recognition, June 1999, vol. 19, pp. 23-25. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 7
Input:  
(1) sequence of N labeled examples {(x1,y1), 
(x2,y2),…,(xN,yN)} 
(2) distribution D over the N examples 
(3) weak learning algorithm WeakLearn 
(4) integer T specifying number of iterations 
Initialization: the weight vector:  
Do for t=1, 2, …, T 
1. Set      ∑= =Ni titt wp 1ω    
2. Call WeakLearn, providing it with 
the distribution pt; get back a 
hypothesis ht:X     [0,1]. →
3. Calculate the error of ht:  
∑ ∑
= −=
+ ⎟⎟⎠
⎞
⎜⎜⎝
⎛ −=
N
i
i
s
sf
fits
t
it yxhroundp
1
2
1 )(ε . 
4. Set  ( ) .  ttt εεβ −= 1
5. Set the new weights vector to be   
( ) iit yxh
t
t
i
t
i
−−+ = 11 βωω    
 Output: the hypothesis 
      
( ) ( ) ( )
otherwise
loglog if   
0
1
1 1
1
2
11
t∑ ∑= =≥
⎩⎨
⎧=
T
t
T
tt
f t
xhxh ββ . 
 
4. 實驗結果 
在實驗當中，每個類別以 200 個樣本
為訓練之依據，訓練分類器的過程
中，以一類之 200 個樣本為正取樣，
另四類共 800 個樣本為負取樣。測試
視訊來自於即時監控系統，此系統監
控時間為早上至下午，並包含不同之
天候狀況如晴天、陰天及雨天。 
實驗數據如表一及表二所示。表一為
對照用數據，其來自於 AdaBoost.M1
訓練器，表二為利用所提出之分類器
所獲得之數據。由兩個表格可知，所
提出之分類器確實可改善僅利用空間
資之分類器其辨識效果。特別在於腳
踏車以及機車兩類的鑑別上。其因可
由圖三以及圖四觀察。圖三顯示腳踏
車以及機車的輪廓形狀相當類似，但
其像素密度有所差異，因此此特性可
用來區分此兩類別，然而由於監控環
境複雜，腳踏車之低像素密度並非皆
可獲得，此時考量連續畫面之資訊，
亦即時間上之一致性，則辨識效果即
可獲得改善。 
 
表一. 利用 Adaboost.M1 [8]之移動物體辨識率 
 V B P M  PU 
V 100% 0 0 0 0 
B 0 89.3% 0 10.7% 0 
P 0 0.2% 99.8% 0 0 
M 0 8.6% 0 91.4% 0 
PU 0 0 6.9% 0 93.1%
 
 
(a)    (b)  
圖三. 經由連續畫面差所獲得之(a)腳踏車及 
(b)機車 
 
表二. 利用提出之植基於時間資訊之分類器所獲得
之移動物體辨識率 
 V B P M  PU 
V 100% 0 0 0 0 
B 0 94.6% 0 5.4% 0 
P 0 0 100% 0 0 
M 0 8.0% 0 92.0% 0 
PU 0 0 3.3% 0 96.7%
 9
1. 簡介(第三年研究成果) 
視訊內容標記以及視訊內容擷取之技術在
近十年以來已經引發廣泛的注意。由於視
訊內容包含相當豐富的資訊，因此經過標
記之視訊內容可應用之範圍相當廣泛，例
如視訊監控系統以及數位內容相關產業。在
眾多的視訊標記方法中，偵測視覺聚焦區
域被認為是一種有效的方法。根據認知心
理學家[10]的研究指出，人類視覺系統選
擇能夠在一場景中選取顯著的特徵。心理
學家相信這樣的過程強化了場景中重要的
部分，同時捨去不重要的資訊。 
然而，何謂場景中重要的部分? 許多視覺
聚焦模型已經被提出[1-5, 7-8] 。這些方
法可以區分為下而上或者由上而下兩大
類。通常由上而下之方法很難適用於一般
的視訊資料，因此我們的方法以由下而上
為基礎。在相關研究[8]中，Itti 以及 Koch
提出由下而上之方法，在靜態影像中尋找
視覺聚焦區域。Itti[5]最早提出一計算模
型，此方法以顏色及方向等為特徵。Ma[2]
在MPEG壓縮視訊中利用運動向量來建立計
算模型。此外，亦可計算 structural 
tensor[1]或者估計 optical flows[3]來
建立模型。在[3]的方法中，Zhai 及 Shah
建立時空模型來偵測視覺聚焦區域。 
本研究提出新穎的方法，其以跨越時空之
視訊分析為基礎，建立動態視覺聚焦模
型。在我們提出的方法中，首先在三度空
間中偵測重要的點，並以此為起點，在所
建立的動態視覺地圖中向外擴張。為尋找
適當的範圍，以滳為基礎之法則作為區域
擴張之依據。 
2. 視覺聚焦模型之建立 
2.1偵測跨越時空之重要點 
以 Laptev and Lindeberg [4]提出的二為
角點偵測方法為基礎，可偵測三維角點
[6] 。透過下列矩陣的計算，可獲得三維
空間中特徵點。 
( ) ⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
∗=
2
2
2
22 ,:,,
ttytx
tyyyx
txyxx
ii
LLLLL
LLLLL
LLLLL
tyxg τσμ ,   (1) 
( ) ( )( ) ( ) 2432222222 222exp,:,, llllll tyxtyxg τσπτστσ −+−=
                                       .(2)  
特徵點計算函式可以下列函式表示 
( ) ( ) ( )33213213tracedet λλλλλλμμ ++−=−= kkH .(3) 
特徵點之計算結果如圖一(b)所示。我們觀
察到特徵點皆鄰近於移動物體之邊界。相
反的，較不顯著的區域皆位於移動物體之
內部，這些區域大都包含相近的運動量。
為了尋找較為有效的特徵點作為起始搜尋
點，二元化後的區域如圖一(c)所示，取其
平均點作為起始點。 
(a) (b) (c) 
 
圖一 (a) 原始影格 (b) 特徵點計算結果 (c) 二元化之結
果  
2.2 動態視覺地圖 
有了特徵點作為起始點之後，所需要的是
一動態視覺地圖作為搜尋的依據。此一地
圖之建立，可以 Eq.(1)為基礎，計算三維
空間中之動態。一般來說，當 ( )μrank 等於三
表示此區域包含多運動方向。另外，當
( )μrank 等於二時，表示此區域包含單一主要
的運動方向。然而，在真實的視訊中，大
部分 ( )μrank 皆等於三。因此我們需要重新定
義來獲得動態視覺地圖。定義如下所示: 
( )( )⎩⎨
⎧
++
<=
otherwise
trace
d
,
,0
2
22
12
12
12
3 ελλλ
γμ
μ .  (4) 
由於我們希望找到運動一致的區域，因此
針對dμ 太大或太小皆不是所要的區塊。因
此，我們利用median filter fμ 來過濾不
要的區域。結果如圖二所示。明顯地，圖
 11
 (b) 
 
 
 
 
(c) 
 
 
 
 
(d) 
圖四 在不同類型的視訊資料中計算動態視覺聚焦
區域之結果  
  
(a) (b) 
  
(c) (d) 
圖五 以不同視覺地圖來計算視覺聚焦區域之結果 (a) 
原始影格 (b) 運動視覺地圖 (c)顏色地圖以及(d)特徵點
能量地圖 
4 結論： 
本研究提出新穎的方法，其以跨
越時空之視訊分析為基礎，建立動態
視覺聚焦模型。在我們提出的方法
中，首先在三度空間中偵測重要的
點，並以此為起點，在所建立的動態
視覺地圖中向外擴張。為尋找適當的
範圍，以滳為基礎之法則作為區域擴
張之依據。實驗結果顯示，所提出的
方法能夠有效地計算連續畫面中之視
覺聚焦區域。提出的方法其準確度可
達 75%。 
 
5 參考資料 
 
[1] S. Li and M. C. Lee, “An Efficient 
Spatiotemporal Attention Model and Its 
Application to Shot Matching,” IEEE Trans. 
on Circuits and Systems for Video Technology, 
Vol.17, No.10, pp. 1383-1387, Oct. 2007. 
[2] Y. F. Ma, L. Lu, H. J. Zhang, and M. Li, 
“A User Attention Model for Video 
Summarization,” In Proc. ACM Multimedia, 
pp.533-541, Dec. 2002.  
[3] Y. Zhai and M. Shah, “Visual Attention 
Detection in Video Sequences Using 
Spatiotemporal Cues,” In Proc. ACM 
 13
Continuous Human Action Segmentation and Recognition Using Spatio-
Temporal Probabilistic Framework 
 
 
Duan-Yu Chena, Hong-Yuan Mark Liaoa, and Sheng-Wen Shihb
aInstitute of Information Science, Academia Sinica, Taiwan 
bDepartment of Computer Science and Information Engineering,National Chi Nan University 
a{dychen,liao}@iis.sinica.edu.tw, bstone@csie.ncnu.edu.tw 
 
 
Abstract 
 
In this paper, a framework of automatic human 
action segmentation and recognition in continuous 
action sequences is proposed. A star-like figure is 
proposed to effectively represent the extremities in the 
silhouette of human body.  The human action, thus, is 
recorded as a sequence of the star-like figure 
parameters, which is used for action modeling. To 
model human actions in a compact manner while 
characterizing their spatio-temporal distributions, 
star-like figure parameters are represented by 
Gaussian mixture models (GMM). In addition, to 
address the intrinsic nature of temporal variations in a 
continuous action sequence, we transform the time 
sequence of star-like figure parameters into frequency 
domain by discrete cosine transform (DCT) and use 
only the first few coefficients to represent different 
temporal patterns with significant discriminating 
power. The performance shows that the proposed 
framework can recognize continuous human actions in 
an efficient way.  
 
1. Introduction 
 
Human behavior is a continuous flow of single or 
discrete human action primitives in succession. An 
example of a human behavior is a sequence of actions 
in which a subject extends the hand, opens the door, 
enters a room, sits down, stands up, and then walks 
away. Each component of the human behavior, such as 
walking, sitting down, standing up, extending the hand 
and getting up, is a discrete action primitive. 
Methodology for automatic segmentation and 
recognition of such continuous human behavior is 
proposed in this paper. When humans move from one 
action to another, the transitions are in general very 
smooth and the boundaries between actions can not be 
well defined. Most human activity recognition systems 
recognize either only single action sequences or static 
postures. This enables them to recognize actions but 
not necessarily to give an accurate temporal 
description of each action [16]. In this paper, we 
analyze continuous human activity by first 
automatically segmenting continuous human behavior 
into discrete actions, and then recognizing each 
segmented actions. Human body motion is actually the 
movement of the body’s parts or components, such as 
the torso, the hands and the feet. We find that the torso, 
the upper and lower limbs are the most informative in 
identifying ’breakpoints’ between the actions that we 
aim to segment and recognize. 
The recognition of human action captured in video 
sequences has been a topic of considerable interest in 
computer vision. The difficulty of human action 
recognition is twofold. First, the movement of a human 
body is an articulated motion. Therefore, it is obvious 
that the issue to be addressed is a problem with high 
dimensionality and complexity. Second, 
characterization of human behavior is equivalent to 
dealing with a sequence of video frames that contain 
both spatial and temporal information. The most 
challenging issue is how to properly characterize 
spatial-temporal information and then facilitate 
subsequent comparison/retrieval tasks. The objective 
of this study is to propose a methodology for 
systematic human action recognition. 
Existing action recognition methods can be categorized 
into two classes, i.e., 2-D based or 3-D based, 
depending on the types of human body model adopted 
[1]. In [2], Haritaoglu et al. proposed the W4 system 
which computed the vertical and horizontal projections 
of a silhouette to determine the global posture of a 
person (standing, sitting, bending and lying). Bobick 
and Davis [3] proposed a temporal template built by 
stacking a set of consecutive frames. The proposed 
temporal template characterized human motion by 
using motion energy images (MEI) and motion history 
where 
tl ,ω  is the weight of the  distribution of pixel 
’s mixture model; 
thl
tx lµ  is the mean of the  
distribution; and  is its covariance matrix, where 
thl
l∑
2
l l Iσ∑ = ,  lσ  is the standard deviation of the  
distribution, and I is an identity matrix. To update the 
model, each new pixel is checked to see if it matches 
the existing Gaussian distributions. If a match is 
confirmed on the l
thl
th distribution at time t, then a 
Boolean variable Ml,t is set, and the weight is updated 
with the following equation. To adjust the weight of 
each distribution, the weight 
tl ,ω  is updated by 
)()1( ,1,, tltltl Mαωαω +−= − ,                                    (2) 
where α  is the learning rate that controls the speed of 
the learning. The definition of M is as follows: =1 
when a match is confirmed on the  distribution at 
time t; otherwise, =0. The parameters 
tlM ,
thl
tlM , µ  and σ  
can be updated as follows: ( ) ttltl xβµβµ +−= −1,, 1                                             (3) 
( ) ( ) ( )2 2, , 1 ,1 Tl t l t t l t t l tx x ,σ β σ β µ µ−= − + − − ,               (4) 
where ( )1,1, ,| −−= tltltxP σµαβ . In each frame, pixels 
far away from the background distributions are 
recognized as foreground. A connectivity algorithm is 
then applied to identify possible objects in motion. It is 
widely recognized that an overly segmented result may 
break the detected objects into pieces. Therefore, 
morphological operations must be applied to ensure 
the completeness of foreground objects. A detected 
foreground object is considered as a blob and 
characterized by its position and color distribution to 
support the subsequent tracking process.  
 
3. Human Action Modeling 
In this section, we shall first represent human postures 
in a compact manner by silhouette-based star-like 
figures and then characterize the spatio-temporal 
distribution of a continuous human action represented 
by a sequence of star-like figures using mixture of 
Gaussians.  
3.1 Star-Like Posture Representation  
An important clue in determining the motion of a 
moving object is the change in its silhouette over time 
and one of the well-known approaches to quantify this 
is to use skeletonization. There are many standard 
techniques for skeletonization, such as thinning and 
distance transform. However, these approaches are 
computationally expensive and are highly sensitive to 
noise in the target silhouette [9]. Therefore, approaches 
that are relatively robust are proposed to detect 
extreme points on the silhouette of the target to 
produce a “star” skeleton [6][15]. The “star” skeleton 
consists of the gross extremities of the target joined to 
its centroid in a “star” fashion. However, the success 
of these methods heavily depends on detecting local 
maxima of the target silhouette that would be very 
sensitive to noise. In addition, decision on these local 
maxima is threshold dependent and thus is difficulty to 
be made systematically and robustly.  
Therefore, to characterize the silhouette of a video 
object robustly while maintaining low computation 
cost, we compute the star-like figure for each frame. 
This representation (refer to Fig. 3(a)) is based on the 
bounding box and thus results in eight representative 
extremities in human silhouette which are the points of 
top-left, top-right, left-top, left bottom, bottom-left, 
bottom-right, right-bottom and right-top.  Considering 
the nature of the number of primary body parts 
comprising the head, right hand, left hand, right foot 
and left foot, grouping these extremities into five 
dominant clusters is reasonable and is necessary for 
concisely representing human postures. Therefore, the 
approach of K-means clustering is appropriate for this 
aim and is thus used to group eight representative star-
sticks into five dominant ones as shown in Fig. 3(b). In 
Fig.3(c), star-sticks are corresponding to dominant 
body parts and thus distinct human actions can be well 
characterized by using the proposed star-like figure.  
(a)    (b) 
 
(C)  
 
Fig.3. (a) Extremities of human posture characterized 
by a star-like figure (b) a star-figure after executing K-
means clustering (c) demonstration of distinct human 
actions represented by star-like figures 
where the parameter set kiiii 1},,{ =∑= µαψ
( 0>iα , iki α1=∑ =1), µ is the mixture 
 covariance 
ture vectors, 
od est
mean ( d
i R∈µ ) and i∑  is a d×d matrix. 
Given a set of fea x1,…,xn, the maximum 
likeliho imation of Ψ is: 
, α is
mixture weight 
 the 
∑
=
=
n
j
iML xf
1
)|(logmaxarg ψψ ψ .                           (6) 
Given the current estimatio ram
each iteration of the EM a
revious subsection, we have demonstrated that 
 well 
CT represents a eccentricity sequence E(n) in 
n of the pa eter set Ψ, 
lgorithm re-estimates the 
parameter set. To initialize the parameter Ψ set, K-
means algorithm [14] is utilized to extract the data-
driven initialization. The updating process is repeated 
until the log-likelihood is increased by less than a 
predefined threshold from one iteration to the next. In 
this study we choose to converge based on the log-
likelihood measure, and we use a 1% threshold. Other 
possible convergence options include using a fixed 
number of iterations of the EM algorithm or defining 
target measures, as well as using more strict 
convergence thresholds. We have found 
experimentally that the above convergence 
methodology works well for our purposes. Using EM, 
the parameters representing the Gaussian mixture are 
found. 
3.2.2 Modeling Temporal Pattern Using DCT 
In the p
the distribution of a star-stick sequence can be
characterized by using GMMs. However, the temporal 
variation of the silhouette in a star-stick sequence is 
not included in the GMM. To discriminate continuous 
human actions based on temporal pattern, an 
eccentricity measure of the silhouette is used as the 
support. The eccentricity is defined as the ratio of the 
distance between the foci and the major axis length of 
the ellipse having the same second-moments as the 
silhouette. An ellipse whose eccentricity is 0 is actually 
a circle, while an ellipse whose eccentricity is 1 is a 
line segment. Since the eccentricity of the silhouette is 
a function of human posture, a continuous human 
action can be characterized by a sequence of silhouette 
eccentricity.  The eccentricity varies slowly as a person 
change his/her posture smoothly. In order to describe 
the temporal pattern in a sequence of eccentricity while 
maintaining low dimensionality, DCT is used to 
transform the eccentricity sequence into frequency 
domain and the first few DCT ac coefficients are 
selected for representing the smoothly varying 
eccentricity. DCT is employed based on the fact that it 
has near-optimal properties for energy compaction of 
highly correlated data and can be computed more 
efficiently than other similar transform (e.g. PCA, and 
ICA). 
The D
terms of its cosine series expansion with coefficients C
λ given by 
n
⎟⎠
⎞⎜⎝
⎛ += ∑
= n
icosnEC
t 2
)12()(
1
λπα λλ .                     (7) 
n
1
0 =α  and 1,..,2,1,2 −== nn λαλ
 
The first two dct ac coefficients, C2 and C3, behaves as 
t
. Action Segmentation 
hile each action frame is represented in γ-θ star-stick 
ence is a nature dissimilarity measure 
an approximation to the gradient of the eccentricity 
sequence E(n) and thus can give a rough 
approxima ion to the overall shape of the expression 
pattern for a eccentricity sequence. 
 
4
 
W
space, a set of action frames is modeled by GMM. In 
order to segment continuous human actions 
represented by temporal γ-θ star-stick GMM features, 
an efficient approach using KL-divergence as the 
underlying distance measure between sliding windows 
is developed.  
The KL-diverg
between two human actions represented by mixture of 
Gaussians. However, since there is no closed form 
expression for the KL-divergence between two MoGs, 
computing this distance measure is done using Monte-
Carlo simulations. Monte-Carlo simulations may cause 
a significant increase in computational complexity 
which can be a major drawback in applications of real-
time human action recognition. Therefore, we aim to 
solve this drawback by using an approximation of the 
KL-divergence between two mixtures of Gaussians 
[11]. KL-divergence between the Gaussians N1(µ1,Σ1) 
and N2(µ2,Σ2) has the following closed form: 
()()((log)||( 12211
1
22
1
21
1
2 TrNNKL T ∑−+∑∑+= −−∑∑ µµ ).)
Let )()( k α∑=  and )()( k β∑=  be the two 
21 d−− µµ
                                                                                   (8) 
i= i=
 mixture d s, where α={α
e can obtain 
1 xfxf ii 1 xgxg ii
Gaussian ensitie 1,…αk} and 
β={β1,…βk} are discrete distribution and fi and gi are 
Gaussian distributions Since no correspondence 
between the two GMM components is assumed, we 
can utilize the fact that for every permutation π defined 
on the set {1,2,…k}, the GMM 
)()(1 ii
k
i gg πππ β=∑=  is 
exactly the same as g. Hence, w the 
following approximation: 
 
Fig.5. Results of action boundary detection. 
 
 
 
Fig.6. Performance evaluation of continuous human 
action segmentation; (a) breakpoints detected by the 
proposed approach; (b) the corresponding ground truth. 
 
 
Fig. 7. Demonstration of the results of continuous 
action segmentation. The boundaries of a punching 
action is successfully detected where two set of action 
frames in the right and left of the breakpoint are at 
least with two different motions of dominant body 
parts. 
6.2. Action Recognition 
 
In order to evaluate the accuracy of the proposed 
spatio-temporal probabilistic framework, we conduct 
the following real experiment of an action recognition 
task based on similarity matching of DCT ac 
coefficients supported GMMs by using Eq.(9). The 
task is to find, for an unknown action sequence f, the 
member of the action models that is most similar to it. 
The number of Gaussians within the MoG was chosen 
as five based on the number of primary human body 
parts. The dimension of all the Gaussians is two, one is 
for star-stick orientation and the other is for its length. 
Image sequences are obtained using a fixed camera 
working at 24-30 frames per seconds and the 
resolution is 320×240. Totally six subjects participated 
in the experiments for providing the test and training 
sequences, in which 7 types of actions are present 
including walking, raising hands, bending, squatting, 
standing up, laying and sitting. In the test dataset, the 
action sequences consist of 126 actions whose number 
of frames ranges from 20 to 150 frames.  
0 200 400 600 800 1000 1200 1400 1600 1800
0
0.5
1
(a) 
 
0 200 400 600 800 1000 1200 1400 1600 1800
0
0.5
1
Table 1. Results of human action recognition by using 
proposed spatio-temporal probabilistic framework 
 
(b) 
The performance of the proposed spatio-temporal 
probabilistic framework is given in Table 1, which 
shows the precision of human action recognition. From 
Table 1, we can observe that the best performance is 
obtained by executing recognition in action type 
“Lying” because of its specific spatio-temporal pattern 
that is very different from the other 6 action types. In 
the actions, such as “Walking”, “Raising Hands”, 
“Bending”, “Sitting”, the detection ratio is above 80%. 
In the worst case, the recognition rate of action 
“Standing Up” is around 73% because usually people 
would move up the shoulders when they are standing 
A Framework of Spatio-Temporal Analysis for Video 
Surveillance 
Duan-Yu Chena, Kevin Cannonsb, Hsiao-Rong Tyanc 
aInstitute of Information Science, Academia Sinica, Taiwan 
{dychen, liao}@iis.sinica.edu.tw 
bDept. of Computer Science and Engineering, York Univ., 
Canada. {kcannons@cse.yorku.ca} 
 
Sheng-Wen Shihd and Hong-Yuan Mark Liaoa 
cDept. of Information and Computer Engineering, Chung 
Yuan Christian Univ., Taiwan 
{tyan@ice.cycu.edu.tw} 
dDept. of Computer Science and Information Engineering, 
National Chi Nan Univ., Taiwan 
 {stone@csie.ncnu.edu.tw} 
Abstract—This paper presents a video surveillance system that is 
capable of detecting and classifying moving targets in real-time.  
The system extracts moving targets from a video stream and 
classifies them into predefined categories according to their 
spatiotemporal properties. Classification of the moving targets is 
completed via a combination of a temporal boosted classifier 
and spatiotemporal “motion energy” analysis. We illustrate that 
a temporal boosted classifier can be designed that successfully 
recognizes five object categories: person(s), bicycle, motorcycle, 
vehicle, and person with umbrella. The proposed temporal 
boosted classifier has the unique ability to improve weak 
classifiers by allowing them to make use of previous information 
when evaluating the current frame.  In addition, we demonstrate 
a method to further process targets in the “person(s)” category 
to determine if they are single moving individuals or crowds.  It 
is shown that this challenging task of moving crowd recognition 
can be effectively performed using spatiotemporal motion 
energies. 
I. INTRODUCTION 
Automatic moving target classification has attracted much 
attention in recent years [1], especially in the field of 
surveillance. In the context of surveillance systems, a variety 
of machine learning classification techniques have been 
investigated, including support vector machines [3], naïve 
Bayes classifier [4], and AdaBoost [2]. AdaBoost is especially 
suitable for surveillance scenarios since it has achieved high 
detection rates using simple Haar-like features in real-time [2]. 
Though boosting paradigms have attracted significant 
attention recently, most previous moving target classification 
work has focused on boosting within a single frame. In fact, 
the use of temporal features as inputs to both weak and strong 
classifier levels has not been carefully studied in the past. 
Therefore, one critical contribution of this work is to present a 
novel method of introducing dynamic information into the 
AdaBoost framework. 
Within the surveillance domain, the targets that are typically 
of primary interest are people.  In this paper, our second goal 
is to further analyze moving targets that our temporal boosted 
Adaboost system classifies as “person(s)”.  The purpose of 
post-processing this class of targets is so that we can further 
identify two sub-classes: moving crowds and moving single 
persons.   
Here, we propose to incorporate spatiotemporal information 
into our sub-classification module through the use of oriented 
energies. The general applicability of spatiotemporal 
orientations and their relationship to motion perception was 
first realized in [6].  One of the applications of this field 
initially considered was the recovery of optical flow using 
filters in space-time [8]. It was illustrated [10] that qualitative 
descriptors can be assigned to a local spatiotemporal region 
using oriented energy signatures. Spatiotemporal orientation-
selective filters are already starting to be adopted in the 
tracking and surveillance domains.  In [7], orientation 
selective filtering of the spatiotemporal domain was 
performed to obtain a pixel-wise measure of coherent motion.   
In light of previous research, the main contributions of our 
proposed spatio-temporal analysis for crowd detection system 
are as follows.  First, efficient self-similarities are computed in 
spatial domain to detect candidates of moving crowds. 
Second, we apply powerful, oriented energy descriptors in 
temporal domain to recognize the real moving crowds within 
the candidates. 
II. TEMPORAL BOOSTED LEARNING 
In real-time scenarios, it is important to use features that are 
computationally inexpensive and invariant to lighting 
condition. Therefore, in this work, the features employed 
include: (i) the eccentricity of the bounding ellipse of a 
moving target; (ii) the orientation of the major axis of the 
bounding ellipse; (iii) the peak position of the normalized 
horizontal and vertical projection of a moving target; (iv) the 
pixel percentage of the peak in the normalized horizontal and 
vertical projection; (v) the difference in pixel density within 
the moving target’s bounding box for two consecutive frames; 
(vi) the difference between the first four eigenvalues 
(computed by PCA) for the moving target in two consecutive 
frames. Therefore, the total dimensionality of the feature 
vector is 11. 
In this work, we propose a temporal boosted learning 
algorithm. Moving objects are tracked using our proposed 
tracking method [12]. For the training algorithm, the error 
This work is supported by the Ministry of Economic Affairs under Contract No. 
96-EC-17-A-02-S1-032. 
IEEE ISCAS'08, Seattle, USA, May 2008.
was completed across a total of four 3D orientations ( )ξηθ ,=  
where η  and ξ  specify polar angles.  The four orientations 
that were selected correspond to upward, downward, leftward, 
and rightward motion.  Thus, a measure of local motion 
energy, e, can be computed using 
( ) ( ) ( )[ ] ( ) ( )[ ]2222; xIHxIGxe ∗+∗= θθθ ,                        
(5) 
where x = (x, y, t) are spatiotemporal image coordinates, I is 
the image sequence, and * denotes convolution. This initial 
measure of local energy, (5) is dependent on image contrast.  
However, a purer measure of oriented energies that is less 
affected by contrast can be obtained through normalization, 
( ) ( ) ( )( )εθθθ θ += ∑ ~ ~;;;ˆ xexexe ,           
(6) 
where ε  is a small bias term to prevent instabilities when the 
overall energy content is small and the summation in the 
denominator covers all orientations. Figure 2 provides an 
illustrative example of the motion energies that are utilized 
throughout the remainder of the crowd detection system.  
  
Fig.2. A sample frame of the PETS 2006 [12] data set with its corresponding 
motion energies. The final two rows show the rightward, leftward, 
downward, and upward motion energies, from left to right, top to bottom. 
2) Moving Crowd Detection Using Motion Energies 
Once the motion energies for a candidate region have been 
computed, they can be used as a vehicle for classifying 
between crowds and non-crowds.  Our proposed moving 
crowd detector evaluates three criteria to determine if a 
candidate is, in fact, a crowd. In what follows, we shall 
describe in detail how these criteria are realized. 
z Criterion #1: Multiple Dominant Motion Orientations 
in Target Region 
The first criterion is to analyze the number of orientations that 
contain significant levels of energy.  Specifically, for each of 
the four orientations, we compute the sum of the energies 
across the target support 
( ) ( )θθ ;ˆ *
1
i
n
i
XeS ∑
=
=
,           (7) 
where ( )*** , yxX i =  is a single target candidate pixel at some 
temporal instant and i ranges such that 
*
iX covers the target’s 
support. We use the following rule to decide if a crowd is 
present:  
( )[ ] ( )

 >
=
≠∈
   otherwise  0,
max if ,1 max],4...1[
1
max
θαθθθ SSD ii i ,        (8) 
where α is an empirically selected threshold and θmax is 
defined as the orientation with the maximal summed energy 
response across the target support.  Furthermore, D1 = 0 and 
D1 = 1 indicate that a crowd is not present and present by 
Criterion #1, respectively. 
z Criterion #2: Detecting Separations between High 
Motion Energy Regions 
The goal of the second criterion is to recognize crowds when 
the individuals involved are far apart from one another.  For 
this work, we assume that the camera angle is similar to that of 
the PETS 2006 image shown in Fig. 2.  Under this 
assumption, we project the single dominant motion energy of 
the candidate crowd region onto the X-axis.  Projection is 
performed onto the X-axis because most crowds will 
predominately contain motion in the horizontal directions.   
The ideas of Criterion #2 can be presented more formally 
as 
( ) ( )∑ ∗∗ =
y
yxexp
~
max;~,ˆ θ ,                       (9) 
where y
~
 varies over all rows in the candidate crowd region 
and x* corresponds to a column of the region.  The projected 
energies can be visualized as a normalized histogram.  Notice 
how the projected energies are close to zero for the image 
columns between the two individuals.  The goal of our second 
criterion is to identify when these low energy “gaps” occur.  If 
such a gap exists, it is concluded that the candidate region is a 
crowd.  Mathematically, our methodology for finding gaps can 
be written as 
( )

 ∈<
=
∗
otherwise,0
,...2,1  where if ,1 max,
2
CiPxp
D i
β ,               
(10) 
where xi* corresponds to a column of the candidate crop box, β 
is a user-defined threshold, i varies over all C columns in the 
target support, and  Pmax is the largest projected sum, (9), for 
any of the candidate's columns.  Furthermore, D2 = 0 and D2 = 
1 indicate that a crowd is not present and present by Criterion 
#2, respectively. 
z Criterion #3: Detecting Complex Energy Patterns 
In a similar manner to Criterion #2, the third criterion is 
employed in cases where the crowd candidate contains motion 
in only one dominant orientation.  The third criterion in our 
system detects crowds based on the fact that the projected 
energy signatures of a crowd are typically much more 
complex than those created by a single person consisting of 
multitude peaks and valleys. 
IV. EXPERIMENTAL RESULTS 
A. Performance of Temporal Boosted Classifier 
In the training process, 200 samples were used for each object 
category. Since our system design uses multiple one-against-
all classifiers, 200 samples for one category were included as 
positive examples while the other 800 samples from the other 
four categories were used as negatives. In the testing process, 
we made the system run for several days from 9 am to 5 pm 
under distinct weather conditions, including sunny, cloudy and 
rainy days.   
In order to compare with the proposed temporal boosted 
classifier, the original AdaBoost algorithm [5] was 
implemented. Table 1 shows the confusion matrix obtained 
using the original AdaBoost algorithm shown in light gray and 
that obtained when using our proposed method shown in deep 
IEEE ISCAS'08, Seattle, USA, May 2008.
DYNAMIC VISUAL SALIENCY MODELING BASED ON SPATIOTEMPORAL ANALYSIS 
 
Duan-Yu Chena, Hsiao-Rong Tyanb, Dun-Yu Hsiaoa, Sheng-Wen Shihc, and Hong-Yuan Mark Liaoa  
 
aInstitute of Information Science, Academia Sinica, Taiwan 
bDept. of Information and Computer Engineering, Chung Yuan Christian Univ., Taiwan 
cDept. of Computer Science and Information Engineering, National Chi-Nan Univ., Taiwan 
a{dychen, dyhsiao, liao}@iis.sinica.edu.tw, btyan@ice.cycu.edu.tw, cstone@csie.ncnu.edu.tw 
 
ABSTRACT 
 
Producing an appropriate extent of visually salient regions 
in video sequences is a challenging task. In this work, we 
propose a novel approach for modeling dynamic visual 
attention based on spatiotemporal analysis. Our model first 
detects salient points in three-dimensional video volumes, 
and then uses them as seeds to search the extent of salient 
regions in a motion attention map. To determine the extent 
of attended regions, the maximum entropy in the spatial 
domain is used to analyze the dynamics obtained from 
spatiotemporal analysis. The experiment results show that 
the proposed dynamic visual attention model can effectively 
detect visual saliency through successive video volumes.  
 
Index Terms— visual attention, spatiotemporal 
analysis 
 
1. INTRODUCTION 
 
Methods for annotating video content as well as related 
video retrieval techniques have attracted a great deal of 
attention in recent years. Since video content contains much 
richer information than other types of media, the annotated 
content can be applied to many real-world problems, such 
as video surveillance systems and entertainment. Among the 
different types of video annotation approaches, detecting 
visual salience from a video is considered a good way of 
understanding the video’s content. According to a study 
conducted by cognitive psychologists [10] the human visual 
system picks salient features from a scene. Psychologists 
believe this process emphasizes the salient parts of a scene 
and, at the same time, disregards irrelevant information. 
However, this raises the question: What parts of a scene 
should be considered “salient”? Many visual saliency (or 
attention) models have been proposed in the past decade 
[e.g., 1-5, 7-8]. Based on the type of attention pattern 
adopted, the models can be roughly categorized into two 
classes: bottom-up approaches, which extract image-based 
saliency cues; and top-down approaches, which extract task-
dependent cues. Usually, extracting task-dependent cues 
requires a priori knowledge of the target(s). However, a 
priori knowledge of attended objects is usually difficult to 
obtain. Therefore, we focus on a bottom-up approach in this 
work.  
In [8], Itti and Koch reviewed different computational 
models of visual attention, and presented a bottom-up, 
image-based visual attention system. Previously, Itti et al. [5] 
had proposed one of the earliest saliency-based 
computational models for determining the most attractive 
regions in a scene. The contrasts in color, intensity and 
orientation of images are used as clues to represent local 
conspicuity in images.  
In [2], Ma et al. used the motion vector fields in MPEG 
bitstreams directly to build a motion attention map. In 
addition, some approaches have proposed extending the 
spatial attention model from images to videos in which 
motion plays an important role. To find motion activity in 
videos, the above-mentioned motion attention models 
compute structural tensors [1], or estimate optical flows 
directly in successive frames [3].  
In [3], Zhai and Shah construct both spatial and temporal 
saliency maps and fuse them in a dynamic fashion to 
produce an overall spatiotemporal attention model. The 
model first computes the correspondence between points of 
interest and then detects the temporal saliency using 
homography. Li and Lee [1] proposed a model of 
spatiotemporal attention for shot matching. Under this 
approach, a motion attention map generated based on 
temporal structural tensors and a static attention map are 
used simultaneously to determine the degree of visual 
saliency. The weights of the feature maps are varied 
according to their contrast in each video frame. However, 
with this mechanism, the degree of visual saliency could be 
biased by the static attention map when the static features 
have higher contrast than the motion features.  
In this paper, we propose a model that extracts the salient 
feature points from the 3D spatiotemporal volumes of video 
sequences. In this manner, we are able to capture the 
dynamics of a video sequence. The feature points are used 
as seeds to search the extent of salient regions in a motion 
attention map, which we discuss in the next section. To 
identify the exact ranges of visually attended areas correctly, 
ICME 2008, Hanover, Germany, June 23-26, 2008.
proper combination of these two maps can determine the 
real extent of a salient area. To combine two salient maps 
that complement each other, the seeds produced in the first 
step, described in Section 2.1, are used as the starting points 
to search the appropriate extent in the motion attention map. 
 
 
Fig. 2. Saliency maps generated by (a) the salient points measure 
H; and (b) the rank-deficiency measure . μμ
fd
2.3 Selective Visual Attention Modeling 
 
To find the extent of attended regions, we take the 
configuration of a motion attention map that corresponds to 
the maximum entropy as our target. For each motion 
attention map, the most appropriate scale Se for each region 
centered at the seed (xs,ys) is obtained by  ( )( ) (( ) ){ }tyxeWtyxeHmaxargS ssssee ,,,,,, expexp ×= ,        (6) 
where  
( )( ) ( ) ( )( )∑
∈
−×=
Dv
tyxevtyxevss ssss
pexpptyxeH ,,,,,,,,exp 1,,, ,      (7) 
and  
   ( )( ) ( )( ) ( )( )tyxeHtyxeHtyxeW . ssexpssexpss ,,,1,,,,,,exp −−=
,,,,
(8)  
D is the set of all values that contains the values of d , 
which correspond to the histogram distribution in a local 
region e around a seed (x
μ
μ
f
s,ys) in a motion attention map at 
time t. The probability mass function, p , is 
obtained from the histogram of pixel values at time t for 
scale e in position (x
( ) tyxev ss
s,ys) and the value v, which belongs to 
D.  
The efficacy of the proposed approach can be observed in 
Fig. 3(a). The green bounding boxes are obtained from the 
map measured by the salient function H, while the red boxes 
are obtained by searching the proper scale in the motion 
attention map using the entropy maximization process. Fig. 
3(b) shows a saliency map generated by exponential entropy 
maximization. The attended regions are determined by 
finding the extent that covers the moving target well.  
 
 
(b)
Fig. 3. Attended regions detected by the proposed approach. 
 
3. EXPERIMENT RESULTS 
 
To evaluate the performance of the proposed visual saliency 
model, we conducted experiments on different kinds of 
videos of size 320× 240 with dynamic backgrounds. The 
test videos included sports games and movies, which were 
comprised of many kinds of activities as well as distinct 
camera operations. 
(a) (b) 
Fig. 4 shows the proper extent of visual saliency in four 
distinct shots. The first row of each example is the original 
video frame, and the second row represents the result of 
detected salient regions highlighted by red boxes. We 
observe that salient regions in the low contrast case in 
Fig.4(d), and the complicated moving background in 
Fig.4(b) can be accurately located with an appropriate 
extent.  
To demonstrate the efficacy of the proposed approach, 
different feature maps, namely an intensity map and a 
salient points’ map, were also employed in the experiments. 
After the salient seeds had been obtained, the extent 
searching process was applied to the above maps. From Fig. 
5, it is clear that our approach is better able to search the 
proper extent of dynamic visual saliency in the motion 
attention map than in the other two feature maps.  
   
 
(a) 
 
 
(a) (b) 
ICME 2008, Hanover, Germany, June 23-26, 2008.
IEEE International Conference on Multimedia and Expo 2008 
 
國際會議心得報告 
會議舉行時間：2008 年 06 月 23 日~2008 年 06 月 26 日 
會議舉行地點：德國 漢諾威 (Hanover, Germany) 
 主題演講心得 
 
此次 tutorials邀請數位 IEEE Fellow蒞臨演講，其中最感興趣的是Carnegie 
Mellon University陳祖翰教授講授之“Learning Human Perception and 
Semantics Understanding in Multimedia Applications” 。演講內容主要
在於介紹如何利用machine learning來學習及辨識多媒體語意內涵，特別是如
何在低階視訊資料中利用學習機制將人臉變的更清晰，並且透過資料庫比對，尋
找相似的物體，或者藉由學習機制在音訊以及視訊資料中尋找某些特定人物等
等，其目的在於提供一新穎的工具以及提供multimedia semantic computing之
方法。 
 
 論文發表心得 
 
此次發表論文題目為“DYNAMIC VISUAL SALIENCY MODELING BASED ON 
SPATIOTEMPORAL ANALYSIS”, 在視訊資料中，偵測人類視覺聚焦區域並且
能夠計算出適當的區域範圍是相當具有挑戰性的研究議題。本研究提出新穎的方
法，其以跨越時空之視訊分析為基礎，建立動態視覺聚焦模型。在我們提出的方
法中，首先在三度空間中偵測重要的點，並以此為起點，在所建立的動態視覺地
圖中向外擴張。為尋找適當的範圍，以滳為基礎之法則作為區域擴張之依據。實
驗結果顯示，所提出的方法能夠有效地計算連續畫面中之視覺聚焦區域。在報告
之後,與會者討論踴躍,幾位學者認為此研究頗具深度,並給予相當正面的評價。 
 
 會後心得 
 
此次發表論文之後，參與討論者眾多，顯見 multimedia semantic computing
之研究議題十分熱門且具有挑戰性，所發問之問題主要在於如何正確地契合使用
者之需求，例如，我們提出的方法中，主要在於在視訊資料中自動化且系統化地
計算視覺聚焦區域，但如何評估是否符合使用者之需求，以及計算結果如何符合
使用者之觀感，此為兩大主要將來研究議題，因此，參加此次會議獲益良多，不
僅與會中參與人員有許多互動，並且也激發未來的研究方向。 
the maximum entropy in spatial domain is calculated and 
used to assist the range determination process. 
The remainder of the paper is structured as follows. In the 
next section, we introduce the proposed visual saliency 
model. Section 3 details the experiment results. Then, in 
Section 4, we present our conclusions.  
 
2. SELECTIVE VISUAL ATTENTION MODELING 
 
To locate the salient regions in a video efficiently, we first 
detect the salient points in the video’s corresponding 
spatiotemporal 3D volume. The points are then used as 
seeds to search the extent of the salient regions in the 
constructed motion attention map, in which the extent of the 
salient regions is determined by finding a motion map that 
corresponds to the maximum entropy. In the following, we 
first describe the process for detecting spatiotemporal 
salient points and then explain how we generate a motion 
attention map. Finally, we introduce the selective visual 
attention model based on finding the maximum entropy.  
 
2.1. Detection of Spatiotemporal Salient Points 
The spatiotemporal Harris detector, proposed by Laptev and 
Lindeberg [4], extends Harris and Stephens’ corner detector 
[6] to consider the time axis. Similar to the operation 
performed in the spatial domain for a given spatial scale  
and temporal scale , the spatiotemporal Harris detector is 
based on a 3×3 second moment matrix 
lσ
lτ
μ . The matrix is 
composed of first order spatial and temporal derivatives 
averaged with a Gaussian weighting function ( )2i2 ,:,, ityxg τσ , i.e., 
( ) ⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
∗=
2
2
2
22 ,:,,
ttytx
tyyyx
txyxx
ii
LLLLL
LLLLL
LLLLL
tyxg τσμ ,              (1) 
where the integration scales are  and ;  
is a first-order Gaussian derivative through the 
22
li sσσ = 22 li sττ = κLκ  axis; and 
the spatiotemporal separable Gaussian kernel is defined as  
( ) ( )( ) ( ) 2432222222 222exp,:,, llllll tyxtyxg τσπτστσ −+−= .(2)  
To detect interest points, regions that have significant 
corresponding eigenvalues 1λ , 2λ , 3λ  of μ  are considered 
salient. The saliency function can be defined as 
( ) ( ) ( )33213213tracedet λλλλλλμμ ++−=−= kkH  .      (3) 
The salient points detected by Eq.(3) are illustrated in Fig. 
1(b). We observe that most salient points are located near 
the boundary of an attended object due to the intrinsic 
nature of corners. In contrast, the regions with relatively low 
saliency are located inside the moving objects, and usually 
correspond to consistent motion. To generate effective seeds 
for searching the appropriate extent of moving regions, we 
adopt the centroid of every salient region to replace the 
commonly used local maxima points. The map shown in 
Fig.1(b) is binarized by performing morphological 
operations; and the regions that correspond to those salient 
points are shown in Fig.1(c). The centroid of each region 
shown in Fig.1(c) is then taken as a seed for the subsequent 
search task.  
 
Fig.1. (a) the original video frame; (b) interest points detected by 
using 3D Harris corner detection; (c) the binarized salient map of 
(b)  
(a) (b) (c) 
2.2. Motion Attention Map 
 
After detecting the seeds, we compute a motion attention 
map to determine the extent of searching. Our goal is to find 
salient regions that have consistent motions. The optical 
flow ( )wvu ,,  of the neighborhood can be estimated by 
solving the following structural tensor [9]:  [ ] 130 ×=⋅ Twvuμ ,                                         (4) 
where μ  is the matrix defined in Eq.(1).  
Ideally, there would be multiple motions within the 
Gaussian smoothed neighborhood with spatial scale  and 
temporal scale when 
iσ
iτ ( )μrank  is equal to three. If there is 
consistent motion in a windowed area, the ( )μrank  would 
be equal to two. In degenerate cases, i.e., ( )μrank  equals 0 
or 1, the motion of an image structure cannot be derived 
directly by Eq.(4). However, in real videos, μ  is always 
with full rank. Therefore, the normalized and continuous 
measure mentioned in [1] is used to quantify the degree of 
deficiency of a matrix. Let the eigenvalues of μ  be 
321 λλλ ≥≥ . The continuous rank-deficiency measure dM is 
defined as ( )( )⎩⎨
⎧
++
<=
otherwise
trace
d
,
,0
2
22
12
12
12
3 ελλλ
γμ
μ ,             (5) 
where ε  is a constant used to avoid division by zero. The 
threshold γ  is used for handling cases when ( ) 0=μrank .  
Since we want to find the regions with consistent motions, 
regions with high and low values of dμ are not considered 
attended regions. In other words, regions with median 
values are the targets of interest. Therefore, we use a 
median filter fμ to filter out regions with multiple motions 
and keep regions with consistent motions. As a result, the 
motion attention map  can be obtained for further 
processing. Fig. 2(b) shows the motion attention map 
generated by  for the original video frame in Fig.1(a).  
μ
μ
fd
μ
μ
fd
Clearly, the motion attention map  complements the 
saliency map generated by H defined in Eq. (3). Hence, a 
μ
μ
fd
ICME 2008, Hanover, Germany, June 23-26, 2008.
