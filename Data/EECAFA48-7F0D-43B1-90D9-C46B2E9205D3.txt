 2
Scheduling Problem for Family of 
Machines”, which is abbreviated as the 
MSPFM. The mathematical model for 
the MSPFM was first presented in Goyal 
and Kusy’s (1985) paper. We extend 
their work in two aspects: First, we 
conduct analysis on the mathematical 
model of the MSPFM, and show that the 
objective function of the MSPFM is 
Lipschitz. Second, we propose a 
dynamic Lipschitz optimization 
algorithm that solves a global optimal 
solution for the MSPFM. 
For the rest of this section, we 
review Goyal and Kusy’s (1985) model 
for the MSPFM. Before presenting the 
mathematical model, we first introduce 
the assumptions made and the notation 
used later. There are n machines in the 
family and they need to be maintained 
periodically. In the MSPFM, the 
decision maker plans the maintenance 
schedules of a family of machines in 
some basic period, denoted by T, (e.g., 
in days, weeks, or bi-weeks, etc.). 
Denote Ti as the maintenance cycle of 
the ith machine, which is the time 
interval between the start-up of 
successive maintenance of the ith 
machine. We assume that preventive 
maintenance of each machine is carried 
out at an integer multiples of the basic 
interval T. (That is, i iT k T= where 
, 1,..., .ik i n
+∈ =] ) We note that the 
model for the MSPFM is for planned 
maintenance, and the model does not 
consider unplanned machine failure in 
the maintenance scheduling. 
The cost of carrying out 
maintenance on a machine primarily 
consists of a fixed cost, which is due to 
the administrative costs and set-up costs 
involved, and a cost that directly 
depends on the actual work carried out 
on the machine. Goyal and Kusy (1985) 
commented: “In many situations, the 
fixed cost associate with maintenance is 
very significant because the 
maintenance work is subcontract or 
repairmen have to make a special trip 
from some central place to carry out the 
work”. Therefore, the focus of the 
MSPFM is to optimally coordinate the 
maintenance schedule of machines to 
save the fixed cost incurred. In the 
following discussion, we present those 
two categories of cost terms in the 
MSPFM. 
First, we provide the derivation of 
the average operating cost. We note that 
the operating cost directly depends on 
the actual work carried out on the 
machine. It is assumed that the operating 
cost of a machine after operating for 
time ‘t’ without maintenance is given by  
Oi(t) = eii tvf +                (1) 
where fi and vi are constants for the 
ith machine, while the exponent ‘e’ is 
the same for all the machines. As the 
maintenance work on the ith machine is 
carried out at time interval of kiT, its 
average operating cost is given by 
( )0(1 ) ( ) /( 1)ik T ei i i i ik T O t dt f v k T e∫ = + +   (2) 
Next, we discuss the terms for the 
fixed cost. The average fixed cost of 
carrying out maintenance for the ith 
machine, is ( )i im k T  where mi is the 
fixed cost for the maintenance of the ith 
machine each time. On the other hand, 
as maintenance work is carried out at 
intervals of T, a set-ups cost, denoted by 
M, will be incurred for the family of 
machines scheduled for maintenance in 
each basic interval. So, we have M T  
as another term for the average fixed 
cost.  
We define 
1 2 1
( , , , , ) ( , )nn i ii
Mk k k T k T
T
∑
=Ψ ≡ + Φ…  
where ( , )
1
e e
i i i
i i i
i
m v k Tk T f
k T e
Φ = + ++
. 
Then, we may express the mathematical 
model for the MSPFM as the problem. 
(P) { }
1 2
0, , 1,...,
10, , 1,...,
 inf  ( , , , , )
inf ( , )
i
i
nT k i n
n
i iiT k i n
k k k T
M T k T
+
+
∑
> ∈ =
=> ∈ =
Ψ
= + Φ
]
]
…
(3) 
In the MSPFM, the decision maker 
needs to determine T (i.e., the basic 
 4
 Before Amotz, et al.’s (2002) work, 
Anily, Glass and Hassin (1998) studied a 
special case of M=1 and 
0, for 1,...,ic i n= = . Later, they presented 
a heuristic with the worst-case 
performance ratio of 1.033 in Anily, 
Glass and Hassin (1999). On the other 
hand, Krämer and Bai (1996) considered 
a production system in which the system 
is shut down when the machine is 
undergoing maintenance. While the 
(periodic) maintenance disruptions make 
the production deviate from the demand, 
the objective of their study is to produce 
following the demand as closely as 
possible. They formulated this 
production-maintenance management 
problem as an optimal control model 
and proposed to solve it by Pontryagin’s 
minimum principle.  
 After reviewing the literature, we 
found that Goyal and Kusy’s (1985) 
paper presented the only model that used 
a nonlinear function for the cost of 
operating a machine when studying the 
periodic maintenance scheduling 
problems. Unfortunately, one is not able 
to directly apply the solution approaches 
in the aforementioned papers to solve 
the MSPFM (with a nonlinear operating 
cost function). In the next subsection, 
we will review Goyal and Kusy’s (1985) 
heuristic for solving the MSPFM. 
 Since the problem (P) is a notorious 
nonlinear-integer program, Goyal and 
Kusy’s (1985) heuristic obviously does 
not guarantee to obtain an optimal 
solution for the MSPFM. (In fact, Goyal 
and Kusy’s (1985) heuristic often stuck 
at some local optimum during the 
iterative procedure in our numerical 
experiments; please refer to Section 4 
for the details later.) Since the authors 
found no solution approach that could 
solve the optimal solution for the 
MSPFM in the literature, we are 
motivated to propose a new approach 
based on global optimization theory in 
this study.  
 
3. A Dynamic Lipschitz Algorithm(動
態 Lipschitz 最佳化演算法) 
It is well-known that one could apply 
efficient global-optimization approaches 
to solve an optimization problem when 
its objective function (with a univariate) 
is Lipschitz. Our focus in this section is 
to propose a dynamic Lipschitz 
algorithm to solve the MSPFM.  
We divide our discussions in this 
section into three parts. Section 3.1 first 
conducts theoretical analysis on the 
mathematical model of the MSPFM. 
Our theoretical results establish 
important foundation to show that the 
objective function of the MSPFM is 
Lipschitz. Next, we employ a relaxation 
of the MSPFM to set the search range 
for the proposed dynamic Lipschitz 
algorithm in Section 3.2. Finally, 
Section 3.3 shows that the objective 
function of the MSPFM is Lipschitz 
after transforming it into a univariate 
function, and summarizes the 
step-by-step procedure of the proposed 
dynamic Lipschitz algorithm.  
3.1 Theoretical analysis on the 
problem (P) (問題(P)的理論分析) 
In this subsection, we conduct 
theoretical analysis on the mathematical 
model of the MSPFM. Our theoretical 
results will establish important 
foundation to show that the objective 
function of the MSPFM is Lipschitz.  
By observing the right-side of the 
objective function, we learn that it is 
separable in terms of ki’s. Therefore, we 
first analyze the properties of ),( TkiiΦ  
individually since they shall establish 
foundation for our further investigation 
on the function 1 2( , , , , )nk k k TΨ … . 
Proposition 1. For any given ik
+∈] , 
the function ),( TkiiΦ  satisfies the 
following properties for },...,1{,0 niT ∈> . 
1. ),( TkiiΦ  is strictly convex. 
2. ),( TkiiΦ  has a minimum for 
 6
function )(TΓ  in (9). Then, it is 
obvious that solving (R) is equivalent to 
obtain an optimal solution for the 
problem (R1) as follows. 
 (R1) ( )
10
( ) inf{ ( )}n RiiTh T M T g T
∑
=>= + (12) 
If we ignore the constraint 1≥ik , 
then ki becomes a continuous variable. 
In such a case, for any given value of 
0T > , we may easily obtain the optimal 
value for 
( , ) ( ) ( 1)e ei i i i i i ik T m k T v k T e fΦ = + + +  
by [ ] TevmeTk eiii 1)1()( 11* ++= .  
Recall that ( ) 1 1* ( 1) ei i ix e m ev += +⎡ ⎤⎣ ⎦ , 
which is expressed in (4). When *iT x≤ , 
we have *( ) 1ik T ≥ , which satisfies the 
constraint 1≥ik . Therefore, ( ) ( )Rig T  
obtains its optimal value as a constant 
by ( ){ } ieiei fveme ++ +11)1(  for *iT x≤ .  
On the other hand, when *iT x> , we 
have *( ) 1ik T < , and we are forced to 
take *( ) 1ik T =  if we take the constraint 
1≥ik  into accounts. By summarizing 
both cases, it follows that 
( ){ }1 ( 1) *( )
*
( 1) , if .
( )
(1, ) ( ) ( 1) , if .
ee
R i i i i
i
e
i i i i i
e m e v f T x
g T
T m T vT e f T x
+⎧ + + ≤⎪=⎨⎪Φ = + + + >⎩
 (13) 
Also, we could easily obtain the 
first derivative of ( ) ( )Rig T : when 
*
iT x≤ , 
( ) ( ) 0
R
idg T
dT = , and when *iT x> ,  
( )
( )
2 1
( ) (1 , )
1 0
R
i i
e
i i
d g T d T d T d T
m T v e T −
= Φ
= − + + >
(14) 
Therefore, we conclude that the 
function )()( Tg Ri  is convex, increasing, 
and continuously differentiable on 
),0( ∞ .  
Without loss generality, we assume 
that **2
*
1 ... nxxx ≤≤≤ , then the strictly 
increasing derivative h’(·) is given by 
( )
( )
2 *
1
' 1 2 * *
1
1 1
1 2 *
1
1 1
, if . 
( ) ( ( 1)) , if ,1 1. 
( ( 1)) , if . 
n le
i i l l
i i
n ne
i i
i i
M T T x
h T e e v T M m T x T x l n
e e v T M m T T x
−
+= =
−
= =
⎧⎪− ≤⎪⎪= + − + ≤ ≤ ≤ ≤ −∑ ∑⎨⎪⎪ + − + ≥∑ ∑⎪⎩
(15) 
By setting the derivative of h(·) in 
(15) to zero, we have the following 
proposition to locate the optimal 
solution T(R) for (R1). 
Lemma 2. Assume without loss of 
generality that **2
*
1 ... nxxx ≤≤≤ . If it 
holds that 
* ' *max{1 : ( ) 0}ii i n h x≡ ≤ ≤ < , then the 
optimal solution T(R) of (R1) is given by  ( )* * 1 ( 1)( ) 1 1(( 1) ) ei iR i ii iT e e M m v +∑ ∑= =⎡ ⎤= + +⎢ ⎥⎣ ⎦  (16) 
Let )(Rυ  be the optimal objective 
function value of (R1). Then, )(Rυ  can 
be obtained by plugging T(R) into the 
objective function of the problem (R1). 
We note that )(Rυ  serves as a lower 
bound on the optimal objective function 
value of the problem (P).  
Sometimes T(R) is the optimal 
solution for the problem (P), as we show 
it by the following lemma. 
Lemma 3. Without loss of generality, 
we assume that **2
*
1 ... nxxx ≤≤≤ . If it 
holds that *)( n
R xT ≥ , then (1,…,1,T(R)) is 
an optimal solution for the problem (P).  
When ( ) *R nT x< , T(R) may not be 
an optimal solution of the problem (P). 
(This is due to the fact that the values of 
ki corresponding to T(R) are not 
necessary integers. This implies that the 
optimal solution of (R1) is generally not 
feasible for the problem (P).) In such a 
case, we propose to use a dynamic 
Lipschitz algorithm to search for an 
optimal solution for (P). Define )(FPυ  
as the objective function value at a 
feasible solution of the problem (P). 
Next, we discuss how to use T(R) and 
)(FPυ  to determine the search range in 
our dynamic Lipschitz algorithm.  
Though T(R) may not be an optimal 
solution of (P) for ( ) *R nT x< , we could 
easily obtain ( )* ( ) * ( )1 ( ),..., ( )R Rnk T k T  at 
T(R) by (11) in Lemma 1, and then, 
( * ( ) * ( )1 ( ),..., ( )
R R
nk T k T ,T
(R)) becomes a 
 8
( ) ( ) ( )FP R Rυ υ ευ− ≤ , then we stop 
since we have anε -optimal solution 
where ( ) ( )( ) ( ( ), )R RFP T Tυ = Ψ k  
4. If the solution on-hand is not 
ε -optimal, locate the bounds Tlow 
and Tup of the search range by some 
line search method by finding the 
two values of T where the objective 
function of (R1) equals )(FPυ .  
5. We apply the algorithm of 
Evtushenko (see Horst and Pardalos, 
1995) as the Lipschitz optimization 
tool with a dynamic Lipschitz 
constant on the interval ],[ uplow TT . 
4. Numerical Experiments(數據實驗) 
In the first part of this section, we 
employ a numerical example to 
demonstrate the implementation of the 
proposed Lipschitz optimization 
algorithm. Then, we use randomly 
generated instances to show that our 
dynamic Lipschitz algorithm 
outperforms Goyal and Kusy’s (1985) 
heuristic. 
4.1 Random experiments(隨機實驗) 
In this subsection, we present a 
summary of our random experiments. 
We design our experimental settings by 
selecting six values for the number of 
machines n (n = 3, 5, 7, 10, 25, 50) and 
six values for the fixed cost associated 
with set-ups required for maintenance 
work (M = 50, 100, 200, 500, 750, 1000). 
This yields 36 combinations from those 
n and M settings. Then, for each 
combination, we randomly generate 100 
instances in which the values of mi, fi, vi 
and e come from uniform distribution 
functions with their ranges indicated in 
Table 1.  
Table 1. The settings of the parameters in our 
random experiments. 
For those 3,600 instances, we solve 
each one of them by the proposed 
Lipschitz optimization algorithm as well 
as Goyal and Kusy’s (1985) heuristic on 
a Pentium-II-MMX with 192 RAM. The 
error allowance in the Lipschitz 
optimization algorithm is set to 
0.001%ε = . We summarize our 
experimental results for the smaller-size 
problems (with n = 3, 5, 7) and the 
larger-size problems (with n = 10, 25, 50) 
in Tables 2 and 3, respectively. 
Table2. Our experimental results for the 
smaller-size problems. 
 Lipschitz algorithm Goyal and Kusy’s heuristic 
n M Run time Run 
time
Non-optimal Max. 
Error 
(%) 
Avg. 
Error 
(%) 
50 3.35 0.00 37/100 9.790 0.812
100 3.19 0.06 26/100 6.900 0.532
200 2.53 0.00 15/100 3.504 0.158
500 1.53 0.05 6/100 1.440 0.055
750 0.99 0.00 6/100 1.235 0.031
3
1000 0.61 0.06 0/100 0.000 0.000
50 6.53 0.00 59/100 9.697 1.502
100 7.75 0.06 33/100 8.275 0.462
200 7.41 0.05 23/100 3.615 0.195
500 4.62 0.06 7/100 0.610 0.015
750 3.13 0.00 9/100 0.860 0.024
5
1000 2.14 0.05 6/100 0.956 0.024
50 9.34 0.05 74/100 7.757 1.284
100 10.60 0.06 39/100 6.114 0.559
200 10.77 0.05 28/100 3.682 0.289
500 9.17 0.06 16/100 1.397 0.085
750 6.15 0.05 9/100 0.484 0.025
7
1000 5.22 0.06 11/100 0.584 0.024
One may observe that the run time 
of Goyal and Kusy’s (1985) heuristic is 
extremely short. Only for the case with a 
small number of machines (n=3) and a 
very large fixed set-up cost (M=1000) 
one can obtain satisfactory solutions 
using Goyal and Kusy’s heuristic. On 
the other hand, the proposed Lipschitz 
optimization algorithm also solves the 
MSPFM with a global optimum 
efficiently. (It takes less than 80 seconds 
for larger-size problems with n=50.) 
On the aspect of solution quality, 
the proposed Lipschitz optimization 
algorithm significantly outperforms 
Goyal and Kusy’s heuristic. The fifth 
n 3,5,7,10,25,50 
M 50,100,200,500,750,1000 
mi [1,500]  
fi [15,50] 
vi [1,20]  
e [1,4]  
