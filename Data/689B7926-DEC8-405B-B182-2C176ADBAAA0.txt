NSC 96-2221-E-027 -097 -MY3 
 
首曲子為何，甚至想去購買所屬的 CD唱片。
但問題是你對它的曲名或歌唱者資訊一無所
知，同時也可能無法準確地哼唱出任何其中的
片段，此時你大概會因不知如何是好而感到掃
興。也許要等到下次某種情況下再聽到廣播或
電視裡播放這首歌時才能知道它的曲名。然
而 ， 如 果 當 時 你 拿 起 手 邊 的 MP3 
Player/Recorder、手機、或是 PDA將它錄一段
下來，之後便可以根據所錄下來的音樂片段向
他人詢問那曲子為何。但詢問他人也許頗費周
章，且不一定能獲得答案，最理想的方式是將
音樂片段輸入電腦或某搜尋引擎網站中查
詢，甚至直接利用手機將音樂片段上傳至某網
路伺服器中查詢。當搜尋引擎或伺服器查出你
想要找的曲子後，也將這曲子的相關資訊，例
如歌詞、演唱者介紹以及如何購買等一併提供
給你。 
欲達成上述的音樂查詢模式，必須具備自
動分析與識別音樂資料的技術。現今網路搜尋
引擎大多著重於文字資料或圖片的網頁搜
尋，甚少提供音樂資料的查詢功能。若想從網
路上取得音樂資料，一般大都使用線上音樂服
務，例如 Napster 等。然而目前的線上音樂服
務皆是以分類選單或是以關鍵字方式提供使
用者查詢音樂，使用者必須輸入曲名或歌手名
中內含字詞。此種操作模式顯然無法達到上述
「以音樂片段輸入找歌」的需求，對於不知該
輸入何種關鍵字的使用者而言，現行線上音樂
服務的查詢功能並不夠實用健全。為了彌補關
鍵字查詢音樂的先天限制，本計畫發展一種範
例式(query-by-example)音樂檢索技術，根據使
用者輸入之範例音樂片段(example query)，自
動分析其中的訊號特性，求取資料庫中符合使
用者需求的音樂。 
範例式音樂檢索架構並不限於僅提供特定
歌曲來源的查詢，它同時能允許使用者指定檢
索系統查詢的根據為何。例如使用者可以指定
檢索系統根據歌手來找歌，當輸入一小段音樂
至系統後，告訴系統：「我要找和這段歌曲相
同歌手的其他歌曲」。這種查詢功能的主要目
的是考慮到使用者可能事先並不曉得其輸入
的音樂片段是由誰所演唱，而之所以找歌的動
機也許是因為某首歌好聽，所以想聽和那首歌
相同歌手所唱的其它歌，因此透過範例式音樂
檢索應可有效地滿足此項使用者需求。此外，
根據歌手來找歌的方式也可以輔助特定歌曲
來源的查詢，特別是當使用者所欲查詢的歌曲
是資料庫中尚未收錄的新歌時，系統仍可告知
使用者該範例歌曲片段的演唱者可能是誰。 
另一方面，使用者也可要求檢索系統根據
曲風(genre)來找歌，告訴系統：「我要找和這段
歌曲相同演奏風格的歌曲」。此項功能主要是
考慮使用者可能並無法確切地說出某一首歌
的曲風，或是某些歌曲屬於多種曲風的混合，
例如「帶有重金屬曲風的藍調」、或「有點鄉
村曲風的搖滾」等。透過範例式音樂檢索可免
除使用者需要對曲風做定性的描述，因此對於
一般大眾而言應非常需要這種使用模式。 
 
2. 音樂聲紋辨識 
如圖 2-1 所示為本計畫之音樂聲紋辨識系統架
構，其運作中包含兩個階段：訓練(training)與
測試(testing)。訓練階段之任務在於建立每一首
歌曲之聲紋，我們將聲紋表示為參數統計模
型。這階段包括兩項模組，一為特徵參數擷
取，將雜亂的聲音波形轉成較利於分析的向量
序列；另一為統計模型的建立，本計畫採用高
斯 混 合 模 型 (Gaussian Mixture Model, 
GMM)[1][4]。另一方面，測試階段之工作在於
辨識使用者之音樂片段為資料庫中的第幾首
歌。該階段也包括一項特徵擷取模組，將聲音
波形轉成向量序列後，計算該序列相對於各模
型的似然率(likelihood)，然後依最大似然率法
則[1][4]決定所屬歌曲。其中各模組說明如下。 
 
最大似然率
決策法則
特徵值
擷取
特徵值
擷取
特徵值
擷取
高斯混合
模型化
高斯混合
模型化
高斯混合
模型化
特徵值
擷取
未知的音
樂片段X
音樂
文件1
音樂
文件2
音樂
文件L
所屬音樂文件 S
訓練階段 測試階段
…
…
…
)λ|(maxarg ll PS X=
1λ 2λ Lλ…
 
圖 2-1 音樂聲紋辨識系統架構圖 
 
2.1 特徵參數擷取 
本計畫採用以音框為基礎(frame-based)之特徵
參數。我們測試幾種已知的聲學特徵參數
[1][4]，並提出一種更為有效的特徵參數。首
NSC 96-2221-E-027 -097 -MY3 
 
2.3 結果與討論 
2.3.1資料庫與工作目標 
本計畫採用五張流行音樂專輯，來當作音樂聲
紋辨識的資料。其中包含一張原住民歌手合
輯、一張粵語歌手合輯、一張國語歌手合輯，
以及兩張包含各類型輕音樂的合輯，每張專輯
內含 12~16 首歌曲不等，總共 68 首歌，以求
能達到在適當大小的資料庫內，有最多元的流
行音樂類型選擇。我們利用Windows作業系統
所提供的Media Player軟體將專輯光碟中以無
失真的 wav格式轉載存至電腦中，作為本次實
驗標準無失真的音樂資料。再從每首歌曲中截
取出 10 秒鐘做為測試資料。另外為避免從頭
開始截取的內容都是前奏音樂，我們截取每首
歌的第 70~80秒來使用。 
 
2.3.2實驗條件 
有別於現階段音樂聲紋辨識相關研究[3]-[8]均
是以原聲 CD訊號作為比對的參考樣本，我們
進行了各種不同訊號失真版之參考樣本與測
試樣本的交互比對實驗，例如以較低取樣率之
音樂檔作為訓練模型的參考樣本，來測試較高
取樣率之音樂片段。然而，如圖 2a所示，由於
各種類訊號失真版本的比對配對有太多的可
能性，我們先考慮單一種失真源之若干變化下
的測試，例如只考慮不同取樣率之參考樣本與
測試樣本的交互比對，如圖 2b。然後固定以某
種取樣率之資料進行另一種訊號失真情況下
的效能評估。另外，為簡單化音樂資料的處
理，本計畫中的實驗統一採用單聲道 16bit 的
wav音樂檔格式做測試，並採用 16個混合高斯
來建立高斯混合模型。 
 
2.3.3實驗結果分析 
表 2-1 整理出從 8kHz到 22kHz的各種常見取
樣率配對作測試，其中測試資料的取樣率若與
參考樣本的取樣率不同時，先將測試資料進行
升取樣(upsampling)或降取樣(downsampling)使
之與參考樣本的取樣率一致。然而可預期的是
訊號經由升或降取樣之後必定引起失真。 
    我們可以從表 2-1 中發現，當測試資料之
取樣率與訓練資料之取樣率相同時，辨識率均
可達到完美的程度。但當測試資料之取樣率與
訓練資料之取樣率不同時，辨認率則明顯地衰
退，尤其當測試資料之取樣率低於訓練資料之
取樣率時，辨識效能更急遽地衰退。這顯示了
訊號由低取樣率升至高取樣率後並無法減低
其與高取樣率之參考樣本的不匹配情況
(mismatch)。另外我們發現當參考樣本為低取 
 
 
圖 2-2a交互測試圖 
NSC 96-2221-E-027 -097 -MY3 
 
表 2-4 通道失真分析實驗 
模
型 
資 
料 
測試資料 特徵值辨識率(%) 
通道
種類 
失真
率(%) 
MF-
CC SC RE 
MSC-
CC 
無 
失 
真 
無 
失真 
0 100 100 100 100 
高通
濾波 
8 2.9 76.5 2.9 75 
帶限
濾波 
30 1.5 26.5 1.5 32.4 
低通
濾波 
65 2.9 22.1 4.4 25 
帶通
濾波 
90 1.5 2.9 1.5 2.9 
 
表 2-5 回音失真分析實驗 
模
型
資
料 
測試資料 特徵值辨識率(%) 
回音 
種類 
位
移 
(sec) 
MFCC SC RE MSC-CC 
無 
失 
真 
無失真 100 100 100 100 
無 5 100 100 100 100 
Default 0.5 100 100 100 100 
Reverb 0.1 100 100 100 100 
Tunnel 
(綜合雜訊失
真) 
82.4 66.2 79.4 75 
 
表 2-5 為回音失真的實驗結果。我們可看
出單純的回音失真對四種特徵參數所獲得的
結果並無差異。但若回音失真的同時又參雜了
雜訊失真等其他類型的失真時，則由表中最後
一列中可看出 MFCC 對於雜訊失真的強健性
略勝 SC和 RE，而本計畫提出的MSCCC，其
辨識率則是介於 SC與 MFCC之間。表 2-6顯
示播放速度變化所造成之失真分析結果。我們
可發現各種特徵參數在此失真情況下並不會
有辨識率衰退的情況。最後，我們考慮實際的
使用情況，將資料庫的 68 個歌曲片段，經過
電腦連接高品質的揚聲器播放，並以 PDA 重
新錄音成 8kHz的 wav檔，藉此討論錄音時可
能伴隨的各樣失真情況。本實驗共錄製四次，
每次的音量大小不同，對錄音檔所造成的爆音
(因量化飽和)程度也不同。表 2-7 中爆音率的
定義，是將錄音檔中所有爆音的取樣點數，除
上全部的取樣點數。另一方面，我們嘗試將含
有爆音的音框去除，不輸入模型的似然率計
算。實驗結果顯示，本計畫中提出的 MSCCC
其總體表現優於其他三種特徵值。但將含有爆
音的音框去除的方法，並無法改善辨識率。 
 
表 2-6 變速失真分析實驗 
模 
型 
資 
料 
測試資料 特徵值辨識率(%) 
速度 MFCC SC RE MSC-CC 
無 
失 
真 
無失真 100 100 100 100 
加快一倍 100 100 100 100 
減慢 
為 80% 100 100 100 100 
 
表 2-7 錄音檔爆音失真分析 
模
型
資
料 
測試資料 特徵值辨識率(%) 
取樣
點爆
音率 
(%) 
處 
理 
MFCC SC RE MSC-CC 
無 
失 
真 
無失真 100 100 100 100 
0 
無 
33.8 42.6 41.2 54.4
0.0875 70.6 58.8 60.3 67.6
0.8845 52.9 51.5 58.8 66.2
3.3110 33.8 42.6 44.1 55.9
0.0875
去
爆
音 
69.1 57.4 60.3 66.2
0.8845 47.1 51.5 52.9 64.7
3.3110 38.2 33.8 30.9 36.8
表 2-8 代表性的失真情況 
No. 失真種類 失真程度 
0 無 無 
1 MP3壓縮 壓縮比(75~120 kbps) 
2 MP3壓縮 壓縮比(55~70 Kbps) 
3 MP3壓縮 壓縮比(40~50 Kbps) 
4 White雜訊 30dB SNR 
5 White雜訊 10dB SNR 
6 通道失真 高通濾波(失真率 8%) 
7 通道失真 帶限濾波(失真率 30%) 
8 Reverb回音 回音位移 0.5秒 
9 Tunnel回音 綜合雜訊失真 
10 變速失真 速度加倍,音高不變 
11
錄音檔 
(有爆音) 
爆音率 0.0875% 
(爆音點數/所有點數) 
12
錄音檔 
(有爆音) 
爆音率 0.8845% 
(爆音點數/所有點數) 
13
錄音檔 
(有爆音) 
爆音率 3.3110% 
(爆音點數/所有點數) 
14
錄音檔 
(無爆音) 
無爆音, 
但相對 SNR較高 
15 取樣率失真 16000 HzÆ8000 Hz 
16 取樣率失真 11050 HzÆ8000 Hz 
NSC 96-2221-E-027 -097 -MY3 
 
用某一位歌手之所有可取得的歌曲所訓練產
生。當系統在輸入端收到一個未知待測音樂片
段之 MFCCs X時，利用最大似然率決測法可
判斷其歌手為 I*： 
)λ|Pr(maxarg
1
*
i
Ni
I X
≤≤
=   (3-2) 
 
 
   訓練 
----------------------------------------------------------- 
   測試 
 
 
 
 
 
 
 
 
 
 
 
 
圖 3-3「獨唱歌手辨識」模組 
 
 
3.3 重唱歌手辨識 
「重唱歌手辨識」模組與上述的「獨唱歌手辨
識」模組相似，除了將單人 GMMS 取代成雙
人 GMMS 外，其他皆為相同。不過，這些雙
人GMMS的訓練方式並不像單人GMMS的訓
練方式那麼直接，由於要事先取得所有歌手之
兩兩合唱的歌聲資料並不切實際。因此，我們
發展兩種方式來產生雙人 GMMS，第一種方法
是直接將獨唱歌聲波形相互混合在一起，如圖
3-4 所示。在訓練方面，混合歌聲是將任兩位
歌手演唱聲音波形以近乎相同能量的方式疊
加而成的。我們將疊加後的波形經過特徵化轉
成 MFCCs後，即可建立雙人 GMMs。假如有
N個可能的獨唱歌手，則我們將產生 C2
N = N! / 
[2!(N−2)!]個雙人 GMMs λi,j, i ≠ j, 1 ≤ i, j ≤ N。
在測試方面，當一個未知音樂片段轉為MFCCs 
X後，我們計算其相對於 C2
N
個 GMMs的似然
率，並利用最大似然率決測法判斷其內含兩歌
手為(I*, J*)： 
).λ|Pr(maxarg),( ,
,,1
**
ji
jiNji
JI X
≠≤≤
=   (3-3) 
然而，上述直接波形混合法有一缺點，就
是當有系統欲包含大量的歌手或是需要增加新
的歌手時，訓練過程就會變得很麻煩。為了解
決此問題，我們另外提出一種平行化模型結合
法(Parallel Model Combination，PMC) [10]。如
圖五所示，假設系統含有 N 個歌手之單人
GMMs，則我們可直接利用這些 GMMs兩兩結
合來產生 C2
N
個雙人 GMMs。由於重唱歌聲訊
號是各歌手歌聲在時域/頻域上的疊加，但是
GMMs是屬於倒頻譜域/倒頻率域的參數，所以
欲結合兩 GMMs 之前必須將 GMMs 的參數轉
至頻率域。另外，考慮到兩個具有混合數為 K
之 GMMs 結合後將會變成一具有 K×K 個混合
數的超大GMM，我們因此使用UBM-MAP [11]
方法來訓練單人 GMMs，以控制結合後的
GMM混合數維持為 K個。其過程是藉由最大
事後機率 (Maximum A Posterior，MAP)估算法
將λs調整成每個單人歌手 GMM，因調整後之
所有單人 GMM 的混合順序(mixture index)一
致，所以當我們欲結合第 i個單人 GMM與第 j
個單人 GMM時，只須考慮 K個混合情況，例
如第 k個混合的期望值以及共變異數結合如下: 
μ i , kj = D{log{ exp(D-1μ i k) + exp(D-1μ j k) }}, 
 (3-4) 
Σi, kj = D{log{exp[D-1Σik (D-1)′]  
+ exp[D-1Σjk(D-1)′]}},  (3-5) 
其中，μi (k) 以及Σi (k) 分別為 GMMs λi的期望值
向量以及共變異數矩陣；D表示為離散餘旋轉
換矩陣；(′)表示為轉置符號。 
 
 
 
 
 
 
   訓練 
----------------------------------------------------------- 
   測試 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
圖3-4 基於直接波形混合法之「重唱歌手辨識」 
特徵抽取 高斯混合模型 
未知的音樂片段 
 
 
 
 
特徵抽取 
似然率計算 
似然率計算 
2,1λ
似然率計算 
3,1λ
... 
NN ,1λ −  
ML 
決策器 
最匹配歌手 
第 i個歌手音樂片段 
第 j個歌手音樂片段 
ji ,λ  
特徵抽取 高斯混合模型 
未知的音樂片段 
 
 
 
 
特徵抽取 
似然率計算 
似然率計算 
1λ
似然率計算 
2λ
... 
Nλ
ML 
決策器 
最匹配歌手 
第 i個歌手 
音樂片段 
iλ  
NSC 96-2221-E-027 -097 -MY3 
 
而言，辨識率 2會比辨識率 1來得高。 
表 3-3為直接波形混合法的 OSID結果，
我們考慮 4 種情況：i)訓練與測試資料皆為
STSL重唱歌曲片段；ii)訓練為 STSL重唱歌曲
片段，測試為 DTDL 重唱歌曲片段；iii) 訓練
為 DTDL重唱歌曲片段，測試為 STSL重唱歌
曲片段；iv) 訓練與測試皆為 DTDL 重唱歌曲
片段。我們從表 3可看出使用 STSL重唱歌曲
片段作為訓練資料之效能比起DTDL的情況來
得好；相似地，測試資料若為 STSL 重唱歌曲
片段比起 DTDL的情況也較好。當訓練與測試
資料皆為 STSL 重唱歌曲片段時，我們得到最
好的 OSID結果。 
 
表 3-3.基於直接波形混合法的 OSID結果 
(a) 訓練與測試資料皆為 STSL重唱歌曲片段 
GMM混合數 辨識率 1 辨識率 2 
16 80.7% 90.1% 
32 84.3% 92.1% 
64 85.0% 92.5% 
(b) 訓練資料為 STSL重唱歌曲片段，測試資
料為   DTDL重唱歌曲片段 
GMM混合數 辨識率 1 辨識率 2 
16 67.9% 83.7% 
32 69.8% 84.8% 
64 73.6% 86.7% 
(c) 訓練資料為 DTDL 重唱歌曲片段，測試資
料為 STSL重唱歌曲片段 
GMM混合數 辨識率 1 辨識率 2 
16 77.3% 88.7% 
32 78.4% 89.3% 
64 80.7% 90.4% 
(d) 訓練與測試資料皆為 DTDL重唱歌曲片段 
GMM混合數 辨識率 1 辨識率 2 
16 52.3% 75.8% 
32 47.1% 73.4% 
64 43.6% 71.6% 
 
 
表 3- 4. 基於平行化模型結合法的 OSID結果 
(a) 測試資料為 STSL重唱歌曲片段 
GMM混合數 辨識率 1 辨識率 2 
16 75.1% 87.1% 
32 75.1% 87.3% 
64 78.1% 88.7% 
(b) 測試資料為 DTDL重唱歌曲片段 
GMM混合數 辨識率 1 辨識率 2 
16 71.1% 85.0% 
32 69.9% 85.0% 
64 75.3% 87.6% 
表 3-4 是改用平行化模型結合法的 OSID
結果。在此實驗中，因訓練過程不需使用重唱
歌曲片段。我們考慮兩種測試情況：i)測試資
料為 STSL 重唱歌曲片段，ii) 測試資料為
DTDL 重唱歌曲片段。我們可發現表 4 與表 3
有相似的趨勢，即當測試資料為 STSL 其結果
總是較 DTDL來得好。另外，我們也可發現，
當測試 STSL 重唱歌曲片段時，直接波形混合
法的效能較平行化模型結合法來得好；但當測
試 DTDL重唱歌曲片段時，平行化模型結合法
的辨認率則比直接波形混合法好。 
 
3.3.5 同時考慮獨唱與重唱歌手辨識實驗 
最後我們考慮若測試資料中可能為獨唱或二
重唱的情形。這裡共測試了 150個獨唱歌手歌
曲片段、675個 STSL重唱歌曲片段以及 4725
個 DTDL重唱歌曲片段。辨識率量測法與 Sec. 
3.4 相同，但注意若所測試的歌曲片段僅含單
一歌手 s1，而系統辨識結果為歌手 s1與 s4時，
則「正確辨識出兩重唱歌手的歌曲片段數目」
為 0，而「正確辨識出內含歌手數目」為 1。
這裡我們根據以上實驗經驗設定 GMM的混合
數為 64，結果獲得辨識率 1為 76.2%，辨識率
2為 88.1%。證實本計畫所提出方法之可行性。 
 
4. 非督導式音樂曲風分類 
本計畫的非督導式曲風分類概念如圖 1所示，
假設現有一群未知曲風的歌曲資料庫，我們將
此資料庫分成若干群歌曲，目標是每一群中內
含相同曲風的歌；但不同群代表不同曲風。分
類的主要步驟如圖 4-2 分為四步。第一步將一
群未知的數位音樂檔案擷取出各自的特徵參
數，第二步我們將每個音樂檔案的特徵參數表
示成統計模型，第三步把音樂檔案的特徵參數
和統計模型進行交互測試，以求出彼此的相似
度，最後執行分群(clustering)以達到非督導式
音樂曲風分類的結果。理想上，系統仍須包括
一個自動決定群數的步驟，亦即所產生的群數
應等於實際曲風數目，但此問題較為困難，因
此本計畫暫不考慮處理此問題，僅探討在給定
最後群數條件下的分群方法。 
 
圖 4-1. 基於曲風之音樂分群概念。 
NSC 96-2221-E-027 -097 -MY3 
 
Pr(Oi|λj)將較 Pr(Oi|λm)為大，因此我們可據此表
示歌曲間的曲風相似性。然而，上述似然率大
小情況並非一定成立，且常有例外的情形發
生。為了增加相似性量測的可靠度，我們發展
了三種量測方式：交相似率(Cross Likelihood 
Ratio, CLR) [11]、歐基理德距離 (Euclidean 
Distance) 以 及 餘 弦 角 距 離 (Cosine angle 
distance)。 
CLR的主要算法如下： 
( ) ( )( ) ( )( ) ( )( )
( )( ) ( )( )
1, log | λ log | λ
2
1 log | λ log | λ
2
i j i j
i i
j i
j j
D P P
P P
⎡ ⎤= −⎣ ⎦
⎡ ⎤+ −⎣ ⎦
O O O O
O O　   　　　　
  
   (4-3) 
其中 O(i)為第 i個音樂檔案的特徵向量，λi為第
i 個音樂檔案的 GMM 模型。由此我們可看出
CLR是考慮兩歌曲模型交互測試的結果，關於
式子(1)更細節的部分介紹可以參考[7]。第二種
量測方式是先將任一歌曲(例如 Oi)測試所有模
型 λ1, λ2,…, λN 所獲得的對數似然率 log 
Pr(Oi|λ1), log Pr(Oi|λ1), …, logPr(Oi|λN)組合成
一向量 
( )
( )
( )
,
λ|Prlog
λ|Prlog
λ|Prlog
2
1
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
=
Ni
i
i
i
O
O
O
V M
   (4-4) 
則我們將可發現若第 i 首歌曲與第 j 首歌曲屬
相同曲風，而第 i首歌曲與第 m首歌曲屬不同
曲風，則 Vi與 Vj的距離將較 Vi與 Vj的距離為
小，其中距離的計算方式可為歐基理德距離或
餘弦角距離。歐基理德距離的算法為 
|| Vi − Vj||， 
相當於 
( ) ( )( ) ( )( ) ( )( ) 2
1
, log | λ log | λ
N
i j i j
k k
k
D P P
=
⎡ ⎤= −⎣ ⎦∑O O O O   
(4-5) 
其中 O(i)為第 i個音樂檔案的特徵向量，λk為第
k個音樂檔案的 GMM模型，而 N為音樂檔案
的數量。餘弦角距離的算法為 
||||||||
,
ji
ji
VV
VV
⋅
><
， 
相當於 
( ) ( )( )
( )( ) ( )( )
( )( ) ( )( )
1
2 2
1 1
log | λ *log | λ
, 1
log | λ * log | λ
N
i j
k k
i j k
N N
i j
k k
k k
P P
D
P P
=
= =
⎡ ⎤⎣ ⎦= −
∑
∑ ∑
O O
O O
O O
 
(4-6) 
其中 O(i)為第 i個音樂檔案的特徵向量，λk為第
k個音樂檔案的 GMM模型，而 N為音樂檔案
的數量。由於文中要作曲風分類的音樂檔案共
有 N個，藉由式子(3)(5)(6)可以分別得到維度N 
x N的矩陣 D。 
 
4.4 根據相似性分群 
有了歌曲間的相似性關係，我們便能將相似曲
風的歌曲歸為一群，不相似曲風的歌曲分為不
同群。分群的方法有很多種，本文所採用的方
式 為 階 層 式 聚 合 分 群 (Hierarchical 
Agglomerative Clustering, HAC)[12]，其主要結
構是以樹狀來表示，資料是由樹狀結構的底部
向上方聚合。首先將 N個檔案視為 N個群組，
再由彼此距離最近的兩個群組開始合併，一直
到我們所指定的收斂數量才停止合併，如圖 4。 
 
 
 
圖4-4. HAC示意圖。 
 
然而我們先前所計算的是歌曲間的相似性關
係，對於群與群之間相似性的關係必須在每次
聚合後求出，其計算方式採完全鏈結法
(complete linkage)，意義是由兩群內最不像的
兩歌曲之相似度作為這兩群的相似度。 
 
4.5 結果與討論 
4.5.1資料庫與分群評估方式 
實驗中所採用的音樂資料庫為MIRIX 2005中
Magnatune [13] 資料集，並從 4種明確的曲風-
古典(classical)、爵士(Jazz)、搖滾與流行(Rock & 
Pop)以及世界(World) 中隨機挑出各 20首、長
度介於 3~6分鐘的音樂作為實驗對象。因此總
共有 80 首歌曲。由於原先取得的音樂資料庫
為 44.1KHz、128Kbps之雙聲道MP3，為方便
處理，我們將其轉成為 22.05KHz 單聲道的
WAV檔。 
我們以「純度」(Purity)[14]與「芮氏指標」
(Rand Index)[15]作為分群的效能估算指標。純
NSC 96-2221-E-027 -097 -MY3 
 
慮系統的強健性，並加大歌手的數目及考慮歌
曲的多樣性，如不同音樂類型、曲風以及語言
等。 
在曲風辨識方面，我們延伸了傳統的督導
式曲風分類至非督導式的曲風分類，將一群歌
曲進行自動分群，藉以組織與管理未經人工標
示的歌曲。此技術也可進一步應用在音樂內容
分析(music content analysis)與音樂資料探勘
(music data mining)等研究主題中。綜合比較來
看，本計畫的非督導式音樂曲風分類效能雖然
不如督導式音樂曲風分類，但其結果仍具有一
定的指標，未來還有四個方向值得我們去研
究：第一是特徵參數的開發，我們需要更多能
呈現音樂內容的特徵擷取演算法，系統也能因
此將曲風分類得更為精細。第二是相似性的量
測方法，嘗試更好的交相似性以及距離估算公
式以提升分群的正確率。第三是非督導式的分
群法，將來可以嘗試將最鄰近 K 點(K-Nearest 
Neighbor, K-NN)與 SVM等技術結合到非督導
式的分群系統中。第四是開發自動決定群數方
法，以求更切合實際應用。 
 
參考文獻 
[1] Huang, X., Acero, A., and Hon, H. W., Spoken 
Language Processing, Prentice Hall, 2001. 
[2] Cano, P., Batle, E., Kalker, T., and Haitsma, J., 
“A Review of Algorithms for Audio 
Fingerprinting,” IEEE Workshop on Multimedia 
Signal Processing, pp. 169-173, 2002. 
[3] Haitsma, J. and Kalker, T., “A Highly Robust 
Audio Fingerprinting System,” Proc. 3rd Int. 
Symp. Music Information Retrieval, pp. 144-148, 
2002. 
[4] Ramalingam, A. and Krishnan, S., “Gaussian 
Mixture Modeling of Short-Time Fourier 
Transform Features for Audio Fingerprinting,” 
IEEE Transactions on Information Forensics 
and Security, 2006. 
[5] Sukittanon, S. and Atlas, L., “Modulation 
Frequency Features for Audio Fingerprinting,” 
Proc. IEEE Int. Conf. Acoustics, Speech, and 
Signal Processing, Vol. 2, pp. 1773-1776, 2002.  
[6] Lu, C. S., “Audio Fingerprinting Based on 
Analyzing Time-Frequency Localization of 
Signals,” Proc. IEEE Workshop on Multimedia 
Signal Processing, pp. 174-177, 2002. 
[7] Herre, J., Hellmuth, O., and Cremer, M., 
“Scalable Robust Audio Fingerprinting Using 
MPEG-7 Content Description,” IEEE Workshop 
on Multimedia Signal Processing, pp. 165-168, 
2002. 
[8] Burges, C.J.C., Platt, J.C., and Jana, S., 
“Distortion Discriminant Analysis for Audio 
Fingerprinting,” IEEE on Speech and Audio 
Processing, Vol. 11, No. 3, pp. 165-174, 2003. 
[9] Kim, Y., and Whitman, B. “Singer identification 
in popular music recordings using voice coding 
features”, Proc. ISMIR, 2002. 
[10] Fujihara, H., Kitahara, T., Goto, M., Komatani, 
K., Ogata, T., and Okuno, H. “Singer 
identi-fication based on accompaniment sound 
reduction and reliable frame selection”, Proc. 
ISMIR, 2005. 
[11] Mesaros, A., Virtanen, T., and Klapuri, A. 
“Singer identification in polyphonic music using 
vocal separation and pattern recognition 
methods”, Proc. ISMIR, 2007. 
[12] Tsai, W., and Wang, H. “Automatic detection 
and tracking of target singer in multi-singer 
music recordings”, Proc. ICASSP, 2004. 
[13] Shriberg, E., Stolcke, A., and Baron, D. 
“Observations on overlap: findings and 
implications for automatic processing of 
multi-party conversation”, Proc. Eurospeech, 
2001. 
[14] Yamamoto, K., Asano, F., Yamada, T., and 
Kitawaki, N. “Detection of overlapping speech 
in meetings using support vector machines and 
support vector regression”, IEICE Transactions 
on Fundamentals of Electronics, 
Communications and Computer Sciences, 89(8): 
2158-2165, 2006. 
[15] Ozerov, A., Philippe, P., Gribonval, R., and 
Bimbot, F. “One microphone singing voice 
separating using source-adapted models”, Proc. 
WASPAA (IEEE Workshop on Applications of 
Signal Processing to Audio and Acoustics), 
2005. 
[16] Lee, Y., and Wang, D. “Singing voice separation 
form monaural recordings”, Proc. ISMIR, 2006. 
[17] Hua, X., Lu, L., and Zhang, H. “Personalized 
karaoke”, Proc. ACM Multimedia, 2004. 
[18] Reynolds, D. and Rose, R. “Robust 
text-independent speaker identification using 
Gaussian mixture speaker models”, IEEE 
Transactions on Speech and Audio Processing, 
3(1): 72-83, 1995. 
[19] Gales, M. and Young, S. “Robust continuous 
speech recognition using parallel model 
combination”, IEEE Transactions on Speech and 
Audio Processing, 4(5): 352 - 359, 1996. 
[20] Reynolds, D., Quatieri, T., and Dunn, R. 
“Speaker verification using adapted Gaussian 
mixture models”, Digital Signal Processing, 10: 
19-41, 2000. 
[21] S.B. Davis, and P. Mermelstein, “Comparison of 
parametric representations for monosyllabic 
word recognition in continuously spoken 
sentences,” IEEE Transactions on Acoustics 
NSC 96-2221-E-027 -097 -MY3 
 
表 2-10 SC失真測試實驗 
 
測試資料之失真總類編號 
平均辨 
識率(%) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 
參 
考 
樣 
本 
之 
失 
真 
種 
類 
編 
號 
0 100 100 99 85 100 22 76 26 100 66 100 59 59 51 43 100 100 76 
1 100 100 99 85 100 21 75 26 100 74 100 59 57 47 43 100 100 76 
2 72 72 100 90 68 1 34 10 62 29 69 66 63 56 59 72 72 59 
3 56 56 97 100 54 3 29 6 53 25 56 65 69 62 60 56 56 53 
4 100 100 97 81 100 24 76 26 100 71 100 50 51 44 37 100 100 74 
5 22 22 12 4 25 100 29 3 24 28 19 7 7 4 4 22 22 21 
6 72 72 54 44 72 22 100 7 72 54 69 43 37 31 25 72 72 54 
7 10 10 6 4 9 3 3 100 10 6 10 3 3 3 3 10 10 12 
8 100 100 94 75 100 18 75 28 100 82 100 57 57 47 38 100 100 75 
9 76 76 47 38 76 25 51 22 87 100 71 29 32 28 24 76 76 55 
10 100 100 93 81 100 21 75 25 100 72 100 56 51 43 37 100 100 74 
11 46 46 76 78 37 3 28 1 37 13 37 100 100 100 99 46 46 52 
12 38 38 69 75 35 4 22 4 31 13 34 100 100 100 91 38 38 49 
13 21 21 43 53 18 1 10 3 19 6 15 93 100 100 63 21 21 36 
14 40 40 71 71 37 1 25 3 34 18 41 100 97 93 100 40 40 50 
15 100 100 100 85 100 21 76 28 100 69 100 59 56 51 41 100 100 76 
16 100 100 99 84 100 25 76 31 100 68 100 57 53 51 41 100 100 76 
表 2-11 RE失真測試實驗 
 
測試資料之失真總類編號 
平均辨 
識率(%) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 
參 
考 
樣 
本 
之 
失 
真 
種 
類 
編 
號 
0 100 100 94 3 99 29 3 1 100 79 100 60 59 44 41 100 100 65 
1 97 97 91 3 96 28 3 3 97 76 97 57 56 34 37 97 97 63 
2 94 94 100 10 90 18 3 1 94 54 94 68 65 46 53 94 94 63 
3 3 3 15 100 1 1 1 1 3 3 3 1 1 1 1 3 3 9 
4 97 97 85 3 100 32 1 1 97 76 97 56 53 29 44 97 97 63 
5 44 44 19 1 53 100 0 1 40 31 44 3 1 1 7 44 44 28 
6 4 4 4 1 4 3 100 1 4 4 4 6 4 4 4 4 4 10 
7 4 4 4 3 4 1 4 100 4 3 4 4 4 4 4 4 4 10 
8 100 100 93 6 99 34 3 3 100 88 100 65 62 37 46 100 100 67 
9 91 91 62 6 91 28 3 3 93 100 88 32 35 13 25 91 91 56 
10 100 100 93 3 99 31 3 1 100 78 100 57 56 37 37 100 100 64 
11 66 63 63 4 53 7 1 1 57 24 56 100 100 93 99 66 66 54 
12 56 56 59 3 49 10 1 1 50 26 54 100 100 100 88 56 56 51 
13 26 26 41 3 18 4 3 3 26 16 22 84 99 100 51 26 26 34 
14 44 44 50 4 47 9 1 1 41 25 40 99 93 59 100 44 44 44 
15 100 100 94 3 99 28 3 1 100 81 100 60 60 37 43 100 100 65 
16 100 100 94 3 99 24 3 1 100 79 100 63 62 35 41 100 100 65 
 
NSC 96-2221-E-027 -097 -MY3 
 
 
  
(a) 採用了CLR距離的分群純度圖    (b) 採用了CLR距離的芮氏指標結果圖 
 
  
(c) 採用了歐基理德距離的分群純度圖     (d) 採用了歐基理德距離的芮氏指標結果圖 
 
  
(e) 採用了餘弦角距離的分群純度圖   (f) 採用了餘弦角距離的芮氏指標結果圖 
    
 
 
 
圖 4-6. 純度對分群數量以及芮氏指標對分群數量的結果圖 
 
 
 
否存在網站非法開放未受權樂曲檔案供人下載的有效工具。為了達成歌手身份
的判定，首要任務是從樂曲中擷取歌手的聲音特徵。然而，由於大多數歌曲皆
伴隨著背景音樂，嘗試利用歌手的清唱曲來求得其發聲統計特性並不可行。針
對此一問題，本論文提出一種清唱訊號之估算與參數模型化架構，先藉由偵測
並統計非歌唱區段的純音樂訊號以模擬歌唱區段之伴奏的聲音特性，再考慮歌
唱聲音與伴奏間的混合關係，由歌唱區段中萃取歌手的清唱訊號並以參數模型
進行特徵化，最後以最大似然率決策法判定未知樂曲之歌手身份。本論文報告
時間為 7月 21日下午 1點至 2點半，以 poster方式發表。 
 
二、 與會心得 
ICME10主要議程為學術論文發表，因此會議無不匯集眾多相關領域之世界
知名學者專家在此提出最新研究成果及互相討論、交換研究心得。參與此一研
討會有助於從事多媒體與資訊處理研究者掌握正確方向。 
在近幾年參與國際研討會時可感受到與會人士中的東方面孔以大陸地區學
者居多，且似乎有逐步增加的趨勢。其原因除了大陸地區研究單位迅速增長外，
對於補助與鼓勵研究人員出席國際會議的策略也是促使其日益活躍國際學術舞
台的主要因素，對於此點我方宜加以重視。 
另外，本次會議的場地各項設施相當完善，政府應該加緊建設以爭取各項
國際會議在國內舉行。此舉不僅有助於提昇國內學術研究水準，更提供最佳的
國民外交機會，對提昇台灣的國際形象將有很大的助益。 
96年度專題研究計畫研究成果彙整表 
計畫主持人：蔡偉和 計畫編號：96-2221-E-027-097-MY3 
計畫名稱：範例式音樂檢索技術研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 2 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 15 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 5 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
