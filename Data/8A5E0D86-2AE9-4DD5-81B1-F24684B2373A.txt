行政院國家科學委員會補助專題研究計畫 
■ 成 果 報 告   
□期中進度報告 
 
真實世界人臉辨識之研究 
 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC 99－2221－E－003－027－ 
執行期間：99年 8 月 1 日至 100 年 9 月 30 日 
 
執行機構及系所：國立臺灣師範大學資訊工程學系 
 
計畫主持人：葉梅珍 
計畫參與人員：碩士班研究生-兼任助理人員：戴志傑 
碩士班研究生-兼任助理人員：鄭宇辰 
碩士班研究生-兼任助理人員：曾銘琦 
 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：本計劃可公開查詢 
 
中   華   民   國  100 年 12 月 15 日 
3 
 
報告內容 
前言 
Due to the need for robust face recognition technologies for consumers to tag digital photos and to facilitate 
their organization and online sharing, research on face recognition has had a revival in recent years. For 
example, online face recognition systems have recently been launched, such as Picasa Web Albums [36]. 
However, face recognition in candid photos suffers from uncontrolled lighting conditions, large variation in 
facial expressions, poses, and several partial occlusions. In spite of the vast amount of research conducted on 
this subject, the task of recognizing faces among known people, especially in uncontrolled environments, is 
still unresolved [35]. Face recognition in well-controlled environments is relatively mature and has been 
heavily studied, but face recognition in uncontrolled environments is still in its early stages. In this research 
project, we aim to address the real-world face recognition problem. 
研究目的 
In a traditional face recognition system, a number of labeled faces, referred to as the gallery face set, are first 
compiled; a new image, or so-called a probe image, is then compared against the gallery face set to be 
recognized as a known face or rejected. To apply the technology on tagging faces with names in digital photos, 
a face detector is first applied to locate face areas [2][4][10][12][13][16][18][19]. This formulation—the input 
of face recognition is the outcome from face detection—creates additional new, specific challenges. First, 
names associated with a few training images initially provided by users are relatively limited. Thus, it’s 
necessary to mark some faces as unknown. These unknown faces might come from people whom we don’t 
know (e.g. pedestrians or strangers in pictures taken during vacation) or from known people whose image we 
have not yet labeled to train the system. Second, the localized face from the face detector could be a false 
positive (i.e. not a human face). Finally, a person’s photo collection grows incrementally over time, so is the 
group of identities of interest. Therefore, the system should offer the flexibility for adding new names based 
on new, additional labeled images. The goals of a designing a face recognition system for uncontrolled 
5 
 
pose alignment, background removal, and illumination normalization. The eventual face or signature image to 
which a cascade of transformations has been applied is compared to the query image. In both papers, the 
authors acknowledge that recognition based on facial similarity is sensitive to factors other than identity. 
These factors include lighting conditions, facial expressions, poses, and partial occlusions, all of which are 
present in real-world conditions. The pre-processing steps employed in these approaches are designed to 
overcome these challenges, but unfortunately these methods are computationally expensive.  
It has recently been demonstrated that SIFT features [15] are effective for matching faces. Ozkan et al. [16] 
proposed a method to improve the performance of person queries in large news video collections. In this 
method, a face image is represented by a various number of interest points extracted by the SIFT operator. The 
dissimilarity between two faces is computed based on matching of interest points. Each point in one face is 
compared with each point the in other face and the point-pairs with the least Euclidean distance are selected. 
Moreover, in order to avoid incorrect matches, the geometrical constraint and the unique match constraint are 
applied during matching. The distance between the two faces is defined as the average distance of all 
matching points between them. Bicego et al. [6] investigate the use of SIFT features for face authentication. 
They implemented a number of matching methods and concluded that matching along a regular grid provides 
the best results, while using the minimum pair distance gives the worst results. The above mentioned studies 
basically follow the match-based principle. That is, two face images are considered similar if many of their 
descriptors are similar. Because one image could have a greater density of descriptors and thus more points 
for comparison, the match-based approached could be neither effective nor efficient in real applications. 
More recently, Hua and Akbarzadeh present an elastic partial matching metric based on a part based face 
representation [11]. A large number of local descriptors are densely extracted from a face area. Next, a 
distance metric is defined as the n-th smallest value in the list of all distances between local descriptor pairs. 
This part-based approach has shown its capability in handling variations in pose, facial expression and partial 
occlusion. However, the computation of local features and matching two sets of features are demanding, and it 
is not clear how this approach performs when applied to a large dataset. Another part-based approach 
proposed by Chu et al. describes a ―visual sentences‖ face representation that can be used to compare the 
similarity between two faces [8]. Each face image is divided into three regions by a ratio of 3:2:3 vertically, 
7 
 
classification confidence, or those labeled by users, could provide feedback to improve both the detection and 
the learning modules. In this study, we focus on the learning algorithm and the connection between detection 
and learning. The analysis of the feedback mechanisms needs further researches and will not be discussed in 
this report. 
Based on the result of labeling clusters of faces from a user, we readily build models for those people who 
have some samples available for training the system. Training samples may be few at the early training stage, 
but will increase over time. The learning method should be able to update the models incrementally and be 
scalable to handle large amount of data. Furthermore, given a number of unlabeled images/clusters, it would 
be interesting to determine their order in terms of discriminative information. This order can be used to decide 
which cluster/image should be labeled earlier in order to correctly tag more face images with less efforts. 
In this study, we explored the whitened Fisher Linear Discriminant (WFLD) [33], which applies the 
Fukunaga-Koontz Transform (FKT) to the LDA problem. Based on the eigen-value ratio of FKT, the whole 
data space is decomposed into four subspaces, where two of them are significant for pattern classification and 
representation. We now present in details the WFLD learning method, and the reasons why it is suitable for 
our target application. 
We denote the between-class scatter matrix, the with-in class scatter matrix, and the total scatter matrix as Sb, 
Sw and St, respectively. Note that St = Sb + Sw. Given the data matrix X, we try to find a linear transformation 
matrix Φ that transforms the original high-dimensional data to a low-dimensional subspace by maximizing 
the Fisher Criterion: 
)}.(){()( 1   b
T
w
T
F SStraceJ        (1) 
 
Figure 1. The proposed face recognition framework 
9 
 
where the similarity function f is computed using a cosine measure. We recognize the test image with the with 
the target identity vector. Moreover, it is able to handle fast classification. We need only C (the number of 
classes) comparisons, comparing with other methods which require N (the number of training samples) 
comparisons. This method is much more efficient in terms of both computational time and space. 
The subspace spanned by V3, those columns in V corresponding to 1w  and 0b , is termed Variation 
Space. In this subspace, all class means project to zero and the within-class variations of each class lie in 
orthogonal subspaces [33][34]. This subspace enables lossless reconstruction by retaining the variation 
information. For each data point, we have achieved a clean decomposition into its identity and variation 
components. This decomposition provides an efficient data representation, and it combines the strengths of 
PCA (which is suitable for representation but not classification) and FLD (which is meant for classification, 
not representation). 
Finally, the linear WFLD can employ a kernel function and can be extended to a nonlinear version. Samples 
 
Figure 2. The kernel FFKT algorithm for constructing the identify space given training samples. 
11 
 
8. In particular, FFKT produces over 99% in classification accuracy even given 3 training samples per class; 
however, KLDA can only produce 88 percent. Moreover, FFKT methods are more stable than other methods, 
evidenced by comparing the standard deviation, as shown in Table 1. More precisely, given 8 samples per 
class, FFKT produces 0.04% in standard deviation; whereas, KLDA produces 4.33%. FFKT performs 
accurate and stable classification by using Identity Space, where samples from one class project onto one 
point. It keeps the most discriminant information and greatly reduces the uncertainty in the projected space. 
Finally, we observe that Kernel-FFKT is comparable to FFKT in terms of recognition accuracy. For example, 
given more than 5 training samples per class, both FFKT and Kernel-FFKT even achieves nearly 100% 
recognition rate. 
As for FERET dataset, we adopt the local features developed in our previous work [27] to tackle the pose and 
expression variations. Each face descriptor is a 1600 dimensional vector. Table 2 shows that our FFKT 
presents the best classification rate with least standard deviation. In particular, given only 3 training samples 
per class, FFKT achieves about 5 percent higher in accuracy than the next highest, KLDA. This shows that 
FFKT can handle pose and expression as well as illumination. This suggests that when designing a 
classification system, SVM may not be superior to other subspace-based methods, especially when very few 
training samples are available. This has been evidenced by the experiments on PIE and FERET datasets. 
Table I. Face classification accuracy rate using the PIE dataset 
# training 
samples 
PCA LDA KPCA KLDA SVM FFKT Kernel-FFKT 
3 64.13 (9.78) 87.41 (11.0) 63.19 (9.61) 88.48 (6.56) 75.35 (16.1) 99.32 (1.38) 97.09 (4.15) 
4 69.79 (11.3) 92.71 (5.91) 68.80 (11.1) 91.05 (5.87) 80.89 (15.0) 99.80 (0.82) 98.48 (3.41) 
5 76.42 (12.1) 96.29 (3.79) 75.39 (12.1) 93.80 (5.80) 86.99 (12.6) 99.96 (0.06) 99.49 (1.24) 
6 80.15 (12.4) 97.13 (3.37) 79.08 (12.2) 94.86 (5.74) 90.46 (11.2) 99.97 (0.04) 99.78 (0.74) 
7 86.66 (10.4) 98.38 (2.44) 85.58 (10.4) 96.44 (5.05) 93.72 (8.25) 99.96 (0.06) 99.94 (0.36) 
8 87.14 (9.23) 98.89 (2.21) 86.11 (9.26) 97.70 (4.33) 95.51 (7.17) 99.98 (0.04) 99.99 (0.06) 
 
Table II. Face classification accuracy rate using the FERET dataset with 3 samples per class 
PCA LDA KPCA KLDA SVM FFKT Kernel-FFKT 
71.23 (4.60) 80.73 (5.97) 71.20 (4.57) 92.63 (2.56) 83.30 (3.89) 97.62 (2.10) 97.45 (2.33) 
13 
 
參考文獻 
[1] D. Anguelov, K. Lee, S. B. Gokturk and B. Sumengen. Contextual identity recognition in personal 
photo albums. In Proceedings of the IEEE International Conference on Computer Vision and 
Pattern Recognition, pp. 1-7, 2007. 
[2] O. Arandjelovic and A. Zisserman. Automatic face recognition for film character retrieval in 
feature-length films. In Proceedings of the IEEE International Conference on Computer Vision and 
Pattern Recognition, pp. 860-867, 2005. 
[3] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces vs. fisherfaces: recognition using class 
specific linear projection. PAMI, 19(7), pp. 711-720, 1997. 
[4] T. L. Berg, A. C. Berg, J. Edwards, and D. A. Forsyth. Who’s in the picture? In Neural Information 
Processing Systems, pp. 137-144, 2004.  
[5] T. L. Berg, A. C. Berg, J. Edwards, M. Maire, R. White, Y. Teh, E. Learned-Miller, and D. A. 
Forsyth. Names and faces in the news. In Proceedings of the IEEE Conference on Computer Vision 
and Pattern Recognition, pages 848-854, 2004. 
[6] M. Bicego, A. Lagorio, E. Grosso, and M. Tistarelli. On the use of sift features for face 
authentication. In Computer Vision and Pattern Recognition Workshop, 2006. 
[7] C.-C Chang and C.-J. Lin. LIBSVM: a library for support vector machines. 2001. 
[8] Y. Cheng and M. Yeh. A Study on Constructing a Representative Photo Stream from Multiple 
Sequences of Travel Photos. In Proceedings of the IPPR Conference on Computer Vision, Graphics, 
and Image Processing, 2011. 
[9] W. -T. Chu, Y. -L. Lee and J. -Y. Yu. Visual language model for face clustering in consumer photos. 
In Proceedings of ACM International Conference on Multimedia, 2009. 
[10] M. Guillaumin, T.Mensink, J. Verbeek, and C.Schmid. Automatic face naming with caption-based 
supervision. In Proceedings of the IEEE International Conference on Computer Vision and Pattern 
Recognition, pp. 1-8, 2008. 
[11] G. Hua and A. Akbarzadeh. A robust elastic and partial matching metric for face recognition. In 
Proceedings of the IEEE International Conference on Computer Vision, 2009. 
[12] G. B. Huang, M.Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: adatabase for 
studying face recognition in unconstrained environments. University of Massachusetts, Amherst, 
Technical Report 07-49, 2007. 
[13] V. Jain, E.Learned-Miller, and A. McCallum. People-LDA: Anchoring Topics to People using Face 
Recognition. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1-8, 
2007. 
[14] A. Kapoor, G. Hua, A. Akbarzadeh and S. Baker. Which faces to tag: adding prior constraints into 
active learning. In Proceedings of the IEEE International Conference on Computer Vision, 2009. 
[15] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2), pp. 91-110, 
2004. 
[16] D. Ozkan and P. Duygulu. A graph based approach for naming faces in news photos. In Proceedings 
15 
 
Recognition, 2009. 
[35] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld. Face recognition: a literature survey. ACM 
Computing Surveys, 35(4), pp.399-458, 2003. 
[36] http://picasaweb.google.com/ 
 
 2 
MPEG. The core technology is a construction of the ray space, which enables the 
rendering of infinite views given a limited (say 150) views captured by a camera array. I 
asked a question on the camera arrangement strategy, and the talk delivers a great tutorial 
on this innovative technology. 
 The first two days have a similar schedule—three oral sessions and one poster session. 
This year PSIVT accepted in total 71 papers out of 168 submissions, where 31 of the 
accepted papers are oral. The acceptance rate of 42% (overall) and 18% (oral paper) 
indicate the commitment to ensure a very high-quality symposium.  I met several 
well-known researchers in the conference, for example, Prof. Klette in the University of 
Auckland, Prof. Ho in Gwangju Institute of Science and Tecnhology, and Prof. Satoh in 
National Institute of Informatics. I also met several research teams from Taiwan. For 
example, Prof. Shang-Hong Lai from National Tsing Hua University had four papers 
accepted by the symposium this year. I had some interesting discussions with Prof. 
Chun-Hung Chen (NTHU), Wen-Nung Lie (NCCU), Pao-Chi Chang (NCU), Prof. 
Guo-Shiang Lin (DYU), and Prof. Fay Huang (NIU).  
 The symposium organized the paper presentations into several important subjects in 
image and video technology. Steering directions include motion analysis, computer vision, 
segmentation and recognition, and image/video coding. One emerging field this year is 
biomedical image processing and analysis. There are five papers—all accepted as oral 
papers—covering topics such as brain axon fiber estimation, CT-MR image registration, 
and sub-cortical structure modeling. The symposium ended with an interesting keynote 
speech on compressed sensing theory by Prof. Heung-No Lee from GIST, where Prof. Lee 
gave a one-hour overview of the mathematical theory and emerging applications of 
Compressed Sensing.  
 Overall, I had a fruitful technical experience and a pleasant stay during the PSIVT 2011 
period. The next PSIVT will take place in Mexico, November 2013. 
二、與會心得 
日期: Wed, 31 Aug 2011 00:15:16 -0700
寄件者: Conference Management Toolkit <cmt@microsoft.com>
回給: eklchan@ntu.edu.sg
收件者: <myeh@csie.ntnu.edu.tw>
主旨: PSIVT2011 : Paper 104 Decision
Dear Mei-Chen Yeh,
PSIVT2011 paper review process is now complete. We are pleased to inform you the outcome
of the reviews.
The status of paper 104 - A Hierarchical Approach to Practical Beverage Package Recognition
is Accept
Please login to the paper submission system to view the comments on your paper. We will
contact you again regarding camera ready submission soon.
Thank you for submitting your work to PSIVT2011.
Best Regards
Program Chairs
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/21
國科會補助計畫
計畫名稱: 真實世界人臉辨識之研究
計畫主持人: 葉梅珍
計畫編號: 99-2221-E-003-027- 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
