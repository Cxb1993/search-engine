1 Introduction
Grid computing is an important mechanism for utilizing computing resources that are distributed in
different geographical locations, but are organized to provide an integrated service. A grid system provides
computing resources that enable users in different locations to utilize the CPU cycles of remote sites. In
addition, users can access important data that is only available in certain locations, without the overheads
of replicating it locally. These services are provided by an integrated grid service platform, which helps
users access the resources easily and effectively. One class of grid computing, and the focus of this paper,
is Data Grids, which provide geographically distributed storage resources for complex computational
problems that require the evaluation and management of large amounts of data. For example, scientists
working in the field of bioinformatics may need to access human genome databases in different remote
locations. These databases hold tremendous amounts of data, so the cost of maintaining a local copy at
each site that needs the data would be prohibitive. In addition, such databases are usually read-only,
since they contain the input data for various applications, such as benchmarking, identification, and
classification. With the high latency of the wide-area networks that underlie most Grid systems, and
the need to access/manage several petabytes of data in Grid environments, data availability and access
optimization have become key challenges that must be addressed.
An important technique that speeds up data access in Data Grid systems is to replicate the data in
multiple locations so that a user can access the data from a server in his vicinity. It has been shown that
data replication not only reduces access costs, but also increases data availability in many applications
[7, 13, 12]. Although a substantial amount of work has been done on data replication in Grid environments,
most of it has focused on infrastructures for replication and mechanisms for creating/deleting replicas [2,
5, 4, 6, 12, 14, 13, 15]. We believe that, to obtain maximum benefits from replication, a strategic placement
of replicas is essential.
Although there has been much work on replica placement problem [10, 11, 17, 18, 20], very few of
them concerns quality of service. A large part of these work concerns the average system performance,
for example, to minimize the total accessing cost, or to minimize the total communication cost, etc.
Although these metrics are important in the overall system performance, they cannot meet the individual
requirement adequately. Grid computing infrastructure usually consists of various type of resources and
the performance of these resources are quite diverse. Moreover, different sites may have different service
quality requirements according to the system performance of the sites. Therefore, quality of service is an
important factor in addition to overall system performance.
An early work by Tang and Xu [16] considered the quality of service in addition to minimize the
storage and update cost. The distance between two nodes is used as a metric for quality assurance.
A request must be answered by a server within the distance specified by the request. Every request
knows the nearest server that has the replica and the request takes the shortest path to reach the server.
Their goal has been to find a replica placement that satisfies all requests without violating any range
constraint, and minimize the update and storage cost at the same time. They show that this QoS-aware
replica placement problem is NP-Complete for general graphs, and provide two heuristic algorithms –
l-Greedy-Insert and l-Greedy-Delete, for general graphs. A dynamic programming solution is given
for tree topology [16].
In this paper, we study the QoS-aware replica placement problem for general graphs; moreover,
we take the workload capacity limit of each replica server and the access cost of each data request into
consideration. When a data request is dispatched to a overloaded server, it will not get a response in time.
Therefore, we believe that quality of service and workload capacity should be considered simultaneously
for quality assurance. In addition, the data access cost has a profound influence on the overall system
performance, therefore the access cost must be taken into account to improve system performance. In
our model, each request must be serviced by a replica server within its quality requirement and without
violating the capacity limits of the replica server. We provide two heuristic algorithms to decide the
positions of the replicas to minimize the sum of update, storage and access costs, and satisfy the quality
requirements specified by the user and the capacity limit that each replica server can service. Our
algorithm computes near-optimal solutions efficiently, so that it can be deployed in various realistic
environments.
1
lem [8]. The model did not consider update cost and assumed that each server has identical storage cost.
The goal was to minimize the number of replicas in the system. They gave a proof of NP-Completeness
for this problem, which is a variation of set covering. Let A be the all-to-all shortest path matrix and
entry (i, j) of A denotes the shortest path distance between server i and server j. Let B be another
matrix and the entries in the i-th row indicate quality of service requirement of server i. We then con-
struct another matrix C according to A−B. If an entry of A− B is less than or equals to 0, we set set
the corresponding entry of C to 1. Otherwise, we set the entry to 0. The non-zero elements of the j-th
column of C represents the servers that are covered by server j. If we find a set of columns that cover
every row in matrix C, we find a replica placement that satisfies all requests within quality of service
requirement.
Won, Indranil and Klara proposed a simpler and quicker algorithm to find a reasonable good solution
for this problem. Every iteration in the algorithm, they select the column j (server j) that covers most
rows that not yet covered so far. This Greedy MSC (Greedy Minimum Set Covering) is compared with
our methods in our simulation testing.
Our model differs from the model in [16] because it considers not only site construction, update
overheads, quality of service, both the capacity constraint and the access costs. The capacity constraint
of each replica server is an important factor in the requsets response time, and the access cost of each
request has a great influence on system performance. We believe that these two factors should be taken
into consideration, along with site construction, update overheads, and quality of service. In this paper,
we propose two heuristic algorithms to find near optimal solutions, while all the constraints, including
capacity constraint and access costs, are considered.
3 System Model
This section describes our system model. The network is represented by an undirected graph G = (V,E),
where V is the set of servers, and E ⊆ V × V denotes the set of network links among the servers. Each
link (u, v) ∈ E is associated with a cost d(u, v) that denotes the communication cost of the link. We
assume that the graph is connected, so that one server can connect to any other server via a path. We
define the communication cost of a path as the sum of the communication cost of the links along the
path. Because we assume that a server knows where to find the replica that services it, we define d(u, v)
between two servers u, v to be the communication cost of the shortest path between them. Every server u
has a storage cost, S(u), that denotes the cost to put a replica on server u. The storage cost on different
nodes may be different. Figure 1 is an example of our model. The numbers in the circles are server
indices between 0 and n− 1, where n is the total number of servers. The number next to a server is its
storage cost. The number on a link is the communication cost of the link.
     
     
     
     




Origin Server
Replicated Server
    
    
    
    




     
     
     
     




     
     
     
     




(10)
(6)
6
3 4
5
1
7
02
7
6
7
4
5
8
5
(7)
(9)
(6)
(3)
(10)
(10)
(5) (7)
(3)
(8) (12)
Figure 1: An example of data replication in connected network.
Each server in the network services multiple clients, although we do not place clients into the network
graph. A client sends its requests to its associated server, then the server processes the requests. If the
client’s requests can be served by the server, i.e., the local server has the requested data, the requests will
be processed locally. Otherwise, the requests will be directed to a server that has the replica. As a result,
we assume that all requests are issued from the servers and there are only servers in the network graph.
In addition, because the communication cost from the clients to servers does not affect the replication
decision, we ignore the communication cost from clients to servers.
There is a special server r, called origin server, in the network graph. Without lose of generality, we
assume that server 0 is the origin server. Initially only the origin server has the data. A replica server is
3
the replica server u can handle. The origin server also has its workload capacity constraint C(r). The
workload and workload capacity constraint on different server may be different. If the total workload
that a server u ∈ R∪{r} services is greater than its capacity constraint, i.e.,
∑
v∈SS(u)W(v) > C(u), the
server u is overloaded.
3.4 QoS-aware Replica Placement With Capacity Constraint
A replication strategy is feasible if all servers are satisfied, and none of the server u ∈ R∪{r} is overloaded.
The problem of QoS-aware replica placement with capacity constraint is to find a feasible replication server
set R and determine the service set function SS(u) for each server u ∈ R∪ {r}, such that the replciation
cost in Equation (1) is minimized.
4 Heuristic Algorithms
In this section we first propose two heuristic algorithms – Greedy-Remove and Greedy-Add for QoS-
aware replica placement with capacity constraint problem. Then, we analyze the time complexity of these
two algorithms.
4.1 QoS Satisfying Set
We define a QoS satisfying set SAT (u) of a server u to be the set of servers from which u is located
within their QoS distance Q. That means should u become a available replica server, it is able to satisfy
those nodes in SAT (u). Formally we have Equation 5.
SAT (u) = {v|d(u, v) ≤ Q(v)} (5)
Each server has its own QoS satisfying set. If there is a replica on server u, each server v ∈ SAT (u)
may be satisfied by u, if u will not be overloaded by doing so . For a feasible replication strategy,
the service set of each server u ∈ R ∪ {r} must be a subset of its QoS satisfying set SAT (u), that is,
SS(u) ⊆ SAT (u) for all u ∈ R ∪ {r}.
4.2 Greedy Remove
The algorithm Greedy-Remove starts with having a replica on every server. This replication strategy is
feasible since every server can serve itself locally so any QoS constraint is satisfied. Therefore the service
set of each server u ∈ R ∪ {r} has only itself. Greedy-Remove then repeatedly adjusts the service
sets SS of a pair of replica servers and try to remove replicas in order to reduce the replication cost
(Equation (1)). While removing replicas, Greedy-Remove must simultaneously maintain the feasibility
of the replication strategy.
We now describe our Greedy-Remove method in details. The Greedy-Remove method works in
rounds. Initially we put a replica on every server in V − {r}, so the replication server set R is V − {r}.
For each iteration Greedy-Remove examines each pair of servers u, v in R∪{r} and computes the cost
reduction function R(u, v). Then Greedy-Remove selects the maximum R(u, v) and adjust the service
sets of u and v accordingly. Greedy-Remove repeats this process until it is impossible to reduce the
total replication cost.
4.2.1 Time Complexity Analysis
We now analyze the time complexity of Greedy-Remove. The first part of the costs is a preprocessing
to find a shortest path between any two servers. That is, we must build all-pair shortest path to check if
one server is within the QoS requirement of another. This takes O(|V |3) time to calculate.
Given the servers u and v, it takes O(|V |) to find out the cost reduction of moving all servers from v
to u for the first case. For the second case it takes O(|V |log|V |) time to sort the servers, then examine
them one by one and move them from u to v if its distance to v is longer than the distance to u.
We first compute the cost reduction functionR of each pair of replica servers. This takesO(|V |3log|V |)
time.
In each iteration, we choose the largest cost reduction R. If the corresponding case is the first case,
we need to consider two situations. The first is that there is no replica in the subtree rooted at v after
5
5 Conclusion
Data replication is an important technique to speed up data access in Data Grid. Grid computing
infrastructure usually consists of various type of resources and the performance of these resources are
quite diverse. So to provide quality assurance for different data access requirements is more and more
important. This replica placement problem become more complex when the storage for replica on servers
is limited. We believe that quality of service and workload capacity should be considered simultaneously
for quality assurance, and the access cost must also be taken into account for system performance.
In this paper, we consider QoS requirement, workload capacity restriction and access cost on replica
placement problems. We believe that all the key issuers, including storage cost, quality of service, server
capacity constraint, access costs, and update costs should be consider. Our proposed algorithms consider
all these key issues, and is very simple and easy to adapt to variant environments. Experiment results
indicate that Greedy-Remove and Greedy-Add can find near-optimal solutions in all parameter
combinations.
Acknowledge The authors thank the support from National Science Council, under grant NSC 95-
2221-E-002-071.
References
[1] GT Internetwork Topology Models (GT-ITM), 2000. http://www-
static.cc.gatech.edu/projects/gtitm/.
[2] A. Chervenak, R. Schuler, C. Kesselman, S. Koranda, and B. Moe. Wide area data replication for
scientific collaborations. In In Proceedings of the 6th International Workshop on Grid Computing,
November 2005.
[3] Israel Cidon, Shay Kutten, and Ran Soffer. Optimal allocation of electronic content. In INFOCOM,
pages 1773–1780, 2001.
[4] W. B. David. Evaluation of an economy-based file replication strategy for a data grid. In International
Workshop on Agent based Cluster and Grid Computing, pages 120–126, 2003.
[5] W. B. David, D. G. Cameron, L. Capozza, A. P. Millar, K. Stocklinger, and F Zini. Simulation of
dynamic grid rdeplication strategies in optorsim. In In Proceedings of 3rd Intl IEEE Workshop on
Grid Computing, pages 46–57, 2002.
[6] M.M. Deris, Abawajy J.H., and H.M. Suzuri. An efficient replicated data access approach for large-
scale distributed systems. In IEEE International Symposium on Cluster Computing and the Grid,
April 2004.
[7] W. Hoschek, F. J. Janez, A. Samar, H. Stockinger, and K. Stockinger. Data management in an
international data grid project. In In Proceedings of GRID Workshop, pages 77–90, 2000.
[8] Won J. Jeon, Indranil Gupta, and Klara Nahrstedt. Qos-aware object replication in overlay networks.
2005.
[9] Xiaohua Jia, Deying Li, Xiao-Dong Hu, and Ding-Zhu Du. Placement of read-write web proxies in
the internet. In ICDCS, pages 687–690, 2001.
[10] Konstantinos Kalpakis, Koustuv Dasgupta, and Ouri Wolfson. Optimal placement of replicas in
trees with read, write, and storage costs. IEEE Trans. Parallel Distrib. Syst., 12(6):628–637, 2001.
[11] Christof Krick, Harald Ra¨cke, and Matthias Westermann. Approximation algorithms for data man-
agement in networks. In SPAA ’01: Proceedings of the thirteenth annual ACM symposium on Parallel
algorithms and architectures, pages 237–246, New York, NY, USA, 2001. ACM Press.
[12] H. Lamehamedi, B. Szymanski, Z. Shentu, and E. Deelman. Data replication strategies in grid
environments. In In Proceedings of 5th International Conference on Algorithms and Architecture for
Parallel Processing, pages 378–383, 2002.
7
