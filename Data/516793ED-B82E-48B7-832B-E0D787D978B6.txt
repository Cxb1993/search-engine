 1 
行 政 院 國 家 科 學 委 員 會 專 題 研 究 計 畫 成 果 報 告 
以視覺為基礎之全天候車輛偵測系統設計及實現 
Vision-Based All-Weather Preceding Vehicle Detection System 
計劃編號：NSC 98-2221-E-167-018  
執行期間：98年 08月 01 日至 99年 10月 31日 
計 畫 主持人：郭英哲 國立勤益科技大學電機系 E-MAIL: kuoyc@ncut.edu.tw 
 
一、中文摘要 
本計畫所提出的全日車輛影像偵測系統
主要應用在先進駕駛者輔助系統(Advanced 
Driver Assistance Systems, ADAS)功能中的前
方車輛追撞預警(Forward Collision Warning, 
FCW)。考量了車內的空間、攜帶性以及商品
化的可行性，使用 INTEL XScale PXA270 
SOC-Based 嵌入式開發平台實作此系統。所
使用到的硬體周邊設備包含(1) USB 網路攝影
機：影像擷取設備；(2)觸控式螢幕：使用者
操作與顯示之介面；(3)音源輸出用喇叭：語
音警告之輸出設備。所使用之軟體包含
Embedded Linux 2.6.15.3 作業系統、網路攝影
機 的 讀 取 與 控 制 用 之 Video for Linux 
Two(V4L2)函式庫、觸控式螢幕使用者圖型介
面設計用之 MiniGUI 的函式庫、語音播放用
之 madplay 應用程式。 
本計畫完成之項目包含以可拓理論為基
礎的日間與夜間車輛偵測演算法的使用選擇、
日間與夜間的前方車輛偵測、追蹤、距離估
測、車道線偵測、危險情況時的語音警示與
影像資訊記錄。其中，影像資訊記錄裡除了
影像之外，也使用文字記載了當時的時間、
日期與車輛之間的距離，以利不幸發生事故
時的調查與責任歸屬的釐清。 
本計畫所提出的系統主要實際測詴於高
速公路上，前方與旁側車道之車輛皆能成功
的偵測、標定車輛在影像中的位置以及測得
車輛之間的相對距離。測距的範圍為五至三
十公尺。由實測結果，本系統在日間與夜間
中，同車道車輛辨識率達 96%以上，旁側車
道車輛則達 92%以上。日間情況下能排除地
面標示文字、橋樑接縫、高架橋陰影與行道
樹陰影等干擾。夜間情況中，能排除路燈、
反光號誌、交通燈的干擾及明暗度不均等情
況，並在各種照度下正常的偵測、追蹤車輛。 
 
關鍵詞：先進駕駛者輔助系統、電腦視覺、
車輛偵測、車道偵測、嵌入式系統。 
英文摘要 
The proposed All-Weather vehicle 
detection system is used in Forward Collision 
Warning (FCW) of Advanced Driver Assistance 
Systems (ADAS), it is achieved by image 
processing algorithm based on computer vision 
technologies. The space of car, portability, and 
commercialization are decisive considerations 
when implementing the system. So, we use the 
embedded hardware platform instead of the 
personal computer (PC) though it has good 
abilities of computing and immediacy. 
In hardware, we use Intel XScale PXA270 
SOC-based platform. The peripheral devices 
include: (1) USB Web Camera: the image 
capture device, (2) tough screen TFT-LCD 
display: graphic user interfaces and image 
display, (3) speaker: the speech warning output 
device. The software of the system contain 
Embedded Linux 2.6.15.3 operating system, 
Video for Linux Two open source code for 
driving Web Camera, the MiniGUI open source 
code that is utilized to design GUI in touch 
screen, and madplay open source code for 
processing voice signal.  
The functions of the vehicle detection 
algorithm include lane detection, vehicle 
detection, vehicle tracking, distance estimation, 
audio warning, and image information record. 
Audio alter and image record will be acted 
when the distance between preceding vehicle 
and the test car is inside the safe range. 
Moreover, the record of image also includes 
text information of time, date and vehicle 
distance, and stored in non-volatile storage 
device for the investigation of accident.  
In this work, the proposed system mainly 
tested on the highway, the preceding and 
passing vehicle are able to be detected 
correctly and the effective distance estimation 
 3 
的方法是以 PC 實現，因此較無考量到硬體資
源限制的問題。其中，對稱性運算方法使用
R、G、B 三灰階度共同計算，在嵌入式系統
中將耗費許多運算的時間。在本研究中，嚐
詴以灰階值運算取代 RGB 運算，也能得到不
錯效果。 
文獻[5]中提出了以光流為基礎的方法偵
測前方車輛。在文獻中提出了多種光流的計
算方法，其中以誤差平方加總法 (Sum of 
Squared Differences, SSD)最為常見，主要計算
子影像區域中的值，並找出該區域中 SSD 的
最小值，將兩張影像之間的位移使用向量表
示，該向量即為兩張影像之光流。在動態影
像中光流位移的大小，背景的變化小於車輛
突然出現的變化程度，利用此特性分析光流
之位移找出車輛的概略位置。運用光流法方
式找出前方車輛能夠一同計算出車輛之間的
相對速度，對駕駛者而言獲得此資訊，在避
免事故的發生中無非是多增加了一項參考的
指標。但要計算整張影像的光流場，在嵌入
式系統中同樣的會陷入有限硬體資源的考驗
且抗雜訊能力較差，在安全防護的系統中，
應用光流法偵測車輛之準確度仍是有待改進
之處。 
基於上述可行方法分析，本計畫提出日
間車輛偵測演算法(如圖一)，主要分成偵測、
追蹤車輛兩個部份。在搜尋模式中，本計畫
利用車輛底部邊緣的特徵(車輛腳印)找尋車
輛，將偵測的程序分為(1)縮小搜尋區域與前
處理，主要是為了讓演算法能在硬體資源有
限的嵌入式系統中更有效率的執行。(2)前景
擷取，分離前景與背景取出車輛輪廓、車道
標示線等影像。(3)路面雜訊濾除，把前景中
的不需要的雜訊移除(例：車道標示線)，留下
車輛底部邊緣的影像。(4)標示車輛腳印，利
用車道線偵測區分出前方車輛腳印所在的車
道，並使用投影分析標示腳印位置。(5)車輛
腳印驗證，利用車輛左右對稱的特性對車輛
區域的影像進行驗證。如驗證成功，再使用
一邊界盒將車輛影像框起。不難發現偵測模
式的多程序影像處理會耗費許多系統運算量，
因此在車輛偵測確認後，系統將改以追蹤模
式繼續標示車輛位置，藉以降低整體運算量。
其處理程序詳細說明如下: 
A.1 縮小搜尋區域與前處理 
為了減少系統運算量並提升車輛搜尋的
效率，在搜尋車輛之前本研究先行定義影像
中的感興趣區域(Region of Interest, ROI)，縮
小搜尋車輛的區域。本論文用輸入影像中的
地平線作為 ROI 之上邊界，地平線以上區域
為天空也是非道路之區域，車輛並不會出現
於此處。下邊界則定義在前方約五公尺之處，
除了塞車等情況，前方車輛在正常行駛狀況
下並不會出現在此區域。 
 
 
   圖一、日間車輛偵測演算法執行之程序 
 
由於輸入影像為 RGB 色彩空間易受光源影響，
因此本研究將 ROI 裡 RGB 影像轉換至YCrCb
色彩空間[6]降低對光源的敏感度，並使用
Y(亮度)色系空間進行之後的影像處理程序，
減少影像維度的計算，節省系統之運算量。 
A.2 前景擷取 
在這一步驟裡使用 Sobel [6]邊緣偵測強
化灰階影像中物件的輪廓，並藉由 Otsu’s 
Threshold Selection Method[7]將物件輪廓與
背景分離，取出前景物件。一開始使用 Sobel
邊緣偵測強化水平與垂直紋理。式(1)中 f 為
一個大小為 M × N 的影像，g(x,y)為邊緣運
算的輸出影像，w(s,t)是一大小 m × n 之濾波
器遮罩，其內容為像素權重值。遮罩(Mask)
中心位置隨著 x 與 y 移動，遮罩內像素之索
引位置則由 s 與 t 改變，其中 a=(m-1)/2、
b=(n-1)/2。x=0,1,2,…,M-1、y=0,1,2,…N-1，遮
罩中 m 與 n 之大小需相同且為奇數，能接受
的最小數值為 3。圖二為 Sobel 的水平與垂直
濾波遮罩權重分配。 
 5 
輛影像之對稱軸是否在已標示區域中心，如
果對稱軸的偏移量在預設區域範圍(Sth)內，則
此區域影像則屬車輛影像。本研究將文獻[8]
的 RGB對稱性運算式修改成式(3)之對稱軸運
算，只計算YCrCb中的 Y，以降低系統之運算
量。 
 
𝑆 𝑗 =     𝑝 𝑗 + ∆𝑥, 𝑖 − 𝑝 𝑗 − ∆𝑥, 𝑖  
𝑉𝑅+∆𝑘
𝑗=𝑉𝐿−∆𝑘
𝑊
2
∆𝑥=1
𝑉𝑇
𝑖=𝑉𝐵
 
                                                                                                (3) 
𝐽𝑠𝑦𝑚 = 𝑚𝑖𝑛 𝑆 𝑗 𝑎𝑛𝑑 𝑆 𝑗  <  𝑆𝑡𝑕  
 
其中，VT為車輛上邊界，是由車底往上
推算零點八倍所求得， VB為車輛底邊界，VL為
車輛左邊界，VR為車輛右邊界，這三個邊界
在 3.43 節中就已得知。W 為車輛寬度，由左
右邊界相減求得。∆x為計算用偏移量，其計
算偏移範圍在 1 至 W/2。j 與 i 分別為 X 軸與
Y 軸之索引值，∆k為 j 往左右方向的擴展值。 
圖四為經過對稱運算後顯示對稱中心的
結果，在確認對稱軸位於車輛影像之中心區
域後，用深藍色的邊界盒(Bounding Box)將車
輛邊界標示。 
 
 
圖四、車輛影像對稱軸與邊界盒 
 
A.5 車輛追蹤程序 
在車輛正確偵測後，為降低運算量，使
用光流法(Optical Flow)[5]與絕對值平均誤差
法(Mean Absolute Error, MAE)實現車輛追蹤
之方法。透過偵測小範圍影像區域之變化量，
調整車輛邊界盒的寬與高，藉由此方法降低
系統整體運算量。 
 
B. 夜間車輛偵測演算法 
我們觀察在夜間情況下的影像中，發現
許多在日間情況下可見的車輛特徵都因周圍
的照度不足而消失，像是車輛陰影與直角特
徵等，因此無法使用日間車輛偵測方法搜尋
前方車輛。在夜間車輛偵測中，主要的工作
是在昏暗的環境與動態影像中偵測出前方車
輛。以下將詳細的介紹夜間環境中車輛偵測
之程序(如圖五)。 
 
  圖五、夜間車輛偵測演算法執行之程序 
 
    夜間車輛偵測由六大步驟所組成，分別
是：(1)感興趣區域的定義與色彩空間之轉換，
主要是為了降低系統運算量與減少不必要的
搜尋動作。(2)亮點物件的擷取，把可能為車
尾燈特徵的影像物件擷取，分離前景與背景。
(3)移除雜訊與強化特徵，把強度不足之亮點
物件移除，並強化其餘的亮點物件，使其特
徵更易於偵測。(4)亮點物件資訊處理，將各
個亮點物件影像轉換成獨立影像物件，並計
算每一個被分割亮點物件的相關資訊(如寬、
高等)。(5)框選車輛車尾燈，先分出不同車道
的車輛車尾燈，並將各個車道的車輛車尾燈
進行配對，找出車輛成對的車尾燈。(6)驗證
車輛影像，利用前一張影像與下一張影像配
對的結果進行比對，如比對成功則將此一配
對結果保留，完成驗證。 
B.1 感興趣區域的定義與色彩空間之轉換 
    如同日間偵測演算法，定義一個感興趣
區域(Region of interest, ROI)其範圍與日間偵
測方式相同。如此可以隔離許多干擾因素(如
路燈、交通號誌等)，減少搜尋的範圍，增加
演算法執行的效率。在夜間時，我們可以發
 7 
迴式方法實現此演算法，圖七為影像經由相
鄰元素編號法分割成各個獨立之亮點物件，
並且用不同顏色代表各個亮點物件之編號。 
 
圖七、亮點物件分割之結果 
 
C. 可拓日夜間辨識方法選擇 
本研究使用可拓理論[8][9]進行日間與
夜間天色的辨識，利用影像中天空與道路的
亮度灰階值作為特徵來區分日間與夜間之情
況，其中加入道路的亮度灰階值特徵，增加
取樣的數目，能增加辨識的準確度。本文對
天空與道路區域取平均灰階值進行取樣來建
立物元，圖八為天空與道路區域的平均灰階
值取樣的範圍，藉由縮小取樣範圍可以降低
運算量，取平均值能降低高頻的雜訊干擾(如：
夜晚的路燈號誌、日間的烏雲或遠山等)。 
 
   
(a)                (b) 
圖八、天空與道路亮度灰階值的取樣區域 (a)
夜間 (b)日間       
 
接著介紹可拓日夜間識別的步驟與程
序： 
Step 1: 建立日間與夜間情況下亮度灰階值
的經典域與節域範圍，如式(5)至式 8)，其中R0
內的V0為經典域之範圍，Rp內的Vp為節域之
範圍。本文各自統計一萬筆影像中天空與道
路區域的平均灰階值並計算其變異量，如圖
九與十，藉由這些統計數據資料定義出經典
域之範圍，節域的選定是根據各個經典域之
上下限而定，其選定範圍僅會影響關聯函數
可拓域之範圍。除此之外，我們加入了道路
與天空灰階值相加的特徵，讓決策時有另一
個可參考之因素。 
 
圖九、日間與夜間天空灰階值統計結果 
 
圖十、日間與夜間道路灰階值統計結果 
 
𝑅0 =  𝑃，𝐶，𝑉0 
=  
𝑁𝑖𝑔𝑕𝑡𝑡𝑖𝑚𝑒 𝑆𝑘𝑦 < 60, 100 >
𝑅𝑜𝑎𝑑 < 40, 80 >
𝑆𝑘𝑦_&_𝑅𝑜𝑎𝑑 < 35, 175 >
       (5) 
𝑅𝑃 =  𝑃，𝐶，𝑉𝑃 
=  
𝑁𝑖𝑔𝑕𝑡𝑡𝑖𝑚𝑒 𝑆𝑘𝑦 < 0, 120 >
𝑅𝑜𝑎𝑑 < 0, 120 >
𝑆𝑘𝑦_&_𝑅𝑜𝑎𝑑 < 0, 190 >
         (6) 
𝑅0 =  𝑃，𝐶，𝑉0 
=  
𝐷𝑎𝑦𝑡𝑖𝑚𝑒 𝑆𝑘𝑦 < 110, 235 >
𝑅𝑜𝑎𝑑 < 85, 200 >
𝑆𝑘𝑦_&_𝑅𝑜𝑎𝑑 < 180, 300 >
       (7) 
𝑅𝑝 =  𝑃，𝐶，𝑉𝑃 
=  
𝐷𝑎𝑦𝑡𝑖𝑚𝑒 𝑆𝑘𝑦 < 100, 255 >
𝑅𝑜𝑎𝑑 < 70, 255 >
𝑆𝑘𝑦_&_𝑅𝑜𝑎𝑑 < 170, 350 >
       (8) 
 9 
 
圖十二、日間車輛偵測 
 
 
圖十三、夜間車輛偵測 
 
四、成果自評 
本計畫已完成日夜間車輛偵測影像辨識演算
法，並實作於嵌入式系統，其成果參加教育
部主辦「98年度微電腦腦應用系統設計製作
競賽」，獲得訊號處理與通訊研究所組『佳作』，
參加教育部 SOC聯盟主辦「2010全國大學校
院嵌入式系統創意應用組」，獲得『設計完整
獎』。並以“Vision-based Vehicle Detection in 
the Nighttime” 投 稿 於 2010 年 之 IEEE 
International Symposium on Computer, 
Communication, Control and Automation 國際
研討會 (已刊登 )及以” Vision-based vehicle 
detection for a driver assistance system “投稿
於 Elsevier 期刊 Journal of Computers and 
Mathematics with Applications(已接受)。 
 
參考文獻 
[1] TRI 拓墣產業研究所, “智慧型車輛引爆車用電
子新發展,” TRI 產業專題報告-132, Dec. 2008. 
[2] 台灣交通部統計處, “臺灣地區高速公路交通事
故 概 況 統 計 ,” from 
www.motc.gov.tw/mocwebGIP/wSite/public/Attac
hment/f1199704440845.doc ,2006. 
[3] M. Y. Chern and B. Y. Shyr, “Locating nearby 
vehicles on highway at daytime based on the front 
vision of a moving car,” in Proc. IEEE Conf. on 
Robotics and Automation, pp.2085-2090, Sep. 
2003. 
[4] H. Y. Chang, C. M. Fu, and C. L. Huang, 
“Real-time vision-based preceding vehicle tracking 
and recognition,” in Proc. IEEE Int’l Conf. on 
Intelligent Vehicles Symposium, pp. 514-519, 2005. 
[5] A. Giachetti, M. Campani, and V. Torre, “The Use 
of Optical Flow for Road Navigation,” IEEE Trans. 
on Robotics and Automation, vol. 14, no. 1, pp. 
34-48, 1998. 
[6] R. C. Gonzalez and R. E. Woods, Digital Image 
Processing, Prentice-Hall, Inc., 2002. 
[7] N. Otsu, “A Threshold Selection Method from 
Gray-Level Histograms,” IEEE Trans. on Systems, 
Man, and Cybernetics, vol. SMC-9, no.1, pp. 
62-66, Jan. 1979. 
[8] D. Ye, Y. Q. Yu, and B. Zenq, “Research 
of Classification Based on Extenics,” in 
Proc. IEEE Int’l Conf. on Computational 
Intelligence and Multimedia Applications, 
pp. 129-132, 2007. 
[9] 王孟輝、林庚賢，2006，“利用可拓類神經網路
第一型來作汽車排氣污染遙測攔檢之工具，第一
屆智慧生活科技研討會，No.ICS-R14，台中，民
國九十五年六月。 
 
 
參加演講者 M.S. EL NASCHIE(IEEE fellow)之 keynote speech。
隨後參加 session: computer & Mathematics with Applications
進行論文發表(論文如附件)。27日則參加其他議程及參觀東華大
學。28日結束研討會行程。 
二、與會心得 
   與會學者及業界參加人數眾多，並且有半數來自世界各地，
可見此研討會雖僅舉辦至第三年，但已受到世界各地研究人員重
視。Keynote speaker為 M.S. EL NASCHIE，講題 “Application of 
chaos and fractals in fundamental physics and set 
theoretical resolution of the two-slit experiment and the 
wave collapse”受益良多。參加兩天研討會發表之論文多達 150
篇，除瞭解目前世界產學界成果外，亦參訪主辦學校東華大學，
近年來大陸主辦國際研討會，讓在地教師及學生能有機會參與國
際研討會，不失為培養教師及學生國際觀的好方法，也提升學校
知名度。 
三、考察參觀活動(無是項活動者略) 
四、建議 
五、攜回資料名稱及內容 
  攜回研討會 preceding及上海市東華大學介紹。 
recording will be activated if the distance is nearer than the safe range. A statistic of 
100 video road images are tested in our experiments, the nature of vehicles includes 
sedan, minivan, truck, and bus. The experimental results show that the proportion of 
correct identifications of proceeding vehicles is above 95.8% testing on highways in 
the daytime. Experimental results also indicate that the system correctly identifies 
vehicles in real time.  
Keywords: Advance driver assistance systems (ADAS), Vehicle detection, Optical 
flow. 
 
1. Introduction 
Advanced driver assistance systems (ADAS) have received considerable 
attentions in recent decade, because many car accidents caused mainly of drivers’ 
unawareness or fatigue. To warn the driver of any dangers that may lie ahead on the 
road is important to improve traffic safety and accident prevention. So a major 
function of ADAS is the detection of vehicles in front of our own one by using 
computer vision technologies since the cost of an optical sensor (such as CMOS and 
CCD) is much lower than an active sensor (such as laser or radar). Furthermore, 
optical sensors can be used in a wider range of applications, such as the lane departure 
warning systems and event video recorders.  
Various vehicle detection approaches have been reported in the computer vision 
While implementing the vehicle detection system, the space of car and the cost of 
hardware platform are the decisive considerations. Our work is to propose a refined 
vehicle detection algorithm to make it can be implemented in a cost-effective 
embedded platform. The vehicle detection algorithm includes road area finding, 
features of vehicle extraction, and vehicle verification. A tracking process dedicated in 
the detected vehicle region of image based on optical flow is also applied for reducing 
the complexity of computing. Furthermore, for the estimation of distance, a preceding 
vehicle range finding method based on a geometric perspective has also been 
presented. Voice alert and image recording will be activated if the distance is nearer 
than the safe range. 
2. Vehicle detection algorithm 
In our work, a CMOS camera is mounted interiorly on the windshield of a test 
car and the optical axis is parallel to the road surface. The camera captures the RGB 
color road environment image forward. For the considerations of the realization in an 
embedded system with limited hardware resources, the procedures of our proposed 
refined vehicle detection algorithm are shown in Fig. 1 and mentioned as follows. 
Firstly, the region of interest (ROI) area, the region vehicles possibly appear in the 
image, is defined. The left and right boundaries of ROI are the left and right 
boundaries of the captured image. The top of ROI is the horizon line in the image. The 
applied subsequently to intensify the binary road image. Finally, the land marks and 
interferences can be removed using the binary edge-detected image and the binary 
road image in a subtraction operation, yielding image with no lane marks (NLM), as 
presented in Fig. 2 (b). 
2.2. Range of the lane 
An area in image bound by lane marks is called road area image (RAI) where 
vehicles possibly appear. To verify the RAI area, the lane marks need to be detected in 
advance. In most images, lane marks are at 45 degrees to appear. Therefore, Sobel 
edge filters with ±45 degree gradients in vertical are applied. Once the lane marks are 
enhanced by edge filters, the procedures of lane marks detection are described as 
follows. 
1)  The start point of this detection is at the center in horizontal direction and Yb 
(the bottom of ROI) in vertical direction. 
2) From the start point toward both sides in horizontal direction, find the first 
bright pixels in both sides. 
3) Once the first bright pixels are found both in left and right sides, recording 
the positions of the found pixels.  
4) If the bright pixel cannot be found while the horizontal position has already 
surmounted last position that found in the last finding, using one pixel inward 
VB. Then, the vehicle image block is found with the boundaries. The width of the 
vehicle image block is W=|VR-VL|. The height of the vehicle image block is set as 
H=0.8W. 
2.4. Footprint verification 
In Sec. 2.3, the footprint image may include wet spots, seam of a bridge, and 
shadows of passing vehicles. Therefore, a rule that determines whether these 
footprints are associated with vehicles is required. In this step, the symmetry operation 
is applied to verify the footprints. The symmetry equation is modified [9] using Eq. 
(1). Our approach is to find the most symmetric axis by minimizing the symmetry 
measure S(j) with the symmetry axis at x=j within the vehicle image block. 
th
j
H
SjSandjS 


 


)(min)(minargj
(1)                                                                  | i)  x,Δ - p(j  -  i)  x,Δ  p(j |           S(j)
sym
V
V  i
2W / 
1 x Δ
k Δ  V
kΔ -V  j
∑ ∑ ∑
B
B
R
L
 
p(j, i) denotes the component in vehicle image block. VB, VL and VR are the 
bottom, left and right position of boundaries of the vehicle image block, respectively. 
W and H represent the width and height of the vehicle image block. Finally, the 
vehicle image block can be identified as a vehicle if the minimum symmetry measure 
is less than a certain threshold Sth. The value of Sth depends on the location of vehicle 
at different longitudinal distance in the image.  
the parameters of the camera are utilized to estimate the longitudinal distance to the 
preceding vehicle. We apply the estimation model presented in [13][14] to estimate 
the distance between the preceding vehicle and the test car from the captured image. A 
3D scene point (Xi, Yi, Zi) in the camera coordinate system captured by the camera is 
projected onto the pixel (ui, vi) on the 2D image coordinate as shown in Fig.3. Eq. (3) 
shows the projective phenomenon relation. f is the focal length of the camera. Sv is the 
scaling factor of height (the ratio of the physical height and the image pixel). Su is the 
scaling factor of width (the ratio of the physical width and the image pixel). We set the 
camera’s optical axis parallel to the road surface at a height H above the ground and 
assume a planar road surface. The underneath of the vehicle on the road at a distance 
Zi in front of the camera will project to the image at a height vi.  Then the distance Zi 
can be estimated by Eq. (4). 
)3(,
c
cv
i
c
cu
i
Z
YSf
v
Z
XSf
u   
)4(
i
v
i
v
HSf
Z   
4. Experimental results 
The proposed system was implemented on the INTEL XScale PXA270 
SoC-based (520MHz system clock, 32MB Flash ROM, and 64MB SDRAM) 
embedded hardware platform with a few peripheral devices (e.g., TFT-LCD touch 
screen, Ethernet, AC-97, USB etc.). A CMOS camera connected to the hardware 
resources. The proposed system is successfully implemented on a test car and tested 
on highway No. 1 in Taiwan, and its effectiveness is verified. The system also 
provides distance information for the further function of Adaptive Cruise Control. 
Moreover, voice alerts and image recording will be activated if the distance is nearer 
than the safe range. We hope the proposed system is useful for building the ADAS. 
For further studies, more environmental factors (e.g. presence of strong shadow, under 
harsh weather, the lighting conditions depend on the time of the day, and artificial 
illumination etc.) must be considered to optimize system performance and make the 
system more robust. 
Acknowledgement 
The present work is supported by National Science Council of Taiwan under Grant 
NSC-98-2221-E-167-018. 
References 
[1]  S.S. Huang, C.J. Chen, P.Y. Hsiao., L.C. Fu, On-board vision system for lane 
recognition and front-vehicle detection to enhance driver's awareness, IEEE Int’l 
conf. on Robotics and Automation 3 (2004) 2456 - 2461. 
[2]  Z. Sun, G. Bebis, R. Miller, On-road vehicle detection using Gabor filters and 
support vector machines, IEEE Int’l Conf. on Digital Signal Processing 2 (2002) 
1019-1022. 
[10]  R.C. Gonzalez, R.E. Woods, Digital image processing, Prentice-Hall , 2002. 
[11]  N. Otsu, A threshold selection method from gray-level histograms, IEEE Trans. 
on Systems, Man, and Cybernetics,  SMC-9, (1) (1979) 62 - 66. 
[12]  A. Singh, Optical flow computation: a unified perspective, IEEE Computer 
Society Press, (1992). 
[13]  G. P. Stein, O. Mano and A. Shashua: Vision-based ACC with a Single Camera: 
bounds on range and range rate accuracy, Proc. IEEE Int’l Conf. on Intelligent 
Vehicles Symposium, (2003) 120-125. 
[14]  B.F. Wu, C.J. Chen, C.C. Kao, C.W. Chang, S.T. Chiu, Embedded weather 
adaptive lane and vehicle detection system, IEEE Int’l Symposium on Industrial 
Electronics (2008) 1255-1260. 
 
Fig. 1. Processing flow of vehicle detection. 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
研究成果獲得以下全國性競賽獎項: 
(1)以「全日車輛影像偵測系統」參加教育部主辦 98 年度「全國微電腦應用系
統設計製作競賽」訊號處理與通訊研究所組『佳作』。 
(2)以「全日車輛影像偵測系統」參加教育部主辦 98 年度「全國微電腦應用系
統設計製作競賽」訊號處理與通訊大學組『第三名』。 
(3)以「多功能駕駛者輔助系統」參加教育部 SoC 聯盟主辦「2010 全國大學校
院嵌入式系統設計競賽」創意應用組『設計完整獎』。 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
