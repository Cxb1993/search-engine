Abstract 
We report the research work for investigating the annotation of judicial documents in 
Chinese and applying Bayesian networks for student modeling. This piece of work embarked in 
the year 2005 and will continue toward 2008. We have achieved reasonable results in the first 
year.  
Overview 
In the past many years, we have studied classification techniques for categorizing judicial 
documents in Chinese. The categorization of judicial documents can be useful in practice if we 
can achieve satisfactory accuracy. Although we hope, the actual application of our system may 
not take place in the courts. With our current achievements, we see that we can build a 
Google-like server for judicial consultation. The main difference between our system and Google 
will be that we do not require users to choose and type in key words for search. In a normal 
prosecution procedure, the defendant will receive a prosecution document from the courts. To 
know that the prosecution documents are about, the users just feed the whole file to our system to 
search similar prior documents. Our system can provide prior documents that are similar to the 
current document based on prosecution reasons or legal articles that might be cited for the current 
case. 
In addition to the categorization of judicial documents, we have also attempted to apply 
machine learning-based methods for learning student models. The input data to our learners are 
simulated students’ records for taking tests. We implemented the simulator in the past year, and 
are continuing to improve it. Given students’ test records, our classifier try to tell how students 
learning composite concepts. We have identified some key issues in this research direction, and 
expect to work on them in the continued projects. At this moment, we see that there are chances 
that computers can help educational experts to select detailed models about students’ learning 
patterns. However, this is not a simple work, particularly when students’ test records do not 
deterministically reflect students’ competence.  
Technical skeletons 
Since we have applied very different approaches for the document classification and student 
modeling problems in our research, we have to provide the skeletons separately. 
Classification of judicial documents 
As in the work that we did in the past many years, we applied k nearest neighbor (kNN) 
methods for the classification task. With a preprocessing procedure, we extracted key information 
from the documents and converted them into a set of features. In applying kNNs, we calculated 
the similarity between two documents with the similarity measure defined based on the feature 
sets.  
In the past, we have been using word-based features in our work. A Chinese word is a 
sequence of characters that we segmented from a normal Chinese text. In our research that took 
place between 2004 and 2005, we have applied the introspective learning method to adjust the 
weights for the keywords, hoping to improve the accuracy of our classifiers.  
This year, we switched to phrase-based features. The hunch is that using phrases, consisting 
of two words, should make the phrase more specific in their semantics, and hopefully can 
Proceedings of the Sixth IEEE International Conference on Advanced Learning 
Technologies (ICALT’06), 187-189. Kerkrade, Limburg, Netherlands, 5-7 July 2006. 
(EI?) 
 C.-L. Liu. Learning how students learn with Bayes nets, Lecture Notes in Computer 
Science 4053: Proceedings of the Eighth International Conference on Intelligent 
Tutoring Systems (ITS’06), 772-774. Jhongli, Taiwan, 26-30 June 2006. (SCIE) 
 
In this paper, we explore computational techniques for comparing the candidate 
models for students. Although we do not expect computational techniques will give 
better model structures than human experts will do in the short term, we hope that 
computational techniques can assist human experts to identify more precise models. 
More specifically, we would like to guess how students learn composite concepts. A 
composite concept results from students’ integration of multiple basic concepts. Let 
dABC denote the composite concept that involves three basic concepts cA, cB, and cC. 
How do we know how students learn the composite concept? Do they learn dABC by 
directly integrating the three basic concepts, or do they first integrate cA and cB into 
an intermediate product, say dAB, and then integrate dAB with cC? 
We compare candidate models that are represented with Bayesian networks, based 
on students’ responses to test items. Students’ responses to test items reflect their 
competences in the tested concepts in an indirect and uncertain manner. The relation-
ship is uncertain because students may make inadvertent errors and luckily hit the 
correct answers. We refer to these situations as slip and guess, respectively, hence-
forth. Slip and guess are frequently cited in the literature, and many researchers 
adopted Bayesian networks to capture the uncertainty in their CAT system, e.g., [6, 8, 
10-12].  
As a result, our target problem is an instance of learning Bayesian networks. This 
is not a new research problem, and a good tutorial is already available [13]. However, 
learning Bayesian networks for student modeling is relatively rare, based on our 
knowledge, particularly when we would try to induce a network directly from stu-
dents’ item responses. Vomlel created network structures from students’ data and 
applied principles provided by experts to refine the structures [11]. Besides those 
difficulties for learning structures from data, learning a Bayesian network from stu-
dents’ data is more difficult because most of the variables of interests are not directly 
observable. Hence, the problem involves not just missing values and not just one or 
two hidden variables. 
In our experiments, we have 15 basic and composite concepts. We cannot observe 
whether students are competent in these concepts directly, though we assume that we 
can collect students’ responses to test items that are related to these concepts. The 
problem of determining how students learn composite concepts is equivalent to learn-
ing the structure of the hidden variables given students’ item responses. 
We propose mutual information (MI) [14] based heuristics, and apply the heuris-
tics for predicting the hidden structures in two ways: a direct application and training 
support vector machines (SVMs) [15] for the prediction task. Experimental results 
indicate that it is possible to figure out the hidden structures under moderate uncer-
tainty between students’ item responses and students’ competence.  
We provide more background information in Section 2, introduce the MI-based 
heuristics in Section 3, present the SVM-based method in Section 4, and wrap up this 
paper with a discussion in Section 5. 
2   Preliminaries 
We provide more formal definitions, explain the source of the simulated students’ 
item responses, and analyze the difficulties of the target task in this section. 
student groups and compe-
tence in concepts through a 
simulation parameter: 
groupInfluence. A student’s 
behavior may deviate from 
his/her typical group compe-
tence pattern with a probabil-
ity that is uniformly sampled 
from the range [0, groupInfluence]. 
We assumed that every concept had three test items in the experiments, and the 
network shown in Figure 1 shows a possible network when we consider the problem 
in which there are only three basic concepts. Note that in this network, we assume 
that students learn dABC by directly integrating the three basic concepts, which is 
indicated by the direct links from the basic concepts to the node labeled dABC. We 
cannot show the network for the case in which there are four basic concepts in this 
paper, due to the size of the network. (There will be 15 nodes for concepts, 3×15 
nodes for test items, and a lot more links between these 60 nodes.) 
The probabilities of slip and guess are also controlled by a simulation parameter: 
fuzziness. Students of a student group may deviate from the typical behavior with a 
probability that is uniformly sampled from [0, fuzziness].  
Given the network structure, the Q-matrix, and the simulation parameters, we can 
create simulated students. In our experiments, we assumed that a student can belong 
to any of the 16 student groups with equal probabilities. Following Liu’s strategy, we 
used a random number, ρ that was sampled from [0, 1] to determine whether a stu-
dent would respond to a test item correctly or incorrectly. The conditional probability 
of correctly responding to a test item given a student belonged to a particular group 
can be calculated easily with Bayesian networks. Consider the instance for iA1, a test 
item for cA. If ρ is smaller than )|1Pr( 1ggroupcorrectiA ==  when we simulated a 
student who belonged to the first group, we assumed that this student responded to 
iA1 correctly. Since there were 15 concepts, a record for a simulated student would 
contain the correctness for each of 45 (=3×15) test items.  
After using the networks to create simulated students, we hid the networks from 
our programs that took as input the item responses and guessed the structures of the 
hidden networks. 
2.2   Contents of the Q-Matrix and Problem Complexity 
The contents of the Q-matrix influence the prior distributions of students’ competence 
patterns and the performance of simulated students [12]. Clearly, there can be many 
different ways to set the contents of the matrix. 
We set the Q-matrix in Table 1, partially based on our experience. Notice that all 
columns for the basic concepts and the target concept, dABCD, are 1. This should be 
considered a normal choice. If we do want to learn how students learn dABCD, we 
should try to recruit students who appear to be competent in dABCD to participate in 
our experiments. In addition, there is no good reason to recruit anyone who is not 
competent in any of the basic concepts in the experiments. We set the values for 
dABC, dABD, dACD, and dBCD to 16 possible combinations, and this is why we 
group
cA cB cC
dAB dBC dAC dABC
iA1
iA3
iA2
iC2
iC3
iC1
iB2
iB3
iB1
iAB1 iAB2 iAB3
iABC1 iABC2 iABC3
iAC3iAC2iAC1
iBC3iBC2iBC1
Figure 1. A Bayesian network for 3 basic concepts 
Before we estimate the MI measures, 
we add 0.001 to the number of occur-
rences of every possible combination 
of variables. This will avoid the zero 
probability problems, and is a typical 
smoothing procedure for estimating 
probability values [20]. 
3.2   Experimental Evaluation 
Figure 3 shows the flow of how we 
evaluated the heuristic. In the experi-
ments, we used five different network structures to create simulated students, and 
their main differences are shown in Figure 2. The parent concepts of other composite 
concepts that do not appear in the sub-networks in Figure 2 are the basic concepts. 
For instance, the parent concepts of dABD in the network that used the leftmost sub-
network in the top row of Figure 2 are cA, cB, and cD. As we mentioned in Section 2, 
we mainly used the Q-matrix in Table 1 in our experiments. We set groupInfluence 
and fuzziness to different values in {0.05, 0.1, 0.15, 0.2, 0.25, 0.3}, so there were 36 
combinations. We did not try values larger than 0.3 because they were beyond con-
sideration normally discussed in the literature. For each of the five network structures 
and a combination of groupInfluence and fuzziness, we sampled 600 network in-
stances with the Q-matrix shown in Table 1, and created a different population of 
10000 simulated students for each of these instances. The choice of “10000” was 
arbitrary, and the goal was to make each of the 16 groups include many students.  
An experiment corresponded to a different combination of groupInfluence and 
fuzziness, so there were 36 experiments. We used accuracy to measure the quality of 
our prediction of the hidden structures. It was defined as the percentage of correct 
prediction of 3000 (=5×600) randomly sampled network instances that were used to 
create the simulated students. According to Equation (1), there were 14 possible an-
swers when β is 4. Hence, to guess the hidden structure of each of these 3000 net-
work instances, we calculated the estimated MI measures for 14 possible answers 
from the item responses of the 10000 simulated students.  
Figure 4 summarizes the ex-
perimental results. The vertical 
axis shows the accuracy, the hori-
zontal axis shows the decimal part 
of fuzziness, and the legends mark 
the values of groupInfluence used 
in the experiments. Curves in these 
charts show a general trend that we 
expected. Increasing the values of 
groupInfluence and fuzziness made 
the relationship between students’ 
item responses and their compe-
tence patterns more uncertain and 
our prediction less accurate. When 
Figure 4. Accuracy achieved by the MI-based heuristics 
Q-matrix
Liu's simulator
estimate the MI measures for
14 possible hidden structures
 item responses for
10000 simulated students
predicted structure record and evaluate
actual network
 structure
choose the structure that
has the largest estimated MI
simulator parameters:
groupInfluence and
fuzziness
 
Figure 3. Flow for evaluating the heuristic 
each of these experiments, there were 600 network instances for each of the candidate 
networks shown in Figure 2. Therefore, we used students’ data obtained from 500 
network instances for each of the candidate network as the training data, and used the 
students’ data obtained from the remaining 100 network instances as the test data.  
 Figure 6 summarizes how we pre-
pared the training and test instances. 
In addition to the original 14 esti-
mated MI measures, we also com-
puted ratios between the estimated MI 
measures as features. The introduction 
of ratios was inspired by analyses that 
we discussed at the end of Section 3.2. 
We divided the original 14 estimated 
MI measures by the largest estimated 
MI measure in each training instance. 
This gave us 14 new features. We also 
divided the largest estimated MI measure by the second largest estimated MI measure, 
and divided the largest estimated MI measure by the average of all estimated MI 
measures. This gave us 2 more features, so we used 30 features for each of the 500 
training instances for each of the five candidate networks. The true answers (also 
called class labels) were attached to the instances for both training and testing. In 
summary, we created a training instance from 10000 simulated students, and there 
were 2500 (=5×500) training instances, each with 30 attributes and a class label. 
When testing the trained SVMs, we produced the 16 extra features from the original 
14 estimated MI measures for each of the test instances as well. The true answer was 
attached to the test instance so that we could compare the true and predicted answers, 
but the SVMs did not peek at the true answers. 
4.2   Results 
Charts shown in Figure 7 show the experimental results. The vertical axis, the hori-
zontal axis, and the legend carry the same meanings as those for charts in Figure 4. 
The titles of the charts indicate what types of SVMs we used in the experiments. We 
used the c-SVC type of SVMs in all experiments, and tried three different kernel 
functions, including polynomial (c-
svm-poly), radial basis (c-svm-rb), 
and sigmoid (c-svm-sm) kernels. 
Among these tests, using polyno-
mial and radial basis kernels gave 
almost the same accuracy, and both 
performed better than the sigmoid 
kernel. However, it took a longer 
time for us to train an SVM when 
we used the polynomial kernel.  
Comparing the curves for the 
same experiments in Figures 3 and 
4 show the significant improve-
Figure 7. Accuracy achieved by the SVM-based methods 
Q-matrix
Liu's simulator
estimate the MI measures for
14 possible hidden structures
 item responses for
10000 simulated students
actual network
 structure
simulator parameters:
groupInfluence and
fuzziness
calculate 16 extra features from
the original 14 estimated MIs
14 features
16 features
create an instance that consists of
30 features and the class label  
Figure 6. Preparation of the training and test data 
based methods for selecting the correct hidden sub-structure even when experts’ 
opinions were not available. 
Acknowledgements 
This research was partially supported by contract NSC-94-2213-E-004-008 of the 
National Science Council of Taiwan. We gratefully thank the reviewers for their 
invaluable comments, and will answer their questions that we cannot do so in this 
page-limited paper during the oral presentation. 
References 
1. 1. W. J. van der Linden and C. A. W. Glas, Computerized Adaptive Testing : Theory and Practice, 
Kluwer, Dordrecht, Netherlands, 2000. 
2. H. Wainer et al., Computer Adaptive Testing : A Primer, Lawrence Erlbaum Associates, NJ, USA, 
2000. 
3. C.-C. Liu, P.-H. Don, and C.-M. Tsai, “Assessment based on linkage patterns in concept maps,” J. of 
Information Science and Engineering, vol. 21, pp. 873–890, 2005. 
4. G. Smith, “Does Computer-Adaptive Testing Make the Grade?” ABCNEWS.com, 17 March 2003.  
5. R. J. Mislevy and R. G. Almond, “Graphical models and computerized adaptive testing,” CSE 
Technical Report 434, CRESST/Educational Testing Services,  NJ, USA, 1997. 
6. C.-L. Liu, “Using mutual information for adaptive item comparison and student assessment,” J. of 
Educational Technology & Society, vol. 8, no. 4, pp. 100−119, 2005. 
7. F. B. Baker, Item Response Theory : Paraemter Estimation Techniques, Marcel Dekker, NY, USA, 
1992. 
8. R. J. Mislevy, R. G. Almond, D. Yan, and L. S. Steinberg, “Bayes nets in educational assessment: 
Where do the numbers come from?” in Proc. of the Fifteenth Conf. on Uncertainty in Artificial 
Intelligence, pp. 437−446, 1999. 
9. J. Pearl, Probabilistic Rasoning in Intelligent Systems : Networks of Plausible Inference, Morgan 
Kaufmann, CA, USA, 1988. 
10. E. Millán and J. L. Pérez-de-la-Cruz, “A Bayesian diagnostic algorithm for student modeling and its 
evaluation,” User Modeling and User-Adapted Interaction, vol. 12, no. 2-3, pp. 281−330, 2002. 
11. J. Vomlel, “Bayesian networks in educational testing,” Int. J. of Uncertainty, Fuzziness and 
Knowledge-Based Systems, vol. 12, no.  Supplement 1, pp. 83−100, 2004. 
12. K. VanLehn, Z. Niu, S. Siler, and A. Gertner, “Student modeling from conventional test data : A 
Bayesian approach without priors,” Lecture Notes in Computer Science, vol. 1452, pp. 434−443, 1998. 
13. D. Heckerman, “A tutorial on learning with Bayesin networks,” in M. I. Jordan (ed.), Learning in 
Graphical Models, pp. 301−355, MIT Press, MA, USA, 1999. 
14. T. M. Cover and J. A. Thomas, Elements of Information Theory, John Wiley & Sons, NY, USA, 1991. 
15. C. Cortes and V. Vapnik, “Support-vector network,” Machine Learning, vol. 20, pp. 273−297, 1995. 
16. K. K. Tatsuoka, “Toward an integration of item-response theory and cognitive error diagnoses,” in N. 
Fredericksen et al. (eds.), Diagnostic Monitoring of Skill and Knowledge Acquisition, Erlbaum, NJ, 
USA, 1990. 
17. D. E. Knuth, The Art of Computer Programming: Fundamental Algorithms, Addison-Wesley, MA, 
USA, 1973. 
18. P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and Search, second edition, MIT 
Press, MA, USA, 2000. 
19. Hugin: http://www.hugin.com 
20. I. H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, second 
edition, Morgan Kaufamnn, CA, USA, 2005. 
21. C.-C. Chang and C.-J. Lin, LIBSVM: A library for support vector machines, 2001. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
22. MATLAB: http://www.mathworks.com 
Partially due to our ignorance, we have not been able to identify sufficient work 
that is directly related to indexing Chinese documents with phrases. More commonly, 
people segment Chinese text with the help of a machine readable lexicon, and then 
index the documents with Chinese words. With special techniques for obtaining in-
formation about Chinese words such as Chien’s PAT tree-based approach [4], one 
may segment Chinese text without using lexicons. Instead of going through Chinese 
word segmentation first, some have used character level bigrams for indexing Chi-
nese documents [5, 6]. This approach offers a much improved performance than 
character-level indexing for Chinese text, while requiring a much larger space of 
index terms [7]. To further improve the quality of search results, some consider short 
Chinese words for indexing [7].  
As an exploration toward phrase-based indexing of Chinese text, we consider 
word-level bigrams for indexing indictment documents in Chinese. To this end, we 
rely on both HowNet [8] and Chien’s PAT tree-based methods for identifying useful 
Chinese words. After obtaining definitions of Chinese words, we segment each 
document for obtaining pairs of words, and use them as the signatures of the docu-
ments. We define the similarity between indictment documents based on the number 
of common term pairs. Having built this infrastructure, we classify indictment docu-
ments based on their prosecution categories as Liu did in [9], and classify indictment 
documents based on their cited articles as Liu did in [10]. Current experimental re-
sults indicate that using term pairs leads to classification of higher quality for the 
former task. However, the new method provides only comparable performance on the 
latter task. Our methods differ from Liu’s methods for the second task in two impor-
tant ways. In addition to using different indexing units, i.e., single terms vs. term 
pairs, we use different ways to obtain weights for these indexing units. We are still 
looking into the second task for further improvements. 
Section 2 provides more background information regarding our work. Section 3 
discusses our methods of obtaining Chinese words for the legal domain from our 
corpus. Section 4 extends the discussion for how we obtain phrases, how we assign 
weights to the phrases, and how we use the phrases for comparing the similarity be-
tween indictment documents. Section 5 contains the experimental results, and Section 
6 wraps up this paper with some discussions. 
2   Background 
We provide more background information on details of our classification tasks. We 
exclude information how we segment Chinese character strings into word strings [9] 
for page limits. We follow a standard procedure for segmenting Chinese, i.e., prefer-
ring the longer matches while using a lexicon to determine the word boundaries, that 
has been adopted in the literature. 
We can classify indictment documents in two different levels of grain sizes. The 
coarser lever is based on the prosecution categories, and the more detailed level is 
based on the cited articles. We consider six different prosecution categories: larceny 
( ), robbery ( ), robbery by threatening or disabling the victims ( ), re-
the PAT tree-
based algo-
rithm, we ex-
tract only 
words that 
consist of two 
or three charac-
ters. We manually filter the candidate words reported by these algorithms, keeping all 
spotted words that were already listed in HowNet and useful words even if they are 
not listed in HowNet. The words are then manually clustered into categories based on 
their semantic similarity. 
Procedure: TermSpotter (input: a training corpus; output: a list of candidate words) 
1. Scan the corpus, and obtain frequencies of all bigrams 
2. Concatenate bigrams that have similar occurrence frequencies into longer words, 
preferring those have higher frequencies 
3. Save all n-grams that exceed the threshold for occurrence frequency into a word-
list 
4. Remove selected words from the wordlist, and return the resulting wordlist 
Our method for spotting terms in our training data is actually very simple. The 
TermSpotter aggregates consecutive n-grams that have similar and high occurrence 
frequencies into a longer word. At step 2, two neighbor n-grams will be aggregated if 
their frequencies did not differ more than 50% of their individual frequencies. At step 
3, n-grams are considered frequent if they occurred more than 30 times. The choices 
of 50% and 30 were arbitrary, which make TermSpotter perform satisfactorily so far. 
Step 4 removes words that meet specific conditions, and we subjectively set up the 
conditions.  
We employed both TermSpotter and Chien’s algorithm to look for useful terms 
from 10372 real world indictment documents. The very first step in processing the 
legal documents that were published as HTML files was to extract the relevant sec-
tions at the preprocessing step. We then ran TermSpotter and Chien’s algorithm over 
the corpus to get the candidate words, and manually filtered the list to obtain the 
keyword database. At the manual filtering step, all extracted words that were also 
included in HowNet would be saved in the keyword database. We subjectively de-
cided whether to save the extracted words that were not included in HowNet. After 
checking each of the algorithmically extracted words, we found 1847 useful words 
that were already included in HowNet. We also found 832 useful words for the legal 
application, but they were not included in the original HowNet. In total, we have 
2679 words in the keyword database. 
To enhance the information encoded by the words, we manually categorized words 
which have similar meanings in legal applications. For instance, we have a category 
for location which includes Chinese words for banks, post offices, night markets, etc., 
and we also have a category for vehicle which includes such Chinese words as pas-
senger cars, busses, taxes, and trucks. We have 143 categories that include more than 
two Chinese words, and we treat a word as a single-word category if that word carries 
a unique meaning. In categorizing the words, we ignored the problem of ambiguous 
words, so a word was assumed to belong to only one category. Although making such 
category database
preprocessing manual termcategorization
keyword database
manual term
filtering
HowNet
published judicial
documents
algorithmic term
extraction  
Figure 1. Constructing databases of lexical information 
words in the sentence, and disregard those pairs which do not occur more than 10 
times in the training documents. We then take the union of word pairs for all sen-
tences in a preprocessed document as the feature list of the document. We employ 
this procedure in ovals labeled with generate instances with phrases and preprocess-
ing for classification in Figure 2.  
Before we explain ways of assigning weights to phrases, we elaborate on how we 
define phrases in more details. Assume that, after being segmented, a sentence in-
cludes three words, α, β, γ. We will preserve the original ordering of these words, and 
come up with three combinations, i.e., α−β, α−γ, and β−γ, as the phrases for the sen-
tence. As a result, if we have a document that includes this sentence, these word pairs 
will all be included in the instance that represents the original document. We recog-
nize that this might not be a good design decision, but doing so relieves us of the task 
of determining which phrase is the “most representative” of the original sentence for 
the current exploration. 
It is expected that with appropriate weights, weighted kNN methods provide better 
performance than plain kNN methods [11]. Hence we would also like to assign 
weights to phrases. The weights for phrases should reflect their potential for helping 
us to correctly classify documents, so defining weights based on the concept similar 
to the inverse document frequency [12] is desirable. As we mentioned in Section 3, 
we actually have converted some words to their semantic categories. Hence, we will 
assign weights to phrases at the level of semantic category, rather than to the phrases 
at the word level.  
We explore two methods for assigning weights to phrases. Let S={s1, …, si, …, sn} 
be the set of different types of documents in an application. Assume that a phrase κ 
appears fi times in documents of type si. Let pi be the conditional probability of the 
current document belonging to si, given the occurrence of κ. We may assign the quan-
tity defined in (1) as the weight of κ. Notice that the denominator in (1) assimilates 
the formula of entropy. Hence a phrase with larger w1 will collocate with fewer types 
of documents. We also explore the applicability of (2). Qualitatively, w2 is similar to 
w1 in that a phrase with larger w2 will collocate with fewer types of documents. 
∑∑ ==
=−= nr r
i
in
t tt f
f
pwhere
pp
w
11
1 ,
log
1)(κ            (1) 
( ) ∑∑ == == nr riint t ffpwherepw 1
2
1
2
2 ,)(κ        (2) 
4.2 Similarity measure 
Now that we have converted the original documents into instances that are repre-
sented by sets of phrases and that we have assigned weights to phrases, we are ready 
to define the similarity measure between instances for our classifier that adopts the 
kNN approach. Assume that we have two instances i1 and i2, each representing a set 
of key phrases. Let u1,2 denote the intersection of i1 and i2. We explore two methods, 
shown in (3) and (4), for computing the similarity between i1 and i2. The basic ele-
ment in both (3) and (4) is the portion of common phrases in the phrases of the in-
cles. We acquired the documents from the web site of the Judicial Yuan, Taiwan 
(www.judicial.gov.tw). We continue to use the notation for representing different 
types of documents, which are discussed in Section 2.  
Since our work is not different from traditional research in text classification, we 
embraced such standard measures as precision, recall, the F measure, and accuracy 
for evaluation [12]. Let pi and ri be the precision and recall of an experiment, F is 
defined as (2×pi×ri)/(pi+ri). Due to page limits, we must summarize the classification 
quality for all different types of documents, and we took the arithmetic average of the 
precision, recall, F, and accuracy of all experiments under consideration. 
Tables 3 and 4 show statistics about the performance of our classifier. These tables 
employ the same format. The top row indicates whether we considered all types of 
word pairs or only phrases that were led by verbs, as we discussed in Section 4.3. The 
second row indiecates whether we employed formula (1) or (2) for defining weights 
of phrases, and the third row indicates whether we computed similarity between 
instances by formula (3) or (4). The numbers in the top row indicate the quantities of 
phrases that were obtained from the training documents. 
Table 3 shows the statistics for the experiments for prosecution category-based 
classifiction, when we extracted phases from sentences which contained no more then 
16 Chinese characters which were segmented into no more than three words. Using 
this setup, we obtained 1504 phrases when we considered all types of phrases, and, if 
we ignored pharses that were not led by verbs, we obtained 989 phrases from the 
training documents. We observed that no matter whether we consider POS of 
constituents of the phrases, the combination of formulas (2) and (4) would offer the 
best performance. This proposition held when we repeated the same experiment 
procedure for setups where we obtained phrases from sentences of different number 
of characters and words. Results for classifying cases based on cited articles, i.e., 
statistics in Table 4, furuther support that (2) and (4) together outperform for the task 
of cited article-based classification than other combinations of the formulas. 
Assuming that we use (1) for defining weights, (3) seems to be a better choice for 
Table 3. Classification based on prosecution categories (3,16) 
POS any phrases (1504) phrases led by verbs (989) 
Weights (1) (2) (1) (2) 
Similarity (3) (4) (3) (4) (3) (4) (3) (4) 
precision 74.7% 81.9% 79.7% 85.7% 72.2% 77.1% 77.9% 85.5% 
Recall 74.3% 67.3% 79.1% 81.5% 70.3% 63.4% 76.2% 81.5% 
F 70.0% 68.8% 77.7% 82.1% 66.1% 63.9% 74.9% 82.2% 
accuracy 74.9% 73.2% 81.6% 86.2% 70.7% 69.0% 78.6% 85.4% 
Table 4. Classification based on cited articles (3,16)
POS any phrases (459) phrases led by verbs (262) 
Weights (1) (2) (1) (2) 
Similarity (3) (4) (3) (4) (3) (4) (3) (4) 
precision 77.8% 73.9% 79.5% 80.6% 75.2% 74.7% 77.6% 77.9% 
Recall 79.0% 75.4% 80.5% 81.7% 76.0% 76.1% 78.7% 79.0% 
F 77.4% 73.8% 78.8% 80.2% 75.0% 74.8% 77.3% 77.7% 
accuracy 78.9% 75.4% 80.8% 81.9% 77.0% 76.1% 79.3% 79.7% 
indexing the case instances. Our two types of phrases correspond somewhat to 
statistical phrases and syntactic phrases [3]. Our results concur with Fagan’s in that 
syntactic phrases do not provide significant better performance than statistical phrases. 
Nevertheless, we cannot be satisfied with the current results, and would like to study 
related issues for fully understanding the applicability of phrase-based indexing for 
specific domains such as legal case classification in Chinese. 
Acknowledgements 
This research was funded in part by contract NSC-94-2213-E-004-008 of the Na-
tional Science Council of Taiwan. We thank the reviewers for their unreserved and 
invaluable comments. Unfortunately, we cannot add more contents for responding to 
the comments when we have to shorten the submitted paper for page limits already. A 
more complete version of this paper is available upon request. 
References 
ICAIL stands for Int. Conf. on Artificial Intelligence and Law, and SIGIR for Annual Int. ACM 
SIGIR Conf. on Research and Development in Information Retrieval. 
1. G. Salton, C. S. Yang, and C. T. Yu, A theory of term importance in automatic text analy-
sis, J. of the American Society for Information Science, 26(1), 33–44, 1975. 
2. J. L. Fagan, Automatic phrase indexing for document retrieval, Proc. of the 10th SIGIR, 
91–101, 1987. 
3. W. B. Croft, H. R. Turtle, and D. D. Lewis, The use of phrases and structured queries in 
information retrieval, Proc. of the 14th SIGIR, 32–45, 1991. 
4. L.-F. Chien, PAT-tree-based keyword extraction for Chinese information retrieval, Proc. 
of the 20th SIGIR, 50–58, 1997. 
5. I. Moulinier, H. Molina-Salgado, and P. Jackson, Thomson Legal and Regulatory at 
NTCIR-3: Japanese, Chinese and English retrieval experiments, Proc. of the 3rd NTCIR 
Workshop on Research in Information Retrieval, Automatic Text Summarization and 
Question Answering, 2002. 
6. L.-F. Chien, Fast and quasi-natural language search for gigabytes of Chinese texts, Proc. 
of the 18th SIGIR, 112–120, 1995. 
7. K. L. Kwok, Comparing representations in Chinese information retrieval, Proc. of the 20th 
SIGIR, 34–41, 1997. 
8. HowNet. <www.keenage.com> 
9. C.-L. Liu, C.-T. Chang, and J.-H. Ho, Case instance generation and refinement for case-
based criminal summary judgments in Chinese. J. of Information Science and Engineering, 
20(4), 783–800, 2004. 
10. C.-L. Liu and T.-M. Liao, Classifying criminal charges in Chinese for Web-based legal 
services, Proc. of the 7th Asia Pacific Web Conf., 64–75, 2005. 
11. T. Mitchell, Machine Learning, McGraw-Hill, 1997. 
12. C. D. Manning and H. Schütze, Foundations of Statistical Natural Language Processing, 
MIT Press, 1999. 
13. P. Thompson, Automatic categorization of case law, Proc. of the 8th ICAIL, 70–77, 2001. 
A_B_C_D
ABC_D
ABD_C
ACD_B
BCD_A
AB_CD
AC_BD
AD_BC
A_BC_D
AC_B_D
AB_C_D
AD_B_D
A_BD_C
A_B_CD
Figure 3. The search space  
(partially shown for readability) 
dents may also behave differently from their typical 
competence patterns, and the degree of such abnormal 
behavior is controlled by groupInfluence. Using larger 
values for these parameters will introduce more ran-
domness into the observed item response patterns. 
Due to the size of the compete network, we do not 
show the networks that we used in our experiments 
completely. Figure 2 shows five substructures of the 
BNs that we used to simulate students’ item responses. 
We considered problems involving four basic concepts, 
and we would like our programs to learn how students 
learn dABCD. We assume that students learn a com-
posite concept from non-overlapping parent concepts, 
i.e., they do not share common basic concepts. This 
simplifies the problem space, but does not make the 
problem trivial. 
3. MI-based heuristics 
If we pretend that we were able to directly observe 
the states of the concept nodes, we can apply mutual 
information-based measures to our task. The mutual 
information between the composite concept and its 
parent concepts should be large than others. Let 
MI(X;Y) denote the mutual information between two 
sets of random variables X and Y. If the true structure 
is A_B_CD (i.e., learning dABCD by integrating cA, cB, 
and dCD), then MI(cA, cB, dCD;dABC) should be lar-
ger than MI(dAB, cC, cD;dABC) and other MI meas-
ures. Analogously, if the true structure is AB_C_D, 
then MI(dAB, cC, cD;dABC) should be the largest 
among all MI measures for all competing structures.  
We have assumed that students will respond to 
three test items for each concept in Section 2, so 
students may correctly answer 0%, 33%, 67%, or 
100% of the test items for a concept. We can use this 
percentage as the estimate for the state for a concept 
node, and, similarly, we can estimate the joint distribu-
tions of multiple concept nodes. For instance, 
Pr(dab=33%, cc=67%) was set to the percentage of 
students who correctly answered one item and two 
items, respectively, for dAB and cC. We also have to 
smooth the probability distributions to avoid zero 
probabilities because some configurations of the 
involved variables may not appear in the simulated 
samples. We add 0.001 to the number of occurrence of 
every different configuration of the variables. With this 
procedure, we have a way to estimate the mutual 
information measures, and we can try the following 
ures, and we can try the following heuristics in ex-
periments. 
Heuristics: The competing structure that has the larg-
est mutual information measure is the hidden structure. 
4. Search-based model selection 
Instead of computing the MI measures for all com-
peting structures, it is possible to do the comparison 
incrementally, and we have a search-based procedure.  
We illustrate the search procedure in Figure 3. The 
filled circle represents the beginning of the search pro-
cedure, and the search goes from the left to the right. 
We compute the estimated MI (EMI) of the competing 
structures in which dABCD has only two parent con-
cepts. The structure that has the largest EMI becomes 
the current candidate. We then compute the EMIs of 
the successors of the candidate. In Figure 3, structures 
on the second 
to the leftmost 
column are 
connected to 
their succes-
sors on the 
second to the 
rightmost 
column by 
lines. Successors are structures that are refined from 
the original candidate to include exactly one more 
component than the original candidate. We do not 
show all the lines in the middle of the graph for read-
ability. If the largest EMI of the successors is smaller 
than the EMI of the current candidate, then the current 
candidate is the answer. Otherwise, the successor that 
has that largest EMI becomes the current candidate. In 
the latter case, we will have to compute the EMI of 
A_B_C_D, which must be a successor of the new can-
didate in Figure 3. If the EMI of A_B_C_D is larger 
than that of the new candidate, then A_B_C_D is the 
answer, otherwise the new candidate is the answer. 
This search procedure is recursive, and can be ex-
panded and applied to more complex situations when 
there are more than four basic concepts. 
In the experiments, we set fuzziness and groupInflu-
ence to different combinations of 0.05, 0.1, 0.15, 0.2, 
0.25, and 0.3. We did not try values larger than 0.3 
because values larger than 0.3 were not considered in 
the literature, to the best of our knowledge. Hence, we 
conducted 36 experiments. In each of these 36 experi-
ments, we created 600 different networks for each of 
the five competing structures in Figure 2. Each of these 
networks was given different underlying joint prob-
ability distributions. In order to compute reliable mu-
tual information, we randomly sampled the IRPs of 
10000 simulated students from each network. 
dABCD dCD
cDcA cB cC
dABCDdACD
cDcA cB cC
dABCDdAB
cDcA cB cC
dABCD
cDcA cB cC
dABCDdABD
cDcA cB cC
Figure 2. Candidate BNs in our experiments 
Learning How Students Learn
Chao-Lin Liu
National Chengchi University, Taipei 11605, Taiwan, chaolin@nccu.edu.tw
Abstract
This extended abstract summarizes an exploration of how computational tech-
niques may help educational experts identify fine-grained student models. In
particular, we look for methods that help us learn how students learn composite
concepts. We employ Bayesian networks for the representation of student mod-
els, and cast the problem as an instance of learning the hidden substructures of
Bayesian networks. The problem is challenging because we do not have direct
access to students’ competence in concepts, though we can observe students’
responses to test items that have only indirect and probabilistic relationships
with the competence levels. We apply mutual information and backpropagation
neural networks for this learning problem, and experimental results indicate
that computational techniques can be helpful in guessing the hidden knowledge
structures under some circumstances.
Summary
Behavior models of activity participants are crucial to the success of computer
systems that interact with human users. When using Bayesian networks (BNs)
as the language for model construction, Mislevy et al. asked where we could
obtain the numbers for the conditional probability tables (CPTs) [1]. We could
ponder where we could obtain the structures of the BNs in the first place. For
educational practitioners, an obvious and practical answer to this inquisitiveness
may be that we should consult experts of the targeted domains to provide the
knowledge structures, such as the prerequisite relationships between concepts,
for building student and instructor models. Indeed this is an effective and the
de facto approach to building computer-assisted educational software in general.
Can computers be more helpful than finding the detailed numbers in the CPTs
for student modeling? More specifically, can computers assist in any way for
finding the structures of student models? Given a composite concept, say dABC,
that requires knowledge about three basic concepts, say cA, cB, and cC, how can
we tell how students learn dABC from cA, cB, and cC? Do students combine cA
and cB into an intermediate product, dAB, and then combine dAB and cC into
dABC? Or, do students integrate the basic concepts directly to learn dABC?
In this exploration, we assume that students learn the composite concept
from ingredient constructs that do not include overlapping basic concepts. For
instance, we subjectively exclude the possibility of learning dABC from two
EMIs. In addition to the EMIs for all competing substructures, we introduced
ratios between the EMIs for training the BPNs. Experience indicated that ratios
between the EMIs, e.g., the ratios between the EMIs and the largest EMI, were
useful for improving the prediction quality of the trained BPNs.
We tested the proposed procedure for guessing how stu-
10 20 30
0.75
0.8
0.85
0.9
0.95
1
fuzziness
a
cc
u
ra
cy
 
 
.05
.10
.15
.20
.25
.30
dents learn dABC. There were four possible answers. We ran-
domly sampled 500 network instances that had different un-
derlying joint probability distributions for each of these four
answers, and simulated item responses of 10000 students that
were generated from these 2000(=4×500) networks. Each sim-
ulated students responded to three items for seven concepts,
i.e., cA, cB, cC, dAB, dAC, dBC, dABC, and the responses
must be either correct or incorrect. We calculated the EMIs
and their ratios for each network instance for training BPNs,
so we trained the BPNs with 2000 training instances. We then
applied the trained BPNs to predict the learning patterns of
400 groups of students—100 groups generated for each of the
four answers. We repeated the above procedure for 36 combinations of fuzziness
and groupInfluence, each ranging between 0.05 to 0.30. The figure on this page
shows the results. The horizontal axis shows the decimal part of fuzziness, the
legend shows the values of groupInfluence, and the vertical axis shows the per-
centage of correct identification of hidden structures in 400 test cases. The results
suggest that it is possible to identify the hidden structure better than 80 per-
cent of the time, if fuzziness and groupInfluence are not large and if educational
experts’ guess list does include the correct structure.
Do we really need student models of better quality? Experimental results re-
ported by Carmona et al. suggested that student models of higher quality could
help us improve the effectiveness of computerized adaptive tests [3]. Hence, we
hope results outlined in this extended abstract can be useful. We have expanded
our experiments to cases where we learned how students learn composite con-
cepts that included four basic concepts [4]. The accuracy remained above 75% in
unfavorable conditions. We thank reviewers for their invaluable comments on the
original manuscript. This work was partially supported by the research contract
94-2213-E-004-008 of National Science Council of Taiwan.
References
1. Mislevy, R.J., Almond, R.G., Yan, D., Steinberg, L.S.: Bayes nets in educational
assessment: Where do the numbers come from? 15th UAI (1999) 437–446
2. Liu, C.L.: Using mutual information for adaptive item comparison and student
assessment. J. of Educational Technology & Society 8(4) (2005) 100–119
3. Carmona, C., Milla´n, E., Pe´rez-de-la-Cruz, J.L., Trella, M., Conejo, R.: Introducing
prerequisite relations in a multi-layered Bayesian student model. Lecture Notes in
Computer Science 3538 (2005) 347–356
4. Liu, C.L., Wang, Y.T.: An experience in learning about learning composite con-
cepts. 6th IEEE ICALT (2006) to appear
Report for Attending ISMIS 2006 
Chao-Lin Liu 
Department of Computer Science, National Chengchi University 
ISMIS 2006 was held in Bari, Italy between 27 and 29 September 2006. I presented two 
papers during this conference on student modeling and legal informatics, and this is a 
brief report for the trip to ISMIS 2006. 
About ISMIS 
The International Symposium on Methodologies for Intelligent Systems (ISMIS) has a long 
history, and this sixteenth ISMIS was held in Bari Italy between 27 and 29 September 2006. 
Normally, there are two ISMISes for every three years. The main initiator and organizer of this 
series of conferences is Professor Ras of the University of North Carolina at Charlotte. The next 
ISMIS will take place in Toronto Canada in May of 2008.  
According to the report provided by the program chair of ISMIS 2006, 192 papers were 
submitted from 34 countries. Among these 192 submitted papers, 66 papers were accepted as 
long papers, and 15 papers were accepted as short papers. The acceptance rate for long papers 
was about 34%. Including the short papers, the overall acceptance rate for ISMIS 2006 reached 
50%.  
Taiwan, followed immediately by Japan, ranked sixth in terms of submitted papers. The 
leading countries are China, Italy, South Korea, USA, and France. Surprisingly the acceptance 
rate achieved by the Chinese papers was just about 10%, suggesting that there were just about 5 
papers authored by people with Chinese identity. The acceptance rate for Taiwanese papers was 
about 70%, meaning that five papers were accepted. If you have access to the conference 
proceedings, you will find that they consist of four long papers and one short paper. In addition, 
these five Taiwanese papers were authored by three groups. NCCU had two long papers, KAUS 
had one long and one short papers, and TKU had one long paper. Unfortunately, NCCU is the 
only group that did show up and present the papers.  
There was a lady who actually came from the Hampton University in Virginia USA, but she 
went to USA from Taiwan. Li-Shiang Tsay is a fresh professor at Hampton and a former student 
or Professor Ras of the UNC.  
About Chinese presence  
Due to the relationship between Taiwan and China, I thought it is appropriate to say 
something about Chinese presence in this report.  
During this conference, I did not have chances to contact with any Chinese who came directly 
from China. However, I did have chances to meet people who left China for other countries. I met 
two from Japan, two from USA, and one from German. 
If we boldly take the status of attendance as a sign, Taiwanese presence is significantly lower 
than Chinese presence. Chinese people have worked very hard in the past years to increase their 
international presence, supported by their huge population and strengthening economic situations. 
The status quo at ISMIS 2006 can be just one of the signs.  
