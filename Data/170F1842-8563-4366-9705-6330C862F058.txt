fault distributions implicitly using a Monte-Carlo approach. We 
further employ a heuristic to customize the use of quasirandom 
sequences, which successfully speeds up the convergence of 
simulation error and hence shortens the runtime. Moreover, 
advanced sampling techniques are also incorporated for variance 
reduction of SSERs. 
2) The support-vector-regression framework is applied to tackle the 
complexity of these natures and build compact yet accurate 
generation and propagation models for transient fault distributions. 
Moreover, we also apply two intensified methods to solve the 
disadvantage of support-vector-regression.  
3) The closed-form analysis framework to overcome the trade-off 
between accuracy and efficiency problem. This framework presents 
accurate cell models in first-order closed-form, thereby enabling the 
analysis of SSERs in a block-based fashion similar to statistical 
static timing analysis (SSTA). These cell models are derived as a 
closed form in the proposed framework and remain precise under 
the assumption of a normal distribution for the process parameters. 
 
Experimental results show that the proposed framework increases 
the SER computation speed by 107X, with only 2% accuracy loss 
compared to the Monte-Carlo SPICE simulation. 
 
 I 
摘 要 
 
在深次微米時代中 CMOS 設計必須要準確地估計電路的統計性的軟性電子
錯誤率（SER）。隨著製程變異日漸嚴重，導致軟性電子錯誤的行為有著相當大的
不確定性。然而，若考慮製程變異的影響，電壓脈衝寬度在傳遞過程中不再只是
像以往被認為的單調遞減。結果顯示，在現今的電子設計中，若以傳統的靜態分
析，將會導致嚴重地低估軟性電子錯誤率。 
 
因此，這篇報告提出了三個有效地架構來應對上述之複雜問題。  
1) 首先，我們利用蒙地卡羅(Monte-Carlo)方法隱性地獲取暫態錯誤之分
佈。另外，我們進一步採用準隨機亂數(quasirandom sequences)，成功地解決
了蒙地卡羅方法中計算時間冗長的缺點，加快了收斂速度並縮短了運行時間。此
外，重要性取樣性(importance sampling)也被加入至此架構當中，以提升計算
軟性電子錯誤率之速度。 
2) 接下來，我們使用支持向量回歸(support-vector-regression)方法精
準地建構出製程變異下軟性電子錯誤率行為之模型。然而，支持向量回歸方法也
有著建構模型時間以及找尋參數之問題存在，在此架構中，我們也提出二個方法
來解決這些問題。 
3) 第三個方法為閉合形式的分析(closed-form analysis)架構，此架構可
以克服準確性及效率之間的權衡問題。此閉合形式的分析架構是利用類似統計靜
態時序分析(SSTA)之方法來分析軟性電子錯誤率。此架構底下，可提供精準地一
階閉合形式模型，以預測軟性電子錯誤率之行為。 
 
實驗結果證明，所提出之三種方法在 ISCAS85電路的驗證下，與蒙地卡羅電
路模擬相比可以達到平均 107倍的加速，而只有 2%的誤差。 
 
關鍵字：軟性電子錯誤率 ; 製程變異 ; 支持向量回歸; 蒙地卡羅; 統計靜態時
序分析 
 
 
 
 
 
 
 
 
 
 III 
Table of Content 
 
List of Figures .............................................................................................................. V 
List of Tables ............................................................................................................. VII 
 
Chapter 1 Introduction ................................................................................................ 1 
1.1  Research goal ................................................................................................ 3 
1.2  Research method ........................................................................................... 3 
1.2.1 First year ........................................................................................... 3 
1.2.2 Second year ....................................................................................... 5 
1.2.3 Third year .......................................................................................... 5 
 
Chapter 2  Fundamental of Statistical Soft Error Rate .............................................. 6 
2.1 Transient-fault behavior in very deep submicron era ..................................... 6 
2.1.1 To be electrically better or worse? .................................................... 7 
2.1.2 When error-latching probability meets process variations ............... 8 
2.2 Impact of spatial correlation ........................................................................... 9 
2.3 Full-spectrum analysis or not ........................................................................ 12 
2.4 Problem formulation of statistical soft error rate (SSER) ............................. 14 
2.4.1 Overall SER estimation................................................................... 15 
2.4.2 Logical probability computation ..................................................... 15 
2.4.3 Electrical probability computation .................................................. 17 
 
Chapter 3 Table-lookup Monte-Carlo (MC) Framework ......................................... 18 
3.1 Cell pre-characterization ............................................................................... 18 
3.1.1 Particle-strike table Tstrike ................................................................ 18 
3.1.2 Transient-fault propagation table Tprop ........................................... 19 
3.2 Sampling and renewal of transient faults ...................................................... 20 
3.2.1 First-strike cases.............................................................................. 21 
3.2.2 Propagation cases ............................................................................ 21 
3.3 Using qauasirandom sequences .................................................................... 23 
3.4 Applying importance sampling on QMC ...................................................... 25 
3.4.1 Importance sampling overview ....................................................... 25 
3.4.2 Advantage of applying importance sampling on QMC .................. 26 
  
Chapter 4 Support-Vector-Regression (SVR) Learning Framework ........................ 29 
4.1 Training sample preparation ......................................................................... 31 
 V 
List of Figures 
 
Figure 1.1: Three masking mechanisms for soft errors ................................................ 2 
Figure 1.2: SER discrepancies between static and Monte-Carlo SPICE simulation 
w.r.t. process variation .................................................................................. 3 
Figure 1.3: Proposed table-lookup framework ............................................................. 4 
 
Figure 2.1: Static SPICE simulation of a path in the 45nm technology ....................... 7 
Figure 2.2: Process-variation vs. error-latching probabilities ....................................... 9 
Figure 2.3: SSER comparison from static and Monte Carlo SPICE simulations, 
the proposed MC with spatial correlations and without spatial 
correlations frameworks ............................................................................. 10 
Figure 2.4: The gates in different grid with different process variations .................... 11 
Figure 2.5: (a) SERs of four-level and full-spectrum charge collection w.r.t. 
different latching-window size (b) SERs w.r.t. different levels of charge 
collection .................................................................................................... 12 
Figure 2.6: Transient-fault distributions induced by four-level and full-spectrum 
charge collection ......................................................................................... 13 
Figure 2.7: An example for illustrating the SSER problem ........................................ 14 
Figure 2.8: Logical probability computation for one sample path.............................. 16 
 
Figure 3.1: Pre-characterization of particle-strike table Tstrike for an AND gate ........ 19 
Figure 3.2: Pre-characterization of transient-fault-propagation table Tprop for OR 
gate ............................................................................................................. 20 
Figure 3.3: Logical probability update for an AND gate ............................................ 21 
Figure 3.4: Re-convergent transient faults on an AND gate ....................................... 22 
Figure 3.5: Distributions from the Monte Carlo methods with random number 
generation and quasirandom sequences ..................................................... 23 
Figure 3.6: Convergence rate, dimension number, and logic depth of benchmark ... 24 
 
Figure 4.1: Proposed statistical SER framework using support-vector-regression 
models ........................................................................................................ 29 
Figure 4.2: Linear decision boundaries for a two-class data set ................................. 32 
Figure 4.3: Quality comparison of 200 models using different parameter 
combinations .............................................................................................. 35 
Figure 4.4: Prioritized scheme for parameter search .................................................. 36 
Figure 4.5: Intensified SVM learning with PSO ......................................................... 37 
 VII 
List of Tables 
 
 
Table 5.1 Comparison of SER w/o and w/ considering the correlation between 
transition signals ......................................................................................... 48 
 
Table 6.1: Summary of table error rate ....................................................................... 54 
Table 6.2: Benchmark circuits, SER and runtime from the baseline MC and QMC 
frameworks ................................................................................................. 55 
Table 6.3: Benchmark circuits, SER from the baseline MC, QMC, and QMC-IS 
frameworks considering spatial correlations .............................................. 56 
Table 6.4: Benchmark circuits, runtime from the baseline MC, QMC, and 
QMC-IS frameworks considering spatial correlations ............................... 56 
Table 6.5: Model quality w.r.t. model type ................................................................. 57 
Table 6.6: Model quality w.r.t. model type constructed by intensified SVM 
learning ....................................................................................................... 58 
Table 6.7: Benchmark circuits, SER and runtime from the baseline MC and SVR 
frameworks ................................................................................................. 60 
Table 6.8: Summary of model error ............................................................................ 62 
Table 6.9: SSER measurement of various benchmark circuits ................................... 64 
 
 
 2 
  
Figure 1.1: Three masking mechanisms for soft errors 
 
techniques to logical and electrical maskings and scale the error probability according 
to the specified clock period, AnSER [17] applies signature observability and 
latching-window computation for logical and timing maskings to approximate SER 
for circuit hardening. SEAT-LA [10] and the algorithm in [11] simultaneously 
characterize cells, flip-flops and propagation of transient faults by waveform models 
and result in good SER estimate when comparing to SPICE simulation. However, all 
of these techniques are deterministic and may not be capable of explaining more 
sophisticated circuit behaviors due to the growing process variations beyond deep 
sub-micron era. 
 
Process variations including various manufacturing defects have grown to be one 
of the major challenges to scaled CMOS designs [19][20]. From [20][21], 25%-30% 
different on chip frequency are observed. For design reliability, 15%-40% SER 
variations are reported in [22] under the 70nm technology. Also, authors in [23] 
proposed a symbolic approach to propagate transient faults considering process 
variations. 
 
Using the 45nm Predictive Technology Model (PTM) [24], the impact of process 
variations on circuit reliability is illustrated in Figure 1.2, where SERs are computed 
by SPICE simulation on a sample circuit c17 from ISCAS 85 under different values 
(σproc’s) of process variation applied to perturbing separately the gate width and 
channel length of each transistor in each cell’s geometry. The X-axis and Y-axis 
denote σproc and SER, respectively, where FIT (Failure-In-Time) is defined by the 
number of failures per 109 hours. Nominal settings without variation are used in static 
SPICE simulation, whereas Monte-Carlo SPICE simulations are used to approximate 
process-variation impacts under different σproc’s. 
 
As a result, SER from static SPICE simulation is underestimated. Considering 
different σproc’s in Monte-Carlo SPICE simulation, all SERs are higher than that from 
static SPICE simulation. As process variations deteriorate, the discrepancy between 
 4 
Monte-Carlo (MC) method, a computational algorithm using repeated random 
samplings to portray complex statistical behaviors of physical or mathematical 
systems as depicted in Figure 1.3. This framework maps to the three masking 
mechanism using three loops: the outmost loop considers various levels of collection 
charge; the second loop accounts for all vulnerable nodes within a circuit; the 
innermost loop computes δstrike and δprop implicitly. As the key component of the 
framework, the last loop can be further decomposed into two parts: (1) cell 
pre-characterization and (2) sampling and renewal of transient faults. 
 
 
Figure 1.3: Proposed table-lookup framework 
 
Furthermore, we customize the use of quasi-random sequences, which 
successfully speed up the convergence of simulation error and hence shorten runtime. 
However, there is still a problem about the uniformity of the quasi-sequence samples 
in multivariate distribution. To solve this problem, we use importance sampling to 
 6 
according to our experiments. Thus, we devised a parameterized SSTA framework 
that takes into account the timing correlation to derive more accurate SER. 
Experimental results demonstrate that our approach can provide reasonable results 
much more rapidly than all previous works. 
 
  
 8 
result suggests that the voltage pulse width of a transient fault is not always 
diminishing, which contradicts some assumptions made in traditional static analysis 
[10]. A similar phenomenon called Propagation Induced Pulse Broadening (PIPB) is 
discovered in [25] and states that the voltage pulse width of a transient fault widens as 
it propagates along the long inverter chain. 
 
2.1.2 When error-latching probability meets process variations 
 
The second observation is dedicated to the timing-masking effect under process 
variations. In [9][18], the error-latching probability (PL) for one flip-flop is defined as 
 
      PL = pw−w
tclk
          (1) 
 
where pw, w and tclk denote the pulse width of the arrival transient fault, the latching 
window of the flip-flop, and the clock period, respectively. However, process 
variations make pw and w become random variables. Therefore, we need to redefine 
Equation (1) as following. 
 
Definition (Perr−latch, error-latching probability) 
Assume that the pulse width of one arrival transient fault and the latching 
window (tsetup+thold) of the flip-flop are random variables and denoted as pw and w, 
respectively. Let x = pw − w be another random variable and μx and σx be its mean 
and variance. The latch probability is defined as: 
 
   Perr−latch(pw, w) = 1tclk ∫ x ∙ P(x > 0) ∙ 𝑑𝑑ux+3σx0            (2) 
 
With the above definition, we further illustrate the impact of process variations 
on SER analysis. Figure 4(a) shows three transient-fault distributions with the same 
pulse-width mean (95ps) under different σproc’s: 1%, 5% and 10%. A fixed latching 
window w = 100ps is assumed as indicated by the solid lines. According to Equation 
(1), static analysis result in zero SER under all σproc’s because 95 − 100 < 0. 
 
From a statistical perspective, however, these transient faults all yield positive 
and different SER’s. It is illustrated using two terms: P(x > 0) and x in Equation (2). 
First, in Figure 2.2(a), the cumulative probabilities for pw > w under three different 
σproc’s are 17%, 40%, and 49%, respectively. The largest σproc corresponds to the 
 10 
therefore it is spatially correlated. 
 
Devices tend to have similar characteristics as it with similar layout patterns and 
proximity structures. In other words, it is globally location-dependent. Devices have 
the similar characteristics than placed far away as it located close to each other. With 
increased process scaling, intra-die variations are becoming a more dominant portion 
of the overall variability of device features, meaning that devices on the same die can 
no longer be treated as identical copies of the same device. 
 
If we do not take into account the value of process variations, it will lead to 
underestimated/overoptimistic estimation on SSER. However, all previous works 
consider the impact of process variations but do not include spatial correlations in the 
statistical soft error rate, leading to incorrect SSERs. Therefore, we investigate the 
impact of spatial correlations in our project to comprehend the accuracy of SSERs as 
shown in Figure 2.3.  
 
 
Figure 2.3: SSER comparison from static and Monte Carlo SPICE simulations, the 
proposed MC with spatial correlations and without spatial correlations frameworks 
 
According to Figure 2.3, circuit SER is overestimated under the process variation 
5% without considering spatial correlations. Circuit SER that considers spatial 
correlations under the process variation 5% is generally lower when comparing with 
the circuit SER under the process variation 5% without considering spatial 
correlations. Therefore, we propose an effective model considering spatial 
correlations of statistical soft error rate. The analysis is extended to include spatial 
 12 
Our algorithm makes a second assumption. Assume that there are no correlations 
between different types of process parameters, and nonzero correlations may exist 
only among the same type of process parameters in different grids. For instance, the 
Lg values for transistors in nearby grids are correlated, but the other parameters such 
as Wg or Wint in any grid are uncorrelated. In other words, we assume that 
interconnect parameters in different layers to be different types of parameters. 
 
2.3 Full-spectrum analysis or not 
 
Some previous works simplify the SER estimation by injecting only four levels 
of electrical charges. Therefore, our project poses a simple, yet important question, 
“Are four levels of electrical charges enough to converge SER correctly and properly 
address the process-variation effect?” 
 
 
Figure 2.5: (a) SERs of four-level and full-spectrum charge collection w.r.t. different 
latching-window size (b) SERs w.r.t. different levels of charge collection 
 
 14 
and therefore may result in mistakes on SER integration. As the latching-window size 
of one flip-flop was far from the first TF distribution, soft errors from such TF 
distributions were entirely masked due to the timing-masking effect. For example, the 
biggest pulse width distribution in the upper part of Figure 2.6 is excluded from SER 
estimation. But, only part of them (those smaller decomposed TF distributions) were 
masked during analysis using all levels of deposited charges (Figure 2.6, lower part). 
As a result, SER estimation was no longer valid with analysis using only four levels 
of charges and instead should comprehensively consider full-spectrum charge 
collection. 
 
2.4 Problem formulation of statistical soft error rate (SSER) 
 
In this section, we formulate the statistical soft error rate (SSER) problem for 
general cell-based circuit designs. Figure 2.7 illustrates a sample circuit subject to 
process variations, where the geometries of each cell vary [21]. Once high-energy 
particles strike the diffusion regions of these variable-size cells, according to Figure 
1.2, 2.1 and 2.2, the electrical performances of the resulting transient faults also vary a 
lot. Accordingly, to accurately analyze the soft error rate (SER) of a circuit, we need 
to integrate both process-variation impacts and three masking affects discussed in 
Chapter 1 simultaneously, which brings up the statistical soft error rate (SSER) 
problem. 
 
Figure 2.7: An example for illustrating the SSER problem 
 
 16 
Fsoft−err(i, q) depends on all three masking effects and can be decomposed into 
              Fsoft−err(i, q) = ∑ Plogic(i, j)Nffj=0 × Pelec(i, j, q)       (6) 
 
where Nff denotes the total number of flip-flops in the circuit under test. Plogic(i, j) 
denotes the overall logical probability of successfully generating a transient fault and 
propagating it through all gates along the path from node i to flip-flop j. It can be 
computed by multiplying the signal probabilities for specific values on target gates as 
follows.  
 
               𝑃𝑙𝑙𝑙𝑖𝑙(i, j) = Psig(i = 0) × ∏ Pside(k)k∈i→j         (7) 
 
where k denotes one gate along the target path (i→j) starting from node i and ending 
at flip-flop j, Psig denotes the signal probability for the designated logic value, and 
Pside denotes the signal probability for the non-controlling values (i.e. 1 for AND gates 
and 0 for OR gates) on all side inputs along the target path. 
 
Figure 2.8 illustrates an example where a particle striking net a results in a 
transient fault that propagates through net c and net e. Suppose that the signal 
probability of being 1 and 0 on one arbitrary net i is Pi and (1-Pi), respectively. In 
order to propagate the transient fault from a towards e successfully, net a needs to be 0 
while net b, the side input of a, and net d, the side input of c, need to be 
non-controlling, simultaneously. 
 
 
Figure 2.8: Logical probability computation for one sample path 
 
Therefore, according to Equation (7), 
 
𝑃𝑙𝑙𝑙𝑖𝑙(a, e) = Psig(a = 0) × Pside(a) × Pside(c)                                     = Psig(a = 0) × Psig(b = 1) × Psig(d = 0) 
        = (1 − Pa) × Pb × (1 − Pd)               
 
 18 
from the way they compute δstrike and δprop. 
 20 
current source according to [7]: 
 
     I(q, t) = q
𝜏𝛼−𝜏𝛽
× (e− t𝜏𝛼 − e− t𝜏𝛽)       (10) 
 
An arbitrary number of cells are also generated and connected as the output 
loading for the AND gate. Capacitance of each cell will be normalized in terms of the 
unit-size inverter (NOT). The final output loading is obtained from summing up each 
output cell and represented by a total number of equivalent NOTs. 
 
 
Figure 3.1: Pre-characterization of particle-strike table Tstrike for an AND gate 
 
Given a fixed q, a number of MC runs with different SPICE settings are repeated 
in Figure 1.3 to compute the means and variances of pulse width and voltage 
magnitude, respectively, for the resulting transient fault. Figure 3.1(b) shows the table 
for the AND gate including four matrices: pulse-width mean matrix ( Mpwμ ), 
pulse-width variance matrix (Mpwσ ), voltage-magnitude mean matrix (Mvmμ ) and 
voltage-magnitude variance matrix (Mvmσ ) to store mean and sigma values for pulse 
widths and voltage magnitudes of transient-fault propagation. Note that since 
first-strike transient faults are sensitive to input vectors, the input vector also serves as 
an index in Tstrike. 
 
3.1.2 Transient-fault propagation table Tprop 
 
The transient-fault propagation table Tprop, on the other hand, reflects the 
changes of electrical properties when propagating the transient fault through one cell. 
Figure 3.2(a) shows the sample SPICE simulation environment to pre-characterize the 
transient-fault propagation through one OR gate. The output loading is set up 
 22 
Transient-fault probability P(f) denotes the updated probability after f propagates 
through one gate and is also incorporated in the proposed table-lookup framework. 
Initially, all inputs are assumed to be a independent variable with equal probabilities 
of being 1 and 0. Probabilities for each node can be derived statically according to its 
input probabilities. Later, during computing the change of f on each cell, P(f) is 
updated simultaneously to reflect the logic masking effect mentioned in Section 2.4. 
Two different cases are discussed in detail as follows. 
 
3.2.1 First-strike cases 
 
For the first-strike cases, the struck node is required to remain 0 for a positive 
transient fault and 1 for a negative transient fault. Let’s take one AND gate shown in 
Figure 3.3(a) for example. Given the collection charge q, transient fault fz =(pwz, vmz) can be looked up in Tstrike and denoted as fz = lutP.S.(q, z). Assume that 
the probabilities of being 1 for input x and y are denoted by Px and Py. For the particle 
strikes the output z to induce a positive transient fault (fz+), z is required to be 0 and 
thus, P(fz+) = (1 − Px) × Py. Similarly, for a negative transient fault (fz−) striking 
output z, P(fz−) = Px × Py. 
 
 
Figure 3.3: Logical probability update for an AND gate 
 
3.2.2 Propagation cases 
 
For the propagation cases, in order to propagate the positive (or negative) 
transient fault through one gate, all other side-inputs are required to be 
non-controlling values (n.c.v.). Besides, non-convergent and convergent conditions 
need to be considered separately. Figure 3.3(b) illustrates a non-convergent example 
of the AND gate. Similarly, given fx, fz can be looked up in Tprop by fz = lutprop.(fx, z) = (pwz, vmz . As to transient-fault probability, since 
non-convergent condition assumes that only one positive (or negative) transient fault 
arrives one input of the AND gate, say x in this example, only y is required to be 1 
(n.c.v. for the AND gate). Therefore, P(fz) = P(fx) × Py. 
 24 
3.3 Using qauasirandom sequences 
 
Pseudorandom number generation plays a key role to the success of the Monte 
Carlo method. However, using rand() function for sampling points often suffers from 
the clustering problem in high dimensional spaces. Figure 3.5(a) illustrates this 
problem on an example of generating a (X, Y)-distribution by the Mont e Carlo 
method using the rand() function . The sampling points are observed not evenly 
scattered among the (X, Y) plate, which means that these sampling points from 
pseudorandom generation may not be representative enough for the entire space. 
 
 
Figure 3.5: Distributions from the Monte Carlo methods with random number 
generation and quasirandom sequences 
 
The clustering problem motivates research of finding a deterministic sequence 
such that well-chosen points are distributed in the high-dimensional spaces uniformly. 
Such sequences are named quasirandom sequences. Figure 3.5(b) shows the same 
number of sampling points using quasirandom sequences on the (X,Y) plate. Sobol 
algorithm is used to generate the corresponding sequences. From Figure 5(b), new 
sampling points are observed more uniformly distributed over the (X,Y) plate and 
thus have better representativeness. 
 
Monte Carlo methods with quasirandom sequences are termed Quasi-Monte 
Carlo (QMC) methods. Given a sampling number N and a dimension d, Monte Carlo 
methods converge with O(1/√N) simulation errors whereas QMC methods converge 
with O(1/√N) for optimal cases. Previous research works have demonstrated better 
results for QMC than MC methods for t he problems with ≤360 dimensions in 
finance and physics. 
 
 26 
3.4 Applying importance sampling on QMC 
 
In this project, we also combine the QMC and importance sampling in order to 
efficiently calculate expectations with respect to multivariate distributions. This 
method can be used to circumvent the definition of non-uniform quasi-random 
varieties. Interpreted as a parameter transformation method, it can get rid of 
singularities of the integrand which increases the speed of convergence of QMC. In 
the case of complicated multivariate distributions the application of QMC techniques 
is much easier for importance sampling than for Markov chain Monte Carlo methods. 
 
3.4.1 Importance sampling overview 
 
In importance sampling, one attempts to avoid taking samples in regions where 
the value of the function is negligible, and to focus on regions where the value is large. 
It is important to allow for this bias in sampling by weighting the sample values 
appropriately. Importance sampling is based on the idea of using weights to correct 
for the fact that we sample from the instrumental distribution g(x) in place of the 
target distribution f(x). Importance sampling is based on the identity shown as 
follows: 
 
( )( ) ( ) ( ) ( ) ( )
( )x x x
f xP X x f x dx g x dx g x w x dx
g x
∈ = =∫ ∫ ∫  
 
For all g(x), such that g(x) > 0 for (almost) all x with f(x) > 0. We can generalize 
this identity by considering the expectation Ef (h(X)) of a measurable function h: 
 
( )( ( )) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ( ) ( ))
( )f g
f xE h X f x h x dx g x h x dx g x w x h x dx E w X h X
g x
= = = = ⋅∫ ∫ ∫
 
Importance sampling is a numerical method in order to approximate an integral. 
It can be implemented to estimate the mean response for a given sample under an 
alternate distribution. Importance sampling is based on the following identity. Let G 
and g be the distribution function and the density function of some distribution, called 
importance distribution in the sequel as following equation: 
 
( )
( )( ) ( ) ( ) ( ) ( ) ( ) ( )
( )d d df q R R R
f xE q x f x dx q x g x dx q x w x dG x
g x
= = =∫ ∫ ∫  
 28 
integration: the integrand can have singularities when the domain of the distribution is 
unbounded and it can be very expensive or difficult to sample points from a general 
multivariate distribution. 
 
( ) ( ) ( ) ( ) ( )
d d
f q
R R
E q x f x dx q x dF x= =∫ ∫  
 
For typical applications, we want to derive expectation, variance and some 
quantities of all marginal distributions together with all correlations between the 
variables. If we consider, for instance, the expectation, it is obvious that we have to 
use q(x) = xk to obtain the expectation of the marginal. 
 
The convergence rate often can be increased when highly uniform point sets 
(HUPS, also called low discrepancy sequences or quasi-random numbers) are used 
instead of (pseudo-) random points. Such methods are called quasi-Monte Carlo 
methods (QMC). There exist HUPS where the star discrepancy (and thus the QMC 
estimator) converges withO((lnN)d/N). When E f (q) has to be evaluated with respect 
to some non-uniform distribution F with bounded domain, similar results exist. 
 
The QMC approach requires point sets with low F–discrepancy. Such point sets 
are also created by applying appropriate transformation methods on low discrepancy 
sequences. However, for general multivariate distributions such transformations are 
hard to ﬁnd and/or numerically very expensive. Moreover, these may introduce 
singularities into our integration problem and thus convergence is not guaranteed by 
the Koksma-Hlawka inequality. 
 
Thus, we need to transform low discrepancy point sets into sets of points with 
low F-discrepancy when we calculate the QMC estimator. However, the 
transformation methods that have been developed for non-uniform random variety 
generation cannot be applied for QMC, because these destroy the structure of the 
underlying point set. Moreover, the theory of non-uniform random numbers does not 
directly apply when quasi-random numbers are used. 
 
The problem of generating non-uniform random points and that of generating 
non-uniform quasi-random points should be seen as different problems. For the first 
one, we need to transform uniform random numbers into random points. The 
correctness of the transformation is verified using probability theory. The structure of 
the used uniform pseudo-random point set is usually not taken into consideration. The 
 30 
Chapter 4  
Support-Vector-Regression (SVR) Learning 
Framework 
 
The table-lookup Monte-Carlo framework is inherently limited in execution 
efficiency because it computes δstrike and δprop indirectly using extensive samplings of 
Monte-Carlo runs. In this section, we propose another learning-based framework to do 
the task directly with support of support vector regression (SVR) and is found to be 
both more efficient and more accurate. Note that our SVR-learning framework can be 
represented in the same flowchart as Figure 1.3 with the replacement of first-strike 
tables (Tstrike) and propagation tables (Tprop) with respective learning models (δstrike 
and δprop) as shown in Figure 4.1. 
 
 
Figure 4.1: Proposed statistical SER framework using support-vector-regression 
models 
 32 
4.1  Training sample preparation  
 
SVR models differ from lookup tables on the way we prepare training samples 
for them. For lookup tables, one starts from selecting a finite set of points along each 
table dimension. On one hand, they should be chosen economically; on the other hand, 
it is difficult to cover all corner cases with only a limited numbers of points. For SVR 
models, we do not need to select these points. Instead, we provide large sets of 
training samples, and let the SVR algorithm do the selection task. 
 
A training sample set S of m samples is defined as: 
 
    S ∈ (X�⃗ × Y)m = {(x�⃗ 1, y1),⋯ , (x�⃗ m, ym)}              (15) 
 
where m pairs of input variables x�⃗ i’s and target values yi’s are obtained from massive 
Monte-Carlo SPICE simulation. For  δstrikeμ ,  δstrikeσ , we use input variables 
including charge strength, driving gate, input pattern, and output loading; for  δpropμ ,  δpropσ , we use input variables including input pattern, pin index, driving gate, input 
pulse-width distribution ( μpwi−1 and  σpwi−1 ), propagation depth, and output loading. 
 
In our training samples, we implement output loading using combinations of 
arbitrary cell input pins. Doing so preserves additional information for the output 
loading status and saves the labor (and risk) of characterizing the capacity of each 
cell’s input pin. Although the number of such combinations can easily explode, there 
are usually only a limited number of representatives, which are automatically 
identified by SVR. Furthermore, from a learning perspective, since both peak voltage 
and pulse width are the responses of charge injection current formulated in Equation 
(10), they are highly correlated. Empirically, using pulse-width information alone can 
yield satisfactory SSERs and thus in our framework, we do not need to incorporate 
models for peak voltage. 
 
4.2  Support vector machine and its extension to regression  
 
Support vector machine (SVM) is one of the most widely used algorithms for 
learning problems [29] and can be summarized with the following characteristics: 
 SVM is an efficient algorithm and finds a global minimum (or maximum) 
for a convex optimization problem formulated from the learning problem. 
 SVM avoids the curse of dimensionality by capacity control and works well 
 34 
substitution. The primal form presents the nature of the regression whereas the dual 
form provides the key to the later non-linear extension using kernel functions. In our 
framework, ϵ-SVR [29] is implemented to realize a family of highly non-linear 
regression models f(x�⃗ i) ∶  x�⃗ → y for δstrikeμ , δstrikeσ , δpropμ , and δpropσ  for pulse-width 
mean and sigma of first-strike functions and pulse-width mean and sigma of 
propagation functions, respectively. 
 
4.2.1 Primal form optimization 
 
The regression’s goal is to derive a function that minimizes slacks and 
meanwhile to make f as smooth as possible. The corresponding constrained 
optimization problem for ϵ-SVR is modified as follows, 
 
 minimize ‖w���⃗ ‖2 + C∑ (ξi2 + ξı�2)mi=1   
 subject to � (〈w���⃗ ∙ x�⃗ i〉 − yi) ≤ ϵ + ξi, i = 1, … , m,yi − (〈w���⃗ ∙ x�⃗ 〉 + b) ≤ ϵ + ξı� , i = 1, … , m,
ξi, ξı� ≥ 0, i = 1, … , m      (17) 
 
where the two slack variables ξi and ξı�  represent variations of the error exceeding 
and below the target value by more than ǫ, respectively. The parameter C determines 
the trade-off between the smoothness of f(x�⃗ i) and the variation amount of errors (ξi 
and ξı�) to be tolerated. Equation (17) is termed the regression’s primal form. 
 
4.2.2 Dual form expansion 
 
Instead of finding w���⃗  directly, the Lagrange multiplier method transforms the 
optimization problem from the primal form to its dual form and derives f as, 
 
     f(x�⃗ i) = ∑ (αi − αi∗)mi=1 〈x�⃗ ∙ x�⃗ i〉 + b               (18) 
 
where αi, αi∗ are Lagrange multipliers and b is a function of ϵ, C, α’s and α∗’s [30]. 
 
Several findings can be inferred from Equation (18). First, the only inner product 
〈x�⃗ ∙ x�⃗ i〉 implies that only an unseen sample x�⃗  and a training sample x�⃗ i, are sufficient 
to predict a new unseen target value y. Second, only training samples x�⃗ i’s that 
correspond to nonzero (αi − αi∗)’s contribute to the prediction outcome. All other 
samples are unnecessary for the model and are filtered out during the training process. 
 36 
In Equation (20), EkR denotes the cost function, where E and R respectively 
denote error rate and compression ratio, and k is a parameter controlling the trade-off 
between E and R. A larger k makes the cost function more sensitive to the error rate, 
and vice versa. Note that if a single k is used, the cost function may wrongly select a 
combination with one matrix being extremely low whereas the other being 
undesirably high, e.g. E = 0.01% and R = 0.99%, as indicated by the triangle in Figure 
4.3. 
 
 
Figure 4.3: Quality comparison of 200 models using different parameter combinations 
 
Therefore we applied a prioritized scheme according to predefined goals on both 
matrices as illustrated in Figure 4.4. Assuming the goal (E < 6%, R < 10%), we draw 
finite grids near these goals, and prioritize them accordingly. For example, G0 is 
preferred over G1, G1 is preferred over G2, and G0 ∼ G5 are preferred over G′. The 
main idea is that in grids where E is small or R is large, the cost function is adjusted to 
be more insensitive to error. Therefore, k is assigned smaller in grids with lower error 
rate or larger compression ratio, as illustrated in Figure 4.4. In G′, k = 3.5 generally 
works well. 
 
After determining the cost function, exhaustive search may still take months. To 
speed up the searching process, we observe two helpful properties from samples of all 
our four types of models. First, a sufficiently large (> 500) sample subset shares 
similar behaviors as the complete sample set. Second, points forming a cluster in 
Figure 4.3 have similar parameter combinations. For example, the combination (ϵ, C, 
γ) of points within the circle has a range of [2−4, 2−6] on ϵ, [24, 28] on C, and [2−2, 2−6] 
on γ. The first property enables the use of subset search; the second property allows 
for incremental search with granularity. Parameter search is critical for building SVR 
models. Using the prioritized cost function, we can systematically find a good 
 38 
 Vik+1 = wVik + c1r1�Pbest − Xik� + c2r2�Gbest − Xik� 
 Xik+1 = Xik + Vik+1 
,where k is the iteration number, w is the inertia weight, c1 and c2 are the learning 
factor, and r1 and r2 are random numbers among range [0,1]. 
 
The advantages of PSO are easy implementation, it requires only a few setting 
parameters to be adjusted, and it is capable of avoiding being trapped in a local 
optimum solution when compared with other evolutionary algorithms, such as the 
genetic algorithm (GA). 
 
Figure 4.5 illustrates the interaction between our intensified SVM-learning and 
PSO. First, PSO generates a set of training parameters required for SVM to build 
behavioral models. After building the training models, the SVM reports model’s 
accuracy to PSO as its fitness value. Based on the model’s accuracy, PSO will breed 
new generations and generate better parameters for training. This process iterates for a 
specific number of generations or until achieving a stopping criteria. 
 
 
Figure 4.5: Intensified SVM learning with PSO 
 
Besides PSO, this study uses a data reconstruction technique to reduce the size of 
the training data and greatly improve the training time and the compression ratio of 
models. This data reconstruction calculates the average value of training data in each 
block (Fig. 4.6). The red points represent the raw data from the extensive SPICE 
simulation. The green points illustrate the average values of each block. After 
reconstruction, the size of the training data is greatly reduced. Combining the 
intensified learning with data reconstruction, the framework can systematically find a 
set of high quality parameters to build accurate models. Furthermore, training time 
significantly reduces from the order of months to the order of hours. 
 40 
latching-window (Fig. 4.8). The pulse-width distributions in dotted lines are entirely 
masked. The pulse-width distributions in solid lines undoubtedly results in soft errors. 
In other words, when the lower bound of a TF distribution exceeds than the 
latching-window size, the SER from such distribution can be replaced by the 
corresponding static results. On the contrary, the SER from a distribution in the dotted 
line induced by a weaker deposited charge (its upper bound is smaller than the 
latching-window size), will be masked completely and can be ignored. Only 
distributions in dash lines require statistical analysis. 
 
 
Figure 4.8: Different pulse-width distributions versus a latching-window size 
 
Algorithm 4.1 shows the pseudocode for automatic bounding-charge selection. 
First, it chooses deposited charge q to strike a gate in the circuit and then it derives the 
upper and lower bounds of TF distributions from each Flip-Flop. After estimating the 
upper and lower bounds, the maximum upper bound and minimum lower bound can 
be found. If the maximum upper bound is smaller than the latching-window size, then 
the minimum charge (Qmin) is obtained. On the other hand, the maximum charge 
(Qmax) is decided when the minimum lower bound of TF distribution is greater than 
the latching-window size. As a result, the algorithm only considers deposited charges 
in the range of [Qmin,Qmax] for SER estimation. 
 
Algorithm 4.1: Automatic bounding charge selection 
 
 42 
 
Figure 5.1: Flowchart of closed-form block-based SSTA 
 
                     μt′ = E(t′) = E(t + d) 
                        = E(t) + E(d) = μt + μd                 (21) 
                    σt′2 = E ��t′ − E(t′)�2� 
                    = E �t′2� − (E(t′))2 
                        = E((t + d)2) − (E(t + d))2 
                        = E(t2) + 2E(td) + E(d2) − (E(t))2 −                                                                    2E(t)E(d) − (E(d))2 
                        = E(t2) − (E(t))2 + E(d2) − (E(d))2 + 2E(td) −                                                                    2E(t)E(d) 
                        = σt2 + σd2 + 2ρtdσtσd                    (22) 
 
where ρtd denotes the correlation coefficient of t and d. 
 
Visweswariah et al. [33] further used the concept of tightness probability to 
deduce the result of the max operation of two timing quantities in closed-form. The 
definition of the max operation is described as follows: 
 44 
Algorithm 5.1: Transient fault at (hitGate ci) 
 
 
The propagation stage starts after the generation stage and can be divided into 
three steps: in the first step, the breath-first search is employed to acquire the 
propagation tree Gprop of the transient fault starting from ci and terminating at any PO 
or PPO. Once a gate is visited, it is added to Gprop and the flag is set as VISITED so 
that any gate on the reconvergent gates will not be added again. After Gprop is built, all 
gates in Gprop are ranked according to their topological orders. 
 
In the second step, the initial transition signals tr0 and tf0 are propagated along 
Gprop using the propagation model Ψprop  in a block-based fashion. During 
propagation, the two conditions are handled in different ways. For the case in which 
the output pin of the current gate cj is a reconvergent fanout node (RFON), sum and 
mix (introduced in next section) operations are deployed to deal with the issue of 
convolution of transient faults. For the opposite case, only the sum operation is 
required. 
 
In the final step, the transient faults arriving at one PPO or PO are reconstructed 
by merging tr and tf, and combined pulse-width distributions are used to compute 
SER, accordingly. Details regarding Ψhit and Ψpropare described in the following 
section. 
 46 
Based on Ψhit  and Ψprop , both tr and tf are then using a parameterized 
SSTA-like method where the approximated distribution of pw can be derived by 
replacing the statistical variables μpw and σpw with the estimators μ�pw and σ�pw. 
The overall analysis is outlined as follows: 
 
1) Transient-fault generation and decomposition: Initially, the first-hit 
model Ψhit is used to look up the distribution of the initial pulse width pw0 
from a pre-characterized table according to the output load of the hit gate 
and the strength of the hitting charge. Then, the estimated pulse width pw0� 
is decomposed into two initial transitions tr0 and tf0 according to the ratio 
of their slopes. 
2) Block-based propagation: Two timing signals are updated by Ψprop 
whenever they are propagated through one gate, reflecting the gate delay. 
This step repeats until both the rising and falling signals arrive at one PO or 
PPO. 
3) Pulse-width reconstruction: Once both signals reach PO or PPO, they are 
merged to reconstruct a new transient pulse to determine whether or not a 
soft error has occurred. The reconstruction step uses the idea proposed in 
(25). 
 
Taking Figure 5.2 as an example, the original transient pulse generated by a 
particle hit at the output of G0 is split into two transition signals, which then 
individually begin their propagation. Finally, both signals end at G2 and are merged to 
reconstruct the transient pulse. 
 
 
Figure 5.2: SSTA-based method w/o considering the correlation between transition 
signals 
 
Details of each step are organized as follows. After introducing the first-hit 
model and propagation model in Section 5.3.1, the distributions of the width in a 
transient fault is estimated by the MME [35] in Section 5.3.2. The two issues related 
to correlation and reconvergence are discussed in Section 5.3.3 and 5.3.4, 
respectively. 
 48 
After the timing signal t passes through the gate, the output timing signal t′ is 
updated as t+d, enabling us to deduce t′  by a sum operation of two normal 
jointly-distributed random variables, as described in Section 5.1. Hence, a rising 
signal trin and falling signal tfin at the gate input can be propagated to the gate 
output and modeled by Ψprop. Accordingly, the two output timing signals become 
 
      trout = trin + dr         (26) 
                        tfout = tfin + df         (27) 
 
where subscripts r and f represent rising and falling, respectively, and the superscript 
(input or output) represent the pin locations. 
 
Since we have deduced the first-hit model Ψhit and the propagation model 
Ψprop, the pulse width of a transient fault can be approximated using (25). 
 
5.3.2 Estimating pulse-width parameters 
 
Given the first-hit model Ψhit  and the propagation model Ψprop , the final 
distribution of pw�  in Fig. 6 can be further expanded according to (25). That is, 
 
                       pw� = tf2 − tr2 
                          = (tf1 − df2) − (tr1 − dr2) 
                          = (tf0 − ∑ dfi2i=1 ) − (tr0 − ∑ dri2i=1 )        (28) 
 
where the superscript is the corresponding topological order originating in the hit gate. 
 
Thus, the distribution of  pw�  can be calculated by performing a series of sum 
operations over transition signals and corresponding gate delays. To derive the 
general form of a transient pulse, which is generated at one hit gate at the m-th level 
and propagated to one flip-flop at the n-th level where n > m, we can generalize (28) 
and rewrite it as: 
 pw� = tfn−m − trn−m 
                          = (tf0 − ∑ dfin−mi=1 ) − (tr0 − ∑ drin−mi=1 )      (29) 
 
5.3.3 Determining whether to consider transition correlation 
 
Correlation is a major concern when using a first-order closed-form method to 
 50 
G0 and induces a transient pulse. The transient faults then propagate along the paths 
in a block-based fashion, finally reconverging at the inputs of U0 and U1. 
Consequently, two positive transient faults appear on the output of U0, and two 
transient faults with different directions appear on the output of U1. 
 
 
Figure 5.4: Reconvergent Structure 
 
To resolve this problem of reconvergence, we propose a two-stage approach. In 
the first stage, transient faults are classified into two groups according to their 
directions. The outcomes of the pulse width and the logic probability of these 
convoluted transient faults are then derived in the second stage. The pulse-width 
distribution of convoluted transient faults is derived using a newly-defined mix 
operation in which the logic probability is updated as the union of the logic 
probabilities associated with these transient faults. 
 
1) Computing re-convergent transient faults: The reason for defining a new mix 
operation for the two timing signals is that the pulse-width result of transient faults is 
underestimated and incorrect if the traditional max operation is used to deduce the 
result of these convoluted timing signals. The process for handling multiple positive 
transient faults can be expressed as 
 
      mix(pw1, pw2, … , pwn) = mix(tf1, … , tfn) + mix(tr1, … , trn)         
(30) 
 
The mix operation with multiple (>2) operands such as in (30) is computed by 
iteratively taking the 2-operand mix. Let t′ = mix(t1, t2), t1 and t2 follow normal 
distributions, and so as t′. 
 
 mix(t1, … , tk) = mix(mix(t1, t2), … , mix(tk, tk+1)) 
 = mix(t
1,�k
2
�
, t
�
k
2
�,k) 
 52 
appearing at one input of an AND gate does not overlap with the negative transient 
fault appearing at the other input of the AND gate, the pulse-width result is the width 
of the positive transient fault pw, because the negative transient fault is completely 
masked by the controlling value on the side input. In the event of overlapping, the 
result is computed as the width of positive transient fault pw subtracted by the 
overlapping period (d) between the positive and negative transient faults due to the 
negative transient fault masking part of the positive transient fault. Other gate types 
can be derived in a similar manner. 
 
It is worth noting that because the timing information of transition signals is 
preserved, the issue of reconvergence can be analyzed in a manner that would be 
impossible in traditional SSER methods. 
 
 
Figure 5.6: The mix operation in opposite directions for AND and OR gates 
 
2) Updating logic probability: The logic probability at reconvergence fanout nodes 
should be updated to reflect the phenomenon of reconvergence. For convoluted 
transient faults, the result of logic probability is the union of the logic probabilities of 
input transient faults, because this condition is equivalent to all of these transient 
faults being able to pass through the reconvergent node.  
 
 
 54 
Chapter 6  
Experimental Results 
 
In this section, all experiments are divided into two parts. The first part is to 
examine the accuracy of the pre-characterized lookup tables, learned models and 
SSTA-like method. In the second part, they are then integrated into their respective 
statistical SER analysis frameworks and are compared in their SSER analysis 
accuracy and runtime. We use a unified framework to generate test samples for the 
pre-characterized tables, learning models, and SSTA-like method considering 
process-variation impacts. For simplicity, we only consider the with-in die geometric 
process variation and other types of variation are not included in our work. The 
perturbed gate widths and channel lengths of each transistor in geometry are used to 
model with-in die variation since they are the dominant factors for gate delay. Note 
that, the other important random variation, threshold-voltage (Vth) fluctuation, can 
also be reflected indirectly by considering the process variation. However, more 
variation sources can be considered as long as their impacts can be reflected onto 
SPICE simulation results. As illustrated in Figure 6.1, the framework first generates a 
path consisting of a random number of cells, which are connected to additional 
random cells as loadings. Using Monte-Carlo spice simulation, the transient-fault 
distributions are recorded along the path, which are later collected as test samples.  
 
 
Figure 6.1: The framework for test sample generation 
 
6.1  The table-lookup Monte-Carlo framework 
 
6.1.1 Model accuracy 
 
We build a series of pre-characterized tables for pulse width pw and voltage 
magnitude Vm with a total size of 9.5MB using about 1 month for data preparation 
and < 1 second for table construction according to Chapter 4. Used with samplings 
 56 
 
Figure 6.2: SSER comparison from static and Monte Carlo SPICE simulations, the 
proposed MC and QMC frameworks 
 
Using the proposed MC/QMC frameworks, we conduct SSER analysis on a 
variety of circuits including the ones in Figure 6.2, the ISCAS'85 benchmark circuits, 
and a series of multipliers. Table 6.2 first lists the name, the total number of nodes, 
and the total number of outputs for each circuit. The following four columns report 
the SSER values and the runtime required by the MC and QMC frameworks, 
respectively. The last two columns compute the SER difference and speedup, 
respectively, by comparing results from the MC and QMC frameworks. 
 
Table 6.2: Benchmark circuits, SER and runtime from the baseline MC and 
QMC frameworks 
 
 
From Table 6.2, SSER is clearly related to the number of nodes and primary 
outputs of a circuit , which correspond to the possibility of the circuit struck by 
 58 
the runtime required by the MC, QMC and QMC-IS (QMC + importance sampling) 
frameworks considering spatial correlation, respectively. The last column computes 
the speedup, by comparing results from the MC frameworks considering spatial 
correlation. The average difference between MC and QMC-IS is 1.68%. That 
indicates that the QMC-IS and MC frameworks are of the same quality. And From 
Table 6.4, for all benchmark circuits, the overall speedup brought by QMC is 2.55X 
in average. For all benchmark circuits, the overall speedup brought by QMC-IS is 
3.72X in average. 
 
6.2  The support-vector-regression framework 
 
6.2.1 Model accuracy 
 
We also build the SVR models for three cells with four charge strength levels. 
Assuming a 5% process-variation, each model is trained with 10K training samples. 
Then, we examine these models’ accuracy and compression ratio using another 10K 
test samples.  
 
The mean error rates and compression ratios are first categorized according to 
model and cell types in Table 6.5. Three messages are observed. (1) All mean error 
rates and compression ratios of  δstrikeµ ,  δpropµ , and  δpropσ  models are below 4% and 
4.5%, respectively. Hence, we found these models accurate and compact. (2)  δstrikeσ  
models have error rates and compression ratios around 13% and 0.4%, respectively. 
This type of model is less accurate and smaller, which means the behavior of  δstrikeσ  
may not be fully explained by its current input variables. (3) Among different cells, 
NOT has the largest mean compression ratio whereas OR has the smallest. It means 
that NOT models generally have a more complex behavior than OR models. 
 
Table 6.5: Model quality w.r.t. model type 
 
 60 
To more closely investigate the SER difference between static and statistical 
analysis, we breakdown the results in Figure 6.3 by charge strength levels, and present 
the results in Figure 6.4. Comparing the results between static and Monte-Carlo 
SPICE simulations across all test circuits, it is observed that the results of the two 
SPICE simulations and the two proposed frameworks are very similar for Q1 ∼ Q3 
parts (difference < 5%). For the Q0 part indicated by the white bars, however, the 
static SPICE simulation constantly underestimates the SER. The table-lookup 
framework performs better than the static SPICE simulation but worse than the 
SVR-learning framework. Overall, the SVR-learning framework can give slightly 
larger but closer (and more stable) results as Monte-Carlo SPICE simulation. 
 
  
Figure 6.4: SER breakdown by charge strength 
 
To further investigate the 9% SER over-estimation of the SVR learning 
framework, one transient fault along a path is particularly identified in Figure 6.5. The 
X-axis and Y-axis denote the propagation level and the standard deviation of the pulse 
width (σpw) of this transient fault, respectively. After two propagations, the σpw drops 
sharply and has not yet been fully captured by the current learning model. This 
behavior does not seem to correlate to any of our existing input variables, and caused 
larger SER estimations according to Equation (2) and Figure 2.2. Such an issue will 
be another topic worth exploration. 
 
Since running Monte-Carlo SPICE simulation with process variations for large 
circuits will be prohibitively time-consuming, we can only run both frameworks on 
large benchmark circuits. However, the entire set of circuits ISCAS’85 benchmark 
circuits, and a series of multipliers. Table 6.7 first lists the name, the total number of 
nodes, the total number of outputs, and the total number of predictions for each 
circuits. The latter columns in the table report the SER and runtime from both the 
 62 
6.3  The closed-form analysis framework 
 
6.3.1 Model accuracy 
 
Figures 6.6 and 6.7 compare the results from the probability density function 
(PDF) of transient faults induced by four particles of different charge strength in the 
proposed models and those of Monte-Carlo SPICE simulation for one AND gate and 
one OR gate, respectively. The solid line represents the PDF results of the 
Monte-Carlo simulation while the PDF results from our models are denoted by a 
dotted line. The means by which PDF results are derived using our models are very 
close to those derived using Monte-Carlo SPICE simulation, while the variances of 
PDF results derived by our models are slightly smaller (6.76% on average). 
 
  
Figure 6.6: Model accuracy of AND gates 
 
Table 6.8 summarizes the accuracy of the first-hit models and propagation 
models. The first column lists the name of the cell libraries, and the following four 
columns denote the mean and variance errors of first-hit models and those of the 
propagation models, respectively. The average mean and variance errors of our 
first-hit model are all less than 2%, as is the average mean error of the propagation 
models. The reason that the variance error associated with the propagation models is 
worse is that the shape of the hitting pulse becomes irregular during propagation. As 
shown in Figure 6.8, because the sinusoidal shape of a hitting pulse is transformed 
into a trapezoid, the variance of the flat part (like f1 and f2) of the trapezoid is hardly 
 64 
which was not considered in that framework. (2) The proposed closed-form 
SSTA-based framework yields more accurate SERs with differences of less than 3%, 
demonstrating that the proposed idea is capable of achieving superior accuracy. In 
addition, the results of Adder2bit were quite accurate, despite the inclusion of many 
reconvergence fanout nodes, demonstrating the effectiveness of our reconvergence 
handling strategy. 
 
 
Figure 6.9: Soft error rate comparison between Monte-Carlo SPICE simulation 
and this framework (CASSER) 
 
Information related to other benchmark circuits and their SSER results as well as 
runtimes derived using the two methods are listed in Table 6.9. Columns 1 to 5 denote 
the name of each circuit, the number of gates, the number of primary inputs (PI), the 
number of primary outputs, and the max topological level, respectively. The 
remaining four columns show more SSER results and runtimes derived by 
SVR-learning framework and the proposed framework on a variety of circuits, 
respectively. The last column computes the improvement in timing cost. The last six 
test cases were aborted because the runtime exceeded one day. The runtime of each 
test case using this framework was less than ten minutes except for bench7 and 
approximately half of the test cases were completed in one second. In addition, the 
timing cost grows slowly even if the circuit size grows rapidly, while that of the 
SVR-learning method increases rapidly as the circuit size increases. The runtime of 
this framework was approximately 286 times faster than that of the SVR-learning 
method. Moreover, because the proposed idea is built upon a closed-form SSTA-like 
analysis, the longer logic depth will induce a longer runtime. For this reason, c6288 
and some multipliers (mul_16 to mul_32)) required a slightly longer runtime. 
 
 66 
Chapter 7  
Conclusion 
 
Traditional SER analysis techniques try to mimic the results of static SPICE 
simulation. However, static analysis tends to increasingly underestimate true SER’s in 
the presence of process variations, especially for nanometer CMOS designs. 
Therefore, we first examined the soft-error effect beyond deep sub-micron 
technologies considering process variations. From the statistical point of view, we 
found that transient faults are not always diminishing in pulse width after propagation 
and may even become larger when reaching flip-flops. We also showed that soft 
errors originated from particle strikes with small charges can easily escape from the 
traditional static analysis.  
 
To cope with these sophisticated issues, a table-lookup Monte-Carlo framework, 
a SVR-learning framework, and SSTA-like framework are proposed, respectively. 
The first framework captures the change of transient-fault distributions implicitly 
using the Monte-Carlo method, whereas the second does the same task explicitly 
using Support Vector Regression. And the third framework considers both efficiency 
and accuracy simultaneously, this framework includes a novel idea for SSER analysis, 
in which a transient pulse is partitioned into two transition signals (one is rising 
transition and the other is falling transition). Because the two signals are expressed as 
timing quantities in closed-form, they can be analyzed using a block-based SSTA-like 
method, which considers the correlation of timing. 
 
Experimental results show that all our frameworks are capable of more 
accurately estimating SERs when comparing to the static SPICE simulation. 
Moreover, the SVR learning framework outperforms the table-lookup framework in 
terms of both SER accuracy and runtime. The runtime of SSTA-like framework is 
about 286 times faster than that of a SVR-learning framework. 
 
Statistical soft error rate (SSER) is an emerging topic. As the IC technology 
keeps evolving beyond deep sub-micron, we envision SSER analysis to become 
increasing critical for reliable scaled designs. As more and more circuit design 
projects are dedicated to automotive and biomedical applications, the circuit reliability 
issue are more and more important. Soft error has also been put into the specification 
 68 
References 
[1] P. Shivakumar, M. Kistler, S. W. Keckler, D. Burger, and L. Alvisi, “Modeling 
the effect of technology trends on the soft error rate of combinational logic,” In 
Proc. Int’l Conf. Dependable Systems and Networks. 389–398, 2002. 
[2] P. E. Dodd, and L. W. Massengill, “Basic mechanisms and modeling of 
single-event upset in digital microelectronics,” IEEE Tran. Nuclear Science 50, 
3, 583–602, 2003. 
[3] O. A. Amusan , L. W. Massengill , B. L. Bhuva, S. Dasgupta, A. F. Witulski , 
and J.R. Ahlbin, “ Design techniques to reduce set pulse widths in 
deep-submicron combinational logic,” IEEE Tran. Nuclear Science 54, 6, 
2060–2064, 2007. 
[4] H. Cha and J. H. Patel, “A logic-level model for α particle hits in cmos circuits,” 
In Proc. Int’l Conf. Circuit Design, pages 538–542, Aug 1993. 
[5] Y. Tosaka, H. Hanata, T. Itakura, and S. Satoh, “Simulation technologies for 
cosmic ray neutron-induced soft errors: models and simulation systems,” IEEE 
Tran. Nuclear Science, 46(3), 774–780, Jun 1999. 
[6] M. Omana, G. Papasso, D. Rossi, and C. Metra, “A model for transient fault 
propagation in combinational logic,” In Proc. Int’l On-Line Testing Symp., 
pages 111–115, 2003. 
[7] R. Garg, C. Nagpal, and S. P. Khatri, “A fast, analytical estimator for the 
seu-induced pulse width in combinational designs.” In Proc. Design Automation 
Conf., pages 918–923, Jul 2008. 
[8] M. Zhang and N. Shanbhag, “A soft error rate analysis (sera) methodology,”  
In Proc. Int’l Conf. Computer Aided Design, pages 111–118, Nov 2004. 
[9] B. Zhang, W.-S. Wang, and M. Orshansky, “Faser: fast analysis of soft error 
susceptibility for cell-based designs,” In Proc. Int’l Smyp. Quality Electronic 
Design, pages 755–760, Mar 2006. 
[10] R. Rajaraman, J. S. Kim, N. Vijaykrishnan, Y. Xie, and M. J. Irwin, “Seat-la: a 
soft error analysis tool for combinational logic,” In Proc. Int’l Conf. VLSI 
Design, pages 499–502, Jan 2006. 
[11] R. Rao, K. Chopra, D. Blaauw, and D. Sylvester, “An efficient static algorithm 
for computing the soft error rates of combinational circuits,” In Proc. Design 
Automation and Test in Europe Conf., pages 164–169, March 2006. 
[12] S. Mukherjee, M. Kontz, and S. Reihardt, “Detailed design and evaluation of 
redundant multi-threading alternatives,” In Proc. Int’l Symp. Computer 
Architecture, pages 99–110, May 2002. 
[13] W. Bartlett and L. Spainhower, “Commercial fault tolerance: a tale of two 
 70 
in chains of inverters-evidence for propagation-induced pulse broadening,” 
IEEE Tran. Nuclear Science 54, 6, 2338–2346, 2007. 
[26] H. Edamatsu, K. Homma, M. Kakimoto, Y. Koike, and K. Tabuchi, “Pre-layout 
delay calculation specification for cmos asic libraries,” In Proc. Asian South 
Pacific Design Automation Conf. (ASP-DAC). 241–248, 1998. 
[27] S. Weisberg. Applied Linear Regression, 3rd Edition. John Wiley and Sons, 
2005. 
[28] D. M. Bates and D. G. Watts. Nonlinear Regression Analysis and Its 
Applications. John Wiley and Sons, 1988. 
[29] V. N. Vapnik. The nature of statistical learning theory. Springer-Verlag New 
York, Inc., New York, NY, USA, 1995. 
[30] A. J. Smola, B. Scholkopf, and B. S. Olkopf. A tutorial on support vector 
regression. Technical report, Statistics and Computing, 2003. 
[31] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines 
and Other Kernel-based Learning Methods. Cambridge University Press, 2002. 
[32] J. Kennedy, and R. Eberhart, “Particle swarm optimization,” Proc. Intl Conf. 
Neural Network, pp. 1942-1948, 1995. 
[33] C. Visweswariah et al., ”First-Order Incremental Block-Based Statistical Timing 
Analysis,” in Proc. Design Automation Conf. (DAC), pp. 331 -336, 2004. 
[34] C. E. Clark, ”The greatest of finite set of random variables,” in Operation 
Research, pp. 145-162, March-April 1961. 
[35] M. Cain, ”The moment-generating function of the minimum of bivariate normal 
random variables,” in The American Statistician, vol. 48, pp. 124-125, May 
1994. 
 
  
 72 
未來，將可以透過此三種分析架構進行軟性電子錯誤率設計最佳化，設計工
程師可依其所需選取適合之分析架構，並經由這些分析架構中獲得一些容錯
設計的建議，藉此強化原先電路設計中的弱點或者架構中需要修正的特性以
期達成相容的功能性。功率及效能因素也將會在這個階段一併被考量以其達
成系統穩健性的最佳化。 
是故，過去三年的研究成果中，由於過去的研究只針對電路對輻射干擾的抵
抗性配合製程變異做分析，無法計算其全晶片錯誤率。於是在本實驗室成功
地運用物理模型結果與計算智能模型化技術，在高品質國際期刊(TODAES)發
表了全世界第一篇針對於製程變異對積體電路地軟性電子錯誤率估計的論
文，並提出統計性軟性電子錯誤率(Statistical Soft Error Rate, SSER)
的概念，希冀能誘發更多研究能量的投入。目前更進一步地準備分析空間關
係 性 (spatial correlation) 與 全 電 量 分 布 (full-spectrum charge 
collection)對電路設計的影響。 
尤其當越來越多的積體電路要使用於車載或生醫相關零件時，可靠度的要求
就更高。在圖 4.3中，SIA (Semiconductor Industry Association) 對可靠
度的預估的里程碑對 IC 長期故障率(Long Term Failure Rate) 從過去的幾
十個 FIT(Failure in Time, Failure unit, or Failure instance/Time)降
到目前只有數個 FIT(甚至希望零瑕疵)。國際車用電子協會(Automotive 
Electronics Council)也因此把軟性錯誤率的分析納入其最新的設計規範
(AEC-Q100-Rev-G)當中。而更如圖 4.4 所示，日本車廠在追求汽車的可靠度
上普遍優於歐美國家的車商。而以國內大廠 TSMC而言，到 2010 年也才有了
符合車用電子的半導體製程技術，可見其積體電路可靠度的困難度。本主持
人未來將以車載專用的 CAN 控制設計為基礎，發展以零瑕疵為目標的軟性電
子錯誤率最佳化方法，並融合其他設計可靠度/可製造性/可測性等對車載與
生醫電路影響做全面分析。 
 
 
 
 
表 Y04 
 
三、建議 
此次大會附帶舉辦的多場 Embedded Tutorials，其中 Testing Low-Power 
Integrated Circuits: Challenges, Solutions, and Industry Practices 討
論了先進製程上的積體電路功率的測試問題，提供想要進入該領域的研究人員一
個很好的綜觀瞭解。以後只要經費許可，應多國內鼓勵老師學生積極參與，拓展
我們的研究視野。此外，讓從事 EDA/Testing的學生在全英文的學術環境與產學
界專業人士共同討論可以激勵其對研究工作的嚮往與動力，並擴展國際之間交
流。 
四、攜回資料名稱及內容 
會議論文集光碟片 
 
1/1 
Diagnosing Interconnect Open Defects With 
Test-Pattern Generation 
Yen-Hou (Ian) Chen, Chia-Ling (Lynn) Chang, Jerry C.-Y. Ku and Charles H.-P. Wen 
Dept. of Electrical Engineering, National Chiao Tung University, Hsinchu, Taiwan 300 
e-mail: b91611014@gmail.com, tinger.cm98g@g2.nctu.edu.tw, jerrycyku@gmail.com, opwen@g2.nctu.edu.tw 
 
Abstract—As an open occurs on a wire segment as a defect 
in the circuit, the Byzantine effect originated from the 
coupling wires of the physical layout and the cell library 
result in complicated faulty behaviors. Many previous 
researches focus on developing the test and diagnosis 
methods for open defects while the issue of pattern quality 
has not been well-addressed. Therefore, in this paper, a 
high-resolution diagnostic framework is proposed and 
combines (1) a diagnostic test pattern generation (DTPG) 
and (2) a diagnosis flow. A precise diagnosis flow generates 
the list of defect candidates in a dictionary-based fashion 
followed by an inject-and-evaluate analysis with physical 
information to greatly reduce the candidate size for future 
silicon inspection. Experimental results show that the 
proposed framework runs efficiently and deduces nearly 
one candidate for each open-segment defect on ISCAS85 
benchmark circuits. 
I. BACKGROUND 
Open defects - the unintended breaks or electrical 
discontinuities in IC interconnect lines - are common defects 
frequently discussed in VLSI testing beyond deep submicron 
era. Along with the scaling of the manufacturing process, the 
distance between interconnects becomes narrower and thus 
induces more neighboring wires. As a result, when an open 
defect occurs in one wire segment, its coupling condition is 
more complicated.  To properly model the defect behaviors, the 
Byzantine effect [1][2] needs to be considered and states that 
the voltages of driven gates connecting an open segment are 
determined by comparing their threshold voltages and the 
floating voltage determined by its coupling condition. Figure 1 
shows the Byzantine effect of interconnect opens.  
G1 G3
G4
G2
open
          
G1 G3
G4
G2
open
S1
S3
S2
S4
S5
 
 (a) (b) 
Fig. 1: The Byzantine effect of an interconnect open 
II. A HIGH-RESOLUTION DIAGNOSTIC FRAMEWORK 
A high-resolution diagnostic framework is proposed on the 
basis of open-segment fault model. Two stages are included: 
(1) diagnostic test pattern generation (DTPG) which applies a 
modified branch-and-bound search from [3] with a SAT solver 
to generate unique patterns for each single open-segment in 
the given circuit and (2) a dictionary-based diagnosis which is 
built upon the inject-and-evaluate analysis and can reduce the 
number of fault candidates greatly.  
In our DTPG, we indirectly target each driven gate of the 
open segment instead of the segment itself directly to avoid 
ambiguities of repeated output syndromes. For Figure 1(b), 
our DTPG generates only three exclusive patterns targeting G2, 
G3 and G4, respectively. If {G2, G3} is the set of gates that 
capture faulty values, S2 is the only open segment. If {G2, G3, 
G4} is the faulty-gate set, S1 is the only open segment. The 
idea behind is not only to reduce the fault number but also to 
improve the diagnosis resolution. After DTPG, the 
information about patterns and their output syndromes are 
collected. Under the single defect assumption, we can 
diagnose the faulty circuit by a diagnosis flow proposed to 
achieve high resolution. This flow mainly consists of two 
stages: (1) a dictionary-based matching and (2) an inject-and-
evaluate pruning.  
III. EXPERIMENTAL RESULTS 
Our experiments are conducted on the ISCAS85 
benchmark circuits. After generating diagnostic patterns and 
collecting the corresponding output syndromes, the diagnosis 
stage proceeds and Table 1 shows the experimental result. For 
all ISCAS85 circuits, near one candidate that exactly matches 
the injected defect can be reported on each defective sample to 
explain the effectiveness of the proposed algorithm. 
Table 1: Diagnosis results of our diagnostic patterns 
circuit Diagnosis time (s) 
#candidate/ 
#detected resolution #patterns 
c432 11.6 91/91 1.00 292 
c499 16.0 74/73 1.01 303 
c880 41.5 83/84 1.01 615 
c1355 145.8 70/70 1.00 729 
c1908 91.9 68/68 1.00 1000 
c2670 259.8 73/70 1.04 1801 
c3540 445.2 79/79 1.00 2170 
c6288 1045.9 79/80 1.01 3444 
c7552 2424.8 84/84 1.00 4344 
IV. REFERENCES 
[1] S. Y. Huang, "Diagnosis of Byzantine Open-Segment Faults," in 
Proc. VLSI Test Symp. (VTS), pp. 248-253, 2002. 
[2] W. Zou., W. T. Cheng and S. M. Reddy. "Interconnect Open 
Defect Diagnosis with Physical Information," in Proc. Asian 
Test Symp. (ATS), pp. 203-209, 2006.  
[3] X. Lin and J. Rajski, "Test Generation for Interconnect Opens," 
in Proc. Int'l Test Conf. (ITC), pp. 1-7, 2008. 
 *This work was supported in part by the National Science 
Council, R.O.C. under Contract NSC 99-2220-E-009-039- and 
NSC 99-2220-E-009-011- ITC2011 Poster#17 
  
 
 
 
 
 
 
  
                                                         
At-a-Glance Tutorials Workshops Exhibits 
Ancillary 
Events Panels Registration Venue 
Plenary & 
Addresses 
Technical 
Papers 
Special 
Tracks Intro  i 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Become an ITC corporate supporter or utilize marketing opportunities 
 
http://www.itctestweek.org 
 
 
www.twitter.com/itctestweek 
 
S
u
n
d
a
y
 
M
o
n
d
a
y
 
T
u
e
sd
a
y
 
W
e
d
n
e
sd
a
y
 
T
h
u
rs
d
a
y
 
F
ri
d
a
y
 
Twelve Full-Day TTTC Tutorials 
A great way to prepare for the ITC Technical program     
    
Test Clinic 
Learn how to test logic and memories in SOCs. 
      
Disney Institute       
Four Panels       
Plenary Session – Keynote Address       
Disney Imagineer       
Closing Keynote Address       
55 Technical Papers       
Poster Session       
Lecture Series and Advanced Industrial Practices 
Special sessions containing introductory and broadening material 
      
World-Class Exhibits 
Free exhibits-only admission on Wednesday afternoon and 
all day on Thursday 
      
Exhibits Passport Program 
Visit booths and be eligible for daily prize drawings 
      
Corporate Presentations 
The latest technical innovations from our exhibitors and 
corporate supporters 
      
Workshops 
Finish your Test Week experience with a choice of three 
      
ITC Welcome Reception       
Fringe Technical Meetings       
 At-a-Glance Tutorials Workshops Exhibits 
Ancillary 
Events Panels Registration Venue 
Plenary & 
Addresses 
Technical 
Papers Intro 
Special 
Tracks 
                                                        Welcome Message            
Test Week Highlights 
i 
ITC Test Week 2011 3 
 
  
 
 
 
 
 
 
 
 
WEDNESDAY, SEPTEMBER  21 – TECHNICAL SESSIONS 
8:30 a.m.–10:00 a.m. Session 5 
Board Diagnosis and Safe 
Boundary-Scan Testing 
Session 6 
RF DFT and Test Cost 
Reduction 
Session 7 
Self-Testing and Test 
Compression Techniques 
Lecture 3 
Elevator Talks 
9:30 a.m.–4:30 p.m. Exhibits  
10:30a.m.–12:00 p.m. Panel 3 
In-Circuit Test (ICT): The King 
Is Dead; Long Live the King! 
Session 8 
BIST and Fault Tolerance for 
SRAM 
Session 9 
Defects in Advanced 
Technologies 
Lecture 4  
 Partner Conference Showcase 2 
Highlights from ISSCC 
12:00 p.m.–2:00p.m. Poster Session - Lunch 
2:00 p.m.–4:00 p.m. Session 10 
Pre- and Post-Silicon Validation 
for μPs and NOCs 
 
Session 11 
Taming High-Speed Digital 
Interfaces > 10 Gbs 
Session 12 
Timing- and Power-aware DFT 
Session 13 
Microprocessor Testing 
4:30 p.m.–5:30 p.m. Disney Imagineer Manufacturing a Disney Spectacular 
 
 
THURSDAY, SEPTEMBER 22 – TECHNICAL SESSIONS 
8:30 a.m.–10:00 a.m. Session 14 
DFT for Complex SOCs 
Session 15 
Learning from Data: Diagnosis and Data 
Mining 
Advanced Industrial  Practices 2 
Electrical Validation from First Chip to 
Product 
9:30 a.m. – 1:00 p.m.            Exhibits  
10:30 a.m.–12:00 p.m. Session 16 
Advancing Mixed-Signal Test 
Session 17 
Stacked Device Test 
Advanced Industrial  Practices 3 
Adaptive Test in Production 
 
12:00 p.m.– 1:00 p.m. Lunch  
1:00 p.m.– 2:00 p.m. Keynote  A Systems Perspective on the R&D of Industrial Technology 
2:00 p.m. – 3:30 p.m. Panel 4   The Gap: Test Challenges from the Asia Manufacturing Field and Today's Tools  
 
THURSDAY, SEPTEMBER 22 – WORKSHOPS 
4:00 p.m. – 6:30 p.m. Testing Three-Dimensional Stacked ICs Silicon Debug and Diagnosis Defect and Adaptive Test Analysis 
 7:00 p.m. – 9:00 p.m. Workshop Reception 
 
FRIDAY,SEPTEMBER 23 – WORKSHOPS 
8:00 a.m. – 4:00 p.m. Testing Three-Dimensional Stacked ICs Silicon Debug and Diagnosis Defect and Adaptive Test Analysis 
  
 At-a-Glance Tutorials Workshops Exhibits 
Ancillary 
Events Panels Registration Venue 
Plenary & 
Addresses 
Technical 
Papers Intro 
Special 
Tracks 
                                                           Sunday–Tuesday          
i 
ITC Test Week 2011  5 
 Test Week At-a-Glance 
  
 
 
Sunday  8:30 a.m.– 4:30 p.m. 
 
TUTORIAL 4    
Power-aware Testing and Test 
Strategies for Low-Power Devices 
Presenters P. Girard, N. Nicolici,  
X. Wen 
Description Managing the power 
consumption of circuits and systems is now 
considered as one of the most important 
challenges for the semiconductor industry. 
Elaborate power management strategies, such 
as voltage scaling, clock gating or power 
gating techniques, are used today to control 
the power dissipation during functional 
operation. The usage of these strategies has 
various implications on manufacturing test, 
and power-aware test is therefore 
increasingly becoming a major consideration 
during design-for-test and test preparation for 
low power devices. This tutorial provides 
knowledge in this area. It is organized into 
three main parts. The first one gives 
necessary background and discusses issues 
arising from excessive power dissipation 
during test application. The second part 
provides comprehensive knowledge of 
structural and algorithmic solutions that can 
be used to alleviate such problems. The last 
part surveys low-power design techniques 
and shows how these low power devices can 
be tested safely without affecting yield and 
reliability. EDA solutions for considering 
power during test and design-for-test are also 
discussed in the last part of the tutorial.
 
 
 
 
 
 
TUTORIAL 5 
The Convergence and Inter-
relationship of Yield, Design-for-
Manufacturability and Test 
Presenters S. Venkataraman, R. Aitken  
Description The tutorial goal is to show how 
design-for-yield (DFY) and design-for- 
manufacturability (DFM) are tightly coupled 
into what we conventionally think of as test. 
As process geometries shrink, the line 
between defects and process variation blurs 
to the point where it is essentially non-
existent. As feature sizes reduced to 90 nm 
and below, systematic mechanism-limited 
yield loss began to appear as a substantial 
component in yield loss due to the interaction 
between design and manufacturing. The 
basics of yield and what fabs do to improve 
defectivity and manage yield are described. 
DFM techniques to analyze the design 
content, flag areas of design that could limit 
yield, and make changes to improve yield are 
discussed. In DFM/DFY circles, it is 
common to speak of defect-limited yield, but 
it is less common to think of test-limited 
yield, yet this concept is common in DFT 
(e.g. IDDQ testing, delay testing). Test 
techniques to close the loop by crafting test 
patterns to expose the defect-prone feature 
and circuit marginality through ATPG, and 
by analyzing silicon failures through 
diagnosis to determine the features that are 
actually causing yield loss and their relative 
impact are covered. This tutorial will provide 
background needed for DFT practitioners to 
understand DFM and DFY, and see how their 
work relates to it. The ultimate goal is to spur 
attendees to conducting their own research in 
the area, and to apply these concepts in their 
jobs. 
 
 
 
 
 
                                                               Monday TTTC Full-Day      Wednesday Embedded 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Workshops Exhibits
Ancillary 
Events At-a-Glance Pan ls Registration Venue 
Plenary & 
Addresses 
Technical 
Papers Intro 
Special 
Tracks Tutorials i 
                                                                 Monday Full-Day Tutorials    Test Clinic  Disney Institute 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
TTTC Full-Day Tutorials 
<  previous Sunday tutorials 
 
Discount Rates! 
Register by  
September 2 
ITC Test Week 2011  7 
 
   
 
 
 
 
 
Monday  8:30 a.m. – 4:30 p.m. 
. 
 
TUTORIAL 10    
Statistical Adaptive Test Methods 
Targeting "Zero Defect" IC Quality 
and Reliability 
Presenters A. Singh 
Description Integrated circuits have 
traditionally all been tested identically in the 
manufacturing flow with little sharing of test 
results between the different test insertions. 
However, as the detection of subtle 
manufacturing flaws becomes ever more 
challenging and expensive in aggressively 
scaled nanometer technologies, innovative 
new statistical screening methods are being 
developed that attempt to improve test 
effectiveness and optimize test costs by 
subjecting ―suspect‖ parts to more extensive 
testing, and also adaptively bring in 
additional tests that target the suspected 
failure mode. The idea is analogous to 
selective security screening approaches 
applied at airports. Such statistical methods 
fall into two broad categories: those that 
exploit the statistics of defect distribution on 
wafers, and those that exploit the correlation 
in the variation of process and performance 
parameters on wafers. This tutorial presents 
test methodologies that span both these 
categories, and illustrates their effectiveness 
with results from a number of recently 
published experimental studies on production 
digital and analog circuits from IBM, Intel 
and LSI, Analog Devices and NXP 
Semiconductor. Commercial tools offered by 
a number of new companies that have 
emerged in the "Adaptive Test" space will 
also be discussed. Broadly, these aim to 
provide support for the sharing and 
leveraging of results from the different tests 
in the test flow for effective test adaptation 
and optimization. 
TUTORIAL 11  
The Economics of Test and 
Testability 
Presenters S. Davidson, H. Colby,        
L. Ungar  
 
Description Test economics provides a way 
of quantifying the costs and benefits of test, 
and helps a test engineer choose an effective 
test strategy. Classical microeconomics is far 
more sophisticated than what is found in test 
economics papers. Recent work in behavioral 
economics, known to the public through 
bestsellers such as Freakonomics and 
Predictably Irrational, has shown that 
classical assumptions about the behavior of 
economic actors are wrong. This tutorial will 
summarize existing work in test economics, 
provide background on microeconomic and 
behavioral economics concepts that are of 
interest to test and DFT engineers, and will 
show their applicability to test. The student 
will emerge with the ability to do traditional 
cost and benefit modeling, and with a deeper 
understanding of the economic principles that 
affect the cost and benefits of test. The 
researcher will emerge with the tools to make 
a much better case for the benefits of 
proposed research. 
 
TUTORIAL 12  
Testing Memories in the Nano-Era: 
Fault Models, Test Algorithms, 
Industrial Results, BIST and BISR 
Presenter S. Hamdioui,  
A.J. Van de Goor, S. Gregor 
Description The objective is to provide 
attendees with an overview of fault modeling, 
test design, BIST and BISR for memory 
devices in the nano-era. Traditional fault 
modeling and recent development in fault 
models for current and future technologies 
are covered. Systematic methods are 
presented for designing and optimizing tests, 
supported by industrial results from different 
companies (e.g., Intel, ST) and for different 
technology nodes (e.g., 0.13 um, 65 nm). 
Impact of algorithmic (e.g., data-background) 
and non-algorithmic (e.g., voltage) stresses is 
explored in order to get better insight in the 
test effectiveness. Novel BIST architectures 
are covered; special attention is given to the 
optimization of address generator designs as 
they typically consume considerable BIST 
area overhead. BISR and redundancy analysis 
are also discussed. Moreover, CPU based 
memory test—which is in some applications 
the only resource to perform at least the 
Power-on test is addressed. Finally, future 
challenges in memory testing are highlighted. 
 
 
  
6 
Venue 
<  previous Monday tutorials 
Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses 
Technical 
Papers Intro 
Special 
Tracks 
                                                            Sunday Full-Day Tutorials    Test Clinic   Disney Institute         
  
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Tutorials 
ITC Test Week 2011  9 
 TTTC Full-Day Tutorials 
 i 
  
                       
 
 
 
 
 
 
 Exhibits hours: Tuesday 10:30 a.m. – 5:30 p.m.  
    Wednesday 9:30 a.m. – 4:30 p.m., Thursday 9:30 a.m. – 1:00 p.m. 
    
 
 
ITC is offering free exhibits-only registration to visit the exhibit hall during all exhibit hours. 
Onsite registration for this special opportunity begins on Tuesday at the ITC registration  
area in the Disneyland Hotel Conference Center. Lunch is not included with free admission. 
 
 
 
Fill in your Exhibit Hall Passport for Prizes. 
 
All registered ITC attendee will receive a passport with their conference 
totes.  Get your passport stamped while visiting exhibitor booths, drop  
your completed passport into the box on the exhibit floor, and be eligible  
for daily drawings for an iPod and an iPod  Shuffle. Winners may choose 
instead a Disney store gift certificate of equal value. 
 
 
Visit the international exhibition that includes the latest 
high-technology test, design and service products. 
Venue 
                             Exhibitors       Corporate Presentations     
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
ITC Test Week 2011  11 
 
Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses 
Technical 
Papers Intro
Special 
Tracks i 
Exhibits 
  
 
 
 
                                                 
Tuesday    9:00 a.m. – 10:30 a.m. 
 
Opening Remarks 
      William Eklow, ITC General Chair 
 
ITC 2010 Best Paper Awards Presentation 
      Erik Volkerink, ITC 2010 Program Chair 
 
Keynote Address 
     Power, Programmability and Granularity: The Challenges of ExaScale Computing  
     Bill Dally, Bell Professor of Engineering, Stanford University, Chief Scientist, NVIDIA Corporation 
 
 
 
Reaching an ExaScale computer by the end of the decade, and 
enabling the continued performance scaling of smaller systems 
requires significant research breakthroughs in three key areas: 
power efficiency, programmability, and execution granularity. To 
build an ExaScale machine in a power budget of 20 MW requires 
a 200-fold improvement in energy per instruction: from 2 nJ to 
10 pJ.  Only 4X is expected from improved technology.  The 
remaining 50X must come from improvements in architecture 
and circuits.  To program a machine of this scale requires more 
productive parallel programming environments—that make 
parallel programming as easy as sequential programming is 
today.  Finally, problem size and memory size constraints prevent 
the continued use of weak scaling, requiring these machines to 
extract parallelism at very fine granularity—down to the level of 
a few instructions.  This talk will discuss these challenges and 
current approaches to address them.  
 
 
About the speaker: Dr. Dally is the Willard R. and 
Inez Kerr Bell Professor of Engineering at Stanford 
University and Chief Scientist at NVIDIA Corporation. 
Bill and his group have developed system architecture, 
network architecture, signaling, routing and 
synchronization technology that can be found in most 
large parallel computers today. While at Bell Labs, Bill 
contributed to the BELLMAC32 microprocessor and 
designed the MARS hardware accelerator. At Caltech he 
designed the MOSSIM Simulation Engine and the Torus 
Routing Chip which pioneered wormhole routing and 
virtual-channel flow control. While a Professor of EECS 
at the Massachusetts Institute of Technology, his group 
built the J-Machine and the M-Machine, experimental 
parallel computer systems that pioneered the separation of 
mechanisms from programming models and demonstrated 
very low overhead synchronization and communication 
mechanisms.  At Stanford University his group has 
developed the Imagine processor, which introduced the 
concepts of stream processing and partitioned register 
organizations.  Bill has worked with Cray Research and 
Intel to incorporate many of these innovations in 
commercial parallel computers, with Avici Systems to 
incorporate this technology into Internet routers, co-
founded Velio Communications to commercialize high-
speed signaling technology, and co-founded Stream 
Processors, Inc. to commercialize stream processor 
technology.   He is a Member of the National Academy of 
Engineering, a Fellow of the IEEE, a Fellow of the ACM, 
and a Fellow of the American Academy of Arts and 
Sciences.  He has received numerous honors including the 
ACM Eckert-Mauchly Award, the IEEE Seymour Cray 
Award, and the ACM Maurice Wilkes award.  He 
currently leads projects on computer architecture, network 
architecture, and programming systems. He has published 
over 200 papers in these areas, holds over 75 issued 
patents, and is an author of the textbooks, Digital Systems 
Engineering and Principles and Practices of 
Interconnection Networks. 
 
Registration Venue 
ITC Test Week 2011 13 
 
 
Plenary & 
Addresses 
                                                                                  Closing Keynote Address     Disney Imagineer 
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Technical 
Papers Intro 
Special 
Tracks 
Plenary & Keynote Address 
i 
   
 
 
 
 
 
 
 
Wednesday   4:30 p.m. – 5:30 p.m. 
 
 
 
 
 
Manufacturing a Disney Spectacular 
 
Chuck Davis, Disney Creative Entertainment Senior Technical 
Director 
 
Chuck discusses the creation of the technical aspects of the 
spectacular show World of Color, which is performed nightly on 
Paradise Bay at Disney California Adventure. This 60-minute 
lecture will take participants on a little-seen journey through the 
design, fabrication, installation and mounting process of World of 
Color. We will discover how the teams use normal 
manufacturing principles and process to insure the eventual 
outcome is safe, reliable, maintainable, and financially viable, 
while still delivering on the highest of creativity. Participants will 
see how the show starts from a creative idea, imagined to provide 
an ―only at Disney‖ emotional experience. They will understand 
how this creative experience is the touchstone to which all project 
decision-making is derived, and how principles of high-
reliability, self-diagnostics, redundancy, self-healing, safety and 
leveraging cutting edge technology are spun together to support 
the show’s creation. We will also see how these systems are used 
to maintain the show, as well as keep the creative vision fresh 
and up-to-date. 
Please join us for a rarely seen backstage view of this truly 
unique nighttime spectacular. 
 
 
 
 
 
 
 
 
About the speaker:  Chuck began his Disney career in 
1996 after successful stints in the worlds of professional 
ballet and education. He currently serves as Sr. Technical 
Director for creative entertainment and is responsible for 
spectaculars, atmosphere and blue-sky projects. He has 
been a key player in the creation of many of the most 
technically sophisticated nighttime spectaculars Walt 
Disney Parks and Resorts has to offer worldwide. Chuck 
leads the automation and advanced technologies teams for 
DLR entertainment. Past projects include Remember 
Dreams Come True, Believe there is Magic in the Stars, 
Believe in Christmas Magic, Fantasmic, Innoventions and 
Disney in the Stars. 
  
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
  
 
Plenary & 
Addresses 
                                                      Plenary/Opening Keynote Address   Closing Keynote Address 
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Technical 
Papers Intro 
Special 
Tracks 
ITC Test Week 2011  15 
 Disney Imagineer  
i 
   
 
 
 
 
 
 
 
 
8:30 a.m. – 10:00 a.m. 
 
SESSION 5 
Board Diagnosis and Safe Boundary Scan 
Testing   
T. Chakraborty, Qualcomm (Chair) 
 
5.1  Smart Diagnosis: Efficient Board-level 
Diagnosis and Repair Using Artificial 
Neural Networks 
Z. Zhang, K. Chakrabarty, Duke University; 
Z. Wang, Z. Wang,  X. Gu, Huawei 
 
5.2  Surviving State Disruptions Caused by 
Test: A Case Study  
K. Parker, Agilent Technologies; 
S. Kameyama, Fujitsu and Ehime University; 
D. Dubberke, Intel  
  
5.3  IEEE Std. 1581—A Standardized Test 
Access Methodology for Memory 
Devices 
H. Ehrenberg, GOEPEL Electronics; 
B. Russell, Technical Consultant 
 
SESSION 6 
RF DFT and Test Cost Reduction  
P. O'Brien, Analog Devices  (Chair) 
 
6.1  Multisite Test of RF Transceivers on 
Low-Cost Digital ATE 
I. Koren, B. Schuffenhauer, F. Demmerle, 
F. Neugebauer, G. Pfahl, Intel; 
D. Rautmann, Infineon Technologies  
6.2  Wafer Probe Test Cost Reduction of an 
RF/A Device by Automatic Testset 
Minimization—A Case Study 
D. Drmanac, L. Wang, University of 
California, Santa Barbara; 
M. Laisne, Qualcomm  
 
6.3  Accurate Signature-driven Power- 
conscious Tuning of RF Systems 
Using Hierarchical Performance 
Models 
A. Banerjee, S. Sen, S. Devarakond 
A. Chatterjee,  Georgia Institute of 
Technology  
 
 
SESSION 7 
Self-Testing and Test Compression 
Techniques  
R. Parekhji, Texas Instruments (India), (Chair) 
7.1  Low-Power Compression Utilizing 
Clock Gating 
E.K. Moghaddam, S.M. Reddy University of 
Iowa; J. Rajski, Mentor Graphics 
7.2  Partial State Monitoring for Fault 
Detection Estimation 
Y. Shi, Brown University; K. Kaewtip, UCLA; 
W-C. Hu, MStar Semiconductor; 
J. Dworak, Southern Methodist University 
7.3  Logic BIST Silicon Debug and Volume 
Diagnosis Methodology 
E. Amyeen, A. Jayalakshmi, S. Venkataraman, 
S. Pathy,  E. Tan, Intel 
 
10:30 a.m. – 12:00 p.m. 
 
SESSION 8 
BIST and Fault Tolerance for SRAM 
S. Hamdioui, Delft University of 
Technology (Chair) 
 
8.1  Generic, Orthogonal and Low-Cost 
March Element-based Memory BIST 
A.J. van de Goor, ComTex; S. Hamdioui,  
H. Kukner, Delft University of Technology 
 
8.2  On Using Address Scrambling to 
Implement Defect Tolerance in SRAMs 
R. Alves Fonseca, L. Dilillo, A. Bosio, 
P. Girard, S. Pravossoudovitch, 
A. Virazel, LIRMM; N. Badereddine, Infineon 
Technologies   
 
8.3  A Fully Cell-based Design for Timing 
Measurement of Memory 
S-Y. Huang, Y-C. Chang, C-W. Tzeng, 
 National Tsing Hua University, Taiwan;  
 J. Yao, Elite Semiconductor Memory 
Technology  
 
 
SESSION 9 
Defects in Advanced Technologies 
E. Amyeen, Intel (Chair) 
 
9.1   Cell-aware Analysis for Small-Delay 
Effects and Production Test Results 
from Different Fault Models 
F. Hapke, J. Schloeffel, W. Redemund, 
A. Glowatz, J. Rajski, Mentor Graphics; 
M. Reese, J. Rearick, J. Rivers, AMD  
 
9.2  Lithography-aware Critical Area 
Estimation and Yield Analysis 
V. Suresh, P. Vijayakumar, 
S. Kundu, University of Massachusetts 
 
9.3  Using Well/Substrate Bias Manipulation 
to Enhance Voltage-Test-based 
Defect Detection 
A. Gattiker, P. Nigh, IBM 
 
 
 
 
 
  
 
 
 
 
 
 
 
  
 
                                                Tuesday      Wednesday P.M.     Thursday      
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Technical 
Papers Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Special 
Tracks i 
ITC Test Week 2011 17 
 Wednesday A.M. 
 
Discount Rates! 
Register by  
September 2 
   
 
 
 
 
 
 
8:30 a.m. – 10:00 a.m. 
SESSION 14 
DFT for Complex SOCs 
J. Dworak, Southern Methodist University (Chair)  
 
14.1  EDT Channel Bandwidth Management 
in SOC Designs with Pattern-
independent Test Access Mechanism 
J. Tyszer, J. Janicki, Poznan University of 
Technology; A. Dutta, M. Kassab, 
G. Mrugalski, N. Mukherjee, J. Rajski, Mentor 
Graphics  
 
14.2  A Novel Test Access Mechanism for 
Failure Diagnosis of Multiple Isolated 
Identical Cores 
M. Sharma, A. Dutta, W-T. Cheng, 
B. Benware, M. Kassab, Mentor Graphics 
 
14.3  Techniques to Improve Memory 
Interface Test Quality for Complex 
SOCs 
VR. Devanathan, S. Vooka, Texas Instruments 
(India)  
 
SESSION 15 
Learning from Data: Diagnosis and Data 
Mining 
 V. Mehta, NVIDIA (Chair) 
 
15.1  Die-level Adaptive Test: Real-Time 
Test Reordering and Elimination 
K. Gotkhindikar, R. Daasch, Portland State 
University; K. Butler, J. Carulli, Jr., 
A. Nahar, Texas Instruments 
 
15.2  Forward Prediction Based on Wafer 
Sort Data—A Case Study 
N. Sumikawa,  D. Drmanac, L. Wang,  
UC-Santa Barbara; L. Winemberg, 
M. Abadir, Freescale Semiconductor 
 
15.3  Deterministic IDDQ Diagnosis Using a 
Net-Activation-based Model 
A. Kun, R. Arnold, P. Heinrich, 
G. Maugard, Infineon Technologies; 
H. Tang,  W. Cheng, Mentor Graphics 
 
10:30 a.m. – 12:00 p.m. 
 
SESSION 16 
Advancing Mixed-Signal Test 
N. Ben-Hamida, Ciena (Chair)  
 
16.1  A Novel Robust and Accurate Spectral 
Testing Method for Noncoherent 
Sampling 
S. Sudani, D. Chen, Iowa State University; 
M. Wu, Xi’an Jiaotong University 
 
 
16.2  Application of a Continuous-Time 
Level-Crossing Quantization Method 
for Timing Noise Measurements 
T. Yamaguchi, Advantest Laboratories; 
M. Soma, University of Washington; 
T. Aoki, Tohoku University; Y. Furukawa, 
K. Degawa, Advantest; K. Asada, M. Abbas, 
S. Komatsu, University of Tokyo 
 
16.3  Adaptive Multidimensional Outlier 
Analysis for Analog and Mixed-Signal 
Circuits 
E. Yilmaz, S. Ozev, ASU; K. Butler, Texas 
Instruments 
  
 
 
SESSION 17 
Stacked Device Test 
A. Yiin, Intel (Chair)  
 
17.1  Pre-Bond Probing of TSVs in 3-D 
Stacked ICs 
B. Noia, K. Chakrabarty, Duke University 
 
17.2  Evaluation of TSV and Micro-Bump 
Probing for Wide I/O Testing 
K. Smith, P . Hanaway, M. Jolley, 
R. Gleason, E. Strid, Cascade Microtech; 
T. Daenen, L. Dupas, B. Knuts, 
EJ. Marinissen, M. Van Dieval, IMEC 
 
17.3  Post-Bond Testing of 2.5D-SICs and 
3D-SICs Containing a Passive Silicon 
Interposer Base 
C-C. Chi, National Tsing-Hua University; 
EJ. Marinissen, IMEC; SK. Goel, TSMC; 
 C-W. Wu, National Tsing-Hua University 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
16 
                                  Tuesday      Wednesday A.M.      Wednesday P.M.        
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Technical 
Papers Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Special 
Tracks 
ITC Test Week 2011 19 
Thursday 
i 
   
 
 
 
 
 
 
 
 
 
The corporate presentation track allows you to stay on top of the latest commercial products in the semiconductor test industry and helps 
you understand how the innovations behind the products can add value to your work. The corporate track allows you to gain an in-depth 
understanding of how some of the technology innovations presented at ITC impact the product portfolios of companies. In this 
interactive forum, ITC exhibitors and supporters will make presentations describing their company, its products and product roadmaps. 
Company representatives are free to hand out relevant literature such as papers or marketing material. Typical presentations include case 
studies, best practices and testimonials. 
 
Corporate presentations are scheduled for Tuesday, September 20. Optimal Test, our Diamond Sponsor, will present from 11:00 a.m. to 
12:00 p.m. Afternoon presentations, 20 minutes in length each, will be given between 1:00 p.m. and 5:00 p.m. Check back in July for 
the detailed presentation schedule. 
   
 
 
 
Tuesday 11:00 a.m.–12:00 p.m. 
                                         
11:00 a.m.  OptimalTest 
OptimalEnterprise™ Solution–A Customer’s 
Experience 
 
Tuesday 1:00 p.m.–5:00 p.m. 
 
1:00 p.m.  Advantest 
 
1:20 p.m.  OpenATE, Inc. 
OpenATE's 16-site System for a Complete, 
Lowest-Cost Motion Sensor 
(Gyroscope/Accelerometer) Test Solution 
 
1:40 p.m.   Corelis 
Celebrating 20 Years of Test Innovation 
 
2:00 p.m.   GOEPEL Electronics 
Multidimensional JTAG/Boundary-Scan 
Instrumentation for Enhanced Test 
 
 
 
Tuesday 2:20 p.m.–4:00 p.m. 
 
2:20 p.m.  Cadence Design Systems  
A Different Synthesis and Test Flow – 
Cadence Encounter Test 
 
2:40 p.m.  Roos Instruments 
High-Speed Phase Noise Measurements  
                    of VCOs using ATE 
 
3:00 p.m.  Johnstech 
Optimize Critical Test Objectives Using 
Kelvin-Ready Configurability 
 
3:40 p.m.  Test Insight 
Production Test Life Cycle Challenges 
 
 
  
Registration Venue 
 Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Technical 
Papers 
Special 
Tracks i 
                                                                     Lecture Series/Adv. Industrial Practices     Posters                                               
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
ITC Test Week 2011  21 
 Corporate Presentations 
   
 
 
 
 
 
 
Monday, 4:30 p.m. – 6:00 p.m. 
 
PANEL 1   Industry Leaders Panel―How Will Testing Change in the Next 10 Years?                                               
P. Nigh, IBM (Moderator/Organizer) 
We will ask a set of industry test experts to answer a set of questions to understand how the testing industry will change in the next 10 years. Fundamental questions will 
address how test equipment, design-for-test, EDA software, test steps/processes―and the companies that support these―will be changing. The discussion will start with a 
small set of questions that each panelist must address during their opening statements. Questions from attendees will be solicited before the session. What is the biggest 
problem in the industry that gets little discussion ? How will be requirements for ATE change in the next 10 years? How will fundamental design-for-test requirements 
change? What is the best area for creating a new business in the "test field"? Will design reconfiguration at test or new adaptive test applications change how we test? 
End-to-end testing (wafer probe through field)―will we develop methods to truly optimize across all steps? 
Panelists:  B. Cory, NVIDIA • W. Eklow, Cisco Systems • D. Josephson, Intel • R. Madge, GLOBALFOUNDRIES 
• J. Rajski, Mentor Graphics • E. Volkerink, Advantest Verigy Group  
 
Tuesday, 4:00 p.m. – 5:30 p.m. 
 
PANEL 2  Challenges and Best Practices in Advanced Silicon Debug                                                              
J. Rearick, AMD (Moderator) • J. Zeng, AMD (Organizer) 
In the deep submicron technology node, it is impossible to bring a complex chip design to production without going through a number of respins. Silicon debug is 
becoming one of the most crucial stages that affect the time-to-market of semiconductor designs these days. Debug effort to improve performance and reduce power 
consumption is traditionally done based on functional/system tests, which can be very expensive. Structural testing, including at-speed scan tests and test-structure-based 
parameter tests etc, on the other hand, can provide useful information of the performance and power related issues that facilitate the post-silicon design optimization 
strategy of the design team. This panel will discuss whether structural-based tests, including at-speed scan tests, test-structure-based parametric tests etc., can have a 
greater role to play in debugging performance and power related issues. The advantages and disadvantages of system test versus structural test for validation will also be 
explored. 
Panelists:  
 
Wednesday, 10:30 a.m. – 12:00 p.m. 
 
PANEL 3  In-Circuit Test (ICT): The King Is Dead; Long Live the King!                   
D. Dubberke, Intel (Moderator) • B. Balangue, Jr. Agilent Technologies (Organizer) 
 
The objective of the panel is to have a good honest discussion from the board test industry experts about the future of ICT. The panel consists of experts from various 
parts of the industry and groups them according to the following: 1. board test managers/experts from the original equipment manufacturing (OEM),  contract 
manufacturing (CM) and original design manufacturing (ODM) side who strongly believe that the current ICT system is insufficient to test the new generation of PCBA; 
2. ICT marketing managers/experts from ICT suppliers (Agilent/Teradyne/TRI) that believe that the current ICT system has enough capabilities and features to maintain 
test coverage and cost needs for new generation of PCBA;  3. board test managers/experts at OEM/CM/ODM who are dependent on ICT system as their main board test 
manufacturing strategy and have invested substantial ICT equipment and infrastructure in their manufacturing. 
 
Panelists: S. Butkovich, Cisco Systems • P. Geiger, Dell •  K. Parker, Agilent Technologies • T. Suto, Teradyne   
 
Thursday, 2:00 p.m. – 3:30 p.m. 
 
PANEL 4   The Gap: Test Challenges from the Asia Manufacturing Field and Today's Available Tools                                                           
X. Gu, Huawei Technologies (Moderator/Organizer) 
 
Today more and more electronic manufacturing is moving to Asia. Test, as one of the important parts of the manufacturing process, is used to guarantee the product 
quality and manufacturing smoothness. By presenting the gap between the test challenges that the Asian companies are facing and the tools they have available today, we 
hope to give the ITC community an opportunity to better understand the needs for innovation in test technologies and tools. 
 
Panelists: J. Cho, Hynix Semiconductor, R. Fang, Cisco Systems (China), J. Qian, AMD, J. Yun, Huawei Technologies 
 
 
 
 
Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Technical 
Papers 
Special 
Tracks 
 
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
 i 
ITC Test Week 2011  23 
 Panels 
Registration Venue 
  
  
 
 
 
 
 3D-TEST: 2nd IEEE International Workshop on Testing Three-Dimensional Stacked ICs 
 
Scope: The second 3D-TEST Workshop focuses exclusively on test of and design-for-test for three-dimensional stacked ICs (3D-SICs), 
including systems-in-package (SIP), package-on-package (POP), and especially 3D-SICs based on through-silicon vias (TSVs). While 
3D-SICs offer many attractive advantages with respect to heterogeneous integration, smaller form-factor, higher bandwidth and 
performance, and lower power dissipation, there are many open issues with respect to testing such products. The 3D-TEST Workshop 
offers a forum to present and discuss these challenges and (emerging) solutions among researchers and practitioners alike. Topics to 
include: 
 
Defect due to wafer thinning 
Defect due to intro-stack interconnects 
DFT Architecture for 3D-SICs 
Failure Analysis for 3D-SICs 
Known-good die/stack testing 
Reliability for 3D-SICs 
Standardization for 3-D testing 
Test cost modeling for 3D-SICs 
Test flow optimization for 3D-SICs 
Tester Architecture for 3D-SICs 
TSV test, redundancy, and repair
 
General Chair: Yervant Zorian, zorian@viragelogic.com    
Program Chair: Erik Jan Marinissen, erik.jan.marinissen@imec.be 
 SDD: 7th IEEE International Workshop on Silicon Debug and Diagnosis 
Scope: Troubleshooting how and why systems and circuits fail is important and is rapidly growing in industry significance. Debug and 
diagnosis may be needed for yield improvement, process monitoring, correcting the design function, failure-mode learning for R&D, or 
just getting a working first prototype. This detective work is, however, very tricky. Sources of difficulty include circuit and system 
complexity, packaging, limited physical access, shortened product creation cycle and time-to-market. New and efficient solutions for 
debug and diagnosis have a much needed and highly visible impact on productivity. SDD is the seventh of a series of highly successful 
technical workshops that consider issues related to debug and diagnosis of semiconductor circuits and systems—from prototype bring-
up to volume production. Topics to include: SDD vs. Yield & TTM 
Debug techniques and methodologies 
Design and debug 
DFT reuse for debug and diagnosis 
Manufacturing and prototype environment 
Debug standardization 
Case studies 
Microprocessor, FPGA, IP, SOC debug 
Infrastructure IP for SDD 
System-level debug and diagnosis 
Emulation and hardware accelerator 
Cross-geography turn-on, debug & diagnosis 
SDD vs. Yield and TTM
 
General Chair: Teresa McLaurin, Teresa.McLaurin@arm.com  
Program Chair: Ismed Hartanto, Ismed.Hartanto@xilinx.com 
 
 
 DATA:  IEEE Workshop on Defect and Adaptive Test Analysis 
 
Scope: New initiatives in getting more out of testing have opened up new avenues of research and development in the areas of 
extracting information about defects and IC behavior through the use of innovative analysis techniques. As the need for these novel 
processes is becoming more widely accepted in the industry, new questions about how these techniques should be executed and 
controlled  in production, the types and sizes of database requirements, and even the format of test data and storage itself are being 
reviewed and discussed. The definition of what is ―Adaptive testing‖ is still being reviewed and defined. Closing the knowledge gap 
about these issues, the process, new test techniques, database requirements, and how defect models are being used to adapt test flows 
will be the goals of this year’s DATA workshop. Paper presentations on topics related to the topics listed below are expected  to generate 
active discussion on the challenges that must be met to ensure high IC quality through the end of the decade. 
 
Outlier identification 
Data-driven testing 
Test data analysis 
Adaptive testing 
Data mining methods for test data processing 
High/low voltage and stress testing 
Noise and crosstalk testing 
Nanometer test challenges 
Defect coverage and metrics 
Mixed-current/voltage testing 
Economics of defect-based testing 
Fault localization and diagnosis
   
General Chair: Jeff Roehr, JLRoehr@Gmail.com 
Program Chair: Sankaran Menon, Sankaran.Menon@intel.com 
  
 Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses 
Technical 
Papers Intro 
Special 
Tracks 
                                                          Registration & Schedules        
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Mon ay: 12 o 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Workshop Summaries 
i 
ITC Test Week 2011 25 
 
  
 
 
 
 
 
 
ITC Welcome Reception 
Tuesday, September 20, 6:00 p.m. – 9:30 p.m. 
Disney California Adventure 
 
 
 
 
 
 The 2011 ITC Welcome Reception will take place at the Disney California Adventure, part of the Disneyland Resort. The 
evening begins in A Bugs Land, a popular attraction reserved for our exclusive use. Guests can socialize with friends old 
and new while enjoying food, drinks and fun elements of this attraction. The second part of the Welcome Reception will be 
a private showing of the spectacular World of Color, a new Disney nighttime show that premiered in June 2010. World of 
Color has more than 1,200 fountains and includes lights, water, fire, fog, and lasers, with high-definition projections on mist 
screens accompanied by musical scores. Please note that this does not include general admission to other park areas. 
 
Each full-conference ITC attendee will receive one free admission to the event. For all other Test Week attendees and/or 
companions, the admission fee is $42 per person. Extra admissions may be purchased during online registration or onsite 
at the ITC registration desk. 
 
                      
 
                                                                              Fringe Technical Meetings 
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Technical 
Papers 
Special 
Tracks i 
ITC Welcome Reception 
ITC Test Week 2011  27 
 
  
 
 
 
 
 
 
 
 
 
 
 
 Conference and 
Tutorials 
 
Workshops 
 
Exhibitors 
Sun, Sept 18 7:30 a.m. – 5:00 p.m.  7:30 a.m. – 5:00 p.m. 
Mon, Sept 19 7:30 a.m. – 5:00 p.m.  7:30 a.m. – 5:00 p.m. 
Tues, Sept 20 7:30 a.m. – 7:00 p.m.  7:30 a.m. – 7:00 p.m. 
Wed, Sept 21 7:30 a.m. – 5:00 p.m.  7:30 a.m. – 5:00 p.m. 
Thurs, Sept 22 7:30 a.m. – 1:30 p.m.  2:00 p.m. – 7:00 p.m. 7:30 a.m. – 12:30 p.m. 
Fri, Sept 23   7:30 a.m. – 10:00 a.m.  
 
Need More Registration Information?  
Contact the ITC office 
2025 M Street, NW, Suite 800, Washington, DC 20036, USA 
Tel. +1 202.973.8665    Fax. +1 202.331.0111 
 
 
 
 
 
 
 
 
  
 
                                                     Categories & Fees        Publications           Hotel Reservations 
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Technical 
Papers 
Special 
Tracks 
Registration Hours 
i 
ITC Test Week 2011  29 
 
   
 
 
 
 
 
 
 
 
Reserve a Disneyland Hotel room online by clicking the button above.  
 
1. Rooms may be reserved for the period from September 13, 2011 to September 27, 2011. 
 
2. All room must be guaranteed with a major credit card. 
 
3. Cancellation: Guests must cancel their reservation more than 72 hours prior to arrival (three full days prior 
to the scheduled date of arrival). Cancellations may be subjected to a penalty fee equal to one night’s room 
rate and tax if less than 72 hours notice is given.  
 
4. The reservation cutoff date is September 2, 2011 at 5:00 p.m. EDT. Reservations made after that date will be 
made at the ITC rate on a space-available basis. 
 
5. Discounted Disneyland theme park tickets may be purchased when you make your reservation. The online store for 
tickets closes September 12, 2011. Tickets are valid September 13 – 26, 2011 
 
6. Parking for hotel guests is $15/night for self-parking and $22/night for valet parking. Follow the "Hotels" signs 
directly to your destination; do not park in a theme park lot. Those staying at the Disneyland Hotel should park in 
the Fantasy parking lot. 
 
7. Sleeping rooms provide free Internet access.                     
 
Click here for more hotel information, location and driving instructions. 
 
 
 
 
 
 
Disneyland Hotel Rate 
          (exclusive of taxes and fees)         
  
                 Standard Room  $169.00  
 
 
 
 
 
                                      
 
 
 
 
 
 
Message to Attendees: ITC has made every effort to secure the best possible group nightly room rate for you at this event. That rate results from a  
negotiated overall package of event  needs such as sleeping rooms, meeting room space and other requirements. Contracts with the venue include a  
provision to reduce event costs if ITC meets or exceeds its minimum sleeping room block guarantee. Conversely, event costs will increase if ITC falls  
short of its minimum room block guarantee. Please help ITC keep the costs of this event as low as possible by booking your housing needs at the  
designated host hotel and through the reservation process created by ITC. Reserving elsewhere means you are booking outside the contracted room  
block, jeopardizing ITC's ability to meet its contracted obligations and to keep registration fees to a minimum. ITC appreciates your support and  
understanding of this important issue. Thank you.  
 Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration Venue 
Plenary & 
Addresses Intro 
Technical 
Papers 
Special 
Tracks 
                                                     Categories & Fees       Registration Hours        Publications         
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
ITC Test Week 2011  31 
 
Online Hotel Reservations 
Hotel Reservations 
i 
   
 
 
 
 
 
 
 
1. The Advance Program was generated with Adobe Acrobat 8.2.6 on 1-September-2011. 
 
2. The program will be updated periodically as new material is available—check back often. 
 
3. Navigate using the tabs at the top of each page. 
 
4. Use underlined links in the At-a-Glance to find specific items.  
 
5. Most of the papers have a ―summary.‖ Place your cursor over the paper number to see it. 
 
                           3.3 Paper Title 
 
 
 
6. For more information contact: 
 
 
Subject Contact Email 
Advance Program 
Don Denburg 
Scott Davidson 
testweek@rcn.com 
scott.davidson@oracle.com 
Advanced Industrial Practices Rob Aitken rob.aitken@arm.com 
Corporate Presentations Ken Mandl kenneth.mandl@teradyne.com 
Exhibits and Exhibiting 
Bill Lowd 
Mike Purtell 
bzintrnatl@aol.com 
m.purtell@ieee.org 
Fringe Technical Meetings Courtesy Associates itc@courtesyassoc.com 
Hotels Connections Housing jay@connectionshousing.com 
Lecture Series Rob Aitken rob.aitken@arm.com 
Plenary and Invited Talks Tim Cheng timcheng@ece.ucsb.edu 
Registration Courtesy Associates itc@courtesyassoc.com 
Support and Advertising Cassandra Koenig  Cassandra.koenig@verigy.com 
Technical Papers and Panels Shawn Blanton blanton@ece.cmu.edu 
TTTC Tutorials Yervant Zorian zorian@viragelogic.com 
Workshops Yervant Zorian zorian@viragelogic.com 
All Other Questions Courtesy Associates  
 
 
 
 i Tutorials Workshops Exhibits 
Ancillary 
Events At-a-Glance Panels Registration 
Plenary & 
Addresses Intro 
Technical 
Papers 
Special 
Tracks 
 
 
 
 
 
 
 
 
      Invited Addresses   
 
 
     Monday: 9 to 11      Monday: 12 to 16 
 
Sunday–Tuesday     Wednesday      Thursday-Friday      
Venue 
ITC Test Week 2011  33 
 Information 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：溫宏斌 計畫編號：99-2220-E-009-011- 
計畫名稱：後次微米時代新興電子設計自動化技術之研究--子計畫四：應用計算智慧推理處理後深次
微米時代電路設計上的可靠度挑戰(3/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100% 
在高品質國際期
刊 (TODAES) 發 表
了全世界第一篇
針對於製程變異
對積體電路地軟
性電子錯誤率估
計的論文，並提出
統計性軟性電子
錯 誤 率
(Statistical 
Soft Error Rate, 
SSER)的概念 
研究報告/技術報告 3 3 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 8 8 100%  
國外 
參與計畫人力 
（外國籍） 博士生 2 2 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
由於近年來在先進的電路設計上軟性錯誤被發現的頻率越來越高，商業電子產品的可靠度
有了新的挑戰。而製程變異使得軟性電子錯誤變得難以分析，本計畫成功地研究了在深次
微米時代下，軟性電子錯誤率的行為。不僅發現了在製程變異下，軟性電子錯誤率的行為
與以往傳統上的分析不同，且於本計畫也建立了三種架構來分析在製程變異下軟性電子錯
誤率的複雜行為。此三種架構分別為 1)蒙地卡羅分析(Monte-Carlo)架構 2)機器學習
(machine learning)分析架構 3)閉合模式(closed-form)分析架構。此三種架構各有其優
缺點，在蒙地卡羅分析架構中，提供了相當高的準確率來預測軟性電子錯誤率之行為，但
其缺點為所需計算時間也較長。而閉合模式分析架構中，提供了相當快的計算，但其精準
度就稍微差了一點。而機器學習分析架構中，其運算速度及精準度都介於蒙地卡羅分析架
構及閉合模式分析架構之間。 
未來，將可以透過此三種分析架構進行軟性電子錯誤率設計最佳化，設計工程師可依其所
需選取適合之分析架構，並經由這些分析架構中獲得一些容錯設計的建議，藉此強化原先
電路設計中的弱點或者架構中需要修正的特性以期達成相容的功能性。功率及效能因素也
將會在這個階段一併被考量以其達成系統穩健性的最佳化。 
是故，過去三年的研究成果中，由於過去的研究只針對電路對輻射干擾的抵抗性配合製程
變異做分析，無法計算其全晶片錯誤率。於是在本實驗室成功地運用物理模型結果與計算
智能模型化技術，在高品質國際期刊(TODAES)發表了全世界第一篇針對於製程變異對積體
電路地軟性電子錯誤率估計的論文，並提出統計性軟性電子錯誤率(Statistical Soft 
