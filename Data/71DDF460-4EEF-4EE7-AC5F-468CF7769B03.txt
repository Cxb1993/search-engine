 
擷取與合成細部表情於三維擬真人臉動畫之研究 
Extracting and Synthesizing Detailed Expressions for 
Realistic 3D Facial Animation 
計畫編號：NSC 94-2213-E-009 -151 
執行期間： 94 年 8 月 1 日 至  95 年 7 月 31 日 
計畫主持人：林奕成　 國立交通大學資訊工程系助理教授 
 
中文摘要 
三維人臉動畫一直是電腦圖學界中頗受注目且重要的研究課題。如今，藉由動態捕捉
技術的輔助，人臉上的特徵點動作可由追蹤貼在其上的標記點來估算。將之應用於動畫上，
產生人臉動畫的時程可因此大幅縮短。現有的動態捕捉技術雖能掌握特徵點的運動軌跡，
但無法擷取人臉表面細微的變化，如皺紋、酒窩等。日常生活中，我們常藉由這些細微的
部分來判斷他人的情緒，如能逼真地合成出這些細微表情變化，對於人臉動畫擬真度可大
為提升。 
在此計畫中，我們針對此關鍵的問題，提出結合擷取人臉細部表情的方法以應用於三
維人臉表情編輯。首先，由受測者臉部的標記點，以立體電腦視覺配合通用人臉模型（generic 
face model）形變的方式，估計出受測者的初步幾何臉部模型。在已校正光源的環境中，採
用 Lambertian 等的光照模型，人臉表面的細部法向量變化可以由影像的色澤變化來計算出。 
藉由收集多個不同表情之細部臉部表面，我們將這些樣本形成一由特徵點群位置來控
制之臉部表情空間。當使用者調整臉部模型的特徵點，我們的系統將利用最佳化的方式，
推估最適當的臉部細部表情變化。現有的動態捕捉系統亦可利用本研究的技術，使其增加
展現人臉細部表情的能力。 
 
關鍵字：三維人臉動畫、動態捕捉技術、細部人臉表情合成、表面重建。 
1. Introduction 
Facial animation has been an attractive research topic for a long time. From movies, games, to 
virtual communication interfaces, nowadays, these research works come to our daily lives.  
Example-based and physical-based approaches are two major approaches to drive facial models. 
Example-based approaches usually animate faces according to motion capture (Mocap) data or 
synthesize novel expressions from images or video samples. 
Motion capture techniques estimate motion trajectories of feature points on real persons. Subjects 
have to be pasted a few markers on the faces. 3D positions of markers can be reconstructed by 
stereo vision or electromagnetic positioning techniques. The estimated motion trajectories can 
then be used to driven face meshes. This approach, however, can’t acquire facial details like 
wrinkles. In order to generate facial details, a lot of labor-intensive post-processing works are 
required. Artists have to adjust meshes or textures to mimic wrinkles according to different 
expressions. Another appearance-based approach is image-based synthesis which animates faces 
from sets of image or video samples. Facial details are usually well preserved in this approach, 
but it is difficult to relight the model and view directions are limited. 
The goal of the proposed work is to build an expression edition system that can synthesize not 
only geometry approximation but surface details. To avoid complex simulation, we adopt to 
capture face details from real persons. We also observed that variations of surface details are 
highly related to adjacent feature points. Therefore face details can be synthesized according to 
positions of feature points. 
 
2. Related work 
Many researches about facial animation have been proposed these decades. Video Rewrite [C. 
Bregler 97] synthesized new movie sequences from existing footages. The results are visually 
convincing but relighting and changing view directions are difficult. In 2001, Z. Liu and Y. Shan 
proposed expression ratio images [Z.Liu 01]. They took the advantage of the property that 
expression ratios of subjects are the same under the Lambertian lighting model. Therefore, the 
expression in a source image can easily be retargetted to a new face by scaling. Like other 
image-space approach, Lighting and view directions of their work are limited. 
V. Blanz et al. [99 and 03] used a large amount of scanned faces to build a morphable model. 
They assumed that human faces can be synthesized by convex combination of prototypes. By 
minimizing intensity differences of an input image and the projection of the morphable model, a 
target 3D model can be reconstructed. In the synthetic step, users can also adjust corresponding 
weight of each scan for exaggerated results. 
Zhang et al. [Zhang 06] proposed a geometry-driven approach to synthesize expressions of a 
particular subject. They assumed the expressive textures on a face are related to geometric 
variation. Their system can generate corresponding textures when users adjust geometric features. 
This concept is similar to our proposed work. But we focused on extraction, synthesis, and 
We assumed that the color of the skin is almost uniform. Based on this assumption, the variations 
of skin color in the image is due to variations of the incident light directions. 
As shown in Fig. 2, let S be the unit vector of the light source direction. To evaluate the surface 
normal of a pixel in the image, first, we have to estimate a projection vector Gxy as shown in Eq.1. 
                        SSIIG xyxyxy )( ⋅∇−∇=                            (1) 
where /,/( yIxII ∂ )0,xyxyxy ∂∂∂=∇ t and  means the projection of vec
 to  S. The cosine of the angle between the surface normal and the 
dent light direction can be evaluated as follows: 
C(x, y) = )( II
 is the image gradien tor 
 the plane perpendicular to
inci
minmaxmin IIxy
xyG
xyI∇
)/(− −   
where , the darkest inten , implies ambient light in the scene and the brightest 
, im
rma
sity valueminI
value, I plies the intensity when a pixel faces the light source. 
The no l N
max
xy can be estimated as Eq. 2.  
xyxyxy GGyx /),                   (2) SSyxCN (),( +=
xyG
xyI∇  
Figure 1: We project the gradient of image to the plane perpendicular to the incident light vector. 
constructing the normal of the subject, we would like to get the normal difference map 
umption, we will have "fault" normals caused by eyebrow, acne, 
The normal of the pixel can be calculated through cosine and sine function estimated from the 
image. 
After re
which represents the normal variations due to a facial expression. The image nI is indicated the 
normal map of the neutral face and the image eI  indicated the normal map of an expression e.g. 
raising eyebrow, laughing, etc.  
Under our uniform skin color ass
color markers, etc. in normal maps (as shown in Fig.3(a)(b)). Notwithstanding, these fault 
Subject to: for i = 0,1,…,m, ∑
=
≥=
m
i
ii cc
0
0,1
The objective function of the optimization problem above can be rewrite as: 
RRRTT GGgCGgCgC
TT +− 2       (3) 
where , . This optimization is a positive semi-definite 
quadratic programming problem with linear constraints. The problem can be solved by various 
methods such as interior-point method or active set method. In our approach, we used the active 
set method to solve this optimization problem. 
),...,,( 10
R
m
RR GGGg = ),...,,( 10 mcccC =
 
4. Result and conclusion 
We propose a 3D expression editing tool where facial details can be synthesized according to 
control points. Since we use the variations of normals to represent face details, the relighting 
problem is overcome and realism can be improved in many interactive applications. For example, 
this approach can make virtual faces more expressive in real-time application like game or 
telecommunication. For synthesizing novel expressions by manual works, editing feature points 
to drive wrinkles is very intuitive and user-friendly. Moreover, we can retarget the synthetic 
results to another subjects or even animals. There are two disadvantages in the current system. In 
normal recovery phase, the recovered normals are usually with unavoidable noise and it may lead 
odd synthetic wrinkles. We will use bilateral filters in the future. Second, currently, our system 
doesn't take into account temporal consistency. We will include temporal penalty terms for 
animation. 
(a) (b)
(c) (d) 
Figure 4. (a) the neutral face (b)(c)(d) synthetic expressions according to configurations of 
feature points 
 可供推廣之研發成果資料表 
□ 可申請專利  5 可技術移轉                                      日期：95 年 10 月 30 日 
國科會補助計畫 
計畫名稱：擷取與合成細部表情於三維擬真人臉動畫之研究 
計畫主持人：林奕成　 國立交通大學資訊工程系助理教授    
計畫編號：NSC 94-2213-E-009 -151 學門領域：計算機 
技術/創作名稱 擷取與合成細部表情於三維擬真人臉動畫 
發明人/創作人 林奕成、莊棨元、林昭自、羅永盛 
技術說明 
中文： 
三維人臉動畫一直是電腦圖學界中頗受注目且重要的研究課題。如
今，藉由動態捕捉技術的輔助，人臉上的特徵點動作可由追蹤貼在
其上的標記點來估算。現有的動態捕捉技術雖能掌握特徵點的運動
軌跡，但無法擷取人臉表面細微的變化，如皺紋、褶紋、酒窩等。
在此計畫中，我們針對此關鍵的問題，提出結合擷取人臉細部表情
的方法以應用於三維人臉表情編輯。首先，由受測者臉部的標記
點，以立體電腦視覺配合通用人臉模型（generic face model）形變
的方式，估計出受測者的初步幾何臉部模型。在已校正光源的環境
中，採用 Lambertian 等的光照模型，人臉表面的細部法向量變化可
以由影像的色澤變化來計算出。 
藉由收集多個不同表情之細部臉部表面，我們將這些樣本形成一由
特徵點群位置來控制之臉部表情空間。當使用者調整臉部模型的特
徵點，我們的系統將利用最佳化的方式，推估最適當的臉部細部表
情變化。 
現有的動態捕捉系統亦可利用本研究的技術，使其增加展現人臉細
部表情的能力。此研究亦可增快人臉動畫的製作速度。 
附件二 
