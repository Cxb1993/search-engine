2 
 
作業系統虛擬化網路效能改善之研究 
馮立琪 
長庚大學資訊工程系 
lcfeng@mail.cgu.edu.tw 
 
摘要 
隨著硬體製造技術的進步，電腦主機所
具備的運算能力逐漸提昇。硬體資源強大，
應用於網路電腦主機的使用量亦大增。但為
了同時兼顧持續性與可靠性，這些電腦主機
往往被單獨用來使用於操作網路。昂貴的伺
服器等級主機將會增加了企業的成本，對於
電腦的硬體資源而言，亦無法達到較好的價
格效能比。使用虛擬化技術將可解決此一問
題，近年來受到日益廣泛的重視。 
虛擬化技術是透過一層虛擬化操作環
境的服務層，使單一硬體資源得以被虛擬成
多分，這些被虛擬出的操作環境分別各自獨
立。就虛擬機器中的軟體而言，其所擁有的
虛擬硬體資源宛如擁有一台實體電腦主機。 
本計劃針對虛擬化環境中造成網路效
能低落的因素進行研究，並在虛擬機器監控
程式 Xen系統上進行效能改善的實作。我們
找到目前 Xen虛擬化網路中的效能瓶頸，針
對這樣的瓶頸提出解決辦法，初步的效能評
量顯示有 10%的提升。 
關鍵詞: 虛擬化、虛擬機器監控程式、網
路效能、Xen  
Abstract 
With progressive hardware production 
technolodgy, computational ability of 
computer keeps rasing. There are more and 
more applications try to let one machine 
providing several software service. Many of 
which are based on network infrastrutcure 
supporting. But the external network 
infrastructure could be hamful to system 
reliability and lead performance degrdation 
unfortunately. It is trend to achive isolation 
with virtualization. 
By means of a service layer virtualizing 
operating environment, single physical box is 
treated as multiple active instance 
concurrently. User software excutes inside just 
like owns full system resource. 
This project investgate problem that 
cause network performance down, we try to 
propose a method to reduce the CPU 
processing time during network packet 
processing. 
Keywords : Virtualization, Virtual Machine 
Monitor, Network Performance, Xen 
1、 前言 
在過去由於大型主機價格昂貴，許多
企業基於成本的考量，往往會在同一部機器
上提供分時多工的作業環境，以讓不同的使
用者於同一個硬體平台上各自進行操作。然
而，在這樣的使用環境下，系統常因為單一
使用者有意或無意的不當操作，造成主機系
統停擺，或是在程式開發的過程中，必須執
行重新開機，使得系統上其他使用者無法正
常使用各項服務。有鑑於此，許多企業寧願
耗費巨額的成本來添購主機設備，以求設置
獨立的作業環境，讓使用者之間的操作相互
隔離(Isolation)而互不影響。 
然而，並非所有的企業均有能力負擔
如此龐大的成本於主機的添購上，因此，
IBM 於 1960 年提出虛擬機器(Virtual Ma-
chine，簡稱 VM)的概念，希望藉由虛擬機
器的架構使同一硬體平台能夠擁有彼此獨
立的作業環境，進而降低企業添置主機的成
本。 
由 IBM 所提出的虛擬機器架構，將虛
4 
 
出最佳化方式是實做單位為 header 大小的
共享記憶體陣列。純資料部份則是利用
scather/gather I/O 機制完成；另一方面，
Receive時提出的策略為：以標準MTU大小
作為共享記憶體陣列的單位。此外，一個
MTU 大小不足以塞滿一個共享的記憶體分
頁，因此以批次處理的方式增加一次處理的
封包量，根據減少存取共享記憶的原理增進
效能。我們認為共享的記憶體在多個作業系
統之間無法動態管理，可能造成低使用度的
虛擬機器記憶體儲存空間的閒置與浪費；而
高使用度的虛擬機器則頻繁發生分配不足。 
4、 研究方法 
導致 Xen效能低落的關鍵因素，主要是
由於系統中頻繁發生的兩種 exception：系統
呼叫(System Call)與 Page Fault。目前針對系
統呼叫的改善，Xen提供的解決方式為『Fast 
Exception Handler』。其作法是讓上層執行的
Guest OS 可向 Xen 註冊 Fast Exception 
Handler，使 Guest OS 本身具有處理
exception 的能力，而無須再經由 Xen 代為
處理。 
另外 Asynchronous Event 的設計也與
Hypercall 造成的 overhead 有關連。由於某
些資源只有 Xen擁有權限操作(如記憶體)，
為了減少產生出的 overhead 所可能造成的
效能低落，Xen 提供上層的 Guest OS 累積
需要 Xen 一次呼叫的介面，利用減少中斷
Guest OS 執行次數的策略企圖降低影響程
度。也就是說，Guest OS 中的系統呼叫
Hypercall可能被延後執行，於是完成服務的
訊息必須延遲並累積起來一次遞送，讓
Asynchronous Event完成。 
依照我們初步的研究分析，網路效能低
落的原因除了上述理由之外，主要和 I/O虛
擬化的技術以及網路處理的方式有關。 
Xen中裝置的存取共享則透過 domain0
統一處理 Asynchronous Buffer Descriptor 
Ring 中的周邊操作需求完成。Descriptor 
Ring 是個存在於 domain0-domainU 之間底
共享記憶體組合。由於只有 domain0可以直
接操作驅動程，於是 domain0中包含了為每
個 domainU 產生虛擬裝置的功能，而
domain0與 domainU就是利用 descriptor ring
傳遞訊息。domainU將虛擬周邊裝置的操作
需求封裝為 descriptor 後推入 ring 中。
descriptor 內容隨著裝置類型有所不同，但
相同的是都包含 domainU 的識別資訊，在
網路 I/O中不同的 domains可能使用不同的
MAC address。隨後 Domain 0 將根據 Xen
的 CPU排程法則，取得 CPU 使用權後操作
這些需求。domain0 完成後跟據 descriptor
中描述的 domain 資訊，封裝 response 推入
ring中，一旦要求的 domain被 CPU排程器
叫醒，取得 CPU 執行權後即可獲得處理後
的結果。 
Xen 的網路裝置共享架構，使用
split-driver的模式將網路卡驅動程式之上，
再 堆 疊 上 一 層 虛 擬 抽 象 層 ， 以
backend-frontend 的對映方式分別存在於
domain0與 domainU中。兩個模組之間使用
稱為 Event Channel 的機制溝通。Event 
Channel 操作的資料結構就是 Asynchronous 
Descriptor Buffer。在 domain0 中操作的
backend 負責接收 frontend 遞送過來的操作
需求，這些操作包括初始化 domainU 中的
虛擬網路介面、要求傳送封包等。為了讓
backend能正確的傳達 domainU有封包接的
事件，backend 包括了根據虛擬的 MAC 
address 與 domainU 建立對映並搜尋的
bridge module。 
 
圖一 Xen Network I/O 架構及 Rx處理流程 
Xen在 Network I/O虛擬化所採用方式
6 
 
Mbps 網路卡。 
以 netperf作為 benchmark時。我們測
試 tcp接收時的 throughput。Enhanced Linux
與 Native Xen的比較圖，效能測試結果如下
圖。 
 
圖一 網路 throughput測試圖 
其中 Enhanced Xen 是我們修改了使用
跨 domain共享記憶的版本，Native Xen則
是未經修改的版本。從實驗結果顯示，比較
了使用 8192 Bytes作為 socket buffer大小的
tcp stream throughtput testing時，分別傳送不
同 大 小 的 message 形 成 的 結 果 。
Benchmark#1 為 1024 bytes、Benchmark#2
為 2048 bytes、Benchmark#3為 4096 bytes。
Enhanced Xen 產出 throughput 為 471.29、
479.38、484.1；Native Xen 則是 447.61、
435.29、 463.5，比較之下約有 10 %的
throughput提昇。 
6、 計畫成果自評 
系統虛擬化的技術不僅有助於提昇系
統資源的使用率，降低資訊系統的複雜度並
節省 IT 成本的支出，同時亦可協助程式的
移植與測試，縮短軟體開發時程，進而提昇
軟體研發之品質。如今網路相關的應用繁
多，虛擬化環境中的網路效能尤其重要。 
本計劃以Xen作為虛擬化系統之效能改
善的實作對象，期望以此研究個案為例，提
供虛擬化平台一個提昇系統中網路效能的
解決方案。我們找到目前 Xen虛擬化網路中
的效能瓶頸，針對這樣的瓶頸提出解決辦
法，初步的效能評量顯示有 10%的提升。 
參考文獻 
[1] A. Awadallah, M. Rosenblum, "The vMatrix: A 
Network of Virtual Machine Monitors for Dy-
namic Content Distribution", 7th International 
Workshop on Web Content Caching and Distri-
bution (WCW 2002), Boulder, Colorado, August 
2002. 
[2] D. A Rusling, “The Linux Kernel”, 
http://www.linuxdoc.org/LDP/tlk/ 
[3] K. Buchacker, H.J. Hoxer, V. Sieh, "Implement-
ing a User Mode Linux with Minimal Changes 
from Original Kernel", 9th International Linux 
System Technology Conference, September 4-6, 
2002. 
[4] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. 
Harris, A. Ho, R. Neugebauer, I. Pratt, A. War-
field, " Xen and the Art of Virtualization ", 
Symposium on Operating Systems Principles, 
2003. 
[5] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. 
Engler, “An empirical study of operating system 
errors”, In Proc. of the 18th ACM Symposium on 
Operating System Principles, Banff, 
da, ,Oct. 2001.J. Smith and R. Nair, “An Over-
view of Virtual Machine Architectures”, Morgan 
Kaufmann Publisher, 2004. 
[6] T. Garfinkel, et al, “Terra: A Virtual Ma-
chine-based Platform for Trusted Computing”, 
The 9th Workshop on Hot Topics in Operating 
System, 2003. 
[7] P. H. Gum,“System/370 Extended Architecture: 
Facilities for Virtual Machines,” IBM Journal of 
Research and Development, vol. 27, no. 6, p. 
530, 1983. 
[8] N. Lawson, “Designing and Attacking Virtual 
Machine”, Cryptography Research, Inc., 2004. 
[9] Joshua LeVasseur, Volkmar Uhlig, Jan Stoess, 
Stefan Gotz, “Unmodified Device Driver Reuse 
and Improved System Dependability via Virtual 
Machines”, 6th Symposium on Operating System 
Design and Implementation, OSDI 2004. 
