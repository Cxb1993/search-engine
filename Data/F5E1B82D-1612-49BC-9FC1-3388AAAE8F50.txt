 i
摘要 
  近年來 multiview video coding 之興起，導因於 3D 顯示技術與多重攝影機陣列
(multiple camera arrays)應用之發展，而使用可攜性設備的多媒體服務，則已是現代
人生活習慣的趨勢，因此，若欲透過無線網路傳輸 multiview video 並在接收端維
持良好的視訊品質，則必須解決多重景觀視訊串流資料量遠大於單一景觀 (single 
view) 資料量的問題，於此，高效率視訊壓縮技術的採用與新一代寬頻無線存取網
路的支援是不可或缺的。 
  目前的視訊壓縮編碼標準技術在減少輸入視訊之 psychovisual redundancy 時，
僅開發人類視覺敏感度之特性，而未考慮主觀的人類心理視覺模型，而當導引編碼
器運作與決策時，是以客觀的視覺量測取代主觀的視覺量測來達成。另一方面，在
寬頻無線網路的 QoS 支援下，當面臨網路傳輸資源動態的變化，傳送端若能隨之
調整編碼串流位元率，則可增進接收端視訊序列的觀看品質。 
  因此，本計畫執行期共兩年，第一年設計以人類心理視覺模型為導向，發展快
速之 scalable video coding 技術，第二年設計則發展快速之 multiview video coding
技術。此兩年之研究執行成果目前已成功發表於三篇國際會議論文，未來將以此兩
年之設計方案為基礎，延伸設計適用於寬頻無線網路的 multiview video coding 
system，以有效降低視訊壓縮編碼位元率，卻同時維持解壓縮端良好之視訊視覺品
質的目標。 
 
 
關鍵字: 可調式視訊編碼、快速區塊模式決定、人類感知特性、視訊品質可調性、
空間可調性、多視角視訊編碼器、快速畫面間預測方向決策演算法、緩慢移動區
塊、時間軸方向預測。 
 iii
目錄 
摘要 .................................................................................................................................. i 
Abstract ........................................................................................................................... ii 
目錄 ................................................................................................................................ iii 
第一章  第一年計畫成果 ....................................................................................... 1 
一、摘要............................................................................................................... 1 
二、前言 ............................................................................................................... 1 
三、研究目的....................................................................................................... 2 
四、文獻探討....................................................................................................... 2 
五、可調式視訊編碼架構................................................................................... 3 
六、本計畫所設計之方案................................................................................... 6 
七、實驗結果與討論........................................................................................... 9 
八、結論............................................................................................................. 10 
參考文獻............................................................................................................. 10 
第二章  第二年計畫成果 ..................................................................................... 13 
一、摘要............................................................................................................. 13 
二、前言 ............................................................................................................. 13 
三、研究目的..................................................................................................... 14 
四、文獻探討..................................................................................................... 14 
五、多視角視訊編碼架構................................................................................. 15 
六、本計畫所設計之方案................................................................................. 16 
七、實驗結果與討論......................................................................................... 19 
八、結論............................................................................................................. 20 
   1
第一章、第一年計畫成果 
一、摘要 
可調式視訊編碼器詳盡的巨集區塊搜
尋方式可達理論上最佳的編碼效率，卻也
造成了龐大的計算複雜度。因此，如何降
低可調式視訊編碼器之計算複雜度為非常
重要的關鍵課題。在本專題計畫中，我們
考量人類視覺系統和可調式視訊編碼器之
階層間巨集區塊模式的相關性，設計一個
全新的快速巨集區塊模式決策之演算法。
首先，我們使用動態注意力模型找出畫面
中易受注意的區域。對於這些區域，沿用
原來可調式視訊編碼器中詳盡的搜尋方
式，以保持最佳的視訊品質。對於不易受
注意的區域，則採用我們所提出的快速巨
集區塊模式決策演算法達成加速的效果。
在我們的實驗中，我們採用 JSVM8.10 來
實現所設計的快速演算法。實驗結果顯示
在最多增加 1.7506%位元率及最大損失
0.1173dB PSNR 的情況下，我們所設計的
方案節省了 21.53%到 45.89%的編碼時
間。而這些視訊品質損失的區域為我們不
容易察覺的區域。 
關鍵字: 可調式視訊編碼、快速區塊模式
決定、人類感知特性、視訊品質可調性、
空間可調性。 
Abstract 
The exhaustive search scheme for 
macroblock mode decision in the working 
draft of scalable video coding extension of 
H.264/AVC achieves theoretically optimal 
coding efficiency. However, this scheme 
also accompanies high computation 
complexity. Thus, for scalable video coding, 
how to reduce heavy computation load while 
there is minor bit rate increase and PSNR 
loss is a critical issue. In this project, we 
propose a fast macroblock mode decision 
algorithm for scalable video coding. We 
consider the psychovisual issue and 
employing the correlations of macroblock 
partitions between base layer and 
enhancement layer. First, we adopt the 
original search scheme for macroblock 
mode decision in the base layer. The visually 
attended regions are identified based on the 
motion vectors in the base layer. Then, fast 
mode decision is applied to motion 
unattended regions in the spatial/CGS 
enhancement layer for time savings. It 
reduces mode candidates by exploiting the 
mode decisions in the previously encoded 
enhancement/base layer to predict the mode 
decisions in the current enhancement layer. 
Experiments conducted by JSVM8.10 
exhibit that the proposed scheme saves 
21.53% to 45.89% encoding time while 
there is a maximum of 1.7506% bit rate 
increase. It is noticed that there is a 
maximum of 0.1173 dB PSNR loss while 
the regions suffer from such loss are visually 
unattended. 
Keywords – Scalable video coding, fast 
mode decision, psychovisual characteristics, 
CGS scalability, spatial scalability. 
二、前言 
ITU-T Video Coding Experts 
Group(VCEG)和 ISO/IDC Moving Picture 
Experts Group(MPEG)將 AVC/H.264 視訊
編碼標準作延伸，發展出可調式視訊編碼
(Scalable Video Coding) [1]。事實上，可
   3
性來降低目前編碼空間階層的巨集區塊候
選區域。在[8-11]，利用基礎層及加強層之
間的巨集區塊模式相關性設計快速可調式
視訊編碼演算法。在[8-10]，所設計的快速
可調式視訊編碼限制只能使用在一個基礎
層及一個加強層。假設基礎層的巨集區塊
模式為空間編碼(intra-coded)，則加強層的
巨集區塊模式為空間編碼的機率也非常高
[8-12]。空間加強層 (spatial enhancement 
layer)的巨集區塊模式通常相似於基礎層
同位置的巨集區塊模式[8][10][11]。視訊品
質加強層(SNR enhancement layer)的巨集
區塊模式往往小於基礎層同為位置的巨集
區塊模式[8][10][11]。藉由空間階層預測
(inter-layer prediction)的移動向量方向，減
少目前空間階層巨集區塊模式的移動向量
候選方向中人們容易的區域。在[16]，使
用光流方向區域及全域的一致性，找出圖
片中我們容易注意的區域。藉由人類視覺
系統的觀點，能使視訊編碼器將位元作更
有效地分配及提供更好[11]。 
人類視覺注意力(human visual attention)
可分為環境驅動(stimulus-driven)和目標導
向(goal-directed)兩種[12]。環境驅動注意
力是指由外面環境的特性驅使我們對於環
境中某些部份感興趣並進而投以視覺注
視，其包含靜態注意力(static attention)和
動態注意力(motion attention)。在[13]，利
用靜態注意力的特性如顏色(color)、亮度
強度(intensity)和方位(orientation)，分析出
圖片中人們容易注意的區域。在動態注意
力部分，[14]利用區塊移動向量的強度及
鄰近區塊移動向量方向的一致性，決定圖
片中容易受我們注意的區域。[15]利用向
量的對比度及速度來決定影片的主觀視覺
品質。在限定的位元數情況下，將較多的
位元分配於我們容易受注意的區域，以獲
得更好的視訊品質。此方案特別在低位元
素率的情況下，對視訊品質有很大的提升
[17]。而若是在場景切換的區域給予較多
的位元數，則較能避免視訊品質跳動的現
象[18]。 
五、可調式視訊編碼架構 
可調式視訊編碼延伸AVC/H.264混合
編碼架構( hybrid coding structure)，再加上
時間、空間、視訊品質上的可調變技術。
圖一為可調式視訊編碼含兩個空間階層
(spatial layer)的編碼端架構之例，包含基
礎層 (base layer)與加強層 (enhancement 
layer)。可調式視訊編碼最多允許八個空間
階層編碼。每個空間階層都為 AVC/H.264
編碼方式單獨編碼。在空間加強層部分，
利用空間基礎層的資訊，使用空間階層預
測(inter-layer prediction)來降低空間加強
層的殘餘資訊及提升加強層的編碼效率。
在基礎層部分，單純為 AVC/H.264 編碼，
因此基礎層與 AVC/H.264 相容。由基礎層
編碼出的位元串流 (bitstream) 可以由
AVC/H.264 解壓縮端順利解壓縮。接下來
我們將敘述可調式視訊編碼的三種可調變
的架構，在[19][20]有更詳細的介紹。 
A、 時間可調性(Temporal scalability) 
 
       圖二，階層式-B 畫面架構。 
可調式視訊編碼在時間上可調性使用
階層式-B 畫面(hierarchical-B picture)架
構，如圖二所示。與之前時間上可調性使
用 向 量 補 償 時 間 上 濾 波 器 (motion 
compensated temporal filtering)架構相比，
   5
 
圖四、空間階層向量預測示意圖。 
 
圖五、空間階層殘餘資訊預測示意圖。 
一般視訊壓縮標準中，目前編碼的巨集
區塊運用相鄰已編好巨集區塊的移動向量
來做移動向量預測值(motion predictor)，進
行移動向量估測(motion estimation )找尋
最佳的參考區塊。可調式視訊編碼的加強
層除了利用此方法外，也使用已編碼過後
的基礎層中同樣位置區塊的向量資訊，根
據不同空間階層間尺寸大小的比率做
scaling 後，當作向量預測值。當鄰近巨集
區塊的向量與目前編碼向量不相似，則可
從基礎層找出更適當的向量預測值，找到
更佳的參考區塊，因而提升加強層的編碼
效率。此稱為空間階層向量預測，如圖四
所示。 
在一般視訊壓縮標準，巨集區塊找尋到
最佳的參考資訊後，將兩者之間的殘餘資
訊進行編碼。可調式視訊編碼的加強層利
用已編碼基礎層中相同位置區塊的殘餘資
訊，經由解碼後，根據空間階層尺寸大小
的比例做內插，然後與加強層巨集區塊的
殘餘資訊相減，減少加強層殘餘資訊的
量，提升加強層的編碼效率。此稱為空間
階層殘餘量預測，如圖五所示。 
 
C、視訊品質可調性(SNR scalability) 
在視訊品質可調性上，目前分成三種可
調 性 ， 分 別 為 CGS(Coarse-Grain 
Scalability)、FSG(Fine-Grain Scalability)和
MGS(Median-Grain Scalability)。CGS 階層
為空間階層的特例。與空間階層同樣使用
AVC/H.264 單獨編碼，再利用空間階層預
測降低殘餘量及提升編碼效率。不同的地
方在於由於 CGS 階層與基礎層的視訊尺
寸大小相同，因此在使用基礎層資訊時，
不需做內插及 scaling 的動作。不同於
CGS，FGS 和 MGS 是在同一個空間階層
做視訊品質的可調變性。FGS 是沿襲
MPEG-4 visual 的技術，使用位元平面
(bit-plane)的表示方法對於位元串流可做
任意的切割，但 CGS 有著與空間階層最多
只能有八層的限制，而 FGS 則是因為對位
元串流做任意切割，導致編碼端及解碼端
有著太高的複雜度。所以 MGS 為改善兩
者的缺點，擇中所做的改良。MGS 比 CGS
有著更多的階層，改善 CGS 階層的不足。
MGS 不像 FGS 可對位元串流做任意切
割，以多階層的方式，降低了編碼端與解
碼端的複雜度。圖六為 CGS、FGS、與
MGS 在位元串流與視訊品質關係圖的比
較。CGS 與 MGS 需要位元串流量達到視
訊品質階層所需要的位元串流量，視訊品
質才能提升，但 MGS 比 CGS 可分的階層
較多，更精確利用網路頻寬，FGS 則是依
據視訊串流的量的多寡，視訊品質就能提
升多少。 
   7
易引起我們的注意。因此當物件與背景的
移動方向相同，但是移動的強度與背景有
很大的不同，則此物件容易引起我們的注
意。另一方面，當物體與背景的移動方向
不同時，則當物體稍微有所移動，則此物
件也很容易讓我們引起注意。圖八為我們
所設計的向量注意力模型的流程圖。此
外，我們所設計的向量注意力模型適用於
任一個視訊壓縮編碼架構。 
可調式視訊壓縮在時間上預測使用階
層式 B 畫面(hierarchical-B picture)，如圖
二顯示，不同的時間階層(temporal level)
與參考畫面的距離有所不同，所以導致移
動向量的強度基準不一。因此我們以巨集
區塊為單位，將每個巨集區塊的向量做標
準化(normalization)。假設畫面分成 LK ×
的巨集區塊(macroblock)。TL 為目前第 n
張畫面屬於的時間階層(temporal level)，範
圍為 0 到 1−MAXTL 。 MAXTL 為最上層的時
間階層。第 n 張第 ( )ji, 巨集區塊經標準化
後的向量 ),(
nijnij yx
VMVM ′′ 的式子如下： 
⎪⎩
⎪⎨
⎧
=′
=′
−−
−−
1
1
2/
2/
TLTL
yy
TLTL
xx
MAX
nijnij
MAX
nijnij
MVVM
MVVM
⎪⎩
⎪⎨
⎧
=′
=′
−−
−−
1
1
2/
2/
TLTL
yy
TLTL
xx
MAX
nijnij
MAX
nijnij
MVVM
MVVM
…………………….(1) 
其中，移動向量(
nijx
MV ,
nijy
MV )為原來的向
量。 
第 n 張第 ( )ji, 巨集區塊經標準化後的
的向量強度的式子如下： 
  22
nijnij yxnij
VMVMI ′+′= ……………………….(2)  
第 n 張的向量平均及變異數的式子如下： 
∑∑−
=
−
=×=
1
0
1
0
1 K
i
L
j
nijn ILK
M ……………………………(3) 
( )∑∑−
=
−
=
−×=
1
0
1
0
21 K
i
L
j
nnijn MILK
V .………………(4) 
再將標準化向量分成 17 種向量方
向，其中將 360 度均分 16 個方向，剩餘一
種方向則為無方向，意指移動向量強度為
0。因此我們得到畫面的移動向量方向圖
(motion direction map)。然後，我們找出
整張畫面向量方向頻率最高的方向，再加
上此方向鄰近的兩個向量方向，此三個向
量 方 向 稱 為 背 景 方 向 (background 
direction)。當最高頻率的方向為無方向時
為一個特例，因無鄰近的向量方向，所以
背景方向只為無方向。 
再來，我們利用一個移動視窗(sliding 
window)和一些限定值(threshold)，來判別
移動視窗中間位置的巨集區塊是否為我們
容易受注意的區塊。移動視窗包含 33× 的
巨集區塊。限定值的式子如下: 
nnn DBDBH
VMTH += …………………………..……..(4) 
nn DBDBnL
VMTH −= ……….………………………….(5)   
),max( εnnn VMTH −= …………………………….(6) 
其中，ε 為小的數值， nDBM 為第 n 張背景
方向的平均向量強度， nDBV 為第 n 張背景
方向的平均變異數。依照以下法則，我們
判定移動視窗的巨集區塊為容易受注意的
   9
品質加強層(CGS enhancement layer)的畫
面，有著相同的畫面大小，不同於畫面視
訊品質的差異，所以當目前編碼加強層為
視訊品質階層(CGS layer)時，視訊品質階
層的巨集區塊模式通常小於基礎層的巨集
區塊模式。第四部分，從我們的實驗結果，
我們發現巨集區塊模式 44xMode 使用機
率很小，因而在我們的所設計的快速巨集
區塊模式演算法不考慮此模式。我們所設
計的快速巨集區塊演算法如圖九。 BLMode
為目前我們要編碼巨集區塊在前一個階層
同樣位置的區塊模式。當 BLMode 為空間編
碼(intra-coded)，我們將目前編碼區塊模式
的候選區域減少為 44intra x 和 BLintra 兩
種。當 BLMode 為時間編碼 (inter-coded)
時，目前區塊模式候選區域只為比 BLMode
相同及較小的巨集區塊模式。並且在我們
所設計的快速巨集區塊模式演算法，我們
不考慮 44xMode 此模式。 
七、實驗結果與討論 
在實驗裡，我們採用 JSVM8.10[24]
來實現我們所設計的演算法。我們使用一
個基礎層及兩個增強層，顯示我們設計的
演算法可以在多層空間及視訊品質階層上
使用。我們共使用五個標準的視訊影片- 
City、Crew、Football、Foreman、與 Soccer
做測試。 
圖十、參數設定。 
參數設定如圖十所示，基礎層的影片
大小為 QCIF (176x144)，量化參數(QP)為
40 。 第 一 個 加 強 層 的 影 片 大 小 為
CIF(352x288)，量化參數為 30，第二個加
強層的影片大小為 4CIF(704x576)，量化參
數為 20。此三個階層有個相同的畫面速率
(frame rate)為每秒播放 15 張畫面，總共的
編碼畫面為 97 張，包含一張 I 畫面及六個
畫面群(GOP-group of pictures) ，一個畫面
群由 16 張畫面組成。向量估測(motion 
estimation)的搜尋範圍為 32 個畫素，參考
的畫面為一張。空間上階層預測(inter-layer 
prediction)選擇可調適性(adaptive)，ε 設定
為 4。 
 
圖十一、所設計方案的編碼效率比較。 
 
圖十二、City 與 Football 視訊影片。 
 
(a)                (b) 
圖十三、所設計方案的視訊品質比較。 
   11
standard,” IEEE Transactions on 
Multimedia, vol. 8, no. 6, Dec. 2006. 
[8] H. Li, Z. G. Li, C. Wen, and L. P. Chau, 
“Fast mode decision for spatial  
scalable video coding,” Proceedings of 
IEEE International Symposium on 
Circuits and Systems, pp. 3005-3008, 
May 2006. 
[9] H. Li, Z. G. Li, and C. Wen, “Fast 
mode decision for coarse grain SNR 
scalable video coding,” Proceedings of 
International Conference on Acoustics, 
Speech and Signal Processing, vol. 
2, pp. 545-548, May 2006. 
[10]  H. Li and Z. G. Li, “Fast mode 
decision algorithm for inter-frame 
coding in fully scalable video coding,” 
IEEE Transactions on Circuits and 
Systems for Video Technology, vol. 16, 
no.7, pp. 889-895, July 2006. 
[11]  H. C. Lin, W. H. Peng, H. M. Hang, 
and W. J. Ho, ”Layer-adaptive mode 
decision and motion search for scalable 
video coding with the combination of 
coarse grain scalability(CGS) and 
temporal Scalability,” Proceedings of 
IEEE International Conference on 
Image Processing, pp. 289-292, Sept. 
2007. 
[12]  W. James, The principles of 
psychology, Henry Holt and Co, 1890. 
[13]  L. Itti, C. Koch and E. Niebur, “A 
model of saliency-based visual 
attention for rapid scene analysis,” 
IEEE Transactions on Pattern Analysis 
a nd Machine Integelligence, vol. 20, 
no. 11, Nov. 1998. 
[14] Y. F. Ma and H. J. Zhang, “A model of 
motion attention for video skimming,” 
Proceedings of IEEE International 
Conference on Image Processing, vol. 
1, pp. 22-25, September 2002. 
[15]  C.-W. Tang, ”Spatiotemporal visual 
considerations for video coding”, IEEE 
Transaction on Multimedia, vol. 9,  
no. 2,  pp. 231-238,  Feb. 2007. 
[16]  S. Li and M. C. Li, “An efficient 
spatiotemporal attention model and its 
application to shot matching”, IEEE 
Transaction on Circuits and  Video 
Technology, Vol. 17, NO. 10, Oct. 
2007. 
[17]  W. Lai, X. D. Gu, R. H. Wang, W. Y. 
Ma and H. J. Zhanga, “A region based 
multiple frame-rate tradeoff of video 
streaming," Proceedings of IEEE 
International Conference on Image 
Processing, vol. 3, pp. 2067– 2070, Oct. 
2004. 
[18]  Z. Chen, G. Qiu, Y. Lu, L. Zhu, Q. 
Chen, X. Gu, and C. Wang, ” 
Improving video coding at scene cuts 
using attention based adaptive bit 
allocation,” Proceedings of IEEE 
International Symposium on Circuits 
and Systems, pp. 3634 – 3638, May 
2007. 
[19]  H. Schwarz,  D. Marpe, and 
T .Wiegand, “Overview of the scalable 
video coding extension of the 
H.264/AVC standard,” IEEE Trans. 
Circuirts and Systems for Video 
Technology,  vol. 17,  No. 9,  
September  2007. 
   13
第二章、第二年計畫報告 
一、摘要 
近年來，多視角視訊應用興起，提供
觀賞者新的視覺感受，因此多視角視訊壓
縮技術成為傳輸與儲存多視角視訊的關鍵
角色。相對於傳統視訊壓縮技術開發時間
方向的資料多餘性，多視角視訊編碼器還
可開發視角間方向的資料多餘性，但為了
獲得最佳的編碼決策，龐大的運算量使得
多視角視訊編碼器不易應用於即時系統。
因此，在本專題計畫中，我們提出快速演
算法加速多視角視訊編碼器，演算法為快
速畫面間預測方向決策，藉由預測畫面內
容編碼區塊之運動向量特性，快速決定區
塊可能採用的預測方向。實驗結果顯示在
位元率最多提升1.534%，及最大PSNR損
失小於0.1dB的情況下，快速畫面間預測方
向決策節省15%提升24%的編碼時間。 
 
關鍵字:多視角視訊編碼器、快速畫面間預
測方向決策演算法、緩慢移動區塊、時間
軸方向預測。 
Abstract 
In recent years, various applications of 
multi-view video provide viewers a new 
viewing experience. Accordingly, 
multi-view video coding (MVC) plays a key 
role in transmission and storage of 
multi-view videos. However, besides 
temporal prediction which is used to reduce 
temporal redundancy, the MVC encoder also 
explores the inter-view redundancy by 
inter-view prediction. Because of the 
optimal decision of inter-view prediction 
and temporal prediction, the heavy 
computational load results in the difficulty 
of its realization in real-time systems. 
Therefore, this project proposes a fast 
algorithm for the MVC encoder. The 
proposed algorithm is related to fast inter 
frame direction predictor. The fast inter 
frame direction predictor decides the 
prediction direction that the current block 
may prefer, according to the motion 
characteristic of the current block inferred 
from the blocks in the neighboring frames. 
Our experimental results show that the fast 
inter frame direction predictor reduces 15% 
to 24% of encoding time while the 
maximum of 1.534% bit rate increase. And, 
the maximum PSNR decrease is less than 
0.1 dB.   
Keywords – multi-view video coding, 
fast inter frame direction predictor, slow 
motion block (SMB), temporal direction 
prediction. 
二、前言 
近年來為了滿足社會大眾生活便利及
娛樂性，各種型態的視訊科技應用興起，
其中多視角視訊應用提供觀眾單一視角視
訊無法實現的視覺效果，包含了立體電視
(Three-Dimensional Television (3DTV))及
自 由 視 角 隨 選 電 視 (Free Viewpoint 
Television (FTV))等[1]，因此，於 2001 年
的 12 月 ISO/IEC MPEG 成 立 3-D 
audio-visual(3DAV) ad hoc group(AHG)，調
查現有的視訊壓縮技術對於多視角視訊應
用是否可以提供足夠的壓縮效能[2]，並於
2005 年 1 月開始制定多視角視訊壓縮標
準，在 2008 年 7 月，以 H.264/AVC 為基
礎的多視角視訊壓縮標準進行最後草稿修
   15
對應巨集區塊，並快速決策該巨集區塊及
其鄰近巨集區塊所使用的編碼模式。在[6]
中，作者使用視差向量估測的方式在鄰近
已完成編碼的視訊中找到對應的區塊，分
析這些對應區塊所使用的運動向量，藉此
決定待編碼之巨集區塊的運動向量或是每
個子區塊的運動向量，運動向量一旦確定
後，該巨集區塊所採用的編碼模式也可以
快速被決定。 
對於視角間方向的畫面間編碼加速演
算法，在[7]中，以 epipolar geometry 為基
礎之演算法加速了視差向量的搜尋時間。
對於雙視角畫面而言，其中一個視角的成
像平面中的點，其可能的對應點落在另一
個視角的成像平面中，這些可能的對應點
所形成的一條直線稱為該點的 epipolar 
line，因此，有別於傳統視差搜尋視窗，演
算法將視差向量搜尋中心投影到 epipolar 
line，並以此點為新的搜尋中心將搜尋視窗
依照 epipolar line 方向旋轉，作者所使用
的視差搜尋演算法可以減少搜尋區域及次
數，實驗結果顯示作者只需搜尋少量的候
選位置，即可得到較佳的搜尋結果。在[8]
中，作者發現了同一畫面中的視差向量有
相同的方向性，因此提出了一個非對稱搜
尋視窗概念，藉由分析全域視差向量的大
小及方向特性，及預測目前待估測視差向
量變化的情況，定義了搜尋視窗四個方向
的控制因子反向水平調變因子(Negative 
Horizontal Scale, NHS)、正向水平調便因
子(Positive Horizontal Scale, PHS)、反向垂
直調變因子(Negative Vertical Scale, NVS)
及正向水平調變因子 (Positive Vertical 
Scale, PVS)，將傳統方正的搜尋視窗之範
圍大小分別除上這四個控制因子，得到一
個新的非對稱搜尋視窗，可大幅縮小視差
向量的搜尋範圍，減少視差向量的搜尋次
數。 
對於畫面間參考方向的快速決策演算
法，在[9]中，作者觀察時間方向參考畫面
同位置(co-located)區塊之參考方向，發現
與待編碼區塊之最佳參考方向有高相關
性，因此，作者所提出之參考方向決策利
用相同視角視訊已完成編碼的同位置區
塊，預測目前待編碼區塊之最佳參考方
向，如果時間方向向前及向後同位置區塊
之參考方向皆選擇視角間方向，則目前待
編碼區塊之最佳參考方向將預測為視角間
方向，除此之外，目前待編碼區塊之最佳
參考方向皆預測為時間方向。在[10]中，
作者利用鄰近已完成編碼區塊之參考方
向，預測目前待編碼區塊之最佳參考方
向，如果鄰近已完成編碼區塊選擇時間方
向(視角間方向)，待編碼區塊之最佳參考
方向同樣會選擇時間方向(視角間方向)。 
五、多視角視訊編碼架構 
多視角視訊編碼器架構將不同視角視
訊分成兩大類，基礎視角視訊(base view)
及附屬視角視訊(enhancement view)，編碼
器先對基礎視角視訊進行單一視角視訊的
編碼，再對附屬視角視訊編碼，在編碼附
屬視角視訊時，編碼器才開發視角之間的
資料多餘性。 
A、 基礎視角視訊編碼器 
基礎視角視訊編碼器架構可視為傳統
單視角視訊編碼器，並未使用到視角之間
參考畫面資料，也不開發視角之間的資料
多餘性，因此在此架構下，並不需使用視
差向量估測及補償，編碼模式只有畫面內
編碼模式(intra mode)及時間方向畫面間編
碼模式，重建畫面可被附屬視角視訊編碼
器做為參考畫面。 
 
   17
間方向的資料多餘性。 
    對於靜態或緩慢運動的畫面內容而
言，時間方向畫面間的差異性較小，相較
於視角方向預測，編碼器使用時間方向預
測容易於待編碼區塊座標附近找到對應區
塊，因此靜態或緩慢的畫面內容的預測方
向偏好時間方向預測，反之，對於動態的
畫面內容而言，使用時間方向預測可能不
易於待編碼區塊座標附近找到對應區塊，
但因為視差向量與相機間距離及物距有
關，當視角間方向預測獲得的資料量少於
時間方向預測的情況下，編碼器偏好使用
視角間方向預測。為了分析畫面靜態區塊
及緩慢運動之區塊位置，本系統提出不等
式比較運動向量各別分量絕對值的和及所
設定的門檻值，式(1)為本系統提出之緩慢
移動區塊(slow motion block, SMB)決策方
式，MVx 為視差向量之水平分量，MVy為
視差向量之垂直分量，如果該值小於所定
義的門檻值(Threshold)則判斷為緩慢移動
區塊，反之，則將該區塊視為動態區塊，
為了尋找合適門檻值，本系統統計採用時
間方向預測之區塊的 K 值，K 等於
|MVx|+|MVy|，並觀察 K 值的機率質量函數
(probability mass function, PMF)，並計算累
績 分 布 函 數 (cumulative distribution 
function, CDF) 。
圖二、測試視訊 Breakdancer 之 K 值機率
質量函數。  
圖三、測試視訊 Ballet 之 K 值機率質量函
數。 
圖二及圖三為測試視訊 Breakdancer
及 Ballet 採用時間方向預測之區塊的 K 值
機率質量函數，當 QP 值等於 32 的情況
下，較多區塊的運動向量為零向量或是向
量長度較小，因此，大部分採用時間方向
預測之區塊的 K 值較小，當 QP 值等於 24
的情況下，較少區塊的運動向量長度較
小，因此，K 值等於 0 之區塊採用時間方
向預測的機率較少，計算發現當 K 值等於
14 的時候，Breakdancer 的累績分布函數
約為 0.95，意旨採用時間方向預測之區塊
有 95%的 K 值小於等於 14，對於另一組
測試視訊 Ballet，當 K 值等於 14 的情況
下，採用時間方向預測之區塊比例也大於
95%，因此本系統所採用之門檻值為 14。 
C、所提出之快速畫面間預測方向決策演
算法 
 
圖四、使用緩慢移動區塊分布之快速畫面
間預測方向決策演算法。 
本專題計劃所提出之快速畫面間預測
   19
本系統提出之快速畫面間預測方向決
策演算法，在相同 GOP 中，當次要關鍵畫
面之向前緩慢移動區塊分布(向後緩慢移
動區塊分布 )同位置區塊為緩慢移動區
塊，及參考視角次要關鍵畫面之向前緩慢
移動區塊分布(向後緩慢移動區塊分布)對
應區塊為緩慢移動區塊時，則待編碼區塊
預測為緩慢移動區塊，其預測方向為時間
方向。除此之外，待編碼區塊將不採用本
論文提出之畫面間預測方向決策演算法。 
七、實驗結果與討論 
在實驗裡，我們採用 JMVC1.0 來實現
我們所設計的演算法。我們使用兩種多視
角視訊，匯聚型多視角視訊及並列型多視
角視訊，。我們共使用四個標準的視訊影
片- Breakdancer、Ballet、Ballroom 及 Exit
做測試。 
 
(a) 
 
(b) 
圖五、參數設定。 (a) Breakdancer 及
Ballet，(b) Ballroom 及 Exit。 
參數設定如圖五所示，視訊解析度為
1024×768 及 640×480 兩種，測試的量化參
數(QP)分別為為 32、30、28、26 及 24。
視訊視角個數均為 8 個，Breakdancer 及
Ballet 的畫面張數為 97 張，Ballroom 及
Exit 的畫面張數為 249 張，Breakdancer 及
Ballet 的畫面每秒張數為 15 張，Ballroom
及 Exit 的畫面每秒張數為 25 張，
Breakdancer 及 Ballet的相機陣列為匯聚性
相機陣列，Ballroom 及 Exit 的相機陣列為
並列型相機陣列，在視訊內容的部分，
Breakdancer 及 Ballet 之內容未做影像校
正，Ballroom及 Exit 之內容未做影像校正。 
除此之外，運動向量及視差向量估測
皆使用 JMVC1.0 內建之快速演算法
extended diamond search (XZS)，實驗對照
組為無使用快速畫面間預測方向決策演算
法之架構，分析平均 PSNR，位元率(bitrate)
及執行時間。 
 
 
 
圖六、所設計方案的編碼效率比較。 
圖六為 Ballet 無使用快速畫面間預
   21
of IEEE International Conference on 
Acoustics, Speech and Signal 
Processing, pp. 1373-1376, March 
2008. 
[7] J. Lu, H. Cai, J.-G. Lou, and J. Li, “An 
epipolar geometry-based fast disparity 
estimation algorithm for multiview 
image and video coding,” IEEE 
Transactions on Circuits and Systems 
for Video Technology, vol. 17, no.6, pp. 
737-750, June 2007. 
[8] X. Xu and Y. He, “Fast disparity motion 
estimation in MVC based on range 
prediction,” proceedings of IEEE 
International Conference on Image 
Processing, pp. 2000-2003, Oct. 2008. 
[9] X. M. Li, D. B. Zhao, X. Y. Ji, Q. Wang, 
and W. Gao, “A fast inter frame 
prediction algorithm for multi-view 
video coding,” Proceedings of IEEE 
International Conference on Image 
Processing, vol. 3, pp. 417-420, Oct. 
2007. 
[10] X. Li, D. Zhao, S. Ma, and W. Gao, 
“Fast disparity and motion estimation 
based on correlations for multiview 
video coding,” IEEE Transactions on 
Consumer Electronics, vol. 54, no. 4, pp. 
2037-2044, Nov. 2008. 
   23
此相關研究仍在繼續延伸進行，預期未來
將發表於國際期刊。此外，此計畫研發之
技術可有效強化國內相關技術之研發能
量。 
 
 
出席國際學術會議心得報告 
 
計畫編號 NSC 96-2221-E-008-015-MY2 
計畫名稱 多維可調式視訊傳輸於寬頻無線網路之研究-子計畫二: 適用於寬頻無線網路之可調式多景觀視訊編碼設計 
出國人員姓名 
服務機關及職稱 國立中央大學通訊工程所碩士生 吳運達 
會議時間地點 日期：3/31~4/2  地點：美國內華達州拉斯維加斯 
會議名稱 2008 IEEE Broadband Multimedia Symposium(Spring) 
發表論文題目 Motion Attention Directed Fast Mode Decision Algorithm for Spatial and CGS Scalable Video Coding 
 
一、 論文接受函 
ROM: 
  
IEEE BROADCAST TECHNOLOGY SOCIETY  
IEEE Technical Activities  
445 Hoes Lane, Piscataway, NJ  08854 
 
Dear Author,  
   
I am pleased to inform you that your paper has been accepted for ORAL 
PRESENTATION at the 2008 IEEE International Symposium on Broadband 
Multimedia Systems and Broadcasting, 31 March – 2 April 2008 at the Las Vegas 
Hilton, Las Vegas, NV, USA. 
   
If you have submitted more than one abstract, please check our Preliminary Program, 
http://www.ieee.org/organizations/society/bt/BMS08/bmsb08program.html, to see 
where your paper(s) has been placed 
 
FINAL PAPER SUBMISSION DEADLINE:  Friday, 29 February 2008 
 
Las Vegas hotels fill up very fast during CTIA. Please make your reservations as soon 
as possible, by visiting our website: 
final paper submission deadline of 29 February 2008. Each participant may present a 
maximum of two papers (oral or poster). Please visit the IEEE Broadcast Technology 
Society web site for links to our on-line registration and hotel information. 
   
Thank you for your participation the 2008 IEEE International Symposium on 
Broadband Multimedia Systems and Broadcasting.  We look forward to seeing you 
in Las Vegas.  We appreciate your interest and support to the IEEE Broadcast 
Technology Society and the 2008 IEEE International Symposium on Broadband 
Multimedia Systems and Broadcasting.  
   
Sincerely,  
   
Thomas M. Gurley  
Chair 
2008 IEEE International Symposium on Broadband Multimedia Systems and 
Broadcasting 
 
And  
 
Yiyan Wu and Demin Wang 
Technical Program Co-Chairs 
2008 IEEE International Symposium on Broadband Multimedia Systems and 
Broadcasting 
 
二、 參加會議經過及與會心得 
IEEE BMS 是每年由 IEEE Broadband Technology Society (BTS)所
主辦。IEEE BTS 每年舉辦一次。此次參加之會議名稱為 IEEE 2008 
BMS Spring，地點選在美國內華達州的拉斯維加斯希爾頓飯店(Hilton 
hotel)。BMS 是以業界為導向的一個會議，藉由此會議除了讓演講者
報告自己最新研究成果外，也讓來自各地的工程師、研究員和系統提
供者彼此討論最新的研究成果及互相交換心得達到技術提升。IEEE 
2008 BMS Spring 從眾多論文中挑選出 61 篇論文以口頭方式發表，及
31 篇以海報方式呈現，會議主題主要以多媒體廣播 (multimedia 
broadcasting)、電信(telecommunication)、消費電子(consumer electronics)
和網路技術(networking technologies)為主。包括了 IP TV、Mobile TV、
英文發表，對比相同情況以中文可以侃侃而談的情況仍有可以努力改
進的空間。這次會議也了解到自己還需改進的地方，這些都是日後我
仍須努力的地方。 
 
三、 參加會議照片 
圖一、2008 IEEE BMS 於美國拉斯維加斯希爾頓飯店舉辦 
四、 建議或分享計畫 
能夠參與國際性大型會議對於國內的研究人才是有很大的幫助， 
讓國內的學術研究能夠銜接國際研究潮流。不過經費的支出往往會是
考量之一，此次前往參加會議，本人很榮幸可以得到國科會專題計畫
補助。參與此類活動會更加深自己做研究的動力，期望日後能夠還有
機會參加這樣的國際性會議。  
 
五、 攜回資料名稱及內容 
會議論文全文集光碟 * 1 
 
（使用中文楷體字，字型大小：標題 16，內文 14） 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
signed copyright form, and 3) your pre-registration for the conference. All of these 
items must be done by 18 February 2008. 
Camera ready copy must be submitted to the IEEE Conference and Custom 
Publishing system, as they will be producing the final proceedings 
**** do NOT upload the final manuscript to TrackChair **** 
Full details can be found on the conference web site at 
http://www.ieeevtc.org/vtc2008spring/final-submission.php 
Reviews of your paper can be found on the TrackChair site at the URL at the top of 
this email, in the "Actions" sidebar, under "Reviews". Please take the reviewers 
comments into account where appropriate in preparing your final manuscript. 
Note that it is the IEEE VT Society policy that each paper must have either a full 
IEEE member or non-member registration (not a student registration). Authors 
submitting more than one paper must pay a full registration for the first paper, plus an 
extra paper charge for each additional paper. Your registration must be completed 
before you upload your manuscript to the IEEE Conference and Custom Publishing 
site. 
Thank you for submitting your paper to IEEE VTC2008-Spring, and we look 
forward to seeing you in Singapore at the conference. 
Mario Gerla and Yuming Jiang 
IEEE VTC2008-Spring Technical Program Committee Chairs 
 
 
 
 
 
 
 
交換心得達到技術提升。IEEE 2008 VTC Spring 從 1536 論文中挑選
出 385 篇論文以口頭方式發表，及 234 篇以海報方式呈現，會議主題
包含所有 Vehicular, Mobile 和 wireless communications 相關領域。細
部 大 致 包 括 了 Ad hoc, sensor networks, antenna, propagation, 
transmission technologies, wireless access, mobile, wireless networks and 
systems, wireless networks applications and services, satellite systems 
and transportation。在會議期間共有六個 Tutorials、四十五個 Session、
以及十一場 Poster Session。 
 
此次我們發表之論文為設計一個在車內的視訊應用，可以讓駕駛
人或是乘客可以跟固定辦公室的人做視訊會議，並有效解決其中頻寬
和光線問題的影響。發表的日期和時間為第一天 (5/12) 的上午10:20
的Vehicular and Transportation Systems 1場次，Chair為美國來的Yi 
Qian。由於前二天提早到達新加坡以避免水土不服的問題，有較充足
時間準備報告內容。雖然在報告場次的投影機出了一點小差錯，時間
延後了一些才開始，而我是第二位報告者並依舊報告完畢，並且對於
來參加此場次人發問的問題給予解答，在下台的同時也有國外學者當
面給我鼓勵和肯定。在這樣國際性的大場合報告自己研究的成果，除
了給予自己成就上的肯定，也給予我無比的信心。  
由於在本國舉辦國際性會議的機會較少，能有機會到國外參與國
的壓力和政府之下讓這個國家的人有強大的競爭力。相對於台灣只要
肯努力即使是較偏向勞力的工作也可以養活自己，我們真的要珍惜自
己所擁有的。 
 
九、 建議或分享計畫 
能夠參與國際性大型會議對於國內的研究人才是有很大的幫助， 
讓國內的學術研究能夠銜接國際研究潮流。不過經費的支出往往會是
考量之一，此次前往參加會議，本人很榮幸可以得到國科會補助，才
能夠順利出國。此外台灣每年參與此類活動的人數眾多，如果能夠把
參加每個國際會議的名冊整理列出，可以有利於大家在國外時解決生
活上的問題。 
 
六、會議照片 
 
  
The Motion Attention Directed Fast Mode Decision for Spatial and CGS Scalable Video Coding 
Yun-Da Wu and Chih-Wei Tang 
Visual Communications Laboratory 
Department of Communication Engineering, National Central University, Jhongli, Taiwan
 Abstract— The exhaustive search scheme for macroblock mode 
decision in the working draft of scalable video coding extension of 
H.264/AVC achieves theoretically optimal coding efficiency. 
However, this scheme also accompanies high computation 
complexity. Thus, for scalable video coding, how to reduce heavy 
computation load while there is minor bitrate increase and PSNR 
loss is a critical issue. In this paper, we propose a fast macroblock  
mode decision algorithm for scalable video coding. We consider the 
psychovisual issue and employing the correlations of macroblock 
partitions between base layer and enhancement layer. First, we 
adopt the original search scheme for macroblock mode decision in 
the base layer. The visually attended regions are identified based on 
the motion vectors in the base layer. Then, fast mode decision is 
applied to motion unattended regions in the spatial/CGS 
enhancement layer for time savings. It reduces mode candidates by 
exploiting the mode decisions in the previously encoded 
enhancement/base layer to predict the mode decisions in the current 
enhancement layer. Experiments conducted by JSVM8.10 exhibit 
that the proposed scheme saves 21.53% to 45.89% encoding time 
while there is a maximum of 1.7506% bit-rate increase. It is noticed 
that there is a maximum of 0.1173 dB PSNR loss while the regions 
suffer from such loss are visually unattended. 
Keywords – Scalable video coding, fast mode decision, 
psychovisual characteristics, CGS scalability, spatial scalability. 
I.INTRODUCTION 
The Joint Video Team (JVT) of the ITU-T Video Coding 
Experts Group (VCEG) and the ISO/IEC Moving Picture 
Experts Group (MPEG) develop a scalable video coding 
(SVC) extended from H.264/AVC standard. This is 
motivated by the significant improved coding efficiency and 
high degree of adoption of H.264/AVC in the recent industry 
compared to prior international coding standards. The 
extension aims at enabling spatial, temporal, and SNR 
scalabilities. And thus, it provides a single bit-stream that can 
adapt to diverse user needs, device capabilities, or network 
conditions.  
In fact, scalability profiles have been included in several 
previous video coding standards such as H.263 and MPEG-4 
Visual although they are rarely utilized in the current video 
transmissions. One reason behind this is the high encoder 
complexity. Regarding the working draft (WD) of the 
scalable video coding extension of H.264/AVC, an 
exhaustive search scheme is employed for macroblock mode 
decision [1]. This scheme achieves possible best coding 
efficiency but results in the extremely large encoding 
complexity. Therefore, how to decrease the encoding 
complexity while there is few bitrate increase and PSNR loss 
is a critical issue. 
Similar to single-layered encoders such as H.264/AVC 
and MPEG-4, motion estimation is the most time-consuming 
module of scalable video encoders [1]. Several fast mode 
decision algorithms have been proposed for the SVC 
extension of H.264/AVC by exploring the correlations of 
macroblock partitions between the base layer and 
enhancement layers [2]-[6]. These algorithms are applied to 
one spatial/CGS enhancement layer. It is observed that if the 
mode decision in the base layer (BL) is intra mode, the 
probability that the corresponding mode decision of the co-
located macroblock in the spatial/CGS enhancement layer 
(EL) is high. Thus, the mode candidates in the enhancement 
layer can be further limited for complexity reduction [3]-[5]. 
It is also found that a macroblock partition in the spatial 
enhancement layer is usually similar to that of the co-located 
macroblock in the base layer [3]. Furthermore, macroblock 
partitions of the CGS enhancement layer are also smaller 
than those of the base layer with high probability [4].  
Although these fast algorithms usually lead to minor 
objective visual quality (i.e., PSNR) loss, viewers may still 
perceive the unwanted subjective visual quality degradations 
since all of these algorithms ignore the psychovisual issue. In 
the past years, there have been several schemes proposed to 
incorporate human visual characteristics into the video 
codecs. One example is allocating more bits to the regions of 
interest (ROI) for better visual quality, especially under the 
constraint of low bit-rate [7]. Moreover, since scene changes 
may attract more visual attentions, the good visual quality 
can be achieved by assigning more bits to the frames of scene 
changes. Such scene change detection can be realized based 
on the human visual characteristics [8].  
In this paper, we propose a novel fast scalable video 
coding scheme by considering the psychovisual 
characteristics. The key idea is employing our fast mode 
decision algorithm to reduce the encoder complexity only in 
visually unattended regions but not all video contents. This 
algorithm reduces the macroblock mode candidates by 
exploiting the correlation between the base layer and the 
enhanced CGS/spatial enhancement layers. Thus, the 
subjective visual quality of visually attended regions is still 
retained while the speedup of the scalable video coder is also 
achieved. 
  
  22
nijnij yxnij VMVMI ′+′=  .                                       (2) 
The mean and variance of the motion intensities of the nth target 
frame are 
∑∑−
=
−
=
×
=
1
0
1
0
1 K
i
L
j
nijn ILK
M ,                                             (3) 
and 
( )∑∑−
=
−
=
−
×
=
1
0
1
0
21 K
i
L
j
nnijn MILK
V .                               (4) 
The motion direction with the highest frequency and its two 
neighboring directions are deemed as the background 
directions (BD) of the target frame. And then, the mean 
nBD
M  and variance 
nBD
V  of the motion intensities of the 
background directions of the nth target frame are calculated. 
In our experiments, we categorize the motion vectors of all 
1616×  blocks in the base layer into seventeen types. 
Seventeen types come from sixteen possible motion directions 
of 360 degree. The other one comes from the skip mode if this 
block is motion free. 
We count the motion directions of macroblocks within 
33×  macroblock neighborhood. Let the number of MBs with 
motion direction not belonging to one of the background 
directions larger than threshold TH  be q. Let the number of 
MBs which have the same motion directions as the 
background direction but have motion intensities larger than 
threshold HTH  or smaller than threshold LTH with lower 
bound ε be r. The thresholds HTHTH , , and LTH are 
computed by 
),min( εnn VMTH −= ,                                            (7) 
nn DBDBH
VMTH += ,                                                 (8) 
nn DBDBL
VITH −= .                                                    (9) 
where ε  is a small value. If the sum of q and r is larger than 
a half of the number of macroblocks in this neighborhood, 
this MB is deemed as visually attended region. 
 
B.Fast Mode Decision Algorithm 
Since mode decisions of the enhancement layer are 
similar to those of the previous enhancement/base layer, it is 
possible to reduce the SVC computation complexity by 
making mode decisions in the enhancement layers based on 
the decision information in the previous enhancement/base 
layer. Therefore, in this paper, we propose a novel fast 
scalable video coding scheme by incorporating the ideas of the 
mode decision and visual attention. With this, the encoding 
speed is significantly improved while the subjective visual 
quality of the reconstructed videos is reasonably retained. 
 
Fig.4 The flowchart of the proposed fast mode decision 
algorithm. (Mode8x8 includes 8x8, 8x4, 4x8 partition modes. 
 
Based on the ideas proposed in [5], our fast mode decision 
algorithm is shown in Fig. 4, and we described it as follows. 
First, mode decisions of macroblocks in the base layer are 
made as those in the original scalable video coding extension 
of the H.264/AVC. Next, for MBs in the spatial/CGS 
enhancement layers, we decide whether the fast mode decision 
is carried out or not according to the mode decision 
information MODEBL and the motion attention information 
obtained from the corresponding base layer. If a macroblock is 
claimed as motion unattended macroblock, the fast mode 
decision is applied. Otherwise, the exhaustive MB mode 
search is adopted as usual. It is observed that if the frames 
containing scene changes, the intra mode is preferred in the 
base layer since the inter  mode cannot work well in this case. 
Thus, if the BLMODE  is intra mode, we limit the mode 
decisions of the spatial/CGS enhancement layer to be intra 
mode only. Furthermore, if the BLMODE  is Inter mode, we 
limit the mode decisions in the spatial/CGS enhancement 
layers to be smaller partition modes only. Finally, our 
experimental results show that 4x4 partition mode of 
Mode8x8 is seldom selected. Therefore, in our proposed fast 
mode decision algorithm, the 4x4 partition mode is not 
considered. 
III.EXPERIMENTAL RESULTS 
  In our experiments, we adopt JSVM8.10 [15] to verify 
the proposed fast algorithm. The parameter settings are 
shown in Fig. 5. In the experiments, we use one base layer 
and two enhancement layers although our proposed scheme 
can be applied to multiple spatial/CGS enhancement layers. 
There are five standard test sequences used in the tests. The 
spatial resolution of the base layer is QCIF ( 144176 × ) with 
frame rate 15 fps, and QP is 40. For the first enhancement 
layer, spatial resolution is CIF ( 288352 × ) with frame rate 15 
An Illumination Adaptive Video Coding Scheme for    
In-vehicle Video Applications 
Hsueh-Han Lai, Yun-Da Wu, and Chih-Wei Tang 
Department of Communication Engineering 
National Central University 
Jhongli, Taiwan 
rocktoto@gmail.com, boxkurt@hotmail.com, cwtang@ce.ncu.edu.tw  
Abstract—With the advance of intelligent vehicle systems, 
drivers or passengers can keep interaction with people in fixed 
offices or other vehicles through visual communications. However, 
the illumination variations due to the changes of environments or 
weather conditions may significantly change the appearance of in-
vehicle videos. Accordingly, the compression efficiency is much 
reduced even though the bandwidth of such wireless 
communications has been quite limited. There is pretty few 
previous work designed for efficient in-vehicle video 
compressions. Thus, we propose an illumination adaptive video 
coding scheme for in-vehicle video applications. Since human 
faces are usually the most visually attended regions in such 
applications, this scheme consists of illumination correction, face 
detection, and the visual attention based video codec. The 
proposed illumination correction strategy combines the 
advantages of the single-scale Retinex (SSR) and the weighted 
histogram separation (WHS). The experimental results show that 
our illumination correction strategy effectively improves the face 
detection performance of in-vehicle videos. Moreover, the 
subjective visual quality of the proposed scheme outperforms that 
of H.264 with rate control since our scheme allocates bits by 
incorporating the psychovisual characteristics. 
Keywords- Video coding, in-vehicle videos, illumination 
correction, visual attention. 
I. INTRODUCTION  
Intelligent transportation system (ITS) consists of intelligent 
infrastructure systems and intelligent vehicle systems. Such 
system is designed to improve transportation safety, mobility, 
and productivity [1]. Regarding the video applications of the 
intelligent vehicle systems, its image analysis is much more 
complex than that of indoor ones. These video contents may 
suffer from serious color distortions due to the illumination 
variations under different weather conditions. The buildings in 
the environments may also cause undesirable shadows. 
Moreover, the backgrounds of the in-vehicle video contents are 
more time-varying than those of the indoor image sequences.  
Video compression is a key technology of visual 
communications between vehicles. Due to the limited bandwidth 
of high-mobility vehicles, how to retain the visual quality of the 
reconstructed videos while reducing the transmission bit-rate is a 
critical issue. One possible solution is incorporating psychovisual 
characteristics into coding process. The visually attended regions 
are identified either through top-down process or bottom-up 
process. Then, coders allocate fewer bits to less attended regions 
without perception degradation [2]-[4]. For in-vehicle video 
applications such as video conferencing and remote driver 
monitoring, faces may attract more attentions since they convey 
the important information of facial expressions. Therefore, the 
visual attention based video encoders where fewer bits are 
allocated to non-face regions can achieve bit-rate savings [5][6]. 
For face localization of in-vehicle videos, face tracking based 
methods fail frequently due to the rapid changes of image 
appearances under lighting variations. Thus, instead of 
combining face detection and tracking, we only apply face 
detection. 
 
Fig.1 The proposed illumination adaptive video coding system. 
 
A variety of facial features can be candidates for face 
detection [7]. Nevertheless, since drivers may rotate heads for 
lane-condition checking and images may be blurred due to the 
vehicle vibrations, skin color is much more preferred for robust 
detection. On the other hand, since illumination variations may 
significantly degrade the face detection performance, the 
illumination correction is a necessary prior stage of face 
detection. Currently, few works are devoted for face detection 
based video codecs where the illumination issue is considered. 
Moreover, vehicle related video coding schemes are mainly 
applied to the snapshots of videos of outdoor scenes [8][9].  Thus, 
this paper proposes a novel illumination adaptive video coding 
scheme for in-vehicle applications (Fig. 1). In [10], the quotient 
image (QI) for recognizing faces under variant illumination 
conditions is proposed. For this, assumptions such as facial shape 
dynamic compression and tonal rendition. K normalizes ),( yxF  
such that 
   
∫ ∫ =1),( dxdyyxF
.                                                          (3) 
 
(a)                      (b)                        (c)                         (d) 
 
(e)                      (f)                        (g)                          (h) 
Fig.4 People1Cloudy: (a) A light image. (b) The skin map of (a). (c) SSR 
enhanced image of (a). (d) The skin map of (c). (e) WHS enhanced image of (a) 
(w=0.5, interval=4). (f) The skin map of (e). (g) WHS enhanced image of 
(a)(w=0.5, interval=2). (h) The skin map of (g).  
 
To improve the lightness appearance of skin regions rather 
than the details of images, larger scales are preferred since they 
contribute to the color constancy effect and fine tonal 
information [13]. In this paper, we adopt SSR and set the scale n 
to be 250 [14].  
Note that to avoid ),( yxRi in (1) less than zero, we revise (1) 
to be (4) 
[ ] ),(),(/),(1log),( yxIyxFyxIyxR iii ∗+= .   (4)     
Finally, the canonical gain/offset is added to improve the image 
lightness as shown in [12]. 
Illumination variations may decrease the correct detection 
rate as shown in Figs. 3(b) and 4(b). However, our experimental 
results show that SSR effectively improves the correct detection 
rates for most color distorted images, although it still fails for 
images where the vehicle windows are very light. This is because 
that we do not change the default parameter settings of AUTO 
WHITE BLANCE and AUTO EXPOSE of the webcam, and 
thus, these functions may lead to overlight the input images in 
some cases. Moreover, the computational load of SSR is pretty 
heavy. Thus, we further use weighted histogram separation 
(WHS) to overcome these problems in section B. 
B. Weighted Histogram Separation 
 
Fig.5 The 2-level tree of weighted histogram separation. 
 
The property of weighted histogram separation scheme (WHS) 
situates between the successive mean quantization scheme [20] 
and the histogram equalization. This scheme enables selecting 
weightings for contrast adjustment [15]. WHS is constructed of 
data separation units (DSUs) which separate an image dataset 
into two subsets (Fig. 5), and each subset can be further separated 
into another two subsets (DSUs). Let H denote the dataset and 
H(p) is the number of pixels with gray level p in H, where p 
ranges from 0 to 255. The threshold for separating H is defined 
as 
∑
=
<≤
−=
t
pHt
p
M
w
00
)(H1minargτ ,                                 (5) 
where w is a weighted factor controlling the size of the two 
separated subsets, and t denotes a gray-level value. M is the 
number of pixels of the gray-level image, and it is computed by 
∑−
=
=
1
0
)(H
H
p
pM ,                                                            (6) 
where H is the dimension of dataset H. 
Next, we separate H into two subsets H1,0 and H1,1 according 
to τ in (5), and H1,0 and H1,1  are defined as 

 ≤
=
otherwise           ,0
 if       ),(H
)(H 0,1
τpp
p      ,                        (7) 
and 

 >
=
otherwise           ,0
 if       ),(H
)(H 1,1
τpp
p .                             (8) 
Finally, a net dataset )(H p
∧
 is generated based on these 
separated subsets, and it is defined by 
   −≤≤⋅== ∑
−
=
∧
120,2|)(H)(H
1
0
,
L
L
H
j
iL iHipjp .    (9) 
We apply WHS algorithm to R, G, and B channels of input 
images. In our experiment, L is 6 and w=0.5. As shown in Figs. 
3(f) and 4(f), WHS indeed improves the detection performance of 
color distorted images (e.g., Figs. 3(e) and 4(e) in the 
experiments). 
For the cases where non-skin pixels are deemed as skin 
pixels, we found that the false detection rate (FDR) can be 
decreased by inserting the modified 2L subsets into the new 
histogram. This is done by revising (9) to be 
   −≤≤•+== −
−
=
∧ ∑ 120,2|)(H)(H 8
1
0
,
L
L
H
j
iL iintervaliHpjp . (10) 
Again, L is 6 and w=0.5 in the experiments. The interval 
between bins is 2. As shown in Figs. 3(h) and 4(h), the false 
detection rates of the revised WHS enhanced images (e.g., Figs. 
3(g) and 4(g)) are indeed decreased, where the new histogram  
bins are located in the middle part (from 64 to 192) of the 
histogram. 
IV. VISUAL ATTENTION BASED ADAPTIVE VIDEO CODING 
After identifying face and non-face blocks, quantization 
parameters are adjusted at macroblock level for better matching 
original People2Cloudy is 98%. However, the face can be 
detected in the whole video sequences after the illumination 
correction is carried out to these sequences. Obviously, there is 
the tradeoff between face detection rate and the value of τ . A 
larger τ  may lead to the smaller face detection rate. In the 
following, the performance of the proposed visual attention 
based video encoder with adaptive QP processes the input video 
sequences where the faces are identified based on the parameter 
setting %30=τ . 
The adopted initial QPs are 22, 28, 32, and 36. 
nijQP∆  is 2 
for a moderate tradeoff between the image quality and the bit-
rate savings. The experimental results show that the rate 
reduction between our adaptive encoder and original H.264 is 
around 20% for different QPs as shown in Tables 1-2. 
However, there is about 1dB PSNR decrease of the 
reconstructed frames of the adaptive H.264 encoder compared to 
that of the original H.264 encoder since H.264 with fixed QP 
leads to the largest bitrate. The PSNR values from H.264 with 
rate control are about 0.1dB larger than that from the proposed 
adaptive encoder. However, the proposed scheme allocates bits 
which more matches human perception as shown by the 
following subjective visual tests. 
Regarding the subjective visual quality tests, test sequences 
are presented on SAMSUNG SyncMaster 940NW TFT. The 
viewing distance is three picture heights (3H). Twelve viewers 
with normal color visions or at least average corrected visions 
attend the test. The testing procedure follows the double stimulus 
continuous quality scale (DSCQS) [19]. The subjective quality is 
expressed as the difference mean opinion score (DMOS) between 
the rating of the reconstructed videos and that of the source 
videos. MOS ranges from 0 to 100. Figs. 7-8 show the 
comparisons of subjective visual tests by applying DSCQS test 
procedure among three different quantization schemes. We can 
find that H.264 encoder with fixed QP leads to the best visual 
quality with the price of more bits. These figures also show that 
the visual quality of the proposed scheme outperforms that of the 
H.264 with rate control even though the total bitrates of these two 
schemes are the same. The reason behind that is the proposed 
H.264 encoder with adaptive QP allocates fewer bits to non-face 
regions, but the H.264 with rate control scheme allocates bits 
without considering the visual attention factor.  
VI. CONCLUSIONS 
In this paper, we propose a novel video coding scheme for 
efficient video compressions of in-vehicle videos. Since human 
faces are the most visually attended regions in such applications, 
this scheme combines illumination correction, face detection, and 
visual attention based video coding techniques. The experimental 
results show the coding efficiency is improved while the visual 
quality is retained. This video coding scheme can be applied to 
in-vehicle video conferencing and remote driver monitoring 
systems. 
REFERENCES 
[1] http://itsdeployment2.ed.ornl.gov/technology_overview 
[2] Z. Wang, L. Lu, and A. C. Bovik, "Foveation scalable video coding with 
automatic fixation selection," IEEE Trans. Image Processing, Vol. 12, No. 
2, pp. 243-254, Feb. 2003. 
[3] L. Itti, “Automatic foveation for video compression using a 
neurobiological model of visual attention,” IEEE Trans. Image Processing, 
Vol. 13, No. 10, pp. 1304-1318, Oct. 2004. 
[4] C.-W. Tang, “Spatiotemporal visual considerations for video coding,” 
IEEE Trans. Multimedia, Vol. 9, No. 2, pp. 231-238, Jan. 2007. 
[5] D. Chai and  K. N. Ngan, “Foreground/background video coding scheme,” 
in Proc. IEEE Int. Symp. Circuits Syst., Vol. II, pp. 1448-1451, June 1997. 
[6] M.-J. Chen, M.-C. Chi, C.-T. Hsu, and J.-W. Chen,“ ROI video coding 
based on H. 263+ with robust skin-color detection technique,” IEEE Trans. 
Consumer Electronics, Vol. 49, pp.724-730, Aug. 2003 
[7] M.-H. Yang, D. J. Kriegman, and N. Ahuja, “Detecting Faces in Images: A 
Survey,” IEEE Trans. Pattern Analysis and Machine Intelligence, Vol. 24, 
pp. 34-58, Jan. 2002. 
[8] M. Li, R.-M. Hu, R. Zhu, and W. Li, “Video streaming on moving vehicles 
over seamless internetworks of WLANs and cellular networks,” in Proc. 
IEEE Intl. Conference on Vehicular Electronics and Safety, pp. 369- 372, 
October 2005. 
[9] K. Tischler, M. Clauss, Y. Guenter, N. Kaempchen, R. M. Schreier, and M. 
M. Stiegeler, “Networked environment description for advanced driver 
assistance systems,” in Proc. IEEE Intl. Conference on Intelligent 
Transportation Systems, pp. 785- 790, March 2005.  
[10] A. Shashua and T. Riklin-Raviv, “The quotient image: Class based re-
rendering and recognition with varying illuminations,” IEEE Trans. Pattern 
Analysis and Machine Intelligence, Vol. 23, No. 2, pp. 129–139, Feb. 
2001. 
[11] M. Y. Nam and P. K. Rhee, “An efficient face recognition for variant 
illumination condition,” in Proc. IEEE Intl. Symposium on Intelligent 
Signal Processing and Communication Systems, pp. 111-115, 2004. 
[12] D. J. Jobson, Z. Rahman, and G. A. Woodell ”Properties and performance 
of a center/surround Retinex,” IEEE Trans. Image Processing, Vol. 6, No. 
3, pp. 451-462, March 1997. 
[13] Z. Rahmna, D. J. Jobson, and G. A. Woodell, ”Retinex processing for 
automatic image enhancement,” Journal of Electronic Imaging, Vol. 13, 
No. 1, pp.100-110, Jan. 2004. 
[14] Z. Rahman, D. J. Jobson, and G. A. Woodell, "A multiscale retinex for 
colour rendition and dynamic range compression," in Proc. SPIE 
International Symposium on Optical Science, Engineering and 
Instrumentation, Applications of Digital Image Processing XIX, 1996. 
[15] S.-C. Pei, Y.-C. Zeng, and J.-J. Ding, “Color images enhancement using 
weighted histogram separation,” in Proc. IEEE Intl. Conference on Image 
Processing, pp.2889-2892, Oct. 2006. 
[16] D. Chai and K. N. Ngan, ” Face segmentation using skin-color map in 
videophone applications,” IEEE Trans. Circuits and Systems for Video 
Technology, Vol. 9, No. 4, pp. 551-564, Jun. 1999. 
[17] S. L. Phung, A Bouzerdoum, and D. Chai, “ Skin segmentation using color 
pixel classification: analysis and comparison,“ IEEE Trans.Pattern 
Analysis and Machine intelligence, Vol. 27, No.1, pp.148-154, January 
2005. 
[18] E. Land, “An alternative technique for the computation of the designator in 
the retinex theory of color vision,” in Proc. Nat. Acad. Sci., Vol. 83, pp. 
3078-3080, 1986. 
[19] F. Pereira and T. Ebrahimi, The MPEG-4 Book, pp. 669-675, 2002. 
[20] M. Nilsson, M. Dahl and I. Claesson,”Gray-scale image enhancement 
using the SMQT,”in Proc. IEEE Intl. Conference on Image Processing, 
pp. 933-936, September 2005. 
the fast inter mode decision is applied based on the 
relationship between MCP and DCP. 
In this paper, we propose a novel fast decision strategy of 
prediction direction between MCP and DCP in MVC. Our fast 
algorithm is applied to enhancement views where the 
correlation in the view direction is utilized and the coded 
information of base view is referenced. In the remainder of 
this paper, Section II introduces the inter frame prediction in 
MVC. Our fast algorithm for decision of prediction direction 
is described in Section III, and the experimental results are 
given in Section IV. Finally, Section V concludes this paper. 
II. INTER FRAME PREDICTION IN MVC 
 
Figure 1. The SMBs with the prediction decision MCP in JMVM 6.0 under 
different QPs and SMB thresholds.  
     
(a)                                                  (b) 
     
                      (c)                                                   (d) 
Figure 2. Analyses of motion and the direction decision of inter frame 
prediction for multi-view videos EXIT and BALLROOM. (a) and (c): MBs 
with fast motion. (b) and (d): MBs with the prediction decision DCP. 
Regarding the MVC prediction structure, the performance 
of distinct prediction structures is compared in [14]. An 
effective prediction structure often spends much time on 
complex inter frame prediction [15]. Fast inter-frame 
prediction will be achieved if inter-view prediction is applied 
only to anchor pictures that are either intra-coded or inter-
coded by using anchor picture from another view as reference. 
The compression ratio and visual quality will be improved if 
bi-directional inter-view prediction is applied. 
Besides MCP, DCP is also considered by exploring the 
inter-view correlation at some time instant in inter frame 
prediction. In fact, after a fixed prediction structure is adopted, 
it is unnecessary to make the decision of prediction direction 
based on prediction results of both DCP and MCP for each 
frame. The reason is that the MVC encoder should prefer 
different prediction direction for different video contents. In 
Fig. 1, our experiments conducted by JMVM 6.0 show that 
the probability that MCP is selected is very large in 
homogeneous or blocks with slow motion. On the contrary, 
the probability that DCP is selected is larger than that of MCP 
in blocks with fast motion. That is, the prediction decisions of 
most blocks with slow motion (SMBs) are temporal direction 
while the prediction decisions of most blocks with high 
motion are inter-view direction. For example, there are 
95.8906% and 97.5491% MBs with slow motion are decided 
to be predicted temporally in BALLROOM and EXIT 
(QP=22), respectively. As shown in Fig. 2, blocks with fast 
motion are colored pink in Figs. 2(a) and 2(c) while blocks 
with decisions of DCP are colored yellow in Figs. 2(b) and 
2(d). 
 
Figure 3. Our adopted MVC prediction structure. 
 
In our design, the adopted prediction structure where the 
hierarchical B-frame is employed is shown in Fig. 3. In this 
structure, views are classified into a base view and 
enhancement views. View S4 is the base view and the others 
are enhancement views. DCP will not be considered in S4 
since S4 should be encoded and decoded before all 
enhancement views at the same time instant. On the other 
hand, since the identification of SMBs in non-anchor frames 
of enhancement views will be inferred from the SMBs of 
base view and an anchor picture of the enhancement view in 
our design, the GOP of each view should be not large. We set 
it to be 4. Finally, to retain the acceptable compression ratio 
and visual quality after the fast algorithm is applied, we 
employ the bi-directional inter-view prediction for non-
anchor pictures, eg. T1,T2, T3, T5, and T6. 
III. THE PROPOSED FAST DIRECTION PREDICTOR OF INTER 
FRAME PREDICTION 
Our fast MVC encoder is based on the fast decision 
strategy of inter frame prediction direction which speeds up 
inter frame prediction of non-anchor pictures in enhancement 
views. The base view is coded with MCP only. In this view, 
the SMBs’ locations of each frame are identified based on the 
motion vectors by MCP. A block will be deemed as a SMB if 
its sum of absolute values of motion in x and y directions is 
smaller than a predefined threshold. In enhancement views, 
