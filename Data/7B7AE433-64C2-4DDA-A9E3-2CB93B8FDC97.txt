行政院國家科學委員會補助專題研究計畫 
□ 成 果 報 告   
■期中進度報告 
 
智慧型空間之多模人機互動介面設計與整合 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC  97－2221－E－259－028－MY3 
執行期間：  97 年 8 月 1 日至  100 年 7 月 31 日 
 
計畫主持人： 江政欽 
共同主持人： 
計畫參與人員： 陳志偉、何明哲、朱倩鴻、郭建國、廖呈祥、鄭婷、吳
育恩、辜紹桓、王力、楊政叡 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          ■涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
          
執行單位：國立東華大學 資訊工程學系 
 
中   華   民   國  99   年  7   月   22   日 
  
中、英文摘要及關鍵詞(keywords)  
本研究建構一個家用智慧型空間系統，並包含三個主要子系統：智慧型家電控制、虛
擬角色通訊、及情緒感測門禁系統。此系統試圖提供家庭成員全方面的科技享受。智慧型
家電控制系統主要應用於室內環境，使用者以肢體動作搭配語音指令，來自然及便利的操
作家電。虛擬角色通訊系統應用與家庭成員與遠端人員互動，提供使用者利用語音或臉部
動作等方式，來驅動遠端的虛擬角色。同時能避免隱私的洩漏，也可降低頻寬的需求。情
緒感測門禁系統作為家的門神，除了能辨識訪問者的身分外，也能辨識訪問者的情緒來提
供家庭成員應對的參考。 
智慧型家電控制系統包含：3D指向、語音指令、及動作指令辨識。本研究利用多角度
攝影鏡頭的輪廓外觀，建構出使用者頭頂與手指尖端兩端點在三維空間座標中的指向連線。
再取得使用者於不同位置指向同一操控物體的兩條指向連線，建立該操控物體的系統位置，
完成操控系統環境的建立。判斷使用者於系統環境中的指向連線是否有指向操控物體，再
以設定好的語音指令或是動作指令對指向裝置進行操控。 
虛擬角色通訊系統，提供語音及臉部動作驅動虛擬角色技術。語音驅動人臉唇形合成
系統部分，利用隱藏式馬可夫模型(Hidden Markov Model)來進行中文語音的辨識，加上表
情合成技術，以合成出嘴型動畫讓人們藉著人臉動畫加深對中文語言的理解程度。適於低
頻寬下虛擬角色影音通訊系統方面，使用了主動式外觀模型(Active Appereance Model，
AAM)於通話中抓取講話者的表情特徵，並且產生表情特徵參數，由於此參數的量非常之小，
可以輕鬆透過網路傳遞，將參數傳遞至受話端，受話端再根據表情參數合成出虛擬角色的
表情，並與聲音進行同步輸出，就能夠擁有生動有趣的通訊服務。 
情緒感測門禁系統，除了能辨認人員身份外，更進一步辨識該員的情緒。本系統主要
利用輸入人臉與重建人臉之間的誤差，來當作身份及情緒的辨識依據。本系統是兩階段的；
第一階段採用個人取向的復原(Person-Specific Recovery)來辨識該員的身分，第二階才回復
人員的表情容貌並判斷所屬情緒。由於我們使用迭代式的PCA重建演算法，系統可以容忍臉
上有大部分的遮蔽，及強烈的光影變化。 
 
  
爬上爬下的，觀看電視可以任意的選台不用走來走去的，欣賞音樂時可以遠端選曲而不用
中斷目前的工作。然而眾多的遙控器可就不會讓人感到那麼幸福的，許多的問題也接踵而
至。像是搞亂裝置對應的遙控器、遺忘遙控器的擺放位置、或是太多的遙控器造成收納的
雜亂。另外各項裝置為提供完善的功能，使得遙控器上佈滿大大小小的按鍵。這些多如毛
的按鍵，反而造成使用者的使用上及學習上的困擾。我們需要一個能整合眾多裝置的控制
介面，同時這個介面應該要能提供直覺及方便的操作模式。利用人的肢體及聲音作為控制
器應該是最好的方式。使用者不用另外手持任何裝置，借助肢體動作及自然語言的輔助來
達成各項裝置的控制。因此在智慧型空間系統的室內部分，我們發展了智慧型家電操控系
統，其中包含：3D指向、語音指令、及肢體動作指令辨識系統。整個智慧型家電操控系
統如圖 2-2 所示：第一步，利用在室內架設的多台攝影機，所擷取的影像來估測3D指向。
第二步，當有家電被指到時，肢體的擺動或是麥克風收音來進行家電控制。同時利用肢體
擺動及聲音來操作，可以增加系統的可用性。使用者可以依靠當時的狀況來選擇，如：嘴
巴正在吃東西時就不適合用口語的方式來操控，手持物品時利用語言指令來操作則較恰
當。 
 
圖 2-2 智慧型家電操控系統示意圖。 
 
隨著網路及網路攝影機的普及，人們對於視訊溝通的需求也日益增加。然而跟過去的
文字及聲音通訊相比起來，影像相對得更容易洩露出使用者的隱私。通然人在家裡是相對
放鬆的，因此更容易失去戒心而將自己的隱私暴露出來。如果通訊的對象有不懷好意，非
常容易將自己陷入危機之中。縱使通訊的對象是可信任的對象，人難免有想要稍稍的隱藏
自己一下。這興起我們發展虛擬角色通訊系統的希望。發送端透過語音驅動或影像驅動接
收端的虛擬角色（如圖 2-3所示）。虛擬角色通訊另一項優點是，可以大大的降低頻寬的
需求。目前手機的網路頻寬不大，且其所需要費用仍相當可觀，在這個應用下虛擬角色通
訊更彰顯其效益。我們同時進行語音驅動及影像驅動的演算法研發，以利使用者適時的選
擇。語音驅動人臉唇形合成系統：人們對於人體運動行為有較高的敏感性，不真實、不自
然的人臉唇形動畫通常會干擾甚至打斷人們對語音的理解。目前在通過語音辨識驅動人臉
 圖 2-4情緒感測門禁系統示意圖。 
 
3. 文獻探討 
以下我們針對三個子系統的細部技術進行回顧，包含：3D指向系統、行為動作的分析
技術、語音驅動人臉唇形合成、低頻寬下虛擬角色之影音通訊、人臉復原、臉部表情辨識。 
 
(一) 3D 指向系統 
基於視覺指向系統方面，根據所採取的人體視覺外觀特徵作為依據可簡單的分為下列
四種作法，手部、眼部指向[1][2][3]、特定物指向[4]、指尖眼部結合指向[5]、手臂指向[6][7]。 
藉由手部的方向判定[1]，進行控制虛擬空間中的移動判斷。首先將攝影畫面轉換至YUV色
彩空間，偵測手部膚色區域。接著用模板比對法(Template Matching)，將事先建立好手指尖
的模板影像與上一步所偵測出來的手部膚色區域進行比對，計算出與模板比對的差異程度。
找到手指尖端點後，搜尋手部膚色區域的中線位置之最上方位置即為手腕的部份定義，此
為構成指向線的第二端點，將手腕點與手指尖端形成的指向線藉由與中線位置的角度差異
來判斷目前指向左或右，手部偵測面積多寡當作向前或後的判斷。藉由眼部所觀看的方向
判定[2]，進行控制虛擬空間中的移動判斷。使用兩台攝影機建立出立體視覺系統，藉由偵
測臉部較明顯的特徵在左右攝影機中相對應的點，計算出各別在三維空間中的位置，再根
據重建出來的特徵點計算出使用者目前所觀看的方向。 
使用特定物體作為偵測指向物，例如[4]使用手持紅色長棍，利用色彩資訊偵測出多台
攝影機畫面中的紅色目標物區域，根據不同的攝影機位置與其角度所偵測出來紅色指向物
方向，在三維空間座標中構成空間平面。得到攝影機構成的平面後，依據空間幾何概念兩
空間平面相交於空間直線，此空間直線即定義為該系統的指向線。接著再藉由指向線與投
影屏幕平面將相交於一點，此點定義為軌跡點。接著利用所找到的軌跡點，將三維空間座
方法都是先辨識框架中的姿勢，並使用符號將框架特徵編碼（代表該框架的姿勢），所以
每一段行為動作就形成了一個字串，由於不同的動作會被編碼成不等長度的字串，所以最
後須使用動態規劃（Dynamic Programming）為基礎的編輯距離演算法[20]做字串間的比對。
此編輯距離演算法可算出二字串之間的編輯距離（將一字串編輯為另一字串所包含插入、
刪除、置換等編輯動作的次數），距離越小則兩個比對的字串相似度越高。 
第二種動態時軸扭曲演算法[21]和編輯距離演算法的差異在於動態時軸扭曲演算法不
將框架一一用符號來做編碼，而是在比對的過程中直接使用框架的特徵來計算序列元素間
的距離。同樣地，動態時軸扭曲演算法算出的距離越小就代表兩序列相似度越高。隱藏式
馬可夫模型[22][23] 為雙重的隨機程序之模型，對於每種行為都可以訓練出一組模型來模
擬它。模型參數分別代表模型中的狀態、狀態的初始機率、狀態之間的轉移機率和觀測機
率。只要將行為動作的特徵抽取出來代入馬可夫模型中計算機率，就可以找出行為的種類。
另外，利用有限狀態機[24]事先定義好行為的移動軌跡，當要分析比對時只要抽取行為序列
的移動軌跡與之進行分析即可。也有利用主成分分析法[25]，從整張影像中將行為動作的前
景物件子影像部份抽取出來做主成分分析，降低維度後找出子影像的特徵空間，將行為動
作利用成串的特徵空間做描述，要進行行為動作分析時，定義計算兩個特徵空間的距離計
算公式即可進行分類比對。 
 
(三) 語音驅動人臉唇形合成 
目前臉部合成技術大多都已達到多維的效果，但是大部分在語音的方面，都是以五個
音素（“a”、“i”、“u”、“e”、“o”）或是英文的發音去逼近輸入的語音訊號。大致上都是記錄
常出現的多種不同的發音在臉部表情上會出現的特定唇形與臉部肌肉狀態，建立臉部表情
模型，當要進行臉部合成時再根據語音辨識的結果進一步合成[26][27]。利用人類臉部肌肉
模型與特性[28]，對每種不同的肌肉分別定義出肌肉收縮及擴張時網格點位移方程，並將語
音訊號中提取出其線性預估碼（Linear Predictive Code，LPC）倒頻譜。將這些特徵參數映
射至相應的物理模型控制參數上，利用肌肉的拉伸和下顎的轉動來驅動嘴型運動，從而實
現語音驅動唇形同步動畫。基於機器學習的語音驅動人臉動畫方法[29]，利用 MPEG-4 定
義的人臉運動參數 FAP（Facial Animation Parameters）和人臉定義參數 FDP（Facial Definition 
Parameters）相對應的特徵點從影像序列中提取出來，根據特徵點座標集合可以計算出 FAP
數據，藉由無監督聚類演算法就可以得到典型的 FAP 模式。對同步錄製的語音訊號提取出
LPC 係數、相對頻譜感知線性預參數(RASTA-PLP)和一些韻律參數：如能量、基頻等，從
而獲取與 FAP 模式一一對應的語音特徵序列，最後在透過訓練神經網路，學習從含上下文
及韻律參數的語音特徵到 FAP 模式上的映射來實現語音與人臉動畫的有效同步。 
 
(四) 低頻寬下虛擬角色之影音通訊 
在一般常見的個人通訊軟體當中，傳輸視覺資料絕大部分都是將輸入影像經過壓縮處
理之後傳輸。但是就算視訊影像經過壓縮之後，在傳輸的過程當中還是需要一定的傳輸量，
並不會是非常小。因此，使用視訊通話的時候網路品質就變成一個影響視訊品質之非常重
要的因素。除了視訊壓縮之外，想要達到更低的傳輸頻寬之視訊個人通訊器通常的做法分
同樣以PCA為基礎，Hwang [52]提出一個非迭代式的人臉影像復原技術。這個方法是
尋求一個最佳的線性組合係數來重建人臉，所謂最佳的線性組合係數是以重建出的人臉在
沒有被遮蔽的部分能達到最小失真為訴求，然後再利用此最佳線性組合係數重建出被遮蔽
的部份。但此法為了能準確區分出未被遮蔽的人臉區域，必須事先以人工方式去標記，然
後用最小平均平方差去找出最好的係數值集合。 
在上述的人臉復原方法中都有一些潛在的問題。首先，他們的PCA都是使用多人的樣
本來建立一個共同的線性子空間，這種多人共用線性子空間會讓一些重要的個人特徵因為
混雜而消失。此時會發生復原後的人臉影像根本與任何人都不太像的模擬兩可臉
（In-Between Person Face），這樣的回復人臉根本無助於人臉的辨識。再者，利用輸入的
人臉和PCA復原的人臉做權重聚合，復原後的人臉也可能會產生問題，當輸入的人臉有較
大程度的破損或干擾時，這些都會嚴重影響聚合出來的人臉。 
除了部分遮蔽的情況外，光影的變化也會對人臉影像產生干擾，前人所提方法中
[50][51][52][53][54]，大多不對光影的變化做處理，每個復原後人臉的光影變化會傾向於平
均的人臉亮度，原本的人臉光影變化就不會出現在復原後的人臉上，如果光線亮度變化很
劇烈，復原的人臉效果將會出現錯誤，也就更不可能把原來的光影回復到人臉上。 
 
(六)臉部表情辨識 
在過去表情辨識系統一直是心理學家研究的課題之一，西元1978 年Suwa[62] 企圖對
一段影片分析臉部表情，但是由於人臉偵測、追蹤及辨識的技術還未臻於成熟，直到1991 年
技術逐漸成熟Mase 和 Pentland[63]兩位學者使用Optical Flow 對臉部特定的區域的肌肉群
粗略的計算其移動向量，也因此重新對表情自動辨識系統加以詮釋。Padgett [57]使用主成
分分析針對眼睛及嘴巴抽取特徵，作者僅針對靜態影像抽取特徵，由於情感通常是由一系
列臉部表情動作所組成，故具有時間的連續性，所以靜態影像的分析無法完整地包含其中
所代表的意義。Black 和 Yacoob [58]使用Optical Flow 對眼睛、眉毛、鼻子及嘴巴進行分
析，然後將資料表示成一組形變的係數作為辨識的主要依據。Lyons [59]選擇34 個基準特
徵點，利用Gabor wavelet 在每個基準點上不同方向及大小的一組係數，同時為了減少資料
量，作者使用主成份分析降低資料維度再將資料送進類神經網路訓練。 
相對於基本情緒的辨認，動作元(Action Units)的分析辨認也就顯得更為敏感(sensitive)
及複雜(complexity)。Bartlett[64] 結合全域分析(holistic spatial analysis) 與Optical Flow 對臉
的上半部(眉毛、眼睛及臉頰)進行單一AUs 的辨認(AU1,AU2,AU4,AU5,AU6,AU7)，研究中
並沒有對臉的下半部(鼻子、嘴巴等)加以分析，其辨識率為91%。Donato[65]則比較了數種
方法，包括有Optical Flow、主成分分析(PCA) 、ICA(independent component analysis) 及
Gabor wavelet 等，實驗結果顯示ICA 及Gabor wavelet 有比較好的辨識效果，在臉的上半
部其總共辨識六個單一AUs、而下半部則辨識有二個單一AUs 及四個組合AUs，其辨識率
皆為95.5%。Lien[8]結合dense flow、特徵點追蹤及邊的抽取等三種方法進行特徵的抽取，
在臉的上半部總共辨認有四個單一AUs 及二個組合AUs、下半部則有四個單一AUs 及五個
組合AUs，辨識率為80%~92%。Tian[9] 提出多狀態的人臉組成成份模型(Multistate Face 
Component Models)，包含有嘴唇、眼睛、眉毛和臉頰，並利用特徵點追蹤進行各個模型特
因為攝影機透鏡的關係會有歪斜的情況導致歪斜線的產生，所以定義兩空間中直線形成之
歪斜線之中點為目標物件的座標。指向系統的運作根據操控物件座標為空間中的一點，使
用者指向操控物件的指向線則為空間中的直線，所以利用空間中點到直線的最短距離與設
置的門檻值進行使用者是否有指向操控物件的判定。 
 
 (二) 行為動作的分析技術 
我們的行為動作的設計著重在雙手的變化，手臂的擷取藉由平均位移演算法（Mean 
Shift Algorithm）找出畫面中的人體軀幹。將軀幹自人體中去除後即可定位出上肢（手臂），
然後再利用主成分分析法（Principal Component Analysis）即可找到各手臂的主軸方向，將
此主軸方向編碼為平面座標系統中四象限的角度即成為辨識身體姿勢（Postures）的重要特
徵。假設肢體主軸左邊端點座標為 ( , )x yL L ，右邊端點為 ( , )x yR R ，徑度值的計算公式如式(1)，
出現主軸垂直的手臂時則例外討論。 
                        arctan
y y
x x
L R
angle
L R
 
 
 
 
 (1) 
鑑於每一個肢體動作是由少數幾個不同的連續姿勢所組成，若能先將所有可能的身體
姿勢做適當分群，將類似的身體姿勢歸於同類，如此就可用更精簡的姿勢表示法來表示各
種不同的肢體動作。但是，某些相似姿勢間的混淆性，可能造成姿勢在分類時的誤判而影
響肢體動作的辨識。基於此因，本論文提出一個以模糊理論的概念所發展出的強健特徵－
姿勢群隸屬度特徵（Posture Cluster Membership）。這個姿勢群隸屬度特徵藉由預先建立各
個姿勢群的高斯機率模型，然後再用以計算某姿勢分別屬於各姿勢群的機率值，並將這些
機率值結合為一個機率向量以描述該姿勢。假設我們有M個姿勢群，針對每一個姿勢群 iP，
我們可以定義出 M 個姿勢隸屬度函數（Membership Function） ( )im x ，可用來計算出姿勢
樣本 x屬於 姿勢群的程度值，因此姿勢群隸屬度的特徵向量就是 1 2[ ( ), ( ),..., ( )]Mm x m x m x 。
本研究中，利用高斯模型來建立出每一個姿勢群的隸屬度函數。 
2
2
1 ( )
( )
22
x
P x exp

 
  
  
   
(2) 
1
1
( | )
M
i d i
d
f p y P
M 
 
 
(3) 
高斯模型公式如上式(2)所示，為所有資料的平均值，為標準差，當有一未知的姿勢樣
本 y輸入，代入每一個姿勢群的高斯模型 ( )iP x 計算所有特徵的機率值加總機率 if 式(3)後，
即可當作姿勢 y 隸屬於姿勢群 的隸屬度。用同樣的方法去計算y 隸屬於其他姿勢群的隸
屬度可得到 1 2( , ,..., )Mf f f f ，其中M 為姿勢群總數，而 f 即為從姿勢 y中抽取出的姿勢群
隸屬度特徵向量。 
同時我們提出模糊動態時軸扭曲（Fuzzy Dynamic Time Warping）演算法，在傳統的動
態時軸扭曲演算法中引入隸屬度交集（Membership intersection）以及姿勢群轉移機率
（Posture Cluster Transition Probability）之新概念來進行肢體動作的辨識分析。經由實驗証
明，姿勢群隸屬度特徵配合模糊動態時軸扭曲演算法進行肢體動作的比對，比起使用傳統
的動態時軸扭曲演算法明顯能有更好的辨識結果。 
iP
iP
11
1
1 1 1
1
1
1
1
1
0                            
  ,     
( ) ( )
( ) ( )
- 1
 ,          
0                                
m
m
m m
m m h
m m
m S
m m
m m
m
k f
k f
f k f
f f B f B fN
B k f B b f m
f k F M
f k f
f f
f k



 





   
    
          
 


，
　　
，
，
 
其中 Mm1 ，M 為頻帶總數， )(kBm 為第m個頻帶的三角濾波器，第m個頻帶濾波器之
中心頻率為 mf ，每個三角濾波器的中心頻率，就是兩個相鄰臨界頻帶的截止頻率，其中
10  Mm ， 1f 為頻帶之最低頻率， hf 為頻帶之最高頻率， sF 為取樣頻率， N 為取樣點
數，B與 -1B 為梅爾刻度。再計算每個濾波器累加能量的對數，如下式： 






 


1
1
)()(log)(
2
m
m
f
fk
m kBkXmY  
最後對全部M 個濾波器的累加能量對數作離散餘弦轉換(discrete cosine transform， DCT)，
即可得到梅爾頻率倒頻譜。本論文取 12 個梅爾倒頻譜係數，另外，將這 12 個係數做一階
差分，因此每個音框的特徵參數個數為 24 個。 
一小段語音，其特徵參數是隨著連續不斷的口腔變化共鳴產生改變，而這連續性的變
化可視為 HMM 中由左至右(left to right)的狀態轉移，可藉由輸入一段測試語音，計算各個
馬可夫模型的機率來判斷辨識結果。HMM 是一種以機率統計為基礎的辨識模型，能有效
處理如語音等非固定長度的輸入，且因為 HMM 辨識並非使用失真度的大小來判斷，因此
對於輸入資訊的敏感度較低，語氣、語調和音量大小等變化的影響抵抗力較高。語音辨識
使用由左至右的 HMM 是由於語音產生的過程和抽取語音特徵參數的方法使得狀態轉移只
會由左至右，例如：「江」(ㄐㄧㄤ)這個中文字，若是把這個字的ㄐ、ㄧ、ㄤ看成三個狀態，
分別標示 1、2、3，以短時段音框為時間單位，當我們觀察它的狀態轉移，狀態只會依照 1、
2、3 這個順序出現，因此使用由左至右的型態。觀察也發現每一次說「江」這個中文字分
別停留在ㄐ、ㄧ、ㄤ各個狀態的時間單位不會一樣，故需計算機率值估計序列可能性，得
到語音序列的機率密度函數後，在以高斯混合密度取得最佳的狀態序列[40]。 
對於中文發音先定義八個基本嘴型，“a”、“i”、 “u”、“e”、 “o”、 “er”、
“n”及閉嘴，依據這八個基本嘴型的影像圖片分別定義其臉部定義參數（Facial Definition 
Parameters），根據中文字的基本嘴型編碼來合成出中文字的嘴型。在合成影像時，合成對
象的形狀及紋理是分別計算的，先依據所定義的人臉特徵點(Landmark)，再利用 Delaunay
三角化演算法 (Delaunay Triangulation)[41]在此特徵點集合上取得三角形網格 (Triangle 
Mesh)，讓起始圖與目的圖能夠產生互相對應的網格（mesh），並將網格間對應的頂點進一
步利用雙線性內插法內插出新的頂點，最後可以產生出新的一組頂點。頂點所構成的網格
內的點的像素值就由頂點的像素值做內插，形成一張內插出的圖片，利用相同的方法，可
以內插出其他的圖片，進而形成動畫。在語音訊號經過辨識後，將每個中文字的辨識結果
依照嘴型資料庫的嘴型編碼，再根據語音前處理的端點偵測，算出每個中文字所需要的內
用函數 () ()S 、 表示合成後的外形以及紋理，式子如上面列的 s和 ( )a x ，而外形與紋理的區
域性變化量則是： 
1
'
1
ˆ( ; ; ) ( ; ; )
ˆ( ; ; ) ( ; ; )
M
i i
i
M
i i
i
S p s S p s S p s S s
A a A a A a A a  


   
   


 
其中 1 2 M 1 2 M'ˆ ˆ ˆ ˆ ˆ ˆ[ , ,..., ] A=[ , ,..., ]
T TS s s s a a a ， 。為了使合成對象與使用者的外形一致，那麼合成
對象的外形與紋理區域性變化量必須要趨近於使用者。因此，需要將合成對象的基底形狀
特徵向量一併考慮，所得的外形即包含了表情特徵，將變化量公式改寫如下： 
'
, ,
1
, / ,
1
ˆ( ; ; )
ˆ( ; ; )
O
O
M
O O O O O i O i
i
M
O O O O O i o i
i
S p s S s p s
A a A a a 


 
 


 
因此，虛擬角色的表情可以藉由基底臉部形狀和基底臉部紋理透過參數 Os 、 Oa 、 ,O is 、
,O ia 、 ,O ip 和 ,O i 求得，在進行影音傳輸時，由於前四個參數是在離線訓練時就會產生的，
故可事先將這些靜態資料存放於說話者與受話者端。由於肉眼對於影像的遺漏或是延遲並
沒有比聲音的延遲或中斷明顯，在影音資料傳輸時我們選擇維護良好的聲音通訊，將聲音
與影像分開傳送並優先處理聲音資料。需要傳遞的資料量，為了使影像與聲音資料同步，
在每一筆傳輸資料皆加上時間標記(Timestamp)，其為一無正負號之整數。影像方面有合成
虛擬角色之臉部表情的基底外形係數 ,O ip 和基底紋理係數 ,O i ，這些係數皆是由浮點數所組
成，為了對齊虛擬角色和說話者之臉部位置、旋轉角度以及大小，還必須透過全域形變轉
換公式[42]來使其對齊。聲音的部份可以依照不同網路條件以不同的取樣頻率 sf (Sample 
Frequency)來錄製聲音，並使用MP3壓縮技術[48]來壓縮音訊資料。為避免受話端音資料傳
輸不連貫造成中斷的情形，使用緩衝器機制將資料預存達到緩衝目的，並採用聲音根據時
間標記驅動影像的方式播放影音資料，使得影音資料能夠同步播放。 
 
 (四)人臉復原 
 遭受遮蔽的人臉，常會造成身份及表情辨識演算法的崩壞。因此本研究導入迭代 PCA
復原演算法，用以回復受遮蔽人臉，進而提升辨識演算法的準確度。現在已有許多抗遮蔽
的人臉特徵的演算法，因此我們設定特徵點可以被定位完整。我們的演算法在特徵點定位
完畢後，開始進行人臉紋理的復原。在復原人臉的過程中，我們會產生兩種不同的人臉，
一個是重建人臉 𝐟′𝒕 = [𝑓′𝑡(1), … , 𝑓′𝑡(𝑙)]
𝑇，另一個是復原人臉 𝐟"𝒕 = [𝑓"𝑡(1), … , 𝑓"𝑡(𝑙)]
𝑇，t是
迭代次數，𝑙是像素數量。重建人臉是藉由個人特徵空間所重建出，而復原人臉則是復原遮
蔽或被破壞後的人臉。 
重建人臉紋理的過程，是將輸入人臉投影至特徵空間中，再重建出具有個人特徵的人
臉。第一次迭代時，我們將復原人臉的初始定義為輸入的人臉 f，亦即 𝐟′𝟎 = 𝐟。將輸入人臉
投影至個人特徵空間，產生重建後人臉 𝐟′𝟏，  
P 張復原後的人臉。將復原後的人臉和復原前的人臉（即輸入之人臉）計算 RBIA 
（Recognition by Input Approximation Method ; 輸入逼近辨識法) 誤差，誤差最小的人臉則
為第一階段人物身份辨識結果。先對人物身份做辨識，再將身份辨識結果投入到第二階段
做表情辨識。此方式在辨識邏輯上是合理的，因為在辨識正確的情況下，輸入的人臉會同
時屬於此人的個人特徵空間與個人表情特徵空間。此方式並可以大量減低復原與辨識所需
的時間成本，若採用一階段辨識方式，即對每個人的所有表情特徵空間做復原以及辨識，
相比之下兩階段辨識方式所花費的成本較低。 
第一階段我們將輸入人臉對所有人的個人特徵空間做復原，總共有 P 個人就會得到 P
張復原後的人臉。在辨識時我們若將復原後人臉與資料庫中的平均樣版人臉拿來做比對，
就會產生不好的辨識結果。因為我們復原的方法會將遮蔽區域以個人的平均人臉紋理取代
並再投影至個人的特徵空間。即使輸入人臉不屬於此個人特徵空間，復原後的人臉會與此
特徵空間接近。若與平均樣版或模型比對，就會因為這兩張臉很相像，導致出現辨識錯誤
的結果。然而我們發現若輸入人臉屬於此人特徵空間，所復原後的人臉產生的還原誤差就
會較小。相對若輸入人臉不輸於此人的特徵空間，所復原出的人臉會與原輸入人臉產生較
大的還原誤差。因此我們提出了一個辨識的方法，採用復原後人臉與復原前的人臉做差異
度比對。我們稱此種比對辨識法為輸入逼近辨識法（Recognition by Input Approximation 
Method），簡稱為 RBIA。這樣的做法中，二個被比較的人臉都是依輸入狀況動態產生的影
像，就不會受限於固定樣板或模型，計算方法如 
𝑝∗ = 𝑎𝑟𝑔 min
1≤𝑝≤𝑃
∑|𝑓′(𝑖)(𝑝) − 𝑓(𝑖)|
𝑝
 ,    1 ≤ 𝑖 ≤ 𝑙
 
其中P為人物總數，𝐟′(𝑝)代表還原後的臉， 𝐟 則是輸入人臉。將輸入人臉𝐟與復原人臉𝐟′(𝑝)的
所有像素相減並做累積後得到差異值，最後將所有人物都計算過後，差異值最小的人物就
為人物身份辨識結果。
                                            
 
在上述的方法中，我們計算兩張影像的所有像素差異值。但是在輸入人臉上有被遮蔽
或被破壞的部分，拿這些被破壞區域像素去做計算是不恰當的。因為一張是有遮蔽的臉另
一張為復原後無遮蔽的臉，這些遮蔽區域像素與復原後的像素差異就會很大也不可靠，會
嚴重影響辨識結果。所以我們必須把這些被遮蔽的區域先行排除再做計算，取第 %的像
素值當做門檻值，將人臉上所有小於這個門檻值的像素都保留下來。決定了臉上要納入
計算像素的數量，這樣的方式可以避免因輸入人臉上有各種破壞或遮蔽而干擾辨識結果。
人物身份辨識的計算公式就修改成下式，  
                    
 𝑝∗ = 𝑎𝑟𝑔 min1≤𝑝≤𝑃 {
1
𝐶(ℛ)
∑ |𝑓′(𝑖)(𝑝) − 𝑓(𝑖)|𝑝 }  ,    1 ≤ 𝑖 ≤ 𝑙 
其中ℛ表示有效像素點，𝐶(ℛ)代表納入計算像素的總數，𝑝∗為第一階段辨識出的人物身份
編號。 
在得知人物身份後，第二階段採用個人表情取向的復原(Personal expression-Specific 
Recovery)，針對此人物的所有表情特徵空間進行復原，此人有 K 個表情，就會得到 K 張表
情復原後的人臉，再使用 RBIA 方法計算誤差時，誤差值最小的表情則為最終表情辨識的
結果。 
物件三 0 0 397 0 11 97.3% 
物件四 0 0 0 404 4 99.0% 
表 4 穿著短袖時指向正確率 
 物件一 物件二 物件三 物件四 無指向 正確率 
整體 
正確率 
物件一 393 0 0 0 15 96.3% 
93.9% 
物件二 0 381 0 0 27 93.4% 
物件三 0 0 350 0 58 85.8% 
物件四 0 0 0 404 4 99.0% 
 
(二) 行為動作的分析技術 
本研究的目標藉由肢體動作的分析，來控制屋內的家電。我們目前設計了八個不同的
肢體動作，使用者可以依喜好決定動作與相對應的指令，例如：開關、調上下、轉左右、
開始、暫停等等。下圖 5-1列出八個指令動作。 
 
指令編號 動作序列 
指令一 
 
指令二  
 
指令三   
 
指令四 
 
指令五    
 
指令六 
 
指令七 
 
指令八 
 
圖 5-1 指令與動作序列對應。 
 
 
圖 5-2聲音驅動型合成之系統展示，輸入語音為“雲端科技會遭遇重大的打擊和恐怖主義” 
 
 
圖 5-3 輸入語音為「東華」與樣本一嘴型合成之對應影像 
 
接下來的實驗將探討在中文唇型動畫中，聲母與韻母分別對整體動畫的影響力。由圖
圖 5-4(a)、圖 5-4 (b)可以觀察出，雖然聲母嘴型有差異，但是對整體動畫的影響卻是很些
微的；由圖 5-5 (a)、圖 5-5 (b)可以觀察出，雖然聲母相同，但是其動畫的呈現上面，使
感受到兩者顯著的差別。故由此可知，及便在整體中文語音辨識率不佳的狀況下，只要韻
母的辨識率能夠好，對中文唇型動畫的效果上就會令人覺得真實且自然。 
 
 
(a) 
 
(b) 
        圖 5-4 (a)韻母同為「ㄡ」，聲母分別為「ㄕ」與「ㄍ」的嘴型動畫差異； 
            (b)結合韻母同為「一ㄝ」，聲母分別為「ㄉ」與「ㄐ」的嘴型動畫差異 
 
圖 5-7 史瑞克在不同情況下之影片透過 Skype 傳輸所需之傳輸量與本研究的方法之比較
圖 
接著進行圖像品質的比較，我們把影片以每秒5張影像製作成片長約10秒的影片，透過
JM工具分別固定不同輸出比產生6組不同壓縮比的影片片段，並計算每一張Frame與未壓縮
影片的Frame之PSNR，再取平均值。PSNR是計算未壓縮影片以及壓縮後的影片之間的誤差
所算出來的一個比值。PSNR越高代表影片的失真越小，跟原本未壓縮的影片也就越像，但
是同樣地影片的大小也就越大。如何在PSNR與Bitrate之間取到一個平衡也是一件重要的事
情。本論文所使用的方法不會造成任何的失真，圖 5-8表示本研究與壓縮阿凡達和史瑞克
之Bitrate與PSNR關係圖。 
 
圖 5-8 本研究與壓縮阿凡達與史瑞克之 Bitrate與 PSNR之關係圖 
 
(四)人臉復原 
我們使用人臉影像資料庫為 cohn-kanade 表情資料庫，由於資料庫中並非所有人都有六
種基本表情（Anger、Disgust、Fear、Happy、Sad、Surprise）的影像，以及每種表情中的
影像數量也不一。我們要使用 PCA 方式對個人以及每個人的表情特徵空間做訓練，必須要
有足夠的樣本影像才能建立特徵空間，因此從資料庫中挑選出 17 個人，5 位男性與 12 位
女性，以及每個人挑選出不同表情的訓練樣本共 7 張並建立個人特徵空間。再挑選每個人
(五)臉部表情辨識 
第一階段辨識，我們將輸入人臉經由每個人的特徵空間復原出的人臉做 RBIA 人物身
份辨識，並統計辨識率。圖 5-11在遮蔽無光影變化人臉上， =50%時有最高的辨識率為
96.6%，在遮蔽並有加光影變化人臉上， =50%時有最好的辨識率為 91%，有光影變化比
無光影變化的人臉辨識率低，因為光影變化的部分也可視為一種遮蔽，有光影變化遮蔽人
臉的遮蔽區域變大，造成在人臉上可用來辨識人物身份的特徵紋理部分就變少，因而導致
辨識錯誤。而遮蔽部分越大，辨識率越低也屬正常。圖 5-11中我們將無亮度正規化與有
亮度正規化的人臉辨識結果做比較，在無光影變化人臉上，無亮度正規化的人臉辨識率為
99%，但在有光影變化上人臉辨識率只剩 84%，而有做亮度正規化的人臉辨識率為 91%。
會比較高的因為是由於在有光影變化的人臉上，如果沒有做亮度正規化，低亮度人臉在加
了亮光影變化後，復原後的人臉就會與高亮度人臉相近，因而造成誤判，相對高亮度人臉
在加了暗光影變化後，復原人臉會與低亮度人臉相近也會造成辨識結果錯誤。  
 
圖 5-11身份辨識結果統計表，縱軸為辨識率，橫軸為前 %相似像素。 
 
接者是第二階段表情辨識結果，依照第一階段人物身份辨識結果，將輸入人臉用該人
的各個表情特徵空間做復原，然後用 RBIA 方法做表情辨識，並統計各個表情的辨識率。
圖 5-12 在無光影變化的情況我們的平均辨識率為 96.25%，在有光影變化情況下平均辨識
率為 91.6%。有光影變化的表情辨識率降低原因於在有遮蔽又有光影變化的情況下，即使
能將光影呈現在復原人臉上，在不同的表情下能夠有效區分的表情特徵紋理都已被遮蔽掉
大半，所以導致辨識錯誤。但也會出現人物辨識錯誤但表情辨認正確情況。即使辨識為錯
誤的人，再利用他的表情特徵空間做復原，復原後表情人臉的誤差就也有可能會小於與其
他表情復原人臉的誤差，因而得到正確的表情辨識結果。 
圖 5-12 中我們也將無亮度正規化與有亮度正規化的表情辨識結果做比較，無亮度正
規化在有光影變化的平均表情辨識率為 86.4%，而有做亮度正規化的平均表情辨識率為
97% 
91% 
99% 
84% 
70%
75%
80%
85%
90%
95%
100%
30% 40% 50% 60%
R
ec
o
g
n
it
io
n
 r
a
te
 
有亮度正規化且無亮度變化 
有亮度正規化且有亮度變化 
無亮度正規化且無亮度變化 
無亮度正規化且有亮度變化 
May 2000. 
[9] Ying-li Tian, Takeo Kanade, and Jeffrey F. Cohn, “Recognizing Action Units for Facial 
Expression Analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 
Vol. 23, No. 2, Feb. 2001. 
[10] R. O. Dudac, P. E. Hart, and D. G. Stork, ”Pattern Classification.” Second ed., Wiley 
Interscience, 2000. 
[11] C. S. Myers and L. R. Rabiner, “A comparative study of several dynamic time-warping 
algorithms for connected word recognition.” The Bell System Technical Journal, 1981. 
[12] L. Rabiner and B. Juang, “An introduction to hidden Markov models.” IEEE ASSP 
Magazine, Vol. 3, No. 1, Part 1, pp.4-16, 1986. 
[13] O. Boiman and M. Irani, “Detecting Irregularities in Images and in Video.” International 
Journal of Computer Vision, Vol. 74, No. 1, pp.17-31 , Aug 2007. 
[14] Y.–C.Chen, “Action Recognition Using Multi-class Boosting.” National Taiwan 
University, 2006. 
[15] Y. Freund and R. E. Schapire, “A Short Introduction to Boosting.” Journal of Japanese 
Society for Artificial Intelligence, Vol. 14, pp.771–780, 1999. 
[16] P. Guo and Z. Maio, “A Home Environment Posture and Behavior Recognition System.” 
in Proc. International Conference on Convergence Information Technology, pp.175-180, 
2007. 
[17] C.–F.Juang and C.–M.Chang, “Human body posture classification by a neural fuzzy 
network and home care system application.” IEEE Trans. on Systems, Man and 
Cybernetics, Part A, Vol. 37, No 6, pp.984-994, 2007. 
[18] A.F. Bobick and J.W. Davis, “The Recognition of Human Movement Using Temporal 
Templates.” IEEE Trans. on Pattern Analysis and Machine Intelligence, Vol. 23,  No. 
3,  pp.257–267, 2001. 
[19] J.–W.Hsieh, Y.–T.Hsu, and H.–Y.Mark Liao, “Video-based human movement analysis 
and its application to surveillance systems,” IEEE Trans. on Multi- media, Vol. 10, No. 3, 
pp.372-384, April 2008. 
[20] Y.–T. Hsu, J.–W. Hsieh, and H.–Y. Liao, “Human Behavior Analysis Using Deformable 
Triangulations.” in Proc. International Conference on Multimedia Signal Processing, 2005. 
[21] J. Wang, Y. Makihara, and Y. Yagi, “Human tracking and segmentation suppo- rted by 
silhouette-based gait recognition.” in Proc. IEEE International Conference on Robotics 
and Automation, pp.1698-1703, May 2008. 
[22] L. Rabiner and B. Juang, “An introduction to hidden Markov models.” IEEE ASSP 
Magazine, Vol. 3, No. 1, Part 1, pp.4-16, 1986. 
[23] N. Oliver, B. Rosario, and A. Pentland, "A Bayesian Computer Vision System for 
Modeling Human Interactions." IEEE Trans. on PAMI, vol. 22, No. 8, pp.831-843, Aug 
2000. 
Vision—ECCV’98, p.484, 1998. 
[44] I.T. Jolliffe, “Principal component analysis. Springer verlag,” 2002. 
[45] I. Matthews and S. Baker, “Active appearance models revisited,” International Journal of 
Computer Vision, Vol. 60, pp.135–164, 2004. 
[46] P. Viola and M.J. Jones, “Robust real-time face detection,” International Journal of 
Computer Vision, Vol.57, pp.137–154, 2004. 
[47] D. Austerberry, “The technology of video and audio streaming,” Focal Pr, 2005. 
[48] Intel. Intel®  Integrated Performance Primitives Unified Media Classes Reference Manual, 
2007. 
[49] C.H. Liao, “A 3-D Pointing and Human-Machine Interaction Interface for Intelligent 
Home,” National Dong Hwa University, 2010. 
[50] B.G. Lee et al. “Enhanced computational integral imaging system for partially occluded 
3D objects using occlusion removal technique and recursive PCA reconstruction.”Optics 
Communications, 283:2084–2091, 2010. 
[51] Z.M. Wang and J.H. Tao.“Reconstruction of Partially Occluded Face by Fast Recursive 
PCA.”In Proceedings of the 2007 International Conference on Computational Intelligence 
and Security Workshops, pages 304–307.IEEE Computer Society, December 2007. 
[52] B.W. Hwang, S.W. Lee, V.M. Inc, and S.K. Seoul.“Reconstruction of partially damaged 
face images based on a morphable face model.”IEEE Transactions on Pattern Analysis and 
Machine Intelligence, 25(3):365–372,2003. 
[53] J.S. Park, Y. Oh, S. Ahn, and S.W. Lee.“Glasses Removal from Facial Image Using 
Recursive PCA Reconstruction.In Audio-and Video-Based Biometric Person 
Authentication,”pages 369–376, June 2003. 
[54] C. Du and G. Su.“Eyeglasses removal from facial images.”Pattern Recognition Letters, 
26(14):2215–2220,2005. 
[55] M. Turk and A. Pentland.“Face recognition using eigenfaces”. In Proc. IEEE Conf. on 
Computer Vision and Pattern Recognition, volume 591, pages 586–591, 1991. 
[56] Yen-Wei Chen, Rui Xu, and Arika Ushikome.“Serially-connected Dual 2D PCA for 
Efficient Face Representation and Face Recognition.”International Journal of Innovative 
Computing, Information and Control,5(11):4367–4372, 2009. 
[57] C. Padgett, G. Cottrell, “Identifying Emotion in Static Face Images,” Proc. Second Joint 
Symp. Neural Computation, vol. 5, pp. 91-101, La Jolla, Calif., 1995. 
[58] M.J. Black, Y. Yacoob, “Recognizing Facial Expressions in Image Sequences Using Local 
Parameterized Models of Image Motion,” International Journal of Computer Vision,vol. 
25, no. 1, pp. 23-48, 1997. 
[59] M.J Lyons, J. Budynek, S. Akamatsu, “Automatic Classification of Single Facial Images,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 21, no. 12, Dec 
1999. 
1. 低頻寬下虛擬角色之影音通訊技術頗具趣味性與實用性，可用於智慧型空間內使用者
與遠端其他使用者之影音通訊，與ＭＳＮ等通訊軟體不同的是我們可以在很低的頻寬
需求下，傳送影音給通訊者，而且可以利用虛擬角色的臉和表情來呈現，相當具有特
色，且具有一定之市場吸引力。 
2. 3Ｄ指向定位技術結合語音命令的組合，建構成一個自然易用的多模式人機互動介面，
可應用於居家生活中的家電操控，特色是使用者不再需要依賴數量眾多的遙控器來操
控家電，只要將手指向家電再直接下達語音命令即可操控，易相當具有實用性與進步
性。 
3. 以聲音驅動唇形變化技術只要輸入語音或文字即可合成出虛擬角色之臉部及唇形動畫，
可應用於虛擬教師、虛擬主播、虛擬演員等，在數位學習和數位娛樂上也有不小之應
用潛力。 
 
 
97 年度專題研究計畫研究成果彙整表 
計畫主持人：江政欽 計畫編號：97-2221-E-259-028-MY3 
計畫名稱：智慧型空間之多模人機互動介面設計與整合 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 3 3 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 2 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 ■洽談中 □無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本研究建構一個家用智慧型空間系統，並包含三個主要子系統：智慧型家電控制、虛擬角
色通訊、及情緒感測門禁系統。此系統試圖提供家庭成員全方面的科技享受。智慧型家電
控制系統主要應用於室內環境，使用者以肢體動作搭配語音指令，來自然及便利的操作家
電。虛擬角色通訊系統應用與家庭成員與遠端人員互動，提供使用者利用語音或臉部動作
等方式，來驅動遠端的虛擬角色。同時能避免隱私的洩漏，也可降低頻寬的需求。情緒感
測門禁系統作為家的門神，除了能辨識訪問者的身分外，也能辨識訪問者的情緒來提供家
庭成員應對的參考。 
 
前所研發技術均甚具前瞻性，以國內而言，所提之方法也與眾不同，且也獲致不錯之效能，
因此甚具有學術研究之價值，各項技術也都建置有雛形展示系統，均能實際操作測試，在
經過適當之系統包裝與調校後，已具實用價值。除部分技術已發表於相關之國際會議與國
際期刊外，其他技術亦正在進行論文撰寫，擬投稿至國際期刊。其中數項技術甚具應用潛
力，包括： 
1. 低頻寬下虛擬角色之影音通訊技術頗具趣味性與實用性，可用於智慧型空間內使用者
與遠端其他使用者之影音通訊，與ＭＳＮ等通訊軟體不同的是我們可以在很低的頻寬需求
下，傳送影音給通訊者，而且可以利用虛擬角色的臉和表情來呈現，相當具有特色，且具
有一定之市場吸引力。 
2. 3Ｄ指向定位技術結合語音命令的組合，建構成一個自然易用的多模式人機互動介面，
