1中英文摘要及關鍵詞
本計畫所要探討的問題是如何辨識與切割數位影像中所包含的各類物件。我們採取的研究
方式是將兩個問題一併考慮，而不只是將物件辨識和物件切割當成兩個獨立的問題來處
理。多年來，電腦視覺領域已經有許多論文單獨探討物件辨識或影像切割問題，儘管有了
相當大的突破，但離實用仍有一段落差。由於物件辨識和切割實為息息相關的問題，近年
來也開始有研究學者從同時解決這兩大難題的觀點切入，試著提出了一些嶄新而實用的方
法，讓這兩個問題可以獲得更好的解決。本計畫希望能從這樣的觀點出發，運用影像特徵
表示法、機率統計模型和機器學習理論等等相關技術，提供一個可能的解決方案。我們已
初步建立了一套系統，能夠利用統計學習模型，從影像資料庫中學出物件類別與影像區塊
和影像特徵之間的關聯，並藉此來辨識物件。目前物件切割的部份仍需依賴使用者手動輔
助，未來將繼續利用我們已建構好的影像特徵統計模型，來完成物件辨識和切割的整合。
The aim of this project is to address the machine vision problems of recognizing and segmenting
multiple objects in an image. Our proposed approach to these two problems is taking both of
them into account in a unified probabilistic framework instead of solving object recognition and
segmentation separately. We try to apply statistical modeling and machine learning to these two
problems, and have built a system that can recognize several categories of objects based on the
learned image features. So far the system relies on manually selected regions to specify the
objects of interest for recognition. We are working on incorporating object segmentation into our
system using the learned statistical model and image features.
關鍵詞：物件辨識 (Object Recognition)、物件切割 (Object Segmentation)
3在這個影像資料庫中的每一張影像，會對應一張利用著上不同顏色，來標記不同類別物件
的影像。如下圖：左邊是資料庫中的一張影像，右邊則是對應的影像分割。影像資料庫中
共有十四類物件。因為部分種類的影像數量不夠多，故只取九種類別的影像來實作。分別
是建築物、草地、樹、牛、天空、飛機、人臉、汽車和腳踏車，總共 240 張原始影像。草
坪、樹、建築物、飛機、牛、人臉、汽車、腳踏車每類各 30 張。天空的影像區塊則是雜散
於其他八種類別的影像中。對於以上八類影像，從每一類 30 張中取出 15 張來作為學習的
影像樣本，剩餘 15 張則用來測試辨識率。每張影像的大小都是 320 像素*213 像素。
四、研究方法
影像前處理：先將影像，從 RGB 顏色空間，轉換到 CIELAB 的顏色空間，其中 L 即是此
像素的亮度性質，A 和 B 則是顏色性質。對每一張影像，透過 3 種濾波器，根據不同的濾
波器參數，得到 17 張濾波之後的影像。將此 17 張濾波後的影像疊在一起，則原始影像中
每一個像素，會對應到一個 17 維的向量。此 17 維向量即為此像素的特徵。
濾波器類別 參數σ L A B
Gaussian 1,2,4 X X X
Laplacian of Gaussian(LoG) 1,2,4,8 X
First Derivatives of Gaussian 2,4 X
表格中 X 表示此濾波器作用於此種性質。舉例來說，LoG 濾波器僅作用於亮度 (L) 性質；
而 Gaussian 濾波器，則是亮度 (L) 和顏色 (A,B) 三種性質均有作用。Gaussian 的三種參數
和三種性質作用，共產生 9 個濾波影像。LoG 有四種參數，僅作用於亮度性質，會產生 4
個濾波影像。First Derivative of Gaussian 的二個參數，分別作用於 X 和 Y 方向，共會產生
4 個濾波影像。所以每一張影像，總共會得到 17 張濾波影像的結果。
5下表是實驗的結果。左邊是測試影像的實際類別，上面則是程式所推論的類別，每個數字
代表分到該類別的次數。
推論類別實際
類別 草地 樹木 建築物 飛機 牛 人臉 汽車 腳踏車 天空
草地 15
樹木 5 8 2
建築物 1 7 1 1 5
飛機 2 1 10 2
牛 1 13 1
人臉 14 1
汽車 13 2
腳踏車 1 14
天空 15
由實驗觀察，在天空和草地這兩類影像的實驗中，分類的正確率非常的高。但是對於建築
物，則非常容易和腳踏車混合，樹木辨識成草坪的比例也不小。由於建築物通常會伴隨天
空、草地一起出現，若是在影像中，天空所佔有影像的比例比較高，則建築物的關鍵字分
佈比例，可能會被影響天空所影響。若是草地所佔有影像的比例較高, 則被草地所支配。
因此我們將每一類的影像，單獨從影像中切割出來再測試，如此可以提升辨識的正確率。
上列四圖是利用學習出來的特徵，先讓使用者從影像中選一塊長方型的區域出來，透過統
計學習辨識出使用者所選取出來的區域中，所包含的物件類別。接下來我們嘗試著去改變
選取的特徵型態以及分群的方法，來觀察是否會改進以上的結果。
7根據一張影像中所出現的物件種類，來累計共同出現的次數。舉例來說，一張影像中，若
同時出現牛和草地，則我們就在牛和草地的欄位加 1。下表即是利用學習影像樣本所得到
的「同時出現」矩陣。
草地 樹木 建築物 飛機 牛 人臉 汽車 腳踏車 天空
草地 13 8 14 23 0 1 3 5
樹木 13 8 6 5 0 0 0 15
建築物 8 8 4 0 0 14 9 11
飛機 14 6 4 0 0 0 0 15
牛 23 5 0 0 0 0 0 1
人臉 0 0 0 0 0 0 0 0
汽車 1 0 14 0 0 0 0 0
腳踏車 3 0 9 0 0 0 0 0
天空 5 15 11 15 1 0 0 0
利用「同時出現」矩陣所提供的資訊，可以對原方法中所得的分類結果再次做修正。
六、參考文獻
[1] G. Csurka, C. Bray, C. Dance, and L. Fan. Visual Categorization with Bags of Keypoints. The
8th ECCV, Pargue, May 2004.
[2] R. Fergus, P. Perona, and A. Zisserman. Object Class Recognition by Unsupervised
Scale-Invariant Learning. CVPR 2003, Madison, WI, June 2003.
[3] R. Fergus, P. Perona, and A. Zisserman. A Visual Category Filter for Google Images. The 8th
ECCV, Pargue , May 2004.
[4] T. Leung and J. Malik. Recognizing Surfaces Using Three-Dimensional Textons. ICCV 1999,
Kerkyra, Greece, 1999.
[5] D. Lowe, Distinctive Image Features from Scale-Invariant Keypoints, IJCV, vol. 60, no. 2, pp.
91–110, November 2004
[6] A .Rabinovich, A.Vedaldi, C.Galleguillos, E. Wiewiora, S.Belongie. Objects in Context. ICCV
2007.
[7] T. Tuytelarrs, C. Schmid. Vector Quantizing Feature Space with a Regular Lattice. ICCV
2007.
[8] M. Varma and A. Zisserman. Texture Classification: Are Filter Banks Necessary? CVPR 2003,
Madison, WI, June 2003.
[9] M. Varma and A. Zisserman. A Statistical Approach to Texture Classification from Single
Images. IJCV, vol. 62 no.1-2, pp. 61–81, April 2005.
[10] J. Winn, A. Criminisi, and T. Minka. Object Categorization by Learned Visual Dictionary.
ICCV 2005.
出席國際學術會議心得報告
計畫編號 NSC 96 - 2218 - E - 007 - 010
計畫名稱 多類別物件辨識與切割
出國人員姓名
服務機關及職稱
陳煥宗 清華大學資訊工程系 助理教授
會議時間地點 美國德州聖安東尼奧
會議名稱 2007 IEEE International Conference on Image Processing (ICIP 2007)
發表論文題目 Finding Familiar Objects and Their Depth from a Single Image
一、參加會議經過
因為遇到颱風使得班機延誤，原本預計當地時間 9月 18日抵達，經過待機和多次轉機，
抵達時已是 9月 19日清晨。僅參加 19日當天的會議，並且於當日下午報告本次錄取的
論文。
二、與會心得
IEEE International Conference on Image Processing (ICIP)會議是影像處理研究領域最大的
國際會議，本年度會議的與會人數眾多，也有許多國內學者和研究人員參加。這次會議
的 Technical Program總共分為 36個 Lecture Sessions 和 48個 Poster Sessions，底下列出
幾篇與我們目前研究較為相關的論文：
1. An Efficient Method for Compressed Sensing (Kim et al.)
2. Image Denoising Based on Adapted Dictionary Computation (Azzabou et al.)
3. On Uncertainties, Random Features and Object Tracking (Badrinarayanan et al.)
4. DistanceCut: Interactive Segmentation and Matting of Images and Videos (Bai & Sapiro)
5. Multiscale Sparse Image Representation with Learned Dictionaries (Mairal et al.)
這次會議的 Plenary Lecture 是由加州理工學院的 Emmanuel J. Candes 為與會者介紹
Compressive Sensing 方法，這個方法在 Machine Learning 領域漸漸受到重視，並且尚需
要配合發展一些能夠有效解決 L1-Regularized Least Squares Problem的最佳化方法。
Compressive Sensing在影像處理方面也有許多可能的應用，相信將會成為一項重要的工
具，值得進一步去探討研究。
2. LEARNING OBJECTS OF MULTIPLE CLASSES
Suppose we have an image set containing C categories of ob-
ject templates. Each image is cropped to roughly enclose an
object of interest such that those in the same category are
depicted with a similar image size. For the ease of imple-
mentation, we use the last category to include all background
templates, even though they do not contain specific objects
and neither do they have similar scales. We run SIFT on
all templates to detect interest points {fi}Ni=1 and compute
the corresponding descriptors {xi}Ni=1, where N is the to-
tal number of interest points extracted from all the templates.
Note that we also keep a mapping t(i) to indicate that in-
terest point i comes from template t. As usual, a descriptor
xi is a 128-dimensional vector encoding the local gradient
orientations. And each interest point fi is represented by a
four-dimensional vector consisting of the location, scale, and
orientation. By labeling each feature descriptor with the cat-
egory of the template from which the feature is extracted, we
haveD = {(xi, yi)|i = 1, . . . , N} as training samples, where
xi is the feature vector and yi ∈ {1, . . . , C} is its label.
WithMKLR, we are to learn from the training samples the
conditional probability P (y|x). Thus, given a detected inter-
est point, we can use its feature descriptor to predict the label
by referencing P (y|x). After predicting the labels of all the
interest points in a test image, we combine the predictions to
identify possible locations of familiar objects and their scales
in the current image.
2.1. Multiple kernel logistic regression
Given the training data D, the conditional probabilities for
predicting the category label are parameterized by the C la-
tent real-valued functions {hc(x)}Cc=1 of MKLR based on the
multiple logistic likelihood:
P (y = c|x,θ) =
eh
c(x)
∑
c′ e
hc
′ (x)
, (1)
where
hc(x) =
∑
i
αciK
c(x,xi) + bc (2)
consists of the kernel expansions, and θ denotes the set of
parameters αci and bc. One way to learn the parameters is to
directly maximize the likelihood in (1). However, the scheme
usually leads to overfitting such that the learned conditional
probabilities may not generalize well. We consider a more
robust MKLR model by adding to (2) the following regular-
ization terms ∑
i,j
αciα
c
jK
c(xi,xj) (3)
to penalize kernel functions of high complexity. The task of
learning θ can now be formulated as a maximum a posteriori
(MAP) approximation, and solved by optimization techniques
[6]. Indeed MKLR can be a very efficient learning scheme.
In Seeger’s implementation [6], the incomplete Cholesky de-
composition is used for the low-rank approximation of the
kernel matrix such that the time complexity of optimization
becomes linear in the number of training samples.
3. DETECTING FAMILIAR OBJECTS AND DEPTHS
In general running SIFT on a hundred templates is sufficient
to give an ample amount of feature vectors for training, say
10, 000 features. We do not perform vector quantization or
clustering on the feature vectors to obtain visual words, e.g.,
[9], [11]. Instead, we use all the feature vectors to gener-
ate the training samples for MKLR. This would give rise to
a large training set D = {(xi, yi) | i = 1, . . . , N} with N 
10, 000. Nonetheless, since we apply low rank approxima-
tion to the kernel matrix, learning the MKLR parameters is
still fast and feasible—be reminded that the time and memory
requirements for training are linear to N .
3.1. Making hypotheses on familiar objects
Given a new test image, we shall generate hypotheses about
all possible locations and scales of the familiar objects that
may appear in the scene. This can be efficiently done by car-
rying out the following steps.
Step 1. Detect interest points from a given image, and
then use MKLR to predict each feature vector’s label. Since
the remaining procedure is universal, it suffices to simply look
at how an interest point is processed. Consider now some
interest point f¯ and its feature vector x¯. We compute the
conditional probability P (y¯|x¯) and predict the label as c if
P (y¯ = c|x¯) has the largest value.
Step 2. Use the kernel expansions to make hypotheses on
the location and the relative size of an object. Based on the
predicted label c, we find the index i∗ such that
i∗ = argmax
i
αciK
c(x¯,xi), (4)
and then pick the corresponding interest point fi∗ in the train-
ing samples. (If the label yi∗ of fi∗ is not c, which is very
unlikely, we drop this interest point and go on processing the
remaining interest points.) The rationale behind looking into
the kernel expansions for generating hypotheses is that for
MKLR every feature vector in D has certain influence over
the conditional probabilities of labels. We naturally choose
the one exerting the strongest influence to the making of a
label prediction.
Step 3. Recall that each interest point fi keeps the infor-
mation of its location, scale (σi), and orientation. We compute
the scale ratio σ¯/σi∗ between f¯ and fi∗ . Because we know
that fi∗ comes from the template t(i∗), this ratio provides the
relative size of the target to the template. According to the
ratio and the relative location of fi∗ in the template t(i∗), we
VI - 390
