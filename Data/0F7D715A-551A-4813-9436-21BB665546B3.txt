invoke these CCPs. Hence, timely recognition of CCPs is a critical task in SPC for determining 
the potential assignable causes. Some typical CCPs are shown in Fig. 1. A natural pattern is one 
in which only common-cause variations (background noise) are present in the control chart; 
consequently, the process is considered to be in-control. 
 
 
 
 
 
 Natural pattern           Upward shift pattern       Downward shift pattern     Increasing trend pattern 
 
 
 
 
Decreasing trend pattern        Cycle pattern            Systematic pattern          Mixture pattern 
                     
Fig. 1. Typical patterns in control charts.  
 
Typically, CCP recognition (CCPR) has been based mainly on the visual judgment of the 
quality engineer. Many supplementary rules, like zone tests or run rules were developed for 
quality engineers in the detection of unnatural patterns (Western Electric, 1958; Nelson, 1984, 
1985; Duncan, 1986; Grant and Leavenworth, 1996). One of the main problems with using run 
rules is that simultaneous application of all these rules is likely to increase the Type I error (the 
probability of wrongly detecting a pattern when no special cause exists) to any unacceptable 
extent. Moreover, the pattern characteristics that are being looked for may be common to more 
than one pattern, so there is no one-to-one mapping between a rule and an unnatural CCP. 
Therefore, run rules are not very useful for CCPR. With the movement towards computer 
integrated manufacturing (CIM), the automation of SPC implementation is considered inevitable. 
Using template matching, statistical testing, run rules, and heuristic algorithms, expert systems 
(ESs) have been applied in CCPR tasks (Lucy-Bouler, 1991; Cheng & Hubele, 1992; Swift & 
Mize, 1995). Although it was proved that the ES can outperform human experts in the CCPR task, 
the wrong pattern classification rate was reported as a common problem of the ESs for CCPR. 
Machine learning techniques have also been widely adopted as a promising tool for 
monitoring process automatically. In particular, artificial neural networks (ANNs) are most 
frequently used in previous literature since the power of classification and prediction is known to 
be better than others. In the machine learning approach, ANNs are powerful tools for pattern 
recognition and classification due to their non-linear non-parametric adaptive-learning properties. 
There are at least 94 papers published in the field of ANN-based CCPR since 1992 according to 
SCI-Expanded (Web of Knowledge) database. The literature in this field can be divided into two 
categories. The first category uses ANNs to recognize a specific unnatural CCP (e.g. shift or 
cycle) (Cheng, 1995; Ho & Chang, 1999; Hwarng, 2004). The second category utilizes ANNs as 
recognizers of multiple CCPs (Cheng, 1997; Yang & Yang, 2002; Guh, 2008). Compared to 
traditional statistics-based approaches, ANN-based approaches have two main advantages. First, 
it is relatively easy to adjust the control scheme by adding or modifying the relevant CCP 
2 
With finite number of addressed CCP types, the CCPR problem can be formulated into a 
classification problem. Therefore, this work employs the superior classification power of SVMs 
to on-line recognize seven unnatural CCP types in the independently and identically distributed 
(i.i.d.) and autocorrelated process data. The autocorrelated process is important because the 
prerequisite of independent or uncorrelated observations is not even approximately satisfied by 
certain manufacturing processes such as those involving continuous manufacturing operations 
(including the manufacture of food, chemicals, and paper and wood products) where the 
consecutive observations of product characteristics are often highly correlated (Vasilopoulos & 
Stamboulis, 1978; Montgomery, 2009). It has been proved that data autocorrelation can 
significantly deteriorate the performance of an on-line CCP recognizer trained with independent 
data (Guh, 2008). The performance of the proposed SVM-based model was evaluated using 
simulation. The performance of the proposed model in the independent process was also 
compared to those of an ANN-based model in the situations with different levels of common 
cause variation (background noise). 
 
2. A brief introduction to SVM algorithm 
 
The SVM model is a relatively new type of learning machine that is based on statistical 
learning theory. The basic procedure for applying SVMs to a classification model can be 
explained briefly with two steps. First, map the input vectors into a feature space, which is 
possible with feature spaces of large dimensions. The mapping is either linear or nonlinear, 
depending on the kernel function selected. Then, make an optimized division in the feature space, 
i.e., construct a hyperplane that separates two or more classes. By using the structural risk 
minimization rule, the training of SVMs always seeks a globally optimal solution and avoids 
overfitting. Therefore, it has the ability to deal with a large number of features. A complete 
description of the SVM theory can be found in Vapnik (1999). In the present research, the CCPR 
model was defined as a nonlinear problem and the radial basis function (RBF) kernel was used to 
optimize the hyperplane. The SVM algorithm is briefly described below to explain how it can be 
used for classification problems. 
The decision function (or hyperplane), determined by an SVM comprises a set of support 
vectors that are selected from the set of training samples X denoted by , 
and  is the corresponding target output. The data set can be linearly separated by a 
hyperplane into two classes: +1 and –1. The set of vectors is considered to be optimally separated 
by the hyperplane if it is separated without errors, and the distance between the closest vector and 
the hyperplane is the maximum. In such a case, the classifier is called the largest margin classifier. 
},...,2,1|),{( )()( Niii =df
{ 1,1)( −+∈id }
To find the optimal hyperplane }0),({ =+∈ bS xwx , the norm of vector w needs to be 
minimized. On the other hand, the margin w/1  should be maximized between two classes. 
 ( ) 1,min
,...1
=+= bNi xw                                                                                                                    (1) 
where i is the pattern index. 
 
In the nonlinear case, we first mapped the data to a Euclidean space H using the 
mapping . The kernel function is constructed such that . The 
RBF kernel function (Burges, 1998) used in the mapping process is shown in Eq. (2). 
HR ad:Φ )()(),( )'()()'()( iiiiK ffff Φ⋅Φ=
4 
are generally known since the autoregressive structure can be identified on-line based on a large 
number of preliminary samples. Many statistical software packages (e.g. MINITAB (Minitab, 
2006)) have routines for evaluating the independence of the process data and for identifying a 
suitable time-series model for the autocorrelated process. 
 
 
No
Yes
CCP example sets
Samples from the 
process
Include next observation into 
the recognition window
Data preprocessing
(Scaling)
Unnatural 
CCP?
Diagnosis of assignable causes
Autocorrelated process
On-line action 
against the 
out-of-control 
process
Simulate CCP examples in 
autocorrrelated process data
SVMs for CCPR
Process knowledge
On-line CCPR
Training and testing of the 
SVM models
Include next four CCP points 
into the recognition window
CCP type
Determine process mean, 
standard deviation, and
autoregressive coefficient.
Further classificatoin of the 
detected CCP
On-line monitoring Off-line learning
 
 
Fig. 2. Architecture of the SVM-based model for on-line CCPR in autocorrelated processes. 
 
6 
The preliminary study shows that ‘four points’ is the appropriate number for obtaining 
satisfactory results. Although waiting for more CCP points from the process delays the final 
decision of the CCP type, this delay is considered worthwhile because the correct CCP type is a 
critical clue to the subsequent search for the assignable cause(s). 
 
 Recognition window 
 
Fig. 3. Illustration of moving window recognition. 
 
3.2. Generation of CCP examples in the autocorrelated processes 
 
The autoregressive process of lag 1 (AR(1)) time series model is chosen in the present 
research as the representative model for the autocorrelated process. In an AR(1) process, the 
current observation is correlated with its previous observation. The role of AR(1) processes in 
process control has been emphasized by several studies in literature (Wardell et al., 1994; Runger 
et al., 1995; Montgomery 2009). An AR(1) time series can be utilized to model many data sets 
collected from manufacturing operations, such as the papermaking data set in Pandit & Wu (1983) 
and the viscosity data set in Box et al. (1994). An AR(1) model can be expressed as Eq. (5). Note 
that the commonly assumed independent case (i.i.d. process) occurs when φ  = 0. Four research 
assumptions were made here. (1) Only one CCP exists at any given time. (2) Only univariate 
quality characteristics are used. (3) The process data are generated from a stable AR(1) process 
with a known and constant φ . (4) The common cause standard deviation (σe) is kept constant. 
 
Rt = μ + φ (Rt－1 – μ) + et                                                                                                     (5) 
where t is the time of sampling, Rt is the sample value at time t, μ is the process mean, φ  is 
the autoregressive coefficient (–1 < φ  < 1), and et is the independent random error term (common 
cause variation) at time t following N(0, λσe). λ is the noise factor (0 ≤ λ ≤ 1). 
 
A Monte-Carlo simulation is applied in the present research to generate the required CCP 
example sets in the autocorrelated processes for two reasons. First, simulation can be used to vary 
elaborately the strength of the pattern feature. Second, simulated CCPs are easily reproduced by 
other researchers. The CCP example is generated by first generating an autocorrelated data 
stream with a specific φ  value. The pattern resulting from a process disturbance is then applied to 
the autocorrelated process data. The practical implication of this generation process is that the 
process disturbance affects the process observations in an all-round manner and is itself 
8 
unnatural CCPs were considered to enter the recognition window from the right (see Fig. 3). The 
shift position of small shift patterns (s ≤ 2.00σe or s ≥ –2.00σe) was set at the middle of the 
recognition window. Similarly, the starting point of large trends (d ≥ 0.20σe or d ≤ –0.20σe) was 
set at point 18 from the right edge of the recognition window. For small trends (d ≤ 0.18σe or d 
≥ –0.18σe), the starting point was set at the left edge of the recognition window (point 24). 
 
Table 1. CCP training example set for SVMs. 
Pattern type Parameter range* Description Quantity
Upward shift s = (1.0, 3.0, 0.25) Displacement of the process mean 450 
Downward shift s = (-3.0, -1.0, 0.25) Displacement of the process mean 450 
Increasing trend d = (0.10, 0.26, 0.02) Slope 450 
Decreasing trend d = (-0.26, -0.10, 0.02) Slope 450 
Cycle (period=8) a = (1.0, 3.0, 0.25) Amplitude 450 
Systematic g = (1.0, 3.0, 0.25) Magnitude of process fluctuation 450 
Mixture r = (1.0, 3.0, 0.25) Magnitude of process fluctuation 450 
Natural  Normal distribution, μ = 0, σe = 1  500 
* The three values in parentheses correspond to initial value, final value, and increment, 
respectively. 
 
In the present research, the RBF was used as the basic kernel function of SVM. There are 
two parameters associated with RBF kernels: C and γ. The penalty parameter C and the kernel 
parameter γ, which control the error-margin tradeoff, play a crucial role in the performance of 
SVMs (Tay & Cao, 2001; Hsu et al., 2004). However, there is little general guidance to 
determine the parameter values of SVM. A grid-search (Hsu et al., 2004) was employed here to 
identify optimal choice of C and γ so that the classifier can accurately recognize unknown data. 
All the pairs of (C, γ) with exponentially growing sequences (e.g. C = 2-5, 2-4, …, 215, γ= 2-15, 
2-14, …, 23) were tried and the one with the best accuracy was selected as the optimal parameters. 
There are three reasons for using the grid-search. First grid-search is an exhaustive parameter 
search. Second, the computational time to find the optimal parameter values by the grid-search is 
not much more than those by advanced methods (e.g. particle swarm optimization) since there are 
only two parameters. Third, the grid-search can be easily parallelized because each pair (C, γ) is 
independent. Because it may not be useful to achieve high training accuracy (i.e. classifiers 
accurately recognize training data whose type labels are indeed known), a testing (unseen) 
example set was used to test the accuracy of the SVM during the training process, so that 
recognition accuracy on this set can more precisely reflect the performance on classifying 
unknown CCPs. The LIBSVM program (Chang & Lin, 2009) executed in MATLAB 7.4 
development environment was applied to build the SVMs of the on-line CCPR model. 
Using the simulated training and testing process data with a medium autocorrelation level (φ  
= 0.5), the optimal (C, γ) was determined as (22, 21) through the gird-search procedure. The 
classification accuracy on the testing example set was 98.1%. This optimal SVM parameter set 
was then applied in the training procedure for other 10 SVMs using process data with other 
autocorrelation levels ( φ  = -0.9, -0.7, -0.5, -0.3, -0.1, 0.0, 0.1, 0.3, 0.7, 0.9). The average 
classification accuracy of these 11 SVMs was 97.8%, indicating that the training was successful. 
Note that these testing results represent the performance of the proposed SVM-based CCPR 
model only in an ‘off-line’ (static) recognition scenario in which the CCP recognizer analyzes 
each testing example pattern in its entirety. The off-line recognition scenario is not realistic in a 
10 
 pattern 
parameter
trigger 1
pattern type
Autocorrelated 
process data 
stream file
Matrix files of 
trained SVMs
simulated 
process 
data
data vector to 
be recognized
SVM matrix
recognized 
pattern type
Temporary 
result file
trigger 2
trigger 3
pattern type
pattern type
pattern 
parameter
run length
 Control unit
 Process 
simulator
 MATLAB 
operation 
engine
autocorrelation 
coefficient
autocorrelation 
coefficient
Aggregate result 
files
User
autocorrelation 
coefficient
pattern type
pattern 
parameter
Performance 
evaluation report
autocorrelation 
coefficient
 
Fig. 4. Data flow diagram of the performance evaluation operation. 
 
The ‘control unit’ element (coded with Visual Basic) in Fig. 4 is the interface between the 
user and the system, and is used to integrate the other two elements, ‘process simulator’ and 
‘MATLAB operation engine’, by coordinating and controlling the execution sequence and data 
exchange. The control unit conveys the process information (i.e. autocorrelation coefficient, 
pattern type, and pattern parameter) from the user to process simulator (coded with C++ 
language), and triggers the process simulation (trigger 1). The simulated process data stream is 
stored in a data file. Note that the element shown as a rectangle with one end open is a ‘data 
store’ that is a point in a system where information is permanently or temporarily stored or held. 
The MATLAB operation engine is then triggered (trigger 2) to recognize the data vector from the 
process data stream with the corresponding matrix file of trained SVM. Note that the vector size 
(recognition window length) is 24. After the recognition is completed, the recognition result 
(pattern type) is stored in a temporary file. The control unit is then triggered (trigger 3) to read the 
recognition result. If the underlying pattern type is ‘natural’, the MATLAB operation engine is 
triggered again (trigger 2) to recognize the data vector constituted with the next recognition 
window. This procedure will be repeated until an unnatural CCP is recognized. Then the time 
index will be furthered by four. After the latest process data vector is recognized by MATLAB 
operation engine recognize, the control unit reads the recognized pattern type and writes the 
recognition results (autocorrelation coefficient, pattern type, pattern parameter, and run length) 
into the aggregate result file. Then the control unit starts another evaluation cycle by triggering 
(trigger 1) process simulator to generate another autocorrelated process data stream. After the 
evaluations regarding all the required autocorrelation coefficients, pattern types, and pattern 
parameters are completed, the aggregate result file can be used to calculate the required RA, ARL, 
and SRL indices for user. Then the user can evaluate the performance of the proposed SVM-
based CCPR model using the final report. 
 
 
12 
λ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
 RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL 
Upward shift (US) 100 3.3 0.2 100 3.4 0.1 100 4.3 0.4 100 5.3 0.5 99 5.2 0.5 97 5.6 0.8 98 5.7 0.6 97 5.8 0.8 98 6.2 0.3 97 8.1 1.1 
Downward shift (DS) 100 3.4 0.2 100 3.6 0.1 100 4.4 0.5 100 5.1 0.6 99 5.6 0.6 95 5.6 0.7 96 5.9 0.7 99 5.9 0.8 99 5.9 0.3 96 8.4 1.2 
Increasing trend (IT) 100 11.4 0.7 100 11.6 0.5 100 11.0 0.4 100 11.6 0.5 99 11.2 0.9 99 11.6 1.1 99 11.5 1.0 99 11.8 1.1 97 11.2 1.8 98 11.3 1.3 
Decreasing trend (DT) 100 11.1 0.5 100 11.3 0.4 99 11.3 0.4 99 11.3 0.6 99 11.5 0.8 99 11.5 1.0 99 11.6 1.0 92 11.6 1.1 96 11.1 1.5 97 10.4 1.3 
Cycle (CY) 100 7.2 0.5 100 8.4 1.5 99 8.4 1.5 99 9.9 1.5 98 9.8 1.7 98 10.2 2.5 93 10.6 2.8 94 11.4 2.8 91 11.8 3.1 98 9.5 2.4 
Systematic (SY) 99 3.9 1.2 99 5.7 1.2 98 5.7 1.2 96 7.4 1.0 96 8.2 1.4 99 8.5 1.7 100 9.4 2.0 100 9.9 1.9 100 10.0 1.6 97 10.6 1.6 
Mixture (MI) 98 5.0 1.1 96 6.5 1.0 98 6.6 1.3 97 7.9 1.1 96 8.8 1.3 96 9.6 1.4 99 10.4 1.9 100 11.2 1.8 100 11.6 1.6 95 10.4 1.8 
Aggregate average 100 6.5 0.6 99 7.2 0.7 99 7.4 0.9 99 8.3 0.8 98 8.6 1.1 98 8.9 1.4 98 9.3 1.4 97 9.6 1.5 97 9.7 1.4 97 9.8 1.5 
φ  -0.9 -0.7 -0.5 -0.3 -0.1 0.0 0.1 0.3 0.5 0.7 0.9 
In-control ARL 382 375 377 372 369 366 360 357 354 345 347 
 RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL RA ARL SRL 
Upward shift (US) 91 7.0 1.1 90 6.6 0.9 93 7.2 1.0 96 7.0 1.1 96 7.3 1.1 97 8.1 1.1 97 7.9 1.3 95 8.5 1.4 93 8.9 2.2 83 9.2 3.4 82 9.8 3.6 
Downward shift (DS) 92 6.8 1.0 90 6.9 0.9 96 7.1 1.0 96 7.2 1.0 96 7.5 1.1 96 8.4 1.2 96 7.7 1.1 95 8.2 1.5 94 8.5 1.8 85 9.1 3.0 78 9.6 4.0 
Increasing trend (IT) 86 13.3 1.2 91 13.4 1.1 95 13.5 1.0 95 13.2 1.1 96 13.8 1.2 98 11.3 1.3 96 13.0 1.3 95 11.4 1.7 93 13.5 2.2 82 13.6 2.9 82 12.3 3.9 
Decreasing trend (DT) 87 13.0 1.3 90 13.5 1.1 93 13.8 0.9 97 13.5 1.1 96 13.7 1.2 97 10.4 1.3 95 13.2 1.2 95 13.5 1.7 92 13.4 2.2 85 12.5 3.1 79 12.4 4.2 
Cycle (CY) 87 12.4 2.1 90 12.8 2.3 95 13.2 2.4 94 12.5 2.3 95 12.1 2.5 98 9.5 2.4 93 13.0 2.8 91 12.2 2.7 87 12.4 3.2 85 11.3 3.4 80 10.7 1.7 
Systematic (SY) 88 17.6 7.3 95 14.1 4.9 95 12.4 2.9 95 12.0 2.1 97 10.3 1.9 97 10.6 1.6 96 11.1 1.5 92 10.5 1.6 91 10.1 1.7 84 10.3 2.1 79 10.8 2.3 
Mixture (MI) 86 17.6 6.5 92 15.2 4.6 91 12.1 2.3 92 12.2 1.8 93 10.7 1.5 95 10.4 1.8 93 10.7 1.9 90 10.0 1.8 88 10.3 2.0 82 10.2 2.6 77 10.7 2.8 
Aggregate average 88 12.5 2.9 91 11.8 2.3 94 11.3 1.6 95 11.1 1.5 96 10.8 1.5 97 9.8 1.5 95 10.9 1.6 93 10.6 1.8 91 11.0 2.2 84 10.9 2.9 80 10.9 3.2 
14 
 
Table 2. Performance of the proposed SVM-based CCPR model with various autoregressive coefficients. 
 
 
Table 3. Performance of the proposed SVM-based CCPR model with different background noise levels (φ  = 0.0). 
 
 
 
The performances of the proposed SVM-based CPP recognizer were compared with those of 
a learning vector quantization network (LVQN) based on-line CCP recognizer in the literature 
(Table 3 of Guh, 2008). The hidden layer (Kohonen layer) of the LVQN contained 32 Kohonen 
neurons, and the output layer contained eight neurons indicating the type of the input pattern data. 
Both the LVQ1 and LVQ2 training algorithms (Kohonen, 1995) were adopted in training the 
LVQN. Figs. 8-10 summarize the comparisons of RA, ARL, and SRL performances, respectively. 
The average in-control ARL of the LVQN-based model was 369. Fig. 8 shows that the SVM-
based recognizer performed better than the LVQN-based recognizer in terms of the RA, except 
for the φ  = 0.7 case. Fig. 9 shows that the SVM-based recognizer performed better than the 
LVQN-based recognizer in terms of the ARL with all the φ  values considered here, i.e. the 
SVM-based recognizer yielded better recognition speed (or sensitivity) than the LVQN-based 
recognizer. Fig. 10 shows that the SVM-based recognizer performed better than the LVQN-based 
recognizer in terms of the SRL with small to medium levels of autocorrelation. The recognition 
speed of the SVM-based model tended to be less stable when the data were highly positively or 
negatively autocorrelated. Based on above empirical comparisons, it can be concluded that the 
SVM-based CCP recognizer was superior to the LVQN-based CCP recognizer with regard to the 
RA and ARL performances. The LVQN-based model was also reported to perform better than 
traditional methods (e.g. residual chart and EWMA chart) in terms of ARL for detecting upward 
shift patterns when φ  = 0.475 (Guh, 2008). Another significant advantage of the proposed SVM-
based recognizer is that the model optimization is easier. There are fewer free parameters (only C 
and γ) in SVM. For the LVQN here, there are a large number of controlling parameters that 
must be determined empirically and simultaneously, including the number of Kohonen neurons, 
learning rates, learning epochs, weight initialization methods, conscience factor, and LVQ2 width 
parameter. 
 
50
55
60
65
70
75
80
85
90
95
100
-0.9 -0.7 -0.5 -0.3 -0.1 0.0 0.1 0.3 0.5 0.7 0.9
Autoregressive coefficient
R
A
(%
)
SVM LVQN
 
Fig. 8. Comparison of RA performances between the proposed 
SVM-based CCP recognizer and an LVQN-based CCP recognizer 
(Guh, 2008) with various autoregressive coefficients. 
 
 
 
16 
tasks. The BPN-based model consists of one BPN that has 24 input neurons and two hidden 
layers with 18 neurons each. The output layer consists of eight neurons, each one used for one of 
the eight types of CCPs considered in this research. Initial learning rate and momentum factor 
were set at 0.5 and 0.4, respectively. The initial connection weights were set between [-0.01, 
+0.01]. These parameters were assigned according to a previous study in this field (Guh, 2004) 
and trial-and-error experiments in the preliminary study of this research. A threshold value (θ) 
was applied to the outputs of unnatural pattern neurons in the output layer. Any value about θ 
was regarded as signaling the presence of an unnatural pattern. In the absence of an unnatural 
pattern, a Type I error occurred when any of the output values from the unnatural pattern neurons 
equaled or exceeded θ. The θ was set at 0.85 here to make the BPN-based model have an in-
control ARL of 357. The BPN-based model was trained using the same training example set as 
the SVM-based model. The connection weights were updated by the Delta-rule (Rumelhart et al., 
1986) and convergence condition was established then the classification rate was larger than 0.95. 
The BPN here was coded using NeuralWorks Professional II Plus. After training and testing 
procedures were completed, the networks were transformed into C++ language and embedded 
into the main model for performance evaluation. 
Figs. 11-13 demonstrate the performance comparison between the SVM and BPN based 
CCPR models in terms of RA, ARL, and SRL, respectively. Figs. 11 and 12 show that the SVM-
based model performed better than the BPN-based model in terms of RA and ARL performances 
with various background noise levels. Note that, in Fig. 12, the ARL value of the BPN-based 
model also contains the four data points included for further classification. When the noise level 
(λ) increased from 0.1 to 1.0, the BPN-based model’s RA and ARL performances decreased 
obviously around a medium noise level (λ = 0.5). The same tendency of performance 
deterioration could not be observed for the SVM-based model. The tendency of performance 
deterioration for the SVM-based model was relatively smooth. With the λ increased from 0.1 to 
1.0, the RA performances of the BPN and SVM based models decreased for 8.1% (from 98.1% to 
90.0%) and only 2.5% (from 99.6% to 97.1%), respectively. The ARL performances of the BPN 
and SVM based models decreased for 7.8 (from 7.4 to 15.2) and only 3.3 (from 6.5 to 9.8), 
respectively. Fig. 13 shows that the SVM-based model performed inferior than the BPN-based 
model in terms of SRL when λ = 0.1 to 0.5. However, with λ increased from 0.1 to 1.0, the 
quantity of SRL performance deterioration of the SVM-based model (1.5 – 0.6 = 0.9) was still 
smaller than that of the BPN-based model (1.8 – 0.4 = 1.4). Hence, it can be concluded that the 
SVM-based CCPR model is more robust toward the data background noise than the BPN-based 
CCPR model. The superior noise robustness of SVM-based model over the BPN-based model 
might be because SVM uses the SRM learning principle, which is superior to the traditional ERM 
principle employed by ANNs. 
 
 
18 
5. Conclusion 
 
Artificial intelligence (AI) techniques, particularly rule-based expert systems and machine 
learning techniques such as ANNs have been successfully applied to CCPR problems. However, 
ANN suffers from difficulties with generalization because of overfitting. In addition, it needs too 
much time and efforts to construct a best architecture. SVM is one of the methods that are 
receiving increasing attention with remarkable results. The major difference between ANN and 
SVM is the principle of risk minimization. While ANN implements ERM to minimize the error 
on the training data, SVM implements the principle of SRM by minimizing an upper bound on 
the generalization error. It is this difference which equips SVM with a greater ability to 
generalize, which is the goal in machine learning. This research presents a SVM-based CCPR 
model to on-line recognize seven typical types of unnatural CCP, assuming that the process 
observations are AR(1) autocorrelated over time. The autocorrelated process is important because 
the observations in both continuous and discrete industrial production processes are often serially 
autocorrelated. The numerical results from an extensive simulation study indicate that the 
proposed model can effectively on-line recognize unnatural CCPs in both the independent and 
autocorrelated processes. Moreover, an empirical comparison indicates that the proposed SVM-
based CCPR model exhibits better RA and ARL performances than an LVQN-based CCPR 
model in literature. Through manipulating the background noise of the simulated process data, 
another empirical comparison indicates that the SVM-based CCPR model is more robust toward 
the background noise of the process than a BPN-based CCPR model built in this research for the 
purpose of comparison. The robustness characteristic of the SVM to the background noise makes 
SVM very suitable for a practical process-monitoring scheme because the process observations 
are often compounded with different levels of noise (i.e. common cause variation). This confirms 
the great potential of SVM models in pattern recognition (classification). This research is useful 
for practitioners seeking efficient approaches for the on-line CCPR in autocorrelated processes 
where the investigation resulting from a false recognition is costly. 
It is well documented that a process should be considered out-of-control when points fall 
outside the chart control limits, or when control charts present unnatural patterns (Montgomery 
2009).  The on-line CCPR models described in this research are not intended to replace the 
control limits employed in control charts. Instead, the proposed model can be used to compliment 
a scheme based on control limits in order to form a comprehensive process-monitoring scheme. 
Many SPC problems are multivariate in nature because the quality of a particular process or 
product is determined by several interrelated variables. This research considered only univariate 
quality characteristics. Therefore, extending this work to multivariate SPC is one direction for 
future research. 
 
Appendix: Mathematical expressions for the process simulator 
 
The CCP herein is expressed in a general form that consists of the process mean, the 
common cause variation, the autoregressive coefficient, and a special disturbance from specific 
cause. Without a loss of generality,μ= 0.0 and σe = 1.0 in Eq. (5). 
Process simulator: 
Rt = φ ×Rt–1 + et  
Xt  = Rt + dt 
20 
Chang, C. C. & Lin, C. J. (2009). LIBSVM 2.89: A library for support vector machines. 
Technical report, Department of Computer Science and Information Engineering, National 
Taiwan University, available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 
Cheng, C. S. & Hubele, N. F. (1992). Design of a knowledge-based expert system for statistical 
process control. Computers & Industrial Engineering, 22(4), 501-517. 
Cheng, C. S. (1995). A multi-layer neural network model for detecting changes in the process 
mean. Computers & Industrial Engineering, 28(1), 51-61. 
Cheng, C. S. (1997). A neural network approach for the analysis of control chart patterns. 
International Journal of Production Research, 35(3), 667-697. 
Cheng, C. S. & Cheng, H. P. (2008). Identifying the source of variance shifts in the multivariate 
process using neural networks and support vector machines. Expert Systems with Applications, 
35(1-2), 198-206. 
Chen, W. H. & Shih, J. Y. (2006). A study of Taiwan’s issuer credit rating systems using support 
vector machines. Expert Systems with Applications, 30(3), 427-435. 
Chinnam, R. B. (2002). Support vector machines for recognizing shifts in correlated and other 
manufacturing process. International Journal of Production Research, 40(17), 4449-4466. 
Cristianini, N. & Shawe-Taylor, J. (2000). An introduction to support vector machines. 
Cambridge, England: Cambridge University Press. 
Duncan, A. J. (1986). Quality Control and Industrial Statistics (5th ed.). Illinois: Irwin Book 
Company. 
Dreiseitl, S., Ohno-Machado, L., Kittler, H., Vinterbo, S., Billhardt, H. and Binder M. (2001). A 
comparison of machine learning methods for the diagnosis of pigmented skin lesions. Journal of 
Biomedical Informatics, 34(1), 28-36. 
Edwards, C & Raskutti, B. (2004). The effect of attribute scaling on the performance of support 
vector machines. Lecture Notes in Artificial Intelligence, Webb, G. I. & Yu, X. (Eds.), 3339, 500-
512. 
Finej, S., Navratil, J. & Gopinath, R. A. (2001). A hybrid GMM/SVM approach to speaker 
identification. International Conference on Acoustics Speech and Signal Processing (ICASSP), pp. 
417-420, May 07-11, Salt Lake City, UT. 
Grant, E. L. & Leavenworth, R. S. (1996). Statistical Quality Control (7th ed.). New York: 
McGraw-Hill Book Company. 
Guh, R. S. (2002). Robustness of the neural network based control chart pattern recognition 
system to non-normality. International Journal of Quality and Reliability Management, 19(1), 97-
112. 
Guh, R. S. (2004). Optimizing feedforward neural networks for control chart pattern recognition 
through genetic algorithms. International Journal of Pattern Recognition and Artificial 
Intelligence, 18(2), 75-99. 
Guh, R. S. (2005). A hybrid learning-based model for on-line detection and analysis of control 
chart patterns. Computers & Industrial Engineering, 49(1), 35-62. 
22 
Rumelhart, D. E., Hinton, G. E. & Williams, R. J. (1986). Learning internal representations by 
error propagation. Parallel Distributed Processing, edited by Rumelhart, D. E. & McClelland, J. 
L. Cambridge, MA: MIT press, pp. 318-362. 
Runger, G. C., Willemain, T. R. & Prabhu, S. (1995). Average run lengths for CUSUM control 
charts applied to residuals. Communications in Statistics—Theory and Methods, 24(1), 273-282. 
Sarle, W. S. (1995). Stopped training and other remedies for overfitting. In Proceedings of the 
twenty-seventh symposium on the interface of computing science and statistics, pp. 352-360. 
Sclkopf, B., Burges, C. & Vapnik, V. (1995). Extracting support data for a given task. In Fayyad, 
U. M. & Uthurusamy, R. (Eds.). Proceedings of First International Conference on Knowledge 
Discovery & Data Mining, Menlo Park, CA: AAAI Press. 
Silver, G. A. & Silver , M. L. (1989). Systems Analysis and Design. Workingham, UK: Addison-
Wesley Publishing Company. 
Sun, J., Rahman M., Wong, Y. S. & Hong, G. S. (2004). Multiclassification of tool wear with 
support vector machine by manufacturing loss consideration. International Journal of machine 
tools & Manufacture, 44(11), 1179-1187. 
Swift, J. A. & Mize, J. H. (1995). Out-of-control pattern recognition and analysis for quality 
control charts using Lisp-based systems. Computers & Industrial Engineering, 28(1), 81-91. 
Tay, F. E. H. & Cao, L. (2001). Application of support vector machines in financial time series 
forecasting, Omega, 29, 309-317. 
Vapnik, V. N. (1999). The Nature of Statistical Learning Theory (2nd ed.). New York: Springer. 
Vasilopoulos, A. V. & Stamboulis, A. P. (1978). Modification of control chart limits in the 
presence of data correlation. Journal of Quality Technology, 10(2), 20-30. 
Wardell, D. G., Moskowitz, H. & Plante, R. D. (1994). Run-length distributions of special-cause 
control charts for correlated processes. Technometrics, 36(1), 3-17. 
Western Electric Company. (1958). Statistical Quality Control Handbook. Indianapolis, USA: 
Western Electric. 
Weston, J.& Watkins, C. (1998). Multi-class support vector machines. Technical report, 
Department of Computer Science, Royal Holloway University of London, England, UK. 
Yang, M. S. & Yang, J. H. (2002). A fuzzy-soft learning vector quantization for control chart 
pattern recognition. International Journal of Production Research, 40(12), 2721-2731. 
 
摘要 
在統計製程管制(statistical process control / SPC)的領域中，及時有效地辨識管制圖上出現
的特殊(異常)形狀(control chart patterns / CCPs)是一個重要的課題，因為異常的 CCP 可被
用來輔助推測正在影響製程的可歸屬原因(assignable cause)。機器學習技術，特別是類神
經網路(artificial neural networks / ANNs)已在文獻中被廣泛地應用於辨識 CCP 的研究中，
但是 ANN 時常由於過度學習而導致訓練出來的模式有一般化(generalization)的困難。 支
援向量機 (support vector machine / SVM)在學習採用系統風險最小化 (structure risk 
minimization)的策略，所以常能避免過度學習的陷阱，其學習效果在文獻中已被證明優於
24 
