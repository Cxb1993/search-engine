simulator. The goal is to allow such simulations for 
multi-core systems to be done very quickly and 
efficiently. The main theme of the first year is to 
develop fundamental building blocks of the parallel 
simulation environment and its technology. We will 
first focus on an ILP processor architecture and the 
timing model in simulations. We will develop sample-
based simulation technology for simulation speedup 
and application-oriented study. Then, we will further 
consider homogeneous multi-core architectures for 
parallel simulations and develop its simulation 
environment. Tracing and profiling tools will be 
developed for performance evaluation. We shall also 
develop inter-core communication libraries and global 
scheduling technology over multi-core platforms. A 
program phase analysis method for parallel programs 
and mapping the phase classification back to a 
parallel execution trace will also be explored for 
sample-based simulations. The main theme of the last 
year is to extend the developed technology in the 
first year and focuses on system integration. We 
shall explore the speedup of simulation by utilizing 
multiple cores, where performance and power 
consumption will be jointly considered. Statistical 
sampling and sampling methods methodology will also 
be developed for sample-based simulation. The 
objective is to develop a parallel virtual platform 
that could be used to develop system and application 
software before hardware (homogeneous and/or 
heterogeneous multi-cores) is available. 
We believe that this project could result in 
innovation ideas for multi-core systems. Useful 
technology and a development environment will be 
developed. This project will be based on our strong 
work in the past years. We expect that the results of 
this project would provide significant advance to 
development tools and design methodology of embedded 
systems. Students who are involved in this project 
will also develop excellent experiences in embedded 
system research. 
英文關鍵詞： SimulatSimulat Simulat ion, Virtualization, 
 行政院國家科學委員會補助專題研究計畫
■成果報告   
□期中進度報告 
 
在多核平台上之多核虛擬環境研發－總計畫 
 
計畫類別：□個別型計畫   ■整合型計畫 
計畫編號：NSC  99－2220－E－002－026－ 
執行期間：99年 8月 1日至 100年 7月 31 日 
 
執行機構及系所：國立中央研究院資訊科學研究所 
 
計畫主持人：游本中 所長 
共同主持人：陳添福、郭大維 
計畫參與人員：楊登峰、周嘉真 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
□出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            ■涉及專利或其他智慧財產權，□一年■二年後可公開查詢 
 
 
中   華   民   國  100 年 10 月 31 日 
 
總計畫：在多核平台上之多核虛擬環境研發 
 3 
中英文摘要: 
中文摘要 
隨著硬軟體技術的快速發展，多核心平台已成為嵌入式系統的泛用環境，但是相關的設
計方法與最佳化技術卻相對缺乏，導致開發成本不易控制，並平添許多開發上的風險，適當
的開發工具與設計方法研發已成為重要的議題，也常常影響系統開發的成敗。 
本研究計劃主要目標是研發一個平行模擬環境來模擬一個多核環境的效能與耗能，該環
境兼具取樣為基礎並採用細部平行模擬之指令集模擬器  (Parallel, Detailed, 
Cycle-Accurate ISS Simulator) 以及平行功能模擬器(Functional Simulator)，目標是希
望可以將多核環境模擬做到快速及有效率。計劃第一年的主軸是發展基礎之平行化模擬環境
與技術，我們將首先針對指令層平行 (ILP) 處理器架構與時間模型 (Timing Model) 進行研
究與實作，我們也會開始發展取樣為基礎的模擬技術以做為模擬加速與應用考量進行先期研
究。之後將進一步考量如何設計與實作多核心同質處理器架構 (Homogeneous Multi-Core 
Architectures) 之平行化模擬，配合開發的研發，並將研發追蹤 (Tracing) 與特徵取得 
(Profiling) 工具，於多核心平台上研發多核作業核心之傳輸方法設計及跨核心資源排程，
並發展程式執行分析與執行記錄對應等取樣為基礎的模擬。計畫最後將著力於系統整合，我
們將整合細部平行模擬之指令集模擬器以及平行功能模擬器，並應用多核心來加速虛擬平台
效能，同時考量能源損耗與系統效能的模擬，我們也將發展統計取樣方法，以搭配動態取樣
為基礎之模擬，目標是建立快速並準確之多核心平台之模擬，協助硬軟體整合設計。 
我們相信本計劃的研發成果可以提供多核心系統的設計創意，並研製其所需之技術與設
計環境。本計劃之進行將構築於計畫相關團隊之研發經驗與成果，我們相信本計畫成果將可
以對多核心系統之開發工具與設計方法研究產生重大影響，所有參與學生也將會於相關的嵌
入式系統發展上有很好的研究經驗。 
關鍵字- Simulation, Virtualization, Multicore, Parallelization 
 
總計畫：在多核平台上之多核虛擬環境研發 
 5 
一、 前言  
 
   隨著多核心科技的演進，幾乎所有的微處理器製造商都將多核心處理器放在其發展目標之中。此
一發展的主要原因之一是效能和耗能的綜合考量，製造商已經無法像過去數十年在不斷提高單核心處
理器的執行效能的同時不去考慮耗能以及溫度所帶來的影響。因此對業界而言，設計多核心處理器已
經成為無法抵禦的趨勢。隨著半導體科技的不斷演進，根據摩爾定律[1]的推估，單一晶片上所能搭載
的核心數目也將不斷的成長。我們相信在未來的十年內將可以看到百核心的晶片被成功開發，如Intel
所開發之兆級晶片(八十核心)[2]、Tilera所開發之TILE64TM[3]以及由AMD所開發之Crossfire[4]技術都
再再指出多核心(超過16核心)的時代即將來臨。 
多核心及多執行緒執行已經被證明是利用程式內部的平行度來增加系統效能的有效方法。然而不幸的
是，多核心的架構使軟體開發以及架構設計變得更加複雜。在開發多核心處理器的過程中，相較於設
計單核心處理器系統設計者必須考慮更多的因素並面對更大的複雜度。經常而言，只有透過詳細的週
期精準架構模擬才能驗證設計的正確性、取得詳細的效能和耗能的資訊、或研究編譯器最佳化的方法
對於程式效能的影響。而在將多核心處理器系統晶片(System-on-Chip)的設計中，由於牽涉到硬體共同
設計及驗證的複雜因素，因此週期精準的架構模擬對於開發過程的重要性更甚於開發多核心處理器。
但假設電晶體的數目成長依然如摩爾定律所預測的一樣，隨著核心數不斷的增加，模擬所需的時間也
將以指數方式成長。尤其詳細精準時序架構模擬以其極為冗長的模擬時間聞名，與實際執行時間往往
有著上萬倍的差距。舉例而言，實際執行一分鐘的程式可能需要一個禮拜的模擬時間。而隨著核心數
的增加，如果沒有新的模擬技術，傳統的模擬所需要的時間也會大幅的增加。因此，一個高速多功能
的精準多核心模擬虛擬平台將是多核心相關的研究及產品能否成功發展的重要基石。於此同時國家晶
片系統設計中心亦持續的推廣開發學界及業界能使用的共同平台(Common Platform)。 
在此一計畫中，我們希望提出一高速多功能之多核心虛擬平台藉由交錯運用功能模擬以及時序精準模
擬、位元轉譯(binary translation)[5]、及取樣模擬技術(simulation sampling)[6][7]來加速多核心架構的模
擬。我們亦會開發作業系統的微核心(microkernel)來優化虛擬平台在多核心母平台上之排程控制，並可
用以測試所開發之虛擬平台之彈性以及效能。我們相信透過這些方法將可成功實作出在多核平台上之
多核虛擬環境以加速多核心相關研究及產品之開發。 
 
二、 研究目的  
 
隨著硬軟體技術的快速發展，多核心平台已成為嵌入式系統的泛用環境，但是相關的設計方法與
最佳化技術卻相對缺乏，導致開發成本不易控制，並平添許多開發上的風險，適當的開發工具與設計
方法研發已成為重要的議題，也常常影響系統開發的成敗。 
本研究計劃主要目標是研發一個平行模擬環境來模擬一個多核環境的效能與耗能，該環境兼具取
總計畫：在多核平台上之多核虛擬環境研發 
 7 
simulator is auto parallelized by LSE. However, this approach trades simulation speed for simulator 
modularity, and thus it is unsuitable for fast exploration in software or idea development. 
 
 Some researches claim that the sequential characteristic of software approaches makes it even more 
difficult to provide high speed parallel simulation. Therefore, RAMP Gold[A-13], FAST[A-8], and 
HASim[A-14] propose that using FPGA could help solving this problem with its hardware concurrency 
nature. FAST divides the simulation process into two different models, a functional model running on 
conventional servers which emulate the function of simulation instructions and a timing model running 
on FPGA to simulate micro-architectural components related to timing and resource usage. The software 
model feeds instruction traces to the timing model FPGA, and FPGA adds timing information and detect 
violations to rollback the functional simulation. RAMP Gold and HASim use the similar approach as 
FAST, but running both models on FPGA. Although these methods show noticeable simulation time 
reduction, it also requires expensive FPGA board as simulation platform comparing to other software 
approaches. Nevertheless, simulating with FPGA requires detail RTL coding which complicates the 
target modeling and increase the time spending on implementation. 
 
 One of the main problems of parallelizing conventional software-based simulator is that the frequent 
needs of synchronization such as memory hierarchy accesses. Jianwei et al.[A-11] proposed a 
slack-based parallel simulation methodology which simulates each core of target chip multiprocessor 
cycle-by cycle in each separate thread and allows these threads become asynchronous in a predefined 
slack cycle. In other words, the simulation cores do not synchronize with each other in each cycle, but 
only synchronize when the cycle difference of these threads have reached a predefined slack cycle. In 
advance, the author enhances the slack simulation with adaptive slack scheme which feedbacks the 
simulation violation information to tune slack size adaptively. Once the simulation violation is detected, 
the speculation mechanism rollback simulation. Graphite[A-7] is a Pin-based simulator with similar 
approach which aims to estimate the performance and other hardware statistics accurately without 
simulating with cycle-level details. Graphite provides three different slack management mechanism with 
different accuracy level including a new synchronization model, LaxP2P, that each tile periodically 
choose a random tile to synchronize with it. Authors emphasizes on parallelizing the simulator onto 
distributed host platform. However, it simulates data communication behavior with mathematical 
queuing model rather than detail cycle-accurate simulation, which may makes it unable to provide the 
accurate performance statistic when memory access latency becomes even more critical to performance. 
PCAsim[A-18] is another slack-based parallel cycle-accurate multicore simulator based on the 
總計畫：在多核平台上之多核虛擬環境研發 
 9 
storage pantries that inventory grocery supplies and notify designated suppliers for just-in-time 
replenishment; medicine dispensers that help to ensure correctness and enforce compliance of medication 
schedules; monitors that record and process vital sign signals, detect irregularities, and send appropriate 
notifications; and robotic helpers that enhance dexterity and accessibility and minimize the effects of 
functional limitations. 
 The thrusts of our work are on technologies for the design, production, and quality assurance of 
easy-to-use, dependable SISARL appliances and services with state-of-art and future capabilities. These 
appliances and services are not only needed to improve the well-being of an increasingly larger segment 
of the global population, but they also present to the ICT (Information and Communication Technologies) 
industry a tremendous new business opportunity. We want to help the industry to shorten the time and 
lower the cost required to bring families of high-quality SISARL products and services to market. 
Problems to be solved include how to partition diverse SISARL appliances and services into common 
components; how to configure and integrate the components in a systematic, verifiable way to build 
diverse appliances and services; how to design and implement the appliances for compositional and 
incremental verification, validation and certification; how to make the appliances easily customizable to 
users’ needs, preferences, and available support infrastructures; how to ease the incorporation of future 
extensions and advancements into existing SISARL; and how to effectively exploit application/platform 
co-design, software/hardware co-design and SoC (system-on-a-chip) technologies. 
 Our research aims to fill voids in the science and technology needed to strengthen the foundation of 
component-based design, integration and quality assurance for SISARL. We are also developing a 
general architectural framework, supported by a repository of user scenarios, SISARL application 
components, integration platforms and middleware, as well as verification and validation methods and 
tools, with which one can evaluate tradeoffs and carry out system integration, quality assurance and 
certification. The framework will provide an environment for experimentation with and evaluation of 
SISARL products and the methodologies and tools used to produce them. Parts of our research results 
have been published on numerous prestigious international conference and journals. In 2006, we 
presented the study on medication scheduling algorithm on IEEE Real-Time Systems Symposiums 
[B-17], the design and implementation for medication authoring tool and prescription algebra for 
medication use on IEEE International Conference on Systems, Mans, and Cybernetics 2006 [B-18, B-19]. 
In 2007, we present the design and implementation for the integration framework for medication-use 
process on IEEE International Conference on Systems, Mans, and Cybernetics 2007 [B-20] and 
component interface design for embedded software on IEEE International Conference on Systems, Mans, 
總計畫：在多核平台上之多核虛擬環境研發 
 11 
for existing services. Only a plug-in module for the web browsing devices is needed to provide the client 
and server mobility services. The approach has been demonstrated using one of the popular web clients, 
Konqueror, and one of the popular web servers, Apache. Parts of the research results have been published 
in the Proceedings of the Sixth International Conference on Information Integration and Web Based 
Applications & Services (iiWAS2004), 2004 [B-11]. 
(4) ESL Design Tool 
 In 2004, we start a project named MFASE for ESL design tool. The acronym MFASE stands for 
multi-function SoC Analysis Environment. MFASE is designed as an open integrated design framework. 
 It consists of system-level design tool to avoid enormous design space exploration, cycle accurate 
low-level HW/SW co-simulation to verify the design, performance analysis tool to identify the 
performance bottleneck for design revision. In addition, the MFASE is designed with an open framework 
in the sense that open document standard is used for intra-components date exchange. The users or other 
design tools can access all the intermediate design results. 
 The four major features of MFASE are the following. 
 1. Single, cohesive environment. In MFASE, the designers start with function blocks for the system, 
which are described by data flow diagram of the system, and conclude with RTL level simulation. All the 
operations are integrated within one integrated development environment.  
 2. HW/SW partition, resource manager synthesis and architecture synthesis. MFASE conducts the 
HW/SW partition with different classes of algorithms including constructive algorithms, iterative 
algorithms, and generic algorithms. In order to meet the timing constraints for embedded real-time 
applications such as multi-media applications, MFASE generates the resource management policy for 
each system in order to meet the design requirements. Playback jitter and average deadline missing rate 
are two examples of design requirements. To synthesize the virtual hardware platform for co-simulation, 
MFASE generates the interface between hardware and software components including the device driver 
interface for hardware components and function definition for software components.  
 3. Reconfigurable framework for third party co-simulation/co-verification tools integration. MFASE 
is a platform independent design tool in the sense that the developers can choose different target platform 
for the systems. In order to verify the design results, MFASE uses an open interface to communication 
with different co-simulation tools. All the design results are stored in XML format for smooth integration. 
總計畫：在多核平台上之多核虛擬環境研發 
 13 
 
2.Multicore Simulation 
 
Since the multicore platforms become prevalent in these years, more and more multicore 
simulators have been developed. A sequential multicore simulator, such as QEMU [C-6], Simics 
[C-4], SystemSim [C-8], MPTLsim [C-9], MARSSx86 [C-10], simulates multiple virtual processors 
in a round-robin fashion with only one simulation thread, as shown in Figure C1. Such sequential 
simulators cannot take advantage of the computing power of multicore host machines. As the 
number of simulated processors increases, the sequential simulation leads to poor scalability or even 
suffers the performance degradation due to the scheduling overhead. The sequential multicore 
simulators include , etc.  
 
Figure C1. The sequential multicore simulator 
 
Parallel multicore simulators have been developed to accelerate the simualtion of a multicore 
system on a multicore host machine. Over et. al. [C-11] showed how they parallelized Sulima, their 
sequential simulator for UltraSPARC-based multiprocessor systems. To perform parallel simulation, 
they used multiple host threads, and each is assigned one or more simulated processors. Within each 
thread, processors are simulated in a round-robin fashion using an inner timeslice. The host threads 
synchronize on a barrier upon the completion of each outer timeslice. This ensures that processors 
remain approximately synchronized.  
Parallel Embra [C-12] extended the Embra [C-13] functional simulator, to support 
threadparallel execution and allow dividing up target hardware components at the granularity of a 
host multiprocessor system. It only supports outdated host and target systems, which are Standford 
總計畫：在多核平台上之多核虛擬環境研發 
 15 
程式(multi-threaded applications)的特徵驅動模擬取樣機制。 
A. 針對單執行緒程式的特徵驅動模擬取樣機制 
針對單執行緒程式，在[D-10]中，Sherwood 等人提出了一個基於程式碼簽章的取樣機制，
並將此機制包裝成軟體，SimPoint，這個機制模擬取樣的過程可以被整理為以下幾個步驟： 
1. 把程式的執行過程切成許多區間 
一個區間是程式執行時一段連續的時間片段，在這步驟裡，程式的執行過程會基於相同
的指令個數切割而成許多相同大小且沒有互相重疊的區間。 
2. 記錄程式行為 
在這個步驟中，每個區間會把該區間裡程式的行為以頻率向量(frequency vector)的方式記
錄下來。在他們的機制中，每個區間都有一個代表它們的基本區塊向量(Basic Block Vector)。
一個基本區塊是程式中一段一進一出的程式片段，而基本區塊向量是一個一維向量，其中向
量裡的每個值都代表在程式中的基本區塊在該區間被執行的次數。他們利用基本區塊向量來
捕捉每個區間裡程式碼被執行的情形。 
3. 進行相位分類 
一個相位是由相同行為區間所組合而成的集合，SimPoint 使用 k-means 分類演算法依各
區間的基本區塊向量來將各區間分類成許多相位。屬於相同相位的一組區間，其基本區塊向
量維度為 n，則該組區間之基本區塊向量可視為 n 維空間中的一組點。 
4. 選擇要進行週期精確模擬之特徵點 
SimPoint 會在各相位中，選擇該組區間之基本區塊向量在 n 維空間中最接近中心點，最
具有代表性的點來代表這個相位，稱為特徵點。每個特徵點有相對應的權重，所有特徵點的
模擬結果搭配它們的權重便可估計出整個程式執行結果。 
B.針對多執行緒程式的特徵驅動模擬取樣機制 
針對多執行緒程式，Perelman 等人[D-11]提出了一個能在多執行緒程式中偵測程式相位的
方法，其步驟與針對單執行緒程式的步驟相似，如下： 
1. 把程式的執行過程切成許多區間 
為了要分析多執行緒程式中的程式行為，知道各執行緒中的哪些程式部份一起執行是很
重要的。然而，在多執行緒程式中由於有許多執行緒的產生及結束，想要對各個執行緒分別
做完相位偵測再看哪些不同執行緒的相位同時執行很不容易。因此，Perelman 等人提出一個
基於全域指令數的分割區間方法，他們基於各執行緒執行過的總指令數，將一個多執行緒程
式的執行過程切成許多相同大小的區間。 
總計畫：在多核平台上之多核虛擬環境研發 
 17 
四、 研究方法 
 
Picture 1 shows the system architecture of our multicore-on-multicore parallel simulation environment. 
We utilize the sampling simulation control provided from subproject 4 to identify critical samples from 
the target applications. After that, the output samples would be processed by the virtual platform control 
provided from subproject 3. Then, it will determine when to switch between functional simulator and 
parallel cycle-accurate simulator provided by subproject 1 based on the samples. The microkernel and test 
benchmark are provided by subproject 2. Picture 2 presents the working example we describe above. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Pic 1. System Architecture of our multicore-on multicore parallel simulation 
environment 
Pic 2. Concept view of our multicore-on multicore parallel simulation environment 
總計畫：在多核平台上之多核虛擬環境研發 
 19 
moment nowadays. We believe that the key to successfully develop a high performance parallel 
simulator is to first understand the opportunities and the difficulties of parallelizing it.  
 
 
A-1. The Design of Conventional Cycle-Accurate Simulator 
As mentioned, conventional cycle-accurate simulator models the instruction pipeline in 
details. Although the abstraction level of the pipeline modeling diverse, these simulators 
usually implement the data and control flow of target pipeline based on the real hardware 
design. For example, we depict the simplified data and control flow of major stages in the 
PTLsim’s instruction pipeline model in Figure A1.  
As the picture shows, the dataflow and control flow in the pipeline are mainly 
unidirectional which start from the fetch stage and ends in the commit stage. One of the 
exceptions is the instruction redispatch which put the instructions back to the dispatch stage 
when the hazards take place. Other exceptions such as the data forwarding mechanism and the 
branch mis-prediction handling also require backward flow. In details, the data forwarding 
mechanism try to forward data which is not yet writebacked for later instructions in dispatch or 
issues stage to proceed with execution. The branch prediction monitors the correctness of 
prediction in commit stage, but handling mis-prediction by flushing the entire pipeline and 
fetching instructions from the correct address. In the hardware instruction pipeline design, it 
uses latches as the temporary data storage between adjacent stages to pass data. The edge 
triggering characteristic decouples the data read and store of adjacent stages in the same cycle, 
 
Fig. A2.  Conceptual View of Intel Nahelem i7 architecture. Each core has its own L1 and L2 cache, and 
all four cores are connected to a unified large L3 cache and lower level off-chip memory hierarchy. 
  
總計畫：在多核平台上之多核虛擬環境研發 
 21 
stage execution. Therefore, in order to exploit the similar parallelism inside a parallel simulator, 
a software-implemented latch-like data structure between modeled stages is strongly required. 
The reason why conventional cycle-accurate simulator does not model the latch is that 
modeling actual data passing in a sequential simulator could neither improve the accuracy nor 
the performance due to its sequential execution characteristic. Once the pipeline has been 
successfully parallelized, the similar approach could be easily adopted to an existing multicore 
simulator to improve the scalability up to a higher level. 
 
A-3. Existing Parallel Cycle-Accurate Multicore Simulator Approach 
Conventional cycle-accurate multicore simulators usually exploit the parallelism between 
cores, that is, each core has mapped onto each different thread and been simulated 
simultaneously. We depict this concept in Figure A3. Due to the fact that cores in a multicore 
usually runs independently to each other except for the access to shared resources such as 
shared cache and off-chip memory, many works believe that this parallelizing approach could 
minimize the synchronization overhead and provide highest simulation speed. However, the 
shared resource accesses in these parallel multicore simulators lead to bidirectional 
synchronization between thread which means that each access directly results in a 
synchronization barrier among all threads. This synchronization overhead could slow down the 
simulation process dramatically. Therefore, in order to reduce the overhead, conventional 
method trades accuracy for simulation speed by allowing inaccuracy in a controlled cycle 
window or using mathematical queuing model to approach the behavior of memory hierarchy. 
Nevertheless, though these methods do improve simulation performance in return, they are not 
suitable for cache/memory system and power related research which required accurate timing 
                                                  
Fig. A4.  Pipeline Partitioned Parallelization method. Partition the pipeline stages according to their workload and 
communication pattern. The synchronization direction which posed by the machine state dependency become unidirectional   
  
總計畫：在多核平台上之多核虛擬環境研發 
 23 
addressed these issues with three different mechanisms and guidelines which we will discuss below 
 
B-1.  Group the same pipeline stages onto the same working thread 
The most important insight of our pipeline-partitioned simulator is to group the same 
pipeline stages onto the same working thread to extract the inherent parallelism. By doing so, 
the instructions flew through the pipeline stages as the thread simulated them. Figure A4 shows 
the concept of a pipeline partitioned parallelization method. 
MARSS models an instruction pipeline with nine pipeline stages which are fetch, rename, 
dispatch, issue, complete, transfer, writeback, and commit. Since the workload of each pipeline 
stage diverse dramatically, directly maps each pipeline stage onto each different threads will 
leads to severe performance degradation caused by workload imbalance between threads. To 
balance the workload between threads, we retrieve the workload breakdown of these nine 
pipeline stages via simulations. Communication behaviors between threads are another issue 
we need to take into consideration to decide the stage partitioning. Misplace frequent data 
sharing stages onto different threads directly results in lots of synchronization barriers. 
Therefore, we have done a thorough analyze to the data sharing between stages. The balanced 
partition result based on the observed workload is shown in Picture A5. 
The advantage of our pipeline partitioned method is two-fold. First of all, as the previous 
works mainly focus on extract the parallelism between cores via core partitioned parallelization, 
our methodology aims to partition the instruction pipeline itself which provide the more 
parallelism. By combining both core and pipeline partitioned method, the scalability of 
multicore simulator could be further improved.  
Second, conventional core partition parallel simulator suffers from the complex 
bidirectional synchronization between sharing models and structures, for all the simulated core 
might access these sharing data at arbitrary time. Since the time of shared data access is hard to 
be predicted correctly, it requires a barrier to synchronize all cores in shared structure accesses 
or memory transactions. However, shared component accesses usually take place in few stages 
across the entire pipeline. Therefore, by clustering the same pipeline stage onto the same core, 
our pipeline partitioned simulator localizes memory hierarchy accesses in limited number of 
individual threads. This method significantly eliminates the need of bidirectional 
synchronization, and thus reducing its overhead.  
B-2.  Latch-like Data-passing Methodology 
Latches are the key component to enable the hardware pipeline stage can parallel 
總計畫：在多核平台上之多核虛擬環境研發 
 25 
and even link list-based hardware components such as event queues of memory hierarchy and 
the pool of free physical register. To simplify the source code structure and reuse the same 
class variables and functions, these component models might be forced to maintain some 
redundant states. Although these implementation compromises do no harm to the simulation 
accuracy and performance in a sequential approach, they could pose many unnecessary data 
dependencies and complicate the thread synchronization mechanism. Hence, we customize 
some new classes for these similar hardware models to reduce those dependencies in our 
simulator. 
Also, MARSS tracks the architectural states of reorder buffer entry and physical register 
with a class variable to record its current state. In addition, it implements bidirectional link lists 
to link the physical registers and reorder buffer entry of the same state together to realize the 
free pool of both resources. However, it is hard to support parallel accesses to a bidirectional 
link list which allow linking and unlinking objects from arbitrary points. Therefore, we have 
rewritten the data structures and access functions used to maintain the state transition 
information and deleted some unused state tracking structures.  
II. Subproject 2 
During the year from 2009 to 2011, we accomplished four major components for Moviala. They 
includes the methods of the software for control-plane processors, zero-buffer inter-core process 
communication protocols, workload migration from mobile devices and cloud servers, and the fairness 
scheduler for multiple virtual cores. The first two components target the physical multicore platform so as 
to enhance the performance on such platforms; the last two components target the platforms with virtual 
cores.  
                              
Fig. A6.  Double-buffering based Latch-like Data structure 
總計畫：在多核平台上之多核虛擬環境研發 
 27 
executed over a virtual core, where independent tasks could preempt the executions of each another at any 
time. Let Ci and Pi denote the maximum number of execution cycles and the period of a task Ti of the set, 
respectively. Suppose that the EDF scheduling algorithm is adopted to schedule the task set.  
Let Ck be the execution cycles that a virtual core can guarantee within any pk, while we set the 
replenish period and maximum budget of the virtual core according to our configuration, i.e., T = gcd(p1, 
p2, . . . , pn) and . Then, we have 
. 
That is, from the perspective of each task Tk, the virtual core can provide sufficient execution cycles 
within its relative deadline Pk such that the virtual core will behave like a processor whose operating 
frequency is  , which is no less than Fv. Since it is similar to that these tasks are executed over a 
processor whose operating frequency is no less than Fv, the total utilization of the task set will be no more 
than 1. Therefore, the timing constraints of these tasks can be satisfied. 
 
Figure B1. The delay of the response time incurred while the virtual core is emulated on a physical core. 
Unlike real-time tasks where the timing constraints are crucial, the concern of non-real-time tasks can 
be different. Such as multimedia applications and batch applications, users might be only interested in the 
throughput provided by the virtual core and the response time of a certain workload. While the throughput 
can be directly reflected by the userset operating frequency of the virtual core, the replenish period of the 
virtual core will affect whether a reasonable delay of the response time is achievable. However, since the 
granularity on the execution of non-real-time tasks is usually not as crucial as that of real-time tasks, users 
do not always want to treat non-real-time tasks as one kind of real-time tasks by setting the maximum 
總計畫：在多核平台上之多核虛擬環境研發 
 29 
virtual core in the virtualization system can be treated as a physical core with the capability of dynamic 
voltage scaling (DVS).  
In addition to the functions to utilize virtual cores, the hypervisor also needs to support admission 
control mechanisms because of limited computing resource of physical cores. That is, at each time of the 
virtual core creation or the virtual core adjustment, the admission control mechanism will be triggered to 
examine whether the request could be granted or not. Thus, we define as the remaining 
utilization for a physical core in the virtualization system, where  is the total utilization of 
virtual-core servers on the physical core. Then, when the remaining utilization of a physical core is 
changed due to the virtual core creation or the virtual core adjustment, the proposed admission control 
mechanism will grant the corresponding request if the new remaining utilization of the physical core is no 
less than 0; otherwise, the request should be rejected to prevent harming the schedulability of the system.  
While the physical cores have the capability of DVS, the hypervisor of a power-aware virtualization 
system should be able to adjust the operating frequencies of physical cores for energy saving. In our 
virtualization system, we exploit the DVS capability of physical cores by integrating a DVS scheduling 
policy with the admission control mechanism. As a result, once our admission control mechanism is 
triggered, it will also invoke the DVS scheduling policy if the physical cores support DVS. Given a 
remaining utilization of a physical core, our DVS scheduling policy will scale down the operating 
frequency of the physical core to the lowest available frequency of the physical core such that the 
remaining utilization of the physical core is no less than 0, and will only ask the admission control 
mechanism to reject the request if the remaining utilization is less than 0 even if the physical core operates 
at the highest available frequency. Similar to the scaling of operating frequencies on virtual cores, the 
replenish periods of virtual cores which have requirements for tolerable response time delay also need to 
be adjusted after the operating frequency of the physical core is changed. 
C. Implementation Remarks 
As we only demonstrate our design for the power-aware virtualization system with a single physical 
platform so far in this report, some extensions through minor modifications of our design will be 
總計畫：在多核平台上之多核虛擬環境研發 
 31 
switch overhead Os is a fixed number of execution cycles resulted from the context switch between any 
two virtual cores assigned to the same physical core. In order to compensate for the impact on the 
execution of virtual cores caused by the implementation overhead, we need to add some extra execution 
cycles to the maximum budget in our virtual-core server model. More specifically, if a user application 
needs a virtual core to provide Ci execution cycles in every Ti time units, we have to create a virtual core 
with parameters (Ci + Os + Om · Ci, Ti, Fi) when the implementation overhead is considered. Hence, under 
the deadline constraints of real-time tasks or the tolerable response time delay of non-real-time tasks, we 
should let the replenish period Ti of a virtual core as long as possible because a long replenish period can 
reduce the effect of the context switch overhead and thus save computing resource. 
D.   Performance Evaluation 
As shown in Figure B2, there are multiple guest operating systems running on multiple virtual cores 
where the number of the guest operating systems and the number of the virtual cores can be different. 
While the hypervisor is responsible for the creation of virtual cores for guest operating systems at the 
initialization of our virtualization system, the Virtual Core Function Server in the L4Env lets guest 
operating systems be able to manipulate virtual cores with virtual core functions during the runtime. The 
Virtual Core Function Server grants or rejects the requests from the guest operating systems according to 
our admission control mechanism and delivers requests that are granted to the Virtual Core Maintainer 
inside the L4:Fiasco. The Virtual Core Maintainer maintains an individual ready queue for each virtual 
core to keep the ready threads of each virtual core and notifies the scheduler of the L4:Fiasco. 
總計畫：在多核平台上之多核虛擬環境研發 
 33 
adjusted. Figure B4 shows the power trace of the chip which corresponds to the variation on the computing 
needs of virtual cores shown in Figure B3. As shown in Figure B4, the power consumption of the chip was 
decreased from 0.649 mWatt to 0.486 mWatt accordingly as the computing needs of virtual cores 
decreased. There was no change in the power consumption during the interval from 8 to 12 second because 
the physical core already operated at the lowest operating frequency. Compared to the system without 
applying any DVS scheduling policy, i.e., fixing the operating frequency of the physical core at the 
maximum available operating frequency (297 MHz), the proposed DVS scheduling policy reduced the 
energy consumption to complete the applications by 13.47% (, i.e, from 10.58 mJoule to 9.16 mJoule). 
 
Figure B3. The workload trace of virtual cores. 
 
總計畫：在多核平台上之多核虛擬環境研發 
 35 
◦ Sb is increased when real execution time is more than ab in the end of execution of a job. 
(Sb= Sb + ab – real execution time) 
◦ A job can continue to be executed when ab>0 or Sb>0 . 
◦ A job consumes ab first, and then consumes Sb when it runs out of ab. 
◦ When both ab and Sb is exhausted, the job is preempted. 
◦ Sb is more than or equal to zero.  
 
 
 
 
 
 
 
 
Figure B5. Slack Budget for Different Scheduling Approaches. 
總計畫：在多核平台上之多核虛擬環境研發 
 37 
◦ EDF: The job with earlier deadline has higher priority. 
◦ Importance: The job with higher Sv has higher priority. 
 The contents of the instance  
◦ Identifications (core id, subtask id and job id) 
◦ Deadline 
◦ Significance value 
◦ Expected migration overhead 
 
D. Flow of MCLSP 
 A. Signal communication protocol 
◦ The master monitors the timing of load sharing by signal communication protocol. 
 B. Migration conditions checking process 
◦ The master checks the migration conditions in runtime for choosing a suitable overrun job 
to be migrated to a idle core.  
 
Figure B8. Illustration of the Data Structure in Master. 
E. Signal Communication Protocol (Slave) 
總計畫：在多核平台上之多核虛擬環境研發 
 39 
 
F. Signal Communication Protocol (Master) 
 
Figure B10. Signal Communication Protocol at Master. 
 
Figure B11. Signal Communication Protocol at Master (Cont’d). 
總計畫：在多核平台上之多核虛擬環境研發 
 41 
 
Table B1 - Term Definition for Migration Conditions 
 
idle time of core X 
 
expected idle time of core X 
 
revised expected idle time of core X 
 
execution time of job Ji on core X 
 
expected execution time of job Ji on core X 
 
allocated budget of job Ji on core X 
 
worst case execution time(WCET) of job Ji on core X 
 
worst case execution time(WCET) of job Ji on core X 
 
Denotes 
1.  
This function defines the “Remaining Stages” before deadline of Jy. m and n are the stage numbers. Stage 
m is the current stage and Stage n is the stage before deadline of job Jy on core X. 
2.  
This function defines the “Remaining Jobs” in this stage. j and k are the job numbers. Job Jj is the current 
job and Job Jk is the last job of core X in this stage. 
3.  
This function averages expected idle time of core X 
 
總計畫：在多核平台上之多核虛擬環境研發 
 43 
 
Migration Conditions III: 
 Situation 
◦ Source core(S) has not sent “On Core Signal” yet. 
◦ The processing time of S has exceeded the original schedule (only for non-preemptive 
approach) 
 
總計畫：在多核平台上之多核虛擬環境研發 
 45 
 
K. Our Solution for Core-to-Core Synchronization Problem 
 Nearly Distributed Global mutex locking (Nearly Distributed GML) algorithm  
◦ The Master controls the right to lock the global mutex. 
◦ The Prime Slave (PS), which is granted to lock the mutex, wakes up the other slaves 
waiting in Mutex Waiting Queue.  
 
Figure B15. Illustration of Core-to-Core Synchronization Problem. 
L. Exception Handling 
 Exception Situation 
◦ If by programming error or because of a software or hardware fault the owner does not 
unlock the mutex, the thread(s) awaiting at a lock operation would remain blocked 
forever.  
 Solution: Timed mutex 
◦ Timeout estimation: WCET of critical section  
 
M. Processes of Nearly Distributed GML Algorithm 
 Enqueue process 
◦ Master add signals into Mutex Waiting Queue  
總計畫：在多核平台上之多核虛擬環境研發 
 47 
 
N. Master: state transition diagram  
 
Figure B16. State transition diagram at Master. 
O. Slave: state transition diagram  
 
總計畫：在多核平台上之多核虛擬環境研發 
 49 
Figure B19 illustrates the call ﬂow for NTU ICPC protocol. The sending process, named sender for 
short, sends the receiving process a data ready mail, including the receiver ID and pointer to the data 
buffer. When there is no error for sending the mail, the sender can continue its work without waiting for 
the mail being read and data being retrieved. On the other hand, the receiving process, named receiver 
for short, checks its mailbox when expecting a mail. When there is any unread mail in the mailbox, it 
requests the data to be copied into its local buffer and sets the mail being read. 
 
Fig.B19 NTU Inter-Core Process Communication Protocol 
To speed up the inter-core communication and ensure its correctness, NTU ICPC consists of three 
major components: mail notiﬁcation, direct data movement, and circular buffer management. The 
protocol is speciﬁcally designed for pipeline scheduled periodic processes on heterogeneous multi-core 
platforms. We discuss the rationale of using the above three mechanisms and how NTU ICPC protocol 
works in the following. 
 
AA.   Major components of NTU ICPC protocol 
Mail notification Traditional mailbox protocol sends the whole data in the mail to the receiving 
processes on behalf of the sending process. The service copies the transmitted data from sender’s buffer 
into the mailbox buffer. When the receiving process reads the mail, the data is copied again from mailbox 
buffer to receiver’s buffer. When the sender and receiver are located in different memory domains, this 
approach assures the autonomy for both parties. However, on heterogeneous multi-core processors, the 
sender and receiver are located on the same memory domain. It is not necessary to use so many memory 
copies. Although mail service is not suitable for large data size movement, it is an effective 
communication service when the sender and receiver agree on the communication in advance. The mail 
notiﬁcation service in NTU ICPC uses a short message to notify the receiving processes and does not 
copy the data to mailbox buffers. 
總計畫：在多核平台上之多核虛擬環境研發 
 51 
function icpc_receive(), the receiving process first checks if there is any unread mail. If any, the process asks 
the memory management sub-system to copy the data from circular buffer to the provided local buffer. 
Otherwise, the receiver polls the mailbox till the timeout expires or a new mail arrives. 
 
NTU ICPC also allows the multiple senders and receivers IPC data exchange. It is needed when a 
sender would like to multi-cast the data to several receivers or a receiver will wait for the data from 
multiple senders before it continues. 
 Algorithm 3 shows the pseudo of function icpc_receivemanysender(). 
 
 
AB. Cache Coherency for NTU ICPC 
Cache coherency refers to the integrity of data stored in local caches of a shared resource. When 
multiple cores share a memory space and the process executing on one of the cores modiﬁes the value of 
an address in the shared memory, the value of that address in other cores’ local caches are all invalid and 
總計畫：在多核平台上之多核虛擬環境研發 
 53 
 
 
Fig.B20 Operations to Assure Cache Coherency 
When the sender conducts the computation using its input data, the results are wrote to the output 
buffer. To ensure that the results are stored in the output buffer rather than in sender’s cache, a “Write 
back” is issued. This operation is illustrated by the dash line labeled by (1). The second case, labeled by 
(2), ensures that the correct mails are sent to receiver’s mailbox. To do so, we must make sure the mail is 
written to the receiver’s mailbox. If the mailbox’s address is cached in the sender’s cache, a “Write back” 
operation is need so that the receiver can get the updated mail. The third case, labeled by (3), ensures that 
the receiver receives the up-to-date mail. When the receiver is ready to receive its input data, an 
“Invalidate” operation is issued since the mailbox’s address may have been cached in receiver’s cache. It 
forces the receiver to get mail from the memory directly instead of fetching out of date mail from cache. 
The forth case, labeled by (4) in the ﬁgure, ensures that the receiver copies the correct data into its buffer. 
After getting the “data ready” mail, the receiver starts to copy data from sender’s output buffer to its 
input buffer. Likewise, before copying data, we should “Invalidate” the cache line corresponding to 
sender’s output buffer in receiver’s cache to ensure the receiver will get the updated data. The ﬁfth case, 
labeled by (5), ensures that the receiver writes its computation results into the output buffer, not its local 
cache. This step is equal to the ﬁrst step described earlier. 
C. Comparison with traditional IPC protocols 
NTU ICPC are different with traditional IPC protocols in several perspectives including its memory 
management and how the services are provided. In Table II, we compare the major features of NTU 
ICPC with that of three IPC communication protocols including message box, mailbox, and share 
memory. 
總計畫：在多核平台上之多核虛擬環境研發 
 55 
once during the initial phase. Hence, it will be more efﬁcient to implement the ICPC service as a 
user-level service module on top of the operating system. Then, an application interface (API) is 
provided for user applications shown in Figure B21. With the API, applications or subtask in a pipeline 
scheduled streaming application can requests the service via the API to avoid a series of expensive 
operating-system calls and reducing the overhead. In addition, implementing ICPC as a user-level service 
module also makes it easier to port the service to different hardware platforms. On multi-core platforms, 
the ICPC service module must be provided in all the cores which may use different instruction sets. 
Hence the portability is one of the most critical desired features for the protocol. 
 
Fig. B21 ICPC Service Module 
AA. Requirements on the Platforms 
To conduct inter-core process communication, NTU ICPC module must be installed on each core on 
the processor. To allow the protocol to be ported to different platforms, there are certain requirements on 
the platforms. 
System software requirements Because NTU ICPC is designed with portability in mind, there are 
limited requirements on system software. When there is no operating system support, the memory 
resource will be fully managed by NTU ICPC service module. Many DSP core in SoC does not provide 
any operating system support. When an operating system is provided on the core and manages the 
memory resources, it must provide the “memory map” function. Thus, ICPC service module can call this 
function to map a range of memory space to processes’ address space so that the processes can have the 
permission and mapped virtual address to access the buffers. If the operating system manages the 
memory without virtual memory mechanism, it should provide the service of assigning permission of 
speciﬁc memory region when the caller has no permission to access it. 
Hardware Requirement NTU ICPC service module can support different memory architecture and 
take advantage of the characteristics of different architectures to optimize the performance. The memory 
architecture of a heterogeneous multi-core platform is classiﬁed as follow. 
總計畫：在多核平台上之多核虛擬環境研發 
 57 
 
AB. Layered Architecture 
As we mentioned above, portability is one of the desired features for the implementation. We use 
layered software architecture for the ICPC service module. Each layer uses the functions provided by the 
underlying layer. When porting the module to a different platform, only the platform dependent layer 
should be changed. 
Figure B24 shows the software architecture for NTU ICPC service module. There are three layers in 
the module: porting layer, main function layer, and protocol layer. An ICPC protocol may use certain 
functions which are platform or operating system dependent. We include all those platform or operating 
system dependent functions in porting layer to provide a uniform interface to upper layers. Main function 
layer consists of I/O buffer subsystem and mailbox subsystem. These two subsystems are responsible for 
direct data movement, circular buffer and mail notiﬁcation mechanism used in NTU ICPC protocol. 
NTU ICPC protocol is implemented in protocol layer. The protocol layer provides application interfaces 
(API) for the processes to communicate with each other. The following describe each layer in detail. 
Protocol Layer It includes the connection configuration, send, receive, sendtomany, and 
receivefrommany. The connection configuration pairs the senders and receivers for data exchange. The 
set of senders and receivers is not supposed to be changed after any data exchange starts. Changing the 
conﬁguration may lost the unread data on the receiver side, which needs to be retransmitted. How to use 
these functions to complete the inter-core communication will be described at the end of this section. 
 
Fig. B24 ICPC Service Module Software Architecture 
Main Function Layer - IO Buffer Subsystem The IO buffer subsystem is responsible for 
managing input and output buffers. When the connection is set, it allocates input and output buffers for 
all the requests and manage them according to the connection topology information speciﬁed by the 
conﬁguration. (We will discuss the connection topology later.) Allocating input and output buffers 
總計畫：在多核平台上之多核虛擬環境研發 
 59 
mapping service provided by MMU will be wrapped to provide the memory management service for the 
upper layer. Using this service, we can request the access permission and map virtual address for a range 
of memory space. 
AC. Use of NTU ICPC 
Using the ICPC service module is simple and can be divided into three main steps. At first step, the 
programmer configures the ICPC service module including the involved processes and hardware 
information of the target system, so that the IO buffer subsystem and mailbox subsystem can allocate the 
input buffer, output buffer and mailbox for all involved processes. The information includes the number 
of cores, the number of involved processes, which processes reside on which cores, the execution flow of 
the subtasks, memory map of the platform and the free working space of each core for communication. 
The configuration is conducted at compile time and is static. Comparing with distributed systems, 
multi-core SoCs have static hardware configuration which means that no core can be added or removed 
from the platform during run-time. Based on this characteristic, NTU ICPC service module provides 
connection oriented and channel based communication. After specifying the configuration at compile 
time, the ICPC service module stores the information and setups the connection channel between the 
subtasks which need to communicate with each other. Figure B25 shows an example. In this figure, there 
are four cores with six subtasks executed on them. The execution flow of these subtasks is listed in the 
bottom of the ﬁgure. With the cores and subtasks’ connection topology information, ICPC service 
module setups the channels for each communication accordingly. The arrows in the ﬁgure mean that each 
channel is unidirectional. Furthermore, with the connection channel, the input and output buffers 
involved in the communication are known; the programmers do not need to provide that information 
each time when using the ICPC service module for communication. This feature can also be seen as a 
manner to reduce the overhead of each transmission. After this step, NTU ICPC conﬁguration tool will 
generate the system dependent codes and header ﬁles automatically. Figure 9 shows an example program 
to illustrate how to use the API functions provided by NTU ICPC service module. 
 
總計畫：在多核平台上之多核虛擬環境研發 
 61 
zero-buffer inter-core process communication protocol, i.e., NTU ICPC. NTU ICPC takes advantage of the 
special processor and memory architecture in heterogeneous multi-core platforms to avoid context switches 
and memory copy operations. In addition, the polling-base mail notiﬁcation is used to notify the senders and 
receivers. The evaluation results show that the above mechanisms effectively shorten the communication 
overhead and enhance the throughput for real-world applications. The frame encoding rate for H.264 increases 
for 30% to 40%. 
III. Workload Migration Mechanism between Networked Embedded System and 
Cloud Server 
A. System Overview 
In this section, we present a high-level overview of components of our system on the mobile device 
and on the cloud in order to understand how they all integrate into one platform for migrating workload. 
Figure 4.1 provides an overview of the system architecture of our system. Our workload migration 
system uses a client-server model. The client program named workload dispatcher runs on the mobile 
device, and the server program runs on a physical/virtual machine on the cloud. The device and the 
machine on the cloud must have the same instruction set and operating system. For example, the mobile 
device is an EeePC running Linux, and there is a x86 virtual machine running Linux on the cloud to 
provide extra computing resource. With this property, the workload on the device can be directly 
migrated to the cloud without binary translation. The users can choose a mechanism according to the 
network connectivity, and pass it to the workload migration system as an argument. The workload 
dispatcher provides an interface to receive arguments from users. The workload dispatcher will send 
input data and necessary ﬁles to the server on the cloud. Once the request from the workload dispatcher 
is accepted by the server, the server will transmit data and ﬁles from the mobile device and use 
computing resource on the cloud to execute the workload.  Then, the workload dispatcher will wait until 
the output data 
is received 
from the cloud. 
 
 
 
Figure B28: 
總計畫：在多核平台上之多核虛擬環境研發 
 63 
In our workload migration system, the workload dispatcher running on the mobile device is an 
interface for the users to migrate workloads to the cloud. The users have to provide the workload 
information ﬁle as the input of the workload dispatcher and pass an argument to indicate the selected 
mechanism. Then, it transmits essential ﬁles to the server according to the arguments provided by the 
user, and 
receives the 
results from 
the server. 
 
 
 
 
 
Figure B29: Flow Chart of Workload Dispatcher 
Figure B29 shows the control ﬂow of the workload dispatcher. If the local execution mechanism is 
applied, the workload dispatcher will execute the workload on the device without workload migration.  
Otherwise, the workload dispatcher will parse information from the workload information ﬁle.  Next, it 
will connect to the server, send workload information to the server and notify the server which remote 
execution mechanism to be applied.  Then, it will send input data ﬁles to the server.  If the remote 
execution with source codes transmission is applied, the workload dispatcher will transmit source code 
ﬁles to the server. Otherwise, it will transmit the main program and dynamic loaded libraries.  
Eventually, it will wait until all the output data ﬁles are received from the server. 
Workload Execution Server 
The server is a daemon program running on the cloud. It accepts requests from the mobile devices. 
The server consists of a transmission thread and a computation thread. These two threads run on different 
cores of the processor to guarantee that overlapping transmission and execution will shorten the RT. The 
transmission thread receives input data ﬁles, main program and libraries from the device. The 
computation thread loads and executes the program of the workload migrated. 
總計畫：在多核平台上之多核虛擬環境研發 
 65 
transmission. 
F-1. Transmission Status Table 
The program of the workload is divided into multiple dynamic loaded libraries. To record the 
status of each dynamic loaded library, the servers maintain a data structure called transmission 
status table.  Each entry in the table is used to indicate whether a library is transmitted or not. 
 
F-2. Control Flow 
 
 
 
 
 
 
Figure B31: Flow Chart of Streaming Execution 
Figure B31 shows the control ﬂow of streaming execution. Computation and transmission are 
handled concurrently by computation thread and transmission thread. The transmission thread 
transmits the dynamic loaded libraries and update entries in the transmission status table when 
corresponding libraries are transmitted.  In the computation thread, once a function is called, it will 
be redirected to a middle function. 
First, the middle function checks the transmission status of the dynamic loaded library 
containing the original function. If the library is transmitted, it will load libraries, get the function 
pointer, and jump to the original function. Otherwise, it will block until the library is transmitted.  
With this approach, we can treat the program as a stream and execute it before the whole program is 
transmitted. 
 
總計畫：在多核平台上之多核虛擬環境研發 
 67 
keep the availability and reduce migration overhead, it provides different execution mechanisms for users 
to choice under different network condition. To reduce the overhead of workload migration, we design a 
remote execution mechanism called streaming execution. It overlapped the computation and transmission 
of a workload to reduce the transmission overhead. We use the programming interface to dynamic linking 
loader in Linux to implement this mechanism. In addition, we introduce a method to make a workload 
compatible with it. We implement the framework on our target platform and do several experiments to 
verify that the mechanism we designed can improve the performance. We verify the performance with two 
workloads, dark chess and whetstone benchmark. In addition, we analyze the results to find out what kinds 
of workload are suitable for streaming execution. Finally, we make a manipulated workload to present the 
best case for streaming execution. 
IV. VM-AWARE FAIR SCHEDULER  
 VM-aware fair schedulers (VMAFS) are divided into the two subroutines - VMAFS-P and VMAFS-N, 
depending on whether the underlying execution environment allows a task to be preempted with reasonably 
low overhead. Tasks run on GPPs can be preempted and context switch to other tasks for execution at lower 
cost in general, typically include some cache misses. Due to the proprietary software stack, binary device 
drivers and the limitation of hardware, tasks run on SPPs, however, are less likely to be pre-emptible or may 
otherwise introduce non-negligible overhead such as massive data-movement across buses. Rather, typical 
execution model requires the current task to ﬁnish or voluntarily relinquish processors before the next task 
could run. We design schedulers for both pre-emptible and non-pre-emptible processors to achieve VM-level 
fairness and are described in Section B35 and B36 respectively. Note that the decision of whether a kernel 
should run on GPPs or SPPs is made by the programmers and is not addressed in this research. 
A. VMAFS-P Scheduler 
Overview There are two queues on each processor, active and budget-exhausted queue. All kernels 
in active and budget-exhausted queue are called ”schedulable kernels”. Initially, all schedulable kernels 
are placed in the active queue. In general, when kernel on one processor has been selected for execution 
and has exhausted its budget, it will be placed into budget-exhausted queue. We adopt the concept of 
round-robin method to schedule kernels. That is, after all kernels have been placed into the 
budget-exhausted queue, the VMAFS-P switches both queues and the processor is advanced to its next 
round. We deﬁne a Switch event as the event when active queue becomes empty and the processor is 
permitted to switch it with the budget-exhausted queue. We deﬁne the time interval between two 
consecutive occurrences of Switch event as a round. We describe in previous sections about how 
VMAFS-P maintains runtime status of each processor and kernel tasks, along with how it distribute 
proportional processor sharing to individual VMs. 
總計畫：在多核平台上之多核虛擬環境研發 
 69 
 
A-2. VMAFS-P Algorithm 
There are four procedures in VMAFS-P algorithm. First, ”Kernel Select Procedure” is the entry 
point of the VMAFS-P scheduler and is invoked by each processor p when the budget of the current 
kernel on p has been used up and p is about to select the next kernel for execution.  Second, in 
Kernel Select Procedure, we design a ”Load Sharing Procedure”, whose goal is to have light-loaded 
processor to share the work from heavy-loaded processor during each round. The main idea is to 
balance the load on each processor so that the kernels on light loaded processors do not receive 
more than enough execution time. Third, ”Lag Boosting Procedure”, also being invoked in Kernel 
Select Procedure, is responsible for keeping the difference between σ max and σ min within U. 
The rationale behind is to ensure that each VM would receive its pro-portional sharing of the P 
processors during any time interval. Finally The ”Entrance Control Procedure” is responsible for 
maintaining the total number of schedulable kernels in system. The details of the procedures are 
described as follows. 
Figure B35 illustrates the algorithm and the ﬂowchart of Kernel Select Procedure, the main 
routine of VMAFS-P, along with its two important subroutines - Load Sharing Procedure and Lag 
Boosting Procedure. 
Upon the time when processor p is about to select a new kernel for execution. Step 1: the 
scheduler checks if round p is equal to roundmax. If the result is true, the scheduler computes the 
current σ max and σ min. Otherwise it goes to Step 2. We introduce the details of Step 2 later. After 
ﬁnding the σ max and σ min, VMAFS-P scheduler checks if σ max − σ min is larger than or equal to 
U. If true, go to ”Lag Boosting Procedure”, otherwise go to ”Load Sharing Procedure”.  Step 
總計畫：在多核平台上之多核虛擬環境研發 
 71 
 
 
Figure B36: Flowchart of the Lag Boosting Procedure. 
Finally, the Entrance Control procedure checks the kernel’s budget. That is, when a kernel 
completes, VMAFS-P checks if the executing time is equal to its budget or not. If the return result is 
false, the scheduler will add a new kernel from same VM to active queue and continue to execute 
for remaining budget. Otherwise, the result shows that the kernel executed full budget then 
scheduler put the kernel into budget-exhausted queue as usual. We also describe the detail in 
Algorithm 4.3 and Figure B38. 
 
 
總計畫：在多核平台上之多核虛擬環境研發 
 73 
 
Figure B39: An Example of Geometric Presentation on VMAFS-N Terminology. 
B-2. VMAFS-N Algorithm 
The main idea of non-preemptive scheduler is to use the vector-like interpretation of fairness as 
described above to identify the most suitable kernel to be executed when the a kernel on a processor 
has ﬁnished. The geometric interpretation enables us to measure exactly how close it is from current 
status to ideal fairness. VMAFS-N scheduler adopts a greedy approach. Whenever invoked, 
VMAFS-N scheduler searches for a kernel yet to be executed so that the angle between the ideal 
vector IN and the status vector v N (after the execution of the to-be-executed kernel) will be the 
smallest. The cosine of the angle is given by the following formula: 
   (B35) 
Since cosine function is a strictly descending function, VMAFS-N scheduler maximizes 
Equation B35 during each scheduling point. In this way, VMAFS-N scheduler tries to keep the 
status vector always pointing to the direction closest to the ideal fairness vector. In other words, 
VMAFS-N scheduler achieves VM-level fairness by always trying to align the status vector and the 
ideal vector. Let vN (Ti) denote the status vector at i t h scheduling instant Ti . To select a new kernel 
at Ti , note that each kernel belongs to exactly one VM and thus could contribute and only 
contribute to one(out of N) components of the status vector. Thus the difference between vN(Ti) and 
vN (Ti+1) is only along one dimension, according to which VM the kernel selected at T i belongs to. 
To achieve fairness, VMAFS-N always selects a kernel from the pool of all runnable kernels at 
scheduling instant T i that will minimize the angle between vN (Ti+1) and IN . The details are 
describes as Algorithm 4.4.  
We present a simple example to further illustrate how VMAFS-N works in Figure B40. There 
are two VMs in the system and ideal vector I 2 and current execution time vector v2(Ti) are shown 
in the ﬁgure. When the current kernel terminates, the scheduler calculates the cosine value at 
scheduling instant Ti+1 for all runnable kernels according to Equation B35. The v2(Ti) is the black 
總計畫：在多核平台上之多核虛擬環境研發 
 75 
contention and avoids the lower priority of Gen-VMs receiving more computing resource than the 
higher priority of GenVMs. In VMAFS-P, Load Sharing Procedure is responsible to dynamically 
migrate kernels from heavy-load processors to light-load processors. This design is beneﬁt to 
system’s utilization and avoid the idle status on each processors. And Lag Boosting 
Procedure is responsible to enhance the fairness accuracy among Gen-VMs. That is, the users 
can set up the parameter of upper bound U, and VMASF-P will maintain the difference of received 
computing time among Gen-VMs always under U. In VMAFS-P, the VM-level fairness is 
interpreted geometrically by using vector-like concepts. This geometric approach is appropriate due 
to the non-preemptive nature of underlying processors. VMAFS-P will choose the next kernel 
depending on achieving ideal fairness with the smallest angle. 
III. Subproject 3 
In previous section, we described that a full cache simulator suffers heavy synchronization overhead. 
Although, it simulates the behavior of coherent caches faithfully and reports accurate results, the 
simulation speed can be very slow with poor scalability. To relax the massive synchronization overhead 
of a full cache simulator, we proposed a simplified multi-threaded execution model with barrier-based 
synchronization scheme. 
Since the non-determinism of parallel programs, the interleaved order of memory accesses by 
different threads may be different from run to run. In a parallel execution, the coherence misses are 
intermixed and difficult to identify during runtime. With the barrier information in the memory traces, 
memory references are separated into stages. The execution order of the stages is fixed by the barrier 
operations. However, the actual memory reference sequence among threads within the same stage is 
uncertain. 
Our approach makes use of the these information to estimate the coherence misses of a 
multi-threaded program. We first estimate the lower bound and upper bound of coherence misses of a 
multi-threaded application. By analyzing the memory traces in parallel, we could efficiently estimate the 
lower bound and upper bound to obtain a brief characterization of the program behavior. Also, we 
developed a method to further estimate the possible coherence misses. Later, we will present the 
methodology of coherence misses estimation in more detail. 
  In our multi-threaded emulator, each thread is responsible for the memory references of an 
emulated private cache, and we would like them to execute in parallel as much as possible. We chose the 
barrier operations of a multi-threaded program as the synchronization points, since a barrier operation is a 
global event that enforce all threads to synchronize. The temporal order of memory references on a local 
總計畫：在多核平台上之多核虛擬環境研發 
 77 
 
Figure C2. System Architecture and Interfaces 
 
In the following sections, we present the trace-driven mode of our cache emulation for the 
evaluation purpose as shown in Figure C3. We examined the emulation results with the ones 
simulated by GEMS. Since the emulation of COREMU behaves similarly as the execution on real 
multicore hardware, the memory references vary from run to run. We employed the trace-driven 
approach for evaluation to ensure feeding the same memory references. 
 
Figure C3. The evaluation flow of the trace-driven approach 
 
Generating Parallel Traces 
COREMU maintains its memory manage unit (MMU) during the emulation. We monitor every 
memory operation in the dynamic binary translation engine. For generating the memory traces, we 
first record memory references into a local buffer. When the buffer is full or at the end of the 
總計畫：在多核平台上之多核虛擬環境研發 
 79 
 
Figure C5. Sequential memory trace 
 
B. Multi-threaded Coherence Cache Emulation 
 
The simulation scheme of our multi-threaded emulator is composed of two phases, the parallel 
processing and the synchronization phase, as shown in Figure C6. As we mentioned previously, the 
barrier events in the parallel memory traces separate memory references into stages. In the parallel 
processing phase, each thread independently consumes the memory references from its own trace 
file. Whenever a barrier event is encountered, each emulation thread has to perform a barrier 
synchronization and then estimate the coherence misses in this stage. 
 
Figure C6. Emulation flow 
 
Figure C7 presents the performance issue of our approach. The simulation speedup is related to 
the parallelism of the multi-threaded applications. Moreover, our approach needs to pay a 
synchronization overhead whenever a barrier point is encountered. However, the experimental result 
總計畫：在多核平台上之多核虛擬環境研發 
 81 
communications conservatively estimates the invalidations happening across stages. 
 
Optional Communication Estimation 
The amount of optional communications is estimated by the number of reads and writes in the 
same stage. Considering the shared cache line in the same stage, optional read misses must be 
caused by write accesses of another thread, as shown in equation A-1, where Ri denotes the number 
of reads of i-th thread, and Wr denotes the number of writes of the rest threads. It is clear that the 
number of misses is not larger than the number of memory references, i.e. Ri. When Ri is larger than 
Wr, the read misses should not exceed the number of writes of other threads. Similarly, optional 
write misses are estimated by equation A-2. Finally, the upper bound of optional communications is 
the summation of the optional read misses and optional write misses, as shown in equation A-3. 
 
OptionalReadMissesi = MIN{Ri, Wr}                             (A-1) 
OptionalWriteMissesi = MIN{Wi, Rr + Wr}                         (A-2) 
OptionalCommunicationsi = MIN{OptionalReadMissesi + OptionalWriteMissesi} (A-3) 
 
 
Table C2. Reference scenarios in the same stage 
 
Heuristic Reference Distance 
The actual number of coherence misses is hardly to reach the upper bound of optional 
communications, which assume the worst situation that the interleaving of references will cause the 
most misses. In addition to estimate a proper optional communication, we apply another method 
with the concept of processor locality. The reference run [C-27], a performance metric for parallel 
programs, means the stream of uninterrupted references by one processor to a cache line. We define 
the conjectural reference run (CRR) to estimate the optional communications in a heuristic approach. 
總計畫：在多核平台上之多核虛擬環境研發 
 83 
offsets of each thread if the line is a false-sharing line. This report is for programmers to diagnose 
the false-sharing problem of programs. If the programmer can know the data structures causing 
false-sharing cache lines, the unnecessary sharing is avoidable. By making the data allocated for 
different threads lie on different cache lines, the false-sharing cache lines can be eliminated 
efficiently. 
 
IV. Subproject 4 
為了解程式在多核心架構下的表現，以找出具代表性的特徵點，在第一年的計畫中，我們
透過 Gtk+ tool[D-3]搭配 Glade[D-4]及 python[D-1]，並利用 Perfmon 工具[D-7]，建立出一個可
在多核心架構系統下觀測程式行為的軟體工具，即 AACS 中的 MS 子系統。MS 利用 Perfmon
工具，根據使用者選定之目標觀察行為，取得程式執行時的行為資料，如單位週期執行指令數
(Instruction Per Cycle，IPC)及快取記憶體誤失率(Cache Miss Rate)等。然後 MS 利用 Gnuplot 繪
圖工具[D-2]將得到的結果以圖形化方式呈現給使用者。本章節中我們將詳細描述 MS 的功能及
程式行為觀察結果。 
I. Monitoring Subsystem (MS)  
本子系統負責執行並監控使用者選擇之待觀測程式，並將結果以繪圖方式顯示，依
功能可分成負責觀察的部份及負責繪圖的部份。 
   觀察部份可分為以下三個模組(Module)： 
(1) 程式執行模組 ( Execution Module )  
主要功能為依據使用者之需求，執行相對應的應用程式並輸入該程式應接收的
輸入資料 (Input Data) ，以及指定程式的平行度等。 
(2) 監看模組 ( Monitoring Module )  
負責呼叫Perfmon來監看被執行的應用程式，依據使用者需求，收集特定硬體計
數器(hardware counter)的資料來紀錄特定事件(e.g., Instruction Retired, Cache Access 
Times, Cache Miss Times, etc )，在固定的週期將計數器的內容讀出並且清空。 
(3) 解析模組 (Parsing Module )  
解析監看模組產生的結果，計算出使用者欲監看的事件(e.g., IPC, Cache Miss 
總計畫：在多核平台上之多核虛擬環境研發 
 85 
 
 
 
 
 
II. MS Subsystem User Interface 
 圖 D3 為 MS 子系統觀察部份的使用者互動介面，依照功能的差異分成四個部份，
分別以 A、B、C 跟 D 稱之。A 部份讓使用者輸入 MS 系統所需的待觀察程式資訊，使用
者在此部份可以選擇要觀察的程式，以及指定該程式的輸入檔或輸入參數。B 部份是讓
使用者選擇要觀察事件，包含 IPC 及 last level cache miss rate。C 部份則是讓使用者輸入
Perfmon 的取樣週期，預設為 100000 個 CPU 週期。D 部份是 MS 子系統給使用者的資訊，
回報使用者已選擇的程式及欲觀察的程式行為。 
 
 
圖 4 為 MS 子系統中繪圖部份的使用者介面圖，依照功能的不同分成四個部份，分別
以 A、B、C 跟 D 稱之。A 部份為一個樹狀圖的檔案結構，MS 觀察部份的輸出檔會在此
以樹狀圖呈現，此檔案結構各層為檔名、程式行為、執行緒 id、已儲存完成的圖檔。B 部
份提供三個控制 A 部份樹狀圖的功能，以左開始分別是重新讀取樹狀結構、存下圖片、刪
除已選擇的圖片。C 部份則是將已選擇的圖檔重繪於 MS 中，圖中可以看到該程式單一執
圖 D2. MS 繪圖狀態流程圖 
圖 D3.  MS 子系統觀察部份的使用者互動介面 
總計畫：在多核平台上之多核虛擬環境研發 
 87 
 
III. Communication Count Vector 
在多執行緒程式中，各執行緒接觸到的資料集合相同與否也可能會影響到程式執行
時的行為。如圖 D8 所示，這兩個情境中執行緒均執行相同的基本區塊，但是執行緒間的
共享資料量不同，執行緒間共享資料較少的情境其溝通量也較少，而共享資料較多的情
境其溝通量也會比較多，執行緒間不同的溝通量可能會導致不同的程式效能，則此兩種
情境應該分屬兩種不同的相位。所以捕捉多執行緒程式中執行緒間共享資料情形，並推
測執行緒間溝通量的多寡，對於偵測程式相位來說很重要。 
 
 
在本計畫中，我們提出了一個捕捉執行緒間共享資料量及溝通量的技術，
Communication Count Vector (CCV)。CCV 是一個一維的頻率向量，每個 CCV 中的數值
代表了每個區間中各個相對應的核心產生的快取一致訊息(cache coherence message)數
量，這種訊息是回應其他核心要存取共享資料而產生的，也稱為監聽回應 (snoop 
response)，因此可用來幫助捕捉共享資料的情形進而推測執行緒間的溝通量。藉由 CCV，
我們便可利用偵測執行緒間的溝通量來進行程式相位分類，並挑選出各個相位中最具代
表性的區間來當作特徵點。 
圖 D9 表示了 CCV 的形成過程，若執行緒 1 想要存取共享資料 A，而執行緒 3 有這
筆資料，則執行緒 3 所在核心便會發出一致訊息來回應這個要求，CCV 便會把這個訊息
在 CCV 中執行緒 3 相對應的位置記錄下來。 
圖 D8. 平行程式中資料共享情形 
總計畫：在多核平台上之多核虛擬環境研發 
 89 
起來，最後便完成了在該區間內記憶體存取位址的記錄。然後我們會將記憶體存取位址
記錄對應回 MFV 中，我們將記憶體位址空間分成 100 個連續的區段，MFV 中的每個數
值就代表每個區段中被存取的次數。我們事先嘗詴了將記憶體位址分成 10、100、1000
個區段的實驗，而實驗結果顯示分成 100 個區段可以產生最好的結果。 
 
 
V. 3.5 Characterization Subsystem (CS) 
我們將以上我們提出的相位偵測方法結合進我們開發的 AACS 工具中，並包裝成
Characterization Subsystem(CS)。在 CS 中，使用者可以選定目標程式，CS 會自動進行程
式行為捕捉、區間分類及產生特徵點。CS 使用的程式行為捕捉方法是將 SBBV 結合 CCV
及 MFV，而能同時考慮被執行的程式碼片段、執行緒間的溝通量以及執行緒間 LLC 共
享資源競爭情形，來進行相位偵測。 
CS 可分成以下幾個模組： 
(1) 程式執行模組 ( Execution Module )  
針對使用者所選定之多執行緒程式，執行該應用程式並輸入該程式應接收
的輸入資料 (Input Data)。 
(2) 側錄模組 ( Profiling Module )  
依據SBBV、CCV、MFV所需，這模組利用Perfmon及Pin工具來收集程式執
行時每個被取樣指令的位址、各執行緒的溝通訊息數量以及每個被取樣且要執
行記憶體存取指令的記憶體存取位址。 
(3) 解析模組 (Parsing Module )  
解析側錄模組產生的結果，將每個區間取樣到的指令位址對應到基本區塊
產生該區間之SBBV，將每個區間收集到的溝通訊息數量組成該區間之CCV，以
及將每個區間收集之記憶體存取位址對應到MFV的元素中並產生該區間之
圖 D10. Memory Footprint Vector (MFV) 
總計畫：在多核平台上之多核虛擬環境研發 
 91 
            
Table A1.  Specifications of Intel i7 and Intel Q8200 
 
 
五、 結果與討論 
I. Subproject 1 
In this section, we are going to present the experiment setup and some preliminary performance 
results, and then we will discuss multiple interesting insights we have discovered during the 
development process.  
A. Experiment Setup 
We use two different platforms with Intel Nahelem i7 and Intel Core2Quad Q8200 to test our 
圖 D15. CS 子系統使用者互動介面 
總計畫：在多核平台上之多核虛擬環境研發 
 93 
Figure A8 demonstrates the experiment results of our parallel simulator with and without the 
redundant statelist structure we have mentioned in the previous sections. As the results show that the 
optimization that we rewrite the data structures to make the simulator design close to the hardware 
design actually pays off, the performance gain of our simulator is mainly coming from this 
modification. 
 
Figure A9 show the performance difference between two different architectures. The results 
indicate that our simulator performs drastically different on different architectures, we will discuss this 
in the later subsection. 
 
C. Discussion 
During the process of parallelizing MARSS with our pipeline partitioned methodology, we have 
            
Figure A8.  Performance result of our pipeline-partitioned parallel simulator with/without statelist structure 
            
Figure A9.  Performance result of our pipeline-partitioned parallel simulator on Intel i7 and Intel Q8200 
總計畫：在多核平台上之多核虛擬環境研發 
 95 
Although the experiment shows that simulation performance is higher on Intel i7 than Intel 
Core2Quad Q8200, the performance difference between sequential and parallel version of MARSS 
show a totally opposite result. A reasonable explanation is that the sequential version is benefited 
more from the even high frequency provided by Intel i7 than parallel version. In fact, as the higher 
frequency helps shortening the execution time, the synchronization overhead such as barrier waiting 
time does not consequentially be reduced as well. On the contrary, the relative performance impact 
of the synchronization overhead is increased, for the waiting-to-execution timing ratio becomes 
higher due to the decrease of the time spent on the execution. 
II. Subproject 3 
Table C3 presents our experimental setup for evaluation. The host system contains 48 AMD 
Opteron 6174 cores running at 2.2GHz with 32GB DDR3 memory installed. Our evaluation is 
performed with COREMU built for emulating the ARM target system at first, which can emulate at 
most 4 cores system. To further evaluate the experimental results on many-core system, we have also 
built our framework to support the x86_64 target system.  
We ran the experiments with parallel applications from SPLASH-2 [26] and PARSEC [28] 
benchmark suite in section 4.2. Note that we only ran 4 out of 10 benchmarks form PARSEC, since 
many of the programs cannot be cross-compiled for ARM. The four programs are blackscholes, 
bodytrack, fluidanimate, and streamcluster. Besides, the number of emulated cores is always 
configured to be the same with the number of threads of benchmarks. 
We compared the simulation result with GEMS, and configured it to simulate the memory 
subsystem with 256MB L1 private cache, 32-way set-associative, and 32Byte cache line size. The 
large capacity is to ensure that there is no capacity miss and conflict miss. We would like to focus on 
the compulsory and coherence miss that is more dependent to the program behavior. In the report, we 
focus on x86_64 case only. 
 
 
Table C3. Experimental setup 
 
A. Case Study: Multicore x86 Systems 
Figure C9 and figure C10 show the behavior of multi-threaded applications running on systems 
with different number of cores. The number of threads of each program is configured to the number 
of simulated cores. The coherence miss is measured by GEMS, and we apply different value of 
CRR from 2 to 64. 
總計畫：在多核平台上之多核虛擬環境研發 
 97 
 
Figure C9. Coherence misses of benchmarks running on different number of cores 
 
 
總計畫：在多核平台上之多核虛擬環境研發 
 99 
 
Figure C11. Comparing the results with coherence misses 
 
III. Subproject 4 
A. 實驗設定 
    A-1 測詴程式 
我們選定 SPEC OMP 程式組裡的三隻程式：wupwise、equake、swim，及
PARSEC 程式組裡的三隻程式：dedup、x264、ferret 來進行測詴。SPEC OMP
程式組裡的程式是利用 OpenMP API 所平行的程式，其程式具有資料層級平行
(data-level parallel)的特性，而選定的 PARSEC 程式則是具有任務層級平行
總計畫：在多核平台上之多核虛擬環境研發 
 101 
量對於偵測多執行緒程式相位之精確度。圖 D11 為利用此三種方法在評估機器
上只考慮在取樣機器上找到的特徵點而推估出來的整體程式效能結果與實際程
式執行結果比對之錯誤率，Y 軸是錯誤率，X 軸是我們選定之目標程式，其中
wupwise、equake、swim 是選自 SPEC OMP 的程式，而 dedup、x264、ferret 則
是選自 PARSEC 的程式。圖 D12 [15]顯示了一些與我們選定的 SPEC OMP 程式
共享資料量有關的資訊，可以看到執行緒間有關共享資料的互動幾乎不存在在
equake、wupwise、swim 中，因此可以看到 equake、wupwise、swim 程式的
SBBV+CCV 結果並沒有什麼幫助。然後在任務平行層級的程式 dedup、x264、
ferret 中，除了 x264 因為有一半的執行緒沒有什麼溝通外，dedup 及 ferret 均有
明顯的錯誤率下降。由實驗結果可以得知我們提出的 CCV 結合 SBBV 能夠有效
地幫助執行緒間共享資料較多的程式。 
 
 
 
 
圖 D11. CCV 之實驗結果 
圖 D12. OMP 程式中資料共享情形 [15] 
總計畫：在多核平台上之多核虛擬環境研發 
 103 
 
六、 參考文獻 
[A-1] O. Certner, Zheng Li, A. Raman, O. Temam, “A Very Fast Simulator for Exploring the Many-Core 
Future” in Proceeding of the Parallel & Distributed Processing Symposium (IPDPS), 2011.   
[A-2] J. Chen, L. Dabbiru, D. Wong, M. Annavaram, M. Dubois, "Adaptive and Speculative Slack 
Simulations of CMPs on CMPs," Microarchitecture, IEEE/ACM International Symposium on, pp. 523-534, 
2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, 2010  
[A-3] J. Donald, M. Martonosi, “An Efficient, Practical Parallelization Methodology for Multicore 
Architecture Simulation”, in Computer Architecture Letters, 2006 
[A-4] Z. Wang, R. Liu, Y. Chen, X. Wu, H. Chen, W. Zhang, and B. Zang. “COREMU: a scalable and 
portable parallel full-system emulator”, in Proceedings of the 16th ACM symposium on Principles and 
practice of parallel programming (PPoPP '11). 
[A-5] D.A. Penry, D. Fay, D. Hodgdon, R. Wells, G. Schelle, D.I. August, D. Connors, “Exploiting 
parallelism and structure to accelerate the simulation of chip multi-processors”, in The Twelfth International 
Symposium on High-Performance Computer Architecture, 2006.  
[A-6] S. Kanaujia, I. E. Papazian, J. Chamberlain, and J. Baxter, “FastMP: A multi-core simulation 
methodology,” in MOBS 2006: Workshop on Modeling, Benchmarking and Simulation, June 2006 
[A-7] J. Miller, H. Kasture, G. Kurian, N. Beckmann, C. Gruenwald III, C. Celio, J. Eastep, and A. Agarwal, 
“Graphite: A distributed simulator for multicores,” Cambridge, MA, USA, Tech. Rep. 
MIT-CSAIL-TR-2009-056, November 2009 
[A-8] D. Chiou, D. Sunwoo, J. Kim, N. A. Patil, W. Reinhart, D. E. Johnson, J. Keefe, and H. Angepat, 
“FPGA-Accelerated Simulation Technologies(FAST): Fast, Full-System, Cycle-Accurate Simulators,” in 
MICRO ’07: Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture, 
2007, pp. 249–261. 
[A-9] C. Bienia, S. Kumar, J. P. Singh, and K. Li, “The PARSEC benchmark suite: Characterization and 
architectural implications,” in Proc. of the 17th International Conference on Parallel Architectures and 
Compila-tion Techniques (PACT), October 2008. 
[A-10] A. Patel, F. Afram, S. Chen, and K. Ghose, “MARSSx86: A Full System Simulator for x86 CPUs,” in 
Design Automation Conference 2011 (DAC’11), 2011. 
[A-11] J. Chen, M. Annavaram, and M. Dubois, “SlackSim: A Platform for Parallel Simulations of CMPs on 
CMPs,” SIGARCH Comput. Archit. News, vol. 37, no. 2, pp. 20–29, 2009. 
[A-12] M. Yourst. PTLsim: A cycle accurate full system x86-64 microarchitectural simulator. In Proceedings 
of 
the 2007 IEEE International Symmposium on Performance Analysis of Systems and Software (ISPASS), 
pages 23{34. Apr. 2007. 
[A-13] J. Wawrzynek, D. Patterson, M. Oskin, S.-L. Lu, C. Kozyrakis, J. C. Hoe, D. Chiou, and K. Asanovic. 
RAMP: Research accelerator for multiple processors. IEEE Micro, 27(2):46{57, Mar. 2007. 
[A-14] M. Pellauer, M. Adler, M. Kinsy, A. Parashar, and J. Emer. “HAsim: FPGA-based high-detail 
multicore 
simulation using time-division multiplexing”. In Proceedings of the 17th International Symposium on 
總計畫：在多核平台上之多核虛擬環境研發 
 105 
Scheduling Using Tasks with Synthetic Periods.” In the Proceedings of IEEE Real-Time Systems Symposium, 
Cancun, Mexico, Dec. 2003.  
[B-9] Chi-Sheng Shih, Sathish Gopalakrishnan, Phanindra Ganti, Marco Caccamo, and Lui Sha, 
“Template-Based Real-Time Dwell Scheduling with Energy Constraint.” In Proceedings of the IEEE 
Real-Time and Embedded Technology and Application Symposium, 2003.  
[B-10] Chi-Sheng Shih, Sathish Gopalakrishnan, Phanindra Ganti, Marco Caccamo, and Lui Sha, 
“Synthesizing Task Periods for Dwells in Multi-Function Phased Array Radars.” In the Proceedings of the 
IEEE Radar Conference 2004.  
[B-11] Yi-Hua Tsai, Jian-Jia Chen, Tei-Wei Kuo, and Chi-Sheng Shih, “Client and Server Mobility for WEB 
Applications.” In the Proceedings of the Sixth International Conference on Information Integration and Web 
Based Applications & Services (iiWAS2004), 2004.  
[B-12] Li Chia and Chi-Sheng Shih, Template-based Run-time reconfiguration scheduling for partial 
reconfigurable SoC, in Proceedings of the 13th IEEE International Conference on Embedded and Real-Time 
Computing Systems and Applications (RTCSA 2007), Daegu, Korea, August 21-23, 2007.  
[B-13] Ya-Shu Chen, Chi-Sheng Shih and Tei-Wei Kuo, Dynamic Task Scheduling And Processing Element 
Allocation For Multi- Function SOCs, in Proceedings of 13th IEEE Real-Time and Embedded Technology 
and Applications Symposium, Bellevue, WA, United States, April 3 - April 6, 2007.  
[B-14] Jian-Jia Chen, Chuan-Yue Yang, Tei-Wei Kuo, and Chi-Sheng Shih, Energy-efficient Real-time Task 
Scheduling in Multiprocessor DVS Systems, In Proceedings of the 12th Asia and South Pacific Design 
Automation Conference, pp. 342 - 349, Yokohama, Japan, February 2007.  
[B-15] Chun-Nan Chou, Yi-An Chen, and Chi-Sheng Shih, Genetic-based Approach for Scheduling, 
Allocation, and Mapping for HW/SW Co-Design with Dynamic Allocation Threshold, in the WIP session of 
the 12th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications 
2006 (To be appeared in NICTA technical report series).  
[B-16] Chun-Nan Chou, Yi-An Chen, and Chi-Sheng Shih, Genetic-based Approach for Scheduling, 
Allocation, and Mapping for HW/SW Co-Design. In Proceedings of VLSI/CAD 2006.  
[B-17] Pei-Hsuan Tsai, H. C. Yeh, C. Y. Yu, P. C. Hsiu, Chi-Sheng Shih, and J. W. S. Liu, Compliance 
Enforcement of Temporal and Dosage Constraints. In Proceedings of IEEE Real-Time Systems Symposium, 
Dec. 2006.  
[B-18] Mark Liao, Jane Liu, and Chi-Sheng Shih, Smart Pantries for Homes, In Proceedings of 2006 IEEE 
International Conference on Systems, Man, and Cybernetics, Oct. 8 - Oct. 11, 2006, Taipei, Taiwan.  
[B-19] Han-Chun Yeh, Pi-Cheng Hsiu, Pei-Hsuan Tsai, Chi-Sheng Shih, and Jane Liu, APAMAT: A 
Prescription Algebra for Medication Authoring Tool, in Proceedings of 2006 IEEE International Conference 
on Systems, Man, and Cybernetics, Oct. 8 - Oct. 11, 2006, Taipei, Taiwan.  
[B-20] Han-Chun Yeh, Pi-Cheng Hsiu, Pei-Hsuan Tsai, Chi-Sheng Shih, and Jane Liu, Integration Framework 
for Medication-Use Process, in Proceedings of 2007 IEEE International Conference on Systems, Man, and 
Cybernetics, Oct. 7 - Oct. 10, 2007, Montreal, Canada.  
[B-21] Jian-Jia Chen, Jun Wu, and Chi-Sheng Shih, Approximation algorithms for scheduling real-time jobs 
with multiple feasible intervals, in the Real-Time Systems Journal, Volume 34, Number 3 / November, 2006, 
Pages: 155-172.  
總計畫：在多核平台上之多核虛擬環境研發 
 107 
[C-15] P. Bohrer, J. Peterson, M. Elnozahy, R. Rajamony, A. Gheith, R. Rockhold, C. Lefurgy, H. Shafi, T. 
Nakra, R. Simpson, E. Speight, K. Sudeep, E. Van Hensbergen, and L. Zhang, “Mambo: a full system 
simulator for the powerpc architecture,” SIGMETRICS Perform. Eval. Rev., vol. 31, no. 4, p. 8–12, 2004. 
[C-16] “Android emulator,” http://developer.android.com/guide/developing/tools/emulator.html. 
[C-17] “Openmoko,” http://www.openmoko.org. 
[C-18] J. Hennessy, D. Patterson, and A. Arpaci-Dusseau, Computer architecture: a quantitative approach, ser. 
The Morgan Kaufmann Series in Computer Architecture and Design. Morgan Kaufmann, 2007. 
[C-19] L. Benini, D. Bertozzi, A. Bogliolo, F. Menichelli, and M. Olivieri, “Mparm: Exploring the 
multi-processor soc design space with systemc,” VLSI Signal Processing, vol. 41, no. 2, pp. 169–182, 2005. 
[C-20] M. M. K. Martin, D. J. Sorin, B. M. Beckmann, M. R. Marty, M. Xu, A. R. Alameldeen, K. E. Moore, 
M. D. Hill, and D. A. Wood, “Multifacet’s general execution-driven multiprocessor simulator (gems) toolset,” 
SIGARCH Comput. Archit. News, vol. 33, pp. 92–99, November 2005. [Online]. Available: http: 
//doi.acm.org/10.1145/1105734.1105747 
[C-21] C. Keenan, HP-UX CSE: official study guide and desk reference, ser. HP Professional Series. Prentice 
Hall PTR, 2004. 
[C-22] N. Nethercote and J. Seward, “Valgrind: a framework for heavyweight dynamic binary 
instrumentation,” in Proceedings of the 2007 ACM SIGPLAN conference on Programming language design 
and implementation, pp. 89–100. [Online]. Available: http://doi.acm.org/10.1145/1250734.1250746 
[C-23] A. Jaleel, R. S. Cohn, C. keung Luk, and B. Jacob, “Cmp$im: A pin-based on-the-fly multi-core cache 
simulator,” in Proceedings of The Fourth AnnualWorkshop on Modeling, Benchmarking and 
Simulation(MoBS), 2008, pp. 28–36. 
[C-24] “Pin,” http://www.pintool.org. 
[C-25] R. Uhlig and T. N. Mudge, “Trace-driven memory simulation: A survey,” ACM Comput. Surv., vol. 
29, no. 2, pp. 128–170, 1997. 
[C-26] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta, “The splash-2 programs: characterization 
and methodological considerations,” in Proceedings of the 22nd annual international symposium on Computer 
architecture, ser. ISCA ’95. New York, NY, USA: ACM, 1995, pp. 24–36. [Online]. Available: 
http://doi.acm.org/10.1145/223982.223990 
[C-27] J. B. Rothman and A. J. Smith, “Analysis of shared memory misses and reference patterns,” in ICCD, 
2000, pp. 187–198. 
[C-28] C. Bienia and K. Li, “Parsec 2.0: A new benchmark suite for chip-multiprocessors,” in Proceedings of 
the 5th Annual Workshop on Modeling, Benchmarking and Simulation, June 2009. 
[D-1] Python http://python.org.tw/ 
[D-2] Gnuplot http://www.gnuplot.info/. 
[D-3] The GTK+ Project http://www.gtk.org/ 
[D-4] Glade - A User Interface Designer http://glade.gnome.org/ 
[D-5] SPEC OMP 2001 http://www.spec.org/omp/ 
[D-6] OpenMP  http://openmp.org/wp/ 
[D-7] S. Eranian. Perfmon2: A ﬂexible performance monitoring interface for linux. 
[D-8] T. Sherwood and B. Calder. Time varying behavior of programs. Technical Report UCSD-CS99-630, 
總計畫：在多核平台上之多核虛擬環境研發 
 109 
計畫成果自評： 
In this project, we have proposed a parallel simulation platform which takes advantage from the 
computation power provided by the multicore. The parallel simulation platform has achieved high 
simulation performance by utilizing the statistical sampling technique to simulate only the critical 
samples with the cycle-accurate simulation and fast forward between irrelevant sample periods. Each 
subproject has successfully accomplished their preset goals. The simulation platform could help not only 
accelerating the development process of new architecture ideas but also shortening the critical 
time-to-market for the related industry.  
 
 
 The International Symposium on Embedded Multicore Systems-on-chip (MCSoC) 
 International Workshop on Parallel Software Tools and Tool Infrastructures (PSTI 2010)  
The conference and the workshops were very well attended and the conference was a 
great success.  
 
 
  
99 年度專題研究計畫研究成果彙整表 
計畫主持人：游本中 計畫編號：99-2220-E-001-001- 
計畫名稱：在多核平台上之多核虛擬環境研發--總計畫(2/2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 3 3 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
