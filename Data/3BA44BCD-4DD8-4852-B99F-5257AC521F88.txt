  i
中文摘要 
語者辨認（包含語者識別與語者驗證）系統在公共電話網路中，通常會遇
到話筒或通道不匹配和辨認語料不足的問題。為增進語者辨認系統之強健性，
我們在本計畫中，研究如何利用先驗知識做補償與利用韻律訊息輔助，抵抗話
筒或通道不匹配問題。研究成果包括期刊論文 3 篇（另外投稿中有兩篇），會議
論文 17 篇，參加 ISCSLP-SRE 2006 語者驗證評比，獲得不錯成績，與建立口語
對話系統與語者驗證系統，以下分年敘述研究內容： 
在第一年研究語者識別問題，提出一融合下層聲學與上層韻律訊息之架
構，首先利用 (1) 最大相似先驗知識內插法 (maximum likelihood-a priori 
knowledge interpolation, ML-AKI)方法做話筒聲學特性估計與補償，並以(2)最小
錯誤鑑別式法則(Minimum Classification Error, MCE)訓練語者模型，拉大不同語
者模型間分數的距離，以得到更精確的語者模型，與利用(3)韻律訊息特徵分析
(eigen-prosody analysis, EPA)為輔助，將所有語者投影至緊密的特徵韻律訊息空
間，量測語者間的距離，最後利用(4)線性迴歸的方式融合聲學與韻律模型分數
得到辨識的結果。 
第二年研究語者驗證問題，最重要的是如何有效改善通道及話筒不匹配的
問題，因此針對通道不匹配及訓練語料、測試語料有限問題，在分數領域，融
合聲學層次與韻律層次之語者訊息，改善語者驗證系統之效能，以建立強健性
語者驗證系統。我們將語者確認問題轉換為類似文件擷取（document retrieval）
之方式提出(1)Latent prosody analysis(LPA)方法，以 probabilistic latent semantic 
analysis（PLSA）分析語者的韻律特徵訊息，透過減少語者模型參數量，藉此得
到較可靠的特徵韻律訊息，來輔助通道不匹配對語者驗證系統效能之影響，與
利用(2)Latent Acoustics Analysis(LAA)方法，分析語者之間聲學訊息，找出可以
有效用來區分語者的特徵訊息，來輔助傳統 GMM 架構下之效能改善。 
  iii
Abstract 
Unseen handset mismatch is the major source of performance degradation for 
speaker recognition (including speaker identification and verification) in 
telecommunication environment. To compensate the mismatch problems with few 
available train/test data, a priori handset/channel knowledge and prosodic information 
are explored in this project to assist the handset/channel mis-match distortion. The 
outcomes of this project include (1) 3 publised and two submitted journal papeprs, (2) 
17 conference pappers, (3) particating the ISCSLP-SRE 2006 with good results and (4) 
implementing several spoken dialog and speaker verification prototypes. 
In the first year, a maximum likelihood a priori knowledge interpolation (ML-AKI) 
and an eigen-prosodic analysis (EPA) approaches were proposed and fused together for 
robust speaker indentification. The experimental results on HTIMIT showed that the 
ML-AKI+EPA+MCE+MAP-GMM/CMS fusion approach achieved 79.3% average 
speaker identification accuracy, which is much better than the traditional 
MAP-GMM/CMS-based baseline (60.2%). Moreover, the average speaker identification 
rates of the nine unseen handset turns in the level-one-out experiment could also be 
increased from 58.3% (MAP-GMM/CMS) to 74.6% 
(ML-AKI+EPA+MCE+MAP-GMM/CMS), respectively. 
In the second year, a latent prosody analyses (LPA) and a latent acoustics analysis 
(LAA) approaches were proposed and fused together for robust speaker verification. 
The proposed methods are evaluated on the standard one speaker detection task of the 
2001 NIST Speaker Recognition Evaluation Corpus where only one 2-minute training 
and 30-second trial speech (in average) are available. Experimental results have shown 
that the proposed LPA+LAA fusion approach could improve the equal error rates (EERs) 
  v
 
目錄 
中文摘要 ............................................................. i 
Abstract ........................................................... iii 
目錄 ................................................................. v 
第一章 簡介 .......................................................... 1 
1.1 研究動機 ..................................................... 1 
1.2 研究背景與方向 ............................................... 2 
1.3 研究方法 ..................................................... 6 
1.4 論文章節之概要 .............................................. 14 
第二章 語者驗證基本系統 ............................................. 15 
2.1 GMM語者驗證的基本系統 ....................................... 15 
2.1.2 MVA前端處理 ........................................... 17 
2.1.3 GMM語者模型 ........................................... 18 
2.1.4 MAP-GMM語者模型 ....................................... 20 
2.2 測試正規化 .................................................. 23 
2.3 語者驗證系統基本系統實驗與討論 .............................. 25 
2.3.1 NIST 2001 實驗語料 .................................... 25 
2.3.2 語者驗證系統效能評估 .................................. 25 
2.3.2.1. 相等錯誤率 ..................................... 26 
2.3.2.2. 偵測錯誤取捨曲線 ............................... 26 
2.3.2.3. 決策代價函數 ................................... 26 
2.3.3 MAP-GMM實驗內容與結果 ................................. 27 
2.3.4 測試正規化之實驗內容與實驗結果 ........................ 28 
2.3.5 短程音高參數(PITCH/ENERGY)之語者驗證系統 .............. 31 
2.3.6 長程韻律參數(PROSODY)之語者驗證系統 ................... 32 
韻律狀態序列求取與正規化 ................................ 32 
以向量量化為基礎之韻律訊息模型 .......................... 35 
標記韻律狀態序列 ........................................ 36 
語者韻律模型 ............................................ 36 
2.4 語者驗證系統效能評估 ........................................ 37 
2.4.2 相等錯誤率 ............................................ 38 
2.4.3 偵測錯誤取捨曲線 ...................................... 38 
2.4.4 決策代價函數 .......................................... 39 
2.5 本章結論 .................................................... 39 
  vii
4.1 韻律模型與自動韻律狀態標記 .................................. 80 
4.1.1 韻律特徵參數求取與正規化 .............................. 80 
4.1.2 以VQ為基礎之韻律訊息模型 .............................. 81 
4.2 Latent Prosody Analysis (LPA) ............................... 83 
4.2.1 語者—韻律關鍵詞關係矩陣 .............................. 84 
4.2.2 以PLSA為基礎之韻律訊息模型平滑化 ...................... 85 
4.2.2.1. Speaker-specific PLSA N-gram Smoothing ......... 86 
4.2.2.2. Speaker-wide PLSA Latent Prosody Analysis ...... 87 
4.3 融合LPA和CMS方法之語者驗證實驗 .............................. 88 
4.3.1 實驗條件 .............................................. 88 
4.3.2 MAP-GMM and T-norm .................................... 89 
4.3.3 Pitch/Energy GMMs ..................................... 89 
4.3.4 Prosody bi-gram speaker models ........................ 89 
4.3.5 Speaker-specific PLSA Prosody bi-gram smoothing ....... 90 
4.3.6 Speaker-wide PLSA Latent prosody space analysis ....... 91 
4.3.7 System fusion ......................................... 92 
4.4 本節結論 .................................................... 94 
4.5 以特徵語者訊息分析為基礎之語者驗證系統 .......................... 94 
4.5.1 特性空間 .............................................. 96 
4.5.1.1. 語者特定模型 ................................... 96 
4.5.2 空間建立 .............................................. 97 
4.5.2.1. UBM tokenization-based LAA ..................... 97 
4.5.2.2. MAP adaptation-based LAA ....................... 98 
4.5.3 計算特性基底之空間 .................................... 99 
4.6 語者驗證分數之測量 ......................................... 100 
4.7 以特徵語者訊息分析為基礎之實驗 ............................. 101 
4.7.1 實驗條件 ............................................. 101 
4.7.2 實驗內容與結果 ....................................... 102 
4.7.2.1. UBM tokenization-based LAA .................... 102 
4.7.2.2. Map Adaptation-based LAA ...................... 104 
4.8 結論 ....................................................... 109 
4.9 融合LPA和LAA方法 ........................................... 109 
4.10 融合特徵韻律訊息和特徵語者訊息 ............................ 110 
4.11 融合LPA和LAA之語者驗證實驗 ................................ 111 
4.11.1 實驗條件 ............................................ 111 
4.11.2 實驗結果 ............................................ 111 
4.11.3 實驗討論 ............................................ 113 
4.12 結論與未來展望 ............................................ 114 
  1
第一章 簡介 
1.1 研究動機 
在公共電話網路中，語者驗證系統需要考慮系統訓練環境與實際測試環境
不匹配（environment mismatch）的影響，這些不匹配的因素通常包含：轉換器
(transducer)的響應、通道（channel）特性、話筒(handset)特性、以及雜訊（noise）
干擾等等。 
 
從前人的研究中，如 SuperSID project【1】，得知結合不同層次的語者訊息，
可以減低受到通道、話筒不匹配或雜訊因素的等影響，這些不同層次語者訊息包
含(如圖 1.1)： 
z 先天口腔器官的結構影響：聲學的訊息，如鼻音、呼吸聲等。 
z 個人的特性和父母的影響：韻律的訊息，如語者說話的節奏、速度、聲調和
聲音的大小等。 
z 社會地位，教育和出生地之影響：語者說話句子的語義、發音、對話和慣用
語的等訊息。 
 
由圖 1.1 中可以得知越下層的語者訊息受到先天發音器官的影響較大，而越
上層的語者訊息則受到後天學習的影響較大，且越往上層的語者訊息，通常越
難自動取得，所以通常需要大量的訓練/測試語料。由前人的研究中得知結合上
層和聲學層次語者訊息的方法，在減輕話筒不匹配的影響方面可以得到不錯的
效果，但前提是能獲得大量的訓練/測試語料下，然而在實際應用上，語料通常
是有限的，所以在本計畫中，我們將嘗試在有限的語料下，結合聲學和韻律兩
方面的語者訊息來減輕通道、話筒不匹配的所帶來的影響。 
  3
 
其中 CMS 和 SBR 技術是藉由移除被話筒不匹配影響的語音之長程平均值
(long-term average)，達到補償話筒的不匹配。Handset detector-based 的方法，是先
鑑別話筒的類型，如判別其是碳盒式(carbon)或是電子式(electronic)的話筒，然後
利用事先訓練好的已知話筒之特性去移除話筒偏移量，如圖 1.2 所示。 
 
 
圖1.2： 話筒偵測方法的架構圖。 
 
然而 CMS 和 SBR 不單只是移除話筒的特性，而且通常也會把語者的特性
移除。而基於話筒偵測(handset detector-based)的方法，對於測試語音是來自未
知話筒時，它就只能從已知話筒集合之中選擇出一個最相似的話筒，或者是當
作未知的話筒直接把它拒絕掉。 
 
在韻律訊息層次上，常使用的模型分為： 
z 短程(short term)韻律模型上，通常使用 Gaussian mixture models(GMMs)【8】
去表現如音高和能量分佈，音高和能量的斜率和音高和能量的持續時間等韻
律特徵參數軌跡。 
  5
而使用 GMMs 統計韻律訊息時，一般只能補捉到音高與能量變化等短程的
韻律訊息，所得到的改善的幅度有限，而使用 DHMM 和 N-gram 的方法，雖然
可以補捉到較長程(long term)韻律訊息變化，但通常得使用大量的訓練/測試語
料。 
 
一般的研究通常是將短程語音特徵參數與長程語音特徵參數各自經由不同之
方式求取參數，以不同之系統訓練模型，每個系統各自執行驗證程序，所得到之
系統效能亦不相同，最後則依據經驗法則以權重合的方式將各個系統所得到的辨
識分數加總，希望能夠以截長補短的方式改善系統效能如圖 1.5。不過權重的問題
卻不易決定，往往需要依據不同的語料庫給予不同的權重。 
 
圖1.5： 以權重方式整合系統。 
 
而現今最常使用之方法是將短程頻譜參數、短程音高參數、長程韻律參數分
別訓練出各自的模型，不同特性的語者資訊經由不同的系統展現，再以權重合的
方式將各個系統所得到的辨識分數加總，希望能以截長補短的方式改善系統效
能。不過權重的問題卻不易決定，往往需要依據不同的語料庫給予不同的權重。
  7
 
圖1.6： ML-AKI的架構圖 
 
ML-AKI 詳細之作法如下： 
(1) 利用 maximum likelihood linear regression (MLLR)方法先求得每一
種已知聽筒模型與註冊聽筒模型之間的轉換函數作為先驗知識。 
(2) 測試時，利用此轉換函數之集合，利用 ML 法則內插先驗知識，再
使用 Expect Maximum(EM)演算法去求取每一個先驗知識內插的最
佳權重，以得到測試聽筒模型與註冊聽筒模型之間的轉換函數。 
(3) 利用求得的未知話筒之轉換函數來調適語者模型。 
 
EPA 的作法，則是將問題想成為類似文件擷取（doccument retrieval）的問題，
首先把語者的韻律特徵參數自動標記成韻律狀態序列，將他當作一虛擬文件，再
運用 latent smantic analysis(LSA)的觀念作分析，建立一個特徵韻律訊息語者空間，
以表現語者的分佈(constellation)，最後利用韻律訊息關鍵字作詢問(query)，以擷取
最相關註冊的語者，其方塊圖如圖 1.7 所示。 
 
1h
AKI-UBM 
hˆ2
α
2h
O
 
 
Maximum 
likelihood 
interpolation
weights 
estimator 
1α
Nα
Nh
+
×
×
×
A priori knowledge
  9
 融合的作法，則以圖 1.8 來說明。進入的未知語音語音先求取梅爾倒頻譜參
數，分別進入傳統做法 MAP-GMM/CMS 求取辨認分數 S1 和本計畫所提出的
ML-AKI 方法求取辨認分數 S2，並經過融合得到分數 S4，接下來在下層中利用進
入語音求取韻律特徵參數，利用本計畫所提出的 EPA 的方法求得辨認分數 S3，最
後融合 S4 和 S3 之後的分數來做辨認，得到最後的辨認結果。 
 
 
圖1.8： 融合MAP-GMM/CMS、ML-AKI、EPA方法的架構圖 
 
 第二年的研究方法，在韻律層次中，許多的潛在因素影響著語者韻律的動態
資訊，例如語者不同說話的方式、不同的說話內容，甚至是語者說話的情緒，
種種的因素造成韻律訊息參數出現的組合非常的龐大與複雜，然而在現實生活
的應用之中，我們很難取得足夠龐大的語料，來獲取足夠的語者韻律訊息，因
此在本計畫中提出 Latent prosody analysis(LPA)方法，探討如何在有限語料的情
形之下，以 probabilistic latent semantic analysis（PLSA）分析，得到較可靠的特徵
韻律訊息，來輔助通道不匹配對語者驗證系統效能之影響。 
 
Recognized 
Speaker
MFCC Fusion
S2
Prosod
S1
S3
Fusion Decision 
Speech 
Input 
S4
MAP-GMM/ 
CMS 
ML-AKI 
EPA 
  11
4. 將自動標記好的韻律狀態序列，統計出韻律序列組合的 bi-gram 模型，當成語
者模型。 
5. 利用 speaker-specific PLSA(見圖 1.10)/ speaker-wide PLSA(見圖 1.11)，分
析、降維，建構出特徵韻律語者空間，達到語者的韻律 bi-gram 模型進行
平滑化之目的。 
6. 將測試語者的測試語句轉成韻律狀態序列，並利用此韻律狀態序列及語者
韻律 bi-gram model，計算此語者辨認分數。 
 
 
圖1.10： Speaker-specific PLSA-based dimension reduction， id 、 iw 、 z 所代表的分別是
document、keyword、latent prosody factors。 
 
 
 
P(w,d) 
PLSA P(d|z) 
P(z)  
P(w|z)
bi-gram model 
Dimension
reduction
 
P'(w,d) 
Reconstruction 
Smoothed 
bi-gram model
  13
得，而是包含頻譜、音高與韻律參數之資訊。以新的韻律狀態序列對頻譜、音高
與韻律參數重訓練模型，再對測試語料執行一次維特比演算法。則整個語者驗證
程序將同時考慮頻譜、音高與韻律資訊。 
如此一來，不同性質之特徵參數無論是利用長程韻律參數為輔助，將短程頻
譜參數與短程音高參數分類，或是利用短程頻譜參數與短程音高參數為輔助重新
求取韻律狀態序列，都充分利用了相互連結之關係。 
 
圖1.12： 基於韻律與頻譜聯合模型之強健性語者驗證。 
 
 
 
 
 
  15
第二章 語者驗證基本系統 
 
本章首先介紹實驗所使用之基本語者驗證系統，接著利用電話線語料庫
NIST2001【11】驗證如何在少量的訓練、測試語料且通道不匹配的情形之下維持
語者驗證的強健性。 
 
在 2.1 節中介紹現今最常被應用在處理。在 2.2 節中介紹本計畫所使用的基本
語者驗證系統架構，包括特徵參數萃取、特徵參數處理、驗證法則、語者模型
(Gaussian mixture models, GMM)【12】，及基於最大事後機率(maximum a posteriori, 
MAP)做調適之語者模型(MAP-Adapted GMM)【13】。在 2.3 節中介紹現今最常被
應用在語者驗證系統裡的分數正規劃技術－測試正規化(Test Normalization)【14】。
2.4 節中介紹語料庫 NIST2001，進行語者驗證基本系統實驗，並針對結果作出結
論。 
 
2.1 GMM語者驗證的基本系統 
 
本篇論文所使用之語者驗證 (speaker verification) 系統如圖 2.1 所示，在本節
我們將聲學整個語者辨認基本系統，分成幾個區塊來描述，包括： 
z 特徵參數萃取(feature extraction) — 將錄音得到語音檔(waveform files)，分成
訓練語料與測試語料兩部分，求出其特徵參數。本篇論文的特徵參數是使用
30ms 為一音框及 10ms 之音框位移求得的 38 維 mel-frequency cepstral 
coefficiences (MFCCs)，包括 12 維 MFCCs、12 維 Δ-MFCCs,、12 維
Δ2-MFCCs、1 維 Δ-log energy 和 1 維 Δ2- log energy。 
  17
背景模型。語者不特定模型由全部語者的語料訓練得到。 
z GMM 與 MAP-GMM 語者模型訓練 — 使用訓練語料的特徵參數進行模型
的訓練，而產生 GMM 語者模型；或是先訓練一通用語者背景模型 (universal 
background model, UBM)，再利用 Maximum a posteriori (MAP)調適出每位語
者的 MAP-GMM 語者模型，以彌補因語料不足而缺少的某些聲學特性。 
 
2.1.2 MVA前端處理 
 
    這個 MVA 方法其步驟分成三個部分，分別使用了平均消去法（Mean 
Subtraction，MS）、變異數正規化法（Variance Normalization，VN）及 ARMA 濾
波器，即可達到語音強健性的效果。圖 2.2 為 MVA 流程圖。 
 
 
圖2.2： 對語音特徵參數做MVA之流程圖。 
 
假設 ,t dX 為一連串語音信號抽取之特徵參數向量序列，其中 t 為時間，d 為維
度，首先經過平均消去法處理過後的特徵參數向量序列可以（2.2）及（2.3）式表
示： 
 
,
1
1 T
d t d
t
X
T
μ
=
= ∑                       （2.2） 
'
, ,t d t d dX X μ= −                       （2.3） 
 
MS  VN ARMA 
 Filter 
Feature 
Extraction
Robust
Feature
  19
 
 
圖2.3： GMM的表示圖。 
 
  1
(  |   )  (  )
M
i i
i
p x p b xλ
=
= ∑K K                  (2.7) 
其中 xG是一個 D 維度的隨機向量，密度函數 ( ), 1,...,ib x i M=G 和 , 1,...,ip i M= 為混合
權重，而每一個密度函數的形式是一個 D 維度的高斯函數。 
 
-1
1/ 2/ 2
1 1(  )  exp - (  -  ) (  -   )
2( 2 ) | |
t
i i iiD
I
b x x xμ μπ
⎧ ⎫= ⎨ ⎬⎩ ⎭∑∑
K K K K K
     (2.8) 
其中 iμG 為平均值向量和 iΣ 為共變異數矩陣，且混合權重要滿足 1 1Mi ip=Σ = 。 
 
    完整的高斯混合密度函數可以使用所有的密度函數的平均值向量、共變異數
矩陣和混合權重等參數描述，這些參數的集合可以由下面的符號表示，如式(2.9)： 
 
{ }    ,   ,    ,     1, ,i i ip i Mλ μ= =∑K "             (2.9) 
每一個語者皆用其相關模型λ表示其 GMM 模型。 
 
    在本篇論文中是利用向量量化(Vector Quantization, VQ)做分群動作，將聲學特
性相近的歸成一類，當作 GMM 參數的初始值，再採用 Expectation- Maximization 
Algorithm (EM algorithm)，訓練出最後的語者模型。 
 
  21
  
圖2.4： 使用MAP調適語者模型的示意圖。(a) UBM的高斯混合模型及調適語料。(b)
使用調適語料的統計量和UBM混合模型衍生出調適後的語者高斯混合模型。 
 
MAP 調適含兩個步驟，類似 EM 演算法，第一步與 EM 演算法的取期望值
步驟完全相同，主要是計算出語者訓練語料在 UBM 中對每一個混合高斯
(mixture Gaussian)的充份統計量(sufficient statistics)。第二步是調適步驟，我們
使用一個與語料相依的混合係數(data-dependent mixing coefficient)將新估算的
充份統計量與 UBM 混合參數的舊充份統計量做結合。這個混合係數的設計是取
決於語者的語料對混合高斯的計數，當混合高斯有較高的計數時，則最後估算
的參數則比較依賴新的充份統計量；反之，當混合高斯有較低的計數時，則最
後估算的參數比較依賴舊的充份統計量。 
 
  1
( )(  |   )  
( )
i i t
r i M
j j tj
w p xP i x
w p x=
= ∑                (2.10) 
 
然後我們為了得到權重、平均值和變異數等參數，使用 ( | )r tP i x 和 tx 計算充份統計
量，如式(2.11)、(2.12)和(2.13)： 
 
  23
2.2 測試正規化 
 
    對語者辨識系統而言，每句測試語料的特徵參數 { }1,..., TX x x= G G ，都可以與經
由訓練語料所訓練出的 GMM 語者模型λs{λ1,λ2,…,λN}，語者背景模型
background speaker model λb，計算出相對應的 log likelihood score，以數學形式來
表示即 
 ( )  log  ( |  ) -  log  ( |  )s bS X X P Xλ λ= Ρ             (2.18) 
 
    測試正規化的原理是利用一組同儕語者模型(cohort model set)，同儕語者模型
指的就是一群相似於目標語者模型(target speaker model)的語者模型，目的是為了
估計出，相對應於每位不同的目標語者，相似於目標語者的冒充語者(impostor)，
測試正規化之定義如式(2.19) 
 
 
-
s I
T
I
S
S λ
μ
σ=                          (2.19)       
其中
s
Sλ 為測試語料與語者模型 sλ 所計算的 log likelihood score， Iμ 與 Iσ 分別代表
測試語料相對於同儕語者模型分數的平均值與變異數， TS 為經過測試正規化後的
分數。 
 
    測試正規化中減去 Iμ ，不僅可以將冒充語者分數的分佈之中心移至原點，同
時也可以拉大目標語者與冒充語者之分數的分佈，除以 Iσ 可以將冒充語者分數的
分佈之標準差限定為一，因此進而提升辨識率。 
 
  25
2.3 語者驗證系統基本系統實驗與討論 
 
2.3.1 NIST 2001 實驗語料 
 
    本計畫中所使用的語音資料庫 NIST 2001 是由美國國家標準局(National 
Institute of Standards and Technology , NIST)所發行的語料庫，其詳細資料如下： 
 
z 語者數目：174 (100 位男性和 74 位女性) 
z 訓練語料：每人約 2 分鐘 
z 測驗語料：每人約 30 秒 
z 取樣頻率：8KHz 
z 檔案格式：PCM 
 
2.3.2 語者驗證系統效能評估 
 
語者驗證的錯誤率有兩種：一種是錯誤拒絕率(False Rejection Rate)，即正確
語者的分數小於門檻值造成拒絕的錯誤率。另一種是錯誤接受率(False Acceptance 
Rate, FA)，及仿冒語者的分數高於門檻值造成接受的錯誤率。FA、FR 這兩種錯誤
率是一種取捨(tradeoff)的關係，若把門檻值提高，則錯誤拒絕率將會提高，而錯誤
接受率則會降低。若門檻值降低，則錯誤拒絕率將會降低，而錯誤接受率則會提
高。 
 
 
  27
2.3.3 MAP-GMM實驗內容與結果 
 
    本實驗中使用 NIST 2001 語料庫，實驗中特徵參數使用 38 維 MFCCs(12 維
MFCCs、12 維 Δ-MFCCs,、12 維 Δ2-MFCCs、1 維 Δ-log energy 和 1 維 Δ2- log 
energy)，因為 NIST 2001 是透過電話線錄音，為了減少通道不匹配所造成的影響
所以在求取 filterbank 能量時，triangle filterbank 頻寬為 300~3400Hz，除此之外也
對特徵參數作 CMS/MVA 處理，進ㄧ步的消除通道不匹配的影響。 
 
    在實驗中使用 174 位語者的訓練語料(每位語者之訓練語料長度大約為 2 分
鐘)，訓練出 1024 高斯混合數的 GMM 背景模型，語者模型則是利用每位語者相對
應之訓練語料經由背景模型調適成 1024 高斯混合數的 MAP-GMM。詳細實驗架構
如下： 
 
z 目標語者數：174 (100 位男性和 74 位女性) 
z 背景模型：  174 位語者之所有訓練語料 
z 調適語料：  每位目標語者之 2 分鐘訓練語料 
z 測試語料：  2038 句(每句測試語料長度約為 30 秒) 
 
    每句測試語料除了當目標語者測試一次之外，亦將當成不同的冒充語者測試
10 次。因此總測試數量為 22418 次(2038 次目標語者測試結果，20380 次冒充語者
測試結果)。 
 
 
  29
 
表2.2：NIST 2001 語料庫，不同個數同儕語者模型個數辨識率之比較 
同儕語者模型個數 EER(%) 
10 Cohort model 10.0 
20 Cohort model  9.6 
70 Cohort model  9.5 
 
 
表2.3：NIST 2001 語料庫，使用MAP-GMM+CMS及MAP-GMM+CMS+T-Norm辨識
率比較 
系統種類 EER(%) 
(a) MAP-GMM+CMS 12.4 
(b) 
MAP-GMM+CMS+T-Norm
 9.5 
(c) MAP-GMM+MVA 10.6 
(d) 
MAP-GMM+MVA+T-Norm
 8.9 
 
 
 
  31
2.3.5 短程音高參數(PITCH/ENERGY)之語者驗證系統 
本計畫所使用之短程音高參數語者驗證系統如圖 2.10 所示，在本節我們將整
個語者辨認基本系統，分成幾個區塊來描述，包括： 
1. 特徵參數萃取 (feature extraction) ：音高與對數能量之求取使用
Wavesurfer/Snack【5】軟體，再對求取出之音高與對數能量分別求取其斜
率，以這 4 維參數作為系統參數。 
2. 語者驗證(speaker verification)：與頻譜參數(MFCC)語者驗證系統相同，
僅將輸入之參數由頻譜參數改為短程音高與對數能量。 
 
圖2.10： 短程音高參數語者驗證系統方塊圖。 
3. 語者背景模型(background speaker model): 與頻譜參數(MFCC)語者驗證
系統相同，為了將系統之分數做正規化的動作將所有註冊語者訓練語料
中所求得之音高與對數能量參數全部一起訓練出之語者不特定模型作為
音高與對數能量語者背景模型。 
4. 語者模型訓練：由於音高所包含之語者資訊較不受語音內容的影響，所
以並非採用最大概似函數調適的方式，而是採用將單獨語者之音高與對
數能量特徵參數以單獨之高斯混合模型訓練之。 
  33
 
A. 將離散輸入訊號之前 3 點(視窗大小為 3)求取一段斜率，以此斜率
估算出訊號第 4 點之相對位置。 
B. 將訊號第 4 點到第 6 點也求取出其斜率，以此斜率推算出訊號第 4
點之相對位置。 
C. 計算步驟 A 與 B 所求出 2 點之距離。若距離大於門檻值(圖 2.12
中的 d)，表示訊號第 4 點即為轉折點，重複執行步驟 A，此時起
始點為方才求出之轉折點(視窗大小回歸為 3)。 
D. 若距離小於門檻值，則重複執行步驟 A 並將 A 中求取斜率之點數
(視窗大小)往後增加 1 點，直到步驟 C 成立或訊號結束為止。 
 
圖2.12： Piece-wise示意圖。 
接著依照相同之流程對對數能量執行一次 piece-wise curve fitting，即可得到音
高與對數能量訊號共同的轉折點。 
  35
 
圖2.13： 以某一區間為例說明 9 維韻律參數之求取。 
以向量量化為基礎之韻律訊息模型 
接著，將正規化處理過後的韻律特徵參數利用向量量化演算法加以分為 M
群，再將 M 個 codeword 執行 EM 演算法，則每個 codeword 可視為一特定韻律狀
態。 
首先我們對有聲部份執行向量量化演算法建立一個 m 個 codewords 的韻律模
型，對無聲部份執行向量量化演算法建立一個 n 個 codewords 的韻律模型。 
再將所得到的 codewords 執行 EM 演算法，得到具有機率意義之 codewords 如
圖 2.14，圖中我們可以看出狀態 1 到 8 為有聲部份，9 到 11 為無聲部份，且狀態
1 為句子之結尾，狀態 3 為句子之開頭。 
  37
2.4 語者驗證系統效能評估 
如圖 2.15、圖 2.16 中可知語者驗證的錯誤率有兩種：一種是錯誤的拒絕(False 
Rejection, FR)，即正確語者的分數小於門檻值造成拒絕的錯誤率。另一種是錯誤的
接受(False Acceptance, FA)即仿冒語者的分數高於門檻值造成接受的錯誤率。對語
者辨識系統而言，每句測試語料的特徵參數 { }1,..., TX x x= G G ，都可以與經由訓練語
料所訓練出來的 GMM 語者模型 sλ 、UBM 語者背景模型 bλ 計算出相對應的 log 
likelihood score，以數學形式表示即： 
( ) log( ( | )) log( ( | ))s bS X P X P Xλ λ= −               (2.20) 
1
1
( | ) ( ( , ) )
T
T
s i s
i
P X S x d xλ λ
=
= ⋅∏ JK K                  (2.21) 
, 1
, ,,
,
1( , ) max{ exp( ( ) ( ) )}
2(2 )
s
s ss
s
j T
i ij ji s jj D
j
w
S x x xλ λ λλ
λ
λ μ μ
π
−= − − ⋅∑ ⋅ −
∑
JK K JK K JK
(2.22) 
 
圖2.15： FA與FR的取捨關係圖。 
 
  39
2.4.4 決策代價函數 
決策代價函數(Decision Cost Function, DCF)，數學式如下： 
| arg arg |Im Im    FR FR T et T et FA FA postor postorDCF C P P C P P= × × + × ×     (2.23) 
其中 FRC 和 FAC 分別代表錯誤的拒絕與錯誤的接受的代價， argT etP 和 Im postorP 分別代
表目標語者與冒充語者出現的機率， | argFR T etP 和 |ImFA postorP 分別代表錯誤拒絕機率語
錯誤接受機率。 
2.5 本章結論 
     
    本章介紹了在本計畫中使用之語料庫 NIST 2001，及前級的特徵參數領域上
之正規化法－MVA，與後級在分數領域上之正規化法－測試正規化法。 
     
    由表 2.1 與表 2.2 之基礎語者驗證實驗結果中，我們可以發現前級的特徵參
數領域上之正規化法－MVA 是一種快速而有效率之方法，可以有效的補償通道
及雜訊對語者語音訊號之干擾，在分數領域上之正規化法－測試正規化法，更
可以進ㄧ步的藉由調整註冊語者和仿冒語者之分數之分佈，達到改善語者驗證
效能之目的，圖 2.17 為各基礎語者驗證技術之效能比較圖。 
 
  41
第三章 結合聲學與韻律之強健性語者辨認 
為補償未知話筒不匹配特性的失真，ML-AKI 先收集已知話筒特性集合當作先
驗知識，在測試時，則以此先驗知識做線性組合以估計補償未知測試話筒特性，
其中先知識內插的最佳權重值的求取，可利用期望值最大化演算法如圖 3.1 及式
(3.1)所示。 
 
 1
N
n n n
n
h hα
=
=∑
 (3.1) 
其中 nα 為內插的權重， nh 為模型領域上的先驗知識 
 
 
圖3.1： ML-AKI的架構圖。 
 
在 3.1.1 節先介紹以 MLLR 方式求取先驗知識，在 3.1.2 節中則利用
expectation-maximization（EM）演算法求取最佳的先驗知識內插權重，以補償未
知測試話筒的特性，在 3.1.3 節中介紹融合傳統 MAP-GMM/CMS 和 ML-AKI 的方
法，進一步來減輕話筒不匹配的影響，在 3.1.4 節中利用 HTIMIT 語料驗證所提出
1h
AKI-UBM
hˆ2
α
2h
O
 
 
Maximum 
likelihood 
interpolation
weights 
estimator 
1α
Nα
Nh
+
×
×
×
A priori knowledge
  43
在本篇論文中，MLLR 被用來量測 N 組轉換函數(N 個已知話筒之集合與註冊
話筒之間的轉換)，也就是 ˆ ˆnm nm nmW b A⎡ ⎤= ⎣ ⎦和 ˆ nmH ，其中 n 為已知話筒的索引。我
們收集平均值轉換函數及變異數轉換函數為一組集合{ }, ,ˆ, , 1 ~n m n mW H n N= 當作先
驗知識，表示話筒可能的空間。 
 
3.1.2 最佳化內插權重值求取 
 
為補償未知測試話筒之特性，我們在測試時利用事先求取的話筒先驗知識
{ }, ,ˆ, , 1 ~n m n mW H n N= ，以內插方式估計未知測試話筒特性的轉換函數，以調適語
者辨認模型，其中內插轉換函數如式(3.5)、(3.6)所示： 
 
 1
N
n n
n
W Wα
=
=∑G
 (3.5)  
   
 1
ˆ
N
n n
n
H Hα
=
=∑G
 (3.6) 
 
以下將依據 ML 準則，以 EM 演算法求取最佳內插權重值，調適語者辨認模
型，以補償未知測試話筒的不匹配特性。若使用 GMM 語者辨認模型，且只考慮
調適 GMM 模型的平均值，則定義 likelihood function 如下： 
 ,
1 1
( | , ) ( | , )
M N
t m t n n m n
m n
P o c o W mα μ
= =
Φ Λ = Σ∑ ∑`  (3.7)
             
其中 1{ ... }TO o o= 為測試語者的觀測值序列，M 為 GMM 的混合高斯數目， mc 為第 m
個混合高斯所佔的權重。 
  45
 
1 2
ˆ ˆ1
1 1 1 1
ˆ ˆ ˆ( , ,..., )
( ) n n
N
TT M N N
m t n m m t n m
t m n n
M
t o e W o e Wβ β
β β β
γ μ μ−
= = = =
=
⎛ ⎞ ⎛ ⎞− − Σ −⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠∑∑ ∑ ∑  (3.12) 
 
則藉由使得 1 2ˆ ˆ ˆ( , ,..., ) 0, 1 ~ˆ
N
n
M
n N
β β β
β
∂ = =∂ ，可以得到一組聯立方程式如下： 
 
 
ˆˆ 1
1 1 1
( ) ( ) ( ( ) ) 0, 1 ~jn
T M N
T
m n m m n m
t m j
t e W o t e W n N
ββγ μ μ−
= = =
⎡ ⎤Σ − = =⎢ ⎥⎢ ⎥⎣ ⎦∑∑ ∑  (3.13) 
 
若解出聯立方程式，可得到一組新的內插權重 *ˆnβ ，最後再將 *ˆnβ 轉回 *ˆnα ，其轉換式
如下： 
 
 
*
*
ˆ
*
ˆ
eˆ ,  1 ~
e
n
n
n n N
β
βα = =∑  (3.14) 
 
 最後反覆執行 EM 演算法，直到所求的內差權重值收斂為止，即求到最佳的內
插權重值，代入式子(3.5)與(3.6)，可得到一組調適後的 GMM 語者辨認模型。 
 
3.1.3 融合ML-AKI和傳統CMS方法 
  
然而考量到話筒之先驗知識並不可能含蓋所有話筒的特性，總是會有一些例
外的未知話筒，因此可以將 ML-AKI 的辨認分數和傳統以 CMS 為基礎之辨認器之
分數作融合(fusion)，如圖 3.2 的架構，其融合的式子如(3.15)所示： 
 
 
( ) ( )2 2
1 2
1 1 (1 )f
s s
s s s s
S λ λσ σ
− −= ⋅ + − ⋅
 (3.15) 
 
  47
3.1.6 使用MAP-GMM語者模型之語者辨認實驗 
 
本實驗中，每一個語者模型是使用一個 256 高斯混合的 MAP-GMM，話筒模
型為 256 高斯混合的 GMM，所做的實驗如下： 
由表 3.1 和表 3.2 可以得知所提出的 ML-AKI，在有未知話筒的情形下，平均
語者辨識率由 60.2%提升至 73.7%，只觀察未知話筒也可由 58.3 提升至 65.0%，融
合 ML-AKI 和 MAP-GMM/CMS 方法後，可由表 3.3 和表 3.4 得知，在有未知話筒
的情形下，平均語者辨識率提升至 74.6%，只觀察未知話筒平均語者辨識率提升至
67.5%。 
 
表3.1：HTIMIT語料庫，在含有未知話筒情形下，使用ML-AKI方法之辨認率(%) 
        測試話筒 
未知話筒 
Senh cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average 
cb1 84.1 77.5 83.1 53.3 64.9 84.1 78.1 70.2 79.1 70.5 74.5 
cb2 84.1 79.1 79.1 53.3 64.9 84.1 78.1 70.9 79.1 70.5 74.3 
cb3 84.1 78.1 82.1 32.5 64.2 83.8 78.5 70.5 78.5 69.9 72.2 
cb4 84.1 78.8 83.1 53.6 50.7 84.1 78.1 70.5 78.8 70.5 73.2 
el1 84.1 78.1 82.5 53.0 64.9 80.5 78.1 70.5 79.1 70.5 74.1 
el2 84.1 78.5 82.5 52.3 65.2 84.8 59.9 70.5 79.1 70.9 72.8 
el3 84.1 78.8 82.5 53.3 64.9 84.1 78.1 71.2 79.1 70.5 74.7 
el4 84.1 78.5 82.8 53.0 64.9 84.1 78.1 70.5 73.5 70.6 74.0 
pt1 84.1 79.5 82.1 53.3 64.9 83.8 78.1 69.5 79.1 60.3 73.5 
average 84.1 78.5 82.2 50.8 63.3 83.7 76.1 70.5 78.4 69.4 73.7 
 
表3.2：HTIMIT語料庫，在含有未知話筒情形下，融合ML-AKI和傳統CMS方法之
辨認率(%) 
        測試話筒 
未知話筒 
Senh cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average 
  49
3.1.7 使用MCE訓練語者模型之語者辨認實驗 
 
實驗條件即使用之前利用最小錯誤鑑別式訓練的語者模型。本計畫所提出的
ML-AKI 和 和傳統 CMS 的方法分別使用語者與音框層次最小錯誤鑑別式再訓練
之語者模型，實驗結果如表 3.7、表 3.8、表 3.11、和表 3.12 所示，可以得知在
MAP-GMM/CMS 在有未知話筒情形下，由 60.2%分別提升至 61.9%和 61.5%，只
觀察未知話筒的情形下可由 58.3%分別提升至 60.3 和 59.4%，ML-AKI 由 73.2%分
別提升至 74.2%和 74.8%，未知話筒由 60.5%分別提升至 66.4%和 66.4%，融合
MAP-GMM/CMS和ML-AKI使用語者與音框層次最小錯誤鑑別式再訓練之語者模
型下，在含有未知話筒的情形下，平均語者辨識率分別可達到 74.9%和 74.9%，只
觀察未知話筒的情形下，平均語者辨識率分別可達到 69.3%和 67.5%。  
 
 
表3.5：HTIMIT語料庫，ML-AKI方法使用音框層次之最小錯誤鑑別式所得到語者
模型在leave-one-out實驗中之語者辨認實驗結果 
        測試話筒 
未知話筒 
Senh cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average 
cb1 86.1 79.1 84.4 54.3 64.6 84.4 78.1 72.5 80.1 71.5 75.5 
cb2 86.4 80.5 80.1 54.3 64.6 84.4 78.1 72.5 80.1 71.5 75.3 
cb3 86.1 79.5 84.1 32.5 64.2 84.4 78.1 72.5 79.8 70.9 73.2 
cb4 86.4 80.5 84.8 55.3 51.7 84.4 78.1 72.8 79.5 72.5 74.6 
el1 86.1 79.5 83.8 54.3 64.6 85.1 78.1 72.5 80.1 71.5 75.6 
el2 86.4 80.1 84.1 54.0 64.2 84.4 60.6 72.5 80.5 71.9 73.9 
el3 86.4 79.1 84.1 54.3 64.6 84.4 78.1 71.9 80.1 71.5 75.5 
el4 86.4 80.1 84.4 54.6 64.9 84.1 77.8 72.9 74.2 71.5 75.1 
pt1 86.1 79.1 83.4 54.6 64.9 84.8 77.8 72.9 80.1 62.3 74.6 
average 86.3 79.7 83.7 52.0 63.1 84.5 76.1 72.6 79.4 70.6 74.8 
 
 
  51
表3.8：HTIMIT語料庫，只觀察未知話筒情形下，使用CMS、ML-AKI及Fusion，使
用音框層次之最小錯誤鑑別式所得到語者模型，在level-one-out實驗下所得到之平
均語者辨認率 
 cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 Average 
(%) 
MAP-GMM/CMS+GPD_F 71.3 76.7 29.8 34.6 76.1 64.9 59.8 65.7 55.3 59.4 
ML-AKI+GPD_F 79.1 80.1 32.5 51.7 85.1 60.6 71.9 74.2 62.3 66.4 
fusion 79.1 81.1 35.8 52.3 86.1 62.3 72.8 74.5 63.9 67.5 
 
 
表3.9：HTIMIT語料庫，ML-AKI方法使用語者層次之最小錯誤鑑別式所訓練之語
者模型在leave-one-out實驗中之語者辨認實驗結果 
         測試話筒 
未知話筒 
Senh cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average 
cb1 85.4 76.8 84.1 52.6 65.2 83.8 78.5 70.9 77.5 71.9 74.7 
cb2 85.8 78.5 78.8 52.6 64.9 83.8 78.5 70.9 77.8 71.5 74.3 
cb3 85.4 77.2 84.4 34.4 63.6 83.8 78.1 71.2 77.5 71.5 72.7 
cb4 85.8 79.5 84.4 53.3 53.0 83.8 78.1 71.5 77.2 71.9 73.9 
el1 85.4 78.1 84.1 52.6 64.9 81.8 78.5 71.5 77.8 71.9 74.7 
el2 85.8 77.8 84.4 52.3 65.2 84.4 63.9 71.2 78.5 71.9 73.5 
el3 85.4 78.5 84.4 52.6 64.9 83.8 78.5 71.5 77.5 71.9 74.9 
el4 85.8 78.5 84.4 53.0 64.9 83.8 78.5 71.5 74.8 71.9 74.7 
pt1 85.4 79.5 84.4 53.3 65.2 83.4 78.5 71.2 77.5 62.3 74.1 
average 85.6 78.3 83.7 50.7 63.5 83.6 76.8 71.3 77.3 70.7 74.2 
 
  53
表3.12：HTIMIT語料庫，只觀察未知話筒情形下，使用MAP-GMM/CMS與ML-AKI
及Fusion等方法，使用語者層次之最小錯誤鑑別式所訓練之語者模型在
level-one-out實驗中之語者辨識結果 
 cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 Average 
(%) 
MAP-GMM/CMS+GPD_S 70.2 75.5 32.2 38.7 75.2 64.6 62.3 67.2 57.0 60.3 
ML-AKI+GPD_S 76.8 78.8 34.4 53.0 81.8 63.9 71.5 74.8 62.3 66.4 
fusion 80.4 82.8 38.1 57.0 85.4 67.2 74.8 76.5 65.2 69.7 
 
 
3.1.8 實驗討論 
如表 3.4 所示，若使用 MAP-GMM 語者模型，ML-AKI 與傳統 CMS 方法作比
較，辨認率平均從 60.2%提升到 73.2%，若與 CMS 作適應性融合，平均語者辨認
率達到 74.6%。若單獨只挑出未知話筒的辨認率來看，平均語者辨認率可從 59.4%
提升到 67.5%。 
 
如表 3.7 與表 3.8 所示，若使用傳統 CMS 方法，MAP-GMM 語者模型和音框
層次之最小錯誤鑑別式訓練新的語者模型(MAP-GMM+GPD_F)做比較，平均語者
辨認率從60.2%提升到61.5%，若使用本計畫所提出ML-AKI方法，使用MAP-GMM
語者模型和音框層次之最小錯誤鑑別式訓練新的語者模型(MAP-GMM+GPD_F)做
比較，平均語者辨認率平均從 73.2%提升到 74.8%，若 ML-AKI 和 MAP-GMM/CMS
作適應性融合，使用音框層次之最小錯誤鑑別式訓練新的語者模型，平均語者辨
認率達到 74.9%，若單獨只挑出未知話筒的辨認率來看，平均語者辨認率可達到
67.5%。 
 
如表 3.11 與表 3.12 所示，若使用傳統 CMS 方法，使用 MAP-GMM 語者模型
  55
對已知及未知話筒都得到不錯的改善。 
 
60.2 61.5 61.9
73.7 74.2 74.8 74.6 74.9 74.9
58.3 59.4 60.3
65 66.4 66.4
67.5 67.5
69.7
0
10
20
30
40
50
60
70
80
CMS 　+GPD_F　 　+GPD_S　 ML-AKI 　 　+GPD_F　 　+GPD_S CMS+ML-AKI CMS+ML-AKI+GPD_F CMS+ML-AKI+GPD_S
方法
語
者
辨
識
率
(%
)
平均值
未知話筒
 
圖3.3：  HTIMIT語料庫，在有未知話筒情形下之語者辨認率，x軸為所使用的方法，y
軸為語者辨認率 
 
 
3.2 以特徵韻律訊息分析為基礎之語者辨認系統 
 
 為進一步補償未知話筒不匹配特性，我們使用較不受話筒影響的韻律訊息，
而為了減輕一般韻律訊息模型，如 bi-gram 或是 DHMM，需要大量訓練與測試語
料的問題，我們提出 EPA 方式，來得到可靠的韻律訊息，其架構(如圖 3.4)。 
 
  57
節為本章的結論。在每個章節中並將使用 HTIMIT 語料庫，以 snack 軟體(詳細說
明在附錄三)求取其音高與能量軌跡，並使用 TIMIT 語料庫所提供的切割位置，做
初步的實驗，說明所有步驟的物理意義。 
 
3.2.1 韻律模型與自動韻律狀態標記 
 
3.2.1.1. 韻律特徵參數求取與正規化 
 
因為音節為最小的韻律單位，我們採用五種音節層次的韻律特徵參數，包括： 
z 一個母音區段的音高斜率 (pitch slope)和長度的延長，變化 (lengthening 
factor)。 
z 兩個母音間的對數能量(log-energy)差和音高跳躍(pitch jump)值。 
z 兩個音節間的暫停長度(pause duration)。 
 
此外為移除語句發音內容（context-information）對韻律變化的影響，必須根
據所處音節的母音類型，對這些韻律特徵參數做正規化的動作，以移去任何非韻
律特性的影響。 
 
  59
 
表3.13：利用HTIMIT語料庫以註冊話筒(senh)之語音資料訓練出之 8-stateVQ韻律模
型 
Feature/State 1 2 3 4 5 6 7 8 
Pitch slop -0.1 0.7 -0.1 -0.2 0.1 0.3 -0.2 -2.5 
Energy diff. -0.4 -0.5 -0.8 -1.9 -0.1 0.2 1.3 0.1 
Pitch jump -0.2 -0.2 1.3 1.4 -0.1 -0.9 0.3 -0.6 
Lengthening 0.3 -0.5 0.3 1.4 0.1 -0.1 0.1 0.1 
Pause 0.4 -0.5 0.5 2.6 0.2 -0.3 0.3 0.1 
 
表3.14：利用HTIMIT語料庫註冊話筒(senh)語音資料訓練出 8-state之VQ韻律模型
狀，其狀態轉移矩陣統計 
 1 2 3 4 5 6 7 8 
1 3424 1256 854 429 1059 2783 919 304
2 1304 599 255 209 451 1282 344 192
3 347 122 77 55 109 405 109 43
4 20 18 5 3 18 50 10 3 
5 1074 510 237 167 348 894 286 91
6 3218 1544 621 364 1005 2804 891 351
7 882 392 255 102 330 829 416 162
8 331 180 100 63 95 349 129 98
 
 
3.2.1.3. 自動韻律訊息標記 
 
 利用建立好的 VQ 韻律模型，即可以自動將輸入之韻律特徵參數軌跡標記成
韻律狀態索引序列。以一輸入測試句子來說，其利用韻律模型做自動韻律訊息狀
態標記的結果如圖 3.6 所示，可看出標示結果符合預期，即 state 6 確實出現在句首
  61
以下詳細說明各步驟的作法。 
 
3.2.2.1. 韻律關鍵字字典 
 
首先將標記好的語者的韻律狀態標記序列，當作一文字文件（韻律文件），並
對所有可能發生的韻律標記序列組合，包含單字詞（single words)和雙字詞（word 
pairs），統計其發生次數，得到所有可能韻率標記序列組合發生頻率的長條統計
圖。接著設定一發生頻率臨界值，擷取所有超過頻率臨界值的韻率標記序列組合
作為韻律關鍵字，藉此建立韻律關鍵字字典，藉此表示一般語者常發生的韻律行
為。 
 
若同樣以 HTIMIT 語料庫中註冊話筒（senh）的語音資料作初步實驗，經過自
動標記統計後產生的長條統計圖如圖 3.7 所示，其中可見較常發生的多為對應到韻
律片語起頭或韻律片語中段的單字詞，如 state 6，1，7 與 3 等，而較少發生的則
多為對應到較少發生的韻律行為，如“8-4＂，“2-4＂等雙字詞。 
 
圖3.7： 韻律關鍵字字典詞頻統計 
 
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
1 6 11 16 21 26 31 36 41 46 51 56 61
1-3 
1-5 
8 
5-1 
6-2 
3-6 
5-7 
6 
1 
7 
6-6 
6-1 
1-6 
3 
8-4 
2-4 
5-8 
3-4 
  63
的，而 state 4 與 3 是 major 與 minor break，可見說話速度較慢的人，聚集在圖的
右下角，而說話速度較快的人，多被分配到圖的右上角，由此可以看出 SVD 確實
可以將不同韻律特性的語者分離開來。 
 
 
 
圖3.8： 語者韻律特徵空間 
 
3.2.2.4. 語者辨認分數的測量 
 
最後可以把語者辨認的問題，看成相當於文件擷取的問題。將輸入之測試語
者的測試句子，同樣轉成韻律狀態索引序列，利用韻律關鍵字典剖析後，當做一
虛擬詢問向量 yQ，再利用式子（3.17），將其投影至特徵-韻律語者空間，得到詢問
向量(query vector) vQ 
 
 
1T
Q Q K KV y U
−= Σ                                       (3.17) 
0 0.02 0.04 0.06 0.08 0.1 0.12
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
Dimension 1
D
im
en
si
on
 2
1
65
7
3
2
4
8
8-4 8-1
8-3
8-5
8-7
8-6
8-8
4-7
4 6
4-1
4-3
2-1
2-7
2-3
2-6
2-4
2-2
2-5
3-1
3-6
3-2
3-5
3-7
3-3
3-4
7-6
7-1
7-57-77-47-8
7-2
7-3
5-7
5-6
5-5
5-1
5-2
5-35-8
5-4
6-5
6-7
6-6
6-1
6-2
6-3
6-4
6-8
1-6
1-5
1-3
1-7
1-1
1-2
1-4
1-8
Keyword
Speaker
Fast speakers 
Slow speakers
More breaks 
Less breaks 
  65
所示： 
 
2 2
2
( )( ) ( )( )
1 1*( ) (1 )*( )
11
f f
f
g s s s s
rr std sstd s
S
ee
α α− −−−
= + −
++
 (3.18) 
其中α 為權重常數， fS  和 2S  為兩個系統的鑑別分數， γ 控制融合非線性程度，
fs , 2s , fsσ  和 2sσ  分別為 fS  和 2S 的期望值和標準差。 
 
 
圖3.10： 融合聲學層次MAP-GMM/CMS與韻律層次EPA方法分數之架構 
 
3.3.1 EPA話筒不匹配效應補償實驗 
 
3.3.1.1. 實驗條件 
 
在訓練語料是使用每一個語者的前七句，測試語料是使用每一個語者的後三
Registered 
Speakers
Decision
S1 
Prosody labeling &
EPA Projection 
Speaker 
recognizer 
Fusio
n 
S2 
Speech 
input 
Prosody 
Features 
Sg 
EPA VQ-based 
Prosodic modeling
CMS 
Speaker  
recognizer 
MAP-GMMs
MAP-GMM/CMS 
MFCC
s 
  67
其中TOP1 和TOP30 分別表示正確語者在辨識結果的第一名及前三十名之內 
 
TOPN cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average 
1 4.0 5.0 4.6 4.3 4.6 5.3 4.6 4.3 4.3 4.6 
30 55.0 52.0 51.7 54.6 54.6 52.3 56.0 54.0 56.2 54.0 
 
表3.17：HTIMIT語料庫，在有未知話筒情形下，使用CMS和融合EPA和
MAP-GMM/CMS方法之語者辨認結果 
方法 senh cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average
MAP-GMM/CMS 75.1 70.9 73.8 30.5 35.8 73.8 63.2 58.9 65.2 54.3 60.2 
+EPA 79.5 74.2 76.5 35.8 42.4 79.8 71.9 65.2 69.9 60.6 65.6 
 
表3.18：HTIMIT語料庫，在只有未知話筒情形下，使用CMS和融合EPA和CMS方法
之語者辨認結果 
方法 cb1 cb2 cb3 cb4 el1 el2 el3 el4 pt1 average
MAP-GMM/CMS 70.9 73.8 30.5 35.8 73.8 63.2 58.9 65.2 54.3 58.5 
+EPA 74.2 76.5 35.8 42.4 79.8 71.9 65.2 69.9 60.6 64.0 
 
 
3.3.1.3. 實驗討論 
 
由表 3.15 與表 3.16 所示，單獨使用 EPA 的方法得到的辨識率是不高的，但經
由不同話筒所得到的辨識結果，平均語者辨識率不會因為受到不同話筒的影響因
此產生劇烈的變化，由此可證明韻律訊息是較不受話筒的影響，但若融合 EPA 與
傳統 CMS 方法，則可提升單獨使用 CMS 方法的整體辨識率，由表 3.17 與表 3.18
所示，融合EPA和傳統CMS方法，和傳統 CMS方法比較，平均語者辨認率由 60.2%
提升至 65.6%，只觀察未知話筒的情形下，平均語者辨認率由 58.5%提升至 64.0%，
  69
本計畫分別在第三章提出在聲學層次改善話筒不匹配的方法 ML-AKI,在第四
章在韻律層次改善話筒不匹配的方法 EPA,並跟傳統 CMS 方法做比較及融合，得到
不錯的效果，所以接下來嘗試利用線性迴歸的方法再把 ML-AKI 和 CMS 和 EPA
做一個線性的結合。在 3.4.1 節中介紹介紹融合 ML-AKI 和傳統 CMS 和 EPA 方法，
在 3.4.2 節中介紹融合 ML-AKI 和傳統 CMS 和 EPA 方法之語者辨認實驗。 
 
3.4.1 融合CMS和ML-AKI和EPA方法 
 
因聲學與韻律訊息具有互補性，因此可以將使用聲學訊息的 ML-AKI，與使用
韻律訊息的 EPA，在辨認分數上作融合，以進一步加強語者辨認系統的強健性。
在本計畫中使用如圖 3.12 的架構，融合 ML-AKI＋MCE，EPA 與傳統
MAP-GMM/CMS 等方法。 
 
我們首先融合 ML-AKI 與傳統 MAP-GMM/CMS 方法，主要是考量到所收集
的話筒先驗知識並不可能含蓋所有話筒的特性，總是會有一些例外的未知話筒，
因此將 ML-AKI 的辨認分數和傳統以 CMS 為基礎之辨認器之分數作融合，其融合
式子如 3.19，然後再把融合後的分數與 EPA 辨認器的分數再做一次分數融合，融
合方法使用 sigmod 函數，如式子 3.19 所示： 
 
 
 
3 3 4 4
43
( )( )
( )( )
1 1*( ) (1 )*( )
11
g s s s s
rr
std sstd s
S
ee
α α− −−−
= + −
++
 (3.19) 
 
其中α 為權重常數， fS 和 2S 為兩個系統的鑑別分數，γ 控制融合非線性程度， fs ，
2s , fsσ  和 2sσ  分別為 fS  和 2S 的期望值和標準差。 
  71
3.4.2.2. 實驗結果 
 
由表 3.19 和表 3.20 所示，利用傳統 CMS 方法使用 MAP-GMM 語者模型的辨
認結果為基礎，利用語者層次最小錯誤鑑別式再訓練語者模型，再加上本計畫所
提出的 ML-AKI 和 EPA 方法，在含有未知話筒情形下，平均語者辨認率由 60.2%
提升至 79.3%，只觀察未知話筒情形下，平均語者辨認率由 58.5%提升至 74.3%。 
 
而若將 ML-AKI 和 MAP-GMM/CMS 作辨認分數融合，其找出的最佳融合權
重如圖九所示，則九輪實驗（共 90 次實驗）的平均語者辨認率可再提升到 74.9%
（如表 3.19 所示），若將九輪實驗中的未知話筒實驗部分獨立出來，則九次未知話
筒實驗的平均語者辨認率亦可提升到 69.7%（如表 3.20 所示）。顯示辨認分數融合
方式可以截長補短，以互補方式提升辨認率，且 ML-AKI 與 MAP-GMM/CMS 的
權重比重約為九比一，權重偏重 ML-AKI。 
 
最後我們再疊加上 EPA 方法，並作辨認分數融合，其找出的最佳融合權重如
圖 5.3 所示，一樣進行 leave-one-out 實驗，則九輪實驗（共 90 次實驗）的平均語
者辨認率可提升到 79.3%（如表 3.19 所示），若將九輪實驗中的未知話筒實驗部分
獨立出來，則九次未知話筒實驗的平均語者辨認率可提升到 74.6%（如表 3.20 所
示）。顯示韻律訊息層次的 EPA 與聲學訊息層次的 ML-AKI 具有非常不錯的互補效
果，且聲學與韻律訊息的權重比重約為七比三。 
 
  73
 
圖3.13： 融合ML-AKI和MAP-GMM/CMS分數時權重α 值與辨認率之關係 
 
 
圖3.14： 融合聲學層次ML-AKI＋MAP-GMM/CMS與韻律層次EPA分數時權重α 值與
辨認率之關係 
 
3.4.2.3. 實驗討論 
  
由表 3.19 與表 3.20 所示，利用傳統 CMS 方法使用 MAP-GMM 語者模型的辨
認結果為基礎，利用語者層次最小錯誤鑑別式再訓練語者模型，再加上本計畫所
提出的 ML-AKI 和 EPA 方法，在含有未知話筒情形下，平均語者辨認率由 60.2%
  75
 
60.2 61.9
74.2
79.3
58.5 60.3
69.7
74.6
0
10
20
30
40
50
60
70
80
90
MAP-
GMM/CMS
+GPD_S +ML-AKI +EPA
方法
平
均
語
者
辨
識
率
(%
)
平均值
未知話筒
 
圖3.15： HTIMIT語料庫，在有未知話筒情形下之語者辨認率，x軸為所使用的方法，y
軸為語者辨認率 
 
3.4.4 結論與未來展望 
 
3.4.4.1. HTIMIT語者辨認實驗結果總結 
 
主要是處理語者辨認系統在面對實際應用環境與訓練環境話筒不匹配之問
題，尤其加強對未知話筒特性偏移之抵抗能力，嘗試利用已知話筒的話筒特性先
驗知識，如 MLLR 轉換函數，以 ML-AKI 方式內插估計未知話筒之特性並補償之，
和使用 eigen-prosody analysis 分析較不受話筒特性的語者韻律訊息為輔助，並由實
驗證明所提出的 ML-AKI 和 EPA 融合方法的有效性，結論如下： 
z ML-AKI 和傳統 CMS 的方法在話筒不匹配情況下作比較，如圖 3.16 所示，
平均語者辨認率從 60.2%提升到 73.2%，若只看未知話筒的情況，平均語者辨
認率由 58.3%上升至 65.0 
z ML-AKI 再結合語者層次 GPD 訓練語者模型，平均語者辨認率從 60.2%提升
到 74.2%，若只看未知話筒的情況，辨認率由 58.3%上升至 66.4%， 
  77
3.5 結論 
 
嘗試融合聲學與韻律層次訊息，以建立強健式語者辨認系統，包括融合聲學
層次的最小錯誤鑑別式法則訓練之 MAP-GMM 語者模型，ML-AKI 和韻律訊息層
次的 EPA 分析。由 HTIMIT 實驗結果來看，平均語者辨認率可從傳統
MAP-GMM/CMS 的 60.2%，提升到 79.3%，而若單看未知話筒部分，平均語者
辨認率亦可從 58.3%，提升到 74.3%。因此聲學與韻律層次訊息的融合，的確可
對於話筒不匹配問題得到一定程度的解決，尤其在未知話筒方面，也得到不錯的
改善。此外若從所使用的語料長度來看，聲學層次系統都只用了 16 秒與 4 秒的訓
練與辨認語料，韻律層次系統都只用了七句與三句的訓練與辨認語料，因此可知
EPA 方法確實是可有效地利用 eigen-space analysis，善用有限的訓練與辨認韻律訊
息語料。綜合以上說明可驗證本計畫所題所提方法的有效性。 
 
3.6 未來展望 
 
 利用聲學和韻律層次訊息來減輕話筒不匹配的影響，也得到不錯的結果，下
一步除了希望在聲學和韻律的層次方面找到更好的方法，另一方面也希望在有限
的語料中，能萃取更上層的語者訊息，更進一步的減輕話筒不匹配的影響，並找
到比線性迴歸結合方式更好的結合方式，使不同層次訊息的融合結果最佳化。 
  79
 
圖4.1： LPA語者驗證系統架構圖。 
 
    以下在 4.1 節中介紹 LPA 語者驗證系統中 Tokenization 之部份，並說明如何建
立 VQ 韻律模型與自動韻律狀態標記，在 4.2 節中介紹特徵韻律分析的詳細步驟，
在 4.3 節中利用 NIST 語料驗證所提出之 LPA 方法，並且以傳統 CMN 方法使用
MAP-GMM 語者模型加上 T-norm 的辨認結果分數為基礎，在以 LNKnet 融合 LPA
之辨認結果分數之實驗結果及討論，最後一節為本章的結論。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  81
此外為移除語句發音內容（context-information）對韻律變化的影響，必須根
據利用整個訓練語料所統計出來之韻律特徵參數的平均值及標準差，對這些韻律
特徵參數做正規化的動作，以移去任何非韻律特性的影響。 
 
 
圖4.3： 音節層次之韻律參數求取。 
 
4.1.2 以VQ為基礎之韻律訊息模型 
 
接下來，我們將做過正規化處理的韻律特徵參數利用 VQ 分群方式，以 EM 演
算法分成 M 個 codewords，則每個 codeword 可視為一特定韻律狀態，據此建立一
韻律模型。 
 
為說明以此方式建立之韻律模型的意義，以下初步利用 NIST 語料庫中註冊語
者的語音資料，對 voiced segment 建立一個 8-codewords 韻律模型，對 unvoiced 
segment 建立一個 3-codewords 韻律模型，其每個 codewords 的質心值如表一所示，
其轉移矩陣的分佈(如表 4.2)。經檢查每個 codewords 質心的值，統計每個 codeword
E 
N 
E 
R 
G 
P 
I 
T 
C 
H 
Pause 
Duration
Pitch Jump 
Mean Energy 
Difference 
Pitch 
Slope 
  83
表4.2：利用NIST 2001 語料庫註冊語音資料訓練出 11-state VQ韻律模型，其狀態轉
移矩陣統計。 
State 1 2 3 4 5 6 7 8 9 10 11 
1 450 43 1029 294 1178 171 1386 443 569 794 417
2 862 2865 1053 1238 2829 2996 3370 4396 1167 750 693
3 151 672 3867 965 654 3455 4073 2055 2397 2100 955
4 303 213 1503 2295 1107 1366 2958 1104 995 835 483
5 1052 113 4109 1084 3740 901 5684 2089 2380 1879 1342
6 1547 5952 1648 3594 5952 6197 5504 12521 2366 1393 1496
7 873 1034 5079 1498 3597 4624 10688 6127 3349 3299 2027
8 1536 470 3028 2194 5316 1466 8532 4225 2716 1659 1818
9 0 1 16 0 0 15896 0 0 - - - 
10 0 10847 2 0 0 1905 0 0 - - - 
11 0 8 10 0 0 9185 0 0 - - - 
 
4.2 Latent Prosody Analysis (LPA) 
 
    PLSA 是一種被應用於分析 co-occurrence data 的統計方式，將我們所得到之語
者韻律訊息文件有效率的擷取語者韻律訊息文件中之語意及分析語者韻律訊息文
件中之統計量，在本計畫中提出(1)利用 speaker-specific PLSA 分解語者之韻律模
型，並且分析出語者韻律模型中韻律關鍵字之間之潛在語意，隨後將韻律模型經
過降維後，在重建回來，透過此種方式對語者之韻律 n-gram 模型平滑化處理。(2)
將所有語者之韻律 n-gram 模型組合成一個 co-occurrence 矩陣，並利用 speaker-wide 
PLSA 分解此 co-occurrence 矩陣，分析出語者韻律模型中語者之間韻律不同關鍵字
之潛在語意，隨後將 co-occurrence 矩陣經過降維後，在重建回語者之韻律 n-gram
模型，達到模型平滑化的目的。 
利用 LPA 輔助語者辨認步驟如下，包括： 
  85
強調較少出現的韻律關鍵詞。此語者—韻律關鍵詞關係矩陣，將進一步使用 term 
frequency-inverse document frequency (TF-IDF)方法作加權。 
 
4.2.2 以PLSA為基礎之韻律訊息模型平滑化 
 
    在實際的應用上，因為受限於訓練語料與測試語料之資料量的不足，因此在
受此限制之下，以韻律訊息所建構出的 bi-gram 語者模型可能不夠具有統計特性，
沒辦法準確的訓練出代表語者韻律特性的語者模型。舉例來說，在 NIST2001 語料
庫中每位語者之訓練語料大約為兩分鐘，經過 piece-wise stylization 後，兩分鐘的
語料大約可以被分割成一千個段落，換而言之，也就是兩分鐘的訓練語資料，只
能標記成ㄧ千個 state sequence，因此對於 11×11 bi-gram 的語者韻律模型而言，資
料量並也許不足夠訓練出 11×11 bi-gram 韻律模型。 
 
    PLSA 是一種廣泛的被應用於 vector-based information retrieval 的分析方法，因
為資料量的不足，造成訓練出來的 11×11 bi-gram 韻律模型矩陣，成為一個稀疏矩
陣。因此在本計畫中提出利用 PLSA 分解語者韻律模型矩陣，經過降維，尋找出
ㄧ個較低維度之 latent semantic space，來達到韻律模型平滑化之目的。 
 
PLSA 分解之定義如式(4.1): 
    
1
( , )  ( ) ( | )  ( ) ( | ) ( | )
K
i j i j i i j k k i
k
P d w P d P w d P d P w z P z d
=
= = ∑             (4.1) 
其中 id、 iw、z 所代表的分別是 document、keyword、latent prosody factors， ( , )i jP d w
所代表的是 document( id )與 keyword( iw )之間的結合機率，其概念如圖 4.4 所示。 
  87
率較低的 noisy latent factors 後，在重建回語者之韻律關鍵詞關係矩陣，達到
平滑化的目的。 
 
4.2.2.2. Speaker-wide PLSA Latent Prosody Analysis 
 
 
圖4.6： Speaker-wide PLSA bi-gram model smoothing。 
 
Speaker-wide PLSA Latent Prosody Analysis 其架構如圖 4.6，包含五個步驟： 
1. 將經過正規化處理的韻律特徵參數利用 VQ 分群方式，以 EM 演算法分成 M
個 codeword，每個 codeword 可視為一個特定的韻律狀態，依據此 codebook
對訓練語料進行韻律狀態標記。 
2. 利用每位語者的韻律狀態序列建立其韻律狀態 n-gram 模型。 
3. 見圖 4.7(a)，依據所有語者之韻律狀態 n-gram 模型，建立韻律狀態之
co-occurrence 矩陣。 
4. 見圖 4.7(b)，經由 PLSA 去分解此韻律狀態之 co-occurrence 矩陣，降維之後找
出一較低維度的語者韻律特徵空間。 
  89
者驗證系統的辨識率(EER)。 
 
    以下將針對傳統 MAP-GMM 及 GMM-MAP 加上 T-norm 辨識分數分別融合本
計畫所提出之 LPA 辨識系統之辨識分數，報告實驗結果，及實驗討論。 
 
4.3.2 MAP-GMM and T-norm 
 
    在此系統中特徵參數使用 38 維 MFCCs(12 維 MFCCs、12 維 Δ-MFCCs,、12 維
Δ2-MFCCs、1 維 Δ-log energy 和 1 維 Δ2- log energy)，語者模型使用 1024 高斯混
合數的 MAP-GMM，測試正規化之同儕模型個數為 50 人，實驗結果如表 3.3 所示，
MAP-GMM 之辨識率(EER)為 12.4%，T-norm 之辨識率(EER)為 9.5%。 
 
4.3.3 Pitch/Energy GMMs 
 
    在此系統中特徵參數使用 log-pitch 及其微分項 Δ-log-pitch、log-energy 及其微
分項 Δ-log-energy，語者語者模型使用 64 高斯混合數的 GMM， pitch/energy GMM
之辨識率(EER)為 32.3%。 
 
4.3.4 Prosody bi-gram speaker models 
 
    在此系統中語者韻律狀態 bi-gram 模型為 11-state(8+3)之韻律模型，並且使用
Good-Turing 的方式來平滑化語者韻律狀態 bi-gram 模型， Bi-gram(Good-Turing)
之辨識率(EER)為 31.2%。 
  91
4.3.6 Speaker-wide PLSA Latent prosody space analysis 
 
    圖 4.9 為韻律狀態及 NIST2001 語者投影在經過 Speaker-wide PLSA Latent 
Prosody Analysis 之韻律特徵空間之分佈圖，從圖中可以看出語者說話速度較慢 
(major break，state 10)與說話速度較快(minor break，state 9、state 11)，在韻律特徵
空間分佈中的差異。 
 
 
圖4.9： Speaker-wide PLSA Latent Prosody Analysis之語者韻律特徵空間分佈圖。 
 
    在此系統中語者韻律狀態 bi-gram 模型為 11-state(8+3)之韻律模型，並且使用
Speaker-wide PLSA Latent prosody space analysis 方式，將所有語者之韻律 n-gram
模型組合成一個 co-occurrence 矩陣，並利用 speaker-wide PLSA 分解此
co-occurrence 矩陣，co-occurrence 矩陣經過降維後，在重建回語者之韻律 n-gram
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Dimension 1
D
im
en
si
on
 2
4,4 4,3
4,7
6,6
8,11
8,10
7,10
2,10
10,2
2,2
3,10
2,3
4,10
5,2
4,8
4,11
3,2
1,1
4,1
7,4
1,4
3,3
2,11
10,6
7,8
7,9
3,8
1,7
10,9
6,7
3,7
Scatter plot of the Bi-gram terms and speakers
 
 
Bi-gram term
Speaker
Slower Speakers 
(More Long Breaks) 
Fast Speakers 
(Less Long Breaks)
  93
上做融合，也確實可以利用互補的方式達到提升辨識率之目的。 
表4.3：NIST 2001 語料庫，結合不同系統之系統效能。 
Approach EER (%) 
(1) MAP-GMM 12.4 
(2) MAP-GMM+T-norm 9.5 
(3) Pitch+Energy 32.3 
(4) Bi-gram (Good-Turing) 31.2 
(5) Speaker-specific PLSA 26.8 
(6) Speaker-wide PLSA 26.8 
(7) Fusion: (1)+(5) 10.4 
(8) Fusion: (1)+(6) 10.6 
(9) Fusion: (2)+(5) 8.4 
(A) Fusion: (2)+(6) 8.4 
(B) Fusion: ALL 8.1 
 
 
圖4.10： NIST 2001 語料庫，結合不同系統之系統效能，偵測錯誤取捨曲線圖。 
 
  95
 
圖4.11： UBM tokenization-based LAA語者驗證系統架構圖。 
 
 
圖4.12： MAP adaptation-based LAA語者驗證系統架構圖 
 
    我們提出(1) UBM tokenization-based LAA (2) MAP adaptation-based LAA，其架
構如圖 4.11、圖 4.12，其中 UBM tokenization-based LAA 是利用所有語者之訓練語
料訓練出來之背景模型(UBM)當作 UBM Tokenization，得到 mixture index sequences
後，統計出其 uni-gram 及 bi-gram，隨後將每位語者之 uni-gram 及 bi-gram 按照順
序串成 supervector。MAP adaptation-based LAA 則是把語者特定模型裡高斯密度的
函數中之平均值向量按照順序串成 supervector。接著分別運用對 R 筆 supervector
  97
4.5.2 空間建立 
4.5.2.1. UBM tokenization-based LAA 
 
    我們使用訓練語料所訓練出來的 UBM，對每ㄧ位語者之訓練語料之每個音框
計算其相對應於 UBM 中所有的 mixture 之 likelihood，在利用此 likelihood 統計出
每個音框相對應於 UBM 中所有的 mixture 之 soft-decision 特徵參數與 hard-decision
特徵參數，最後在經由這些 soft-decision 特徵參數序列與 hard-decision 特徵參數序
列，統計出其 uni-gram、bi-gram 之統計值，接下來將每位語者之 uni-gram、bi-gram
統計值依照順序串成一個 supervector，利用所有語者之 supervector 建構出語者聲
學訊息文件。 
 
UBM tokenization-based based LAA 語者辨認包括四個步驟，包括： 
1. 將每位語者之聲學參數(MFCCs)經過 MVA 前級處理後，在利用訓練語料所訓
練出來的 UBM，經過 UBM tokenization，產生 mixture index sequence 特徵參
數，並且統計、建立 mixture index sequence 之 uni-gram 及 bi-gram。 
2. 使用得到之聲學狀態之 uni-gram 及 bi-gram，建立出語者聲學訊息文件。 
3. 利用 UBM tokenization-based，分析、降維，建構出特徵聲學語者空間，來將
我們所得到之語者聲學訊息文件有效率的擷取語者聲學訊息文件中之語意及
分析語者聲學訊息文件中之統計量。。 
4. 將測試語者的測試語句子轉成聲學狀態序列，並利用此韻律狀態序列建構出
虛擬詢問向量，並計算詢問向量和註冊語者向量間夾角的餘弦值(cosine of 
angle)，其中夾角的大小相當於與註冊語者之間的相關性。 
 
 
  99
型，將語者特定模型裡高斯密度的函數中之平均值向量按照順序串成一個
supervector。 
2. 接著運用對 R 筆 supervector 所組成之矩陣 A，做奇異值分解(Singular Value 
Decomposition, SVD)，保留 R 筆中前 K 筆的主成分視為一組新的基底(basis)，
利用此組基底來生成(span)一個敘述語者間變異性的特徵空間。 
3. 將測試語者的測試語同樣也經由 MAP-GMM 調適出之語者特定模型，並將語
者特定模型裡高斯密度的函數中之平均值向量按照順序串成一個
supervector，以此 supervector 當作一虛擬詢問向量 Qy ，將此虛擬詢問向量 Qy
投影至語者聲學特徵空間，得到詢問向量(query vector)，並計算詢問向量 QV 和
註冊語者向量 KV 間夾角的餘弦值(cosine of angle)，其中夾角的大小相當於與
註冊語者之間的相關性。 
 
4.5.3 計算特性基底之空間 
 
    對 R 筆 supervector 所組成之矩陣 A，做奇異值分解 (Singular Value 
Decomposition, SVD)，在所有特性向量中，具有較大的特徵值所相對應的特徵向
量帶有較多的資料量，我們保留 R 筆中前 K 筆的主成分視為一組新的基底(basis)，
利用此組基底來生成(span)一個敘述語者間變異性的特徵空間，如圖 4.14 所示。
SVD 分解公式表示如下： 
T T
K K K KA = U V A = U VΣ Σ                   (4.2) 
其中 K K KA ,U ,Σ 和 TKV 分別為A,U,Σ和 TV 各自矩陣的降秩(rank-reduced)矩陣。 
 
  101
Q K
Q K
Q K
V V
similarity (V ,V ) = 
V V
K KiK K                (4.4) 
 
 
4.7 以特徵語者訊息分析為基礎之實驗 
4.7.1 實驗條件 
 
    本實驗特徵參數使用經過 MVA 之 38 維 MFCCs(12 維 MFCCs、12 維
Δ-MFCCs,、12 維 Δ2-MFCCs、1 維 Δ-log energy 和 1 維 Δ2- log energy)，UBM 
tokenization-based LAA 中之 UBM 為 1024 mixture，使用 occupation probability 
sequences 特徵參數(soft-decision)與 count sequences 特徵參數(hard-decision)，統計
每ㄧ位語者 uni-gram 、bi-gram，便且按照順序串成一個 supervector。MAP 
adaptation-based LAA 則是利用 MAP-GMM 調適出 1024 高斯混合數的語者模型，
使用調適出的語者特定模中高斯密度的函數之平均值向量，按照順序串成一個
supervector，174 位註冊語者產生之 174 筆 supervector 所組成之矩陣 A，，並且對
矩陣 A 做奇異值分解(Singular Value Decomposition, SVD)。 
 
    以下將針對傳統 MAP-GMM 及 GMM-MAP 加上 T-norm 辨識分數分別融合本
計畫所提出之特徵語者訊息分析辨識系統之辨識分數，報告實驗結果，及實驗討
論。 
 
 
 
  103
表4.4：NIST 2001 語料庫，使用UBM tokenization-based LAA語者驗證實驗結果 
Approach EER(%) 
UBM tokenization-based LAA  
 (Bi-gram)(soft-decision) 
        18.4 
UBM tokenization-based LAA  
 (Uni-gram + Bi-gram)(soft-decision) 
        16.0 
UBM tokenization-based LAA  
 (Bi-gram)(hard-decision) 
        17.0 
UBM tokenization-based LAA  
 (Uni-gram + Bi-gram)(hard-decision) 
        19.8 
 
圖4.15： 使用UBM tokenization-based LAA之偵測錯誤取捨曲線圖。 
 
  105
圖4.16： Map Adaptation-based LAA語者聲學特徵空間。 
 
圖4.17： 語者在Map Adaptation-based LAA聲學特徵空間的分佈與辨認分數測量。 
 
Female 
speaker 
Male 
speaker 
  107
表4.6：NIST 2001 語料庫，使用Map Adaptation-based LAA結合傳統MVA方法之語
者驗證實驗結果 
MAP adaptation-based LAA EER(%) 
100 Dimension 9.2 
120 Dimension 8.8 
140 Dimension 8.5 
160 Dimension 8.2 
174 Dimension 7.7 
 
 
表4.7：NIST 2001 語料庫，使用Map Adaptation-based LAA結合傳統CMS方法與測
試正規化方法及傳統CMS結合測試正規化方法之語者驗證實驗結果比較 
Approach EER(%) 
MAP-GMM/CMS+T-norm 9.5 
MAP adaptation-based LAA/CMS+T-norm 10.3 
 
 
表4.8：NIST 2001 語料庫，使用Map Adaptation-based LAA結合傳統MVA方法與測
試正規化方法及傳統MVA結合測試正規化方法之語者驗證實驗結果比較 
Approach EER(%) 
MAP-GMM/MVA+T-norm 8.9 
MAP adaptation-based 
LAA/MVA+T-norm 
7.6 
 
  109
4.8 結論 
 
    在 本 章 節 中 我 們 提 出 (1) UBM tokenization-based LAA (2) MAP 
adaptation-based LAA，兩種不同的 Latent Speaker Analysis 的方式，來分析語者之
間聲學模型長程(long-term)特徵訊息之潛在語意，藉此得到可靠的聲學訊息，從實
驗結果可以得知 UBM tokenization-based LAA 在結合了 Uni-gram 與 Bi-gram 之聲
學關鍵字後可以得到比單獨使用 Uni-gram 之聲學關鍵字更好的驗證結果，以 MAP 
adaptation-based LAA 的方式更是可以得到與傳統 MAP-GMM 架構之語者驗證相
近之效能。 
4.9 融合LPA和LAA方法 
 
    由前人的研究中，得知結合不同層次的語者訊息可以減低語者驗證時語料受
到通道不匹配及雜訊因素的影響，因此接下來在本章節中嘗試利用 MIT 林肯實驗
室所發展的 LNKnet 軟體，以 multi-layer perceptron（MLP）類神經網路的方式做
不同系統分數上的結合。利用傳統MVA方法使用MAP-GMM語者模型加上T-norm
的辨認結果分數為基礎，在以 LNKnet 融合特徵韻律訊息辨認結果分數與特徵語者
訊息之辨認結果分數，希望能夠進ㄧ步的加強語者驗證系統之強健性。 
 
 
 
 
 
  111
4.11 融合LPA和LAA之語者驗證實驗 
4.11.1 實驗條件 
 
    本實驗條件特徵參數使用經過 CMS/MVA 之 38 維 MFCCs(12 維 MFCCs、12
維 Δ-MFCCs,、12 維 Δ2-MFCCs、1 維 Δ-log energy 和 1 維 Δ2- log energy)，在本
實驗中是以 1024 高斯混合數 MAP-GMM 語者模型加上同儕模型個數為 50 人之測
試正規化為辨識分數基礎，希望能結合具有互補性之特徵韻律訊息與特徵語者訊
息，進ㄧ步的提升語者驗證系統的辨識率(EER)。 
 
    以下將針對傳統 GMM-MAP 加上 T-norm 辨識分數融合本計畫所提出之辨識
Latent Prosody Analysis 系統之辨識分數及 Latent Acoustics Analysis 系統之辨識分
數，報告實驗結果，及實驗討論。 
 
4.11.2 實驗結果 
 
    由表 4.9 所示，我們利用傳統 MVA 方法使用 MAP-GMM 語者模型辨認結果
加上測試正規化為基礎，在融合 Latent Prosody Analysis 系統之辨識分數，語者驗
證之辨認率 (EER)可以由為 8.9% 下降至 7.7%。利用傳統 MVA 方法使用
MAP-GMM 語者模型辨認結果加上測試正規化為基礎，在融合 Latent Acoustics 
Analysis 系統之辨識分數，語者驗證之辨認率(EER)可以由為 8.9% 下降至 6.8%。
融合 Latent Acoustics Analysis 系統之辨識分數及 Latent Prosody Analysis 系統之辨
識分數，語者驗證之辨認率(EER)可以由為 7.6% 下降至 6.8%。最後在本計畫中我
們利用傳統 MVA 方法使用 MAP-GMM 語者模型辨認結果加上測試正規化為基
礎，融合 Latent Prosody Analysis 系統之辨識分數及 Latent Acoustics Analysis 系統
  113
 
圖4.21： NIST 2001 語料庫，使用特徵語者訊息分析結合特徵韻律訊息分析與傳統MVA
方法與測試正規化方法之語者驗證實驗結果比較之偵測錯誤取捨曲線圖 
 
4.11.3 實驗討論 
 
    如表 4.9 所示，利用傳統 MVA 方法使用 MAP-GMM 語者模型辨認結果加上
測試正規化為基礎，在融合本計畫提出的 Latent Prosody Analysis 系統之辨識分數
及 Latent Speaker Analysis 系統之辨識分數，語者驗證之辨認率(EER)可以由為 8.9% 
下降至 6.6%，結果足以證明結合使用韻律訊息之 Latent Prosody Analysis，與使用
聲學訊息之 Latent Speaker Analysis，確實能夠改善通道不匹配所造成的影響，並
且能夠在少量語料的情形之下有效的改善語者辨識系統之辨識率。 
 
  115
4.13 結論 
 
     在本計畫中嘗試著融合聲學與韻律層次訊息，利用聲學與韻律層次訊息互補
之特性，建立強健式語者驗證系統，包括了韻律訊息層次的 Latent Prosody Analysis
以及聲學層次的 Latent Acoustics Analysis。由 NIST 2001 語者驗證實驗結果來看，
語者辨識率(EER)可以由傳統 MAP-GMM/CMS 的 9.5 結合韻律訊息層次的 Latent 
Prosody Analysis 下降至 8.1%，由此結果可以證明聲學與韻律層次資訊的融合，確
實對於通道/話筒不匹配的問題有所改善。此外若從所使用的語料長度來看，韻律
層次系統只用了平均長為兩分鐘之訓練語料及平均長為三十秒之測試語料，由此
可以證明 Latent Prosody Analysis 可以在有限的訓練與測試韻律訊息語料，輔助傳
統 MAP-GMM/CMS 語者驗證系統，改善辨識率。 
 
4.14 未來展望 
 
    本計畫主要的目的主要在於結合聲學層次訊息與韻律層次訊息來減輕通道不
匹配之影響，此外為了減輕ㄧ般韻律訊息模型，如 bi-gram【14】或是 DHMM【14】，
需要大量訓練與測試語料的問題，我們提出 Latent Prosody Analysis 方式及 Latent 
Acoustics Analysis 方式，來得到可靠的韻律訊息及聲學訊息，並且結合傳統
MAP-GMM/CMS 的方式，改善語者驗證系統之辨識率。未來除了希望在聲學和韻
律層次上，萃取更上層的語者訊息，找到更具有鑑別性，且較不受通道影響之參
數之外，另ㄧ方面也希望能夠在少量語料的情形之下，能夠更準確的估測語者韻
律及聲學模型，並且融合更多不同層次之語者訊息，使語者驗證系統更具強健性。 
  117
在 5.1 節中將介紹我們提出的韻律狀態相關語者模型，5.2 節將介紹基於韻律
與頻譜聯合模型之強健性語者驗證之架構，最後將使用 NIST2001-SRE 與
ISCSLP2006-SRE 語料庫驗證系統之效能。 
 
圖5.1： 以韻律狀態為基底MFCCs分類示意圖。 
 
  119
常將韻律訊息與頻譜訊息視為獨立資訊，但是一般人在說話時，通常在句首總是
較大聲且音高較高，而在句尾總是較小聲且音高較低，說話時所處的韻律狀態，
實際上會直接影響其所發出之語音的頻譜統計特性，反之亦然。所以我們將考慮
語音的頻譜特性與所處的韻律狀態間之關聯。 
首先對韻律狀態而言， Tokenization 方式，對整個語料庫做 stylization，將所
得韻律單位求取韻律參數，再以向量量化之方式，將訓練語料中相同韻律特性的
韻律單位歸類為同一狀態，以此韻律狀態對整個語料庫之韻律單位做分類而得到
韻律狀態序列。 
將頻譜與音高系統所求取之頻譜與音高參數，依據所得之韻律狀態序列分群
如圖 5.1。接著將頻譜與音高之高斯混合模型以圖 5.2 之方式，將相同韻律特性之
頻譜參數與音高參數對各自的語者高斯混合模型做第二次最大概似函數調適。這
次的最大概似函數調適是以每位註冊語者的模型作基準，將註冊語者的高斯混合
模型依據其各個韻律狀態，細分為與該語者韻律狀態相關的高斯混合模型，讓同
類韻律特性的短程語音訊息得以歸為同一類，進一步融入韻律資訊。 
5.2 基於韻律與頻譜聯合模型之強健性語者驗證 
在語音辨識中隱藏式馬可夫模型是最廣泛的一種模型，因為其不僅考慮了語
者訊息於高斯混合模型的機率分佈，其中狀態的特色更加考慮了資訊與模型對時
間前後的關係。利用此一特性，我們試圖以馬可夫模型的狀態與狀態間的轉移機
率，描述上層語音韻律狀態的變化，並使用與韻律狀態相關的高斯混合模型，來
描述下層頻譜訊息在某韻律狀態下的統計特性。建立一 ergodic 隱藏式馬可夫聯合
模型將系統於模型領域整合，以此聯合模型來描述韻律與頻譜訊息隨時間變化之
情形。 
 
  121
,
,
,
,
1
,
1
,
log ( | ) log ( | )
( , , ) arg max log ( | )
log ( | )
n end
x n
n start
n end
z n
n start
n n n n
tN
joint
t s
n t t
t
t s
t t
P P Y
S X Y Z P X
P Z
φφ
φ
φ φ φ
λ
λ
−
= =
=
⎧ ⎫⎪ ⎪+⎪ ⎪⎪ ⎪⎪ ⎪≈ +⎨ ⎬⎪ ⎪⎪ ⎪⎪ ⎪+⎪ ⎪⎩ ⎭
∑ ∑
∑
          (5.1) 
其中 1( | )n nP φ φ − 為韻律狀態轉移機率， ( | )n nP Y φ 為在韻律狀態 nφ 下觀測到韻律
特徵參數Y 之機率， X 為頻譜特徵參數，Z 為音高特徵參數， ,x ns φλ 為 MFCC 與韻
律狀態 nφ 相關之語者頻譜高斯混合模型， ,z ns φλ 為音高與韻律狀態 nφ 相關之語者頻
譜高斯混合模型，
,n startt 與 ,n endt 為某一韻律狀態單元的起始與結束時間。 
由於我們以隱藏式馬可夫模型的狀態與狀態間轉移之變化來描述語者的韻律
狀態與狀態轉移，根據(5.1)式之定義，以隱藏式馬可夫模型常使用的維特比演算法
(圖 5.6)，對訓練語料做辨識以得到 ( , , )jointS X Y Z ，接著重新追朔並紀錄其中最大
機率韻律狀態φ，做為新的韻律狀態序列，此韻律狀態序列不在單純由韻律參數所
求得，其包含了頻譜參數、音高參數與韻律參數之資訊。接著以新的韻律狀態序
列重新訓練韻律相關語者高斯混合模型與韻律參數之高斯混合模型，接著對測試
語料以維特比演算法以最佳路徑之方式計算出系統分數 ( , , )jointS X Y Z 。則以此基於
韻律與頻譜聯合模型之強健性語者驗證可將頻譜參數、音高參數與韻律參數之資
訊整合於聯合模型中。 
  123
2. 接下來，我們將做過正規化處理的韻律特徵參數利用向量量化分群方
式，再經 EM 演算法分成 11 個 codewords，其中有聲部份分為 8 個
codewords，無聲部份分為 3 個 codewords，則每個 codeword 可視為一特
定韻律狀態，據此建立一韻律模型。 
3. 利用建立好的韻律模型，即可以自動將輸入之韻律特徵參數標記成韻律
狀態索引序列。 
4. 將自動標記好的韻律狀態序列，統計出韻律序列組合的雙聯文模型，當
成語者模型。 
    根據此系統之實驗結果，NIST2001-SRE 中韻律雙聯文模型之相等錯誤率為
36.06%。ISCSLP2006-SRE 中韻律雙聯文模型語者模型之相等錯誤率為 24.70%。 
表5.1：NIST2001-SRE中韻律雙聯文模型之相等錯誤率。 
系統種類 特徵參數 EER(%) 
Prosodic bigram Prosody 36.06% 
 
表5.2：ISCSLP2006-SRE中韻律雙聯文模型之相等錯誤率。 
系統種類 特徵參數 EER(%) 
Prosodic bigram Prosody 24.70% 
 
5.3.2 韻律相關語者高斯混合模型 
在此系統中特徵參數使用 38 維 MFCCs(12 維 MFCCs、12 維 Δ-MFCCs、12
維 Δ2-MFCCs、1 維 Δ-log energy 和 1 維 Δ2- log energy)，而實驗中之語者模型採用
最大概似函數調適，分別先將語料庫中之註冊語者訊練出一背景模型，此背景模
型為一高斯混合模型，混合數為 1024，最大概似函數調適時採用之尺度因子為 16，
混合數保持為 1024。執行最大概似函數調適時只對 UBM 之平均值做調適，其餘
模型參數皆繼承 UBM 的參數。接著利用圖 5.2 之架構，將語者模型再使用第二次
  125
表5.3：NIST2001-SRE之韻律狀態相關語者高斯混合模型相等錯誤率。 
系統種類 EER(%) 
MAP-GMM 8.64% 
Hard decision 8.28% 
Soft decision 8.35% 
 
 
圖5.7： NIST2001-SRE之韻律相關語者高斯混合模型DET Curve。 
 
  127
5.4 基於韻律與頻譜聯合模型之強健性語者驗證 
維特比決策：以 stylization 後所得的區間為主，Tokenization 後所得之韻律狀
態序列為路徑，分別計算(1)MFCC 與其韻律相關語者高斯混合模型之分數、(2)音
高參數與其韻律相關語者高斯混合模型之分數以及(3)韻律參數與韻律特徵參數觀
測機率模型計算分數，統計韻律狀態序列上每個路徑所累積之分數。取出最後一
個區間所累積到最大機率值做為語者驗證之分數。NIST2001-SRE 中其系統之相等
錯誤率為 8.19%，ISCSLP2006-SRE 中其系統之相等錯誤率為 4.17%。 
表5.5：NIST2001-SRE之基於韻律與頻譜聯合模型之強健性語者驗證相等錯誤率。 
系統種類 EER(%) 
Viterbi：(1)+(3) 8.19% 
Viterbi：(1)+(2)+(3) 8.93% 
 
  129
 
 
圖5.10： ISCSLP2006-SRE之基於韻律與頻譜聯合模型之強健性語者驗證DET Curve。 
5.5 本章結論 
由韻律相關語者高斯混合模型與傳統之 MAP-GMM 比較，NIST2001-SRE EER
可從 MAP-GMM 的 8.64%下降至韻律相關語者高斯混合模型的 8.28%，
ISCSLP2006-SRE EER 可從 5.04%下降至 4.7%。可見上層語音韻律狀態的變化與
下層頻譜與音高訊息的統計特性間有相互之關聯性，所以將 MFCC 利用韻律狀態
分群後在重新訓練，能使系統得到較佳之效能。 
  131
第六章 研究成果 
6.1 發表論文列表 
A. Journal Papers:  
1. Zi-He Chen, Yuan-Fu Liao and Yau-Tarng Juang, “Eigen-Prosody Analysis 
for Robust Speaker Recognition under Mismatch Handset Environment”, 
Electronics Letters, Volume 40, Issue 19, 16 Sept. 2004 Page(s):1233 – 1235.  
2. Yih-Ru Wang and Bo-Xuan Lu and Yuan-Fu Liao and Sin-Horng Chen, 
Distributed Speech Recognition of Mandarin Digits String", ISCSLP 
Proceedings (Springer LNAI Book, SCI Indexed)' 2006  
3. Yuan-Fu Liao, Zi-He Chen and Yau-Tarng Juang, “Latent Prosody Analysis 
for Robust Speaker Identification”, submitted to IEEE Trans. on Speech, 
Audio and Language Proc. July 2007 
4. Yuan-Fu Liao, Jyh-Her Yang, Zhi-Xian Chuang and Sin-Horng Chen, 
“Maximum Likelihood A Priori Knowledge Interpolation-Based Handset 
Mismatch Compensation for Robust Speaker Identification”, submitted to 
IEEE Trans. on Speech, Audio and Language Proc. (revised).  
5. Yuan-Fu Liao, Jyh-Her Yang, Zhi-Xian Zhuang and Sin-Horng Chen, “A 
Soft-decision A Priori Knowledge Interpolation Approach of Handset 
Characteristic Estimation for Telephone Speaker Identification”, submitted to 
IEE Signal Processing  
 
B. Conference Papers:  
1. Zi-He Chen, Yuan-Fu Liao and Yau-Tarng Juang, “Eigen-Prosody Analysis 
for Robust Speaker Recognition under Mismatch Handset Environment”, 
ICSLP’2004.  
2. Jyh-Her Yang and Yuan-Fu Liao, “Unseen Handset Mismatch Compensation 
Based on A Priori Knowledge Interpolation for Robust Speaker Recognition”, 
ICSLP’2004.  
3. Jyh-Her Yang and Yuan-Fu Liao, “Unseen Handset Mismatch Compensation 
Based On Feature/Model-Space A Priori Knowledge Interpolation For Robust 
Speaker Recognition”, ISCLSP’2004  
4. Zi-He Chen, Yuan-Fu Liao and Yau-Tarng Juang, “Prosody Modeling and 
  133
16. X. Wang, K. Hirose, J. Zhang, N. Minematsu, C. Chiang, Y. Wang, Y. Liao, 
“Tone recognition of continuous Mandarin speech based on tone nucleus 
model and neural network,”, Technical Report of IEICE, SP2006, (2006-12)  
17. Chen-Yu Chiang, Xiao-Dong Wang, Yuan-Fu Liao, Yih-Ru Wang, 
Sin-Horng Chen, Keikichi Hirose, “LATENT PROSODY MODEL OF 
CONTINUOUS MANDARIN SPEECH”, to be appear in ICASSP’2007  
6.2 雛形展示系統 
 ITS Spoken Dialogue System  
 Year 2004: In-vehicle speaking assistant system (spoken dialog system 
for GPS car navigation around HsinChu Science Park and its 
neighborhood)  
 ITS video (mpg, 25.3M, Mandarin)  
 Year 2005: Enhanced in-vehicle speaking assistant system  
 Microphone array  
           video (wmv, 19.1M, Mandarin)  
 Robust parser and spontaneous speech recognition  
           video (wmv, 13.6M, Mandarin)  
 
 
  135
 Speaker Verification Security System  
          Demo                                                                                 
         Prof. Fujisaki, Tokyo Univ. visit our demo    
 Multi modal spoken dialog system-based multimedia retrieval 
system  
             Windows video (wmv, 15.8M, Mandarin) 
 Voice over IP (VOIP) system and real-time ETSI extended 
advanced DSR front-end codec (ETSI ES 208 212)  
             Windows video (wmv, 20.4M, Mandarin) 
  137
【11】 NIST - Speaker Recognition Evaluations, 
http://www.nist.gov/speech/tests/spk/index.htm 
【12】 Reynolds,D.A.,and Rose,R.C.,”Robust text-independent speaker 
 identification using Gaussian mixture speaker models,”IEEE Trans.  Speech 
 Audio Process. 3(1995), 91-108. 
【13】 Gauvain, J. L. and Lee, C.-H., “Maximum a posteriori estimation for 
multivariate Gaussian mixture observations of Markov chains, “ IEEE Trans. 
Speech Audio Process. 2(1994), 291-298 
【14】 R. Auckenthaler, M. Carey, and H. Lloyd-Thomas, “Score Normalization for 
Text-Independent Speaker Verification Systems,” Digital Signal Processing 
10,200, pp. 42-54. 
【15】 S. Furui, “Cepstral analysis technique for automatic speaker verification,” 
IEEE  Trans. Acoust., Speech, Signal Processing, vol. ASSP-29, pp. 254-272, 
Apr. 1981. 
【16】 M. G. Rahim and B. H. Juang: ‘Signal bias removal by maximum likelihood 
est-imation for robust telephone speech recognition’, IEEE Trans. On Speech 
andAudio Processing, vol. 4, no. 1, pp. 19-30, Jan 1996. 
【17】 D. A. Reynolds: ‘HTIMIT and LLHDB: Speech corpora for the study of 
handset transducer effects’, in Proc. ICASSP’97, vol. II, pp. 1535-1538, 1997. 
【18】 D. Reyolds, T. Quatieri and R.Dunn, “Speaker Verification Using Adapted 
Gaus-sian Mixture Models,” Digital Signal Processing, vol. 10, pp. 19-41, 
January 2000. 
【19】 R. Auckenthaler, M. Carey, and H. Lloyd-Thomas, “Score Normalization for 
Te- xt-Independent Speaker Verification Systems,“ Digital Signal Processing, 
vol. 10, pp. 42-54, January 2000. 
【20】 D. A. Reynolds, “Channel Robust Speaker Verification via Feature Mapping,” 
in Proc. ICASSP’03, vol 
【21】 D. A. Reynolds et. Al., “The superSID project; exploiting highlevel 
information for high-accuracy speaker recognition, ”Proc.ICASSP’03, vol, 
  139
andAudio Processing, vol. 4, no. 1, pp. 19-30, Jan 1996. 
【32】 D. A. Reynolds: ‘HTIMIT and LLHDB: Speech corpora for the study of 
handset transducer effects’, in Proc. ICASSP’97, vol. II, pp. 1535-1538, 1997. 
【33】 D. Reyolds, T. Quatieri and R.Dunn, “Speaker Verification Using Adapted 
Gaus-sian Mixture Models,” Digital Signal Processing, vol. 10, pp. 19-41, 
January 2000. 
【34】 R. Auckenthaler, M. Carey, and H. Lloyd-Thomas, “Score Normalization for 
Te- xt-Independent Speaker Verification Systems,“ Digital Signal Processing, 
vol. 10, pp. 42-54, January 2000. 
【35】 D. A. Reynolds, “Channel Robust Speaker Verification via Feature Mapping,” 
in Proc. ICASSP’03, vol 
【36】 D. A. Reynolds et. Al., “The superSID project; exploiting highlevel 
information for high-accuracy speaker recognition, ”Proc.ICASSP’03, vol, 
IV,pp.784-787,2003. 
【37】 NIST - Speaker Recognition Evaluations, 
http://www.nist.gov/speech/tests/spk/index.htm 
【38】 Reynolds,D.A.,and Rose,R.C.,”Robust text-independent speaker 
 identification using Gaussian mixture speaker models,”IEEE Trans.  Speech 
 Audio Process. 3(1995), 91-108. 
【39】 R. Auckenthaler, M. Carey, and H. Lloyd-Thomas, “Score Normalization for 
Text-Independent Speaker Verification Systems,” Digital Signal Processing 
10,200, pp. 42-54. 
【40】 L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition.  Prentice 
Hall, 1993． 
【41】 Reynolds,D.A.,and Rose,R.C.,”Robust text-independent speaker 
 identification using Gaussian mixture speaker models,”IEEE Trans.  Speech 
 Audio Process. 3(1995), 91-108. 
【42】 D. A. Reynolds et. al., “The superSID project: exploiting high-level 
information for high-accuracy speaker recognition,” Proc. ICASSP’03, vol. IV, 
國外差旅心得報告 - Interspeech 2007, Antwerp, Belguim, 27~31 August, 2007 
口語對話系統之強健式語者識別研究(3/3) NSC 95-2221-E-027-102 
主持人：廖元甫 台北科技大學電子系 
1. 簡介 
這次Interspeech 2007會議在比利
時的安特衛普舉行，主要是在於各方
面的語音科學技術，會議包括由國際
知名專家全體會議講座，教學，展覽…
等。在今年總投稿數為1269，但只接
受738篇論文，比例約為58%，這可能
是歷史上錄取率最低的一次，而我們
很榮幸可以在poster section 中，提出
Model Weighting-Based Method 論
文，所以下一章為簡介我們所發表之
論文，第三章為會場中比較有興趣在
韻律方面的論文，第四章為結論。 
 
2. Robust speech recognition 
2.1. Reference Model Weighting-base 
Method for Robust Speech 
Recognition 
快速即時的 HMM調適法。RMW
先在訓練時收集一組已看過的雜訊環
境MLLR轉換矩陣，用來表示雜訊環
境的空間。在測試時，則以最佳化方
式直接內插已收集的雜訊環境特性，
調適出最適合之辨認模型。 
 
圖一、RMW之方塊流程圖  
EMLLR與 RMW不同之處，主要
是 EMLLR進一步藉由主成分分析
（Principal Components Analysis，
PCA）來建立出雜訊環境特性的特徵
空間，並且減少所需估計的權重個數。  
 
學習之後 
 
當演算法自動停止時，錯誤率變得穩
定或增加。一段句子的音節結構是參
考我們已採取的F0曲線模型。 
 
  
圖五、Example of an HMM class and a 
F0 contour taken within this class. 
實驗結果對於 Sinica Tree-Bank語
料顯示出大多數韻律標籤標記和參數
估計對於這四種模型與我們所知的國
語韻律具有良好的匹配。
 
 
3.2. An Automatic Prosody Labeling 
Method for Mandarin Speech 
在這篇論文提出一種新的模型為
自動韻律標記方法對於國語語音。首
先，先介紹了四種模型來形容韻律標
籤標記關係，語音信號的韻律特徵和
語言特徵的相關文本。 
圖七、The decision tree for initial break 
type labeling 
 
 
 
3.3. F0 analysis of perceptual distance 
among Cantonese level tones. 
圖六、A conceptual prosody hierarchy of 
Mandarin speech. 
  
模型的訓練方法為： 這篇論文是介紹粵語的聲學分析
四種聲調高度和去尋找出感知距離中
量的關聯。 
 
 
 A Reference Model Weighting-based Method for Robust Speech Recognition 
YUAN-FU LIAO 1, JYH-HER YANG 2, CHI-HUI HSU 1, CHENG-CHANG LEE 1 and JING-TENG ZENG 1 
1 Department of Electronic Engineering, National Taipei University of Technology, Taipei 106, Taiwan 
2 Department of Communication Engineering, National Chiao Tung University, HsinChu, 300, Taiwan 
1yfliao@ntut.edu.tw 
 
 
Abstract 
In this paper a reference model weighting (RMW) method is 
proposed for fast hidden Markov model (HMM) adaptation 
which aims to use only one input test utterance to online 
estimate the characteristic of the unknown test noisy 
environment. The idea of RMW is to first collect a set of 
reference HMMs in the training phase to represent the space of 
noisy environments, and then synthesize a suitable HMM for 
the unknown test noisy environment by interpolating the set of 
reference HMMs. Noisy environment mismatch can hence be 
efficiently compensated. The proposed method was evaluated 
on the multi-condition training task of Aurora2 corpus. 
Experimental results showed that the proposed RMW 
approach outperformed both the histogram equalization (HEQ) 
method and the distributed speech recognition (DSR) standard 
ES 202 212 proposed by European Telecommunications 
Standards Institute (ETSI) 
 
1. Introduction 
 
Robustness of automatic speech recognition (ASR) to various 
noisy environments is the most important issue for mass 
deployment of ASR-based applications. To this end, speech 
enhancement or compensation methods are often applied to 
eliminate the noise effect. They can be roughly divided into 
two categories: blind and non-blind approaches. 
The blind ones try to eliminate/compensate the noise effect 
using the testing utterance only. They include two-pass 
Wiener filtering [1], histogram equalization (HEQ) [2] and 
parallel model combination (PMC) [3], etc. The non-blind 
ones use not only the input testing utterance but also a priori 
knowledge of noisy environments to eliminate/compensate the 
noise effect. 
Basically, the non-blind approach is preferable because it 
can take the advantage of a priori noisy environment 
knowledge to assist ASR in better dealing with the distortion 
of a specific noisy environment. This is especially true for the 
case when very limited data of the unknown test noisy 
environment is available. Two good examples of the approach 
are noise environment sniffing (NES) [4] and piecewise-linear 
transformation (PLT) [5]. The NES method detects, classifies 
and does smart tracking of the noisy environmental condition 
to direct the ASR engine to use the best local solution specific 
to each environmental condition. PLT clusters various types of 
noises based on their spectral property and trains one set of 
noisy acoustic HMM models for each cluster under a variety 
of SNR conditions. In recognition, the best matched HMM set 
is selected and further adapted by the maximum likelihood 
linear regression (MLLR) [6] method. 
But, it is generally difficult if not impossible to prepare a 
complete set of a priori noisy environment knowledge in 
advance. The noisy environments which are not seen in the 
training phase (aka unseen noisy environment) may therefore 
become potential sources of serious performance degradation 
for those non-blind speech enhancement/compensation 
methods. 
To take the advantage of a priori noisy environment 
knowledge and, at the same time, alleviate the problem of 
unseen noisy environments, a reference model weighting 
(RMW) method is proposed in this paper for fast online HMM 
adaptation. Its basic idea is to first collect a set of 
environmental characteristics from all seen noisy 
environments in the training phase to represent the space of 
noisy environments, and to then optimally estimate (in the 
sense of maximum likelihood) a suitable environmental 
characteristic for the unknown test noisy environment by 
interpolation using the set of the pre-collected environmental 
characteristics in the test phase. The estimated environmental 
characteristic can then be used to adapt the HMMs of the ASR 
engine to a new set of HMMs matching with the test 
environment for robust speech recognition. 
The paper is organized as follows. Section II describes the 
proposed RMW-based method of fast HMM adaptation in 
detail. Section III reports the experimental results evaluated on 
the multi-condition training task of Aurora2 corpus [7]. Some 
conclusions are drawn in the last section. 
 
2. RMW-based Fast HMM Adaptation 
 
Fig. 1 displays a block diagram of the proposed RMW-based 
fast HMM adaptation method for robust speech recognition. It 
is operating in two phases: a training phase to construct a 
priori noisy environment knowledge and a test phase to 
synthesize a set of test environment-adaptive HMM models for 
robust speech recognition. 
In the training phase, a noisy environment-independent 
HMM, niΛ , is first trained from the observed speech feature 
vectors of all seen noisy environments. Then, a set of noisy 
environment-specific HMMs, { }, 1 ~n n NΛ = , is generated 
by using MLLR to adapt niΛ  to each of these N seen noisy 
environments using its own observed speech feature vector 
sequence{ }, 1 ~nO n N= . The set of MLLR transformation 
matrices { }, 1 ~nW n N=  and the noisy environment-
vector of the k-th mixture component of the s-th state using 
[′Φ = ]′ ′ ; and 1,..., TNα α
 
( ) ( )
( ) ( )
,
, ,
, ,
1 1
( )
ˆ( ( ); , )
ˆ( ( ); , )
s k
t t s k s k s k
S K
t t j s j s j
i j
t
s s c N o t
i i c N o t
,
γ
α β μ
α β μ
= =
=
Σ⋅
Σ∑ ∑
 (5) 
 
is the occupancy probability for ( )( ) ( ), ( )t s t k tθ =  calculated 
by the forward-backward algorithm using the adapted HMM 
 derived from  with the old interpolation weight vector 
. 
Λˆ niΛ
Φ
By ignoring some terms not related to , Eq. (5) can be 
simplified and expressed by 
′Φ
 
( )
( ) ({ }
,
1 1 1
1
, , ,
1 ( )
2
( ) ( )
T S K
s k
t s k
T
s k s k s k
Q t
o t o t
γ
= = =
−
′ ′Φ = − ⋅
′ ′− Φ Σ − Φ
∑∑∑
Z Z )
 (6) 
 
Then the new interpolation weight vector  can be found by 
solving the following linear system: 
′Φ
 
( )
( )1, , , ,
1 1 1
( ) ( ) 0
T S K
T
s k s k s k s k
t s k
Q
t o tγ −
= = =
′ ′∂ Φ =′∂Φ
⎡ ′Σ − Φ⎣∑∑∑ Z Z ⎤ =⎦
⎫⎬⎭
(7) 
 
A close form solution can hence be obtained and expressed by 
 
1
1
, , , ,
1 1 1
1
, , ,
1 1 1
( )
( ) ( )
T S K
T
s k s k s k s k
t s k
T S K
T
s k s k s k
t s k
t
t o t
γ
γ
−
−
= = =
−
= = =
⎧′ ⎡ ⎤Φ = Σ ⋅⎨ ⎣ ⎦⎩
⎡ ⎤Σ⎣ ⎦
∑∑∑
∑∑∑
Z Z
Z
 (8) 
 
The expectation and maximization steps of the EM algorithm 
(i.e., Eqs.(5)-(8)) are iteratively applied until the likelihood 
converges. 
 
3. Experimental Results on Aurora2 
Database 
 
The proposed RMW-based robust speech recognition method 
was evaluated on the multi-condition training task of Aurora2 
corpus. 
3.1. Multi-Condition Training Task 
Aurora2 database was generated based on a corpus of English 
connected digit strings (subset of TIDIGIT corpus) sampled at 
8 kHz. Eight different types of noisy environments, including 
(1) subway, (2) babble, (3) car, (4) inhibition, (5) restaurant, (6) 
street, (7) airport and (8) railway, and two different 
communication channels, as representatives of real-world 
environments and channels in mobile phone application 
scenario, were used. These eight noises were artificially added 
to the original noise-free TIDIGIT digit strings with various 
signal-to-noise ratios (SNRs). Therefore, for each type of 
noise, a set of clean utterance and 5 sets of noisy utterances 
with SNRs ranging from 0~20 dB were produced. 
In the multi-condition training task of Aurora 2 corpus, the 
eight noises are divided into 4 disjoint sets, i.e., one training 
and three test sets, sets A, B and C. For the training set, only 
noise types (1)~(4) are available to build the speech recognizer. 
Test set A also includes noise types (1)~(4), but set B has 
noise types (5)~(8). On the other hand, test set C also has 
noise types (1)~(2) but they go through different 
communication channels compared with the training set. For 
each type of noise, a set of clean utterances and 5 sets of noisy 
utterances with SNRs ranging from 0~20 dB are used for 
evaluation. It is noted that set A, B, and C represent the 
scenario of seen noisy environment, unseen noisy environment, 
and unseen communication channels, respectively. 
In all following evaluations, we simply followed the 
experimental conditions of Aurora 2 protocols by using 
Hamming window of 25 ms with a frame shift of 10 ms for 
framing. 39-diemsional MFCCs were used to train 16-state 
word HMMs with 3 mixtures in each state. 
3.2. Analysis of the Seen Noisy Environments 
In the Aurora2 multi-condition training task, only the first 4 
types of noises were available for building the speech 
recognizer. For each noise type, there are 6 different SNRs 
including “clean”, 0 dB, 5 dB, 10 dB, 15 dB and 20 dB. 
Therefore, there are in total 24 (4*6) different seen noisy 
environments. These 24 seen noisy environments were used to 
train 24 sets of noisy environment-specific MLLR 
transformation matrices (6 MLLR matrices per set) and treated 
as the a priori noisy environment knowledge. 
The set of 24 environment-dependent MLLR transformation 
matrices were re-arranged to form 24 MLLR super-vectors. 
These 24 super-vectors were analyzed using the principle 
component analysis (PCA). Fig. 2(a) shows the distribution of 
the 24 training noisy environments in the first two dimensions 
of the constructed eigen-space. Furthermore, Fig. 2(b), (c) and 
(d) show the projection of the test sets A, B and C, 
respectively, to the same eigen-space. 
From those figures, it is very interesting to see that the first 
dimension of the constructed eigen-space is highly related to 
the SNR value, while the second dimension is strongly related 
to the type of noise. This analysis results suggests that it is 
possible to interpolate a set of MLLR matrices for fast HMM 
adaptation. 
3.3. Performance Evaluation 
Several noise compensation approaches were evaluated and 
compared on the test sets A, B and C of Aurora 2 database 
including (1) ETSI DSR front-end ES 201 050 (ETSI), (2) 
ETSI advanced DSR front-end ES 202 212 (ETSI-advanced), 
(3) histogram equalization (HEQ), and three schemes of the 
proposed RMW-based method using (4) hard-decision- 
(RMW-1), (5) a posteriori- (RMW-2) and (6) ML-based 
optimal weights estimation (RMW-3). 
Performance comparison and summary of those approaches 
are shown in Fig. 3. It can be seen from the figure that the 
ETSI-advanced method, which uses blind two-stage Wiener 
filtering, and the HEQ method performed well compared with 
the ETSI method. However the RMW-based schemes, 
especially the RMW-2 and RMW-3 schemes could further 
improve the recognition rates. So the RMW is a promising 
approach for fast noisy environment adaptation.  
