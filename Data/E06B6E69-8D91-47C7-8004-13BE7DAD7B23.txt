 2
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
 
本計劃旨在研究人類對材質的感知及視覺行為研究，希望能建立對合成材質
的視覺行為模型以及改善現有的材質處理及相關的電腦圖學演算法。我們藉
由眼動追蹤儀蒐集資料配合機器學習理論建立人類對材質的視覺行為模型，
此為合成材質研究中的第一個視覺凝視模型。實驗顯示，我們的模型比影像
研究常使用的 Saliency Map 更能準確預測在合成材質的視覺焦點。我們將來
希望能進一步建立對三維模型乃至動畫的視覺凝視模型，相信對電腦圖學及
應用感知研究領域將有更大貢獻。另外，由於此研究的啟發，我們亦改善與
材質處理應用有關的電腦圖學技術，例如將材質合成技術應用於非擬真顯像
(NPR)的應用，以及在材質空間實現即時透明材質顯像的技術。最近，我們更
將其應用在材質雙向函數(BTF)資料的最佳化參數表示上，可以有效壓縮材質
雙向函數並能確保其在即時顯像時的影像品質。以上三項技術，均已發表或
被接受在電腦圖學及視覺的重要期刊 IEEE TVCG、IEEE TPAMI 及學術會議
Eurographics 中。本計劃相關的研究結果，相信在未來應能對電腦圖學領域
有所貢獻。 
附件二 
 4
四、其他 
 
 
參與 Siggraph Asia 2009 會議與其他各校教授(後排)及學生(前排)合影 
 
 
 
 
 
 
 
FINAL REPORT OF NSC 96-2221-E-009-152-MY3 2
recorded by the eye-tracking system. 79% of predicted fixa-
tions match with the actual fixations. We also compared our
model with saliency map by testing it on the same textures.
Only 55% of fixations predicted by the saliency map match
with the actual fixations.
The rest of this report is organized as follows. We briefly
review texture synthesis algorithm and related work in visual
attention in Section II. In Section III, we describe how we
get the feature vectors and our visual attention model for
textures. In Section IV, we describe the procedures of our eye-
tracking experiments. We show the evaluation method and our
results in Section V. We also briefly introduce other results
of this project on improving texture processing algorithms
and computer graphics applications in section VI. Finally,
conclusion and future work are presented in VII.
Fig. 1. Schematic illustration of our model. In the training phase, we
decompose each texture into feature vector by computing the structure error
of the texture. We also collect eye-tracking data for the texture and transform
them into gaze density map. Texture vectors and gaze density map are passed
to a learner which learns the association between them.
II. RELATED WORK
We will briefly introduce texture synthesis algorithms and
methods being used to evaluate texture synthesis results. Then,
we will introduce some visual attention models related to our
work, and their applications.
A. Texture Synthesis and Evaluation Method
Texture synthesis is an algorithmic process that generates
images which are perceptually similar to sample images while
containing sufficient variations. For a through survey on tex-
ture synthesis algorithms, we refer readers to [11]. A method
of evaluation on texture synthesis result has been proposed
in [7]. They proposed a method to systematically compare
the performance of four texture synthesis algorithms on near-
regular textures. They used two evaluation methods, one is
based on the regularity of textures, the other is to ask human
subjects to examine and rate the overall quality of the synthesis
result in comparison to the input texture. In the user evaluation
test, they found strong correlations between the preservation
of global regularity and the high user evaluation scores.
B. Texture Perception
Texture and its effect on human visual perception has been
the subject of interest to the vision community, which has been
extensively studied in multiple disciplines including computer
science, psychophysics, and psychophysiology. In computer
vision, texture studies focus on simulating human texture
perception via computing technologies and deriving appro-
priate mathematical representations of textures to facilitate
computerized texture processing [12]–[15], classification [16]
and segmentation [17]–[19].
In psychophysics and psychophysiology, texture studies
focus on neural processes involved in visual perception, with
a great amount of efforts on understanding the mechanisms
of texture detection and segregation. Texture pattern consists
of simple geometric elements such lines, dots, circles, or
rectangles are used as visual stimuli since the texture patterns
can be easily controlled. Julesz et al. [17], [20] empirically
investigated the perceptual significance of various image statis-
tics of texture patterns in order to determine how the human
low-level visual system responds to the variation of a particular
order statistic. Researchers in psychophysics show that texture
segregation is not determined by the presence or absence of
local features, but rather by spatial gradients [21]–[24].
Although there are many studies on human texture percep-
tion, most of them focus on the low-level vision functions,
such as texture detection and segregation, and use artificial
textures made of random dots or repeated shapes in the
experiments. The developed texture perception models are
not suitable for computer graphics applications in which
natural, complex textures are commonly used. Therefore, it is
necessary to develop a human texture perception model that
fits the need for computer graphics application.
C. Visual Attention
In the early stage of visual attention research, it was often
considered to be related to the subjective awareness of the
world. Recently in cognitive neuroscience, visual attention
has been divided into separated subsystems being performed
independently and inter-related. Tresiman and Gelade [25] pro-
posed a theory which describes visual attention as a two-stage
process: bottom-up stage and top-down stage. The bottom-up
stage is completely stimulus driven, which means an observer
give his/her attention pre-attentively. The top-down stage is a
volitional stage of focusing on objects which are relevant to
the observer’s thought, reasoning and memory. The bottom-
up stage provides the context over which the top-down stage
operates. Hence, the bottom-up stage is fundamental of visual
attention.
Many models have been proposed to describe visual at-
tention. Saliency map [6] is a computational model which
simulate the bottom-up process of visual attention. It includes
feature channels sensitive to color contrast, luminance contrast,
and four preferred orientations [26]. These features detect
salient part in the visual stimulus using the center-surround ar-
chitecture. The generated feature map will then be normalized
to mimic the lateral inhibition effect. The sum of the feature
map for each feature channel results in the conspicuity map,
which will also be normalized and then summed up to obtain
the saliency map that quantifies visual attention.
Since saliency map demands high computational cost, other
models have been proposed to solve this problem. Spectral
residual (SR) approach based on Fourier Transform was pro-
posed by Hou and Zhang [27]. Their approach is simpler
and faster than [6]. In [27], the difference of SR between
the original signal and a smoothed one in the log amplitude
FINAL REPORT OF NSC 96-2221-E-009-152-MY3 4
Fig. 3. (a) is the original image of the two textures. Left one is the input
texture, right one is the synthesis result from the input texture. (b) shows the
edge images of (a). In (b), we calculate the difference of Ri with Ij which
is the most similar patch to Ri in the edge image of the input texture. We
visualize these error values as a structural error map as shown in (c). Brighter
region means larger error. The red rectangle shows the structural error of Ri.
We measure the structural error of a synthesized textures
by comparing the differences between the edges detected in
the synthesized texture and the input texture. We acquire
many image patches from a synthesized texture by uniformly
sampling the synthesized texture. For a near-regular texture,
the size of the patches is equal to the size of the tile [41];
For an irregular texture, the size of patch is manually set
to be the average size of its texture elements. We denote
(xR1, yR1), (xR2, yR2), ..., (xRn, yRn) as the centers of the
patches R1, R2, ..., Rn, respectively. These patches are 30
pixels apart along the horizontal and vertical directions. The
ranges of xRi and yRi are 1 ≤ xRi ≤WR and 1 ≤ yRi ≤ HR,
respectively. Here, WR and HR are the width and height of
the synthesized texture, respectively. Figure 4 (b) shows this
sampling process.
(a) (b)
Fig. 4. (a) shows some patches on the edge image of synthesized texture.
(b) shows some patches on the edge image of input texture.
To compute the structural error of each Ri, we compare
it with the edge image of its input texture. For each Ri,
we find the best matched patch in the edge image of the
input texture. We denote these patches in the edge im-
age as I1, I2, ..., Im(with the same patch size with Ri) and
(xI1, yI1), (xI2, yI2), ..., (xIn, yIn) as the centers of these
patches. A center of a patch for Ii can be represented by
(xIi, yIi),∀i
{
1 ≤ xIi ≤WI
1 ≤ yIi ≤ HI , (1)
whereWI andHI are the width and height of the input texture.
Figure 4 (a) illustrates the patches in the edge image of the
input texture. Note that the center of Ij is moved pixel by
pixel in finding the best match process.
As the portion of patch Ij that is not inside the input texture
will bias the computation of the structure difference ‖Ij −
Ri‖, we multiply ‖Ij − Ri‖ by a mask to mark the valid
region. Another purpose of the mask is to calculate the area
of the valid region. We divide the structural difference by the
area of the valid region of a patch. This avoids the area bias
the computation of structural difference. Hence, the structural
error value,
SEi = min
j
S(‖(Ij −Ri)‖ ∗Maskj)
area(Maskj)
, j ∈ 1, 2, ...,m, (2)
where S(·) denotes the sum of all elements of a matrix, ‖ · ‖
is the absolute value, ∗ is the element-wise product of two
matrices, and Mask mark the valid region.
After we get the structural error value of each Ri, we
normalize these values among all textures synthesized from
the same input texture. The normalized values nSEi of the
same synthesized texture will be collected to form a feature
vector,
FV =

nSE1
nSE2
.
.
.
nSEn
 , (3)
for the synthesized texture. We also visualize it as which can
be seen in Figure3(c). The brighter part shows that the error
is larger in that area, the darker part shows that the error is
smaller.
B. Top-down Stage: Association
The top-down process is simulated by a mapping function
that associates fixations with feature vectors. Many machine-
learning approaches can be used to learn this mapping
function. These models include multi-layer neural networks,
support-vector machines, and the Expectation/Maximization
algorithm. Here we choose the Self-cOnstructing Neural Fuzzy
Inference Network (SONFIN) [40] as our model for two
reasons. First, SONFIN can find its structure and parameters
to model an economic network size automatically. Second, the
learning speed as well as the modeling ability of the SONFIN
are all appreciated.
Figure 5 shows the structure of the SONFIN model. There
are six layers within the structure of the SONFIN. This six-
layered network realizes a fuzzy model of the following form:
Rule i: IF xi is Ai1 and xn is Ain
THEN y is a0i + ajixj + · · ·
where Aij is the fuzzy set of the jth linguistic term of input
variable xj , and aji’s are the consequent parameters. a0i is
the center of a symmetric membership function on y. Let u(k)i
and o(k)i denote the input and output of the ith node in layer
k, respectively. The followings describe the functions of each
layer of the SONFIN.
Layer 1:
o(1) = u(1)i . (4)
FINAL REPORT OF NSC 96-2221-E-009-152-MY3 6
(a) (b) (c)
Fig. 7. (a) The eye-tracking data of a synthesized texture collected from
all testers. (b) set salient 50% as threshold. (c) 40 samples from positive and
negative fixations (20 for positive and 20 for negative).
5000. One problem is that we might gather ambiguous samples
which would cause bad training result, so we run sampling and
training for three times to guarantee a better result.
IV. EYE TRACKING EXPERIMENT
Eye tracking is the process of measuring either the point
of gaze or the motion of an eye relative to the head. It
has been broadly used in many fields of study in order to
understand human gaze behaviors. Currently, the most widely
used technique is the video-based eye tracking. In a video-
based eye tracker, when a viewer looks at some kind of
stimulus, a camera would focus on one or both eyes of the
viewer and record their movement. Most modern eye-trackers
locate the center of the pupil using contrast and create a
corneal reflection (CR) using infrared and near-infrared non-
collimated light. The vector between the center of pupil and
CR can be used to compute gaze intersection with a surface
after a simple calibration for an individual. Bright Pupil and
Dark Pupil are two general types of video-based eye tracking
techniques. Their difference is the location of the illumination
source with respect to the optics. If the illumination is coaxial
with the optical path, then the eye acts as a retroreflector as the
light reflects off the retina creating a bright pupil effect similar
to red eye. In contrast, if the illumination source is offset
from the optical path, then the pupil appears dark because
the retroreflection from the retina is directed away from the
camera. Bright Pupil tracking creates greater iris/pupil contrast
allowing for more robust eye tracking. The eye movements
recorded by eye trackers are typically divided into fixations
and saccades, where fixations refer to the eye gaze pausing in
a certain position, and saccades refer to the eye gaze moving to
another position. We recorded the fixations of human observers
while they are watching, comparing and judging a synthesized
texture. The collected eye-tracking data are utilized to train
our model. In this section, we will describe the settings of our
experiment and how we process the eye-tracking data.
A. Experiment Settings
Our experiment was done on the Tobii T120 Eye-tracker,
which is a contact free gaze measurement device, as shown in
Figure 8. Tobii T120 can tolerate both large and rapid head
movement, so it ensures a natural and relaxing environment
for the subject. This helps to acquire a more realistic response
from human subjects. The following are the specifications of
Tobii T120 eye-tracker:
Data Rate: 120Hz
Accuracy: typical 0.5 degrees
Head Movement Error: typical 0.2 degrees
Head Movement Box: 30*22cm at 70cm
Tracking Distance: 50-80 cm
Max Gaze Angles: 35 degrees
Top Head-motion Speed: 25 cm/second
Screen Size: 17” TFT
Screen Resolution: 1280*1024 pixels
Display Colors: 16.7M
We recruited 20 undergraduate and graduate students to
participate our experiment. After excluding those subjects
whose eye movements cannot be successfully tracked, there
are 18 subjects remained. The remaining 18 subjects include
14 males and 4 females, aged from 19 to 24 with normal
or corrected to normal vision. None of them has relevant
knowledge in texture synthesis. No subjects have been exposed
to this experiment more than once, so the learning effects can
be avoided. They are all naive to the purpose of the whole
process.
The images used in this experiment are eleven different
structural textures as shown in Figure 9and 10, including
near-regular and irregular textures. Each texture has four syn-
thesized textures generated by graph-cut [42], image-quilting
[43], patch-based texture synthesis [44], or near-regular texture
synthesis [45]. There are 44 synthesized textures in total.
Figure 9 and 10 show all the input and synthesized textures
used in our experiment.
Fig. 8. This photo shows Tobii T120 Eye-tracker and experiment environ-
ment.
Figure 11 illustrates the procedure of our eye-tracking
experiments. In the beginning of the experiment, subjects were
asked to sit down with a position that they feel comfortable
to look at the screen. The viewing distance from a subject
to the screen is controlled within 50-80 cm with the screen,
which is acceptable by the eye tracker. All subjects have to
do calibration for their eye-positions. During the procedure, a
subject was asked to look at specific points on the screen. The
resulting information is then integrated in the eye model and
the gaze point for each image sample is calculated. When the
procedure is finished the quality of the calibration is illustrated
by green line of varying length. If the accuracy of calibration
pass the requirement, we can start the following procedure;
FINAL REPORT OF NSC 96-2221-E-009-152-MY3 8
Fig. 12. This image shows what subjects really see on the screen.
fixation points on each texture. The score of each texture is
also recorded.
V. RESULTS
In this section, we will first introduce our evaluation method
and then the visual attention prediction results on textures.
We then compare our model with saliency map. A modified
saliency map is also being evaluated with the same method.
A. Evaluation Method
To quantitatively evaluate how accurate our model’s predic-
tion matches observers’ actual fixation positions, we use two
metrics, normalized scan-path saliency (NSS) [46] and receiver
operating characteristic (ROC). Figure 13 shows an example
result of NSS and ROC of the prediction rate on a synthesized
texture. The NSS method is defined as the response value at
the current fixations on a model’s predicted gaze density map
that has been normalized to have zero mean and unit variance.
Here, we put all observers’ fixations on the predicted gaze
density map and compute the average of all response values
of the current fixations. This average value is called average
NSS. We will compute the prediction rate P%, which means
average NSS is above P% of the response value across the
entire gaze density map. Higher prediction rate means better
prediction. Another method is ROC, which is well-known
for being a useful technique for organizing classifiers and
visualizing their performance. ROC graphs are commonly used
in medical decision making. Here we set a threshold between
a range from 5% to 100%. For each threshold , two values
are calculated, the True Positive Ratio (the number of outputs
greater or equal to the threshold, divided by the number of one
targets), and the False Positive Ratio (the number of outputs
less than the threshold, divided by the number of zero targets).
Note that the more each curve hugs the left and top edges of
the plot, the better the classification.
B. Visual Attention Prediction on Structural Textures
We designed two sets of experiments. In the first set, two
synthesized textures were picked as training data from each
kind of synthesized textures, and then the remaining two
(a) (b)
(c) (d)
Fig. 13. (a) The original image E1 shown to the subjects. (b) The
average normalized response value across all fixations is taken as the average
normalized scan-path saliency (NSS). The dashed line shows average NSS,
which is compared against the distribution of response value across the entire
gaze density map (gray histogram). (c) The normalized gaze density map of
(a), where red circles represent a series of fixations of a subject. (d) The ROC
curve defined by true positive ratio and false positive ratio. The more each
curve hugs the left and top edges of the plot, the better the classification.
synthesized textures were tested to predict where human would
be interested. That is, we test how a trained model performs
on unseen textures. We call the first set of experiment New-
Texture-Prediction (NTP). In the second set, all subjects were
divided into a training group and a testing group. We predict
the testing group’s fixation by the model trained from the
training group. That is, we evaluate how a trained model
perform on unseen subjects. We call the the second set of
experiment New-Subject-Prediction (NSP).
We tuned the parameters for the SONFIN to obtain the
best results. We found that the value of membership threshold
and the distribution of sampling play an important role in our
experiments. The member threshold affecting the number of
rules is set as 0.005 and the learning rate is set as 0.003. To
avoid overtraining, the minimal error is set to 0.05 and the
maximal number of iteration is 5000. One problem is that we
might gather biased samples which would cause bad training
result, so we repeated sampling and training for three times
to guarantee a better result.
C. New-Texture-Prediction Experiment
In this experiment, we compare our results with the results
generated by the saliency map. One can observe that Saliency
map usually cannot predict fixations well. Its predicted fix-
ations scatter in the entire synthesized textures (Figures 14-
15) Our model has NSS average predictive rate of 79.34%
and saliency map only has 56.00% (Figures 16). Some of
our results do not have a higher predictive rate than saliency
map since those results are irregular textures, on which our
structural error feature does perform well. Figure 17 shows
FINAL REPORT OF NSC 96-2221-E-009-152-MY3 10
(a)
(b)
Fig. 16. This figure shows the NSS average rate of our model and saliency
map for each synthesized texture. (a) shows the results for texture A, B, C,
D, E, F. (b) shows the results for G, H, I, J, and K. Most of our result
have obviously higher predictive rate than saliency map except for I3, whose
predictive rate is equal or lower than the saliency map’s. This is because
textures I is an irregular texture, its higher variations make it harder to be
extracted to have valid features.
Fig. 17. This graph shows the comparison of our model and saliency map
in ROC curve. Since that the more each curve hugs the left and top edges the
better the classification, we find our model more accurate than saliency map.
of a given artwork can be achieved. This new approach works
particularly well for a rich variety of brushstrokes ranging from
simple 1D and 2D line-art strokes to very complicated ones
with significant variations in stroke characteristics. During the
rendering or animation process, the coherence of brushstroke
textures and color changes over 3D surfaces can be well
maintained. With PAM, we can also easily generate the illusion
of flow animation over a 3D surface to convey the shape of
a model. Figure 23 shows three examples of our results. This
work was published in IEEE Transactions on Visualization and
Computer Graphics in 2008 [1].
(a)
(b)
Fig. 18. The NSS average value of Modified Saliency Map is only 57%, not
higher than the result of original saliency map. Also, compare to our model,
the results of modified saliency map are very unstable. The lowest accuracy
is around 10% but the highest is more than 90%. This comparison shows that
saliency map has lower correlation to the fixations on synthesized textures
than our structural error feature.
(a) (b)
(c) (d)
Fig. 19. (a) Texture B2 is well-synthesized and (b) Texture B1 is poor-
synthesized. (c) and (d) show the overlaid fixation density map of (a) and (b)
respectively. B2 has higher structural similarity with the input texture than B1.
One can observe from (c) that subjects are more likely to focus on the center
of a well-synthesized texture, but they would focus on both the high structural
error and the low structural error regions while viewing a poor-synthesized
texture. This surely affects the prediction capability of our model.
FINAL REPORT OF NSC 96-2221-E-009-152-MY3 12
(a)
(b)
Fig. 22. The testing result on Group 2 using the model trained on Group
1. The figure shows that the results of G, H, I, J, K are much more unstable
than the others for the irregularity.
Fig. 23. The input painted images and our results. (a) Rendering with ”Palais
des Papes Avignon” painted by Paul Signac. (b) and (c) Rendering with ”The
Starry Night” painted by Van Gogh. Originally, the models in (a) and (b) are
uncolored, and the model in (c) is colored.
Fig. 24. Real-time rendering results using our BTF models.
the textures characteristics and the model can find the
association between human eye-tracking data and their
textures characteristics. Once our model learned the
association between them, we can use it on new synthesized
textures, to predict their fixation locations. As saliency map
is also designed to predict viewers visual attention, we also
use it to predict our synthesized textures, and find it does
not have satisfying results, since it did not include human
information in their model. Although our model works
well on near-regular textures, our model remains limited
to processing irregular textures. This limitation should be
able to be solved by finding better feature vectors for both
near-regular and irregular textures.
As we did collect users ratings for each synthesized textures
while we were collecting their eye-tracking data, we can also
include these information in our model, or we can find an
association between their gaze behavior and their ratings for
each synthesized texture. Elhelw et al. use Markov transition
matrix to investigate the underlying viewing strategy of the
subjects in [39]. They labeled different features in the stimuli
as the states for Markov model, and try to find subjects
viewing transitions from one feature to the others. We can also
label our features from textures as regularity-preserved and
regularity-violated, and find the relation between the transition
of fixations and user’s rating.
ACKNOWLEDGMENT
We would like to thank Prof. Chen-Chao Tao at National
Chiao Tung University for providing the eye-tracking facility
for us to conduct eye-tracking experiments.
REFERENCES
[1] C.-R. Yen, M.-T. Chi, T.-Y. Lee, and W.-C. Lin, “Stylized rendering
using samples of a painted image,” IEEE Transactions on Visualization
and Computer Graphics, vol. 14, pp. 468–480, March 2008.
[2] C.-W. Chang, W.-C. Lin, T.-C. Ho, T.-S. Huang, and J.-H. Chuang,
“Real-time translucent rendering using gpu-based texture space impor-
tance sampling,” Comput. Graph. Forum (Eurographics 2008), vol. 27,
no. 2, pp. 517–526, 2008.
[3] Y.-Y. Tsai, W.-C. Lin, K. B. Cheng, J. Lee, and T.-Y. Lee, “Real-time
physics-based 3d biped character animation using an inverted pendulum
model,” IEEE Transactions on Visualization and Computer Graphics,
vol. 16, no. 2, pp. 325–337, 2010.
[4] Y.-T. Tsai, K.-L. Fang, W.-C. Lin, and Z.-C. Shih, “Modeling bidirec-
tional texture functions with multivariate spherical radial basis func-
tions,” To appear in IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2011.
[5] J. M. Henderson and A. Hollingworth, “high-level scene perception,”
Annual Review of Psychology, vol. 50, pp. 243–271, 1999.
[6] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual
attention for rapid scene analysis,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 20, no. 11, pp. 1254–1259, 1998.
[7] W. Lin, J. Hays, C. Wu, Y. Liu, and V. Kwatra, “Quantitative evaluation
of near regular texture synthesis algorithms,” in CVPR ’06: Proceedings
of the 2006 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, 2006, pp. 427–434.
[8] R. J. Peters and L. Itti, “Beyond bottom-up: Incorporating task-
dependent influences into a computational model of spatial attention,”
in Proc. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2007.
[9] A. L. Yarbus, “Eye movement during perception of complex objects,”
Eye movement and vision, plenum press, pp. 171–196, 1967.
國科會補助計畫衍生研發成果推廣資料表
日期:2010/12/31
國科會補助計畫
計畫名稱: 圖學導向材質感知模型與感知最佳化材質處理演算法的開發
計畫主持人: 林文杰
計畫編號: 96-2221-E-009-152-MY3 學門領域: 計算機圖學
無研發成果推廣資料
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本計劃成果以期刊或國際會議發表為主,惟發展過程中與國內外圖學領域相關
研究學者均有良好交流,隨著計劃成果發表於重要期刊及學術會議,相信本計劃
日後對於此領域將有所貢獻。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
