目錄 
1. 中文摘要………………………………………………....1 
2. 英文摘要…………………………………………………2 
3. 前言………………………………………………………3 
4. Certainty-Enhanced Active Learning for Improving 
Imbalanced Data Classification…………………………..6 
5. Certainty-Based Active Learning for Pooled Datasets…...6 
6. An aggressive margin-based algorithm for incremental 
learning………………………………………………….28 
附錄 1…………………………………………………………40 
I
2、英文摘要 
In practical applications, active learning and incremental learning are two kinds of 
useful approaches to decrease the requirement of labeling samples by human experts 
and to periodically update existing classifiers using new datasets. We discuss several 
encountering practical problems, including bias in label prediction, efficiency in 
classifier adjustment in Section 4 (2011 ICDM workshop), 5 (ready to submit), and 6 
(submitted to 2012 PAKDD). 
In active learning algorithms, informative samples are usually queried for true labels 
according to the disagreement of existing hypotheses. However when the dataset has 
skewed class membership, the imbalanced data classification problem is caused in 
active learning. In this project, we design two active learning approaches for 
pool-based and streaming-based datasets, respectively. For each unlabeled sample we 
propose to utilize only local behavior in the certainty-based neighborhood to generate 
the error minimization hypotheses. Notably there is no need to define the sizes of 
neighborhoods in advance. In our analysis and experimental results, it is presented 
that our approaches have the ability of dealing with the imbalanced data classification 
problem in active learning. 
In incremental learning, the classification model is incrementally updated using the 
small datasets. Different with existing methods, our approach updates the current 
classifier according to each sample in the dataset, respectively. The classifier is 
updated by adjusting more than the margin of each sample. Then the new classifier is 
generated by carefully analyzing classifier adjustments caused for labeled samples. In 
details, we formulate simple constrained optimization problems and then the updated 
classifier is the solution derived using Lagrange multipliers. The experimental results 
are shown that our incremental learning approach is suitable to be applied for the 
requirement of frequently adjusting the existing classifiers. 
keywords: active learning, agnostic active learning, imbalanced data 
classification, incremental learning, passive-aggressive online learning 
  
2
classifiers [12,13] could not be quickly adjusted when encountering diverse sample 
distribution. In this project, we are motivated by the simplicity of online 
Passive-Aggressive (PA) algorithm [14]. For using the new labeled sample, PA 
updates the classifier by adjusting more than the margin of this sample. Thus in our 
approach, while a sample is used for updating and its sign is incorrectly predicted, the 
classifier adjustment is aggressively achieved within the margin. And similar with 
other incremental learning methods, our updated classifier shall correct the prediction 
mistakes of the current classifier as many as possible. In details, we formulate the 
simple constrained optimization problem for each sample and then the candidate 
updated classifier is the solution derived using Lagrange multipliers. Particularly the 
selected new classifier, updated by the suitable margin, shall obtain the best 
classification accuracy on the collected dataset. This selection strategy is able to avoid 
the new classifier being extremely specific to the previous one. Moreover our 
incremental approach does not maintain previously seen samples so the updated 
classifier could flexibly adapt the diverse sample distribution. In the following, this 
approach is introduced in Section 6. 
 
Reference 
[1] S. Ertekin, J. Huang, L. Bottou, and L. Giles. Learning on the Border: Active 
Learning in Imbalanced Data Classification. In ACM Conference on Information and 
Knowledge Management, pp. 127-136, Lisboa, Portugal, November 2007. 
[2] M. Li an I.K. Sethi. Confidence-based active learning. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 28(8): 1251-1261, 2006. 
[3] M.F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In 
International Conference on Machine Learning, pp. 65-72, Pennsylvania, USA, June 
2006. 
[4] S. Hanneke. A bound on the Label Complexity of Agnostic Active Learning. In 
International Conference on Machine Learning, pp. 353-360, Oregon, USA, June 
2007. 
[5] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active 
learning. In International Conference on Machine Learning, pp. 49-56, Montreal, 
Canada, June 2009. 
[6] A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic Active Learning 
without Constraints. In Neural Information Processing Systems, Vancouver, Canada, 
December 2010. 
[7] Utgoff, P. E.:Incremental Induction of Decision Trees. Journal of Machine 
Learning, vol. 4, pp. 161-186, 1989. 
[8] Mohamed, S., Rubin, D., Marwala, T.: Incremental Learning for Classification of 
4
4、Certainty-Enhanced Active Learning for Improving Imbalanced Data 
Classification 
This approach will be published in International Conference on Data Mining 
Workshops (Workshop on Domain Driven Data Mining) , 2011. 如附錄 1. 
 
5、Certainty-Based Active Learning for Pooled Datasets 
 
I. INTRODUCTION 
Since labeling data by human labors is a time-consuming job, many active 
learning approaches [1], [2], [3], [4], [5], [6], [7] have been proposed to decrease the 
requirement of labeled samples. The framework of active learning is to identify 
informative unlabeled data and then query their true labels from human experts or 
oracles. Relatively, passive learners obtain all data labels in advance. The objective of 
active learning is to query only a small number of unlabeled data and perform as well 
as passive learners. While the noisy oracles exist (agnostic cases) and all unlabeled 
data are collected in a pool, several approaches [1], [2], [8] have been designed to 
query unlabeled samples for their true labels iteratively depending on existing 
hypotheses. Generally those queried samples are near the current class boundary 
(current best hypothesis), or their predicted labels are disagreed among existing 
hypotheses. However there are three challenges worth to be concerned for practical 
applications: 
 
z Practical use. A2, [8], [9] is designed to maintain a candidate set of hypotheses 
and query the true label of the unlabeled sample of which predicted labels are 
disagreed by any two existing hypotheses. Thus A2 uses the theoretical analysis 
to guarantee the hypothesis that potentially is the optimal hypothesis would not 
be eliminated in the active learning process. And it is theoretically shown that 
they have polynomial improvement over passive learners on required labeled 
samples. However we observe that it is difficult to properly define the hypothesis 
set in practical applications. And the queried samples, of which predicted labels 
are disagreed by any two existing hypotheses, should be decided using specific 
importance measurements.  
z Theory-guarantee. In some approaches like [1], [2], unlabeled samples are 
queried by probabilities that are measured using their distance from the class 
boundary or by the diversity of their predicted labels. Those measurements are 
properly designed and effective for real-world datasets, but they are not derived 
from learning theories and could not obtain theoretical bounds. 
6
hypotheses and has the ability of dealing with the imbalanced data classification 
problem in active learning. Specifically, our contributions in this paper include:  
 
z Determination of the query probability. In the data pool, the importance of an 
unlabeled sample is measured depending on two specific hypotheses without 
maintaining the candidate hypothesis set. Also this importance is theoretically 
interpreted as a query probability. That is, the proposed approach achieves the 
purposes of practical use and the guarantee of theoretical bounds.  
z Utilization of the local classification behavior. Rather than the entire skewed 
dataset, only labeled samples in the neighborhood are used for generating the 
two error minimization hypotheses. Those hypotheses are specifically trained 
depending on the local behavior and without being affected by irrelevant data. In 
this paper, while hypotheses are generated in active learning, the problem of 
imbalanced data classification is effectively handled by our proposed approach. 
z Exploration of the specific neighborhood. Generally previous research works 
[23], [25], [27] that utilize local behavior for learning problems have to define 
the neighborhood in advance. In this paper, for each unlabeled sample, we 
extract the distribution of its labeled neighbors to explore the neighborhoods 
with different sizes. The neighborhood is decided for gaining the potential query 
probability (as large/small as possible). There is no need to define the 
neighborhood size before our active learning approach is executed.   
 
In our proposed approach CBAL, two possible problems might be concerned: 
time consumed in exploring the neighborhoods, and overfitting in selecting samples 
and in label prediction. At first, for each unlabeled sample, we explore its 
neighborhood where the potential query probability could be gained as large/small as 
possible. In term of the efficiency for finding the optimal solution, our approach is to 
find a local optimal one by incrementally exploring the neighborhoods using the 
log-uniform sizes and the binary-search strategy. Secondly, rather than depending on 
amounts or distances among neighbors, the sample distribution on the neighborhood 
is carefully observed by the separating hypotheses. In this paper, we utilize the local 
behavior of each unlabeled sample to determine its query probability and the 
predicted label. Particularly, we design a hypothesis-based strategy which depends on 
the difference between two error minimization hypotheses to explore the 
neighborhoods. So that our approach could gain the well-informed local behavior 
which has the ability to carefully handle the overfitting problem. Moreover, in this 
paper, our experimental results show that these two problems mentioned above are 
properly handled in the proposed approach. 
8
II. RELATED WORK
A. Imbalanced Data Classification
While classifiers are built by the imbalanced datasets, minority classes are usually over-
whelmed by majority classes [10]. Several approaches, like boosting algorithm [16], cost-
sensitive Support Vector Machines (SVM) [17], one-class SVM [18], neural network [19],
oversampling [20], and undersampling [21], have been proposed to deal with these classifica-
tion problems. Recently there are some research results [22], [23], [24] to utilize additional
local behaviors of specific neighbors for building classification models. Global classification
models are partially corrected by these local behaviors so that minority samples are able
to be explicitly recognized. Nevertheless, if the global models are seriously affected by the
imbalanced datasets, those additional local behavior might not be effective for the model
correction. Hence in [25], [26], [27], all classifiers are specifically trained based on the
neighbors. Rather than the entire labeled dataset, only some relevant labeled samples are
used for constructing classifiers. That is the typical framework of some research work [25],
[26], [28], [29] which assumes the local behavior in the specific area is as effective as the
global one when dealing with the learning problems. Given an unlabeled sample, relevant
labeled samples are identified and then their class membership is used for its label prediction.
The specific classifier is built by these relevant class membership without being affected by
irrelevant samples. However most of those approaches that use local behavior should define
the size of neighborhoods in advance. In this paper, the idea of utilizing local behavior is
adopted in our active learning method to solve the imbalanced data classification problem for
pooled datasets, and there is no need to predefine the size of each neighborhood.
B. Active Learning
Generally, active learning is to query a small number of informative unlabeled samples and
expected to perform as well as passive learners. Those informative samples usually exist near
the class boundary using which it is ambiguous to separate them [1], [2]. Or those samples
receive the predicted labels that are disagreed among designate hypotheses so as to eliminate
inconsistent hypotheses [4], [12], [13], [31]. In order to deal with the agnostic cases in active
learning, [30] assigns penalties values to the experts/oracles to improve the quality of labeling
results. Additionally several algorithms, like [9], [8], [11], [14], [15], have focused on dealing
with noisy queried samples through theory-based strategies. Those approaches fall into two
main categories in terms of input samples. The first category is that unlabeled samples are
10
Theorem 2. With probability at least 1− δ, the expected number of labels queried by IWAL-
ERM after n iterations is at most
1 + θ· 2err(h∗)· (n− 1) +O(θ·
√
C0nlogn+ θ· log3n),
where θ is the disagreement coefficient.
III. THE PROPOSED METHOD
In this section, for each unlabeled sample we propose a strategy to explore the suitable
neighborhood within which a query probability is determined based on some learning theories.
A. Determination of Query Probabilities
For practical purposes, IWAL-ERM [11] is proposed to generate only two specific hypothe-
ses for each streaming unlabeled sample to handle the agnostic cases without maintaining a
candidate set of hypotheses. Similarly in this paper, at each round the two hypotheses hk and
h′k are generated to assign a query probability for each unlabeled sample xk in the pool: hk
is the existing best hypothesis (current class boundary), and h′k is the best hypothesis among
a set of hypotheses which disagree with hk on xk’s predicted label. Then Gk is the difference
between the empirical errors of hk and h′k, and xk’s query probability Pk is determined based
on Theorem 1 and the rejection threshold (presented in Alg. 3).
B. Exploration of Certainty-Based Neighborhoods
The most important is that, we assumes local behavior in a well-informed area is as effective
as the global one in dealing with the learning problems. Rather than the entire dataset, only
labeled samples in the neighborhood of the unlabeled sample are used to generate those two
specific hypotheses. Consequently, the large/small difference between the empirical errors of
the two hypotheses means this unlabeled sample locates on the certain/uncertain area in this
specific neighborhood. An example is illustrated in Fig. 1. x1 locates on the certain area in its
neighborhood and would be assigned a low query probability since x1 obtains large difference
between h1 and h′1. Comparatively x2, that obtains small difference between h2 and h
′
2, locates
on the uncertain area in its neighborhood and would be assigned a high query probability.
Moreover in Fig. 1, the certain/uncertain area where x1 and x2 locate in the neighborhood is
similar with certain/uncertain area on the whole data distribution. This example presents the
advantage from our assumption: local behavior is as effective as the global one in used for
12
minSmk ∈L Gk
= minSmk ∈L err(h
′
k, S
m
k )− err(hk, Smk )
= minSmk ∈L {minh∈H∧h(xk)6=hk(xk) err(h, Smk )
−minh∈H err(h, Smk )} (3)
In terms of efficiency for finding the optimal Grk in eq. 2 and 3, our approach is to find a
local optimal Gk by incrementally exploring the neighborhoods using the log-uniform sizes,
{m,m + 20,m + 21,m + 22, ...}, and the binary-search strategy. Our proposed method to
explore certainty-based neighborhoods is presented in Algorithm 1. And Gk,m, a value of Gk
based on Smk , is calculated by the function calculate Gk. At first, our method determines the
exploring strategy (the value of flag) based on the comparison between the Gk,m within the
initial neighborhood Smk and Gk,m+1 within the first expanded neighborhood S
m+1
k , at line 3.
Then the neighborhood is expanded with the log-uniform sizes to explore the bound of the
neighborhood size, at line 5 to 7. At last, we use binary-search to find the local optimal Gk,
at line 9 to 14.
C. Certainty-Based Active Learning
After the query probability of each unlabeled sample in the data pool is determined,
our active learning algorithm, Certainty-Based Active Learning (CBAL) in Algorithm 3, is
designed to query the true label of the unlabeled sample that has the greatest query probability.
For each unlabeled sample xk in the data pool U , CBAL explores the specific neighborhood
and measures Gk in each round, at line 6. Then the query probability Pk is determined by
the rejection threshold [11] where C0, c1, and c2 are constant values, at line 7 and 8. At last,
the sample xk that has the greatest Pk is queried and added into the labeled dataset L.
D. Correctness Analysis
In the previous research work [11], it is proven that IWAL-ERM satisfies Pk ≥ 1/kk
where k is the sequence number of samples of which probabilities are assigned, and then
its deviation bound for errors of the hypotheses is based on learning theorems and explicitly
stated in Theorem 1. Through Theorem 1, the error of the final hypothesis is bounded based
on the rejection threshold of determining query probabilities. And, the expected number of
requested labels is measured by Theorem 2. Notably, the major difference between our CBAL
14
Algorithm 2: functions
calculate Gk(xk,m, L) begin1
Let Smk = {x|x ∈ L ∧ d(x, xk) ≤ dmk } be a neighborhood within the distance dmk ;2
hk = argmin{err(h, Smk ) : h ∈ H}, and3
h′k = argmin{err(h, Smk ) : h ∈ H ∧ h(xk) 6= hk(xk)} ;
return Gk = err(h′k, S
m
k )− err(hk, Smk ) ;4
end5
compare(Gk, G
′
k, f lag) begin6
if flag == 1 and G′k > Gk then return true ;7
else if flag == 1 return false ;8
if flag == −1 and G′k ≤ Gk then return true ;9
else if flag == −1 return false ;10
end11
Algorithm 3: Certainty-Based Active Learning
Initialize: L =Ø;1
Let U be a set of unlabeled samples ;2
Let m be the initial size of each neighborhood ;3
for r = 1, 2, ..., R do4
for xk ∈ U do5
Gk = find Gk(xk, L,m), and6
Pk =
 1, if Gk ≤
√
C0logk
k−1 +
C0logk
k−1
s, otherwise.
(= min{1, O( 1
G2k
+ 1
Gk
)· C0logk
k−1 }), where
7
s ∈ (0, 1) is the positive solution to the equation
Gk = (
c1√
s
− c1 + 1)·
√
C0logk
k−1 + (
c2
s
− c2 + 1)· C0logkk−1 ;
Toss a biased coin with Pr(heads) = Pk.8
end9
Find xk ∈ U that has the greatest Pk ;10
L = L ∪ {(xk, yk, 1/Pk)} ;11
U = U\{xk} ;12
end13
Return: L14
16
TABLE I
THE DISTRIBUTION OF SAMPLES QUERIED BY EM-IWAL AND CEAL. (MAJ. = MAJORITY CLASS AND MIN. =
MINORITY CLASS)
A2 CBAL
Area maj. min. total maj. min. total
x < 120 0 0 1 2 0 2
120 ≤ x < 320 68 0 68 4 0 4
320 ≤ x < 520 55 11 66 48 7 55
520 ≤ x < 720 37 18 55 61 42 103
720 ≤ x < 920 37 24 61 51 22 73
920 ≤ x 0 0 0 13 0 13
p(y = 1|x) =

100%, if x < 120
90%, if 120 ≤ x < 320
80%, if 320 ≤ x < 520
70%, if 520 ≤ x < 720
60%, if 720 ≤ x < 920
0%, if 920 ≤ x
(4)
This experiment is used to show the effectiveness of querying informative unlabeled samples
by exploring their certainty-based neighborhoods. Table I demonstrates the distributions of
queried samples of A2 and CEAL when totally 250 samples are queried. Compared with the
samples queried by A2, fewer unlabeled samples located on the area from 120 to 520 are
queried by CBAL since a large part of samples on this area belong to the majority class. That
means fewer certain (majority) samples are selected by our method than those by A2. Also
Table I shows that more unlabeled samples on the area from 520 to 920 are queried by CBAL
because many majority samples and minority ones gather on this area. That is, our CBAL
queries more uncertain samples and less certain samples than A2. Hence, this experimental
result demonstrates that our proposed method enhances the ability of active learning to query
informative samples, especially when handling the skewed dataset.
B. Real-World Datasets
Table II presents document categories of five real-world data collections used in our
experiments. abalone, ecoli, and yeast have low-dimension datasets, and ModApte and 20News
18
 
 

 

 

 

 

 


   

  	 

 

 

 

  


 








 

 








ﬀ
 
ﬁﬂﬃ
 
ﬁ
!"#$%
&'()*+
'
,
-
.'
/
0-
.'
/
-
1'
/
Fig. 2. the performance of active learning approaches on cp+im
 

 

 

 

 

 

 

 


   

  	 

 

 

 

  


 








 

 








ﬀ
 
ﬁﬂﬃ
 
ﬁ
!
"#$$
%
&'()*
&+
,
-
&.
/,
-
&.
,
0
&.
Fig. 3. the performance of active learning approaches on im+pp
In Fig. 2-8, all active learning methods including Random have precise classification results
after parts of unlabeled samples in the pool are queried. Notably, on these separable datasets,
our proposed CBAL, that properly assign a query probability to each unlabeled sample,
slightly improves the classification performance of other approaches. In Fig. 9-11, Random and
A2 have very biased classification performance. Their strategies to select unlabeled samples
are strongly affected by the skewed datasets, 9+16, 10+16, and MIT+VAC, which have the
closest distances between class means (shown in Tab. II). Even so CBAL still gains the
enhanced classification accuracy on these skewed datasets because querying samples within
the certainty-based neighborhoods is deliberately designed to improve the imbalanced data
classification problems in active learning.
20
 
 
 
 


	 


 














ﬀﬁﬂ
ﬃ
 !
 "
 #
 $
"  #  $          %  

& '
'
	(
	
)*+,
-
+
.
 
/012
3
+/
4 !
5!
Fig. 6. the performance of active learning approaches on ship+silver
 
 
 
 
 
 
 
 
 
 
 
              	  


 








 

 








ﬀ
 
ﬁﬂﬃ
 
ﬁ
!"#$%&
'()*+,
(
-
.
/(
0
1.
/(
0
.
2(
0
Fig. 7. the performance of active learning approaches on misc+x
neighborhood for each unlabeled sample while totally 90 samples are queried. It is found
that, only several iterations of neighborhood exploration are required by our CBAL. Thus the
proposed CBAL is practical to explore the neighborhood to find the local optimal Gk. Notably,
on the low-dimension dataset 9+16, it requires extraordinary sizes of the neighborhoods where
local behavior might be corresponding to global behavior. On this dataset, the advantage of
local behavior is hardly utilized for determining query probabilities so that the classification
accuracy of CBAL on 9+16 is not impressed (see Fig. 9). Hence, this experiment is shown
that CBAL is capable of exploring certainty-based neighborhoods efficiently except while the
local behavior on the dataset is complicated to be recognized.
22
  
 
 
 
 
 

	

 

 












ﬀﬁ
 
 
 

ﬂ        ﬃ         

! "
"

#


$%&'
(
&)
 
*+,-.&*
/ﬀﬁ
0ﬁ
Fig. 10. the performance of active learning approaches on 10+16
 

 

 

 

 

 

 

 
	

 

 

 

 

 
	
  
 

  

 







 

 





ﬀﬁ
 
ﬂﬃ 
!
ﬂ
"#$%&'(
)*+,-.
*

/
0*
1
2/
0*
1
/
3*
1
Fig. 11. the performance of active learning approaches on MIT+VAC
CBAL still has impressive performance on the quality of queried samples and the classification
accuracy. At last, the efficiency in exploring the neighborhoods is shown that, only several
iterations of neighborhood exploration are needed by CBAL, except while the local behavior
on the dataset is complicated to be recognized.
REFERENCES
[1] S. Ertekin, J. Huang, L. Bottou, and L. Giles. Learning on the Border: Active Learning in Imbalanced Data
Classification. In ACM Conference on Information and Knowledge Management, pp. 127-136, Lisboa, Portugal,
November 2007.
[2] M. Li an I.K. Sethi. Confidence-based active learning. IEEE Transactions on Pattern Analysis and Machine Intelligence,
28(8): 1251-1261, 2006.
24
 
 

 

 

 

 

 

	

 

 

 

 

 

 

 

  	  

 







 

 





ﬀﬁ
 
ﬂﬃ 
!
ﬂ
"#$%&'(
)*+,-.
/

0
1
/2
Fig. 14. the performance of active learning approaches with cost-sensitive SVM on MIT+VAC
TABLE III
ANALYSIS OF THE AVERAGE ITERATIONS OF NEIGHBORHOOD EXPLORATION AND THE AVERAGE SIZE OF THE
EXPLORED NEIGHBORHOOD FOR EACH UNLABELED SAMPLE.
Dataset Iterations Sizes
9+16 8.8 54.5
10+16 5.66 18.12
10+4 6.41 18.63
cp+im 5.62 13.28
im+pp 6.68 15.29
CYT+ME1 6.57 26.74
MIT+VAC 5.82 18.68
ship+silver 7.48 34.14
ibm+mac 7.91 74.45
ms+x 7.76 36.46
[16] Y. Sun, M.S. Kamel, A.K.C. Wong, and Y. Wang. Cost-sensitive boosting for classification of imbalanced data. Pattern
Recognition, 40(12): 3358-3378, 2007.
[17] V. Vapnik. Statistical Learning Theory. Wiley, New York, NY, 1998.
[18] L.M. Manevitz and M. Yousef. One-class svms for document classification. Machine Learning Research, 2:139-154,
2002.
[19] N. Japkowicz. Supervised versus unsupervised binary-learning by feedforward neural networks. Machine Learning,
42(1-2): 97-122, 2001.
[20] N.V. Chawla, K.W. Bowyer, L.O. Hall, and W.P. Kegelmeyer. SMOTE: synthetic minority over-sampling technique.
Artificial Intelligence Research, 16(1): 321-357, 2002.
[21] M. Kubat and S. Matwin. Addressing the Curse of Imbalanced Training Sets: One-Sided Selection. In International
Conference on Machine Learning, pp. 179-186, Tennessee, USA, July 1997.
26
I. INTRODUCTION
Requests of analyzing collected period data have been emerged in recent practical ap-
plications that includes network traffic analysis [1], anomaly detection [2], and intrusion
detection [3]. Generally, those applications are implemented for adjusting classifiers/detectors
periodically. Most of incremental learning approaches have been proposed based on decision-
tree [4], neural network [5], [6], and Support Vector Machines (SVM) [3], [7], [8], [9], [10].
Typically they are designed to build the statistic classification model based on the previously
seen samples and to correct its prediction mistakes on new labeled samples. While focusing
on the sample space, SVM generalizes the separating hyperplane based on the whole sample
distribution, and maximizes the margins of the labeled samples (support vectors). The margin
of a sample is the distance between the sample and the separating hyperplane. And SVM is
theoretically proven that the hyperplane is able to well separate samples with different labels.
In [10], an incremental SVM approach was designed to update the hyperplane by solving a
constrained optimization problem based on all collected samples. This approach should solve
a complicated constrained optimization problem since all collected samples are considered
simultaneously. Other approaches [8], [9] identified distinct new samples as support vectors
to adjust SVM hyperplanes without solving the constrained optimization problems. The
advantage is to maintain useful samples that were previously seen as support vectors and to
obtain efficient update steps. They shall have stable abilities of label prediction. But, relatively,
those SVM classifiers could not be quickly adjusted when encountering diverse sample
distribution. Thus in this paper, our approach is to simplify the constrained optimization
problem for update steps and to adapt the diverse sample distribution for classifiers.
At first, rather than incrementally training the SVM classifier based on all collected
samples [10], our approach selects one sample’s margin as the basis for individual classifier
adjustment. For update steps, we divide a complicated constrained optimization problem into
several simpler ones. Consequently for the update steps, we formulate several optimization
problems with single constraint. Then we are motivated by the simplicity of online Passive-
Aggressive (PA) algorithm [11]. For using the new labeled sample, PA updates the classifier
by adjusting more than the margin of this sample. Thus in our approach, while a sample is
used for updating and its sign is incorrectly predicted, the classifier adjustment is aggressively
achieved within the margin. And similar with other incremental learning methods, our updated
classifier shall correct the prediction mistakes of the current classifier as many as possible.
28
3wt+1 = argminw∈Rn
1
2
||w − wt||2 s.t. l(w, (xt, yt)) = 0, (1)
where l(w, (xt, yt)) is the hinge loss of w’s prediction on xt.
l(w, (x, y)) =
 0, y(w·x) ≥ 11− y(w· x), otherwise (2)
Typically whenever the loss is zero, PA is passive and wt+1 = wt means no classifier
adjustment. And while the loss is positive, wt is aggressively updated by adjusting more than
the margin, yt(wt·xt), and then the constrain l(wt+1, (xt, yt)) = 0 can be satisfied. Then the
Lagrangian of the optimization problem in Eq. (1) is defined as Eq. (3).
L(w, τ) =
1
2
||w − wt||2 + τ(1− yt(w·xt)) (3)
Let the partial derivation of l with respect to w be zero and then let the deviation of τ
with respect to τ be zero, we have
w = wt + τytxt
τ =
1− yt(wt·xt)
||xt||2
Ultimately the PA update is performed by solving the constrained optimization problem in
Eq. (1). And it is theoretically shown that the aggressive update strategy of PA modifies the
weight vector as less as possible. The effectiveness of PA in solving problems of classification
and regression is formally analyzed in [11]. Based on this well-defined learning model of PA,
several online algorithms [17], [18] have been proposed for adding confidence information
and handling non-separable data..
III. INCREMENTAL PASSIVE-AGGRESSIVE LEARNING ALGORITHM
While each set of labeled period samples comes, the existing classifier shall be periodically
updated for adapting the latest sample distribution. In this paper, we propose an incremental
learning algorithm, named Incremental Passive-Aggressive (IPA). For each labeled sample,
there are two update steps in IPA: 1) to correct the prediction mistakes of the current
classifier, and 2) to aggressively update the current classifier by adjusting more than the
margin. At last, the error minimization classifier on the collected dataset is selected as the
October 5, 2011 DRAFT
30
5Let the partial derivation of l with respect to w be zero,
OwL(w, τ) = w − wt −
∑
xi∈κt,xi 6=xk
yixi − τykxk
=> w = wt +
∑
xi∈κt,xi 6=xk
yixi + τykxk (7)
Then substituting Eq. (7) into Eq. (6), we have
L(τ) =
1
2
||
∑
xi∈κt
yixi + τykxk||2
+
∑
xi∈κt,xi 6=xk
(1− yi((wt +
∑
xi∈κt,xi 6=xk
yixi + τykxk)·xi))
+ τ(1− yk((wt +
∑
xi∈κt,xi 6=xk
yixi + τykxk)· xk)) (8)
At last let the deviation of Eq. (8) with respect to τ be zero,
0 = −τy2k||xk||2 + (1− yk(wt·xk))− ykxk
∑
xi∈κt,xi 6=xk
yixi
=> τ =
1− yk(wt·xk)− ykxk
∑
xi∈κt,xi 6=xk yixi
||xk||2 (9)
Ultimately, the update of the proposed incremental learning algorithm is performed by
solving the constrained optimization in Eq. (4) and the updated classifier is determined
by solving Eq. (5). It is theoretically presented in Eq. (7) and (9) that the update to the
current classifier wt is performed by correcting its prediction mistakes κt, and by adjusting it
within the margin when the sample is incorrectly predicted. Overall the proposed algorithm
is presented in Algorithm 1. At each round t, the dataset Kt is collected to update the current
classifier wt. Particularly, the samples of which predicted labels are incorrectly assigned by
wt are identified as κt, at line 4-5. Then for each sample xk ∈ Kt, the current classifier wt is
individually updated as the candidate classifier wk according to Eq. (7) and (9), at line 7-8.
At last, the classifier wk is selected as wt+1 if it gains the least prediction errors on Kt, at
line 10.
IV. EXPERIMENTS
In this section, our experimental results are presented to show the classification performance
while the classifiers are incrementally updated by the small training sets. For the sake of
comparisons, the online PA and an incremental SVM [9] are also run in our experiments. In
October 5, 2011 DRAFT
32
7TABLE I
9 REAL-WORLD DATASETS: SIZES OF THE CLASSES AND THE SIZE OF FEATURE DIMENSIONS
Dataset Class(size) Class(size) Dimensions
Australian Credit Approval 0(383) 1(307) 14
Connectionist Bench 1(111) 2(97) 60
Statlog (Heart) 1(150) 2(120) 13
Ionosphere b(126) g(225) 34
abalone 10(634) 4(57) 8
Pima Indians Diabetes 0(500) 1(268) 8
ecoli cp(143) im(77) 8
Spamming Bots normal(1560) spamming(150) 5
yeast CYT(463) ME1(44) 9
proposed IPA has better performance than IPA with C0 = 0. That means, it is effective to
correct the previous mistakes to gain the more suitable classifier, especially on the small and
insufficient labeled dataset. At last, it is shown IPA has the similar classification performance
with the incremental SVM. Although the SVM approach is outstanding at classification
accuracy, it is required to concern about its computational complexity. Comparatively, our
update strategy is to solve several simple optimization problems with single constraint in
order to improve the computing efficiency. Moreover, in Fig. 1-3, the proposed approach has
higher accuracy in classification than the incremental SVM at the starting rounds. When only
few labeled samples are seen, the sample distribution is diverse. Thus it is shown that our
approach has an ability of adapting the diverse labeled samples efficiently.
Interestingly in Fig. 5-9, our experimental results on the imbalanced datasets show that
the proposed IPA still obtains the stable and impressive ability in label prediction. That is
because the proposed approach adopts weighted errors for the skewed classes in both of the
update steps and the candidate classifier selection. However the incremental SVM approach
has poor results since it does not focus on handling the imbalanced datasets. In details it is
noted on abalone, ecoli, and Spamming Bots that suitable SVM classifiers are generated at
the latter rounds. That is, those SVM classifiers are well updated while enough knowledge
of the class distribution is accumulated.
October 5, 2011 DRAFT
34
9 









	    



  	 		 	  	 	 	

	 	

	
m
ic
ro
-a
v
er
a
g
e 
a
cc
u
ra
cy
rounds
Heart
Online PA
IPA C=0
IPA C=0.5
Incremental SVM
Fig. 3. Classification results of incremental learning approaches on heart
 





er
a
g
e
 a
c
cu
ra
cy
Ionosphere
Online PA
IPA C=0




 	  

 

    	   

  


m
ic
ro
-a
v
e
rounds
IPA C=0.5
Incremental SVM
Fig. 4. Classification results of incremental learning approaches on Ionosphere
resulted classifier. And our approach is able to adapt the diverse labeled samples efficiently.
It is also presented the proposed approach has the similar classification results with other
incremental methods. Furthermore on some real-world datasets, our approach has the ability of
handling the problem of skewed classes. Therefore it is presented that the proposed approach
is suitable to be applied for effectively adjusting the existing classifiers using periodically
collected datasets.
REFERENCES
[1] Sena, G. G. and Belzarena, P.: Early traffic classification using support vector machines. In: 5th International Latin
American Networking Conference, pp. 60-66. ACM New York (2009)
[2] Robertson, W. K., Maggi, F., Kruegel, C., Vigna, G.: Effective Anomaly Detection with Scarce Training Data. In: the
Network and Distributed System Security Symposium. ISOC (2010)
[3] Du, H., Teng, S., Yang, M., Zhu, Q.: Intrusion Detection System Based on Improved SVM Incremental Learning. In:
International Conference on Artificial intelligence and Computational intelligence, pp. 23-28. IEEE Press (2009)
October 5, 2011 DRAFT
36
11
 









 	   



    	    

 


m
ic
ro
-a
v
er
a
g
e 
a
cc
u
ra
cy
rounds



 



 





 






ﬀ
ﬁﬂ
ﬃ

 
 
!
Fig. 7. Classification results of incremental learning approaches on cp+im
 





er
a
g
e 
a
cc
r
u
a
cy
		



 


 













 



 
ﬀ
ﬁ

ﬂ
ﬃ


  ﬂ ﬃ

 

     ﬂ ﬃ 

  


m
ic
ro
-a
v
e
rounds


 
ﬀ
ﬁ
! 


"#$%&
 
'(
)
Fig. 8. Classification results of incremental learning approaches on bot
37, 277-296 (1999)
[13] Ng, H. T., Goh, W. B., Low, K. L.: Feature selection, perceptron learning, and a usability case study for text
categorization. In: International Conference on Research and development in information retrieval, pp. 67-73. ACM
New York (1997)
[14] Cesa-Bianchi, N., Conconi, A., Gentile, C.: A Second-Order Perceptron Algorithm. J. Computing, 34(3), 640-668
(2005)
[15] Wang, S., San, Y., Wang, S.: An Online Modeling Method Based on Support Vector Machine. In: International
Conference on COmputer Science and Software Engineering, pp. 98-101. IEEE Press (2008)
[16] Sculley, D. and Wachman, G. M.: Relaxed Online SVMs for spam filtering. In: International Conference on Research
and development in information retrieval, pp. 415-422. ACM New York (2007)
[17] Dredze, M., Crammer, K., Pereira, F.: Confidence-Weighted Linear Classification. In: International Conference on
Machine Learning, pp. 264-271. ACM New York (2008)
[18] Crammer, K., Kulesza, A., Dredze, M.: Adaptive Regularization of Weight Vectors. In: Neural Information Processing
Systems. MIT Press, Cambridge, MA (2009)
[19] Lin, P., Yen, T., Fu, J., Yu, C.: Analyzing Anomalous Spamming Activities in a Campus Network. In: TANET. (2011)
October 5, 2011 DRAFT
38
Certainty-Enhanced Active Learning for
Improving Imbalanced Data Classification
JuiHsi Fu
Department of Computer Science
and Information Engineering
National Chung Cheng University
Chiayi, Taiwan, R. O. C.
fjh95p@cs.ccu.edu.tw
SingLing Lee
Department of Computer Science
and Information Engineering
National Chung Cheng University
Chiayi, Taiwan, R. O. C.
singling@cs.ccu.edu.tw
Abstract—In active learning algorithms, informative sam-
ples are usually queried for true labels according to the
disagreement of existing hypotheses. However we observed
that, when the streaming dataset has skewed class member-
ship, the imbalanced data classification problem is caused
in active learning. The Minority class is overwhelmed by the
majority class in generating the hypotheses. In this paper,
for each unlabeled sample we propose to utilize only local
behavior in the certainty-enhanced neighborhood, rather
than the entire dataset, to generate the error minimization
hypotheses. Consequently, our proposed method enhances
the prediction of hypotheses and is able to determine the
query probabilities properly. In our experiments, synthetic
and real-world datasets are used for presenting the effec-
tiveness of our active learning approach. It is shown that the
proposed approach decreases the probability of querying a
certain (majority) sample and has the ability of dealing
with the imbalanced data classification problem in active
learning.
Index Terms—Imbalanced Data Classification, Active
Learning, Lazy Learning, Streaming Datasets, Certainty-
Enhanced Neighborhood
I. INTRODUCTION
Nowadays massive messages are consecutively trans-
fered among applications and large data are streamingly
accessed through the networks. For example, business
emails are sent for official affairs and network pack-
ets are collected for abnormal detection. Generally, it
is useful to design specific handlers like email filters
[1], [2] and abnormal traffic detectors [3] to manage
those collected data. Those handlers are usually trained
by certain supervised learning approaches after enough
streaming datasets are collected and labeled. However
the critical issue is to waste lots of experts’ effort and
time to label the massive datasets. Several strategies of
unlabeled sample selection, like semi-supervised learning
[4], [5], membership-query [6], and active learning [7],
[8], [9], [10], [11], [12], [13], [14], have been designed to
This work is supported by NSC, Taiwan, R.O.C. under grant no.
NSC 99-2221-E-194-023.
utilize unlabeled samples for advanced usage in generat-
ing classification models. Different with semi-supervised
learning and membership-query, active learners have
access to query true labels of the unlabeled samples.
And, their objective is to query only a small number
of unlabeled samples and perform as well as supervised
learners.
A practical and statistically consistent scheme for
streaming-based active learning, Importance Weighted
Active Learning(IWAL) [12], has been proposed to
determine probabilities of querying streaming samples,
also called query probabilities, based on the rejection
threshold and the disagreement of candidate hypotheses.
The rejection threshold is used to guarantee that IWAL
has the ability of handling agnostic cases where unknown
noises exist in the labeled datasets. The main idea is
to assign a high query probability for the unlabeled
sample that locates on the uncertain area (close to the
existing class boundary). Recently, authors in [13] re-
vised IWAL, named EM-IWAL for simplicity, to find the
error minimization hypotheses for each unlabeled sam-
ple without maintaining the candidate hypotheses. The
learner determines the query probability of an unlabeled
sample according to the difference between the errors
of these hypotheses. It is presented that EM-IWAL is
computationally tractable and effective for active learning
with varied kinds of classifiers. However, we notice that
in EM-IWAL each query probability strongly depends on
the error minimization hypotheses. While the streaming
dataset has skewed probabilities of sample occurrence,
the biased hypotheses, of which prediction is dominated
by majority samples, tend to be inappropriately trained.
The imbalanced data classification problem is caused,
and the minority class is overwhelmed by the majority
class in generating the (biased) hypotheses ℎ𝑘 and ℎ′𝑘.
The consequence is that, majority samples are frequently
queried and the biased hypothesis is returned for the
resulting classifier. (see our experimental results in Table
III)
40
sion of IWAL, EM-IWAL, that finds error minimization
hypotheses without maintaining the candidate set is pro-
posed. For an unlabeled sample 𝑥𝑘, EM-IWAL generates
two error minimization hypotheses: one is the existing
best hypothesis (current class boundary) ℎ𝑘, and the other
is the best hypothesis ℎ′𝑘 among a set of hypotheses
which disagree with ℎ𝑘 on 𝑥𝑘’s predicted label. 𝐺𝑘 is the
difference between empirical errors of ℎ𝑘 and ℎ′𝑘. Then
the query probability 𝑃𝑘 of 𝑥𝑘 is decided by 𝐺𝑘 and the
rejection threshold in Theorem 1 where 𝐶0 is a constant
and ℎ∗ is the optimal hypothesis. Notably Theorem 1
guarantees EM-IWAL has the ability of handling agnostic
cases. Also Theorem 1 bounds the value of 𝐺𝑥. That is,
a large value of 𝐺𝑥, which means it is insignificant to
query 𝑥𝑘 for discriminating ℎ𝑘 between ℎ′𝑘, results in a
low 𝑃𝑘 for 𝑥𝑘. Ultimately, it is theoretically proven by
Theorem 2 that EM-IWAL has polynomial improvement
over supervised learning on required labeled samples.
Theorem 1. For any 𝑛 ≥ 1, EM-IWAL satisfies 𝑃𝑛 ≥
1/𝑛𝑛 so that the following holds with probability at least
1− 𝛿.
𝑒𝑟𝑟(ℎ𝑛) ≤ 𝑒𝑟𝑟(ℎ∗) +
√
2𝐶0𝑙𝑜𝑔𝑛
𝑛− 1 +
2𝐶0𝑙𝑜𝑔𝑛
𝑛− 1 .
Theorem 2. Let 0 ≤ 𝛿 ≤ 1. With probability at least
1 − 𝛿, the expected number of labels queried by EM-
IWAL after n iterations is at most
1 + 𝜃⋅ 2𝑒𝑟𝑟(ℎ∗)⋅ (𝑛− 1) +𝑂(𝜃⋅
√
𝐶0𝑛𝑙𝑜𝑔𝑛+ 𝜃⋅ 𝑙𝑜𝑔3𝑛),
where 𝜃 is the disagreement coefficient, a quantity first
used by [13].
III. THE PROPOSED METHOD
In this section, our active learning method is explained
how to utilize local information from the neighborhoods
in order to improve the imbalanced data classification
problems. The strategy of finding a proper neighborhood
is proposed after our observation is explicitly presented.
A. Exploration of the Certainty-Enhanced Neighborhood
In the settings of our approach, it is assumed that local
behavior extracted from the specific area/neighborhood
is as effective as global one in dealing with the learn-
ing problems. Our key idea is to define a suitable
neighborhood for each unlabeled sample to utilize only
labeled samples in the neighborhood, rather than the
entire dataset, to determine the query probability. Then
two error minimization hypotheses are generated without
being affected by irrelevant samples. It is noted that,
the large difference between empirical errors of these
two hypotheses means the unlabeled sample locates on
the certain area in the neighborhood. And this unla-
beled sample should be queried in a low probability.
An illustration is presented in Figure 1 where solid
x1
(a) unlabeled sample 𝑥1
x2
(b) unlabeled sample 𝑥2
Fig. 1. 𝑥1 on a certain area in its neighborhood should be queried in
a low probability. Comparatively 𝑥2 close to the class boundary in its
neighborhood has to be queried in a high probability.
initial
expanded
x
Fig. 2. The unlabeled sample 𝑥’s inner neighborhood is expanded as
the outer one because its 𝐺𝑥 is increased.
circles are the neighborhoods. 𝑥1 locates on a certain
area in its neighborhood, and undoubtedly 𝑥1 should
be queried in a low probability. Comparatively 𝑥2 is
close to the class boundary in its neighborhood so 𝑥2
has to be queried in a high probability. Generally, the
objective of active learning is to assign a low query
probability for the unlabeled samples on the certain areas.
Hence for each unlabeled sample 𝑥, our active learning
algorithm is designed to explore a certainty-enhanced
neighborhood where the large difference, 𝐺𝑥, is mea-
sured by the error minimization hypotheses. An example
is illustrated in Figure 2 where the gray circle instance
is an unlabeled sample 𝑥. The inner neighborhood is
initially generated by its three nearest neighbors. Then
the inner neighborhood is expanded as the outer one
because 𝐺𝑥 is increased. That is, a large 𝐺𝑥 for the
unlabeled sample 𝑥 on the certain area could be obtained
by exploring the neighborhoods incrementally. In terms
of sample numbers, the probabilities of querying majority
samples is also decreased because they usually gather
on the certain areas in the neighborhoods. Let 𝐿 be the
entire labeled dataset, a collection of sample-label pairs
{(𝑥1, 𝑦1), (𝑥2, 𝑦2), ...}, and 𝑆𝑚𝑘 be 𝑥𝑘’s neighborhood
that is a set of 𝑚 specific neighbors. 𝑒𝑟𝑟(ℎ𝑘, 𝑆𝑚𝑘 ) is the
empirical error of the hypothesis ℎ on the dataset 𝑆𝑚𝑘 .
Our strategy shown in eq. (1) is to find the largest 𝐺𝑘
which is particularly based on 𝑆𝑚𝑘 and is measured by
ℎ𝑘 and ℎ′𝑘.
𝑚𝑎𝑥𝑆𝑚𝑘 ∈𝐿 𝐺𝑘
= 𝑚𝑎𝑥𝑆𝑚𝑘 ∈𝐿 𝑒𝑟𝑟(ℎ
′
𝑘, 𝑆
𝑚
𝑘 )− 𝑒𝑟𝑟(ℎ𝑘, 𝑆𝑚𝑘 )
= 𝑚𝑎𝑥𝑆𝑚𝑘 ∈𝐿 {𝑚𝑖𝑛ℎ∈𝐻∧ℎ(𝑥𝑘) ∕=ℎ𝑘(𝑥𝑘) 𝑒𝑟𝑟(ℎ, 𝑆𝑚𝑘 )
−𝑚𝑖𝑛ℎ∈𝐻 𝑒𝑟𝑟(ℎ, 𝑆𝑚𝑘 )} (1)
42
queried samples are those 𝑚 neighbors. Thus, our CEAL
also satisfies 𝑃𝑘 ≥ 1/𝑘𝑘. Significantly the rejection
threshold of CEAL is also effective for determining query
probabilities. That means, there is only small difference
between errors of the resulting certainty-enhanced clas-
sifier and the best one. At last, our expected number of
requested labels is also bounded by Theorem 2 which
presents a polynomial label complexity improvement
over supervised learning.
IV. EXPERIMENTS
In this section, our experimental results are presented
to show the effectiveness of CEAL in active learning.
For the sake of comparison, EM-IWAL and Random
selection are also run in our experiments. In the following
experiments, the training set is the collection of queried
samples, and the testing set is the whole dataset. The ac-
curacy of a classifier ℎ is the macro-average: 𝑒𝑟𝑟(ℎ, 𝑆) =
{∑∣𝑆∣𝑥∈𝑆 ℎ(𝑥) ∕=𝑦∧𝑦=+1}+{
∑∣𝑆∣
𝑥∈𝑆 ℎ(𝑥) ∕=𝑦∧𝑦=−1}
2 .
A. Synthetic Data set
The synthetic dataset is a collection of 1-dimension
samples with labels 1 (majority) and 0 (minority). Given
1,000 total samples at coordinates from 0 to 1,000,
the skewed class distribution is presented in eq. (3).
Approximately, there are 800 majority samples and 200
minority samples. On this dataset, our objective is to find
a separating threshold 𝑡 such that 𝑥 < 𝑡 if 𝑥’s label is 1
and 𝑥 > 𝑡 if 𝑥’s label is 0.
𝑝(𝑦 = 1∣𝑥) =
⎧⎨
⎩
100%, if 𝑥 < 120
90%, if 120 ≤ 𝑥 < 320
80%, if 320 ≤ 𝑥 < 520
70%, if 520 ≤ 𝑥 < 720
60%, if 720 ≤ 𝑥 < 920
0%, if 920 ≤ 𝑥
(3)
This experiment is used to show the effectiveness of
decreasing probabilities of querying certain (majority)
samples by exploring their certainty-enhanced neighbor-
hoods. Table I demonstrates the distributions of queried
samples of EM-IWAL and CEAL when they have similar
classification performance. In order to gain the similar
accuracy, 76.34% and 76.33%, and the similar number
of queried samples, 353 and 327, we set 𝐶0 = 0.35
for EM-IWAL, and 𝐶0 = 1 and 𝑚 = 10 for CEAL.
Compared with samples queried by EM-IWAL, fewer
unlabeled samples located on the area from 120 to 720
are queried by CEAL since a large part of samples on
this area belong to the majority class. That means fewer
certain (majority) samples are selected by our method
than those by EM-IWAL. Also Table I shows that more
unlabeled samples on the area from 720 to 920 are
queried by CEAL because many majority samples and
TABLE I
THE DISTRIBUTION OF SAMPLES QUERIED BY EM-IWAL AND
CEAL. (MAJ. = MAJORITY CLASS AND MIN. = MINORITY CLASS)
EM-IWAL CEAL
Area maj. min. total maj. min. total
𝑥 < 120 1 0 1 2 0 2
120 ≤ 𝑥 < 320 33 0 33 19 0 19
320 ≤ 𝑥 < 520 136 28 164 77 16 93
520 ≤ 𝑥 < 720 84 43 127 73 44 117
720 ≤ 𝑥 < 920 18 10 28 59 37 96
920 ≤ 𝑥 0 0 0 0 0 0
TABLE II
5 REAL-WORLD DATASETS: SIZES OF THE CLASSES, THE SIZE OF
FEATURE DIMENSIONS (DIM), AND THE DISTANCE BETWEEN CLASS
MEANS (DIS)
Dataset Class(size) Class(size) Dim Dis
abalone 9(689) 16(67) 8 0.412
abalone 10(634) 16(67) 8 0.189
ecoli cp(143) im(77) 8 0.581
ecoli im(77) pp(52) 8 0.54
yeast CYT(463) ME1(44) 9 0.458
yeast MIT(244) VAC(30) 9 0.213
ModApte ship(286) silver(29) 7,202 6.581
20News ms(300) x(30) 9,476 3.184
20News ibm(300) mac(30) 10,048 3.13
minority ones gather on this area. That is, our CEAL
queries more uncertain samples than EM-IWAL. Hence,
this experimental result demonstrates that our proposed
method enhances the ability of active learning to query
uncertain samples, especially when handling the skewed
dataset.
B. Real-World Datasets
Table II presents document categories of five real-
world data collections used in our experiments. abalone,
ecoli, and yeast have low-dimension datasets, and
ModApte and 20News are collected by high-dimension
datasets. Specially, to cause imbalanced data classifica-
tion on 20News, 300 samples are randomly selected from
ibm an ms, and 30 samples are randomly picked from
mac and x. Moreover, we compare our approach with
Random selection, EM-IWAL, and our opposite method,
named Certainty-Enhanced Active Learning (UCEAL),
which revises CEAL to explore the uncertainty-enhanced
neighborhood for each unlabeled sample. In the setting
of active learning, we set 𝐶0 = 1 for all algorithms
and 𝑚 = 3 for CEAL and UCEAL. libsvm [30] is
used to implement the SVM classifiers as hypotheses. At
each experiment, the performance of an active learning
algorithm is measured based on the number of queried
samples, the percentage of majority samples (mj. %) in
all queried samples, and the classification accuracy of
44
TABLE VII
PERFORMANCE OF CEAL ON SHIP+SILVER WITH DIFFERENT SIZES
OF INITIAL NEIGHBORHOODS
Neighborhood Size (mj%) Accuracy
m=5 110(80.0%) 89.1%
m=7 143(83.3%) 92.9%
m=9 121(82.0%) 87.8%
m=20% 152(84.3%) 96.6%
m=40% 206(87.4%) 98.3%
m=60% 224(88.0%) 98.3%
m=80% 175(86.3%) 94.8%
TABLE VIII
PERFORMANCE OF CEAL ON IBM+MAC WITH DIFFERENT SIZES OF
INITIAL NEIGHBORHOODS
Neighborhood Size (mj%) Accuracy
m=5 134(90.3%) 70.0%
m=7 154(88.4%) 78.2%
m=9 164(89.0%) 78.9%
m=20% 220(88.2%) 91.8%
m=40% 225(88.9%) 90.2%
m=60% 212(88.0%) 91.8%
m=80% 231(88.8%) 91.9%
presents that EM-IWAL and Random have impressive
performance. These two approaches are not affected by
the skewed high-dimension datasets in determining query
probabilities. However, CEAL and UCEAL do not work
very well for selecting informative unlabeled samples on
the high-dimension feature space. It seems that the initial
size of the neighborhood leads to gain an improper query
probability. Typically, classification algorithms require
enough labeled samples to solve complicated problems
which usually have a high-dimension feature space.
The large size of each unlabeled sample’s neighborhood
should be given in order to model the complicated classi-
fication problem. Thus on these high-dimension datasets,
additional experiments using CEAL with different initial
sizes of neighborhoods are presented in Table VII, VIII
and IX, where 𝑚 = 5, 7, 9 and m=20%,40%,60%,80% of
current queried samples. It is shown that CEAL obtains
similar or better performance than EM-IWAL although
several additional samples are required for generating
the well-informed neighborhood. Hence, CEAL has the
ability of determining a query probability for the high-
dimension unlabeled sample when the initial size of
neighborhoods is properly given.
Therefore, our experimental results are shown that the
certainty-enhanced neighborhood is properly explored by
finding large difference between empirical errors of error
minimization hypotheses. And the percentage of queried
majority samples is decreased since they are usually on
the certain areas and assigned for low query probabilities.
Ultimately, the proposed CEAL utilizes suitable local
behaviors of the neighbors and improves imbalanced
TABLE IX
PERFORMANCE OF CEAL ON MS+X WITH DIFFERENT SIZES OF
INITIAL NEIGHBORHOODS
Neighborhood Size (mj%) Accuracy
m=5 103(94.2%) 59.7%
m=7 107(91.0%) 67.6%
m=9 142(89.5%) 75.5%
m=20% 225(88.9%) 90.3%
m=40% 221(88.7%) 90.3%
m=60% 214(87.4%) 93.4%
m=80% 217(87.1%) 95.2%
TABLE X
ANALYSIS OF THE NUMBER OF NEIGHBORHOOD EXPANSIONS AND
THE SIZE OF THE EXPLORED NEIGHBORHOOD FOR EACH
UNLABELED SAMPLE. (AVG=AVEARGE, STD=STANDARD
DEVIATION)
Dataset Expansions Sizes
AVG STD AVG STD
9+16 2.34 2.72 5.34 2.73
10+16 1.78 1.68 4.78 1.68
cp+im 3.22 5.6 6.22 5.6
im+pp 2.1 2.48 5.1 2.48
CYT+ME1 2.23 3.6 5.23 3.6
MIT+VAC 2.0 2.6 5.0 2.6
ship+silver 1.69 1.65 45.2 22.07
ibm+mac 1.61 1.81 18.02 8.47
ms+x 1.68 1.7 21.81 10.69
data classification problems in streaming-based active
learning.
It is concerned that CEAL is additionally required to
identify nearest neighbors and generate hypotheses in
each neighborhood expansion. Those local information is
able to present specific behavior of samples explicitly, but
actually it consumes computational time to explore those
neighbors and produce hypotheses iteratively. Table X
presents, for each unlabeled sample, the average iteration
of neighborhood expansion and the average size of
the certainty-enhanced neighborhood. CEAL learners are
those that have the best performance on the real-world
datasets in Table III, IV, and VII. It is shown in Table X
that only several iterations of expansion are required to
explore the certainty-enhanced neighborhoods on each
datasets. Significantly as discussed above, large neigh-
borhoods are needed on the high-dimension datasets,
so that performance efficiency could be improved by
carefully handling high-dimension samples in measur-
ing distance among samples and generating classifiers
(hypotheses). Basically, this paper does not focus on
reducing the complexity of high-dimension datasets since
our main strategy is to utilize local behaviors in certainty-
enhanced neighborhoods for improving the imbalanced
data classification problem in active learning.
46
 9
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期： 100 年 9 月 27 日 
一、 參加會議經過 
2011 年國際電機電子工程師學會消費電子、通信和網路國際學術會議(IEEE CECNet) 
於 2011 年 4 月 16 日至 19 日在中國大陸咸寧舉行。此會議為消費電子、通信和網
路領域中最重要之年度大型研討會，各項消費電子、通信和網路相關研究議題、前
瞻之研究方向以及實際服務與應用皆在會議中被探討。研討會中的討論內容相當廣
泛，共分為四大類，有 Consumer Electronics Technology、
CommunicationEngineering and Technology、Wireless Communications 
計畫編號 NSC99－2221－E－194－023－ 
計畫名稱 Designing agnostic active learning algorithms to improve class 
imbalance problems 
出國人員
姓名 吳建佑 
服務機構
及職稱 
國立中正大學資訊工程研究所 
博士候選人 
會議時間 
2011年 4月 16日
至 
2011年 4月 19日 
會議地點 中國、咸寧 
會議名稱 
(中文) IEEE 消費電子、通信和網路國際學術會議 
(英文) International Conference on Consumer Electronics, 
Communications and Networks 
發表論文
題目 
(中文) 於 802.16e 網路之用戶站台俱多重連線的能源節約 
(英文)  Energy Saving with Multi-Connections over IEEE 802.16e 
Networks 
 11
Location and Handoff Management" 、"Capacity, Throughput, Outage and Coverage 
Multimedia in Wireless Networks" 、"Optical Networks and Systems Emerging 
Wireless/Mobile Applications "、"Intelligent Transportation Systems RFID 
Technology and Application" 、"Service Oriented 
Architectures,ServicePortability Communications Software and Services" 、
"Mobile Computing Systems "、"Agents, Knowledge-Based Technologies 
Bioinformatics Engineering "、"Computer Architecture and design Computer 
Networks and Security" 、"Data Mining and Database Applications Grid 
Computing" 、"High Performance Networks and Protocols Multimedia and Web 
Services "、"Network Reliability and QoS Neural Networks and Intelligent 
Systems" 和"Software Engineering and Agile Development"等議題。主要就是探
討消費性電子技術、通訊網路、無線網路領域及電腦工程技術的議題，本次會議所
收錄的論文篇數近六百篇，內容非常的豐富。除了主要會議之外，另有數個小型
workshops 同時舉行，及學術海報張貼會場。與會的學者有來自世界各國家。大會
每天都有邀請知名的專家學者進行專題演講，三天的會議在專題演講之後會進行分
組論文報告和討論。每天都有三個時段，多個 session 同時進行。參加本次會議的
主要目的是發表由筆者、指導教授李新林博士及何漢彰博士共同撰寫之論文─「於
802.16e 網路之用戶站台俱多重連線的能源節約」。本次筆者的論文被分類在學術海
報張貼群。另外，本屆會議中的 Keynote Speech 有不少新的研究方向被拿出來探討，
其中包括 Semantic Networking in Cyber-Society、Cloud Computing-Issues& 
Challenge、Standardization in 4G networks、Next Decade of Social,economical 
 13
定位議題、繞路議題、認證議題、廣播群播議題、換手議題、服務品質議題等等。
個人覺得網路安全在這個部分顯得特別重要，除了學術的理論之外，現在也很注重
是否可以實際用在日常生活 
中。因此，現在越來越多學者在各種議題裡都會特別地考慮安全的因 
素。 
最後，在服務與應用這個主題，以車載網路及多媒體網路為主要 
討論的議題。筆者特地去聆聽車載網路相關的 session。由於電子元件 
價格漸漸下降以及行車安全問題越來越受重視，各國政府紛紛投入智 
慧型運輸系統(Intelligent Transport Systems, ITS)的研究。包括美國的 
聯邦通訊委員會(Federal Communications Commission, FCC)就分配 
75MHz 的頻寬給車用系統使用，這技術被稱為專用短距離通信 
(Dedicated Short Range Communication, DSRC)。車載網路實際上是隨 
意型網路的一個衍生。不過車載網路有了一些特性：高移動性、移動 
方向是有規率性的、強調安全性、擁有全球定位系統(GPS)、行動裝 
置的整體數量很龐大。因此，設計一個安全以及有效率的資料傳輸， 
是有其必要性和重要性。至於多媒體網路議題，已經有許多學者利用 
跨層設計去達到一個比較好的效能，用來提供一些即時的服務 
(real-time service)。 
二、 與會心得 
「消費電子、通信和網路國際學術會議」是 IEEE 的新舉辦的會議，今年收錄各種
消費性電子、通訊網路及無線網路先進議題的論文，因此參與這個會議，可以吸取
The International Conference on Consumer Electronics, Communications and Networks
（CECNet 2011）
www.cecnetconf.org Apr. 16th-18th,2011 Xianning，China
IEEE Catalog Number（Print Version）: CFP1153N-PRT ISBN: 978-1-61284-470-1
Invitation
Mar. 24th,2011
Thank you very much for your paper submission to the International Conference on Consumer
Electronics, Communications and Networks (CECNet 2011) ; we are pleased to inform you that
your paper:
Paper ID: AC95662
Author(s): Chien-YuWu
Paper Title: Energy Saving with Multi-Connections over IEEE 802.16e Networks
has been accepted by CECNet2011. We cordially invite you to participate in CECNet2011 from
April 16th to 18th,2011 in Xianning, China. The conference program consists of keynote speech,
special sessions, oral and poster presentation. All accepted papers will be published by IEEE and
will be submitted to Ei Compendex.
CECNet has successfully invited some outstanding experts to make keynote speeches on the
conference. Welcome to participate in CECNet2011 and we are looking forward to meeting you in
Xianning, China!
For more information, pls visit our website: www.cecnetconf.org
Thanks again for your support!
Best regards!
CECNet2011 Organizing Committee
Xianning University, China
Scheduling
Results
f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12
M1
M2
C1,1
C1,2
C1,3
(a) The results is scheduled by tank-filling algorithm[5]
f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12
M1
M2
Scheduling
Results
C2,1
C2,2
C2,3
(b) The results is optimal solution
Figure 1. The example for tank-filling algorithm[5] and optimal solution with bandwidth requirement (BR) and delay bound (DB).
of each MS such that the energy consumption of all MSs is
minimized and maintains QoS.
A. Problem Formulation
We formulate the energy saving problem as an Integer
Linear Programming (ILP) problem, with the objective
function and constraints shown as Equations (1)-(5). The
decision variable yi, t ∈ {0, 1}, is set to one if the MS must
be active on frame t. The variable T is the scheduling
length (time unit is OFDM/OFDMA frame length). The
objective function of the ILP problem is to minimize energy
consumption of all MSs, shown as Equation (1).
min∑
t∈T
∑
si∈ ¯S
yi, t (1)
∙ Frame capacity constraint is represented as Equation
(2). Equation (2) denotes the total bandwidth assigned
to all connections that cannot exceed more than 1Ω,
where Ω denotes the capacity of a frame. Where Ci is
the connection sets that are established by MS si. The
connection ci, j ∈Ci denotes that the j-th connection is
established by MS si. The variable bi, j is the bandwidth
requirement of connection ci, j. The decision variable
xti, j ∈ {0, 1}, is set to one if the connection ci, j is
assigned to active on frame t.
∑
si∈ ¯S
∑
ci, j∈Ci
xti, j ×bi, j ≤Ω , ∀ t ∈ T (2)
∙ Bandwidth requirement constraint is represented as
Equation (3). On each sleep cycle, the bandwidth is
assigned to connection ci, j must satisfy its QoS require-
ment. In equation (3), the number of sleep cycle on
connection ci, j during scheduling length T is denoted
by T/T Si, j, where the T Si, j is the length of sleep cycle. For
effectively using bandwidth, the amount of bandwidth
assignment has to equal the bandwidth requirement of
connection ci, j during each sleep cycle.
∑k×T
S
i, j
t=(k−1)×T Si, j+1
xti, j ×bi, j = bi, j
,∀ si ∈ ¯S, ∀ ci, j ∈Ci, and 1 ≤ k ≤ TT Si, j (3)
∙ Energy consumption constraint is represented as
Equation (4). Equation (4) denotes that even though
multiple connections of an MS are active on a frame
simultaneously, the MS is only active on a frame period.
yi, t ≥ xti, j ,∀ si ∈ ¯S, t ∈ T, and ci, j ∈Ci (4)
∙ Sleep cycle constraint is represented as Equation (5).
The sleep cycle must be fixed on the Type II connection.
Hence, we design a sleep cycle constraint to fix the
sleep cycle of each connection. Equation (5) denotes
the two consecutive frames, with the interval being T Si, j,
must be the same as active or must sleep.
xti, j − x
(t+T Si, j)
i, j = 0
,∀ si ∈ ¯S, ∀ ci, j ∈Ci, and 1 ≤ t ≤ T −T Si, j (5)
B. An Example of Bandwidth Assignment
Assume that the wireless network system has two MSs
{s1,s2}. Each MS si has three connections ci, j, 1 ≤ j ≤ 3.
The connection ci, j has a data arrival rate τi, j, the arrival data
has a delay bound DBi, j, and the sleep cycle fixed at T Si, j.
Hence, assume that the data arrival rate for each connection
during a frame period as τ1,1 = 0.1Ω, τ1,2 = 0.1Ω, τ1,3 =
0.125Ω, τ2,1 = 0.1Ω, τ2,2 = 0.1Ω, and τ2,3 = 0.125Ω. Then,
assume that the delay bound for each connection as DB1,1 =
2Γ, DB1,2 = 3Γ, DB1,3 = 4Γ, DB2,1 = 2Γ, DB2,2 = 3Γ, and
DB2,3 = 4Γ, where Γ is the time length of a frame. Then,
the bandwidth requirement is bi, j = τi, j ×T Si, j.
10 20 30 40
0
1
2
The number of Mobile Stations
Th
e 
to
ta
l a
ct
iv
e 
ra
tio
 o
f a
ll 
M
Ss
TFA
BRS
Figure 3. The active ratio simulation with high bandwidth requirement
settings, 4Γ, 8Γ, and 16Γ, respectively. Therefore, the sys-
tem establishes a connection with nine combinations of
data arrival rates and transmission delay bounds, such as
(200bits/frame, 4Γ). In the high bandwidth requirement
experiment, the number of MS is ranged from 5 to 45.
The data arrival rate have three settings, 500bits/frame,
1000bits/frame, and 2000bits/frame, respectively. The trans-
mission delay bound also have three settings, 32Γ, 64Γ, and
128Γ, respectively. We consider two performance metrics:
(i) active ratio: the ratio of active frames for the system and
(ii) throughput: the capacity of the wireless network system.
A. Low bandwidth requirement experiment
In Figure 2, we show the bandwidth assignment results
of the TFA, the BRS, and the ILP with regard to active
ratio. The active ratio simulation is shown in Figure 2. The
BRS is almost close to the optimal solution, which is solved
using the ILP. While the number of MS is 20, the total
active ratio of all MSs of TFA is approximately 6.6; the
BRS is approximately 4.1. Hence, we demonstrate the energy
consumption of BRS as being much less than the TFA in the
low bandwidth requirement experiment.
B. High bandwidth requirement experiment
In Figure 3 and Figure 4, we show the bandwidth assign-
ment results of TFA and BRS with regard to active ratio and
throughput. While the number of MS is less than 15, the
active ratio is the same between TFA and BRS. Moreover,
the active ratio of the BRS is bigger than the TFA when the
number is ranged from 15 to 40, as Figure 3 shows; but the
TFA has less throughput, as Figure 4 shows. Since the TFA
has more connections is dropped than the BRS.
V. CONCLUSION AND FUTURE WORK
In this paper, we consider the energy saving problem of
IEEE 802.16e networks. We propose an ILP model and
a bandwidth reservation scheme that not only guarantees
the quality of service for real-time connections but also
minimizes power consumption of MSs. Simulation results
demonstrated that the proposed schemes BRS outperform
previous approaches TFA. The energy saving problem on
10 20 30 40
200
600
1000
1400
The number of Mobile Stations
Th
ro
ug
hp
ut
 (f
ram
es)
TFA
BRS
Figure 4. The throughput simulation with high bandwidth requirement
the WiMAX relay network will be considered in the future.
Regarding the path selection problems, we should select the
next hop with energy conditions considered.
ACKNOWLEDGMENT
This work was supported in part by Taiwan NSC under
grant no. NSC 99-2221-E-274-007 and NSC 99-2221-E-
194-023-. The author like to thank reviewers for their
insightful comments which helped to significantly improve
the paper.
REFERENCES
[1] IEEE Std 802.16e-2005, ”IEEE Standard for Local and
metropolitan area networks Part 16: Air Interface for Fixed
and Mobile Broadband Wireless Access Systems Amendment
2: Physical and Medium Access Control Layers for Combined
Fixed and Mobile Operation in Licensed Bands and Corrigen-
dum 1,” Feb. 2006.
[2] IEEE Std 802.16-2009, ”IEEE Standard for Local and
metropolitan area networks Part 16: Air Interface for Broad-
band Wireless Access Systems,” May 2009.
[3] T.-C. Chen, J.-C. Chen, and Y.-Y. Chen, ”Maximizing Un-
availability Interval for Energy Saving in IEEE 802.16e
Wireless MANs,” IEEE Transactions on Mobile Computing,
vol. 8, no. 4, pp. 475-487, Apr. 2009.
[4] S.-C. Huang, R.-H. Jan, and C. Chen, ”Energy Efficient
Scheduling with QoS Guarantee for IEEE 802.16e Broadband
Wireless Access Networks,” in Proc. of the International con-
ference on Wireless Communications and Mobile Computing
(IWCMC), 2007.
[5] J.-J. Chen, J.-M. Liang, and Y.-C. Tseng, ”An Energy Effi-
cient Sleep Scheduling Considering QoS Diversity for IEEE
802.16e Wireless Networks”, IEEE ICC, 2010.
[6] W.-H. Liao , W.-M. Yen, ”Power-saving scheduling with a
QoS guarantee in a mobile WiMAX system”, Journal of
Network and Computer Applications, v.32 n.6, p.1144-1152,
November, 2009
[7] S.-L. Tsao and Y.-L. Chen, ”Energy-efficient packet schedul-
ing algorithms for real-time communications in a mobile
WiMAX system,”Computer Communications, vol. 31, no. 10,
pp. 2350-2359, June 2008.
 7
少。 
二、與會心得 
該會議針對消費性電子產品、通訊、無線通訊、與計算機之工程與技術議題。其中
消費性電子產品之廣告行銷主題與本人之研究主題相符，參與該會議增加學術研究
之國際觀。  
筆者依研討會預定之時間以海報演示呈現。與數名國際學者介紹並討論目前的研究
成果與方向。除此之外，亦了解不同研究領域學者看待筆者研究之問題的思考點與
其未來發展。這些國際學者不約而同的提及一個相同的問題：論文中設計的拍賣機
制是否能實際運用於實做系統中。這是一個關於實做的考慮，表示除了針對理論的
學術研究之外，應加入實做考量的因素。學術研究若有助於解決現實生活中的問題，
民眾才能受惠。故於未來的研究中，能否實際運用於實做系統中是一個必備的考量
因素。 
三、考察參觀活動(無是項活動者略) 
四、建議 
參與國際會議能接觸不同國家的學者，透過演示與解說的討論能夠提供學生相當有
用的建議，對於未來的研究能夠給予正面的幫助。同時，聆聽與了解其它學者目前
的研究結果有助於掌握國際上目前的主流研究，並增加國際學術觀。感謝教育部補
助筆者參加本次國際會議的相關費用。也希望教育部未來能提供更多各方面的補助
案，讓研究學者能以更少的負擔參加學術交流會議。 
五、攜回資料名稱及內容 
由主辦單位攜回物品如下 
The International Conference on Consumer Electronics, Communications and Networks 
（CECNet 2011） 
 
www.cecnetconf.org    Apr. 16th-18th,2011    Xianning，China 
IEEE Catalog Number（Print Version）: CFP1153N-PRT  ISBN: 978-1-61284-470-1 
 
Acceptance Notification 
Mar. 25, 2011 
 
 
Dear Author, 
 
Congratulations! We are extremely glad to inform you that your paper: 
 
Paper ID: AC46659 
Author: Tsung Chen-Kun,Ho Hann-Jang,Lee Sing-Ling, 
Paper Title: Repeated Sponsored Search Auction with Non-decreasing Bid Values 
 
has been accepted for presentation at the International Conference on Consumer Electronics, 
Communications and Networks (CECNet 2011) . 
 
Please finish all registration procedures before Mar. 31, 2011 by the Registration Instructions in 
attached.  
 
We are grateful for your contribution to CECNet2011. And we are looking forward to meeting 
you in Xianning, China. We also hope that you will contribute your excellent work to future 
CECNet conferences. 
  
For more information, please visit our website: www.cecnetconf.org 
Thanks for your support again! 
Best regards! 
 
 
CECNet2011 Organizing Committee 
 
Xianning University, China 
 
only at the minimum price to win the target position [11], e.g.
one dollar higher than the target bid rather achieving Nash
equilibrium. For the repeated SSA, although bidding at higher
value will not increase the payment in this round, the advertiser
in the next round may pay more money due to choose a higher
bid. For advertiser’s perspective, proposing the minimum price
to winning the target position is more close to the real world
behavior.
Cary et al. propose balance bidding strategy to discuss the
convergence in repeated SSA [6]. If a repeated SSA converges
to a fixed outcome, SEP’s revenue is expectable. However, not
all instances will converge in [6], and SEP’s revenue is hard
to expect. For SEP’s perspective, designing a mechanism with
convergence property is our objective.
III. NDSSA
We follow the auction definition in [6], which is proposed
by Cary et al.
Consider n advertisers {ad1, ad2, . . . , adn} compete for
k advertising slots {sl1, sl2, . . . , slk}, while n > k. Each
advertiser adi has the worth vi when the advertisement is
clicked by the Internet user, vi > v2 > . . . > vn. Each
advertising slot slj has a CTR θj which is a probability that the
Internet user will click on, θ1 > θ2 > . . . > θk and ∀θj < 1.
After proposing bids to SEP, all advertisers are ranked in
the decreasing order of bid value. The clicked advertiser is
charged according to GSP. Therefore, if adi is allocated slj ,
the expected profit of adi is ui(j) = θj(vi − pji ), where pji is
adi’s payment in slj .
The modification of the mechanism is as follows. Similar
to English auction, only non-decreasing bids are allowed in
NDSSA. Suppose adi is in slj . Each advertiser will propose
higher bids to compete for previous slot slj−1 until overbid-
ding or the profit is maximized in the current slot slj . In
other words, if winning the previous slot is benefited and
without overbidding, i.e. ui(j) ≥ ui(j − 1) and bi ≤ vi,
the advertiser will propose the price (bj−1 + 1) to defeat the
previous advertiser, where bj−1 is the (j − 1)th high ranked
bid. Otherwise, the bid value will not be updated.
When all advertisers propose the bids equal to that in the
last round, we say that NDSSA is converged.
IV. REVENUE ANALYSIS
NDSSA will converge while 1) overbidding or 2) the profit
of current slot is maximized, which is also called locally envy-
free equilibrium [1]. The bid value of condition one will not
be lower than that of condition two. So we focus on envy-free
bids after converging.
The superscripts of “N” and “V” in the following context
represent the labels about NDSSA and VCG respectively. For
the convenience, we assume that adi is ranked in sli, i.e. b1 >
b2 > . . . > bn.
Theorem 1. SEP’s revenue in NDSSA, denoted by TRN , is
at least 1/θ1 times of that in VCG, denoted by TRV , when
the convergence bid set meets the locally envy-free equilibrium
under truthful bidding.
TRN ≥ 1
θ1
× TRV
Proof: According to the locally envy-free bids in [1], we
get the following inequality.
bNi ≥
vNi θi−1 − uNi (i)
θi−1
=
1
θi−1
× (vNi (θi−1 − θi) + bNi+1θi) (1)
Similarly, we have bNi+1 ≥ 1θi×(vi+1(θi−θi+1)+bNi+2θi+1).
Replacing bNi+1 in eq. (1), we receive more details about b
N
i .
bNi ≥
1
θi−1
{vNi (θi−1 − θi) + [
1
θi
(vNi+1(θi − θi+1)+
bNi+2θi+1)]θi}
bNi ≥
1
θi−1
{vNi (θi−1 − θi) + vNi+1(θi − θi+1) + bNi+2θi+1)}
(2)
After repeatedly replacing bid values in eq. (2), we have the
general form of bNi .
bNi ≥
1
θi−1
(vNi (θi−1 − θi) + vNi (θi+1 − θi) + . . .+ bNk θk)
=
1
θi−1
×
k∑
s=i
vNs (θs−1 − θs) (3)
Since pNi = b
N
i+1, adi’s payment is:
pNi = b
N
i+1
≥ 1
θi
×
k+1∑
s=i+1
vNs (θs−1 − θs) (4)
When each advertiser bids truthfully, the bid value of
NDSSA and VCG is identical, i.e. bVi = b
N
i . According to
VCG’s payment pVi =
∑k+1
s=i+1 v
V
s (θs−1−θs) and the truthful
bidding assumption bVi = b
N
i , this proof completes.
TRN =
k∑
i=1
pNi =
k+1∑
i=2
bNi
≥
k+1∑
i=2
1
θi−1
k∑
s=i
vs(θs−1 − θs)
=
k+1∑
i=2
1
θi−1
pVi−1
≥ 1
θ1
k+1∑
i=2
pVi−1 =
1
θ1
× TRV
bN2 ≥
1
θ1
(vV2 (θ1 − θ2) + vV3 (θ2 − θ3) + . . .). (9)
Adding vN3 + v
N
4 + . . . v
N
k to the LHS of eq. (9), the
inequality of bN2 can be rewritten as follows.
vN2 + . . .+ b
N
k ≥ bN2
≥ 1
θ1
(vV2 (θ1 − θ2) + vV3 (θ2 − θ3) + . . .)
= vV2 (
θ1 − θ2
θ1
) + vV3 (
θ2 − θ3
θ1
) + . . .
(vN2 − vV2 (
θ1 − θ2
θ1
)) + (vN3 − vV3 (
θ2 − θ3
θ1
)) + . . . ≥ 0 (10)
The valuation of NDSSA can be replaced by VCG due to
our assumption vN2 = v
V
2 .
vV2 (1−
θ1 − θ2
θ1
) + vV3 (1−
θ2 − θ3
θ1
) + . . . ≥ 0 (11)
Each term is positive in eq.(11) because of (θi−θi+1)/θ1 ≥
0 and vVi ≥ 0. The assumption vN2 = vV2 is correct.
Our assumption does not have any contradiction from ad2
to adk+1. NDSSA consists of k winners, and the valuations
are the same with VCG except for ad1. According to the pi-
geonhole principle [4], ad1’s valuation in NDSSA is identical
to that in VCG. No contradiction is in our assumption. Thus,
the advertiser order in NDSSA is equivalent to that in VCG,
and NDSSA is efficient.
V. CONCLUSION
Applying GSP to the repeated SSA may reduce SEP’s
revenue because advertisers may be benefited by proposing
lower bids. We propose NDSSA to improve SEP’s revenue
through applying the ascending biding behavior of English
auction. We proof that SEP’s revenue lower bound in NDSSA
is α-VCG, while α = 1/θ1. In other words, SEP in NDSSA
is more beneficial than in VCG in repeated SSA. Moreover,
NDSSA is efficient.
Similar to the bid increment issue in English auction,
the SEP in NDSSA also confronts the same determination
problem. In this paper, the bid increment can be treated as
a small value. Higher bid increment settings will speed up
the convergence, but SEP’s revenue may be maximized in
lower bid increment settings. This is the tradeoff between
convergence speed and SEP’s revenue. We will focus on
finding the strategy of determining bid increment to balance
convergence speed and SEP’s revenue.
ACKNOWLEDGEMENT
This work was supported in part by Taiwan NSC under grant
no. NSC 99-2221-E-194-023 and NSC 99-2221-E-274-007.
The author would like to thank reviewers for their insightful
comments which helped to significantly improve the paper.
REFERENCES
[1] B. Edelman, M. Ostrovsky, and M. Schwarz, “Internet Advertising and
the Generalized Second Price Auction: Selling Billions of Dollars Worth
of Keywords,” American Economic Review, Vol. 97(1), pp. 242-259,
March 2007.
[2] H. R. Varian, “Position Auction,” International Journal of Industrial
Organization, Vol. 25(6), pp. 1163-1178, December 2007.
[3] A. Wayne, “Inequalities and Inversions of Order,” Scripta Mathematica,
Vol. 12(2), pp. 146-169, 1946.
[4] R. P. Grimaldi, Grimaldi Discrete and Combinatorial Mathematics: An
Applied Introduction, Addison-Wesley Press, 1998.
[5] T. M. Bu, X. Deng, and Q. Qi, Forward looking Nash equilibrium for
keyword auction, Information Processing Letters, Vol. 105, pp. 41-46,
2008.
[6] M. Cary, A. Das, B. Edelman, I. Giotis, K. Heimerl, A. R. Karlin, C.
Mathieu, and M. Schwarz, Greedy Bidding Strategies for Keyword Auc-
tions, Proceedings of the 8th ACM Conference on Electronic Commerce
(EC-07), pp. 262-271, 2007.
[7] A. Ghosh and M. Mahdian, Externalities in Online Advertising, Proceed-
ing of the 17th international conference on World Wide Web (WWW),
pp. 161-168, 2008.
[8] G. Aggarwal, J. Feldman, S. Muthukrishnan, and M. Pal, Sponsored
Search Auctions with Markovian Users, Proceedings of the 4th Interna-
tional Workshop on Internet and Network Economics (WINE), pp. 621-
628, 2008.
[9] G. Aggarwal, A. Goel, and R. Motwani Truthful Auctions for Pricing
Search Keywords, Proceedings of the 7th ACM Conference on Electronic
Commerce (EC-06), pp. 1-7, 2006.
[10] F. Brandt, T. Sandholm, and Y. Shoham, Spiteful Bidding in Sealed-
Bid Auctions, Proceedings of the 20th International Joint Conference on
Artificial intelligence (IJCAI), pp. 1207-1214, 2007.
[11] G. Aggarwal, S. Muthukrishnan, and J. Feldman, Bidding to the Top:
VCG and Equilibria of Position-Based Auctions, Proceedings of the 4th
Workshop on Approximation and Online Algorithms (WAOA), 2006.
[12] G. Linden, C. Meek, and M. Chickering, Keyword Auction Protocol for
Dynamically Adjusting the Number of Advertisements, Web Intelligence
and Agent Systems, Vol. 8, No. 4, pp. 331-341, 2010.
99 年度專題研究計畫研究成果彙整表 
計畫主持人：李新林 計畫編號：99-2221-E-194-023- 
計畫名稱：以改善 class imbalance problems 為目標設計 agnostic active learning 演算法 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 7 6 100%  
博士生 4 2 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 2 100%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
