2the user can conveniently design (and re-design)
his personal pet robot according to his
preferences. In our framework, we adopt the
behavior-based architecture ([5]) to implement
control system for a pet robot. Different
interfaces are constructed to support various
human-robot interactions, including device-based
control, speech-based control and gesture-based
control. To evaluate our framework, we use it to
construct a control system for the popular LEGO
Mindstorms robot. Experimental results show that
the proposed framework can efficiently and
rapidly configure a control system for a pet robot,
and its human partner can interact with the pet in
different ways. In addition, experiments are
conducted in which a neural network is used for
the pet robot to learn how to coordinate different
behaviors for a user-specified emotion model.
2. Metholology
Our aim is to develop a user-centered
framework that can assist a user to rapidly design
a pet robot. Robot design involves the
configuration of hardware and software.
Expecting an ordinary user to build a robot from a
set of electronic components is in fact not
practical. Therefore, in this work we concentrate
on how to imbue a robot with a reliable and
robust control system.
Our framework mainly includes three
modules to deal with the problems in building pet
robots. The first module is to design an efficient
control architecture that is responsible for
organizing and operating the internal control flow
of the robot. Because behavior-based control
architecture has been widely used to construct
many robots acting in the real world successfully,
our work adopts this kind of architecture for
robots. The second module is about human-robot
interaction. As a pet robot is designed to
accompany and entertain its human partner in
everyday life, interactions between the owner and
his pet are essential. Our framework provides two
natural ways, voice-based and gesture-based
methods, for human-robot interaction and
communication. The third and the most important
module is a learning mechanism that includes two
parts to resolve the problems of behavior creation
and behavior arbitration. The first part is to
evolve/learn new behavior primitives; and the
second part, a neural network-based emotion
system for behavior arbitration. With the emotion
system, a pet robot can act autonomously. It can
choose whether to follow the owner’s instructions,
according to its internal emotions and body states.
Our framework is designed to be user-centered
and has a modular structure. The user can use it
build and change any part of the control system
for his robot.
To automate the process of building
behavior primitives, we use an evolutionary
approach to learn behavior primitives. In this
work, we employ a genetic algorithm system to
evolve reactive behavior controllers, in which a
controller is a two-layer feedforward neural
network. The encoding scheme here is a
one-to-one mapping in which each weight and
bias of the neural network is a floating point
number corresponding to a gene within the string
chromosome.
To coordinate different behavior controllers
obtained from the above procedures, our
framework uses a special mechanism that exploits
emotions for selection of behavior. To implement
the emotion model, we need to select the emotion
elements to constitute the model. As our goal here
is to establish a framework for constructing
personal pet robots, rather than to investigate the
interactions and relationships between cognitive
and emotion systems, our work only models basic
emotions to coordinate the behave controllers the
user has pre-chosen for his pet. To quantify the
emotions, we define each of the basic emotion as
a special mathematical formula. In addition to
emotions, another set of internal variables is
defined to describe a robot’s body states.
The emotion model determines the innate
characteristics of a robot, which are highly related
to its abilities of learning and adaptation. As we
have emphasized, the design of pet robot must be
user-centered. Therefore, in addition to the
emotion model described above, the user’s
opinions (or expectations) need to be taken into
account in training a pet robot. To achieve this
goal, the neural network module is used to map a
set of emotion and body state data into
appropriate behaviors under the user’s guidance.
3. Results
To evaluate the proposed ANN-based
methodology, experiments have been conducted.
In the experiments of learning robot behaviors,
the robot was expected to use its gripper to pick
up the object in the environment, and then put the
4investigates the issues of distributed robot control
and robot emotion, in which the control system
for a pet robot is composed of a set of
feedforward network networks. To automate the
design of control systems, an evolutionary system
has been developed for creating new behavior
primitives. In addition, an emotion system has
been developed in which different emotions and
internal states have been modeled and used to
derive a behavior arbitrator. The behavior
arbitrator is a neural network and the user is
allowed to define training examples to infer a
personal arbitrator for his robot. To evaluate our
framework, we have used it to build robot
controllers to achieve various tasks successfully.
Reference
[1] Fujita, M. (2004) ‘On activating human
communications with pet-type robot AIBO’,
Proceedings of the IEEE, Vol. 92, pp.1804-1813.
[2] Perzarcowski, D., Schultz, A., Adams, W., Marsh,
E. and Bugajska, M. (2001)‘Building a multimodal
human-robot interface’, IEEE Intelligent Systems,
Vol. 16, pp.16-21.
[3] Fellous, J.-M., Arbib, M. Eds. (2005) Who Needs
Emotions? The Brain Meets the Robot. Oxford
University Press.
[4] Damasio, A. R. (1994) Descarte’s Error: Emotion, 
Reason, and Human Brain. NY: Grosset/Putnam
Press.
[5] Arkin, R. C. (1998) Behavior-Based Robotics. MA:
MIT Press.

control methods. These methods mean to establish direct
contact with robots through their own receivers. Pet robots are
service robots that live in the same space with people. The
direct and natural interactions between pet robots and human
are thus especially essential. With such considerations,
specific interfaces for human-robot interactions need to be
developed to design.
The third issue to be considered is emotion, an important
drive for a pet to present certain level of intelligence [8][9]. In
fact, Antonio Damasio has suggested that efficient
decision-making largely depends on the underlying
mechanism of emotions, and his research has shown that even
in simple decision-making processes, emotions are vital in
reaching appropriate results [10]. Therefore, our work
includes an emotion system to take advantages of cognitive
processes to solve the behavior selection problem within the
behavior-based control system. By the emotional model, the
pet robot can explicitly express its internal conditions through
the external behaviors, as the real living creature does. The
owner can thus understand the need and the status of his pet
robot to then make appropriate interactions with it.
Though there have been several toy-type entertainment
robots available, mostly designed with a fixed prototype and
characteristics (e.g., [11][12][13]). Sony’s robot dog AIBO
and humanoid robot QRIO are sophisticated toys with
remarkable motion ability generated from many flexible
joints. But these toy robots are too expensive to be popular.
Also the owners are not allowed to reconfigure the original
design. Instead of building an accurate but expensive
entertainment robot, in this work, we present a system through
which an ordinary robot owner can easily reconfigure his
robot, including the changes of robot behaviors, the
interacting ways, and the emotion expressions. The system is
user-oriented, so the robot owner can decide the
characteristics of his robot according to his preferences, and
has a unique personal robot to be one’s companion.
III. BUILDING EMOTION-BASED PET ROBOTS
A. A User-Oriented Framework
Our aim is to develop a user-oriented approach that can
assist a user to rapidly design (and re-design) a special and
personal robot. Robot design involves the configuration of
hardware and software. Expecting an ordinary user to build a
robot from a set of electronic components is in fact not
practical. Therefore, instead of constructing a personal pet
robot from the electronic components, in this work we take
the reconfigurable LEGO Mindstorms robot as the hardware
platform, and concentrate on how to imbue a robot with a
personalized control system.
Fig. 1 illustrates the system framework of the proposed
approach. As can be seen, our system mainly includes three
modules to deal with the problems in building a control
system for a pet robot. The first module is to design an
efficient control architecture that is responsible for organizing
and operating the internal control flow of the robot. Because
behavior-based control architecture has been used to
construct many robots acting in the real world successfully,
our work adopts this kind of architecture to design control
systems for robots. The second module is about human-robot
interaction. As a pet robot is designed to accompany and
entertain its human partner in everyday life, interactions
between the owner and his pet are essential. Here we develop
three ways for human-robot interaction and communication,
including device-based (keyboard and mouse), voice-based
and gesture-based methods. The third module is an emotion
system that works as the arbitration mechanism to resolve the
behavior selection problem within the behavior-based control
architecture. With the emotion system, a pet robot can act
autonomously. It can choose whether to follow the owner’s
instructions, according to its internal emotions and body
states. Our system is designed to be user-oriented and has a
modular structure. With the assist of the presented system, a
user can build his robot according to his preferences. If he is
not satisfied with what the robot behaves, he can change any
part of the control software for further correction. Details of
each module are described in the following subsections.
Fig.1: The overall architecture of our system framework
B. Control Architecture
As in the behavior-based control paradigm, the behavior
system here takes the structure of parallel task-achieving
computational modules to resolve the control problem. In
order to achieve more complex tasks in a coherent manner,
the behavior modules developed have to be well organized
and integrated. Inspired by the ethological models originally
proposed to interpret the behavior motivations of animals,
(for example, [14][15]) robotists have developed two types of
architectures: the flat and the hierarchical ones. The former
arranges the overall control system into two levels; and the
latter, multiple levels. In the flat type arrangement, all
subtasks are independent and have their own goals. Each of
them will be achieved by a separate behavior controller,
which generates its own output according to the sensory input
directly relevant to the subtask itself. Those independent
outputs from the separate controllers will be combined
appropriately in order to generate the final output for the
overall task. As this work means to provide an interactive and
easy-to-use framework for the design and implementation of
pet robots, the straightforward way (that involves less task
decomposition techniques), the flat architecture, is chosen to
be the default control structure for a pet.
Using behavior-based control architecture, one needs to
deal with the corresponding coordination (or action selection)
problem. It is to specify a way to combine the various outputs
from the involved controllers. Our work here takes the
method of command switching that operates in a
winner-take-all fashion. That is, only one of the output
commands from the involved behavior controllers is chosen
cognitive processes and are thus much faster in controlling
motion than those culturally determined experiences residing
in the cognitive system (long term memory). As our goal here
is to establish a framework for constructing personal pet
robots, rather than to investigate the interactions and
relationships between cognitive and emotion systems, our
work only models basic emotions to coordinate the behave
controllers the user has pre-chosen for his pet. To quantify the
emotions, we define each of the basic emotion as: Ei(t) =
Ei(t-1) +α× Eventi, in which Ei(t) represents the quantity of
emotion Ei at any time t, αis a user-specified weight, and
Eventi is a procedure describing how emotion Ei is affected by
a set of events pre-defined by the user. For example, a user
can define an event to be the appearance of a stranger. When
this happens, “fear”(one kind of emotion of the robot) will
increase one unit accordingly. An experienced user can define
a more sophisticated procedure to alleviate the effect caused
by the same event occurring during a short period of time.
In addition to emotions, another set of internal variables is
defined to describe a robot’s body states (e.g., hungry). These
variables represent the basic requirements the robot has to be
satisfied, and they must be regulated and maintained in certain
ranges. In our framework, the range of each internal variable
can be defined by the user, and the corresponding
specification determines the innate characteristics of his robot.
Similar to the emotion functions defined above, an internal
variable here is described by a mathematical formula with a
set of involved events, also specified by the user.
Fig. 4 shows the aspect of the emotion system used. The
emotion model determines the innate characteristics of a robot,
which are highly related to its abilities of learning and
adaptation. With the above model, at any given time, the
emotions of the robot can be derived and used to trigger the
behavior controller to generate a specific action. After the
action is performed, the external environment conditions will
thus change and that can further affect the emotion of the
robot. The pet robot then makes new decision for behavior
selection, based on the modified quantities of emotions. For
example, a pet dog in a hungry state may be angry, may keep
looking for food, and would eat anything as soon as the pet
dog finds it. After that, the dog may not be hungry any more
(the body state has been changed). Then it is happy (the
emotion has been changed too) and may want to sleep or fool
around (now new behavior is selected). The above procedure
is repeated and the emotion model continuously works as a
decision-making mechanism for behavior selection.
Fig. 4: Emotion-based behavior coordination
As can be observed, in the above operating model, the most
important part is the one for selecting appropriate behavior at
any time. In this work, we use a feedforward neural network to
implement such a mechanism that maps the emotions and
body states of a robot into a set of desired behaviors. To allow
the user to determine the characteristics of his pet, our work
also provides an interface by which the user can define
examples to train the neural network to achieve the specified
mapping of emotions and behaviors. Once the neural network
is obtained, it works as a behavior selector to choose
appropriate controllers for the robot.
IV. EXPERIMENTS AND RESULTS
To evaluate our approach, two LEGO robots have been
built and a distributed and networked computing environment
has been developed for the robots. An example is
demonstrated in which two robots cooperate together to
achieve a box-pushing task. In the second phase, we show
how the emotion-based mechanism can be used to train a
neural network as a behavior selector for the robot.
A. Implementation
The robot used in the experiments is LEGO Mindstorms
NXT 9797. It has light sensors for light detection, ultrasonic
sensors for distance measurement, and micro-switches for
touch detection. In addition, to enhance its vision ability, we
equip an extra mini-camera on the head of the robot so that it
can capture images for further interpretation. Fig. 5 shows the
pet robot built for the experiments, in which the robot uses
two connected NXTs to collect signals and to execute control
programs.
Fig. 5: The LEGO robot with a mini-camera
Because the LEGO NXT only has limited computational
power and memory devices, it is thus not possible to perform
all the computation for the emotion system and speech/vision
processing on the on-board processor of the robot. To
implement the architecture presented in section 3.1, a
distributed and networked computing environment is
constructed for the robot. Figure 7 illustrates the architecture
of the computing environment. As can be seen in Figure 7, the
individual pre-programmed behavior controllers are installed
in the NXT on-board memory, and two PCs are connected
through the Internet for other computation. Here, one PC is
responsible for the computation of emotion modeling, speech
recognition, and communication management, and the other
PC is used for image/gesture recognition. The images
collected from the mini-camera on the robot are sent to the
host PC wirelessly for further processing. After the images are
interpreted as described in section 3.3, the result is sent to
another PC and used as perceptual information to determine
how to command the robot. The network-based computing
environment can be extended to deal with more computation
when more robots are involved.
mini-camera
