  
 
 
 
 
 
 
摘要 
 本計劃已針對辦公室環境完成設計、組
裝一台全新的人工智慧互動式服務型機器
人。其中機器人可在辦公室環境中,提供多樣
性的服務;本研究團隊也根據各種辦公室情
境賦予機器人各種不同的功能使得其可與
辦公室成員共同合作。 
除此之外,本團隊為了使機器人有完整的
人機互動功能,我們使用了兩種不同的攝影
鏡頭來實現行人偵測與追蹤以及物品偵測
與辨識,並且在機器人前後都搭載了雷射測
距裝置,使其能更準確的偵測出辦公室成員
進而提供服務。 
    除了使用上述裝置之外,本團隊更建置了
一套更自然的方式提供辦公室成員與機器
人互動,辦公室成員可經由觸控式介面與機
器人互動,以及經由機器人身上的麥克風透
過語音辨識系統直接透過聲控的方式與機
器人互動。然而,由於辦公室環境是非常多變
的,因此本團隊研發出一套情境感知系統,使
得此辦公室機器人可以更靈活的於辦公室
環境中提供服務,同時透過有限狀態機來處
理各種緊急任務以及辦公室成員所要求的
服務,本報告內容將針對此計畫的計畫執行
成果進行詳細的說明。 
 
關鍵詞: 人機互動介面、機器人導航技術、
機器人視覺、 機器人情境感知與運算技術 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Abstract— This report presents our new 
intelligent interactive robot, which is con-
structed to eagerly provide multi-functional 
services in an office environment. We inves-
tigate the conditions under which users can 
accept collaborating work with a robot in an 
office environment as well as general users' 
expectation of the robot’s ability. Besides, in 
order to endow a full interactive capability 
of our robots for realizing so-called hu-
man-robot interaction (HRI), there are two 
kinds of cameras to accomplish human de-
tection/tracking and object detec-
tion/recognition. In addition to cameras, 
there are two kinds of lasers mounted in the 
front and at the back of the robot to detect 
and track human legs. Not only by these 
sensors, human interact with the robot also 
by some natural way, such as touching the 
interface screen and talking with the robot 
through microphone. Last but not the least, 
since the office environment is dynamic, this 
paper proposes a context-aware navigation 
method to deal with some situations which 
may take place in the office environment. 
For overall situation, we design a finite state 
machine (FSM) approach to enable the ro-
bot to solve the emergent tasks and to ac-
commodate human users in real office en-
vironment. Finally, the effectiveness of the 
proposed work is tested and validated by 
some of experiments. 
 
Keywords: HRI, Navigation, Vision, Con-
text-aware. 
 
 
 
新世代辦公大樓之「智慧型服務機器人族」(1/3) 
子計畫一：辦公室多功能智慧型秘書機器人 (1/3) 
 
計畫編號：NSC99-2221-E-002-191 
 主  持  人：傅立成 教授   台大電機系、資工系 
 共同主持人：岳修平 教授   台大生物產業傳播暨發展學系 
 
Table 1  
The specifications of the office robot 
size 50cm × 70cm × 150cm 
weight 70kg 
shape fiber glass reinforced plastics 
skeleton aluminum 
driving type two-wheel differential type 
driving motor DC servo motor 
max. speed 1 m/s 
batteries 16V Li-ion Battery 
computers Industry computer and Mac mini 
display 8.9”  panel and 12” touch panel 
camera  monocular camera and stereo 
laser SICK and Hokuyo 
continuous op-
eration time 
0.5~2 hours 
 
  
(a) (b) 
 Fig. 2. The office robot. 
Intelligent Interactive Robot 
The appearance, as shown as in Fig. 2, and 
specification of our office robot is briefly 
described in this section. The design 
philosophy is to build an autonomous mobile 
robot with various functions so that it can 
exhibit the capability for interacting with 
human. There are two computers employed in 
charge all of the computational tasks, and they 
communicate with each other through a local 
area network. The computational tasks of the 
office robot are divided into low and high levels. 
PC-1 is employed for low level computations, 
taking care of such as motor motion and 
intelligent navigation, whereas PC-2 is 
employed for high level computations, 
handling e.g. human-robot interaction, speech 
recognition, vision, etc.  
     We plan to show the robot’s expression on 
the top screen; human can interact with our 
robot by touch screen or through speech 
recognition. Moreover, the single camera is 
used for face or human detection, whereas the 
stereo camera is used to obtain depth 
information of the objects. For grasping ability, 
the robot has a strong arm which can move up 
and down, and stretch forward owing to catch 
the objects powerfully. Our robot can get 360 
degree range information by using two lasers, 
they not only are used for localization and 
navigation but also detect human legs to 
understand where human are. Due to the safety 
and emergency considerations, one can use a 
laptop computer to monitor the status of the 
robot through wireless network. Table 1 lists 
various specifications of our office robot. 
Context-Aware Navigation 
In this section, we proposed a context-aware 
motion planner considering both human and 
environment contexts. We define four 
situations as shown in the flow chart in Fig. 3. 
First, we examine if there is any human with 
interaction interest. That is, we check if there is 
any laser point with Human Sensitive Field 
(HSF-I) attribute. If so, the robot will approach 
the human and interact with him/her. On the 
contrary, we will apply the second criterion in 
gap. 
 
analysis to investigate if there is any navigable 
region. If so, the robot is in situation (b) with 
blending context. The corresponding action 
will depend on the attribute of the distributed 
laser data. If there is no navigable region, the 
robot will check if there is any human in sight. 
If not, the robot is in a dead end, and hence it 
has to make an about turn (situation (c)). But if 
yes, we design a special mechanism for the 
robot to leave the crowd when it is sort of stuck 
and blocked by them (situation (d)). 
 
 
Fig. 3. Flow Chart of Motion Decision 
 
'
s i
i
s
D d
t
D

  (2) 
where 
sD  is the security region and
'
iL   is the 
laser attribute, and   
     is the property at-
tribute, “Near Obstacle”. We also define the 
angle threat imposed from '
iL  with Ai
' = NO  
 
1 ,    if 
1,     if 
start i
i start
i
i end
i end
 
 


 
 


 
 
  

 (3) 
Noticed that [0,1]it   and [0,1]i  . Next, we 
define the deflection factor 
 b =
t
n
d
n
n|A
n
=NO  and n>end{ }
å
N
L
+
t
n
d
n
n|A
n
=NO  and n<start{ }
å
N
R
b Î [-1,1] (4) 
If the obstacles on the right-hand side dominate, 
1  . On the contrary, 1   when left-hand 
ones dominate. Thus, we can derive the safe 
angle as follows : 
 
( 1)
( )
2
safe start end start

   

    (5) 
and 
start safe end    . In this case, the urgency 
index   will be taken into account. Eventually, 
the desired angle 
d  can be computed as  
 ( )d safe closest safe        (6) 
 
Taking Fig. 5, for example, if 1  , the robot 
will take an aggressive path. On the other hand, 
the robot will take a safe path when its task is 
not urgent. In this case, it satisfies the collision 
free criterion and human-oriented criterion. 
Moreover, we use the urgency index   in this 
situation so that the interaction-admissible cri-
terion and task-dependent motion criterion are 
also fulfilled. 
 
           
 
(a)                                         (b) 
Fig. 6. (a) Subset of feasible paths (b) Lat-
tice-based graph 
 
4) About Turn 
Due to the range of laser range finder, it is 
likely for the robot to encounter a dead end. 
When there are many static obstacles in front of 
the robot, it is impossible for the robot to pass 
through such area with no navigable region. 
Our strategy is to rotate either clockwise or 
counter-clockwise until it finds navigable re-
gion or humans. We can prevent unnecessary 
collision with this method. In this situation, it 
satisfies collision free criterion. 
5) Pass through the Crowd 
In this situation, although no navigable re-
gion is found, there are crowds in front of the 
robot. In real world when we encounter crowds, 
we usually approach the crowds slowly and see 
if there is any potential free walking area to 
pass through. 
We develop a short-term path planning 
technique by using lattice-based graph search 
[14], [15]. In addition, in order to estimate 
human's motion more precisely, we use Ex-
tended Kalman Filter (EKF) [16] to serve the 
purpose more reliably. Unlike previous work, 
we generate a path under a probabilistic 
framework. Compared with traditional ap-
proach, our method considers the tendency of 
the human's motion. Therefore, we are able to 
derive a near-optimal path in a short period. 
 
 
Fig. 7. The system architecture of the office 
robot 
 
 
which were thought “Need” included envi-
ronment clean service, document delivery in 
different departments, and automatic reminding 
of schedule or visitors. In the appearances as-
pects, the employees were more comfortable in 
associating the office robot with futuristic, 
brighter color, and female figure. The preferred 
size of office robot was middle, which was 
about the high of a child. In the HRIs aspects, 
the results showed that the office robot should 
be friendly, understandable others, sincerity 
and protect the privacy. Additionally, office 
robot did not need to express their emotion. 
90% of the employees would adopt an office 
robot in the future. 
B. Context-Aware Navigation 
We perform some experiment to evaluate the 
performance of Context-Aware Navigation. 
We use Pioneer 3-DX to be our mobile plat-
form. As for range sensors, we use laser range 
finder LMS100, whose scanning range is 20cm 
to 20m with 180 degree of spanning angle. The 
human tracking system refers to our previous 
work [18] using hybrid laser range finder based 
human leg detection system. The map for lo-
calization module is built by FG-SLAM algo-
rithm based on Rao-Blackwellized particle fil-
ter and integration of grid map and feature map 
[17]. 
 We create a scenario to demonstrate 
various actions of the robot based on different 
contexts. At first, the robot receive a task with 
urgency index 1  . It is going to interact with 
someone else at the desk. The robot is able to 
locate the desk by our localization module with 
pre-processed map. We will discuss the action 
of the robot over time. 
 
1) t=0~13 sec: Goal Approaching (a =1) 
   At first, the robot goes forward because there 
is no obstacle in front of it (Fig. 11 (a)). After 
the person in grey coat walks out of the door, 
the robot will start to avoid colliding with her. 
In this period the robot still can detect there is a 
navigable region.  
 
2) t=13~19 sec: Pass through the Crowd 
( 1  ) 
   In Fig. 11 (b), because of the effect of Human 
Sensitive Field, the robot find that there is no 
navigable region in front of it. We apply our 
proposed lattice-based search that enables the 
robot to get to a relative safe place. We choose 
the total planning time 
pT  to be 2.5 sec and 
planning time of single lattice to be 0.5 sec. 
Since the human in grey coat is leaving from 
the robot, the occupancy probability of the node 
will be low there. Therefore, the robot will turn 
right and follow the leaving human (Fig. 11 (c)). 
Fig. 11 indicates that there is a navigable region 
at the left hand side of the robot. The action of 
the robot will change to goal approaching 
subsequently. 
 
  
(a) t=0 sec        (b) t=13 sec    (c) t=16 sec 
   
(d) t=19 sec       (e) t=27 sec    (f) t=35 sec 
   
(g) t=40 sec      (h) t=51 sec    (i) t=56 sec 
   
(j) t=70 sec      (k) t=79 sec    (l) t=85 sec 
   
(m) t=92 sec     (n) t=99 sec     (o) t=106 sec 
   
(p) t=107 sec   (q) t=110 sec     (r) t=115 sec 
Fig. 11. Overall experiment 
 
 
五、參考文獻 
[1] ASIMO,Honda, 
http://world.honda.com/ASIMO/ 
[2] Wakamaru, Mitsubishi Heavy Industries, 
http://www.mhi.co.jp/kobe/wakamaru/engl
ish/ 
[3] Emiew2, Hitachi, http://www.hitachi.com/ 
[4] QA, Anybots, http://anybots.com/ 
[5] Wei-Jen Kuo, Shih-Huan Tseng, Jia-Yuan 
Yu, and Li-Chen Fu.“A Hybrid Approach 
to RBPF Based SLAM with Grid Mapping 
Enhanced by Line Matching”, IEEE/RSJ 
International Conference on Intelligent 
Robots and Systems (IROS), 2009. 
[6] Chi-Pang Lam, Chen-Tun Chou, 
Kuo-Hung Chiang, and Li-Chen Fu. “Hu-
man-Centered Robot Naviga-
tion—Towards a Harmoniously Human–
Robot Coexisting Environment,” IEEE 
Transactions on Robotics (TRO), 2010. 
[7] R. Alami, I. Belousov, S. Fleury, and M. 
Herrb, “Diligent: towards a human-friendly 
navigation system,” in Proc. IROS 2000, 
vol. 1, pp. 21-26, 2000.  
[8] P. Althaus, H. Ishiguro, T. Kanda, T. 
Miyashita, and H. I. Christensen, “Naviga-
tion for human-robot interaction tasks,” in 
Proc. ICRA 2004, vol. 2, pp. 1894-1900, 
2004. 
[9] E. A. Sisbot, L. F. Marin-Urias, and T. 
Simeon, “A Human Aware Mobile Robot 
Motion Planner,” IEEE Trans.Robotics , 
vol. 23, no. 5, pp. 874-883, 2007. 
[10]Khan, Z. (1998). Attitudes towards 
intelligent service robots. NADA KTH 
Technical Report, Stockholm. 
[11]Numao, M., & Kuniyoshi, Y. (2008). 
50-Year Outlook of Robot Technology 
Future Vision and Technical Challenges. 
[12]J. W. Durham and F. Bullo, "Smooth 
Nearness-Diagram Navigation," in 
IEEE/RSJ International Conference on 
Intelligent Robots and Systems, pp. 
690-695, 2008. 
[13]M. Mujahad, D. Fischer, B. Mertsching, 
and H. Jaddu, "Closest Gap based (CG) 
reactive obstacle avoidance Navigation for 
highly cluttered environments," in 
IEEE/RSJ International Conference on 
Intelligent Robots and Systems, pp. 
1805-1812, 2010. 
[14]M. Pivtoraiko and A. Kelly, "Generating 
near minimal spanning control sets for 
constrained motion planning in discrete 
state spaces," in IEEE/RSJ International 
Conference on Intelligent Robots and 
Systems, pp. 3231-3237, 2005. 
[15]M. Likhachev and D. Ferguson, "Planning 
Long Dynamically Feasible Maneuvers for 
Autonomous Vehicles," The International 
Journal of Robotics Research, vol. 28, pp. 
933-945, August 1, 2009 2009. 
[16]S. Thrun, W. Burgard, and D. Fox, 
"Probabilistic Robotics," 2005. 
[17]K. Wei-Jen, T. Shih-Huan, Y. Jia-Yuan, 
and F. Li-Chen, "A hybrid approach to 
RBPF based SLAM with grid mapping 
enhanced by line matching," in IEEE/RSJ 
International Conference on Intelligent 
Robots and Systems, pp. 1523-1528, 2009. 
[18]C. T. Chou, J.-Y. Li, M.-F. Chang, and L. C. 
Fu, "Multi-robot Cooperation Based 
Human Tracking System Using Laser 
Range Finder," in IEEE International 
Conference on Robotics and Automation, 
pp. 532-537, 2011. 
[19]H. Asoh, Y. Motomura, F. Asano, I. Hara, 
S. Hayamizu, K. Itou, T. Kurita, T. Matsui, 
N. Vlassis, R. Bunschoten, and B. Krose, 
“Jijo-2: an office robot that communicates 
and learns,” IEEE Trans. Intel. Systems, vol. 
16, pp. 46-55, Sep. 2001. 
[20]H. Huttenrauch, A. Green, M. Norman, L. 
Oestreicher; and K. S. Eklundh, “Involving 
users in the design of a mobile office robot,” 
IEEE Trans. Contr. Systems Tech., vol. 34, 
pp. 113-124, Mar. 2004. 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：傅立成 計畫編號：99-2221-E-002-191- 
計畫名稱：智慧型辦公大樓之「服務型機器人族」--子計畫一：辦公室多功能智慧型秘書機器人 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
