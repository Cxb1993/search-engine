 2 
行政院國家科學委員會專題研究計畫成果報告 
多關聯表格關聯規則隱藏與 K-隱匿之研究 
A Study of Association Rule Hiding and K-Anonymization on Multi-Relational Tables 
計畫編號：NSC  98－2221－E －390－030－ 
執行期間：98 年 8 月 1 日至 99 年 7 月 31 日 
計畫主持人： 王學亮教授  執行機構：高雄大學資管系 
E-mail: slwang@nuk.edu.tw 
 
 
Abstract 
 
The goal of this project is to develop a set 
of techniques that can hide association 
rules on multi-relational tables.  Many 
approaches for preserving association rule 
privacy, such as association rule mining 
outsourcing, association rule hiding, and 
anonymity, have been proposed.  In 
particular, association rule hiding on 
single transaction table has been well 
studied. However, hiding multi-relational 
association rule in data warehouses is not 
yet investigated.  This work presents a 
novel algorithm to hide predictive 
association rules on multiple tables.  
Given a target predictive item, a technique 
is proposed to hide multi-relational 
association rules containing the target 
item without joining the multiple tables.  
Examples and analyses are given to 
demonstrate the efficiency of the 
approach. The proposed work has been 
published in two EI indexed international 
conferences. 
 
Keywords: association rule; privacy 
preserving; hiding; multi-relational; data 
mining 
 
摘要 
 
本研究計畫之主要目的在於研發多關聯
資料庫上關聯規則隱藏之最新技術。近
年來有許多學者提出有關關聯規則隱私
保護之研究，例如關聯規則外包探勘、
關聯規則隱藏、及關聯規則隱匿等。其
中在單表格之關聯規則隱藏的報告最
多。但在資料倉儲中之多關聯規則隱藏
則尚為有任何研究報告。本研究提出一
個新的在多表格上隱藏關聯規則之演算
法。針對給定之隱藏項目，我們提出一
個不需直接用表格合併(join)之隱藏多
表格關聯規則之方法。我們並對所提方
法之效率做詳盡之分析與探討。我們的
研究結果已發表於兩篇國際研討會。 
 
關鍵詞：關聯規則、隱私保護、隱藏、
多維度關聯、資料探勘 
 
Introduction 
Recent studies in preserving 
association rule privacy have proposed 
many techniques [5,8,12,16].  For 
example, in association rule mining 
outsourcing, encoding/decoding schemes 
are developed to transform data and ship 
it to a third party service provider (server).  
The data owner sends mining queries to 
the server and recovers the true patterns 
from the extracted patterns received from 
the server.  The data mining privacy is 
protected as the server does not learn any 
sensitive information from the data.  In 
privacy-preserving data mining, 
perturbation, and anonymization 
techniques have been developed to hide 
the association rules from been 
 4 
75%), AC=>B (50%, 75%), BC=>A(50%, 
100%), C=>AB(50%, 75%), B=>AC(50%, 
75%), where the percentages inside the 
parentheses are supports and confidences 
respectively. 
 
TID Items 
T1 {ABC} 
T2 {ABC} 
T3 {ABC} 
T4 {AB} 
T5 {A} 
T6 {AC} 
 
Figure 1.  A sample dataset 
The objective of data mining is to 
extract hidden or potentially unknown but 
interesting rules or patterns from 
databases.  However, the objective of 
privacy preserving data mining is to hide 
certain sensitive information so that they 
cannot be discovered through data mining 
techniques [3,11,13-15].  For association 
rule hiding, given a transaction database 
D, a minimum support, a minimum 
confidence and a set of sensitive 
association rules X, the objective is to 
minimally modify the database D such 
that no association rules in X will be 
discovered. 
Continue from previous example with 
minimum support 33%, minimum 
confidence 70%, and a sensitive 
association rule { C=>B }, if transaction 
T1 is modified from ABC to AC, then the 
rule C=>B (33%, 50%) will be hidden.  
However, rules B=>C (33%, 66%), 
AB=>C (33%, 66%), B=>AC (33%, 66%), 
AC=>B (33%, 50%), C=>AB (33%, 
50%),will be lost as side effects. 
The techniques in association rule 
mining has been extended to work on 
numerical data, categorical data, and 
others in more conventional databases.  
In a relational database, a set of relational 
tables may exist.  A star schema in a data 
warehouse is typical made up of multiple 
dimension tables and a fact table.  
Consider the following star schema [7] 
with fact table, ATMactivity(acct#, atm#, 
amount), and two dimension tables, 
Customer(acct#, balance, zipcode, age) 
and ATM(atm#, type, zipcode, limit), as 
shown in Figure 2.  If limited to single 
table, the association rule mining 
algorithms on transaction data can be 
easily extended and discover rules such as: 
age(20..29) => balance(1000..1999) from 
table Customer, and type(in) => 
limit(10000..19999) from table ATM, for 
each individual table.  However, to 
discover cross table association rules such 
as limit(0..9999) => balance(1000..1999), 
limit(0..9999) =>age(20..29), all three 
tables must be joined.  The significant 
redundancy in such a joined table would 
seriously degrade the performance of 
multi-relational association rule mining.  
To efficiently discover frequent itemsets 
and association rules across multiple 
tables, many techniques have been 
proposed [4,6,7,9,10,17].   
In this work, we consider the problem 
of efficiently hiding predictive association 
rules on multiple tables in star schema.  
Given a predictive item, a predictive 
association rule set is the minimal set of 
association rules that can make the same 
prediction as all association rules.  For 
example, given predictive item 
balance(1000..1999), with min_supp = 
0.4 and min_conf=0.6, the predictive 
association rule set contains two rules: 
{age(20..29) => balance(1000..1999), 
limit(0..9999) => balance(1000..1999)}.  
Even though there may be other rules that 
contain balance(1000..1999) on the right 
hand side. 
More specifically, given a fact table 
and a set of dimension tables in a star 
schema, a minimum support, a minimum 
confidence, and a predictive item, the 
objective is to minimally modify the 
dimension tables such that no predictive 
association rules will be discovered.  For 
example, given the three tables in Figure 
2, minimum support = 0.4, minimum 
confidence = 0.6, and predictive item 
 6 
rules, we will consider hiding association 
rules with 2 items, x=>z, where z is a 
predictive item and x is a single large one 
item.  In theory, predictive association 
rules may have more specific rules that 
contain more items, e.g., xY => z, where 
Y is a large itemset.   However, for such 
rule to exist, its confidence must be 
greater than the confidence of x=>z, i.e., 
conf(xY=>z) > conf(x=>z) or |xYz| > 
conf(x=>z) * |xY|.  For higher 
confidence rules, such as conf(x=>z) = 1, 
there will be no more specific rules.  In 
addition, once the more general rule is 
hidden, the more specific rule might be 
hidden as well. 
 
Algorithm HPAR 
Input: (1) a fact table FT, and a set of 
dimension tables DT1, 
DT2…,  
(2) minimum support 
(min_supp),  
(3) minimum confidence 
(min_conf), 
(4) a set of hidden items Y, 
Output: fact table and a transformed set 
of dimension tables where the 
predictive association rules 
containing Y on the RHS are 
hidden. 
1. Find all frequent 1-itemsets; //scan 
FT, build vector vt for each FK and 
TID lists for each item; 
2. For each frequent target item y in Y, 
2.1 Find all in-table frequent 2-itemsets 
containing target item y; //use 
TID list 
2.2 Find all cross-table frequent 
2-itemsets containing target item 
y; // scan FT and build vector ct 
2.3 Calculate the confidences of all 
predictive association rules, 
x=>y; // Conf(x=>y) >= 
min_conf 
2.4 Sort the PARs in descending order of 
confidence; 
2.5   Repeat  
2.5.1    Get the first PAR; 
2.5.2    Calculate the number of 
records, iterNum, needs to be 
modified; // for confidence to 
be smaller than min_conf or 
support to be smaller than 
min_supp; 
2.5.3    Find the TIDs containing target 
item with sum of vt counts >= 
iterNum; 
2.5.4     Delete the target item in 
the TIDs; 
2.5.5     Update the confidences of 
PARs; 
2.5.6     Sort the PARs in 
descending order; 
2.6   Until (no more PAR); 
 
The HPAR algorithm first tries to find all 
frequent 1-itemsets from one scan of fact 
table by building a vector vt to record the 
support counts for the primary key values 
of dimension tables.  It also records the 
TID list for each 1-itemset.  It then tries 
to find all frequent 2-itemsets by scanning 
the fact table to calculate the support 
counts for cross-table itemsets.  The 
in-table 2-itemsets can be calculated by 
performing intersection on TID lists in the 
same dimension table.  To hide a 
predictive association rule, the algorithm 
calculates the number of records required 
to be modified and perform deletion (or 
suppression).  It repeats the same 
process after modification and update 
until no more predictive association 
exists. 
Example 
This section shows an example to 
demonstrate the proposed algorithm in 
hiding predictive association rules from 
multiple tables of star schema. 
Given dimension tables Customer, 
ATM, fact table ATMactivity with 
min_support = 0.4, min_conf=0.6, and 
hidden item = {balance(1000..1999)}, the 
execution of the proposed algorithm is 
shown as follows. 
 8 
form acct#1 and acct#3 from dimension 
table Customer. 
Analysis  
In this section, we provide complexity 
analyses on the proposed algorithm and 
compare it with joining-then-mining 
approach. 
Instead of calculating and hiding 
predictive association rules on the joined 
table formed by fact table and all 
dimension tables, the proposed algorithm 
find the association rules in two phases 
similar to [7].  It first calculates all 
1-itemsets by one scan of fact table.  It 
then calculates all 2-itemsets, both 
in-table and cross-table, by another scan 
of fact table, without performing join 
operation and materialize the join.  For 
each predictive association rule, it 
calculates the number of records that need 
to be modified.  The process is repeated 
after modification and update until all 
rules are hidden.  We first provide the 
calculation of number of modification 
required and then estimate the operation 
on itemsets required to hide the predictive 
association rules. 
Lemma 1: For a predictive association 
rule x=>y, y  Y, algorithm HPAR 
performs min (|FT|*(supp(xy) - min_conf 
* supp(x)) , |FT|*(supp(xy) - 
min_supp)) modifications to hide the 
rule. 
Proof.  Line 2.5.2 calculates the 
number of records that needs to be 
modified so that the confidence of a given 
rule is either smaller than min_conf or 
support to be smaller than min_supp.  
Let the confidence of a predictive 
association rule be |xy|/|x| and greater 
than min_conf.  The support count of |xy| 
is decreased by one when one transaction 
containing xy is modified to support x 
only.  Assuming it takes k1 executions to 
reduce the confidence to be less than 
min_conf, i.e., (|xy| – k1)/|x| < min_conf.  
This inequality can be rewritten as k1 = 
|xy| - min_conf *|x|  = |FT|*(supp(xy) 
- min_conf * supp(x)) .  Assuming it 
takes k2 executions to reduce the support 
to be less than min_supp, i.e., (|xy| – 
k2)/|FT| < min_supp.  This inequality can 
be rewritten as k2 = |xy| - min_supp 
*|FT|  = |FT|*(supp(xy) - min_supp) .  
Therefore, it would take min(k1, k2) 
modifications to hide the rule, since either 
the confidence is less than the min_conf 
or the support is less than min_supp.□ 
Lemma 2: The time complexity of 
HPAR algorithm is O(|FT| + 
|Y|*(|FT|+(l2
2
*logl2) /2)), where |FT| is 
the number of records in fact table, |Y| is 
the number of items in Y, and l2 is the 
maximum number of frequent 2-itemsets. 
Proof.  The time complexity 
estimated here is based on the number of 
operations required on itemsets.  To find 
all frequent 1-itemsets in line one, it takes 
O(|FT|) operations.  On line two, there 
are at most |Y| number of frequent target 
items.  For each of these items, the 
following operations are executed.  To 
find frequent 2-itemsets, it is line 2.2 that 
calculating cross-table itemsets will 
require most operations.  It takes O(|FT|) 
operations to scan the fact table again.  
Assuming the maximum number of 
frequent 2-itemset is l2, step 2.4 and step 
2.5.6 perform sorting of these frequent 
2-itemsets repeatedly.  It would require 
O((l2
2
*logl2) /2) operations.  In total, the 
estimated operation is O(|FT| + 
|Y|*(|FT|+(l2
2
*logl2) /2)) . □ 
For joining-then-mining approach, a 
straight forward calculation of predictive 
association rule is to join the fact table 
with all dimension tables and then 
perform Apriori algorithm.  Let JT = FT 
 DT1  ... DTn be the joined table.  
If we apply same strategy to reduce the 
confidence of an association rule, then 
time complexity can be estimated 
similarly. 
Lemma 3: The time complexity of 
joining-then-hiding approach is O(|JT| + 
|Y|*(|JT|+(l2
2
*logl2) /2)), where |JT| is the 
number of records in fact table, |Y| is the 
 10 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應
用價值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、
是否適合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜
合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■   達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：已發表於國際研討會並持續做進一步之探討以投稿於國際期刊。  
 
 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應
用價值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能
性）（以 500 字為限） 
 
本研究為探討多關聯表格上關聯規則隱藏之先導工作。文獻上尚未見此
方面之論文發表。目前之結果為初步之隱藏方法之設計及效率分析，具
學術探討價值。可進一步探究其實際應用層面並評估其對社會網路上資
料隱私、個資隱私之可行性應用及效益。 
 
 
 
 
 
 
 2 
 
(一)參加會議經過 
 
2009年第 IEEE International Conference on Systems, Man, and Cybernetics 是由美國 IEEE 
Systems, Man and Cybernetics Society、與德州州立大學聖安東尼奧分校(University of Texas at San 
Antonio)所主辦之國際性學術研討會。另協辦單位有Stanford Research Institute及Tucker-David 
Technologies。其會議之宗旨是希望提供各國學者參與共同研討系統、人機、控制理論與應用相關之
研究，並在美國國內及世界各地輪流舉辦研討會。 
 
2009年之會期是從10月11日至10月14日。2009年的研討會地點為美國聖安東尼奧市之Hyatt 
Regency Riverwalk Hotel。除了大會邀請之4個專題演講外，此次超大型會議共有1300篇論文投稿，
並有874篇論文作口頭報告。其中344篇為特殊邀請場次論文，71篇展示論文。論文來源涵蓋65個國
家地區。每篇論文皆有兩位以上之評審審稿。另外10月11日整天並舉行15門短期課程及2個工作坊。
筆者此次發表之論文是國科會計畫之部分研究成果。本人除親自參加各場座談會外，於會議期間並
與各國學者交換意見與心得，收穫甚多。 
 
 
(二)與會心得 
 
 本會議的議程安排非常簡單。因為限於三天的時間關係並且人數眾多，因此每天有14個場次同
時進行，以及走道間之幾個展示，另外只有一個開幕式，一場晚宴，及一個閉幕式。整個研討會未
安排任何旅遊或其他社交活動。 
 
與其他國際性學術研討會相比較，個人列舉三點看法: 
(1)   該研討會發表之論文範圍廣泛，參與人數眾多，是與各國學者交流之最佳場所。國內有非常多
之學者專家參與此次盛會，的確大大提高我們在此領域之聲望。 
(2)   參與人員不但有各國之重要學者，並且漸趨年輕化、地區化。以專題演講為例，其背景皆為美
國及世界主流大學的新生代，與主辦人員相當熟悉。而一般論文發表作者中，更不乏來自中國
大陸及東歐國家及印度的學生。 
(3)   主辦單位德州州立大學聖安東尼奧分校人力並不是相當充沛。但由於大會主席華裔陳教授得到
國內相當多教授的協助與支援，使得大會辦得相當圓滿、順利。此合作模式似乎可多加應用，
以提昇我們在國際上之服務精神與形象。 
 
 
(三)攜回資料名稱及內容 
 
 4 
 
Mining High Average-Utility Itemsets 
 
Tzung-Pei Hong 
Dept. of Computer Science and 
Information Engineering 
National University of Kaohsiung 
Kaohsiung, Taiwan 
tphong@nuk.edu.tw 
 
 
Cho-Han Lee 
Institute of Electrical Engineering 
National University of Kaohsiung 
Kaohsiung, Taiwan 
prescott2005@hotmail.com 
 
 
 
Shyue-Liang Wang 
Dept. of Information Management 
National University of Kaohsiung 
Kaohsiung, Taiwan 
slwang@nuk.edu.tw 
 
 
Abstract—The average utility measure is adopted in this 
paper to reveal a better utility effect of combining several items 
than the original utility measure. A mining algorithm is then 
proposed to efficiently find the high average-utility itemsets. It 
uses the summation of the maximal utility among the items in 
each transaction including the target itemset as the upper 
bounds to overestimate the actual average utilities of the itemset 
and processes it in two phases. As expected, the mined high 
average-utility itemsets in the proposed way will be fewer than 
the high utility itemset under the same threshold. Experiments 
results also show the performance of the proposed algorithm. 
Keywords—utility mining, average utility, two-phase mining, 
downward closure 
 Introduction 
In the past, Liu et al. then presented a two-phase 
algorithm for fast discovering all high utility 
itemsets [2, 3]. In this paper, we proposed a new 
idea to evaluate the utilities of itemsets. 
Traditionally, the utility of an itemset is the 
summation of the utilities of the itemset in all the 
transactions regardless of its length. Thus, the 
utility of an itemset in a transaction will increase 
along with the increase of its length. That is, 
longer itemsets in a transaction result in higher 
utility values. Thus, using the same minimum 
threshold to judge itemsets with different lengths is 
not fair. In order to alleviate the effect of the 
length of itemsets and identify really good utility 
itemsets, the average utility measure is adopted in 
this paper to reveal a better utility effect of 
combining several items than the original utility 
measure. It is defined as the total utility of an 
itemset divided by its number of items within it. 
The average utility of an itemset is then compared 
with a threshold to decide whether it is a high 
average-utility itemset. An algorithm is also 
proposed to find all the high average-utility 
itemsets. 
Like two-phase mining for high utility itemsets, 
the proposed mining algorithm for high 
average-utility itemsets uses average-utility upper 
bounds to overestimate the actual average utilities 
of itemsets for satisfying the downward closure 
property. The average-utility upper bound of an 
itemset is designed here as the summation of the 
maximal utility among the items in each 
transaction including the itemset. Only the 
combinations of the itemsets which have their 
average-utility upper bounds beyond the 
user-defined threshold are added into the candidate 
set in a level-wise way. The downward closure 
property can thus be maintained in this way. 
Finally, the performance of the proposed mining 
algorithm is verified by real-world market data.  
Review of Related Mining Algorithms 
Agrawal and Srikant proposed the Apriori 
algorithm [1] to mine association rules from a set 
of transactions. In each pass, Apriori employs the 
downward-closure (anti-monotone) property to 
prune impossible candidates, thus improving the 
efficiency of identifying frequent itemsets. Many 
other algorithms based on the property have then 
been proposed to discover frequent itemsets 
rapidly [4-7]. 
Traditional association-rule mining does not, 
however, consider the quantities sold in 
transactions and the profit of each item sold, which 
are important to some applications as well. Yao et 
al. thus proposed the utility model to measure how 
“useful” an itemset is by considering both the 
quantities and the profits of items [8]. In utility 
mining, the downward-closure property no long 
exists since the utility of an itemset will grow 
monotonically and the frequency of an itemset will 
reduce monotonically along with the number of 
items in an itemset. The two different monotonic 
 6 
 
STEP 3: Calculate the average-utility upper 
bound ubj of each item ij as the summation 
of the maximal utilities of the transactions 
which include ij. That is: 
j k
j k
i T
ub mu

 
. 
STEP 4: Check whether the average-utility upper 
bound of an item ij is larger than or equal to 
. If ij satisfies the above condition, put it in 
the set of candidate average-utility 
1-itemsets, C1. That is: 
1 { | ,1 }j jC i ub j m    . 
STEP 5: Set r = 1, where r is used to represent the 
number of items in the current candidate 
average-utility itemsets to be processed. 
STEP 6: Generate the candidate set Cr+1 from Cr 
with all the r-subitemsets in each candidate 
in Cr+1 must be contained in Cr. 
STEP 7: Calculate the average-utility upper 
bound ubs of each candidate average-utility 
(r+1)-itemset as the summation of the 
maximal utilities of the transactions which 
include s. That is: 
k
s k
s T
ub mu


. 
STEP 8: Check whether the average-utility upper 
bound of each candidate (r+1)-itemsets s is 
larger than or equal to . If s does not 
satisfy the above condition, remove it from 
Cr+1. That is: 
1 1{ , }r s rNew C s ub s original C    . 
STEP 9: IF Cr+1 is null, do the next step; 
otherwise, set r = r + 1 and repeat STEPs 6 
to 9. 
STEP 10: For each candidate average-utility 
itemset s, calculate its actual 
average-utility value aus as follows: 
| |
k j
jk
s T i s
s
u
au
s
 


, 
where ujk is the utility value of each item 
ij in transaction Tk and |s| is the number of 
items in s. 
STEP 11: Check whether the actual average-utility 
value aus of each candidate average-utility 
itemset s is larger than or equal to . If s 
satisfies the above condition, put it in the 
set of high average-utility itemsets, H. 
That is: 
{ , }sH s au s C   , 
where C is the set of all the candidate 
average-utility itemsets. 
An Example 
In this section, an example is given to demonstrate 
the proposed mining algorithm based on the 
average-utility of items. Assume the ten 
transactions shown in Table 3 are used for mining. 
Each transaction consists of two features, 
transaction identification (TID) and items 
purchased. 
TABLE 3.  THE SET OF TEN TRANSACTION DATA FOR THIS EXAMPLE. 
TID A B C D E 
t1 1 1 4 1 0 
t2 0 1 0 3 0 
t3 2 0 0 1 0 
t4 0 0 1 0 0 
t5 1 2 0 1 3 
t6 1 1 1 1 1 
t7 0 2 3 0 1 
t8 0 0 0 1 2 
t9 7 0 1 1 0 
t10 0 1 1 1 1 
 
Also assume that the predefined profit value for 
each single item is defined in Table 4. 
TABLE 4.  THE PREDEFINED PROFIT VALUES OF THE ITEMS. 
Item Profit 
A 3 
B 10 
C 1 
D 6 
E 5 
 
Moreover, the minimum average-utility threshold 
 is set as 45.4, which is the 20% of total utility. In 
order to find the high average-utility itemsets from 
 8 
 
America was used for the experiments. There were 
21,556 transactions and 1,559 distinct items in the 
database. Each transaction consisted of the 
products sold and their quantities. The average 
transaction length is 4.03. The total profit of the 
dataset is $104,450,739. Figure 1 shows the 
number of candidate itemsets generated by our 
approach (TPAU) vs. Liu’s TP. The minimum 
utility threshold varies from 0.008% to 0.012%. 
Compared to TP, TPAU generates much fewer 
candidate itemsets. The number of candidate 
itemsets generated by TPAU decreases 
substantially. 
0
50000
100000
150000
200000
250000
0.008% 0.009% 0.010% 0.011% 0.012%
N
u
m
b
e
r 
o
f 
C
a
n
d
id
a
te
 I
te
m
se
ts
Minimum Utility Threshold
TPAU
TP
 
Figure 1.  Number of candidate itemsets with varying minimum utility 
threshold of TPAU vs. TP. 
 
Table 9 presents the summary of the number of 
candidate itemsets (CI), high average-utility 
itemsets (HAUI), and high utility itemsets (HUI) 
of TPAU vs.TP. In Phase I, TPAU generates much 
fewer candidate itemsets. In Phase II, the number 
of high average-utility itemsets (HAUI) is much 
less than that of high utility itemsets (HUI). TPAU 
can discover high average-utility itemsets whose 
utility values are much closer to the minimum 
utility threshold compared to high utility itemsets. 
 
 
TABLE 9.  COMPARISON OF THE NUMBER OF CANDIDATE ITEMSETS (CI), 
HIGH AVERAGE-UTILITY ITEMSETS (HAUI), AND HIGH UTILITY ITEMSETS 
(HUI) OF TPAU VS.TP. 
Threshold CI (Phase I) Phase II 
TPAU TP HAUI HUI 
0.012% 1583 37707 1556 3497 
0.011% 1614 53324 1557 4557 
0.010% 1677 80735 1565 6486 
0.009% 1896 125920 1579 9997 
0.008% 2288 197251 1605 18005 
 
Figure 2 shows the execution time of TPAU vs. TP. 
The execution time of TPAU is less than TP in all 
the compared minimum threshold setting. 
0
500
1000
1500
2000
2500
3000
3500
0.008% 0.009% 0.010% 0.011% 0.012%
Ex
e
cu
ti
o
n
 T
im
e
(s
e
c.
)
Minimum Utility Threshold
TPAU
TP
 
Figure 2.  Execution time with varying minimum utility threshold of 
TPAU vs. TP. 
 
CONCLUSIONS 
This paper defines a new mining measure called 
average utility and proposes a two-phase mining 
algorithm to discover high average-utility itemsets. 
The proposed mining algorithm is divided into 
phases. In phase I, this algorithm overestimates the 
utility of itemsets from the perspective of 
transactions, this process matins the “downward 
closure” property to efficiently prune impossible 
utility itemsets level by level. In phase II, 
according to the candidate itemsets generated from 
phase I, one database scan is needed to determine 
the actual high average-utility itemsets. 
Considering that the length of itemsets is a major 
factor to influence the utility values of itemsets. 
The measure “average-utility” is used to evaluate 
the utility values. The experimental results show 
that our algorithms can obtain fewer itemsets with 
purer and accurate utility. 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
