same scheduling programs. As a result, we can collect and estimate the execution time for each task and the
size of data flowing between tasks, and use such result in subsequence simulation. Using the framework, we
are able to implement several scheduling algorithms and apply them on several problems. To investigate the
feasibility of our approach, we modify Grid-NPB benchmarks and express them as workflows, and perform
simulation with different configurations of the underlying machine model using different scheduling
algorithms we implement.
2 Background and Motivation
During the past few years there have been many grid computing infrastructure developed. Examples
include NSF TeraGrid and UK e-Science Grid. The vision of virtual organization (VO), as pioneered by Ian
Foster, concerns about harnessing heterogeneous, independently managed computing resources distributed
across the Internet and combine them into a logical cohesive to solve computation or data intensive
problems.
A grid system is often described as a layered architecture [Anatomy], including Fabric, Connectivity,
Resource, Collective, and Application layers. What concerned us are the Resource and Collective layers
that provide convenient middleware and services to simplify grid application development. Example
facilities include directory services, resource co-allocation, scheduling, data replication, and so on. In
particular, we are interested in the resource management aspect of grid computing. Many systems already
exist today that provide sophisticated resource management and scheduling capabilities. For example, the
Nimrod/G system [Nimrod/G] includes a scheduling framework based on the computational economy
concept, where users can express the deadline and cost for a given work while the system attempts to
complete the work under the stated constraint. Another popular system is the Condor system [Condor] that
designates the role of matchmakers that are responsible of finding acceptable matches based on the
advertisements from both requestors and resources.
In contrast to these existing, conventional approaches, we are more interested in using simulation as
a means to help deciding among candidate scheduling policies. So far there is no grid system that attempts
to use simulation as a means to help scheduling or more general resource management tasks. This is quite
reasonable because the high execution cost associated with simulation. However, while this concern may be
true in more traditional parallel and distributed computing platforms, it is less so for grid environment due
to many factors: First, the granularity of tasks subject to scheduling is large in a grid environment to justify
the cost of distributing them across the network and manage their execution and data transfer. The related
overhead for simulation decreases when the granularity increases. Secondly, when grid systems become
widely used and resources become commodities, and when cost of resource usage becomes high, it becomes
crucial to make sure that better, if not best, resource management and scheduling policies are used because
differences in the resulting execution performance matters. Simulation can provide more adequate
information about the qualities of the scheduling policies. Thirdly but not lastly, Grid applications are
specific managerial tasks such as setting up data or control flows properly.
Our task scheduling framework is implemented as a service managed by a service container. There
are four core modules that comprise of the scheduling framework, namely, workflow store, scheduler
manager, process manager, and information module. Two special types of services are important,
namely, task type registry and task processor. The former maintains a list of defined task types. The latter,
like a procedure declaration, defines a unit of work to be done, where the work involves processing the
input parameters and produce data as output parameters. New task types can be created and registered.
Accordingly, the job of a task processor is to process tasks, where each task is associated with a task type.
However, different task processors may have different capabilities by the types of tasks they can process.
As the name suggests, a workflow store maintains workflows. A workflow is a parallel program that
can be instantiated and executed; it consists of multiple tasks that are interrelated via both control flows and
data flows. In our system, workflows are expressed as XML documents. The process manager maintains
running workflow instances, or just processes. A process makes progress through schedulers that are
maintained by the scheduler manager. In particular, when a process is created, a scheduler is associated with
it such that the scheduler decides which task processor a task is assigned to whenever the task is ready for
processing. In other words, a process maintains the execution status of a workflow, including the tasks
under processing and the data flow through them.
A scheduler assigns tasks to task processors based on information about task processors as well as
other information maintained in the information module, such as historical information about the data size
and execution time of a particular task inside a workflow, if any. Different schedulers can be defined each
employing different strategy or scheduling algorithm. However, they all see the same information, that is,
the process and the associated workflow (its internal structure), the task types, and those from the
information module.
Workflows can run under actual and simulation modes. Only the process manager and the
information manager are different. Therefore, the same schedulers as well as workflows are used. In the
simulation mode, different information module can be provided each providing different kind of
system/network model (number of processors and the bandwidth of communication links) with different
degree of sophistication. Our workflow language is designed to be simple yet flexible. The language permits
ordinary programming capabilities. Briefly speaking, a workflow consists of the declarations of input and
output parameters, followed by a sequence of statements. To support parallel execution, common constructs
such as nested parallel statements or more flexible fork and join statements are supported.
For illustration purpose, Figure 2 shows an example benchmark from Grid-NPB expressed as a
workflow using our language. Note that the example belongs to a special but common category, namely task
graphs, which consist of only tasks that have acyclic dependencies. For this kind of workflows we have
simplified syntax that also makes the implementation of static scheduling algorithms easier.
standard machine by setting its speed to 1 and change the other machine speeds related to it. To model
system load, we use queuing model with Poisson distribution to simulate machines’load (the tasks current
executing in machine)
Parallel programs. Programs used in the experiments are adapted from Grid-NPB benchmarks
[Frumkin01-02] for our framework, where one of them has been introduced in Figure 2. Figure 3 shows
other benchmarks we have adapted:
4.2 Performance Study
In the first set of experiments, we test four benchmarks on different numbers of machines using three
algorithms. All processors and network links are uniform (i.e. processing and data transfer speed). Figure
4(a) shows that the average execution time of ED.S scheduled using three algorithms decreases as processor
number increases, due to the fact that ED.S simply includes a set of independent tasks. Figure 4(b) shows
for the case of HC.S that the average execution time improves only when scheduled using dynamic
scheduling algorithms. This also makes sense since HC represents a long chain of repeating processes and
the dynamic scheduling algorithms can dispatch a ready task to the least loaded machine. Figure 4(c) shows
that the average execution time of MB.S improved as processors increased but stop after three machines. If
we look at the structure of MB (Figure 3), we see that the maximal parallel execute-able tasks are three and
this explains the experiment result. Finally, Figure 4(d) shows that the average execution time of VP.S
improves when there are multiple machines, and HCPT scheduling algorithms is better than other
scheduling algorithms. However, the efficiency does not improve further when given more machines.
Overall, all three algorithms are not that good for scheduling VP.S under the condition of uniform machine
and network load.
Figure 3: Other adapted benchmarks from Grid-NPB
sufficiently accurately under slow network conditions. In this case, we should either just let one machine
handle the whole work or consider other scheduling algorithms.
To summarize, the experiment results suggest that under different machine and network conditions,
especially when their load change over time, the effectiveness of different scheduling algorithms also vary.
Note that, however, the Grid-NPB benchmarks are task graph based and their structural characteristics can
be exploited by various (static) scheduling algorithms. In contrast, the simulation approach is more general
and can be applied in more situations.
5 Conclusion
We have proposed and developed an extensible scheduling framework where parallel programs
represented as workflows or simpler task-graphs can be executed in both real-time and simulation modes.
With program syntax defined and execution status and performance characteristics accessible via standard
0
2000
4000
6000
8000
10000
12000
ED.S HC.S MB.S VP.S
problem type
tim
e
(m
s) dynamic
simple
hcpt
average execute time
0
2000
4000
6000
8000
10000
100 500 1000 2000
mean
time
( milliseconds )
dynamic
simple
hcpt
average execute time
0
2000
4000
6000
8000
10000
1 2 3 4 5
processor
time
( milliseconds )
dynamic
simple
hcpt
0
2000
4000
6000
8000
10000
12000
HC.S VP.S
problem type
tim
e
(m
s) dynamic
simple
hcpt
average execute time
0
20000
40000
60000
80000
1 2 3 4 5
processor
time
( milliseconds )
dynamic
simple
hcpt
Figure 5: Varying machine and network conditions
(a) (b)
(c) (d)
(e)
[Buyya02b] Rajkumar Buyya and Manzur Murshed2, GridSim: a toolkit for the modeling and simulation of
distributed resource management and scheduling for Grid computing, Concurrency Computat.: Pract.
Exper. 2002; 14:1175–1220.
[Frumkin01] Michael Frumkin, Rob F. Van der Wijngaart, NAS Grid Benchmarks: A Tool for Grid Space
Exploration, 10th IEEE International Symposium on High Performance Distributed Computing (HPDC-
10'01) p.0315
[Frumkin02] Michael Frumkin, Matthew Schultz, Haoqiang Jin, and Jerry Yan, Implementation of the NAS
parallel benchmarks in Java, NAS-02-009, NASA Ames Research Center.
[Frumkin03] Michael Frumkin, Robert Hood, Using Grid Benchmarks for Dynamic Scheduling of Grid
Applications, NAS Technical Report NAS-03-015
[Gallop] J. Weissman. Gallop: The benefits of wide-area computing for parallel processing. Technical
report, University of Texas at San Antonio, 1997.
[Hagras] Tarek Hagras, Jan Janecek , Proceedings of the Second International Symposium on Parallel and
Distributed Computing (ISPDC’03)
[Liu] Xin Liu and Andrew A. Chien, Traffic-based Load Balance for Scalable Network Emulation,
Conference on High Performance Networking and Computing: Proceedings of the 2003 ACM/IEEE
conference on Supercomputing, page 40
[Nakai] Junko Nakai, Rob F. Van Der Wijngaart, Applicability of Markets to Global Scheduling in Grids,
NAS Technical Report NAS-03-004
[Phatanapherom] Sugree Phatanapherom , Putchong Uthayopas, and Voratas Kachitvichyanukul, FAST
SIMULATION MODEL FOR GRID SCHEDULING USING HYPERSIM, Proceedings of the 2003
Winter Simulation Conference
[Sulistio05] Anthony Sulistio , Gokul Poduvaly, Rajkumar Buyya , and Chen-Khong Thamy, Constructing
A Grid Simulation with Differentiated Network Service Using GridSim, Proceedings of the 6th
International Conference on Internet Computing (ICOMP'05), June 27-30, 2005, Las Vegas, USA.
[Sulistio03] Anthony Sulistio, Chee Shin Yeo, and Rajkumar Buyya, Visual Modeler for Grid Modeling and
Simulation (GridSim) Toolkit, P.M.A. Sloot et al. (Eds.): ICCS 2003, LNCS 2659, pp. 1123–1132, 2003.
[Sherwani] Jahanzeb Sherwani, Nosheen Ali, Nausheen Lotia, Zahra Hayat, and Rajkumar Buyya, Libra: a
computational economy-based job scheduling system for clusters. Softw., Pract. Exper. 34(6): 573-590
(2004)
[Wijngaart01] Rob F. Van der Wijngaart, Michael Frumkin, Benchmarking Grid Environments at the User
Level, GGF3, Oct 7-10, 2001
[Wijngaart02a] R. F. Van der Wijngaart and M. A. Frumkin, NAS Grid Benchmarks Version 1.0, NAS
Technical Report NAS-02-005
[Wijngaart02b] Rob Van Der Wijngaart, NAS Parallel Benchmarks Version 2.4, NAS Technical Report
NAS-02-007
[Wijngaart04] Rob F. Van der Wijngaart, Evaluating the Information Power Grid using the NAS Grid
Benchmarks, 18th International Parallel and Distributed Processing Symposium (IPDPS'04) - Workshop
17 p. 275b
[Wong] Parkson Wong, Rob F. Van der Wijngaart, NAS Parallel Benchmarks I/O Version 2.4, NAS
Technical Report NAS-03-002
Related Master Theses
[YCC] Yi-Chiu Chan,“A Task Scheduling Framework for Grid Computing”, Master Thesis, Department of
Computer Science, National Chiao Tung University, Jan. 2006
[HCJ] Chiou-Rung Hung, “Resource-Oriented Computing: Towards a Universal Virtual Workspace”,
Master Thesis, Department of Computer Science, National Chiao Tung University, Aug. 2006.
