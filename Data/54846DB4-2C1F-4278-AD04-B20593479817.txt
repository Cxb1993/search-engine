  2 
semiconductor yield enhancement case [5]. 
The regression tree is built by iteratively splitting data set 
and selecting factors into a hierarchical tree model. The 
fundamental assumption is that each tree node with a data 
subset in the hierarchy has its own governing model and the 
entire tree consists of governing models in different 
hierarchical levels. The term “regression tree” is referred to 
decision tree dealing with numerical response while 
“classification tree” is referred to decision tree dealing with 
categorical response. In the literature, the AID algorithm [6] 
first implements the idea of top-down induction decision 
tree. In each step, AID searches over all possible binary 
partitions and select one that maximizing variance reduction. 
Splitting stops if variance reduction doesn’t reach a pre-
specified level or the sample size is too small. However, it 
is hard to specify the stopping threshold for variance 
reduction. An inappropriate value will result in over- or 
under-fitting. The famous CART [7], first derived in 1984, 
avoids the difficulty of choose the pre-specified value by 
employing a backward-pruning strategy to determine the 
final tree model. Cart is able to induce rules from data no 
matter the response is numerical or categorical. The 
algorithms used in application [1]-[5] are CART or its 
extensions. C4.5 [8], which embodies CART, deals with the 
categorical response variable in the 1990s. These systems 
produce the predictive rules take the form of if-then 
predicates. Each branch of a split node represents a 
conditional clause composed by the selected parameter at 
the node. For each terminal node, called leaf, an if-
statement is derived by jointing the conditions from a path 
along the root node to the leaf. If a sample fits in with an if-
statement in a regression tree case, then its response is 
predicted by the median of samples at the corresponding 
leaf. Since the predicted value is assigned by a constant, 
these systems are also known as piecewise constant 
regression trees.  
Latter, piecewise linear regression tree, called model tree, 
are proposed. It predicts response more accurately by linear 
combination of feature variables referenced to the leaf. M5 
[9] first implements the concept of model tree by integrating 
the parametric regression model [10] into regression tree. It 
inducts a tree structure with pure leaves in the first phase 
and then associates a multiple regression model with each 
leaf in the second phase.  With the advantage of advanced 
computer power, some more sophisticated model trees are 
proposed such as RETIS [11], SUPPORT [12], GUIDE [13], 
SECRET [14], MAVUE [15], TSIR [16] and SMOTI [17]. 
More complex models are considered and built by these 
methods. 
However, these trees are basically top-town tree splitting, 
which depletes the sample size quickly after few levels of 
splitting and factor selections. With sample size depletion, 
factor selection in the lower hierarchical levels becomes 
extremely unreliable. Especially when the aim is getting 
insight to the underlying process, such a sample size 
depletion problem will carry one into idiosyncratic results. 
All of them suffers sample size depletion problem when 
being used as analysis tools for yield ramp-up and foundry 
fabrication where the number of available lots for analysis 
is usually small. The shortcoming makes the conventional 
decision tree an inappropriate tool when dealing with tool 
combination effect in small sample size problem. For 
example, suppose the yield loss is caused by a combination 
effect of four semiconductor tools (represented by four 
binary variables x1,…,x4) with only 20 lots of wafers 
available for analysis. The conventional top-down 
regression tree shown in Fig. 1 depletes the sample size 
quickly and fails to select x4 into the model. 
 
In the literature, GUIDE [13] has noticed that greedy 
recursive selection is ineffective to discovery interaction 
effect and proposed a remedy. However, all the above 
regression trees can’t effectively select the combination 
effect of four tools once using the entire 20 lots of wafers 
and construct a tree structure like Fig.2.  
 
Another problem of these above methods is unnecessary 
split. It intensifies the sample size depletion. Most of them 
always split data after feature selection except TSIR and 
SMOTI which are able to explain residual variance by 
regression step without data split. The unnecessary split 
problem is only concerned by SMOTI under a special 
condition. We use an example to explain why some splits 
are unnecessary and need to be avoided. As shown in Fig. 
3a, suppose a binary x1 is chosen as root splitting attribute 
and the data set is split into two sets: one with x1 =1 and the 
other with x1 =0. Suppose further a continuous x2 is chosen 
by both split data sets.  
 
 
 
Fig. 2. Selecting tool combination effect once with the entire 
sample 
 
 
 
Fig. 1. Failure to select a complete combination effect of 
tools (x1, x2, x3, and x4) due to sample size depleting. 
  
  4 
means that a lot entering step-machine 127 and 197 has a 
yield 22.80 lower than the others. The p-value and effect of 
X247(≥ 1) is 0.00039 and 10.06, respectively. Hence, X247 also 
has a significant impact on the yield.  For a lot entering 
step-machine 247 but not entering both step-machine 127 
and 197, the yield is 10.06 higher than others. The resulting 
R2 is about 0.7136. 
The second case is to identify significant effect of combined 
metrology items on the final yield. The dataset contains 247 
metrology items, which is normalized, and 47 wafer lots. 
The response is the sort yield and the continuous variable 
IMi denotes ith metrology items. Again, using SERT we 
obtain a tree in Fig. 6. 
IM172IM222
IM172(≥-0.0111)
IM206(≥-0.0029)
IM244 (≥0.181...) IM128 (≥-0.048…)
 
Fig. 6. Case study for metrology combination effect  
We select a continuous combined factor IM172 IM222 in the 
root without splitting. In the second level, an encoded 
IM172(≥ -0.0111) is selected and then dataset is split into lots 
with IM172(≥-0.0111)=1 and lots with IM172(≥-0.0111)=0, i.e. 
[IM172(≤-0.0122)]=1. In the third level, IM206(≥-0.0029) is selected, 
lots with [IM172(≤-0.0122)]=1 is split into lots with IM206(≥-0.0029) 
[IM172(≤-0.0122)]=1 and lots with [IM206(≤-0.0031)][IM172(≤-
0.0122)]=1. We can then write the corresponding empirical 
regression model in Equation (2) 
εβ
β
β
βββ
++
+
+
++=
−≤−≤−≥
−≤−≥−≥
−≤−≥
−≥
]][[      
][      
][      
)0122.0(172)0031.0(206)...181.0(2445
)0122.0(172)0029.0(206)...048.0(1284
)0122.0(172)0029.0(2063
)0111.0(172222217210
IMIMIM
IMIMIM
IMIM
IMIMIMy
  (2) 
Metrology items 122 and 222 combine to have a significant 
effect with p-value 4.22e-5. The significant tests and 
ANOVA results are shown in Table 2 and the R2 is near 
0.9016. 
3. SERT SOFTWARE SYSTEM 
Fig. 7 shows the graphical user interface (GUI) of the SERT 
software system. As shown in the GUI, the upper-left 
window shows the tree structure with the node information 
in the upper-right window while the ANOVA is shown in 
the bottom window. 
REFERENCES 
[1] D. Braha and A. Shmilovici, “Data mining for 
improving a cleaning process in the semiconductor 
industry”, IEEE Transaction on Semiconductor 
Manufacturing, Vol. 15, No. 1, 2002,  pp. 91-101. 
[2] C.-F. Chien, W.-C. Wang, and J.-C. Cheng, “Data 
mining for yield enhancement in semiconductor 
manufacturing and an empirical study,” Expert System 
with Application, Vol. 33 No. 1, Jul. 2007, pp.192-198. 
[3] F. Mieno, T. Sato, Y. Shibuya, K. Odagiri, H. Tsuda, 
and R. Take, “Yield improvement using data mining 
system,” in Conference Proceeding IEEE International 
Symposium on semiconductor Manufacturing, pp. 391-
394. 
[4] A. Kusiak, “Decomposition in data mining: An 
industrial case study,” IEEE Transaction on Electronic 
Package Manufacturing, Vol. 23, 2000, pp.345-383. 
[5] D. Braha and A. Shmilovici, “On the use of Decision 
Tree Induction for Discovery of Interactions in a 
Photolithographic Process,” IEEE Transaction on 
Semiconductor Manufacturing, Vol. 16, No. 4, 2003, 
pp. 644-652. 
[6] A. Fielding, “Binary segmentation: The automatic 
detector and related techniques for exploring data 
structure,” In C. A. O'Muircheartaigh and C. Payne, 
editors, The Analysis of Survey Data, Volume I, 
Exploring Data Structure, Wiley, New York, 1977, pp. 
221-257. 
[7] L. Breiman , J. H. Friedman, R. A. Olshen, and C. J. 
Stone, Classification and regression trees. Monterey, 
CA: Wadsworth. 1984. 
[8] J. R. Quinlan, C4.5: Programs for machine learning. 
San Francisco: Morgan Kaufmann, 1993. 
[9] J. R. Quinlan, “Learning with continuous classes”, in 
Proceeding Fifth Australian Joint Conference on 
Artificial Intelligence, 1992, pp. 343-348. 
[10] B. L. Bowerman, and R. T. O’Connell, Linear 
Statistical Models: An Applied Approach 2nd ed., 
Boston: PWS-KENT, 1990. 
[11] A. Karalic, “Linear regression in regression tree leaves", 
in Proceeding of International School for Synthesis of 
Expert Knowledge, Bled, Slovenia, 1992, pp.151-163. 
[12] P. Chaudhuri, M. Huang, W. Loh, and R. Yao, 
“Piecewise-Polynomial Regression Trees,” Statistica 
Sinica, vol. 4, 1994, pp. 143-167. 
[13] W. Loh, “Regression Trees with Unbiased Variable 
Selection and Interaction Detection,” Statistica Sinica, 
vol. 12, 2002, pp. 361-386. 
[14] A. Dobra and J.E. Gehrke, “Secret: A Scalable Linear 
Regression Tree Algorithm,” in Proceeding Eighth 
ACM SIGKDD International Conference Knowledge 
Discovery and Data Mining, 2002. 
[15] C. Vens and H. Blockeel, "A simple regression based 
heuristic for learning model trees", Intelligent Data 
Analysis, Vol.10, No.3, 2006, pp.215-236 
[16] D. Lubinsky, "Tree Structured Interpretable 
Regression", In D. Fisher & H. Lenz, Editors, Learning 
from Data, Lecture Notes in Statistics, Vol. 112, 
Springer, pp. 387-398, 1994. 
[17] D. Malerba, F. Esposito and M. Ceci, A. Appice, “Top-
Down Induction of Model Trees with Regression and 
Splitting Nodes,” IEEE Transactions on Pattern 
