Abstract 
The open source software has become popular in recent years. In order to better evaluate and 
control the quality of open source software, many design metrics were proposed to indicate the 
design properties that influence the software quality during design phase. The major focus of our 
research is to empirically investigate the relationship between class defect count and an existing 
design complexity metric, Interaction Level (IL). Six open source software are empirically 
studied to perform such an investigation. The result shows this metric can be a useful reliability 
indicator at the class level. 
Keywords: Open source software, object-oriented design metrics, metric validation. 
List of Figures
3.1 Class quadrilateral’s interactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
6.1 IL and CBO metrics visualization result. Classes are sorted according to defect count . . 37
6.2 IL and CBO metrics visualization result. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
6.3 NP and NA metrics visualization result. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
ii
Chapter 1
Introduction
Software reliability is more emphasized on software market. Thus, software developer
would like to build a reliable software product within limited time and budget. Software
failures may occur due to many causes. In particular, during software design, the design
decisions influence the software reliability. Therefore, in order to produce high quality
software, a strong emphasis on design aspects, especially during the early design phase,
is necessary since early correction actions are less costly. One way to accomplish this
goal is software metrics. Software design metrics are defined to measure or model some
particular design perspectives and provide developers a quantified approach to evaluate
the design. Thus, software metrics can be used to set up a model to identify the part at
high risk of software system and improve the reliability.
Many traditional software product metrics, such as McCabe’s cyclomatic complexity
[1] or line of code [2] are proposed. It has been shown that these metrics are associated
with defects and maintenance performance [3] [4]. However, as object-oriented technology
is popular, traditional software metrics can not model the key concepts in object-oriented
1
help developers to be easily aware of the relationship between software metrics and defect
count at the class level.
The organization of the rest of this report is as following: In the next chapter, some
related works are mentioned. In Chapter 3, we introduce IL metric. In Chapter 4, we
present the model and experiment hypotheses. Chapter 5 describes the design of the
experiments, data collection process and the experiment result. Chapter 6 presents the
visualization tool, and Chapter 7 gives the conclusion and future work.
3
• Depth of the Inheritance Tree (DIT): It is the length of the longest path from a
given class to the root class in the inheritance hierarchy.
• Number of Children (NOC): This is a count of the number of immediate child classes
that have inherited from a given class.
• Response for a Class (RFC): This is the count of the methods that can be potentially
invoked in response to a message received by an object of a particular class.
• Lack of Cohesion of Methods (LCOM): A count of the number of method-pairs
whose similarity is zero minus the count of method pairs whose similarity is not
zero. Similarity refers to the sharing of class attributes by methods.
In CK metrics suites, CBO, and RFC model coupling relationship, LCOM models the
cohesion degree of a class, DIT, and NOC model the inheritance relationship, and WMC
models complexity of a class. Each of these metrics models particular design perspective
affecting software quality and they are complementary. For example, aggregating many
responsibilities to single object to lower the coupling degree makes the complexity of this
object higher. Distributing responsibilities to different objects makes the coupling degree
higher. Hence, these metrics should be chosen and used to reflect different particular
design perspectives. However, not all the metrics can be measured during the design
phase such as RFC and LCOM. During the design phase when object interface and
attributes are defined, structural relationship (coupling and inheritance relationships) can
be collected. At this time, and the weight of complexity metric, WMC which models the
similar design perspective with IL, is usually given to one [5][6]. Some traditional static
complexity metric, such as cyclomatic complexity [1], can be used as the weight value
5
with fault-proneness of classes. In [13], Briand et al. performed an industrial case study
and CBO was found to be associated with fault-proneness of classes. In [6], Subra-
manyam et al. collected industry data from a software system developed in two popular
object-oriented languages, C++ and Java. The authors validated the association be-
tween defects and a subset of CK metrics suite and compared the results across different
languages. The result showed that in C++ language, higher CBO value associates with
higher defects count. But in Java language, this result did not support the hypothesis
about the relationship between CBO and defect count. With regard to IL, in [7], the
authors defined several design alternatives for experts to choose their preference and
modelled the complexity of these design alternatives by IL. The result shows IL can re-
flect experts’ design preference. The other empirical literature is [8]. In [8], the authors
designed a controlled experiment to empirically explore the relationship between class
interface information which modelled by IL, IS (Interface Size) and OAC (Object Argu-
ment Complexity) and maintenance time. Each of the three metrics by itself was found
to be useful in the experiment in predicting the maintenance performance. Nevertheless,
our work focuses on investigating the relationship between class reliability and IL. In
addition, CBO, one of CK metrics, can be complementary to IL. We also investigate this
relationship.
On the other hand, “Software visualization is the use of the crafts of typography,
graphic design, animation and cinematography with modern human-computer interac-
tion and computer graphics technology to facilitate both the human understanding and
effective use of computer software” [9]. In [14], the authors proposed a novel visualization
of classes called class blueprint that is based on a semantically enriched visualization of
7
Chapter 3
Interaction Level Metric
3.1 Conceptual Model of Interaction Level
The design complexity metric, IL, is based on the dependency relationship between
data items within methods of a class, since class provide opportunities for data items
(instance variables, parameters) to interact and influence each others’ value or state
in complex ways within their methods. For example, there are two data items A and
B. If the state of value of data item B is (directly) influenced by data item A, this is
said there is an interaction from A to B. Interaction level metric models the degree of
interactions. The more data dependency relationships (interactions) exist in a class, the
more complexity a class has.
The assumption of interaction level is that the larger interface size of an object, the
more opportunities for data items to interact with each other within a method, and
interaction is the cause of complexity. High interaction level implies complex data de-
9
queried. If the interaction participant objects are both inspectable and modifiable,
there are two interactions counted.
IL for a method is defined as following:
interaction level= k1 * (value based on number of interactions) + k2 * (value
based on strength of interactions)
Because the complexity of each interaction should not be all the same, strength is the
measurement to distinguish the complexity of interaction constituted from different data
types. The strength of interaction is defined as the product of size of the data items
involved in an interaction. The size values of different data types are based on [8] and we
make some modifications. Size values are listed in Table 3.1. It is necessary to use both
number and strength because they typically have an inverse relationship that decrease
in either number or strength could increase the other and vice versa. Here, the constant
k1 and k2 means the different importance of number and strength of interaction. We set
these constants to one for simplicity and balance the influence of number and strength
of interaction.
IL value can be aggregated to different level of granularity. IL for a class is the
summation of all methods’ IL value in the class . In our work, constructors and utility
functions are excluded for computing IL metric value.
11
X        Y
Return 
Value
x1  x2  x3  x4  y1  y2  y3  y4
Figure 3.1: Class quadrilateral’s interactions
First, we compute the IL value of first method. Assume the parameters of the first
method are inspectable only, return type and the attribute are modifiable and inspectable.
The permitted interaction of the first method can be shown in Fig 3.1. And,
• The size of each float parameter = 3.
• The size of return type = 1.
• The strength of interaction (involving parameter x or parameter y) = size of a
attribute * size of parameter = 3*3 = 9.
• The strength of interaction which involves return type = 1*3 = 3.
• There are two interactions between return type and each attribute.
IL value for method “hasVertex” = k1*(number of interaction) + k2*(strength of inter-
action), where k1 = k2 = 1. Thus, IL = 1*(8+8+16)+1*(9*8+9*8+16*3) = 224.
About the other method ”area”, there are total 16 interactions between instance
variables and return type, and strength of each interaction is 9. Therefore, the IL value
for ”area” method is 9*16=144.
Compare these two methods. The interactions within these methods should be differ-
ent. Different interactions level may result in different detailed design and implementation
difficulty. Thus, the effect on class defect count of these two methods should be different,
13
Chapter 4
Model and Hypotheses
IL metric has been subjectively validated by comparing IL values and design experts’
preference [7] and empirically validated the relationship between IL and maintainability
[8] by simple controlled experiments. As the previous chapters describe, we believe
that IL is a more appropriate design complexity metric and to be an indicator of class
defect count. However, it lacks the empirical validation of the relationship between IL
complexity metric and class defect count in object-oriented software system. Therefore,
our main objective was to focus on exploring the relationship between the object-oriented
design complexity metric and the number of defects at the class level. We expected to
find a positive relationship between IL metric and the number of defects of each class.
After such relationship has been validated, IL can be used as the defects indicator at the
class level. The design of the software system can be evaluated and developer can figure
out the classes at high risk. i.e. More defects may occur within the class. Designer can
try to redesign the high risk part or other approaches in order to rise the reliability. In
order to validate the relationship, the following hypothesis should be statistically tested.
15
object reference data item will be more complex than using an primitive data type in
an interaction. And that may cause different degree of influence on class reliability.
Therefore, the size of object should be larger than primitive data type. Besides, the
size of array type is set to “+2”. It means , for example, the size of an integer array
is the size of integer plus two. The reason to make such setting is when using an
array type, developers need to pay attention to the size and index of this array. The
strength of interaction means the complexity to correctly arrange the interaction to
achieve the intended effect. Thus, large strength may result in more defects.
• Both.
Therefore, it is suggested that higher interaction level correlates with increased difficulty
in determining how to develop or implement a design. That means a design with higher
interaction level will result in the detailed design and the implementation of this class to
be more difficult. There will be more defects in this class.
On the other hand, the defects count of an object may be influenced by the role of
this object played in a software system. Different role an object plays, different level
of design complexity an object will have and associate with different number of defects.
For example, in model-view-controller architecture [16], the responsibility of objects in
view component is to render the user interface. However, the responsibility of objects
in model component may be complex computations. Here, the role of an object is not
only the responsibility of itself but also the relationship with other objects in the system.
However, using IL to model the role of an object is limited, because IL metric only models
the complexity for interaction between its surface and its interior of an object. The focus
of IL is only the internal complexity of class’ methods. Therefore, we try to find other
17
objects classes with similar design complexity within themselves but one needs to collab-
orate with more objects will have more defects. So, CBO is complementary to IL since
they model different design perspectives. Then we have the second model.
Defects = β0 + β1× IL+ β2× CBO (4.2)
In this model, β1 and β2 represent the degree of influences of the metrics on class defect
count. The constant β0 represents the y-intercept of this regression line, but it does
not show enough information to interpret. We can model a class’ complexity along two
dimensions. CBO models the relationship of a class with other classes in the software
system, and it reflects the difficulty of using other classes when developers want to fulfill
the responsibility of the class. On the other way, IL mainly models the interaction
complexity of the data items within methods for achieving the intended responsibility of
a class. There may be association between IL and CBO. For instance, large CBO may
imply the data items that constitute an interaction contain many object types. Then
large IL metric value will be obtained. However, basically they model different dimension
design concept.
By combing CBO, we can model the design complexity of a class more precisely than
using IL only. It is expected that the explanation ability of the number of defects of a
class by using IL and CBO will be higher than using IL only. Therefore, we have the
second hypothesis.
19
Chapter 5
Experiment Analysis
There are several empirical ways to test the hypotheses in last chapter. Firstly, small
scale controlled experiments like homework assignment at school can be set up, but it
may be meaningless to what we want to validate. Although in the small scale controlled
experiment we may better control the factors which have impact on software quality, such
as design complexity, student skill, development tools etc, the defect count of classes in
simple project may have just little difference. Thus, it may be hard to validate the
relationship between design complexity and class defect count. Secondly, the experiment
setting in software industry should better reflect such relationship, but we do not have
any available industry software data such as design document, the results of testing etc,
to set up experiments. So, our approach is taking open source projects as experimental
subjects. We focus on Apache [17] open source projects since the projects in Apache
Software Foundation have more detailed documentation and every java class in a project
has its own change log. Information about each class of an open source project can be
collected from its change log. However, adapting open source projects as experimental
21
the interface and instance variables information of a Java class. Although CBO definition
usually includes not only the classes in the interface and instance variables but also the
variables declared locally within the method, CBO value we measure may be somewhat
different from this definition. What we want to model is the coupling relationship which
can be identified during the earlier design phase and the measurement time should be
the same as interaction level applies. And large portion of coupling relationship can be
identified by class interface and instance variables. The small portion of inaccuracy only
causes a little influence.
5.2 Experimental Subjects
We focus on the projects implemented with Java language and follow several principals to
take projects as our experimental subjects. Firstly, the relationship we want to validate
is design complexity and its influences on class defect count. We identify a more clear
time interval during which major bug fix activities were performed for a particular release
in the project history. The project document and change history help to perform the
judgement. For example, in Table 5.1, the first project is Jakarta-ORO. In this project,
the interval we identify is from 2.0.0 release to 2.0.8 release. During this interval, there
is other release, such as 2.0.2, 2.0.3 and so on. The changes documented between these
releases contain many bug fixing activities. Since the releases following 2.0.0 contain
many bug fixing activities and have the same major release number 2.0, we suggest these
activities are performed for release 2.0. The second is the length of the time interval
we identified. If the length of time is too short, few defects will be collected and it is
hard to show any significant influences on defect count result from design complexity.
23
Table 5.2: The Class Count of Start and End release
Project Name # classes of start release # classes of end release
Jakarta-ORO 61 61
Jakarta-BCEL 325 336
Jakarta-POI 308 311
Jakarta-Struts 176 185
Cocoon 495 515
Jakarta-Tapestry 405 424
Commons-Collections 248 267
5.3 Analysis Method
In the model of the relationship between design metrics and class defect count, we want
to know whether each metric has significant influence on class defect count. In addition,
we also need to know the explanatory ability of the model on class defect count. We
judge whether a metric has significant influence on class defect count by p-value. The
explanatory ability of the model is evaluated by adjusted R-square value.
5.3.1 P-Value
Here, P means probability. A p-value is a measure of how much evidence we have against
the null hypothesis. The smaller the p-value, the more evidence we have against H0. If
the metric does not have influence on the class defect count, the null hypothesis H0 we
need to test is the coefficient of the metric is equal to zero. Traditionally, researchers
will reject a hypothesis if the p-value is less than 0.05. Then we can say the metric has
significant influence on the class defect count.
25
5.4.1 Discussion of Hypothesis one
From Table 5.3 and 5.5, six out of seven projects indicate that increase in IL value
associates with increase in defect count because the parameters of IL in seven project
regression models are positive. And the p-values of IL’s parameter are all smaller than
0.05. That means IL has significant influence on class defect count at alpha level of 0.05.
These six projects support our hypothesis one. The only one exception is Commons-
Collections. We define a measurement ”percent“ to investigate the reason why IL is not
applicable in this project.
percent =
∑
(attributesUsedInEachMethod)/
∑
(numberOfAttributes∗numberOfMethods)
(5.3)
Define percent is equal to zero when number of attributes is zero. We found that the
percent measurement values of about one-second classes in Commons-Collections and
BCEL projects are zero while in other projects, there are smaller than one-third classes
with zero percent values. In BCEL, the classes with zero percent value are due to zero
attributes (177/186). In Commons-Collections, the classes with zero percent value are
not mainly due to zero attributes (39/99). Because IL assumes each method used all
attributes, percent measurement reflects such assumption. We believe we can judge
whether IL is applicable based on the percent measurement. Thought we can not collect
percent value during early design phase, we can reference number of attributes (NA)
metric value. When classes with zero NA is smaller than one-third, we believe IL is
applicable.
27
hard to distinguish different design complexity. In such case, CBO does not cause
too significant different of design complexity between classes.
• Model of Table 5.4 has higher R-square value, but model of Table 5.3 or 5.4 can
achieve simliar adjusted R-square value of model in Table 5.5. POI and ORO belong
to this type. In these project, the correlation between IL and CBO is very high
(above 0.7). Under such condition, the variation of IL can reflect the variation of
CBO, or vice versa. Thus, the design complexity of class can be represented using
single metric. In the projects, single variate model using IL of six projects has better
explanation ability than using CBO, so IL is recommend in this situation.
• The result of model 4.2 is better than single variate model. Cocoon belongs to this
type.
5.4.3 Other discussion
The adjusted R-square values of regression models in Table 5.5 are from 0.25 to 0.76.
Different amount of class defect count is explained by the design complexity and CBO.
Low adjusted R-square value represents there should be more factors influencing the class
reliability. According to different application domain, the factor that affects class relia-
bility should be different. For example, Jakarta-Struts is a web application framework,
and web application may be influenced by the server configuration and execution envi-
ronment. But Jakarta-POI basically handles the file content transformation, the factor
causes class to generate fault may be less than web application framework. This may be
the reason of the different amount of defect count explained by IL and CBO.
29
Table 5.3: Linear Models Using IL as the Independent Variable
Project Name Coefficient of Intercept Coefficient of IL Adjusted R-square
Jakarta-ORO -3.48E-02 (.727) 6.04E-04 (.000) .868
Jakarta-BCEL .154 (.000) 8.24 E-05 (.000) .345
Jakarta-POI .367 (.000) 9.85E-05 (.000) .365
Jakarta-Struts .491 (.000) 1.55E-04 (.000) .347
Cocoon .222 (.000) 6.59E-05 (.000) .198
Jakarta-Tapestry .163 (.000) 2.51E-05 (.000) .192
Commons-Collections .557 (.000) 3.82E-06 (.623) -.003
Table 5.4: Linear Models Using CBO as the Independent Variable
Project Name Coefficient of Intercept Coefficient of CBO Adjusted R-square
Jakarta-ORO -.323 (.219) .699 (.000) .370
Jakarta-BCEL .203 (.000) 1.02E-02 (.000) .083
Jakarta-POI .177 (.001) .352 (.000) .475
Jakarta-Struts .558 (.001) .416(.005) .040
Cocoon .156 (.000) .183 (.000) .126
Jakarta-Tapestry .100 (.002) 5.44E-02 (.000) .092
Commons-Collections .409 (.000) .150 (.001) .049
31
defects exist in the software and these activities may generate additional defects which
may be counted as the defect caused by design complexity before enhancement or update,
these activities result in the inaccuracy of the relationship between design complexity and
defect count.
Design complexity of some projects are not first major release. For example, Commons-
HttpClient is 3.0 alpha release 1. In these projects, some classes in the system are devel-
oped from the project initiation. They may be tested and corrected after the previous
several major releases. The classes in the software system undergo different development
time, and the development time have effect on the class reliability. In our experiment,
we regard the development time of each class as the same. But During the time interval
we identify, more defects may be founded in newly defined or modified classes than the
classes which are tested before and to be fit in new context.
The defects collection approach is based on the key words “ PR ” and “ bug ”. Some de-
fects are fixed and not documented in such format will not be counted in our experiment.
This will have some influences in the experiment result.
33
• 1. A small rectangle represents a Java class and the large rectangle which consists
of many small class rectangles represents a package of the software system. When
user presses the class rectangle, the other classes it is coupled to will be shown by
green lines.
• 2. The IL value is visualized by the size of class rectangle. The width of class
rectangle is square root of IL value multiply by 0.8, and the height is square root of
IL value multiply by 0.6.
• 3. The CBO value is visualized by the color of class rectangle. The deeper color a
class rectangle is, the higher CBO value the class has.
• 4.When the mouse enters the class rectangle, a pink line (or pink lines) will be shown
to connect this class and its ancestor classes. It is used to represent the inheritance
relationship in the software system.
• 5. The inheritance relationships of all classes can be also shown at the same time.
• 6. The width and height of class rectangle can be transformed to model different
metrics related to IL. We implement this functionality by transferring class rectan-
gle’s width to model the value of the number of parameter (NP) metric, and the
height to model the value of the number of attribute (NA) metric. Other existing
metrics like number of method (NM), interface size (IS) [7] may be the candidates
used to extend this functionality. By this size transformation, we can have more
clear insight about the cause of large IL value of a class. That is, large IL may be
caused by large class interface or large instance variable count even both.
• 7. The defect count is displayed in the upper-left corner of the class rectangle.
35
Figure 6.1: IL and CBO metrics visualization result. Classes are sorted according to defect count
37
Figure 6.3: NP and NA metrics visualization result.
39
data about metrics and class defect count of similar project, model can be set up to
combine the design metrics of different viewpoint.
Transforming the class rectangle size to represent different metrics value helps devel-
opers to get more insight to understand the IL complexity. If NA is too large, developers
can check whether this class is an appropriate data abstraction. If NP is too large, that
may mean the class has many responsibilities. Developers can alleviate this situation
by distributing responsibilities to other classes. In some cases, it is hard to modify the
design. In order to make sure that the risky classes will have better reliability, formal
verification methods or testing should be adopted after using our visualization tools.
6.3.1 Formal Methods
During design phase, some formal methods can be applied in the method’s design.
Aspect’s [20] specification notation permits users to write pre- and postconditions
about the data dependencies of an operation. Dataflow analysis is used to compute an
upper bound on the data dependencies of the implementation. If an asserted dependency
is missing, an error is reported.
Quick Defect Analysis (QDA) [21] also uses a simple annotation language. Annotations
called hypotheses are embedded in comments to describe properties that objects should
have at particular program points. Other comments contain assertions about properties
of objects. An interpreter builds an abstract model of the implementation from the
assertions and the implementation’s control flow graph. Hypotheses are verified with
respect to this model.
41
Chapter 7
Conclusion and Future Work
Firstly, we empirically investigate the relationship between IL metric and class reliability.
The results seem to show IL can be a better indicator of the class reliability during
design phase than method count. IL would be able to help developers to make design
inspection more efficient and provide useful information both to project managers and
developers. Secondly, by combing CBO and IL to model the role of class, there is higher
explanation ability of the empirical models to explain class defect count. This implies the
quality of a class can be modelled in different dimensions software metrics. And the last,
we developed a visualization tool to help developers easily understand the relationship
between complexity metrics and class defect count. Developer can also easily identify
the classes that should be paid special attention to.
The future work includes:
• Compare the defects explanation ability of IL across different programming lan-
guage, such as C++ and Java. Different programming language may have different
constraint on the interface design and IL value should be affected. Therefore, the
43
References
[1] T. J. McCabe, “A complexity measure,” IEEE Transactions on Software Engineering, vol. 2, no. 4,
pp. 308–320, Dec. 1976.
[2] I. Sommerville, Software Engineering. Addison-Wesley, 6 ed., 2000.
[3] V. Y. Shen, T.-J. Yu, S. M. Thebaut, and L. R. Paulsen, “Identifying error-prone softwarexan
empirical study,” IEEE Transactions on Software Engineering, vol. 11, no. 4, pp. 317–324, April
1985.
[4] G. K. Gill and C. F. Kemerer, “Cyclomatic complexity density and software maintenance produc-
tivity,” IEEE Transactions on Software Engineering, vol. 17, no. 12, pp. 1284–1288, Dec. 1991.
[5] V. R. Basili, L. C. Briand, and W. L. Melo, “A validation of object-oriented design metrics as
quality indicators,” IEEE Transactions on Software Engineering, vol. 22, no. 10, pp. 751–761, Oct.
1996.
[6] R. Subramanyam and M. Krishnan, “Empirical analysis of ck metrics for object-oriented design
complexity: Implications for software defects,” IEEE Transactions on Software Engineering, vol. 29,
no. 4, pp. 297–310, April 2003.
[7] D. H. Abbott, T. D. Korson, and J. D. McGregor, “A design complexity metric for object-oriented
development,” Tech. Rep. 94-105, Department of Computer Science, Clemson University, 1994.
45
