2P2P Layer
Post Office
Locating & Tracking
Security
Control
Resources
Discovery
GUI for Monitoring & Control
Immigration
&
Migration
Chord CANPastry
Common API for Structured P2P Overlays
Tapestry
Mobile AgentMobile Agent
Navigator
Mail Folder
Mobile Agent
System Layer
Agent Layer
Fig. 1. Visitant system architecture.
move over different mobile agent systems—a distributed abstraction layer to provide mobile agents with mobility,
communication, and security [1], [29]. If each peer has a mobile agent system to function as a middleware between
mobile agents and the underlying execution platform, then P2P users can deploy their programs in mobile agent
format to remote peers easily and flexibly without complex configuration.
To help mobile agents allocate resources (including agents themselves), we use structured P2P networks as the
base layer over which agents are travelling and migrating between different hosts. Structured P2P networks, e.g. [37],
[32], [30], organize their overlay topology into some distributed data structure, over which objects are placed via
some globally-agreed scheme. They typically can provide a distributed object locating service that guarantees to find
an object within O(logN) steps, and takes only O(logN) space per site for storing routing information, where N
is the network size. Their counterpart, unstructured P2P networks, abandon efforts to maintain a specific topology,
so as to increase system robustness in a highly dynamic network environment. This, however, is at the cost of
search, as flooding is essential in such a chaotic network.
We call the agent-based P2P system we designed Visitant. Below we give an overview of the system. Section III
presents some fundamental problem in the design and our results.
II. VISITANT
As shown in Figure 1, Visitant is divided into, from bottom to top, the following three layers: P2P Layer, System
Layer, and Agent Layer. P2P Layer plays as an execution platform of the Visitant system, and provides agent
migration and communication between Visitant systems.
System Layer provides an execution environment for mobile agents. It is composed of six modules: Locating &
Tracking, Post Office, Immigration & Migration, Security Control, GUI for Monitoring & Control, and Resources
Discovery. The Locating & Tracking module locates a mobile agent in the system. The Post Office module is
responsible for message delivery between mobile agents. The Immigration & Migration module allows mobile
agents to migrate from one host to another. After migration, agents need to update their current location to the
Locating & Tracking module. The Security Control module adopts Java Security Architecture as the basic resources
access control mechanism, and on top of which it needs to handle clone management. A GUI is provided for
monitoring and controlling mobile agents. It uses the Locating & Tracking module to get locations of mobile
agents, and transmits messages to them via the Post Office module. The last module Resources Discovery provides
a keyword search mechanism for mobile agents to search resources in the P2P network.
4for H, the load of agent hosts in handling mobile agents can be made balanced even though the creation of agents
may not uniformly occur in the system.
2) Post Office: The Post Office module of a host x is responsible for delivering messages sent to the agents of
which x is their home, as well as transmitting the messages sent by the agents currently in the host to their targets.
In Visitant, messages between mobile agents are called mails. Each mobile agent a has a mailbox in its home
H(a). All mails to an agent will first be delivered to the home host of the agent, and then delivered to the agent by
the host according to the mail type. Two types are supported: Express and Ordinary. Express mails will be forwarded
by home to agents immediately when they arrive at the home. They can be used for real time communication with
the agents. Ordinary mails, on the other hand, will be stored at home and be taken away by agents when they contact
with their homes; this occurs when agents migrate to a new host, or when they actively check their mailboxes.
3) Immigration & Migration: A distinguishing characteristic of mobile agents is migration. To support migration,
a mobile agent system needs to stop an agent’s execution, records its status, and then transmits the agent to the
destination. The agent will then be restarted by the remote mobile agent system. The Immigration & Migration
module of Visitant provides this functionality. For portability, we use JAVA as the implementation language in
Visitant. When an agent finishes its tasks and is ready to migrate, it notifies the Immigration & Migration module
for migration. The module then removes the agent from the host’s visiting agent list and uses Java serialization for
transmitting objects and variables in the program.
Specifically, Visitant supports weak mobility [15], meaning that agent systems do not record executing states
such as program counter and stacks before agent migrates. Rather, agents migrate at specific part of programs. Most
Java-based mobile agent systems use weak mobility because Java Virtual Machine (JVM) limits programmers to
obtain agent execution information for security reasons.
4) Resources Discovery: Mobile agents need resources to complete their tasks. Resources include CPU cycles,
storage space, bandwidth, as well as agents themselves. To obtain resources, agents need to know their locations,
and the Resources Discovery module is designed for this purpose. In Visitant, each available resource must be
registered into the system by the site that owns the resource. Registration information includes the name of the
resource, its attributes, and the location of the resource. Visitant provides two types of search for resource discovery:
by name (white page service) and by attribute (yellow page service).
To support name search, when a site registers a resource s, it inserts a name record of the resource into the site
H(s). So when an agent needs to locate s, it can use the lookup service provided by the base layer to send its
query message to the site H(s), and then obtains the location of s.
For yellow page service, we have developed a hypercube index and search scheme [18] to index attributes of
resources to facilitate efficient search. More details of this will be given in Section III-B.
5) Security Control: Security threats in mobile agent systems can be roughly classified into two categories:
uncoordinated attacks, and coordinated attacks. Uncoordinated attacks refer to individual attacks to hosts, such as
unauthorized access to a host’s resources. For uncoordinated attacks to individual hosts, Visitant uses JAVA sandbox
to provide the basic security mechanism. Each mobile agent host can define its own security policy for incoming
mobile agents, which must then meet the policy in order to execute on the host.
Coordinated attacks refer to systematic attacks to a host or to the entire system, typically done by consuming
an extraordinary amount of resources to paralyze the target. Visitant’s strategy to this problem is the following.
First, every agent, when created, is given a capability, specifying its TTL (Time-To-Live)—the time it can live in
the system, the maximum number of resources (e.g., CPU time, bandwidth, and memory space) it can use at each
site, the number of times it can migrate, and the number of clones/decedents it can produce.
Second, each agent is assigned with a guardian to monitor its capability. Like agent homes, we use a one-way
6around the network, to avoid flooding the network, some TTL is set to limit the search space. So search cannot
be guaranteed if target objects are distant. Much work on unstructured P2P networks has been devoted to search
improvement (e.g., [17], [39], [6], [25]), with some specifically focusing on keyword search [28], [2].
Our contribution in this direction is a simple, practical, yet powerful index scheme to enhance search in
unstructured P2P networks. The index scheme uses a data structure “Bloom filters” to index files shared at each
node. Let Bu be the Bloom filter of node u. If Bu is replicated and distributed to other nodes in the network, then
every node v that has Bu can answer queries about whether u might have a particular file one is looking for. If
the answer is yes, v can ask u to perform an actual check to its local directory to see if u does have the file.
In general, the replication allows a query to u’s files to be answered by any node having a copy of Bu. So if
every node has a copy of Bu, then file search can be efficiently performed in the system. However, for the system
to scale, only a limited number of replicas of Bu can be distributed. In practice, the number of replicas should
allow a query initiating from a node to be answered within a reasonable search space. Here, the search space refers
to the set of nodes to be visited for the query. It corresponds directly to the set of Bloom filters to be searched
during the query resolving process.
Still, the distribution of a node’s Bloom filters may not be so uniform to let the search space of any given
node v contain a copy of a particular Bu. As a result, a query to u’s files will not be resolved at v. To solve
this problem, we let nodes dynamically exchange their Bloom filters so that the search space of each node is
also dynamically changing. This then allows every query from a node to have some probability to be successfully
resolved, regardless of the time the query is issued. By properly setting the system parameters, we can tune the
probability to be reasonably high.
The experimental results show that our approach can improve the search in Gnutella by an order of magnitude.
For example, in a typical Gnutella network consisting of about 89,000 nodes, by replicating a node’s Bloom filter
to less than 0.45% of the nodes in the network, 70% of the queries can be resolved within a search space of
200 nodes. In contrast, within the same search space size, only 1.6% of the queries can be resolved without the
index scheme; or, alternatively, more than 48,000 nodes need to be searched in Gnutella in order to reach the same
success rate as our index scheme. The result is published in [4].
B. Search in Structured Peer-to-Peer Networks
For structured P2P networks like DHTs , they basically support only exact name match [14], [24], as objects
are given a unique identifier obtained by hashing their names to determine their location in the network. Keyword
search must be built on top of the overlay to enhance search functionality. Several mechanisms have been proposed
for keyword search in DHTs (e.g., [13], [40], [31], [38], [36], [11]), but all of them use inverted index as the
primary data structure.
An inverted index is a set of entries of pairs (w,O), where w is a keyword, and O is the set of objects containing
this keyword; see Fig. 2. Once an inverted index is built, a set of keywords can be entered to find all objects that
contain these keywords. For example, in Fig. 2, by taking an intersection of the sets associated with keywords
term1, term2, and term3, we can find the object (i.e., Object1) that has all these keywords.
To implement keyword search in a P2P network, a distributed version of inverted index can be built. A simple
way is to distribute the entries so that each keyword is assigned a node to index the objects that have this keyword.
By incorporating into DHT networks, one can use a given keyword as key to determine the node that is responsible
for the keyword, and obtains objects that contain the keyword. By taking a join operation, one can retrieve objects
with a given keyword set.
8C. Hypercube-Based Index and Search scheme
Our main result for keyword search is to devise a general keyword index and search scheme for DHT networks.
The idea is to represent each object as an r-bit vector according to its keyword set. Then we construct the index
scheme over an r-dimensional logical hypercube Hr(V,E). The hypercube can be constructed directly from a
physical hypercube (e.g. HyperCuP [35]), or conceptually built on a DHT. The advantage of using a physical
hypercube is that communication between two neighboring nodes in the logical layer costs only one hop of message
transmission in the physical overlay.
To construct Hr(V,E) over a physical DHT G = (V ′, E′), we simply need a mapping g : V → V ′ so that every
logical node in the hypercube has a corresponding physical node in the network. However, as mentioned before,
most DHTs offer O(logN) hop-to-hop delay for communication between any two nodes, where N is the size of the
network. So a message transmission between any two nodes in the hypercube will cost O(logN) messages in the
DHT. Another advantage is that the size of the hypercube can be decoupled from the size of the DHT. The former
is often determined by the object set to be indexed, while the latter is determined by the number of participating
nodes in the system.
Our index scheme does not impose any specific requirement on the mapping of hypercube nodes to DHT nodes,
and thus makes it a general keyword search layer over any chosen DHT. Nevertheless, some guidelines may be
provided to choose the mapping. For example, when the size of the hypercube is larger than the size of the DHT
(i.e., there are more logical nodes than physical nodes), then for load balancing, we can use a hash function (e.g.,
SHA-1) to uniformly map logical nodes (by their IDs) to physical nodes. When the size of the hypercube is smaller,
only a portion of the physical nodes will actually be responsible for indexing objects. This allows some leeway
in selecting indexing nodes. For example, many researches have observed that nodes in P2P networks are not
homogeneous: some are more stable/powerful than the others [34], [5]. So we may select stable/powerful nodes to
serve as indexing nodes in the hypercube. The use of “supernodes” as index servers for ordinary nodes has been
practically adopted in several popular unstructured P2P networks like KaZaA and eMule.
The index scheme works as follows. Let W be the set of all keywords considered in the system. Let h : W →
{0, 1, . . . , r− 1} be a uniform hash function that maps every keyword in W to an integer in {0, 1, . . . , r− 1}. We
define a mapping Fh : 2W → V as follows: Fh(K) = u if, and only if, One(u) = {h(w) |w ∈ K}. In other words,
Fh(K) is the node whose bits are set by the hash function h according to the keywords in K. We say that u is
responsible for K if Fh(K) = u. Thus, for every possible set of keywords in the system, there is a unique node in
the hypercube responsible for the set. Note that a node may be responsible for more than one set of keywords (as
Fh(K) might be equal to Fh(K ′) for some K and K ′). We use Ru to denote the set of keyword sets for which
u is responsible; that is, Ru = {K ⊆ W |Fh(K) = u}.
To build the index scheme, for each object σ that is associated with keyword set Kσ, we let the node Fh(Kσ)
in the hypercube maintain an entry 〈Kσ, σ〉 in its index table. We say that σ is indexed at the node, and we use
Ou to denote the set of objects that are indexed at u; that is, Ou = {σ ∈ O |Kσ ∈ Ru}.
Fig. 3(a) illustrates the index scheme over a 4-dimensional hypercube. The mapping of hash function h is also
shown in the figure. For example, h(b) = 3, h(c) = 1, and h(e) = 0. So the keyword set {b, c, e} is responsible
by node 1011. Objects x and y both have keyword set {b, c, e}, so they are indexed at node 1011. In addition, the
node also indexes object w, which has keyword set {b, c, f}.
1) Search in the Hypercube: Given a keyword set K, we can locate a copy of object associated with K by first
finding the node in Hr that is responsible for K. The node is determined by Fh(K). Once the node is located, its
index table can be searched to obtain the ID of an object σ that is associated with the keyword set K. Then, a
call Read(σ) to the underlying DHT network will invoke the DOLR scheme to return a copy of σ. So pin search
10
approaches, including GDFS, query expansion, and caching. All of them proved very effective in reducing the
nodes to be contacted. In particular, with just a small size of cache, the number of nodes need to be contacted is
significantly reduced: less than 1% of nodes per query in order to retrieve all the matching objects.
To summarize, our hypercube index and search scheme has the following interesting properties compared to the
inverted index approach: First, the index entries of a single keyword are handled by a set of nodes. The population
of this set depends on the popularity of the keyword: the more the popularity of a keyword, the more the number of
nodes responsible for the keyword. As a result, the load of nodes can be balanced even though keyword distribution
follows Zipf’s Law, and no node is likely to be swamped even if it handles a very popular keyword. Moreover,
since a number of nodes are responsible for a keyword, any failure of them cannot block queries involving the
keyword.
Secondly, an object σ associated with a keyword set K can be efficiently ‘pinpointed’ if the set K is given.
This is analogous to exact name search in DHT networks. As discussed earlier, DHT networks use an object’s
name to determine its handling node. Thereafter, locating the object is simply a message routing to the node, which
can be done very efficiently in the networks. In our index scheme, we use the keyword set associating with an
object to determine a unique node to index the object. When the set is known, locating the object is as efficient
as exact name search in DHT networks. In contrast, this kind of ‘pin search’ is usually very expensive in existing
P2P networks. Likewise, object insert, delete, and maintenance can also be done efficiently, as no unnecessary
redundancy is introduced in our scheme to index objects.
Third, in a search operation, in addition to objects whose keyword sets match exactly with a given keyword
set K, one may also wish to retrieve objects whose keyword sets contain K. All these objects can be easily and
efficiently retrieved in our index scheme. Moreover, the larger the set K a user has specified, the more restriction
a user has placed on his target objects. Accordingly, our index scheme will require fewer number of nodes to be
contacted. On the other hand, when a small set of K is given, a large number of objects may satisfy the search
request. In this case, a user often expects to see only a small subset of them. Our index scheme can also support
this kind of operations effectively and efficiently.
Specifically, objects in our index scheme are easily distinguished by the number of keywords they associate. For
example, let K be a set of keywords. Our index scheme can easily locate objects that are associated with exactly
the set K of keywords, objects that are associated with K plus one more keyword, K plus two more keywords,
and so on. Moreover, within each category, e.g., K plus one more keyword, objects can further be distinguished
by the extra keyword they have, e.g., K plus a specific keyword σ1, K plus a specific keyword σ2, and so on.
This interesting feature allows upper level applications to retrieve objects in the order they wish. For example,
an application might prefer more specific objects to be retrieved first. In this case, when a search request with a
keyword set K is issued, our index scheme can return objects containing this keyword set K in the order by giving
preference to those with more extra number of keywords. On the other hand, if an application prefers more general
objects, then our index scheme can give preference to those with fewer number of extra keywords. Furthermore,
our index scheme may also sample some objects in each category described above, e.g., objects that have an extra
keyword σ1, an extra keyword σ2, . . ., two extra keywords σ1, σ2, two extra keywords σ1, σ3, . . ., and so on; and
then return these sample objects along with their extra keyword(s) to help users refine their queries. Note that no
global knowledge is required to implement this ranking mechanism. Moreover, the clustering effect of keyword
sets also makes query expansion easy to achieve, because a node can obtain which additional keywords are likely
to co-occur with its keyword set by contacting only its neighbors.
Part of the results have been published in the 25th International Conference on Distributed Computing Systems
(ICDCS 2005) [18], and in the IEEE Journal on Selected Areas in Communications (JSAC), Special issue on
12
keytoken space
<a,1> <z,20> <0,1> <9,4> <a,1> <k,1>
object
DTITLE: norah jones unforgettable DYEAR: 2002 DGENRE: jazz (f)
o
. . . . . . ...
. . . . . .
range {1, ..., r}
1r-bit vector 0 0 00 1 1 1 0 1
a node in hypercube with node id={101001101010}
. . . . . . ...
01
tokenization
clustering
hash mapping
<0,21> <9,24><a,25> <k,25>shift shift
Fig. 4. Keytoken-based index scheme.
results in high search costs), or else hash collision would be high (which results in unbalanced index loads). To
reduce keytoken set size, we observe that some keytokens are highly correlated. For example, in English, many
words begin with de∗, so 〈d, 1〉 has high probability to occur with 〈e, 2〉 simultaneously. We define the correlation
coefficient of two keytokens tj and tk as follows
corr (tj , tk) =
∑
i
fij · fik√∑
i
fij ·
∑
i
fik
where fij is a binary value representing if tj is in object oi. Then, given a threshold thr , we can use some clustering
technique, e.g., simple-link [33], to cluster highly correlated keytokens and use a single keytoken to represent each
cluster.
We note that according to our preliminary experiments, keytoken correlation is quite stable for English words.
This means that the correlation coefficients can be obtained from some sample dataset and so need not be calculated
online.
1) Search Strategies: To search objects in the index hypercube, suppose, for example, one is looking for some
record in the jazz category with unforg* in the DTITLE field. Then, analogous to the index scheme, we first
extract the keytokens from the query: T = {〈u, 1〉, 〈n, 2〉, 〈f, 3〉, 〈o, 4〉, 〈r, 5〉, 〈g, 6〉, 〈f, 25〉}. We then reduce the
keytoken set by merging keytokens that are highly correlated, and use the mapping Fh to determine the node that
is responsible for indexing the set. Let us assume that this node is 000001101010.
Observe that any object that matches the query must have a keytoken set that is a superset of T . So the matching
objects can only be indexed at nodes whose IDs are of the form xxxxx11x1x1x. These nodes form a “subhypercube”
of the original hypercube Hr, and search over a hypercube can be done by traversing a corresponding spanning
binomial tree [18]. Moreover, the information in keytokens helps “guide” and “prune” the search tree. For example,
in our sample query unforg*, the next matching keytoken must be of the form 〈?, 7〉 (a word expanded from unforg),
or of the form 〈?, 1〉 (accompanied by a new keyword). For the character field, the keytoken correlation also helps
us know which candidate keytoken is likely to follow. As such, we can develop an efficient search strategy to
explore the search tree. Finally, the information stored in keytokens also allows us to rank and return objects in
14
The index scheme of KISS-W relies on a very fundamental process called tokenization to extract characters
and their position information in a keyword to index objects. Let A be the set of alphabets in consideration. A
keytoken is a pair 〈c, i〉, where c ∈ A and i an integer. For notational simplicity, we sometimes write 〈c, i〉 as
ci when no confusion is possible. Let W ⊂ A+ be the set of keywords used in the system. For each keyword
w ∈ W , we use w[i] to denote the ith character of w, and ||w|| to denote the length of w. A tokenization is a
process to extract keytokens—the character-position—from a given word w. The position can be counted “forward”
(1, 2, . . .) and “backward” (−1,−2, . . .). Therefore, we define two types of tokenization, forward and backward,
and a combination of both:
• A forward tokenization is a function τf that extracts keytokens from a given keyword w by counting the
position forward; that is
τf (w) =
{
〈w[i], i〉
∣∣ i ≤ ||w||}
We call τf (w) the forward keytoken set of w. For example, τf (jazz) = {j1, a2, z3, z4}.
• A backward tokenization is a function τr that extracts keytokens from a given keyword w by counting the
position backward; that is
τb(w) =
{
〈w[i],−||w|| + i− 1〉
∣∣ i ≤ ||w||}
We call τb(w) the backward keytoken set of w. For example, τf (jazz) = {j-4, a-3, z-2, z-1}.
• A symmetric tokenization is a function τs that extracts all the keytokens from a given keyword w in both the
forward and backward style. That is,
τs(w) = τf (w) ∪ τb(w)
We call τs(w) the symmetric keytoken set of w.
Given W , the number of all possible keytokens that can be extracted from W is no greater than 2|A|×lmax, where
lmax is the maximum length of a keyword. We shall use T =
⋃
w∈W τs(w) to denote the set of all possible keytokens
considered in the system. Clearly, T can be partitioned into two subsets Tf =
⋃
w∈W τf (w) and Tb =
⋃
w∈W τb(w).
For any set of keytokens T ⊂ T , we say that T is valid if the keytokens are extracted from some words in W; i.e.,
T =
⋃
w∈K τs(w) for some K ⊂ W . Note that the character positions in a valid keytoken set must be continuous
and span from 1 to some k and from −1 to −k.
To make our index scheme general and independent of the underlying physical network, we again present the
scheme along with its search mechanisms over an r-dimensional logical hypercube Hr(V,E). Implementation issues
concerning the hypercube over an existing DHT network can be found in [18], [22].
To index objects in the hypercube, assume that each node in the hypercube has a unique r-bit binary string as
its id. We use u[i], 1 ≤ i ≤ r, to denote the ith bit of u (counting from the right), and One(u) = {i |u[i] = 1}
for the set of positions at which u has bit 1. Let h : T → {1, . . . , r} be a hash (and onto) function that uniformly
and independently maps every keytoken in T to an integer in {1, . . . , r}. We define a mapping Fh : 2T → V as
follows: Fh(T ) = u if, and only if, One(u) = {h(t) | t ∈ T}. In other words, Fh(T ) is the node with a binary
id whose bits are set by the hash function h according to the keytokens in T . For example, suppose h(w1) = 3,
h(e2) = 1, h(b3) = 7, h(w-3) = 2, h(e-2) = 6, h(b-1) = 3, and r = 10. Then the keytoken set {w1, e2, b3, w-3,
e-2, b-1} is mapped to the node 0001100111 in H10.
We say that a node u is responsible for a keytoken set T if Fh(T ) = u. Thus, for every possible keytoken set
in the system, there is exactly one node in the hypercube responsible for the set. Note that due to hash collision,
a node may be responsible for more than one set of keytokens. A keytoken set T responsible by u is maximal if
Fh(T ) = u and for all T ′ such that Fh(T ′) = u, we have T ′ ⊆ T .
16
{ /ab,
  b/a }
{ /cde,
  e/cd,
  de/c }
{ /ce,
  e/c }
{ /cb,
  b/c }
{ /aeb,
  b/ae,
  eb/a }
Object O1
term3 : cb
term4 : aeb
Object O2
Object O3
term1 : ab
term3 : ce
{ /ab,
  b/a }
{ /ce,
  e/c }
{ {/a, /ab},
  {b/, b/a} }
{ {/c, /cd, /cde},
  {e/, e/c, e/cd},
  {de/, de/c} }
{ {/c, /ce},
  {e/, e/c} }
{ {/c, /cb},
  {b/, b/c} }
{ {/a, /ae, /aeb},
  {b/, b/a, b/ae},
  {eb/, eb/a} }
{ {/a, /ab},
  {b/, b/a} }
{ {/c, /ce},
  {e/, e/c} }
term permutation prefix expansion inverted index
Index
term
List of
objects
/a
/ab
/aeb
/c
/cd
/cde
/ce
b/
b/a
eb/
eb/a
de/
de/c
e/
b/c
b/ae
e/c
e/cd
{O 1 , O 2, O 3 }
{O 2 }
{O 1 }
{O 1 }
{O 2 }
{O 1 }
{O 1 }
{O 2 }
{O 2 }
{O 1 }
term1 : ab
term2 : cde
term3 : ce
{O 1 , O 2, O 3 }
{O 1 , O 2, O 3 }
{O 1, O 3 }
{O 1, O 3 }
{O 1, O 3 }
{O 1 , O 2, O 3 }
{O 1, O 3 }
{O 2 }
{O 2 }/ae
{O 2 }/cb
Fig. 7. Transforming wildcard query to prefix match.
will often represent a query α1 ∧ . . . ∧ αl simply as a set Q = {α1, . . . , αl}. For example, the query {a∗b, c?} is
looking for a keyword set containing a keyword with prefix a and suffix b, and another keyword that starts with c
and is of length 2.
When a query Q is combined with pin search, an object σ satisfies Q if the keyword set associated with σ
matches Q. Similarly, when combined with superset search, an object σ satisfies Q if σ’s keyword set contains
a subset that matches Q. The search is to retrieve objects that satisfy the query. For example, pin search for the
query {a∗b, c?} in Fig. 7 will return {O2, O3}, while superset search for the same query will return {O1, O2, O3}.
The detailed search algorithm, along with simulation results, can be found in [22].
IV. CONCLUSIONS
To conclude, in this project we have designed and implemented Visitant, an agent-based P2P system, to integrate
mobile agent technology with P2P computing. Unlike existing agent-based P2P systems that are often built over an
unstructured P2P network, Visitant adopts a structured topology. Likewise, communication can be done efficiently
and search of an agent is guaranteed in the sense that the target can be located in bounded time and using only
bounded resources.
What is more interesting and challenging in the design of the system is to study the resource allocation problem,
which, not only important to the system, but is also a very fundamental issue in P2P networks. In this direction, we
have achieved several important results, and some (prefix and wildcard search) are pioneer in the field. In addition
to the work cited here, there are still some work yet to be published, and some (e.g., more complex search) left in
the future work. We hope that more interesting results will be obtained in the near future.
REFERENCES
[1] J. Baumann, F. Hohl, K. Rothermel, and M. Straser. Mole - concepts of a mobile agent system. World Wide Web, 1(3):123–137, 1998.
[2] H. Cai and J. Wang. Foreseer: a novel, locality-aware peer-to-peer system architecture for keyword searches. In Proceedings of the 5th
ACM/IFIP/USENIX international conference on Middleware, volume 3231 of Lecture Notes in Computer Science, pages 38–58, New
York, NY, USA, 2004. Springer-Verlag.
[3] H.-C. Chang. On the implementation of application programming interface of visitant. Master’s thesis, National Central University,
Taoyuan, Taiwan, 2007.
[4] A.-H. Cheng and Y.-J. Joung. Probabilistic file indexing and searching in unstructured peer-to-peer networks. Computer Networks,
50(1):106–127, 2006.
18
[30] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker. A scalable content-addressable network. In Proceedings of the
2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications (SIGCOMM 2001), pages
161–172. ACM Pres, August 2001.
[31] P. Reynolds and A. Vahdat. Efficient peer-to-peer keyword searching. In Proceedings of the 2003 ACM/IFIP/USENIX International
Middleware Conference (Middleware 2003), volume 2672 of Lecture Notes in Computer Science, pages 21–40. Springer-Verlag, 2003.
[32] A. Rowstron and P. Druschel. Pastry: Scalable, decentralized object location and routing for large-scale peer-to-peer systems. In
Proceedings of the 2001 IFIP/ACM International Conference on Distributed Systems Platforms (Middleware 2001), volume 2218 of
Lecture Notes in Computer Science, pages 329–350. Springer-Verlag, 2001.
[33] G. Salton. Automatic Text Processing. The Transformation, Analysis and Retrieval of Information by Computer. Reading, MA: Addison-
Wesley, 1989.
[34] S. Saroiu, P. K. Gummadi, and S. D. Gribble. A measurement study of peer-to-peer file sharing systems. In Proceedings of the 2002
Multimedia Computing and Networking (MMCN 2002). The International Society of Optical Engineering, January 2002.
[35] M. Schlosser, M. Sintek, S. Decker, and W. Nejdl. HyperCuP: Hypercubes, ontologies and efficient search on P2P networks. In
Proceedings of the 2002 International Workshop on Agents and Peer-to-Peer Computing (AP2PC 2002), volume 2530 of Lecture Notes
in Computer Science, pages 112–124. Springer-Verlag, 2003.
[36] S. Shi, G. Yang, D. Wang, J. Yu, S. Qu, and M. Chen. Making peer-to-peer keyword searching feasible using multi-level partitioning.
In Proceedings of the 3rd International Workshop on Peer-to-Peer Systems (IPTPS 2004), volume 3279 of Lecture Notes in Computer
Science, pages 151–161. Springer-Verlag, 2005.
[37] I. Stoica, R. Morris, D. R. Karger, M. F. Kaashoek, and H. Balakrishnan. Chord: A scalable peer-to-peer lookup service for Internet
applications. In Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer
Communications (SIGCOMM 2001), pages 149–160. ACM Press, August 2001.
[38] C. Tang and S. Dwarkadas. Hybrid global-local indexing for efficient peer-to-peer information retrieval. In Proceedings of the First
Symposium on Networked Systems Design and Implementation (NSDI 2004), pages 211–224. USENIX, 2004.
[39] B. Yang and H. Garcia-Molina. Improving search in peer-to-peer systems. In Proceedings of the Twenty-Second International Conference
on Distributed Computing Systems (ICDCS 2002), pages 5–14. IEEE Computer Society, July 2002.
[40] F. Zhou, L. Zhuang, B. Y. Zhao, L. Huang, A. D. Joseph, and J. Kubiatowics. Approximate object location and spam filtering on
peer-to-peer systems. In Proceedings of the 2003 ACM/IFIP/USENIX International Middleware Conference (Middleware 2003), volume
2672 of Lecture Notes in Computer Science, pages 1–20. Springer-Verlag, 2003.
