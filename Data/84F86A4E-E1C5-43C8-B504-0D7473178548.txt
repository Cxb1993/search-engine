 2
一、 計畫緣起及目的: 
現實世界中眾多問題最終仍歸結為最佳化的問題，最佳化問題存在於各個領域。
對於其求解的方式過去已有人研究出不少經典的演算法，對某些問題確實能得到較
好的結果。但這些傳統的方法對目標函數有較強的限制，如連續、可微等。近年來
得到快速發展的演化計算，是一種借鑒達爾文進化論而模擬出的隨機最佳化方法。
演化計算的方法著眼於從一組（種群）潛在解（個體）中尋找問題的最佳解。通過
在這一組當前潛在解之間進行適當的遺傳操作，如篩選，重組和突變，便有望產生
更好的解。它克服了傳統數值方法的缺點，借助大自然的演化過程，採用種群和隨
機搜尋機制來解決最佳化問題。演化計算在應用問題方面，至今已成功的運用在許
多不同領域的最佳化問題上，如藥物設計 [1]、生物資訊 [2] [3]、演化硬體 [4] [5]、
電路設計 [6]、資料探勘 [7]、財經分析 [8] [9]。 
演化策略 (Evolution Strategy, ES) 為演化計算的其中一個重要分支，其歷史可
回溯到 1960 年代德國柏林大學 Schwefel 與 Rechenberg 兩位研究學者為了解決
工程上所遇到的最佳化難題而發展出的一套演算法 [10] [11] [12] [13]。演化策略中，
個體利用實數向量表示，經由高斯分佈隨機變數的加入達到擾動的目的。ES 演算
法本身的改良一直是演化計算中的重要的研究主題，目前已有許多文獻提出對 ES 
演算法的改良 [12] [14] [15] [16] [17]，唯無一個演算法能同時滿足對問題求解的迅
速性、問題領域的通適性以及處理高維問題的穩定性。(μ, λ)-ES 演算法的流程
中，主要包含四大程序：初始化人口 (population initialization)、突變 (mutation)、
重組 (recombination)、篩選 (selection)。在使用者設立的終止條件 (例如：運作時
間上限、計算資源上限、解品質下限…等) 尚未成立前，將持續進行演化與計算的
動作。 
而在傳統的 ES 中，由於搜尋的過程缺乏 global search space 的資訊，搜尋的
過程易被限制在 local optima 而導致整體效能降低。為改善此缺點，本研究計畫企
圖將『粒子演算法 (Particle Swarm Optimization, PSO)』 [18] [19] [20]中全域搜尋
的概念與傳統 ES 做整合，以機率統計中『主成份分析技巧 (Principal Component 
Analysis, PCA)』當手段，使 ES 能夠利用整體的資訊進而達到更好的搜尋效能。 
綜合以上所述，本研究計劃的主要目標為：『擷取 Swarm Intelligence 全域搜
尋的概念，研發一個能利用個體相互之間的資訊與群體整體的結構，使得 ES 能
以更直接有效的方式來達到演化的效果』。為達到此目的，吾人整合了 PSO、PCA、
Evolution Path 的特牲，企圖發展出『PSGES 演算法  (Particle Swarm Guided 
Evolution Strategy)』，針對傳統 ES 缺乏 global search space 資訊的缺點來做改良，
使之能在搜尋整體最佳解的時候達到空間上引導的效果，以期達到對問題求解的迅
速性、問題領域的通適性、以及處理高維問題的穩定性，由於以上原因，本計畫的
執行將有助於演化計算領域的發展、提供 ES 在研究領域上的新方向。同時，所
發展之實數參數全域最佳化演算法，亦可應用於許多現實世界的實際問題和系統，
進而促進其他各類工程或科學領域之可能發展與潛在突破。 
 
 4
[3] B. A. Shapiro and J. C. Wu, "An Annealing Mutation Operator in the GA for RNA 
folding," CABIOS, vol. 12, pp. 171--180, 1996. 
[4] M. Sipper, E. Sanchez, D. Mange, M. Tomassini, A. Perez-Uribe and A. Stauffer, 
"A Phylogenetic, Ontogenetic, and Epigenetic View of Bio-Inspired Hardware 
System," IEEE Transactions on Evolutionary Computation, vol. 1, pp. 83--97, 
1997. 
[5] E. Sanchez and M. Tomassini, "Towards Evolvable Hardware," Lecture Notes in 
Computer Science, vol. 1062, 1996. 
[6] J. Lienig, "A parallel genetic algorithm for performance-driven VLSI routing," 
Transactions on Evolutionary Computation, vol. 1, pp. 29--39, 1997. 
[7] A. A. Freitas, Data Mining and Knowledge Discovery with Eolutionary Algorithm, 
s.l. : Springer, 2002. 
[8] L. Tesfatsion, "Preferential partner selection in evolutionary labor markets: A 
study in agent-based computational economics," Proceedings of the seventh 
Annual Conference on Evolutionary Programming, pp. 15--24, 1998. 
[9] —, "Agent-based modeling of evolutionary economic systems," IEEE 
Transactions on Evolutionary Computation, vol. 5, pp. 437--441, 2001. 
[10] T. Baeck, H. Hammel and H.-P. Schwefel, "Evolutionary computation. Comments 
on the History and Current State," IEEE Transactions on Evolutionary 
Computation, vol. 1, 1997. 
[11] H.-P. Schwefel, Evolution and Optimum Seeking, New York : John Wiley and Sons, 
1995. 
[12] I. Rechenberg, Evolutions-strategie '94, Stuttgart : Fromman-Hozlboog Verlag, 
1994. 
[13] —, Evolutionstrategie: Optimierung Technisher Systeme nach Prinzipien des 
Biologischen Evolution, Stuttgart : Fromman-Hozlboog Verlag, 1973. 
[14] N. Hansen and A. Ostermeier, "Adapting arbitrary normal mutation distributions in 
evolution strategies: The covariance matrix adaptation," Proceedings of the 1996 
IEEE International Conference on Evolutionary Computation, pp. 312--317, 1996. 
[15] —, "Completely Derandomized Self-Adaptation in Evolution Strategies," 
Evolutionary Computation, vol. 9, no. 2, pp. 159--195, 2001. 
[16] N. Hansen, S. D. Mueller and P. Koumoutsakos, "Reducing the Time Complexity 
of the Derandomized Evolution Strategy with Covariance Matrix Adaptation 
(CMA-ES)," Evolutionary Computation, vol. 11, no. 1, pp. 1--18, 2003. 
[17] C. Igel, N. Hansen and S. Roth, "Covariance Matrix Adaptation for 
Multi-objective Optimization," Evolutionary Computation, vol. 15, no. 1, pp. 
1--28, 2007. 
[18] J. Kennedy and R. Eberhart, "Particle Swarm Optimization," Proceedings of IEEE 
International Conference on Neural Networks, pp. 1942--1948, 1995. 
[19] Y. Shi and R. Eberhart, "A Modified Particle Swarm Optimizer," Proceedings of 
IEEE Congress on Evolutionary Computation (CEC '98), pp. 69--73, 1998. 
[20] —, "Empirical study of particle swarm optimization," Proceedings of IEEE 
Congress on Evolutionary Computation (CEC '99), pp. 1945--1950, 1999. 
[21] P. N. Suganthan, N. Hansen, J. J. Liang, K. Deb, Y.-p. Chen, A. Auger and S. 
Tiwari, "Problem definitions and evaluation criteria for the CEC-2005: Special 
session on real-parameter optimization," NCLab NCL-TR-2005001, 2005. 
Particle Swarm Guided Evolution Strategy
Chang-Tai Hsieh
Dept. of Computer Science
National Chiao Tung University
HsinChu City 300, Taiwan
cthsieh@nclab.tw
Chih-Ming Chen
Dept. of Computer Science
National Chiao Tung University
HsinChu City 300, Taiwan
ccming@nclab.tw
Ying-ping Chen
Dept. of Computer Science
National Chiao Tung University
HsinChu City 300, Taiwan
ypchen@nclab.tw
ABSTRACT
Evolution strategy (ES) and particle swarm optimization
(PSO) are two of the most popular research topics for tack-
ling real-parameter optimization problems in evolutionary
computation. Both of them have strengths and weaknesses
for their diﬀerent search behaviors and methodologies. In
ES, mutation, as the main operator, tries to ﬁnd good so-
lutions around each individual. While in PSO, particles are
moving toward directions determined by certain global in-
formation, such as the global best particle. In order to lever-
age the specialties oﬀered by both sides to our advantage,
this paper combines the essential mechanism of ES and the
key concept of PSO to develop a new hybrid optimization
methodology, called particle swarm guided evolution strat-
egy. We introduce swarm intelligence to the ES mutation
framework to create a new mutation operator, called guided
mutation, and integrate the guided mutation operator into
ES. Numerical experiments are conducted on a set of bench-
mark functions, and the experimental results indicate that
PSGES is a promising optimization methodology as well as
an interesting research direction.
Categories and Subject Descriptors
G.1.6 [Numerical Analysis]: Optimization—Global opti-
mization; I.2.8 [Artificial Intelligence]: Problem Solving,
Control Methods, and Search—Heuristic methods
General Terms
Algorithms, Design, Experimentation
Keywords
PSGES, Swarm intelligence, Particle swarm optimization,
Evolution strategy, Global search, Local search
1. INTRODUCTION
Evolution strategy (ES) mimics natural mechanisms of
evolution and has been proven as a good solver for real-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
GECCO’07, July 7–11, 2007, London, England, United Kingdom.
Copyright 2007 ACM 978-1-59593-697-4/07/0007 ...$5.00.
parameter optimization problem. ES was proposed in 1960s
[18, 4], and its most important concept is to search the so-
lution space by adaptive mutation. Compared with other
optimization methods, ES is a very eﬃcient approach to
solve the non-linear model problems in engineering. Further-
more, the idea of self-adaptation is also ﬁrst introduced in
ES. Self-adaptation embeds the algorithmic parameters into
the representation and evolves the parameters together with
the decision variables. Such a design makes ES to converge
quickly and to ﬁnd solutions eﬃciently. In the recent year,
there have been a host of attempts to improve the mutation
mechanism of ES [6, 7, 12, 11], and many of these studies
successfully create advanced versions of evolution strategy
with excellent performance.
Particle swarm optimization (PSO) [8, 9], on the other
hand, is a relatively new branch of evolution computation.
It was proposed according to the swarming behaviors of in-
sects or animals. Each individual or particle decides its own
search direction to approach a better result of entirety with
the internal communication between one another. Since its
introduction, PSO has been widely adopted to solve prob-
lems in many disciplines for the simplicity of its key concept
as well as the implementation. PSO receives rapid recog-
nitions [14] and therefore draws lots of research attentions
focusing on the practical improvement, methodological en-
hancement, and theoretical understandings.
For the major mechanism, ES concentrates on how to gen-
erate new search points around the current individuals, and
thus, in a way, such an operation can be viewed as local
search. While PSO focuses on how to determine the next
move for each particle according to certain population-wise
information, such as the global best particle. Hence, this
process can be considered as global search. Based on the
thought to integrate the local search and the global search
capabilities from the two paradigms, we try to merge PSO
and ES in this study. Particularly, we employ the key con-
cept and mechanism of swarm intelligence to determine the
search directions, guiding the rotation of ES mutation el-
lipses, for global search, and use the regular ES operations
to conduct local search to ﬁnd promising solutions.
This paper is organized as follows. Section 2 brieﬂy intro-
duces evolution strategy, particle swarm optimization, and
closely related studies. Section 3 describes the proposed
method, PSGES, in detail, including the adoption of swarm
intelligence in mutation and the architecture of the frame-
work. Numerical experiments and the results are presented
in section 4, and ﬁnally, section 5 concludes this paper.
650
Mutate Mutation
Strengths
Calculate Rotation
Angles
Build Guiding
Rotation Matrix
Build Guiding
Vector
Build Guided
Mutation Vector
Mutate Decision
Variables
αg = Angle(pg,a)
Gm = Gs · N(0, 1)Gs = M · z
M =
n−1∏
p=1
n∏
q=p+1
Mpq(αjg )
pg
x′ = x + Gm
σi = σi · eτ
′·N(0,1)+τ ·Ni(0,1)
Figure 1: The flow of operations for guided mutation
where i = 1, 2, . . . , n, τ ′ ∝ 1/√2n can be interpreted
as a global learning rate, and τ ∝ 1/
p
2
√
n a local
one [18]. In this step, we reserve the self-adaptation
mechanism for the mutation strength.
• Step 2: Compute the rotation angle vector αg between
the individual vector a and the current global best
solution pg by repeatedly applying Equation (5).
αg = Angle(pg,a) , (7)
where αg = (α1g , α2g , . . . , αn(n−1)/2g ), αig ∈ [−π, π],
i = 1, 2, . . . , n(n− 1)/2.
• Step 3: Construct the guiding rotation matrix M .
M =
n−1Y
p=1
nY
q=p+1
Mpq(αjg ) , (8)
where j = 1
2
(2n−p)(p+1)−2n+q [17]. Rotation ma-
trix Mpq(αjg ) forms a n×n identity matrix except that
mpp = mqq = cos(αjg ) and mpq = −mpq = − sin(αjg ).
By multiplying all the rotation matrices, we can con-
struct the guiding matrix eﬃciently.
• Step 4: Construct the guiding vector.
Gs = M · z , (9)
where z = (z1, . . . , zn), zi ∼ σi ·N(0, 1) denotes a ran-
dom variable drawn from a Gaussian distribution of
which the mean is zero and the standard deviation
corresponds to the step-size of each dimension.
• Step 5: Compute the guided mutation vector Gm.
Gm = Gs · N(0, 1) , (10)
where N(0, 1) is a n× 1 vector and each dimension in
the vector is a random variable drawn from a Gaussian
distribution of which the mean is zero and standard
deviation is one.
• Step 6: Mutate the decision variable x by using the
guided mutation vector Gm.
x′ = x + Gm . (11)
These steps will turn the mutation ellipse of each individual
toward the position of pg with controlled randomness. Given
the current global best pg, we can now determine how the
rotation of mutation ellipses is done without the assistance
of n(n − 1)/2 more strategy parameters. To illustrate the
guided mutation operator, Figure 2 shows the operations of
PSO and how the swarm intelligence mechanism inﬂuences
the ES mutation operator.
The essential advantage of correlated mutation that the
mutation ellipse is independent of the coordinates of search
space is preserved in guided mutation, while the n(n− 1)/2
strategy parameters, self-adaptive or not, are eliminated.
3.2 Guided Evolution Strategy
In order to integrate the concept and mechanism of swarm
intelligence into ES for deciding the mutation ellipse rota-
tion, designing the guided mutation operator is one part of
the work. The other part is to modify the ﬂow of ES such
that the necessary information for the operations, such as
pg, can be prepared and updated at each iteration. Since a
population of size more than one is needed for determine the
search direction in swarm intelligence, we employ (μ+λ)-ES
as the base framework for PSGES. ES with a population of
size one, such as (1, λ)-ES and (1 + λ)-ES, is not applica-
ble for being modiﬁed to accommodate the new mechanism.
Moreover, the individual representation now consists of only
two parts: the decision variables and the mutation strengths:
I = (x;σ) = (x1, . . . , xn;σ1, . . . , σnσ ) , (12)
where x is the decision variable vector and σ is the mutation
strength vector. In PSGES, we always have an independent
mutation strength for each dimension, i.e., nσ = n. Rotation
652
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(a) Generation 2
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(b) Generation 4
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(c) Generation 6
Figure 3: Uncorrelated mutation.
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(a) Generation 2
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(b) Generation 4
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(c) Generation 6
Figure 4: Correlated mutation with β = 5◦.
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(a) Generation 2
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(b) Generation 4
-150 -100 -50 0 50 100 150
-150
-100
-50
0
50
100
150
(c) Generation 6
Figure 5: Correlated mutation with β = 360◦.
654
ES [5] PLES [5] PSGES LR-CMA-ES [1] IPOP-CMA-ES [2]
f1 9.86e-9 (5) 8.40e-9 (4) 0.00e+0 (1) 5.14e-9 (2) 5.20e-9 (3)
f2 2.90e-6 (5) 9.65e-9 (4) 0.00e+0 (1) 5.31e-9 (3) 4.70e-9 (2)
f3 3.52e+5 (5) 1.18e+5 (4) 3.17e+0 (3) 4.94e-9 (1) 5.60e-9 (2)
f4 4.13e+3 (3) 6.03e+3 (4) 1.36e-14 (1) 1.79e+6 (5) 5.02e-9 (2)
f5 1.36e+3 (5) 9.05e+2 (4) 1.05e+2 (3) 6.59e-9 (2) 6.58e-9 (1)
f6 7.49e+1 (5) 3.05e+1 (4) 1.59e-1 (3) 5.41e-9 (2) 4.87e-9 (1)
f7 1.18e+0 (4) 4.09e+0 (5) 7.39e-3 (3) 4.91e-9 (2) 3.31e-9 (1)
f8 2.03e+1 (3) 2.03e+1 (4) 2.09e+1 (5) 2.00e+1 (2) 2.00e+1 (1)
f9 4.48e+1 (4) 1.67e+1 (3) 3.46e+0 (2) 4.49e+1 (5) 2.39e-1 (1)
f10 1.03e+2 (5) 2.56e+1 (3) 1.46e+1 (2) 4.08e+1 (4) 7.96e-2 (1)
f11 8.91e+0 (3) 9.52e+0 (4) 1.35e+1 (5) 3.65e+0 (2) 9.34e-1 (1)
f12 4.40e+3 (5) 3.25e+3 (4) 3.60e+2 (3) 2.09e+2 (2) 2.93e+1 (1)
f13 9.59e+0 (5) 8.66e+0 (4) 8.21e-1 (3) 4.94e-1 (1) 6.96e-1 (2)
f14 3.53e+0 (2) 4.13e+0 (4) 5.00e+0 (5) 4.01e+0 (3) 3.01e+0 (1)
f15 5.71e+2 (5) 3.79e+2 (4) 3.26e+2 (3) 2.11e+2 (1) 2.28e+2 (2)
f16 4.38e+2 (4) 1.46e+2 (2) 2.01e+2 (3) 1.05e+2 (1) 9.31e+4 (5)
f17 4.49e+2 (4) 1.95e+2 (2) 3.03e+2 (3) 5.49e+2 (5) 1.23e+2 (1)
f18 1.14e+3 (5) 1.01e+3 (4) 7.15e+2 (3) 4.97e+2 (2) 3.32e+2 (1)
f19 1.12e+3 (5) 1.00e+3 (4) 6.69e+2 (3) 5.16e+2 (2) 3.26e+2 (1)
f20 1.12e+3 (5) 9.98e+2 (4) 7.05e+2 (3) 4.42e+2 (2) 3.00e+2 (1)
f21 1.31e+3 (5) 1.07e+3 (4) 8.89e+2 (3) 4.04e+2 (1) 5.00e+2 (2)
f22 9.29e+2 (5) 8.80e+2 (4) 8.11e+2 (3) 7.04e+2 (1) 7.29e+2 (2)
f23 1.34e+3 (5) 1.11e+3 (4) 1.08e+3 (3) 7.91e+2 (2) 5.59e+2 (1)
f24 1.19e+3 (5) 2.82e+2 (2) 4.19e+2 (3) 8.65e+2 (4) 2.00e+2 (1)
f25 4.15e+2 (2) 6.92e+2 (5) 4.15e+2 (3) 4.42e+2 (4) 3.74e+2 (1)
Rank 4.36 (109/25) 3.76 (94/25) 2.92 (73/25) 2.44 (61/25) 1.52 (38/25)
Table 2: The mean best objective function error values for 105 evaluations in 25 runs for ES, PLES, LR-
CMA-ES, IPOP-CMA-ES, and PSGES on the IEEE CEC 2005 benchmark. The values in the parentheses
are the ranks.
of problems which are “solved” according to the deﬁnition
provided in the benchmark. Table 3 lists the number of
solved test functions for the evolutionary algorithms, and
the numbers in the parentheses are the ranks. As we can
see in Table 3, for the unimodal functions, PSGES can solve
f1, f2, and f4. For the basic functions, PSGES can solve
f6, f7, f9, f12, and f13. For the total number of solved
problems, PSGES can solve 8 out of the 25 test functions.
PSGES is ranked top 3 in the comparison and is inferior only
to IPOP-CMA-ES and DMS-L-PSO, which are capable of
solving 11 and 9 test functions, respectively.
In addition, it might be interesting to compare the results
of PSGES to that of the most advanced evolution strategies,
IPOP-CMA-ES [2] and LR-CMA-ES [1]. Before conducting
the numerical experiments, we expected both IPOP-CMA-
ES and LR-CMA-ES to outperform PSGES because they
have been proven able to solve real-parameter optimization
problems eﬀectively and eﬃciently. By analyzing the com-
position of the solved functions, we can ﬁnd that IPOP-
CMA-ES does outperform PSGES. However, LR-CMA-ES
performs better on the unimodal functions and PSGES per-
forms better on the basic multimodal functions. Further-
more, such a condition also holds when we analyze the re-
sults for other algorithms in Table 3. PSGES solved the
least number, 3, of the unimodal functions but the second
most number, 5, of the basic multimodal functions. Accord-
ing to this situation, we might speculate that PSGES may
have a good global search mechanisms and need a further
enhancement for its local search facility.
5. SUMMARY AND CONCLUSIONS
This paper ﬁrst gave a brief background of evolution strat-
egy (ES) and particle swarm optimization (PSO) to present
the fundamentals for this study. To retain the capability of
rotating mutation ellipses and eliminate the need of a large
number of rotation angles in ES, we integrated the concept
and mechanism of swarm intelligence into ES mutation as
well as ES work-ﬂow for determining the search direction of
mutation ellipses. By visualizing the search behavior of the
guided mutation operator and conducting numerical exper-
iments on a set of benchmark functions, we demonstrated
that PSGES should be an interesting research topic.
PSGES considers swarm intelligence as a global search op-
erator and ES mutation as a local search facility. By combin-
ing the strengths coming from the two diﬀerent methodolo-
gies, PSGES outperforms the classic ES and performs as well
as the most advanced ES in the experiments. However, in
ES, there also exist certain global search mechanisms, such
as recombination, as well as in PSO, there exist some local
search operations, such as the randomness in particle move-
ment. To bring good, working mechanisms from ES and
PSO together, we need to further understand the strengths
and weaknesses of all their components to avoid possible de-
structive side-eﬀects caused by the conﬂicting components.
Finally, the framework of PSGES may reveal a paradigm
for integrating diﬀerent methodologies together by analyzing
the individual capabilities and assembling the components
in a proper manner. Along this line may there be a great
possibility to create more advanced evolutionary algorithms.
656
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                            96 年 8 月 14 日 
報告人姓名 陳宏偉 服務機構及職稱 
交通大學資訊科學與工程所 
博士生 
     時間
會議 
     地點 
96 年 7 月 7 日至 7 月 11 日
英國倫敦 
本會核定
補助文號
 
會議名稱 
 (中文) 基因暨演化計算學術會議 
 (英文) Genetic and Evolutionary Computation Conference 
發表論 
文題目 
 (中文) 將容錯機制導入 XCS 分類系統中 
 (英文) Introducing fault tolerance to XCS 
報告內容應包括下列各項： 
一、 參加會議經過 
 今年的基因暨演化計算學術會議於 7 月 7 日至 7 月 11 日在英國倫敦 (London, UK) 的 
University College London (UCL) 舉行。 
 整個會議的議程包括精彩的 Keynote Speaker 講演、十數場的 Tutorials、Workshops、數項演化計算
方法成果競賽、以及論文之口頭報告和海報展。其內容之豐富、主題之精彩，實為基因暨演化計算界最
高水準之國際學術會議。 
 本屆會議之論文接受比例為百分之四十六左右，為歷屆罕見之低接受率。雖然本會議組織章程中有
規定，接受比例不得超過百分之五十，但除上屆外，過去皆接近百分之五十。本屆接受比例下降三個百
分點，足顯演化計算領域現正蓬勃發展，受到各界之重視。 
 
二、 與會心得 
 吾人於到場聽取各研究相關論文之口頭報告，增進對演化計算各領域，獲益良多。由於此會議之審
稿方式為 double blind (即 author 與 reviewer 皆不知對方)，不但嚴格於絕大多數之學術會議，事實上更
甚於一般之 single blind 期刊審稿。因此，如原先所預期之，大會所選出的論文品質都相當優良，演議者
之缺席率非常低，各講者也都盡心盡力，做出最好的表現。 
 吾人所屬之實驗室今年在此會議，發表有三篇口頭報告論文，以及一篇海報展論文，成果頗為豐碩。
由於本實驗室四篇論文的研究主題，在演化計算領域中，分屬不同的子領域，因此，不但在各論文之子
領域中，與國際級之相關學者切磋最先進的研究議題，同時本實驗室研究方向的多樣性亦吸引許多研究
同儕之目光，收穫豐盛。 
 
三、 考察參觀活動(無是項活動者省略) 
 
四、 建議 
 能夠參與這種國際大型且優良的研討會，確實令人對學術研究感到前途光明。整個演化計算領域的
研究主題不斷地前進、研究內容不斷地提升，而台灣學者被制度面壓迫至專心投稿研究進度慢如牛車的
期刊，甚少機會直接和國際級的頂尖學者面對面談論研究議題，實為台灣學術發展之重要問題。因此，
若研究之評鑑制度能放棄唯期刊獨尊之思維，並配合更優渥之學術會議出席制度，方能讓台灣學界在全
世界的能見度提升、從而全面性地增進台灣的研究水準與學術國力。 
 
五、 攜回資料名稱及內容 
 攜回資料有研討會論文集上下兩冊與相同內容之光碟一片。 
 
六、其他 
 
附件三
 
