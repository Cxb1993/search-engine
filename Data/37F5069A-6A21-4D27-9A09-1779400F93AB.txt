I目錄
目錄 ................................................................................................................................I
行政院國家科學委員會專題研究計畫成果 .............................................................. II
計畫參與人員：楊竣崴、謝依蓓 .............................................................................. II
中文摘要 ...................................................................................................................... II
Abstract......................................................................................................................... II
1. Introduction.............................................................................................................1
2. Background.............................................................................................................2
3. Proposed Edge Detection Methodology.................................................................3
4. Experimental Results ..............................................................................................5
5. Conclusions.............................................................................................................9
6. References...............................................................................................................9
7. 計畫成果自評 ........................................................................................................9
11. Introduction
Distinguishing the objects from their backgrounds is a critical task in the widespread applications of
computer vision and pattern recognition. To attain the complete information of edges in an image is
all it takes to generate the sharp profile of the objects. Thus, edge detection is one of the
fundamental issues of image processing. As the name implies, edge detection is meant to extract
correct edge lines with accurate orientation in a tested image. Many methods of edge detection have
been proposed during the last two decades. Low error rate is one of common performance indicators
for edge detectors. It is significant for edge detectors to regard edges occurring in the image as edge
points and consider non-edges as spurious responses being as less as possible. Besides, making the
distance between edge pixels as found by the detector and the actual fact of itself minimum is also
another performance indicator.
Several well-known methods for edge detection, such as the Sobel operator and Laplacian
operator [1], use convolution masks to take the gradient of an image, and edges with dramatic
changes of the gray level will be detected. However, while the gray level variation in the image is
smooth, extracted edges are usually thicker. Canny [2] improves Sobel method and utilizes the
information obtained by analyzing the vertical and horizontal edge intensities of the pixel to resolve
the direction angle of the edge point, and then four possible edge directions are exploited to
approximate the edge angle. Finally, by applying the non-maxima suppression (NMS) [2] to edge
directions and edge intensities, the edge image can be attained. Being sensitive to noise, anisotropy
and thick lines are some common problems of those edge detectors as mentioned above. Kang and
Wang [3] draw on the maximizing objective function with four different directions of edge to
estimate the edge intensities such that the edge map and direction map will be obtained. Afterwards
an appropriate threshold and NMS are utilized to determine edge points. Two artificial techniques,
fuzzy logics and neural networks (NN), are applied for edge detection in recent two decades. Some
approaches based on fuzzy rules are proposed by [4, 5], while those methods require rather large
rule sets in comparison with another simpler fuzzy method [6]. Competitive fuzzy edge detection
(CFED) proposed by Liang and Looney [7] splits edge categories into six temples according to a
fuzzy classifier. Nevertheless, speckles in some more detailed regions of an image are usually
extracted by CFED. Rakesh et al. [8] use statistics to get an estimated threshold for the whole image
and obtain a rough sketch of edges. Subsequently, the edge points with largest edge intensity will be
determined by NMS. However, this method is a time-consuming process, and some double edges
appear.
In this paper, a novel edge detection method based on sliding window technique is proposed.
The key concept is that the sliding window size is dynamic. Moreover, the proposed method also
can generate edge images with different size of width.
The remainder of this paper is organized as follows. Several edge detectors, such as the Sobel
method, Canny method and Kang and Wang’s method will be described in the following section.
Section 3 proposes our method. Numerous example experiments will be illustrated and analyzed in
3patterns to divide the coefficients in the 3×3 mask into two pixel set (S0 and S1). Subsequently, a
predefined objective function is applied to calculate four edge intensities of each pixel
corresponding to those possible directions. The edge map and direction map are obtained by
selecting the maximum edge intensity of each pixel and the direction of the maximum edge
intensity. Finally, edge points are extracted by performing NMS on the edge and direction maps.
More details about this method will be mentioned in the literature [3].
3×3 mask
Fig. 3. The 3×3 mask and four possible edge directions in Kang and Wang’s method.
3. Proposed Edge Detection Methodology
The basic idea of our method is using sliding window with dynamic window size to detect
edge maps from eight different directions. In each of these eight directional edge maps, edges are
selected by checking whether the length of its consecutive edge candidates is no less than a
predetermined threshold. Finally, the basic logical operation, OR, is performed for those edges
generated from different directional edge maps to determine the edge image. The concept of the
sliding window of our method is shown in the following figure.
Fig. 4. The concept diagram of the proposed method.
5candidate shall be erased. This action can not only reduce the noises but also make edges
thinner in the final edge image.
(3) Along the sliding direction, any connected edge candidates with length exceeding two, all
the candidates will be disregarded except for the candidates locating at two ends. This
process also facilitates edges much thinner in the final edge image.
Afterwards, for any connected edge candidates (i.e., not only along the sliding direction), the
connected length equivalent to or exceeding a threshold TS will be regarded as the rough directional
edges, otherwise the connected edge candidates will be ignored. Fig. 5(d) depicts an example of
directional edge rough. So far, each edge map can be extracted a directional edge rough.
Finally, by doing the logical operation, OR, to those directional edge roughs (such as left to
right directional edge map and top to bottom directional edge map), the final edge image will be
obtained.
4. Experimental Results
In order to measure the performance of our proposed method and others (including the Sobel
method, Canny method and Kang and Wang’s method) more objectively, two artificial images are
considered. Besides, several examples for gray level and natural images are also utilized to compare
our method with other methods. Note that the optimal threshold values applied to the Sobel and
Canny methods are selected from many experiments, and those values vary according to different
edge detecting images. Moreover, the parameters used for Kang and Wang’s method are referred to
[3], and those values (w1 = 90, w2 = 40 and T = 50) are recommended by the authors. In addition, all
the edge images in our method presented in this section are acquired by doing logical operation, OR,
to edge roughs generated from left to right directional edge map and edges created from top to
bottom directional edge map. Several parameters (TF = 2, TL = 2, T = 160 and TS = 3) are obtained
by a series of trial and error, and those values are invariable even though detected images are
diverse.
Example 1: Fig. 6(a) illustrates a synthetic image with line width only one pixel. Fig. 6(d) is
the line detection result of our proposed method. It is clear that the proposed method can extract all
the lines in Fig. 6(a) while some double edges appear in the Sobel method and Canny method, as
shown in Figs. 6(b) and (c), respectively. In Kang and Wang’s edge detector, the extracted result (as
indicated in Fig. 6(e)) shows that the corners of the square are disconnected obviously. Note that
while detecting the diagonal lines the proposed method can extract thinner lines than other methods.
(a) (b) (c)
7the authors. In those images, it is clear that our proposed method can extract thinner edges in the
cameraman’s back.
(a) (b) (c)
(d) (e)
Fig. 8. Comparison with different methods for“cameraman”image: (a) the original image; (b) our method; (c) Sobel
method; (d) Canny method; (e) Kang and Wang’s method.
Example 4: Consider the “peppers”image (Fig. 9(a)) which is with size 256×256 and 8-bit
gray level. Thresholds for the proposed method and Kang and Wang’s method are the same as set in
Fig. 8(b) and 8(e), respectively. The optimal threshold value T = 170 for the Sobel and Canny
methods is obtained by a serious of parameter tuning. It is obvious that the Sobel method still
extracts thicker edges and other methods can detect almost thin edges.
(a) (b) (c)
95. Conclusions
In this paper, a novel edge detection method based on sliding window technique is proposed. The
size of the sliding window is floatingly applied on an image to generate eight directional edge maps
for eight different directions. Subsequently, some pixels in those edge maps will be defined as edge
candidates, and some connective possible edges will be regarded as edges. Therefore, we can obtain
eight directional edge roughs. By doing OR operation to some different edge roughs, the final edge
image is generated. Relevant experiments show that the proposed method can detect much thinner
edges than Sobel method, and extract as thinner edges as Canny method. However, some double
edges will be detected by Canny method. Besides, while comparing to Kang and Wang’s method,
the proposed approach also can extract thinner edges, especially in the diagonal edges.
6. References
[1] Efford, N.: Digital Image Processing. Addison-Wesley, Reading, MA (2000)
[2] Canny, J. F.: A computational approach to edge detection. IEEE Transactions on Pattern
Analysis and Machine Intelligence, Vol. 8. Issue 6 (1986) 679-698
[3] Kang, C.-C., Wang, W.-J.: A novel edge detection method based on the maximizing objective
function. Pattern Recognition. Vol. 40. Issue 2 (2007) 609-618
[4] Law, T., Itoh, H., Seki, H.: Image filtering, edge detection, and edge tracing using fuzzy
reasoning. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 18. Issue 5
(1996) 481-491
[5] Russo, F., Ramponi, G.: Fuzzy operator for sharpening of noisy images. IEE Electronics Letters,
Vol. 28. Issue 18 (1992) 1715-1717
[6] Looney, C.G.: Nonlinear rule-based convolution for refocusing. Real-Time Imaging, Vol. 6.
Issue 1 (2002) 27-37
[7] Liang, L.R., Looney, C.G.: Competitive fuzzy edge detection. Applied Soft Computing, Vol. 3.
Issue 2 (2003) 123-137
[8] Rakesh, R.R., Chaudhuri, P. Murthy, C.A.: Thresholding in edge detection: a statistical
approach. IEEE Transactions on Image Processing, Vol. 13. Issue 7 (2004) 927-936
7. 計畫成果自評
本篇研究主要是利用可調整大小之滑動視窗技術，分別從影像的不同方向去偵測出邊緣
圖，並從中挑選出符合邊緣特性的候選邊緣，再藉由定義邊緣特徵準則來減少斑點發生，而
最後的邊緣影像，則對具方向性之邊緣輪廓圖，執行簡單的 OR 運算便可產生。經由實驗的
結果得知，本研究所提出的方法，可以有效偵測出細膩的影像邊緣，並減少邊緣誤判的情況
發生。除此之外，在偵測邊緣過程中，所產生的具方向性邊緣圖，亦可作為未來進一步研究
之用：
參加ICALIP 2008 國際音頻、語言與影像處理研討會心得報告
沈肇基教授
中興大學資訊管理學系
2008年國際音頻、語言與影像處理研討會(2008 International Conference on
Audio, Language and Image Processing)，係於2008年7月7日至7月9日在中國上海
市(Shanghai, China)舉行，會議地點是在上海市徐匯區的華亭賓館(Hua-Ting Hotel
& Tower)。本次會議主辦單位為上海大學(Shanghai University)，協辦單位有IEEE
上海部門、 IET上海部門、清華大學 (Tsinghua University)與復旦大學 (Fudan
University)，開幕儀式則由上海大學副校長兼教授Min Wang、上海大學教授
Wanggen Wan、上海大學教授Shuozhong Wang與上海大學教授Huiming Chen等人
共同主持。本次研討會吸引了來自世界各地的音頻、語言與影像處理專家學者超
過三百多人與會，根據大會統計，總共有673篇論文投稿，只有341篇論文被接受
於大會展示報告與在IEEE發表，所以本次會議的論文接受度低於51%，詳細的論
文收錄於大會發行之論文集與光碟中。
本次研討會有來自世界33個不同國家的專家學者親臨出席會議。每天會議從
上午八點三十分開始安排到下午五點整左右，第一天上午由三位重要的學者演講
其專業領域之研究現況，下午分二個時段進行論文發表。第二天上午安排四位重
要的學者演講其研究的發展現況，下午則同時有四個時段的海報展出與二個時段
進行論文發表。最後一天分四個時段進行論文發表，每一時段同時有不同的主題
分別在四個不同會議室發表。與會期間大會也很用心地在晚間安排了文化參觀活
動，促進與會人員之間的交流，讓我們漫步於濱江大道參觀了上海的黃埔江以及
外灘的夜景，與看了一場中國五千年歷史服裝秀，模特兒分別展示了唐宋元明清
的服飾。
至於本次會議論文發表內容和主題則包括了五個不同類別主題，分別是：
1. Audio and Music Processing.
2. Hearing and Auditory Systems.
3. Language and Speech Processing.
4. Image Processing and Virtual Reality.
5. Software and Hardware Implementation.
所發表的主題均為各研究人員最新之研究成果，在影像處理方面，整個會議的主
題較偏重於提升影像品質與影像重建以及影像切割之研究，在音頻方面，則是音
頻的壓縮與編碼為主。總之，此會議相當值得國內音頻與影像處理領域之研究者
參與。
本人參加本屆國際音頻、語言與影像處理研討會(ICALIP 2008)，特別帶了
圖二 ICALIP 2008大會開幕當天會場現場照片
圖三 ICALIP 2008大會開幕當天會場現場照片
圖六 參觀中國五千年歷史服裝秀之表演
本人此次參加國際音頻、語言與影像處理研討會(ICALIP 2008)是被安排在7
月9日星期三下午13:10~15:25的Session W-L12當主持人，位於Dynasty room 4。
該場會議總共有九篇論文將發表，但由於是研討會的最後一天，所以在報到時只
有四位學生前來。會前與準備報告的學生們交談後，發現他們都很珍惜此次發表
論文的機會，無疑也是現今各國大學推動國際化的具體成果。因此值得建議本校
多鼓勵碩士生和大學生出國學習與參與會議，不僅學生可以增加見識且也可提昇
國際觀，還可增加學校在國際上的知名度。圖七為本人在ICALIP 2008大會主持
報告現場照片，圖八是本人主持會議的議程。除此，本人亦聽了此四位學生的論
文發表，詳細的內容整理如下：
1. “Image Compressed Sensing Based on DT-CWT”
此篇論文是由中國燕山大學(Yanshan)一群研究生所發表，主要內容為簡單介紹
Compressed sensing(CS)與dual tree complex wavelet(DT-CWT)，且針對自然影像
而言，他們成功的應用基於DT-CWT的CS系統結合total variation(TV)與門檻值設
定完成目的。因為位移的不變性與DT-CWT增加方向的選擇性，因此由他們的方
法獲得的影像品質都比其他頻率域轉換演算法所獲得的品質高。
2. “Sparse MRI Reconstruction via Different Norms Based on Total Variation”
此篇論文也是由中國燕山大學(Yanshan)的研究生所發表，發現與上一位報告的
同學都是同一間研究室的學生，所以主題都是相同的，主要的研究方向是針對提
圖八 本人主持會議的議程
攜回資料：
本人在此次研討會中所攜回之資料包括：
1. 研討會會議議程及論文摘要集二本(如圖九)。
2. 研討會會議論文全文光碟一片(如圖十)。
3. Map of ICALIP 2008會議地點(如圖十一)。
圖十一 ICALIP 2008會議地點地圖
rule between the original image and watermark is tried
to find out according to the hiding method that
proposed by the expert. The frequency domain
coefficient that are selected on the original image are
modified to replace the original coefficient so that
watermarked image can be derived from the inverse
transform.
Original image
Transform
Selection of
coefficients
Random sequence
generator
Coefficients
modification
Watermarked image
Watermarking key
Inverse
transform
(a) Watermark embedding
(b) Watermark examination
Figure 1. Flowchart of watermark embedding
and examination (cited from [12]).
The watermark that is generated by the default
testing key is calculated repeatedly. The frequency
domain coefficient of detection image are selected
through the rules set by the embedding approach, and
selected frequency coefficient and watermark are
calculated to obtain the individual similarity values,
which form a similarity diagram. The similarity
diagram is used for proving the existence and
uniqueness of watermark. Different keys can get a
different similarity values respectively. Among them,
the similarity value of the correct key will be displayed
on the top spot, and the rest of the similarity value will
be near 0. If the highest similarity value S exceeds the
threshold T, and then there is embedded watermark in
the detection image. The interested reader is
encouraged to get the details from [11, 12].
In this paper, another technique called Sobel edge
detection [7] uses two diferent direction’s masks 3×3, 
as shown in Figure 2. SH and SV take convolution
respectively on the gray level image, and then use the
formula (1) below to calculate the edge intensity in
different directions. At last, whether a pixel is on the
edge can be determined by setting a threshold value
and comparing edge intensity values. If it’s on the edge,
Z4 is on the edge point.
W
Figure 4. Flowchart of examining
and restoring original image.
Step 1.The receiver get Wand D, and then use
watermarked detection technique to do the
watermark test of Wand get a similarity
diagram. If the highest similarity value S in the
diagram exceeds the threshold value we set,
there is watermark in W. If not, there is no
watermark in W.
Step 2.After verifying watermarked image, the
validation of the image origin is proved, and
then add the pixel value of D and W
correspondingly, such that I is restored, namely
that I(i, j)= W(i, j)+D(i, j), where (i, j)
represents the position of image pixel.
4. Experimental Results
In this paper, the experiment is conducted by the
grey-level images of Lena, Baboon and Peppers, with
the size of 256×256, as shown in Figure 5. (a), (b) and
(c). Aimed on the watermark with the size of 208×208
that is generated by the 200 different keys, this
experiment utilizes the Robust Associative
Watermarking Technique proposed by Shen and Hsu
[12] to embed them into Lena, Baboon and Peppers
respectively, thereby getting 200 embedded images, so
the total number of the embedded images is 600.
Experimental parameter cited from [12]. First, the new
image of embedded watermark is derived from these
600 images by using the approach in this paper, and
then is testified by watermark. The result from Table 1.
shows that the watermark is verified correctly after
being modified by our approach. There are two
standards for the judgement: False-negative errors and
False-positive errors. False-negative errors means
there is embedded watermark in the image, but this
cannot be successfully examined. False-positive errors
means there’s no embedded watermark in the image,
but be judged mistakenly there is.
The random sequence that is generated by the 100th
key is embedded into Lena, Baboon and Peppers, and
the embedded image is obtained by the approach of the
paper, as shown in Figure 5. (d), (e) and (f). The
watermark that is generated by the 200 different keys
mentioned above is examined to obtain the similarity
diagram, as shown on Figure 6. (a), (b) and (c). It
shows that, in the 100th position, the similarity values
of the three images are the highest, other values are
near 0. For D, it is compressed by arithmetic code, and
4.41 kb, 8.15 kb and 4.27 kb are obtained for Lena,
Baboon, and Peppers, respectively.
(a) Lena (d) Watermarked Lena
(b) Baboon (e) Watermarked Baboon
(c) Peppers (f) Watermarked Peppers
Figure 5. The contrast between original image
and watermarked image.
