 1
行政院國家科學委員會專題研究計畫成果報告 
 
用於歌聲合成之歌唱共鳴產生之研究 
Singing Resonance Generation for Singing Voice Synthesis 
 
 計畫編號：NSC 100-2221-E-011-157 
 執行期限：100 年 8 月 1 日至 101 年 7 月 31 日 
 主持人：古鴻炎 國立台灣科技大學資訊工程系 
 計畫參與人員：張家維、簡延庭、張世穎、林祐靖、陳彥華 
e-mail: guhy@mail.ntust.edu.tw 
 
一、中文摘要 
 
我們藉由此計畫重新製做了一個基於
HMM頻譜模型的華語歌聲合成系統，而HMM
模型之建造必需考慮不同文脈與音域組合，如
此才能合成出一致性的歌聲音色。此外，我們
由實驗發現，以歌唱方式發音的語料來訓練
HMM，即可讓合成出的歌聲具有歌唱共鳴之
特性。關於歌唱共鳴的產生模型，我們基於高
斯混合模型(GMM)之頻譜對映機制，研究了兩
種轉換的程序，而經由聽測實驗發現，第一種
轉換程序(稱為基本轉換法)的效能稍好一些，
不僅可把 A 的音色轉換成近似 B 的，且可以
轉換出共鳴的特性，但是音質則仍需改進。 
 
關鍵詞：歌聲合成、歌唱共鳴、頻譜包絡、高
斯混合模型 
 
 
ABSTRACT 
 
In this project, an HMM (hidden Markov 
model) based Mandarin singing-voice synthesis 
system is implemented. Notice that the context of 
a lyric vowel and the pitch height of the note 
sung must be taken into account when training 
the HMM models in order to have consistent 
timbre among the synthesized lyric syllables. To 
have the characteristic of singing resonance kept 
in the synthesized singing voice, it is enough that 
the corpus for training the HMM is collected by 
recording a real singer’s singing voice signals. 
As to the model for singing resonance generation, 
the GMM based conversion mechanism for 
spectrum mapping is adopted here. We have 
developed two conversion procedures. The first 
conversion procedure (called the basic 
conversion method) is found to be slightly better 
in performance. It can not only convert the 
timbre of speaker A into a similar timbre of 
speaker B, but also keep the characteristic of 
singing resonance in the synthesized singing 
voice. Nevertheless, the voice quality still needs 
to be improved. 
 
Keywords: singing voice synthesis, singing 
resonance, spectral envelope, GMM 
 
 
二、緣由與目的 
 
電腦歌聲合成技術可用於製作歌唱教導
之軟體，以教導許多喜愛唱歌但是看不懂樂譜
的人士。作詞、作曲的專業人士，也可藉由歌
聲合成軟體快速地聆聽、評估自已的作品，以
作必要的修改。更進一步當歌聲合成技術日漸
成熟時，電腦虛擬歌手的想法，將變成實際可
行。近年來日本 Yamaha 公司製作的歌聲合成
軟體 Vocaloid [1]，就塑造了一個虛擬歌手，稱
為”初音”。 
過去，我們在歌聲合成領域研究了好幾
年，目前也已經發展出一個即時的華語歌聲合
成系統，可合成出自然度不錯的歌聲。不過，
當我們仔細聽前述系統合成出的歌聲時，會查
覺到歌聲的音質仍然存在一個不能忽視的缺
點，那就是歌聲信號太過於清晰，而缺少專業
歌手的歌聲中所呈現出的共鳴(resonance)特
性，一個重要原因是，過去我們都是使用以說
話方式發音的華語音節，去分析、求得信號合
成模型的參數。 
因此，在本次計畫裡，我們把焦點放在
歌唱共鳴上，去分析歌唱共鳴在聲學上所呈現
出的特性，然後研究建立歌唱共鳴的產生模
型，以便讓我們的系統能夠合成出自然、且具
有共鳴特性的歌聲。與歌唱共鳴相關的幾個問
題是：(a)使用以歌唱方式發音的華語音節來分
析出信號模型的參數，是否就可以讓合成出的
 3
4.1 歌聲合成系統主流程—訓練階段 
 
在本計畫中我們改採 HMM 頻譜模型來
重新製作一個華語歌聲合成系統，此系統在訓
練階段的主要處理流程如圖 2 所示。我們先將
歌聲資料庫的歌曲一句一句切成樂句作標
音、切音，之後使用 STRAIGHT[18]程式對每
一個切割出的音節作分析以求得頻譜包絡資
料及音高資料；然後使用先前發展的離散倒頻
譜係數之估計程式，去求取歌聲音節各音框的
DCC 頻譜係數，由於使用 STRAIGHT 來分析
出頻譜包絡曲線，所以在 DCC 係數的計算過
程，我們調整了頻譜峰值之挑選方式；接著，
我們依據所標記的音標資訊，將 DCC 係數分
類為聲母、韻母兩部分，並且進一步依據文脈
及音高資訊將聲、韻母作細分類；之後對各個
聲、韻母細類作 HMM 訓練。 
Start 
標音、切音 
STRAIGHT 分析 
歌聲 
資料庫 
頻譜包絡資料 
音高資料 DCC 係數估計 
聲、韻母分類 
聲、韻母細類 
HMM 訓練 
聲、韻母細分類 
(依文脈、音高) 
End 
 
圖 2 訓練階段之主流程 
 
 
4.2 歌聲合成系統主流程—合成階段 
 
    合成階段的主要處理流程如圖 3 所示。首
先對一個讀入的歌譜作文字剖析，以取得各個
歌詞的音高資料及音長資料。接著，將歌譜的
各個歌詞轉成聲、韻母之單位，再依據其文脈
及音高資料從HMM模型集合中選取出最搭配
的 HMM 模型。然後，根據音長資料及狀態駐
留參數，來決定一個聲、韻母 HMM 模型內各
狀態應該指派的音框數量，並且使用 HMM 各
狀態上的平均 DCC 向量去產生出各音框的
DCC 係數。求得 DCC 係數之後，再將各音框
的 DCC 係數帶入 GMM 音色轉換模組，以轉
換出具有另一人音色的 DCC 係數。接著，依
據音高資料去訂定各音框的音高數值；最後將
各音框的 DCC 係數、音高軌跡等資訊，帶入
HNM 模組去合成出歌聲信號。 
 
選取聲、韻母 
HMM 
歌譜分析 
音框 DCC 
係數產生 
訂定音高軌跡 
GMM 
音色轉換 
HNM 
信號合成 
音高 
資料 
音長資料 
HMM 
模型 
歌聲信號 
GMM 參數
歌譜 
音框 
DCC 係數
Start
End
圖 3 合成階段之主流程 
 
 
4.3 DCC 係數估計 
 
    由於 STRAIGHT 分析出的頻譜包絡再據
以作合成而得到的語音信號，具有相當高的信
號自然度，這表示 STRAIGHT 分析出的頻譜
包絡非常準確。 
因此，在本計畫裡，我們將切割好的歌聲
音節先拿去作 STRAIGHT 分析，以求得該音
節各音框的頻譜包絡曲線；然後對於一個有聲
音框的諧波部分，先將 STRAIGHT 分析所得
到的基頻值 F0 除於一個倍數 m，使得 x0 = 
F0/m 落在 40Hz 至 80Hz 之間，接著在頻率範
圍 0 至 5500Hz 內，記錄 x0 的各個倍頻上的頻
率值及振幅值；對於雜音部分(即 5500Hz 之
後)，則直接去偵頻各頻譜峰點的頻率值及其
對應的振幅值。依據前述記錄下來的頻率值及
振幅值的一些組合，接著就可執行離散倒頻譜
之計算模組，而求得一個音框的 DCC 係數。 
    關於 DCC 係數的階數，我們量測了頻譜
包絡的逼近誤差，發現一直到階數高於 80 時，
逼近誤差值的下降幅度才趨於緩和，因此我們
決定把階數值設為 80。 
 
 5
聲在一些相鄰的音節之間會顯得不夠流暢，因
此我們把聲、韻母依照其文脈(前後文)作更細
的分類，以建造出較多的文脈相關之 HMM 模
型，如此合成出的歌聲就能夠變得較流暢。除
了文脈因素之外，我們也發現歌唱者唱同一個
韻母時，如果所唱的音高(pitch)高低差許多，
則頻譜也會有明顯的差異(即造成音色的差
別)，因此我們再加入了音高因素(分成高、中、
低音域)，而分別建造出不同文脈與音域組合
的 HMM 模型，如此合成的歌聲其音色才會讓
人覺得是同一個人所唱的。 
另外，我們也比較了兩種語料所訓練出
HMM 模型的差別，第一種語料是 A 歌聲(A 語
者以歌唱方式發音)，而另一種語料是 A 語音
(A 以說話方式發音)，把兩種 HMM 模型分別
拿去合成出 A 語者的歌聲信號，我們聆聽後發
現，A 歌聲語料所訓練出的 HMM 模型，的確
可讓合成的歌聲具有歌唱共鳴的特性，然而 A
語音語料所訓練出的 HMM 模型，卻不能讓合
成的歌聲顯現歌唱共鳴特性，所以要讓合成歌
聲具有歌唱共鳴特性，只要使用歌唱方式發音
的語料來訓練 HMM 模型就可以達成。 
關於歌唱共鳴的產生模型，目的是讓以說
話方式錄音的人，能夠使用這一個模型來合成
出具有共鳴特性的歌聲。我們基於 GMM 頻譜
對映去研究了兩種轉換方法，分別如圖 4 和圖
5 所顯示的處理流程。在圖 4 的基本轉換法，
雖然只能夠使用平行說話語料去訓練GMM頻
譜對映模型，但是使用該轉換法去合成出歌聲
後，我們聆聽合成的歌聲，發現基本轉換法確
實能夠讓合成出的歌聲具有共鳴的特性，並且
音色已經被轉換成很近似 B 語者的音色。 
在圖 5 的相對振幅轉換法，當使用此轉換
法去合成出歌聲後，我們聆聽歌聲的感覺是，
圖 5 的歌聲比圖 4 的清晰許多(即比較不悶)，
但是音色方面就差了一些，音色比較不像 B 語
者的音色。所以，圖 4 和圖 5 的轉換法各有其
優缺點。 
 
 
六、參考文獻 
 
[1] Yamaha, VOCALOID: New Singing Synthesis 
Technology, 
http://www.vocaloid.com/en/index.html . 
[2] X. Rodet, “Synthesis and processing of the 
singing voice,” Proc. 1st IEEE Benelux 
Workshop on Model based Processing and 
Coding of Audio (MPCA-2002), Leuven, 
Belgium, pp. 99-108, 2002. 
[3] P. R. Cook, “Singing Voice Synthesis: History, 
Current Work, and Future Directions”, 
Computer Music Journal, Vol. 20(3), pp. 38-46, 
1996. 
[4] J. Sundberg, The Science of the Singing Voice, 
North Illinois University Press, DeKalb, Illinois, 
1987. 
[5] N. Schnell, G. Peeters, S. Lemouton, P. 
Manoury, and X. Rodet, “Synthesizing a Choir 
in Real-time Using Pitch Synchronous Overlap 
Add”, Int. Computer Music Conference, Berlin, 
Germany, pp. 102-108, 2000. 
[6] H. Y. Gu and W. L. Shiu, “A Mandarin-syllable 
Signal Synthesis Method with Increased 
Flexibility in Duration, Tone and Timbre 
Control,” Proc. Natl. Sci. Counc. ROC(A), Vol. 
22, pp. 385-395, 1998. 
[7] C. Dodge and T. A. Jerse, Computer Music: 
Synthesis, Composition, and Performance, 
Schirmer Books, New York, NY, 1997. 
[8] Y. E. Kim, Singing Voice Analysis/Synthesis, 
Ph.D. thesis, Massachusetts Institute of 
Technology, 2003. 
[9] M. W. Macon, L. Jensen-Link, J. Oliverio, M. A. 
Clements, and E. B. George,  “A Singing 
Voice Synthesis System Based on Sinusoidal 
Modeling”, Int. Conf. Acoustics, Speech, and 
Signal Processing, Munich, Germany, pp. 
435-438, 1997. 
[10] J. Bonada and X. Serra, “Synthesis of the 
Singing Voice by Performance Sampling and 
Spectral Models”, IEEE Signal Processing 
Magazine, Vol. 24, pp. 67-79, 2007. 
[11] Y. Stylianou, Harmonic plus Noise Models for 
Speech, Combined with Statistical Methods, for 
Speech and Speaker Modification, Ph.D. thesis, 
Ecole Nationale Supèrieure des Télécom- 
munications, Paris, France, 1996. 
[12] H. Y. Gu and H. L. Liao, “Mandarin 
Singing-voice Synthesis Using an HNM Based 
Scheme”, Journal of Information Science and 
Engineering, Vol. 27(1), pp. 303-317, 2011. 
[13] M. E. Lee and M. J. T. Smith, “Spectral 
Modification for Digital Singing Voice 
Synthesis Using Asymmetric Generalized 
Gaussians”, Int. Conf. on Acoustics, Speech, and 
Signal Processing, pp. I260-263, 2003. 
[14] M. E. Bestebreurtje and H. K. Schutte, 
“Resonance Strategies for the Belting Style: 
Results of a Single Female Subject Study”, 
Journal of Voice, Vol. 14(2), pp. 194-204, 2000. 
[15] E. Joliveau, J. Smith, and J. Wolfe , "Vocal 
Tract Resonances in Singing: The Soprano 
 1 
 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 100-2221-E-011-157 
計畫名稱 用於歌聲合成之歌唱共鳴產生之研究 
出國人員姓名 
服務機關及職稱 
古鴻炎 
國立台灣科技大學資訊工程系 副教授 
會議時間地點 2011/10/15 ~ 2011/10/17, 中國上海 
會議名稱 International Congress on Image and Signal Processing (CISP 2011) 
發表論文題目 An Improved Voice Conversion Method Using Segmental GMMs andAutomatic GMM Selection 
 
 
一、參加會議經過 
 
CISP 2011 國際研討會，由上海的東華大
學 所 主 辦 ， 而 由 IEEE 生 醫 工 程 學 會
(Engineering in Medicine and Biology Society)協
辦，接受的論文將收錄於 IEEE Xplore 資料
庫。由研討會的名稱可知，接受投稿的領域包
含了影像處理、視訊處理、信號處理相關之子
領域及語音信號的處理。個人投稿的論文，屬
於語音處理，研究的成果是，提出以分段式
(segmental)高斯混合模型(GMM)的觀念，來改
進語音轉換的效能，並且發展了一個基於動態
規劃之自動 GMM 挑選的演算法，以實際應用
該觀念於線上(on-line)進行的語音轉換處理。 
 
CISP 2011 接 受 的 論 文 ， 分 成 28 個
sessions 分別進行口頭發表和壁報發表。由於
事先向主辦單位回應，希望以壁報方式來發
表，所以我的論文排於 10 月 16 日 13:30 ~ 
15:30 的時段，以壁報方式進行發表。右邊上
圖就是在發表會場所拍攝的照片，而右邊下
圖，則是在晚宴會場所拍攝的照片。 
 
  
 3 
 
An Improved Voice Conversion Method Using 
Segmental GMMs and Automatic GMM Selection 
 
Hung-Yan Gu and Sung-Fung Tsai 
Department of Computer Science and Information Engineering 
National Taiwan University of Science and Technology 
Taipei, Taiwan 
 
 
Abstract—In this paper, the idea of segmental GMMs is proposed 
for voice conversion. Also, to apply this idea to on-line voice 
conversion, we have developed an automatic GMM selection 
algorithm based on dynamic programming. In addition, to map a 
vector of DCC (discrete cepstrum coefficients) with only one 
Gaussian mixture, we have designed a mixture selection 
algorithm. For evaluating the performance of the idea, segmental 
GMMs, three voice conversion system are constructed and used 
to conduct listening tests. The results of the listening tests show 
that segmental GMMs proposed here can indeed help to improve 
the performances in both timbre similarity and voice quality. 
Keywords-voice conversion; discrete cepstrum; Gaussian 
mixture model; timbre similarity; harmonic plus noise model 
 Introduction 
The GMM based voice conversion method was introduced 
by Stylianou [1]. Afterward, many researches had tried to 
improve this method by considering one or several related 
issues [2-5]. Nevertheless, some serious problems still exist 
when applying the GMM based voice conversion method. The 
most noticeable one is that the converted spectrums are often 
over smoothed [2-4]. As a result, the converted voice is 
perceived with apparent distortion, i.e. the voice quality is 
significantly decreased. In addition, another noticeable 
problem is that two adjacent frames’ converted spectrums may 
become discontinuous when the over smoothing problem is 
tried to solve by using just the most probable Gaussian 
mixture to map the source spectral coefficients [4, 6]. 
In this paper, we study to solve the over smoothing problem 
with a different approach. Note that the cause results to over 
smoothing is the summation across many Gaussian mixtures 
(usually 128 mixtures) in the GMM based mapping function, 
( ; , )y F x μ= Ψ =  
( ) ( ) 1
1
1
( ; , ) ( )
( ; , )
x xxM
y yx xx xm m m
m m m mM
x xxm
m m m
m
w N x x
w N x
μ μ μ
μ
−
=
=
  
⋅ Ψ   + Ψ ⋅ Ψ ⋅ −   
⋅ Ψ   
 
(1) 
where x denotes a feature vector of the source speaker, y 
denotes the converted feature vector  for the target speaker, 
M is the number of Gaussian mixtures, and μ  and Ψ 
represent the sets of  mean vectors and covariance matrices, 
respectively. To solve the problem of over smoothing, we 
think reducing the number of Gaussian mixtures, M, in the 
mapping function is necessary. Nevertheless, the probability 
density function (PDF) of the trained GMM would become 
coarse when the number of mixtures is directly decreased. 
Therefore, we consider to segment each of the training 
sentences into a sequence of speech segments, and to group 
these speech segments into several classes. For example, a 
speech segment may be a phoneme or a syllable. After 
segmentation, the signal frames grouped to a class are taken to 
train a corresponding GMM with fewer mixtures (e.g. 16 
mixtures). Then, this GMM is dedicated to convert the source 
frames recognized to belong to the corresponding class. In this 
way, the GMM based mapping function, i.e. (1), can be 
applied with fewer mixtures. That is, a complicated GMM is 
now replaced with multiple simpler GMMs, and each GMM is 
dedicated for converting the signal frames recognized to 
belong to its corresponding class. 
In this paper, we study voice conversion for Mandarin, and 
Mandarin is a syllable prominent language. Therefore, we take 
each syllable of a labeled training sentence as a speech 
segment. Next, each segment is grouped to one of the 37 
classes according to its syllable final. For each of the 37 
syllable-final classes, a corresponding GMM is then trained. 
After training, the 37 GMMs are used for on-line voice 
conversion. Nevertheless, there is a problem that must be 
solved beforehand. That is, how can the right class that an 
input frame belongs to be picked out? For this problem, we 
have developed an automatic selection algorithm based on 
dynamic programming. This algorithm will be described in 
Subsection III.A. 
Besides using multiple segmental GMMs to reduce the 
number of mixtures, we advanced furthermore to use only one 
Gaussian mixture for mapping a source spectrum into its 
converted spectrum in order to solve the problem of over-
smoothed converted spectrum. Nevertheless, two adjacent 
source frames’ converted spectrums may become 
discontinuous and result in artifact sounds. Therefore, we 
studied to design a dynamic programming based algorithm to 
consider both the likelihood (when using a particular Gaussian 
mixture) and spectral continuity simultaneously for a sequence 
of signal frames. This algorithm will be described in 
Subsection III.B. In addition, we have integrated the two 
solution methods mentioned to build an on-line voice 
 5 
 
where pt is the detected pitch frequency, μx and σx are the 
average and standard deviation of the source speaker’s pitch 
frequencies.  
As for the right flow of Fig. 2, the input frames are 
processed one after another basically. Nevertheless, in the 
block, “Selecting a GMM”, we propose a selection algorithm 
that processes every 20 voiced frames in a batch. With this 
algorithm, the correct GMM (or its nearby GMM sometimes) 
can be picked out from the 37 GMMs for each frame. Then, in 
the block, “Mapping with single mixture”, only one mixture of 
the selected GMM is used to map the DCC in order to avoid 
spectral over smoothing. Nevertheless, the mixture selected 
for mapping is not always the most probable one. This is 
because spectral continuity between adjacent converted frames 
must also be considered to prevent artifact sounds from being 
generated. For the problem of mixture selection, we have 
developed a dynamic programming based algorithm that is 
different from the one studied by previous researchers [4]. 
Hence, in this block, a sequence of voiced frames bounded 
with left and right unvoiced frames are processed in a batch. 
Finally, in the jointed block, “HNM based speech synthesis”, 
speech signals are re-synthesized using an HNM (harmonic 
plus noise model) based method [8, 11]. 
 
HNM based 
speech synthesis 
Pitch 
adjusting 
Selecting a 
GMM 
Mapping with 
single mixture 
Estimating 
DCC 
Converted 
voice
Detect pitch 
freq. 
Unknown spoken 
sentence 
Framing 
 
Figure 2.  Processing flow for the conversion stage. 
GMM Selection 
Since the content of the input speech is unknown, which 
one of the 37 GMMs should be selected for mapping each 
frame’s DCC becomes a problem that must be solved. In 
general, this is a problem of speech recognition. Nevertheless, 
it is not so serious because some frames are assigned with 
incorrect but similar GMMs are tolerable. 
Here, we intend to use the 37 GMMs trained to take the role 
of HMM usually used for speech recognition. In addition, we 
observe that it is impossible for a person to utter more than 2 
segments (i.e. syllables here) within a very short time interval, 
e.g. 100ms. Therefore, we decide to select GMMs for every 20 
successive voiced frames (spanning 100ms of time) in a batch. 
Then, only one or two of the 37 GMMs should be picked out. 
Here, we have developed a dynamic programming based 
algorithm that selects one or two GMMs according to the 
criterion of maximum likelihood.  
Let the probability that the t-th input frame’s DCC are 
generated by the s-th GMM be Gt(s). That is,  
( )
1
( ) = ( ) ;  ( ),  ( ) .
M
x xx
t m t m m
m
G s w s N x s sμ
=
⋅ Ψ  (3) 
where wm(s) is the weight of the m-th mixture, and xt is the 
vector of DCC for the t-th frame. In addition, let R(t, s) be the 
logarithmic likelihood that the frames from time 1 to time t are 
all generated by the s-th GMM. In contrast, let D(t, s) be the 
logarithmic likelihood that the frames from time 1 to t are 
generated by two GMMs and the t-th frame is  generated by 
the s-th GMM. In terms of these definitions, we can derive the 
two recursive formula, 
( )( , ) log ( ) ( 1, ) ,tR t s G s R t s= + −   (4) 
( ) [ ]
0 37,
( , ) log ( ) max max ( 1, ) , ( 1, ) ,t
v v s
D t s G s R t v D t s
≤ < ≠
 
= + − −  
 
 (5) 
where the boundary values are D(1, s) = 0 and R(1, s) =  
log(G1(s)). Then, the maximum likelihood can be calculated as 
[ ] [ ]{ }0 37 0 37( ) max max ( , ) , max ( , ) .v vA T R T v D T v≤ < ≤ <=   (6) 
where the final time T is set to 20 in this study. In terms of (4), 
(5), and (6), we can calculate the maximum likelihood, A(20), 
and then back track to find the sequence of GMM indices that 
are best for assigning to the batch of 20 voiced frames. 
Mapping with Single Mixture 
Mapping an input frame’s DCC with a single Gaussian 
mixture is meant that the summation and the weighting term of 
(1) are removed. That is, the converted DCC vector, y, is 
calculated as, 
( ) ( ) 1( ) ( ) ,y yxk xx xk kk ky F x xμ μ−= = + Ψ ⋅ Ψ ⋅ −   (7) 
where x is the input frame’s DCC and F k(x) denotes the 
mapping function using the k-th mixture. 
    The developed dynamic programming based algorithm 
for mixture selection is as the following. Let the index of the 
GMM selected by Subsection III.A for the t-th frame be I(t). 
Denote the mapping function using the k-th mixture as 
( ) ( )
k
I t tF x . In addition, let C(t, k) represent the cumulated 
distance from time 1 to time t and the index of the mixture 
used at time t be k. Then, we design the recursive formula, 
   
( )( ) ( 1) 10 ,
( ( 1)) 
( , ) min ( ), ( ) ( 1, ) ,
m
k m
I t t I t tm M
w I t H
C t k dist F x F x C t m
− −
≤ <
− >
 
= + − 
  (8) 
 7 
 
M1 to M2). In addition, when the average scores of the three 
systems are compared, it can be found that the average scores 
of the system SLG are much better than those of the system 
SSG whereas the average scores of SSG are slightly better 
than those of SOG. Therefore, the idea of segmental GMMs 
and automatic GMM selection can indeed help to improve the 
timbre similarity of the converted voices. 
TABLE I.  AVERAGE SCORES AND STD FOR TIMBRE SIMILARITY TESTS 
  SOG SSG SLG 
AVG 6.08 6.24 7.05 M1=>M2 STD 1.11 1.09 0.93 
AVG 6.92 7.24 7.60 M1=>F1 STD 1.13 1.07 1.10 
 
Voice Quality Tests 
In the tests of voice quality, the three converted voice files, 
VX1, VX2, and VX3 are used. These files are played in the 
order AX where A is fixed to VX1 and X is randomly selected 
from VX2 and VX3. Each time that two files, AX, are played, 
the participant is requested to give a score.  Here, the score 
range is from 1 to 9. The score 9 (1) means the quality of X is 
much better (worse) than A, the score 7 (3) means the quality 
of X is slightly better (worse) than A, and the score 5 means 
the quality of X cannot be distinguished from that of A. 
After listening tests, the scores given by the 25 persons are 
collected to compute average scores and standard deviations 
for the two systems, SSG and SLG, respectively. The results 
are those values listed in Table II. From Table II, it can be 
found that the average scores for conversion from M1 to M2 
(i.e. same gender) is about 0.5 better than the average scores 
for conversion from M1 to F1. This indicates that the quality 
of the converted voice from different genders is harder to 
improve. In addition, when the average scores of the two 
systems, SSG and SLG, are compared, it can be found that the 
scores of SLG are all higher than 5.0 and are much better than 
those of SSG. Therefore, the idea of segmental GMMs and 
automatic GMM selection can indeed help to improve the 
quality of the converted voice.  
TABLE II.  AVERAGE SCORES AND STD FOR VOICE QUALITY TESTS 
  SSG vs SOG SLG vs SOG 
AVG 5.23 6.04 M1=>M2 STD 1.43 1.45 
AVG 4.89 5.55 M1=>F1 STD 1.50 1.47 
 
 Cepstrum Distances Measuring 
There are 25 remaining parallel sentences that are not used 
in the training stage. Here, these source sentences are fed to 
the three systems to obtain their corresponding converted 
sentences, respectively. Then, a geometric distance of DCC is 
measured between each voiced frame of the converted 
sentences and its corresponding frame in the target sentences 
according to the saved DTW alignment data. Next, the 
measured distances are averaged across all voiced frames. As 
a result, the average distances obtained for the three systems 
are as those listed in Table III. From this table, it is seen that 
the system SOG will obtain the smallest average distances. 
Nevertheless, the results of listening tests show that the system 
SOG is the worst in timbre similarity and is worse than SLG in 
voice quality. Therefore, the average cestrum distances are 
inconsistent with the results of the listing tests for SOG. On 
the other hand, the performance improvements of SLG as 
compared with SSG are reflected in the measured average 
distances. SLG and SSG both use single Gaussian mixture to 
map the spectral parameters of DCC but GMM selection is 
only adopted in SLG. 
TABLE III.  AVERAGE DISTANCES FOR THE THREE SYSTEMS 
 SOG SSG SLG 
M1=>M2 0.543 0.609 0.601 
M1=>F1 0.598 0.634 0.612 
 
Conclusion 
According to the results of the listening tests, the system 
SLG is the best in both timbre similarity and voice quality 
among the three systems. This demonstrates that the idea of 
segmental GMMs and the automatic GMM selection 
algorithm proposed here can indeed help to improve the 
performances of the GMM based voice conversion mechanism. 
As to cepstrum distance measured, the system SOG still 
obtains the smallest average distance. Nevertheless, it suffers 
the fault of over-smoothed converted spectrums, which result 
in degraded voice quality and timbre similarity. In the future, 
the unit of speech segments can be reduced from syllables to 
voiced consonant and vowels. It is hopeful that the 
performance of voice conversion using segmental GMMs is 
further improved. 
Acknowledgment 
This study is supported by National Science Council of 
Taiwan under the contract number, NSC 99-2628-E-011-107. 
 
 
Date: Thu, 30 Jun 2011 15:54:51 +0000 
From: PC Chair <cisp-bmei@dhu.edu.cn> 
To: guhy@mail.ntust.edu.tw 
Subject: CISP'11-BMEI'11 P1473 Acceptance Notification 
 
 
Dear Hung-Yan Gu, 
 
Paper ID : P1473 
Paper Title : An Improved Voice Conversion Method Using Segmental GMMs and Automatic GMM Selection 
 
(All Chinese characters in this email are intended for authors from China's mainland only. 
请浏览会议网站上的中文注册和终稿上传信息。) 
 
Congratulations! We are pleased to inform you that your above paper has been accepted for presentation at the 4th 
International Conference on Image and Signal Processing (CISP'11) to be held from 15-17 October 2011, in 
Shanghai, China. After you complete the requirements below, your paper will appear in conference proceedings 
and will be indexed by both EI Compendex and ISTP, as well as the IEEE Xplore (IEEE Conference Record 
Number for CISP'11: 18205; IEEE Conference Record Number for BMEI'11: 18206. CISP IEEE Catalog Number: 
CFP1194D-CDR, ISBN: 978-1-4244-9305-0; BMEI IEEE Catalog Number: CFP1193D-CDR, ISBN: 
978-1-4244-9350-0. CISP-BMEI 2008-2010 papers have already been indexed in EI Compendex). Substantially 
extended versions of best papers will be considered for publication in a CISP'11-BMEI'11 special issue of the 
Computers and Electrical Engineering journal (SCI-indexed). The conference will feature world-renowned keynote 
speakers: Thanos Stouraitis, President-Elect of IEEE Circuits and Systems Society; Seong-Whan Lee, Hyundai-Kia 
Motor Chair Professor; Metin Akay, Fellow of the Institute of Physics (IOP), the American Institute of Medical 
Biological Engineering (AIMBE) and the American Association for the Advancement of Science (AAAS); 
Yuan-Ting Zhang, Editor-in-Chief for IEEE Trans. on Information Technology in Biomedicine (all Fellow of the 
IEEE). 
 
In order for your paper to be included in the proceedings indexed by Ei Compendex/ISTP, it is important that you 
closely follow each and every instruction below, as the acceptance is conditional on your accurate and timely 
reactions : 
 
1. Revise your paper, appropriately addressing the reviewer comments (at the end of this email, if any) which are 
intended to help you improve your paper for final publication. If any review comments seem vague, please revise 
your paper according to your best understanding. 
 
2. Strictly follow the IEEE format requirements; incorrectly formatted papers cannot be included in the 
proceedings. Please refer to the conference website http://cisp-bmei.dhu.edu.cn/ for detailed formatting instructions 
and templates. Some of the formatting instructions are given below. Closely follow the instructions at the 
conference website (Final Submission page) to convert your paper to IEEE Xplore-compliant pdf file using PDF 
eXpress and upload your final camera-ready full paper as soon as possible, but latest by 15 July 2011. Please 
ensure that all formulas, figures and embedded objects in your file are error-free. It is crucial to make sure your pdf 
file is IEEE Xplore-compliant using PDF eXpress. Otherwise your paper may not be included in the IEEE Xplore 
or indexed in EI/ISTP. Please submit your final paper, IEEE Copyright Form, Registration Form, and Payment 
Confirmation by clicking "Upload Final Paper" at the conference submission system. In addition, please click "Edit 
Submission" at the conference submission system to ensure that all paper information are accurate, including the 
paper title and all author names, emails, and affiliations. This step is very important, since the same author and 
paper information will appear in the proceedings and 
indexing. 
 
3. Please download IEEE Copyright Form at the conference website, complete the form, sign it, and upload a 
scanned form to us. In the Copyright Form, you will need to enter the conference name. Please note that your paper 
above is accepted in CISP'11, not BMEI'11. If you have more than one paper accepted, each paper may be accepted 
by a different conference. In addition, some papers were originally submitted to one of the two conferences (i.e., 
CISP'11 and BMEI'11), but were later transferred to the other conference by the Program Committee for better 
matches in topics. 
 
4. Each paper must have 1 dedicated registration with full payment received by 15 July 2011 for the paper to be 
Accept Letter
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/19
國科會補助計畫
計畫名稱: 用於歌聲合成之歌唱共鳴產生之研究
計畫主持人: 古鴻炎
計畫編號: 100-2221-E-011-157- 學門領域: 自然語言處理與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
