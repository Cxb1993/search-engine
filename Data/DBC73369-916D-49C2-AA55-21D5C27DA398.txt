normalization, free energy function, mixed integer 
nonlinear programming, generative model, mean field 
annealing, Boltzmann assumption, long term forecast, 
chaotic time series 
 
中文摘要 
關鍵字：多層類神經網路，函數近似，
正規化，自由能函數，混合整數規劃，
生成模型，平均場退火法 
本 計 劃 已 提 出 多 重 正 規 化 RBF 
(normalized radial basis functions, 
NRBF) 網路的組織架構、完成有效學
習法則實作與應用。新的成對資料隨
機生成模型包含多組成對高斯機率分
布，使用波茲編碼(Potts encoding)
將模型匹配問題轉換為混合整數規化
問題，並發展退火式自由能學習方
法，解隨機生成模型的參數估計問
題。本計劃已證明多層類神經網路的
監督式學習問題可轉換為生成模型的
匹配問題。三層正規化 RBF 網路是成
對資料隨機生成模型所衍生的多層類
神經網路，其隱藏層由正規化半徑基
底函數所組成，並使用共同權重矩陣
測量 Manhalanobis 半徑距離。本計劃
提出由 K 個正規化 RBF 網路所組成的
新式多層類神經網路。新式多重 NRBF
網路定義廣泛的對應函數空間，大部
分的既有 RBF網路或 NRBF 網路的對應
函數都包含在內。 
英文摘要 
Keywords: multilayer neural networks, 
function approximation, normalization, 
free energy function, mixed integer 
nonlinear programming, generative 
model, mean field annealing, Boltzmann 
assumption, long term forecast, chaotic 
time series 
 This project has proposed neural 
architecture of multiple NRBF 
(normalized radial basis functions) 
networks and developed reliable and 
effective learning methods. The 
proposed generative model for paired 
training data is composed of multiple 
paired Gaussian probability density 
functions. Potts encoding has been 
applied to translate fitting a generative 
model to paired data to a mixed integer 
programming, which is then resolved by 
an annealed free energy (Fe) approach. 
The multi-layer neural network oriented 
from a generative model is a 3-layer 
normalized RBF network composed of 
normalized radial basis functions with 
Manhalanobis distances measured by a 
common weight matrix. To strengthen 
generalization capability, this project 
devises neural organization of multiple 
normalized RBF networks. The novel 
architecture spans a functional space that 
covers mappings of most existing RBF 
network or NRBF network. This project 
will extensively employ constrained 
optimization techniques for learning 
multiple normalized RBF networks. In 
constrained optimization, we will apply 
leave-one-out approximation theory to 
extend annealed Fe learning for multiple 
normalized RBF networks. 
 
  
 
 
Let r = (r1, ..., rM )T ∈ RM collect means in all qm. δ indicates
the sub-model retrieved by given x. Its multiplication to r
denotes an expected desired target created by the retrieved
sub-model. The conditional expectation of the desired target
to given x proposed by a generative model is expressed by
F (x;θ) = δT r, (6)
where θ denotes collection of model parameters. The output
of F relies on the exclusive membership determined by Man-
halanobis distances between x and all wm. The range of F is
a finite set that consists of discrete values in r.
The overlapping memberships are further introduced to de-
velop smoother mappings. Let v = (v1, ...,vM )
T ∈ [0, 1]M be
a unitary vector that represents the overlapping membership
of x to joined sub-models. Assume
vm ∝ exp(−βhm), (7)
where positive β modulates the overlapping degree. Since vm
distributes within [0 1], subject to the unitary condition of
probabilities, X
m
vm = 1,
it follows
vm ≡ φm(x) =
exp(−βhm)X
j
exp(−βhj)
. (8)
When β = 1, vm in (7) is proportional to pm in (2) due to
the use of a common weight matrix. All vm represent the
overlapping membership of x to joined sub-models. If diﬀerent
weight matrices coexist in a generative model, the definition
in (7) should be further revised to represent pm in (2).
β modulates the overlapping degree for membership calcu-
lation. By definition (8), smaller β implies higher overlapping
degree. At suﬃciently small β, all vm approach 1/M inde-
pendent of distances between x and all wm. In contrast, at
suﬃciently large β the denominator of (8) is dominated by the
exponential term whose center is closest to x. In the occasion
elements in v reduce to binary δ in (5).
With overlapping memberships, the conditional expectation
of the desired target to given x equals the product of v and r,
such as
Fβ(x;θ) ≡ vT r
=
X
m
rmφm(x), (9)
which exactly sketches the input-output relation of the 3-layer
NRBF network in Figure 2. In the second layer, v represents
the overlapping membership that is calculated with normaliza-
tion. All hidden units in the network activate normalized out-
puts (8) and their external fields (4) are Manhalanobis radial
distances measured by a common weight matrix. The neural
organization of normalized radial basis functions is oriented
from the generative model shown in Figure 1.
It is notable that β plays a significant role of modulating
the smoothness of Fβ . The output of Fβ is the mean of all rm
at suﬃciently small β, becomes a linear combination of all rm
at intermediate β, and belongs {rm}m at suﬃciently large β.
2
(6) to (11), attains a mixed energy function, written
E(Λ,θ) = E1 + λE2
=
1
2
X
i
X
m
δim(xi −wm)TA(xi −wm)−
N
2
log |A|
+
λ
2
X
i
X
m
δim(rm − yi)2, (12)
where λ is non-negative and Λ denotes collection of exclusive
memberships. Minimizing function E subject to constraints,
X
m
δim = 1,∀i,
δim ∈ {0, 1},∀i,m,
sketches a mathematical framework for fitting the generative
model to paired data.
Like Hopfield energy functions of modeling the travelling
salesman problem (TSP) (Peterson & So˝dergerbg, 1989) and
the graph bisection problem (Wu, 2004), the mixed energy
function E is not diﬀerentiable with respect to discrete depen-
dent variables. In analogy with statistical mechanism, under
thermal equilibrium, Λ is assumed to obey the Boltzmann dis-
tribution (Hertz, et al., 1991),
Pr(Λ) ∝ exp(−βE(Λ|θ)),
where β denotes the inverse of a temperature-like parameter
and Λ|θ denotes the case of fixing θ. Λ is composed of N
M -state random variables. Since there are MN possible con-
figurations, the mean configuration under the Boltzmann as-
sumption is intractable in computation for large N . At suﬃ-
ciently small β, all feasible configurations have the same occur-
ring probability. In contrast, at suﬃciently large β, under the
Boltzmann assumption, the physical-like system is dominated
by the optimal configuration that minimizes E(Λ|θ), such as
lim
β→∞
Pr(Λ) =
⎧
⎪⎪⎨
⎪⎪⎩
1, if Λ = Λopt
0, otherwise
where
E(Λopt|θ) = min
Λ
E(Λ|θ).
At intermediate β the physical-like system is characterized
by a free energy function that sums up the negative system
entropy and the mean of E. By mean field approximation all
Potts variables are assumed statistically independent. Follow-
ing the assumption, the system entropy is approximated by
the sum of individual entropies, and the mean of E is simply
a result of substituting means of individual variables to E. By
the argument, we have the following free energy function,
ψ(hΛi,u|θ) = E(hΛi|θ) +
X
i
X
m
hδimiuim
− 1
β
X
i
ln(
X
m
exp(βuim)), (13)
where hδimi denotes the mean of δim and u denotes collection
of auxiliary variables uim.
The first term represents the mean of E and the last two
terms sum up all individual entropies. Similar tractable
free energy functions have been formulated in previous works
(Hertz et al., 1991; Rose et al., 1990; Peterson & So˝dergerbg,
1989; Wu & Lin, 2002; Wu, 2002) for solving complex tasks
in the field of neural networks. The contribution of individ-
ual entropies to ψ (13) inversely proportional to β vanishes
at suﬃciently large β, where ψ reduces to recover original E.
4
The optimal β is defined by
βopt = argminβ,r
DS,β(θ),
where
DS,β(θ) =
1
2
X
i
kyi − Fβ(xi;θ)k2 . (20)
A screening process is applied to scan values that linearly par-
tition the possible range of β. At each β the process employs
equations (4) and (8) to determine new overlapping member-
ships based on A and {wm}m derived by the learning proce-
dure, then substituting them to equation (18) to recalculate
r ={rm}m, and applying refined r to calculate DS,β. The final
mapping derived from paired data in S is denoted by Fβopt , of
which posterior weight r in θ has been refined by substitut-
ing overlapping memberships determined under βopt to equa-
tion (18). The network function Fβopt behaves smoother and
is more accurate for function approximation than the version
with exclusive memberships.
III. Learning multiple NRBF networks
The annealed Fe approach developed in the previous section
is extended for learning multiple NRBF networks by leave-
one-out approximation. Let zi[l] denote the output of the lth
NRBF network in Figure 3 at some intermediate β, such as
zi[l] = Fβ(xi;θl).
As in (Wu et al., 2006), an intermediate target, denoted by
yi[k] for some module k, is set to compensate for the error
of approximating yi by the sum of the other K − 1 NRBF
networks, such as
yi[k] = yi −
X
l6=k
zi[l], (21)
= (yi −
X
l
zi[l]) + zi[k] (22)
which sketches an individual goal of learning the kth NRBF
network. The minus term in (21) approximates yi as if the
kth module were absent. The definition of yi[k] follows the
strategy of leave-one-out approximation.
The proposed 4-layer NRBF network consists of multiple
normalization modules, each being realized by a 3-layer NRBF
network. The output unit in the fourth layer sums up mod-
ular outputs formed by integrators in the third layer. The
multi-modular architecture is capable of providing common
information required in both predicting and learning phases.
Integrators in the third layer, each being the output unit of an
individual 3-layer NRBF network, form distinguishable mod-
ular outputs as well local targets in equation (22), which are
critical for decomposing the task of learning the proposed 4-
layer NRBF network to sub-tasks of learning joined 3-layer
NRBF networks. With integrators in the third layer the lo-
cal target yi[k] in (22) can be obtained by adding the modular
output zi[k] to the backward propagated global approximating
error. The multi-modular architecture form local targets that
facilitat extension of annealed Fe learning to multiple 3-layer
NRBF networks following leave-one-out approximation. Re-
placing integrators in the third and fourth layers with a sole
6
Table 1 lists two or higher dimensional target functions. f1
is a post-sinusoidal projection. Both f2 and f3 are second-
order polynomials with ranges wider than f1. Each of target
functions f5-f8 consists of two elementary functions that are ei-
ther projective or radial basis functions with post-nonlinearity
realized by a one-dimensional exponential, hyper-tangent or si-
nusoidal function. For each target function, 600 predictors are
uniformly sampled from the domain, denoted by [−2π, 2π]d,
where d denotes the dimension of input vectors. Substituting
each predictor to fj forms a desired target. Paired data ori-
ented from the same target function are randomly partitioned
to two equal sets respectively for training and testing. Figure
4 shows approximating functions derived by annealed Fe learn-
ing of a 3-layer NRBF network subject to paired data oriented
from f1, where the number of hidden units increases from 3
to 18 in 3. Blue dots in each subplot draw paired training
data. It can be observed that annealed Fe learning of a 3-layer
NRBF network with M > 9 approximates f1 faithfully. The
mean square testing error of annealed Fe learning withM = 41
reduces to 7.4e-5 that is much better than 2.3e-2 of the Rätsch
method. Figure 5 sketches the change of the mean square test-
ing error with respect to the number of hidden units employed
by annealed Fe learning and the Rätsch method. Numerical
results show annealed Fe learning of a 3-layer NRBF network
outperforms leaning the traditional RBF network by the other
two methods in approximating f1.
Tables 2 and 3 list quantitative performances of the three
relevant methods for approximating f1-f8 in terms of mean
square training and testing errors summarized over ten exe-
cutions subject to the training set oriented from each target
function.
The mean square testing errors of annealed Fe learning in
average are less than one sixtieth, three thousandths and two
hundredths of those obtained by the Rätsch method respec-
tively for approximating f1, f2 and f3. Annealed Fe learning
apparently outperforms the other two methods in approximat-
ing the first four target functions. Numerical results show
that neural organization of normalized hidden units as well as
Manhalanobis radial distances based on a common weight ma-
trix is significant for function approximation and annealed Fe
learning is more eﬀective and reliable for approximating f1-f4
relative to the other two methods.
Based on leave-one-out approximation, annealed Fe learning
of a 4-layer NRBF network with K = 2 and M = 41 is veri-
fied for approximating f5-f8. The traditional RBF network is
tested with M = 41, because the Rätsch method suﬀers from
invalid initialization and the performance of the LM method
is not improved with more hidden units. Table 3 shows the
mean and variance of the mean square training and testing er-
rors of the three methods for approximating f5-f8. The mean
square testing error of annealed Fe learning is smaller than
one hundredth and one tenth of those derived by the Rätsch
method respectively for approximating f5 and f6-f7, and one
hundredth than that derived by the LM method for approx-
8
[3] NØrgaard, M., Ravn, O., Poulsen, N. K. and Hansen, L.
K. (2000). Neural networks for Modelling and Control of
Dynamic Systems, Springer-Verlag, London, UK.
[4] Peterson, C. and So˝dergerbg, B. (1989). A new method
for mapping optimization problems onto neural network,
Int. J. Neural Syst. 1,3.
[5] Rose, K., Gurewitz, E. and Fox, G. C. (1990). Statistical
mechanics and phase transitions in clustering, Phys. Rev.
Lett., Vol. 65, No. 8, pp. 945-948.
[6] Ra¨tsch, G., Onoda, T. and Muller, K. R. (2001). Soft
margins for AdaBoost, Machine Learning 42 (3): 287-320.
[7] Wu, J. M., and Lin, Z. H. (2002). Learning generative
models of natural images, Neural Networks 15 (3): 337-
347.
[8] Wu, J. M. (2002). Natural discriminant analysis using in-
teractive Potts models, Neural Computation 14 (3): 689-
713 Mar.
[9] Wu, J. M., and Chiu, S.J. (2001). Independent component
analysis using Pots models, IEEE Transactions on Neural
Networks 12 (2): 202-211.
[10] Wu, J. M., Lin, Z. H., Hsu, P. H. (2006). Function approx-
imation using generalized adalines, IEEE Transactions on
Neural Networks 17 (3): 541-558.
[11] Wu, J. M. (2008). Blind separation of fetal electrocardio-
grams by annealed expectation maximization, Neurocom-
puting 71, 1500—1514.
[12] Wu, J. M., Chen, M. H., Lin, Z. H. (2008). Independent
component analysis based on marginal density estimation
using weighted Parzen windows, Neural Networks 21, pp.
914-924, .
[13] Wu, J. M. (2008). Multilayer Potts perceptrons with
Levenberg-Marquardt learning. IEEE Trans Neural Net-
works, 19(12):2032-43.
[14] Wu, J. M. (2004). Annealing by two sets of interac-
tive dynamics, IEEE Transactions on Systems, Man and
Cybernetics–PART B: Cybernetics, Vol. 34, No. 3.
10
Flip
Coin
)1,|(),,|( 11 ryqp Awx
)1,|(),,|( mm ryqp Awx
)1,|(),,|( MM ryqp Awx
1π
mπ
Mπ
),( ii yx
…
…
…
…
Figure 1
);( 1θxβF
);( 2θxβF
.
.
.
);( KF θxβ
x ∑
y
Figure 3
Figure 5
2011 International Conference on Signal and Information Processing (ICSIP2011) 
 
- 1 - 
Notification of Acceptance of the ICSIP 2011 
(Workshop of ICCEE 2011) 
Round II 
October 14-16, 2011, Singapore 
http://www.icsip.org/  
           
Dear JIANN-MING WU, YA-TING ZHOU and CHUN-CHANG WU, 
Paper ID : P028         
Paper Title : LEARNING MARKOV-CHAIN EMBEDDED RECURRENCE RELATIONS FOR 
CHAOTIC TIME SERIES ANALYSIS 
 
Congratulations! The review processes for 2011 International Conference on Signal and Information Processing 
(ICSIP2011) has been completed. The conference received submissions from nearly 20 different countries and regions, 
which were reviewed by international experts, and about 70 papers have been selected for presentation and publication. 
Based on the recommendations of the reviewers and the Technical Program Committees, we are pleased to inform you 
that your paper identified above has been accepted for publication and oral presentation. You are cordially invited to 
present the paper orally at ICSIP 2011, the workshop of ICCEE 2011, which will be held on October 14-16, 2011, 
Singapore. 
ICSIP 2011 is organized by International Association of Computer Science and Information Technology (IACSIT) and 
Singapore Institute of Electronics. Submitted conference papers will be reviewed by technical committees of the 
Conference. 
(Important) So in order to register the conference and have your paper included in 
the proceeding successfully, you must finish following SIX steps. 
1. Revise your paper according to the Review Comments in the attachment carefully. 
2. Format your paper according to the Template carefully.  
http://www.icsip.org/ASME.conf.template.doc (DOC Format) 
3. Download and complete the Registration Form. 
http://www.icsip.org/reg.doc (English) 
4. Finish the payment of Registration fee by Credit Card. (The information can be found in the Registration form)  
http://www.icsip.org/reg.doc (English) 
 LEARNING MARKOV-CHAIN EMBEDDED RECURRENCE 
RELATIONS FOR CHAOTIC TIME SERIES ANALYSIS  
JIANN-MING WU 
 
YA-TING, ZHOU 
Department of Applied Mathematics 
National Dong Hwa University, Taiwan
CHUN-CHANG WU  
 
 
ABSTRACT 
This work explores Markov-chain structured recurrence relations for generation 
and perception of chaotic time series. A recurrence relation realized by a 
multilayer neural network of radial basis functions (RBF) is employed to 
characterize the memory-less conditional expectation of a high order Markov 
process in short time scale. On the basis, a stochastic Markov chain is employed 
to emulate model switching among multiple recurrence relations embedded 
within chaotic time series. The high-order recurrence relation is expressed by a 
nonlinear RBF network derived by Levenberg-Marquardt learning subject to 
paired data auto-regressively sampling from dynamically segmented chaotic 
time series. The proposed Markov modeling is applied to synthesize chaotic 
time series of laser data. Numerical simulations show encouraging results. 
 
KEY WORDS 
mutlilayer neural networks, chaotic time series, Markov-chain embedded pattern 
analysis, supervised learning, Levenberg-Marquardt learning 
1 INTRODUCTION 
Recently learning a multilayer feed-forward neural network by the Levenberg-
Marquardt algorithm [15] has been shown applicable for chaotic time series 
prediction [16]. The success demonstrates chaotic time series prediction 
transformable to data driven function approximation and resolvable by 
supervised learning of a multilayer neural network. It also motivates our 
exploration to the domain of predictable chaotic time series by learning a 
multilayer neural network.  
Learning a multilayer neural network for prediction aims to extract a 
recurrence relation of characterizing given chaotic time series. The extracted 
recurrence relation expresses the expected value of an upcoming event in terms 
of instances of most recently events with an order τ identical to the finite 
dimension of the network input. The finite order τ in consistent with the 
memoryless property following the assumption that given chaotic time series are 
oriented from a generative source well characterized by a high-order Markov 
process. The extracted recurrence relation refers to the mapping underlying the 
expectation of an upcoming event conditional to most recently events. 
The application domain is limited whenever given chaotic time series are 
oriented from a generative source that is more complex than a high-order 
Markov process. The complexity of given chaotic time series may be caused by 
long-term dependency[6] or stochastic switching among embedded recurrence 
relations. Recurrent neural networks have been serving as important components 
for ordered sequence analysis[1][2][3][4] of symbolic grammar[1][4], 
continuous spatiotemporal patterns[3] and sensory-motor sequence patterns 
 ( ) ( ) ( )2, | , | ,k k k k kP y f g y r σ=x x μ A  .            (6) 
Exclusive memberships and overlapping memberships are further illustrated. 
Each paired data y[t])[t],(x  possesses its exclusive membership to joined sub-
models. Let ][tδ denote an exclusive membership. It belongs { }Mee ,...,1 , 
where ie  denotes a unitary binary vector with the ith bit one and others zero. 
kt eδ =][ means y[t])[t],(x  being generated by the kth sub-model.  
Given x , if )(max)( xx jjk pp = , the exclusive membership δ  is set to 
ke . Suppose IA =k . The exclusive membership can be determined by the 
nearest center jjk uxux −=− min . The expectation of y conditional to 
given x  is defined by rδx Ty =| . The overlapping membership is defined 
by 
( ) ( )2Pr exp [ ]k ktβ= ∝ − −δ e x μ  ,                 
where β  denotes the inverse of a temperature-like parameter that modulates the 
freedom of δ . Since 
( )
1
Pr 1
M
k
k=
= =∑ δ e  ,                              
where M denotes the model size, it follows 
( ) ( )( )
2
2
exp [ ]
Pr
exp [ ]
k
k k
h
h
t
v
t
β
β
− −= ≡ = − −∑
x μ
δ e
x μ
 .                
Let [ ]1, , TMv v=v L . Because | Ty =x v r , where [ ]1, , TMr r=r L , we have 
 ( )
( )
2
2
exp [ ]
|
exp [ ]
k kT
k h
h
r t
y
t
β
β
− −= = − −∑∑
x μ
x v r
x μ
 ,              
which exactly defines the network function of an NRBF (normalized radial basis 
function) network. 
3. MARKOV-CHAIN ORGANIZED RECURRENCE RELATIONS 
This section presents Markov-chain organized PGMs (pair-data generative 
models) for time series analysis. Stochastic transitions among generative models 
are considered. Figure 2 shows a Markov chain of generative models, where 
stochastic transitions are represented by a matrix [ ]ijT  and ijT  represents the 
probability of transition from the i th to j th generative model. State transition 
occurs after a segment of short-term data generation by a current model. 
 length of training data required for optimizing an RBF network. We define the 
following average error for detecting a switching point,  
2);][(][(][ θx∑+
−=
−=
τ
τ
t
ti
tFtyterror  
where θ  denotes built-in parameters of a current PGM. When error[t] exceeds a 
pre-determined ε ,  t  is regarded as a switching point for segmentation. The 
goodness of fitting the thj  PGM to paired data in iS  is defined by the 
following mean square error, 
∑
∈
−=
iSt
j
i
ji tty
][
2
, )];[(F][(L
1E
x
x θ . 
Let K denote the number of required hidden states after reduction and L denote 
the number of detected switching points. L also denotes the number of initiated 
RBF networks. Given K, the task of merging PGM models can be stated as a 
graph partition problem. If L > K, 
,i jE  denotes the quasi-distance between node i  and j  since the matrix of 
,i jE  is non-symmetric. Merging L  PGM models is 
equivalent to partition L  nodes to K exclusive sets in a way that maximizes 
cuttings crossing different sets. This task can be approached by relaxing 
interactive dynamics of Hopfield-like neural networks [13][14].
 
The other 
approach is simply to merge PGM models by a pre-determined threshold ε . If 
the mean of 
,i jE  
and 
,j iE  is less than ε , iS  and jS  are regarded oriented from 
the same hidden state.  
Let switching points partition given time series to L  segments, which further by 
auto-regressive sampling lead to { }m mS . Now the number of paired data in mS  is variant. L  PGM models are further merged to K hidden states. Based on 
sources or memberships of 1, , LS SL  to K hidden states or PGM models, we 
can calculate transition probabilities among hidden states and eventually obtain 
a Markov chain of PGMs for characterizing given time series.  
4. NUMERICAL SIMULATIONS 
The first example employs a Markov chain of pair-data generative models to 
generate time series and applies the proposed approach for model reconstruction. 
There are three PGM models for data generation. According to transition 
probabilities, each time a PGM model is selected to generate data instances of 
fixed length. The goal of model reconstruction is to reconstruct PGM models 
and determine transition probabilities among PGM models. The post-nonlinear 
functions for this example are 1 2 3sin( ), cos( ), exp( )
T T Tx x xa a a ,whose receptive fields 1a , 2a and 3a  are randomly generated.  
The transition probabilities among PGM models for data generation are 
shown in the left plot of figure 4. According to transition probabilities, each time 
a PGM is selected to generate a fixed length of time instances. Figure 3 shows 
the whole generated time series. By the proposed approach, three PGM models 
were derived. The right plot of figure 4 shows the estimated transition 
probabilities of derived PGM models. Figure 5 shows original Markov chain of 
PGM models and its reconstruction. The simulation condition includes M=30, 
  
Fig3. A complex time series generated by a Markov chain of generative models. 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
03333.06667.0
0370.02963.06667.0
0286.02571.07143.0
2,
06667.03333.0
0290.07101.02609.0
0357.06786.02857.0
1 trantran
 
 
Fig4. The original and estimated transition matrices 
 
 
Fig5. Stochastic transitions among hidden states: original and estimated 
ones. 
 
Fig6. Laser data 10000 from in the SFI competition 
 science society 531-546. (1986) 
[3] Williams, R. J., and Zipser, D., A learning algorithm for continually running 
fully recurrent neural networks. Neural Computation, 1, 270-80. (1989) 
[4] Pollack, J., The induction of dynamical recognizers. Machine Learning, 7, 
227-252. (1991) 
[5] Jordan, M. I.,Indeterminate motor skill learning problems. In M. Jeannerod 
(Ed.), Attention and performances, XIII. Cambridge, MA: MIT Press. (1988) 
[6] Bengio, Y., Simard, P., and Frasconi, P., Learning long-term dependencies 
with gradient descent is difficult. IEEE Transactions on Neural Networks, 
5(2), 157-166. (1994) 
[7] Rumelhart, D. E., Hinton, G. E., and Williams, R. J., Learning internal 
representations by error propagation. In D. E. Rumelhart, and J. L. 
McClelland (Eds.), Parallel distributed processing, (pp. 318-362). 
Cambridge, MA: MIT Press. (1986) 
[8] Hichreiter, S., and Schmidhuber, J., Long short-term memory. Neural 
Computation, 9(8), 1735-1780. (1997) 
[9] Schmidhuber, J., Gers, F., and Eck, D., Learning nonregular languages: A 
comparison of simple recurrent networks and LSTM. Neural Computation, 
14, 2039-2041. (2002) 
[10] Jaeger, H., Short term memory in echo state networks. GMD report, 152, 1-
60 (2001) 
[11] Jaeger, H., and Haas, H., Harnessing nonlinearity: Predicting chaotic 
systems and saving energy in wireless communication. Science, 78-80. 
(2004) 
[12] Namikawa Jun., and Tani Jun., A model for learning to segment temporal 
sequences, utilizing a mixture of RNN experts together with adaptive 
variance. Neural Networks 21, 1466-1475. (2008) 
[13] Frolov A.A., Husek D., and Muraviev I.P., and Polyakov PavelY., Origin 
and elimination of two global spurious attractors in Hopfield-like neural 
network performing Boolean factor analysis. Neurocomputing 73, 1394–
1404. (2010) 
[14] Frolov A.A., Husek D., and Muraviev I.P., Informational capacity and recall 
quality in sparsely encoded Hopfield-like neural network: analytical 
approaches and computer simulation, Neural Networks 10, 845–855. (1997) 
[15] NØrgaard, M., Ravn, O., Poulsen, N. K. and Hansen, L. K. (2000). Neural 
networks for Modelling and Control of Dynamic Systems, Springer-Verlag, 
London, UK. 
[16] Mirzaee, H. (2009). Long-term prediction of chaotic time series with multi-
step prediction horizons by a neural network with Levenberg--Marquardt 
learning algorithm, Chaos, Solitons and Fractals, 41, 1975--1979. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：吳建銘 計畫編號：99-2221-E-259-024- 
計畫名稱：多重正規化 RBF 網路學習與類神經函數近似研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 1 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 1 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
