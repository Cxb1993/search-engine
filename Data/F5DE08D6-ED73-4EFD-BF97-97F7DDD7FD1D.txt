obtained without BER performance loss. 
英文關鍵詞： SISO decoding algorithm； turbo product codes； bit-
error rate； closed-chains error pattern； iterative 
decoding； flipping decoding 
 
快速區塊渦輪碼解碼演算法及平行解碼電路的研究與設計
Study and Design of Fast BTC Decoding Algorithms and Parallel Decoding Circuits
計畫編號：NSC 100-2221-E-182-040
執行期間：100 年 08 月 01 日至 101 年 10 月 31 日
主持人：長庚大學電機工程學系教授 盧而輝
中文摘要
本研究報告分為兩部分。在第一部份，我們提出硬
式輸入硬式輸出(HIHO)之渦輪乘積碼(TPC)的解碼演算
法。此演算法在預測行/列(row/column sequence)之外部
訊息時，可省略 2p-1 次的硬式判決解碼(HDD)運算。採
用此演算法，我們修改了 Al-Dweik 之混合式 TPC 解碼
器。與原混合式 TPC 解碼器比較，此修改型混合式解碼
器可以再不犧牲解碼錯誤率(BER)品質的情況下省略了
大量 HDD 運算。
在第二部分，我們針對 TPC 的 HIHO 重複解碼所
產生的封閉鏈結錯誤型態(CCEP)提出更正方法。此方法
採用位元反轉(bit-flipping)解碼解開 CCEP，以提升 BER
品質。依據計算機模擬結果與 Al-Dweik 等人的方法比
較，我們的方法在高的訊號雜訊比(SNR)情況下，可在
較低的解碼複雜度下得到較佳的 BER 品質；在低的
SNR 情況下，我們的方法僅需較低的解碼複雜度就可獲
得相同的 BER 品質。
關鍵詞：SISO 解碼演算法；渦輪乘積碼；位元錯誤
率；封閉鏈結錯誤型態；疊代解碼；位元反轉解碼
Abstract
The research report is divided into two parts. In part I,
we present an efficient soft-input soft-output (SISO)
decoding algorithm for turbo product codes (TPCs). The
proposed algorithm can estimate the extrinsic information of
a row/column sequence without performing 2P–1 hard-
decision decodings (HDDs). Based on the new algorithm, a
modified version of the hybrid TPC decoder of Al-Dweik et
al., is also introduced. Compared with the hybrid TPC
decoder of Al-Dweik et al., the modified hybrid TPC
decoder can greatly reduce the number of HDDs without
noticeable bit-error rate performance loss. In Part II, we
present a novel CCEP correction approach in the HIHO
iterative TPC decoding. The proposed approach utilizes
flipping decoding to break CCEPs instead of erasure
decoding. Compared with the state-of-the-art CCEP
correction approach, Al-Dweik et. al.’s CCEP correction
approach, simulation results demonstrate that for high SNRs,
the proposed method can offer a substantial BER
performance improvement with even less computational
complexity. For low SNRs, a significant reduction in the
computation can be obtained without BER performance loss.
Keywords: SISO decoding algorithm; turbo product
codes; bit-error rate; closed-chains error pattern; iterative
decoding; flipping decoding
Part I
A New Soft-Input Soft-output Decoding Algorithm for
Turbo Product Codes
1. Introduction
Turbo product codes (TPCs) are a very useful class of
error control codes, because they have good bit-error rate
(BER) performance, a high code rate, and a simple
interleaving structure [1], [2]. Since good BER performance
depends on soft-input soft-output (SISO) iterative decoding,
a SISO algorithm for decoding row and column sequences in
a TPC is required. In 1998, Pyndiah [1] proposed a SISO
decoding algorithm for TPCs to achieve near-optimal BER
performance. However, numerous hard-decision decoding
(HDD) operations are involved in Pyndiah’sTPC iterative
decoding process (typically with four iterations), causing a
long decoding latency. In particular, the codelengths of the
component codes in the TPC are long and have a multi-error
correcting capacity [4]. To reduce the decoding latency of
TPCs, numerous works have focused on how to reduce the
number of HDDs in Pyndiah’salgorithm without noticeable
degradation of BER performance. Some investigations [2-4]
have employed the reliabilities or syndromes of test
sequences to reduce the number of required HDDs in the
Chase algorithm [5]. Recently, Al-Dweik et al. proposed a
hybrid TPC decoder, which involves Pyndiah’s SISO
decoding and hard-input hard-output (HIHO) decoding [6].
The hybrid decoder replaces the last half of Pyndiah’s SISO
iterations with four HIHO decoding iterations, such that the
total number of HDDs can be reduced. However, the hybrid
TPC decoder still requires that many HDDs be performed in
first 3.5Pyndiah’sSISO iterations.
conventional order [7], three useful properties are satisfied:
(i) HR = TS0, (ii) the codeword obtained from HDD(TSi) can
have a higher probability of being correct than that obtained
from HDD(TSj) if ei < ej, and (iii) TSi generally contains
more information about the transmitted codeword than TSj
when ei = ej with i < j. Notably, ei can be determined from
the syndrome of TSi [4, 8]. The new SISO decoding
algorithm is elucidated as follows. Figure 1 shows the flow
chart of the proposed algorithm.
For TPC(n,k,1)2, the algorithm computes the syndrome of
TS0 (= R
H) and checks whether e0 = 0. If e0 = 0, then R
H can
be assumed to be the most likely codeword such that RH
(=TS0) 0 1 1( , ,..., )nD d d d   . Hence, the extrinsic
information can be approximated by 0,1 1j j jts d   for
all j. Otherwise, Pyndiah’s SISO decoding algorithm is used
to process R.
For TPC(n,k,2)2, the algorithm computes the syndrome of
TSi and checks whether the corresponding ei = 0 from i = 0
to 2 1p  . As soon as the first ei satisfies the criterion, the
most likely codeword can be assumed to be D = TSi, and
,2 2j i j jts d     for all j. If 0ie  for all i, then the
algorithm checks whether e0 = 1. If e0 = 1, let D = HDD(TS0)
and the extrinsic information can be obtained using
1j jd  for all j. Otherwise, Pyndiah’s SISO decoding
algorithm is used to process R.
For TPC(n,k,t)2 with t3, the algorithm computes the
syndrome of TSi and checks whether the corresponding ei =
0 from i = 0 to 2 1p  . As soon as the first ei satisfies the
checking process, TSi is assumed to be the most likely
codeword, such that D = TSi and ,2 2j i j jts d     for all
j. If 0ie  for all i, then the algorithm checks whether ei = 1
from i = 0 to 2 1p  . Once the first ei = 1 is found, let D =
HDD(TSi) and 2j jd   for all j. If 1ie  for all i, then the
algorithm checks whether e0 = 2. If e0 = 2, let D = HDD(TS0)
and 1j jd  for all j. Otherwise, the extrinsic information
is calculated using Pyndiah’sSISO decoding algorithm.
Several points concerning the proposed algorithm must
be addressed. Because of the aforementioned useful
properties, no HDD operation is required to calculate the
extrinsic information if one ei = 0 is checked. Besides, in
decoding the TPC(n,k,t)2 with t3, if 0ie  for all i, then
the operations of HDD(TSj) for i < j are saved when ei = 1.
Thus, the algorithm very efficiently reduces the decoding
complexity. Finally, a modified hybrid TPC decoder can be
deduced directly from the hybrid TPC decoder of Al-Dweik
et al. [6], by replacing Pyndiah’sSISO algorithm with the
proposed SISO algorithm during the first 3.5 SISO decoding
iterations. According to the simulation results (presented in
Section IV), the modification significantly improved the
decoding complexity without noticeable loss of BER
performance.
4. Simulation Results
This section introduces a simulation that assumes BPSK
modulation over an AWGN channel and TPC(64,57,1)2,
TPC(64,51,2)2, and TPC(64,45,3)2 are considered as
examples. The number of the least reliable bits is set with
p = 4 here. As mentioned in Section III, the proposed
hybrid TPC decoder is a modified version of the hybrid
decoder of Al-Dweik et al. [6], and both involve 3.5 SISO
iterations followed by 4 HIHO iterations, but that in the
first 3.5 iterations, the former uses the new SISO
decoding algorithm (proposed in Section III) whereas the
latter uses Pyndiah’s SISO decoding algorithm. Since the
new SISO decoding algorithm can estimate the extrinsic
information of a row/column sequence without
performing 42 1 HDD operations, the modified hybrid
TPC decoder efficiently reduces the number of HDDs for
decoding a TPC. To measure the reduction of the number
of HDDs for decoding the TPC(n,k,t)2, the efficiency is
defined as follows.
mod
org
(1 ) 100%
N
N
   (2)
where Nmod and Norg denote the number of HDDs for
decoding a TPC using the modified and original hybrid TPC
TABLE I. THE EFFICIENCIES OF THE MODIFIED HYBRID TPC
DECODER FOR DECODING TPC(64,57,1)2, TPC(64,51,2)2 AND
TPC(64,45,3)2
Eb/N0
(dB)
2 2.5 3 3.3 3.5TPC
(64,57,1)2
η 6.72
%
12.3
%
40.7
%
56.7
%
63.5
%
Eb/N0
(dB)
2 2.3 2.5 2.6 2.8TPC
(64,51,2)2
η 21.7
%
42.7
%
50.8
%
54.7
%
61.7
%
Eb/N0
(dB)
2 2.2 2.4 2.5 2.6TPC
(64,45,3)2
η 31.6
%
40.5
%
49.7
%
53.3
%
56.7
%
Figure 2. BER versus Eb/No of the modified and original hybrid
decoders for TPC(64,57,1)2, TPC(64,51,2)2,and TPC(64,45,3)2.
For conventional hard iterative TPC decoding, each row
or column d of D is decoded using hard-decision decoding
(HDD) according to maximum likelihood decoding (MLD)
rule, which chooses the estimate cˆ that differs from d in the
fewest number of positions [10], i.e.
ˆ if ( , ) ( , ) [1,..., 2 ] ,i i l kH Hd d l l i   c v v d v d (3)
where iv is the ith codeword and ( , )iHd v d is the Hamming
distance between iv and d. For each iteration, the rows and
columns of D are decoded in a sequential manner. Hence,
there are n1 + n2 HDDs for an iteration. For the reason of
comparison with [9], in this paper, we focuses on the TPCs
which are constructed with two identical extended Bose-
Chaudhuri- Hocqenghem (eBCH) codes [10], i.e. C1 = C2
and denoted by eBCH(n, k, t)2 and HDD is performed using
the standard-array decoding approach.
A major flaw of HIHO iterative TPC decoding is
vulnerable to CCEPs [5], [8][9]. A CCEP are not corrected
successfully with enough iterations because the number of
errors of a CCEP in the row/column direction exceeds the
component code correction capability. In addition, extra
errors are introduced after wrong decoding performed,
which degrades BER performance significantly. An efficient
CCEP correction approach is proposed by Al-Dweik et al.
[9]. The approach adopts erasure decoding to break the
CCEP after locating the bit errors that form the CCEP. It can
be shown a code with minimum distance dmin is capable of
correcting e errors and αerasures if the following condition
is satisfied [10]:
2 1.mind e    (4)
Therefore, it makes the CCEP correction approach of Al-
Dweik et al. doubles the correction capability, i.e. 1mind  ,
provided that all 1mind  errors are located in the αerasure
positions. However, erasure decoding is sensitive to the case
that there are errors outside erasure positions. This is due to
the fact the minimum distance of the shorted code, formed
by deleting the α erasure positions, min mind d   , is
smaller than dmin. For instance, consider 4mind  and erasure
positions 3 . If there is a error existing outside the α
erasure positions, the erasure decoder will makes a wrong
decoding due to 1mind such that the total number of errors
increases. In the following section, a CCEP correction
approach is proposed to improve this problem.
3. The Proposed CCEP Correction Approach
The proposed CCEP correction method flips the bits
possibly involving in CCEPs to break CCEPs. The error
correction capability of flipping decoding is more powerful
than erasure decoding, provided that the probability that the
operated bits is erroneous is high. In the previous example, if
the three bits in the erasure positions are all erroneous, an
error existing outside the erasure positions is able to be
corrected using flipping decoding. This is because there is
only one error after the flipping operation and 4mind  is
enough to correct the error. Table I shows the probability
that the located bits is erroneous using estimating method of
Al-Dweik et al. [9]. It can be observed the probability is
very high at high signal-to-noise ratios (SNRs). At low
SNRs, flipping decoding is more likely to result in more
decoding errors than erasure decoding due to low probability.
However, it does not affect BER significantly because the
errors introduced by channel noise dominate BER at low
SNRs. For the sake of comparison with the CCEP correction
method of Al-Dweik et al. [9], the proposed CCEP
correction method follows several iterations of the non-
sequential decodings [8]. The overall decoding procedure is
described as follows:
A. Non-sequential decoding stage:
Perform the non-sequential decoding for the first i
iterations.
B. CCEP correction decoding stage:
Apply the proposed CCEP method for j iterations as
follows:
(a). Set the iterations counter z = 1.
(b). Compute the possible number of errors ˆkre for
each row, where ˆkre is defined as the Hamming
distance between kth row krd and the decoding
estimate ˆkrc corresponding to
k
rd , i.e.
ˆ( , )k kH r rd c d . Note ˆ
k
re can be computed from the
representation of syndromes [11] before HDD.
TABLE II. THE PROBABILITY THAT THE LOCATED BITS IS
ERRONEOUS USING ESTIMATING METHOD OF [9]
Eb/N0
(dB) 4.5 5 5.5 6 6.5eBCH
(32,26,1)2
Probab
-ility 0.03 0.09 0.35 0.77 0.94
Eb/N0
(dB) 5 5.5 6 6.2 6.5
eBCH
(128,120,1)2
Probab
-ility 0.11 0.36 0.71 0.86 0.95
Eb/N0
(dB) 3.5 4 4.5 5 5.2
eBCH
(32,21,2)2
Probab
-ility 0.28 0.32 0.36 0.69 0.84
The reliability threshold of the non-sequential decoding
stage is the same as [9].
Fig. 1, Fig. 2 and Fig. 3 show the BER for
eBCH(32,26,1)2, eBCH(128,120,1)2, and eBCH(32,21,2)2,
respectively. It can be observed that the proposed CCEP
correction approach offers about 0.4, 0.5, and 0.1dB coding
gain over the CCEP approach of Al-Dweik et al. [9] at BER
= 10-6 for eBCH(32,26,1)2 and BER = 10-8 for
eBCH(128,120,1)2 and eBCH(32,21,2)2. At low SNRs, the
performances of two approaches are almost the same, which
is consistent with the previous discussion; for low SNRs, the
errors introduced by channel noise dominate the BER
although extra errors are more likely to be produced using
the proposed CCEP correction approach.
The average reduced numbers of HDD operations
compared with [9], reducedN at five sampled SNRs for eBCH
(32,26,1)2, eBCH(128,120,1)2, and eBCH(32,21,2)2 are
listed in Table II. It can be observed that all the values of
reducedN are positive, which means less HDDs are required in
the overall decoding process with the proposed CCEP
correction approach than [9]. It is worth noting that the value
of reducedN is decreasing with increasing SNR, which results
from the factor that more erasure decodings are required for
low SNRs.
5. Conclusion
This work presented a novel CCEP correction approach
in HIHO iterative TPC decoding. The proposed approach
employs flipping decoding to break CCEPs instead of
erasure decoding [9]. Compared with the state-of-the-art
CCEP correction approach, Al-Dweik et. al.’s CCEP
correction approach [9], simulation results have confirmed
that for high SNRs, a significant coding gain is offered by
the proposed approach. Besides, the computational
complexity of the proposed approach is less than Al-Dweik
et. al.’s approach, especially for low SNRs. This is because
more erasure decodings are required for Al-Dweik et. al.’s
CCEP correction approach at low SNRs.
References
[1] R. M. Pyndiah, "Near-optimum decoding of product codes: block
turbo codes," IEEE Trans. Commun., vol. 46, Aug. 1998, pp. 1003-
1010.
[2] S. Dave, J. Kim, and S. C. Kwatra, "An efficient decoding algorithm
for block turbo codes," IEEE Trans. Commun., vol. 49, Jan. 2001, pp.
41-46.
[3] A. Mahran and M. Benaissa, "Adaptive Chase algorithm for block
turbo codes," Electronics Letters, vol. 39, Apr. 2003, pp. 617-619.
[4] G. Bosco, G. Montorsi, and S. Benedetto, “A new algorithm for
“hard” iterative decoding of concatenated codes,” IEEE Trans.
Commun., vol. 51, no. 8, Aug. 2003, pp. 1229-1311
[5] J. Andersen, “Product codes for optical communications,”in Proc.
ECOC 2002, vol. 3, 2002, pp. 1-2.
[6] O. SAB, “FEC techniques in submarine transmission systems,”in
Proc. OFC 2001, vol. 2, 2001, pp. TuF1-1–TuF1-3.
[7] J. Cuevas, P. Adde, S. Kerouedan, and R. Pyndiah, “”New
architecture for high data rate turbo decoding of product codes,”in
Proc. IEEE Globecom 2002, pp. 1363-1367.
[8] A. Al-Dweik and B. Sharif, “Non-sequential decoding algorithm for
hard iterative turbo product codes,”IEEE Trans. Commun., vol. 57,
Jun3 2009, pp. 1545-1549.
[9] A. Al-Dweik, and B. Sharif, “Closed-chains error correction
technique for turbo product codes,”IEEE trans. Commu., vol. 59, no.
3, March 2011, pp. 632-638
[10] S. Lin and D. J. Costello, Jr.,“Error Control Coding,”2nd ed. Pearson
Internation Edition, 2004.
[11] G. T. Chen, L. Cao, L. Yu, and C. W. Chen, "Test-pattern-reduced
decoding for turbo product codes with multi-error-correcting eBCH
codes," IEEE Trans. Commun., vol. 57, Feb. 2009, pp. 307-310.
TABLE III. THE AVERAGE REDUCED NUMBERS OF HDD
OPERATIONS COMPARED WITH AL-DWEIK ET. AL.’S APPROACH FOR
DIFFERENT TPCS
Eb/N0
(dB) 4.5 5 5.5 6 6.5
eBCH
(32,26,1)2
reducedN 5.07 1.82 0.49 0.21 0.04
Eb/N0
(dB) 5 5.5 6 6.2 6.5
eBCH
(128,120,1)2
reducedN 17.5 5.71 1.25 1.61 0.65
Eb/N0
(dB) 3.5 4 4.5 5 5.2
eBCH
(32,21,2)2
reducedN 2.49 2.13 0.07
2.4×
10
-3
8.2×
10
-4
到的 component code 的可靠性的方法。針對可靠性高的 component
codes 僅採用一般的 hard-decision decoding，可靠性低的 component
codes 才採用 Chase 所提的 soft-decision decoding 因此可將解碼複雜度
降低至現有方法的百分之四十左右，同時維持 error performance，甚
至比現有方法的 error performance 略佳。（接受發表之大會證明函及論
文全文如附件一）。
﹙二﹚A novel closed-chains error patterns correction approach for turbo product
codes。大會同樣安排於 October 18 的 Oral 3 會場發表。Turbo product
codes 的 error performance 雖然可以近 Shannon’s limit，但由於 turbo
product codes 的 row 及 column 之 component codes 解碼時採用
soft-input soft-output decoding，因此複雜度極高；如果 component codes
解 碼 時 採 用 hard-input hard-output(HIHO) decoding 則 會 產 生
closed-chains 的現象而造成 error performance 大幅下降的現象。如何採
用 HIHO decoding 而不會產生 closed-chains 的現象，乃是近年研究錯
誤控制碼的研究重點之一。本論文採用 flipping decoding 來打破
closed-chains(2011 年 Al-Dweik and Sharif 採用 state-of-the-art)來提高
HIHO decoding 解 turbo product codes 的 performance.所得到的結
果是解碼複雜度大幅下降，但不會犧牲 error performance。（接受發表
之大會證明函及論文全文如附件二）。
二、與會心得
參加此次研討會的國內人士有大部分為學校老師及一部分的研究生﹐但參
加的國內研究生很少(此次會議中，我個人有接觸到的台灣研究生僅有台科大
一位博士生及我帶去的兩位長庚大學博士生)。建議國科會或教育部應更鼓勵
研究生參加國外舉辦的國際研討會﹐以加強國內研究生的國際觀。
三、考察參觀活動(無是項活動者省略)
無
四、建議
建議國科會或教育部應更鼓勵研究生參加國外舉辦的國際研討
會﹐以加強國內研究生的國際觀。
五、攜回資料名稱及內容
論文光碟片。
六、其他
The 2nd International Conference on Electronics, Communications and Control 
(ICECC 2012) 
 
 
©  Copyright 2011 All Rights Reserved by ICECC 2012 
October 16th -18th, 2012, Zhoushan, China 
Acceptance Notification 
                                                  July 1
th
, 2012 
Dear Author, 
Congratulations! It is our great pleasure to inform you that your paper 
 
Paper ID: CC92947 
Author(s): Lu Pen-Yao,Lu Erl-Huei, 
Title: A New Soft-Input Soft-Out Decoding Algorithm for Turbo Product Codes 
has been accepted for presentation at ICECC2012 (The 2
nd
 International Conference on Electronics, 
Communications and Control ). 
The conference proceedings will be published by IEEE, which will be included in IEEE digital library and 
then be indexed by EI Compendex. 
 
Please complete all registration procedures before July 5
th
, 2012 by the registration information attached. 
Otherwise your paper will be excluded from the proceedings and then be submitted to Ei Compendex. 
 
Thank you for submitting paper to ICECC2012 and we sincerely hope that you could attend the conference. 
We would also appreciate it that you contribute your excellent work to future ICECC conferences. 
For more information, please visit the conference website:  
http://www.icecc-conf.org/ 
 
 
Best regards,  
 
ICECC2012 Organizing Committee 
 
the most likely competing codeword with jˆ jd d among
these decoded 2P codewords, where ˆand { 1, 1}.j jd d   It
has be shown in [1] that the log-likelihood ration (LLR) of
jd can be approximated by
2 2ˆ| | | |
( ) ,
4j j
R D R D
d d
       
(1)
where 2| |X Y is the squared Euclidean distance between
X and Y. Then, the corresponding normalized extrinsic
information j of the bit jd is given by ( ) .j j jd r   If
Dˆ does not exist, a reliability factor [1] is used to
estimated j , as j jd   . Once the extrinsic
informations of all bits in the TPC are calculated, the half
iteration for Pyndiah’s TPC decoding is completed. The
extrinsic information will be scaled by a scaling factor [1]
and fed to the next half iteration for the TPC decoding.
The hybrid TPC decoder of Al-Dweik et al. [6] replaces
the last half Pyndiah’s SISO decoding iteration with four
HIHO decoding iterations. Notably, the numbers of HDDs
required in a half of Pyndiah’s SISOand HIHO decoding
iteration are 2 pn and n, respectively. Since most
transmission errors can be corrected during the first 3.5
SISO iterations, successive HIHO iterations can clean the
residual errors. Thus, the hybrid TPC decoder reduces
decoding complexity while maintaining reasonable BER
performance. However, 7 2 pn HDDs are required during
3.5 SISO iterations, causing a long decoding latency. The
problem is more serious for TPCs that are built from a long
component code with multi-error correcting capability [4].
The following section proposes an efficient SISO algorithm
to replace Pyndiah’s SISO algorithm in the first 3.5 SISO
iterations of the hybrid TPC decoder, to mitigate the
problem.
III. PROPOSED SISO DECODING ALGORITHM AND
MODIFIED HYBRID TPC DECODER
Like that of Pyndiah’sSISO decoding algorithm, the
first operation of the proposed SISO decoding algorithm
perturbs the p least reliable bits of the sequence
0 1 1( , , ..., )
H H H H
nR r r r  to yield 2
p test sequences, denoted by
TSi = ,0 ,1 , 1( , , , )i i i nts ts ts  (tsi,j { 1, 1}  for all j) for i =
0,1, …, 2 1p  . Let ei and HDD(TSi) denote the number of
errors in TSi and the operation of HDD on TSi, respectively.
Since the perturbing operation is conducted in the
conventional order [7], three useful properties are satisfied:
(i) HR = TS0, (ii) the codeword obtained from HDD(TSi)
can have a higher probability of being correct than that
obtained from HDD(TSj) if ei < ej, and (iii) TSi generally
contains more information about the transmitted codeword
than TSj when ei = ej with i < j. Notably, ei can be
determined from the syndrome of TSi [4, 8]. The new SISO
decoding algorithm is elucidated as follows. Figure 1 shows
the flow chart of the proposed algorithm.
For TPC(n,k,1)2, the algorithm computes the syndrome of
TS0 (= R
H) and checks whether e0 = 0. If e0 = 0, then R
H can
be assumed to be the most likely codeword such that RH
(=TS0) 0 1 1( , ,..., )nD d d d   . Hence, the extrinsic
information can be approximated by 0,1 1j j jts d   for
all j. Otherwise, Pyndiah’s SISO decoding algorithm is used
to process R.
For TPC(n,k,2)2, the algorithm computes the syndrome of
TSi and checks whether the corresponding ei = 0 from i = 0
to 2 1p  . As soon as the first ei satisfies the criterion, the
most likely codeword can be assumed to be D = TSi, and
,2 2j i j jts d     for all j. If 0ie  for all i, then the
algorithm checks whether e0 = 1. If e0 = 1, let D = HDD(TS0)
and the extrinsic information can be obtained using
Let q = 0
T = f(t)
Let i = 0
i <
=q q=T-1
q=q+1 i=i+1
End
p2
ie
no
yes
yes
no
no
yes
algorithm
sPyndiah'using
decodingSISO
 functionfloor:
otherwise,3
3if,
)(



  tttf
1,,2,1,0for
2
)1(
and
otherwise,)(HDD
0if,
Let




 


 
nj
d
qT
TS
eTS
D
jj
i
ii


Figure 1. Flow chart of the proposed SISO decoding algorithm.
V. CONCLUSIONS AND REMARKS
This work developed a modified version of the hybrid
TPC decoder for decoding TPCs. The modified hybrid TPC
decoder replaces Pyndiah’s SISO algorithm with the new
SISO decoding algorithm, which can exploit the properties
of test sequences to estimate the extrinsic information of a
row/column sequence without fewer HDDs then 2 p times.
Simulation results indicate that the required number of
HDDs is substantially reduced using the proposed SISO
algorithm without noticeable loss of BER.
ACKNOWLEDGMENT
The authors would like to thank the National Science
Council of the Republic of China, Taiwan for financially
supporting this research under Contract No. NSC 100-2221-
E-182-040.
REFERENCES
[1] R. M. Pyndiah, "Near-optimum decoding of product codes: block
turbo codes," IEEE Trans. Commun., vol. 46, pp. 1003-1010, Aug.
1998.
[2] S. Dave, K. Junghwan, and S. C. Kwatra, "An efficient decoding
algorithm for block turbo codes," IEEE Trans. Commun., vol. 49, pp.
41-46, Jan. 2001.
[3] A. Mahran and M. Benaissa, "Adaptive Chase algorithm for block
turbo codes," Electronics Letters, vol. 39, pp. 617-619, Apr. 2003.
[4] G. T. Chen, L. Cao, L. Yu, and C. W. Chen, "Test-pattern-reduced
decoding for turbo product codes with multi-error-correcting eBCH
codes," IEEE Trans. Commun., vol. 57, pp. 307-310, Feb. 2009.
[5] D. Chase, "Class of algorithms for decoding block codes with
channel measurement information," IEEE Trans.Inform.Theory, vol.
18, pp. 170-182, Jan. 1972.
[6] A. Al-Dweik, S. Le Goff, and B. Sharif, "A Hybrid Decoder for
Block Turbo Codes," IEEE Trans. Commun., vol. 57, pp. 1229-1232,
May 2009.
[7] S. A. Hirst, B. Honary, and G. Markarian, “Fast Chase algorithm 
with an application in turbo decoding,” IEEE Trans. Commun., vol.
49, pp. 1693-1699, Oct. 2001.
[8] W. W Peterson and E. J. Weldon, Jr., Error-Correcting Codes, MIT
Press, Cambridge, MA, 1972.
The 2nd International Conference on Electronics, Communications and Control 
(ICECC 2012) 
 
 
©  Copyright 2011 All Rights Reserved by ICECC 2012 
October 16th -18th, 2012, Zhoushan, China 
Acceptance Notification 
                                                  July 1
th
, 2012 
Dear Author, 
Congratulations! It is our great pleasure to inform you that your paper 
 
Paper ID: CC88375 
Author(s): Lu pen-yao,Lu Erl-Huei, 
Title: A Novel Closed-Chains Error Patterns Correction Approach for Turbo Product Codes 
has been accepted for presentation at ICECC2012 (The 2
nd
 International Conference on Electronics, 
Communications and Control ). 
The conference proceedings will be published by IEEE, which will be included in IEEE digital library and 
then be indexed by EI Compendex. 
 
Please complete all registration procedures before July 5
th
, 2012 by the registration information attached. 
Otherwise your paper will be excluded from the proceedings and then be submitted to Ei Compendex. 
 
Thank you for submitting paper to ICECC2012 and we sincerely hope that you could attend the conference. 
We would also appreciate it that you contribute your excellent work to future ICECC conferences. 
For more information, please visit the conference website:  
http://www.icecc-conf.org/ 
 
 
Best regards,  
 
ICECC2012 Organizing Committee 
 
fewest number of positions [10], i.e.
ˆ if ( , ) ( , ) [1,..., 2 ] ,i i l kH Hd d l l i   c v v d v d (1)
where iv is the ith codeword and ( , )iHd v d is the Hamming
distance between iv and d. For each iteration, the rows and
columns of D are decoded in a sequential manner. Hence,
there are n1 + n2 HDDs for an iteration. For the reason of
comparison with [9], in this paper, we focuses on the TPCs
which are constructed with two identical extended Bose-
Chaudhuri- Hocqenghem (eBCH) codes [10], i.e. C1 = C2
and denoted by eBCH(n, k, t)2 and HDD is performed using
the standard-array decoding approach.
A major flaw of HIHO iterative TPC decoding is
vulnerable to CCEPs [5], [8][9]. A CCEP are not corrected
successfully with enough iterations because the number of
errors of a CCEP in the row/column direction exceeds the
component code correction capability. In addition, extra
errors are introduced after wrong decoding performed,
which degrades BER performance significantly. An efficient
CCEP correction approach is proposed by Al-Dweik et al.
[9]. The approach adopts erasure decoding to break the
CCEP after locating the bit errors that form the CCEP. It
can be shown a code with minimum distance dmin is capable
of correcting e errors and α erasures if the following
condition is satisfied [10]:
2 1.mind e    (2)
Therefore, it makes the CCEP correction approach of
Al-Dweik et al. doubles the correction capability, i.e.
1mind  , provided that all 1mind  errors are located in theα
erasure positions. However, erasure decoding is sensitive to
the case that there are errors outside erasure positions. This
is due to the fact the minimum distance of the shorted code,
formed by deleting the αerasure positions, min mind d   ,
is smaller than dmin. For instance, consider 4mind  and
erasure positions 3 . If there is a error existing outside
the αerasure positions, the erasure decoder will makes a
wrong decoding due to 1mind such that the total number of
errors increases. In the following section, a CCEP correction
approach is proposed to improve this problem.
III. THE PROPOSED CCEP CORRECTION APPROACH
The proposed CCEP correction method flips the bits
possibly involving in CCEPs to break CCEPs. The error
correction capability of flipping decoding is more powerful
than erasure decoding, provided that the probability that the
operated bits is erroneous is high. In the previous example,
if the three bits in the erasure positions are all erroneous, an
error existing outside the erasure positions is able to be
corrected using flipping decoding. This is because there is
only one error after the flipping operation and 4mind  is
enough to correct the error. Table I shows the probability
that the located bits is erroneous using estimating method of
Al-Dweik et al. [9]. It can be observed the probability is
very high at high signal-to-noise ratios (SNRs). At low
SNRs, flipping decoding is more likely to result in more
decoding errors than erasure decoding due to low
probability. However, it does not affect BER significantly
because the errors introduced by channel noise dominate
BER at low SNRs. For the sake of comparison with the
CCEP correction method of Al-Dweik et al. [9], the
proposed CCEP correction method follows several iterations
of the non-sequential decodings [8]. The overall decoding
procedure is described as follows:
I. Non-sequential decoding stage:
Perform the non-sequential decoding for the first i
iterations.
II. CCEP correction decoding stage:
Apply the proposed CCEP method for j iterations as
follows:
(a). Set the iterations counter z = 1.
(b). Compute the possible number of errors ˆkre for
each row, where ˆkre is defined as the Hamming
distance between kth row krd and the decoding
estimate ˆkrc corresponding to
k
rd , i.e.
ˆ( , )k kH r rd c d . Note ˆ
k
re can be computed from the
representation of syndromes [11] before HDD.
TABLE I. THE PROBABILITY THAT THE LOCATED BITS IS
ERRONEOUS USING ESTIMATING METHOD OF [9]
Eb/N0
(dB) 4.5 5 5.5 6 6.5eBCH
(32,26,1)2
Probab
-ility 0.03 0.09 0.35 0.77 0.94
Eb/N0
(dB) 5 5.5 6 6.2 6.5
eBCH
(128,120,1)2
Probab
-ility 0.11 0.36 0.71 0.86 0.95
Eb/N0
(dB) 3.5 4 4.5 5 5.2
eBCH
(32,21,2)2
Probab
-ility 0.28 0.32 0.36 0.69 0.84
and eBCH (32, 21, 2)2, are used for the simulation. The
performance is compared with the CCEP correction
approach of Al-Dweik et al. [9]. 6 and 4 iterations are used
in the non-sequential and CCEP correction decoding,
respectively, in the overall decoding procedure with the
proposed or Al-Dweik et al.’s CCEP correction approach.
The reliability threshold of the non-sequential decoding stage
is the same as [9].
Fig. 1, Fig. 2 and Fig. 3 show the BER for
eBCH(32,26,1)2, eBCH(128,120,1)2, and eBCH(32,21,2)2,
respectively. It can be observed that the proposed CCEP
correction approach offers about 0.4, 0.5, and 0.1dB coding
gain over the CCEP approach of Al-Dweik et al. [9] at BER
= 10-6 for eBCH(32,26,1)2 and BER = 10-8 for
eBCH(128,120,1)2 and eBCH(32,21,2)2. At low SNRs, the
performances of two approaches are almost the same, which
is consistent with the previous discussion; for low SNRs, the
errors introduced by channel noise dominate the BER
although extra errors are more likely to be produced using
the proposed CCEP correction approach.
The average reduced numbers of HDD operations
compared with [9], reducedN at five sampled SNRs for eBCH
(32,26,1)2, eBCH(128,120,1)2, and eBCH(32,21,2)2 are
listed in Table II. It can be observed that all the values of
reducedN are positive, which means less HDDs are required
in the overall decoding process with the proposed CCEP
correction approach than [9]. It is worth noting that the
value of reducedN is decreasing with increasing SNR, which
results from the factor that more erasure decodings are
required for low SNRs.
V. CONCLUSION
This work presented a novel CCEP correction approach
in HIHO iterative TPC decoding. The proposed approach
employs flipping decoding to break CCEPs instead of
erasure decoding [9]. Compared with the state-of-the-art
CCEP correction approach, Al-Dweik et. al.’s CCEP
correction approach [9], simulation results have confirmed
that for high SNRs, a significant coding gain is offered by
the proposed approach. Besides, the computational
complexity of the proposed approach is less than Al-Dweik
et. al.’s approach, especially for low SNRs. This is because
more erasure decodings are required for Al-Dweik et. al.’s
CCEP correction approach at low SNRs.
ACKNOWLEDGMENT
The authors would like to thank the National Science
Council of the Republic of China, Taiwan for financially
supporting this research under Contract No. NSC 100-2221-
E-182-040.
REFERENCES
[1] R. M. Pyndiah, "Near-optimum decoding of product codes: block
turbo codes," IEEE Trans. Commun., vol. 46, Aug. 1998, pp. 1003-
1010.
[2] S. Dave, J. Kim, and S. C. Kwatra, "An efficient decoding algorithm
for block turbo codes," IEEE Trans. Commun., vol. 49, Jan. 2001, pp.
41-46.
[3] A. Mahran and M. Benaissa, "Adaptive Chase algorithm for block
turbo codes," Electronics Letters, vol. 39, Apr. 2003, pp. 617-619.
[4] G. Bosco, G. Montorsi, and S. Benedetto, “A new algorithm for
“hard” iterative decoding of concatenated codes,” IEEE Trans.
Commun., vol. 51, no. 8, Aug. 2003, pp. 1229-1311
[5] J. Andersen, “Product codes for optical communications,”in Proc.
ECOC 2002, vol. 3, 2002, pp. 1-2.
[6] O. SAB, “FEC techniques in submarine transmission systems,”in
Proc. OFC 2001, vol. 2, 2001, pp. TuF1-1–TuF1-3.
[7] J. Cuevas, P. Adde, S. Kerouedan, and R. Pyndiah, “”New
architecture for high data rate turbo decoding of product codes,”in
Proc. IEEE Globecom 2002, pp. 1363-1367.
[8] A. Al-Dweik and B. Sharif, “Non-sequential decoding algorithm for
hard iterative turbo product codes,”IEEE Trans. Commun., vol. 57,
Jun3 2009, pp. 1545-1549.
[9] A. Al-Dweik, and B. Sharif, “Closed-chains error correction
technique for turbo product codes,”IEEE trans. Commu., vol. 59, no.
3, March 2011, pp. 632-638
[10] S. Lin and D. J. Costello, Jr.,“Error Control Coding,”2nd ed. Pearson
Internation Edition, 2004.
[11] G. T. Chen, L. Cao, L. Yu, and C. W. Chen, "Test-pattern-reduced
decoding for turbo product codes with multi-error-correcting eBCH
codes," IEEE Trans. Commun., vol. 57, Feb. 2009, pp. 307-310.
TABLE II. THE AVERAGE REDUCED NUMBERS OF HDD
OPERATIONS COMPARED WITH AL-DWEIK ET. AL.’S APPROACH FOR
DIFFERENT TPCS
Eb/N0
(dB) 4.5 5 5.5 6 6.5
eBCH
(32,26,1)2
reducedN 5.07 1.82 0.49 0.21 0.04
Eb/N0
(dB) 5 5.5 6 6.2 6.5
eBCH
(128,120,1)2
reducedN 17.5 5.71 1.25 1.61 0.65
Eb/N0
(dB) 3.5 4 4.5 5 5.2
eBCH
(32,21,2)2
reducedN 2.49 2.13 0.07
2.4×
10
-3
8.2×
10
-4
100年度專題研究計畫研究成果彙整表 
計畫主持人：盧而輝 計畫編號：100-2221-E-182-040- 
計畫名稱：快速區塊渦輪碼解碼演算法及平行解碼電路的研究與設計 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
