 II 
Abstract 
 
We study how to extract randomness from some sources. For some function  f(x) 
and some distribution χ , we say that such a source has computational min-entropy 
k if any circuit of size 2k can only predict f(x) correctly with probability at most 
(1/2k) given input x sampled from χ . We first show that it is impossible to have a 
seedless extractor to extract from one single source containing computational 
entropy. Then we show that it becomes possible if we are allowed a seed which is 
weakly random (instead of perfectly random) but contains some statistical min-
entropy, or even a seed which is not random at all but contains some 
computational min-entropy. This can be seen as a step toward extending the study 
of multisource extractors from the traditional, statistical setting to a computational 
setting. We reduce the task of constructing such extractors to a problem in 
computational learning theory: learning linear functions under arbitrary 
distribution with adversarial noise, and we provide a learning algorithm for this 
problem. In fact, this problem is a well-recognized one in computational learning 
theory and variants of this problem have been studied intensively before. Thus, in 
addition to its application to extractors, our learning algorithm also has 
independent interest of its own. 
 
   Then, we consider computational independent-symbol sources. Just as an 
independent-symbol source, which is a distribution X=(X1,…,Xn) over the set 
({0,1}d)n where these n symbols X1,…,Xn are independent, and the whole min-
entropy of X is k, a computational independent-symbol source consists of n 
mutually independent parts, (f1(X1)|X1)，…，(fn(Xn)|Xn), each fi(Xi) of length d 
such that for each i if given input xi sampled from Xi, any circuit of size s can only 
predict fi(Xi) with probability at most 2-ki for some ki≤d, and the sum of ki's is k. 
We generalize Impagliazzo’s well-known hardcore set lemma [Imp95] to show 
that the extractor for independent-symbol sources in [LLT06] still works for 
computational independent-symbol sources. In fact, the result of computational 
extractors for computational independent-symbol sources implies a generalization 
of the well-known XOR lemma. Finally, we provide a size upper bound on a 
binary hardcore set in any black-box construction of hardcore set. 
 
Keywords: randomness extractors, computational min-entropy, learning linear 
functions, independent-symbol sources, computational independent-symbol 
sources, generalized hardcore set lemma, generalized XOR lemma, blak-box 
construction, size of hardcore set.
 1 
一、前言:背景與文獻探討 
在資訊科學的許多領域裡，隨機性(Randomness)既是非常有用的工具，也
是非常重要的計算資源。例如，就許多難以處理的問題而言[MR95, Gol98]，隨
機演算法(probabilistic algorithm)往往是效率較高的解決辦法，這是因為相
較於決定性演算法(deterministic algorithm)，當加入隨機性之後，使得時間
以及空間的複雜度(time and space complexity)可能會更有效率。又例如，在
密碼學的應用裡，我們常藉由隨機的特性來隱藏秘密，以避免在傳輸的過程中
被第三者竊聽而洩露資訊。因此，是否能取得真正的隨機元(truly random 
bits)對於相關應用的影響非常深遠，但遺憾的是，真正的隨機元並非垂手可
得。類似於我們製造亂數表的情況，在真實世界中，我們幾乎只能退而求其次
地使用一個決定性的函數(deterministic function)從一些可取樣的弱隨機源
(weak random sources)中萃取出真正的隨機元，其中，所謂的弱隨機源只能保
證任一個字串出現的機率都不會非常高。我們說一個弱隨機源的「統計觀點最
小熵(statistical min-entropy)」為 k則表示每個字串出現的機率都不會超過
2 k− ，直覺上我們會認為這樣的弱來源「包含」k個隨機元。 
 
• 萃取器 (Extractors) 
1988 年時，Chor 等人證明不存在任何決定性的函數(deterministic 
function)可以從單一個 statistical min-entropy < n 的弱隨機源中萃取出
一個隨機元[CG88]。於是，研究者們試圖在單一個弱隨機源外，再加上一個很
短(相較於弱隨機源的長度)的真正隨機元，以萃取出隨機元。我們稱這個很短
的 真 正 隨 機 元 為 種 子 (seed) ， 而 這 種 萃 取 器 為 種 子 萃 取 器 (seeded 
extractor)。在近幾十年來，國內外的理論學家們都致力於建造出使用最少種
子，而能從各種弱隨機源中萃取出非常靠近真正隨機元的種子萃取器，如
[ILL89、Zuc97、RSW00、RVW00、TSZS01、TSUZ01]等。最後終於造出了幾近完
美的種子萃取器[LRVW03]。 
然而在使用種子萃取器時，我們仍然需要種子，其為一些真正的隨機元。
在一些應用中，我們可以解決此問題(比如在解隨機 BPP 問題中列舉所有可能的
種子)，而在其他應用中，則又回歸到最初的問題: 如何獲得一些真正的隨機元
呢? 這個爭議讓學者們轉而建造不需種子輔助的萃取器，即決定性萃取器
(deterministic extractors or seedless extractors)。 
當弱隨機源擁有某些特殊的性質時，我們確實可以從一個隨機源中萃取隨
機元。一個(n, k)-固定某些位元的來源 (A (n,k)-bit fixing source) 為一
個在{ }0,1 n 的分布 ( )1, , nX X X= ⋯ ，其中 n-k 個位元是固定的，而其餘的 k個
位元則是均等的(uniform)且彼此獨立的(independent)。Kamp 等人[KZ03]提出
從一個固定某些位元的來源中萃取出一些隨機元的方法，而 Gabizon 等人則在
2004 年提出改進的方法使之萃取出幾乎所有隨機源所含的隨機性[GRS04]。 
 3 
• 學習 parity函數的演算法 
(Algorithms for learning parity functions) 
在計算學習理論(computational learning theory)中，學習 parity 函數
是一個最基本的問題。令 ( ) { }1, , 0,1 nnx x x= ∈⋯ ，則一個 parity 函數是一個對
某個 { }1, 2, ,T n⊆ ⋯ 的函數 ( ) ii Tf x x∈= ⊕ 。在此學習模型(learning model)中，我
們有一個想學習的函數 { } { }: 0,1 0,1nf → ，以及一個在{ }0,1 n上的分佈 W，從中
我們可以取樣 w並獲得訓練樣本(training example)(w,q(w))，其中 q 為某個
函數。如果我們得到的樣本都是正確的，亦即 ( ) ( ) ,q w f w w= ∀ 時，則我們可以
對 W作多次取樣後利用所獲得的訓練樣本作高斯消去法(Gaussian elimination)
則可獲得我們想要學習的函數 f。但是當我們所獲得的樣本可能被雜訊影響而
變得不一定正確時，如何從此種模型中學習函數 f就變成一個具有挑戰性的問
題了。一般廣被討論的是以下的兩種雜訊模型: 隨機雜訊(random noise)跟對
手雜訊(adversarial noise)。在隨機雜訊的模型中，針對每一個訓練樣本都是
獨立的並且最多有η 的機率使得 q(w)≠f(w)。而在對手雜訊的模型中則是當 w
是依照分布 W所取樣時，最多有η 比例的 w使得 q(w)≠f(w)。不難看出，隨機
雜訊模型要比對手雜訊模型更容易分析。另外，對手雜訊的模型亦可看成是不
可知的模型(agnostic model)[KSS94]，而此種模型確實符合我們在現實生活中
所可能遭遇的現象。 
針對在隨機雜訊模型中學習一個有 n個變數的 parity 函數且
( )1/ 2 1η ≤ − Ω 的問題，目前最好的結果為 Blum, Kalai,和 Wasserman[BKW03]
於 2003 年提出的一個需要 ( )log2 n nO 個樣本且時間複雜度為 ( )log2 n nO 的演算法，更值
得一提的是此演算法對任何分布 W都是有效的。而在對手雜訊模型中，此問題
跟條列解碼(list decode) Hadamard codes 有關。然而目前已知的結果都只能
作用在 W 為一個均等分布的情況[GL89, FGKP06]。最近，Feldman 等人[FGKP06]
証明當 W 為一個均等分布時，則可以利用針對隨機雜訊模型的演算法來造出針
對對手雜訊模型的演算法。因此利用上述[BKW03]的演算法，則可以得到一個需
要 ( )log2 n nO 個樣本且時間複雜度為 ( )log2 n nO 的演算法，但其中 W 為一個均等分布。 
 
 
 5 
出一些隨機性，使得即使在給定 X時，這些隨機性在某些大小的電路看來幾乎
是非常隨機的。本計劃考慮放寬對隨機源的限制，試著從一個擁有
statistical min-entropy 以及另一個只擁有 computational min-entropy 的
兩個隨機源中萃取出隨機元。最後希望能從兩個都只擁有 computational min-
entropy 的隨機源中萃取出隨機元。 
 
 
• 考慮利用任何分布來學習一個線性函數 
在之前的研究結果中，要在對手雜訊模型中學習一個 parity 函數都需
要分佈 W為一個均等分布。因此，在學習理論中，如何在對手雜訊模型以及
W 為任意一個分布時學習一個 parity 函數是一個令研究者們很感興趣的問
題。在本計劃中，我們希望能解答這個問題，甚至更進一步地，我們希望即
使在 W為任意一個分布時，仍能在對手雜訊模型中學習一個線性函數。 
事實上，由之前的研究我們可以得知在此種模型下的學習演算法跟從一
個擁有 statistical min-entropy 的隨機源和另一個獨立但只擁有
computational min-entropy 的隨機源中萃取隨機性有很大的相關性。 
 
 
• 從只擁有計算觀點獨立符號的隨機來源萃取隨機元 
在統計的觀點中，我們已經知道可以從單一個獨立符號來源中萃取隨機
性。在本計劃中，我們考慮計算的獨立符號來源(computational independent-
symbol source)。如同獨立符號來源，每個計算的(n,D,k,s)-來源 
(computational (n,D,k,s)-source) ( ) ( ) ( )1 1 n nV X V X V X= ⋯ ，包含 n個彼
此獨立的部分 ( ) ( )1 1 , , n nV X V X⋯ ，其中每個 iV 都是分布在[D]={1,2,…,D}中，
而 iX 則是分布在{ }0,1 iℓ 上，且對每一個 { }1, 2, ,i n∈ ⋯ ，和每一個大小為 s的電
路 C， ( )Pr 2 iki iC X V − = ≤  對某個值 logik D≤ ，並滿足 1n ii k k= =∑ 。我們將試
著證明在統計觀點中針對獨立符號來源的萃取器亦可從此種計算的獨立符號來
源中萃取出隨機元。 
 
 7 
步放寬對隨機源的限制，希望能從兩個都只擁有 computational min-
entropy 的隨機源中萃取出隨機元。我們利用隨機方法證明無法從單一個長
度為 n，computational min-entropy 為 n-2 的隨機源中萃取出隨機元，即
使我們僅希望萃取出一個隨機元。 
考慮[LLTT05]中針對兩個弱隨機源的萃取器 { } { } { }mnnExt 1,01,01,0: →× ，
其中 m|n。在 [LLTT05]中，我們把任何一個 { }nv 1,0∈ ，看成一個 ℓ-
dimensional vector ( )lvv ,...,1 ， 其 中 ℓ=n/m ， 而 定 義
( ) ( )∑ ⋅=≡
i
iim
myxyxyxExt mod,, 。我們希望能證明對任何一個只擁有
computational min-entropy 的隨機源(V|X)與另一個擁有 statistical min-
entropy 的隨機源 W，任何的小電路都無法分辨 ( )
m
WVWX ,,, 以及
( )UWX ,, 。我們利用反證法，假設存在一個小電路可以分辨 ( )
m
WVWX ,,,
以及 ( )UWX ,, 。則可以推得會存在一台推測器 (predictor) Q 使得對許多
的 ( )vx, ，給定 x,w 之後，Q 有足夠的機率可以猜對
m
wv, 的值。給定一對此
種 ( )vx, ，我們希望能有足夠的機率可以從 x 猜對 v。而這個問題可以轉化成
由一些有誤差的訓練樣本，即(w,q(w))其中 q(w)=Q(x,w)，中學習一個線性
函數
m
v ⋅, 的問題。對此，我們建造了一個可以輸出所有可能的線性函數之學
習演算法。最後我們從這些可能的線性函數中隨機選一個當我們的輸出，我
們可以證明此輸出所包含的函數並不會太多，因此有足夠的機率會猜中正確
的 v，而這就會跟(V|X)有足夠 computational min-entropy 的假設相矛盾。
由以上的方法，我們可以證明[LLTT05]中的萃取器可以從一個擁有
computational min-entropy 為 ( )kkOknk log1 +−= 的隨機源與另一個擁有
statistical min-entropy 為 k的隨機源中萃取出隨機元。 
最後我們考慮由兩個只擁有 computational min-entropy 的隨機源萃取出
隨機元。我們發現如果一個隨機源 (W|Y) 擁有 computational min-entropy 
k，則 W 所擁有的 statistical min-entropy 至少為 k。經由以上的觀察，我
們就可以利用上述的証明方式證明[LLTT05]的萃取器亦可從一個擁有
computational min-entropy 為 ( )kkOknk log1 +−= 的隨機源與另一個擁有
computational min-entropy 為 k的隨機源中萃取出隨機元。 
 
• 利用任何分布來學習一個線性函數 
在建造這一種萃取器時，我們也發現可以將問題轉化為計算學習理論的問
題，也就是考慮在任意機率分布且具有對手雜訊的情況下，如何學習一個線
性函數的問題。我們也針對這個學習問題提出了一個學習演算法。具體而
言，在之前的研究結果中，要在對手雜訊模型中學習一個 parity 函數都需要
分佈 W 為一個均等分布。因此，在學習理論中，如何在對手雜訊模型以及 W
為任意一個分布時學習一個 parity 函數是一個令研究者們很感興趣的問題。
在本計劃中，我們希望能解答這個問題，甚至更進一步地，我們希望即使在
 9 
• 建造針對計算獨立符號來源的萃取器 
我們首先利用[STV01]的方法以及延伸的核心集引理證明對每一個來源
( )i iV X 都存在一個來源 iY ，使得任何小電路都無法分辨 ( ),i iX V 以及
( ),i iX Y ， 且 來 源 ( )i iY X 的 statistical min-entropy 與 ( )i iV X 的
computational min-entropy 有關。所以給定一個計算的(n,D,k,s)-來源
( ) ( ) ( )1 1 n nV X V X V X= ⋯ ， 其 中 ( )2 22 logmk D≥ Ω ， 對 任 何 大 小 為
( )( )2logs n nkDΩ 的電路 C， 
( ) ( )
2
1 1 1 1
2 logPr , , , , , 1 Pr , , , , , 1
m
n n n n
nC X X V V C X X Y Y
k
   = − = ≤   ⋯ ⋯ ⋯ ⋯ 。 
進一步地，我們可以證明對任何大小為 ( )( )2logs n nkDΩ 的電路 C，  
2
1 1
1 1
2 logPr , , , 1 Pr , , , 1
mn n
n i n i
i i
nC X X V C X X Y
k
= =
      
= − = ≤      
      
∑ ∑⋯ ⋯ 。 
另 一 方 面 ， 有 ( )log1 k De−Ω− 的 機 率 ， 即 使 在 知 道 1, , nX X⋯ 後 ， 來 源
1 nY Y Y= ⋯ 的 min-entropy 至 少 也 有 ( )logk DΩ 。 再 加 上
( ) ( )1 1 , , n nV X V X⋯ 是彼此獨立的，我們可推得 1, , nY Y⋯ 也是彼此獨立的。因
此，我們可以利用[LLT06]中， 
( )1 1, , nn iiExt V V V== ∑⋯  
 是針對獨立符號來源的萃取器的結果，證明 
( ) ( )( ) ( )22 log1 11, , , , , , , mk Dnn i n miX X Y X X U e−Ω=∆ ≤∑⋯ ⋯ 。 
結合以上的結果，我們推得對任何大小為 ( )( )2logs n nkDΩ 的電路 C，  
( ) ( ) 21 11 2 logPr , , , 1 Pr , , , 1 mnn i n mi nC X X V C X X U O k=     = − = ≤       ∑⋯ ⋯ ， 
亦即 ( )1 1, , nn iiExt V V V== ∑⋯ 為針對計算獨立符號來源的萃取器。 
 
 
• 黑箱子建造法中二元核心集大小的上限 
在延伸的核心集引理中，我們證明存在若干個二元核心集，其大小總和
至少為 ( )2 2δ ℓ。大家可能會猜想是否存在單一個夠大的核心集，比如說大小
為 ( )2Dδ ℓ 。我們最後證明一個對任何黑箱子建造法的二元核心集大小的上
限。假如一個演算法 Dec(⋅)滿足對任意的函數 { } [ ]: 0,1f D→ℓ ，以及任何的函
數集合 [ ]{ }, 2IG g I D I= ⊆ = ，其中任何在G 裡面的函數 Ig ，以及任何大小
 11 
四、結果與討論 
• 獨立符號源萃取器的建造與簡化版本的證明 
(此部份之結果發表於畢業論文 [Lee 10]) 
由於在[LLT06]中的隨機散步其每一步所對應的矩陣即為一個循環矩陣
(circulant matrix)，且此種矩陣的特徵向量 (eigenvectors) 是互相垂直的，我
們即可利用此矩陣的特徵值 (eigenvalues) 來幫助我們證明每走一步後所得到的
分布都會比原先的分布更接近均等分布，且得到一個比[LLT06]更好的結果。更精
確 地說， 我們考 慮在 ZM 上做隨機散步，目前的分布為 P=(P1,…, PM), 
U=(1/M,…,1/M)為均等分布，而利用一個 min-entropy 為 k 的來源 Xi 走一步隨機
散步之後所得到的分布為 P’。在[LLT06]中，我們可以證明 
.
log4
1' log42
22
2
2
2
2
2 DM
k
eUP
DM
kUPUP
−
⋅−≤





−⋅−≤−  
然而，利用循環矩陣的特性，我們可以證明 
.'
1
1222
2
2
2
2
2
−
−
−
⋅−≤− M
k
eUPUP  
我們針對獨立符號源建造與證明萃取器時，雖然結果與 [KRVZ06] 類似，但所採
取的方法卻相當不同。我們也發現傳統上為了增加最小熵都會同時使用加法與乘
法，但這個證明卻顯示其實只要使用加法即可增加最小熵，雖然缺點是最小熵增加
的速度相對之下比較慢。 
 
• 關於計算觀點 min-entropy的不可能結果 
(此部份之結果發表於 [LLT11a]，原文請參看附錄) 
 
[定理] 對於任何自然數 n與 1n ，且對於任何函數 :{0,1} {0,1}nEXT → ，只要 1 3n n>
成立，則必存在決定性的函數 1:{0,1} {0,1}n nf → ，使得對於所有
1n
Uχ = ，有
( ( ) | ) 2cH f nχ χ = − ，且 ( ( ))EXT f x 的值在任何 x都會一樣。因此，我們可以很容
易區別隨機元與 ( ( ))EXT f x 。 
 
這個結果顯示：在沒有種子的前提下，即使 computational min-entropy 高達
2n − ，我們也無法從弱隨機源萃取出真正的隨機元。另外，這個結果可以和已知
的統計式萃取器[CG88]相互比較。 
 
 13 
 
 
• 核心集的推廣 
(此部份之結果發表於 [LLT11a]，原文請參看附錄) 
 
我們將 Impagliazzo 著名的 hardcore lemma 加以推廣，得到下列結果： 
 
[Lemma] 令 : [ ]f X V→ 為(δ,s)-hard 的函數，其中 11 (1 )Lδ γ≥ − − ， [ 1]L V∈ − 且
(0,1)γ ∈ 。則對於任何 0ε > ，都存在 ' / ( ,1/ , log(1/ ))s s poly V ε γ= 與
1
V
I
L
 
∈ + 
使得
f 具有密度為 1| | / | | 1
V
H X
L
γ  ≥  + 
且難度為 ( , , ')I sε -hard 的核心集 IH 。 
 
我們可以進一步將上述的想法推廣到一般的函數 : [ ]f X V→ ，其中 3V ≥ 。 
 
[Lemma] 假設對於某個 [ ]I V⊆ ， : [ ]f X V→ 在 X 上不具有難度為 ( , , ')I sε -hard 的
核心集。則存在輸入的一個子集合 1( )IT f I−⊆ ，該子集在 X 中的密度小於ρ，也
存在尺寸大小為 2| | ((1/ ) log(1/ ))IA O ε ρ≤ 的電路集合 ( ')IA SIZE s⊆ 使得對於所有
1( )x f I−∈ 下列成立: 
1Pr [ ( ) ( )] | |IA A A x f x I∈ = > 。 
 
 
 15 
( )( ) 1 2
ii kδ −≥ − 且 ( )
[ ]
i
i t
k k
∈
= ∑ 。令 (1) ( )( , , )tχ χ χ= … ，其中每一個 ( )iχ 都是在 ( )iX 上的獨
立均等分布。令 (1) (1) ( ) ( )( ) ( ( ), , ( ))t tf f fχ χ χ= … 。對於任何不具有種子的 t-來源
( / 7, )k ε -萃取器 : ({0,1} ) {0,1}l t mEXT → ，若此萃取器可被大小為 0( )SIZE s 的電路計
算，則存在某一組 2 2( / )( 1) 2 k tltε ε −Ω≤ + + 與 0/ (2 ,1/ )ls s poly sε≥ − ，使得分布
( , ( ( ), ))dEXT f Uχ χ 與 ( , )mUχ 不存在 ( , )sε -distinguisher(識別器)。 
 
• 電路大小的差距 
(此部份之結果發表於 [LLT11b]，原文請參看附錄) 
 
[定理] 假設 (1)V ω≥ ，0 1 (4log ) /V Vδ< ≤ − ，0 1/ 3ε< ≤ ，0 1ρ< < ，且
1 3 2(( / ) log(1/ ))kS V k ε ρ+≥ Ω 。考慮任何配備有 oracle 的演算法，該演算法使用長
度 ( | |)o Xτ δ≤ 的 advice(建議)，並針對
,X VF 中的函數實現一個核心集的
( , , , , )k Sδ ε ρ -黑箱子證明。則此演算法詢問 oracle 的次數至少為
2(( / ) log(1/ ))Vk ε δΩ 。 
 
 17 
fixing sources by obtaining an independent seed. In Proceedings of 
the 45th Annual IEEE Symposium on Foundations of Computer 
Science, 2004. 
[HILL99] J. Håstad, R. Impagliazzo, L. A. Levin, and M. Luby. A 
pseudorandom generator from any one-way function. SIAM J. 
Comput., 28(4):1364--1396, 1999. 
[HLR07] C. Y. Hsiao, C. J. Lu, and L. Reyzin. Conditional computational 
entropy, or toward separating pseudoentropy form compressibility. 
In Proc. Advances in Cryptology-EUROCRYPT07, 2007. 
[IJKW08] R. Impagliazzo, R. Jaiswal, V. Kabanets, and A. Wigderson. Uniform 
direct product theorems:  simplified, optimized, and derandomized.  
In Proc.40th Annual ACM Symposium on Theory of Computing 
(STOC’08), pages 579–588, 2008. 
[ILL89] R. Impagliazzo, L. A. Levin, and M. Luby. Pseudorandom generation 
from one-way functions. In Proceedings of the 21st ACM 
Symposium on Theory of Computing, 1989. 
[Imp95]  R. Impagliazzo, Hard-core distributions for somewhat hard 
problems. In: FOCS 1995, pp. 538–545 (1995) 
[KMV08] A. Kalai, Y. Mansour, and E. Verbin, “On agnostic boosting and 
parity learning,” in Proc. 40th Annu. ACMSymp. Theory Comput. 
(STOC’08), pp. 629–638. 
[KRVZ06] J. Kamp, A. Rao, S. Vadhan, and D. Zuckerman. Deterministic 
Extractors for Small-Space Sources. In Proceedings of the 38th 
Annual ACM Symposium on Theory of Computing (STOC `06), 
pages 691-700, May  2006. 
[KSS94] M. Kearns, R. Schapire, and L.Sellie. Toward efficient agnostic 
learning. Machine Learning, 17(2/3): 115-142, 1994.  
[KZ03] J. Kamp and D. Zuckerman, Deterministic extractors for bit-fixing 
sources and exposure-resilient cryptography. In Proceedings of the 
44th Annual IEEE Symposium on Foundations of Computer Science, 
2003. 
[KZ07] J. Kamp and D. Zuckerman, Deterministic extractors for bit-fixing 
sources and exposure-resilient cryptography. SIAM Journal on 
Computing, 36(5):1231–1247, 2007. 
[Lee10] C. J. Lee, Seedless Eztractors: Constructions and Analysis, Doctor 
dissertation, National Chiao Tung University, Hsinchu, Taiwan, 2010. 
[LLTT05] C. J. Lee, C. J. Lu, S. C. Tsai, and W. G. Tzeng. Extracting 
randomness from n independent weak random sources. IEEE 
Transactions on Information Theory (SCI), 51(6) 2224-2227, 2005. 
[LLT06]  C. J. Lee, C. J. Lu, and S. C. Tsai, Deterministic extractors for 
independent-symbol sources. In Proceedings of the 33rd 
International Colloquium on Automata, Languages and 
Programming (ICALP), pages 84-95, 2006. 
[LLT11a]  C. J. Lee, C. J. Lu, and S. C. Tsai, Extracting Computational Entropy 
and Learning Noisy Linear Functions, IEEE Transactions on 
Information Theory, Vol. 57(8) 5485-5496, 2011. 
 19 
計畫成果自評 
 
一、我們利用循環矩陣的特性來簡化針對獨立符號來源萃取器的證明並獲得一個
更好的結果。 
二、由計算的角度，我們發現無法從一個只擁有 computational min-entropy 的
隨機源中萃取出一個隨機元。 
三、我們能證明[LLTT05]所提出的針對兩個弱隨機源的萃取器也是一個針對兩個
只擁有 computational min-entropy 的隨機源的萃取器。 
四、我們提出一個可利用任何分布來學習線性函數的學習演算法。 
五、我們延伸核心集引理從原本只考慮函數 f 為布林函數的情況到考慮
{ } [ ]: 0,1f D→ℓ 。 
六、我們利用延伸的核心集引理證明[LLT06]中針對獨立符號來源的萃取器亦可從
計算的獨立符號來源中萃取出隨機元，其在小電路的眼中看起來是非常隨機
的。 
七、我們亦利用證明針對計算獨立符號來源之萃取器的方法證明延伸的 XOR 引
理，其將原本只考慮函數 f 為布林函數的情況延伸到考慮 { } [ ]: 0,1f D→ℓ 。 
八、最後我們證明一個核心集的黑箱子建造法中二元核心集大小的上限。 
九、前述結果已分別整理並發表於期刊：IEEE Transactions on Information 
Theory, Vol.57(8) 5485-5496, 2011 以及國際會議 18th International 
Symposium on Fundamentals of Computation Theory, Oslo, NORWAY. 
 
 
5486 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 57, NO. 8, AUGUST 2011
but there are provable discrepancies among them [4], [15]. To
extract randomness from a source with so-called HILL-entropy
[4], the strongest among them, one can simply use any statis-
tical extractor, but we would like to extract randomness from
a broader class of sources. Here we consider a weaker (more
general) notion of computational randomness, which appears in
[15], and we call it computational min-entropy. A comparison
with other notions of computational randomness can be found
in [15].
A. Computational Min-Entropy
To model the more general situation that one may observe
some correlated information about the source, we consider the
setting with a pair of jointly distributed random variables and
, where is the source from which we want to extract and
(could be empty) is some information which one can observe. To
stress that we want to measure the randomness of conditioned
on and to extract randomness from given the information
, we use the notation to denote such a joint distribution.
The correlation between and is modeled by for
some function . In the example of one-way permutation, is
the inverse function , which is hard to compute, and is the
distribution of over a random . Here in our definition, we
allow to be probabilistic and we even do not require it to have
an efficient (or even computable) algorithm, and furthermore,
we do not require to be efficiently samplable either. We say
that such a source has computational min-entropy
if given input sampled from , any circuit of size can only
predict correctly with probability at most .1 From the
distribution , we would like to extract randomness which
when given still looks random to circuits of a certain size.
Note that a source with statistical min-entropy can be seen
as such a source with computational min-entropy ,
where we can simply have no or just have taking a fixed
value, and let be a probabilistic function with as its output
distribution. This means that extractors for sources with com-
putational min-entropy can immediately work for sources with
statistical min-entropy, and thus results in the computational set-
ting can be seen as a generalization of those in the traditional,
statistical setting. On the other hand, for a deterministic func-
tion , has no statistical min-entropy at all when given .
Still, according to our definition, as long as is hard to compute,
in fact can have high computational min-entropy.
Extractors for such sources were implicitly proposed before
[11], [14], and they are seeded ones. That is, they need an addi-
tional seed which must be perfectly random and independent
of the source. In fact, it is known that any seeded statistical
extractor with some additional reconstruction property (in the
sense of [27]) gives a seeded extractor for such sources [4], [26],
[15]. However, just as in the statistical setting, several natural
questions arise in the computational setting too. To extract from
such sources, do we really need a seed? Can we use a weaker
seed which is only slightly random, instead of perfectly random,
in a statistical sense, or an even weaker seed which only looks
slightly random in a computational sense but may contain no
1A more general definition is to have the circuit size as a separate parameter,
but our extractor construction does not seem to work for this more general def-
inition.
randomness in a statistical sense? Seeing the seed as an addi-
tional independent source, a general question is: Can we have
seedless extractors for multiple independent sources in which
each source contains some computational min-entropy? We will
try to answer these questions in this paper. One can see this as a
step toward extending the study of multisource extractors from
the traditional, statistical setting to a new, computational setting.
One can also see this as providing a finer map for the landscape
of statistical extractors, according to the degree of their recon-
struction property.
B. Our Results
First, we show that it is impossible to have seedless extractors
for one single source, even if the source of length can have a
computational min-entropy as high as and even if we only
want to extract one bit.
Next, we show that with the help of a weak seed, it becomes
possible to extract randomness from such sources. We use a
two-source extractor of Lee et al. [20], denoted as EXT, which
takes two input strings , sees them as vectors
from , where for some with , and
outputs their inner product, denoted as , over . As shown
in [20], it works for any two independent sources both con-
taining some statistical min-entropy. Moreover, it is also known
to work when one source contains some computational min-en-
tropy and the other, the seed, is perfectly random (in a statis-
tical sense) [12]. Our second result shows that it even works
when the seed only contains some statistical min-entropy. More
precisely, we show that given any source with com-
putational min-entropy and an-
other independent source with statistical min-entropy , the
output given cannot be distinguished from
random with advantage by circuits of size
. That is, for any such Boolean circuit
, where
denotes the uniform distribution. Then we proceed to show
that the extractor even works when the seed only contains com-
putational min-entropy. More precisely, when we replace the
source by a source with computational min-en-
tropy given still cannot be distin-
guished with advantage by circuits of size about . This can be
seen as a seedless extractor for two independent sources, both
with computational min-entropy.
We do not know if the statistical extractors of [2], [3], [24],
[6], and [23] for multiple independent sources can also work in
the computational setting, since to work in this setting, we need
them to have some reconstruction property. For the extractors
from [11] and [12], this property can be translated to a task in
learning theory, and the proofs there can be recast as providing
an algorithm for learning linear functions under uniform distri-
bution with adversarial noise. Our second result can be seen as
a generalization of [11] and [12], but we are facing a more chal-
lenging learning problem: learning linear functions under arbi-
trary distribution with adversarial noise. Our third result pro-
vides an algorithm for this problem, which, in addition to being
used to prove our second result, may have interest of its own.
In the learning problem, there is some unknown linear func-
tion , defined as , which we want
5488 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 57, NO. 8, AUGUST 2011
lution for the system, there are in fact too many good solutions
for it. If this happens, then in the backward phase when we try to
solve this system, we cannot afford to keep all such solutions, and
we have the risk of losing the actual solution . This tricky situa-
tion does not arise in the random-noise model considered in [5],
so a much simpler algorithm works there. However, in the adver-
sarial-noise model, this seems unavoidable. Fortunately, we can
show that with high probability, the systems we produce indeed
do not have too many good solutions. This turns out to rely on the
fact that our extractor is also a good statistical extractor, together
with the property, which we will show, that each system is likely
to have a distribution which is close to some good distribution
with high statistical min-entropy.
II. PRELIMINARIES
For any , let denote the uniform distribution over
. Let be the class of functions computable by
Boolean circuits of size . We say that a function
is an -distinguisher for two distributions and over
if
All logarithms in this paper will have base two.
We consider two types of min-entropy: statistical min-en-
tropy and computational min-entropy. The notion of statistical
min-entropy is a standard one, usually just called min-entropy.
Definition 1: We say that a distribution has statistical min-
entropy at least , denoted by , if for any
.
Next, we define the notion of computational min-entropy.
Here, we consider the more general setting of measuring the ran-
domness of a distribution given a correlated distribution ,
and we use to denote such a joint distribution. The cor-
relation between and is modeled by for some
function , which could be either probabilistic or deterministic.
Definition 2: We say that a distribution has compu-
tational min-entropy , denoted by , if for any
.
We consider three kinds of extractors: statistical extractors,
hybrid extractors and computational extractors. The notion of
statistical extractors is a standard one for 2-source extractors,
usually just called 2-source extractors, while we introduce the
notions of hybrid extractors and computational extractors.
Definition 3: A function
is called a
• -statistical-extractor if for any source with
and any source , independent of , with
, there is no -distinguisher (without any
complexity bound) for the distributions
and .
• -hybrid-extractor if for any source
with and any source , independent of
, with , there is no -distinguisher
in for the distributions and
.
• -computational-extractor if for any source
with and any source ,
independent of , with , there
is no -distinguisher in for the distributions
and .
Remark 1: Note that the definition above corresponds to the
notion of strong extractors in the setting of seeded statistical ex-
tractors, which guarantees that even given the seed (the second
source), the output still looks random.
We will need the following statistical extractor from [20],
which generalizes the construction from [7]. For any
with , let , and see any as an -dimen-
sional vector over . Then for
any , let be their inner product over defined as
Theorem 1: [20] The function
defined as is a -statis-
tical-extractor when .
We will need the following fact about statistical extractors.
Lemma 1: Let be
any -statistical-extractor. Then for any source over
with and any function
, there are at most different ’s satisfying
Proof: Let be the set consisting of such ’s and let
be the uniform distribution over . Consider the distinguisher
defined as if and
otherwise. Then, the difference
is equal to
which is at least
This implies that , because otherwise it
would contradict the fact that EXT is a good statistical extractor.
Finally, we will need the following lemma about obtaining
predictors from distinguishers. The Boolean case is
well known, and a proof for general can be found in [12].
Lemma 2: For any source over and any function
, if there is an -distinguisher for the
distributions and , then there is a predictor
with as oracle which calls once and runs in time
such that
5490 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 57, NO. 8, AUGUST 2011
That is, we have
We are almost done except that we still cannot bound the
complexity of the algorithm because it needs a way to sample
elements from the source which may not have an efficient
sampling algorithm, unlike in the learning setting where one
does not count the complexity of sampling. Fortunately, by an
average argument, the bound above still holds for some fixed
, and we can simply hard-wire it into . Similarly, we can do
this for other random choices of , and it is not hard to show
that one can have a resulting circuit of size
which is at most
Thus, for some large enough , we have
a circuit of size smaller than which can predict correctly
with probability at least
This contradicts the assumption that , which
means that the distinguisher assumed at the beginning cannot
exist, so EXT is a good hybrid extractor as claimed.
Next, we prove that EXT is also a good computational ex-
tractor, and the proof is almost identical. Consider two inde-
pendent sources and , with and
. Observe that the distribution of must have
statistical min-entropy at least , because otherwise the pre-
dictor which always outputs the value with the largest mea-
sure can predict correctly with probability larger than ,
a violation of the assumption that . Then we
can follow the proof above: assuming the existence of a distin-
guisher for EXT, we can obtain a predictor of size smaller than
, with some elements from hard-wired
in it, which can predict correctly with probability larger than
. This contradicts the fact that , so EXT is a
good computational extractor.
V. LEARNING NOISY LINEAR FUNCTIONS
In this section, we prove Theorem 4. Recall that given any
source over with , any
, and any function , we would like
to learn some unknown such that
(1)
Since such may not be unique, we will list them all. Let us
first imagine one such fixed .
We start by randomly choosing indepen-
dent training examples (with replacement) from the distribution
, for some large enough constant (depending on
). Let denote the matrix and the -dimen-
sional vector, both over , such that for each training example
has as a row and has
Fig. 1. FORWARD PHASE.
Fig. 2. BACKWARD PHASE.
as an entry. Note that each training example , with
, gives us a linear equation
for . Thus from these training ex-
amples, we obtain a system of linear equations, denoted as
, and we would like to reduce the task of learning
to that of solving this system of linear equations. However, this
system is highly noisy as about fraction of the equa-
tions are likely to be wrong, according to (1). We will roughly
follow the approach of Gaussian elimination (which works for
noiseless systems of linear equations), but will make substantial
changes in order to deal with our noisy case.
Our algorithm consists of two phases: the forward phase,
shown in Fig. 1, and the backward phase, shown in Fig. 2. The
forward phase works as follows, which is similar to an ap-
proach of Blum et al. [5]. Starting from the system
of linear equations, we use several iterations to produce smaller
and smaller systems with fewer and fewer variables, until we
have a small enough system which we can afford to solve using
brute force. More precisely, we choose the parameters
divide each row of into blocks, with each block con-
taining elements in , and proceed in iterations, as shown in
Fig. 1. Note that after iteration , we have the system
which has variables and equations, with
5492 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 57, NO. 8, AUGUST 2011
, let denote the fraction of equations
in the group which are off by a value in the sense that
Note that for to satisfy a new equation, which is the dif-
ference between two equations, these two involved equations
must be off by the same value. Therefore, the expected frac-
tion of new satisfied equations in this group is , which
under the constraint achieves its minimum when
for all other . Hence, after one itera-
tion, the expected fraction of new equations in group (before
removing pivots) satisfied by is at least
Combing all groups together, the expected fraction of satisfied
equations overall (before removing the pivots) is at least
where the first inequality is due to Jensen inequality, and the
second inequality uses the bound implied by that
in (2).
To get the expected fraction of satisfied equations in the final
system , after performing Step 1(d), observe that we
only need to discard at most equations, each
with measure , so the total discarded measure, de-
noted as , is at most
for a large enough constant . As a result, the expected fraction
of equations in satisfied by is at least
by recalling that and . Finally, by a
Markov inequality, we have the lemma.
Then by Lemma 5 and an induction, the forward phase is good
with probability at least
This proves Lemma 3.
B. Proof of Lemma 4
Recall that a solution is -good for the system if
it satisfies at least fraction of the equations. For any
such that , consider the following event:
• : the number of -good solutions for ex-
ceeds .
Thus, our goal is to show that
We will prove this by a union bound, so our goal is reduced to
bounding each for .
To get a quick idea, let us first consider how to bound
. Note that since EXT is a good statistical ex-
tractor and has a high min-entropy, Lemma 1 guaran-
tees that the number of satisfying the probability bound
is at most . Any
other is very unlike to be -good for by a
Chernoff bound because each row of is sampled indepen-
dently from . Since happens only when any such (not
satisfying that probability bound) is -good, a union bound
shows that is indeed small.
Now for , to follow this idea to bound , we
would also like the distribution of , denoted as , to
have the nice property that each of its rows comes indepen-
dently from a high min-entropy source. Unfortunately, this is not
true in general,2 and a much more involved analysis is needed.
Our approach is to consider the distribution conditioned on
the choice of pivots in the first iterations. We call a particular
choice of the pivots a restriction of the pivots, which includes
fixing the indices and the values of some rows as pivots while
leaving other rows free. We will show that the distribution
conditioned on most restrictions is close to a distribution with
the nice property. For our purpose here, instead of using the
standard definition of “closeness” (which would be measured
according to the statistical distance), we consider the following
one.
Definition 4: We say that two distributions are -close if the
probabilities of any event according to the two distributions are
within a multiplicative factor of from each other.
Observe that one can generate the matrix in an alterna-
tive way by first choosing the pivots in iterations and then gen-
erating the matrices consistent with the pivots.
Formally, the distribution (the distribution of the matrix
) can be generated in two passes as follows. In the first pass,
we select a restriction of pivots in the first iterations, denoted
as , by running the forward phase on the matrix
sampled from and collecting the pivots, which include
the indices and the values of rows as pivots, in each iteration. In
2This is true in the simple case considered by [5] that one has     to
start with. In this case, for each  , one can easily show that each row of
does come independently from the uniform distribution  .
5494 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 57, NO. 8, AUGUST 2011
Fig. 3. If   is close to   , then   is close to   , conditioned on   .
of the matrix as pivots and it has the effect on the dis-
tribution that all the rows of must belong to the
groups of those rows. We would like the effect to be small,
and we consider the following event, over the selection of .
• : those elements in the support of which would
belong to those groups of when selected as rows
of (i.e., those with their first blocks matching one
of the first blocks of the rows in ) have a combined
measure of in the distribution .
We will show that if happens then happens. For this,
let us consider any fixed restriction such that happens,
and let us use to denote the event that the pivots chosen in
iteration match those in . Our approach is illustrated in
Fig. 3.
First, let us consider the case of starting iteration from the
nice distribution , instead of , conditioned on
, and let be the resulting distribution after iteration .
The following claim shows that is in fact close to a nice
distribution.
Claim 1: For some , the distribution is
-close to some nice distribution described in the event
(i.e., has rows, each coming independently from
a distribution with ).
Next, let us go back to the actual situation of starting itera-
tion from the distribution , instead of as we did
in the above claim. Using the assumption that is close
to , our next claim shows that when we start iteration
from the distribution conditioned on , the resulting
distribution is close to the distribution .
Claim 2: The distribution is -close to the distribu-
tion .
From these two claims, we can conclude that is -close
to , for , which by induction is at
most
This implies that for any restriction such that the event
happens, the event must happen as well. Therefore, the
probability that does not happen is at most the probability
that does not happen, which we bound by the following
claim.
Claim 3: The probability over the selection of that
does not happen is at most .
We have shown that for any restriction such
that the event happens, the probability, over the selection
of , that the event does not happen is at most .
This implies that , which proves
Lemma 7. Thus, it remains to prove the three claims above,
which we do next.
Proof: (of Claim 1)
Recall that we have fixed a restriction which fixes some
rows as pivots such that the event happens, and we use
to denote the event that the pivots selected during iteration
match those in the restriction . In this claim, we consider the
situation of starting iteration from the nice distribution
conditioned on the event .
First, let us see how the distribution is affected by the
conditioning on . Consider any fixed matrix of
rows, insert the rows of at the proper places to
get a fixed matrix of rows, and let us use
to denote the event that a randomly sampled matrix from
equals this matrix . If the matrix has a row not in the
groups of , then . Otherwise,
is
where is the measure of the ’th row of in
is the number of rows of in group , and is
the measure of group in . Note that for some
, the numerator equals
while the denominator equals
5496 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 57, NO. 8, AUGUST 2011
[12] O. Goldreich, R. Rubinfeld, and M. Sudan, “Learning polynomials with
queries: The highly noisy case,” SIAM J. Discrete Math., vol. 13, no.
4, pp. 535–570, 2000.
[13] V. Guruswami, C. Umans, and S. Vadhan, “Unbalanced expanders and
randomness extractors from Parvaresh-Vardy codes,” J. ACM, vol. 56,
no. 4, 2009, Art. 20.
[14] J. Håstad, R. Impagliazzo, L. A. Levin, and M. Luby, “A pseudorandom
generator from any one-way function,” SIAM J. Comput., vol. 28, no.
4, pp. 1364–1396, 1999.
[15] C.-Y. Hsiao, C.-J. Lu, and L. Reyzin, “Conditional computational en-
tropy, or toward separating pseudoentropy from compressibility,” in
Proc. Adv. Cryptol.—EUROCRYPT, 2007, pp. 169–186.
[16] A. Kalai, Y. Mansour, and E. Verbin, “On agnostic boosting and parity
learning,” in Proc. 40th Annu. ACM Symp. Theory Comput. (STOC’08),
pp. 629–638.
[17] J. Kamp, A. Rao, S. Vadhan, and D. Zuckerman, “Deterministic extrac-
tors for small-space sources,” in Proc. 38th Annu. ACM Symp. Theory
Comput. (STOC’06), pp. 691–700.
[18] J. Kamp and D. Zuckerman, “Deterministic extractors for bit-fixing
sources and exposure-resilient cryptography,” SIAM J. Comput., vol.
36, no. 5, pp. 1231–1247, 2007.
[19] C.-J. Lee, C.-J. Lu, and S.-C. Tsai, “Deterministic extractors for inde-
pendent-symbol sources,” in Proc. 33rd Int. Colloq. Automata, Lang.,
Program. (ICALP 2006), pp. 84–95.
[20] C.-J. Lee, C.-J. Lu, S.-C. Tsai, and W.-G. Tzeng, “Extracting random-
ness from multiple independent sources,” IEEE Trans. Inf. Theory, vol.
51, no. 6, pp. 2224–2227, Jun. 2005.
[21] C.-J. Lu, O. Reingold, S. Vadhan, and A. Wigderson, “Extractors: Op-
timal up to constant factors,” in Proc. 35th Annu. ACM Symp. Theory
Comput. (STOC’03), pp. 602–611.
[22] N. Nisan and D. Zuckerman, “Randomness is linear in space,” J.
Comput. Syst. Sci., vol. 52, no. 1, pp. 43–52, 1996.
[23] A. Rao, “Extractors for a constant number of polynomially small min-
entropy independent sources,” SIAM J. Comput., vol. 39, no. 1, pp.
168–194, 2009.
[24] R. Raz, “Extractors with weak random seeds,” in Proc. 37th Annu. ACM
Symp. Theory Comput. (STOC’05), pp. 11–20.
[25] R. Shaltiel, “Recent developments in explicit constructions of extrac-
tors,” Bull. Eur. Assoc. Theor. Comput. Sci., vol. 77, pp. 67–95, 2002.
[26] A. Ta-Shma and D. Zuckerman, “Extractor codes,” IEEE Trans. Inf.
Theory, vol. 50, no. 12, pp. 3015–3025, Dec. 2004.
[27] L. Trevisan, “Extractors and pseudorandom generators,” J. ACM, vol.
48, no. 4, pp. 860–879, 2001.
[28] L. Trevisan and S. Vadhan, “Extracting randomness from samplable
distributions,” in Proc. 41st Annu. IEEE Symp. Found. Comput. Sci.
(FOCS’00), pp. 32–42.
[29] A. C. Yao, “Theory and applications of trapdoor functions,” in Proc.
23rd Annu. IEEE Symp. Found. Comput. Sci. (FOCS’82), pp. 80–91.
[30] D. Zuckerman, “General weak random sources,” in Proc. 31st Annu.
IEEE Symp. Found. Comput. Sci. (FOCS’90), pp. 534–543.
Chia-Jung Lee received the B.S. degree from the National Taiwan Normal
University, Taipei, Taiwan, in 2000, and the Ph.D. degree in computer science
from the National Chiao-Tung University, Hsinchu, Taiwan, in 2010. She is now
doing postdoctoral research at the Institute of Information Science, Academia
Sinica, Taipei, Taiwan. Her research interests are randomness in computation,
cryptography, and theoretical computer science.
Chi-Jen Lu received his B.S. and M.S. degrees from National Taiwan Univer-
sity, Taiwan, in 1988 and 1990 respectively, and his Ph.D. degree from Univer-
sity of Massachusetts at Amherst, USA, in 1999, all in computer science. He
is currently a research fellow in the Institute of Information Science, Academia
Sinica, Taiwan. His research interests include randomness in computation, com-
putational complexity, cryptography, game theory, and machine learning.
Shi-Chun Tsai (M’06) received his B.S. and M.S. degrees in computer science
and information engineering from National Taiwan University, Taiwan, in 1984
and 1988, respectively, and the Ph.D. degree in computer science from the Uni-
versity of Chicago, USA, in 1996. During 1993–1996, he served as a Lecturer in
the Computer Science Department, University of Chicago. During 1996–2001,
he was Associate Professor of Information Management Department, and Com-
puter Science and Information Engineering Department, National Chi Nan Uni-
versity, Taiwan. He has been with the Department of Computer Science, Na-
tional Chiao Tung University, Taiwan since 2001, and was promoted to full
Professor in 2007. He is currently serving as the Director of the Information
Technology Service Center of National Chiao Tung University. His research in-
terests include computational complexity, algorithms, coding theory, and com-
binatorics.
Computational Randomness from Generalized Hardcore Sets 79
we call such a function (δ, s)-hard where the parameter δ is called the hardness
of f . Then the hardcore lemma asserts that there exists a subset H ⊆ {0, 1}n
of density δ such that any circuit of size s′ must disagree with f on at least
1−ε
2 fraction of inputs from H , for some s
′ slightly smaller than s. This means
that given a random input x in H , although the value of f(x) is ﬁxed and thus
has no randomness at all in a statistical sense, it still looks like a random bit to
small circuits. Because of this nice property, the hardcore lemma has become an
important tool in the study of pseudo-randomness. For example, it was used in
[9] for an alternative proof of Yao’s XOR lemma [20], used in [17] for constructing
a pseudo-random generator directly from a mildly-hard function without going
through the XOR lemma, and more recently used in [15,18,19,6] for amplifying
hardness of functions in NP. The parameters of the hardcore lemma were later
improved by [10,7,2].
Note that Impagliazzo’s hardcore lemma works for Boolean functions. It says
that the output of a hard function given a random input looks like a random bit
and thus contains statistical randomness, when the input falls in the hardcore set.
When using the lemma, the hard function is usually evaluated at several inputs
in order to obtain several output bits, which together can be argued to contain
some suﬃcient amount of randomness. Usually, the amount of randomness in
a distribution is measured by its min-entropy, where a distribution has min-
entropy at least k if every element occurs with probability at most 2−k. Then
from a distribution with some min-entropy, one applies a so-called randomness
extractor [21,14] to extract a distribution which looks almost random.
On the other hand, there are natural functions with many output bits which
are believed to be hard, such as factoring and discrete logarithm, and one may
be able to extract several bits at once from one output value. This is also related
to the problem of extracting randomness from sources with computational ran-
domness, studied in [3,8,12]. One may wonder if there is an analogous hardcore
lemma for a general non-Boolean function, which can guarantee that the out-
put distribution given a random input will look like one with some min-entropy,
hopefully much larger than one. For example, assume that a one-way permu-
tation g : {0, 1}n → {0, 1}n exists, whose inverse function f = g−1 is hard to
compute by small (say, polynomial-size) circuits. Then, if one could show that the
distribution of x = f(y) given a random y looks like having some min-entropy to
small circuits, one could simply apply any extractor on x. However, the conven-
tional wisdom does not suggest so and the following counter example seems to be
known as a folklore. Given an eﬃciently-computable extractor E and a one-way
permutation g, the function Ext deﬁned as Ext(x, u) = E(g(x), u) is still an ex-
tractor, but its output can be easily computed (and hence does not look random
at all) given y = g(x) and u. To extract such computational randomness, pre-
vious works all resorted to extractors with some reconstruction property, which
roughly corresponds to error correcting codes with eﬃcient decoders (see, e.g.,
[16] for a deﬁnition).
Does this mean that there is no analogous hardcore lemma for general func-
tions? If we consider a hard function f : {0, 1}n → {0, 1}2 with two, instead of
Computational Randomness from Generalized Hardcore Sets 81
With a small hardcore set, one can only say that the output of a hard function
f looks somewhat random when the input falls into that small set. This alone
is not good enough for the purpose of randomness extraction because the vast
majority of inputs are outside of the hardcore set and may contribute a large
error. Our next result shows that in fact we can have not just one but a collection
of disjoint hardcore sets, and they together cover all but a small fraction of the
inputs, which implies that the output of f looks somewhat random for most
input. More precisely, we show that for a (δ, s)-hard function, with δ ≥ 1− 2−k,
its output distribution given a random input looks close, within some distance ε,
to a distribution with min-entropy Ω(k), by circuits of size s′ = s/poly(V, 1/ε).
This implies that we can simply apply any seeded statistical extractor to extract
computational randomness from the output of f as long as s is large (say, super-
polynomial) or V is small (say, polynomial). This also works for seedless multi-
source extractors, and in particular, it ﬁts nicely with the setting of independent-
symbol sources studied in [12] in which each symbol is considered to come from a
small set. Therefore, we can generalize the result of [12] from a statistical setting
to a computational one: given multiple independent sources, over a small set of
symbols, which look slightly random to polynomial-size circuits but may have
no min-entropy at all, the statistical extractor there can be used to produce an
output which looks almost random to polynomial-size circuits.
Note that in our hardcore set result, there is a security loss of some factor
poly(V, 1/ε) in circuit size. That is, starting from a function which is hard against
circuits of size s, we can only guarantee the hardness of a hardcore set against
circuits of size s′, with s′ smaller than s by that factor. Consequently, with s =
poly(n), we can only extract randomness from a function with V ≤ poly(n) (or
equivalently, with O(log n) output bits). One may wonder if such a security loss of
circuit size can be avoided. Our ﬁnal result shows that this is basically impossible,
if the proof is done in a certain black box way. Here, we use the notion of black-
box proofs for hardcore sets introduced in [13]. Informally speaking, a black-box
proof is realized by an oracle algorithm R such that for any function f and any
collection G of circuits, if G breaks the hardcore set condition, then R breaks the
hardness of f by using G only as an oracle. In this black-box model, we show that
any algorithm R must make at least q = Ω((V k/ε2) log(1/δ)) queries in order
to show the existence of a hardcore set with k output values. This translates to
a loss of a q factor in circuit size, because the resulting circuit of RG is larger
than those in G by this factor. This explains the need of using reconstructive
extractors, instead of just any extractors, on the input of a one-way permutation
discussed before, since there we have a large V = 2n. Finally, we would like
to clarify a potential confusion with the security loss of using reconstructive
extractors in previous works. When applying reconstructive extractors on the
output of a hard function f , previous results also suﬀered some loss of circuit
size in the same sense: the outputs of extractors only look random to smaller
circuits compared to those which the hardness of f is measured against. However,
the loss is in terms of the output length m of extractors, instead of the output
Computational Randomness from Generalized Hardcore Sets 83
When there are at least two independent sources which are weakly random, it
becomes possible to have a seedless extractor, which is deﬁned as follows.
Definition 5. A function Ext : ({0, 1}n)t → {0, 1}m is called a (seedless)
t-source (k, ε)-extractor if for any t independent distributions X1, . . . ,Xt over
{0, 1}n with ∑i∈[t] H∞(Xi) ≥ k, there is no ε-distinguisher for the distributions
Ext(X1, . . . ,Xt) and Um.
3 Generalized Hardcore Set
In this section, we generalize Impagliazzo’s hardcore lemma [9] from the case of
Boolean functions to the case of general functions. More precisely, we have the
following.
Lemma 1. Let f : X → [V ] be a (δ, s)-hard function, with δ ≥ 1− 1L(1− γ) for
some γ ∈ (0, 1) and some integer L ∈ [V − 1]. Then for any ε > 0, there exist
s′ = s/poly(V, 1/ε, log(1/γ)) and I ∈ ( [V ]L+1
)
such that f has an (I, ε, s′)-hardcore
set HI of density |HI |/|X | ≥ γ/
(
V
L+1
)
.
To prepare for the proof of Lemma 1, let us ﬁrst recall Nisan’s proof of Impagli-
azzo’s hardcore lemma (for Boolean functions) described in [9]. The proof is by
contradiction, which starts by assuming that a (ρ, s)-hard function f : X → I,
with |I| = 2, has no hardcore set of density ρ. Then the key step there is to
use the min-max theorem of von Neumann to show the existence of a subset of
inputs T ⊆ X of density less than ρ and a collection of circuits AI ⊆ SIZE(s′)
with |AI | ≤ O((1/ε2) log(1/ρ)) such that for any x /∈ T ,
Pr
A∈AI
[A(x) = f(x)] >
1
2
.
Then by letting C be the circuit computing the majority of those circuits in
AI , one has C(x) = f(x) for every x /∈ T , which contradicts the fact that f is
(ρ, s)-hard.
We would like to extend this idea to a general function f : X → [V ], with
V ≥ 3. First, it is straightforward to verify that a similar argument using the
min-max theorem can also prove the following lemma.
Lemma 2. Suppose f : X → [V ] does not have an (I, ε, s′)-hardcore set of
density ρ in X, for some I ⊆ [V ]. Then there exist a subset of inputs TI ⊆ f−1(I)
of density less than ρ in X and a collection of circuits AI ⊆ SIZE(s′) with
|AI | ≤ O((1/ε2) log(1/ρ)) such that for any x ∈ f−1(I) \ TI ,
Pr
A∈AI
[A(x) = f(x)] >
1
|I| .
However, unlike the Boolean case, it is not clear how to construct a circuit C
to approximate f from these collections of circuits. This is because for an input
Computational Randomness from Generalized Hardcore Sets 85
4 Density of Hardcore Sets
For the generalized hardcore set lemma in Section 3, one may wonder whether
it is possible to guarantee the existence of a much larger hardcore set. In this
section, we show that this is basically impossible. Formally, we have the following.
Theorem 1. For any δ = 1− 1L(1−γ), with γ ∈ (0, 1/2(L+1)) and L ≤ V −1,
there is a (δ, s)-hard function f : {0, 1}n → [V ], for some s ≥ poly(γ, 1/L, 2n),
such that the following condition holds:
– For any I ∈ ( [V ]L+1
)
and ε < 12L , there exists some s
′ ≤ poly(n) such that f
has no (I, ε, s′)-hardcore set of density 4(L + 1)γ/
(
V
L+1
)
in {0, 1}n.
Note that the theorem says that even for a function which is hard against very
large circuits of exponential size, one can only guarantee a hardcore set of a small
density against small circuits of polynomial size. However, there is a gap of a
4(L + 1) factor between the density of a hardcore set ruled out by Theorem 1
and the density achievable by our Lemma 1.
Proof. We show the existence of such a function f by a probabilistic method. Let
T denote the ﬁrst 2(L+1)γ fraction of the input space {0, 1}n, and let us divide
T into
(
V
L+1
)
disjoint parts of equal size (assuming for simplicity of presentation
that T can be divided evenly), denoted by TI , for I ∈
(
[V ]
L+1
)
. Then we choose
the function f : {0, 1}n → [V ] randomly in the way such that independently for
each input x,
f(x) =
{
a random value in I, if x ∈ TI for some I ∈
(
[V ]
L+1
)
;
a random value in [L], if x /∈ T .
We need the following lemma; the proof is by a standard probabilistic argument
and is omitted here due to the page limit.
Lemma 3. Prf [f is not (δ, s)-hard] < 1, for some s = poly(γ, 1/L, 2n).
This lemma implies the existence of a function f which is (δ, s)-hard, and let
us ﬁx one such f . It remains to show that this f satisﬁes the condition of the
theorem. For any I ∈ ( [V ]L+1
)
and any H ⊆ f−1(I) of density 4(L + 1)γ/( VL+1
)
in
{0, 1}n, consider the algorithm A which outputs a random value in I ∩ J when
x ∈ TJ for some J ∈
(
[V ]
L+1
)
, and outputs a random value in [L] when x /∈ T .
Then the probability, over x ∈ H and the randomness of A, that A(x) = f(x) is
at least
Pr
x∈H
[x ∈ TI ] · 1
L + 1
+ Pr
x∈H
[x /∈ TI ] · 1
L
=
1
L + 1
+ Pr
x∈H
[x /∈ TI ] ·
(
1
L
− 1
L + 1
)
which is at least 1L+1 +
1
2 · 1L(L+1) > 1+εL+1 for any ε < 12L . This means that
there exists a ﬁxing of the randomness of A to get a deterministic circuit which
preserves the above bound. Since we can do this for every I and H , the condition
of the theorem is satisﬁed, which proves the theorem. unionsq
Computational Randomness from Generalized Hardcore Sets 87
and k ≥ c for a large enough constant c, we have ε¯ ≤ 2−Ω() and s¯ ≥ s/2O().
Again, this means that when s is large enough (or  is small enough), any seedless
multi-source extractor can also work in the computational setting.
6 Loss of Circuit Size
Recall that in our generalized hardcore set lemma (Lemma 1), there is a loss of
circuit size by a factor of poly(V ) for functions with V output values. That is,
from a (δ, s)-hard function, we can only guarantee the existence of an (I, ε, s′)-
hardcore set with s′ ≤ s/poly(V ). In this section, we show that such a loss of
circuit size is in fact unavoidable, if the proof is done in a black-box way. Before
we can formally state our result, we need to introduce some deﬁnitions. Let FX,V
denote the collection of functions from X to [V ].
Definition 6. Given a collection G ⊆ FX,V , we say that a function f ∈ FX,V is
(k, ρ, ε,G)-easy if for any I ∈ ([V ]k
)
and any H ⊆ f−1(I) of density |H |/|X | ≥ ρ,
there is a function g ∈ G such that Prx∈H [g(x) = f(x)] ≥ 1+εk .
Next, we deﬁne our notion of a black-box proof, which is realized by some oracle
algorithm R(·). We allow R to be non-uniform and randomized, and we use the
notation RG;αr (x) to denote that R is given an oracle G, an advice string α, a
random string r, and an input x.
Definition 7. We say that an oracle algorithm R(·) realizes a (δ, k, ρ, ε, S) black-
box proof of hardcore sets for functions in FX,V , if the following holds. For any
f ∈ FX,V and any G ⊆ FX,V with |G| = S, if f is (k, ρ, ε,G)-easy, then there
exists some advice α such that
Pr
x,r
[
RG;αr (x) = f(x)
]
< δ.
Here we allow R to make adaptive queries, but for simplicity we consider only
the case that R on input x queries functions in the oracle at all x. That is, R
may ﬁrst queries gi(x) for some gi ∈ G, and depending on the answer, R next
queries gj(x) for some gj ∈ G, and so on. Note that our proof for the generalized
hardcore sets is done in this black-box way, and so do all the known proofs
for Impagliazzo’s hardcore set lemma. Our result in this section shows that any
algorithm realizing such a black-box proof must make many queries to the oracle.
Theorem 4. Suppose V ≥ ω(1), 0 < δ ≤ 1 − (4 logV )/V , 0 < ε ≤ 1/3,
0 < ρ < 1, and S ≥ Ω((V k+1k3/ε2) log(1/ρ)). Consider any oracle algorithm
which uses an advice of length τ ≤ o(δ|X |) and realizes a (δ, k, ε, S, ρ) black-
box proof of hardcore sets for functions in FX,V . Then it must make at least
Ω((V k/ε2) log(1/δ)) oracle queries.
Note that the theorem says that even if we start from a very hard function,
with δ close to one, and even if we only want a hardcore set with k = 2 output
values, any algorithm realizing such a black-box proof still need to make many
queries, which corresponds to a large loss of circuit size. In particular, a loss by
a V factor is unavoidable. Now let us prove the theorem.
Computational Randomness from Generalized Hardcore Sets 89
From Lemma 6 and the bound in (2), we can conclude the existence of some
f and G such that f is (k, ρ, ε,G)-easy but Prf,G,r[RG;αr (x) = f(x)] ≥ δ for
any advice α, which contradicts the requirement for a black-box proof of hard-
core sets. Therefore, any R realizing such a black-box proof must make at least
Ω((V k/ε2) log(1/δ)) queries, which proves Theorem 4. unionsq
References
1. Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R.: The non-stochastic multi-
armed bandit problem. SIAM J. Comput. 32(1), 48–77 (2002)
2. Barak, B., Hardt, M., Kale, S.: The Uniform Hardcore Lemma via Approximate
Bregman Projectionss. In: SODA 2008, pp. 1193–1200 (2008)
3. Barak, B., Shaltiel, R., Wigderson, A.: Computational analogues of entropy. In:
Proc. APPROX-RANDOM, pp. 200–215 (2003)
4. Cover, T., Thomas, J.: Elements of Information Theory. Wiley, Chichester (1991)
5. Goldreich, O., Rubinfeld, R., Sudan, M.: Learning polynomials with queries: the
highly noisy case. SIAM J. Disc. Math. 13(4), 535–570 (2000)
6. Healy, A., Vadhan, S., Viola, E.: Using nondeterminism to amplify hardness. SIAM
J. Comput. 35(4), 903–931 (2006)
7. Holenstein, T.: Key agreement from weak bit agreement. In: STOC 2005, pp. 664–
673 (2005)
8. Hsiao, C.-Y., Lu, C.-J., Reyzin, L.: Conditional computational entropy, or toward
separating pseudoentropy from compressibility. In: Naor, M. (ed.) EUROCRYPT
2007. LNCS, vol. 4515, pp. 169–186. Springer, Heidelberg (2007)
9. Impagliazzo, R.: Hard-core distributions for somewhat hard problems. In: FOCS
1995, pp. 538–545 (1995)
10. Klivans, A., Servedio, R.A.: Boosting and hard-core sets. Machine Learning 51(3),
217–238 (2003)
11. Lee, C.-J., Lu, C.-J., Tsai, S.-C.: Deterministic extractors for independent-symbol
sources. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.) ICALP 2006.
LNCS, vol. 4051, pp. 84–95. Springer, Heidelberg (2006)
12. Lee, C.-J., Lu, C.-J., Tsai, S.-C.: Extracting computational entropy and learning
noisy linear functions. In: Ngo, H.Q. (ed.) COCOON 2009. LNCS, vol. 5609, pp.
338–347. Springer, Heidelberg (2009)
13. Lu, C.-J., Tsai, S.-C., Wu, H.-L.: On the complexity of hard-core set constructions.
In: Arge, L., Cachin, C., Jurdzin´ski, T., Tarlecki, A. (eds.) ICALP 2007. LNCS,
vol. 4596, pp. 183–194. Springer, Heidelberg (2007)
14. Nisan, N., Zuckerman, D.: Randomness is linear in space. J. Comput.Syst.
Sci. 52(1), 43–52 (1996)
15. O’Donnell, R.: Hardness amplification within NP. In: STOC, pp. 751–760 (2002)
16. Shaltiel, R.: Recent developments in explicit constructions of extractors. Bulletin
of the EATCS 77, 67–95 (2002)
17. Sudan, M., Trevisan, L., Vadhan, S.: Pseudorandom generators without the XOR
lemma. J. Comput.Syst. Sci. 62(2), 236–266 (2001)
18. Trevisan, L.: List decoding using the XOR lemma. In: FOCS, pp. 126–135 (2003)
19. Trevisan, L.: On uniform amplification of hardness in NP. In: STOC, pp. 31–38
(2005)
20. Yao, A.: Theory and applications of trapdoor functions. In: FOCS 1982, pp. 80–91
(1982)
21. Zuckerman, D.: General weak random sources. In: FOCS, pp. 534–543 (1990)
Polar codes的主要精神：將兩個相同的頻道，轉化成兩個容量總合相同，但各別容量相
異的頻道。經過反覆做用之後，便能得到許多極高或極低容量的頻道，這就是所謂的極化
(Polarization)，進而利用高容量的頻道來傳送訊息。而這個觀點與計算機科學上，亂數
萃取器(Randomness Extractor)是互補的，極低容量的頻道，就會有接近良好亂數來源
(Random source)的特性，因此，今年的大會也有論文是探討此一關聯性。 
 
    在眾多講座之中，令我印象最深刻的是由 Zhanna Reznikova教授(Head of the 
Department of Comparative Psychology, Novosibirsk State University)的演講。在
演講中，他以資訊理論的角度去研究其他生物溝通的方式，尤其是螞蟻。他假定螞蟻溝通
的方式是透過肢體接觸，並將傳遞的資料量訂為肢體接觸的時間。透過一連串的實驗，他
的研究指出，螞蟻具有表達數字的能力，而且表達所需要的時間，確實與需要描述的資料
量成正比。藉由將實驗環境小幅改變，他發現螞蟻也具有加減法的能力。而最後藉由一個
類似資訊重複出現的實驗，他發現螞蟻甚至有資料壓縮的能力。 
   
 
    此外，腦神經科學家兼 MIT電機工程學博士 Todd P. Coleman(伊利諾大學香檳校區電
機電腦工程系助理教授)，也以資訊理論中的 Causal system model 來分析腦的結構、神
經連結以及訊息傳遞的方式。藉由探針觀察猿猴接受到各種刺激與發出不同反應的腦部電
波訊號，將信號數據化、並且套入數學模型分析，用以判明各個神經元運作與消息傳遞過
程與方式。 
 
    在本論文報告時，同場報告的 Olgica Milenkovic博士(伊利諾大學香檳校區電機電
腦工程系助理教授)，針對我們的論文提出問題與建議，讓我們有許多收穫。藉以了解統
計物理上可能有些新方法可以幫助解決一些目前面臨的問題。在會議結束後，我們也藉機
與研究領域頗有重疊的學者 Andrew Jiang博士(德州農工大學計算機科學系助理教授)及
同一個研究群的其他人共同餐敘。也藉此交流了一些資訊，並增進彼此情誼。 
三、 建議 
    資訊理論實為現下當紅的雲端計算領域中，最基礎的學問之一。尤其是雲端計算離不
開網路通訊以及分散式資料儲存系統，而解決這些領域問題的方法，核心就是資訊理論相
關的研究。在台灣舉國大力發展雲端運算之時，建議資訊科學與工程系所將本會議放入重
要會議或是頂尖會議的名單之中，因為 Network coding可能是未來雲端網路傳輸資料的
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/25
國科會補助計畫
計畫名稱: 亂數粹取之計算複雜度研究
計畫主持人: 蔡錫鈞
計畫編號: 97-2221-E-009-064-MY3 學門領域: 計算機理論與演算法
無研發成果推廣資料
Fundamentals of 
Computation 
Theory, Oslo, 
NORWAY, August 
22-25, 2011. 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 3 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之
成果如辦理學術活
動、獲得獎項、重要
國際合作、研究成果
國際影響力及其他協
助產業技術發展之具
體效益事項等，請以
文字敘述填列。) 
論文發表在優質期刊。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
