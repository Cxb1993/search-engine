中文摘要： 
在「內涵式音樂資訊查詢(content-based music information retrieval)」領域的研究中，如
何從音樂物件中擷取出具有代表性的特徵，並根據此提供使用者查詢以及其他進階的服務，
一直以來都是一個很重要而且基礎的研究主題。然而，隨著相關研究的迅速發展，只利用
單一 domain 上的特徵已變得不足夠。因此，在本計劃中，我們以 cross-modal 的方式來探
討，以及分析使用者行為(user behavior)與音樂物件特徵的關聯，以提供個人化音樂推薦
(personalized music recommendation)，並且將結果作視覺化(visualization)的呈現。 
在這一次的研究計畫中，涵蓋兩個主題：cross-modal 特徵擷取、推薦系統與視覺化。
我們所擷取的音樂物件特徵，所屬之特徵域包含訊號方面的特徵(例如：MFCC)與標籤式音
樂特徵(例如：歌手、專輯名稱)，完成特徵擷取後，我們以 cross-modal 整合不同領域的特
徵，並且以 MMG 儲存。 
推薦系統方面，我們分析使用者過往的使用資訊，產生兩種推薦清單，第一種推薦清
單屬於單一物件推薦，清單中的歌曲存在著相互關係，順序改變，推薦清單對使用者的影
響力就會不同；第二種推薦清單屬於組合式推薦，清單是由一組一組的歌曲集合而成。視
覺化方面。在完成推薦後，會將歌曲播放介面以新穎的方法顯示出來。 
 
英文摘要： 
In the content-based music information retrieval, one of the fundamental and most important 
research issues is to extract features from music objects for providing retrieval function, as well 
as other advanced services. By reviewing previous work, most of research work focus on feature 
extraction in a single domain, in which the effectiveness can be hardly improved any more. 
Therefore, in this project, we use cross-modal to handle multi-features, and to explore and 
analyze the connection between use behavior and music object features. Then, we provide a 
personalized music recommendation, present and visualize those results in a novel interface.  
There are two research topics in this project, i.e., cross-modal feature extraction, and 
recommendation with visualization. Among features in multi-domain, we choose the MFCCs in 
signal-feature domain, and tags in label-like domain. The two different kinds of features are 
integrated and organized as a MMG. 
Regarding our recommendation system, we analyze user behavior to provide two 
mechanisms, "single-object recommendation" and "between-objects recommendation". The 
recommendation list will be presented and visualization in our tailored interface. 
2 
 
四、 研究方法： 
我們分成兩部分討論，分別是「cross-modal 的特徵擷取」以及「推薦系統與視覺化」。 
 
四－A：cross-modal 特徵擷取 
在我們的系統中，我們討論兩個部份：G୫୫୥以及 Querying 
z Constructing MMG 
在這個部分，我們要建立一個 multimedia graphG୫୫୥去代表音樂物件的集合，以及音樂
圖
物件之間的關聯性。如下圖一。 
一: MMG for m music objects. 
z Querying/Tag Labeling 
，我們就調查音樂物件的標籤去計算RWR的分數，
其中
edia graph Gmmg 的音樂物件上，為了篩選特徵，我們選
用 M
物件的特徵，主要由程式先將音樂訊號加強，接著透
過 H
Process 
當給予一個multimedia graph Gmmg後
RWR 指隨機的 random walk。 
接著討論我們提出的方法。 
我們把重點放在表示成 multim
el-scale Frequency Cepstral Coefficents (MFCC)來分析音樂物件的頻譜，此外，為了降
低 MFCC 的係數，我們將音樂物件使用 Gaussian mixture model，最後我們解釋如何使用三
種不同的 random walk 演算法去收尋 unquantized 的特徵。 
1. Music Object Representation 
首先我們使用 MFCCs 來取得音樂
amming Windows 來執行快速傅立葉轉換，最後使用 Mel tiangle filter 後，得到的結果
就是 MFCC 的特徵值。 
4 
 
而最主要的部份，是我們採取的策略方式，在我們系統中，我們採用了六種策略，分
別是：Item-based, Collaborative, Association, Attribute Comparison, Popularity 以及 Random. 
每當我們選擇其中一個策略去挑選歌曲後，會等待使用者是否喜歡我們所推薦的歌曲，我
們把使用者的判斷視為策略的好壞，因此若使用者對於這首歌有不好得評分，我們便會讓
這個策略出現的機率降低。下圖二我們的介面。 
圖二: The user interface of our system. 
我們從以下三點討論我們的做法： 
1. Behavior and Preference Score 
我們設計一個介面可以去紀錄使用者的 behavior，我們定義當使用者接受這首歌時，
此首歌的評分就會是大於負一；反之，若使用者拒絕(不喜歡)這首歌時，評分就會小於零。
而操控介面也會隨著不同的按鈕反映出不同的分數，像是移動時間軸時，若是快轉則會減
一分倒帶則會加一分。 
2. Strategies Implement 
在進入策略實施之前，我們會先建立四個陣列，分別表示歌曲跟歌曲的關聯性、歌曲
跟歌曲之間的相似度、人跟歌曲之間的相似度以及人跟人的相似度。之後並開始討論每一
種策略的結果。 
z Item-based Strategy 
這裡我們透過歌曲相似度的陣列，找出相似與不相似的歌曲。當使用者接受 s 這
首歌，我們會找出與 s 最相似的歌曲；若使用者不接受 s 這首歌，我們會找出與 s
最不相似的歌曲給予推薦。 
z Collaborative Strategy 
這裡我們透過人與人的相似度陣列來找出相似與不相似的歌曲。我們收尋哪個使
用者對於歌曲的評分狀況和現在這個使用者 u 的評分狀況相近，便將找到的使用
者所喜歡聽的歌曲推薦給 u。 
6 
 
五、 結果與討論： 
在這部分，我們分別討論兩個部分的結果。 
 
五－A：cross-modal 特徵擷取 
我們採用的音樂資料是來至 up2002 所提供的資料，裡面有 44 個 genres。我們使用 2000
個 samples 支持 Aucouturier and Pachet 去滿足 Monte Carlo Method，並去計算兩個高斯模型
的 KL distance。 
在實驗中，我們使用每個資料庫中的音樂物件去當作音樂物件的查詢，此外，我們每
次使用不同數量的標籤去查詢音樂物件，然後我們評估準確度、精準度以及回傳解釋的結
果，最後我們討論了 random walk restart 演算法。 
 
五－B：推薦系統與視覺化 
至於針對我們推薦的系統，討論了四個問題： 
1. 系統花多久的時間篩選策略？ 
數據上顯示，系統篩選時間約 3.82 秒，並且沒有太大落差，顯示我們系統穩定度。 
2. 對每個使用者分析最後 10 次、20 次、30 次和整個播放過程，使用者對新的歌曲
是否得到的偏好分數是否大於負一？ 
從最後 30 首到最後 10 首的統計過程中，我們可以明顯發現大多使用者對於系統
新推薦的歌曲接受度有越來越好得趨勢。 
3. 我們策略表顯示討論的結果？ 
我們將這個系統給 20 個使用者測試，得到的結果顯示大多數的使用者對於 popular 
strategy 有較高的成功率，而且可以發現實驗的結果顯示 Random Strategy 並不會
讓使用者覺得這是個好得策略。 
4. 我們詢問 20 位使用者針對我們的系統提出了四個相關的問題。 
我們詢問了四個問題，分別是： 
z 介面的滿意度 
z 歌曲的播放滿意度 
z 歌曲播放時間的滿意度 
z 熟悉我們系統的時間 
結果統計，我們的系統讓許多使用者容易接受，而且有很高的滿意度。 
  
8 
 
出席國際學術會議心得報告 
計畫編號 音樂物件的跨模態特徵擷取、音樂推薦與視覺化之研究 
計畫名稱 97-2221-E-030-013- 
出國人員姓名 
服務機關及職稱 徐嘉連 輔仁大學資訊工程系助理教授 
會議時間地點 2009-08-12 ~ 2009-08-14 , 瀋陽遼寧 
會議名稱 International Conference on Hybrid Intelligent Systems 
發表論文題目 A Modified K-means Algorithm for Sequence Clustering 
 
一、參加會議經過 
 
今年，國際混合智慧系統會議（International Conference on HIS）的主題是：Hybridization 
of Intelligent Systems。HIS-2009會議，過去已經連續成功地舉辦了八屆。今年，在八月 12日
到 14日，於中國瀋陽舉辦，許多研究學者、系統開發技術人員都參與了這次第九屆的 HIS，
提供計算智慧技術、交流研究成果。 
 
在參與會議的過程中，無論是報告人或者與會的人員，彼此都很熱絡的討論著。會議中，
邀請到Duke University的 Prof. Paul Wang, Kyushu University的 Prof. H. Takagi, 以及張真誠教
授等人，發表 keynote speech。其中，keynote speaker之一的 Prof. H. Takagi，全程參加多個
presentation sessions，包含：我所發表論文的場次（Semantics Technologies and Knowledge 
Reasoning）。針對論文發表者，提出深刻、精彩的問題與建議。讓與會者受益良多。 
 
本次會議的每篇論文，至少有兩個審查委員，進行審查。同時，會議論文集由 IEEE 
Computer Society出版，論文全文也會收錄在 IEL中。 
 
二、與會心得 
 
Hybridization of intelligent systems，是近日蓬勃發展的研究領域之一，承襲現代人工智慧
的技術，所發展的下一代計算智能與發展系統，來處理多個現實世界的複雜性，包含不精確、
不可靠、含糊。並嘗試將 soft computing, computational intelligent, agent, logic programming, 以
及 intelligent computing等技術，加以整合、延續。 
 
本次會議的主題，包括： 
 Hybrid Intelligent Systems: Architectures and Applications 
 Soft Computing for Image and Signal Processing 
 Intelligent Internet Modeling, Communication and Networking 
 Intelligent Data mining 
 Computational Biology and Bioinformatics 
 Intelligent Business Information Systems 
 Soft Computing for Control and Automation 
 Multi Agent Systems and Applications 
 
這次在 Soft Computing for Image and Signal Processing 以及 Multi Agent Systems and 
Applications兩個主題，與我的 research有關，主要對訊號的分析以及多媒體的應用都有很大
的幫助。 
A Modiſed K-means Algorithm for Sequence Clustering∗
Jia-Lien Hsu and Hong-Xiang Yang
Department of Computer Science and Information Engineering
Fu Jen Catholic University, Hsinchuang, Taipei Hsien 24205, Taiwan, R.O.C.
alien@csie.fju.edu.tw
Abstract
In this paper, we extend our research to construct a system
which provides clustering services, more than user-active search.
We use DCT mapping to extract features from sequences, and dis-
cuss sequence similarities of whole similarity and partial simi-
larity. The two kinds of similarity concepts will be applied when
clustering sequences of equal-length and variable-length, respec-
tively. In the case of equal-length, we map a sequence to a f -
dimensional point in the feature space, and then cluster these se-
quences accordingly by applying hierarchical clustering and par-
titional clustering (i.e., K-means). In the case of variable-length,
we cut a sequence into subsequences by sliding window, and map
subsequences to f -dimensional points. We propose a Modiſed
K-means (MK) algorithm to handle partial similarity of subse-
quences. Finally, we implement our methods and perform exper-
iments to show the efſciency and effectiveness of our approach.
1. Introduction
Sequence data mining is one of the key issues in data min-
ing research. A sequence data may be a collection of obser-
vations made sequentially in time [10], or simply a order col-
lection of events and symbols. Typical sequence data are stock
value, medical, weather records and multimedia data, such as mu-
sic data. Some fundamental operations of sequence data min-
ing have been discussed, including searching/matching, cluster-
ing, classiſcation and ſnding nontrivial patterns. Our research
interest has been focused on “content-based retrieval for multime-
dia database” [11, 15]. In this paper, we extend our research to
provide a clustering service for sequence data.
1.1. Clustering
Also, in multimedia database, clustering analysis is an impor-
tant technique. We can group a collection of patterns into similar
clusters. The problem is to group unlabeled patterns into similar
clusters in a collection.
Clustering analysis is an unsupervised classiſcation consisting
of three fundamental steps: (1) feature selection and extraction,
(2) similarity measure, and (3) grouping (see Fig. 1). Different
kinds of approaches for clustering show in Fig. 2.
One of the most widely approach is the hierarchical clustering
which yields a dendrogram representing the hierarchical group of
∗This research was supported by Fu Jen Catholic University with Project No.
409731044039, and sponsored by the National Science Council under Contract
No. NSC-97-2221-E-030-013.
Figure 1. Three stages in clustering [7].
Figure 2. A taxonomy of clustering approaches [7].
patterns. In the hierarchical clustering, users do not have to speci-
ſy the number of cluster, and the clustering result is a tree (see
Fig. 3).
Figure 3. The dendrogram of hierarchical clustering.
In the partitional clustering, we can group patterns into k clus-
ters. One of the most common and simple methods in partitional
clustering is the K-means algorithm, which will be discussed in
the following section.
1.2. Related Work
Agrawal, et al. [2] propose an indexing method, called F -
index method, for similarity search of time sequences. They apply
the Discrete Fourier Transform (DFT) to extract features from se-
quences, and then choose DFT coefſcients to build a multidimen-
sional index (see Fig. 4). Also, they use a spatial access method to
process range and approximate queries. Since the approach focus
on the whole matching, Faloutsos, et al. [5] extend the F -index
method to an indexing method called ST -index. Similarity, the
basic idea is to map a sequence into a set of rectangles in the fea-
ture space. They use a sliding window over the sequence data,
2009 Ninth International Conference on Hybrid Intelligent Systems
978-0-7695-3745-0/09 $25.00 © 2009 IEEE
DOI 10.1109/HIS.2009.64
287
Figure 10. The three ſnal centers found by k-means on
the cylinder-bell-funnel dataset. The shapes of the cen-
ters are close approximation of the original patterns [8].
Figure 11. Cut into subsequence by a sliding window in
one long sequence.
Figure 12. The three ſnal centers found by subsequence
clustering using the sliding window approach [8].
Figure 13. The curve-based method for ſnding approxi-
mate repeating patterns [11].
ods to improve the curve-based method. The author designs a
frame sampling technique for searching patterns more efſciently,
and a re-mapping method for better performance.
2. The Problem Formulation
We ſrst describe distance between sequences, and feature ex-
traction of sequence by mapping sequences into f -dimensional
points in the feature space. When determining clusters of se-
quences, we consider two cases of data sequences, as well as the
corresponding sequence similarities.
2.1. The Distance and Feature Extraction of Se-
quence
In our approach, the distance measure is deſned by Euclidean
distance. Also, by applying the Discrete Fourier Transform, each
sequence will be mapped into a f -dimensional points in the fea-
ture space, as deſned as follows:
Deſnition 1 Given a sequence of length n, S = {s1, s2, ..., sn},
apply DFT for S to derive the following coefſcients
{sˆ1, sˆ2, ..., sˆn}. With 1 ≤ f ≤ n, we map S to a f -dimensional
point, denoted by (sˆ1, sˆ2, ..., sˆf ), in feature space.
According to Parseval’s theorem [12], we have:
n−1∑
i=0
|xi|2 =
n−1∑
k=0
|Xk|2 (1)
where
−→
X is the Discrete Fourier Transform (or any orthonormal
transform, such as the Discrete Cosine Transform (DCT) [4]) of
the sequence −→x , both of length n.
Therefore, as two sequences are similar, the corresponding
points will be closer, shown in Fig. 14.
Figure 14. Feature extraction of sequence: The DCT
mapping.
2.2. Similarity of Sequence
Whole similarity: Consider sequences of equal-length, those
sequences which are in accordance with whole trend will be
grouped into similar clusters. For example, as shown in Fig.
15, the six sequences will be clustered into three clusters, which
are C1 = {Seq.1, Seq.2, Seq.3}, C2 = {Seq.4}, and C3 =
{Seq.5, Seq.6}, The three clusters are normal, upward shift, and
downward shift, respectively.
Figure 15. Six sequences of equal-length.
Partial similarity : In some collections of sequences, these se-
quences may be of different lengths (e.g., in a music collection).
We would like to ſnd those similar and mutual pieces from se-
quences, and group the pieces into similar clusters accordingly.
For example, there are three sequences in Fig. 16. The Seq.1.(a)
is similar to the Seq.3.(a), and the Seq.1.(b) is similar to the
Seq.2.(b). Note that, in our approach, one sequence may have
289
Table 1. The Modiſed K-means (MK) Algorithm
Parameter: k and tlcp
Begin
1 Mark labels for all points randomly.
2 Calculate cluster centers.
3 Calculate distance between all object points and
cluster center, and assign object points to the clus-
ter of nearest center.
4 Re-calculate cluster centers using the current clus-
ter memberships.
5 If cluster memberships do not change anymore, go
to step 6. Otherwise, go to step 3.
6 For each sequence S with cluster labels <
l1, l1, l2, l1, ... >, if the consecutive subsequence
whose length is less than tlcp, discard the labels
of shorter subsequences.
7 Report the clustering results.
End.
B, C and D. If k = 2 and tlcp = 1, thus:
⎧⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
O = {A,B,C,D}
OAP = {oAp1, oAp2, ..., oAp6}
OBP = {oBp1, oBp2, ..., oBp5}
OCP = {oCp1, oCp2, ..., oCp5}
ODP = {oDp1, oDp2, ..., oDp4}
l = {l1, l2}
CC = {cc1, cc2}
The clustering result of Figure 19 is illustrated in Figure 20.
Among the points of object B, four points are assigned into cluster
C1, and one point is assigned into cluster C2. The two clusters are
C1 = {A,B} and C2 = {B,C,D}.
Figure 20. The clustering result of Figure 19 (k = 2).
4. Experiments
The speciſcation of our testing platform is a PC of Pentium 4,
2GHz, with memory of 1GB, running Windows XP. We imple-
ment our method by Java (j2sdk1.4.2 08) and MATLAB 6.5.
4.1. The Dataset
We use the synthetic control chart time series data in UCI KDD
Archive [1]. There are six different classes in this dataset: (1) nor-
mal, (2) cyclic, (3) increasing trend, (4) decreasing trend, (5) up-
ward shift, and (6) downward shift. Each class has 100 instances
and the length is 60.
4.2. Cluster Validity
In order to measure the clustering results, we apply a statistical
index, called Rand statistic [14]. Suppose that we have two clus-
tering results C = {C1, C2, ..., Cn} and P = {P1, P2, ..., Pm} in
X = {x1, x2, ..., xs}, and the number of clusters in C and P are
not required to be the same (i.e., n = m or n = m).
For example, let X = {x1, x2, x3, x4, x5, x6}, the cluster-
ing results are the C = {{x1, x2, x3}, {x4, x5}, {x6}} and P =
{{x1, x2, x3}, {x4, x5, x6}}. For each pair (xi, xj), there are four
conditions as following: (1) SS, if xi and xj belong to the same
cluster in C and to the same cluster in P . (2) DD, if both belong
to the different cluster in C and to the different cluster in P . (3)
SD, if both belong to the same cluster in C and to the different
cluster in P . (4) DS, if both belong to the different cluster in
C and to the same cluster in P . Table 2 shows the result of our
example.
Table 2. Statistical index matrix [14].
x1 x2 x3 x4 x5 x6
x1 SS SS DD DD DD
x2 SS DD DD DD
x3 DD DD DD
x4 SS DS
x5 DS
x6
Let a, b, c and d be the number of SS, SD,DS and DD, re-
spectively, and M = a+ b+ c+d. In this example, a = 4, b = 0,
c = 2, d = 9 and M = 15. The statistical index is deſned as
follows:
Rand statistic R =
(a + d)
M
(2)
Therefore, the R = 0.867. The R of larger value indicates the
higher agreement between C and P .
4.3. Sequence Clustering
Equal-Length: In the ſrst experiment, we select ten instances
for each class from the dataset in a random manner, and apply
three linkage methods of hierarchical clustering, and K-means al-
gorithm (with k = 6) on the selected sequences. The elapsed time
and cluster validity are shown in Fig. 21 and Fig. 22, respectively.
Figure 21. Elapsed time of three linkage methods and
K-means algorithm ( equal-length).
Variable-Length: In this experiment, we randomly selected
twenty instances for each class from the dataset, and then mod-
ify equal-length sequences to generate variable-length sequences
291
