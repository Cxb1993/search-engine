II 
 
英文摘要 
Keywords: Medical Image Retrieval; Content Based Image Retrieval; Similar Image 
Retrieval; Personalized Oriented; Relevance Feedback 
Medical Images (like as X-Ray, CT, MRI, etc.) is an important medium for doctor 
to diagnose patient. Large amount of medical images were produced by the radial 
department daily. The medical images need a management system help to fast 
retrieval similar images. In this project, we study on a convenient and precision 
medical image retrieval system.  
  This project is composed of three parts and was conducted in the past three years. 
The goal of this research is to investigate and develop the techniques focusing on 
content based medical image retrieval and relevance feedback, Combine semantic and 
visual features for medical image retrieval and Auto annotation for medical images 
with user feedback.  
The First-Year Project: A Study on Content Based Medical Image Retrieval and 
Relevance Feedback. 
In this project we propose a new relevance feedback model suitable for medical image 
retrieval. The proposed method enables the user to rank the results in relevance order. 
According to the ranking, the system can automatically determine the importance 
ranking of features, and use this ranking to automatically adjust the weight of each 
feature. The experimental results show that the new relevance feedback mechanism 
outperforms previous relevance feedback models. 
 
The Second-Year Project: A Study on Semantic and Visual Features for Medical 
Image Retrieval 
In this project we combine textual and content-based approaches to retrieve relevant 
medical images. The content-based approach represents the visual features and the 
text-based approach using word expansion is developed to find similar semantic 
concept documents. The mixed retrieval of visual and textual data was done by 
combining the content-based approach and the text-approach to get better precision 
result. Experimental results show that combining both the content-based and 
text-based approaches is better than using only one approach. 
 
The Third-Year Project: A Study on Auto Annotation for Medical Images with User 
Feedback 
In this project, content based image retrieval technique and a multi-class classification 
method based on support vector machines is used to classify un-annotated medical 
images. In this article, we proposed a pyramid spatial image feature representation to 
denote the local and global spatial relationship between pixels. 
We investigate a supervised learning approach to associate image features with 
semantic categories for image classification. A support vector machine (SVM) is used 
to elucidate image feature characteristics for image classification. Several image 
visual features are designed to describe the visual content of medical images. The 
SVM is trained by 1600 images to predict the categories of 400 test images. 
Experimental results show that using a combination of all the image features will 
IV 
 
 
目       錄 
中文摘要............................................................ I 
英文摘要........................................................... II 
目    錄........................................................... IV 
圖目錄.............................................................. V 
表目錄............................................................. VI 
 
計畫概述............................................................ 1 
執行成果............................................................ 3 
 
計畫分年報告 
回饋式醫學影像意涵查詢系統之研究(1/3)............................... 5 
回饋式醫學影像意涵查詢系統之研究(2/3).............................. 18 
回饋式醫學影像意涵查詢系統之研究(3/3).............................. 32 
 
  
VI 
 
表目錄 
Table 1: The mean average precision at n-iteration relevance feedback. .................... 17 
Table 2: The query results of visual only runs and the weight of visual image features
 ......................................................................................................................... 29 
Table 3: The result of mixed retrieval runs and the weight of visual image features and 
textual features ................................................................................................. 29 
Table 4: The experiment results of different features. ................................................. 39 
 
 
 2 
 
求得代表此影像之關鍵字。如此一來使用者可以利用熟悉的關鍵字檢索的方式來
查詢所要尋找之影像。由於利用機器學習與圖形識別的技術並不一定可以達到十
分準確的效果，所以我們預期利用使用者相關回饋 (User Relevance Feedback) 
的方式來提升我們系統的準確度。 
本計畫共分為三年期計畫，研究範疇及目的分述如下: 
 第一年：以使用者觀點考量之回饋式醫學影像內容相似查詢之研究 
研究範疇包含：1) 提高影像檢索的精確率，影像的詮釋依各人不同的
認知會影響查詢的結果，使用者回饋式的查詢方式可提高查詢的精確率，
本計畫預期提出一個可以由使用者引導的方式，來修正相似度評估計算
以期能夠較符合使用者的觀點來檢索相似醫學影像；2) 影像呈現及回
饋方式的客製化介面設計，以提供使用者能夠反應個人需求的查詢結
果。 
 第二年：結合語意與醫學影像內容查詢之技術研究 
研究範疇包含：1)建立以語意模式來查詢醫學影像的環境；2)分析病例
內容與醫學圖像的關係，建立文件與醫學影像間的關聯，以建構複合式
醫學病例查詢系統。 
 第三年：回饋式醫學影像自動註解查詢系統之研究 
研究範疇包含: 1) 使用機器學習 (Machine Learning)與圖形識別 
(Pattern Recognition) 的方法來自動將對影像建立註解，提供語意式
的查詢。2)建立使用者對於註解的回饋機制來提高註解系統的正確性。 
 
  
 4 
 
在本年度的研究中我們針對所提出的影像特徵加以分析並以(support vector 
machine) SVM 來學習類別特徵並對沒有註解的影像進行自動分類，建立註解。
為了提高註解的正確性，在使用者的查詢過程中我們設計了一個回饋行為累積器，
利用回饋影像及文字矩陣調整關鍵字與類別之間的關係，有效的來改進註解的正
確性。 
 
 
  
 6 
 
Abstract 
Content-based image retrieval (CBIR) is a group of techniques that analyzes the 
visual features (such as color, shape, texture) of an example image or image sub 
region to find similar images in an image database. Relevance feedback is often used 
in a CBIR system to help users express their preference and improve query results. 
Traditional relevance feedback relies on positive and negative examples to 
reformulate the query. Furthermore, if the system employs several visual features for 
a query, the weight of each feature is adjusted manually by the user or system 
predetermined and fixed by the system. In this project we propose a new relevance 
feedback model suitable for medical image retrieval. The proposed method enables 
the user to rank the results in relevance order. According to the ranking, the system 
can automatically determine the importance ranking of features, and use this ranking 
to automatically adjust the weight of each feature. The experimental results show that 
the new relevance feedback mechanism outperforms previous relevance feedback 
models. 
Key words: 
Content-based image retrieval; Relevance feedback; Image Database. 
1. Introduction 
Content-based image retrieval (CBIR) is a promising technology to assist image 
finding. CBIR retrieves images by visual features inherent in images. CBIR allows the 
user to query an image database by image examples, partial regions of an image, or 
sketch contours example, etc. IBM in 1995 developed the QBIC system [10] that 
allows the user to query a large image database based on visual image features such as 
color percentages, color layout, and textures occurring in images. The user can match 
colors, textures and their positions without describing them in words. CBIR offers an 
alternative to retrieve desired images. CBIR is more convenient and economic than 
annotation-based image retrieval because the visual image features of all images in 
database can be automatically extracted. 
Many studies show that relevance feedback can significantly improve the 
effectiveness of CBIR because relevance feedback helps the system to refine the 
feature’s weight according to user’s preference. Some users may want to find images 
with similar colors, whereas others may want to find images with similar shapes. 
Relevance feedback allows the user to reflect his preference to the system, then the 
system can reformulate the query according to the positive and/or negative examples 
responded by the user. In the Spink’s [22] study show that the degree of relevance 
will better identify the user needs and preferences 
 8 
 



n
j
j
i
T
i
Y
q
1


        (2) 
, where Yi is the n  Ki ( Ki is the length of feature i) training sample matrix for the 
feature i obtained by stacking the n positive feedback training vectors into a matrix. 
The n element vector =[1, 2,…n] represents the degree of relevance of each of the 
n positive feedback images, which can be determined by the user at each feedback 
interaction. The system then uses qi as the optimal query to evaluate the relevance of 
images in the database. This strategy is widely used by many image retrieval and 
relevance feedback systems [12][20].  
The Bayisian estimation method has been used in many probabilistic approaches to 
relevance feedback. Cox et al. [9], Vasconcelos and Lippman [25] used Bayesian 
learning to incorporate user feedbacks to update the probability distribution of all 
images in the database. They consider the feedback examples as a sequence of 
independent queries and try to minimize the retrieval error by Bayesian rules. In other 
words, given a sequence of queries, they attempt to minimize the probability of 
retrieval error as  
)},....,|()|({maxarg    
),...,|(maxarg)(
11
i
1


tt
t
i
xxiyPiyxP
xxiyPxg
       (3) 
, where {x1,…,xt} is a sequence of queries (feedback examples) and P(y=i|x1,…,xt) is a 
prior belief about the ability of the ith image class to explain the queries. 
PicHunter [9] implements a probabilistic relevance feedback mechanism, which tries 
to predict the target image the user wants based on his actions (the images he selects 
as similar to the target in each iteration of a query session). A vector is used for 
retaining each image's probability of being the target. This vector is updated at each 
relevance feedback, based on the history of the session (images displayed by the 
system and user's actions in previous iterations). The updating formula is based on 
Bayes' rule. If the n database images are denoted as Tj, j=1,…,n, and the history of the 
session through iteration t is denoted as Ht={D1,A1,D2,A2,……,Dt,At}, with Dj and Aj 
being the images displayed by the system and, respectively, the action taken by the 
user at the iteration j, then the iterative update of the probability estimate of an image 
Ti being the target, given the history Ht, is:  
  






n
j tjttjt
tittit
tttiti
HTTPHDTTAP
HTTPHDTTAP
HADTTPHTTP
1 11
11
1
)|(),,|(
)|(),,|(
),,|()|(
       (4) 
Most current relevance feedback schemes use dichotomy relevance measurement, 
relevant or non-relevant. Many relevance research works indicates that users’ 
 10 
 
We propose an algorithm to judge which methods are the most suitable for the user. In 
the feedback process, the user ranks a sequence of relevant images in an order with 
respect to their similarity to the query image. Based on the ranking sequence, we can 
estimate how each designed method is close to user’s opinion. If the feature used by 
one method is closer to user’s opinion, then the ranking sequence generated by this 
method must be closer to the ranking sequence responded by the user.  
If the user considers that p1 is more similar to the target image than p2, we denote p1 
> p2.  If the similarity degree of p1 and p2 are the same, we denote p1=p2. 
(p1>p2>p3>p4>p5>p6) is such a ranking sequence, the leftmost and rightmost of 
which are, respectively, the most and least similar to the target.  
In the system, each feature will affect the resultant ranking sequence. We can analyze 
how each feature is close to the sequence responded by the user to adjust the weight 
of each feature. For example, suppose that the ranking sequence responded by the 
user is (p1>p2>p3>p4>p5>p6). If the output sequence of the method M1 is 
(p1>p2>p3>p4>p6>p5) and that of the method M2 is (p6>p5>p4>p3>p2>p1), we can 
find that the method M1 is closer to user’s expectance than the method M2. Therefore, 
reducing the weight of the feature used by M2 and increasing the weight of the feature 
used by M1 will produce a better result. Based on the above idea, the problem of 
evaluating the importance of each feature (and the corresponding method) becomes 
sequence comparison. 
We employ the Rnorm [3]method to evaluate how close two sequences are. The Rnorm 
comparison is defined as follows: 
Definition 1: Let I be a finite set of images with a user-defined preference relation  
that is complete and transitive (weak order). Let user be the rank ordering of I induced 
by the user preference relation. Also, let system be some rank ordering of I induced by 
the similarity values computed by an image retrieval system. Then Rnorm is defined as  
)1(
2
1
),(
max

 

S
SS
R usersystmenorm     (5) 
where S
+
 is the number of image pairs where a better image is ranked ahead of a 
worse one by system; S- is the number of pairs where a worse image is ranked ahead of 
a better one by system;; and maxS  is the maximum possible number of S
+
 from user. 
It should be noted that the calculation of S
+
, S
-
, and maxS is based on the ranking of 
image pairs in system relative to the ranking of corresponding image pairs in user. 
Example: consider the following two rank orderings: 
 user=(p1=p4>p2=p3>p5) and system=(p5>p2=p4>p1=p3). According to the user, p1 
and p4 have the highest preference, followed by p2 and p3 at the next level of 
 12 
 
medical image retrieval. The types of features used in our system are quite different, 
and they have been shown excellent performance in medical image retrieval [7]. 
4. Medical Image Features 
An image consists of a large amount of pixels. In order to efficiently retrieve images 
relevant to a query, a CBIR system usually extracts low-level image features to 
represent an image in an off-line preprocessing stage. Image features can be 
categorized into color, shape, texture and spatial relationships. In this section, we 
design four features based on human’s viewpoint to capture a medical image’s color, 
shape and spatial relationships. They are Color histogram, Gray Level Histogram, 
and Semantic Moment. This section describes these features in detail.  The 
proposed features reduce semantic gap effectively and have excellent result in 
medical image retrieval task of ImageCLEF 2004 [7]. 
Color histogram defines the similarity degree between color bins by a mechanism 
corresponding to human's perception. Color histogram [24] is a basic method for 
representing image content and has good performance.  
The colors of an image are represented in the HSV (Hue, Saturation, and Value) space, 
which is closer to human perception than spaces such as RGB (Red, Green, and Blue) 
or CMY (Cyan, Magenta, and Yellow). In implementation, we quantize HSV space 
into 18 hues, 2 saturations, and 4 values, with additional 4 levels of gray values; as a 
result, there are a total of 148 bins. 
Gary level histogram concentrating on the contrast of medical images avoids the 
effect of different parameters caused by the environment creating images. In this 
project we propose a relative normalization method. First, we cluster the whole image 
into four clusters by the K-means cluster method [12]. We sort the four clusters in 
ascendant order according to their mean values. We shift the mean of the first cluster 
to value 50 and that of the fourth cluster to value 200; then each pixel in a cluster is 
multiplied by a relative weight to normalize. Let mc1 is the mean value of cluster 1 
and mc4 is the mean value of cluster 4. The normalization formula of pixel p(x,y) is 
defined as Eq. (8). 
)(
200
))50(),((),(
14
1
cc
cnormal
mm
myxpyxp

       (8) 
After normalization, we resize each image into 128*128 pixels, and use one-level 
wavelet with Haar wavelet function to generate the low frequency and high frequency 
sub-images. Processing an image using the low-pass filter will obtain an image more 
consistent than the original one; on the contrary, processing an image using the 
high-pass filter will obtain an image that has high variation. The high-frequency part 
keeps the contour of the image. 
 14 
 
project we follow the ImageCLEF 2004 evaluation to evaluate the performance of the 
feedback mechanism. The process of evaluation and the format of results employ the 
trec_eval tool [8]. There are 26 queries for test. The corresponding answer images of 
each query were judged as either relevant or partially relevant by at least 2 assessors. 
 We conduct three experiments. Color histogram, Gray Level Histogram, 
Semantic Moment, and Shape Correlogram are the four features for retrieving 
similar medical images. To show that the proposed relevance feecback mechanism is 
very flexible, the types of image features we use are quite different. The first 
experiment, called BASE, uses the visual features of the query example to query the 
database without relevance feedback. The comparison has been done with the method 
by Y. Rui in ref.[20], called RUI, that associates larger weights with more important 
dimensions and smaller weights with less important ones. This method generalizes a 
relevance feedback framework of the physical features based on positive feedback 
examples. We normalize different concept features by Gaussian normalization, and 
the weights of concept features are equal. Another ranked based method for 
comparison has been done by B. BARTELL in ref.[2], denoted as BT. The 
experiment, denoted as ARF (Adaptive Relevance Feedback), is the result that uses 
the proposed feedback mechanism. The system integrates the four features by 
Gaussian normalization in the first run. While the second run, we adjust the weight of 
concept features by the Rnorm method. In the physical level, the query of color 
histogram and gray-level histogram featues are reformulated by the method 
proposed in [20]. The weight of Coherence moment and Shape correlogram 
features are tuned by the Rnorm method. The test result shows that the feedback 
mechanisms (RUI and ARF)have better result than the mechanism without relevance 
feedback (BASE). While the user feedbacks its interests to the system, the proposed 
method (ARF) is more precise and quicker to reach user’s requirement. Figure 1 
shows the precision and recall graphs. RUI, BT and ARF curves are the result after 
conducting relevance feedback three times  
The mean average precision of BASE is 0.3273. The mean average precision of 
RUI is 0.3884. The mean average precision of BT is 0.412. The mean average 
precision of ARF is 0.4412. Table 1 is the mean average precision and relevance 
feedback iterations. As shown in Table 1, the ARF method reaches the user’s interests 
faster than the RUI method.  
The experimental result shows that the proposed feedback method can be used for 
integrating arbitrary concept features. We can estimate which features are more 
important although the scales of features are different. 
 16 
 
psychophysical experiments. IEEE Trans. Image Processing—Special Issue on 
Digital Libraries. 
[10] Flickner, M., Sawhney, H., Niblack, W., Ashley, J., Huang, Q., Dom, B., Gorkani, 
M., Hafner, J., Lee, D., Petkovic, D., Steele, D., & Yanker, P. (1995). Query by 
Image and Video Content: The QBIC system. IEEE Computer, 28(9), 23-32. 
[11] Hampapur, A., Gupta, A., Horowitz, B., Shu, C.-F., Fuller, C., Bach, J., Gorkani, 
M., & Jain, R. (1997). Virage video engine. in: I. K. Sethi, R. C. Jain (Eds.). 
Storage and Retrieval for Image and Video Databases V, Vol. 3022 of SPIE 
Proceedings (pp. 352-360). 
[12] Han, J., & Kamber, & M. Data mining: concepts and techniques. Academic press, 
San Diego, CA, USA. 
[13] Huang, J., Kumar, S. R., Mitra, M., Zhu, W. J., & Zabih, R. (1997) Image 
indexing Using Color Correlograms. Proceedings of IEEE Conference on 
Computer Vision and Pattern Recognition, San Juan, Puerto Rico. 
[14] Ishikawa, Y., Subramanya, R., & Faloutsos, C. (1998). Mindreader: Query 
databases through multiple examples. in Proc. 24th VLDB Conf., New York. 
[15] Kushki, A., Androutsos, P., Plataniotis, K. N., & Ventsanopoulos, A. N. (2004). 
Query Feedback for Interactive Image Retrieval. IEEE transactions on circuits 
and systems for video technology, vol 14, No 5. 
[16] Lu, Y., Hu, C., Zhu, X., Zhang, H., & Yang, Q. (2000). A unified framework for 
semantics and feature based relevance feedback in image retrieval systems. in 
Proc. 8th ACM Multimedia Int. Conf., Los Angeles, CA. 
[17] Ma, W. Y., Deng, Y., & Manjunath, B. S. (1997). Tools for texture- and 
color-based search of images. in: B. E. Rogowitz, T. N. Pappas (Eds.), Human 
Vision and Electronic Imaging II, Vol. 3016 of SPIE Proceedings (pp. 496-507). 
San Jose, CA. 
[18] Pentland, A., Picard, R. W., & Sclaro, S. (1996). Photobook: Tools for 
content-based manipulation of image databases. International Journal of 
Computer Vision 18 (3), 233-254. 
[19] Rocchio, J. J. (1971). Relevance feedback in information retrieval. in The 
SMART Retrieval System: Experiments in Automatic Document Processing 
(pp.313–323). 
[20] Rui, Y. & Huang, T. S. (1999). Relevance feedback: A power tool for interactive 
content-based image retrieval. IEEE Circuits Syst. Video Technol., vol. 8, no. 5. 
[21] Shaw, W. M. (1995). Term-relevance computation and perfect retrieval 
performance. Inform. Process. Manage, vol. 31, pp. 491–498. 
 18 
 
 
 
 
 
回饋式醫學影像意涵查詢系統之研究(2/3) 
 
 
 
計畫類別： 個別型 
計畫編號： NSC 96-2221-E-259-017-MY3 
執行期間： 97 年08 月01 日至98 年07 月31 日 
執行單位： 國立東華大學資訊管理學系 
計畫主持人： 楊維邦 
 
  
 20 
 
medical domain. 
The medical image retrieval systems allow user can use image example to retrieve 
related documents or use native language to query another language documents or 
images. Figure 2 is the data model of cross medical image retrieval system. When 
user issues query by image example first, visual feature is used to find visual similar 
images. The images are associated with case notes; a written description of a previous 
diagnosis for an illness and the image identifies (e.g. Figure 3). Case notes consist of 
several fields including: a diagnosis, a free textual description, clinical presentation, 
keywords and title. We can use the associated notes to improve the query result. 
Extracting the possible keyword from the visual similar result as query words used to 
execute text query. In the last phase, we can combine the text and visual result to 
refine the query result. 
 
Figure 2: The data model of our system 
An example case and images 
<Description> 
 
X ray: Mass effect within the soft tissues of the proximal part of the left calf, difficult 
to outline, seen only as it displaces the fat planes. There are no calcifications. The 
adjacent bone is normal. 
 
MRI: Oval mass within the medial gastrocnemius muscle, very well delineated, 
slightly lobulated (axial cuts). Its structure is heterogeneous. On T1, it is slightly 
hyperintense compared to the adjacent muscle (red arrow). It is isointense to fat on 
proton density images (DP), very hyperintense on T2 and IR with some hypointense 
Content Based 
 Image Retrieval 
Image 
Database 
Merging Results and 
 Re-Ranking of Images 
Text Retrieval 
Retrieved Cases & 
Images Associated 
Text query 
generatio
n 
Associated 
Cases 
Language 
Translation 
 22 
 
for image and multimedia databases. In the experiment result show that combines the 
text and visual feature will get better performance. 
In Section 2 of this article the image features for the content-based approach are 
described. Section 3 illustrates the text-based approach that processes the word 
expansion. Section 4 show the experiment result that use combining approach will get 
better performance. We provide an explanation and discussion of our experimental 
results. Finally, Section 5 provides concluding remarks and future direction for 
medical image retrieval research. 
2. Image Features 
This section describes the features used for the ImageCLEF [16] evaluation. In an 
image retrieval system image features are extracted from pixels of images. To get fast 
response time the image features should be simple and concise. Further, the image 
features must include enough meaningful information to represent the image. In this 
paper we adopt several image features that we have proposed [2], which have good 
performance in medical image application. 
Normalization should be done before quantization to reduce the illuminative influence. 
In [2], we proposed a relative normalization method concentrating our attention on the 
contrast of an image. A whole image was separated into four clusters by the K-means 
method [3]. The four clusters were sorted in ascending order according to their mean 
values. After clustering, we shifted the mean of the first cluster to a value of 50 and 
the fourth cluster to 200; then, each pixel in a cluster was multiplied by a relative 
weight to normalize. Let mc1 be the mean value of cluster 1 and mc4 be the mean value 
of cluster 4. The normalization formula of pixel p(x,y) is defined in Eq. (1). 
)(
200
))50(),((),(
14
1
cc
cnormal
mm
myxpyxp

   (1) 
After normalization, we scaled an image to 128*128 pixels and extracted image 
features. 
2.1 Facade Scale Image Feature 
The pixel values of an image are trivial and straight-forward features. For 
computational efficiency, images are always scaled to a common small size and 
compared using the Euclidean distance. [4] has shown that optical character 
recognition and medical image retrieval based on this image features have obtained 
excellent results. In this work we scale down an image into 88 pixels to form a 
64-feature vector as facade scale image feature. 
 24 
 
four classes and calculate the number of big objects (COH) and small objects 
(COH). Each pixel will be influenced by its neighboring pixels. Two close objects of 
the same class may be merged into one object. Then, we can analyze the variation 
between the two images before and after smoothing. The coherence moment of each 
class forms a seven-feature vector, (COH, COH, COH, COH, COH, COH, 
COH). The coherence moment of an image is a 56-feature vector that combines the 
coherence moments of the four classes. 
2.4 Color Histogram Features  
The color histogram [[24]] is a basic method and has demonstrated good performance 
in representing image content. The color histogram method gathers statistics about the 
proportion of each color as the signature of an image. In this work the colors of an 
image are represented in the HSV (Hue/Saturation/Value) space, which is closer to 
human perception than other models, such as RGB (Red/Green/Blue) or CMY 
(Cyan/Magenta/Yellow). The HSV space is quantized into 18 hues, 2 saturations, and 
4 values. Also, we consider an additional 4 levels of gray values and thus have 148 
(i.e., 1824+4) bins total. Let C (|C| = m) a set of colors (i.e., 148 bins), PI is 
represented as Eq. (3), which models the color histogram H(PI) as a vector in which 
each bucket 
ic
h  counts the ratio of pixels of PI  in color ci. 
 )(),...,()(
1 IcIcI
PhPhPH
m
  (3) 
In many previous studies, each pixel is only assigned to a single color. Consider the 
following situation: I1, I2 are two images and all pixels of I1 and I2 fall into ci and ci+1 
respectively; I1 and I2 are indeed similar to each other, but the similarity computed by 
the color histogram will regard them as different images. To address this problem we 
set an interval range  to extend the color of each pixel and introduce the idea of a 
partial pixel as shown in Eq.(4), 
||
||
)(
I
Pp
pp
Ic
P
Ph I
i



 

 
(4) 
Let ci-1, ci, and ci+1 stand for a color bin; p is the value of a pixel. ]
2
,
2
[

 pp  
denotes the interval range ,  ],[
pp
  is the intersection of ]
2
,
2
[

 pp  and ci. The 
contributions of the pixel to ci and ci-1 are computed as 

 ||
pp

 and 

 |)2(|
p
p 
, 
respectively. It is clear that a pixel has its contributions not only to ci but also to its 
neighboring bins. 
 26 
 
.)|),((|  ,|),(|),( 2   dxdyuyxWδanddxdyyxWyx mnmnmnmnmn   (7) 
This image feature is constructed by mn and mn of different scales and orientations. 
Our experiment uses four (S=4) as the scale and six (K=6) as the orientation to 
construct a 48-feature vector f , as shown in Eq. (8). 
].,,......,,,,[ 353501010000  uuuf   (8
) 
3. Textual Vector Representations 
In the medical image always accompany with diagnosis by doctor. Two doctors may 
have different note of diagnoses, but the medical image of the disease are similar. In 
this section, we proposed a method based on co-occurrence relationship from similar 
image to solve the synonym problem. The overall search process is shown in Fig. 3. 
Given an initial query Q, the system performs retrieval and returns a set of relevant 
documents to the user. We use the representation expressing a query as a vector in the 
vector space model [6]. The Textual Vector Representation is defined as follows. Let 
W (|W| = n) be the set of significant keywords in the corpus. For a document D, its 
textual vector representation (i.e., DT) is defined as Eq. (9), 
 
Figure 4: Text-based query refine flowchart 
 
 )(),...,(
1 TtTtT
DwDwD
n
 (9) 
where the n dimensions indicate the weighting of a keyword ti in DT, which is 
measured by TF-IDF [6], as computed in Eq.(10); 
i
Ti
i
t
Dt
Tt
n
N
tf
tf
Dw log
max
)(
,
  
(
10) 
In Eq.(7), 
tf
tf
Ti Dt
max
,
 stands for the normalized frequency of ti in DT, max tf is the 
maximum number of occurrences of any keyword in DT, N indicates the number of 
New Query 
Formula 
Query 
Expansion 
Similar Image 
Analysis 
Wordnet 
 
Co-occurrence 
Relationships 
Query 
Q 
 28 
 
4. Experimental Results 
In ImageCLEF, the medical image retrieval task contains 25 queries for evaluation. 
The queries are mixed visual images and semantic textual data. The visual queries use 
image examples to find similar images; each topic contains at least one image 
example. The semantic textual queries allow user query by a sentence, from which 
high-level semantic concepts are directly derived from images with difficulty. The 
goal is to promote visual and textual retrieval together and find out what works well 
in which cases. 
In the automatic category, the methods can be classified into three sub-categories: 
Text only, Visual only and Mixed retrieval (visual and textual) according to the 
feature used. The category ―Text only‖ means that systems use textual features only to 
retrieve relevant images. The ―Visual only‖ category means that systems only use 
visual image features without combining textual annotation to retrieve similar images. 
Mixed retrieval means the systems combine visual and textual features to retrieve 
images. 
In this article we examine the visual feature, textual and the mixed retrieval automatic. 
In the content-based approach we combine five image features by weighted adjusting 
to retrieve related images. Table 2 lists the query results of visual only runs with 
different weight of four image features. Table 3 lists the results of textual only and 
mixed retrieval runs and the setting weight of image features and textual features. The 
difference of each run is the weighted setting of features. 
The query topics contain color and gray images. We first determine whether the 
query’s image is color or gray by color/gray feature. Depending on whether the 
image is color or gray, we set different weights for the image features. In the Table 2, 
―C‖ denotes that the query image is a color image and ―G‖ denotes that the query 
image is a gray image. The run ―visual_R6‖ has the better result in our experiment. 
The weights of each feature are set equal, which means that five image features have 
the same importance.  
The setting weights of mixed runs and results are listed in table 3. The result of 
runs 8, 9 and 10 illustrate that combining the visual and textual features will achieve 
better results than single features. Run 8 assume that the significant of visual and 
textual feature are equal. Run 9 emphasize the weight of visual features and Run 10 
emphasizes the weight of textual features. The results show that the text-based 
approach is better than the content-based approach, but the content-based approach 
can improve the textual results. 
In the table 3, the result of Textual_1 and Textual_2 denote textual feature only runs. 
Textual_1 use pure keyword without word expansion to retrieve similar documents. 
Textual_2 had the word expansion process. The result show that textual_2 integrate 
 30 
 
5. Conclusions 
The results of the evaluation show that the method we proposed is excellent. The best 
result by MAP is 0.242. In the experiment results we find that a content-based 
approach retrieving similar images relies on visual feature, which has less semantic 
expansion. The text-based approach has better performance than the content-based 
approach. Combining the textual and visual features will achieve the best results.  
The results in the medical retrieval task show that the weighted settings between the 
features are very important. The variation between different settings of weight is 
extreme. Suitable weight adjusting will improve the results. In this article several 
image features are examined for medical image data. The medical image application 
is unlike general-purpose images. Medical images have more stable camera settings 
than general purpose images. Therefore, the spatial information becomes an important 
factor in medical images, and we must improve the representation regarding spatial 
relations in these kinds of images. 
References 
1. Miller, G.: WordNet: A Lexical Database for English, Communications of the 
ACM (1995) 39-45 
2. Cheng, P. C., Chien, B. C., Ke, H. R., and Yang, W. P.:KIDS’s evaluation in 
medical image retrieval task at ImageCLEF 2004, Working Notes for the CLEF 
2004 Workshop September, Bath, UK (2004) 585-593 
3. Han, J., and Kamber, M.: Data Mining: Concepts and Techniques. Academic Press, 
San Diego, CA, USA (2001) 
4. Keysers, D., Macherey, W., Ney, H., and Dahmen, J.: Adaptation in Statistical 
Pattern Recognition using Tangent Vectors. IEEE transactions on Pattern Analysis 
and Machine Intelligence, 26 (2), February (2004) 269-274 
5. Swain, M.J. and Ballard, D. H.: Color Indexing, International Journal of Computer 
Vision, Vol. 7 (1991) 11-32 
6. Salton, G. and McGill, M. J.: Introduction to Modern Information Retrieval. New 
York: McGraw-Hill (1983) 
7. Manjunath, B. S., and Ma, W. Y.: Texture features for browsing and retrieval of 
large image data, IEEE Transactions on Pattern Analysis and Machine Intelligence, 
Vol. 18 (8), August (1996) 837-842 
8. Flickner, M., Sawhney, H., Niblack, W., Ashley, J., Huang, Q., Dom, B., Gorkani, 
M., Hafner, J., Lee, D., Petkovic, D., Steele, D., Yanker, P.: Query by Image and 
Video Content: The QBIC system, IEEE Computer 28 (9) (1995) 23-32 
9. Carson, C., Thomas, M., Belongie, S., Hellerstein, J. M., Malik, J., Blobworld: A 
system for region-based image indexing and retrieval, in: D. P. Huijsmans, A. W. 
M. Smeulders (Eds.), Third International Conference On Visual Information 
Systems (VISUAL' 99), no. 1614 in Lecture Notes in Computer Science, Springer, 
Verlag, Amsterdam, The Netherlands (1999) 509-516 
10. Belongie, S., Carson, C., Greenspan, H., Malik, J.: Color and texture based 
image segmentation using EM and its application to content-based image retrieval, 
 32 
 
 
回饋式醫學影像意涵查詢系統之研究(3/3) 
 
 
 
計畫類別： 個別型 
計畫編號： NSC 96-2221-E-259-017-MY3 
執行期間： 98 年08 月01 日至99 年07 月31 日 
執行單位： 國立東華大學資訊管理學系 
計畫主持人： 楊維邦 
  
 34 
 
medical researchers, CBMIR provides an advanced method to search, navigate and 
browse large medical image databases aiding them in diagnoses, research [15], 
teaching, and file systems management [16]. 
Automatic image annotation or image classification is an area of active research 
in the field of machine learning and pattern recognition. Retrieval systems have 
traditionally used manual image annotation for indexing and then later for retrieving 
their image collections. However, manual image annotation is an expensive and labor 
intensive procedure [13].  
An efficient step for image searching is image content clustering. The goal is to 
map the archive images into classes such that the set of classes provides essentially 
the same prediction, or information, about the image archive as the entire image-set 
collection. The generated classes provide a concise summarization and visualization 
of the image content. The clustering process may be a supervised process using 
human intervention, or an unsupervised process.  
Here, an automatic approach was proposed to categorize images based on a 
supervised learning technique. In supervised classification, a collection of training 
images (labeled images) are given, and the problem is to label a newly encountered, 
yet unlabeled image. Each instance in the training set contains category or class 
specific labels and several image feature descriptors in the form of a combined feature 
vector. 
In this paper, the support vector machine (SVM) is used to learn image feature 
characteristics. Based on the SVM model, several image features that consider, from 
the viewpoint of a human, the invariance in image rotation are employed in our 
system. The goal of SVM is to produce a model which will predict the target value or 
the category of images with the highest probability or confidence in the testing set 
which is given as a series of input feature vectors to the classification system. 
In the experiments, the support vector machine serves as a classifier to categorize 
the 400 test images into 36 classes. The experimental result shows that proposed 
image features are useful when applied to medical images. The rest of this paper is 
organized as follows: In Section 2, the employed image features are described. 
Section 3 illustrates the SVM model that is used to classify the training data. In 
Section 4, the experimental results are discussed. Section 5 illustrates the user 
feedback accumulation mechanism. Finally, Section 6 provides concluding remarks 
and future directions for medical image retrieval. 
 
2 Feature extractions for image representation 
This section describes the image features used for medical image representation. 
It is difficult to classify image categorization only with one type of image feature. In 
this paper, we represent images with different images features: including color 
histogram, Pyramid-spatial feature, Coherence Moment, Correlogram and Gabor 
Texture feature, Then we merge them together for modality classification. Next, we 
simply introduce the used features for medical image representation. 
 
Color histogram: histograms are widely used to capture the distribution information 
in an image. They are robust against small changes of camera viewpoints. Color 
histogram [Swain91] is a basic method and has good performance for representing 
image content. The color histogram method gathers statistics about the proportion of 
each color as the signature of an image. Let C be a set of colors, (c1, c2…cm)  C, that 
 36 
 
Coherence Moment: The coherence moment feature attempts to describe the features 
from the human’s viewpoint in order to reduce the semantic gap. 
We cluster an image into four classes by the K-means algorithm. Figure 5 is an 
example. Figure 5 (a) is the original image and Figure 5 (b) is four-level gray image. 
We almost can not visually find the difference between the two images. After 
clustering an image into four classes, we calculate the number of pixels (COH), mean 
value of gray value (COH) and standard variance of gray value (COH) in each class. 
For each class, we group connected pixels in eight directions as an object. If an object 
is bigger than 5% of the whole image, we denote it as a big object; otherwise it is a 
small object. We count how many big objects (COH) and small objects (COH) in 
each class, and use COH and COH as parts of image features. 
 
(a)                           (b) 
Figure 5: (a) original image with 256 levels; (b) new image after clustering with only 4 
levels 
 
We intend to detect the reciprocal among neighbor pixels and eliminate the effect 
of noise; we apply smoothing method to the original image. If two images are similar, 
they will also be similar after smoothing. If their spatial distributions are quite 
different, they may have different result after smoothing. After smoothing, we cluster 
an image into four classes and calculate the number of big objects (COH) and small 
objects (COH). Figure 6 is an example. Each pixel will be influenced by its 
neighboring pixels. Two close objects of the same class may be merged into one 
object. Then, we can analyze the variation between the two images before and after 
smoothing. The coherence moment of each class is a seven-feature vector, (COH, 
COH, COH, COH, COH, COH, COH). The coherence moment of an image is a 
28-feature vector that combines the coherence moments of the four classes.  
 38 
 
filters are a group of wavelets, with each wavelet capturing energy at a specific 
frequency and a specific direction. Expanding a signal using this basis provides a 
localized frequency description, therefore capturing local features/energy of the signal. 
Texture features can then be extracted from this group of energy distributions. The 
scale (frequency) and orientation tunable property of Gabor filter makes it especially 
useful for texture analysis.  
The Gabor wavelet transformation Wmn of Image I(x, y) derived from Gabor 
filters according to [18] is defined in Eq. (5) 
 
.),(),(),( 1111 
 dydxyyxxgyxIyxW mnmn  (5) 
The mean mn and standard deviation mn of the magnitude |Wmn| are used for the 
feature vector, as shown in Eq. (6). 
 
.)|),((|  ,|),(|),( 2   dxdyuyxWδanddxdyyxWyx mnmnmnmnmn  (6) 
This image feature is constructed by mn and mn of different scales and 
orientations. Our experiment uses four (S=4) as the scale and six (K=6) as the 
orientation to construct a 48 feature vectors f , as shown in Eq. (7). 
 
].,,......,,,,[ 353501010000  uuuf   (7) 
  
3. Classification method 
In this work, support vector machine (SVM) [11] is used to classify the 
un-annotated image. SVM is an effective classification method. Its basic idea is to 
map data into a high dimensional space and find a separating hyper plane with the 
maximal margin. Intuitively, given a set of points which belongs to either one of two 
classes, a linear SVM finds the hyper plane leaving the largest possible fraction of 
points of the same class on the same side, while maximizing the distance of either 
class from the hyper plane. 
Given a training set of instance-label pairs (xi, yi), i=1,…,k where xiR
n
 and 
y{1,-1}k, the support vector machine optimizes the solution of the following 
problem: 
ii
T
i
k
i
i
T
bw
bxwyCwwMin 

 

1))(( and )
2
1
(
1
,,
, 0i  (8) 
Training vectors xi are mapped into a higher dimensional space by the function . 
Then SVM finds a linear separating hyper plane with the maximal margin in this 
higher dimensional space. C>0 is the penalty parameter of the error term. Furthermore, 
K(xi,xj) )()( j
T
i xx   is called the kernel function. In this paper we use LIBSVM [12] 
to classify the training data with a radial basis function or a polynomial function as 
the kernel function. The radial basis function (RBF) and the polynomial function used 
is defined in Eq. (9) and Eq. (10), respectively, where γ, r, and d are kernel 
parameters. 
.0),| || |exp(),( 2   jiji xxxxK           (9) 
.0,)(),(   dj
T
iji rxxxxK             (10) 
 40 
 
We create a Image-Keyword Association Map IKAM (see Figure 7) to 
interactive with the user feedback. IKAM collects the feedback information by users’ 
query behavior to adjust the correlation between Image and keywords and raises the 
query precision of system. 
The correlations between Images and words in IKAM are learned from users, so 
after using the system for a while, the correlation information can improve the 
annotation accuracy of each image in previous training module. Here, jiwps ,  is the 
weight of Image i and keyword j through feedback. 





















nknkkk
nknkkk
nn
nn
k
k
wpswpswpswps
wpswpswpswps
wpswpswpswps
wpswpswpswps
p
p
p
p
,1,2,1,
,11,12,11,1
,21,22,21,2
,11,12,11,1
1
2
1
n 1-n21   w                 w          w           w          







 
Figure 7: Image-Keyword Association Map-IKAM 
When users use query by keyword, the system will send information through 
IKAM to assess which Images have high correlation with the keyword and the system 
will provide important information of calculating the correlation of keywords and 
images for querying image. When users enter a keyword set  ''2'1  , ... , , mwwwW   to 
query, the system will calculate the correlation of any image I in image database and 
these keywords W by formula (11). 
  


m
i
miwps
m
WISIM
1
,
1
,   (11) 
 WISIM ,  is the correlation of keyword W and image I; Through the 
calculation of (Eq.5), the similarities of keywords entered by users and all images can 
be acquired and the image with high similarity can be sent back to users through 
sorting. The images through the query process could be incorrect or not the one that 
users want, therefore user relevance feedback mechanism is proposed to improve the 
efficiency of this system. There are three parts of this feedback module.  
1. Modify the probability of the occurrence of keywords appeared in 
assigned images.  
2. Modify the weight of IKAM. 
3. Return new query results to users. 
In the process of modifying the probability of the occurrence of keywords 
appeared in assigned images is to collect all images of positive and negative examples 
that users feedback and adjust the probability of images and keywords. Adjust the 
probability of keywords of positive images. The way of adjusting is to increase the 
probability. All values of keywords appeared in images is between 0 to1, so after 
adding, they system will check if the value is over 1, if so, then the value will be set 1. 
 42 
 
Weights and Relevance Feedback,‖ In Scandinavian Conference on Image 
Analysis, Kangerlussuaq, Greenland, June 1999, pp. 143-149. 
[6] J. Z. Wang, J. Li, and G. Wiederhold ‖SIMPLIcity: Semantics-Sensitive Integrated 
Matching for Picture Libraries,‖ IEEE Transaction on Pattern Analysis and 
Machine Intelligence, vol.23(9), September 2001, pp. 947-963. 
[7] J., Han, M., Kamber, ―Data Mining: Concepts and Techniques,‖ Academic Press, 
San Diego, CA, USA 2001. 
[8] D. Keysers, W. Macherey, H. Ney, and J. Dahmen. ―Adaptation in Statistical 
Pattern Recognition using Tangent Vectors‖, IEEE transactions on Pattern 
Analysis and Machine Intelligence, Vol.26(2), February 2004, pp. 269-274. 
[9] M.J. Swain and D. H. Ballard, ―Color Indexing‖, International Journal of 
Computer Vision, Vol. 7, 1991, pp.11-32. 
[10] P. C. Cheng, B. C. Chien, H. R. Ke, and W. P. Yang, “KIDS’s evaluation in 
medical image retrieval task at ImageCLEF 2004”, Working Notes for the CLEF 
2004 Workshop September, Bath,UK 2004 , pp. 585-593. 
[11] Boser, B., I. Guyon, and V. Vapnik, ―A training algorithm for optimal margin 
classifiers,‖ In Proceedings of the Fifth Annual Workshop on Computational 
Learning Theory, 1992. 
[12] C. C. Chang, and C. J. Lin. ―LIBSVM: a library for support vector machines,‖ 
http://www.csie.ntu.edu.tw/~cjlin/libsvm, 2001 
[13] E. Chang, G. Kingshy, G. Sychay, W. Gang, ―CBSA: Content-based Soft 
Annotation for Multimodal Image Retrieval Using Bayes Point Machines,‖ IEEE 
Transactions on Circuitsand Systems for Video Technology, 13, 2003, pp. 26–38. 
[14] H. Muller, N. Michoux, D. Bandon, and A. Geissbuhler, ―A review of 
content-based image retrieval systems in medicine - clinical benefits and future 
directions,‖ International Journal of Medical Informatics, 2004, pp.1-23. 
[15] M. Aisen, L. S. Broderick, H. Winer-Muram, C.E. Brodley, A.C. Kak, C. 
Pavlopoulou, J. Dy, C.R. Shyu, and A. Marchiori. ―Automated storage and 
retrieval of thin-section CT images to assist diagnosis: System description and 
preliminary assessment,‖ Radiology, Vol. 228, 2003, pp. 265-270. 
[16] T.M. Lehmann, M.O. Güld, C. Thies, B. Fischer, K. Spitzer, D. Keysers, H. Ney, 
M. Kohnen, H. Schubert, B.B. Wein, ―Content-based image retrieval in medical 
applications,‖ Methods of Information in Medicine , 2004, pp. 354-361. 
[17] G. P. Robinson, H. D. Targare, J. S. Duncan, C. C. Ja_e, Medical image 
collection indexing: Shape-based retrieval using KD-trees, Computerized 
Medical Imaging and Graphics 20 (4), 1996, pp. 209-217. 
[18]W. Y. Ma, Y. Deng, and B. S. Manjunath, ―Tools for texture- and color-based 
search of images,‖ in: B. E. Rogowitz, T. N. Pappas (Eds.), Human Vision and 
Electronic Imaging II of SPIE Proceedings, Vol. 3016, San Jose, CA, pp. 496-507, 
1997. 
[19]S. Ardizzoni, I. Bartolini, and M. Patella, ―Windsurf: Region-based image 
Retrieva1 Using Wave1ets,‖ Proc. Int. Workshop on Similarity Search (IW0SS), 
pp.167-173, 1999. 
[20]A. Pentland, R. W. Picard, and S. Sclaro, “Photobook: Tools for content-based 
manipulation of image databases,” International Journal of Computer Vision, 
vol.18(3), pp. 233-254, 1996. 
 
96年度專題研究計畫研究成果彙整表 
計畫主持人：楊維邦 計畫編號：96-2221-E-259-017-MY3 
計畫名稱：回饋式醫學影像意涵查詢系統之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 3 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
