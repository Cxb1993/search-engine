 1
行政院國家科學委員會專題研究計畫期末成果報告 
電影內容分析、表述與索引系統建立藉由多模式資訊 
計畫編號：NSC96-2628-E-110-020-MY2 
執行期限：96 年 8 月 1 日至 98 年 7 月 31 日 
主持人：葉家宏      國立中山大學電機工程學系暨研究所 
計畫參與人員：  賴忠平、周伯胤、鄧誌忠、曾文郁 
                葉晉維、許正杰、林裕敦、吳嘉修 
      國立中山大學電機工程學系暨研究所 
范姜淑貞 
國立東華大學資訊工程學系暨研究所 
  紀明傑 
國立東華大學電機工程學系暨研究所 
 
中文摘要 
近年來多媒體與相關硬體技術的快速發
展，不論是網路傳輸、儲存空間、或是電腦
運算能力都大幅提升，使得數位多媒體的應
用日漸開展，電影是一項重要的娛樂產業，
電影相關產業擁有上兆億元以上的產值。由
於蘋果電腦的 ipod 和 ipod video 的推出徹底
顛覆人們的生活娛樂習慣，然而受限於移動
式與嵌入式裝置的特性，例如有限的電池容
量與計算量，如何擷取電影重要內容，提供
使用者快速有效瀏覽、搜尋與管理電影數位
內容變成一個重要的研究課題，電影資訊可
謂是包羅萬象，其中包含了字幕、視訊、音
訊。如何同時分析這麼多種型態的資料，並
對結果進行判讀是需要深入研究的議題。視
訊內容檢索分析是一套能夠有系統的組織原
視訊資料結構，使其能更有效地表達其語意
內容的方法。成功的視訊內容檢索分析能更
快速的索引、搜尋、瀏覽、編輯、摘要與略
讀多媒體資料。 
在本計畫中，我們將針對研究電影內容
所含視訊、音訊、語音、描述性資料。同時
研究電影的結構、導演常用拍攝手法與電影
情境分析，並透過其相關語音及配樂，來達
成對於電影內容的瞭解。首先我們將探討音
訊視訊低階特徵值的擷取方法與一些經常使
用的低階特徵值，同時發展有效的視訊與音
訊特徵值以建立視訊與音訊場景的分析，隨
後我們將提出多模式分析法來融合所有低階
特徵值的特性並建立一個更有效的模型來擷
取電影資料中的語意結構以符合電影節奏與
導演的手法和故事的情境。在本計畫中第一
年我們將發展五大演算法來協助電影內容的
分析: (1)相似場景合併演算法，(2)電影情境
轉換圖演算法，(3)音高與音色追蹤演算法，
(4)電影音訊場景切換偵測演算法，(5)以場景
切換為基礎之攝影機動作估計。在第二年的
計畫中我們將發展和電影索引系統有關的六
大演算法: (6)物件移動估計率演算法，(7)電
影視訊與音訊節奏擷取，(8)事件為基礎之電
影節奏曲線，(9)電影音樂音效情緒偵測，(10)
多型態特徵值融合分析演算法，最後建立高
階語意索引建立。 
  
關鍵詞：電影內容分析，多媒體內容檢索，
音訊視訊節奏擷取，音訊場景偵測，電影音
樂/音效情緒偵測，動作估計率，多模式資料
融合，語意索引，場景偵測，情境產生，事
件偵測，梅爾倒頻譜   
 
 
 3
and movie camera motion detection. In the au-
dio part, we construct movie from the perspec-
tive of audio structure and the developed tech-
niques are pitch and timber tracking and 
large-scale audio shot detection. In the follow-
ing, we will briefly introduce these techniques 
for the purpose of movie analysis.  
 
Proposed Methods 
1. Scene Generation (相似場景合併演算法) 
 
Most of movies are composed of several shots. 
These shots can be easily distinguished by the 
difference of two successive frames. In order to 
analyze movie semantic, we need to compose 
shots into scenes which can represent the major 
topic in the certain time duration. The scene 
generation could be observed from the follow-
ing three perspectives: 
 
(1) Visual similarity 
(2) Audio similarity 
(3) Time locality similarity 
 
 
 
Figure 1: Scene of Goal 
 
Because the same scene means that the events 
happen in the same occasion such as similar 
background, we can analyze the similarity of 
shots to decide whether these shots belong to 
the same scene or not. Therefore, a scene gen-
eration algorithm is proposed based on shot 
similarity analysis especially for movie struc-
ture. Our algorithm is composed of two steps:  
Key-frame extraction and Histogram-based 
key-frame similarity caparison. In the 
key-frame extraction, because frames in the 
same shot must be very similar so we can 
choose the representative key-frames from each 
shot. In our experiment, we choose the first, 
middle and the last frames of each shot as 
key-frames. Then, we calculate the key-frame 
similarity in a predefined time duration defined 
in Eq. 1, where S  is key-frame similarity in 
the time duration, T  is number of shots in this 
time duration and H  is key-frame histogram 
of brightness component. In our experience, we 
define the initial time duration as three shots. If 
S  is small than the predefined threshold, it 
implies shots in this time duration may belong 
to the same scene; then T  will increase one 
by one to detect whether the next shot belongs 
to this scene or not. If S  is larger than the 
predefined threshold T  will decrease one by 
one, because shots may not be similar if they 
locate in the long time duration.  
         .
,
.
,
i j
T T
i j i
S H H
S thd T increase one
S thd T decrease one
≠
= −
≤⎧⎨ >⎩
∑∑         (1) 
2. Movie Scene Transition Graph (電影情境
轉換圖演算法) 
 
Scene analysis plays an important part in movie 
analysis because scenes have their semantic 
concepts in movie. For example, dialogue 
scene is an important part in movie construc-
tion. Thus, an algorithm to analyze scenes is 
very important. Therefore, an algorithm is pro-
posed to detect dialogue scenes called movie 
scene transition graph.  
 
A time constrain clustering algorithm is pro-
posed to deal with this problem. Here, the 
K-means algorithm is used to classify these 
shots along time duration. We predefine time 
duration of k shots and overlap 1/3k shots. We 
classify shots at first time duration and then 
second time duration is overlap 1/3 of the first 
time duration. We set the same clustering num-
ber if the non-overlapped shot in the second 
time duration is classified into the same cluster 
with overlapped shot and so on shown in Fig-
ure 2. Thus, we can generate scene graph 
shown in Figure 3. 
 
  
Figure 2: Time constrain clustering 
 
Figure 3: Scene transition graph 
Because dialogue scenes must be constructed 
by people speaking shots occurred at similar 
background. Therefore, we use histogram dif-
 5
5. Shot-based Camera Motion Detection (以
場景切換為基礎之攝影機動作估計) 
 
Except contents, camera motion is always em-
ployed by the conductors to catch users’ atten-
tion, emphasizing or neglecting some video 
content. Therefore, camera motion is a very 
important cue to construct movies. Many tradi-
tional methods have been proposed to detect 
camera motion; however, most of them use 
motion vector information of each frame to dis-
tinguish camera motion status. However, mov-
ies are well-structured videos that are com-
posed of the smallest units of shots. Therefore, 
it is more reasonable to detect camera motion 
from the perspective of shots especially movie 
content. Generally speaking, camera motion 
can be regarded as partial shot change hap-
pened in the frame boundary. Luminance his-
togram difference is commonly used to detect 
shot change. Thus, we can easily fetch bound-
ary histogram difference information while shot 
change detection without additional computa-
tions. We fetch top, bottom, right, left bounda-
ries as our camera motion detection basis. Then, 
we apply shot change detection method in these 
four boundaries. Therefore, we can detect cam-
era motion type according to Eq. 5. 
 
                                     
(5) 
where SC  represent whether the shot change 
occurs or not; 1 and 0 represent the occurrence 
or non-occurrence of shot change. Figure 7 
shows the detected results of zoom, tilt and 
pan.        
Shot1          Shot2 
 
 
 
 
Zoom 
Shot1          Shot2 
 
 
(1) Tilt 
 
Tilt  
Shot1          Shot2 
 
 
 
 
Pan 
Figure 7: Camera motion 
6. Moving Object Motion Activity Algorithm 
(物件移動估計率演算法)  
 
Movies, for human beings, can be watched and 
experienced in the forms of slow sequence, 
fast-paced sequence, action sequence, and so 
on. Motion activity reflects the temporal corre-
lations between consecutive frames. Camera 
motion or object motion produces high motion 
activity of higher tempo, and vice versa. In 
video, each frame can be divided into many 
fixed-size (16×16) blocks and the motion vec-
tor of each macroblock can be calculated. Fig-
ure 8 shows two ways of displaying motion 
vector. In Figure 8(a), the dark block and light 
block represent moving and non-moving blocks, 
respectively. In Figure 8(b), the white light il-
lustrates the direction and magnitude of motion 
vector.   
 
(a) 
 
(b) 
Figure 8: Illustration of motion vector 
The motion activity can be obtained by aver-
aging the all motion vectors in each frame. Mo-
tion activity is very useful feature for motion 
content analysis. Figure 9 shows the motion ac-
tivity of a video sequence that consists of 70 
frames. However, this motion activity contains 
camera motion and object motion. In order to 
describe object motion activity, we have to re-
move the effect of camera motion.  
 
Figure 9: Motion activity 
1& 1,
1& 1,
1& 1& 1& 1,
top bottom
right left
top bottom right left
SC SC Tilt
SC SC Pan
SC SC SC SC Zoom
⎧ = =⎪ = =⎨⎪ = = = =⎩
 7
45 50 55 60 65 70 75 80 85 90 95
0
10
20
30
40
50
60
70
80
90
100
Minutes
S
to
ry
 in
te
ns
ity
 (
%
)
Substory Substory Substory
ResolutionExposition Conflict Climax
 
Figure 13: Tempo curves of the last part of the 
film Kung Fu Hustle 
 
Figure 13 shows the tempo curves of the last 
part of the film Kung Fu Hustle. The process of 
obtaining ( )P t  is important to our algorithm. 
Interesting shots detected through the low-level 
feature extraction provide fragmented shot in-
formation. Through tempo curves, however, the 
extracted segments are more acceptable and 
complete, matching human perception. Here, 
four low-level features are used to form a curve 
and we select these features because they are 
high relevant to tempo, which is explained in 
the following.  
 
Energy is used to measure the variations of 
waveform amplitude over a period of time, and 
it indicates the volume people hear within a 
certain time interval. As in typical Hollywood 
movies, audio energy of larger volumes reveals 
higher tempo (content density). 
Zero-crossing rate Zero-crossing rate is the 
rate of sign-changes within audio waveform. In 
general, it can be used to measure the propor-
tion between non-noise (e.g. music and speech) 
and noise signals within a certain time interval. 
Motion activity Movies, for human beings, can 
be watched and experienced in the forms of 
slow sequence, fast-paced sequence, action se-
quence, and so on. Motion activity reflects the 
temporal correlations between consecutive 
frames. Camera motion or object motion pro-
duces high motion activity of higher tempo, 
and vice versa. 
Shot stability Shot stability selects those shots 
whose luminance differences are low. They are 
shots that are usually long in length, and re-
quire longer time to be presented by the direc-
tor and understood by the audience. 
 
 
8. Event-Based Movie Tempo Analysis (事件
為基礎之電影節奏曲線) 
 
In 7, several interesting shots are extracted 
from a tempo curve to describe a movie and 
interesting shot selection are based on four 
low-level features, including energy, 
zero-crossing rate, motion activity and shot 
stability. If we identify the scene in each shot, 
we can obtain an event-based tempo curve. The 
following are detected dialogue scene in movie. 
 
 
 
 
Figure 14: Dialogue scene in movies 
 
9 Movie Music Emotion Detection (電影音樂
音效情緒偵測) 
 
An important issue regarding emotion detection 
is the way to classify emotions of music. Tradi-
tional approaches usually use adjectives to 
classify emotions. For example, the Hevner’s 
model [43] uses eight adjectives including Dig-
nified, Sad, Dreaming, Soothing, Graceful, Joy-
ous, Exciting and Vigorous. However, there are 
infinite adjectives of emotions. 
 
 
Figure 15: Thayer’s 2-D emotion model 
 
Hence, in our proposed algorithm, we adopt 
Thayer’s 2-D emotion model. Thayer uses a 
two dimensional arousal-valence emotion 
 9
where ( )vS τ  is the number of interesting 
shots selected by visual features, such as mo-
tion activity and shot stability, around the in-
terval time τ . Yet, the way to select fusion 
model. automatically for various genres of 
movies is still an open problem. Movies can be 
roughly classified into genres such as drama, 
comedy, action, horror, animation, etc. To 
solve this problem, we obtain some information 
from the preprocessing step as well as the 
low-level feature extraction step to select the 
best fusion model for the input movie. For ex-
ample, color analysis is used for a first general 
classification of movies genres. Furthermore, 
EPG (Electronic Program Guide) can provide 
metadata information regarding the genres of 
recorded movies. 
 
Experimental Results  
 
In the experiments, four popular films (i.e. 
Kung Fu Hustle, Banlieue 13, Goal and 
Madagascar) illustrated in Figure 18 are used 
and the length of Kung Fu Hustle, Banlieue 13, 
Goal and Madagascar are 93, 81, 114, and 82 
min, respectively. In addition, all three episodes 
of the Spiderman film series are used to evalu-
ate the performance of the proposed algorithm 
shown in Figure 19. The lengths of these mov-
ies are 122, 128 and 133 min. These movies are 
recorded and encoded in MPEG-2. The frame 
size, frame rate and audio track are 352×240 
pixels, 30 fps, and 44.1 kHz, respectively. In 
this section, we analyze and evaluate the tem-
pos of these films. 
 
 
 
 
 
 
 
            (a)          (b) 
  
 
 
 
 
 
           
(c)          (d) 
Figure 18: Four test movie videos: (a) Kung Fu 
Hustle, (b) Banlieue 13, (c) Goal and (d) 
Madagascar 
   
     (a)           (b)           (c) 
Figure 19: Spiderman film series: (a) Episode 1, 
(b) Episode 2 and (c) Episode 3 
 
Figure 20 illustrates the tempo curves of these 
four movies obtained through the complex fu-
sion model. In these figures, x-axis and y-axis 
represent the time as well as the story intensity 
of the movies, respectively. From these tempo 
curves, movies can be analyzed easily for 
movie abstraction. In these figures, several 
story sections with high story intensity in a 
tempo curve exist in these figures. In any given 
film genre, interesting shots can be effectively 
detected through visual or audio cues such as 
high-motion activities and percussive sounds. 
Visual and audio cues are observed manually in 
each of the film genres: in action movies, cli-
max scenes consists usually of trick shots, fight 
shots, car crashing shots—shots that attract the 
attention through the special visual and sound 
effects; in drama films, climatic story sections 
are made up of dialogue and monologue scenes 
that consist of long stable shots; in horror 
movies, highlights of the film are scenes with 
special sound effects, such as interrupted 
screams, eerie wind, and demonic laughter; and 
in comedies and animations, sound effect and 
motion activity also play important roles in se-
lecting interesting shots. In general, most film 
genres can be efficiently described by tempo 
curves, since tempo curves display a story’s in-
tensity and a director’s film grammar. To fur-
ther demonstrate our method, all three episodes 
of the Spiderman film series are used to evalu-
ate the performance of the proposed algorithm. 
Figure 21 illustrates the tempo curves of the 
Spiderman film series through the complex fu-
sion model. Several sections with high story 
intensity and their corresponding themes are 
demonstrated in these figures. These three Spi-
derman episodes all seem to demonstrate simi-
lar movie structure (i.e. the arrangement of ex-
position, conflict, climax and resolution). 
 11
are (1)Scene generation, (2)Movie scene transi-
tion graph (3)Pitch and timber tracking 
(4)Large-scale audio shot detection 
(5)Shot-based camera motion detection. In the 
second year, six algorithms related to the in-
dexing system organization are developed that 
are (6)Moving object motion activity algorithm, 
(7)Movie audiovisual tempo extraction, 
(8)Event-based movie tempo analysis, 
(9)Movie music emotion detection 
(10)Multimodal fusion analysis.. In particular, 
this project emphasizes tempo feature for the 
enhancement of collaboration among semantic 
cues from various kinds of information sources. 
These semantics structures will help us achieve 
movie content analysis and its indexing system 
establishment. 
 
References 
 
[1] Y. Zhai, Z. Rasheed & M Shah, “Semantic 
classification of movie scenes using finite state 
machines,” in Proceedings on Vision, Image 
and Signal Processing, vol. 152, pp. 896 – 901, 
2005.  
[2] H. A. Rowley, S. Baluja & T. Kanade,  
“Neural network-based face detection,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 20, pp. 23-38, 1998. 
[3] B. S. Manjunath, T. Huang, A. M. Teklap & 
H. J. Zhang, “Guest editorial introduction to the 
special issue on image and video processing for 
digital libraries,” IEEE Transactions on Image 
Processing, vol. 9, pp. 1–2, 2000. 
[4] S. F. Chang, A. Puri, T. Sikora & H. J. 
Zhang, “Introduce to the special issue on 
MPEG-7,” IEEE Transactions on Circuits and 
Systems for Video Technology, vol. 11, pp. 
685–687, 2001. 
[5] B. S. Manjunath, P. Salembier & Thomas 
Sikora, “Introduction to MPEG-7,” John Wiley 
& Sons, LTD, England, 2002. 
[6] A. Hampapur, R. Jain & T. Weymouth, 
“Digital video segmentation,” ACM Multimedia, 
pp. 357–364, 1994. 
[7] H. J. Zhang, J. Wu, D. Zhong & S. W. 
Smoliar, “An integrated system for con-
tent-based video retrieval and browsing,” Pat-
tern Recognition, vol. 30, no. 4, pp. 62–72, 
1994. 
[8] S. F. Chang, J. R. Smith, M. Beigi & A. 
Benitez, “Visual information retrieval from 
large distributed on-line repositories,” in Pro-
ceedings of ACM Communication, vol. 40, no. 
12, pp. 63–71, 1997. 
[9] Y. Rui, T. S. Huang & S. Mehrotra, “Con-
structing table-of-content for video,” ACM 
Journal of Multimedia Systems, vol. 7, pp. 
359–368, 1998. 
[10] N. Sebe, M. S. Lew, X. Zhou & T. S. 
Huang, “The state of the art in image and video 
retrieval,” in Proceedings of International Con-
ference on Image and Video Retrieval, pp. 1–8, 
2003. 
[11] R. Lienhart, S. Pfei.er & W. E.elsberg, 
“Video abstracting,” Communications of the 
ACM, pp. 55–62, 1997. 
[12] A. Hanjalic & H. J. Zhang, “An integrated 
scheme for automated video abstraction based 
on unsupervised cluster-validity analysis,” 
IEEE Transactions on Circuits and Systems for 
Video Technology, vol. 9, no. 8, 1999. 
[13] Y. Li, T. Zhang & D. Tretter, “An over-
view of video abstraction techniques,” HP 
Laboratories Technical Report, HPL-2001-191, 
2001. 
[14] D. Dementhon, V. Kobla & D. Doermann, 
“Video summarization by curve simplifica-
tion,” in Proceedings of ACM Multimedia, pp. 
211–218, 1998. 
[15] X. D. Sun & M. S. Kankanhalli, “Video 
summarization using sequences,” Real-time 
Imaging, pp. 449–459, 2000. 
[16] C. M. Taskiran, A. Amir, D. Ponceleon & 
E. J. Delp, “Automated video summarization 
using speech transcripts,” Proceedings of SPIE, 
pp. 371–382, 2002. 
[17] M. Smith & T. Kanade, “Video skimming 
for quick browsing based on audio and image 
characterization,” Technical Report 
CMU-CS-95-186, pp. 1–12, 1995. 
[18] M. Smith & T. Kanade, “Video skimming 
and characterization through the combination 
of image and language understanding tech-
niques,” in Proceedings of the IEEE Computer 
Vision and Pattern Recognition, pp. 775–781, 
1997. 
[19] Q. Wei, H. J. Zhang & Y. Zhong, “A ro-
bust approach to video segmentation using 
compressed data,” in Proceedings of SPIE, vol. 
3022, pp. 448–456, 1997. 
[20] H. Jiang, T. Lin & H. J. Zhang, “Video 
segmentation with the assistance of audio con-
tent analysis,” in Proceedings of IEEE Interna-
tional Conference on Multimedia and Expo, vol. 
3, pp. 1507–1510, 2000. 
[21] H. Sundaram & S. F. Chang, “Video scene 
segmentation using video and audio features,”  
in Proceedings of IEEE Conference on Multi-
media and Expo, vol. 2, pp. 1145-1148, 2000. 
 13
附件一：計畫相關資訊 
附件二: 可供推廣之研發成果資料表 
附件三: 本計畫相關研究成果 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 15
可供推廣之研發成果資料表 
■ 可申請專利  □ 可技術移轉                                      日期：98 年 10 月 31 日 
國科會補助計畫 
計畫名稱：電影內容分析、表述與索引系統建立藉由多模式資訊 
計畫主持人：葉家宏         
計畫編號：NSC96-2628-E-110-020-MY2            
學門領域：資訊學門 
技術/創作名稱 電影節奏之分析方法 
發明人/創作人 葉家宏 
中文： 
電影中存在著因劇情起承轉合而配合發展出的故事結構，不同結構
各有其故事強度。電影故事結構跟視訊與音訊結構相關；人類對於
音樂或電影有感知其節奏快慢的能力，要形容一部電影的故事情節
快慢，節奏特性有絕對的重要性。我們分析每個分鏡的視訊與音訊
特性，進而擷取出整體電影的故事強度，並據此發展成電影節奏圖
以描述一部電影的故事內容。 
技術說明 
英文： 
Most movie contents are well structured and has the structures includes 
exposition, conflict, climax and resolution. The story structure corre-
sponds to its audiovisual structure, and story intensity map is correlated 
to each other due to the principle of contrast and affinity. Tempo is a 
musical terminology that refers to the speed or pace of a piece of mu-
sic. An extremely crucial element of music/sound, tempo describes the 
complexity of the sound/music and affects the audiences’ mood. Visual 
and audio cues are fused to determine interesting shots of a given video 
and screen out irrelevant shots so as to form a tempo curve that is itself 
a distribution of the interesting shots. Various tempo curves are ex-
tracted from a video through multimodal fusion techniques according 
to different needs. 
可利用之產業 
及 
可開發之產品 
本計畫所發展之技術，可提供多媒體相關產業-例如視訊音訊分
析、監控系統或是數位內容服務提供業者等-加以應用。可開發之
產品為個人數位錄放影機或是電影搜尋引擎。 
技術特點 
本計畫為跨領域性(multi-disciplinary)計畫，其研究領域包含視訊、
音訊分析與訊號處理等，跨越了多媒體訊號處理的各個層面，更結
合了理論和實作，故具備相當高的複雜度和專業要求。我們依主題
進行關鍵技術應用系統的開發，最後再整合成電影索引系統。 
推廣及運用的價值 數位內容分析檢索勢將成為未來視訊服務的一項重要工作所研發出的創新技術可供相關產業參考。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
附件二 
Multimed Tools Appl (2009) 44:205–228
DOI 10.1007/s11042-009-0278-8
Movie story intensity representation through
audiovisual tempo analysis
Chia-Hung Yeh · Chih-Hung Kuo · Rung-Wen Liou
Published online: 12 May 2009
© Springer Science + Business Media, LLC 2009
Abstract A comprehensive method for movie abstraction is developed in this re-
search for applications in fast movie content exploring, indexing, browsing, and
skimming, Most current approaches rely heavily on specific domain knowledge or
models to identify and extract the determining scenes of a given movie; however,
the segments extracted are often isolated, presenting a fragmented outline of the
original. Our proposed method fuses simple audiovisual features, and measures the
“tempos” of a movie directly, especially that of long-term ones. These tempos form
a curve that catches the high-level semantics of a movie, indicating the events of in-
terests named as “story intensity.” Through tempo, the proposed algorithm provides
a natural way that segments a movie into manageable parts. As our experimental re-
sults demonstrate, the condensed skimming clips efficiently extract semantic content
that contains the most interesting and informative parts of the original movie.
Keywords Movie analysis · Movie abstraction · Video abstraction · Tempo analysis ·
Skimming · Summarization · Indexing
1 Introduction
With the rapid growth of multimedia information, video content analysis has become
crucial when dealing with large amounts of multimedia data [11, 14, 29]. Suitable and
C.-H. Yeh (B)
Department of Electrical Engineering, National Sun Yat-Sen University,
No. 70, Lien-hai Road, Kushan District, Kaohsiung 80424, Taiwan (R.O.C.)
e-mail: yeh@mail.ee.nsysu.edu.tw
C.-H. Kuo · R.-W. Liou
Department of Electrical Engineering, National Cheng Kung University,
No. 1, University Road, Tainan 701, Taiwan (R.O.C.)
C.-H. Kuo
e-mail: chkuo@ee.ncku.edu.tw
Multimed Tools Appl (2009) 44:205–228 207
all shots, selecting interesting shots. Then, the distribution of these interesting shots
forms tempo curves that catch the high-level semantics of the movie called “story
intensity.” Multimodal data fusion techniques [13] will be employed to fuse a tempo
curve that contains other information, including high-level tempo curves, middle-
level detectors, or low-level features, for each of the different scenarios. Our method
represents movie content in more manageable ways. Experimental results are given
to demonstrate the feasibility and efficiency of the proposed technique for movie
abstraction.
The rest of this paper is organized as follows: relevant works on story intensity,
and the cinematic structure are reviewed in Section 2. Section 3 shows the ways
to obtain the audiovisual tempo information of movie content, and the method to
fuse tempo curves with different-levels of information for various kinds of scenarios
through multimodal data fusion techniques. Experimental results are demonstrated
in Section 4. Finally, concluding remarks and future research directions are given in
Section 5.
2 Background
Several video abstraction techniques have been proposed recently, and they can
be categorized largely into two schools. To detect meaningful scenes and text
occurrence, one group employs low-level audiovisual features and text information.
Smith and Kanade [22, 23] presented an idea for skimming based on audio and
image characterization. Special information such as audio keyword, moving objects
or relevant video structure is extracted, and a skim is created through selecting
video frames that include text, human face, or camera motion. Toklu and Liou [27]
proposed a video abstract scheme, and their work depends heavily on the analysis
of text information. Pfreiffer et al. [19] proposed a VAbstrat system to select video
clips from movies by detecting special events such as dialogs and explosion, The
events detected through VAbstrat system are combined with important shots based
on action criterion. In the research of Sundaram and Chang [25], a skimming scheme
that measures the visual complexity of a shot called Kolmogorov complexity is
developed. The complexity of a shot indicates the minimum time required for human
understanding.
The second group grounds its fundamental principles on predefined models of
special domain knowledge, and tools such as probabilistic network and Hidden
Markov Models (HMM) are leveraged to extract semantic entities. Turning to
Bayesian networks to model content and context, Jasinschi et al. [10] drew on these
content models that consist of different information to define the levels of abstraction
and used these context models to describe the underlying structure information of
multimedia content. Naphade et al. [17] proposed a graph framework based on
HMM for semantics video indexing, where probabilistic detectors called Multiject
are used to model semantic concepts. Identifying what triggered the excitement of
the audience, Hanjalic [7, 8] used motion activity, density of cuts, and audio energy,
and developed a function for detecting exciting scenes. An approach based on the
ontology or taxonomy of a sports event is also popular among the second group.
Zhou et al. [34], for instance, proposed an ontology/taxonomy structure to retrieve
the semantic concept of sports videos.
Multimed Tools Appl (2009) 44:205–228 209
applications, including indexing, browsing, retrieval, and skimming for movie content
management. The next section provides a discussion of our proposed scheme.
3 Proposed scheme
As illustrated in Fig. 2, our framework consists of five major components: prepro-
cessing, low-level feature extraction, audiovisual tempo analysis, multimodal data
fusion, and skim authoring scheme. First, adaptive shot change detection algorithm
is used to decompose a movie in the preprocessing stage. Second, a group of simple
audiovisual features is extracted to measure the visual and audio tempo of shots
in a movie. Then, the shots of interests are detected to form different kinds of
tempo curves. This map can be adopted robustly in many applications with minimal
effort. Moreover, multimodal data fusion stage is used to fuse various kinds of
information so as to represent the story intensity more efficiently, catching the high-
level semantics of a movie. Finally, the skim authoring scheme is proposed for skims
generation.
3.1 Preprocessing
Many research regarding shot change detection have been conducted [4, 9, 30, 33].
In the past decade, the differences of color histograms between two consecutive
frames were widely used in shot detection methods [33]. Here, Eq. 1 is used to detect
hard cut shot changes for movies so as to divide a movie into basic individual units.
Hard cut indicates the sharp transition between two shots without visual effects,
and that most frequently appears in movies. Here we first aim to extract long stable
shots that faithfully represent movies [21]. We therefore regard transitions of special
editing techniques, such as the dissolve, wipe, fade in/out, and computer-generated
Fig. 2 The architecture of the proposed movie abstraction system
Multimed Tools Appl (2009) 44:205–228 211
audio energy and zero-crossing rate to measure it. These four basic features related to
tempo concept are employed to measure visual tempo and audio tempo of shots in a
video. Selected features, in general, should correspond to specific domain knowledge.
These four features are employed to select interesting shots for the generation of the
tempo curve. Detailed explanations of the ways these features are related to tempo
are explained as what follows:
Energy Energy is used to measure the variations of waveform amplitude over a
period of time, and it indicates the volume people hear within a certain time interval.
As in typical Hollywood movies, audio energy of larger volumes reveals higher
tempo (content density).
Zero-crossing rate Zero-crossing rate is the rate of sign-changes within audio
waveform. In general, it can be used to measure the proportion between non-noise
(e.g. music and speech) and noise signals within a certain time interval.
Motion activity Movies, for human beings, can be watched and experienced in the
forms of slow sequence, fast-paced sequence, action sequence, and so on. Motion
activity reflects the temporal correlations between consecutive frames. Camera
motion or object motion produces high motion activity of higher tempo, and vice
versa.
Shot stability Shot stability selects those shots whose luminance differences are low.
They are shots that are usually long in length, and require longer time to be presented
by the director and understood by the audience [21].
Automatic threshold setting is an important issue in video content analysis. Here,
we determine the adaptive threshold values of interesting shot selection through
shots obtained from the preprocessing step, avoiding threshold setting problems
that often appear in various genres of movies. Motion activity and shot stability are
greatly affected by camera motion, and, therefore, making the process to determine
the appropriate thresholds for motion activity and shot stability for interesting shot
selection difficult. Here, we consider these two features together in order to increase
the accuracy of interesting shot detection. We first average the histogram difference
of all shots obtained by the preprocessing step to generate threshold value Tlum
shown in Eq. 7,
Tlum =
∑Ns
n=1 Ln
Ns
, (7)
where Ln and Ns are the luminance histogram difference averaged in the shot n
and the total number of shots, respectively. Then, we average only the motion
vector average corresponding to the shots, whose histogram difference average is not
greater than Tlum so as to generate the second threshold value Tmv shown in Eq. 8,
Tmv = 1Nk
∑
n:Ln<Tlum,n<Ns
Mn, (8)
where Nk is the number of the shots whose L value is smaller than Tlum. Then, we
determine Tmv as the visual threshold value in order to select interesting shots. Audio
analysis determines a first audio threshold value TE by averaging audio energy values
Multimed Tools Appl (2009) 44:205–228 213
Fig. 3 Visual tempo, audio
tempo, and synthesized tempo
curves for the last half of the
film, Kung Fu Hustle
50 55 60 65 70 75 80 85 90
0
10
20
30
40
50
60
70
80
90
100
Minutes
St
or
y 
In
te
ns
ity
 (%
)
Visual tempo
Audio tempo
Synthesized tempo
3.4 Multimodal data fusion
Figure 3 shows the audio tempo, visual tempo, and synthesized audiovisual tempo
curves of the last half of the film Kung Fu Hustle. From 62 to 75 min, the volume of
audio tempo curves is similar to that of the visual tempo curve. The volume of audio
tempo curve, however, is much stronger than that of visual tempo from 75 to 85 min.
The simplest way to fuse visual and audio tempo information is to synthesize the two
tempo curves with equal weight. The fusion model can be represented simply as
Ps(t) = αPv(t) + (1 − α)Pa(t), (12)
where Pv(t) and Pa(t) represent visual and audio tempo curves, respectively, and α is
the weighting factor, which equals to 0.5 in this case. Alternatively, audio energy Fae,
shot frequency Fsf , motion activity Fma—results of the low-level feature extraction
stage shown in Fig. 4—also help synthesize tempo curves. For example, we can
redefine Eq. 12 as follows
P′s(t) = β(t)Pa(t), (13)
β(t) = Fae(t)
max{Fae} , (14)
where β is the weighting factor determined by audio energy information Fae. Here,
Fae can be replaced by shot frequency Fsf or motion activity Fma according to
different needs.
Tempo curve fusion, of course, can be achieved, for instance, through more
complex processes of visual-based constraints. We first process visual tempo analysis
Multimed Tools Appl (2009) 44:205–228 215
a complete story section from tempo curves. Yet, the way to select fusion models
automatically for various genres of movies is still an open problem. Movies can be
roughly classified into genres such as drama, comedy, action, horror, animation, etc.
To solve this problem, we obtain some information from the preprocessing step as
well as the low-level feature extraction step to select the best fusion model for the
input movie. For example, color analysis is used for a first general classification of
movies genres [3, 15]. Furthermore, EPG (Electronic Program Guide) can provide
metadata information regarding the genres of recorded movies.
3.5 Skim authoring
In this section, a scheme is proposed to create a skim through tempo curves.
In general, a skim should meet the following criteria: a skim should capture the
significant parts of a movie and preserve the continuous flow of the entire movie,
providing a smooth synoptic representation of the film within strict time constraint.
As discussed previously, the storyline of a typical movie is usually structured. By
locating the local extremes of the tempo curve of a movie, story section boundaries
can be identified.
For skim generation, the most important criterion is to obtain the most determin-
ing parts of the original movie. Story sections obtained from the tempo curve of a
movie are suitable for this purpose. The proposed skim authoring scheme contains
the following three steps:
1. Extract independent story sections from a tempo curve through story boundary
detection. To characterize the story section boundaries of a tempo curve, we
define the gradient ∇P of a tempo curve as
∇P(t) = dP(t)
dt
. (16)
We first pick the candidate local maxima peak P(ω) with the following criteria,
∇P(ω) > ∇P(ω − 1)||∇P(ω) ≤ ∇P(ω + 1). (17)
As in Fig. 6, let ωmin be the time index at the local minima, and let P(ω) and
P(ω + 1) be the left and right peak points determined by ∇P of the tempo curve
P(t) around ωmin, respectively. A search window with size ε is centered at
ω+(ω+1)
2 .
If the search window contains peak points, the size of ε will be narrowed down.
Please note that the default setting of ε is 3 mins. The local minima will be the
median of these time indices when multiple time indices are obtained from the
search window ε. After obtaining the local minima as the story section boundary,
we compare the story intensity value of the local minima point with the story
intensity value of its two neighboring peak points. If one of the differences is
smaller than the threshold TSI , no story boundary section is detected, and the
local minima point is discarded. In Fig. 6, the solid vertical lines indicate potential
story section boundary.
Multimed Tools Appl (2009) 44:205–228 217
Fig. 7 Four test movie videos: a Kung Fu Hustle, b Banlieue 13, c Goal, and d Madagascar
4.1 Tempo curve extraction
Figures 9–12 illustrate the tempo curves of these four movies obtained through the
complex fusion model shown in Eq. 15. In these figures, x-axis and y-axis represent
the time as well as the story intensity of the movies, respectively. From these tempo
curves, movies can be analyzed easily for movie abstraction. In these figures, several
story sections with high story intensity in a tempo curve exist in these figures. In
any given film genre, interesting shots can be effectively detected through visual or
audio cues such as high-motion activities and percussive sounds. Visual and audio
cues are observed manually in each of the film genres: in action movies, climax scenes
consists usually of trick shots, fight shots, car crashing shots—shots that attract the
attention through the special visual and sound effects; in drama films, climatic story
sections are made up of dialogue and monologue scenes that consist of long stable
shots; in horror movies, highlights of the film are scenes with special sound effects,
such as interrupted screams, eerie wind, and demonic laughter; and in comedies and
animations, sound effect and motion activity also play important roles in selecting
interesting shots. In general, most film genres can be efficiently described by tempo
curves, since tempo curves display a story’s intensity and a director’s film grammar.
To further demonstrate our method, all three episodes of the Spiderman film series
are used to evaluate the performance of the proposed algorithm. Figures 13–15
illustrate the tempo curves of the Spiderman film series through the complex fusion
Fig. 8 Spiderman film series:
a Episode 1, b Episode 2, and
c Episode 3
Multimed Tools Appl (2009) 44:205–228 219
Fig. 11 Story structure of the
film, Goal
0 20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
100
Minutes
St
or
y 
In
te
ns
ity
 (%
)
Goal
Recall
scene
Soccer field
scene
Soccer field scene
training/conflict
Soccer field scene
game in progress
/romance
Soccer field scene
game in progress
Soccer field scene
Winning
shot boundaries for each test videos is determined by humans directly. Table 1 shows
the results of hard cut shot change detection and the ground truth of the four test
movies. From Table 1, we can see that hard cut detection is quite robust and accurate
in most cases. With low computational complexity, this method achieves a correct
detection rate of 90%. Although some shots are not detected in our experiments, the
luminance differences between missed shots and detected shot are very minor given
that detected and missed shots are often about similar context. The impact of these
missed shots on long-term tempo extraction is insubstantial.
For story section boundary detection, our method focuses on long-term story
sections that usually cover a complete thematic topic from 5 to 15 min in length.
Fig. 12 Story structure of the
film, Madagascar
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
90
100
Minutes
St
or
y 
In
te
ns
ity
 (%
)
Madagascar
Funny show scene
in the zoo
Escape/pursue scene
On ship scene
fall into sea
Dangerous situation scene
in the wild
Multimed Tools Appl (2009) 44:205–228 221
Fig. 15 Story structure of
the Spiderman film series:
Episode 3
0 20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
100
Minutes
St
ro
y 
In
te
ns
ity
(%
)
Spiderman 3
Recall    
Animation 
Fight   
scene   
(Alley) 
Atomic experiment
scene            
(Sandman         
shows up)        
Rescue 
scene  Fight          
scene          
(with Sandman) 
Fight scene
(Subway)   
Fight scene
(New green 
goblin)    
Fight scene
(Bar)      
Fight scene           
(Final fight          
with Venom & Sandman) 
story sections are not detected by our method in movies such as Kung-Fu Hustle and
Banlieue 13, given that, in action movies, the story intensity of violent scenes, such
as fights and gunfires, are higher than that of others. In our skim generation method,
however, transitional shots are still capable of being selected from missing regions,
making the transition of the skim semantically smoother. Similar results can be found
in Fig. 17 and Table 3 for Spiderman film series.
Since it is tough to objectively evaluate the comprehensiveness of the generated
skims, subjective tests are employed to evaluate the performance of the proposed
skim authoring algorithm. 20 people were invited as subjects to watch the skims, with
skim ratio ranging from 0.1 to 0.5, and 0.1 as interval. The subjects are requested
to address the synoptic meanings of the generated skims, the skims’ degrees of
faithfulness to the original video, based on a 0 ∼ 100 scale, after watching the original
movies and fully understand the implications and significance of the four movies
without fast forwarding and rewinding. The skim will be given benchmark, 100, by
the subjects for grasping completely the content of the original video. Figure 18
reveals the averaging results of the skims of the four test movies. From the figure, we
similarly conclude that the skim content coverage is relatively good at the skim rate
Table 1 Results of shot change detection
Movie Length Ground truth Hits False Miss Recall Precision
(mins) hard cut (gradual) alarm
Kung-Fu 93 1485 (18) 1336 140 149 89.96% 90.52%
Banlieue 13 81 2637 (11) 2365 286 272 89.69% 89.21%
Goal 114 2317 (44) 2111 204 206 91.11% 91.19%
Madagascar 82 1105 (17) 980 133 125 88.69% 88.05%
Spiderman 1 122 2071 (35) 1870 242 201 90.29% 88.54%
Spiderman 2 128 1927 (21) 1745 194 182 90.55% 90.00%
Spiderman 3 133 2138 (28) 1926 198 212 90.08% 90.67%
Multimed Tools Appl (2009) 44:205–228 223
0 20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
100 Spiderman 1
Minutes
St
or
y 
In
te
ns
ity
(%
)
(a)
0 20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
100 Spiderman 2
Minutes
St
or
y 
In
te
ns
ity
(%
)
0 20 40 60 80 100 120
0
10
20
30
40
50
60
70
80
90
100 Spiderman 3
Minutes
St
or
y 
In
te
ns
ity
(%
)
(b) (c)
Fig. 17 Story section boundary of Spiderman film series: a Episode 1, b Episode 2, and c Episode 3
standard to evaluate the performance of the generated skims due to subjective
determination. User study has been widely employed to evaluate the performance of
video abstraction such as works [18] and [16]. As can be seen in Fig. 19, the subjective
performance of the proposed method is better than that of Smith’s method because
the proposed method can obtain a tempo curve that describe the structure of a film.
With the proposed authoring scheme, the generated skim is produced according to
the length and story intensity of each story section. Compared to [22], the length of
each selected fragments depends on their significance (i.e. story intensity). Therefore,
the skim of the proposed method can convey more concrete concept than that of
[22]. The proposed method has similar performance with Ma’s scheme. Though
Ma’s scheme can obtain the promising results, huge amount of computations are
Table 3 Results of story section boundary detection of Spiderman film series
Movie Length Human option Detected False alarm Miss
Spiderman 1 122 mins 10 stories 9 0 1
Spiderman 2 128 mins 8 stories 8 0 0
Spiderman 3 133 mins 9 stories 8 0 1
Multimed Tools Appl (2009) 44:205–228 225
required to establish user attention models, including visual and aural attention
models, restricting the practicability of their proposed system. The visual features
selected in this work can be obtained with partial decoded bitstream. The use of
partial decoded bitstream shares the advantage of avoiding high complexity analysis
in feature extraction. This is driven by affordable computational cost in many
applications, such as video adaptation.
5 Conclusion
In this paper, a comprehensive method is proposed for the abstraction of movies via
audiovisual tempo analysis. Visual and audio cues are fused to determine interesting
shots of a given video and screen out irrelevant shots so as to form a tempo curve that
is itself a distribution of the interesting shots. Various tempo curves are extracted
from a video through multimodal fusion techniques according to different needs.
Experimental results show that one can easily analyze the structures of movies via
these tempo curves. With the proposed authoring scheme, a thorough and complete
skim of a movie, like that of a movie trailer, can be successfully extracted. The
proposed method efficiently analyzes various types of movies in many areas of
applications, including video indexing, retrieval, summarization, and skimming.
Acknowledgements The authors would like to thank the the National Science Council of the
Republic of China for financially supporting this research under Contracts No. NSC95-2218-E-259-
047 and NSC96-2628-E-110-020-MY2.
References
1. Benini S, Migliorati P, Leonardi R (2007) A statistical framework for video skimming based on
logical story units and motion activity. In: Proceedings of international workshop on content-
based multimedia Indexing. IEEE, Piscataway, pp 152–156
2. Block B (2001) The visual story: seeing the structure of film, TV, and new media. Focal, Boston
3. Fischer S, Lienhart R, Effelsberg W (1995) Automatic recognition of film genres. In: Proceedings
of international ACM conference on multimedia. ACM, New York, pp 295–304
4. Gargi U, Kasturi R, Strayer SH (2000) Performance characterization of video-shot-change de-
tection methods. IEEE Trans Circuits Syst Video Technol 10(1):1–13
5. Gong Y, Sin L-T, Chuan C-H, Zhang H-J, Sakauchi M (1995) Automatic parsing of TV soccer
programs. In: Proceedings of the international conference on multimedia computing and systems.
IEEE, Piscataway, pp 167–174
6. Gouyon F, Pachet F, Delerue O (2000) On the use of zero-crossing rate for an application of
classification of percussive sounds. In: Proceedings of the COST G-6 conference on digital audio
effects, Verona, 7–9 December 2000, pp 1–6
7. Hanjalic A (2003) Generic approach to highlights extraction from a sport video. In: Proceedings
of the IEEE international conference on image processing. IEEE, Piscataway, pp 1–4
8. Hanjalic A (2003) Multimodal approach to measuring excitement in video. In: Proceedings of
the IEEE international conferences on multimedia and expo. IEEE, Piscataway, pp 289–292
9. Huang CL, Liao BY (2001) A robust scene-change detection method for video segmentation.
IEEE Trans Circuit Syst Video Technol 11(12):1281–1288
10. Jasinschi RS, Dimitrova N, McGee T, Agnihotri L, Zimmerman J, Li D, Louie J (2002) A
probabilistic layered framework for integrating multimedia content and context information. In:
Multimed Tools Appl (2009) 44:205–228 227
Chia-Hung Yeh received his B.S. and Ph.D. degrees from National Chung Cheng University,
Taiwan, in 1997 and 2002, respectively, all from the Department of Electrical Engineering. With
research fellowships, he then worked as a postdoctoral fellow at Prof. C.-C. Jay Kuo’s research group
at the Department of Electrical Engineering-Systems, University of Southern California until Dec.,
2004. Dr. Yeh was a technical consultant of ALi cooperation from 2003 to 2004, and joined MAVs
Lab. Inc., U.S. in 2004, and MAVsLab. Inc., Taiwan, from 2005 to 2006, as the vice president and
chief technology officer. He was an assistant professor at the Department of Computer Science
and Information Engineering and the Institute of Electronics Engineering, National Dong-Hwa
University from 2006–2007, and taught as an adjunct assistant professor at National Chung-Hsing
University from 2005–2007. In Aug. 2007, he joined the Department of Electrical Engineering,
National Sun Yat-Sen University as an assistant professor, and in Aug. 2008, its Institute of
Communications Engineering. Dr. Yeh’s research interests include multimedia communication,
multimedia database management, image/audio/video signal processing, bioinformatics and optical
information processing/computing. In addition to being a member of the Institute of Electrical and
Electronics Engineers (IEEE) and International Society for Optical Engineering (SPIE), he serves
on the Editorial Board of Journal of Multimedia (JMM) and is a council of Taiwan Institute of
Electrical and Electronic Engineering (TIEEE). Dr. Yeh has been an invited speaker, a chair at
conferences, a reviewer, and a co-author of a book chapter and more than 80 technical international
conference and journal papers. He received the 2002 outstanding student award from CCU, and
2007 distinguished assistant professor award of NSYSU, 2009 outstanding mentor award of College
of Engineering of NSYSU, and is listed in Who’s Who in Asia, Who’s Who in America, Who’s Who
in Science and Engineering, Who’s Who in the World.
Chih-Hung Kuo received the B.S. and M.S. degrees from the National Tsing Hua University,
Hsinchu, Taiwan, in 1992 and 1994, respectively, and the Ph.D. degree from the University of
Southern California (USC), Los Angeles, USA, in 2003, all in Electrical Engineering. He was with
 1
AN EFFICIENT EMOTION DETECTION SCHEME FOR POPULAR MUSIC 
 
Chia-Hung Yeh1, Hung-Hsuan Lin2 & Hsuan-Ting Chang3 
 
1Department of Electrical Engineering,  
National Sun Yat-Sen University, 
804 Taiwan. 
yeh@mail.ee.nsysu.edu.tw   
2Department of Computer Science and Information 
Engineering, National Dong-Hwa University,             
974 Taiwan. 
3Department of Electrical Engineering, 
National Yunlin University of Science and Technology, 
640 Taiwan. 
htchang@yuntech.edu.tw  
 
ABSTRACT 
 
With the rapid growth of multimedia information, the ability 
to efficiently manage data from large amount of multimedia 
database has become a crucial issue. In this paper, a 
framework for music emotion detection is proposed. First, a 
Thayer’s 2-Dimentinal model that represents the music 
emotion space is employed as our emotion model. Second, 
three features such as intensity, rhythm regularity, and 
tempo are extracted to describe a music clip. Then, features 
are trained by constructing Gaussian Mixture Models 
(GMM). Finally, the likelihood radios of test music clips to 
GMM are calculated for emotion identification. Experiemtal 
results show that the average recall and precision all are up 
to 80% for the database that is comprised of 145 music clips.   
 
Index Terms— music emotion detection, Thayer’s 
model, Gaussian Mixture Model 
 
1. INTRODUCTION 
 
In recent years, multimedia information evolved an 
escalated level. According to the Ipsos-Reid Survey [1], the 
digital music has taken a significant place in people’s life. 
Since mass storage have become easy to get, to handle large 
amount of music database became an important research 
issue. Traditional approaches are searching by album, artist, 
alphabet, and so on; however, these methods do not satisfy 
people in some situations and specific requirements. For 
example, when people want to sleep, they need a sleepy 
music with slow tempo to relax their bodies and help them 
easy sleep. When participating in the party, some exciting 
music with fast tempo is required to help them release their 
passion. In these cases, to classify music by emotions is 
more reasonable than traditional ways because people may 
not have patient to find suitable music by checking one file 
to another. That is the critical part we want to break 
through – managing files based on emotions.  
In this paper, a framework for emotion detection is 
proposed. Figure 1 shows the flowchart of the proposed 
 
  
Fig. 1. Flowchart of the proposed scheme 
 
algorithm. First, three features including intensity, rhythm 
regularity, and tempo are calculated for each music clip in 
feature extraction step. After feature extraction, we 
construct GMMs by using training data in advance. Then, 
the probability of each GMM is calculated to identify the 
emotion of each testing music clip. Experimental results 
show that the accuracy of the proposed method can achieve 
up to %80.  
The rest of this paper is organized as follows. The 
Thayer’s 2-D emotion model is introduced in Sec. 2. The 
feature extraction including intensity, rhythm regularity, and 
tempo is explained in Sec. 3. GMM testing and training are 
discussed in Sec. 4. Experimental results are shown in Sec. 
5.  Concluding remarks are given in Sec. 6. 
 
2. EMOTION MODEL 
 
An important issue regarding emotion detection is the way 
to classify emotions of music. Traditional approaches 
usually use adjectives to classify emotions. For example, the 
Music Clip 
Gaussian Mixture Model 
(GMM) 
Feature Extraction 
Emotion 
Detection 
978-1-4244-3828-0/09/$25.00 ©2009 IEEE 1799
 3
 
Fig. 4. Detection function 
 
Fig. 5. Example of envelope extraction 
 
where l is the length of half-Hamming window. Figure 5 
shows an example of the envelope extraction. 
 
3.2.2. Rhythm Regularity 
The peaks in a detection function are used to calculate 
rhythm regularity. First, only the peaks in the detection 
function are preserved and others are set to be zero. Second, 
in order to extract the significant peaks, peaks with their 
amplitudes less than 50% of the maximum amplitude are set 
to zero. Figure 6 shows an example of significant peak 
extraction in a detection function. Finally, a numerical 
sequence can be obtained by calculating the distances of 
each neighboring peak, and then the standard deviation of 
this sequence is calculated as the matching criterion. If there 
is a song with steady rhythm, the distances of peaks in the 
detection function will be almost the same, and the standard 
deviation of its numerical sequence will be small; otherwise, 
the standard deviation of the numerical sequence will be 
greater. The rhythm regularity is represented by the standard 
deviation of peak distances. 
 
3.2.3. Discrete Fourier Transform 
As can be seen in Fig. 5, it seems to be periodical in the 
detection function. To find out the main period of these 
peaks, Discrete Fourier Transform (DFT) is applied. DFT 
transforms a detection function from the time domain into 
the frequency domain. A periodic signal not only has the 
fundamental frequency, but also has its harmonic frequency 
of its integer multiples or integer divisions. Our goal here is 
to extract the fundamental frequency. First, only the values 
of peaks are kept. Then, the peak with less than 20% of the 
maximum energy is set to zero for the inconsiderable 
information elimination. Second, the similar approach to the 
fundamental frequency detection [4] is employed to estimate 
the maximum common divisor of the detected peaks, as 
defined as  
 
Fig. 6. Example of significant peak extraction in a detection 
function 
1
arg min ,
k
Np
i i
P i k k
P PFF round
P P=
⎛ ⎞= − ⎜ ⎟⎝ ⎠∑      (4) 
 
where FF  is the fundamental frequency, iP  is the i th 
detected peak, and ( )round x  calculates the nearest integer 
to x . 
 
3.2.4. Approximation Function 
The approximation function can be expressed in terms of an 
infinite sum of sines and cosines by the Fourier series 
theorem. Here, the periodical signal can be represented by a 
cosines function, for more detail seen in [3]. The 
approximation function is defined as  
2( ) cos( ),app n C FF n
fs
π= × × ×           (5) 
where C  is a constant that represents the amplitude of 
( )app n , and fs  is the sampling rate. Figure 7 shows an 
example of the approximation function.  
 
 
Fig. 7. Example of approximation function 
 
The blue line is the detection function and the green 
line is the approximation function curve. Finally, to obtain 
tempo speed, the total number of peaks in the approximation 
function is calculated. Then, the ratio between the number 
of peaks and the corresponding time duration is calculated 
to obtain the beats per second (BPS) that will be our tempo 
feature. 
4. GAUSSIAN MIXTURE MODEL 
 
After feature extraction, we obtain three feature sets 
including intensity, rhythm regularity, and tempo. In the 
training step, we model feature sets by using GMM to 
describe the distribution. In constructing each GMM, the 
1801
 1
AN EFFICIENT EMOTION DETECTION SCHEME FOR POPULAR MUSIC 
 
Chia-Hung Yeh1, Hung-Hsuan Lin2 & Hsuan-Ting Chang3 
 
1Department of Electrical Engineering,  
National Sun Yat-Sen University, 
804 Taiwan. 
yeh@mail.ee.nsysu.edu.tw   
2Department of Computer Science and Information 
Engineering, National Dong-Hwa University,             
974 Taiwan. 
3Department of Electrical Engineering, 
National Yunlin University of Science and Technology, 
640 Taiwan. 
htchang@yuntech.edu.tw  
 
ABSTRACT 
 
With the rapid growth of multimedia information, the ability 
to efficiently manage data from large amount of multimedia 
database has become a crucial issue. In this paper, a 
framework for music emotion detection is proposed. First, a 
Thayer’s 2-Dimentinal model that represents the music 
emotion space is employed as our emotion model. Second, 
three features such as intensity, rhythm regularity, and 
tempo are extracted to describe a music clip. Then, features 
are trained by constructing Gaussian Mixture Models 
(GMM). Finally, the likelihood radios of test music clips to 
GMM are calculated for emotion identification. Experiemtal 
results show that the average recall and precision all are up 
to 80% for the database that is comprised of 145 music clips.   
 
Index Terms— music emotion detection, Thayer’s 
model, Gaussian Mixture Model 
 
1. INTRODUCTION 
 
In recent years, multimedia information evolved an 
escalated level. According to the Ipsos-Reid Survey [1], the 
digital music has taken a significant place in people’s life. 
Since mass storage have become easy to get, to handle large 
amount of music database became an important research 
issue. Traditional approaches are searching by album, artist, 
alphabet, and so on; however, these methods do not satisfy 
people in some situations and specific requirements. For 
example, when people want to sleep, they need a sleepy 
music with slow tempo to relax their bodies and help them 
easy sleep. When participating in the party, some exciting 
music with fast tempo is required to help them release their 
passion. In these cases, to classify music by emotions is 
more reasonable than traditional ways because people may 
not have patient to find suitable music by checking one file 
to another. That is the critical part we want to break 
through – managing files based on emotions.  
In this paper, a framework for emotion detection is 
proposed. Figure 1 shows the flowchart of the proposed 
 
  
Fig. 1. Flowchart of the proposed scheme 
 
algorithm. First, three features including intensity, rhythm 
regularity, and tempo are calculated for each music clip in 
feature extraction step. After feature extraction, we 
construct GMMs by using training data in advance. Then, 
the probability of each GMM is calculated to identify the 
emotion of each testing music clip. Experimental results 
show that the accuracy of the proposed method can achieve 
up to %80.  
The rest of this paper is organized as follows. The 
Thayer’s 2-D emotion model is introduced in Sec. 2. The 
feature extraction including intensity, rhythm regularity, and 
tempo is explained in Sec. 3. GMM testing and training are 
discussed in Sec. 4. Experimental results are shown in Sec. 
5.  Concluding remarks are given in Sec. 6. 
 
2. EMOTION MODEL 
 
An important issue regarding emotion detection is the way 
to classify emotions of music. Traditional approaches 
usually use adjectives to classify emotions. For example, the 
Music Clip 
Gaussian Mixture Model 
(GMM) 
Feature Extraction 
Emotion 
Detection 
978-1-4244-3828-0/09/$25.00 ©2009 IEEE 1799
 3
 
Fig. 4. Detection function 
 
Fig. 5. Example of envelope extraction 
 
where l is the length of half-Hamming window. Figure 5 
shows an example of the envelope extraction. 
 
3.2.2. Rhythm Regularity 
The peaks in a detection function are used to calculate 
rhythm regularity. First, only the peaks in the detection 
function are preserved and others are set to be zero. Second, 
in order to extract the significant peaks, peaks with their 
amplitudes less than 50% of the maximum amplitude are set 
to zero. Figure 6 shows an example of significant peak 
extraction in a detection function. Finally, a numerical 
sequence can be obtained by calculating the distances of 
each neighboring peak, and then the standard deviation of 
this sequence is calculated as the matching criterion. If there 
is a song with steady rhythm, the distances of peaks in the 
detection function will be almost the same, and the standard 
deviation of its numerical sequence will be small; otherwise, 
the standard deviation of the numerical sequence will be 
greater. The rhythm regularity is represented by the standard 
deviation of peak distances. 
 
3.2.3. Discrete Fourier Transform 
As can be seen in Fig. 5, it seems to be periodical in the 
detection function. To find out the main period of these 
peaks, Discrete Fourier Transform (DFT) is applied. DFT 
transforms a detection function from the time domain into 
the frequency domain. A periodic signal not only has the 
fundamental frequency, but also has its harmonic frequency 
of its integer multiples or integer divisions. Our goal here is 
to extract the fundamental frequency. First, only the values 
of peaks are kept. Then, the peak with less than 20% of the 
maximum energy is set to zero for the inconsiderable 
information elimination. Second, the similar approach to the 
fundamental frequency detection [4] is employed to estimate 
the maximum common divisor of the detected peaks, as 
defined as  
 
Fig. 6. Example of significant peak extraction in a detection 
function 
1
arg min ,
k
Np
i i
P i k k
P PFF round
P P=
⎛ ⎞= − ⎜ ⎟⎝ ⎠∑      (4) 
 
where FF  is the fundamental frequency, iP  is the i th 
detected peak, and ( )round x  calculates the nearest integer 
to x . 
 
3.2.4. Approximation Function 
The approximation function can be expressed in terms of an 
infinite sum of sines and cosines by the Fourier series 
theorem. Here, the periodical signal can be represented by a 
cosines function, for more detail seen in [3]. The 
approximation function is defined as  
2( ) cos( ),app n C FF n
fs
π= × × ×           (5) 
where C  is a constant that represents the amplitude of 
( )app n , and fs  is the sampling rate. Figure 7 shows an 
example of the approximation function.  
 
 
Fig. 7. Example of approximation function 
 
The blue line is the detection function and the green 
line is the approximation function curve. Finally, to obtain 
tempo speed, the total number of peaks in the approximation 
function is calculated. Then, the ratio between the number 
of peaks and the corresponding time duration is calculated 
to obtain the beats per second (BPS) that will be our tempo 
feature. 
4. GAUSSIAN MIXTURE MODEL 
 
After feature extraction, we obtain three feature sets 
including intensity, rhythm regularity, and tempo. In the 
training step, we model feature sets by using GMM to 
describe the distribution. In constructing each GMM, the 
1801
 -1- 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                           97 年 10 月 28 日 
報告人姓名 葉家宏 
服務機構 
及職稱 
國立中山大學電機工程所 
助理教授 
時間 
會議 
地點 
97 年 10 月 6 日~10 月 15 日 
澳洲，凱恩斯，莫爾本 
本會核定 
補助文號 
NSC96-2628-E-110-020-MY2  
會議 
名稱 
(中文) 2008 多媒體信號處理國際研討會 
(英文) 2008 International Workshop on Multimedia Signal Processing 
發表 
論文 
題目 
(中文)  
(英文)  
 
報告內容應包括下列各項： 
 
一、 參加會議經過 
 
2008 MMSP (International Workshop on Multimedia Signal Processing) 國際多媒體訊號處
理年會是由國際電機電子工程師學會訊號處理分會(IEEE Signal Processing Society)、
NICTA 研究中心(NICTA Research Lab.)等機構共同舉辦，於 10 月 08 日到 10 日在澳洲
凱恩斯(Cairns, Australia)舉行。此次會議中共發表 180 論文。MMSP 2008 是第十屆多媒
體訊號處理國際研討會，此研討會是由 IEEE 訊號處理協會的多媒體訊號處理技術委員
會所組織，此研討會中討論一個新的研究主題，就是生命科學研究中的仿生物多媒體訊
號處理(Bio-Inspired Multimedia Signal Processing)。MMSP 2008 主要的目標是能促進多媒
體訊號處理發展的廣度，同時也希望能與新興的研究領域例如生命科學能互相影響，此
研討會將會專注於此領域中的主要趨勢以及挑戰，包括共同討論一個出未來研究與應用
的準則。 MMSP 2008 研討會包括下列幾項熱門的研究主題。 
1. 仿生物多媒體訊號處理 (Bio-inspired multimedia signal processing) 
2. 結合影音處理, 模型辨識,感應器融合,醫學影像 2D 與 3D 圖片/幾何圖型的編碼與
動畫、數位影像的預處理與後處理,結合通道與訊號源編碼、資料串流、聲音以及
影像的編碼與處理   [Joint audio/visual processing、pattern recognition、sensor 
fusion, medical imaging, 2-D and 3-D graphics/geometry coding and animation、
pre/post-processing of digital video、joint source/channel coding、data streaming, 
speech/audio、image/video coding and processing] 
3. 多媒體資料庫（內容分析、表述、索引、識別與修正） [Multimedia databases (content 
analysis、representation、indexing、recognition and retrieval)] 
4. 結合多種程式來達到人機介面與互動(Human-machine interfaces and interaction 
