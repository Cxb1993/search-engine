 2
中文摘要 
隨著相機與攝影機的普及化，人們在影像品質的要求也日益提高。取像元件發展與其製造技術，
仍受限於光學基本原理，時常造成拍攝時物體失焦或影像模糊，為解決這個問題，可分別拍攝多張不
同焦距之影像，進而將它們融合成擁有完整訊息的清晰影像。開發這項技術將能夠從原本龐雜的影像
資料中，整合出一張擁有完整訊息的影像，除了讓融合後的影像在人眼辨識上更清楚之外，還能夠提
供更完整的影像訊息，增進後續數位影像處理的效能，例如物件偵測、特徵擷取和物件辨識等。 
過去的研究方法已經可以解決影像中的高頻訊號部份，但是低頻訊號因涉及影像變化平緩或是真
正模糊的區域，比較難以處理。本計畫擬發展一種以區域概念為基礎的漸進式多層次解析之影像融合
技術，分析融合過程中前後張影像相互訊息(mutual information)的關聯性，在四分樹的架構中漸進
式地分割影像清晰與模糊的區域，並設計適合的融合演算法，將多張輸入影像結合成一張清晰的影像。
此外，動態視訊資料擁有非常大量的影像資訊，但是並非單一張影像就足以提供使用者進行後續處理
與判斷，如何有效融合多張影像也是我們希望達成的目標。計畫中將建置測試用的影像資料庫，在不
同的影像品質測量方法下，分析效能並完成最佳化設計。 
 
關鍵詞：影像處理、影像融合、四分樹、互訊息 
 
英文摘要 
Since the rapid change of technology in electric industry, cameras are very popular in 
our daily lives. At the same time, the demand of images with high quality is also increasing. 
The purpose of image fusion is to integrate complementary information from multiple images. 
The result of image fusion is a new image which is more suitable for human and machine 
perception or further image-processing tasks such as image segmentation, feature extraction 
and object recognition. 
The traditional method used wavelet transform to get the information about high frequency 
and low frequency in images. Fusion rules are usually developed by simple formulas like 
maximum or average. The new image is obtained by taking the inverse wavelet transform of 
the fused wavelet coefficients. Recently, area-based maximum selection and consistency 
verification are then applied for decision making in image fusions. The objectives of the 
 4
化的融合程序，產生某一個景象下的最佳影像品質，是非常值得深入研究的主題。 
文獻探討 
使用基於小波變換的影像融合方法，必需考慮能夠在沒有損失資訊的狀況下將輸入的影
像做處理。以前的研究曾使用計算比較簡單的方法來結合小波係數，像是權重法，選擇最大
值甚至做線性及非線性分析。Hong Zhang, Lei Liu 和 Nan Lin [5]提出了一個新的醫學影
像融合方法，使用小波係數的基礎上去分析影像能量。 Huaixin Chen 提出了以主成分分析
法為基礎的影像融合方法 [5] 。Nikolaos Mitianoudis 和 Tania Stathaki 使用獨立分量
分析，發展另一種不同的方法[8] 。而最近的研究有些基於分割影像的做法，例如，Zhang 
Yingjie 和 Ge Liling[13]提出的方法是先區分物件，然後判斷各物件的優先次序。利用這
些區域的各種特徵來做計算，以確定其中的圖片哪些特徵需要融合到影像之中。這些研究改
進了原有的方法，可以考慮根據影像的特徵，針對不足訊息的來源影像做分割，發展出更有
智慧的融合規則。 
本文提出的融合方法，包括了小波轉換及自動判斷分割規則。在判斷分割的規則中，結
合了小波係數的計算，以四分樹來做切割，針對其中每個區域的小波係數，產生一個區域資
訊量（local measurement value, LMV）的計算方法。最後，我們用一些模擬的影像做實驗，
來比較本文所提出的方法與以往的融合方法的效果。過去有許多研究顯示，頻率域轉換在影
像處理中是非常有用的 [3]，不但能夠方便的表示影像，且能經過計算處理後，又可以還原
成影像。所以我們使用小波轉換中的離散小波轉換（discrete wavelet transform），來轉
換我們要融合的兩張或更多的圖片。接著，我們針對小波係數的低頻段和高頻段特性的不同
亦使用不同的規則來做計算。最後，利用我們反轉小波變換完成了融合後的影像。圖 1為基
於小波變換的影像融合流程圖。 
 
圖 1.  小波融合流程圖 
 6
值。 
四分樹是在每個部分影像在兩個方向的維度，分割該區域成為四個平等的象限，原影像
則分成四個區塊。用四分樹的方式來看，每個節點中若符合切割規則就會分割產生四個葉節
點，若沒有需要切割就不會產生葉節點。每個影像區塊的四分樹會切割為四份相同的大小，
圖 2顯示影像使用四分樹做分割的情形。圖 3為分割後的四分樹表示圖。 
 
 
圖 2. 經過分割的影像示意圖 
 
 
R
R2 R4R3R1
R34R33R32R31R24R23R22R21
R321 R322 R323 R324  
圖 3. 影像分割後的四分樹表示圖 
 
使用分割法區分，然後選擇適當的區域。所以，使用分割方法可以改善原來沒有分割方
法的融合效果。我們在圖 4說明，如何加上這個分割的方法。 
 
 8
(b) 對焦於遠處之影像 
 
圖 5. 原始焦距影像 
 
 
 
(a) 最大值法影像融合 
 
(b) 最大值加上自適應
分割法影像融合 
(c) 權重值法影像融合 
 
(d) 權重值加上自適應
分割法影像融合 
(e) 平均值法影像融合 
 
(f) 平均值加上自適應
分割法影像融合 
圖 6. 不同方法的影像融合結果 
 
我們使用 RMSE（均方根誤差）的計算方法作為我們的實驗的測量值，其計算方法。表示如下： 
 
( ) ( )[ ]
2/1
1 1
2,,1 ⎥⎦
⎤⎢⎣
⎡ −= ∑∑
= =
M
i
N
j
FR jixjixMN
RMSE (6) 
 
 10
[13] Zhang Yingjie, Ge Liling, “Region-based Image Fusion by Using Region Priorities”, Software Engineering, 
Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2007. SNPD 2007. Eighth ACIS 
International Conference on, pp. 854-860, 2007. 
[14] Z. Zhang, R.S. Blum, “A Categorization of Multi-scale Decomposition-based Image Fusion Schemes with a 
Performance Study for a Digital Camera Application”, Proc. IEEE87, Vol. 8 , pp.1315–1326, 1999. 
 
 
著作發表 
 
1. C. C. Chen and Y. H. Tsai, “Adaptive Reversible Image Watermarking Scheme,” Journal of 
Systems and Software, vol. 84, no. 3, 2011, pp. 428-434. (SCI) 
 
2. Y. H. Tsai and K. C. Kao, “License Plate Detection on Autonomous Surveillance Systems,” 
Journal of Computational Information Systems, vol. 6, no. 14, 2010, pp. 4941-4949.(EI) 
 
3.蔡耀弘、林思妤, 以手勢識別為基礎之視覺化人機互動技術, International Conference on 
Advanced Information Technologies, 2011/04/22~23 
 
4. 蔡耀弘、溫明勳, 利用多模態膚色機率模型之膚色偵測, International Conference on Advanced 
Information Technologies, 2011/04/22~23 
 
5. 蔡耀弘、溫明勳,可應用於飯店整合經營管理之影像偵測系統, 2011 台灣商管理論與實務研討會, 
2011/05/13 
 
6. 蔡耀弘、林思妤, 飯店餐飲業之服務型機器人視覺介面技術, 2011 台灣商管理論與實務研討會, 
2011/05/13 
 
 12
國科會補助計畫衍生研發成果推廣資料表 
日期：100年 8 月 31 日 
國科會補助計畫 
計畫名稱：區域式多層次解析之影像融合技術 
計畫主持人：蔡耀弘 
計畫編號：NSC 99-2221-E-364 -003領域：影像處理 
（中文）區域式多層次解析之影像融合技術 
研發成果名稱 （英文）Region based hierarchical image fusion 
 
成果歸屬機構 國科會 發明人 (創作人) 
蔡耀弘 
技術說明 
該項技術將能夠從原本龐雜的影像資料中，整合出一張擁有完整
訊息的影像，除了讓融合後的影像在人眼辨識上更清楚之外，還
能夠提供更完整的影像訊息，增進後續數位影像處理的效能，例
如物件偵測、特徵擷取和物件辨識等。以區域概念為基礎的漸進
式多層次解析之影像融合技術，分析融合過程中前後張影像相互
訊息的關聯性，在四分樹的架構中漸進式地分割影像清晰與模糊
的區域，並設計適合的融合演算法，將多張輸入影像結合成一張
清晰的影像。 
產業別 
資訊、電子 
技術/產品應用範圍 
數位照相、影像處理軟體、圖形辨識軟體、安全監控 
技術移轉可行性及預期
效益 
本技術可作為多數應用的前處理，或數位相片的編修，具有應用
價值 
     註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
 
 
 2
行各場次的研討主題。報到時因為場地太小，排隊等候的人很多，所以花了很
長的等待時間。 
5. 七月二十八日參加 IMAGE PROCESSING等相關技術場次的相關研討主題，期
間與各國學者有相當多的意見交流。 
6. 七月二十九日參加 RECOGNITION TECHNIQUES 等相關技術場次的相關研討
主題，並進行論文發表，題目為 Fast Facial Feature Extraction and Matching with 
Artificial Face Models，引起很多迴響。 
7. 七月二十九日搭機返國，於七月三十日中午抵達台灣。 
 
二、與會心得 
論文發表數量多而且品質佳，相關論文收錄在論文集 ISSN: 2010-376X 與
ISSN: 2010-3778 中，聽眾與講者進行許多意見交流，這些觀念對於影像處理、電
腦視覺與認知學習等領域，在方法設計上的著眼有適切的描述並且可行，我們將
運用參考這些觀念做為今後研發與設計的考量。講者介紹最新的技術，並且提供
實作技術經驗分享，符合人類視覺與影像相關技術中的經驗，技術項目的取捨與
應用方向。 
主辦單位並沒有依循往例在晚上舉辦 BANQUET，提供與會人士餐敘，而是在
中午時段以茶敘的方式讓與會者用餐，另一方面也提供一個交流的平台，席間和
多人談天，獲得其他許多專業領域上的經驗。或許是因為巴黎的物價太貴了無法
在飯店內續行晚宴，主辦單位除了提供簡易的餐點外，還有當地的舒適的休息空
間。 
INVITATION LETTER
April 15, 2011
Assist. Prof. Dr. Yao-hong Tsai
Hsuan Chuang University
Taiwan
To Whom It May Concern,
We are pleased to inform you that your peer-reviewed & refereed full paper entitled "Fast
Facial Feature Extraction And Matching With Artificial Face Models" is accepted for oral
presentation at the ICACSE 2011: International Conference on Applied Computer Science and
Engineering  to be held in Paris, France during July 27-29, 2011.
This letter serves as confirmation of your participation in the conference.
We would greatly appreciate if you could facilitate granting the conference delegate the
necessary visa.
Sincerely Yours,
C. Ardil,
Conference Secretariat
ICACSE 2011 Paris
France
Holiday Inn Paris
Montparnasse-Av.Du Maine
79-81 Avenue Du Maine
Paris, 75014 France
Phone: +33-1-43201393
Fax: +33-1-43209560
www.holidayinn.com/parisgare
    
 
  
II. THE OVERVIEW OF  ADABOOST AND ACTIVE SHAPE 
MODELS 
The AdaBoost was first proposed by Freund and Schapire 
[14]. It can automatically select some weak classifiers from the 
weak classifier space to construct a strong classifier through the 
weighted integration of selected weak classifiers. After that, 
authors proposed new AdaBoost just like Schapire and Singer 
[15], and Friedman, Hastie and Tibshirani [16]. They was used 
extensively on face detection [10][17]. 
In this paper, the proposed method is based on the Adaboost 
algorithm of Viola and Jones [10], which has excellent 
performance on robust real-time face detection. Toward this 
end they constructed a frontal face detection system which 
achieves detection and false positive rates. This system 
achieves high frame rates working only with the information 
present in a single grey scale image. These alternative sources 
of information can also be integrated with their system to 
achieve even higher frame rates. There are two main 
contributions in the detection framework. We will introduce the 
ideas briefly below. 
The first one is a new image representation called in integral 
image that allows for very fast feature evaluation. The summed 
area table (SAT) is shown in Fig. 1. The formula is listed in the 
following. 
 
,0)1,(),1(
),1,1(),(
),1()1,(),(
,),(),(
'' ,
''
=−=−
−−−+
−+−=
= ∑
≤≤
xSATySAT
yxSATyxI
yxSATyxSATyxSAT
yxIyxSAT
yyxx
 
 
 
 
 
Fig. 1. The summed area table 
 
The second one is a simple and efficient classifier that is 
built by selecting a small number of important features from a 
huge library of potential features using AdaBoost. Within any 
image sub-window the total number of Haar-like features is 
very large, far larger than the number of pixels. In order to 
ensure fast classification, the learning process must exclude a 
large majority of the available features, and focus on a small set 
of critical features. As a result each stage of the boosting 
process, which selects a new weak classifier, can be viewed as a 
feature selection process. AdaBoost provides an effective 
learning algorithm and strong bounds on generalization 
performance. The algorithm is listed in the following. 
 
Algorithm 
Given the image features (x1, y1), (x2, y2)…, (xn, yn). 
Each yi= 0,1 indicates the negative and positive pattern. 
Initially the weighting is set by lm
w i 2
1,
2
1
,1 =
. 
The amounts of the negative and positive patterns are 
expressed by m and l. 
For t = 1, 2, …, T   
Normalize the weight 
∑ =← nj jt
it
it
w
w
w
,
,
,
, 
Calculate the weighting to select weak classifier ( )∑ −=∈
i
iiipft ypfxhw .,,,min ,, θθ
 
Define ( ) ( ),,,, tttt pfxhxh θ=  when the tt pf ,  and tθ  are 
minimum of the t∈ . 
The way to update the weighting is 
ie
titit ww
−
+ = 1,,1 β , when 
the ix  is correct, 0=ie , otherwise is 1=ie  , then 
t
t
t ∈−
∈=
1
β
. 
At last, the decision of the strong classifier is obtained by  
( ) ( )
⎪⎪⎩
⎪⎪⎨
⎧ ≥
=
∑ ∑
= =
otherwise
xh
xC
T
t
T
t
ttt
0
2
11
1 1
αα
 ,where t
t βα
1log=
. 
 
In ASM [11], each sample pattern contour is first marked to 
identify fiducial points. These points were subsequently 
normalized by rotation, scaling, and translation. Next, PCA was 
used to correlate points in order to get parameters for 
controlling object variations. An initial shape was placed on the 
image and adjusted by the above derived parameters to match 
strong edges and corners. The above two methods are adapted 
in the proposed face feature extraction. 
III. THE PROPOSED METHOD 
Since skin color is an important apparent property for a face 
image, skin color detection is performed to get the initial 
placement of the extraction algorithm. But skin color may 
differ from race to race, and depend on the lighting conditions. 
A good modeling for skin color requires choosing an 
appropriate color space and identifying a cluster associated 
with skin color in this space. Some color models in the YCbCr 
space were proposed. Hsu et al. [18] developed a non-linear 
transformed YCbCr color space and an elliptical skin model in 
the transformed space. Hsu’s Gaussian model can perform well 
in skin pixel classification. It is also observed that the 
distribution of Z in the transformed space changes with the 
small or large luminance. Skin color was also applied to other 
applications [19]. Fig. 2 shows the input image and the detected 
face region by skin color detection.  
 
  
 
(a)                                           (b) 
Fig. 4. (a) the detected eyes pattern (b) the face region for the 
AdaBoost detection algorithm. 
 
The resulted facial features are evaluated by matching with 
artificial face models in the applications of physiognomy. 
Hence, a matching system is developed by Hausdorff distance 
[21]. The Hausdorff distance was originally proposed for 
binary image comparison. Unlike most shape comparison 
methods, the Hausdorff distance can be calculated without the 
explicit pairing of points in their respective data sets, and can 
be extended to allow partial matching. It is because of this 
desirable property that we use it to measure the proposed 
algorithm. Given two finite points sets A={a1, a2, … ,am} and 
B={b1, b2, … ,bm}, the Hausdorff Distance is defined as 
 
)},(),,(max{),( ABhBAhBAH = , 
where 
baBAh
BbAa
−=
∈∈
minmax),(  
 
is called the directed Hausdorff distance from A to B. 
The feature vector of a human face consists of 21 feature 
values. Length, width and angle to the x-axis are calculated 
from eyebrow, eyes, nose and mouth. Furthermore, we divide 
the features of eyebrow, eyes and mouth into left and right part 
to describe detailed information of the human face. 
IV. EXPERIMENTATIONS 
A testing image database containing images from 50 
persons was used to test the performance. They were captured 
under three different illumination conditions. The experimental 
platform is a PC with an Intel P4 2.8GHz CPU. The artificial 
face models in the applications of physiognomy are illustrated 
in Fig. 5. For more detailed information about the face models 
in physiognomy, please visit the website [22]. 
 
 
(a) 
 
(b) 
 
(c) 
 
(d) 
Fig. 5. Face features of the face models in physiognomy 
 
The online fortune telling system based on face feature 
matching is shown in Fig. 6. Our demo system summarized the 
patterns in physiognomy according to some books and websites. 
The system first captures the face image of the user and extracts 
the corresponding facial features. After the matching process, it 
will retrieve the most similar artificial face model in the 
database and report the result of physiognomy for the 
corresponding human face. For the measuring the performance 
of the system, each matching result was examine with people’s 
vision. The precision rates for each single face feature are listed 
in Table I. Since there is much difference between the real face 
and an animated face, it is hard to improve the precision rate of 
the system. If the artificial face model can be modified to 
approach real human faces, the performance of the entire 
system will be more efficient than the current version. 
 
 
Fig. 6 . Face features of the face models in physiognomy 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/08/02
國科會補助計畫
計畫名稱: 區域式多層次解析之影像融合技術
計畫主持人: 蔡耀弘
計畫編號: 99-2221-E-364-003- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
