 
 
 
A New Face Recognition Method for Blurred 
Images Using Multi-Channel Wiener Filter  
Ching-Te Wang, Tung-Shou Chen, Chiu-Hsiung Liao, and Feng-Yung Chang  
 
  
Abstract—The techniques on face recognition are often 
recognizing a clear image currently. However, there will be not 
achieved successfully for unapparent face features, if the blurred 
images are needed to recognize the face area. For this reason, this 
paper will develop the face location and restoration techniques to 
recognize the blurred images. Firstly, the blurred image is 
transformed to a horizontal edge image by a masking operation. 
Then, the system detects the features of face region by using the 
integral image scheme. To determine whether the area is a face 
region or not, the system evaluates the characteristic values and 
compares with the threshold values. Afterward, the system 
performs the locating procedure and finds the face region from 
the blurred image. Furthermore, the recognition procedure 
executes the formal operations and restores a clear image from 
the blurred images by using the multi-channel Wiener filters. 
From the experimental results, the proposed method has a good 
performance on the success rate of recognizing face. 
 
Index Terms—Face location, face recognition, multi-channel 
Wiener filters  
I. INTRODUCTION 
 On technical development of face recognition, the computer 
techniques can speed up the searching time and process 
operations to recognize the face region instead of artificial 
looking for the correct image from the numerous materials. In 
recent years, the researchers had devoted to develop the face 
recognition and had several achievements. For examples, 
Clippingdale applied the method of Gabor Wavelets and the 
characteristics of people’s face to distinguish and track the face 
region by the microwave transform function [1]. Kurita et al. 
extract the features of face position and combine them into the 
valid values [2]. The system then can recognize the face 
according to the characteristic values. Turk and Pentland 
proposed the concept of projection from the image subspace to 
different face spaces, and transform the Eigenface to a linear 
representation [3]. Finally, the system can recognize face 
region by using the linear expression of the Eigenface. In these 
methods, the targets are all clear images. However, many 
images are blurred in real situations. For examples, the objects 
are captured in motion or incorrect focus, the images will tend 
to blur. In [4], they use discrete wavelet transform to detect the 
face region. The steps of the system are very complicated and 
the computation time is very huge. In [5], the system has a good 
performance on recognizing the face region in a clear image, 
but the success rate is affected on a blurred image. Thus, we 
will propose a method to recognize the blurred images instead 
of the clear images in those methods. To recognize the face 
region in a blurred image, we have to distinguish the 
characteristics of face area. Unfortunately, the features are 
often ambiguous. The success rate of recognition is influenced 
by some factors. In this paper, we will propose a face 
recognition system to recognize a person who is the target 
object in a blurred image. The proposed method reduces the 
affected factors of recognition and then detects the important 
characteristics of face region so as to distinguish the features of 
face identity. By the use of characteristic values, the face region 
can be recognized correctly. Afterward, the system restores the 
blurred image to a clear image by the technique of 
multi-channel Wiener filter [7] and then recognizes the image. 
From the experimental results, the system has a good 
performance in the face recognition and ensures that it can be 
applied to the practical applications.  
II. REVIEWS OF RELATED TECHNIQUES  
 A. Integral Image  
Manuscript received March 9, 2006.  This research is supported in part by 
the National Science Council, Taiwan, Repulic of China, under Grant NSC 
94-2213-E-167-010.  
The technique of integral image [6] uses the face features to 
look for the eye region. In integral image, each pixel value can 
be computed from a target image  as follows:  TSC. T. Wang is with Institute of Distribution and Technology Management, National Chin-Yi Institute of Technology, Taichung, Taiwan 411, R.O.C. 
(Phone: 886-4-23924505ext.2256; fax: 886-4-23929585; e-mail: 
ctwang@ncit.edu.tw).  ,
( , ) ( , )T
X X Y Y
ii X Y X YS
′ ′< <
′ ′= ∑ ,               (1) 
T. S. Chen is with Department of Information Management, National 
Taichung Institute of Technology, Taichung, Taiwan, R.O.C. (e-mail: 
tschen@ntit.edu.tw).  
where ii  is the value of the position (X, Y) in the integral 
image,  is the pixel value of the position ( '  in 
target image, which is marked operation of the horizontal edge. 
In Eq. (1), the pixel value ii  of integral image is the 
summation of total pixel values from original point (0, 0) to the 
( , )X Y
( ',TS X ')Y , ')X Y
( , )X Y
C. H. Liao is with General Education Center, National Chin-Yi Institute of 
Technology, Taichung, Taiwan 411, R.O.C. (e-mail: cliao@ncit.edu.tw).  
F. Y. Chang is with Department of Information Management, National 
Taichung Institute of Technology, Taichung, Taiwan, R.O.C. (e-mail: 
howame@yahoo.com.tw). 
2
 
 
 
The value S i  is the pixel value of point ( ,  in the 
image S, which is shown in Figure 3. Then the system detects 
and searches the eye region of the image S.  
( , )j )i j
 
 
 
 
 
 
 
 
 
 
B. 
C. 
Detecting Characteristic Block 
After obtaining the horizontal edge image, the system will 
search the characteristic values for the image. To extract the 
values, the proposed paper selects the eye area as the 
characteristic block, because the area is the most easily 
recognize in the face. Thus, the eye area is the search target in 
the scanning processes. Due to the left and right hand sides in 
the horizontal image may not be symmetrical. If applied single 
direction to search the characteristic blocks, the system may 
loss partial characteristic values in the searching process. 
Therefore, we will use bi-directional scanning method to search 
the characteristic block. The search method is described as 
follows. 
Step 1: [Scanning the horizontal image] The system uses the 
bi-directional searching method to scan the horizontal image 
S row by row. The scanning method is started from the 
left-hand and right-hand sides simultaneously. The left-hand 
side is scanned from the started point (0, 0) to the right, while 
the right-hand side is scanned from the started point (w-1, 0) 
to the left, where w is the width of the image. If one direction 
is scanned and found a pixel value be 255, which is a white 
pixel. This horizontal pixel may be included in the eye block. 
Thus, the system stops the scanning and determines whether 
or not the pixel belongs to the eye block. 
Step 2: [Selecting the initial point] The stopped point is used as 
the position (TopX, TopY) of upper left corner, which is 
called initial point, and selects the variable n=1. 
Step 3: [Computing the integral image] Compute the 
summation of all pixel values from the point (TopX, TopY) to 
the point (TopX+n, TopY+n) by using the technique of 
integral image [6]. The summation includes the pixel values 
in the extent of the rectangle. 
Step 4: [Determining the eye block] The system computes the 
summation. When the value does not change as comparing 
with the rectangle of n-1, the rectangle from point (TopX, 
TopY) to point (TopX+n, TopY+n) is determined the 
possible characteristic block of eyes. The value of n is the 
width of the region. Then, the system compares the 
characteristic values of face, if the result is not the face region, 
go to Step 1 and scan all pixels until to the end; otherwise, let 
n=n+1, go to Step 3. Because the fetched horizontal image in 
this step will be integrated to eye block. When the pixels of 
eye area are computed by the technique of integral image, the 
pixel value is always 255. Therefore, the summation of pixel 
values of rectangle is increased by the increment of n value. 
When the pixel value is zero and the summation is unchanged. 
That is, the pixel is not a part of eye block. 
horizontal edge image
Transform 
Original image 
Comparison of Face Characteristic Values 
After finding the possible eye block in the initial, the system 
will expand the discrimination to the face regions of their 
characteristic values and then determine whether or not this 
area is a face block. The proposed method will define the face 
characteristic values of discrimination and improve the 
matching scheme in [9]. Furthermore, we will propose three 
methods to extract the characteristic values. Figure 3. The horizontal edge transformation 
In Fig. 4, the pixel values are summed up and expressed 
as:VA ija= ∑  in the area A. It is the sum of pixel values of right 
eye. The area B is the middle of two eyes. These pixel values 
are summed up and expressed as VB . Similarly, the 
summation, VC
ijb= ∑
ijc= ∑ , is the total pixel values of left eye in the 
area C. The features are described as follows. 
B C
a        c       e      g
b       d        f      h
A        B     CA
Figure 4. The feature of eyes  
Feature 1: Let min( , ),MAC VA VC=  the areas of A, B and C have 
the phenomenon, which the value VA (or VC) in area A (or in 
area C) is greater than the value VB in area B, respectively. That 
is, MAC VB> . In other word, the pixel values of each eye are 
greater than the pixel values between two eyes in a horizontal 
edge image. This feature is called the eye characteristic. 
The width of eyes and their middle has a proportion. This 
property allows us to judge whether the block is an eye region 
or not. The system uses the point (TopX, TopY), which is 
found in previous section, as the basic point. The points (TopX, 
TopY) and (TopX+n, TopY+n) form to a rectangle, which is 
the region of right eye and enclosed by the points a, b, c and d 
as shown in Fig. 4. The value n is the width of the eye. Similarly, 
The points (TopX+2n, TopY) and (TopX+3n, TopY+n) also 
form to a rectangle, which is the left eye and enclosed by the 
points e, f, g and h as shown in Fig. 4. The middle region of two 
eyes is enclosed by the points (TopX+n, TopY) and (TopX+2n, 
TopY+n), which is bounded by the points c, d, e and f as shown 
in Fig. 4. Then, the system uses the technique of integral image 
to compute the summations of pixel values for the areas A, B, 
and C. By the use of Feature 1, the system checks that whether 
or not the summation of pixel values in area A or in area C is 
greater than the summation of pixel values in area B. If Feature 
1 is hold, the system considers it as the possible eye block and 
4
 
 
 
The variables r and s are the offsets and − ≤ , 
, where FS is the filter size of masking. The 
values  are evaluated by interchanging of each 
blurred image as shown in Fig. 7.  
,p r s p≤
2(2 1)FS p= +
[ ,
j iy y
R r ]s
, ]n s
E{ [ , ] [j iy m n y m ,r
−
⋅ −
,I J jy
j
iy
[ , ]
j iy y
R
2(2 1)p +
5 5×
C. 
iy
5
r s
[ , ]
j iy y
R r s
[ , ]
i
def
x yR r s =
[ , ]
{E
x m n ]m n
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The products of y m  and  are summed 
for all r s . Thus, the expected value can be 
computed and defined as follows:  
[ , ]j n [ , ]iy m r n s− −
, [ , ]p p∈ −
1y
2y
jy
1y
2y
iy
1 1
0 0
[ , ] [
]}
J I
j i
m n
y m n y m r
n s
Z
− −
= =
⋅ −
− =
∑ ∑
,   (6) 
where  are the width and high of image , respectively, 
Z is the number of pixels for the position in image y  
corresponding to the offset of point in image . All the 
expected values  of the values r and s are formed to a 
matrix , which is called the expected matrix. Because the 
values r and s are between –p and p, the combination of r and s 
has  variations. In the proposed system, the size of 
masking filter is , the value p is 2 and the size of expected 
matrix is . 
[ , ]
j iy y
R r s
5×
Computing Expected Values 
In order to find the relationship between the blurred images 
and images in database, the system selects an image from the 
database as the original image x. The original x and the blurred 
images  are interchanged by convolution operation. The 
computation formula of expected values 
ixy
R  is the same 
as  and defined as follows:  
[ , ] [ , ]ix m n y m r n s⋅ − − ,           (7) 
where  is the pixel values of position [ ,  in the 
image x and r, s are offsets. After evaluating all 
expected values, the expected matrix 
, [ ,r s p p∈ −  ]
ixy
R  can be obtained as 
the matrix 
j iy y
R . 
−
j
j i
N P
y y
u P= =−
,...,i N
0
1 2
4 5
7 8
r r
r
r r
1w w
4
5 2
2
' '
' '
r w
r w
w w
+ ⋅
⋅
+ ⋅ +
[ , ]m n
Sum =
D. Computing the Inverse Masking Filter 
In this section, we will use the expected matrices  and 
j iy y
R
ix y
R  to find the inverse masking function. Assume that ][ ,jf t u , 
0 j N 1≤ ≤
y
 are inverse masking functions corresponding the 
images , they are defined as follows: 
  
1
0 ,
[ , ] [ , ] [ ,
ij x y
j t
R r t s u f t u R r s
−
− − =∑ ∑ ],       (8) 
for 0,1 1= −  , ,p r s p− ≤ ≤ . If there are N blurred 
images and the size of masking filter is Q , they will generate 
N Q×  equations containing  unknown variables. To 
find the inverse masking functions , 0
N Q×
f ],u[tj 1−≤≤ Nj
3
0f
, we 
can solve united equations simultaneously. For example, let 
N=1, p=1, the size of inverse masking is 3 . That is, there is 
one blurred image and one inverse masking function . The 
size of the expected matrices 
×
0xy
R  and 
0 0y y
R  is also 3 3× . 
Assume the expected matrices 
0xy
R , 
0 0y y
R  and inverse masking 
function  are expressed as follows: f
… …
.
Figure 7. The relationship of the blurred images 
0
3
6
9
xy
r
R r r
r
  =    
,  
0 0
1 2 3
4 5 6
7 8 9
y y
r r r
R r r r
r r r
′ ′ ′  ′ ′ ′=  ′ ′ ′  
,  1 2 3
0 4 5 6
7 8 9
w w w
f w w w
w w w
  =    
. 
From Eq. (8), there are nine equations to solve nine unknown 
variables , ,…, . The united equations are shown as 
follows:  
2 9w
15 2 3 2 4 1 5 6 7 8 9
6 1 4 3 3 4 2 5 1 6 7 8 9 2
1 3 4 9 5 8 6 7 6 8 5 9 9
0 ' ' 0 0 0 0
' ' ' ' 0 0 0
0 0 0 0 ' ' 0 ' '
r w w r w r w w w w w r
r w r w r w r w r w w w w r
w w r w r w w r w r w r
1⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ = ⋅ + + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ = ⋅ ⋅ ⋅ ⋅ ⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ + ⋅ =
0f
After solving the united equations, the system obtains the 
inverse masking function , which will be used to recover a 
clear image from the blurred images. 
E. Computing the Restored Image 
In this section, we will discuss how to restore an image by 
using the inverse masking functions [ , ]if t u  and the blurred 
images . The restored image iy xˆ  can be calculated as follows: 
1
0 ,
ˆ { [ , ] [ , ]} /
pN
i i
i t u p
x f t u y m t n u Fsum
−
= = −
= ⋅ − −∑ ∑ ,  (9) 
where 
1
0 ,
[ , ]
pN
i
i t u p
F f t u
−
= =−
∑ ∑ . In the above equation, the 
numerator is calculated by the convolution of ][ ,if t u  and . 
The point [m, n] of the blurred image  is used as the central 
point, and its adjacent points [ ] are computed until 
all the images are executed completely. The numerator is the 
sum of the products of the inverse masking functions and the 
iy
iy
n ±,m t u±
6
