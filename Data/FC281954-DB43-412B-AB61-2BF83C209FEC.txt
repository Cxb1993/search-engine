detection-of-test suite’ and ’decision-detection-
of-test suite’ in large size program when severity 
was not taken into consideration, although AGA 
excelled in small size programs. In summary, while 
severity was taken into account, EAGA outperformed 
AGA and GA in test case prioritization problem. 
英文關鍵詞： Test case prioritization, code coverage, search 
algorithm, APFD, APFDc 
 
 1
應用嚴重性權重貪婪演算法於軟體測試案例排序之研究 
 
A Study of Applying Severity-Weighted Greedy Algorithm to 
Software Test Case Prioritization 
 
 
 
摘要 
在軟體測試中迴歸測試是一項實用的技術。迴歸測試包
含了三種主要的應用：測試個案的選擇、測試個案的最
小化以及測試個案的排序。本研究集中在探討測試個案
的排序問題上。傳統上，測試個案的排序有許多方法；
其中兩種較常用的方法是 Greedy 和 Additional Greedy
演算法。然而這兩種排序方法存在著一項弱點：當這兩
種方法在排序測試個案時，只考慮到偵測到的覆蓋率大
小，並不會考量到不同條件下的權重(例如：錯誤的嚴
重性)。因此，本研究提出一改良的 Additional Greedy 演
算法-Enhanced Additional Greedy Algorithm (EAGA)來做
測試案例的排序。在實驗中使用了八組程式為樣本來研
究在不同的排序方法下，以不同的排序準則及權重上的
調整在排序效能上的表現。研究的結果指出：當權重納
入考量的時候，EAGA 在每單位測試成本可偵測到的單
位權重上會有較佳的表現；當不考慮權重的時候，在較
大的程式中，EAGA 和 AGA 在平均程式分支偵測以及
平均錯誤偵測上的表現一樣好，而在較小的程式中，
AGA 在平均錯誤偵測上有較佳的表現。總的來說，當
權重被列入考量的時候，EAGA 在測試案例排序問題的
表現上較 AGA 和 GA 來的好。 
 
關鍵詞：軟體測試、迴歸測試、測試個案、錯誤嚴重
度。 
I. 計劃背景 
Software testing can be used to validate the correctness 
of software in development processes. According to system 
requirements, testers write test plans and design test cases 
based on the plans. Generally, testing types can be classified 
into three categories: construction testing, system testing, 
and special testing [1]. To begin with, construction testing 
tests codes of programs for execution correctness in the early 
stage of software development processes. Next, system 
testing is one kind of an integration testing that belongs to 
the later period of software development processes. System 
testing tests software from the perspective of a user. Special 
testing tests software based on particular purposes, for 
example, regression testing, performance testing, stress 
testing, and compatibility testing [2]. 
Traditionally, there are two approaches for software 
testing: white-box and black-box testing [2]. According to 
the characteristics of these two approaches, we can classify 
white-box testing as construction testing and black-box 
testing as system testing. Black-box testing tests whether 
functions or outputs of software meet expected results.  
White-box testing, also called structural testing, can be 
classified into two categories: data flow testing and control 
flow testing [2]. In white-box testing, testers need to know 
how the internals of software work. Thus, test cases need to 
be generated for testing processes to validate the correctness 
of a software execution result and evaluate the performances 
of testing techniques. How to generate and select test cases is 
an interesting issue in software testing. Test suite generation 
techniques can be classified into two categories: test case 
minimization [3], [4]and test case prioritization [5], [6]. In 
this study, we focus on test case prioritization problem that 
belongs to regression testing, also a kind of code coverage 
testing in white-box testing. 
The following sections of this study are as follows. 
Section II introduces test case prioritization problem while 
Section III describes techniques on test case prioritization. 
Section IV reports the experiment setup and result evaluation; 
Section V is an improvement to experiment for internal 
validity. Section XI is conclusions and future work. 
II. 研究目的 
Regression testing is a useful testing technique in 
software development processes. It assures a change in the 
software, like a bugfix, and can avoid introducing another 
bug. Three applications in regression testing are mostly used: 
regression test selection [7], [8], [9], [10], test suite 
minimization [3], [4], [11], [12], and test case prioritization 
[5], [6], [13], [14]. In this study, we focus on the test case 
prioritization problem due to two reasons. First, some 
companies prefer to re-execute a complete test suite rather 
than minimize the test suite in regression testing [15]. 
Second, test case prioritization performs better efficiency 
under some conditions, such as to finish a testing process in 
a limited time period [13], [16]. The test case prioritization 
problem is defined as follows: 
Definition 1. The Test Case Prioritization Problem [13]: 
Given: T, a test suite, PT, the set of permutations of T, 
and f, a function from PT to the real numbers. 
Problem: Find   such that . 
In this definition, PT is any possible ordering set of T, 
and f is the function such that any ordering from PT yields 
 3
two test cases had the same additional coverage 
information, AGA selected any of them into the ordered 
test suite first; thus, AGA selected either test case B or C 
after the selection of the test case A. Therefore, the 
ordered test suite in this example was A-B-C-D or A-C-
B-D. 
Table II. Test Suite and Additional Branch Coverage 
Branch 
Test Cases 1 2 3 4 5 6 7 8 
Additional 
Branch  
Covered 
A X X X   X X X - 
B X X X X     1 
C     X X X X 1 
D  X X   X X X 0 
 
c) Observation 
Yet, we notice a gap in AGA technique. Basically, 
AGA selects test cases based on the additional coverage 
information of each test case. That is, for example, AGA 
considers the number of branch coverage of each test 
case, but never the number of test cases covering each 
branch, where we are interested. Our technique is more 
appropriate when a requirement (including branches or 
faults) can only be covered by a test case, and when this 
requirement is important (e.g. at high risks or with high 
fault severity). By GA and AGA, instead, the 
requirement may not be in a prior order for testing. For 
example, in Table II, either test case B or C was the 
candidate test case after test case A was selected, owing 
to their additional coverage of 1. Specifically, in the 
ordered test suites generated by GA and AGA, test case 
B or C might not be the first to be tested, even if they 
were important. 
As shown in Table III, the numbers in the bottom row 
are the test case coverage information of each branch. 
Both test case A and B, for instance, covered branch one, 
so the coverage of branch one was 2, indicating a second 
priority for consideration of this branch. Thus, our 
technique proposes that requirements (e.g. branches or 
faults) with less test case coverage be testes at earlier 
stages. 
Table III. Test Case Coverage of Each Branch 
Branch Test Cases 1 2 3 4 5 6 7 8 
A X X X   X X X 
B X X X X     
C     X X X X 
D  X X   X X X 
TCR 2 3 3 1 1 3 3 3 
 
B. Proposed EAGA 
In our proposed EAGA technique, we define TCR as 
coverage of the numbers of test cases covering each 
requirement (ex. branch). When applying EAGA, we treated 
TCR as a weight. The lower a TCR was, the higher priority a 
requirement and its corresponding test case became. Take 
Table III for example. Branches 4 and 5 had a higher priority 
because both of them could only be detected by one test case, 
B and C respectively. The same as the AGA technique, 
EAGA has two test suite queues initially: an ordered test 
suite and a non-ordered test suite. EAGA also considers the 
additional coverage information in selecting processes. 
We further illustrate our technique by using an example. 
In Table IV, requirements 6, 7, and 9 were weighed 1, and 
the corresponding test cases T3 and T5 covering these 
requirements had the highest priority to be selected into the 
ordered test suite. When the weights of requirements were 
the same, EAGA considered the additional requirement 
coverage of these candidate test cases. The additional 
requirement coverage means those requirements yet covered 
by the selected test cases in the ordered test suite. In this 
example, test cases T3 and T5 had the additional requirement 
coverage of 6 and 4 respectively; thus, test case T3 was the 
first selected test case. After EAGA selected T3 into the 
ordered test suite, test case T5 was the next candidate test 
case because requirements 7 and 9 were weighed 1, and the 
corresponding test case was T5. At this stage, the ordered test 
suite was T3-T5, and all requirements were covered. When 
EAGA detected that all requirements were covered, it 
stopped selecting processes, and the remaining test cases 
were picked into the ordered test suite by sequence. Thus, the 
result of the ordered test suite in this example was T3-T5-T1-
T2-T4. 
Table IV. TCR Coverage Information Table 
Requirement Test 
Cases 1 2 3 4 5 6 7 8 9 10
T1 X  X        -
T2 X X X  X   X  X -
T3 X X X X X X     6
T4  X  X       -
T5       X X X X 4
TCR 3 3 3 2 2 1 1 2 1 2
 
IV. 實驗結果與討論  
We have implemented our proposed technique in Section 
III. In this section, we perform two experiments to analyze 
the performances of EAGA, AGA, and GA on test case 
prioritization. The subject programs—Siemens test suites [30] 
used in our experiment are developed in C language and 
provided by [31].  
A. Experiment Environment 
The details of Siemens programs are listed in Table IV. 
There were eight subject programs in our experiments. We 
further divided these programs into two groups: small and 
large groups based on the program size, aiming to analyze 
the performances of different prioritization techniques with 
different program sizes. Specifically, the first seven 
programs were categorized into the small group with their 
program sizes less than 1000. The eighth program was 
categorized into the large group with program size more than 
9000. 
 
 
 
 
 5
EAGA and AGA generally significantly outperformed 
that with GA. Performances between EAGA and AGA 
of each subject program had no significant differences 
except for programs tcas, replace, and space. We 
discuss the results of the small group programs first, 
followed by the large one. For tcas, APDC with AGA 
detected one more branch than that with EAGA, but the 
overall performances of these two techniques were the 
same. For replace, APDC with AGA had a minor lead 
on branch detection in the early stage, but the overall 
performances between EAGA and AGA were still the 
same. For the performances of techniques based on 
advanced branch based criterion (part (b) of Figure 4.1), 
APDC with EAGA and AGA significantly outperformed 
that with GA and MCDC. Moreover, APDC with 
MCDC had better performance than that with GA, 
especially on programs tcas, replace, and space. 
In summary, whether advanced branch criterion 
was taken into account, EAGA performed as well as 
AGA on APDC metric in small group programs. Next, 
we discuss the cause of the gap among different 
techniques on APDC metric in the large group program. 
As for the large group program space (bottom row 
of Figure 1), the result showed that APDC with AGA 
significantly outperformed those with other techniques. 
In this case, the test suite generated by EAGA did not 
achieve prospective performances. In order to 
understand the cause, we analyzed TCR of branch for 
space. The results had excluded branches that were not 
covered by any test case. We found that TCR of space 
was between 2400 and 2600; in other words, every 
branch in space was covered by 2500 test cases in 
average. If TCR of a program is uniformly distributed, 
the requirement severity-oriented property of EAGA 
fails to perform. Thus, AGA performed better than 
EAGA in this case. In case space, there was no 
difference among each requirement when EAGA 
prioritized test cases. Thus, the result demonstrated that 
APDC with AGA outperformed those with EAGA, GA, 
and MCDC. 
b) Case 2: Performance on APFD 
The second purpose of our experiment is to 
investigate the performances of different prioritization 
techniques based on different criteria. Thus, we analyzed 
APFD with EAGA, AGA, and GA based on fault, 
branch, and DU criterion for each subject program. The 
result demonstrated that test suites generated by EAGA, 
AGA, and GA based on fault criterion had better 
performances than those based on the other two criteria 
in most subject programs. Owing to the character of 
APFD that focuses on fault detection efficiency, test 
suites generated based on fault criterion achieve optimal 
measurement values. Thus, APFD of the test suites 
based on fault criterion in each subject program can be 
treated as an optimal index to those test suites based on 
the other two criteria: branch and DU. In Figures 2, 
techniques’ name with “BR” represent test suites 
generated by techniques based on branch—a control 
flow criterion. Names with “DU” represent test suites 
generated by techniques based on du—a data flow 
criterion. Test suites generated based on these two 
criteria were made an indirect measurement of APFD, 
compared with the optimal one based on fault criterion. 
APFD with GA based on fault criterion 
outperformed GA-BR and GA-DU in four of eight 
programs: tcas, replace, schedule, and schedule2. In 
addition, GA-BR had better performance than GA-DU 
in above-mentioned programs except for replace. APFD 
between GA-BR and GA-DU in replace had no 
significant differences. APFD with GA, GA-BR, and 
GA-DU had no significant differences in the rest four 
programs: totinfo, printtokens, printtokens2, and space. 
Compared to APFD with GA, APFD with AGA 
based on fault criterion outperformed AGA-BR and 
AGA-DU in all programs. To compare differences of 
APFD between AGA-BR and AGA-DU, there were no 
significant differences between them in four of eight 
programs: tcas, totinfo, schedule2, and space. However, 
there were significant differences between AGA-BR and 
AGA-DU in the rest four programs: replace, schedule, 
printtokens, and printtokens2. APFD with AGA-DU 
outperformed AGA-BR in half of forward four programs: 
replace and printtokens; instead, APFD with AGA-BR 
outperformed AGA-DU in the other two programs: 
schedule and printtokens. The same as the results of 
AGA, APFD with EAGA based on fault criterion 
outperformed EAGA-BR and EAGA-DU in all 
programs. Unlike the results of the comparisons between 
AGA-BR and AGA-DU, there were no significant 
differences between EAGA-BR and EAGA-DU in five 
of eight programs: tcas, totinfo, schedule2, printtokens, 
and space. However, APFD with EAGA-BR 
outperformed EAGA-DU in the remaining programs: 
replace, schedule, and printtokens. 
In summary, APFD with EAGA and AGA 
outperformed that with GA in subject programs. More 
specifically, techniques, especially EAGA, based on 
control flow criterion performed better than that based 
on data flow criterion. 
c) Case 3: Performance on APFDc 
The characteristic of our technique was severity 
oriented; thus, we want to investigate the performances 
of fault detection ability among different prioritization 
techniques while severity was taking into consideration. 
For simply discussion, in our experiment, we assume 
that severity of a fault was set to 10 under the condition 
that TCR was less than 2, and the rest was set to 1. For 
cost of each test case, we did not take it into 
consideration in our experiment; thus, we set it to 1. 
For comparisons, we performed paired-samples t-
tests to examine the differences between APFD and 
APFDc with different techniques: EAGA, AGA, and GA. 
 7
significant differences among GA, AGA, and EAGA in 
printtokens2. To validate the differences among GA, 
AGA, and EAGA, we performed Analysis of Variance 
(ANOVA). For hypopaper testing, the null hypopaper 
assumes that there is no significant difference in 
performances of different prioritization techniques on 
APFD, and the confidence interval is set to 95%. Table 
VI shows the results of one-way ANOVA, and it 
revealed that there were significant differences in the 
performances of different prioritization techniques on 
APFD in all programs except printtokens2. 
Performances of different prioritization techniques on 
printtokens2 had no significant differences. Thus, except 
for printtokens2, we reject the null hypopaper of no 
significant differences in performances of different 
prioritization techniques on APFD. For further analysis, 
a post hoc test using the Least Significant Difference 
(LSD) was used to examine pairwise comparisons 
differences. The results of tcas, totinfo, replace, 
schedule, and schedule2 revealed that two in three pairs 
had significant differences. In other words, APFD with 
AGA significantly outperformed those with EAGA and 
GA. The result of printtokens revealed that APFD with 
GA and AGA significantly outperformed APFD with 
EAGA, and there was no significant difference between 
GA and AGA. The result of printtokens2 revealed that 
there were no significant differences among APFD with 
GA, AGA, and EAGA. The result of space revealed that 
APFD with AGA significantly outperformed APFD with 
EAGA, and APFD with EAGA significantly 
outperformed APFD with GA. 
  
Figure 3. Boxplots of APFD for subject programs by GA, AGA, and EAGA 
 
Table VI. ANOVA Analysis for Subject Programs 
 9
on Software Engineering and Methodology, Vol. 2, No. 3, 
pp.270-285, Jul. 1993. 
[13] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold, 
“Prioritizing Test Cases For Regression Testing,” IEEE 
Transactions on Software Engineering, Vol. 27, No. 10, pp. 
929-948, Oct. 2001. 
[14] W. E. Wong, J. R. Horgan, S. London, and H. Agrawal, “A 
Study of Effective Regression Testing in Practice,” In 
Proceeding of the 8th International Symposium on Software 
Reliability Engineering, pp. 230-238, New Mexico, USA, 
Nov. 1997. 
[15] K. Onoma, W. T. Tsai, M. Poonawala, and H. Suganuma, 
“Regression Testing in an Industrial Environment,” 
Communications of the ACM, Vol. 41, No. 5, pp. 81-86, 
May 1988. 
[16] P. R. Srivastava, “Test Case Prioritization,” Journal of 
Theoretical and Applied Information Technology, Vol. 4, 
No. 3, pp. 178-181, Mar. 2008. 
[17] H. Do and G. Rothermel, “On the Use of Mutation Faults in 
Empirical Assessments of Test Case Prioritization 
Techniques,” IEEE Transactions on Software Engineering, 
Vol. 32, No. 9, pp. 733-752, Sep. 2006. 
[18] A. Srivastava and J. Thiagarajan, “Effectively prioritizing 
tests in development environment,” In Proceeding of the 
International Symposium on Software Testing and Analysis, 
pp. 97-106, Rome, Italy, Jul. 2002. 
[19] D. Leon and A. Podgueski, “A Comparison of Coverage-
Based and Distribution-Based Techniques for Filtering and 
Prioritizing Test Cases,” In Proceeding of the International 
Symposium on Software Reliability Engineering, pp. 442-
453, Denver, Colorado, USA, Nov. 17-20, 2003. 
[20] H. Do, G. Rothermel, and A. Kinneer, “Prioritizing JUnit 
Test Cases: An Empirical Assessment and Cost-Benefits 
Analysis,” Empirical Software Engineering: An 
International Journal, Vol. 11, No. 1, pp. 33-70, Mar. 2006. 
[21] S. Elbaum, P. Kallakuri, A. Malishevsky, G. Rothermel, and 
S. Kanduri, “Understanding the Effects of Changes on the 
Cost-Effectiveness of Regression Testing Techniques,” 
Journal of Software Testing, Verification and Reliability, 
Vol. 13, No. 2, pp. 65-83, Jun. 2003. 
[22] S. Elbaum, A. G. Malishevsky, and G. Rothermel, “Test 
Case Prioritization: A Family of Empirical Studies.” IEEE 
Transactions on Software Engineering, Vol. 28, No. 2, pp. 
159-182, Feb. 2002. 
[23] S. Elbaum, A. Malishevsky, and G. Rothermel, 
“Incorporating Varying Test Costs and Fault Severities into 
Test Case Prioritization,” In Proceeding of the 23th 
International Conference of Software Engineering, pp. 329-
338, Toronto, Canada, May 12-19, 2001. 
[24] R. Santelices, J. A. Jones, Y. Yu, and M. J. Harrold, 
“Lightweight Fault-Localization Using Multiple Coverage 
Types,” In Proceeding of the 31th International Conference 
of Software Engineering, pp. 56-66, Vancouver, Canada, 
May 16-24, 2009. 
[25] Z. Li, M. Harman, and R. M. Hierons, “Search Algorithms 
for Regression Test Case Prioritization,” IEEE Transactions 
on Software Engineering, Vol. 33, No. 4, pp. 225-237, Apr. 
2007. 
[26] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand, 
“Experiments on the Effectiveness of Dataflow- and 
Controlflow-Based Test Adequacy Criteria,” In Proceeding 
of the 16th International Conference of Software 
Engineering, pp.191-200, Sorrento, Italy, May 16-21, 1994. 
[27] K. J. Hayhurst, D. S. Veerhusen, J. J. Chilenski, and L. K. 
Rierson, “A Practical Tutorial on Modified 
Condition/Decision Coverage,” Report MASA/TM-2001-
210876, NASA, 2001. 
[28] K. J. Hayhurst, D. S. Veerhusen, J. J. Chilenski, and L. K. 
Rierson, “A Practical Approach to Modified 
Condition/Decision Coverage,” In Proceeding of the 20th 
Digital Avionics Systems Conference, NASA Langley 
Technical Report Server, pp. 1B2/1-1B2/10, Daytona Beach, 
Florida, USA, 2001. 
[29] J. A. Jones and M. J. Harrold, “Test-Suite Reduction and 
Prioritization for Modified Condition/Decision Coverage,” 
IEEE Transactions on Software Engineering, Vol. 33, No. 4, 
pp. 195-209, Mar. 2003. 
[30] H. Do, S. Elbaum, and G. Rothermel, "Supporting 
Controlled Experimentation with Testing Techniques: An 
Infrastructure and its Potential Impact," Empirical Software 
Engineering: An International Journal, Vol. 10, No. 4, pp. 
405-435, Jul. 2005. 
[31] “The Software-artifact Infrastructure Repository,” Available: 
http://sir.unl.edu/portal/, 15 Feb. 2008. 
[32] J. J. Chilenski, S. P. Miller, “Applicability of Modified 
Condition/Decision Coverage to Software Testing,” 
Software Engineering Journal, Vol. 9, No. 5, pp. 193-200, 
Sep. 1994. 
[33] J. Badlaney, R. Ghatol, and R. Jadhwani, “An Introduction 
to Data-Flow Testing,” Technical Report NCSU CSC TR-
2006-22, University of North Carolina State, Aug. 2006. 
[34] A. G. Malishevsky, J. R. Ruthruff, G. Rothermel, and S. 
Elbaum, “Cost-cognizant Test Case Prioritization,” 
Technical Report TR-UNL-CSE-2006-0004, University of 
Nebraska-Lincoln, Mar. 2006. 
[35] M. J. Rummel, G. M. Kapfhammer, and A. Thall, “Towards 
the Prioritization of Regression Test Suites with Data Flow 
Information,” In Proceedings of the 20th Symposium on 
Applied Computing, pp. 1499-1504, Santa Fe, New Mexico, 
USA, Mar. 13-17, 2005. 
 
 
 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
■ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱 高等軟體測試技術研究：理
論、設計、與實現 
計 畫 編 號
97-2221-E-007-052-MY3 
報 告 人 
姓 名 黃 慶 育  
服 務 機 構
及 職 稱
國立清華大學資訊工程學系
副教授 
會議/訪問時間 
 地點 2010.12.7~2010.12.9  地點: 中國．澳門 
會 議 名 稱 2010 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM 2010)   
發表論文題目 
1. A Study of Applying the Bounded Generalized Pareto Function to the 
Analysis of Software Fault Distribution 
2. An Investigation into Whether the NHPP Framework Is Suitable For 
Software Reliability Prediction and Estimation 
 
一、 主要任務摘要  
個人今年在 IEEE工業工程與工程管理國際研討會(International Conference on Industrial 
Engineering and Engineering Management，IEEM 2010)國際會議一共發表２篇論文，而
今年IEEM 2010的論文錄取率為40%(400/1200)，其中的一篇論文：「A Study of Applying 
the Bounded Generalized Pareto Function to the Analysis of Software Fault Distribution」甫
獲得大會最佳論文佳作獎(Best Paper Award, Honorable Mention)。 
 
二、對計畫之效益（一百字以內） 
工業工程與工程管理國際研討會(IEEM)係IEEE下屬之國際會議（新加坡），為工業工程
領域之高級別國際學術會議，尤甚Best Paper Award部份更採雙盲審方式。本次會議吸
引了超過50個國家/區域的研究人員廣泛參與，參加工業工程與工程管理國際研討會使
我們得以吸收到最新的系統開發觀念與技巧。 
 
三、經過 
※ 第一天(2010/12/7) 
台灣時間上午10:00搭乘長榮航空BR801班機前往澳門，一下飛機後隨即搭車前往會議
所在地－威尼斯人渡假村酒店(The Venetian Macao-Resort-Hotel)辦理報到手續，當場被
工作人員告知我的論文獲得最佳論文佳作獎。 
 
□ 赴國外出差或研習 
□ 赴大陸地區出差或研習 
■ 出席國際學術會議 
□ 國際合作研究計畫出國 
心得報告 
計 畫 名 稱 高等軟體測試技術研究：理
論、設計、與實現 
計 畫 編 號
97-2221-E-007-052-MY3 
報 告 人 
姓 名 黃慶育  
服 務 機 構
及 職 稱
國立清華大學資訊工程學系
副教授 
會議/訪問時間 
 地點 2010.7.18~2010.7.23  地點: 韓國．首爾 
會 議 名 稱 The 34th Annual IEEE International Computer Software and Applications Conference (COMPSAC 2010) 
發表論文題目 
1. A Study on the Applicability of Modified Genetic Algorithms for the 
Parameter Estimation of Software Reliability Modeling 
2. A Study of Cost-Cognizant Test Case Prioritization Using Genetic 
Algorithm with Test History 
3. A Study of Improving the Accuracy of Software Effort Estimation 
Using Linearly Weighted Combinations 
 
一、 主要任務摘要  
個人及所指導的清大資工系博士班許兆榕同學、與資應所博士班張君儒同學在今年
的第34屆IEEE國際電腦軟體及應用會議共發表３篇論文，而今年COMPSAC 2010的正
式長篇論文(regular full paper)錄取率只有20%。 
 
二、對計畫之效益（一百字以內） 
COMPSAC是IEEE計算機學會的旗艦會議，從1977年在美國芝加哥召開第一屆會議開
始，到今年已經是第三十四屆。本次會議吸引了許可國際上知名企業及學校的軟體開
發與研究人員的廣泛參與，共同就軟體品質、軟體開發生命週期、軟體測試、軟體安
全等問題開展了深入的研討。參加COMPSAC 2010會議使我們得以吸收到最新的軟體
開發與測試的技巧，可大大地幫助我們提昇相關軟體開發及測試之可靠度、妥用率及
工作量預估精確度，更使我們的更進一步地了解我們所開發出來的軟體的品質、改進
現有軟體測試的方法與測試程序。 
 
三、經過 
※ 第一天(2010/7/18) 
台灣時間中午12:50搭乘泰航TG634班機前往韓國首爾，一下飛機後隨即搭車前往下榻
飯店，到達飯店時已是韓國當地時間晚間八點。 
Fault-prone Module Detection Approaches: Complexity Metrics and Text Feature Metrics
（by Osamu Mizuno, Hideaki Hata），係由來自Kyoto Institute of Technology的水野修
(Osamu Mizuno)副教授報告，水野修教授與本人則為多年朋友，相見甚歡。 
 
  
 
   
 
※ 第四天(2010/7/21) 
今天聆聽了二篇論文報告：Reasoning about Human Intention Change for Individualized 
Runtime Software Service Evolution（by Hua Ming, Carl K. Chang, Katsunori Oyama, Hen-I 
Yang）與A Consistency Model for Identity Information in Distributed Systems（by Thorsten 
Hoellrigl, Jochen Dinger, Hannes Hartenstein）。會後我則與來自愛荷華州立大學資訊系的
張可昭教授(Professor Carl K. Chang，IEEE Fellow與AAAS Fellow)有著單獨面對面溝
通，張可昭教授與本人為舊識。 
 
※ 第五天(2010/7/22) 
今天於13:30-15:00時段我們則發表了二篇重要論文：A Study of Cost-Cognizant Test Case 
Prioritization Using Genetic Algorithm with Test History (by Yu-Chi Huang, Chin-Yu 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/12/10
國科會補助計畫
計畫名稱: 高等軟體測試技術研究：理論、設計、與實現
計畫主持人: 黃慶育
計畫編號: 97-2221-E-007-052-MY3 學門領域: 程式語言與軟體工程
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
個人於 2010 年在 IEEE 工業工程與工程管理國際研討會(International 
Conference on Industrial Engineering and Engineering Management，IEEM 
2010)國際會議一共發表２篇論文，而今年 IEEM 2010 的論文錄取率為
40%(400/1200)，其中的一篇論文：「A Study of Applying the Bounded 
Generalized Pareto Function to the Analysis of Software Fault 
Distribution」獲得大會最佳論文佳作獎(Best Paper Award, Honorable 
Mention)。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
