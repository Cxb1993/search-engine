  
行政院國家科學委員會補助專題研究計畫 
■成果報告   
□期中進度報告 
 
基於 Kernel Method與空間資訊之監督式、非監督式及半監督式高
光譜遙測影像分類(2/3)(3/3)  
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC 99-2221-E-142-002 
執行期間： 99 年 08 月 01 日至 100 年 07 月 31 日 
 
執行機構及系所：國立臺中教育大學教育測驗統計研究所 
 
計畫主持人：郭伯臣 
計畫參與人員：黃孝雲 輔仁大學統計資訊學系/助理教授 
何省華 國立中興大學電機工程學系/博士生               
李政軒 國立交通大學電控工程研究所/博士生 
              陳怡伶 國立臺中教育大學教育測驗統計研究所/碩士生 
              朱慧珊 國立臺中教育大學教育測驗統計研究所/碩士生 
              張偉民 國立臺中教育大學教育測驗統計研究所/碩士生 
              余韋任 國立臺中教育大學教育測驗統計研究所/碩士生 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告   
 
 
 
 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國  一 百  年  十  月  三 十 一  日
 3 
 
capability. Trying to solve this optimal problem directly with inequality constraints is 
generally difficult. However, the original optimal problem has an equivalent dual 
representation using the Lagrange optimization. The corresponding dual Lagrange 
function is defined as:  
α
max
 

 

n
i
n
j
j
T
ijiji
n
i
i yy
1 11
)x()x(
2
1

 (2) 
Subject to niCy i
n
i
ii ,,2,1,0,0
1



 
where the artificial variables, i , are Lagrange multipliers, and 
T
n ],,[α 1   . 
The kernel trick uses a kernel function RRR dd :  to implicitly map the 
data from the original space dR  to H  without knowing the feature mapping  . 
The inner product of samples in the feature space can be computed directly from the 
original data items using a kernel function. This is because a kernel function   
satisfies the Mercer’s theorem [7], i.e., there is a feature map   into a Hilbert space 
H  such that )z()x()z,x(  T , where dRz,x , if and only if   is a symmetric 
function for which the matrices njijiK  ,1)]x,x([  formed by restriction to any 
finite subset }x,,x{ 1 n  of the space 
dR  are positive semi-definite. Hence, Eq. (2) 
can be written as the following Eq. (3) by applying the kernel trick. Since, for a kernel 
function, the corresponding kernel matrix is positive semi-definite for all training sets, 
this in turn means that the optimization problem of (3) is always convex. 
α
max  
 

n
i
n
j
jijiji
n
i
i yy
1 1
,
1
)xx(
2
1

 (3) 
Subject to niCy i
n
i
ii ,,2,1,0,0
1


  
Once the i ’s are determined, the decision function for an unlabeled pattern x  
is defined as 
byf
n
i
iii 
1
SVM )x,x()x(  , 
where b  is chosen so that 1))x,x((
1


byy
n
i
jiiij   for any jx  with 
Cj 0 , and a corresponding forecasting label is ))x(sgn( SVMf . 
2. Dynamic subspace method 
Dynamic subspace method (DSM) proposed by [8] aims at overcoming two 
drawbacks of RSM that how to choose a suitable subspace dimensionality for the 
employed classifier and another is on how to appropriately select the advantageous 
features from other features for classification. In DSM, there are two major 
distributions added on the process of random feature selection in RSM to build the 
process of dynamic features selection (DFS) as shown in the red part of Figure 1. That 
is, for constructing component classifiers with adaptive subspaces to adjust the 
shortcomings of RSM, DSM works on the basis of W and R distributions that denote 
the distributions of feature weights and subspace dimensionality respectively. Figure 2 
illustrates a whole framework of DSM with using the SVM as the base classifier. 
During the process of DFS, the amount of desirability features, rb+j, is 
automatically determined based on the Rj-1 distribution, and the component features to 
 5 
 
R distribution by using base classifier to estimate the resubstitution accuracy in each 
subspace. Especially, the corresponding time of training SVM is much more than 
training other type of base classifiers in order to get the efficient effect by using the 
five-fold cross validation and the gird search to find the best parameters in SVM. 
Therefore, the DSM needs to spend much more time to exchange the enough accuracy 
when using the SVM as the base classifier. 
三. 研究方法 
1. A Spatial-Contextual Support Vector Machine for Remotely Sensed Image 
Classification (SCSVM) 
In SCSVM [6], spatial information exploits the semi-labels for the pixels belonging 
to the neighborhood system in the original space of each sample from the preceding 
discriminated process by applying standard SVM to overcome similar spectral 
properties. SCSVM can have good generalization, especially, for the pixels with 
similar spectral attributes but located in different regions. This approach decreases 
speckle-like errors and significantly improves classification performance. Let 
},,1|x{x Mjij
O
i   represent a neighborhood system of the pixel ix  in the 
original space, where M  is 4 or 8 to represent that Oix  is a first-order or 
second-order neighborhood system, respectively (Figure 3). 
  1x i    1x i  2x i  3x i   
 4x i  ix  2x i   8x i  ix  4x i   
  3x i    7x i  6x i  5x i   
 
 
 
Fig. 3.  The left and right images above represent the first-order and second-order 
neighborhood systems in the original space, respectively.  
After performing the standard SVM, SCSVM can obtain the semi-labels of ix , 
which is equal to Oix  and denote them as 
M
jijy 1}{  , i.e., 
Mjnify ijij ,,1,,,1)),x(sgn( SVM   . The constrained minimization problem 
associated to SCSVMs by taking into account the semi-labels of the whole image is 
defined as follows: 
ξw,
min  


n
i
i
T C
1
ww
2
1
  
(6) 
Subject to 
ni
mmby
i
iiii
T
i
,,2,1,0
,1)))x()x(()x(w(

 


 
where ),0[   is a nonnegative parameter that controls the effect of 
spatial-contextural information. )x( im
  and )x( im
  are the number of pixels in the 
neighbor system ix  which belongs to class +1 and class -1, respectively. 
The SCSVM cost function does not need to be modified and still maintains the 
property of convex property. Since the objective function of the minimization 
problem of the SCSVM only contains training samples without samples with 
 7 
 
1),,(1   Ji xx , nji ,,1,  , i.e., the cosine value of two training samples 
ix  and jx  in the feature space can be computed by ),,(  ji xx  and it determines 
the similarity between these two samples. 
Based on the above two observations and the concepts, two properties are 
desired and described as follows: (1) The samples in the same class should be mapped 
into the same area in the feature space and (2) the samples in the different classes 
should be mapped into the different areas. We want to find a proper parameter   
such that 
(1) Lii ,,1,,if,1),,(  zxzx   and 
(2) .,,if,1),,( jiii  zxzx   
In this paper, two criteria are proposed for measuring these properties. First one 
is the mean of values applied by the normal kernel function on the samples in the 
same class: 

    

L
i
L
i i
1
1
2
i i
),,(
1
)(
x z
zx  , 
where 
i  is the number of training samples in class i . The parameter   should 
be determined such that )(  closes to 1. Second one is the mean of values applied 
by the normalized kernel function on the samples in the different classes: 
.),,(
1
)(
1 1
1
1

  

  
 
 

L
i
L
ij
j
L
i
L
ij
j ji i j
b
x z
zx   
So   should be determined also such that )(b  closes to -1. Hence, the optimal 
  can be obtained by solving the following optimization problem: 
).()(2))(1())(1()(min 

bbJ   
Note that if ),,(  zx  is differentiable, e.g., the based kernel is RBF kernel 
function, with respect to  , the gradient descent method, 
,2,1,0),(1  nJ nnnnn   
is used to solve the proposed optimization problem, where 
)()()( nnn bJ 









 , 
and n  is the step size at the n-th iteration. 
Otherwise, if the parameter   is discrete, e.g., the based kernel is polynomial 
kernel, then we can find the best *  such that 
 sJ ,,2,1)(minarg*  

 
where s  is an integer and should be pre-determined. 
Tables 1 and 2 are the overall and kappa accuracies in Indian Pine Site dataset by 
applying the RBF kernel and the polynomial kernel as the based kernel functions, 
respectively. One can find that the cost of time for proposed method is much less than 
the 5-fold cross-validation on both two datasets. Moreover, the classification results 
 9 
 
(KDSM, Ni = 40 ), and 97.43% (KDSM, Ni = 300 ), respectively. In terms of 
computer time as were displayed in Table 4, it’s intuitive and necessary that the costs 
of classification by applying any ensemble are more than using single classifier. 
However, it is not difficult to discover that the KDSM save us a lot of time that 
achieved 12 times at least and even reached 68 times compared with DSM. 
Table 4. The average classification accuracy ± standard deviation and average computer time 
of ten test data in Washington DC Mall dataset 
四. 計畫成果自評 
本計畫研究成果豐碩，共發表 3 篇期刊論文（含接受），其中兩篇刊於「IEEE 
Transactions on Geoscience and Remote Sensing」。1 篇被「Journal of Information 
Science and Engineering」接受。100 年 6 月份時，本團隊亦於「2011 IEEE 
nternational Conference on Fuzzy Systems」會中發表 1 篇論文。100 年 7 月份時於
「International Geoscience and Remote Sensing Symposium 2011」會中發表 3 篇論
文。本團隊的研發成果詳列如下： 
[1] Cheng-Hsuan Li, Bor-Chen Kuo, Chin-Teng Lin, and Chih-Sheng Huang, “A 
Spatial–Contextual Support Vector Machine for Remotely Sensed Image 
Classification,” IEEE Transactions on Geoscience and Remote Sensing, 2011. 
(SCI and EI; Impact Factor=2.470; Accepted) 
[2] Cheng-Hsuan Li, Hsin-Hua Ho, Yu-Lung Liu, Chin-Teng Lin, Bor-Chen Kuo, and 
Jin-Shiuh Taur, “An Automatic Method for Selecting the Parameter of the 
Normalized Kernel Function to Support Vector Machines,” Journal of 
Information Science and Engineering, 2011. (SCIE and EI; Impact Factor=0.265; 
Accepted)  
[3] Hsiao-Yun Huang and Bor-Chen Kuo, “Double Nearest Proportion Feature 
Extraction for Hyperspectral Image Classification,” IEEE Transactions on 
Geoscience and Remote Sensing, Vol.48, No.11, pp. 4034 – 4046, Nov. 2010. 
(SCI and EI; Impact Factor=2.234) 
[4] I-Ling Chen, Cheng-Hsuan Li, Bor-Chen Kuo, and Chih-Cheng Hung, 
“Combining Ensemble Technique of Support Vector Machines with the Optimal 
Kernel Method for Hyperspectral Image Classification,” 2011 IEEE International 
Geoscience and Remote Sensing Symposium (IGARSS 2011), 2011. (EI) 
[5] Kai-Chih Pai, I-Ling Chen, Bor-Chen Kuo, and Cheng-Hsuan Li, “An Automatic 
Method to Determine the Coefficient of the Composite Kernel for Hyperspectral 
Method 
Accuracy (%) CPU Time (sec) 
case 1 case 2 case 3 case 1 case 2 case 3 
SVM_CV 83.66±1.98 86.39±1.31 94.69±0.61 30.35 116.02 5858.18 
SVM_OP 83.79±1.07 87.89±0.73 95.31±0.50 3.10 6.65 376.99 
DSM_WACC 85.49±1.54 88.74±1.32 95.94±0.70 6045.31 21113.75 1165048.6 
DSM_WLDA 86.83±3.06 90.76±2.38 96.94±1.18 2902.00 6767.77 220121.62 
KDSM 88.64±1.74 92.53±1.27 97.43±0.66 155.31 308.26 17847.7 
 11 
 
Vector Machine for Remotely Sensed Image Classification,” IEEE Transactions 
on Geoscience and Remote Sensing, 2011. 
[7]  S.T. John and C. Nello, Kernel Methods for Pattern Analysis. Cambridge 
University Press, 2004. 
[8]  J.-M. Yang, B.-C. Kuo, P.-T. Yu, and C.-H. Chuang, “A Dynamic Subspace 
Method for Hyperspectral Image Classification,” IEEE Transaction on 
Geosciences and Remote Sensing, Vol. 48, No. 7, pp. 2840-2853, 2010. 
[9] K. Fukunaga, Introduction to Statistical Pattern Recognition. (2nd ed.). San Diego, 
CA: Academic, 1990. 
[10] C.-C. Chang, and C.-J. Lin, LIBSVM: A Library for Support Vector Machines, 
2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm 
[11] H. L. Xiong, M. N. S. Swamy, M. Omair, and Ahmad, “Optimizing the kernel in 
the empirical feature space, ” IEEE Trans. Neural Networks, Vol. 16, No. 2, pp.  
460–474, 2005. 
[12] C.-H. Li, H.-H. Ho, Y.-L. Liu, C.-T. Lin, B.-C. Kuo, and J.-S. Taur, “An 
Automatic Method for Selecting the Parameter of the Normalized Kernel 
Function to Support Vector Machines,” Journal of Information Science and 
Engineering, 2011. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：郭伯臣 計畫編號：99-2221-E-142-002- 
計畫名稱：基於 Kernel Method 與空間資訊之監督式、非監督式及半監督式高光譜遙測影像分類
(2/3)(3/3) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備註（質化說明：
如數個計畫共同
成果、成果列為
該期刊之封面故
事...等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
Hui-Shan Chu, 
Cheng-Hsuan Li, 
and Bor-Chen 
Kuo, ’Linear 
Discriminate 
Analysis based on 
both Spectral and 
Spatial 
Information for 
Hyperspectral 
Image 
Classification,’ 
International 
Conference on 
Advanced 
Information 
Technologies, 
April 22-23, 2011, 
Taichung County, 
Taiwan. 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 1 4 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
國外 論文著作 期刊論文 3 3 100% 篇 [1] Cheng-Hsuan 
Li, Bor-Chen Kuo, 
Chin-Teng Lin, and 
Chih-Sheng 
Huang, ’ A 
Spatial –
4034 – 4046, 
Nov. 2010. (SCI 
and EI ； Impact 
Factor=2.234) 
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% [1] I-Ling Chen, 
Cheng-Hsuan Li, 
Bor-Chen Kuo, and 
Chih-Cheng 
Hung, ’Combining 
Ensemble 
Technique of 
Support Vector 
Machines with the 
Optimal Kernel 
Method for 
Hyperspectral 
Image 
Classification,’ 
2011 IEEE 
International 
Geoscience and 
Remote Sensing 
Symposium (IGARSS 
2011), 2011. (EI)
[2] Kai-Chih Pai, 
I-Ling Chen, 
Bor-Chen Kuo, and 
Cheng-Hsuan 
Li, ’An Automatic 
M th d t
