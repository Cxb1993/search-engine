 2
A Class Possibility Based Kernel to Increase Classification Accuracy 
for Small Data Sets Using Support Vector Machines 
1. Introduction 
Recently, researchers in the area of pattern recognition have given more attention to kernel-based 
learning algorithms such as support vector machines (SVM), first introduced by Vapnik et al. [9][10]. 
Mathematically, SVM is a linear classifier in the parameter space, and it uses kernels to extend data 
into a high-dimensional feature space to improve the classification performance. Hence, the 
performance of an SVM is highly dependent on the selection of an appropriate kernel. The current 
widely used kernels are all functional kernels for general purposes, such as polynomial kernel, 
Gaussian kernel, and sigmoid kernel. Generally the kernels are computed on the distance or algebraic 
operators of all paired points.  
 
2. Literature Review 
2.1 Review of kernel function 
We first give the definition of Mercer kernels: Let { }1 2, , , NX = x x xL  be the data set where 
( )1 2, , , Mi i i iMx x x= ∈x L R , 1, ,i N= L . A function :k X X× → R is called a positive 
semidefinite kernel (or Mercer kernel) if and only if k is symmetric (i.e. ( , ) ( , )i j j ik k=x x x x ) and 
the following equation holds: 
1 1
( , ) 0 2
p p
i j i j
i j
b b k p
= =
≥ ∀ ≥∑∑ x x ,    where qb ∈R 1, ,q p∀ = L . 
Each Mercer kernel can be expressed as ( , ') ( ), ( ')k φ φ=x x x x , : X Fφ →  is a mapping of 
feature selection and ,⋅ ⋅  is the inner product.  
In the following we give some examples of Mercer kernels: 
For the case ( )φ =x x , k is defined by 
1
( , ') '
M
i i
i
k x x
=
=∑x x is a linear kernel, where 
( )1 2, , , MMx x x= ∈x L R  and ( )1 2' ' , ' , , ' MMx x x= ∈x L R . It is symmetric, and the 
positive semidefinite result is followed by the simple computation: For any 2p ≥ , 
1 2, , ,
M
N ∈x x xL R , and 1 2, , , pb b b ∈L R : 
2
1 1 1
( , ) || || 0
p p p
i j i j i i
i j i
b b k b
= = =
= ≥∑∑ ∑x x x . 
Moreover, commonly used kernel functions are polynomial kernels, radial-basis function kernels, 
and two-layer perceptron kernels [11]: 
 4
analyze the relationship of data on a single attribute for reasons of simplification.  
2
3
4
5
1
23
45
6
7
0
0.5
1
1.5
2
2.5
petal length
sepal width
pe
ta
l w
id
th
 
0 0.5 1 1.5 2 2.5
1
2
3
4
5
6
7
petal width
pe
ta
l l
en
gt
h
 
0 0.5 1 1.5 2 2.5
2
2.5
3
3.5
4
4.5
petal width
se
pa
l w
id
th
 
2 2.5 3 3.5 4 4.5
1
2
3
4
5
6
7
sepal width
pe
ta
l l
en
gt
h
 
Fig. 2 (a) The distribution of the three classes in the iris data set using three attributes. (b), (c), and (d) The distribution of the 
three classes in the iris data set using two attributes. (The iris data set is downloaded from the UCI Repository of machine 
learning database http://archive.ics.uci.edu/ml/ ) 
 
3.2 Class possibility function of an attribute 
   Given a two-class training data set { }1 2, , , NX = x x xL , where 
1 2( , , , )
M
i i i iMx x x= ∈x L R , 1, ,i N= L means that the set has N elements, and each datum has 
M attribute values. Let the set { }1 2, , , NT t t t= L  represent data classes, where { }1, 1it ∈ − + are 
the class values of xi, 1, ,i N= L . 
Considering the data set { }1 2, , , MY = y y yL  is the set of attributes for training data set X. 
The element of Y, { }1 2, , ,j j j Njx x x=y L , means the values of the jth attribute of data set X. In this 
section, we will discuss the similarity of a paired element on attribute 1, 1y , in every class. The 
calculation of similarity for other pairs of elements in other attributes, 2 , , My yL , can be done using 
in the same procedure. 
We divide the elements of 1y  into two sets (or classes), C1 and C2, by the target value 
, 1, ,it i N= L , where 11 2C C∪ = y  and 1 2C C φ∩ = . 
(a) (b) 
(c) (d) 
 6
high-dimensional representation [7][8]. However, not every mapping of pairs of objects to “similarity 
values” is a valid kernel [7]. In this paper, we call 1 C2and Ck k  class possibility based kernels.  
The degree of  point 1x∈y  in C1 can be determined using the following equation : 
  1 [ , ) [ , ]( ) ( ) ( )C a c c b
x a b xx x x
c a b c
μ χ χ− −⎛ ⎞ ⎛ ⎞= +⎜ ⎟ ⎜ ⎟− −⎝ ⎠ ⎝ ⎠   
where [ , ) ( )a c xχ and [ , ) ( )c b xχ  are the characteristic functions defined as  
[ , )
1     [ , ) 
( )
0    otherwisea c
x a c
xχ ∈⎧= ⎨⎩  and [ , )
1     [ , ] 
( )
0    otherwisec b
x c b
xχ ∈⎧= ⎨⎩  
By the extension principle [5], we define the class possibility based kernel in C1 as follows: 
   Given 1, 'x x ∈y , the class possibility function in C1 is 1 1: [0,1]Cμ →y , then the class 
possibility based kernel in C1 is:  
1 1 1( , ') ( ) ( ')C C Ck x x x xμ μ= ⋅  
There are four different cases when we compute the similarity of the pair x and 'x  by the triangle 
membership function.  
Case 1: When the two points x and 'x are both on the left-side of the triangle, the similarity of x 
and 'x  becomes: 
        I [ , ) [ , )
'( , ') ( ) ( ')a c a c
x a x ak x x x x
c a c a
χ χ− −⎛ ⎞⎛ ⎞= ⎜ ⎟⎜ ⎟− −⎝ ⎠⎝ ⎠  
  Case 2: When the two points x and 'x  are both on the right-side of the triangle, the similarity of x 
and 'x  becomes: 
        II [ , ] [ , ]
'( , ') ( ) ( ')c b c b
b x b xk x x x x
b c b c
χ χ− −⎛ ⎞⎛ ⎞= ⎜ ⎟⎜ ⎟− −⎝ ⎠⎝ ⎠  
  Case 3: When the two points x and 'x  are on either sides of the triangle ( 'x in the right and x in 
the left), the similarity of x and 'x  becomes: 
        III [ , ) [ , ]
'( , ') ( ) ( ')a c c b
x a b xk x x x x
c a b c
χ χ− −⎛ ⎞⎛ ⎞= ⎜ ⎟⎜ ⎟− −⎝ ⎠⎝ ⎠  
  Case 4: When the two points x and 'x  are on either sides of triangle (x in the right and 'x in the 
left), the similarity of x and 'x  becomes: 
IV [ , ] [ , )
'( , ') ( ) ( ')c b a c
b x x ak x x x x
b c c a
χ χ− −⎛ ⎞⎛ ⎞= ⎜ ⎟⎜ ⎟− −⎝ ⎠⎝ ⎠  
After considering all conditions, we define the class possibility based kernel for any two x and 
 8
   The echocardiogram data consists of 132 data, with 12 attributes and two classes. However, 
there are 57 data with missing target values, and seven data have more than two unknown attributes. 
Hence, we delete the total of 64 data from the 132 original data and used the mean as the missing 
value of each attribute. The number of training data is 5, 10, 20, and 30, and the number of testing 
data is 20. We randomly select the training and testing data to conduct each experiment for 30 times 
and compute the average accuracies. The results for the echocardiogram data are summarized in 
Table 1, showing that the kernel we proposed has better classification performance than the other 
two kernels.  
 
4.2.2 The Wisconsin diagnostic breast cancer data 
The Wisconsin diagnostic breast cancer data consists of 569 data with 30 attributes, and it is a 
two-class data set. However, since 30 attributes can generate a very large size of numbers for the 
kernel, we use principle component analysis (PCA) to reduce the attribute dimensions. Finally, we 
choose ten attributes to be transformed by PCA. The number of training data is 5, 10, 20, 30, 50 and 
100, and the number of testing data is 400. Similar to the echocardiogram data case, we conduct each 
experiment 30 times and compute the average accuracies. The results for echocardiogram data are 
summarized in Table 2, showing that the kernel we proposed also has better classification performance 
than the other two kernels.  
 
4.3 The BUPA liver disorders data 
   The BUPA liver disorders data consists of 345 data with six attributes and it is a two-class data 
set. The number of training data is 5, 10, 20, 30, 50, and 100, and the number of testing data is 200. We 
conduct each experiment 30 times and compute the average accuracies. The results for echocardiogram 
 10
2. Li, D. C., Lin,Y. S. 2006. Using virtual sample generation to build up management knowledge in 
the early manufacturing stages. European Journal of Operational Research, 175 413-434. 
3. Mercer, J., 1909. Functions of positive and negative type, and their connection with the theory of 
integral equations, Transactions of the London Philosophical Society (A). 209, 415-446. 
4.   Georgej, K., 2002. Fuzzy Sets And Fuzzy Logic - theory and applications. Prentice Hall 
5.   Didier D., Henri. P., 1980. Fuzzy Sets and Systems Theory and Applications. New York, 
London, Toronto. 
6.  Bazaraa M. S., Jarvis J. J. and Sherali H. D. 1990., Linear Programming and Network Flows. 2ed. 
Wiley 
7.  Nathan, S., 2007 How Good is a Kernel When Used a Similarity Measure? Proceedings 20th 
Annual Conference on Learning Theory,  
8. Balcan, M. F., Glum, A., 2006. On a theory of learning with similarity function. In Proceedings of 
the 23rd International Conference on Machine Learning.  
9. Cortes, C., and Vapnik, V., 1995, Support Vector Networks, Machine Learning, 20, 273-297. 
10. Shawkat, A., Kate, A., Smith, M., 2006, A meta-learning approach to Automatic Kernel Selection 
for Support Vector Machines. Neurocomputing, 70, 173-186. 
11. Boser, B., Guyon, I. And Vapnik, V., A Training Algorithm for Optimal Margin Classifiers. Fifth 
Annual Workshop on Computational Learning Theory. New York: ACM Press.  
12. Aizerman, M. A., Braverman, E. M. and Rozoner, L. I. 1964. The probability problem of pattern 
recognition learning and the method of potential functions. Automation and Remote Control. 
25,1175-1190. 
 
計畫成果自評 
    本研究發展了一個專屬於小樣本資料分析的模糊基底的核函數來提升小樣
本的資料分類正確率. 該方法為一項全新創新的數學方法並且這樣的核函數可
廣泛的應用在各項分類需要的領域中,如: 影像識別與醫療診斷等相關領域. 因
此本研究成果不僅受到國際期刊SCI的肯定 其相對的應用價值 也是十分廣大
的. 
98年度專題研究計畫研究成果彙整表 
計畫主持人：利德江 計畫編號：98-2221-E-006-101- 
計畫名稱：以類別模糊核函數為基底的支撐向量機來提升小樣本資料的分類正確率 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 4 0 100% 4 個博士級兼任研究員 
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
