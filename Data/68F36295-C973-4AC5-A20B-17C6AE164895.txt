  
行政院國家科學委員會補助專題研究計畫 
□成果報告   
■期中進度報告 
 
支援 Cloud-aware嵌入式行動多核心平台--子計畫三：整合嵌入式
系統與雲端計算的音樂與語音服務 
 
計畫類別：□個別型計畫   ■整合型計畫 
計畫編號：NSC 100－2219－E－007－008 
執行期間：2011年 5 月 1 日至 2013年 4 月 30 日 
 
執行機構及系所：清華大學資訊工程系 
 
計畫主持人：張智星 
共同主持人： 
計畫參與人員：葉子雋、任佳民、吳福海、邱莉婷、陳杰興、彭郁雅 
 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
□出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國   101 年  3  月  17  日 
II 
 
摘要： 
1. 中文摘要： 
隨著無線網路以及行動裝置播放音樂功能的普及，使音樂與語音服務成為行動加值的基
本應用之一，而提供服務的形式亦日趨多元，在語音方面，不管是利用行動裝置搜尋網路上
的關鍵字或是編寫手機簡訊，人們希望可以利用語音輸入取代行動裝置的按鍵輸入，對於這
項應用，語音辨識的速度及準確性則至關重要；在音樂方面，與實體音樂 CD相較，人們更
希望能隨時隨地找到最想聽到的音樂。因此，我們希望建構一個有效率的系統，可以在行動
裝置上透過各種檢索方式搜尋上萬首的歌曲資料庫，找到使用者想聽的音樂，再加上如節奏、
曲風等特徵的輔助，推薦使用者可能會喜歡的曲目，以滿足使用者個人喜好。 
本計畫擬利用行動多核心平台發展音樂與語音相關之應用服務，使用者可透過各種方式
搜尋音樂，如藉由哼唱音樂的片段、語音輸入歌名、情緒或敲擊節奏進行搜尋。首先由手持
式裝置對輸入的音訊進行前端處理以取得音訊特徵，之後利用雲端技術在伺服端進行資料庫
比對，同時搜尋相近曲風的歌曲推薦給使用者，最後將所有資訊回送至用戶端並呈現給使用
者。此架構避免了直接在前端裝置上進行比對的龐大運算量以及資料庫的儲存空間，因此可
以適用於大部分的手持式裝置，此外，有鑑於抽取某些音訊特徵的運算量對於手持式裝置的
負擔仍太重，因此利用多核心系統進行平行處理，預期可大幅改善計算所需的時間，加速整
體處理的速度，增進應用服務的價值。研究的工作項目如下： 
 
1. 建立多重特徵辨識的音樂搜尋系統 
2. 蒐集做為辨識用途的音樂及語料 
3. 實作個人化音樂推薦系統 
4. 整合系統並實作於多核心嵌入式平台 
5. 測試及評估應用服務於雲端運算的效能 
 
關鍵詞：語音辨識、哼唱檢索、敲擊檢索、曲風分類、雲端運算、平行處理 
 
2. 英文摘要： 
Due to the rapid advances of wireless networks and embedded mobile devices, more and more 
applications and services have emerged for people’s daily needs. Music service is one of them that 
have been receiving more and more attention. In particular, people want to be able to search music 
in an efficient way, Moreover people want to find songs that are similar to their favorite ones. As a 
result, the goal of this project is to develop an integrated music service system that fulfill common 
people’s needs, with an emphasis on the synergism between the client-side embedded devices and 
the server-side cloud computing. Specific tasks include: 
 
1 
 
一、 前言與研究目的 
音樂搜尋及其應用服務一直是一個充滿開發潛力的領域，相對於傳統文字的搜尋而言，使
用者可以使用各種方法搜尋到想聽的歌曲，像是哼唱或敲擊某個旋律的片斷、語音輸入歌曲名
稱，即可搜尋到想要的歌曲，是一個較直覺也較人性化的方法。在哼唱檢索方面，系統必須追
蹤使用者哼唱的旋律的音高，並與資料庫內歌曲的音高輪廓進行比對。此外，藉由敲擊音符或
節拍的起始位置，可以描述歌曲的部分旋律特徵，並利用這些特徵搜尋資料庫。當然，系統也
允許直接用語音輸入關鍵字進行搜尋，此部分則需要語音辨識的相關技術。本計畫將規劃整合
以上技術，製作一個多重特徵辨識的音樂搜尋系統，以符合使用者的各種需求，其相關應用列
表如下： 
 
1. 與視聽系統結合，如 KTV的點歌系統，使操作的過程更為簡便且人性化。 
2. 將音樂搜尋技術濃縮成一個晶片，便可裝置在互動式的玩偶上，玩偶即可辨認使用者的
聲音進而產生不同的反應。 
3. 音樂搜尋具有分辨音樂的能力，因而也可將音樂搜尋技術包裝成音樂學習、教唱或評分
的軟體。 
 
二、 研究方法 
在本期中報告內，研究方法主要包含三個部份，分別為 CASA (Computational auditory scene 
analysis，計算聽覺場景分析)的改進，QBSH (Query by singing/humming，哼唱搜尋)在雲端
系統上的實作，及使用 GPU (Graphic Processing Unit，圖形處理器)平行處理哼唱搜尋的演
算法，以加速哼唱搜尋的速度，以下將分別就此三部份進行介紹。 
 
1. CASA 的改進 
 
在本期中，我們發展了一個混成法改進了 CASA的辨識率，並將此研究結果投稿至 ICASSP 
(International Conference on Acoustics, Speech, and Signal Processing) 獲得接受，
以下是論文中的方法摘錄： 
 
In this work, we propose a hybrid method for singing pitch extraction from polyphonic audio 
music. We have observed several kinds of pitch errors made by a previously proposed algorithm 
based on trend estimation. We also noticed that other pitch tracking methods tend to have other 
types of pitch error. Then it becomes intuitive to combine the results of several pitch trackers to 
achieve a better accuracy. In this paper, we adopt 3 methods as a committee to determine the pitch, 
including the trend-estimation-based method for forward and backward signals, and training-based 
HMM method. Experimental results demonstrate that the proposed approach outperforms the best 
algorithm for the task of audio melody extraction in MIREX 2010. 
In the proposed hybrid method, trend estimation plays an important role in determining 
3 
 
pitch errors are likely to happen at the beginning of a music phrase. If we reverse the input signals 
in time axis and send it for pitch tracking, the pitch errors occur elsewhere. As a result, forward 
and backward (in time) signals are likely to generate complementary results. This observation 
motivates us to combines pitch contours obtained from forward and backward signals to achieve a 
better accuracy. 
 
 
Fig. 3.1.3 Illustrations of a typical pitch error and its recovery. (a) The ground-truth pitch contour, 
and the one generated by the forward trend-estimation-based method.  (b) The result of merging 
pitch contours generated by 3 different methods, which can recover from the pitch error displayed in 
(a). 
 
HMM-based methods have been widely used for audio melody extraction. In this study, we 
use the maximum 2 values of the NSHS (normalized sub-harmonics summation) map and their 
locations (frequencies) as the features for HMM. Fig. 3.1.4(a) illustrates the scatter plot of the first 
two dimensions of the features, including the maximum value of a frame’s NSHS and the 
corresponding index. Fig. 3.1.4(b) shows the corresponding NSHS curves for (a). As can be 
observed in the Fig. 3.1.4, the max values and the corresponding indexes are mostly likely to be 
around the state bin(indicated as the middle bar in both Fig. 3.1.4(a) and Fig. 3.1.4(b)). Let the 
feature vector set be denoted as V = {v0, …, vt, …}, our target is to find the most likely state 
sequence S = {S0, …, St, …}: 
5 
 
 
Fig. 3.1.5 Evaluation results of different pitch combination methods 
 
After obtaining multiple pitch contours, we need to combine them in an optimal sense. We can 
arrange these pitch contours into a matrix C, where each row is a pitch contour and each column is 
possible semitones of a frame. Here we adopt 3 different methods to derive the final pitch contour: 
1. Median method: The optimum pitch contour P𝑚𝑒𝑑𝑖𝑎𝑛(𝑗) at frame 𝑗 is expressed as: 
 
P𝑚𝑒𝑑𝑖𝑎𝑛(𝑗) = {𝑚𝑒𝑑𝑖𝑎𝑛
𝑖
(𝐶𝑖,𝑗), 𝑗 = 1~𝑀},             (2) 
           
where 𝑀 is the number of frames. 
2. Mean method: The optimum pitch contour P𝑚𝑒𝑎𝑛(𝑗) at frame j is expressed as: 
 
P𝑚𝑒𝑎𝑛(𝑗) = {𝑚𝑒𝑎𝑛
𝑖
(𝐶𝑖,𝑗), 𝑗 = 1~𝑀}                  (3) 
 
3. DP-based method: The recurrence function of this method can be formed as follows: 
 
𝐷(𝑖, 𝑗) = 𝐶(𝑖, 𝑗) − min (𝐶𝑗−1 − 𝐷𝑗−1)                 (4) 
𝐷(𝑖, 1) = 0, ∀𝑖 = 1~𝑁,                            (5) 
 
where 𝑁 is the number of pitch contour candidates. We can easily obtain the optimal path P𝑑𝑝 
by the backtracking, which is a common method in dynamic programming. 
The evaluation results of the above methods are illustrated in Fig. 3.1.5. As can be easily 
observed, the result of median method is the best among all 3 methods. Thus, the median method 
will be adopted for the rest of the evaluations presented below.  
The dataset for our evaluation is MIR-1k, which contains 1000 pop-song singing mixture with 
leading vocals and music accompanies. A 5-fold singer-specific cross validation is then employed 
to obtain the average raw-pitch accuracy for the proposed method. 
Fig. 3.1.6 shows the evaluation results of raw-pitch accuracy with a pitch tolerance of 0.5 
semitones. The proposed method outperforms Hsu’s trend-estimation-based method (the best 
7 
 
 
圖 3.2.1 client 端的系統架構 
 
 Server端的伺服器則是透過網頁服務來回應使用者的旋律搜尋需求，當網頁服務接收
到了使用者哼唱的音高後，會將音高送至主伺服器，主伺服器會分配歌曲資料庫的辨識範圍
交給 GPU進行運算，下圖顯示的是系統後端的運作流程。 
 
 
圖 3.2.2 Server 端的系統架構 
 
使用者在哼唱選歌的流程中主要會經過四個步驟，如圖 3.2.3。在 Client端必須取得
使用者哼唱的旋律進而得到旋律之音高，取得音高的方式是使用由 Lawrencer R.Rabiner所
提出的 ACF (Autocorrelation Function)，而後透過 Web service 將音高資料送到伺服器
去做辨識的複雜運算，最後把最符合使用者哼唱的歌曲辨識結果送回給使用者。 
 
9 
 
 
圖 3.2.5 使用者在哼唱選歌時錄音的使用介面 
 
   在 GPU實作方面，相關細節僅在下一節討論，在此暫不論述。 
 
3. 以 GPU 平行處理哼唱搜尋的演算法 
 
呈上所述，對本計畫而言很重要的另一個成果即為 GPU平行處理演算法，這部份能使我
們原本使用的演算法之計算時間大大降低，此成果亦投稿至 ICASSP，並獲得接受，以下摘
錄論文片段來解釋此部份的實作成果： 
 
In this work, we develop a QBSH system which starts the comparison from anywhere in a 
song where notes begin, and the schemes of parallelization are directly designed for searching 
many songs in the database. The proposed method is used to improve a web-deployed QBSH 
system called MIRACLE which won the championship of 2011 CUDA programming contest in 
Taiwan, hosted by NVIDIA. 
We use linear scaling as the comparison method in our work. Linear scaling is a simple yet 
effective method for query by singing/humming. Since the keys and tempos may be different 
between the input pitch vector and the songs in the database, we need to transpose the key and 
scale the tempo of the input vector. Key transposition can be simply handled by shifting the mean 
values of pitch vectors of query input and the intended songs in the database to the same value. 
Usually we shift one of them to match another when comparing these two vectors. For tempo 
scaling, since tempo variation is usually linear, we can apply LS (linear scaling) to the input pitch 
vector for comparison. Assuming that the input pitch vector has a duration of 𝑑 seconds, then we 
need to compress or stretch the vector to obtain 𝑟 versions of the original vector, with durations 
equally spaced between 𝑠𝑚𝑖𝑛 × 𝑑  and 𝑠𝑚𝑎𝑥 × 𝑑 , where 𝑠𝑚𝑖𝑛  (<1) and 𝑠𝑚𝑎𝑥  (>1) are the 
minimum and maximum of the scaling factor, respectively. The distance between the input pitch 
vector and a particular song is then the minimum of the 𝑟 distances between the 𝑟 vectors and 
the song, as shown in figure 3.3.1 where we compress/stretch the 𝑑-second vector to obtain 5 
vectors with lengths equally spaced between 0.5 × 𝑑 and 1.5 × 𝑑. The best result is obtained 
when the scaling factor is 1.25. 
11 
 
optimum parallelization, as shown next. 
 
 In the first scheme, we simply launch 𝑁 threads for comparing  𝑁 different songs in 
the database. The current song segment for comparison is copied to local storages for 
speeding up the computing. 
 
 In the second scheme, we launch 𝑟 threads for one song. These 𝑟 threads are grouped 
into a block, with 𝑁 blocks in total. However, though the degree of parallelization is higher, 
the computation time is even longer. The reason is that there are many blocks but only a few 
of threads within one block – the SPs are not fully utilized. 
 
 In the third scheme, we still have 𝑁 blocks for 𝑁 songs, but now each block has 𝑘 
threads in a block. The computation tasks starting at different notes in a song are equally 
distributed to the 𝑘 threads. Since 𝑘 is usually larger than 𝑟, the utilization of SPs is better 
than that of scheme 2. Moreover, since there are multiple threads for one song, we obtain the 
minimum distance between the input pitch vector and this song in parallel by using these 
threads.  
 
After obtaining the distance between the query input and each of the songs in the database, we 
then sort all the distances on CPU to obtain the top-n list. We did try the sorting using GPU, but 
the performance is not satisfactory due to excessive access time over the global memory. 
We used the public corpus MIR-QBSH for our experiments of QBSH with GPU. Note that in 
this corpus, the anchor positions for all queries are from the beginning of a song. In order to test 
the accuracy of "anchor anywhere", we duplicate the last one fourth of each song and prepend it to 
the beginning of the song. The corpus contains 6197 clips which correspond to 48 children's songs. 
To increase the complexity of the comparison, we added 12887 noise songs (which correspond to 
pop songs in the past decades) to the database, such that the number of songs in the database is 
12935. Figure 3.3.3 shows the distribution of song lengths, in terms of number of music notes and 
number of pitch points, respectively. This plot indicates the complexity of our task. The number of 
music notes indicates how many positions we need to start the comparison algorithm, while the 
number of pitch points represents how long the sequence we need to run through the comparison 
algorithm. 
 
13 
 
 
Figure 3.3.5 The computation time per query with respect to the number of songs in the database 
for the three different schemes of parallelization. The number of threads in a block is 1024 for 
schemes 1 and 3, and 31 for scheme 2. 
 
 
 
 
 
Figure 3.3.6 The computation time per query with respect to the number of songs in the database 
for the three different schemes of parallelization. The number of threads in a block is 128 for 
schemes 1 and 3, and 31 for scheme 2. 
 
The above figures suggest that number of threads in a block is an important factor for scheme 
3. Thus we investigated the effect of number of threads in a block for scheme 3, as shown in figure 
3.3.7. The best performance is achieved when there are 128 threads in a block. Utilization of cores 
in GPU will be lower if we launch fewer threads in a block. On the other hand, if we have more 
threads than 128, then it becomes time consuming to compute the minimum distance for a song 
which is obtained from these threads. The snapshot of our system is shown in figure 3.3.8.  
 
15 
 
singing melody transcription system. Li et al. in proposed a method that first filtering and selecting 
channels from the spectrogram. Peaks in these selected channels are then extracted as the feature to 
train HMMs, where the transition probabilities are estimated from the pre-labeled training data. Both 
studies focus on the contextual relationship between neighboring music notes. However, the 
concurrent pitches produced by music accompany or chord, which may critically influence the overall 
vocal pitch estimation, were not dealt with in details. To address this problem, Hsu et al. introduced a 
method based on trend estimation, where the singing voices were enhanced first and the concurrent 
pitches were eliminated by vibrato and tremolo features. After that, pitch range of each frame was 
identified by finding a coarse path (pitch trend) within high-magnitude T-F (time-frequency) blocks. 
The use of such a pitch trend can remove the undesirable concurrent pitches produced by accompanied 
music. 
 
另一些可參考的文獻如下列表： 
 
1. C. K.Wang, R. Y. Lyu, and Y. C. Chiang, “An automatic singing transcription system with 
multilingual singing lyric recognizer and robust melody tracker,” in Proc. 8th European Conf. on 
Speech Communication and Technology, Geneva, Switzerland, 2003. 
2. T. Zhang, “System and method for automatic singer identification,” in Proc. IEEE International 
Conference on Multimedia and Expo (ICME), pp. 33-36, 2003. 
3. D. Yang and W. Lee. “Disambiguating music emotion using software agents,” In Proc. Symp. 
Music Inf. Retrieval (ISMIR’04), Barcelona, Spain, pp. 52-57, 2004. 
4. S. Vembu and S. Baumann, “Separation of vocals from polyphonic audio recordings,” in Proc. Int. 
Symp. Music Inf. Retrieval (ISMIR’05), pp. 337–344, 2005. 
5. T. Virtanen, A. Mesaros, and M. Ryynänen, "Combining Pitch-Based Inference and Non-Negative 
Spectrogram Factorization in Separating vocals from polyphonic music," in Proc. ISCA Tutorial 
and Research Workshop on Statistical and Perceptual Audition (SAPA'08), Brisbane, Australia, 
September 2008. 
6. A. Ozerov, P. Philippe, F. Bimbot and R. Gribonval, "Adaptation of Bayesian models for single 
channel source separation and its application to voice / music separation in popular songs," IEEE 
Trans. on Audio, Speech and Lang. Proc., special issue on Blind Signal Proc. for Speech and Audio 
Applications, vol. 15, no. 5, pp. 1564-1578, 2007. 
7. B. Raj, Smaragdis, P., Shashanka, M.V. and Singh, R., “Separating a Foreground Singer from 
Background Music.”, Intl Symposium on Frontiers of Research on Speech and Music (FRSM), 
Mysore, India, Jan 2007. 
8. G. J. Brown and D. L. Wang, “Timing is of the essence: Neural oscillator models of auditory 
grouping,” in Listening to Speech: An Auditory Perspective, S. Greenberg and W. Ainsworth Ed, 
Lawrence Erlbaum, Mahwah NJ, 2006. pp. 375-392. 
9. G. Hu and D.L. Wang, “An auditory scene analysis approach to monaural speech segregation,” in 
Acoustic Echo and Noise Control, E. Hansler and G. Schmidt Ed., Heidelberg: Springer, 2006, pp. 
485-515. 
10. D.L. Wang, "On ideal binary mask as the computational goal of auditory scene analysis," in Speech 
17 
 
A similar study of accelerating QBSH on GPUs is proposed by Ferrao et al. However, the 
comparison only started from the beginning of a song in the database, which may violate people’s 
singing habits. In this paper, comparison starts from anywhere in a song where notes begin, and the 
schemes of parallelization are directly designed for searching many sequences (i.e. many songs in the 
database). The proposed method is used to improve a web-deployed QBSH system called MIRACLE 
which won the championship of 2011 CUDA programming contest in Taiwan, hosted by NVIDIA. 
 
關於 QBSH其他可以參考的文獻如下： 
 
1. A. Ghias, J. Logan, D. Chamberlain, B. C. Smith, “Query by humming-musical information 
retrieval in an audio database”, ACM Multimedia ’95 San Francisco, 1995. 
2. Rodger J. McNab, Lloyd A. Smith, Jan H. Witten, “Towards the Digital Music Library: Tune 
Retrieval from Acoustic Input” ACM, 1996. 
3. Rodger J. McNab, Lloyd A. Smith, Jan H. Witten, “Signal Processing for Melody Transcription” 
Proceedings of the 19th Australasian Computer Science Conference, 1996. 
4. Rodger J. McNab, Lloyd A. Smith, “Melody transcription for interactive applications” Department 
of Computer Science University of Waikato, New Zealand. 
5. Yianilos, Peter N. “Data structures and algorithms for nearest neighbor search in general metric 
spaces,” In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, 
pages 311-321, Austin, Texas, 25-27 January 1993 
6. J.-S. Roger Jang and Chuen-Tsai Sun and Eiji Mizutani, "Neuro-Fuzzy and Soft Computing: A 
Computational Approach to Learning and Machine Intelligence", MATLAB Curriculum Series, 
Upper Saddle River, NJ, Prentice Hall, 1997. (ISBN: 0-13-261066-3) 
7. Jyh-Shing Roger Jang, Hong-Ru Lee, Jiang-Chuen Chen, and Cheng-Yuan Lin, "Research and 
Development of an MIR Engine with Multi-modal Interface for Real-world Applications", Journal 
of the American Society for Information Science and Technology, 2003. 
8. J.-S. Roger Jang, and Yung-Sen Jang, "On the Implementation of Melody Recognition on 8-bit 
AND 16-bit Microcontrollers", The Fourth IEEE Pacific-Rim Conference on Multimedia, 
Singapore, December 2003. 
9. J.-S. Roger Jang and Shiuan-Sung Lin, "Optimization of Viterbi Beam Search in Speech 
Recognition", International Symposium on Chinese Spoken Language Processing, Taiwan, August 
2002. 
10. J.-S. Roger Jang, Hong-Ru Lee, "Hierarchical Filtering Method for Content-based Music Retrieval 
via Acoustic Input", The 9th ACM Multimedia Conference (Oral paper, acceptance rate 16%), PP. 
401-410, Ottawa, Ontario, Canada, September 2001. 
11. J.-S. Roger Jang, Jiang-Chun Chen, Ming-Yang Kao, "MIRACLE: A Music Information Retrieval 
System with Clustered Computing Engines", 2nd Annual International Symposium on Music 
Information Retrieval 2001, Indiana University, Bloomington, Indiana, USA, October 2001. 
12. J.-S. Roger Jang, Hong-Ru Lee, Ming-Yang Kao, "Content-based Music Retrieval Using Linear 
Scaling and Branch-and-bound Tree Search", IEEE International Conference on Multimedia and 
Expo, Waseda University, Tokyo, Japan, August 2001. 
19 
 
 
 任意處比對的辨識率及辨識時間比較： 
 
辨識方法 移調方法 Pitch scale Top-10 辨識率 辨識時間(秒) 
2-Stage LS Median 10 77.08% 6.82 
2-Stage LS Median 1024 77.83% 6.91 
2-Stage LS Mean 10 75.09% 2.45 
2-Stage LS Mean 1024 76.13% 2.73 
2-Stage LS 
optimized 
Median 10 77.08% 4.87 
2-Stage LS 
optimized 
Median 1024 77.83% 4.84 
2-Stage LS 
optimized 
MedianByVote 10 77.08% 3.16 
2-Stage LS 
optimized 
Mean 10 75.09% 1.65 
2-Stage LS 
optimized 
Mean 1024 76.13% 1.69 
表 4.1 辨識率及時間列表 
 
 使用 SEV (Sorted error vector)於 LS任意處比對辨識率及辨識時間比較： 
 未使用 SEV的辨識率：77.08% 
 當 sevBound設為 0.08，使用 median 移調時的辨識率：80.28%（辨識率增加 3.2%） 
 
21 
 
 
圖 4.2 使用 SEV 在不同之距離計算法下之計算時間比較 
 
 第二部份的測試環境與語料介紹如下： 
 測試環境： 
Windows 7 Enterprise, 64-bit 
Intel® Core™ i7-2600 Processor (8M Cache, 3.40 GHz) 
16GB Main Memory 
Geforce GTX 560 Ti (384 core, 1024MB RAM GDDR5), compute Capability 2.1 
CUDA Toolkit 4.0 for Windows 
NVIDIA Driver 270.81 
 測試語料： 
  總歌曲數：6,197首 
  由 264人所錄製的兒歌哼唱片段 
 哼唱取樣規格： 
  錄音長度：8秒 
  單聲道 
  取樣頻率：16,000 
  解析度：16bits 
 資料庫： 
  總歌曲數：12,935 首 
 
 從頭比對的辨識率比較： 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
2011 台灣 CUDA 程式設計比賽冠軍 
2011 電信創新應用大賽（行動應用 創新遊戲類 校園組）亞軍 
2012 第二屆美律電聲論文獎金質獎 
 
 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
