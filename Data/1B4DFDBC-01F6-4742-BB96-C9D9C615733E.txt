 1
行政院國家科學委員會補助專題研究計畫成果報告 
基於核心機器的多種新型機器學習方法 
Novel Machine Learning Methods based on Kernal Machines 
計畫編號： NSC 98-2221-E-022-009 
執行期限： 98 年 08 月 01 日至 99 年 10 月 31 日 
主持人：黃淇竣副教授  國立高雄海洋科技大學資訊管理系 
計畫參與人員：蔡瑞鴻  國立高雄海洋科技大學資訊管理系 
張峻豪  國立高雄海洋科技大學資訊管理系 
楊順仁  國立高雄海洋科技大學資訊管理系 
葉羽安  國立高雄海洋科技大學資訊管理系 
曹慧慈  國立高雄海洋科技大學資訊管理系 
林奕均  國立高雄海洋科技大學資訊管理系 
王香婷  國立高雄海洋科技大學資訊管理系 
 
一、中文摘要 
 
本研究計畫之主要研究目的為提出適
用於超大型資料集、基於核心機器(Kernal 
Machines)的多種新型機器學習方法。該新
型機器學習方法將具備數項優點，包括可
有效選取大型資料集或超大型資料集中的
訓練樣本(Training Intsnaces)，之後所形成
之新的、精簡(Pruned)的訓練集合(Training 
Set)將可供各種分類機器 (Classification 
Machine)訓練或分類之用。另外，在進行
多個分類機器(Classification Machines)的
集成學習(Ensemble Learning)部份，除了評
估 單 一 或 個 別 分 類 機 器 (Individual 
Classification Machine)的分類正確率或分
類效能(Performance Evaluation)，並融合混
淆矩陣(Confusion Matrix)的概念，考量各
單一或個別分類機器對於不同類別(Class 
Lebel)的分類能力及所衍生的分類正確與
分類錯誤情形，作為後續進行分類機器選
取(Classification Machine Selection)以供集
成學習之用的依據。此外，並運用田口式
方法(Taguchi Methods)可大幅減少分類機
器分類決策組合之評估實驗次數的優點，
同時以訊號雜訊比 (Signal-to-Noise Ratio 
(SNR))為計算基準，提供有效的分類機器
集成學習機制(包括考量多個分類機器之
間 的 分 類 機 器 多 樣 性 (Classification 
Machine Diversity))，進而得以有效提升或
改善整體分類效能。 
關鍵詞：核心機器、新型機器學習方法、
樣本選取、分類機器、混淆矩陣、田口式
方法 
 
Abstract 
 
In recent years, the concepts of selecting 
instances from the original training set for 
pattern classification and considering 
different classification machines’ abilities 
with respect to different class label have 
gained great popularity in machine learning 
and data mining.  Based on these concepts, 
the overall performance (i.e., classification 
accuracy) of individual classification 
machine can be improved.   
    In this research project, novel 
machine learning methods based on kernel 
machines will be proposed and investigated.  
The concepts of support vectors in kernel 
machines will be used for selecting instances 
from the original training set for pattern 
classification.  Consequently, a novel 
instance selection method based on kernel 
machines will be proposed.  A new, pruned 
training set can then be used for training and 
classification.  Accordingly, the confusion 
matrix of a kernel machine will be used to 
examine the kernel machine’s classification 
ability with respect to different class label.  
Taguchi methods are then employed for 
kernel machine selection and ensemble.  As 
a result, better kernel machines will be 
 3
分類錯誤情形[19]進行探討。事實上，就一
問題領域(Problem Domain)而言，各分類機
器對於不同的類別提供了不同的分類能
力，所衍生的分類正確與分類錯誤成本
(Cost)亦不相同。舉例來說，假設某商業銀
行擬發展一分類機器模型，用以決定接受
或拒絕客戶的信用卡申請。針對 1200 名客
戶，A 分類機器和 B 分類機器有相同的平
均分類正確率 90%(亦即可正確分類 1080
名客戶，其中 600 名接受其信用卡申請，
480 名拒絕其信用卡申請)，其中 A 分類機
器將36名應可接受其申請信用卡的客戶誤
判為拒絕，將 84 名應拒絕其申請信用卡的
客戶誤判為接受，而 B 分類機器將 84 名應
可接受其申請信用卡的客戶誤判為拒絕，
將36名應拒絕其申請信用卡的客戶誤判為
接受。雖然 A 和 B 兩分類機器有相同的平
均分類正確率，但商業銀行對於信用卡違
約必須付出的成本，往往遠大於拒絕一信
用優良的申請人所造成的收益損失，在此
考量下，B 分類機器顯然優於 A 分類機器。
另外，在應接受客戶申請信用卡的分類能
力方面，A 分類機器的平均分類正確率為
94.34%(600/(600+36))，B 分類機器的平均
分類正確率為 87.72%(600/(600+84))，在應
拒絕客戶申請信用卡的分類能力方面，A
分 類 機 器 的 平 均 分 類 正 確 率 為
85.11%(480/(480+84))，B 分類機器的平均
分類正確率為 93.02%(480/(480+36))。很顯
然地，A 分類機器對於應接受客戶申請信
用卡的分類能力較佳，而 B 分類機器對於
應拒絕客戶申請信用卡的分類能力較佳。
為達前述有效提升或改善整體分類效能的
目標，或可藉助 A 和 B 兩分類機器各自對
於不同類別較佳的分類能力表現。綜上所
述，有關各分類機器對於不同類別的分類
能力及所衍生的分類正確與分類錯誤情形
之研究主題，亦值得深入探究。 
一般來說，一個分類機器進行訓練
(Training)或分類(Classification)時不一定需
要使用及儲存所有的訓練樣本。因此，需
要一個有效的樣本選取方法，將不具代表
性及具雜訊(Noise)的訓練樣本從原始訓練
集合中移除，除了可減少訓練樣本所需的
儲存空間，也可減少分類計算或訓練一分
類機器所需的成本[10]。在樣本選取的概念
中 ， 除 了 著 重 於 提 昇 分 類 正 確 率
(Classification Accuracy)之外，另一個被用
來評估樣本選取方法結果好壞與否的指標
是資料保留率(Data retention rate)[20]，亦即
是進行樣本選取後最終的訓練樣本集合之
樣本個數除以原始訓練集合之樣本個數的
比率(ratio)。資料保留率顯然被期望越小越
好，以減少訓練樣本所需的儲存空間，同
時尚能保有良好的分類效能。 
在進行樣本選取的架構下，最重要的
是如何從原始訓練集合中選取或決定最具
代表性的訓練樣本。在濃縮的最近鄰居規
則 (Condensed Nearest Neighbor Rule ，
CNN)[21]方法中，會先從原始的訓練樣本
集合中隨機挑選幾個訓練樣本，假設該樣
本子集合為 S。若有一訓練樣本 a 以 S 為訓
練 集 ， 經 分 類 後 產 生 分 類 錯 誤
(Misclassified)，則該訓練樣本 a 會被加到
子集合 S 中，此步驟將重複進行直到所有
的訓練樣本都被檢查完成。最後，所得到
的訓練子集合 S 即為 Condensed 最近鄰居
規則(Nearest Neighbor Rule)中進行樣本選
取後的訓練子集合(Pruned Training Set)，此
新的訓練集合將被真正運用於分類計算
上。 
另 一 種 訓 練 樣 本 選 取 方 法 稱 為
Reduced 最近鄰居規則(Nearest Neighbor 
Rule)[22]，在此方法中，一訓練樣本 b 將
會從原始的訓練集合中被剔除(Pruned)，前
提條件是其他訓練樣本的分類正確與否不
會因為剔除訓練樣本 b 而降低，換言之，
所有的訓練樣本都將會一一被檢查，若 N
為所有訓練樣本的個數，則該樣本選取方
法的計算複雜度為 O(N*N)。 
另外，針對一訓練樣本 c 是否從原始
訓練集合中剔除，Edited Nearest Neighbor 
Rule[23]則提出檢查該訓練樣本 c是否可被
它 的 k 個 最 近 鄰 居 樣 本 (k Nearest 
Neighbors)正確地分類。在此樣本選取規則
中，無法被正確地分類者從原始訓練集合
中剔除，可被正確地分類者則保留(Retain)
在原始訓練集合中。 
在[24]中，作者提出一種以格網為基
(Lattice-based)的樣本選取方法，在格網的
架構下描繪整個原始訓練樣本集合的特
性。在此方法中，假如兩個屬於同一類別
後所形成的新的、精簡(Pruned)的訓練集合
(Training Set) 將 可 供 各 種 分 類 機 器
(Learning Machines)訓練或分類之用。此處
擬提出之樣本選取(Instance Selection)方法
係以核心機器(Kernal Machines)及支持向
量機(Support Vector Machine)的相關概念
為基礎，以下先針對核心機器 (Kernal 
Machines)及支持向量機 (Support Vector 
Machine)作一詳細的說明。 
首先描述一個較為簡化的兩類別
(Class Label)分類問題，假設其原始資料集
合 D 由{(x1,y1),(x2,y2),…,(xm,ym)}所組
成，其中 xi 代表具有類別 yi 的值組，
m=|D|。每個 yi 為+1 或-1，分別代表該分
類問題的正向樣本(Positive Instance)及負
向樣本(Negative Instance)。假設該分類問
題是線性可分割(Linearly Separable)，其樣
本空間可呈現於一個二維圖形中(如圖五
所示，亦即假設該問題僅包含兩個屬性
A1、A2)。由於是線性可分割的分類問題，
將可在樣本空間中劃出一條直線以區分所
有的正向樣本(Positive Instance)及負向樣
本(Negative Instance)，如圖五中的六條虛
線所示，將可能有無限多條直線可進行上
述的區域劃分。然而，分類機器或分類方
法的目標，即是在尋找一條直線，對於未
知類別的樣本可產生最小的分類誤差(即
分類錯誤率)。同樣地，若一分類問題其樣
本有 n 個屬性，則分類機器或分類方法的
目標，即是在尋找一個最佳的分隔超平面
(Hyperplane)。 
支持向量機(Support Vector Machine)
即是用來尋找最大邊距分隔超平面
(Maximum Marginal Hyperplane, MMH)的
重要分類機器或分類方法。在支持向量機
的分類機器架構下，一個分隔超平面
(Hyperplane)可定義為 
w．x+b=0 
其中 w={w1,w2,…,wn}代表權重向
量，n 代表一分類問題其樣本有 n 個屬性，
b 為一個常數，代表偏差值(bias)。假設一
訓練樣本的值組 x={x1,x2}，而 x1 與 x2 分
別為前述屬性 A1 及 A2 的數值，假設 b 為
w0，則該分隔超平面(Hyperplane)可重新改
寫為 
w2 x2+ w1 x1+ w0=0 
在此，任何分佈在該分隔超平面之上
的樣本滿足 
   w2 x2+ w1 x1+ w0>0 
而任何分佈在該分隔超平面之下的樣
本則滿足 
   w2 x2+ w1 x1+ w0<0 
透過調整權重向量 w，可分別定義正
向樣本(Positive Instance，yi 為+1)及負向樣
本(Negative Instance，yi 為-1)兩類別的邊界
分隔超平面： 
H1：w2 x2+ w1 x1+ w0 1  ≧ 對於 yi
為+1 的正向樣本 
H2：w2 x2+ w1 x1+ w0≦-1  對於 yi
為-1 的正向樣本 
換言之，任何分佈在分隔超平面 H1
或 之 上 的 樣 本 為 正 向 樣 本 (Positive 
Instance，yi 為+1)，任何分佈在分隔超平面
H2 或之下的樣本為負向樣本 (Negative 
Instance，yi 為-1)。 
將 H1 及 H2 兩方程式合併後可得 
 yi(w2 x2+ w1 x1+ w0) 1≧ ，對於所有的
i 皆成立 
在此，位於分隔超平面 H1 及 H2 上的
樣本稱為支持向量(Support Vector)。一般
而言，屬於支持向量的樣本最不易進行分
類，但它們也是最具資訊價值的樣本[26]。
在決定兩分隔超平面 H1 及 H2 之後，兩超
平面之間的最大邊距即可求得，為
2/ w∥ ∥，其中 w∥ ∥為 w 的歐幾里得距離
(Euclidean Distance)。 
將上述支持向量機 (Support Vector 
Machine)的基本分類架構對應到一般通用
的分類問題(不區分分類問題為線性可分
割 (Linearly Separable) 或非線性可分割
(Nonlinearly Separable))，尋找最大邊距分
隔超平面(Maximum Marginal Hyperplane, 
MMH) ，即是求解下列最佳化的問題
(Optimization Problem)[27-29]： 
( )( )
.0
1 tosubject
2
1min
1,,
≥
−≥+
+ ∑
=
i
ii
T
i
m
i
i
T
b
by
C
ξ
ξφ
ξξ
xw
ww
w
 
在此，所有的訓練樣本 透過一函數 
轉換到較高維度的空間 (Mapped into a 
higher dimentional space)。而 C>0 代表錯誤
項  的懲罰參數(Penalty Parameter of the 
 5
 7
是整個樣本選取問題最大的困難所在。綜
合前面的種種探討分析，以上述的核心機
器(Kernal Machines)及支持向量機(Support 
Vector Machine)為基礎，我們將得以有效
地決定(或選取)最具代表性或關鍵性的訓
練樣本；進而具體歸納出幾種不同類型的
訓練樣本(包括最具代表性或關鍵性的訓
練樣本)，這些不同類型被預期將有助於從
原始訓練集合中進行樣本選取，之後所形
成的新訓練集合(Training Set)將可供各種
分類機器(Learning Machines)訓練或分類
之用，並得以提高分類正確率。藉由前面
的探討分析，我們得以整理出不同類型的
訓練樣本，分列如下： 
(1) 在樣本空間(Instance Space)中，處
於 中 心 的 訓 練 樣 本 (Central Training 
Instances)或是關鍵的、具代表性、具有資
訊價值且有助於提昇分類正確率的訓練樣
本(Critical Training Instances)。 
(2) 處於樣本空間中邊界(Border)的
各個訓練樣本。 
(3) 雜訊樣本(Noisy Instances)，或是
容易產生分類錯誤的訓練樣本。 
在考量訓練樣本是屬於哪一類型的同
時，針對訓練樣本的重要性、關鍵性、其
資訊價值與採用該訓練樣本進行分類時的
正確率與否，我們亦可獲得各個訓練樣本
在重要性、關鍵性或影響分類正確率等相
關指標的數據，進一步具以決定所有訓練
樣本選取或剔除的優先順序。這是本研究
擬提出之訓練樣本選取方法的重要特性，
這個特性將使訓練樣本選取方法得以獲得
最小且具前後一致性(Minimal Consistent 
Subset)[33]的樣本集合，換言之，在本方法
中，經選取後所得到的最終樣本集合 S 不
會因訓練樣本出現的先後順序(Order)，而
產生在前述其他眾多樣本選取方法中 S 不
是唯一(Unique)的情形(這個最終訓練集合
不一致(Consistent)或不唯一的問題，降低
了某些樣本選取方法的實用性，如
CNN[34]；而本計畫擬提出的樣本選取方法
不會產生此問題)。 
在考量前述不同類型的訓練樣本，我
們得以訂定多個選取或剔除訓練樣本的規
則，進而發展有效而快速的訓練樣本選取
方法，之後所形成的新訓練集合(Training 
Set) 將 可 供 各 種 分 類 機 器 (Learning 
Machines)訓練或分類之用，並得以提高分
類正確率。這些過濾或選取的規則將包括： 
(1) 在樣本空間中處於中心的訓練樣
本(Central Training Instances)或是關鍵的、
具代表性(Typical)、具資訊價值且有助於其
他分類機器提昇分類正確率的訓練樣本
(Critical Training Instances) 將保留在原始
的訓練樣本集合中。這個選取規則將比其
他規則優先執行。 
(2) 處於樣本空間中邊界(Border)的
各個訓練樣本將從原始的訓練集合中刪
除。 
(3) 雜訊樣本(Noisy Instances)，或是
容易產生分類錯誤的訓練樣本亦將從原始
的訓練集合中刪除。 
藉由上述樣本選取規則，我們將可小
心地避免刪除一些關鍵而具影響性的訓練
樣本(Critical Training Instances)，只刪除處
於樣本空間中邊界 (Border)、雜訊樣本
(Noisy instances)，或是容易產生分類錯誤
的訓練樣本。本樣本選取方法所得到的最
終樣本集合將被預期可供其他分類機器保
有良好的分類正確率(甚至分類效果變得
比進行樣本選取前更佳，主因是過濾了雜
訊樣本或容易產生分類錯誤的訓練樣
本)。在計算複雜度方面，所提的樣本選取
方法亦不會劣於現有的樣本選取方法。 
 
四、結果與討論 
 
藉由上述種種探討分析擬發展的方法
(以下稱本方法)將有下列多項優點(與文獻
探討與整理中現有樣本選取方法不同)： 
(1) 優於其他方法的是，本方法將同時考
量在樣本空間中處於中心的訓練樣本
(Central Training Instances)或是關鍵的、具
代表性(Representative)、具資訊價值且有助
於提昇分類正確率的訓練樣本 (Critical 
Training Instances)、處於樣本空間中邊界
(Border) 的訓練樣本、雜訊樣本 (Noisy 
Instances)、以及容易產生分類錯誤的訓練
樣本。特別是挑選可產生較高分類正確率
的訓練樣本，以形成最終的訓練集合，之
後所形成的新的、精簡(Pruned)的訓練集合
(Training Set) 將 可 供 各 種 分 類 機 器
 9
(7) 完成撰寫研究報告，並整理相關研究
分析結果，以「基於核心機器的新型
樣本選取方法」為研究主軸，積極發
表在 IEEE Trans. on Pattern Analysis 
and Machine Intelligence、Machine 
Learning、Pattern Recognition、Applied 
Intelligence 或 Computational 
Intelligence 等國際著名人工智慧與機
器學習領域 SCI、EI 學術期刊與相關
國際學術研討會議。 
 
本研究之具體成果如下： 
 
(1) 已完成蒐集相關文獻、確定研究方
向、擬定研究目的、方法與步驟。 
(2) 有關理論分析與文獻探討部份，已蒐
集相關重要文獻，以作為本研究的理
論基礎，與基於核心機器的新型機器
學習方法研究及設計之參考。 
(3) 已完成建構所提新方法的相關實驗
環境。 
(4) 已完成研究、分析與設計本研究所提
出新方法，與相關各項測試與驗證。 
(5) 已將研究心得歸納結論，並提出未來
研究方向及建議。 
(6) 已完成撰寫研究報告。 
 
六、參考文獻 
 
[1] T. M. Mitchell, Machine Learning, 
McGraw-Hill, New York, 1997. 
[2] I. Witten and E. Frank, Data mining - practical 
machine learning tools and techniques with java 
implementations, Morgan Kaufmann, San 
Francisco, CA, 2nd ed., 2005. 
[3] S. Russell and P. Norvig, Artificial Intelligence. 
A modern approach, Prentice Hall, 1995. 
[4] J. R. Quinlan, C4.5: Programs for Machine 
Learning, Morgan Kaufmann Publishers, San 
Mateo, CA, 1993. 
[5] R. Kohavi, “The power of decision tables,” in 
European Conference on Machine Learning, 
1995. 
[6] G.H. John and P. Langley, “Estimating 
continuous distributions in bayesian classifiers,” 
In Proc. of the 11th Conference on Uncertainty 
in Artificial Intelligence, pp. 338-345, San 
Francisco, 1995. Morgan Kaufmann. 
[7] R. Agrawal, T. Imilienski, and A. Swami, 
“Mining association rules between sets of items 
in large databases,” In Proceedings of the ACM 
SIGMOD International Conference on 
Management of Database, pp. 207-216, 1993. 
[8] V. Vapnik. Statistical Learning Theory. John 
Wiley and Sons, New York, 1998. 
[9] V. Vapnik. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York, 1995. 
[10] D. W. Aha, D. Kibler, and M. K. Albert, 
“Instance-based learning algorithms,” Machine 
Learning, vol. 6, pp. 37-66, 1991. 
[11] K.P. Zhao, S.G. Zhou, J.H. Guan, A.Y. Zhou, 
“C-Pruner: An improved instance prunning 
algorithm,” Second International Conference on 
Machine Learning and Cybernetics (ICMLC'03), 
pp. 94-99, 2003. 
[12] Shirish S. Sane, Ashok A. Ghatol, “Using 
Instance Typicality to Build Compact and 
Accurate Neural Network Classifiers,” ICDIM, 
pp. 234-239, 2006. 
[13] Ai-Jun Li, “An Improved Algorithm for 
Incremental Learning LEARN++”, in Proc. of 
ICWAPR '08. International Conference on 
Wavelet Analysis and Pattern Recognition, vol. 
1 pp. 310-315, 2008.  
[14] Souza, J.T. De,Carmo, R. Do, Campos, G. De, 
“A novel approach for integrating feature and 
instance selection,” in Proc. of 2008 
International Conference on Machine Learning 
and Cybernetics, vol. 1, pp. 374-379, 2008. 
[15] N. Zhang, X. Z. Wang, and T. Xiao, “An 
instance selection algorithm based on 
contribution,” in Proc. of 2008 International 
Conference on Machine Learning and 
Cybernetics, vol. 2, pp. 919-923, 2008. 
[16] S. S. Sane and A. A. Ghatol, “Using Instance 
Typicality to Build Compact and Accurate 
Neural Network Classifiers,” in Proc. of 2006 
1st International Conference on Digital 
Information Management, pp. 234-239, 2006. 
[17] Yixin Chen; Jinbo Bi; J.Z Wang, “MILES: 
Multiple-Instance Learning via Embedded 
Instance Selection,” IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 
28, pp. 1931-1947, 2006. 
[18] S. S. Sane and A. A. Ghatol, “Use of Instance 
Typicality for Efficient Detection of Outliers 
with Neural Network Classifiers,” in Proc. of 
ICIT '06. 9th International Conference on 
Information Technology, pp. 225-228, 2006. 
[19] R. J. Roiger and M. W. Geatz, Data Mining - A 
Tutorial-Based Primer, Addison Wesley, 2003. 
[20] W. Lam, C.K. Keung and D. Liu, “Discovering 
Useful Concept Prototypes for Classification 
Based on Filtering and Abstraction,” IEEE 
Trans. Pattern Analysis and Machine 
Intelligence, vol. 24, no. 8, 2002. 
[21] P.E.Hart, “The Condenced Nearest Neighbor 
Rule,” IEEE Trans. Information Theory, vol. 14, 
no. 3, pp.515-516, 1968. 
[22] G. W. Gates, “The reduced nearest neighbor 
rule,” IEEE Trans. on Information Theory, vol. 
18, no. 5, pp. 431-433, May 1972. 
[23] D. L. Wilson, “Asymptotic properties of nearest 
neighbor rules using edited data,” IEEE Trans. 
on Systems, Man and Cybernetics, 2, 408-421, 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/01/31
國科會補助計畫
計畫名稱: 基於核心機器的多種新型機器學習方法
計畫主持人: 黃淇竣
計畫編號: 98-2221-E-022-009- 學門領域: 人工智慧
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
在實務應用方面，本研究所提新方法將有助於各種實際分類問題領域(Problem 
Domains)的解決與應用發展，如文件分類、醫療診斷、戶內與戶外場景分類、
遙測資料分類、入侵偵測、機器錯誤自動診斷、語音辨識、人臉辨識、影像分
類、人物識別、中文辨識與手寫字識別等諸多複雜的實務分類問題。 
在專業技術人才培養方面，本計畫將有助於我國培養具備人工智慧、機器學習、
資料探勘與軟體開發等專門領域知識的高級專業技術人才。本計畫相關研究參
與人員藉由本年度所提新方法的各項理論基礎、建立相關實驗環境與其在各分
類問題領域的實驗結果及分析等訓練，將可提升相關問題的深入分析討論與解
決能力。 
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
