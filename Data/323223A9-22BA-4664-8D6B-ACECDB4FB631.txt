similarity measure and video matching algorithm are de-
scribed in Section 3. In Section 4, relevance feedback
technique is introduced to improve the video retrieval
result. The performance evaluation of our approach is
reported in Section 5. Finally, some concluding remark
is given in Section 6.
2. Visual Feature
Shot is the fundamental unit of a video. To facilitate sub-
sequent video analysis, in our system, the query video clip
and database video are segmented into shots. This task is
achieved by applying shot boundary detection algorithm
[17] to the original video sequence. A major requirement
for shot matching is to define a content representation
that captures the common aspects or characteristics of
the shot. One common method is to select one key-frame
from the shot and use the image features of the key-frame
as an abstract representation of the shot. For shot with
fast changing content, one key-frame per shot is not ade-
quate. Besides, the content description it provides varies
significantly with the key-frame selection criterion. To
avoid these problems, a more favorable approach is to
consider the visual content of all the frames with a shot
for shot representation.
Color is one of the most widely used visual fea-
tures in video content analysis, because it is an important
source of information in visual content for discrimination.
However, the amount of color information in video is vast.
The raw data of video has to be transformed into compact
feature representation that conveys only the most salient
color aspects of the visual content. Color histogram is the
most commonly used color feature representation. The
histogram-based approach is relatively simple to calcu-
late and can provide reasonable results. However, due to
the statistical nature, color histogram does not capture
spatial layout information of each color. When the image
collection is large, two different content images are likely
to have quite similar histograms. To remedy this defi-
ciency, the distribution state of each single color in the
spatial (image) domain needs to be taken into account.
The color histogram for an image is constructed by
counting the number of pixels of each color. The main
issues regarding the construction of color histogram in-
volve the choice of color space and quantization of color
space. The RGB color space is the most common color
format for digital images, but it is not perceptually uni-
form. Uniform quantization of RGB space gives per-
ceptually redundant bins and perceptual holes in color
space. Therefore, the non-uniform quantization may be
needed. Alternatively, HSV (hue, saturation,intensity)
color space is chosen since it is nearly perceptually uni-
form. Thus, the similarity between two colors is deter-
mined by their proximity in the HSV color space. When
a perceptually uniform color space is chosen, uniform
quantization may be appropriate. Since the human vi-
sual system is more sensitive to hue than to saturation
and intensity[18], H should be quantized finer than S and
V . In our implementation, hue is quantized into 20 bins.
Saturation and intensity are each quantized into 10 bins.
This quantization provides 20 × 10 × 10 distinct colors
(bins), and each bin with non-zero count corresponds to
a color object.
Since we are interested in the whole shot rather than
single image frame, only one histogram is used to count
the color distribution of all image frames within a shot.
The use of one histogram as color descriptor for a group
of frames has been accepted as the MPEG-7 standard[19].
Then, each bin of the resulting histogram is divided by
the number of frames in a shot to obtain the average
color histogram. Next, several spatial features are calcu-
lated to characterize the distribution state of each color
object in each image frame. Assuming a set of pixels
S = {(x1, y1), · · · , (xn, yn)} belong to color object ci, k is
the image size and m is the total number of 4-connected
pixels in S. Then, we define
(i) density of distribution
fi1 =
n
k
(ii) compactness of distribution
fi2 =
m
n
(iii) scatter
fi3 =
1
n
√
k
n∑
j=1
√
(xj − xµ)2 + (yj − yµ)2
where xµ =
1
n
∑n
i=1
xi and yµ =
1
n
∑n
i=1
yi
To define the fourth feature, the image is equally
partitioned into p blocks of size 16×16. A block is active,
if it contains some subset of S. Let the number of active
blocks in the image frame be q, we define
(iv) ratio of active block
fi4 =
q
p
After the spatial features of all image frames within
a shot are computed, we take average of these values
respectively. Let fi1, fi2, fi3 and fi4 be the average fea-
ture values of color object ci within a shot, for two color
objects ci and cj, the difference of spatial distribution
within a shot is defined as
Ds(ci, cj) =
1
4
(|fi1 − fj1|+ |fi2 − fj2| +
|fi3 − fj3|+ |fi4 − fj4|) (1)
2
where Wc and Wm are the weights for the color and mo-
tion features, respectively. The setting of Wc and Wm is
discussed in the next section.
Given the query video clip Q = {q1, · · · , qm} and the
database video V = {v1, · · · , vn}, where qi and vj denote
the segmented shots, the similarity measure between the
query clip and the database video segment starting at
the i-th shot is defined as
Di =
m∑
j=1
ShotSim(qj, vi+j−1) (5)
If Di is a local maxima and is also greater than a thresh-
old T then a similar clip is detected at the i-th shot of
database video. In our system, the threshold T is set to
be 0.5. The smaller T value the more similar video clips
are detected. Since our system is able to rank the similar
video clips, the choice of threshold is irrelevant to the
determination of the most similar video clip.
4. Relevance Feedback
The various techniques developed for content-based video
retrieval are all efforts to try to map low level features to
high level concepts. However, it is not easy to fill in the
gap between these two levels in every case. In addition,
different persons, or even the same person under different
circumstances, may perceive the same visual content dif-
ferently. Therefore, any method with a fixed set of visual
feature representations and their corresponding weights
cannot always effectively model high level concepts and
user’s subjective perceptions. To address this limitation,
one possible solution is the relevance feedback [26]. It is
an interactive mechanism that involves a human as part
of the retrieval process.
In the relevance feedback approach, under the as-
sumption that high level concepts can be represented by
low level features, the technique tries to establish the link
between the two levels from a user’s feedback. The user
only needs to specify which video clips he or she thinks
are relevant to the query. The weights embedded in the
similarity measure are then dynamically updated to ad-
just the importance of the visual features used according
to the user’s subjective perceptions during each round of
the retrieval process. As relevance feedback was applied
to content-based image retrieval[26], we extend this tech-
nique to video retrieval. Our objective is to update the
weights Wc and Wm in equation (4) to reflect the user’s
different emphasis on the feature representation in the
overall similarity metric according to his or her feedback.
This is done with the following algorithm.
Let R be the set containing the most similar N
retrieved video clips according to the overall similarity
value Di, with Wc and Wm initially set to 0.5:
R = [R1, · · · , RN ]
Let Score = [Score1, · · · , ScoreN ] be the set containing
the relevance scores feedback by the user for each of the
retrieved clips in R:
Scorei =

3, highly relevant
1, relevant
0, no-opinion
−1, non-relevant
−3, highly non-relevant
The choice of these numbers as the scores are arbitrary.
The user may choose other numbers for their conve-
nience.
Then, let Rc and Rm be the sets containing the
most similar N clips to the query, according to only the
color similarity measure and only the motion similarity
measure, respectively.
Rc = [Rc1, · · · , RcN ]
Rm = [Rm1 , · · · , RmN ]
Now, to calculate the new values for Wc and Wm, first
set Wc = 0 and Wm = 0, then update these two weights
using the following procedure:
Wk =
{
Wk + Scorei if R
k
i ∈ R
Wk + 0 otherwise
i = 1, · · · ,N
k = c,m
After this procedure, if Wk < 0, set it to be 0. The
raw weights obtained by the above procedure are then
normalized by the total weights to make the sum of the
normalized weights equal to 1. It is observed that the
more the overlap of relevant clips between R and Rk , the
larger the weights of Wk. In other words, if a partic-
ular feature representation reflects a user’s need, it re-
ceives more emphasis. Moreover, this algorithm can be
repeated to iteratively fine-tune the retrieval results until
the user is satisfied.
5. Experimental Results
To evaluate the performance of the proposed approach,
we set up a database that consists of 3 hours of videos
approximately. The genres of videos include home video,
news, sports, movies and documentaries. The testing
with different genres of videos would ensure that the
overall performance of the algorithm is not biased to-
ward a specific video category. Figure 3 shows an ex-
ample of retrieving and ranking similar video clips with
query clip (shown in the first row). In each row, sam-
pled frames (one for each shot) are used to represent
4
[3] M.R. Naphade, M.M. Yeung, and B.L. Yeo. A novel
scheme for fast and efficient video sequence match-
ing using compact signature. In SPIE Conference
on Storage and Retrieval for Media Database, pages
564–572, January 2000.
[4] T. Hoad and J. Zobel. Fast video matching with
signature alignment. In ACM SIGMM Interna-
tional Workshop on Multimedia Information Re-
trieval, pages 262–269, Berkeley, CA, November
2003.
[5] W. Ren and S. Singh. Video sequence matching
with spatio-temporal constraints. In International
Conference on Pattern Recognition, pages 834–837,
August 2004.
[6] C. Kim and B. Vasudev. Spatiotemporal sequence
matching for efficient video copy detection. IEEE
Transactions on Circuits and Systems for Video
Technology, 15(1):127–132, January 2005.
[7] X. Liu, Y. Zhung, and Y. Pan. A new approach
to retrieve video by example video clip. In ACM
International Conference on Multimedia, pages 41–
44, 1999.
[8] A.K. Jain, A. Vailaya, and X. Wei. Query by video
clip. Multimedia Systems, 7:369–384, 1999.
[9] R. Lienhart, W. Effelsberg, and R. Jain. Visual-
GREP: A systematic method to compare and re-
trieve video sequences. Multimedia Tools and Appli-
cations, 10(1):47–72, January 2000.
[10] S.H. Kim and R.H. Park. An efficient algorithm for
video sequence matching using the modified Haus-
dorff distance and the directed divergence. IEEE
Transactions on Circuits and Systems for Video
Technology, 12(7):592–596, July 2002.
[11] N. Diakopouos and S. Volmer. Temporally tolerant
video matching. In ACM SIGIRWorkshop on Multi-
media Information Retrieval, Toronto, Canada, Au-
gust 2003.
[12] Y.X. Ping, C.W. Ngo, Q.J. Dong, Z.M. Guo, and
J.G. Xiao. Video clip retrieval by maximal match-
ing and optimal matching in graph theory. In Inter-
national Conference on Multimedia and Expo, pages
317–320, 2003.
[13] Y.X. Peng and C.W. Ngo. Clip-based similarity
measure for hierarchical video retrieval. In ACM
SIGMM International Workshop on Multimedia In-
formation Retrieval, pages 53–60, October 2004.
[14] K.W. Sze, K.M. Lam, and G. Qiu. A new key frame
representation for video segment retrieval. IEEE
Transactions on Circuits and Systems for Video
Technology, 15(9):1148–1155, September 2005.
[15] K. Kashino, T. Kurozumi, and H. Murase. A quick
search method for audio and video signals based on
histigram pruning. IEEE Transactions on Multime-
dia, 5(3):348–357, September 2003.
[16] J. Yuan, Q. Tian, and S. Ranganath. Fast and ro-
bust search method for short video clips from large
video collection. In International Conference on
Pattern Recognition, pages 866–869, August 2004.
[17] L.-H. Chen, C.-W. Su, H.-Y. Liao, and C.-C. Shih.
On the preview of digital movies. Journal of
Visual Communication and Image Representation,
14(3):357–367, September 2003.
[18] X. Wan and C.-C. Jay Kuo. A new approach to im-
age retrieval with hierarchical color clustering. IEEE
Transactions on Circuits and Systems for Video
Technology, 8(5):628–643, September 1998.
[19] T. Sikora. The MPEG-7 visual standard for con-
tent description - An overview. IEEE Transac-
tions on Circuits and Systems for Video Technology,
11(6):696–702, June 2001.
[20] H.S. Wang and R.M. Mersereau. Fast algorithms
for the estimation of motion vectors. IEEE Trans-
actions on Image Processing, 8(3):435–438, March
1999.
[21] D. Tzovaras and M.G. Strintzis. Motion and dispar-
ity field estimation using rate-distortion optimiza-
tion. IEEE Transactions on Circuits and Systems
for Video Technology, 8(2):171–180, April 1998.
[22] D.L. Gall. MPEG: A video compression standard for
multimedia applications. Communication of ACM,
34(4):46–58, April 1991.
[23] M.J. Swain and D.H. Ballard. Color indexing. Inter-
national Journal of Computer Vision, 7(11):11–32,
1991.
[24] S. Haykin. Neural Network: A Comprehensive Foun-
dation. Prentice Hall, New Jersey, U.S.A., 1999.
[25] B.B. Boycott. Color Vision. Cambridge University
Press, Cambridge, U.K., 2001.
[26] Y. Rui, T.S. Huang, M. Ortega, and S. Mehro-
tra. Relevance Feedback: A power tool for interac-
tive content-based image retrieval. IEEE Transac-
tions on Circuits and Systems for Video Technology,
8(5):644–655, September 1998.
[27] J.R. Smith and S.F. Chang. Tools and techniques
for color image retrieval. In SPIE Conference on
Storage & Retrieval for Image and Video Databases,
pages 426–437, San Jose CA, February 1996.
6
   
 
   
   
   
   
   
    Figure 4: Retrieval result for a ``free throw” query without relevance feedback. 
 
 
   
   
   
   
   
   Figure 5: Retrieval result for a ``free throw” query with relevance feedback. 
(Similar) 
(Similar) 
(Similar) 
(Dissimilar) 
(Similar) 
Rank 
switched 
Rank 
switched 
可供推廣之研發成果資料表 
□ 可申請專利  □ 可技術移轉                                      日期：96 年 8 月 1日 
國科會補助計畫 
計畫名稱 : 整合色彩及移動特徵以進行視訊檢索 
計畫主持人：陳良華 
計畫編號：NSC 95-2221-E-030-015 學門領域：影像處理 
技術/創作名稱 一個整合式的視訊檢索技術 
發明人/創作人 陳良華 
中文： 
在這個研究計畫中，我們提出一個整合性的方法來進行以shot
為基礎的視訊片段比對。不像以前的方法只用ㄧ張畫面來代表ㄧ段
shot，我們的方法分析整段 shot 內的所有畫面以抽取更多的視覺
資訊來代表 shot。由於僅用一種視覺特徵不能完整地代表視覺內
容，因此我們整合色彩及移動特徵來充分掌握時空資訊。在視訊比
對階段，我們訂出一個相似度量來找出資料庫中的相似片段。為了
根據使用者的視覺判斷來提高檢索績效，我們引用關聯回饋技術來
調整各項視覺特徵的權重。我們所要發展的技術可用於數位圖書 
館、遠距教學、新聞傳播及影片內容監視等。 
技術說明 
英文：  
  In this research project, we propose an integrated approach for 
shot–based video retrieval. Unlike key-frame based representation of 
shot, the proposed approach analyzes all frames within a shot to extract 
more visual information for shot representation. Because there does not 
exist a single visual feature for the best representation of video content, 
our approach integrates color and motion features to capture the 
spatio-temporal information contained in video. In the video matching 
step, a similarity measure is defined to locate the occurrence of similar 
video clips in the database. To improve the retrieval performance 
according to user’s visual judgment, relevance feedback technique is 
also incorporated to adjust the weight of each visual feature. The 
proposed video retrieval technique has potential applications in digital 
library, distance learning, news broadcasting and video monitoring. 
可利用之產業 
及 
可開發之產品 
新聞傳播業可利用本技術來搜尋過去歷史的新聞影片。 
數位圖書館可利用本技術來過濾色情暴力影片。 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                             95 年    9 月     
報告人姓名 陳良華 服務機構 
及職稱 
輔仁大學資訊工程系教授 
     時間 
會議 
     地點 
2006 年 8 月 20 日至 24 日 
香港 
本會核定 
補助文號 
專題研究計畫 
NSC95-2221-E-030-015 
會議 
名稱 
 (中文) 第 18 屆國際圖形識別會議 
 (英文)18th International Conference on Pattern Recognition 
發表 
論文 
題目 
 (中文)運用 mosaic 技術來進行視訊場景抽取 
 (英文)Video Scene Extraction Using Mosaic Technique 
報告內容應包括下列各項： 
一、 參加會議經過 
 
國際圖形識別會議(ICPR)是國際圖形識別學會(IAPR)每二年舉辦一次的大型論文研討會。本人承蒙
國科會補助，得以參加今年 8月底在香港舉行的第 18 屆國際圖形辨別會議。首先，我於 8月 19 日
搭乘長榮客機飛至香港。在旅館休息一晚之後，隔天便到會議所在地－香港會議與展示中心，辦理
報到手續，領取名牌及會議輪文集。與以往比較，今年這一屆大會可說是規模最大，總共約有一千
五百人參加。雖然增加不少大會的工作負擔，但也象徵著圖形識別這一研究領域的日益蓬勃發展。
在開幕典禮中，大會主席 Y.Y.Tang 首先致歡迎詞，接著 IAPR 會長也上台感謝大會全體工作人員。
最後壓軸的是頒發 K.S.Fu 獎。此獎乃是國際圖形識別學會為紀念該會創會會長傅京孫教授對學會及
學術領域的貢獻而設立。今年由英國的 J.Killter 教授以其在運用機率理論來發展新的圖形識別方
法而獲獎。Killter 教授已在學術界三十多年，在場的中青輩很多人在學生時代就拜讀其所編的教科
書，此次得獎，可謂實至名歸。典禮結束後，會議論文分五組在不同的會議室發表。我的論文則被
安排在有關於視訊處理暨應用的場次發表。此次會議最大的特點是邀請不少大師級的人物作專題演
講，主題大都配合當今科技發展的最新趨勢如網路，生物認證和資料庫方面的應用，內容均十分精
闢。 
最後值得一提的是迎新晚會是以自助餐的形式進行，大會故意不提供座椅，以方便大家可以盡情交
誼及交換意見。席間我遇見許多大師包括 T.S.Huang，A.K.Jain 和 R.M.Haralick 等，這些人對台灣
去的同行皆十分友善，對我們所提的問題也耐心而詳細的回答，使人有不虛此行的感覺。 
 
二、與會心得 
 
從這次會議可以大略看到當今圖形識別技術的發展趨勢--朝向更生活化，更應用化的課題發展。此
外，我亦觀察到很多熱門的研究主題，國內甚少有人觸及。平心而論，雖然台灣發表的論文數量不
少，但質方面仍稍稍遜於歐美各國。就自我而言，經由和一些高手的討論，我發現到了自己研究上
的盲點，謝謝他們的指導，使我受益良多。 
 
三、建議 
 
雖然國際上的學術會議種類很多且良莠不齊，但知名度高及具有權威性的會議就是固定那幾個。因
此希望今後有關單位能夠多鼓勵及補助國內的專家學者參加此類高水準的學術會議，因為從會議中
我們不但可以學到不少東西，更可以展現自己的研究成果以提昇國家的學術形象與水準。 
 
四、攜回資料的名稱及內容 
 
The 18th IEEE International Conference on Pattern Recognition 論文集 (光碟片)。 
附件三
 
Constructing mosaic of each video shot. (3) Determina-
tion of similarity measure between two shots. (4) Group-
ing correlated shots into one scene. The ﬁrst two com-
ponents are implemented by our previous algorithms[8-
9]. The last two components are described in the rest of
this paper. To build a static mosaic of the background
scene, one must be able to align frames from a video
shot. In Fig. 1, the original 20th, 40th and 60th frames
of “Skater” sequence are shown. The constructed mosaic
is shown in Fig. 2. It is noted that our mosaic technique
is able to recover the background scene even if the camera
is moving. Using the background mosaic, the background
scene image of each video frame is reconstructed. In the
subsequent processing, we will focus on these background
image sequences.
2. Shot Similarity Measure
Color is one of the most widely used visual features
in video content analysis. Most of the scene extraction
algorithms compare color histograms between keyframes
to determine the shot similarity measure. However, due
to the statistical nature, color histogram does not cap-
ture spatial layout information of each color. When the
image collection is large, two diﬀerent content images are
likely to have quite similar histograms. To remedy this
deﬁciency, in our approach, the distribution state of each
single color in the spatial (image) domain is also taken
into account. The color histogram for an image is con-
structed by counting the number of pixels of each color.
Each bin with non-zero count corresponds to a color ob-
ject. HSV (hue,saturation,intensity) color space is cho-
sen since it is nearly perceptually uniform. As we are in-
terested in the whole shot rather than single image frame,
only one histogram is used to count the color distribution
of all background images within a shot. Then, each bin of
the resulting histogram is divided by the frame number
of a shot to obtain the average histogram. Next, several
spatial features are calculated to characterize the distri-
bution state of each color object in each image frame.
Assuming a set of pixels S = {(x1, y1), · · · , (xn, yn)} be-
long to color object ci, k is the image size and m is the
total number of 4-connected pixels in S. Then, we deﬁne
(i) density of distribution: fi1 =
n
k
(ii) compact of distribution: fi2 =
m
n
(iii) scatter:
fi3 =
1
n
√
k
∑n
j=1
√
(xj − xμ)2 + (yj − yμ)2 where xμ =
1
n
∑n
i=1
xi and yμ =
1
n
∑n
i=1
yi
To deﬁne the fourth feature, the image is equally
partitioned into p blocks of size 16×16. A block is active,
if it contains some subset of S. Let the number of active
blocks in the image frame be q, we deﬁne (iv) ratio of
active block: fi4 =
q
p
After the spatial features of all background images
within a shot are computed, we take average of these val-
ues respectively. Let fi1, fi2, fi3 and fi4 be the average
feature values of color object ci within a shot, for two
color objects ci and cj , the diﬀerence of spatial distribu-
tion within a shot is deﬁned as
Ds(ci, cj) = |fi1 − fj1|+ |fi2 − fj2|+
|fi3 − fj3|+ |fi4 − fj4| (1)
Texture refers to the visual patterns that have prop-
erties of homogeneity that do not result from the presence
of only a single color or intensity. We deﬁne the coarse-
ness of texture in an image in term of the distribution
density of edges. The Canny edge detector is used to
extract edges from an image. Again, the detected edge
image is partitioned into a set of 16× 16 blocks. A block
is textured, if the number of edge points in this block is
greater than a threshold. Then, we can compute the ra-
tio of textured block of each image and its average value
over a shot. The texture similarity between two shots is
determined by the minimum of the two average values.
Let A,B be the set of all color objects in shot S1
and S2 respectively, for a given u ∈ A, its similar color
object in B is some v ∈ B such that ||u − v|| < , where
||u− v|| denotes the Euclidean distance between u and v
in the HSV color space and  is a threshold. Then, (u, v)
is called a similar color pair. Let Ω = {(u, v)|(u, v) ∈
A × B, (u, v) is a similar color pair}, the shot similarity
measure between S1 (with the average histogram H1) and
S2 (with the average histogram H2) is deﬁned as
ShotSim(S1, S2) =
1
k
∑
(u,v)∈Ω
{W (Ds(u, v))×
min(H1(u), H2(v))} + wt ×min(t1, t2) (2)
where k is the image size, t1 and t2 are the average ratios
of textured block for shot S1 and S2 respectively, wt is the
weight of texture feature, Ds is the diﬀerence of spatial
features as deﬁned in equation (1) and W is a weight
function deﬁned as
W (x) =
1
1 + e10×x−5
The weight function W is one form of sigmoid function
which is frequently used in neural networks computation.
In our work, it is used to fuse spatial distribution infor-
mation with histogram. The construction of this weight
function is motivated by the psychophysical observation:
the eﬀect of spatial distribution on human perception is
progressive[10]. Only when the diﬀerence in spatial fea-
tures is greater than a threshold, human perceive signif-
icant visual variation. The property of sigmoid function
fulﬁlls this requirement.
3. Scene Extraction
Movies directors, while ﬁlming the scenes, also con-
trol the pace of ﬁlm in order to sustain the viewers inter-
est. One important factor known to inﬂuence the pace
of a movie is the Montage. In order to convey idea that
has strong resonance with the viewers, Montage is widely
used as the basis to model scenes. In most situations,
Montage can be simpliﬁed as a set of cinematic rules.
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  © 2006
