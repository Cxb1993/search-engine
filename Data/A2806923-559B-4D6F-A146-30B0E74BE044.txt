Author's personal copy
J. Parallel Distrib. Comput. 68 (2008) 1297–1304
Contents lists available at ScienceDirect
J. Parallel Distrib. Comput.
journal homepage: www.elsevier.com/locate/jpdc
Research note
On the assessment of input streams for incremental network computingI
Cho-Chin Lin ∗, Hao-Yun Yin
Department of Electronic Engineering, National Ilan University, Yilan, Taiwan 260, Taiwan
a r t i c l e i n f o
Article history:
Received 13 August 2007
Received in revised form
4 May 2008
Accepted 20 May 2008
Available online 4 June 2008
Keywords:
Input stream
Network computing
Incremental computing
Scheduling
Assessment
a b s t r a c t
Network latency is an adverse factor for computations performed across the network. Overlapping
computation with communication is an important technique for hiding latency. It has been shown
that network latency cannot be effectively hidden without considering the order of sending data [C.-
C. Lin, Strategies for achieving high performance incremental computing on a network environment,
in: Proc.18th Int’l Conf. on Advanced Information Networking and Applications 1, 2004, pp. 113–118].
However, finding a data-sending order for the input to a task which minimizes the remote execution
time for any network traffic pattern is NP-hard [C.-C. Lin, D.-W. Wang, T.-S. Hsu, Bounds on the client-
server incremental computing, IEICE Trans. Fundamentals E89-A (5) (2006) 1198–1206]. Thus, heuristic
algorithms are often employed to search an optimal input stream. The performance of algorithms relies
on an effective mechanism for guiding the search toward a promising direction. In this paper, the
computation-progress graph is proposed for transforming an input stream of a task to its corresponding
pattern of progressive computations. Then, the assessing function is defined for assigning scores to the
found input streams based on the computation-progress graph. Based on the scores, the promising search
directions can be determined. Finally, the effectiveness of our assessing function is also demonstrated by
the search of the optimal orders for computing the product of two polynomials, matrix multiplication
and FFT.
© 2008 Elsevier Inc. All rights reserved.
1. Introduction
Network latency is an adverse factor for computations per-
formed across the network. To fully employ the computational
power of the networked computing devices, latency hiding by
overlapping computation with communication is a well known
technique [9,10]. It can be achieved by partitioning the input data
into several data pieces, then pipelining the data pieces into the
network. In incremental computing mode [5], the computing de-
vice starts its computations using the partially arrived data to
aggressively produce the partial result. However, network latency
cannot be effectively hidden without considering the order of
sending data [4]. That is, a well-organized input stream is impor-
tant to maximize the processor utilization of a computing device.
Consider the task d2f (d0)+d1 to be performed at a computing device
using data items d0, d1 and d2 sent from a data provider, where
f (x) = x2+ 1x +
√
x. The basic arithmetic operations for the device
to compute f (x) are: one square, one square root, one reciprocal
and two additions. Assume each basic arithmetic operation takes
I This research is partially supported by National Science Council under the grant
96-2221-E-197-007.∗ Corresponding address: National Ilan University, Department of Electronic
Engineering, No. 1, Sec. 1, Sheng-Long Rd., 260 Yilan, Taiwan.
E-mail address: cclin@niu.edu.tw (C.-C. Lin).
one clock to complete and the arriving intervals of the three data
items are ω0 and ω1 clocks. If the input stream is d2d1d0 then no
computation can start until all the data items have been received.
In this case, the time to complete the task is t = ω0+ω1+7. If the
input stream is d0d1d2 then the arrival of any data item triggers at
least one computation. In this case, the time to complete the task
is t ′ = ω0 + max{ω1,max{5 − ω0, 0} + 1} + 1. Since t ≥ t ′, it is
obvious that the first input stream is worse than the second one.
Given a network traffic pattern in a fixed time period,
the number of data items which can be sent between a pair
of communicating devices is a constant. However, the same
number of data items may trigger different number of executable
computations. A best input stream maximizes the utilization of
a computing device for all numbers of leading data items under
any network traffic pattern. Thus, finding a best input stream is
different from other optimization problems such as the traveling
salesman problem which only minimizes one objective, i.e. the
total length of a path. It is known that the problem of input
stream finding is NP-Hard in the strong sense [5]. Heuristic search
algorithms are often employed to solve the class of problems.
In general, the algorithms rely on an effective mechanism for
guiding the search toward a promising direction. The contribution
of this paper is twofold: the computation-progressive (CP) graph is
developed tomodel incremental computing tasks and the assessing
function is proposed for assigning scores to the input streams of an
0743-7315/$ – see front matter© 2008 Elsevier Inc. All rights reserved.
doi:10.1016/j.jpdc.2008.05.002
Author's personal copy
C.-C. Lin, H.-Y. Yin / J. Parallel Distrib. Comput. 68 (2008) 1297–1304 1299
(a) Expression tree. (b) CP graph.
Fig. 2. Comparison of an expression tree and the CP graph for the summation task.
(a) CP graph. (b)Ωi functions.
Fig. 3. CP graph for the product of two polynomials and itsΩi functions.
(a) CP graph. (b)Ωi functions.
Fig. 4. CP graph for matrix multiplication and itsΩi functions.
3.2. The problem of input stream finding
Let the set of input data be {d0, d1, d2, . . . , dN−1} and ρ be a
permutation function on the integer set {0, 1, 2, . . . ,N − 1}. An
input stream I is a sequence of data items in which the data-
sending order dρ(0)dρ(1)dρ(2) · · · dρ(N−1) is determined beforehand.
In the incremental computing mode, the computing device tries to
do as much work as possible using the incoming and previously
received data items. A dominant input stream I maximizes the
utilization of a computing device for all numbers of leading
data items under any network traffic pattern. Denote Ik =
dρ(0)dρ(1)dρ(2) · · · dρ(k) and Fk as the numbers of CPU cycles needed
to complete the executable computations triggered by a given Ik.
Fk is defined using a 2-tuple (Gcp, Ik), where Ik is the input stream
to the task Gcp. Fk can be calculated using theΩi function of Gcp and
it is equal to
∑m−1
i=0 Ωi(xi), wherem is the number of computation
vertices. In Fig. 1(a), x0 = 2, x1 = 2, x2 = 1 and x3 = 1. Thus, the
CPU cycles needed to complete the computations is 6.
Definition 1. An input stream I of length N is a dominant input
stream to a task if Fk ≥ F ′k for 0 ≤ k < N , where I ′ is any other
input stream to the task.
The problem of input stream finding is defined as one which
decides whether there exists a dominant input stream for a given
CP graph or not, where the CP graph is an instance of the task
class solvable using incremental computing mode. Unfortunately,
it has been proved that the problem is NP-hard [5]. The heuristic
algorithms which improve their solutions by repetitively selecting
the superlative from the feasible solutions are often employed to
solve the class of problems. Thus, effectivemechanisms are needed
by the heuristic algorithms to evaluate input streams. Since Fk
is the number of executable computations triggered by an input
substream Ik, it can be used to assess the advantages of the input
substreams with length k. However, for a pair of input streams I
and I ′, it is possible that Fi > F ′i and Fj < F
′
j . Thus, it is obvious
that a novel assessing scheme which evaluates input streams at all
lengths is needed for finding a dominant input stream.
4. CP graphs for applications
In this section, the CP graphs of several practical tasks are
given as examples. They are product of two polynomials, matrix
multiplication and FFT computation.
The product of two polynomials A(x) = ∑ji=0 aixi and B(x) =∑k
i=0 bixi is defined as C(x) =
∑j+k
i=0 cixi, where ci =
∑
r+s=i aras.
Fig. 3(a) shows the CP graph of multiplying two polynomials with
degrees equals 3.
In the graph, ai and bi, 0 ≤ i ≤ 3, are data vertices. The Ωi’s
associatedwith the computation vertices are given in Fig. 3(b). The
next example is matrix multiplication. Let A = [aij] and B = [bij]
be n by nmatrices. Although there are algorithms which compute
matrixmultiplicationwith time complexity less thanΘ(n3) [1], we
consider themost frequently used algorithmwith time complexity
Θ(n3). The product of A and B is C = [cij], where cij =∑n−1r=0 airbrj.
The CP graph of multiplying two matrices with n = 3 is shown in
Fig. 4(a). In the figure, aij and bij are data vertices for 0 ≤ i, j ≤ 2.
The Ωi’s associated with the computation vertices are given in
Fig. 4(b).
FFT is a method to compute Discrete Fourier Transform of a
polynomial with degree N − 1 in time complexity O(N logN).
Author's personal copy
C.-C. Lin, H.-Y. Yin / J. Parallel Distrib. Comput. 68 (2008) 1297–1304 1301
Table 1
The αI values of the typical input streams for the three applications
(a) αI for product of two polynomials.
Number of data items 32 64 128 256 512
Traditional 0.76271 0.75610 0.75299 0.75148 0.75074
Genetic0 0.86810 0.94717 0.96464 0.98607 0.98785
Genetic50000 1 1 1 0.99997 0.99991
Dominant 1 1 1 1 1
(b) αI for matrix multiplication.
Number of data items 72 128 200 450 800
Traditional 0.53981 0.52744 0.52083 0.51292 0.50934
Genetic0 0.72699 0.71442 0.70217 0.68043 0.67595
Genetic50000 1 1 1 0.98405 0.93947
Dominant 1 1 1 1 1
(c) αI for FFT computation.
Number of data items 32 64 128 256 512
Traditional 0.45855 0.45192 0.45230 0.45534 0.45910
Genetic0 0.52396 0.42728 0.37224 0.29753 0.25691
Genetic50000 1 1 1 0.90755 0.69556
Dominant 1 1 1 1 1
6. Evaluation of the assessing function
In this section, the usefulness of our assessing function in
search of a dominant input stream is demonstrated using a genetic
algorithm. The dominant input streamswhich have found for some
restricted class of instances [5] are used to serve as the base for
evaluating the effectiveness of the assessing function.
6.1. Search based on a genetic algorithm
Genetic algorithms are frequently employed to solve NP-hard
problems. The performance of a genetic algorithm depends on
the effectiveness of its fitness function. The assessing function
is used as the fitness function of our genetic algorithm for
evaluation purpose. The order-based encodingmethod [6] encodes
an input stream I = dρ(0)dρ(1) · · · dρ(N−1) into an integer string
ρ(0)ρ(1) · · · ρ(N − 1), where ρ is a permutation function on the
integer set {0, 1, 2, . . . ,N − 1}. Our genetic algorithm begins with
a population of integer strings. The score of a string is computed
using the assessing function. The string with a higher score is
given a higher probability to survive and to breed its offsprings.
In a genetic algorithm, the following steps are repeated until
the gth generation is reached: (1) select individuals from the
current population of size p as the parents to breed their offsprings
based on their scores, (2) crossover the parents to breed a new
generation, (3) mutate each offspring with a mutation probability
u, and (4) preserve the best individual of the current generation
to the next generation and replace the current generation with
the new generation. In the selection step, p2 pairs of strings are
selected to breed their offsprings. Note that a string can be selected
repeatedly.When the scores of a population are in a large variance,
it may result in premature convergence and a global optimum can
be missing. Rank selection [2] has been proposed to smooth the
scores in a large variance. In the rank selection, the probability
of selecting string si is rank(si)/(
∑
sj∈P rank(sj)), where rank(si)
is the rank of the string si decided based on its score in the
populationP. The genetic algorithmemploys a two-point crossover
operator to generate new offsprings. By performing the two-point
crossover, cut regions are randomly selected for each pair of strings
and the string pair exchange the elements in their cut regions.
Let a pair of strings si and sj be 3905817426 and 7581209463,
respectively. Assume their cut regions ci and cj are located at the
areas between the 4th and the 8th elements. That is, ci and cj
consist of 58174 and 12094. The sibling strings generated using
the two-point crossover are 3901209426 and 7585817463. Since
there are duplicate elements in the sibling strings, a repairing
operator is needed to fix the invalid strings. Unlike the GeneRepair
operator proposed in [7], our repairing operator uses sibling strings
to fix each other. By performing repairing operation, the sibling
strings switch the duplicate elements outside their cut regions.
For example, after the repairing operator is performed, the sibling
strings 3901209426 and 7585817463 become 3751209486 and
9025817463, respectively. After the crossover step, the mutation
operator randomly selects two non-overlapping areas in an
offspring and switches the elements in the areas with a probability
u. In our experiment, the genetic algorithm is tested on the
applications that their dominant input streams have been found:
the product of two polynomials, matrix multiplication and FFT [5].
The parameters used in the experiment are given as follows: (1)
population size p = 80, (2) mutation probability u = 0.05, (3)
termination condition g = 50 000. Without loss of generality, the
weights of all computation types are set to one.
By adopting our assessing function as the fitness function of the
genetic algorithm, two sets of experimental data are gathered for
each of the applications. The first set of data shows the speed of
convergence and the second set of data shows the patterns of F
sequences for the applications of various input sizes where F =
(F0, F1, . . . , FN−1). Let Id and I denote a dominant input stream and
another input stream to algorithm J, respectively. The fit ratio of
I is defined as αI = Ψ (I,J)/Ψ (Id,J), where Ψ is the assessing
function. The fit ratio of a generation is defined as themaximum αI
of the input streams at the same generation. Table 1 summarizes
theαI values of the typical input streams for the three applications.
The typical input streams are defined as follows.
• Traditionally, the elements of an array are stored in consecutive
memory locations. Similarly, the elements of two dimensional
array are stored in row major order. Thus, we define the
order of sending data in a traditional input stream is the
same as that of the data stored in memory. Traditional input
streams for the three applications are given as follows. For
the product of two polynomials P(x) and P ′(x) with degree k,
the traditional input stream is defined as a0a1 · · · akb0b1 · · · bk.
For multiplying two matrices A = [aij] and B = [bij]
with size k × k, the traditional input stream is defined as
a00a01a02 · · · ak−1k−1b00b01b02 · · · bk−1k−1. For the polynomial
P(x) = ∑kj=0 ajxj, the traditional input stream for computing
FFT is defined as a0a1 · · · ak.
Author's personal copy
C.-C. Lin, H.-Y. Yin / J. Parallel Distrib. Comput. 68 (2008) 1297–1304 1303
(a) Fit ratios. (b) F sequences.
Fig. 8. Fast Fourier transform.
Table 2
Efficiency of input stream EIS
Gap Traditional Genetic0 Genetic500 Genetic8500 Genetic50000 Dominant
Polynomial multiplication
5 0.98069 0.99910 0.99926 0.99936 0.99948 0.99948
10 0.96212 0.99690 0.99748 0.99791 0.99816 0.99816
15 0.94424 0.99377 0.99489 0.99583 0.99611 0.99611
20 0.92701 0.98968 0.99173 0.99295 0.99325 0.99328
25 0.91040 0.98474 0.98791 0.98938 0.98971 0.98974
Matrix multiplication
5 0.82758 0.96026 0.98602 0.99625 0.99625 0.99625
10 0.70588 0.85249 0.92012 0.98574 0.98574 0.98574
15 0.61538 0.72181 0.76275 0.79250 0.79250 0.79250
20 0.54545 0.59076 0.59751 0.59896 0.59917 0.59917
25 0.48007 0.48007 0.48007 0.48007 0.48007 0.48007
FFT
5 0.82714 0.84257 0.95403 0.99097 0.99579 0.99579
10 0.70281 0.66235 0.79380 0.87646 0.93974 0.94465
15 0.60592 0.53260 0.61281 0.66768 0.70865 0.70865
20 0.51813 0.44374 0.49709 0.53697 0.54759 0.54759
25 0.43025 0.38010 0.41542 0.44593 0.44619 0.44619
permuting the data items randomly can be very similar to a
dominant input stream. For multiplying two polynomials with
degree k, one multiplication is followed by at most one addition.
The resulting polynomial has 2k+1 termswhich are reduced from
(k+1)2 terms using (k+1)2−2k−1 additions. It implies thatmost
of the computations proceed in that way that one multiplication
is followed by one addition. Thus, a good input stream for
multiplications is almost a good input stream for additions.
The genetic algorithm takes 16500 and 9500 generations to
find dominant input streams for the product of two polynomials
and matrix multiplication of sizes equal to 128 as shown in
Figs. 6(a) and 7(a). However, for computing 128-point FFT,
an input stream approximating a dominant one is found at a
generation reaching 50000 as shown in Fig. 8(a). It implies that
searching for a dominant input stream of matrix multiplication
is easier than searching for that of multiplying two polynomials
and searching for a dominant input stream for multiplying two
polynomials is easier than searching for that of computing FFT.
The reasons are explained as follows. Let qi be the subsequence
formed by interleaving the elements in the ith column of the
first matrix with those in the ith row of the second matrix.
The dominant input streams for multiplying two matrices with
size k × k are formed by permuting the k subsequences
q0, q1, q2, . . . , qk−2qk−1 [5]. Define σ(i, k) = imod k. The
dominant input streams for the product of two polynomials with
degree k− 1 is aσ(i,k)bσ(j,k)aσ(i+1,k)bσ(j+1,k) · · · aσ(i+k−1,k)bσ(j+k−1,k)
or bσ(j,k)aσ(i,k)bσ(j+1,k)aσ(i+1,k) · · · bσ(j+k−1,k)aσ(i+k−1,k), where 0 ≤
i, j < k [5]. Compared with the dominant input streams for
matrix multiplication, the dominant one for the product of two
polynomials is more restricted. The dominant input stream for
computing FFT is unique and it is arev(0)arev(1) · · · arev(N−1), where
arev(i) is the coefficient of xrev(i) [5]. It is the reason that searching
for the dominant input streams for the product of two polynomials
or matrix multiplication is easier than that for computing FFT.
Let ∆k = Lk − Fk, where Lk is the time interval needed to
receive the data items dρ(0), dρ(1) · · · dρ(k). In [4], it has been proved
that the time T to complete a task is FN−1 + maxN−1k=0 {∆k}. The
equation indicates that the task completion time is determined by
two factors: the data-sending order and the network traffic. Define
efficiency of input stream (EIS) as FN−1/T . Table 2 shows the EIS
for computing the product of two polynomials with degree 127,
multiplying two matrices with size 12 × 12 and computing 256-
point FFT under various network traffics.
In the table, gap is the time interval of receiving two consecutive
data items dρ(i) and dρ(i+1). The table shows that either a smaller
gap or the input stream at a higher generation can increase EIS.
Compared with the traditional input streams of matrix multiplica-
tion and computing FFT, the EIS values of the dominant streams are
improved significantly for gap = 10. The input streams for polyno-
mial multiplication accumulates a large number of computations
in a small time period such that a small gap cannot consume the
executable computations. The improvement on EIS is obvious for
gap=120, where the EIS values for the traditional input stream and
genetic50000 input stream are 0.63920 and 0.76821 respectively.
計畫成果自評 
 
此計畫目前有一篇論文已於 EI 國際研研討會(CIT2008)發表，兩篇已於 SCIE 期刊(JISE 與 JPDC)
發表，一篇於國內研討會(NCS2007)發表，另有兩篇正在撰寫中。總體來說，此計畫的執行成
效優良，目前需要加強的部份在於整合已發展出來的技術。希望藉由來年更多研究生的加入能
整合已發展出的機制。發表之論文表列如下： 
 
Cho-Chin Lin and Hao-Yun Yin, 2008 September, On the assessment of input streams for 
incremental network computing, Journal of Parallel and Distributed Computing, vol.68, 
pp. 1297-1304. (SCI, EI)  
 
Cho-Chin Lin, On message sequences for incremental network computing, 2008 July, 
Journal of Information Science and Engineering, vol. 24. no. 4, pp. 1023-1040. (SCIE, 
EI) 
  
Cho-Chin Lin and Chun-Wei Shih, 2008 July, An efficient scheduling algorithm for grid 
computing with periodical resource reallocation, Proc. IEEE 8th International Conference 
on Computer and Information Technology,pp.295-300. (EI) 
 
林作俊、許致軒, 2007 December, Linux 叢集電腦程序群組即時監控系統之實現, 全國計算
機會議論文集，pp. 683-691 
 
 
 Proceedings 
 
 
2008 IEEE 8th International Conference 
on Computer and Information Technology 
 
CIT 2008 
 
8-11 July 2008 • Sydney, Australia 
 
Editors 
Qiang Wu, Xiangjian He, Quang Vinh Nguyen, Wenjing Jia and Maolin Huang 
 
Organized by 
CIT Organizing Committee 
 
In cooperation with 
IEEE Computer Society  
IEEE Technical Committee on Scalable Computing  
University of Technology, Sydney 
Research Institute for Information and Communication Technology,  
Korea University, Korea  
BK 21 Information Technology Division, Korea University, Korea  
ARC Research Network in Enterprise Information Infrastructure (EII), Australia  
Federation of Chinese Scholars in Australia (FOCSA), Australia  
Australian Chinese ICT Professional Society, Australia 
completion time of an application by minimizing the max-
imum execution times for the computing nodes. The ﬁrst
step is graph coarsening which is employed to reduce the
communication cost needed by the subtasks. The second
step is initial partitioning in which the coarsened subtasks
are assigned to the computing nodes. The third step is re-
ﬁnement and optimization which further reduce the max-
imum execution time by reassigning the subtasks to other
computing nodes. In [6], an algorithm called QM was pro-
posed. The algorithm coarsens the application graph until
the number of tasks is less than a threshold. Then, each
task in the coarsened graph is mapped on a randomly cho-
sen computing node. To improve the mapping quality by
reassigning the subtasks to other computing nodes, the the-
orem regarding the candidate node selection is provided for
minimizing the search time. In [8], the authors chose dif-
ferent scheduling algorithms for various task arrival rates.
If the task arrival rate is high, the batch mode scheduling
strategy based on a genetic algorithm is employed to ob-
tain high throughput and utilization. If the task arrival rate
is low, the scheduler immediately assigns the task to the
computing node where the task can be ﬁnished earliest. In
[7], the authors proposed a task scheduling algorithm for a
parallel application which can be modeled using a layered
diagraph. In a layered diagraph, the subtasks in the same
layer can be executed independently and the outputs of the
subtasks in the previous layer are used as the inputs of those
in the subsequent layer. The scheduling algorithm reassigns
anticipated subtasks to faster computing nodes if the migra-
tion of the subtasks can overlap with the computations of
other subtasks in the same layer. In [3], an algorithm called
FastMap was proposed to exploit a hierarchical resource
management infrastructure on the grid to distribute the over-
head of mapping among a tree of schedulers. FastMap con-
sists of three phases: task clustering, cluster mapping and
recursive distribution. The ﬁrst phase contracts the task in-
teraction graph so that the number of task clusters equals the
number of resource clusters. The second phase uses genetic
algorithms to map task clusters onto the resource clusters.
The third phase recursively distributes each of the mapped
task clusters to its respective resource cluster. In [2], two al-
gorithms called minmin and maxmin were proposed. Both
algorithms determine, for each unassigned task, the earli-
est time the task can be completed given the idle time of
each machine and the estimated execution time of the task
on each machine. The algorithms differ in their selection of
which task to assign next by given these minimum complete
times. The maxmin algorithm selects the task that will take
the maximum time to ﬁnish, whereas the minmin algorithm
selects the task that could ﬁnish in the minimum time. In
[5], the scheduling algorithms MQD and SIL were proposed
for computationally intensive applications and data inten-
sive applications, respectively. Since a task can launch or
exit on a grid computing platform at any moment, the work-
load of the computing nodes change dynamically. Thus, it is
impossible to precisely know the computing powers of the
nodes when a task is submitted to the grid. The authors pro-
posed a strategy to estimate the computing power for each
node. Based on their estimation, a higher rank is given to
a faster node and the task of larger size is assigned to the
computing node with higher rank. The algorithms also em-
ploy duplication heuristics to further reduce the execution
time.
3. Models
In this section, a task model is proposed to character-
ize the task execution mode for which our scheduling al-
gorithm is developed. In addition, a computation model is
also proposed to capture the characteristics of a grid plat-
form in which the resource broker periodically reallocates
resources.
3.1. Task model
In this paper, a task is a parallel application which can be
decomposed into several subtasks. The subtasks are clus-
tered into groups based on their data dependency. The sub-
tasks in one group work independently, thus, no communi-
cation is needed for those in the same group. However, the
outputs of the subtasks in a group are used as the inputs of
those in another group. The task execution goes through of
several stages. The subtasks in the same group are all exe-
cuted in one stage. We deﬁne stage 0 as the ﬁrst stage. The
output of a subtask is written to the local depository of the
computing node where the subtask is performed. The sub-
tasks to be performed in the subsequent stage access their
input from local or remote depositories before they proceed
to work independently without further communication.
Based on the above, the weighted layered digraph G =
(V0 ∪ V1 ∪ · · ·Vs−1, E1 ∪ E2 ∪ · · ·Es−1) is proposed to
model the task execution mode in consideration. The ver-
tices in Vi form the ith layer and they are executed in stage
i. Let vij be a vertex in Vi. Denote w(vij) as the weight of
vertex vij which is the number of computations needed by
subtask vij to complete its work. The edges in Ei connect
the vertices in Vi−1 to those in Vi. If the output of sub-
task vi−1,h is used as the input of subtask vi,k, then edge
(vi−1,h,vi,k) ∈ Ei . That is, an edge is used to represent the
existence of data dependency between two subtasks in the
neighboring layers. It is obvious that the stages of a task and
the layers of its corresponding weighted digraph is a one-to-
one correspondence. Denote G+1 = (V ∪ V+1, E+1) as
the subgraph which consists of the vertices in the th and
 + 1th layers as well as the existing edges for connecting
the vertices in V ∪ V+1 of the digraph G. The subgraph is
296
the latter. In general, each subtask of a parallel application
may have more than one predecessor. The predecessor with
the smallest index is named as the leading predecessor. De-
ﬁne the home node of a subtask as the computing node on
which the leading predecessor of the subtask is performed.
When PR2 algorithm begins to compute a mapping solu-
tion, all the subtasks in layer +1 are assigned to their home
computing nodes by default. However, PR2 algorithm can
reallocate a subtask in layer  + 1 from its home node to
another computing node. Note that a computing node with
maximum workload is named as a critical node. According
to the deﬁnition, a layer execution time equals the execution
time of a critical node in the layer. Detail steps of the PR2
algorithm are given in Figure 1. At step 2, the completion
times for all the computing nodes are calculated using the
function NodeCT and the node with maximum workload is
extracted at step 3. At step 6 of the algorithm, the subtasks
in critical node pmaxloadp form the emigrant list. The sub-
tasks in the list are the candidates to migrate out for reduc-
ing the layer execution time. PList at step 8 is the list formed
by excluding the node with maximum load from CTimeList.
From step 11 to step 13, the workloads of the other nodes
are checked one by one to determine the target computing
node for the emigrant candidates. PR2 algorithm ﬁrst ﬁnds
the target computing node for the largest subtask in the em-
igrate list. It searches all but the critical node for the tar-
get node in the order of increasing workload. The function
MigrateTest at step 13 tests the eligibility of migrating an
emigrant candidate to another computing node. A subtask
can migrate to a computing node if assigning the subtask to
the target node reduces the layer execution time. Once a tar-
get computing node is found for the current largest emigrant
candidate, the layer execution time is updated as shown in
step 17; otherwise, the current largest emigrant candidate is
deleted from the candidate list and it will be executed at its
original node. The migration process repeats until the al-
gorithm checks all the emigrant candidates or the execution
time of the original critical node is less than the updated
layer execution time as shown in step 9. At the end of the
migration process, the layer execution time is updated and
a new critical node is identiﬁed. The algorithm exits if the
critical node in the next iteration is identical to the critical
node in the previous iteration as shown in step 4.
5. Experimental results
We ﬁrst describe the parameter setting in the simulator
regarding our computation model. In the simulator, the
scheduler ﬁnds a mapping solution for the subtasks in the
next stage. The execution of each subtask goes through two
phases: the communication phase for accessing input data
and the computation phase for performing the computations
speciﬁed by the subtasks. Without being reallocated, each
Algorithm PR2(G+1,R+1,A,C)
1. temp ←− −1;
2. CTimeList←− NodeCT (G+1,R+1,A,C);
3. maxloadp←− MaxLoad(CTimeList);
4. while (maxloadp = temp) do
//migrate out the subtasks from the node with maximum load
5. temp ←− maxloadp;
6. JobList←− { i | where Ai,maxloadp = 1 and
0 ≤ i < |V+1|};
7. layertime ←− CTimeListmaxloadp;
8. PList ←− Exclude(CTimeList,maxloadp));
9. while( (maxloadp = temp) and JobList = ∅)
//select a job with maximum size to migrate out
10. maxtimej ←− ExtractMaxJob(JobList);
11. repeat
//select the node with minimum load as its target
12. minloadp ←− ExtractMinLoad(PList);
13. agree ←− MigrateTest(maxtimej,minloadp,
maxloadp, layertime,G+1,R
+1,A,C);
14. if ( agree=true) then
15. Amaxtimej,minloadp ←− 1;
16. Amaxtimej,maxloadp ←− 0;
17. CTimeList←− NodeCT (G+1,R+1,A,C);
18. maxloadp← MaxLoad(CTimeList);
19. endif;
20. until(agree=true) or (PList = ∅);
21. end while;
22. end while;
EndAlgorithm;
Figure 1. Scheduling Algorithm PR2
of the subtasks in the subsequent stage will be performed at
its home computing nodes. The subtask migrate to another
node if the execution of the subtask at the chosen node leads
to a smaller layer execution. Our experiment are conducted
for 6, 12, 24, 48 and 96 available computing nodes in the
subsequent stage. Three types of computing nodes pα, pβ
and pγ of various processing speeds exist in the gird, where
pα = 200 ops (operations per second), pβ = 500 ops and
pγ = 1000 ops. The number of computing nodes for each
type is p/3, where p is the number of available computing
nodes. We also assume that the communication capabil-
ity between a pair of communicating nodes depends on the
processing speed of the node which receives data. Thus,
there are three levels of communication capabilities in the
grid. They are denoted by a 3-tuple (cα, cβ , cγ). In this ex-
periment, the three levels of communication capabilities are
20, 100 and 200 measured in data items per second. Thus,
there are six different combinations for (cα, cβ , cγ).
In the experiment, the subtasks and their predecessors
are described by graph G+1. Two types of subgraph G+1
are given in this simulation. The common characteristics of
the subgraphs are described ﬁrst. The number of subtasks
to be executed at each stage is 100. That is, |V| = |V+1| =
298
