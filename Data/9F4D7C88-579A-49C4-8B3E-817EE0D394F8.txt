2（一）計畫中文摘要：
關鍵詞：文件分群、分群演算法、資訊檢索、資料探勘、知識開發
隨著網路的蓬勃發展，在網路可以獲得的資訊越來越多元化，然而也伴隨著資訊過量
的問題產生。使用者收集資訊的方式，從以往獲得資訊的不足變成了資訊過量，導致使用
者總是擁有大量而缺乏處理的資訊。因此如何幫助使用者整理龐大的資訊並且加速獲得真
正有用的資訊正是文件分群系統的目的，也是我們研究的動機。
分析目前中文文件分群技術的發展，至今仍存在許多困難需要解決，例如中文斷詞、
高分群維度以及如何設定分群數目的問題，這些問題都影響著分群系統的效率與正確率。
本研究提出一套結合詞庫斷詞技術與複合詞偵測的中文斷詞系統，在文件分群的處理上則
是採用類神經網路技術中 SOM（Self-Organizing Maps）的聚類方法，設計動態分群改善分
群群數設定的問題，並以詞分群取代傳統以文件向量為主的分群方法，強調詞分群的優點
能有效改善分群維度過大的問題。透過實驗的分析，證實我們的以詞分群為主的分群系統
能夠得到良好的分群正確率。
4報告內容
(一) 前言
在網路的蓬勃發展下，雖然帶給我們生活上更多的便利，然而也伴隨著資訊過量
（Information Overload）的問題產生。使用者收集資訊的方式，從以往獲得資訊的不足變
成了資訊過量，導致於使用者總是擁有過多的資訊。這些大量而缺乏處理的資訊反而變成
使用者另一方面的困擾，如何在這些雜亂無章的資訊中找出符合使用者需求的資訊成為我
們急欲解決的問題。
資訊檢索（Information Retrieval）[11][34]就是因應網路快速發展而產生的課題，如何
從網路上大量流通的資訊中迅速取得符合使用者需求的資訊成為資訊檢索最主要的目的。
資訊檢索領域中的文件自動分群技術更是解決資訊過量的最佳途徑，透過文件自動分群技
術幫助我們整理從網路上獲得的資料，幫助使用者不再需要面對動則數以百計的文件，而
是透過文件分群的結果建立對整體文件集的概念，加速使用者需求的搜尋。此外，文件分
群的技術不止於應用在文件的處理上，更可以幫助網路使用者的建立個人化資訊[39]，達
到個人化的網路資訊呈現。比方透過分析使用者喜好的文件類型，有效率的幫助使用者達
到資訊過濾的處理。就是因為文件分群技術在這資訊爆炸的網路時代提供我們一條吸收資
訊更有效率的途徑，更吸引我們深入的研究文件分群技術。
不過網路文件是由自然語言所構成，一個文件分群系統可能因為各領域的專業知識、
語言或是語法的不同遭遇不同的問題，使得目前尚無一文件分群系統能解決所有的分群問
題，只能就遭遇的問題產生相對應的解決方案。而本研究的分群系統著重於中文文件的分
群研究，並有鑒於結合資料探勘（Data Mining）廣泛應用在資料庫分析，本研究遂以相關
技術萃取文件中最常出現的詞彙組合作為中文文件的分群知識，再結合 SOM[20]分群技術
實做一個中文文件自動分群系統。
(二) 研究目的與文獻探討
我們研究的主要目的就是實作一個完整的文件分群系統，首先探討中文斷詞的方法，
以資料探勘技術萃取文件中最常出現的詞彙組合作為中文文件的分群知識；接著以另一種
角度分析文件分群器的設計，我們的系統以詞為出發點，利用詞分群的結果代表每一群文
件的分群代表詞，得到較小更有效率的文件分群器，能解決以文件為主造成的向量維度過
大的問題。此外，我們的詞分群也試圖解決無法事先得知分群群數的問題，利用動態分群
的技術分析文件的分群群數取代事先設定群數的方法。在實驗成果中，證實我們的文件分
群器普遍比以文件為主的分群器得到更好的分群效果。
就文件分群的功能而言，文件分群能純粹的依據文件內容自動將相關的文件群聚，而
不需要知道文件的類別資訊或者是經由事先的分群訓練。就文件分群的應用來看，文件分
群的技術被廣泛的應用在資訊檢索（Information Retrieval）的領域上。在初期研究中，文
件分群技術主要用於增加文件檢索系統的正確率[23][35] ，目前的研究則著重於幫助使用
6現資料集聚類後的結果，每一個點代表為一群，群數則由使用者定義。此外SOM的輸出層
設計具有鄰近區域（Neighborhood）的概念，也就是群與群之間存在相對位置的概念。(3)
網路連接：每個輸出層的點與所有輸入層的點相連接，而且所有連接的線具有加權值。針
對每一個輸出層類別而言，與輸入層所連接的線可構成向量，用以表示每一個類別的質心。
而線的加權值初始時為任意設定，隨著資料的輸入而調整其值。(4)分群流程：SOM聚類方
式的主要想法在於調整每一群質心向量的值。一開始每一群質心向量的值是任意設定的，
SOM的聚類則透過資料向量的輸入，將這筆資料屬於的質心向量調向此資料，如此每一群
的質心向量能逐漸的代表屬於它這群資料。
面對不同的分群資料量，我們無法預知分群數目是否足以表達資料的分群狀況。為了
解決這個問題，SOM 發展出許多動態輸出層的架構，主要分為階層式與非階層式兩種類型。
圖 1 WEBSOM—文件向量
WEBSOM[15][19][20][24]採取的是標準的 SOM 分群架構，主要的系統目的是希望透
過 SOM 將大量的文字文件分群並提供分群結果的視覺化呈現[25][38]，讓使用者可以透過
SOM 的輸出層快速瀏覽與查詢文件集[18]。觀察 WEBSOM 的分群架構大致分為詞分群與
文件分群兩個部份。系統的分群維度隨著詞的增加而成長，分群維度越大不但影響分群的
效率，不當的分群維度也會影響分群的正確率。因此 WEBSOM 採用詞分群的技術降低文
件向量的維度大小；假設詞分為 k 群，文件則根據詞出現在每一群的次數編為 K 維的向量，
如圖 1。經過詞分群的處理與得到文件向量後，開始文件向量的分群動作；在這個步驟中
WEBSOM 採取標準的 SOM 作為文件分群的工具，輸出層的網路連結採用 6 角形的架構。
GSOM 顧名思義是一種成長的分群架構，為了解決分群群數無法預知的問題，GSOM
提供另一種解決的模式，分群開始時設定較小的分群群數，接著再依據分群狀態，決定是
否增加分群群數。而此 GSOM 的架構中最重要的環節在於分群狀態的評量與新增類別的
處理。如何的判斷每一群的分群好壞，在[1]中是藉由計算每群的文件向量與質心的距離來
評量這群的分群狀態；若是分群效果好，屬於這群的文件向量應該與質心距離近，相反的
若整體的文件向量與質心的距離遠，表示此群的分群結果不好。而新增類別的處理就是以
新增加群的方式降低整體文件向量與質心的距離，提高分群品質。
HSOM 的目的同樣是動態新增類別，與 GSOM 的不同點在於 HSOM 是以階層式類別
的建立提高分群品質，而何時建立階層式的依據可以採取如同 GSOM 的分群評量方式，展
開分群狀態不好的群，也有論文採用分水嶺技術來判斷是否產生階層式[8]，屬於比較特別
寬頻 加值 服務 五年 64% 成長 VoADSL 阿爾卡特 研發 技術 架構
ADSL 寬頻語音服務 台灣 阿爾卡 陳俊明 ADSL 上網 VoADSL …
寬頻
加值
研發
技術
台灣
（… 2, 2, 1, 1, …）
架構
8調整信心度(confidence)作為複合詞多寡的挑選，相對於傳統方式的統計斷詞方式，我們的
技術能有效減少時間與空間的浪費。
根據實驗，對500篇的文件集作斷詞處理產生近10000個不重複的詞，而我們欲辨識一
篇文件所表達的內涵只需五至十個詞。例如一篇文件中出現SARS或疾病的詞彙，我們就能
判別出這篇文件的主題。由此不難推論大部分的詞對於文件分群而言不具效用，而且會大
幅影響分群速度，因此我們必須先過濾這麼龐大的斷詞結果。我們藉由計算每篇文章中出
現的詞的權重來做過濾，當得到文件中每個詞的詞權重，我們挑選每篇文件中一定比例詞
權重較高的詞當作分群的關鍵詞。由關鍵詞分群再進行文件分群的優點，由分群資料量的
觀點來分析，文件可以無限量的成長，而詞卻存在一定的數量的組合；由分群精確率來討
論，考慮文件之間的差異需憑藉的文件內容中關鍵詞的差異，當直接考慮關鍵詞間的差異，
對於文件分群的精確性應能有所提升。如何進行關鍵詞分群，我們採用向量空間模組，將
關鍵詞轉換成有意義的向量，再藉由SOM的聚類以動態類別產生的方式將詞分群，有效的
將相關的關鍵詞聚類，而相似的關鍵詞群處於鄰近位置。經由此關鍵詞分群結果再將文件
做分群，相信可以提供更好的文件瀏覽模式。然後將詞分群架構做為我們展現文件的工具，
計算文件與每一個詞群的相似度，將文件分配到詞群中；並由每一群的文件中挑選具代表
性的關鍵詞作為文件集標記，加速使用者瀏覽文件的速度。
舊有的複合詞偵測方式如 N-gram 技術或是統計方式，藉由設定最大可能的詞長度，考
慮可能的單字詞組合形成複合詞的機率，處理過程中龐大的計算複雜度與空間的浪費是最
大的缺點。因此本系統採用 DHP 技術有效減少資料集掃描次數的特點[6]，修改 DHP 演算法
為我們系統複合詞偵測的技術，以節省時間與空間上的浪費。而 DHP 的關鍵技術在於使用
建立雜湊表格（hash table）的技巧，大幅降低長度為二的可能項目集個數，並透過已偵
測到的大項目集對於資料庫進行篩檢。
由於複合詞是連續的詞所組合，因此必須修改DHP的處理過程，我們藉由修改雜湊表格的建
立方式達到我們的需求；另外最小支持度的的過濾對於複合詞的偵測也不盡合理，我們不
採取最小支持度的做法，改用修改的最小信心度，直接考慮詞與詞的組合為複合詞的可能
性。建立雜湊表格時，我們擷取每筆交易中兩兩連續的項目組合丟入雜湊表格中，並記錄
我們所產生的所有可能組合作為我們的可能項目集。此外，由於以文章作為交易，每筆交
易存在多個相同的項目。我們考慮到複合詞必須為多數的作者或是文章所認同，因此我們
的系統針對每篇文章只會累加一次相同的項目集，而不考慮每筆交易中項目次數。
透過此信心度的設定，我們可以保證長度較大的複合詞中的子複合詞同樣也滿足最小
信心度。舉例來看，若A,B,C滿足最小信心度，其子複合詞的組合A,B與A,C同樣滿足最小信
心度。因為分母為所有項目的聯集出現次數，A,B的出現聯集出現次數一定比A,B,C的出現
聯集出現次數小；而分子中為所有項目一起出現的文件數，A與B一起出現的限制較小也保
證比A,B,C一起出現的次數要來的大。因此透過此信心度來篩選複合詞，並不會造成整個偵
測過程中的困擾。利用改後的信心度，我們只需在0~1之間調整最小信心度，當最小信心度
設定的越靠近1，得到的複合詞越精確也越少；相反的越靠近0，我們得到的複合詞越多也
越沒有意義。
10
而整體的分群效果則綜合每一群的內部相似度得到整體的分群品質（quality of
cluster），公式表示如下。當分群到最後為一群一筆資料時，每一群的內部相似度為0，整
體的分群品質為0。
||
1
)(___
Cluster
cSimilarityIntraClusterofQuality
clusterc
i
i
 

我們系統的動態分群架構從兩方面設計，分別為同層間的動態類別成長與階層式類別
展開兩部份。首先我們設定分群群數為2，藉由評量分群的結果是否達到我們的要求，決定
是否新增一個新的類別，而階層式類別則是針對每一個類別進行更精確的分群分析，同樣
是依照每一群的評量來決定階層式類別的建立。下面章節將詳細介紹這兩部份動態類別成
長的設計。
首先我們將未分群的文件集視為一群，計算這一群的內部相似度做為此資料集的最初
分群品質值，接著設定介於 0~1 之間的成長閥值，以此成長閥值要求分群的品質。SOM 開
始聚類時我們設定分群群數為 2 群；當聚類停止時評量分群品質值，假設分群品質值小於
最初的分群品質值乘以成長閥值即停止成長，否則就分群品質最差的群展開新的一群，而
新群的質心以此群中距離質心最大的資料向量表示。並重新開始 SOM 聚類，重複執行直
到停止類別的成長。
當類別成長的步驟完成即開始階層式類別的建立，而階層式類別的目的在於以階層式
的方式加強分群的品質。同樣的，我們設定 0~1 之間的階層式類別成長閥值，當某群的分
群品質值大於上層的分群品質值乘以此階層式閥值時，我們將由此群展開階層。以此群的
關鍵詞為輸入並設定分群群數為 2，開始動態類別成長的處理，如上節所示，每次新增一
群直到分群品質達到一定比例的要求。
而文件如何分配給詞群則藉由計算文件與詞群的相似度計算，將文件分至相似度最高的詞
群中。此處相似度計算方式採用共同關鍵詞出現次數來評量，以文件 id 中出現的關鍵詞個
數為準，計算詞群 jc 與文件 id 共同擁有的關鍵詞數目。透過重新取得分群關鍵詞的方式，
階層式類別中產生的分群關鍵詞與上層的分群關鍵詞將有所不同。而這樣的設計也能符合
文件的特性，能確實以階層式分群的方式分辨出同一群文件內的次主題類別，好比之前所
舉的環保事件的例子。
為了能準確的評估相異演算法間分群效果的優劣，我們採取相同的斷詞處理與分群關
鍵詞的挑選，並設定相同的分群群數。我們以資料集（一）作測試，關鍵詞挑選方式為一
篇文章挑選前 2%較高 TF*IDF 值的詞，得到的文件分群正確率如圖 2。
我們的分群演算法得到較好的分群正確率，在分群群數為真實群數 10 群的時候具有最
高的分群正確率 85%，而且我們的動態成長類別能真實反應文件集分布狀況，當分群群數
不符合真實群數時具有較低的正確率。但是 K-means 與 Bisecting K-means 則因為任意設
定分群質心的問題產生震盪的分群正確率曲線，在分群群數較高的情狀下反而得到較高的
正確率，不符合文件真實狀況的分佈。
12
群有所幫助，在文件摘要的處理或是對於文件推薦系統等等的應用都能提供更好的效果。
而動態關鍵詞分群的部份，藉由詞向量的建立並以 SOM 為基礎設計動態的詞分群處
理，相較於傳統以文件為主的文件分群系統，我們的系統產生比較有效率的分群器並改善
了分群維度過大的問題；而動態分群的設計同樣的改善大部份系統面臨的分群群數設定問
題。而 SOM 的輸出層採用一維的輸出架構，採用樹狀的瀏覽方式更能提供使用者快速與
熟悉的文件集瀏覽。
參考文獻
[1] D. Alahakoon, S.K. Halgamuge, and B. Srinivasan,“Dynamic Self-Organizing Maps with
Controled Growth for Knowledge Discovery”, IEEE Transactions on Neural Networks,
vol. 11, Pages 601-614, 2000.
[2] A. Azcarraga, A. Gopez and T. J. Yap,“Word-Streams for Representing Context in Word
Maps”, 7th International Conference on Neural Information Processing , Taejon, Korea,
Nov 14-18, 2000.
[3] L.D. Baker and A.K. McCallum,“Distributional Clustering of Words for Text 
Classification”, ACM SIGIR, Pages 96-103, 1998.
[4] J. L. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithm, Plenum
Press, 1981.
[5] A. Chen, J. He, L. Xu, F. Gey, and J. Meggs,“Chinese Text Retrieval Without Using a 
Dictionary”, ACM SIGIR, Pages 42-49, 1997.
[6] J. S. Park, M.S. Chen and P. S. Yu, “Using a Hash-Based Method with Transaction
Trimming for Mining Association Rules,” IEEE Trans. On Knowledge and Engineering, 
Vol. 9, No. 5, pp. 813-825, Oct. 1997.
[7] T.H. Chiang, J.S. Chang, M.Y. Lin, and K.Y. Su,“Statistical Models for Word 
Segmentation and Unknown Word Resolution”, In Proceedings of
ROCLING-Ⅴ, ROC Computational Linguistics Conferences, Taiwan, Pages 123-146,
1992.
[8] J.A.F. Costa and M.L.A. Netto,“A New Tree-Structured Self Organizing Map for Data
Analysis”, Proceedings of IJCNN, Pages 1931-1936, 2001.
[9] D. Cutting, D. Karger, J. Pedersen, and J. Tukey,“A Cluster-based Approach to Browsing
Large Document  Colections”,ACM SIGIR, Pages 318-329, 1992.
[10] Michael Dittenbach, Dieter Merkl, and Andreas Rauber,“Hierarchical Clustering of 
Document Archives with the Growing Hierarchical Self-Organizing Map”, Proceedings
of ICANN, Pages 500-508, 2001.
[11] W.B. Frakes and B.Y. Ricardo, Information Retrieval : Data Structure & Algorithms,
Prentice Hall PTR, 1992.
[12] Richard Freeman, Hujun Yin, and Nigel M. Allinson,“Self-Organising Maps for Tree
View Based Hierarchical Document Clustering”, Proceedings of IJCNN, vol. 2, Pages
14
No. 8, Pages 537-546, 1998.
[28] S.P. Lloyd,“Least squares quantization in PCM”,IEEE Transactions on Information
Theory, number 28 in IT, Pages 127-135, 1982.
[29] Jane Morris and Graeme Hirst,“Lexical Cohesion Computed by Thesaurus Relations as
an Indicator of the Structure of Text”,Computational Linguistics, Vol. 17, NO. 1, 1991.
[30] Andreas Nfirnberger,“Clustering of Document Collections Using a Growing
Self-Organizing Map”, Proceedings of BISC International Workshop on Fuzzy Logic and
the Internet , Pages 136-141, 2001.
[31] Jian-Yun Nie, Jiangfeng Gao, Jian Zhang, and Ming Zhou,“On the use of words and 
n-grams for Chinese information retrieval”, IRAL-2000. Hong Kong, September 30 -
October 1, 2000.
[32] J.S. Park, Ming-Syan Chen, and Philip S. Yu,“Using a Hash-Based Method with
Transaction Trimming for Mining Association Rules”, IEEE Transactions On Knowledge
And Data Engineering, Vol. 9, NO. 5, Pages 813-825, 1997.
[33] Carreira Perpinan and M. A.,“A Review of Dimension Reduction Techniques”, Technical
report CS-96-09 of Computer Science of University of Sheffield UK.
[34] B.Y. Ricardo and R.N. Berthier, Modern Information Retrieval, Addison Wesley
Longman Limited, 1999.
[35] C.J. van Rijsbergen, Information Retrieval, Butter-worths, London, 2nd edition, 1979.
[36] G. Salton, Automatic Text Processing: The Transformation, Analysis, and Retrieval of
Information by Computer, Reading, Mass. Workingham: Addison-Wesley 1988.
[37] Fabrizio Sebastiani,“Machine Learning in Automated Text Categorization”, ACM
Computing Survey, Pages 1-47, 2002.
[38] Juha Vesanto,“SOM-Based Data Visualization Methods”, In Intelligent Data Analysis,
Volume 3, Number 2, Elsevier Science, Pages 111-126, 1999.
[39] Kun-Lung Wu, Charu C. Aggarwal and Philip S. Yu,“Personalization with Dynamic 
Profiler”, IBM Research Report , 2001.
[40] D.S. Yeung and X.Z. Wang, “Improving Performance of Similarity-Based Clustering by
Feature Weight Learning”, IEEE Transaction on Pattern Analysis and Machine
Intelligence, VOL. 24, NO. 4, April 2002.
[41] Oren Zamir, Oren Etzioni, Omid Madani, and Richard M. Karp,“Fast and Intuitive 
Clustering of Web Documents”, In Proceedings of the 3rd International Conference on
Knowledge Discovery and Data Mining, Pages 287-290, 1997.
