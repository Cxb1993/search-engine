2and shifting from the mother wavelet. Wavelet-based coding is more robust
under transmission and decoding errors, and also facilitates progressive trans-
mission of images. Because of this inherent multi-resolution nature, wavelet
coding schemes are especially suitable for applications where scalability and
tolerable degradation are important.
Predictive coding works on the basis that adjacent pixels in an image are
generally very similar, and thus the redundancy between successive pixels can
be removed and only the residual between the actual and predicted pixel values
is encoded. Differential Pulse Code Modulation (DPCM) is one particular
example of predictive coding. Entropy coding relies on uneven distribution of
values to be encoded, and the code length of a value associated inversely with
the probability the value occurs in the image. Two popular entropy coding
schemes are Huffman coding and arithmetic coding. Huffman coding aims to
produce variable-length codes for symbols to represent data at a rate of its
entropy. From a given probability function for each value of image pixels, a
tree is constructed by summing the two lowest probabilities iteratively until no
merge can be done. Then each symbol is encoded by tracing the corresponding
path in the tree. In this way, Huffman coding maps fixed length symbols
to variable length codes. Arithmetic coding converts a variable number of
symbols into a variable length code-word. A sequence of symbols is represented
by an interval with length equal to its probability. The interval is specified by
its lower and upper boundaries. The code-word of the sequence is the common
bits in binary representations of its lower and upper boundaries.
1.2 Video Compression
Video data usually contains streams of image frames, each stream consisting
of a set of sequential image frames. Transmitting a video stream requires a
large bit rate and needs a high compression ratio for real applications. Two
types of approaches, block-based and object-based, have been developed for
video compression. In block-based approaches, e.g., MPEG-2 [2], an image
frame is encoded as a set of fixed-size blocks, and redundancies are removed
by attempting to match and reuse the blocks from the previous frames of
the current frame. The most basic form of block-based approaches checks if
a block in the current frame is identical to a block around the same place in
the previous frame. If they are the same, the data of the underlying block
is not encoded. Otherwise, the best matched block in the previous frame is
found and the difference among them is encoded. The area in which matching
is searched affects the quality of the reconstructed image frame. The larger
the area in which matching is searched, the larger the chance that matching
can be found. However, most matched blocks are found around the place
of the original block. Besides, increasing the search area also increases the
computation required.
Object-based approaches, e.g., MPEG-4, treats an image frame as a set of
objects. Instead of coding each individual block of an object, we can represent
4the input to the output perform association between weights and inputs. Not
only the winner of the competition but also its neighbors have their weights
updated according to the competitive rule. The HEC network [44] performs a
partitional clustering using the regularized Mahalanobis distance. It consists
of two layers. The first layer employs a number of principal component anal-
ysis subnetworks to estimate the hyper-ellipsoidal shapes of currently formed
clusters, and the second layer performs a competitive learning using the clus-
ter shape information provided by the first layer. The spiking neural network
[11] consists of a fully connected feedforward network of spiking neurons.
Each data-point is translated into a multidimensional vector of spike-times
in the input layer. If the distance between clusters is sufficiently small, the
winner-takes-all competition tunes output neurons to the spike-time vectors
associated with the centers of the respective clusters. The cluster-detection-
and-labeling (CDL) network [21] consists of two layers and a threshold calcu-
lation unit. The first layer performs similarity matching, whereas the second
layer implements cluster assignments.
Recently, neuro-fuzzy approaches for data clustering have attracted a lot
of attention [35, 66, 55, 69, 68]. Such approaches combines advantages of both
fuzzy theory and neural networks. Fuzzy ART [15] is capable of learning cat-
egories in response to arbitrary sequences of analog or binary input patterns.
Input vectors are normalized according to a complement coding process which
makes the MIN operator and the MAX operator of fuzzy theory complemen-
tary to each other. The fuzzy min-max clustering neural network proposed
in [59] adopts hyperbox fuzzy sets. Learning is done by creating and expand-
ing/contracting hyperboxes in the pattern space. The fuzzy Kohonen cluster-
ing network (FKCN) [61] combines the fuzzy c-means algorithm and KCN.
It offers automatic control on the learning rate distribution and the extent of
topological neighborhood using fuzzy membership values. The fuzzy bidirec-
tional associative clustering network (FBACN) [64] is composed of two layers
of recurrent networks, performing fuzzy-partition clustering according to the
objective-functional method. The first layer of FBACN is implemented by a
Hopfield network, while the second layer is implemented by a multi-synapse
neural network with added stochastic elements of simulated annealing. The
self-constructing fuzzy neural network (SCFNN) [41, 50] is able to partition a
given dataset into a set of clusters based on similarity tests. Membership func-
tions associated with each cluster are defined according to statistical means
and variances of the data points included in the cluster. Besides, parameters
can be refined to increase the precision of the resulting clusters.
2 Neuro-Fuzzy Techniques
As indicated earlier, a lot of neuro-fuzzy techniques have been developed. In
this section, we describe three techniques: FKCN, Fuzzy-ART, and SCFNN.
FKCN is a non-sequential fuzzy neural network, i.e., all the training data
6V =


v1
v2
...
vc

 =


v11 v12 · · · v1c
v21 v22 · · · v2c
...
...
...
...
vn1 vn2 · · · vnc

. (1)
exists between the input layer and the distance layer, where vij is the weight
of the connection between the input node i and the distance node j. Basically,
V is obtained by minimizing the following objective function:∑
k
∑
j
(ukj)
m||xk − vj ||
2, (2)
where ukj is the membership degree of xk belonging to cluster j and m is a
controlling exponent to be calculated later.
During the training, each input pattern xk, where xk = [x1k, . . . , xnk] and
1 ≤ k ≤ N , is presented to the input layer. Each node j, j = 1, 2, · · · , c, in
the distance layer calculates the distance dkj between xk and vj as follows:
dkj = ‖xk − vj‖
2 = (xk − vj)
T (xk − vj). (3)
Then all the weights are adjusted by
vj(t) = vj(t− 1) + [
N∑
k=1
ηkj(xk − vj(t− 1))]/
N∑
k=1
ηkj (4)
where ηkj is the learning rate defined by
ηkj = (ukj)
m, (5)
m = m0 − z∆m, (6)
∆m =
m0 − 1
zmax
(7)
with m0 being a positive constant greater than 1, z being the current epoch
count, zmax being the limit of the epoch count, and ukj being calculated by
ukj =
1∑c
i=1(
dkj
dki
)
2
m−1
. (8)
The process iterates until either that the weight matrix V in two consecu-
tive epochs are close enough or the epoch count z exceeds zmax. The FKCN
algorithm can be summarized below.
procedure FKCN
Set the number of clusters to be c and the limit of epoch count
to be zmax;
Initialize the weight matrix V and choose m0, where m0 > 1;
for z = 1, 2, · · · , zmax
8Fig. 2. Architecture of Fuzzy-ART networks.
i.e., J = J + 1, create a new subnet for xk, as shown in Figure 3, and add
it to the network. Note that vJ is set to xk in this subnet. The Fuzzy-ART
algorithm can be summarized below.
procedure Fuzzy-ART
Set parameters, η, β, and ρ. Initialize J = 0;
for k=1, 2, · · · , N
Input the training vector, xk;
Choose the winner node j∗ by Eq.(9);
Do vigilance test of the winner node by Eq.(10);
while fails the vigilance test, do
Find the next winner node;
if one node passes the vigilance test
then update the weights of the winner node by Eq.(11);
10
2.3 Self-Constructing Fuzzy Neural Networks (SCFNN)
The task of SCFNN is to partition the given data set into fuzzy clusters,
with the degree of association being strong for data within a cluster and weak
for data in different clusters. Let x = [x1, x2, . . . , xn] be an input vector of
n dimensions. A fuzzy cluster Cj is defined as a Gaussian function of the
following form:
Cj = G(x;vj ,σj) =
n∏
i=1
g(xi; vij , σij) (12)
where g(xi; vij , σij) is
g(xi; vij , σij) = exp
[
−
(
xi − vij
σij
)2]
. (13)
where vj = [v1j , . . . , vnj ] denotes the mean vector and σj = [σ1j , . . . , σnj ]
denotes the deviation vector for Cj . Gaussian functions are adopted in SCFNN
for representing clusters because of their superiority over other functions in
performance [71].
Like Fuzzy-ART networks, clusters are generated as needed and the num-
ber of clusters is determined automatically in SCFNN networks. A SCFNN
network consists of three layers: an input layer, a fuzzification layer, and a
competition layer, as shown in Figure 4 [48, 41]. The input layer contains
n nodes. It receives input patterns and broadcasts them to the fuzzification
layer. The fuzzification layer contains J groups each of which contains n nodes.
The corresponding weight vector [v1j , v2j , . . . , vnj ] of each group j represents
the prototype of cluster j. The ith node of group j calculates the Gaussian
function value g(xi; vij , σij). The competition layer contains J nodes. The
output of node j of this layer is the product of all its inputs from the previous
layer, i.e.,
n∏
i=1
g(xi; vij , σij). (14)
Note that the weights {(vij , σij)|1 ≤ i ≤ n, 1 ≤ j ≤ J} between the input
layer and the fuzzification layer are adjustable, and the other weights are fixed
to 1.
Let xk be a training pattern. We define that xk belongs to cluster Cj if
xk contributes to the distribution of patterns in Cj , i.e., vj and σj have to
be recalculated due to the addition of xk. Let Sj indicate the size of Cj , i.e.,
the number of patterns that belong to Cj . Also, we define an operator, comb,
to combine a cluster Cj and xk to result in a new cluster C
′
j , as follows:
C
′
j = comb(Cj ,xk) = G(x;v
′
j ,σ
′
j) (15)
=
n∏
i=1
g(xi; v
′
ij , σ
′
ij) (16)
12
Then we check if xk is similar enough to cluster Cj∗ by the following similarity
test:
n∏
i=1
g(xi; vij∗ , σij∗ ) ≥ ρ (20)
where ρ, 0 ≤ ρ ≤ 1, is a predefined threshold. If xk passes the similarity
test on cluster Cj∗ , we further check the variance of the resulting cluster
C
′
j∗ = comb(Cj∗ ,xk) induced by the addition of xk. We say that pattern xk
passes the variance test on cluster Cj∗ if
‖σ
′
j∗‖ ≤ τ (21)
where σ
′
j∗ is computed by Eq.(18) and τ is another user-defined threshold. If
either Eq.(20) or Eq.(21) fails, the current winner node is deactivated and the
next winner is chosen.
Two cases may occur. First, there are no existing fuzzy clusters on which
pattern xk has passed both the similarity test and the variance test. For this
case, pattern xk is not close enough to any existing cluster and a new fuzzy
cluster CJ , J = J + 1, is created with
vJ = xk, σJ = [σ0, σ0, . . . , σ0] (22)
as shown in Figure 5. Note that the new cluster CJ contains only one member,
pattern xk. The reason that σJ is initialized to a non-zero vector is to avoid
the null width of a singleton cluster. Of course, the number of clusters is
increased by 1 and the size of cluster CJ should be initialized, i.e.,
J = J + 1, SJ = 1. (23)
On the other hand, if there is a winning node j∗ on which pattern xk has
passed both the similarity test and the variance test, pattern xk is close enough
to cluster Cj∗ and the weights of the winning node j
∗ are modified to include
pattern xk by Eq.(18) and Eq.(17), and the size of Cj∗ is increased by 1, i.e.,
Sj∗ = Sj∗ + 1. (24)
Note that J is not changed in this case.
The above process is iterated until all the training patterns are processed.
The SCFNN algorithm can be summarized below.
procedure SCFNN
Set parameters, σ0 ,ρ and τ ;
for k=1, 2, · · · , N
Input the training vector xk;
Choose the winner node j∗ by Eq.(19);
Perform similarity and variance tests of the winner node;
14
3 Neuro-Fuzzy Based Vector Quantization for Image
Compression
As mentioned, vector quantization (VQ) is attractive for image compression
due to its simplicity in decoding at the receiving end. One of the key issues for
VQ is the generation of code-words based on which image blocks are encoded
and decoded. Many methods have been proposed for generating code-words for
VQ [10, 32]. The LBG algorithm [52, 53] is one of the most famous methods. It
starts with a set of randomly selected code-words which form initial clusters.
According to the Euclidean distances from code-words, a training pattern
is clustered to the code-word nearest to it. Data in the same cluster form
a new code-word which replaces the old one. The process repeats until the
variation of average distortion of all clusters is smaller than a predefined
threshold. Modified ART2 [30, 62] is another approach for VQ based on the
ART algorithm. Code-words are constructed gradually as the data are fed
one by one. When the first block of image comes in, the first code-word is set
up. Incoming blocks are compared with existing code-words. If a code-word is
similar enough to an incoming block, the block is categorized to the code-word
and the code-word is modified to include the block. The similarity threshold
increases in each iteration. The algorithm terminates when the number of
code-words reaches the desired one or the threshold reaches the predefined
upper-bound.
VQ can incorporate nicely with neuro-fuzzy clustering methods for deriv-
ing the code-words for image compression. Any neuro-fuzzy clustering method
presented in Section 2 can do the job. Obviously, the code-book size is identical
to the number of clusters and all the vj vectors form the desired code-words. In
this section we describe how the SCFNN clustering method is used to generate
code-words for vector quantization. The fuzzy clusters obtained by SCFNN
have a high-degree of intra-cluster similarity and a low-degree of inter-cluster
similarity. The mean vector of each obtained fuzzy cluster becomes naturally
a code-word. The advantages of using SCFNN include that the fuzzy clus-
ters generated are compact and dense, the real distribution of image content
can be captured, and image content can be represented by code-words more
appropriately.
3.1 VQ Encoding/Decoding
For simplicity, we only focus on gray-level images. Extension to color images
is obvious. Given an original image, I, of Nx by Ny pixels to be transmitted,
namely, 

I11 I12 · · · I1Nx
I21 I22 · · · I2Nx
...
...
...
...
INy1 INy2 · · · INyNx


16
Fig. 8. The VQ-based compression system.
receiving end, a received index is decoded by checking against the code-book
which is the same as that used by the encoder at the transmitting end. The
corresponding code-word is recalled to become the block reconstructed. When
we are done with all the indices received, the whole image is reconstructed
which is an approximate version of the original image at the transmitting end.
3.2 Clustering by SCFNN
As mentioned earlier, the pixels of a given image are divided into NxNy/p
2
blocks and each block contains p2 pixels. We represent each block as a vector
of size p2. Let N = NxNy/p
2. Therefore, we have N training vectors, x1, x2,
. . ., xN , and each training vector xk has p
2 dimensions, i.e., n = p2. These
training vectors are given to the SCFNN algorithm of Section 2.3. Suppose J
clusters are obtained. Then the mean vector of each cluster obtained becomes
a code-word for encoding/decoding.
Note that SCFNN determines the number of clusters by itself. However,
it is desirable that the size of the code-book be 2b. Therefore, we have to
make the number of clusters obtained as close to 2b as possible, i.e., J ≃
2b. This is achieved by adjusting iteratively the values of the two involved
parameters, ρ and τ . Suppose we would like the number of final clusters to be
B = 2b. Initially, each training pattern is treated as a cluster and the number
of clusters is N . We randomly choose some values for ρ and τ and perform
fuzzy clustering. Let the number of clusters generated be L. If L > B, then
we decrease ρ and increase τ by an amount proportional to (L−B)/(N −L).
If L < B, then we increase ρ and decrease τ by an amount proportional to
(B−L)/(N−L). This process iterates until L is close enough to B. Therefore,
a code-book obtained by SCFNN can be summarized below.
procedure Code Book SCFNN
Let B = N ;
while B is not close to 2b do
Adjust ρ and τ appropriately;
B = 0;
for each training block xk, 1 ≤ k ≤ N
18
compression with local code-books have better quality of reconstruction, but
results in a low compression ratio.We do compression with local code-books on
four benchmark images, Elaine, Peppers, Man and Boat, as shown in Figure 9.
All these images are 256×256 in resolution. MSE, SNR, and CR associated
(a) (b)
(c) (d)
Fig. 9. The original benchmark images: (a) Elaine; (b) Peppers; (c) Man; (d) Boat.
with these images are shown in Table 1. Note that 2b in the first column of
these tables indicates the size of the code-book, and the transmission of the
associated code-book is considered in the calculation of CR for each image.
Figure 10(a), Figure 10(b) and Figure 10(c) show the reconstructed images of
Elaine, Man and Boat, respectively.
Benchmark Images (Global Code-book)
Next, we show the performance of image compression using a global code-
book. A global code-book is defined to be one that is used for encod-
20
Table 1. MSE, SNR, and CR for benchmark images using local code-books.
Image MSE SNR CR
Elaine(28) 72.28 24.62 3.76
Elaine(29) 19.26 30.37 1.93
Man(28) 211.37 17.28 3.76
Boat(28) 125.14 21.86 3.76
ing/decoding independent of images to be transmitted. With this idea, com-
pression ratio can be increased since each image is transmitted without trans-
mitting its own code-book together. Usually, we test this idea by encoding one
image with a code-book that is obtained from another image. We compress
Peppers, Man, and Boat based on the code-book obtained from Elaine(28) and
the results are shown in Table 2. Figure 10(d), Figure 10(e) and Figure 10(f)
Table 2. MSE, SNR, and CR for benchmark images using the code-book of
Elaine(28).
Image MSE SNR CR
Peppers(28) 365.45 16.83 64.00
Man(28) 676.87 12.22 64.00
Boat(28) 379.59 17.04 64.00
show the reconstructed images of Peppers, Man and Boat, respectively, based
on the code-book of Elaine(28).
Benchmark Images with Noise
We test the effectiveness of SCFNN with another two benchmark images, Lena
and Bird, contaminated with the white Gaussian noise, as shown in figure 11.
Figure 12 show the reconstructed images obtained using local code-books of
size 256 and 128 of original images, respectively. The values of SNR and MSE
for these reconstructed images are given in Table 3.
Table 3. MSE, SNR, and CR for benchmark images contaminated with noise.
Image MSE SNR CR
Lena(28) 104.31 22.23 3.76
Bird(27) 67.92 24.25 7.21
22
associated with it. This technique allows for overlays that can readily be re-
moved through an application. NITF is more than just an image file format; it
goes beyond supporting the core needed to share imagery between disparate
systems. It facilitates the increasing need for greater flexibility in using mul-
tiple images with annotation in a composition that relates the images and
annotation to one another.
NITF can accept and decompress data that has been compressed using
a VQ compression scheme. Images contained in a NITF file can be in either
color or gray scale. For simplicity, we only consider gray scale images here.
The components of a NITF file, as shown in Figure 13, include:
Fig. 13. NITF file Structure with VQ compressed images.
• NITF File Header. This gives the basic description of the file, e.g., how
many subcomponents such as images, symbols, or texts, exist.
• Image Segments. NITF allows each image contained in the file to be com-
pressed. Currently, bi-level, JPEG, JPEG2000 and VQ are the compression
methods supported by NITF.
• Symbol Segments.
• Text Segments.
• Remaining segments.
The subheader of each image identifies the image compression method used.
If an image is compressed with VQ, then the code-book is placed in the VQ
header followed by the compressed image codes. The VQ header also provides
information about the organization of the code-book, indicating how many
code-words included in the code-book, the size of each code-word, and how
the data that makes up the code-words is organized. As a multi-component
24
and the second code-word is

150 151 150 151
148 148 148 148
135 132 132 131
112 112 113 113

 .
Then the bitstream of the first code-word, i.e., [153 153 153 · · · 154 154 154],
is followed by that of the second code-word, i.e., [150 151 150 · · · 112 113 113],
and so on. The bitstream of the code-book is therefore encoded as [153 153
153 · · · 154 154 154 150 151 150 · · · 112 113 113 · · · · · ·]. Following the code-
book, the indices of the image blocks from left to right and top to bottom are
encoded into another bitstream of compressed image data. Suppose we have
the following image with each image block replaced by its index:


127 55 · · · 13
8 288 · · · 512
...
...
...
...
64 175 · · · 336

 .
The bitstream consists of these indices becomes [127 55 · · · 13 8 288 · · · · · ·
512 · · · 64 175 · · · 336].
Encoding with Row-Based Code-books
NITF allows the organization of the VQ code-book to be optimized for the
specific use of the VQ data. For the row-based scheme, four code-books with
each code-word of 4 pixels in size are created. That is, it stores different
rows of 4×4 code-words in different code-books such that the image can be
reconstructed line-by-line, instead of block-by-block.
The first code-book is used to group row 1 of all the 4×4 code-words
together. For instance, for the previous example, the first code-book has the
following form:


153 153 153 153
150 151 150 151
...
...
...
...

 .
Row 2, row 3, and row 4 of all the 4×4 code-words are placed in respective
code-books as above. Then these row-based code-books are encoded one by
one. The bitstream of these code-books thus looks like [153 153 153 153 150
151 150 151 · · · 155 155 155 155 148 148 148 148 · · · 155 155 155 155 135 132
132 131 · · · 154 154 154 154 112 112 113 113 · · ·]. Note that the quantized
image data remains the same as those compressed with the block-based code-
book scheme.
26
Fig. 16. Spatial decompression with the block-based code-book scheme.
Fig. 17. Spatial decompression with row-based code-books.
28
Fig. 18. Block diagram of the human-object segmentation system.
base frame of a given video stream into fuzzy clusters. The number of clusters
is determined automatically by the algorithm. Connected segments in the
clusters are then combined to form larger segments. In the detection stage,
each segment is checked if it is a part of human face using chrominance values
and the variance of luminance values. By referring to the position of the
face region and related motion information, the corresponding body is found.
Then the base frame is divided into three regions: foreground, background,
and ambiguous. In the last step, a supervised network is constructed from
the fuzzy clusters obtained by SCFNN and is trained by a highly efficient
SVD-based hybrid learning algorithm using the data points obtained from
the foreground and background regions. The trained fuzzy neural network
is then used to decide the category of the pixels in the ambiguous region.
Finally, the pixels belonging to the foreground region form the desired human
object in the base frame. Note that the same neural network can be used for
segmentation of the remaining frames in the same video stream.
By this approach, similar pixels are grouped together in the first stage
and are processed collectively afterwards. The face region is determined by
referring to both chrominance and luminance values. Also, motion information
is used to help the determination of the corresponding body. The usage of such
information is confined to the neighborhood of a specific area. By using a
combination of multiple criteria in determining face and body, the difficulties
associated with other methods can be alleviated. Therefore, the human object
can be extracted more precisely in a video stream.
5.2 Clustering by SCFNN
We use composite signals, chrominance Cr and Cb and luminance Y , as the
basis for clustering. Given an image frame of N1×N2 in size, we divide it into
N1/4×N2/4 blocks each having 4×4 pixels. Each block is associated with a
feature vector x = [x1, x2, x3] where x1, x2, and x3 denote the average Cr ,
Cb, and Y values, respectively, of all the constituent pixels of the block. The
composite signals (Cr, Cb, Y ) of a pixel can be denoted by
30
Note that L may be equal to or greater than J . Segments of very small size
are considered to be noise or meaningless parts in the image. Therefore, they
are combined to bigger segments in order to reduce their influence on final
results. Let n(Si) denote the size of Si, i.e., the number of blocks in Si, and
κ = ζN1N2/16 where ζ is a predefined parameter and 0 < ζ < 1. We check
n(Si) for each segment Si. If n(Si) ≤ κ, then Si is combined into Sj which
is the smallest of the segments connected to Si and n(Sj) > κ. This process
is iterated until all segments are bigger than κ. Let the resulting segments
be labeled as R1, R2, . . ., and RQ, where Q ≤ L is the total number of
such segments. Later on, the image will be processed based on the connected
segments, instead of individual pixels or blocks.
Let’s apply the above procedure on Figure 19(a) which is an image of
360×288 pixels consisting of 90×72 blocks. The image is divided into J =
24 clusters containing L = 931 connected segments. After the combination
process, we have Q = 120 segments shown in Figure 19(b) in which different
segments are represented by different gray values.
(a) (b)
Fig. 19. (a) An example image; (b) Obtained connected segments.
5.4 Human Object Estimation
We apply chrominance values and the variation of luminance values to esti-
mate the face region. By referring to the position of the face region and motion
information, the body region is also estimated.
Face Estimation
Cr and Cb values of human skin have been found to occupy only a small region
in the CrCb space, as the white area shown in Figure 20, with approximately
5 ≤ Cr ≤ 45 and −1 ≤ Cb ≤ −50 [17]. We use this property for identifying
32
is chosen to be the estimation of the face region.
Note that Hg is obtained from the calculation on blocks. We’d like to link
it to segments and make a possible refinement on it. Consider a possible face
segment Pi, 1 ≤ i ≤ K. Let Ψi denote the set of blocks which belong to both
Hg and Pi, i.e.,
Ψi = Hg ∩ Pi. (35)
We check
NB(Ψi)/NB(Pi) ≥ λ (36)
where NB(Ψi) and NB(Pi) are the number of blocks in Ψi and Pi, respectively,
and λ, 0 < λ ≤ 1 is a predefined threshold. If Eq.(36) holds, then Pi is accepted
to be a part of the estimated face. Otherwise, it is not. This process is repeated
for all possible face segments. Finally, we have a set of possible face segments
that constitute the estimated face, and let these segments be labeled as F1,
F2, . . ., and FW . For convenience, we use Ef to denote the estimated face
region, i.e., Ef = {Fi|1 ≤ i ≤W}.
Body Estimation
We assume that the body is located directly below the head. A circle below
the face region is drawn to detect the corresponding body region. The circle
is defined with center being (cx, cy + h) and radius being h/2, where (cx, cy)
and h are the center and the height, respectively, of the face region Ef . Then,
the labeled segments, Ri, e.g., referring to Figure 19(b), covered partly or
totally by the circle region are regarded as possible body segments. A block
is called a possible body block if it belongs to a possible body segment. For
convenience, we use Eb to denote the estimated body region which is the set
of all possible body segments.
Based on the possible body segments obtained so far, we can add more
segments, if any, to the estimated body by looking into the motion information
associated with such segments. Let t represents the index of the current frame.
We define the motion index of a segment Ri as follows:
V (Ri) =
ΣB∈RiΣ
m=k−2
m=0 Σ
j=3
j=1 |x
t+m+1
j (B)− x
t+m
j (B)|
NB(Ri)
(37)
where k is the number of frames in a video sequence to be referenced, xmj (B)
denotes the xj value of block B in the mth frame, and NB(Ri) is the number
of blocks in Ri. A segment Ri is regarded as a possible body segment if
• Ri is neither contained in Ef nor in Eb;
• Ri is connected to a segment in Eb;
• V (Ri) ≥ β, where β is a user-defined constant.
When such a Ri is found, Ri is added to Eb. This process is iterated until no
Ri satisfies the above conditions.
34
IF x1 IS g(x1; v1j , σ1j) AND x2 IS g(x2; v2j , σ2j) AND x3 IS g(x3; v3j , σ3j)
THEN y IS cj (39)
where x1, x2, x3, and y are variables for Cr, Cb, Y , and the corresponding
output, respectively. The output cj is set as follows:
cj =
{
1 if Cj totally covers a block in Eu,
0 otherwise,
(40)
for 1 ≤ j ≤ J . A rule with cj = 1 specifies the conditions under which a block
belongs to the foreground region. Note that we have J fuzzy rules. These rules
form a rough discriminator for classification.
Based on the J rules, a four-layer supervised fuzzy neural network is con-
structed, as shown in Figure 22. The four layers are called the input layer,
Fig. 22. Architecture of the fuzzy neural network.
36
T = {(x1k, x2k, x3k, yk)|1 ≤ k ≤ NT } where (x1k, x2k, x3k) is the input vector,
denoting Cr, Cb, and Y values, respectively, of the training data, and yk = 1
for the data taken from the foreground region and yk = 0 for the data taken
from the background region.
The learning algorithm we use is a combination of a recursive SVD-based
least squares estimator and the gradient descent method, which was demon-
strated to be efficient for the network architecture of Figure 22 [49]. In each
iteration, the learning of vij , σij , and cj are treated separately. To optimize cj ,
vij and σij stay fixed, and the recursive SVD-based least squares estimator
is applied. To refine vij and σij , cj stays fixed and the batch gradient de-
scent method is used. The process is iterated until the desired approximation
precision is achieved.
Let k.o(4) and k.o
(3)
j denote the actual output of layer 4 and the actual
output of node j in layer 3, respectively, for the kth training pattern. By
Eq.(44), we have
k.o(4) = ak1c1 + ak2c2 + . . .+ akJcJ (45)
where
akj =
k.o
(3)
j∑J
j=1 k.o
(3)
j
(46)
for 1 ≤ j ≤ J . Apparently, we would like |yk − k.o(4)| to be as small as
possible for the kth training pattern. For all NT training patterns, we have
NT equations in the form of Eq.(45). Clearly, we would like
J(X) = ‖B−AX‖ (47)
to be as small as possible, where B, A, and X are matrices of NT×1, NT×J ,
and J×1, respectively, and
B =


y1
y2
...
yNT

, A =


a11 a12 · · · a1J
a21 a22 · · · a2J
...
...
...
...
aNT 1 aNT 2 · · · aNT J

, X =


c1
c2
...
cJ

. (48)
As mentioned earlier, we treat vij and σij as fixed, so X is the only variable
vector in Eq.(47). The optimal X which minimizes Eq.(47) can be found by
a recursive estimator based on the technique of singular value decomposition
(SVD) [24, 25]. The method considers training patterns one by one, starting
with the first pattern until the last pattern, resulting in less demanding in
time and space requirements [49].
On the other hand, parameters vij and σij , 1 ≤ i ≤ 3 and 1 ≤ j ≤ J , are
refined by the gradient descent method, treating cj , 1 ≤ j ≤ J , as fixed. The
error function used is defined as follows:
38
(a) (b)
Fig. 23. (a) Foreground, background, and ambiguous regions; (b) refined human
object.
(a) (b)
Fig. 24. The base frame of Akiyo: (a) original image; (b) extracted human object.
(a) (b)
Fig. 25. The base frame of Silent: (a) original image; (b) extracted human object.
40
(a) (b)
Fig. 27. The 50th frame of Silent: (a) original image; (b) extracted human object.
6 Video Transmission by MPEG-4
We demonstrate the usage of fuzzy neural networks in real communication
of videos in this section. We adopt MPEG-4 [58, 4, 5] as an example. In
the following, MPEG-4 is introduced first, and the functions of fuzzy neural
networks in MPEG-4 are then described.
6.1 Introduction of MPEG-4
MPEG-4 is an ISO/IEC standard developed by MPEG (Moving Picture Ex-
perts Group), and is used for encoding/decoding object-based compressed
video. It describes a multimedia system for communicating interactive scenes
composed of natural, synthetic, audio or visual objects. The central concept
is to divide each frame of an input video sequence into a number of arbi-
trarily shaped image regions called video object planes (VOPs). The input
to be coded can be a VOP of arbitrary shape, and the shape and location
of the region can vary from frame to frame. Successive VOPs belonging to
the same physical object in a scene are referred to as a Video Object (VO).
Thus, VO can be defined as a sequence of VOPs of possibly arbitrary shape
and position. Take the video sequence in Figure 28(a) as an example, which
consists of three frames. Each frame can be divided into two VOPs belonging
to different objects as shown in Figure 28(b) and Figure 28(c), respectively.
Thus, each VOP in Figure 28(b) can be taken as an instance of human object
at a certain specific time.
To facilitate interaction, VOs in a video stream are organized in a hier-
archical fashion and are characterized by intrinsic properties such as shape,
texture, and motion. For example, a scene of a talking person, as shown in
Figure 29, can be regarded as the composition of related objects, e.g. voice,
human image, and background image. To handle the objects, each object has
its own description element that allows the object to be combined with other
42
Fig. 29. The scene of a talking person.
Fig. 30. A MPEG-4 encoder.
used to remove redundancies between frames. I-VOPs are self-contained and
intra-frame coded. P-VOPs are predictively coded with respect to previously
coded VOPs, while B-VOPs are bi-directionally coded using the differences
between both the previous and next VOPs. I-VOPs must appear regularly in
the stream since they are required to decode subsequent inter-coded VOPs
such as P-VOPs and B-VOPs. Motion estimation is necessary for encoding
P-VOPs and B-VOPs and works by matching blocks with special attention
being given to blocks that lie on the boundary of the VOP.
44
(a)
(b)
Fig. 32. (a) VOPs of another background object; (b) reconstructed video sequence.
7 Conclusion
Finally, we give a summary of this chapter and provide some discussions on
image/video compression.
7.1 Summary
With the rapid growth in the amount of multimedia communication, e.g.,
images and videos, relying solely on high-speed connections can not guarantee
the quality of service. At present, the only possible solution needs the help of
data compression, so that the user can access images or videos in a reasonable
time with satisfactory quality.
Neuro-fuzzy techniques can be used for image/video compression due
to their advantages of learning ability, resistance to noise, and parallel ar-
chitectures. We have introduced three fuzzy neural networks for clustering,
i.e. FKCN, Fuzzy-ART and SCFNN. In particular, We have described how
SCFNN is used to generate code-words for vector quantization. The fuzzy
clusters obtained by SCFNN have a high-degree of intra-cluster similarity
and a low-degree of inter-cluster similarity. The mean vector of each obtained
fuzzy cluster becomes naturally a code-word. The advantages of using SCFNN
include that the fuzzy clusters generated are compact and dense, the real dis-
tribution of image content can be captured, and image content can be repre-
sented by code-words more appropriately. We have demonstrated the usage
of fuzzy neural networks in real communication of images. NITFF (National
Imagery Transmission Format) [7, 1] was adopted for encoding/decoding VQ
compressed images.
46
Object-based techniques seems more preferable than block-based ones and
have become the main stream for video compression in recent years. By using
MPEG-4, content can be organized as a composition of objects, and video
coding jumps from seeing to taking actions, and from passivity to activity.
However, a lot of work needs to be done for extracting, filtering, and iden-
tifying objects properly. A new work item, MPEG-7 [3], has been initiated
as the description interface of multimedia content. MPEG-7 is used for de-
scribing various types of multimedia information. It allows fast and efficient
identification of material that is interesting to the user. To fully exploit the
possibilities of MPEG-7, we need an automatic extraction method to extract
useful features, which is another challenging issue.
Further work will need to be invested in feature extraction, indexing, ob-
ject identification, object-based description, and content-based image/video
retrieval in order to achieve the goal of bandwidth reduction, high fidelity,
and content-based interactivity. Furthermore, to increase the sense of reality,
3D viewing of images is necessary. Tools for acquisition, coding, and manip-
ulation of 3D objects are also required. Features and objects are, in many
cases, ill-defined in nature. Besides, they may not be described in an exact
form. Therefore, any mechanism that can manipulate features or objects suc-
cessfully should be able to learn and deal with uncertainties and imprecision.
Because neuro-fuzzy techniques possess such capabilities, they may continue
to play an important role in image/video compression.
References
1. Basic Image Interchange Format (BIIF) Annex B, 1998. ISO/IEC 12087-5.
2. Information technology - Generic coding of moving pictures and associated audio
information: Video, 2000. ISO/IEC 13818-2.
3. Information technology - Multimedia content description interface - Part 5: Mul-
timedia description schemes, 2002. ISO/IEC 15938-5.
4. MPEG-4 video verification model version 18.0, 2001. ISO/IEC
JTC1/SC29/WG11 N3908.
5. Overview of the MPEG-4 Standard, 2002. ISO/IEC JTC1/SC29/WG11 N4668.
6. Special session on robust video. In Proc. of the IEEE International Conference
on Image Processing, Kobe, Japan, 1999.
7. Vector Quantization Decompression for the National Imagery Transmission For-
mat Standard, 1994. MIL-STD-188-199.
8. A. M. Alattar and S. A. Rajala. Facial features localization in front view head
and shoulders images. In Proc. of IEEE International Conference on Acoustic,
Speech, and Signal Processing, pages 3557–3560, Arizona, USA, March 1999.
9. R. E. J. Bezdek and W. Full. FCM: The fuzzy c-means clustering algorithm.
Computers and Geoscience, 10(2-3):191–203, 1984.
10. J. C. Bezdek and N. R. Pal. Two soft relatives of learning vector quantization.
Neural Networks, 8(5):729–743, 1995.
11. S. M. Bohte, H. L. Poutre and J. N. Kok. Unsupervised clustering with spiking
neurons by sparse temporal coding and multilayer RBF networks. IEEE Trans.
Neural Networks, 13(2):426–435, 2002.
48
31. R. M. Haralick and L. G. Shapiro. Computer and Robot Vision, Vol. 1. Addison-
Wesley, Reading, MA, 1992.
32. C. M. Huang and R. W. Harris. A comparison of several vector quantization
codebook generation approaches. IEEE Trans. Image Processing, 2(1):108–112,
1993.
33. C. F. Juang. A TSK-type recurrent fuzzy network for dynamic systems pro-
cessing by neural network and genetic algorithms. IEEE Trans. Fuzzy Syst.,
10(2):155–170, 2002.
34. C. F. Juang and C. T. Lin. An on-line self-constructing neural fuzzy inference
network and its applications. IEEE Trans. Fuzzy Syst., 6(1):12–32, 1998.
35. C. F. Juang and C. T. Lin. A recurrent self-organizing neural fuzzy inference
network. IEEE Trans. Neural Networks, 10(4):828–845, 1999.
36. S. H. Kim and H.G. Kim. Face detection using multi-modal information. In
Proc. of 4th IEEE International Conference on Automatic Face and Gesture
Recognition, pages 14–19, Grenoble, France, March 2000.
37. S. H. Kim, N. K. Kim, S. C. Ahn, and H. G. Kim. Object oriented face detection
using range and color information. In Proc. of 3th IEEE International Confer-
ence on Automatic Face and Gesture Recognition, pages 76–81, Nara, Japan,
April 1998.
38. T. Kohonen. Self-Organization and Associative Memory. Springer-Verlag, 3rd
edition, 1989.
39. I. Kompatsiaris and M. G. Strintzis. Spatiotemporal segmentation and tracking
of objects for visualization of videoconference image sequences. IEEE Trans.
Circuits Syst. Video Technol., 10(4):1388 – 1402, 2000.
40. C. M. Kuo, C. H. Hsieh, and Y. R. Huang. A new temporal-spatial image
sequence segmentation for object-oriented video coding. In Proc. of IEEE Asia
Pacific Conference on Multimedia Technology and Applications, pages 117–127,
I-Shou University, Taiwan, ROC, Dec. 2000.
41. S. J. Lee and C. S. Ouyang. A neuro-fuzzy system modeling with self-
constructing rule generation and hybrid SVD-based learning. IEEE Transac-
tions on Fuzzy Syst., 11(3):341-353, 2003.
42. S. J. Lee, C. S. Ouyang, and S. H. Du. A Neuro-Fuzzy Approach for Segmen-
tation of Human Objects in Image Sequences. IEEE Trans. Syst., Man, and
Cybern. B, 33(3):420-437, 2003.
43. Y. Lin, G. A. Cunningham III, and S. V. Coggeshall. Using fuzzy partitions
to create fuzzy systems from input-output data and set the initial weights in a
fuzzy neural network. IEEE Trans. Fuzzy Syst., 5(4):614–621, 1997.
44. J. Mao and A.K. Jain. A self-organizing network for hyper-ellipsoidal clustering
(HEC). IEEE Trans. Neural Networks, 7(1):16–29, 1996.
45. S. J. Mckenna, S. Jabri, Z. Duric, and H. Wechsler. Tracking interactive peo-
ple. In Proc. of 4th International Conference on Automatic Face and Gesture
Recognition, pages 348–353, Grenoble, France, March 2000.
46. T. B. Moeslund and E. Granum. A survey of computer vision-based human
motion capture. Computer Vision and Image Understanding, 81(3):231–268,
2001.
47. F. Moscheni, S. Bhattacharjee, and M. Kunt. Spatio-temporal segmentation
based on region merging. IEEE Trans. Pattern Analysis and Machine Intelli-
gence, 20(9):897–915, 1998.
