 1
行政院國家科學委員會專題研究計畫成果報告 
 
時頻卡式轉換與改良式高斯混合模型為基礎之 
不特定語句語者辨識系統設計 
 
計畫編號：NSC 95-2221-E-110-082 
執行期限：95 年 08 月 01 日至 96 年 07 月 31 日 
 
主持人：陳志堅 
 
國立中山大學  電機工程學系 
 
中文摘要 
 
我們以時頻卡式轉換與改良式高斯混合
模型為基礎，設計了兩階段式的不特定語句
語者辨識系統。計劃中我們錄製了 1500 位國
語語者之資料庫來測試系統的正確率。結果
證實，僅使用 72 個時頻卡式特徵，配合巴式
距離量度，以第一階段選取前 2% 之 30 位最
相近之候選語者，並配合第二階段之 16 個分
量的改良式高斯混合模型的辨識策略，來作
不特定語句之語者辨識，可獲得比高斯混合
模型，高出約 2% 之正確辨識率，同時計算速
度更可加快將近 50 倍。 
 
關鍵字:時頻卡式轉換，改良式高斯混合模型 
 
Abstract: 
 
A classification scheme that 
incorporates TFKLT and IGMM for text 
independent speaker identification has been 
designed in this project.  Our results show 
that the combination is beneficial to both 
classification accuracy and computational 
cost. For a database with 1500 Mandarin 
speakers, it is demonstrated that the accuracy 
improvement of up to 2% and the 
computational cost saving of 50 times 
compared to those of the conventional GMM 
model can be achieved. 
Keywords:   Time-frequency Karhunen-
Loeve transform，Improved 
Gaussian mixture model 
 
Introduction:  
 
Time-frequency Karhunen-Loeve trans-
form (TFKLT) has been applied to extract 
temporal-spectral evolution pattern for text 
independent speaker identification of a small 
population of 112 speakers [1]. Theoretically, 
it is the optimal transform in minimum mean 
square error and maximal energy packing 
sense. The transformed data is totally 
uncorrelated and contains most of the 
classification information in the first few 
coordinates.  But, as the number of speakers 
increases, performance eventually decreases 
due to speaker distribution overlap.  
 
Gaussian mixture speaker model (GMM) 
statistically represents the underlying sounds 
or vocal tract configurations that characterize 
a person’s voice, and it has been proven very 
effective for speaker identification [2]. But, 
as the population size and the length of test 
material increase, the computational cost of 
performing the identification can increase 
substantially.  Improved Gaussian mixture 
model (IGMM) derived from the local KLT 
concept [3] is then devised. 
 3
The first term of eqn. 6 gives the class 
separability due to the difference between 
class covariance matrices, while the second 
term gives the class separability due to the 
difference between class means. Furthermore, 
the optimal Bayes classification error 
between the two classes is bounded by the 
following expression: 
 
)exp( 221 BDPP −≤ε        (7) 
 
We will refer to the upper bound of the 
error probability evaluated from the 
inequality (7) with 5.021 == PP , as the 
Bhattacharyya error, 
 
)exp(5.0 2BB D−=ε         (8) 
 
The Bhattacharyya distance directly 
compares the estimated mean vector and the 
covariance matrix of the test speaker with 
those of the training speakers. If inclusion of 
the test covariance in the metric is useful, 
Bhattacharyya distance will outperform 
others distances [4]. 
 
Gaussian mixture speaker model: 
 
Gaussian mixture model uses multi-
modal Gaussian distribution to represent 
speaker’s voice and vocal tract 
configurations. For a feature vector denoted 
as C, the mixture density for speaker s is 
defined as ∑
=
=
M
i
s
i
s
is CbpCp
1
)()|( λ . The 
density is a weighted linear combination of 
M component uni-modal Gaussian 
densities )(Cbsi , each parameterized by a 
mean vector siμ  and a covariance matrix siΣ . 
Collectively, the parameters of a speaker’s 
density model are denoted 
as },,{ si
s
i
s
is p Σ= μλ , i = 1,2,...,M.  In this 
Report, 32 component mixtures with 
diagonal covariance matrices are used. 
maximum likelihood estimates of the model 
parameters are obtained using the expecta-
tion-maximization (EM) algorithm [2]. 
Improved Gaussian mixture model: 
 
 The computational cost of performing 
identification for the conventional Gaussian 
mixture model can increase substantially as 
the population size and the length of test 
material increase. The improved Gaussian 
mixture model is devised to increase the 
computational efficiency by the concept of 
local KLT [3].  Design a vector quantizer 
for each training speaker’s data to partition 
the feature space into K disjoint regions jR  
with centers jc , Kj ,...,2,1= ,then estimate 
the regional covariance matrix: 
 
]|))([( j
T
jjj RxcxcxE ∈−−=Σ    (9) 
 
Solve the local KL equation jjjj ΛΦ=ΦΣ , 
and transform the feature data into low 
dimensional space t
T
jt xy j Φ=  if  jt Rx ∈ . 
Since the high energy packing property of 
the local KLT, the dimension reduction of 
this approach will be quite significant. The 
parameters of a speaker’s density model are 
denoted as },,{ ,,, ijijijp Σ= μλ  , Kj ,...,1= , 
and jMi ,...,1=  ,  and the mixture density 
for the training speaker is then defined as:  
  
∏∏
==
= K
K
K
T
t
t
T
t
t ypypYp
11
)|()|()|(
1
1
1
λλλ L   (10)  
 
where jT  is the number of training vectors 
in partitioned region jR ,and maximum 
likelihood estimates of the model parameters 
are also obtained using the EM algorithm [2] 
 
Speaker feature training:  
 
A database with 1500 Mandarin 
speakers, 1093 males and 407 females, 
recorded from TV was used for system 
evaluation.  The data is sampled by Creative 
Lab’s Sound Blaster Live sound card at 
11.025 Khz and is linearly 16-bit PCM coded. 
 5
For this two-stage classification system, 
30 speakers, that is 2% of the population size, 
are chosen in the initial candidate selection 
process for further decision in the second 
stage.  The identification rate, Pr(correct),  
is 88.3%, 90.1%, and 89.2% after the final 
speaker recognition process for the context 
length of 1, 3, or 5 vectors respectively. As 
the context size gets bigger (q = 2), the 
performance does not increase. Merely using 
a larger contextual size does not benefit in 
terms of performance and imposes a cost in 
terms of memory and search complexity. 
 
The results indicate that the hybrid 
model has an improvement of 2% over the 
conventional GMM approach. Furthermore, 
for the conventional GMM, the algorithm 
requires 32.46 s of CPU time on a Pentium-4 
2.4Ghz machine to perform the search in the 
entire 1500-speaker database. But, for the 
hybrid TFKLT/IGMM under the context size 
of 3 vectors, the algorithm requires only 0.64 
s of CPU time to find the correct speaker 
from the 30 candidates. 
 
Conclusions:  
 
A hybrid TFKLT/IGMM approach has 
been introduced to text independent speaker 
identification.  In the first stage, the TFKLT 
features are used as the initial candidate 
selection tool to discard those speakers with 
larger separability using Bhattacharyya 
distance.  Then in the second stage, the 
IGMM is utilized as the final speaker 
recognition means to make the ultimate 
decision.  The results indicate that for the 
1500-speaker database, a candidate size of 30 
is adequate.  The best identification rate, 
using 72 TFKLT features and 16-component 
IGMM, is 90.1% that outperforms the 
conventional GMM approach by 2%. 
Furthermore, the computational time saving 
is approximately fifty-fold (32.64/0.64). 
References 
 
[1] Ivan Margrin-Chagnolleau, Geoffrey 
Durou, and Frederic Bimbot, 
“Application of time-frequency 
principal component analysis to text 
independent speaker identification,” 
IEEE Transactions on Speech and 
Audio Processing, Vol. 10, No. 6, pp. 
371-378, 2002 
 
[2] Douglas A. Reynolds, and Richard C. 
Rose, “Robust text-independent speaker 
identification using Gaussian mixture 
speaker models,” IEEE Transactions on 
Speech and Audio Processing, Vol. 3, 
No. 1, pp. 72-83, 1995 
 
[3] N. Kambhatla and T.K. Leen, “Dimen-
sion reduction by local PCA,” Neural 
Computation, Vol.9, pp.1493-1503, 
1997 
 
[4] JOSEPH P. CAMPBELL,: “Speaker 
recognition: A Tutorial,” IEEE 
Proceedings,  Vol. 85,  pp. 1437-
1462, 1997 
 
[5] Frank K. Soong, and A.E. Rosenberg, 
“On the use of instantaneous and 
transitional spectral information in 
speaker recognition,” IEEE 
Transactions on Speech and Audio 
Processing, Vol.36, No.6, pp.871-879, 
June 1988 
 
[6] J.W. Pitton, K. Wang, and B.H. Juang, 
“Time-frequency analysis and auditory 
modeling for automatic recognition of 
speech,” in Proceedings of the IEEE, 
ICASSP, Vol. 84, No. 9, pp 1199-1215, 
September, 1996 
 
 
 
