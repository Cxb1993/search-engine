摘要 
自動化視訊監控(Video surveillance)由於其在社會與居家安全廣泛的應用性，因而近年來相關研究日漸
蓬勃發展。本計畫預計發展監視視訊影片之時空摘要，將長時間的監視視訊影片以前景物體為基準進
行時間與空間上的影片濃縮，使得使用者能夠在短時間內對整段監視視訊影片的前景事件進行瀏覽，
有助於快速發現其中的異常事件。為達到此目的，首先需要將前景物體由監視視訊影片中擷取出來。
因此本計畫首先發展監視視訊背景模型與前景擷取演算法，來分離長時間固定出現在畫面中背景以進
行壓縮，並針對不定時出現在畫面中的前景物體，保留其運動軌跡以供後續視訊摘要之用。 
 
關鍵字：背景模型、視訊摘要，時空視訊摘要 
 
Abstract 
Video surveillance is a raising research domain in computer vision. One of the major problems of the 
long-time surveillance videos is the storage problem. Since a surveillance video records 24 hours a day, the 
required hard disk space is relatively huge. How to reduce the storage space of the video becomes an 
important issue. We aim to propose a new spatial-temporal video synopsis method which can reduce the 
redundant content of the surveillance videos and condense the whole video to a short one. In this way, the 
storage and browsing problems can be solved efficiently. Nevertheless, to achieve our goal, an efficient 
foreground object extraction method is required. In this project, we propose Voronoi-based dynamic partition 
and time-varying contrast histogram representation (TV-CHR) to build up Gaussian mixture background 
models. Experiments show that the proposed method can perform good results. 
 
Keyword: background modeling, video synopsis, spatial-temporal video synopsis 
I. Introduction 
Background modeling is a key research issue for detecting moving objects in the visual surveillance 
domain. The main idea is to automatically maintain a representation of the background scene to obtain 
foreground objects. Most background modeling methods are suitable for stationary backgrounds. Dynamic 
scenes such as water ripples, waving trees, and fountains in the garden, remain a great challenge. This project 
presents a new efficient background modeling method for dynamic scenes. To achieve the goal, Voronoi-based 
dynamic partition and time-varying contrast histogram representation are proposed.  
Over past decades, various techniques for background modeling have been developed. However, for most 
existing practical systems, each pixel is modeled by intensity or RGB color values independently. A good 
survey of pixel-based background modeling methods could be found in [1][2]. Monnet et al. [3] consider a 
sub-space analysis model to obtain the most recent variation for dynamic scenes modeling. Among pixel 
based approaches, the Gaussian mixture model proposed by Stauffer [4] is one of the most famous statistical 
methods. An online expectation minimization (EM) algorithm is used to update parameters of background 
models under mixture Gaussian distributions. However, the convergence speed of the method is relatively 
slow. Thus, Zivkovic et al. [5] developed an efficient adaptive algorithm to update the parameters in [4] 
constantly by using Gaussian mixture probability. In [6], Lee improves the online EM learning algorithm for 
non-stationary distributions under short-term statistics. Besides statistical approaches, Bayesian framework is 
also adapted to pixel-based background modeling. Li et al. [7] propose a Bayesian background modeling 
framework using spectral, spatial, and temporal features. In [8], Lee et al. model the background segmentation 
problem as a pixel-based Gaussian mixture model and formulated it with a Bayesian rule. Sheikh and Shah [9] 
treat the background model as a single probability density and also regard temporal persistence as a detection 
criterion. Furthermore, they use a MAP-MRF decision framework to determine the backgrounds. Tuzel et al. 
[10] model the background as a set of layered normal distributions for each pixel and used a recursive 
Bayesian leaning mechanism to estimate the mean, the variance and the probability distribution of the mean 
and covariance of them. Rahtu and Heikkilä [11] propose using Bayes classifiers to achieve background 
modeling. Bhaskar et al. [12] modify the distributions of Gaussian mixture model to alpha-stable distributions, 
which lead better background representation. 
In practice, these pixel-based methods are still vulnerable to the effect of noise. Therefore, some 
researchers begin to consider region based schemes. In [13], Huang et al. record color information of a region 
and represent them with mean and covariance matrix values. In order to avoid noise, partial Hausdorff 
(a) Feature Points Detection 
Given an image I(x, y) in our off-line sequence, the Gaussian kernel is applied for decreasing noise at first, 
f(x, y) = G(γ; σ)* I(x, y), where G(γ; σ) is a Gaussian function with mean γ and variance σ, and * is the 
convolution operator. Suppose that we have an obtained Gaussian smoothed image f(x, y) and a local area 
window w, which size is 3×3. For each point (x, y) of the image f(x, y) within window w, we could get a 2×2 
local structure matrix C as follows: 








 



wyx
y
wyx
y
wyx
y
wyx
fff
fff
C
x
xx
,
2
,
,,
2
,                                (1) 
where fx and fy are the partial derivatives along the x and y direction of the f(x, y), respectively. By using the 
singular value decomposition (SVD), the 2×2 local structure matrix C could be rewritten as C = UDVT, where 
D is a diagonal matrix diag(λ1, λ2) with the eigen values λ1 and λ2 of C, and U is a 2×2 matrix whose column 
vectors are the orthogonal eigen vectors e1 and e2 of C corresponding to its eigen value λ1 and λ2, respectively. 
“Eigen Component” represents the energies of a given window of an image after projecting according to its 
eigen-vectors. Therefore, if the energies are not constant in all direction, it will have corners. Then, these 
corners are treated as the candidate feature points. 
(b) Verification Model  
After we obtain the feature points, we would like to verify them via the verification model. The idea of our 
verification model is to examine the features were generated with correctly consistent appearing in the video 
or not. For each feature point of image f(x, y) in our off-line video sequence, we have a set of matched feature 
points that are “geometrically” consistent (inliers) and a set of features that are not consistent (outliers). 
Geometrically consistent here means feature points are detected at the same position for any two given images 
in our off-line video sequence.  
Let us denote the total number of candidate features in the off-line video sequence m and the number of 
inliers n. The i-th feature point is geometrically consistent/or not consistent is represented by g(i){1,0} and 
assumed to be an independent Bernoulli distribution as follows: 
 
Figure 1. Our framework. Our framework could be categorized into off-line and on-line stages. The main 
goal of the off-line stage is to obtain minimum partially overlapping regions from some verified feature 
points. At the on-line stage, we are going to model the background by using our time-varying contrast 
histogram representation (TV-CHR) according to those partition regions. 
histogram is sensitive to illumination changes and dynamic scenes. Thus, to avoid the mentioned problem, we 
proposed to use time-varing contrast histogram representation (TV-CHR) which inspired from contrast 
context histogram [32] to model the background partitions. In order to reduce the effect of light condition, we 
normalized our image gray level into [0, 1] and then an image smooth function is applied. Each TV-CHR 
contains 3 kinds of descriptors, a) spatial descriptor S; b) backward temporal descriptor Tb; and c) forward 
temporal descriptor Tf. This representation not only provides spatial but also temporal variation information. 
Given a video sequence with n frames and the window size of the partition is given by w×h. By considering 
the spatial information of our partition, we quarter the partition and counterclockwise label the regions for i = 
1, 2, 3, 4. For each region i, we sampled the center point c1, . . . , c4 and calculate the contrast variant of the 
intensity within each region. The contrast variant Ct(p) of pixel p in region i is defined by 
Ct(p) = It(p) – I(ci),                                  (8) 
where It(p) is the intensity of pixel p in region i of frame t and I(ci) is the intensity of sampled center point in 
region i. Then, the contrast variant could be represented by the histogram R(i)t+ and R(i)t- according to the 
variant value Ct(p) which is greater or smaller than 0, respectively. In other words, we accumulate the number 
of positive variant as follows: 
i
ttt
t N
pCiRppC
iR   0)( and,)(),()( ,                          (9) 
 
and the negative variant as follows: 
i
ttt
t N
pCiRppC
iR   0)( and,)(),()( ,                         (10) 
where R(i)t is the region i at frame t, and Ni is the total pixel number within each region i. Thus, the spatial 
descriptor S of a partition could be represented by a 8-dimensional vector as follows S = { tiR )( ,

tiR )( }, for i 
= 1, 2, 3, 4. Instead of only calculating the contrast descriptor at frame t, the backward temporal descriptor Tb 
shows all previous temporal contrast variants in each region. Thus, the positive and negative variants of frame 
t-1 are defined as follows:  
i
ttt
t N
pCiRppC
iR    0)( and,)(),()( 1111 ,                     (11) 
and 
i
ttt
t N
pCiRppC
iR    0)( and,)(),()( 1111 .                     (12) 
Then, the backward temporal descriptor Tb could be also represented by a 8-dimensional vector as follows: Tb 
= { 1)( tiR ,

1)( tiR }, for i = 1, 2, 3, 4. Moreover, the forward temporal descriptor Tf indicates all following 
temporal contrast variants between frame t and t+1 of each region i. The positive and negative variants of 
frame t+1 are defined as follows: 
i
ttt
t N
pCiRppC
iR    0)( and,)(),()( 1111 ,                     (13) 
and 
i
ttt
t N
pCiRppC
iR    0)( and,)(),()( 1111 .                     (14) 
As a result, the forward temporal descriptor Tf  is also a 8-dimensional vector as follows: Tf = 
{ 1)( tiR ,

1)( tiR }, for i = 1, 2, 3, 4. Thus, the obtained TV-CHR totally contains 24-dimensional contrast 
on Systems, Man and Cybernetics, vol.4, pp. 3099–3104, 2004. 
[2] D. Hall, J. Nascimento, P. Ribeiro, E. Andrade, P. Moreno, S. Pesnel, T. List, R. Emonet, R. Fisher, J. 
Santos Victor, and J. L. Crowley,“Comparison of target detection algorithms using adaptive background 
models,” in Proc. of International workshop on Performance evaluation of Tracking and Surveillance, 
Beijing, China, 2005. 
[3] A. Monnet, A. Mittal, N. Paragios, and V. Ramesh, “Background modeling and subtraction of dynamic 
scenes,” in Proc. of IEEE International Conference on Computer Vision, vol.2, pp. 1305–1312, 2003.  
[4] C. Stauffer and W. Grimson, “Adaptive background mixture models for real-time tracking,” in Proc. Of 
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 2, 1999. 
[5] Z. Zivkovic, “Improved adaptive gaussian mixture model for background subtraction.” in Proc. of 
International Conference on Pattern Recognition, vol. 2, pp. 28–31, 2004. 
[6] D.-S. Lee, “Effective gaussian mixture learning for video background subtraction,” IEEE Transactions 
on Pattern Analysis and Machine Intelligence, vol. 27, no. 5, pp. 827–832, 2005. 
[7] L. Li, W. Huang, I. Y.-H. Gu, and Q. Tian, “Statistical modeling of complex backgrounds for foreground 
object detection,” IEEE Transactions on Image Processing, vol. 13, no. 11, pp. 1459–1472, 2004. 
[8] D.-S. Lee, J. Hull, and B. Erol, “A bayesian framework for gaussian mixture background modeling,” in 
Proc. of International Conference on Image Processing, vol. 3, 2003. 
[9] Y. Sheikh and M. Shah, “Bayesian modeling of dynamic scenes for object detection,” IEEE Transactions 
on Pattern Analysis and Machine Intelligence, vol. 27, no. 11, pp. 1778–1792, 2005. 
[10] O. Tuzel, F. Porikli, and P. Meer, “A bayesian approach to background modeling,” in Proc. of IEEE 
Computer Society Conference on Computer Vision and Pattern Recognition, pp. 58, 2005. 
[11] E. Rahtu and J. Heikkilä, “A Simple and Efficient Saliency Detector for Background Subtraction,” in 
Proc. of IEEE International Workshop on Visual Surveillance, pp. 1137–1144, 2009. 
[12] H. Bhaskar, L. Mihaylova, and A. Achim, “Video Foreground Detection Based on Symmetric 
Alpha-Stable Mixture Models,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 
20, no. 8, pp. 1133 – 1138, 2010. 
[13] S.-S. Huang, L.-C. Fu, and P.-Y. Hsiao, “A region-based background modeling and subtraction using 
partial directed hausdorff distance,” in Proc. of IEEE International Conference on Robotics and 
Automation, vol. 1, pp. 956–960, 2004. 
[14] D. Russell and S. Gong, “A highly efficient block-based dynamic background model,” in Proc. of IEEE 
Conference on Advanced Video and Signal Based Surveillance, pp. 417–422, 2005. 
[15] H.-L. Eng, J. Wang, A. Wah, and W.-Y. Yau, “Robust human detection within a highly dynamic aquatic 
environment in real time,” IEEE Transactions on Image Processing, vol. 15, no. 6, pp. 1583–1600, 2006. 
[16] J. K. Suhr, H. G. Jung, G. Li and J. Kim, “Mixture of Gaussians-Based Background Subtraction for 
Bayer-Pattern Image Sequences,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 
21, no. 3, pp. 365 - 370, 2011. 
[17] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers, “Wallflower: principles and practice of background 
maintenance,” in Proc. of IEEE International Conference on Computer Vision, vol. 1, pp. 255–261, 1999. 
[18] O. Javed, K. Shafique, and M. Shah, “A hierarchical approach to robust background subtraction using 
color and gradient information,” in Proc. of Workshop on Motion and Video Computing, pp. 22–27, 2002. 
[19] M. Cristani, M. Bicego, and V. Murino, “Integrated region- and pixel based approach to background 
modeling,” in Proc. of Workshop on Motion and Video Computing, pp. 3–8, 2002. 
[20] S.-S. Huang, L.-C. Fu, and P.-Y. Hsiao, “A region-level motion-based background modeling and 
subtraction using MRFs,” in Proc. of IEEE International Conference on Robotics and Automation, pp. 
2179–2184, 2005. 
[21] L. Li and M. Leung, “Integrating intensity and texture differences for robust change detection,” IEEE 
Transactions on Image Processing, vol. 11, no. 2, pp. 105–112, 2002. 
[22] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto, “Dynamic textures,” International Journal of Computer 
Vision, vol. 51, no. 2, pp. 91–109, Feb. 2003.  
[23] Q. Zhu, S. Avidan, and K.-T. Cheng, “Learning a sparse, corner-based representation for time-varying 
background modeling,” in Proc. of the IEEE International Conference on Computer Vision, vol. 1, pp. 
678–685, 2005. 
[24] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of 
Computer Vision, vol. 60, no. 2, pp. 91–110, 2004. 
[25] M. Heikkilä, M. Pietikäinen, “A Texture-Based Method for Modeling the Background and Detecting 
Moving Objects,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 4, pp. 
657–662, 2006 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/11/28
國科會補助計畫
計畫名稱: 監視視訊影片之時空摘要
計畫主持人: 黃春融
計畫編號: 99-2218-E-005-005- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本計畫依照研究步驟與時程順利完成。其結果可用在安全監控產業上，未來可
提供國內安全監控廠商背景模型與前景擷取相關技術，以協助國內廠商技術自
有化，提高國際競爭力。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
