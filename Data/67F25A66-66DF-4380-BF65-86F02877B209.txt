行政院國家科學委員會專題研究計畫成果報告
使用基於樣本之雛型的面叢集演算法以及
其在電腦視覺之應用的研究
The investigation of shell clustering algorithms with template-based
prototypes and their applications in computer vision
計畫編號：NSC－96-2221-E-009-192-
執行期間：2007年8月1日 至 2008年7月31日
主持人：王才沛 國立交通大學資訊工程學系(所)
中文摘要
根據模糊 C 均值 (FCM) 或可能性 C 均值 (PCM) 的面叢集法 (shell clustering) 曾被建議
可以使用在物件與形狀偵測。它的優點包括收斂速度快，對記憶體需求低，以及對資料誤
差與不完美邊界有較高的容忍度。然而其現有的應用也受限於已有的演算法，而只被用來
偵測具有二次曲線或曲面特徵的物件。由於絕大部份的物件並不具有二次曲線的形狀，一
個有效率的、可以使用由任意形狀的樣本定義的叢集雛型的面叢集法，將會是面叢集法能
否有更廣的應用的關鍵。
這個計畫的目標是發展一個使用根據樣本定義的叢集雛型的面叢集法，並對其效能做
有系統的分析。我們的重心是在於使用二維資料的面叢集法，而物件與形狀的偵測是預設
的應用。我們的成果包括：
更具彈性的面叢集雛型轉換，允許每個維度有不同的比例縮放參數或是仿射轉換。
基於漸進叢集法的演算法，來降低叢集結果對未知叢集總數、資料誤差度、離群值比
例、以及初始叢集雛型參數的敏感度。
一個能夠有系統的分析面叢集法效能的評估架構，來幫助我們了解面叢集法的各種實
作方式的優缺點。
我們的研究結果顯示，我們所發展的演算法對於模擬與影像資料皆有不錯的效果。
關鍵詞：面叢集法、C 樣本叢集法、模糊/可能性叢集法、漸進叢集法、物件偵測
Abstract
Shell clustering algorithms based on fuzzy c-means (FCM) or possibilistic c-means (PCM)
were previously suggested as a possible method for object and shape detection. The advantages of
shell clustering include fast convergence, low memory demand, and better tolerance of data
deviation and imperfect edges in images. However, limited by available algorithms, currently shell
clustering has only been applied to the detection of objects whose shapes are described by
quadratic curves or surfaces. Since most objects do not possess such quadratic shapes, the key to
wider applications of shell clustering will be the development of shell clustering algorithms that
use cluster prototypes defined by templates of arbitrary shapes.
The goal of this project is the development of a shell clustering algorithm which can cluster
prototypes defined by templates of arbitrary shapes. The focus will be on shell clustering of
Here N is the number of data points, C is the number of clusters, m is the fuzzification factor, uij is
the possibilistic membership of xi (the ith feature point; 1iN) in the jth cluster, and dij is the
distance between xi and the jth cluster prototype. The parameter j is termed the "bandwidth" or
"zone of influence" in [3]. The second term prevents the trivial solution of all zero memberships if
we try to minimize (1) without it.
The standard AO scheme involves iteratively updating the memberships and the prototype
parameters in separate steps. The update equations for uij and the prototype parameters are derived
from the necessary conditions for minimizing J.
B. The Templates
Each template is defined as a set of vertices and the edges that connect them. Fig. 1 displays
several example templates. Each cluster prototype is a transformed version of the template, and the
prototype parameter set j consists of the parameters that define the transformation for Pj, the jth
prototype. Let H represent the transformation, which is applied to the vertices of a template. A
point p on Pj is related to its corresponding point in the template, p*, through H.
Circle Square Star Arrow Square+X
Clip "S" "U" Heart Leaf
Fig. 1. Templates used in our experiments. The template name is listed above each template for later reference.
The distance between any point xi and Pj, dist(xi,Pj), is defined as shortest distance between q
and any of the edges of Pj. The matching point (termed pij) of xi in Pj is defined as the point in Pj
closest to xi. While there are many possible transformations to derive a cluster prototype from a
template, we have focused on three types of transformations, termed Type I, II, and III,
respectively. They are explained in the next three subsections.
C. Type I Prototype Transformations
A Type I transformation is a shape-preserving transformation consisting of translation,
rotation, and a single scalar scaling factor. A point p on a prototype Pj is related to its
corresponding point p* in the template according to
jjjj sH tpRθpp  ** );( . (2)
Here sj, Rj, and tj are the scalar scaling factor, rotation matrix, and translation vector of Pj,
respectively. For two-dimensional data, Rj is determined by the rotation anglej.
The update equations for j, sj, and tj are obtained by solving the necessary conditions for
minimizing J, allowing their updates in separate steps:












 

N
i
m
ij
N
i
ijjji
m
ijj usu
11
* )( pRxt , (3)












 

N
i
ij
m
ij
N
i
ijj
T
ji
m
ijj uus
1
2*
1
* )()( ppRtx , (4)
and


















 





N
i
ij
T
ji
m
ij
N
i
ij
T
ji
m
ij
j
u
u
1
*
1
*
1
)(
01
10
)(
tan
ptx
ptx
 . (5)
points, j to a random value within , and the scaling factor(s) according to some pre-defined
constraints. For Type III transformations, the initial Aj is obtained by combining random scaling
and random rotation.
Prototype convergence test: A prototype is considered to have converged if the distance between
the same prototype at consecutive iterations is within a thresholdc.
Prototype merging: When two prototypes whose distance is within a threshold m, the prototype
with lower density is merged with the other prototype.
Extraction of Good prototypes: Prototypes with density above a threshold, good, is considered a
"good" one and moved to a separate list. Data points withinw of this prototype are also removed.
Deletion of Spurious and bad prototypes: Convergent prototypes with densities below a threshold
min are deleted. Also deleted are prototypes whose parameters fall outside of some predefined
bounds. In addition, "stuck" prototypes (convergent prototypes whose densities are betweenmin
andgood) are deleted with a probability of rstuck.
Adjustment of : The value of j is set to a relatively large value (0) ((0)1/2 (total range of
data)/10 has worked well for our data sets) for a newly initialized prototype and is reduced by a
multiplicative factor r(0<r<1) per iteration but is kept no less thanw2.
Prototype replacement: At the end of each iteration, deleted, extracted, and merged prototypes are
replaced with new randomly initialized prototypes. One advantage of this approach is that we do
not need to over-specify the number of initial prototypes as in [7,10]. Actually we can use C0=1 to
extract one cluster at a time.
Termination criterion: The main loop terminates after the ratio between the amounts of remaining
and total data is less than a threshold rT.
G. A Performance Measure for Shell Clustering
Even the progressive clustering procedure does not guarantee that all the actual clusters will
be detected. We introduce a performance measure that considers both correctness and efficiency.
Assume that we know the locations of all the actual shell clusters (target clusters). Let C* be
the number of target clusters in a given data set. We can define a "grade of detection" gd for a target
cluster PT:






otherwise,,0
3,2/)/3(
1
)( www
w
Td dd
d
Pg 
,
, (13)
where d is the minimal distance between PT and all the extracted prototypes. The overall gd is just
the average gd of all the target clusters. This measure for evaluating clustering results is more
intuitive and meaningful for shell clustering than the objective function. We also define kc as the
number of pij and dij computations per data point, as is the most computationally expensive step of
the whole algorithm and consumes the majority (above 80%) of the total execution time. We then
definegd=gd(end)/kc(end) as the performance measure; the subscript "(end)" indicates that the values
of kc and gd when our overall algorithm terminates. gd represents the mean grade of detection
achieved per unit of computation. We want to note that this performance measure is not tied to our
particular algorithms. For the same data set, this is useful for parameter optimization or even the
choice among different algorithms for the same purpose.
三、結果與討論
III. Results and Discussion
A. Results for Synthetic Single-Shape Data Sets
Fig. 2 display the clustering results of synthetic data sets obtained using our algorithms. We
include results for all three types of prototype transformations. For each data set we only look for
clusters of one particular shape, i.e., with all the prototypes derived from the same template. The
results indicate that the algorithm is capable of detecting the desired shapes in the presence of
minor scatter and noise, and of detecting the correct number of clusters. We also demonstrate that
C. Results for Multi-Shape Synthetic Data Sets
It is interesting to test the ability of our algorithm to cluster data sets that contain more than
one shapes. This can be further divided into two approaches: (i) The detection of one shape at a
time, with the other shapes in the data considered as clutter; and (ii) the detection of several shapes
simultaneously. The second approach requires a minor modification in the progressive clustering
procedure: The prototype initialization step now involves the assignment of a different template
from several user-specified possible choices for each prototype. An example of clustering
multi-shape data is shown in Fig. 4, which contains three different shapes as well as some noise.
The second approach is found to be more efficient if we want to detect multiple shapes from the
same data set.
(b) (c) (d)
(a)
Fig. 4. Experimental results with multi-shape synthetic data. (a) A data set with three different shapes, and the
prototypes obtained with simultaneous clustering of all the shapes. (b)-(d) The prototypes obtained using only
the square, heart, and star templates, respectively.
D. Discussion
We have developed the algorithms that facilitate possibilistic shell clustering with
template-based prototypes, with three different types of transformations for obtaining cluster
prototypes from a given template with different degrees of complexity and flexibility, and a
detailed progressive clustering procedure to reduce the reliance on prior knowledge of the number
of clusters or good initialization of prototypes. We believe that this approach has great potential for
use in various image analysis problems.
There are a number of research questions and topics that can be pursued further. First, for
practical applications, it is important to devise more efficient methods for the task of locating the
matching points. We also plan to investigate shell clustering approaches that incorporates local
directionality information of the data points, as well as the combination of shell clustering with
other shape/object detection algorithms.
四、參考文獻
IV. References
[1] J.C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms, New York:
Plenum, 1981.
[2] I. Gath and A.B. Geva, "Unsupervised optimal fuzzy clustering", IEEE Trans. PAMI, vol. 11,
pp. 773-781, 1989.
[3] R. Krishnapurum and J.M. Keller, "A possibilistic approach to clustering", IEEE. Trans.
Fuzzy Systems, vol. 1, pp. 98-110, 1993.
[4] M.S. Yang and K.L. Wu, "Unsupervised possibilistic clustering", Pattern Recognition, vol. 39,
pp. 5-21, 2006.
出席國際學術會議心得報告
計畫編號 96-2221-E-009-192-
計畫名稱 使用基於樣本之雛型的面叢集演算法以及其在電腦視覺之應用的研究
出國人員姓名
服務機關及職稱
王才沛
國立交通大學資訊工程學系助理教授
會議時間地點 June 1-6, 2008 (Hongkong)
會議名稱 2008 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2008) aspart of 2008 IEEE World Congress on Computational Intelligence (WCCI 2008)
發表論文題目 Possibilistic Clustering of Generic Shapes Derived from Templates
一、參加會議經過
WCCI is currently held only in even years, where researchers from the three main branches of
computational intelligence -- neural networks, fuzzy systems, and evolutionary computation -- can
hold the respectively annual conferences together. The individual conferences -- IJCNN,
FUZZ-IEEE, and CEC, are certainly the most significant conferences in their respective fields.
Therefore, the conference is very large, with up to 19 parallel sessions. I was at the conference site
for only one day (June 2) of the 6-day program (June 1-6), and was focused on the sessions on
fuzzy pattern recognition, which included my own paper.
二、與會心得
I was glad to see that fuzzy pattern recognition continues to be an active field, with a total of three
dedicated sessions during the day. While new classification and clustering algorithms continue to be
developed, recently there seemed to be more emphasis on automatic feature and data selection. In
addition, I feel that we should explore more real-world applications of this field.
conditions for minimizing J. For the memberships, the
necessary condition isJ/uij=0, whose solution is [3]:
  1)1/(121  
m
jijij du  .
(2)
Let j represents the set of parameters that define the jth
cluster prototype. The update equations of the prototype
parameters are obtained by solving the necessary condition
J/j=0. The exact forms of these update equations depend
on the type of prototypes and the distance measure used.
The general form of our PCM-based algorithms is given
below:
Initialize the prototypes
Repeat
Update the membership values
Update the prototype parameters
Until convergence or the maximal allowed
iterations
B. The Templates
In this paper, each template is defined as a set of vertices
and the edges that connect them. Fig. 1 displays several
example templates used in this paper. Each cluster prototype
is a transformed version of the template, and the prototype
parameter set j consists of the parameters that define the
transformation for Pj, the jth prototype. Let H represent the
transformation, which is applied to the vertices of a template.
A point p on Pj is related to its corresponding point in the
template, p*, through H.
The distance between any point q and Pj, dist(q,Pj), is
defined as shortest distance between q and any of the
edges of Pj. We use Euclidean distance throughout the
subsequent derivations. The matching point of q in Pj is
defined as the point in Pj closest to q. The computation of
matching points is a required but most time-consuming step
in our algorithms. The distance between two prototypes Pk
and Pj is defined as







),(max),,(maxmax),( k
P
j
P
kj PdistPdistPPdist
jk
pp
pp
. (3)
While there are many possible transformations to derive a
cluster prototype from a template, in this paper we focus on
three types of transformations, termed Type I, II, and III,
respectively. Their definitions and the derivations of their
corresponding prototype update equations are presented in
the next three subsections.
C. Type I Prototype Transformations
A Type I transformation is a shape-preserving
transformation consisting of translation, rotation, and a
single scalar scaling factor. This is the same type of
transformations described in [17]. A point p on a prototype
Pj is related to its corresponding point p* in the template
according to
jjjj sH tpRθpp  ** );( . (4)
Here sj, Rj, and tj are the scalar scaling factor, rotation matrix,
and translation vector of Pj, respectively. For
two-dimensional data, Rj is determined by the rotation angle
j.
We also use pij to represent the matching point of xi on Pj,
and dij as the distance between xi and Pj:
 2*22 jijjjiijiij sd tpRxpx  .
(5)
The necessary conditions for minimizing J with respect to
the cluster parameters are obtained by setting to zero the
partial derivatives of J with respect toj, sj, and tj:
0))(2(
1
* 

 

N
i
ijijjj
m
ij
j
su
J
xtpR
t
, (6)
0)()()2(
1
** 

 

N
i
ijj
T
ijijjj
m
ij
j
su
s
J
pRxtpR ,
(7)
and
.))((2
))((20
1
*
1
**2
























N
i
ij
j
jT
ij
m
ijj
N
i
ij
j
jT
ijj
m
ijj
j
d
d
us
d
d
us
J
p
R
xt
p
R
pR


(8)
It is difficult to solve (6)-(8) simultaneously. However, we
can find closed-form expressions if we choose to update one
parameter at a time. This means that we solve (6) for tj, (7)
for sj, and (8) for j. The update equations for tj and sj are
straightforward results of (6) and (7):












 

N
i
m
ij
N
i
ijjji
m
ijj usu
11
* )( pRxt (9)
and












 

N
i
ij
m
ij
N
i
ijj
T
ji
m
ijj uus
1
2*
1
* )()( ppRtx . (10)
To solve forj, we first observe that the first term in (8) is
Circle Square Star Arrow Square+X
Clip "S" "U" Heart Leaf
Fig. 1. Templates used in our experiments. The template name is listed
above each template for later reference.
 2*22 jijjiijiijd tpAxpx  . (20)
The update equation for tj is similar to (9):












 

N
i
m
ij
N
i
ijji
m
ijj uu
11
* )( pAxt . (21)
By settingJ/Aj=0, we obtain
  .0))((2)(2
1
***
1
2*










N
i
T
ijij
T
ijijj
m
ij
N
i
ijijj
j
m
ij
j
u
u
J
pxtppA
xtpA
AA
(22)
Rearrangement of (22) yields the update equation for Aj:
1
1
**
1
* )())((














 
N
i
T
ijij
m
ij
N
i
T
ijij
m
ijj uu pppxtA .
(23)
For prototypes derived with Type III transformations, we
then have the following clustering algorithm:
Initialize the prototypes
Repeat
Find all pij and corresponding pij*
Compute dij using (20)
Update uij using (2)
Update tj using (21)
Update Aj using (23)
Until convergence or the maximal allowed
iterations
III. PROGRESSIVE CLUSTERING PROCEDURE
The strong dependence of clustering results on
initialization, including the number of clusters and initial
prototype parameters, is a well-known difficulty for shell
clustering. For this reason, we implement a progressive
clustering procedure that enables the detection of an
uncertain number of clusters and the estimation of their
parameters. The basic idea of progressive clustering is to
progressively remove prototypes that appear to represent
good clusters as well as the data points belonging to those
clusters. The partial removal of data increases the likelihood
of finding good clusters in the remaining data. The use of
progressive clustering for hyperspherical and hyperquadratic
shells has been described in [7,10]. Our algorithm employs a
completely possibilistic approach. Instead of focusing on
determining "the correct number of clusters", we are more
concerned finding "as many good clusters as possible in a
reasonable amount of time".
In the main loop of our progressive clustering procedure,
a user-specified number (C0) of prototypes are allowed to
search for their local optima individually. Within each
iteration of the main loop, we incorporate several ideas in
[10], including the deletion of spurious prototypes, the
merging of similar prototypes, and the extraction of "good"
prototypes with their associated data points. The main added
elements are the coarse-to-fine searching process through the
adjustment ofvalues, and the re-initialization of extracted,
deleted, or merged prototypes.
In order to be able to determine the goodness of a
prototype, we define the prototype density as



wijd
ij
j
j uL 
 1 ,
(24)
where Lj is the total edge length of the j
th prototype. This is
similar to the surface density used in [10] as a single-cluster
validity measure for shell clusters. We use Lj instead of the
effective arc length used in [10] because we are not
interested in detecting objects that do not contain at least
most of the template. In our experiments, the value of w is
1.0 and 2.5 for synthetic and image data sets, respectively. In
any real applications this should be determined according to
the expected amount of scatter in the data.
More details of the main loop are explained below:
Prototype initialization: To initialize a prototype, we
randomly set tj to one of the remaining data points, j to a
random value within , and the scaling factor(s) according
to some pre-defined constraints (to be discussed later). For
Type III transformations, the initial Aj is obtained by
combining random scaling and random rotation.
Prototype convergence test: A prototype is considered to
have converged if the distance between the same prototype
at consecutive iterations is within a threshold c (currently
0.25).
Prototype merging: When two prototypes whose distance
is within a threshold m (currently 1.0), the prototype with
lower density is merged with the other prototype.
Extraction of Good prototypes: Prototypes with density
above a threshold, good, is considered a "good" one and
moved to a separate list. Data points within w of this
prototype are also removed. We initialize good to a high
value (currently 0.8) and then slowly reduce its value
(currently by 0.1 percent per iteration). This gradually
relaxes the requirement for "good" prototypes and allows
less than perfect clusters (such as somewhat obscured
objects in images) to be detected at later stages of the
process.
Deletion of Spurious and bad prototypes: Convergent
prototypes with densities below a threshold min (currently
0.3) are deleted. Also deleted are prototypes whose
parameters fall outside of some predefined bounds, which
are given based on prior knowledge of the data. In addition,
"stuck" prototypes (convergent prototypes whose densities
are between min and good) are deleted with a probability of
rstuck (currently 0.2).
Adjustment of: The value ofj is set to a relatively large
value (0) ((0)1/2 (total range of data)/10 has worked well
for our data sets) for a newly initialized prototype to make
sure that it is able to move toward nearby data efficiently. It
is reduced by a multiplicative factor r (0<r<1, currently
points (and hence minimizes J) may be still so large that its
density is too low to be considered a good cluster. The
consequence is that our clustering procedure will have
difficulty and much worse efficiency in identifying such
clusters. Shapes that have this problem include "X", "V", "T",
and any shape that can "contain a smaller version of itself"
after some translation and rotation. In addition to "U" and
"S", open shapes that can be clustered with our current
algorithms include "C", "W", "Z", etc.
B. Results for Single-Shape Image Data Sets
Fig. 4 demonstrates clustering results using real-world
images with all three types of prototype transformations.
Each row of these figures, from left to right, shows the
original image, the extracted edge points used as the data
points for clustering, and the original image overlaid with
the final extracted prototypes. The images are all 160120 in
size. The steps of edge point extraction include smoothing,
gradient computation, thresholding, and thinning. We do not
explain the detail of edge point extraction here as this is
intrinsically application dependent and is not the focus of
this paper.
C. A Performance Measure for Shell Clustering
Even the progressive clustering procedure does not
guarantee that all the actual clusters will be detected. Here
we introduce a performance measure that considers both
correctness and efficiency.
Assume that we know the locations of all the actual shell
clusters (target clusters). Let C* be the number of target
clusters in a given data set. We can define a "grade of
detection" gd for a target cluster PT:






otherwise,,0
3,2/)/3(
1
)( www
w
Td dd
d
Pg 
,
,
(25)
where d is the minimal distance between PT and all the
extracted prototypes. The overall gd is just the average gd of
all the target clusters. This measure for evaluating clustering
results is more intuitive and meaningful for shell clustering
than the objective function. We also define kc as the number
of pij and dij computations per data point, as is the most
computationally expensive step of the whole algorithm and
consumes the majority (above 80%) of the total execution
time. We then define gd=gd(end)/kc(end) as the performance
measure; the subscript "(end)" indicates that the values of kc
and gd when our overall algorithm terminates.gd represents
(a) (b) (c) (d)
(e) (f) (g) (h)
(i) (j) (k) (l)
Fig. 3. Experimental results with synthetic data. Prototype transformations: (a)-(d) Type I; (e)-(h) Type II; (i)-(l): Type III. The templates used: (a), (e),
and (i): circle; (b): "U", (c): square+X; (d), (h), and (l): square; (f): "S"; (g) and (k): arrow; (j): star. See Fig. 1 for the actual templates.
select smaller(0) and rwhen we are not sure of the optimal
values. The only drawback, which is somewhat lower
performance and slower convergence for some data sets, is
preferred over the performance breakdown by using (0) and
rvalues that are too large.
D. Results for Multi-Shape Synthetic Data Sets
It is interesting to test the ability of our algorithm to
cluster data sets that contain more than one shapes. This can
be further divided into two approaches: (i) The detection of
one shape at a time, with the other shapes in the data
considered as clutter; and (ii) the detection of several shapes
simultaneously. The second approach requires a minor
modification in the progressive clustering procedure: The
prototype initialization step now involves the assignment of
a different template from several user-specified possible
choices for each prototype.
An example of clustering multi-shape data is shown in Fig.
5. This data set (N=641)contains three different shapes: two
each of squares, hearts, and stars, as well as some noise. The
prototypes obtained with the second approach are plotted
with the data points in Fig. 5(a). Fig. 5(b)-(d) plot the
prototypes extracted when only one template is used at a
time. It is clear that we are able to extract only the clusters of
the correct shape each time. This is a clear evidence of the
ability of our algorithm in distinguishing among different
shapes. For the performance comparison between the two
approaches under similar conditions (C0=6 per shape and
identical parameters otherwise), when averaged over 20 runs,
the first approach takes 21 seconds and achieves gd(end)=0.79,
and the second approach takes 17 seconds and achieves
gd(end)=0.93. This indicates that the second approach is more
efficient if we want to detect multiple shapes from the same
data set.
V. CONCLUSION
This paper describes the algorithms that facilitate
possibilistic shell clustering with template-based prototypes.
We present three different types of transformations for
obtaining cluster prototypes from a given template with
different degrees of complexity and flexibility. Type I
transformations are special cases of Type II transformations,
which are in turn special cases of Type III transformations.
Simpler transformations (e.g., Type I as compared to Type II)
are applicable to fewer shapes, but perform better for shapes
they are applicable than more general transformations. The
algorithms all employ the efficient alternating optimization
scheme with closed-form update equations for prototype
parameters. We also describe a possibilistic progressive
clustering procedure. Our results indicate that we are able to
obtain good clustering performance without prior knowledge
of the number of clusters or good initialization of prototypes
within seconds. As a result, we believe that this approach
has great potential for use in various image analysis
problems.
We have also identified a number of research questions
and topics that we will continue to pursue. First, for practical
applications, it is important to devise more efficient methods
for the task of locating the matching points. We also plan to
investigate shell clustering approaches that incorporates
local directionality information of the data points, as well as
the combination of shell clustering with other shape/object
detection algorithms.
ACKNOWLEDGMENT
This work was supported in part by the National Science
Council of Taiwan under Grant NSC-96-2221-E-009-192.
The author thanks Prof. James Keller for helpful discussion.
REFERENCES
[1] J.C. Bezdek, Pattern Recognition with Fuzzy Objective Function
Algorithms, New York: Plenum, 1981.
[2] I. Gath and A.B. Geva, "Unsupervised optimal fuzzy clustering",
IEEE Trans. PAMI, vol. 11, pp. 773-781, 1989.
[3] R. Krishnapurum and J.M. Keller, "A possibilistic approach to
clustering", IEEE. Trans. Fuzzy Systems, vol. 1, pp. 98-110, 1993.
TABLE I
CLUSTERING PERFORMANCE FOR SYNTHETIC DATA SETS
Type Plot(Fig. 3) Template C
* C0 N gd(end)
time
(s)
gd
(x10000)
(a) Circle 3 3 219 0.83 1.7 35
(b) "U" 4 6 203 0.68 3.4 12
(c) Square+X 4 4 356 0.95 1.8 32
I
(d) Square 3 3 226 0.94 1.2 54
(e) Circle 3 6 197 0.82 2.5 23
(f) "S" 4 6 202 0.83 3.6 16
(g) Arrow 4 6 270 0.93 3.3 18
II
(h) Square 3 8 242 0.66 2.9 19
(i) Circle 3 6 197 0.53 2.4 15
(j) Star 5 10 252 0.58 5.9 6
(k) Arrow 4 6 270 0.77 4.1 10
III
(l) Square 3 8 193 0.86 2.7 18
TABLE II
CLUSTERING PERFORMANCE FOR IMAGE DATA SETS
Type Plot(Fig. 4) Template C
* C0 N gd(end)
time
(s)
gd
(x10000)
I Top Clip 4 8 1093 0.85 13.8 11
II Middle Leaf 5 8 740 0.70 7.7 7
III Bottom Square 3 4 403 0.88 1.9 27
(b) (c) (d)
(a)
Fig. 5. Experimental results with multi-shape synthetic data. (a) A data
set with three different shapes, and the prototypes obtained with
simultaneous clustering of all the shapes. (b)-(d) The prototypes
obtained using only the square, heart, and star templates, respectively.
