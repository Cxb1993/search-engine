 II
Abstract 
This project presented a robust wavelet support vector machine (RWSVM) to 
model nonlinear systems with outliers. In RWSVM, wavelet support vector machine 
(WSVM) integrating annealing robust time-varying learning algorithm (ARTVLA) is 
proposed. Due to the similarity between wavelet support vector regression (WSVR) 
and wavelet neural networks (WNNs), WSVR approach is adopted to determine the 
initial translation and dilation of a wavelet kernel and the weights of the wavelet 
neural networks (WNNs). Then, ARTVLA is used as the learning algorithm for the 
WNNs (ARTVLA-WNN) and applied to accommodate the translation, the dilation, 
and the weights of the WNNs. In the learning procedure, the ARTVLA is proposed to 
overcome the problems of initialization and the cut-off points in the robust learning 
algorithm. Hence, when an initial structure of the WNNs is determined by a WSVR 
approach, the WSVR-based ARTVLA-WNN (WSVR-ARTVLA-WNN) has fast 
convergence speed and can robust against outliers. Three examples are simulated to 
confirm the performance of the proposed algorithm. From the results, the feasibility 
and superiority of the proposed WSVR-ARTVLA-WNN for modeling nonlinear 
systems are verified. 
 
Keywords: Wavelet support vector machine; Annealing robust time-varying learning 
algorithm; Particle swarm optimization; Wavelet neural networks; Outliers 
 
  
2
feature space. Consider a set of training data, {( , ),  1, , }i iy i m= "x  such that di R∈x  is the 
input vector, Ryi ∈  is the corresponding output, and m  denotes the number of data in the 
training set. The linear problem is formulated as 
         
1
( , ) ( ) ( )
m
T
i i
i
y f w w wϕ γ ϕ γ
=
= = + = +∑x x x ,   (1) 
where 1( ) [ ( ), , ( )]mϕ ϕ ϕ= "x x x  is the nonlinear mapping function, ],,[ 1 mwww L=  is the 
weight vector and γ  is the bias.  
Using Lagrange multiplier techniques and solve the dual optimization problem, then (1) 
can be repressed as 
 *
1
( , ) ( ) ( , )
m
i i i
i
f Kα α α γ
=
= − +∑x x x ,          (2) 
where α  and *α  are Lagrange multipliers obtained from the dual problem, and the dot 
product function ( , ) ( ) ( )Ti iK ϕ ϕ=x x x x  is the support vector (SV) kernel which must 
satisfies the condition of Mercer’s theorem [21]. 
2.2. Wavelet theory 
Using a multi-resolution analysis (MRA), the wavelet transform expands a signal or 
function onto a set of wavelet basis function [22, 23]. The basis function , ( )a bΨ x  can be 
derived from a mother wavelet ( )xΨ  through translations and dilations as 
    ,
1( ) ( )a b
b
aa
−Ψ = Ψ xx ,   (3) 
where , , 0,a b R a∈ >  a  is the dilation and b is the translation. The normalization factor 
a  in (3) ensures that , ( )a bΨ x has a constant norm in the space of square integrable 
functions as 
   2, ,|| ( ) || | ( ) | 1a b a b dx
∞
−∞Ψ = Ψ =∫x x .   (4) 
For a mother wavelet 2( ) ( ),L RΨ ∈x  it must satisfy the following admissibility condition. 
Then, an approximation of 2( ) ( )f L R∈x  can be regarded as a linear combination of 
wavelets [24], it is expressed as  
     ,
1
ˆ ( ) ( )
m
i a b
i
f w
=
= Ψ∑x x ,                  (5) 
where ˆ ( )f x  is the approximation of the function ( )f x . 
For multidimensional wavelet functions, a d-dimensional mother wavelet ( )Ψ x  can be 
generated by tensor products of one-dimension (1-D) wavelet function [24] as 
  
4
weights of the WNNs are determined through the WSVM, then the ARTVLA is adopted to 
adjust those parameters via the robust learning algorithms. 
Given a set of data {( , ), 1, , },i iy i m=x L  the output obtained by using the WSVM can 
be written as 
      
1
( ) ( )
L
i ij j i
j
f w K γ
=
= +∑x x   for 1, ,i m= L ,     (12) 
where  
 
2
2
|| ||
( ) cos 1.75 exp
2
i j i j
j i
j j
b b
K
a a
⎛ ⎞ ⎛ ⎞− −= × −⎜ ⎟ ⎜ ⎟⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠
x x
x .   (13) 
L is the number of the wavelet SV, ijw  is the weight of the WNNs, ja and jb  are the 
dilation and the translation of the wavelet SV, respectively. 
In the ARTVLA, a robust cost function is defined as 
       
1
1( ) ( ( ); ( ))
m
i
i
E t e t t
m
ρ β
=
= ∑ , (14) 
where 
       
1
ˆ( ) ( ) ( )
L
i i i i j j i
j
e t y f y w K γ
=
= − = − +∑x x , (15) 
t  is the epoch number, ( )tβ  is a deterministic annealing schedule acting like the cut-off 
point, and ( )ρ ⋅  is a logistic loss function defined as 
       
2
[ ; ] ln 1
2
i
i
ee βρ β β
⎡ ⎤⎛ ⎞= +⎢ ⎥⎜ ⎟⎝ ⎠⎣ ⎦
.                  (16) 
Based on the gradient-decent kind of learning algorithm, the weights ijw , the translation jb , 
and the dilation ja  can be iteratively updated by follows: 
       
1
( )( ) ( ; )
m
w i
ij w i
ij j
E t ew t e
w m w
ηη φ β
=
∂ ∂Δ = − = −∂ ∂∑ , (17) 
       
1
( )( ) ( ; ) ,
m
b i
j b i
ij j
eE tb t e
b m b
ηη φ β
=
∂∂Δ = − = −∂ ∂∑    (18) 
 
1
( )( ) ( ; ) ,
m
a i
j a i
ij j
eE ta t e
a m a
ηη φ β
=
∂∂Δ = − = −∂ ∂∑  (19) 
        2
( ; )( ; )
1 ( )
i i
i
ii
e ee
ee
t
ρ βφ β
β
∂= =∂ +
, (20) 
where ,wη  ,bη  and aη  are learning rates, ( )φ ⋅  is usually called the influence function.  
  
6
)(kx
)1( +ky
)1( +ke
)1(ˆ +ky
 
Fig. 1. The proposed PSO-based WSVR-ARTVLA-WNN scheme for system identification. 
  
 
( )2( ) ( )1 ˆN k kk y yRMSE
N
= −= ∑ , (24) 
where ( )ky  is the desired output and ( )ˆ ky  is the output of the WNNs. Meanwhile, the value 
of RMSE is chosen as the fitness value to measure the population in PSO for the proposed 
SVR-ARTVLA-WNN. 
Example 1: In this example, the nonlinear system in [27] is described as 
    ),2(2.0)1(01.0)()1(025.0)(72.0)1( 2 −+−+−+=+ kukukukykyky   (25) 
where 
      
⎪⎪⎩
⎪⎪⎨
⎧
≤≤++
<≤−
<≤
<≤
=
.1000750),10/sin(6.0)32/sin(1.0)25/sin(3.0
,750500,1
,500250,1
,2500),25/sin(
)(
kkkk
k
k
kk
ku
πππ
π
  (26) 
With the 1000 training data with 10 artificial outliers and applying the WSVR method, 
the initial values of ijw , jb , and ja  in Eq. (13) can also be determined. After initialization, 
two annealing robust algorithms are then separately applied to train the WNNs. 
Problem 1-1:  
After initialization, the ARLA with various learning rates,0.05 2.5η≤ ≤ , is then used to 
train the WNNs. After 200 training epochs, the RMSE values of y  for various learning rates 
are obtained, respectively. The best value of RMSE is found to be 0.0061 for the learning rate 
1.5η = . The details of the simulation results are shown in Table 1. 
Problem 1-2: 
In the ARTVLA, the optimal learning rates are determined by PSO method to train the 
WNNs. The results of Fig. 2 illustrate the fitness values of PSO through 200 iterations. Figure 
3 illustrates the values of RMSE using the ARTVLA with the optimal set of  
( ,  ,  ) (4.0743,  0.2135,  3.5564)r s t =  for learning rate functions (21) through (23) and the 
ARLA with the fixed learning rate 1.5.η = The final value of RMSE for the ARTVLA-WNN 
is found to be 0.0057. 
  
8
0 50 100 150 200
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
epoch
RM
SE
 
Fig. 3. The values of RMSE in Eq. (24) after 200 training epochs, in which the comparison between 
the ARTVLA-WNN (solid line) and the ARLA-WNN (dashed line) with the fixed learning rate 
1.5η =  for Example 1. 
 
Using the training data with 10 artificial outliers and applying the SVR method, the 
initial values of ijw , jb , and ja  in Eq. (13) can also be determined. Then, two annealing 
robust algorithms are then applied to train the WNN, respectively. 
Problem 3-1:  
After initialization, the ARLA with various learning rates, 0.05 2.5η≤ ≤ , is then used 
to train the WNNs. After 600 training epochs, the RMSE values of y  for various learning 
rates are obtained, respectively. The best value of RMSE is found to be 0.2603 for the learning 
rate 1.0η = . The details of the simulation results are shown in Table 1. 
 
0 50 100 150 200
4
6
8
10
12
14
x 10
-4
iter (i)
Fi
tn
es
s
 
Fig. 4. The fitness values of PSO through 200 iterations for determining the optimal learning rates of 
the ARTVLA-WNN based on WSVR for Example 2. 
 
  
10
 
0 50 100 150 200
0.245
0.25
0.255
0.26
0.265
0.27
0.275
0.28
iter (i)
 F
itn
es
s
 
Fig. 6. The fitness values of PSO through 200 iterations for determining the optimal learning rates of 
the ARTVLA-WNN based on WSVR for Example 3. 
 
0 100 200 300 400 500 600
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
epoch
RM
SE
 
Fig. 7. The values of RMSE in Eq. (24) after 200 training epochs, in which the comparison between 
the ARTVLA-WNN (solid line) and the ARLA-WNN (dashed line) with the fixed learning rate 
1.0η =  for Example 3. 
 
Table 1. The values of RMSE in Eq. (24) for three Examples after 200 training epochs, in which the 
ARLA with various learning rates, 0.05 2.5η≤ ≤ , and ARTVLA with time-varying learning rates is 
applied to train the WNN based on WSVR method. 
ARLA (η ) 
Example ARTVLA 
2.5 2.0 1.5 1.0 0.8 0.6 0.4 0.2 0.1 0.05 
1 0.0055 0.0098 0.0088 0.0061 0.0112 0.0094 0.0101 0.0105 0.0143 0.0122 0.0085
2 0.0004 0.0352 0.0314 0.0130 0.0089 0.0042 0.0033 0.0022 0.0040 0.0042 0.0041
3 0.2465 0.3107 0.3050 0.2977 0.2603 0.2778 0.2718 0.2847 0.2856 0.3270 0.3096
 
  
12
[16] R. Min, H.D. Cheng, Effective image retrieval using dominant color descriptor and fuzzy 
support vector machine. Pattern Recognition 42:147-157 (2009). 
[17] V. Fernandez, Wavelet- and SVM-based forecasts: An analysis of the U.S. metal and 
materials manufacturing industry. Resources Policy 32:80-89 (2007). 
[18] X. G. Zhang, D. Gao, X.G. Zhang, S.J. Ren, Robust wavelet support vector machine for 
regression estimation. International Journal of Information Technology 11(9):35-45 
(2005). 
[19] L. Zhang, W. Zhou, L. Jiao, Wavelet support vector machine. IEEE Transaction on 
Systems, Man, and Cybernetics– Part B: Cybernetics 34(1):34-38 (2004). 
[20] V.N. Vapnik, Statistical Learning Theory. Wiley, New York, 1998. 
[21] J. Mercer, Functions of positive and negative type and their connection with the theory 
of integral equation. Philosophical Transactions of the Royal Society A: Physical, 
Mathematical and Engineering Sciences 209:415-446 (1909). 
[22] Daunbechies, Ten Lectures on Wavelets. Philadelphia, SIAM, 1992. 
[23] S.G. Mallat, A theory for multiresolution signal representation: The wavelet 
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence 
11(7):674-693 (1989). 
[24] Q.H. Zhang, A. Benveniste, Wavelet networks. IEEE Transactions on Neural Networks 
3:889-898 (1992). 
[25] K. Liano, Robust error measure for supervised neural network learning with outliers. 
IEEE Transactions on Neural Networks 7:246-250 (1996). 
[26] A. Ratnaweera, S.K. Halgamuge, C. Watson, Self-organizing hierarchical particle swarm 
optimizer with time-varying acceleration coefficients. IEEE Transactions on 
Evolutionary Computation 8(3):240-255 (2004). 
[27] V.N. Vapnik, S. Golowich, A. Smola, Support vector method for function approximation, 
regression estimation, and signal processing. Advances in Neural Information Processing 
Systems 9:281-287 (1996). 
[28] X.G. Zhang, D. Gao, X.G. Zhang, S.J. Ren, Robust wavelet support vector machine for 
regression estimation. International Journal of Information Technology 11(9):35-45 
(2005). 
[29] G.E.P. Box, G.M. Jenkins, G.C. Reinsel, Time Series Analysis, Forecasting and Control. 
fourth ed., Wiley, New York, 2008. 
台報告，亦應大會之邀擔任2月5日的GS11場次擔任會議主席。其中第一篇論文的主要內
容是提出ㄧ種混合基因演算法與粒子群聚優化方法設計能漸近穩定控制三維架式起重
機，所設計的控制器具有較少的模糊規則數，但仍能達到漸近穩定控制。至於第二篇論
文的研究內容則是探討如何以非線性時變演化PSO方法估算混沌系統的參數，達到同步
的效果。在發表論文的過程中有多位學者提出問題和我們討論，彼此交換心得，由於敝
人在國際會議用英文發表論文略顯緊張，整體表現尚稱滿意，藉由經驗的累積有助於信
心的建立。 
二、與會心得 
能有機會和各國的專家學者專家交換研究心得是相當寶貴的經驗，參與此次國際會
議以及在日本的所見所聞，讓我們收穫良多，而且也有下列幾點心得： 
(一)對促進台灣學術界之國際聲望有相當助益，如果經費允許，當盡力維持。 
(二)從各國發表之論文來看，可發現雖然台灣學者在各方面的研究，有不錯的表現，不
過各國的進步相當快速，我國學者應該持續努力，以保持和世界同步。 
(三)政府目前有多項獎勵措施，鼓勵博士班學生出國參加國際會議，以增廣見識，相信
博士班學生對此行應有相當多的心得與收穫，此種獎勵措施應繼續實施，以培養台灣未
來的優秀研究人才。 
(四)日本的街道整潔，交通秩序良好，人民普遍相當守法，值得我們效法。 
三、建議 
大會安排的論文發表場次過於密集，致使與會者無法參加許多感興趣的場次，此為
美中不足之處，另外大會提供與會者討論和休息的空間嚴重不足，也造成與會者在相互
討論和交流方面的諸多不便，這些缺失可提供我國承辦國際會議時借鏡之用，台灣有許
多風景名勝，應也可乘主辦國際會議之際加以宣傳，以吸引各國嘉賓。 
四、攜回資料名稱及內容 
2010 15th AROB論文集之光碟片乙片，以及會議議程手冊乙本。 
 
 
 
 optimization can be illustrated with Fig. 1. 
 
0X 1,..., MX X
1,..., MY Y
0 0( , , )F=X X X Q
0
ˆ ( , , )F=X X X Q
 
Fig. 1 The parameter estimation scheme for chaotic 
systems 
 
Due to the unstable dynamic behavior of chaotic 
systems, the parameters are not easy to obtain. In 
addition, there are often multiple variables in the 
problem and multiple local optima in the landscape of J, 
so traditional optimization methods are easy to trap in 
local optima and it is difficult to achieve the global 
optimal parameters. 
III. NONLINEAR TIME-VARYING 
EVOLUTION PSO APPROACH 
1. Review of some PSO methods 
Particle swarm optimization, first introduced by 
Kennedy and Eberhart[14], is based on observations of the 
social behavior of animals, such as bird flocking, fish 
schooling and the swarm theory. PSO is initialized with a 
population of random solutions. Each individual (called a 
particle) is assigned with a random velocity and evolves 
according to the flying experiences of its own and 
companions. The particles then fly through hyperspace 
and approach the global optimum. In PSO algorithm, 
each particle keeps track of its own position and velocity 
in the problem space. At each iteration, the new positions 
and velocities of the particles are updated using the 
following two equations: 
( 1) ( ) ( +1)i i iP k P k V k+ = +   for   (4) 1,  2, ,  i m= "
1 1
2 2
( 1) ( ) ( ( ) ( )
( ( ))
l
i i i i
g
i
V k V k c r P k P k
c r P P k
+ = + ⋅ ⋅ −
+ ⋅ ⋅ −
)   (5) 
where  is the number of particles in a population,  
is the number of current iteration,  and  are 
acceleration coefficients,  and  are random 
numbers between 0 and 1, , , and  are 
the position, the local best, and the velocity of ith particle 
at iteration , 
m k
1c 2c
1r 2r
( )iP k ( )
l
iP k ( )iV k
k gP  is the global best of all particles. 
Several researchers have put much effort to improve 
the original version of PSO since the introduction of the 
PSO method in 1995[14]. Shi and Eberhart[18] used a 
linearly varying inertia weight over iterations. The 
mathematical representations of this PSO method are 
given as shown in (4) and 
1 1
2 2
( 1) ( ) ( ) ( ( ) ( ))
( ( ))  for 1,  2, ,    
l
i i i i
g
i
V k k V k c r P k P k
c r P P k i m
ω+ = ⋅ + ⋅ ⋅ −
+ ⋅ ⋅ − = "
    (6) 
where the acceleration coefficients  and  are fixed, 
 and  are two random numbers. The inertia weight 
starts with a high value 
1c 2c
1r 2r
maxω  and linearly decreases to 
minω  at the maximal number of iterations. From hereafter, 
this PSO algorithm will be referred to as the time-varying 
inertia weight factor method (TVIWPSO). 
Eberhart and Shi[19] found that the TVIWPSO method 
is not very effective in tracking dynamic systems. 
Considering the dynamic nature of real-world 
applications, they proposed a random inertia weight factor 
to track dynamic systems. In their method, the 
representations are the same as those in the TVIWPSO 
method except that the inertia weight factor changes 
randomly. In the rest of this paper, this algorithm will be 
referred to as the RANDWPSO method. 
An automation strategy for the PSO with time- 
varying acceleration coefficients was proposed[20]. The 
objective is to enhance the global search in the early part 
of the optimization and to encourage the particles to 
converge toward the global optimum at the end of the 
search. In their method, the representations are the same 
as those in the TVIWPSO method except that the 
acceleration coefficients change according to linear 
time-varying evolution. From hereafter, this algorithm 
will be referred to as the TVACPSO method. 
A time-varying nonlinear function modulated inertia 
weight adaptation was proposed by Chatterjee and 
Siarry[21]. In this method, the acceleration coefficients are 
also fixed. However, the inertia weight starts with a high 
value maxω  and nonlinearly decreases to minω  at the 
maximal number of iterations. This means that the 
representations are the same as those in the TVIWPSO 
method except that the inertia weight factor changes 
according to 
max
min max min
max
( ) ( )
iter iter
k
iter
α
ω ω ω ω−= + ⋅ −⎛ ⎞⎜ ⎟⎝ ⎠   (7) 
where  is the maximal number of iterations and 
 is the current number of iterations. 
maxiter
iter
2. PSO-NTVE method based on orthogonal arrays 
In this section, based on the concept presented[20,21], an 
NTVEPSO method is proposed. In the proposed PSO 
method, the inertia weight is given as described in (7). 
The cognitive parameter  starts with a high value 
and nonlinearly decreases to . Meanwhile, the 
social parameter  starts with a low value  and 
nonlinearly increases to . This means that the 
mathematical expressions are given as shown in (4), (7), 
and 
1c
1maxc 1minc
2c 2 minc
2 maxc
 
1 1
2 2
( 1) ( ) ( ) ( ) ( ( ) ( ))
( ) ( ( ))  for 1,  2, ,    
l
i i i i
g
i
V k k V k c k r P k P k
c k r P P k i m
ω+ = ⋅ + ⋅ ⋅ −
+ ⋅ ⋅ − = "
 (8) 
The Fifteenth International Symposium on Artificial Life and Robotics 2010 (AROB 15th ’10),
B-Con Plaza, Beppu,Oita, Japan, February 4-6, 2010
©ISAROB 2010 327
 [4] Saha P, Banerjee S, Chowdhury AR (2004), Chaos, 
signal communication and parameter estimation. 
Phys Lett A 326(1-2):133-139 
[5] Alvarez G, Montoya F, Romera M, Pastor G (2003), 
Cryptanalysis of an ergodic chaotic cipher. Phys 
Lett A 311(2-3):172-179 
[6] Wu XG, Hu HP, Zhang BL (2004), Parameter 
estimation only from the symbolic sequences 
generated by chaos system. Chaos, Solitons & 
Fractals 22(2):359-366 
[7] Xu DL, Lu FF (2005), An approach of parameter 
estimation for non-synchronous systems. Chaos, 
Solitons & Fractals 25(2):361-366 
[8] Fostin HB, Woafo P (2005), Adaptive 
synchronization of a modified and uncertain chaotic 
van der Pol–Duffing oscillator based on parameter 
identification. Chaos, Solitons & Fractals 
24:1363-1371 
[9] Gu M, Kalaba RE, Taylor GA (1996), Obtaining 
initial parameter estimates for chaotic dynamical 
systems using linear associative memories. Appl 
Math Comput 76:143-59 
[10] Guerra FA, Coelho LS (2008), Multi-step ahead 
nonlinear identification of Lorenz’s chaotic system 
using radial basis neural network with learning by 
clustering and particle swarm optimization. Chaos, 
Solitons & Fractals 35:967-979 
[11] Dai D, Ma XK, Li FC, You Y (2002), An approach 
of parameter estimation for a chaotic system based 
on genetic algorithm. Acta Phys Sinica 
11:2459-2462 
[12] He Q, Wang L, Liu B (2007), Parameter estimation 
for chaotic systems by particle swarm optimization. 
Chaos, Solitons & Fractals 34:654-661 
[13] Liu B, Wang L, Jin YH, Tang F, Huang DX (2006), 
Directing orbits of chaotic systems by particle 
swarm optimization. Chaos, Solitons & Fractals 
29:454-61 
[14] Kennedy J, Eberhart R (1995), Particle swarm 
optimization. Proc of the IEEE Int Conf on Neural 
Networks, pp. 1942-1948 
[15] Gaing ZL (2004), A particle swarm optimization 
approach for optimum design of PID controller in 
AVR system. IEEE Trans on Energy Conversion 
19(2):384-391 
[16] Boeringer DW, Werner DH (2004), Particle swarm 
optimization versus genetic algorithms for phased 
array synthesis. IEEE Trans on Antennas and 
Propagation 52(3):771-779 
[17] Elbeltagi E, Hegazy T, Grierson D (2005), 
Comparison among five evolutionary-based 
optimization algorithms. Advanced Eng Inform 
19(1):43-53 
[18] Shi Y, Eberhart RC (1998), A modified particle 
swarm optimizer. Proc of the IEEE Int Conf on 
Evolutionary Comput, pp. 69-73 
[19] Eberhart RC, Shi Y (2001), Tracking and optimizing 
dynamic systems with particle swarms. Proc of the 
IEEE Int Congress on Evolutionary Comput, pp. 
94-100 
[20] Ratnaweera A, Halgamuge SK, Watson HC (2004), 
Self-organizing hierarchical particle swarm 
optimizer with time-varying acceleration 
coefficients. IEEE Trans on Evolutionary Comput 
8(3):240-255 
[21] Chatterjee A, Siarry P (2006), Nonlinear inertia 
weight variation for dynamic adaptation in particle 
swarm optimization. Computers and Operations 
Research 33:859-871 
[22] Montgomery DC (1991), Design and Analysis of 
Experiments, third edition. Wiley, New York 
[23] Hicks CR (1993), Fundamental Concepts in the 
Design of Experiments, fourth edition. Saunders 
College, Texas 
 
The Fifteenth International Symposium on Artificial Life and Robotics 2010 (AROB 15th ’10),
B-Con Plaza, Beppu,Oita, Japan, February 4-6, 2010
©ISAROB 2010 329
98年度專題研究計畫研究成果彙整表 
計畫主持人：柯嘉南 計畫編號：98-2221-E-252-004- 
計畫名稱：混合智慧型系統方法對含離異點之非線性系統模型化之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 發表於2009年 第
17 屆模糊理論及
其應用研討會 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 2 0 100% 
人次 
二位大專生擔任
兼任助理參予此
計畫 
期刊論文 1 0 100% 
一篇文章發表於
Applied 
Mathematics and 
Computation 期刊
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
發 表 於 The 
Fifteenth 
International 
Symposium on 
Artificial Life 
and Robotics 
2010 (AROB 
15th ＇10), 
B-Con Plaza, 
Beppu,Oita, 
Japan, February 
4-6, 2010 
論文著作 
專書 0 0 100% 章/本  
國外 
專利 申請中件數 0 0 100% 件  
 
