1 
 
行政院國家科學委員會專題研究計畫 期終進度報告 
 
基於雲端技術應用於手持式行動裝置的自由視角立體 
影像監視系統(第3年) 
期終進度報告(精簡版) 
 
 
計畫類別：個別型 
計畫編號：NSC 99-2221-E-032-032-MY3 
執行期間：101 年08月01日至102年07月31日 
執行單位：淡江大學電機工程學系 
 
 
計畫主持人：李維聰 
 
報告附件：出席國際會議研究心得報告及發表論文 
 
處理方式：期終報告不提供公開查詢 
 
 
 
中  華  民  國 1 0 2 年 0 7 月 3 1 日 
 
3 
0. 摘要 
Recently, within the developing of 3D imaging technologies, a major issue 
which to reduce time cost of 3D imaging computing are virtual and rising 
simultaneously. To face this issue, we establish 3D imaging computing system in 
cloud environment. In this paper, we propose a practical solution which combines 
cloud technology with 3D imaging computing system. 
近年，在各式 3D 成像技術被廣泛應用的同時，如何解決即時 3D 成像因
大量運算所付出的時間成本一直是相當重要的議題，因此本計畫將建立即時 3D 
成像系統並運用雲端技術輔助運算，雲端技術是為了解決單一電腦所不能完成的
龐大運算任務而發展出的技術，透過聯合許多電腦遠端分散運算，進而大幅縮短
運算時間，本計畫將研究 3D 成像系統配合雲端技術以即時取得自由視角系統
與如何將本系統結合雲端技術以降低運算時間。 
1. 前言 
本計畫是著眼於近年來，雲端技術的發展以及影像傳播系統的演進，在未來
資訊裡將會走向隨手取得的方向。基於這個立足點，雲端技術就相對的顯得重要。
而近年高畫質電視的發展幾乎已達顛峰，各種新的高畫質播放系統、傳輸方式皆
被提出來。本計畫的應用是基於高畫質影像系統已經成熟的今天，能夠帶給大眾
一種全新的體驗。 3D 以及自由視角技術的展現，除了可以提升大眾的視覺效
果外，在學術研究上更是未來的一大挑戰。硬體設發展至今，隨身設備仍有效能
不夠強大的問題。所以本計畫為了達到讓大眾能隨時隨地體驗真實感較高的影像，
提出了以雲端技術結合 2D 轉 3D 技術。以雲端的隨機存取性來解決取得資訊
的問題，利用分散式處理來達成 2D 轉 3D 成像的目標，最後達成任何手持式
裝置皆能隨時隨地取得自由視角的視訊系統。 
2. 關鍵字 
雲端運算、Hadoop、自由視角監視系統、Harris、RTSP 
 
3. 研究目的 
本計畫將建立一個基於雲端技術應用於手持式行動裝置的自由視角立體影
像監視系統，提供使用者可以透過手機監看即時的自由視角影像，計畫將利用一
個結合即時影像處理、雲端技術應用以及高速影像傳輸技術於手持終端顯示來達
到自由視角監視系統。 
 
 
5 
(1) 雲端系統 
雲端運算是資料中心因應網路上資訊暴增而提出的服務及管理思維，資訊服
務提供者投入資源進行雲端運算的服務及架構開發， Google 可說是最大量使用
雲端運算的組織之一。 「Hadoop」 就是由 Google 雲端架構得到啟發而開始
的開放原始碼計劃，目前有許多組織參與 Hadoop 的研究開發，並以 Hadoop 做
為雲端運算的平台。 
Hadoop 是 Apache 軟體基金會(Apache Software Foundation)底下的開放原始
碼計劃 (Open source project)，最初是做為 Nutch 這個開放原始碼的搜尋引擎的
一部份；  
Hadoop 是以 java 寫成，可以提供大量資料的分散式運算環境，而且 
Hadoop 的架構是由 Google 發表的 BigTable 及 Google File System 等技術提
出的概念實做而成，主要技術是 「MapReduce」 這是由 Google 所提出的概念，
目前 Yahoo! 及 Cloudera 等公司都有開發人員投入 Hadoop 的開發團隊，也有
將近一百個公司或組織公開表示使用 Hadoop 做為雲端運算平台， Google 及 
IBM 也使用 Hadoop 平台為教育合作環境。 
下列為 Google 以及 Hadoop 功能的對照表: 
 
表4.1、<對照表> 
 
 
a. MapReduce 
MapReduce 為一套提供開發者進行平行運算的編程模型，這是由 Google 所
提出的平行化運算概念，簡單的來說，就是利用分割任務的方式進行分散式運算，
運算完之後再透過收集結果的動作將資料進行統整以及安排。 
7 
 
圖 4.2、<MapReduce簡易結構圖> 
 
 
圖 4.3、<Key/Value示意圖> 
 
 如圖 4.3所示， Input File 經過 Map 之後將輸出檔案轉為 Reduce 所需要
的 Key/Value 形式檔案，這時候 Reduce 節點會從 Map 節點取得 Intermediate 
Data在經過 Reduce 的計算，也就是將相同的 Key 組合在一起並將結果輸出。 
 
 
9 
的大小預測虛擬機的性能。那麼在本計劃中我們採用了 Virtual Machine 的系統
建置，為了區分 VM 的能力高低，我們引進了 Benchmark 的機制，並訂立的
Benchmark Score 好讓系統區分 VM 能力的高低。 
d. 雲端架構(網路部分) 
 
圖 4.4、<Green Master Architecture> 
如圖 4.4所示，雲端運算簡單來說就是藉由有效率的管理 Resources Pool 來
達到我們需要的目的，使用者們或是開發者們可以透過一個有效率的平台來操作
或是管理這些資源，那麼一個有效率的平台或是一個有利於管理資源的演算法就
顯得格外重要。本計劃將在這章節的部分介紹本計劃所提出的 Green Master[3]
架構，可以有效的解決 Cloud Computing Cluster 中不必要的虛擬機器或是在不
嚴重影響整體系統效能的前提下刪除不必要的節點來達到節能的目的。 
 
e. 雲端(研究方法) 
如圖 4.5所述，此圖為 Green Master 的系統流程圖， Input File Index 一旦
接收到有資料進入則會把 Index 值變為 1，此時就會要求各節點傳送自己的
Server Information ，根據這些資訊進行 Benchmark Score 評估數值。分數評估
完之後會按照分數高低配給 VM Number ，也就是進行 Sorting 的動作。Task 
Distribution 會根據 Benchmark Score 進行工作的初始化，接著將工作的分配、
伺服器分數以及 VM Number 更新到 Record Table 上。接下來根據公式進行系
11 
GM 裡。另外，每當有新的伺服器或是新的 VM 加入或是退出， Benchmark 
Score 也會隨之改變。 
 Benchmark 的評分方式(範圍從 0~100)是根據 VM 的 CPU computing , 
Memory read/write 以及  Disk I/O 的速率高低來做評分。在這部分我們採用的
是相對值，也就是說我們先將數值最高的 VM 作為 Benchmark 最高的 100分，
之後再根據比率對其他效能較低的 VM 進行評分。 
                                    
  
     
                    (4.1) 
其中， 
Top為 VM 中分數最高的數值 
Xn為待測 VM 的數值 
因為我們需要考慮的 VM 變數不只一項，因此我們將變數的項目數目設為
W，所以又將公式演化如下： 
                  
  
     
                                    
 
 
 (4.2) 
範例如下： 
表 4.2、<Record Table> 
 Benchmark Score Loading Ratio 
VM 1 100 33.3% 
VM 2 90 30% 
VM 3 75 25% 
VM 4 20 6% 
VM 5 10 3% 
VM 6 5 1.6% 
在 Record Table 的部分，除了記錄 Benchmark Score 之外也會做 Sorting
的動作，也就是會依照 Benchmark Score 的高低值來進行排列，因此 VM1 性
能會優於 VM2 ，依此類推。 Sorting 這個動作是為了方便我們在稍後的 Power 
Saving Algorithm 作應用。 
13 
其中， 
  代表的是 n 台虛擬機器時的系統運作時間 
  表示的是第 i 台單獨運算的時間 
  以及  則是代表 Benchmark Score 值 
 
                                                          (4.4) 
其中， 
  表示 n台虛擬機器運算的消耗能量 
  則是上述所說的系統運算時間 
   為虛擬機器的數量 
   則為單位虛擬機器的耗能功率 
 
                          
       
       
                          (4.5) 
其中， 
  表示在不同時間段裡的單位時間耗能，我們稱之為單位時間差值比。 
 
                                 
     
     
                           (4.7) 
其中， 
      表示從一開始到系統結束的平均單位消耗，我們稱之為平均單位時間差值
比。 
 
在這個章節我們利用實驗結果先得知虛擬機器跑完任務的系統時間，之後再
利用觀察以及統計推算得到公式的近似值公式，如公式(4.4)；之後再得到系統耗
能，如公式(4.5)；利用公式(4.4)以及(4.5)得到單位時間差值比，如公式(4.6)。通
過比較差值比的關係，我們可以判斷在哪個時間段裡的系統單位時間耗能式呈現
正向成長的。 
 
 
15 
 
圖4.6、<影像簡易流程圖> 
 
b. 特徵點擷取 
「特徵點」(feature point)是計算機視覺中用來提取圖像特徵以推測其內容的
一種方法。擷取特徵點方式有許多種：「Harris」[5]觀察圖像 X、Y方向的梯度
變化歸納出「角」的位置，並定義其為特徵點； David Lowe[6] 將圖像高斯卷
積後，以像素與相鄰尺度的差異性來定義特徵點； Mohammad[7] 將影像邊緣化
後取出像素的曲率來定義特徵點，並將高維度的特徵點與低維度重新定位。 
擷取特徵點的目的除了為了辨識之外，維持物件結構的完整度也是相當重要
的一環，因此我們選擇「角點」(Corner)作為我們的特徵點。所謂的「角點」，
也就是物件邊緣有稜角的位置。在計算機視覺中，常以邊緣梯度出現劇烈變化做
為判斷「角點」的依據。 
本計畫使用「Harris」角點偵測做為判斷角點的演算法，原因為該演算法速
度較快，且誤差與誤判皆在接受範圍，為了不降低系統整體的即時性，故選擇此
方法。「Harris」角點偵測是 Chris Harris 與 Mike Stephens 提出的特徵點演算
法。 
Harris 此演算法是根據 Moravec Operataor[8] 改良而來的。 Moravec 使用
兩個邊長為三的矩形 A、B套用在灰階圖像上，以 A矩形的中心為目標像素，B
矩形的外圍與目標像素進行重疊，並將兩矩形範圍內的的梯度進行累加，得到強
度直 V： 
17 
               
   
  
  
   
  
 
 
                 
 
      
 
   
 
  
    
   
  
   
  
   
   
 
  
 
                 
 
                                                            (4.11) 
其中， 
       
   
 
  
       
   
 
  
       
   
  
   
  
    
將            整理可得 
            
           
                                      
 
 
                          (4.12) 
其中， 
    
  
  
  
Harris 觀察出，藉由矩陣 M 的特徵值與角點的關係，可判斷該像素是否為
角點。 與角點的關係如圖 4.8所示： 
 
圖 4.8、<特徵值與角點關係圖> 
當  與  皆為極小值時，判斷該像素為平緩；當  與  其中一方出現極大值
時，判斷該像素為邊緣；當  與  皆為極大值時，便判斷該像素為角點。 
19 
d. 特徵點穩定 
角點對本計畫提出的系統而言不僅僅是用於辨識，也是維持物件結構完整度
的一個重要手段。因此在即時影像系統中，每一幀之間的角點變化對系統的影響
都是相當巨大的。然而自然界中的光影變化、色彩多樣，常常出現某像素的特徵
點強度值落在角點偵測的閾值周圍，導致角點閃爍以及角點位移的情形出現。有
許多研究嘗試解決這個問題，如[10][11]，將閾值改成動態設定，有效降低角點
閃爍的情況出現，但卻無法處理角點位移問題。計畫裡提出一套適用於所有特徵
點偵測在即時影像系統的穩定機制，能大幅降低特徵點閃爍與位移的出現。 
在特徵點閃爍的問題方面，本機制會紀錄特徵點於每一幀出現的次數後，給
予該特徵點一個強度值 D。當此強度值 D 大於給定的閾值後，判斷該特徵點為
有效特徵點，強度值 D的公式如下： 
                                    
 
                       (4.13) 
其中， 
   代表該角點於時間點 t 時是否「有」出現，若有出現則    ，否則    。 
   代表該角點於時間點 t 時「無」出現，若無出現則    ，否則    。 
 
   與  為  和  的權重，且     以確保強度值在角點出現時能確實上升。
強度值 D必須要制訂一個上限值，一般不會大於  的三倍，以避免在角點正常
消失時系統無法及時反應。 
而在處理角點位移的方面，本機制會判斷該特徵點先前的位置，給予權重後
重新將特徵點定位。定位後的角點  nP 公式如下： 
                      
         
   
 
     
       
    
                   (4.14) 
其中， 
    代表     時定位後的特徵點位置 
  代表   時未經穩定前的特徵點位置 
  與  
 代表  與  
 的權重 
 
e. 匹配校正 
SURF 演算法在匹配兩張圖的特徵點時，歸納先算出的主向量及 64筆特徵資
訊後，計算出一筆相異度資訊，並將相異度最低且與次低的差距較突出的點匹配，
兩重步驟是為了防止過於相似的點發生誤判，但也就是變相無視一些較難辨識的
特徵點以避免出錯。 
在本計畫的系統中每一個角點都是必要的，若是出現差異度較低的特徵點，
便會出現被 SURF 匹配機制篩選掉的窘境。為了解決這個問題，本計畫在匹配
21 
表 4-3、< d 、 A、 L與其權重之關係> 
 數值範圍 權重 
d  1  10d   
A  0 ~ 90。 。 1A   
L  0~I 之對角線長 0.1L   
 
f. Real Time Streaming Protocol(RTSP) 
傳統的網路多媒體必須等待檔案完全下載完畢才能做撥放， Streaming 技術
的出現，使得網路多媒體能夠達到即時性的撥放多媒體，而 Real Time Streaming 
Protocol(RTSP) 就是其中一種。 RTSP 協定是由 RealNetworks 和 Netscape 共
同提出，後由 MMUSIC IETF 團隊制定，是應用層伺服器模式的時間同步串流
控制協定。 RTSP 協定在諸多方面與 HTTP  協定類似，諸如兩者有相同的擴
展機制；不同在於 HTTP 是非對稱協定(只能由 Client 端發出請求)，而 RTSP
是一對稱協定( Client、Server 端皆可發出請求)，也不特別強調時間同步，在網
路延遲上有較高的時間容忍度。 RTSP 本身建立於 RTP、RTCP 協定之上，提
供一個可供擴展的框架，建立並控制一個或複數個時間同步媒體串流。 
RTSP 本身並不傳送多媒體資訊，僅作為多媒體資料的即時控制，諸如撥放、
暫停或快轉等操作，傳送所選擇的通訊協定部不在 RTSP 的定義範圍內，需依
賴下層傳輸協議 TCP 或是 UDP 。Real-time Transport Protocol (RTP) 是一個網
路傳輸協定，RTP 協議詳細說明了在網際網路上傳遞多媒體的標準資料包格式。
Real-time Transport Control Protocol (RTCP) 與 RTP 是一個姊妹協定，RTCP 與
RTSP 相同本身並不傳輸多媒體數據，主要功能是收集多媒體相關的統計信息，
諸如傳輸位元組數、遺失位元組數、單向和雙向網路延遲等等，提供給網路程序
已提升整體服務質量。 RTSP 在建立連線後會自動與播放器交涉選擇最佳的傳
遞機制，然後指示 Real-time Transport Protocol (RTP) 通訊協定依照網路狀態使
用 UDP 或 TCP 傳遞串流處理內容，並由 RTCP 收集收集相關媒體連接的統
計信息，以協助 RTP 將多媒體數據打包和發送。 
 
 
23 
(四)圖像重建與貼圖 
 為了模擬不存在攝影機的視角，本系統使用內插法計算匹配點間的座標，如
公式(4.19)所示 
        
           
 
                          (4.19) 
其中， 
   與  分別為左攝影機和右攝影機捕捉到的角點 
  為  和  以被匹配成功，並將兩點間分成 n個單位 
  則為與  的單位距離 
 
圖 4.12 中，左圖為左攝影機所拍攝到的畫面，右圖為右攝影機拍攝到的畫
面，三角形 ABC 為 Delaunay 分別於左右圖切割出的三角形，模擬視角座落於
兩攝影機中，計算各對應點的座標位置 P後，可模擬出該視角的三角形形狀。 
左攝影機 右攝影機模擬視角
 
圖 4.12、<模擬視角的角點計算> 
在推算出模擬視角中每一個三角形位置後，計算與較接近該視角的攝影機對
應的三角形的仿射矩陣，並該三角形對此矩陣進行卷積，以計算出新三角形的圖
像。 
當得到所有完成仿射轉換後的三角形後，將這些三角形重新貼圖，以拼湊出
模擬視角的視點，如圖 4.13所示。 
 
圖 4.13、<Delaunay三角重新貼圖> 
25 
 
圖5.2、<雲端伺服器測試結果> 
(2)影像處理系統 
本計畫第一年建構了基礎的雙眼視覺系統與 2D to 3D成像系統。 
雙眼視覺系統為最簡單的立體視覺系統，使用兩台攝影機取代左眼與右眼，
來擷取影像，流程圖如圖5.3所示，其使用左右兩攝影機得到影像，將影像進行
特徵點擷取，本計畫採用Harris所改良的方式來擷取特徵點，並將兩影像之特徵
點以NCC方式將特徵點匹配。 
 
圖5.3、<雙眼視覺流程圖> 
 
Harris 角點偵測演算法基於 Moravec 的角點偵測所衍伸出來，由於 
Moravec 的角點偵測方向性考慮不夠周全，且容易受到雜訊影響，因此本計畫採
用 Harris 所提出改良的方法[17][18]。公式定義如(5.1)式所示。 
                                                              (5.1)                  
 
27 
 
圖5.4、<雙眼視覺成果> 
 
本計畫 2D to 3D 成像系統也以人眼視覺系統為原理，使用攝影機將像素為
320x240 pixel 動態影像擷取後(如圖5.5上半部所示，其分別為左視圖與右視圖)，
先將代表左眼的左視圖和代表右眼的右視圖，分別做顏色上的處理(左眼影像只
留有紅色的資訊、右眼影像只留有綠色的資訊) ，再將兩張圖合併到一起(如圖
5.5下半部所示)，這樣就變成了一張透過顏色來區隔左右眼影像的立體圖片了。
再透過對應顏色的鏡片（一般是使用玻璃紙）來觀看這樣的影像，就可以讓兩眼
看到各自不同的影像，進而產生立體的視覺效果了。 
29 
6. 第二年研究成果 
(1)雲端系統 
在第二年我們向中華電信申請 hiCloud 雲端服務，並於 hiCloud 的 Cluster 
上面架設 Hadoop 工作環境。區別於第一年的 Single Node ，我們在 hiCloud 上
面實施 MultiNode Working，確實在 Map 階段將工作分配給各個節點運算，並
於 Reduce 階段將運算結果儲存備份於多節點上。運行結果圖6.1所示: 
 
 
圖6.1、<運行結果> 
31 
 
圖6.3、<原始影像> 
 
圖6.4、< SURF演算法擷取之結果圖> 
 
圖6.5、< Harris角點偵測結果圖> 
 
圖6.6、< Harris-SURF演算法結果圖> 
 
 
33 
7. 第三年研究成果 
(1)實驗結果 
本計畫所提出的特徵點穩定機制根據公式(7.1)來定義對系統改進的百分比 
                     
          
                      
 
                         (7.1) 
其中， 
         為未使用穩定機制的每偵差異性 
         為使用穩定機制的每偵差異性 
我們統計兩千偵的資訊後計算出各種物件下的穩定度，如表七-1所示。 
 
  
改進 
比例和 
1523.6 1647.2 
改進度 76.2% 82.4% 
 
  
改進 
比例和 
1856.1 1586.4 
改進度 92.8% 78.4% 
表 7.1、<特徵點穩定機制實驗數據> 
從數據中可看出，經過本計畫提出的特徵點穩定機制對於 Harris 角點偵測
的改進。本機制對於較複雜的物件有顯著的提升，對於較簡單的物件亦有七成以
上的穩定提升。 
匹配校正機制則根據肉眼統計 200幀的匹配結果的來計算正確度，結果如表
7.2所示。 
從表 7.2可看出，當物件有較多相似特徵點時，本機制較能有效地提升匹配
準確率。 
35 
8. Reference 
[1] Ling-Shang Kuo, “MapReduce-based Image Processing System with 
Priority-based DSRF Algorithm,” 淡江大學, 2012. 
[2] Xia Xie, Qingcha Chen, Wenzhi Cao, Pingpeng Yuan, Hai Jin, “Benchmark 
Object for Virtual Machines,” 2010 Second International Workshop on 
Education Technology and Computer Science. 
[3] Ming-Zhi Wu, “Power Saving of Virtual Machine Assignment Research based 
on Different Performance of Virtual Machine Distribution,” 淡江大學, 2013. 
[4] Tzu-Ti Chang, Fang-Yi Yu,  Wei-Tsong Lee,  Feng-Yu Chang, Jason Wu, “Free 
View Point Real-Time Monitor System Based on Harris-SURF,” Smart 
Innovation, Systems and Technologies Volume 21, 2013, pp 423-430 
[5] C. Harris and M. Stephens, “A Combined Corner and Edge Detector,” 
Proceedings of the Fourth Alvey Vision Conference, pp.147-151, 1988. 
[6] Lowe, David G., "Object recognition from local scale-invariant features". 
Proceedings of the International Conference on Computer Vision 2. pp. 
1150–1157. 
[7] Farzin Mokhtarian , Riku Suomela, "Robust Image Corner Detection Through 
Curvature Scale Space," IEEE Transcations on Pattern Analysis And Machine 
Intelligence, Vol. 20, NO. 12 
[8] H.P. Moravec, “Towaeds Automatic Visual Obstacle Avoidance,”Proc. Int’l Joint 
Conf. Artificial Intelligence, p584, 1977 
[9] Zhili Li ,Yanchun Shen, “A Robust Corner Detector Based on Curvature Scale 
Space and Harris,” International Conference on Image Analysis and Signal 
Processing 
[10] Mohammad Awrangjeb, Guojun Lu, “An Improved Curvature Scale-Space 
Corner Detector and a Robust Corner Matching Approach for Transformed 
Image Identiﬁcation,” IEEE Transactions on Image Processing, Vol. 17, Issue 12 
[11] Sun Junding , Zhang Zhaosheng, “A new contour corner detector based on 
curvature scale space,” Fuzzy Systems and Knowledge Discovery, 2009. FSKD 
'09. Sixth International Conference ,Volume 5, Aug. 2009 
[12] Niitsuma H., Maruyama T., “Sum of Absolute Difference Implementations for 
Image Processing on FPGAs,” International Conference on Field Programmable 
Logic and Applications, pp. 167 – 170, Sept. 2010 
[13] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, “Speeded-Up Robust Features 
(SURF),” Computer Vision and Image Understanding, vol. 110, pp. 346-359, 
2008. 
[14] Engin Tola, Vincent Lepetit, Pascal Fua, “DAISY: An Efficient Dense Descriptor 
Applied to Wide-Baseline Stereo,” Pattern Analysis and Machine Intelligence, 
IEEE Transactions, Volume 32 , Issue 5 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/10/29
國科會補助計畫
計畫名稱: 基於雲端技術應用於手持式行動裝置的自由視角立體影像監視系統
計畫主持人: 李維聰
計畫編號: 99-2221-E-032-032-MY3 學門領域: 計算機網路與網際網路
無研發成果推廣資料
博士後研究員 0 0 100%  
專任助理 0 0 100%  
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
參與多項國際研討會，並與各國學者進行影像與雲端技術上之討論。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
