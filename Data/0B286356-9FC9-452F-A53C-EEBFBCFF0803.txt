2 
multi-resolution disparity maps so as to produce full 
disparity map. Ground purge deletes disparity values 
resulting from ground so as to obtain obstacle disparity map. 
Edge map is extracted from original right image and used to 
segment the obstacle disparity map. Obstacles are detected 
as segments with significant disparity values. Locations of 
interested obstacles are calculated by triangulation. 
 
Fig. 1 Procedures of multi-resolution obstacle detection 
2-1 Multi-resolution Correlation Matching 
As shown in Fig. 2, multi-resolution correlation matching 
constructs full disparity map from stereoscopic images by 
reduction, correlation matching, expansion, and linear 
combination. Fig. 3 shows an example of reduction and 
correlation matching. Fig. 4 shows the results of correlation 
matching step-by-step. Fig. 5 shows an example of 
expansion and linear combination. 
1)  Reduction 
The reduction procedure samples stereoscopic images 
sequentially to obtain multi-resolution stereoscopic 
images called left and right image pyramids. For 
efficient computation, reduction is done by simply 
reserving odd pixels. Denote an image in resolution level 
l by lg  where L,3,2,1=l , the reduction procedure can 
be represented by 
( )1−= ll greduceg   (1)  
where ( ) ( )12,12, 1 −−= − jigjig ll , i and j index row and 
column, respectively. In Fig. 3, the left image pyramid is 
composed of left image, L_reduce1, L_reduce2, and 
L_reduce3. The right image pyramid is composed of 
right image, R_reduce1, R_reduce2, and R_reduce3. 
2)  Correlation matching 
The correlation matching procedure transforms a pair of 
image pyramids into a disparity pyramid. Define the 
coordinate systems and parameters of a non-verged and 
well aligned stereoscopic camera as shown in Fig. 6. The 
ordinate of the projection of a point P is calculated by 
θθ
θαθθαθ
cossin)(
]sincos[)](cossin[ 00
ZhY
ZvhYv
v
++
−+++
= (2)  
Correlation matching is measured by the sum of absolute 
difference (SAD) for 77×  block [1]. 
( ) 





+−= ∑
vu
SAD vduIvuIvumatching
,
21 ),(),(min,  (3) 
To reduce the search burden, correlation matching is not 
done on full images, but simply on potential obstacles. 
Potential obstacles are found by a sequence of edge 
extraction, binarization and hole-filling on the right 
image. Fig. 4 (a) and (b) are original right and left images, 
respectively. Fig. 4 (c) is the edge information of the 
right image obtained by applying Sobel Filter, Fig. 4 (d) 
is the binarized the edge information. After filling in the 
holes, the region of potential obstacle is found as shown 
in Fig. 4(e). Correlation matching is executed on the 
potential obstacle with respect to the left image to obtain 
the disparity map of the obstacle as shown in Fig. 4 (f). In 
Fig. 5, the disparity pyramid is shown in the middle. 
With correspondences of P, the disparity ∆  is 
calculated by  
θθ
α∆
cosZsinhY
b
uu rl
++
=−= )(                    (4)  
 
Fig. 2 Procedures of multi-resolution correlation matching 
4 
 
 (a)  (b) 
Fig. 8 (a) Original right image, (b)V-disparity map of (a) 
 
 (a)  (b) 
Fig. 9 (a) Ground correlation line of images in Fig. 7 (a); 
(b) Linear ground correlation line 
 
 (a)  (b)  (c) 
Fig. 10 Using Fig. 8 (a) as input, (a) Full disparity map, (b) 
Ground disparity map, (c) Obstacle disparity map 
In the ground purge procedure, we assume ground within 
a scene is a plain so that the ground correlation line of clear 
ground should be linear. Hence the ground correlation line 
of a stereoscopic image pair is transformed into a linear 
ground correlation line. Specifically, area under the linear 
ground correlation line equals that of under the ground 
correlation line. In addition, linear ground correlation line 
has minimum and maximum corresponding to most distant 
and nearest points, respectively. 
min)n(f
k
max
k
n
−= ∑
=0
2
 (6)  
For instance, Fig. 9 (a) shows the ground correlation line 
of Fig. 8 (a), and the corresponding linear ground correlation 
line is presented in Fig. 9 (b). Then according to linear 
ground correlation line, ground disparity map is produced 
and subtracts from full disparity map. Fig. 10 shows an 
example of ground purge. Fig. 10 (a) is the full disparity map 
of Fig. 8 (a), Fig. 10 (b) is the ground disparity map, and (c) 
is the difference of full disparity map and ground disparity 
map. 
2-3 Segmentation and Triangulation 
To discriminate obstacles from each other, obstacle 
disparity map must be segmented. Segmentation of obstacle 
disparity map is obtained by overlapping edge map with 
obstacle disparity map. Edge map is extracted from original 
right image by using Sobel filter. Obstacles are detected as 
segments with significant disparity values (larger than a 
threshold). Considering nearby segments may belong to a 
single object, dilation can be invoked to combine nearby 
segments into a single one. As the example shown in Fig.11, 
(a) shows the right image of a car, (b) is the corresponding 
obstacle disparity map, (c) is the binary obstacle disparity 
map, (d) is the edge map of right image, (e) is the segmented 
obstacle disparity map without dilation, and (f) is the 
segmented obstacle disparity map with dilation.  
For navigation, each obstacle or simply the interested 
obstacle is located by transforming its disparity values into 
depths by triangulation. Assuming the stereoscopic camera 
is non-verged and well aligned so that epipolar constraint is 
satisfied. Given baseline b and focal length f, and a non-zero 
disparity ∆  taken from full disparity map, depth is 
calculated by 
∆
fbZ =    (7)  
2-4 Test on Multi-resolution Obstacle Detection 
To test the functionality of multi-resolution obstacle 
detection, an experimental smart stereo camera has been 
built. The stereoscopic camera has been aligned accurately 
so that each pair of scan lines rests on one epipolar line. The 
smart stereo camera is equipped in a car to capture scenes 
just like driver sees. Fig. 12 (a) shows original right image 
captured for this test. After applying multi-resolution 
obstacle detection, Fig. 11 (b) shows the full disparity map, 
(c) shows the obstacle disparity map, (d) shows the 
segmented obstacle disparity map, (e) shows the binary map 
of the segmented obstacle disparity map, (f) shows the 
interested region, (g) shows the interested region after 
dilation, and (h) shows the location of the interested obstacle. 
Experimental results show that multi-resolution obstacle 
detection works nicely in real road environment.  
3. Conclusion 
Smart stereo camera equipped with the multi-resolution 
obstacle detection algorithm has been shown able to find and 
locate obstacles in the scene. This makes it useful in active 
safety surveillance such as assisting driver in visual 
navigation. But before going to real applications, the 
reliability of the system must be improved and tested strictly. 
Furthermore, in spite of human vision can promptly choose 
an interested target out of complicated scene, it seems not so 
easy for a smart stereo camera to achieve it. Future work will 
involve multi-resolution obstacle detection with an 
intelligent system to find interested obstacles.  
   
  (a) (b) (c)  
    
 (d) (e) (f)  
 
 
  
Abstract—Two dimensional shape recognition needs to 
remove the effect of geometric transformations from the 
contour pattern. This is usually obtained by transforming the 
contour pattern into an affine invariant form. In this paper, the 
cosine transform is used to decompose the contour pattern of the 
object shape into a set of synthesized feature signals (SFS). Then 
the SFSs are used to evaluate the cosine transform based 
synthesized affine invariant function (CSAIF) for the shape 
recognition. The information loss of representing the contour 
pattern with the CSAIF is minimized by simply neglecting the 
translation component. This assures high accuracy in 
recognizing the object shape by discriminating its CSAIF from 
others. Experiments examine the performance and robustness 
of the CSAIF based shape recognition and the results are 
compared with that of using the Fourier affine invariant 
function. 
I. INTRODUCTION 
In the field of computer vision, object recognition is an 
important and fundamental problem which focuses on how to 
identify the objects. Common approaches can be categorized 
into the region- and the contour-based methods. The 
region-based method handles complete information within 
the whole region, but usually it confronts computing 
difficulties due to plenty of data and domain-dependency [1], 
[2]. Instead of considering the overall region, the 
contour-based method simply takes notice of the boundary of 
the object to obtain high computing efficiency. Using the 
contour-based method to recognize the objects, the effect of 
the geometric transformation resulting from variant 
viewpoints of the camera should be removed from the contour 
patterns. When the depth of the object is significantly smaller 
than the distance from the camera to the object, the geometric 
transformation can be approximated by the affine 
transformation. General affine transformations involve 
translation, rotation, scaling, and skewing. To remove the 
effect of affine transformations, the contour pattern is 
transformed into an affine invariant form. Affine invariant 
functions (AIFs) are derived for this purpose. In general, AIFs 
can be divided into local and global approaches. The local 
approach needs specific local shape features such as line 
segments, edges, and corners of the object to be the invariants 
 
Chun-Hsiung Fang is with the Department of Electrical Engineering, 
National Taiwan University, Taipei 106, Taiwan ROC (corresponding author 
to provide phone: +886-2-33663638; fax: +886-2-23638247; e-mail: 
f1921049@ee.ntu.edu.tw ).  
Wei-Song Lin is with the Department of Electrical Engineering, National 
Taiwan University, Taipei 106, Taiwan ROC ( e-mail: 
weisong@cc.ee.ntu.edu.tw ) 
 
[3]−[6]. On the other hand, the global approach considers the 
global shape features to represent the entire contour of the 
shape. Many researchers have paid their attention on the local 
approach. Common techniques are the arc length [7], 
enclosed area [8] and moments [9], Fourier descriptors [8], 
[10], and wavelet decomposition [11], [12]. The synthesized 
affine invariant function (SAIF) proposed by the authors in 
[13] is an innovative global approach based upon the wavelet 
transform. Instead, this paper proposes using the cosine 
transform to decompose the contour pattern into synthesized 
feature signals (SFSs) for evaluating the SAIF. For 
simplicity, the cosine transform based SAIF is abbreviated as 
CSAIF. Except the translation component, every band of the 
contour pattern is involved in evaluating the CSAIF so as to 
minimize information loss. This assures high accuracy in 
recognizing the object shape by discriminating its CSAIF 
from others. Experiments examine the performance and 
robustness of the CSAIF based shape recognition and the 
results are compared with that of the Fourier affine invariant 
function [10].  
The remainder of this paper is organized as follows: 
Section 2 briefs previous works about the affine invariant 
function. Section 3 develops the CSAIF and the 
corresponding method of 2D shape recognition. Section 4 
presents the experimental results. Finally, some conclusions 
are given in Section 5. 
II. PREVIOUS WORKS ABOUT AFFINE INVARIANT FUNCTION 
Represent the contour in the form of a parametric curve 
( ) Nyx ,...,2,1,)(),( =τττ  in the 2D space with arc length 
parameter τ . The enclosed area which is linear under the 
general affine transformation is defined as [8] 
 
∫ −= baa dxyyx τττττσ )()()()(21    (1) 
 
where )(τx  and )(τy  denote the first derivatives of  )(τx  
and )(τy , respectively, and (a, b) represents the start point 
and the end point of a segment of the contour. This 
formulation comes from the idea that under an affine 
mapping, all areas are changed in the same ratio [14]. One 
essential condition for using this descriptor is that the centroid 
of the contour has to be located at the origin of the coordinate 
system. In other words, the enclosed area is not invariant to 
translation. To reduce the noise effect, the absolute area in (1) 
is substituted by the signed parameterization as [15] 
Two-dimensional Shape Recognition with Cosine Transform-Based 
Synthesized Affine Invariant Function 
Chun-Hsiung Fang and Wei-Song Lin, Member, IEEE
10721-4244-0991-8/07/$25.00/©2007 IEEE
 
 
Using the matrix form, (10) is rewritten as 
 
BA +


=


+




=


)(
)(
)(
)(
)(~
)(~
2
1
2221
1211
τ
τ
τ
τ
τ
τ
y
x
b
b
y
x
aa
aa
y
x
  (11) 
 
where A is a nonsingular square matrix representing the 
rotation, scaling, and skewing transformations, and the vector 
B represents the translation. By applying the weighted cosine 
synthesis (9) to (10), we obtain 
 
( ) ( ) ( )
( ) ( ) ( ))()()(~
)()()(~
2221
1211
τττ
τττ
ααα
ααα
ySaxSayS
ySaxSaxS
CCC
CCC
+=
+=
  (12) 
 
where α  denotes the vector of weights.  
Let ( ))(),( ySxS CC αα  and ( ))(),( ySxS CC ββ  be the SFSs of 
the same shape under different affine transformations. Then, 
using (12) and matrix representation, we obtain 
 







=




)()(
)()(
)~()~(
)~()~(
2221
1211
ySyS
xSxS
aa
aa
ySyS
xSxS
CC
CC
CC
CC
βα
βα
βα
βα   (13) 
 
Taking the determinant of (13), a relative CSAIF is 
obtained as 
 
)]([)]([)]([)]([)(, τττττµ βαβαβα xSySySxS CCCCC −=   (14) 
 
The absolute CSAIF is derived by normalization 
 
*)]([*)]([*)]([*)]([
)]([)]([)]([)]([
*)(
)(
)(ˆ
,
,
,
ττττ
ττττ
τµ
τµ
τµ
βαβα
βαβα
βα
βα
βα
xSySySxS
xSySySxS
CCCC
CCCC
C
C
C
−
−
=
=
  (15) 
 
where *)(, τµ βαC  represents the maximum of )(, τµ βαC , and 
*τ  denotes the instant of occurrence. Fig. 1 shows the 
process of generating the CSAIF from an object shape. 
B. Discrimination Function of Shape Recognition  
The shape recognition in the following study is obtained by 
discriminate the similarity measures. The similarity measure 
is defined as the correlation function 
 
22
1
)()(
)()(
),(Similarity
τµτµ
τµτµ
µµ τ
C
T
C
M
N
C
T
C
M
C
T
C
M
∑
=
⋅
=   (16) 
 
where N is the length of the CSAIF, CMµ  and CTµ  denote the 
model and test CSAIFs, respectively. The similarity measures 
of each model CSAIF with respect to each test CSAIF are 
calculated. A test CSAIF hits the model CSAIF obtaining the 
greatest similarity measure.  
 
 
 
Fig. 1.  The process of generating the CSAIF from an object image. 
 
IV. EXPERIMENTAL RESULTS  
Two experiments are conducted to study the performance 
of the CSAIF-based shape recognition. The first experiment 
explores the discrimination power of the CSAIF method by 
recognizing several object shapes under variant affine 
transformations. The second experiment examines the 
robustness of the method by recognizing object shapes with 
pattern signals contaminated by Gaussian white noise. The 
results of these experiments are compared with those 
obtained by using the FAIF method [10]. 
A. Experiment 1: Performance of the CSAIF-Based Shape 
Recognition 
Ten model patterns are extracted from different electronic 
devices as shown in Fig. 2. M1, M2, M3 and M4 are 
integrated circuit devices similar in the rectangular shape but 
different in the pin numbers. Others are LED, buttons, and 
switches. Fig. 3 shows five test patterns obtained by applying 
affine transformations to the corresponding test patterns. 
Express the parameters of affine transformation in (11) as 
 




 −
⋅=
1
1
cossin
sincos
skew
skew
y
x
scale
θθ
θθ
A  and 



=
.
.
trans
trans
y
x
B   (17) 
 
The parameters associated with the scaling, rotation, 
skewing, and translation of the test patterns in Fig. 3 are listed 
in Table 1. Each pattern including the model and the test 
1074
