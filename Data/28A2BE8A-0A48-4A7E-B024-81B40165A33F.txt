 face authentication. An iterative algorithm is also 
proposed to automatically determine a suitable 
number of HMM states and a suitable number of 
observation classes to achieve good 
authentication accuracy. 
 
Keywords - Video compression, video 
surveillance, person authentication, face 
recognition. 
 
I. Introduction 
In the three-year project, we investigate 
efficient schemes for video surveillance and 
retrieval. We have developed in the first two 
years several efficient tools for video 
surveillance and retrieval, such as object tracking, 
compressed-domain fall incident detection, 
shot-based video retrieval. In this year, the goal 
of this project is to develop a 
computation-efficient H.264 codec such that it 
can be used in realtime video surveillance 
applications. Besides, we also develop an 
accurate person authentication scheme using face 
video. 
We first propose a fast inter-mode decision 
scheme based on reliable spatio-temporal 
predictions of neighboring macroblocks and an 
early termination scheme for the algorithm-level 
optimization. We use a commercial profiling tool 
to identify most time consuming modules and 
then apply code-level optimization techniques 
including frame-memory rearrangement and 
Single-Instruction-Multiple-Data (SIMD) 
implementation based on the Intel 
MMX/SSE/SSE2 instruction sets to achieve 
further speed-up. Simulation results show that 
our proposed joint optimization H.264 encoder 
achieves a speed-up factor of up to about 20 
compared to the reference encoder, without 
introducing significant quality degradation. 
We also propose a novel face authentication 
scheme using the Active Appearance Model 
(AAM) and the Hidden Markov Model (HMM). 
The proposed face authentication system can be 
divided into two parts. First, the AAM is used to 
extract the low-dimensional feature vectors 
including combined texture and shape 
information of individual face images. The 
extracted feature vectors are further classified 
into several clusters using vector quantization. 
The clustered feature vectors are then 
characterized using HMMs to make full use of 
the temporal information across the face images. 
After all parameters in the HMMs are calculated, 
we can dynamically determine the thresholds for 
face authentication. An iterative algorithm is also 
proposed to automatically determine a suitable 
number of HMM states and a suitable number of 
observation classes to achieve good 
authentication accuracy. 
We have integrated all the components 
developed in the three years into an intelligent 
realtime video surveillance system, comprising a 
video codec, streaming protocol stack, object 
tracking, fall detection, and person 
authentication. 
The report is organized as follows. In 
Section II, we brief review our results in the first 
two years’ projects. Section III elaborates on the 
results of this year’s project about H.264 codec 
optimization. Our results about face video-based 
person authentication are provided in Section VI. 
Conclusions about this 3-year project are drawn 
in Section V. 
II. Brief Review of the Results in 
FY94 and FY95 
II.1. FY94 (2004/8~2005/7) 
In FY94, we developed a feature-based 
compressed-domain fall-down detection scheme 
for intelligent surveillance applications. The 
proposed scheme involves two steps: 
compressed-domain object extraction and fall 
incident detection. In the object extraction step, 
the MVs and the DC+2AC image of each frame 
are firstly extracted. GME is then performed to 
distinguish moving object MBs from background 
MBs to obtain a rough object segmentation mask. 
The CDM is then used to refine the object mask. 
Should the video shot contain GMs, the GM 
compensation is performed prior to the change 
detection operation. Finally, object clustering is 
performed to separate the object mask into 
multiple individual objects. In the second step, 
three feature values: the change ratio of the 
centroid of a human object, the change ratio of 
the maximum of vertical projection histogram, 
and the duration of an event detected are used to 
identify and locate fall-down events. 
Our proposed object segmentation method 
can extract moving objects with or without 
 divides each MB into 16 4×4 sub-blocks and 
performs ME for the 16 sub-blocks to obtain 16 
seed MVs, respectively. The distribution of the 
16 seed MVs is then used to classify the motion 
field of the MB so as to determine its block-size 
mode accordingly. The method in [16] 
determines search centers and diamond search 
patterns by the deviation of the MVs of smaller 
blocks. The method in [17] makes use of the 
spatial homogeneity and the temporal stationarity 
characteristics of video objects to reduce the 
number of search modes. In this method, the 
spatial homogeneity of a MB is decided based on 
the MB's edge intensity, and the temporal 
stationarity is measured by the difference of the 
current MB and it colocated counterpart in the 
reference frame. The methods proposed in 
[18][19] introduce a new concept of diversity into 
fast MV search and propose fast ME strategies by 
MV merging and splitting and exploring the 
correlation of MVs of overlapping blocks. The 
method presented in [20] also extends the 
concept of diversity to VSBME by initiating 
multiple search-centers, and adaptively exploit 
search strategies with different search step-sizes 
for different block-sizes. The merging procedure 
in [13] and an early-termination scheme is 
adopted to save computation. 
In the above fast MD schemes, three 
strategies: top-down splitting, bottom-up merging, 
and merging-and-splitting are most commonly 
used for reducing the number of search modes for 
MD. All of the strategies need to select one initial 
block-size mode for motion prediction. For the 
bottom-up merging method, the smallest 
block-size mode is chosen among available 
block-size modes as the initial mode for 
performing the motion estimation and mode 
decision. Conversely, for the top-down splitting 
method, the largest block-size mode is chosen as 
the initial block-size mode. For 
merging-and-splitting, the middle block-size is 
chosen. By the assumption that the costs behave 
monotonically, some unlikely modes are 
excluded. In addition, early termination is used to 
exclude more unlikely modes. In general, for 
higher bit rates, smaller block size modes are 
preferred, that is, the bottom-up merging method 
can save more computation. For lower bit rates, 
the top-down splitting method may be better. 
Since multimedia applications are getting 
increasingly popular, most modern 
microprocessors have been embedded with 
specific multimedia instructions to speed up 
image and video processing programs. The 
Single-Instruction-Multiple-Data (SIMD) model 
is available in Intel processors. Utilizing the 
SIMD technology (e.g., the Intel 
MMX/SSE/SSE2 instructions [20]), several 
data-independent instructions can be executed in 
parallel. In video coding applications, a large 
number of small-size native data type operations 
are performed frequently, and the operations on 
different data are independent to others. These 
features make it suitable to exploit the 
parallelism with the SIMD technologies to 
optimize video codecs [22]-[27]. The method in 
[23] speeds up video encoding by adopting 
optimization techniques such as reduced-range 
block-matching, parallel DCT/IDCT with 
MMX/VIS, and code optimization techniques 
such as loop unrolling, data type optimization, 
and redundant operation reduction. In [25], 
several optimization methods are introduced, 
including data alignment for MMX, avoiding 
branch using SIMD conditional select masks, 
omitting 4×4 block-type, temporary pixel 
window for deblocking, etc. [26] proposes some 
SIMD implementation methods for fractional 
pixel interpolation, integer transform, etc. to 
optimize the H.264 decoder for 2-4 times faster 
decoding speed. [27] proposes an optimized 
H.264 encoder utilizing a hyper-threading 
scheme (exploiting parallelism at the MB level) 
which achieves a speed-up factor of 1.2×. 
Before applying optimization to the encoder, 
complexity analyses have to be performed first 
to identify the computationally critical paths. 
Space and time complexity analyses for the 
H.264 codec on a tool-by-tool basis are reported 
in [28]. Theoretical complexity analyses of the 
H.264 baseline decoder are presented in [29][30]. 
It is shown in [26][27] that the most 
time-consuming modules of an H.264 encoder 
are Motion Estimation, Interpolation, SATD, and 
DCT. Therefore, these modules should be put 
into the top priority list to optimize. 
In this work, both algorithm-level and 
code-level optimization techniques are applied to 
accelerate the H.264 software encoder on a 
commercial personal computer that supports 
SIMD instruction sets as shown in Fig. 1. We 
propose a fast inter-mode decision scheme based 
on Reliable Spatio-Temporal Prediction (RSTP) 
  
Fig. 2. Illustration of the five spatio-temporal 
predictions used in the proposed method. 
By comparing the five modes from the five 
spatio-temporal predictions, the best one is 
chosen as the coding mode for the current MB. 
However, such Direct Spatio-Temporal 
Prediction (DSTP) may lead to significant PSNR 
performance degradation (up to 0.6 dB on 
average according to our experiments) due to 
error propagation of incorrect coding modes. To 
mitigate the coding performance degradation 
caused by DSTP, we propose a Reliable 
Spatio-Temporal Prediction (RSTP) scheme 
which further analyzes the reliability of each 
predicted mode before using the predicted mode 
for encoding. If the predicted mode is reliable, 
the predicted mode is used for encoding. 
Otherwise, a full-mode search will be performed 
to search for the best mode. We use the MV 
variance within the current MB and the absolute 
MV difference of the current MB and its 
reference MB defined in (1) and (2), respectively, 
to evaluate the reliability of the neighboring 
prediction information, in order to reduce the 
quality degradation caused by incorrect 
predictions. 
2 2
cur cur cur cur cur
0 1 1
1 1 1_
n n n
i j i j
i j j
MV VAR MVx MVx MVy MVy
n n n= = =
⎧ ⎫⎛ ⎞ ⎛ ⎞⎪ ⎪= − + −⎨ ⎬⎜ ⎟ ⎜ ⎟⎝ ⎠ ⎝ ⎠⎪ ⎪⎩ ⎭
∑ ∑ ∑
      (1) 
where n represents the number of motion vectors 
existing in the current MB; MVx and MVy are the 
horizontal and vertical component MVs, 
respectively. 
cur cur ref cur refADMV MVx MVx MVy MVy= − + −
(2) 
where MVcur and MVref  represent the MVs of 
the current block and its best spatio-temporal 
prediction block, respectively. 
 
Fig. 3. Illustration of motion blocks that have a 
large MV variance and a small MV variance. 
Fig. 3(a) and (b) illustrate two MBs that have 
a large MV variance and a small MV variance, 
respectively. As shown in the figures, a 16×16 
MB is divided into four 8×8 sub-blocks: Block 
#1~Block #4.  In Fig. 3(a), the four 8×8 
sub-blocks have diverse MVs so that the whole 
motion block has a large MV variance.  On the 
contrary, in Fig. 3(b), each of the four 8×8 
sub-blocks has a similar MV so that the MV 
variance within the MB is small. In this case, the 
16×16 MB itself should be used for encoding. 
That is, when the MV variance of smaller blocks 
is small, a larger block encompassing the smaller 
blocks is more suitable for encoding.  However, 
if the block mode is 16×16, there is only a single 
MV after the motion estimation. Consequently, 
no MV variance can be calculated therefrom. 
Accordingly, the proposed method only 
considers the MV variance for determining the 
reliability when the predicted block modes are 
smaller than the 16×16 mode. Only the modes of 
which the MV variances are larger than a 
threshold value THVAR are determined as reliable 
ones. Table 2 shows that the average accuracy of 
using spatio-temporal predictions to replace the 
full-mode search is about 86%, given that the 
predicted modes (smaller than the 16×16 mode) 
are determined reliable using the MV variance 
test. 
As described above, only the reliabilities of 
the reference block-sizes smaller than 16×16 are 
determined by the MV variance in our method. 
For the 16×16 mode, the reliability is determined 
from the absolute MV difference of the current 
16x16 block and its adjacent prediction 16x16 
block as defined in (2). It is known that if two 
adjacent blocks belong to a same object or have 
a same motion trajectory, the chance of the two 
adjacent blocks using a same block-size mode 
for encoding will be very high. Accordingly, the 
MVs of the two adjacent blocks are also similar, 
 five test sequences. Accordingly, if more than 
half of the prediction MBs use a majority 
block-size mode, the method then determines 
whether this majority mode is reliable using the 
reliability check method described in Fig. 5. If 
the majority mode is reliable, the method then 
uses the majority mode to encode the current 
block. If the majority mode is unreliable, 
however, the method then has to perform a 
full-mode search on the current MB to select the 
best block-size mode. 
The detailed flowchart of the proposed RSTP 
algorithm is illustrated in Fig. 6.  First, the 
process obtains prediction block-size modes 
according to the predicted information from the 
neighboring MBs. The sub-8×8 prediction 
modes (i.e., the 8×4, 4×8, and 4×4 modes) are all 
replaced with the 8×8 mode. The method then 
determines whether or not more than half of the 
prediction modes are the same. If so (i.e., along 
the left branch of the flowchart), the process 
performs motion estimation for the majority 
reference block-size mode, and then checks the 
reliability of the majority reference mode 
according to the process described in Fig. 5. 
Should the majority reference mode be reliable, 
the best reference mode is used as a basis for 
determining the final coding mode; otherwise, a 
reduced full-mode search over the 16×16, 16×8, 
8×16, and 8×8 modes is performed to find out 
the best mode. 
Furthermore, when no more than half of the 
prediction MBs adopt the same mode (i.e., along 
the right branch of the flowchart), that is, no 
majority mode is found, the process performs 
motion estimation for all prediction block-size 
modes. All prediction modes are then checked 
for their reliabilities according to the process of 
Fig. 5. After checking the reliabilities of all of 
the prediction modes, the prediction modes that 
are considered reliable are recorded. The process 
then checks whether more than half of the 
reference modes are reliable. If not, the process 
then performs a reduced full-mode search (over 
the 16×16, 16×8, 8×16, and 8×8 modes) on the 
current block to find out the best block-size 
mode. 
After finishing either of the above two branch 
tests, the process performs an early-termination 
check to determine whether or not to stop block 
splitting to further reduce the computation. If the 
best prediction mode is 8×8, the process goes on 
checking the best sub-partition for each 8×8 
sub-block. As each 8×8 sub-block can be further 
divided into 8×4, 4×8, and 4×4 sub-blocks, it 
should be noted that, when the 8×8 sub-block is 
not the best mode, there is no need to analyze the 
sub-blocks smaller than the 8×8 sub-blocks 
because the possibility that a smaller sub-block 
is the best mode is very small. As such, the 
encoding time can be greatly reduced. Finally, if 
an 8×8 block-size mode is not the best, the 
process then adopts a mode that has the 
minimum cost for encoding the current MB. 
 
Table 2 
Average accuracy of using spatio-temporal predictions to replace the full-mode search under three 
reliability conditions 
QP = 28 
MV Variance Absolute MV Difference Majority Mode 
P(T|A) P(F|A) P(T|B) P(F|B) P(T|C) P(F|C) 
Foreman 81% 19% 75% 25% 66% 34% 
Coastguard 84% 16% 70% 30% 62% 38% 
Carphone 85% 15% 78% 22% 70% 30% 
Container 92% 8% 93% 7% 93% 7% 
Akiyo 87% 13% 94% 6% 93% 7% 
Average 86% 14% 82% 18% 77% 23% 
A: MV_VARcur > THVAR 
B: AMVDcur < THAMVD 
C: more than half of the predicted modes are the same (i.e., a majority mode exists)
T: predicted block-size mode is correct 
F: predicted block-size mode is incorrect 
 vertically to the temporary image which 
is created in Step 1. All integer and half 
pixels are then stored into the 
up-sampled image. 
Step 3.  The quarter-pixels are subsequently 
interpolated by applying the bilinear 
filter and stored in the up-sampled 
image. 
In Step 1 of the proposed SIMD 
implementation, eight half-pixels (namely, 
half-pixel labeled with ‘b’ in Fig. 7) will be 
obtained in parallel, except that the 
image-boundary cases are still implemented by 
non-parallel C functions since less parallelism 
could be exploited around the boundary. Eight 
integer-pixels are loaded into the SSE2 registers 
before being packed into 16-bit short words. The 
6-tap FIR filter is applied with only shift and 
add/subtract operations on the integer-pixel 
samples. It is done by loading six rows of eight 
integer-sample pixels into six SSE2 registers. 
After applying the FIR filter to each one utilizing 
the shift and add operations, intermediate values 
are obtained by summing up these data in the 
registers in parallel. The final half-pixel values 
are obtained by performing parallel shift on the 
intermediate values, and are then stored into a 
temporary memory together with the 
integer-pixel samples. 
In Step 2, the eight pixels, including four 
half-pixel samples labeled with ‘h’ and four 
labeled with ‘j’, are interpolated in parallel using 
a procedure similar to Step 1, except that the FIR 
filter is applied vertically. After the parallel 
interpolation is done, all the half-pixel samples 
and integer-pixel samples are stored in the 
rearranged memory array. Note that 
fractional-pixel samples of the same type are 
stored contiguously in the rearranged memory 
arrays, thereby facilitating the implementation of 
the interpolation process with the SIMD 
instructions (contiguous pixels can be loaded 
into the SSE2 registers with one instruction 
rather than multiple memory accesses). 
In Step 3, the parallel-average (PAVG) 
instruction depicted in Fig. 8 is utilized for the 
quarter-pixel interpolation. 
 
 
Fig. 8.  The PAVG instruction. 
III.3.2. The SATD Computation 
The SATD function is used in the 
Intra-prediction and the fractional-pixel motion 
estimation process in the JM reference coder. 
When the fractional-pixel motion estimation is 
performed, the sum of absolute Hadamard 
transform coefficients of a residual block is 
adopted as a block-matching criterion. The 4×4 
SATD values are obtained by performing a 4×4 
Hadamard transform on the 16 sub-block DC 
values of an Intra MB or on the prediction 
residues of each 4×4 sub-block in an Inter MB. 
In the SATD function, only add and subtract 
instructions are involved. We can use the SSE2 
instructions (e.g., parallel-add and 
parallel-subtract) to accelerate the SATD 
function. Computing the SATD values with 
SSE2 is described as follows. 
Step 1:  Load the block of pixels in the source 
and reference frames into the SSE2 
registers in parallel. 
Step 2:  Perform block difference operations in 
parallel. Eight operations can be 
executed simultaneously in a single 
instruction. 
Step 3:  Use unpack or shuffle instructions to 
generate operands which will be applied 
to the same operations together. 
Step 4:  Perform parallel-add/subtract 
instructions to apply the Hadamard 
transform. 
Step 5:  Calculate the absolute values of the 
coefficients without any branches in 
parallel. 
Step 6:  Sum up all the intermediate values in the 
SSE2 registers to obtain the SATD. 
 
III.3.3. The Integer Transform Pair 
 OS. All codes are compiled by the Intel® 
compiler [31]. The run-time complexities are 
profiled using the Intel® VTune [32] 
performance analyzer. Five QCIF (176×144) 
sequences of 150 frames, including Foreman, 
Coastguard, Carphone, Container, and Akiyo, are 
used in our experiments. Table 3 lists the detailed 
setting of coding parameters. The two thresholds 
used in are empirically set as THADMV = 2 and 
THVAR = 5. 
Table 3 
 Setting of encoding parameters 
QP 20, 24, 28, 32, 36 
Search range ±16 
No of reference frames one 
Frame rate 30 Hz 
GOP structure IPPP…IPPP 
GOP size 30 frames 
Entropy coder CAVLC 
R-D Optimization (RDO) off 
Table 4 shows the R-D performance 
comparison between the proposed RSTP mode 
decision and a reduced exhaustive search with a 
selected subset of the seven block-size modes. In 
the reduced exhaustive search, we choose the 
most frequently used modes in the full-mode 
search to form the subset. The number of modes 
is chosen as the closest integer to the average 
number of modes used in the RTSP method. For 
example, in Table 4, the average number of 
block-size modes searched using the RTSP 
scheme for Foreman is 3.61. We thus pick only 
the 4 most frequently used modes for the reduced 
exhaustive-search mode decision. Table 4 and Fig. 
10 show that the proposed method outperforms 
the reduced exhaustive search with a similar 
number of modes in a wide range of bit-rates for 
different test sequences. In practice, the encoder 
does not possibly know which modes are the 
most frequently used ones without doing 
full-mode search for the whole frame. This 
comparison shows that the proposed RSTP 
method obtains reliable block-size modes with a 
significantly reduced number of search modes 
compared to the full-mode search. The RSTP 
method also significantly improves the R-D 
performance of the direct spatio-temporal 
prediction that does not perform mode reliability 
check, while keeping the computational 
complexity comparable. 
Fig. 11 compares the average execution time 
between the reference coder and the proposed 
code-level optimized one for the four modules 
described in Sec. 4. The proposed memory 
rearrangement reduces the execution time of 
sub-pixel interpolation from 5.9 s to 0.98 s, 
leading to a speed-up ratio of about 6. With the 
proposed SIMD implementations, the four 
modules: sub-pixel interpolation, SATD, integer 
transform, and SAD computation, are accelerated 
by about 3×, 4.2×, 1.5×, and 1.5×, respectively. 
 
Table 4 
 R-D performance comparison between the proposed RSTP mode decision and a reduced exhaustive 
search with a similar number of modes 
QP = 28 
PSNR difference Bit-rate difference Average no. of modes
RSTP Most freq. RSTP Most freq. RSTP Most freq.
Foreman -0.09 dB -0.12 dB 1.16 % 1.81 % 3.61 4 
Coastguard -0.04 dB -0.05 dB 1.81 % 2.76 % 3.17 3 
Carphone -0.13 dB -0.18 dB 1.38 % 2.03 % 3.06 3 
Container -0.04 dB -0.09 dB 3.45 % 5.46 % 2.06 2 
Akiyo -0.04 dB -0.07 dB 1.42 % 2.57 % 1.95 2 
 0.98
1.334
0.35
0.143
0.331 0.319
0.228
0.093
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Sub_Pel_Interpolation SATD Integer_Transform Integer_Pel_ME
Ti
me
 (s
)
Original
SIMD
 
Fig. 11. Runtime comparison of four modules with and without using the proposed SIMD 
techniques. 
Table 5 
 Comparison of runtime and speed-up ratios for different-levels of optimization for five test sequences 
(150 frames; QP = 28) 
QP = 28 JM7.3 Ref. [17] RSTP FME [10] Code Opt Joint Opt.
Foreman 47.30 s 39.42 s (1.2×) 
30.74 s 
(1.5×) 
23.65 s 
(2.0×) 
21.39 s 
(2.2×) 
3.01 s 
(15.7×) 
Coastguard 56.30 s 43.30 s (1.3×) 
31.41 s 
(1.8×) 
25.59 s 
(2.2×) 
27.57 s 
(2.0×) 
3.21 s 
(17.5×) 
Carphone 42.01 s 32.57 s (1.29×) 
25.12 s 
(1.7×) 
21.01 s 
(2.0×) 
18.92 s 
(1.7×) 
2.61 s 
(16.1×) 
Container 40.63 s 29.66 s (1.37×) 
19.13 s 
(2.1×) 
21.38 s 
(1.9×) 
17.62 s 
(2.3×) 
2.25 s 
(18.2×) 
Akiyo 34.68 s 30.16 s (1.15×) 
17.12 s 
(2.0×) 
20.40 s 
(1.7×) 
13.27 s 
(2.6×) 
2.17 s 
(16.0×) 
 
Table 5 shows the runtime comparison for 
different levels of optimization. According to the 
simulation results, the encoding speed of the JM 
7.3 reference coder is only about 2.7~4.3 QCIF 
fps. The proposed RSTP fast mode decision 
scheme itself achieves 1.5×~2.1× speedup. The 
fast motion estimation (FME) scheme adopted 
from  [10] accelerates the encoder by 1.7×~2.2×. 
The proposed code-level optimization leads to 
1.7×~2.6× speedup. With the joint optimization 
method, which integrates the RSTP, FME, and 
code-level optimization, the overall speedup 
achieved is about 15.7×~18.2×. 
The fast mode-decision and fast motion 
estimation reduces the coding efficiency slightly. 
Table 6 and Fig. 12 compare the RD 
performances of the reference coder, the RSTP 
mode decision scheme, and the joint 
optimization scheme. It can be observed that, for 
sequences without intensive motions (e.g., Akiyo, 
Container, and Coastguard), the optimized 
encoder yields very close coding performance as 
compared to the non-optimized JM7.3 coder. For 
sequences with relatively large motions (e.g., 
Foreman), the overall PSNR degradation caused 
by the jointly optimized encoder is about up to 
0.3 dB degradation, in which about 0.2 dB is 
contributed by the RSTP mode decision, and 0.1 
dB by the FME. The simulation results show that 
our proposed joint optimization H.264 encoder 
achieves a significant speedup without 
introducing serious quality degradation. 
 CARPHONE
34
35
36
37
38
39
40
41
42
50 100 150 200 250
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(c) 
CONTAINER
33
34
35
36
37
38
39
40
41
25 45 65 85 105 125 145
Bitrate
PS
N
R
JM7.3
RSTP+FME+Code-level Opt.
RSTP
Ref. [17]
 
(d) 
 receiving broad attentions in biometrics-based 
identification and authentication research fields 
[33][34]. 
The face recognition methods can be roughly 
divided into three categories: the feature-based 
methods [33], the template-based methods [34], 
and the model-based methods [35]. The 
feature-based methods are the earliest and 
intuitive methods. In the training phase, several 
salient facial features are detected by using a 
face detector, and the relation of distances 
between the feature points is taken be the feature 
vector and stored in the database. In the 
verification phase, the minimum distance 
approach is utilized to measure the similarity 
between the feature vectors of the test image and 
the trained image. The template-based methods 
use the entire face template to recognize faces 
and usually outperform the feature-based 
approaches. The model-based methods such as 
Active Shape Model (ASM) based and 
AAM-based are also popular in recent years. Not 
only a compact representation is provided for 
faces by decoupling shapes and textures, 
effective tracking mechanisms are also presented 
for capturing the dynamics of deformable 
objects.  
Traditionally, most of previous face 
identification methods only focus on the single 
face image. But it is vulnerable to attacks with 
fake or photoed face pictures. Thus, one way to 
resolving this problem is to use the face video 
instead of a single image. Video-based face 
authentication cannot only be taken as a live 
verification to prevent fraud, but potentially it 
can improve the discriminability by making use 
of temporal information across the video 
sequence. Several video-based authentication 
approaches have been proposed recently. The 
method in [36] proposes to use the SVM 
classifier for video-based face recognition, but it 
is just a simple voting mechanism similar to 
image-based recognition with multiple images. 
In [37] a method uses the trained identity 
surfaces with the multi-view dynamic faces for 
recognition is proposed. The method presented 
in [38] instead uses a trained probabilistic 
appearance manifold with a set of rotation faces. 
Although both these methods use the temporal 
information across a video sequence, the 
information of face rotation is not a good 
password for face authentication. This password 
is not secure since the action of face rotation can 
be easily seen by others and the degree of 
freedom is small. Tang and Li proposed to 
segment video according to speech alignment 
[39]. Face features are extracted separately from 
each video segment for face authentication. This 
approach uses the temporal information across a 
video sequence, but dynamics is still not fully 
explored. Liu and Chen used adaptive HMMs to 
learn the dynamics of face rotation sequences 
[40]. This paper shows that the HMM can be 
successfully applied to model temporal 
information, but the behavior of long-term 
unintentional face rotation may not be suitable 
for most face authentication applications. Based 
on aforementioned observations, we propose a 
new video-based face authentication by using 
appearance models for feature representation and 
the HMM for exploring the temporal feature 
models. 
IV.2. Review of AAM and HMM 
The concept of a Statistical Appearance 
Model (SAM) was proposed in [41]. It can 
model both the shape and texture of an image of 
an object. The models are generated by 
combining a model of shape variation with a 
model of the texture variation in a normalized 
frame, and it is usually used to synthesize a 
complete image of an object or structure. The 
shape and the shape-free texture of a face can be 
extracted using the statistical appearance models, 
and the shape-free texture means that the texture 
of a face is not dependent on the shape of a face. 
Fig. 13 shows that each training example can be 
split into a set of landmark points and a 
shape-free texture patch. Alternatively, some 
methods like Eigenfaces or Fisherfaces are only 
used to extract the lower dimensional texture to 
interpret a face, but discarding the shape 
information which is also very useful to interpret 
a face.  
AAM [42][43] is an extension to the ASM. 
Instead of manually selecting feature points in 
SAM, it is used to extract the representative 
features of the object in an image automatically. 
The appearance model is built based on a set of 
labeled images, where the landmark points are 
marked on each face. After labeling all the 
sample images, Procrustes analysis is utilized to 
align each face according to a mean shape. 
Principal Component Analysis (PCA) is then 
 suitable class number of observations.  
IV.3.2. Face Authentication Using HMM 
As shown in Fig. 3, the sequences in the 
database are divided into two parts: one part for 
training and the other part for testing. In the 
training stage, we first extract the features of 
each training face image and use these features 
to construct an appearance model. Then we can 
easily obtain the appearance parameter c in (3) 
by using this model. After all the appearance 
parameters are computed, we use the vector 
quantization approach to separate these feature 
vectors into N classes. Thus, we can obtain all 
the observations O, and each training face image 
sequence can be represented as a symbol 
sequence. Finally, we use these symbol 
sequences for training the HMM. 
When we train the HMM, a transition 
probability matrix A, an initial state probability 
distribution B, and a set of probability density 
functions π are initialized. Here we set the initial 
value of π = [1,0,0,…,0], because the 
left-to-right HMM is suitable for our application. 
We then use the training symbol sequences and 
the Expectation Maximization (EM) algorithm to 
calculate the final parameter vector (A, B, π) of 
each trained HMM. After the face authentication 
system is built, we need to determine the suitable 
parameters of HMM. Finally, we use the test 
sequences with the suitable parameters in HMM 
to test the system and check if the system is 
stable for face authentication. 
 
Fig. 16. Procedure of testing the trained HMMs. 
 
Fig. 17. Determining the 25 thresholds for 
HMM. 
IV.3.3. Determining the HMM Model 
Parameters 
While testing the face authentication system, 
we need to determine a suitable threshold TH, 
the hidden state number S in HMM, and the class 
number of observations C. The threshold TH for 
face authentication is firstly considered. Taking 
Fig. 16 as an example in which four persons are 
involved in the database, “A_A” is a sequence 
which represents person A says A’s password 
(A’s name in this example); “HMM A_A” 
represents a pre-trained HMM obtained from 
training by a number of “A_A” sequences; p1 is 
the probability that is computed from using the 
“A_A” sequences to test the trained “HMM A_A” 
model.  In order to find a suitable threshold for 
“HMM A_A,” the probabilities, p1, p2, p3, and p4 
are calculated first. The “HMM A_A” model has 
to accept “A_A” and reject “A_B,” “A_C,” and 
“A_D.” Thus, the threshold for “HMM A_A” 
must be higher than the value log(p1) and lower 
than the values, log(p2), log(p3) and log(p4). We 
can roughly separate these log-probabilities into 
two classes. The log-probability (i.e., p1) in the 
first class must be accepted by “HMM A_A” and 
the other class of log-probability values (e.g., p2, 
p3 and p4) must be rejected for “HMM A_A.” We 
can then compute the mean values, μ1 and μ2, of 
the corresponding classes, respectively. The 
difference between μ1 and μ2 is also calculated. 
We can then determine 25 equally spaced 
thresholds as shown in Fig. 17. Finally, the 
following iterative algorithm with these 25 
thresholds is performed to find a suitable state 
number of HMM and a suitable class number of 
observations.  
Step 1. Set an initial state number. 
Step 2. Given the fixed state number, change 
the class number and the threshold in 
order, and then use the training 
sequences to test each HMM. We can 
find a best class number according to 
the ROC curve. 
 extract useful features for face authentication. 
We proposed a scheme to adaptively determine 
the thresholds used in the system. We have also 
proposed an iterative algorithm to determine a 
suitable hidden state number in HMM and a 
suitable class number of observations using test 
sequences. The experimental results show that 
the proposed method achieves very low FAR and 
FRR 
 
V. 計畫成果自評 
In this three-year project, we have 
developed efficient tools for video surveillance 
and retrieval. In the first year (FY94), we 
developed a compressed-domain object 
segmentation and fall incident detection scheme 
for homecare applications. In the second year 
(FY95), we developed a shot-based 
coarse-to-fine video retrieval to fast search 
similar video clips in video database. In the final 
year, we have developed a highly optimized 
coder by integrating the proposed fast mode 
decision and code-level optimization schemes 
with a hexagon-search fast motion estimation 
scheme. Our encoder achieves a speedup factor 
of up to about 20× compared to the reference 
encoder without introducing significant quality 
degradation. In addition, we proposed a novel 
video-based face authentication scheme using 
AAM and HMMs that can achieve achieves very 
low FAR and FRR. 
We have integrated all the components 
developed in the three years into an intelligent 
realtime video surveillance system, comprising a 
video codec, streaming protocol stack, object 
tracking, fall detection, and person 
authentication. 
Our results have resulted in several 
publications, including eight international 
conference papers [47]-[54] and four 
international journal papers (three papers 
published or accepted and one revised) [52]-[58]. 
The results meet the goals of this project quite 
well.  
 
REFERENCES 
[1] Joint Video Team of ITU-T and ISO/IEC JTC 1, 
Draft ITU-T Recommendation and Final Draft 
International Standard of Joint Video Specification 
(ITU-T Rec. H.264 | ISO/IEC 14496-10 AVC), Doc. 
JVT-G050, Mar. 2003. 
[2] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. 
Luthra, “Overview of the H.264/AVC video coding 
standard,” IEEE Trans. Circuits Syst. Video Technol., 
vol.13, no. 7, pp. 560-576, July 2003. 
[3] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and 
G. J. Sullivan, “Rate-constrained coder control and 
comparison of video coding standards,” IEEE Trans. 
Circuits Syst. Video Technol., vol. 13, pp. 688–703, 
July 2003. 
[4] M. Ravasi, M. Mattavelli, and C. Clerc, “A 
computational complexity comparison of MPEG4 
and JVT codecs,” Joint Video Team (JVT) of 
ISO/IEC MPEG and ITU-T VCEG, Doc. 
JVT-D153r1-L, July 2002. 
[5] X. Zhou, E. Q. Li and Y. K. Chen, “Implementing 
H.26L decoder on general-purpose processors with 
media instructions”, SPIE Conf. on Image and Video 
Communications and Processing, San Diego, USA, 
Jan. 2003. 
[6] T. Koga, K. Iinurna, A. Hirano, Y. Iijima, and T. 
Ishiguro, “Motion-compensated interframe coding 
for video conferencing,” Proc. Nat. Telecommun. 
Conf., New Orleans, LA, pp. C9.6.1-9.6.5, Dec. 
1981. 
[7] S. Zhu and K.K. Ma, “A new diamond search 
algorithm for fast block-matching motion 
estimation,” IEEE Trans. Image Processing, vol. 9, 
pp. 287-290, Feb. 2000. 
[8] C. Zhu, X. Lin, and L.-P. Chau, “Hexagon-based 
search pattern for fast block motion estimation,” 
IEEE Trans. Circuits Syst. Video Technol., vol. 12, 
no. 5, pp. 349–355, May 2002. 
[9] J.-N. Zhang, Y.-W. He, S.-Q. Yang, Y.-Z. Zhong, 
"Performance and complexity joint optimization for 
H.264 video coding," IEEE Int. Symp. Circuits Syst. 
(May) (2003) II.888-II.891. 
[10] Z. Chen, P. Zhou, and Y. He, “Fast integer pel and 
fractional pel motion estimation in for JVT”, 
JVT-F017r1.doc, Joint Video Team (JVT) of 
ISO/IEC MPEG & ITU-T VCEG, 6th meeting, Dec. 
2002. 
[11] M. Bierling, “Displacement estimation by 
hierarchical block matching,” Proc. SPIE Conf., 
Visual Commun. And Image Processing’88, vol. 
1001, part 2, pp. 942-951, 1988. 
[12] B. Liu and A. Zaccarin, “New fast algorithms for the 
estimation of block motion vectors,” IEEE Trans. 
Circuits Syst. Video Technol., vol. 3, no. 2, pp. 
148-157, Apr. 1993. 
[13] Y.-K. Tu, J.-F. Yang, Y.-N. Shen, and M.-T. Sun, 
“Fast variable-size block motion estimation using 
merging procedure with an adaptive threshold,” in 
Proc. IEEE Conf. Multimedia & Expo, vol. 2, pp. 
789-792, July 2003. 
[14] X. Li and G. Wu, "Fast integer pixel motion 
estimation," JVT-F011, 6th Meeting, Awaji Island, 
Japan, Dec. 2002. 
[15] T.-Y. Kuo and C.-H. Chan, “Fast macroblock 
partition prediction for H.264/AVC,” in Proc. IEEE 
Conf. Multimedia & Expo, June 2004, Taipei, 
Taiwan. 
[16] A. C. Yu, “Efficient block-size selection algorithm 
for inter-frame coding in H.264/MPEG-4 AVC,” in 
 [48] J.-F. Chen, H.-Y. M. Liao, and C.-W. Lin, “Fast 
video retrieval via the statistics of motion,” in Proc. 
IEEE Int. Conf. Acoustics, Speech & Signal 
Processing, Mar. 2005, Philadelphia, PA, USA. 
[49] C.-W. Lin, Z.-H. Ling, Y.-C. Chang, and C. J. Kuo, 
“Compressed-domain fall incident detection for 
intelligent home surveillance,” in Proc. IEEE Int. 
Symp. Circuits and Systems, May 2005, Kobe, 
Japan. 
[50] Y.-H. Ho, C.-W. Lin, J.-F. Chen, and H.-Y. M. Liao 
“Fast coarse-to-fine video retrieval via shot-level 
statistics,” in Proc. SPIE Conf. Visual 
Communication and Image Processing, July 2005, 
Beijing, China. (won the Young Investigator Award) 
[51] Y.-L. Lai, Y.-Y. Tseng, C.-W. Lin, Z. Zhou, and M.-T. 
Sun, “H.264 encoder speed-up via joint 
algorithm/code-level optimization,” in Proc. SPIE 
Conf. Visual Communication and Image Processing, 
July 2005, Beijing, China. 
[52] C.-W. Su, H.-Y. M. Liao, K.-C. Fan, C.-W. Lin and 
H.-R. Tyan, “A motion-flow-based fast video 
retrieval system,” in Proc. 7th ACM SIGMM 
International Workshop on Multimedia Information 
Retrieval, Nov. 2005, Singapore. 
[53] K.-Z. Chen, Y.-J. Chang, and C.-W. Lin, 
“Video-based authentication using appearance 
models and HMMs,” in Proc. IEEE Int. Symp. 
Circuits and Systems, May 2006, Island of Kos, 
Greece. 
[54] C.-W. Lin and Z.-H. Ling, "Automatic fall incident 
detection in compressed video for intelligent 
homecare," in Proc. Int. Workshop Multimedia 
Analysis & Processing, August 2007, Hawaii, USA. 
[55] Y.-H. Ho, C.-W. Lin, J.-F. Chen, and H.-Y. M. Liao 
“Fast coarse-to-fine video retrieval using shot-level 
statistics,” IEEE Trans. Circuits and Systems for 
Video Technology, vol. 16, no. 5, pp. 642-648, May 
2006. 
[56] C.-W. Lin, Z.-H. Ling, Y.-C. Chang, and C. J. Kuo, 
“Compressed-domain fall incident detection for 
intelligent homecare,” accepted and to appear in 
Journal of VLSI Signal Processing Systems for 
Signal, Image, and Video Technology  (Special 
Issue on Audio-Visual Signal Processing for 
Intelligent Security Systems) 
[57] C.-W. Su, H.-Y. M. Liao, H.-R. Tyan, C.-W. Lin, 
D.-Y. Chen, and K.-C. Fan, “Motion flow-based 
video retrieval,” accepted and to appear in IEEE 
Trans. Multimedia. 
[58] Y.-L. Lai, Y.-Y. Tseng, C.-W. Lin, Z. Zhou, and M.-T. 
Sun, “H.264/AVC encoder speed-up using joint 
algorithm/code-level optimization,” Journal of 
Visual Communication and Image Representation. 
(revised, June 2007) 
