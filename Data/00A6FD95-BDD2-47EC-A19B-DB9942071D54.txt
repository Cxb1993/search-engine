 1
行政院國家科學委員會補助專題研究計畫成果報告 
小腦模型的基底函數與記憶體結構在學習 
及影像資料傳輸過程影響之研究 
The study of effects on basis functions and memory structures of 
CMAC in learning and image data transmission processes 
計畫編號：NSC 95-2221-E-149 -003- 
執行期間： 95 年 08 月 01 日至 96 年 07 月 31 日 
計畫主持人：陶逖 
共同主持人：丁轉初 
計畫參與人員：陳良宇、林弘倫 
Email: tedtao@ms15.hinet.net 
 
一. 計畫中英文摘要 
本研究計畫將探討小腦模型的基底函
數與記憶體結構對學習及資料傳輸過程之
影響。首先介紹小腦模型，它可被視為模仿
人類小腦的一種學習機制，相當於一種類神
經網路，它是經由輸入向量可立即決定輸出
的一種映射，而這種映射關係可利用一對應
表來實現，因此小腦模型具有快速學習與快
速收斂的特性。但由於傳統的小腦學習策略
是將修正誤差平均分布於所有對應到的記
憶細胞（或稱為超立方體），卻不考慮學習
的信度，因此將影響線上學習的效果。本計
劃利用記憶細胞學習次數的倒數作為學習
信度值以構成小腦模型之基底函數。藉著所
提出「以學習信度為基底函數的小腦模
型」，可克服線上調整小腦模型權重的問
題。而當傳輸資料時，單一小腦結構無法利
用少量記憶細胞傳輸高解析度資料，這是一
個兩難的問題。因此本計畫將提出金字塔結
構的小腦模型來解決資料傳輸的問題，並利
用提出的結構來處理影像資料傳輸問題。本
方法乃是利用金字塔上層結構傳輸粗略影
像給接收的一方，再依據接收方的要求將影
像誤差信號藉由金字塔下層結構傳輸至接
收方，藉此逐漸改善影像的品質。故影像信
號經由金字塔底層結構傳輸，在接收方可得
到高品質的的重建影像。 
關鍵詞：信度基底函數、類神經網路、金字
塔結構的小腦模型 
Abtract 
The study of effects on basis functions 
and memory structures of CMAC (cerebellar 
model articulation controller) in learning and 
data transmission processes are proposed in 
this project. The CMAC can be thought as a 
learning mechanism that imitates the 
cerebellum of a human being. It is often 
referred to as an associative neural network, 
where the input vector maps into output 
instantaneously. Besides, it can be regarded as 
a type of look-up table, so it has several 
advantages such as fast learning speed and 
convergence. But the conventional CMAC is 
still inadequate for on-line learning systems. 
This is due to the fact that the conventional 
CMAC learning scheme equally distributes 
the error correction into all addressed memory 
cells (or so-called hypercubues) regardless of 
the credit of those memory cells in the 
learning process. In this project a credit basis 
function that uses the inverse of learned times 
 3
With the proposed scheme, it can on-line learn 
the control variable by adjusting the weights 
in the CMAC during the control process. 
Simultaneously the CMAC is used to predict 
the actual control variable for following 
required dynamic trajectories. In this project, 
the CMAC is also applied to fuzzy logic 
controllers and image compression.  
According to the applications and design 
of neural networks in the image compressive 
domain, there are three different categories. 
These include direct development of neural 
learning algorithms for image compression 
[21,22], neural network implementation of 
traditional image compression algorithms 
[23,24], and indirect applications of neural 
networks to assist with those existing image 
compression techniques. Indirect neural 
network applications are developed to assist 
with traditional techniques and provide 
excellent potential for further improvement on 
conventional image coding and compression 
algorithms [25]. This is typified by significant 
research work on image pattern recognition, 
feature extraction, and classification by neural 
networks. When traditional compression 
technology is applied to those pre-processed 
patterns and features, less memory is needed 
due to the fact that the neural networks have 
already been well established, such as 
“Combined Kohonen neural networks and 
discrete cosine transform method for iterated 
transformation theory” [25]. In this project, 
the one-short learning in CMAC belongs to 
direct development of neural learning 
algorithms for image compression and the 
pyramidal CMAC structure applied to 
downsample [26,27] data size of discrete 
cosine transform (DCT) is the indirect 
application of neural networks to assist with 
those existing image compression techniques.   
三. 研究方法 
I. ONE-SHOT TRAINING ALGORITHM IN 
TWO-DIMENSION CMAC NETWORKS 
One-shot training algorithm in 
two-dimension CMAC networks is discussed 
in this section. Firstly, we will briefly 
introduce the structure and learning algorithm 
of a two-dimension CMAC for learning image 
data. In general, let v=(x,y) be the input vector 
and 
( )v jw  be the stored weight in the j-th layer 
memory cells (or so called hypercubes) which 
are mapped by input vector in the K-layer 
CMAC network. Then the output value ( , )g x y  
can be computed in the following equation: 
( )
1
( , ) .
K
v j
j
g x y w
=
=∑              (1) 
Since each pixel addresses exactly K 
hypercubes, only those weights are used in (1) 
for each input in the CMAC network. Because 
the output value ( , )g x y  is generated from the 
CMAC hypercubes, the data of hypercubes 
need to be updated by the differential code 
e(x,y) that is the difference between the 
desired value d(x,y) and output value ( , )g x y  
in the learning process. The updating 
algorithm for ( )v jw  is defined by the following 
equations: 
( ) ( ) ( , ); 1,2, ,
new old
v j v i
uw w e x y j K
K
= + = L         (2) 
),(),(),( yxgyxdyxe −= ,            (3) 
Where u is a learning rate in the learning 
process, and according to references [12], if 
20 << u  is true, then the above learning 
algorithm will converge. 
 5
CMAC network. When we choose learned 
samples such that every weight is updated by 
only one time in each learning cycle, this 
method is called one-shot training. The 
weights in a CMAC network will not be 
corrupted which is the advantage of one-shot 
training. However, it reveals that CMAC 
networks will stop learning in one-shot 
training method. In order to prevent the 
phenomenon of stop learning, we propose 
pyramidal CMAC networks where the 
information can still be updated in the lower 
level’s CMAC without corrupting previous 
learned information, which can improve the 
above disadvantage. This will be discussed in 
detail in the next section.   
II. DIFFERENTIAL CODING IN PYRAMIDAL 
CMAC NETWORKS 
We propose a method of differential coding in 
pyramidal CMAC networks illustrated in the 
block diagram as shown in Fig. 1. The 
pyramidal architecture contains m’s levels, and 
each level is composed of four processes: 
downsampling, compression (CR), 
decomposition (DC), and upsampling. Note 
that the upsampling process is done by an 
independent CMAC network for different 
levels, which ensures that later information 
can still be updated but not corrupt the learned 
information updated in higher levels of 
CMAC networks. The original image dada are 
used as initial differential codes 0e  in the 
first level, then the differential codes are 
selected every 2m  units in each dimension 
(downsampled data are 1/ 4m  of whole image 
data), and then the sampled differential codes 
through a compression process become 
compressed data 0e , which will be 
transmitted to the receiver. When the receiver 
accepts the compressed data 0e , which should  
CR4m ↓
0eˆ
DC
CMAC(1) 4m≅ ↑
0e
+
-
0e
CR14m− ↓
1ˆe
DC
1CMAC(2) 4m−≅ ↑
1e
1e
+
-
+
+
1 1g h=
2g
2h
1h
1CMAC( ) 4m ≅ ↑ 1mg −
1mh −
+
++
-
1ˆm me g− =
DC
mh
2e
1me −
CR
+
+
1me −
2g
1mg −
1g
DecoderEncoder
0e
1e
1me −
level 1
level 2
level m
Fig. 1 The block diagram of closed-loop  
pyramidal CMAC networks. 
 
be decomposed, and the decomposed data 0eˆ  
are upsampled by the first level CMAC 
network (CMAC(1)) then they become the 
reconstructed data 1g . The original data 0e  
minus reconstructed data 1g  are 
simultaneously sent to next level as the second 
level’s differential codes 1e . These four 
closed-loop processes repeat m’s times in the 
m’s level pyramidal CMAC networks, but 
downsampling and upsampling ratio are 
different in different levels. The 
downsampling ratio equals 11/ 4m i− + , and the 
upsampling ratio equals 14m i− +  in the i-th level, 
but it needs not downsample and upsample in 
the last level because all differential codes 
should be transmitted. 
Let CMAC(i) be a CMAC network in the in 
i-th level of pyramidal CMAC networks, so 
hypercubes are also divided into m levels, and 
 7
),(),( 11 yxgyxh = ,        (11) 
1( , ) ( , ) ( , ), for 2,3, , .i i ih x y g x y h x y i m−= + = L   (12) 
From (11) and (12), it needs only one addition 
per pixel to improve the PSNR of 
reconstructed image in each level. Now the 
error after m’s level reconstruction process is 
discussed, and it can be computed by using 
the following equation: 
.),(),(),(),(),(
1
00 ∑
=
=−=−
m
i
mim yxeyxgyxeyxhyxe (13) 
The decomposed codes ),(ˆ 1 iim yxe −  will 
approach the downsampled part of differential 
codes ),(1 yxem− , when a good transform such 
as DCT, Wavelet or JPEG is used. And the 
decomposed differential codes ),(ˆ 1 iim yxe − will 
converge if the learning rate u satisfies 
condition 20 << u  in (10), which has been 
proved in reference, so the error ),(1 yxem−  will 
converge too. Thus, it can be concluded that 
the lower the level of pyramidal CMAC 
networks is, then the smaller errors will be. If 
the value of differential codes ),(1 yxem−  is 
smaller, then the compressed data volume will 
be lesser too. Thus, we can use smaller 
differential codes to get higher compression 
rate in the bottom of pyramidal CMAC 
networks.  
四. 模擬與分析 
We take a 512x512 size standard “Lena” as 
an original image whose data volume equal 
258KB and gray levels equal 256. In order to 
measure the quality of the reconstructed image, 
the peak signal to noise ratio (PSNR) is 
defined in (14), where the gray level of image 
ranges from 0 to 255.  
255PSNR=10 log( ) dB
MSE
×       (14) 
The JPEG method is applied to the proposed 
pyramidal CMAC network as compression 
(CR) and decomposition (DC) processes in the 
block diagram of pyramidal CMAC networks. 
The proposed method not only can reduce the 
bit rate during transmission, but also can 
improve PSNR in the lower level of the 
pyramid. We also compare the proposed 
method with Wavelet transform, and 
experimental results prove that the proposed 
method is better. The level of pyramid CMAC 
networks is set to be 3 (m=0~2) and learning 
rate u is set to be 1 in the following 
experiments.  
In the first level the original image data are 
used as initial differential codes ( 0e ), and the 
downsampled differential codes whose size is 
1/16 of original image through JPEG 
compression become compressed image ( 0e ) 
shown in Fig. 2(a) whose data volume is 5KB 
and bit rate H is 0.115(bits/pixel). Then the 
compressed data through JPEG decomposition 
and reconstruction processes CMAC(1) 
become a coarse image ( 1 1h g= ) with 
PSNR=26.93dB as shown in Fig. 2(b). 
In the second level the original image data 
minus the first level’s reconstructed data 
( 0 1e g− ) are used as second level’s differential 
codes ( 1e ), and the downsampled differential 
codes whose size is 1/4 of original image 
through JPEG compression become 
compressed image ( 1e ) as shown in Fig. 3(a) 
whose data volume is 11KB and bit rate H is 
0.3411(bits/pixel). Then the compressed data 
through JPEG decomposition and 
reconstruction processes CMAC(2) become 
the reconstructed differential codes ( 2g ) which 
 9
 
 
 
 
 
 
 
 
Fig. 2(a) The compressed differential codes 0e  
transmit in the first level, whose 
size=128x128, data volume=5KB, 
H=0.155(bits/pixel) and gray level range=256. 
 
 
 
 
 
 
Fig. 3 (a). The compressed differential codes 
1e  transmit in the 2
nd level, whose size= 
256x256, data volume=11KB, 
H=0.3411(bits/pixel) and gray level 
range=256. 
 
 
Fig. 4(a) The compressed differential codes 2e  
transmit in the 3rd level, whose size=512x512, 
data volume=21KB, H=0.6512 (bits/pixel) 
and gray level range=128. 
 
Fig. 2(b) The reconstructed image 1h  gets 
from the first level of pyramidal CMAC 
networks, and its PSNR equals 26.93dB. 
 
 
Fig. 3(b) The reconstructed image 2h  gets 
from the 2nd level of pyramidal CMAC 
networks, and its PSNR equals 31.38dB. 
 
Fig. 4(b) The reconstructed image 3h  gets 
from the 3rd level of pyramidal CMAC 
networks, and its PSNR equals 35.60dB. 
 
 11
cerebellar model articulation controller,” 
The Journal of Grey System, vol.9, no. 3, 
pp. 199-218, 1997. 
[17] R. Smalz and M. Conrad, “Combining 
evolution with credit apportionment: A 
new learning algorithm for neural nets,” 
Neural Networks, vol. 7, no. 2, pp. 
341-351, 1994. 
[18] V. L. Plantamura, B. Soucek, and G. 
Visaggio, “Holographic fuzzy learning 
for credit scoring,” Proceedings of the 
International Joint Conference on Neural 
Networks, vol. 1, 1993, pp. 729-732.  
[19] S. F. Su, T. Tao, and T. H. Hung, “Credit 
assigned CMAC and its application to 
online learning robust controllers,” IEEE 
Trans. on Systems, Man and Cybernetics, 
Part B: Cybernetics, vol. 33, no.2, pp. 
202-213, 2003. 
[20] T. Tao, H. C. Lu, and S. F. Su, “Robust 
CMAC control schemes for dynamic 
trajectory following,” Journal of the 
Chinese Institute of Engineers, vol. 25, 
no. 3, pp. 253-264, 2002. 
[21] M. Mougeot, R. Azencott, and B. 
Angeniol, “Image compression with 
backpropagation: improvement of the 
visual restoration using different cost 
function,” Neural Networks, vol. 4, no. 4, 
pp. 467-476, 1991.  
[22] A. Namphol, S. Chin, and M. Arozullah, 
“Image compression with a hierarchical 
neural network,” IEEE Trans. Aerospace 
Electronic Systems, vol. 32, no.1, pp. 
326-337, 1996. 
[23] W. L. Merrill John and R. F. Port, 
“Fractally configured neural networks,” 
Neural Networks, vol. 4, no. 1, pp. 53-60, 
1991. 
[24] J. Stark, “Iterated function systems as 
neural networks,” Neural Networks, vol. 
4, no.5, pp. 679-690, 1992. 
[25] J. M. Mas Ribes, B. Simon, and B. Macq, 
“Combined Kohonen neural networks 
and discrete cosine transform method for 
iterated transformation theory,” Signal 
Processing: Image Communication, 
vol.16, no. 7, pp. 643 –656, 2001. 
[26] N. Merhav and V. Bhaskaran, “Fast 
algorithms for DCT domain image 
down-sampling and for inverse motion 
compensation,” IEEE Trans. on Circuits 
System Video Technology, vol. 7, no. 3, 
pp. 468-476, 1997. 
[27] T. Tao, H. C. Lu, and T. H. Hung, “The 
CA-CMAC for downsampling image 
data size in the compressive domain,” 
Proceedings of the IEEE International 
Conference on Systems, Man and 
Cybernetics, Hammamet, Tunisia, vol. 5, 
Oct., 2002, pp. 555-560. 
[28] T. Tao, and C. C. Ding, “Differential 
codes transmitting in pyramidal CMAC 
networks,” Proceedings of the IEEE 
International Conference on Systems, 
Man and Cybernetics, Taipei, Taiwan, pp. 
3330-3335, Oct., 2006.  
[29] T. Tao and C.Y. Huang, “Adaptive fuzzy 
sliding mode control schemes for 
tracking time-various trajectories,” 
Proceedings of the IEEE International 
Conference on Fuzzy Systems, London, 
UK, pp. 34-38, July, 2007. 
表 Y04 
There are also three different categories according to the applications and design of 
neural networks in the image compressive domain. These include direct development of 
neural learning algorithms for image compression, neural network implementation of 
traditional image compression algorithms, and indirect applications of neural networks to 
assist with those existing image compression techniques. Indirect neural network 
applications are developed to assist with traditional techniques and provide very good 
potential for further improvement on conventional image coding and compression 
algorithms. This is typified by significant research work on image pattern recognition, 
feature extraction and classification in neural networks. When traditional compression 
technology is applied to those pre-processed patterns and features, it can be expected to 
achieve improvement by using neural networks since their applications in this area are 
well established. For example, “Combined Kohonen neural networks and discrete cosine 
transform method for iterated transformation theory” in reference belongs to this type. The 
proposed method “Differential codes transmitting in pyramidal CMAC networks” is also 
the indirect application of neural networks to assist with those existing image compression 
techniques. We propose a novel coding procedure, which can make pyramidal CMAC 
networks learn the feature of transmitted image with only one-shot training. The 
pyramidal architecture is also employed when a coarse image needs to be sent to the 
receiver in initial time, and then the image quality is gradually improved at the request of 
the receiver. In other words, the receiver recognizes the image content at an early stage, 
and then decides whether further transmission is necessary. Consequently, transmission 
time can be saved. Finally, pyramid CMAC networks are applied to JPEG compression. 
Experimental results demonstrate the proposed method can get higher peak signal to noise 
ratio (PSNR) at lower bit rate after reconstruction. 
 
三、考察參觀活動(無是項活動者省略) 
無 
 
四、建議 
增加出席國際會議費用的額度。出席國際會議費用的額度太低，常導致獲補助
者一年僅能參加一次國際會議費或無法到較遠的地方參加會議，喪失為國爭光以及
提升個人研究水平的機會。由於我國地理位處東亞邊緣，除日本之外，距離歐美先
進國家都甚遠。假若國內學者無法獲得較高額度的經費補助，常常只好屈就國內或
附近國家所舉辦的國際會議。長此以往，對於國內研究水平之增長是不利的。因此，
建議政府寬列經費，提高出席國際會議費用的補助額度，以利獲補助之學者能夠至
世界各處發表論文，參與國際事務，為國爭光。 
 
五、攜回資料名稱及內容 
1. 會議論文 CD-ROM 一份，內容包含此次會議之行程、論文全文、委員名單、地
圖及 SMC 出版品等。 
2. 2006 IEEE International Conference on Systems, Man, and Cybernetic 之 Conference 
Digest 一本。 
 
六、其他 
無 
 
 
 
 
 
 
 
 
[11,12,16]. Then the output value ( , )g x y  can be computed in 
the following equation: 
( )
1
( , ) .
K
v j
j
g x y w
=
= ∑                               (1) 
Since each pixel addresses exactly K hypercubes, only those 
weights are used in (1) for each input in the CMAC network. 
Because the output value ( , )g x y  is generated from the 
CMAC hypercubes, the data of hypercubes need to be 
updated by the differential code e(x,y) that is the difference 
between the desired value d(x,y) and output value ( , )g x y  in 
the learning process. The updating algorithm for ( )v jw  is 
defined by the following equations: 
( ) ( ) ( , ); 1,2, ,
new old
v j v i
uw w e x y j K
K
= + = L                    (2) 
),(),(),( yxgyxdyxe −= ,                        (3) 
Where u is a learning rate in the learning process, and 
according to references [12, 16], if 20 << u  is true, then the 
above learning algorithm will converge. 
Suppose there are N discrete units to be distinguished for 
each dimension in the CMAC network. Then there are 
2)]/)1(([ KjNceil −+  hypercubes in the j-th layer CMAC 
network where the function ceil(x) rounds the elements of x to 
the nearest integer towards infinity. Thus the total number of 
hypercubes in a K-layer CMAC network is described in the 
following equation: 
2
1
[ (( 1) / )]
K
m
j
N ceil N j K
=
= + −∑                      (4) 
It is seen that there are only mN  hypercubes needed to 
distinguish 2N  pixels. The significant feature of CMAC 
network is that the learning algorithm changes the output 
values for the neighboring inputs. Similar inputs lead to 
similar output even for untrained inputs because each 
hypercube covers 2K  inputs. This property is called 
generalization [17], which is of great use in the CMAC based 
coding. Moreover, we can control the degree of generalization 
by changing the size of K. The larger K is then the wider the 
generalization region is.  
In the traditional CMAC networks, the updating algorithm 
must go through the input vector to address hypercubes 
whose weights will be updated. Here, we propose an 
addressing function to code those addressed hypercubes. In 
our approach, the addressing vector iv  is used to 
simultaneously generate the indices of K’s addressed 
hypercubes. Let us define the associative input vector 
( , )i i iv x y=  to describe the mapping D M→  and the inverse 
mapping M G→ : Where D is the domain of the input signal 
( , )i id x y ; iv  is the addressing vector mapping the input signal 
( , )i id x y  into K’s hypercubes; ( )iv jw  is the weight of the j-th 
layer hypercube mapped by iv ; G is the output domain; and 
the actual output ( , )ig x y  equals summation of K’s weights 
mapped by vector iv . Mapping the input signal to associative 
memory, D M→ , can be regarded as the encoding process, 
and the inverse mapping from the mapped memory to the 
actual output value, M G→ , can be regarded as the decoding 
process during the reconstruction process. In our approach, 
the addressing vector iv  is used to simultaneously generate 
the indices of K’s addressed hypercubes. Take a 
two-dimension CMAC network as an example, and there are 
N discrete units to be distinguished in each dimension. If 
there are K layers in a CMAC network, it needs only mN  
hypercubes to distinguish 2N  pixels as described in (4). Now, 
consider a signal ( , )i id x y  representing the quantized value 
of the pixel ( , )i ix y , and define the addressing function ( )iv j , 
which is produced by the pixel in the j-th layer for j = 1,2, … 
K. Then the addressing function ( )iv j  for j = 1,2, … K, can 
be generated from the following function:   
( ) ( ) ( ( ) 1) [ (( 1)/ )]i i iv j a x a y ceil N j K= + − × + −    
2( 1) [ (( 1)/ )]j ceil N j K+ − × + − .            (5) 
With this addressing function ( )iv j , 
2N  states can be 
mapped into mN  hypercubes. When a signal ( , )i id x y  is 
quantized, the addressed hypercubes can be obtained directly 
with the above addressing function. Thus, the required data 
extraction or data updating can be performed with those 
hypercubes directly both in the encoding process and the 
decoding process. 
After proposing the coding processes, we will discuss 
one-short training methods in the CMAC network. When we 
choose learned samples such that every weight is updated by 
only one time in each learning cycle, this method is called 
one-shot training [18]. The weights in a CMAC network will 
not be corrupted which is the advantage of one-shot training. 
However, it reveals that CMAC networks will stop learning 
in one-shot training method. In order to prevent the 
phenomenon of stop learning, we propose pyramidal CMAC 
networks where the information can still be updated in the 
lower level’s CMAC without corrupting previous learned 
information, which can improve the above disadvantage. This 
will be discussed in detail in the next section.   
III. DIFFERENTIAL CODING IN PYRAMIDAL CMAC 
NETWORKS 
We propose a method of differential coding in pyramidal 
CMAC networks illustrated in the block diagram as shown in 
Fig. 1. The pyramidal architecture contains m’s levels, and 
each level is composed of four processes: downsampling, 
compression (CR), decomposition (DC), and upsampling. 
Note that the upsampling process is done by an independent 
CMAC network for different levels, which ensures that later 
information can still be updated but not corrupt the learned 
information updated in higher levels of CMAC networks. The 
original image dada are used as initial differential codes 0e  
in the first level, then the differential codes are selected every 
2m  units in each dimension (downsampled data are 1/ 4m  of 
whole image data), and then the sampled differential codes 
331931
 
 
 
From (11) and (12), it needs only one addition per pixel to 
improve the PSNR of reconstructed image in each level. Now 
the error after m’s level reconstruction process is discussed, 
and it can be computed by using the following equation: 
0 0 1
1
( , ) ( , ) ( , ) ( , ) ( , )
m
m i m
i
e x y h x y e x y g x y e x y−
=
− = − =∑ .    (13) 
The decomposed codes ),(ˆ 1 iim yxe −  will approach the 
downsampled part of differential codes ),(1 yxem− , when a 
good transform such as DCT, Wavelet or JPEG is used. And 
the decomposed differential codes ),(ˆ 1 iim yxe − will converge 
if the learning rate u satisfies condition 20 << u  in (10), 
which has been proved in reference [9, 15], so the error 
),(1 yxem−  will converge too. Thus, it can be concluded that 
the lower the level of pyramidal CMAC networks is, then the 
smaller errors will be. If the value of differential codes 
),(1 yxem−  is smaller, then the compressed data volume will be 
lesser too. Thus, we can use smaller differential codes to get 
higher compression rate in the bottom of pyramidal CMAC 
networks.  
IV. SIMULATION RESULTS AND COMPARISONS 
We take a 512x512 size standard “Lena” as an original 
image whose data volume equal 258KB and gray levels equal 
256. In order to measure the quality of the reconstructed 
image, the peak signal to noise ratio (PSNR) is defined in (14), 
where the gray level of image ranges from 0 to 255.  
255PSNR=10 log( ) dB
MSE
×                  (14) 
The JPEG method is applied to the proposed pyramidal 
CMAC network as compression (CR) and decomposition 
(DC) processes in the block diagram of pyramidal CMAC 
networks. The proposed method not only can reduce the bit 
rate during transmission, but also can improve PSNR in the 
lower level of the pyramid. We also compare the proposed 
method with Wavelet transform, and experimental results 
prove that the proposed method is better. The level of 
pyramid CMAC networks is set to be 3 (m=0~2) and learning 
rate u is set to be 1 in the following experiments.  
In the first level the original image data are used as initial 
differential codes (
0e ), and the downsampled differential 
codes whose size is 1/16 of original image through JPEG 
compression become compressed image ( 0e ) shown in Fig. 
2(a) whose data volume is 5KB and bit rate H is 
0.115(bits/pixel). Then the compressed data through JPEG 
decomposition and reconstruction processes CMAC(1) 
become a coarse image ( 1 1h g= ) with PSNR=26.93dB as 
shown in Fig. 2(b). 
In the second level the original image data minus the first 
level’s reconstructed data ( 0 1e g− ) are used as second level’s 
differential codes ( 1e ), and the downsampled differential 
codes whose size is 1/4 of original image through JPEG 
compression become compressed image ( 1e ) as shown in Fig. 
3(a) whose data volume is 11KB and bit rate H is 
0.3411(bits/pixel). Then the compressed data through JPEG 
decomposition and reconstruction processes CMAC(2) 
become the reconstructed differential codes ( 2g ) which must 
add to the first level’s reconstructed image, and those codes 
( 2 1 2h h g= + ) become this level’s output image whose PSNR 
equals 31.38dB as shown in Fig. 3(b). It is obvious that the 
quality of the reconstructed image is improved, but the cost is 
increased bit rate in the second level. 
In the third level the previous level’s differential codes 
minus the reconstructed data ( 21 ge − ) are used as this level’s 
differential codes ( 2e ), which through JPEG compression 
become compressed image ( 2e ) as shown in Fig. 4(a) whose 
data volume is 21KB and bit rate H is 0.6512(bits/pixel). 
Finally the compressed data through JPEG decomposition 
process become reconstructed differential codes ( 3g ) which 
must add to the second level’s reconstructed image, and those 
codes ( 323 ghh += ) become this level’s output image whose 
PSNR equals 35.60dB in Fig. 4(b). In this level all the 
differential codes are transmitted, so they need neither the 
downsampling process nor CMAC reconstruction. Because 
the value of differential codes is smaller in the this level as 
shown in Fig. 4(a), the gray level range of differential codes is 
set to 128 in compression for getting lower bit rate H=0.6512 
(bits/pixel). 
V. CONCLUSIONS 
In this paper, the differential codes transmitting in 
pyramidal CMAC networks assist with JPEG method is 
proposed. It can save transmission time and memory, and the 
PSNR of reconstructed image is gradually improved in the 
lower level of the pyramid. Meanwhile, some advantages 
proved by the proposed method are listed as follows: 
(a) A novel coding method proposed in pyramidal CMAC 
networks makes on-shot training possible, and only 
11/ 4m i− +  (same as sample ratio) of computation is 
needed during compression and decomposition 
processes in the i-th level of pyramidal CMAC 
networks.   
(b) Differential codes can be normalized into different 
gray level ranges, which deduce different compression 
bit rate and image quality.  
(c) The coarsest reconstructed image can be quickly 
produced, and the image quality can be improved in 
the lower level of the pyramid. 
(d) Due to the generalization ability of CMAC networks, 
the sizes of all reconstructed images are equal to the 
original image sizes.  
Because of the above advantages, the proposed method not 
only can decrease the bit rate and computation but also can 
improve PSNR in the compressive domain.  
 
 
 
 
 
 
3319-2333
 
 
 
REFERENCES 
[1] D. Kornreich, Y. Benbenisti, H. B. Mitchell, and P. Schaefer, 
“Normalization schemes in a neural network image compression 
algorithm,” Signal Processing: Image Communication, vol. 10, pp. 
269-278, 1997. 
[2] M. Mougeot, R. Azencott, and B. Angeniol, “Image compression with 
back propagation: improvement of the visual restoration using different 
cost function,” Neural Networks, vol. 4, no. 4, pp. 467-476, 1991. 
[3] A. Namphol, S. Chin, and M. Arozullah, “Image compression with a 
hierarchical neural network,” IEEE Trans. on Aerospace Electronic 
Systems, vol. 32, no.1, pp. 326-337, 1996. 
[4] W. L. Merrill John and R. F. Port, “Fractally configured neural 
networks,” Neural Networks, vol. 4, no. 1, pp. 53-60, 1991. 
[5] J. Stark, “Iterated function systems as neural networks,” Neural 
Networks, vol. 4, no. 5, pp. 679-690, 1992. 
[6] J. M. Mas Ribes, B. Simon, and B. Macq, “Combined Kohonen neural 
networks and discrete cosine transform method for iterated 
transformation theory,” Signal Processing: Image Communication, 
vol.16, pp. 643–656, 2001. 
[7] J. S. Albus, “A new approach to manipulator control: The cerebellar 
model articulation controller (CMAC),” ASME Journal of Dynamic 
Systems, Measurement, and Control, vol. 97, no. 3, pp. 220-227, 1975. 
[8] J. S. Albus, “Data storage in the cerebellar model articulation controller 
(CMAC),” ASME Journal of Dynamic Systems, Measurement, and 
Control, vol. 97, no. 3, pp. 228-233, 1975. 
[9] C. S. Lin and C. T. Chiang, “Learning convergence of CMAC 
technique, ” IEEE Trans. on Neural Networks, vol. 8, no. 6, 
pp.1281–1292, 1997. 
[10] Y. Wong and A. Sideris, “Learning convergence in the cerebellar model 
articulation controller,” IEEE Trans. on Neural Networks, vol. 3, no. 1, 
pp.115–121, 1992.  
[11] F. C. Chen and C. H. Chang, “Practical stability issues in CMAC neural 
network control systems,” IEEE Trans. on Control Systems Tech., vol. 4, 
no. 1, pp. 86-91, 1996. 
[12] W. T. Miller, R. P. Hewes, F. J. Glanz, and L. G. Kraft, “Real-time 
dynamic control of an industrial manipulator using a 
neural-network-based learning controller,” IEEE Trans. on Robot. 
Automat, vol. 6, no. 1, pp. 1–9, 1990. 
[13] T. Tao, H. C. Lu, and S. F. Su, “Robust CMAC control schemes for 
dynamic trajectory following”, Journal of the Chinese Institute of 
Engineers, vol. 25, no. 3, pp.253-263, 2002. 
[14] Y. Iiguni, “Hierarchical image coding via cerebellar model arithmetic 
computers,” IEEE Trans. on Image Processing, vol. 5, no. 10, pp. 
1393-1401, 1996. 
[15] T. Tao, H. C. Lu, C. Y. Hsu, and T. H. Hung, “The one-time learning 
hierarchical CMAC and the Memory limited CA-CMAC for image data 
compression,” Journal of the Chinese Institute of Engineers, vol. 26, no. 
2, pp. 133-145, 2003. 
[16] S. F. Su, T. Tao, and T. H. Hung, “Credit assigned CMAC and its 
application to online learning robust controllers,” IEEE Trans. on 
Systems, Man and Cybernetics, Part B: Cybernetics, vol. 33, no.2, pp. 
202-213, 2003. 
[17] D. E. Thompson and S. Kwon, “Neighborhood sequential and random 
training technique for CMAC,” IEEE Trans. on Neural Networks, vol. 6, 
no. 1, pp. 196-202, 1995. 
[18] H. C. Lu and T. Tao, “Closed-loop method to improve image PSNR in 
pyramidal CMAC networks," International Journal of Computer 
Applications in Technology, v 25, no 1, pp. 22-29, 2006. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
3319-4335
表 Y04 
techniques to solve this problem. These adaptive control schemes have the advantage of 
requiring no prior knowledge of system dynamics. But a general drawback of adaptive 
controllers is that the computational requirements tend to grow undesirably, when the 
number of system state variables increases. A common feature of the various approaches 
of robust control is the use of a large loop gain in the system to suppress the effects of 
plant uncertainty over a specific active frequency range. Another approach of robust 
control is to directly cancel the uncertainties instead of suppressing the uncertainty effects 
using a high loop gain. The latter method is utilized for the proposed adaptive fuzzy 
sliding mode control (SMC) schemes in this paper. The proposed adaptive fuzzy sliding 
control schemes are so simple that only a nominal plant needs to be considered, where the 
vary load and parameters are regarded as plant uncertainties in the nominal plant. Those 
uncertainties can be expressed as a single disturbance force injected at the input of the 
nominal plant in the robust control process. Thus, the proposed adaptive sliding fuzzy 
controller is simpler to realize than other complex adaptive control laws. 
In this paper, we propose adaptive sliding fuzzy control schemes for tracking 
time-various trajectories. In the beginning the center positions of fuzzy sets are fixed, and 
then they will be adjusted by the adaptive fuzzy sliding mode control schemes. Thus, the 
proposed adaptive sliding fuzzy controller can achieve a certain goal accurately in the 
presence of significant plant uncertainties without concern for instability of the controlled 
system. Finally, simulation results show that the proposed robust fuzzy controller not only 
can track the time-various trajectories quickly but also can decrease the error after 
following the reference trajectory.  
 
三、考察參觀活動(無是項活動者省略) 
無 
 
四、建議 
增加出席國際會議費用的額度。例如本次在倫敦舉行會議所需註冊費、機票與
生活費約新台幣 10 萬元，國科會僅補助新台幣 5 萬元，不足款需自行負擔。出席
國際會議費用的額度太低，常導致獲補助者無法到較遠的地方參加會議，喪失為國
爭光以及提升個人研究水平的機會。由於我國地理位處東亞邊緣，除日本之外，距
離歐美先進國家都甚遠。假若國內學者無法獲得較高額度的經費補助，常常只好屈
就國內或附近國家所舉辦的國際會議。長此以往，對於國內研究水平之增長是不利
的。因此，建議政府寬列經費，提高出席國際會議費用的補助額度，以利獲補助之
學者能夠至世界各處發表論文，參與國際事務，為國爭光。 
 
五、攜回資料名稱及內容 
1. 會議論文 CD-ROM 一份，內容包含此次會議之行程、論文全文、委員名單、地
圖及 2008 IEEE International Conference on Fuzzy Systems Call for paper。 
2. 2007 IEEE International Conference on Fuzzy Systems 簡介一本。 
 
六、其他 
無 
 
 
 
 
 
 
 
 
   jR~ : jjn
j
2
j
1 CAAA →××× L .                  (2) 
This fuzzy set is defined in the product space YX × . Based 
on generalizations of implications in multi-value logic, many 
fuzzy implication rules have been proposed in literature on 
fuzzy logic [12]. 
The defuzzifier maps fuzzy sets C to a crisp point in the 
output domain. In general, there are three possible choices 
for this mapping, namely: maximum, center-average, and 
modified center-average defuzzifier. In this paper, we use the 
center of area (COA) with product operation “ • ” mapping 
as the defuzzifier, which is described in the following 
equation: 
  
.
)}({
)}({
1 1
1 1
∑
∑
= =
= =
•
= K
j
n
i
iA
K
j
n
i
jiA
f
xu
yxu
u
j
i
j
i
I
I         (3) 
Where the center position jy  can be viewed as an adjustable 
center position in FLC, and the operation I
n
i 1=  may be the 
production or minimum of n’s input degree, which is define 
as minimum operation in this paper. If we define a fuzzy 
bases function )(Xjε  as the following equation: 
,
)}({
)(
)(
1 1
1
∑
= =
== K
j
n
i
iA
n
i
iA
j
xu
xu
X
j
i
j
i
I
I
ε            (4) 
where each rule contains n-dimension input fuzzy sects A, 
then Eq. (3) can be written as follows: 
 ,ˆˆ)(
1
εθε TK
j
jjf yXu =•= ∑
=
           (5) 
where )](,),(),([ˆ 21 XXX K
T εεεε L=  is a fuzzy bases vector, 
and TKyyy ],,,[ˆ 21 L=θ  is the adjustable center position 
vector. 
III. THE SLIDING MODE CONTROL SYSTEM 
Consider the nth-order nonlinear systems of the form 
xy
duxxxbxxxfx nnn
=
+′+′= −− ),,,(),,,( )1()1()( LL ,    (6) 
where f and b are unknown but bound continuous functions, d 
is an external bound disturbance, and Ryu ∈,  are the input 
and the output of the system, respectively. Let 
( 1) 1ˆ ( , , , , )n n nx x x x x R− +′= ∈L  be the state vector of the system. 
The control objective is to force output to follow a given 
bound reference signal r under the constraints that all signals 
involved must be bound. A sliding mode controller is 
designed for adjusting the control variable u to ensure the 
Lyapunov stability of the system. Now, we design a sliding 
mode controller for an uncertain but bound system. It is 
assumed that the bound continuous function 
1)1( )],,,([ −−′ nxxxb L  can be divided into a nominal part 0M  
and an uncertain part MΔ (i.e. 1)1( )],,,([ −−′ nxxxb L  
MM Δ+= 0 ), and then Eq. (6) becomes the following 
equation: 
du
xxxxfxxxfx
M
M
M
nn
M
Mnn
)1(
]),,,([),,,(
00
0
1
)()1()1()(
Δ
−Δ−
+++
−′+′= LL .   (7) 
If let the uncertain part ),,,,( )()1( nn xxxxg −′ L  
]),,,([ )()1(
0
nn
M
M xxxxf −′= −Δ L , then Eq. (7) can be deduced to 
Eq. (8). 
duxxxxgxxxfx MMM
nnnn )1(),,,,(),,,(
00
1)()1()1()( Δ−− +++′+′= LL
    (8) 
Firstly, if there is not any disturbance in the system (i.e. 
d=0), and then Eq. (8) becomes the following equation: 
   uxxxxgxxxfx M
nnnn
0
1)()1()1()( ),,,,(),,,( +′+′= −− LL     (9) 
Suppose that an ideal control variable u* ensures the system 
output will approach to the reference signal r. In other words, 
it will satisfy Eq. (10).  
*),,,,(),,,(
0
1)()1()1()( uxxxxgxxxfr M
nnnn +′+′= −− LL    (10) 
Now, let an equivalent control described in Eq. (11) take the 
place of control variable u in Eq. (9), and then we can deduce 
Eq. (12), which is also defined as the differential term of 
sliding surface in Eq. (14). 
ekMu
ekxxxxgxxxfrMu
T
Tnnnn
eq
ˆˆ*
]ˆˆ),,,,(),,,([
0
)()1()1()(
0
+=
+′−′−= −− LL ,           
(11) 
0ˆˆ)( =+ eke Tn                 (12) 
Where kˆ , and eˆ  they are defined as follows: 
T
nkkkkk ],,,,[ˆ 1210 −= L , Tneeeee ],,,,[ˆ )1( −′′′= L , and tracking 
error )()()( txtrte −= . If we choose 
T
nn kkkkk ],,,,[ˆ )1(1)2(10 −−= L  such that the roots of 
characteristic Eq. (12) are all in the open left-half plane, and 
then the system will asymptotically track the reference input 
r. Note that the pre-step control variable u(k-1) takes the 
place of u in simulations, and the equivalent control becomes 
the Eq. (13). 
 
)ˆˆ()1(
ˆˆ*
)(
0
0
ekeMku
ekMuu
Tn
T
eq
++−=
+=                    (13) 
 Secondly, a system with an external bound disturbance is 
discussed. The sliding surface )(tS  is defined as Eq. (14), 
which is the integral term of )(tS& . 
           ∫= dttStS )()( & ; eketS Tn ˆˆ)( )( +=&                    (14) 
Here, a hitting control uh described in Eq. (19) is considered 
to drive the state toward the sliding surface, and then the 
overall control becomes Eq. (15), which ensures the state will 
move toward the sliding surface. 
heqs uuuu +==                           (15) 
Now we define a Lyapunov function as Eq. (16) 
2
2
1)( StVe = ,                              (16) 
and deduce its derivative form as following equations: 
35
 
 
 
V. SIMULATION RESULTS 
To illustrate the above design approaches, an inverted 
pendulum system [14] is considered. The dynamics of this 
nonlinear system can be given as follows. 
dbufx
xx
++=
=
2
21
&
& , 1xy =                          
)/cos3/4(
)/()cossin(sin
1
2
11
2
21
mmxml
mmxxmlxxgf
c
c
+−
+−=           
)/cos3/4(
)/(cos
1
2
1
mmxl
mmxb
c
c
+−
+=                    (29) 
Where x1 (rad) is the angle of the pole with the range of initial 
angles in 0~ ± 0.2(rad), x2 (rad/ sec) is the angular velocity 
of the pole, g is the acceleration due to gravity( 9.8m/ s2 ), mc 
is the mass of the cart (1.0kg) , m is the mass of the pole 
(0.1kg) , u (Nt) is the force applied to the cart, d is the external 
disturbance (-5Nt ≤ d ≤5Nt), and the length of the pole l 
equals 0.5m. The reference signal is r(t),  the tracking error is 
defined as e(t) = r(t) − y(t), and the differential error is 
defined as 
stkekete /)]1()([)( −−=& , where ts is the sample 
time.  
The following simulations are discussed here in order to 
prove that the adaptive fuzzy sliding mode controller is better 
than a fixed fuzzy sliding mode controller. There are 25 rules 
utilized in this two input and one output fuzzy controller. The 
principle of those rules is that if the error and acceleration of 
error is larger then the control output should be larger, and 
vice versa. The membership functions of fuzzy sets are 
shown in Fig.2. The max bound (bmax) value of fuzzy set is 
set to be is set to 1 for input and 5 for output, and the center 
position vector is set to be T]1.0,05.0,0,05.0,1.0[ −− for input 
and T]5,5.2,0,5.2,5[ˆ −−=θ for output. After adaptive fuzzy 
tuning, the output fixed center position vector θˆ will become 
T]4.9871 0,1.9802, 1.9161,- -4.9759,[ .  
The reference signal r is defined as )]3sin(3.0)[sin(10 tt +π  
(rad), the initial y(0) equals 0.2(rad) and the external 
disturbance is a square wave whose amplitude equals 5(Nt). 
From the above conditions and Eq. (29), we can get the range 
of parameter 1/b is 72.0/168.0 ≤≤ b as shown in Fig.4. We 
have defined the parameter bMMM /10 =Δ+=   in section 
III, and the mean value of parameter M equals 0.7, so we 
choose the nominal value 7.00 =M . Owing to Eq. (19) 
( 6.3)]},,,(/[max{ )1(max =′= −nxxxbdabsD L ), we let the 
bound condition Dmax=4. The performance of the fixed fuzzy 
sliding mode control system is shown in Fig.4 and the 
performance of the adaptive fuzzy sliding mode control 
system is shown in Fig.5. The part (a) of Fig.4 and Fig.5 
shows the reference signal r and output y, and the part (b) of 
Fig.4 and Fig.5 shows control variable u and external 
disturbance d. From the simulation results, we observe that 
although the reference signal is time-various and with 
external disturbances the output still can follow reference 
signal very well with the proposed adaptive fuzzy sliding 
mode control schemes. The simulation results confirm that 
the root mean square error (RMSE=0.02493) of the adaptive 
fuzzy sliding mode control system is smaller than that 
(RMSE=0.03296) of the fixed fuzzy sliding mode control 
system. It is obvious that the performance of proposed 
adaptive fuzzy controller is better than the fixed fuzzy sliding 
mode controller both in root mean square error and control 
output. The drawback of fixed fuzzy controller is that its 
control output may chatter or delay when tracking the 
time-various trajectories. However, the adaptive fuzzy 
control schemes can tune the center position of fuzzy sets 
such that the plant output can track the time-various 
trajectories quickly and accurately.    
 
1
PSZNSNL PL
bmax-bmax
x
xu
2x1x 5x4x3x
 
Fig.2 The membership functions of fuzzy controllers. 
 
Fig.3. The range of parameter M for time-various  
reference signals. 
 
Fig.4 The performance of fixed fuzzy sliding mode control  
(a) the reference signal r and output y, (b) control variable u 
and external disturbance d. 
37
