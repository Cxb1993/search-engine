目錄 
摘要.......................................................................................................................................... 3 
Abstract .................................................................................................................................... 3 
1 Introduction...................................................................................................................... 4 
2 Literature review .............................................................................................................. 4 
3 The objective of research and report layout..................................................................... 5 
4 3D Z-string ....................................................................................................................... 5 
5 The presentations of spatial-temporal relations ............................................................... 6 
6 Similarity Retrieval .......................................................................................................... 7 
7 Performance Analysis..................................................................................................... 11 
8 Implementation .............................................................................................................. 12 
9 Conclusion and discussion ............................................................................................. 13 
References .............................................................................................................................. 13 
計畫成果自評........................................................................................................................ 14 
 2
1 Introduction 
With the advances in information technology, videos have been promoted as a valuable 
information resource. Because of its expressive power, videos are an appropriate medium to show 
dynamic and compound concepts. Recently, there has been widespread interest in various kinds 
of database management systems for managing information from videos. The video retrieval 
problem is concerned with retrieving videos that are relevant to the users’ requests from a large 
collection of videos, referred to as a video database. 
2 Literature review 
Over the last decade, many image/video indexing and retrieval methods have been proposed, for 
example, OVID, KMED, QBIC, VideoQ, VideoText, and VideoQA etc. [1], The T2D-histogram 
[2] and Pixel Change Ratio Map (PCRM) [3] used the histogram method to index the videos. Su 
[4] used the motion vectors embedded in MPEG bit streams and do not consider the shape of a 
moving object and its corresponding trajectory. Hsieh [5] also proposed a hybrid motion-based 
video retrieval system to retrieve desired videos from video databases through trajectory 
matching. Spatial-temporal visual map (STVM) [6] defined the spatial-temporal visual similarity 
to rerank the text-retrieval results and find new results. Snoek [7] proposed an automatic video 
retrieval method based on high-level concept detectors that provided three strategies, namely: text 
matching, ontology querying, and semantic visual querying to select a relevant detector from the 
video database. To retrieve desired videos from a video database, one of the most important 
methods for discriminating videos is the perception of the objects and the spatial-temporal 
relations that exist between the objects in a video. To represent the spatial relations between the 
objects in a symbolic image, many iconic indexing approaches and image retrieval algorithms 
have been proposed. Such as, 2D string, 2D G-string, 2D C-string, 2D C+-string, unique-ID-based 
matrix, GPN matrix, virtual image , BP matrix, and 2D Z-string [8]. To represent the spatial and 
temporal relations between the objects in a symbolic video, many iconic indexing approaches, 
extended from the notion of 2D string to represent the spatial and temporal relations between the 
objects in a video, have been proposed. For example, 2D B-string, 2D C-Tree, 9DLT strings, 
3D-list, 3D C-string, and 3D Z-string [9]. The 3D Z-string, extended from the 2D Z-string, used 
the projections of objects to represent spatial and temporal relations between the objects in a 
video. Since there are no cuttings between objects in the 3D Z-string, compare to the 3D C-string, 
the integrity of objects is preserved. The 3D Z-string is more compact and efficient in terms of 
storage space and execution time than 3D C-string. 
The capability of similarity retrieval is important in video database management systems. In 
3D C-string similarity retrieval [10], used the time interval sets of similar spatial relation 
sequence and temporal relations for each pair of objects in a video and defined various types of 
similarity measures (type-std, Spatial, Temporal, and Duration) to construct the association graph, 
which connected by an edge which associates with an interval set. The similarity is the 
intersection of the interval sets to find the largest type-std clique. Therefore, the exactly match 
 4
projections of the initial locations of objects A, B, C, and D in the x- and y-axes are shown in Fig. 
1(b). The corresponding 3D Z-string of the video is shown in Fig. 1(c).  
 
 
 
 
 
 
 
Frame 1                  Frame 2                Frame 3  
(a) A video Va contains 3 frames. 
  
 u-string: ((A6 %2B21,11,0.8)|(D2%1C0.52,1)) 
 v-string: ((D3/0.5(B2.50,10,1.25 [C0.51.5,1))|A2) 
 t-string: ((B3#1#2 = C3#3 = A3)] D1) 
  
  
(b) Projecting the initial locations of objects.   (c) The corresponding 3D Z-string. 
Fig. 1. An example video and the corresponding 3D Z-string. 
5 The presentations of spatial-temporal relations  
First of all, we can use the definition of spatial-temporal relation in the Table 1 to determine the 
spatial relation between the x- (or y-) projections of a pair of objects. Similarly, we also can 
determine the temporal relation between the time-projections of a pair of objects. Second, we 
define the spatial relation sequence SRS to precisely record the spatial relation changes and 
temporal relation TRPQ to record the temporal relation between object pairs in a video as 
following. 
Definition 1 
If P and Q are the object pair in video V, a spatial relation sequence SRSu= SRu1, SRu2,…, 
SRun (or SRSv= SRv1, SRv2,…, SRvn) where SRuk (or SRvk) means that the spatial relation between 
P and Q in the x (or y) dimension. We said the SRSuPQ (or SRSvPQ) is the spatial relation sequence 
between P and Q in the x (or y) dimension of video V. 
Definition 2 
If P and Q are the object pair in video V, a temporal relation TRPQ means the temporal 
relation between the time-projection of object P and that of object Q in video V. 
For example in Fig 1, there are two objects A and B, if the spatial relation between objects A 
and B is “A%B” in frames 1 and 2, and is “A]B” in frame 3. The spatial relation sequence of 
objects A and B is SRSuAB ={“A%B”, “A]B”}, and the temporal relation TRAB = {“A=B”}. 
Therefore, we can obtain all the spatial relation sequences for each pair of objects. 
A 
B C 
A 
B 
C 
 
D 
A 
B 
C 
A
C 
D B 
 6
Spatial type 1: If (P, Q) is a spatially c-similar pair, the similarity of Spatial type 1 (SimS1) is 
equal to 1; otherwise, it is equal to 0. 
Spatial type 2: If (P, Q) is a spatially similar pair, the similarity of Spatial type 2 (SimS2) is equal 
to 1; otherwise, it is equal to 0. 
Temporal type 1: If (P, Q) is a temporally i-similar pair, the similarity of Temporal type 1 (SimT1) 
is equal to 1; otherwise, it is equal to 0. 
Temporal type 2: If (P, Q) is a temporally similar pair, the similarity of Temporal type 2 (SimT2) 
is equal to 1; otherwise, it is equal to 0. 
Since video data contain very rich spatial-temporal information, users may extract different 
spatial-temporal levels of information according to their interests. Therefore, we can define the 
similarity between an object pair (P, Q) as follows:  
Similarity(P, Q) = WSRS * SimS1(P, Q) +WSCS * SimS2(P, Q)+WT * SimT1(P, Q)+WTR * SimT2(P, Q)     (1) 
where the sum of WSRS, WSCS ,WT, and WTR is equal to one. 
To find the similarity between videos V’ and V, we must consider all possible matched object 
sets from both videos. However, there are a large number of matched object sets, and it seems 
difficult to find all of them. We solve such a problem by the dynamic programming approach to 
decrease the calculate cost.  
Let O = O1, O2, … ,On , be objects contained in query video V, where n is the number of 
objects of V, and O = O’1, O’2, … ,O’n, be matched objects in video V’ that O1, where (Oi, O’i) is 
an similar object pair, i=1, 2, …, n. We can form a weighted directed graph G = (V, E), where G 
contains n vertices v1, v2,…,vn, vi denotes object pair (Oi, O’i), a weight function w: ER maps an 
edge to the real-valued similarity. We wish to find, for each path of vertices vi to vn, where i=1 to 
n-1, to calculate a maximum similarity path, where the similarity of a path is the sum of the 
similarities between each vertex. The weighted directed graph G = (V, E) can be represented by 
an adjacency-matrix W, where W is an n* n matrix, and wij represents W the similarity between 
objects Oi and Oj if objects Oi and Oj is a similar pair; otherwise, wij is equal to 0.  
 0 if i<j 
wij = the similarity between objects Oi and Oj if objects Oi and Oj is a similar pair  
 0 if objects Oi and Oj is not a similar pair 
The tabular output of the maximum similarity algorithm presented is an n* n matrix D= (dij), 
where entry dij contains maximum similarity of a path from vertex i to vertex j. To solve the 
maximum similarity problem on an input adjacency matrix, we need to compute not only the path 
of maximum similarities but also a predecessor matrix R= (rij), where rij is NIL, if i > j; otherwise, 
rij represents a predecessor of j on a path starting from i. For each vertex i V, we define the 
predecessor subgraph of G for i as Gr,i =(Vr,i ,Er,i ), where  
Vr,i ={ j V: rijNIL and i < j }{j}, and 
Er,i ={(rij , j): Vr,i and rijNIL and i < j } 
We denote the matrices by uppercase letters, such as R or D, and their individual elements by 
subscripted lowercase letters, such as rij or dij. Some matrices will have parenthesized 
 8
In the similarity retrieval algorithm, we compute the temporal relation and spatial sequence 
for each object pair in Step 1. If the number of objects is equal to n, there are n*(n-1)/2 
combinations of spatial-temporal relations to be inferred in order to construct the similarity 
martix D(1) in Step 3, the complexity is bounded by O(n2). In step 4, we apply maximum 
similarity algorithm to compute the maximum similarity in each row of D(1), there are 
(n-3)*(n-m)*(n-m+1) of three loops to derived the final matrix D(n-1), where n>m, the complexity 
is bounded by O(n3). Therefore, the total cost of applying the similarity retrieval algorithm is 
bounded by O(n3+ n2) as O(n3). 
Lemma 1 
For a 3D Z-string, the time complexity of the similarity retrieval algorithm is bounded to 
O(n3), where n is the number of objects in query video.  
Proof: As the previous description. 
   For another problem, in some videos, there are many similar attribute objects, as the team 
member or referees in sports. To compare the different number of similar attribute objects 
between videos, although they may have different relative spatial-temporal relations, but the 
similarity between them must be seen as a group. We modified our algorithm to include this 
condition. The weighted, directed graph G = (V, E), which some vertices may contain more than 
one object that with the similar attribute. We grouped those objects to a list with same vertex as 
vi1, vi2,…, vjk in a vertex vi, where k is the number of similar attribute objects, and the similarity 
between matched object pair also form a list as weights of G, that is, W=(wip, jq), where 
,
0                                                              if  
the similarity between object  and     if object ,  is similar pair
0                                               
p qi j p q p q
i j
w i j i j


               if object ,  is not similar pairp qi j

  
 The tabular output of the maximum similarity also is an n* n matrix D= (dij), where n is the 
number of different attribute objects of video. The dij(m) be the maximum similarity path of from 
vertex j to vertex n that contains at most n-j-1 edges. If vertex j with the number of similar 
attribute objects, k, then dij(m) contain a list of di(j1), di(j2),…, di(jk), where di(jp) is the maximum 
similarity path of from vertex jp to vertex n that contains at most n-j-1 edges, where p=1,2,..,k. 
Thus, we still can use the maximum similarity algorithm to find the similarity rank list with 
similar attribute objects. We only take maximum similarity of the final matrix D(n-1) of each path 
and remove the object list from similarity rank list that be contained in the others. Since the cost 
still bounded by the total number of object in video, so the complexity is bound by O(n3) as 
Lemma 1. 
In comparison with the retrieval method of 3D C-string [10], which is used to find the 
exactly matched object sets, the retrieval method of 3D Z-string can find the partly matched 
object sets. That is, the retrieval method of 3D Z-string provides a more flexible way to retrieve 
similar videos.  
 10
Fig. 5(a) illustrates the execution time versus the number of objects of in a database video. 
The 3D Z-string approach needs less execution time than the 3D C-string approach does. The 3D 
Z-string approach is 15~30 times faster than the 9DLT approach. Fig. 5(b) illustrates the 
execution time versus the number of frames in a database video. The 3D Z-string approach needs 
less execution time than the 3D C-string approach does. The 3D Z-string approach is 15~20 times 
faster than the 9DLT approach. 
In summary, the execution time of both 3D Z-string and 3D C-string approaches increase 
nearly linearly as the number of database videos or the number of frames in database video 
increases. The execution time of the 3D Z-string approach grows comparatively slowly as the 
number of objects in a query increases. For all cases, the 3D Z-string approach requires less 
execution time than the 3D C-string approach does. The 3D Z-string approach is 8~30 times 
faster than the 9DLT approach. 
 
8 Implementation 
In the research, we developed a prototype video database management system that supports our 
proposed retrieval method. The prototype system provides two basic functions: indexing videos 
to generate 3D Z-string and retrieving the similar videos.  
A video consists of a sequence of shots in general and it is convenient to adopt a shot as a 
basic indexing unit. In our prototype system, we use a shot detector to automatically parse a raw 
video into a sequence of shots. For each shot, we implement a graphical user interface to index 
the objects in the shot. The snapshot of the interface is show in Fig. 6(a). After the indexing step, 
we can use the save function to generate the corresponding 3D Z-string and store it in database. 
In the database, the user can use an example video to query the video database as showed in Fig. 
6(b). This interface can use to select an example video from the database, and use it to retrieve 
the similar videos. The query result is showed in a similar way. Also, the user can assign the 
weights of similarity for different spatial-temporal criteria to meet his (her) requirement. 
         
(a) The video indexing tool             (b) the interface of query by an example video 
Fig. 6. The video indexing and query tools 
 
 12
