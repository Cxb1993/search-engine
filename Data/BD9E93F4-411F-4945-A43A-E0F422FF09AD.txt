 N. Hu et al. [6] 提出ㄧ個非線性嵌入與映射 (Non-linear embedding and mapping)方法，估算頭部影像
角度。其利用流型 (Isomap) 發展並以非監督式訓練出一個與個人無關之頭部角度的非線性嵌入空間，其
可呈現出角度變化的連續性；然而在訓練過程中須經由 ISOMAP的轉換、正規化、平滑化，得到一組能
反映頭部角度的對應點後，再進行翻轉平滑化運算，之後再利用原影像與計算出的對應點訓練出一個對
應函數，需花費大量的運算時間，且累積的誤差較大。圖 1為完成訓練後的人臉角度分佈結構圖。 
 
圖 1. 依照 N. Hu et al. [6] 所提出的模型而形成的分佈結構 
 
(二). 人臉識別 
 Turk and Pentland [7] 在 1991 年提出了 PCA 人臉辨識方法，使用許多相同大小的人臉訓練影像，
將每個人的臉部影像進行轉換，以最重要的數個 eigenvectors，其所生成 (span) 出來的空間稱之為 
eigenspace。所有影像在計算距離前均先投影至這個空間，而達到保留重要資訊與降維度的目的。投影後
的人臉影像由 eigenvectors (也稱為 Eigenface)的線性組合所組成，因而此方法也稱為 Eigenface 法。 
Jia-Zhong He et al. [8] 認為在某些應用上，像是護照的人臉只有單獨的一張照片，能夠使用的訓練資
影像只有一張，因而利用離散餘弦轉換 (Discrete Cosine Transform，DCT) 得到的頻率域資訊，強化人臉
影像，再進行 PCA 的處理，完成人臉影像的訓練與辨識。X. Hong ea al. [9] 同樣利用DCT的頻率域資訊，
針對嘴唇的影像，以不同的方式進行係數的處理，在以 PCA 進行辨識的處理。 
 Jian Yang et al. [10]指出進行PCA的運算處理，需要大量的儲存空間以及時間來處理求解eigenvectors
與eigenvalues，因而提出改變共變異數矩陣的計算方式來解決此問題，發展出了Two Dimension Principle 
Component Analysis (2DPCA)，同時也做出可行的證明。 
 R. A. Fisher [11] 在 1938 年提出了一個線性區別 (Linear Discriminant) 的方法，稱之為 Linear 
Discriminant Analysis (LDA)，由於 R. A. Fisher 是第一個提出這個的方法，所以也稱之為 Fisher Linear 
Discriminant。 LDA 利用 scatter matrix 的概念，區隔出同類與不同類的資訊，並導入維度空間轉換的方
Hierarchical Isomap,” in Proc. of IEEE Workshop on Automatic Identification Advanced Technologies, Alghero, 
pp. 103-106, June 7-8, 2007.  
[3]. Wei Fan and Dit-Yan Yeung, “Face Recognition with Image Sets Using Hierarchically Extracted Exemplars 
from Appearance Manifolds,” in Proc. of the 7th International Conference on Automatic Face and Gesture 
Recognition, pp. 177-182, 2006. 
[4]. Chao Shao, Houkuan Huang, and Chunhong Wan, “Selection of the Suitable Neighborhood Size for the 
ISOMAP Algorithm,” in Proc. of International Joint Conference on Neural Networks, pp. 300-305, 2007. 
[5]. M. H. Yang, “Extended Isomap for Pattern Classification,” in Proc. of the 18th National Conference on 
Artificial Intelligence (AAAI 2002), pp. 224-229, 2002. 
[6]. Nan Hu, Weimin Huang, and Surendra Ranganath, “Head Pose Estimation by Non-linear Embedded and 
Mapping,” in Proc. of IEEE International Conference on Image Processing, Vol. 2, pp. 342-345, 2005. 
[7]. Matthew A. Turk and Alex P. Pentland, “Face Recognition Using Eigenfaces,” in Proceedings of IEEE 
Computer Society Conference on Computer Vision and Pattern Recognition (CVPR '91), pp. 586-591, June 3-6, 
1991. 
[8]. Jia-Zhong He, Qing-Huan Zhu, and Ming-Hui Du, “Face Recognition Using PCA on Enhanced Image for 
Single Training Images,” in Proc. of the Fifth International Conference on Machine Learning and Cybernetics, 
pp. 3218-3221, 2006. 
[9]. Xiaopeng Hong, Hongxun Yao, Yuqi Wan, and Rong Chen, “A PCA based Visiual DCT Feature Extraction 
Method for Lip-Reading,” in Proc. of the 2006 International Conference on Intelligent Information Hiding and 
Multimedia Signal Proceeding (IIH-MSP’06), pp. 321-326, 2006. 
[10]. Jian Yang, David Zhang, Alejandro F. Frangi, and Jing-Yu Yang, “Two-Dimensional PCA: A New 
Approach to Appearance-Based Face Representation and Recognition,” IEEE Transactions on Pattern Analysis 
and Machine Intelligence, Vol. 26, No. 1, pp. 131-137, 2004. 
[11]. R. A. Fisher, “The Statistical Utilization of Multiple Measurements,” Annals of Eugenics, Vol. 8, pp. 
376-386, 1938.  
[12]. Ming Li, Baozong Yuan, “2D-LDA: A Statistical Linear Discriminant Analysis for Image Matrix,” Pattern 
Recognition Letters, Vol. 26, pp. 527-532, 2005. 
[13]. Aleix M. MartõÂnez and Avinash C. Kak, “PCA versus LDA,” IEEE Transactions on Pattern Analysis and 
Machine Intelligence, Vol. 23, No. 2, pp. 228-233, 2001. 
[14]. Jian Yang, David Zhang, Alejandro F. Frangi, and Jing-Yu Yang, “Two-Dimensional PCA: A New 
Approach to Appearance-Based Face Representation and Recognition,” IEEE Transactions on Pattern Analysis 
and Machine Intelligence, Vol. 26, No. 1, pp. 131-137, January, 2004. 
[15]. Ying Wen and Pengfei Shi, “Image PCA: A New Approach for Face Recognition,” in Proc. of the 
量的運算時間，而正規化的過程不但要先找出不平滑橢圓的中心、長軸、短軸和角度，之後還必須經由
轉換與翻轉運算，將頭部影像對應的特徵點根據其角度，依一定的順序分布於一個近似單位圓(如圖3所
示)，進而再訓練一個非線性內插映射函數。 
 由於在取得訓練樣本過程是我們可掌控，被拍攝者坐在一張可以 360 度旋轉的椅子上面，以身體為
中心，從面對鏡頭的方向開始，順時針水平方向等速旋轉 360 度，取得影像序列，接著再從中均勻取樣。
因而每個樣本的影像的頭部角度可以精確預估，使得我們可以直接把所得到的頭部影像均勻依序對應至
一個以原點為圓心的單位圓上，從圓上 0 度的方位沿著逆時針的方向開始走 360 度對應。對應到的點
稱為該頭部影像的特徵點。有了這樣的訓練樣本與其對應的特徵點，就可以直接有效地監督式訓練一個
非線性內插映射函數。其為數個輻狀基底函數(Radial Basis Functions，RBF) 之線性組合。輻狀基底函數
( )•ψ 定義於式(三)，其中c和 2iσ 分別表示基底函數的中心與寬度。非線性內插映射函數則表示於式(四)，
其中 iω 為實係數(Real coefficients)， ic 表示第i個基底函數的中心， ||||• 表示Rd中向量的長度(Norm)。  
( ) = ||c-y|| ψ  2
2
2
c)-(y
-
e σ     式(三) 
f(y) = ( )∑M
1i
ii0  ||c-y||  
=
⋅+ ψωω  式(四) 
首先結合每組頭部角度影像樣本於序列 Γ = iP
K
i
Y
1=∪ = },...,,{ 021 nyyy 和特徵空間中的相關座標 
(Corresponding coordinates) 於 Λ = iP
K
i
Z *
1=
∪ ={z *1 ,…,z *n0 }，其中 n0 表示全部的輸入影像之畫格總數。接著再
以取得的樣本來訓練非線性內插映射函數 f。 
藉由設計好的拍攝流程，我們可以得到樣本群，並經由裁切和正規化的步驟，完成樣本的前處理(大
小為 35 × 40)，如圖3.所示。接下來針對四個象限均勻取樣，以便對應至正確的角度，並訓練出一組非線
性內插映射函數。 
  
  
  
oo 50≤<40 θ  187 269 
oo 60≤<50 θ  106 171 
oo 70≤<60 θ  27 119 
oo 80≤<70 θ  24 72 
oo 90≤<80 θ  9 60 
o90>θ  8 197 
表 1. 兩方法之辨識角度誤差統計表 
 
       方法 
角度範圍 
本文所提的方法 N. Hu et al.的方法 
0
o
~ 360
o
 18.5
o
 30
o
 
-90
o
~ 90
o
 10.21
o
 27.47
o
 
-60
o
~ 60
o
 8.25
o
 22.85
o
 
表 2. 針對不同角度辨識範圍，兩方法之平均角度誤差 
從以上實驗結果可觀察出，本計畫所提的方法比 N. Hu et al. 的方法估計出的頭部角度具較低的誤
差。此外，本計畫所提的方法之訓練過程較為簡單且快速，圖 4為一組資料使用本計劃所提的方法與 N. Hu 
et al. 的方法之測試結果之角度分布圖。 
接下來分別說明這三個辨識模組、權重式投票法以及如何訓練出最佳的權重值。 
在 E-2DPCA辨識模組中，首先針對原影像 A的每個 8x8區塊分別進行 DCT轉換，擷取出部份的頻
率域係數，以 IDCT轉換回空間域得到影像 B，再將它和原始影像做比例的疊加(權重平均)，便可以強化
影像中的特徵，結果如圖 5所示。假設比例參數為α ，那麼疊加後的新影像 C，可用式(五)表示。 
y)(x,y)(x,)1(y)(x, BAC αα +−=          式(五) 
 
(a)                    (b)                   (c) 
圖 5. (a)原影像、(b)每個 block取 20個 DCT係數後，使用 IDCT運算後形成之影像、(c)原影像與(b)
強化後影像( 5.0=α ) 
 
當所有的訓練影像皆經過 DCT 的強化處理後，再使用 2DPCA 人臉辨識，其辨識方式就如以下的
2DPCA模組。 
在 2DPCA模組訓練過程中，將所有訓練影像減去影像平均資訊後，計算它們的共變異數矩陣 (Sum of 
Covariance Matrix)，再算出矩陣的 eigenvectors 和 eigenvalues。利用預設好的門檻值求得所取用之
eigenvectors的數量，生成出一個 eigenspace。訓練影像投影至此 eigenspace得到影像特徵，儲存於資料庫
中。 
在測試階段時，將測試影像先減去訓練影像平均資訊，再將之投影至出訓練過程所得到的 eigenspace
取得影像特徵，利用歐氏距離找出資料庫裡前幾個最相似的訓練影像特徵，依相似度將之排序輸出。 
DCT coefficient模組會先將影像進行 DCT轉換，得到影像在頻率域的係數，並以 Zigzag的順序取出
前數個係數，作為影像在頻率域的特徵，再利用歐氏距離計算相似度，以達辨識之目的。 
投票法的基本概念，便是當多數成員有不同的意見時，採用大多數成員所支持的想法，即得票數最
多的想法。有時候為了表現每個投票者具不同的可信度，常賦予不同的權重來完成，稱之為權重式投票
法。 
本研究提出的系統即是結合前面所提的三個模組與權重式投票法，建立出一個人臉辨識系統。對於
Accuracy E-2DPCA 2DPCA DCT Coef. Weighted Voting 
Average 74.38% 73.50% 73.72% 77.96% 
表 4. 三個辨識模組與權重投票法的平均辨識率比較 
結果與討論： 
本計畫目標達到理論與實際並重。目前，我們已經完成了以下工作項目： 
一、尋找多樣化的人臉特徵與改良目前的辨識方法，以建立一高辨識率之人臉辨識系統，適用於多
樣的變異，如角度、表情、或光線等的變化。 
二、開發一有效能的人臉辨識系統，能辨識人臉影像角度的能力。 
本計畫建立出一個快速且正確的人臉識別系統，期能達到更高的應用效能，對社會安全方面能有所
貢獻。 
  
五、攜回資料名稱及內容 
攜回資料為會議論文集的光碟片。會議論文集中包含有所有在會議中發表的文章全文。 
六、其他 
 無 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
D. Allocation 2: 
To allocate the texture of a reference region to a target region, 
first mark a pair of corresponding points (xT, yT) and (xR, yR) in 
the target region T and reference image R, respectively, and 
then allocate the luminance of the point (x’, y’) in R to the point 
(x, y) in T, where (x’, y’) - (xR, yR) = (x, y) - (xT, yT), as shown in 
Fig. 3. A scale w could be added to adjust the texture size to 
create the new correspondence: (x’, y’) - (xR, yR) = w((x, y) - (xT, 
yT)) or (x’, y’) = (wx + xR - wxT, wy + yR - wyT). Let T’ be the 
texture-allocated image and Tl’(x, y) be the luminance of (x, y). 
Then, the allocation can be described by (2). Fig. 4(b) shows an 
example of such allocation with w = 2. 
 
 
),(),(' TRTRll wyywywxxwxRyxT'         (2) 
 
 
 
 
 
 
E. Allocation 3: 
In order to retain the original mean luminance of the target 
image, the system provides another version of allocation which 
simply adds the difference of the luminance of its 
corresponding pixel from the mean luminance in the reference 
image, as shown in (3). Fig. 5 shows an example of such 
allocation. 
 
 
)),((),(),(' mwyywywxxwxRyxTyxT TRTRlll  (3) 
 
 
 
  
 
III. EXPERIMENTAL RESULTS 
Our experiments were implemented using the Windows XP 
operating system on a 2.0 GHz P4 PC with 1GB memory using 
a Borland C++ Builder 6.0 compiler. Test images were 
downloaded from the Internet [8]. 
Fig. 6 shows a stylization result using a single reference 
image. Stylization can be carried out by more than one of the 
allocation methods with more than one reference image. This 
makes the system more flexible and practical in use, especially 
for interior design. Figs. 7 and 8 show some stylization results 
with the combination of different allocations using more than 
one reference image. 
 
 
 
   
(a) original image       (b) reference        (c) Allocation 3  
                                                                   (w = 2，k = 0.5) 
Fig. 6 Allocation result using a single reference image (1) 
 
       
    (a) reference image        (b) result of color allocation
Fig. 2 Color allocation (Allocation 1) 
 
Fig. 5 Texture allocation (Allocation 3, w = 2)
       
(a) reference image R       (b) target region T  
Fig. 3 Correspondence of the points selected from two images 
 
 
  
(a) Reference texture image    (b) Texture allocate results 
  
Dear Prof. / Dr. Lin, 
 
We would like to inform you that your paper: 
ID: 303-355 
Title: Stained Glass Rendering for Images 
 
submitted to the International conferences in Corfu Island, Greece, July 
14-17, 2011 has been accepted for publication in the conference 
proceedings.  After the conference, CD-ROM and Book proceedings will be 
sent to ISI, EI Compendex, SCOPUS, IET, Elsevier and all the other indexes 
of http://www.wseas.us/indexes . 
(Conferences are sponsored by IEEEAM, EUROPMENT, WSEAS) 
 
Registration deadline: July 1st 
Revised paper submission deadline: July 3rd 
 
Note that the reviewers' comments will be sent to you after your 
registration. You will also receive a username and password that will grant 
you access to the WSEAS E-Library. 
 
Please consult the attached form for your registration. You will need to 
fill in the required information on the first page and send it to us either 
by email or by fax. 
 
E-mail: support@wseas.org 
Fax: 0030 211 800 1964 
 
Make sure that you book your accommodation early because there is a limited 
number of rooms available. 
 
Notice that three options of payment are given: 
 
1) Bank Deposit 
You may deposit your registration fee to the following account: 
 
----------------------------------------------------------- 
BANK: PIRAEUS BANK: Account Number 1602 1490 91 
IBAN: BG64 PIRB 8050 1602 1490 91  
  
 
Best Regards, 
 
Rayna Moskova 
Alkis Polyrakis 
WSEAS 
Ag. I. Theologou 17-23 
Zografou, 15773, Athens, Greece 
Phone +30 210 747 3313 
FAX +30 211 800 1964  
URL: http://www.wseas.org 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：林慧珍 計畫編號：99-2221-E-032-069- 
計畫名稱：頭部角度估計與人臉辨識系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 0%  
研究報告/技術報告 0 0 0%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 0%   
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 1 1 20%  
博士生 1 1 40%  
博士後研究員 0 0 0%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
 
期刊論文 3 2 150%  
研究報告/技術報告 0 0 0%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 0% 章/本  
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 0 0 0%  
博士生 0 0 0%  
博士後研究員 0 0 0%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 0% 
人次 
 
 
