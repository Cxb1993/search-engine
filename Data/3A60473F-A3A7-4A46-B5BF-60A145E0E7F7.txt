行政院國家科學委員會專題研究計畫 結案報告 
 
計畫編號：NSC 100-2628-E-002-015 
執行期間：100 年 8 月 1 日至 101 年 7 月 31 日 
計畫主持人：台大電子所 李建模 副教授 
共同主持人： 
計畫參與人員： 廖官榆 
計畫類別：□個別型計畫 √整合型計畫 
 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、 
          列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
執行單位：國立台灣大學電子工程研究所 
 
 
 
 
 
 
中 華 民 國 101 年 6 月 26 日 
奈米 IC 設計之前瞻電子設計自動化技術 
子計畫六：在奈米製程下考量信號完整度之測試與診斷技術 
附件一 
Section 3 gives the background of GPU 
architecture.  Section 4 describes the details of 
our proposed algorithm.  Section 5 shows the 
experimental results. Section 6 gives an analysis 
on GPU memory size.  Section 7 concludes this 
report. 
3. GPU Architecture 
NVIDIA GPUs support thousands of 
threads running concurrently.  We implement 
our ATPG using compute unified device 
architecture (CUDA) on NVIDIA’s graphic card.  
CUDA is the hardware and software architecture 
that allows programs written in C, C++, OpenCL, 
and other languages to run on NVIDIA’s GPUs.  
A task running on GPU is called a kernel, which 
specifies how each thread is to be executed.  To 
manage the threads running on GPUs, CUDA 
provides a three-level architecture: grid, block, 
and thread.  On GPU, all threads of the same 
tasks reside in the same grid.  Each grid 
contains three-dimension of blocks and each 
block contains three-dimension of threads.  In 
the particular GPU we use in our experiment, 
GeForce GTX590, the maximum allowable 
blocks within a grid is 248 and the maximum 
allowable threads within a block is 210.  
Though theoretically 258 of threads can be 
invoked at the same time, the actual number of 
threads is limited by the memory capacity of the 
GPU. 
4. GPU-Based ATPG Algorithm 
The test generation algorithm generates test 
patterns by repeatedly backtracing objectives and 
propagating fault effects logic level by logic 
level through the netlist.  The main concept of 
this algorithm is converting backtrace and 
propagation into bitwise logic operation so 
different search space can be explored at the 
same time.  This algorithm extends a previous 
work, bitwise parallel stuck-at fault test 
generation on CPU [9], to handle N-detect 
transition delay faults test generation on GPU. 
4.1 Signal Representation 
In our ATPG, a signal Y is represented by 
seven words of w bits: ,,,,,, 1010 bbdd YYYYYY  
and pY .  Each bit in a word represents an 
individual clone, which performs an independent 
search.  The meaning of the first four words is 
as follows. 
0
kY =1: signal Y is zero (good 0/faulty 0) for 
the kth clone. 
1
kY =1: signal Y is one (good 1/faulty 1) for 
the kth clone. 
d
kY =1: signal Y is d (good 1/faulty 0) for the 
kth clone. 
d
kY =1: signal Y is d  (good 0 /faulty 1) for 
the kth clone. 
For a given clone, the above four values are 
mutually exclusive so at most one of them equals 
one at a time.  If all of them are zero, Y is 
unknown ( xkY = 1). 
)( 01 ddx YYYYY +++=            (eq. 1) 
where ‘+’ represents a bitwise-OR operation and 
the bar means a bitwise-complement.  When Y 
is unknown, the other three words indicate 
whether Y is on one of two paths: the 
propagation path and the objective path.  The 
former is a path that faulty effect (d or d ) will 
potentially propagate to reach an output.  The 
latter is a path that the objective backtrace 
follows to reach an input.  For the kth clone, 
p
kY =1: signal Y is on a propagation backtrace 
path. 
0b
kY =1: signal Y is on an objective 0 backtrace 
path. 
1b
kY =1: signal Y is on an objective 1 backtrace 
path. 
Again, these three values are mutually 
exclusive so at most one of them equals one at a 
time. 
4.2 Word-level Test Generation for a Block 
Figure 1 shows the test generation flow of 
each block in our GPU-based test generation for 
N-detect transition delay faults.  Given a set of 
faults F, word size w, and the target number of 
detection N, each fault in F are handled by N 
clones.  For example, if w = 64, F = {f1, f2, f3, 
f4}, and N = 8, then the first eight clones 
generates patterns for f1, and the following eight 
clones generates patterns for f2, and so on.  For 
transition delay faults test generation, the initial 
objectives control fault sites to their fault-free 
values.  A backtrace is then performed from the 
fault site to the inputs.  After inputs are 
OBJCBBAA pddxb )(1 +=      (eq. 7a) 
OBJAOBJ b += 1           (eq. 7b) 
After inputs are assigned, the fault effect (d or 
d ) is propagated from a gate input to its gate 
output when two conditions are true: 1) at least 
one of the input is d or d , and 2) none of the gate 
input holds the controlling value.  For an 
example of an AND gate with inputs AB and 
output C, the following logic operations 
implement d-propagation. 
111 BAC =                      (eq. 8a) 
dddd BABABAC +++= 000      (eq. 8b) 
ddddd BABABAC ++= 11      (eq. 8c) 
ddddd BABABAC ++= 11       (eq. 8d) 
4.3 Block-Level Test Generation for a Kernel 
Figure 2 illustrates the block-level 
parallelism of t blocks generating test patterns 
simultaneously.  Each block handles a logic 
level and each thread in the block handles a gate.  
Each x represents a target fault.  The ith thread 
in the block is shown as thi in the figure.  For 
example in Figure 2, block 0 handles logic level 
4.  In block 0, th0 and th1 handle the upper and 
the lower gate in logic level 4, respectively.  
The number of threads needed in a block is equal 
to the maximum number of gates in a logic level.  
If the number of gates in a logic level is larger 
than the maximum allowed threads in a block, 
multiple blocks are assigned to the logic level.  
Each block has a copy of signals and performs 
test generation for different faults independently.  
As shown in Figure 2, block 0 is performing 
backtrace and block 1 is performing propagation 
on different faults.  All blocks are synchronized 
after all threads have finished. 
Two variables, current_level and 
current_status, are stored in the shared memory 
of each block.  current_level is an integer 
which represents the current logic level the block 
executes.  current_status indicates the four 
status of the block (namely initialize, backtrace, 
propagation, and finished).  current_level and 
current_status are updated by the last thread in a 
block.  The current_level is incremented every 
time when the block is propagating signals and is 
decremented when the block is backtracing 
signals.  The current_status is set to backtrace 
when block propagates to outputs and is set to 
propagation when block backtraces to inputs.  
If all clones have at least one d or d  at the 
outputs, current_status is set to finished. 
 
Figure 2.  Test generation block-level parallelism 
 The test generation kernel terminates when 
all blocks have finished or their execution time 
reached a user defined limit.  To fully utilize 
the computing resource on GPU, the runtime of 
each block is approximately the same.  This is 
accomplished by assigning faults within same or 
near logic levels to different blocks.  Take 
Figure 2 for example, target faults for all blocks 
belong to logic level 4 or logic level 5.   
4.4 Backtrack 
Suppose v GPU devices are available, all 
transition delay faults are statically partitioned 
into v fault lists.  Each device performs test 
generation and fault simulation for a fault list 
independently.  To balance the load between 
devices, faults within the same logic level are 
evenly distributed into different fault lists. 
5. Experimental Results 
To validate the proposed GPU-based ATPG, 
experiments are performed on large ISCAS’89, 
ITC’99, and IWLS’05 benchmark circuits.  
Results are compared with a commercial ATPG 
tool with multi CPU core support.  Table I 
shows the results of a commercial tool in terms 
of runtime, test length, and fault coverage for 
8-detect transition delay fault ATPG.  
Single-core, 4-core, and 8-core commercial 
ATPGs are used to generate test patterns.  The 
last row shows the averaged result normalized to 
that of the single-core commercial ATPG. 
In our test generation kernel, 64 blocks are 
VLSI Systems Defect Fault Tolerance, pp. 59-70, 
2006. 
[5] K. Peng, J. Thibodeau, M. Yilmaz, K. 
Chakrabarty, and M. Tehranipoor, “A Novel 
Hybrid Method for SDD Pattern Grading and 
Selection,” Proc. IEEE VLSI Test Symp., pp. 
45-50, 2010. 
[6] K. Gulati and S. P. Khatri, “Towards 
Acceleration of Fault Simulation using Graphics 
Processing Units,” Proc. IEEE Design 
Automation Conf., pp. 822-827, 2008. 
[7] M. Li and M. S. Hsiao, “FSimGP^2: An Efficient 
Fault Simulator with GPGPU,” Proc. IEEE Asian 
Test Symp., pp. 15-20, 2010. 
[8] H. Li, D. Xu, Y. H., K.-T. Cheng, and X. Li, 
“nGFSIM: A GPU-Based Fault Simulator for 
1-to-N Detection and Its Applications,” Proc. 
Int’l Test Conf., pp. 1-10, 2010. 
[9] C.-Y. Chang, “Compact Test Pattern Selection 
for Small Delay Defect,” M. S. thesis, Dept. 
Elect. Eng., National Taiwan Univ., 2011. 
[10] S. Mitra, E. Volkerink, E.J. McCluskey, and S. 
Eichenberger, “Delay Defect Screening using 
Process Monitor Structures,” Proc. IEEE VLSI 
Test Symp., pp. 43-52, 2004. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 2
bound information was used for transition fault diagnosis [Wang 
05][Mehta 09].  Upper bounds of fault size can be calculated 
from<?> the timing of passing patterns while lower bounds can 
be calculated from <?> the timing of failing patterns.  SDD 
diagnosis test generation has been proposed in [Guo 10].  
Diagnosis test generation may not be applicable in the production 
test environment.  Timing-aware diagnosis based on 
satisfiablility [Lu 07] is not applicable for large circuits.   
SLAT (Single Location at a Time) diagnosis was proposed by 
IBM and has been successfully demonstrated on industrial 
designs [Huisman 04] [Schuermyer 05].  Because it is very 
difficult to find a fault model for all defects, SLAT uses only 
those patterns whose outputs match that of one single stuck-at 
fault.  This technique first identifies SLAT patterns, followed by 
a minimum covering to find a small set of candidate faults that 
explain as many SLAT patterns as possible.  The original SLAT 
patterns were proposed for single stuck-at fault.  In this research, 
we extend the idea of SLAT to diagnose multiple SDD.    
<comment: related to our work ?> 
3. PROPOSED TECHNIQUE 
3.1 Overall Diagnosis Flow 
 
Figure 1 Overall Diagnosis Flow  
Figure 1 illustrates the overall flow of the proposed diagnosis 
technique. <Figure should follow text> There are four required 
input files to this technique: SDF file, test patterns, netlist, and 
test failures from ATE.  First of all, structure analysis is 
performed by backtracing from failing observed points (primary 
outputs or scan flip-flops) to obtain an initial candidate list.  
Transition fault simulation and timing analysis are then 
performed for each candidate in the initial candidate list to 
acquire simulation failures and candidate timing slacks.  <D? The 
details about structure analysis and timing analysis can be found 
in section 3.2.>  After that, timing-aware SLAT (TA-SLAT) 
patterns are identified <RW by comparing test failures and 
simulation failures with considering candidate timing slack.>  <D? 
The details about TA-SLAT identification are described in 
section 3.3.>  Minimum covering then selects minimum sets of 
candidates that explain all TA-SLAT patterns.  The details about 
minimum covering are described in section 3.4.  Finally, the 
ranking list of those candidate sets is reported as diagnosis result.   
3.2 Structure Analysis and Timing Analysis 
Structure analysis traces back from failing observed points to 
obtain an initial candidate list (ICL).  Candidates in ICL are gate 
I/O pins which are the potential locations of culprit faults.  The 
selection of structure analysis approaches is a tradeoff between 
correctness and runtime.  A possible aggressive approach takes 
the intersection of each backtrace cone as ICL, where a backtrace 
cone consists of fan in candidates traced back from a failing 
observed point.  The size of ICL is small whereas the actual 
locations of culprit faults may not be included in the presence of 
multiple defects.  A possible conservative approach takes the 
union of backtrace cones as ICL.  Although, this conservative 
approach copes with multiple defects, runtime of fault simulation 
and timing analysis is long.   
Our structure analysis technique copes with multiple defects 
by analyzing netlist structure and test failures of each failing <D? 
test> pattern.  The idea of our technique is similar to the Failing 
PO Partition Technique proposed in [Wang 03].  First of all, an 
undirected <failing?> observed point graph is constructed.  Each 
node in the graph is a failing observed point.  An edge between 
two <vertices?> nodes indicates that the intersection of their 
backtrace cones is not empty.  Disjoint sets on the graph are then 
identified by performing depth-first search.  Given a failing 
pattern Pi, a disjoint set DSj and observed failure point Ok, ICL is 
obtained as follows:  
⎪⎭
⎪⎬
⎫
⎪⎩
⎪⎨
⎧=
∈∈∀ kPOP&&ODSOji
CI
ikjk )(
, I  (1)  ⎭⎬
⎫
⎩⎨
⎧=
∀ ∀
UU
i j
jiIICL ,  (2) 
where OP(Pi) represents the set of failing observed points for 
failing pattern Pi.  Ck is a backtrace cone from observed point Ok.  
Ii,j is the intersection of cones from all OP(Pi) which belong to 
DSj.  ICL is obtained by taking union of all Ii,j for all patterns and 
all disjoint sets. 
Figure 2(a) shows an example circuit to demonstrate our 
structure analysis technique.  Suppose there are three culprit 
faults f1 f2 f3 (located at candidates F1, F2, and F3) in the circuit.  
TABLE I shows the test failures of these culprit faults.  Each ‘X’ 
in the table indicates a failure <RW> observed at the observed 
point (column) by applying the test pattern (row).  In this 
example, all six outputs: {O1~O6} in the circuit are failing 
observed points.  Figure 2(b) shows the corresponding observed 
point graph, which consists of six nodes <vertices>?: {O1~O6} 
and five edges {O1O2, O3O4, O4O5, O4O6, O5O6}.  Two disjoint 
sets DS1: {O1, O2} and DS2: {O3, O4, O5, O6} are identified in the 
graph.  TABLE II summarizes the results of our structure 
analysis.  Take the first row for example, <GRAMMER 
ERROR>there are two observed points, O1 and O2, both belong 
to DS1.  Therefore I1,1: {A, B, F1} for test pattern P1 is acquired 
by taking the intersection of backtrace cones: C1 and C2.  For the 
second row, observed points: O1 and O2 belong to DS1 whereas 
observed points: O3 and O4 belong to DS2.  Therefore I2,1: {A, B, 
F1} and I2,2: {C, D, F2} for test pattern P2 are <D?acquired by 
taking> of intersection of backtrace cones: {C1, C2} and {C3, C4} 
respectively.  The last row shows ICL of our structure analysis: 
{A, B, F1, C, D, F2, E, H, F3} which is much better than ICLs 
obtained by the other two approaches.  The aggressive approach 
 4
CTSF1,O1  which is 0.6 time unit (see TABLE III).  LBF1,P1  is 
smaller than UBF1,P1 which equal to test cycle time 1.0 time unit 
and there exist no TFSP observed points, so P1 is identified as a 
TA-SLAT pattern for candidate F1.  The third row <Dof the 
table> shows the other test failures of f1 with defect size 0.5 time 
unit.  Observed point O1 is a TPSF observed point, so UBF1,P1 is 
set to CTSF1,O1 which is 0.6 time unit.  Observed point O2 is a 
TFSF observed point, so LBF1,P1 is set to CTSF1,O2 which is 0.4 
time unit.  Because LBF1,P1  is smaller than UBF1,P1  and there 
exist no TFSP observed points, P1 is still identified as a TA-
SLAT pattern for candidate F1.   
<separate This example also demonstrates the problem that 
number of SLAT patterns decreases in the presence of SDD.  It 
could be observed that P1 is not a SLAT pattern for F1 if the 
defect size of culprit fault located on F1 is 0.5 time unit.  This 
problem leads to poor resolution for traditional SLAT diagnosis. 
<too long and too many notations> 
 
 
Figure 3 TA-SLAT Pattern Identification 
 
Figure 4 UpdateLB and UpdateUB 
TABLE IV.  TA-SLAT PATTERN IDENTIFICATION 
P1 Failures O1 O2 O3 O4 O5 O6
Simulation failures  of 
transition fault on candidate F1
X X     
Test failures 
(defect size = 0.8 time unit) X X     
Test failures 
(defect size = 0.5 time unit)  X     
TABLE V.  TA-SLAT PATTERN INDENTIFICATION FOR CANDIDATE E 
Pattern LB UB Comment 
P3 0.7 1.0 TFSF pattern 
P5 0.8 1.0 TFSF pattern 
P6 N/A 0.7 TPSF pattern 
P3, P5  0.8 0.7 LBE,P5 is larger than UBE 
P3  0.7 0.7 P3 is a TA-SLAT pattern 
  TABLE V demonstrates another example for TA-SLAT pattern 
identification.  Suppose that the simulation failures of candidate 
E perfectly match the test failures of two failing patterns: P3 and 
P5.  Then the first two rows of the table show that P3 and P5 are 
EQ LB\O(\S\do2(E\,P\O(\S\do2(5))))which is 0.8 time unit.  Suppose 
that there is an additional pattern P6 which is a TPSF pattern for 
candidate E.  Then the third row of the table shows that UBE is 
EQ UB\O(\S\do2(E\,P\O(\S\do2(6)))) which is 0.7 time unit.  The 
EQ LB\O(\S\do2(E\,P\O(\S\do2(5)))) is larger than UBE, so P5 is 
removed from TA_SLAT_PList for candidate E (see Figure 3 
lines 24 to 27).  Therefore TA_SLAT_PList contains only P3 at 
the end of TA-SLAT pattern identification for candidate E.  The 
reason why P5 is not identified as a TA-SLAT pattern for 
candidate E is that test failure observed at a TFSF observed point 
EQ CTS\O(\S\do2(E\,O\O(\S\do2(k))))is smaller than UBE could 
actually caused by the culprit faults located on other candidates.  
It could be observed that the test failures of P5 (see TABLE I and 
Figure 2) are actually result from both culprit faults located on 
candidate F2 and candidate F3.  This technique reduces the 
number of TA-SLAT pattern explained by equivalent candidate, 
so the diagnosis resolution can be improved. 
3.2 Minimum Covering 
Minimum-covering, selects a minimum set of candidates that 
explains <DEF> all TA-SLAT patterns.  TABLE VI shows an 
example of a covering table used by minimum covering.  Every 
row in the covering table represents a candidate and every 
column is a TA-SLAT pattern.  An ‘S’ indicates that the 
candidate covers the corresponding TA-SLAT pattern.  A greedy 
covering algorithm is used to find a minimum set of candidates 
that covers all TA-SLAT patterns.  This algorithm first selects 
essential candidates that have at least one unique S in a column.  
The algorithm then selects candidate that covers the largest 
number of TA-SLAT patterns.  When multiple candidates cover 
the same number of TA-SLAT patterns, the algorithm first 
selects the candidate with the smallest number of TPSF patterns.  
In TABLE VI, the essential candidates( F1, F2 ) are fist selected.  
F3 rather than E is then selected to cover P3, because F3 has no 
TPSF pattern.  Therefore the result of minimum covering is {F1, 
F2, F3}.   
TABLE VI.  SLAT COVERING TABLE 
Candidate P1 P3 P4 TPSF pattern 
F1 S   None 
F2   S None 
F3  S  None 
E    S   P6 
   UpdateLB (d, Pi, Ok) {  
5: if (CTSd,Ok  > LBd,Pi ) 
EQ 
EQ 
CTS\O(\S\do2(d\,O\O(\S\do2(k)
)));
  UpdateUB (d, Pi, Ok) {  
1: if (CTSd,Ok  < UBd,Pi ) 
EQ 
EQ 
CTS\O(\S\do2(d\,O\O(\S\do2(k
))));
   TA-SLAT Identification (d, TPSF_PList, TFSF_PList) {  
1: TA_SLAT_PList = EMPTY; 
2: UBd = test_cycle_time; 
3: LBd = 0; 
4: foreach pattern Pi in TPSF_PList { 
5:  UBd, Pi = test_cycle_time; 
6:  foreach observed point Ok of Pi { 
EQ UB\O(\S\do2(d\,P\O(\S\do2(i)))) and UBd by the
EQ CTS\O(\S\do2(d\,O\O(\S\do2(k)))) 
8:  } 
9: } 
10: foreach pattern Pi in TFSF_PList { 
11:  LBd, Pi = 0; 
12:  TFSP_flag= false; 
13:  foreach observed point Ok of Pi { 
14:   if (Ok is TPSF) 
EQ UB\O(\S\do2(d\,P\O(\S\do2(i)))) and UBd by the
EQ CTS\O(\S\do2(d\,O\O(\S\do2(k)))) 
16:   if (Ok is TFSF) 
EQ LB\O(\S\do2(d\,P\O(\S\do2(i)))) and LBd by the
EQ CTS\O(\S\do2(d\,O\O(\S\do2(k)))) 
18:   if (Ok is TFSP) 
19:    TFSP_flag = true; 
20:  } 
EQ UB\O(\S\do2(d\,P\O(\S\do2(i))))) 
22:   push Pi into TA_SLAT_PList; 
23: } 
24: foreach Pi in TA_SLAT_PList { 
EQ LB\O(\S\do2(d\,P\O(\S\do2(i))))> UBd) 
26:   remove Pi from TA_SLAT_PList; 
27: } 
28: retrun TA_SLAT_PList; 
29:} 
 6
TABLE XI summarizes the diagnosis results of each class.  
There are ten dies belong to Class I and the average resolution is 
one, which means only the top rank candidate meets the 
constraints of Class I (i.e. all failing patterns are TA-SLAT 
patterns and there exist no TPSF patterns).  One of the ten dies in 
Class I has been verified by physical failure analysis (PFA).  
However, the PFA photo is not available for publication. 
5. SUMMARY 
This paper presents a novel diagnosis algorithm for small 
delay defects diagnosis.  Both at-speed test sets and faster-than-
at-speed test sets have been applied.  The proposed timing-aware 
diagnosis technique uses timing upper and lower bound 
information to improve the diagnosis resolution.  This technique 
extends the SLAT technique to diagnose multiple SDD.  Test 
results of both AS/FS test sets are combined to further improve 
the diagnosis resolution.  Experimental results on five advanced 
industrial designs show the accuracy of the proposed technique.  
One of the failing dies has been verified by physical failure 
analysis.   
REFERENCES  <RE-CHECK> 
[Ahmed 06] N. Ahmed, M. Tehranipoor, V. Jayaram, “A Novel Framework for 
Faster-than-at-Speed Delay Test Considering IR-drop Effects”, Int’l Conf. on 
Computer-Aided Design, 2006. <italic, miss page #> 
[Aikyo 07] T. Aikyo, H. Takahashi, Y. Higami, J. Otsu, K. Ono, and Y. 
Takamatsu, “Timing-Aware Diagnosis for Small Delay Defects,” Proc. Defect 
and Fault Tolerance in VLSI Systems Symp., pp.223-231, 2007. 
[Cadence 11] Cadence Inc. “Encounter True Time ATPG,” 2011 [Online] 
http://www.cadence.com/products/ld/true_timetest/pages/default.aspx 
[Girard 92] P. Girard, C. Landrault, and S. Pravossoudovitch, “Delay-Fault 
Diagnosis by Critical Path Tracing,” IEEE Design & Test of Computer, Vol. 9, 
No. 4, pp. 27-32, 1992. 
[Goel 09] S. K. Goel, N. Devta-Prasanna, and R. P. Turakhia, “Effective and 
Efficient Test Pattern Generation for Small Delay Defect,” Proc. IEEE VLSI 
Test Symp., pp.111-116, 2009. 
[Gosh 98] J. Ghosh and N. Touba, “A systematic Approach for Diagnosing 
Multiple Delay Faults,” Proc. IEEE Int’l Symp. Defect and Fault Tolerance in 
VLSI Syst., pp. 211-216, 1998. 
[Guo 10] R. Guo, W. T. Cheng, T. Kobayashi, and K. H. Tsai, “Diagnostic Test 
Generation for Small Delay Defect Diagnosis,” Proc. VLSI Design Automation 
and Test Symp., pp.224-227, 2010. 
[Gupta 04] P. Gupta and M. S. Hsiao, “ALAPTF: A New Transition Fault Model 
and the ATPG Algorithm,” Proc. IEEE Int’l Test Conf., pp.1053-1060, 2004. 
[Hamada 06] S. Hamada, T. Maeda, A. Takatori, Y. Noduyama, and Y. Sato, 
“Recognition of Sensitized Longest Paths in Transition Delay Test,” Proc. 
IEEE Int’l Test Conf., paper 11.1, 2006. 
[Hsu 98] Y. C. Hsu and S. K. Gupta, “A New Path-Oriented Effect-Cause 
Methodology to Diagnose Delay failures,” Proc. IEEE Int’l Test Conf.  pp. 
758-767, 1998. 
[Huisman 04] L. M. Huisman, “Diagnosing arbitrary defects in logic designs 
using single location at a time (SLAT),” IEEE Trans. on Computer-aided 
Design of Integrated Circuits and Syst., Vol. 23, No. 1, pp. 91-101, 2004. 
[Kajihara 06] S. Kajihara, S. Morishima, A. Takuma, X. Wen, T. Maeda, 
S.Hamada, and  Y. Sato, “A Framework of High-Quality Transition Fault 
ATPG for Scan Circuits,” Proc. IEEE Int’l Test Conf., paper 2.1, 2006. 
[Kruseman 04] B. Kruseman, A. K. Majhi, G. Gronthoud, and S. Eichenberger, 
“On  Hazard-free Patterns for Fine-delay Fault Testing,” Proc. IEEE Int’l Test 
Conf., pp.213–222, 2004. 
[Lee 06] H. Lee, S. Natarajan, S. Patil, and I. Pomeranz, “Selecting High-quality 
Delay Tests for Manufacturing Test and Debug,” Proc. IEEE Int. Symp. Defect 
Fault Tolerance Very Large Scale Integr. Syst., pp.59-70, 2006. 
[Lu 07] S. Y. Lu, M. T. Hsieh, and J. J. Liou, “An Efficient SAT-based Path 
Delay Fault ATPG with an Unified Sensitization Model,” Proc. IEEE Int’l Test 
Conf., 2007. 
[Mehta 09] V. J. Mehta, M. Marek-Sadowska, K. H. Tsai and Janusz Rajski 
“Timing-Aware Multiple-Delay-Fault Diagnosis,” IEEE Trans. on Computer-
aided Design, Vol. 28, No.2, pp245-258, 2009. 
[Mitra 04] S. Mitra, E. Volkerink, E. J. McCluskey, and S. Eichenberger, “Delay 
Defect Screening using Process Monitor Structures,” Proc. IEEE VLSI Test 
Symp., pp.43-52, 2004. 
[Pant 99] P. Pant and A. Chatterjee, “Efficient Diagnosis of Path Delay Faults in 
Digital Logic Circuits,” Proc. IEEE Int’l Conf. on Computer-Aided Design, pp. 
471-476, 1999. 
[Peng 10] K. Peng, J. Thibodeau, M. Yilmaz, K. Chakrabarty, M. Tehranipoor, 
“A Novel Hybrid Method for SDD Pattern Grading and Selection,“ Proc. IEEE 
VLSI Test Symp., pp.45-50, 2010. 
[Qiu 04] W. Qiu, J. Wang, D. M. H. Walker, D. Reddy, X. Lu, Z. Li, W. Shi, and 
H. Balachandran, “K Longest Paths Per Gate (KLPG) Test Generation for 
Scan-Based Sequential Circuits,” Proc. IEEE Int’l Test Conf., pp.223-231,2004. 
[Sato 05 a] Y. Sato, S. Hamada, T. Maeda, A. Takatori, and S. Kajihara, 
“Evaluation of the Statistical Delay Quality Model,” Proc. IEEE Asian and 
South Pacific Design Automation Conf., pp.305-310, 2005. 
[Sato 05 b] Y. Sato, S. Hamada, T. Maeda, A. Takatori, and S. Kajihara, 
“Invisible Delay Quality – SDQM Model Lights Up What Could Not Be 
Seen,” Proc. IEEE Int’l Test Conf., pp.1202-1210, 2005. 
[Schuermyer 05] C. Schuermyer, K. Cota, R. Madge, and B. Benware, 
“Identification of Systematic Yield Limiters in Complex ASICS Through 
Volume Structural Test Fail Data Visualization and Analysis,” Proc. Int’l Test 
Conf., paper 7.1, 2005. 
[Shao 02] Y. Shao, I. Pormeranz, and S. M. Reddy, ”On Generating High Quality 
Tests for Transition Faults,” Proc. Asian Test Symp., pp.1-8, 2002. 
[Synopsys 11] Synopsys Inc., “TetraMAX DSMTest for Small Delay 
Defect Testing,” 2011 [Online] Available: http://www.synopsys.com/ 
Tools/Implementation/RTLSynthesis/Pages/TetraMAXATPG.aspx 
[Turakhia 07] R. Turakhia, W. R. Daasch, M. Ward, and J. van Slyke, “Silicon 
Evaluation  of Longest Path Avoidance Testing for Small Delay Defects,” Proc. 
IEEE Int’l Test Conf., pp.1-10, 2007. 
[Venkataraman 01] S. Venkataraman and S. B. Drummonds, “Poirot: 
Applications of a Logic Fault Diagnosis Tool,” IEEE Design & Test Computer, 
Vol. 18, No. 1, pp. 19-30, 2001. 
[Wang 03] Z. Wang, K. H. Tsai, M. Marek-Sadowska, J. Rajski, “An Efficient 
and Effective Methodology on the Multiple Fault Diagnosis”, IEEE Int’l Test 
Conf., paper 12.3, 2003. 
[Wang 05] Z. Wang, M. Marek-Sadowska, K. H. Tsai, and J. Rajski, “Delay-fault 
diagnosis using timing information,” IEEE Trans. on Computer-aided Design, 
Vol. 24, No.9, pp1315-1325, 2005.   
TABLE IX.  COMPARISON OF OVERALL FLOW 
Traditional (TA) Ours (AS+FS) cases 
# of 
detection 
rank-1  
hit rate 
success rate average 
resolution
# of 
detection
rank-1 hit 
rate 
success rate average 
resolution 
1-9 0 0/9 0% N/A 9 6/9 77%(7/9) 1.71 
10-20 6 1/11    9%(1/11) 2 11 10/11 91%(10/11) 1.90 
21-25 4 1/10 20%(1/5) 2 5 8/10 100%(5/5) 3.80 
Total 10 2/30    8%(2/25) 2 25 24/30 88%(22/25) 2.27 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
