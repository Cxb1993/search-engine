 I
摘 要 
本研究計畫原規畫三年期的計畫，其主要是發展完整的數位典藏影像的切割
(segmentation)、壓縮(compression)與檢索(retrieval)技術，由於本計畫申請通過一年
期的研究，因此，以第一年研究計畫「將建立一套數位典藏器物植基於紋理特徵
(texture feature)之切割(segmentation)技術」做為本專題研究計畫成果報告內容。 
本研究主要以器物典藏影像進行物件切割，數位典藏產生了龐大的器物典藏
影像，為輔助使用者能更快速且方便的進行檢索，避免因影像背景而影響檢索的
結果，需先進行物件切割處理，使其達到高精確度的影像檢索。 
本研究器物典藏影像所進行的影像切割，首先透過擷取影像的coarseness, 
contrast, directionality, and gradient等 4個紋理特徵，其中包括 PCA(Principal 
Component Analysis)的技術以找出特徵值之最適權重，再以Otsu方法的二值化處
理，再經由影像的形態學處理(Morphological processing)之閉合運算(Object Filter 
for closing operation)，最後將物件過濾(Object Filter)後之物件進行邊界抽取
(Boundary Extraction)，以及使用分水嶺 (Watershed Segmentation)分割技術，以達
到器物與影像背景分離的目的。主要能克服影像紋飾及顏色分佈不均與不規則的
問題，即使影像背景的顏色與物件邊緣的顏色相似也能順利達到切割的目的；除
此之外，不論是簡單或複雜的紋飾也能順利進行切割，還能克服了背影漸層與器
物陰影與背景干擾等問題。 
本研究以通貨、瓷器與琺瑯器影像進行實驗，其中包含了不同時代、不同外
形與不同顏色的器物影像，進行器物影像切割(image segmentation)的實驗，並與影
像切割方法Level Set等方法進行比較，本研究結果能夠順利將影像背景與器物物件
分離，並且快速地切割出與原始影像外形相似的物件。為了能更精確表示器物影
像切割的正確性，本研究也採用了三個效能標準(performance criteria) [47] 進行評
估，以證明本研究之物件切割方法能快速正確且精確地切割出該器物物件。並得
到器物影像完整與精確的結果，進而可做為判斷該器物朝代的依據，期望未來能
發展成器物影像之影像檢索技術，以方便使用者研究並發揚典藏文化。 
關鍵詞：數位典藏、器物主題、紋理特徵、主成份分析、影像切割、效能標準 
 III
目    次 
第一章 緒論 .................................................................................................................. 1 
1.1 研究背景 .......................................................................................................... 1 
1.2 文獻回顧 .......................................................................................................... 5 
1.3 研究目的 .......................................................................................................... 6 
1.4 研究架構 .......................................................................................................... 7 
第二章 相關研究探討 .................................................................................................. 8 
2.1 前言 .................................................................................................................. 8 
2.2 邊緣基礎切割(edge-based segmentation) ....................................................... 8 
2.2.1 Sobel 邊緣偵測 ...................................................................................... 9 
2.2.2 Canny 邊緣偵測..................................................................................... 9 
2.2.3 Laplacian 邊緣偵測 ............................................................................... 9 
2.2.4 門檻值之技術(threshold-based techniques) ........................................ 10 
2.3 區域基礎切割(region-based segmentation) .................................................. 10 
2.3.1 自動輪廓模型(Active Contour Model, ACM) .................................... 10 
2.3.2 Graph cuts based active contours(GCBAC)..........................................11 
2.3.3 幾何式主動輪廓線模型(Level Set) .....................................................11 
2.3.4 CCTA(connected coherence tree algorithm).........................................11 
2.3.5 P.W. Huang’s Method .......................................................................... 12 
2.3.6 紋理切割(texture segmentation) .......................................................... 12 
第三章 器物典藏之影像切割 .................................................................................... 14 
3.1 前言 ................................................................................................................ 14 
3.2 紋理特徵(Texture Feature) ............................................................................ 15 
3.2.1 梯度(Gradient) ..................................................................................... 15 
3.2.2 粗糙度(Coarseness) ............................................................................. 16 
3.2.3 對比度(Contrast).................................................................................. 17 
3.2.4 方向度(Directionality) ......................................................................... 18 
3.3 型態學處理(Morphological Processing) ....................................................... 19 
3.3.1 特徵取出(Feature Extraction).............................................................. 19 
3.3.2 二值化處理(Binary Processing) .......................................................... 22 
3.3.3 物件過濾(Object Filter) ....................................................................... 23 
3.4 切割(Segmentation) ....................................................................................... 25 
3.4.1 邊界抽取(Boundary Extraction) .......................................................... 25 
3.4.2 分水嶺分割(Watershed Segmentation) ............................................... 26 
第四章 實驗結果與討論 ............................................................................................ 27 
4.1 前言 ................................................................................................................ 27 
 V
圖 目 錄 
圖(1) 器物主題典藏類別 ............................................................................................... 1 
圖(2) 通貨影像 ............................................................................................................... 2 
圖(3) 瓷器影像 ............................................................................................................... 3 
圖(4) 琺瑯器影像 ........................................................................................................... 3 
圖(5) 器物呈現架構圖 ................................................................................................... 4 
圖(6) 故宮器物典藏資料檢索 ....................................................................................... 5 
圖(7) 二值影像與自然影像之邊緣 ............................................................................... 8 
圖(8) 瓷器影像 ............................................................................................................. 14 
圖(9) 梯度影像 ............................................................................................................. 16 
圖(10) 原始粗糙度影像 ............................................................................................... 17 
圖(11) 經由門檻值過濾後............................................................................................ 17 
圖(12) 對比度影像 ....................................................................................................... 18 
圖(13) 方向度影像 ....................................................................................................... 19 
圖(14) 各項特徵的正規化後之影像 ........................................................................... 20 
圖(15) PCA 投影影像之比較........................................................................................ 21 
圖(16) 經 PCA 投影之影像.......................................................................................... 22 
圖(17) 二值化處理後之影像 ....................................................................................... 23 
圖(18) 膨脹影像之示意圖 ........................................................................................... 24 
圖(19) 膨脹運算的瓷器影像 ....................................................................................... 24 
圖(20) 侵蝕運算的瓷器影像 ....................................................................................... 25 
圖(21) chain code 方向圖 ............................................................................................. 26 
圖(22) 輪廓切割結果 ................................................................................................... 26 
圖(23) 物件輪廓示意圖 ............................................................................................... 26 
圖(24) watershed segmentation...................................................................................... 26 
圖(25) 第一類原始瓷器實驗影像 ............................................................................... 28 
圖(26) 第一類影像之切割結果比較 ........................................................................... 29 
圖(27) 第二類原始瓷器實驗影像 ............................................................................... 31 
圖(28) 第二類影像之切割結果比較 ........................................................................... 32 
圖(29) 第一類通貨實驗影像及切割之結果 ............................................................... 35 
圖(30) 第二類通貨實驗影像及切割之結果 ............................................................... 35 
圖(31) 第一類瓷器實驗影像及切割之結果 ............................................................... 36 
圖(32) 第二類瓷器實驗影像及切割之結果 ............................................................... 37 
圖(33) 第一類琺瑯器實驗影像及切割之結果 ........................................................... 38 
圖(34) 第二類琺瑯器實驗影像及切割之結果 ........................................................... 39 
圖(35) 物件背景與輪廓之示意圖 ............................................................................... 41 
 1
第一章 緒論 
1.1 研究背景 
資訊科技的進步與人類生活型態的改變，文化資產不知不覺中逐漸消失，其
中文化資產包含古蹟、古物、民族藝術、民俗文物、自然文物景觀等。近年來，
人們意識到文化資產的重要，為了讓後代子孫可以體會到祖先們的歷史背景、生
活習慣的演進，使老祖宗的智慧能繼續被保存、累積與傳承 [1]，於是於西元 2002
年成立了數位典藏國家型科技計畫 [1]，期望能以更多元的方式呈現、有依據的保
存以及傳承舊有的文化資產，以促進文化的普及，使資訊科技與人文資產結合，
達到經濟與產業的共同發展。 
除了臺灣的數位典藏國家型科技計畫外，世界各國亦紛紛投入保有歷史文化
資產的計劃，例如：聯合國的世界記憶計畫(Memory of the Word)、美國的美國記
憶計畫(American memory)等，其目標皆為保護具有文化意義的文獻、檔案…等，
並喚起全世界的人們能攜手保護珍貴的歷史文化資料。 
「數位化」為數位典藏的首要工作，藉由文物資產數位化的轉換，已達到永
久保存之目的。目前「數位典藏國家型科技計畫 [2]」，是以現有的「數位博物館」、
「國家典藏數位化」及「國際數位圖書館合作研究」三個計畫為基礎，並依據國
家整體發展，重新規劃整合而成。 
數位典藏的領域很廣，其中包含了動物、植物、地質、人類學、檔案、器物、
書畫、地圖與遙測影像、金石拓片、善本古籍、考古、新聞、漢籍全文、語言、
影音、建築共十六項主題類型 [3]。其中一項重要的主題為器物典藏如圖(1)所示，
器物代表人類歷史的演進過程，其中蘊藏著不同的朝代、文化、經驗、知識和技
術之演進。 
 
    
玉石器 陶瓷器 銅
器、金
屬器 
琺瑯器
與玻璃
器 
木竹漆
器 
骨牙角
蚌器 
通貨 牙骨 編織 
圖(1) 器物主題典藏類別 [3] 
器物主題為人類日常生活中所用之物品，從最早發現人類遺跡的時間點開
始，便記載著人類的生活方式、地點、風俗以及生活習慣。不同的生活方式與地
點使用不同的貨幣進行交易，例如：早期海島居民，使用貝類作為當時的貨幣，
 3
貼、鏤雕為主，但到了元、明、清時代則以彩繪的方式做為主要的紋飾，如圖(3)
所示。 
  
圖(3) 瓷器影像 [4] 
琺瑯器為藝術之精萃，以表層來說不僅包含了玉的溫潤、珠寶的光輝還包含
了瓷的細膩。主要係由冶金家、銅匠、鑲嵌的知識、玻璃質鎔鍊技術、畫家、雕
刻家等，集合各類賢者技術結合而成 [10]，其影像如圖(4)所示。南宋(12 世紀)時
期，琺瑯技術向東傳入 [11]，導致明朝形制多仿古代銅器，例如百环尊及貫耳觚
等，到了清朝則演變為日常生活用品。琺瑯器之美在於各種美觀的器物外形及表
面的華麗顏色，再加上特別的圖案及鍍金等處理。依據琺瑯器裝飾性質與製作過
程之不同可區分為三類，分別為掐絲琺瑯器、內填琺瑯器、畫琺瑯器。景泰藍-掐
絲琺瑯器具有獨特的工藝技術價值 [10]。 
   
圖(4) 琺瑯器影像 [12] 
綜合以上所述，無論貨幣、瓷器或琺瑯器，從早期使用的海貝到現今的紙幣
與硬幣、從簡單的紋飾到劃、刻、貼、印花等，色彩豐富之工藝技術豐富了瓷器
藝術；在琺瑯器上，從基本的瓶、罐、杯等簡單外形，發展到碗、盤、爐、瓶、
燭臺、筆筒、煙壺等十餘種。換句話說，器物隨著時代的演進與工藝技術的進步，
促使人類生活方式的改變，但各種器物的本質是不變的，其主要是皆由簡單的紋
飾、顏色及外形，如圖(5)所示，隨著當時的風俗習慣、文化背景與工藝技術的進
步而鑄冶出有意義且豐富的紋飾，使用豐富華麗的顏色提高人們生活的審美觀，
鑄造各種外形的生活用品以方便人們使用。 
其中，器物上的紋飾係指器物表面的裝飾或圖案，它不僅代表了圖案的內容，
更記錄了當時工藝技術的演進與文化特色。紋飾的演進，從早期以簡單幾何圖形、
動、植物紋飾，到了清代已達到了「圖必有意，意必吉祥」的境界；技巧方面，
宋代以前的器物紋飾多以刻劃、模印、堆貼、鏤雕為主，但到了元、明、清時代
則以彩繪的方式作為其主要之紋飾。 
 5
 
圖(6) 故宮器物典藏資料檢索 
在博物館的古文物檢索中，從簡易的關鍵字查詢中，使用該文物之名稱或類
型進行檢索，到進階的關鍵字檢索，能讓使用者輸入更多與該文物相關的關鍵字
進行檢索；Cawkell [15] 指出「下一個世紀的影像資料庫，查詢方式將併同查詢用
語與影像瀏覽，這使得原來用在建立、控制、更新與學習使用索引典的時間與人
力花費將可望大幅減少。」；加入語意(semantic)的檢索進行輔助，能讓使用者與
資料庫之間進行良好的溝通，知識本體(ontology)的智慧型代理人(intelligent agent)
能根據語意進行附加註解影像的檢索 [16]；此外「一張圖片勝過千言萬語」，於
是若能以關鍵字的方式再搭配影像檢索以輔助使用者描述欲查詢之影像，將能更
精確且快速的達到使用者之需求。 
綜合以上所述，為了能妥善的保存與分享豐富的典藏資源，以提供學術、商
業、產業、教育、出版等運用，並促進文化發揚、推展至全球，使資訊與人文結
合達到經濟與產業的發展，以陸續提出多種檢索方式，供使用者進行研究、傳承
並發揚至世界各地。而提供影像檢索的首要工作須藉由資訊科技-影像切割技術，
將瓷器物件與該影像背景分離，達到物件切割的目的。其中紋飾除了能表現出各
朝代的文化特色外，亦能作為該物件歷史演進之過程。因此，本研究將利用器物
的紋飾作為影像切割的依據，並透過影像處理技術中的紋理切割技術取得該器物
的外型物件與紋飾特徵，達到物件紋理切割的目的，以方便未來進行檢索的研究。 
1.2 文獻回顧 
近十幾年來，影像切割技術為非常重要之議題 [17-35]，其目的在於區隔出使
用者所感興趣的區域。針對器物典藏影像而言，取出器物物件達到物件與影像背
景分離為目的。 
環視過去自動影像切割之方法，並依照影像像素的特性整理為三大類，分別
為： (1)不連續性 (discontinuity)、 (2)相似性 (similarity) 以及 (3)混合性 (hybrid 
techniques)作為區分的標準；不連續性係指像素強度值的劇烈改變，例如：基於邊
 7
不如直接讓使用者瀏覽影像還來得清楚。」，認為利用器物影像做為輔助查詢器
物的檢索，將可提供數位典藏更多元且更完整之器物檢索。依時代的演進及工藝
技術的進步，發展出不同表面紋飾變化的器物也鑄冶出更符合現今人們所需之器
物。為能維護現有文化以連接過去並塑造未來，透過有依據的保存、累積與傳承
文化，並讓後代子孫能從中體驗、學習與研究的重要性。為方便使用者進行體驗、
學習與研究須仰賴精確之影像檢索技術。 
然而器物典藏影像不適用於一般的影像檢索(image retrieval)，一般的影像檢索
係以整張影像擷取特徵再進行檢索。為能精確檢索出使用者欲查詢之器物，須先
使用影像處理-影像物件切割技術，精確的將影像背景與物件分離，以防止影像背
景干擾檢索的結果、並提升影像檢索率，故本研究主要著重於器物影像之物件切
割。 
由於本研究所研究及探討的問題與紋理有關，在影像切割的處理上將使用混
合型(hybrid)的影像切割技術，其中整合了基於邊緣的方法(edge-based methods)、
門檻值之技術 (threshold-based techniques)與區域為基礎的方法 (region-based 
methods)。因此，在特徵變數方面將萃取影像的梯度(Gradient)及紋理資訊(texture)
作為特徵，達到強化邊緣與紋理特徵萃取的目的，並透過 Principal Component 
Analysis(PCA)與 Otsu’s [38] 群聚分類方式來完成紋理影像切割。在未來的研究
上，期望針對已切割之器物物件進行特徵擷取並進行特徵比對達到影像檢索的目
的。 
1.4 研究架構 
本研究架構依序為緒論、相關研究探討、器物典藏影像之物件切割、實驗結
果與討論以及結果與未來展望等五個章節。 
第一章緒論中，簡述目前數位典藏國家型科技計畫的重要與內容以及目前典
藏的主要分類、影像切割技術之文獻回顧和本研究的架構。於第二章相關研究討
論中，將針對幾種著名的影像切割方法和近期的影像切割技術進行探討。第三章
為器物典藏影像之物件切割，以瓷器影像為例進行研究方法之詳細說明，並於第
四章實驗結果與討論中，詳細說明實驗的結果並加以分析與評估。實驗結果包含
視覺上的實驗結果呈現、時間花費之比較與分析、數據化的效能評估比較分析，
最後於第五章中歸納整理本研究所得結論與未來相關研究的發展。 
 9
[20] 係採一次微分的觀點進行邊緣強化、Laplacian 邊緣檢測法 [20] 也是一個常
用來作邊緣檢測的工具，係採二次微分的觀念來進行邊緣的加強化、Sobel 邊緣檢
測法 [20]，具有強化邊緣和平滑邊緣之優點。由於邊緣檢測法會取得邊緣(edge)
周邊的梯度值(gradient)，因此完成邊緣檢測與邊緣增強後，一般會透過細線化
(Thining)進行處理，取得單一像素寬度之邊緣，但所取得的邊緣往往會產生位移偏
差，亦或是不如預期中物件與背景之間的邊緣。 
2.2.1 Sobel 邊緣偵測 
Sobel 邊緣偵測 [20] 的原理是從影像中找出顏色變化最大的像素。Sobel 邊緣
偵測是透過二維空間的分析，運用遮罩來偵測顏色變化最大的像素。並針對影像
中的每一像素偵測其像素的變化量，主要是利用兩個大小為 33× 的遮罩，一個是
測量水平方向像素的變化量，另一個則是測量垂直方向像素的變化量。在得到兩
個變化量後，計算出該像素整體的變化量，當變化量越大時即表示該像素越接近
於邊緣部份。 
2.2.2 Canny 邊緣偵測 
Canny 邊緣偵測 [21-22] 為最具代表性的邊緣偵測方法，也是近年來學術上最
常使用的邊緣偵測方法之ㄧ。Canny 邊緣偵測是藉由偵測影像色階而產生的邊緣偵
測器，其主要是希望達到三個目標：(1) 良好的偵測能力(good detection)，意圖找
出所有的邊緣，即增強邊緣值、降低不必要的雜訊；(2)良好的定位能力(good 
localization)，期望能出找與邊緣較接近的位置，其差異越小表示其定位能力越好；
(3) 低重複感應，即在多邊緣中確認出一個較符合物理現象且正確的邊緣線段。 
Canny 邊緣偵測方法分為四個部份，(1) 高斯濾波(gaussian filter)、(2) 梯度運
算(gradient computation)、(3) 非最大值刪除(non-maximum suppression)及(4) 遲滯
性門檻(hysteresis thresholding)。Canny 輪廓偵測法為了改善上述邊緣偵測方法所造
成的問題，將原始影像透過簡單的高斯函數進行邊緣增強的處理，以有效過濾影
像中的雜訊，其次再利用 Sobel 邊緣偵測方法取得該影像的梯度大小和梯度方向，
並依梯度方向對梯度(gradient)大小進行非最大值刪除(non-maximum suppression)
之細線化(thining)處理，最後以遲滯性門檻(hysteresis thresholding)定義出合理的門
檻值，刪除不正確的邊緣，以取得適當且正確的邊緣。 
2.2.3 Laplacian 邊緣偵測 
Laplacian 邊緣偵測法 [20] 也是常見的邊緣檢測工具之一，Laplacian 邊緣偵
測法係採二次微分的觀念進行邊緣增強。數學上的意義為： 
2
2
2
2
2 ),(),(),(
y
yxf
x
yxfyxf ∂
∂+∂
∂=∇               (1) 
 11
( )∫ ++= 10 intint ))(())(())(( dsscEscEscEE imagesnack    (4) 
其中，內力 intE 如下式所示，為輪廓對參數 s 之一次及二次微分， 
2
)()()()( 22
int
scsscs
E sss
βα +=       (5) 
而影像力或外力 imageE 可以表示為 
termtermedgeedgelinelineimage EwEwEwE ++=    (6) 
ACM 在進行物件切割時，必須先設定物件的初始輪廓，其次透過輪廓形變不
斷的修正，最後將輪廓線推向實際邊緣處收斂。ACM (active contour model)影像切
割能力，雖然廣受學者的肯定，但必須事先於影像中訂定物件的初始輪廓，以及
耗費較長的執行時間，為其重大缺陷。因此，初始輪廓的訂定，對於是否能正確
切割物件與執行時間的長短皆有重大之影響。 
就影像切割而言，使用主動輪廓模式除了可獲得完整、連續的物件輪廓外，
亦可藉由輪廓線直接套取物件，不像傳統邊緣偵測 [38] 所獲得之邊緣常破碎不連
續，不易於後續處理應用。 
2.3.2 Graph cuts based active contours(GCBAC) 
GCBAC 的方法主要是組合自動輪廓反覆變形的概念及圖像切割的理想工
具。不同於傳統自動輪廓的地方在於，該方法使用圖形切割反覆變型的輪廓和切
割邊緣權重的總合做為花費函數的定義。此演算法會顯示收斂的情況，並於最後
得到理想的輪廓 [34]。 
2.3.3 幾何式主動輪廓線模型(Level Set) 
幾何式主動輪廓線模型，又名等位函數法(Level Set method)[30, 42-43]，其中
Osher and Sethian [42] 是用來研究並追蹤物體的幾何變化；Chen and Vese [43] 提
出了一種叫做『Active Contours Without Edges』的方法，利用輪廓曲面定義出一個
以能量(energy)表示的泛函(functional)能量(energy)乃是利用初始的輪廓曲面之影
像資訊，再運用此泛函找出能量的最小值，定義出最佳之輪廓。 
2.3.4 CCTA(connected coherence tree algorithm) 
CCTA(connected coherence tree algorithm)不需事前資訊即可達到影像切割的
目的。其目標為找出鄰近區域一致性的標準進行切割，換言之，該方法可透過空
間大小與不同強度大小的調整來改善物件切割之效。CCTA 通常能夠達到單一影像
 13
歸模型(autoregressive model)，馬可夫隨機域模型(Markov random field model；MRF)
等。 
紋理特徵係一個描述影像區域的方法，它能表現出人類視覺上一些可供識別
的資訊。Rishi Jobanputra 和 David A. Clausi [42] 對於紋理進一步提到，萃取出合
適的紋理特徵，將有益於影像切割之處理。所以，我們認為紋理對影像而言是一
個非常重要的性質。因此，如何能萃取出合適、辨識力強的紋理特徵，然後再利
用此特徵做進一步的分割、分類或是辨識，已成為近幾年影像處理研究的議題。 
儘管紋理分析的議題已廣受重視，且有眾多學者紛紛提出研究、探討，但目
前仍然不易以定量分析的角度來找出良好的紋理模型；通常紋理都是針對某些特
定應用，而特別加以設計，較常見的一種紋理模型為定義紋理影像是由一基本元
素，根據某空間排列規則重覆而組合成的一張紋理影像。這種模型較適用於人造
紋理影像及規則排列的自然影像。但事實上許多紋理影像卻是由隨機且不規則的
排列組合而構成的。因此，要找出能廣泛適用的紋理模型實屬不易。 
對於描述區域紋理的方法 Gonzalez 和 Woods [20] 認為有三種主要方法，分
別為統計方法(statistical)、結構方法(model-base)和頻譜方法(frequency spectrum)。
簡單的來說，統計方法係透過統計學上的物理意義，得到顏色之間的對比程度、
區塊的平滑或粗糙等紋理特徵，例如 Haralick [44] 等人提出了利用灰階共生矩陣
(Gray level co-occurrence matrix)取出紋理特徵，藉由統計出各頻帶兩相鄰像素間灰
階值在此頻帶所出現的機率，分別描述出該灰階頻帶之各種不同分布，並透過空
間灰階之相關矩陣建立，統計指標以評估紋理特徵；結構方法則係以結構的概念
為基礎，藉由影像原始單元的排列規則來得到紋理的特徵描述，如 Hideyuki et al.
利用線段的相似程度來進行紋理結構之描述；頻譜方法則是透過傅立葉頻譜描
述，來檢測一幅影像中的整體周期性。 
 15
滑邊緣，進而獲得更精確完整的物件輪廓，以達到去除微小缺口之目的。 
(3) Segmentation：經由閉合(closing)運算處理後的影像，已能明顯的區隔出影像物
件與背景，再以邊界抽取(Boundary Extraction)方式分隔出的影像物件。因此，
本研究再以 8-way 的 chain code 處理[20]，以切割出物件外形。物件外形輪廓
係透過上述步驟取得物件完整的封閉曲線後，再使用分水嶺分割(Watershed 
Segmentation) [20] 的方法正確取得物件位置與區域，以成功切割出影像內的器
物。 
3.2 紋理特徵(Texture Feature) 
本研究中所使用的特徵萃取方法，除了考慮物件的邊緣資訊之梯度特徵之
外，亦增加了 Tamura [45] 等人所提出表示和描述紋理特徵的方法，它的理論基礎
係基於紋理對於人類視覺上的心理學研究。Tamura 提出了六個特徵對應於心理學
角度上紋理特徵的屬性，分別為粗糙度(coarseness)、對比度(contrast)、方向度
(directionality)、線像度(linelikeness)、規整度(regularity)和粗略度(roughness)。而其
中粗糙度(coarseness)、對比度(contrast)、方向度(directionality)對於影像檢索來說具
有相當程度之影響。 
3.2.1 梯度(Gradient) 
本研究在梯度特徵的萃取上使用Sobel邊緣偵測。梯度影像函數由Eq(7a)方程
式中得到 ),( yxf  
y
yxf
x
yxfyxf ∂
∂+∂
∂=∇ ),(),(),(       (7a) 
當 
x
yxfyxx ∂
∂=Δ ),(),(  及 
y
yxfyxx ∂
∂=Δ ),(),( 。梯度大小 ),( yxΔ 可由下式中得知 
),(),(),( 22 yxyxyx yx Δ+Δ=Δ ,      (7b) 
以及梯度方向為  
⎟⎟⎠
⎞
⎜⎜⎝
⎛
Δ
Δ= −
x
yyx 1tan),(θ        (7c) 
水平方向及垂直方向的邊緣變化表示為 xΔ 及 yΔ 。因此，當 xΔ 及 yΔ 皆為0時，
表示該像素值與其八個鄰近像素值中的其中一個像素值是相同的，且沒有邊緣變
化的情況發生。故本研究採用 33× 區塊來計算水平 xΔ 及垂直 yΔ 的變化。 
 17
向與區塊時，則我們選取其中比較大的 k 值當作 ),( yx 的粗糙度 ),( yxk p 值；(2)若
取得的最大值 ),(, yxE jk 與其他不同方向與區塊的 ),(, yxE jk 的值相差甚小時，則依
據 Eq(10)之規則獲其結果，如圖(10)所示，因為粗糙度 pk 值選擇不當，導致背景
部分會有雜訊的產生，進而產生影像背景呈現漸層色階之像。因此，本研究將針
對 ),(, yxE jk 差值甚小時，設立一個門檻值標準，該方法係計算出每一像素 ),( yx 在
5~1=k 、 水 平 與 垂 直 方 向 共 10 個 ),(, yxE jk 值 的 標 準 差 (standard 
deviation) ),( yxσ ，若 ),(, yxE jk 之標準差(standard deviation) 2),( ≤yxσ 時則表示
),( yx 的鄰近區域為平滑區域。因此，我們將 ),( yx 的粗糙度以 5),( =yxk p 取而代
之，其所獲得的結果如圖(11)所示。 
  
圖(10) 原始粗糙度影像 圖(11) 經由門檻值過濾後 
  的粗糙度影像 
3.2.3 對比度(Contrast) 
對比度(Contrast)係指影像顏色之間對比的程度，當對比度計算出來的值越大
時，表示該影像顏色之間的變化程度越大、越明顯。 
本研究所計算之對比度，係針對影像中每個像素點之鄰近 33× 大小的區塊
(block)，分別計算出每個像素點的對比程度， ),( yx 其表示式如下： 
[ ] 4/14 ),(/),( ),(),( yxyx yxyxFcon ση σ=        (11) 
其中 ),( yxσ 為標準差、 ),( yxη 為影像的四次距(forth moment of the image)以及
),( yxμ 為每個區塊的平均數，分別表示如下： 
∑ ∑
−= −=
++=
1
1
1
1
),( 
9
1),(
j i
jyixfyxμ ,      (12a) 
[ ]1 1 2
1 1
1( , )  ( , ) ( , )
9 j i
x y f x i y j x yσ μ
=− =−
= + + −∑ ∑ ,    (12b) 
[ ]∑ ∑
−= −=
−++=
1
1
1
1
4),(),( 
9
1),(
j i
yxjyixfyx μη .    (12c) 
 19
示，黑色背景表示為顏色變化特性較小的部份；反之，越接近白色則表示該影像
顏色變化特性較大。 
 
圖(13) 方向度影像 
3.3 型態學處理(Morphological Processing) 
型態學處理(Morphological Processing)主要係以數學上的特性做為物件抽取的
工具，以及進行瓷器影像內瓷器物件結構形狀的處理。本研究首先將上一節的紋
理特徵(Texture feature)經由主成份分析(Principal component analysis, PCA)做特徵
萃取(Feature extraction)後，再將特徵取出後的影像做 Otsu [29] 二值化處理，藉此
描述與表示此瓷器物件的形狀。最後，使用影像形態學(morphology)中的閉合運算
(closing operation)做物件的過濾(filter)。 
3.3.1 特徵取出(Feature Extraction) 
主成份分析 (Principal component analysis, PCA) 係將一組高維度空間
(high-dimensional space)中的資料投影到低維度空間(low dimension space)，進而找
出最佳的投影方式，使得投影後的資料點能盡量散開，以保持原資料在高維度空
間分佈的特性，達到適當地縮減變數個數之目的。本研究運用 PCA 之方法，從每
一個像素(pixel)上的四個紋理(texture)特徵值中，找出最佳的投影方式，並將該四
個紋理(texture)特徵值投影至單一像數中。該方法除了保有原紋理(texture)特徵的資
訊內涵外，亦將 4 個紋理(texture)特徵適當的縮減為 1 個。理想的投影方法步驟如
下所示：  
Step1 特徵值正規化(normalization) 
粗糙度值越小表示該影像越粗糙，然而對比度、方向度及梯度，數值越大表
示對比程度、方向變化、梯度變化越大。因此，本研究將針對粗糙度的值進行補
值(complement)處理，也就是說將粗糙度的值改變為越大越粗糙，即將圖(11)補值
處理後為圖(14-b)所示。描述物件的四個特徵值之特性一致後，使梯度、粗糙度、
對比度、方向度的值之範圍均為相同。最後，再將四組特徵資料值以 0 至 255 的
灰階影像表示，達到資料的正規化。 
 21
0.9201、對比度權重為 0.2319、方向度權重為 0.2353 及梯度權重為 0.2104，如表(2)
所示，並計算主成分分析結果， 1λ 為第一主成份，其主成份負荷(loading)為 0.7814，
如表(3)所示，特徵權重投影結果如圖(16-b)所示，其粗糙度佔四個權重的 58%，其
餘之對比度、方向度和梯度各佔不到 15%的比重，僅能突顯粗糙度的特徵。 
表(2) 特徵值與特徵向量 
特徵值(eigenvalue) 特徵向量(eigen vector) 
18.3321 0 0 0 -0.0048 -0.2035 -0.3347 0.9201
0 589.7556 0 0 0.7096 0.6152 0.2533 0.2319
0 0 944.3888 0 -0.0523 -0.3936 0.8871 0.2353
0 0 0 5550.1210 -0.7026 0.6520 0.1921 0.2104
表(3) 主成份分析結果 
  λ1 λ2 λ3 λ4 
特徵值 
(eigenvalue) 
5550.12 944.39 589.76 18.33 
解釋變異量 0.7814 0.1330 0.0830 0.0026  
主成份負荷 
(loading) 
0.7814 0.914385 0.997419 1 
原圖 PCA 投影( 1λ ) PCA 投影( 1λ , 2λ ) 
 
(a) (b) (c) 
圖(15) PCA 投影影像之比較 
為能更有效率的保留特徵資訊並減少空間維度的計算，在此本研究將擷取主
成份負荷(loading)大於 80%之變異比例，當第一主成份( 1λ )小於 80%，則擷取第一
及第二主成份之特徵值乘上對應之特徵向量並進行正規化，因此能明確且更有效
率地表示出特徵權重之比例，如圖(15-c)所示，以便後續進行處理。 
Step3 求出第 l 個像素的 4 個特徵值在主成份負荷(loading)大於 80%的投影分量之
和，表示如下： 
∑
=
=
4
1
1
i
il
i
l WfF        (19) 
經由 PCA 基礎之特徵選取，能夠簡單地探索出在影像中更具有影響力的特徵
值，例如：針對較粗糙的影像，粗糙度給予較大的權重，當與鄰近像素值顏色變
化較大的影像，則對比度給予較大的權重，以此類推方向度與梯度之特徵權重，
結果如圖(16)所示。 
 23
                 
( ) ( ) ∑∑
+=+=
−=−=
L
ki
i
L
ki
piCii
1
1
2
1
1
1
2
1
2
1 /)(Pr ωμμσ            (22) 
群體變異數總和σ 表示為： 
2
11
2
00
2 σωσωσ ×+×=                   (23) 
當找到一個臨界值 k 使得 2σ 最小時，則這個臨界值 k 即為二值化所需之門檻值。 
經由二值化處理後，能將影像物件與影像背景之間的模糊區域，透過門檻值
的設定強迫分為二群，高於門檻值的為物件，低於門檻值的則為背景，如圖(17)
所示，藉由二值化之處理後，可更明確的區分出物件與背景的界線。 
 
圖(17) 二值化處理後之影像 
3.3.3 物件過濾(Object Filter)  
上述之的二值化(binary)處理係以背景為黑色(以 0 表示)與物件為白色(以 1 表
示)來表示，如圖(17)所示。圖中可以發現物件周圍的邊緣會產生些許的雜訊，亦
或是資訊於二值化的處理過程中遺失，例如物件的邊緣可能因為與背景顏色相近
或與物件本身陰影所造成的影響。為了避免影像雜訊影響了後續的演算法，以方
便於後續切割之動作。因此，我們使用影像形態學(morphology)中的閉合運算
(closing operation)，來達到去除雜訊與獲得完整資訊之目的。閉合運算(closing 
operation)主要係針對二元影像先進行膨脹(dilation)處理以擴大物件之區域，其次再
使用侵蝕(erosion)處理縮小區域，以符合原始影像物件之大小，進而獲得更完整的
物件外形及邊緣。不同於斷開運算的是，閉合運算除了可以平滑影像物體的邊緣
之外，還可填補物體邊緣的微小缺口，達到去除雜訊與獲得完整資訊的目的。其
中，閉合運算的處理步驟包含了膨脹(dilation)與侵蝕(erosion)處理 [20]。 
膨脹(dilation)主要係在於擴張影像的物件區域 [20]。假設影像 F 中的物件 A為
F 的集合，且膨脹集合表示為B，則 A被B 膨脹記作 BA⊕ ，如圖(19)及表示式(24)
所示： 
 25
該方程式係根據 B 集合對所有 A集合中的像素值 ),( yx 進行侵蝕運算，其中
),( yxB 包含於 A集合中。如同上述，同樣採用 33× 區塊(block)做為侵蝕之處理區域，
其中 A集合與B 集合如表(4a)與(4c)所示，當 A☉B 滿足方程式(25)時，則將像素點
),( yx 由 1 轉換為 0 即影像由白色轉變為黑色，圖(19)之二值化影像經侵蝕處理後
的結果如圖(20)所示。 
  
A☉B  ( A☉B )☉B  
圖(20) 侵蝕運算的瓷器影像 
膨脹與侵蝕的處理方式相當類似，不同點在於一開始所判斷的像素點 ),( yx ，
以膨脹來說如果像素點 ),( yx 為 1，以侵蝕來說如果像素點 ),( yx 為 0 時，才針對該
像素點 ),( yx 的鄰近 8 個像素值進行判斷等處理，達到去除微小缺口之目的，以獲
取完整的物件邊緣輪廓。 
3.4 切割(Segmentation) 
切割(segmentation)係依影像的結構將一張影像細分為區域(region)及背景，並
將該物件從影像中分割出來，其細分的程度視物體的不同而定。影像分割演算法
通常係以灰階值(gray value)的不連續性與相似性之特性為基礎。本研究結合不連續
性之 Gradient 邊緣萃取及相似性之紋理特徵進行分析，並作為器物切割的依據。
器物切割係將物件進行物件過濾(Object Filter)後，再藉由邊界抽取(Boundary 
Extraction)之處理，以透過類似分水嶺分割(Watershed Segmentation)之運用，將物
件與背景分離出來。 
3.4.1 邊界抽取(Boundary Extraction) 
經由影像形態學中的閉合運算處理後，已能由影像中明顯的區隔出影像背景
及物件。為了達到精確切割的目的，本研究將使用 8-way chain code 找出該物件之
輪廓，以進行物件切割之處理。Chain Code 為 Freeman [46] 於 1961 年提出，該方
法係使用於二元影像的物件邊界判讀或曲線編碼工作，8-way chain code 如圖(21)
所示，以八個向量碼作為八鄰接點。 
藉由 chain code 方法之特性，以 33× 像素(pixel)的矩陣，如圖(21)之順序由左
而右，由上而下的方向進行掃描，以找出物件最靠近左上角之起點。其次，再以 33×
的矩陣，如圖(21)之順序找出下個起始點並以遞迴的方式進行掃描，直到起點與終
點為同一像素(pixel)位置時結束。最後將獲得一個封閉的物件輪廓，達到物件切割
的目的，其結果如圖(22)所示。 
 27
第四章 實驗結果與討論 
4.1 前言 
本研究將針對通貨、瓷器與琺瑯器等 80 張影像分別進行實驗，影像來源是由
「台灣貨幣」[7]、「近期台灣的紙幣」[8]、「中國瓷器藝術」[9]、「明清琺瑯工
藝」[12]等網站中。其中包含各個朝代、不同外形與顏色之器物影像；本研究將器
物影像切割之實驗結果分別與 Level Set method [30] 、區域導向切割(region 
oriented segmentation)之 CCTA (Connected Coherence Tree Algorithm) [35]、基於
ACM 物件切割之 GCBAC (graph cuts based active contours) [34]及基於紋理基礎之
P.W. Huang’s Method [36] 等其他方法進行比較與分析。為驗證本研究方法於器物
影像切割處理之效能。為驗證本研究方法能快速切割，將分別計算 Level Set method 
[30]、CCTA [35]、GCBAC [34]、P.W. Huang’s Method [36] 與本研究方法之切割
時間進行分析與比較。預期該研究方法能快速、精確的由影像中區分出影像背景
與器物，並獲得與原始器物外形相似之結果。 
4.2 瓷器影像物件切割之比較分析 
為了驗證本研究方法的特性與其物件切割的可抗性，本研究將以瓷器物件、
背景及瓷器物件的粗糙度(coarseness)、對比度(contrast)、方向度(directionality)與梯
度(gradient)做為實驗分類之依據。我們將現有瓷器影像依照瓷器物件與背景
(background)之間人類識別區隔的難易度區分為二類：第一類為簡易切割影像與第
二類複雜切割影像。 
本研究所得之瓷器物件切割結果將分別與 Level Set method [30]、CCTA [35]、
GCBAC [34] 及 P.W. Huang’s Method [36] 等方法進行分析與比較。Level Set 
method [30] 、 GCBAC [34] 及 P.W. Huang’s Method [36] 係 採 用
MATLAB®7.1.0.246 軟體編譯後所獲得的結果，其中 P.W. Huang’s Method 考慮了
split and merge 兩個步驟所得到的切割結果；至於 CCTA [34] 則採用 JAVA 1.4.2
版本編譯後所得之結果。 
4.2.1 簡易紋理之物件切割 
簡易切割影像係指可以明顯區別出背景與物件的影像如圖(25)所示，其中物件
表面紋路(texture)為平滑紋理(a fine texture)與粗糙紋理(a coarse texture)之影像，且
該影像背景則分別呈現單色與漸層色系的色階分佈，例如影像編號 2、5、9 與 1、
3、4、8 及 10 的背景皆為漸層色階之分佈，而瓷器物件表面紋路(texture)分別是平
滑紋理(a fine texture)與粗糙紋理(a coarse texture)，影像編號 6、7 及 9 的背景是屬
於單色系的影像。
 29
 CCTA 
  
 
GCBAC 
(Initial 
Contour) 
  
 
GCBAC 
  
 
P.W. 
Huang’s 
Method 
  
 
本研究方法 
  
 
圖(26-1) 第一類影像之切割結果比較 
影像編號 6 7 8 9 10 
原始影像 
   
參考影像 
  
 
Level Set 
method 
   
 31
2、3、4 及 9 紅色圈選之部份，還包含了漸層背景及打光的關係而造成影像背景與
瓷器邊緣模糊等情況。 
影像編號 1 2 3 4 5 
圖 
    
影像編號 6 7 8 9 10 
圖 
   
圖(27) 第二類原始瓷器實驗影像 
影像編號 1 2 3 4 5 
原始影像 
   
參考影像 
   
Level Set 
method 
   
CCTA 
   
 33
CCTA 
  
GCBAC 
(Initial 
Contour) 
  
GCBAC 
  
P.W. Huang’s 
Method 
  
本研究方法 
  
圖(28-2) 第二類影像之切割結果比較 
本研究依續進行第二類影像的實驗，實驗結果將顯示於圖(28)中。於實驗結果
中發現 Level Set method 容易受到陰影和背景漸層的影響，而無法順利切割出該類
器物之輪廓，且切割結果與第一類影像實驗結果有異曲同工之處，明顯不適用於
此類器物之切割；CCTA 雖然可以克服背景漸層的問題，但對於紋路分佈疏密不均
的物件，容易造成無法順利完成切割的情形，如影像編號 2-4及 6-9所示；於GCBAC
的實驗結果中發現，當影像背景顏色與物件顏色相似時，則無法完整地取得器物
之外形，如影像編號 1、3、4、5、6、8 及 9 所示；P.W. Huang’s Method 會因影像
的漸層及紋飾疏密分佈不均等問題，造成過度切割之情況如影像編號 1-4、6 及 10
所示。同時該方法也容易受到光線及陰影影響而造成錯誤切割等情況，如影像編
號 1、4、5、8 及 10 所示；本研究於圖(28)之第二類影像切割結果分析中論證，本
研究方法除了可以克服影像的背景顏色漸層與器物顏色相近容易造成誤判的問題
之外，亦能克服器物紋路分佈疏密不均、物件陰影以及數位化打光等問題所造成
的邊緣模糊之情形如影像編號 2、3、4 所示，以順利的切割出與參考影像相似之
物件。 
 35
切割結果 
   
圖(29) 第一類通貨實驗影像及切割之結果 
第二部份的貨幣影像包含物件邊緣模糊如圖(30)之影像編號 2、3、4 及 5 所示，
以及立體物件所造成背景部份有陰影的影像，如圖(30)之影像編號 7、8 及 9 所示。
於圖(30)之切割結果中論證出，本研究方法能夠在克服邊緣模糊與物件陰影等問題
之餘，亦能從貨幣影像中精確切割出物件輪廓。 
影像編號 1 2 3 4 5 
原始影像 
  
切割結果 
  
影像編號 6 7 8 9 10 
原始影像 
    
切割結果 
    
圖(30) 第二類通貨實驗影像及切割之結果 
 37
如圖(32)所示，其中影像包含多處缺口即使肉眼也難以辨識物件與背景之區別，如
影像編號 2 及 3 紅色圈選部份所示，以及包含嚴重陰影干擾之影像，如影像編號
為 4、5、7 及 10 紅色圈選部份所示。然而，根據實驗結果論證，本研究方法確實
能有效地克服上述問題，達到精確地輪廓切割，並獲得與原始物件相似之結果。  
影像編號 1 2 3 4 5 
原始影像 
  
切割結果 
 
 
影像編號 6 7 8 9 10 
原始影像 
 
 
切割結果 
 
 
圖(32) 第二類瓷器實驗影像及切割之結果 
4.3.3 琺瑯器影像之物件切割 
本研究之琺瑯器影像來源是由「明清琺瑯工藝」[12] 網站中取得，其中包含
了明朝與清朝時代的琺瑯器影像，以及掐絲琺瑯器、內填琺瑯器及畫琺瑯器等三
種類別的琺瑯器影像，其中包含了多項不同外形的琺瑯器影像，如：爐、盒、盤、
壺、罐、瓶、尊、碗、碟、盂、豆及觚等，另外於色彩變化的部份有紅、黃、緣、
靛藍、灰紫、白、黑、赭及具有玻璃光澤的洋紅、淡紅、淺緣、深綠、淺黃及葡
 39
影像編號 1 2 3 4 5 
原始影像 
  
切割結果 
 
影像編號 6 7 8 9 10 
原始影像 
   
切割結果 
  
圖(34) 第二類琺瑯器實驗影像及切割之結果 
4.4 物件切割之時間花費評估比較與分析 
為驗證本研究方法能快速地自動切割出精確的瓷器物件，本研究將與人工切
割方法、Level Set Method [30]、CCTA [35]、GCBAC [34] 及 P.W. Huang’s Method 
[36] 進行物件切割時間之比較與分析。在時間的比較上，本研究以 Microsoft 
Windows XP Pro Sp2 operation system, Intel(R) Pentium(R)4 CPU 2.8GHz CPU and 
RAM with 1G 進行實驗，實驗結果如表(5)所示。於實驗證明本研究方法比人工切
割方法平均快 10.92 倍的速度完成切割；Level Set 方法則需要花費較多的處理時
間；雖然 CCTA 方法的處理時間較 Level Set 方法來的快，但普遍上的執行速率皆
低於本研究之方法；GCBAC 方法改進了 Snake 初始輪廓的定位方式，約略訂定義
出矩形之初始輪廓以進行切割，雖然物件切割處理的時間花費快於 Level Set [30] 
與 CCTA [35]，但該方法會因背景與物件顏色相似而無法完整的切割出物件；P.W. 
Huang’s Method [36] 係基於紋理相似的特性進行切割，雖然在執行時間的花費上
少於 Level Set 方法，但該方法只能大致區分出器物物件，且容易受光線及器物紋
飾不均之影響，無法達到精確的物件切割；本研究方法除了不需使用者自行定義
 41
影像的輪廓切割結果與其採用的切割方法所獲得之輪廓切割結果比較，其輪廓內
外的錯誤百分率。即為實驗影像的背景被錯誤指派到參考影像物件的區域及實驗
影像的物件被指派到參考影像的背景區域，ME 的表示式如 Eq(26)所示： 
 
圖(35) 物件背景與輪廓之示意圖 
  ,1
OO
TOTO
FB
FFBB
ME +
∩+∩−=         (26) 
其中， OB 及 OF 分別表示參考影像輪廓外部及輪廓內部之區域， TB 及 TF 則分別
為實驗影像輪廓外部像素值的區域及輪廓內部像素值的區域。ME 數值的變化呈現
0 至 1 之分佈，0 代表精確的影像切割，1 代表完全錯誤的影像切割，換句話說 ME
的數值越小表示影像切割的精確度越好。 
RAE(relative foreground area error)為測量參考影像與切割影像的誤差比例，即
比較參考影像與實驗影像之面積來表示切割的精確度，RAE 的表示式為： 
 
(27) 
 
其中， OF 表示參考影像輪廓內部像素值的區域， TF 則為實驗影像輪廓內部像素值
的區域。RAE 數值的變化為 0 至 1 之間，0 表示實驗影像與參考影像所切割的物
件完全符合，換句話說 RAE 數值越小，表示實驗影像所切割的區域與參考影像所
切割的越符合，精確度越好。 
MHD(modified Hausdorff distance)方法為測量參考影像物件輪廓內的範圍及
實驗影像物件輪廓內的範圍中的像素值距離，其主要係測量實驗影像輪廓之外型
與參考影像輪廓外型的變形程度，MHD 的方法可定義為： 
)),,(),,(max(),( OTMHDTOMHDTO FFdFFdFFMHD =     (28) 
式中 
   .1),( min TO
Ff Ffo
TOMHD ffF
FFd
OO TT
−= ∑
∈ ∈
    (29) 
物件 
影像背景 
物件輪廓 
RAE= O
TO
F
FF −
 if ,OT FF <  
T
OT
F
FF −
 if ,OT FF ≥  
 43
表(7) RAE 之比較(%) 
 影像編號 本研究方法 GCBAC 
[34] 
 影像編號 本研究方法 GCBAC 
[34] 
1 3.09 33.76 1 12.04 40.88 
2 9.36 3.38 2 1.63 7.68 
3 8.70 7.91 3 7.54 10.18 
4 7.38 9.76 4 5.07 25.97 
5 5.67 15.01 5 7.31 28.51 
6 5.93 11.01 6 7.68 29.77 
7 9.25 5.77 7 7.32 18.12 
8 12.24 11.53 8 6.45 6.44 
9 4.79 0.92 9 6.76 32.11 
第
一
類
影
像 
10 11.01 52.52 
第
二
類
影
像
10 9.00 12.00 
表(8) MHD 之比較(%) 
 影像編號 本研究方法 GCBAC 
[34] 
 影像編號 本研究方法 GCBAC 
[34] 
1 7.95 324.14 1 36.04 984.45 
2 32.69 7.59 2 17.82 50.36 
3 27.20 52.34 3 19.82 75.81 
4 20.86 55.19 4 12.89 337.59 
5 34.06 122.20 5 19.08 667.01 
6 24.88 50.93 6 21.95 556.14 
7 31.66 19.32 7 30.12 195.97 
8 43.47 43.08 8 23.47 168.86 
9 14.29 2.64 9 16.49 545.32 
第
一
類
影
像 
10 33.25 2878.90 
第
二
類
影
像
10 47.28 52.63 
為了獲得更公平的執行效果，本研究將分別以 ME (misclassification error)、
RAE(relative foreground area error)、MHD(modified Hausdorff distance)三個評估標準
取一正規化的算術平均值即為 MEN , RAEN , and。其中 ME 為實驗影像的背景被錯
誤指派到參考影像物件的區域及實驗影像的物件被錯誤指派到參考影像背景的區
域；RAE 為參考影像與切割影像誤差的比例；MHD 為測量實驗影像輪廓外型與參
考影像外形的變形差異。換句話說，使用本研究方法與參考影像計算所獲得的三
種評估標準加以平均，以獲得更精確的影像切割品質評估。 
3/)( MHDRAEME NNNAVE ++=       (30) 
 45
第五章 結論與未來展望 
5.1 結論 
為保存文化資產使後代子孫也能從中學習、體會與研究，並於國際中發揚民
族文化，於是成立了數位典藏國家型計畫。其中數位典藏第一期計畫中的數位化
子計畫，係基於產生龐大的器物典藏影像，以輔助使用者在高精確率的查詢體制
下進行檢索。因此，本研究主要係以擷取紋理特徵的方式並透過影像處理之技術，
使影像背景與影像物件分離，達到器物影像切割之目的。 
本研究以器物典藏影像進行物件切割之實驗，該研究方法除了能克服簡單及
複雜的紋飾、顏色分佈不均、背景漸層、陰影干擾等問題外，當影像背景顏色與
瓷器物件顏色相近不易分辨時也能順利完成切割。 
本研究除了完成器物影像切割之實驗外，亦與 Level Set 方法、CCTA、GCBAC
及 P.W. Huang’s Method 進行比較，以驗證本研究方法能有效地區分出影像背景與
器物物件，並切割出與原始瓷器外形相似的結果。為了更精確、具體的驗證本研
究方法之效能，本研究將與 GCBAC 方法進行物件切割之 ME、RAE、MHD 效能
評估以及 AVE 效能評估和執行時間之比較，以證明本研究之物件切割方法能精確
地切割器物物件。 
5.2 未來展望 
在資訊科技普及導致數位影像的與日俱增，影像檢索技術更為重要。故精確
且快速地檢索出欲查找之影像，並發展一套影像檢索系統，以供使用者查詢，以
傳承並發揚至世界各地。 
本研究經由上述影像切割之實驗，精確地切割出器物物件，期望能做為未來
發展一套有效縮減典藏影像資料量與高精確度影像檢索的前置處理技術，以達到
更多元、完整之數位典藏器物檢索，使資訊科技與人文資產結合，達到經濟與產
業的共同發展。 
 
 47
models,” in Proc. 1st ICCV, pp. 259–267. 
[19] P. L. Palmer, H. Dabis, and J. Kittler, 1996, “A performance measure for 
boundary detection algorithms,” Computer Vision and Image Understanding, Vol. 
63, pp. 476-494. 
[20] Rafael C. Gonzalez, Richard E. Woods, 2002, Digital Image Processing, 
Prentice-Hall. 
[21] J. F. Canny, 1986, “A Computational Approach to Edge Detection,” IEEE 
Transaction on Pattern Analysis and Machine Intelligence, Vol. 8, No. 6, pp. 
679-698. 
[22] L. Ding, and A. Goshtasby, 2001, “On the Canny Edge Detector,” Pattern 
Recognition, Vol. 34, pp. 721-725. 
[23] Y. W. Lim and S. U. Lee, 1990, “On the color image segmentation algorithm 
based on the thresholding and the fuzzy C-means technique,” Pattern Recognition, 
Vol. 23, No. 9, pp. 935-952. 
[24] Jianping Fan, David. K. Y. Yau, Member, IEEE, Ahmed. K. Elmagarmid, Senior 
Member, IEEE, and Walid G. Aref, Member, IEEE, 2001, “Automatic Image 
Segmentation by Integrating Color-Edge Extraction and Seeded Region 
Growing”, IEEE Transactions on Image Processing, Vol. 10, No. 10, October, pp. 
1454-1466. 
[25] F. Y. Shih, S. Cheng, 2005, “Automatic seeded region growing for color image 
segmentation,” Image and Vision Computing, Vol. 23, pp. 877-886. 
[26] Mumford, D., Shah, J., 1898, “Optimal approximations by piecewise smooth 
function and associated variational problems,” Commun.Pure Appl. Math.42, 
pp.577-684. 
[27] Deng, Y., Manjunath, B.S., 2001, “Unsupervised segmentation of color texture 
regions in images and video,” IEEE Trans. Pattern Anal. Machine Intell. 23(8), 
pp.800-810. 
[28] Dupuis, A., Vasseur, P., 2006, “Image segmentation by cue selection and 
integration,” Image and Vision Computing, Vol. 24, pp. 1053-1064. 
[29] Kato, Z., Pong, T.C., 2006, “A Markov random field image segmentation model 
for color textured image,” Image and Vision Computing, Vol. 24, pp.1103-1114. 
[30] Tony F. Chan, Member, IEEE, and Luminita A. Vese, 2001, ”Active Contours 
Without Edges”, IEEE Transactions on Image Processing, Vol. 10, No. 2, pp. 
 49
641-621.  
[45] H. Tamura, S. Mori, T. Yamawaki, 1978, “Texture features corresponding to 
visual perception,” IEEE Transactions on Systems, Man, and Cybernetics, Vol. 8, 
pp. 460-473.  
[46] H. Freeman, 1961, “On the encoding of arbitrary geometric configurations,” IRE 
Trans. on Electron. Comput., EC-10, pp. 260-268.  
[47] Sezgin, M., Sankur, B., 2004, “Survey over image thresholding techniques and 
quantitative performance evaluation”, Journal of Electronic Imaging 13(1), 
pp.146-165.  
 
 51
(5) 98 年 7 月 15 日會議結束。 
二、與會心得 
 此次於 Informs APS 2009 international conference 論文發表會場，會議本身可說
為 Applied Probability Society 應用領域相當優異之國際會議，Present 的 paper 都
經過 rigorously refereed。規模不小，各地專家學者聚集於此會議，討論與交流非常
熱列，較熱門之場次，聆聽之觀眾皆滿座。會場詢問時間中，問題之尖銳、考量
點之廣泛，使人深刻體會到表達之技巧與學術研究之嚴謹的重要。 
本人 present的 paper安排在 July 13的Am 8:00-Am 9:30 ”System Reliability and 
Queueing Model(1)” section，在該 section 中本人所 present 的題目為” An Object 
Detection Using Adaptive Threshold for ThinPrep-Cervical Smear”，在這醫學發達的
現代，子宮頸癌細胞檢測判定，仍以人工方式一一檢視每個細胞，而缺乏自動化
的特徵萃取之輔助工具。我們 Present 的 paper，透過以 edge detection 為基礎，自
動的偵測子宮頸抹片影像中細胞核物件，給予細胞病理人員重要的輔助判定之特
徵。在這 section 與會的人員，有很多學者給予本人某些有用的 comments。 
參加此次 Informs APS 2009 international conference，可說獲益良多，看到 New 
York 的進步，有些地方值得我們學習。除此之外，對於我本身之專業而言，藉由
參與即聆聽精采的論文發表場次外，與各地相同研究領域的菁英相處，並就研究
主題進行研討的過程獲益匪淺，更加堅定目前研究的主題與方向，並且暸解到目
前研究主題之未來趨勢。因此，希望在今年能夠朝著未來研究方向，接續以往之
研究成果，再次參與明年的國際學術研討會，使得自己的研究不斷的成長，並期
望與各國先進並駕齊驅。 
三、建議 
 感謝國科會的用心，給予參與國際學術研討會的補助款項，使參加今年 Informs 
APS 2009 減輕不少經濟負擔。此次參與會議所得非常豐碩，視野亦增進不少；不
但能開拓本人的國際視野，提升本人的學術水準，更能讓本人深刻體認學術須與
國際學術潮流接軌和交流的重要性。 
 本人相當期望國科會對此一補助政策能持續實施。而實質經費的補助，才能
引發國內學者到國外發表論文的動機，以及克服經濟上之難題，使台灣的研究成
果得以在國際學術場合中展露，也相信有些於現場所得之震撼，並非在實驗室、
電腦室閉門造車所能得到的。 
四、攜回資料名稱及內容 
1. 大會詳細議程一份； 
2. 大會紀念背袋一個。 
五、其他 
    衷心感謝國科會給予補助此次研討會。 
 53
detection methods need to model the gradient and edge before using a specific 
algorithm to detect edge lines. Gradient filter [1] is an edge enhancement method most 
commonly seen in the research of edge model. Many approximation methods are 
available for estimating the gradient of gray-level in x and y directions ),( yxxΔ  and 
),( yxyΔ . Generally, they can be classified by operators into first-order derivative 
operator and second-order derivative operator. So far, a number of object edge 
enhancement and contour detection methods have been proposed. For instance, Gradient 
[1], Laplacian edge detection [1], Sobel edge detection [1], Canny edge detection [2][3], 
and Watershed [4][5] can be used for edge linking and boundary detection. Otsu method 
[6] is based on thresholding. Active contour model (ACM) [7]-[9] and Level Set 
[10]-[13] are region-based segmentation methods.  
In this paper, an object detection and segmentation technique is developed for cell 
nucleus on cervical cells from ThinPrep-cervical smear. An edge detection method is 
used to segment cell nucleus from a cervical cell image. First of all, through 
pre-processing and Haar Discrete Wavelet Transform (HDWT), undesired noise is 
removed to enhance object edges. Later, a series of edge detection techniques, including 
Sobel edge detection, false edge processing, and edge enhancement are applied. For 
edge line thinning, non-maximum suppression and hysteresis thresholding are adopted. 
Finally, the contour of cell nucleus is extracted using edge extraction techniques. 
2. Image preprocessing 
In this paper, the regions of interest in a cervical cell image are superficial cells, 
intermediate cells, and parabasal cells on stratified squamous epithelial cells of the 
vagina and the portio vaginalis. A cervical cell image is partitioned into three regions, 
including background, cytoplasm, and cell nucleus, as shown in Fig. 1.  
 
Fig. 1 Cervical cell image. 
The proposed image preprocessing steps involve (1) transformation of the cervical 
cell image from the color space to the grayscale space, (2) use of Haar Discrete Wavelet 
Transform (HDWT) to obtain the low frequency portion of the image, and (3) 
smoothing with a mean filter and enhancement of gray-level values. These 
preprocessing steps are intended to facilitate the subsequent edge detection.  
2.1. Color transformations 
Cytoplast 
Cell nucleus 
Backgroun Background 
Cytoplast
Cell nucleus
 55
 
(a) Original image F  
 
(b) gray-level imageY  
 
(c) Low frequency band image g  
 
(d) Result of mean filtering f  
Fig. 3 Image preprocessing result 
3. Edge detection 
For cell nucleus objects, a cell nucleus edge detection algorithm is developed. This 
algorithm is integrated with Sobel edge detection, non-maximum suppression, and 
hysteresis thresholding.  
3.1. Sobel edge detection 
Sobel edge detection [1] relies on the measurement of gradient in 2D image. In this 
paper, a convolution matrix with 3×3 mask is used to calculate the gradient of each 
pixel of ),( yxf  in the vertical and horizontal directions, respectively. The gradient 
can be calculated as follows: 
),,(),(),( 22 yxyxyx yx Δ+Δ=Δ                           (3) 
where,  
[ ]
[ ],)1,1()1,(2)1,1(
)1,1()1,(2)1,1(),(
−++−+−−−
++++++−=Δ
yxfyxfyxf
yxfyxfyxfyxx                   (4) 
[ ]
[ ],)1,1(),1(2)1,1(
)1,1(),1(2)1,1(),(
+−+−+−−−
+++++−+=Δ
yxfyxfyxf
yxfyxfyxfyxy                   (5) 
and the gradient direction ),( yxθ  is: 
⎥⎦
⎤⎢⎣
⎡
Δ
Δ= −
),(
),(
tan),( 1
yx
yx
yx
x
yθ                           (6) 
 57
detection method according to the properties of cell nucleus. This method involves the 
following steps: approximate cell contour repair and thinning, watershed-based region 
edge extraction, and cell nucleus edge extraction.  
4.1. Approximate cell contour repair and thinning 
After binarization, some contours still have tiny broken segments. We define this 
kind of broken contours as “approximate contour”.  
Approximate contour repair is to repair true edge pixels that have been missed. In 
this method, the non-edge pixels after binarization are considered. If the distribution of 
a pixel’s surrounding values belongs to any of the eight contour samples, as shown in 
Fig. 5, the pixel will be repaired according to the samples. First of all, we define that an 
edge value is denoted by 1 and a non-edge value by 0. If the value of a pixel ),( yx  is 0, 
pixels around the center pixel ),( yx  are selected to form a 3×3 mask. If these adjacent 
pixels resemble the eight samples shown in Fig. 5, the value of the center pixel is 
modified to 1 and the pixel ),( yx  is listed as an edge pixel. Coordinates in gray 
background and not denoted by numerical figures are not considered.  
1 0   0 1 1 0
0 0 0  0 0 0 0 0 0 1 0 1
 0 1  1 0 1 0
 0 1  1 0 1 1
1 0   0 1 0 0 0 0
    1 1
Fig. 5 The 8 samples for approximate contour repair 
After approximate contour repair, some edge lines will become thicker. Hence, line 
thinning should be applied to the image to obtain more accurate edge lines. 
We use Hit-and-Miss Transform-based Skeletonization (HMTS) algorithm [1] to 
obtain the skeleton of edges. This algorithm can exclude redundant edge pixels, retain 
true edge pixels, and maintain the edge line width to one pixel. It is called line thinning 
processing. HMTS algorithm evaluates the edge pixels (value=1) one by one. Using the 
edge pixel as the center, a 3×3 mask is selected. If the distribution of pixels in the mask 
satisfies any given condition shown in Fig. 6, the center pixel is redefined as a non-edge 
pixel, i.e. the pixel value is modified from 1 to 0. Through processing of HTMS 
algorithm, redundant edge pixels can be removed to maintain the edge line width to one 
pixel. The line thinning process is thus completed.  
0 0 0  0 0 1 0 1
 1   1 1 0 1 1 0 1 1 0
1 1 1  1 1 0 0 0
1 1 1  1 0 1 0 0
 1   0 1 1 0 1 1 0 1 1
0 0 0  0 0 0 1 1
Fig. 6 The reference edge structures for line thinning 
 59
(a) Result of watershed-based region 
edge extraction 
(b) Result after cytoplasm contours 
are removed. 
Fig. 8 Cell nucleus dge extraction. 
After cell nucleus edge extraction, the image still contains some non-cell nucleus 
contours caused by the texture noise inside cytoplasm or the background noise. 
However, the level to which a cervical cell image is affected by noise varies depending 
on magnifications. At 100x or 200x magnification, because the resolution is lower, the 
textures inside cytoplasm objects are less significant, as shown in Fig. 9(a). At 400x, 
because the resolution is higher, cytoplasm textures are significant, and some non-cell 
nucleus contours will be generated, as shown in Fig. 9(b). Therefore, these non-cell 
nucleus contours should be filtered according to magnifications.  
  
(a) Cervical cell image at 200x 
  
(b) Cervical cell image at 400x. 
Fig. 9 Result of cell nucleus edge extraction.  
We first process the non-cell nucleus contours at 100x and 200x magnifications. 
The gray values inside the non-cell nucleus contours are far greater than those inside 
cell nucleus contours. It can be affirmed that the average gray value inside the non-cell 
nucleus contours is a minority outlier of all values inside all contours (including cell 
nucleus contours and non-cell nucleus contours). Our method is to compute the mean 
Tμ  and standard deviation Tσ  of gray values inside all contours. The mean gray value 
of pixels inside the ith contour can be denoted as iμ . If iμ  is greater than Tμ  plus 2 
times Tσ , i.e. TTi σμμ ×+> 2 , the ith contour is viewed as a non-cell nucleus contour 
and removed. The result is shown in Fig. 10(a).  
At 400x magnification, the gray values inside the non-cell nucleus contours are 
similar to those inside cell nucleus contours, so the above filtering method is not 
applicable. Our observation tells us that of the mean gray values of pixels inside 
contours, the minimum is certainly derived from a cell nucleus object. Thus, we sort all 
the means from small to large and use “the second smallest mean should be smaller than 
mean gray value plus 2 times standard deviation” (i.e. iii σμμ 21 +≤+ ) as a rule to 
determine cell nucleus contours. In other words, the means are ranked in the following 
order nii μμμμμμ <<<<<<< + LL 1321 . Let 1+iμ  be the mean gray value of pixels 
inside i+1 contour, 1+iμ  must satisfy:  
iii σμμ 21 +≤+         (7) 
where 
 61
(a) Cervical cell image at 200x 
 
(b) Cervical cell image at 400x 
Fig. 10 Result of non-cell nucleus edge filtering.  
5. Experimental results 
To verify the proposed method’s accuracy and effectiveness in processing fuzzy 
edges, artificial cell images and ThinPrep cervical smear images are tested in the 
experiment. Artificial cell images are shown in Fig. 11(a). The original artificial cell 
image is blurred at different levels using Photoshop CS2. The cervical cell images are 
shown in Fig. 12(a). Superficial cell image at 100x and 200x, intermediate cell image at 
200x, and abnormal cell image at 400x are tested. At lower magnification, more cells 
can be seen on each cervical cell image. The texture of cytoplasm or cell nucleus is even, 
and there is less noise in the background. In contrast, at higher magnification, fewer 
cells can be seen. The texture of cytoplasm or cell nucleus becomes very evident, and a 
huge amount of non-cell noise can be seen in the background.  
Segmentation result of the artificial cell images is shown in Fig. 11(b). It can be 
discovered that the proposed method can effectively resist the impact of blurred edges 
to successfully segment cell nucleus regions at different levels of fuzziness, without any 
human intervention or configuration of parameters.  
(a) 
    
(b) 
    
Fig. 11 Segmentation result of the artificial cell images: (a) original images; (b) result of 
cell nucleus segmentation 
The experimental result of the ThinPrep cervical cell images is shown in Fig. 12. It 
can be discovered that the proposed method can successfully resist fuzziness of cell 
nucleus contours and also effectively filter out the noise edges caused by internal 
texture of cell nucleus and background noise to accurately segment cell nucleus regions.  
 63
[4] D. Wang, “A Multiscale Gradient Algorithm for Image Segmentation Using 
Watersheds,” Pattern Recognition, Vol. 30, Issue: 12, December, 1997, pp. 
2043-2052. 
[5] K. Haris, S. N. Efstratiadis, N. Maglaveras, and A. K. Katsaggelos, “Hybrid Image 
Segmentation Using Watersheds and Fast Region Merging,” IEEE Transactions on 
Image Processing, Vol. 7, Issue: 12, December 1998,  pp. 1684-1699. 
[6] N. Otsu, “A threshold selection method from gray-level histograms,” IEEE 
Transactions of Systems, Man, and Cybernetics 9, 62–66. 1979. 
[7] C. A. Davatzikos, and J. L. Prince, “An Active Contour Model for Mapping the 
Cortex,” IEEE Transactions on Medical Imaging, Vol. 14, 1995, pp65-80. 
[8] M. Kass, A. Witkin, and D. Terzopoulos, “Snakes: Active Contour Models,” 
International Journal of Computer Vision, Vol. 1, 1998, pp. 321-331. 
[9] Chenyang Xu and Jerry L. Prince, “Snakes, Shapes, and Gradient Vector Flow,” 
IEEE Transactions On Image Processing, Vol. 7, No. 3, March 1998, pp. 359-369. 
[10] Osher, S., and Sethian, J.A., “Fronts Propagating with Curvature Dependent Speed: 
Algorithms Based on Hamilton-Jacobi Formulations,” Journal of Computational 
Physics, 1988, 79, 12-49. 
[11] T. Toda, and N. Kimura, “Active Contours Without Edges,” IEEE Transactions On 
Image Processing, Vol. 10, No. 2, February 2001, pp. 266-277. 
[12] E. Sifakis, C. Garcia, and G. Tziritas, “Bayesian Level Sets for Image 
Segmentation,” Journal of Visual Communication and Image Representation, Vol. 
13, No. 1, 2002, pp. 44-64. 
[13] Chunming Li, Chenyang Xu, Changfeng Gui, and Martin D. Fox, “Level Set 
Evolution Without Re-initialization: A New Variational Formulation,” Proceedings 
of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern 
Recognition (CVPR’05). 
 
