 1
行政院國家科學委員會專題研究計畫成果報告 
 
 
即時不特定語句大量語者辨識系統之設計與實作 
 
 
計畫編號：NSC 96-2221-E-110-078 
執行期限：96 年 08 月 01 日至 97 年 07 月 31 日 
 
 
主持人：陳志堅 
 
國立中山大學  電機工程學系 
 
中文摘要 
 
我們以卡式轉換，小波封包轉換與高斯
混合模型為基礎，設計了兩階段式的即時不
特定語句大量語者辨識系統。計劃中我們錄
製了 500 位國語語者之資料庫來測試系統的
正確率。結果證實，使用 32 個卡式轉換特
徵，16 個梅爾頻段小波封包轉換特徵，並配
合巴式距離量度，以第一階段由卡式轉換及
小波封包轉換，各自選取前 1% ，共 2%之 10 
位最相近之候選語者，並配合第二階段之 32
個分量的高斯混合模型的辨識策略，來作即
時不特定語句之語者辨識，可獲得比高斯混
合模型，高出約 9.6% 之正確辨識率，同時
計算速度更可加快將近 125 倍。 
 
關鍵字: 卡式轉換，小波封包轉換，高斯混
合模型，即時不特定語句語者辨識
系統 
 
Abstract: 
 
This Report proposes a classification 
scheme that incorporates Karhunen- Loeve 
transform, wavelet packet transform and 
Gaussian mixture model for real-time text-
independent speaker identification.  A 
database with 500 Mandarin speakers is 
collected for system evaluation.  It is 
demonstrated that the accuracy improvement 
up to 9.6% and the computational cost 
saving of 125 times compared to those of the 
conventional GMM model can be achieved.  
 
Keywords:   Karhunen-Loeve transform, 
Wavelet packet transform, 
Gaussian mixture model, 
Real-time text independent 
speaker identification system 
 
Introduction:  
 
Chen [1] used a two-stage hybrid 
Karhunen-Loeve transform and Gaussian 
mixture model approach (KLT/GMM) to 
improve the accuracy and save the 
computational cost for text-independent 
speaker identification system.  For a 500-
speaker database, it was found that the two-
stage mechanism can increase the correct 
rate by 4% and save the computational cost 
by 10 times compared to those of the 
conventional GMM model.  In this Report, 
a more delicate feature and decision fusion 
scheme is proposed to further improve the 
system performance by adding the wavelet 
packet transform (WPT) and the 
 3
is calculated, where ),(2 jig is the j ’th 
energy component in the i ’th frequency 
band, and iN is the total number of 
components in the i ’th frequency band.  
These 16 coefficients are then taken as the 
WPT features. 
 
 
 
 
Fig. 2  Sixteen band Mel-like wavelet 
packet tree structure 
(The numbers indicate the pass 
band of each filter, in Hz) 
 
 
Bhattacharyya distance:  
 
Bhattacharyya distance is closely tied 
to the probability of error as an upper bound 
on the Bayes error for normally distributed 
classes [6].  For normal pdf’s, the 
Bhattacharyya distance between class i and j 
is 
 
)()
2
()(
8
1
2
ln
2
1
1
2
2
1
2
1
ji
jiT
ji
ji
ji
BD
μμμμ −Σ+Σ−+
ΣΣ
Σ+Σ
=
−
  (2) 
 
 
The first term of eqn. 2 gives the class 
separability due to the difference between 
class covariance matrices, while the second 
term gives the class separability due to the 
difference between class means. 
Furthermore, the optimal Bayes 
classification error between the two classes 
is bounded by the following expression: 
 
)exp( 221 BDPP −≤ε        (3) 
 
We will refer to the upper bound of the 
error probability evaluated from the 
inequality (3) with 5.021 == PP , as the 
Bhattacharyya error, 
 
)exp(5.0 2BB D−=ε         (4) 
 
The Bhattacharyya distance directly 
compares the estimated mean vector and the 
covariance matrix of the test speaker with 
those of the training speakers. If inclusion of 
the test covariance in the metric is useful, 
Bhattacharyya distance will outperform 
others distances [1]. 
 
 
Classification of speech data: 
 
A database with 500 Mandarin speakers, 275 
males and 225 females, recorded from TV 
was used for system evaluation [1].  The 
data is sampled by Creative Lab’s Sound 
Blaster Live sound card at 11.025 KHz and 
is linearly 16-bit PCM coded.  The analysis 
frame used in this study has 256 points that 
is approximately 23ms in length.  For each 
speaker, 1320 frames of speech data, ~30s in 
duration, are collected where all silences and 
all noise are eliminated. The first 1100 
frames of data, ~25s in duration, are used for 
training and the remaining 220 frames, ~5s 
in duration, for test.  In the first stage of the 
initial candidate selection process, the test 
data (~5 s) is transformed by the 32-
dimensional KLT features based on 256-
point STFT.  At the same time, the data is 
also transformed by the WPT where 
‘Daubechies 8’ wavelets are applied and the 
log energy of the wavelet coefficients in 
 5
Conclusions:  
 
A real-time text-independent speaker 
identification system based on the KLT, the 
WPT and the GMM is developed in this 
project.  It indicates that the features 
derived from both the KLT and the WPT can 
compensate each other.  And this property 
can be applied to diminish the use of the 
more speaker-ambiguous and time-
consuming GMM classifier as much as 
possible.  For a database with 500 
Mandarin speakers, it is demonstrated that 
the accuracy improvement up to 9.6% and 
the computational cost saving of 125 times 
compared to those of the conventional GMM 
model can be achieved. 
 
References 
 
[1] Chen, C.C.T., Chen, C.T. and Hou, 
C.K.: ‘Speaker identification using 
hybrid Karhunen-Loeve transform and 
Gaussian mixture model approach,’ 
Pattern Recognition, 2004, 37, pp. 
1073-1075 
 
[2] Farooq, O., and Datta, S.: ‘Mel filter-
like admissible wavelet packet structure 
for speech recognition’, IEEE Signal 
Processing Letters, 2001, 8, (7), pp. 
196–198 
 
[3] Chen, C.C.T., Chen, C.T. and Tsai, 
C.M.: ‘Hard-limited Karhunen-Loeve 
transform for text-independent speaker 
recognition,’ Electronics Letters, 1997, 
33, (24), pp. 2014-2016 
 
[4] Chih-Chien Thomas Chen, C.T. Chen 
and P.W. Cheng, “Hybrid KLT/GMM 
approach for robust speaker 
identification,” Electronics Letters, 
Vol.39, No.21, pp.1552-1554, October 
2003 
 
[5] J. P. Campbell,: “Speaker recognition: 
A Tutorial,” IEEE Proceedings,  Vol. 
85,  pp. 1437-1462, 1997 
 
[6] K. Fukunaga, Introduction to Statistical 
Pattern Recognition, Academic Press, 
New York, 1972 
 
[7] J. Kittler, M. Hatef, R.P.W. Duin and J. 
Matas, “On combining classifiers,” 
IEEE Transactions on Pattern Analysis 
and Machine Intelligence, Vol.20, No.3, 
pp.226-239, 1998 
 
[8] B.L. Pellom and John H.L. Hansen, 
“An efficient scoring algorithm for 
Gaussian mixture model based speaker 
identification”, IEEE Signal Processing 
Letters, Vol.5, No.11, pp.281-284, 
November 1998 
 
[9] H.S. M. Beige, S.H. Maes, J.S. 
Sorensen, and U.V. Chaudhari, “A 
hierarchical approach to large scale 
speaker recognition,” in Proceedings of 
6th Euro. Conf. Speech Communication 
and Technology (Eurospeech 1999), 
Budapest, Hungary, pp.2203-2206, 
1999 
 
[10]Z. Pan, K. Kotani, and T. Ohmi, “An 
on-line hierarchical method of speaker 
identification for large population,” in 
Proceedings of NORSIG 2000, 
Kolmarden, Sweden, 2000 
 
[11]T. Kinnunen, E. Karpov, and P. Franti, 
“A speaker pruning algorithm for real-
time speaker identification,” in 
Proceedings of Audio- and Video-
Based Biometric Authentication 
(AVBPA 2003), Guildford, UK, pp.639-
646, 2003 
 
[12]T. Kinnunen, E. Karpov and P. Franti, 
“Real-time speaker identification and 
verification,” IEEE Transactions on 
Audio, Speech, and Language 
Processsing, Vol.14, No.1, pp.277-288, 
2006
