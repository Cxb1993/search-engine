1 
行政院國家科學委員會補助專題研究計畫 ; 成 果 報 告   
□期中進度報告 
※※※※※※※※※※※※※※※※※※※※※※※※※※
※                                                ※ 
※        最佳動態解析度轉換之視訊編碼演算法           ※ 
※                                                ※ 
※※※※※※※※※※※※※※※※※※※※※※※※※※ 
計畫類別：; 個別型計畫  □ 整合型計畫 
計畫編號：NSC 94－2213－E－251－004－ 
執行期間：94年 8月 1日至 95年 7月 31日 
 
計畫主持人：王隆仁 
共同主持人：  
計畫參與人員：謝坤融 唐英倫 
 
 
成果報告類型(依經費核定清單規定繳交)：;精簡報告  □完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
;出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立屏東商業技術學院 資訊科技系 
 
中   華   民   國  95  年  9  月   16  日 
3 
    在傳統的MPEG[1]視訊壓縮模型中，位
元控制是藉由量化參數與編碼畫面率的自動
調整而執行。然而，如果發生在視訊內容變
化較大的場景之下編碼，運用動作補償來做
畫面間的預測其成效是不夠有效率的，並且
還需要更多的位元數量來描述場景。因此，
位元控制機制為了要減輕資料串流緩衝區的
負擔，通常會選取較低的編碼畫面率，因此
導致頻繁的圖框跳躍(frame-skipping)或重建
畫質較差的狀況。為了盡量降低此現象，動
態解析度轉換 (DRC, dynamic resolution 
conversion)[2]方法於是被提出，其編碼方式
是在視訊內容變化較大時使用縮小的解析
度，否則保留使用原高解析度來編碼。但是
縮小解析度後的編碼，在視訊重建時常會有
畫質較差的情形。因此本研究報告針對常用
的彩色影像空間的特性，提出一種快速立方
取樣法，用以改善 CSI[3][4][5]不易以方塊為
基準(block-based)的取樣方式，也較 CSI方法
更能適應於不同的彩色影像空間。而且，若
將此方法應用於MPEG-4 DRC，經實驗證實
可以大幅提高MPEG-4 DRC的編碼效率與畫
面品質。 
 
三、MPEG-4動態解析度轉換 
    高速數位資料的傳輸愈來愈常見，然而
對頻寬的需求卻比實際上所提供的還要多的
多。因此像是 H.263與MPEG-4[6][7]等強而
有力的影像編碼演算法就經常被推崇與應
用。但是這並非完美無瑕，因為在高度活動
的場景之下常會產生大量的位元量。為了保
持畫面的流暢性，位元控制機制便會略過某
些畫面不傳送以減少位元量的暴增，這就是
所謂的畫面跳躍。畫面的跳躍使畫面品質變
的低劣，因此就有動態解析度轉換(DRC)的
方法被提出來試圖解決這些畫面低劣的問
題。 
    在MPEG-4 DRC方法中每個畫面都會使
用一個簡單的演算法來判斷畫面是高度活動
性或低活動性的。當畫面是高度活動性時，
預測誤差區塊(prediction error block)將被以
原始解析度做二分之ㄧ的垂直與水平方向的
取樣，也就是縮小原始畫面的解析度，因此
傳送的畫面便是原始畫面的四分之ㄧ解析度
大小。畫面的解析度縮小了也節省了編碼每
張畫面的位元數，如此就能達到畫面的流暢
與降低畫面低劣的情形。詳細的步驟將在下
面的節次中作說明。為了儘可能的降低視訊
內容變化較大的(高度活動性)畫面在重建後
畫質更差的現象(畫面低劣)，MPEG-4採用縮
小的解析度編碼來替代原始解析度的方式。
解析度上的縮減節省編碼每張畫面所使用的
位元數，能夠達到較平順的移動也保存了較
高的重建畫質，稱為動態解析度轉換(DRC)
方法，說明如圖一。 
 
 
 
 
圖一、動態解析度轉換方式(DRC)的原理圖 
 
 
    圖一中左方為傳送端，右方為接收端，
傳送端及接收端透過網路連接。用虛線框起
來的部份，就是MPEG-4 DRC制訂的動態解
析度轉換方式。一般來說，將動態影像以資
料壓縮方式傳送，傳送端會先將與先前畫面
不同的部份抽出來，並將這部份資料再度進
行壓縮後做傳送。當畫面移動迅速時，由於
變化量增大，因此需要傳送的資料可能會超
出網路頻寬的上限。在過去的影像編解碼方
式中，如果發生這種情形，會改送出畫面速
率較低(畫面跳躍)的動態影像資料，因此接收
端看到的影像會變得不流暢。改以動態解析
度轉換方式來實作的話，當判斷出變化量增
大時，傳送端會先將變化部份縮小成四分之
一的尺寸後，再進行壓縮傳送，接著接收端
先把收到的資料解碼後擴大成四倍尺寸顯
示。這樣一來播放出來的影像仍然相當流
暢，且不會有畫面跳躍的現象。雖然因為經
過放大縮小處理，畫面變化部份的細膩度會
暫時下降，但只有畫面中變動較劇烈的地方
會受到影響，人眼並不容易看出畫質的劣化。 
 
MPEG-4 所制定的動態解析度轉換架
構，在縮小的解析度模式之下所提出的編碼
5 
至原始垂直與水平的尺寸之一半。然後此下
取樣(down-sample)預測誤差被編碼並且如同
MPEG-4 的方式被傳送。在接收端，被傳送
的預測誤差則被上取樣(up-sampled)至原始
的尺寸大小，然後再被附加到原始解析度的
預測上。由於預測影像之未更新的部分仍保
持原本之解析度，靜止區域的細節部份即便
在縮小的解析度模式下依舊能被保存下來。
因此，動態解析度轉換方法能改善編碼畫面
率(coding frame rate)且在畫面品質上沒有劣
化。 
 
六、實驗結果 
    為了測試所提出的方法，實驗結果將利
用主觀和客觀的評估方法來評斷，並將列出
本研究報告之實驗數據與影像。實驗的對象
是以下四種編碼方式：CSI-MP4、DRC-MP4、
LI-MP4與 CSP-MP4。並依據MPEG-4 Simple 
Profile 以及變動位元率 (TM5)所設計出來
的。所有實驗用之視訊檔皆為每秒 30張頁框
之頁框組 (group of frames)，頁框型態為
IPPPPPPPPPPPPPPPPPPPPPPPPPPPPPI ( P 
between I = 29)。 
為了比較各視訊編碼器的效能，除了挑
選"Stefan"、"Dancer"、"Foreman"、"Table"
等數個標準 CIF 格式(352 × 288)的視訊檔
外，另擷取一些較大格式(640×480)的影片檔
來做測試，其分別為 "尖峰時刻 (Rush 
hour)"、"張庭"、"魔戒(The Lord of The 
Rings)"影片的片段。在這所有視訊檔中，皆
會分別以 CSI-MP4、DRC-MP4、LI-MP4 與
CSP-MP4 編碼器做編碼，並將編解結果做效
能比較。同時，為了評斷影像重建品質的優
劣，除了利用主觀的人眼視覺來觀察原始視
訊與重建後視訊的差異外，並利用客觀的尖
峰訊雜比(Peak Signal-to-Noise Ratio, PSNR)
來證明本研究報告的 CSP編解碼器的視訊重
建畫質優於MPEG-4 DRC所採用的取樣與編
碼方式。 
 
(ㄧ) 主觀評估方法 
    主觀評估方法是利用人眼的視覺來做觀
察，就原始影像和重建後的影像做比較後，
可以很明顯地看出，CSI與 CSP的效能相似
且不論在小圖(352×288)或是大圖(640×480)
上都比其他編碼方式佳。 
圖六為標準 Stefan 視訊 (352×288)於
CSI、DRC、 LI 和 CSP 編碼器的內框
(Inter-frame)編碼結果，此重建後的視訊頁框
是由視訊之第 10 張影像所擷取出來的局部
放大畫面，使用的每秒編解碼畫面率(frame 
per second, fps)為 30fps，位元率為 900kbps。
其中(a)為原始視訊頁框。(b)為 CSI重建後的
視訊頁框。(c)為 CSP重建後的視訊頁框。(d)
為 DRC 重建後的視訊頁框。(e)LI 重建後的
視訊頁框。圖七為"張庭"視訊於 CSI、DRC、
LI 和 CSP 編碼器的內框(Inter-frame)編碼結
果，此重建後的視訊頁框是由視訊之第 2 張
影像所擷取出來的局部放大畫面，使用的每
秒編解碼畫面率(Frame Per Second;FPS)為
30fps，位元率為 240kbps。(a)為原始視訊頁
框。(b)為 CSI重建後的視訊頁框。(c)為 CSP
重建後的視訊頁框。(d)為 DRC 重建後的視
訊頁框。(e)LI重建後的視訊頁框。 
 
(a) (b) 
(c) (d) 
(e) 
 
圖六、Stefan(zoom in)影像序列(352×288)於位元
率為 900kbps時重建後的第 10張畫面 
 
7 
 
表二、LI、DRC、CSI與 CSP於解析度為 640×480之靜態影像下的效能比較表 
CSI CSP DRC LI CSI CSP DRC LI CSI CSP DRC LI
GIRL 38.26 36.87 36.70 36.11 42.88 51.81 51.30 51.75 42.39 50.98 50.05 50.32
JACKY 41.79 41.19 40.77 40.52 42.83 53.52 52.55 52.92 42.77 52.03 51.33 51.44
JACKY_1 38.07 36.41 36.10 35.89 42.78 53.76 51.21 51.26 42.46 51.24 49.79 49.81
JACKY_2 43.08 42.27 42.00 41.73 42.85 54.61 52.58 53.11 42.77 53.27 51.45 51.71
PARTNER 34.72 33.15 32.92 32.89 42.79 52.01 51.50 51.68 42.60 50.99 49.99 50.10
BUILDING 34.95 33.20 32.92 32.89 42.17 48.12 47.11 47.17 41.74 46.01 45.14 45.33
BUILDING_1 33.37 31.71 30.86 30.65 42.69 51.63 50.03 50.06 42.56 51.26 49.75 49.82
BAD GUY 37.84 36.51 35.58 35.38 42.49 51.82 50.23 50.46 42.64 50.96 49.38 49.68
MEN 42.52 41.55 41.27 40.87 42.80 52.01 51.40 51.44 42.34 50.37 48.87 48.99
OLD MAN 39.78 38.72 38.38 38.20 42.75 52.40 51.50 51.56 42.73 53.48 51.75 51.95
THREE MEN 41.77 40.32 39.87 39.51 42.68 51.95 50.73 51.12 42.18 49.01 47.78 48.17
Image
Name
Y,PSNR[dB] U,PSNR[dB] V,PSNR[dB]
 
 
 
(三) 與MPEG-4 DRC結果比較 
    本研究報告運用取樣與插補法於視訊壓
縮技術上，並與目前視訊壓縮標準的
MPEG-4 視訊編碼器結合(圖八)。將 CSI、
CSP、LI與 DRC分別搭配MPEG-4可以對連
續影像進行編碼。CSI 搭配 MPEG-4 即為
CSI_MPEG-4(以下簡稱 CSI)；CSP 搭配
MPEG-4 即 為 CSP_MPEG-4( 以 下 簡 稱
CSP)；LI搭配 MPEG-4即為 LI_MPEG-4(以
下簡稱 LI)； DRC 搭配 MPEG-4 即為
DRC_MPEG-4(以下簡稱 DRC)。 
 
圖八、運用壓縮比為 4:1的取樣與 1:4的插補法
之MPEG-4編解碼器 
 
    上一小節評比的是四種編碼方式在靜態
畫面下的效能，這一小節則使用動態影像來
比較四種編碼方式的優劣。首先使用標準格
式 CIF(352×288)的 Stefan 及 Dancer 視訊影
像，圖九是量化係數從 1至 31的條件下，四
種編碼方式對亮度元素(Y)編碼的效能之曲
線圖；圖十與圖十一分別是對灰度元素(U、
V) 編碼的效能之曲線圖。可以清楚地看到對
亮度元素(Y)編碼的效能還是 CSI最佳，編碼
效能由高至低為 CSI＞CSP＞DRC＞LI；至於
對灰度元素(U、V) 編碼的效能，編碼效能由
高至低為 CSP＞LI＞DRC＞CSI。 
最後，則是模擬實驗解析度為 640×480
電影【尖峰時刻】與【魔戒】中的一段視訊
影像， 量化係數從 1至 31的條件下，透過
四種編碼方式評比編碼效率。大圖(640×480)
的實驗結果幾乎與小圖(352×288)的結果大同
小異：四種編碼方式對解析度為 640×480 之
視訊影像而言，CSI對亮度元素(Y)的編碼效
能依舊是最佳的；至於對灰度元素(U、V) 編
碼的效能，編碼效能由高至低為 CSP＞LI＞
DRC＞CSI。 
 
七、結論 
  本研究報告實驗結果顯示，不論是解析
度為 352×588或是 640×480的Y圖元(亮度元
素)我們發現使用 CSI 編碼架構要比原始的
MPEG-4 動態解析度轉換的編碼效能明顯優
越，效能是四種編碼方式之冠，也就是說優
劣依序為 CSI＞CSP＞DRC＞LI，但由於 CSI
的運算過程與複雜度較高，並且不易以方塊
為基準(Block-based)的方式取樣，所以本研究
報告捨棄 CSI改採用編碼效能僅次於 CSI的
CSP；至於 U與 V圖元(灰度元素)，透過 CSI、
CSP、DRC 與 LI 四者取樣後的編碼效能由
高而低為 CSP＞LI>DRC＞CSI。因此本研究
報告建議使用 CSP 取樣 U 與 V 圖元。綜觀
CSP 的編碼效能在亮度或是灰度元素的編碼
效能皆比原本的MPEG-4 DRC為佳。總結，
本研究報告改變 DRC 原本的取樣方式，對
Y、U與 V圖元使用 CSP取樣，整體的編碼
效能都較原本的MPEG-4 DRC為佳。 
  相關的研究結果及其他探討，已分別發
表於 [8]、[9]論文，未來將進一步將主要結
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          95年 4月 3日 
報告人姓名  
王 隆 仁 
 
服務機構 
及職稱 
國立屏東商業技術學院 
資訊科技系 
副教授 
     時間 
會議 
     地點 
2006/3/28～2006/3/30 
 
法國南錫(Nancy) 
本會核定 
補助文號 
NSC 94-2213-E-251 -004 
 
會議 
名稱 
 (中文) 第三屆台法資訊技術國際研討會議 (TFIT 2006) 
 (英文) Third Taiwanese-French Conference on Information Technology (TFIT 2006)
 
發表 
論文 
題目 
 (中文) MPEG-4臉部動態參數之漸進式演算法 
 (英文) A Progressive Algorithm for MPEG-4 Facial Animation Parameters 
 
報告內容應包括下列各項： 
一、參加會議經過 
這次參加的第三屆台法資訊技術國際研討會議在法國南錫(Nancy)的 LORIA / INRIA
Lorraine舉辦，LORIA是 Lorraine Laboratory in Computer Science and its Applications之簡稱，
而 INRIA則是 National Research Institute for Computer Science and Automation之簡稱，是法國
國家研究院在電腦科學及其應用的研究單位。這次國際研討會除了來自法國、台灣，還有來
自全世界各地的許多學者、專家共襄盛舉，會議時間從 3月 28日起至 3月 30日止共舉行三
天，所研討的主題以多媒體(Multimedia)、資訊安全(Security)、即時/嵌入系統(Real Time / 
Embedded Systems)、正規驗證(Formal Verification)等四大主題為研究主軸。 
由於參與這次國際研討會議是自行前往，也是第一次去法國，而且南錫也並非觀光勝地，
因此往返的行程較為辛苦。於 3月 26日先從高雄小港機場搭乘港龍航空至香港機場，再從香
港搭乘國泰航空飛往法國巴黎，經過十多小時的飛行，於 3月 27日早上抵達巴黎戴高樂機場，
接著再搭乘快速鐵路 RER B至 Paris Est車站，再轉搭往 Strasbourg的快車，於 Nancy車站下
車，下車後帶著行李花了很多時間才找到預訂的旅館，時間已是傍晚了。旅館正好在 Place 
Stanislas 廣場旁邊，這個廣場擁有優美的歐式鐵門、噴泉，及華麗的大廣場，同時此時有許
許多多的法國人下班後在這裡散步，有些人則坐在廣場旁邊的咖啡店喝咖啡、聊天，看他們
愉悅的神情，感覺法國人很會享受生活。 
3 月 28 日起得很早，因為從旅館至 LORIA 會議地點，不知道要花多少時間。我先研究
過地圖並在馬路旁的車站，搭乘市內電車 Tram1，於 Callot 車站下車，再步行到會議地點參
加研討會。 
3月 28日(星期二)上午 10:00報到後，由大會副主席 Dr. Samuel Cruz-Lara在地點 Room 
A008主持，並説明整個議程，及介紹與會的貴賓。午餐後，13:30-16:00分成兩個主題進行：
(1)多媒體(Multimedia)：由 Dr. Yi-Ping Hung主持，地點在 Room A008；(2)資訊安全(Security)：
由 Dr. Tzong-Chen Wu主持，地點在 Room B013。因為我被安排在多媒體(Multimedia)主題今
天下午的第二位報告者，所以我就參加 Room A008的研討會。我們這一主題與會的學者相當
多，下午共有六人要報告，因此小小會議室擠滿了許多人。我是第二位報告者，時間
13:55-14:20， 題目為A Progressive Algorithm for MPEG-4 Facial Animation Parameters，報告完
後有兩位學者發問，同時 Chair 也問了兩個問題，對於問題我都能當場回答，部份寶貴的建
議，也可提供我未來進一步研究參考用。16:30-17:30 Keynote Speech由 Dr. Keith Baker主講
Role of Invention in the Age of Mass Customization。 
 
???
 
表 Y04 
 
五、攜回資料名稱及內容 
1. 研討會論文集一冊(Proceedings of the Third Taiwanese-French Conference on Information 
Technology，ISBN：2-905267-48-8)。 
2. 研討會參與人員名單 
3. LORIA / INRIA Lorraine簡介資料 
 
六、其他 
1. 發表論文全文(如附件) 
L. J. Wang, “A progressive algorithm for MPEG-4 facial animation parameters,” in Proc. of Third 
Taiwanese-French Conference on Information Technology(TFIT 2006), Nancy, France, March 28-30, 2006. 
 
2. 本研討會網址 http://tfit2006.loria.fr 
 
 
 
表 Y04 
system and better quality in encode or decode FAP stream file. 
2   Facial Animation 
Motivated by the need of integrating natural and synthetic contents in multimedia applications, analysis and 
compression of synthetic image data have recently drawn great attention. The activities of synthetic and natural hybrid 
coding (SNHC) group under MPEG-4 show great effort in this area. Particularly, a facial animation parameter set is 
being standardized which will allow the communication between different application interfaces such as teleconference, 
Internet agents, etc, through a common control parameter stream. 
MPEG-4 enables the animation of a proprietary face model or a face model download to the 
MPEG-4 client [3]. For the face model, MPEG-4 defines FP’s that are animated by the FAPs. 
Synchronization of animated faces with other media streams like audio is achieved using time 
stamps in the FAP stream. Decoded audio-visual objects can be composed into 2D and 3D scenes 
using the Binary Format for Scenes (BIFS) [5]. MPEG-4 specifies a 3D face model and its 
animation using face definition parameter (FDP), facial animation parameter (FAP), and FAP 
interpolation table (FIT) as effective animated tools. 
2.1   Facial Animation Parameter (FAP) 
FAPs are based on the study of minimal perceptible actions (MPA) and closed related to muscle actions [4]. They have 
68 parameters and categorize into 10 groups as shown in Table 1. FAPs represent a complete set of basic facial actions 
such as head motion, tongue, eye, and mouth. FAPs in group 1 represent visemes and expressions and high-level 
parameters. FAPs in groups 2 to 10 are considered low-level parameters. FAP 1 defined for coarticulation of speech and 
mouth movement [8]. FAP 2 (expressions) defines six primary facial expressions. 
2.2   Facial Definition Parameter (FDP) 
FDPs are designed to enable the customization of a proprietary 3D facial model at the receiver or the downloading of a 
new model at the sender. Because of configuring the facial model, they are sent only once per session, particularly in 
some applications as shown in Fig.1. They are encoded in BIFS, using the nodes specified in the MPEG-4 systems 
standard. In order to minimize differences in decoder, the encoder should send enough FDP configuration data [5]. 
表 Y04 
3.1   Calculation of Weight 
By calculating the movement displacements of FAPs, it is useful to establish the weight α which calculated by FAPs 
movements. The weight α to interpolate other FAPs [10] is defined as 
nn FAPjFAPi ⋅= α  (1) 
where n is the value of the FAP in correspondence of the nth frame of an image sequence with N frames, nFAPi  is the 
parameter to be interpolated, nFAPj  is the parameter actually to be transmitted, and α is defined by 
ementtalDisplacFrameFAPTomentntDisplaceFAPsMoveme  / =α  (2) 
Fig. 2 shows that the face model simulated by frames changing. FAPs can be animated because of 
using mapping actions method. From frame 1 to frame 5, FAPs in the face model have the position 
displacements. Using the total displacement of frame FAPs, the weight α can be computed and 
also the other FAPs can be interpolated. 
 
Fig. 2. Action variation of FAPs between frames 
 
3.2   A GC FAP Algorithm 
Original MEPG-4 FAPs can be simplified by the proposed GC algorithm that used to decide which FAPs will be 
transmitted and which FAPs will be interpolated. Consider FAPs as the vertexes and the relationship of FAPs as the 
edges between FAPs. The proposed algorithm will choose and color each FAPs vertex, that is, uncolored FAPs vertex is 
0, and colored FAP vertex is 1. After all FAPs vertexes are traversal, their colors are all 1. The proposed GC FAP 
algorithm is described as follows: 
Step 1: Create the FAP correlation matrix R, computed [10] 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
MMM
ij
M
rr
r
rr
R
L
LL
L
1
111
 (3) 
where M is the number of FAPs under investigation and ijr  is the correlation coefficient between the i
th and jth 
FAPs, defined as 
表 Y04 
4.1   GC FAPs Selection 
Because FAP groups 1, 3, 6, 9, and 10 are hard to simulate by the face model and less interrelate measuring the human 
face. So the proposed method ignores these FAP groups and only considers the FAP groups 2, 4, 5, 7, and 8 as the 
adopting data. Observing the FAPs corresponding actions movements helps this method to calculate and analysis the 
relationships between each FAPs when they are active. At first, selecting FAP is based on some FAP correlations 
matrix [10]. The correlation matrix table is shown as Tables 2 and 3. The correlation value in (4) to improve the FAP 
correlation matrix is computed. Furthermore, in order to use the GC algorithm to improve FAPs relationship graph, the 
threshold value γ selects 0.75 experimentally. This method uses the γ > 0.75 correlation values as 1 and others are 
0 in the relationship matrix. Then the relationship matrix of FAP is built as shown in Tables 4 and 5. 
After the relationship matrixes are created, next is to create the adjacency matrix for FAPs 
relationship graph as shown in Figs. 4 and 5. These vertex nodes of the adjacency matrix are FAPs, 
and the edges are the relationship between FAPs. In the adjacency matrix, 0 means not connected to 
other FAPs, and 1 means connected to some other FAPs. 
According to the adjacency matrix as Figs. 4 and 5, a FAP graph immediately is created as shown in 
Fig. 6. The FAP graph is divided to 6 sub-graphs, each sub-graph means one relationship. If the root 
in the sub-graph is decided, it is explicitly that using the weight relationship can calculate the other 
FAPs. 
Table 2. Group 2, 5, and 8 matrix 
 
 
Table 3. Group 4 matrix Table 4. Group 4 relationship matrix 
 
表 Y04 
 
Table 6. Weight of FAP3, 16, 14, 52, 57 Table 7. Weight of FAP31, 35, 33, 37 
 
  
Table 8. Weight of FAP51, 55 Table 9. Weight of FAP53, 39 
 
 
Table 10. Weight of FAP59, 41 
 
4.3   FAPs Pass Sequence 
After the weight of FAPs and FAP graph computed, the progressive priority FAPs Pass sequence is created. Fig. 7 
shows that FAP graph Pass sequence procedure. 
 
Fig. 7. FAP graph Pass sequence procedure 
4.4   PSNR Results 
In this section, the proposed GC FAPs method comparing with the MPEG-4 FAPs standard is computed. The PSNR 
(Peak Signal-to-Noise Ratio) shows a better objective quality in performance of “expression.fap” stream file based on 
the I.S.T model which is utilized in the MPEG-4 SNHC compatible 3D facial animation system. The experiment takes 
the frame from 30 to 75 and from 575 to 620 of the “expression” stream file for testing. The PSNR of the proposed 9 
FAPs system is calculated by using the standard MPEG-4 68 FAPs system as the reference source. The average PSNR 
is for 40-50 bits/frame as shown in Figs. 8 and 9. Furthermore, Table 11 shows the number of the selected FAPs and 
their corresponding bit rates. Finally, Fig. 10 shows the PSNR of the selected FAPs with various bit rates. 
