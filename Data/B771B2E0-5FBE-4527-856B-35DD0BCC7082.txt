 2
 
計畫編號：NSC 97-2221-E-011-104 
執行期限：97 年 8 月 1 日至 98 年 7 月 31 日 
主持人：楊英魁   國立台灣科技大學 電機工程研究所 
 
 
一、中文摘要 
 
本 計 畫 提 出 三 種 自 動 學 習 架 構 
(learning framework) 來處理自網際網路
中自動抽取 (automatic extraction) 音
譯詞組的問題。在自動抽取過程中可以透
過 中 英 文 字 詞 間 的 音 相 似 度 模 型 
(phonetic similarity model, PSM) 來計
算音相似度，這裡所指的音相似度模型包
含混淆音矩陣 (confusion matrix) 及中
文 n-gram 語言模型 (Chinese n-gram 
language model) 。使用這個音相似度模
型，音譯詞組自動抽取過程變成包含有『辨
識  (recognition) 』 及 『 驗 證 
(validation) 』兩個步驟：首先，在辨識
過程，先找出一個英文字，然後在英文字
附近的上下文語境 (context) 中找出其最
有可能的中文候選詞 (candidate) ；其次
在 驗 證 過 程 中 ， 經 由 假 設 檢 驗 
(hypothesis test) 來篩選 (select) 候
選詞，以確認最後可能的音譯詞。在計畫
中還針對用做效能評量 (performance 
evaluation) 的音譯詞組集合進行了統計
分析，以便對音譯的特性有更進一步了
解，進而更準確的模型化 (model) 音譯規
則，從而改善音譯詞組自動抽取的效能。 
關鍵詞：音譯詞組，機器音譯，音相似度
模型，主動式學習 
 
Abstract 
 
This research proposes three learning 
frameworks for the automatic transliteration 
extraction from the Web. We formulate the 
machine transliteration process using a 
phonetic similarity model (PSM) which 
consists of phonetic confusion matrices and a 
Chinese character n-gram language model. 
With the phonetic similarity model, the 
extraction of transliteration pairs becomes a 
two-step process of recognition followed by 
validation: First, in the recognition process, 
we identify the most probable transliteration 
in the k-neighborhood of a spotted English 
word. Then, in the validation process, we 
qualify the transliteration pair candidates 
with a hypothesis test. We also carry out an 
analytical study on the statistics of several 
key factors, such as lexical variation and 
phonetic variation, which result in casual 
transliteration, in English-Chinese 
transliteration to help formulation of the 
phonetic similarity modeling. 
 
Keywords: transliteration pairs， machine 
transliteration ，  phonetic similarity     
model， active learning 
 
 4
更進一步了解，進而更準確的模型化 
(model) 音譯規則，進而改善音譯詞自動
抽取的效能。 
  
三、研究結果與討論 
   
 機器音譯研究有兩類：音譯模型建立 
(TM) 和音譯詞組抽取 (TE) 。本計畫所提
之 PSM 模型得利於這兩項研究，它採用了
音譯模型建立方法去處理音譯詞組抽取問
題。 
音相似模型 (PSM) 是由依循吵雜通道
框架下的生成模型推導出來，它融合 
(fuse) 了多層次的知識來源，這些知識來
源包括有中文羅馬拼音、 語言學規則及音
素和音節。這個啟發是來自在音譯研究
上，研究人員努力找尋不同層次的知識來
源來處理音譯問題，舉例來說， 利用音相
似度來找尋音譯詞,以音素為單元的專有
名詞音譯,直接文字映射框架，在這個 PSM 
模型下，在音譯詞抽取過程中有效的融合
了三個層次的知識來源。  
在本計畫的實驗中，本計畫的方法可以
達到不錯的效能，但仍有改善的空間。因
此為了改善音譯詞組抽取效能，在此更進
一步研究 SET1 以及它的獨特音譯詞組的
一些特性。為了計算音相似度，中文字先
被轉換成音節，因此暸解音節轉換的複雜
度將有助於處理此問題。 
在整個 SET1 中總共使用了 80,501 個
中文字，其中有 3,595 個不同的中文字；
而在中文獨特合格音譯詞組 (DQTPs) 中則
總共只有使用 7,902 ，其中有 1,210 個
不同的中文字，在這樣一個以中文為主的
中英文混合語料庫中中文字數幾倍於英文
字數，可以發現這樣的特性也增加了音節
對應的複雜度。另外中英文音節比例也反
映出 PSM 模型的複雜度，在整個 SET1 中
發現分別有 394 和 1,012 不同的中文及
英文音節；在所有獨特合格音譯詞組 
(DQTPs) 中則分別有 333 和 824 不同的
中文及英文音節，換句話說，平均而言每
個中文音節大約可對應至 2.5 英文音節，
若以手工撰寫這些音節對應規則，這將是
一個極大的挑戰。在本計畫裡， SCM 及 PCM 
等 PSM 模型參數可以有效的自語料庫中學
習得到，而不需以人工來建立。 
自 SET2 及 SET3 語料庫中運用『先辨
識後驗證』演算法抽取出了大量音譯詞
組，這些抽取出來的音譯詞組形成了一個
雙語詞典，如果可以進一步分析這一個抽
取出來雙語詞典將有助於了解音譯特性。  
以抽取出來的雙語詞典中的英文詞與
卡 內 基 美 隆 大 學  (Carnegie Mellon 
University, CMU) 發音詞典1及 Shorter 
牛津英語詞典2比較，發現分別有 31.1% 及 
47.8% 的英文字不在各自的詞典中，因此
使用 PSM 模型不僅可以從網際網路上抽取
出既新且真實音譯詞組，也獲得了大量新
____________________________________ 
 
 
 6
抽取過程。辨認是透過動態規劃的搜尋策
略找出英文字詞並經中英文字詞的音相似
度計算，找出最有可能的中文音譯候選
詞，最後透過驗證也就是假設測試，確認
最有可能的中文音譯候選詞是否就是真正
的音譯詞。使用『先辨認後驗證』的音譯
詞抽取過程可以從不同的網路語料庫 (包
含一般網頁資料、超連結資料以及從網路
搜尋引擎收集回來的雙語摘要) 中來建構
中英文音譯詞典。因此提供了一個低成本
的替代方案可以從動態網頁中找出新的音
譯詞組。 
( 三 ) 提 出 非 監 督 式 學 習 
(unsupervised learning) 、主動式學習 
(active learning) 等 兩 大 學 習 架 構  
(learning framework) 來抽取音譯詞組。
非監督式學習利用自動語音辨識產生的混
淆音矩陣來提昇 (bootstrap) 初始的 PSM 
模型，進而改善音譯詞組抽取效能。主動
式學習則篩選最富有資訊的樣本加以人工
標記，透過這種方式可大幅減少在監督式
學習 (supervised learning) 時需要人工
標記的樣本數量。使用這兩種學習演算法
都可得到與監督式學習非常接近到的效
能。  
( 四 ) 提 出 利 用 中 英 文 詞 共 現 
(co-occurrence) 資訊來抽取音譯詞的策
略。這個策略被用來改善初始的 PSM 模
型，進而改善整體的音譯詞組抽取效能。  
 
從本計畫的實驗結果也確認了應用於
中英文音譯詞組的 PSM 模型的有效性，在
不失一般性的狀況下，這個架構同樣也可
以應用於其它語言對，諸如英-日文及英-
韓文等等。雖然在本計畫中實驗是先從中
文網頁開始，但也成功的擴充這個架構至
超連結文字語料庫上。  
在本計畫中還發現，網際網路是一個
很生活化的語料庫，利用這個語料庫可以
來建構真實 (特別是非正規音譯詞) 的中
英文音譯詞典。善用這些音譯詞組，將有
助於跨語言檢索及專有名詞辨識等自然語
言處理研究。 
 
四、重要參考文獻 
 
 
E. Brill, G. Kacmarcik, C. Brockett. 2001. 
Automatically Harvesting 
Katakana-English Term Pairs from Search 
Engine Query Logs, In Proc. of NLPPRS, 
pp. 393-399. 
S. Brin and L. Page. 1998. The Anatomy of a 
Large-scale Hypertextual Web Search 
Engine, In Proc. of 7th WWW, pp. 107-117. 
H.-H. Chen, W.-C. Lin, C.-H. Yang and 
W.-H. Lin. 2006, 
Translating-Transliterating Named Entities 
for Multilingual Information Access, 
Journal of the American Society for 
Information Science and Technology, 
57(5), pp. 645-659. 
 8
Mining New Translations, In Proc. of 27th 
ACM SIGIR, pp. 289-296. 
W.-H. Lu, L.-F. Chien and H.-J. Lee. 2002. 
Translation of Web Queries Using Anchor 
Text Mining, ACM TALIP, Vol. 1, Issue 2, 
pp. 159- 172. 
A. McCallum and K. Nigam. 1998. 
Employing EM in Pool-based Active 
Learning for Text Classification, In Proc. 
of 15th International Conference on 
Machine Learning, pp. 350-358. 
H. M. Meng, W.-K. Lo, B. Chen and T. Tang. 
2001. Generate Phonetic Cognates to 
Handle Name Entities in English-Chinese 
Cross-Language Spoken Document 
Retrieval, In Proc. of ASRU, pp. 311-314. 
J.-Y. Nie, P. Isabelle, M. Simard, and R. 
Durand. 1999. Cross-language Information 
Retrieval based on Parallel Texts and 
Automatic Mining of Parallel Text from 
the Web, In Proc. of 22nd ACM SIGIR, pp 
74-81. 
V. Pagel, K. Lenzo and A. Black. 1998. 
Letter to Sound Rules for Accented 
Lexicon Compression, In Proc. of ICSLP, 
pp. 2015-2020. 
R. Rapp. 1999. Automatic Identification of 
Word Translations from Unrelated English 
and German Corpora, In Proc. of 37th ACL, 
pp. 519-526. 
G. Riccardi and D. Hakkani-Tür. 2005. 
Active Learning: Theory and Applications 
to Automatic Speech Recognition, IEEE 
Transactions on speech and Audio 
Processing, Vol. 13, No. 4, pp. 504-511. 
R. Sproat, T. Tao and ChengXiang Zhai. 
2006. Named Entity Transliteration with 
Comparable Corpora, In Proc. of 44th ACL, 
pp. 73-80. 
P. Virga and S. Khudanpur. 2003. 
Transliteration of Proper Names in 
Cross-Lingual Information Retrieval, In 
Proc. of 41st ACL Workshop on 
Multilingual and Mixed Language Named 
Entity Recognition, pp. 57-64. 
S. Wan and C. M. Verspoor. 1998. 
Automatic English-Chinese Name 
Transliteration for Development of 
Multilingual Resources, In Proc. of 17th 
COLING and 36th ACL, pp.1352-1356. 
C. Zhang and T. Chen. 2002. An Active 
Learning Framework for Content-based 
Information Retrieval, IEEE Transactions 
on Multimedia, 4(2), pp. 260-268.
 1
參加 ICAI2009研討會心得報告 
報告人： 楊英魁  2009.07.14 
時間： 2009年 7月 13日 ~ 2009年 7月 16日 
地點： Las Vegas, USA 
報告內容： 
這次在 Las Vegas, USA為期四天所舉行的研討會 The 2009 International Conference on 
Artificial Intelligence (ICAI2009)，是學術界在人工智慧與控制領域上重要的一次會議。ICAI2009
是WORLDCOMP09 (The 2009 World Congress in Computer Science, Computer Engineering, 
and Applied Computing)中 25個研討會之一。由於它的重要性，所以有另外九個與人工智慧領
域有關的研討會一起舉行。總共有來自全球不同領域的超過兩千三百個專家學者參與，氣氛熱
絡，連當地旅館都不容易定到，每位 Keynote Speaker都是在人工智慧領域裡大師級的人物。 
參加這次 ICAI2009，主要是去發表一篇由國科會所支持研究的成果論文：A CMAC Learning 
Approach Based on Grey Relational Ananlysis。發表此論文時，大約有 70幾位專家學者參與
討論，氣氛對非常熱絡，對此論文所提出的方法與理論，與會者都極為肯定。 
這幾天期間，與各地學者專家深入討論各個不同的領域，受益良多，也能正確的掌握目前
人工智慧的領域，尤其是每天第一場的 keynote speech更是精采。主講者不但學術豐富，有幽
默感，而且明確指出今後在此領域上可以進行的幾個方向，足以當作最好的參考。 
參加此研討會，不但有機會與來自世界各地的學者專家廣泛討論，相互切磋，也因此更確
定目前所進行的研究方向是正確的。而且同時可以參加六個相關研討會，真是不虛此行
 3
number of trainings, and the concept of adaptive grey learning rate in a CMAC model in order to 
mitigate the influence of learning interference so that the learning speed and output accuracy can be 
effectively improved. During the learning phase, the grey relational coefficient is calculated for each 
input state after a learning iteration. An appropriate grey learning rate is then derived by incorporating 
the calculated grey relational coefficient with the number of learning iterations. Then the credit 
distributed to an addressed hyper cube is in inverse proportion to the number of trainings.  
 
2. CMAC Architecture 
The basic structure of a conventional CMAC model [1] is shown in Figure 1. A CMAC model 
quantizes the learning space into several discrete states that serve as input states of the CMAC model 
and are represented as set S in Figure 1. Each input state is mapped from indexed memory A to the 
corresponding real memory cells W that store the input states information and are summed to produce 
actual output value. A mapping block is a hyper cube between axes. The total hyper cubes are real 
memory cells that store relational information regarding addressed input states.  
1S
kS
S A
W
∑
l e a r n i n g
s p a c e
m e m o r y
i n d i c e s
m e m o r y
c e l l s
y
a c t u a l
o u t p u t
∑
yˆ t a r g e to u t p u t
+
−e r r o r
  
Figure1. Basic structure of a CMAC model 
If there are Nh hyper cubes in which each input state maps to Ne hyper cubes, then the actual 
output is shown in following Equation (1). The ys denotes the actual system output for input state s, 
T
sa  is the indexed vector, and w is the memory cell vector.  
 
                                                                   (1) 
 
 
 
 
During the learning phase, the error of actual output value to the desired output value is uniformly 
distributed to regulate and train the memory cells of a CMAC model. The weight relation between 
before and after trainings of a memory cell is shown in Equation (2). 
( 1)
( ) ( 1)
,
( )T ii i ss
j j s j
yw w a
N e
α
∧ −
− −= + ⋅ ⋅ a w                 (2) 
In Equation (2), s denotes an input state, ( )ijw  represents the weight of j-th hyper cube in the training 
iteration of number i, ,s ja  is an index vector for input state s and hyper cube of number j, 
1
2
,1 ,2 , ,
1
[ , , , ]
h
h
h
N
T
s s s s N s s j j
j
N
w
w
y a a a a w
w
=
⎡ ⎤⎢ ⎥⎢ ⎥= = =⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦
∑a wL M
 5
CMAC at every state is denoted as )}(),...,2(),1({ nyyyy = . According to Equation (3), the grey 
relational coefficient for input state ks  in a single comparison sequence can be written as follows. 
max
maxmin
)(
))(ˆ),(( Δ⋅+Δ
Δ⋅+Δ= ξ
ξ
k
kykyGRC
y
                            (5) 
where )(ˆ)()( kykyky −=Δ  , )(maxmax kyk Δ=Δ , )(minmin kyk Δ=Δ  and 1))(ˆ),((1max
min
≤≤+
+Δ
Δ
kykyGRCξ
ξ . 
Because both maxΔ  and minΔ  are constant values for a CMAC model, the grey relational 
coefficient increases with decreasing output error )(kyΔ . 
During the learning phase, the output errors of input states should be also related to the inverse of 
the number of training iterations. Therefore, this paper proposes an adaptive regulation of learning rate, 
termed grey learning rate, that is based on the number of training iterations and grey relational 
coefficients of input states in a CMAC model. During the i-th training iteration, the grey learning rate 
at the input state ks  is proposed as follows.   
))(),((
)(
)1(
1)(_
kykyGRC
i
i
i
kgrey ∧−=α                            (6) 
where ))(),(()1( kykyGRC i
∧−  is the grey relational coefficient at state ks  in the (i-1)-th training 
iteration. Initially, 1))(),(()0( =∧ kykyGRC  and 1)(_ )1( =kgrey α . When the training iteration i becomes 
larger, it means the systems has been trained more times already. Similarly, when the grey relational 
coefficient GRC becomes large, it means the system is closing to the final state. In these two cases, 
Equation (6) shows that the learning rate should be a smaller value. That is, the system is tuned by 
smaller changes to avoid any overshooting or instability. 
 
3.3 Grey-area-time Credit Apportionment  
  A hyper cube that includes more input states is more influenced by the learning interference 
during the learning phase. To mitigate this effect, the distribution of errors among the addressed hyper 
cubes must be proportional to the hyper cube creditability. The key information available for use as 
credit is the number of times a hyper cube has been updated [8][9]. In addition, conceptually, the 
accuracy of the stored weights in hyper cubes should increases with the number of input states during a 
learning phase. For this, the trained proportion of input states is proposed to be considered as one 
factor of creditability. Further, as discussed previously, an adaptive learning rate is necessary to avoid 
an unstable system. This means the grey relational grade in a hyper cube should be also a factor 
involving creditability of the hyper cube. Consequently, the number of updated times for hyper cubes, 
the trained proportion of input states and grey relational grade in a hypercube can be integrated to 
provide an indicator of hyper cube creditability. The credibility, termed grey-area-time, is defined as 
shown below. 
∑
=
−
−
××+
××+=−− m
c
ii
ii
i
jGRGjact
jGRGjajtjtimeareagrey
1
1)()(
1)()(
)(
))(()()1)(((
))()()1)((()(             (7) 
 7
(RMSE) is employed for comparison. Figure 2 shows the performance of different CMAC models. It is 
observed in Figure 2 that the proposed Grey-area-time CMAC has the best performance. Furthermore, 
Grey-area-time CMAC has been stable and achieved convergence faster than other three methods. 
 
Learnig comparisons
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
1 2 3 4 5 6 7 8 9 10
training iteration
RM
SE
 er
ro
r Conventional CMAC
Time-credit CMAC
Fuzzy-time-credit CMAC
Grey-area-time CMAC
 
Figure 2: Learning comparison for different CMAC models 
 
Example 2:  
 The target function is 2 2( , ) ( ) sin 5z x y x y x= −  where -1< x < 1 and -1< y < 1. The 
distinguishing coefficient is set to 1.0=ξ . Learning comparison adopts root mean square error 
(RMSE). Figure 3 shows the performance for different CMAC models. It can be observed from Figure 
3 that the proposed Grey-area-time CMAC results in less RMSE than other three models since the first 
learning cycle. Furthermore, the RMSE monotonically decreases in the Grey-area-time CMAC until it 
stabilizes after 9 learning cycles. 
Learning comparisons
0
0.01
0.02
0.03
0.04
1 2 3 4 5 6 7 8 9 10
training iteration
RM
SE
 er
ro
r Conventional CMAC
Time-credit CMAC
Fuzzy-time-credit CMAC
Grey-area-time CMAC
 
Figure 3: Learning Comparisons for different CMAC models   
 
5. Conclusions 
 This paper presents an enhanced strategy for creating a novel learning framework for the CMAC 
model. The accumulated frequency of updating to the hyper cubes [2][6], the trained proportion of 
input states and grey relational grade of hyper cubes are integrated into a measure of the credibility of 
the hyper cubes for each input state. This paper also considers the adaptive regulation of learning rate 
with the number of training iterations and grey relational coefficients in the CMAC model. The credit 
