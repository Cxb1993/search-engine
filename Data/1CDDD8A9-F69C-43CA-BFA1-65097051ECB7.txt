摘要 
本計畫為兩年期國科會整合型計畫-子計畫三，題目為：「LTE/LTE-A為基礎之次世代網路建置
技之研究(子計畫三)：LTE/LTE-A 服務品質保證下允入控制與排器關係之跨層設計研究」。成果報
告主要貢獻包括 3項：1. Femtocell 微型基地台之發射功率控制機制，以降低 FBS與MU之間的信
號干擾；2. Femtocell 微型基地台干擾之降低，期在相同情況下，收容更多之使用者；3. DL subframe
之排程最佳化，以獲的較高之 slot使用率等。其中 1、2項已發表，3項進行中。以下就整合計畫執
結果分為六個章節說明：1)已發表二篇文章；2)三項研究概況；3)研究工作成果與評估；4)結論與未
來展望，5)參考資料及 6)附錄。 
關鍵詞: 演算法、微型基地台、博弈理論、允入控制、排程機制 
1. 已發表文章(共計三篇國際期刊(SCI)、二篇國際會議論文) 
在第四代行動通信中，由於提高用戶收容數、增加頻譜使用效能已成為主要研究項目。在論
文附錄 A中，我們發現，由於微型基地台(femto base station, FBS)，不可預期地被部署在
Macrocell的區域當中，對於用戶的干擾以及 FBS間下行的干擾造成傳輸的問題。本研究中，我
們描述了採用非合作博奕理論系統模型。我們製定一個實用功能，提供 FBS適當的功率強度去最
小化干擾。我們的功率控制演算法是建基在博奕模型並可以利用到集中式之環境。這種新穎的功
率控制演算法使發射功率達到一個穩定的狀態，即奈許均衡。通過數學分析和數值結果，我們證
明了我們提出的功率控制演算法有幾個好的特點。 
在在論文附錄 B中，討論 FBS使用子載波功率控制和分配機制，以避免移動用戶與其所從屬
的基地台之間不必要的共頻道干擾(co-channel interference, CCI)是一個自組織網路
(self-organizing network, SON)新興的研究議題。本論文不僅涉及 CCI問題的解決，同時也考慮
在可用頻帶飽和且滿足微型基地台使用者(FU)服務品質(QoS)的要求下最大化頻道的利用率。 
有些功率控制的方法利用定期測量從微型基地台使用者發出的回饋信號執行動態 femtocell
覆蓋範圍調整。有研究[3]提出一個微型基地台試點測量方法，它依賴微型基地台使用者頻繁的將
信號強度回報給微型基地台以保持其覆蓋範圍。然而，它使微型基地台使用者額外消耗能量去回
報信號強度的資訊給微型基地台。另有研究[4]提出 switched multi-element antennas (SMA)方
法，使用換手事件的發生協助調整微型基地台的覆蓋率。然而這個方法利用從基地台不想要的換
手要求去最佳化微型基地台的功率控制，這會招致許多不需要的換手要求並降低系統效能。 
在 complementary TRi-control loops (CTRL) [5]利用稱為無線資源管理(resource 
management, RRM)的額外硬體去收集通過回程網路中Macrocell的回饋。Macrocell送出負載餘
量資訊給 RRM去保障Macrocell 上行的通運。RRM可以集中管理微型基地台的干擾以達到奈
許均衡。從微型基地台和基地台到 RRM的大量反饋，和 RRM所傳送的控制訊息都需要在頻帶
和回程網路中花費高開銷。 
另一個種用來避免基地台和微型基地台之間的干擾是利用子載波排程機制[6]去動態排程在
DL和 UL的傳輸子載波以求避免碰撞。這個機制從微型基地台預先計劃子載波的分配以避免不
要的碰撞中獲益。然而動態的子載波排會在基地台使用者數量多的時候浪費 femtocell的頻帶，
因為被相鄰基地台使用者佔用的頻帶不能重複使用。在熱點存取情況，微型基地台需要更多的頻
率資源去支援更多的微型基地台使用者。 
 
對。這種利己的行為將導致整個網路，包括基地台與微型基地台形成一個不好的均衡。我們提出
一種實用的非合作博奕方法，目標是有效地利用有限的頻段，服務所有網路中的用戶。 
我們假設微型基地台是參與一個 N個玩家的非合作賽局 { } ( ){ }[ ].,, iin UPBG = ，其中
{ }Nn ,,1= ，表示玩家的索引集合，Pi為 Si領域中發射功率策略集合，其最終目標在藉由 utility 
function Ui 以求得所有用戶之最大各別收益(發射功率)。藉此，所有玩家依其他參與者的行動，
選擇適當之發射功率做出最佳之反應。給定 BSi，並給定 p-i做為對除了 Bi以外的所有基地台的傳
輸功率向量。 
我們定義，對於所有 0 ≤ i ≤ N的用戶，功率控制的賽局可以被表示為 
  ( )iiip pUi −,max γ          (3) 
對於除了 p0以外的所有 pi，我們假設它們的傳輸功率上限皆為 pmax。我們對於在(3)當中找出
每個用戶在 N個人當中最大化其效用的平衡點。這種均衡優化的問題稱為奈取均衡，沒有用戶可
以在不考慮其他用戶的狀況下提升效能。此外，我們假設所有用戶定期通報其服務基地台它們目
前的 SINR和每個基地台發射的 SINR值藉由通過回程來計算基地台的發射功率以求達到奈許均
衡並且基地台分配平衡的功率給微型基地台。 
由於所有的微型基地台皆會參加功率控制的賽局，微型基地台各自調整它們的功率去達成奈
許均衡。在這當中，B0的傳輸功率將會根據與微型基地台用戶的距離影響參與者的策略，但策略
的改變並不會影響 B0的功率。鑑於 intering功率 p-i與當前 SINR γi，基地台獲得一個使用的集
合 ( )iii pU −,γ 。為避免太多的傳輸功率將會影響基地台的用戶以及它鄰近的的微型基地台，我們建
模一個 Bi的效用函式(utility function)，即當 0≠i ，Bi被以下來個函式結合起來。 
( ) ( ) ( )iiiiiii pCbRpU −− +Γ= ,, γγ       (4) 
其中： 
報酬函式(Reward function) ( )iiR Γ,γ ，是由用戶當前的 SINR值γi與被要求最低的 SINR
值Γi所組成。這個函數表示，如何在當前接收的 SINR值下 Si獲得多少利益。 
懲罰函式(Penalty function) ( )ipC − ，記錄所有 Si所遭受的干擾。這個函數 C抵消 Si從其他
微型基地台得到的跨層干擾以及接收到的傳輸功率所獲得的 utility。 
 使用[12]的架構，對於第 i個用戶，我們假設效用函式 Ui對於其 SINR值γi，是一個單調
遞增凹型向上函式(monotonically increasing concave upward function)，當 p-i值已知，且當前
SINR值γi超越Γi時，前述假設成立。若 Ui對於其他 BS之發射功率 p-i為單調遞減凸型向下函
式monotonically decreasing concave downward function)時，越多的干擾，將導致懲罰函式的
增加，根據以上假設， 
00,00 >⇒>
∂
∂
>⇒>
∂
∂
−− ii
i
ii
i
pd
Cd
p
U
d
RdU
γγ
   (5) 
00,00 2
2
2
2
2
2
2
2
>⇒>
∂
∂
>⇒>
∂
∂
−− ii
i
ii
i
pd
Cd
p
U
d
RdU
γγ
   (6) 
者的集合。 
定義二：一個由基地台和數個基地台使用者組成Macrocell網路 ),( 0 MMM EVVG ∪= 是 ),( EVG = 的
子圖且 GGM ⊆ ，當{V0, VM}⊆V 且 EM⊆E，且 nM KG ,1∈ 。 
定義三：一個由數個微型基地台和數個微型基地台使用者所組成的網路 ),( FFF EVG = 是 ),( EVG =
的子圖且 GGF ⊆ ，當 EVV EFF ⊆⊆ 。當 Vf是一個微型基地台的集合且 Vu是微型基地台
使用者的集合， ufF VVV ∪= ，而且 nF KG ,1∈ ，Vu(f)是 FU的集合，其 SBS滿足 f fV∈ 。 
定義四：在集合 V中之關連 R是一個從 V中的有序配對集合，若且唯若 v可以藉由無線電信號聽
到 u。當 R是一個關聯，我們通常寫作 uRv以代表(u, v)∈R且說(u,v)滿足關連 R。 
定義五：鏈接是說連線(connected)或邊緣(edge)若且唯若 uRv且(u,v)∈EM或(u, v)∈EF，並且
0=∩ fM EE 。 
定義六：若 uRu或 uRv，兩個頂點 u與 v被在圖 G中被稱為鄰居，當{u, v}∈V。A(u)是一個頂點 u
的所有鄰近頂點之集合，其中對所有頂點 v∈V時滿足 uRv。|A(u)|是 A(u)集合中所有元
素的數量。 
定義七：一個被稱為干擾的關係式(u, v)∈R，記為 I(u,v)，若且唯若(u, v)∉E。如果(u, v)∉R，此一
對(u,v)間不會相互干擾。 
如果網路 G包含 I(u,v)元素，代表此一網路受到干擾，反之，便是未受干擾的。在混合網路
中，共有四種干擾情況： 
情況一，在 DL下，I(u, v)，u∈V0，v∈Vu； 
情況二，在 DL下，I(u, v)，u∈Vf，v ∈VM； 
情況三，在 UL下，I(u, v)，u∈Vu，v∈V0； 
情況四，在 UL下，I(u, v)，u∈VM，v∈Vf。 
假設微型基地台和基地台分享同樣頻率的頻道且微型基地台位在基地台所覆蓋的無線電範
圍內。資料和聲音是使用多使用者的正交頻分複用(OFDM)系統，在這當中每個使用者分享所有
的子載波而且正交頻分複用存取(OFDMA)方法被通過在使用者間分享子載波。假設 LTE-A和
802.16m使用者的存取方法是多時分工(TDD)，一個頻道會被分割成較小、固定長度的子框架。
每個子框駕都包括數個 OFDM符號。 
令 { }ksssS ,,, 21 = 是微型基地台 f可操作的子載波之集合，而且 S = {Sc∪Sx}，其中 Sc是未
使用子載波的集合，表不曾被任何 A(f)所使用，而 Sx 是會被 A(f)−Vu (f)所使用的子載波之集合。 
定義八：令 Fi (S)為從 S選出的函式給 i使用者傳送被要求的位元且 Fi (S)∈S。如果 SBS並未分配
頻帶給使用者 i，則|Fi (S)| = 0；反之，|Fi (S)| ≠ 0。 
定義九：令 Q=Qv∪Qd是微型基地台 f中被允許的 QoS連線之集合，其中 Qv { }vivv qqq ,,, 21 =  是
聲音連線之集合，Qd { }djdd qqq ,,, 21 = 是資料線之集合。B(qi)為 qi 所需要的頻寬，亦即所
需之位元數，而微型基地台 f所需的全部頻寬，記為 { }∑ ==
Q
i i
qBfB
1
)()( 。 
依據 cij，Zhang等[22]得出相應的功率 )( ij
ij
t cP ，這是個為了子載波能在每個符號 cij個位元下以平
均每個符號的能量所需求功率函式，當通道增益相同[15]時， 
( )12
43
)(
2
10 −











= − ijcijij
ij
t
BER
QNcP ，    (14) 
其中 0N 是雜訊功率頻譜密度，而 ∫
∞ −
=
x
t
dtexQ 2
2
2
1)(
π
，其中隨著使用者 i針對不同 QoS要求而
採用的調變和編碼方式而變。為了維持接收端的 QoS要求，接收端的功率 2)()( ijij
ij
tij
ij
r gcPcP = 可
寫為 
( )12
43
)(
2
1
2
0 −











= − ijcij
ij
ij
ij
r
BER
Q
g
NcP ，    (15) 
其中 ijg 為使用者 i在第 j個子載波的通道增益。 
為滿足傳輸率 iω ，使用者 i的總接收功率可表為 
( ) ( )
( )
∑
=
=
SF
j
ij
ij
rr
i
cPiP
1
 
( )
( )
∑
=
− −











=
SF
j
cij
ij
i
ij
BER
Q
g
N
1
2
1
2
0 12
43
，   (16) 
類似地，滿足使用者 i在滿足傳輸率 iω 時之總傳輸功率為 
( ) ( )
( )
∑
=
− −











=
SF
j
cij
t
i
ij
BER
QNiP
1
2
10 12
43
，    (17) 
為瞭解微型基地台對MU的影響， 某一MU ui的接收功率 ( )iPMUr 可由下式得之 
( ) ( ) ( )dLiPiP oMtMUr −= ，       (18) 
其中 )(log10 iPP t
M
t = ，單位是 dBm，此為服務基地台(SMBS, serving MBS)的傳輸功率。任何基
地台使用者 ui可以正確的從其 SMBS接收訊息，前提是如果從所選的子載波所接收的 SNR值 
( )
( )( )
( ) 







=
∑
∑
=
=
SF
j
ij
f
SF
j ij
ij
r
r i
i
N
cP
i
1
1log10ψ ，     (19) 
2.2.4 自主的干擾減輕(AIM演算法) 
第一步：微型基地台 f1透過有線網路(backhaul networks)，定期更新 ( ) ( )11 fVfA u− 的位置資訊，
與MBS在 DL下的 SSx ∈ 。 
第二步：如果 ( ) ( ) 011 =− fVfA u ，或是 ( ) ( ) 011 =− fVfA u ，則 f1不受干擾，且 SSc = 。反之，f1是被
MU所干擾，且 xc SSS −= 。 
第三步：依據 xS 進入相對應之情況處置。 
 
情況一： 0=xS   
  symbolcc TSR *= ，並且分配所有 Rc 給 B(f1) 直到 B(f1) = 0 (無需求)。 
情況二： 0≠xS  
  若可用的 Rc能滿足 B(f1)，則分配所有 Rc 給 B(f1) 直到 B(f1) = 0。 
  若可用的 Rc不能滿足 B(f1)，則分配所有 Rc 給 B(f1)直到 Rc = 0 ，則 symbolxx TSR *=  
分配所有 Rx 給 B(f1) 直到 B(f1) = 0或 Rx=0 
情況三： 0=cS  
  分配所有 Rx 給 B(f1) 直到 B(f1) = 0或 Rx=0 
詳細的分配程序演算法如圖二。 
間之矩形區域產生白色無burst之區域。 
以下圖為例，(a)圖中僅有一個 inter-burst wastage，(b)圖中則有五個 inter-burst wastage，此外，
(b)圖中尚有一個 intra-burst waste。 
現行針對 burst之配置，主要有以下幾種方法： 
1. 固定式(Fixed burst scheme)：此一方法，在尚未進行封包之排程前，即已先行協調確認，切
割區間之長寬，已先行排定，此一方法優點為簡單，而針對彈性與資源之運用，則有改進空間。 
2. 水平掃瞄演算法(Raster Scanning Algorithm)：此一演算法，是2006年由Y. Ben-Shimol, I. 
Kitroser, and Y. Dinitz所提出，其優點為具高效率之throughput，缺點是由於較多的burst，
造成較大量之控制信號(control data overhead DL-MAP IE)。 
3. 類似方法二之改良演算法：2007年由T. Ohseki, M. Morita, and T. Inoue等人所提出，此法藉
由預先定義最小burst unit之大小，保留了方法二中高效率throughput之優點，但避免了增加
控制信號之缺點。 
 
圖三. 不佳的對應安排將造成 subframe資源之浪費 
2.3.2 burst mapping可能具備之問題描述 
在 IEEE 802.16e-2005標準中，採用 OFDMA為調變選項之一，OFDMA採用正交式子載波
調變方式，故可同時間提供不同使用者連線所需，在 DL subframe中，包含多個 DL burst，而
不同之封包，被安排至不同之 DL burst中。在每一個 frame前端，皆包含標頭(preamble)及 frame 
control header (FCH)，而 FCH中則包含了 Downlink Map (DL-MAP)相關之調變與編碼資訊
(modulation and coding scheme)，在 DL-MAP中，存在 DL-MAP Information Element (IE)，
記錄了 DL-Burst的資訊，圖四所示即為 DL-MAP IE之格式。 
 圖五 封包之排程與 burst mapping之架構 
Burst mapping algorithm MBA 
依我們所觀察，在 DL subframe中，可用來配置 burst之矩形區域，在完成一個 burst之配
置後，會形成矩形或 L形之待配置區域(以下我們簡稱為 resource)，若能持續保持此一特性，即
可運用遞迴之方式，繼續配置下去。 
在MBA演算法中，我們區分矩形與 L形兩種狀況討論(如圖六)，其運作方式以下列兩步驟
說明： 
 
圖六 資源配置之兩種狀況 
Step I：首先將封包排程器中已賦予之 burst大小依降冪排序，依大小進行配置，如此，即可
確保較大之 burst避免因剩於空間之形狀因素而無法被排入 resource內，相反的，剩
餘之空白區域，恰可提供較小尺寸之 burst配置，此一方法，可充份運用彈性配置，
而減少 slot之配置浪費。 
Step II：依據前述，若空白之矩形與 L形 resource，我們依此兩種狀況分別討論： 
Case I：依我們觀察，矩形之 resource之使用，是較具彈性的，如果有可能，我們將儘量維持
矩形之 resource，其做法，是將待配置之 burst其 slot數以 x或 y來除，若能整除，
 圖八 L形 resource的配置範例 
下面則為本演算法之分析；在MBA演算法中，步驟一已先針對 packet size依降冪進行排序，
假設上一次配置之 packet大小為 k，則下一次欲配置的 packet大小必不大於 k，若要求 1, 2, …, 
k間有多少數可被 x, y, s, t, u,和 v整除？我們求得： 
( )  
 

 61
5
61
61
6
1
6
1
1 AA
AAA
AAA
ji
kji
ji
ji
i
i
i
i
−+−
+
−=
∑
∑∑
≤≤≤
≤≤≤==
α
       (27) 
其中 Ai為 k除以 li之值，則可求得所欲配置之 packet可被整除之機率為
k
A
P i
i6 1=
= 當 P
越大，表示越可能被整除，則其產生waste slot之機率越低。 
 
3. 研究工作成果與評估 
3.1. Femtocell 微型基地台之發射功率控制機制 
3.1.1. 模擬環境 
我們詳細評估我們所提出的方法的表現，考慮在環境中包含了數個隨機分佈在範圍內的
femtocell並且假設這些家用基地台在室內的環境中被設置以及使用
( ) ( )dBLdL walls++= 10log205.38 [5]為我們路徑損耗的模型。當使用最大傳輸率時，在上式中
AWGN功率 σ2被設為 20dB。模擬的參數總結於表一。 
表一 模擬使用參數 
 
3.2. Femtocell 微型基地台干擾之降低 
3.2.1. FTM模擬環境 
我們提出的 AIM，和mobility eventbased (MEB)[26]及 the subcarrier scheduling allocation 
(SSA) [21]的表現將利用 C++語言進行模擬。模擬是假設在流量飽和的前題下，用戶封包是以
Poisson程序方式到達。一個Macrocell部署在七個六角形中，中心的六角形被其他的環狀包圍。
家用基地台隨機分佈在階層式細胞結構，並重覆利用與基地台同樣頻率。住宅的寬與長皆為十公
尺。基地台使用者在六百公尺的範圍內均勻分佈並與家用基地台共頻道。系統的參數如表二和表
三。 
表二 系統模擬參數 
 
表三 各服務類別之需求量 
 
3.2.2. 成果與評估 
圖十二顯示了干擾功率引起的嚴重退化都是因為不管是家用基地台室內外的使用者或鄰近
基地台使用者都沒有做功率控制。鄰近基地台使用者因為被家用基地台干擾而停電，而且當基地
台使用者逐漸遠離時依然不能取得足夠強度的訊號。因為許家用基地台洩漏多餘的電力提升了室
外環境的干擾程度。相反的，由於室內有牆壁的屏蔽效應，家用基地台所受到的干擾會隨著基地
台使用者的遠離而減少。從家用基地台產生的干擾應該透過 AIM被有效的管理並減低干擾程度。 
 圖十四 平均家用基地台使用者輸出與基地台使用數量之關係 
3.3. DL subframe之排程最佳化 
首先，我們討論在不同 packet大小時，其 slot使用率與 intra-burst waste之情況，在圖十
五中每個 burst平均大小為 1800 bits，我們得知隨著 burst數量的增加，頻寬的使用率亦隨之增
加，在模擬結果中，初期 Bucket和MBA兩種方法的差別不大，但當 burst數大於 7以後，即逐
漸拉開距離，在高負載情況下(11個 burst) Bucket法只達到 78%使用率，但MBA法卻可以達到
95%使用率。 
 
圖十五 burst數量與 slot使用率關係圖 
在圖十六中，討論了 burst數量與 intra-burst waste關係，intra-burst waste表示 slot無法
分配到任何的 burst，此為無效率之配置方式，在模擬結果顯示，MBA法，在不同的 burst數量
下，其 intra-burst waste皆較 Bucket法為低。由圖十五、十六中，我們可得到，在 burst數為 5
時，MBA法可得到 45%的可分配資源，Bucket法只有 30%，此一結果顯示，MBA法較具效能。 
[3] S. Y. Choi, T.-J. Lee, M. Y. Chung, and H. Choo, “Adaptive coverage adjustment for femtocell 
management in a residential scenario,” Lecture Notes in Computer Science, pp. 221–230, Sept. 2009. 
[4] H. Claussen and F. Pivit, “Femtocell coverage optimization using switched multi-element antennas,” 
in Proc. IEEE ICC’09, pp.1–6, Dresden, Germany, June 2009. 
[5] E. Altman, T. Boulogne, R. El-Azouzi, T. Jiminez, and L. Wynter, “A survey of network games in 
telecommunications,” Computers and operations research, pp. 286–311, Feb. 2006. 
[6] M. Xiao, N. B. Shroff, and E. K. P. Chong, “Utility-based power control in cellular wireless 
systems,” in Proc. IEEE INFOCOM, vol. 1, Anchorage, AK, pp. 412–421, Apr. 2001. 
[7] V. Chandrasekhar, J. G. Andrews, T. Muharemovic, Z. Shen, and A. Gatherer, “Power control in 
two-tier femtocell networks,” IEEE Trans. Wirel. Comm., vol. 8, no. 8, pp. 4316–4328, Aug. 2009. 
[8] E. Altman, T. Boulogne, R. El-Azouzi, T. Jiminez, and L. Wynter, “A survey of network games in 
telecommunications,” Computers and Operations Research, vol. 33, no. 2, pp. 286–311, Feb. 2006. 
[9] J. F. Nash, “Non-cooperative games,” Ann. Math., vol. 54, no.2, pp. 289–295, Sept. 1951. 
[10] H. Claussen, “Performance of macro- and co-channel femtocells in a hierarchical cell structure,” in 
Proc. IEEE PIMRC, pp. 1–5, Athens, Greece, Sept. 2007. 
[11] H. Claussen, L. T. W. Ho, and L. G. Samuel, “ An overview of the femtocell concept,” 
Next-Generation Wireline Access Networks, vol. 13, no. 1, pp. 221–245, May 2008. 
[12] H. Ji and C.-Y. Huang, “Non-cooperative uplink power control in cellular radio systems,” Wireless 
Networks, vol. 4, no. 3, pp.233–240, Mar. 1998. 
[13] I. L. Glicksberg, “A further generalization of the Kakutani fixed point theorem with application to 
Nash equilibrium points,” American Mathematical Society, vol. 3, no. 1, pp. 170–174, 1952. 
[14] J. B. Rosen, “Existence and uniqueness of equilibrium points for concave n-person games,” 
Econometrica, vol. 33, no. 3, pp. 520–534, 1965. 
[15] G. Debreu, “A social equilibrium existence theorem,” National Academy of Sciences, vol. 38, pp. 
886–893, 1952. 
[16] 3GPP, “Technical Specification Group Radio Access Network for EUTRA (LTE-Advanced): TDD 
Home eNode B (HeNB) Radio Frequency (RF),” TR 36.922 v9.1.0, Release 9, June 2010. 
[17] IEEE 802.16 Working Group, “Part 16: Air Interface for Fixed and Mobile Broadband Wireless 
Access Systems: Advanced Air Interface,” IEEE P802.16m/D12, February 2011. 
[18] Y.Y. Li and E.S. Sousa, “Base Station Pilot Management for User-Deployed Cellular Networks,” in 
Proc. IEEE ICC 2009, Dresden, June 2009. DOI: 10.1109/ICC.2009.5198778. 
[19] H. Claussen and F. Pivit, “Femtocell Coverage Optimization Using Switched Multi-Element 
Antennas,” in Proc. IEEE ICC 2009, Dresden, June 2009. DOI: 10.1109/ICC.2009.5199033. 
[20] J. Yun and K. G. Shin, “Adaptive Interference Management of OFDMA Femtocells for Co-Channel 
Deployment,” IEEE J. Select. Areas Commun., vol. 29, pp. 1225–1241, June 2011. 
[21] M.E Sahin and I. Guvenc, “Handling CCI and ICI in OFDMA Femtocell Networks through 
Frequency Scheduling,” IEEE Trans. Consumer Electronics, vol. 55, no. 4, pp. 1936–1944, 
November 2009. 
[22] H. Zhang, G. Bi, and L. Zhang, “Adaptive Subcarrier Allocation and Bit Loading for Voice/Data 
Transmission in Multiuser OFDM Systems,” Wirel. Commun. Mob. Comput., vol. 9, no. 7, pp. 
894–908, July 2009. 
[23] ITU-R Rec M.1225: “Guidelines for Evaluation of Radio Transmission Technologies for 
IMT-2000,” ITU-R Rec M.1225, February 1997. 
Int. J. Ad Hoc and Ubiquitous Computing, Vol. 7, No. 4, 2011 211 
Copyright © 2011 Inderscience Enterprises Ltd. 
A novel Delay-Based Multicast Routing Protocol  
in ad hoc wireless networks 
Wu-Hsiao Hsu 
Department of Computer Science and Information Engineering, 
Ming Chuan University, 
333 Taoyuan, Taiwan 
E-mail: wuhsiao@mail.mcu.edu.tw 
Jenhui Chen 
Department of Computer Science and Information Engineering,  
Chang Gung University, 
333 Taoyuan, Taiwan 
E-mail: jhchen@mail.cgu.edu.tw 
Sheng-Cheng Yeh* 
Department of Computer and Communications Engineering, 
Ming Chuan University, 
333 Taoyuan, Taiwan 
E-mail: peteryeh@mail.mcu.edu.tw 
*Corresponding author 
Abstract: This paper proposes a Delay-Based Multicast Routing Protocol (DBMRP) for 
multicast transmissions in ad hoc wireless networks. The DBMRP uses an on-demand,  
source-based multicast routing protocol, which is based on the tree forwarding methodology.  
The queuing delay and link delay of each visited mobile node are used to establish the multicast 
tree. In addition, DBMRP selects Displacement Nodes (DNs) to reduce the number of control 
packets when establish the multicast tree. Simulation results demonstrate that DBMRP exhibits 
good throughput at low mobility, and has the smallest control overhead, which results in better 
end-to-end delays, as compared to other algorithms. 
Keywords: multicast; on-demand; protocol; routing; ad hoc. 
Reference to this paper should be made as follows: Hsu, W-H., Chen, J.H. and Yeh, S-C. (2011)  
‘A novel Delay-Based Multicast Routing Protocol in ad hoc wireless networks’, Int. J. Ad Hoc 
and Ubiquitous Computing, Vol. 7, No. 4, pp.211–220. 
Biographical notes: Wu-Hsiao Hsu received the PhD Degree in Department of Computer 
Science and Information Engineering from Tamkang University, Taipei County, Taiwan in 1999. 
He is currently a Chair and Associate Professor in Department of Computer Science and 
Information Engineering at Ming-Chuan University, Taoyuan Country, Taiwan. From 2000 to 
2003, he served in the Eastern Multimedia Corporation (EMC) as a CTO and consultant, and was 
responsible for planning the ISP IP infrastructure and IDC operation. His recent research interests 
include QoS unicast/multicast routing, traffic engineering and IPv6. 
Jenhui Chen received the BS and PhD Degrees from Tamkang University, Taipei, Taiwan, in 
July 1998 and January 2003, respectively, both in Computer Science and Information 
Engineering. Since 2003, he has been with the Department of Computer Science and Information 
Engineering, Chang Gung University, Taoyuan, Taiwan, where he is currently an Associate 
Professor and a Research Fellow with the High Speed Intelligent Center. His main research 
interests include the design, analysis, and implementation of communication and network 
protocols, wireless networks, artificial intelligence, and bioinformatics. 
Sheng-Cheng Yeh received the BS Degree in Electrical Engineering from National Taiwan 
University of Science and Technology, Taipei, Taiwan, in 1991, the MS Degree in Electrical 
Engineering from National Central University, Taoyuan, Taiwan, in 1993, and the PhD Degree  
in electrical engineering from National Central University, Taoyuan, Taiwan, in 2000.  
 
 
 
 A novel Delay-Based Multicast Routing Protocol in ad hoc wireless networks 213 
multicast efficiency, in comparison with ODMRP.  
In addition, the results show that as the number of mobile 
sources increases, CQMP has a 2–3% improvement over 
ODMRP, in terms of data packet delivery ratio. 
TFZMP (Zhou et al., 2008) combines three methods, 
mesh-based, on demand, and zone-based, that are suitable 
for MANET. On-demand techniques are usually adaptive  
to network topology changes. Mesh-based multicast 
protocols have been proven robust for mobility. Zone-based 
techniques, such as ZRP, have been shown to have low 
overhead and good scalability. With cohesive integration of 
the above-mentioned three techniques, the TFZMP scheme 
provides adequate multicast service to MANET, where 
bandwidth is limited, topology changes frequently, and 
power is constrained. The simulation results showed  
that TFZMP performs better on normalised overhead and 
packet forwarding efficiency, when compared with 
ODMRP. 
In this paper, we propose a novel multicast algorithm, 
namely a DBMRP, for wireless ad hoc networks. Unlike 
previously described tree-based schemes that build multicast 
trees in a best-effort manner, the DBMRP intends to 
estimate both queuing and link delays of each MN during 
the multicast tree establishing stage. Thus, it is possible  
to choose a longer hop-count path but less delay  
while forwarding a multicast packet to multicast members. 
In addition, the DNs, which are computed by Path  
Matrix (PM), are used to reduce the number of control 
packets when establishing the multicast tree. These DNs  
can be used to reduce collisions during multicast 
communications. 
The remainder of this paper is organised as follows. 
Section 2 introduces the estimation of queuing and link 
delays. Sections 3 and 4 describe the multicast tree 
establishment and maintenance, respectively. The 
simulation models and results are introduced in Section 5. 
Conclusions are given in Section 6. 
2 The Delay-Based Multicast Routing Protocol 
The metric of hop counts is the most commonly used 
measurement during multicast tree establishment. However, 
it cannot reflect the influences on realistic access delays 
even though this measure is easy to get. Therefore, a  
Delay-Based Multicast Routing Protocol (DBMRP), which 
is based on the queuing and link delays of each MN, is used 
to construct the multicast tree in wireless ad hoc networks. 
2.1 Queuing delay estimation 
Each MN in an ad hoc network may have different power, 
computational capacities and memory. Therefore, DBMRP 
will estimate a mean delay for each MN when establishing a 
multicast tree in an ad hoc network. 
The expected queuing delay is estimated for each MNi 
with a queue length of Ni in an ad hoc network. Suppose the 
ratio of the packet arrival rate (λi) to the packet service rate  
 
(µi) of an MNi is ρi. Let inP and Li denote the probability  
of n packets in the queuing system and the average  
number of packets in the queuing system, respectively. 
From a queuing theorem using (M/M/1): (Ni/∞/FCFS) 
model, we have: 
1
1( ) , 1, 0,1, 2 , ... ,
1 ( )
1 , 1, 0,1, 2, ... ,
1
i
n i
i i iN
i i
n
i i
i
n N
P
n N
N
ρρ ρ
ρ
ρ
+
  
−
≠ =   −  
= 
= = +
 (1) 
1
1
( 1)( )
, 1
1 1 ( ) .
, 1
2
i
i
N
i i i
iN
i ii
i
i
N
L
N
ρ ρ ρ
ρ ρ
ρ
+
+
 +
− ≠ −
−
= 
=
 (2) 
Hence, the probability that there are no packets in the 
queuing system is: 
0
0 1 1
1 1
( ) .
1 ( ) 1 ( )i i
i i i
i N N
i i
P ρ ρρ
ρ ρ+ +
 
− −
= = 
− −  
 (3) 
The average waiting time for each packet in the queuing 
system is as follows. 
.
(1 )
i
i
i i
i N
LW
Pλ
=
−
 (4) 
Finally, the expected queuing delay for each MNi is given 
by equations (3) and (4): 
0 1
1
(1 ) 1 .
(1 ) 1 ( ) i
i
i i i
i i i N
i N i
LQ W P
P
ρ
λ ρ +
   
− = − = − 
− −     
 (5) 
The relationship between the queuing delay, ρ, and the 
queue occupation is illustrated in Figures 1 and 2. Figure 1 
shows that the queuing delay will increase as the ρ increases 
under the case of ρ > 1. Similarly, Figure 2 describes when 
ρ is fixed to 1.1, the queuing delay will increase as the 
queue occupation increases. With comprehensive analysis, it 
is obvious that the queuing delay of each MN significantly 
depends on ρ and queue occupations. 
Figure 1 The relationship of queuing delay and ρ (see online 
version for colours) 
 
 
 A novel Delay-Based Multicast Routing Protocol in ad hoc wireless networks 215 
3.2 Route set-up process 
As mentioned before, each member of a multicast group 
will select a route with minimum delays and send an  
MRR packet back to the multicast source via the minimum 
delay route. Hence, several MRR packets (each one from 
different multicast member) will be received by the 
multicast source. Each MRR packet records the sum  
of delay of the traversed path and all visited MN IDs.  
Once the multicast source receives all MRR packets,  
it will use these recorded IDs to build a PM. Therefore,  
the PM constructs all the multicast routes from the multicast 
source to each multicast member. The PM is defined  
as follows. 
PM { ( , ) |1 , 1 )},xs u v M N u M v N= ≤ ≤ ≤ ≤  (7) 
where s(u, v) = k, k∈ {0, 1} and s(u, v) = 1 indicates there is 
a route between MNs u and v. The M rows and N columns 
of the PM represent the numbers of the multicast member 
and the on-tree MN, respectively. Taking Figure 4 for 
example, if the multicast source wants to send packets to all 
members, the path to multicast member 9 will be 
MN1 → MN3 → MN6. 
Figure 4 The illustration of PM (see online version for colours) 
 
After building the PM, the multicast source propagates  
it along each path that each received MRR packet traversed. 
Each MN receiving the PM stores it, updates its multicast 
routing table and unicasts the PM to its downstream MNs.  
A multicast routing table contains a multicast group address, 
an upstream MN ID, a downstream MN ID and lifetime.  
All the intermediate MNs, which are responsible for 
forwarding the PM, update the multicast group address,  
the upstream MN ID and the downstream MN ID to their 
individual multicast routing table. On the other hand, all the 
multicast members only add the downstream ID to their 
individual multicast routing table. The lifetime is associated 
with each entry in the multicast routing table, indicating  
the length of time the route entry is valid. As a result,  
all the MNs in the established multicast tree have an  
 
 
 
 
 
identical PM. In such case, the multicast tree has already 
been established. 
3.3 The Displacement Node (DN) 
Owing to the reason of inability to exchange request-to-send 
or clear-to-send (RTS/CTS) and acknowledgement (ACK) 
packets with multiple receivers, it is difficult for IEEE 
802.11 to support reliable multicast (Kuri and Kasera, 2001; 
Gupta et al., 2003). A receiver cannot receive a packet 
correctly if two or more packets are sent to it 
simultaneously, due to packet collision. This is a major 
problem for the reliability of multicast, as several members 
in a multicast group may simultaneously respond to a 
multicast-RTS or a data packet sent by a multicast source. 
As a result, a CTS or ACK collision will occur at the 
multicast source. 
To avoid the collisions mentioned earlier, it seems 
feasible to assign a specific recipient to send a CTS or an 
ACK. Actually, a combination of ACKs sent to attain a 
degree of reliability is often used in designing multicast 
protocols. Different methods have been proposed to provide 
reliable multicasting. For example, Probability-Based 
Protocols (PBPs) use probabilistic feedback schemes  
that allow each receiver to send an ACK immediately,  
with only a certain probability. In Delay-Based Protocols 
(DBPs), the recipient must wait a random amount of time 
before sending an ACK. Leader-Based Protocol (LBP) 
tackles the problem by electing a recipient node as a  
leader, and only this leader is allowed to send an ACK.  
It has been demonstrated that the LBP exhibits a higher 
throughput in comparison with PBP and DBP, which  
use traditional delayed feedback-based probabilistic 
methods (Kuri and Kasera, 2001). However, these three 
methods support only the infrastructure-based wireless 
networks. 
We propose an algorithm, which is based on LBP,  
to find DNs in the established multicast tree. The function 
of the DN is the same as the leader node described in LBP. 
Unlike the LBP, which elects only a leader node for a 
multicast group, the proposed algorithm may elect several 
DNs as leader nodes in a multicast group. 
With our algorithm, an MN within a multicast tree is 
elected as the DN if one of the following conditions is 
satisfied: 
• it has more than two branches 
• it is a multicast member, and has a branch that connects 
to at least one multicast member. 
For example, as shown in Figure 5, MNs 1, 2 and 3 are 
elected as the DN because they have more than two 
branches. Similarly, MN 4 is also elected as the DN because 
it has a branch that connects to multicast member MN 5.  
 
 
 
 
 
 A novel Delay-Based Multicast Routing Protocol in ad hoc wireless networks 217 
Figure 7(b) The quit process without downstream MNs  
(see online version for colours) 
 
4.3 Route recovery process 
As MNs in an ad hoc network are capable of moving 
independently, a link has a limited lifetime. Furthermore, 
current existing links are no longer valid when any two 
MNs move out of transmission range each other. Therefore, 
a multicast tree is subject to disruption due to link/node 
failure, node mobility, or route expiration timers.  
In DBMRP, the route recovery process is responsible for 
repairing broken multicast links. 
An intermediate MN may fail or move. In this case, the 
multicast tree can break into two or more sub-trees. Similar 
to Royer and Perkins (1999) and Toh et al. (2000), the 
DBMRP also employs a localised repair strategy to deal 
with link breakage. As shown in Figure 8, when MN 1 
moves out of the transmission range of the multicast source, 
the link between MN1 and the multicast source breaks. 
When a link breakage is detected, the upstream MN of the 
broken link broadcasts an MRD packet to all downstream 
MNs of the broken link to find new routes. These 
downstream MNs of the broken link can be easily found by 
checking the PM. In Figure 8, the multicast source will 
check the PM to find that MN 3 is the downstream MN  
of the broken link. The multicast source will broadcast  
an MRD packet to MNs 2, 4 and 5 to find a new route to 
MN 3. Since both MNs 2 and 4 know how to reach MN 3, 
they will send their individual MRR packets back to the 
multicast source. Note that this MRD packet differs from 
the MRD packet used in the route discovery process in one 
way: it has a limited TTL (i.e., limited hop count). The 
limited TTL can limit the broadcasting area, which reduces 
the communication overhead. 
Figure 8 The route recovery process in DBMRP (see online 
version for colours) 
 
The upstream MN chooses a received MRR packet with the 
smallest sum of queuing and link delays, updates its PM and 
the downstream entry in multicast routing table, and then 
broadcasts the updated PM to all the on-tree MNs.  
In Figure 8, assume that the MRR packet sent by MN 4 has 
the smallest sum of delay. After receipt of all the MRR 
packets, the multicast source updates its PM and multicast 
routing table, and broadcasts the PM to all on-tree MNs. 
If the route recovery process fails, the lifetime of the 
broken multicast route in the multicast routing table of the 
upstream MN will expire. In this case, the upstream MN 
will send a route error packet to the multicast source. When 
the multicast source receives the error packet, it will 
establish a new multicast tree. 
5 Simulation models and results 
5.1 Simulation models and assumptions 
A simulated network with 50 MNs is randomly distributed 
in a rectangular coordinate grid of 1000 m × 1000 m.  
The radio propagation range for each MN is 250 m and the 
channel capacity is 2 Mbits/s. The link layer model is the 
Distributed Coordination Function (DCF) of the IEEE 
802.11 wireless LAN standard. Each MN moves 
individually within a certain probability, and the probability 
of movement will initially be given by 0.1 and increased by 
0.1 each movement until reaching 1. Each MN selects a 
random destination and moves there at a uniformly 
distributed speed in a predefined range of 0–30 m/s  
(0–108 km/hr). We assume that the mobility is high as the 
multicast group size increases. The TTL is fixed at 2. 
To reflect a realistic situation, each MN will stay at least 
60 s in the new position after movement. The packet arrival 
rate of each MN will be given randomly, from 640 bps to 
800 bps. The packet length considered in this simulation 
follows the statistical average packet size in real computer 
networks as 50–150 bytes long (Khalil et al., 1990).  
The buffer size of each MN will be given randomly, from 
0.4 Kbytes to 1 Kbytes. The number of multicast members 
will initially be given by 5, and increase by 5 each 
movement until 35. 
The simulation platform is based on Free BSD,  
and C language is used to code the simulation environment. 
For each data point in the simulation result, 20 random 
graphs are generated. For each graph, a corresponding 
multicast tree is established and measured. Each reported 
data is calculated as the average of the 20 collected data. 
The various parameters used in our simulation are shown in 
Table 1. 
Table 1 Simulation parameters 
Programming Language C 
Simulation platform FreeBSD 
Channel Capacity 2 Mbps 
MAC Layer IEEE 802.11 
Total number of MNs 50 
 A novel Delay-Based Multicast Routing Protocol in ad hoc wireless networks 219 
In contrast, ODMRP gets a higher packet delivery ratio 
than both MAODV and DBMRP when the multicast group 
size is 35. This is because the ODMRP is a mesh-based 
protocol, for which the forwarding group provides multiple 
paths, and periodically reconfigures its multicast forwarding 
group. Unlike ODMRP maintaining multiple paths, 
DBMRP needs to reconfigure broken routes once routes are 
broken since it only maintains one route. We notice that the 
gap between ODMRP (0.79) and DBMRP (0.75) is 0.04 
when the group size is 35. Obviously, the gap 0.04 is not 
large. Nevertheless, this little improvement will cost a lot of 
control overheads when the group size is 35 as shown in 
Figure 10, where the gap of number of control packets 
between ODMRP (= 9765) and DBMRP (= 3942) is 5823 
packets. This result indicates that DBMRP is suitable for 
multicast transmission in ad hoc networks when group size 
is large because more control overheads will dramatically 
decrease the maximum packet delivery ratio of the ad hoc 
networks. 
5.2.2 Control overhead 
The control overhead is defined as the total number  
of control packets transmitted during the multicast tree 
establishment and maintenance. Each control packet 
includes unicast packets, e.g., RREP, MRR, other tree 
maintenance packets, and broadcast packets, e.g. RREQ, 
MRD. Figure 10 shows the comparison of control overheads 
based on different multicast group sizes. The x-axis 
represents the different multicast group sizes whereas the  
y-axis shows the control overheads (number of control 
packets). 
The control overhead produced by MAODV and 
DBMRP is significantly lower than that by ODMRP. This is 
because the multicast tree is established and maintained  
on-demand in MAODV and DBMRP, and do not maintain 
multiple routes to a same destination. It is obvious that  
the total number of control packets produced for  
each multicast session depends on the total number of  
tree reconfigurations. Since the total number of tree 
reconfigurations made by MAODV and DBMRP are 
smaller than that of ODMRP, both MAODV and DBMRP 
provide better performance than ODMRP. 
Furthermore, the DBMRP provides the best 
performance of control overhead. This is because the 
DBMRP only send the MRD and MRR packets after tree 
reconfiguration instead of broadcasting a group hello packet 
in MAODV. For a larger multicast group size, it is obvious 
that the gap of the number of control overheads between 
these two protocols is also large. 
Clearly, the ODMRP performs the highest control 
overhead because it uses a soft state approach to maintain 
connectivity among multicast members. As a result, the 
MNs in a multicast tree must periodically generate  
control packets. For example, to refresh the membership 
information and update the routes, the JQ packet is 
periodically flooded in the network, regardless of whether 
the multicast tree is stable. 
5.2.3 Average number of hops 
Figure 11 shows the average number of hops under different 
multicast group sizes. Since both MAODV and ODMRP 
have almost the same performance, we only compare 
ODMRP with DBMRP. 
From the results, the value of the average hop count is 
approximately 2.297 for ODMRP and 3.161 for DBMRP.  
In most scenarios, ODMRP results in a smaller number of 
hops than DBMRP because ODMRP broadcasts the data 
packets through its forwarding group and the first surviving 
packet to the receiver is taken. Usually, this smallest delay 
path implies shortest-hop path in long-delay link, much like 
the wireless physical links in our simulation. On the other 
hand, since DBMRP selects a path based on the smallest 
delay, the resulting path is normally longer than the 
shortest-path. 
5.2.4 End-to-end delay 
The end-to-end delay is measured when packets have been 
successfully received by the multicast members. Since 
MAODV and ODMRP have almost the same performance, 
we only focus on ODMRP and DBMRP under different 
multicast group sizes. 
The results show that the mean of end-to-end delay is 
86.606 ms for ODMRP, and 76.893 ms for DBMRP, as 
shown in Figure 12. Clearly, the ODMRP produces a longer 
end-to-end delay than DBMRP. This is because packets are 
periodically broadcast in the ODMRP, and it is quite easy to 
collide with other broadcast packets being transmitted 
during the same period of time. On the other hand, the 
multicast route is selected by the DBMRP based on 
minimum delay from the multicast source to multicast 
member. In addition, the DNs also plays a key role in 
reducing the number of control packets in a multicast tree; 
therefore, there is little control packet collision. The result 
of end-to-end delay can correspond to previous result on the 
average number of hops. It is obvious that ODMRP chooses 
the shortest multicast path, but has a longer end-to-end 
delay. On the other hand, DBMRP chooses a longer 
multicast route, but has a smaller end-to-end delay. 
6 Conclusions 
This paper proposes a novel DBMRP for mobile ad hoc 
networks. DBMRP is based on tree forwarding and an  
on-demand source-based multicast routing protocol, and 
uses queuing and link delays for each visited MN to 
establish the multicast tree. It also uses the DNs to reduce 
the number of control packets in an established multicast 
tree, and the PM to provide an efficient method for local 
error recovery in route recovery processes. 
Simulation results show that ODMRP has better  
packet delivery ratio than the other two algorithms in high 
mobility, but incurs extremely large control overhead.  
On the other hand, DBMRP exhibits good packet delivery 
ratio at low mobility, and has the smallest control overhead 
This article appeared in a journal published by Elsevier. The attached
copy is furnished to the author for internal non-commercial research
and education use, including for instruction at the authors institution
and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or
licensing copies, or posting to personal, institutional or third party
websites are prohibited.
In most cases authors are permitted to post their version of the
article (e.g. in Word or Tex form) to their personal website or
institutional repository. Authors requiring further information
regarding Elsevier’s archiving and manuscript policies are
encouraged to visit:
http://www.elsevier.com/copyright
Author's personal copy
algorithms for three bandwidth constraint models, respectively,
and focuses on how to design a bandwidth manager to support
DiffServ-TE.
This paper proposes a new per-class bandwidth constraint algo-
rithm, called the multipath selection algorithm (MSA), for a Diff-
Serv-TE network. Unlike previous studies, which focus on
preemption policy, restoration routing, or bandwidth manager,
the proposed MSA ﬁnds multiple LSPs per-class and allows ﬂexible
division of trafﬁc over these LSPs. The MSA comprises three steps.
First, a given source uses the MSA to ﬁnd multiple LSPs from the
source to a given destination for a CT. Second, the source uses
the available bandwidth of the CT on all the links along these LSPs
to allocate initial trafﬁc. Third, the source dynamically adjusts traf-
ﬁc for these LSPs based on individual round trip time.
1.1. Bandwidth constraint models
The maximum allocation model (MAM) [9], the Russian doll
model (RDM) [10], and the maximum allocation with reservation
(MAR) [11] are three IETF-proposed bandwidth constraint models
for supporting DiffServ-TE. The author of [12] compared these
three models and concluded that the RDM best matches DiffServ-
TE. Hence, the MSA proposed in this study uses the RDM as a band-
width constraint model. The RDM provides each class with a min-
imum amount of bandwidth, but lower priority classes can use the
bandwidth of higher priority classes when that bandwidth is
available.
1.2. The link state interior gateway protocol (IGP)
In the proposed MSA, each node in a DiffServ-TE network works
in conjunction with the extensions of the open shortest path ﬁrst
(OSPF) protocol [13]. In the extended OSPF protocol, each node
running a link state QoS routing protocol uses reliable ﬂooding to
exchange link state advertisements (LSAs) with its neighboring
routers. Each LSA must advertise the available bandwidth per-CT
on every link. Based on the reliable ﬂooding of LSA, all nodes build
identical link state databases that depict the entire OSPF network
topology interconnected by a group of nodes. When the available
bandwidth per-CT of one or more links changes, the link state data-
base in each node must be updated immediately.
The remainder of this paper is organized as follows. Section 2
introduces the notation and problem in this study. Section 3 pre-
sents the proposed algorithm. Section 4 describes the simulation
model and results, and Section 5 provides some conclusions.
2. Notation and problem description
2.1. Notation
Before formally introducing the proposed algorithm, the nota-
tion used throughout this paper will ﬁrst be described. Let
G = (V,L) denote a DiffServ-TE network, where V is the set of nodes
and L is the set of links. Suppose that a node represents a router. A
path p from a node x to a node y is a sequence of nodes and links
x = v0, l(v0, v1), l(v1 v2), . . . , l(vk vk+1), y = vk+1 and is denoted by
p(v0,v1,v2, . . . ,vk,vk+1). AB(p) represents the maximum available
bandwidth of path p, and is deﬁned as ABðpÞ ¼ minflðv i ;v iþ1Þa ðCTjÞj0
6 i 6 kg. A link capacity between nodes x and y is denoted by
lc(x,y). Let n denote the number of CTs, and BCi denote the band-
width reserved by CTi, 0 6 i 6 n. The available bandwidth of a link
between nodes x and y for the CTj can be denoted by l
ðx;yÞ
a ðCTjÞ,
0 6 j 6 n. Since the proposed MSA uses the RDM as the bandwidth
constraint model, the link available bandwidth lðx;yÞa ðCTjÞ can be
computed as follows,
lðx;yÞa ðCTjÞ ¼
Xn
i¼j
lðx;yÞa ðCTiÞ þ lcðx; yÞ 
Xn
i¼0
lðx;yÞa ðBCiÞ
 !
: ð1Þ
2.2. Problem description
In Fig. 1, the number indicated at each link represents the cur-
rent link available bandwidth for a CTj (for example, l
ðA;BÞ
a ðCTjÞ ¼ 8Þ,
and the bandwidth requested for a particular LSP from the source A
to the destination H is 10 Mbps. Clearly, no single path has enough
bandwidth to meet the 10 Mbps requirement. If the round trip
time from source A to the destination H is also considered, ﬁnding
a path which meets the request and has the minimum round trip
time is a NP problem [14]. In fact, the network architecture in
Fig. 1 shows that there is more than one path from the source A
to the destination H. Thus, concurrent multi-path transmission
can meet the request when a single path transmission cannot.
3. The multipath selection algorithm (MSA)
The main purpose of the MSA is to meet the requested band-
width by ﬁnding multi-LSPs for a CT in a DiffServ-TE network.
The MSA procedures can be divided into two parts:
(1) Finding multi-LSPs for the request.
(2) Allocating network trafﬁc to the selected LSPs.
3.1. Finding multi-LSPs
The MSA uses two metrics to ﬁnd multi-LSPs: the path round
trip time and the available bandwidth of each link comprising
the path. Since router queuing delay completely dominates the
path transmission delay from a source to a destination, the source
selects the path with the fewest hops as a LSP. The reason for
selecting a path with as few hops as possible is that more links
on a path not only consume more network resources, but also in-
crease propagation delay [15]. In other words, a LSP with fewer
hop counts (HCs) will have a shorter path round trip time.
The three principles of ﬁnding the multi-LSPs are as follows:
(I) All the found LSPs have no loop.
(II) The source selects the path with sufﬁcient available band-
width and the fewest number of nodes as a LSP. Any link
in the LSP can be selected repeatedly by other LSPs as long
as the link has enough available bandwidth. When a link
no longer has enough bandwidth to carry more LSP trafﬁc,
these links are not selected.
(III) If nodes want to keep the most current view of the available
bandwidth on all links in the network, they must update the
link state database frequently. However, frequent link state
database updates are neither scalable nor practical every
A
D
G
E
HSource
I
7 9
5
10
5
9
8
3 6
7
10
8
3
8
B
C F
Destination
8
5
Fig. 1. A simple network architecture.
1558 W.-H. Hsu et al. / Computer Communications 33 (2010) 1557–1565
Author's personal copy
the largest available bandwidth. That is, AB(p1) and AB(p2) can
be computed as follows:
ABðp1Þ ¼ minflðA;BÞa ðCTjÞ; lðB;EÞa ðCTjÞ; lðE;HÞa ðCTjÞg ¼ minf8;5;8g
¼ 5 Mb:
ABðp2Þ ¼ minflðA;BÞa ðCTjÞ; lðB;FÞa ðCTjÞ; lðF;HÞa ðCTjÞg ¼ minf8;3;7g
¼ 3 Mb:
Thus, Node A selects path p1(A,B,E,H). At this time, the LSPB is
found and denoted by LSPB(A, B, E, H).
Step 6: The available bandwidth of each link the LSPB passes
through must be updated. In this update procedure, Node A
subtracts the maximum available bandwidth of the LSPB from
the available bandwidth of each link passing through the LSPB.
Thus, Node A subtracts 5 Mb from the available bandwidth of
links l(A,B), l(B,E) and l(E,H). Fig. 5 shows the updated result.
Note that only Node A records the update for the available
bandwidth of each link. Therefore, the available bandwidth of
each link in the link state database remains unchanged.
Step 7: Node A takes Nodes C and D as starting points, and
repeats Steps 2 through 6 to ﬁnd the LSPC and LSPD, separately.
According to Principle II described in Section 3.1, l(B,E) will not
be selected again because Node A has updated the available
bandwidth of this link to 0. Fig. 6 shows that the LSPC and LSPD
are found and denoted by LSPC(A,C,F,H) and LSPD(A,D,G, I,H),
respectively.
3.2. Minimizing the maximum round trip time of selected LSPs
Assume that the delay from the source to the destination is at
its minimum when the round trip time of each selected LSP is
the same or similar. If this assumption is true, allocating and
adjusting trafﬁc to these selected LSPs involves minimizing the
round trip time between the source and the destination. Therefore,
before describing how to allocate and adjust trafﬁc, this assump-
tion must be validated ﬁrst.
Suppose that N LSPs from the source to the destination have
been found, and the variables required are deﬁned as follows:
T: the maximum round trip time among N LSPs.
ti: the round trip time from the source to the destination via the
LSPi when it carries no trafﬁc.
ci: the round trip time from the source to the destination via the
LSPi when it carries some trafﬁc.
mi: the allocated trafﬁc for the LSPi.
Validation:
To validate that the delay from source to destination is at its
minimum when the round trip time of each selected LSP is equal
or similar, set T = (t1 + d1) = (t2 + d2) =    = (tN+dN), where di = ci  ti,
1 6 i 6 N.
Assuming that there exists a T
0
, T
0
< T, both T
0
and T have the
same network environment, and the amount of trafﬁc load is also
the same. T
0
can be represented as follows:
T 0 ¼ maxfðt1 þ d01Þ; ðt2 þ d02Þ; . . . ; ðtN þ d0NÞg ð2Þ
Similarly,
T ¼ maxfðt1 þ d1Þ; ðt2 þ d2Þ; . . . ; ðtn þ dnÞg ¼ ðt1 þ d1Þ
¼ ðt2 þ d2Þ ¼    ¼ ðtN þ dNÞ ð3Þ
A
D
G
E
HSource
I
7 9
5
10
5
9
8
3 6
7
10
3
3
8
B
C F
Destination
3
0
LSPB
Fig. 5. Node A updates the available bandwidth of the links that the LSPB passes
through.
A
D
G
E
HSource
I
7 9
5
10
5
9
8
3 6
7
10
8
3
8
B
C F
Destination
8
5
LSPB=(A-B-E-H)
LSPD=(A-D-G-I-H) LSPC=(A-C-F-H)
Fig. 6. Three LSPs found by node A.
A
D
G
E
HSource
I
7 9
5
10
5
9
8
3 6
7
10
8
3
8
B
C F
Destination
8
5
HC=0 HC=1
HC=1
HC=1
HC=2
HC=2
HC=2
HC=2
The HC value does not
 need to be changed
Fig. 4. Node A sets the HC values of the nodes adjacent to Nodes C, E, and F at 2.
Table 1
All the HC values recorded by Node A.
Destination node Hop count
A 0
B 0
C 1
D 2
E 1
F 1
G 2
H 2
I 2
1560 W.-H. Hsu et al. / Computer Communications 33 (2010) 1557–1565
Author's personal copy
to the LSPe (as shown in formula (12)). Note that the sum of individ-
ually allocated trafﬁc of x light LSPs must equal the trafﬁc released
from the heavy LSPj. That is,
te þ de
t1 þ d1
 
RtPN
i¼1
teþde
tiþdi
 þ te þ de
t2 þ d2
 
RtPN
i¼1
teþde
tiþdi
 þ   
þ te þ de
tx þ dx
 
RtPN
i¼1
teþde
tiþdi
  ¼ Rt : ð13Þ
3.4. The discussion of dynamic adjustment
Normally, a LSP that releases trafﬁc will have a shorter round
trip time, and a LSP that receives the released trafﬁc will have a
longer round trip time. As a result, all the round trip times will
be similar. However, one main issue with this design is whether
or not the dynamic adjustment will cause the network iteratively
adjust and become unstable. For example, suppose the range of
round trip time is D and D is greater than the st. Step 2 changes
the value of D to D0. When D0 is larger than D, the dynamic adjust-
ment is invalid since the range is too large. Therefore, the dynamic
adjustment must be repeated. If D0 is always larger than D, the net-
work will be iteratively adjusted and become unstable.
To overcome the possible problem, the tuning factor, say a, is
used to control the amount of released trafﬁc. As mentioned above,
if D0 is larger than D after Step 2 is complete, this adjustment is in-
valid. In this case, the trafﬁc carried on each LSP is not updated and
remains unchanged. a times its original value by a constant value r
(a = a  r) and Step 2 is repeated until D0 is less than or equal to D
(D0 6 D). If D0 is less than D and still larger than st, the trafﬁc carried
on each LSP is updated, a is reset to its initial value, and Step 2 is
repeated. Otherwise, the network is completely adjusted and all
the round trip times will be similar. Fig. 8 shows the dynamic
adjustment ﬂow chart (assuming that the initial value of a is 1).
Fig. 9 shows the dynamic adjustment procedures. Let LSPS be
the set of selected LSPs, tfc be a function from the LSPS to the real
numbers to denote the trafﬁc assigned to each LSP, and rtt be a
function from the LSPS to the real numbers to denote the round
trip time of each LSP. Also, let TFC be the type of functions from
the LSPS to the real numbers. The main purpose of the dynamic
adjustment procedures is to ﬁnd a TFC function that performs
the dynamic adjustment on each selected LSP.
3.5. Description of the example of allocating trafﬁc to a LSP
Taking Fig. 6 as an example, source A ﬁrst calculates that the
maximum available bandwidth of LSPB is AB(LSPB) =min{8,5,
8} = 5 Mbps; the maximum available bandwidth of LSPC is AB(-
LSPC) =min{7,8,7} = 7 Mbps; the maximum available bandwidth
of LSPD is AB(LSPD) =min{5,5,8,3} = 3 Mbps. Source A then uses
the maximum available bandwidth of LSPB, LSPC, and LSPD to calcu-
late their individual initial trafﬁc. The initial trafﬁc of LSPB is
5
5þ7þ3
h i
 br Mb; the initial trafﬁc of LSPC is 75þ7þ3
h i
 br Mb; the ini-
tial trafﬁc of LSPD is 35þ7þ3
h i
 br Mb.
Suppose that st equals 1 ms, a equals 1, and the round trip times
of LSPB, LSPC, and LSPD are 28 ms, 37 ms, and 25 ms, respectively.
Mt is 30 ms after calculation. The range of the round trip time is
12 ms (=37 ms  25 ms), which is greater than st, and only the
round trip time of LSPC is longer than Mt. Thus, only LSPC needs
to release its trafﬁc. Source A releases part of the LSPC trafﬁc and
allocates it to LSPB and LSPD. Since the initial trafﬁc of LSPC is
7
15
  br Mb, the LSPC releases 1 373030 	 715  br 	 Mb of trafﬁc
and allocates it to the LSPB and LSPD. Since the LSPB has a longer
round trip time than the LSPD, the following trafﬁc may be in-
creased for LSPB
7
30  715  br
28
28 þ 2825
 !
¼ 25
53
 
 7
30
 
 7
15
 
 br Mb:
The increased trafﬁc of the LSPB can serve as the basis of calculating
the increased trafﬁc of the LSPD
28
25
  7
30  715  br
28
28 þ 2825
 !
¼ 28
53
 
 7
30
 
 7
15
 
 br Mb:
After the source A calculates and adjusts the trafﬁc of each LSP, the
round trip time of the LSPB and LSPD may increase slightly, but the
round trip time of LSPC decreases slightly. This process is repeated
until D0 is less than st, which achieves the goal of decreasing the de-
lay from the source to the destination.
4. Simulation results and analysis
4.1. Simulation environment
ADiffServ-TE networkwith 20 nodes is randomly distributed in a
rectangular coordinate grid. Each node is located at integer coordi-
nates. The number of links between any two nodes is decided ran-
domly, and can range from one to four. The available link
bandwidth between nodes is randomly set from 1 Mb to 100 Mb.
Two nodes are selected as the source and the destination. A LSP
requests 10 Mb of bandwidth, and the total data quantity transmit-
ted by the source is 100 Mb. There are twoCTs, CT0 and CT1, and they
are mapped to AF PHB and EF PHB, respectively. Each CT is assigned
to a BC. BC0 and BC1 are set at 90% and 70% of the link capacity,
respectively. The rest of the link capacity is reserved for best-effort
trafﬁc. Assume that CT0 has a lower priority than CT1. Thus, CT0
can use the bandwidth of CT1 when that bandwidth is available.
The initial valuesofa and r are set at 1 and0.5, respectively. st is ﬁxed
atMt*5%.Note that theMt is theoriginal valuebefore adjustment. For
example as described in Section 3.5, st = 30*5% = 1.5 ms.Fig. 8. The dynamic adjustment ﬂow chart.
1562 W.-H. Hsu et al. / Computer Communications 33 (2010) 1557–1565
Author's personal copy
from the source to the destination. The number 40 on the x-axis
means that the available bandwidth of each link can be selected
from [1 Mb to 40 Mb] at random, while 10 means that the available
bandwidth of each link can be randomly selected from [1 Mb to
10 Mb]. Thus, the available bandwidth of each link decreases along
with the value of the x-axis. Experimental results show that the
average delay difference between the proposed algorithm and CSPF
is within 5 s when the source transmits 100 Mb of data and the
available bandwidth of network is sufﬁcient. On the contrary, the
proposed algorithm produces a much smaller average delay than
CSPF when the available bandwidth of network decreases.
Fig. 10 shows that it is not easy for the CSPF to ﬁnd a path which
meets the requested bandwidth when the link’s available band-
width is less than 21 Mb. The average delay determined by the pro-
posed algorithm increases gradually when the available bandwidth
of the link is less than 15 Mb. This is primarily because the avail-
able bandwidth of each link is small. In addition, the average delay
produced by the CSPF is abnormally high when the available band-
width of the link is 13 Mb and 11 Mb. This is because the maxi-
mum available bandwidth from the source to the destination is
very small in some paths.
4.2.2. Average packet loss rate comparison
Fig. 11 shows the result with the y-axis displaying the average
packet loss rate for all selected LSPs from the source to the destina-
tion. The packet loss rate is deﬁned as follows:
Average Packet loss rate ð%Þ ¼ Number of lost packet
Number of transmitted packet
:
ð14Þ
Experimental results show that the packet loss rate based on the
proposed algorithm is below 2%. The main reason for this small loss
rate is that most of the found LSPs meet the requested bandwidth.
On the contrary, it is not easy for CSPF to ﬁnd a LSP when the avail-
able network bandwidth becomes smaller. This is why CSPF exhib-
its a greater increase in average packet loss rate than the proposed
algorithm.
4.2.3. Average throughput comparison
Fig. 12 shows the result with the y-axis displaying the through-
put. Normally, throughput is inversely proportional to the packet
loss rate. Fig. 12 shows that both the proposed algorithm and CSPF
produce optimal average throughput when the available band-
width of network is sufﬁcient. However, CSPF exhibits a severe de-
crease in average throughput when available bandwidth becomes
smaller. On the contrary, the average throughput based on the pro-
posed algorithm decrease only slightly when the available band-
width is smaller. In other words, the proposed algorithm can
effectively increase the average throughput of the network.
4.2.4. Comparison of variance of available bandwidth of all links
Figs. 13 and 14 show the results with the y-axis displaying the
variance of available bandwidth of all the links for a CT j on the net-
work. The variance is deﬁned as follows:
Fig. 13. Comparison of variance of available bandwidth for each link.
Fig. 14. Comparison of difference in variance of available bandwidth.
Fig. 15. The number of adjustments.
1564 W.-H. Hsu et al. / Computer Communications 33 (2010) 1557–1565
This article appeared in a journal published by Elsevier. The attached
copy is furnished to the author for internal non-commercial research
and education use, including for instruction at the authors institution
and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or
licensing copies, or posting to personal, institutional or third party
websites are prohibited.
In most cases authors are permitted to post their version of the
article (e.g. in Word or Tex form) to their personal website or
institutional repository. Authors requiring further information
regarding Elsevier’s archiving and manuscript policies are
encouraged to visit:
http://www.elsevier.com/copyright
Author's personal copy
ARTICLE IN PRESS
algorithm in IEEE 802.16 broadband wireless networks, they
mainly focus on the QoS architecture and bandwidth request
algorithm to satisfy diverse QoS requirements in a subscriber
station.
More recently, the IEEE 802.16e provided a new service
discipline aimed at different QoS for subscriber stations (SSs) that
contend for resources by requesting BS bandwidth. The IEEE
802.16e standard supports ﬁve different types of QoS scheduling
in the uplink: unsolicited grant service (UGS), real-time polling
service (rtPS), extended-real-time polling service (ertPS), non-
real-time polling service (nrtPS), and best effort (BE). The ﬁrst
three types of services support a VBR stream in real-time demand.
VOIP and video streaming over a wireless WMAN can use a VBR
stream or a constant bit rate (CBR) stream that follows a bursty
trafﬁc model determined by the situation. With UGS, BSs
simultaneously generate ﬁxed size data packets (such as CBR) on
a periodic basis, and unsolicited grants allow SSs to transmit their
packet data units (PDUs) without requesting bandwidth for each
frame (Chen et al., 2006). Since the bandwidth is allocated without
request contention, the UGS provides hard guarantees in terms of
both bandwidth and access delay. However, the UGS class
allocates a grant size, such as VoIP without silence suppression
(audio streaming without silence suppression). rtPS supports
service ﬂows that generate variable data packets on a periodic
basis, such as MPEG video and VoIP with silence suppression. The
bandwidth requirements have an indeterminate data rate for the
UGS grant interval period of the peer node connection setup time.
As a result, a method that is based on the peak stream bit rate,
using CBR allocation, would reduce resource network utilization,
whereas average bit rate CBR allocation can result in unacceptable
packet delay and jitter. The rtPS has been introduced to
accommodate such ﬂows; however, it can adjust the variable bit
rate to transmit a video stream or data stream. For rtPS, the BSs
provide periodic transmission opportunities by means of a basic
polling mechanism. The SSs can exploit these opportunities to ask
for bandwidth grants, ensuring that the bandwidth requests arrive
at the BSs within a given guaranteed interval.
Although growing numbers of researchers have considered the
bandwidth allocation of using QoS scheduling and controlled
packet transmission, very little attention has been given speciﬁ-
cally to suppose simply to adaptively allocate the VBR stream
with the largest queue size at each scheduling decision point.
Since the packet arrivals are stochastic there is no way to predict
when the next frame for the real-time stream will be scheduled.
The purpose of this study is to address using ertPS class through a
simulation of some QoS scheduling algorithm rule for real-time
trafﬁc with variable data rate (such as VoIP service with silence
suppression) over the IEEE 802.16e Wireless Broadband network.
Thus, this study model attempts for BSs to continuously offer the
same amount of bandwidth to SSs unless otherwise explicitly
requested by the SSs. The ertPS is a scheduling mechanism that
builds on the efﬁciency of both UGS and rtPS. A BS provides
unsolicited unicast grants as in UGS, while it records the
bandwidth request latency for every SSs. However, ertPS does
not allocate bandwidth on the basis of a ﬁxed packet size as in
UGS. The ertPS can adapt the amount of bandwidth allocation
dynamically depending on the trafﬁc data rate. Once the BS has
allocated bandwidth over transmit ﬂow buffer, ertPS can decrease
the amount of bandwidth allocation by SSs bandwidth request
character. The BS detects the allocated bandwidth resulting in
insufﬁciency in the event that SSs request an additional
bandwidth by piggybacking or acknowledging its amount on the
packet header.
The remainder of this article is organized as follows: in Section
2, we describe a VBR stream source system model and design a
Poisson Markov process for this VBR system model. In Section 3,
an algorithm is proposed that can be adopted for ertPS; this
algorithm improves the performance of the VBR transmission of a
multimedia stream with an indeterminate data rate over IEEE
802.16e BWA. In Section 4, we analyze and compare the
differences between three ertPS scheduling algorithms for a VBR
stream using similar VBR technology. Finally, in Section 5, some
conclusions are presented.
2. Supporting of VBR trafﬁc system model
The real-time multimedia trafﬁc on 802.16e WMANs is quite
bursty in nature. Simple models such as the Poisson processes are
unable to capture the important characteristics of these sources.
Different approaches are available to model bursty trafﬁc sources.
Many of these use Markov modulated processes (MMP). These are
doubly stochastic processes in which each state of N states of an
embedded Markov chain originates another stochastic process. If
this new process is a Poisson process, the MMP is called a Markov
modulated Poisson process. One simple case that is very suitable
for modeling VBR streaming sources is the On–Off model (see
Fig. 1). According to the On–Off model, the signal stream of a
single source is modeled through an alternating sequence of burst
and silent periods. The duration times of these burst and silent
periods are exponentially distributed by means of TON ¼ 1=l and
TOFF ¼ 1=m, respectively. Therefore, the expectation of the period
duration is denoted as E[TON] for the ON period and E[TOFF] for the
OFF period.
Assume that the call arrival probability in an ON state is
determined by a Poisson random variable X with parameter l,
where E½X ¼ l. Let r represent the utilization of the time-slot for a
VBR source (the percentage of a slot being used). Then, we have
r¼ lE½TON =ðE½TONþE½TOFF Þ. Given a cycle with length N and a
VBR source with the claimed peak rate ci and mean rate mi, we
have ci ¼ lN and mi ¼ rN¼ lNE½TON=ðE½TON þE½TOFF Þ. During the
burst period, packets with ﬁxed length are generated with a
constant inter-arrival time Dt.
Let N be the independent identical number of VBR streaming
ON/OFF sources is modeled by the Markov process with N+1
states (Beylot, 1993). In this model, the aggregate bit rate of the N
sources is quantiﬁed into a number of discrete levels, as shown in
Fig. 2. The state of the process is determined by the number of
active VBR sources k=0,1,y,N. The corresponding burst arrival
intensities are (0,1A,2A,y,NA). The denoted separate states and
quantiﬁed aggregate bit rate constitutes a Markov chain. The
probability densities of passing between nearby states are ðNkÞl
(passing from state k to k+1) and km (passing from k to k1).
Time sequence
Fluid flow
Time sequence
Fig. 1. VBR ON–OFF ﬂuid source coverage and packet source.
C.-P. Lin et al. / Journal of Network and Computer Applications 33 (2010) 467–476468
Author's personal copy
ARTICLE IN PRESS
used for connections in which there are no ﬁxed timing relation-
ships between samples, but which still require a guaranteed QoS.
This paper adapts the above Markov chain function, which must
consider VBR packet transmission for a real-time environment
under an 802.16 broad bandwidth (see Fig. 3). Moreover, the QoS
is adapted to transmit variable bit rate (VBR) packets under
802.16 broadband by considering the following parameters:
1. Network loss: IP datagram lost due to network congestion
(router buffer overﬂow).
2. Delay loss: IP datagram arrives too late for active state at
receiver. The packet service ﬂow delays include processing,
network queuing, end-system (sender, receiver) delays, etc.
3. This paper assumes that supporting of VBR trafﬁc packet size
tolerable between 50 and 400 bytes (see Fig. 4) for UGS
scheduling which has variable rate different from ﬁx data rate
sent to received packet for CBR (constant bit rate) transmitted.
4. Loss tolerance: There are some limitations of simulate research
from Fig. 5 upper limit data rates 1000 bytes for packet real-
time transmission VBR stream, while packet loss rates can be
tolerated between 1% and 10% for stable ON–OFF VBR
streaming at every slot time.
For an assumed set of N (SSs) process, SSs queue an M/M/1
single server system with a unity service rate, and the optimal
system total delay is characterized by the constraint caused by the
individual arrival rates of each input trafﬁc stream. We are
concerned with priority scheduling such as WRR, RR, and max–
min fairness queue scheduling statements that balance trafﬁc
characterization on the MAC layer. Once we take into a VBR
stream, with a bit-rate variable between a minimum and a
maximum, it will be quite inconvenient to assign more bandwidth
to a stream than is truly required. The proposed algorithm has
shown that variable bit rate (VBR) trafﬁc can be represented as a
two-state Markov chain. In one state, the transmission rate is less
than the average bit rate, while in the other state, it is more. Since
the system returns to the initial state after a complete cycle in the
Markov chain, it is possible to introduce the probability to deliver
all the packets accumulated during the cycle. However, with the
use of sequential round-robin allocation levels, this problem is not
likely to be an issue, because most of the time the space allocated
for real-time trafﬁc is much less than the space booked. Moreover,
the bandwidth allocated by BSs to SSs for real-time polling data
can be allocated fairly between minimum and maximum to adjust
for the real-time polling trafﬁc of each SS. Fairness measures or
metrics are used in wireless network engineering to determine
whether SSs or applications are receiving a fair share of BS
resources. There are several mathematical and conceptual deﬁni-
tions of fairness.
In the literature, Jain’s Fairness Index (Jain et al., 1984; Bulusu
et al., 2006) is often used for MAC layer networks to evaluate
throughput fairness. Therefore, it is proposed to include this
metric in the bandwidth allocation process. To verify the
throughput fairness, Jain’s Fairness Index can be calculated as
follows:
f ðx1; x2; . . . ; xiÞ ¼
ðPni ¼ 1 xiÞ2
ðNPni ¼ 1 x2i Þ ; ð5Þ
where xi indicates the mean throughput value of connection i and
N is the number of directed links in the network. The maximum
fairness index is Fairness Index (FI)=1. It corresponds to a
network where all links access the channel equally. If only k
directed links have an equal access to the channel and the
remaining links have no access to the channel, the fairness index
is k/N.
The deﬁnition of ﬂow xi also should take into account N, the
number of competing hosts. We propose to normalize the queue
size with respect to the number of SSs and compute the Jain index
for the queue sizes which are multiples of N, because only in this
case computing the Jain index makes sense. We call m such that
xi=mN, where m=0,1,y, a normalized queue size. The Jain index
will be computed as FI(m). This metric identiﬁes under-utilized
channels and is not unduly sensitive to atypical network ﬂow
patterns.
The max–min fairness queue scheduling states that small
ﬂows receive what they demand and larger ﬂows share the
remaining capacity equally. Bandwidth is allocated equally to all
ﬂows until one is satisﬁed; then, the bandwidth is equally
increased among the remainder and so on until all ﬂows are
satisﬁed or the bandwidth is exhausted. The FI values range
between 0 and 1, where a value of 1 implies that the
measurement values x1,y,xn are equal.
3.1. The max–min fairness
The notion of max–min fairness scheduling is well known in
political science (Rawls, 1971). It was introduced as a design
objective for communication networks by Bertsekas and Gallager
(1987). The principle of max–min fairness is to allocate network
resources in such a manner that the bit rate of a ﬂow cannot be
 0
 200
 400
 600
 800
 1000
20 20.2 20.4 20.6 20.8 21
Pa
ck
et
 S
iz
e 
(B
yte
s)
Time (second)
VBR: UGS with packet load
CBR: UGS with packet load
Fig. 4. UGS scheduling of VBR Venus CBR packet payload situation.
 0
 200
 400
 600
 800
 1000
17 17.5 18 18.5 19
Pa
ck
et
 S
iz
e 
(B
yte
s)
Time (second)
VBR: Real-time Polling Service with Packet Load
CBR: Real-time Polling Service with Packet Load
Fig. 5. The ertPS scheduling of VBR Venus CBR packet payload situation.
C.-P. Lin et al. / Journal of Network and Computer Applications 33 (2010) 467–476470
Author's personal copy
ARTICLE IN PRESS
In step 4 of iteration k the service rate of each SS requesting
service pAPk receives a minimum rate increment (calculated in
step 3) for the proposed algorithm in this paper. If service p is
not included in Pk, i.e., service p already uses a saturated link
from iteration k1 from the BS bandwidth allocation, then
its rate remains the same. Having calculated the total ﬂow Fa
(step 5 of iteration k), we then determine the new sets Ak+1, Pk+1
at steps 6 and 7, respectively, by removing all of the saturated
SSs with no successful links from a request to the BS, the
services that use them, and the capacity associated with the QOS
services. If we apply the aforementioned algorithm to the
WiMAX network of Fig. 7, then after four iterations we will
obtain the following results. The service calls A and D will be
allocated a rate of 5.55Mbps; service calls B and E, a rate of
11.1Mbps; service call C, a rate of 33.3Mbps; and service call F,
a rate of 50Mbps. The notion of fairness can be explained by
observing that services A and D, which have the same bottleneck
link (1–2), and services B and E, which also have the same
bottleneck link (4–5), require an equal amount of bandwidth
(5.55 and 11.1Mbps, respectively). As has already been shown,
the computation of a max–min fairness rate vector depends on
an eight-step algorithm whose iterations are equal to the
number of network links.
3.3. The bottleneck and active ﬂows
As mentioned above (see Algorithm 1), given a feasible rate
vector r, we can say that link a is a bottleneck link with respect to
r for an SS of P node crossing a if Fa ¼ Ca and rp ¼ r0p for all SSs¼ P0
crossing link a.
For notational convenience, we can deﬁne c0 ¼ C and cN+1=0. If
Xn
i ¼ 1
xici4C; ð9Þ
fair queuing realizes max–min fairness sharing with a fair rate y
given by
y¼ C
P
i4 JxiciP
i4 Jxi
; ð10Þ
Algorithm 1. Calculate bandwidth allocation using max–min
fairness queue scheduling algorithm.
Ak : denotes the set of SS links ﬂow not saturated at the
beginning of step k.
Pk : denotes the set of SS not passing through any saturated
link at the beginning of step k.
nka : denotes the number of SSs that use BS link a and are in P
k.
~ak : denotes the rate increment added to all of the SSs in Pk at
the k th step.
1: Initial conditions: k¼ 1; F0a ¼ 0; r0p ¼ 0; P1 ¼ P and A1 = A
2: nka= the number of SSs with a different QOS service pAP
k
crossing link a.
3: while nkaaNULL do
4: ~ak ¼minaAAk CaF
k1
a
nka
.
5: if pAPk then
6: build a high priority link a
7: else
8: fthe bit rate over the constrain set up bandwidth for
system allocation mechanismg:
9: end if
10:
rkp ¼
rk1p þ ~rk for pAPk
rk1p otherwise
:
(
11: Fka ¼
P
par
k
p (for those p, which use linka.)
12: Akþ1 ¼ fajCaFka40g _Akþ1 denotes the set of SS links
not saturated at the beginning of iteration k+1 under a
BS bandwidth allocation condition
13: Pkþ1 ¼ fpjdo not cross any SS link aAAkþ1g
14: k :¼k + 1
15:
16: if Pk is empty then
17: STOP
18: else
19: go to 1
20: end if
21: return per k
22: set of paths Pk and ﬂows 8
pj
i
ePi
f ðpjiÞ
23: end while
where J= J(x) is a unique integer, 1r JrN such that cJZgZcJþ1.
Variable ﬂows with a peak rate greater than or equal to cJ are
bottlenecked and realize the fair rate g, while the others preserve
their peak rate through the scheduler. If
Xn
i ¼ 1
xicirC;
all ﬂows preserve their peak rate.
Let J(x)=0 in this case. Note that J deﬁnes a partition of the state
space. Consider now the operation of the ertPS algorithm. The
bandwidth allocated for initial ranging and request contention
opportunities may be grouped together and always used with the
uplink burst proﬁled speciﬁed for initial ranging interval and
request intervals. During its scheduled bandwidth, an SS transmits
with the burst proﬁle speciﬁed by the BS. All bottleneck ﬂows are
determined by the operations that add and remove ﬂows from
burst proﬁled list. We deﬁne ﬂows included in burst proﬁled list to
be active ﬂows. The complexity of the sort in fact depends only on
the number of ﬂows in burst proﬁled list whose ﬁrst packet has a
time stamp different from Virtual Time (VirtualTime). This is the
number of bottleneck ﬂows whose packets are transmitted at the
current fair rate. In addition, any non-bottlenecked ﬂow that has
Fig. 7. The max–min fairness scheduling algorithm allocation in WiMAX MAC
layer.
C.-P. Lin et al. / Journal of Network and Computer Applications 33 (2010) 467–476472
Author's personal copy
ARTICLE IN PRESS
of 1000bps. Setting a higher priority weighting rate consumes
more transmission time and does not seem to do anything
noticeable.
Indeed, this scheduler allocates all of the symbols to one SS
even if it has no data to send. It should be noted that the ertPS
throughput of the existing RR scheduler is nearly 2 times worse
than that of other implemented ertPS schedulers (such as max–min
fairness queue scheduling, WRR scheduling, etc). The max–min
fairness queue schedulers clearly outperformed the other sche-
dulers, with a maximum throughput of 2Mbps. The schedulers
favor SSs with the highest values, and then the most efﬁcient. We
also observed that the RR scheduler provided less ertPS through-
put than that of the max–min fairness queue scheduling and WRR
schedulers. This is due to the fact that the channel quality of the
different SSs was not taken into consideration. Fig. 9 shows the
mean maximum threshold delay time of the ertPS connections as
a function of the ertPS trafﬁc load submitted in the network. The
mean delay time is a vital parameter for real-time applications.
We noted that the existing RR scheduler required a large average
delay to deliver a data packet. This is because this scheduler does
not provide sufﬁcient symbols to the SSs.
However, this average queue jitter includes the packet in the
BS server (physically, the packet being transmitted). Comparison
of jitter for the three scheduling algorithms. It is observed that the
jitter value has improved the accuracy of the predicted VBR
streaming trafﬁc to stable. Fig. 10 shows that there is a difference
between the packet start time and jitter value of the computed
index. The compute index is the average number of packets jitter
frequency for the BS resource of the queue. Fig. 11 show that RR
scheduling can balance the packet load for ertPS which it is
maximum no over 0.1 s queue jitter buffer under BS server. We
also notice that the ertPS RR scheduling less packet jitter
frequency than the max–min fairness queue scheduling (see
Fig. 13) and WRR schedulers (see Fig. 12) (Fig. 13).
We observe that as fairness-index (FI) increases, the difference
in the number of slot assignments to different SSs also increases
(as expected). We see from Figs. 14–16 that Jains Fairness Index
(Jain et al., 1984; Bulusu et al., 2006), however, remains above
about 80% for a fairly large range of FI, suggesting that it is
possible for the provider to trade off fairness for delay by choosing
an appropriate value of FI at which the fairness and the
bandwidth requirements of different users are satisﬁed. As the
number of contention SSs node and data rate increased, all
the fairness index also displayed a sharp increase to 0.96–0.99
index factor, whereas Fig. 14 RR scheduling indicated no
signiﬁcant proﬁciency between the contention SSs node on the
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 2.4
5500 5600 5700 5800 5900 6000 6100 6200 6300 6400
D
e
la
y
 T
im
e
 (
s
e
c
)
Packet Sequence
VBR with RR
VBR with Max-Min Fairness Scheduling
VBR with WRR
Fig. 9. The end-to-end delay time of ertPS QoS scheduling series for simulation
VBR transmit situation.
-0.1
0
0.1
0.2
0.3
0.4
0.5
J
it
te
r 
(s
e
c
)
Packet Start Time (sec)
VBR with RR
VBR with Max-Min Fairness Scheduling
VBR with WRR
0 20 40 60 80 100 120
Fig. 10. The ertPS-QoS-series scheduling of VBR packet jitter situation.
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0 20 40 60 80 100
J
it
te
r 
(s
e
c
)
Packet Start Time (sec)
VBR with RR
Fig. 11. The ertPS-RR scheduling of VBR packet jitter situation.
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0 20 40 60 80 100
J
it
te
r 
(s
e
c
)
Packet Start Time (sec)
VBR with WRR
Fig. 12. The ertPS-WRR scheduling of VBR packet jitter situation.
C.-P. Lin et al. / Journal of Network and Computer Applications 33 (2010) 467–476474
Author's personal copy
ARTICLE IN PRESS
References
Belghith A, Nuaymi L. Comparison of WiMAX scheduling algorithms and proposals
for the rtPS QoS class. In: Proceedings of the 14th European wireless
conference, January 2008. p. 1–6.
Bertsekas D, Gallager R. Data networks. Englewood Cliffs, NJ: Prentice-Hall; 1987.
Beylot A-L. Models De trafﬁc et commutateurs pour l’evaluation de la perte et du
delai dans les reseaux ATM, these de doctorat de l’Universite Paris 6;
September 1993.
Bulusu V, Durresi A, Paruchuri V, Jain R. Key distribution in mobile heterogeneous
sensor network. In: Proceedings of the IEEE GLOBECOM, San Francisco, CA,
November 27–December 1, 2006.
Camarda P, Striccoli D. Queueing networks approach for bandwidth estimation of
smoothed VBR video streams. Performance Evaluation 2004;57:1–18.
Chen J, Wang C-C, Chang C-W, Liu S-S, Guo J, Lien W-J, et al. The design and
implementation of WiMAX module for ns-2 simulator. In: Proceedings of the
ACM valuetools, Pisa, Italy, October 10, 2006.
Hahne EL. Round-robin scheduling for max–min fairness in data networks. IEEE
Journal on Selected Areas in Communications 1991;9(7):1024–39 vol. 46,
no. 6, pp. 784–98, 1998. Switzerland, 2003.
Jain R, Chiu D, Hawe W. A quantitative measure of fairness and discrimination for
resource allocation in shared computer systems. DEC Research Report TR-301,
September 1984.
Kelly FP. Charging and rate control for elastic trafﬁc. European Transactions on
Telecommunications 1997;8(1):33–7.
Kelly FP, Maulloo AK, Tan DKH. Rate control in communication networks: shadow
prices, proportional fairness and stability. Journal of the Operational Research
Society 1998;49(3):237–52.
Park E-C, Kim H, Kim J-Y, Kim H-S. Dynamic bandwidth request-allocation
algorithm for real-time services in IEEE 802.16 broadband wireless access
networks. In: IEEE INFOCOM 2008 proceedings, 2008. p. 1526–34.
Radunovic B, Le Boudec J-Y. Why max-min fairness is not suitable for multi-hop
wireless networks. Document, Ecole Polytechnique Fe´de´rale de Lausanne
(EPFL), Lausanne, Switzerland, 2003.
Rawls J. A theory of justice. Belknap Press; 1971.
Scorza GB, Sacchi C, Granelli F. An adaptive MAC-PHY approach for medium
access control in VBR MC-CDMA systems. In: Proceedings of the
International Conference on Multimedia Services Access Networks, June
13–15, 2005. p. 96–100.
Seungwan R, Byunghan R, Hyunhwa S, Mooyong S. Urgency and efﬁciency based
packet scheduling algorithm for OFDMA wireless system. In: Proceedings of
the IEEE ICC’05, vol. 4, May 2005. p. 2779–85.
Tassiulas L, Sarkar S. Maxmin fair scheduling in wireless networks. In: Proceedings
of the IEEE INFOCOM’02, vol. 2, New York, NY, USA, June 2002. p. 763–72.
Yang J, Yifan Z, Ying W, Ping Z. Average rate updating mechanism in proportional
fair scheduler for HDR. IEEE Communications Society Globecom 2004;57:
364–6.
C.-P. Lin et al. / Journal of Network and Computer Applications 33 (2010) 467–476476
Fig. 1. Two-tier networks deployment scenario.
targets, while femtocells causing smaller cross-tier interference
obtain higher SINR margins. [6] and [7] are adopt the concept
of utility function to calculate the interference between two-
tier networks in uplink, although the authors claim that their
proposed method can deal with downlink either, but in actually,
MBS subscriber can not correspond to the MBS as the way
FBS subscriber does to the FBS because MBS will not adjust
transmit power when suffered interference from FBS. To find
the proper solution for FBSs transmit power in downlink, we
attempt to solve this problem through non-cooperative game
[8] and integrated Nash Equilibrium (NE) [9] to achieve the
goal of centralized control the transmit power of each FBSs
when serve subscribers in downlink and avert subscribers
unable to receive data normally.
The rest of this paper is organized as follows. In section
II, a system model is presented. In section III describes game
theoretic formulation and section IV shows the performance
of proposed scheme. In section V, conclusions are given.
II. SYSTEM MODEL
The femtocell network under consideration is an MBS with
N FBSs, which are distributed around the MBS. There are
M femtocell subscribers randomly allocated in the N FBSs.
Assume that the distance between the FBS and its femtocell
subscribers is much shorter than the distance between the MBS
and the FBS, we neglect the disparity of different subscribers
which belong to the same FBS, as shown in Fig. 1. The
locations of FBSs are known and the location of femtocell
subscriber is same as its belonging FBS. Let Bi denote FBS i
and Si denote femtocell subscriber i belonging to Bi, where
1 ≤ i ≤ N and i ∈ I. The B0 represents the MBS and S0
represents its subscriber station. We assume all FBSs belong
to the same Internet Service Provider (ISP) so that they all
work in the same frequency (channel).
Open access and closed access are both femtocell access
policies [10]. In open access, FBS provides service to any
subscribers as long as close to it. In this case, the traffic load
are balanced but the quality of service and security are not
guaranteed for paying home subscriber. This work assumed
closed access, which means only licensed subscribers can
access it. The received signal-to-interference plus noise ratio
(SINR) by Si, denoted as γi, is expressed as
γi =
pigi∑
i ̸=j pigij + σ2
≥ Γi, (1)
where pi is the transmit power of Bi, gi is the channel gain
between Si and Bi, gij is the channel gain between Si and
other FBSs except gi, Γi means the low bound of SINR
required by Si, and σ2 represents for the variance of Additive
White Gaussian Noise (AWGN) at Si and Bi. We reference
the path loss model for fentocells from [11] , which shows
in (2) to get the attenuation when signal transmit between
femtocells.
L = 38.5 + 20log10(d) + Lwalls(dB) (2)
In (2), d is the distance from base station in meters and Lwalls
is the type of walls which signal penetrated. We determine
Lwalls as 10 dB for internal walls which suggested in [11].
III. UTILITY-BASED NON-COOPERATIVE GAME FOR
FEMTOCELL DOWNLINK NETWORKS
Since an individual FBS attends to maximize its transmit
power selfishly, all of other FBSs forced to increase transmit
power to maximum to response this situation. This egoistic
behavior will lead the network including MBS and FBSs to a
detest equilibria. In this section, we present a utility-based non-
cooperative game and our goal is serving all the subscribers
in the network effectively with limited frequencies.
We assume that FBSs are participate in a N player non-
cooperative power control game G = [Bn, {Pi} , {Ui (.)}].
Here n = {1, . . . , N} represents the player index set, Pi is
the strategy set describing the domain of transmit powers for
Si and finally each subscriber maximizes its individual payoff
according to utility function Ui. In this way, all players select
their transmit power for the best response to the actions of
other participants. Given BS i, designate p−i as the vector of
transmit powers of all BS except Bi.
We define, for all subscribers 0 ≤ i ≤ N , the power control
game to be expressed as
max
pi
Ui (γi, p−i) . (3)
for all pi except p0, we assume that their upper bound of
transmit power are all pmax. We are interested to find the
equilibrium point of transmit power for each subscriber in
N individually maximizes their utility in (3) while transmit
power of other subscribers are given. Such an equilibrium
in optimization problem is known as NE, no subscriber can
improve its utility without considering other subscribers. Also,
we assumed that all subscribers periodically inform its serving
BS their current SINR and each BS transmit the SINR value
2083
TABLE I
PARAMETERS USED IN NUMERICAL RESULTS.
Variable Parameter Value
dmax Max. femtocell radius 30m
pmax Max. transmission power per subscriber 1 watt
σ2 Additive White Gaussian Noise 20 dB
Lwalls Indoor walls attenuation 10 dB
gmax Max. channel gain 23 dB
γmin, γmax Max. and min. femtocell SINR target 5, 25 dB
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
83.34
83.36
83.38
83.4
83.42
83.44
83.46
83.48
83.5
Parameter ’a’ in proposed reward function 
S
c
o
re
 o
f 
u
ti
lit
y
 f
u
n
c
ti
o
n
 
 
SINR
Fig. 2. Relation between score of utility function and a.
The coefficient ai and bi should be considered carefully
because its related to the trade-off between FBS subscribers’
aspiration of maximize their transmit power to level up the
data rate and importance of satisfying the MBS subscribers’
QoS requirement. We will have a simulation to find the proper
value of these coefficient in the following section.
IV. NUMERICAL RESULTS
In this section, we numerically evaluate the performance
and details of the proposed method, considering the scenario
includes several femtocells randomly distributed in an area
and assuming that FBSs are set up in indoor environment and
use (2) as our femtocell path loss model . The AWGN power
σ2 in (2) was assumed 20 dB when employing maximum
transmission power. Simulation paramaters are summarized in
Table I.
For ∀i ≥ 1, assuming ai = a in (8) and we further
assume bi = 1 for ensuring the equal weight of reward
function and penalty function in (7). Fig. 2 shows the curve
of utility function when fixed SINR γi with increasing a. We
can observe that SINR value reaches a highest score of utility
function when raising a to 0.4.
Fig. 3 indicates the average SINR γi of Si, ∀i ≥ 1, will
decreased with increasing number of neighboring femtocells.
Nevertheless we still can obtain 17.6 dB when 50 femtocell
arounded which in much higher than Γi and gain about 1.52
dB compared with no power control used.
Fig. 4 shows the score of utility function of proposed
method with increasing number of femtocells and compared
with all Bi, ∀i ≥ 1, transmit with maximal power. This illus-
trate that our function can improve the throughput compared
with no power control scheme exercised. We still can earn
about 16 point in utility function when no power control
0 5 10 15 20 25 30 35 40 45 50
17
18
19
20
21
22
23
Number of femtocells (N) 
A
v
e
ra
g
e
 S
IN
R
 f
e
m
to
c
e
ll 
s
u
b
s
c
ri
b
e
rs
 (
d
B
)
 
 
No power control used
Chandrasekhar et al..
Proposed method
Fig. 3. Relation between average SINR of femtocell subscribers and number
of femtocells arounded.
0 5 10 15 20 25 30 35 40 45 50
0
10
20
30
40
50
60
70
80
Number of femtocells (N)
A
v
e
ra
g
e
 s
c
o
re
 o
f 
u
ti
lit
y
 f
u
n
c
ti
o
n
 
 
Max. power
Proposed method
Fig. 4. Score of utility function with different strategy.
scheme used only earned about 10 point within 50 femtocells
around and we can obtain the largest gain of 8 point when 10
femtocells arounded.
V. CONCLUSION
In this paper, we proposed a novel power control scheme.
We consider the femtocell environment which consists of a
macrocell and femtocells. Since the procedure to determine the
power of each femtocell can be regarded as a non-cooperative
game, we have described the behaviors of a whole system
using game model. Due to our proposed game model, a
centralized power control algorithm has been set up easily. We
have formulated and verified utility function for N femtocells.
Finally, through the analysis, we have shown that our proposed
power control scheme could be practically implemented in real
environment.
REFERENCES
[1] V. Chandrasekhar, J. G. Andrews, and A. Gatherer, “Femtocell networks:
a survey,” IEEE Commun. Mag., vol. 46, no. 9, pp. 59–67, Sept. 2008.
[2] A. Zemlianov and G. D. Veciana, “Cooperation and decision-making in
a wireless multi-provider setting,” in Proc. IEEE INFOCOM, vol. 1, pp.
386–397, Miami, FL, Mar. 2005.
[3] S. Y. Choi, T.-J. Lee, M. Y. Chung, and H. Choo, “Adaptive coverage
adjustment for femtocell management in a residential scenario,” Lecture
Notes in Computer Science, pp. 221–230, Sept. 2009.
[4] H. Claussen and F. Pivit, “Femtocell coverage optimization using
switched multi-element antennas,” in Proc. IEEE ICC’09, pp.1–6, Dres-
den, Germany, June 2009.
[5] E. Altman, T. Boulogne, R. El-Azouzi, T. Jiminez, and L. Wynter,
“A survey of network games in telecommunications,” Computers and
operations research, pp. 286–311, Feb. 2006.
2085
Max-Min Burst-Block Construction Algorithm for
Burst Mapping Problem in IEEE 802.16e System
Jenhui Chen∗ and I Chang
Dept. Computer Science and Information Engineering, Chang Gung University, Kweishan, Taoyuan, Taiwan, R.O.C.
Email: jhchen@mail.cgu.edu.tw
Abstract—The IEEE 802.16 communication protocol adopts
the burst data transmission scheme, which combines several data
units into one burst-block for transmission to one mobile station
(MS) over the OFDMA physical layer. Due to the characteristic of
downlink transmission in IEEE 802.16 specification, the shape of
the burst-block has to be constructed as rectangle. This constrain
leads to a problem that there may exist intra-burst and inter-
burst bandwidth wastage as constructing burst-blocks since each
block contains different size of combined data units. This paper
proposes an max-min burst-block algorithm (MBA) to reduce the
bandwidth wastage during the burst constructing stage, where the
term max-min we use here is defined as the minimum bandwidth
wastage MBA achieves is maximized. Simulation results show
that MBA outperforms other burst construction schemes.
Index Terms—Burst, OFDMA, IEEE 802.16, max-min, map-
ping, wireless
I. INTRODUCTION
The IEEE 802.16 family [1], [2] is designed for fixed and
mobile wireless access technologies. The orthogonal frequency
division multiple access (OFDMA) is a multiplexing tech-
nique, where different terminals are multiplexed in time and
frequency. The 802.16e standard mainly adopts OFDMA as its
physical layer (PHY). It can adapt the different environments
where the user is, provide high capacity, allocate the spectrum
resources more flexible and even improve coverage. However,
some restrictions on the usage of downlink radio resource
have been defined. Importantly, the burst which is a basic
unit for data transmission in the downlink period must be
constructed as rectangle. In the restriction, length and width
of the rectangle are defined on domain of time and frequency.
About composing burst, packets form scheduler will be
assigned into a suitable burst. But the method for defining
and allocating the burst is not standardized. According to dif-
ferent methods, mapping bursts will make different bandwidth
efficiency. Because the restriction leads that there may exist
wasted slots, intra-burst and inter-burst bandwidth wastage,
as constructing burst-blocks since the size of each burst is
various. Intra-burst waste slots is caused by the restriction
that all burst must be rectangle. The slots is allocated to a
burst, it does not carry any data and can’t carry any data.
Inter-burst waste slots means those slots are not be allocated
to any burst, and they can carry some other data. Fig. 1 shows
the good and bad mapping. Fig. 1(a) just has one inter-burst
∗This work was supported in part by the National Science Council, Taiwan,
R.O.C., under Contract NSC97-2221-E-182-035-MY2.
Intra-burst wastage
Inter-burst wastage
(unallocated resource)
Downlink subframe
time
fre
q
u
e
n
c
y
P
re
a
m
b
le
F
C
H
D
L
-M
A
P
Bad mapping (b)
Downlink subframe
time
fre
q
u
e
n
c
y
P
re
a
m
b
le
F
C
H
D
L
-M
A
P
Good mapping (a)
allocated resource 
Burst #1
Burst #2 Burst #3
Burst #4
Burst #5
cannot allocate
Burst #1
Burst #2
Burst #3
Burst #4
scheduled request (5 slots)
Fig. 1. The results caused by different mapping methods.
wastage. Fig 1(b) has five inter-burst wastage and one intra-
burst wastage, and there is one burst which can’t be allocated
and divided. Hence, Fig. 1(a) has more flexibility and less
wastage than Fig. 1(b).
There are just some approaches were presented for burst
construction and resource mapping. The first one, called Fixed
burst scheme, of the current approaches in IEEE 802.16
OFDMA downlink is that pre-segmenting the two-dimension
domain before assigning packets. The bursts that packets will
be assigned into are fixed and prearranged. This approach
is simple, thus the flexibility of resource allocations and the
efficiency of resource usage can be improved. The second
approach, Raster Scanning Algorithm has been presented in
[3]. It advances the throughput efficiently, but even might raise
the number of bursts. In [4], it tells that the amount of control
data overhead (DL-MAP IE) increases as the number of bursts
increases. This control overhead will cause the efficiency of
radio resource usage down overall. In [4], the problem has to
be considered. The third approach has been presented in [5].
By pre-defining the minimal unit of bursts and then combining
several units appropriately, the throughput of the third is as
same as the second. And the control data overhead is less
than the second. This approach seems to solve the problem of
the second, but there might cause wasted slots when allocating
packets into the pre-defining the minimal unit of bursts. Thus,
the same throughput in [3] and [5] represents that the wastage
caused by increasing control data in [3] is nearly equal to the
wasted slots in [5].
Necker et. al. [6] present a solution for burst mapping
problem. Its mapping policy is based on genetic algorithm.
It have to be run a lot of times repeatedly at one single period
Downlink subframe
time
fre
q
u
e
n
c
y
P
re
a
m
b
le
F
C
H
D
L
-M
A
P
Burst #1
(UL-MAP)
Burst #2
Burst #3
Burst #4
Burst #5
Burst #6
Burst #7
Burst #8
Burst #9
U
n
a
llo
c
a
te
d
re
s
o
u
rc
e
R
2
 (3
 s
lo
ts
)
R
3
 (1
0
 s
lo
ts
)
R
4
 (5
 s
lo
ts
)
R
5
 (2
 s
lo
ts
)
R
6
 (2
 s
lo
ts
)
R
7
 (4
 s
lo
ts
)
R
8
 (2
 s
lo
ts
)
R
9
 (4
 s
lo
ts
)
R
e
q
u
e
s
t 1
(3
 s
lo
ts
)
Burst Mapper
(2D Mapper)
Packet
Scheduler
enqueue
dequeue
scheduled
requests
Allocation status 
feedback

Packet 1-4
Packet 1-3
Packet 1-2
Packet 1-1
conection 2

Packet 2-4
Packet 2-3
Packet 2-2
Packet 2-1
conection 1
Resource schedule module

Fig. 3. Structure of packet scheduling and burst mapping.
to connection. In this paper, we focus on burst mapping sub-
module and assume that Qos aware scheduler has been well
defined.
For a purpose to waste as less as possible at every iteration
and make it recursive, we propose an algorithm MBA to reduce
waste slots at our best.
A. Precondition
We assume following preconditions in our system:
1) It is assumed that packet scheduler which has scheduled
all packets with their Quality of Service (QoS) already
finish its work. Thus, we focus on burst construction and
mapping.
2) We fix the length of DL subframe. Therefore we also
have the fixed resource of DL subframe. Also, packet
scheduler will arrange a set of bursts, which is composed
of some packets, at each frame. And the total size of the
set, bursts, coming form the scheduler every frame will
not exceed the resource for allocating burst.
3) Every packet size will not tempestuously change,
namely, we use Poisson distribution for all packet size.
In Section IV, we also use this for generating packet.
B. Burst mapping algorithm
Under the restriction of rectangular burst, we observe that
the rectangle resource will form rectangle or L shape after any
burst mapped. If we can make L shape resource to still be L
shape or rectangle, the resource shape is recursive. And for the
reason that the two type of resource shape can cause it low
complexity, we propose a mapping algorithm, MBA, with only
two case, rectangle and L shape (see Fig. 4). Our algorithm is
briefly shown in Fig. 6. We explain it in the following steps:
1) Sort the burst form packet scheduler with their size in
descend order. It can pretend that the bigger burst can’t
fit into the remaining resource at a later period of allocat-
ing bursts. Contrarily, the smaller burst will enhance the
possibility to be allocated into the left resource. Sorting
Case 1:
Case 2:
1 2 3 4 5 6 7 8 9 10 11
1
2
3
4
p
q
1
2
3
4
x
t
v
y
s
u
Allocated
region
Unused
region
Fig. 4. Two cases of our system.
in descend order can make our algorithm more flexible
and cause less wasted slots.
2) According to the main idea of two resource shapes, this
step will be divided into two case for allocating bursts.
Case 1: In the two shape of resource, it is obviously that
rectangle is much flexible for allocating. Hence,
we try to keep the resource shape to be rectangle
by allocating burst which is attached to length and
width of rectangle resource, p or q. By doing this,
we use division to determine if it can be allocated.
If it is not divisible by x or y, we list all factor
of user packet. There are two situations, prime or
not. In non-prime situation, we choose two nearest
factors from the set of factors as two sides of
rectangle for that the left resource, L shape, can be
used flexibly. For the same reason, we assign the
be divided by x, y, s, t, u, and v between 1 and κ:
|
6⋃
i=1
Ai| =
6∑
i=1
|Ai| −
∑
1≤i≤j≤6
|Ai
⋂
Aj |
+
∑
1≤i≤j≤k≤6
|Ai
⋂
Aj
⋂
Ak|
−...+ (−1)5|A1
⋂
...
⋂
A6|. (3)
In equation (3), the numbers that are divided by li denotes
Ai. And then we can get the ratio of coming packet is divisible
by six sides:
P =
|⋃6i=1Ai|
κ
(4)
We can get high ratio when the total number of slots in
time and frequency domain is small. If coming packet can be
divided, there will not be intentional waste slots for keeping
rectangle and L shape. There might be few intentional waste
slots with the smallest unless coming packet cannot be divided
by six sides. Therefore, it is also expected that the amount of
control data at head of a frame, which increases and causes
deterioration of resource efficiency in [3], can be decreased.
And the wasting slots in [5] even can be decreased more.
IV. SIMULATION
This section presents simulation result of the algorithm pro-
posed in [5], which we called Bucket later, and our algorithm,
MBA. In the following simulations, referring to [8], several
criteria are investigated to evaluate the performance of the
proposed scheme.
In our simulation, we define three condition for slots, used,
intra-burst waste, and inter-burst waste. The used slots presents
the slot in DL subframe is allocated to one of the bursts. That
also means it carries some user’s data. Intra-burst waste slots
is caused by the restriction all burst must be rectangle, and it
does not carry any data and can’t carry any data. Last, inter-
burst waste slots means those slots are not be allocated to carry
any data, and they can carry some other data. Therefore, slot
usage means the utilization of bandwidth. Intra-burst wastage
presents the disadvantage of using resource for transmission,
and inter-burst wastage is the space that still can carry data.
We assume packets arrival rate and the size are randomly
generated. Burst mapping will cut packet from burst which
can not be allocated by insufficient resource until that burst
can be put into.
Table. I lists simulation parameters.
TABLE I
PARAMETERS USED IN OUR SIMULATION.
The number of slots in time domain 15 slots
The number of slots in frequency domain 12 slots
Frame Duration 5 ms
Modulation types QPSK, 16-QAM, 64-QAM
Coding rate 1/2, 2/3, 3/4
5 6 7 8 9 10 11
50
55
60
65
70
75
80
85
90
95
Number of burst in a DL frame
Sl
ot
 u
sa
ge
 (%
)
 
 
Bucket
MBA
Fig. 7. Slot usage as function of burst number.
A. Performance against traffic load
First, we focus on the slot usage and intra-burst wastage
under different packet size. For the reason that we fix packet
size and average number of packets in one burst, the burst
number equally means traffic load.
The condition of slot usage is showed in Fig. 7. In this
simulation, the average size of burst will set to be 1800 bits.
We can observe the usage of bandwidth increase when burst
number growing, which also means the traffic load is heavy.
At the begin of this picture, the performance of two algorithm,
Bucket and MBA, is very likely. When the traffic load is
heavy, the slot usage Bucket just reach almost 78 percent,
but our algorithm, MBA, can reach 95 percent. Another
Fig. 8 shows intra-burst wastage of two algorithm. Intra-
burst wastage represent the slots which can not be allocated
to any burst, and this is a inefficient allocation. From this
figure, it can find that our algorithm, MBA, clearly behaves
the characteristic, our algorithm will waste as less as possible
at every burst were allocated into resource. The left resource,
total bandwidth minus used slots and intra-burst wasted slots,
can represent the flexibility. In Fig. 7 and Fig 8, our algorithm,
MBA, reach nearly 45 percent of left resource at the beginning,
and Bucket get only 30 percent. This means our algorithm use
the resource more efficiently.
B. Efficiency (slot usage)
From Fig. 7, we can find a phenomenon our algorithm will
perform much better when traffic is heavy. So, we assume a
situation that the traffic is very heavy, which means the data
packet scheduler assigned overtake the capacity resource can
carry. Fig. 9 shows the performance in 1000 DL subframes.
The slot usage generated by MBA is nearly 10 percent higher
than Bucket. Under two condition, heavy traffic and cutting
for last burst, there is no inter-burst wastage in unused slots.
So the slots that are not belong to used slot all are intra-burst
wastage. This means our algorithm, MBA, use the bandwidth
in a better way.
出 席 國 際 會 議 報 告 書 
報告人：陳仁暉 
會議地點及時間：自 2011 年 5 月 30 日~2011 年 5 月 31 日       中國香港 
會議名稱：HSIC 2011 
發表論文題目： 
Max-Min Burst-Block Construction Algorithm for Burst Mapping Problem in IEEE 
802.16e System 
一、參加會議經過 
此次參與兩岸四地(台灣、香港、新加坡、中國大陸)所共同與辦的國際會議
HSIC 2011，今年舉辦地點在香港九龍塘City University of Hong Kong國際會議廳
舉辦。會議共舉辦1天，包括Keynote Speech以及下午的Parallel Sessions: Oral 
Presentations。此一會議的主要討論議題以通訊技術為主，內容涵蓋Active 
Millimeter-Wave Circuit Design, Millimeter-Wave Bandpass Filters, RF and Wireless 
Networks Systems, Millimeter-Wave Antennas, Couplers and Passives, and Optical 
Networks and Systems等研究議題進行研討。以下是筆者在此次會議中所記錄下
來的相關技術報告： 
1. Keynote Speech 
此次大會共邀請三位國際通訊領域的著名學者蒞臨專題演講，分別為新加坡
南洋理工大學沈平教授Prof. Ping Perry Shum，中國大津大學的Prof. Jianguo Ma
以及香港城市大學的副校長Prof. Kwai-Man Luk等三位學者。第一場演說為
Optical Fiber Technology – Past, Present, and Future。演講內容介紹新加坡目前在
光纖通訊的發展方向以及目前的發展成果，並介紹新加坡的最新通訊實驗發展中
心，除了有40nano-meters製程中心外，其毫微米波振盪頻率可以達到150GHz的
水準，在此工業中佔有相當重要的領導角色。 
第二場Keynote Speech演講者為中國大津大學的Prof. Jianguo Ma，其演講的講
題為“Barcodes, RFIDs and Internet-of-Things”。其內容為描述近年RFID在應用上
會面對的一個實務面問題 –「RIFD的identification在實務應用上可能會面對同一
個特性加以發揮，以達到QoS的高品質服務實為一種，具挑戰性且具研究價值的
良好方向。 
2. Opening Parallel Sessions 
此次大會論文報告部分共分2個場次(6 parallel sessions)，每個場次皆以三個
parallel sessions來進行。我是分屬於第2個場次在Session 5: RF and Wireless 
Networks and Systems，報告時間在15:45pm~17:15pm。因為距離要報告的時間仍
然有一點時間，於是在中午Launch time時與香港城市大學的一位年輕學者Prof. 
薛泉(Quan Xue), IEEE Fellow (2011),討論了有關電磁波的最新研究與發展。薛教
授為兩岸三地少有的年輕學者，同時在年輕時期就當選了IEEE Fellow。圖二是
我與薛泉教授於HSIC 2011 venue上的合影。 
 
圖二、與薛泉教授於HSIC 2011合影。 
下午的報告包含了 
1. A Miniature Chip Filter Constructed with 90-nm CMOS Technology for 
77-GHz Unlicensed Applications 
2. Low Cost Microstrip Line Bandpass Filter for 60-GHz Applications 
3. LTCC Dual-band Filter with Stepped-Impedance Stub and Vertically Folder 
Structure 
4. Novel Tri-Band Bandpass Filter Based on Double-Sided Parallel-Strip Line 
5. Dual-Mode Dual-Band Bandpass Filter Based on a Single Patch Resonator 
 
圖三、筆者於HSIC 2011報告時的照片。 
第二天，在傅翔教授的邀請下參與了TPC的會議討論，在會中與兩岸四地的
學者討論要如何將這個重要的會議規模做到更好更大，這是一個十分難得的寶貴
經驗，它讓我認識了如何將一個重要的國際會議，推廣成為一個全球十分重要的
研究會議，並且從中學習舉辦國際重要會議的方法，這對於本校的未來國際化發
展，都是一項難能可貴的經驗。明年的HSIC2012最後確定在中國的古都南京舉
辦，為期二天同時接受工業界的系統進行Demo，這也是一項重要的決定。 
二、與會心得 
此次的與會認識了許許多多的教授與專家學者，對於電磁波通訊以及相關在
物理層的應用技術上，有許多的發展新技術與研究方向，這些重要的研究方法在
未來將可以有助於我個人在此領域的未來研究，有許多的幫助。圖四為此次參與
HSIC 2011共同與會的國內研究學者群。 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/08/11
國科會補助計畫
計畫名稱: 子計畫三：LTE/LTE-A服務品質保證下允入控制與排程器關係之跨層設計研究
(I)
計畫主持人: 陳仁暉
計畫編號: 99-2221-E-182-038- 學門領域: 網路
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
