  
exact time of performing ⊕ and that of 
communicating a message into account to obtain a 
member that requires the minimal running time on a 
specific multicomputer. 
In this paper, new families of computation-
efficient parallel prefix algorithms for message-
passing parallel computers with p PEs, where p < n, 
are presented. The communication time of the A 
family can be decreased to result in another family. 
We then use collective communications to further 
improve the communication time, and derive one 
more family of algorithms from each of the first two 
families. Collective communication operations, such 
as broadcast and scatter, can be better than an 
equivalent sequence of send or receive operations for 
ease of programming and execution efficiency [55]. 
The rest of this paper is organized as follows. 
Section 2 reviews the A family of parallel prefix 
algorithms for half-duplex message-passing 
multicomputers, including the computation time and 
communication time, as well as other properties, of 
this family. Section 3 shows how the communication 
time of the A family can be decreased to become the 
second family. Section 4 reduces the communication 
time by using collective communications to obtain 
two more families of algorithms. Section 5 shows 
that the four families of algorithms are not always 
effective when p < n, and derives a much stronger 
precondition of the algorithms. Section 6 compares 
prefix algorithms for multicomputers. Conclusions 
are finally drawn in Section 7.   
 
 
2   Review of the A family of parallel 
prefix algorithms 
In this section, we review the A family of parallel 
algorithms for solving the prefix problem of n inputs 
with p PEs, where p < n, on the half-duplex 
multicomputer model. The p PEs are represented by 
P1, P2,…, Pp. For ease of presentation, i:j is used to 
represent the result of computing  
xi ⊕ xi+1 ⊕…⊕ xj, where i ≤ j. 
 
Algorithm A(n, p, k) {Solving the prefix 
problem of n inputs, x1, x2,…, xn, using p PEs to 
generate y1, y2,…, yn, where p = kq + 1, k ≥ 1, q ≥ 1. 
In phase 1, k PEs are assigned to perform only 
computations; all the other PEs need to communicate 
among themselves, except when p – k = 1. To use 
this algorithm effectively, n ≥ (p2 + kp + k + 1)/2 is 
required. For ease of presentation, assume that all 
numerical values are integers.} 
Phase 1: Partition the inputs into two parts N1 = 
(x1, x2,…, xv) and N2 = (xv+1, xv+2,…, xn), where  
0 < v < n. The value of v will be explained shortly. If 
p = k + 1, then P1 uses N1 to compute outputs y1, 
y2,…, yv sequentially; otherwise, P1, P2,…, Pp–k use 
N1 to compute y1, y2,…, yv by invoking A(v, p – k, k) 
recursively. In the mean time, N2 is first distributed 
evenly among the other k PEs, Pp–k+1, Pp–k+2,…, Pp; 
each of the PEs holds c = (n – v)/k input values. 
These k PEs then take c – 1 parallel computation 
steps to compute z1 = (z1,1, z1,2,…, z1,c), z2 = 
(z2,1, z2,2,…, z2,c),…, zk = (zk,1, zk,2,…, zk,c), 
respectively, where 
zi,j = (v + (i – 1)c + 1):(v + (i – 1)c + j). 
The value of v is chosen to make the total number 
of computation steps in this phase required by the 
first p – k PEs equal to that required by the other k 
PEs, and it is given later. Note that yv is obtained by 
Pp–k. 
(Figure 1 should help the reader understand Phase 
1.) 
 
Phase 2: Initially, Pp–k sends yv to all the other 
PEs. Next, Pp–k+1 scatters, i.e., partitions and 
distributes, z1 among all the PEs evenly, each PE 
having c/p of the c values. All the PEs then 
concurrently perform  
yv+i = yv ⊕ z1,i, i = 1, 2,…, c, 
in c/p computation steps. Note that yv+c is computed 
by Pp. 
(Figure 2 should help the reader understand Phase 
2.) 
 
Phase m (m = 3, 4,…, k + 1): Initially, Pp sends 
yv+(m–2)c to all the other PEs. Next, Pp–k+m–1 scatters  
zm–1 among all the PEs evenly, each PE having c/p 
values. All the PEs then concurrently perform 
yv+(m–2)c+i = yv+(m–2)c ⊕ zm–1,i, i = 1, 2,…, c, 
in c/p computation steps. Note that yv+(m–1)c is 
computed by Pp. 
(Figure 3 should help the reader understand 
Phases 3 through k + 1.) 
 
 
(x1, x2,…, xn)
(x1, x2,…, xv) (xv+1, xv+2,…, xn)
used in running
A(v, p – k, k)
on the first p – k
PEs recursively
y1, y2,…, yv z1, z2,…, zk
where zi = (zi,1, zi,2,…, zi,c)
zi,j = (v+(i–1)c+1):(v+(i–1)c+j)
distributed to the last k PEs,
each with c = (n – v)/k values
 
Fig. 1. Phase 1 of Algorithm A(n, p, k)  
when p > k + 1. 
 
  
C(n, 7, 3) = 7n/37 – 1 + (n/37) × 3  
= 10n/37 – 1. 
The total number of communication steps is 
R(n, 7, 3) = R(16n/37, 4, 3) + 6 × 5 = 45. 
C(n, p, k) and R(n, p, k) in the general case, as 
well as other properties of Algorithm A, such as the 
values of p and k that lead to the smallest C(n, p, k) 
and R(n, p, k), are given in the following theorems. 
 
Theorem 1. When p = k + 1, v=n/p; otherwise, 
v =
2
2
1
1
p kp k
p kp k
− + +
+ + +
n. 
 
Theorem 2. C(n, p, k) =
2
2 ( )
1.
1
n p k
p kp k
+
−
+ + +
 
 
Theorem 3. If d > a and e > b, then  
C(n, a, b) > C(n, d, b) > C(n, d, e). 
 
Theorem 4.  R(n, p, 1) = p(p − 1); 
R(n, p, k) = (2k − 1)(p – 1)(p + k – 1)/2k   for k ≥ 2. 
 
Theorem 5. If d > a, then R(n, d, k) > R(n, a, k). If  
p ≥ 2k2 – 3k + 1 and k ≥ 2, 
then  
R(n, p, k) ≤ R(n, p, 1) 
and  
R(n, p, 2) ≤ R(n, p, k) < R(n, p, k + 1); 
otherwise,  
R(n, p, 1) < R(n, p, k) < R(n, p, k + 1). 
 
Theorem 6. Algorithm A is cost optimal when n = 
Ω(p
3
). 
 
Theorem 7. To use Algorithm A(n, p, k) effectively, 
it is required that n ≥ (p2 + kp + k + 1)/2. 
 
Note that Theorems 4 and 7 reveal that the 
communication time is independent of n when n ≥ 
(p
2
 + kp + k + 1)/2. Like Algorithms EK and L, 
Algorithm A is practical when the amount of time 
required to perform a binary operation ⊕ is greater 
than that required to transfer a message between two 
PEs. This situation may happen, for example, when 
the binary operation is a time-consuming floating-
point matrix multiplication or a series of matrix 
multiplications. 
 
 
3 Algorithm B with reduced 
communication time 
When p ≥ 2k + 1 and k ≥ 2, Algorithm A can be 
modified to become a faster algorithm named B. 
Recall that in phase 1 of Algorithm A, the number of 
computation steps required by the first p – k PEs 
equals the number of computation steps required by 
the other k PEs. In addition, the first p – k PEs 
communicate among themselves in phase 1, but the 
last k PEs perform no communication operations; 
thus, the last k PEs are idle for the amount of time 
that the first p – k PEs take to communicate. 
To reduce this idle time, we note that in Algorithm 
A there are communications among the last k PEs in 
phases 2 through k + 1, which can be performed in 
phase 1, but the communications involving the first 
p – k PEs in phases 2 through k + 1 should not be 
moved to phase 1. In each of these k phases, the 
original communications that can be moved to phase 
1 are message transfers from one of the last k PEs to 
the other k – 1 PEs. Thus, at most k(k − 1) 
communication steps can be moved to phase 1.  
Note that we still need to make sure whether all or 
only part of the k(k – 1) communication steps should 
be moved. Comparing k(k – 1) with the number of 
communication steps performed in phase 1 of 
Algorithm A, which is R(v, p – k, k), can clarify this. 
From Theorem 4,  
R(v, p – k, k) = (2k − 1)(p − k − 1)(p − 1)/2k. 
Since p ≥ 2k + 1, we have 
R(v, p – k, k) ≥ (2k − 1)k(2k)/2k = k(2k − 1). 
Thus, 
R(v, p – k, k) – k(k – 1) ≥  k2 > 0. 
That is, R(v, p – k, k) > k(k − 1). Therefore, all the 
k(k – 1) communication steps can be performed in 
phase 1 without increasing the communication time 
needed in phase 1. Clearly, Algorithm B(n, p, k) 
takes less communication time than Algorithm 
A(n,  p, k). Note that the two algorithms take the 
same amount of computation time. 
Let S(n, p, k) denote the number of 
communication steps required by Algorithm 
B(n, p, k). Thus, S(n, p, k) is the sum of the following 
components:  
(i) S(v, p – k, k) communication steps required by 
the first p – k PEs when invoking B(v, p – k, k) 
in phase 1.  
(ii) p – 1 communication steps required by Pp–k to 
send yv to the other p – 1 PEs in phase 2.  
(iii) (k – 1)(p – 1) communication steps required by 
Pp to send yv+ic to the other p – 1 PEs in phase 
i + 2, for i = 1, 2,…, k – 1. 
(iv) k(p – k) communication steps taken to distribute 
part of zi, for 1 ≤ i ≤ k, evenly among P1, P2,… 
Pp–k in phases 2 through k + 1. 
 
Note that in phase k + 1, since the 
communications that contribute to component (iii) 
and those that contribute to component (iv) are all 
  
Theorem 2, a larger k leads to less computation time. 
As for the communication time, from Theorem 5, 
R(n, p, k) ≤ R(n, p, 1) when p ≥ 2k
2
 – 3k + 1 and k ≥ 
2; otherwise, R(n, p, k) > R(n, p, 1). That is, A(n, p, k) 
is definitely faster than L when p ≥ 2k
2
 – 3k + 1 and 
k ≥ 2. However, when p < 2k
2
 – 3k + 1, we need to 
know the ratio of the time required by a 
communication step to the time required by a 
computation step, τ, to decide which algorithm is 
faster. 
Algorithm B can also be faster than Algorithm L. 
To compare their communication times, we see that 
when k ≥ 2, 
R(n , p, 1) – S(n, p, k) = (p
2
 – kp – 2k
3
 + 6k
2
 –4k)/2k. 
Recall that Algorithm B exists only when p ≥ 2k + 1 
and k ≥ 2. Therefore, when p2 – kp – 2k3 + 6k2 – 4k > 
0, p ≥ 2k + 1, and k ≥ 2, Algorithm B is faster than 
Algorithm L; otherwise, we need to know the value 
of τ  to decide which is faster. 
To help the reader understand the relative merits 
and drawbacks of these algorithms, we give some 
example figures in the following. Fig. 4 shows the 
numbers of computation steps required when n = 
8192 and p = 13. Note that k represents the number 
of PEs that perform no communications in phase 1 of 
our algorithms. Since PLL is a very different 
algorithm, it should not appear in the figures; 
however, for easy comparison, we show its related 
data in every figure as if k = 1. Clearly, Algorithms 
A and E each take the fewest number of computation 
steps when k = 12. Note that when 2 ≤ k ≤ (p – 1)/2, 
i.e., when Algorithms B and F are defined, the two 
algorithms each take the same number of 
computation steps as Algorithm A. Algorithm PLL 
takes the most, 1265, computation steps. 
Figure 5 shows the numbers of communication 
steps required by four algorithms. We assume that 
each collective communication involving p PEs 
requires the same amount of time as 2 log2 p point-
to-point communications. Algorithm E takes the 
fewest communication steps among the four 
algorithms. As already mentioned, although the exact 
number of communication steps of Algorithm F is 
not derived, Algorithm F, when defined, needs 
slightly less communication time than Algorithm E. 
Moreover, EK takes 3550 communication steps, 
which is too large to fit in the figure; PLL takes 7 
communication steps, which is too small to fit in the 
figure. 
k
0 2 4 6 8 10 12 14
N
u
m
b
e
r 
o
f 
c
o
m
p
u
ta
ti
o
n
 s
te
p
s
1200
1210
1220
1230
1240
1250
1260
1270
Algorithms A and E
Algorithms EK and L 
Algorithm PLL
Algorithms B and F
 Fig. 4. The numbers of computation steps required 
when n = 8192 and p = 13. 
 
k
0 2 4 6 8 10 12 14
N
u
m
b
e
r 
o
f 
c
o
m
m
u
n
ic
a
ti
o
n
 s
te
p
s
50
100
150
200
250
300
Algorithm A
Algorithm L
Algorithm B
Algorithm E
 Fig. 5. The numbers of communication steps 
required when n = 8192 and p = 13. 
 
 
Note that τ  has an impact on the total running 
time. If τ  is small, which may be true when the 
binary operation ⊕ is matrix multiplication, our new 
algorithms are more probable to be faster than 
previous ones. For example, Figs. 6 and 7 show the 
running times of algorithms when n = 8192 and p = 
13, for τ = 1 and 0.1, respectively. When τ = 1, Fig. 6 
shows that Algorithm E is faster than A, B, and L; 
however, Algorithm PLL is the fastest. Algorithm F, 
which is not shown in the figure, should be faster 
than E, but might be slower than PLL. 
  
usually needs less communication time than the 
multicomputer. Thus, the multiprocessor should also 
be a good platform for the presented algorithms. 
 
 
References:    
[1] S. G. Akl, Parallel Computation: Models and 
Methods, Prentice-Hall, 1997. 
[2] S. Aluru, N. Futamura, and K. Mehrotra, 
Parallel biological sequence comparison using 
prefix computations, Journal of Parallel and 
Distributed Computing, Vol. 63, No. 3, 2003, 
pp. 264-272. 
[3] A. Bilgory and D. D. Gajski, A heuristic for 
suffix solutions, IEEE Transactions on 
Computers, Vol. C-35, No. 1, 1986, pp. 34-42. 
[4] G. E. Blelloch, Scans as primitive operations, 
IEEE Transactions on Computers, Vol. 38, No. 
11, 1989, pp. 1526-1538. 
[5] R. P. Brent and H. T. Kung, A regular layout for 
parallel adders, IEEE Transactions on 
Computers, Vol. C-31, No. 3, 1982, pp. 260-264. 
[6] D. A. Carlson and B. Sugla, Limited width 
parallel prefix circuits, Journal of 
Supercomputing, Vol. 4, No. 2, 1990, pp. 107-
129. 
[7] L. Cinque and G. Bongiovanni, Parallel prefix 
computation on a pyramid computer, Pattern 
Recognition Letters, Vol. 16, No. 1, 1995, pp. 
19-22. 
[8] R. Cole and U. Vishkin, Faster optimal parallel 
prefix sums and list ranking, Information and 
Computation, Vol. 81, No. 3, 1989, pp. 334-352. 
[9] A. Datta, Multiple addition and prefix sum on a 
linear array with a reconfigurable pipelined bus 
system, Journal of Supercomputing, Vol. 29, No. 
3, 2004, pp. 303-317. 
[10] C. Efstathiou, H. T. Vergos, and D. Nikolos, 
Fast parallel-prefix modulo 2n + 1 adders, IEEE 
Transactions on Computers, Vol. 53, No. 9, 
2004, pp. 1211-1216. 
[11] O. Egecioglu and C. K. Koc, Parallel prefix 
computation with few processors, Computers 
and Mathematics with Applications, Vol. 24, No. 
4, 1992, pp. 77-84. 
[12] S. C. Eisenstat, O(log* n) algorithms on a Sum-
CRCW PRAM, Computing, Vol. 79, No. 1, 
2007, pp. 93-97. 
[13] A. Ferreira and S. Ubeda, Parallel complexity of 
the medial axis computation, in Proc. Int. Conf. 
on Image Processing, Washington, D.C., 1995, 
pp. 105-108. 
[14] F. E. Fich, New bounds for parallel prefix 
circuits, in Proc. 15th Symposium on the Theory 
of Computing, 1983, pp. 100-109. 
[15] A. L. Fisher and A. M. Ghuloum, Parallelizing 
complex scans and reductions, in Proc. ACM 
SIGPLAN '94 Conf. on Programming Language 
Design and Implementation, Orlando, FL, 1994, 
pp. 135-146. 
[16] W. Gropp, E. Lusk, and A. Skjellum, Using 
MPI: Portable Parallel Programming with the 
Message-Passing Interface, MIT Press, 1994. 
[17] T. Han and D. A. Carlson, Fast area-efficient 
VLSI adders, in Proc. 8th Computer Arithmetic 
Symposium, Como, Italy, 1987, pp. 49-56. 
[18] D. R. Helman and J. JaJa, Prefix computations 
on symmetric multiprocessors, Journal of 
Parallel and Distributed Computing, Vol. 61, 
2001, pp. 265-278. 
[19] L.-L. Hung and Y.-C. Lin, Parallel prefix 
algorithms on the multicomputer, WSEAS 
Transactions on Computer Research, Vol. 3, No. 
4, 2008, pp. 229-239. 
[20] L.-L. Hung and Y.-C. Lin, Two families of 
parallel prefix algorithms for multicomputers, in 
Proc. 7th WSEAS International Conference on 
Telecommunications and Informatics, Istanbul, 
Turkey, 2008, pp. 37-43. 
[21] Inmos, The Transputer Databook, 3rd ed., 
Inmos, 1992. 
[22] P. M. Kogge and H. S. Stone, A parallel 
algorithm for the efficient solution of a general 
class of recurrence equations, IEEE 
Transactions on Computers, Vol. C-22, No. 8, 
1973, pp. 783-791. 
[23] D. W. Krumme, G. Cybenko, and K. N. 
Venkataraman, Gossiping in minimal time, 
SIAM Journal on Computing, Vol. 21, No. 1, 
1992, pp. 111-139. 
[24] C. P. Kruskal, T. Madej, and L. Rudolph, 
Parallel prefix on fully connected direct 
connection machines, in Proc. Int. Conf. on 
Parallel Processing, St. Charles, IL, 1986, pp. 
278-284. 
[25] C. P. Kruskal, L. Rudolph, and M. Snir, The 
power of parallel prefix, IEEE Transactions on 
Computers, Vol. C-34, 1985, pp. 965-968. 
[26] R. E. Ladner and M. J. Fischer, Parallel prefix 
computation, Journal of the ACM, Vol. 27, No. 
4, 1980, pp. 831-838. 
[27] S. Lakshmivarahan and S. K. Dhall, Parallel 
Computing Using the Prefix Problem, Oxford 
University Press, 1994. 
[28] S. Lakshmivarahan, C. M. Yang, and S. K. 
Dhall, On a new class of optimal parallel prefix 
circuits with (size + depth) = 2n – 2 and lοg n 
≤ depth ≤ (2 log n – 3), in Proc. Int. Conf. on 
Parallel Processing, St. Charles, IL, 1987, pp. 
58-65. 
  
[54] H. Wang, A. Nicolau, and K. S. Siu, The strict 
time lower bound and optimal schedules for 
parallel prefix with resource constraints, IEEE 
Transactions on Computers, Vol. 45, No. 11, 
1996, pp. 1257-1271. 
[55] Z. Xu and K. Hwang, Modeling communication 
overhead: MPI and MPL performance on the 
IBM SP2, IEEE Parallel & Distributed 
Technology, Vol. 4, No. 1, 1996, pp. 9-23. 
 [56] F. Zhou and P. Kornerup, Computing moments 
by prefix sums, Journal of VLSI Signal 
Processing Systems, Vol. 25, No. 1, 2000, pp. 5-
17. 
[57] H. Zhu, C.-K. Cheng, and R. Graham, 
Constructing zero-deficiency parallel prefix 
circuits of minimum depth, ACM Trans. on 
Design Automation of Electronic Systems, Vol. 
11, No. 2, 2006, pp. 387-409. 
[58] R. Zimmermann, Binary Adder Architectures 
for Cell-Based VLSI and Their Synthesis, Ph.D. 
thesis, Swiss Federal Institute of Technology 
(ETH), Zurich, 1997. 
 
 
Note: This report has been published as               
New families of computation-efficient parallel 
prefix algorithms, WSEAS Trans. on 
Computers, Vol. 8, No. 10, 2009, pp. 1651-
1660.  
 
無研發成果推廣資料 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
