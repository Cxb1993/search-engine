 1 
中文摘要 
 
摘要-本計畫的主要目的是結合線性參數化
回歸理論中之強韌回歸理論  (robust regression 
theory) 及神經網路 (neural networks) 發展出新
一代具神經網路架構之強韌學習機  (robust 
learning machines)。其原始動機是將線性參數化統
計回歸理論中之韌性估測器 (robust estimators) 
對 離 群 值 或 稱 歧 異 點  (outliers) 之 韌 性 
(robustness) 或抗性 (resistance) 引入非參數化非
線性機器學習之領域中，期望能發展出對離群值
具韌性或抗性的強韌學習機。本計畫擬使用的學
習機為類神經網路 (artificial neural networks) 及
核基回歸器 (kernel-based regressors) 兩種回歸模
型 (以下通稱為神經網路)。 
 
本研究計畫分為兩年期進行。第一年研究計
畫是探討具韌性之 M- 神經網路  (M-neural 
networks) 及其學習法則。在訓練 M-神經網路方
面，我們使用 Tukey’s biweight function, Huber’s t 
function, Hampel’s 17A function, Ramsay’s aE  
function 等為代價函數。第二年研究計畫則探討
具抗性之強抗神經網路 (resistant neural networks) 
及其學習法則。我們將推廣 Least Timmed Squares 
(LTS) 估測法於上述的兩種回歸模型，以發展強
抗神經網路。 
關鍵詞：強韌回歸、強抗回歸、離群值、神
經網路、機器學習、強韌學習機、類神經網路、
核基回歸器。 
 
英文摘要 
 
Abstract—The goal of this two-year project is to 
develop new-generation robust learning machines by 
combining the robust regression theory for 
parametric linear models and the nonparametric 
neural networks.  The motive of this research is to 
generalize the linear parametric estimators, which 
are robust or resistant to outliers in the data, to 
nonparametric nonlinear learning machines.  The 
artificial neural networks and the kernel-based 
regressors will be used as our example learning 
machines. 
 
In the first year, we will develop robust M-neural 
networks together with their updating rules.  
Tukey’s biweight (or bisquare) function, Huber’s t 
function, Hampel’s 17A function, and Ramsay’s aE  
function will be used as the cost functions when 
training the aforementioned learning machines.  
The focus of the second year is the development of 
resistant neural networks and their updating rules.  
The method of the least trimmed squares will be 
utilized for the development of the aforementioned 
learning machines. 
 
Keywords—Robust regression, resistant 
regression, outlier, neural network, machine learning, 
robust learning machine, artificial neural network, 
kernel-based regressor. 
 
一、前言 
 
In many practical applications, data collected 
inevitably contain one or more anomalous 
observations called outliers; that is, observations that 
are well separated from the majority of the data, or 
in some way deviate from the general pattern of the 
data.  As is well known in linear regression theory, 
classical least squares fit of a regression model can 
be very adversely influenced by outliers, even by a 
single one, and often fails to provide a good fit to the 
bulk of the data [1].  Robust or resistant regression 
that is resistant to the adverse effects of outlying 
response values has been the subject of an enormous 
amount of literature over the past decades.  In 
robust or resistant regression, rather than omitting 
outliers, it dampens their influence on the fitted 
regression curve by down-weighting them.  One of 
the main approaches to robust regression involves 
M-estimation [1]-[6].  One of the main resistant 
methods is the least trimmed squares (LTS) [7]-[10].  
LTS method is good for dealing with data where we 
expect there to be a certain number of observations 
that we want to have no weight in the modeling.  
Another resistant method is the least median squares 
(LMS).  Unfortunately, according to the 
R-documentation, there seems no reason other than 
historical to use the LMS because of its low 
efficiency.  Consequently, we will not consider the 
LMS method in this study.  A regressor or a 
learning machine is said to be robust if it is not 
sensitive to outliers in the data. 
 
Machine learning, namely learning from examples, 
has been an active research area for several decades.  
Artificial neural network (ANN) has long been 
reckoned as a successful learning machine for many 
problems in science and engineering.  The class of 
approximating functions represented as artificial 
neural networks, with the number of hidden nodes 
unfixed, possesses the universal approximation 
 3 
Let nX ℜ⊆  and pY ℜ⊆ .  Suppose we are 
given the training set 
 
 
( ){ } YXdxS l
qqq
×⊆=
=1
,:  or 
( ){ } pnl
qqq
dzS ℜ×ℜ⊆= +
=
1
1
,: . 
 
In the following, we will use the subscript q to 
denote the qth example.  For instance, qix  denotes 
the ith component of the qth pattern nqx ℜ∈ , lq ∈ , 
ni ∈ .   
 
The error (or residual) qke  at the kth output node 
due to the qth example is defined by 
 
qkqkqk yde −=: , lq ∈ , pk ∈ . 
 
In the usual LS (least squares) approach, the goal is 
to choose weights that minimize the total sum of 
squared errors given by 
 
 ∑∑
= =
=
l
q
p
k
qktotal eE
1 1
2
2
1
: . 
 
In the LTS approach, the goal is to choose 
network weights that minimize the total sum of 
trimmed squared errors given by 
 
 ( ) ( )∑∑∑∑
= == =
==Ψ
p
k
l
q
qkeR
p
k
l
q
kqtotal eae
qk
1 1
2
1
*
1
2
22
1
2
1
,  
 
where ll ≤*  and the penalizing weight ja  is 
defined by 
 



≤<
≤≤
=
.*,0
*,1,1
:
ljl
lj
a j  
 
Here, ( )2qkeR  denotes the rank of the residual 2qke  
among 21ke , …, 
2
lke  and ( ) ( )
22
1 ... klk ee ≤≤  are the 
ordered values of 21ke , …, 
2
lke .  A popular choice 
of *l  is  
 
[ ] ( )[ ]222* ++= mll ,  
 
where [ ]z  indicates the largest integer less than or 
equal to z.  In the following, we call ( ) lll *−  the 
trimming percentage. 
 
Define the sum of trimmed squared errors at the 
output node k as 
 
 ( ) ( )∑∑
==
==Ψ
l
q
qkeR
l
q
kqk eae
qk
1
2
*
1
2
22
1
2
1
: , pk ∈ , 
 
then, we have 
 
 ∑
=
Ψ=Ψ
p
k
ktotal
1
. 
 
Now, we introduce an incremental gradient 
descent algorithm.  In this algorithm, kΨ s are 
minimized in sequence [19].  The updating rules 
are given by 
 
 
jk
k
wjkjk
w
ww
∂
Ψ∂
−← η  
( ) ( )∑
=
+=
l
q
qjqkokqkeRwjk rsfeaw qk
1
'
2η , 
 
ij
k
vijij
v
vv
∂
Ψ∂
−← η  
( ) ( ) ( )∑
=
+=
l
q
qiqjhjjkqkokqkeRvij zufwsfeav qk
1
''
2η , 
 
where 0, >vw ηη  are learning rates. 
 
II. KERNEL-BASED LTS-REGRESSORS 
(KLTSRs) 
 
In this part, we introduce the kernel-based LTS 
regressors (KLTSRs).  Let nX ℜ⊆  and ℜ⊆Y .  
Suppose we are given the training set 
 
 
( ){ } YXdxS l
qqq
×⊆=
=1
,: . 
 
Motivated by the support vector regressor (SVR), 
suppose the predictive function is given by 
 
 
( ) ( ) bxxKzxfy l
j
jj +== ∑
=1
, , 
 
where ( )⋅⋅,K  is a given kernel on XX × .   
 
The KLTSR can be represented as a feed-forward 
network as shown in Fig. 2.  Note that when we use 
the usual Euclidean inner product as the kernel, the 
KLTSR becomes a linear regressor.   
 
 5 
randomly chosen y-values corrupted by adding 
random values from a uniform distribution defined 
on [ ]1,1− .  In the following simulations, 30% 
randomly chosen y-values of the training data points 
will be corrupted.  As will be clear from the 
corresponding figures, this indeed means that the 
target values are highly corrupted.   
 
In the following simulations, the activation 
functions of the output nodes for LTS-ANNs are all 
linear functions with unit slope, and those of the 
hidden nodes are bipolar sigmoidal functions. 
 
Example 1: Suppose the true regression function 
is given by the sinc function 
 
 ( ) ( )

≠
=
=
,0,sin
,0,1
xxx
x
xg  [ ]10,10−∈x . 
 
The parameter settings for LTS-ANNs and 
KLTSRs are shown in Table 1.  For the given 
corrupted data, Fig. 3 shows the fitted curves using 
ANN, LTS-ANNs, and KLTSRs with 10%, 20%, 
and 30% trimming percentages.  As observed from 
this figure, the predictive functions with 20% and 
30% trimming percentages perform much better than 
those with 0% (ANN) and 10% trimming 
percentages, and are robust against outliers.  
Moreover, the predictive function determined by the 
KLTSR with 20% trimming percentage is almost 
indistinguishable from the true regression function. 
 
Example 2: Suppose the true regression function 
is given by the Hermite function 
 
 ( ) ( ) 22 2211.1 xexxxg −⋅+−⋅= , [ ]5,5−∈x . 
 
The parameter settings for LTS-ANNs and 
KLTSRs are shown in Table 2.  For the given 
corrupted data, Fig. 4 shows the fitted curves using 
ANN, LTS-ANNs, and KLTSRs with 10%, 20%, 
and 30% trimming percentages.  As observed from 
this figure, the LTS-ANNs with 20% and 30% 
trimming percentages and KLTSR with 20% 
trimming percentage perform much better than the 
others.  Moreover, the predictive function 
determined by the KLTSR with 20% trimming 
percentage is almost indistinguishable from the true 
regression function. 
 
The advantage of using simulated data is that we 
can see how close our models come to the truth.  
Certainly, one could argue that the efficacy of 
LTS-ANNs and KLTSRs in Examples 1 and 2 was 
tested only on contrived problems with known true 
regression functions.  This means that the structure 
of the underlying process and the correct values of 
the output were known.  Of course, we usually do 
not have such fortune in most real-world problems.  
In the following, we will provide a real-world 
problem to illustrate the use of LTS-ANN and 
KLTSR.  For better visualization, we choose the 
problem with low input dimension. 
 
Example 3 [20]: Consider the telephone data as 
shown in the following:  
 
 year (x)  no. calls (y) 
1 50       0.44 
2 51      0.47 
3 52      0.47 
… …      … 
22 71      2.40 
23 72      2.70 
24 73      2.90 
 
 The responses (y) for this data set are the 
number of calls (tens of millions) made in Belgium 
for the years 1950 through 1973.  Time in years 
serves as the predictor variable (x). 
 
The parameter settings for LTS-ANNs and 
KLTSRs are shown in Table 3.  For the given 
corrupted data, Fig. 5 shows the fitted curves using 
ANN, LTS-ANNs, and KLTSRs with 10%, 20%, 
and 30% trimming percentages.  As observed from 
this figure, the predictive functions with 30% 
trimming percentage perform best, and are robust 
against outliers.  LTS-ANNs and KLTSRs with 
10% and 20% trimming percentages do not perform 
well because they include too many data points 
which contain false information about the true 
relationship.  In fact, the outliers in this data set 
were recording errors [20]. 
 
From the illustrative examples above, it is clear 
that, when applying the LTS approach to machine 
learning problems, it is important to choose an 
appropriate trimming percentage for a good fit to the 
data at hand.  Unfortunately, there is no systematic 
way to select a proper trimming percentage for a 
general data.  The choice is usually problem 
dependent. 
 
五、結果與討論 
 
 7 
learning algorithm,” Lecture Notes in 
Computer Science, vol. 4507, pp. 102-109, 
2007. 
[20] R. V. Hogg, J. W. McKean, and A. T. Craig, 
Introduction to Mathematical Statistics, 6th ed. 
Englewood Cliffs, NJ: Prentice-Hall, 2005. 
[21] D. C. Montgomery, E. A. Peck, and G. G. 
Vining, Introduction to Linear Regression 
Analysis, 4th ed. Hoboken, NJ: Wiley, 2006. 
 
 
 
z1
zi
zn+1
rj
vij
rm+1
uj
.
.
.
.
.
.
.
.
.
.
.
.
wjk
yk
y1
yp
sk
rmum
r1u1
.
.
.
s1
sp
v w
fh1(u1)
fhj(uj)
fhm(um)
fo1(s1)
fok(sk)
fop(sp)
.
.
.
 
 
Fig. 1.  Artificial neural network. 
 
 9 
-4 -2 0 2 4
0
1
2
3
LTS-ANN: Hermite function
x
True
LS
10%
20%
30%
-4 -2 0 2 4
0
1
2
3
KLTSR: Hermite function
x
True
10%
20%
30%
  
Fig. 4.  Predictive functions in Example 2.  
 
50 55 60 65 70
0
5
10
15
20
LTS-ANN: Telephone data
x
LS
10%
20%
30%
 
50 55 60 65 70
0
5
10
15
20
KLTSR: Telephone data
x
10%
20%
30%
  
Fig. 5.  Predictive functions in Example 3.  
 
 
TABLE 1 
PARAMETER SETTINGS IN EXAMPLE 1. 
 LTS-ANN KLTSR 
Parameters No. of hidden nodes = 4 102 =σ  
Trimming percentage 10%, 20%, 30% 10%, 20%, 30% 
Initialization [ ]1,1−∈ijv  
[ ]4,4−∈jkw  
SVR2 
102 =σ  
2=C  
01.0=ε  
 
無研發成果推廣資料 
