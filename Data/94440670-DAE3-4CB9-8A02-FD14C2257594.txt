可供推廣之研發成果資料表
■ 可申請專利 ■ 可技術移轉 日期：96 年 7 月 31 日
國科會補助計畫
計畫名稱：流行音樂之歌唱語言辨識方法研究
計畫主持人：蔡偉和
計畫編號：NSC 95-2218-E-027 -020
學門領域：自然語言處理與語音處理
技術/創作名稱 流行音樂之歌唱語言辨識方法
發明人/創作人 蔡偉和
中文：本技術提供自動識別音樂內含歌聲之演唱語言的方法，亦即
判斷歌曲所屬語言為何，例如中文歌、英文歌、或日文歌等。由於
流行歌曲大都含有背景音樂伴奏，直接應用現有基於「語音辨認」
之「口說語言辨識」技術於歌唱語言辨識問題將不可行。本技術特
色在於抽取伴奏歌聲之基本發音單元，利用各語言擁有其獨特之基
本發音單元組合的特性來識別語言。其中，我們開發了(1)歌聲/純
伴奏切割技術以針對含有歌聲之音樂區段進行語言辨識、(2)清唱訊
號碼書求取技術以將基本發音單元表示成不需要語音辨認即可獲
得的碼字、以及(3)描述歌唱中各基本發音單元之出現機率與彼此左
右相鄰關係之統計模型與辨識技術。
技術說明
英文：As part of content-based music information retrieval, techniques
are developed to automatically identify the language sung in a music
recording. Since a vast majority of popular music contain background
accompaniment during most or all vocal passages, it is infeasible to
apply the existing methods of spoken language identification based on
speech recognition to the problem of sung language identification.
Recognizing this, we develop techniques for extracting the basic
phonological units from accompanied singing voices, thereby
exploiting the phonotactic characteristics of each language embedding
in the combinations of the basic phonological units to distinguish one
language from another. Specific methods include (1) vocal/non-vocal
segmentation for locating the segments containing voices; (2)
generation of solo voice codebook for deriving the basic phonological
units in an unsupervised manner; and (3) phonotactic modeling for
characterizing both the static and dynamic information of the basic
phonological units within a music recording, thereby identifying the
language to which the recording belong.
可利用之產業及
可開發之產品
網路搜尋引擎、卡拉 OK 點唱機
技術特點
推廣及運用的價值
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
※ 3.本表若不敷使用，請自行影印使用。
附件二
成為一項重要的研究課題。
為了提供快速正確的查詢服務，首要工作是將音樂資料分門別類，以建立相關類別的
索引。然而，目前音樂資料的分類與索引大多仰賴人工方式來進行，例如人工標示歌曲的
曲名、歌手或演奏者、歌詞、曲風等等，此種方式耗時費力，無法因應現今龐大的音樂資
料處理需求。因此，近幾年許多專家學者相繼投入了自動音樂分類技術的研究工作中。較
熱門的研究主題包括音樂主旋律辨認 (main melody recognition) [2]、曲風分類 (genre
classification) [3]、歌手辨識(singer identification) [4]、樂器辨認(instrument recognition) [5]、
與音樂聲紋辨識(audio fingerprinting) [6]等。在本計畫中，我們嘗試探討另一項重要卻仍未
被深入研究的問題歌唱語言辨識。其目標在於自動識別音樂中的歌聲是以何種語言進行
演唱，亦即判斷歌曲所屬語言，例如中文歌、英文歌、或日文歌等。
歌唱語言辨識的最直接用途是協助分類多語言(multilingual)音樂資料，以提供內涵式
音樂資訊檢索(content-based information retrieval)。由於大多數歌曲曲名與其內容無關，例
如許多以英文取名的歌曲並非以英文演唱，利用曲名判斷歌曲語言種類並不可靠。因此，
若能根據音樂訊號自動判斷所屬語言，則可省去人工檢查歌唱內容的負擔。另外，流行音
樂盛行歌曲的翻唱，一首賣座的歌曲經常被翻唱成為各種不同語言的版本。在許多情形下，
使用者可能想聽某一首曲子的英文原版、中文的翻唱版、或是法文的演唱會版等。對於缺
乏後設資料(metadata)的音樂演奏而言，自動歌唱語言辨識是區別不同語言之翻唱曲的必要
工作。
四、文獻探討
目前，有關音樂資訊檢索的研究大多專注於純音樂的內涵資訊抽取問題，例如鋼琴演奏曲
的採譜(music transcription)或鼓聲節奏的分析等，而較少深入探討含有人聲的歌唱內涵資訊
抽取，例如歌手辨識、歌詞辨認[7]、以及本計畫所探討的歌唱語言辨識等。其中歌詞辨認
問題一般認為太過困難，目前相關研究皆僅處理清唱曲的歌詞辨認，而避開實際歌曲大都
含有背景音樂伴奏的複雜情形。另外歌手辨識問題雖較歌詞辨認容易，但在流行歌曲中通
常具有強烈背景音樂的情形下，識別歌手聲音仍是一項深具挑戰的問題。至於歌唱語言辨
識方面，目前仍無相關文獻，與它最直接相關的另一項問題是「口說語言辨識」(spoken
language identification)，亦即辨識說話者所說的語言。口說語言辨識的用途主要是多語言語
音辨認(multilingual speech recognition)與口說語言翻譯(spoken language translation)等之前置
處理，亦即先辨認出語音所屬語言，再進行該語言的語音辨認或翻譯成另一種語言。
口說語言辨識的研究過程已有二十多年的歷史。最早進行該問題的研究[8]並未處理真
實的語音資料，而僅使用語音標示(phonetic transcription)的文字資料進行實驗。直至九 O 年
代中，由於統計方式的語音辨認技術盛行，該問題才又重新被深入地探討。其中較具代表
性的著作為 Zissman 之「Comparison of four approaches to automatic language identification of
telephone speech」[9]，文章中探討四種口說語言辨識方法，這些方法目前仍是最為普遍常
用的方法。另外，[10]則探討結合韻律與音段各項特徵於單一語言辨識系統中。
五、研究方法
一般，當我們聽到自己所熟悉的語言時，很快就能精確地知道其中的隻字片語；而聽到自
己不懂的語言時，雖然無法瞭解其中的意義，但經常仍可做出類似「這應該是日語」的判
斷。由此可知，各種語言必有其獨特的發音特性，這些發音特性往往不須要逐字分析音意
便能察覺。根據語言學(linguistics)層次上的分析，人們判斷口說語言時大致依照下列的線
索：
純伴奏
含歌聲





  





 )|(log)|(log
1 1
0
1
0
W
i
NitW
W
i
VitW ppW
xx ， (1)
其中 W 表示加總的音框數目，表示一決策門檻值。
另外，考慮到模型訓練時可能會因為所選的音樂資料差異而有所偏頗，例如資料中欠
缺與待測音樂相似的曲風等，因此我們擬更進一步應用語音辨認研究領域中的模型調適技
術(model adaptation)來提升模型的可靠度。基本概念是先利用原本的「純伴奏模型」N 與
「含歌聲模型」V 進行似然率計算，根據似然率判斷每一音框的類別，並找出判斷結果較
為可靠(遠高於決策門檻值)的音框來。接著利用這些音框來調整模型N 與V，其中調整方
式為 MAP adaptation [12]。當模型N 與V 被調整為N 與V 後，同樣的似然度計算以及音
框類別判斷將再進行一次，所獲得之判斷結果較為可靠的音框也將再次被用來調整模型N
與V。如此重覆進行多次直到最後音框判斷結果不再改變為止。
高斯混合模型訓練
最大似然率決策待測音樂訊號
人工標記「純伴奏」與「含歌聲」區段之音樂資料
「純伴奏模型」 、「含歌聲模型」
訓練階段
模型調適
調適後模型
VN
NV
NV


與取代
與以 V N
測試階段
「純伴奏」與「含歌聲」區段判斷結果
圖一、歌聲/純伴奏分類架構。
B. 清唱訊號碼字求取
當識別出音樂資料中含有歌聲與不含歌聲的區段後，接下來的工作是如何從含有歌聲的音
樂區段中產生一組代表清唱訊號的碼字。如圖二所示為一段流行音樂的訊號波形，其中「伴
奏歌聲訊號」代表直接從音樂資料中截取出的一段訊號，它是由「清唱訊號」與「純伴奏
訊號」所相疊加而成。我們可以看到圖二中含有歌聲之區段內的背景音樂(圓形虛線內)與純
伴奏區段(方形虛線內)的波形特徵相似。因此可推測，清唱訊號特性應可由「伴奏歌聲訊號」
中扣除「純伴奏區段」的特性而求得。然而，由於實際伴奏區段的訊號並不等於歌聲區段
內的背景音樂，直接由伴奏歌聲訊號中扣除純伴奏區段訊號並無法得到清唱訊號。針對此
問題，我們利用統計特性的疊加關係來取代直覺的訊號疊加關係。
令 X = {x1, x2, ..., xT}、S = {s1, s2, ..., sT}與 B = {b1, b2, ..., bT}分別表示一段伴奏歌聲訊
號、清唱訊號與背景音樂訊號的頻譜特徵參數，其中僅有 X 可直接由音樂資料中求取，S
與 B 均無法直接取得。我們的目標是將 S 歸納成一組碼書(codebook)，Cs = {cs,1, cs,2,…,
cs,Ks}，以表示基本歌唱發音單元(共 Ks 個碼字)。雖然 B 無法直接由音樂資料中求取，但其
特性可假設近似於純伴奏區段訊號。因此，我們可以將純伴奏區段的頻譜特徵參數進行向
我們利用期望最大化演算法來解方程式(4)。其步驟是先產生一組初始碼字 Cs，然後反
覆地估計另一組新碼字 sCˆ 使得 p(X| bs CC ,ˆ ) p(X|Cs,Cb)。此一估計法相當於求取下列函數
的最大值：

  

T
t
K
i
K
j
jbistss
s b
pjjiiQ
1 1 1
,,
** ),|(log),()ˆ( ccxCC ， (5)
其中()表示一克羅內克脈衝函數(Kronecker delta function)，另外 i*與 j*的求取是根據
),|(maxarg),( ,,
,
**
jbist
ji
pji ccx 。 (6)
令Q )ˆ( ss CC  = 0，我們可獲得如下的參數估計公式：
 


 
 


 T
t
N
j
jbist
T
t
N
j
jbisttjbist
is
pjjii
Epjjii
1 1
,,
**
1 1
,,,,
**
,
),|(),(
,,|),|(),(
ˆ
ccx
ccxsccx


 ， (7)
 
isisT
t
J
j
jbist
T
t
J
j
jbistttjbist
is
pjjii
Epjjii
,,
1 1
,,
**
1 1
,,,,
**
,
),|(),(
,,|),|(),(
ˆ  





 
 
ccx
ccxssccx


Σ ， (8)
其中條件期望值的計算方式為
 
),,,|(
),;(),;(
,,|
,,,,
),(
,,,,
,,
jbjbisist
f
jbjbisis
jbistt p
dd
ccE ttt
ΣΣ
ΣΣ


x
bsbss
xs bsv


NN
， (9)
 
),,,|(
),;(),;(
,,|
,,,,
),(
,,,,
,,
jbjbisist
f
jbjbisis
jbisttt p
dd
ccE ttt
ΣΣ
ΣΣ


x
bsbsss
xss bsv




NN
。 (10)
D. 歌唱語言辨識
當獲得清唱訊號碼書之後，任何音樂訊號都將經由向量量化的方式轉換成一連串的向量碼
字索引，每一種碼字對應某一基本發音單元。由於碼字所代表的訊號長度僅是極短暫的一
個音框，所以一般歌唱或說話時的基本發音單元應包含若干個相同的碼字。然而，實際上
從訊號求取的向量碼字索引序列可能出現某一碼字與其左右相鄰瑪字都不相同的情形。為
改善此種不合理的碼字序列，我們以一固定長度之滑動視窗平滑法(smoothing)來修正碼字
序列；簡而言之，在一視窗內，所有碼字都以視窗內出現次數最多的碼字來取代。最後，
如圖四所示，相鄰同碼字串構成一基本發音單元。
接著，我們統計這些基本發音單元在句中出現的頻率與彼此相連接關係，藉以捕捉語
言特徵。本計畫中採用雙連文模型(bigram model)來描述基本發音單元的相繼關係，並根據
上述兩項技術發展一如圖五之歌唱語言辨識系統。對不同語言分別產生各自的清唱訊號碼
書。，若欲辨識的語言種類為 L，則系統分別建立 L 個多連文模型(1),(2), …, (L)。
當收到一待測音樂資料時，系統先將其切割為純伴奏區段與含歌聲區段。純伴奏區段
經由向量分群而得到一模擬背景音樂的碼書。接著利用式(6)將含有歌聲區段的倒頻譜進行
DB-2 與 DB-3 則用來評估系統效能。DB-1 中包括 60 首中文歌、60 首英文歌、與 60 首日
文歌，分別記為 DB-1-M、DB-1-E、與 DB-1-J。而 DB-2 中包括 20 首中文歌、20 首英文歌、
與 20 首日文歌，分別記為 DB-2-M、DB-2-E、與 DB-2-J。為了測試的客觀性，DB-1 之內
含歌手與 DB-2 之內含歌手完全不同。
另一方面，DB-3 包含 53 對的「翻唱歌曲」，每一對為兩首旋律近乎相同但歌唱語言不
同的歌曲。這 53 對歌曲中有 32 對為「中英文翻唱」，另外 21 對為「中日文翻唱」，分別記
為 DB-3-EM 與 DB-3-JM。並且，53 對歌曲中有 18 對是由具雙語能力(bilingual)歌手所演唱，
即同一歌手演唱兩首相似旋律但不同語言的歌曲。同時，為了測試的客觀性，DB-3 之內含
歌手與 DB-1 及 DB-2 之內含歌手完全不同。我們設計 DB-3 的目的是為了避免因測試資料
型態上的偏頗而影響系統效能評估的可靠性。例如，由於中英文歌的演奏方式普遍有所差
異，因此系統可能很容易區別某些英文歌曲與中文歌曲，這種區別的能力往往是根據演奏
方式上的特徵，而非歌唱語言上的特徵。因此，使用兩首演奏方式相同但歌唱語言不同的
歌曲來測試系統應可獲得較客觀的效能評估。
Table 1. Music database
Subset
Vocal/Non-vocal
Labeling Purpose
DB-1-E 60 Mandarin Songs Yes
DB-1-M 60 English Songs YesDB-1
DB-1-J 60 Japanese Songs No
Training
DB-2-E 20 Mandarin Songs Yes
DB-2-M 20 English Songs YesDB-2
DB-2-J 20 Japanese Songs No
Evaluation
DB-3-EM 32 Pairs of English-Mandarin Cover Songs Yes
DB-3
DB-3-JM 21 Pairs of Japanese-Mandarin Cover Songs No
Evaluation
由於本項研究仍屬初步發展階段，我們只以人工方式標示部分歌曲中的歌聲/純伴奏切
換邊界，如表一所註明之資料。另外，我們將歌曲訊號由 44.1 kHz 的 CD 品質取樣率降為
22.05 kHz 以剔除歌聲以外的高頻成份。之後，以每 32-ms 的 Hamming-window frame 求取
20 Mel-scale frequency cepstral coefficients，frame 之間重疊 10-ms.
首先，實驗是從使用具有人工標示歌聲/純伴奏切換邊界之資料開始進行，藉以了解因自
動切割歌聲/純伴奏時難免的誤判造成對歌曲辨識的影響。我們以 DB-1-E 與 DB-1-M 來訓
練歌聲/純伴奏模型，並以 DB-2-E、DB-2-M、與 DB-3-EM 評估自動歌聲/純伴奏之切割準
確度。此準確度的量測方式為正確被判斷的音框數除以總測試音框數。但考慮人耳判斷切
換邊界的誤差，凡於切換邊界 0.5 秒內的錯誤不列入考慮。我們所獲得的最佳歌聲/純伴奏
切割準確度為 79.2%，其中 GMM 所包含的混合數為 64，而式(1)中的 W 值設為 60 frames。
接著我們使用 DB-2-E、DB-2-M、與 DB-3-EM 測試語言辨識系統。為了解系統對不同
長度歌曲的辨識能力，我們將每首歌切成若干長度為 T feature vectors 之片段，每一片段
被視為單獨的測試單位。在訓練階段，清唱訊號碼字數目設為 32，而背景音樂碼字數目設
為 16。在測試階段，若歌曲片段之長度超過 200 個音框，則背景音樂碼字數目設為 4；若
[8] A. S. House and E. P. Neuburg, “Toward automatic identification of the language of an 
utterance. I. Preliminary methodological considerations,” Journal of the Acoustical Society of
America, 62(3): 708-713, 1977.
[9] M. A. Zissman, “Comparison of four approaches to automatic language identification of 
telephone speech,” IEEE Transactions on Speech and Audio Processing, 4(1): 31-44, 1995.
[10] T. J. Hazen and V. W. Zue, ‘Segment-based automatic language identification,’ Journal of
the Acoustical Society of America, Vol. 101, pp. 2323–2331, 1997.
[11] A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from incomplete data via the
EM algorithm,” J. R. Statist. Soc., vol. 39, pp.1–38, 1977.
[12] C. Lee, C. Lin, B.Juang, “A study on speaker adaptation of parameters of continuous density 
hidden markov models,” IEEE Trans. Signal Process., 1991, 39(4): 806–814.
3000 6000 9000
Test Recording Length (# frames)
30.0
35.0
40.0
45.0
50.0
55.0
60.0
65.0
70.0
75.0
80.0
85.0
90.0
95.0
S
un
g-
LI
D
A
cc
ur
ac
y
(%
)
Using Solo Voice Codebooks; Manual Seg.
Using Solo Voice Codebooks; Automatic Seg.
Without Background Music Codebooks; Manual Seg.
Without Background Music Codebooks; Automatic Seg.
Entire
(a) 測試 DB-2-E 與 DB-2-M 之歌曲
3000 6000 9000
Test Recording Length (# frames)
30.0
35.0
40.0
45.0
50.0
55.0
60.0
65.0
70.0
75.0
80.0
S
un
g-
LI
D
A
cc
ur
ac
y
(%
)
Using Solo Voice Codebooks; Manual Seg.
Using Solo Voice Codebooks; Automatic Seg.
Without Background Music Codebooks; Manual Seg.
Without Background Music Codebooks; Automatic Seg.
Entire
(b) 測試 DB-3-EM 之歌曲
圖六、歌唱語言辨識結果。
表二、辨識中文、英文、與日文三種歌曲之混淆矩陣
(a) 測試 DB-2 之歌曲
辨識結果
實際語言 中文 英文 日文
中文 0.75 0.10 0.15
英文 0.20 0.65 0.15
日文 0.30 0.15 0.55
(b) 測試 DB-3 之歌曲
辨識結果
實際語言 中文 英文 日文
中文 0.65 0.14 0.21
英文 0.29 0.57 0.14
日文 0.29 0.21 0.50
技術以提供使用者快速而有效率地進行檢索有其必要性。本篇論文探討多媒體資訊檢索中的
一項重要課題 口語資料的索引。其中我們針對口語資料之內含語者進行無督導式的分類，
目標是希望能將一群未知的口語資料自動分成若干僅含單一語者之語音群。目前，語者分群
研究最普遍使用的方法是階層式分群法(hierarchical clustering; HC)。其原理是先計算兩語音區
段間的聲學特徵相似度，再逐步合併相似度高的語音區段(agglomerative clustering)，或逐步分
離相似度低的語音區段(divisive clustering)，然後建立一「群樹」(cluster tree)，其包含各種不
同群數的分群結果。另外為求得群數等於人數的最佳分群結果，許多相關研究利用BBN Metric
[5]或貝氏資訊準則(Bayesian information criterion; BIC)等量測值來決定如何切割群樹，使得最
後留下的分群結果為最佳。然而，階層式分群法雖然非常普遍實用，但其分群效果明顯並非
最佳。首先，我們可發現，階層式分群法雖致力於使任何新產生群的內含語者相同性
(homogeneity)達最大，但卻無法保證所有群的內含語者相同性加總達到最大，其原因是該方
法並未考慮新產生群與原存在群之間的關係。因此，當某次合併過程或分離過程中，若發生
某些不同語者的語音區段被誤置於同一群或某些相同語者的語音區段被誤置於不同群時，這
種錯誤將在之後的合併或分離過程中持續地蔓延，造成整體系統效能不佳。另一方面，階層
式分群法一般仰賴 BBN Metric或 BIC來決定最佳群數，但由於前級的群樹產生過程與後級的
群數決定過程兩者幾乎為獨立運作，且後級完全信任前級的結果，因此前級所產生的錯誤將
累加至後級，造成群數決定的偏差。為了克服上述方法的限制，本研究提出一種利用 Rand
Index量測來同時最佳化群內語句以及群數目之分群方法，使得最後分群結果達到群內語者單
一化的目標。本論文報告時間為 4月 19日下午 3點 30分至 5點 30分，以 lecture方式發表。
二、與會心得
ICASSP主要議程為學術論文發表，因此歷屆會議無不匯集眾多相關領域之世界知名學者
專家在此提出最新研究成果及互相討論、交換研究心得。參與此一研討會有助於從事訊號處
理研究者掌握正確方向。
上屆會議已經感受到現場參展的廠商數量大幅減少，本屆的情況似乎仍未好轉。已往會
SPEAKER CLUSTERING BASED ON MINIMUM RAND INDEX 
 
Wei-Ho Tsai1 and Hsin-Min Wang2 
 
1Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan  
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
1whtsai@en.ntut.edu.tw, 2whm@iis.sinica.edu.tw 
 
ABSTRACT 
 
This paper presents an effective method for clustering unknown 
speech utterances based on their associated speakers. The proposed 
method jointly optimizes the generated clusters and the number of 
clusters by estimating and minimizing the Rand index of the 
clustering. The Rand index, which reflects clustering errors that 
utterances from the same speaker are placed in different clusters, 
or utterances from different speakers are placed in the same cluster, 
reaches its minimal value only when the number of clusters is 
equal to the true speaker population size. We approximate the 
Rand index by a function of the similarity measures between 
utterances and employ the genetic algorithm to determine the 
cluster where each utterance should be located, such that the 
overall clustering errors are minimized. The experimental results 
show that the proposed speaker-clustering method outperforms the 
conventional method based on hierarchical agglomerative 
clustering in conjunction with the Bayesian information criterion 
to determine the number of clusters. 
 
Index Terms—Clustering methods, Speech processing, 
Speaker recognition  
 
1. INTRODUCTION 
 
With the burgeoning availability of digital audio material, speaker 
clustering is gaining importance as a means of indexing the 
voluminous spoken data accumulated daily for archival use [1-14]. 
Given N speech utterances produced by P speakers, the goal of 
speaker clustering is to partition N utterances into M clusters, such 
that M = P and each cluster consists exclusively of utterances from 
only one speaker. Since no prior information regarding the 
speakers involved and the speaker population size is available in 
most practical applications, solving the speaker-clustering problem 
usually involves characterizing the voice similarities between 
utterances, generating clusters based on those similarities, and 
determining the optimal number of clusters. 
Currently, the most popular method of speaker clustering 
generates a cluster tree by sequentially merging the utterances 
deemed similar to each other, and then cuts the tree via a Bayesian 
information criterion (BIC) [5,8,10-12,15], in order to retain an 
appropriate number of clusters. During the agglomeration 
procedure, the nearest neighborhood selection rule is usually 
employed in an attempt to maximize the similarities between all 
the utterances within each cluster. Since the interaction between 
clusters is not considered, this method can only make each 
individual cluster as homogeneous as possible; however it cannot 
guarantee that the homogeneity for all the clusters can finally be 
summed to reach a maximum. In particular, mis-clustering errors 
arising from grouping different-speaker utterances together can 
propagate down the whole process, and hence limit the clustering 
performance. In addition, the cluster tree is generated separately 
from the determination of the optimal number of clusters. Since the 
latter trusts the former completely, the inevitable errors from the 
former can propagate to the latter, which may lead to a poor 
estimation of the speaker population size. 
To overcome the above-mentioned limitations of the 
conventional method, we propose a new clustering method that 
jointly optimizes the generated clusters and the number of clusters 
by estimating and minimizing a metric called the Rand index 
[16,17]. This metric indicates the clustering errors that place 
utterances from the same speaker in different clusters, or place 
utterances from different speakers in the same cluster. We 
approximate the Rand index by a function of the similarity 
measures between utterances, and employ the genetic algorithm 
[18] to determine the cluster where each utterance should be 
located. The resulting clusters are thus optimized in a global 
fashion, rather than a pair-by-pair manner used in the conventional 
method. In addition, by exploiting a characteristic of the Rand 
index that it only reaches the minimal value when the number of 
clusters equals the true speaker population size, speaker clustering 
based on the minimization of the estimated Rand index also 
enables the resulting number of clusters to approach the optimum. 
 
2. PROBLEM FORMULATION 
 
For convenience of discussion, we begin by defining the following 
symbols. 
X1, X2,…, XN : N speech utterances to be clustered; 
s1, s2,…, sP : P unknown speakers involved in N utterances; 
c1, c2,…, cM : M clusters to be generated; 
on : index of the speaker producing utterance Xn; 
hn : index of the cluster that utterance Xn is assigned to; 
nm∗ : number of utterances in cm; 
n∗p : number of utterances spoken by sp; 
nmp : number of utterances in cm spoken by sp. 
The goal of speaker clustering is to produce a set of indices H = 
{h1, h2, …, hN} that satisfy  hi = hj for any Xi and Xj from the same 
speaker, and hi ≠ hj for any Xi and Xj from different speakers. 
Depending on the application, there are a number of ways to 
evaluate the performance of speaker clustering. This study uses 
two metrics: cluster purity [4] and the Rand index [4,16,17]. 
Cluster purity represents the probability that if we pick any 
utterance from a cluster twice at random, with replacement, both of 
and 
,),(),(2),()(ˆ
1 1
)()(
1 1
)()()( ∑∑∑∑
= == =
−Ω+=
N
i
N
j
ji
M
j
M
i
N
i
N
j
M
j
M
i
M oohhhhR δδδH  
(13) 
where δ(⋅) in Eqs. (10)−(13) is a Kronecker Delta function.  
However, as the computation of δ(oi, oj) requires that the true 
speaker of each utterance be known in advance, it is impossible to 
find H∗ directly from Eqs. (12) and (13). To solve this problem, we 
propose estimating δ(oi, oj) by means of the similarity measure 
between Xi and Xj. Specifically, 
, 
0 and , if  ,),(
0 and , if  ,),(
          if  ,             1
),(
maxmax
maxmax⎪⎩
⎪⎨
⎧
<≠
>≠
=
←
SjiSS
SjiSS
ji
oo
ji
jiji
XX
XXδ    (14) 
where S(Xi, Xj) denotes a certain similarity measure between Xi 
and Xj that could be either positive or negative, but cannot be zero, 
and Smax is the maximum among the similarities S(Xi, Xj), ∀ i ≠ j. 
In our implementation, S(Xi, Xj) is computed by the generalized 
likelihood ratio (GLR) [1,4]: 
S(Xi, Xj) = logPr(Xij |λij) − logPr(Xi |λi) − logPr(Xj |λj),      (15) 
where  Xij is the concatenation of Xi and Xj, and λi, λj, and λij are 
parametric models trained using Xi,  Xj, and Xij, respectively. 
Using this estimation, we can solve Eq. (12) by further assigning 
to Ω an arbitrary positive constant that ensures . 0)(ˆ )( ≥MR H
Given that neither a gradient-based optimization method nor an 
exhaustive search is applicable in this scenario, we propose using 
the genetic algorithm (GA) [18] to find H∗ by virtue of its global 
scope and parallel searching power. The basic operation of the GA 
is to explore a given search space in parallel by means of iterative 
modifications of a population of chromosomes. Each chromosome, 
encoded as a string of alphabets or real numbers called genes, 
represents a potential solution to a given problem. In our task, a 
chromosome is exactly a legitimate H(M), and a gene corresponds 
to a cluster index associated with an utterance. However, since the 
index of one cluster can be interchanged with that of another 
cluster, multiple chromosomes may amount to an identical 
clustering result. For example, the chromosomes {1 1 1 2 2 3 3}, 
{1 1 1 3 3 2 2}, {2 2 2 1 1 3 3}, and {1 1 1 5 5 4 4} represent the 
same clustering result derived by grouping seven utterances into 
three clusters. Such a non-unique representation of the solution 
would significantly increase the GA search space, and may lead to 
an inferior clustering result. To avoid this problem, we limit the 
inventory of chromosomes to conform to a baseform 
representation defined as follows.  
Let I (cm) be the lowest index of the utterance in cluster cm. 
Then, a chromosome is a baseform  
iff ∀ cm, cl ≠ {φ}, if m < l, then I (cm) < I (cl),   (16) 
where {φ} indicates that a cluster does not contain any utterance. 
Among the above chromosomes, {1 1 1 2 2 3 3} is a baseform, 
since the lowest index of the utterance in clusters c1, c2, and c3 is 1, 
4, and 6, respectively, which satisfies Eq. (16). In contrast, 
chromosomes {1 1 1 3 3 2 2} and {2 2 2 1 1 3 3} are not 
baseforms, since the lowest index of the utterance in clusters c1, c2, 
and c3 does not satisfy Eq. (16). In addition, chromosome {1 1 1 5 
5 4 4} implies that clusters c2 and c3 do not contains any utterance; 
hence it is not a baseform, either. However, it is conceivable that 
all the non-baseform chromosomes can be converted into a unique 
baseform representation by re-arranging the cluster indices. 
GA optimization starts with a random generation of 
chromosomes according to a certain population size, Z. Then, the 
fitness of all chromosomes is evaluated via the inverse of the 
estimated Rand index, i.e., )(ˆ1)( )()( MM RF HH = . Based on this 
evaluation, a particular group of chromosomes is selected from the 
population to generate offspring by subsequent recombination. To 
prevent premature convergence of the population, the selection is 
performed with the linear ranking scheme described in [19]. Next, 
crossover among the selected chromosomes proceeds by 
exchanging the substrings of two chromosomes between two 
randomly selected crossover points. A crossover probability is 
assigned to control the number of offspring produced in each 
generation. After crossover, a mutation operator is used to 
introduce random variations into the genetic structure of the 
chromosomes. This is done by generating a random number and 
then replacing one gene of an existing chromosome with a 
mutation probability. The resulting chromosomes that do not 
conform to the baseform representations are converted into their 
baseform counterparts.  
The procedure of fitness evaluation, selection, crossover, and 
mutation is repeated continuously, in the hope that the overall 
fitness of the population will increase from generation to 
generation. When the maximum number of generations is reached, 
the best chromosome in the final population is taken as the 
solution, H*. Note that the estimated speaker population size can 
be obtained by selecting the maximal value of the cluster index in 
H*. For example, if H* = {1 2 1 3 4 3 1}, the estimated number of 
speakers in a seven-utterance collection is 4. 
 
4. EXPERIMENTAL RESULTS 
 
The speech data used in this study consisted of six excerpts of 
broadcasts from the evaluation set of the 2002 Rich Transcription 
Broadcast News and Conversational Telephone Speech Corpus 
[20]. Each excerpt was segmented into speaker-homogeneous 
utterances, according to the annotation files in the corpus. Speaker 
clustering was then applied to each excerpt separately. Prior to the 
experiments, every speech utterance was converted from its digital 
waveform representation into a sequence of feature vectors, each 
of which consisted of 12 Mel-scale frequency cepstral coefficients 
(MFCCs) and 12 delta MFCCs. Then, the similarities between the 
utterances were computed using Eq. (15), in which all the 
parametric models are of a uni-Gaussian model with a full 
covariance matrix. 
In GA optimization, the parameter values used for the 
maximum number of generations, the population size, the 
crossover probability, and the mutation probability were 
empirically determined to be 2000, 5000, 0.5, and 0.1, 
respectively. For the performance comparison, we also 
implemented a baseline speaker-clustering system based on 
hierarchical agglomerative clustering (HAC) in conjunction with 
the Bayesian information criterion (BIC) to determine the optimal 
number of clusters [5]. In the agglomeration procedure, the 
similarities between clusters were computed using the complete 
linkage of the GLR-based inter-utterance similarities. In addition, 
in using the BIC, the penalty weight was set to one. 
Table 1 shows the speaker-clustering results. First, we 
evaluated the performance of the proposed minimum Rand index 
clustering (MRIC) by specifying the number of clusters a priori as 
the true number of speakers. This served as an upper bound of the 
performance that could be achieved by the automatic 
