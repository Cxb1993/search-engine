         
Our approach named NOMMD (Non-Ordered Multi-
valued and Multi-labeled Discretization algorithm). 
NOMMD splits v-valued and l-labeled records into v 
single-valued ones and modifies the counts of class labels 
to maintain the original data distribution. Then, a splitting 
metric which is based on contingency coefficient and the 
simulated annealing search approach are used to generate 
discretization schemes. The idea of NOMMD is simple 
but novel. In our knowledge, we are the first one to 
address the problem of discretizing multi-valued and 
multi-labeled data. Note that, the problem we address in 
this paper is different from the multivariate discretization 
[3][4][11][13], which considers the interdependent 
relationship between attributes.  
The rest of this paper is organized as follows. Section 
II reviews some famous and proposed discretization 
approaches. In Section III, we give the formal definitions 
of a multi-valued and multi-labeled dataset and then 
introduce our NOMMD. The experimental analysis is 
presented in Section IV. Finally, Section V concludes this 
paper. 
II. PREVIOUS DISCRETIZATION APPROACHES 
Many good discretization algorithms have been 
proposed in the past. Reference [20] stated that 
discretization approaches have been proposed along five 
lines: supervised versus unsupervised, static versus 
dynamic, global versus local, splitting versus merging, 
and direct versus incremental. Below we give a brief 
introduction of the five lines. Supervised methods 
discretize attributes with the consideration of class 
information, while unsupervised methods do not. 
Dynamic methods [27] discretize continuous attributes 
when a classifier is being built while in static methods 
discretization is completed prior to the learning task. 
Global methods, which use total records to generate the 
discretization scheme, are usually associated with static 
methods. On the contrary, local methods are usually 
associated with dynamic approaches in which only parts 
of records are used for discretization. Merging methods 
start with the complete list of all continuous values of the 
attribute as cut-points and remove some of them by 
merging intervals in each step. Splitting methods start 
with an empty list of cut-points and add new ones in each 
step. The computational complexity of merging methods 
is usually larger than splitting ones [24]. Direct methods 
require users to decide on the number of intervals k and 
then discretize the continuous attributes into k intervals 
simultaneously. Instead incremental methods begin with 
a simple discretization scheme and pass through a 
refinement process although some of them may require a 
stopping criterion to terminate the discretization. Take 
two simplest discretization algorithms Equal Width and 
Equal Frequency as the examples, both of them are 
unsupervised, static, global, splitting and direct method. 
In the follows, we review some typical discretization 
algorithms by following the line of splitting versus 
merging. 
A. Splitting-based Discretization Methods 
Famous splitting methods include Equal-Width and 
Equal-Frequency [7], maximum entropy [26], CADD 
(Class-Attribute Dependent Discretizer algorithm) [6], 
IEM (Information Entropy Maximization) [12], CAIM 
(Class-attribute Interdependence Maximization) [18], 
FCAIM (Fast Class-attribute Interdependence 
Maximization) [17], and CACC (Class-Attribute 
Contingency Coefficient algorithm) [24]. Experiments 
in [24] showed that CACC discretization algorithm is 
superior to the other splitting discretization algorithms. 
B. Merging-based Discretization Methods 
A common characteristic of the merging methods is in 
the use of the significant test to check if two adjacent 
intervals should be merged. The computational 
complexity of merging methods is usually worse than 
the splitting ones. For example, the time complexity for 
discretizing a single attribute in the state-of-the-art 
merging method Extended Chi2 is O(kmlogm) while that 
of the newest top-down methods CAIM, FCAIM and 
CACC is O(mlogm), where m is the number of distinct 
values of the discretized attribute and k is the number of 
incremental steps. This condition will get worse when 
the difference between the number of values in a 
continuous attribute and the number of produced 
intervals is large.  
ChiMerge [16] is the most typical merging algorithm. 
In addition to the problem of high computational 
complexity, the other main drawback of ChiMerge is that 
users have to provide several parameters during the 
application of this algorithm that include the significance 
level as well as the maximal and minimal intervals. Hence, 
Chi2 [19] improved ChiMerge by automatically 
calculating the value of the significance level. However, 
Chi2 still requires the users to provide an inconsistency 
rate to stop the merging procedure and does not consider 
the freedom which would have an important impact on 
discretization schemes. Thereafter, Modified Chi2 [23] 
takes the freedom into account and replaces the 
inconsistency checking in Chi2 by the quality of 
approximation after each step of discretization. Such a 
mechanism makes Modified Chi2 a completely automated 
method to attain a better predictive accuracy than Chi2. 
After Modified Chi2, Extended Chi2 [22] considers that 
the labels of instances often overlap in the real world. 
Extended Chi2 determines the predefined 
misclassification rate from the data itself and considers the 
effect of variance in two adjacent intervals. With these 
modifications, Extended Chi2 can handle an uncertain 
dataset. Experiments on these merging approaches by 
using C5.0 show that the Extended Chi2 outperformed the 
other bottom-up discretization algorithms since its 
discretization scheme, on the average, can reach the 
highest accuracy [22]. 
III. THE PROBLEMS AND OUR SOLUTION 
Although Reference [20] provided a good overview of 
previous discretization algorithms; we believe two 
dimensions should be added to make the discretization 
literature more complete. The sixth dimension is 
univariate versus multivariate [3][4][11][13]. Most 
previous discretization algorithms are univariate in that 
they consider each attribute independently and do not 
consider interactions with other features. Bay [3] claims 
that discretization schemes should semantically 
meaningful to human expert. He also uses the XOR data 
         
handle non-ordered multi-valued and multi-labeled 
datasets. The discretization metric used in NOMMD is  
  C’ = 2/1
1 1
2
)]}log(/)1)[((1{ I
NN
nS
i
I
r ri
ir −+ ∑∑
= = ++
,  (1) 
which is proposed in our previous work [24]. This metric 
is inspired by statistical contingency coefficient and 
empirical analyses in [24] shows that it is a good 
discretization metric. Finally, instead of adopting a 
simple greedy search method as that used in CAIM and 
CACC, NOMMD uses simulated annealing approach to 
reduce the probability of sticking on local maxima. 
Below is the pseudocode of NOMMD. Given a non-
ordered multi-valued and multi-labeled dataset D, 
NOMMOD splits v-valued and l-labeled records into v 
single-valued and l-labeled records and maintains the 
original data distribution in Line 1 to Line 3. Line 5 to 
Line 12 is responsible for some initial setups for each 
continuous attribute. Then NOMMD iteratively partition 
a continuous attribute from Line 13 to Line 31. Line 13 to 
Line 25 means that the discretization process goes to next 
loop if a cut-point reaches better C’ value in Equation 1. 
Please note that in Line 18, the condition I < L is inspired 
by CAIM that a reasonable discretization scheme should 
contain intervals whose number is more than or equal to 
the number of class labels. Finally, NOMMD uses 
simulated annealing search in Line 27 to Line 31 to 
decide if terminating the discretization. 
 
Algorithm NOMMD 
D: a non-ordered multi-valued and multi-labeled dataset; 
i: the number of continuous attributes in D; 
L: the number of class labels in D; 
I : the number of intervals ; 
Cut[]: the set of possible cut-points which are the 
midpoints of adjacent values belonging to different class 
labels in a continuous attribute; 
NOMMDS: a non-ordered multi-valued and multi-labeled 
discretization scheme  
 
NOMMD (D) 
For each v-valued and l-labeled record  
Split this record into v single-valued and l-labeled 
records; 
Set the count of each label belonging to this record as 
1/v; 
For each continuous attribute Ai 
Sort Ai;  
Get the minimum value v0 and the maximum value vn;  
Get Cut[]; 
NOMMDS = {[v0,vn]}; 
C’ = 0; /* the C’ of initial discret-ization scheme is 0 
I = 1; 
MMDStemp = {φ}; 
Itemp = 0 
For each possible cut-point in Cut[]  
Add it into NOMMDS; 
Calculate the corresponding C’ in Equation 1; 
Get the cut-point cut whose C’ value is the 
maximum maxC’; 
ΔC’ = maxC’ - C’ 
If ΔC’  > 0 or I < L  then  
Remove cut from Cut[]; 
Add cut and the cut-point(s) in NOMMDStemp 
into NOMMDS;  
C’ = maxC’; 
I = I + Itemp + 1 ; 
NOMMDStemp = {φ}; 
Itemp = 0; 
Goto Line 13;    
Else 
Remove cut from Cut[]; 
Add cut into NOMMDStemp; 
Itemp = Itemp + 1 
Goto Line 13 with probability eΔC; 
End If 
Output NOMMDS for continuous attribute Ai; 
 
IV. PRELIMINARY EXPERIMENTAL ANALYSIS 
In this section, we evaluate the performance of 
NOMMD. MMDT is the typical algorithm to classify 
non-ordered the multi-valued and multi-labeled data. 
Therefore, we implement MMDT and our approach in 
Microsoft Visual C++ for performance analysis. We 
evaluate if the discretization datasets generated by 
NOMMD can improve the performance of MMDT. The 
improvement is measured in two aspects through 
before/after discretization comparison: 1) accuracy, 2) the 
number of rules (i.e. the learning time of MMDT).  
MMDT propose two measuring strategies appearance 
basis and similarity basis to measure of the goodness of a 
splitting attribute. As reported in MMDT, appearance 
basis strategy on average reaches better accuracy and 
produces fewer rules. In our experiment, we use MMDT 
with appearance basis strategy as the baseline. It is 
worthy to note that as claimed in MMDT, a prediction 
has accuracy 0 or 1 in traditional classification problems. 
However, for multi-labeled classification problems, it is 
not fair to assign accuracy 1 or 0 to a prediction if this 
prediction is similar but not totally the same or different 
to the true answer. Therefore, we follow MMDT to use 
the similarity function to calculate the accuracy. The 
similarity function is similarity(Li,Lj) = {[same(Li,Lj) / 
card(Li,Lj)] – [diff(Li,Lj) / card(Li,Lj)] + 1}/2, where Li 
and Lj denotes two two label-sets, same(Li,Lj) is the 
number of labels that appear in both Li and Lj, diff(Li,Lj) 
denotes the number of labels that appear either in Li or Lj 
but not both, and card(Li,Lj) is the number of different 
labels that appear in Li or Lj. 
Due to the lack of a benchmark containing multi-
valued and multi-labeled dataset, the authors of MMDT 
develop a synthetic data generator to generate 
experimental datasets. This data generator is a modified 
version of IBM data generator [2], which has been widely 
used to evaluate the performance of proposed algorithms 
in the literature of machine learning. In our experiments, 
the same synthetic data generator developed in MMDT is 
used. The synthetic data consists of nine attributes. The 
class labels of records are assigned by five functions 
defined in MMDT. If a record meets any of these five 
functions, this record will be assigned with the 
corresponding class labels. In other words, the possible 
number of class labels of a record is from one to five. The 
         
AAAI Press.  
[8] Cios, K. J., and Kurgan, L. A. (2004). CLIP4: 
hybrid inductive machine learning algorithm that 
generates inequality rules. Information Science, 
163(1-3), 37-83. 
[9] Clark, P., and Niblett, T. (1989). The CN2 
algorithm, Machine Learning, 3(4), 261-283. 
[10] Chou, S., and Hsu, C. L. (2005). MMDT: a multi-
valued and multi-labeled decision tree classifier 
for data mining. Expert System with Application, 
28(4), 799-812.  
[11] Elomaa, T., Kujala, J., and Rousu, J. (2006). 
Practical approximation of optimal multivariate 
discretization. In proceedings of the 16th 
international symposium on foundations of 
intelligent systems (pp. 612-621). 
[12] Fayyad, U. M., and Irani, K. B. (1993). Multi-
interval discretization of continuous-valued 
attributes for classification learning. In proceeding 
of the 13th international conference on artificial 
intelligence (pp. 1022-1027). 
[13] Ferrandiz, S., and Boullé, M. (2005). Multivariate 
discretization by recursive supervised bipartition 
of graph. In proceedings of the 4th international 
conference on machine learning and data mining 
in pattern recognition (pp. 253-264). 
[14] Gómez, D., Montero, J., and Yáñez, J. (2006). A 
coloring fuzzy graph approach for image 
classification. Information Sciences, 176(24), 
3645-3657. 
[15] Han, J., and Kamber, M. (2006). Data Mining: 
Concepts and Techniques. (2rd ed.). Morgan 
Kaufmann. 
[16] Kerber, R. (1992). ChiMerge: discretization of 
numeric attributes. In proceeding of the 9th 
international conference on artificial intelligence 
(pp. 123-128).  
[17] Kurgan, L., and Cios, K. J. (2003). Fast class-
attribute interdependence maximization (CAIM) 
discretization algorithm. In proceeding of 
international conference on machine learning and 
applications (pp. 30-36). 
[18] Kurgan, L., and Cios, K.J. (2004). CAIM 
discretization algorithm. IEEE Transactions on 
Knowledge and Data Engineering, 16(2), 145-153. 
[19] Liu, H., and Setiono, R. (1997). Feature selection 
via discretization. IEEE Transactions on 
Knowledge and Data Engineering, 9(4), 642-645. 
[20] Liu, H., Hussain, F., Tan, C. L., and Dash, M. 
(2002). Discretization: an enabling technique. 
Journal of Data Mining and Knowledge Discovery, 
6(4), 393-423. 
[21] Mehta, S., Parthasarathy, S., and Yang, H. (2004). 
Correlation preserving discretization data mining. 
In proceeding of the 4th IEEE international 
conference on data mining (pp. 479-482).  
[22] Su, C. T., and Hsu, J. H. (2005). An extended chi2 
algorithm for discretization of real value attributes. 
IEEE Transactions on Knowledge and Data 
Engineering, 17(3), 437-441. 
[23] Tay, F., and Shen, L. (2002). A modified chi2 
algorithm for discretization. IEEE Transactions on 
Knowledge and Data Engineering, 14(3), 666-670. 
[24] Tsai, C. J., Lee, C. I, and Yang, W. P. (2008). A 
discretization algorithm based on class-attribute 
contingency coefficient. Information Sciences, 
178(3), 714-731. 
[25] Tsai, C. J., Lee, C. I, and Yang, W. P. (2009). 
Mining decision rules on data stream in the 
presence of concept drifts. Expert Systems with 
Applications, 36 (2), 1164-1178.  
[26] Wong, A. K. C., and Chiu, D. K. Y. (1987). 
Synthesizing statistical knowledge from 
incomplete mixed-mode data. IEEE Transactions 
on Pattern Analysis and Machine Intelligence, 9, 
796-805. 
[27] Wu, Q. X., Bell, D. A., McGinnity, T. M., Prasad, 
G., Qi, G., and Huang, X. (2006).  Improvement 
of decision accuracy using discretization of 
continuous attributes. In Proceedings of the 3rd 
international conference on fuzzy systems and 
knowledge discovery (pp. 674-683). 
[28] Yang, B., Sun, J. T., Wang, T., and Chen, Z. 
(2009). Effective multi-label active learning for 
text classification. In proceedings of the 15th 
ACM SIGKDD international conference on 
knowledge discovery and data mining (pp. 917-
926). 
[29] Zadrożny, S., and Kacprzyk, J. (2006). Computing 
with words for text processing: an approach to the 
text categorization. Information Sciences, 176(4), 
415-437. 
[30] Zhang, M. L., Peña, J. M., and Robles, V. (2009). 
Feature selection for multi-label naive bayes 
classification. Information Sciences, 179(19), 
3218-3229. 
 
98年度專題研究計畫研究成果彙整表 
計畫主持人：蔡政容 計畫編號：98-2218-E-018-002- 
計畫名稱：多值與多類別資料離散化之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100% 期刊論文目前在審查階段 
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 2 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
