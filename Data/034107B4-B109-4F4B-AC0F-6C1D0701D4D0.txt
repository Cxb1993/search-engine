溝通障礙之多模互動溝通行為 
資料庫建置與輔助訓練平台之研究(I) 
Development of Multimodal Communication 
Database and Assistive Training System (I) 
計 畫 編 號 ： NSC 97-2221-E-218 -043 
執 行 期 間 ： 97 年 8 月 1 日至 98 年 7 月 31 日 
主 持 人 ： 陳有圳  南臺科技大學電機工程系暨研究所  助理教授 
共同主持人 ： 吳俊良  國立成功大學醫學系耳鼻喉科  助理教授 楊惠美  國立成功大學研究總中心      技士 
 
摘要 
建置主要照顧者與孩童的口語溝通行為語料庫是非常重要的研究，此語料庫可以應用
於計算語言學與教育等方面；在計算語言學方面，可以使用語料庫中的標記資料進行發展
與評估自動化方法於標記言語行動、溝通傾向、言語變通與斷詞等方面。本研究完成設計
口語溝通行為語料庫，首先利用橫斷式實驗法選擇主要照顧者與孩童，採用直接觀察法進
行蒐集對話語料並轉譯成純文字，在資料儲存格式方面，應用兒童語料交換系統的格式使
之成為機器可讀的內容，其格式包含有三個主要成分：主要檔頭、固定的內容型態檔頭及
可變動的內容，此研究依據口語溝通行為的資料類型，設計出主要檔頭為使用者相關資訊，
固定的內容檔頭則成為每一個資料類型，類型包含了說話者為孩童或主要照顧者，而口語
溝通行為之資料類型分為詞彙、詞性、言語行動與溝通傾向。此口語溝通語料庫已經初步
分析，並使用於驗證其主要照顧者與孩童的口語發展模式，結果顯示語料庫的實用性。 
 
Abstract 
The oral communication of caregivers and children is very important for computational 
linguistics and education. It can be used to develop and evaluate many automatic approaches to 
identify items such as speech acts, pragmatic, word and part of speech. Besides, it can be used to 
analysis the relations of oral communication between caregiver and children and used to develop 
many applications. The goal of this project is to design an oral communication based corpus. The 
cross-sectional method is applied to select caregiver and child pairs. Observational method is 
used to collect the conversation corpus. In this task, caregiver-child interactions including reading 
a book and playing toys were recorded by two cameras and contained video and audio streams. 
The Child Language Data Exchange System (CHILDES) is used to transcribe and code the 
recordings into machine-readable text. For each caregiver and child pair, there are three headers: 
obligatory headers, constant headers, and changeable headers and two capital letters used to 
indicate the status of the speaker. *CHI is the child and *MOT the mother. Besides, the five 
dependent tiers used in this corpus are %WRD: words, %POS: part-of-speech code, %SPA: 
speech acts, %ICG: interchange, and %PFX: pragmatic flexibility. The collected corpus had been 
preliminary analyzed to find the relations between child’s language ability and caregiver’s oral 
communication skills. 
 
關鍵詞：口語溝通行為語料庫、言語參與程度、溝通傾向、言語行動、言語變通 
 
二、研究方法與步驟 
 
本研究依序完成了受測者挑選、活動設計、口語溝通行為之語料蒐集環境建置、口語
溝通行為之標記系統設計及標記信度分析，詳細描述如下： 
1. 受測者挑選 
以橫斷式(The Cross-Sectional Method)的抽樣方式，隨機選取志願參加互動溝通行為資
料收集的家長。兒童的年齡分布是 2.0 歲以上至 4.0 歲以下，以每 12 個月為一組，每組有
4 對的親子對，共計有 12 對的親子對。 
2. 兒童互動式遊戲活動設計： 
由主要照顧者與兒童在實驗室的遊戲室中，雙方在一張小桌子上面進行互動式遊戲，
在時間安排上，原則上每次錄影 50 分鐘以上，前 10 分鐘為預熱時間，目的是幫助孩子適
應環境；剩下的時間為自由遊戲時間，母子進入正常自然的溝通互動。在互動式遊戲活動
設計方面，預熱時間裡，主要照顧者和孩子一起玩積木；此外依據兒童活動表達特性與遊
戲性質進行分類，可得如下四種遊戲型態： 
A. 操作性質活動：在此活動中，讓兒童透過操作性玩具 (如球、積木、黏土等)進行
遊戲，使兒童進行手腳等操作活動。 
B. 角色扮演活動：給予兒童玩偶性質的玩具 (如芭比娃娃、機器人、變形金剛等) ，
讓小孩子與主要照顧者進行角色扮演的活動。 
C. 親子故事活動：依據兒童年齡與閱讀狀況挑選適當的故事書 (如厚紙板書、翻翻
書、圖畫書、繪本等)，由主要照顧者唸故事給孩子聽，並在故事中與孩子進行對
話，以瞭解其溝通行為。 
主要照顧者與孩子需要將所有的遊戲都玩一遍，當主要照顧者與兒童子把每項活動進
行 10 分鐘後(沒有上限時間)，或兒童提前放棄遊戲為止才停止拍攝。在活動進行之前，會
依據不同遊戲活動性質給予家長指導語，以利於促進兒童進行口語與非口語之溝通行為表
達。 
3. 語料蒐集環境建置 
在轉譯口語溝通行為時，許多資料需要依據主要照顧者與孩童的活動狀況進行判定，
因此除了語音資料需要錄製外，亦有必要錄製其對話之活動影像，因此，本研究透過高感
度麥克風錄製對話語料，亦採用兩個數位影像擷取裝置錄製孩子與主要照顧者的正面影
像，錄製環境架設如圖一所示。 
麥克風
兒童互動畫面
母親互動畫面  
圖一  語料蒐集環境示意圖 
 
4. 口語溝通行為之標記系統設計 
口語溝通行為主要有溝通傾向(interchange)、言語行動(Speech act)與言語變通(pragmatic 
flexibility)，本研究之標記系統採用「語用溝通行為目錄-簡要版，INCA-A」，其溝通傾向
共 22 種，如表一所示，而言語行動共 65 種，如表二所示，而言語變通則為溝通傾向與言
語行動的組合，因此不特別列出。 
為協助人工標記之便利性，本研究中設計電腦輔助介面進行協助人工標記，其執行畫
面如圖二所示，其中斷詞與詞性則採用中研院的 CKIP 斷詞系統協助初步標記，同時透過
此介面進行修正與檢視電腦輔助標記結果，最後檔案採用 CHILDES 的格式，使之具備有
機器可讀性，在 CHILDES 格式中，使用”*”進行標記與轉譯語者，其中”*CHI”表示兒童
而”*MOT”表示主要照顧者；每一個句子單元中使用”%WRD”、%POS、%SPA 與%ICG 進
表一、語用之言語行動類型，共 65 種 
 
三、結果與討論 
本研究共收集 12 對的母子對話語料(男孩 6 位及女孩 6 位)，總觀察時間 14 小時 12 分
鐘，而全部的對話句數為 20704 句，其中孩童有 6556 句，主要照顧者則有 14148 句，由此
可以觀察到，在孩子口語發展階段，母子對話的主控權在主要照顧者身上，多數為主要照
顧者主導孩子的話題。 
本研究中隨機挑選出 90 份口語溝通行為資料，平均分成兩份進行評分者間與評分者內
之信度分析，評分者由兩位口語溝通行為專家進行標記與分析，整體評分者間一致性為
80%，其中溝通傾向為 79%，且言語行動為 81%；整體評分者內一致性為 0.88，其中溝通
傾向為 92%，且言語行動為 84%。 
在口語溝通行為語料庫中，我們針對年齡進行單因子變異數分析，分別考驗三組不同
年齡階段的孩童(2、3、4 歲)或是主要照顧者，互動時之(1) 溝通傾向類型項目數量(2) )言
語行動類型項目數量(3)言語變通類型項目數量，觀察標記類型的數量之平均數的差異顯著
性，其結果如表三所示，可以看出，在年齡分類上面，使用三種溝通行為數量上並沒有顯
著差異，因此表示本研究中 2 至 4 歲孩童與主要照顧者互動時的對話量並沒有顯著的差異，
故可將三個年齡層視為同一個族群做進一步的口語溝通行為分析。 
 
表三、不同年齡階段口語溝通行為分析表 
口語溝通類型 年齡 人數 平均數 標準差 F 值 p 值 
溝通傾向(項) 3 歲 4 11.9 3.45 0.39 0.68 
 4 歲 4 10.9 1.85   
 5 歲 4 11.3 2.06   
 整體 12 11.37 2.5   
言語行動(項) 3 歲 4 26.4 3.86 3.24 0.06 
 4 歲 4 23.9 4.28   
 5 歲 4 22.1 3.14   
 整體 12 24.13 4   
言語變通(項) 3 歲 4 66.2 14.54 0.17 0.84 
 4 歲 4 70.2 16.01   
 5 歲 4 67.3 16.12   
 整體 12 67.9 15.13   
 
此外，我們運用此口語溝通行為料庫進行驗證主要照顧者與孩童口語溝通行為的關連
性，在主要照顧者與孩童的溝通傾向、言語行動、言語變通三個類型項目數量之間的相關
性分別為 0.855、0.625 及 0.765，具高度相關性，顯示主要照顧者與孩童間的口語溝通行為
有很大的關聯。 
 
四、結論 
本研究完成口語溝通行為語料庫，藉由影像與聲音的輔助，可協助進行言語行動、溝
通傾向與語言變通的標記，藉由「語用溝通行為目錄-簡要版，INCA-A」的編碼，口語溝
通行為可能有效成為機器可讀的資料。口語溝通行為語料庫未來可應用於發展自動化言語
行動與溝通傾向的自動標記系統，此外，在標記系統設計方面，斷詞與詞性標記部分，由
於孩童的語料中極多是屬於童言童語的詞彙，因此，現有的斷詞與詞性標記程式並沒有辦
法精準的標記，因此全部人工校准需要極大的工作量，因此現階段並沒有辦法完成，但是
童言童語的詞彙在幼兒教育部分是極為重要的應用領域，故藉由此語料庫的建置，能提供
一個童言童語的語彙，協助建構或加強現有斷詞程式與詞性標記程式的發展。 
可供推廣之研發成果資料表 
■ 可申請專利  □ 可技術移轉                                      日期：98 年 8 月 15 日 
國科會補助計畫 
計畫名稱： 
計畫主持人：         
計畫編號：             學門領域：殘障輔具研究
技術/創作名稱 具情意感知之構音訓練系統 
發明人/創作人 陳有圳 
中文：整合視覺、聲音與構音器官資訊於構音訓練可提高訓練之效
果。本研究藉由語音情緒辨識取得使用者情緒狀態，並依此挑選構
音訓練教材，最後使用語音、舌位影像資訊、嘴唇影像資訊等 3D
動畫影像輸出於使用者，讓他進行模擬正確之構音過程。 
技術說明 
英文：Articulation training with many kinds of stimulus and messages 
such as visual, voice, and articulatory information can teach user to 
pronounce correctly and improve user’s articulatory ability. In this 
project, an articulation training system with intelligent interface and 
multimode feedbacks is proposed to improve the performance of 
articulation training. Clinical knowledge of speech evaluation is used 
to design the dependent network. Then, automatic speech recognition 
with dependent network is applied to identify the pronunciation errors. 
Besides, hierarchical Bayesian network is proposed to recognize user’s 
emotion from speeches. With the information of pronunciation errors 
and user’s emotional state, the articulation training sentences can be 
dynamically selected. Finally, a 3D facial animation is provided to 
teach users to pronounce a sentence by using speech, lip motion, and 
tongue motion. 
可利用之產業 
及 
可開發之產品 
數位語言學習、構音障礙特性偵測系統、構音訓練系統 
技術特點 
 整合語音情緒辨識於使用者情緒狀態感知 
 應用 3D 合成技術合成具舌位與唇型資訊之 3D 虛擬臉部動畫 
 結合客製化構音訓練教材挑選於臨床語言治療之構音訓練 
 
推廣及運用的價值 
 建構構音合成多模輸出，可有效提升語言學習之介面設計，提
升語言學習的趣味性。 
 整合構音障礙類型判別技術，可建構語言治療師師訓練輔助系
統。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
• supply chain management  
• collaborative commerce  
• e-tailing and multi-channel selling  
• the latest trends in web services  
• e-commerce technology adoption  
• production of knowledge economy  
• knowledge economy and e-commerce  
• e-commerce business models  
• e-commerce, e-business strategies 
• customer relationship management  
• law, copyright, and intelligent property in e-commerce 
• business-to-business, business-to-customer, and business-to-government e-commerce 
• any aspects on buying, selling, marketing, distributing, and servicing of products 
• services over the Internet, wireless network, mobile phone, and other cybernetic systems 
• evolution of e-commerce 
• business-oriented or consumer-oriented e-commerce 
• cryptography for enabling e-commerce 
• future development of e-business, 
• e-business applications 
• trust or security for e-Commerce 
• digital economics, and digital content 
• EDI and the Internet 
• global or local e-commerce development 
•  retailing in e-commerce (e-tailing), 
• e-marketplaces other relative topics 
• other relative topics  
• EDI and the Internet 
• global or local e-commerce development 
• retailing in e-commerce (e-tailing), 
• e-marketplaces other relative topics 
• other relative topics 
e-Administration： 
• co-production in e-administrative services  
• innovation and diffusion of e-administrative services  
• practices and cases in e-governments, e-organizations, or e-institutions  
• system applications in e-governments, e-organizations, or e-institutions  
• technology integration in e-governments, e-organizations, or e-institutions  
• practices and cases in e-administration,  
 2
• role and mission of cyber police  
• technology adoption and diffusion of e-society  
• issues on e-society or e-community such as privacy, rumor, fraud, crime, attitude, and 
addiction 
• governmental and social main concerns on e-community 
• the growth and development of global or local e-community/e-society 
• main impact on traditional communities, multimedia and web-casting on the web community 
• moral or ethic Issues from the impact of communication via Internet 
• the future of e-society 
• the regulatory environment of e-society 
• perception differences of trust between e-society and traditional society 
• e-technologies in human interaction, community, education, or knowledge sharing 
• technical or psychological issues for e-society 
• open standards or software on e-society 
• innovation diffusion or rumor spread via e-society 
• specific cases on e-society 
• other relative topics 
e-Education： 
• practices and cases in e-education  
• systems and technologies in e-education  
• applications and integration of e-education  
• e-learning evaluation and content  
• campus information systems  
• e-learning technologies, standards and systems  
• mobile learning  
• computer aided assessments  
• knowledge management  
• virtual learning environments  
• multimedia in e-learning  
• marketing and promoting e-learning  
• social benefits of e-learning  
• organization learning  
• technology adoption and diffusion of e-learning  
• barriers to e-education or e-learning 
• e-education development in libraries 
• e-education application in museums 
• e-education or e-learning cases in military 
• corporate, government, or in higher education 
 4
 6
 
 
三、攜回資料 
1. 2009 International Joint Conferences on e-CASE and e-Technology之論文集光碟
一片及大會手冊各乙份。 
 
 
四、心得 
1. 當我國學術水準達到某層次之後，如何進入大會或國際組織核心變成比發
表論文更重要，此次會議中也看到許多我國進入國際組織並舉辦研討會的
成功例子。 
2. 此次會議議題相當廣泛，大家參與會議時討論亦非常熱烈，大會安排多場
海報式發表，使得大家可以針對自己興趣之主題充分與發表者深入討論，
因此收獲相當多。 
3. 此次會議台灣參加之人員約五十幾位，可說相當熱烈，也由於有多人參
會，大家亦可交換心得及了解目前在各個領域之研究情形。 
4. 經由這次會議的參與，不但得以認識一些相關領域之學者，互相交換研究
心得，而且可吸收最新資訊，對日後計劃之執行將有所助益。 
evaluation and language training, a time-consuming and expensive process. Moreover, 
for personal training procedure, the language training is very difficult to articulation 
disorders and caregivers. Therefore, an automatic process for speech evaluation and 
personal language training are very helpful to assist speech-language pathologists, 
articulation disorders, and caregivers. 
In speech evaluation, it is important to identify articulation errors and most 
articulation errors fall into those three categories: omissions, substitutions, or 
distortions. For speech-language pathologist, the articulation errors are examined in 
terms of the place and manner of articulation and can be classified into six articulation 
error patterns: fronting, backing, de-aspiration, stopping, affrication, and omission 
(Bernthal et al., 2004). In a typical fronting error, for example, a child may say /t/ 
instead of /k/ in the Chinese word /kan4/ so it would be heard as /tan4/. For backing 
error, the /q/ will be pronounced as the /k/ and /qi4/ would be heard as /ki4/. Therefore, 
the articulation error patterns are the most useful information to language training and 
should be identified in speech evaluation. 
Recently, researchers proposed various approaches to identify articulation errors using 
statistical models (Georgoulas et al., 2006) or tongue detection models (Sterns et al., 
2006, Hueber et al., 2007, Robineau et al., 2007). For statistical models, Georgoulas 
et al. applied support vector machine to classify only three consonant phonemes. Only 
those phonemes were insufficient to identify the articulation error patterns and the 
error information of pronunciation was insufficient. For tongue detection models, 
using ultrasound to examine speech production was gaining popularity because of its 
portability and noninvasiveness. The place of tongue could be detected from the 
ultrasound image. However, the resolution of detected results was insufficient to 
distinguish the five articulation error patterns. Moreover, the manner of articulation 
was also cannot be detected by this approaches. 
Automatic speech recognition (ASR) with spontaneous speech had been widely 
applied to many applications (Ali et al., 2001, Siniscalchi et al., 2007, Rajamanohar et 
al., 2005, Orzechowski et al. 2005, Li et al., 2005, Ali et al., 2001b, Mak et al., 2003). 
The features used in ASR were succeeded to identify the articulation attributes, which 
correlate closely with the place and manner of articulation (Siniscalchi et al., 2007, 
Rajamanohar et al., 2005, Orzechowski et al. 2005, Li et al., 2005, Ali et al., 2001b, 
Mak et al., 2003). Hence, ASR will be very useful to identify the pronunciation errors 
without manually labeling the articulatory information. With such approaches, the 
testing vocabularies with multiple syllables can provide more articulatory information 
and effectively reduce the evaluation time in clinical speech evaluation. 
Besides, dependency network (DN) had been applied to collective classification and 
knowledge discovery (Tian et al., 2006, Preisach et al., 2006). It is well suited to the 
2.1 Photo naming task 
In clinical protocol, speech-language pathologists use PNT represented as pictures to 
obtain the articulatory information of a child in speeches. The articulation error 
patterns of a articulation disorder can be identified from those speeches. Therefore, 
PNT should include familiar vocabulary words with recognizable pictures. This will 
decrease or eliminate the need for the child to imitate the clinician when presenting 
test stimulus items. Second, it should assess the production of all phonemes in a 
specific language. Those phonemes should be presented in at least two different word 
positions. Third, it should assess sounds in increasingly complex contexts. It should 
include target sounds in mono-syllabic and multi-syllabic words. Consequently, PNT 
with a set of familiar words can be written as 
W={w1, w2, …, wN} (1) 
where N is the number of testing words. Each testing word wi can be treated as a 
concatenation of phonemes and written as 
1 2 ii N
w s s s= "  (2) 
where sj is the j-th phoneme and Ni is the number of phonemes in wi. Therefore, PNT 
can be treated as a set of phonemes and written as 
S={s1, s2, …, sM} (3) 
where M is the number of phonemes appeared in each testing words of W. Each sj in S 
is modeled as Hidden Markov Model (Sher et al., 2006). 
 
Caregivers
Language Training
Activities in Game
User
Identification of 
Aritculation Error 
Patterns
Articulation 
Training Activities 
Selection
Photo Naming Task
Text Corpus for 
Language Training
Activities
Articulation 
Training Activities 
Selection
Multi-model 
Feedbacks
 
Fig. 1.  System architecture of automatic speech evaluation and multi-model 
feedback language training 
 
( ) ( ) ( ) ( ) ( )
( ) ( ) ( )( ) ( ) ( )1
ˆ ˆ ˆ
ˆ ˆ ˆl a l a
i i
m m m m m m m m m m m m
i l a
m m m m m m m m m
P E s s o P E s s P s s o P o P s
P E s s P s s P s o P o P s
ω ω ω ω+
=
≈
 (6) 
When a testing subject is actuated to articulate 1 2 ii Nw s s s= " , the corresponding 
speech observations Oi can be recorded. A DN defined in Fig. 3 is applied to 
automatically find the segmentations 1 2 ii NO o o o= "  and corresponding labeling 
results 1 2ˆ ˆ ˆ ˆ ii Nw s s s= "  with maximum posterior probability as follows: 
( )
( )
1 2
1 1 1 2 2 2
ˆ arg max
arg max
i
i i i
Ni
i i i i
w
N N N
s s s
w P w w O
P s s o s s o s s o
=
=

  "

  "  (7) 
For articulation disorders, the articulatory information of each phoneme are consistent 
and the coarticulation effect is modeled by context-dependent models. Thus, each 
phoneme is assumed to be independent and Eq. (7) integrated with Eq. (4) and Eq. (5) 
can be derived as 
( )
( ) ( ) ( )
( ) ( )( ) ( ) ( )
1 2
1 2
1 2
1
1
1
1
ˆ arg max
arg max
arg max
i
Ni
i
Ni
i
l a l a
Ni
N
i j j j
s s s j
N
j j j j j
s s s j
N
l a
j j j j j j
s s s j
w P s s o
P s s o P o P s
P s s P s o P o P s
ω ω ω ω
=
=
+
=
=
=
=
∏
∏
∏
  "
  "
  "


 
 (8) 
As the observation oj and target sj are constant across each estimation, the 
denominators P(oj) and P(sj) are omitted to reduce the complexity of estimation. 
Finally, the probability of labeling and segmentation can be estimated as 
( ) ( )( )
1 2
1
1
ˆ arg max
i
l a l a
Ni
N
l a
i j j j j
s s s j
w P s s P s o
ω ω ω ω+
=
= ∏
  "
   (9) 
In this paper, ( )aj jP s o  is estimated by HMM and ( )lj jP s s  is estimated by 
maximum likelihood estimation (MLE). 
Finally, a decision model with M tested phonemes is applied to identify Ei of testing 
subject. The likelihood for identifying Ei of testing subject is estimated as 
μ1 and μ2 is the mean of speech segments before and after boundary b, respectively. Σ 
is the common covariance matrix of O. For VV (V following V) boundaries, Bayesian 
information criterion is then applied to measure the difference of boundary b and 
written as 
( ) ( )( ) ( )1 21 1 1log log log 1 log2 2 2BIC b N b N b d d d Nλ ⎛ ⎞Δ = Σ − Σ − − Σ − + +⎜ ⎟⎝ ⎠
 (14) 
where Σ1 and Σ2 are the variance of segments before and after boundary b. λ is the 
penalty factor to compensate for small sample size cases. 
 
 
Fig. 4.  The block diagram of 3D facial animation 
 
For mandarin speech, there are 16 vowels and 21 consonants. Base on knowledge of 
acoustic phonetics, there are 105 categories of lip motions can be defined to represent 
the lip motions of all 408 Mandarin syllables. Given a feature point with location   
(xt, yt) in frame t, the location of this feature point in frame t+1 is decided by: 
( ) ( ) ( ) ( )( )2 2 21 1 ,
2 2
, arg min , ,
t u t v
m m
t t t t t t
x y m mi j
x y I x i y j I x i u y j v
+ +
+ +
− −= =
⎛ ⎞⎜ ⎟= + + − + + + +⎜ ⎟⎜ ⎟⎝ ⎠
∑ ∑  (15) 
where 
2 2
z zu− ≤ ≤  and 
2 2
z zv− ≤ ≤ . I(x, y) indicates the intensity of pixel (x, y). z is 
the block size that contains the possible location of the feature point in frame t+1. 
Finally, the control points are transformed to feature points of 3D facial models. 
Tongue models had been used in many research areas and non-offensive estimation 
was proposed in this paper. The 2D animations of mouth cavity for articulatory are 
collected. For each mouth cavity pictures of 2D animation of a phoneme, sobel 
operator is applied to detect the edge of tongue. Base on the edge of tongue, the 
respectively. The articulation error patterns of those samples were manually labeled 
by speech-language pathologists. In the training database, there are 45, 179, 88, 297, 
106, and 42 samples for fronting, backing, de-aspiration, stopping, affrication, and 
omission, respectively. Moreover, in the testing database, there were 15, 57, 28, 95, 33, 
and 13 samples for fronting, backing, de-aspiration, stopping, affrication, and 
omission, respectively. 
The priori probability of each pronunciation error, ( )ˆim m mP E s s , is different to 
determine the articulation error patterns and should be estimated in the training 
database. The probability of distributions of pronunciation errors for articulation error 
patterns is shown in Table II. It is clear that the correlation between pronunciation 
error and articulation error pattern is different. Some pronunciation errors can give 
confident to identify an articulation error pattern. However, for clinical practice, to 
identify an articulation error pattern should be verified by different phoneme’s 
pronunciation characteristic. 
 
Table II. Probability distributions of pronunciation error for 
each articulation error pattern 
Fronting Backing De-aspiration 
PE PB PE PB PE PB 
g→d 
k→d 
k→t 
zhi→d
chi→d
chi→t
83.33 
76.92 
81.08 
18.80 
27.48 
15.85 
d→g 
t→k 
zi→d 
zi→g 
ci→d 
ci→t 
87.96
92.57
16.94
93.09
10.42
14.89
p→b 
t→d 
k→g 
q→j 
ci→zi 
73.99 
68.79 
7.41 
65.96 
76.92 
Stopping Affrication Omission 
PE PB PE PB PE PB 
f→b 
l→g 
ri→g 
zi→d
zi→g
ci→d
ci→t 
75.26 
90.91 
83.87 
28.13 
96.28 
89.58 
87.23 
x→j 
shi→zi
si→zi
66.15
81.58
70.97
b→NULL
p→NULL
m→NULL
f→NULL
d→NULL
t→NULL
n→NULL
87.50 
45.61 
38.89 
47.50 
71.79 
75.00 
23.26 
PE: Pronunciation Error 
PB: Probability 
 
To decide the identification results, a threshold of articulation error pattern should be 
determined. The receiver operating characteristic (ROC) curves for each identification 
results of articulation error patterns in training database were estimated. The equal 
error rates of Fronting, Backing, De-aspiration, Stopping, Affrication, and Omission 
were 7.32%, 11.78%, 9.87%, 8.76%, 7.07%, and 4.89%. Moreover, the thresholds 
with 2D animation of mouth cavity and parts of animation were shown in Fig. 8. The 
experimental results achieve practical performance. Finally, the interface with 3D 
facial animation and text corpus for language training was shown in Fig. 9. In this 
approach, the text corpus for language training can be easily changed to be suitable 
for a user. 
 
 
Fig. 8.  Examples of synthesized 3D tongue animation and 2D animation of 
mouth cavity 
 
 
Fig. 9.  The interface with 3D facial animation and text corpus for language training 
 
4. Conclusion 
This work had presented an innovative approach to identify articulation error patterns 
and 3D facial animation based language training. It can assist speech-language 
pathologists in clinical speech evaluation and training. Using spontaneous speech 
based interactive interface, the articulatory information of articulation disorders can 
be effectively and friendly acquired in speech signal. Integrating dependency network, 
the pronunciation errors and articulation error patterns can be automatically identified. 
Moreover, a 3D facial animation including speech signal, lip movement, and tongue 
movement can promote articulation disorder to simulate the articulatory behavior of 
tongue. The training corpus can be also easily modified to satisfy user’s articulatory 
ability. Besides, the language training activities in games is provided to caregivers to 
enhance articulatory ability of articulation disorders in daily life. The experimental 
results show that this method is feasible. 
Orzechowski, T., Izworski, A., Tadeusiewicz, R., Chmurzynska, K., Radkowski, P., 
and Gatkowska, I., "Processing of pathological changes in speech caused by 
dysarthria," in Proc. of 2005 International Symposium on Intelligent Signal 
Processing and Communication Systems, pp. 49-52, Dec. 2005. 
Preisach, C. and Schmidt-Thieme, L., "Relational Ensemble Classification," in Proc. 
Sixth International Conference on Data Mining, pp. 499-509, Dec. 2006. 
Rajamanohar, M. and Fosler-Lussier, E., "An evaluation of hierarchical articulatory 
feature detectors," in Proc. of 2005 IEEE Workshop on Automatic Speech 
Recognition and Understanding, pp. 59-64, Nov. 2005. 
Robineau, F., Boy, F., Orliaguet, J.P., Demongeot, J., and Payan, Y., "Guiding the 
Surgical Gesture Using an Electro-Tactile Stimulus Array on the Tongue: A 
Feasibility Study," IEEE Trans. Biomedical Engineering, vol. 54, issue 4, pp. 
711-717, 2007. 
Sheng, H. et al, "Report of the police recommendation for manpower of linguistic 
therapist," The Magazine of Hearing and Language, The 
Speech-Language-Hearing Association of the Republic of China, vol. 16, pp 
76-91, 2001. 
Sher, Y.J., Chen, Y.J., Chiu, Y.H., Chung, K.C., and Wu, C.H., "MAP-based 
Perceptual Speech Modeling for Noisy Speech Recognition," Journal of 
Information Science and Engineering, vol. 22, no. 5, pp. 999-1013, Sep. 2006. 
Siniscalchi, S. M., Schwarz, P., and Lee, C. H., "High-Accuracy Phone Recognition 
By Combining High-Performance Lattice Generation and Knowledge Based 
Rescoring," in Proc. of IEEE International Conference on Acoustics, Speech 
and Signal Processing, vol. 4, pp. 869-872, April 2007. 
Stearns, M. and Frisch, S. A., "Production and perception of place of articulation 
errors," Journal of Acoustic Society of America, vol. 120, no. 5, pp. 3251, Nov. 
2006. 
Tian, Y., Yang, Q., Huang, T., Ling, C. X., and Gao, W., "Learning Contextual 
Dependency Network Models for Link-Based Classification," IEEE Trans. 
Knowledge and Data Engineering, vol. 18, issue 11, pp. 1482-1496, Nov. 
2006. 
Westbury, J. R., X-Ray Microbeam Speech Production Database User's Handbook, 
WI: University of Wisconsin Waisman Center, 1994. 
Wu, S. L., Teaching activities of language disorder, National Kaohsiung Normal 
University Special Education Center, 2000. 
Yu, B.L., "Appraisal and therapy of language disorder for children," The 
Speech-Language-Hearing Association of The Republic of China, pp.29-35, 
1992. 
• supply chain management  
• collaborative commerce  
• e-tailing and multi-channel selling  
• the latest trends in web services  
• e-commerce technology adoption  
• production of knowledge economy  
• knowledge economy and e-commerce  
• e-commerce business models  
• e-commerce, e-business strategies 
• customer relationship management  
• law, copyright, and intelligent property in e-commerce 
• business-to-business, business-to-customer, and business-to-government e-commerce 
• any aspects on buying, selling, marketing, distributing, and servicing of products 
• services over the Internet, wireless network, mobile phone, and other cybernetic systems 
• evolution of e-commerce 
• business-oriented or consumer-oriented e-commerce 
• cryptography for enabling e-commerce 
• future development of e-business, 
• e-business applications 
• trust or security for e-Commerce 
• digital economics, and digital content 
• EDI and the Internet 
• global or local e-commerce development 
•  retailing in e-commerce (e-tailing), 
• e-marketplaces other relative topics 
• other relative topics  
• EDI and the Internet 
• global or local e-commerce development 
• retailing in e-commerce (e-tailing), 
• e-marketplaces other relative topics 
• other relative topics 
e-Administration： 
• co-production in e-administrative services  
• innovation and diffusion of e-administrative services  
• practices and cases in e-governments, e-organizations, or e-institutions  
• system applications in e-governments, e-organizations, or e-institutions  
• technology integration in e-governments, e-organizations, or e-institutions  
• practices and cases in e-administration,  
 2
• role and mission of cyber police  
• technology adoption and diffusion of e-society  
• issues on e-society or e-community such as privacy, rumor, fraud, crime, attitude, and 
addiction 
• governmental and social main concerns on e-community 
• the growth and development of global or local e-community/e-society 
• main impact on traditional communities, multimedia and web-casting on the web community 
• moral or ethic Issues from the impact of communication via Internet 
• the future of e-society 
• the regulatory environment of e-society 
• perception differences of trust between e-society and traditional society 
• e-technologies in human interaction, community, education, or knowledge sharing 
• technical or psychological issues for e-society 
• open standards or software on e-society 
• innovation diffusion or rumor spread via e-society 
• specific cases on e-society 
• other relative topics 
e-Education： 
• practices and cases in e-education  
• systems and technologies in e-education  
• applications and integration of e-education  
• e-learning evaluation and content  
• campus information systems  
• e-learning technologies, standards and systems  
• mobile learning  
• computer aided assessments  
• knowledge management  
• virtual learning environments  
• multimedia in e-learning  
• marketing and promoting e-learning  
• social benefits of e-learning  
• organization learning  
• technology adoption and diffusion of e-learning  
• barriers to e-education or e-learning 
• e-education development in libraries 
• e-education application in museums 
• e-education or e-learning cases in military 
• corporate, government, or in higher education 
 4
 6
 
 
三、攜回資料 
1. 2009 International Joint Conferences on e-CASE and e-Technology之論文集光碟
一片及大會手冊各乙份。 
 
 
四、心得 
1. 當我國學術水準達到某層次之後，如何進入大會或國際組織核心變成比發
表論文更重要，此次會議中也看到許多我國進入國際組織並舉辦研討會的
成功例子。 
2. 此次會議議題相當廣泛，大家參與會議時討論亦非常熱烈，大會安排多場
海報式發表，使得大家可以針對自己興趣之主題充分與發表者深入討論，
因此收獲相當多。 
3. 此次會議台灣參加之人員約五十幾位，可說相當熱烈，也由於有多人參
會，大家亦可交換心得及了解目前在各個領域之研究情形。 
4. 經由這次會議的參與，不但得以認識一些相關領域之學者，互相交換研究
心得，而且可吸收最新資訊，對日後計劃之執行將有所助益。 
evaluation and language training, a time-consuming and expensive process. Moreover, 
for personal training procedure, the language training is very difficult to articulation 
disorders and caregivers. Therefore, an automatic process for speech evaluation and 
personal language training are very helpful to assist speech-language pathologists, 
articulation disorders, and caregivers. 
In speech evaluation, it is important to identify articulation errors and most 
articulation errors fall into those three categories: omissions, substitutions, or 
distortions. For speech-language pathologist, the articulation errors are examined in 
terms of the place and manner of articulation and can be classified into six articulation 
error patterns: fronting, backing, de-aspiration, stopping, affrication, and omission 
(Bernthal et al., 2004). In a typical fronting error, for example, a child may say /t/ 
instead of /k/ in the Chinese word /kan4/ so it would be heard as /tan4/. For backing 
error, the /q/ will be pronounced as the /k/ and /qi4/ would be heard as /ki4/. Therefore, 
the articulation error patterns are the most useful information to language training and 
should be identified in speech evaluation. 
Recently, researchers proposed various approaches to identify articulation errors using 
statistical models (Georgoulas et al., 2006) or tongue detection models (Sterns et al., 
2006, Hueber et al., 2007, Robineau et al., 2007). For statistical models, Georgoulas 
et al. applied support vector machine to classify only three consonant phonemes. Only 
those phonemes were insufficient to identify the articulation error patterns and the 
error information of pronunciation was insufficient. For tongue detection models, 
using ultrasound to examine speech production was gaining popularity because of its 
portability and noninvasiveness. The place of tongue could be detected from the 
ultrasound image. However, the resolution of detected results was insufficient to 
distinguish the five articulation error patterns. Moreover, the manner of articulation 
was also cannot be detected by this approaches. 
Automatic speech recognition (ASR) with spontaneous speech had been widely 
applied to many applications (Ali et al., 2001, Siniscalchi et al., 2007, Rajamanohar et 
al., 2005, Orzechowski et al. 2005, Li et al., 2005, Ali et al., 2001b, Mak et al., 2003). 
The features used in ASR were succeeded to identify the articulation attributes, which 
correlate closely with the place and manner of articulation (Siniscalchi et al., 2007, 
Rajamanohar et al., 2005, Orzechowski et al. 2005, Li et al., 2005, Ali et al., 2001b, 
Mak et al., 2003). Hence, ASR will be very useful to identify the pronunciation errors 
without manually labeling the articulatory information. With such approaches, the 
testing vocabularies with multiple syllables can provide more articulatory information 
and effectively reduce the evaluation time in clinical speech evaluation. 
Besides, dependency network (DN) had been applied to collective classification and 
knowledge discovery (Tian et al., 2006, Preisach et al., 2006). It is well suited to the 
2.1 Photo naming task 
In clinical protocol, speech-language pathologists use PNT represented as pictures to 
obtain the articulatory information of a child in speeches. The articulation error 
patterns of a articulation disorder can be identified from those speeches. Therefore, 
PNT should include familiar vocabulary words with recognizable pictures. This will 
decrease or eliminate the need for the child to imitate the clinician when presenting 
test stimulus items. Second, it should assess the production of all phonemes in a 
specific language. Those phonemes should be presented in at least two different word 
positions. Third, it should assess sounds in increasingly complex contexts. It should 
include target sounds in mono-syllabic and multi-syllabic words. Consequently, PNT 
with a set of familiar words can be written as 
W={w1, w2, …, wN} (1) 
where N is the number of testing words. Each testing word wi can be treated as a 
concatenation of phonemes and written as 
1 2 ii N
w s s s= "  (2) 
where sj is the j-th phoneme and Ni is the number of phonemes in wi. Therefore, PNT 
can be treated as a set of phonemes and written as 
S={s1, s2, …, sM} (3) 
where M is the number of phonemes appeared in each testing words of W. Each sj in S 
is modeled as Hidden Markov Model (Sher et al., 2006). 
 
Caregivers
Language Training
Activities in Game
User
Identification of 
Aritculation Error 
Patterns
Articulation 
Training Activities 
Selection
Photo Naming Task
Text Corpus for 
Language Training
Activities
Articulation 
Training Activities 
Selection
Multi-model 
Feedbacks
 
Fig. 1.  System architecture of automatic speech evaluation and multi-model 
feedback language training 
 
( ) ( ) ( ) ( ) ( )
( ) ( ) ( )( ) ( ) ( )1
ˆ ˆ ˆ
ˆ ˆ ˆl a l a
i i
m m m m m m m m m m m m
i l a
m m m m m m m m m
P E s s o P E s s P s s o P o P s
P E s s P s s P s o P o P s
ω ω ω ω+
=
≈
 (6) 
When a testing subject is actuated to articulate 1 2 ii Nw s s s= " , the corresponding 
speech observations Oi can be recorded. A DN defined in Fig. 3 is applied to 
automatically find the segmentations 1 2 ii NO o o o= "  and corresponding labeling 
results 1 2ˆ ˆ ˆ ˆ ii Nw s s s= "  with maximum posterior probability as follows: 
( )
( )
1 2
1 1 1 2 2 2
ˆ arg max
arg max
i
i i i
Ni
i i i i
w
N N N
s s s
w P w w O
P s s o s s o s s o
=
=

  "

  "  (7) 
For articulation disorders, the articulatory information of each phoneme are consistent 
and the coarticulation effect is modeled by context-dependent models. Thus, each 
phoneme is assumed to be independent and Eq. (7) integrated with Eq. (4) and Eq. (5) 
can be derived as 
( )
( ) ( ) ( )
( ) ( )( ) ( ) ( )
1 2
1 2
1 2
1
1
1
1
ˆ arg max
arg max
arg max
i
Ni
i
Ni
i
l a l a
Ni
N
i j j j
s s s j
N
j j j j j
s s s j
N
l a
j j j j j j
s s s j
w P s s o
P s s o P o P s
P s s P s o P o P s
ω ω ω ω
=
=
+
=
=
=
=
∏
∏
∏
  "
  "
  "


 
 (8) 
As the observation oj and target sj are constant across each estimation, the 
denominators P(oj) and P(sj) are omitted to reduce the complexity of estimation. 
Finally, the probability of labeling and segmentation can be estimated as 
( ) ( )( )
1 2
1
1
ˆ arg max
i
l a l a
Ni
N
l a
i j j j j
s s s j
w P s s P s o
ω ω ω ω+
=
= ∏
  "
   (9) 
In this paper, ( )aj jP s o  is estimated by HMM and ( )lj jP s s  is estimated by 
maximum likelihood estimation (MLE). 
Finally, a decision model with M tested phonemes is applied to identify Ei of testing 
subject. The likelihood for identifying Ei of testing subject is estimated as 
μ1 and μ2 is the mean of speech segments before and after boundary b, respectively. Σ 
is the common covariance matrix of O. For VV (V following V) boundaries, Bayesian 
information criterion is then applied to measure the difference of boundary b and 
written as 
( ) ( )( ) ( )1 21 1 1log log log 1 log2 2 2BIC b N b N b d d d Nλ ⎛ ⎞Δ = Σ − Σ − − Σ − + +⎜ ⎟⎝ ⎠
 (14) 
where Σ1 and Σ2 are the variance of segments before and after boundary b. λ is the 
penalty factor to compensate for small sample size cases. 
 
 
Fig. 4.  The block diagram of 3D facial animation 
 
For mandarin speech, there are 16 vowels and 21 consonants. Base on knowledge of 
acoustic phonetics, there are 105 categories of lip motions can be defined to represent 
the lip motions of all 408 Mandarin syllables. Given a feature point with location   
(xt, yt) in frame t, the location of this feature point in frame t+1 is decided by: 
( ) ( ) ( ) ( )( )2 2 21 1 ,
2 2
, arg min , ,
t u t v
m m
t t t t t t
x y m mi j
x y I x i y j I x i u y j v
+ +
+ +
− −= =
⎛ ⎞⎜ ⎟= + + − + + + +⎜ ⎟⎜ ⎟⎝ ⎠
∑ ∑  (15) 
where 
2 2
z zu− ≤ ≤  and 
2 2
z zv− ≤ ≤ . I(x, y) indicates the intensity of pixel (x, y). z is 
the block size that contains the possible location of the feature point in frame t+1. 
Finally, the control points are transformed to feature points of 3D facial models. 
Tongue models had been used in many research areas and non-offensive estimation 
was proposed in this paper. The 2D animations of mouth cavity for articulatory are 
collected. For each mouth cavity pictures of 2D animation of a phoneme, sobel 
operator is applied to detect the edge of tongue. Base on the edge of tongue, the 
respectively. The articulation error patterns of those samples were manually labeled 
by speech-language pathologists. In the training database, there are 45, 179, 88, 297, 
106, and 42 samples for fronting, backing, de-aspiration, stopping, affrication, and 
omission, respectively. Moreover, in the testing database, there were 15, 57, 28, 95, 33, 
and 13 samples for fronting, backing, de-aspiration, stopping, affrication, and 
omission, respectively. 
The priori probability of each pronunciation error, ( )ˆim m mP E s s , is different to 
determine the articulation error patterns and should be estimated in the training 
database. The probability of distributions of pronunciation errors for articulation error 
patterns is shown in Table II. It is clear that the correlation between pronunciation 
error and articulation error pattern is different. Some pronunciation errors can give 
confident to identify an articulation error pattern. However, for clinical practice, to 
identify an articulation error pattern should be verified by different phoneme’s 
pronunciation characteristic. 
 
Table II. Probability distributions of pronunciation error for 
each articulation error pattern 
Fronting Backing De-aspiration 
PE PB PE PB PE PB 
g→d 
k→d 
k→t 
zhi→d
chi→d
chi→t
83.33 
76.92 
81.08 
18.80 
27.48 
15.85 
d→g 
t→k 
zi→d 
zi→g 
ci→d 
ci→t 
87.96
92.57
16.94
93.09
10.42
14.89
p→b 
t→d 
k→g 
q→j 
ci→zi 
73.99 
68.79 
7.41 
65.96 
76.92 
Stopping Affrication Omission 
PE PB PE PB PE PB 
f→b 
l→g 
ri→g 
zi→d
zi→g
ci→d
ci→t 
75.26 
90.91 
83.87 
28.13 
96.28 
89.58 
87.23 
x→j 
shi→zi
si→zi
66.15
81.58
70.97
b→NULL
p→NULL
m→NULL
f→NULL
d→NULL
t→NULL
n→NULL
87.50 
45.61 
38.89 
47.50 
71.79 
75.00 
23.26 
PE: Pronunciation Error 
PB: Probability 
 
To decide the identification results, a threshold of articulation error pattern should be 
determined. The receiver operating characteristic (ROC) curves for each identification 
results of articulation error patterns in training database were estimated. The equal 
error rates of Fronting, Backing, De-aspiration, Stopping, Affrication, and Omission 
were 7.32%, 11.78%, 9.87%, 8.76%, 7.07%, and 4.89%. Moreover, the thresholds 
with 2D animation of mouth cavity and parts of animation were shown in Fig. 8. The 
experimental results achieve practical performance. Finally, the interface with 3D 
facial animation and text corpus for language training was shown in Fig. 9. In this 
approach, the text corpus for language training can be easily changed to be suitable 
for a user. 
 
 
Fig. 8.  Examples of synthesized 3D tongue animation and 2D animation of 
mouth cavity 
 
 
Fig. 9.  The interface with 3D facial animation and text corpus for language training 
 
4. Conclusion 
This work had presented an innovative approach to identify articulation error patterns 
and 3D facial animation based language training. It can assist speech-language 
pathologists in clinical speech evaluation and training. Using spontaneous speech 
based interactive interface, the articulatory information of articulation disorders can 
be effectively and friendly acquired in speech signal. Integrating dependency network, 
the pronunciation errors and articulation error patterns can be automatically identified. 
Moreover, a 3D facial animation including speech signal, lip movement, and tongue 
movement can promote articulation disorder to simulate the articulatory behavior of 
tongue. The training corpus can be also easily modified to satisfy user’s articulatory 
ability. Besides, the language training activities in games is provided to caregivers to 
enhance articulatory ability of articulation disorders in daily life. The experimental 
results show that this method is feasible. 
