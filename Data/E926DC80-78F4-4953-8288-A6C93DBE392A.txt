the reconstructed 2-D map. The robot navigation and 
basic human-robot interaction functions are 
implemented under the framework of Microsoft Robotics 
Developer Studio.  An efficient probability desnsity 
function (PDF) estimation method is developed for the 
time difference of arrival.  The estimated PDF can be 
used to locate multiple sound sources in a 
probabilistic framework.  Also, to track a yong child 
in the in the context of a particularly complex and 
dynamic environment, we developed an IMU-based 
tracking system and also a cooperative-game-theory-
based PTZ camera network control algorithm.  To deal 
with the location category recognition problem, we 
propose a KNN-sifted sparse coding method which 
outperforms the existing methods in the experiments.  
Furthermore, we have developed a system for 
predicting age progression in children＇s faces from 
a small exemplar-image set, which is a critical task 
to assist in the search for missing children. Lastly, 
in developing a camera robot system, we have proposed 
a method for detecting and avoiding common 
photographic mistakes.  Also, we have implemented a 
set of camera works allowing the robot to film a 
social activity automatically.  Based on a face 
recognition method, we have proposed a video editing 
system that can compose a customized video for each 
person in the raw video shots. The abovementioned 
methods have been tested thoroughly and the results 
all show that the proposed methods are very 
promising. 
英文關鍵詞： Robot Pet, Simultaneous Localization and Mapping, 
Sound Source Localization, Lie Group, Accurate Image 
Registration, Trust Region Algorithm, Location 
Category Recognition, Face Age Progression Predition, 
Photo Composition Rules, Customized Video 
Composition. 
 
 2
中文摘要 
在本三年期計畫中，我們的主要任務是發展嬰幼兒照護環境中的機器寵物所需的定
位、機器竉物基本功能、聽音辨位、人物軌跡追蹤、場所類別辨識、臉型成長預測、機
器人攝影師（靜態照片構圖或動態影片剪輯合成）等七項主題。在定位計算方面，我們
使用 Lie Group 與 Homography 模型作多重影像貼合，以整合成一個總體一致的 2-D 地
圖，並發展於大量資料貼合的快速演算法。其次為利用抽取出來的特徵，計算目前所拍
得影像與已建立好的 2-D 地圖的關係，加以精確定位出機器寵物的位置。另外我們也在
Microsoft Robotics Developer Studio 架構下實現了機器寵物的控制導航與基本人機互動的
功能。在聽音辨位的研究上，我們推導出訊號延遲時間快速最大概度估測法 
(Maximum-Likelihood Estimator)，以估算聲音來源的方位。為了解決在複雜且動態的環境
中的嬰幼兒軌跡追蹤問題，我們發展了使用慣性量測單元的追蹤法，以及以合作戲論控
制 PTZ 攝機的方法。在場所類別辨識方面，我們改良現有稀疏編碼的缺點，引入 KNN
預選編碼機制，以達到比現有方法更佳的辨識效果。在臉型成長預測方面，我們展一個
適用於小型訓練資料庫的預測法，可用於失蹤兒協尋。在機器人攝影師方面，我們提出
偵測以避免常犯攝影錯誤的方法，另外也發展一套基於人臉辨識的視訊剪輯系統，可以
為每個人量身打造一部活動記錄片。上述方法都經過實測，以確認有良好成效。 
 
關鍵詞： 機器寵物，同時定位及建地圖，聲音定位，Lie 群，精密影像貼圖，信賴區間
演算法，場所類別辨識，臉型成長預測，照片構圖，客製化影片合成。 
 
  
 4
 
一、前言 
微軟的創辦人 Bill Gates 在 2006 年底時於 Scientific America 撰文表示，他在機器人
產業發展上，看到了 30 年前個人電腦產業發展的影子。他並大膽預言將來機器人的普及
率將可與今日的個人電腦普及率相比。台灣機器人產業發展協會表示：「服務型機器人將
成為台灣的發展重點之一，預估將來機器人產業的產值將以每年 30%的速度成長，並可
望加速我國機械產業突破兆元大關」；其中所謂的服務型機器人又稱為「智慧型機器人」。
我們可以預見智慧型空間、智慧型家具、以及智慧型機器人將構成數位化生活科技與應
用中的骨幹。而這個整合型計畫的主要目的是藉由發展一套以增進幼兒安全與健康為目
標又可留下幼兒時期記憶的智慧型幼兒照護環境與回憶系統。 
嬰幼兒總是需要父母投入很多的精神與時間來照護，對於還要擔負家計與處理家務
的父母，在精神與體力上都可能無法完全負荷。目前有關嬰幼兒照護的系統多以無線監
看與監聽為主，但是通常在照護者家務纏身時，除非幼兒發出異常聲響否則根本無暇理
會是否有緊急狀況。幸運的是這些缺憾應該都可以藉由科技加以彌補，在這個計畫中，
我們除了要照顧到幼兒的健康狀況之外，也將研究如何自動地將小朋友在成長過程中的
重要時刻，以擬似專業攝影師的方式記錄下來。藉由這套系統可以留下幼兒各式記錄，
提供一個人在其人生最初幾年中的寶貴記憶。 
 
二、研究目的 
在這個整合型計畫中，我們將建構一個智慧型嬰幼兒照護環境。這個照護環境包含
一個由多重攝影機構成的智慧型空間(Intelligent Space, 簡稱 i-Space)，一個智慧型搖籃
(Intelligent Cradle, 簡稱 i-Cradle)，一個智慧型機器竉物(Intelligent Pet Robot, 簡稱 i-Pet)，
等三套子系統。嬰幼兒在幾個月大時每天需要 16 到 20 小時的睡眠時間，及其稍長仍然
需要有 12 小時以上的睡眠。i-Cradle 可藉由監控幼兒的呼吸等生理訊號、或由加速度計
收集其活動資訊，整合不同感測資料以偵測異常狀態。幼兒清醒的時候，如果離開 i-Cradle，
即由 i-Space 以及 i-Pet 加以照顧。其中 i-Space 中的攝影機裝設在固定位置，所以易產
生死角，但是 i-Space 因為可以取得市電，所以其可擴充的計算能量相當大。另一方面，
因為 i-Pet 可以自由活動，所以可以取得最佳視角的影像，但是其電源來自於有限的電池，
使得其計算能量先天受限。因此，這兩套系統必須相互合作，除了要偵測嬰幼兒異常活
動提供警訊外，也同時將自動擷取嬰幼兒成長過程中值得紀念的影音資訊，並記錄於總
計畫建置的 Data Center。除了記錄嬰幼成長過程的點點滴滴，也將發展臉型成長預測功
能，平日可作為機器竉物與父母互動之娛樂活動，萬一嬰幼兒不幸遭拐騙又無法即時尋
回，也可提供嬰幼成長後的臉型預測供持續協尋，提供全方位的守護。 
 
本子計畫在此一整合型計畫中所要擔負之主要任務與研究目標如下： 
1. 機器寵物定位技術研發：研究 i-Pet 之定位技術，在陌生環境中自動建構地圖模型供
發展快速定位法，以利導航及追蹤，並將定位結果標識於搜集到的影音資料上。 
2. 以 MSRDS (Microsoft Robotics Developer Studio) 為基礎，在半靜態（即活動障礙物位
置及移動速度對路徑規劃造成的影響在 i-Pet 之 CPU 計算能量所能負荷的範圍內）環
境中規劃路徑，以到達所指定位置，並可追蹤或尋找幼兒去向。另外也將開發語音、
運動控制、物體辨識與追蹤等模組，並用以完成支援與其他子計畫合作研究項目所需
之功能。 
3. 機器寵物聽音辨位技術研發：研究機器人在聽到異常聲響（如：哭喊、碰撞）時如何
利用麥克風陣列收音之時間差，進行定位，並回報至照護系統系統。 
4. 機器寵物人物軌跡追蹤技術研發：研究機器人在動態且多人的複雜環境中，如何利用
 6
技術較少且多為多媒體應用所開發，精確度不足相關文獻回顧請參見 [Chuang10]。研究
成果 [Chuang10] 是我們在執行第一年的計畫中，應用李群理論 (Lie Group Theory) 於通
用線性群 (General Linear Group) 所完成的精確全域貼合之理論及實作成果。但是這個研
究成果在機器人導航的應用上有兩個問題，第一個是計算量與參與貼合的影像量成正比，
貼合進行到後期，其計算量會成長到阻礙即時定位運算。第二個問題是我們是使用通用
線性群中的投影座標 (Projective Coordinates) 來近似表示機器人的歐式座標 (Euclidean 
Coordinates)，其間的誤差量降低定位精確度，導致障礙物地圖的建構不準。針對這兩個
問題，我們將提出有效的解法。 
 
3.2 機器人定位---定位計算 
在地圖中計算機器人的位置可分為兩種不同的狀況，第一種狀況是機器人的前一時
刻位置為已知，因此目前的位置也可由里程計或運動模型粗估而得。此時要尋找新的位
置搜尋範圍較小，可用 Extended Kalman Filter [Castellanos99, Dissanayake00, Newman02] ，
Particle Filter [Dellaert99, Sim07] 或者是 Gaussian Process Models [Brooks08] 做定位計算。
第二種情況是在機器人剛開機或者是位置追蹤失敗以致於失去定位資訊時，即需要在整
張貼合好的地圖中，快速地找到與攝影機新取到的影像相符的區塊。快速定位常用的方
法有使用 SIFT 做特徵點抽取比對[Se01, Loncomilla05, Gil05, Tamimi05, Frontoni07, 
Park08]，但是 SIFT 特徵的比對方法並不適用於地板這種特徵重複度比較高的紋理
[Lowe99]。Williams 等人使用 Randomized Lists Classifier 比對特徵點作快速定位 
[Williams07]，但是這個方法如同 SIFT 一樣，在紋理特徵重複度高的應用中也容易出錯。
此時常用的方法是影像 Cross-Correlation [Weiss95]，例如 Kelly 將影像特徵強化之後再以
Cross-Correlation 計算機器人定位 [Kelly00]。 
在本研究中第一種情形為已知粗略位置，其搜尋範圍較小，比對資料量少，所以可
以採用全影像比對的方法，如 Cross-Correlation [Weiss95]。第二種情況位置不明，所以要
和所有地圖影像做比較，因完整的地圖資料量大，甚至可能是由不同材質的地板所組成
的地圖。因此需要使用特徵比對方法，加速其運算結果。所以其特徵抽取法的選擇正確
與否極為重要，目前的特徵抽取都有其不適用的地方，我們將研究在不同的地板上其各
適用的特徵抽取法。 
 
3.3 MSRDS i-Pet 基本操控功能 
i-Pet 的發展功能中，包含了導航、Text-to-Speech、基本的物體辨識與追蹤等三項基
礎功能。i-Pet 自動導航工作包含了超音波聲納資料處理、安全動作規畫、障礙物閃避、
目的地導向等。由於 i-Pet 是活動式機器人，其電源來自於有限的電池，使得其計算能量
先天受限，因此使用的導航和障礙物閃避方法，以計算簡單且可達成效果為主要目標。 
由於 i-Pet 必定會和嬰幼兒有一定的互動，因此其安全動作的規劃十分重要。Fraichard
和 Asama 對機器人的安全動作規範出三項準則，指出機器人的安全動作規劃必須考慮到
機器人自己的動態特性、障礙物的未來動向和未來環境估測等 [Fraichard07,  
Fraichard03]。 
Velocity Obstacles (VO) [Fiorini98] 是一套由 Fiorini 和 Shiller 提出，用於機器人於
環境中閃避障礙物的方法。VO 雖然考慮了機器人的動態特性和動態環境下的障礙物，並
規範出機器人的安全速度，達到讓機器人閃避障礙物的目的，但是 VO 的運算也較複雜，
較不適合用在使用電池電力運轉的 i-Pet 上。 
Ray, Behera 和 Jamshidi 則提出使用 GPS 來對機器人導航，並配合超音波聲納感測
器來實現障礙物閃避的方法 [Ray09]。這套方法比較適合用於室外，於空間狹小的室內較
不合適，且室內環境也會影響到 GPS 的信號接收。 
 8
 
 
Fig. 3.4-1 訊號延遲之關係圖。 
 
麥克風陣列的擺放方式將會影響聲源到達麥克風之時間點以及所對應的角度，所以
由麥克風收到的訊號，會因麥克風排列的不同，聲源到兩麥克風間角度的差異性以及處
理訊號方式也會有所改變，一般常見陣列組合大致分為三類： 
1. 線性均勻陣列： 
將麥克風依照固定距離配置成線性排列，其優點為在遠場聲源 (Far-Field) 之假設條
件下，聲源波型為一平行波，各麥克風接收到之訊號只有時間延遲的差異，所以可利用
較簡單的幾何關係處理訊號系統。其缺點則是麥克風之間距不可大於信號波長一半，否
則會產生 Location Aliasing，導致其訊號解析度下降。 
2. 線性非均勻陣列： 
主要是針對線性均勻陣列的缺點而改進，使得在不同間距下的麥克風陣列可以偵測
出更高頻寬之方位，缺點為方向角度估計會有所偏差，代表性陣列為巢狀陣列 (Nested 
Array)。 
3. 平面/3D 陣列： 
為二維至多維之空間陣列排列，雖然其信號計算複雜度較高，但是對於多聲源之訊
號或是雜訊較大之情況下能有較好的結果，常見的有圓形、環型以及十字陣列。 
目前的聲音定位方法中較常用的方法是 GCC，但若要使用 GCC-ML 估測，則必須由
聲音中估測出兩訊號的 Coherence，而若要正確估測 Coherence 則需要很長的取樣數據，
所以其分析計算也很耗時。使用 PHAT 的速度雖然快很多，但是估測到的時間延遲沒有
其它的精確度/可靠度資訊，所以並沒有系統化的方法可以在陣列訊號分析中整合 PHAT
估測的時間延遲量。在本研究中，我們將著眼於時間延遲量的機率模型建立，目標是要
建立快速的時間延遲量 ML 估測法。 
 
3.5 人物軌跡追蹤文獻探討 
在軌跡追蹤方面，一般是利用感測器訊號來做步態分析，方便後續限制條件的建立
以去除感測器雜訊的影響，並增加軌跡追蹤的準確度。因此，在步態分析部份， Zijlstra 
和 Hof [Zijlstra03] 將三軸加速規裝置於軀幹上收集常人行走的加速度資料，透過峰值偵
測法 (Peak Detection Method) 對每個步伐做切割。接著， Zijlstra 等人 [Zijlstra08] 將加
速規資料經低通濾波器(Low-Pass Filter) 後結合陀螺儀資料經高通濾波器 (High-Pass 
Filter) 來求得身體的旋轉量，以進一步分析軀幹的移動量以利定位。在這部份，我們是
採用 Jiménez 等人 [Jiménez10] 所提出的方法，利用加速度值、加速度的變異量與角速度
值與閥值的比較，做為判斷當前步態是否為站立狀態的依據。 
在步態分析完成之後，即可利用此一資訊做有效的軌跡追蹤。Song 等人 [Song07] 將
   
v: sound speed 
td: time difference of arrival 
acoustic source s(t) 
microphone 02 microphone 01 
noise 
wavefront 
distance: v*td 
() 
() 
() 
noise () 
 10 
方式更新字彙樹，以利處理更龐大的資料[Ji08]。也有人由不同場所中拍攝的多張影像取
出 SIFT 特徵用來描述每個場所，並且利用隱藏式馬可夫模型 (Hidden Markov Model, 
HMM) 來學習各特徵點之間的相對位置關係，用來克服環境中改變與模糊的特徵所造成
的辨識困難 [Košecká05, Torralba03, Li06]。Dudek 與 Jugessur 使用 Interest Operator 抽
取物體或場所的局部特徵，然後利用主成份分析 (Principal Components Analysis, PCA) 降
維並進行比對 [Dudek00]；但這個方法不能克服視角改變所造成差異的問題。 
第三類方法是以切割好的影像區塊以及區塊間的相對關係來描述場景 [Katsura03, 
Matsumoto00, Murrieta-Cid02]，這類方法最大的問題是缺乏穩定的影像切割方法。在光源
有變化時，可能會造成影像切割的結果產生劇烈的變化，而使得辨識效果不佳。 
第四類方法是由整張影像中抽取出可以描述這個場景的特徵向量，常用的特徵有影
像色彩的 Histogram [Ulrich00]，或者是紋理形式的 Histogram [Renniger04]。Wu 和 Rehg 
使用 Census Transform (CT) 將原始影像亮度值轉換成比較不受照明變化影響之 CT 值，
然後將 CT 影像切割成很多小區塊，再將每個小區塊的 Histogram 以 PCA 降維用來描
述這個場景 [Wu08]。但是這個方法無法克服視角改變所造成差異的問題。Oliva 和
Torralba 使用局部 Fourier Transform 加上 PCA 來產生低維的特徵向量用以分辨場所 
[Oliva01]。 
第五類方法是由影像中學習出最具代表性的影像區塊或特徵用來描述不同的場景。
Siagian 和 Itti 參考靈長類動物對視覺資訊中顯著特點 (Saliency) 及摘要資訊 (Gist) 的
計算模型，發展由 Gabor Filter 及 Difference of Gaussian (DoG) Filter 計算出色彩、亮度
以及邊緣方向等低階特徵，再將這些低階特徵取平均得到摘要向量 (Gist Vector)，最後再
使用 PCA 求得低維度的特徵向量 [Siagian07]。這個方法的最主要缺點是在由低階特徵
取平均得到摘要向量的過程中，毫無根據地直接將影像切割成 4×4 共 16 個區塊，這個
動作會使得場所的特徵向量容易受影像尺度變化以及影像平移的影響，造成辨識結果不
正確。Ni 等人將場所外觀以及幾何資訊利用 Jojic 等人在 2003 年所提出的 Epitome 技
術  [Jojic03] 貼合成一個  Generalized Panorama，對每一張輸入的影像都可以在這個 
Generalized Panorama 中找到最相似的區塊，然後由預先指定的場所標籤經由投票機制即
可決定其場所判斷結果[Ni08]。在同一篇論文中，他們也展示可將輸入影像粗分為 2×3 的
區塊，然後使用這個區塊的 Histogram 來取代這個影像區塊，代入 Epitome 技術貼合成
一個 Histogram 的 Generalized Panorama。這個做法的好處是影像微幅形變對 Histogram 
的影響較小，所以辨識精確度會提昇而且計算量也會縮小。 
在以上這五類場所辨識技術中，以第五類辨識方法的彈性最高，不但可以處理外觀
資訊也可以引入空間相對關係，所以最為吸引人。但是這類技術對視角的變化較為敏感，
而且其中都包含了一個較不具說服力的粗略影像區塊切割法 ([Siagian07] 將影像切成 4
×4 的區塊，[Ni08] 則將影像切成 2×3 的區塊)。[Siagian07] 的 Gist Feature 是由低階特
徵的平均值經過 PCA 萃取而出，所以 Gist Feature 與原本影像的視覺關聯較弱；另一方
面，Epitome 的技術是由場所影像資料庫所學習而出的影像縮圖，縮圖與原本影像的關
聯性很強。但是 Epitome 與原本影像的空間關係是被表示成一個隨機程序，有無窮多個
由 Epitome 還原成原本影像的貼圖方法。由空間關係未定這個觀點來看，Epitome 比較
像是 Bag of Words 的描述方式。但是 Epitome 並未完全打散影像的空間關係，如同 
[Jojic03] 中計算出來的 Epitome 所示，在 Epitome 的局部影像區塊上，還是可以看到完
整的結構，而且原始影像的精華資訊都被收錄在 Epitome 當中，而 [Ni08] 並沒有妥善
的應用這些資訊。因此， Epitome 的技術其實可以被改善來應用於場所辨識中，但這個
技術的最大致命傷是其計算量太大。在本研究中，我們將以 SIFT 特徵建構 Codebook，
並利用影像區塊將空間資訊編進特徵向量中，以期能快速並精準地辨識出場所的類別。 
 12 
們使用這個成長函數搭配一個現有的紋理變換方法來進行臉上成長發展預測，但是這項
研究並沒有測試兒童的 FAPP 結果 [Gandhi05]。 Scherbaum 等人從影像重建了 3-D 臉部
外形，而使用 SVR 來建構成長的 3-D 臉型頂點和紋理，以合成成長後的影像。 
上述這三種方法全都需要一個由許多不同人的數據/圖像合成的大型訓練資料集合。
他們都試圖找出大多數人的平均成長模型來執行與人體測量數據統計、PCA、AAM 或
SVR 的 FAPP。因此，這三種方法的主要缺點是他們往往會忽略個體差異。其實用來預估
兒童臉型成長的最佳方式，是使用該名孩童之近親在不同年齡的照片。但是近親的人數
可能非常有限，導致訓練資料不足，所以現有的方法在實用上可能會受到限制。 
從一個小型影像訓練集合找出輸入照片的全臉成長模型其困難度相當高，除非這一
小型影像集都是由長得非常相似的人（例如，同卵雙生）所提供的一系列照片。在另一
方面，因為近親有著共同的遺傳因子，所以在親屬之間比較容易找到相似的局部臉部特
徵（眼、鼻、嘴、面部外形等）。因此，本研究主要貢獻是提出一個以五官元件為主的 FAPP
方法，它非常適合小型的 FAPP 訓練資料庫。 
 
3.8 機器人攝影師自動構圖文獻探討 
西門子公司的 Huang 等人在 1998 年時就提出自動利用攝影機來取得事件影片的概
念 [Huang98]，但是他們的系統只強調事件的偵測與人物的追蹤，完全沒有引入構圖美學
的規則，所以完成的系統比較像是一套整合式監控系統。Byers 等人則是在 2004 年展示
了一套加入美學規則的靜態照片自動拍攝機器人 (Robot Photographer) [Byers04]，這個照
相機器人藉由自動偵測人臉來找出要拍攝的對象並依構圖美學規則找尋最佳拍攝位置。
Byers 等人的系統提供了相當令人滿意的取像方式，但是他們的系統與拍攝對象間沒有其
它的互動，而且因為在計算取像角度時必須以雷射掃描取得三維資訊，所以受限較多。
Campbell 和 Pillai 提出使用 Motion Parallax 偵測模式來取得深度資訊，以避開使用雷射
掃描器 [Campbell05]。至於互動系統方向則有 Ahn 等人提出可以藉由偵測招手的方式來
決定要拍攝的對象，並由其揮動的左右手來決定其拍攝模式是要單純取人像，或是要拍
攝帶景人像  [Ahn06]。利用機器人拍攝帶景人像的研究亦可見於[Dixon02, Byers03, 
Byers04, Kim10]，或也有人專攻以機器人拍攝風景照[Campbell05, Gadde11]。近年來 Sony 
也發表一個稱為 PartyShot 的產品，可以自行決定構圖與最佳拍照時機，不需人工介入，
即可取得高品質照片。 
此外，在亞洲微軟也有一些學者將研究目標轉為在一張已擷取好的靜態照片上運用
構圖法則來裁切出最符美學規則的區塊 [Zhang05b]，清大資工的陳煥宗教授團隊，也曾
發表在一張全景影像中搜尋最像專家取景構圖的風景影像技術 [Chang09]。  
有鑑於實現基本攝影構圖法則的相關研究已經相當普遍，在本研究中我們將研究重
心置於常犯攝影錯誤的避免。至於相片構圖的部份，我們將直接實作現有的構圖技術
[Byers04]，以清楚呈現出主題。 
 
3.9 機器人攝影師影片剪輯合成文獻探討 
相較於自動照像機器人文獻數量，用於動態攝影機的機器人文獻數量就少很多。大
部份的相關文獻皆注重於如何協助初學者，避免拍攝到搖晃不穩定、不好的構圖、不停
的變換 Zoom 的大小、以及單調冗長的運鏡等易犯的錯誤。例如在 Adams 和 Venkatesh 的
論文中即提出一個輔助專家系統，透過讓使用者回答一系列的問題，即可瞭解欲拍攝的
場合與事件背景，然後系統便會自動產生拍攝的主題、運鏡模式、每一鏡頭拍攝連續時
間、以及構圖模式等等建議 [Adams05a, Adams05, Adams06]。但是他們的方法只能在事
先給予建議，在實際拍攝時無法實際介入攝影者的操作，故取得的影片仍然可能不具吸
引力。 
 14 
Lie Algebra: ( ),gl +h  
Exponential map: ( ), loge= =hH h H  
一般而言，General Linear Group 不滿足交換率， 1 2 1 2e e e +× ≠h h h h 。但是當 1h 與 2h 數值
極 小 ， 根 據 Baker-Campbell-Hausdorff (BCH) 公 式 [Govindu04, Varadarajan84] ，
1 2 1 2 1 2( , )e e e e +× = ≈h h BCH h h h h 。 
在多張影像貼合的過程中，將參考並修改我們先前的研究的方法 [Shih08]。在之前
的研究 [Shih08] 要解決的是三維座標轉換矩陣 (Special Euclidean Group, SE(3)) 的估測
誤差，而在建立地圖中，我們要探討的是 Homography (General Linear Group, GL(3) ) 的
估測誤差。這兩者最大的差異是 SE(3) 的矩陣限制條件多，所以計算數值較穩定，而 
GL(3) 的唯一限制是矩陣為可逆，且為齊次 (Homogeneous) 矩陣，僅有八個自由度。在
本研究中，已提出解決變數與自由度不符的方法。 
假設每張影像當成一個節點 (Node)，並使用邊 (Edge) 連結有兩兩貼合的影像節點，
如此即可將影像貼合的關係表示成一個圖 (Graph)。此圖中若有迴圈，沿著此迴圈的邊走
回同一個節點，在理想的狀況下，迴圈中的邊所代表之貼合參數相乘之後應是一個單位
矩陣，但因拍照時可能會有輪胎半徑不均勻或是地面不平等等所造成影像的投影失真，
會讓兩兩貼合產生誤差。每個迴圈是由多個邊之相乘，因此其誤差會逐漸累積起來，這
時需要有可以縮減累積誤差的方法。 
為方便說明，我們將以一簡單的例子作說明。其假設圖中的某一四個邊的迴圈其兩兩
貼合相乘的結果如下: 
 
1 2 3 4
ˆ ˆ ˆ ˆ
≈ + ∆H H H H I                          (4.1-3) 
 
ˆ
iH 為有誤差的兩兩貼合參數，∆即為累積的誤差量。若使用 (4.1-2) 式中的理想貼合參
數值，則迴圈中的貼合參數相乘可得： 
 
31 2 4
1 2 3 4
ˆ ˆ ˆ ˆe e e e ≈
hh h hH H H H I                      (4.1-4) 
 
因為 11i i i i ii ie e e
−
′
−
= =
h h H h HH H ，上式中的修正量可以利用下式，全部移至方程式前方。 
 
1
j i j i
−
′ =h H h H                            (4.1-5) 
 
因此可得 
 
31 2 4
1 2 3 4
ˆ ˆ ˆ ˆe e e e
′′ ′ ′
≈
hh h h H H H H I                         (4.1-6) 
 
將 (4.1-3) 代入 (4.1-6) 式，同時假設修正量很小 （即 ih 的數值很小），上式可簡化為 
 
 
1 4
1 4
e e
c
′ ′ ′ ′ ≈

′ ′ ′ ′ + =
2 3h +h +h +h ∆
2 3
I
h + h + h + h ∆ I
                       (4.1-7) 
 
在 (4.1-7) 式中，參數 c 為 Homography 的自由參數所形成，其值可為任意非零值。在
本研究中，我們將此一數值定為 ∆ 的第三行第三列數值，故可將  (4.1-7) 式改為
 16 
其中 N2 是與新進影像可能有關聯的影像，而 N1 則是不可能與新進影像關聯的影像。要
簡化計算，最直覺的方法是要將 N1 化簡為廣義載維寧等效電路 (Generalized Thévenin 
Equivalent Circuit)，如 Hosoyo [Hosoya07] 的方法。但是現有的化簡方式涉及 Y-∆電路互
換，而這種轉換的方法僅適用於純量電阻，不適用於在本研究中的對稱電阻矩陣式，因
此必須自行研發化簡方法。 
 在研究中，我們證明 (4.1-8)-(4.1-9) 式可以化簡為下式 
 
2 2 2 2 2
1 1
2 2c c c c c
′ ′+ + +t t t tx Q x x Q x g x g x                  (4.1-10) 
subject to 2 22 2 2 0c c′ ′+ + =L x L x d                      (4.1-11) 
 
其中 Q1, Q2, Qc 與 x1, x2, xc 分別代表在 N1, N2 與 Cut-set 中的電導矩陣與電阻電壓，
A1 與 A2 分別代表在 N1, N2 中的節點與分支 (Branch) 相鄰矩陣 (Adjacency Matrix)， 
Ac1 與 Ac2 代表在 N1, N2 中的節點與 Cut-set 分支的相鄰矩陣，L21、L2c 與 L22代表包
含了N2 或 Cut-set 中任一分支的基本廻路矩陣 (Fundamental Loop Matrix)，而 
 
( ) 11 1 1 1 1c c c c c c−′ = +t tQ Q A A Q A A Q Q                     (4.1-12) 
( )
( )
( )
1
1 1 1 1 1 10 1
1
1 1 1 1 1 1 1
1
1 1 1 1 1 1
c c c c c c c c
c c
c c
−
−
−
′ = + −
+
−
t t t t t t
t t t
t t t
g g g A A Q A A Q y A Q
h Q A A Q A A Q
g A A Q A A Q
           
 (4.1-13) 
( ) 12 2 21 1 1 1 1 1c c c c−′ = − t tL L L A A Q A A Q                    (4.1-14) 
( ) 12 2 21 1 10 21 1 1 1 1 1c c−′ = + −t t td d L A y L A A Q A A g             (4.1-15) 
( ) ( )110 1 1 1 1 1 1 1 1−= −ty A Q A A Q h A g                      (4.1-16) 
 
這個化簡的電路中已將相關的變數去除，如此即可大幅降低計算量。 
 
4.2 定位計算 
在定位計算方面，是將已經貼好的地圖，與目前 i-Pet 所拍攝到的地板照片做影像
比對，影像比對方法大致可以分為直接影像比對、影像特徵值比對和混合這兩種技術等
三種方法，其執行效能與適用的環境各有不同。我們假設 i-Pet 已知前一刻的位置，目
前的位置可以由移動位移量粗估而得，因此要找尋新位置的搜尋範圍較小，可以採用影
像直接比對的方法，即針對其影像中的每個像素 (Pixel) 比較，找出其像素之差總合最小
的，即為所求。 
在較大的範圍中搜尋影像時，我們則是以 Ordinal Filter 先將影像轉為二值化的特徵
影像，並利用二值化影像 Hamming Distance 快速比對的特性，以濾除不相像的區塊，留
下較有可能的結果，供第二階段的精密比對。 
另外為了將投影座標 (Projective Coordinates) 轉為機器人的歐式座標 (Euclidean 
Coordinates)，我們引入平面的成像模型： 0 0 0 01 2w ww w =  H K r r t ，其中K 為攝影機的
校正矩陣， 0
wi
r 為旋轉矩陣 0 wR 的第 i 行 (Column)。移動後的機器人相對於參考影像的 
 18 
Fig. 4.3-1  i-Pet 導航控制架構圖。 
 
i-Pet 使用 Nearness Diagram (ND) [Minguez04a, Durham08]來實現 Local Navigation。
ND 是一種 Reactive Navigation Method [Minguez04b]，適合處理在複雜凌亂的環境中機
器人的障礙物閃避問題，並往目的地靠近。它使用了 “Divide and Conquer” 的方式，
將 i-Pet 週遭複雜的環境區分為 5 種狀況，再賦予處理每種狀況相對應的動作。 
接下來會先闡述 ND 的資料，即是超音波聲納感測器的資料。再來是闡述 ND 這
套方法： 
1. 超音波聲納感測器(Sonar Sensor) 
超音波聲納感測器能夠偵測障礙物的距離。由於 i-Pet 只有架設前端的超音波聲
納感測器 Fig. 4.3-2，為了讓 i-Pet 盡可能的取得完整的資訊，因此在開啟電源後，會
先讓 i-Pet 先自轉一圈，取得 360 度的完整資訊。 
當 i-Pet 開始進入導航時，超音波聲納感測器對應到的角度資訊會不斷被更新，
沒被對應到的角度資訊則根據 i-Pet 的移動來計算更新。每一次資訊更新完成後，就
會使用 ND 計算處理。Fig. 4.3-3 是實際測得的聲納回傳值。 
2. ND Method 
ND [Minguez04a] 對超音波聲納感測器所建立的環境資訊進行描述後 Fig. 4.3-4，
其最終目的是要找到可通行且最靠近目的地的區間和行駛方向。ND 根據環境資訊描
述和使用者定義的安全距離，將 i-Pet 周圍環境歸納為 5 種狀況，並提供相對應的處
理動作： 
I. Low Safety 1 & 2 (LS1 & LS2) 
安全距離範圍內有障礙物。此時會以閃避障礙物為優先。 
II. High Safety 
安全距離範圍內沒有障礙物。可再細分為 Goal Region (HSGR)、Wide Region 
(HSWR)和 Narrow Region (HSNR)。此時會以目的地導向為優先。 
 
 
Fig. 4.3-2 超音波聲納感測器，圖上是前視圖，編號 0 和 7 的聲納在側面，圖下是上
視圖，分別標示前端與各聲納所在之位置的角度。 
  
 20 
 
 
Fig. 4.3-6 距離轉換法找到的最短路徑 
 
在探索過程中所收集到的每一筆超聲波聲納感測器的資料都包含障礙物的距離資訊，
但是因為聲納感測器發射的聲波寬度為 20 度角。如 Fig. 4.3-7 所示，要利用障礙物的距
離資訊反推障礙物位置時會無法精確定位。圖中 di 為聲納可能偵測到的障礙物距離，li
為表示障礙物可能分佈範圍的一條弧線，弧線長度與距離成正比。 
 
 
Fig. 4.3-7  聲納模型 
 
計算障礙物位置時是在聲納模型的弧上建立多條切線，並使用 Hough Transform 統計
直線障礙物最可能的位置，如 Fig. 4.3-8 所示。 
 
 
Fig. 4.3-8 Hough Transform 統計障礙物位置計算示意圖 
 
 
基本功能 (二)： Text-to-Speech 
使用 MSRDS 提供的 TTS Service 來實現 Text-to-Speech 功能。 
基本功能 (三)： 物體辨識與追蹤 
物體辨識的功能是為了實現與幼兒的互動遊戲，並不是要發展通用且大量的物體
辨識方法，因此我們直接採用 SIFT 作為辨識的基礎方法。 
 
4.4 聲音定位 
在本研究中的聲音定位乃是利用一組線性均勻分布的麥克風陣列，並以兩兩麥克風
估算聲音到達之時間差以估測位置。當麥克風接收到信號時，系統會自動對各個單位的
 22
 
Fig. 4.4-2  能量偵測系統流程圖。 
 
下圖表示一個麥克風單位經由聲響偵測過後所得到的訊號資料，Fig. 4.4-3(a) 及 Fig. 
4.4-3 (b) 分別為兩個麥克風所擷取的音訊資料，Fig. 4.4-3 (c) 則是將 Fig. 4.4-3 (a) 和 
Fig. 4.4-3 (b)的訊號同時顯現做比較，可以發現 Fig. 4.4-3 (b) 的訊號比起 Fig. 4.4-3 (a) 
明顯延遲了一段時間，導致兩訊號無法完整重合在一起，因此我們使用 TDOA 聲音定位
方法求出此段延遲時間，使得兩訊號能夠盡可能的重疊，得到最正確的定位角度。 
 
 
Fig. 4.4-3  聲響偵測之訊號資料。 
儲存音框
資料 
計算每筆
音框之能
量 
State
=1 是
儲存資料
清空 
State=3 State=2 
假訊號，儲存
資料清空，儲
存目前音框
資料，state=1 
default 
State
=3 
State
=2 
是
是
State=3 
是否達
到最大 是
否
Error! 
Switch(stat
e 初始化為
1) 
呼叫定位系
統，儲存資
料清空，
state = 1 
否
是 是
否
是
是
否
否否
是
 24
變化，呈線性遞增，將 kφ∆ 正規化後，Fig. 4.4-4(a) 便會改以 Fig. 4.4-4(b) 的型式表現。
因此只要在 Fig. 4.4-4(b) 中利用 Hough Transform 找出經過原點的線性遞增直線，便能
得到兩麥克風之相對延遲時間。兩麥克風間的 TDOA 可表示成： 
 
( )
2
k
k
ND C
k
φ
pi
∆
= −
                        
(4.4-9) 
 
 
Fig. 4.4-4  k- kφ∆ 之座標圖。 
 
Ck 為未知數，主要是將 kφ∆ 降到 0 到 2π 之間的最小整數值，我們將上式反推求 Ck可得
到： 
 
2
k
k
kC D
N
φ
pi
∆
= −
                           
(4.4-10) 
 
延遲時間 D 可以由兩麥克風之間的距離求出其範圍，所以我們定義 Ck 在最大延遲時間
Dmax的範圍內： 
 
max max
max max
min max
[ , ]
2
2
k
k
D D D
kC D
N
kC D
N
φ
pi
φ
pi
∈ −
∆
= +
∆
= −
                         
(4.4-11) 
 
最後利用 Hough Transform 對每個可能的 D 做投票，得票數最高即為兩麥克風之相對延
遲時間。 
而在實際情況中，因為雜訊 Vi(k) 的影響，其 k 值所對應的 kφ∆ 會在直線附近飄移，
導致準確率降低。為了改善此問題，我們將經由傅立葉轉換後的兩頻域訊號做前置處理，
取出兩訊號在頻域的極座標( kρ , kφ )： 
 
 26
化簡後可發現
 
2
, , ,
( ) ~ ( , ( / ) )i k i k k i kp Nϕ φ σ ρ
                      
(4.4-17) 
 
利用(4.4-17)式可推得相位差的 PDF，即 ( )1, 2 ,( ) k kkp pφ ϕ ϕ∆ = − ，最後將每個 k 值下的
( )kp φ∆ 取 log 相加，其最大值即為所求之 D 最大概度估測值。 
而由 TDOA，即 D，轉換 θ 的方式，可利用線性均勻分布的麥克風陣列特性 Fig.4.4-5
求得，即 
 
TDOA
v
b
s
=
θsin
                           (4.4-18) 
 
b 為代表麥克風間的距離，vs 為聲速，θ 代表聲源到兩麥克風中垂線之角度。 
 
 
Fig.4.4-5  均勻線性陣列之模型圖。 
 
4.5 人物軌跡追蹤 
人物軌跡追蹤是使用三軸加速規訊號 ®bk、三軸角速度陀螺儀訊號 !bk
 
與三軸磁力
計訊號 ´bk，做行人行走的路線追蹤與定位，其中 ®bk、!bk
 
與 ´bk
 
的值皆為在時間k時，
區域的身體坐標系統 (Body Coordinate System) 下所表示的三維向量。Fig. 4.5-1 為本方
法系統流程之主要區塊，包括步態偵測、感器測姿態估測與更新及行人位置估測三大區
塊。 
 
 
Fig. 4.5-1 系統流程圖 
 28
域坐標系統採用的是右手坐標系統，定義 X 軸為正北方、Y 軸為正西方、Z 軸為正上方。
接著，在步態處於擺動狀態時，利用三軸陀螺儀訊號 !bk
 
對身體坐標系進行旋轉，更新
目前感測器姿態的資訊。最後，在擺動狀態結束時，我們將加入判斷式來避免感測器的
訊號誤差。感測器姿態的估測與更新流程如 Fig. 4.5-3 所示。 
 
 
Fig. 4.5-3 感測器姿態的估測與更新流程 
 
在步態處於站立狀態時，全域坐標系統的 X 軸方向在身體坐標系統表示下，可藉由
將磁力計量測出的向量 ´bk
 
去除掉垂直方向的分量，即 ´bk
 
投影在 ®bk
 
方向的分量，求
得水平方向的正北方即全域坐標系統的 X 軸方向在身體坐標系統下的表示： 
 
gXbk = ´
b
k ¡
´bk ¢ ®
b
k
j®bkj
®bk。                     (4.5-5) 
 
全域坐標系統的 Z 軸方向在身體坐標系統的表示下，即為站立狀態時三軸加速規訊
號值 ®bk
 
的方向： 
 
gZ bk = ®
b
k。                           (4.5-6) 
  
得到全域坐標系統的 X 軸與 Z 軸在身體坐標系統的向量後，Y 軸方向即可藉由兩軸外
積求得： 
 
gY bk = gZ
b
k £ gX
b
k。                     (4.5-7) 
 
因此，身體坐標系統轉換至全域坐標系統的轉換關係矩陣 Cgbk
 
可由上述三式結果的
單位向量組成，如下表示： 
 
Cgbk =
h
gXb
k
jgXb
k
j
gY b
k
jgY b
k
j
gZb
k
jgZb
k
j
i¡1
3£3
。                  (4.5-8) 
  
另外，旋轉矩陣在身體坐標系統表示下，即為沿著 !bk
 
軸旋轉 µbk
 
角度，其中 µbk
 
角度
為角速度 !bk
 
在時間 ¢ t
 
區間的積分。以斜對稱矩陣 (Skew Symmetric Matrix) 來表示
旋轉軸 [!bk ]£
 
再乘上旋轉角度 µbk
 
後，取其方陣指數，即為在身體坐標系統下時間 k
 
時
表示的旋轉矩陣： 
 
Rbk = e
[!b
k
]£ µbk。                         (4.5-9) 
  
 30
加入此概念做修正，假設其估算夾角 Ãbk
 
包含一常數值為誤差夾角 Ãbek，將之去除即可
求得該時間點的實際夾角 Ãbak： 
 
Ãbak = Ã
b
k ¡
Ãbek
¢T
k，                     (4.5-14) 
 
即可使擺動狀態後的夾角發散問題解決，並與靜止狀態的夾角做適當的吻合。 
 
 
 
 
Fig. 4.5-4、位置估測區塊細部流程 
 
 另外，在行進過程中，腳步擺動狀態結束時的航向角 (Yaw Angle) Áks方面，我們
也加入條件做修正，防止陀螺儀訊號偏差及雜訊的影響與磁力計訊號受到週遭磁場干擾
的影響。這邊我們參考 Borestein 等人 [Borestein09] 及 Jiménez 等人 [Jiménez10] 所提
出的方法做了修改套入我們的流程中，想法是認為一般人在行進時，基本上是以直線行
走為主，因此在航向角 Áks
 
加入一個條件式判別是否為錯誤航向角 Áeks，在此考量到
航向角的改變量 ¢Ák，定義如下： 
 
¢Ák = Áks ¡
Áks¡1 + Áks¡2
2
，                   (4.5-15) 
 
其中，ks
 
為最後的腳步擺動狀態結束時間點，ks¡1
 
為當前腳步擺動狀態的前一個腳步
擺動狀態結束時間點，ks¡2
 
以此類推。根據假設，當行人的航向角改變量 ¢Ák
 
小於閥
值 th¢Á
 
時，則我們視該改變量為陀螺儀的偏差，需將之移除；反之，則我們認為行人
是確實改變了行進方向。因此，錯誤航向角 Áeks
 
的判別條件如下： 
 
Áeks =
(
¢Ák j¢Ákj · th¢Á
0 otherwise
。                 (4.5-16) 
 
 經由判別條件判斷後，若認為行人確實改變了行進方向 ¢Ák，則將改變後的航向
角 Áks，及該時間點身體坐標系統的 X 軸與正北方的夾角 Ácompassk做比較，若兩角的差
小於閥值 th¢Ácompass，則我們相信由磁力計訊號 ´bk
 
所推算出來的角度；反之，我們認
為磁力計訊號在該點受到外部磁場變化的干擾，因此相信由陀螺儀訊號積分所得到的角
度： 
 
Áks =
(
Ácompassk jÁks ¡ Ácompassk j · th¢Ácompass
Áks otherwise
。       (4.5-17) 
 
 32
如何根據影像的內容來進行分類為近幾年影像處理的熱門問題，例如將影像根據場
景 (森林、海邊、廚房、客廳…等) 來進行分類。目前許多依據場景的分類方法大多是
適用於戶外場景，而適用於室內場景的分類的方法則寥寥無幾。用 Bag-of-Features 的技
術來處理整張影像的分類在[Grauman05, Wallraven03, Willamowski04] 展現出不錯得成
果。但是 Bag-of-Features 的特徵擷取卻會損失空間的資訊，使得特徵的代表性有限。 
本研究中，我們探討使用稀疏編碼和支持向量機來進行室內場景類別辨識。我們採
用 Yang 等人在 2009 年 CVPR 所提出使用稀疏編碼的 Sparse-coding Spatial Pyramid 
Matching (ScSPM) 方法來建構特徵向量來表示影像資訊。ScSPM 建構特徵向量的方式
主要有四個步驟，其流程圖如 Fig. 4.6-1 所示。 
首先是局部特徵的抽取，為取得空間資訊，將輸入的影像依照不同的層次分隔成不
同大小的影像區塊。並將在相同層次的影像區塊分割成 2×2 的子影像區塊，此子影像
區塊即為下一個層次的影像區塊大小。接下來，在不同層次的每一個影像區塊中計算其
局部特徵，我們計算局部特徵的方式是使用 Fei Fei 等人在 2005 年 CVPR 所提出的
“強特徵＂方法。。所謂的強特徵就是在影像中每 16 × 16 像素當作一個小方塊，在
每一個小方塊裡計算一SIFT 描述子，每個小方塊間彼此重疊 p 個像素長度，0 < p < 16，
如 Fig. 4.6-2 所示。 
 
 
Fig. 4.6-2 利用強特徵當作局部特徵的示意圖。每 16 × 16 像素的 
小方塊 (紅色方框) 計算一個 SIFT 描述子，每個小方塊重疊 8 個像素長度， 
依序計算描述子當作影像的局部特徵，如黃色部份所示。 
 
抽取完局部特徵後，我們修改 Lee 等人在 2007 年 NIPS 所提出的稀疏編碼演算
法將局部特徵轉換成高維度的稀疏係數向量，稀疏編碼的過程主要分成編碼表的建立與
稀疏係數向量的編碼兩個部份。為了有效率的利用稀疏編碼來建構編碼表，選擇 Lee 等
人在 NIPS 2007 所提出的稀疏編碼演算法 [Lee07]。其演算法的目標函數如下： 
 
min
B;S
kX¡BSk2F + ¯
X
i;j
Á
³
s
(i)
j
 ´                (4.6-1)
 
受限於
 
P
i
³
b
(j)
i
´2
· c; 8j = 1; :::; n:               (4.6-2)
 
 
其中X 2 Rd£m 為由輸入訓練資料所構成之矩陣，每一行為一筆訓練資料；B 2 Rd£n 為
一基底矩陣，每一行為一個基底向量; S 2 Rn£m 為一稀疏係數矩陣，每一行為一個稀
 34
針對這個問題，我們提出一項修改方法，主要目標是確保相似的局部特徵可以編碼
出相似的稀疏係數向量，如 Fig. 4.6-3 (b) 所示。具體作法是在稀疏係數向量的編碼前，
加入 KNN 的限制，先搜尋 k 個與局特徵最相近的編碼向量後，再執行 Feature-sign 搜
尋演算法求得稀疏係數向量。 
第三個步驟是將落於每一個影像區塊中的稀疏係數向量利用 Max Pooling 函數整
合成一個特徵向量，如 Fig. 4.6-4 所示。將每一個影像區塊所計算出特徵向量堆疊成單
一個特徵向量來表示影像資訊。 
 
Fig. 4.6-4 Max Pooling 示意圖 
 
第四個步驟是使用線性 SVM 來訓練分類器辨識場景類別，SVM 的訓練採取
One-Verse-All 的方式：每一個訓練器學習如何分辨是否為此類別。 
 
4.7 臉型成長預測 
我們所提出的算法的流程圖如 Fig. 4.7-1 所示。假設有一個包含了多個受試者在不同
年齡臉部影像的成長資料庫。並且假設在資料庫中的每張影像已經利用適當軟體 (如
FaceGen) 轉變成一個 3-D 人臉模型。同時為了簡化後面的計算，我們將所有生成的 3-D
人臉都調整為無表情的臉，如 Fig. 4.7-2 所示。 
 
 
Fig. 4.7-1 臉型成長預測流程圖 
 
 36
 
給定一個 3-D 成長資料庫，我們將描述如何選擇 k 個最相近的臉部元件來合成預測
的新面貌。為了要做到這一點，我們必須決定兩個五官元件之間的相似度。而相似度有
兩個方面需要考慮，我們不僅要評估兩個五官元件 3-D 網格的相似性，而且還要評估他
們的成長曲線的相似性。五官元件的成長曲線包括了標準化位置的軌跡 (即,	,) 和相
對大小 (即,	,)，軌跡與大小皆為 a 的函數。 
 
因為歐幾里德距離 (Euclidean Distance) 可以直接反應位置和大小的差異，成長曲
線 ', = )	,	,*+ ∈ 	,-. 和 ,	 = ),	,*+ ∈ 	,-. 的差異定義如下: 
 
D0',, ',12 = ∑ 4,	, − ,	,14																																																	∈67 (4.7-1) 
D0,, ,12 = ∑ 4,	, − ,	,14																																														∈67 (4.7-2) 
 
其中,- ⊂ A。在決定相似的五官元件中，選擇一個適當的距離測量是必不可少的。在相
同年齡組當中，為了要評估兩個受試者的五官元件 3-D 網格相似度，兩個 3-D 網格需要
具有相同質量中心和相同寬度 (即,	,)。因為每個元件的網格結構都是相同的，所以
對應的網格位置可以很簡單的完成。在網格點對應完成之後，每個定位網格頂點&的
3-D 座標連接起來形成一個向量，記做9,	, ∈ :;<。然後距離可以表示為 D(9,	,, 9,	,1)，
其中 D(∙,∙)是一個距離函數，將在後面討論。因為 Euclidean Distance 不符合大多數人主
觀認定的兩個網格間的距離，在這項研究中考慮到兩個距離函數，即曲率加權加上彎曲
能量距離 (Curvature-Weighted Plus the Bending Energy Distance) 和基於學習的馬氏距離 
(Learning-Based Mahalanobis Distance) [Davis07]。 
 
 
Fig. 4.7-4 臉部元件合成示意圖 
 
 如 Fig. 4.7-4 所示，在本研究中所使用的臉部元件合成方法是基於小時候相似的五
官，長大後依然相似的假設。假設 >;,? 與 >?,? 分別為由三歲與十歲輸入照片抽取之某
一臉部特徵，而 @; 與 @? 分別是在成長資料庫中和 >;,? 與 >?,? 同一年齡組最像的
五官特徵集合。假設欲預測之年齡為 16 歲，由 @; 與 @? 在成長資料庫中對應到 16
歲的成長後五官特徵，可做為該名兒童 16 歲時之五官特徵參考。由於所有的五官特徵
都是表示成固定格式之 3-D 網格，故可直接利用 3-D 網格之加權平均來合成該名兒童
16 歲時之五官特徵。權重的計算方式是採取小時候像的權重高，年齡差距小的權重高的
 38
Fig. 4.8-1 人臉朝向估測系統流程 
 
 
Fig. 4.8-2 用攝影構圖法則拍攝的人臉偵測影像 
 
Fig. 4.8-3 突出物偵測演算法流程圖 
 
Fig. 4.8-4 (a) 是利用分水嶺演算法分割影像成小區塊的結果，為避免分水嶺演算法
造成過度分割，我們已事先使用高斯濾波器平滑化影像。Fig. 4.8-4 (b) 是使用 KLT 特徵
演算法去偵測並追蹤影像中在有利於追蹤的特徵點。Fig. 4.8-4 (c) 則是適應性背景去除
法的前景偵測結果。 
 
 
Fig. 4.8-4 (a) 分水嶺分割結果 (b) KLT 特徵點 (c)前景偵測結果 
 
整合這兩種前景偵測結果可以減少錯誤偵測率。Fig. 4.8-5 為所提出的前景偵測演算法結
果： 
 
(a)  
(b)  
Fig. 4.8-5、前景偵測結果 (a)原始影像 (b)前景偵測結果 
 40
 
Fig. 4.9-1 動態影片攝製流程圖。 
 
 
在本報告中，我們將動態攝影分為構圖與運鏡、異質攝影機、視訊的剪輯與合成等
三個子問題分別探討於下： 
4.9.1 構圖與運鏡 
一般而言，一個高品質的影片經常是以一種講故事的方式來呈現其內容。使用攝影
機來講故事就像是一種視覺語言，沒有適當的運鏡會使得影片不夠生動，而且也不能感
動觀眾。常見的攝影機拍攝規則 [Hampe97, Artis07, Rabiger09] 大多需要知道被拍攝人
物的位置，所幸這些資訊可以用 Viola 和 Jones 的人臉檢測方法 [Viola04] 或者是使用
Kinect 偵知人體骨架 [OpenNI]。 
構圖 (亦稱為"Framing") 是視覺敘述的一部分，一個好的構圖可以使拍攝的影像具
平衡的組織並可傳遞給觀眾賞心悅目的感覺。在攝取影片時，我們利用 Kinect 的人體
骨架及人臉偵測，以控制機器人及 PTZ 攝影機，實現井字構圖 (Rule of Thrids)、頭上空
間 (Head Room)、水平構圖 (Horizontal Composition) 等三種構圖規則。 
在攝影機的運鏡方面，我們已實現三種類型的相機移動： 
1. Zoom In/Out: Zoom In/Out 運鏡是透過改變光學的焦距，來縮放拍攝主題、取得長景、
中景和近景影片。其中，長景是將視角放大，以拍攝整個主題。根據 [Artis07]，這
是一個保守但安全的業餘愛好者拍攝類型、中景基本上是取人物腰部以上的景、而
近景則是用來強調拍攝對象的對話與表情。 
2. Follow and Track: Follow Mode 的基本概念是，攝影機正對著、並保持一定距離跟隨
著拍攝對象前後移動。Track Mode 與 Follow Mode 很像，唯一的區別是相機的方向
與攝影機的運動的方向不同。在 Follow Mode 下，相機的方向是平行於攝影機運動
方向，與被拍攝對象相對而行。相反地，在 Track mode，攝影機的指向垂直於其運
動方向，攝影機與被拍攝對象並行。 
3. Dolly In/Out: Dolly 通常是透過將攝影機裝設於滑車上平移，並進行拍攝。在進行
Dolly 運鏡時，被拍攝者應保持不動為原則。Dolly 能進一步被分類為 Dolly In、Dolly 
Out 和 Dolly Zoom 三種類型，但是我們實作 Dolly Zoom 後發現它展現的張力太
強，在一般的記錄攝影中用途不大，故略而不用。在 Dolly In 的模式下，相機朝向
被攝體移動，同時保持其視角不變。若攝影機遠離被拍攝者，則稱之為 Dolly Out。 
 42
動中也可能僱用專業攝影師來記錄事件，而這些由人們操作攝影機的影像也是非常
重要的記錄，通常也會依拍攝者的攝影技能而呈現出具個人風格的拍攝手法。可提
昇攝製影像的整體品質。 
 
4.9.3 視訊的剪輯與合成 
冗長一鏡到底 (Long take) 的拍攝手法通常使影片單調且缺乏節奏感，這是業餘攝影
師最常犯的錯誤。根據 Bordwell 和 Thompson 說法，一個鏡頭的平均長度應保持在 10
秒左右  [Bordwell97]。因此，我們首先將異質攝影機拍攝的所有影片依 Dandan 
[Dandan10] 的影像顏色直方圖變化量鏡頭轉換檢測 (Shot Change Detection) 方法，分割
成眾多的 Video Shots，太短的 Shots 則被併到最近的影片之中。最後，再將 Video Shots
分為 Critical Shots、Mainline Shots 和 Face-retrieved (FR) Shots，說明如下。 
 
1. Critical Shot：在活動場合中由專業攝影師或者活動的主辨者手動選擇的 Critical Shots，
通常包含主要的活動內容。例如：標示此活動主題的迎賓告示、活動的旗幟、地標
性的建築物和其他事情。這些 Critical Shots 可以提醒我們這個視訊是關於什麼事和
在何時何地拍攝的。 
2. Mainline Shot：在活動中，通常會有某些攝影機配置來記錄活動的主要事件。而由這
些攝影機取得的鏡頭，則被稱為 Mainline Shots。例如，在畢業典禮中，對著講台上
攝影取到的影片就是 Mainline Shots 的一種。在其他的活動上，例如一場並行會議上
可能有多台相機在拍攝 Mainline Shots。 
3. Face-Retrieved Shot：影片的吸引力是一項剪輯時的重要考量，而無疑地參與對象的
臉是一個很重要的增強吸引力的因素。在這項工作中，我們先使用 Viola 和 Jones 
[Viola04] 的人臉偵測方法，來擷取含有人臉的影像。然後，再使用 Picasa 的人臉辨
識技術，協助將這些人臉分類。在實行中，使用 Picasa 的預設參數即可達到 80% 以
上的人臉辨識率。這個方法的好處是電腦可自動將人臉分群，因此只要為每一群臉
加上文字標識，而不需處理每一張照片的標識。當參與活動者有興趣得到他(她)自己
在活動的視訊時，可從分類好的人臉來標記他(她)自己和朋友的照片，也可以糾正錯
誤分類的結果。最後，從選好的人臉類別可選擇相對應到的 Video Shots。因為這些 
Video Shots 是由人臉線索取得，故稱為 FR Video Shots。 
 
在剪輯合成客製化視訊之前，有必要依影片品質進行 Mainline Shots 和 FR Shots 的
分級。另一方面，由於 Critical Shots 是藉由專業攝影師或者活動主辨者手動選擇的，
對於活動的主題與時、地、人、物等具有重要的提示作用，因為所有的 Critical Shots
都會被剪輯入輸出影像中，所以毋需評估 Critical Shots 的品質。 
目前有許多現成的方法可用於評估照片的品質  [Datta06, Ke06, Banerjee07, 
Cerosaletti09, Li10, Marchesotti11] 或視訊的品質 [Mei07, Luo08, Yang11]。因為攝影者非
專業攝影師，因此我們選擇由 Mei 等人 [Mei07] 提出的視訊品質評估方法。並藉由主
觀評價來測試所實現的品質評估程式。結果顯示主觀的評分和程式輸出的品質分數之間
有明確的相關性。故可確認此一影片品質評估的方法適合用於選擇視覺上吸引人的
Video Shots。 
最後則是發展演算法，利用影片品質進行排序抽選，並依時間順序剪輯合成所需之
輸出影片。 
 
五、 成果與討論 
5.1 建立地圖 
 44
 
 
 
Fig. 5.1-3 此為 Fig. 5.1-2 的 Graph，所建構出處理前（上圖）後（下圖）整體貼合
照片。 
 
Fig. 5.1-4 顯示為僅顯示兩張照片的貼合效果，由圖中可看出，因為貼合參數不準確，
照片重疊部份會變模糊，在利用我們的方法自動貼合後的結果如 Fig. 5.1-5，可以看出貼
合成果有很大的改善。同時，每個迴圈的不一致性可完全消除。只是貼合過程較花時間，
計算一對影像的貼合參數的 Hessian 矩陣與梯度向量即需約一分鐘的時間，所幸這個計
算只是在初始化階段要算一次，之後的估算則非常有效率。但是在類似Fig. 5.1-2中Graph，
因其連結的邊較多，所以初始化的計算量頗為驚人，效率上有待改進。 
 
 
 46
Fig. 5.1-6 (a) 為 Fig. 5.1-6 紅框部份放大的影像，維度：900 x 700 像素 
 
 
Fig. 5.1-6 (b) 為 Fig. 5.1-6 相同區域未執行全域貼合前的影像，維度：900 x 700 像素 
 
全域貼合範例二，Fig. 5.1-7 是由 80 張地板照片貼合而成，Fig. 5.1-7 (a)為紅色框線
的部份放大圖，Fig. 5.1-7 (b)為相同區域但是未經過全域貼合的結果。 
 
 
Fig. 5.1-7 80 張地板貼合後結果，維度：2163 x 2555 像素 
 
 
Fig. 5.1-7 (a) 為 Fig. 5.1-7 紅框部份放大的影像，維度：800 x 600 像素 
 
 
Fig. 5.1-7 (b) 為 Fig. 5.1-7 相同區域未執行全域貼合前的影像，維度：800 x 600 像素 
 
 
5.2 定位計算 
Fig. 5.2-1 是上一節中貼合範例一的定位結果，紅色線是使用投影座標推估攝影機的
位置的結果，藍色線將投影座標轉為歐式座標的 i-Pet 所在位置圖。由於攝影機是裝在
 48
 
  
位置 1 位置 2 
  
位置 3 位置 4 
  
位置 5 位置 6 
 
 
位置 7  
Fig. 5.2-3 機器人定位計算結果 
 
5.3 MSRDS i-Pet 基本控制功能 
物體辨識的功能是為了實驗與幼兒的互動遊戲，並不是要發展通用且大量的物體辨
識方法，因此我們直接採用 SIFT 作為辨識的基礎。實驗 1 和實驗 2 為以 SIFT 進行簡
單物體辨識的實驗，由於 SIFT 的穩定度不錯，對有充足紋理的物件辨識效果良好。 
 
 
實驗 1 小玩具的辨識。 
 
  
 
 
實驗 2 小枕頭的辨識。 
  
 50
4-1 4-2 4-3 
   
4-4 4-5 4-6 
   
4-7 4-8 4-9 
   
4-10 4-11 4-12 
 
實驗 5  i-Pet 在室內環境中導航
和閃避障礙物。 
   
5-1 5-2 5-3 
   
5-4 5-5 5-6 
   
5-7 5-8 5-9 
   
5-10 5-11 5-12 
 
為測試障礙物地圖建構方法，我們進行了大量的實驗，而在本報告中僅展示兩個具
代表性的實驗成果。第一個實驗環境較為單純，只有一個轉彎處。Fig. 5.3-2(a) 是障礙
物位置和機器人初始位置圖。Fig. 5.3-2(b) 是障礙物漸層圖，綠色為實驗初期、黑色為
實驗中期而紅色為實驗後期至實驗結束。Fig. 5.3-2(c) 是最後的障礙物地圖和機器人移
動路徑。Fig. 5.3-3 是實驗一環境剪影。由實驗結果可看出，i-Pet 使用超聲波聲納感測
器建立的障礙物地圖和真實環境的障礙物相符合，重疊性高。 
實驗二的環境較複雜，轉角較多，由實驗結果可觀察出，i-Pet 在初期建立的障礙
物地圖和真實環境的障礙物重疊性高。但是在經過多次的轉彎和移動之後，i-Pet 的
Odometry 資訊漸漸產生偏差，導致障礙物地圖變型。Fig. 5.3-3 是實驗環境剪影，Fig. 
5.3-4 實驗二的障礙物地圖。 
 
 52
 
(a) 環境障礙物和機
器人初始位置 
 
(b) 障礙物漸層圖 
 
(c) 完整障礙物地圖和機器人行走路徑 
Fig. 5.3-4  實驗二環境設定和結果 
 
5.4 聲音定位 
本實驗目的主要是求出聲源到兩麥克風之間的角度 θ，實驗場所為一 5x10 公尺之室
內場所，兩麥克風 M1 和 M2 之間的間距 bm 為 1 公尺，聲源和兩麥克風中心之距離 ds 為
2 公尺，θ 從-90 度至 90 度，以 15 度為一單位，對麥克風依序發出 1~5 的語音信號，Fig. 
5.4-1 為實驗之幾何關係圖。 
 
 
Fig. 5.4-1 麥克風和聲源之幾何關係圖。 
 
 
 
 
 
 
 
 
 
 
 
 
 54
符號為偵測的結果。 
在某些角度有黑色的機率分布，顯示系統偵測出 6 個訊號，原因為聲源所發出的氣
音觸發定位系統，某些氣音的訊雜比非常低，導致系統無法定位準確，通常會顯示在 0
度。因為氣音數據可由訊號過零率測得並排除，因此在後續實驗數據分析中將忽略氣音
的定位結果。 
 
Fig. 5.4-3 波達方向偵測誤差圖。 
 
Fig. 5.4-3 顯示三種方法估測到的波達方向角度的均方根誤差，由實驗結果可知我
們所提出來的方法其準確率高於 CC，但與 PHAT 相當。由於 PHAT 只能依靠最大值來
決定角度，而我們所提出的方法可以提供不同角度音源之機率分布值，有利於陣列訊號
處理，所以此方法在角度估測的彈性方面遠大於 PHAT。而由準確率的結果來比較，其
誤差隨著角度越大而越來越差，尤以 90 度較為明顯，原因可能為音速值不正確，或是
因為室內環境的回音所造成的影響，但其誤差值仍在可接受之範圍內，已經足夠用於一
般室內空間之定位。 
 
5.5 人物軌跡追蹤 
 本實驗所使用之感測器取樣頻率約為 100Hz，感測器如 Fig. 5.5-1 所示，裝置於鞋
子腳尖處以束帶固定，經由 USB 傳輸線提供電源，並透過 RS-232 傳送即時的感測器資
料至電腦端，包括三軸加速規、三軸角速度陀螺儀、三軸磁力計及時間資訊。 
 
 
Fig. 5.5-1 感測器裝置於鞋子腳尖處。 
 
實驗環境為國立暨南國際大學科技三館的三樓室內走廊，本次實驗之實際行走路徑
共約 36 公尺，行走路徑為直線行走 4 公尺後右轉，再直線行走 28 公尺右轉，最後直線
 56
靠加速規訊號或角速度陀螺儀訊號的基本處理，來做步態偵測。然而，這樣的方法在某
些情況下容易出現判別錯誤，例如緩慢行走或是隨機變速行走等情況。因此我們參考了
Jiménez 等人 [Jiménez10] 的方法，結果如 Fig. 5.5-5 所示，每個顏色的高值表示步態為站
立狀態，低值表示步態為擺動狀態。紅、緣、藍三色線段分別代表 C1、C2 與 C3 三個
條件式的成立與否，黑色線段為此三個條件結果做邏輯“AND＂運算所產生，再將此結
果經由中值濾波器後便得到桃紅色線段，即為判斷步態是否為站立狀態的結果。 
 
 
Fig. 5.5-5 步態偵測實驗： 
紅色線段為 C1 條件結果，綠色線段為 C2 條件結果，藍色線段為 C3 條件結果，黑色線
段為 C1、C2 與 C3 結果做“AND＂邏輯運算所得，桃紅色線段為黑色線段經由中值濾
波器所得。圖中步態偵測在每個顏色線段的高值為站立狀態，低值為擺動狀態。 
 
有了良好的步態偵測為基礎後，接下來為路徑軌跡追蹤方面，方法如前面章節所介
紹，實驗結果如 Fig. 5.5-6 所示。在距離估測方面，本實驗之實際行走距離約為 36 公尺
長，而估測出來的行走距離為 37.6431 公尺，誤差為 4.56%。在路徑估測方面，我們採
用角速度陀螺儀量測到之角度變換，對感測器姿態進行估測，進而移除步態處於擺動狀
態時之重力加速度，同時求得航向角的改變。在 Fig. 5.5-6 中可以看到藍色估測路徑在
直線行走及轉彎時，與紅色實際行走路徑之差距。整體結果而言，在行走路徑之軌跡追
蹤方面效果相當不錯。 
 
 58
 
Fig. 5.5-7 使用 Particle Filter 整合 GPS 於戶外/室內長距離行走路徑追蹤。 
 
為測試不同的 PTZ 攝影機控制策略成效，我們採用電腦模擬的方式，以便控制測試
環境。Fig. 5.5-8 為攝影機控制的模擬環境，而 Fig. 5.5-9 則為實驗結果，其中紅色的曲
線為場景中的目標總數，而藍色與紫色的線則為我們所提出來的兩種方法的觀測數據。
這個實驗結果顯示，我們的方法可提供最接近實際觀測人數的 PTZ 控制成效。這個 PTZ 
 60
中辨識效果都優於其他現有方法，但其中客廳場景的辨識率僅能達到六成。不過場所辨
識的主要目的是要讓機器竉物到一個新的家庭中即能約略知道每一個場所的類別，因此
這樣的辨識率已經足夠本計畫使用。 
 
Table 5.6-1. 場所辨識結果 
 
 
5.7 嬰幼兒臉型成長預測 
我們採用 FG-NET 資料庫及由網路上收集到的 Michael Jackson 及其兄弟的照片，
以測試嬰幼兒臉型成長預測之方法。同時我們也實現了 CG (Craniofacial Growth) 方法
[Ramanathan06]，做為比對的基準。 
 
 62
5.8 機器人攝影師自動構圖研究方法 
突出物偵測演算法測試裝於三角架上的相機拍攝的 4 個影像序列。在每個影像的背
景變化緩慢是主要主題。Fig. 5.8-1 記錄了 4 個影像序列的各個場景。目的是評估該演算
法在不同背景場景下和大尺寸頭飾的性能。 
 
 
(a)室內 
 
(b)有純背景的室外 
 
(c)凌亂背景的室外 
 
(d)黑人髮型 
Fig. 5.8-1 測試影像序列的各個場景 
 
而突出物偵測演算法的四個性能測試系統中的評價測試表如下： 
 
Image Frames with Protruding 
objects 
Image Frames w/o Protruding 
objects 
Detected True Positive (TP) False Positive (FP) 
Non-detected False Negative (FN) True Negative (TN) 
 
偵測率(DR)與錯誤偵測率(FDR)計算如右：
  ,
FNTP
TPDR
+
= .
TNTP
FPFDR
+
=
 
(TP+TN)是在影像偵測人臉的 Frame 數，測試這 4 個不同場景的平均偵測率和錯誤偵測
率表示為 DRall 和 FDRall。而測試四個影像的偵測率和錯誤偵測率分別從 74.00% 到
95.24% 和 0% 到 27.59%。如下表所示： 
 
Table 5.8-1. 各種場合的突出物偵測結果 
 
Indoor 
scene 
Outdoor scene with  
simple background 
Outdoor scene with  
cluttered 
background 
Subject with  
Afro hair 
DR 90.91% 74.00% 95.24% 89.47% 
FDR 17.95% 0.00% 2.90% 27.59% 
DRall 87.40% 
FDRall 12.11% 
 
在所有情況下測試，計算前景區域偵測花費近一半的執行時間，平均而言，每個 Frame
的計算時間為 0.1596 秒。時間計算於下表(系統平均計算時間：秒/Frame)： 
 
Table 5.8-2. 執行時間表 
Video Sequence Indoor 
Outdoor 
with pure 
scene 
Outdoor 
with clutter 
scene 
Subject 
with 
Afro hair 
 64
 
Fig. 5.9-1 (a) 小型朋友餐會和 (b) 研討會場地及設置攝影機的位置。 
 
小型朋友餐會現場錄製的影片經分析處理，由主辨人選擇的 Critical Shots 照片如 
Fig. 5.9-2 所示，由指定攝影機取得的 Mainstream Shots 的截圖如 Fig. 5.9-3 所示，活動
影片中的人臉分群需要一天的時間由機器自動處理，並由有興趣的人自行標識人臉。Fig. 
5.9-4 顯示幾張標識好的人臉示意圖。由 Fig. 5.9-4(a) 查詢所得的 FR Shots 顯示在 Fig. 
5.9-5 之中，這些 FR Shots 是由 PTZ 或機器人攝影機隨機選取 Zoom 運鏡 (遠景、中景、
近景)，並依拍到的人數，自動為單人選取井字構圖或為多人選取水平構圖，並盡可能
依偱 Head Room 構圖規則，所得的影片。這些影片截圖可以展示本計畫發展的自動攝
影系統的構圖與運鏡方法的良好成效。 
同樣的，由 Fig. 5.9-4 (b) 亦可查詢到如 Fig. 5.9-5 所示不同人專屬的 FR Shots，甚
至也可以同時指定多個好友人臉（如 Fig. 5.9-4 (c)之三張人臉），同時查詢朋友的 FR 
 66
 
Fig. 5.9-3 小型朋友餐會的 Mainstream Shots 中擷取的影像。 
 
 
Fig. 5.9-4 小型朋友餐會影片中擷取的人臉影像。 
 
 68
 
Fig. 5.9-6 由 Fig. 5.9-4(b) 人臉查詢到的 FR Shots 
 
 70
 在研討會現場中（如 Fig. 5.9-1 (b) 所示），除了大會於論文口頭發表會場設置的兩
台攝影機外，我們在講台側邊也佈置了兩台拍攝觀眾的 PTZ 攝影機。大會的攝影機即指
定作 Mainstream Shots 的來源。另外在茶點區與報到區我們則是佈置了兩部固定攝影機。
為了不干擾會議參與者，我們將機器人放在最偏遠的地方。不過在會議進行當天，強烈
的陽光由採光窗透入，使得 Kinect 大部份時間皆無法正常運作。因此機器人拍攝的時
間很短，大部份影片是由固定攝影機及 PTZ 攝影機所攝得。 
 研討會的 Critical Shots 截圖如 Fig. 5.9-7 所示，而其 Mainstream Shots 截圖則展
示於 Fig. 5.9-8 之中。各種影片長度統計則列於 Table 5.9-2 之中。因為影片數量多，故
人臉偵測與分群處理使用四台 PC 同時處理，共用掉了四天的時間。但是這些計算只需
處理一次，便可提供參與者使用。未免侵犯肖像權，相關的 FR Shots 無法公開。我們
情商十個本校的志願者使用此系統，協助評估本研究的成效。這十個志願者與本計畫無
關，他們在使用系統並觀賞完產出之影片後，主觀地評定：Very Interesting (5), Interesting 
(4), Fair (3), Boring (2), and Very Boring (1)。評分統計列於 Table 5.9-3 之中，其結果顯
示出大多數志願者都覺得本系統產生的視訊包含個人相關視訊，非常有意思。相關研究
成果正在整理，撰寫成論文準備投稿中。 
 
Table 5.9-2: 在研討會現場錄製的不同類型 Video Shots 長度統計 
 
 
 
 
Fig. 5.9-7 CGW Critical Shots 截圖 （人物臉孔經霧化處理）。 
 
 72
vol. 14, no. 3, pp. 76-87, 2007. 
[Banerjee07] S. Banerjee and B. L. Evans, “In-Camera Automation of Photographic 
Composition Rules,” IEEE Transactions on Image Processing, vol. 16, no. 7, pp. 
1807-1820, 2007. 
[Bellotto09] N. Bellotto, E. Sommerlade, B. Benfold, C. Bibby, I. Reid, D. Roth, C. 
Fernández, L. V. Gool, and J. Gonzàlez,, “A distributed camera system for 
multi-resolution surveillance,” in Proceedings of the Third IEEE/ACM International 
Conference on Distributed Smart Cameras, pp. 1-8, 2009. 
[Bookstein89] F. L. Bookstein, “Principal warps: Thin-plate splines and the decom-position of 
deformations,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 11, 
no. 6, pp. 567-585, 1989. 
[Bordwell97] D. Bordwell and K. Thompson, “Film Art: An Introduction,” McGraw-Hill, 
New York, 1997 
[Borestein09] J. Borestein, L. Ojeda and S. Kwanmuang, “Heuristic reduction of gyro drift in 
IMU-based personnel tracking system,” In SPIE Defense, Security and Sensing 
Conference, April 13-17, Orlando, Florida, USA, pp. 1-11, 2009 
[Brooks08] A. Brooks, A. Makarenko, and B. Upcroft, “Gaussian process models for indoor 
and outdoor sensor-centric robot localization,” IEEE Transactions on Robotics, vol. 24, 
no. 6, pp. 1341-1351, 2008. 
[Brown03] M. Brown and D. G. Lowe, “Recognising Panoramas,” in Proceedings of the 
IEEE International Conference on Computer Vision, pp. 1218-1225, 2003. 
[Brown07] M. Brown and D. G. Lowe, “Automatic Panoramic Image Stitching using 
Invariant Features,” International Journal of Computer Vision, vol. 74, no. 1, pp. 59-73, 
2007. 
[Bruce02] J. Bruce and M. M. Veloso, “Real-Time Randomized Path Planning for Robot 
Navigation,” Robot Soccer World Cup VI, pp. 288-295, 2002. 
[Byers04] Z. Byers, M. Dixon, W. D. Smart, and C. M. Grimm, “Say Cheese! Experiences 
with a robot photographer,” AI Magazine, vol. 25, no. 3, pp. 37-46, 2004. 
[Campbell05] J. Campbell and P. Pillai, “Leveraging limited autonomous mobility to frame 
attractive group photos,” in Proceedings of the IEEE International Conference on 
Robotics and Automation, pp. 3396-3401, 2005. 
[Can02] A. Can, C. V. Stewart, B. Roysam and H. Tanenbaum, “A Feature-based Technique 
for Joint, Linear Estimation of High-order Image-to-mosaic Transformations: Mosaicing 
the Curved Human Retina,” IEEE Transactions on Pattern Analysis and Machine 
Intelligence, vol. 24, pp. 412-419, 2002, 
[Carter88] G. C. Carter, “Coherence and time delay estimation,” in Signal Processing 
Handbook, C. H. Chen, Ed., pp. 443-482, 1988. 
[Castellanos99] J. A. Castellanos, J. Montiel, J. Neria, and J. D. Tardos, “The SPmap: A 
probabilistic framework for simultaneous localization and map building,” IEEE 
Transactions on Robotics and Automation, vol. 15, no. 5, pp. 948-952, 1999. 
[Cerosaletti09] C. D. Cerosaletti and A. C. Loui, “Measuring the perceived aesthetic quality 
of photographic images,” International Workshop on Quality of Multimedia Experience, 
pp. 47-52, 2009. 
[Chang09] Y.-Y. Chang and H.-T. Chen “Finding good composition in panoramic scenes,” in 
Proceedings of the International Conference on Computer Vision, 2009. 
[Chang11] K. Y. Chang, C. S. Chen, and Y. P. Hung, “Ordinal hyperplanes ranker with cost 
sensitivities for age estimation,” in Proceedings of the IEEE Computer Society 
Conference on Computer Vision and Pattern Recog-nition, pp. 585-592, 2011. 
[Chen02] J. Chen, K. Yao and R. Hudson, “Source localization and beamforming,” IEEE 
Signal Processing Magazine, vol. 19, no. 2, pp. 30-39, 2002. 
 74
image creating,” in Proceedings of the Eurographics Workshop on Rendering Techniques, 
pp. 83-88, 2001. 
[Govindu04] V. M. Govindu, “Lie-algebraic averaging for globally consistent motion 
estimation,” in Proceedings of the IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition, vol. 1, pp. 684-691,2004. 
[Gracias00] N. Gracias, and J. Santos-Victor, “Underwater Video Mosaics as Visual 
Navigation Maps,” Computer Vision and Image Understanding, vol. 79, no. 1, pp. 
66-91,2000, 
[Grauman05] K. Grauman and T. Darrell. “Pyramid match kernels: Discriminative 
classification with sets of image features”. In Proc. ICCV, 2005. 
[Gray97] A. Gray, “Modern Differential Geometry of Curves and Surfaces with Mathematica,” 
CRC Press, Inc. Boca Raton, FL, USA, 2 edition, 1997. 
[Grill90] T. Grill and M. Scanlon, “Photographics Composition,” Orlando, Amphoto Books, 
1990. 
[Guo 09] G. Guo, G. Mu, Y. Fu, and T. S. Huang, “Human age estimation using bio-inspired 
features,” in Proceedings of the IEEE Computer Society Conference on Computer Vision 
and Pattern Recognition, pp. 112-119, 2009. 
[Guo10] Y. Fu, G. Guo, and T. S. Huang, “Age synthesis and estimation via faces: A survey,” 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. 
[Hampe97] B. Hampe, “Making Documentary Films and Reality Videos: A Practical Guide to 
Planning, Filming, and Editing Documentaries of Real Events,” Holt Paperbacks, 1997. 
[Hill05] C. M. Hill, C. J. Solomon, and S. J. Gibson, “Aging the human face – a statistically 
rigorous approach,” in Proceedings of the IEE International Symposium on Imaging for 
Crime Detection and Prevention, pp. 89-94, 2005. 
[Hosoya07] M . Hosoya, “Derivation of the Equivalent Circuit of a Multi-Terminal Network 
Given by Generalization of Helmholtz-Thevenin's Theorem,” Bulletin of the College of 
Science, University of the Ryukyus, no.84, p.1 -3, 2007 
[Huang98] Q. Huang, Y. Cui, and S. Samarasekera, “Content based active video data 
acquisition via automated cameramen,” in Proceedings of the IEEE International 
Conference on Image Processing, pp. 808-812, 1998. 
[Ikeda06] S. Ikeda and J. Iura, “3D indoor environment modeling by a mobile robot with 
omnidirectional stereo and laser range finder,” in Proceedings of the IEEE/RSJ 
International Conference on Intelligent Robots and Systems, pp. 3435-3440, 2006. 
[Jiménez10] A. R. Jiménez, F. Seco, J. C. Prieto, and J. Guevara, “Indoor pedestrian 
navigation using an INS/EKF framework for yaw drift reduction and a foot-mounted 
IMU,” in Proceedings of 7th Workshop on Positioning, Navigation and Communication 
(WPNC’10), (Dresden, Germany), Mar. 2010. 
[Ji08] R. Ji, X. Xie, H. Yao, Y. Wu, and W.-Y. Ma, “Vocabulary tree incremental indexing for 
scalable location recognition,” in Proceedings of the IEEE International Conference on 
Multimedia and Exposition, pp. 869-872, 2008. 
[Jojic03] N. Jojic, B.J. Frey, and A. Kannan, “Epitome analysis of appearance and shape,” in 
Proceedings of the IEEE International Conference on Computer Vision, vol. 1, pp. 34-41, 
2003. 
[Katsura03] H. Katsura, J. Miura, M. Hild, and Y. Shirai, “A view-based outdoor navigation 
using object recognition robust to changes of weather and seasons,” in Proceedings of the 
IEEE/RSJ International Conference on Intelligent Robot and Systems, pp. 2974-2979, 
2003. 
[Ke06] Y. Ke, X. Tang and F. Jing, “The Design of High-Level Features for Photo Quality 
Assessment,” in Proceedings of the IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition, pp. 419-426, 2006. 
 76
University, 2012. 
[Madjidi05] H. Madjidi and S. Negahdaripour, “Global alignment of sensor positions with 
noisy motion measurements,” IEEE Transactions on Robotics, vol. 21, no. 6, pp. 
1092-1104, 2005. 
[Maeyama97] S. Maeyama, A. Ohya, and S. Yuta, “Long distance outdoor navigation of an 
autonomous mobile robot by playback of perceived route map,” in Proceedings of the 
International Symposium on Experimental Robotics, pp. 185-194, 1997. 
[Marchesotti11] L. Marchesotti, F. Perronnin, D. Larlus and G. Csurka, “Assessing the 
aesthetic quality of photographs using generic image descriptors,” in Proceedings of the 
IEEE International Conference on Computer Vision, pp. 1784-1791, 2011. 
[Marins01] J. Marins, X. Yun, E. Bachmann, R. McGhee, and M. Zyda, “An extended kalman 
filter for quaternion-based orientation estimation using marg sensors,” in Proceedings of 
the IEEE/RSJ International Conference on Intelligent Robots and Systems, vol. 4, pp. 
2003 -2011 vol.4, 2001. 
[Matsumoto00] Y. Matsumoto, M. Inaba, and H. Inoue, “View-based approach to robot 
navigation,” in Proceedings of the International Conference on Intelligent Robots and 
Systems, pp. 1702-1708, 2000. 
[Mei07] T. Mei, X.-S. Hua, C.-Z. Zhu, H.-Q. Zhou and S. Li, “Home Video Visual Quality 
Assessment With Spatiotemporal Factors,” IEEE Transactions on Circuits and System 
Video Technology, vol. 17, no. 6, pp. 699-706, 2007. 
[Minguez04a] J. Minguez and L. Montano, “Nearness Diagram Navigation (ND): Collision 
Avoidance in Troublesome Scenarios,” IEEE Transactions on Robotics and Automation, 
vol. 20, no. 1, pp. 45-59, 2004. 
[Minguez04b] J. Minguez, J. Osuna and L. Montano, “A "divide and conquer" strategy based 
on situations to achieve reactive collision avoidance in troublesome scenarios,” in 
Proceedings of the IEEE International Conference on Robotics & Automation New 
Orleans, vol. 4, pp. 3855-3862, 2004. 
[Murrieta-Cid02] R. Murrieta-Cid, C. Parra, and M. Devy, “Visual navigation in natural 
environments: Fromrange and color data to a landmark-based model,” Autonomous 
Robots, vol. 13, no. 2, pp. 143-168, 2002. 
[Nagatani99] K. Nagatani, H. Choset and N. Lazar, “The Arc-transversal Median Algorithm: 
an Approach to Increasing Ultrasonic Sensor Accuracy,” in Proceedings of the IEEE 
International Conference on Robotics and Automation, vol. 1, pp. 644-651, 1999. 
[Newman02] P. Newman, J. Leonard, J. Tardós, and J. Neira, “Explore and return: 
Experimental validation of real-time concurrent mapping and localization,” in 
Proceedings of the IEEE International Conference on Robotics and Automation, pp. 
1802-1809, 2002. 
[Ni08] K. Ni, A. Kannan, A. Criinisi, and J. Winn, “Epitomic location recognition,” 
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 
1-8, 2008. 
[Oliva01] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic 
representation of the spatial envelope,” International Journal of Computer Vision, vol. 42, 
no. 3, pp. 145-175, 2001. 
[OpenNI] http://openni.org/ 
[Park08] S. Y. Park, S. C. Jung, Y. S. Song, and H. J. Kim, “Mobile robot localization in 
indoor environment using scale-invariant visual landmarks,” in Proceedings of the IAPR 
Workshop on Cognitive Information Processing, pp. 159-163, 2008. 
[Park09] S. Park and S. Hashimoto, “Autonomous mobile robot navigation using passive 
RFID in indoor environment,” IEEE Transactions on Industrial Electronics, vol. 56, no. 7, 
pp. 2366-2373, 2009. 
 78
accelerometer using neural networks,” in Proceedings of the Annual International 
Conference of the IEEE Engineering in Medicine and Biology Society, pp. 3224-3227, 
2007. 
[Song08] B. Song, C. Soto, A. K. Roy-Chowdhury, and J. A. Farrell, “Decentralized camera 
network control using game theory,” in Proceedings of the Second IEEE/ACM 
International Conference on Distributed Smart Cameras, pp. 1-8, 2008. 
[Stewart03] C. V. Stewart, C. L. Tsai and B. Roysam, “The dual-bootstrap iterative closest 
point algorithm with application to retinal image registration,” IEEE Transactions on 
Medical Imaging, vol. 22, no. 11, pp. 1379-139, 2003. 
[Suo07] J. L. Suo, F. Min, S. C. Zhu, S. Shan, and X. Chen, “A multi-resolution dynamic 
model for face aging simulation,” in Proceedings of the IEEE Computer Society 
Conference on Computer Vision and Pattern Recog-nition, 2007. 
[Suo09] J. L. Suo, X. Chen, S. Shan, and W. Gao, “Learning long term face aging patterns 
from partially dense aging databases,” in Proceedings of International Conference on 
Computer Vision, 2009. 
[Suo10] J. L. Suo, S. C. Zhu, S. Shan, and X. Chen, “A compositional and dynamic model for 
face aging,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 
3, pp. 385-401, 2010. 
[Szeliski97] R. Szeliski, H.-Y. Shum, “Creating full view panoramic image mosaics and
 environment maps,” ACM SIGGRAPH, pp. 251-258, 1997. 
[Tamimi05] H. Tamimi and A. Zell, “Global robot localization using iterative scale invariant 
feature transform,” in Proceedings of the International Symposium on Robotics, 2005. 
[Tardós02] J.D. Tardós, J. Neira, P. Newman, and J. Leonard, “Robust Mapping and 
Localization in Indoor Environments using Sonar Data,” International Journal of 
Robotics Research, vol. 21, no. 4, pp. 311-330, April 2002. 
[Thrun01] S. Thrun, “Learning Occupancy Grids with Forward Models,” in Proceedings of 
the IEEE International Conference on Intelligent Robots and Systems, vol. 3, pp. 1676, 
2001. 
[Thrun98b] S. Thrun, “Finding landmarks for mobile robot navigation,” in Proceedings of the 
IEEE International Conference on Robotics and Automation, pp. 958-963, 1998. 
[Tomaszewska07] A. Tomaszewska and R. Mantiuk, “Image registration for multi-exposure 
high dynamic range image acquisition,” in Proceedings of the 15th International 
Conference on Central Europe on Computer Graphics, Visualization and Computer 
Vision, 2007 
[Torralba03] A. Torralba, K. P. Murphy, W. T. Freeman, and M. A. Rubin, “Context-based 
vision system for place and object recognition,” in Proceedings of the IEEE International 
Conference on Computer Vision,  vol. 1, pp. 273-280, 2003. 
[Ukita05] N. Ukita and T. Matsuyama, “Real-time cooperative multi-target tracking by 
communicating active vision agents,” Computer Vision and Image Understanding, vol. 97, 
no. 2, pp. 137-179, 2005. 
[Ulrich00] I. Ulrich and I. Nourbakhsh, “Appearance-based place recognition for topological 
localization,” in Proceedings of the IEEE International Conference on Robotics and 
Automation, pp. 1023-1029, 2000. 
[Unnikrishan02] R. Unnikrishan. “Globally consistent mosaicking for autonomous visual 
navigation,” Technical report, CMU-RI-TR-02-22, Robotics Institute, Carnegie Mellon 
University, 2002. 
[Varadarajan84] V. S. Varadarajan, Lie Groups, Lie Algebras, and Their Representations. New 
York: Springer-Verlag, 1984. 
[Viola04] P. Viola and M. Jones, “Robust Real-Time Object Detection,” International Journal 
of Computer Vision, vol. 57, no. 2, pp. 137-154, 2004 
 80
techniques using microphone arrays,” School of Engineering Report, no. 619, 2005. 
[Zhang05b] M. Zhang, L. Zhang, Y. Sun, L. Feng, and W. Ma, “Auto cropping for digital 
photographs,” in Proceedings of the IEEE International Conference on Multimedia and 
Expo, pp. 438-441, 2005. 
[Zhang10] Y. Zhang and D. Y. Yeung, “Multi-task warped gaussian process for personalized 
age estimation,” in Proceedings of the IEEE Computer Society Conference on Computer 
Vision and Pattern Recognition, pp. 2622-2629, 2010. 
[Zijlstra03] W. Zijlstra and A. L. Hof, “Assessment of spatio-temporal gait parameters from 
trunk accelerations during human walking,” Gait & Posture, vol. 18, no. 2, pp. 1-10, 
2003. 
[Zijlstra08] A. Zijlstra, J. H. Goosen, C. C. Verheyen, and W. Zijlstra, “A body-fixed-sensor 
based analysis of compensatory trunk movements during unconstrained walking,” Gait & 
Posture, vol. 27, no. 1, pp. 164-167, 2008. 
 
註：沈同學家境清寒，故計畫內出國費用由他支用。 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：100 年 12 月 30 日 
一、參加會議經過 
第十三屆多媒體國際會議 (IEEE International Symposium on Multimedia, ISM 2011)，為
期三天，於 2011 年 12 月 5 日到 7 日，在美國加州Dana Point 的Laguna Cliffs Marriott 
Resort & Spa 所舉行。會議主席由義大利 degli Studi di Firenze 大學 Alberto Del Bimbo 
教授、韓國 ETRI 的 Kwang-ro Park 教授及加州大學 Irvine 分校的 Phillip C.Y. Sheu 教
授共同擔任。籌備單位為加州大學 Irvine 分校，經嚴謹的審稿過程後，總共提出九十
八篇論文。在為期三天的會議中，共有十餘國專家學者與會，涵蓋了專題演講、論文
發表及海報展示等活動，分屬二十二個領域： 
 Multimedia systems, architecture, and applications 
 Multimedia networking and QoS 
 Peer-to-peer multimedia systems and streaming 
計畫編號 NSC 98－2221－E－260－022－MY3 
計畫名稱 智慧型嬰幼兒照護環境與回憶系統之研究－嬰幼兒照護環境中之智慧
型機器寵物之研究 
出國人員
姓名 沈政達 
服務機構
及職稱 暨大資訊工程學系博士候選人 
會議時間 100 年 12 月 5 日至 100 年 12 月 7 日 會議地點 迪納角, 加州, 美國 
會議名稱 
(中文)IEEE 多媒體國際會議 
(英文) IEEE  International Symposium on Multimedia 
發表論文
題目 
(中文) 基於人臉範例影像之兒童成長面貌預測 
(英文)  Exemplar-based Age Face Progression Prediction in Children Faces 
 3
位國外先進及學者相互討論彼此之研究，並在 poster session 看了許多海報，發現
不同領域之學者，切入問題的角度完全不同，很容易會有新的啟發。且看著許多國外
學者報告的方式，與台灣學生報告的方式完全不同，令學生印象相當深刻，收穫不少。
這之中也遇到許多來自台灣、中國、日本的專家學長與會，可見亞洲地區國家亦有相
當程度的研究能量投入。 
三、建議 
此次至加州 Dana Point 參加 ISM 2011 研討會有許多收穫。學生在國內也參加
過幾次研討會，發現國內外研討會有許多不同之處。使用語言是主要的不同，在國外
一切都是要使用英文對答，不論是報告或是與先進之間的討論，這是在國內研討會不
常見的景象。加強英文對答能力，不只是讀跟寫，實在是身為台灣學生亟需加強的一
部分。此次開會地點非常的偏僻，所幸會場之間，各個 session 距離都不遠，可以
輕易地參加自己感興趣的題目演講。國外學生會積極地介紹自己的作品，展現熱情，
與亞洲學生相當的不一樣，在台灣參加國內研討會，似乎較難看到如此現象，值得我
們好好省思。 
四、攜回資料名稱及內容 
攜回會議議程資料及論文光碟一片。 
五、其它 (相關照片) 
 
	

	

	
		

		
	


	

	

	 !
	 !
Figure 1. Flowchart for age progression prediction.
II. RELATED WORK
As human faces grow across ages, facial aging effects
are mainly attributed to the growth of craniofacial and
skin related deformations associated with the introduction
of wrinkles and reduction of muscle strength [9, 10]. In
particular, the growth of craniofacial (human face shape)
is the most obvious from birth to adulthood. Including
the growth of the face size, eyes, nose, mouth and skull,
and the augmentation of the chin and cheek, each type of
growth affects the facial appearance. Although there are
many changes in facial skin over time, the skin change is
more subtle in proportion to the growth of the craniofacial,
such as pores thick, skin color changes, and even mustache
growth.
Existing FAPP methods can be roughly classiﬁed into
model-based approaches and subspace-based approaches.
Detailed surveys of existing FAPP methods can be found in
[11, 12]. Model-based methods estimate facial aging with
a face model controlled by a high-dimensional parameter
vector. Ramanathan and Chellappa [6] proposed a craniofa-
cial growth model to predict the progression of the human
face under the age of eighteen years old. They employed
facial landmarks and anthropometric deﬁned by Farkas [13]
and ‘revised’ cardioidal strain transformation proposed by
Todd et al. [14] to obtain human face synthesizing results
with optimal growth parameters. However, their method
can only handle one input image. The method to integrate
the information of multiple images is unknown. Suo et al.
proposed a multi-resolution dynamic model [15–17] which
categorizes face images in the same age group by using a
hierarchical and-or graph. The and-or graph accounts for
the large variations of the facial structures. Suo et al. [15]
models the aging procedure as a dynamic Markov process to
predict adult aging faces. This method provides satisfactory
results for predicting adult aging, but is not able to handle
the FAPP problem for children.
Subspace-based methods project data into a subspace
to model the variation of the principle components for
reconstructing/predicting human faces [5, 7, 18]. Lanitis et
al. [5] built a statistical face model and extracted changing
components of facial shapes and intensity from training data
by using principal component analysis (PCA). Then, the fa-
cial parameters in the proposed aging function are optimized

Figure 2. 3-D face model. (a) A 2-D face image. (b) The converted 3-
D face model without adjusting. (c) The converted 3-D face model after
adjusting to have a neutral expression.
by using the genetic algorithm (GA). The proposed aging
function can not only estimate the age of the child facial
image but also reconstruct the appearance of the human face
at any age. The subspace-based method is also employed
by Geng et al. [7] for reconstructing human faces and
estimating ages. They deﬁned an aging pattern as a sequence
of personal face images sorted in time order and construct a
subspace by using PCA. The constructed subspace captures
the main aging variation in the data set. Since the aging
pattern vector is incomplete, they used an EM-like algorithm
to learn a representative subspace. Then, an unseen input
face image was projected into the subspace and an optimal
aging pattern minimizing the error between the input image
and the reconstructed image was chosen to represent the
aging pattern of this input face image.
III. PROPOSED METHOD FOR PREDICTING AGE
PROGRESSION
The ﬂowchart of the proposed algorithm is shown in
Fig. 1. Suppose that there is an aging database containing
face images of I subjects at different ages. The aging
database does not have to include all-age images for each
subject. Suppose that each image in the database has been
converted into a 3-D face model using a commercialized
software package such as FaceGen [19]. Furthermore, for
simplifying the subsequent computation, all the generated
3-D faces have been adjusted to have neutral expressions as
shown in Fig. 2.
For predicting the progression appearance of a child, a se-
ries of age-progressed images of the child are required. The
input images are also converted into expression-neutralized
3-D face meshes. The main idea of our method is to
ﬁnd subjects in the aging database who have similar facial
124
for c ∈ {e, n,m}. The curvature-weighted distance works
well for computing the distances between the meshes of the
eye, the nose, and the mouth. However, when comparing
two craniofacials which do not include the regions of eye,
nose and mouth, the remaining surface is very smooth and
the curvature values are very low. Therefore, the curvature-
weighted distance is not suitable to compare the dissimilarity
of two face shapes and, hence, the bending energy [21] is
used instead to compute the distance between two face shape
components, Mf,a,i and Mf,a,j , i.e., DB (vf,a,i,vf,a,j).
Furthermore, because the goal of this work is to synthesize
a frontal view of the missing child, the contour of the
face dominates the face shape in the synthesize face image.
Therefore, we only compute the bending energy of the face
contours as a measure of the dissimilarity of two face shapes.
The dissimilarity of the growth curves Lc,i =
{lc,a,i |a ∈ A
′ } and Ψc,i = {ψc,a,i |a ∈ A
′ }, where A′ ⊂
A, are deﬁned as follows.
D(Lc,i,Lc,j) =
∑
a∈A′
‖lc,a,i − lc,a,j‖ , (4)
D(Ψc,i,Ψc,j) =
∑
a∈A′
‖ψc,a,i − ψc,a,j‖ . (5)
C. Selection of Similar Facial Components
Recall that a facial component c is described by three
parameters lc,a,0, ψc,a,0, and Mc,a,0. Since the subsequent
method for processing all the parameters of all the facial
components are all the same, for simplicity, we will use
pa,i to represent every parameter of a facial component c ∈
C, i.e., pa,i ∈ gc,a,i. By using the aforementioned distance
functions, we can select the K closest parameters of pa,0
from the aging database of the same age group. Let S denote
the subject indices of the K closest components of pa,0.
D. Facial Component Synthesis
A heuristic equation for predicting parameter paˆ,0 is given
by
paˆ,0 
∑
a∈A′
(
ωa,aˆ
∑
k∈S
ωa,k paˆ,k
)
, (6)
where ωa,aˆ and ωa,k are two kinds of unit-sum weighting
factors which are proportional to e−λ|a−aˆ| (λ is an empiri-
cally determined constant) and e−D(pa,i,pa,j), respectively.
By combining the predicted parameters computed using
equation (6), we have each component of the facial features,
gc,a,i = {lc,a,i, ψc,a,i,Mc,a,i} for c ∈ C.
E. Composition of a Face Image Using the Synthesized
Components
The facial components computed using the method de-
scribed in the previous subsection may not be consistent
because the locations and sizes of the eye, nose and mouth
may have been changed. Therefore, the face mesh has to
be adjusted according to the new locations and sizes of
(a) (b)
Figure 4. (a) Query input. (b) Components selected with the CW+BE
distance. The blue (red) rectangle indicates the two most similar (dissimilar)
facial features extracted from the aging database.
the facial components using the thin plate spline (TPS)
method [21]. The contour points of the eye, nose, mouth and
face are used as the control points while the other vertices
of the face mesh are adjusted. Finally, the texture image of
the child at age max(A′) is mapped onto the 3-D mesh to
generate a frontal face image.
IV. EXPERIMENTS
The FG-NET aging database [4] is used to evaluate the
proposed method. The database is a publicly available image
database including 1,002 face images of 82 subjects at
different ages. Only subjects having at least one photo in
each age group are selected to construct our aging database.
Therefore, the aging database contains 125 photos of 25
subjects. In the following experiments, the parameter K is
set to three.
The ﬁrst experiment is to verify the effectiveness of
both the feature selection procedure and the face synthesis
procedure. Figure 4 shows the results of facial feature
selection with the proposed distance measure. The two most
similar and the two most dissimilar components are enclosed
in blue and red rectangles, respectively. The results of the
CW+BE distance are satisfactory.
Before we show that the proposed method can be used to
predict the appearance of a child at different age, it has to
be veriﬁed that the face synthesis procedure can reproduce
a face similar to the original face of the same age (i.e., aˆ ∈
A′) using the selected similar facial components. The faces
synthesized using the most similar and the most dissimilarK
features/parameters are shown in Table I. The results show
that faces synthesized using similar components are similar
to the original ones whereas faces synthesized using the most
dissimilar components are quite different from the original
ones.
In the second experiment, the goal is to test the perfor-
mance of the proposed method in predicting the progression
appearance of a child, i.e., aˆ /∈ A′, with a single input image
so as to compare the results with those computed in [6]. The
method proposed by Ramanathan and Chellappa can only
accept one input image. Table II shows the prediction results
126
Table III
PREDICTION RESULTS WITH TWO INPUT IMAGES.
"1230

	
	
 *
) %
. ,
')
')
')
	
	 
#

')+'*
')+'*
')+'*
Table IV
PREDICTION RESULTS WITH THREE INPUT IMAGES.
"1230
'(
'*
'%
	
	 
#

,
, -

	
	
'%+'(
'%+'(
')+'*
.

' % /
-
Faces. Lawrence Erlbaum Associates, Hillsdale, New
Jersey, 1988.
[4] T. Cootes. FG-NET aging database. http://www.fgnet.
rsunit.com.
[5] A. Lanitis, C. J. Taylor, and T. F. Cootes. Toward
automatic simulation of aging effects on face images.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 24(4):442–455, 2002.
[6] N. Ramanathan and R. Chellappa. Modeling age
progression in young faces. In Proceedings of the IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition, pages 387–394, 2006.
[7] X. Geng, Z. H. Zhou, and K. S. Miles. Automatic age
estimation based on facial aging patterns. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
29(12):2234–2240, 2007.
[8] X. Geng, Z. H. Zhou, and K. S. Miles. Correction
to “automatic age estimation based on facial aging
patterns”. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 30(2):368, 2008.
[9] A. K. Albert, K. Ricanek, and E. Patterson. A review of
the literature on the aging adult skull and face: Impli-
cations for forensic science research and applications.
Forensic Science International, 172(1):1–9, 2007.
[10] M. G. Rhodes. Age estimation of faces: A review.
Applied Cognitive Psychology, 23:1–12, 2009.
[11] N. Ramanathan, R. Chellappa, and S. Biswas. Compu-
tational methods for modeling facial aging: A survey.
J. Vis. Lang. Comput., 20(3):131–144, 2009.
[12] Y. Fu, G. Guo, and T. S. Huang. Age synthesis
and estimation via faces: A survey. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 99
(PrePrints), 2010.
[13] L. G. Farkas. Anthropometry of the Head and Face.
Raven Press, New York, 1994.
[14] J. T. Todd, S. M. Leonard, R. E. Shaw, and J. B.
Pittenger. The perception of human growth. Scientiﬁc
American, 242(2):106–114, 1980.
[15] J. L. Suo, F. Min, S. C. Zhu, S. Shan, and X. Chen.
A multi-resolution dynamic model for face aging sim-
ulation. In Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recogni-
tion, 2007.
[16] J. L. Suo, X. Chen, S. Shan, and W. Gao. Learning long
term face aging patterns from partially dense aging
databases. In Proceedings of International Conference
on Computer Vision, 2009.
[17] J. L. Suo, S. C. Zhu, S. Shan, and X. Chen. A
compositional and dynamic model for face aging. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 32(3):385–401, 2010.
[18] C. M. Hill, C. J. Solomon, and S. J. Gibson. Aging
the human face - a statistically rigorous approach. In
Proceedings of the IEE International Symposium on
Imaging for Crime Detection and Prevention, pages
89–94, 2005.
[19] FaceGen. http://www.facegen.com/.
[20] A. Gray. Modern Differential Geometry of Curves and
Surfaces with Mathematica. CRC Press, Inc. Boca
Raton, FL, USA, 2 edition, 1997.
[21] F. L. Bookstein. Principal warps: Thin-plate splines and
the decomposition of deformations. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 11(6):
567–585, 1989.
128
98年度專題研究計畫研究成果彙整表 
計畫主持人：石勝文 計畫編號：98-2221-E-260-022-MY3 
計畫名稱：智慧型嬰幼兒照護環境與回憶系統之研究--嬰幼兒照護環境中之智慧型機器寵物之研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 2 1 50%  
研究報告/技術報告 0 0 100%  
研討會論文 3 3 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 17 9 50%  
博士生 3 3 20%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 4 3 70%  
研究報告/技術報告 0 0 100%  
研討會論文 4 3 60% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 ■申請中 □無 
技轉：□已技轉 ■洽談中 □無 
其他：（以 100字為限） 
在本計畫執行 3年期中，共發表了 6 篇期刊論文與 5篇研討會論文，詳列在期末報告中的
最後一頁。其中最有趣的是在利用機器寵物拍攝人物照片時，發現在聚焦不正確時，人物眼
睛會出現與光圈形狀類似的光點。我們分析並利用此現象，研發了關於虹膜快速對焦的技術，
並獲 IEEE T. SMC-B 接受，此技術目前正在申請國內專利中，也有廠商正與我們冾談技轉中。
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
本子計畫的主要任務是負責研發機器寵物的地圖建模與定位、導航、場所辨識 (Location 
Category Recognition)、聽音辨位、人物軌跡追蹤、嬰幼兒臉型成長預測、自動照像與
攝影、以及影片剪輯等功能。在本研究中，我們已完成了原定的進度。在地圖建模的工作
上，我們提出並實作了使用 Homography 模型與 Lie Group 理論的貼合方法，並發展地圖
貼合的化簡法，這個貼合法類似電路理論中的 Thevenin/Norton 等效電路，可以降低貼
合大範圍地圖時的計算量。我們也完成了利用地圖來做定位計算的方法，目前此方法能在
限定範圍的地圖中搜尋其目前 i-Pet 所在之拍攝的地圖照片。另外，我們也利用 MSRDS 撰
寫程式並順利的操作 i-Pet，進行障礙物偵測與導航，根據給定終點規劃路徑，自動的迴
避障礙物走到目的地。在場所辨識問題上，我們使用大量影像資料庫，密集計算 Sift 
Descriptors來當作其影像的特徵，並使用 Sparse coding 的技術化簡特徵以利辨識，最
後完成的場所辨識率平均表現在 70%以上，可讓機器竉物到一個新的家庭中即能約略知道
每一個場所的類別。在聲音定位方面，我們發展了一套理論可以描述訊號時間延遲的機率
分佈。基於這個機率分佈，我們進一步發展出時間延遲量的 ML 估測器，並以 GPU 加速達
到即時運算效能，以在看不到嬰幼兒但聽到聲響時快速定位幼兒位置。這個方法與
Generalized Hough Transform 一樣，可適用於偵測頻率-相位平面上的直線，更可供多
