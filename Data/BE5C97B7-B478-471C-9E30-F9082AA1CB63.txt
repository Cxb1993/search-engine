TABLE I
RESEARCH WORKS ON THE ANALYSIS OF A FAULT TOLERANT NN.
Ref. Fault NN Work
[48] Any weight noise Madaline Probility of output error
[15] Any noise Any Output sensitivity measure
[43] Any noise Madaline Precision requirement
[10] Mul. weight noise RBF Generalizaton ability
[54] Any noise RBF Output sensitivity matrix
[2] Any weight noise MLP Output sensitivity measure
[41] - - Relationship between FT,
generalization and VC dim.
[4] Any weight noise MLP Generalization ability
[19] Any weight noise FNa Error sensitivity measure
a Functional net
sensitivity approach to derive different output sensitivity mea-
sures of a network due to different type of noise. Consider
a Madaline is with sigmoidal output neuron, Piche in [43]
followed an approach from signal to noise ratio (SNR) and
came up with a set of measures for the output sensitivity
of a network with respect to different noises. Using such
SNR, a weight accuracy selection algorithm is developed and
applied to determine the precision requirement in hardware
implementation. Townsend and Tarassenko [54] considered a
radial basis function (RBF) network with multiple outputs and
derived the output sensitivity in matrix form for an RBF that is
suffered from perturbations in input data, radial basis function
centers and output weights.
As output sensitivity is just an indirect view point to
understand the effect of NN due to noise, the actual effect
of noise to the performance of a NN cannot be identified
easily. A more practical view point to the problem is from its
actual performance — the generalization ability. Catala and
Parra proposed a fault tolerance parameter model and studied
the performance degradation of a RBF network if the RBF
centers, widths and the corresponding weights are corrupted
by multiplicative noise [10]. Bernier et al extended from Choi
& Choi statistical sensitivity approach [15] and derived the
error sensitivity measure for MLP [2], [4], RBF network [6]
Similarly, Fontenla-Romero et al derived the error sensitivity
measure for functional net [19].
Noise can be harmful to a NN. But sometimes, it can be
beneficial. Murray & Edwards [36] investigated and found
that adding multiplicative weight noise (and other kinds of
noise) during training can improve the generalization ability
of a MLP. While noise during training can improve the
generalization ability of a NN, Bishop [7] showed that adding
small additive white noise to a NN during training is equiv-
alent to Tikhnov regularization. Jim et al [24] noticed that
adding multiplicative weight noise not just can improve the
generalization ability, but also can improve the convergence
ability in training a recurrent NN.
B. Algorithms for dealing with multiplicative weight noise
While lot of works have been done to understand the effect
of noise to the network performance, various training methods
aiming to improve the fault tolerant ability of a NN have been
developed. Since the effect of a multiplicative weight noise
is proportional to the magnitude of the associated weight,
one intuitive approach is to control the magnitude of the
weights to small values. Cavalieri & Mirabella in [11] have
proposed a modified backpropagation learning algorithm for
multilayer perceptron. In their algorithm, a weight magnitude
control step has been added in each training epoch. Whenever
the magnitude of a weight has reached a predefined upper
limit, it will not be updated unless the update can bring its
magnitude down. Consider that the noise effect can eventually
be cancelled out at the output node if all the weight values
are equal, Simon in [47] suggested a distributed fault tolerance
learning approach for optimal interpolation net and formulated
the learning as a nonlinear programming problem, in which
training error is minimized subjected to an equality constraint
on weight magnitude. Extended from the work done in [10],
Parra and Catala in [38] demonstrated how a fault tolerant RBF
network can be obtained by using a weight decay regularizer
[33]. From model sensitivity point of view, Bernier et al
developed a method called explicit regularization to attain
a MLP [3], [5] or RBF network [6] that is able to tolerate
multiplicative weight noise.
C. Algorithms for dealing with node fault
To deal with node fault, those learning algorithms developed
can be classified into two approaches : (1) adding heuristics
(random fault or network redundancy) during training and (2)
formulating the training as a nonlinear optimization problem.
Adding heuristic in the training algorithm is essentially to
enforce the internal representation ability of a NN distributed
widely within the hidden nodes or weights. So that, no single
node or single weight is particularly important and then
random removal of a node or a weight will only gracefully
degrade the performance of the network. For this approach,
injecting random node fault alone [45], [9] or together with
random node deletion and addition [14] during training are
two techniques that have demonstrated succeed in attaining
fault tolerance. Adding network redundancy by replicating
multiple hidden layers after a NN has been well trained [18],
[40] is another one. Under the same scenario, limit weight
magnitude either by adding weight decay regularizer [14] or
hard bounding the weight magnitude to a small value during
training [11] are another two techniques that can succeed in
obtaining a fault tolerant NN.
Another approach is to formulate the learning directly as
a constraint optimization problem. Neti et al [37] defined
the problem as a minimax problem, in which the objective
function to be minimized is the maximum of the mean square
errors over all fault models. Deodhare et al took a similar idea
in [16] by defining the objective function to be minimized
as the maximum square error, over all fault models and all
training samples. As the computational cost in solving these
minmax problem could be severe for large number of hidden
units, Simon & El-Sherief in [46] and Phatak & Tcherner in
[42] formulated the learning problem to a simpler unconstraint
optimization problem, in which the objective function consists
of two terms namely the mean square errors of the fault-
free model and the ensemble average of the mean square
errors over all fault models. Although solving unconstraint
optimization problem is a lot more easy compared with a min-
imax problem, these formulations are still suffered from sever
computational burden when their formulations are extended
to handling multiple nodes fault. In view of the lacking of
a theoretical framework and the difficulty in extending the
existing approaches to multiple nodes fault, Leung et al in
For k = 1, 2, · · · , N
M0 : yk = f(xk) + ek, (5)
where (xk, yk) is the kth input-output pair that is measured
from an unknown deterministic system f(x) with random
output noise ek. To model the unknown system, we assume
that f(x) can be realized by an RBF network, i.e.
M : yk =
M∑
i=1
θiφi(xk) + ek (6)
ek ∼ N (0, Se), (7)
for all k = 1, 2, · · · , N . Se is known in advance, a model M
in Ω can indeed be represented by an M -vector,
θ = (θ1, θ2, · · · , θM )T .
The model set Ω is isomorphic to an M -dimension Euclidean
space, RM .
The best estimated model Mˆ is thus represented θˆ. Equa-
tion (1) is rewritten as follows :
θˆ = arg min
θ∈RM
{J(θ|D)} . (8)
Here J(θ|D) can be defined in one of the following forms.
1) Sum Square Errors (SSE) :
J(θ|D) = 1
N
N∑
k=1
(yk − f(xk, θ))2. (9)
2) SSE with Regularizer (Weight Decay) :
J(θ|D) = 1
N
N∑
k=1
(yk − f(xk, θ))2 + λθT θ, (λ > 0).
(10)
3) Likelihood Probability :
J(θ|D) = −P (D|θ). (11)
4) Log Likelihood :
J(θ|D) = − logP (D|θ). (12)
5) A Posterior Probability :
J(θ|D) = −P (D|θ)P (θ)
P (D) . (13)
6) Log A Posterior Probability :
J(θ|D) = − logP (D|θ)− logP (θ). (14)
The probability P (θ) which appears in Equation (13) and
Equation (14) is the A Prior distribution of θ.
The best implemented model MˆI is thus represented θˆI .
Equation (2) can be rewritten as follows :
L(θ|D) =
∫
θ˜∈Ω˜θ
J(θ˜|D)P (θ˜|θ)dθ˜ (15)
θˆI = arg min
θ∈RM
{L(θ|D)} . (16)
The integration is taken over the RM space. The probability
P (θ˜|θ) is depended on the fault model concerned. Note that
this probability is not the same as the A Prior probability P (θ).
If there are only finite number of possible faulty models, the
objective function defined in Equation (15) would be given by
L(θ|D) =
∑
θ˜∈Ω˜θ
J(θ˜|D)P (θ˜|θ)dθ˜. (17)
The set of faulty models is depended on the estimated model
θ.
One should note that the best estimated model (i.e. the fault-
free model) obtained either by Equation (11) or Equation (12)
are the same because
arg min
θ∈RM
{−P (D|θ)} = arg min
θ∈RM
{− logP (D|θ)} .
However, for fault tolerant cases, there will have no such
guarantee that
arg min
θ∈RM
{
−
∫
P (D|θ˜)P (θ˜|θ)dθ˜
}
is the same as
arg min
θ∈RM
{∫ (
− logP (D|θ˜)
)
P (θ˜|θ)dθ˜
}
.
The same reason applies to Equation (13) and Equation (14).
Apart from defining an RBF network as in Equation (7),
one can also define the estimated model in other forms. For
instance,
yk = θ0 +
M∑
i=1
θiφi(xk) + ek, (18)
ek ∼ N (0, Se), (19)
for k = 1, 2, · · · , N . For a given Se, the estimated model set
will be isomorphic to the RM+1 space.
If we assume that the values of cis and σ in the M
basis functions are not predefined, an RBF model will be
parameterized by an (2M + 2)-vector,
(θ0, θ1, · · · , θM , c1, c2, · · · , cM , σ)
The estimated model set Ω will thus be isomorphic to the
R2M+2 space.
V. IMPLEMENTED MODELS Ω˜M
Recall that an implemented model of M is a model, in
which part of its structure is faulty. In this section, three typical
fault models will be introduced including (1) the multiplicative
weight noise (2) single-node fault and (3) multiple-nodes fault.
Similarly, we use RBF network as an example for illustration.
A. Multiplicative weight noise with J(θ|D) = SSE
Mulplicative weight noise exists whenever a weight is
encoded in a low precision binary form. In order not to divert
the focus of this section, the exaplination of this effect is
presented in Appendix A.
Using the model described in Equation (55), an implemen-
tation of a model θ (denoted by θ˜) can be defined as follows :
θ˜i = θi + βi θi, (20)
βi ∼ N (0, Sβ), (21)
Faulty Node
 
 













Normal Node
Fig. 3. Single-node fault NN models. For a network of M hidden nodes,
there are M possible single-node fault models.
By using the idea of gradient descent, a training algorithm
can thus be derived. Taking the gradients of the 2nd and the
3rd terms in Equation (37), it is readily obtained
∂
∂θ
logS(xk, θ) =
2Sβ
S(xk, θ)
G(xk)θ, (39)
∂
∂θ
(yk − φT (xk)θ)2
S(xk, θ)
= −2Sβ(yk − φ
T (xk)θ)2
S2(xk, θ)
G(xk)θ
−2(yk − φ
T (xk)θ)
S(xk, θ)
φ(xk), (40)
where G(xk) is a diagonal matrix defined as in Equation (26).
A fault tolerant RBF network can thus be obtained by the
following gradient descent algorithm :
θ(t+ 1) = θ(t)− µ ∂
∂θ
L(θ(t)|D), (41)
where µ is a small positive value corresponding to the step
size and
∂L(θ|D)
∂θ
=
Sβ
N
N∑
k=1
(
1
S(xk, θ)
− (yk − φ
T (xk)θ)
2
S2(xk, θ)
)
G(xk)θ
− 1
N
N∑
k=1
(yk − φT (xk)θ)
S(xk, θ)
φ(xk). (42)
The initial condition θ(0) is set to be a small random vector
close to null.
C. Single node fault with J(θ|D) = SSE
Once a node has been faulty, we assume that its output will
be stuck at zero. Therefore, an RBF network with its ith node
being faulty will be denoted by an M -vector θ−i, which is
identical to θ except that the ith element is zero.
θ−i = (θ1, θ2, · · · , θi−1, 0, θi+1, · · · , θM )T
Assume that there is at most one node will be removed
randomly. The probability that a network will be faulty is q.
Once a network is faulty, there is uniformly random for any
one of the node is fault, Figure 3. Under such circumstance,
Ω = RM , (43)
Ω˜θ = {θ, θ−1, θ−2, · · · , θ−M}. (44)
A node will be fault is about q/M probability.
P (θ˜|θ) =

1− q if θ˜ = θ
q/M if θ˜ = θ−1
.
.
.
.
.
.
q/M if θ˜ = θ−M .
(45)
For J(θ|D) is defined as the sum square errors,
L(θ|D) = (1− q)J(θ|D) + q
M
M∑
i=1
J(θ−i|D). (46)
In which,
J(θ−i|D) = J(θ|D) + θ2i gi
+ 2θi
1
N
N∑
k=1
(yk − φT (xk)θ)φi(xk) (47)
where gi is the ith diagonal element of Qg . Hence, the
objective function for attaining a RBF network to tolerate
single node fault can be written as follows :
L(θ|D) = J(θ|D) + 2q
M
1
N
N∑
k=1
ykφ
T (xk)θ
+
q
M
θT [Qg − 2Hφ]θ. (48)
Taking the derivative of L(θ|D) and setting it to zero, θˆI can
be obtained as follows :
θˆI =
(
Hφ +
q/M
1− q/M Qg
)−1 1
N
N∑
k=1
ykφ(xk). (49)
The matrix q/M1−q/MQg which appears in the last equation plays
a role similar to a regularizer.
D. Multiple nodes fault with J(θ|D) = SSE
We assume that a node fault is equivalent to permanently set
the output of the node zero. Therefore, a faulty RBF fˆ(x, θ˜),
where θ˜ = (θ˜1, θ˜2, · · · , θ˜M )T and
θ˜i = βiθi, (50)
could be defined by multiplying each φi(x) by a random
binary variable βi :
f(x, θ, β) =
M∑
i=1
βiθiφi(x). (51)
When βi = 1, the ith node is normal. When βi = 0, the ith
node is fault. We assume that all nodes are of equal fault rate
p, i.e.
P (βi) =
{
p if βi = 0
1− p if βi = 1. (52)
for i = 1, 2, · · · ,M and β1, · · · , βM are independent random
variables.
[15] Choi J.Y. and C.H. Choi, Sensitivity of multilayer perceptrons with dif-
ferentiable activation functions, IEEE Transactions on Neural Networks,
Vol. 3, 101-107, 1992.
[16] Deodhare D., M. Vidyasagar and S. Sathiya Keerthi, Sythesis of fault-
tolerant feedforward neural networks using minimax optimization, IEEE
Transactions on Neural Networks, Vol.9(5), 891-900, 1998.
[17] Eickhoff R. and U. Riickett, Tolerance of radial basis functions against
stuck-at-faults, Proc. ICANN 2005, LNCS 3697 Springer, 1003-1008,
2005.
[18] Emmerson M.D. and R.I. Damper, Determining and improving the fault
tolerance of multilayer perceptrons in a pattern-recognition application,
IEEE Transactions on Neural Networks, Vol.4, 788-793, 1993.
[19] Fontenla-Romero O. et al, A measure of fault tolerance for functional
networks, Neurocomputing, Vol.62, 327-347, 2004.
[20] Hassibi B and D.G. Stork (1993). Second order derivatives for network
pruning: Optimal brain surgeon. In Hanson et al. (eds) Advances in
Neural Information Processing Systems, 164-171.
[21] T. Hastie, R. Tibshirani, J. H. Friedman The Elements of Statistical
Learning, Springer, 2001.
[22] S. Haykin, Neural networks: A comprehensive foundation, Macmillan
College Publishing Company, Inc. 1994.
[23] Holt J. and J. Hwang, Finite precision error analysis of neural networks
hardware implementation, IEEE Transactions on Computers, Vol.42,
p.281-290, 1993.
[24] Jim K.C., C.L. Giles and B.G. Horne, An analysis of noise in recurrent
neural networks: Convergence and generalization, IEEE Transactions on
Neural Networks, Vol.7, 1424-1438, 1996.
[25] LeCun Y. et al. (1990). Optimal brain damage, Advances in Neural
Information Processing Systems 2 (D.S. Touretsky, ed.) 396-404.
[26] Kullback S., Information Theory and Statistics, Wiley, 1959.
[27] Chi-sing Leung, Kwok-wo Wong, Pui-fai Sum and Lai-wan Chan, A
pruning method for recursive least squared algorithm, Neural Networks,
14:147-174, 2001.
[28] Leung C.S., G.H. Young, J. Sum and W.K. Kan, On the regularization of
forgetting recursive least square, IEEE Transactions on Neural Networks,
Vol.10, 1842-1846, 1999.
[29] C.S.Leung, K.W.Wong, John Sum, and L.W.Chan, On-line training and
pruning for RLS algorithms, Electronics Letters, Vol.32, No.23, 2152-
2153, 1996.
[30] Chi-sing Leung and John Sum, Fault tolerant regularizer for multiple
nodes fault RBF, accepted for publication in IEEE Transactions on
Neural Networks.
[31] Mackay D.J.C. (1992), A Practical Bayesian Framework for Backprop
Networks, Neural Computation, Vol.4(3) 448-472.
[32] Merchant S, G.D. Peterson, S.K. Park and S.G. Kong, FPGA implemen-
tation of evolvable block-based neural networks, Proc. IEEE Congress
on Evolutionary Computation, Vancouver Canada, p.3129-3136, 2006.
[33] Moody J.E., Note on generalization, regularization, and architecture
selection in nonlinear learning systems, First IEEE-SP Workshop on
Neural Networks for Signal Processing, 1991.
[34] Murata N., S. Yoshizawa and S. Amari. Network information criterion–
Determining the number of hidden units for an artificial neural network
model, IEEE Transactions on Neural Networks, Vol.5(6), pp.865-872,
1994.
[35] Murray A.F. and P.J. Edwards, Synaptic weight noise during multilayer
perceptron training: fault tolerance and training improvements, IEEE
Transactions on Neural Networks, Vol.4(4), 722-725, 1993.
[36] Murray A.F. and P.J. Edwards, Enhanced MLP performance and fault
tolerance resulting from synaptic weight noise during training, IEEE
Transactions on Neural Networks, Vol.5(5), 792-802, 1994.
[37] Neti C. M.H. Schneider and E.D. Young, Maximally fault tolerance
neural networks, IEEE Transactions on Neural Networks, Vol.3(1), 14-
23, 1992.
[38] Parra X. and A. Catala, Fault tolerance in the learning algorithm of radial
basis function networks, Proc. IJCNN 2000, Vol.3, 527-532, 2000.
[39] Pedersen M.W., L.K. Hansen and J. Larsen. Pruning with generaliza-
tion based weight saliencies: γOBD, γOBS. Advances in Information
Processing Systems 8 521-528, 1996.
[40] Phatak D.S. and I. Koren, Complete and partial fault tolerance of
feedforward neural nets., IEEE Transactions on Neural Networks, Vol.6,
446-456, 1995.
[41] Phatak D.S., Relationship between fault tolerance, generalization and the
Vapnik-Cervonenkis (VC) dimension of feedforward ANNs, IJCNN’99,
Vol.1, 705-709, 1999.
[42] Phatak D.S. and E. Tcherner, Synthesis of fault tolerance neural net-
works, Proc. IJCNN’02, 1475-1480, 2002.
[43] Piche S.W., The selection of weight accuracies for Madalines, IEEE
Transactions on Neural Networks, Vol.6, 432-445, 1995.
[44] Reed R., Pruning algorithms – A survey, IEEE Transactions on Neural
Networks, Vol.4(5), 740-747, 1993.
[45] Sequin C.H. and R.D. Clay, Fault tolerance in feedforward artificial
neural networks, Neural Networks, Vol.4, 111-141, 1991.
[46] Simon D. and H. El-Sherief, Fault-tolerance training for optimal inter-
polative nets, IEEE Transactions on Neural Networks, Vol.6, 1531-1535,
1995.
[47] Simon D., Distributed fault tolerance in optimal interpolative nets, IEEE
Transactions on Neural Networks, Vol.12(6), 1348-1357, 2001.
[48] Stevenson M., R. Winter and B. Widrow, Sensitivity of feedfoward
neural networks to weight errors, IEEE Transactions on Neural Net-
works, Vol.1, 71-80, 1990.
[49] John Sum, Chi-sing Leung and Kevin Ho, On objective function,
regularizer and prediction error of a learning algorithm for dealing
with multiplicative weight noise, in submission to IEEE Transactions
on Neural Networks.
[50] John Sum and Chi-sing Leung, Derivation of explicit regularization from
KL divergence, Neural Processing Letters (In revision).
[51] John Sum, On a multiple nodes fault tolerant training for RBF: Objective
function, sensitivity analysis and relation to generalization, Proceedings
of TAAI’05, Tainan, ROC, 2005.
[52] Elko B. Tchernev, Rory G. Mulvaney, and Dhananjay S. Phatak, Inves-
tigating the Fault Tolerance of Neural Networks, Neural Computation,
Vol.17, 1646-1664, 2005.
[53] Elko B. Tchernev, Rory G. Mulvaney, and Dhananjay S. Phatak, Perfect
fault tolerance of the n-k-n network, Neural Computation, Vol.17, 1911-
1920, 2005.
[54] Townsend N.W. and L. Tarassenko, Estimations of error bounds for
neural network function approximators, IEEE Transactions on Neural
Networks, Vol.10(2), 217-230, 1999.
[55] Vollmer U. and A. Strey, Experimental study on the precision require-
ments of RBF, RPROP and BPTT training, Proc. IEE Ninth International
Conference on Artificial Neural Networks, p.239-244, 1999.
APPENDIX
A. Source of Multiplicative Weight Noise
Multiplicative weight noise usually appears when a neural
network is implemented by a FPGA. Owing to increase com-
putational efficiency and reduce circuit complexity, floating
point arithmetic is avoided. Each number in FPGA is encoded
by a low precision (finite bits) binary number [23], [32],
[55], in a form as follows ±rrr.pppppppppppp. The first
bit is the sign bit. Then, a decimal number 3.125 will be
encoded to 1011001000000000. For an 8-bits format of the
form ±rr.ppppp, 3.125 will be encoded to 11100100.
The beauty of this encoding scheme is that the arithmetic
operations, such as + and ×, can be accomplished by integer
arithmetic. The drawback is that quantization error will exist.
For example, 3.124 and 3.126 cannot be encoded perfectly.
Using the 8-bits format, 3.124 and 3.126 will be encoded to
11100100, if the number is rounded to its nearest 8-bits binary
number. Therefore, an error will exist between a decimal z and
its encoded counterpart z˜.
To study the behavior of this error, we consider the 8-bits
format and let b = (z − z˜)/z. Then 10000 zs are uniformly
random sampled in the range [−4, 4]. The histogram of the
corresponding bs is plotted in Figure 5. Clearly, the distribution
can be treated as a Gaussian distribution with mean zero.
Hence, z˜ can be modeled as an random variable given by
z˜ = z + bz, (55)
where b ∼ N (0, Sb). In accordance with the simulation, the
value of Sb is 0.0054.
