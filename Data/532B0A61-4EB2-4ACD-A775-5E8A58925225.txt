1行政院國家科學委員會專題研究計畫年度報告
代理人導向軟體工程方法之研究(3/3)
Research on an Agent-Based Software Engineering Method
計畫編號：NSC 95-2221-E-194-009
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日
主持人：劉立頌 國立中正大學電機工程學系
計畫參與人員：程彥儒、黃皇賓、黃珉漢、黃宣銘、許嘉文
國立中正大學電機工程學系
中文摘要及關鍵詞
智慧型代理人導向軟體工程的研究，涵蓋智慧型代理人的分析與設計以及多重代理人的
應用。本研究藉由代理人系統的分析與設計，研究服務導向架構中使用者偏好並開發具
案例式推理的代理人系統。本研究的重點在於代理人與使用者互動中，學習服務系統能
發展的方向。使用案例式推理可擴充代理人架構以發展慎思型代理人並可以讓代理人開
發更加容易自動化。本研究發展一個不限於特定領域的系統開發工具，輔助開發者在利
用案例式推理擴充代理人的開發過程，能夠更容易的開發並維護所需要的案例式推理系
統。
關鍵詞：智慧型代理人, 軟體工程, 服務導向架構, 案例式推理
Abstract with keywords
The research in agent-based software engineering covers many aspects from analysis and
design for an intelligent agent to applications of multiagent systems. Through agent-based
requirements and analysis, we applied such method in service-oriented architecture in order to
collect user requirements. By extending this approach, a case-based reasoning mechanism can
be implemented in an agent system for enabling a learning ability. Our research focus is based
on the interaction between an intelligent agent, and a tool for developing a case-based
reasoning mechanism is developed.
Keywords: intelligent agents, software engineering, service-oriented architecture, case-based
reasoning
1.0.前言
經過第一年代理人導向的分析與設計的研究，在第二年有了 ontology 相關主題的研
究，在第三年將這些經驗使用在代理人系統上，使其功用為能夠針對環境所發出的訊息做
出回應，根據代理人本身的目標作出一些主動的行為，跟其他的代理人或是人類做互動，
然後用之前的經驗來達到目前的目標[1]。而慎思型代理人(deliberative agent)則是指行為是
以推理系統做為基礎的代理人。針對慎思型代理人，有些系統架構被設計出來，而其中大
部分都是以 BDI (Belief-Desire-Intention)作為其設計的基礎。在 BDI 的架構下，代理人的行
為由三個心理屬性所組成：信念(beliefs)、 渴望(desires)與意圖(intentions) [2]。信念表示代
理人所認知有關於它自己以及環境的資訊狀態；渴望代表代理人想要達到的狀態；意圖則
是代表推理的狀態，也就是要如何的由現階段達到所想要到達的狀態。意圖可以用一連串
的行動來表示，這些行動可以視為規劃(Plan)。
BDI 架構的優點是非常的直觀且其決策過程非常容易被理解和執行。然而它的缺點是
很難找到一個有效率的方式來實做這個架構。Rao 和 Georgeff [2]指出 BDI 架構有兩個問
3個概念之間的相似度，可利用它們存在的本文集中所共有的父類別來做測量，而這個父類
別又稱為 MSCA(Most Specific Common Abstraction)，也就是當 MSCA 等於零時，這二個概
念相似度為零。Jiang 和 Conrath [8]所提出的此方法和 Resnik 有點類似，但他們的方法比
Resnik 還要準確，因為他們把點的計算也列入考慮。Nuno Sec [9]的計算方法所計算出的
語意相似度是最接近於人所做的語意相似度，它跟其他兩種方式的最大不同在於它只取字
詞在 WordNet 中的下位詞。
3.0 研究方法
由於案例式推理系統可應用在許多的領域[10]，所以案例式推理系統對各領域的研究
者可以產生非常大的幫助，但目前有相關案例式推理系統產生器的研究實為少數。所以趨
使我們發展一個不限特定領域的案例式推理系統開發工具，並且將之運用在不同的領域之
中，使得案例式推理系統開發者可以節省開發時間。
3.1 縮短案例擷取之相似度測量時間方法
由於案例式推理系統具有學習能力，因此案例庫會隨時間的增長而儲存越來越多的案
例。當案例庫的案例一多也就代表找出最相似的案例時間會更久，因此我們提出一個比較
簡單的方法來加速案例擷取時間。
案例擷取的目的是為了由案例庫中找出跟所遭遇的問題描述特徵相似值最高的案例。
利用這樣的特性來縮短擷取出最相似案例的時間，當可以預先找出案例相似值不會比目前
最高相似度案例高時，就可以省略計算之後的特徵相似度。在我們的研究中，發現越早找
到一個相似度很高的案例，那擷取最相似案例的時間就會越短。









N
k
k
N
k
kk
ASV
N
k
k
ASV
w
simw
simAgerageWeighted
N
sim
simAgerageWeightNo
1
1
1
:_
:__
上式是案例相似度計算公式，主要分成有權重和沒有權重這二種計算方式，系統開發
者決定要使用那種方法計算相似度。 ASVsim (Average Similarity Value)代表整個案例相似度，
nsim 代表第 n 個特徵值的相似度，N 是案例中的特徵數量，而 nw 是第 n 個特徵值的加權。
以下列計算公式，對我們的方法做說明。 ASVPsim (Previous Average Similarity Value)為案
例擷取過程中最高案例相似值， ASVCsim (Current Average Similarity Value)為案例擷取過程中
目前正在計算的案例相似值，假設目前的 ASVPsim 為 0.76，並且有五個特徵值。當 ASVCsim 值高
於 ASVPsim 值時，把 ASVPsim 值用 ASVCsim 值取代。在下列計算公式中先把 ASVPsim 乘以特徵數量，所以
會得到 3.8。再將 3.8 減去特徵數量，然後加 1 便會得到-0.2，接下來把-0.2 跟 1sim 做比較，
假設 1sim 小於-0.2 時，就不會再計算後面的四個特徵相似值。反之 1sim 大於-0.2 時，就把-0.2
加 1 得到 0.8，然後再繼續計算 2sim 。當 1sim 加上 2sim 的總合小於 0.8 時，就不再計算後面的
三個特徵相似值，反之 1sim 加上 2sim 大於 0.8 時，就把 0.8 加上 1 得到 1.8，然後再繼續計
算 3sim 。以上述的方式以此類推上去，就可以省略案例擷取時的一些不必要的計算。
53.2.1 物件相似度比較函式
物件相似度比較函式是將二個比較值當成物件做比較，設定的比較方式是比較二個
Object 等於或不等於，因此相似值只會產生 0 或 1。下式為物件相似度比較函式的數學函
式。





21
21
21 0
1
),(
ObjectObjectif
ObjectObjectif
ObjectObjectsim
3.2.2 間距相似度比較函式
由於間距相似度比較函式是計算二個比較值之間的間距，並將此間距轉為相似度，因
此使用這個函數時還需要初始的最大間距，即二個比較值的間距等於零時，就會得到相似
度 1。
下式為間距相似度比較函式的數學函式，Double1 代表第一個比較數字是一個 Double
類型的資料型態，Double2 代表第二個比較數字也是一個 Double 類型的資料型態。Double1
減掉 Double2 後在它們相減的結果取絶對值，即可算出兩者之間的相差值，而 MaxInterval
是最大的數字間距。將 Double1 和 Double2 的相差值除以 MaxInterval，最後再用 1 減去
右邊的值即可求出相似值。
IntervalMax
DoubleDouble
DoubleDoublesim 2121 1),(

3.2.3 字串相似度比較函式及語意相似度比較函式
MaxString 是一個利用 longest common substring 演算法來計算字串相似度的方法，主
要是先取得兩個字串之間的最長相同子字串長度，然後除以兩個字串中的最長字串長度以
取得相似值。
資訊內容是測試二個字之間語意相似度的重要技術，一般是用知識本體論 (如
WordNet)的階層結構，測量字與字之間的語意相似度。
IC 值是資訊理論方法中，決策文集分析的一個重要值，獲得 IC 值的方法為
)(log)( cpcicres  。c代表文集中的 concept，而 )(cp 是 c在文集獲得的機率。下式是 Nuno Seco
[9]利用 WordNet 當作決策資源，而獲得的 IC 值：
)log(max
1)(log(
1
max
1
log
)log(max
1)(
log
wn
wn
wn
wn
chypo
chypo
ic








 

上式中 hypo是 c的下位詞的數量， wnmax 是個常數，它代表最大數量的概念分類。




 
2
),(2)()(
1),( 21'2121
ccsimciccic
ccsim reswnwnjcn
jcnsim 是這個方法的名字， wnic 是 Nuno Seco 修改過的 IC 值， 'ressim 是將 Resnik 方法中原
有的 IC 計算方式修改為 Nuno Seco 的 wnic 。
我們在[11]的實驗數據中發現名詞的相似度比率偏高。作者隨機的從 WordNet 中取出
1000 對名詞、動詞、形容詞、副詞，再將它們帶入相似度比較函式進行比較後，得到形容
詞、副詞的相似度比率都偏低，而名詞、動詞的相似度比率相對的比較高，這是因為形容
詞和副詞在 WordNet 中不一定會有相連的關係來連接二個字之間，但名詞和動詞在
WordNet 的分類中數量龐大，所以幾乎可以有一條上位詞或下位詞來連接。因此系統開發
者在選擇案例特徵時，盡量選擇使用名詞或動詞，因為案例特徵需要有明顯差別才能做為
7的提升。
圖 3 Case Base
3.4.3 Connector
JCBRDT 提供了四種案例庫的形態，分別是 MySQL、Text 、XML 或 ConnectorClass，
如圖 4 所示。ConnectorClass 代表直接使用 Java 物件來產生案例庫。MySQL 因為是存放
在硬碟中，所以可存放的案例量大，但和其他方式的相較下會速度較慢。Text、XML 以及
ConnectorClass 這三種方式，它們直接將案例儲存在記憶體中，所以存取案例的速度比較
快，但由於記憶體容量比硬碟小，所以可存的案例量也較少。Text 的修改、新增、刪除案
例比 XML 方式較方便，不過卻不像 XML 定義了案例內容形態，所以很難保證使用 Text
的案例式推理系統在長久使用後不會產生錯誤，而且發生錯誤時也不會有明確的錯誤資
訊。XML 方式優點是可驗證案例庫在經過修改、新增、刪除後，案例庫資料內容後是否
依然正確無誤。ConnectorClass 這種方式不需要使用其它外部程式或是使用一份文件檔來產
生案例庫，因此它的獨立性較高也比較省資源。但最大的缺點是每個案例都要系統開發者
用 Java 建入案例庫中，對系統開發者是一大負擔，同時案例庫的維護也較不易，而且這種
方式也無法保留在程式執行中所新增的案例。
JCBRDT 將連接案例庫方法放在 Connector XML Schema 文件中。從這文件可知道系
統設計者選擇那一種案例庫來源。而且這樣的好處是更換案例庫時只需修改這份文件。
圖 4 Connector
4. 實作
第三章中介紹了 JCBRDT 的研究方法，在這章中描述了 JCBRDT 的系統架構，並用一
個實例來說明。最後，分析 JCBRDT 與其他開發工具之異同。
4.1 系統架構
JCBRDT 主要分為二個子系統，CBRFrames Generation 與 CBR Core。CBR Frames
Generation 的功用是讓系統開發者可以使用圖形介面設定案例式推理系統相關資訊，如：
案例結構、案例庫儲存方式、適應規則方式。圖形介面是依據 CBR Core 所能提供的功能
來發展，目的是讓系統開發者能從簡單的操作中產出想要的案例式推理系統，而不用瞭解
9圖 6 擷取最相似案例畫面
從圖 7 也可以看到原本的比薩配料為 TomatoTopping 、PeperoniSausageTopping、
MozzarellaTopping ， 經 過 適 應 修 改 後 得 到 的 新 解 為 PeperoniSausageTopping 、
MozzarellaTopping 。這是因為在問題描述的 NEGATIVE 特徵中，使用者輸入了
TomatoTopping 也就是不想吃的比薩配料，所以擷取出的最相似案例比薩配料，刪除有包
含 TomatoTopping 的部份。
圖 7 比薩配料適應解
4.3 系統比較
由於 jCOLIBRI 所產生的案例式推理系統只能擷取最相似的案例，所以是一種零適
應；而 JCBRDT 利用了 Jess 來逹到案例適應，所以 JCBRDT 是屬於規則適應。JCBRDT
利用了 Protégé 來製作適應知識庫，使 JCBR 的案例適應解部份能更符合使用者的需求。
不管是使用者或其他系統要求 JCBRDT 所開發系統對某個問題提供解答時，所回傳的解可
以讓使用者或其他系統直接使用而不用再做修改。
案例庫來源方面，JCBRDT 比 jCOLIBRI 多提供了 XML 的案例庫。利用 XML 是為了
補足 Text 的缺點，Text 雖然容易使用，而且新增案例的速度也較快，但在結構性上卻不
比 XML。由於 Test 的內容比較沒有結構性，所以 Text 案例內容不易進行修改。
語意相似度比較函數讓 JCBRDT 的系統開發者可以加入語意的案例特徵，比如可以比
較學生跟作者這二個單字的語意相似度。表 1 為 JCBR 與 jCOLIBRI 的功能比較表。
表 1 系統比較表
11
Workshop on Software Engineering, Databases, and Knowledge Discovery, conjunction with
International Computer Symposium (ICS 2006)
References
[1] J. M. Corchado and M. A. Pelicer, “Development of CBR-BDI Agents,” International Journal of 
Computer Science & Applications, Vol. 2, No. 1, pp. 25 -32, 2005.
[2]A. S. Rao and M. P. Georgeff, “BDI Agents: From Theory to Practice,” First International Conference 
on Multi-Agent Systems (ICMAS-95). San Franciso, USA, 1995.
[3] J. M. Corchado and R. Laza, “Constructing Deliberative Agents with Case-based Reasoning
Technology,” International Journal of Inteligent Systems. Vol 18, No. 12, 2003.
[4] M. Glez-Bedia and J. M. Corchado, “A planning strategy based on variational calculus for 
deliberative agents,” Computing and Information Systems Journal. Vol 10, No 1, pp: 2-14, 2002
[5] J. A. R. Garcia, A. Sanchez, B. Diaz-Agudo, P. A. Gonzalez-Calero, “jCOLIBRI 1.0 in a nutshel. A
software tool for designing CBR systems,” In M. Petridis, editor, Proccedings of the 10th UK 
Workshop on Case Based Reasoning, pp. 20-28, 2005.
[6]G. A. Miler, “WordNet : A Lexical Database for English,” Communications of the ACM, Vol. 38, pp. 
39-41, 1995.
Engineering Group, Tsinghua University, pp. 36-45, 2005
[7]P. Resnik, “Using information content to evaluate semantic similarity in a taxonomy,” in Proceedings 
of the 14th International Joint Conference on Artificial Intelligence, pp. 448–453, 1995.
[8] J. J. Jiang, D. W. Conrath, “Semantic similarity based on corpus statistics and lexical taxonomy,” In
Proceedings of International Conference on Research in Computational Linguistics, Taiwan, 1997.
[9] N. Seco, T. Veale, J. Hayes, “An Intrinsic Information Content Metric for Semantic Similarity in 
WordNet,” In Proceedings of the 16th European Conference on Artificial Intelligence, Valencia,
Spain, 22–27 August, pp. 1089-1090, 2004.
[10] J. M. Corchado, J. Pavon, E. Corchado, L. F. Castilo, “ Development of CBR-BDI Agents: A
Tourist Guide Application,” In 7th European Conference on Case-based Reasoning, pp. 547-559,
2004.
[11] P. Zhang, “The research and implementation of Semantic Based Web Services Discovery,” 
Knowledge
[12]M. Menken, “Jess Tutorial,” Vrije Universiteit, Amsterdam, The Netherlands, pp. 1-57, 2002.
[13]E. F. Hil, “Jess in Action: Rule-Based Systems in Java,” Manning Publications, pp. 1-480, 2003.
[14] A. Rector, N. Drummond, M. Horridge, J. Rogers, H. Knublauch, R. Stevens, H. Wang, C.
Wroe, ”OWL Pizzas: Practical Experience of Teaching OWL-DL: Common Errors & Common
Paterns,” 14th International Conference on Knowledge Engineering and Knowledge Management
(EKAW), Whittlebury Hall, UK, 2004.
[15] A. Sanchez, J. A. Recio, B. Diaz-Agudo, P. Gonzalez-Calero, “Case structures in jCOLIBRI,” Best 
Poster Award. Twenty-fith SGAI Int. Conf. on Innovative Techniques and Applications of Artificial
Intelligence, AI. Cambridge, UK, pp. 1-13, 2005.
Abstract
Planning has been commonly applied to automating Web
service composition recently. However, most of automated
systems of Web service composition contain two problems.
First, most of them overlook some user needs which
sometimes combine services provided by systems themselves
and services from external systems to provide a much more
flexible service model. Second, most of them do not record
information about service providers having already served
the users and about planning having already been processed
in order to speed up the pace and facilitation of systems
providing services. Therefore, this paper presents a method
of merging internal and external service systems to reach
users’ needs. The internal service means the system
functions designed to satisfy user needs. The external one
means a Web service provided by external service providers.
We apply techniques of planning to combine both types of
services so that we can create planning made by a series of
operations to satisfy user needs. We also apply Case-Based
Reasoning to store planning and related information into a
case base, so that it creates planning in much faster way
when users have similar needs.
1 Introduction
Service composition is a challenging research topic [1].
To solve such problem, there are different approaches, and
there are a few reported systems, such as MIND [2] and
Pistor [3], which use planning from artificial intelligence
(AI). Our work is to add a feature of user satisfaction in
service composition using planning and also provide a
mechanism which remembers the usage of previous service
request. For this reason, we have designed a system which
uses planning, namely Hierarchical Task Network Planning
(HTNP) [2], in service composition which uses Case-Based
Reasoning (CBR) [4] in reusing old experiences.
This work was supported in part by Ministry of Economic Affairs (Taiwan)
under the grant 95-EC-17-A-02-S1-029 and National Science Council (Taiwan) under
the grants 95-2752-E-008-002-PAE and 94-2213-E-194-010.
Our prototype system described in this paper uses our
previous work on intension-aware goal model [5] and
personal ontology [6] to provide a service which is most
suitable to the user. By using CBR, the system remembers
how the user has been satisfied by certain services in order
to reuse the previous experiences in future usages.
The work reported in [2] uses HTNP as the base in
deriving services. The OWL-S files provided by service
providers are converted to the domain in the tool, SHOP2,
and the service request entered by the user will be used in
planning with the domain information. Finally, a description
file resulted from the service composition in OWL-S will be
produced. The authors in [7] also use HTNP for service
composition, but the difference between the work above and
this is that this provides an algorithm, called Enquirer, in the
query manger to obtain information. The advantage is that it
still produces a reasonable result in the initial stages when
information is still lacking. An approach in planning as
model checking is used in [3]. The authors use MBP Planner
[8] for solving nondeterministic and partial observable
problems along with problems associated with extended
goals. CBR is used in [9] for service composition. The
authors use six different kinds of relationship among
services, and the service name along with its service
description is used for retrieving a case for providing a
solution.
For the discussion above, Table 1 summarizes the related
work with features and criteria, such as the main methods
used; the capability in handling unexpected situation, in
achieving a goal, and in recording what service providers
had been used; and the ability in performing composition
among internal services and external services. All systems
have the ability in achieving their goals from the known
providers, but only some of them can handle unexpected
situations. Since none of the systems learns the providers
that they have used, every service request is processed from
scratch. In addition, none of the systems feature the ability in
composing internal and external services together. If a
system has its internal services, it can use such services in
place of external services. This will save resources in
searching and transmission. In addition, such internal
services may also play the roles of substituting external
services in case of failure.
Service Composition Using Planning and Case-Based Reasoning
Kuan-Hsian Huang and Alan Liu
Department of Electrical Engineering
Center for Telecommunication Research
National Chung Cheng University
Chiayi, 621, Taiwan
aliu@ee.ccu.edu.tw
uses S and T to come up with a plan. In the task network, T,
t' represents a sub-task. A sub-task may be a method or an
operator, so task decomposition follows the following rules:
1. If t' represents a method, then t' needs to be
decomposed further until t' becomes an operator.
2. An operator represents the basic action in this
paper, and the basic action cannot be divided.
Thus, decomposition stops when t' is an
operator.
A subtask, 1t , may be viewed as four methods, such as
Select_Service, Check_Service_Data, Check_all_Condition,
and Show_Data. Table 2 summarizes the precondition for
those actions and the stauts changes afterward. In Table 2
and Figure 1, S represents a service provider and C
represents the parameter for filtering information after a
service.
In Figure 1, we represent Table 2 as a state chart. in
which we can see that what preconditions need to be
satisfied before executing a method and what status change
a method brings after execution. In the figure, we see that to
execute Show_data, we need to satisfy two statuses,
Check_ready and Have_main_service_data. These two
statuses rely on two methods, Check_all_condition and
Check_service_data. These two methods instead rely on
Select_service.
Figure 1. Relationship between Four Methods
Those four methods in Table 2 can be further
divided into other methods or operators. Taking the
method, Select_Service as example, we can divide it
into two types, primary service and secondary service.
These two types are then defined as Main_Goal and
Seond_Gaol, respectively, and they have different
preconditions. This is depicted in Figure 2.
As for the method, Check_all_Condition, the
main purpose is to check whether or not sorting or
filtering status still exists in the service. If there is a
service still needing sorting or filtering status, then
Check_all_Condition will clean the status after
applying sorting or filtering and produces
Check_Ready status.
Figure 2 Flow of Select_Service
2.2 Process with CBR
A case is divided to two parts, the description part and
the solution part. We use three attributes, action, object, and
constraints, to define the description part of a case. As for
the solution part, it includes the locations of the OWL-S
files by service providers, the parameters needed for the
service providers, and the plans.
At the case retrieval stage, we use the goal model
provided as the features for finding a case. At the case reuse
stage, we check the content of the goal model and the
retrieved case. If the content is an exact match, then we can
just simply copy the solution part as a composite service for
the user. If there is still a difference, an adaptation
mechanism will modify the solution part accordingly.
As for the prototype reported here, we have not yet
come up with a satisfactory adaptation method, so we
simply use substitution for the adaptation part. For that we
find the data of Constraints and Parameters of objects for
comparison and apply If-Then rules to modify the content.
Currently, we have two categories of rules, oone based on
constraints and the other based on objects. The following
are some of the rules:
A. Rules based on constraints (18 rules)
 IF Case_Constrain =’Before Date’ AND New
Constrains=’After Date’                             
Then Update Case_Constrains_Data
 IF Case_Constrain =’Before Date’ AND New
Constrains=’About Date’                             
Then Update Case_Constrains_Data
 IF Case_Constrain =’After Date’ AND New
Constrains=’Before Date’                             
Then Update Case_Constrains_Data
 ...
B. Rules based on Parameters of objects (8 rules)
 IF Case_Date != New_Date
THEN Case_Date = New_Date
The DG delivers service information and planning results
to Plan Generator, where Select_Plan_Generate and
Order_plan_Generate are performed, as shown in Figure 6.
This process is to set up the plan generator and to produce
suitable plans to execute. Figure 7 shows the sequence
diagram of the CBR Planner, which compares the goal
model with the previous cases to decide whether to perform
the adaptation process. The similarity measurement is
basically to check the parameters of objects and constraints.
If both sections are matched, then no modification is
necessary. If there is no exact match, then the rules are
triggered to modify the solution part of the case retrieved.
Figure 4. Sequence Diagram of Our System
Figure 5 Sequence Diagram of Data Gatherer
Using Personal Ontology in Evaluating Service Quality∗ 
 
Jeng-Shin Hung and Alan Liu 
Department of Electrical Engineering 
Center for Telecommunication Research 
National Chung Cheng University 
Chiayi, 621, Taiwan 
aliu@ee.ccu.edu.tw 
 
 
                                                
∗ This work was supported in part by Ministry of Economic Affairs (Taiwan) under the grant 95-EC-17-A-02-S1-029 and National Science Council (Taiwan) under the 
grants 95-2752-E-008-002-PAE and 94-2213-E-194-010. 
Abstract 
 
How to find a suitable service for a user is a challenging 
task. This paper introduces a method of concept 
categorization with the use of personal ontology to select 
more suitable service according to the preferences of 
users. In addition, a new attribute called 'reformation' is 
added to evaluate service quality. Our method is to aide 
the evaluation process with user preferences to reflect the 
fact that the user difference will affect the selection of 
different service providers. 
 
1. Introduction 
 
How to find a suitable service for a user is a 
challenging task. We use the concept of Quality of 
Service (QoS) [1] to evaluate the services to the users 
according to user preferences. For such requirements, 
personal ontology [2] is used in modeling user 
preferences and also user backgrounds. By establishing 
personal ontology for each user, we may automate the 
matching process for finding more suitable service 
providers for the users. This work uses goal models [3] to 
capture user intention to aide the personal ontology for 
providing more information for the system to filter the 
services. 
Personal ontology is used for storing information about 
each individual user. How much information and what 
information to keep raises a new problem and will be 
discussed in the latter section. The work in [4, 5] uses 
personal ontology to record the habit and interests of a 
user in browsing Web pages. A system then uses such 
information for recommending the user to view certain 
Web pages. The authors in [4] use personal ontology in an 
agent-system for a meeting scheduling system. In [5], 
personal ontology and domain ontology were used in 
keeping personal information and food preferences for 
finding Web services. 
There are many service providers offering similar 
services, and the services delivered by different providers 
may differ depending on situations and platforms. Quality 
attributes that are usually considered include execution 
price, latency, response time, throughput, availability, 
reliability, reputation, etc. In addition, the attributes like 
security and transaction may also be considered [1]. In 
using quality attributes in selecting services, the work 
reported in [6] uses QoS for evaluating service 
composition. The authors in [7,8] define ontology for 
service quality to provide strategies for QoS between 
providers and requestors. Such strategies will be used for 
selecting matching services. Service Level Agreement 
(SLA) [9] is to protect the requestors for receiving quality 
services. For providers, they can use this index for 
making themselves more appealing to the requestors.  
Table 1 shows the features used by the systems 
mentioned in the previous paragraph. All systems use 
community usage history which is the record of services 
used for selecting services. Most of them use some kinds 
of service quality parameters and make a good use of user 
feedback. However, Personal service selection policy is 
supported by only one system. 
 
Table 1. Comparison of features 
 [6] [7] [11] [12] 
Service quality parameters v v v  
User feedback v  v v 
User usage history   v v 
Community usage history v v v v 
Personal selection policy  v   
User preferences     
 
The discussion above on related work is the motivation 
to our work in blending personal ontology with the QoS 
concept. This paper is organized with Section 2 
presenting how personal ontology is constructed. Section 
3 discusses on the subject of service quality, while 
Section 4 presents the reasoning method for service 
selection. Section 5 explains the simulation work and the 
last section states the conclusion. 
 
Step 3. Categorize the concepts according to the 
meaning in domains. 
Step 4. Categorize the concepts according to the user's 
point of view. 
Step 5. Determine whether the concept is of UDC or 
USC. If so, more information is needed. If not, the 
step 6 can be skipped. 
Step 6. Use 'Interpretation' to determine the user's view 
on such concept. 
Step 7. Collect the weights between the personal 
information and the user preferences. 
Step 8. Produce personal ontology. 
 
Figure 3 shows an example in a transportation system. 
A partial view of the personal ontology shown here has 
four sub-domains. From the domain point of view, the 
attributes 'price', 'company', and 'time' all belong to DIC. 
The train type and ticket rank belong to DSC. 
 
 
Figure 2. Personal ontology construction 
process 
 
 
Figure 3. Partial list of personal ontology in the transportation domain 
 
Finally, putting all together, we can have the value for 
reformation, reformS , defined as 
r
RW
S
r
i
ada
reform
ii∑
=
×
=
1  
 
3.5. Reputation 
 
With the availability of compliance, verity, user rating, 
and reformation, we can define reputation as a quadruple 
below: 
),,,( reformratingveritycomplreput SSSSS =  
Considering the promises and reliability from the past 
experiences with a service provider, a user may add some 
weights to the above. Thus, we will have 
reformratingveritycomplreput SWSWSWSWS 4321 +++=  
 
4. Reasoning method for service selection 
 
We use fuzzy reasoning method in this section for 
evaluation. A service quality model consisting of 
'reputation', 'compliance', 'verity', 'user ranking', and 
'reformation' is used and 'bad', 'normal', and 'good' are 
used for defining a linguistic value.  
 
 
Figure 4. Definition 'Normal' 
 
Figure 4 shows a 'normal' fuzzy set, where Quality 
Value shows the arguments entered. The y-axis, Degree is 
the membership degree with the values between 0 and 1. 
We also provide some hedges with the definition of 
'extremely', 'very', 'normal', 'somewhat', and 'less'. A 
degree of concern, AQ , can also be determined. The 
degree of concern for 'normal' then is defined as AS . ( )GoodNormalBadAQ µµµ ,,= , ( )ANormalA QUS ,  
The degree of concern, 'Extremely' can be defined as 
Figure 5. Figure 6 shows the degree of concern for 'Very'. 
In the same way 'Somewhat' and 'Less' can be determined 
and are depicted in Figures 7 and 8. 
 ( ) ( )[ ]3,, ANormalAAExtremelyA QUSQUS =  
 
Figure 5. Definition 'Extremely' 
 
( ) ( )[ ]2,, ANormalAAVeryA QUSQUS =  
 
Figure 6. Definition 'Very' 
 
( ) ( )[ ]21,, ANormalAASomewhatA QUSQUS =  
 
Figure 7. Definition 'Somewhat' 
 
user. The QoS Calculator uses the attributes presented in 
Section 3 and the execution history is kept in a database. 
The purpose of this simulation is to evaluate the 
differences between 'Prime', 'Good', 'Normal', 'Bad', and 
'Worst'. To start with a simple experiment, we first 
consider 'response time' as a factor in determining how 
five services which are viewed as similar services may 
differ. For this simulation, 'User Rating' is assumed to be 
all 'Normal'. To setup the simulation, we assume that we 
have five services, namely Service 0, Service 1, etc. The 
are defined as Table 4: 
 
Table 4. Sample services 
  Service 0 
Service 
1 
Service 
2 
Service 
3 
Service 
4 
time slow slow medium fast fast 
stability bad good good bad good 
 
 
Service Execution History
700
800
900
1000
1100
1200
1300
1 6 11 16 21 26
Invocation No.
Re
sp
on
se
Ti
me
 (m
s) Service 0
Service 1
Service 2
Service 3
Service 4
 
Figure 11. Time for service execution 
 
Figure 11 shows the result of performing 5 different 
services for 30 times. We can see individual 
characteristics of each service. Service 0 and Service 3 
have a great variation in response time, and this causes 
unstable services. Table 5 shows the statistical data 
concerning response time. 
 
Table 5. Response time (Unit: ms) 
  Service0 Service1 Service 2 Service 3 Service 4
Average 1110.93 1100.93 981.4 895.966 915.633 
Std. Div. 102.664 62.9827 71.6281 103.048 68.4218 
Max. 1302 1242 1132 1102 1031 
Min. 911 972 861 711 771 
 
In our service quality evaluation mode, 'Compliance' 
keeps the record of whether the service is completed 
every time. 'Verity' shows the stableness of the quality. 
Table 6 shows three different policies with four attributes 
set in different values. This setting is to simulate a user 
whose main concern is on compliance. 
 
Table 6. Polices based to quality attributes 
  Compliance Verity User Rating Reformation
Policy 1 Less Less Less Less 
Policy 2 Normal Less Less Less 
Policy 3 Extremely Less Less Less 
 
By taking Service 3 as example, we can see that 
Service 3 is sensitive to the compliance value, so that the 
value varies greatly according to the degree of 'less', 
'normal', and 'extreme' as shown in Figure 12. 
 
Service 3
0.4
0.5
0.6
0.7
0.8
1 4 7 10 13 16 19 22 25 28
Invocation No.
M
atc
hin
g V
alu
e
Normal
Less
Extremely
 
Figure 12. Result of the service 3 
  
 
Table 7. Service selection strategies 
 Service0 Service1 Service2 Service3 Service4
Less 0 0 34.50% 27.60% 44% 
Normal 3.40% 0 24.10% 37.90% 34.50% 
Extremely 0 0 20.60% 41.40% 37.90% 
 
Table 7 shows the result of having completed all 30 
service selections except for the very first one which does 
not a previous service. In this simulation 'Compliance' and 
'Response time' along with the value 'Extremely' are used. 
The table shows that Service 1 is never chosen and 
Service 3 is most chosen when 'Extreme' is of concern, 
while Service 4 is chosen when 'Less' is the primary 
concern. More simulation was conducted according to 
different preferences, and the results show that our 
definition of reputation with the concept of reformation is 
informative. 
 
6. Conclusion 
 
This paper proposed the usage of concept 
categorization from the domain and user point of view. 
Personal ontology is used in representing difference in 
users by recording their preferences and other 
information. The attribute of 'reputation' is used for 
