in the first part of this research and the 
knowledge ontology.  
Key-Words: - Semantic Web, RDF, word sense 
disambiguation, semantic annotation, ontology, 
Wordnet 
 
1. A Semantic Annotation and 
Retrieval Framework 
For the proposed framework of semantic 
information extracting, annotation, searching 
and inference, three resource and techniques 
are required: 
z Word Sense Disambiguation 
z Wordnet ontology 
z Resource Definition Framework (RDF) 
 
1.1 The Workflow 
The flow of adding semantics to textual 
documents can be as follows: 
1. Acquisition of textual document(s); 
2. Text processing; 
3. Semantic processing; 
4. Semantic annotation and storage.  
This flow is shown in figure 1. 
 
 
Figure 1. The flow of adding semantics to the 
textual data 
In the retrieval progress, documents can 
be retrieved according to the concepts 
contained in their content. The flow of 
retrieving textual documents based on their 
annotations can be as follows: 
1. Identify the concept(s) intended for 
searching; 
2. Query for documents; 
3. Infer the related concepts; 
4. Output the results. 
The flow is shown in the figure 2.  
 
 
Figure 2. The flow of textual data retrieval 
using semantic annotations and ontology. 
 
2. Implementation Issues 
 
Acquisition of textual documents 
For textual documents to get 
semantically annotated, the web service is a 
good choice as the service interface. A Web 
service is a software system designed to 
support interoperable machine-to-machine 
interaction over a network. The World Wide 
Web is more and more used for application 
to application communication. The 
programmatic interfaces made available are 
referred to as Web services. It has an 
 2
several years by the Dublin Core Metad
Initiative (DCMI) on its web site.  
ata     int *frmto;   
 
For RDF manipulations (parse, write, 
query … etc.) and repositories (database), 
there are various tools available. For Java 
developers, there are Jena 
[http://www.hpl.hp.com/semweb/], Sesame 
[http://www.openrdf.org/] and Kowari 
[http://kowari.org/]. For python developers, 
there are Redfoot [http://redfoot.net/], 4Suit 
[http://4suite.org/]. For C, Redland 
[http://librdf.org/], RDFDB 
[http://www.guha.com/rdfdb/] are available. 
 
Inference 
 
To make inference from a concept to 
other related concepts, the Wordnet ontology 
can be used. Wordnet API can be used to 
access the synsets and semantic relations thus 
to be able to search and make inference 
through the Wordnet semantic net. The data 
structure of a synset is defined as table 1. 
 
Table 1. Data structure defined for a synset. 
/* Structure for data file synset */ 
typedef struct ss { 
    long hereiam;    
    int sstype;   
    int fnum;  
    char *pos;   
    int wcount;   
    char **words;     
    int *lexid;   
    int *wnsns;   
    int whichword;  
    int ptrcount;   
    int *ptrtyp;   
    long *ptroff;   
    int *ppos;   
    int *pto;    
    int *pfrm;   
    int fcount;   
    int *frmid;   
    char *defn;   
    unsigned int key; 
 
    /* these fields are used if a data structure is returned 
       instead of a text buffer */ 
 
    struct ss *nextss;   
    struct ss *nextform;   
    int searchtype;   
    struct ss *ptrlist;   
    char *headword;   
    short headsense;   
} Synset, *SynsetPtr; 
 
 
The major functions possibly used are listed 
in table 2. These functions provide the 
abilities to: find semantic relations defined 
for (connected to) a synset, find/trace sysnets 
along certain semantic relations, look up 
word senses (concepts) and part-of-speech 
defined for a word form, read the data in a 
synset structure (as defined in table 2) … etc. 
Through the proper use of these functions, 
we can access to the knowledge defined in 
Wordnet. 
 
Table 2. Major functions used to access and 
inference the knowledge defined in Wordnet. 
findtheinfo  Primary search function for 
WordNet database. Returns 
formatted search results in text 
buffer. Used by WordNet 
interfaces to perform requested 
search.  
findtheinfo_d
s  
Primary search function for 
WordNet database. Returns 
search results in linked list data 
structure.  
is_defined  Set bit for each search type 
that is valid for the search 
word passed and return bit 
 4
Function traceptrs_ds() is a recursive 
search algorithm that traces pointers 
matching ptr_type starting with the synset 
pointed to by synptr . Setting depth to 
indicate the times of recursion when 
traceptrs_ds() is called (0 indicates a 
non-recursive call). The synptr points to the 
data structure representing the synset to 
search for a pointer (semantic link) of type 
ptr_type. When a pointer type match is found, 
the synset pointed to by this pointer type 
(semantic link) is read is linked onto the 
nextss chain in the data structure. Levels of 
the tree generated by a recursive search are 
linked via the ptrlist field structure until 
NULL is found, indicating the top (or bottom) 
of the tree. For example, if ptr_1 points to a 
synset which represents the concept: “eagle 
as any of various large keen-sighted diurnal 
birds of prey noted for their broad wings and 
strong soaring flight”, then the function call 
traceptrs_ds(ptr_1, HYPERPTR, NOUN, 1) 
will return the hypernym concept of ptr_1: 
“bird of prey, raptor, raptorial bird”, and the 
double hypernym concept of ptr_1 (one time 
recursion): “bird”. 
 
3. Conclusions and Future Research 
 
In the Semantic Web, to publish content, 
information providers need to use (or create 
new) appropriate domain ontologies to create 
instances and make assertions. When 
accessing knowledge, information requesters 
need to search for instance data and pursue 
corresponding ontologies to understand the 
knowledge encoded. 
 
This research demonstrates an 
automated approach to extract and annotate 
the semantics of the data using techniques of 
word sense disambiguation, RDF and the 
resource of Wordnet ontology. After this the 
data are associated with the concepts in the 
ontology. In the retrieval, the query can also 
be first associated with the concepts in the 
ontology, then search for the instances data. 
Related concepts can be inferred from the 
ontology, the inference results can be use to 
facilitate the data management such as 
retrieval or classification. The approach is 
fully automated and can be apply to very 
large scale data. This work facilitates the 
migration from today’s web to the future’s 
Semantic Web in that the semantics can be 
annotated to all the existing textual data on 
the Web in this way. 
Ontologies are knowledge bodies that 
provide a formal representation of a shared 
conceptualization of a particular domain. 
Ontologies are widely used in the Semantic 
Web. Recently ontologies have become 
increasingly common on WWW where they 
provide semantics of annotations in web 
pages. In this work, in ontology level the 
semantic annotation and retrieval are against 
Wordnet, which is general domain ontology. 
However, for specific applications, specific 
domain ontologies can be adopted as the 
basis to describe the semantics of data in a 
similar way. Different semantics extraction 
techniques may be needed against different 
ontologies too. 
Also, how to share the semantic 
annotations against/between different 
ontologies will be a future research, e.g. 
 6
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                             97 年 4 月 4 日 附件三
 
報告人姓名 洪啟舜 服務機構
及職稱 
僑光技術學院資訊科技系 
助理教授 
 會議時間 
     地點 
2008/3/25~2008/3/28 
日本,琉球 
本會核定
補助文號
NSC 96-2221-E-240-003-  
 
會議 
名稱 
 (中文) 第二十二屆進階資訊網路與應用國際研討會(AINA2008) 
 (英文) The IEEE 22nd International Conference on Advanced Information 
Networking and Applications (AINA 2008) 
發表 
論文 
題目 
 (中文) 以樣板為基礎的異質手持裝置課程內容調整工具 
 (英文) A Learning Content Adaptation Tool with Templates for Different 
Handhelds 
報告內容應包括下列各項： 
一、參加會議經過 
三月二十五日會議開始，各國學者陸續來到。 
總共有二場 Keynote Speech:第一場是 Prof. Michel Raynal ，講題是 Synchronization is 
Coming Back, But is it the Same?；第二場則是 Prof. Shigeki Yamada，講題是 Cyber Science 
Infrastructure (CSI) for Promoting Research Activities of Academia and Industries in Japan 
三月二十六日，大會開始進行論文的發表，本人在下午五點擔任會議主持人，晚上是
Reception。 
三月二十七日本人於下午一點發表論文、並於晚上參加晚宴 
三月二十八日則是最後一天，依然討論熱絡，中午結束後則準備離開。 
 
二、與會心得 
在會議中與不少學者有進一步的討論，互相切磋，可說是獲益良多，大致上對於一些有
關網路計算相關的新研究主題，有不少新的看法，這對本人在教學與研究上有更多的啟
發。 
 
三、考察參觀活動(無是項活動者省略) 
 
四、建議 
建議國科會可多多補助國內學者參與此會議，相信也會有一些很好的收穫。 
 
五、攜回資料名稱及內容 
論文集 
 
六、其他 
 
 
表 Y04 
seriously degrade its educational quality and the 
learner’s learning will. So, our main purposes are 
specifically listed as follows, (1) we propose a web-
based content adaptation tool that uses templates to 
automatically and efficiently adapt content for mobile 
learning; (2) through adjusting the template’s 
parameters, users can easily manipulate the process 
and change the result if they are dissatisfied; (3) the 
tool allows user to preview the adapted content and 
decide whether it is appropriate to read on handhelds. 
Now, the author or educator also play a supervisor role, 
which determine whether the adapted content is 
available for publishing on corresponding handhelds; 
such that prevents the user to waste much time and 
limited bandwidth to download the insufficient content 
to their specific handheld devices. (4) last but not least, 
the author’s modifications and adjustments will be 
recorded using the adaptation tool. These data will be 
valuable and referenced in our advanced studies to 
improve the adaptation predictions. 
The rest of this paper is organized as follows. 
Section 2 describes related works. Section 3 presents 
the primary adaptation modes, and describes how we 
augment the existing content summarization and 
adaptation approaches to apply on learning materials. 
Section 4 details the implementation of the web-based 
adaptation tool. Finally, section 5 concludes the paper 
and describes our future work. 
 
2. Related Works 
 
Adaptation is a well-studied topic in mobile and 
pervasive computing for years [8][9]. Hwang et al. 
proposed a transcoding framework [10] that represents 
a Web page as a modified tree structure to efficiently 
analyze and transcode pages. It is based on html syntax 
analysis and structure-aware techniques that intend to 
make complex Web pages accessible and reflect the 
relative importance of Web components during the 
transcoding process.  
The design guidelines for such PDA devices are 
also introduced and discussed in [11]. These guidelines 
can be classified according to which aspect of the Web 
media they are related: software/hardware, content and 
its organization, or aesthetic and layout.  
On the other hand, refer to textual web content 
summarization [12], the methods for summarizing are 
introduced to handle the textual Web pages and HTML 
forms. A Web page is separated into text units that can 
each be hidden or partially displayed. Six different 
display modes are introduced that utilizes the 
progressive displaying textual units with keyword 
extraction and paragraph summarization to gain an 
overview of a page. They found that the combination 
of keywords and summaries provides the most 
significant improvements in access time and number of 
required pen actions.  
Usage-AwaRe Interactive Content Adaptation 
(UARICA) and Feedback-driven Context Selection 
(FCS) [13] made adaptation prediction for a user based 
on the history of the community of users and reflect 
both the user’s context and content’s usage semantics. 
Iqbal et al. think optimal adaptation is a challenging 
problem because it often depends on the usage 
semantic of content, as well as the context of users 
(e.g., screen size of device being used, network 
connectivity, location, etc.) Their works included an 
automatic techniques, UARICA that allows a user who 
is unsatisfied with the adaptation prediction to take 
control of the adaptation process and make changes 
until the content is suitably adapted for his/her 
purpose. Moreover, FCS takes advantages of user 
interaction to determine those contextual 
characteristics that have the most impact on the 
adaptation requirements of an object, and therefore 
should be the basis of grouping users into 
communities. 
 
3. Proposed Learning Content Adaptation 
Models 
 
Our proposed adaptation framework is composed 
of three main adaptation models. First, the textual 
adaptation model is responsible for handling the 
complicated textual body that may make users feel 
confused or lost while reading on a restricted small 
screen. Precisely speaking, our content adaptation tool 
will summarize textual body and utilize progressive 
disclosure presentation to revel the original content. 
We referenced Buyukkokten’s [12] progressive 
disclosure for text, it combined with keywords and a 
summary help present the original content 
incrementally, has the best improvement of average 
I/O expenditure and completion time across all tasks 
they had experimented. As shown in figure 3.1.  
 
458
 
Keyword extraction from a text body relies on an 
evaluation of each word’s importance. According to 
the idea captured in the TF/IDF measure. The 
importance of a word W is dependent on how often it 
occurs within the body of text, and how often the word 
occurs within a larger collection that the text is part of. 
Intuitively, a word in given text will be considered as 
the most important one if it occurs frequently within 
text, but infrequently in the larger collection. The 
formula is shown as follows: 
 
       Wij = Tfij * log2 N/n where                                   (2) 
       Wij = weight of term Tj in document Di 
       Tfij = frequency of term Tj in document Di 
       N = number of documents in collection 
       n = number of documents where Tj occurs at least once 
 
The documents of the formula must be modified as 
a individual package. The N of the collection in our 
case is learning content repository and the parameter n 
in this formula requires knowledge of all words within 
the collection that holds the text material of interest. 
For calculating each word’s importance, we need to 
construct a dictionary that contains the information of 
how frequently it occurs across course packages in 
learning content repository. To construct our learning 
dictionary, we identified word frequencies across all 
course packages that we had previously stored in our 
repository. 
 
 
Figure 3.2 Construct a dictionary of weighted words 
 
Figure 3.2 shows each step of constructing the 
dictionary of weight words from our learning content 
repository. It begins from the content parser which 
fetches learning courses from the repository and 
extracts all the words from each course, unless the 
frequent stopped words such as “is”, “are”, “and”, etc. 
Then, each uniquely extracted word will be tagged by a 
counter module with a number and keeps track of the 
number of courses where the word occurred. Once the 
counting is complete, the words that occurred less than 
a chosen threshold value across all the courses are 
eliminated. The value is required to be tuned because it 
depends on the size of the repository. It would 
conserve too many insignificant words if the value is 
too large. On the contrary, it is probable to remove rare 
words that may quite important and have the potential 
to become keywords. The remaining words are passed 
through a spell checker and finally, words that have the 
same grammatical stem are combined into single 
dictionary entries. For example adaptive and adapted, 
would share an entry in the dictionary. Accordingly, 
the size of the dictionary will continually shrink. 
When the significant keywords must be extracted 
from a PU, all the words in the PU are stemmed. For 
each word, the module will search the dictionary to 
discover the frequency with which the word occurs in 
the course. The word’s frequency within the course 
package that contains the PU is found by scanning the 
course package in real time. Finally, these values are 
computed for the word’s TF/IDF weight. Words with a 
weight beyond the chosen threshold are selected as 
significant. 
A special situation arises when a word is not in the 
dictionary, either because it was discarded during our 
dictionary-pruning phase or it was a specific word that 
has never been shown in other learning contents. Such 
words are probable more rare than any of ones that 
survived pruning and were included in the dictionary. 
Therefore these words are considered as special 
keywords in this course and as important as any of the 
words we retained. 
Finally, notice that our implementation directly 
extract and store the words as important ones if they 
are somehow highlighted with bold, italic, different 
color, specific punctuation marks, etc. 
 
3.2.2 Summary Sentence Extraction 
 
Rather than summarizing the input text 
automatically, we can only pick up a few significant 
sentences to represent the text summary. Because of 
the previously revealed keywords in a PU a user 
intends to explore the portion of the content’s summary 
due to his/her interesting. A sentence will be intuitively 
considered significant if it contains one or more 
keywords. Therefore, the method of extracting 
summary sentences is based on keyword extraction 
result. 
 
460
                  
Figure 4.1 Uploading learning contents to on-line 
adaptation application 
 
           
Figure 4.2 Adapting Content according to the pocket 
PC template 
 
The system will automatically accomplish the 
adaptation requirement according to the parameter 
configure of the template. Figure 4.2 demonstrates the 
html-based content has been adapted properly to be 
displayed in a PU and arranged by a default singular 
column layout. Next, users can manually delete the 
PUs, which contain relatively significant contents that 
have to be determined by users themselves. The 
adaptation result relies on configures of the template 
parameters as shown in left side of the figure 4.2. 
Users can adjust the adaptation parameters, including 
the value of the TID, size of a PU and PU 
rearrangement panel for changing the layout manually. 
 
 
         
Figure 4.3 Rearranging PUs’ presentation order 
 
Furthermore, the PU rearrangement panel 
illustrated in figure 4.3, allows users to change the PUs 
presentation layout. For example, it is probable there 
are several relevant PUs that are intended to be 
presented. Users are able to move the PUs to the top to 
capture the learner’s first sight. Users also have a 
flexible adaptation to decide which PU is relative 
important and where it should be located to satisfy the 
specific requirement for the displaying layout. Finally, 
we provide a handhelds simulator to let users browse 
the adapted content and simulate the reading 
environment on real handhelds shown in figure 4.4. 
 
  
Figure 4.4 Previewing the adapted learning content 
with pocket PC simulator 
 
 
5. Conclusion and Future Work 
 
In this paper, we develop a web-based content 
adaptation tool that focus on adapting html-based 
learning materials composed of texts and images. 
Instead of real-time adapting general Web pages on 
Internet, we believe automatic and manual adaptations 
are equally important for learning contents. Authors 
can not only utilize the proposed adaptation templates 
automatically and efficiently to adapt the learning 
content for handhelds but also adjust the template 
parameters to influence the adapted result, as well as 
the educational quality is assured by authors 
themselves. Although, the mobile devices are 
becoming more and more powerful, there still have 
been a lot of various multi-media resources can not be 
displayed properly on handhelds such as flash files and 
specific format videos. As a result, how to adequately 
adapt or represent the multi-media resources included 
in learning contents is our main future work. 
 
6. References 
 
[1] R. H. Katz. Adaptation and mobility in wireless 
information systems. IEEE Personal Communications, 
1994, pp. 1(1):6–17,. 
[2] T. Kindberg and A. Fox. System software for 
ubiquitous computing. IEEE Pervasive Computing, 
Jan. 2002. 
462
