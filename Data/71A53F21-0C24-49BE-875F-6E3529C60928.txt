 1 
行政院國家科學委員會補助專題研究計畫  
þ成果報告    
□期中進度報告 
 
2020 年教室:智慧教室、Web3.0 互動網路平台與 
雲端運算服務混生之未來動態學習生態體系研究 
 
 
計畫類別：þ個別型計畫   □整合型計畫 
計畫編號：NSC   100-2221-E-006 -224 
執行期間： 100  年 8 月  1 日至 101  年 7 月  31 日 
 
執行機構及系所：國立成功大學建築系 
 
計畫主持人：鄭泰昇 
計畫參與人員：沈揚庭、吳典育、許晏嘉 
 
 
 
成果報告類型(依經費核定清單規定繳交)：þ精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
þ出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國  101  年  9  月 10   日 
 3 
態智慧學習生態體系」(Dynamic Smart Learning Ecosystem)，此一體系依照教室、教具、教師、教材、
教法五大面向，分為五種創新學習環境研究構成元素： 
1. 智慧教室(Smart Classroom)：透過感應器與動態錄影裝置，隨時偵測教室內的活動，並完
整的記錄教室內的教學活動。 
2. 互動教具(Interactive Tools)：教室內的嵌入式教具形成物連網(Internet of Things)，得以利
用觸控或是自然的手勢，自然傳輸教室內學習的檔案與資訊。 
3. 第三位教師(The 3rd Teacher)：將 TED 18 分鐘演講模式，導入智慧教室，集體創造出網
路的第三位教師，透過數位串流機制，輔以 Web 3.0 社群網路，改變知識交換與創意傳播方
式；同時，以創意傳播方式，結合實體課程，進行創意思考、批判思考與問題解決。 
4. 雲端數位多媒體教材(Education Cloud Computing)：透過雲端運算將教學活動記錄上傳
至共通的教育雲平台，達到教學共享的目的，並利用雲端運算平台，使教育資源成為一個生
態系統，再從學校推展到整個終身學習型社會。 
5. 專題導向學習(Project-Based Learning)：在科技化教學的概念之中，藉由專題製作，在合
作學習中，建構並搜索問題的核心知識，提升解決問題、批判思考、創意思考等關鍵能力。  
 
三、  文獻探討  
近年來世界各國相繼推動科技賦權教育學習的理念(Technology Empowered Learning)，開始思考如何
研發新科技、或是利用既有的技術與產品，來打造未來創新型的教育學習環境。有關於本計畫之相
關研究，有許多來自工業界與學術界的驅動力，相關研究包括智慧教室、互動網路平台與雲端運算
服務三個方面。 
 
「未來教室」的研究，早在 1999 年美國喬治亞理工學院的 Gregory Abowd 教授率先領導”Classroom 
2000”研究計畫(Abowd, 1999)，此研究計畫的目標試圖擷取教室內上課與學習的經驗，Abowd 教授
在學院內建構一間實驗教室，每個上課的學生手持 ClassPad，可以直接與上課的白板相連溝通，課
堂上每一個人的筆記包括老師的註解，都被記錄下來放在課程網頁上，可以作課後的複習，接下來
智慧教室的研究，就都偏重於老師教課白板上內容的擷取與存取(Capture and Access)(Mukhopadhyay 
and Smith, 1999)(Brotherton and Abowd, 2004)(Burdet, Bontron, and Burgi, 2007)；另外，運用擴增實境的
未來教室(Cooperstock, 2001)、教室互動空間(Johnson, Fox, and Winagrad, 2009)、教育科技研究，包括
e-Learning (Clark and Mayer, 2007)(Horton, 2006)、網路教學(Shi et.al, 2003)(Deal, 2007)(Suo et.al, 2009)、
混成教學(Blended Learning)(Bersin, 2004)(Wang, Fong, and Kwan, 2009)(Snart, 2010)；最後，智慧教室科
技的調查整理相當完整的報告，詳見於(O’Driscoll, 2009)。 
 
基於以上的相關研究，本研究的方法偏向於將「未來教室」定位在「智慧教室」的建構，搭配互動
網路平台與雲端運算服務，有關於智慧教室空間相關的技術、理論與研究，大略歸納為下列幾項： 
• 彈性空間系統：當代的電腦輔助設計/製造(CAD/CAM)，利用 CNC 切割機對金屬模具及薄片
加工系統，輔助以數位製造(Digital Fabrication)的技術，加速空間的原型開發。除此之外，教
室的模組化與系統傢具，有助於教室之創新學習模式。 
• 人機介面系統：人機介面系統包含互動媒體與介面設計二方面，教室的互動設計，包含的不
只是科技的互動介面，還包括自動化擷取教學經驗與學生的學習經驗，改變學習環境的氛
圍。 
• 感應控制系統：感應控制系統包含感測網路與普及運算基礎設施(Singh, Bhargava, and Kain, 
 5 
 
 
3. 智慧教室互動模組研發 
智慧教室的互動模組，乃是本研究之核心技術之一，以下簡單描述本研究所研發的智慧教室互動模
組原型製作程序： 
l 可彈性變形之行動智慧白板：彈性變形智慧白板在其主體內設置一台小型電腦以及一台２４吋
薄型 LED 觸控螢幕，並融入展示需求，將其背面設定展覽板，同時成為即時互動區，其桌板腳架
具備彈性調整與移動滾輪之功能，完整結合智慧教室的多元資訊化。 
l 可彈性擴充之智慧化課桌椅模組：感測元件與智慧教室課桌椅組合之構成類型，可以透過不同
的關係整合，包括微算機，網路攝影機與微型投影機跟線性元件整合，RFID 與照明設備跟點元件
整合，電腦與面狀元件整合。透過將設備整合成點、線、面元件，即可使用元件的附加式與組合
式，改裝現有傢具使成為智慧化，或組合成新的智慧化家具套件。 
 
 
圖三：未來教室的智慧地板施作過程 
 
 
五、結果與討論  
本計畫最後實際在成大建築系耐震擴建大樓的三樓打造一個實體的「2020 年未來教室」，建構完成後，
並配合實際舉辦國際設計工作營，讓研究、教學與實驗同步進行場域實驗。本計劃之「2020 年教室」
取名為 iSTUDIO，已經完成的研究與實體建構項目包括： 
1. 可程式化教室(Programmable Classroom): 目前 iSTUDIO 的教室分為三個感應區塊，分屬於上課模式、
討論模式、評圖分享模式，管理者可以根據不同的使用活動，更換智慧地板位置，同時以程式改
變智慧地板的設定模式，智慧地板控制著不同的燈光與投影片放映模式，因此稱之為可程式化教
室。 
2. 互動網路平台(Interactive Web 3.0 Platform)：互動網路平台的「eTag」系統為全球首創，可以同步/
非同步針對演講 video 做註解，所有使用者的註解，將集體被集中收錄在雲端，可以作為教師回溯
研講的參考，目前正申請發明專利當中。 
3. 雲端運算服務(Cloud Computing Services)：iSTUDIO 之雲端服務建置在成大雲端中心，以加速其傳
播速度。 
4. 創意傳播(Creative Broadscating)：iSTUDIO 自從開放使用以來，廣獲學生喜愛，目前已經成為
TEDxTainan 聚集開會討論的場所。 
 
 7 
3. Shen, Y. and P. Lu, “Learning by Annotating: A System Development Study of Real-Time 
Synochronous Supports for Distributed Learning in Multiple Locations”, Proceedings of IEEE 
International Conference on Machine Learning and Applications (ICMLA), 2012. 
4. Hsu, Y, T. Jeng, Y. Shen, and P. Chen, “SynTag: A Web-based Platform for Labeling Real-Time Video”, 
Proceedings of the 2012 ACM Conference on Computer Supported Cooperative Work (CSCW), 
February 11-15, 2012, Seattle, Washington.  
5. Shen, Y. T. Jeng, and Y. Hsu, “A“Live”Interactive Tagging Interface for Collaborative Learning”, 
Lecture Notes in Computer Science (LNCS), 2011, Springer Verlag. [EI] 
6. Chang, S., T. Jeng, and Y. Yang, “Developing a Real-time Interactive Social Learning Platform Across 
Classroom Borders”, Proceedings of the 19th International Conference on Computers in Education 
(ICCE), T. Hirashima et al. (Eds.), November 28-December 2, 2011, Chiang Mai, Thailand.  
 
附錄三：專利申請 
 
1. 鄭泰昇，互動式視訊教學方法(iSTUDIO 未來教室)，發明專利，申請案號 100115070。 
2. 鄭泰昇，可變形多功能教具(iSTUDIO 未來教室) ，新型專利，申請案號 100207266，中華民國(100)
晉專一乙字第 71476 號。 
 
附錄四：媒體報導 
 
1. (專題採訪) 與最初的感動相遇：鄭泰昇教授談未來教室, 國科會《工程科技通訊》，第 149 期，
2011 年 9 月。 
 
2. 成大新聞:	 「成大全台首座未來教室問世」	 
『用創意驅動學習熱情，改變教育新創見，國立成功大學建築系副教授鄭泰昇帶領互動建築研究室團隊以未來 2020 年的教
育學習為夢想藍本，融合情境控制、變形教具與同步標記等創新技術，成功打造了全台第一間以數位製作結合高科技的未
來教室－iStudio，並提出教室即創意工坊、教室即攝影棚、教室即資訊站三個概念，將引領台灣的教育學習環境進入一個
全新的境界...』 http://news.secr.ncku.edu.tw/files/14-1054-75328,r81-1.php 
3. 自由時報：「互動科技教室	 成大創意打造」	 
『成大建築系副教授鄭泰昇帶領研究生以「創意智慧學習」為宗旨，打造出結合情境互動、資訊通訊、雲端等科技的教室
–iStudio，希望將教育學習環境帶入新領域。』 
http://www.libertytimes.com.tw/2011/new/feb/22/today-south21.htm 
4. 聯合報：「成大未來教室	 像影棚能互動」	 
『未來的教室是什麼樣子？成大最近在國科會補助下打造「未來教室」，除了有可當桌面、也能當白板的移動式白板，學生
還能在老師上課精采、或聽不懂時，像臉書般即時發出「讚」或問號的顯示，增加師生的互動。』 
http://udn.com/NEWS/DOMESTIC/DOM1/6167115.shtml 
5. 中央社:	 「成大全台首座未來教室問世」	 
 1 
國科會補助專題研究計畫項下 
出席國際學術會議心得報告 
                                     
日期： 101 年 9 月 8 日 
 
                                 
計畫編號 NSC  100-2221-E-006 -224 - 
計畫名稱 
2020 年教室:智慧教室、Web3.0 互動網路平台與雲端
運算服務混生之未 來動態學習生態體系研究 
出國人員
姓名 
許晏嘉 
服務機構
及職稱 
成功大學 
建築研究所 
會議時間 
2012 年 2 月 13 日
至 
2012 年 2 月 15 日 
會議地點 美國西雅圖 
會議名稱 
ACM Conference on Computer Supported Cooperative 
Work 
發表論文
題目 
SynTag: A Web-based Platform for Labeling 
Real-time Video 
 3 
講 Science, Technology and Society Revisited: What Is Happening to 
Anthropology and Ethnography，敘述人類學家對 CSCW 領域的貢獻，以及人類
學介入之後對數位科技的改變與影響。 
 
二、與會心得 
本次 CSCW 是近幾年來規模最大的，由此會議感受到了 CSCW 領域的多樣性。在三
天的行程中，每天早上都會有 Madness 的活動，就是當天的所有演講者輪流上
台，每個人在 25 秒以內簡單扼要地敘述研究內容，讓台下的觀眾能決定並選擇
要聽誰的簡報。因為人數太多，每天的簡報總共分成六個講廳舉行，每個講廳都
有不同的大方向研究主題。 
 
來參加會議的都是世界各地著名大學的頂尖學者，比如麻省理工學院媒體實驗室
(MIT Media Lab)就有許多論文在此發表，其中有一個是做互動的電子童書，小
孩與父母能以虛擬實境的方式出現在書中，並成為書中主角與對方互動，讓人意
識到科技也能用來增進親子的感情。另外一位是研究小孩之間會互相排擠與成群
的問題，研究哪種類型的小孩會自然地成為群體的領導者，有哪種類型的會被孤
立，因而發展出一種群體合作的方法避免太強勢方與太弱勢方的出現，進而提高
群體解決問題的效率。另外 Stanford 大學的一組研究了一個舞步學習系統，創
造了一個網路的跳舞交流平台，還可以分享舞步與教學。這些研究都是針對人類
的社會學與心理學出發，探討如何將科技運用得更人性化，也展現了科技並不只
是傳統用來解決問題的工具。 
 
在第一天晚上也有海報與現場展示的活動，學者們將自己的研究成果用海報的方
式呈現在一個廳中，此場地也備有自助 Buffet，可以讓大家在享用餐點的同時
放鬆心情的與其他人討論。在此會議中的研究人員都很熱情的與對方分享自己的
想法與研究。另外也有一些現場的展示，比如 Google 公司中做 google document
的部門展示的新共同合作平台，讓遠端的人可以更容易且人性化的透過網路攝影
機與網路平台互相修改與討論簡報。這個活動充分地展現出 CSCW 研究者們對互
相分享意見的重視，他們會傾聽別人的意見，也會勇於分享自己的看法。在互相
切磋之下讓研究能產生出更多有趣與有用的點子。 
 
SynTag:  
A Web-based Platform for Labeling Real-time Video 
Yen-Chia Hsu Tay-Sheng Jeng Yang-Ting Shen Po-Chun Chen 
Interactive Architecture Laboratory, Department of Architecture, 
National Cheng Kung University, Tainan, Taiwan 
{N76994017, tsjeng, N78941044, PA6984029}@mail.ncku.edu.tw 
+886-6-275-7575 ext. 54158 
 
ABSTRACT 
Real-time video streaming has been widely used in 
multimedia learning environments. As production of online 
videos is increasing exponentially, it is becoming more 
difficult for users to reach relevant content. In this paper, 
we propose SynTag, a web-based platform that enables 
users to label three types of tags—Good, Question, and 
Disagree—and to make comments synchronously and 
asynchronously with visualization of time-stamp video 
previews on an interactive timeline. SynTag generates real-
time thumbnails by using real-time tags for presenters to 
receive instant feedback and for other users to retrieve 
presentation videos. In a pilot study, we found our users’ 
tagging behaviors significantly different when they were in 
lecture events or discussion events. We envision that 
enabling users to apply tags in real-time will help reduce 
the complexity of classification of videos. 
Author Keywords 
Timeline Visualization; Collaborative Tagging. 
ACM Classification Keywords 
D.0 [Software]: General; J.4 [Computer Applications]: 
Sociology; H.5.1 [Multimedia Information Systems]: 
Video; H.5.2 [User Interfaces]: Graphical user interfaces, 
Interaction styles, Screen design, User-centered design; 
H.3.1 [Content Analysis and Indexing]: Indexing methods; 
H.3.3 [Information Search and Retrieval]: Information 
filtering, Relevance feedback, Retrieval models; H.3.5 [On-
line Information Services] 
General Terms 
Algorithms; Design; Experimentation; Human Factors. 
MOTIVATION 
Many learning environments record videos, but they lack 
efficient tools to record the interactions. The traditional 
education process tends to give students knowledge but not 
to encourage them to express their feedbacks, especially 
those which are implicit. This leads to information loss. 
Even if the feedbacks can be stored, most environments do 
not establish the relationships between audience feedbacks 
and the video. Even if presentations can be recorded 
properly, the current environments still lack an appropriate 
user interface for visualizing results. As a result, the 
retrieval process is complicated and time-consuming. 
People may need to spend tremendous time searching 
through videos aimlessly. 
This paper focuses on the problem of how to reduce the 
complexity of indexing video content by labeling video 
presentations in real time through a web-based platform in 
today’s multimedia learning environments. 
Related Work 
Some studies have addressed reducing the complexity of 
video classification by automatically or semi-automatically 
tagging the content in videos [1, 2]. However, SynTag uses 
collaborative tags as metadata which are designed to assist 
users with locating specific instances in a given resource [3]. 
Social tags yield an effective retrieval process, whereas 
automatically generated metadata do not [4]. As Marlow et 
al. [5] indicate, users attempt to express their opinions 
through the tags. 
Similar research on video- or audio-sharing systems has 
allowed users to make real-time tags manually [6, 7]. The 
EVA system [8] lets users assign collaborative tags to video 
key frames to enable efficient retrieval. Unlike these 
systems, SynTag provides instant feedback to the presenter 
through a thumbnail. The visualization of tags makes it 
easier for presenters to track audience feedback instantly. 
SynTag also provides timeline visualization for archived 
timestamps and displays the density of tags inside 
timestamps. Timelines connote a story rather than simply a 
logical collection [9]. 
METHOD 
The SynTag System 
In this paper, we propose a software named SynTag 
(ialab.tw/legenddolphin/TagDisplay16/TagDisplay16.html), 
written in Flex and AmCharts (www.amcharts.com). It can 
record and broadcast videos, alternate between two cameras, 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or republish, to post on servers or to redistribute to lists, requires prior 
specific permission and/or a fee. 
CSCW’12,  February 11–15, 2012, Seattle, Washington, USA. 
Copyright 2012 ACM  978-1-4503-1086-4/12/02...$10.00. 
 Type Description Date Scale Presenter Listener 
Total number of 
Tags/
min Time(min) Tags Effective Peaks 
A 
Disc. 
lab meeting  Mar 2 S graduates graduates 19.90 214 8 10.75 
B lab meeting Mar 2 S graduates graduates 68.03 768 29 11.30 
C TED Tainan Mar 29 M students students 177.77 366 12 2.06 
D 
Lect. 
design lecture Mar 1 S teacher sophomore 24.57 207 4 8.42 
E design lecture Mar 10 M alumni sophomore 30.82 136 4 4.41 
F official speech Mar 25 L professor citizens 20.24 279 10 13.78 
G official speech Mar 25 L professor citizens 40.96 590 16 14.40 
Average 54.61 365.71 11.86 6.70 
Table 1. The properties of events. Ten listeners used SynTag in each event. Lect. and Disc. in type column represent lecture and 
discussion. S, M, and L scales represent approximately 15-20, 30-35, and 60-70 persons, respectively. 
 Type 
% of associated number of tags % of associated number of effective peaks 
Good Que.+Dis. Question Disagree Good Que.+Dis. Question Disagree 
A 
Disc. 
43.93 52.34 52.34 0.00 50.00 50.00 50.00 0.00 
B 46.74 50.00 50.00 0.00 48.28 51.72 51.72 0.00 
C 46.45 48.90 19.67 29.23 41.67 58.33 25.00 33.33 
D 
Lect. 
85.50 6.28 6.28 0.00 75.00 25.00 25.00 0.00 
E 74.26 5.15 5.15 0.00 100.00 0.00 0.00 0.00 
F 71.68 13.26 11.83 1.43 90.00 10.00 10.00 0.00 
G 91.53 3.56 1.02 2.54 100.00 0.00 0.00 0.00 
Table 2. The analysis of different types of tags. Que.+Dis. on the top row represents combined Question and Disagree tags. 
Before beginning the recording, we asked 10 listeners 
willing to use SynTag to join the pilot study. The presenter 
then set up cameras and links to the lecturing service, 
entered information, and started recording. SynTag streams 
videos to the server for broadcasting. Listeners linked to the 
broadcasting service, started watching the live video, and 
expressed their feedback through SynTag.  
During the recording, SynTag ran the sorting algorithm to 
add nodes to the tag curve on the thumbnail in real time. 
The presenter could monitor the thumbnail and respond to 
listeners’ instant feedback, for example, by adjusting the 
speed of his presentation.  
After the presentation, anyone could link to the archiving 
service and apply tags asynchronously or retrieve particular 
video segments by clicking time nodes on the thumbnail to 
adjust the video time bar. 
Analysis 
We recorded 51 videos from January 26 to March 31, 2011. 
Of these videos, 4 were lectures, 3 were discussions, and 
others were demonstrations and tests. In this pilot study, we 
collected 7 recordings of lectures and discussions. The 
videos are analyzed by three steps:  
1. In Table 1, we classify event types and calculate the scale 
and total time of the event, the numbers of tags, and the 
effective peaks as shown in Figure 4 on the thumbnail 
visualization for video content indexes with the number of 
tags exceeding 70% of the number of people using SynTag. 
 
2. In Table 2, we calculated the percentage of associated 
number of tags and the effective peaks in different events.  
Figure 4. The circles indicate effective peaks.  
國科會補助計畫衍生研發成果推廣資料表
日期:2012/09/09
國科會補助計畫
計畫名稱: 2020年教室：智慧教室、Web3.0互動網路平台與雲端運算服務混生之未來動
態學習生態體系研究
計畫主持人: 鄭泰昇
計畫編號: 100-2221-E-006-224- 學門領域: 建築都巿
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
「未來教室」研發創意獲得國內眾多媒體報導： 
成大新聞: 「成大全台首座未來教室問世」、自由時報：「互動科技教室 成大創
意打造」、聯合報：「成大未來教室 像影棚能互動」、中央社: 「成大全台首座
未來教室問世」等報導(網頁見期末報告) 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
