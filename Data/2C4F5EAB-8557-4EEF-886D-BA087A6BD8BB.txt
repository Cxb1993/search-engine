on retrieval performance? Longer queries would often 
convey more information about 
user’s need, but also easily include ineffective 
query terms. 3) What impact do data collections have 
on the effectiveness of the query terms? A query term 
would be important for a certain data set but not for 
other data sets. 
We first plan to develop a machine learning-based 
method that ranks query terms according to their 
effectiveness on IR to deal with the three 
challenges. Several experiments will be conducted on 
the NTCIR ad-hoc IR tasks to evaluate the performance 
of the proposed method. Then we expect to apply the 
learning-based method to the application of 
estimating the effect of query translation on cross-
language IR. We are interested in realizing when a 
query term is required to be translated or not, and 
examining our method on the NTCIR CLIR tasks. 
 
英文關鍵詞： Query Term Selection, Query Reformulation, Query 
Performance 
 
摘要: 
 
當使用者有資訊需求，欲進行資訊檢索時，常面臨一嚴重問題：哪些關鍵字才是適合的查詢詞彙。過
去相關研究假設「查詢詞彙中名詞涵蓋較多的資訊」且「愈多查詢詞彙包含愈多資訊」，此衍生三個
問題：(1)查詢詞彙的詞性對檢索結果的影響為何？不同詞性的詞彙(如名詞、動詞、形容詞等)有不同
資訊描述能力及檢索效能，不同的具名實體(如人名、地名及組織名等)亦有不同檢索效能。(2)查詢詞
彙的多寡對檢索結果的影響為何？愈長的查詢通常較有能力描述資訊需求，但亦較易包含無效查詢詞。
(3)欲查詢資料集合對查詢詞彙檢索效能的影響為何？一查詢詞對某資料集在檢索上的重要性，不一定
適用於其它不同的資料集。 
 
本計畫預期建立一個「查詢詞檢索效能之預測」機制，採用機器學習技術，依檢索效能排序查詢詞，
探討上述三個問題，所提方法可選出較有檢索效力的查詢詞，以改善檢索結果。將上述成果應用於「跨
語檢索中查詢翻譯問題」上，原「查詢詞檢索效能之預測」的問題轉變成「跨語查詢詞翻譯重要性之
預測」問題，探討一跨語查詢詞在什麼情形下需要翻譯，預測結果將採標準測試集(NTCIR)評估。 
 
關鍵字: 查詢詞選取、查詢重製、查詢詞效能 
 
Abstract:  
 
This research project aims to investigate a technology for predicting the effectiveness of query terms on 
information retrieval (IR) and apply it to the application of estimating the effect of query translation on 
cross-language IR. When a user wants to search documents based on her information need, what comes into 
her mind is a set of relevant keywords to the need. She, therefore, faces a serious problem: what keywords 
would be good enough as query terms. Previous work assumes that nouns are often more informative and 
more query terms might carry more information. Such assumption, however, brings other three challenges: 1) 
what impact do the part-of-speech (POS) tags of the query terms have on retrieval performance? Different 
POS tags such as nouns, verbs and adjectives and different named entities such as person, location and 
organization names might affect retrieval performance differently. 2) What impact do the numbers of the 
query terms have on retrieval performance? Longer queries would often convey more information about 
user’s need, but also easily include ineffective query terms. 3) What impact do data collections have on the 
effectiveness of the query terms? A query term would be important for a certain data set but not for other data 
sets. 
We first plan to develop a machine learning-based method that ranks query terms according to their 
effectiveness on IR to deal with the three challenges. Several experiments will be conducted on the NTCIR 
ad-hoc IR tasks to evaluate the performance of the proposed method. Then we expect to apply the 
learning-based method to the application of estimating the effect of query translation on cross-language IR. 
We are interested in realizing when a query term is required to be translated or not, and examining our method 
on the NTCIR CLIR tasks. 
 
Keyword: Query Term Selection, Query Reformulation, Query Performance 
However, their focus is to evaluate performance of a whole query whereas we consider units at the level of 
terms. 
Given a set of possible query terms that a user may use to search documents relevant to a topic, the goal of 
this project is to formulate appropriate queries by selecting effective terms from the set. Since exhaustively 
examining all candidate subsets is not feasible in a large scale, we reduce the problem to a simplified one that 
iteratively selects effective query terms from the set. We are interested in realizing (1) what characteristic of a 
query term makes it effective or ineffective in search, and (2) whether or not the effective query terms (if we 
are able to predict) can improve IR performance. We propose an approach to automatically measure the 
effectiveness of query terms in IR, wherein a regression model learned from training data is applied to 
conduct the prediction of term effectiveness of testing data. Based on the measurement, two algorithms are 
presented, which formulate queries by selecting effective terms and dropping ineffective terms from the given 
set, respectively. 
The merit of our approach is that we consider various aspects that may influence retrieval performance, 
including linguistic properties of a query term and statistical relationships between terms in a document 
collection such as co-occurrence and context dependency. Their impacts on IR have been carefully examined. 
Moreover, we have conducted extensive experiments on NTCIR-4 and NTCIR-5 ad-hoc IR tasks to evaluate 
the performance of the proposed approach. Based on term effectiveness prediction and two query formulation 
algorithms, our method significantly improve MAP by 9.2% on average, compared to the performance of the 
original queries given in the benchmarks.  
In the rest of this report, we describe the proposed approach to term selection and query formulation in 
Section 2. The experimental results of retrieval performance are presented in Sections 3. Finally, in Section 4, 
we give our discussion and conclusions. 
 
2. Term Selection Approach for Query Formulation 
2.1 Problem Specification 
When a user desires to retrieve information from document repositories to know more about a topic, many 
possible terms may come into her mind to form various queries. We call such set of the possible terms query 
term space T={t1, …, tn}. A query typically consists of a subset of T. Each query term tiT is expected to 
convey some information about the user’s information need. It is, therefore, reasonable to assume that each 
query term will have different degree of effectiveness in documents retrieval. Suppose Q denotes all subsets 
of T, that is, Q=Power Set(T) and |Q|=2
n
. The problem is to choose the best subset ∆q* among all candidates 
Q such that the performance gain between the retrieval performance of T and ∆q (∆q ∈ Q ) is maximized: 
 
                                        (1) 
where pf(x) denotes a function measuring retrieval performance with x as the query. The higher the score pf(x) 
is, the better the retrieval performance can be achieved. 
An intuitive way to solve the problem is to exhaustively examine all candidate subset members in Q and 
design a method to decide which the best ∆q* is. However, since an exhaustive search is not appropriate for 
applications in a large scale, we reduce the problem to a simplified one that chooses the most effective query 
ineffective terms from space T based on function r. 
When formulating a query from query term space T, the Generation algorithm computes a measure of 
effectiveness r(ti) for each term ti T, includes the most effective term ti* and repeats the process until k 
terms are chosen (where k is a empirical value given by users). Note that T is changed during the selection 
process, and thus statistical features should be re-estimated according to new T. The selection of the best 
candidate term ensures that the current selected term ti* is the most informative one among those that are not 
selected yet. 
Compared to generation, the Reduction algorithm always selects the most ineffective term from current T in 
each iteration. Since users may introduce noisy terms in query term space T, Reduction aims to remove such 
ineffective terms and will repeat the process until |T|-k terms are chosen. 
2.4 Features Used for Term Selection 
 
Linguistic and statistical features provide important clues for selection of good query terms from viewpoints 
of users and collections, and we use them to train function r. 
 
Linguistic Features: Terms with certain linguistic properties are often viewed semantics-bearing and 
informative for search. Linguistic features of query terms are mainly inclusive of parts of speech (POS) and 
named entities (NE). In our experiment, the POS features comprise noun, verb, adjective, and adverb, the NE 
features include person names, 
 
Algorithm Generation  Algorithm  Reduction 
Input: T={t1,t2,…,tn} (query term space) 
k (# of terms to be selected) 
∆q { } 
for i = 1 to k do 
     
                     
∆q  ∆q     
   
T  T     
   
end 
Output ∆q 
 Input: T={t1,t2,…,tn} (query term space) 
k (# of terms to be selected) 
∆q { t1,t2,…,tn } 
for i = 1 to n-k do 
     
                     
∆q  ∆q      
   
T  T     
   
end 
Output ∆q 
Fig. 1. The Generation Algorithm and the Reduction Algorithm 
 
locations, organizations, and time, and other linguistic features contain acronym, size (i.e., number of words 
in a term) and phrase, all of which have shown their importance in many IR applications. The values of these 
linguistic features are binary except the size feature. POS and NE are labeled manually for high quality of 
training data, and can be tagged automatically for purpose of efficiency alternatively. 
Statistical Features: Statistical features of term ti refer to the statistical information about the term in a 
document collection. This information could be about the term itself such as term frequency (TF) and inverse 
document frequency (IDF), or the relationship between the term and other terms in space T. We present two 
methods for estimating such term relationship. The first method depends on co-occurrences of terms ti and tj 
that term-topic co-occur features are used in measuring the relationship between ti and query topic T-{  }. The 
co-occur features can be quickly computed from the indices of IR systems with caches. 
Term-term & term-topic context features: The co-occurrence features are reliable for estimating the 
relationship between high-frequency query terms. Unfortunately, term ti is probably not co-occurring with 
T-{ti} in the document collection at all. The context features are hence helpful for low-frequency query terms 
that share common contexts in search results. More specifically, we generate the context vectors from the 
search results of ti and tj (or T-{ti}), respectively. The context vector is composed of a list of pairs <document 
ID, relevance score>, which can be obtained from the search results returned by IR systems. The relationship 
between ti and tj (or T-{ti}) is captured by the cosine similarity between their context vectors. Note that to 
extract the context features, we are required to retrieve documents. The retrieval performance may affect the 
quality of the context features and the process is time-consuming.  
3. Experiments 
3.1 Experiment Settings 
Table 1. Adopted dataset after data clean. Number of each setting is shown in each row for NTCIR-4 and 
NTCIR-5 
 NTCIR-4 NTCIR-5 
 <desc> <desc> 
#(query topics) 58 47 
#(distinct terms) 865 623 
#(terms/query) 14.9 13.2 
 
Table 2. Number of training instances. (x : y) shows the number of positive and negative MAP gain instances 
are x and y, respectively 
 Indri TFIDF Okapi 
Original 674(156:518) 702(222:480) 687(224:463) 
Upsample 1036(518:518) 960(480:480) 926(463:463) 
Train 828(414:414) 768(384:384) 740(370:370) 
Test 208(104:104) 192(96:96) 186 (93:93) 
 
We conduct extensive experiments on NTCIR-4 and NTCIR-5 English-English ad-hoc IR tasks. Table 1 
shows the statistics of the data collections. We evaluate our methods with description queries, whose average 
length is 14.9 query terms. Both queries and documents are stemmed with the Porter stemmer and stop words 
are removed. The remaining query terms for each query topic form a query term space T. Three retrieval 
models, the vector space model (TFIDF), the language model (Indri) and the probabilistic model (Okapi), are 
constructed using Lemur Toolkit [21], for examining the robustness of our methods across different 
frameworks. MAP is used as evaluation metric for top 1000 documents retrieved. To ensure the quality of the 
training dataset, we remove the poorly-performing queries whose average precision is below 0.02. As 
different retrieval models have different MAP on the same queries, there are different numbers of training and 
test instances in different models. We up-sample the positive instances by repeating them up to the same 
number as the negative ones. Table 2 summarizes the settings for training instances.  
 Table 3. R
2
 of regression model r with multiple combinations of training features. L: linguistic features; C1: 
co-occurrence features; C2: context features 
Performance of 
Regression 
Model r 
One Group of Features Two Groups of Features  Three Four (3+1) All 
L C1 C2 L&C1 L&C2 C1&C2  L&C1 &C2 m-Cl m-SCS 
 
R
2
 
Indri 0.120 0.145 0.106 0.752 0.469 0.285  0.975 0.976 0.975 0.976 
TFIDF 0.265 0.525 0.767 0.809 0.857 0.896  0.932 0.932 0.932 0.932 
Okapi 0.217 0.499 0.715 0.780 0.791 0.910  0.925 0.926 0.925 0.926 
Avg. 0.201 0.390 0.529 0.781 0.706 0.697  0.944 0.945 0.944 0.945 
 
In the linguistic side, we find that two features “size” and “phrase” show positive, medium-degree 
correlation (0.3<ρ<0.5) with MAP. Intuitively, a longer term might naturally be more useful as a query term 
than a shorter one is; this may not always be the case, but generally it is believed a shorter term is less 
informative due to the ambiguity it encompasses. The same rationale also applies to “phrase”, because terms 
of noun phrases usually refer to a real-world event, such as “911 attack” and “4th of July”, which might turn 
out to be the key of the topic. 
We also notice that some features, such as “noun” and “verb”, pose positive influence to MAP than others 
do, which shows high concordance to a common thought in NLP that nouns and verbs are more informative 
than other type of words. To our surprises, NE features such as “person”, “geo”, “org” and “time” do not show 
as high concordance as the others. This might be resulted from that the training data is not sufficient enough. 
Features “idf” and “m-SCS” whose correlation is highly notable have positive impacts. It supports that the 
statistical features have higher correlation values than the linguistics ones. 
3.4 Evaluation on Information Retrieval 
In this section, we devise experiments for testing the proposed query formulation algorithms. The benchmark 
collections are NTCIR-4 and NTCIR-5. The experiments can be divided into two parts: the first part is a 
5-fold cross-validation on NTCIR-4 dataset, and in the second part we train the models on NTCIR-4 and test 
them on NTCIR-5. As both parts differ only in assignment of the training/test data, we will stick with the 
details for the first half (cross-validation) in the following text. 
 
Fig. 2. Three correlation values between features and MAP on Okapi retrieval model 
 
The result is given in Table 4. Evaluation results on NTCIR-4 and NTCIR-5 are presented in the upper- and 
lower-half of the table, respectively. We offer two baseline methods in the experiments: “BL1” puts together 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
ac
ro
n
ym
 
n
o
u
n
 
ve
rb
 
ad
j 
ad
v 
p
er
so
n
 
o
rg
 
ge
o
 
ti
m
e 
si
ze
 
p
h
ra
se
 
llr
 
llr
m
in
 
llr
m
ax
 
llr
m
in
_r
 
llr
m
ax
_r
 
p
m
i 
p
m
iin
c 
p
m
im
in
 
p
m
im
ax
 
p
m
im
in
_r
 
p
m
im
ax
_r
 
x2
 
x2
in
c 
x2
m
in
 
x2
m
ax
 
x2
m
in
_r
 
x2
_m
ax
_r
 
tf
 
id
f 
co
si
n
e
 
co
si
n
ei
n
c 
co
si
n
e_
m
in
 
co
si
n
e_
m
ax
 
co
si
n
e_
m
in
_r
 
co
si
n
e_
m
ax
_r
 
m
_C
l 
m
_S
C
S 
pearson kendall spearman 
shows the improvement percentage of MAP corresponding to BL1 and BL2. TFIDF and Okapi models have 
PRF involved, Indri model does not. Best MAP of each retrieval model is marked bold for both collections. 
Settings Method Indri TFIDF Okapi Avg. 
NTCIR-4 
<desc> 
Queries 
UB 0.2233 0.3052 0.3234 0.2839 
BL1 0.1742 0.2660 0.2718 0.2373 
BL2 0.1773 0.2622 0.2603 0.2332 
Gen-C 0.1949** 0.2823** 0.2946** 0.2572(+8.38%,+10.2)%) 
Gen-R 0.1954** 0.2861** 0.2875* 0.2563(+8.00%,+9.90)%) 
Red-C 0.1911** 0.2755** 0.2854** 0.2506(+5.60%,+7.46)%) 
Red-R 0.1974** 0.2773** 0.2797 0.2514(+5.94%,+7.80)%) 
NTCIR-5 
<desc> 
Queries 
UB 0.1883 0.2245 0.2420 0.2182 
BL1 0.1523 0.1988 0.1997 0.1836 
BL2 0.1543 0.2035 0.1969 0.1849 
Gen-C 0.1699** 0.2117* 0.2213* 0.2009(+9.42%,+8.65)%) 
Gen-R 0.1712** 0.2221* 0.2232* 0.2055(+11.9%,+11.1)%) 
Red-C 0.1645** 0.2194* 0.2084 0.1974(+7.51%,+6.76)%) 
Red-R 0.1749** 0.2034** 0.2160* 0.1981(+7.89%,+7.13)%) 
filter out noisy terms in original query even though good expansion terms are selected. Finally, note that we 
use the NTCIR-4 5-fold cross validation regression model, which is trained to fit the target performance gain 
in NTCIR-4 dataset, rather than instances in the query expansion terms set. However, results in Table 5 show 
that this model works satisfactorily in selection of good expansion terms, which ensures that our approach is 
robust in different environments and applications such as query expansion. 
 
Table 5. MAP of query expansion based on GET-C and GET-R model. (%) shows the improvement 
percentage of MAP to BL. Significance test is tested against the baseline results.  
Settings Method Indri TFIDF Okapi Avg. 
NTCIR-4 
<desc> 
BL 0.2470 0.2642 0.2632 0.2581 
GET-C 0.2472** 0.2810** 0.2728** 0.2670 
(+3.44%) GET-R 0.2610** 0.2860** 0.2899** 0.2789 
(+8.05%) NTCIR-5 
<desc> 
BL 0.1795 0.1891 0.1913 0.1866 
GET-C 0.1868 0.1904 0.1927 0.1899 
(+1.76%) GET-R 0.1880* 0.1918* 0.1945* 0. 914 
(+2.57%)  
Figure 3 shows the MAP curve for each scheme by connecting the dots at (1, MAP
(1)
), … , (n, MAP(n)), 
where MAP
(i)
 is the MAP obtained at iteration i. It tells that the performance curves in the generation process 
share an interesting tendency: the curves keep going up in first few iterations, while after the maximum 
(locally to each method) is reached, they begin to go down rapidly. The findings might informally establish 
the validity of our assumption that a longer query topic might encompass more noise terms. The same 
“up-and-down” pattern does not look so obvious in the reduction process; however, if we take the derivative 
of the curve at each iteration i (i.e., the performance gain/loss ratio), we might find it resembles the pattern we 
have discovered. We may also find that, in the generation process, different ranking schemes come with 
varying degrees of MAP gains. The ranking scheme “max-order” constantly provides the largest performance 
boost, as opposed to the other two schemes. In the reduction process, “max-order” also offers the most 
reduction. The reason for this greedy algorithm is that it is inappropriate to exhaustively enumerate all 
sub-queries for online applications such as search engines. Further, it is challenging to automatically 
determine the value of parameter k in our algorithms, which is selected to optimize the MAP of each query 
topic. Also, when applying our approach to web applications, we need web corpus to calculate the statistical 
features for training models. 
5. References 
[1] Allan, J., Callan, J., Croft, W. B., Ballesteros, L., Broglio, J., Xu, J., Shu, H.: INQUERY at TREC-5. In: 
Fifth Text REtrieval Conference (TREC-5), pp. 119--132 (1997) 
[2] Amati, G., Carpineto, C., Romano, G.: Query Difficulty, Robustness, and Selective Application of Query 
Expansion. In: 26th European Conference on IR Research, UK (2004) 
[3] Bendersky M., Croft, W. B.: Discovering key concepts in verbose queries. In: 31st annual international 
ACM SIGIR conference on Research and development in information retrieval, pp. 491--498 (2008) 
[4] Cao, G., Nie, J. Y., Gao, J. F., & Robertson, S.: Selecting good expansion terms for pseudo-relevance 
feedback. In: 31st annual international ACM SIGIR conference on Research and development in 
information retrieval, pp. 243--250 (2008) 
[5] Carmel, D., Yom-Tov, E., Soboroff, I.: SIGIR WORKSHOP REPORT: Predicting Query Difficulty - 
Methods and Applications. WORKSHOP SESSION: SIGIR, pp. 25--28 (2005) 
[6] Carmel, D., Yom-Tov, E., Darlow, A., Pelleg, D.: What makes a query difficult? In: 29th annual 
international ACM SIGIR, pp. 390--397 (2006) 
[7] Carmel, D., Farchi, E., Petruschka, Y., Soffer, A.: Automatic Query Refinement using Lexical Affinities 
with Maximal Information Gain. In: 25th annual international ACM SIGIR, pp. 283--290 (2002) 
[8] Chang, C. C., Lin, C. J.: LIBSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvm (2001) 
[9] He, B., Ounis, I.: Inferring query performance using pre-retrieval predictors. In: 11th International 
Conference of String Processing and Information Retrieval, pp. 43--54 (2004) 
[10] Jones, R., Fain, D. C.: Query Word Deletion Prediction. In: 26th annual international ACM SIGIR, pp. 
435--436 (2003) 
[11] Kumaran, G., Allan, J.: Effective and efficient user interaction for long queries. In: 31st annual 
international ACM SIGIR, pp. 11--18 (2008) 
[12] Kumaran, G., Allan, J.: Adapting information retrieval systems to user queries. In: Information 
Processing and Management, pp. 1838-1862 (2008) 
[13] Kwok, K., L.: A New Method of Weighting Query Terms for Ad-hoc Retrieval. In: 19th annual 
international ACM SIGIR, pp. 187--195 (1996) 
[14] Lioma, C., Ounis, I.: Examining the Content Load of Part of Speech Blocks for Information Retrieval. 
In: COLING/ACL 2006 Main Conference Poster Sessions (2006) 
[15] Mandl,T., Womser-Hacker, C.: Linguistic and Statistical Analysis of the CLEF Topics. In:  Third 
Workshop of the Cross-Language Evaluation Forum CLEF (2002) 
[16] Mothe, J., Tanguy, L: ACM SIGIR 2005 Workshop on Predicting Query Difficulty - Methods and 
Applications (2005) 
[17]  Vapnik, V. N.: Statistical Learning Theory. John Wiley & Sons (1998) 
[18] Yom-Tov, E., Fine, S., Carmel, D., Darlow, A., Amitay, E.: Juru at TREC 2004: Experiments with 
Prediction of Query Difficulty. In: 13th Text Retrieval Conference (2004) 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
達成目標 
□ 未達成目標（請說明，以 100字為限） 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中  無 
技轉：□已技轉 □洽談中  無 
 
表 Y04 
會議心得報告書                                                             
報告人姓名 戴瑋彥 就讀學校及
系所 
國立台灣大學資訊工程學系碩士
班二年級 
會議時間 2009/11/3~7 會議地點 香港 
會議 
名稱 
(中文) 第十八屆資訊與知識管理會議 
(英文) The 18th ACM Conference on Information and Knowledge Management  
發表 
論文 
題目 
(中文) 以詞彙依賴相關基礎之檢索查詢詞彙排序 
(英文) A Term Dependency-Based Approach for Query Terms Ranking 
報告內容應包括下列各項： 
一、參加會議經過 
於香港亞洲國際展覽館抵達會場後，先在報到處簽到並領取會議資料，之後參加隨後的
會議，並聆聽來自各地的與會者之論文發表。論文報告時間安排於隔天之下午最後一
場，報告結束後，至報到處領取 2009 資訊檢索相關會議之相關資料，以為日後投稿作
準備，至此四天的與會行程圓滿結束。 
 
二、與會心得 
此次參與第十八屆資訊與知識管理會議(以下簡稱 CIKM 2009)之行程，為學生第二次參
與國際會議。CIKM 2009 為資訊檢索領域中之重要的國際會議，每年均吸引眾多國際間
的學者投稿，競爭激烈。本次會議地點選於香港作為舉辦地點，來自世界各地之學者齊
聚一堂共同參與此次的盛會。有了之前的國際會議報告經驗，學生準備充分並抱持充沛
之信心參與並完成報告。完成報告後更受到資訊檢索領域著名學者 Bruce Croft 之青睞，
除了向學生詢問相關之研究議題外，更給予了學生許多建議及改進，著實讓學生獲益良
多。期間之茶會以及休息時間，學生亦與各國之研究生以及學者分享研究主題以及經
驗，更巧遇資訊檢索領域另一著名學者翟成祥教授。除了一般的會議報告，學生更參與
了海報會場(poster)，向各國學者討教許多有趣的應用和想法。在如 CIKM 這種著名之國
際會議下，能與各國學者參與討論並集思廣益，實為非常難得之經驗。在許多討論以及
研究議題分享後，學生本次在 CIKM 2009 的收穫非常豐富，除了學生之研究主題的想法
有更寬闊的思考，對於學生之未來研究方向也有極大之鼓勵及助益。 
 
三、攜回資料名稱及內容 
 
會議議程 
會議論文集(光碟) 
CIKM 2009 紀念袋、筆記本、資料夾、筆及相關與會紀念品 
 
四、其他 
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：鄭卜壬 計畫編號：99-2221-E-002-189- 
計畫名稱：查詢詞檢索效能之預測及其在跨語檢索中查詢翻譯上的應用 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 2 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 0 100%  
博士生 1 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
