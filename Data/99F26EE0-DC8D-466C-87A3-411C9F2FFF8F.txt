(二)中、英文摘要及關鍵詞(keywords)。 
 中文摘要: 
為協助電腦使用者觀看電腦螢幕，本研究設計出”直覺式視覺輔助工具＂來幫助視覺
上有困難的人在觀看電腦螢幕時的困擾。本研究使用目前已普及化的 Webcam 網路攝
影機作為主要的互動設備，結合人臉辨識技術(Face Tracking)，設計出基礎的視覺輔
助機制。Webcam 裝置於電腦螢幕的正上方中央位置，使用者在觀看電腦螢幕時，利
用人臉辨識技術追蹤使用者的頭部位置，當使用者因為看不清楚電腦螢幕上所顯示的
圖片或文字時，不自覺的將頭部傾向及貼近電腦螢幕，利用這個直覺的動作來啟動視
覺輔助機制，將電腦營幕畫面放大(Zoom in)，並追隨著使用者頭部的位置，使用者只
需要擺動頭部，便可以自由的放大和縮小螢幕畫面，以及在放大的畫面中上下左右移
動觀看，畫面的控制只需靠使用者頭部的動作來操作。視覺輔助工具將經由電腦使用
者實驗測試，評估使用效率及滿意度。這套工具，將可以減少電腦使用者觀看螢幕時，
使用鍵盤和滑鼠來做放大鏡的動作。可應用於非接觸式導覽、復健、互動廣告以及遊
戲等產業。 
 
關鍵字: 頭控式設備、人機介面、臉部追蹤 
 
Abstract: 
This study develops “Intuitive vision accessory tool” to help computer user who are 
difficult to watch contents of monitor screen. This research uses webcam as the main 
interactive equipment, with face tracking technology, to design the auxiliary mechanism of 
the vision. The user is while watching the screen of the computer. The tool uses webcam 
and face tracking to follow the trail of the head position. When user can’t clearly watch 
texts or images on the screen, user’s head will close to the monitor screen intuitively. This 
tool applies to this movement of intuition to a construct the auxiliary mechanism of vision. 
It also can make the display zoom in, and follows user’s head position, in control of it only 
needs of user’s head movements. The experiment will be tested and evaluated via computer 
user, this study can reduce usages of the keyboard and mouse by making movements and 
zoom in screen. The result of this project can be applied to navigator, game, interactive 
advertisement, physical medicine and rehabilitation. 
 
Keyword: Head-Controlled Device, Human-Computer Interface, Face Tracking 
computer users are familiar to use it. So, the authors use this kind of device to be the interactive 
interface between human and computer.  
 
1.2 Objects of This System 
According the above discussions, the authors must design a non-contact device for navigating the 
contents of computer first. Second, use the low-cost device, web-camera, to be the interface 
between human and computer. Third, people can use this system easily. 
 
1.3 Developing Steps 
For developing this system, the authors investigated the relative works, like the words 
visualization study, head-controlled devices and fact-attacking technologies. Then, the indicators 
of this system will be investigating by interface usability analyzing. The authors gathered the 
different distances about words visualization, then decide the ratios of the face width and distance 
by many testers. According these results, the developers set the parameters of this system for 
practical use. 
 
2. Relative Works 
In this section, the authors investigated the words visualization study first, and then the 
head-controlled device is the key point of this system and will be discussed in Section 2.2. 
Head-controlled devices technologies have Infrared-based, Ultrasonic- based, and Image-based. 
This system uses the face-tracking technologies that will be discussed in Section 2.3. Finally, the 
authors depicted the usability evolution technologies for deciding the indicators of this system in 
Section 2.4 
 
2.1 Words Visualization 
Sanders & McCormick proposed that the indicators of designing and displaying words are three 
items: visibility (detectability), legibility (discriminability), Readability (meaningfulness) 
(Sanders & McCormick, 1993). All of these items must be investigate in this system.  
 
2.2 Head-controlled Devices 
There are about three kinds of head-controlled devices: Infrared-based, Ultrasonic- based, 
Image-based. In recent years, the brain wave control has been developed rapidly, but the authors 
do not consider it for the high-cost and contacting head skin by wires.  
Evans, Drew and Blenkhorn had proposed an IR-based joystick to control computer (Evans, 
Drew & Blenkhorn, 2000). There is also a product, named TrackIR, using IR for 3D games to 
help player to switch the different views (Natural Point, 2005). 
 
2.3 Face-Tracking Technologies 
There are many face-tracking technologies like Neuron-network (Rowley, Baluja, & Kanade, 
1998), Pattern Recognition (Pappu & Beardsley, 1998), Hough Transform (Zhang & Mersereau, 
2000), and Shape Recognition (Birchfield, 1998). 
 
3. Implementation 
 
Figure 2. Testing Environment 
 
Figure 3 shows the face width developed by Macro Media Flash software. 
 
Figure 3. Face Width Testing 
 
Finally, we tested the moving speed of testers like figure 4. When user moves their head left, the 
contents shift to left area. In the opposite way, the contents shift to right. 
 
 
Figure 4. Moving Speed Testing 
 
Distance 50cm 100cm 135cm 150cm 
1 67 46 44 
2 82 53 44 
3 72 46 45 
4 81 50 43 
5 81 48 44 
6 77 51 44 
7 80 46 44 
8 68 44 43 
9 74 45 43 
10 83 49 45 
11 69 47 44 
Total 834 525 263 220 
Average 
(rounded) 76 48 44 44 
 
 
4.3 Moving Speed 
We change the speed by 30 pixels per unit. The testers adjusted speed to reading the Map of the 
screen like in Figure 4. In Table 3, Phase I means speed up slowly, while Phase II means speed up 
extremely. We set the distance at 1m because of the normal distance between face and screen 
when testers stand in front of webcam. We can see the 5 and 10 degrees are the average results 
that mean moving 150 pixels and 300 pixels are suitable for most testers. 
 
Table 3. Moving Speed Testing Results 
unit: 30 pixels 
Distance 1m 
Testers No Phase I Phase II 
1 3 5 
2 15 20 
3 3 7 
4 3 5 
5 3 8 
6 10 20 
7 4 10 
8 3 7 
9 5 13 
10 3 8 
11 8 12 
Total 60 115 
Average(rounded) 5 10 
 
c) Users can use web-camera easier than use mouse, especially for challenged users. 
d) The indicators, such as font size, distance, fact width, moving speeds are more useful to 
development navigator for head-controlled tools for human computer interface. 
 
REFERENCES 
Birchfield, S. (1998). Elliptical head tracking using intensity gradients and color histograms. 
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 232. 
Evans, D. G., Drew, R., & Blenkhorn, P. (2000). Controlling Mouse Pointer Position Using an 
Infrared Head-Operated Joystick. IEEE Transactions on Rehabilitation Engineering, 8(1), 
107-117. 
Natural Point (2005). http://www.naturalpoint.com/trackir/ 
Rowley, H. A., Baluja, S., & Kanade, T. (1998). Rotation Invariant Neural Network-Based Face 
Detection. IEEE Computer Society Conference on Computer Vision and Pattern 
Recognition, 963-963. 
Sanders, M. S., & McCormick, E. J. (1993). Human Factors in Engineering and Design. New 
York: McGraw-Hill Companies. 
Zhang, X., & Mersereau, R. M. (2000). Lip feature extraction towards an automatic 
speechreading system. 2000 International Conference on Image Processing, 2000. 
Proceedings, 3, 226-229. 
 
