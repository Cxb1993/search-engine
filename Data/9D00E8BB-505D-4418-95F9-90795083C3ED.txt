2研究計畫成果報告中文摘要
關鍵詞: 多模組融合, 人物偵測, 人物追蹤, 卡門過濾法
使用單一特徵之人物偵測系統很容易因為環境條件改變而失效；因此，許多
人物偵測系統同時考慮多個特徵，使用經驗法則來進行特徵間的整合；但是，這
些不勝枚舉的經驗法則大多是設計者根據其主觀的觀察所做的推測，缺乏理論根
據；而且，每當我們要增加或減少一個模組時，設計者必須重新調整其整合方式，
過程耗時費力。為了解決這些問題，我們設計一個快速、準確、穩定而且具有彈
性的多模組融合(multimodal fusion)人物偵測與追蹤系統，單一模組可以獨立
運作進行偵測，系統將使用卡門過濾器(Kalman filter)將個別模組的結果融合
在一起並進行追蹤，而多模組融合的結果將回饋給每一個模組，以作為其修正的
依據。
我們以循序漸進的方式逐步進行所規劃的研究項目，在第一年，我們根據顏
色、運動與距離三個特徵，分別開發三個使用單一特徵之人物偵測模組；就顏色
而言，我們設計一個可區別人臉與手腳的膚色偵測系統；就運動而言，我們開發
一個以時間差相減法為基礎之運動偵測系統；就距離而言，我們完成了以立體求
距產生深度圖，進而由深度圖偵測偵測的系統。於第二年，我們設計一個多模組
融合之演算法，整合各模組之偵測結果並進行追蹤。此外，為了提高整個系統的
速度，我們採用多工作站網路(network of workstations)的硬體架構，來實現
多模組即時偵測與追蹤的高速運算需求。
4目錄
一、前言……………………………………………………………………………5
二、研究目的………………………………………………………………………5
三、文獻探討………………………………………………………………………7
四、研究方法………………………………………………………………………12
研究成果(一)以顏色為根據之人物偵測模組…………………………………13
研究成果(二)以運動為根據之人物偵測模組…………………………………15
研究成果(三)以距離為根據之人物偵測模組…………………………………17
研究成果(四)多模組融合技術之研發…………………………………………26
研究成果(五)物件追蹤技術之研發……………………………………………31
研究成果(六)即時人物偵測與追蹤系統之實作………………………………35
五、結果與討論……………………………………………………………………39
六、結論與建議……………………………………………………………………52
七、參考文獻………………………………………………………………………53
八、計畫成果自評…………………………………………………………………59
附錄一、已發表之 IJIST (SCI)期刊論文………………………………………60
6因此，本計畫的目的即在於針對上述準確度與速度的技術困難點嘗試提出改
進的方案，並開發一個具備下列四項特性的多模組人物偵測與追蹤系統:
1. 準確性(accurate): 系統必須有高的偵測率(detection rate)與低的誤判
率(false alarm)，以達成可靠的人物偵測與追蹤。
2. 穩定性(robust): 系統必須可以在不同的採光與不同的背景下正常運作，而
不需要人為調整。
3. 速度(fast): 系統必須可以快速運作，以適應即時(real-time)應用之需要。
4. 彈性(flexible): 系統必須可以適應不同數量的模組數目，而不需要繁複的
修改。
如圖一所示，我們設計一個實用的即時人物偵測與追蹤系統。就穩定性而
言，我們設計根據顏色、運動與距離三個特徵之獨立人物偵測模組，彼此截長補
短以補償單一模組可能發生之誤判。就準確度而言，我們結合多個偵測模組來提
高偵測與追蹤的效果。就速度而言，我們以多工作站網路(Network of
Workstations)來達成即時系統的高速運算需求。就彈性而言，我們以多模組融
合(Multimodal Fusion)技術來適應不同數目與性質之偵測模組。
Color module
Motion module
Depth module
Multi-modal
integration
Camera input Kalman filter
tracking
圖一: 人物偵測與追蹤系統流程圖
8[40]中，作者提出一個用區塊平均的灰階值比較來取代對每一個 pixel 灰階值比
較來作運動物體的偵測，可藉由分析影像的曲線將運動物體的輪廓找出。在[24]
中，作者提出一個用背景影像相減(background subtraction)的方式將影像中的
背景減掉留下前景人物的即時人物追蹤系統，並用一個可隨時間更新的樣板
(template)偵測移動中的人物並更新樣板，再依人體的比例關係找出頭、手、身
體、及腳的相對位置。在[27]中，作者用 optical flow 的方式來偵測影像中移
動中的小物體，用影像相減的方式可能會因為差異太小而容易和雜訊混在一起，
作者藉由分析一連串影像中的特徵點來找出移動中的小物體。在[76]中，作者發
展一個即時的人物偵測系統 P-finder，這個系統使用 multi-class 的統計模型
並使用顏色及形狀特徵找出手及頭的位置；第二種方法將影像與事先準備好的背
景影像相減(background subtraction)[11,15,74,76] ；第三種方法嘗試透過移
動 向 量 的 分 析 將 前 景 與 背 景 區 分 開 來 (foreground/background
segmentation)[17,31,32,48,50]，此法常見於視訊壓縮的應用中；第四種方法
則是透過時空高斯過濾器(spatio-temporal Gaussian filter)[46]，同時在時
間與空間上做變動之偵測。
第一種方式(temporal differencing)可將靜止的背景濾除，但對於靜止中的
人物就無法偵測出來，當人物在下一時間移動的太遠也會造成多一個物件的陰
影，移動的不夠多也可能會被當成靜止的物體被濾除。相對而言，第二種方式
(background subtraction)可以有效濾出和背景不同的物件，但如果環境光線改
變或背景有所變動便會造成偵測錯誤。
3. 距離特徵(depth):
影像中每一個點的距離(或稱深度)的資訊亦可做為物件偵測[18,35,41,47]
的線索。立體求距(depth by stereo)的原理[61]是利用兩個不同位置的攝影機
所攝取的一對影像中，同一特徵點所在位置差(disparity)，來還原此特徵點與
攝影機的距離(Depth)，藉此獲得一個距離影像(Range Image)。由於通常影像中
之人物與背景物件有明顯的景深差異，因此距離特徵提供人物偵測非常重要的線
索。距離偵測的文獻可分為三個部分，第一個部分為攝影機校正(camera
calibration):在[49]中提出一套攝影機校正的軟體工具，可讓我們藉由拍攝不
同位置的棋盤格影像來找出攝影機的內部參數(intrinsic parameters)及外部
參數(extrinsic parameters)。
第二個部分為特徵區域搜尋:在[65]中，作者用環狀的遮罩(mask)產
生”SUSAN AREA”，然後對這個”SUSAN AREA”執行邊緣偵測、角落偵測過濾掉
一些雜訊即可找出我們要的特徵點(interesting point)。
第三個部分為關聯點搜尋(stereo matching)，並製作距離圖(depth map)，
depth map 主要的來源就是由兩張以上的影像搜尋影像中的對應點來還原其在
3-D 空間中的位置，其中最主要的問題就是對應點搜尋(corresponding point
search)，目前現有的對應點搜尋方法大致分成兩種，第一種方法是 local match
system[5]，先把兩張 stereo image 中其中一張的所有特徵點找出，再一一對每
一點搜尋另一張圖上對應的點，此種方法計算時間比較短，但因只考慮鄰近
(local)的點所以無法達到最佳化的效果，對於遮蔽的區域及單調材質區域的偵
測效果較差。第二種方法是 global search system[7]，先分別找出兩張圖的特
徵點，再對所有的點進行配對工作，此種方法對於遮蔽區域及單調材質區域有較
好的效果，整張影像的搜尋比對效果較佳，但計算時間比較長。在人物追蹤應用
10
的技術來克服上述的問題。在許多狀況下單一模組可能產生錯誤判斷，此時我們
可藉由其他模組的判斷來壓抑或修正此一錯誤，藉此作出正確的偵測。
多模組融合之目的在於找出最適當的方法將許多模組的判斷結合在一起，以
產生最佳的決策。多模組融合有許多的原因，分別列舉如下:
1. 實用上的原因: 如上所述，使用單一模組之人物偵測與追蹤系統大多缺乏穩
定度與準確度。
2. 生理上的原因: 人類本身做判斷時也會考慮多種線索，例如動態物體比靜態
物體更容易被發現，又例如同時用兩隻眼睛比單用一隻眼睛更容易發現物體。
3. 數學上的原因: 以兩個模組的決策為例，假設第一個模組的判斷是 z1(誤差σ1)
而第二個模組的判斷是 z2(誤差 σ2)，則使用最小平均平方差(minimum mean
square error)的融合方法可得到下列結果:
2
2
2
1
2
22
2
2
1
2
1
12
2
2
1
2
2
111









 zz
觀察上面的式子可以發現融合所得之誤差 σ比單一模組的誤差 σ1或 σ2都還來的
小。以此類推，多模組融合之結果必定比單一模組之結果準確。
多模組融合之時機一般可分為資料融合(data fusion)、特徵融合(feature
fusion)與決策融合(decision fusion)三個層次。資料融合[20]是最低階層的融
合，將不同來源但具備相同屬性的未經處理的資料直接整合在一起，資料融合的
優點是可以充份利用各模組間資料的關聯性，缺點則是對雜訊非常敏感。特徵融
合[6]是中間階層的融合，每個模組各自將資料處理成有意義的特徵後，再將各
個模組所得之特徵結合在一起，特徵融合比較不受雜訊干擾，可是結合許多特徵
通常需要大量的計算。決策融合是最高階層的融合，每個模組各自做完決策後，
再將所有的決策整合起來，決策融合[62]的優點是可以容忍單一模組的失誤，缺
點則是無法利用各模組間資料的關聯性。在本計畫中由於資料來源相同，導致資
料融合不具意義，而決策融合又缺乏彈性，因此，我們使用特徵融合的技術來整
合多模組人物偵測的結果。
多 模 組 之 特 徵 融 合 依 其 輸 出 之 種 類 不 同 又 可 分 為
FIFO(feature-in-feature-out)與 FIDO(feature-in-decision-out)兩大類。前
者將各模組所得之特徵結合成一個整體特徵，以做為後續決策模組判斷的依據，
後者則將各模組所得之特徵整合之後，直接做出判斷。綜觀文獻中的方法，卡門
過濾法屬於前者，而類神經網路(ANN)與馬可夫模型(HMM)則屬於後者。
類神經網路(Artificial Neural Network)[33,62]的缺點是成功的融合網路
通常需要冗長的訓練過程，而且典型的類神經網路不適合用來處理會隨著時間而
改變的特徵。馬可夫模型(Hidden Markov Models)[55,56,63,67]的限制在於個
別觀察的機率完全獨立(independent)而每一個狀態所發生的機率只由前一個狀
態決定，無法將連續數個狀態的關聯性列入考慮。由於各人物偵測模組間彼此有
複雜的關聯性，因此，並不適用類神經網路與馬可夫模型。相對地，卡門過濾法
12
四、研究方法
第一年我們已針對人物偵測相關文獻做深入的研究，同時我們設計由單一特
徵來進行人物偵測的方法，並分別實作三個獨立之人物偵測模組: 顏色、運動、
與距離模組。第二年我們已完成多模組融合之演算法，整合各模組之偵測結果並
進行追蹤，而多模組融合的結果則回饋給每一個模組，以作為其修正的依據。此
外，為了提高整個系統的速度，我們採用多工作站網路(NOW: network of
workstations)的硬體架構，來實現多模組即時偵測與追蹤的高速運算需求。本
計畫兩年期間已完成之研究成果整理如下:
【第一年】
1. 以顏色(Color)為根據之人物偵測模組
 白平衡與膚色唇色模型參數之調整
 膚色各部位(人臉與手腳)之幾何式分辨
2. 以運動(Motion)為根據之人物偵測模組
 背景顏色擾動的差值之分析
 時間差相減之運動偵測演算法之設計
3. 以距離(Depth)為根據之人物偵測模組
 攝影機精密定位與校正(calibration)演算法之研製
 漸進式關聯點搜尋(correspondence)與深度圖之建構
 深度圖中人物偵測演算法之研製
【第二年】
4. 多模組融合(Multimodal Fusion)技術之研發
 以卡門過濾法為基礎之多模組融合演算法之研製
 錯誤回復機制之設計
5. 物件追蹤 (Object Tracking) 技術之研發
 以卡門過濾法為基礎之物件追蹤演算法之研製
 主動式攝影機控制機制之設計
6. 即時人物偵測與追蹤系統之實作
 使用多工作站網路(NOW)之高速運算平台之建構
 偵測與追蹤系統整體效能之評估
14









BGR
G
g
BGR
R
r
3.00743.13767.1)( 2  rrrS
1766.05601.0776.0)( 2  rrrS
22 )33.0()33.0(  grW
6.02.0
0
))(&)009.0(&)(&)((1



  
r
otherwise
BGRWSgSgif
S
嘴唇顏色分布我們使用[66]的方法再針對我們的攝影機調整係數，其唇色模
型如下:
165.05601.0776.0)( 2  rrrL


  
otherwise
WrLgSgif
M
0
))03.0(&)6.02.0(&)(&)((1
使用膚色過濾可以找出人物的膚色的部分，包括臉部、手掌、手臂、腳。由
於人物的動作可能造成手及腳的來回擺動使定位的動作不易進行，所以我們只針
對頭部人臉的部分來做人物位置的定位，手部及腳部及其他非臉部的膚色區域則
必須從辨識出的膚色區域濾除，所以我們先用 connected component labeling
將每個區域分開，再分別找出包圍每個區域的 bounding box，並計算每個區域
的 pixel 數目，如果小於 10 則將此區域視為雜訊，由於人臉的形狀基本上大約
趨近於正方形，再考慮脖子的區域，所以當此區域的下界在影像上半部且
bounding box 的長寬比 a在 0.8 到 2.5 之間時，則此區域視為人臉的候選區域，
接下來搜尋人臉候選區域的下半部，算出此區域的嘴唇顏色分布此區域 t%，我
們將設定此偵測模組的標準差 1
5.1
5.1
21 
 ss ktk
a ， 1sk 為 bounding box
長寬比差值所佔的權重=1， 2sk 為唇色區域比例所佔的權重=0.5，然後傳出此
bounding box 區域的中心座標(x,y)及做為顏色模組的結果。
r
g
S
S
圖三: 膚色分布圖
16
Temporal differencing
Perform 3*7 dilation in order to connect the
broken regions
Use connected component labeling to segment
the moving pixel area
Find bounding box of each region，if ((area of
bounding box>1500)or(1.7<ratio of bounding
box<7)) then set this region as candidate
圖四: 運動偵測流程圖
18
CM就是 camera matrix，包含攝影機的內部參數。對於一般非廣角的攝影機鏡頭，
我們可以假設放射狀扭曲的係數 5k =0，假設攝影機的 pixel 為矩形則=0，可簡
化運算的複雜度。
首先，我們必須先大致對準兩台攝影機的中心，所以我們把兩台攝影機架設
在視差器及腳架上在夜間拿到戶外以利對遠方的亮點進行定位。首先，先將裝有
攝影機及視差器的腳架先用水平儀調整到水平，再將其中一台攝影機用水平儀調
整水平後中心對準遠方的亮點，以這台攝影機位置為基準，再調整視差器將另一
台攝影機中心對準同一個亮點，當此亮點的距離足夠遠時，兩台攝影機主軸的角
度可近似於零，再利用此亮點附近的其它亮點的相對位置來確定兩台攝影機是否
有旋轉:
接下來將主軸定位好的攝影機拿到室內要偵測的定點，將腳架用水平儀調整到水
平，到此完成兩台攝影機大致的校正，接下來我們要利用”Camera Calibration
Toolbox for Matlab”攝影機校正軟體工具來做更精密的定位，並算出攝影機的
內部參數和外部參數。
我們將預先準備好的棋盤格放在攝影機前，把棋盤格放在攝影機前不同的位
置及角度拍攝，再從這些拍攝好的棋盤格影像依序手動抓取棋盤格上面的邊角，
流程如圖五所示。
依此方法將 23 張影像中的棋盤邊角抓出後，即可算出不包含鏡頭扭曲的校
正參數，接下來可把上面所算出的參數大約的估計直拿來當初始值，作非線性的
最佳化將誤差值(最小平方差) 降到最低，即可算出所有參數的最終估計值，這
些最終的估計值大約三倍的標準差。
將左邊攝影機的內部參數鏡頭焦距、鏡頭中心、偏斜係數、放射狀方
圖五: 用 corner operator 找出影像中邊角的真正位置
20
利用物體距離攝影機的差別來擷取出影像中的人物，並依距離的不同來分辨影像
中不同的人物，在距離模組中，我們的輸入資訊是兩台主軸平行且只有水平 x
軸方向位移的攝影機影像，我們可以藉由影像中對應點的位移量找出人物相對於
攝影機的距離，輸出影像中人物的 x,z座標，其流程圖如圖八所示。
2. 關 聯 點 的 搜 尋 與 深 度 圖 的 建 構 (correspondence and depth map
construction)
由於圖形中沒有材質的區域對於對應點搜尋是非常困難的，所以我們要先將
有材質和沒有材質的區域分開，首先，我們先將整張圖解析度長寬各降五倍後，
用 sobel edge detector 對整張影像做處理，再取一個 threshold 依 edge 強度
將比較小的值濾掉視為沒有材質的區域，剩下來的區域視為有材質的區域，再將
此張區分材質區域的圖還原到原來的尺寸，接下來再針對有材質的部分去做關聯
點搜尋。
找到有材質的區域後，接下來就是將這些區域的點對應到另外一張影像上，
先將影像轉為灰階後，我們把有材質的區域所包含的點視為特徵點，利用
epipolar geometry[61]的特性搜尋以特徵點為中心水平方向的 epipolar
line，此搜尋區域(search area)的大小正比於影像的解析度及兩台攝影機水平
位移(baseline)，當我們所要測量的距離範圍比較大時我們需要比較大的
baseline，我們設定最遠的測量距離在 5公尺左右，此時同一特徵點在兩台攝影
機擷取的影像必須有約 7~8 個 pixel 的位置差(disparity)，所以我們將
baseline 定為 15 公分，攝影機解析度為 1380*1030，為加快運算速度，因此我
們將實際處理影像的解析度降為 276*206，此時在距離攝影機 1.2 公尺時有最大
disparity 為 30 個 pixel，並假設有垂直方向上下最多 1個 pixel 的誤差，所以
設定 31*3 的 search area。我們為了加速運算，我們用最小 3*3 的樣版(compare
area)在此 search area內做 SAD(sum of absolute difference)比對。將 compare
area裡的每一點亮度相減取絕對值，再將整個 compare area內每個點的差值相
加算出 ),,,( 2211 yxyxD ，最後在 search area 內取一個比對出來差異值最小的區
域 ),( 11 yxDs 視為對應點，計算方式如下:
    11 11 2221112211 ),(),(),,,(  i j jyiximagejyiximageyxyxD
)),,,((min),( 2211
),(
11
22
yxyxDyxD
areasearchyx
s 

當上面樣板比對完成後，如果找到的對應點差異度 ),( 11 yxDs 大於我們所定的
threshold 時，則視為此點搜尋失敗。如果找到的對應點差異度 ),( 11 yxDs 與對應
點附近的差異度 ),,,( 2211 yxyxD 相似的點數 ),( 11 yxS 超過(search area)/3 時，就
表示此點附近的特徵都很相似，所以即使對應點差異度 ),( 11 yxDs 很小也是視為搜
尋失敗，最後搜尋到的對應點如圖七。





elseyxs
areacompareyxDyxyxDifyxs s
0),(
)(*3),(),,,(1),(
22
11221122



areaserachyx
yxsyxS
),(
2211
22
),(),(
兩張圖的點對找出後，接下來就是針對找出來的點對來找出相對於攝影機的
距離，從圖八中可知，影像中每一點相對於攝影機的距離 z和此點對的disparity
d 成反比，相關係數定義如下:點對在影像中的位移 d= ld + rd ，攝影機焦距 f，P
22
3. 深度圖中人物之偵測 (human detection in depth map)
有了上一步驟做出來的 depth map 後，我們就針對這張 depth map 來分析找
出影像中的人物。我們希望能從 2-D 的圖形還原 3-D 位置。在文獻[4]中，使用
background subtraction 的方法在沒有人物出現時先將背景的 depth map 預先
儲存起來，以後偵測時先將預先儲存的背景 depth map 減掉，再由剩下的前景圖
形經由樣板比對來分析人物的位置，background subtraction 的好處是可以節
省許多分析不必要背景的時間及分析的影像可以比較簡單，但如果屬於背景的物
件在系統追蹤的過程有移動或改變顏色，或追蹤的過程中環境光線有所改變，則
此法將會失效。所以我們使用一個可以適用於大部分環境變化的方法，我們將使
用 depth map 還原攝影機所照到影像中每一個物件的三度空間位置的(x,z)座
標。流程圖大致如圖八所示。
原本我們使用的方法是先將整張 depth map 依 disparity 由大到小由近到
遠分成 30 層，分別為 303,2,1 d ，由近到遠分別去分析每一層 d的分布狀況，
使用 connected component labeling 先將此層 d 所包含的物件分開，再找出每
個物件的 bounding box，再由 bounding box 的長寬比來判斷是否為人物，但如
果每一層都做 connected component labeling 的動作將非常的耗時，而且如果
有兩層以上的 d 找到的 bounding box 有交錯時還必須考慮合併或取捨的問題。
為了節省運算時間及降低分析影像的複雜度，我們將整個分析分成五個步驟:
(1)分層:將 depth map 分成 d=1~d=30，並將每層存成 binary image，如圖
八(1)所示。
(2)對每一層人物可能出現的區域做垂直方向 histogram:如圖八(2)，我們
只在人物可能出現的區域 du 到 dl 間分析，由人物頭頂及腳底兩條界線所圍
出的區域就是人物在影像中可能出現的區域，所以我們只要在此區域做垂直
方向的histogram統計垂直方向pixel數即可判斷其是否為人物高度找出人
物所在位置。因此，我們必須找出人物在影像中的位置上界 u、下界 l作為
histogram 的上下界。假設在每一層 d所對應到的人物在影像中的上界 du 、
下界 dl ，人物實際三度空間的上界 dU 、人物實際三度空間的下界 dL 、人物
所在深度 z的影像 FOV 上下界距離 dH 。由比例關係我們可以找出 du 、 dl 、
h、 dt 及 dU 、 dL 、 dH 、T的關係，我們可以藉由分析上述關係算出 dU 、 dL 、
dH 的值。人物實際高度 T、寬度 W、深度 E的定義如圖九。我們可以找出 du 、
dl 、h、 dt 及 dU 、 dL 、 dH 、T間的關係如下:
h
H
L
l
d
d
d  )( , hH
U
u
d
d
d  )( , hH
T
t
d
d  )(
h 為影像的高度 pixel 數=206。我們可以由攝影機側視圖投影面找出
dU 、 dL 、 dH 和 dz 的關係如下:


 


else
)
2
cot(if)
2
tan(
d
dd
d
d
H
czcz
L
L 


 


else0
)
2
cot()(if)()
2
tan(

cTzcTz
U
U
dd
d
d
24
zds 為在每一個距離 z= z所對應到的 disparity。實際偵測到的 wi 經
smoothing後 305 ~ zz 的俯視圖如圖八(5)，可看出人物大致的位置，靠近攝
影機的亮處為人物的位置，後面的亮處為背景。再對這個 smoothing後的俯
視圖取一個 threshold P，這個 P正比於 W平方，將前面人物較亮的地方取
出，將背景較暗處過濾掉。再做 connected component labeling 找出裡面
不相鄰的物件，每一個不相鄰的物件就代表一個人物可能的位置。
用這種俯視圖的方式加上smoothing分析人物位置可以解決同一人物分
散在不同 dz 的問題，如果同一人物 histogram 存在相鄰層附近 smoothing
步驟就會加強相鄰層的亮度值使其不會被後面的 threshold P 過濾掉，而相
鄰層經由 smoothing 加強亮度步驟及 connected component labeling 分析
後仍視為同一個物件，所以同一人物在兩相鄰層同時被偵測到也不會有
bounding box 交錯的問題。經由距離偵測模組我們可以得到影像人物的 x,z
座標，影像座標及攝影機相對關係，我們設定此偵測模組的標準差
1 dkW
Wp ， dk 設為常數=0.9，p 為偵測到物體 bounding box 的寬
度如圖九，我們傳出人物中心的 x,z座標及做為深度模組的結果。
26
研究成果(四)、多模組融合(Multimodal Fusion)技術之研發
單一模組很可能在特定環境下失效，因此，我們設計了多模組融合的演算法
來增加系統的穩定性與準確度；另外，我們所開發的多模組融合系統必須要有足
夠的彈性可以適應不同數量的模組；最重要的，多模組融合系統必須要能在極短
時間內完成以符合即時應用的需要。卡門過濾器是一套用來解決最佳化問題的遞
迴運算架構，在許多不同的應用上都有相當令人滿意的結果。卡門過濾法
[6,45,62,75]的基本假設是一個線性(linear)系統受到白色(white)與高斯
(Gaussian)分布的雜訊干擾，能符合人物偵測多模組融合的需求，因此，我們使
用卡門過濾器來整合相同時間不同模組間之特徵。
假設 n 代表的是模組的編號，第 n 個模組實際觀察到的特徵點的座標是
(z1(n),z2(n))，n個模組融合的結果是(x1(n),x2(n))。卡門過濾器將分別對水平
軸與垂直軸做獨立之融合。卡門過濾器可分為預測與修正兩個物驟，在預測
(prediction)階段，根據前 n-1個模組的觀測值，第 n個模組的預測值是:
)1()1()(
)1()(




nQnPnP
nxnx
ii
ii
在修正(correction)階段，考慮第 n個模組的觀測值，針對預測值做修正，n個
模組融合可得到下列結果:
)())(1()(
)(
)(
)(
))()()(()()(
nPnKnP
RnP
nP
nK
nxnznKnxnx
iii
i
i
i
iiiii






其中 Q與 R分別代表觀測協方差(measurement noise covariance)與程序協方差
(process noise covariance)。觀測協方差可根據各個模組本身產生結果的誤
差，事先計算得知；觀測協方差也可以根據需求與狀況不同而動態改變，各模組
的觀測協方差與偵測結果的把握度（Conference Factor）成反比。程序協方差
無法事先得知，通常可將其設為零或一個很小的值(例如 0.00001)。透過 Q 與 R
的設定我們可以將多模組融合的結果調整到最佳狀態。另外我們也要給定一個 P
的初始值 P(0)，通常可將其設為一個不等於零的任意數，初始值無論給定的數
值大小，最終將自動收斂到正確的結果。
多模組融合的一個重要的問題是對模組間衝突(discordance)之處理，各個模
組獨立判斷之結果可能會有彼此不協調或衝突的狀況產生，不協調或衝突的原因
可能有物件遮蔽、光線差異、硬體或軟體失誤等。針對衝突處理之方式將對融合
的成功造成很大的影響，模組間衝突之處理一般可分為四種，分別列舉如下:
1. 無論衝突與否，一律融合在一起。
2. 通知不協調的模組，使其重新做判斷。
28
圖十二: Sensor C 的 probability density 分布
假設上面三個 probability density 分布為在同一個時間的測量值，則可以
推導出:
c
cba
ba
b
cba
ca
a
cba
cb zzzu )
)(2
()
)(2
()
)(2
(
222
22
222
22
222
22












)
1
()
1
()
1
(
1
2222
cba 

則最佳的估計值由圖十三的分布圖中的中間值(median) u 即為最佳的估計
值，所以我們判定最後的位置 z =u。
圖十三: 由 probability density 分布圖判斷出最佳的估計值 u
經由上面的整合計算過程後，我們可以得到位置估計值 z 以及，由上面的
推導我們可以知道一定比單獨 a或 b或 c還要小，由此可知經由整合後的位
置估算值正確率會比只用 sensor A 或 sensor B 或 sensor C 還準確。所以我們
依此原理整合距離、膚色及運動三個模組的輸出。首先，我們必須訂出哪些變數
由哪些模組提供輸出進行整合，距離模組輸出(x,z)，顏色模組輸出(x,y)，運動
偵測模組輸出(x,y)，只有 x 座標是三個模組都輸出，所以我們只針對 x 座標用
Kalman filter 做整合，在偵測過程中顏色模組的 y值明顯比運動模組的 y值正
確率高，所以 y 座標由顏色模組決定，如果顏色模組失效才由運動偵測模組決
定，z座標則由距離模組單獨決定。
Probability density
x
c
cz
x
Probability density
b
a
bz

az u
c
cz
30
圖十四: Kalman filter 選取影像中整合物件流程圖
Input all candidates detected by all
sensors, sort all candidates by 
If available, choose a
candidate with the
smallest 
Set the width of search window as w; if
there is no z value in this object, then
set w as default value 40
Use Kalman filter to integrate all
candidates in the search window
Delete all candidates in the search
window and save the integrated
position in the detect table
Start
End
32
在前一個工作項目中，Kalman filter 可以用來整合多個模組的輸出作最佳
化的判斷，在[39]中提到我們也可用相同的原理利用 Kalman filter 用現在的位
置來預測未來可能活動的軌跡。首先，我們必須假設人物活動的過程是等速度的
運動，則人物的加速或減速都必須視為雜訊，所以我們假設:
wudtxd /
u 為人物的預測運動速度，w 為真實運動速度 dtxd / 和預測運動速度u 的誤
差，我們假設w 為符合”White”且”Gaussian”的雜訊，其平均值為零標準差
為 w ，則我們可以用上一時間的位置 )( 2tx 及預測運動速度u 來預測下一時間 3t
的預測位置(如圖十五所示)，其預測位置 )( 3
tx 其標準差定義 )( 3
tx 如下:
)()()( 2323 ttutxtx 
)()()( 23
2
2
2
3
2 tttt wxx  
當時間 3t 偵測模組偵測到人物位置 3z 其可信度為 3z 後，我們在 3t 時間點就
有兩個可能的估計值由偵測前軌跡預測的 )( 3
tx 及偵測值 3z ，此時可用第五章類
似的方式修正時間 3t 的估計值，修正方式及結果如下:
322
3
2
3
322
3
2
3 )
)(
)(
()()
)(
()(
33
3 z
t
t
tx
t
tx
zx
x
zx
z







 



)
1
()
)(
1
(
)(
1
22
33
2
3zxx
tt 
 
)( 2tx
Probability density
x
)( 2tx )( 3
tx

u
圖十五: 用 )( 2tx 及u 預測下一時間

3t 的位置 )( 3
tx
)( 3
tx
34
Start
Multi-modal integration detect people in image
If (candidate in track table=0
&&candidate in detect table=0)
If there is any candidate in
detect table that is not found
by candidate in track table
Add this candidate to track table
Detect table Track table
true
true
false
false
If any candidate in
track table that >3
Delete this candidate in track table
Go to next frame
true
false
For each candidate in
track table
If find candidate in detect
table within search region
Use kalman filter combine detect and
predict value, then update track table of
this candidate
Use predict value
update track table of
this candidate
true
false
圖十七: 人物追蹤流程圖
Show this candidate
If any candidate in track
table that <1
true
false
36
考慮不同工作在不同處理器上的執行時間的差異會使實作太過複雜，因此，
我們假設每一個處理器的速度皆相同。並且由於對應點搜尋演算法內的工作執行
時間和資料量是成線性關係，因此，當某一工作被資料分割時，傳送資料的時間
會依照被分割的資料大小線性縮減。並且處理器在計算時仍然可以傳送資料而不
影響計算時間。由於我們可使用的處理器數量為 8個，並根據以上條件，為了實
作方便，我們將我們的排程演算法中的使用三種資料分割的大小簡化只考慮一種
資料分割的方式，所得到的排程結果為圖十九，而流程圖相對應的變化為圖二十。
Source Image
Calculate Absolute Difference
Modified Mean Filter
Vertical Search Diagonal Search
Median Filter Median Filter
Left Disparity Right Disparity
1(0.07, y)
2(0.16, y)
4(0.07, y)3(0.07, y)
6(0.05, y)5(0.05, y)
0.64
0.64 0.64
0.01 0.01
圖十八： 對應點比對的演算法包含計算和傳輸時間
Left Right Check Left Right Check
Connected Component Labeling Connected Component Labeling
Size Filter Size Filter
0.01
0.01
0.01
0.01
0.01
Optional
38
由圖十八我們可以得知，此演算法的每一個工作都可使用資料分割，因此，
我們根據擁有的處理器數量找出以 0.06 秒為每一個工作的執行時間上限和每一
個管線階段的執行時間上限為 0.07 秒時，可以得到最佳的產能為
07.0
1
= 14.28
個 FPS，反應時間為 0.56 秒。圖十九中的每一個以橫線分開的區域表示一個管
線階段。圖十九中有 4個管線階段並沒有被分配到處理器，沒有被分配處理器的
這 4 個管線階段只是為了等待此管線階段的上一個管線階段傳送資料至此管線
階段的下一個管線階段。這種空的管線階段情況只會增加系統的反應時間
(Response time)，對於產能(Throughput)是沒有影響的。而每個管線階段內的
列數就表示使用的處理器數量，因此在圖十九中總共需要 8個處理器。而每一個
處理器上的有幾組括號數量就表示此處理器所需要處理的工作數量。括號外左邊
的數字表示此工作的工作編號，括號內的數字分別表示此工作是屬於原始流程圖
的哪個工作、此工作在這個管線階段內的開始執行時間和此工作完成的時間。因
此以 7(1, 0.035, 0.07)為例，表示此工作是第 7號工作、此工作是由原始流程
圖的 1號工作所分割而成、此工作在這個管線階段內是從 0.035 秒時開始執行，
而完成時間為 0.07 秒。而圖二十中每一個方框就表示原始流程圖對應的工作，
方框內的圓圈和數字表示對應的工作和編號，編號旁的 Pn 就表示所使用的處理
器。而虛線分隔開的部分則表示不同的管線階段。
由於各模組可在所分配到的工作站群中獨立運作，模組間的運算速度可能並
不相同，造成多模組整合上之困難。針對運算速度差異之問題，可以有下列兩種
解決方式:
1. 使用 MPI 之同步機制控制各模組之運行，使其嚴格保持相同之速率
(throughput)。這個方法的缺點是所有模組將配合最慢模組的速度，造成整體速
率之瓶頸與資源之浪費。
2. 各模組可各自以其最佳速率運行，系統以「隨到隨用」的方式進行整合。例
如顏色與運動模組每 30ms 各產生一個結果，距離模組每 15ms 產生一個結果，則
系統將以顏色與運動為主、距離為輔的方式進行整合。這個方法的問題是各模組
間之公平性將受到質疑，因此，我們設計動態調整的機制，將卡門過濾法中各模
組之觀測協方差(measurement noise covariance)根據其速率進行適當的調整，
以維護多模組融合之公平性。
40
(二) 運動模組偵測實驗結果
運動偵測模組將移動的物體擷取下來並依人物外型的比例及面積為依據將
屬於人物的區選取下來，並將頭部及身體依比例分開。如圖二十二所示，只要人
物持續的走動，運動模組就可以正確地找出人物的位置。
圖二十二: 運動模組偵測實驗結果
42
左至右依序為整合追蹤結果、膚色模組偵測結果、運動模組偵測結果、距離模組
偵測結果。圖二十四中我們可以發現光線劇烈變化時，運動模組會瞬間失效，所
以我們將運動模組的輸出結果權重降低，由膚色模組及距離模組進行持續追蹤。
圖二十四: 光線變化追蹤實驗結
depthmotioncolorresult
44
(六) 靜止人物追蹤實驗結果
圖二十六中在人物靜止時，運動模組將無法偵測出影像中的人物，所以
kalman filter 將只整合膚色模組和距離模組進行偵測。
圖二十六: 靜止人物追蹤實驗
depthmotioncolorresult
46
(八) 人戴口罩追蹤實驗結果
圖二十八中人物戴口罩時膚色模組偵測失敗率提昇，此時可由運動模組及距
離模組持續追蹤。
圖二十八: 人戴口罩追蹤實驗結果
depthmotioncolorresult
48
(十) 偵測錯誤原因分析
由上面實驗相關數據可知，雖然我們的系統在大部分的情況都可以正確的偵
測人物，但仍有一些狀況會造成偵測追蹤的錯誤發生，分析其失敗的原因如下:
1.人物位置太過接近:兩個以上人物的位置太過接近到追蹤模組所定的搜尋範圍
時，會造成追蹤模組選錯追蹤的對象(圖三十(a))，或將兩者合併為一(圖三十
(b))。
(a)
(b)
圖三十: 兩個人物(a)距離太近(b)合併為一
2.人物衣著缺乏材質:因為距離模組是靠影像明亮度的差異來進行偵測，當人物
的衣著接近暗色或缺乏材質時會造成距離模組的失效。如圖三十一所示。
圖三十一: 距離模組因人物衣著缺乏材質比對而失效
3.人物靜止不動且背對攝影機造成運動及膚色模組偵測失敗:當人物背對攝影機
將造成臉部的遮蔽使膚色模組偵測失敗，人物靜止會造成運動模組的偵測失敗，
如圖三十二所示。
圖三十二: 人物靜止背對攝影機造成追蹤失敗
depthmotioncolorresult
depthmotioncolorresult
depthmotioncolorresult
50
距離的結果有很大的高低落差，加入距離偵測可在複雜環境變化下讓整體追蹤正
確率提高且穩定許多。
在執行速度的比較上，加入距離模組和未加入距離模組的執行速度相差大約
十倍，運動模組的加入對於運算時間只有小幅度的增加，所以整個系統的距離模
組偵測佔據大部分的運算時間。影像中人物的多寡和整體系統的執行速度並無很
大的關聯，但系統中的距離模組運算速度和場景有材質區域面積成反比，因此對
影像本身的亮度變化非常敏感，因為光線暗時有材質區域的面積會縮小使距離模
組的運算速度加快，所以光線的明暗變化會影響整體的運算速度。
在多工作站網路平行實作方面，我們使用由 Argonne National Laboratory
提供的一套 MPICH 函式庫來完成不同電腦之間的資料傳遞的工作。而我們使用的
處理器為 8部不同速度的處理器，分別為：Pentium 4-2.8G * 3、Pentium 4-2.0G
* 2、K7-1.0G * 3。並使用 100MB/s 的乙太網路連接各部電腦。依上列環境針對
深度模組中的對應點比對演算法所實作的結果如表七所示。
表七：加速前後的理論值和實驗值
反應時間(s) 得到新輸出的時間(s)
使用 1個處理器 0.47 0.47
使用 8個處理器(理論值) 0.56 0.07
使用 8個處理器(實驗值) 1.47 0.093
我們取計算 500 個影像的結果取平均，可以得到個每次得到計算結果大約需
要的執行時間為 0.093 秒左右，即表示系統的產能約為 10.75 FPS，而反應時間
約為 1.47 秒。這個結果和我們的演算法所估計的結果有著一些差距，原因可歸
類為下列幾點。第一點是由於我們所得到的每個工作的處理時間是由其中某部處
理器的計算結果，但是我們所使用的處理器包含速度不同的 8部處理器，因此在
排程時會造成工作的執行時間的差異。第二點是傳輸資料所花費的時間上的誤
差，我們使用的這個搜尋對應點的演算法在計算過程中會產生非常大的暫存資
料，若考慮網路傳輸的速度時，在傳送資料的時間的部分反而會遠遠超過計算所
花費的時間。由於我們用來連結處理器之間的是 100MB/s 乙太網路，因此，不同
處理器之間傳送資料時可能會造成封包的碰撞，而增加資料傳送實際所花費的時
間，也影響了工作的開始執行時間，因此這是造成結果誤差的主要原因。另外，
由於 MPICH 函式庫在傳送不同大小的資料時，傳輸時間並不是呈線性變化，因此
當我們把使用資料分割時，工作之間的傳輸時間並沒有隨著分割的大小呈現線性
分佈，這也是造成傳輸時間誤差的原因之一。
52
六、結論與建議
由上面相關實驗結果我們可以了解，如果只使用膚色模組及運動模組
偵測雖然有比較快的執行速度，但整體的正確率卻比加入距離模組進行偵
測的結果還低，所以加入距離模組進行偵測追蹤不但可以提昇整體追蹤的
正確率，對於人物在空間中的資訊也由二維提升到三維，可增加我們對追
蹤過程人物位置及運動狀態的了解，當人物缺乏膚色資訊(如背對鏡頭或
蒙面)的狀況時或人物靜止不動時將可彌補其他模組的盲點持續進行偵
測，另外距離模組動態分析每一個 frame 的 depth map 中每段深度的物件
資訊可有效在攝影機移動及環境背景移動或光線改變的情況下持續偵測
而不至失效。
由目前的實驗可以看出，這是一個可以在一般室內環境下偵測並追蹤
影像中多個挺直站立人物，並能有效適應光線改變及移動攝影機及人物靜
止及移動等狀態的改變等複雜環境下執行的多模組人物偵測追蹤系統。然
而，每一個單獨的模組在某些狀況下都會有偵測失敗的情況，膚色偵測模
組雖然在光線有強烈變化也能夠有很穩定的表現，但在面對人物戴口罩或
背對攝影機時往往無法偵測，臉部和手的分辨常會有失誤發生，當背景有
和皮膚相近的顏色也會失敗。運動偵測模組在人物行走活動時有較佳的表
現，但在人物靜止時或光線有劇烈的改變時就會發生錯誤的判斷。距離偵
測模組在人物服裝材質變化較多以及人物靜止時有較好的表現，人物交錯
時也有交錯後重新找整合對象的依據，但如果人物和背景物體接近或服裝
過於單調時就會無法偵測。
多模組融合系統利用每種模組的優點去彌補其他模組的缺點，將可信
度高的模組設定較高的權重來決定偵測的最佳估計值，來提昇整體偵測的
正確率。除了讓偵測過程更加穩定外，在系統辨識的正確率方面，可針對
這三個模組，由三方面來個別進行改善，其中膚色模組可將靜態的膚色模
型改成動態可自我調整式的動態模型，可針對偵測到的人臉部分進行膚色
分布的調整，可以針對不同的人物及環境調整不同的膚色模型，可以彌補
靜態膚色在遇到和膚色相近顏色會偵測錯誤的缺點。在運動偵測模組方
面，可再發展一個攝影機運動補償的機制，對於已知可預測的攝影機運動
找出攝影機的運動模型，在攝影機移動時將移動的位移算出即可對影像中
的人物位置繼續偵測追蹤，此時攝影機可以不受視野的限制持續將攝影機
鏡頭對準目標偵測追蹤。在距離偵測模組方面，可以採用運算量較大但準
確率較高的 global matching system，或增加攝影機的數目可提供更多的
資訊並減少遮蔽的區域可使點對搜尋的正確率提高。在多模組整合的部
分，可以再加入一些其它的特徵下去整合，如材質、形狀模組等，可利用
Kalman filter 各模組獨立性，在不用修改其他模組係數的狀況下，增加
或減少偵測的模組。
54
[17]R. Crinon & I. Sezan. Sprite-based video coding using on-line
segmentation. IEEE International Conference on Acoustics, Speech, &
Signal Processing, 1998.
[18]T. Darrell, G. Gordon, M. Harville & J. Woodfill, Integrated person
tracking using stereo, color, and pattern detection, IEEE
International Conference on Computer Vision and Pattern Recognition,
1998.
[19]P. Fieguth & D. Terzopoulos. Color-based tracking of heads and other
mobile objects at video frame rates. IEEE International Conference
on Computer Vision and Pattern Recognition, 1997.
[20]J. Fisher III & T. Darrell. Signal level fusion for multimodal
perceptual user interface. Workshop on Perceptive User Interface,
2001.
[21]M. Fleck, D. Forsyth & C. Bregler. Finding naked people. Proceedings
of European Conference of Computer Vision, pp. 593-602, 1996.
[22]T. Gandhi, M.T. Yang, R. Kasturi, O. Camps & L. Coraor. Detection of
Obstacles in the Flight Path of an Aircraft. IEEE Transactions on
Aerospace and Electronic System, vol. 39, no. 1, p. 176-191, January,
2003.
[23]T. Gandhi, M.T. Yang, R. Kasturi, O. Camps & L. Coraor. Performance
characterization of the dynamic programming obstacle detection
algorithm, IEEE Transactions on Image Processing, 2002.
[24]I. Haritaoglu, D. Harwood & L. Davis. W4: Who? When? Where? What? A
real time system for detecting and tracking people. IEEE
International Conference on Automatic Face and Gesture Recognition,
1998.
[25]H. Hattori & A. Maki. Stereo without depth search and metric
calibration. IEEE International Conference on Computer Vision and
Pattern Recognition, 2000.
[26]B. Heisele, U. Kressel & W. Ritter. Tracking non-rigid, moving objects
based on color cluster flow, IEEE International Conference on
Computer Vision and Pattern Recognition, pp. 257-260, 1997.
[27]T. Hirai, K. Sasakawa, S. Kuroda & S. Ikebata. Detection of small
moving object by optical flow. Industrial Electronics and Systems
Laboratory, Mitsubishi Electric Corporation, 1992.
[28]E. Hjelmas & B. Low. Face detection: a survey. Journal of Computer
Vision and Image Understanding, 2001.
[29]C. Hsieh, Y. Huang, H. Zhao, J. Chen & Y. Tsai. Multiple-person
tracking system using support vector machine. Proceedings of CVGIP,
2001.
[30]C. Hsieh & D. Lin. Tilt, scale invariant human face detection system.
Proceedings of CVGIP, 2001.
[31]Q. Huang, B. Dom, D. Steele, J. Ashley & W. Niblack.
Foreground/background segmentation of color images by integration of
56
system based on human finder and human tracker. IEEE International
Conference on Intelligent Robots and Systems, 1997.
[48]F. Moscheni, F. Dufaux & M. Kurt. A new two-stage global/local motion
estimation based on a background/foreground segmentation. IEEE
International Conference on Acoustic, Speech, and Signal Processing,
1995.
[49]MRL–Intel Corporation. Camera Calibration Toolbox for Matlab.
http://www.vision.caltech.edu/bouguetj/calib_doc/
[50]J. Nicholls & D. Monro. Scalable video with background segmentation.
IEEE International Conference on Image Processing, 1996.
[51]M. Okutomi & T. Kanade. A multiple-baseline stereo. IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 15, no. 4, April
1993.
[52]S. Pei & C. Tseng. Face detection for different chromatic
illuminations. Proceedings of CVGIP, 2001.
[53]G. Pingali, Y. Jean, & I. Carlbom. Real time tracking for enhanced
tennis broadcasts. IEEE Conference on Computer Vision and Pattern
Recognition, 1998.
[54]Point Grey Research. http://www.ptgrey.com
[55]L. Rabiner. A tutorial on hidden markov models and selected
applications in speech recognition. IEEE Proceedings, vol. 77, no.
2, February 1989.
[56]L. Rabiner & B. Juang. Fundamentals of Speech Recognition. Englewood
Cliffs, Prentice Hall, 1993.
[57]M. Rossi & A. Bozzoli, Tracking and counting moving people, IEEE
International Conference on Image Processing, vol. 3, pp. 212-216,
1994.
[58]J. Schmidt, H. Niemann & S. Vogt, Dense disparity maps in real-time
with an application to augmented reality, IEEE Workshop on
Applications of Computer Vision Proceedings. pages: 225-230, 3-4 Dec.
2002.
[59]J. Segen, A camera-based system for tracking people in real time, IEEE
International Conference on Pattern Recognition, vol. 3, pp. 63-67,
1996.
[60]J. Sethian. Level set methods: evolving interfaces in geometry, fluid
mechanics, computer vision and material science. Cambridge
University Press, 1996.
[61]L. Shapiro & G. Stockman. Computer Vision. Prentice Hall, 2001.
[62]R. Sharma, V. Pavlovic, T. Huang. Toward multimodal human-computer
interface. IEEE Proceedings, 1998.
[63]H. Sheu & D. Yang. Energy Spectrum based HMM for gesture recognition.
Proceedings of CVGIP, 2001.
[64]L. Sirovinch & M. Kirby. Low-dimensional procedure for the
characterization of human faces. Journal of Opt. Soc. Amer.,pp.
58
Technology, vol. 15, issue 2, p. 131-142, 2005.
[82]A. Yuille, P. Hallinan & D. Cohen. Feature extraction from faces using
deformable templates. International Journal of Computer Vision, pp.
99-111, 1992.
[83]H. Zhao, Y. Tasi & Y. Huang. Real time video surveillance system.
Proceedings of CVGIP, 2001.
[84]王世君, 多模組整合人物偵測追蹤系統,碩士論文,國立東華大學資訊工程
研究所,臺灣,2003.
[85]蔡宗霖, 在分散式系統上完成資料、功能和管線分割三種架構的整合-即時
實作立體影像中的對應點比對,碩士論文,國立東華大學資訊工程研究所,臺
灣,2004.
[86]江萬強,利用攤平圓柱樣板搭配中心移動演算法之人物追蹤系統,碩士論文,
國立東華大學資訊工程研究所,臺灣,2006.
[87]林永淵, 運用中心移動演算法於可適應性目標模型之視訊物件追蹤,碩士論
文,國立東華大學資訊工程研究所,臺灣,2006.
[88]李連勇, 外觀式人物模型應用於多人物追蹤,碩士論文,國立東華大學資訊
工程研究所,臺灣,2006.
60
附錄一、已發表之 IJIST(SCI)期刊論文
62
64
66
68
70
