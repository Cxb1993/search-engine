使用演化式演算法訓練類神經網路之研究 
”A study into the use of Evolutionary Computation techniques to train Neural Networks with application to 
WeiQi” 
計畫編號：NSC 97-2221-E-155-066 
執行期間：97 年 8 月 1 日 至 98 年 7 月 31 日 
主持人：白小明 元智大學電機工程學系副教授  
 
一、 中文摘要 
    使用演化式類神經網路在以沒有專家知
識的介入之下來使電腦學會玩黑白棋，使其類
神經網路能在自我的競爭比較之下找出對應
的遊戲策略。這一篇我們將探討到前置的隱藏
層的處理對演化效率的影響。以一個簡單的八
方向性的取樣樣式的隱藏層處理，卻可以涵蓋
大部分的黑白棋策略，由模擬結果顯示透過此
種自我訓練下的類神經網路能夠有機會以零
步預測的條件下贏過四步預測的位置專家策
略以及六步預測的最大贏棋量的專家策略。同
時，我們也以存活次數的數據來跟神經網路學
習能力的狀況來做對應的探討。然而，同樣的
訓練模式之下的神經網路卻無法輕易的贏過
兩步預測的最大可移動子的專家策略，這點促
使我們去思考不同的取樣樣式會有不同的演
化結果，且樣式的設計必須基於遊戲的規則而
定。 
 
二、英文摘要 
     Neural networks (NNs) were evolved to learn 
to play the zero-sum game Othello (also known as 
reversi) without relying on a-priori or expert 
knowledge. Such neural networks were able to 
discover game-playing strategies through 
co-evolution, where the neural networks just play 
against themselves across generations. The effect of 
the spatial processing layer on evolution was 
investigated. It was found that the evolutionary 
process was crucially dependent on the way in 
which spatial information was presented. A simple 
sampling pattern based on the squares attacked by a 
single queen in Chess resulted the networks 
converging to a solution in which the majority of 
networks, handicapped by playing Black and 
playing without using any look-ahead algorithm, 
could defeat a positional strategy using look-ahead 
at ply-depth=4 and a piece-differential strategy 
using look-ahead at ply-depth=6.  Improvement 
and convergence was observed to be accompanied 
by an gradual increase in the survival time of 
neural network strategies from less than 10 
generations to about 600 generations. Surprisingly, 
evolved neural networks had difficulty in defeating 
a simple mobility strategy playing at a ply-depth=2. 
This work suggests that in deciding a suitable way 
所順練出的神經網路以 0 預設步數分別與 2、4
以及 6 步的預設步數的最大贏子策略的比較數
據圖。演化至 4300 代左右，我們的神經網路
幾乎都可以贏過 4 步預測的最大贏子策略，而
至 8000 代之後，幾近 9 成的神經網路可以擊
敗 6 步預測的最大贏子策略，以這個結果來看
學習效能相當良好。 
 
Fig.3.神經網路與最大贏子策略競爭比較圖 (a) 可以贏
過等級 2的位置策略的網路數量 (b) 可以贏過等級 2、
4的位置策略的網路數量(c) 可以贏過等級 2、4、6的
位置策略的網路數量。 
    以同樣的參數條件，我們再與最佳位置策
略做比較，結果如 Fig.4 所示。大約在 4300代
左右，我們的神經網路已經逐漸開始學會擊敗
4 步預測的最佳位置策略。直至 6000代左右，
同時也可以發現幾乎所有的神經網路都可以
勝過 2 步預測的最佳位置策略，但不幸的是直
到模擬結束，我們的神經網路似乎無法克服 6
步預測的最佳位置策略。但以客觀的方式來評
判，最佳位置策略本身就比其他策略高等一
點，要以 0 步預測來擊敗 6 步預測的最佳位置
策略實屬困難。 
 
Fig.4.神經網路與最佳位置策略競爭比較圖 (a) 可以贏
過等級 2的位置策略的網路數量 (b) 可以贏過等級 2、
4的位置策略的網路數量(c) 可以贏過等級 2、4、6的
位置策略的網路數量。 
然而與最大移動量策略的比較並沒有多大的
突破，直至模擬結束僅能贏過 2 步預測的最大
移動量策略。 
    為了要讓此改變更具說服力，我們以一個
知名的黑白棋專家程式作為評估的標準 – 
Iagno[8]. 以下是模擬參數： 
• Population size: 100 
• Generation: 10000 
• Opponents: 10 
• Ply-depth for training: 2 
• Hyperbolic Tangent Coefficient:0.01 
• Tournament Selection: Fair 
• Number of hidden layer: 1  
除了沿習先前的參數之外，我們也特別針對幾
項關鍵性的部分做了參數的調整，有別於之前
的模擬，這次的模擬以 2 步預測來做演化訓
練，並且在每一代裡挑出第一名的神經網路以
同步預測步數的方式跟 Iagno 做競爭比較 25
次。模擬數據如 Fig.5 所示： 
 Abstract— Neural networks (NNs) were evolved to learn to 
play the zero-sum game Othello (also known as reversi) without 
relying on a-priori or expert knowledge.  Such neural networks 
were able to discover game-playing strategies through co-
evolution, where the neural networks just play against 
themselves across generations. The effect of the spatial 
processing layer on evolution was investigated.  It was found 
that the evolutionary process was crucially dependent on the 
way in which spatial information was presented.  A simple 
sampling pattern based on the squares attacked by a single 
queen in Chess resulted the networks converging to a solution 
in which the majority of networks, handicapped by playing 
Black and playing without using any look-ahead algorithm, 
could defeat a positional strategy using look-ahead at ply-
depth=4 and a piece-differential strategy using look-ahead at 
ply-depth=6.  Improvement and convergence was observed to 
be accompanied by an gradual increase in the survival time of 
neural network strategies from less than 10 generations to 
about 600 generations. Surprisingly, evolved neural networks 
had difficulty in defeating a simple mobility strategy playing at 
a ply-depth=2.   This work suggests that in deciding a suitable 
way to spatially sample a board position, it is important to 
consider the rules of the game. 
I. INTRODUCTION
AMES have been widely used as a testbed for self-
learning techniques as there are a specific set of (1) 
rules that govern the game, (2) behaviors (rules for 
making legal moves), and (3) goals (to win), as well as, a 
diverse environment of players. Considerable attention has 
been received by zero sum board games such as checkers, 
chess and Othello.  In the middle of 1990, Leouski and 
Utgoff [1] used temporal difference learning (TDL) to aid a 
neural network to learn to play Othello.  Several years later, 
Fogel and coworkers [2] showed that multilayer perception 
(MLP) with a spatial layer can be evolved to play checkers 
at a high level without expert knowledge. Chong et. al. [3] 
showed that a similar architecture having a similar spatial 
layer could be evolved as an evaluation function for the 
game of Othello.  In addition, they observed changes in 
strategies being adopted by the neural network by making 
use of fixed algorithms playing at a variety of ply-depths.  In 
recent research, Lucas [4] showed that a n-tuple systems 
Manuscript received October 31, 2008.   This work was supported in part 
by the National Science Council of the Republic of China under Grant #97-
2221-E-155-061  
S. Y. Lin is with the Department of Electrical Engineering, Yuan Ze 
University, Taoyuan 320, Taiwan (e-mail:  xinyu0123@gmail.com).  
 J. D. White is with the Department of Electrical Engineering, Yuan 
Ze University, Taoyuan 320, Taiwan (phone: 886-3-463-8800x7514; fax: 
886-3-451-4281 e-mail: whitejd@xiaotu.com). 
having ~15,000 weights could be used as position value 
functions for the game of Othello.  Coupled with TDL, good 
performance was obtained after only 500 generations at the 
cost of having ~15,000 adjustable weights.  
In this paper we report on the effects of replacing the 
spatial input layer of Chong et. al. [3] with one based on the 
rules of the game of Othello coupled with a modification of 
the selection method from unfair to fair tournament.  We 
have found that under these changes successful strategies 
can be evolved that are equal or better than those evolved 
using the previous architecture. In addition, these evolved 
strategies require significantly fewer adjustable weights 
(~4700).  Finally, we show that once successful strategies 
have been identified, there is a gradual reduction of noise 
and the rate of evolution slows as seen in an extension of the 
survival times of neural networks which slowly rises from 
less than 10 for the reported architecture. 
I.BACKGROUND
Othello is a popular zero-sum game in which two-players, 
designated Black and White, alternatively place their pieces 
on an eight-by-eight board.  Starting from the initial board 
position in which each player has two pieces on the board, 
the Black player makes the first move.  A legal move is one 
in which the new piece is placed adjacent (horizontally, 
vertically, or diagonally) to an opponent's existing piece in 
such a way that at least one of the opponent's pieces lies 
between one of the player's existing pieces and the new 
piece. The move is completed when the surrounded pieces 
are removed and replaced with pieces of the player's own 
color.  The game is completed when neither player can make 
a legal move, which, in general, occurs when there are no 
empty squares remaining on the board.  The winner is the 
player with the most pieces on the board at the end of the 
game.  In the event that both players possess an equal 
number of pieces, both players are awarded a draw. 
This game is an interesting test-case for evolutionary 
programming as, unlike in other games of skills such as 
checkers and chess, who is winning, as defined by the 
number of pieces on the board, can vary drastically from 
move to move. 
                                                                                                  
Spatial Processing Layer Effects on the Evolution of Neural 
Networks to Play the Game of Othello 
S.Y. Lin and J.D. White, Member, IEEE
G
êìêçéèóïóìîììóîçëçóîñðçñüîëòðð ½ îððç ×ÛÛÛ
function. 
The input layer is designed to represent to the Othello 
board. It is an 8x8 vector consisting of 64 components to 
corresponding to the 8x8 Othello board positions. Each 
component in the 2D vector can take on the elements {-1, 1, 
0}, where “-1” was the value assigned for black piece, “1” 
was the value assigned for white piece and  “0” represented 
an empty square. 
The first hidden layer is the spatial preprocessing layer -- 
designed for processing, analyzing and emphasizing the 
most important areas on board. The sampling pattern is the 
key change relative to the models used for Checkers [2] and 
Othello [3]. In the previous works, the spatial processing 
method was sub-squares sampling.  This pattern is related to 
the rules of checker but not to rules of Othello. That is, for 
checkers, the key special information required is the status 
of adjacent diagonal squares as jumps are made diagonally. 
In other words, any move changes the status of these 
squares. In contrast to checkers, in Othello, the rules indicate 
that placing a piece on any given square affects the status of 
squares in the horizontal, vertical and diagonal directions --  
a total of eight directions.  So, just as sub-squares is a natural 
pattern for Checkers, the queen attack positions (in 
International Chess, henceforth called Q-position patterns)  
is a natural pattern for Othello. 
The sampling procedure of the Q-position pattern is 
illustrated in Fig. 3 for four different squares. For example, 
in Fig. 3a the central point of the base Q-position pattern is 
mapped to the top-left corner of the input board. The 
occupancy of 22 squares thus forms the input to this node. 
Similarly, in Fig. 3b, the pattern is mapped to the second 
square in the top row.  Again the occupancy of 22 squares 
forms the input.  This is repeated for all 64 squares of the 
Othello board.  In addition, the full board pattern, which 
serves the function of a weighted piece counter (WPC), is 
sampled. As shown in Fig. 1, there are total 65 patterns in 
this spatial processing layer (hidden layer one) having a total 
of 1584 inputs. The second and third layers consist of 40 and 
10 nodes, respectively.  The output of every first layer node 
is connected to the input of each second layer node, the 
output of each second layer node is connected to the input of 
each third layer node.  Finally the output of each third layer 
node forms the input of a single output node which provides 
a scalar assessment of the value of a the inputted board 
configuration.. Note, that in contrast to previous work [3], 
piece differential information is not given to the output node, 
forcing the neural network to learn to defeat the piece 
maximum strategy naturally. 
In the above fully connected architecture, the total number 
of weights and biases (denoted wi(j)):  [(1649 in hidden layer 
one)] + [(40 nodes in hidden two) × (65 inputs + 1 bias)] +[ 
(10 nodes in hidden layer 3) × (40 inputs + 1 bias)] + [(1 
node in output layer) × (10inputs + 1 bias)] = 4710.   For 
each hidden node and for the output node, the hyperbolic 
tangent (bounded by +/-1) function is used with a variable 
bias.  The initial population was created by random 
generation of the weights and biases by sampling from a 
uniform distribution over [-0.2, 0.2]. We note that the total 
number of weight and bias is quite small compared to 
previous architecture (5900 weights and biases) saving 
calculation time [3]. 
B.  Tournament Selection 
In order for evolution to occur, in each generation, a 
selection mechanism must be employed to determine each 
neural network’s overall fitness relative to the whole 
population.  This allows parents to be selected for the next 
generation.   
In the current simulation, a fair tournament selection 
model is used [6]. The population at every generation 
consists of 100 neural networks (strategies): fifty parents and 
Fig. 4. Evolution in playing ability of the parent neural networks with 
generation. Networks were evaluated every 50 generations by playing 
against the fixed positional strategy player, with the ply-depth being 
increased in steps of two until the network lost. Number of networks that 
can defeat the fixed strategy player playing at ply-depth of (a) two (b) four 
and (c) six. 
Fig. 5. Evolution in playing ability of the parent neural networks with 
generation. Networks were evaluated every 50 generations by playing 
against the fixed piece differential strategy player, with the ply-depth being 
increased in steps of two until the network lost. Number of networks that 
can defeat the fixed strategy player playing at ply-depth of (a) two (b) four 
and (c) six. The comparison stops at a ply-depth of six because at ply-depth 
of eight, the minimax algorithm was unable to run in a reasonable time.
êìè îððç ×ÛÛÛ Ý±²¹®»-- ±² Ûª±´«¬·±²¿®§ Ý±³°«¬¿¬·±² øÝÛÝ îððç÷
evolution of the neural networks: a positional strategy 
(evaluation function based on the positional component of 
Iago's strategy of Iago [7]), a piece differential player who 
for a given board position sought to maximize the number of 
pieces the board, a mobility player who seeks to maximize 
the number of available moves and the complete expert 
strategy of Iagno version 22.3 (June 30, 2008) [8]  During 
the fixed strategy comparison, all the neural networks played 
black (generally considered to be a handicap compared to 
the white) and used ply-depth of zero.  
II. RESULTS 
Fig. 4 compares the performance of the neural networks 
with that of a computer player using the positional strategy 
to evaluate the quality of a move, while Fig. 5 compares the 
performance of the neural network with that of a computer 
player using piece differential as the sole means of 
evaluating the quality of a move.  
Let us consider first the performance of the neural 
networks relative to the positional player.  After only 100 
generations, over half of neural networks have learned to 
defeat the positional strategy playing at a ply-depth of two 
despite the fact that they are playing black.  While a majority 
of networks quickly learn to defeat the ply-depth=2, it takes 
another 6000 generations for essentially the whole 
population to consistently maintain this level of 
performance. Correlated to this is the fact that only after 
4000 generations, do we see performance improving such 
that the networks that a shift from defeating ply-depth of two 
to ply-depth of four takes place. Around generation 4500 
there sudden improvement with some neural networks 
learning to defeat the positional player with a ply-depth=6.  
As can be seen in the figure the lifespan of these exceptional 
neural networks is quite short with the play of the best 
networks dropping back quickly.  As will be discussed later, 
we believe that this is due to the networks becoming overly 
specialized at this point -- developing a specialized strategy 
to defeat the positional player that is not applicable for other 
playing strategies.    Over the next 3500 generations an 
increasing number of networks develop the ability to defeat 
the ply-depth=4 positional player.  By generation 7000 
essentially all networks have obtained this ability.  It is 
significant to note that this ability is not lost over time.  
However, despite one attempt at around generation 7000 it is 
not possible for the networks to defeat the ply-depth=6 
player.   
Fig. 5 shows that these same neural networks evolved at 
the end to outperform the piece differential as well.  By the 
100th generation, over 25 neural networks can defeat ply-
depth two piece differential strategy.  However, in contrast 
to their performance against the positional player, they are 
unable to maintain this level with the level of play appearing 
to be sporadic., By generation 3000, the neural networks 
appear to loose all ability to defeat the piece differential 
player.  It is only in the range between generation 4000 to 
5000 does the play relative to the piece differential player 
improve drastically.  In contrast to play with the positional 
player, in which networks first learned to defeat a ply-
depth=2 player before learning to defeat the ply-depth=4 
player, there is a sudden shift being unable to defeat the ply-
depth=2 player to being able to defeat a ply-depth=6 player.  
In order to investigate the change in playing competence  
around generation 4500, we have increased the sampling rate 
from every 50 to every generation in this region. so as to be 
able to see how the strategy improvement occurred. Figures 
5 and 6 compare the performance of the neural networks 
with that of a computer player using the positional strategy 
and piece differential strategies, respectively. Considering 
first the positional strategy. Against ply-depth of two 
positional strategy (Fig.6b), the key change of the neural 
networks’ ability happened over 40 generations starting from 
generation 4525. This improvement is quickly followed, 
with a delay of about 10 generations, by a considerable 
portion of the networks learning to defeat the positional 
player playing at ply-depth=4. The improvement in play 
relative to the piece-differential player is not, surprisingly, 
coincident with the improvement seen with respect to play 
against the positional player but rather delayed by about 15 
to 20 generations.  Perhaps more amazing is the fact that the 
jump is from not being able to defeat a ply-depth=2 strategy 
to being able to defeat a ply-depth=6 strategy and that this 
ability is gained by about 50% of the population in only 
about 11 generations.  We note that the networks have 
gained this ability without explicitly being given any piece 
differential information as was the case in previous 
simulations [3].  
Next we compared the ability of the neural networks with 
the simple mobility strategy.  At the beginning, the networks 
slowly learn to defeat pure mobility strategies with 
considerable improvement being seen in the range from 
generation 4000 to generation 5000.  However, after this 
generation, despite improved play against the positional and 
piece differential players, the networks seem to have lost all 
ability to defeat the mobility strategy operating at a ply-
depth greater than themselves.  In addition, they never regain 
the ability.  Finally, in competition with the general expert 
strategy Iagno [8], the evolved neural networks exhibited 
good performance when playing at equal ply-depths.  
In Fig. 8 we plot the average survival time of the neural 
networks as a function of generation.  Before the generation 
4000, the average survival time is less than 10 generations. 
After the improvement in play occurs (~generation 5000) the 
survival time starts to slowly increase reaching over 600 
generations by generation 10000 when the simulation was 
stopped.  At the 10000th generation, the average survived 
times is already reach 600 generation.  It would seem that 
the evolutionary algorithm has converged to a solution that 
can defeat the positional strategy playing at ply-depth=4, the 
piece differential strategy playing at ply-depth=6 and the 
mobility strategy playing at ply-depth=0. In repeating 
Chong's simulation [3], we found no increase in network 
survival time over the simulation window.   
êëð îððç ×ÛÛÛ Ý±²¹®»-- ±² Ûª±´«¬·±²¿®§ Ý±³°«¬¿¬·±² øÝÛÝ îððç÷
