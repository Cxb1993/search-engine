 1
行政院國家科學委員會專題研究計畫成果報告 
高維度類化型小腦模型控制器之學習收斂性 
The Learning Convergence of High Dimension CMAC_GBF 
計畫編號：NSC 95-2221-E-231-011- 
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
主持人：江青瓚   清雲科技大學電機工程系 
共同主持人：李建國   實踐大學資訊管理系 
 
Abstract 
High-Dimension Cerebellar Model Articulation 
Controller with General Basis Function (CMAC_GBF) 
is developed and its learning convergence is also 
proved in this study.  Up till now, the applications of 
CMAC are mainly used as controller or system 
identification (function mapping).  Due to the 
guaranteed convergence and learning speed of CMAC, 
all the applications have shown good performance.  
But for high-dimensional mapping or control, it 
requires a lot of memories; the consequence is not 
able to use CMAC_GBF or to use enormous resources 
to complete its mission.  When CMAC-GBF is 
employed, the necessary memory is growing 
exponentially with increasing input dimensions, and 
this slows down the learning speed or turns out to be 
impossible.  The author developed the network 
structure of High-Dimension CMAC-GBF, the title of 
the papers are A Simple-Structure of Addressing 
Technique for CMAC-GBF (S_CMAC_GBF) and 
Local Learning for S_CMAC_GBF.  In this project, 
two 6-input nonlinear systems are demonstrated the 
learning performance and the practicability of 
required memories of S_CMAC_GBF and Local 
Learning for S_CMAC_GBF in high-dimensional 
applications are also shown in this paper.  In 
conclusion, the learning convergence is proved, and 
more over, the guaranteed learning convergence to 
least square error is also proved. 
Keywords:  High-Dimension, S_CMAC_GBF, 
Local Learning, Learning Convergence. 
 
I. Introduction 
 
The advantages of CMAC_GBF are rapid learning 
speed, guarantee of learning convergence, good 
generaliability, and lower computational load.  But it 
is difficult for CMAC_GBF to solve problems of 
high-dimensional inputs system; the reason is the 
necessary memory is growing exponentially with 
increasing input dimensions.  This study proposed 
S_CMAC_GBF and Local Learning for 
S_CMAC_GBF in high-dimensional applications.  
The simplest addressing technique procedure is not 
only to provide fixed addressing procedure for 
CMAC_GBF, also to reduce the usage of memory.  
S_CMAC_GBF still posses the same learning 
convergence characteristics of CMAC_GBF, but with 
more enhanced accurate system performance.  Local 
learning achieves a goal of increasing better efficiency 
in both learning and getting output of the 
S_CMAC_GBF.  Local learning utilizes the 
relationship between the input status and hypercubes 
to make the input status of every area only use the 
better relative hypercubes rather than all hypercubes; 
local learning can exclude the low contribution 
hypercubes without computation in getting output and 
learning.  S_CMAC_GBF will be briefly introduced 
in the followings, and the method of local learning 
will be stated.  Two 6-input non-linear functions are 
introduced as examples and demonstrated the learning 
performance and the practicability of required 
memories.  Finally, the learning convergence is 
proved and shows it converges to least square error. 
 
II. S_CMAC_GBF and Local Learning 
for S_CMAC_GBF 
 
A. S_CMAC_GBF 
CMAC_GBF combines the traditional CMAC and 
the General Basis Function, GBF to enable the CMAC 
to have the ability of differentiable output and 
improve the output accuracy.  S_CMAC_GBF was 
proposed by author in 2004; it simplified the 
addressing structure of the CMAC_GBF to decrease 
memory space and to be easier to be performed by 
hardware. 
The quantified method of input variables for 
S_CMAC_GBF is different from the quantified 
conventional CMAC_GBF.  In S_CMAC_GBF, each 
layer is only one block, and each block has different 
basis function.  The weighting of each layer can be 
obtained by multiplying basis function of each layer 
with its height.  Like in Figure 1, from recall the 
process of S_CMAC_GBF, it is known that the output 
is the sum of the weight of 20 layers.  The learning 
method of S_CMAC_GBF is just like the one for 
CMAC_GBF to adjust the required parameters in the 
recall process, and after repeated learning processes, 
the output of S_CMAC_GBF can reach the same 
accurate level as of CMAC_GBF. 
 3
with less data, but for learning a unknown function or 
plant training patterns, the data are normally 
enormous, and the local learning can be performed 
appreciably. 
-2
-1
0
1
2
-2
-1
0
1
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x1
x2
k
 
Figure 2. The plot of the constrains identification of a 
Gaussian function. 
 
Software simulation 
To demonstrate the local learning effect, two 
non-linear functions that are the same as in the paper 
of S_CMAC_GBF are selected to be the learning 
targets.  Training patterns are generated randomly, 
1000 input variables ( 1x , 2x ) are generated in the 
range between -1 and +1, 1000 target values are 
obtained.  Then the layer of the addressing structure 
is to be decided, here it is set to be 20 layers.  The 
two non-linear functions are shown below: 
 )cos()sin(),( 2121 ππ xxxxf =  (7) 
 )5sin()(),( 1
2
2
2
121 xxxxxg −=  (8) 
In the learning process, the sum of root square 
error can be obtained by equation (9) to observe the 
error of every learning cycle to decide whether the 
learning speed is to be adjusted or to be stopped.  
The equation of the sum of root square error is shown 
below: 
 ( )∑
=
−=
N
i
SRSE
1
2out realtarget  (9) 
where, N is the number of the patterns. 
In order to test the above stated learning 
performance, 1024 test patterns are selected uniformly 
from the state plane, to be the learning completion test 
of the stated localized learning of a S_CMAC_GBF. 
First of all, setting limited parameter, and the 
condition is the contribution less than 10%; set 
1.0=k , and learned by localized learning.  There 
are 20 weights in the addressed structure, for every 
weight, the point with 1.0=∏ ijφ  can constitute 
one ellipse; there are 20 ellipses in Figure 3.  The 
plot of the values of ∏ ijφ  of the 15th is shown in 
Figure 4.  From Figure 3, it can be observed that the 
effective area of each weights only covers one small 
area of the state plane.  Similarly, for 
function ),( 21 xxg , a similar result can be obtained by 
using same learning method, like in Figure 5 and 6.  
For the function ),( 21 xxf , if divides a state plane into 
16 areas, like in Figure (3), then in every area, the 
highly relative (effective range wholly or partial 
covered) weights will be less than 11 (20 originally), 
and it is almost the same for the function ),( 21 xxg , 
which is less than 9.  The simulation result shows 
that the required training time is obviously lower 
down, only takes 60% of the originally time.  For the 
presence of time-effective, it is efficiently reduced the 
training time.  Although there will be some loss in 
the accuracy, but it will not affect its performance.  
The simulation parameters and result are shown in 
Table 1. 
 
In order to test the above stated learning 
performance, 1024 test patterns are selected uniformly 
from the state plane, to be the learning completion test 
of the stated localized learning of a S_CMAC_GBF. 
First of all, setting limited parameter, and the 
condition is the contribution less than 10%; set 
1.0=k , and learned by localized learning.  There 
are 20 weights in the addressed structure, for every 
weight, the point with 1.0=∏ ijφ  can constitute 
one ellipse; there are 20 ellipses in Figure 3.  The 
plot of the values of ∏ ijφ  of the 15th is shown in 
Figure 4.  From Figure 3, it can be observed that the 
effective area of each weights only covers one small 
area of the state plane.  Similarly, for 
function ),( 21 xxg , a similar result can be obtained by 
using same learning method, like in Figure 5 and 6.  
For the function ),( 21 xxf , if divides a state plane into 
16 areas, like in Figure (3), then in every area, the 
highly relative (effective range wholly or partial 
covered) weights will be less than 11 (20 originally), 
and it is almost the same for the function ),( 21 xxg , 
which is less than 9.  The simulation result shows 
that the required training time is obviously lower 
down, only takes 60% of the originally time.  For the 
presence of time-effective, it is efficiently reduced the 
training time.  Although there will be some loss in 
the accuracy, but it will not affect its performance.  
The simulation parameters and result are shown in 
Table 1. 
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
 
Figure 3. The distribution of 20 weights in ),( 21 xxf . 
 5
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
1.5
2
x1x3
 
(b) The plot of the actual output from 
S_CMAC_GBF. 
Figure 7 The plots of function )5.0,5.0,,,5.0,5.0( 43 xxh . 
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
0.5
1
1.5
2
x3x4
 
(a) The plot of the target value. 
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
0.5
1
1.5
2
x3x4
 
(b) The plot of the actual output from 
S_CMAC_GBF. 
Figure 8 The plot of function )5.0,5.0,5.0,,5.0,( 31 xxh . 
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
-0.5
0
0.5
1
1.5
x1x3
 
(a) The plot of the target value. 
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
-0.5
0
0.5
1
1.5
x1x3
 
(b) The plot of the actual output from 
S_CMAC_GBF. 
Figure 9 The plots of function )5.0,5.0,5.0,,5.0,( 31 xxk . 
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
0.4
0.6
0.8
1
1.2
1.4
x5x6
 
(a) The plot of the target value. 
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
0.4
0.6
0.8
1
1.2
1.4
x5x6
 
(b) The plot of the actual output from 
S_CMAC_GBF. 
Figure 8 The plot of function ),,5.0,5.0,5.0,5.0( 65 xxk . 
 
For S_CMAC_GBF with local learning, it also 
shows the excellent approximations.  In local 
learning, if the absolute value of weight 
∏= ijii vw φ is below 0.001 then the ith weight will 
not be employed in this learning procedure.  For h(．) 
function, there are average 31.94 weights are active 
for each pattern.  There is also 25.42 weights is 
employed for function k( ． ).  The comparison 
between S_CMAC_GBF and S_CMAC_GBF with 
local learning is shown in Table 2.  There is almost 
no difference in average error per each pattern; but 
 7
Since 
2
1
⎟⎟⎠
⎞
⎜⎜⎝
⎛∑
=
hN
j
jijbm  is non-negative, if 
( ) 02
1
2
2
<⎥⎥⎦
⎤
⎢⎢⎣
⎡
−⎟⎟⎠
⎞
⎜⎜⎝
⎛ ∑
= h
N
k
k
h N
b
N
h αα , 2 of row Qi  will not be 
greater than ∑
=
hN
j
ijm
1
2 , which is 2 of row Mi .  To 
make ( )
h
N
k
k
h N
b
N
h αα 2
1
2
2
−⎟⎟⎠
⎞
⎜⎜⎝
⎛ ∑
=
 negative, we must have 
( ) ( ) 022 21
2
1
2
2
<
⎟⎟⎠
⎞
⎜⎜⎝
⎛ −
=−⎟⎟⎠
⎞
⎜⎜⎝
⎛ ∑∑ =
= h
h
N
k
kN
k h
k
h N
Nb
N
b
N
h
h
αααα  (17) 
 
i.e., 
 ( )∑
=
<<
hN
k
k
h
b
N
1
2
20 α  (18) 
 
With (13) and (14), the vector ( )ksDv  can be 
derived as  
 
( ) ( )
( )
)0(
)0(
121
)1(
121
s
k
s
s
k
sNss
k
ssNss
k
s
s
s
DvG
DvEEEEE
DvEEEEEDv
≡
=
=
−−
−
−−
LL
LL
 (19) 
 
Where  sNsss s EEEEEG LL 121 −−≡  and ( ) ( )( ) ( )( ) ( ) ⎟⎟
⎟⎟
⎠
⎞
⎜⎜
⎜⎜
⎝
⎛
−+⋅⋅⋅
+−
+−
=
−−−−
++++
11
)1(
11
11
)0(
11
)0(
)0(
ˆ
ˆ
ˆ
sss
T
s
sss
T
s
s
T
s
i
e
s
k
s
y
y
y
N
axBwa
axBwa
axBwa
GDvG
ss
s
α .(20) 
 
According to the above lemmas, the update 
weights ( ) )0(sksks DvGDv =  as k approaches to infinity 
will be null and the learning converges. 
In Localing Learning for S_CMAC_GBF, not 
every weight will be used in learning iteration. Such 
that the convergence of the S_CMAC_GBF with local 
learning is same as CMAC_GBF[3]. 
A further proof that the learning results in the 
least square error is the same as CMAC_GBF in [3].   
 
V. Conclusions 
 
This project proposes high-dimension 
applications of S_CMAC_GBF and Local Learning 
for  S_CMAC_GBF, and the learning convergence 
of these two networks are also been proved.  In this 
project, there are two 6-input nonlinear systems are 
used to demonstrate the learning performance.  The 
ability of these two networks for using less memory to 
accomplish a high-dimension mapping is also 
revealed in this study.  
 
References  
 
[1] Albus, J. S., "A New Approach to Manipulator 
Control: The Cerebellar Model Articulation 
Controller (CMAC)," Journal of Dynamic 
Systems, Measurement, and Control, 
Transactions of ASME, p.220-227, September 
1975. 
[2] Chiang, C. T., Lin, C. S., “Integration of CMAC 
and Radial Basis Function Techniques”, 
Proceedings of the 1995 IEEE International 
Conference on Systems, Man and Cybernetics, 
Part4 (of 5), pp. 3263-3268, 1995. 
[3] S. Lin and C. T. Chiang, "Learning Convergence 
of CMAC Technique," IEEE Transactions on 
Neural Networks, Vol. 8, No. 6, pp. 1281-1292, 
Nov. 1997. 
[4] Ching-Tsan Chiang, Tung-Sheng Chiang, and 
Chien-Kuo Li, “A Simple and Converged 
Structure of Addressing Technique for 
CMAC_GBF”, 2004 IEEE International 
Conference on Systems, Man and Cybernetics, 
pp. 6097~6101, Oct. 2004. 
[5] Ching-Tsan Chiang, Chao-Ming Chong, 
“Hardware Implementation of a Simple Structure 
of Addressing Technique for CMAC_GBF”, 
Proceedings of the IEEE International 
Symposium, Vol. 1,  pp.139 - 144, 2005. 
[6] Almeida, P.E.M.; Simoes, M.G.; “Parametric 
CMAC networks: fundamentals and applications 
of a fast convergence neural structure”, IEEE 
Transactions on Industry Applications, vol. 39, n. 
5, p. 1551-1557, Sept., 2003. 
[7] Shun-Feng Su; Tao, T.; Ta-Hsiung Hung; “Credit 
assigned CMAC and its application to online 
learning robust controllers”, IEEE Transactions 
on Systems, Man and Cybernetics, Part B, vol. 
33 n. 2 , Page(s): 202-213, April 2003. 
[8] Hahn-Ming Lee; Chih-Ming Chen; Yung-Feng 
Lu; “A self-organizing HCMAC neural-network 
classifier”, IEEE Transactions on Neural 
Networks, vol. 14, n. 1, p. 15 -27, Jan. 2003. 
[9] Jan, J.C.; Shih-Lin Hung; “High-order MS 
CMAC neural network”, IEEE Transactions on 
Neural Networks, vol. 12, n. 3, p. 598 -603, May 
2001. 
[10] Lin, C.C.; Chen, F.C.; “Improved CMAC neural 
network control scheme”, Electronics Letters , 
vol. 35, n. 2, p.157 -158, Jan. 1999. 
[11] Gonzalez-Serrano, F.J.; Figueiras-Vidal, A.R.; 
Artes-Rodriguez, A.; “Generalizing CMAC 
architecture and training”, IEEE Transactions on 
Neural Networks, vol. 9, n. 6, p. 1509 -1514, 
Nov. 1998. 
[12] Ker, J.-S.; Kuo, Y.-H.; Liu, B.-D.; “Systolic 
implementation of higher-order CMAC and its 
application in colour calibration”, IEE 
Proceedings G- Circuits, Devices and Systems, 
vol. 144, n. 3, p. 129 -137, June 1997. 
[13] Wai, R.-J.; Lin, C.-M.; Peng, Y.-F.; “Robust 
CMAC neural network control for LLCC 
