Figure 1: Framework overview 
 
2. Related Work 
 
In the following, we investigate content- based 
video retrieval (CBVR), the background related to our 
work. Since we restrict our attention to the indexing and 
the matching in human motion retrieval, this survey will 
omit some computer vision issues such as object 
segmentation and tracking. 
 
Several content-based video retrieval (CBVR) 
frameworks [1-4] use shots as elemental units for 
indexing and matching, and visual features, like color, 
texture, shape, motion, are extracted as the shot 
representation. Among these visual features, motion 
provides a notice- able visual cue with respect to 
spatiotemporal relation in a video sequence. 
 
Some systems [6-9] utilize the motion trajectory, 
one of motion descriptors in MPEG-7 [5], to model more 
complex spatiotemporal relation. They mainly focus on a 
single object trajectory while giving less attention to 
multiple ones. However, human motion, which contains 
multiple trajectories of skeletal segments, corresponds to 
a high-dimension spatiotemporal feature vector. As we 
known, naïve table-lookup or tree-based techniques for 
high-dimension indexing are computationally expensive. 
To overcome the “curse of dimensionality,” Li et al. [10] 
and Sundaram and Chang [11] proposed algorithms to 
decompose a high-dimension feature vector into several 
low-dimension ones. Searching in low-dimension spaces 
can reduce the retrieval time for a given query. 
 
Another crucial issue is that the length of a 
consecutive human motion sequence may be very long. 
While most video retrieval systems segment a video 
sequence into small shots and perform matching at the 
shot level, the long- length motion sequence is not 
segmented in general because there is no interrupted 
transition (e.g., cuts, fades, etc.). It can be regarded as a 
very long shot. Therefore, in this study, we consider 
human motion retrieval as a process of finding 
sub-sequences that are similar to the query example in 
the given long-length motion sequence. 
 
In short, the multi-trajectories and long-length in a 
motion sequence are two major issues that distinguish 
this study of motion retrieval from conventional motion 
studies in video. 
 
3. Indexing 
Index Maps
Candidate Clip
Searching
Dynamic
Time Warping
Query
Online
Offline
User
Interface
Human Motion
Sequence
Posture
Representation
Index Map
Construction
Indexing
Result
Matching
 
Suppose that a human motion collection of frames is 
given. To achieve efficient and effective retrieval, a 
well-structured index mechanism should be developed. 
The first step is to select appropriate features to represent 
spatiotemporal information. In order to retrieve similar 
human motion trajectories regardless of their absolute 
positions, orientations, and skeleton sizes, a feature 
representation that is invariant to skeleton translation, 
rotation, and skeletal scaling is first proposed. Based on 
the proposed posture representation, we then cluster 
motion frames in the given collection through 
self-organizing map clustering to construct the index 
structure. In this section, we detail both the 
representation of the posture and the construction of the 
index structure. 
 
3.1. Posture Representation 
r ϕ
θ
N
v
π v'
r'
o
Y
 
Figure 2: Feature extraction by οv  
 
In this study, a simplified human skeleton model is 
defined according to H-Anim [12] and MPEG-4 Body 
Definition Parameters (BDPs) [13] standards. The 
human skeleton model in a frame consists of nine major 
skeletal segments and a root orientation. These skeletal 
segments are the torso, upper arms and legs, and lower 
arms and legs, while the root orientation is a unit vector 
starting at the root joint and directing to the torso facing 
direction. For the sake of achieving affine invariance of 
body transformations, local spherical coordinates relative 
to the root orientation represent the segment-posture of 
each skeletal segment. Figure 2 illustrates the 
segment-posture representation of a skeletal segment. 
Let ov be a skeletal segment vector at joint o, and or be 
the vector starting at o and being equivalent to the root 
orientation vector. Suppose π  is the plane passing 
through the joint o and parallel to the floor (e.g., the XZ 
plane). Let the projection of ov and or on π  be 
the 'ov and 'or respectively. Then θ and φ, the spherical 
coordinates of , are measured in angular radians 
fro
v
'ov to 'or and from 'ov to N respectively, where N be 
the normal vector of π   
from o. Then we have: 
X
Z
)
||'||||'||
''(cos 1
orov
orov
×
⋅= −θ )
||||||||
(cos 1
Nov
Nov
×
⋅= −ϕ
motion trajectories are similar to Q’s are retrieved from 
the given motion collection. Since the frame number in 
the collection is large and each frame contains many 
feature values (18 feature values in our case), searching 
similar clips at an interactive rate is not a trivial task. In 
order to achieve this goal, we propose a novel matching 
mechanism. The query example is first indexed in 
low-dimension index maps according to its respective 
skeletal segments. By integrating index results from 
these skeletal segments, candidate clips are then 
extracted from the given motion collection. Finally we 
compute the simi larity between the query example and 
each candidate clip through dynamic time warping. The 
matching mechanism is detailed in following sections. 
 
4.1. Candidate Clip Searching 
 
Based on the proposed index structure, an algorithm 
is developed to search candidate clips from the given 
human motion collection for a query example. We 
illustrate the procedure in Figure 3. Suppose that a clip is 
given as a query example. First, the start frame and the 
end frame of the query example are indexed into the 
index map of each skeletal segment to find the nearest 
cluster center and its neighboring cluster centers. If a 
frame in the given motion collection belongs to one of 
the above found clusters, we mark the frame on the 
frame axis. That is, the segment- posture of the marked 
frame is similar to that of the start (end) frame of the 
query example. Next, we count the marked frequency of 
each frame in the given motion collection by totaling all 
frame axes of index maps. These Frames whose marked 
frequencies are above a given threshold are denoted as 
start-candidates or end-candidates. Finally, candidate 
clips can be extracted according to the relative ordering 
among these start-candidates and end-candidates. 
 
Figure 3: Candidate clip searching 
 
Let the query example be denoted as 
, where the i-th 
frame  is represented by ,  is the 
segment-posture vector of the j-th skeletal segment, and 
start and end are the first and last frames of Q 
respectively. The searching algorithm is summarized in 
the following. 
},...,4,3,2,|{ endstartifQ i ==
if ),...,,( )9()2()1( iii bbb )( jib
Algorithm 2. Candidate Clip Searching 
Input: The query example Q, the posture sequences 
)( jP  of the given motion collection Ω, and the 
index maps )( jΜ  of nine skeletal segments, 
9,...,2,1=j . 
Output: a set of candidate clips. 
Step:  
1:  Initialize a histogram array , 
where stores the occurring frequency for the 
i-th frame and n is the number of frames in Ω. 
):1( nH
)(iH
2:  Compute start-candidates for the j-th skeletal 
segment, 9,...,2,1=j . 
2.1: Compute the index of  (the first frame of 
Q): 
)( j
startb
||||minarg_ )()( jk
j
startk
Cbstartk −= , 
where . (/* Find the nearest cluster 
center in the index map . */) 
)()( jj
kC Μ∈
)( jΜ
2.2: Find the δ-neighborhood  of 
, where δ is the neighborhood radius. If 
the segment-posture of frame 
)( )( _
j
startkCNBδ
)(
_
j
startkC
Ω∈iF  belongs 
(indexes) to , set )( )( _
j
startkCNBδ 1)()( += iHiH . 
3:  If H(i) is greater than a given threshold, mark the 
i-th frame as a start-candidate in Ω. If several 
consecutive frames are marked as start-candidates, 
select their median frame as the start-candidate only. 
4:  Find end-candidates (similar to Step 1-3). 
5:  Find all candidate clips. 
5.1: Sort all frame indices of start- start-candidates 
and end-candidates found in Step 3 and Step 4. 
5.2: Sweep the sorted list and report clip 
),( es FFY =  in Ω as a candidate clip according 
to following conditions, where Fs and Fe are the 
start-candidate and end-candidate respectively, 
and s and e are frame indices in Ω: 
Query Example
Index Map
S S S SE E E EE
Threshold
.........
...
...
Torso
Left
Upper Arm
Right
Lower Leg
Start Frame End Frame
...
...
Frame Axis
Indexing
...
...
...
...
...
...
Frame Axis
Frame Axis
Candidate Clip Frame Axis
Counting
Frame Axis
(1) es <  
(2) There is no other start-candidates and end- 
candidates lies between Fs and Fe. 
 
In the searching algorithm, we consider the 
high-dimension index space as the direct sum of 
low-dimension ones to reduce the time complexity for 
nearest neighbor search. Besides, the candidate clip 
searching algorithm can avoid exhausting search in the 
long-length sequence of mot- ion collection. These 
advantages can effectively shorten the computation time 
in matching. 
 
4.2. Dynamic Time Warping 
 
In this study, we apply the Dynamic Time Warping 
(DTW) technique [15] to compute the similarity between 
the query example Q and the candidate clip Y. DTW is a 
well-known matching method in the field of speech and 
motion recognition. Even if the lengths of two patterns 
are different, DTW can compensate for length difference 
clustering to arrange 312 cluster centers for all 
skeletal segments. In the proposed and fixed-grid 
methods, however, there are 312 cluster centers for 
each of nine skeletal segments and totally 2808 
cluster centers for the whole body. It is known that 
using more cluster centers can obtain better retrieval 
accuracy. Therefore, the k-d tree method has worse 
accuracy performance than other two methods. 
 
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
recall
pr
ec
is
io
n
Proposed
Fixed-grid
k-d Tree
 
Figure 4: The PR graph for three indexing methods 
 
Table. 1: The matching time cost for three indexing methods 
 
 Proposed Fixed-grid k-d Tree
Testing Set 
Size 5 5 5 
Minimal 
Window 
Size 
6 12 6 
Searching 
Time 0.110 sec 0.125 sec 0.531 sec
Similarity 
Computation 
Time 
0.187 sec 0.234 sec 0.031 sec
Total Time 0.297 sec 0.359 sec 0.562 sec
 
5.3. Retrieval Time 
 
Another performance evaluation is the time cost in 
matching. The matching time cost consists of the 
searching time cost (in candidate clip searching) and the 
similarity computation cost (in dynamic time warping). 
To compute the matching time cost, a group consisting 
of five relevant clips in our ground truth is first selected 
as the testing group. Then given a query example from 
the testing group, we gradually enlarge the window size 
and perform retrieval. When all five relevant clips of the 
testing set are retrieved at a minimal window size, we 
evaluate the matching time cost for the minimal window 
size. Table 1 lists the matching time cost of the 
above-mentioned three methods. We observe that the 
retrieval time of the proposed method is less than those 
of the fixed-grid and k-d tree methods. Based on Table 1, 
we have the following discussions: 
 
• For the similarity computation cost, the k-d tree 
method spends much more time than the proposed 
and fixed-grid methods. This is because indexing 
(nearest neighbor search) in a high-dimension k-d tree 
is computationally expensive; the time complexity 
grows exponentially with the number of dimensions. 
However, other two methods are indexing in 
low-dimension spaces; the time complexity grows 
linearly with the number of spaces. 
• For the similarity computation cost, the k-d tree 
method spends less time than the other two methods. 
This is because the DTW algorithm is processed for 
the all-skeletal segments in the k-d tree method. 
Therefore it does not need extra time for integrating 
information from the all-skeletal segments. 
• The matching time cost of the proposed method is 
slightly less than that of the fixed-grid method. In the 
fixed-grid method, since the imbalanced clustering 
impairs its retrieval accuracy, to retrieve all five 
relevant clips of the testing set requires a larger 
window size. Therefore, longer retrieval time is 
needed for processing the larger window size. 
 
6. Conclusions and Future Directions 
 
In this study, we propose a novel framework for 
constructing a content-based human motion retrieval 
system. The major components, namely, indexing and 
matching, are discussed and their corresponding 
algorithms are presented. In indexing, we introduce an 
affine invariant posture representation and propose a 
SOM-based index map according to the distribution of 
the raw data. In matching, the start and end frames of the 
query example are used to find candidate clips from the 
given motion collection. Then the similarity between the 
query example and each candidate clip is computed by 
using the dynamic time warping algorithm. To avoid the 
curse of dimensionality, the high-dimension feature 
space of the entire skeleton are decomposed into the 
direct sum of low-dimension feature spaces of skeletal 
segments. Besides, several experimental tests show that 
the proposed indexing method performs better than 
conventional fixed-grid and k-d tree methods in the 
aspects of retrieval accuracy and computation time. 
 
For future work, we will develop a more friendly 
user interface to assist user for specifying their queries. 
For example, we try to develop a motion script language 
so that users can specify a motion example through 
motion scripts. Another interesting research direction is 
to extend our framework to motion synthesis by 
examples [19, 20]. 
 
References 
 
[1]  S. W. Smoliar, H. J. Zhang, Content-based video 
indexing and retrieval, IEEE Multimedia, 1(2), 
1994, pp. 62-72. 
[2]  A. K. Jain, A. Vailaya, X. Wei, Query by video clip, 
Multimedia Systems, 7(5), 1999, pp. 369-384. 
[3]  M. R. Naphade, T. S. Huang, A robabilistic 
framework for semantic video indexing, filtering 
and retrieval, IEEE Transactions on Multimedia, 
3(1), 2001, pp. 141-151. 
