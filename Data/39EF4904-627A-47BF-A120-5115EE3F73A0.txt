I摘要
在多數電腦遊戲中，電腦對手通常是以人工撰寫的腳本來控制。然而撰寫腳本經常
得耗費遊戲開發者許多時間與精力，同時此過程亦是冗長與乏味的。此外，電腦對手的
遊戲策略通常相當有限且固定，因此當玩家玩了一段時間之後，便能察覺其行為模式的
重覆性，於是感覺失去挑戰性而覺得無趣。再者，遊戲中固定的難易度設定亦無法滿足
市場上各式各樣、不同程度的玩家。為了克服上述問題，已有研究者提出以離線學習方
法來自動產生電腦對手之策略，以省去人工撰寫腳本的時間成本。另外則有研究提出線
上學習方法以進行動態難度調整，使遊戲的難易度能自動調整到符合玩家的程度。然而
這些方法大多使用類神經網路來學習遊戲策略，因此有不易理解及難以維護的缺點。另
外許多動態難度調整技術之效率不佳，因而難以實際運用到商業遊戲中。為了改進現有
方法的缺點，本研究提出一套以模糊邏輯為基礎的離線及線上學習方法。在離線學習的
部份，本研究提出以基因演算法自動產生一組模糊規則庫，以作為電腦對手之策略。而
線上學習的部份，本研究則提出以機率方法來調整電腦對手之程度，使得遊戲難度能貼
近玩家的程度，以自動達到遊戲平衡。實驗結果證明了此一離線學習方法所產生的模糊
規則庫，可有效地控制電腦對手。同時實驗結果亦證明本研究之線上學習方法可有效率
地調整遊戲難易度，並且適用各種不同程度的玩家。
關鍵字：電腦對手、遊戲人工智慧、模糊規則、基因演算法、動態難度調整、獵食者與
獵物遊戲
III
Table of Contents
Chinese Abstract…………………………………………………………….……………I
English Abstract………………………………………………………………………….II
Table of Contents……………………………………………………………………..….III
1. Introduction……………………………………………………………………………01
2. The Dead End World…………………………………………………….……………06
3. Offline Generation of Game Tactics……………………………………….…...…….09
3.1 Specification of Fuzzy Variables………………………………………….…..…….09
3.2 Evolution of Fuzzy Rules.…………………………………………………….…….12
3.2.1 Encoding of Chromosome…………………………………………………….12
3.2.2 Fitness Value of Chromosome………………………………………………...14
3.2.3 Evolution of Chromosome…………………………………………………….16
3.3 Experimental Results………………………………………………………………..17
4. Online Adaptation of Game Tactics………………………………….……………….21
4.1 The Online Adapting Approach………………………………………..……………21
4.2 Experimental Results………………………………………………………………..25
5. Conclusions……………………………………………………………………………..34
References…………………………………………………………………….…………...36
Appendix A. List of Publications of this Project………………………………………...38
Appendix B. 可供推廣之研發成果資料表………………………………...…………….39
2development), whereas it is called online if applied after a game’s release (or during
gameplay). The purpose of using offline learning is to automatically generate the game tactics
of computer-controlled opponents such that the development cost of game AI may be reduced.
The use of online learning is to attain the purpose of dynamic difficulty adjustment by
adapting the computer-controlled opponents’tactics to the behavior of players so that game
balance can be achieved. The maintenance of game balance increases a player’s interest in a
game.
For offline learning, Geisler [8] applied several machine learning techniques to a
first-person shooter game, including decision trees, Naïve Bayes classifiers, and neural
networks. These learning techniques require a large amount of training examples, which are
obtained by modeling the expert players’behavior. Spronck et al. [9] proposed a
neuro-evolutionary technique to train a computer-controlled opponent of a shooting game by
playing with a scripted opponent. Lucas [10] used a neural network as a location evaluator for
the game Ms. Pac-Man. The evolved neural network takes a feature vector based on a
candidate maze location as input, and produces a score for that location as output to guide
Pac-Man’s movement. Jin et al. [11] suggested a method called adaptive constraint of
evolution to evolve agents in a shooting game. The method reduces the computation cost of
evolving a multi-weights neural network and prevents evolution from premature convergence.
These neural network-based approaches evolve game tactics without any training sample, but
their lack of transparency enhances the difficulty of maintenance for game developers.
The purpose of online learning is to increase the player’s interest in a game through
dynamic difficulty adjustment. The effectiveness of dynamic difficulty adjustment is
supported by Csíkszentmihályi’s research on flow [18]. Flow is the holistic sensation that
people feel when they act with total involvement [19]. Csíkszentmihályi identified nine
elements of the flow experience, and one of the elements is balance between ability level and
4round ends. If the weight of a rule exceeds an upper limit, it is disabled. Game balance is
achieved by adjusting the upper limit according to the player’s win-loss rate. The number of
rounds needed for learning sometimes varies greatly even when playing against the same
fixed-strategy player. It shows the instability of this method.
Fig. 1.1 The flow channel
In this study, we propose both offline and online learning approaches for a predator-prey
game. In offline learning, we use an evolutionary approach to generating a fuzzy rulebase,
which will be used as the game tactics during gameplay to guide the computer-controlled
opponents. The use of fuzzy rules makes game AI easy to design. The evolved fuzzy rulebase
is easily understood and modified by game developers. Furthermore, since fuzzy rule systems
generally require fewer rules than crisp rule systems to accomplish identical tasks [20], the
overhead of maintenance may be reduced because of fewer rules required. In online learning,
we use a probabilistic method to adapt the game tactics to the player. In adaptation, we
evaluate whether the difficulty level of the game fits for a player according to his/her win-loss
rate. The win-loss rate is defined as the ratio of the number of wins to the number of losses
from the game begins. The difficulty level of the game is said to fit for the player if his/her
win-loss rate achieves a desired rate, which can be set by the player before gameplay
according to his/her preference for challenge. In the proposed method, the use of offline
62. The Dead End World
In this study, we choose a predator-prey game called Dead End [21] as our test bed
because it is an interesting multi-agent game and its graphics is simple. Hence, we can place
emphasis on the investigation of game AI but not graphics. Dead End is a two-dimensional
multi-agent predator-prey game. Fig. 2.1 shows a snapshot of a Dead End game. In the game
field, the x axis directs from left to right and the y axis from top to bottom. The characters at
the upper place of the game field are Ghosts, and the one at the lower is Player. The top of the
game field with no barrier is assumed as the exit. The dimension of the game field is 320 ×
480 pixels, and that of Ghosts and Player is 20 × 20 pixels. Player is initially at the lower
place of the game field, while Ghosts are at the upper ones.
Fig. 2.1 The snapshot of the Dead End game
In the Dead End game, Player aims to reach the exit and avoid Ghosts, whereas Ghosts
aim to defend the exit and to kill Player. Player owns health points which denote Player’s 
health. A Ghost attacks Player by touching Player. Once Player is touched by a Ghost,
8where ),( pp yx denotes the Player’s coordinate after a movement, and py also denotes the
y-axis distance from Player to the exit. Since the whole top of the game field is the exit, the
distance from Player to the exit is equal to py . gN is the number of Ghosts;  )(),( nynx gg
is the coordinate of the thn Ghost. The parameter  indicates the weight of ),( pp yxG to
),( pp yxC . A low value of  denotes that CB Player prefers moving toward the exit to
avoiding Ghosts, whereas a high value of  denotes that avoiding Ghosts has high priority
for CB Player. By changing the value of , CB Players with different ghost-avoiding and
exit-achieving strategies can be obtained. Before making a movement, CB Player first
calculates the cost of each possible movement in 20 steps, that is, 40 pixels. Then it moves 20
steps along the path with the minimal cost.
10
consequents of fuzzy rules. The five input variables provide Ghost with information about
distances to Player, to the exit, to the nearest Ghost, and directions to Player and to the nearest
Ghost, as listed in Table 3.2. The four fuzzy output variables are used to derive Ghost’s 
moving direction, as listed in Table 3.3. The defuzzified output of a fuzzy rule is used as a
weight of the derived moving direction. For example, the defuzzified output of a rule whose
consequent is ChasePlayer is used as a weight of the moving direction toward Player. When
two or more rules are activated at a time, the weights corresponding to the same derived
moving direction are summed. Then Ghost’s moving direction is the derived one with the
highest weight.
Table 3.2 Input variables
Variable name Meaning
DistToPlayer Distance from Ghost to Player
DirToPlayer Direction from Ghost to Player
DistToExit Distance from Ghost to the exit
DistToGhost Distance from Ghost to the nearest partner
DirToGhost Direction from Ghost to the nearest partner
Table 3.3 Output variables
Variable name Meaning
ChasePlayer Move toward Player
InterceptPlayer Move toward Player in x-axis and move toward the exit in y-axis
ApproachGhost Move toward the nearest Ghost
PartFromGhost Move apart from the nearest Ghost in x-axis and move toward Playerin y-axis
We define three fuzzy sets for each fuzzy variable, as listed in Table 3.4. Their
membership functions are obtained through evolution except for the two input variables
DirToPlayer and DirToGhost, because the membership functions of directions can be easily
12
Fig. 3.2 Membership functions of DirToPlayer and DirToGhost
3.2 Evolution of Fuzzy Rules
In this study, we apply a genetic algorithm to automatically generate a fuzzy rulebase for
controlling Ghosts in the Dead End game. The encoding of chromosome, the computation of
fitness value, and the genetic operators used in evolution are described in the following.
3.2.1 Encoding of Chromosome
The chromosome architecture used in our approach is similar to that proposed by
Angelov and Buswell [25]. A chromosome is composed of the coding of both a complete
rulebase and the membership functions of the fuzzy variables, as shown in Fig. 3.3. In the left
part of the chromosome, a gene denotes a rule. In the right part of the chromosome, a gene
denotes a parameter of the membership functions of a fuzzy variable.
R1 R2 ... Rn c1 w1 c2 w2 ... c7 w7
Rulebase Membership functions
Fig. 3.3 The chromosome architecture
The antecedent and consequent of a rule is encoded separately. We use five numbers
corresponding to the above-defined five fuzzy input variables to encode the antecedent of a
14
3, 0, 2].
To simplify the encoding of membership functions, we assume that each fuzzy set
overlaps its adjacent set by 25 percent for smooth transitions and the membership functions of
fuzzy sets are of symmetric shape. Once the center and width of the membership function of
the medium fuzzy set are given, membership functions of the two adjacent sets can be derived.
Fig. 3.4 shows an example to illustrate the relationship of the fuzzy sets near, medium, and far.
In this figure, c and w denote the center and width, respectively, of the medium fuzzy set; min
and max are the predefined minimum and maximum values of the crisp input. The encoding
of membership functions of a fuzzy variable is to pair its c and w into two-tuples. Except for
the two fuzzy variables DirToPlayer and DirToGhost (whose membership functions are
predefined, as shown in Fig. 3.2), the membership functions of the other seven fuzzy variables
need to be encoded into the chromosome. Thus, as shown in Fig. 3.3, a chromosome consists
of a set of rule codes and seven pairs of tuples defining membership functions of the seven
fuzzy variables.
Fig. 3.4 An example to illustrate the relationship of the fuzzy sets near, medium, and far
3.2.2 Fitness Value of Chromosome
In evolution, we consider different players and different initial settings of the Dead End
16
to 1; if Ghosts lose all the roundN rounds, the value of W is equal to 0. )(iScore represents
Ghosts’game score of the thi round; )(iH is the ratio of the Player’s number of health
points to the Player’s initial number of health points when the thi round ends; )(_ iD ep
denotes the y-axis distance from Player to the exit when the thi round ends; ),(_ inD pg is
the distance from the thn Ghost to Player when the thi round ends; gN is the number of
Ghosts; )(_ iD movp is the total distance that Player moves in the
thi round. The fitness value
of a chromosome is in the range [0, 1) if W is less than 1, but in the range (1, 2] if W is equal
to 1.
3.2.3 Evolution of Chromosome
To evolve the population, three genetic operators including selection, crossover, and
mutation are used. In the generation of a new population, the best two chromosomes are first
selected to clone but crossover and mutation are skipped. Afterward, two different
chromosomes are selected by using roulette selection, and then crossover and mutation are
applied. The above roulette selection, crossover, and mutation are repeated until enough
offspring are propagated in the new population.
In the evolution, different crossover operators are used for rule genes and
membership-function genes. We adopt the randomized single-point crossover for rule genes
and Wright’s heuristic crossover [26] for membership-function genes. The mutation operators
for rule genes and membership-function genes are slightly different. For rule genes, the
mutation operation randomly replaces one index number in a gene with other randomly
generated index number. For membership-function genes, the parameter c (or w) is mutated
and replaced by a randomly generated real number, but this real number should bear the
characteristic that the mutated membership functions of the corresponding fuzzy variable also
18
144 ( posggroupgpospp NNNN ___  ) rounds with different players and initial settings, and its
fitness value is calculated through Eqs. (3)-(6). The values of  of the four CB Players are
50000, 200000, 300000, and 550000. Player’s three different initial positions are shown in Fig.
3.5, where the positions are left, middle, and right. Ghosts’four different initial positions are
shown in Fig. 3.6, where the formations from left to right are of types centering, horizontal,
vertical, and expanding. The evolution stops when 1000 generations have been evolved, and
the best chromosome is recorded. Besides, the rules whose derived moving direction is never
used to guide Ghosts in the 144 rounds of evaluation are removed from the best chromosome.
The above evolution process was repeated for five runs with different initial populations, and
therefore we obtained five best chromosomes in this phase. The fitness values of the five
evolved chromosomes are listed in Table 3.5. As shown in the table, all the five evolved
chromosomes had a fitness value higher than one, which means Ghosts controlled by them
achieved the win-loss rate of 100% in the 144 rounds (as explained in Section 3.2.2).
In the second phase of the experiment, we tested the performance of the five evolved
best rulebases obtained from the first phase. In the test, we used GA Player and another four
CB Players different from those in the first phase. The values of  of the four CB Players
used in testing are 100000, 250000, 400000, and 800000. Similar to the first phase, each of
the five evolved rulebases is tested by playing with GA Player and the four CB Players with
different initial settings. The average and standard deviation of the win-loss rates of the five
evolved rulebases in the test are listed in Table 3.6. To examine the influence of the number of
Ghosts on win-loss rates, the win-loss rates for different numbers of Ghosts are separately
listed in the table. Table 3.6 shows that Ghosts achieve the win-loss rate of at least 96.33%;
hence the evolved rulebase is well applicable to the game with different Players and different
numbers of Ghosts.
20
Table 3.6 Ghosts’win-loss rates in the testing phase
Number
of
Ghosts
Average StandardDeviation
3 96.33% 3.21%
4 96.67% 2.04%
5 98.33% 1.67%
22
directly toward Player.
We evaluate whether the difficulty level of the game fits for Player according to Player’s
win-loss rate.Player’s and Ghosts’ skil levels are said to be balancedif Player’s win-loss rate
achieves a desired rate, which is set by the player before gameplay according to his/her
preference for challenge. If the player prefers a game with more (or less) challenge, he/she
can set a desired rate with the value smaller (or larger) than 50%. The default value of the
desired rate is 50% if the player does not set its value. The game is considered hard for Player
if Player’s win-loss rate is decreasing during gameplay and is lower than the desired rate. On
the other hand, if Player’s win-loss rate is increasing and is higher than the desired rate, we
consider the game easy for Player. Fig. 4.1 shows the flowchart of the proposed online
adapting approach. The details are described in the following.
As shown in the flowchart, we first initialize the values of p and goal before the game
starts. The p denotes the probability that Ghosts use suboptimal tactics, and the goal is used to
indicate either“Raise the win-loss rate”or“Lower the win-loss rate.”Initially, the parameter
p is set to 1%, and goal is set to“Raise the win-loss rate”because Player’s win-loss rate is 0%
when the game starts. Then we evaluate whether the difficulty level of the game fits for Player
every two rounds. The first step of the evaluation is to check if goal needs to be changed by
comparing Player’s current win-loss rate with the desired rate. If the value of goal is “Raise 
the win-loss rate”but Player’s win-loss rate is higher than the desired rate, the value of goal is
changed to“Lower the win-loss rate.”In contrast, if the value of goal is “Lower the win-loss
rate”but Player’s win-loss rate is lower than the desired rate, the value of goal is changed to
“Raise the win-loss rate.”If goal is changed from “Lower the win-loss rate”to “Raise the 
win-loss rate”(or from“Raise the win-loss rate”to“Lower the win-loss rate”), the value of p
is multiplied (or divided) by two. If goal is not changed, we check whether the game is hard
or easy for Player. The game is considered hard for Player if Player’s win-loss rate is
24
Fig. 4.1 The flowchart of the online adapting approach
26
Table 4.1 The computational requirements for an online learning technique
Requirement Description
Speed
Online learning must be computationally fast, because it takes place during
gameplay.
Effectiveness
The online learning technique must be effective, to prevent agents from
learning inferior behavior.
Robustness
The online learning technique has to be robust with respect to the
randomness inherent in most games.
Efficiency
The online learning technique must achieve the adaptation goal within
acceptable time.
Table 4.2 The functional requirements for an online learning technique
Requirement Description
Clarity
The learned results must be easily interpretable, because game developers
distrust those which are hard to understand.
Variety
The online learning technique must produce a variety of different behaviors,
because predictable opponents are less entertaining than unpredictable ones.
Consistency
The average number of rounds required in learning to achieve the
adaptation goal should have a low variance, to ensure that its achievement
is independent both from the behavior of the player and from random
fluctuation in the learning process.
Scalability
The online learning technique must be able to scale the difficulty level of
the game to fit for players with different skill levels.
To show the proposed online adapting approach fulfills the other requirements (including
efficiency, robustness, consistency, and scalability), three set of experiments (called
Experiments A, B, and C) are conducted. In these experiments, Ghosts are controlled by the
same evolved rulebase obtained from the offline experiment. The horizontal formation of the
second one in Fig. 3.6 is selected as Player’s and Ghosts’initial positions, because the
formation is fair to both of them. Experiment A aimed at evaluating the robustness of the
28
Table 4.3 Players’win-loss rates in Experiment A
Player A
( = 50000)
Player B
( = 200000)
Player C
( = 550000)
The value
of p Average
win-loss
rate
Standard
deviation
Average
win-loss
rate
Standard
deviation
Average
win-loss
rate
Standard
deviation
0.25% 6.4% 2.91% 32.4% 5.06% 21% 6.24%
0.5% 12.2% 3.36% 47.9% 5.78% 60.1% 6.54%
1% 24.4% 4.74% 64.4% 6.11% 89.6% 3.07%
2% 37.8% 5.29% 79.6% 3.86% 93.3% 2.71%
4% 60.8% 4.69% 92.7% 4.14% 98.6% 1.35%
Experiment B is conducted to evaluate the efficiency and scalability of the proposed
approach. We ran the experiment with different numbers of Ghosts; they are three, four, and
five. The different numbers of Ghosts correspond to different difficulty levels of the game.
The experiments are conducted by playing against the same three Players as those in
Experiment A, that is, Player A, B, and C. Besides, GA Player is also used in the experiments
to examine whether the proposed approach can adapt Ghosts’tactics to novice players. We set
three desired win-loss rates: 25%, 50%, and 75%. For each Player, we ran 100 rounds and
recorded its win-loss rates every round. The experimental results are shown in Figs. 4.2-4.5.
These figures demonstrate that the online adapting approach can efficiently adapt Ghosts’
level to Player’s desired level after several dozens of rounds. Players’win-loss rates usually
varied unsteadily before the 20th round, because the first several rounds are spent to find the
appropriate value of p to fit for the desired rate. Furthermore, the differences between Players’
win-loss rates and the desired rates were all within 2% after 100 rounds of adaptation, which
shows that Players’win-loss rate can be kept well after achieving the desired rate.
30
0
0.25
0.5
0.75
1
1 11 21 31 41 51 61 71 81 91
number of rounds
w
in
-l
os
s
ra
te
(%
)
A
B
C
(a) Desired rate = 25%
0
0.25
0.5
0.75
1
1 11 21 31 41 51 61 71 81 91
number of rounds
w
in
-l
os
s
ra
te
(%
)
A
B
C25
50
75
100
0
0 20 30 40 50 60 70 80 90 100
(b) Desired rate = 50%
0
0.25
0.5
0.75
1
1 11 21 31 41 51 61 71 81 91
number of rounds
w
in
-l
os
s
ra
te
(%
)
A
B
C
(c) Desired rate = 75%
Fig. 4.3 The variations of Players’win-loss rates (number of Ghosts: 4)
32
0
0.25
0.5
0.75
1
1 11 21 31 41 51 61 71 81 91
number of rounds
w
in
-l
os
s
ra
te
(%
)
25%
50%
75%
(a) Number of Ghosts: 3
0
0.25
0.5
0.75
1
1 11 21 31 41 51 61 71 81 91
number of rounds
w
in
-lo
ss
ra
te
(%
)
25%
50%
75%
(b) Number of Ghosts: 4
0
0.25
0.5
0.75
1
1 11 21 31 41 51 61 71 81 91
number of rounds
w
in
-l
os
s
ra
te
(%
)
25%
50%
75%
(c) Number of Ghosts: 5
Fig. 4.5 The variations of Players’win-loss rates (GA Player)
34
5. Conclusions
In this study, we proposed an offline evolutionary approach to automatic generation of
game tactics, and an online adapting approach to dynamic difficulty adjustment. The offline
evolutionary approach produces a fuzzy rulebase to guide Ghosts in the Dead End game. In
the offline approach, we first design a set of fuzzy variables to model Ghosts’movement, and
then encode fuzzy rules and membership functions of the fuzzy variables into chromosomes
for evolution. The results of the experiments demonstrate the effectiveness of the evolved
fuzzy rulebase. Besides, the evolved fuzzy rulebases are well applicable to the game with
different CB Players and different numbers of Ghosts. The online approach adapts Ghosts’
tactics for Player based on an adjustable probability to balance their skill levels. The
experimental results show that the online approach can adapt Ghosts’tactics to fit for different
CB Players and to achieve different desired win-loss rates. Furthermore, the online adapting
approach also fulfills all the computational and functional requirements proposed by Spronck
[17], and this proves the feasibility of the proposed approach. In conclusion, the use of the
offline evolutionary approach saves the development cost of coding game tactics by hand, and
the use of the online adapting approach provides game balance and provides players with both
challenging and interesting gameplay experiences.
Although our experiments were conducted on the Dead End game, there is no
requirement in the proposed approach that is specific to the game. To apply the proposed
approach to other games, the main modification required is to design the domain-specific
fuzzy variables, because game actions and information required by game characters are
different depending on game contents. Moreover, in some games, a character may have
different states, and the game actions the character can take are different in each state. For
example, in the Pac-Man game, the role of Pac-Man changes from a prey to a predator when it
eats a power pill. Generally, in this case a Boolean variable is used in the antecedent of a rule
36
References
[1] J. E. Laird and M. van Lent, “Human-level AI’s kiler application: computer game AI,” 
in Proceedings of AAAI 2000 Fall Symposium on Simulating Human Agents, 2000, pp.
80-87.
[2] S. Rabin, “Common game AI techniques,”in AI Game Programming Wisdom 2, S.
Rabin Ed. Hingham, MA: Charles River Media, 2004, pp. 3-14.
[3] J. Manslow, “Learning and adaptation,”in AI Game Programming Wisdom, S. Rabin Ed.
Hingham, MA: Charles River Media, 2002, pp. 557-566.
[4] Burnout, retrieved June 19, 2008, from web site: http://burnout.ea.com/
[5] Mario Kart, retrieved June 19, 2008, from web site: http://www.mariokart.com/
[6] Rubber band AI, retrieved June 19, 2008, from web site:
http://en.wikipedia.org/wiki/Rubberband_effect
[7] B. Scott, “The illusion of intelligence,”in AI Game Programming Wisdom, S. Rabin Ed.
Hingham, MA: Charles River Media, 2002, pp. 16-20.
[8] B. Geisler, An Empirical Study of Machine Learning Algorithms Applied to Modeling
Player Behavior in a First Person Shooter Video Game. MS Thesis, Department of
Computer Sciences, The University of Wisconsin at Madison. 2002.
[9] P. Spronck, I. Sprinkhuizen-Kuyper, and E. Postma,“Improving opponent intelligence
through offline evolutionary learning,” International Journal of Intelligent Games and
Simulation, vol. 2, no. 1, pp. 20-27, Feb. 2003.
[10] S. M.Lucas, “Evolving a neural network location evaluator to play Ms. Pac-Man,”in
Proceedings of the IEEE Symposium on Computational Intelligence and Games,
Piscataway, NJ, 2005, pp. 203-210.
[11] X. H. Jin, D. H. Jang, and T. Y. Kim,“Evolving game agents based on adaptive
constraint of evolution,”in Proceedings of International Conference on Convergence
Information Technology, Korea, Nov. 2007.
[12] P. Demasi and A. J. de O. Cruz, “Online coevolution for action games,” International
Journal of Intelligent Games and Simulation, vol. 2, no. 2, pp. 80-88, 2003.
[13] R. Houlette, “Player modeling for adaptive games,”in AI Game Programming Wisdom 2,
S. Rabin Ed. Hingham, MA: Charles River Media, 2004, pp. 557-566.
[14] R. Hunicke and V. Chapman, “AI for dynamic difficult adjustment in games,” in
Proceedings of Challenges in Game Artificial Intelligence AAAI Workshop, San Jose,
38
Appendix A. List of Publications of this Project
1. H. M. Hsieh and L. L. Wang, “A fuzzy approach to generating adaptive opponents in a 
Dead End game,” Asian Journal of Health and Information Sciences, Vol. 3, Nos. 1-4, pp.
39-57, 2008.
2.H. M. Hsieh and L. L. Wang, “Generation of adaptive opponents in a predator/prey game,” 
submitted for publication.
3. H. M. Hsieh and L. L. Wang, “A fuzzy approach to generating adaptive opponents for a
predator/prey game,”Proceedings of the 12th Conference on Artificial Intelligence and
Applications (TAAI), Yunlin, Taiwan, Nov. 2007.
4. H. M. Hsieh and L. L. Wang, “An adaptive predator/prey game,” Proceedings of National 
Computer Symposium (NCS), Taichung, Taiwan, Dec. 2007.
