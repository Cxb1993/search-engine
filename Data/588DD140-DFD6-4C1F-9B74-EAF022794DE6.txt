 2 
中文摘要 
 
本計畫研究的目的在探討視覺密度 ”Visual Density”對圖文合成作品(text-overlaid images)美感
的影響，並提出其計算模型。計畫定義出三種視覺密度的計算模型，包括 Page-based Lump 
Density (DPL)，Page-based Weighted Lump Density (DWPL) 及 Background-based Text Density 
(DBT)。實驗中觀察到 DBT 與人類對圖文合成作品美感程度呈現明顯相關性，經由實驗也找出了
最佳的 DBT 約等於 0.12。基於視覺平衡與視覺密度 DBT 與美感的關係式，本計畫開發了一個利
用粒子群最佳化演算法（PSO）來計算”最美”文字擺放位置的最佳化引擎，從而建立出一套智
慧型的圖文合成系統。 
 
關鍵詞: 計算美學，視覺密度，自動化圖文合成系統 
 
 
Abstract 
Computational models for the visual density of colored text-overlaid images are proposed in this 
project. We define Page-based Lump Density (DPL), Page-based Weighted Lump Density (DWPL) 
and Background-based Text Density (DBT). The relationships between these three models and 
human perceptions on the aesthetic appeal of text-overlaid images were investigated using human 
subjective rating experiments. The experiments show the Background-based Text Density (DBT) 
is significantly related to human subjective ratings. Also, it is shown that human considers 0.12 as 
the best DBT that maximize the aesthetic appeal of a text-overlaid image. Based on the analysis of 
the computation aesthetics of text-overlaid images, we have implemented a prototype image 
composing system which first automatically partitions the input texts to a sequence of paragraphs, 
and then chooses the optimal position in a given image for placing a partitioned paragraph such 
that the final DBT and visual balance is optimized.  
 
Keywords: computational aesthetics, visual density, composing text-overlaid images  
 
 
 
 4 
  
(a)                (b) 
  
(c)                 (d) 
圖一：圖文合成作品範例及 Notation 相關示意圖 
 
  
 
Notations （參考圖 1）: 
w: width of image in pixels  
h: 
I  
height of image in pixels 
entire region of the original image 
F : all region(s) containing foreground object(s) in original image 
B  background region(s), FIB   
T : text region 
Ab: area of all pixels in background region B  
At: area of all pixels in text region T  
ΔCi: HSV color difference between a pixel i and background region 
 
 hwAAhwD tbPL  /)(  
 hwCD
TFi
iWPL  

/)(
)(
 
btBT AAD /  
 
 
 
 6 
 
圖二：圖像美感的評估介面 
 
Table 1: 測試圖片的視覺平衡 BM 值，BM= (BV+BH)/2 
PHOTO4 PHOTO6 PHOTO8 PHOTO10 PHOTO11 
文字數 (BV+BH)/2 
30 0.884 
60 0.930 
90 0.895 
120 0.861 
150 0.850 
180 0.855 
210 0.871 
240 0.846 
 
文字數 (BV+BH)/2 
50 0.912 
100 0.889 
150 0.862 
200 0.848 
250 0.855 
300 0.874 
 
文字數 (BV+BH)/2 
32 0.826 
64 0.898 
96 0.916 
128 0.942 
160 0.973 
192 0.968 
224 0.953 
 
文字數 (BV+BH)/2 
30 0.919 
60 0.963 
90 0.978 
120 0.946 
150 0.920 
180 0.899 
210 0.881 
 
文字數 (BV+BH)/2 
30 0.813 
60 0.887 
90 0.943 
120 0.965 
150 0.976 
180 0.946 
210 0.951 
240 0.923 
 
PHOTO12 PHOTO14    
文字數 (BV+BH)/2 
30 0.812 
60 0.911 
90 0.940 
120 0.965 
150 0.964 
180 0.955 
210 0.930 
240 0.889 
 
文字數 (BV+BH)/2 
32 0.937 
64 0.951 
96 0.891 
128 0.862 
160 0.853 
192 0.858 
 
   
 8 
  
  
  
(a) (b) 
圖三： (a)平均美感分數與 Page-based Weighted Lump Density (DWPL) 的分佈圖 (b) 平均美感
分數與 Page-based Lump Density (DPL)的分佈圖  
 
 
 10 
PHOTO4 PHOTO6 PHOTO8 
1.64
1.73
1.77 1.78
1.70
1.65
1.56
1.60
1.50
1.55
1.60
1.65
1.70
1.75
1.80
0.03 0.06 0.10 0.13 0.16 0.19 0.23 0.26
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
1.53
1.74
1.70
1.66
1.61
1.55
1.40
1.50
1.60
1.70
1.80
0.06 0.13 0.19 0.26 0.32 0.38
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
1.58
1.74
1.78
1.70 1.71
1.66
1.53
1.50
1.55
1.60
1.65
1.70
1.75
1.80
0.04 0.09 0.13 0.17 0.22 0.26 0.30
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
PHOTO10 PHOTO11 PHOTO12 
1.60
1.63
1.74
1.70
1.60
1.56
1.43
1.40
1.50
1.60
1.70
1.80
0.04 0.07 0.11 0.15 0.18 0.22 0.26
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
1.63
1.74
1.77 1.76
1.70
1.66
1.51
1.43
1.40
1.50
1.60
1.70
1.80
0.03 0.07 0.10 0.14 0.17 0.21 0.24 0.28
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
1.60
1.76 1.76
1.70 1.68
1.53
1.45
1.311.30
1.40
1.50
1.60
1.70
1.80
0.04 0.08 0.12 0.16 0.20 0.24 0.28 0.32
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
PHOTO14   
1.52
1.65
1.70
1.72
1.66 1.65
1.50
1.55
1.60
1.65
1.70
1.75
0.04 0.08 0.11 0.15 0.19 0.23
density
lo
g 
of
 th
e 
ge
om
et
ri
c 
m
ea
n
 
  
圖五：不同照片的測試圖片的美感分數與 Background-based Text Density (DBT) 的關係圖 
 
以上實驗證實密度與美感之間是確實存在著相當的相關性，而 Background-based Text Density 
(DBT，即文字面積占背景區塊面積) 的密度值約 0.12 時，人的美感分數最高。基於視覺平衡與
視覺密度 Background-based Text Density (DBT)與美感的關係式，本計畫也開發了一個基於「粒
子群最佳化演算法」（PSO）來計算”最美”文字擺放位置的最佳化引擎，針對圖片越大計像量越
大而導致計算時間變長的問題提出解決雲端分散式運算平台，藉由平行化處理加速運算處理時
間提高整體系統效率，從而建立出一套智慧型的圖文合成系統，系統架構如圖六所示。 
 
 12 
計畫成果自評 
本計畫的研究進展順利，主要成果在於以實驗找出視覺密度與人類對圖文合成作品
(text-overlaid images)美感的定量關係，並應用於圖文自動合成系統的實作。針對圖文合成作品，
目前我們已經掌握了 density 與 balance 對人類美感的影響，也依據實驗設計出有關 density 及
balance 的美學計算公式，藉此，希望在大量的電子排版的需求之下，日後設計出完善的具美學
的智慧型圖文排版系統。整體而言計畫的成果符合計畫書內容，相關論文目前也正撰寫當中。 
 
REFERENCE 
1. Chien-Yen Lai, Pai-Hsun Chen, Sheng-Wen Shih, Yili Liu, Jen-Shin Hong: Computational 
models and experimental investigations of effects of balance and symmetry on the aesthetics 
of text-overlaid images. International Jounral on Human Computer Studies: 68(1-2): 41-56, 
2010 
2. David Chek Ling Ngo, Lian Seng Teo and John G. Byrne,“Modelling Interface Aesthetics”, 
Information Sciences152, pp. 25-46, 2003 
 
 14 
7 
 
 
  
8 
 
   
 
 
 PHOTO11 PHOTO12 PHOTO14  
1 
   
 
2 
   
 
3 
   
 
4 
   
 
國科會補助出席國際會議報告 
                                           99  年   11   月   25   日 
報告人姓名 洪政欣 服務機關名稱及職稱 國立暨南國際大學資訊工程系教授 
會議期間及地點 
自 2010年 11月 8日 
至 2010年 11月 12日 
紐西蘭皇后鎮 
國科會核定 
補助計畫編號 
   
 NSC 99-2221-E-260-028  號 
會議名稱 
（ 中文 ）第 10 屆亞洲電腦視覺研討會 
（ 英文 ）The 10th Asia Conference on Computer Vision (ACCV2010)  
發表論文題目 
（ 中文 ）帶景人像攝影的頭部突出物自動偵測 
（ 英文 ）Automatically detecting protruding objects when shooting 
environmental portraits 
報告內容應包括下列各項： 
一、參加會議經過 
二、與會心得 
三、攜回資料名稱及內容 
四、建議 
五：致謝 
六、附件：會議發表論文全文 
 
一、參加會議經過 
 
Asia Conference on Computer Vision (ACCV 2010)每年召開一次，是亞洲電腦視覺
領域的盛會。ACCV 2010於 2010年 11月 8 -12日，由紐西蘭 University of Otago
及 University of Auckland承辦，在 Queenstown Rydges Lakeland Resort召開。 
 
  
 
圖：與會學者於 Reception中相互交流 
 
    二、與會心得 
 
首先，感謝國科會預算支助教師出國參加國際研討會，才能有機會參與這次
ACCV會議。由於目前台灣資訊界進行計算美學研究的學者極少，本人研究團隊
在國內可以切磋的機會較為缺乏，本人特別選擇這次參加 ACCV 2010 研討會的
Workshop on Computational Photography and Aesthetics，期待更多了解到電腦視覺
及影像處理領域在計算美學應用的新發展趨勢。 
 
Workshop on Computational Photography and Aesthetics Keynote由 Dr. Alfred M. 
Bruckstein (Technion, Israel and NTU, Singapore) 進行演講，主要是在討論
non-photorealistic, engraving-like renderings等技術應用在模擬畫家作畫風格的一
些藝術效果，現有發展主要是在先擷取出影像中的 contour, edges等資訊，再自
然又”藝術”的結合應用這些 features，以便模擬藝術大師的作畫風格，主題相
當有趣，後續發展可期。 
 
 圖：本文作者（右一）與會國內外學者的晚宴餐敘 
 
本 人 發 表 論 文 Automatically detecting protruding objects when shooting 
environmental portraits”主旨在開發帶景人像攝影自動偵測頭部突出物的影像處
理技術。主要利用了Watershed Segmentation, KLT Feature Tracker等技術來進行
前景物件的辨識(foreground region detection)，再結合 Face Detection技術及一些
heuristic algorithm來估測被攝影者是否接近了背景中線條過度明顯區域。目前的
初步成果達到 Detection rate: 88% 及 False detection rate: 12%。此技術經過精進
後當可以實務應用在數位照相機中。本次會議中本人得以與計算攝影及計算美學
的學者有交流的機會，也接觸到國外整合設計藝術與資訊技術研究的跨領域成
果，對本人的未來相關研究有相當的激勵作用。 
 
四、建議 
本次會議中本人得以與影像計算美學的學者有交流的機會，由本次研討會看起
來，電腦視覺領域在 computational photography及 computational aesthetics應用方
面投入的研究團隊還是相對少數，在目前各界逐漸期待台灣「美學新經濟」時代
來臨的同時，本人謹建議國科會考慮對計算美學領域的更多資源投入。 
 
五、誌謝 
    本次經費支出係由國科會專題計劃(NSC NSC 99-2221-E-260-028號)出國經
費補補助，謹此致謝。 
 
六、附件：會議發表論文全文 
 
Aiming to develop advanced intelligent digital cameras, there have been 
commercial interests these years to develop digital still camera with ‘‘composition 
advising” functions (e.g., [2][3]). A number of research studies have also devoted 
towards this goal. For example, [14] developed an intelligent system that positions the 
features of interest in an automatic robot camera using the rule of thirds. [1] 
developed computational models of photographic aesthetics and a system that aids the 
user to select the optimal composition of a given scene. [13] developed computational 
models for visual balance/symmetry for photos overlaid with texts. Overall speaking, 
incorporating composition advising functions in a digital camera often require 
sophisticated visual object recognition and aesthetic computing techniques.  
Beyond those widely-applied photo composition rules, there are also certain 
common photographic “mistakes” that may degrade the aesthetic appeal of photos, 
including tilted horizon, unintentional dissection lines, unintentional amputation, 
protruding objects from a subject’s head, unwanted distracting objects in a scene, etc. 
Avoiding these mistakes is particularly critical when taking environmental portraits, 
which often focus both on the main subject and on their surroundings backgrounds 
that provide more character to the subject. A protruding object in an environmental 
portrait usually refers to objects such as trees, street lights, windows frames or 
steeples which protrude abruptly from the head regions. Examples of environmental 
portraits with unintentional protruding objects are shown in Fig. 1. Certainly, it is 
preferred if the camera can automatically cut down the harshness of the protruding 
objects by using a smaller depth of field to lessen its impact, or simply provide 
warning messages to advice the photographer to eliminate the object by moving 
around the camera. Such a tool, if properly incorporated in an intelligent camera, can 
potentially become an indispensable tool for amateur photographers. 
 
Fig. 1. Examples of an unintentional protruding object in an environmental portrait.  
In line with [4], aiming to develop intelligent composition-advising functions to 
avoid common photography compositional mistakes, this study aims to develop 
algorithm for the detection of protruding objects in environmental portraits. As an 
initial endeavor for this purpose, this study focuses on applications where the camera 
is mounted a tripod and the vibration of human hand is not concerned. The rest of this 
paper is organized as follows. Section 2 describes algorithms for automatically 
detecting protruding objects. Section 3 describes experimental results conducted to 
evaluate the performance of the proposed algorithms. Conclusions are given in 
Section 4. 
 
(a) 
 
(b) 
3) Protruding object classification module: for classifying protruding objects based 
on the edge numbers and color clusters of the background regions inside the ROI. 
The flowchart of the algorithm is shown in Fig. 2 and will be elaborated in the 
following subsections. 
2.1   Foreground Region Extraction  
Overall, an intra-frame and an inter-frame foreground region detection processes are 
developed and integrated for reliably detecting foreground regions. Assuming a 
stationary camera setting, the Watershed segmentation algorithm [6][7] and KLT 
(Kanade-Lucas-Tomasi) feature tracking algorithm [8][9] can be applied to detect 
moving image blocks and thereby determine the background regions and the intra-
frame foreground regions. In addition, an adaptive background subtraction method 
[10] is incorporated to further improve the detection rate. The background subtraction 
method can automatically develop a self-update reference background model to 
determine the inter-frame foreground regions. Since the background subtraction uses 
more than one image frames to determine the foreground regions, it is referred to as 
the “inter-frame” foreground detection process. Details of these detection procedures 
are elaborated in the following. 
 
2.1.1 Intra-frame Foreground Region Detection 
 
The intra-frame foreground detection process consists of the following three 
procedures. 
 
1) Watershed algorithm is first applied to segment images. A pre-processing 
Gaussian filter is used to smooth images in order to avoid over-segmentation 
caused by watershed algorithm.  In addition, a mathematical morphology filter is 
applied for post-processing segmented images to deal with cluttered scenes. The 
segmented regions with the same watershed label are drawn in the same color as 
shown in Fig. 3(a).  
2) KLT feature tracking algorithm is applied to detect favorable feature points at 
corners or edges of objects in the image. Motions of features in an image stream are 
calculated based on these feature points. We extract pixels with 5×5 masks 
centering the feature points with the same watershed label as the candidate 
foreground regions. Examples of KLT features and the candidate foreground 
regions are shown in Fig. 3(b) and 3(c) respectively. 
3) The foreground colors are modeled using a Gaussian Mixture Model (GMM) 
with five Gaussian components. The mean and the standard deviation parameters 
]     [ igb
i
gg
i
gr
i
gb
i
gg
i
gr σσσµµµ  
of the foreground colors in Gaussian component i for 
each R, G and B channel are estimated. The foreground color model is constantly 
updated in real time by using simple recursive updates [10]. In practice, since 
computing the GMM probability of every pixel in the image is rather time 
consuming, a simplified method is adopted in this work to speed the foreground 
detection process. In the simplified method, the foreground color model is used to 
segment the intra-frame foreground regions based on the following criterion. 
   
   
Fig. 4. First row: original images. Second row: foreground detection results. 
 
2.2 Estimating the Region of Interest (ROI) 
 
In principle, the ROI for detecting objects protruding across the head region of the 
subject can be approximated based on the subject’s face region. Viola-Jones face 
detector [11] is applied in this study to estimate the face region. The output of the face 
detector is a list of rectangular regions circumscribing the detected potential face 
regions. The ROI for protruding objects is specified as a rectangular region slightly 
larger than the face region (as shown in Fig. 5a). As such, potential protruding objects 
adjacent to both top and side regions around the head are properly attended to. An 
example of the face detection result and the corresponding ROI are shown in Fig. 5.  
 
(a) (b) (c) 
Fig. 5. (a) ROI template; (b) the face detection result is circumscribed by a green rectangular; 
(c) the corresponding ROI for protruding object detection is circumscribed by a yellow 
rectangle. 
 
2.3 Estimating the Protruding Objects 
 
In general, a ROI of an image frame with protruding objects should have more edges 
and color clusters than ROIs of adjacent frames without any protruding objects. 
Therefore, after the foreground regions and the ROI are extracted, whether there is a 
protruding object is determined based on the color and edge information of the 
background regions inside the ROI. To get the background regions inside the ROI, the 
foreground region is first masked on the ROI. Further, mimicking the typical shape of 
a face, an elliptic mask which is centered on the detected face region is used to 
compensate possible fragile foregrounds computed.  
For obtaining the edge features, Canny edge detector [12] is applied to detect the 
edges inside the ROI. Define Epixs as the number of edge pixels in the current frame t, 
and Epre_pixs as the number of edge pixels in the previous frame (t-1). For each image 
frame, an adaptive base value of the number of edge pixels, denote by Ebase , is used 
3   System Evaluations and Analysis 
The proposed protruding object detection algorithm was tested on four video 
sequences shot with cameras mounted on a tripod. The background in each video 
changes slowly relative to the motions of the main subject in the scene. Section 3.1 
describes the testing data set. Evaluation experiments and effectiveness analysis are 
presented in Section 3.2 and Section 3.3 respectively. 
 
3.1 Data Set 
 
We recorded four video sequences in a variety of scenes as shown in Fig. 7. The 
videos are designed to evaluate the performance of the proposed algorithm in scenes 
with different backgrounds, lighting conditions, and large-size head ornaments. 
 
 
(a) 
 
(b) 
 
(c) 
 
(d) 
Fig. 7. Four different test scenes: (a) indoor; (b) outdoor with a pure background; (c) outdoor 
with a cluttered background; (d) subject with afro hair. 
 
3.2 Evaluations 
 
The performance of the proposed protruding object detection algorithm was evaluated 
by comparing the system outputs with the ground-truth data using the four 
performance measurements listed in Table 1.  
Table 1. Four performance measurements applied in the system evaluation experiments. 
 
Image Frames with 
Protruding objects 
Image Frames w/o 
Protruding objects 
Detected True Positive (TP) False Positive (FP) 
Non-detected False Negative (FN) True Negative (TN) 
 
The detection rate (DR) and the false detection rate (FDR) are calculated 
respectively based on the following formulas: 
 
a protruding object classifier. Experimental evaluations show that the detection rate 
and false detection rate of protruding objects in the test videos are around 88% and 
12%, respectively. In average, the computation time costs about 0.16 second per 
image frame. We believe the proposed algorithm is sufficiently computational 
efficient once embedded in digital cameras. Ongoing works are currently underway to 
improve the current techniques with further concerns on the vibrations of hand-held 
cameras. 
 
Acknowledgements. This study is supported by the National Science Council of 
Taiwan, Republic of China, under Grant No. NSC 99-2410-H-260-052 and NSC 99-
2221-E-260-028. 
References 
1. Liu L., Chen R., Wolf L., Cohen D.: Optimizing Photo Composition. Computer Graphics 
Forum 29-2, pp. 469--478 (2010) 
2. Miyake, T., Soga, T.: Digital Still Camera with Composition Advising Function, and 
Method of Controlling Operation of Same. Fujifilm Corporation, United States Patent, 
Patent Number: 7317458 (2008)  
3. Suarez, L.A.F.: Picture Composition Guidance System. Sony Corporation, Sony 
Electronics Inc., United States Patent, Patent Number: 5873007 (1999)  
4. Shen, C.T., Liu, J.C., Shih, S.W., Hong, J. S.: Towards Intelligent Photo Composition-
Automatic Detection of Unintentional Dissection Lines in Environmental Portrait Photos. 
Expert Systems with Applications 36, pp. 9024--9030 (2009) 
5. Cavalcanti, C., Gomes, H., Meireles, R., Guerra, W.: Towards Automating Photographic 
Composition of People. In: IASTED International Conference on Visualization, Imaging, 
and Image Processing, pp. 25--30 (2006) 
6. Vincent, L., Soille, P.: Watersheds in Digital Spaces: An Efficient Algorithm Based on 
Immersion Simulations. IEEE Transactions on Pattern Analysis and Machine Intelligence 
13, pp. 583--598 (1991) 
7. Meyer, F.: Color Image Segmentation. In: International Conference on Image Processing 
and its Applications, pp. 303--306 (2002) 
8. Tomasi, C., Kanade, T.: Detection and Tracking of Point Features. Carnegie Mellon 
University, Technical Report CMU-CS-91-132 (1991) 
9. Shi, J., Tomasi, C.: Good Features to Track. In: IEEE International Conference on 
Computer Vision and Pattern Recognition, pp. 593--600 (1994) 
10. McKenna, S.J., Jabri, S., Duric, Z., Wechsler, H., Rosenfeld, A.: Tracking Groups of 
People. Computer Vision and Image Understanding 80, pp. 42--56 (2000) 
11. Viola, P., Jones, M.: Rapid Objects Detection using a Boosted Cascade of Simple 
Features. In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 511--
518 (2001).  
12. Canny, F.J.: A Computational Approach to Edge Detection. IEEE Transaction on Pattern 
Analysis and Machine Intelligence 8, pp. 679--698 (1986) 
13. Lai C.Y., Chen P.H., Shih S.W., Liu Y., Hong J.S.: Computational Models and 
Experimental Investigations of Effects of Balance and Symmetry on the Aesthetics of 
Text-Overlaid Images. International Journal of Human Computer Studies 68(1-2), pp. 41-
-56 (2010) 
14. Byers, Z., Dixon, M., Smart, W.D., Grimm, C.: Say Cheese! Experiences with a Robot 
Photographer. AI Magazine 25-3, pp. 37--46 (2004) 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：洪政欣 計畫編號：99-2221-E-260-028- 
計畫名稱：應用於自動圖文合成系統的圖像計算美學初探 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
