 1 
   
行政院國家科學委員會補助專題研究計畫
■ 成 果 報 告   
□期中進度報告 
 
基於尺寸不變性之穩健式三維頭部追蹤 
Robust 3D Tracking of Human Head Based on Invariance of Size 
 
 
計畫類別：■ 個別型計畫  □ 整合型計畫 
計畫編號：NSC  98－2221－E－009－124－MY2 
執行期間： 2009 年 8 月 1 日至 2011 年 10 月 31 日 
 
計畫主持人：莊仁輝 
共同主持人： 
計畫參與人員：羅國華、郭育瑋、余東諺、陳芝穎、盧沛怡、陳怡廷、
陳暐晴、馬國濂、周致傑、姚柏安、王之容、廖偉成、黃星陸 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
 
 
 
 
執行單位： 
中    華    民    國   99  年   8  月     30  日 
 3 
sets of head candidates, and are regarded as feature points in subsequent people localization. 
The proposed localization scheme assumes that the height of a feature point is known a priori 
so that the point location in the plane of the known height can be found by homographic 
transformation and 3D geometry which only require camera locations. Experimental results 
show that the proposed head detection is performed satisfactorily even for crowded scene. 
However, the localization results are not always accurate enough due to errors in the 
localization process. The errors may arise from radial distortion of cameras, inaccuracy 
associated with manual measurement, and noises in the imaging process. In the second year, 
we analyze these errors and use the analytic results to fuse multi-camera information so as to 
reduce the localization error and improve system stability. 
 
簡介 
本計畫的主要目的是以高角度的攝影機對場景進行監控，藉由對人物頭部偵測找出人物
的頭部中心點後，再以投影幾何的平面轉換對頭部中心進行持續定位。當環境中具有多
攝影機時，系統可透過選擇性與整合性的多攝影機資訊融合，來整合多攝影機的定位資
訊，以達成準確的定位。此兩年計畫的研究成果，共包含下列三大主題： 
一、 穩健式之人頭偵測研究: 
 目標是以多種特徵進行人物頭部的偵測，包含髮膚色資訊、前景資訊，以及特徵間
的幾何關係進行穩健的頭部偵測，以供後續進行定位與誤差分析。 
二、基於影像特徵點定位之人員定位: 
 以頭部中心點，以單一攝影機並利用投影幾何的平面轉換對人物進行定位，並持續
產生俯視圖定位結果。 
三、基於影像特徵點定位之人員定位之誤差分析: 
 針對單一攝影機所取得的目標點，進行誤差分析以及穩定程度的估計，並利用估計
的結果，整合多攝影機的資訊，提升定位的準確性與穩定度。 
在第一年的研究計畫中，我們發展一套頭部偵測方法，可穩定地偵測出影像中人物
的頭部區域。並且，我們也發展了一套不需要複雜的校正之影像特徵點定位方法，可對
人物頭部中心點進行定位。在頭部偵測方面，我們利用兩種主要方法偵測人物頭部，分
別為基於髮膚色特徵和前景物輪廓區域最高點之偵測演算法。首先我們利用高斯混合模
型分割出人員所在的前景區塊，再藉由偵測各前景區塊中的髮膚色區域，過濾出候選頭
部區域。另一方面我們找出前景物輪廓的區域最高點後，由此點附近找出候選頭部區
域。最後藉由整合髮膚色特徵和區域最高點所找出之候選頭部區域，即可正確地偵測出
場景中的人物頭部中心點位置，並可被作為一影像特徵點進行後續人物定位。在影像特
徵點定位的方面，我們可以藉由單一攝影機，在已知特徵點高度以及攝影機位置的條件
下，透過投影幾何的平面轉換和三維幾何關係，計算出特徵點在其高度平面上之位置。
實驗的結果顯示頭部偵測的結果應用於多人的場景中，仍有良好的正確性，而特徵點定
位也能落在一定的誤差範圍之內。 
其後第二年的研究我們分析特徵點定位過程中產生的誤差，例如徑向畸變誤差、人
為測量誤差和取像誤差，透過影像校正與空間測量點自動修正，以減少後續定位的誤
差。後續則依據特徵點位於影像中不同的位置，動態地估計出定位可能產生的動態誤
差，再利用這樣的資訊整合多攝影機資訊，使得系統的定位結果能夠穩定、準確。 
 5 
域中的各個不相連的獨立區域。其結果如圖 1-2 中的 (a)與(b)所示，我們以不同的顏色
顯示這些區域。我們可以看到圖中仍有很多非頭部的區域，因此我們將以過濾的方式移
除這些區域。 
 
       
               (a)                                      (b) 
圖 1-2 元件連通標記法結果，(a) 圖 1-1(b)執行後之結果，(b) 圖 1-1(c)執行後之結果。 
 
1.3.2 以過濾方式篩選出髮膚色的頭部候選區域 
接著我們利用頭部的其他特徵，包含: (1)頭部大小有一定之範圍，(2)頭部形狀近似
球形，(3)頭部位於身體最高處，故其下方具有足量之前景區域。針對上述之各項特徵，
我們提出了以下幾種過濾方式，針對特徵(1)採用面積大小過濾，針對特徵(2)採用長寬
比過濾，針對特徵(3)檢查下方需具足量前景區域，將依序介紹各種過濾的方式如下。 
方法 A. 面積大小 
針對頭部面積所給予的限制，我們設定了兩個門檻值，來排除掉面積過大或過小的
髮膚色區域，透過這樣的方式，影子與黑色衣服等雜訊區域將會被移除。 
方法 B. 長寬比 
雖然理論上頭部的區域會接近球形，但經過觀察發現我們所偵測出的髮膚色區域
中，頭部的區域並非呈現正圓形，因此我們保留長寬比例在一定範圍內的髮膚色區域。
我們使用式 1.1 來計算候選區域的長寬比： 
 
R
*
 = H / W                           (1.1) 
 
如果 R*值越大，代表此候選區域越細長。本系統中，只要長寬符合 0.8≦R*≦2，我們
便保留此候選區域。 
方法 C. 下方具足量前景區域 
通過上述兩種過濾方式後，我們將更進一步判斷這些區域下方是否有足量之前景區
域，藉此判斷其是否為頭部位置所在。我們採用的方法是以區域之質心作為基準點，以
垂直方向向下搜尋前景區域的輪廓邊緣，如在一距離內找尋不到輪廓邊緣，代表此區域
下方有足量前景區域。如圖 1-3 所示，紫色框線為髮色區域示意圖，我們由紫色框線之
質心為基準點，並以黃色箭頭的方向向下搜尋輪廓邊緣，如果搜尋到輪廓邊緣則表示下
方前景區域不足，將會被視為非頭部區域而予以刪除。 
 7 
 LMFH LMFH 
yttt TFHyPyP    and  0)()(1                     (1.2) 
 
otherwiseFH
yPyPifFHyPyPFH
t
tttttt
,1
1,11
0
0)()()()(




            (1.3) 
 
其中 Pt(y)為第 t 個輪廓點的 y 座標值、FHt 為累計高度值(foreground height)而 Ty為系統
設定的門檻值，當式子 1.2 成立時，Pt(x,y)即為區域最高點，系統設定 Ty=2，先將輪廓
上所有區域最高點(起伏點)找出，再利用後續演算法排除非頭部位置的區域最高點。 
 
圖 1-5 座標原點示意圖。 
 
找出區域最高點後，我們可以利用一些簡單的判斷排除掉明顯不是頭部位置的區域
最高點，如圖 1-6 所示，紅色圓圈所在位置皆符合式 1.2 區域最高點之定義，但是明顯
不是頭部位置。我們根據先前假設：頭部為人體之頂端，若區域最高點上方為前景，則
可能是輪廓中人員之腋下、胯下(圖 1-6 紅色圓圈)或是前景切割結果破碎處，故只保留
區域最高點上方為非前景區域者。此外，利用顏色資訊對保留下來的區域最高點再次進
過濾，我們偵測此區域最高點下方 3×3 區塊之顏色範圍，判斷其是否落在髮色與膚色之
範圍，若不符合則進行移除。 
 
 
 
 
 
 
 
 
圖 1-6 LMFH 之偵測以及需刪除之位置示意圖。 
 
經由實驗發現，同一輪廓由正向及反向所搜尋到的區域最高點可能會略有差異，以
圖 1-7 為例，假設灰色部分為某前景區域，而綠點為輪廓點，若我們由左到右執行 LMFH
演算法，我們會發現找到的累計高度值(RFH)都很小以至於無法偵測出區域最高點。反
之，若我們由右到左執行 LMFH 演算法則找到的累計高度值(LFH)明顯會超過 Ty值，這
也是為什麼同一個輪廓要執行兩次 LMFH 演算法的原因。因此，不同方向執行 LMFH
演算法都會產生各自之結果，所以必須檢查兩個不同方向的結果，而檢查的方式如式子
1.4 所示，同一點必須在兩個不同方向都被判定成區域最高點才選出，而像是圖 1-7 中
只有單方向被判定成區域最高點的輪廓點將會被濾除。 
 
RFH > Ty  and  LFH > Ty                                 (1.4) 
 
 9 
  
                              (a)                              (b) 
圖 1-9 頭部候選區域偵測之結果(整合前)。 
 
 
圖 1-10 頭部候選區域示意圖。      
 
 
圖 1-11 整合重疊的頭部候選區域示意圖。 
 
  
(a)                                     (b) 
圖 1-12 頭部候選區域偵測之結果(整合後)。 
 
1.4 研究結果與討論  
在實驗的部分，我們以三個不同的室外場景進行詴驗。圖 1-13 為執行頭部偵測之結
果，其中黃色圈為偵測到的頭部位置，而在每張影像的左上角則統計出該張影像的人(頭)
數，以圖 1-13 為例，其中圖 1-13(a)、(b) 、(c) 、(e) 、(f)中頭部的偵測都相當正確，
僅有圖 1-13(d)、(f)發生了偵測失誤。紅色箭頭所指為未偵測到之人員，由於這些人員所
在位置皆於輪廓內部且頭部顏色特徵不明顯，因此無法有效的偵測出頭部區域。而圖
1-13(d)中藍色箭頭所指人員為因為手持雨傘而導致偵測失誤，雨傘內部有兩位人員，但
是經由頭部偵測演算法只偵測到一位。 
 11 
主題二: 基於影像特徵點定位之人員定位 
2.1 研究目的 
在電腦視覺領域中，以攝影機為基礎做物體追蹤與定位一直是個典型且重要的研究
議題。此研究議題之所以典型在於物體追蹤與定位技術，在簡單的環境設定中，已有許
多效能良好的技術被提出，例如靜態而簡單的背景、單一追蹤目標物、使用已校正的攝
影機等。而此研究議題之所以重要在於一旦攝影機拍攝的是真實且複雜的場景，則諸多
的變因，像是光線變化、人群遮蔽、場景變動快速等，將使得物體追蹤與定位問題變得
複雜而難解。因此近年來視覺監控追蹤與定位的研究發展，多朝向將原本適用於簡單場
景的技術，逐步擴展應用到真實而複雜環境中。 
在追蹤的過程中常常會需要利用位置資訊，所以準確的定位可以幫助追蹤。因此我
們實作一套基於電腦視覺的定位系統，該系統的運用到平面投影轉換關係和三維幾何關
係，達成對影像中的目標物進行定位。由於我們的定位系統是以點做為定位目標
(point-based)，因此只要存在目標點即可進行定位。至於目標點的選擇可以是立足點、
頭部中心點等。 
 
2.2 文獻探討 
 在[1]中，Mittal 和 Davis 提出以立體視覺(stereo vision)為基礎的多攝影機追蹤模式，
用來追蹤人群，透過機率模型決定影像中的每個像素分別是屬於哪一個被追蹤人員；在
[2]的方法中，則透過立體視覺資訊，進一步作空間中人物的定位與追蹤；對於[1, 2]的
方法，由於需要攝影機之間事先進行校正，故在實際應用上較為不便。在[3, 4]的追蹤研
究中，則提出以「頂視監控」的概念來進行人群追蹤；類似的頂視追蹤概念在[5, 6]中亦
有探討，構想是基於影像與頂視面的 homographic transformation，在頂視面中估計人物
腳點位置並進行多人追蹤的方法；在[7]中，則整合了攝影機資訊與地板感測器，作人員
的定位與追蹤。不同於上述[3-7]的追蹤與定位研究，在此計畫中，我們非僅著眼於透過
homographic transformation 轉換，發展頂視平面的定位追蹤。 
 
2.3 研究方法 
我們進行影像特徵點定位時需要幾個已知條件，第一是知道真實空間中攝影機的中
心位置資訊，第二是知道真實空間中目標點的高度資訊，第三則是預先計算好的單應性
轉換矩陣(homographic matrix)用來將影像上的座標轉換到參考平面上。由於在估算真實
空間目標點位置的過程中，我們會利用攝影機的中心位置和目標點的高度，因此在實作
的方面，攝影機的中心位置與單應性轉換矩陣可藉由事先測量與計算得到；目標物的高
度資訊，則可假設場景中有一道閘門，閘門上有高度資訊的刻度，當目標點進入場景時
會先通過這道閘門，因此我們事先得到此目標點的高度資訊，在此我們用頭部中心點當
作目標點。 
有了這些資訊後，我們即可估算出真實空間中目標點的位置。如圖 2-1 所示，首先
我們在影像平面上取出目標點 PIP，再藉由單應性轉換矩陣 H，將影像目標點投影到參
考平面 π1 上，即得到投影點 PRP，轉換關係如式子 2.1，其中 PIP=[u, v, 1]
T，[x, y]T則為
真實空間中參考平面上的 X 座標和 Y 座標。接著算出參考平面投影點和攝影機中心點的
三維連線 L 和 π2 平面之方程式，其中 π2 平面與 π1 平面平行且平面高度等於目標點的高
度。最後再計算出三維連線 L 與 π2 的交點 PHP，此交點即為估算的定位結果。 
 
 13 
 
圖 2-3 實驗場景之定位結果。 
 
由於攝影機和目標點皆靜止不動，並且我們設定目標點所在高度平面與參考平面相
同，但是卻存在明顯的定位誤差，其誤差來源有徑向畸變誤差(radial distortion)、人為測
量誤差和取像誤差，這些誤差將會導致定位的準確度下降。因此我們未來將對定位過程
中可能的產生的誤差加以分析與改善，期望減少誤差對於定位的影響。 
 
2.5 參考文獻 
[1] A. Mittal and L. S. Davis, “M2Tracker: a multi-view approach to segmenting and 
tracking people in a cluttered scene,” International Journal of Computer Vision, vol. 51, 
no. 3, pp.189-203, 2003. 
[2] S. Bahadori, L. Iocchi, G. R. Leone, D. Nardi, and L. Scozzafava, “Real-time people 
localization and tracking through fixed stereo vision,” Applied Intelligence, vol. 26, pp. 
83-97, 2007. 
[3] T. Darrell, D. Demirdjian, N. Checka, and P. Felzenszwalb, “Plan-view trajectory 
estimation with dense stereo background models,” IEEE International Conference 
Computer Vision, pp. 628-635, 2001. 
[4] S. V. Martnez, J. F. Knebel, and J. P. Thiran, “Multi-object tracking using the particle 
filter algorithm on the top-view plan,” European Signal Processing Conference, 2004. 
[5] S. M. Khan and M. Shah, “A multiview approach to tracking people in crowded scenes 
using a planar homography constraint,” European Conference Computer Vision, pp. 
133-146, 2006. 
[6] R. Miezianko and D. Pokrajac, “Localization of detected objects in multi-camera 
network,” 15th IEEE International Conference Image Processing, pp. 2376-2379, 2008. 
[7] C. R. Yu, C. L. Wu, C. H. Lu, and L. C. Fu, “Human location via multi-cameras and 
floor sensors in smart home,” IEEE International Conference Systems, Man and 
Cybernetics, pp. 3822-3827, 2006. 
 
2.6 計畫成果自評 
 由實驗結果可以看出來我們所提出的定位方法確實可行，且只需經由簡單的事先校
正就可以達成即時的定位。然而定位的結果仍有一些定位誤差。目前我們已著手分析這
些誤差對於定位的影響。未來，我們希望能運用誤差分析的結果，更進一步的提升系統
的定位穩定性與準確性。 
 15 
 
圖 3-1 受到徑向畸變影響的影像。 
 
3.3.2 靜態誤差 
由於徑向畸變誤差和人為測量誤差定位誤差是不會隨著 Frame 不同而發生改變，因
此我們將其定義為靜態誤差。此兩種將可透過影像作校正與微調參考點的測量位置，有
效地降低後續對於定位造成的誤差。 
 
3.3.2.1 徑向畸變誤差 
受到鏡頭的影響使得影像發生徑向畸變，如圖 3-1 所示。我們可以發現離影像中心
點越遠的直線段變成了曲線，這樣將會使得後續使用單應性轉換發生嚴重的誤差。為
此，我們將透過參考式(3-1)對影像進行校正，希望使得彎曲的線段變成直線，如圖 3-2
所示。 
    P′u = Pu(1-au||P||
2
) 
P′v = Pv(1-av||P||
2
)       (3-1) 
其中 P = (Pu,Pv)為正規化(normalization)後的影像中心點座標，||·||為向量的長度；au 和 av
為校正參數，其數值皆小於 0。參數 au 為調整影像 U 座標的比值，參數 av為調整影像 V
座標的比值。更詳細的校正影像的步驟為取得校正特徵點的影像座標，該座標的起始點
為影像的中心，並進行正規化，如圖 3-3 (a)與(b)所示。則式子 3-1 會依據正規化後的影
像座標與影像中心點的距離作調整，當距離較大時，往外推的幅度較大，當距離較小時，
往外推的幅度較小。舉例來說，假設目前要調整藍色點和紅色點的影像 V 座標，如圖
3-4 所示，由於藍色點與影像中心的距離較小，因此調整的距離較小；反觀紅色點與影
像中心的距離較大，因此調整的距離也較大。 
 
圖 3-2 徑向畸變校正示意圖。 
 17 
  
(a)                  (b) 
圖 3-5 影像校正。(a) 校正前，(b) 校正後。 
 
 
圖 3-6 實驗場景校正後之定位結果。 
 
3.3.2.2 人為測量誤差 
以手動測量參考點於真實空間中的位置時，勢必存在測量誤差，因此我們希望透過
微調真實空間中參考點的測量位置，以推估參考點的正確空間位置，如圖 3-7，紅色米
字代表測得的 5 個參考點位置。我們微調的方式為，針對一個參考點而言，對其作上、
右上、右、右下、下、左下、左、左上移動固定的距離，再加上一個不移動則共有 9 種
可能。後續再選取一些驗證點，以驗證各種組合估算出來的定位誤差。實驗場景有 5 個
參考點，總共具有 59049 (95)種可能的組合。圖 3-8 顯示圖 3-7 中參考點微調後的位置，
而圖 3-9 顯示影像校正並微調參考點位置後的定位結果。我們可以看出校正加上微調參
考點位置後，定位誤差已由 17.36 公分降到 4.26 公分，下降幅度達 75%。到目前為止，
經由校正影像和微調參考點後已大幅改善定位結果。接下來，我們欲討論取像時受雜訊
干擾所造成的動態誤差。 
 19 
 
 
圖 3-9 實驗場景校正和微調參考點後之定位結果。 
 
3.3.3 動態誤差 
由於攝影機在取得影像的過程中，常常會受到光影變化等雜訊干擾，使得在影像上
取得目標點位置不準確，所產生的誤差我們稱之為動態誤差。為了瞭解動態誤差所造成
的影響，我們以模擬的方式進行觀察。首先，我們以高斯雜訊產生 100 個受到干擾後的
影像目標點位置。圖 3-10(a)中為原始目標點所在的位置，而藍色點為模擬受高斯雜訊影
響的目標點位置。我們可以其分佈的情況大致上呈現圓形分佈；(b)為將(a)中的雜訊經由
單應性轉換矩陣作用後，投影到參考平面上的情形，即為受干擾後的定位分佈，可看得
出來其呈現橢圓形的分佈。由此可知，在影像上受到雜訊干擾的分佈，經過投影後會有
不同的結果，即不同的攝影機拍攝角度與不同的距離將會造成不同的分佈結果。由圖
3-10(b)我們還可以觀察到，誤差分佈在橢圓長軸的方向與短軸方向之大小亦不同，在橢
圓長軸的方向誤差分佈較大，反觀在橢圓短軸方向則誤差分佈較小，此代表著定位結果
在橢圓短軸的方向具有較高的穩定性，而在橢圓長軸的方向則穩定性較差，因此可藉由
此特性幫助我們評估定位結果的可靠度。然而為了獲得此定位分佈，每次都要對影像目
標點模擬多個高斯雜訊點，再經由單應性轉換矩陣將各個雜訊點投影到參考平面上，最
後才可知道定位誤差的分佈，此過程太耗時間。因此，若能較方便且快速地找出定位誤
差的分佈情形，才有機會能應用於實際的即時定位系統中。 
 
圖 3-10 (a) 原始目標點與模擬受高斯干擾的目標點，(b) 經轉換於參考平面上的目標點分布情形。 
 21 











ihg
fed
cba
H       (3-2) 
 
























































111
v
u
ihygx
feydx
cbyax
y
x
ihg
fed
cba
y
x
H   (3-3) 
將 u 和 v 代入邊界圓方程式中得到式(3-4)，再將式(3-4)整理成式(3-5)。 
((ax + by + c) / (gx + hy + i) - p)
2
 + ((dx + ey + f) / (gx + hy + i) - q)
2
 = r
2 
(3-4) 
Ax
2
 + Bxy + Cy
2
 + Dx + Ey + F = 0 
A = a
2
 - 2pag + p
2
g
2
 + d
2
 - 2qdg + q
2
g
2
 - r
2
g
2
 
C = b
2
 – 2pbh + p2h2 + e2 – 2qeh + q2h2 – r2h2             
F = c
2
 - 2pci + p
2
i
2
 + f
2
- 2qfi + q
2
i
2
 - r
2
i
2    
(3-5) 
B = 2ab – 2p(ah + bg) + 2p2gh + 2de – 2q(dh + eg) + 2q2gh – 2r2gh 
D = 2ac – 2p(ai + cg) + 2p2gi + 2df – 2q(di + fg) + 2q2gi – 2r2gi 
E = 2bc – 2p(bi + ch) + 2p2hi + 2ef – 2q(ei + hf) + 2q2hi – 2r2hi 
此(斜)橢圓又可以表示為式(3-6)，其中[j, k]為橢圓的中心，展開後為式(3-7)。比較式(3-5)
和(3-7)後，可得出 D = -2Aj –Bk 和 E = -Bj –2Ck。 
A(x - j)
2
 + B(x - j)(y - k) + C(y - k)
2
 + f = 0    (3-6) 
Ax
2
 + Bxy + Cy
2
 + (-2Aj –Bk)x + (-Bj – 2Ck)y + (Aj2 + Bjk + Ck2 + f) = 0  (3-7) 
因此，橢圓的中心點[j, k]可用式(3-8)算出；橢圓的旋轉角度可由 tan(2θ) = B/(A-C)算出；
令旋轉前的橢圓為[X, Y]T，旋轉後的橢圓為[x, y]T，則兩者的關係如(3-9)。 
                           



















E
D
CB
BA
k
j
2
2
      (3-8) 
           










 






Y
X
y
x


c o ss i n
s i nc o s
     (3-9) 
 
至於橢圓的長軸與短軸可由式(3-10)算出。 




 ba axisaxis ,  
α = Acosθ2 + Bsinθcosθ + Csinθ2 
β = Asinθ2 – Bsinθcosθ + Ccosθ2                (3-10) 
λ = Dcosθ + Esinθ 
 23 
3.3.5 暴力法估算定位誤差分佈 
對於此一對攝影機的定位方法，我們透過與上小節相似的模擬動態誤差的方式希望
能夠先了解誤差的分佈情形。首先，我們在影像目標點之邊界圓上放置 24 個模擬點，
如圖 3-14 所示，接著利用 3.3.4 中整合一對攝影機的定位方法，我們可以暴力法估算所
有 FP 共有 576(24 ×  24) 個，接著我們採取凸包(convex hull)演算法將所有的點圍繞起
來，形成一個四邊形的區域，此區域即可被視為一對攝影機之定位的 CR。當 CR 的面
積較小時，我們可以判斷該對攝影機將具有較準確的定位結果。因此，當環境中具有多
攝影機時，我們可以從每對攝影機的 CR 結果，挑選出具最小 CR 的一對攝影機，以提
供較準確的多攝影機定位結果。 
 
圖 3-14 整合多攝影機間的定位誤差分佈。 
 為了準確地估算 CR，邊界圓上放置的模擬點個數不宜太少。但是隨著模擬點數的
增加，暴力法估算誤差分佈的計算量也會呈現平方的速度增加。因此，下面我們提出一
個較為簡化的方法，利用之前找出來的橢圓參數，期望能更有效率地估算出誤差分佈。 
 
3.3.6 簡化法估算定位誤差分佈 
 由於暴力法估算 CR 所需之計算量很高，因此我們將利用在 3.3.3 小節中推導的橢
圓參數，快速地估算出一個大概的信心區域(approximate confidence region, ACR)以有效
地減少計算量。如圖 3-15 所示，我們可以由先前攝影機 A 求出的橢圓參數分別算出橢
圓短軸上的兩個端點 ACR和 ACL，再各自算出兩端點與 AHP的兩條直線。同理，利用攝
影機 B 求出的橢圓參數，我們可以找出另外兩條直線，則此四條直線的交點所圍成之四
邊形即為 ACR。由於攝影機架設都具有一定的高度，即與物體相距一定的距離，因此
 25 
接著我們要驗證 3.3.6 節中介紹的 ACR 是否能有效地藉由挑選減低定位的誤差。由
於場景中有 5 台攝影機，每次選兩台攝影機配對的定位結果，共有 10 種可能。從表 3.2
可看出來，Selection 的定位誤差仍然是最小的。另外，在表 3.2 中亦可發現 Pair 1&5 的
誤差很大，其原因為在所有的位置上，1 號攝影機和 5 號攝影機皆對看，這將會使得 3.3.4
節中介紹的整合定位結果較容易變得不穩定，因為兩條接近平行的直線，焦點的位置很
容易因雜訊的影響發生大量的偏移。 
 
(a) 
 
(b) 
圖 3-16 虛擬場景。(a)場景頂視圖，(b)場景影像。 
 
表 3.1 單攝影機的定位誤差。 
Camera Mean of errors Variance of errors 
Camera 1 2.991 1.429 
Camera 2 4.967 2.558 
Camera 3 2.989 1.352 
Camera 4 4.978 2.649 
Camera 5 2.984 1.295 
Selection 1.872 0.209 
 27 
    
(a)                                          (b) 
 
     
(c)                                          (d) 
 
 
(e) 
圖 3-18 單攝影機的定位結果。(a) 攝影機 1 的結果，(b) 攝影機 2 的結果，(c) 攝影機 3 的結果，(d) 攝
影機 4 的結果，(e) 挑選過後的結果。 
 29 
 
圖 3-20 單多攝影機一起挑選的結果。 
 
 接著第二個實驗我們以人物的頭部中心為目標點的定位結果，如圖 3-21 所示，(a)、
(c)、(e)和(g)分別為 4 隻攝影機所拍到的影像，(b)、(d)、(f)和(h)分別為偵測出來的前景
區域，其中紅色點代表人物的頭部中心點。定位結果以相似於前面圖 3-18、圖 3-19 與
圖 3-20 相似，呈現於圖 3-22、圖 3-23 和圖 3-24。雖然實驗結果較不明顯，但仍可看出，
由挑選過後的定位結果都會比沒有挑選的更靠近黑色矩形。 
    
                             (a)                      (b) 
    
                  (c)                      (d) 
    
                  (e)                      (f) 
    
                  (g)                      (h) 
圖 3-21 實際場景－人物。 
 31 
    
(a)                                   (b)  
    
(c)                                    (d) 
    
(e)                                     (f)  
 
(g) 
圖 3-23 多攝影機的定位結果。(a) 攝影機 1 和攝影機 2 的結果，(b) 攝影機 1 和攝影機 3 的結果，(c) 攝
影機 1 和攝影機 4 的結果，(d) 攝影機 2 和攝影機 3 的結果，(e) 攝影機 2 和攝影機 4 的結果，(f) 攝影機
3 和攝影機 4 的結果，(g) 挑選過後的結果。 
 33 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。 
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
 Kuo-Hua Lo, Jen-Hui Chuang, Yueh-Hsun Hsieh, and Hon-Yue Chou, "A Point-Based 
Localization with Error Analysis," The 2010 International Computer Symposium, Dec 16-18, 
2010, Taiwan. 
 Kuo-Hua Lo and Jen-Hui Chuang, “Vanishing Point-Based Line Sampling for Efficient 
Axis-Based People Localization,” IEEE International Conference on Image Processing, 
Belgium, Sep.11-14, 2011. 
 1 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：100年 10月 4日 
                                 
一、 參加會議經過 
此次 9月的會議，舉辦在比利時首都布魯塞爾的 SQUARE Meeting Center，會議時
間為 2011年 9月 11日至 14日。會議的內容非常豐富，包含了 Image & Video 
Representation、Multi-Resolution Methods for Image Processing、Image Coding、Hardware 
and GPU Issues in Video / High Dynamic Range Imaging and Shape Estimation 、CT Image 
Processing Methods、Tracking、Preprocessing and Features for Biometrics…等相關議題。 
第一天 Plenary talks由 Brian Wandell主講，主題為: SEEING AND THE BRAIN，內
容說明了過去的 25年，人類因為可以使用MRI(核磁共振)因此可以使用非侵入性的手段
計畫編號 NSC  98－2221－E－009－124－MY2 
計畫名稱 基於尺寸不變性之穩健式三維頭部追蹤 
出國人員
姓名 
羅國華 
服務機構
及職稱 
交通大學資工系博士生 
會議時間 
100年 9月 11日
至 
100年 9月 14日 
會議地點 
比利時 
會議名稱 
(中文)國際性電子技術與電子工程師協會 影像處理國際會議 
(英文) IEEE International conference on image processing 
發表論文
題目 
(中文) 基於消失點取樣之人物軸式定位 
(英文) VANISHING POINT-BASED LINE SAMPLING FOR 
EFFICIENT AXIS-BASED PEOPLE LOCALIZATION 
附件四 
 3 
的部分， 他的方法特別的具有結構，並且能夠提供(支撐)後續繪畫的參考。有時甚至一
幅畫中會出現兩種以上不同結構的 underdrawings。 
   
               (a)                            (b)                          (c) 
圖片說明: (a) 報告人攝於會場外。(b) 報告人與海報於會場內。(c) 報告人與其他研究學者討論的情形。 
二、 與會心得 
 參加本次會議除了提升台灣與本校於學術領域的能見度外，也讓我更了解到目
前各國的學者所投入的研究方向，例如: 中國大陸與台灣在視訊的追蹤與監控這塊領域
相當活耀，而 HDR方面則是西方的國家較為熱衷。此外也在會議的過程中認識了多位
中西方的研究學者，相信將能對於本人以及所屬的研究單位都能有所幫助。 
三、 建議: 
無 
四、 攜回資料名稱與內容 
隨身碟一支內含 ICIP2011 proceeding 一份 (ISBN: 978-1-4577-1302-6)  
五、 其他 
無 
> *** Importance and relevance: How you rate the importance and the relevance of this paper
sufficient interest (3)
> *** References: Quality of references included in the paper.
references adequate (3)
> *** Clarity of Presentation: How well the paper is written.
Reads very well (4)
> *** Technical correctness: How correct the paper is
Definitely correct (4)
> *** Detailed comments: Provide detailed comments that will be helpful to the Technical Programme
Committee for assessing the paper, as well as useful feedback to the authors.
Originality: 7/10
A new computationally efficient algorithm is presented.
Technical Content: 7/10
Good.
Clarity: 7/10
Good.
References: 7/10
Good.
Overall: 7/10
======= Review 3 =======
> *** Overall rating: Your overall rating of the paper.
Weakly accept (3)
> *** Novelty: How original the problem and/or solution method is.
Moderate original (3)
> *** Importance and relevance: How you rate the importance and the relevance of this paper
sufficient interest (3)
> *** References: Quality of references included in the paper.
references adequate (3)
> *** Clarity of Presentation: How well the paper is written.
Clear enough (3)
> *** Technical correctness: How correct the paper is
Probably correct, did not check completely (3)
> *** Detailed comments: Provide detailed comments that will be helpful to the Technical Programme
Committee for assessing the paper, as well as useful feedback to the authors.
In comparison of computing time with [7], the authors should provide more about the experimental data. As
Fig.6 shows, FPS varies dependent on the input video sequence, it is not clear that the author can use frame
rate of 5.365FPS for the comparison with that of [7].
Regards,
Pierre Moulin & R. Inald Lagendijk, TP Chairs
2011/4/24 Gmail - [IEEE ICIP2011] Your paper '…
https://mail.google.com/mail/?ui=2… 2/2
Fig. 1. Foreground pixels and the 2D MA estimated by PCA.
Fig. 2. Illustration of finding intersection points of two axes
on a reference plane.
2.1. Finding a 3D major axis of one person
As shown in Fig. 1, we assume the foreground region of a per-
son derived by background subtraction is given, so that we can
estimate its 2D MA via principal component analysis (PCA).
By further assuming that the correspondences between the 2D
MAs in two views are known, the 3D MA of the person can be
reconstructed. Take the green axes in Fig. 2 for example. The
2D MAs of the same person, denoted by L1 and L2 in View 1
and View 2, respectively, can be projected to a reference plane
pi by homographic matrices H1pi and H2pi . The intersection
of the two projections Lpi1 and L
pi
2 on pi can be denoted by P
pi
12.
By computing the intersection points of the projections of L1
and L2 on different reference planes of different heights, the
3D MA of the person can be formed. Specifically, a 3D MA
computed from View 1 and View 2 can be denoted by an axis
set A of intersection points, i.e., Ahbht12 = {Phb12 , . . . , Pht12 },
where hb and ht are the heights of bottom and top end points
of the 3D MA, respectively.
2.2. Finding 3D major axes for multiple persons
Now, we will extend the above method to estimate 3D MAs
for multiple persons. As shown in Fig. 2, a 3D MA obtained
from the ith 2D MA in View 1 and the jth 2D MA in View
2 can be denoted by Ahbht1i,2j = {Phb1i,2j , . . . , Pht1i,2j}.Given M
2D MAs in View 1 andN 2D MAs in View 2, since we do not
have correspondences of 2D MAs in the two views, the total
number of possible 3D MAs in reconstruction will be MN .
We then apply the following rules to filter out incorrect 3D
MAs.
1. The length ‖ht−hb‖ of a 3D MA is shorter than THlen.
2. The bottom height hb of a 3D MA is higher than THbot.
3. The foreground coverage of a 3D MA by projecting its
intersection points {Phb , . . . , Pht} back to all image
views are lower then THfg . (Will be detailed in the
next section.)
Fig. 2 gives an illustration of two incorrect 3D MAs in gray
that can be identified by the above filtering rules. After the
filtering procedure, the remaining 3D MAs will correspond to
the detected persons in the scene.
3. VANISHING POINT-BASED LINE SAMPLING
In a real scenario, many people may appear in a monitored
scene at the same time so that each segmented foreground
region may contain more than one person. Under such situ-
ations the 2D MA of each person is hard to detect correctly.
In [13], a possible solution to find 2D MAs is based on the
analysis of peaks and valleys in a histogram of foreground.
However, this approach may not work well for a very dense
group of people. Therefore, instead of estimating 2D MAs
for people, we apply 2D line samples on foreground regions
to construct 3D line samples. Then, 3D MAs for people local-
ization can be derived by clustering the 3D line samples into
individual groups.
3.1. Generating 2D line samples from vanishing points
Since the upper body of people are almost always perpendic-
ular to the ground plane when they are standing or walking,
we first generate 2D line samples which are originated from
the vanishing point1 corresponding to vertical 3D lines in the
scene (see Figs. 3(a) and (b)). Sampling lines not contain-
ing enough foreground pixels will be discarded since they are
expected to be far away from a 2D MA and will have little
contribution to 3D localization. Next, the remaining 2D line
samples are used to construct 3D line samples by the scheme
described in Section 2 for each pair of views. Fig. 3(c) shows
the reconstructed 3D line samples2. Since each of these 3D
line samples is constructed by observations from two views
only, some incorrect 3D line samples need to be removed by
applying the filtering rules, as discussed in the next subsec-
tion.
3.2. Generating 3D line samples and 3D major axes
While the first two filtering rules listed in Sec. 2.2 are more
intuitive, we focus on elaborating the third rule in this sub-
1The vanishing point in each view is estimated by calculating the inter-
section of 4 calibration pillars that are perpendicular to the ground plane.
2The 3D line samples are adjusted slightly to guarantee their perpendicu-
larity to the ground plane.
2011 18th IEEE International Conference on Image Processing530
(a) (b)
Fig. 6. Processing speed (in frame rate per second) of (a) our
method. (b) the generation of accumulated synergy map from
all reference planes.
tion is performed with an implementation based on C lan-
guage on Windows 7 with, 4 GB RAM and a 2.4G Intel Core2
Duo CPU. Fig. 6(a) shows the processing speed, in frame rate
per second (FPS), of our method for different portions of the
video, with intervals A to F corresponding to an increase from
1 to 6 persons in the scene, respectively. One can see that
the processing speed varies with people count and more than
2.790 FPS can be achieved when there are six people in the
scene. The average is 5.365 FPS. Fig. 6(b) shows the FPS re-
quired for the generation of synergy maps, as proposed in [7],
which varies much less with time and has an average value
of 0.118 FPS. (Note that CUDA adopted in [7] is not used
here). This is because its time complexity mainly depends on
the size of the whole image but not just the foreground.
5. CONCLUSIONS
We proposed a method for people localization which obtains
2D line samples, with each line originated from the vanish-
ing point of vertical lines in the scene, of foreground regions
in each view. Geometrically, a pair of line samples obtained
from two different views corresponds to a vertical line in the
scene. 3D point samples along such a vertical line can then be
obtained by projecting the above 2D line samples and identi-
fying their intersecting point on reference planes of different
heights, using homographic matrices each associating an im-
age to a reference plane. Finally, the 3D MA of each per-
son is estimated by clustering 3D line segments derived from
point samples satisfying some location and shape constraints.
Since the most time-consuming process of homographic pro-
jections are performed for line samples instead of the whole
image, the proposed approach can achieve near-real time per-
formance for localization accuracies similar to that in [7].
6. REFERENCES
[1] Q. Cai and J. K. Aggarwal, “Automatic tracking of hu-
man motion in indoor scenes across multiple synchro-
nized video streams,” in Proc Int. Conf. on Computer
Vision, 1998, pp. 356–362.
[2] I. Haritaoglu, D. Harwood, and L. S. Davis, “W4: A
real time system for detecting and tracking people,” in
Proc IEEE Int. Conf. on Computer Vision and Pattern
Recognition, 1998.
[3] S. Khan and M. Shah, “Tracking people in presence of
occlusion,” in Asian Conference on Computer Vision,
2000, vol. 5.
[4] M. Isard and A. Blake, “Condensation–conditional den-
sity propagation for visual tracking,” International Jour-
nal of Computer Vision, vol. 29, no. 1, pp. 5–28, 1998.
[5] K. Nummiaro, E. Koller-Meier, and L. Van Gool, “An
adaptive color-based particle filter,” Image and Vision
Computing, vol. 21, no. 1, pp. 99–110, 2003.
[6] H. Wang, D. Suter, K. Schindler, and C. Shen, “Adaptive
object tracking based on an effective appearance filter,”
IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 29, no. 9, pp. 1661–1667, 2007.
[7] S. M. Khan and M. Shah, “Tracking multiple occluding
people by localizing on multiple scene planes,” IEEE
Trans. Pattern Analysis and Machine Intelligence, vol.
31, no. 3, pp. 505–519, 2009.
[8] J. Orwell, S. Massey, P. Remagnino, D. Greenhill, and
G. A. Jones, “A multi-agent framework for visual
surveillance,” in Proc. Int. Conf. Image Analysis and
Processing, 1999, pp. 1104–1107.
[9] A. Mittal and L.S. Davis, “M2 Tracker: a multi-view ap-
proach to segmenting and tracking people in a cluttered
scene,” International Journal of Computer Vision, vol.
51, no. 3, pp. 189–203, 2003.
[10] T.-H. Chang and S. Gong, “Tracking multiple people
with a multi-camera system,” in Proc. IEEE Workshop
Multi-Object Tracking, 2001, pp. 19–26.
[11] R. Eshel and Y. Moses, “Homography based multi-
ple camera detection and tracking of people in a dense
crowd,” in Proc IEEE Int. Conf. on Computer Vision
and Pattern Recognition, 2008, pp. 1–8.
[12] R. Eshel and Y. Moses, “Tracking in a Dense Crowd Us-
ing Multiple Cameras,” International Journal of Com-
puter Vision, vol. 88, no. 1, pp. 129–143, 2010.
[13] W. Hu, M. Hu, X. Zhou, T. Tan, J. Lou, and S. Maybank,
“Principal axis-based correspondence between multiple
cameras for people tracking,” IEEE Trans. Pattern Anal-
ysis and Machine Intelligence, vol. 28, no. 4, pp. 663–
671, 2006.
2011 18th IEEE International Conference on Image Processing532
98 年度專題研究計畫研究成果彙整表 
計畫主持人：莊仁輝 計畫編號：98-2221-E-009-124-MY2 
計畫名稱：基於尺寸不變性之穩健式三維頭部追蹤 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 50% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
