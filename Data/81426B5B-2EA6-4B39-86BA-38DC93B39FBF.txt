三年的主題，但因台中榮總醫生們的建議而提前進行。目的
是藉由在病房中建置一套電子視窗，其以視覺分析技術為基
礎進行手勢遙控，以提升醫護人員的決策的效率、降低工作
負荷、減少人為疏失，使得醫護人員與病患、家屬等人，能
夠以非常簡便的手勢遙控，做資訊瀏覽與操控。 
中文關鍵詞： 智慧型視訊分析、硬體晶片設計、電子窗台、手勢遙控 
英 文 摘 要 ：  
英文關鍵詞： intelligent video analysis, Soc implementation, 
electronic window, hand-based remote control 
 
2 
行政院國家科學委員會專題研究計畫期中進度報告 
子計畫七：智慧型 ICU 視訊分析、顯示及晶片控制系統計畫(1/3) 
Sub-project 7 : Intelligent Video Display, Analysis and SoC Design for ICU (1/3) 
 
計畫編號：NSC  100-2220-E-009-057 
執行期限：2011 年 05 月 01 日 至 2012 年 04 月 30 日 
主持人：莊仁輝 國立交通大學資訊工程學系(所) 
 
中文摘要：(關鍵字：智慧型視訊分析、硬體晶片設計、電子窗台、手勢遙控) 
本計畫為期三年，主要目的為研發 ICU 醫療型視訊監控系統、智慧型日夜視訊顯示與分
析，及以手勢遙控之多功能電子窗台。在第一年的研究主題：醫療型視訊監控系統，主要以
視訊方式分析 ICU 的病患與病床周遭相關的動態資訊，其中包含了「強健性背景模型硬體化
之規劃」、「醫療之強健性背景模型硬體實現」與「智慧型病床動態分析演算法設計與效能分
析」。 
在「強健性背景模型硬體化之規劃」中，為了能準確、快速偵測移動物件，我們使用高
斯混合模型(Gaussian mixture model, GMM)，但傳統 GMM 中，對影像中每個像素皆給予相同
的學習速率，會造成背景變化之強健性與異常前景之敏感度的權衡(R-S tradeoff)及不同監控場
景之管理效率低落。因此，本計畫使用改良的 GMM，利用兩項獨立、動態更新之學習速率
以維持模型預測的準確性及調整權衡 R-S tradeoff。此外，為了讓此 GMM 演算法能夠應用在
嵌入式系統，本計畫提出一個去除浮點數參數的資料格式。在實驗中，我們將不同資料格式
的 GMM 演算法進行比較。實驗結果顯示，本計畫提出的方法可以維持高準確度。 
在「醫療之強健性背景模型硬體實現」中，因乘除法的精準度將直接決定最後演算法的
效果，而其與硬體的成本息息相關，所以為使 Memory Bandwidth 的需求下降，本計畫加入了
MMSQ 影像壓縮的方法。由實驗結果顯示，此演算法失真仍控制在理想的範圍內，且可以有
效降低 Memory Bandwidth，使得強健性背景模型硬體實現中，能支援更大的 Frame Size 或者
是更高的 FPS。 
「智慧型病床動態分析演算法設計與效能分析」是由本計畫到台中榮總進行探訪，經過
雙方討論後，對方對本研究方向感到興趣而進行的。主要是預防病患擅自進行非經由醫護人
員計畫之拔管行為。本計畫提出一套拔管偵測演算法，利用病床床頭上方攝影裝置所得到的
視訊影像，進行分析，其演算法主要是利用偵測視訊影像中手部位置與臉部位置，並加以分
析判斷病患是否意圖進行拔管動作，並且在病患進行拔管成功前發出預警訊息給予醫護人員。
其中手部位置評估是用連續影像進行背景相減得出變動部分，並以 Derivative of Gaussian 過
濾結果。在偵測臉部位置的方法中，使用 Haar 特徵，並透過 AdaBoost 訓練模型訓練出適合
ICU 模擬影片環境的人臉分類器。實驗結果顯示，對於模擬影像中病人的手部位置與模擬實
際上的位置有相當一致的指標性，而臉部辨識也能得到相當程度的理想結果，能初步提供警
示訊息給醫護人員做為參考、以期減少非計畫性拔管的發生。 
4 
will be able to give a warning to the paramedics when the positions’ information is well-combined and 
indicates that a possible unplanned extubation is about to take place. Position estimation of patient’s 
hands is carried out by performing derivative of Gaussian on background subtraction results of 
consecutive windows of frames and calculating the weighted center of gravity of the image values 
obtained. As for face detection, we use AdaBoost to train the face detection model with Haar-like 
features basing on example images generated from the simulated videos so as to make our detector 
applicable in the ICU environment. The result shows that for this specific case of unplanned extubation, 
our hand position estimation can correspond to real positions of patient’s moving hand, and the trained 
face detector can detect patient’s face at a high true-positive rate.  
“Gesture-based remote control system of multi-functional electronic windows” is the third-year 
topic of this project; however, due to advice from doctors of Taichung Veterans General Hospital, this 
research topic has been developed in advance. The electronic windows are remotely controlled by 
visually analyzing the hand gestures, and thus with this non-contact-type of system set in the ICUs, we 
can then help the paramedics to speedup their medical decision processes at lower risk of human errors 
and with greater ease. Additionally, with simple hand gestures, patients and their visitors can also enjoy 
the browsing services provided by the electronic windows.  
 
前言 
本計畫第一年的目標主要是以視訊方式分析 ICU 的病患與病床周遭相關的動態資訊。並
基於第一年研發的成果，在第二年發展夜間型「醫療型視訊監控系統」，以達到全天的醫療型
監控需求。且為了讓醫護人員能第一時間掌握 ICU 病患的狀況，本計畫在第三年會研發一套
電子視窗，並將其設置於 ICU 病房，其能顯示病患的生理資訊和醫療紀錄，並可進行資訊瀏
覽與操控;且本設備為避免因為接觸而受感染的風險，是以電腦視覺的方式分析手勢操作。綜
合此三年的研究，期待能對次世代的 ICU 設備做出貢獻。以下將分別介紹第一年研究主題「醫
療型視訊監控系統」的內容，「強健性背景模型硬體化之規劃」、「基於醫療之強健性背景模型
硬體化」、「智慧型病床動態分析演算法設計與效能分析」與因台中榮總醫生們的建議而提前
進行的第三年的主題，「手勢遙控之多功能電子窗台之初期研究分析」。 
(一)、強健性背景模型硬體化之規劃 
1.1 研究目的   
為了達到醫療型視訊監控系統的目的，本計畫將利用能即時偵測並適應漸進變化之環境
的高斯混合模型(Gaussian mixture model，簡稱 GMM) [1]為基礎，以監測多位病人的異常情形，
然而這需要大量的攝影機。在監控系統中，許多的管理方式都是將攝影機連接至伺服器，作
統一管理，所以伺服器必須負責許多影像處理及數據判斷。若將影像的演算法硬體化至每一
台攝影機，則每一台攝影機可以獨立處理自己擷取的影像，甚至直接判斷結果，最後只需將
處理後的結果傳送至伺服器即可，或是等到有需要應變的時候，再通知伺服器，這樣可以大
幅的降低傳輸的資料量及伺服器運算的負擔，進而提升伺服器的反應效率，因此可以增加伺
服器對攝影機的管控數量。 
6 
當在時間 t 內，Pixel x 有一連串的 Intensity， 𝐼0,𝑥 ,⋯ , 𝐼𝑡,𝑥，可為其建立 N 個 Gaussian 
Distributions，公式如下所示 : 
 
P(𝐼𝑡,𝑥) = ∑ 𝑤𝑡−1,𝑥,𝑛
𝑁
𝑛=1 𝒩(𝐼𝑡,𝑥; 𝜇𝑡−1,𝑥,𝑛 , 𝜎𝑡−1,𝑥,𝑛
2 )     (1.1) 
 
其中 𝑤𝑡−1,𝑥,𝑛，𝜇𝑡−1,𝑥,𝑛 和 𝜎𝑡−1,𝑥,𝑛
2  分別為第 n 個 Gaussian 的 Weight 、 Mean 和 Variance。
而這裡的𝒩 代表 Gaussian Probability Density Function 
 
𝒩(𝐼; 𝜇, 𝜎2) =
1
√2𝜋𝜎2
exp (−
(𝐼−𝜇)2
2𝜎2
)      (1.2) 
 
當觀察到新的 Pixel Intensity 𝐼𝑡,𝑥，會依情況更新 Background Model 中的各個 Gaussian 中
的參數。[8] 提出使用兩個 Learning Rate， 𝜌(𝛼) 和 𝜂(𝛽) 來更新 Gaussian Model。 假設 𝐼𝑡,𝑥 
match 到第 n 個 Gaussian Model，Gaussian Model 中參數的更新方式變為:  
 
𝜇𝑡,𝑥,𝑛 = (1 − 𝜌𝑡,𝑥,𝑛(𝛼))𝜇𝑡−1,𝑥,𝑛 + 𝜌𝑡,𝑥,𝑛(𝛼) ∙ 𝐼𝑡,𝑥    (1.3) 
𝜎𝑡,𝑥,𝑛
2 = (1 − 𝜌𝑡,𝑥,𝑛(𝛼))𝜎𝑡−1,𝑥,𝑛
2 + 𝜌𝑡,𝑥,𝑛(𝛼) ∙ (𝐼𝑡,𝑥 − 𝜇𝑡−1,𝑥,𝑛)
2   (1.4) 
𝑤𝑡,𝑥,𝑛 = (1 − 𝜂𝑡,𝑥(𝛽))𝑤𝑡−1,𝑥,𝑛 + 𝜂𝑡,𝑥(𝛽)    (1.5) 
Model中的精確度跟可靠度及靈敏度分別由這兩個 Learning Rate 控制更新 : (1)精確度：
經由 𝜌(𝛼) 的更新來維護場景的 𝜇 和 𝜎2 ，盡量提高場景的準確度。一般來說 0.001 ≤ 𝛼≤ 
0.1 ，而 𝛼 在這個範圍內越大越好。(2) 可靠度及靈敏度：Weight 則相對於用來判斷前景或
背景的標準，如公式(1.6)所示。當 Weight 大於 𝑇𝑤 時，將這一點視為背景，否則即將其視
為前景，而 Weight 則由 𝜂(𝛽) 來控制更新。 
 
𝐹𝑡,𝑥,𝑛 = {
0    if  𝑤𝑡,𝑥,𝑛 ≥ 𝑇𝑤
1   otherwise      
       (1.6) 
每個 Pixel 在時間 t 內可依據公式(1.7)的定義分為 4 種不同的 Object Types，包括 
Background、Shadow、Still foreground 和 Moving foreground。 
 
𝑂𝑡,𝑥 =
{
 
 
 
 
 0,         if 𝐹𝑡,𝑥,𝑙(𝑡,𝑥) = 0,   (Background)                                  
 1,         if 𝐹𝑡,𝑥,𝑙(𝑡,𝑥) = 1 and Type(𝐼𝑡,𝑥) = Shadow               
 2,         if 𝐹𝑡,𝑥,𝑙(𝑡,𝑥) = 1 and Type(𝐼𝑡,𝑥) = Still foreground
 3,        Otherwise(Moving foreground)                                
    (1.7) 
8 
圖 1-2 原始演算法 
因為考慮到其他因素及複雜度，本計畫實作的部分為: (1)兩種 Learning Rates。(2) Weight 
四種狀況的 Learning Rates 中，實作了三種狀況:(a) Background，(b) Still Foreground，(c) 
Moving Foreground。修改後之演算法，如圖 1-3。在第一部分，將 𝜌 設定成定值: 0.025，以
及記錄 Gaussian 是否 Matched 的𝑀𝑡,𝑥變數初始值設定為 N ，若第 n 個 Gaussian Matched，則
令𝐼𝑡,𝑥 =  。這樣在判斷 Gaussian是否Matched 時，則只需要判斷是否跟初始值一樣為 N即可。
第二部分則採用原始的演算法，使用兩種不同的 Learning Rates 𝜌(𝛼)、𝜂(𝛽)對 𝜇、 𝑉𝑎𝑟 和 𝑤
進行更新。 
 
圖 1-3 修改後之演算法。 
而整個演算法的流程圖如圖 1-4 所示，除了需要實作上述的 GMM 的演算法外，還需要
實作 “Erosion & Dilation”(第三個步驟) 以及“Connected Component & Check Type” (第
四個步驟) 的影像處理演算法。以下簡略敘述整個流程的各個步驟。第一個步驟，“Check 
Match Gaussian”是檢查 Model 中的是否有 Gaussian 被新觀察到的像素 Match 到，如果有，
則紀錄是哪一個 Gaussian。第二步驟，“Update Foreground”，是利用第一個步驟找出的
Gaussian，並判斷它的 Weight 是否大於 𝑇𝑤，若大於 𝑇𝑤 則為背景，反之則為前景。第三個
10 
在 Partial 32-bit 的格式中，定義資料格式時，我們以速度為最高考量，避免使用到 64 位
元的緩衝記憶體。 𝐼𝑡,𝑥 和 𝜇𝑡,𝑥,𝑛的最大值為 255，所以我們保留整數的位元數為 8 ( 2
8 = 256)，
8 個位元給小數用；因為 𝑉𝑎𝑟𝑡,𝑥,𝑛 的值最大值為 65025，但是為了使運算能在 32-bit 格式的
範圍內完成，所以我們將整數最大值限定在 10 個位元(210= 1024)，大於 1023 的值將會被
視為 1023。小數的部分則保留 8 個位元來表示； 𝑤𝑡,𝑥,𝑛、 𝜂𝑡,𝑥,𝑛 及 𝜌 其最大值為 1，所以
僅需用 1 個 bit 來表示整數部分，小數部分則使用 15 個位元表示。 
定義完資料格式後，須檢查溢位的問題。我們檢查所有的乘法的運算。第一部分，因為乘
數與被乘數皆在 16 個位元內，相乘後的結果不會超過 32 個位元。所以這些運算皆可以在 
32 位元的格式中完成運算，不會有溢位的問題，如表格 1-1。但是第二個部分，如表格 1-2
所示，因為被乘數為 16 個位元，乘數為 18 個位元，所以運算的結果會達到 34 個位元，
可能無法在 32-bit 的格式中完成運算。前半部的 (1 − 𝜌) ∙𝑉𝑎𝑟𝑡,𝑥,𝑛的計算 ，其中 𝜌 的小數有 
15 個位元，為了使運算能在 32-bit 的格式中完成，所以我們將 (1 − 𝜌) 的值右移 2 個位元，
其格式將暫時轉換成 1.13 的格式 (格式為 : 整數.小數)。這樣運算將可轉換成 14 位元乘上 
18 位元，即可在 32 位元的格式中完成運算，其計算轉換流程如圖 1-5。後半部的計算為，
 ρ ∙ (𝐼𝑡,𝑥 − 𝜇𝑡−1,𝑥,𝑙(𝑡,𝑥))
2
= 𝜌 ∙ 𝑉𝑎𝑟𝑡,𝑥,𝑛，其中 𝜌= 0.025 ，𝑉𝑎𝑟𝑡,𝑥,𝑛 容許的最大值為 1023，將這
兩個數轉換成相對應的格式 1.15 及 10.8 ，所以𝜌 ∙ 𝑉𝑎𝑟𝑡,𝑥,𝑛 的最大值等於(0.025 × 2
15)  × 
(1024 × 28) = 214696936 < 232 ，其結果小於 32 位元的最大值，所以ρ ∙ (𝐼𝑡,𝑥 − 𝜇𝑡−1,𝑥,𝑙(𝑡,𝑥))
2
 
的結果不可能超過 32 位元的格式可以確定在 Partial 32-bit 的格式中不會發生溢位的問題。 
 
表格 1-1 Partial 32-bit 運算式與運算格式對照表 1
 
 
表格 1-2 Partial 32-bit 運算式與運算格式對照表 2 
 
 
 
圖 1-5 Partial 32-bit 𝑉𝑎𝑟 ,𝑥, 格式轉換流程 
12 
 
 
1.5 參考文獻   
[1] C. Stauffer and W. Grimson, “Adaptive background mixture models for real-time tracking,” in 
Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 1999, vol. 2, pp. 246–252.  
[2] T. E. Boult, R. Micheals, X. Gao, P. Lewis, C. Power, W. Yin, and A. Erkan, “Frame-rate 
omnidirectional surveillance and tracking of amouflaged and occluded targets,” in Proc. 2nd IEEE 
Workshop on Visual Surveillance, 1999, pp. 48–55. 
[3] X. Gao, T. E. Boult, F. Coetzee, and V. Ramesh, “Error analysis of background adaption,” in 
Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2000, vol. 1, pp. 503–510. 
[4] M. Harville, “A framework for high-level feedback to adaptive, perpixel, mixture-of-Gaussian 
background models,” in Proc. Eur. Conf. Computer Vision, 2002, vol. 3, pp. 543–560. 
[5] Y.-L. Tian, M. Lu, and A. Hampapur, “Robust and efficient foreground analysis for real-time 
video surveillance,” in Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2005, vol. 1, pp. 
1182–1187. 
[6] D.-S. Lee, “Effective Gaussian mixture learning for video background subtraction,” IEEE Trans. 
Pattern Anal. Mach. Intell., vol. 27, no. 5, pp. 827–832, May 2005. 
[7] H. Yang, Y. Tan, J. Tian, and J. Liu, “Accurate dynamic scene model for moving object 
detection,” in Proc. IEEE Conf. Image Process., 2007, vol. 6, pp. 157–160. 
[8] H.-H. Lin, J.-H. Chuang, T.-L. Liu, “Regularized background adaptation: a novel learning rate 
control scheme for Gaussian mixture modeling,” IEEE Trans. Image Processing, vol. 20, no. 3, pp. 
822 – 836, 2011. 
[9] Y. Chen, Q. Hong, X.-Y. Chen, and C. Zhang, “Real-time speaker verification based on 
GMM-UBM for PDA” in Proc. 5th IEEE Int. Symposium Embedded Computing, 2008, pp. 243 – 
246. 
14 
1. 測出的執行速度是由測試時所使用的處理器與編譯器決定，和實際將 GMM 實施於硬體上
仍會有些許的落差。 
2. 硬體實現會面臨 Memory Bandwidth 這個重要的問題，但以軟體為基礎的「強健性背景模
型硬體化之規劃」無法對這方面多加著墨。 
所以在「醫療之強健性背景模型硬體實現」的部分，將會以降低 Memory Bandwidth 為主要考
量。  
2.3 研究方法  
2.3.1 降低 Memory Bandwidth 的需求 
為進一步使 Memory Bandwidth 的需求降低，本計畫對選取的演算法做了以下的改進: 
(a) 影像壓縮 
在以 Full 32-bit 計算的基礎上，加上影像壓縮的處理。本計劃採用的影像壓縮方法為
MMSQ [1]，它是一個失真壓縮的演算法，其所需的硬體量和延遲都相當低，而且可以隨機讀
寫，相當適合作為壓縮 GMM 模型時的方法。壓縮端的詳細的流程如下： 
Step 1. 每次取 4*4 大小的影像作為 block。(也可是 5*5 或其它大小，依實際狀況決定) 
Step 2. 在這個 block 中，找出數值最大者，和數值最小者，並將他們的差值設為這個 block
的 range 值。 
Step 3. 若是以 20 位元表示壓縮完的輸出，則 Quantization Parameter (QP) 
=(1<<20)/range。 
Step 4. 依序把 block 中的輸入值與數值最小者相減，並除上 QP。 
Step 5. 把最小數值和 QP 放在每個 packet 的前面，並輸出 Step 4.的結果。 
 
圖 2.1 MMSQ的部分步驟 
MMSQ 的部分步驟顯示於圖 2.1。 
(b) Erosion 和 Dilation 的改進 
16 
消 Connected Component Labeling 後的演算法仍有一定的效果，但這仍需做大量的實驗才能下
結論。 
 
 
 
圖 2.2 第一列是未對變數壓縮的原圖和偵測到的前景。第二列為經壓縮處理後的結果。可看出原本的演算法對 
於物件較為敏感，例如火車的部份偵測得更為準確，不過汽車的邊緣也因為敏感度，有更多錯誤警報。 
 
 
(a)           (b) 
圖 2.3 共有兩個測試影片，分別為(a)和(b)。第一列是未對變數壓縮的原圖，第二列為經壓縮處理的結果。 
 
18 
3.2.1 手部位置估測 
以視覺為基礎的手部偵測有許多種方法，多為利用皮膚顏色(skin color)直進行偵測[2-3]
或是以學習為基礎的偵測方式[4]。G. Z. Mao 等人[3]提出一套即時手部偵測與追蹤演算法，
利用影像中手部特徵進行複雜背景下即時偵測。但由於模擬影片中病人的手部覆蓋於棉被之
下，無法以皮膚的色彩資訊等直接進行手部辨識與追蹤。因此，我們必需使用其它的式分析
影像變化與手部移動之關聯。 
3.2.2 人臉偵測 
在人臉偵測的方法中大致可分為兩類，第一類是以特徵為基礎(Feature-base)的偵測，例
如：直接利用膚色的方式去辨別人臉[5]；另外一類是以學習為基礎 (Learning-base)的人臉偵
測，例如：類神經網路(neural network, NN)[6]和支持向量機(support vector machines, SVM) [7]
等。其方法主要是根據影像中是否含有人臉特徵來判斷，例如：雙眼、鼻子與嘴巴。近年來， 
Paul Viola 等人[4]提出基於 Boosting 之強韌性即時人臉偵測演算法，使用積分影像(Integral 
Image)進行大量的 Haar-like 特徵擷取，接著再利用 Boosting 演算法[8]找到少量獨特特徵，並
調整不同的權重對應不同的特徵，建構成強分類器，如圖 3-1 所示，Boosting 演算法所挑選的
的第一及第二個特徵。最後使用層疊分類器(cascade classifier)有效降低偵測人臉的時間花費。Lun 
Zhan 等人[9]提出基於 Multi-block Local Binary Patterns (MB-LBP)特徵的人臉偵測，MB-LBP 特徵
是利用矩形區域的灰階強度進行 LBP 編碼運算，其運算的結果可以描述該區域與鄰近之間灰階強
度的關係，如圖 3-2 區域與鄰近之間灰階強度的關係圖，接著使用 multi-branch regression tree 的
Boosting 演算法將弱分類器(weak classiﬁers)建構成強分類器。MB-LBP 人臉偵測較 Haar-like 特徵
更能有效描述人臉特徵與降低特徵數[9]。 
 
圖 3-1，經由 Boosting 選出的第一及第二個特徵[1] 
 
 
 
10 10  0 0 0     
200 100 10 Thresholding 1  0 Describing    
200 200 200  1 1 1     
MB-LBP: 0001111 
區域內灰階平均值:10 
10 10 10 
10 
10 
10 
10 10 
10 
20 
(a) (b) 
(c) (d) 
(e) (f) 
 
(g) 
圖 3-4 手部位置估測流程，(a) 輸入影像，(b) 背景相減的結果以黃色顯示疊合在原影像之上，(c) x 方向執行
derivative of Gaussian 的結果，(d) y 方向執行 derivative of Gaussian 的結果，(e) 為以(c)的結果加權計算出 x 方向
22 
本計畫將攝影機架於 ICU 模擬病床床頭、拍攝模擬病人自行拔管的動作，並拍攝數次模
擬病人自行拔管影片，並進行拔管前的「手部位置估測」與「人臉偵測」，以下將分別進行討
論。 
3.4.1 手部位置估測 
圖 3-7 為對一段模擬影片執行手部追蹤之結果。其中圖 3-7 (a) 中紅色交叉點所標示的追
蹤位置與實際上手部位置有很大的誤差，其因為在模擬影片片段前於病人並沒有動作，也還
沒有手部的動作，因此演算法中取得的變動前景僅包影片中的小偏動而非正確的手部追蹤位
置。 
(a) Frame number: 25 (b) Frame number: 40 
 (c) Frame number: 55  (d) Frame number: 70 
 (e) Frame number: 85  (f) Frame number: 100 
圖 3-7 病人自行拔管手部追蹤之結果，(a) – (f) 為依照模擬影片時序取出的六張影像，各張影像的 frame 編號如
圖中附註所示。(frame rate: 30 fps) 
24 
(c)  (d) 
(e) (f) 
圖 3-8 模擬影片人臉辨識之結果。 
3.5 參考文獻   
[1] C.-M. Hsu, H.-C. Li, C.-H. Hsueh, Y.-L. Wang, and H.-G. Shie, "Exploring the influences on 
successful weaning ventilator with unplanned extubation patients," Journal of Respiratory 
Therapy, vol. 10, no. 2, pp. 57-57, 2011. 
[2] M. J. Jones and J. M. Rehg, "Statistical color models with application to skin detection," in 
Proc. IEEE Conf. Comput. Visi. Pattern Recogn., 1999, vol. 1, pp. 1274.  
[3] G.-Z. Mao, Y.-L. Wu, M.-K. Hor, and C.-Y. Tang. "Real-time hand detection and tracking 
against complex background," in Proc. Int. Conf. Intelligent Information Hiding and 
Multimedia Signal Processing, 2009, pp. 905-908. 
[4] P. Viola and M. Jones. "Robust real-time object detection," in 2nd Int. Workshop on Statistical 
and Computational Theories of Vision – Modeling, Learning, Computing, and Sampling, July 
2001. 
[5] J. Terrillon, M. David, and S. Akamatsu, "Automatic detection of human faces in natural scene 
images by use of a skin color model and of invariant moments," in Proc. 3rd Int. Conf. on 
Automatic Face and Gesture Recognition, pp. 112-117, Japan, 1998. 
[6] H. A. Rowley, S. Baluja, and T. Kanade. "Neural network-based face detection," IEEE Trans. 
26 
影像進行前後景切割，或是在身上穿戴用於識別的物件，而採用單一且具深度感應攝影機為
基礎，達成非接觸、不須穿戴任何裝置且具即時反應能力的互動系統。 
4.3 研究方法   
以手勢遙控之多功能電子窗台，使用具有深度感測能力的 Kinect 攝影機直接對前景進行
分析，手勢控制部份包含滑鼠移動控制與滑鼠點擊。 
4.3.1 滑鼠移動控制 
在圖形化介面下不可或缺的就是滑鼠控制，本計畫用 OpenNI 函式庫[7]偵測影像中手掌
的質心，利用如揮手或舉手動作進行初始化，隨即開始進行追蹤。用此手掌的質心做為滑鼠
移動的參考點，計算出與電腦螢幕相對應的座標點，讓滑鼠游標能及時移動到該位置上，如
圖 4-1。 
 
圖 4-1 手掌質心與滑鼠指標 
4.3.2 滑鼠點擊 
由於手掌質心向前推移時會造成參考點有深度上的變化，利用此方式來達成模擬滑鼠點
擊功能。 
  
(a) (b) 
圖 4-2 利用深度變化達成點擊效果 
4.4 研究結果與討論 
本計畫以 Java SE6 與 OpenNI[4]進行系統實作，實驗的硬體平台為 CPU 為 Intel i5 2500K
記憶體為 16G 的個人電腦與 Kinect。在實驗中，我們以控制滑鼠為目標，透過即時分析 Kinect
攝影機所拍攝之深度影像，成功達成以手勢控制滑鼠的移動控制與滑鼠點擊。 
手掌質心 
滑鼠指標 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/02/29
國科會補助計畫
計畫名稱: 子計畫七：智慧型ICU視訊分析、顯示及晶片控制系統計畫(1/3)
計畫主持人: 莊仁輝
計畫編號: 100-2220-E-009-057- 學門領域: 智慧電子科技計畫-整合型學術研
究計畫
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
