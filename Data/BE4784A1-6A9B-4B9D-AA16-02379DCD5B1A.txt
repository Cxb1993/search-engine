  
  
Abstract—In this paper, a neuron-fuzzy classification 
model (NFCM) is proposed that enables the extraction the 
feature variable and infers the classification results. The 
proposed classification model synergistically integrates the 
standard fuzzy inference system and a supervised learning 
concept of neural networks. The NFCM automatically 
generates the fuzzy rules from the numerical data and 
triangular functions are used as the membership functions 
both in the feature extraction unit and the inference unit. 
To improve the proposed NFCM, one modificatory 
algorithm is applied. We utilize the Evolutionary 
Programming (EP) to determine the distribution of the 
fuzzy sets for each feature variable of the feature 
extraction unit. Finally, the proposed NFCM has been 
validated using one benchmark data set that is the Wine 
database. Computer simulation results demonstrate that 
the proposed classification model can provide a sufficiently 
high classification rate in comparison with other models in 
the literature. 
 
Keywords: Fuzzy set, neural networks, 
classification problem. 
I. INTRODUCTION 
Classification problem is the most fundamental 
element in human’s representation the reasoning 
process.[1, 2, 5, 10, 18, 19 ] Since the representation and 
the inference process of human being are uncertain, a 
great quantity of research has been dedicated to develop 
valid approaches to modeling a classification system. In 
empirical classification problems, the actual probability 
distribution of the feature variables is unknown, so we 
can not directly utilize Bayesian decision theory to 
produce the optimal results. Under this circumstance, 
many non-Bayesian classification techniques are 
designed to solve this problem. Expert systems have 
become more and more significant in the area of the 
classification problem. The aim of the expert system is 
to discover a structure that can effective divide the data 
into the correct class based on a priori knowledge or on 
the statistical information extracted from the training 
                                                 
 
data. An expert system can be viewed as a certain 
method of the computer program that approximates the 
reasoning process of a human expert who explains 
particular real world problems. Taking advantage of the 
state of the pattern classification, a large variety of 
classifiers and learning methods are available for 
performing classification based detection. Moreover, 
when the classification system is identified as part of an 
expert system, the linguistic interpretability is also an 
important aspect which must be taken into account. The 
classification methods based on expert system include 
fuzzy theory, statistical methods, artificial neural 
networks and genetic algorithm, etc. 
Fuzzy set theory originally proposed by Zadeh [25] 
provides a general way to acquire the form of linguistic 
IF–THEN rules. Fuzzy logic aids to improve the 
interpretability of knowledge based system through its 
linguistic that provides insight in the classification 
model and the decision marking process. Design of 
fuzzy systems should operate with knowledge in the 
form of linguistic if-then rules. But the translation of 
these linguistic rules into the structure of fuzzy set 
theory relies on the choice of certain parameters. In 
general the fuzzy classification system has two problems, 
one is the number of the fuzzy partition, and the other is 
the number of if-then rules. Ishibuchi et al. [15, 20] have 
proposed distributed fuzzy rules which considered all 
possible fuzzy rules with several different fuzzy 
partitions. The main drawback of this approach is the 
number of fuzzy rules becomes enormous for the 
classification problem with high dimensional feature 
variables. In order to reduce the number of rule, many 
methods have been applied, such as genetic algorithms 
(GAs) [10], fuzzy binary box tree and so on [23]. 
In recent years, neural network ensemble has become 
a hot topic in the machine learning [6, 16, 21]. Artificial 
neural network is named from the network of nerve cells 
in the human brain and have been widely used as a tool 
for the classification problem. The capability of a neural 
network originates with the ability to learn from a given 
database without the use of any information about the 
underlying process. As the internal configuration is 
行政院國家科學委員會專題研究計畫報告 
以模糊-基因演算法及模糊-類神經網路為智慧型分類器之設計與應用 
Design and Application of Intelligent Classification Systems Based on A 
Fuzzy-GAs Model and A Fuzzy-Neural Network 
計畫編號：NSC  97－2221－E－272－003 
執行期限：97 年 8 月 1 日至 98 年 8 月 31 日 
主持人：郭乃仁 東方技術學院電機系 
共同主持人：蔡宗吉 東方技術學院電機系 
計畫參與人員：林俊宏、魏證倫、曾志宏 
 
  
inference system. Triangular membership functions are 
employed at the membership nodes. In the 
defuzzification layer, neurons are connected using the 
link Layerw . The framework is showed in Figure 2. 
 
The input to NFCM is a non-fuzzy data vector, 
represented by 1 2[ , ,  , ]p p p pnx x x x= … . The net input to the 
ith node of layer1 is pix . The corresponding weight of 
layer 1 is  1Layerw  and equals 1. The output from the 
corresponding node is 
1 1Layer
i pi if x w= ⋅  , 1,  2,  ...,  i n=     (3) 
A.  Layer 2: fuzzification layer 
In this layer, each feature variable is divided into iP  
fuzzy partitions by the fuzzy grids. The membership 
functions can be represented by the triangular function. 
The number of nodes on this layer is 
1
n
i
i
P
=
∑ , and nodes in 
this layer are named as linguistic nodes. The linguistic 
nodes will directly transmit the non-fuzzy inputs from 
the layer 1, so the  2Layerw  are defined 1. The output 
from the corresponding node is 
2 2max(1 ,  0)
fK
pi j Layer
ij
x a
f w
b
−= − ⋅      (4) 
where 2
1i
b
P
= − , 1 ( 1)
iP
ja j b= − + − , 1,  2, ..., i n=  and 
1,  2, ..., ij P= ， iP  presents the number of partition in the 
ith feature variable. Each node receives only one input 
as one dimension of the data vector to the whole 
network, and output to iP  nodes of the next layer. 
B.  Layer 3: fuzzy rule layer 
This layer defines the fuzzy if-then rules and the form 
represents in (1). Each node in this layer can regard as a 
fuzzy rule and the output is a vector weight that can be 
considered as the weight of the rule over every class. 
The weights are calculated from the training database, 
that is, more data belong to the rule and that node will 
with higher weight. From the all training data, the  Class vijψ  
is calculated by the ith feature variable that conform 
with jth linguistic variable and belongs to class v . The 
 3Layerw  is formulated by  
 
 3
 
1
Class v
ijLayer
ijv C
Class v
ij
v
w
ψ
ψ
=
=
∑
( 1,2,...,v C= )  (5) 
where  
  2
 
( )Class vij ij pi
p Class
f x
ν
ψ
∈
= ∑   (6) 
Since the denominator of (5) equals zero, it means no 
training datum is compatible with that rule, and then the 
weights will be set to zero. The output node is given as 
3  3 2max( )Layeriv ijv ijf w f= ⋅   (7) 
C.  Layer 4: output new data layer 
The task of this layer is to obtain new feature 
variables that the number of dimension is from n to C. 
The weight  4Layerw =1, and the output node is defined as 
4  4 3max( )Layer newv iv iv pvf w f x= ⋅ =∑    (8) 
Until now, the ideal of the feature extraction unit has 
realized. The rule base is created by the property of the 
training data. In this section, the membership functions 
are fixed for each feature variable, however, this is not 
an optimal design. In order to solve this problem, we 
will propose an EP algorithm to improve the 
performance. The feature extraction unit is used to 
obtain the new feature variables through the above 
layers, where the dimension of feature decreases from n 
to C. 
 
IV. THE NETWORK OF THE INFERENCE UNIT 
This network contains four layers that the dimension 
of the input variables is now equal to C and the output is 
the inference class. The rule of the inference unit is 
represented in equation (3) where it can be regarded as 
an C-input-one-output problem. The architecture of the 
inference unit is shown as Fgure 3. In the input layer, 
each neuron connects to each element of a 
C-dimensional input vector. The if-then rules layers also 
separate into two parts that are an antecedent part and a 
consequent part. The form of if-then rules in the 
inference unit is different from the feature extraction 
unit. For each if-then rule, a fuzzification procedure and 
a fuzzy ``AND'' operation are performed in the if-then 
rules layers. The classification result is determined by 
the defuzzification layer of the inference unit.  
5Layerw
6Layerw
7Layerw
8Layerw
 
Fig. 3. The inference unit  
A.  Layer 5: input layer 
In this layer, the new data will be partitioned into Q 
fuzzy subsets. It is similar to the layer 2, but the number 
of partition is fixed for each feature variable in this layer. 
The weight 5Layerwν  equals to one, and the output node 
is defined as follows. 
5  5
'max{1 ,  0}
new Q
pv h Layer
vh v
x a
f w
b
−= − ⋅   (9) 
where ' 2
1
b
K
= − , 
'1 ( 1)Kha h b= − + − , 1,  2, ..., Cν =  and 
1,  2, ..., h Q= ， Q  presents the number of partition in the 
new feature variable. 
B.  Layer 6: the conditional layer 
The aim of this layer is to realize the antecedent part 
of the rule. The links of the nodes perform the condition 
matching of if-then rules. Thus, the rule nodes execute a 
fuzzy AND operation (or product inference) to figure 
out the excited strength. The output node is defined by 
6  6 5Layer
a vh vhf w f= ⋅∏   (10) 
where a  is the index of the corresponding node, 
  
evaluate the performance of the NFCM. The detailed 
discussions will present in Section 6.  
VI. SIMULATION RESULT AND CONCLUSIONS 
To evaluate the performance of the proposed NFCM, 
it is employed to obtain classification rate on the 
well-known database including the Wine and the Iris 
database. The Wine database is the result of a chemical 
analysis of wines grown in the same region in Italy. This 
database consists of 178 data with 13 feature variables 
belonging to three classes. In this work, we use 13 
feature variables namely: Alcohol, Malic acid, Ash, 
Alcalinity of ash, Magnesium, Total phenols, Flavanoids, 
Nonflavanoid phenols, Proanthocyanins, Color intensity, 
Hue, OD280/OD315 of diluted wines, Proline. The 
numbers of samples in the three classes are 59, 71, and 
48. 
A. The classification results of the training phase 
The classification results of the original NFCM are 
shown in Table I, where CR means the classification 
rate and defines as (16). And the training data are used 
as testing data. The P and Q represent the partition 
number of the feature extraction unit and the inference 
unit, respectively. From the classification, we can find 
that the relationship between CR and partition numbers 
(P and Q) is not significant for these two databases. The 
Wine .database can accomplish a CR of 100% as (M1, 
M2)=(5, 7) for the original NFCM.  
The correct classification patterns
Total patterns
CR =  (16) 
 
TABLE  I  
THE CR OF WINE DATABASE ON THE ORIGINAL MODEL 
P CR (%) 3 4 5 6 7 
3 86.5 92.7 92.7 90.5 92.3 
4 93.8 96.6 96.1 97.2 98.9 
5 93.8 98.9 97.2 96.6 98.3 
6 94.4 97.2 97.8 98.9 98.9 
Q 
7 95.5 99.4 100.0 98.9 99.4 
 
TABLE II 
 SIMULATION RESULTS WITH EP ALGORITHMS  
CR(%) Wine Database 
3 97.8 
4 99.4 
5 100.0 
6 100.0 
Q 
7 100.0 
 
Next, the EP is utilized to search the fuzzy sets for every 
original feature universe that is described in the above 
section, and the simulation results are shown in Table II. 
From the Table II, we can obtain the performances of the 
proposed NFCM to be effectively improved when the 
EP is applied to tuning the membership functions. 
Particularly, the CR of the Wine database based on our 
classification model is 100% in three cases that Q is 
bigger than 4. 
For the training-data-itself-as-testing phase of the 
Wine database, Corcoran and Sen [1] applied a 
real-coded genetic-based-machine learning approach to 
acquire. 100% CR. Setnes and Roubos [24] apply the 
fuzzy model to get 94.9% CR and the GA-based fuzzy 
model to obtain 98.3% CR. Baralis and Garza[2] use the 
non-fuzzy classification method, the Live and Let 
Live ( 3L ) one, to find the 97.19% CR. Ishibuchi et al. [4] 
applied a fuzzy classifier with the best CR 99.4%. The 
CR of the proposed NFCM can achieve 100%.with 
Q=5~7. 
 
TABLE III 
Simulation results (Testing phase) 
Testing 
manner K 3 4 5 6 7 
leaving-one-out CR (%) 97.75 99.44 100 100 97.75
The best 
CR (%) 97.75 99.44 100 99.44 100
The worst 
CR (%) 96.63 98.31 98.88 96.63 97.19twofold-crossvalidation 
The 
average 
CR (%)
97.44 99.18 99.69 99.11 99.47
The best 
CR (%) 97.75 99.44 100 100 100
The worst 
CR (%) 96.63 98.88 99.44 99.44 98.88fivefold-crossvalidation 
The 
average 
CR (%)
97.65 99.42 99.99 99.99 99.84
The best 
CR (%) 97.75 99.44 100 100 100
The worst 
CR (%) 97.19 98.88 100 100 99.44tenfold-crossvalidation 
The 
average 
CR (%)
97.68 99.45 100 100 99.99
 
B.  The classification results of the testing phase 
To examine the CR of the proposed NFCM for testing 
samples, there are two techniques are used that one is 
the leaving-one-out and the other is K-fold cross 
validation. The leaving-one-out technique is an almost 
unbiased estimator of the true CR of a classifier [18]. 
Namely, one should iterate this process until the entire 
given data have been used as a testing datum and 
calculate the average classification rate. For the Wine 
database, the weights of the NFCM rules are generated 
from 177 training data and tested on the single 
remaining datum. The K-fold cross validation technique 
means the data divided into K subsets that have similar 
size and class distributions. Each subset will left out 
once, while the other K-1 are applied for the 
construction of the classifier which is subsequently 
validated for unseen cases in the left out subset. The CR 
may decrease in the testing phase, because the testing 
data has no information in the training phase. In Table 
III illustrates the simulation results by the proposed 
NFCM. Some different number of partition (M2=3~7) 
have been considered. The CR of the Wine database can 
achieve 100% for all testing techniques. 
There are many papers have been used the 
leaving-one-out technique to estimate the performance 
of the propose classification model In comparison with 
the leaving-one-out technique, the best CR of the 
proposed NFCM for the Wine database is 100%. Zhou 
[11] applied GA-based fuzzy classifiers to got a 98.31% 
CR, Wu et al. [12] has 96.63% CR, Wang et al. [14] got 
a 97.5% correct classification rate. By using the ten-fold 
cross-validation to examine the performance, the CR of 
the principal component analysis[7], the decision 
tree-based fuzzy classifier[7], the GA-based non-fuzzy 
classifier[7], and the hybrid algorithm with 
  
參加國科會專題計畫補助國際研討會心得報告 
 
報 告 人 郭乃仁 職 級 助理教授 任職單位 電機系 
研 習 日 期 98 年 3 月 26 日至 98 年 3 月 30 日  共 5 日 
主 辦 單 位 IEEE Systems, Man, and Cybernetics Society, 日本岡山大學 
研 (討 )習會
名 稱 2009 IEEE International Conference on Networking, Sensing and Control 
心 得 
報 告 
此次會議議程時間由 98 年 3 月 26 日~30 日，共計五天，於日本岡山
大學館舉行，會中除發表本人之研究論文外，也至各議程，聆聽各
專家學者之論文發表。印象較深的是 27 日早上由 Yutaka Yamamoto
教授主講 Sampled-Data Control and Its Applications to Digital Signal 
Processing，此演講內容提在連續系統中針對數位化所設計出數為系
統之間的關係，並利用H∞ 的強健控制法則，評估所設計出之取樣系
統之性能。另外，亦將可所提出系統針對聲音、影像等相關部分進
行處理與分析，同時也展示例子說明其對問題研究之重要性，受益
良多。此外，本次會議成功大學工科所廖德祿教授也帶領數為博士
班學生與會參加。 
  
與廖德祿教授合影         遠東科大與會教授 
另外，遠東科技大學副校長鐘明吉教授也帶領數位老師與會參加，
以下為遠東科技大學的與會教授。 
2009 IEEE International Conference on 
Networking, Sensing and Control 
 
 
Sponsored by the IEEE Systems, Man, and Cybernetics Society 
March 26-29, 2009, Okayama City, Japan 
CONFERENCE PROGRAM 
Thursday, March 26, 2009 
10:00 - 17:30     Registration 
18:30 - 20:00   Welcome Reception (Peach Union) 
 
 
 
 
 
 
 
 
 
 
 
 
 
Saturday, March 28, 2009 
09:00 - 10:00     Plenary Speech 3 (PS03)                                 Special Lecture Room  
Title: Wearable Power Assist Robot Driven with Pneumatic Rubber Artificial Muscles 
Speaker: Toshiro Noritsugu, Okayama University, Japan 
10:00 - 10:20     Break 
10:20 - 12:00     Parallel Paper Sessions 
No. 2 Lecture Room No. 5 Lecture Room No. 6 Lecture Room No. 7 Lecture Room 
Special Lecture 
Room  
SatA01 
Advanced Intelligent 
Algorithms for 
Control 
SatA02 
Smart Car and 
Vehicle Control/CIS 
SatA03 
Advanced 
Technologies for 
Intelligent Systems 
SatA04 
Human Adaptive 
Mechatronics II 
SatA05 
Control Theory I 
12:00 - 13:00     Lunch 
13:00 - 14:00     Plenary Speech 4 (PS04)                                  Special Lecture Room  
Title: Optimal Operation and Feedback Control for Complex Industrial Process 
Speaker: Tianyou Chai, Northeastern University, P.R. China 
14:00 - 14:20     Break 
14:20 - 16:00     Parallel Paper Sessions 
No. 2 Lecture Room No. 5 Lecture Room No. 6 Lecture Room No. 7 Lecture Room 
Special Lecture 
Room  
SatM01 
Control of Networks 
II 
SatM02 
Sensor Networks II 
SatM03 
Networked Control 
Systems II 
SatM04 
Control Theory II 
SatM05 
Smart Vision and 
Image Processing 
16:00 - 16:20     Break 
16:20 - 17:40     Parallel Paper Sessions 
No. 2 Lecture Room No. 5 Lecture Room No. 6 Lecture Room No. 7 Lecture Room 
Special Lecture 
Room  
SatP01 
Applications and 
Reviews of 
Workflow Design 
and Management in 
Industry II 
SatP02 
Decision Making 
and Management 
of Infrastructure 
Networks II 
SatP03 
Advanced Controller
Design and 
Applications II 
SatP04 
Safety and 
Functional 
Maintenance Issues 
of Industrial 
Processes 
SatP05 
Control Application 
III 
2009 IEEE International Conference on 
Networking, Sensing and Control 
 
 
Sponsored by the IEEE Systems, Man, and Cybernetics Society 
March 26-29, 2009, Okayama City, Japan 
 
TECHNICAL PROGRAM  
                                                                                          
2009 IEEE ICNSC 2009 IEEE ICNSC 
Thursday, March 26, 2009 Friday, March 27, 2009 
                                                                                          
09:00 - 10:00               Special Lecture Room  10:00 - 17:30   
Registration Plenary Speech 1 (PS01) 
                                             Sampled-Data Control and Its Applications to Digital 
Signal Processing: Beyond the Shannon Paradigm 18:30 - 20:00 
Welcome Reception 
Yutaka Yamamoto                                              
Kyoto Univ., Japan 
                                             End of the 1st Day 
                                             
10:00 - 10:20     Break  
                                               
Morning Technical Sessions  
10:20-12:00, Friday, March 27, 2009  
                                              
 FriA01                       No. 2 Lecture Room 
 Control of Networks I 
 Lecture Session 
 Chair: Tsu-Tian Lee   National Taipei Univ. of Tech., Taiwan 
 Co-Chair: Shaoyuan Li     Shanghai Jiao Tong Univ., China 
                                              
10:20-10:40                               FriA01-01 
 
Improved Backoff algorithm for IEEE 802.11 Networks 
 
  Qassim Nasir             Univ. of Sharjah, United Arab Emirates 
 
  Maali Albalt             Univ. of Sharjah, United Arab Emirates 
 
10:40-11:00                                       FriA01-02 
 
Designing A New Lyapunov-based Congestion Controller for 
Time-Delayed Differentiated- Services Framework 
 
 
  R. Vahidnia                     Univ. of Tarbiat Modares, Iran 
 
  H. Firouzi                       Univ. of Tarbiat Modares, Iran 
 
M. H. Beheshti                   Univ. of Tarbiat Modares, Iran 
 
11:00-11:20                                       FriA01-03 
 
Predictive Multirate Control with Random Network-Induced Delays 
 
  Yuanyuan Zou                 Shanghai Jiao Tong Univ., China 
 
  Tongwen Chen                       Univ. of Alberta, Canada 
 
  Shaoyuan Li                   Shanghai Jiao Tong Univ., China 
 
11:20-11:40                                       FriA01-04 
 
Wireless Joystick Control for Human Adaptive Mechatronics  
  
Based on New Relaxed Conditions 
  Xiangpeng Xie                  Northeastern University, China 
  Huaguang Zhang                Northeastern University, China 
  Derong Liu                      Chinese Academy of Sciences 
10:40-11:00                                      SunA05-03 
Fractal Features for Cardiac Arrhythmias Recognition Using Neural  
Network Based Classifier 
  Chia-Hung Lin                       Kao-Yuan Univ., Taiwan 
  Chao-Lin Kuo                         Far-East Univ., Taiwan 
  Jian-Liung Chen                      Kao-Yuan Univ., Taiwan 
  Wei-Der Chang                         Shu-Te Univ., Taiwan 
11:00-11:20                                      SunA05-04 
Model Study of Transformer Fault Diagnosis Based on Principal  
Component Analysis and Neural Network 
  Zhengwei Zhu                 Jiangsu Polytechnic Univ., China 
  Zhenghua Ma                 Jiangsu Polytechnic Univ., China 
  Zhenghong Wang              Jiangsu Polytechnic Univ., China 
  Jianming Jiang                Jiangsu Polytechnic Univ., China 
                                              
12:00 - 13:00    Closing Reception (Muscat Union) 
                                             
 
End of the Conference 
                                             
 
 
 
 
Fig. 1. The framework of the proposed NFCM. 
 
intention of the proposed model is to provide a framework for 
learning algorithms, to be interpretable in form of linguistic 
rules, and to be able to use prior rule-based knowledge. The 
proposed NFCM contains two networks and each network 
has four layers: one input layer, two if-then rule layers, and 
one output layer. The layers are characterized by the fuzzy 
operations which are fuzzification, rule base and 
defuzzification. For learning classification rules from the 
database, we utilize the proposed NFCM for data analysis 
problem. The architecture of the NFCM is depicted in Figure 
1, where the first network is the feature extraction unit, and 
the second network is the inference unit. Suppose we have an 
M-class classification problem to be solved. Every original 
feature variable pix  ( ni   ...,  ,1= ) reflects some important 
property of a datum and each datum should be classified as 
one of the C classes. In general, C<n and C<<m. For the 
feature extraction unit network, the form of the rules has n 
inputs and M outputs 
 
If pix  is isA , then 1 1 2[   ... ]i i iMw w w w= ,  
1,  2,  ...,  p m=  (1) 
where n  is the number of original feature variables, m is 
number of original data, isA  is the antecedent fuzzy set of the 
ith feature variable for sth linguistic label, and 1w  is the 
weight of the feature extraction unit. The outputs of this 
network, the dimensions of the original feature variables will 
be reduced from n  to C . Because, the number of extracted 
new feature variables is the same as the number of classes. 
Next, we use the new feature variables pN  that reduce the 
dimension from the feature extraction unit to construct the 
rules of the inference unit. The form of rules being written as 
If 1pN  is 1jB  and 2pN  is 2jB  …and pCN  is jCB , 
then pN  belongs to Class T with 2Cw .             (2) 
 
Where 1jB , …, jCB  are the antecedent fuzzy sets, Class T 
represents one of the C classes, and 2Cw  is the corresponding 
weight of the second network. The parameter 2Cw  is also 
viewed as the grade of certainty that the input pattern belongs 
to class T. In order to improve the performance of the 
proposed  
1L ay erw
2L ayerw
3L ayerw
4L a yer
ijw
 Fig. 2. The feature extraction unit 
 
classification model, the evolution programming will be 
employed to tune the position of the membership function for 
the feature extraction unit. We will address all these parts in 
detail in the following sections. 
III. THE NETWORK OF THE FEATURE EXTRACTION UNIT  
The network of the feature extraction unit contains fours 
layers. In the input layer, each neuro connects to each element 
of an n-dimensional input vector. The if-then rules layers 
separate into two parts - an antecedent part and a consequent 
part. The antecedent and consequent construct a fuzzy if- then 
rule for the inference system. Triangular membership 
functions are employed at the membership nodes. In the 
defuzzification layer, neuros are connected using the 
link Layerw . The framework is showed in Figure 2. 
A. Layer 1: input layer 
The input to NFCM is a non-fuzzy data vector, represented 
by 1 2[ , ,  , ]p p p pnx x x x= … . The net input to the ith node of layer1 
is pix . The corresponding weight of layer 1 is 1Layerw  and 
equals 1. The output from the corresponding node is 
1 1Layer
i pi if x w= ⋅  , 1,  2,  ...,  i n=     (3) 
B.  Layer 2: fuzzification layer 
In this layer, each feature variable is divided into iP  fuzzy 
partitions by the fuzzy grids. The membership functions can 
be represented by the triangular function. The number of 
nodes on this layer is 
1
n
i
i
P
=
∑ , and nodes in this layer are named 
as linguistic nodes. The linguistic nodes will directly transmit 
the non-fuzzy inputs from the layer 1, so the 2Layerw  are 
defined 1. The output from the corresponding node is 
2 2max(1 ,  0)
fK
pi j Layer
ij
x a
f w
b
−
= − ⋅      (4) 
where 2
1i
b
P
=
−
, 1 ( 1)iPja j b= − + − , 1,  2, ..., i n=  and 
1,  2, ..., ij P= ; iP  presents the number of partition in the ith 
feature variable. Each node receives only one input as one 
dimension of the data vector to the whole network, and output 
to iP  nodes of the next layer. 
919
 
 
 
7  7 6
a{max( ) the consquence part of rule R }
Layer
h a af w f h= ⋅ ∈  (13) 
D.  Layer 8: the classified result layer 
A defuzzification process is applied in this layer, where the 
output of this layer is determined based on the crisp 
classification result. The number of the output node depends 
on the number of the class. The output node is determined by 
the maximum value that figures out from the output of the 
layer 7 and defined as (14). The weight 8Layerhw  equals to one. 
8 8 7arg( . )Layerh hf w f=  (14) 
V. THE EVOLUTIONARY PROGRAMMING AND WEIGHT 
REVISED ALGORITHM 
In this section, we will introduce EP to search the position 
of the membership function for the feature extraction unit and 
WRA to modify the weight of the inference unit. 
A. EP 
The EP is a global search technique that is proposed to 
solve the optimization problem. EP simulates evolution at the 
phenotypic level and emphasizes behavioral evolution rather 
than genetic evolution. Based on this reason, EP does not rely 
on any crossover operators. Because, the basic concerns of 
the classification problem are the high classification rate and 
the smaller rule base, but it seems difficult to consider both 
requirements at the same time. To make the proposed NFCM 
effective, we employ the EP that are based on the mechanics 
of natural selection and natural genetic to solve both 
problems. Using the EP, the optimization problem should be 
transformed into an adequate function, which is called the 
objective function. The length of a chromosome equals to the 
partition number of each initial feature variable. Thus, each 
chromosome string corresponds to the position of the fuzzy 
membership function. In summary, the process of EP has 
been described in the following. 
1) Define the length of a chromosome:  
We use the binary code to describe the position of the 
initial feature variables that contain 8 bits ( 1 2 8,  ,  ..., A A A ). 
The maximum number of fuzzy subsets for every 
feature is 8, the peaks are uniformly distributed by [-1, 
1]. Suppose a chromosome of one feature universe is 
coded as [1 1 0 0 1 0 1 0], then the corresponding 
membership functions are shown in Figure 4.The 
length of a chromosome is 8 n⋅  and the number of the 
populations is 50. 
9
7
− 9
5
−
9
3
−
9
1
−
9
1
9
3
9
5
9
7
 
Fig. 4. Membership function of the code [0 1 0 1 0 1 1 0] 
. 
2) Define the objective function:  
In general, the classification rate of the classification 
problem is more significant than its rule numbers, so 
we define the objective function contains the 
classification rate and sum of the partition number for 
each feature variable.  
Objective Function= (Classification rate)
     (Number of rules)
α
β
⋅
− ⋅
  (15) 
where α  is greater than or equal to the number of 
data, and 0 1β< < . The intention of EP for our 
classification model is to search the best classification 
results that possess the highest classification rate and 
the fewest rule number.  
3) Mutation:  
When a gene is selected for the mutation operation, its 
binary code is changed to its compliment. By using 
the mutation process the 50 populations are generated. 
After the mutation, the number of populations 
becomes 100. The mutation rate is set 0.5 in the 
manuscript. 
4) Selection of next population:  
To keep the superiority of the population, we 
calculate the fitness value based on (15). Then, the 
fittest performance 50% are kept, and the others are 
killed off to maintain the original population size. 
After this EP the position of the fuzzy membership 
function for the feature extraction unit is redeveloped, and 
then the testing phase will be re-executed to evaluate the 
performance of the NFCM. The detailed discussions will be 
presented in Section 6.  
B. WAR 
Besides the weightings of Layer 3 and Layer 7 that are 
calculated by the training data, the others are set to be 1. Now, 
we propose a tuning algorithm that is called WRA that is 
according to the rule (node) that can not infer the correctly 
class. We propose two aspects to produce and revise the 
weight for the particular node which has the critical ability to 
infer the final classified result. The output node of the layer 6 
has one weight to link with the layer 7; we utilize (16) to 
produce an additional weight  7Layersw  that the initial value is 0. 
Because the 8Layerw  equals 1, the inferred result is determined 
by 7hf . 
7Layer
sw =
7Layer
sw +
7 7 2( )f fα βγ −  (16) 
where 7fα  is equivalent to 7hf  and makes the incorrect 
results, γ  is a small positive constant. And 7fβ  possesses the  
capability of correct classification result, however, 7fα  is 
bigger than 7fβ . The dotted line on the Fig. 5 is shown the (16) 
produce an additional weight. On the other hand, the 
formulation (17) is also employed to revise the weight of the 
layer 6. In theoretical, we expect 7fα should smaller than 7fβ . 
We can use (17) to slightly increase the weight 7Layerwβ . 
 
921
