 2 
行政院國家科學委員會專題研究計畫成果報告 
 
以影像特徵擷取為基礎的相機位置估測方法之設計與實現 
 
計畫編號：NSC 99-2221-E-269 -020 
執行期限：99 年 08 月 01 日至 100 年 07 月 31 日 
主持人：夏郭賢  遠東科技大學資訊管理系副教授 
 
一、中文摘要 
基於無人飛行載具的空拍應用與輔助降落的
研究上，相機位置的紀錄是一項重要的主題。本研
究的主要目的，是對相機位置與影像間進行投影幾
何的分析，找出與相機位置相關的影像特徵，並以
影像特徵擷取的技巧，擷取相關特徵。同時以推算
法建立影像特徵資料庫，並以智慧型學習法則進行
資料庫學習，作為快速估測的基礎。 
本研究計畫所定的研究範圍，設定為圓形目標
物出現在畫面特定區域的情況，並以此條件設計影
像特徵擷取演算法與相機位置估測演算法，用以模
擬旋翼型無人飛行載具接近停機坪時的影像定位。 
然而在某些情形下，資料庫將會很大而不利於
學習，所以我們運用資料分割與分類法則來加以簡
化，以加速資料庫學習。此外，本計畫自行設計並
建立一套附有尺規的影像測試平台，以驗證其準確
性。  
在本研究計畫中，我們設計了模擬無人飛行載
具接近地面目標物的影像測試平台，用以驗證演算
法，並搭配地面端監控電腦的顯示與資料儲存功
能，驗證演算法的正確性。未來希望以本研究成
果，可以輔助進行無人飛行載具之特定區域空拍或
自主安全降落。 
關鍵字：影像特徵擷取、智慧型學習、資料分類、
數位影像處理 
 
Abstract 
Computer vision is a very useful and important 
technique for aerial photography and applications of 
UAV (Unmanned aerial vehicle), e.g., auxiliary 
landing. The objective of this project is to estimate 
the position of the UAV for landing. The most 
common landing mark for helicopter is usually 
marked with a circle and/or a letter "H", so as to be 
visible from the air. In this project, the landing mark 
considered is a simple circle, which is the outer shape 
of the commonly used landing mark, for simplicity. 
Its image on the captured picture will be a deformed 
circle which is like an ellipse. The objective of this 
project is to determine the position of the camera by 
an image of the circle. 
In this work, the projective geometry analyzing 
method is proposed for finding out the relationship 
between images and camera positions in the 
particular range. Base on projective geometry 
analyzing method, the image feature database has 
been constructed, and the intelligent learning 
methods are developed for learning the I/O mapping 
of the database. Since the amount of the training 
database is so huge that the training time is 
intolerably long, the data pre-processing procedure is 
proposed for easing the database analysis. On the 
other hand, the image feature extraction algorithm is 
proposed for extracting the important features. The 
camera position estimation procedure has been 
verified on the specially designed plane for 
corroborating its capability. 
Also we have designed an image-base camera 
position system for UAV. The receiver and ground 
station receive the image data and memorize the 
image features. The proposed algorithms in this 
project are coded into the system for UAV position 
estimation. 
By the proposed algorithms, there are more 
information sensed for UAV controlling. When the 
UAV carries out the task of aerial photography or 
landing, it will be much easier. 
Keywords ： Image feature extraction, intelligent 
learning method, data pre-processing procedure, 
digital image processing. 
 
二、前言 
 近年來電腦視覺(computer vision)的研究十分
興盛，由於應用層面十分廣泛，不論是空拍的影像
接圖[1]、監控系統[2]、各式輔具開發、工業檢測
[3]、機器人應用[4]或無人飛行載具(UAV)[5][6][7]
上都有應用成功的例子。以[4]與[6]為例，機器人
與無人飛行載具的開發需要許多型式的感測器來
感測周圍環境，以作為控制器的控制參考。一般常
見且較為低價位的光學感測器(如紅外線感測器)
 4 
個點，其間存在著一齊次轉換關係，如式(1)： 



























3
2
1
333231
232221
131211
'
3
'
2
'
1
x
x
x
hhh
hhh
hhh
x
x
x
 (1) 
式(1)可進一步表示為 HXX ' ，其中 H 為線性齊
次且非奇異的矩陣。所以，在一平面的任意點，存
在著一個線性的投影轉換矩陣 H ，投影到另一特
定的平面上。由於投影轉換矩陣 H 是描述二特定
平面間的轉換關係，所以對於不同平面間的投影轉
換，會有不同的矩陣 H 。此現象解釋了不同的相
機位置會拍攝到不同的影像，同時也說明了平面投
影轉換的唯一性。 
 
5.2 目標物影像落在畫面中心特徵分析與影像特
徵資料庫建立 
5.2.1 影像特徵分析 
在相機存在一個傾角的狀況下，所拍攝到影像
會形成一個變形的橢圓。經圖1的影像特徵分析，
可以歸納出演算法1。 
H
a
b
D
C
E
φ
σ
ρ
θ
μ
U V
G
O
R
 
圖1 相機位置與目標物的幾何投影 
 
演算法1：求得目標影像特徵之演算法 
 
 
5.2.2 影像特徵分析影像特徵資料庫建立 
以類神經網路[12]對前面的演算法所產生之
資料庫進行學習，可以得到相機位置與影像間的轉
換關係。為了使學習誤差最小化，本計畫將採用誤
差倒傳遞學習法則（ Error back propagation, 
EBP）。EBP為一種監督式的學習法則，利用已知
的樣本作為網路輸入，可以達到準確的結果。 
 
 
5.2.3 影像特徵擷取演算法設計 
    影像特徵擷取法的設計，是為了擷取影像中有
用的資訊。在圖1中，與相機位置有關的影像特徵
為 a、b、 r、d 。圖2則是說明影像處理與影像特
徵擷取的流程。 
 
Connected components 
labeling algorithm
Basic rectangle 
computing
Major axis and minor 
axis computing
Image Data input






d
r
r
a
b
a
Feature extraction
Thresholding
Filtering
Image processing
The  ratios 
of the Target  
 
圖2 影像處理與影像特徵擷取流程 
 
5.2.4 實驗構想 
以單張照片作為影像輸入，並於 PC 上進行影
像特徵擷取演算，取出相關的影像特徵。同時利用
本例的實驗模擬流程如圖 3 所示，分為二部分：1. 
以 Database 訓練類神經網路。2. 將影像信號經由
影像特徵擷取演算法運算，擷取出必要的特徵值作
為類神經網路的輸入，進而估測出一組可能的相機
位置。 
 
Neural network training Neural network 






'
'
'
'
'
'
d
r
r
a
b
a
 '' R
Image  processing
Feature extraction Training/checking 
data
 intput data
Output data
 R  '' R
Position of the camera
Image Data input
{- - -}a  a  r
b  r  d 
{- - -}a    r
b  r  d 
 
圖3 實驗構想與進行流程 
 
5.3 目標物影像落在畫面中心垂線上特徵分析與影
像特徵資料庫建立 
  
5.3.1 影像與相機位置之分析 
    在相機存在一個傾角的狀況下，所拍攝到影像
會形成一個變形的類橢圓。然而在實際的例子中，
影像很少會剛好出現在畫面的中心。如圖4(a)與圖
4(b)所示，實際拍攝到的圓形目標物會呈現橢圓的
變形，由前述的投影幾何分析，可以得到如圖5與
圖6的攝影機位置與影像投影的關係，同時可以將
此投影關係整理為演算法2。觀察圖5與圖6，可以
發現這時候情況變得複雜許多，這是因為條件放寬
的緣故。由圖6(b)中取出相對應的影像特徵有 h、
k 、Q、 r 、 d ，並定義Ratioproject= kh ，Ratioheight 
= dr ，Ratioangle= rh 與Ratioposition= k Q四個特徵，其
 6 
 
Data stage 1 Data stage k
Data related to R Data related to θ Data related to A 
 Data base
Data Class2Data Class 1
Correlation of the 
I/O  is low
Data Class nData Class (n-1)
ANFIS 2ANFIS 1 ANFIS nANFIS (n-1)
Correlation of the 
I/O  is high
Data segmentation
Data classification
 
圖7 資料分割與資料分類流程 
 
Step(4) 對二值影像進行連通標記  (Connected 
components labeling algorithm)，標記出需處理的目
標物影像。 
Step(5) 框出基本矩形，同時計算主副軸及其交點。 
Step(6) 計算 Ratioproject= kh ，Ratioheight = dr ，
Ratioangle= rh 與 Ratioposition= k Q。 
 
Connected components 
labeling algorithm
Major axis and minor 
axis computing
Feature extraction
Image Data input
Thresholding & Filtering Image processing
Binary image
Basic rectangle 
computing
Multiple objects
Image Features
Y
N
 
圖 8 影像處理與影像特徵擷取流程 
 
5.4 實驗結果與討論 
5.4.1 相機校正結果 
    為了確定相機的光軸是精確地對準降落標記
的中心，所以研究中所使用的webcam需要經過相
機校正的步驟。一般相機的針孔模型可以利用相機
內部參數與外部參數來描述，內部參數可以定義為
K (稱為四參數模型) 如(2)所表示，內部參數P如(3)
所表示。 











1
yy
xx
pf
pf
K  (2) 
P= [R| t ] (3) 
其中R為旋轉矩陣，t為傳送矩陣。至於P的求解，
在本研究中是以 Zhang所提出的校正方法進行相
機校正，其校正結果如表一所示。如此可以確定，
光軸通過影像的座標為(293.7, 237.91)。確定了光
軸的方向，有助於減少影像特徵擷取時所出現的誤
差。 
 
表一、相機校正結果 
Focal Length 
Webcam 
fx fy 
688.92 690.84 
Principal point 293.70 237.91 
 
5.4.2 目標物影像落在畫面中心特徵分析與影像特
徵資料庫建立之實驗結果 
實際影像與相機位置相對應的特徵值如圖 1
所示。在影像特徵擷取演算法的設計上，可以透過
對目標物圈取“基本矩形”的方法，快速的取得。
同時利用演算法 1 可以計算出在拍攝位置 20 ~ 
300CM，角度在89~89 度間所有的影像特徵變
化，並同時定義 Ratioproject = a / b, Ratioheight = r / d, 
與 Ratioangle = a / r 等三個與相機位置相關的特徵
比例，作為智慧型學習法則的資料庫。在本研究計
畫中所提出的二種智慧型學習法則分別為類神經
網路與 ANFIS。 
類神經網路估測 
對於類神經網路的設計，為了使學習誤差最小
化，本研究採用誤差倒傳遞學習法則（EBP）。EBP
為一種監督式的學習法則，利用已知的樣本作為網
路輸入，可以達到準確的結果。類神經網路學習輸
出與其訓練誤差，分別如圖 9 與圖 10 所示，除了
攝影機角度位於 89度與 89度兩個極端的位置附
近的訓練效果較差外，其餘的資料訓量誤差皆在
1.0 附近，訓練效果不錯。學習誤差如圖 10，圖
11 為學習歷程的均方差。由圖 11 顯示，均方差也
在疊代訓練 1000 次後可達近似最佳。 
    實驗結果顯示於表二，本實驗共取出 12 組實
驗數據，可以發現誤差都小於 0.1 度，推估之準確
度很高，表示演算法的設計是正確無誤的，不但可
以經由影像處理的技巧，取出影像的特徵長度的比
例，而類神經網路的學習效果也不錯。 
 
 8 
 
(a-1) Results of checking data for ANFIS for . 
 
 
(a-2) Results of estimating data for ANFIS for  
 
 
(b-1) Results of checking data for ANFIS for R. 
 
 
(b-2) Results of estimating data for ANFIS for R. 
圖 13 ANFIS 訓練與測試結果 
 
 
由上述二種方法所估測的結果可知，估測誤差
都在足夠小的範圍內，然而在計算時間上，由於
ANFIS的計算時間遠小於類神經網路，所以ANFIS
相對之下是更適合本研究計畫的目標的。 
 
5.4.3 目標物影像落在畫面中心垂線上特徵分析與
影像特徵資料庫建立之實驗結果 
經由演算法2建構完成的資料庫十分龐大，資料數
高達數十萬筆，將需要很久的訓練時間，所以先將
資料做前處理，將關聯度高與關聯度低的資料分別
做訓練，則可以節省大量的訓練時間。其訓練結果
以“height 126~130cm” “θ= 62° to 62°”此區間資料
作展示，其ANFIS訓練結果如圖14所示。訓練完成
後所產生的從屬函數如表三所示，關連度較高的資
料則以數量較少的從屬函數可以獲得不錯的學習
效果，反之關聯度低的資料，則需要數量較多的從
屬函數。光軸在地面投影點到降落標記的距離訓練
無人載具到降落標記的距離與光軸與地面的夾角
的RMSE，分別為 0.641, 0.124 與 0.365。圖15為
影像處理的結果，利用基本矩形所框出的目標物，
表三與表四所示則是以圖15為例所估測的結果。 
基於上述的分析與模擬結果顯示，研究中所提
出以智慧型學習法則來學習 2D 影像與 3D 空間中
的 mapping 關係，可以由單一的影像中取出目標物
特徵，並快速的估測出一個誤差在可接受的範圍內
的近似拍攝位置。 
 
 
(a) 光軸在地面投影點到降落標記的距離 
 
 
(b) 無人載具到降落標記的距離 
 
 
(c) 光軸與地面的夾角 
圖 14 ANFIS 訓練結果 
 
   
(a) (b) (c) 
圖 15 影像處理的結果。 (a) Image input (b) Binary 
image (c) Basic rectangle of image 
 
表三、圖 15 中的 Image features  
 Ratioproject Ratioheight Ratioangle Ratioposition 
Actually 
Image 
Ratios 
1.1092 1.0247 8.9016 0.0817 
 10 
tracking of constrained robot manipulators, 
American Control Conference, pp.3694-3700. 
[5] 陳俊志，2006，無人飛行載具地面移動目標影
像自動鎖定追蹤系統之研發，碩士論文，國立
成功大學航空太空工程學系，台南，台灣。 
[6] 蘇仲鵬、黃柏凱、張政雄、李紀瑩，2009，自
主無人直升機地面導控與監測系統的設計
與實現，2009 中華民國航空太空學會/中華民
用航空學會聯合學術研討會，1-8 頁，台北市。 
[7] 蘇仲鵬、李偉君、陳逸強、連紹帆，2008，應
用於自主飛行無人直升機垂直爬升與降落
的影像處理技術，中華民過第五十屆航太學
會學術研討會，台北，台灣。 
[8] 李志傑、陳傳生，2006，無人直昇機停懸穩定
控制系統，碩士論文，元智大學機械工程學
系，中壢，台灣。 
[9] J.M. Roberts, P.I Corke and G. Buskey, 2002, 
“Low-Cost Flight Control System for a Small 
Autonomous Helicopter”, Australasian 
Conference on Robotics and Automation, 
pp.27-29. 
[10] S.B. Kim, S.Y. Lee, J.H. Choi, K.H. Choi and 
B.T. Jang, 2003, “A Bimodal Approach for GPS 
and IMU Integration for Land Vehicle 
Applications”, IEEE Vehicular Technology 
Conference, vol. 4, pp.2750-2753. 
[11] R. Hartley and A. Zisserman, 2003, Multiple 
View Geometry in computer vision, Cambridge 
University Press. 
[12] M.T. Hagan, H.B. Demuth and M. Beale, 2004, 
Neural Network Design, Thomson. 
[13] J.S.R. Jang, 1993, ANFIS: Adaptive network 
based fuzzy inference system, IEEE Trans. On 
System, Man and Cybernetics, Vol. 23, no. 3, 
pp.665-685, May/June. 
 
附錄： 
本計畫衍生著作清單： 
研討會論文： 
1. Lien, S.F., K.H. Hsia and J.P. Su (2010). 
Image-Guided Height Estimation for Unmanned 
Helicopter Landing, Third International 
Symposium on Intelligent Informatics, Dalian 
City, China. (2010.9.5-7, BoHai Pearl Hotel 
Dalian, Dalian, China) 
2. Hsia, K.H., S.F. Lien, and J.P. Su (2010). Camera 
Position Estimation by ANFIS from Incomplete 
Landmark Image, The 18th National Conference 
on Fuzzy Theory and Its Applications, Hualien, 
Taiwan, pp. 67-72. (2010.12.3-4, 東華大學) 
3. Hsia, K.H., S.F. Lien, C.C. Wang, T.N. Lee, and 
J. P. Su (2011).  Camera Position Estimation 
and Feature Extraction from Incomplete Image of 
Landmark, Proceeding of International The 
Sixteenth Symposium on Artificial Life and 
Robotics (AROB 16th '11), Oita, Japan, pp. 
277-280. (2011.1.27-29, B-Con Plaza, Beppu, 
Oita, Japan) 
期刊論文： 
1. Hsia, K.H., S.F. Lien, C.C. Wang, T.N. Lee, and 
J. P. Su (2011).  Camera Position Estimation 
and Feature Extraction from Incomplete Image of 
Landmark, Artificial Life and Robotics, Vol. 16, 
No. X, pp.XX-XX. (To appear) 
專書專章： 
1. Hsia, K.H., S.F. Lien and J.P. Su (2011). Height 
Estimation via Stereo Vision System for 
Unmanned Helicopter Autonomous Landing, in 
Machine Vision (ISBN 979-953-307-677-4), 
Fabio Solari ed., InTech - Open Access Publisher. 
 
 
of Residual Disability (低風險的微創手術癲癎治療)。山川教授是九州工業大學大
學院生命體工學研究科腦情報專攻的特聘教授，也是日本 Fuzzy Logic Systems 
Institute (FLSI) 的 Chairman，他同時也是 IEEE 的 Fellow，具有相當崇高的學術
地位。山川教授除了學術地位崇高之外，也是空手道黑帶五段的功夫高手，也能
彈奏三弦琴和尺八等日本的傳統樂器，真是值得讓人敬仰。在山川教授的演講中
提到，癲癎是一種會反復發作的慢性腦部疾病，全世界約有 6 千 8 百萬個癲癎患
者，其中 80%可以以藥物控制而能有良好的生活品質，但還是有 1360 萬左右的
癲癎患者因為得面對癲癎的突然發作而使得生活作息無法正常，解決的方法就是
要以手術除去所謂的『致癇區』。但要正確地找到這個致癇區是不太容易的，找
到後要在不損傷正常腦部的狀況下去除這個致癇區也是相當地困難。山川教授的
研究團隊在獨立行政法人日本學術振興會 (Japan Society for the Promotion of 
Science, JSPS) 的支持下從 2008 年八月開始進行致癇區定位這一方面的研究，這
個計畫將持續到 2012 年三月。在此次的專題演講中，山川教授發表了目前的研
究成果。 
第二場專題演講是在 9 月 7 日上午 8:50~9:50 進行，由大連理工大學管理學
院的楊德禮教授主講，大连理工大學信息安全研究中心常務副主任孔祥維教授主
持，講題是『Internet of Things: Next Generation of Internet?』，屬於 ISIKM2010
的專題演講。在這場演講中，楊教授介紹了網際網路的過去、現在及未來的可能
發展。我們所發表的論文在 6 日下午的第一個場次，在四樓的 D 場地進行，共
有六篇論文在那個場次中發表，場次的主題是：智慧型系統。本人上台宣讀的論
文基本上是延續先前的研究，這次是利用兩個鏡頭得到直升機降落標記影像的像
差來協助無人直升機自主降落時的高度與位置之判斷，為本人今年國科會專題研
究計畫的部份成果，不過因最終定前稿，國科會計畫仍未核定，所以無法在論文
中致上謝忱。一般而言，無人直升機都是透過 GPS 來進行定位，利用 IMU 來輔
助定位的修正。不過，GPS 的誤差通過都在數公尺以上，高度誤差會比水平誤
差要來得更大，這對無人直升機要進行自主降落而言，是相當危險的。在我們的
方法中，利用兩個簡單的 WEBCAM 組合而成的機構，花費總共不過數百元，在
無人直升機自主降落的關鍵 6 公尺之內的高度範圍，所量得的高度之誤差均不到
10 公分，具有相當不錯的實用價值。 
二、與會心得 
在這次的研討會中，個人對於同場次由台灣師大陳美勇教授的文章
『Reconstruction of a 3-D Object Model Using 2D Image Contours Data』感到高度
的興趣。在陳教授的論文中，他用兩個 CCD 照相機以距離簡單結構的被測物等
距的前視與側視圖，即可估測出這個被測物的立體外框結構。陳教授的研究與我
目前所進行的研究，在某些方面有異曲同工之妙。 
本次赴大陸參加研討會，除了正規的行程之外，順道參訪了中國的一些古蹟
及山川風光，收穫相當豐碩。特別是能和一些教授學者交換研究心得，感覺對日
後的研究必將有所助益。 
三、考察參觀活動(無是項活動者略) 
 
夏郭賢  
寄件者: "Shao-Fan Lien" <g9510801@yuntech.edu.tw>
日期: 2010年5月3日 上午 09:06
收件者: "夏郭教務長" <khhsia@cc.feu.edu.tw>; "蘇仲鵬教授" <sujp@yuntech.edu.tw>
主旨: Fwd: Acceptance Letter: ISII2010-283
b第 1 頁 (共 3 頁)(B)
2011/9/1(B)b
 
 
---------- Forwarded message ---------- 
From: Yan SHI <yshi@ktmail.tokai-u.jp> 
Date: 2010/5/1 
Subject: Acceptance Letter: ISII2010-283 
To: g9510801@yuntech.edu.tw 
Cc: isii2010 <isii2010@ijicic.org> 
 
 
Dear Dr. Shao-Fan Lien, 
  
It is our pleasure to inform you that your paper, 
  
Reference No.: ISII2010-283 
Title: Image-Guided Height Estimation for Unmanned Helicopter Landing 
Author(s): Shao-Fan Lien, Kuo-Hsien Hsia and Juhng-Perng Su 
  
submitted to The Third International Symposium on Intelligent Informatics (ISII2010), which will be held
on September 5-7, 2010, in Dalian, China, has been accepted for an oral presentation at the symposium, and
for the publication in ICIC Express Letters (ICIC-EL) with the conditions described below. Please prepare your
paper  (No  more  than 6  pages)  based  on  the  Acceptance  Conditions  and  ICIC  Express  Letters  style
(download from http://www.ijicic.org/icicel.htm) for publication. 
  
----------------------------------------------------------- 
Acceptance Conditions: 
  
1)  The motivation on the study should  be further  emphasized.  The key features  of  the results  obtained
compared with some existing ones should be clearly explained in the Introduction section.  
  
  
(1)      Based on different methods, Figure 6 and Figure 7 present the estimation error curves for different baseline. 
From these figures, the range of measurement distance of BMA is much longer than that of other method.  
(2)      In the second line below Figure 7, the BMA could find corresponding points quickly and effectively within 7.5 
meters. But, the word “quickly” has not been proven in the experiments. 
(4)   Is GPS 18‐5Hz a correct type number for Garmin GPS? Maybe it is named GPS 18 5Hz, which frequency is 5Hz. 
(5)   Some grammar mistakes should be corrected in this paper. For example, in section 5 experimental results, “…,
Fig. 4 show that …” should be “… shows that …”.  
(6)   The section number of “Conclusion and Discussion” is 6, not 5. 
  
In abstract, the authors describe that the proposed image‐guided estimation method can provide accurate height 
information for amending the error of GPS. It seems to me that the proposed image‐guided method is employed to 
estimate the height information within a short distance instead of being utilized to correct the error of GPS.  The 
purpose and key feature of the proposed method in this paper need to be further explained. 
  
  
2) English should be substantially improved. The authors should carefully and thoroughly check the paper,
and correct all the grammar and composition mistakes in the final version.  
  
3)  Please  cite  some  recently  published  papers  in  IJICIC  (www.ijicic.org  )  or  ICIC-EL
(http://www.ijicic.org/icicel.htm) in your paper, which is related to the topic in the paper, and this will promote
your work in wider community and have a positive impact on the journal.  
  
  
  
Kind regards, 
  
  
Ms. Satomi Kojima 
  
On behalf of  
Dr. Yan SHI  
Organizing Committee Chair, ISII2010 
Professor, School of Industrial Engineering 
Tokai University  
9-1-1, Toroku, Kumamoto 862-8652, Japan  
Tel. & Fax: +81-96-386-2666  
E-mail: isii2010@ijicic.org 
  
Dr. Xiangwei Kong 
Program Committee Chair, ISII2010 
Professor, School of Electronic and Information Engineering 
Dalian University of Technology 
No.2, Linggong Rd., Dalian, China 
Tel.: +81-411-84706368 
  
  
  
  
 
b第 3 頁 (共 3 頁)(B)
2011/9/1(B)b


ICIC Express Letters ICIC Internationalⓒ2009 ISSN 1881-803X
Volume 3, Number 3, September 2009 pp. 1–6
Image-Guided Height Estimation for Unmanned Helicopter Landing
Shao-Fan Lien1, Kuo-Hsien Hsia2 and Juhng-Perng Su 3
1Graduate School of Engineering Science and Technology
3Department of Electrical Engineering
National Yunlin University of Science and Technology
No. 123, University Road, Section 3, Douliou, Yunlin 64002, Taiwan
{ g9510801; sujp }@yuntech.edu.tw
2Department of Management Information System
Far East University
No. 49, Chung-Hwa Road, Hsin-Shih, Tainan 744, Taiwan
khhsia@cc.feu.edu.tw
Received April 20xx; accepted June 20xx
ABSTRACT. Auto landing control is a very important control mode of unmanned helicopter.
Global positioning system (GPS) is the typical position sensor for landing control.
However, the height measurement error of GPS is so large that it is not accurate enough
for helicopter landing. In this paper, we propose a stereo vision system composed with
two webcams to provide more accurate height information in certain range of height. The
block matching algorithm (BMA) is proposed for searching corresponding points. From
the experimental results, BMA can authentically search the corresponding point in low
resolution images and can provide an acceptable height estimation result in a reasonable
range.
Keywords: Stereo vision, Block matching algorithm, Height estimation
1. Introduction. During the past few years, the development of the unmanned helicopter
has been an important subject of research. The unmanned helicopter is a highly nonlinear
system. Therefore many researchers focus on the dynamic control problems (e.g. [1-3]).
The most important flight mode of autonomous unmanned helicopter is the landing mode.
In consideration of the unmanned helicopter landing problem, the height position
information is usually provided by global positioning system (GPS). However, the error of
GPS height position is usually in the range of ±10 meters, and it is too large for helicopter
landing. For example, the accuracy of Garmin GPS 18-5Hz is less than 15 meters [4]. After
many times of measurement, the average error of this GPS was obtained to be around 10
meters. Several landing guided methods were brought up for correcting the error range. In
[5], tether guided method for autonomous helicopter landing was considered. In [6-7],
camera-image based relative pose and motion estimation for unmanned helicopter were
discussed.
In this paper, we focus on the problem of estimating the height of the helicopter for the
landing problem via a simple stereo vision system. The key problem of stereo vision system
is to find the corresponding points in the left image and the right image. The block
we will apply the block matching algorithm (BMA) for searching the corresponding points.
BMA is a standard technique for encoding motion in video sequences [8]. It aims at
detecting the similar block between two images. The matching efficiency depends on the
choosing of block size and search region. Usually, bigger blocks are less sensitive to the
noise but will spent more computation time. The sum of absolute difference (SAD) is used
for the block similarity measuring:
SAD( x , y, r , s )=      



 

mi
i
nj
j jsyirxjyix0 0 ,),(
L_ImageR_Image , (3)
where m and n are the length and width of the block, (x, y) is the poison of the block on
right image and (r, s) denotes the motion vector. In Fig. 2, the first pixel of the chosen
block on left image is (x, y) and the block size is N×N. The search region on the right image
is defined to be a rectangular with the image width as the width and height of 2k. Since the
left and right cameras of the stereo vision system are placed on the same line, the k could be
small in order to reduce the computation time.
FIGURE 2. Block template and the search region.
4. Stereo Vision System with Webcams. A stereo vision system with two images is
illustrated in Fig. 3. The geometry of stereo vision system is very useful for measuring the
depth. Consider that a point P=[X, Y, Z] in the world space is projected on both left and
right images on stereo vision system. In Fig. 3, the projected coordinates of point P on the
left and the right images are (xl , yl) and (xr , yr), respectively.
FIGURE 3. Stereo vision geometry.
The x-coordinates of P on left image and right image are:
f
x
Z
X l
 ,
f
x
Z
bX r

 , (4)
where f is the focal length and b is the length of baseline. Rewrite (4),
right image. The distance of target is 350cm.
The estimated results of distance by BMA and epipolar geometry constraint method are
shown in Fig. 6 and Fig. 7, respectively. From these two figures, we can conclude that the
measurement distance increase with baseline increasing. We can also conclude that the
range of measurement distance of BMA is more than that of epipolar geometry constraint
method, in the sense of the same error tolerance. However, as the measuring range
increasing, BMA searching results almost in the same range because of the low resolution
of the image, and this causes that the measurement error increases quickly.
FIGURE 6. Estimation errors with BMA.
FIGURE 7. Estimation errors with epipolar geometry constraint method.
6. Conclusion and Discussion. On the helicopter autonomous landing problem, stereo
vision system could estimate the height of the helicopter. In this paper, we propose a low
cost stereo vision system which is much cheaper than a GPS. The proposed system can
provide acceptably accurate height information for the unmanned helicopter landing control
system in certain range. To increase the measurement range, one should use cameras of
higher resolution and/or increase the baseline.
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                    日期： 100 年 2 月 13 
日 
                                 
一、參加會議經過 
本屆的國際智慧生活與機器人研討會  (International Symposium on 
Artificial Life and Robotics)已經是第十六屆了，舉辦地點在日本九州別府市的
B-Con Plaza。這個研討會的規模算是中型，每年都有不少的業界共同支持這個
會議，研討會舉辦的主要目的就是交流日本與世界各國在智慧生活方面、機器
人學方面、及相關領域的研究成果。 
今年的研討會共辦理三天，同時都有三個場地在進行論文報告或交流，共
有 250 篇論文在 46 個場次 (其中包括 1 個壁報場次) 中發表，主題涵蓋機器人
相關的各項主題，包括：影像處理、電腦視覺、感測技術、控制技術、人機介
面、混沌理論、類神經網路、模糊理論、機構設計…等，並且安排有 3 場的主
題演講及 2 場的專題論壇。 
主題演講的講者包括：韓國科學技術學院 (KAIST) 機械工程系教授兼人
型機器人研究中心主任 Jun Ho Oh 教授【講題是：人型機器人 HUBO II 的發展
概況】、美國華盛頓大學教授暨北京清華大學量子資訊科技中心主任談志中教
授【講題是：機器人--從製造到具智慧的機器】、日本京都大學的松野文俊教
授【講題是：救難機器人系統--從蛇形機器人到人型的介面】。HUBO II 機器
人是韓國人型機器人研究中心 Oh 教授的團隊所研發出來的人型機器人，身高
大約一米五左右，能夠用雙腳行進，也能夠單腳呈現金雞獨立似的平衡動作。
Oh 教授在短短的三年內就研發出 HUBO 的第一代，使韓國的機器人技術躋身
到世界第六的水準。在談教授的演講中，他簡單地介紹機器人的演化歷程，從
最早的電控製造用機器人，演化到能夠與人類互動的機器人，進而到現在所發
展具智慧的機器人。在松野教授的演講中，則是介紹了從 1995 年阪神大地震
後，在日本積極發展之結合資通科技與機器人科技的智慧型救難系統。這樣的
計畫編號 NSC 99－2221－E－269－020－ 
計畫名稱 以影像特徵擷取為基礎的相機位置估測方法之設計與實現 
出國人員
姓名 
夏郭賢 
服務機構
及職稱 
遠東科技大學資訊管理系副教授 
會議時間 
100年 1月 27日至 
100年 1月 29日 
會議地點 日本大分縣別府市 
會議名稱 
(中文) 第十六屆智慧生活與機器人國際研討會 
(英文) The Sixteenth International Symposium on Artificial Life and 
Robotics (AROB 16th ’11) 
發表論文
題目 
(中文) 降落標記影像不完整下之特徵抽取與相機位置估測 
(英文) Camera Position Estimation and Feature Extraction from 
Incomplete Image of Landmark 
三、考察參觀活動(無是項活動者略) 
本次參加研討會，未安排任何考察參觀活動。 
四、建議 
無特別建議事項 
五、攜回資料名稱及內容 
研討會論文集光碟、研討會會議手冊(含論文摘要) 
六、其他 
無 
 
參與研討會之照片集錦 
 
 
 





	
 

 
 
	

	 
 
 !"#$%&'$


(& )*+(,)
(,- .
/"
0	1	

00	.	 
 
 
$*012*20)3 䛆
+,
& &
&䛇 
+*45")'6
+7'+8+/&+
 9):+:.:, ;+<
5+*#5="-'6
+%&8+/&+
 ;+<
 
OS10-1 The study of path error for an Omnidirectional Home Care Mobile Robot 
Jie-Tong Zou (National Formosa University, Taiwan) 
Feng-Chun Chiang (WuFeng Institute of Technology, Taiwan) 
 
OS10-2 A* searching algorithm applying in Chinese chess game 
Cheng-Yun Chung (National Yunlin University of Science and Technology, Taiwan) 
Te-Yi Hsu (Industrial Technology Research Institute, Taiwan) 
Jyh-Hwa Tzou (National Formosa University, Taiwan) 
Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan)  
 
OS10-3 Multi-robot based intelligent security system 
Yi-Lin Liao, Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan) 
 
OS10-4 Implementation of an auction algorithm based multiple tasks allocation using mobile robots 
Kuo-Lan Su, Jr-Hung Guo (National Yunlin University of Science and Technology, Taiwan) 
Chun-Chieh Wang (Chienkuo Technology University, Taiwan) 
Cheng-Yun Chung (National Yunlin University of Science and Technology, Taiwan) 
 
OS10-5 Fuzzy programming for mixed-integer optimization problems 
Yung-Chin Lin (National Yunlin University of Science and Technology, Taiwan) 
Yung-Chien Lin (WuFeng University, Taiwan) 
Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan) 
Wei-Cheng Lin (I-Shou University, Taiwan) 
Tsing-Hua Chen (WuFeng University, Taiwan) 
 
OS10-6 Develop a vision based auto-recharging system for mobile robots 
Ting-Li Chien (Wu-Feng University, Taiwan) 
 
	*331>*$3)  䛆
.'

䛇 
+*45="=&+%&
8+/&+
 ;+< 
5+*45")'6
+7'+8+/&+
 9):+:.:, ;+<
 
OS11-1 Super-twisting second order sliding mode control for a synchronous reluctance motor 
Huann-Keng Chiang, Wen-Bin Lin, Chang-Yi Chang (National Yunlin University of Science and 
Technology, Taiwan) 
Chien-An Chen (Automotive Research and Testing Center, Taiwan) 
 
OS11-2 Shape recognition applied in a semi-autonomous weapon robot 
Chun-Chieh Wang, Chyun-Luen Lin (Chienkuo Technology University, Taiwan) 
Kuo-Lan Su (National Yunlin University of Science and Technology, Taiwan) 
 
OS11-3 Camera position estimation and feature extraction from incomplete image of landmark  
Kuo-Hsien Hsia (Far East University, Taiwan)  
Shao-Fan Lien (National Yunlin University of Science and Technology, Taiwan) 
Juhng-Perng Su (National Yunlin University of Science and Technology and Overseas Chinese 
University, Taiwan) 
 
 
 
The Sixteenth International Symposium on Artificial Life and Robotics 2011(AROB 16th ’11), 
B-Con Plaza, Beppu, Oita, Japan, January 27-29, 2011
©ISAROB 2011 P - 32
camera this restriction has to be overcome. The case
in which more than half the landmark image is available,
called “Case 1”, has been discussed in [6].
In this paper, we will focus on the case in which more
than half of the landmark image is defective, called “Case
2”. This scenario presents serious problems for extracting
useful features. A feature extraction method is developed
for extracting the features from the incomplete image of
landmark in which more than half of the landmark image
is defective. We use ANFIS to capture and estimate the
useful features from an incomplete landmark image.
Simulation results verify that under some conditions
it is possible to estimate the camera position from a
landmark image in which less the than half the total
image is available.
II. MAPPINGRELATIONSAND CAMERA
POSITION ESTIMATINGMETHOD
1. Projective geometry of landmark image
The five quantities, as defined in (1-5), obtained via
geometric projection [7] are the projection of the
features of the landmark image. We defined (6) for
estimating the camera position [5].
  Rk 	
  (1)
  Rh 	
	
  (2)
RQ 	
  (3)
   22  RRd  (4)
   
   22
22
 RTR
RSr
r


 (5)


	





QkRatio
rhRatio
drRatio
khRatio
position
angle
height
project
=
=
=
=
(6)
We further consider the cases of incomplete
landmark images, specifically, the case of the landmark
image in which more than half of the image is available
[6]. The four quantities: k, r , x, and y, related to the
completed landmark image, are selected for estimating
the approximate h. The estimated h is called h] . Here k
and r have been derived from (1) and (2), respectively.
Consider the case in which more than half the
landmark image is defective, as shown in Fig.2. The
five quantities: I1, I2, I3, I4 and I5 which are related to the
complete landmark image are selected for estimating the
approximate h and k, called h] and k] , respectively.
h]
k]
Fig.2. Incomplete landmark image and features
The projective relation between the landmark and
camera position is shown in Fig. 3. Definitions of  ʿ
 and e are provided as



	











 




 





 




 





 




 



h
bf
h
ebf
h
rebf
h
ebf
h
ebf
h
rebf
e
11
11
11
tantan
tantan
tantan





(7)



e
limCut
k]h]
Fig.3. Projective geometry of landmark features
Fig. 4 illustrates the projective relations of features
I2, I3 and I4. From Fig.3 and Fig.4, the derivations of
h] , k] , I1, I2, I3, I4 and I5 can be obtained as shown in (8-
13) and the estimated h] and k] in (14).
  RI e   tan5 (8)
  51 tan IRI   (9)
   
 22
22
51
22
2
fbH
RIIer
I


 (10)
k
g
(a) (b)
Fig.4. (a) Top view of the landmark features
(b) Projections of the landmark features
The Sixteenth International Symposium on Artificial Life and Robotics 2011 (AROB 16th ’11), 
B-Con Plaza, Beppu,Oita, Japan, January 27-29, 2011
©ISAROB 2011 278
RMSE of h] and k] are 0.000615 and 0.000471,
respectively. The h] and k] are utilized to approximate
the complete landmark image features. Table 2
illustrates the estimation results of camera position with
the features h] and k] . The RMSE of the , R and Q
are 0.45868, 0.506617 and 0.240798, respectively.
IV. CONCLUSION
In this paper, a scenario in which more that half of a
landmark image is defective has been analyzed. The
method for estimating the approximate features of
incomplete landmark images is provided. From
simulation results, it is clear that the approximate
features can be very closely estimated via the known
quantities. Moreover, we have shown that the
approximate features can be used for camera position
estimation. The inherent camera position estimation
error range is acceptable for practical applications. The
method proposed in this paper is a functional tool for
camera position estimation.
REFERENCES
[1] GPS 18 Technical Specifications (2005), Revision D,
Garmin International, USA.
[2] Mori R, Hirata K and Kinoshita T (2007), Vision-
Based Guidance Control of a Small-Scale Unmanned
Helicopter, International Conference on Intelligent
Robots and Systems, 2648-2653.
[3] Mori R, Kubo T and Kinoshita T (2006), Vision-
Based Hovering Control of a Small-Scale Unmanned
Helicopter, SICE-ICASE International Joint Conference,
1274-1278.
[4] Wang CC, Lien SF and Hsia KH et al (2009),
Image-Guided Searching for Landmark, Artificial Life
and Robotics, 14(1):95-100.
[5] Lien SF, Hsia KH and Su JP (2010). Image-Guided
Height Estimation for Unmanned Helicopter Landing,
ICIC Express Letters, 4(6B:2299-2304.
[6] Hsia KH, Lien SF and Su JP (2010), Camera
Position Estimation by ANFIS from Incomplete
Landmark Image, Conference on Fuzzy Theory and Its
Applications, Accepted.
[7] Hartley R and Zisserman A (2003), Multiple View
Geometry in Computer Vision, Cambridge University
Press.
Acknowledgement
This research was supported by the National Science
Council, Taiwan, R.O.C., under Grant No. NSC99-
2221-E-269-020.
Table 1. The estimation results of features h] and k]
No.   (cm) k k] Error of feature k h h] Error of feature h
1 32ദ 31 1.58891 1.58823 0.00068 1.70474 1.70463 0.00011
2 21ദ 44 2.87088 2.87023 0.00065 2.73393 2.73451 -0.00058
3 42ദ 78 2.06710 2.06790 -0.00080 1.96832 1.96786 0.00046
4 53ദ 113 0.16088 0.16187 -0.00099 0.15886 0.15934 -0.00048
5 19ദ 157 1.78014 1.78001 0.00013 1.76272 1.76347 -0.00075
6 60ദ 163 0.77189 0.77171 0.00018 0.74055 0.73976 0.00079
7 10ദ 173 2.78694 2.78656 0.00038 2.77071 2.77091 -0.00020
8 35ദ 201 0.92258 0.92308 -0.00050 0.91298 0.91274 0.00024
9 13ദ 240 3.22209 3.22278 -0.00069 3.22209 3.22176 0.00033
10 29ദ 289 2.92995 2.92951 0.00044 2.91630 2.91611 0.00019
Table 2. The estimation results of camera position with the features h] and k]
No.  R (cm) Q (cm) Error of estimated  Error of estimated R Error of estimated Q
1 32ദ 31 10 -0.619 0.421 -0.174
2 21ദ 44 23 -0.354 -0.572 0.243
3 42ദ 78 189 0.475 -0.435 0.314
4 53ദ 113 78 -0.487 -0.376 -0.197
5 19ദ 157 63 0.221 0.636 0.231
6 60ദ 163 284 -0.334 0.551 0.299
7 10ദ 173 174 -0.531 -0.417 -0.143
8 35ദ 201 124 0.437 0.623 0.354
9 13ദ 240 254 -0.634 -0.581 -0.208
10 29ദ 289 365 -0.312 0.352 0.146
The Sixteenth International Symposium on Artificial Life and Robotics 2011 (AROB 16th ’11), 
B-Con Plaza, Beppu,Oita, Japan, January 27-29, 2011
©ISAROB 2011 280
99 年度專題研究計畫研究成果彙整表 
計畫主持人：夏郭賢 計畫編號：99-2221-E-269-020- 
計畫名稱：以影像特徵擷取為基礎的相機位置估測方法之設計與實現 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 1 1 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
