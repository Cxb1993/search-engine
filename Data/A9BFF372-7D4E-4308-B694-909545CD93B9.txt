英文關鍵詞： Occluded Object detection, Behavior analysis, 
Gaussian Mixture Models, Model-driven Object 
Segmentation, Event Analysis 
 
 2
一、中文摘要 
此計畫的重點主要處理在遮蔽情況下的不同角度行為分析問題，第一年技術已解決不同角
度行為辨識的問題，第二年則為了解決遮蔽切割不確定性與相關的行為分析問題，我們先提出
一個關鍵姿態分類(clustering)的方法，利用此方法建立模組空間，以這此模組空間為基礎，我
們提出模組驅動(model-driven)的方法將遮蔽區域分割成個別有意義的物件，此方法利用粒子追
蹤技巧來對可能的遮蔽物件進行追蹤其可能的位置，然後從這位置，我們利用距離樣圖(distance 
maps)，從模組空間中，找出最佳的模組，然後利用樣板投影技巧，我們可以將遮蔽物件修補
成一個完整物件，接著我們將對遮蔽物件間的行為作分析，無論如何由於遮蔽的問題，我們做
姿態符號轉換時會發生很多的錯誤，為了解決著這個問題，我們對輸入的姿態不只利用最佳的
比對樣板做編碼還利用其他相似的樣板做姿態編碼，這樣就可形成一個矩陣表示法，以這個矩
陣表示法，不同的行為與動作就可以藉由 KL 距離來做有效比對。 
關鍵詞：遮蔽物件偵測、行為分析、高斯混合模型、模組驅動物件切割、事件分析 
二、英文摘要 
This project addresses the problem of occluded human segmentation and then uses its results for human 
behavior recognition.  To tackle this ill-posed problem, a novel clustering scheme is proposed for 
constructing a model space for posture classification. Then, a model-driven approach is proposed for 
separating an occluded region to individual objects.  For reducing the model space, a particle filtering 
technique is then used for locating possible positions of each occluded object.  Then, from the positions, the 
best model of each occluded object can be then selected using its distance maps. Then, a novel template 
re-projection technique is proposed for repairing an occluded object to a complete one.  Due to occlusions, 
there will be many posture symbol converting errors in this representation.  Instead of using a specific 
symbol, we code a posture using not only its best matched key posture but also its similarities among other 
key postures. With the matrix representation, different actions can be more robustly and effectively matched 
by comparing their KL distance. 
Keywords: Occluded Object detection, Behavior analysis, Gaussian Mixture Models, Model-driven Object 
Segmentation, Event Analysis 
三、前言、研究目的與文獻探討 
Human action analysis [1]-[5] is an important task in various application domains like video surveillance, 
video retrieval, human-computer interaction systems, and so on. The challenge in human action analysis is 
how to properly characterize spatial-temporal information and then facilitate subsequent recognition tasks.  
There have been many approaches proposed for tackling different problems in video-based human action 
analysis. For example, Mikic et al. [1] used complex 3-D models and multiple video cameras to extract 3-D 
 4
四、 研究方法 
This project addresses the problem of human behavior when occlusions happen.  Figure 1 shows the 
flowchart of our proposed system. In what follows, details of each component are described. 
1. Frame-to-Symbol Converting 
Assume that P is a binary posture which is extracted using an image subtraction technique. This paper uses 
the constrained Delaunay triangulation technique proposed by Chew [8]to divide P into different triangular 
meshes. Then, we define a new shape descriptor, i.e., centroid context to finely represent P.  
A. Polar Transform of a Posture 
After triangulation, we project P on onto a log-polar coordinate and label each mesh.  For the centroid r of 
one triangular mesh of P, we can construct a vector histogram ( (1),  ..., ( ),r r rh h h k …) for posture 
representation, in which ( )rh k  is the number of triangular mesh centroids in the kth bin when r is considered 
as the origin, i.e., 
    ( ) #  { |  ,  ( - )  }krh k q q r q r bin   , (1) 
where kbin  is the kth bin of the log-polar coordinate. Then, given two descriptors ( )
ir
h k  and ( )
jr
h k , the 
distance between them can be measured by a normalized intersection, i.e., 
    
1
1( , ) 1- min{ ( ), ( )}
bin
i j
K
i j r r
kmesh
C r r h k h k
N 
  , (2) 
where binK  is the number of bins and meshN  denotes the number of used meshes calculated from P. 
B. Centroid Context 
To define the centroid context of P, we need to derive a skeleton of P using a graph search.  Details of this 
method had been described in our previous work[10].  Given the ith body part, we can collect a set of 
triangular meshes PiV  from it. Let 
P
ic  be the centroid of the triangular mesh closest to the center of 
P
iV .  
Given Pic , its corresponding histogram ( )P
ic
h k  can be obtained using Eq.(1). Assume that the set of body 
parts is PV .  Based on PV , the new shape descriptor “centroid context”  is defined as follows: 
0,...,| |-1
{  }P P
i
P
c i V
C h  , 
where | |PV  is the number of elements in PV . Given two postures, P and Q, their distance ( , )ccd P Q  is 
measured by 
 
| |-1 | |-1
0 | | 0 | |0 0
1 1( , ) min  ( , ) min  ( , )
2| | 2 | |
P Q
P Q
V V
P P Q Q P Q
cc i i j j i jP Qj V i Vi j
d P Q w C c c w C c c
V V    
   , (3) 
where Piw  and 
Q
jw  are the area ratios of the ith and jth body parts residing in P and Q, respectively. 
2. Template Construction 
To well separate two occluded objects to different individuals, a model space should be constructed.     
A. Key Model Selection 
Since the occlusion problem is still ill-posed in computer vision, this paper assumes that two persons have 
 6
However, edged  is not accurate when an occluded object is compared.  When considering this occlusion 
effect, we weight a pixel r with the following equation: 
 ( ) exp( | |)rw r x  ,  (6) 
where 0.1  and rx  is the difference between r and the non-occluded side in the x coordinate. It means the 
pixels close to the non-occluded side should be more important than other occluded ones.  With ( )w r , Eq.(4) 
is modified as: 
    1( , ) ( ) | ( ) ( ) |
k
k
edge k O D
r O
d O D w r DT r DT r
C 
  . (7) 
where ( )
kr O
C w r

  . 
4. Occluded Object Segmentation 
Templates
+ 
Color Model
Triangulation Matching
Segmentation
 
Figure 3.  Flowchart of our segmentation method. 
This section will we use the triangulation technique to over-segment R to different triangulation meshes. 
Then, like Figure 3, the template and color models will be integrated for well relabeling each mesh so each 
occluded object can be successfully extracted from R.   
A. Over-segmentation Using Triangulation 
To extract objects from R, we first over-segment it to different triangulation meshes using its edge 
features.  Assume that  kt  is one triangular mesh in R and there are K objects in R. Then, a label field L 
={ |l l [0,..., -1]}K  can be created. Thus, the segmentation problem can be converted to a graph cut problem 
on G.  Given kt , its optimal label ktl  can be estimated by maximizing a posteriori probability ( | )kP l t : 
 arg max ( | ) arg max ( | ) ( )
kt k kl L l L
l P l t p t l P l
 
  , 
where ( )P l  is the priori probability of the lth object and ( | )kp t l  is the likelihood probability of kt  
belonging to the lth object. ( | )kp t l  can be further decomposed to two components, i.e., 
 ( | ) ( | ) ( | )k template k color kp t l p t l p t l , 
where 2( | ) exp(- ( , )/ )template k template k templatep t l d t l   and 2( | ) exp(- ( , )/ )color k color k colorp t l d t l  . ( , )template kd t l  
and .. denote the template and color distances between kt  and the lth object, respectively.  ( , )color kd t l  is 
measured by comparing the color histograms between kt  and the lth object.  About ( , )template kd t l , in what 
follows, a re-projection technique will be described for calculating its value.   
 8
similar to AtK .  Thus, the correct type of each posture 
A
tQ  in A will not be always found.  To tackle this 
problem, we represent AtQ  not only using 
A
tK  but also other similar postures in Z .  Thus, a feature vector 
A
th  for representing the tth posture AtQ  is constructed as follows: 
 ( ) ( , )  A At cc t i ih i S K K for K Z  ,  (11) 
where Z is the set of key postures.  With Eq.(11), A is converted to a matrix representation as: 
 0,...,| | 1{ }
A
A t t AH h   , (12) 
Then, given an action type D in databases, we can convert it to a matrix DH .  For the ith element 
A
ih  in 
AH  and the jth element 
D
jh  in DH , their dissimilarity is measured as follows:    
        
90| | 1
0
( ,  ) log
KP AS
iA
i D
k i
h k
cost i j h k
h k
 

      . (13) 
Then, the dynamic time warping method [7] is used to measure the distance between AH  and DH . 
五、結果與討論 
    
                 (a)             (b)    (c)  
Figure 6.   Segmentation results of an occluded region. 
To test the efficiency and effectiveness of the proposed approach, we constructed a large database of 
more than thirty thousand postures, from which we obtained over three hundred human movement sequences.  
Figure 6 shows the results of object extraction and shape repairing after occlusion elimination.  (b) and (c) 
are the extracted objects from (a) after repairing.  The most difficult case is the people wearing the clothes 
with similar color.  Figure 8 shows the case when occluded objects are with similar cloth colors.  Table I 
lists the precision analysis of our method for occluded object segmentation under different test sequences.  
Clearly, our method can well all the occlusion cases. Once an occluded region is separated to different objects, 
we can then recognize their corresponding behavior types.  In this paper, four behavior types were 
recognized, i.e., walk-left, walk-right, stretch-left, and stretch-right. Table II shows the accuracy of behavior 
recognition for these four behavior types when occluded objects were handled without using the matrix 
representation.  It is noticed if the occluded region is not separated to different objects, further behavior 
analysis is impossible.  Due to occlusion, the accuracy is not so high.  However, if the matrix representation 
was used, the recognition accuracy can be further improved.  Table III lists the accuracy analysis when the 
matrix representation was used. Table IV shows the confusion matrix among four behavior types for 
illustrating the accuracy of behavior recognition. From this table, clearly, the number of “true-positive” 
 10
參考文獻 
 
[1] I. Mikic, et al., “Human body model acquisition and tracking using voxel data,” IJCV., vol. 53, no. 3, pp. 
199-223, 2003. 
[2] R. Cucchiara, et al., “Probabilities posture classification for human-behavior analysis,” IEEE Trans. on 
SMC-Part A: System and Humans, vol. 35, no. 1: pp. 42-54, Jan. 2005. 
[3] D. Weinland, E. Boyer and R. Ronfard, “Action Recognition from Arbitrary Views using 3D Exemplars,” 
ICCV., pp. 1-7, Oct. 2007. 
[4] Y. Ivanov and A. Bobick, “Recognition of visual activities and interactions by stochastic parsing,” IEEE 
Trans on PAMI., vol. 2, no. 8, pp. 852-872, Aug. 2000. 
[5] C. S. Lee and A. Elgamma, “Modeling View and Posture Manifolds for Tracking,” IJCV, pp. 1-8, 2007. 
[6] S. Kullback, Information theory and statistics, Dover Books on Mathematics. 
[7] S. Chu, et al., “Iterative Deepening Dynamic Time Warping for Time Series,” In Proceedings of SIAM 
International Conference on Data Mining, 2002. 
[8] L. P. Chew, “Constrained Delaunay triangulations,” Algorithmica, vol. 4, no.1, pp.97-108, 1989. 
[9] M. Isard and A. Blake, “CONDENSATION: conditional density propagation for visual tracking,” IJCV., 
vol. 29, no. 1, pp. 5-28, 1998. 
[10] J.-W. Hsieh, et al., “Video-based Human Movement Analysis and its Application to Surveillance 
Systems,” IEEE Trans. on Multimedia, vol. 10, no. 3, pp.372-384, April 2008. 
 
 12
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本計畫設計完成一個可容忍遮蔽發生之視角不變人體肢體特徵分析與行為辨識系統，來解
決在實際情況下，行為分析時所會發生的問題，為了能更仔細地分析人的肢體行為，我們
利用三角化技術，搭配樣板模組技術，開發出一「以樣板(template)為基礎之肢體分析技
術」，從視訊中將人的肢體特徵抽取出來；接著我們研發出「視角不變之行為辨識技術」，
可從任何角度來分析可疑人物之各類行為，基本上在一個監控環境內，同一行為會因視角
變動而有改變（例如從正面與側面觀看），此模組所發展的技術不因視角的改變，而造成
行為分析的誤判，搭配不同的攝影機，進行特定物體之行為持續追蹤與分析；接著我們開
發「可容忍遮蔽發生之行為辨識技術」，此技術主要在處理多人間行為辨識會產生遮蔽的
問題，因為遮敝會造成系統無法有效地分析人體，不知道何者是要分析的標的。此計畫所
發展的監控技術可應用在許多方面，例如，在安全監控、防盜、醫療診斷等方面，並可置
於辦公大樓、走廊、旅館大廳、停車場、銀行、超商與商務飯店之大廳櫃檯、或行人步道
等公共空間，對進出之人員作安全控管，以達到事件偵測警示與犯罪預防等目標。 
 
 
 14
（英文） 
This technique addresses the problem of occluded human 
segmentation and then uses its results for human behavior 
recognition.  To tackle this ill-posed problem, a novel 
clustering scheme is proposed for constructing a model space 
for posture classification. Then, a model-driven approach is 
proposed for separating an occluded region to individual 
objects.  For reducing the model space, a particle filtering 
technique is then used for locating possible positions of each 
occluded object.  Then, from the positions, the best model of 
each occluded object can be then selected using its distance 
maps. Then, a novel template re-projection technique is 
proposed for repairing an occluded object to a complete one.  
Due to occlusions, there will be many posture symbol 
converting errors in this representation.  Instead of using a 
specific symbol, we code a posture using not only its best 
matched key posture but also its similarities among other key 
postures. With the matrix representation, different actions can 
be more robustly and effectively matched by comparing their 
KL distance. 
產業別 
電腦、互動電玩、視訊監控、異常事件分析 
技術/產品應用範圍 
此技術特色利用樣板投影技巧，將遮蔽物件修補成一個完整物
件，然後可對遮蔽物件間的行為作分析 
可應用在互動電玩、視訊監控 
技術移轉可行性及預期
效益 
可用在視訊監控產業，從視訊資訊中，可即時及穩定地作異常事
件分析如搶劫事件，為台灣廠商提供無限的商機，並提高技術門
檻與 
     註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
 
 
 16
   在論文議程上的安排，邀請五位國際知名的學者 Prof. Horst Bunke、Prof. Christopher M. 
Bishop、Prof. Shree Nayar、Prof. Prabhakar Raghavan 和 Prof. Antonio Torralb 來做 keynote speakers，
詳細地介紹他的研究成果，他們的題目分別為： “Towards the Unification of Structural and Statistical 
Pattern Recognition”、“Embracing Uncertainty: The New Machine Intelligence”、“Computational 
Cameras: Redefining the Image”、 “The Quantitative Analysis of User Behavior Online Data, Models 
and Algorithms”、 “Scene and Object Recognition in Context”，這些知名學者針對未來熱門的題目，
深入簡出地介紹他們的理論技術，讓聽者獲益良多。 
二、與會心得 
 
圖一、演講會場 
本人參加此次的第 20 屆圖形識別國際會議，會中本人發表一篇利用使用視覺互動特徵作抽菸動
作分析之系統，很意外的這篇論文獲得本次論文的最佳論文獎，發表後獲得許多人的興趣與討
論，許多人並詢問相關執行速度、辨識率如何？是否有實驗的視訊資料，可以做實驗。除了宣讀
論文之外，我並與參加此次會議的各種 keynote 演講(如圖一)，與各類 oral 和 poster 的議程，並
與各國學者或研究員彼此互相討論，除可得知這方面最新技術的研發與進展，並可了解目前各國
在圖形識別與影像處理技術上的研發情況與進展，圖二為本人論文發表的情況，因為這會議是在
土耳其舉辦的，大部分的學者來自歐洲，亞洲的學者也不少，例如韓國、日本等等，只要是多媒
體（包含視訊監控）與網路盛行的國家，參加的人就不少，台灣來的教授也很多如中研院的廖弘
源教授、清大電機的黃仲陵教授、交大電信的張文忠教授、中央大學資工的范國清教授等等。在
研討會中與這些學者討論與交換意見，深深的覺得在圖形識別與視訊監控這個領域，很多研究都
在探討利用統計的方式來做視訊應用的分析與辨識，不過台灣的學生似乎在統計上面的訓練不大
夠，這是台灣學者需要重視的。在此會議碰到許多大陸學生與香港的學生與學者，出來的人數與
論文發表數已超過台灣了，這是過去沒見到過的，質方面也有相當不錯的進步，相對而言台灣的
論文數就少了許多。 
表 Y04 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          99 年  11 月 10  日 
報告人姓名  謝君偉 
 
服務機構 
及職稱 
 
 海洋大學 資工系 教授 
     時間 
會議 
     地點 
 2010 年 8 月 23 日至 8月 26 日
 土耳其 伊斯坦堡  
本會核定 
補助文號 
 NSC 
98-2221-E-019-070-MY2 
會議 
名稱 
 (中文) 2010 年第 20 屆國際圖形識別會議 
 (英文) The 20th International Conference on Pattern Recognition 
發表 
論文 
題目 
 (中文) 使用視覺互動特徵作抽菸動作分析 
 (英文) Human Smoking Event Detection Using Visual Interaction Clues 
報告內容應包括下列各項： 
一、參加會議經過 
 
本次研討會在土耳其的伊斯坦堡舉行，由於此研討會主要是從事影像處理與圖形辨識學
者務必參加的盛會，因此匯集了全世界的知名研究學者，剛好本人有作視訊監控與行為
分析等相關研究，故投稿至此。此會議論文共有 2140 的投稿，接受 1147 篇，接受率為
54%。為了討論目前圖形識別最熱門的問題與應用，此大會主要議程分成六個討論議題
如下 
Track I  Computer Vision  
Track II  Pattern Recognition and Machine Learning  
Track III  Signal, Speech, Image and Video Processing  
Track IV  Biometrics and Human Computer Interaction  
Track V  Multimedia and Document Analysis, Processing and Retrieval  
Track VI  Bioinformatics and Biomedical Applications 
 
這些議程討論了目前圖形識別與多媒體訊號處理領域，各種不同的最新應用與理論，吸
引了許多知名優秀的學者參與討論，除此之外，這個會議更有許多工業界的人來參加，
所以會議裡會有很多圖形識別與多媒體的學者學術討論與工商業者的產品展示，更讓我
得知現在全世界的未來的研究趨勢及課題，或許有些課題是自己所熟知的，但更多卻是
自己所欠缺的，不論是熟知或新知，不同的環境造就不同的人，亦產生不同的想法和動
機，這些都能對自己的未來研究提供不少助益，可以藉由這些討論，增加自己的能力！
表 Y04 
與學者，出來的人數與論文發表數已超過台灣了，這是過去沒見到過的，質方面也有相
當不錯的進步，相對而言台灣的論文數就少了許多。 
 
 
圖二、論文發表 
 
三、建議 
 
    此會議主要探討在圖形識別等方面課題，只要用心參與都能吸收到不少新知，包括
了各種新的技術、觀念或是應用等等，都讓我受益良多，尤其是這個會議有許多學者與
工業界參加，讓我發覺理論跟實際真的差距蠻大的，因此，如何讓理論與實際應用合為
一體是我這次主要的收穫。除此之外，由參加這次會議，發覺許多優秀與有深度的論文
都強調實作，但對於多數的台灣研究生似乎都很欠缺，尤其是現補習教育的盛行，許多
研究生能考上好學校，是因從大二或大三就開始補習，對同樣的習題與科目一直磨練，
雖然研究所考試能獲得高分，但不代表真的有對應的能力來解決實際問題，只能說當時
的運氣及記憶力比較好，創造能力並無跟著增長，有點令人感嘆，希望國內教育能多加
強實做與系統整合之能力。 
 
四、攜回會議論文紙本內容 
 
五、其它 
 
98 年度專題研究計畫研究成果彙整表 
計畫主持人：謝君偉 計畫編號：98-2221-E-019-070-MY2 
計畫名稱：可容忍遮蔽發生之視角不變人體肢體特徵分析與行為辨識系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 0 100% 
篇 
兩篇論文發表於
國內的CVGIP會議
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 0 100% 
分別為莊凱婷 王
宇世 林國進 季
均霖 
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
國外 論文著作 
研討會論文 2 0 100% 
篇 
兩篇國際會議論
文,發表如下 
1.Jun-Wei Hsieh, 
Sin-Yu Chen, 
Chi-Hung Chuang, 
Miao-Fen Chueh, 
and Shiaw-Shian 
Yu, ’Occluded 
Human Body 
Segmentation and 
its Application 
to Behavior 
Analysis,’ 2010
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
