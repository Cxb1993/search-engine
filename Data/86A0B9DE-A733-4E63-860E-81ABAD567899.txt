1行政院國家科學委員會補助專題研究計畫
■ 成 果 報 告
□期中進度報告
智慧型監視系統之研究與設計(I)
計畫類別：■個別型計畫 □整合型計畫
計畫編號：NSC 99-2221-E-027-106
執行期間： 99 年 8 月 1 日至 100 年 7 月 31 日
執行機構及系所：國立台北科技大學電子工程系
計畫主持人：范育成副教授
共同主持人：
計畫參與人員：姜義峰、黃品綱、李宗熹、陳玠霖、邱奕誌
成果報告類型(依經費核定清單規定繳交)：■精簡報告 □完整報告
本計畫除繳交成果報告外，另須繳交以下出國心得報告：
□赴國外出差或研習心得報告
□赴大陸地區出差或研習心得報告
■出席國際學術會議心得報告
□國際合作研究計畫國外研究報告
處理方式：除列管計畫及下列情形者外，得立即公開查詢
■涉及專利或其他智慧財產權，□一年■二年後可公開查詢
中 華 民 國 100 年 7 月 31 日
附件一
3都需要經過大量演算法作處理，並透過事件錄影方式作資料的儲存，以達到快速搜尋、並降低資料量
的儲存。因此，如何能夠預測物件的行為、快速追蹤物件以及降低資料量的儲存，為入侵偵測研究的
主要目的。此外，攝影機的動態調節能力是有限的，因此，使用者應儘量避免出現影像上的明亮區域。
明亮的影像會導致曝光過多，而物體就會顯得過暗。使用者試圖捕捉窗前的拍攝目標時通常會遇到這
個問題。基於上述的考量，我們的研究架構也必須要排除這方面的問題，因此，不管在光線強弱的狀
態下也要能夠偵測到物件，並且完整的框選出來。未來還必需要加入關於天候方面的因素，使這套系
統能夠真正適應氣候變化所造成的影響。關於亮度變化方面，也需要使學習率更新更快，讓這套系統
不管於室內或室外亮度都能即時反應，讓任何遺留物體都能夠被完整的偵測。
3. 文獻探討：
本計劃所提出的「智慧型監視系統之研究與設計」，主要的核心技術包含了影像資料取樣技術、多
目標物件移動追蹤技術與亮度變化之遺失物體偵測技術，我們依據這三方面的文獻依序加以探討：
影像資料取樣文獻探討:
快速運算能使智慧監控功能有效的發揮出來，因此取樣技術扮演重要的角色。在安全監控影像取
樣應用包含物件侵入之影像取樣、影像特徴角取樣、人影輪廓學習取樣、人臉學習特徵及人身特徵取
樣。在人影追蹤所進行的人影輪廓學習取樣 [1]方式，將人影取樣的資料，送到快速演算(FEP) 得到取
樣模型輸入座標，找出的模型座標，再使用三角型模型分解法，建構出人影取樣模型，[2]之FEP(Fast and
Efficient Pruning)演算快速找出取樣座標，以取樣座標當頂點與兩個取樣點產生三角形，透過三角形建
構出T形人影模型，人影模型取樣流程，首先輸入影像，再針對輸入的影像進行取樣，接著將取樣後的
影像進行標籤演算法作處理，透過標籤演算處理後，找到最佳人影綠色輪廓，再將取樣輪廓置入物件
上。人臉辨識是藉以完整的影像特徵作為基礎，並進行特徵辨識人臉學習作取樣[3]，辨識經統計測量
方法取得特徵向量集合，這個集合被表示臉部或身體型態相關的特徵，找出影像的特徵資料，進行快
速辨識運算。
多目標物件移動追蹤文獻探討:
至於偵測移動物件的方法雖然有很多種，而且方法上都不盡相同，但是基本的觀念則都源自於光
流偵測法(Optical Flow)[4]、高等集合法(Level Set Method)[5]、背景相減法(Background Subtraction) [6]、
時間差異法(Frame Difference)[7]、背景相減結合時間差異法[8]、特徵分析(Feature Analysis)[9]以及區塊
比對(Block Matching)[10]等的方法。J. F. Vega-Riveros 和 K. Jabbour[4]的論文研究指出電腦視覺的移動
分析是一個重要組成的成分，移動訊息的時間分配與測量來自於多變的連續影像。於動態分析中的影
像序列找到了廣泛的應用，其中包括機器人導航、目標物追踪、交通監控、生物細胞的移動、雲和天
氣系統的追踪、影像 編碼等等。
亮度變化之遺失物體偵測文獻探討:
近幾年來，國內外對數位監視系統的探討投入了不少心力。對於亮度處理方面，目前國外學者所
提出的方法，有以類神經網路去處理亮度變化的有 D. Culibrk et.al [11]，而 L. Maddalena and A. Petrosino
[12]與 M. I. M. Chacon et.al [13]都是用類神經網路的自組織特徵映射(Self-organizing Feature Maps)網路
(一般簡稱為SOM），是屬於競爭式學習與無監督式學習網路的一種，一般競爭式學習神經網路則採用
「贏者通吃」。也就是說，自我組織特徵映射在競爭之後，不只獲勝神經元有資格學習，它週圍的神經
元也能夠學習，在自組織特徵映射中，輸出層的類神經元是以矩陣方式排列於一維或二維的空間中，
並且根據目前的輸入向量彼此競爭以爭取得到調整鏈結值向量的機會，而最後輸出層的類神經元會根
據輸入向量的「特徵」以有意義的「拓樸結構」（Topological Structure）展現在輸出空間中。而這種陣
列的拓樸關係，簡單地來說，就是神經元之間的鄰居關係。由於所產生的拓樸結構圖可以反應出輸入
向量本身的特徵，因此我們將此種網路稱為自我組織特徵映射網路。J. B. Huang and C. S. Chen [14]、
5色圖框，表示物件侵入警戒區;影像變動超出第二條警戒線，使用紅色圖框，代表物件侵入危險區。
使用慣性加速度之 X 軸向量與 Y 軸向量速率比，進行移動預測，透過移動估計演算法如公式(3)
及移動預測如公式(4)進行演算，在目前影像之 Jmax 找出移動軌跡如公式(5)，有了正確的追蹤座標，
進行物件移動預測，預測物件移動的方向是否往安全區或是往警戒區方向移動，若移動預測出有危險
移動時，會在 Jmax 的座標顯示移動軌跡，透過移動軌跡來判斷物件移動方向。其中 n 為影像序號，
Xn 為目前影像 Imax 的 X 座標，Xn-1 為前一張影像 Imax 的 X 座標，Yn 為目前影像 Imax 的 Y 座標，
Yn-1 為前一張影像 Imax 的 Y 座標。
YXYYIXXIYXIYXInv
n
n
nn
n
n
nn
n
n
nnnn  






1
1
1
1
1
11maxmax ),(),(),(),()( (3)
1


Y
X (4)
),()()( 11max YXJnvnC  (5)
我們採用慣性加速度方法，將連續影像物件移動的向量座標進行預測，預測物件往Y軸或是X軸方
位移動，因為物件往X軸方向移動是朝向警戒區的危險方向，因此我們關心物件往X軸方向移動的向
量，我們忽略Y軸移動向量，雖然物件往Y軸方向移動，會有X向量位移，但是不會有即時的危險，所
以我們忽略物件在Y軸移動所產生的X軸位移，我們將物件移動的軌跡型態依方向性可分為X方向、Y
方向、先X方向然後Y方向、先Y方向然後X方向以及環狀五種類別。首先是由已偵測的四點移動座標 (水
平最大及最小值，垂直最大及最小值)取出水平軸最大座標Imax，透過移動方程式，計算物件的移動向
量，找出移動軌跡，再透過移動向量預測演算法，判斷物件是往X軸方向移動或往Y軸方向移動，若物
件往Y軸方向移動，就可忽略，若物件往X軸方向移動時，可透過X位移方向，判斷物件是否往危險區
移動，若物件往危險區域移動時，移動座標Jmax出現紅色軌跡，安全相關人員可以藉由紅色軌跡線，
觀察物件移動情形，進行因應措施，避免危險事件發生。
b.多目標物件移動追蹤演算法介紹
所謂移動物件即是一連續的影像序列當中，屬於變動的區塊，此部分的影像區塊與靜止的背景影
像是有所差異的。通常偵測移動物件的方法，大致上可分為2種，分別是時間差異法(Temporal Difference)
以及背景相減法(Background Subtract)。其中時間差異法不需要事先建立背景影像，而是將連續影像的
畫面作連續相減，如公式(6)所示， ji, 代表影像的像素點位置， ),,( tjiFD 為差值影像， ),,( tjif 為目前影
像， )1,,( tjif 為前張影像。
)1,,(),,(),,(  tjiftjiftjiFD 。 (6)
背景相減則必須事先將建構好的參考影像當作背景影像，與輸入的影像相減，如公式(7)所示。其
中 ji, 代表影像的像素點位置， ),( jiBS 為背景相減後的影像， ),( jiI 為原始背景的影像， ),( jiC 為目前的
影像。以上的介紹屬於影像的前處理(Pre-Processing)部分。
),(),(),( jiCjiIjiBS  。 (7)
接下來的介紹是屬於影像後處理(Post-Processing)的部分。首先影像的擷取當中，難免會有一些雜
訊的產生，然而，過多的雜訊則會影響我們欲辨識以及判斷的影像資訊，若是得到的影像資訊不夠充
足或者是不確定因素太多，就有可能會造成判別錯誤。因此，在此介紹中值濾波器(Median Filter)的應
用。原始像素值經過排序後的數值順序為10、11、11、12、12、13、14、60，原來3×3遮罩中間的像素
值為60，經由中值濾波器排序後的結果為12取代原來的60，以此方式達到消除雜訊的效果。關於陰影
消除的部分採用垂直投影(Vertical Projection)的方式，利用垂直投影量的數量設定臨界值，若是投影數
量小於臨界值即為雜訊(Noise)或是陰影(Shadow)的部分。為了要使移動物件的區塊更加完整，並且減
少區塊的破損及斷點，我們使用了型態學(Morphological)的膨脹(Dilation)及侵蝕(Erosion)來進行修補的
7,,
, , 1/ 2, ,/ 2
,
,, 1 ,
, , , ,,
1
( , )
(2 )
1
exp ( ) ( ) ( )
2
t kt t k
i j i j i j t kn
i j
t kt t k T t t k
i j i j i j i ji j
g x m
x m x m



     



(11)
,
t
i jx 是(i,j)的顏色向量，
,
,
t k
i jm 與
,
,
t k
i j 為分佈的平均向量和共異矩陣。當建立好高斯混合模型後，當有
一張新的影像進來，以像素為單位，與該像素點的高斯混合分佈去比對，判斷像素是否為前景。當比
重越大、變異數越小的高斯分佈可否成為前景或是背景時?就需要設一個門檻值來判別，在一些靜態的
變化所造成的前景，我們希望他盡快被併入到背景模型，所以需要一個自動調整學習比率的機制。透
過這樣的方式，每個像素可以根據自己的分佈狀況，調整屬於自己的學習比率，來決定自己併入背景
的速度。為了獲得更簡單的運算，有利於抽前景形狀，因此設定 ,
k
i jP 為前景資訊，並設定 ,
k
i jB 為背景資
訊，當差異畫面在計算時，會取得差異資訊，如下所示:
, ,
k k
i j i jdifference P B  (12)
中值濾波器可以消除雜訊又可以保存物體的細節，是一個不錯的演算法，但因為會使用到排序才
能找到中間值，因此演算法的速度會比平均濾波來的慢。有時考量到即時性的問題，會退而求其次選
擇平均濾波，但是在硬體夠快的情況下，則應該採用中值濾波器來過濾才有更好的雜訊處理效果。一
般而言，在偵測移動物體都會產生亮度失真與彩度失真的雜訊，我們會使用5 5 的中值濾波器去過濾
其雜訊，使用後可以刪除大多數的雜訊和保護移動物體。在做完背景濾除後，雖然由中值濾波器可以
過濾大部分的雜訊，但是還是有一些區塊大的雜訊需要用形態學的濾波方式來消除它。侵蝕(Erosion)
是用來除去影像中不相干的細節，而膨脹(Dilation)卻是應用於橋接縫隙，並把破碎的空洞填滿。當 B
是欲做濾波的影像，而 X 則是結構元素(Structuring Element)，這裡我們採用 3x3 的矩形遮罩(Mask)而
和這兩個運算子分別代表侵蝕與膨脹。它們定義如下:
{ ( ) }bB X b X B   (13)
{ ( ) }bB X b X B   I (14)
d. DaVinci實作
我們計劃採用TI daVinci™ DM644x 的開發平台，其設計流程可分為四個步驟，首先先將相關開發
環境安裝與設置，接著撰寫ARM的程式，緊接著完成DSP程式的撰寫，最後將結果輸出。在開發環境
安和設置中：Davinci的程式分DSP程式及ARM程式，DSP程式我們在MontaVista kernel Linux編輯和編
譯，ARM程式在Source Insight中編輯以及Linux中的dvsdk2.0 toolchain下編譯。在撰寫ARM和DSP程式，
由於可直接執行C程式，最後將顯示結果於指定的顯示器上。
5. 結果與討論（含結論與建議）：
本計劃的主要目的是提出一套自動化的移動物件偵測與追蹤的方法，並且以 TI 所提供的 Davinci
TMS320DM6446 嵌入式平台作實現。本計劃將移動物件偵測與追蹤的演算法分別為影像的前置處理-
移動物件偵測部分與影像的後置處理-移動物件追蹤部分。影像的前置處理部分，我們先將數位攝影機
所擷取的影像轉灰階化處理，接著將整張影像作降取樣處理，之後作背景相減使得前景影像與背景影
像作分離，再將影像作二值化處理以偵測出移動物件的部分，上述影像的前置處理不但可以使欲處理
的影像單純化，更可以大量的減少處理時的運算量。影像的後置處理部分，為了要更準確的框選出移
9IEEE Transactions on Neural Networks, vol. 18, pp. 1614-1627, Nov. 2007.
[12] L. Maddalena and A. Petrosino, “A Self-Organizing Approach to Background Subtraction for Visual
Surveilance Applications,” IEEE Transactions on Image Processing, vol. 17, pp.1168-1177, Jul. 2008.
[13] M. I. M. Chacon, G. D. Sergio, and V. P. Javier,.“Simplified SOM-neural model for video segmentation
of moving objects,” International Joint Conference on Neural Networks.(IJCNN 2009), Jun. 2009, pp.
474-480.
[14] J. B. Huang and C. S. Chen, “A physical approach to Moving Cast Shadow Detection,” IEEE
International Conference on Acoustics Speech and Signal Processing.(ICASSP2009), Apr. 2009, pp.
769-772.
[15] Z. Tang and Z. Miao, “Fast Background Subtraction and Shadow Elimination Using Improved Gaussian
Mixture Model,” IEEE International Workshop on Haptic Audio and Visual Environments and
Games.(HAVE2007), Oct. 2007, pp. 38-41.
[16] N. Martel-Brisson and A. Zaccarin et al., “Learning and Removing Cast Shadowsthrough a
Multidistribution Approach,” IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE
INTELLIGENCE, vol. 29, no. 7, pp.1133-1146, jul. 2007.
[17] L. Li, et al., “Foreground object detection from videos containing complex background,” in Proc.
International Multimedia Conference.(ACM2003), USA, 2003, pp. 2-10.
7. 論文發表(計劃執行期間)
1. Yu-Cheng Fan, Jung-Ching Chiou, and Yan-Hong Jiang, “Hole-Filling Based Memory Controller of
Disparity Modification System for Multi-view Three Dimensional Video,”IEEE Transactions on
Magnetics, vol. 47, issue 3, pp. 679-682, March 2011. (SCI, EI)
2. Yu-Cheng Fan, Yu-Ting Kung, and Bing-Lian Lin, “3D Auto-stereoscopic Image Recording, Mapping
and Synthesis System for Multi-view 3D Display,”IEEE Transactions on Magnetics, vol. 47, issue 3, pp.
683-686, March 2011. (SCI, EI)
3. Yu-Cheng Fan, Shu-Fen Wu, and Bing-Lian Lin, “3D Depth Map Motion Estimation and Compensation 
for 3D Video Compression,”IEEE Transactions on Magnetics, vol. 47, issue 3, pp. 691-695, March 2011.
(SCI, EI)
4. Jung-Ching Chiou, Yan-Hong Jiang, and Yu-Cheng Fan, “Chip Design and Implementation of Depth 
Map Reconstruction for 3DTV System,” 2010 Conference on Innovative Applications of System
Prototyping and Circuits Design, Taoyuan, Taiwan, Oct. 15, 2010, pp. 24-29. (2010 CIASPCD Best
Paper Award)
5. Chi-Cheng Lin, Wei-Chun Ting, and Yu-Cheng Fan, “Depth Adjustment System of 3D Stereo Image,”
2010 Conference on Innovative Applications of System Prototyping and Circuits Design, Taoyuan,
Taiwan, Oct. 15, 2010, pp. 84-89.
6. Yu-Chun Huang, Po-Cheng Chi, and Yu-Cheng Fan, “Multiple Motion Objects Detection for Inteligence 
Surveilance System,” 2010 Conference on Innovative Applications of System Prototyping and Circuits
Design, Taoyuan, Taiwan, Oct. 15, 2010, pp. 108-113. (2010 CIASPCD Best Paper Award)
7. Yu-Ting Kung, Bing-Lian Lin, and Yu-Cheng Fan, “A Development System of  Auto-stereoscopic
Three-Dimensional Display,” 2010 Conference on Innovative Applications of System Prototyping and
Circuits Design, Taoyuan, Taiwan, Oct. 15, 2010, pp. 120-125.
8. Yu-Cheng Fan, and Jan-Hung Shen “Low Luminance Dynamic Range Conversion Component for 
出席國際學術會議心得報告
計畫編號 NSC 99-2221-E-027-106
計畫名稱 智慧型監視系統之研究與設計(I)
出國人員姓名
服務機關及職稱
范育成 副教授 國立台北科技大學電子工程系
會議時間地點 Seoul, Korea, June 20-22, 2011
會議名稱 2011 International Conference on 3D Systems and Applications, 3DSA2011
發表論文篇數 1篇
一、參加會議經過
今年的國科會計劃執行期間，我參與了一場國際研討會，同時亦在國際研討會中發表了
一篇國際會議論文，這場會議的名稱為 2011 International Conference on 3D Systems and
Applications, 3DSA2011。
2011 International Conference on 3D Systems and Applications, 3DSA2011，於今年六月 20
日至六月 22 日在韓國首爾舉行，這個會議每年舉辦一次，內容主要針對前瞻型視訊相關研
究，有許多前瞻的視訊會議論文都會在這個會議發表。
會議在 6月 20日下午開始，由三場 Keynotes揭開序幕，分別為 Dr. Naomi Inoue (NICT,
Japan)介紹 Glasses-free 3D display systems being developed at NICT，Prof. Yi-Pai Huang
(National Chiao Tung University, Taiwan)介紹 Intelligent and interactive 3D display system以及
Prof. Byoungho Lee (Seoul National University, Korea)針對 Future 3D technology –focusing on
integral imaging, holography and see-through display做引述，內容十分精彩。
第二天與第三天的會議共分為八場 Session，分別針對七個主題作介紹，七個主題分別是
Session 1 3D Capturing & Processing I、Session 2 3D Human Factor、Session 3 3D Displays &
Systems I、Session 4 3D Coding & Transmission、Session 5 3D Contents & Applications、Session
6 3D Capturing & Processing II、Session 7 3D Displays & Systems II與 Session Poster Session。
首先登場的是 Session 1 3D Capturing & Processing I，這個 Session主要針對 3D Capturing
與 Processing 的議題，其中包含了 Gradient weighted joint bilateral filtering for depth map
upsampling、Multiview calibration of a ring-shaped camera array based on essential matrices
Reliability、Application of long range depth camera for generating virtual-viewpoint videos of
sporting events 、 Depth estimation from uncalibrated stereo images via hierarchical
over-segmentation、Image processing in an integral imaging pickup system with dual-green
會議中所聽到的 session中我所感興趣的部分心得記錄於下：
1.在立體電視領域 3D Capturing & Processing、3D Human Factor、3D Displays & Systems、3D
Coding & Transmission與 3D Contents & Applications為立體電視設計的主流方向。
2.每每出國參加會議，都會感覺到世界各地的研究是絲毫沒有鬆懈的，在這次的會議中我也
有深刻的體會。在世界各地幾個頂尖的實驗室，每年總有相當多的研究成果在這裡發表，
這也提醒我在研究的道路上是不能心生懈怠的。
3.親身體會參加一個國際會議是一個很不錯的經驗，包括如何與來自世界各地的研究人員使
用英語來進行學術討論。
4.在這個會議中總是會認識一些新的朋友，雖然碰面的時間並不長，但總是可以從簡單的問
候寒暄中討論到一些熱門重要的研究議題，也算是此行的收穫之一。
5. 亞洲地區，台灣日本韓國的研究領域十分相信，可以透過彼此互相合作與學習，相互影響
並且相互成長。
Panel Discussion
主持會議 Session Chair
website.
Best regards,
Sukhee Cho, Secretary
Jinwoong Kim, TPC Chair
adjustment system for 3D stereo system to provide a 
flexible 3D vision. 
 
3. Proposed method 
In order to provide a flexible depth map adjustment 
for 3D image system, we proposed an adaptive object 
based depth segmentation and tuning techniques. We 
divide our method into four parts that includes adjust and 
record the depth of individual object, adjust and record 
the depth of entire image, optimize the depth effect and 
depth map processing. We describe the design procedure 
as follows. 
At first, object segmentation is performed before 
depth map adjustment. We adopt Sobel edge detection 
algorithm to enhance the edges of 3D depth image and 
use characteristics labeling to separate objects. 
According to the characteristics and distribution of edges, 
image masks are indicated. Then, the objects of depth 
map could be segmented and extracted based on masks. 
After segmentation, the local maximum and minimum 
depth values of each object are analyzed and recorded. 
This step records the depth range of each object. 
Depending on object selection, the mask marks the 
location of selected depth values. User tunes the depth 
value adopting increaser and decreaser. The tuning range 
is changed in the light of the local maximum and 
minimum depth values of each object. Then, the 
distribution of depth value is re-arranged. 
We discuss object segmentation, depth adjustment 
of an individual object above. Then, we address depth 
adjustment of entire image.  When a whole image is 
performed depth adjustment, over-tuning should be 
avoided. Therefore, the range of depth values are placed 
restrictions on 0 to 255 after adjustment. Next, object-
based linear adjustment is achieved depending on the 
limitation of tuning range. 
We define a global maximum depth value Zmax and 
minimum depth value Zmin of whole objects are searched 
during the procedure. Afterward, new global maximum 
depth value Z’max and minimum depth value Z’min are 
defined according designer’s and user’s information and 
requirement. Next, a new depth value Z’ is adjusted 
referring to original depth value Z. Depth map 
adjustment formula is indicated as 
 ( )
( )
( )
( )minmax
minmax
min
min
ZZ
ZZ
ZZ
ZZ
−
′−′=−
′−′                                          (1) 
 ( )
( ) ( ) minminminmax
minmax '''' zzz
zz
zzz +−×−
−=                            (2) 
 
Because we adopt object-based depth adjustment to 
tune the depth map, each pixel in the same object should 
be increased or decreased adopting a linear ratio. 
Therefore, we define a tuning value d as follow. 
 
ZZd −′=                                                           (3) 
 
Combining formula (2) and (3), formula (4) is 
derived as follow. 
 ( )
( ) ( ) ZZZZZZ
ZZd −′+−×−
′−′= minmin
minmax
minmax             (4) 
 
 
After obtaining the formula of depth value 
adjustment, we give an example to describe how to 
increase or decrease depth value of entire image. 
Figure 1 (a) shows depth range of an entire image 
before adjustment. After increasing depth value for 
entire image, we obtain figure 1(b). We assume there are 
three objects (Object A, B and C) in this image and one 
single background (Background D). When user increase 
the depth value of the entire image, object An increases 
depth value from 150 to 255. Next, we adjust object B 
and C according to the background D and ratio of object  
 
0 25530 100 150
Depth value
minz maxz
Object A
Bz
BD C
Cz  
 
(a) 
 
min'z max'zBz'
0 25551
Depth value
Object ABD C
170
C'z  
 
(b) 
 
Figure 1 Increasing depth value for entire image.
(a) Original depth value. (b) After increasing depth
value. 
 
0 25530 100 150
Depth value
minz maxz
Object A
Bz
BD C
Cz  
 
(a) 
 
min'z max'zBz'
0 25524 80
Depth value
Object ABD C
120
C'z  
 
(b) 
 
Figure 2 Decreasing depth value for entire image.     
(a) Original depth value. (b) After decreasing depth 
l
depth map vision for 3D display system. The proposed 
approach provides practical solution of depth map 
adjustment. 
 
Acknowledgements  
This work was supported by the Taiwan E-learning 
and Digital Archives Programs (TELDAP) sponsored by 
the National Science Council of Taiwan under Grants 
NSC 99-2631-H-027-001, NSC 99-2221-E-027-106 and 
NSC 98-2622-E-027-033-CC3. The authors gratefully 
acknowledge the Chip Implementation Center (CIC), for 
supplying the technology models used in IC design. 
References 
 
[1]   C. Ware, C. Gobrecht, and M. A. Paton, “Dynamic 
Adjustment of Stereo Display Parameters,” IEEE 
Transactions on Systems, Man and Cybernetics-Part A: 
Systems and Humans, vol. 28, no. 1, pp. 56-65, Jan. 1998. 
[2]   C. Wang and A. A. Sawchuk, “Region-based Stereo 
Panorama Disparity Adjusting,” 8th IEEE Workshop on 
Multimedia Signal Processing, pp. 186-191, Oct. 2006. 
[3]   H. Tong, Y. Zhang, and Z. Shao, “3D Remote Sensing 
Images Online Refining,” International Conference on 
Geoinformatics, pp. 1-6, Aug. 2009. 
[4]   D. Comaniciu and P. Meer, “Mean Shift: A Robust 
Approach Toward Feature Space Analysis,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, vol. 24, no. 5, pp. 603-619, 2002. 
[5]   Y. C. Fan, C. C. Lin, and W. C. Ting, “Depth Map 
Recording and Adjustment Component for 3D Stereo 
Image Storage System,” 2010 International Asia-Pacific 
Data Storage Conference, APDSC 2010, Hualien, 
Taiwan, Oct. 27~29, 2010. 
[6]   C. Fehn, “Depth-Image-Based Rendering (DIBR), 
Compression and Transmission for a New Approach on 
3D-TV,” Proc. of SPIE Stereoscopic Displays and 
Virtual Reality Systems XI, pp. 93-104, 2004. 
[7]   M. Kawakita, T. Kurita, H. Kikuchi, and S. Inoue, 
“HDTV Axi-vision Camera,” NHK Science & Technical 
Research Laboratories, Japan. 
[8]   G. Iddan and G. Yahav, “3D Imaging in The Studio,” 
Proc. SPIE, vol. 4298, pp. 48-55, 2001. 
[9]   R. Gvili, A. Kaplan, E. Ofek, and G. Yahav, “Depth 
Keying,” Proc. SPIE, vol. 5006, pp. 554–563, 2003. 
[10]   Q. Wei, “Converting 2D to 3D: A Survey,” ICT, The 
Digital Signal Processing Group of Philips Research, 
Netherlands, 2005. 
[11]   W. J. Tam and L. Zhang, “3D-TV Content Generation: 
2D-to-3D Conversion,” IEEE International Conference 
on Multimedia and Expo, pp. 1869-1872, 2006. 
[12]   A. Smolic, K. Mueller, N. Stefanoski, J. Ostermann, A. 
Gotchev, G. B. Akar, G. Triantafyllidis, and A. Koz, 
“Coding Algorithms for 3DTV—A Survey,” IEEE 
Transactions on Circuits and Systems for Video 
Technology, vol. 17, no. 11, pp. 1606-1621, Nov. 2007. 
[13]   ISO/IEC JTC 1/SC29/WG 11, “Joint Video Specification 
(ITU-T Rec. H.264 — ISO/IEC 14496-10 AVC),” JVT 
Document E146d34, Geneva, Switzerland, 2002. 
 
    
(a)                                             (b) 
 
   
(c)                                             (d) 
 
   
(e)                                             (f) 
 
Figure 3 Simulation results of depth map adjustment 
scheme for “Orbi” 3D stereo test image. (a) Original 
color image. (b) Original depth image. (c) Increase depth 
value of “Orbi” object. (d) Decrease depth value of 
“Orbi” object. (e) Increase depth value of entire image. 
(f) Decrease depth value of entire image. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：范育成 計畫編號：99-2221-E-027-106- 
計畫名稱：智慧型監視系統之研究與設計(I) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 4 4 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 8 8 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 3 100%  
研究報告/技術報告 0 0 100%  
研討會論文 13 13 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 ■洽談中 □無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
(1)學術成就與技術創新:1.以取樣為基礎的影像監控偵測技術之設計：以物件大小及其形
狀分類所建立的取樣方式，透過連續影像取樣相減技術，偵測移動物件，以進行後續的移
動預測及物件追蹤。2. 多目標物件追蹤實作：鑽研相關監視器論文所使用的演算法架構，
自行拍攝 2D 影像藉由演算法作靜態圖像的模擬，最後再以嵌入式系統(DaVinci)完成連續
影像的處理。3.亮度變化之遺失物體偵測：提出關於亮度變化與環境變化的論文，自行拍
攝 video 藉由演算法作模擬，並且以嵌入式系統(DaVinci)完成亮度與複雜環境的處理。
4. DaVinci 實作：以攝影機擷取影像，由 ARM 處理器收到訊號，分配記憶體位置，通知
DSP 抓取資料作處理，最後將結果存回記憶體，再輸出顯示。 
(2)貢獻與社會影響:1. 本計畫預計提出完整的「智慧型監視系統」演算法、架構及電路，
並實現低硬體複雜度、高視訊品質及即時視訊處理的智慧型監視系統。2. 在學術研究方
面，可建立「智慧型監視系統」演算法及嵌入式系統的設計及分析能力，並開發新型的監
視系統設計，相信對國內監視系統的發展必有助益，同時因為完整地提出一套自「數位視
訊處理」、「數位訊號處理」及「類神經網路」相關理論所衍生的演算法，進而結合嵌入式
系統與 FPGA 設計等硬體整合的技巧，實現低硬體複雜度、與即時視訊處理的「智慧型監
視系統」，對學術界中，有關於影像處理、數位訊號處理架構設計，有一定之幫助。3. 在
國家發展方面，「智慧型監視系統」是正在興起中的產業，相關聯的產業群十分的龐大，
台灣目前具有良好的環境與基礎，若能成功的完成此計畫，必定能提升國家長遠在社會治
安、居家安全維護、環境安全維護以及軍事要地的監控。因此有必要在「智慧型監視系統」
