  1 
以機器學習為基礎之鼠類生理狀態之分類系統 
A Machine Learning Approach to Classify Vigilance States in Rats 
計畫編號：NSC 99-2221-E-238 -017 
執行期限：99 年 08 月 01 日 至 99 年 09 月 31 日 
主持人：   周建興 助理教授  淡江大學電機工程系 
共同主持人： 郭昶志 助理教授 慈濟大學神經科學研究所 
計畫參與人員：戴賢榜    淡江大學電機工程系 
e-mail：chchou@mail.tku.edu.tw 
 
一、摘要 
近年來，哺乳類動物的生理狀態辨識，是生
醫科學領域中一個十分重要的研究課題。常
見的生理狀態大致上可區分為：清醒期
(awakening) 、 慢波 睡眠 期 (slow wave 
sleep)，以及快速動眼睡眠期(rapid eye 
movement sleep)三大類。雖然可以透過經
驗豐富的專業研究人員，以人工方式判讀所
收集到的每一段生理記錄資料，是屬於哪一
種生理狀態。然而，這樣的作法，必定需求
大量的研究人力。因此，此計畫發展出一套
以 EEG 訊號為基礎之自動化鼠類生理狀態
分類系統。在此計畫中，慈濟大學郭昶志教
授協助我們收集鼠類的 EEG 與 EMG 生理訊
號。透過分析 EEG、EMG 訊號的頻帶、震幅、
能量大小之資訊，搭配所拍攝的錄影資料；
將所收集的 EEG 訊號標示出其正確的生理
狀態。接著我們將依據這些已被正確標示生
理狀態的 EEG 訊號，開發出一套自動化鼠類
生理狀態分類系統。此分類系統會先將 EEG
訊號，以快速傅利葉轉換(FFT)擷取出特徵
資訊。然後以機器學習演算法，針對這些收
集到的訓練資料，自動化地建立鼠類生理狀
態分類器。目前採用的機器學習演算法包
括：結合主軸成分分析(PCA)與 k 最近鄰居
法(k-NN)等方法。對生醫學者而言，他們將
不需要人為地進行參數調整，而是透過機器
學習演算法，自動化地建立生理狀態的分類
法則。 
 
關鍵詞：生理狀態、模式識別、腦電波、機
器學習、主軸成分分析、k最近鄰居法 
 
Abstract 
Identifying mammalian vigilance states 
has recently become an important topic in 
biological science research. The vigilance 
states are usually categorized in at least 3 states, 
including slow wave sleep (SWS), rapid eye 
movement sleep (REM), and awakening. To 
identify different vigilance states, even a 
well-trained expert must spend a lot of time 
analyzing a mass of physiological recording 
data. This project proposes an automatic 
vigilance stages classification method for 
analyzing EEG signals in rats. In this project, 
the EEG signals were transferred by fast 
Fourier transform before extracting features. 
These extracted features were then used as 
training patterns to construct the proposed 
classification system. Before constructing the 
classification system, a well-experienced 
expert made sure that the training and testing 
patterns were accurately labeled the 
corresponding vigilance state. The data were 
categorized into one of three states by 
examining EEG and EMG activity and the 
locomotor behavior in video files. A 
vigilance-state classifier can be implemented 
by the combination of the principle component 
analysis (PCA) method and k-nearest neighbor 
(k-NN) method. Based on machine-learning 
algorithms, the classifier learns to approach the 
configuration that best fits the categorization 
task. Therefore, additional training in searching 
best parameters and thresholds can be avoided. 
 
Keywords: Vigilance stages, pattern 
recognition, EEG, Machine Learning, PCA, 
k-NN  
  3 
些分類演算法分析 EEG 訊號的模式，將生理
狀態分類轉化為一個模式辨認的問題；而影
響辨識率的主要關鍵就在於特徵擷取
(feature extraction)的方式[18, 30-35]以及分
類演算法的選擇。EEG 訊號的特徵擷取方式
包含了： 
(1) 訊號震幅值 (amplitude values of EEG) 
[18], 
(2) 頻帶能量(band power) [30], 
(3) 頻 譜 密 度 能 量 (PSD, power spectral 
density) [31, 32], 
(4) 自迴歸(autoregression) [33, 34], 以及 
(5) 時間頻率特徵 (time-frequency features) 
[35]。 
 
另一方面，分類演算法大致可分為五種類型： 
(1) 線性分類器 (linear classifiers) [17-19],  
(2) 類神經網路(neural networks) [20-21],  
(3) 非線性貝式分類器 (nonlinear Bayesian 
classifiers) [22-23],  
(4) 最 近 鄰 居 分 類 器  (nearest neighbor 
classifiers) [24], 以及 
(5) 混合型分類器(combinations of classifiers) 
[25-26]。 
在參考文獻[29]的比較中，倒傳遞演算法
(MLP)[37-38]與支持向量機(SVM) [38-39]等
分類方法，一般而言，擁有較準確的辨識率。
然而上述的方法都是以人類作為研究主體之
研究。所以，此研究計畫中，我們也比較這
些分類演算法，設計出合適的自動化鼠類生
理狀態分類系統。 
 
三、研究方法 
 
本計畫一開始需要收集與標記鼠類生理訊
號。這部分我們將與慈濟大學郭昶志教授合
作，郭教授會先協助我們收集鼠類的 EEG 與
EMG 生理訊號。透過分析 EEG、EMG 訊號
的頻帶、震幅、能量大小之資訊，搭配所拍
攝的錄影資料；將所收集的 EEG 訊號標示出
其正確的生理狀態。 
我們將依據這些已正確標示生理狀態的
EEG 訊號，開發出一套自動化鼠類生理狀態
分類系統。此分類系統會先將 EEG 訊號，以
快速傅利葉轉換(FFT)來擷取出特徵資訊（如
圖 2 所示）。然後以機器學習演算法，針對
這些收集到的訓練資料，自動化地建立鼠類
生理狀態分類器。目前採用的機器學習演算
法包括：結合主軸成分分析(PCA)與 k 最近
鄰居法(k-NN)、倒傳遞演算法(MLP)、支持
向量機(SVM)等方法。對生醫學者而言，他
們將不需要人為地進行參數調整，而是透過
機器學習演算法，自動化地建立鼠類生理狀
態分類系統。 
 
圖 2. 將 EEG 訊號透過 FFT 轉換成頻譜資
訊。然後再取每段頻帶的能量強度，作為訓
練資料的特徵。 
 
四、結果與討論 
 
在此研究中，我們總共收集了 918 筆鼠類
生理訊號的資料。其中 108 筆作為訓練資
料，另外的 810 筆當作訓練資訊。表一列出
所收集到的生理訊號資訊。 
表一、收集到的生理訊號 
State Number of  Training Epochs 
Number of  
Testing Epochs 
REM 36 97 
SWS 36 403 
AW 36 310 
Total 108 810 
 
在此研究中，我們使用三種機器學習演算
法，來建立鼠類生理狀態的分類系統，表二
為三種機器學習演算法的測試時的準確率。
由實驗結果顯示出，結合主軸成分分析
(PCA)[9]與 k 最近鄰居法(k-NN)的辨識率最
高。因此，此計畫最後採用結合 PCA 與 k-NN
的機器學習演算法，作為此鼠類生理狀態分
類系統的核心演算法。 
  5 
[10] C. Gottesmann, “The transition from slow-wave 
sleep to paradoxical sleep: evolving facts and 
concepts of the neurophysiological processes 
underlying the intermediate stage of sleep,” 
Neuroscience and Biobehavioral Reviews, vol. 20, 
no. 3 pp. 367-387, 1996. 
[11] P. Mandile, S. Vescia, P. Montagnese, F. Romano, 
and A.O. Giuditta, “Characterization of transition 
sleep episodes in baseline EEG recordings of adult 
rats,” Physiology and Behavior, vol. 60, no. 6, pp. 
1435-1439, Dec. 1996. 
[12] M. Steriade, D.A. McCormick, and T.J. Sejnowski, 
“Thalamocortical oscillations in the sleeping and 
aroused brain,” Science, vol. 262, no. 5134, pp. 
679-685, Oct. 1993. 
[13] C.H. Vanderwolf, “Hippocampal electrical activity 
and voluntary movement in the rat,” 
Electroencephalography and Clinical 
Neurophysiology, vol. 26, no. 4, pp. 407-418, Apr. 
1969. 
[14] E. Werth, P. Achermann, D.j. Dijk, and A.A. 
Borbely, “Spindle frequency activity in the sleep 
EEG: individual differences and topographic 
distribution,” Electroencephalography and Clinical 
Neurophysiology, vol. 103, no. 5, pp. 535-542, Nov. 
1997. 
[15] J. Winson, “Patterns of hippocampal theta rhythm 
in the freely moving rat,” Electroencephalography 
and Clinical Neurophysiology, vol. 36, no. 3, pp. 
291-301, Mar. 1974. 
[16] G.L. Westbrook, “Seizure and Epilepsy,” Principles 
of Neural Science, E.R. Kandel, J.H. Schwartz, T.M. 
Jessell, eds, New York, 2000. 
[17] K.R. Müller, M. Krauledat, G.. Dornhege, G. Curio, 
and B. Blankertz, “Machine learning techniques for 
brain–computer interfaces,” Biomedical Engineering, 
vol. 49, no. pp. 11–22, 2004. 
[18] M. Kaper, P. Meinicke, U. Grossekathoefer, T. 
Lingner, and H. Ritter, “BCI competition 2003–data 
set IIb: support vector machines for the p300 speller 
paradigm,” IEEE Trans. Biomed. Eng., vol. 51, no. 
1073–1076, Jun. 2004. 
[19] D. Garrett, D.A. Peterson, C.W. Anderson, and 
M.H. Thaut, “Comparison of linear, nonlinear, and 
feature selection methods for EEG signal 
classification,” IEEE Trans. Neural Syst. Rehabil. 
Eng., vol. 11, no. 2, pp. 141–144, Jun. 2003. 
[20] A. Hiraiwa, K. Shimohara, and Y. Tokunaga, “EEG 
topography recognition by neural networks,” IEEE 
Engineering in Medicine and Biology Magazine, vol. 
9, no. 3, pp. 39–42, Sep. 1990. 
[21] C.W. Anderson and Z, Sijercic, “Classification of 
EEG signals from four subjects during five mental 
tasks Solving Engineering Problems with Neural 
Networks,” Proc. Int. Conf. on Engineering 
Applications of Neural Networks, pp. 407-414, 
London, UK, June 1996.  
[22] K. Tavakolian and S. Rezaei, “Classification of 
mental tasks using Gaussian mixture Bayesian 
network classifiers,” Proc. IEEE Int. Workshop on 
Biomedical Circuits and Systems, pp. 9-11, Dec. 
2004. 
[23] S. Rezaei, K. Tavakolian, A.M. Nasrabadi, and S.K. 
Setarehdan, “Different classification techniques 
considering brain computer interface applications,” 
Journal of Neural Engineering, vol. 3, pp. 139–44, 
2006. 
[24] B. Blankertz, G. Curio, and K.R. Müller, 
“Classifying single trial EEG: towards brain 
computer interfacing,” Adv. Neural Inf. Process. Syst. 
(NIPS 01), vol. 14, pp. 157–164, Jan. 2002. 
[25] R. Boostani and M.H. Moradi, “A new approach in 
the BCI research based on fractal dimension as 
feature and Adaboost as classifier,” Journal of 
Neural Engineering, vol. 1, no. 4, pp. 212–217, 
2004. 
[26] U. Hoffmann, G. Garcia, J.-M. Vesin, K. Diserens, 
and T. Ebrahimi, “A boosting approach to P300 
detection with application to brain–computer 
interfaces,” Proc. of 2nd Int. IEEE EMBS Conf. 
Neural Engineering, pp. 97-100, 2005. 
[27] J.R. Wolpaw, N. Birbaumer, D.J. McFarland, G.. 
Pfurtscheller, and T.M. Vaughan, “Brain–computer 
interfaces for communication and control,” Clin. 
Neurophysiol, vol. 113, no. 6, pp. 767–791, Jun. 
2002. 
[28] A. Vallabhaneni, T. Wang, and B. He, “Brain 
computer interface Neural Engineering,” Neural 
Engineering, B. He (Ed), Kluwer/Plenum Publishers, 
pp. 85–122, 2005.  
[29] F Lotte, M. Congedo, A. Lécuyer, F. Lamarche, and 
B. Arnaldi, “A review of classification algorithms for 
EEG-based brain–computer interfaces,” Journal of 
Neural Engineering, vol. 4, no. 2, pp. R1-R13, Jun. 
2007. 
[30] G. Pfurtscheller, C. Neuper, D. Flotzinger and M. 
Pregenzer, “EEG-based discrimination between 
imagination of right and left hand movement,” 
Electroencephalogr. Clin. Neurophysiol., vol. 103, 
pp. 642–651, 1997. 
[31] S. Chiappa and S. Bengio, “HMM and IOHMM 
modeling of EEG rhythms for asynchronous BCI 
systems,” European Symposium on Artificial Neural 
Networks, ESANN, 2004. 
[32] J.R. Mill´an and J. Mouri˜no, “Asynchronous BCI 
and local neural classifiers: an overview of the 
adaptive brain interface project,” IEEE Trans. 
Neural Syst. Rehabil. Eng., vol. 11, pp. 159–161, 
2003. 
[33 W.D. Penny, S.J. Roberts, E.A. Curran and M.J. 
Stokes, “EEG-based communication: a pattern 
recognition approach,” IEEE Trans. Rehabil. Eng., 
vol. 8, pp. 214–215, 2000. 
[34] G.. Pfurtscheller, C. Neuper, A. Schlogl and K. 
Lugger, “Separability of EEG signals recorded 
during right and left motor imagery using adaptive 
autoregressive parameters,” IEEE Trans. Rehabil. 
Eng., vol. 6 pp. 316–325, 1998. 
[35] T. Wang, J. Deng and B. He, “Classifying 
EEG-based motor imagery tasks by means of 
  1 
附件一 
 
發表於 Expert Systems and Applications 期刊 
 
 
A Machine Learning Approach to Classify Vigilance 
States in Rats 
 
Zong-En Yu1, Chung-Chih Kuo2,3, Chien-Hsing Chou4*,  
Chen-Tung Yen5, Fu Chang6 
 
 
Department of Electrical Engineering, National Taiwan University, Taiwan1 
Institute of Neuroscience, Tzu Chi University, Taiwan 2 
Institute of Phsyiological and Anatomical Medicine, Tzu Chi University, Taiwan 3 
Department of Electrical Engineering, Tamkang University, Taiwan 4* 
Institute of Zoology, National Taiwan University, Taiwan5 
Institute of Information Science, Academia Sinica, Taipei, Taiwan6 
Author's personal copy
classiﬁcation method that automatically builds the classiﬁer using
the collected data.
To classify vigilance states, many classiﬁcation algorithms (Acir,
2005; Acir & Güzelis, 2004; Anderson & Sijercic, 1996; Blankertz,
Curio, & Müller, 2002; Boostani & Moradi, 2004; Duman, Erdamar,
Erogul, Telatar, & Yetkin, 2009; Garrett, Peterson, Anderson, &
Thaut, 2003; Güler, Übeyli, & Güler, 2005; Hiraiwa, Shimohara, &
Tokunaga, 1990; Hoffmann, Garcia, Vesin, Diserens, & Ebrahimi,
2005; Kaper, Meinicke, Grossekathoefer, Lingner, & Ritter, 2004;
Müller, Krauledat, Dornhege, Curio, & Blankertz, 2004; Rezaei,
Tavakolian, Nasrabadi, & Setarehdan, 2006; Subasi, 2005; Subasi,
2007; Tavakolian & Rezaei, 2004; Yildiz, Akin, Poyraz, & Kirbas,
2009; Übeyli, 2008) have been proposed to automatically estimate
the class of EEG-data as represented by feature vectors. Previous
studies provide an overview of these classiﬁcation algorithms
(Lotte, Congedo, Lécuyer, Lamarche, & Arnaldi, 2007; Vallabhaneni,
Wang, & He, 2005;Wolpaw, Birbaumer, McFarland, Pfurtscheller, &
Vaughan, 2002). These studies consider an EEG-based classiﬁcation
system as a pattern recognition system, and its performance de-
pends on both the features and the classiﬁcation algorithm em-
ployed. Lotte et al. compared these classiﬁcation algorithms and
divided them into ﬁve different categories: linear classiﬁers (Gar-
rett et al., 2003; Kaper et al., 2004; Müller et al., 2004), neural net-
works (Acir, 2005; Acir & Güzelis, 2004; Anderson & Sijercic, 1996;
Güler et al., 2005; Hiraiwa et al., 1990; Yildiz et al., 2009), nonlin-
ear Bayesian classiﬁers (Rezaei et al., 2006; Tavakolian & Rezaei.,
2004), nearest neighbor classiﬁers (Blankertz et al., 2002), decision
tree (Duman et al., 2009) and combinations of classiﬁers (Boostani
& Moradi, 2004; Hoffmann et al., 2005; Subasi, 2005; Subasi, 2007;
Übeyli, 2008). However, most proposed methods are aimed at hu-
man as experimental subjects. Using human as experimental sub-
jects might limit the researchers to perform some additional
experiments, e.g. drug test, genetically modiﬁed, etc. For this rea-
son, rats are used as experimental subjects in this study.
In this study, vigilance stages are mainly categorized states into
the following three states: the awake (AW) state, slow wave sleep
(SWS) state, and rapid eye movement sleep (REM) state. During the
AW state, the animal exhibits low amplitude and high frequency
EEG results. Some investigators distinguish active awake from
quite awake by high EMG activity. The spectrum of EEG in the
AW state includes high alpha (8–13 Hz) and gamma (20–50 Hz)
power levels (Westbrook, 2000). The SWS state, which is deﬁned
by a high amplitude and low frequency EEG, begins with a sleep
spindle and is dominated by delta (0.5–4 Hz) waves. In the REM
state, the animal also shows the low amplitude and high frequency
EEG results characteristic of the AW state. However, the animal is
atonic and shows ﬂat EMG activity. High activity in the theta and
gamma bands are also characteristics of REM state (Louis et al.,
2004; Robert et al., 1999; Westbrook, 2000), as Fig. 1 shows.
The current study also adopts machine learning techniques to
develop an automatic classiﬁer to classify the vigilance states of
rats. The typical signals of one channel EEG recording in three dif-
ferent states were labeled in advance. A machine-learning based
classiﬁcation system was then constructed to recognize the pat-
terns in the test data for different states. With the proposed meth-
od, three types of vigilance state can be categorized automatically
without human intervention.
There are two advantages to the proposed method. First, it re-
duces the amount of effort required to tune classiﬁer parameters
or thresholds. In machine-learning based approaches, the classiﬁer
itself learns the parameters to achieve the best categorizing of the
training samples. Second, previous classiﬁcation procedures in-
volve high dimensional computations and some non-linear optimi-
zation techniques that may not be intuitive for users. This topic is
another focus of the proposed method, as it visualizes the training
patterns and the predication results by projecting the high dimen-
sional features into a 3-D space using the PCA algorithm. The pro-
posed approach provides more meaningful information to sleep
researchers than those that merely provide classiﬁed labels.
2. The proposed classiﬁcation method
The proposed classiﬁcation method consists of two key pro-
cesses: (1) feature extraction processes which convert the patterns
in the EEG signal into a series of variables for classiﬁcation, and (2)
pattern classiﬁcation processes that learn to categorize the vigi-
lance states of rats from variables extracted in the feature extrac-
tion unit. Fig. 2 illustrates this process.
Fig. 1. (a) The rat in our experiment. (b) The EEG and EMG signals of three vigilance states. (c) The spectrum of three vigilance states.
10154 Z.-E. Yu et al. / Expert Systems with Applications 38 (2011) 10153–10160
Author's personal copy
that EMG signals are not a good feature to differentiate vigilance
states. That is why the automatic classiﬁcation system in this study
is based on EEG signals only.
2.2. Feature extraction
This study computes the EEG spectrum using the FFT method
with a 4-s window size, a frequency range from 0 to 50 Hz, and a
resolution of 32 frequency bands, as Fig. 4 shows. The power of
each frequency band is normalized by the sum of power for each
frequency band. As a result, the EEG in each epoch can be trans-
formed into 32 numbers and used as the classiﬁer input.
Before constructing the classiﬁcation system, a well-experi-
enced expert made sure that the training and testing patterns were
accurately labeled the corresponding vigilance state. The data were
categorized into one of three states by examining EEG and EMG
activity and the locomotor behavior in video ﬁles. A total of 108
and 810 EEG epochs were used as the training and testing patterns,
respectively. Table 1 lists the number of epochs in each state.
2.3. Pattern classiﬁcation methods
A vigilance-state classiﬁer can be implemented by one of the
following machine-learning based approaches: condensed nearest
neighbor (CNN) (Hart, 1968), generalized condensed nearest
neighbor (GCNN) (Chou, Kuo, & Chang, 2006), multilayer percep-
tron (MLP) (Rumelhart, Hinton, & Williams, 1986; Werbos, 1974),
and support vector machine (Cortes & Vapnik, 1995; Vapnik,
1995), etc. Although the accurate prediction of vigilance states is
an important topic, visualizing dynamic state-changing informa-
tion is also important to biological scientists. In many cases, sleep
researchers need more than accurate predictions. They need to
know the changing dynamic between states to make more infer-
ences and verify their predictions. To simply reach this goal, the
ﬁrst step is providing the prediction result of each testing epoch
on the 2-D or 3-D Euclidean space. This is helpful to biological sci-
entists by visualizing the data patterns. The classiﬁer in this study
uses the PCA (Pearson, 1901) and the k-nearest neighbor (k-NN)
(Dasarathy, 1991; Levine, Lustick, & Saltzberg, 1973; O’Callaghan,
1975) as learning algorithms. The most important information
for classiﬁcation in the spectrum of EEG can be extracted by apply-
ing PCA and k-NN. Then, the process of projecting the high-dimen-
sional features into a lower-dimension feature space provided a
way to visualize these data patterns.
PCA is an unsupervised dimension reduction technique com-
monly used to project high dimensional vectors into a lower
dimensional space while maintaining as much variability as possi-
ble. After PCA projection, each data pattern is classiﬁed by the k-
NN method, which matches each testing pattern with all possible
training patterns and considers k-nearest samples, kP 1, in its
classiﬁcation decision. The asymptotic error rate of k-NN is less
than twice the Bayes rate (Cover & Hart, 1967). In many applica-
tions, the k-NN method achieves good accuracy rates, and is also
easily implemented. To determine the appropriate number of com-
ponents used in PCA and the parameter k in k-NN, we perform a
cross validation operation that partitions all training samples em-
ployed in the experiment into ﬁvefold. We conducted ﬁve tasks,
using fourfold in each task as training data to construct k-NN clas-
siﬁers and the remaining fold as testing data. For a speciﬁc number
of PCA components, we selected the value of k that maximizes the
average accuracy rate in the ﬁve tasks. Fig. 5 shows the average
accuracy curve for different numbers of components. The highest
accuracy occurs when using six components, but the proposed
method also achieves comparable accuracy using three compo-
nents. This ﬁgure also illustrates that the top three components
achieve a 96.4% average prediction accuracy rate for the training
patterns. Finally, these top three components are used to project
the data patterns into a 3-D subspace. This approach makes it pos-
Fig. 4. The EEG spectrum obtained by applying FFT.
Table 1
The number of epochs in each state.
State Number of
training epochs
Number of
testing epochs
REM 36 97
SWS 36 403
AW 36 310
Total 108 810
10156 Z.-E. Yu et al. / Expert Systems with Applications 38 (2011) 10153–10160
Author's personal copy
3.3. Comparison with Louis et al.’s algorithm
This study also compares the proposed methods with the auto-
mated sleep staging algorithm proposed by Louis et al. (2004).
Louis et al.’s algorithm divides the EEG into ﬁve frequency bands:
delta (d; 1.5–6 Hz), theta (H; 6–10 Hz), alpha (a; 10.5–15 Hz), beta
(b; 22–30 Hz), and gamma (c; 35–45 Hz). This algorithm makes
predictions according to the following steps:
Step 1: If the EMG amplitude is greater than the threshold T0,
output AW.
Step 2: If the EEG amplitude ratio (d  a)/(b  c) is greater than
the threshold T1, output SWS.
Step 3: If the EEG amplitude ratioH2/(d  a) is greater than the
threshold T2, then output ‘‘REM’’. Otherwise, output AW.
Because our comparisons do not consider the EMG amplitude,
Step 1 is skipped here. The threshold T1 and T2 should be decided
before prediction. The value range of the threshold T1 is set to
{T1 = 0.1, 0.5, 0.75, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 2}, and the value
range of threshold T2 is set to {T2 = 0.1, 0.5, 0.75, 1, 1.1, 1.2, 1.3,
1.5, 1.8, 1.9, 2, 2.1}. Then, the optimal thresholds (T1, T2) are set as
(1.1, 2) after applying ﬁvefold cross validation to the training pat-
terns. Table 4 shows that Louis et al.’s algorithm ultimately obtains
an 81.61% accuracy rate for the testing patterns.
4. Discussion
The results of this study indicate that principle components can
effectively classify the sleep states of rats based on EEG spectrum
data. In fact, each component is the weighted sum of the power
values at different frequency bands. Further analysis of the weight-
ing of each frequency band may reveal more information about
how each component operates, making it possible to differentiate
vigilance states. The box plots in Fig. 8(a) and (b) summarize the
ﬁrst and second principle components of the testing samples,
respectively. The box plot for the ﬁrst component shows no overlap
between the conﬁdence regions for the SWS and AW states or the
SWS and REM states. A similar situation occurs in the second com-
ponent. This indicates that either the ﬁrst or second component
acts as a key factor in distinguishing SWS from AW and REM.
This study also analyzes the spectrum weight curves of two
principle components to explain how the vigilance states are clas-
siﬁed by the power spectrum. The spectrum weight curve in
Fig. 9(a) indicates that the ﬁrst component performs a weighted
sum on the power of different frequency band. In this case, the
smaller negative weight values appear at 1.6 and 11.2 Hz, and
the larger negative weight value appears at 32 Hz. In another spec-
trum weight curve (Fig. 9(b)), the second component computes the
contrast between the power of the low frequency band and high
frequency band. The larger negative values of weight appear at
3.2 and 4.8 Hz, and the larger positive value occurs at 32 Hz. West-
brook’s research results (Westbrook, 2000) (see Section 2.1) shows
that the major difference of spectrum pattern in the delta (0.5–
4 Hz) and gamma bands (20–50 Hz) are the key features for classi-
fying three vigilance stages. The analysis of the method proposed
in this study agrees with their results.
5. Conclusion
This paper proposes a machine-learning based classiﬁer for
identifying the vigilance states in rats. Users may beneﬁts from this
classiﬁer in two ways. (1) Human intervention in tuning the
threshold and system parameters is not necessary, as this has been
replaced by the machine learning algorithm. (2) The classiﬁcation
results can be visualized by projecting the data patterns into a 3-
D space using principle component analysis, allowing users to eval-
uate the validity of their classiﬁcation results. Moreover, the spec-
Table 3
Testing performance of three machine learning methods.
Machine learning method Testing accuracy rate (%)
PCA+k-NN 95.43
MLP (# of hidden node =15) 87.16
SVM 94.48
Table 4
Testing performance of two automatically vigilance
stages classiﬁcation methods.
Method Testing accuracy
rate (%)
PCA+k-NN 95.43
Louis et al.’s algorithm 81.61
Fig. 8. (a) Box plot of the ﬁrst principle component. (b) Box plot of the second
principle component.
10158 Z.-E. Yu et al. / Expert Systems with Applications 38 (2011) 10153–10160
Author's personal copy
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal
representations by error propagation. In D. E. Rumelhart & J. L. McClelland
(Eds.), Parallel distributed processing. Cambridge, MA: MIT Press.
Steriade, M., McCormick, D. A., & Sejnowski, T. J. (1993). Thalamocortical oscillations
in the sleeping and aroused brain. Science, 262(5134), 679–685.
Subasi, A. (2005). Automatic recognition of alertness level from EEG by using neural
network and wavelet coefﬁcients. Expert Systems with Applications, 36(6),
701–711.
Subasi, A. (2007). EEG signal classiﬁcation using wavelet feature extraction
and a mixture of expert model. Expert Systems with Applications, 32(4),
1084–1093.
Tavakolian, K., & Rezaei, S. (2004). Classiﬁcation of mental tasks using Gaussian
mixture Bayesian network classiﬁers. In Proc. IEEE int. workshop on biomedical
circuits and systems, (pp. 9–11).
Übeyli, E. D. (2008). Wavelet/mixture of experts network structure for EEG signals
classiﬁcation. Expert Systems with Applications, 34(3), 1954–1962.
Vallabhaneni, A., Wang, T., & He, B. (2005). Brain computer interface neural
engineering. In B. He (Ed.), Neural engineering (pp. 85–122). Kluwer/Plenum
Publishers.
Vanderwolf, C. H. (1969). Hippocampal electrical activity and voluntary movement
in the rat. Electroencephalography and Clinical Neurophysiology, 26(4), 407–418.
Vapnik, V. (1995). The nature of statistical learning theory. New York: Springer.
Werbos, P. J. (1974). Beyond regression: new tools for prediction and analysis in the
behavioral sciences. Doctoral dissertation, Harvard University.
Werth, E., Achermann, P., Dijk, D. J., & Borbely, A. A. (1997). Spindle frequency
activity in the sleep EEG: Individual differences and topographic distribution.
Electroencephalography and Clinical Neurophysiology, 103(5), 535–542.
Westbrook, G. L. (2000). Seizure and epilepsy. In Kandel, E. R., Schwartz, J. H., &
Jessell, T. M. (Eds.). Principles of neural science, New York.
Winson, J. (1974). Patterns of hippocampal theta rhythm in the freely moving rat.
Electroencephalography and Clinical Neurophysiology, 36(3), 291–301.
Wolpaw, J. R., Birbaumer, N., McFarland, D. J., Pfurtscheller, G., & Vaughan, T. M.
(2002). Brain–computer interfaces for communication and control. Clinical
Neurophysiology, 113(6), 767–791.
Yildiz, A., Akin, M., Poyraz, M., & Kirbas, G. (2009). Application of adaptive neuro-
fuzzy inference system for vigilance level estimation by using wavelet-entropy
feature extraction. Expert Systems with Applications, 36(4), 7390–7399.
10160 Z.-E. Yu et al. / Expert Systems with Applications 38 (2011) 10153–10160
A Novel Feature Selection Algorithm by Using False Feature 
 
Yu-Xiang Zhao1, Yi-Zeng Hsieh2, Hsien-Pang Tai3, Chien-Hsing Chou3* 
Department of Computer Science & Information Engineering, National Quemoy University, Taiwan1 
Department of Computer Science & Information Engineering, National Central University, Taiwan2 
Department of Electrical Engineering, Tamkang University, Taiwan3* 
Email: chchou@mail.tku.edu.tw 
 
 
Abstract: Feature selection plays a critical role in pattern classification. Of the various feature selection methods, 
sequential floating search (SFS) is well known and widely adopted. In this paper, we propose a feature selection 
method combining feature ranking and SFS. In feature ranking, we adopted the new idea of false feature to rank 
features based on their importance, and then applied SFS to features that are less important and of lower rank. By 
doing so, we not only overcame issues with the original SFS but also extracted more critical features. 
 
Key-Words: feature selection, sequential floating search, false feature, pattern classification 
 
1   Introduction 
In recent years, many feature selection methods 
have been proposed, [1-10] and many researchers 
have collected and compared these methods. As 
shown in Fig. 1, a feature selection algorithm 
involves the following four important steps [1]. First, 
create a feature subset generated from an original 
feature set, then evaluate the subset and analyze the 
evaluation to see if it matches the stopping criterion. 
If it does, proceed the result validation, otherwise, 
regenerate a new subset for evaluation and 
re-estimate it until the stopping criterion matches. 
 
Fig. 1. Four important steps for feature selection 
algorithm. 
 
Of these methods, the sequential forward floating 
search (SFFS) and sequential backward floating 
search (SBFS) algorithms of Pudil et al. are the most 
widely adopted [8]. In the two methods, users first set 
up a criterion function (e.g. the classification 
accuracy) and search for the best feature subset by 
constantly adding or removing a certain feature. 
When SFFS (or SBFS) evaluates the criterion 
function after adding (or removing) a feature from 
the feature set, the same performance of criterion 
function may be derived by different features. 
However, users do not have additional information to 
decide which feature to first add (or remove). For 
example, to classify the Chinese characters “䔻” and 
“⮶” in Fig. 2, we adopt SBFS and the classification 
accuracy as the criteria function for SBFS. By 
removing a single feature in the character area of a, b 
or c, the accuracy rate can be increased in the same 
way. However, no additional information was 
provided to the users to distinguish the feature in 
which character areas is to be first removed. 
Therefore, in the end, the users have to decide which 
one of the character areas need to be first removed by 
themselves. We noticed this issue often arises in 
classification problem, for example, character 
recognition.    
 
Fig. 2. Removing either one of the features in 
character area of a, b or c obtains the same accuracy 
rate.  
 
In order to overcome the problems listed above, 
we proposed a hybrid feature selection algorithm. 
Feature ranking is the first stage of this algorithm and 
feature selection is the second. In this paper, we 
propose a new feature ranking algorithm based on the 
idea of false feature. By applying the new feature 
ranking algorithm, each feature is ranked according 
to its importance, and then users have enough 
information to decide which feature in the character 
area needs to be first removed when they encounter 
the problems listed in Fig. 2. In addition, in the stage 
of feature selection, we chose those less important 
features to process the SBFS algorithm. As a result, 
World Academy of Science, Engineering and Technology 80 2011
926
 3   Experimental Results 
In the experimental simulations, we used the 
SBFS and HSBFS algorithms to do a comparison. 
The parameters for the HSBFS algorithm were M=5, 
n=64, and K=640. The datasets for the experiment 
were combination of three similar Chinese characters 
selected from the ETL9b [13] handwritten Chinese 
characters database, each class involves 200 
handwritten Chinese characters. We generated two 
different datasets for simulation, as shown in Table 1. 
In the experimental simulations, all the characters are 
uniformly partitioned into three datasets; training 
dataset, development dataset and testing dataset. To 
extracted feature subsets by feature selection 
methods, we used the training dataset and NN 
[11-12] to build a classifier, and then used the 
development dataset to estimate the accuracy rate as 
criterion function. Finally, we integrated the training 
dataset with the development dataset and extracted 
the feature subset to build a classifier for testing, and 
obtained the validation result from the testing dataset.  
 
Table 1. Two datasets generated for experimental 
simulations. 
 
Number 
of Class 
Character 
Label 
Number 
of Data 
Dataset 1 2 ⮹, ⮶  400 
Dataset 2 3 ⮹, ⮶, 䔻 600 
 
For each character image, we adopted the 
non-linear normalization technique [14] to normalize 
it to a size of 64×64. Each character image was then 
divided into 16×16 blocks, as shown in Figure 3. 
Each block comprised 4×4 pixels, and compiled the 
numbers of the black pixels in each blocks to be the 
features. Finally, each of the datasets is given the 
feature with 256 dimensions, and each dimension 
feature represents the information of a certain area in 
a character image. Therefore, if a certain block is the 
critical area for classifying, it should be extracted by 
the feature selection method as well. Next, we 
present the results of the experimental simulation. 
 
 
Fig. 3. Character Image divided into 16x16 block. 
Dataset 1:  
Dataset 1 comprised the Chinese characters ‘⮹’ 
and ‘⮶’. Table 2 represents the simulation results of 
Dataset 1. We can see that each of the two feature 
selection algorithms is able to improve the accuracy 
rate after selecting its feature subset. SBFS extracted 
fewer features, but we obtained a higher accuracy 
rate from HSBFS. Next, we display the feature 
subsets selected by two algorithms in Figure 4. In this 
figure if any feature is selected, we label the 
corresponding character area in pink. Compared 
HSFBF with SBFS, it can be clearly seen that most of 
the features extracted by HSBFS are mainly located 
in the bottom of the central area, as shown in Fig. 
4(b). Obviously, it is the critical area to classify the 
characters ‘⮹ ’ and ‘⮶ ’. This is reasonable 
explanation why HSBFS obtained a higher accuracy 
rate than SBFS. 
 
Table 2. Simulation results from Dataset 1 
 
  
Dataset 1 
Without 
Feature 
Selection 
SBFS HSBFS 
Number of 
Selected 
Features 
256 21 35 
Accuracy of 
Development 
Data (%) 
69.92 90.22 98.05 
Accuracy of 
Test Data 
(%) 
68.66 82.09 91.04 
World Academy of Science, Engineering and Technology 80 2011
928
distributions,” IEEE Trans. Information Theory 
vol. 13 no. 1, pp. 21-27, 1967. 
[12] R. O. Duda, P. E. Hart, D. G. Stork, Pattern 
Classification, Wiley, New York, 2001. 
[13] R. Collobert, S. Bengio, and J. Mariéthoz, 
“Torch: a modular machine learning software 
library,” Technical Report IDIAP-RR 02-46, 
IDIAP, 2002. 
[14] H. Yamada, K. Yamamoto, and T. Saito, “A 
nonlinear normalization method for handprinted 
Kanji character recognition – line density 
equalization,” Pattern Recognition, vol. 23, no. 9, 
pp. 1023-1029, 1990.  
World Academy of Science, Engineering and Technology 80 2011
930
  4 
參與國際會議之心得報告 
 
一、 參加會議經過 
2011 電機電腦電子通訊國際研討會 (2011 International Conference on Electrical, Computer, 
Electronics and Communication Engineering)為一個大型國際研討會議，主辦單位是國際科學
工程與科技學會(World Academy of Science, Engineering and Technology)，此會議於法國之巴
黎(Paris)舉行，此會議的討論內容除了模式識別與影像處理等研究領域外，還包括電力系
統、多媒體、電腦網路與視訊技術等會議於此地一同進行。可是說是一個概括電子電機與
資訊各領域的大型研究會。此次會議要發表之論文共計有 167 篇，其中發表論文者來自許
多個不同的國家。所有論文分為 3 個會場進行發表，共計 3 天，個人所要發表的論文被分
至第三天中午。 
  
會議現場 
 
二、與會心得 
 由於個人的研究領域偏向模式識別與影像分析辨認，所以與會期間，都是選擇與此兩
方面有關的議程，予以聆聽，發現許多在這兩領域學有專精的學者均有參與此次盛會，而
他們所發表的論文的確又是十分可貴，點出了未來的發展，而不是專注於細節的改良。除
此之外，從整個會議議程看來，有關雲端技術與手機軟體開發等領域受到全球學者越來越
多的重視及投入，直得我們探討及學習。 
 
三、建議 
 個人在與會空檔時間，亦參訪巴黎的羅浮宮與奧賽美術館。個人覺得法國當局對藝術
文物的保存，以及展覽館內的規劃確有其獨到之處，此經驗值得我們借鏡。此外，與此次
國內的學者教授較少參與此學術會議，顯示國內在這些領域較少人投入，若在經費許可下，
  6 
附件三 
 
Submitted to “International Journal of Innovative 
Computing, Information and Control”期刊接受審查中 
 
 
Modified Sequential Floating Search Algorithm with a 
Novel Ranking 
 
Chien-Hsing Chou1*, Yi-Zeng Hsieh2 and Chi-Yi Tsai1 
 
Department of Electrical Engineering, Tamkang University, Taiwan1* 
 
Department of Computer Science & Information Engineering, National Central 
 
  
 
 
lead to certain diseases from the data of microarray [11]. Another example is text 
categorization, in which feature selection makes it possible to extract the keywords 
contributing to text classification [12-15]. In addition, feature selection only selects the 
features that users are most interested in, but also save time in training and testing the 
classifier, and reduce the memory space required for data storage. 
Researchers have proposed many feature selection methods in recent years [9-24] 
while other have tested and compared these methods [12-20]. A feature selection algorithm 
involves the following four important steps (Fig. 1) [16]. First, create a feature subset 
generated from an original feature set, then evaluate the subset to see if it matches the 
stopping criterion. If it does, proceed to result validation, otherwise, regenerate a new 
subset for evaluation and re-estimate it until the stopping criterion is met. 
 
 
FIGURE 1. Four important steps in a feature selection algorithm. 
 
Guyon and Elisseeff [17] categorized feature selection algorithms as wrappers, filters, 
and hybrid algorithms. The following discussion provides a brief introduction to these 
feature selection algorithms. 
(1) Wrappers: The wrappers method extracts the best feature subset by adopting a specific 
searching strategy and performing constant evaluation [25-28]. These strategies 
include sequential floating search [25], adaptive floating search [26], branch and 
bound [27], and genetic algorithm [28], etc. 
(2) Filters: The filters method ranks the importance of the features according to their 
statistical criteria or information-theoretic criteria [29-30]. This method usually uses 
information gain or 2X  statistic to extract the features in text categorization [12-14].  
(3) Hybrid: The hybrid method extracts several feature subsets by combining both filters 
and wrappers through an independent feature evaluation method. The hybrid method 
extracts the best feature subset by processing the classification algorithm. This strategy 
is performed repeatedly until it is unable to obtain any better feature subsets [31-32]. 
The sequential forward floating search (SFFS) and sequential backward floating search 
(SBFS) algorithms proposed by Pudil et al. are the most widely adopted of these three 
methods [25]. In these two methods, users first set up a criterion function (e.g., the 
classification accuracy) and search for the best feature subset by constantly adding or 
removing a certain feature. When SFFS (or SBFS) evaluates the criterion function after 
adding (or removing) a feature from the feature set, the same performance of criterion 
function may be derived by different features. However, users do not have the necessary 
information to decide which feature to add (or remove) first. For example, to classify the 
  
 
 
 
FIGURE 3. Four similar Chinese characters. The pink block represents the critical area for 
classifying the corresponding character. 
 
This study proposes a one-against-all strategy for feature selection to overcome this 
problem. The key point of this strategy is to partition the multi-class classification into 
several binary sub-classifications. Assuming that we have N classes, it is possible to 
generate N binary sub-classifications from the training dataset. An individual feature subset 
can be extracted for each binary sub-classification. During the result validation stage, a 
more appropriate feature subset is used for the corresponding sub-classification to improve 
the testing performance.   
 
2. The Proposed Hybrid Sequential Backward Floating Search. This section presents a 
hybrid sequential backward floating search (HSBFS) algorithm. This algorithm includes 
two stages: feature ranking and feature selection. HSBFS gradually extracts a critical 
feature subset through the iterative process of feature ranking and feature selection. This 
study uses the nearest neighbor (NN) method as the classification method, and uses the 
accuracy rate as the criterion function for evaluation. The following section introduces 
detailed steps of the HSBFS. 
 
2.1 Feature Ranking Method with False Feature. This study proposes a novel feature 
ranking method based on false features. The purpose of this approach is to rank the 
importance of all the features and select those of lesser importance to further execute the 
SBFS algorithm. The steps are listed as below: 
 
Step 1: Assume that the original feature set contains D features. Then put m false features 
artificially into the D original features as the new full feature set. “False feature” means the 
feature is set to 0 in this case. The full feature set includes D+m features, and the value of m 
can be customized by the users. It was set as m=5 in this paper.  
Step 2: Randomly generate K feature subsets from the full feature set. Denote each feature 
subset as S, which includes n features to be selected randomly from the full set (a feature 
can be reselected from the full set). The values K and n can be customized by users, and 
were set at K=640 and n=0.25D in this paper. 
Step 3: For a feature subset S, the classifier is constructed by nearest neighbor method and 
obtains a corresponding accuracy rate Acc(S).   
Step 4: After obtaining accuracy rates Acc(S) for all feature subsets, evaluate the 
importance of each feature f (includes m false features) using the following formula, 
  
 
 
classification or multi-class classification. Experimental simulations indicate that the 
greater the number of classes for multi-class classification, the greater the difficulty in 
feature selection. Therefore, this study adopts a “one-against-all” strategy to overcome this 
issue. This strategy has been successfully adopted for pattern classification techniques such 
as SVM [7-8]. Therefore, we used the same concept to design a feature selection strategy to 
address the multi-class classification problem.  
Assuming that there are N classes in the dataset, generate N binary sub-classifications, 
where each binary sub-classification (class i for instance) involves the dataset of class i and 
class non-i. Class non-i represents all other data patterns not belonging to class i. Next, use 
HSBFS and NN to extract individual feature subset to each sub-classification. Because the 
selected feature subsets of each sub-classification are different from the feature amounts, 
this study proposes a process to classify data pattern in the testing stage (Fig. 4). We sent 
the data pattern x to each independent sub-classifier to calculate the corresponding 
membership(i). The formula is listed below, 
 
inoni
inon
dd
d
imembership
−
−
+
=)(      (2) 
where ||||min jiClassxi xxd j −= ∈ ,    (3) 
||||min kinonClassxinon xxd k −= −∈−     (4) 
 
where id  is the shortest distance between x and the data pattern belongs to class-i in the 
ith sub-classifier, and inond −  is the shortest distance to class non-i. The bigger the 
membership, the higher the possibility that x belongs to class i. Select the class with the 
largest membership as the classified class to data pattern x. The formula is shown as below: 
 
)(max*
,,1
imembershipArgi
Ni K=
=     (5) 
where i* is the classified class to data pattern x. 
 
 
FIGURE 4. The process of classifying data pattern x in the testing stage. 
 
 
  
 
 
For each character image, we adopted the non-linear normalization technique [35] to 
normalize it to a size of 64×64. Each character image was then divided into 16×16 blocks 
(Fig. 6). Each block consisted of 4×4 pixels, and compiled the numbers of the black pixels 
in each blocks to be the features [36]. If a block contains more black pixels, then the 
corresponding feature value is large. For examples, Fig. 6 shows that the feature value of 
block ‘a’ is 10 because more than half of pixels are black pixels within block ‘a.’ And the 
feature value of block ‘b’ is 0, because there are no black pixels within block ‘b.’ A 
character image finally consists of 256 features (i.e., 16×16 blocks), whose values range 
from 0 to 16; each feature represents the information of a certain area (block) in a character 
image. Therefore, if a certain block is the critical area for classifying (e.g., block ‘a’), the 
feature selection method should extract the corresponding feature. On other hand, the 
corresponding feature would not be extracted if the block is not the critical area for 
classifying (e.g. block ‘b’). The following discussion presents the results of the 
experimental simulation. 
 
 
 
 
FIGURE 6. Character Image divided into 16x16 blocks. 
 
Dataset 1: Dataset 1 included the Chinese characters ‘太’ and ‘大.’ Figure 5 shows some 
examples of two Chinese characters from the ETL9b database. For further analysis, the 
bottom of the central area of character image is the critical area to classify the characters 
‘太’ and ‘大’ (Fig. 7). If a feature selection method extracted more critical features from 
this area, the recognition result should be more accurate. Table 2 shows the simulation 
results of Dataset 1. Each of the two feature selection algorithms improved the accuracy 
rate after selecting its feature subset. SBFS extracted fewer features, but HSBFS obtained a 
higher accuracy rate than SBFS. Besides, HSBFS also spent less time than SBFS in seeking 
the corresponding feature subset.  
 
  
 
 
TABLE 3. Simulation results from Dataset 2. 
Dataset 2 Without Feature Selection SBFS HSBFS 
Number of Selected 
Features 256 44 67 
Accuracy of Test Data (%) 63.5 69.5 79.5 
Computational Time for 
Feature Selection  1.8 hours 0.8 hour 
 
  
(a)   (b) 
FIGURE 9. Feature subset selected by (a) SBFS; (b) HSBFS. 
 
Because the classes have been increased to three groups in this problem, both methods 
extracted more features. Therefore, we adopted HSBFS and the one-against-all strategy. 
Table 4 shows the result after applying HSBFS in combination with the one-against-all 
strategy. The one-against-all strategy individually extracted 34, 59, and 39 features for the 
‘太’, ‘大’, and ‘犬’ sub-classifications, respectively. Figure 10 shows the selected feature 
subsets of these three classes. The feature subset selected to discriminate class ‘太’ from 
another two classes ‘大’ and ‘犬’ is mainly located in the lower central area (see Fig. 10(a)); 
the features mainly located in the upper right part are selected for the feature subset to 
discriminate class ‘犬’ from ‘太’ and ‘大’ (see Fig. 10(c)). The feature subset selected to 
discriminate class ‘大’ from the other two classes covers the two above-mentioned areas 
(see Fig. 10(b)). These results perfectly matched our intuition and expectations. In addition, 
the accuracy rate increased from 79.5% to 88%.  
 
TABLE 4. Simulation result of Dataset 2 by applying one-against-all strategy. 
Dataset 2 HSBFS with one-against-all strategy 
Class 太 Class 大 Class 犬 
Number of Selected 
Features 
34 59 39 
Accuracy of Test Data (%) 88 
Computational Time for 
Feature Selection 1.4 hours 
 
  
 
 
  
(a)           (b) 
FIGURE 11. Feature subset selected by (a) SBFS; (b) HSBFS. 
 
  
(a)              (b)              (c) 
  
(d)              (e) 
FIGURE 12. Feature subset selected for (a) Class ‘太’; (b) Class ‘大’; (c) Class ‘犬’; (d) 
Class ‘天’; (e) Class ‘木’. 
 
Dataset 4: Dataset 4 includes 10 similar characters. Table 7 shows the simulation results 
from Dataset 4. There was a greater number of selected features and not such a significant 
increase in the accuracy rate of SBFS or HSBFS. After adopting the one-against-all strategy 
(see Table 8), the accuracy rate increased from 80.51% to 88.61% and there were less 
selected features. For classes ’人’ and ’土,’ only 15 and 9 features were selected to deal 
with the corresponding sub-classifications, respectively. 
 
TABLE 7. Simulation results from Dataset 4.  
Dataset 4 Without Feature Selection SBFS HSBFS 
Number of Selected 
Features 256 138 185 
Accuracy of Test Data (%) 77.06 77.51 80.51 
Computational Time for 
Feature Selection  68 hours 12 hours 
 
  
 
 
[8] C. Cortes and V. Vapnik, Support vector machines, Machine Learning, vol. 20, pp. 1-25, 1995. 
[9] H. Liu, J. Li and L. Wong, A Comparative study on feature selection and classification methods using gene 
expression profiles and proteomic patterns, Genome Informatics, vol. 13, pp. 51-60, 2002. 
[10] T. Li, C. Zhang and M. Ogihara, A comparative study of feature selection and multiclass classification 
methods for tissue classification based on gene expression, Bioinformatics, vol. 20, no. 15, pp. 2429-2437, 
2004. 
[11] E. P. Xing, M. I. Jordan and R. M. Karp, Feature selection for high-dimensional genomic microarry data, 
International Conference on Machine Learning, pp. 601-608, 2001. 
[12] Y. Yang and J. O. Pedersen, A comparative study on feature selection in text categorization, International 
Conference on Machine Learning, pp. 412-420, 1997. 
[13] T. Liu, S. Liu, Z. Chen and W. Y. Ma, An evaluation of feature selection for text categorization, 
International Conference on Machine Learning, 2003. 
[14] G. Forman, An Extensive empirical study of feature selection metrics for text classification, Journal of 
Machine Learning Research, vol. 3, pp 1289-1305, 2003. 
[15] T. Joachims, Text categorization with support vector machines: learning with many relevant features, 
Proceedings of the European Conference on Machine Learning (ECML), Springer, 1998. 
[16] H. Liu and L. Yu, Toward Integrating Feature Selection Algorithms for Classification and Clustering, 
IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 4, pp. 491-502, 2005. 
[17] N. Guyon and A. Elisseeff, An introduction to variable and feature selection, Journal of Machine 
Learning Research, vol. 3, 2003, pp. 1157-1182. 
[18] R. Kohavi and G. John, Wrappers for feature selection, Artificial Intelligence, vol. 97, no. 1-2, pp. 
273-324, 1997. 
[19] A. Blum and P. Langley, Selection of relevant features and examples in machine learning, Artificial 
Intelligence, vol. 97, no. 1-2, pp. 245–271, 1997. 
[20] A. Jain and D. Zongker, Feature selection: evaluation, application, and small sample performance, IEEE 
Trans. on Pattern Analysis and Machine Intelligence, vol. 19, no. 2, pp. 153-158, 1997.  
[21] G. J. Mun, B. N. Noh and Y. M. Kim, Enhance stochastic learning for feature selection in intrusion 
classification, International Journal of Innovative Computing, Information and Control, vol. 5, no. 11(A), 
pp. 3625-3635, 2009. 
[22] C.C. Lai, C. H. Wu and M. C. Tsai, Feature selection using particle swarm optimization with application 
in spam filtering, International Journal of Innovative Computing, Information and Control, vol. 5, no. 2, 
pp. 423-432, 2009. 
[23] Y. Chen, J. Chang and C. Cheng, Forecasting IPO returns using feature selection and entropy-based 
rough sets, International Journal of Innovative Computing, Information and Control, vol. 4, no. 8, pp. 
1861-1875, 2008. 
[24] H. Ben´ıtez-P´erez and A. Ben´ıtez-P´erez, The use of armax strategy and self organizing maps for 
feature extraction and classification for fault diagnosis, International Journal of Innovative Computing, 
Information and Control, vol. 5, no. 12(B), pp. 4787-4796, 2009. 
[25] P. Pudil, J. Novovičová and J. Kittler, Floating search methods in feature selection, Pattern Recognition 
Letters, vol. 15, no. 11, pp. 1119-1125, 1994. 
[26] P. Somol., P. Pudil, J. Novovičová and P. Paclík, Adaptive floating search methods in feature selection, 
Pattern Recognition Letters, vol. 20, no. 11-13, pp. 1157-1163, 1999. 
RDT04 
  
 
行政院國家科學委員會補助國內專家學者出席國際學術會議報告 
                                                          100  年   08   月   31   日   
  報    告    人 
  姓          名  
周建興 
 服 務 機 關 
 及   職  稱  
淡江大學電機系助理教授 
           時  間  
  會   議 
           地  點            
自 2011 年 08 月 24 日至
2011 年 08 月 26 日 
法國巴黎
 本 會 核 定 
            
 補 助 文 號 
NSC 99-2221-E-238-017 
                     
會  議  名   稱 （ 中文 ）2011 電機電腦電子通訊國際研討會 
（ 英文 ）2011 International Conference on Electrical, Computer, 
Electronics and Communication Engineering (ICECECE 2011) 
                    
 發 表 論 文 題 目 （ 中文 ）一種使用假特徵的特徵選取演算法 
（ 英文 ）A Novel Feature Selection Algorithm by Using False Feature 
報告內容應包括下列各項： 
 
一、參加會議經過   
 
 
二、與會心得 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
四、建議 
 
 
五、攜回資料名稱及內容 
 
 
六、其他 
 
 
 
 
 
 
RDT04 
  
 
四、攜回資料名稱及內容 
    與會後攜回的主要資料，除了本次會議的詳細議程外，以及論文光碟 CD 一片及一本大
會論文摘要集。以及國際科學工程與科技學會的相關介紹資料。 
 
五、誌謝 
    本次經費支出係由國科會專題計劃(NSC 99-2221-E-238-017)出國經費補助 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：周建興 計畫編號：99-2221-E-032-072- 
計畫名稱：以機器學習為基礎之鼠類生理狀態之分類系統 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 1 1 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100% 
目前將計畫研究
之內容，發表國際
期刊論文一篇 
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
目前將計畫研究
之內容，發表國際
會議論文一篇 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
