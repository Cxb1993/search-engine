行政院國家科學委員會補助專題研究計畫成果報告 
 
以全天候視訊為基礎的日常生活、睡眠與配
合醫療行為監控之遠距健康照護系統 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：NSC 99-2221-E-009-156 
執行期間： 99 年 8月1日至 100年7月31日 
 
執行機構及系所：國立交通大學電控工程研究所 
 
計畫主持人：張志永 
計畫參與人員： 
 
 
 
成果報告類型(依經費核定清單規定繳交)：■精簡報告  □完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
            
 
中   華   民   國  100年  10月  31日 
 
2 
 
I. INTRODUCTION 
Human activity recognition plays an important 
role in applications such as automatic surveillance 
systems, human-machine interface, home care sys-
tem and smart home applications. Several human 
activity recognition methods have been proposed in 
the past few years. Bobick and Davis [1] recognize 
human activities by comparing motion-energy and 
motion-history of template images with temporal 
images. Foreground subject extraction is an im-
portant step of the vision-based systems. such as 
automatic surveillance systems, human-machine 
interface, home care system and smart home ap-
plications. Many authors have developed methods 
of detecting foreground subject in images [2], 
[3].Most of this work has been based on background 
subtraction using color or luminance component. In 
these approaches, difference between the coming 
frame and the background image is performed to 
detect foreground objects. W4 [4] is a typical one 
with some modifications. It records the maximum 
and minimum grayscale value and luminance and 
the maximum interframe difference in every posi-
tion of a frame in a background video. Then every 
image frame subtracts the maximum and minimum 
grayscale value and luminance at each position. If 
the pixel’s absolute value of the subtraction opera-
tion is over the maximum interframe difference, the 
pixel is a foreground one. Under the W4 framework, 
we cannot detect a foreground pixel correctly when 
it is similar in color to the background pixel. To this 
end, foreground subject extraction is done in the 
grayscale space and the HSV color space. We can 
have both the grayscale value and the luminance 
information in the background subtraction task. 
Furthermore, the moving cast shadows mostly ex-
hibit a challenge for accurate foreground subject 
detection. A lot of attempts have been developed to 
tackle the shadow suppression [5], [6] encountered 
in background subtraction. Horprasert et al. [5] and 
Cucchiara et al. [6] utilized the rationale that sha-
dows have similar chromaticity, but lower bright-
ness than the background model. Under the pro-
posed framework in the HSV color space, we can 
effectively identify the shadow existent in our de-
tected foreground subject.  
Our taking medicine recognition system can be 
separated into “skin color detection,” “medical 
pouch color recognition” and “activity recognition 
system.” Then, we also do activities of daily living 
by only using our “activity recognition system.” 
II. FUZZY RULE INFERENCE BASED HUMAN 
ACTIVITY RECOGNITION 
A. Transformation of Image Data 
In video and image processing, the dimensions of 
image data are often extremely large. Because there 
are great deals of redundancies in the images, it is 
common to transform image from one space to 
another space to reduce redundancy. Our transfor-
mation method combines eigenspace transforma-
tion and canonical space transformation. The sub-
sequent transformation, Canonical space transfor-
mation (CST) is used to reduce data dimensionality 
and to optimize the class separability and improve 
the classification performance. Recognition is ac-
complished in the canonical space. 
B. Activity Template Selection 
There are few differences between two postural 
image frames if they are captured in a short interval. 
Besides, a human body is a rigid body, thus has its 
natural frequency; namely, it has restriction on ac-
tion speed when doing some specific actions. In our 
approach, we select one image frame, as called the 
essential template image, with a fixed interval. 
These essential templates are transformed to a new 
space by eigenspace transformation (EST) and ca-
nonical space transformation (CST). 
C. Fuzzy Rule Construction 
Transitional relationships of postures in tem-
poral sequence are important information for human 
activity classification. If we only utilize one image 
frame to classify the action, classification result 
may be failed easily because human’s actions may 
have similar postures in two different activity se-
quences. Human activities have lots of ambiguity, 
so we propose a method which can not only com-
bines temporal sequence information for recogni-
tion but also is tolerant to variation of actions done 
by different people. Relevant articles using the 
fuzzy theory are described as follows. Wang and 
Mendel proposed that fuzzy rules to be generated by 
learning from examples in [9].  
Let ji,t  be the transformed vector of a tem-
plate image of the j-th training model and the i-th 
category. ka  is the transformed vector of an instant 
image. We make use of the membership functions to 
represent the instant images’ possibility to each 
cluster. We choose the Gaussian type membership 
function to represent the instant images. The 
membership function is given by 
                  
4 
 
IV. EXPERIMENTAL RESULTS 
In our experiment, we tested our system on videos 
taken by digital camera. We took the video in our 
laboratory at the 5th Engineering Building in NCTU 
campus. The camera has a frame rate of thirty 
frames per second and image resolution is   pixels. 
The light source is fluorescent lamps and is stable. 
A. Skin Color Detection 
Skin color detection is used for segmenting the 
object’s hands. If we segment the skin region more 
precisely, we can extract object’s hands more cor-
rectly. A threshold   skink is applied in Eq. 9 to ob-
tain binary image B(x,y). The value of skink is cho-
sen by experiment and varies with different envi-
ronments. Hence, we ran a series of experiments to 
determine the optimal threshold  skink  and the cor-
responding binary images are shown in Fig. 1. The 
threshold skink =45 was adopted in our experiment. 
B. Medical Pouch Color Recognition 
As described in Sec. III.B, we recognize medical 
pouch’s color twenty times for each color, and we 
use seven consecutive images for one color recog-
nition. Then, we suppose the recognized time is rN , 
and the consecutive time is cN .We get two recog-
nition rates, the first recognition rate 1acc  is defines 
as 
1
1a correct
r c
Ncc
N N
= ×                                                  (10) 
where 1correctN  is the number of correct color rec-
ognition in all recognized images, rN  is twenty, 
and cN  is seven. 
The second recognition rate 2acc , majority vot-
ing in the seven consecutive images, is defined as  
2
2a correct
r
Ncc
N
=                                                (11) 
where 2correctN  is the number of correct color rec-
ognition in twenty recognitions. Table 1 shows the 
color recognition result of the recognition rates. 
C. Activity Recognition System 
The activity recognition system can operate in not 
only off-line videos but also real time videos. In 
order to calculate the recognition rate of activities, 
we use off-line videos and each of them includes 
only one activity in our experiment. Then, we input 
the testing video from different starting frames 
which is similar to the way for the training fuzzy 
rules. Namely, we recognize the video from the first 
frame, the second frame, the third frame and the 
fourth frame, etc. with the sampling intervals of five 
frames. Hence, there are four video databases for 
training and testing. 
An example of recognition rate of a testing video 
start from different frames is shown in Table II. In 
this table, WRL is the activity “walking from right 
to left,” WLR is the activity “walking from left to 
right,” WS is the activity “walking straight,” READ 
is the activity “reading,” UCP is the activity “using 
computer,” SLEEP is the activity “sleeping,” TAKE 
is the activity “taking medicine,” PICK is the ac-
tivity “picking up.” Here, the recognition rate is the 
number of correct recognition divide by the total 
number of recognition for each video. Table 2 
shows the action recognition result of an experiment 
day. 
D. Activity of Daily Living 
We use the above activity recognition system to 
record the activities of daily living. In our experi-
ment, we record the daily living of a student in the 
laboratory, and the common activities we used are 
“reading,” “walking straight,” “using computer,” 
and “sleeping.” And we represent the above four 
activities as state 1, state 2, state 3, and state 4, re-
spectively. In the Fig. 2, the abscissa is represented 
as time and the ordinate is represented as current 
action. This record data are accurate in validation to 
the doing actions in the recorded video. 
V. CONCLUSION 
In this paper, we proposed the foreground subject 
extraction in the grayscale value space and the HSV 
color space. In this way, we can reliably extract the 
foreground subject, even when the foreground color 
is similar to that of the background. Moreover, we 
make use of the hue component to recognize the 
medical pouch’s color when one is taking medicine. 
By combining with the hue-based pouch’s color 
model and human activity recognition system, we 
can know someone is taking medicine and its 
medical pouch’s color as well. Finally, we also va-
lidate the correctness of the activity recognition 
system to record a student’s daily living for ADL 
application. 
ACKNOWLEDGMENT(S) 
This work was supported  in part by the Taiwan 
National Science Council under Grant Number: 
NSC-99-2221-E-009-156. 
REFERENCES 
[1] F. Bobick and J. W. Davis, “The recognition of 
human movement using temporal templates,” IEEE 
Trans. Pattern Anal. Machine Intell., vol. 23, no. 3, 
pp. 257-267, Mar., 2001. 
 
 
6 
 
(a) 
 
 
 
 
 
 
 
 
 
 
 
 
13.5 14 14.5 15 15.5 16 16.5 17
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
st
at
e
time
6/20 afternoon 13:45~16:59
 
(b) 
 
  Fig. 2.  The activities of daily living in (a) the morning of 
6/20, (b) the afternoon of 6/20. 
 
 
 
 
                                                                                            2 
一、參加會議經過    
IEEE Symposium Series in Computational Intelligence 2011 (SSCI 
2011)，是 IEEE 計算智慧學會的年度會議之一，較著重應用與新發展方向，
也是計算智慧領域最重要、歷史悠久、參與者極多的國際會議；本會議聯
合智慧型技術基礎於各方面領域之多個研討會組合而成之型國際會議，本
會議包括研討會如後，適應動態規劃和增強學習 Adaptive Dynamic 
Programming and Reinforcement Learning； 認知算法、心智和人腦 Cognitive 
Algorithms, Mind and Brain; 生物資訊和計算生物 Bioinformatics and 
Computational Biology; 生物測定學和身分管理 Biometrics and Identity 
Management; 控制和自動化Control and Automation; 資料探勘Data Mining; 
動態和不確定的環境 Dynamic and Uncertain Environments; 金融工程和經
濟 Financial Engineering & Economics; 產業 Industry; 可移動的機器人 
Mobile Robots; 多媒體，信號和視覺處理 Multimedia, Signal and Vision 
Processing; 行程安排 Scheduling; 安全和國防應用 Security and Defense 
Application; 視覺智慧 Visual Intelligence; 車輛和運输系统 Vehicles and 
Transportation Systems; 感知器技術 Sensor Technology; 演化和能適應的聰
明的系統 Evolving and Adaptive Intelligent Systems; 基因和演化模糊系统
Genetic and Evolutionary Fuzzy Systems; 混雜智慧的模型和應用 Hybrid 
Intelligent Models and Applications; Intelligent Agents 智慧型代理人; 人工
生命 Artificial Life; Memetic 計算 Memetic Computing; 多指標政策制定
Multicriteria Decision-Making; 有機計算  Organic Computing; 群體智力 
Swarm Intelligence; 第二類型模糊邏輯系統 Type-2 Fuzzy Logic Systems; 
感動計算智力 Affective Computational Intelligence等研討會。本屆 2011年， 
於 2011 年 4 月 11 日至 4 月 15 日在法國 巴黎市巴黎第七大學舉行， 
Bernadette Bouchon Meunier教授為大會主席，巴黎第六大學為主辦單位。
本人，於 2011年 4月 8日 晚上搭長榮航空赴巴黎，於 4月 9日早上到達
巴黎，於 4月 11日至 4月 15日參加會議； 4月 16日早上搭長榮航空返
回，4月 17日早上返國。此次國內參與會議及發表論文者，另有台灣大學
林巍聳教授，中華大學林君明教授，成功大學詹寶珠教授，及台南大學李
健興等校教授及研究生數名。  
                                                                                            4 
Qiang Shen (Aberystwyth University, United Kingdom)  
April 13 Wednesday, 11:00-12:00 
(7) How to make a robot that feels 
Kevin O'Regan (Université Paris Descartes, France)      
April 13  Wednesday, 14:00-15:00 
(8) From Heuristics to Statistics: An Overview of the ADEPT project  
Gavin Brown (University of Manchester, United Kingdom)      
April 13  Wednesday, 16:30-17:30 
(9) Self-Adaptive Systems for Machine Intelligence  
Haibo He (University of Rhode Island, USA)      
April 14  Thursday, 9:30-10:30 
(10) Multilevel Evolution and Evolvability  
Paulien Hogeweg (Utrecht University, The Netherlands)      
April 14  Thursday, 11:00-12:00 
(11) From Processing Interval-Valued Fuzzy Data to General Type-2: Towards 
Fast Algorithms  
Vladik Kreinovich (University of Texas at El Paso, USA)      
April 13  Wednesday, 15:00-16:00 
(12) Mechanisms and constraints for open-ended learning in developmental 
robotics 
Pierre-Yves Oudeyer (INRIA, France) 
April 14  Thursday, 16:30-17:30 
(13) Fuzzy Integration for Buried Landmine Detection      
James Keller (University of Missouri, USA) 
                                                                                            6 
題，我們結合了影像中特徵、輪廓和空間分佈等資訊來判別不同的分割區
域是否代表著同一個物體。接著我們使用兩種不同的深度估測方法對已經
從場景中被截取出來的人體作深度的估測，分別是以消失線及消失點為基
礎的估測方式，和以固定相機之查表法的估測方式來重建二維平面影像的
三維景深資訊。 
在本文中，我們結合了影像分割和人臉偵測技術，希望能在不同場景
中將人體抽取出來。首先，我們利用了膚色資訊與橢圓樣板比對找出人臉
在影像中的位置，接著我們提出了一套改良式自動種子區域成長演算法來
分割影像並根據人體的形態完成前景人物偵測，最後再依據人物在影像中
的垂直 y 座標值，對其做深度估測。發表演講期間發問熱烈，計有德國、
美國、大陸等學者、公司研發人員詢問討論廣泛，氣氛極佳。 
 
二、 與會心得 
法國，位於歐洲西部，與比利時、盧森堡、德國、瑞士、義大利、
摩納哥、安道爾和西班牙接壤，隔英吉利海峽與英國隔海相望。面積 55
萬平方公里，是歐洲面積第三大的國家。工業、農業發達。法國是第一、
第二次世界大戰的主要戰勝國之一，因而成為聯合國安理會常任理事
國，對議案擁有否決權。法國亦是歐盟和北約創始會員國之一，八國集
團之一和歐洲四大經濟體之一，亦是《申根公約》的成員國。法國精品
與旅遊之發達，相當令人震撼與值得學習。 
談到精品，多數人馬上會聯想到座落於法國香榭麗舍大道
(Champs-Elysees)上的路易威登 (Louis Vuitton Moet Hennessy；LVMH) 旗
艦店，絡繹不絕排隊等待購買的亞洲遊客！時尚和精品這個部門包括高
級時裝，珠寶，高檔皮具，香水，化妝品和精細玻璃器皿。年營業額 350
億歐元。除了龍頭路易威登，還有古奇，香奈兒，愛馬仕等集團。其實，
法國的精品產業是除了國防武器外，僅次於汽車、飛機、飲料(葡萄酒、
香檳、礦泉水等)的第 4大外銷產業。 
法國人保存傳統文化的精神，除了表現在藝術、建築等人文景觀上，
品牌也深受其影響，每個商品似乎都在訴說著不同的故事，間接造就了
國科會補助計畫衍生研發成果推廣資料表
日期:2011/11/09
國科會補助計畫
計畫名稱: 子計畫五：以全天候視訊為基礎的日常生活、睡眠與配合醫療行為監控之遠
距健康照護系統
計畫主持人: 張志永
計畫編號: 99-2221-E-009-156- 學門領域: 智慧型照護系統
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
1. 在台北市 舉辦 IEEE Fuzzy Systems 年度會議 
 
2. 擔任 IEEE Trans. Fuzzy Systems 學術期刊副編輯 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
