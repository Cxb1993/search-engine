  
 
行政院國家科學委員會補助專題研究計畫
■成 果 報 告
□期中進度報告 
 
Criti-core：超越多核心之高可靠度晶片系統平台技術開發 
子計畫五：全景立體視訊播放系統於 Criti-core 之設計與實現(2/2) 
 
計畫類別：□ 個別型計畫   整合型計畫 
計畫編號：NSC99－2220－E－194－012 
執行期間：99 年 8 月 1 日至 100 年 7 月 31 日 
 
計畫主持人：郭峻因 
共同主持人：蘇慶龍 
計畫參與人員：簡國安、簡呈安、李睿勝、張正妍、張家豪、王柏青 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、
列管計畫及下列情形者外，得立即公開查詢 
          涉及專利或其他智慧財產權，□一年二年後可公開查詢 
          
執行單位：國立中正大學資訊工程系 
 
中華民國 100 年 9 月 30 日 
  ii 
4.1.4 Video Stitching with Multiple Cameras .................................................. 55 
4.1.5 Vertical and Four-channel Video Stitching ............................................. 57 
4.2 Realization on an Embedded System ............................................................... 59 
4.2.1 Simulation Environment ......................................................................... 59 
4.2.2 Optimization of the Proposed Algorithm ................................................ 61 
4.3 Performance Evaluation and Demonstration ................................................... 64 
4.3.1 Performance Evaluation .......................................................................... 64 
4.3.2 Demonstration ......................................................................................... 66 
5. Conclusion and Future Work ................................................................................... 70 
References .................................................................................................................... 71 
 
  iv
本子計畫今年度已針對所設定之目標進行研發，目前已完成multiple standard 
video decoding之演算法分析及最佳化，根據實驗結果，計算複雜度可降低約 80% 
~ 90%，使用 ARM9處理器以時脈 200 MHz執行時，解碼速度可達到 CIF@12 ~ 
14fps及 QCIF@47 ~ 50fps。 
另一方面，我們還提出一套 3-D景深產生演算法，此演算法主要是以景物消
失點為基礎之物體景深估測方法來計算一般 2-D image/video 的景深，與目前其
它 3-D 景深產生演算法相比，我們的方法不但可達到類似的 3-D 景深效果，更
可節省許多運算，然而，3-D景深產生比起一般的應用，如 video decoding等，
仍需耗費相當可觀的運算，因此我們設計了一個 3-D視訊播放系統，此系統結合
了我們所提出之 3-D景深產生演算法，並透過多核心運算來達到平行化，實驗結
果顯示，使用 32 核心處理器以時脈 2.3 GHz 執行時，處理速度可達到
HD720@63.66 fps。 
同時，本計畫也探討全景視訊接合技術，將兩個有重疊區域之視訊內容進行
即時視訊接合成單一全景視訊後輸出。值得一提的是，即使兩來源視訊有角度或
焦距遠近之差異，本演算法仍然可以合成出高品質的全景影像/視訊。本計畫所
提之全景影片接合技術效能較現有技術大幅降低 93%之計算量，有助於在嵌入式
系統或以低成本硬體方式來實現。 
 
  2
With the coming of the age of high definition video, video compression 
technologies in new generation such as VC-1[7][8], H.264[9], and AVS[10], have 
been developed to replace the traditional video compression technologies like 
MPEG-1[11], MPEG-2[12], and MPEG-4[13]. VC-1 is an open standard that is 
standardized by SMPTE (Society of Motion Picture and Television Engineers). Its 
primary technology comes from the WMV-9 (Windows Media Video 9) developed by 
Microsoft. H.264 is a standard standardized by JVT (Joint Video Team) that is 
composed of ITU-T VCEG and ISO/IEC MPEG. The coding efficiency and video 
quality of VC-1 and H264 are better than those of the traditional video standards. AVS 
is a standard developed by the Audio Video Coding Working Group of China. In AVS, 
AVS-P2 defines the basic video compression for common use. For developing mobile 
applications, AVS has an additional version named AVS-M[14]. AVS-M is also called 
AVS-P7 since it is fully defined in the part 7 of AVS. In general, the decoding flow 
and main components in AVS are almost the same as those in H.264. However, the 
complexity of AVS is less than that of H.264 but the quality of AVS is close to that of 
H.264[15][16]. Now that the traditional video compression technologies are not 
sufficient to satisfy the request for high definition video, video compression 
technologies in new generation will introduce some features that can enhance the 
quality of compressed video. Firstly, they do the transform in integer mode so that the 
precision problem can be easily handled. Secondly, they employ the in-loop filter to 
eliminate the block effect in order to make the video more elegant. Thirdly, they use 
more complicated interpolation to achieve better quality for the sake of minimizing 
the error caused by the motion prediction. However, it encounters more complicated 
computation when pursuing better video quality. For this reason, we propose a series 
of software optimization schemes from the aspects of both algorithm-level and 
code-level so as to achieve real-time decoding of the new generation videos. In 
  4
performance analysis. Finally, we give some conclusions and briefly describe the 
research direction in the future in Section 5. 
  6
where  


























−−−−
−−−−
−−−−
−−−−
−−−−
−−−−
−−−−
=
75311357
62266226
51733715
44444444
37155173
26622662
13577531
00000000
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
aaaaaaaa
A
 (2) 
On the other hand, we find a relationship shown in (3) existing in matrix A. 
( ) ( ) ( )jiAjiA i ,17, −=−  (3) 
 
From this relationship, we can organize output data to form four pairs (y0, y7), (y1, y6), 
(y2, y5), and (y3, y4). We can calculate them as follows. 
 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]
[ ] [ ] [ ] [ ]7733551166224400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
3
0
12
3
0
2
7
0
0
0,70,30,50,1
0,60,20,40,0
0,340,140,240,4
0,120,20,
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
k
k
+++++++=
++++
+++=
++++++=
++==
∑∑∑∑
∑∑∑
=
+
=
+
=
+
=
=
+
==
 
(4) 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]{ }
[ ] [ ] [ ] [ ]{ }7733551166224400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
12
3
0
12
2
3
0
2
7
0
7
0
7
0,70,30,50,1
0,60,20,40,0
0,340,140,240,4
0,1210,21
0,17,
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAx
kAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
r
r
k
k
k
k
k
+++−+++=
+++−
+++=






+++−++=
+−+−=
−==
∑∑∑∑
∑∑
∑∑
=
+
=
+
=
+
=
+
=
+
=
==
 (5) 
  8
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]
[ ] [ ] [ ] [ ]
[ ] [ ] [ ] [ ]7135531766224400
7135531766224400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
3
0
12
3
0
2
7
0
3
3,73,33,53,1
3,63,23,43,0
3,343,143,243,4
3,123,23,
xaxaxaxaxaxaxaxa
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
k
k
+−+++−+=
−−+++−−++=
++++
+++=
++++++=
++==
∑∑∑∑
∑∑∑
=
+
=
+
=
+
=
=
+
==
 (10) 
( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( ) ( ) ( )
( ) ( )[ ] ( ) ( )[ ]
( ) ( )[ ] ( ) ( )[ ]{ }
[ ] [ ] [ ] [ ]{ }
[ ] [ ] [ ] [ ]{ }7135531766224400
7135531766224400
7351
6240
1
0
34
1
0
14
1
0
24
1
0
4
12
3
0
12
2
3
0
2
7
0
7
0
4
3,73,33,53,1
3,63,23,43,0
3,343,143,243,4
3,1213,21
3,14,
xaxaxaxaxaxaxaxa
xaxaxaxaxaxaxaxa
AxAxAxAx
AxAxAxAx
sAxsAxsAxsAx
rAxrAx
kAxkAxy
s
s
s
s
s
s
s
s
r
r
r
r
r
r
k
k
k
k
k
+−+−+−+=
−−++−−−++=
+++−
+++=






+++−++=
+−+−=
−==
∑∑∑∑
∑∑
∑∑
=
+
=
+
=
+
=
+
=
+
=
==
 
(11) 
 
Then we can obtain the data flow charts as shown in Fig. 2.1, Fig. 2.2, Fig. 2.3, and 
Fig. 2.4 for computing the 1-D 8-point inverse transform. 
x[0]
x[4]
x[2]
x[6]
x[1]
x[5]
x[3]
x[7]
a0
a4
a2
a6
a1
a5
a3
a7
y[0]
y[7]
-
 
Fig. 2.1. Data flow for fast 1-D 8-point inverse transform (Part I for y[0] and y[7]) 
  10 
proposed fast 1-D 8-point inverse transform only takes 22 multiplications and 32 
additions while the original one has to take 64 multiplications and 56 additions. It is 
seen that we save 65.6% multiplications and 42.9% additions for 1-D 8-point inverse 
transform. In consequence, a lot of redundant multiplications and additions in the 
inverse transform of VC-1 and AVS can be saved through the proposed fast algorithm. 
2.1.2 Zero Skipping for Inverse Transform 
In the video decoding process, the occurrence of zero values means computation 
can be reduced or simplified. As shown in Table 2.1, we have performed some 
probability analysis for zero occurrence in the inverse transform. According to the 
analysis, all the cases listed in Table 2.1 cover about 90% of occurring samples in 
VC-1 and 93% of occurring samples in AVS. So we can detect these cases before 
doing the inverse transform to avoid the unnecessary computation. 
Table 2.1. Analysis for zero occurrence in 1-D 8-point inverse transform 
Cases (where xi is input data) 
Occurrence Probability 
(in VC-1) 
Occurrence Probability 
(in AVS) 
x0=x1=x2=x3=x4=x5=x6=x7=0 33.2 % 35.3 % 
x1=x2=x3=x4=x5=x6=x7=0 20.3 % 11.6 % 
x2=x3=x4=x5=x6=x7=0 9.5 % 12.3 % 
x4=x5=x6=x7=0 3.9 % 9.5 % 
x1=x3=x5=x7=0 1.5 % 6.6 % 
x0=x2=x4=x6=0 11.7 % 9.5 % 
x6=x7=0 10.4 % 8.5 % 
In Total 90.5 % 93.3 % 
 
 
 
  12 
decoded frame buffer. If not, we have to perform the inverse transform, the 
summation of the predicted block and the residual block, and copying the 
reconstructed block to the decoded frame buffer. 
Current 8×8 
block is all-zero?
Current 4×4 
block is all-zero?
Inverse Transform
VLD & Inverse 
Quantization
Reconstruct 
block4×4
Copy block8×8
All 4×4 blocks 
are processed?
All 8×8 blocks 
are processed?
Copy block4×4
End
No
Yes
Yes
No
Yes
No
Yes
No
 
Fig. 2.5. Skipping of all-zero residual blocks 
2.1.5 Early Termination for Deblocking 
In AVS-M, deblocking will be applied to all the boundaries among the blocks in 
order to reduce the block effect since AVS-M also divides each picture into a lot of 
small blocks for coding. Different from H.264, AVS-M doesn’t check the boundary 
strength. The way of deblocking is mainly based on the type of the macroblock. When 
the current macroblock is intra coded, the intra mode deblocking will be performed. 
On the other hand, the inter mode deblocking will be performed when the current 
marcoblock is inter coded and not skipped or the QP of the current marcoblock is 
  14 
2.1.6 Data Reusing for Luma Interpolation 
Fig. 2.7 shows the luma interpolation of the AVS-M video decoder. In AVS-M, 
both the half pixel and quarter pixel are adopted. However, the interpolation always 
wastes a lot of redundant computation. For example, the half pixel j will be vertically 
interpolated by the other four adjacent half pixels aa, b, t, and hh. On the other hand, 
each of these four half pixels is interpolated by the eight adjacent integer pixels that 
are located on the same row. In other words, the half pixel b is interpolated by the 
integer pixels C, D, E, F, G, H, I, and J. When we calculate another half pixel j 
(assuming it located one integer pixel downward from the current half pixel j), all the 
computation for the interpolation of this half pixel j should be performed again. In 
fact, we can reuse partial computation results so as to reduce the execution cycles 
coming from interpolation. In the case mentioned above, we have to store the 
computation results of half pixels b, t, and hh since they will respectively become the 
adjacent half pixels aa, b, and t of the new half pixel j. 
d
bC ED
K ML
A
F
S
N
B
G IH
O
T
QP
J
R
n
a c
h j
t
aa
hh
e f
i
p
g
k
q r
m
 
Fig. 2.7. Luma interpolation of the AVS-M video decoder 
2.2 Code-Level Optimization 
Sometimes, programmers may write the program in a straightforward way 
because they just want to realize a specific function without considering much on its 
performance when executed on an embedded processor. Nevertheless, there may be 
  16 
operations at run time. For example, the clipping operations are frequently used in 
video processing. Nevertheless, they have to check if the data exceeds the range or not. 
Such a checking operation will increase the number of branches so as to result in a lot 
of execution cycles because the branches usually cause pipeline stalls. In this situation, 
we can find out the dynamic range of the data and further decide the results in 
advance. In consequence, we can replace the clipping operations by inquiring a 
look-up table as shown in Fig. 2.9. 
 
Fig. 2.9. Illustration for Guideline 2 
Guideline 3. Using “memcpy” function to duplicate large amount of 
data 
This guideline tells us that we should use “memcpy” function to copy large 
amount of data rather than using a loop to copy them one by one because “memcpy” 
function can access memory more efficiently. In video processing, it is very frequent 
that we need to move a large amount of temporal data from buffer to buffer. Such a 
data movement will result in a lot of memory access operations. Once the processor 
accesses the memory, the performance will decrease since the penalties of accessing 
memory are quite expensive. For the sake of improving the performance, we have to 
make the data movement more efficient. The function called “memcpy” is a 
subroutine defined in the standard library. In other words, it will be optimized 
according to the target processor so that it can access memory in the most efficient 
way. Therefore, we use “memcpy” function to copy data instead of using a loop, as 
shown in Fig. 2.10. 
  18 
Guideline 6. Making the repeating number of loop be a constant value 
In software optimization, loop unrolling is an approach in common use. Such an 
approach can avoid the branches brought by loops and improve the execution 
performance. But the compiler won’t unroll the loop without knowing the repeating 
number of the loop. This guideline is established to let compiler know the repeating 
number of a loop at compile time so as to unroll the loop automatically. So we make 
the repeating number of the loop be a constant value if possible. 
Guideline 7. Moving the cases with higher probability forward 
In video processing, there exist many modes in order to deal with all kinds of 
situations. However, a lot of branches will appear if we just arrange them arbitrarily. 
In fact, not all of the cases will be hit averagely. According to our experience, it has 
more than 50% possibility to hit someone of all cases. For example, there are four 
kinds transform in VC-1, such as 4×4, 4×8, 8×4, and 8×8 but the case of 8×8 transform 
is the most frequently used one. This guideline means that we have to sort the code 
segment with many cases according to the probability of each case as shown in Fig. 
2.13 so that the hit rate of the code segment will be increased. 
if ( C1 )
    S1
else if ( C2 )
    S2
else if ( C3 )
    S3
else if ( C4 )
    S4
Higher probability
Lower probability
 
Fig. 2.13. Illustration for Guideline 7 
Guideline 8. Replacing “MOD” operation with “AND” operation 
Generally speaking, MOD operations are always implemented by division 
operations. However, embedded processors like ARM processors don’t have division 
instructions to handle division operations. For this reason, they use a subroutine that 
  20 
lessens the computation. 
if ( C )
{
    S1
}
if ( C )
{
    S2
}
if ( C )
{
    S1
    S2
}
 
Fig. 2.14. Illustration for Guideline 10 
Guideline 11. Adding early termination mechanism to deeply nested 
loops or branches 
In video processing, loops or branches are often nested deeply. Moreover, deeply 
nested loops or branches will cause a lot of execution cycles. This guideline means 
that we should try to find an early termination mechanism in order to avoid entering 
deeply nested loops or branches under some specific conditions. 
Guideline 12. Avoiding using arrays that have more than two dimensions 
Arrays are useful components in programming. They can help programmers 
handle a large amount of data with a simple index. However, the complexity brought 
by arrays will increase with the number of their dimensions. According to our 
experience, the redundant execution cycles will dramatically increase when we use 
arrays that have more than two dimensions. So we should use arrays with smaller 
dimension as possible. In video processing, 2-D arrays are data buffers in common 
use since each frame of the video is a 2-D image. In fact, we can change a 2-D array 
to a 1-D array with a simple index transition because accessing 2-D arrays will cause 
more execution cycles than accessing 1-D arrays after all. 
 
 
 
  22 
3. 3-D Depth Map Generation 
3.1 Algorithm of 3-D Depth Map Generation 
Fig. 3.1 shows the proposed depth map generating algorithm based on a single 
2D image. As shown in Fig. 3.2, the proposed algorithm can deal with three categories 
of input images, including normal images (with vanishing point (VP)), scenery 
images (with sky/mountain objects), and close-up images. In the proposed algorithm, 
we classify the input images according to the three categories mentioned above and 
generate the depth map based on some techniques, such as edge detection using Sobel 
mask filtering, line detection using 5×5 Hough Transform, Vanishing Region 
Detection (VRD), segmentation, depth map merging, depth map post-processing by 
Simplified Joint Bilateral Filtering (SJBF), and block-based contrast filtering for 
identifying the foreground objects in the close-up images. In the classification of the 
input images, we take advantage of some characteristics existing inside the images. 
For close-up images, they are identified for the characteristic of having few vanishing 
lines. For the images that have enough vanishing lines, they will be checked if there 
exist the sky or mountains within them. Although scenery images always have the 
characteristic mentioned above, we still need a suitable threshold value to differentiate 
between normal images and scenery images. 
To make the proposed algorithm suitable for real-time applications, we have 
optimized the processing steps mentioned above to reduce the computational 
complexity while preserving good visual quality. In the following, we will illustrate 
each processing step in more details. 
  24 
filtering, we can generate the edge information map shown in Fig. 3.3 for detecting 
vanishing lines in the next step. 
 
(12) 
 
( )22 GyGxG +=  (13) 
In order to reduce the computational complexity of Sobel mask filtering in (13), 
we replace the square and square root operations by the sum of absolute value 
operation, as shown in (14), since we observe that the results generated by (14) are 
similar to those generated by (13). In this way, we can reduce about 65% 
computational complexity in the Sobel mask filtering. 
GyGxG +=
 (14) 
 
 
Fig. 3.3. Result of applying Sobel filtering to the input image in Fig. 3.2(a) 
3.1.2 5×5 Simplified Hough Transform 
Hough transform is used to detect lines in the images. The input data of Hough 
transform is the edge map from the output of Sobel mask filtering. In the original 
Hough transform algorithm in [17], searching all the pixels located in every degree 
between 0 and 180 is necessary for line detection. As listed in (15), the coordinate of 
each pixel will be transformed from Cartesian coordinate (x, y) to polar coordinate (ρ, 
  26 
The reason to detect the vanishing region instead of the vanishing point results from 
the fact that the vanishing region is usually close to the associated vanishing point and 
the vanishing region can be located by searching fewer vanishing lines. Based on the 
vanishing line information, we can also identify which type the input image belongs 
to. 
The proposed image classification is shown in Fig. 3.5. In the first step of VRD, 
we analyze the number of points and lines from Sobel filtering and Hough transform. 
If the number is smaller than a pre-defined threshold, we classify it as a “close-up” 
image. In the second step, we start to analyze the image to detect the sky and 
mountain objects. If there is any sky or mountain object within the image, we will 
classify it as a “scenery” image. Finally, we calculate the intersection points of 
vanishing lines in the “normal” type of images. After calculating all intersection 
points of vanishing lines, we use an 8×8 region to group the nearest points in the 
image, which is named the vanishing region. According to the position of vanishing 
regions, there are two situations we may face. We can easily get the vanishing region 
when it is inside the image. If the vanishing region is outside the image, we will 
replace it with a new one located on the image margin closest to it. Then, we generate 
the Gradient Depth Map (GDM) according to the distance between every pixel and 
the vanishing region for the “normal” type of images. The example of the GDM for 
the image in Fig. 3.2(a) is shown in Fig. 3.6(a). As for the “scenery” images, the sky 
or mountains usually appear on the top of them. So we set the top region of image as 
the vanishing region of GDM, as shown in Fig. 3.6 (b). 
  28 
result in the proposed algorithm for the input image shown in Fig. 3.2(b). 
 
Fig. 3.7. Result of proposed segmentation for the image in Fig. 3.2(b) 
3.1.5 Merging Depth Map 
Though segmentation and edge detection, we get several intermediary depth 
maps. In the following, we will merge them into a merged depth map as shown in Fig. 
3.8(a) and Fig. 3.8(b). So we use the GDM as the base and make an adjustment in the 
value that belongs to the same object in segment map and in the same side of edge 
map. 
     
(a) Normal image (with VP)     (b) Scenery image (without VP) 
Fig. 3.8. Merging depth map for the images in Fig. 3.2(a) and Fig. 3.2(b) 
3.1.6 Simplified Joint Bilateral Filtering (SJBF) 
Simplified Joint Bilateral Filtering (SJBF) is used to refine the merged depth 
map by strengthening the edge information of the objects related to the original image. 
In the original JBF algorithm [5], the target image will be produced through 
fine-tuning the source image with a reference image. In the proposed algorithm, we 
slightly adjust the formula defined in the original JBF algorithm. As listed in (16), D, 
D’, and I mean the source, target, and reference images, respectively. At the same time, 
p represents the position of current pixel while q represents any position included in 
the region S that means a pre-defined region around the position p. For example, Iq 
  30 
foreground object. 
min
min
max
max
II
II
contrast
+
−
=
 (17) 
After detection, we assign the depth value 0 to the background pixels meaning 
that they are located at the farthest place in the image, and assign the depth value 1 to 
the foreground pixels. For the close-up images, we only represent the depth map in 
two layers, i.e. foreground and background. Fig. 3.10 shows the final depth map for 
the close-up image shown in Fig. 3.2(c) using 3×3 contrast filtering. 
 
Fig. 3.10. Result of depth map for the image shown in Fig. 3.2(c) 
3.2 Parallelization of 3-D Depth Map Generation 
For the sake of realizing the 3D depth map generation on the multi-core platform, 
we propose a parallel 3D video playing system based on the superscalar-like 
parallelization as shown in Fig. 3.11. This system is composed of a video dispatcher, a 
pseudo display, and several pipelined 3D depth map generators. At the front end of the 
proposed system, video dispatcher can receive the video decoded by any kind of video 
decoders. Then it will split the input video into several individual frames and dispatch 
each frame to one pipelined 3D depth map generator. Finally, the pseudo display will 
collect all the depth maps in order. In the following sections, we will give a detailed 
description about the pipelined-based parallelization. 
  32 
In the following, we make a description of how the proposed synchronized FIFO 
achieves the synchronization between two threads. Fig. 3.13 is the pseudo code for the 
synchronization at the producer end of the synchronized FIFO. At first, the thread at 
the producer end checks if the FIFO is full. When the FIFO is full, the thread has to 
wait until the FIFO is not full. After getting the access permission, the thread starts to 
write its data to the FIFO. Finally the thread calls a confirmation function that will 
update the information recorded in the FIFO and issue a notification signal to the 
thread at the consumer end of the FIFO. Similarly, as shown in Fig. 3.14, the thread at 
the consumer end of the synchronized FIFO performs almost the same steps as 
mentioned for the producer end except it checks if FIFO is empty rather than full. 
Moreover, we will use a taskmaster to monitor many workers in the proposed 
pipeline-based parallelization. So we need another kind of thread synchronization 
mechanism to achieve the communication between the taskmaster and its workers. 
This synchronization mechanism is carried out through a synchronized mailbox as 
shown in Fig. 3.15. Each worker will put the message in the mailbox and issue a 
notification signal to the taskmaster. Then the taskmaster will get the message put in 
the mailbox. Only one taskmaster or worker can access the mailbox at a time. 
 
Fig. 3.13. The pseudo code for the synchronization at the producer end of the 
synchronized FIFO. 
 
  34 
have distributed the workload as balanced as possible, the workload at some stage still 
has a wide gap to the others. As depicted in Fig. 3.17, the workload at the stage S5 is 
much smaller than the ones at other stages. But we cannot merge S5 with S4 because 
the stage S4 has been the one that has the largest workload. 
3.2.3 Data Packetization 
Under the proposed pipeline-based parallelization, we use FIFOs to connect 
several threads for constructing a pipeline. As shown in Fig. 3.18, we can imagine 
each FIFO between two workers is a conveyer. However, each data needs buffers to 
save temporary data and some necessary information at every stage in the pipeline 
since these data will be used at the following stages. For this reason, we pack all the 
related buffers into a packet for each input data and deliver the packet from stage to 
stage. With such a technique, the worker at every stage can easily access the related 
data from the current packet and perform the designated task on it. 
others
2.82%
sobel
7.85%
write_yuv
7.85%
depth map
generation
17.30%
read_yuv
5.23%
segmentation
2.21%
JBF
29.58%
hough
transform
27.16%
 
Fig. 3.16. Workload distribution for the proposed 3D depth algorithm. 
 
  36 
over-stage control signal. It is used between the pipeline taskmaster and all the 
pipeline workers. This control signal is carried out through the proposed synchronized 
mailbox mentioned in the previous section. 
 
Fig. 3.19. Pipeline control system in the proposed system. 
In the following, we describe about the working flow in the pipeline taskmaster 
and pipeline workers. As shown in Fig. 3.20, the pipeline taskmaster initializes the 
synchronized FIFOs at first and then creates the threads that are made to be pipeline 
workers. After the steps mentioned above, all the pipeline workers will start working. 
At the same time, the pipeline taskmaster enters into a waiting state. Once any 
pipeline worker finishes its work, it has to issue a notification signal to the pipeline 
taskmaster. When the pipeline taskmaster receives this signal, it will make a check on 
the condition “Nfinished < Ntotal”. Here the number Nfinished means the number of 
pipeline workers that have finished, while the number Ntotal means the total number of 
pipeline workers. The pipeline taskmaster will wait until all the pipeline workers have 
finished. Finally, it destroys the synchronized FIFOs and terminates the task. 
 
Fig. 3.20. Working flow in the pipeline taskmaster. 
 
  38 
from the target of real-time processing. To increase the throughput, we exploit the 
concept used in superscalar processors. Superscalar processors can improve the 
instruction-level parallelism because they use multiple pipelines. As a result, we 
propose a superscalar-like parallelization that is extended from the pipeline-based 
parallelization mentioned in the previous section. As depicted in Fig. 3.22, we 
compare the execution time and the throughout under three different kinds of 
computation. In this example, we assume that the pipelined computation uses a 
five-stage pipeline and the superscalar-like computation uses three five-stage 
pipelines. T1, T2, and T3 represent the timestamps for the three kinds of computation, 
respectively. Clearly, the superscalar-like computation can spend less time to achieve 
the same throughput as compared with the pipelined computation and the sequential 
computation. 
 
Fig. 3.22. Comparison of the execution time and throughout for sequential, 
pipelined, and superscalar-like computation. 
3.3 Performance Evaluation 
In this section, we discuss about the complexity reduction and quality 
measurement for the proposed 3D depth algorithm and the performance improvement 
for the proposed parallel 3D video playing system. 
3.3.1 Complexity Reduction and Quality Measurement 
In progress of the development for the proposed 3D depth algorithm, we perform 
  40 
   
   
   
  
  
Fig. 3.23. Result of more depth maps and stereo images. 
 
Fig. 3.24. 3D demonstration on an embedded platform. 
 
  42 
superscalar-like parallel computing. With such a model, users can get some rough 
information about the performance under the configuration they set. In the proposed 
pipeline-based computation, the execution time can be evaluated by (18). In this 
equation, Nstage represents the number of the stages used in the pipeline and Nprocessed 
represents the total number of the frames to be processed. In addition, Tlatency 
represents the latency in the pipeline and Tcritical represents the execution time at the 
critical stage. 
( )
( )
( ) criticalprocessedstage
criticalprocessedcriticalstage
criticalprocessedlatencypipelined
TNN
TNTN
TNTT
⋅−+=
⋅−+⋅=
⋅−+=
1
1
1
 (18) 
As shown in (19), we define the critical stage as the stage that takes maximum 
execution time among all the stages in the pipeline. At the same time, TSi represents 
the execution time in the stage Si and RSi represents the ratio between the execution 
time in the stage Si and the overall execution time in the sequential mode. For 
example, the critical stage in the proposed five-stage pipeline is S4 so that the ratio RS4 
is equal to 0.2958. 
{ }
stage
processed
sequentials
scritical
Ni
N
TR
TT i
i
,,2,1 where
max}max{
K∈







 ⋅
==
 (19) 
In the following, we also define the execution time for the proposed 
superscalar-like computation. As shown in (20), we can deduce Tsuperscalar from 
Tpipelined. Npipeline represents the number of the pipelines used in the superscalar-like 
architecture. Based on the execution time for the proposed superscalar-like 
computation, we further get the performance as shown in (21). 
( )
pipeline
criticalprocessedstage
pipeline
pipelined
rsuperscala N
TNN
N
T
T
⋅−+
==
1
 (20) 
  44 
4.16 fps, respectively. Moreover, the ratio RS4 for the critical stage is equal to 0.2958. 
Based on the information mentioned above, we can perform the performance 
estimation for each configuration listed in Fig. 3.25. The gap between the two curves 
is less than 10% except for the configuration of using six pipelines. In fact, there are 
still several potential factors we have not considered in the proposed estimation model 
yet. However, the proposed model certainly provides users an overview on the 
performance so that they can choose an adequate configuration to fit their 
requirements. 
{ }








⋅
−
+=








⋅
−
+







 ⋅
=
+=′
pipelinedlysequentialsequential
pipelinedlysequentialsequential
sequential
s
pipelinedlysequentialsequential
pipelinedlysequentialsequential
processed
sequentials
overheadscritical
PP
PP
P
R
PP
PP
N
TR
TTT
i
i
i
_
_
_
_
max
max
}max{
 
(23) 
 
( )
{ }








⋅
−
+=′
′⋅−+
⋅
=′
pipelinedlysequentialsequential
pipelinedlysequentialsequential
sequential
s
critical
criticalprocessedstage
pipelineprocessed
rsuperscala
PP
PP
P
R
T
TNN
NN
P
i
_
_
max
 where
1
 (24) 
63.66
58.81
4.30
12.16
23.97
35.23
46.50
64.40
77.28
38.64
12.88
25.76
51.52
0
10
20
30
40
50
60
70
80
sequential 1 2 3 4 5 6
Pe
rfo
rm
a
n
ce
 
(fp
s)
measured by execution estimated by model
 
Fig. 3.25. Performance evaluation for the proposed parallel 3D video playing 
system. 
  46 
4.1.1 Block-based Image Alignment Estimation 
The main purpose in the “Image Alignment” stage is to find the relationship 
between input frames. It is the most important part in the whole system because it 
would directly influence the accuracy of the stitched frame. The lower the accuracy of 
linear relationship between input is, the more complicated the post-processing would 
be, and vice versa. Therefore, finding the correct matched pairs of feature points is 
necessary to compute the linear relationship of inputs. A feature point is the 
representative point and has larger intensity than most points in the same image. 
Feature points are usually located on the edge of objects, that is, located on the 
junction between foreground and background objects. Therefore, these points are very 
helpful for object segmentation. In the presence of researches on feature extraction, 
there are many algorithms of image alignment proposed in prior articles. The number 
of extracted feature points could be tens to hundreds based on different algorithms, 
threshold value, or property of input frame. 
The proposed system adopts the “Scale Invariant Feature Transform” algorithm 
(denoted as SIFT algorithm) to find the feature point in individual view. The main 
feature of the SIFT algorithm is the invariant of scale, rotation and zooming action, 
that is, no matter what degree of rotation or zooming action is, the correctness of SIFT 
algorithm will not be influenced. 
As mentioned above, the SIFT algorithm has the invariance in degree of 
zooming. But from another point of view, the complexity of performing 
high-resolution image would be much larger than that of performing low-resolution 
image. For the purpose, we propose a method to reduce the SIFT execution time. The 
detail would be discussed in Section . 
  48 
Lens distortion is a deviation from rectilinear projection. As shown in Fig. 4.2, 
the lens distortion bends the tower and foundation of church. It is a form of optical 
aberration. Lens distortion is easier happened in a camera with wide-angle lenses than 
that with normal lenses. Besides, the edge of an image usually has a higher distortion 
than the center of an image. In fact, the probability of a feature point that falls on the 
edges is higher than that of a feature point that falls on the center. Therefore, if we 
randomly choose the feature pairs to solve the matrix, the solution of matrix would be 
bad once all the chosen referencing feature pairs are located on the edge of image. 
Moreover, a bad solution of matrix would further lead to a wrong stitched result. 
In order to avoid the problem mentioned above, we proposed a method to choose 
the adequate and representative sets among the feature pairs to compute the 
homologous matrix. The detail of the method is described as follows. 
(1) To split the source image into blocks with the same size. The size of each block 
depends on the resolution of the source image. 
(2) To record which block each feature point locates on 
(3) To find the block with the largest amount of feature points (called Block X). 
(4) To calculate the barycentric coordinate of feature points in Block X, and then to 
divide Block X into four regions according to the barycentric coordinate. 
(5) To choose a feature point with the high intensity from four individual regions 
respectively, and then to calculate the Homologous Matrix by using the four 
chosen feature point and their corresponding point in adjacent frame. 
4.1.2 Image Projection 
The main purpose in the “Image Projection” stage is to generate an initial 
stitched image. The process of generating the initial stitched frame uses the 
  50 
impracticable for a real-time panoramic video stitching process. Therefore, we 
proposed a low complexity repairing process, using a series of methods to deal with 
the different artifacts generated in previous stage, as shown in Fig. 4.3. Next, I will 
introduce the proposed work in the stage. 
 
Fig. 4.3 The Block Diagram of the Proposed Image Repairing and Blending 
Method 
 Texture-based Repairing 
The work of previous stage, “Image Projection and Warping” stage, contains the 
coordinate transformation. However, the transformation is usually not a one-to-one 
but a many-to-one function. This property would result in a fact that some pixels have 
no corresponding pixel in the source image. In this way, there would be some hole 
appeared in the stitched frame/video. If the degree of rotation difference between 
cameras is larger, or a frame with lower resolution is projected into a frame higher 
resolution, the hole will be much more. Fig. 4.4 shows the stitched image without 
being repaired. The black lines in the right side of Fig. 4.4 are composed of the 
amount of holes. 
  52 
function is shown in (26), where the ω is the weighted value. It depends on the 
proportion of distance between the hole and x1 or x2. In this example, the pixel will 
be interpolated with the 75% pixel 1 and 25% pixel 11. 
4
5
1
3
5
 
Fig. 4.6 Example of Removing the Hole by Performing Texture-based 
Repairing 
)1(** 21 ωω −+= xxxhole
 
(26) 
 Dynamic Optimal Seam Adjustment 
In some realistic applications of panoramic video stitching, there are often 
moving objects across the seam of stitched video. The seam is a line across two sides 
of the stitched frame, a border to distinguish the reference of source frame, and it 
must be located on the overlapping area of two source frame. If the seam is 
immovable in the video, once an object moves from one side of seam to another side, 
there may be a distortion happened to the moving object. To solve this problem, we 
proposed the “Dynamic Optimal Seam Adjustment” method to find the best position 
of the seam in the individual frame. The proposed method can efficiently prevent the 
seam from lying through the body of an object. The proposed method is described 
below. 
  54 
moving object. 
 Brightness Adjustment 
In the processing of capturing a video from multiple cameras, due to the various 
placements of cameras, the difference of sensitivity among adjacent cameras may be 
very large. The case would result in brightness unbalance of the source videos. 
Moreover, the seam will be very clear, as shown in Fig. 4.7. 
 
Fig. 4.7 The Stitched Frame before Performing Brightness Adjustment 
To avoid the situation, we propose a brightness adjustment scheme by utilizing 
the matched feature pairs acquired in “Image Alignment” stage and information of 
source frames. First, we calculate the average luminance value of all feature pairs in 
individual source frame (shown in (29)). Then we adjust all the luminance value of 
one source frame according to the difference of average between two source frames 
(shown in (30)). 
∑
∑
∈
∈
=
=
FPyxY
Rright
FPyxY
Lleft
R
L
yxY
n
Y
yxY
n
Y
),(
),(
)),((*1
)),((*1
 
(29) 
,),(),( rightleftoriginRresultR YYyxYyxY −+= RightViewyx ∈),(
 
(30) 
  56 
 
Fig. 4.9 Block Diagram of the Proposed Flow in Stitching Multiple Videos (5 
Videos) 
(1) First, we choose one of the source views as the major view. The perspective and 
brightness of the stitched view would take that of the major view as the reference. 
We usually choose the camera placed in the middle of all cameras as the major 
view. For example, we would choose the View3 as the major view in Fig. 4.9. 
(2) To divide the frame captured in the major view into two pieces. Then, we 
perform Image Alignment to find the features, and find the matching feature 
pairs of each couple of adjacent views. 
(3) For the left side of the major view, we first perform Image Projection and Image 
  58 
 
Fig. 4.10 Block Diagram of the Proposed Vertical Video Stitching Algorithm 
For the availability of vertical video stitching, we proposed a method to achieve 
it. First, we detect the spatial relationship of two views. If one of them is the topper 
view of another, we perform ninety degrees of rotation before the processing of image 
alignment. In this way, the original top view will be transferred into left view in 
spatial relationship and another will be transferred into right view. Then, we perform 
the image alignment, image projection and image repairing to obtain a stitched frame. 
Due to the previous ninety degrees of rotation, the rotation degrees of the output 
stitched frame will be also incorrect, so we perform the two hundreds and seventy 
degrees of rotation after the stitching process to recover it. The experimental results of 
stitching algorithm for the vertical cases will be illustrated in the later chapter. 
Fig. 4.11 shows the block diagram of the four-channel stitching flow (i.e., four 
views in spatial relationship of a two-by-two arrangement). First, views at a horizontal 
arrangement will be stitched respectively. After all of the horizontal stitching are 
finished, the vertical stitching process will be performed. The proposed algorithm has 
  60 
 
Fig. 4.12 PandaBoard with TI OMAP4430 
Table 4.1 Technical Specs of PandaBoard with TI OMAP4430 
Core Logic 
Dual-core ARM® Cortex™-A9 MPCore™ with 
Symmetric Multiprocessing (SMP) at 1 GHz each 
Memory 
1 GB low power DDR2 RAM 
Full size SD/MMC card cage with support for 
High-Speed & High-Capacity SD cards 
Main peripheral functions 
Onboard 10/100 Ethernet 
1x USB 2.0 High-Speed On-the-go port 
2x USB 2.0 High-Speed host ports 
General purpose expansion header (I2C, GPMC, USB, 
MMC, DSS, ETM) 
Camera expansion header 
HDMI v1.3 Connector (Type A) to drive HD displays 
DVI-D Connector (can drive a 2nd display, simultaneous 
display; requires HDMI to DVI-D adapter) 
LCD expansion header 
3.5"  Audio in/out 
HDMI Audio out 
Fig. 4.13 shows the setup of the experimental environment, including 
PandaBoard, a monitor with HDMI connector, and several USB cameras. In 
  62 
resolution of the source frame is high, we first obtain the sampling from the source 
image with lower sampling, and then use this sampling to find feature points. Table 
4.2 shows the look-up table of image resolution and down-sampling scale. 
Table 4.2 The Source Image Down-sampling Table 
Resolution Scale 
CIF 2×2 
VGA 2×2 
SVGA 2×2 
XGA 3×3 
WXGA 4×4 
HD1080 4×4 
Fig. 4.14 shows the results in comparison of two different processes, where (a) is 
obtained without using the dynamic down-sampling and (b) is acquired with using it. 
The results show that the subjective quality will not be influenced. 
  64 
will release the key that is originally locked by process of current frame, and then we 
check if the key locked by process of the neighboring frame has been released. If not, 
we will return to the parent process and release the CPU resource to the system; 
otherwise, we will directly match the features of these two adjacent frames. With the 
mentioned strategy, we can not only perform a good CPU utilization, but also reduce 
the overhead of allocating threads. 
After all the feature matches are obtained, we start the follow-up works. In 
Section 0, we have introduced the processing in stitching from multiple views shown 
in Fig. 4.9. We choose the intermediate view as the major view, and then we stitch and 
repair the major view and its two neighboring view first. In realization on PandaBoard, 
we separate the all processing into left side of major view and right side of major view 
and assign them to the different CPU. After finishing the individual processing, we 
perform the translation stitching and trimming at last. The performance of this 
strategy will show in next chapter. 
4.3 Performance Evaluation and Demonstration 
4.3.1 Performance Evaluation 
In this section, I will show the performance with the proposed approach. First, 
we show the improvement of performance with down-sampling method on PC with a 
Pentium 4 3.0GHz CPU. Fig. 4.15 shows the execution time in SIFT stage. In 
principle, the execution time of SIFT has the positive correlation with the resolution 
of the image. As shown in Fig. 4.16, with down-sampling method, we can see that the 
execution time of higher-resolution is not always longer than that of lower-resolution. 
That’s because the image with higher resolution may be defined a larger scale factor 
than the image with lower one. The relationship between image resolution and scale 
  66 
Alignment is only performed during processing of the first frame. That is, we can 
regard it as the initialization of the system, or even ignore its long execution time. 
Except the execution of Image Alignment, we can get performance of about 12 frames 
per second. 
Table 4-3 Execution Time on PandaBoard 
Stage Execution Time (Second) 
Image Alignment 7.148 
Image Projection and Warping 0.037 
Image Repairing and Blending 0.043 
file I/O 0.005 
4.3.2 Demonstration 
In this section, we show the experimental results in different cases. Fig. 4.17 
shows the experimental result before and after performing image repairing. Besides, 
there is a difference of rotation degree in this set of test cases and the result also 
shows the invariance of rotation in proposed algorithm. 
  68 
 
Fig. 4.18 Multiple Test Frames and the Experimental Result 
(a), (b) and (c) are the input frames. (d) is the final stitched frame. 
Fig. 4.19 shows the experimental results of stitching vertical views. The result 
shows that we can generate a panoramic view from the inputs at not only the 
horizontal arrangement but also the vertical arrangement. 
 
Fig. 4.19 Vertical Test Frames and the Experimental Result 
(a), (b) are the input frames. (c) is the final stitched frame. 
  70 
5. Conclusion and Future Work 
In this report, we have proposed a series of software optimization schemes 
covering both algorithm-level and code-level aspects to perform real-time 
VC-1/H.264/AVS video decoding on ARM-based platform. According to the 
proposed techniques, we have reduced about 80% ~ 90% of complexity by way of 
optimizing the VC-1/H.264/AVS video decoders in C code level without using any 
hand-written assembly code. Therefore, it can be seen that the proposed optimization 
schemes can effectively enhance the performance of new generation video decoders 
on embedded processors. 
On the other hand, we have also proposed a low complexity algorithm of 3-D 
depth map generation for stereo applications. Base on the proposed algorithm, we can 
generate the depth maps in good quality for most 2-D images. In addition, the 
processing steps in the proposed algorithm have been optimized for reducing its 
complexity while preserving good quality. We have achieved about 90% of 
complexity reduction in terms of execution time as compared to the original ones. 
Moreover, we have applied the proposed 3D depth algorithm to a 3D video playing 
system and realize this system on multi-core processors based on superscalar-like 
parallelism. Under the proposed superscalar-like parallel computing, the proposed 3D 
video playing system can achieve real-time processing for the videos in HD720 
resolution. 
In addition, we have also proposed a low complexity panoramic video stitching 
algorithm to smoothly stitch two or more videos with overlapped area into a 
panoramic one. By combining the video decoding, panoramic video stitching and 3D 
video depth map generation, we are able to carry out a panoramic 3-D video player.  
 
  72 
[9] ISO/IEC FDIS 14496-10, “Information Technology - Coding of Audio-Visual 
Objects, Part 10: Advanced Video Coding”, 2004. 
[10] GB/T 20090.2, “Information Technology - Advanced Coding of Audio and Video, 
Part 2: Video”, 2006. 
[11] ISO/IEC 11172-2, “Information Technology - Coding of Moving Pictures and 
Associated Audio for Digital Storage Media at up to about 1.5Mbit/s, Part 2: 
Video”, 1993. 
[12] ISO/IEC 13818-2, “Information Technology - Generic Coding of Moving Pictures 
and Associated Audio Information, Part 2: Video”, 1996. 
[13] ISO/IEC FDIS 14496-2, “Information Technology - Coding of Audio-Visual 
Objects, Part 2: Video”, 2001. 
[14] “Information Technology - Advanced Coding of Audio and Video, Part 7: Mobile 
Video”, The Audio Video Coding Working Group of China, 2005. 
[15] L. Fan, S. Ma, F. Wu, “Overview of AVS Video Standard”, IEEE International 
Conference on Multimedia and Expo, vol. 1, pp. 423-426, Jun. 2004. 
[16] Z. Ma, X. Jin, W. Y. Liu, G. X. Zhu, “Complexity Analysis of AVS-M Jiben 
Profile Decoder”, International Symposium on Intelligent Signal Processing and 
Communication Systems, pp. 769-772, Dec. 2005. 
[17] R. O. Duda and P. E. Hart, “Use of the Hough Transformation to Detect Lines and 
Curves in Pictures”, Communications of the ACM, vol. 15, no. 1, pp. 11-15, Jan. 
1972. 
[18] M. T. Pourzad, P. Nasiopoulos, and R. K. Ward, “Conversion of H.264-encoded 
2D video to 3D format”, IEEE International Conference on Consumer Electronics, 
pp. 63-64, Jan. 2010. 
[19] L. Zhang, W. J. Tam, and D. Wang, “Stereoscopic image generation based on 
depth images”, IEEE International Conference on Image Processing, vol. 5, pp. 
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                                100 年 1 月 18 日 
報告人姓名 
簡呈安 
就讀校院 
（科系所） 
                     ■博士班研究生 
國立中正大學 
資訊工程研究所 
                     □碩士班研究生 
     時間 
會議 
     地點 
2011/01/25 ~ 2011/01/28 
 
日本 橫濱 
本會核定 
補助文號 NSC99-2220-E-194-012 
會議 
名稱 
 (中文) 亞洲南太平洋設計自動化研討會 
 (英文) Asia and South Pacific Design Automation Conference (ASP-DAC) 
發表 
論文 
題目 
 (中文) 支援時間空間調整之H.264/MPEG-2雙模式視訊解碼器 
 (英文) A H.264/MPEG-2 Dual Mode Video Decoder Chip Supporting 
Temporal/Spatial Scalable Video 
 
報告內容應包括下列各項： 
 
一、參加會議經過 
2011亞洲南太平洋設計自動化研討會議(ASP-DAC 2011)於2011年1月25日至1月28日在日本橫
濱Pacifico Yokohama會議中心舉行，此會議主要是一個以設計自動化需求為導向和主軸之資訊電子
科技研討會，但同時也涵蓋許多系統與電路設計的議題在其中，因此會議議程涵蓋了先進的記憶體
設計、多核心架構設計處理、網路處理、多媒體訊號處理還有低功率設計等研究領域。此項會議是
由IEEE所主辦，且是屬於DAC的亞洲太平洋地區研討會，因此此項會議每次皆會有許多來自日韓
中國等優秀的人才來一起參與這個研討會。 
本人此次參加此會議之主要目的為參加其中的大學大型積體電路設計競賽(University LSI 
Design Contest)並在其中發表名為” A H.264/MPEG-2 Dual Mode Video Decoder Chip Supporting 
Temporal/Spatial Scalable Video”的論文，此篇參賽論文是由本人的指導教授郭峻因教授所指導，並
透過中正電機的王進賢教授以及逢甲電子的鄭經華教授及其學生們的合作，以及本人的博士班學長
楊曜彰、張修誠和陳嘉偉共同開發以及撰寫，才有此篇參賽論文的產生。此篇論文的發表方式是
oral presentation 以及 poster presentation 兩種方式。論文發表當日，本人需於會議室前用口頭的方
式配合投影片用英文發表，且於會後需在另外一間展示室張貼海報，並站於海報前方對在場觀看學
者進行一場簡略之介紹，並且針對部分技術細節進行意見交換或是回答問題，皆全程使用英文。因
此在這會後一小時張貼海報的時間當中本人透過討論的過程，獲得許多同領域學者之寶貴建議，可
做為未來改進研究成果之重要參考依據。 
且最後本篇參賽論文並獲得所有參與學生競賽的論文中最好的獎項最佳設計獎，並獲得獎牌一
面，並於講台上接受大會主席的搬獎以及合照。 
 
 
行政院國家科學委員會補助國內研究生出席國際學術會議報告 
                                                                100 年 1 月 20 日 
報告人姓名 簡國安 
就讀校院 
（科系所） 
                     ■博士班研究生 
國立中正大學 
資訊工程研究所 
                     □碩士班研究生 
     時間 
會議 
     地點 
2011/01/09 ~ 2011/09/12 
 
美國拉斯維加斯 
本會核定 
補助文號 
NSC99-2220-E-194-012 
會議 
名稱 
 (中文) 國際消費性電子會議 
 (英文) International Conference on Consumer Electronics 
發表 
論文 
題目 
 (中文) 適用於嵌入式系統之低複雜度影像接合演算法 
 (英文) A Low-Complexity Image Stitching Algorithm Suitable for Embedded Systems 
 
報告內容應包括下列各項： 
六、參加會議經過 
2011國際消費性電子會議(ICCE 2011)於2011年1月9日至1月12日在美國拉斯維加斯國際會議
中心舉行，此會議是一個以消費性電子為主軸之整合型研討會，會議議程涵蓋了行動通訊、影音多
媒體、家庭娛樂、車用電子、居家看護等研究領域。 
本人此次代表本人的指導教授郭峻因教授參加國際消費性電子會議並發表一篇名為”A 
Low-Complexity Image Stitching Algorithm Suitable for Embedded Systems”的論文，此論文由本人的
指導教授郭峻因教授所指導，作者包括本人的學弟簡呈安、張道城及張家豪，此篇論文的發表方式
是 poster presentation，論文發表當日，本人與簡呈安學弟透過事先準備的解說海報與在場學者專家
進行面對面的交流，並且針對部分技術細節進行意見交換，透過討論的過程，我們獲得許多同領域
學者之寶貴建議，可做為未來改進研究成果之重要參考依據。 
 
七、與會心得 
此次會議乃本人第七次參與之國際研討會，比較特殊的是，此研討會每年都與著名的國際消費
性電子展(CES)聯合舉行，因此，在此次會議中，除了在會議中進行學術交流外，本人也撥空參觀
了國際消費性電子展，此展覽可說是消費性電子產業的年度盛事，如 Intel、Microsoft、Sony、Samsung
等國際知名大廠皆卯足全力推出各式各樣的新產品，由於 CES 展場甚多，本人參觀的是以多媒體
影音為主的展場，其中可看到許多日韓大廠推出之 3D 電視及多視角電視，充分讓人感受到 3D 影
像技術的蓬勃發展，展場中某些 3D 電視所顯示出的 3D 影像甚至具備相當細膩的層次感，然而，
比較可惜的是大多數 3D 電視仍需要使用者配戴 3D眼鏡才能體驗到立體影像，希望在不久的將來
3D 電視能讓使用者在裸視的情況下就能看得到逼真的立體影像，此外，Sharp 公司所開發之高亮
彩螢幕也著實令本人大開眼界，其螢幕所呈現出的色彩鮮豔度幾乎等同於肉眼所見一般，讓人不得
不為之驚豔，希望明年能有機會再次參與這項年度盛事。 
  
  
  
  
  
  
 
 
 
 
 
 
 
 
  
  
  
  
  
  
 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：郭峻因 計畫編號：99-2220-E-194-012- 
計畫名稱：Criti-core：超越多核心之高可靠度晶片系統平台技術開發--子計畫五：全景立體視訊播
放系統於 Criti-Core 之設計與實現(2/2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 2 1 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 8 4 100%  
博士生 7 3 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 3 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 7 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 2 2 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
