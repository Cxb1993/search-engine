a ’job’. In this work, we would design a kernel 
which can use multiple kernel threads to process 
a ’job’. 
 
The second goal of this project is to design a 
power/thermal-aware OS kernel. Thermal issues are 
fast becoming major design constraints in high-
performance systems. In this project, we would like 
to propose a system-level framework for doing thermal 
management using a hybrid of hardware techniques 
(e.g., clock gating) and software techniques (e.g., 
thermal-aware process scheduling). We will further 
use the thermal information from embedded sensors to 
optimize the schedule. 
 
The third goal of this project is to design a 
multithread multi-core simulator. Current 
architecture trends focus on designs that exploit 
thread-level parallelism using multiple cores on 
chip. With increasing number of cores, the simulation 
run time increases accordingly with best-case linear 
scaling. These large turnaround times prohibitively 
limit the ability to evaluate performance tradeoffs 
during the design phase. In this project, we would 
like to design a multi-core simulator which can take 
the advantage of multi-core platform (e.g., Pentium 
Core i7) to shorten the simulation time. 
 
英文關鍵詞： multithreading-safe kernel, simulator 
 
目錄 
 
  
英文摘要 
 
The title of this project is “Power/thermal-aware multithreading OS kernel”. The first goal of this project is to 
design a “real” multithread operating system kernel. Most operating systems including Linux, BSD and 
Windows NT are designed as reentrant kernels. A reentrant kernel is a multithreading-safe kernel which 
means the access of shared variables is controlled /protected by using locking mechanisms (e.g., spinlock, 
semaphore). Please notice that there are many kernel threads running in kernel mode and each kernel thread is 
designated to process a “job”. In this work, we would design a kernel which can use multiple kernel threads to 
process a “job”. 
 
The second goal of this project is to design a power/thermal-aware OS kernel. Thermal issues are fast 
becoming major design constraints in high-performance systems. In this project, we would like to propose a 
system-level framework for doing thermal management using a hybrid of hardware techniques (e.g., clock 
gating) and software techniques (e.g., thermal-aware process scheduling). We will further use the thermal 
information from embedded sensors to optimize the schedule. 
 
The third goal of this project is to design a multithread multi-core simulator. Current 
architecture trends focus on designs that exploit thread-level parallelism using multiple cores on chip. With 
increasing number of cores, the simulation run time increases accordingly with best-case linear scaling. These 
large turnaround times prohibitively limit the ability to evaluate performance tradeoffs during the design 
phase. In this project, we would like to design a multi-core simulator which can take the advantage of 
multi-core platform (e.g., Pentium Core i7) to shorten the simulation time. 
 
 
Keyword：criticore, multithreading-safe kernel, simulator 
  
 圖二：目前 Linux 及 BSD 上的主要省電機制。在系統進入 idle 狀態以後（執行於 idle loop），硬體
將逐漸的由淺眠模式切換到最省電的休眠模式。 
 
 硬體控制之溫度、頻率調控雖然足以保證硬體系統之穩固性，但軟體系統因無法得知或者只能被
動的告知當前之系統執行頻率，因此作業系統難以保證於上運行之應用程式都能獲得足夠的質量保證
（quality of service, QoS）。在本計畫中，軟體將和硬體合作，協同調整處理器之溫度。 
 
 
	    
錯誤（如：race condition。如圖四是一個錯誤的Peterson’s solution，在真實的機器上面可能發生錯
誤，但在一些虛擬機器上面（如：Qemu），因為這些虛擬機器將數道指令編譯成同一個basic block，
而每個basic block 執行完成以後才會檢查是否有中斷或者I/O 需要處理，因此對虛擬機器上的target 
machine 而言，這個錯誤的Peterson’s solution程式碼會被視為一個不可分割（atomic）之指令群，
因此這個錯誤永遠都不會發生。詳見圖五）。 
 
圖四：一個錯誤的 Peterson’s algorithm。由於 flag[]與 turn 的順序錯誤（正確的方式是先設定 flag[]再
設定 turn），將造成二個行程（process0 及 process1）都進入到 criticalsection 中。 
	  
 
圖五：圖四所述之錯誤的 Peterson’s solution 在 Qemu 上的執行狀況。由於 Qemu（或其它種 binary 
translation 的方法）把一連串的指令轉譯到一個 basic block 中，而每一個 basic block 執行結束後才會
檢查中斷發生狀況。因此在 basic block 間不會發生 context switch，而錯誤的 Peterson’s solution 的執行
結果將永遠正確。 
 
說，它可以把這一件事，分給兩顆核心各自做，每顆核心各做 5 次，所以所需要的時間就只需要 
50 秒！當然，多執行緒的程式實際上沒這麼簡單。在工作的切割、結合上，也是要多花時間的，
所以在現實中，即使最佳狀況，雙核心的效能也不會是 1 + 1 = 2 這樣的理想化。除此之外，也不
是所有工作都是可以切割的！很多工作是有關聯性的，這樣如果直接切割給不同的處理核心各自
去平行運算，出來的結果是肯定有問題的。而且，多執行緒的程式在編寫、維護上，也都比單一
執行緒的程式複雜上不少。 
 不過，如果電腦本身是多處理器、多核心處理器，或是處理器擁有像Intel Hyper-Threading 
Technology 這類的能在同一個時間處理多個執行緒的功能的話，那把各自獨立的工作由單一執行
緒改成多執行緒，在執行的效率上，大多還是會有增進的！但是，實際上要去控制 thread是滿麻
煩的。在程式的編寫上，也會複雜不少；而如果我們只是想要把一些簡單的迴圈平行化處理，用 
thread library 來控制，實在有點殺雞用牛刀的感覺。這時候，用 Open MP 就簡單多了！在最簡
單的情形，甚至可以只加一行指令，就可以將迴圈內的程式平行化處理了！ 
 然而OpenMP[1][2][3][4][5]就是一種能透過高階指令，很簡單地將程式平行化[10]、多執行緒
化的 API，他是一種Shared-Memory Multiprocessing的架構[6]；除了OpenMP之外，還有
MPI[5][7][8][13][16]也是以scalable High  Performance Computing[9]為中心發展的技術，但是MPI 
則是使用Distributed-Memory 的架構。近年來在提高程式平行化方面的研究[14]中上述兩種方法都
經常出現。關於這兩者之間的性能比較[12]，在近年來也是一個熱門的研究方向，至於另外一個方
向則是在多核架構下研究兩者之間的混合編程[15][17][18]互動對系統帶來的效益。本專案所選用
的OpenMP對compiler 有依存性，所以另外也衍伸出OpenMP compiler[13]設計這一個方向，另外
也有很多新議題是從這個方向衍伸出來的。 
 至於Linux中多個thread 可能同時存取的記憶體、變數或函數稱為Critical Section[19]。Critical 
Section 類別用於在多執行緒環境中保護資源。通常這種要受保護的程式區段稱為Critical Section。
為什麼要保護呢？因為在程式裡有可能有兩個 thread (可看成一個小小的function)同時存取一個
global variable(全域變數)(或函數)，這時後，因為程式的需要，thread 不想被其他程式中斷(不被其
他thread 插入影響)，所以必須要一口氣執行完畢，而該需要一口氣執行完的程式區段則要設定 
Critical Section，以保護目前執行的thread 不被其他thread 影響。而有許多相關研究是在如何測量
資源競爭和死結的問題[20][21]，甚至研究如何發展新的演算法[22]來解決該問題。而在Unixlike 作
業系統底下還存在一種特殊的 thread 稱之為kernel thread[18](核心執行緒)，kernel thread 未必只
有執行核心的某種功能，亦可能與某個 User program 有關係，然而 kernel thread 與一般的 
process 比較起來具有較輕鬆的環境切換代價，通常稱之為 Context switch，它發生在 process 調
用 scheduler 進行排程的時候，要把當前 process 所用到的 CPU 資訊給儲存起來，包含了 
Register(暫存器)等等。因為 thread 屬於簡單的 execution flow 所以每當要進行 Context switch 時，
要儲存的資料會比較少，因此負擔會輕一些。kernel thread 的特性如下：運作在Kernel space & 
Kernel Mode、不與 User 進行互動，因此不需要終端機設備 以及於系統啟動時期建立，存活
到系統關閉為止。在傳統的Unix like 作業系統，偶爾會將一些關鍵的任務委派給執行中的 
process ，比如像是 Page swapping、服務網路連線、Disk cache等等之類的。不過現在的作業系統
大多都會把這些任務放在背景去執行，避免使用線性的方式而得到不好的效能特別是 kernel 
thread 只運作在 kernel space 之中，因此這些工作最好都交給 kernel thread 來做最適當不過了！
一些典型的例子，例如Swapper, init process 都是屬於 kernel thread，雖然在Linux 使用 kernel 
thread地方很有限，但是卻是用來執行幾個重要的核心功能。 
 目前linux kernel module 的設計方式，已有相關書籍以及網路資源可以參考(如[23][25])。在傳
統上，UNIX 系統將硬體裝置分成三種類型，每個模組(kernel module)通常只實作其中一種類型，
因此，模組也可以分成三大類，分別是字元式模組(char module)、區塊式模組(block module)、以
 多重模擬環境目前也有許多被設計用來模擬不同的環境，例如HotLeakage 模擬器[70]能夠用
來測量耗電量，而 SSMT[71], M-Sim[72]或是SMTSim[73], 則是SimpleScalar 的改良版，能模擬多
個執行緒，但他們之間能用的資源以及workload是固定的。多執行緒和多核心的延伸也被應用到
Turandot 模擬器[74][75]，它model一個PowerPC架構而且也被使用於電源管理的目標。針對
Turandot 的延伸平行的微架構，它是最常被引用的[76]，但並不是公開可用的。 
 Simics [77]是一個通用整合系統模擬器的，通常使用於多處理器系統的模擬，很不幸的，這套
模擬器並不是免費提供的。各種Simics 衍生出來的工具，已經被實做在各種特定研究目的的領域。
例如GEMS [78]，它導入了timing simulation module 去模擬一個完整的處理器管線和一個支援
cache conherence 的記憶體階層。然而，GEMS提供較低彈性的 modelling  multi-threaded 設
定，而且它合併了沒有互連的network model。一個重要的特點包括在一些processor simulators使用
timing-first方法，GEMS提供和採納Multi2Sim。在此項計畫，一個timing module 會追縱processor 
pipeline 的狀態，當有道指令阻礙它時，目前系統有可能是不穩定的狀態。 
 另外有一個模擬器是M5[79]，它支援了亂序SMT-capable CPUs、多處理器、cache coherency、
能執行整個系統和應用程式。而有些限制會好發於較低彈性的multi-threaded pipeline 的設計。目
前有[89][90]提到可行的技 術，可以用來減低模擬時間，包括透過縮減模擬精確度，以及採用靜
態模擬。靜態模擬可以減少動態執行時，需付出額外運算時間進行模擬。而[91]則 是利用 
"off-line" 的方式模擬，它可以動態進行 profiling，以及插入hotspot可用來偵錯。利用這項技術可
以達到減少模擬時的時間，以及可用來量測模擬器的執行效率。但利用此項技術，模擬時所需的
時間無法精確測量，而且當需要模擬的 CPU 變多時，模擬時間將會提升。MinneSpec[92]以及SPEC 
train 則是用來提供模擬器模擬的資料。 
 最近的一個方法是使用有 FPGA 的硬體及軟體共同模擬去加速模擬器的速度。Chiou et al [93]
等人把模擬器分割成timing和functional models 和實做timing model在硬體。Dave et al [94]實做上述
所描述的兩項在tightly coupled fashion的FPGA上。使用FPGA用於加速模擬器是一項非常有用的方
法。但是一個較詳細的評估必須充份的利用system models 和multiple workloads。 
 也有一些研究特別專注於加速multi-core 的模擬[85][86][87][88].在模擬多處理器時這些方法
用平行模擬去simulator有些時候，平行模擬包含了在原生的硬體上直接執行然而模擬器仍然執行
任何無法在host上面直行的運作[88]專注於一個平行的多核心模擬器，使用simplescalar模擬各個
thread Similarly Barr et al [87]修改了ASIM 的基礎部分去使他平行，Mukherjee et al [86]辨識模擬 
運作且長是去最小化他們在host 端的相依性，[85]專注於自動模擬平行化與整合此硬體到CMP。 
 而[80][81]，包涵了大部分相關於dynamic binary translations：dynamic translators、dynamic 
optimizers、及相關的硬體研發。[13]將binary translators做了大致上的分類。Transmeta CMS[81]是
一種在VLIW 硬體上模擬IA-32 的動態轉譯，它除了可以運行IA-32 作業系統，也模擬了整個運
算架構。FX!32[83]用IA-32模擬Alpha 架構的動態與靜態混和式translator，有更低階的相容性（例
如：倍精度浮點數運算）。而HP Aries[84]可在HPUX* OS 上轉譯PA-RISC 的二元碼到IPF，它關
鍵點為在basic-block level 作最佳化。Optimizers 如Dynamo，其轉譯相同的指令集架構，試著產生
能比原始編譯出來有更好效能的code，轉譯到相同的指令集架構，無論最佳化結果是否無效
Dynamo 可以“bail-out＂原本的執行。“Bailing out＂不適用於binary translation。 
 [95] Wisconsin Wind Tunnel 模擬一個平行,CC-NUMA 系統在在一個 Myrinet-connected 
cluster 的工作站[96]在messaging layer提供同步的訊息 而不是在MPI 做WWTmakes 也使用直接
執行每一個平行的process在不同 的cpu，只在記憶體架構中對記憶體存取的時間順序去做同步這些
研究沒辦法提供processor model 的彈性，而且無法去模擬像是issue width, speculative memory 
accesses, out-of-orderexecution 的效果[99] Analytical modeling 已經被使用來近似WWT 在不同的
系統大小上面的效能，有趣的是Falsafi and Wood 展示了在目標機器，16-32 顆處理器上最佳化的
(四) 研究方法  
在開始實驗情境上，我們預設在 ARM11 MPCORE 上進行 OS 的開發，其中透過開發 CCOS 和
與 Linux 做結合，來達到建構多核心執行環境。而 CCOS 與 Linux 之間的互動是透過 IPI(inter processor 
interrupt)和預先設定好，位於 CCOS 記憶體區段的共用記憶體來達成。採用 CCOS 區段的記憶體當
作共用記憶體的除了能提高可移植性外，而且也可以避免更動 Linux 的記憶體管理機制。當系統中發
生事件，如 Linux 需要指派工作到 CCOS，或是 CCOS 的工作完成時，可以透過 IPI 來通知對象核心
有事件發生。而實質的溝通則是透過 share memory 交換訊息達成。 
 
 Agent and proxy 
利用上述兩個溝通手段，我們設計出 Agent 和 Proxy 這兩個機制。下圖為 CCOS 的架構示意圖，
Agent 位於 Linux 核心中，負責記錄和取得目前 CCOS 上目前正在執行的工作資訊，發佈工作到 CCOS 
TCB，協助 CCOS 載入可執行檔以及代理 CCOS 執行 System call，每一個 Criticore 核心上都有一個
對應的 Agent。相對於位於 Linux 端的 Agent，Proxy 是位於 CCOS 上的溝通輔助機制，主要的工作
是通知 Linux CCOS 需要 System call 的支援。不直接讓 CCOS 處理 system call 的目的除了能讓 CCOS
能全力來處理平行化運算外，更重要的原因是 Linux 有完整的 driver、IO 支援，比起剛起步的 CCOS
更為適合處理這些工作。 
 
 
圖 LINUX 和 CCOS 溝通架構圖 
 
Agent mechanism 
下圖說明這個機制的詳細流程。使用者如果需要 CCOS 來協助平行化加速時，就把需要平行化的
工作用函式包覆後利用 System call 發送到 Agent。Agent 從 user  space 複製資料到 kernel 後，載入
CCOS 執行的 binary 載入 CCOS 記憶體，然後發出 systemcall。再根據傳入的參數設置 TCB 和 virtual 
address。前三點單純是基於載入時的流程一起完成，值得一提的是第四點。CCOS 不使用 demand 
paging 的方式分配實體記憶體，而是一次就把全部需要的部分都載入記憶體中，而且只要載入後就不
會被換出，所以不會發生 page fault，可以有效增進效能並提高結果的可預測性。 
	  
 以上是在 ARM11 MPCORE 上開發出的架構。在後續開發時，為了開始進行與其他相關計畫作結
合，考慮許久後暫停進行更進一步的記憶體管理機制開發與溝通介面的完善。轉而採 Virtual Machine
的概念讓 Linux 與 CCOS 的互動有類似的環節，Linux 負責當作特定的核心處理，控管所有的工作，猶
如 Virtual Machine Monitor（VMM），而 CCOS 接收從 Linux 分配來的工作進行處理，也就是在 VMM
上運行的 Virtual Machine。因此，在這樣設計上轉為由 Linux 當作底層，控制所有的 core，而 CCOS
透過設計好的一套排程系統，將 CCOS 上的工作分配給 Linux 逕行處理。這樣的設計理念除了環境容
易建構之外，也為了具有移植性。在 criticore 計畫中，考慮道友一天將會在實際上的硬體運作，這樣
的模式可以幫助開發者可以快速的將 OS 移植上去。 
 
 在設計介面上，為了與相關計畫做整合，也為了讓一般多核心開發者可以快速的上手這套系統，
因此藉由仿 pthread library 的介面，讓使用者要進行平行化處理時，只需依照 pthread 的使用方式來進
行程式撰寫。實作上，我們設計一套 library 來包裝 Micro C/OS-II 的工作處理函式，透過這套 library，
使用者可使用 thread 函式來控制 micro C/OS-II 的 task，包涵 task create、task exit、lock 等。在與子計
畫 3b 整合中，3b 所建立的 SpMT library 可使用我們這套 library 來進行他們的工作。 
(五) 結果與討論  
本計畫於實作面上最主要的產出為 
1. 將 CCOS 與 Linux 同時執行於 ARM MPCore 上。我們使用 Linux 作業系統以單核心的方式開
機，隨後由 Linux 載入 CCOS，並且讓 CCOS 執行於另外的三顆核心。由於所使用的實驗版（ARM 
versatile）具有相當豐富的周邊，這些周邊的驅動程式撰寫不易，因此將執行 Linux 的 ARM 視
為 I/O processor，將執行 CCOS 的 ARM 視為 application processor。在這樣的運行模式下（於
同質處理器上，運行異質作業系統），可以很快的驗證 CCOS 的多功能力。 
 
圖：啟動程序，單核心的 Linux，將多個 CCOS 載入到 Core2~Core3 
 
2. 將部分的 pthread library 移植到 CCOS 上。由於子計畫三所產生的多執行緒程式使用到 pthread 
library，因此在這個計畫案中，我們將 pthread 放到 CCOS 上。 
ACM/IEEE SC 1998 Conference (SC '98), 1998。 
[15] Felix Wolf a,*,1, Bernd Mohr b,"Automatic performance analysis of hybrid MPI/OpenMP 
applications,Journal of Systems Architecture 49 (2003) 421–439。 
[16] W. Gropp, E. Lusk, N. Doss, and A. Skjellum. AHigh-Performance, Portable Implementation of the 
MPI,Message Passing Interface Standard. Technical report,Argonne National Laboratory and Mississippi 
StateUniversity. 
[17] Woo-Chul Jeun,Yang-Suk Kee,Soonhoi Ha,Changdon Kee,＂Overcoming performance bottlenecks in 
using OpenMP on SMP clusters＂ 
[18] The Linux Threads library, http://pauillac.inria.fr/~xleroy/linuxthreads/ 
[19] Chandrasekhar Boyapati, Robert Lee and Martin Rinard. "Ownership Types for Safe Programming: 
Preventing Data Races and Deadlocks Proceedings of the 17th ACM SIGPLAN conference on Object, 2002 
[20] Dawson Engler and Ken Ashcraft. RacerX: Effective, Static Detection of Race Conditions and Deadlocks. 
Operating systems review, 2003 
[21] Robert O＇Callahan and Jong-Deok Choi. "Hybrid Dynamic Data Race Detection.Proceedings of the 
19th ACM SIGPLAN symposium on Principles" , 2003 
[22] G.-I. Cheng, M. Feng, C. E. Leiserson, K. H. Randall, and A. F. Stark. Detecting data races in Cilk 
programs that use locks. Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and 
Architectures 
[23] Linux Device Drivers，3rd Edition 
[24] Advanced Programming in UNIX Environment 
[25] http://www.jollen.org/LinuxDeviceDriver/ 
[26] https://computing.llnl.gov/tutorials/pthreads/ 
[27] J. D. Davis, J. Laudon, and K. Olukotun. Maximizing CMP Throughput with Mediocre Cores. In 14th 
International Conference on Parallel Architecture and  Compilation Techniques (PACT’05), 2005. 
[28] J. Donald and M. Martonosi. Techniques for Multicore Thermal Management: Classification and New 
Exploration. In Proceedings of the 33th International Symposium on ComputerArchitecture (ISCA-33), 
2006. 
[29] R. Kumar, N. P. Jouppi, and D. M. Tullsen. Conjoined-Core Chip ltiprocessing. In Proceedings of the 
37th International Symposium on  Microarchitecture (MICRO-37), 2004. 
[30] Y. Li, D. Brooks, Z. Hu, and K. Skadron. Performance, Energy and Temperature Considerations for SMT 
and CMP Architectures. In 11th International Symposium on High Performance Computer Architecture 
(HPCA-11), 2005 
[31] M. Powell,M. Gomaa, and T. N. Vijaykumar. Heat-and-run: Leveraging SMT and CMP to manage 
power density through the operating system. In Eleventh International Conference on Architectural 
Support for Programming Languages  and Operating Systems (ASPLOS XI, 2004) 
[32] F. Zhang, S.T. Chanson. ”Processor voltage scheduling for real-time tasks with non-preemptible sections,” 
In Proc. of the 23rd Real-Time Systems Symposium, 2002. 
[33] P. Pillai, K. G. Shin, “Real-time dynamic voltage scaling for low-power embedded operating systems,” 
In Proc. of the 18th ACM Symposium on Operating Systems Principles.2001. 
[34] S. Eggers. “Simultaneous Multithreading Project,”http://www.cs.washington.edu/research/smt/ 
[35] R. Chabukswar, “Maximizing Power Savings on Mobile Platforms,” 
http://www.intel.com/technology/magazine/ 
[36] R. Jain, C. Hughes, S. Adve. “Soft real-time scheduling on simultaneous multithreaded processors,” In 
Proc. of the 23rd IEEE Real-Time Systems Symposium, 2002. 
[58] J.R. Lorch, A.J. Smith. “Improving dynamic voltage scaling algorithms with PACE,” In Proc. of the 
ACM SIGMETRICS 2001 Conference, 2001. 
[59] D. Mosse, H. Aydin, B. Childers, R. Melhem. “Compiler-assisted dynamic power-aware scheduling for 
real-time applications,” In Proc. of the Compilers and Operating Systems for Low-Power, 2000. 
[60] T. Pering, T. Burd, R. Brodersen. “Voltage scheduling in the lpARM microprocessor system,” In Proc. of 
the International Symposium on Low-Power Electronics and Design, 2000. 
[61] J. Pouwelse, K. Langendoen, H. Sips. “Dynamic voltage scaling on a low-power microprocessor,” In 
Proc. of the 7th Conference on Mobile Computing and Networking, 2001. 
[62] E.G. Coffman, Jr., M. R. Garey, D. S. Johnson.. “Approximation algorithms for bin packing: A survey,” 
in D. Hochbaum (ed), PSW publishing, Boston.  Approximation Algorithms for NP-Hard Problems, 
1996. 
[63] Y. Li, D. Brooks, Z. Hu, K. Skadron. “Performance, Energy, and Thermal Considerations for SMT and 
CMP Architectures,” In Proc. of the High-Performance Computer Architecture, 2005. 
[64] R.L. Ramesh. “Embedded Operating System Power Management” 
[65] I Kadayif, M Kandemir, U Sezer,. “An Integer Linear Programming Based Approach for Parallelizing 
Applications in On-Chip Multiprocessors,” In Proc. of the 39th Design Automation Conference, 2005. 
[66] B Lee, D Brooks. “Effects of Pipeline Complexity on SMT/CMP Power-Performance Efficiency,” In 
Proc. of the Workshop on Complexity  Effective Design, 2005 
[67]M. Annavaram, E. Grochowski, and J. Shen. Mitigating Amdahl’s Law Through EPI Throttling. In 
Proceedings of the 32nd International Symposium on Computer Architecture (ISCA-32), 2005 
[68] J. Li and J. Martinez. Power-Performance Implications of Thread-Level Parallelism on Chip 
Multiprocessors. In IEEE International Symposium on Performance Analysis of Systems and Software 
(ISPASS’05), 2005. 
[69] J. Li and J. Martinez. Dynamic Power-Performance Adaptation of Parallel Computation on Chip 
Multiprocessors. In Proceedings of the 12th International Symposium on High-Performance Computer 
Architecture (HPCA-12), 2006. 
[70] Y. Zhang, D. Parikh, K. Sankaranarayanan, K. Skadron, and M. Stan. HotLeakage: A 
Temperature-Aware Model of Subthreshold and Gate Leakage for Architects. Univ. of Virginia Dept. of 
Computer Science Technical Report CS-2003-05,2003. 
[71] D. Madon, E. Sanchez, and S. Monnier. A Study of a Simultaneous Multithreaded Processor 
Implementation. In European Conference on Parallel  Processing, pages 716–726, 1999. 
[72] J. Sharkey. M-Sim: A Flexible, Multithreaded Architectural Simulation Environment. Technical Report 
CS-TR-05-DP01, Department of Computer  Science, State University of New York at Binghamton, 
2005. 
[73] D. M. Tullsen. Simulation and Modeling of a Simultaneous Multithreading Processor. 22nd Annual 
Computer Measurement Group Conference, December 1996. 
[74] M. Moudgill, P. Bose, and J. Moreno. Validation of Turandot, a Fast Processor Model for 
Microarchitecture Exploration. IEEE International Performance, Computing, and Communications 
Conference, pages 451–457, 1999. 
[75] M. Moudgill, J. Wellman, and J. Moreno. Environment for PowerPC  Microarchitecture Exploration. 
IEEE Micro, pages 15–25, 1999. 
[76] B. Lee and D. Brooks. Effects of Pipeline Complexity on SMT/CMP  Power-Performance Efficiency. 
Workshop on Complexity Effective Design, 2005. 
[77] P.S. Magnusson, M. Christensson, J. Eskilson, D. Forsgren, G. Hallberg, J. Hogberg, F. Larsson,A. 
Wisconsin Wind Tunnel II: A fast and portable  parallel architecture simulator. IEEE Concurrency, 8(4), 
pp. 12-20, Oct. 2000. 
[96] S. Reinhardt, M. Hill, J. Larus, A. Lebeck, J. Lewis, and D. Wood. The Wisconsin Wind Tunnel: Virtual 
prototyping of parallel computers. In Proceedings of the 1993 ACM Sigmetrics Conference on 
Measurement and  Modeling of Computer Systems, pp. 48-60, May 1993. 
[97] V. Pai, P. Ranganathan, and S. Adve. RSIM Reference Manual version 1.0, Technical Report 9705, 
Department of Electrical and Computer Engineering,  Rice University, Aug.1997. 
[98] A. George and S. Cook. Distributed simulation of parallel DSP architectures on workstation clusters. 
Simulation, 67(2), pp. 94-105, Aug. 1996. 
[99] B. Falsafi and D. Wood. Modeling cost/performance of a parallel computer simulator. ACM Transactions 
on Modeling and Computer Simulation, 7(1), pp. 104-130, Jan. 1997. 
[100] A. George, R. Fogarty, J. Markwell, and M. Miars. An Integrated Simulation Environment for parallel 
and distributed system prototyping. Simulation, 75(5), pp. 283-294, May 1999. 
  
 
 
  
附錄 
 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：羅習五 計畫編號：99-2220-E-194-009- 
計畫名稱：Criti-core：超越多核心之高可靠度晶片系統平台技術開發--子計畫二：Criti-Core 之高
可靠度多線程及溫度感知作業系統(2/2) 
量化 
成果項目 
實際已達
成數（被接
受或已發
表） 
預期總達成
數(含實際
已達成數)
本計畫
實際貢
獻百分
比 
單位 
備註（質化說明：如數
個計畫共同成果、成
果列為該期刊之封面
故事...等） 
期刊論文 0 0 100%  
研究報告 /技術報
告 4 4 75% 相關的碩士論文四篇 
研討會論文 0 0 100% 
篇 
 
 
論文著作 
專書 0 0 100%   
申請中件數 0 2 50% 
一件專利申請遭駁回，另
一件已通過。未達到目標
十分抱歉。 專利 
已獲得件數 1 0 100% 
件 本計劃延伸自較早的研
究，專利順利的在計劃執
行期間取得。 
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 50% 
期刊論文需要較多的時
間。目前以投稿的論文尚
在回應 reviewer 的意見。
研究報告 /技術報
告 0 0 100%  
研討會論文 2 2 100% 
篇 
Shi-Wu Lo, Wei-shiuan 
Tsai, Jeng-gang Lin, 
Guan-shiung Cheng: 
Swap-before-hibernate: 
a time efficient method 
to suspend an OS to a 
flash drive. SAC 2010: 
201-205 
論文著作 
專書 0 0 100% 章/本  
國外 
專利 申請中件數 0 2 100% 件 
一件專利申請遭駁回，另
一件已通過。未達到目標
十分抱歉。 
