的知識分類去做學習，進而產生能夠自動
對知識產生分類，而這樣的方法將可以更
有效率的節省許多大量的人力資源並可以
在藉由一些技術上的改良，讓這樣形式的
分類更為精準、更有效率。 
Guided Clustering method(導引式概
念分群，簡稱為 GCC)為我們本計劃之第一
年的核心，大體來說，GCC 提出了希望藉由
學習的方式去對其他未分類過的 MetaData
做分類，而分類的方式是利用貼標籤
(Labeling)的方式，更詳細的說法是指學
習兩個分類過的網站模組，進而對一堆雜
亂無章的數位資料進行分類，當中的分類
模組分為兩個形態，一個為路徑(Path)型
態標記，一個則為內文(Content)型態標
記，而在 GCC 當中都會以 Similarity 去作
群 組 分 類 ， 並 採 用 到 TFIDF 去 做
Similarity 的進一步計算。 
  
三、演算方式說明 
   
在此，我們提出了一個概念式階層自
動產生的方法，傳統產生的概念式階層是
利用了階層式分群去分類相似的字詞，但
是往往這樣所產生出來的結果都不足以作
為人為可以辨識分別的資訊，因為資訊不
夠完整，另外，若是完全以人為介入去處
理所有資訊，雖然可以產生相當強健的語
意資料，但是這樣的產生方法是相當的耗
費人力成本且網路資訊變動速度與規模極
大，並非單純依靠人為介入可以維護的。
我們提出的方法它結合了明確的網路資訊
與自動產生概念式分群，進而來產生人為
可以輕易辨識的語意導向的分類架構，這
樣的自動產生處理可以有效的減少人為介
入分類的負擔，畢竟這對人類一個多麼複
雜疲累的負荷，另外我們也提出了一個統
計估計的方法來印證分群結果的精確度。 
自從人類有能力開始辨別產生富有語
意階層的分類之後，就不斷有大量的人力
介入此項研究，起初，這樣一慣的分類方
法是被認為相當有效率且精確的，但是好
景不常，隨著大量高速成長的真實世界資
訊，這樣做下去將會是無止盡，且除了分
類之外還要另外去考量到他們彼此之間的
關連性，這些種種因素都會影響到其分
類，這些都已經不再是人為可以去處理與
維護的能力範圍之內了，同時，既使我們
使用了人為的介入硬是將其分類，但這些
被分類完成的網路資訊的分類品質卻很難
去保證他會具有相同的標準，不同的分類
人員所分出的標準與規範都一定會有相當
程度的差異，這些都是值得去考量的因素。 
然而，有人為介入所建構出的分類結
構是很難去維護或是延伸的，像是 Web 的
目錄、較大結構種類的分類階層，所以我
們所提出的方法主要著重在自動概念式階
層產生處理的分類導向方法，儘可能的去
減輕人類的負擔並同時維持合理且強健的
語意品質。 
在資訊檢索領域中，基於向量空間模型
的 TFIDF 方法被廣泛地用來計算文詞之間
的相似度。若 Document 中包含的所有的詞
为 、 、…、 ,則 Document 中的每
一個詞句都可以用一個 n 维的向量
1w 2w nw
>=< nTTTT ,...,, 21 來 表 示 。 其 中 ，( )niTi ≤≤1 的計算方法為：設 n 為 在這
個問句中出現的個數，m 為 Document 中含
有 的詞句的個數，M為 Document 中問句
的總數，那麼
iw
iw
)/log( mMnTi ×= 。從這個
式子中可以看出，出現次數多的詞將被賦
予較高的 n 值，但這樣的詞並不一定具有
較高的 值。例如，在語句中
“的＂出现的頻率非常高，即 tf 值(n 值)
很大，但由於“的＂在很多文章中都出
現，它對於我们分辨各個文章並没有太大
的幫助，它的 idf 值
)/log( mM
( ))/log( mM 将是一個
很小的數。因此，這種方法综合地考慮了
一個詞的出現頻率和這個詞對不同文章的
分辨能力。 
用同樣的方法，我們可以計算目標詞句
的 n维向量 >′′′=<′ nTTTT ,...,, 21 。得到T 和
T ′後，它們所對應的兩個句子之間的相似
度就可以利用T 和T ′這兩個向量之間夾角
的餘弦值來表示。 
 2
程，每當做完一次 EQ1 與 EQ2 的計算，
就將其結合為 similarity 總值（簡稱
sim），如下 EQ3 所示： 
EQ3：
 
1 1,0 1,0,),(
:belowshown  isfunction  estimation similarity grand The
21212211 =+<≤<≤+= wwwwsimwsimwvusim
 有了這些計算結果，我們就能逐
一的將 sim 值合併，每當合併完一次，
進而又將新的合併群組去做重複的
EQ1、EQ2 與 EQ3 的計算，最後合併出
一個樹狀結構，即符合我們最後的要
求。 
 其中，我們在做樹狀結構合併的
時候，我們利用到了 Hierarchical 
agglomerative clustering（簡稱
HAC）演算法，其處理方式為將計算出
來的最大值兩兩合併，直到最後產生
出一個最大值即完成。 
 4
 整個 GCC 處理的流程我們可以歸
納整合如下圖 Figure2： 
 
Figure 2: Cluster construction process 
 其演算法大致如下： 
procedure; end
;    
end;    
;1         
);,(         
);,(           
 do 1     while
1    
),(    
},{    
GCC procedure
11
1
11
1
k
kkk
kkk
k
i
Canswer
kk
MCsimM
MCclusterC
C
k
CsimM
idC
=
+=
=
=
>
=
=
∀=
++
+
φ
 
Figure 3: Pseudo code for guided 
concept clustering process 
完成了 GCC 部份之後，由於建構
出來的結果使得我們在瀏覽的過程仍
然有可能一個很大的負擔，因為其檢
索出來的瀏覽資料量依然有可能是很
大的，所以我們又將文件中的字詞擷
取出來，結構大致如下 Figure 4： 
 
(a) Original schema-metadata 
relationship 
 (b) After leaf of schema leaf 
schema t
processing 
Figure 4: The relationship between 
ree and metadata element value 
由於 GCC 的前處理都是藉由電腦
自動分類的方式去進行處理，而這樣
的方式往往還是讓使用者無法去分
辨，換句話說就是分類還分的不夠乾
 6
righ
ecord 
identifier source relation ge languacoverage ts … 
edition rating dRfixenotes 
Table
真的
減少
Experiment: 
Average-link 
Structural 
 2: Cluster construction result  
最後，我們可以很明顯看到，的
確做了schema+1的部分
。 
讓搜尋的
負擔大
w
為
1=0.8, w2=0.2 
HAC 
similarity 
newsRecord 
Structural s
artWorks with 
imilarity with 
Our result 
“ schema+1
(level1 and 
88.63% 80.52% 
without 
”  part 
level2) 
Our result with 
“ schema+1
(level1 and 
63.65% 54.48% ”  part 
level2) 
 
Figure 6: Output example of the 
GCC+1 algorithm Table 3: Structural similarity measure for 
concept hierarchy construction result 
 
 
五、結論 
 
在這個方法當中，他不僅僅可以用在網路
瀏覽搜尋的分群，他也可以做為一般數位
典藏的分類，這對數位典藏的部份是一個
大有幫助的方法，因為它可以有效減輕人
類瀏覽的負擔並提高搜尋的效率，但是仍
然有一些地方是值得我們再進一步去做研
究的，在做相似度合併部分的權重值是我
們利用人工的部分去設置的，這部分我們
期待利用 stacked generalization 的 方
法去解決。 
 
五、參考文獻 
 
 
[1] Jian-Hua Yeh, Shun-hong Sie, "Towards 
Automatic Concept Hierarchy Generation for 
 8
行政院國家科學委員會補助專題研究計畫成果報告 
※※※※※※※※※※※※※※※※※※※※※※※※※※
※                        ※ 
※支援時空情境, 無限虛擬導覽與認知架構的數位博物館※ 
※知識平台之研究, 設計與實作 - 支援時空情境的數位博※ 
※         物館知識平台之研究, 設計與實作         ※ 
※                        ※ 
※※※※※※※※※※※※※※※※※※※※※※※※※※ 
 
計畫類別：■個別型計畫  □整合型計畫 
計畫編號：NSC 95-2221-E-156-007 
執行期間：95 年 8 月 1 日至 96 年 7 月 31 日 
 
計畫主持人：葉建華   真理大學資訊科學系 
共同主持人： 
計畫參與人員：劉仲璿   真理大學數理科學研究所 
 
 
 
 
 
 
 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
■出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
執行單位：真理大學資訊科學系 
 
 
中 華 民 國   年   月   日 
 
 
二、與會心得 
 
2006 IEEE/WIC/ACM 國際智慧型 Web 會議於十二月中旬於國際會議中心舉行。本次獲得
國科會出國會議費用補助出國參加此會，與國際上多位學界友人相互切磋與交換心得，獲得許
多實際上的研究經驗，實為難得。尤其與本研究領域相關的學界名人 Frank van Harmelen 也出
席並發表演說(講題: Two Obvious Intuitions: Ontology-Mapping Needs Background Knowledge 
and Approximation)，也顯出本領域研究於本會的重視。 
 
 本次報告題目為，本研究主要由葉建華與謝順宏兩人完成，展示了可以藉由學習其他
認知或分類架構的特性，進而針對為分類的新資料集進行接近人類認知的本體認知架構。本研
究的目的在設計出一套自動化的運作模型，讓各種機構能夠快速藉由系統學習網路上的相關本
體認知架構資訊，並且依照自身的需求與分類，建立適用於自身發展的概念性架構(concept 
hierarchy)。這樣的運作模型，能夠將多個既存的分類架構，如美國政府入口網站(US Government 
Portal), LookSmart, ODP(Open Directory Project)和 Yahoo 等等，以自動化的網頁擷取程式(Web 
spider)，收集各個本體認知架構下相關的網頁以及附加資訊。然後依照網頁所存在的階層架構
以及網頁內容，進行特徵學習。然而，本計劃將採用不同於傳統的學習方式，我們希望可以藉
由使用者表達的預設需求，進行使用者所提供內容的指引式分群。這樣的運作方式，是將大量
已經分類過的資訊，藉由特徵資訊的萃取，作為特定領域的專業註解，使得分類結果更能夠貼
近使用者的期望。至於本篇論文於自動群聚上產生的預測參數問題，在參考文獻中即可發現此
問題之解決方法不下其數。在會議過程中，除了與多位專家學者進行溝通外，私底下更討論此
問題之深入層面進行切磋與交流，並且由討論過程中可發現諸位學者對此問題的興趣。 
 
就研究題材而言，本人參加這次會議，在此提出幾點意見： 
 
(1)  自其他認知架構中快速學習分類知識目前有許多學者視為一個重要的研究主題，應持續重
視。 
(2)  在自動分群技術方面，具語意成分的決策函數值得繼續開發。 
(3)  另一重要的研究趨勢將焦點置於網頁連結的探勘上(Link Mining)，此部份也應持續重視。 
(4)  此次發現到大陸部份學者相較香港本地學者來說，英文能力均非甚佳，有待提升，台灣學
者或多或少也有這樣的問題，需多加強。 
 
本次出國開會攜回資料計有: 
 
(1)  T. Nishida, et. al, Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web 
Intelligence (WI2006), December 18-22, 2006, Hong Kong. 
(2)  T. Nishida, et. al, Proceedings of the 2006 IEEE/WIC/ACM International Conference on 
Intelligent Agent Technology (IAT2006), December 18-22, 2006, Hong Kong. 
 
有興趣的學者可向真理大學知識工程實驗室借閱。 
approaches in taxonomy generation [8, 9, 10] and 
ontology mapping researches [16, 17, 18], but with an 
important distinction: our path similarity takes the path 
depth into account. In our experiences, the consideration 
of path similarity with path depth is critical, which makes 
the generated ontology more semantic-aware. 
 
In this paper, we locate the problem in digital library 
domain, and the data to be processed consists of metadata 
documents and their schema information. Since the data 
domain is different from our previous work, we need to 
define the basic data unit in a different way. According to 
our observations, we can treat the relationship between 
schema and metadata in the following structure, as shown 
in Fig. 1(a): 
 
 
(a) Original schema-metadata relationship 
 
(b) After leaf of schema leaf processing 
Figure 1: The relationship between schema tree and metadata 
element value 
 
The goal this study is to render a single ontology 
for digital libraries, so we decide to merge existing 
schema hierarchy information stored in a digital library. 
Because digital libraries usually contain a huge amount of 
data, it is possible to have a lot of metadata documents in 
a digital library. If we suppose to use external node of a 
schema tree as the leaf nodes of our generated ontology 
(leaf node of an schema tree), the user will suffer from 
browsing activities since there still contains huge 
amounts of data items in each category (ontology leaf). 
So it is necessary to do a further processing step to 
categorize those data items contained under each 
ontology leaf, we call it "leaf of schema leaf processing" 
since the ontology leaf will come from the results of our 
concept clustering algorithm. In this paper, we use kNN 
clustering to act as the further processing step, a sample 
result is shown in Fig. 1(b). 
 
 According to the above observations, our study 
puts the focuses on two parts: 
1. Concept hierarchy merging, this part will 
base on our previous work, the GCC 
concept clustering algorithm, but we have 
to re-define the basic data unit so as to fit 
into the digital library environment. 
2. The processing of "leaf of schema leaf", 
this process includes text processing and 
term grouping. The traditional information 
retrieval (IR) techniques and machine 
learning approaches will be adopted in this 
part. We name this step as "schema+1" 
processing stage, the "+1" means an 
additional level of leaf should be generated 
under this scenario. 
 
The whole process will be able to combine two or 
more existing schema hierarchy and process the 
"schema+1" requirement, we call it the "GCC+1" 
algorithm, as illustrated in Fig. 2. 
 
 
Figure 2: Processing stages of the GCC+1 algorithm 
 
3. The GCC+1 Framework 
 
As illustrated in Fig.3, the framework called "GCC+1" 
is expected to automatically generate ontology from 
metadata and schema information. The basic data unit 
here is quite different from our previous work since what 
we have this time are metadata documents but not web 
documents. The characteristics of web documents are: 
(newsRecord) (newsRecord>Description, 
for example) 
type 
format 
title 
subject 
description 
creator 
contributor 
publisher 
date 
identifier 
source 
relation 
language 
coverage 
rights 
abstract 
conditions 
acquireMethod 
comments 
grade 
prizewinningRecord 
edition 
fixedRecords 
notes  
… 
(a) Partial structure of NewsRecord schema 
Level1 
(artWorks) 
Level2 
(artWorls>Description, 
for example) 
type 
format 
title 
subject 
description 
creator 
contributor 
publisher 
date 
identifier 
source 
relation 
language 
coverage 
rights 
abstract 
conditions 
acquireMethod 
material 
comments 
grade 
seal 
release 
prizewinningRecord 
edition 
rating 
fixedRecord 
notes 
… 
(b) Partial structure of artWorks schema 
Table 1: Example category information from (a) news archive 
and (b) art works in NRCH 
 
Level1 
(GCC-cluster) 
Level2 
(GCC-cluster>Description)
type 
format 
title 
subject 
description 
creator 
contributor 
publisher 
date 
identifier 
source 
relation 
language 
coverage 
rights 
abstract 
conditions 
acquireMethod 
material 
comments 
grade 
seal 
release 
prizewinningRecord 
edition 
rating 
fixedRecord 
notes 
… 
Table 2: Cluster construction result  
 
 
 
 
 
 
Experiment: 
w1=0.8, w2=0.2 
Average-link HAC
Structural 
similarity 
with 
newsRecord 
Structural 
similarity 
with 
artWorks 
Our result without 
“schema+1” part 
(level1 and level2)
88.63% 80.52% 
Our result with 
“schema+1” part 
(level1 and level2)
63.65% 54.48% 
Table 3: Structural similarity measure for concept hierarchy 
construction result 
 
 
Figure 2: Output example of the GCC+1 algorithm 
 
To verify the quality of the result concept hierarchy 
generated by our algorithm, we refer to several tree 
similarity measures [13,14]. The similarity measure is 
shown in Table 3. This result shows the experiment result 
is over 80% similar to a mixture hierarchy of newsRecord 
and artWorks when the "schema+1" part is not taken into 
account. In Table 3, the term "structural similarity" shows 
the portion of result hierarchy concepts shown in the 
union of newsRecord and artWorks hierarchies. 
"Towards (Semi-)automatic Generation of 
Bio-medical Ontologies", Poster Proceedings of the 
AMIA 2003 Annual Symposium, November, 2003, 
Washington, DC. 
[10] Daphne Koller, Mehran Sahami, "Hierarchically 
classifying documents using very few words", 
Proceedings of ICML-97, 14th International 
Conference on Machine Learning, 1997. 
[11] R.E. Valdes-Perez and etc, "Demonstration of 
Hierarchical Document Clustering of Digital 
Library Retrieval Results", Joint Conference on 
Digital Libraries (JDCL '01), Roanoke, VA 
(presented as a demonstration), June 24-28, 2001. 
[12] Y. Yang, J. Zhang and B. Kisiel, "A scalability 
analysis of classifiers in text categorization", ACM 
SIGIR'03, pp 96-103, 2003. 
[13] Bhavsar, V., Boley, H., Yang, L., "A Weighted-Tree 
Similarity Algorithm for Multi-Agent Systems in 
E-Business Environments", Computational 
Intelligence Journal, 20(4), 2004. 
[14] Tom Morrison, "Similarity Measure Building for 
Website Recommendation within an Artificial 
Immune System", Ph.D. Thesis, University of 
Nottingham, 2003. 
[15] National Repository of Cultural Heritage, Council 
of Culture Affairs, Taiwan. http://nrch.cca.gov.tw/ 
[16] AnHai Doan, Jayant Madhavan, Pedro Domingos, 
and Alon Halevy, "Ontology Matching: A Machine 
Learning Approach", In S. Staab and R. Studer, 
editors, Handbook on Ontologies in Information 
Systems. Springer-Velag, 2003. 
[17] Ying Ding, Schubert Foo, "Ontology Research and 
Development (Part 1 - A Review Of Ontology 
Generation) ", Journal of Information Science, 
October 1, 2002; 28(5): 375 - 388. 
[18] Ying Ding, Schubert Foo, "Ontology Research and 
Development (Part 2 - A Review Of Ontology 
Mapping and Evolving) ", Journal of Information 
Science, October 1, 2002; 28(5): 375 - 388. 
[19] Kai Ming Ting, Ian H. Witten, "Issues in Stacked 
Generalization", Journal of Artificial Intelligence 
Research (JAIR), 10:271-289, 1999.
