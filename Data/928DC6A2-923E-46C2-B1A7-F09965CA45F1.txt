2. Web page scraping.  Only few web sites offer 
server-side services.  To achieve Mashup, most cases 
still rely on web page scraping.  That is, the service 
consumer still sends a normal http request to the 
target web site.  After the result web page is 
received, the service consumer tries to parse the 
result, and extract the needed part.  Then, the 
service consumer displays the parsed result with a 
pre-specified format. 
The above mechanisms can by no means be broadly 
adopted.  With regards to the ’service perspective’ 
of web, it is obvious that there are gaps between 
demands and supplies today.  In this research, the 
researcher aim at developing a methodology and a 
framework for building a service-oriented web. 
 
 
2 
 
Mashup, most cases still rely on web page scraping.  That is, the service consumer 
still sends a normal http request to the target web site.  After the result web page is 
received, the service consumer tries to parse the result, and extract the needed part.  
Then, the service consumer displays the parsed result with a pre-specified format. 
The above mechanisms can by no means be broadly adopted.  With regards to the 
“service perspective” of web, it is obvious that there are gaps between demands and 
supplies today.  In this research, the researcher aim at developing a methodology and 
a framework for building a service-oriented web. 
 
Keywords: Mashup Service, Information Extraction
4 
 
extract the needed part.  Then, the service consumer displays the parsed result 
with a pre-specified format. 
The above mechanisms can by no means be broadly adopted.  First of all, most 
existing web sites fail to support the first mechanism.  Implementing server-side 
services require higher technical skills; running and maintaining server-side services 
incur higher costs and overhead.  Even if a web site provides server-side services, in 
most cases these services can not cover all functionalities provided by the web site.  
Moreover, the lack of broad adoption further postpones the overall acceptance of 
standards of server-side services such as SOAP and REST [W3C a], [W3C b].  On 
the other hand, the web page scraping approach is rather unstable.  Despite of the 
research efforts spent on it, existing web page scraping mechanisms are still too 
sensitive to modifications of the target web page.  With high possibilities, web page 
scraping procedures have to be re-written whenever the target web pages are 
modified. 
In this research, the author proposes a virtual web browsing environment that is 
aimed at solving the above issues. The virtual web browsing environment simulates a 
real browser, that is, it has to be capable of parsing web pages into DOM structure and 
interpreting scripts.  Scripts defined in these web pages will then be exposed as web 
services.  In that way, to do Mashups, one simply “invoke” script functions defined 
on the target pages through the virtual web browsing environment. The author 
believes this can be a good advance for Mashup-related technologies. Since the 
research is still ongoing, in this paper, we represent our concept and some partial 
results. 
Methodology 
The solution proposed in this research is to build a server-side virtual Web browsing 
environment. Specifically, the server-side virtual Web browsing environment 
simulates the behavior of a Web browser. The main components of the environment 
are a DOM simulator that parses HTML pages into DOM data structure and a 
JavaScript simulator that executes scripting functions written in JavaScript. Running 
at server side, the environment exposes its functionalities through a Web service 
interface layer. It accepts requests of the form: (WebPageURL, 
JavaScriptFunctionName, Paramaters). WebPageURL represents the URL address of 
the Web page hosting the JavaScript function to be invoked while 
JavaScriptFunctionName is the name of the target JavaScript function and Parameters 
is the list of related parameters. Through the Web service layer, the environment 
indirectly transforms JavaScript functions into invokable Web services and thus fixes 
6 
 
Result 
To validate our design, a demo application has been created. In this section, the demo 
application will be presented. 
System Requirement 
In order to execute the demo application, please make sure that the running 
environment meets the following system requirements: 
1. Java Runtime Environment with version 6.0 or higher 
2. RAM: 128mb 
3. Hard Disk Space: 98mb 
Start Application 
The demo application is distributed as a zip file. After unzipping the file, there should 
be a directory named as “bin”. The directory contains a file named as “run.bat” and a 
jar file named as “VWBECore.jar”. For windows-based operating systems, users can 
either double click the run.bat file or the VWBECore.jar file. On other platforms, 
users have to execute the VWBECore.jar file. Note that if your platform supports the 
execution of executable jar files, double clicking on the VWBECore.jar file or issuing 
the “java –jar VWBECore.jar” command through the command line interface should 
always work. The following figure shows the appearance of the demo application. 
 
8 
 
On the left hand side, there is a drop down list, the function list, showing 
JavaScript functions declared in the loaded xhtml file and a text area displaying the 
content of the selected JavaScript function. On the right hand side, there is a table, the 
parameter table, allowing users to input the values of parameters for the invoking 
JavaScript function. 
Execute 
To execute a JavaScript function, at first, one has to select the target function from the 
function list. Then, the parameter table will display the corresponding parameter list. 
Users have to specify values and data types for these parameters. Finally, click the 
“Execute” button to execute the function. The “Result” tab will display the result of 
the execution. 
Discussion 
In this project, we have built a virtual web browsing environment to perform Mashups. 
Existing Mashup solutions typically rely on server-side service invocation or web 
page scraping. These approaches cannot be widely adopted since there are only few 
web sites providing server-side services and web page scraping is rather unstable. 
With our solution, client programs can invoke JavaScript functions defined in other 
HTML pages and receive the results. To define JavaScript functions has much lower 
entrance barrier than to define server-side services, thus, the researcher believes that 
our approach is more adoptable. 
Publications 
The researcher’s publications related with the project are listed below: 
1. Chun-Hsiung Tseng: Techniques for Building a Read/Write/Execute-able Web. 
Applied Mechanics and Materials (EI) 
2. Chun-Hsiung Tseng: Virtual Browsing Environment for Mashups. In 
Proceedings of the 2011 International Conference on Advanced Information 
Technologies, 2011, page 154 
Future Work 
In the future, the research team plans to improve the system in the following ways: 
1. event handling: the current version does not support DOM events, however, the 
lack of event supports may incur inconvenience, so the research team plans to 
 1
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：   年   月   日 
一、參加會議經過 
這是一個四天的會議，會議地點位於舊金山的 google 總部。會議的第一天是以 
workshop 的形式進行，分為上午、下午兩個階段。上午階段有三個 workshop： 
1. Multimedia Document Processing in an HTML5 World 
2. Google Mystery Workshop 
3. Documenting Social Networks 
由於是同一個時間進行，本人只挑選了較感興趣的 Multimedia Document Processing 
in an HTML5 World 這個 workshop 參加。該 workshop 的主持人為 Rodrigo Laiola
計畫編號 NSC 99－2218－E －262 －001 － 
計畫名稱 網頁服務網之建構 
出國人員
姓名 
曾俊雄 
服務機構
及職稱 
龍華科技大學資訊網路工程系助
理教授 
會議時間 
100 年 9 月 19 日
至 100 年 9 月 22
日 
會議地點 
Mountain View, California 
會議名稱 
(中文) 
(英文) the 11th ACM Symposium on Document Engineering 
發表論文
題目 
(中文) 
(英文) Developer-Friendly Annotation-Based HTML-to-XML 
Transformation Technology 
 3
DocEng 研討會在國內並非十分知名的研討會，然而，在文件處理的領域，該研討會
有極高的重要性，且在 ACM 的 SIGWEB社群中，也相當受到重視，去年本人也參加了
這個研討會，連續兩次，SIGWEB 的主席皆親自參與，可見其重要性一斑。由於本人
的研究領域主要在 Web Information Extraction，其中除了大量資訊的處理，亦牽
涉到 XML 的運用，因此參與 DocEng 研討會對本人的學術發展有極大的助益及重要
性。特別是本次研討會在 Google 總部舉行，外界對於 Google 的運作總是抱持好奇
的目光，能有機會受邀在其總部參與研討會，可謂幸何如之！在 Google 總部的參觀，
本人最大的感想是，Google 這樣一個資訊界的龍頭公司，具有五多的特色：標語多、
電腦多、書櫃多、會議室多、休閒設施多。其中，特別是電腦多、書櫃多這兩項，
最讓本人感慨萬千。電腦多，讓員工有非常充足的設備，然後再來要求員工的工作
效能，這是 Google能取得如此成績的原因之一。反觀國內，我們口口聲聲要發展資
訊科技，但重要的設備確實付之闕如，各級學校經常受限於設備不足，而只能使用
精簡的，甚至報廢的設備，如此要期望能取得超越他人的成果，是非常困難的。書
櫃多，亦是十分讓人驚訝，在這個網路至上的時代，有很多人已經不看實體書，但
在 Google，不但到處都有書櫃，裡面有滿滿的書，他們也鼓勵員工看書，員工希望
閱讀的書籍，完全不需要負擔成本，Google 直接在辦公室建立印刷設備，直接將員
工想看的書籍印刷出來，我相信以 Google 這個等級的公司，不會去冒犯智慧財產權
的大不諱，背後的成本勢必是 Google直接吸收掉了。而休閒設施這些更毋庸置疑是
在爭取員工向心力的手段，事實上，我看到很多 Google 的員工一大早就到辦公室，
到晚上 10 點還沒有下班，但這些員工卻是心情愉快且充滿自信。這一點其實值得國
內企業深思，有時候花一些成本，對員工好一些，可以得到的生產力的回饋，是難
 5
另外一個個人感興趣的主題是 XML Versioning，一部分的原因是該 session 的其中
一位發表者是我的舊識。Cheng Thao 是一位不會說中文的華裔人士，亦是去年最佳
論文獎的得主。這位先生是 SIGWEB 中重要人物 Ethan Munson 的學生，或許因為同
樣是東方面孔，Cheng 與我特別談得來。他的 work 是在沒有 central repository 的
情況下，對 XML 文件做版本追踪。這是一個非常困難的工作，因為沒有 central 
repository就沒有可靠的 timestamp，他的 work 是以計算 delta 的方式來取代，目
前可以做到在不影響原始 XML 文件呈現的情況下，加入一些 delta 資訊來控制版本，
並且進行 merge、diff等動作。事實上，Cheng去年獲獎的 3-way Diff 機制論文也
在探討相關的問題，這會讓我想到適當的集中火力，對於產出有價值的成果的重要
性。反思自身，個人以為國內對學界的評鑑制度，造成許多學者不得不跑短線，快
速的發表許多不夠深入的 work，長期以來，雖然 SCI 件數多了，卻沒有真正有影響
力的成果，這並非學界之福。 
除了上述兩個主題，本人對於 Metadata 的處理也很感興趣，因為 Information 
Extraction 其實可以依賴 Metadata 的幫助。在這次的研討會中，Interoperable 
Metadata Semantics with Meta-Metadata正好是相關的題目。儘管我們現在有多種
標準的 Metadata語言，比如 Dublin Core等等，礙於被採用率都不是非常高，因此
相近的 Metadata語言之間實際上沒有辦法相容。這個 work 的做法是在 Metadata 之
上再建立一層 Meta-Metadata，利用 Meta-Metadata來分析 Metadata 的關聯性，再
利用 Metadata來處理實際的 data。不過，個人以為，如果能推動 Metadata 的標準
化，比如採用應用更廣泛的 XML Schema來定義 Metadata，自然可減少相容性的問題，
這樣好像比再多建一層 Metadata 的收效更為明確。 
notification.txt
Thank you for considering DocEng 2011 for presenting your research. We are 
pleased to inform you that your paper entitled Developer-Friendly 
Annotation-Based HTML-to-XML Transformation Technology has been accepted as a 
short paper. As a result, a published version with up to 4 pages will be 
included in the proceedings and you will be required to present your paper in 
the form of a poster and/or demo at the conference in September.
In preparing your final version for the proceedings, please take into account 
all the referees' comments, so that your published paper is as good as it can 
be. Detailed formatting and submission instructions will be sent in a few days. 
Your final paper must be received by Friday, June 17.
Please note that in order for your paper to be included in the conference 
proceedings, at least one of the authors must present the paper in person. This,
of course, requires at least one author to register for DocEng 2011. You will be
informed when registration information becomes available and registration is 
open (within the next few weeks).
On behalf of the Program Committee, Organizing Committee, and Steering 
Committee, congratulations. We all look forward to seeing you in Mountain View 
in September.
Frank Tompa (PC Chair)
----------------------- REVIEW 1 ---------------------
PAPER: 4
TITLE: Developer-Friendly Annotation-Based HTML-to-XML Transformation Technology
AUTHORS: Chun-Hsiung Tseng
OVERALL RATING: -1 (weak reject)
REVIEWER'S CONFIDENCE: 1 (low)
Relevance: Will DocEng attendees find the paper interesting?: 2 (A few: mostly 
those in this specialty)
Quality of Technical Contribution (correctness, depth, novelty): 3 (fair)
Quality of Presentation (writing, organization, figures, etc.): 3 (fair)
Improvability: Are deficiencies correctable via revision?: 3 (Likely: some 
rewriting required)
Recommended mode of presentation: 5 (Full paper: Presentation)
Contribution of Paper (significance of problem, potential impact of
solution):
The author presents an annotation system which enables the proposed mechanism to
port contents stored within HTML files into an XML structure. Therefore, the 
HTML needs first to be transformed to XHMTL. After that the proposed mechanism 
crawls through the DOM and searches for previously added annotations. These 
annotations must be inserted manually. The mechanism described in this paper 
solves the "problem" of disordered items by using a
multi-layered and multi-passed strategy.
I wonder about the lack of other annotation systems described within related 
work.
第 1 頁
notification.txt
Introduction: please use e.g. instead of e.x.
             a missing ".": "Mashup methods may become unstable"
P2:
Enumeration 2: kick the "s": "to annotate every node."
2.2 Web...    please use e.g. instead of "for example"
             and remove ",": "e.g., the table part of a web page."
             tense?: "[22] is an unsupervised web information extraction
system."
P3:
first enum.:  both "that" can be deleted.
             "...to be able to applied to all element nodes, attributes,
and text nodes."
3th enum. 4.: kick the "of": "(inclusive attribute nodes)"
         6.: kick the "s":  "this is useful when then container element
has more than one text child node"
             kick the remaining sentence, because no one knows "content
models" so far
                  and btw. a list of text child nodes is no mixed content(?)
3.2 Annotations:
         "Hence, the returned target node will be a transformed tree node."
P4:
         "The "as_root" annotations must be declared on HTML nodes that
should be..."
         "A target node can only have one "as_root" annotation
associated..."
         "Two types of positioning rules, the explicit and the implicit
ones."
         "(listed from highest to lowest priority):"
3.3 Node Pos...
2nd enum. 2.: here something is wrong within the second sentence.
3th enum. 1.: "...the context of a transforming tree node is selected..."
P5:
         6.: here something is wrong within the last sentence.
         I can understand a multi-passes / -passed strategy,
         but a multi-passes architecture?
         What is the difference between a desktop-based and HTML-based
application?
         Both are visible within the desktop? -> stand-alone application(?)
P6:
         Please do not waste space by format source code this way.
P7:
         "...will automatically convert the input document to ensure
第 3 頁
notification.txt
Presentation of Ideas (clarity of writing, appropriate use of figures and 
tables, inclusion of informative background and references):
The paper presents the proposed approach in a precise and methodical manner.
General Comments (including overall advice to improve the paper):
A relatively simple example of the approach, and application of the technique, 
is provided. However, the reviewer would have liked to have seen how the 
approach scales to larger data sets and performance implications.
Specific Comments (including specific corrections to make):
----------------------- REVIEW 3 ---------------------
PAPER: 4
TITLE: Developer-Friendly Annotation-Based HTML-to-XML Transformation Technology
AUTHORS: Chun-Hsiung Tseng
OVERALL RATING: -1 (weak reject)
REVIEWER'S CONFIDENCE: 2 (medium)
Relevance: Will DocEng attendees find the paper interesting?: 3 (Some: 30-40% 
will be interested)
Quality of Technical Contribution (correctness, depth, novelty): 2 (poor)
Quality of Presentation (writing, organization, figures, etc.): 2 (poor)
Improvability: Are deficiencies correctable via revision?: 2 (Perhaps: 
significant rewriting required)
Recommended mode of presentation: 2 (Short paper: Poster)
A demonstration paper has also been submitted ("Demonstration of H2X: ...")
* Contribution of Paper (significance of problem, potential impact of solution):
The author aims at tackling the problem of extracting information from HTML 
pages, and proposes and annotation-based approach to HTML-to-XML transformation.
This is an important problem, with many recent results reported not only at 
DocEng but also at other conferences (e.g. WWW, KDD,  JCDL) and journals (e.g. 
IEEE TKDE and TSMC). Referring to his previous experience in providing a 
supporting to create bindings between GUI components and XML Schema elements 
(published at DocEng´2010), the author reports that it demanded developers to 
learn additional tools. 
The proposal: starting from an XHTML-compliant document, the approach supports 
the production of an annotation-augmented HTML document (containing annotations 
associated with nodes). These annotations are then used to generate an 
associated XML document.
The authors define a set of rules that gear the process of generating the XML 
document from the annotated HTML document. The rules are based on the DOM tree, 
and the transformation mechanism defines extended attributes which allow 
associating the annotations without interfering with the original HTML 
attributes. 
第 5 頁
notification.txt
http://www.eset.com
第 7 頁
2. RELATED WORK 
Rather than simple web surfing, web users nowadays request for 
an integrated view of information on the web. Roughly speaking, 
web information extraction systems can be divided into four 
different types: manually constructed systems, supervised systems, 
semi-supervised systems, and unsupervised systems [2]. Manually 
constructed systems such as WebOQL [1] require users to use 
provided programming or query languages for extraction and thus 
become extremely expensive.  Supervised systems such as 
DEByE [5] require users to provide a set of pre-labeled web pages 
for training and then will automatically infer rules for extracting 
data from similar web pages.  Semi-supervised systems such as 
OLERA [3] require no pre-labeled training pages, but post-efforts 
from the user is required to choose the target pattern and indicate 
the data to be extracted.  Unsupervised systems, on the other hand, 
require no user interactions.  However, they may only be 
applicable for data-rich regions, e.g. the table part, of a web page.  
The work proposed by Wang and Lochovsky [10] is an 
unsupervised web information extraction system. 
Generating XML documents from information sources is not an 
uncommon request for web information extraction. There are 
several technologies and research works targeting at providing 
such functionality. For example, XSLT [9] is a popular 
technology aiming at transforming an existing XML document 
(probably an XHTML document) into a new one. Besides 
programming-based transformation, some technologies may rely 
on pre-specified mappings to generate XML documents from user 
inputs on GUI components. Examples of applications/software 
libraries adopting such technology are XForms [8], FormsXML 
[4], InfoPath [7], and XUIB [6]. 
Compared with existing works, the proposed mechanism appears 
to be both feature-rich and user-friendly. XSLT-based approaches 
require programming skills. XForms-based technologies need 
special browser support and thus have not been widely adopted. 
Most mapping-based technologies lack of flexibility due to the 
constraints caused by XML schemas. XUIB is flexible, but 
probably too complex for ordinary developers. On the other hand, 
the proposed mechanism requires developers to add only needed 
annotation and can determine some transformation rules 
automatically. 
3. Annotation-Based HTML-to-XML 
Transformation 
The proposed mechanism is aimed at providing a transformation 
framework for generating XML documents from annotation-
augmented HTML documents. Here, it is assumed that the HTML 
documents to be processed are always XHTML-compliant. If not, 
a tidy process will be invoked in advance to ensure XHTML-
compliance. An annotation-augmented HTML document is an 
HTML document with HTML nodes augmented by annotations. 
The augmenting information is then used as rules for generating 
the target XML document. Two types of rules are needed during 
the generation process: 
1. node construction rules that are used for providing the 
specification of the target XML nodes 
2. positioning rules that are used for determining the final 
positions of generated XML nodes  
That is, for all nodes (including element nodes, attribute nodes, 
and text nodes) within the HTML document to be processed, a 
(node, node construction rule, positioning rule) tuple is 
maintained. The transformation process is listed below: 
1. walk through the HTML DOM tree and construct a meta 
tree, the transforming tree, which reflects the original tree 
structure; nodes on the transforming tree, the transforming 
tree nodes, maintain the original HTML DOM nodes and 
associated annotation information 
2. for each HTML node, create the corresponding XML node 
according to its associated construction rules and store the 
generated XML node into the meta tree 
3. traverse the meta tree and position XML node contained in 
each tree node according to its explicit or implicit (will be 
explained later) positioning rule 
The proposed mechanism defines the following extended 
attributes for specifying the augmenting information: 
1. as_root: an element node annotated with this attribute is 
regarded to as a root node in the resulting XML document; 
if there are more than one element HTML nodes annotated 
with this attribute, more than one XML documents will be 
generated from this HTML document 
2. root: specify the root XML element of the resulting XML 
node generated from the annotated target 
3. def: specify how to generate a resulting XML node from 
the annotated target 
4. relation: define the relationship among the resulting XML 
node and other XML nodes; allowable relationships are 
child and next; these relationships are used for positioning 
resulting XML nodes 
Most extended attributes expect values in the form: (target_exp, 
parameter). target_exp is used for locating the target nodes to 
apply this annotation. The design of target_exp allows developers 
to annotate HTML nodes more freely. For example, 
<input type=”text” value=”book1” h2x:def=”@{}value, 
@{}bookName”></input> 
indicates that the attribute node “value” is associated with (and 
will be transformed to) the “bookName” attribute on the target 
XML page. Multiple rules separated by “;” can be defined within 
one annotation if they have different target_exp. For instance, 
h2x:def=”@{}value, @{}bookName; ., {}bookMetaData” defines 
an additional rule that will map the annotated HTML node itself 
to the “bookMetaData” element on the target XML page. 
Furthermore, two types of positioning rules, the explicit and the 
implicit ones, are used for determining the final position of a 
generated XML node. Explicit positioning rules are specified with 
the relation annotation and have the highest priority. Implicit 
rules are adopted when no explicit rule is specified. Two types of 
implicit rules will be adopted when no explicit rule declared 
(listed from highest to lowest priority): 
1. schema position: to position a transforming tree node 
according to the restrictions imposed by the associated 
XML schema 
2. natural position: to position a transforming tree node 
according to the relative position of nodes with def 
annotations 
XML node generated by this XHTML node is the next sibling 
node of the XML node generated by the XHTML node with id 
title1. Furthermore, XHTML nodes not associated with the 
relation annotation will be positioned according to implicit 
positioning rules, so there is no need to specify positioning rules 
for all XHTML nodes. For example, XHTML node with id price1 
and title1 will be automatically positioned according to the 
associated XML schema. 
Below is the XML document generated from the sample XHTML 
document: 
<books xmlns="test1"> 
 <book> 
  <title>Book1</title>  
 <author>Author1</author> 
  <price>100</price> 
 </book> 
</books> 
Now, consider another web page with a different layout:  
<html xmlns="http://www.w3.org/1999/xhtml" 
xmlns:h2x="h2x"> 
    <head> 
        <title>TODO supply a title</title> 
    </head> 
    <body h2x:def="{test1}books" h2x:as_root="books.xsd"> 
        Books:<br/> 
        <ol> 
            <li h2x:def="{test1}book" id="book1"> 
                <ul> 
                    <a h2x:def="{test1}title" id="title1"></a><li 
h2x:relation="id(title1),text(),child" 
h2x:def="text(),text()">Book1</li> 
                    <a h2x:def="{test1}price" id="price1"></a><li 
h2x:relation="id(price1),text(),child" 
h2x:def="text(),text()">100</li> 
                    <a h2x:def="{test1}author" id="author1"></a><li 
h2x:relation="id(author1),text(),child" 
h2x:def="text(),text()">author1</li> 
                </ul> 
            </li> 
        </ol> 
    </body> 
</html> 
Note that in the above web page, dummy <a> tags are introduced 
as the container node holding the information for the generation 
of some XML elements. The resulting XML document is 
compatible with the previous one since they adhere to the same 
XML schema. Hence, software agents can understand information 
extracted from both web sites. As a result, instead of performing 
web page scraping, the agent service mentioned above can simply 
extract information from the resulting XML documents. This will 
be simpler and more stable. 
5. CONCLUSION AND FUTURE WORK 
In this paper, an annotation-based technology for HTML to XHTML 
transformation is proposed. Utilizing the provided set of annotations, 
web site developers can specify transformation rules directly on 
HTML documents. These annotations can configure not only the 
generation of XML nodes but also the positioning rules of the 
generated XML nodes. Despite of the richness in functionalities, the 
proposed mechanism is carefully designed to reduce the usage 
complexity. By employing schema information associated with the 
HTML document, the mechanism is capable of determining the 
position of most generated XML nodes. Hence, web site developers 
do not have to specify positioning rules for all generated XML nodes. 
Furthermore, additional tool support is not needed for adopting the 
proposed technology. Developers can utilize the proposed technology 
with their acquainted HTML tools. 
In the near future, it is scheduled to extend the current work to 
provide scripting support. By allowing scripting in the value part of 
annotations, flexibility can be further enhanced; take the “id(#id)” 
style target expression as an example, with scripting support, the 
“#id” can be determined by evaluating an scripting function. 
Moreover, it is planned to implement the web database concept based 
on H2X. With H2X, it will be easier to extract information from the 
web; as a result, it appears reasonable to regard the web as a huge 
database of machine-understandable information; by extending 
current design with ontology concept and query functionalities, it will 
be possible to perform more advanced queries than current search 
engines can support. For example, type information can be included 
in a query thus the result will be more accurate. 
6. REFERENCES 
[1] Arocena, G.O. and Mendelzon, A.O. 1998. WebOQL: 
Restructuring documents, databases, and webs. In Proceedings of 
the 14th IEEE international conference on Data engineering, 1998, 
24-33 
[2] Chang, C.H. and Girgis, M. 2006. A survey of web information 
extraction systems. IEEE Transactions on Knowledge and Data 
Engineering 18, 1411-1428 
[3] Chang, C.H. and Kuo, S.C. 2004. OLERA: A semisupervised 
approach for web data extraction with visual support. IEEE 
Intelligent Systems 19, 56-64 
[4] Kuo, Y.S., Shih, N.C., Tseng, L., and Hu, H.C.: Generating form-
based user interfaces for xml vocabularies. In Proceedings of the 
2005 ACM symposium on Document engineering, pp. 58–60. 
ACM, New York, NY, USA, 2005 
[5] Laender, A.H.F., Ribeiro, B., and Silva, A.S. 2002. DEByE---
Data extraction by example. Data and Knowledge  40, 121-154. 
[6] Lendle Tseng, Y.S. Kuo, Hsiu-Hui Lee, and Chuen-Liang Chen: 
XUIB: XML to User Interface Binding. In Proceedings of the 
2010 ACM Symposium on Document Engineering, 2010, page 
51-60 
[7] Microsoft: Infopath. http://office.microsoft.com/en-us/default.aspx 
[8] W3C: XForms. http://www.w3.org/MarkUp/Forms/ 
[9] W3C: XSLT. http://www.w3.org/TR/xslt 
[10] Wang, J. and Lochovsky, F.H. 2003. Data extraction and label 
assignment for web databases. In Proceedings of the 12th 
international workshop on World wide web, 2003, 187-196
99 年度專題研究計畫研究成果彙整表 
計畫主持人：曾俊雄 計畫編號：99-2218-E-262-001- 
計畫名稱：網頁服務網之建構 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 1 1 50%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 0 50%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 50% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
