 II 
一、 中文摘要  
關鍵詞：瞬間最大功率消耗，螞蟻演算法，相依狀態變數之識別，可達性分析，熱能導向之功率
分析，瞬間最高溫度，單電子電晶體 
 
隨著電晶體的 feature size 不斷縮小，相對的晶片內可以容納的電晶體數量也就增大，此晶
片内電晶體密度之上升將使得晶片內部功率的消耗提高，及連帶的產生晶片過熱的問題，過熱的晶
片也會使得晶片的失敗率提高。一個 chip從完成設計，到製作出實體，這個過程需要耗費很長的
時間，倘若我們在 chip已完成實體後，才發現晶片因為過熱的問題導致無法正常使用，這樣將會
造成成本浪費，因此我們希望在電路設計出來時，能夠先預估該 design的瞬間最大功率消耗，以
免造成不必要的時間及金錢的浪費。 
 
 本計劃第一年針對組合電路研究瞬間最大功率消耗的問題，我們曉得一個電路的功率消耗和電
路中的每個 gate的 transition 狀況，以及所連接的 fanout capacitance有很大的關係，如何找
到一個 pattern pair，可以使電路中之 transition數目最大，是決定電路所消耗的 peak power
的關鍵。因此我們將利用螞蟻演算法來當作我們的搜尋引擎，試圖找出兩個 input vectors，使其
有最大數目的 transition。 
 
 本計劃第二年針對序向電路研究瞬間最大功率消耗的問題，循序電路相較於組合電路的不同在
於，我們必頇去確認造成瞬間最大功率消耗的狀態是否為可到達的狀態，如果此狀態為非可到達的
話，那麼由此狀態所造成的最大功率消耗實際上是不會發生的。所以我們會對此循序電路，先作相
依狀態變數之識別，以減小 state space，接著再對其做可達性分析，也就是去確認從初始狀態是
否存在一條路徑到達此狀態。若是可到達的狀態，則此對應的 vector pair即是有效的。 
 
另外，晶片上的 thermal 問題隨著晶片密度增加也日趨嚴重，當功率消耗所產生的熱無法快速
移除時，將使得晶片上的熱累積進而造成溫度上升，連帶影響晶片的效能。最大瞬間功率的發生不
一定會造成晶片上有最高的溫度，這是因為溫度是有地區性的，即一個區域有最多功率的消耗才會
造成高溫，而不是晶片上有瞬間最大功率即會造成高溫，換言之，若晶片上瞬間最大功率發生了，
但是其均勻分散在不同的區域內，那麼此瞬間最大功率並不會造成最高溫度。因此我們想找出會造
成晶片最高溫度的 vector pair，以提供分析。所以本計劃原來規劃第三年研究有考慮 thermal效
應的功率分析，也就是晶片上最高溫度的預測，但是已經提前於第二年進行完畢。遂於本計劃第三
年進行更先進的低功率及低耗能的研究，我們進行以單電子電晶體為奈米電路元件的超低耗能電腦
輔助設計的研究。 
 
 
 
 IV 
目錄 
中文摘要 II 
英文摘要 III 
目錄 IV 
內容 V 
參考文獻 XII 
計畫成果自評 XIV 
附錄   
DAC 2011 .................................................................................................................  
SOCC 2011 ...............................................................................................................  
ICCAD 2011 (TO APPEAR) ........................................................................................  
IEEE TRANSACTIONS ON CAD-1 ...............................................................................  
IEEE TRANSACTION ON CAD-2 (TO APPEAR)………………………………………. 
 
 
 
 
 
 
 VI 
耗，相較於組合電路的不同在於，我們必頇要去確認造成瞬間最大功率消耗的狀態是否為可到
達的狀態，也就是序向電路從初始狀態出發是否存在一條路徑可以到達這個狀態。如果此狀態
為非可到達的話，那麼由此狀態所造成的瞬間最大功率消耗，實際上是不會發生的。然而，大
量的 state variables 會導致龐大的 state space，這會使得確認狀態的可達性之難度增加許多。因
此為了簡化此序向電路的 state space，我們可以尋找 state variables 之間可能存在的 functional 
dependency，來簡化此序向電路的 state space。利用此關係我們可以找到某些 dependent state 
variables，其可由其他 state variables 所取代的。換言之，這些 dependent state variables 是可以
被忽略的，並且利用其他 state variables 取代其 functionality，以簡化原本的電路。然而傳統的
dependent latch identification，其使用 Binary Decision Diagram (BDD)及 sequential automatic test 
pattern generation (ATPG)，只可以找出 typical functional dependency，例如，equivalence relation 
(Si≡Sj) and opposition relation (Si≡￢Sj)。但是，仍有其它 general functional dependencies 存在，
且可以用來簡化電路，例如，AND relation (Si=Sj•Sk)。 
 
        因此，我們的 dependent latch identification 欲找出 state variables 的 typical 和 general 的
functional dependencies。除此之外，我們會利用 Satisfiability (SAT) solver 來找 dependent state 
variables，因此可以處理傳統方法所不能處理的較大序向電路。 
 
     在做完 dependent latch identification 之後，我們接著對簡化過的電路做可達性分析。我們
的可達性分析和傳統的可達性分析有所不同，傳統的可達性分析，在每個 timeframe 都把所有
可能出現在這個 timeframe 的狀態算出來，如此將會花費很多時間在重複計算同一個狀態集
合，然而在這裡我們的目的只是要去確認某個狀態是否為可到達的，不必要也不需要在同一個
timeframe 花太多時間計算。傳統方法對於很多大的電路而言，其完整的可到達狀態集合在短
時間內(< 5000秒)是無法計算出來的。 
 
     因此，我們做的可達性分析並不是為了求出完整的可到達狀態集合，而是在有限的時間內
確認某個狀態是否為可到達的狀態，為此，我們將盡量提高在單位時間內所計算的可以到達之
狀態數，以期在有限的時間內鑑別這個狀態是否為可到達的。所以我們不會依照每個 timeframe
去計算可到達狀態，相反的，我們會盡量挑出與 next state set 不同的 reached states，將其放到
previous state 裡去計算出下一個 timeframe 會到達的 states，以避免對同一個 state 做重複的計
算，還有我們也提出了一個機制來避免對 reached state 做多餘地計算。 
 
有了上述可以判斷一個 state 是否為 reachable 之方法後，我們就可以知道 ACO 所找出來
之兩個 input vectors 是否為真正會造成最大瞬間功率的 vector pairs。 
 
另外，晶片上的 thermal 問題隨著晶片密度增加也日趨嚴重，當功率消耗所產生的熱無法
快速移除時，將使得晶片上的熱累積進而造成溫度上升，連帶影響晶片的效能。最大瞬間功率
的發生不一定會造成晶片上有最高的溫度，這是因為溫度是有地區性的，即一個區域有最多功
率的消耗才會造成高溫，而不是晶片上有瞬間最大功率即會造成高溫，換言之，若晶片上瞬間
 VIII 
以 Fig. 2(a)中的 ROBDD 為例，利用 CUDD package 所提供的函式[19]，我們可以計算出
連接到終點 1 的路徑有 abcd={11--, 101-, 00-0, 010-, 0110 }，這些即為此電路的 product terms。
接著，我們開始對應(mapping)每個 product term 至 SET 架構中。給定一個 product term p，從
架構中的頂點開始，我們依序尋找或設定一個邊給 p 裡面由左到右的每個 bit。設定邊的規則
如下：當考慮的 bit 值為 1 (or 0)，我們先試著找一個 active high (or low)的邊；如果找不到的
話，我們可設定一個邊為 active high (or low)。然而，假使此 bit 值為“-”(don’t care)，我們
則先試著找一 short 邊，不行的話再設定一個邊為 short。當所有的 product terms 都對應完後，
未設定的邊則設定為 open。我們使用 Fig. 3 的例子來說明對應的方法。在這裡，由於 SET 架
構中的垂直邊皆為 short，為簡單表示此 SET 架構，我們省略這些垂直邊而只顯示可設定的斜
邊。假設有四個 product terms：p0=0110、p1=010-、p2=11--和 p3=101-，如 Fig. 3(a)所示。首
先考慮 p0=0110，從頂點 n(0, 0)(此為座標，x 軸以 0 為中心，可往右(正)，往左(負)，y 軸 0
在最上方)開始，對於第一個 bit 0，我們設定 n(0, 0)的左邊為 low。接下來，對於第二個 bit 1，
我們設定 n(-1, 1)的右邊為 high。使用相同的方法，我們接著分別設定 n(0, 2)的左邊和 n(-1, 3)
的右邊為 high 和 low，給最後兩個 bits 10。對應後的結果如 Fig. 3(b)所示。在這裡，我們決
定要設定一個節點的左邊或右邊乃是取決於此節點的 x 軸座標位置。當 x 小於 0 時，我們先
考慮右邊，不成功的話再考慮左邊。反之，則先考慮左邊。此法可以平衡左右兩邊的電路。 
 
 
Fig. 2. An example of ROBDD by node duplication. (a) The original ROBDD. (b) The 
resultant BDD. 
     
接下來，我們考慮 p1=010-。一樣從頂點 n(0, 0)開始，因為前兩個 bits 01 和 p0 的前兩個
bits 相同，我們部份地重複使用先前的對應結果。所以我們只頇設定 n(0, 2)的右邊為 low 和
n(1, 3)的左邊為 short，給最後的兩個 bits 0-。對應的結果如 Fig. 3(c)所示。對於 p2=11--，當
我們設定完 n(0, 0)的右邊為 high 給第一個 bit 後，我們首先試著試定 n(1, 1)的左邊為 high 給
第二個 bit 1。然而，因為 n(0, 2)的左邊和右邊分別已經被設定為 high 和 low，其皆與第三個
bit“-”不一致，所以我們取消先前對 n(1, 1)左邊的設定，改設定 n(1, 1)的右邊為 high。然後，
n(2, 2)和 n(1, 3)的左邊都設定為 short，給最後的兩個 bits --。對應的結果如 Fig. 3(d)所示。最
後，我們考慮 p3=101-。在找到 n(0, 0)的右邊為 high 給第一個 bit 1 後，我們並不設定 n(1, 1)
的左邊為 low 給第二個 bit 0，因為如此將產生一路徑：n(0, 0)->n(1, 1)->n(0, 2)->n(1, 3)->n(0, 
4)，此路徑將對應到一個不合法的 product term 100-。此外，因為 n(1, 1)的右邊已設定為 high，
我們選擇擴展(expand)此架構並設定 n(2, 0)的左右兩邊為 short，接著從 n(3, 1)開始考慮剩下的
三個 bits，而結果如 Fig. 3(e)所示。最後，我們把未設定的邊設定為 open，對應結果如 Fig. 3(f)
所示。 
 X 
此外，因為一個頂點只有左右兩個邊，為了要能成功地對應所有的 product terms，當三
種 bit 值:0、1 和“-”同時出現在第一個 bit 時，我們會複製以“-”為第一個 bit 的 product terms
並分別改指定為 0 和 1。也就是確保最多只有兩種不同的值會出現在第一個 bit。而且，一旦
有兩種不同的值出現在第一個 bit，我們會在開始對應程序前先設定 n(0, 0)的左右邊不相同，
以確保能成功地對應所有的 product terms。 
 
 
Fig. 5. The algorithm of product term mapping. 
 
Fig. 5 為所提出的對應演算法。在主函式 Mapping()中，我們首先依據所有 product terms
的第一個 bit 值設定 n(0, 0)的左右兩邊。當有兩個不同值時，則設定 n(0, 0)的左右兩邊不相等
(n(0, 0).left≠n(0, 0).right)。接著，從 n(0, 0)開始對應所有的 product terms。對於每個 product term 
t, 我們使用一個以 depth first search 為基礎的方法建構一條路徑給 t。LeftConfigure()和
RightConfigure()分別為設定一個節點的左邊和右邊的函式。假使我們無法從 n(0, 0)開始而成功
地對應 t，則我們使用 Expand()來擴展架構。最後，我們設定所有未設定的邊為 open。 
 XII 
 
本研究群近 3年已發表之論文及競賽獲獎統計如下: 
 
 97.8-98.7 98.8-99.7 99.8-100.7 
IEEE/ACM Journal 2
$
 1
**
 2
@
 
Int. Conf 5
&
  4
*
  3
#
  
CAD Contest 佳作 1 0 0 
&Ref.[1~5]  $Ref.[6~7]  **Ref.[8]  *Ref.[9~12]    @Ref.[13~14]  #Ref.[15~17] 
 2009 ASPDAC Best paper nominee, "Dependent Latch Identification in the Reachable State 
Space".  
 2009 Student Forum ASPDAC Best poster award, "An Implicit Approach to Minimizing 
Range-Equivalent Circuits". 
 2010 DAC Best paper nominee, "Node Addition and Removal in the Presence of Don’t 
Cares ".  
 
四、 結論與討論 
本研究探討針對一組合電路研究如何在面積最佳化的目標下，自動化合成至一個以二
元決策圖(BDD)為基礎的單電子電晶體(SET)架構。此項研究成果已獲得肯定且在國際會議
上發表[15]。另外本研究群在本計畫之支持下，同時進行了多項與邏輯合成，驗證，測試
相關的研究，分別為"Logic Restructuring Using Node Addition and Removal "，" Fast Node 
Merging with Don’t Cares Using Logic Implications "， " A Register-Transfer Level Testability 
Analyzer "及" On Rewiring and Simplification for Canonicity in Threshold Logic Circuits "，
也已經/即將發表於國際學術會議[16][17]及 IEEE Transactions on CAD 期刊[13][14]。 
 
五、 參考文獻 
 
[1] C.-H. Lin, C.-Y. Wang, "Dependent Latch Identification in the Reachable State Space", 2009 IEEE 
Asia and South Pacific Design Automation Conference (ASPDAC'09).  
[2] M.-S. Chan, C.-Y. Wang, and Y.-C. Chen, "An Efficient Approach to SiP Design Integration", 2009 
IEEE International Symposium on Quality Electronic Design (ISQED'09). 
[3] Y.-L. Liu, C.-Y. Wang, Y.-C. Chen, and Y.-H. Chang, "A Novel ACO-based Pattern Generation for 
Peak Power Estimation in VLSI Circuits", 2009 IEEE International Symposium on Quality Electronic 
Design (ISQED'09). 
[4] C.-C. Lin, C.-Y. Wang, "Rewiring Using Irredundancy Removal and Addition", 2009 IEEE Design, 
Automation and Test in Europe (DATE'09). 
[5] Y.-C. Chen, C.-Y. Wang, "Enhancing SAT-based Sequential Depth Computation by Pruning Search 
 XIV 
六、計畫成果自評 
 
(一)與原計畫相符程度: 第一二年相符，且第三年的目標提前於第二年完成，故第三年進行與熱能
相關之更先進的低功率及低耗能的研究，我們進行以單電子電晶體為電路元件的超低耗能電腦
輔助設計的研究[15]。 
 
(二)達成預期目標情況: 均順利達成，且有國際學術會議論文及期刊論文發表。 
 
(三)研究成果之學術或應用價值: 成果均有國際學術論文發表[13~17]，具學術及應用價值。 
 
(四)是否適合在學術期刊發表或申請專利: 適合，已投稿至國際學術期刊 (ACM Journal on 
Emerging Technologies in Computing Systems) 。 
 
(五)主要發現或其他有關價值等: 本研究群在本計畫之支持下，同時進行了多項與邏輯合成，驗
證，測試相關的研究，分別為"Logic Restructuring Using Node Addition and Removal "，" Fast Node 
Merging with Don’t Cares Using Logic Implications "， " A Register-Transfer Level Testability 
Analyzer "及" On Rewiring and Simplification for Canonicity in Threshold Logic Circuits "，也已
經/即將發表於國際學術會議[16][17]及 IEEE Transactions on CAD 期刊[13][14]。
 
 1 
Current detector 
1 
(a) (b) 
a⊕b 
a a’ 
b’ b 
Active high  
Active low 
Short  
Figure 2: (a) A SET array fabric. (b) An example of a
xor b.
a conducting nanowire or have a wrapped gate. Consequently,
this structure is not very regular and cannot be restructured to
implement a different function due to the physical etching process
involved in its realization. Furthermore, if any of the nanowire
segments or the wrap gates is defective, the whole circuit be-
comes non-functional. This is a significant limitation considering
that nanowires and few electron nanodevices have traditionally
suffered from the variability and reliability issues.
To solve the problem, a reconfigurable version of SET using
wrap gate tunable tunnel barriers was proposed [2] and the in-
depth device simulation to study the electrostatic properties was
presented [8]. This device can operate in three distinct operation
states: a) active b) open and c) short state based on the wrap gate
bias voltages. Such programmability leads to immense flexibility
in designing a circuit. The device simulation shows that this
device can provide an order of magnitude lower energy-delay than
CMOS device [8].
However, the synthesis of a BDD using the device in [2] is man-
ual rather than automated. The reason is that mapping a reduced
ordered BDD (ROBDD) into a planar SET array could be very
complicated, especially when the BDD has crossing edges, which
is typical in minimized BDDs. In this work, we address this
mapping problem and propose an automated mapping approach.
Instead of mapping a BDD directly, the proposed approach first
divides a BDD into a set of product terms that represent the
paths leading to the 1 terminal in the BDD. Then, it sequentially
maps these product terms. Since the mapping order of the prod-
uct terms affects the mapping results, we propose four sorting
heuristics to reduce area cost. Additionally, the automated map-
ping approach incorporates the granularity and fabric constraints
that are imposed in order to decrease the number of metal wires
used for programming the SET array and for supplying the input
signals, respectively [2].
We conduct experiments on a set of MCNC benchmarks [10].
The experimental results show that the proposed approach can
complete mapping within 1 second for most of the benchmarks.
The main contribution of this work is proposing an automated
synthesis tool for the promising energy-efficient SET array archi-
tecture.
The rest of this paper is organized as follows: Section 2 uses
an example to demonstrate the problem considered in this paper,
and introduces some notations. Section 3 presents the proposed
mapping approach. Section 4 discusses and addresses two map-
ping constraints. Finally, the experimental results and conclusion
are presented in Sections 5 and 6.
2. BACKGROUND
2.1 An example
A SET array can be presented as a graph composed of hexagons.
As shown in Fig. 2(a), like the hexagonal fabric mentioned above,
there is a current detector at the top that measures the current
coming from the bottom of the hexagonal fabric. All the vertical
edges of the hexagons are electrical short. All the sloping edges
can be configured as active high, active low, short or open. An
active high edge is controlled by a variable x. It is conducting
and non-conducting when x = 1 and x = 0, respectively. Con-
 
root node (0, 0) 
4 
x 1 4 3 2 0 -1 -2 -3 
y 
3 
2 
1 
0 
node (-3, 3) 
left edge 
right edge 
root node (0, 0) 
x 1  3 2 0 -1 -2 -3 
y 
3 
2 
1 
0 
node (-3, 3) 
left edge 
right edge 
-4 4 
Figure 3: An abstract diamond fabric.
versely, an active low edge is an electrical opposite of an active
high edge and it is controlled by a variable x′.
A Boolean function can be implemented using a SET array.
All the active edges at the same row of the hexagonal fabric are
controlled by a single variable, i.e., a primary input (PI). They
determine whether there exists a path for the current to pass
through, and thus, be detected at the top. If so, the functional
output of the array is 1; otherwise, it is 0. For example, Fig.
2(b) shows a SET array implementing a xor b. When a = 1
and b = 0, the current can be detected by passing through the
left path. However, if a = 1 and b = 1, the current cannot be
detected.
Thus, the addressed problem of this work is synthesizing a given
Boolean function into a SET array with minimized area, i.e., the
number of configured hexagons.
Previous work [2] tries to manually map a Boolean function
by directly mapping its BDD into a SET array. However, the
mapping process could be very complicated due to the structural
difference of a BDD and a SET array. For example, an ROBDD
usually has some crossing edges. Since a SET array is a planar
architecture, much effort is required to avoid having the crossing
edges in the ROBDD when mapping it into a SET array. Node
duplication could be a trivial method for solving this crossing
edge issue while not considering the area overhead. In addition,
determining the exact location of each ROBDD node in a SET
array is a challenge. Thus, to address this problem, we propose
a product term-based method. It first collects all the paths that
lead to the terminal 1 in the ROBDD, i.e., product terms. Then,
it maps each product term into a path in the SET array. The
proposed method simultaneously avoids the crossing edge and the
BDD node mapping issues.
For example, the product terms of a xor b are 10 and 01. Using
the proposed method, we first map 10 and then 01. Finally, we
obtain the resultant SET array as shown in Fig. 2(b), where the
left path is configured for 10 and the right path is for 01.
2.2 Notations
For ease of discussion, we use an abstract graph to present a
SET array. Compared to Fig. 2(a), only the configurable edges
are preserved as shown in Fig. 3. In this diamond fabric, each
node n, i.e., the root of a pair of left and right edges, has a unique
location (x, y). Based on the root node located at (0, 0), which
is below the current detector, the y value increases from top to
bottom. The x value increases and decreases from center to right
and left, respectively.
For simplification, let n.left and n.right denote the status of the
left and right edges of a node n, respectively. The status could
be empty, high, low, short, or open. empty indicates the edge is
not configured yet (is used primarily for algorithm illustration).
high, low, short, and open indicate the edge is configured as active
high, active low, short, and open, respectively. Additionally, let
n(x,y) denote the node located at (x, y).
3. AUTOMATED MAPPING
In this section, we first discuss the motivation of our method.
Next, we introduce two key mapping procedures. Finally, the
overall flow is presented. Here, we first assume that each edge
can be configured independently without any constraint. In the
879
48.3
= 
1 1 1 – 0 
0 1 0 – 0 
0 1 0 0 1 
 Active high 
Active low 
Short 
Open 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
1 1 1 – 0 
0 1 – – 0 
0 1 1 1 1 
 
3 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
3 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
3 4 
(a) (b) 
(c) (d) 
4 
Figure 6: Incorrect mapping examples.
p2 = 111−, and p3 = 101−, sorted by ForInertiaSort as shown in
Fig. 5(a). First, let us consider p0. Starting from the root node
n(0,0), we first configure n(0,0).left as low for the first bit 0. Next,
we configure n(−1,1).right as high for the second bit 1. Using the
same method, we configure n(0,2).left and n(−1,3).right as high
and low for the last two bits 10, respectively. The mapping result
is shown in Fig. 5(b). Here, the decision of configuring the left
edge or the right edge of a node depends on its location (x, y). If
x < 0, we first try to configure its right edge. If inapplicable, we
then try to configure its left edge. Conversely, if x ≥ 0, we try
the left edge first and then the right edge.
Next, for p1, because the first two bits are the same as that
of the first product term, we partially reuse this mapping result.
Then, we configure n(0,2).right as low and n(1,3).left as short for
the last two bits 0−, respectively. The mapping result is shown
in Fig. 5(c).
For p2, after we configure n(0,0).right as high for the first bit 1,
we do not configure n(1,1).left as high for the second bit 1. This
is because if we do so, there will exist a path n(0,0) → n(1,1) →
n(0,2) → n(1,3) → n(0,4), which corresponds to an invalid product
term 110−. Thus, we configure n(1,1).right as high for the second
bit 1. Finally, n(2,2).left and n(1,3).left are configured as high and
short for the last two bits 1−, respectively. The mapping result
is shown in Fig. 5(d).
Next, let us consider p3. After finding n(0,0).right = high for
the first bit 1, we do not configure n(1,1).left as low for the sec-
ond bit 0. This is because it will construct a path for an invalid
product term 100−. Additionally, since n(1,1).right has been con-
figured as high, we expand the structure by configuring both
n(2,0).left and n(2,0).right as short, and start from n(3,1) for the
last three bits. The mapping result is shown in Fig. 5(e). Finally,
we configure all the non-configured edges as open, and obtain the
final mapping result in Fig. 5(f).
To avoid creating an invalid path, we need to prevent two paths
from merging and then branching during mapping. Thus, when
we detect a merging node, like n(0,2) for p2 or p3, we will check
if there exists only one path from n(0,2). If not, there possibly
exists an invalid path. Thus, we prevent the paths from merging.
With this checking rule, each path from top to bottom exactly
corresponds to one product term. In addition, from the viewpoint
of conducting paths, this checking rule is not enough and we have
to add another rule considering the conducting path issue. Fig.
6(a), (b) show two mapping examples, which are incorrect while
satisfying the merging and branching rule.
In Fig. 6(a), when the input pattern is 11101, which is not a
minterm, the current can be detected at the top. This is because
the right edge of n(−1,3), the left edge of n(1,3), and the right
edge of n(1,3) as highlighted are conducting simultaneously. This
partial conducting path forms like a bridge that connects two
paths such that the current can pass through the path n(1,5) →
n(2,4) → n(1,3) → n(0,4) → n(−1,3) → n(0,2) → n(−1,1) →n(0,0).
In addition, a partial conducting path could be composed of the
Mapping(set PTs) // PTs: product terms
1. Configure n(0,0).left and n(0,0).right based on the first bit values
of the product terms in PTs;
2. For each product term t in PTs
2.1. If (LeftConfigure(t, 0, 0)), continue;
2.2. If (RightConfigure(t, 0, 0)), continue;
2.3. Expand(t);
3. Configure all the edges that are not configured yet as open;
bool LeftConfigure(productterm t, int x, int y)
1. If n(x,y).left is inconsistent to the y
th bit in t, return 0;
2. If n(x−1,y+1) is a merging node and there is more than one path
from n(x−1,y+1), return 0;
3. If the configuration of n(x,y).left will make the left edge of n(x,y)
and the right edge of n(x−2,y) could be conducting simultane-
ously, return 0;
4. If n(x,y).left is empty, configure it based on the mapping rules;
5. If (x− 1 < 0)
5.1. If (RightConfigure(t, x− 1, y + 1)), return 1;
5.2. If (LeftConfigure(t, x− 1, y + 1)), return 1;
6. If (x− 1 ≥ 0)
6.1. If (LeftConfigure(t, x− 1, y + 1)), return 1;
6.2. If (RightConfigure(t, x− 1, y + 1)), return 1;
7. Undo n(x,y).left if necessary, and return 0;
bool RightConfigure(productterm t, int x, int y)
1. If n(x,y).right is inconsistent to the y
th bit in t, return 0;
2. If n(x+1,y+1) is a merging node and there is more than one path
from n(x+1,y+1), return 0;
3. If the configuration of n(x,y).right will make the right edge of
n(x,y) and the left edge of n(x+2,y) could be conducting simul-
taneously, return 0;
4. If n(x,y).right is empty, configure it based on the mapping rules;
5. If (x− 1 < 0)
5.1. If (RightConfigure(t, x + 1, y + 1)), return 1;
5.2. If (LeftConfigure(t, x + 1, y + 1)), return 1;
6. If (x− 1 ≥ 0)
6.1. If (LeftConfigure(t, x + 1, y + 1)), return 1;
6.2. If (RightConfigure(t, x + 1, y + 1)), return 1;
7. Undo n(x,y).right if necessary, and return 0;
bool Expand(productterm t)
1. Determine the expansion direction (left or right) based on the
first bit in t.
2. If the expansion direction is left, x = −2; otherwise, x = 2;
3. While(1)
3.1. Configure n(x,0).left and n(x,0).right as short if they are
empty;
3.2. If (x− 1 < 0)
3.2.1 If (RightConfigure(t, x− 1, 1)), return 1;
3.2.2 If (LeftConfigure(t, x− 1, 1)), return 1;
3.2.3 x = x− 2;
3.3. If (x− 1 ≥ 0)
3.3.1 If (LeftConfigure(t, x + 1, 1)), return 1;
3.3.2 If (RightConfigure(t, x + 1, 1)), return 1;
3.3.3 x = x + 2;
Figure 7: The algorithm of product term mapping.
edges at the different rows. For example, Fig. 6(b) shows a par-
tial conducting path that crosses two rows as highlighted. This
path, n(3,3) → n(2,2) → n(1,3) → n(0,4) → n(−1,3), constructs an
invalid conducting path for the input pattern 11111.
A necessary condition for causing a partial conducting path is
that there exist two pairs of two adjacent conducting edges: one
pair is two lower edges of a diamond that could be conducting si-
multaneously, and the other pair is two upper edges of a diamond
that could be conducting simultaneously. For example, in Fig.
6(a), the right edge of n(−1,3) and the left edge of n(1,3) are the
former, and the left and right edges of n(1,3) are the latter. One
simple method for avoiding partial conducting paths is to ensure
that one of the mentioned two pairs of two adjacent conducting
edges is never constructed. Thus, if a configuration results in a
merging node, we check if the two edges connecting to the merg-
ing node could be conducting simultaneously. If so, we avoid this
configuration. With this method, we can prevent two lower edges
of a diamond from conducting simultaneously. Fig. 6(c) and Fig.
6(d) show the correct mapping results for the product terms in
Fig. 6(a) and Fig. 6(b), respectively.
Additionally, because the root node has only two edges (left
881
48.3
Table 1: The experimental results of using differ-
ent product term sorting heuristics and mapping con-
straints.
Bench. PI PO PT
Constraint-free Granu. Fabric
Lex Inert. FInert. BFInert. FInert. FInert.
C17 5 2 8 *18 *18 20 20 58 66
cm138a 6 8 48 *116 158 120 120 360 438
x2 10 7 33 *149 152 153 154 725 790
cm85a 11 3 49 219 197 197 *195 608 528
cm151a 12 2 25 406 427 *400 *400 885 1045
cm162a 14 5 37 292 336 294 *287 1077 1163
cu 14 11 24 240 242 *238 *238 609 662
cmb 16 4 26 195 216 *170 *170 710 855
cm163a 16 5 27 275 *257 260 260 907 1029
pm1 16 13 41 337 342 *335 *335 1186 1239
pcle 19 9 45 *291 292 293 293 1553 1775
sct 19 15 142 1890 *1661 1725 1741 4665 5186
cc 21 20 57 618 658 *585 603 2214 2306
i1 25 16 38 632 650 *627 *627 1773 1920
lal 26 19 160 1968 2157 1832 *1799 7838 8684
pcler8 27 17 68 *737 850 *737 *737 3160 3435
frg1 28 3 399 5993 *5602 5612 5612 11029 13731
c8 28 18 94 *836 884 881 894 4663 4869
term1 34 10 1246 23494 25297 *22426 23856 70844 80293
count 35 16 184 1936 1861 *1336 1465 13509 14678
unreg 36 16 64 1288 *1259 1280 1280 4518 4632
b9 41 21 352 *6333 8650 6478 6542 24272 22089
cht 47 36 92 *2380 2390 *2380 *2380 7857 7934
apex7 49 37 1440 36252 44001 *35999 36317 123003 135543
example2 85 66 430 9737 10164 9623 *9494 53597 50471
Total 96632 108721 94001 95819 341620 365361
Best count 8 5 11 11
4.2 Fabric constraint
In SET array implementation, the inputs to the active edges in
a row are supplied by metal wires. We need two wires to supply
both the normal and complement of an input to a row. Each
edge is connected to either x or its complement x′ wires for the
row. The pattern of connections of x and x′ in a row defines the
SET fabric and it is fixed during manufacturing.
For example, using x to control all left edges and x′ to control
the right edges results in the symmetric fabric proposed in [2].
In our mapping tool, we use the symmetric fabric constraint. In
the future, we will extend our mapping tool to accept any fabric
specification.
In such an array, both (high, low) and (low, high) cannot
simultaneously appear at the same row in a SET array. Note
that the entire row pattern of (high, low) (or (low, high)) can be
changed to (low, high) (or (high, low)) by swapping the normal
value and its complement in the control input signals for the row.
To satisfy this symmetric fabric constraint, we need to identify
which combination ((high, low) or (low, high)) appears at a
certain row. One method is to follow the first configuration result
at the row. For example, if (high, low) is first configured at a
row, we then do not configure (low, high) at this row. Another
easy method is to allow only one of (high, low) and (low, high)
to appear in a SET array. For example, for a bit value 1 or 0, we
can always configure the left edge as high and the right edge as
low, i.e., only (high, low) is allowed. For simplification, we use
the second method in this work.
Fig. 9(b) shows the mapping result for the same set of prod-
uct terms in Fig. 5(a) considering the fabric constraint. In this
example, only (high, low), (short, short), and (open, open) are
allowed.
5. EXPERIMENTAL RESULTS
We implemented the algorithm in C language. The experi-
ments were conducted on a 2.67 GHz Linux platform (Red Hat
5.5). The benchmarks are from the MCNC benchmark suite [10].
For each benchmark, we separately map the Boolean function of
each primary output (PO), and measure the total number of con-
figured hexagons and the total CPU time. In the experiments, we
compare different product term sorting heuristics and mapping
constraints.
Table 1 summarizes the experimental results. Column 1 lists
the benchmarks. Except the C17 benchmark, all the benchmarks
have the crossing edge issue in their ROBDDs. Directly mapping
each of these ROBDDs into a SET array could be very diffi-
cult. Columns 2 and 3 list the number of PIs and POs in each
benchmark, respectively. Column 4 lists the number of computed
product terms. The remaining columns list the mapping results
in terms of the number of hexagons by using different sorting
heuristics and constraints. The number marked with “*” means
that it is the best result among all sorting heuristics. Columns 5
to 8 are the constraint-free mapping results by using LexSort, In-
ertiaSort, ForInertiaSort, and BackForInertiaSort, respectively.
Columns 9 and 10 are the mapping results of applying the gran-
ularity and fabric constraints by using ForInertiaSort only. This
is because the ForInertiaSort heuristic has better results for con-
sidering all benchmarks or large benchmarks in the experiments.
We omit the results by using the other sorting heuristics due to
page limit.
According to Table 1, there is no a specific sorting heuristic
that completely outperforms the others for all the benchmarks.
By all accounts, ForInertiaSort results in the best mapping for
considering all benchmarks. Additionally, when the constraints
are considered, the number of configured hexagons increases.
This is because the number of edges shared by different paths
decreases. As for the CPU time, the proposed method can map
each benchmark within 1 second except the term1 and apex7
benchmarks that spent approximately 6 seconds.
6. CONCLUSION
In this paper, we propose a product-term-based approach that
can efficiently map a Boolean function into a SET array. It solves
the problem of automatically mapping a BDD into a SET array
that previous works suffer from. The proposed approach sim-
plifies the mapping problem by transforming a BDD into a set
of product terms, and then individually mapping these product
terms. Additionally, four product term sorting heuristics are pro-
posed to enrich the approach. The granularity and fabric con-
straints can also be handled by the proposed approach. The
experimental results show its effectiveness and efficiency of map-
ping a set of MCNC benchmarks. Our automated mapping is a
key enabler for using the promising BDD technology.
7. REFERENCES
[1] R. Bryant, “Graph-based Algorithms for Boolean Function
Manipulation,” IEEE Trans. Computers, vol. 35, pp. 677-691,
Aug. 1986.
[2] S. Eachempati, V. Saripalli, V. Narayanan, and S. Datta,
“Reconfigurable Bdd-based Quantum Circuits,” in
Proc. Int. Symp. on Nanoscale Architectures, 2008, pp. 61-67.
[3] H. Hasegawa and S. Kasai, “Hexagonal Binary Decision Diagram
Quantum Logic Circuits Using Schottky In-Plane and Wrap Gate
Control of GaAs and InGaAs Nanowires,” Physica E:
Low-dimensional Systems and Nanostructures, vol. 11, pp. 149-154,
Oct. 2001.
[4] S. Kasai, M. Yumoto, and H. Hasegawa, “Fabrication of GaAs-based
Integrated 2-bit Half and Full Adders by Novel Hexagonal BDD
Quantum Circuit Approach,” in Proc. Int. Symp. on Semiconductor
Device Research, 2001, pp. 622-625.
[5] M. Keating, D. Flynn, R. Aitken, A. Gibbons, and K. Shi, Low
Power Methodology Manual: For System-on-Chip Design,
Springer, 2007.
[6] S. W. Keckler, K. Olukotun, and H. P. Hofstee, Multicore
Processors and Systems, Springer, 2009.
[7] C. Piguet, Low-power CMOS Circuits: Technology, Logic Design
and CAD Tools, CRC Press, 2006.
[8] V. Saripalli, L. Liu, S. Datta, and V. Narayanan, “Energy-Delay
Performance of Nanoscale Transistors Exhibiting Single Electron
Behavior and Associated Logic Circuits”, Journal of Low Power
Electronics (JOLPE), vol. 6, pp. 415-428, 2010.
[9] F. Somenzi, CUDD: CU decision diagram package - release 2.4.2,
2009. http://vlsi.colorado.edu/∼fabio/CUDD/
[10] S. Yang, “Logic Synthesis and Optimization Benchmarks, Version
3.0,” Tech. Report, Microelectronics Center of North Carolina,
1991.
[11] http://embedded.eecs.berkeley.edu/pubs/downloads/
espresso/index.htm
[12] http://www.intel.com/go/terascale/
883
48.3
nodes. The algorithm may be efficient because
of vectorless. However, the estimated testability
might be inaccurate when the design contains lots
of high-level constructs. This is the constraint of this
work.
In this work, we propose a statistic-based
method to estimate the testability of a design at RTL.
To accomplish this, we propose a new representa-
tion Register-Transfer Level with Assignment Deci-
sion (RTL-AD), which is similar to Assignment De-
cision Diagram (ADD) [3], but is more appropriate
to precisely computing the testability and easily per-
forming parallel simulation. Furthermore, our ap-
proach applies a statistic method in the Monte Carlo
Simulation to bound the error rate and confidence
level. As a result, our approach can directly report the
testability of each assignment statement in an RTL
design.
II. RTL-AD STRUCTURE
A. The Skeleton of RTL-AD
RTL-AD contains six components as shown in
Fig. 1: data node, assignment decision node, assign-
ment selection node, statement flow node, logical
node, and arithmetic node. The data node can be ei-
ther a read node or a write node. The assignment de-
cision node decides whether or not the data source
can be passed to the target. The assignment selec-
tion node has multiple data sources at a time and
decides which data source could be passed through.
The statement flow node determines the switching
timing of all assignment decision nodes and assign-
ment selection nodes. All operators are divided into
logical node, e.g., & (AND), | (OR), or ^ (XOR), and
arithmetic node, e.g., + (addition),  (right shift), or
% (modulo). In other words, logical nodes are the
same as logic gates, and arithmetic nodes have the
functions of high-level arithmetic operations.
Figure 1: The six components of RTL-AD structure.
B. Transformation of RTL-AD from RTL Descrip-
tion
To better explain the construction of RTL-AD, we
use a hierarchical design as shown in Fig. 2 to go
through the construction of RTL-AD, which is very ef-
ficient. There are five steps to construct the RTL-AD
from an RTL description.
B.1 Generate statement flow
The statement flow acts as an FSM to control
the simulation sequence of these assignment state-
ments by controlling the corresponding assignment
decision nodes and assignment selection nodes.
Before generating the statement flow, we first label
each assignment statement a unique ID as shown in
Fig. 2.
In general, the assignment statements within an
always block are sequentially executed. However, for
the assignment statements that belong to the same
high-level construct, e.g., if-else block, case block,
they are parallel executed. For example in Fig. 2,
since the assignment statements 3, 6, 7 of module A
belong to the same if-else block, they are parallel ex-
ecuted. Thus, the initial statement flow of module A is
shown in Fig. 3(a). Since the left-hand side of assign-
ment statement 7 is Dout, which is also the left-hand
side of the assignment statements 4 and 5 which are
controlled by variables con and DFF. Therefore, the
assignment statement 7 is postponed to be executed
in the same level of the assignment statements 4 and
5 as shown in Fig. 3(b).
Figure 2: An example of hierarchical RTL design.
B.2 Build data path and condition flow
The data path is a passage for data transfer. The
condition flow includes the control information in all
conditional statements. We insert assignment deci-
sion nodes and assignment selection nodes in be-
tween the left-hand side and right-hand side of all
assignment statements in building the data path. Af-
terward, we build the condition flow by connecting
is indicated by the index 1. We use a comparator as
shown in Fig. 7(c) to do the comparison operation to
facilitate the parallel simulation.
Figure 7: An example for virtual simulation.
High-level Simulation For the complex arithmetic
operations, such as division and modulo, we imple-
ment them from the high-level viewpoint. We use an
example of division operation A/B as shown in Fig.
8 to demonstrate this idea. We first convert the ran-
dom values of A and B from binary to decimal, and
then repeat the division operation four times to get
the answers (3, 1, 0, 0). The computed results are
converted back from decimal to binary again, and are
saved in the data node C.
Figure 8: An example for high-level simulation.
A.2 Sampling Rule
Each sampling in Monte Carlo method will re-
turn a result for the desired answer, which is the fault
detection probability (testability) of each wire in the
RTL-AD structure in this work. Here, we apply the
similar method in [6] to obtain this value. Due to page
limit, we skip the detail of this part.
A.3 Scoring
With the sampling rule, the data obtained from
a sufficient amount of samplings are collected to
approximate the exact result.
The next important issue we have to deal with
is how many samplings are sufficient for just obtain-
ing an accurate result. Specifically, when considering
parallel simulation in this work, the bit width of one
simulation also relates to the number of samplings
needed. That is, when the bit width is larger, the
number of samplings can be fewer, and vice versa.
On the other hand, since our statistic model uses
the t-distribution [10], which is more appropriate for
the sampling times under 30, to estimate the error
rate and confidence level. We also take this issue
into consideration in determining the bit width r in our
parallel simulation architecture when conducting the
experiments.
Figure 9: A demonstration of confidence level.
A.4 Error Estimation
We exploit the Confidence Interval in statistics to
estimate the error rate of fault detection probability of
a wire as compared with the mean of the normal dis-
tribution. Suppose that we have N samplings of the
fault detection probability of a wire w, then we can
compute the sample standard deviation of this wire
denoted as sd(w) among the samplings. The param-
eter of the predefined confidence level is α as shown
in Fig. 9 where (1 − α) × 100% represents the confi-
dence level. Then we can look up the value tα
2
from
the t-distribution table with (N − 1) degrees of free-
dom. Hence, we have (1 − α) × 100% confidence
level that the estimated error rate of the fault detec-
tion probability ofw, e(w), is expressed as EQ(1) [10].
e(w) =
tα
2
× sd(w)√
N
(1)
Consequently, for a desired error rate  in the
fault detection probability estimation, and for a given
confidence level (1 − α) × 100%, we have to repeat
sampling until
tα
2
× sd(w)√
N
<  (2)
EQ(2) is called the stopping condition of our
Monte Carlo approach. However, it is time-
consuming to check whether the fault detection prob-
ability of every wire meets the stopping condition de-
fined in EQ(2). As a result, a heuristic of selecting
some appropriate check points such that most wires
would satisfy the stopping condition while the se-
lected check point wires did is proposed. In addition,
after observing EQ(2), we realize that only the sam-
ple standard deviation sd(w) of each wire differs from
others. Thus, we use the sd(w) to represent each
wire and choose the top 10% of wires with higher
sd(w) as the check points after several initial sam-
220 and 215 in our experiments, respectively. We only
report the faults of those matching points by using
the signature-based verification method mentioned.
For example in s9234 benchmark, it has 7546
nodes, total CPU time is 33.86 seconds, and 52% of
matching points are within 0~5% error rate.
According to Table 1, the CPU time required for
the proposed approach is not much which indicates
that the RTL-AD construction is not a burden for the
approach. Furthermore, 56% of matching points in
a benchmark are within the error rate range of 0~5%
on average. Thus, the proposed statistic approach is
quite accurate.
Table 2: The results of low testability point identification.
circuit | node having low testability | ratio
ours exact
s27 5 7 71%
s208 55 98 56%
s298 33 83 40%
s344 65 126 52%
s349 51 108 47%
s382 67 114 59%
s386 63 74 85%
s400 55 103 53%
s420 121 200 51%
s444 62 103 60%
s510 84 128 66%
s526 71 141 50%
s641 162 353 46%
s713 183 389 47%
s820 118 139 85%
s832 123 145 85%
s838 192 276 70%
s953 225 348 65%
s1423 181 386 47%
s1488 188 260 72%
s1494 221 302 73%
s5378 928 1858 50%
s9234 1593 3256 49%
s13207 2037 6333 32%
s15850 2663 6492 41%
s35932 1341 2220 60%
s38417 6073 17692 34%
s38584 7074 14539 49%
counter8 0 0 -
vendor 3 3 100%
blackjack 8 9 89%
rankf 0 0 -
div8 206 212 97%
average - - 61%
From the application viewpoint, we also would
like to know if our approach can identify those low
testability statements in RTL designs. Table 2 shows
the comparison between our approach and exact ap-
proach on identifying the low testability statements.
A point with less than 20% testability is regarded as
a low testability point in this paper. Column 2 shows
the number of low testability points identified by our
approach. Column 3 is the exact number of low testa-
bility points from netlists. The last column shows the
ratio of identified low testability points as compared to
the exact number. Take s832 as an example, 145 low
testability points are identified by the exact method,
and 123 or 85%, low testability points are identified
by our approach. According to Table 2, 61% of low
testability points are identified on average by our ap-
proach.
V. CONCLUSION
In this paper, we propose a statistic-based testa-
bility analyzer based on the proposed RTL-AD struc-
ture and Monte Carlo method at RTL design descrip-
tion. It directly reports the testability of each assign-
ment statement in RTL designs. With this approach,
designers can efficiently identify many low testability
statements such that the design cycle can be short-
ened when considering design-for-testability issue.
REFERENCES
1. V. D. Agrawal and S. C. Seth, “Probabilistic testability,” in Proc.
ICCD, pp. 562–565, 1985.
2. F. Brglez, “On testability of combinational networks,” in Proc.
ISCAS, pp. 221–225, 1984.
3. V. Chaiyakul and D. D. Gajski, “Assignment decision diagram
for high-level synthesis,” UC Irvine, Technical Report ICS-TR-
92-103, Dec. 1992.
4. S. Chakravarty and H. Hunt, “On computing signal probabil-
ity and detection probability of stuck-at faults,” IEEE TC, pp.
1369–1377, Nov. 1990.
5. S. C. Chang, W. B. Jone, and S. S. Chang, “Tair: Testability
analysis by implication reasoning,” IEEE TCAD, pp. 152–160,
Jan. 2000.
6. C.-C. Chiou, C.-Y. Wang, and Y.-C. Chen, “A statistic-based
approach to testability analysis,” in Proc. ISQED, pp. 267–270,
2008.
7. L. H. Goldstein and E. L. Thigpen, “Scoap: Sandia controlla-
bility/observability analysis program,” in Proc. DAC, pp. 190–
196, 1980.
8. Y. Huang, N. Mukherjee, W.-T. Cheng, and G. Aldrich, “A
rtl testability analyzer based on logical virtual prototyping,” in
Proc. ATS, pp. 121–124, 2007.
9. S. K. Jain and V. D. Agrawal, “Statistical fault analysis,” IEEE
Design & Test of Computers, vol. 2, pp. 38–44, Feb. 1985.
10. I. R. Miller, J. E.Freund, and R. Johnson, “Probability and
statistics for engineers,” Englewood Cliffs, NJ: Prentice Hall,
1990.
11. K. P. Parker and E. J. McCluskey, “Probabilistic treatment
of general combinational networks,” IEEE TC, pp. 668–670,
1975.
12. C. P. Ravikumar and G. S. Saund, “A stafan-like functional
testability measure for register-level circuits,” in Proc. ATS, pp.
192–198, 1995.
13. J. Savir, G. S. Ditlow, and P. H. Bardell, “Random pattern
testability,” IEEE TC, pp. 79–90, Jan. 1984.
14. S. C. Seth, L. Pan, and V. D. Agrawal, “Predict: Probabilistic
estimation of digital circuit testability,” in Proc. FTCS, pp. 220–
225, 1985.
15. J. Strnadel, “Testability analysis and improvements of register-
transfer level digital circuits,” Computing and Informatics, pp.
1001–1024, Sep. 2006.
16. SYNOPSYS. [Online]. Available: http://www.synopsys.com
17. M. Takahashi, R. Sakurai, H. Noda, and T. Kambe, “A testa-
bility analysis method for register-transfer level descriptions,”
in Proc. ASP-DAC, pp. 307–312, 1997.
18. D. T. Wang, “An algorithm for the generation of test sets for
combinational logic network,” IEEE TC, pp. 742–746, July
1975.
19. S.-J. Wang and T.-H. Yeh, “High-level test synthesis with hier-
archical test generation for delay-fault testability,” IEEE TCAD,
pp. 1583–1596, Oct. 2009.
20. S.-C. Wu, C.-Y. Wang, and Y.-C. Chen, “Novel probabilistic
combinational equivalence checking”, IEEE TVLSI, pp.365-
375, April 2008.
ab
1
1 n12
c
d
g
h
3
1
1
1
n24
b
h
2
1
1
2
n5
2
d
f
g
h
1
1
1
1
n44
e
f
1
1
2
2
n3
2
(a)
a
b
1
1 2
c
d
g
h
3
1
1
1
4
d
f
g
h
1
1
1
1
n4
4
2
2
2
n5
2
n7
n1
n2
1
1 2
b
h
1
1 2
2
2
2
n3
2
n6
e
f
(b)
a
b
1
1
n12
c
d
g
2
1
1
n23
2
2
2
n5
2
d
f
g
h
1
1
1
1
n44
2
2
2 n3
2
1
1 n62
b
h
1
1
n72
1
1 n81
c
h
1
1
n92
The rectification network 
for h’s removal 
e
f
(c)
a
b
1
1
n12
c
d
g
2
1
1
n23
2
2
2
n5
2
2
2
2 n3
2
1
1 n62
b
h
1
1
n72
1
1 n81
c
h
1
1
n92
f
g
h
1
1
1
n4
3
d
1
1 n102 The rectification 
network for d’s removal 
e
f
(d)
Figure 1: (a) The original threshold network. (b) The resultant network after input grouping and gate decomposition. (c) The resultant network
after rewiring target wire h. (d) The resultant network after rewiring target wire d.
number2, designers can either resynthesize the network by using
the same synthesis methodology, or remove some wires from the
gates violating this fanin number constraint and add the rectiﬁcation
network for correcting the functionality.
This work makes two main contributions:
1) It is the ﬁrst rewiring algorithm employing on a threshold
networks that changes its connectivity while preserving its
functionality.
2) The proposed simpliﬁcation procedure produces a canonical
representation of a threshold gate.
The rest of the paper is organized as follows. Section II gives
an example of our rewiring algorithm. Section III introduces the
background. Section IV presents the proposed rewiring algorithm.
Section V presents the simpliﬁcation procedure. Section VI shows
the experimental results of our rewiring algorithm. Finally, Section
VII concludes this work.
II. AN EXAMPLE FOR REWIRING
In this section, we use a brief example to demonstrate the capability
of our rewiring algorithm. We take a threshold network consisting of
ﬁve threshold gates, as shown in Fig. 1(a). Here we assume the fanin
number constraint of the network is four. If we want to produce
another network with the same functionality but with a smaller fanin
number constraint, e.g., three, we can rewire the network by using
our algorithm instead of resynthesizing the whole network.
In the threshold network of Fig. 1(a), gates n2, n3, n4, and n5
violate this fanin number constraint. First, for gates n3 and n5, we
can extract two new gates, n6 and n7, respectively, using the proposed
gate decomposition method, as shown in Fig. 1(b). For gate n2 in
Fig. 1(b), we can remove target wire h. The rectiﬁcation network n8
is inserted at n2’s transitive fanout cone, as shown in Fig. 1(c). For
gate n4 in Fig. 1(c), we can remove target wire d. The rectiﬁcation
network n10 is inserted at n4’s fanout cone, as shown in Fig. 1(d).
Using our rewiring method, a threshold network with a new
fanin number constraint is obtained. Previous threshold network
synthesis tools resynthesized the network in order to satisfy different
fanin number constraints [17][31]. Our rewiring algorithm, however,
can achieve the same goal by focusing on single gates without
resynthesizing the whole network.
III. PRELIMINARIES
The section introduces deﬁnitions and some characteristics about
threshold logic.
A. Threshold logic
A linear threshold gate (LTG) is an n binary inputs and one
binary output function. The parameters of an LTG are weights
2This operation is trivial for traditional Boolean networks, but it is not the
case for threshold networks.
2
x1
x2
x3
f
2
1
-1
3
x1
x2
y3
f
2
1
1
y3 = x3’
(a) (b)
Figure 2: (a) An LTG implementing the function f = x1(x2 + x′3).
(b) The same threshold function f = x1(x2+x′3) after applying the
positive-negative weight transformation.
wi; i = 1 ∼ n, which correspond to inputs xi; i = 1 ∼ n,
and a threshold value T . A Boolean logic function is called a
threshold logic function if and only if it can be realized as a single
LTG. Furthermore, a threshold logic function may have many dif-
ferent threshold logic representations that are represented as weight-
threshold vectors 〈w1, w2, . . . , wn;T 〉. A network that is composed
of LTGs is called a threshold network.
The output f of an LTG is evaluated by EQ(1). If the summation
of corresponding weights wi of inputs xi that are assumed to be 1
in an input vector, is greater than or equal to the threshold value T ,
the output f is 1. Otherwise, the output f is 0.
f(x1, x2, . . . , xn) =
⎧⎪⎪⎪⎨
⎪⎪⎪⎩
1 if
n∑
i=1
xiwi ≥ T
0 if
n∑
i=1
xiwi < T
(1)
For example, in Fig. 2(a), the LTG with a weight-threshold vector
〈2, 1,−1; 2〉 generates 1 if 2x1 + x2 − x3 ≥ 2, and generates 0
otherwise.
The weights wi; i = 1 ∼ n associated with corresponding inputs
can be any real, positive, or negative numbers. However, these
weights are usually integers due to technological considerations [9].
In this work, we assume the weights are integers for simplicity. In
the last example, since {x1 = 1, x2 = 1} or {x1 = 1, x3 = 0}
can make the LTG become 1, the Boolean function it represents is
f = x1x2 + x1x
′
3 = x1(x2 + x
′
3). From this example, we can see
that the threshold logic provides a more compact representation than
traditional Boolean logic, with fewer nodes and a shallower depth.
Unateness is an important property of a threshold logic function,
because all threshold logic functions are unate [21]. However, not
all unate functions can be realized as threshold logic functions. If
the weights of an LTG are all positive (negative), the function it
represents is positive (negative) unate.
The rewired threshold network in this work is generated by an
ILP-based approach [31] where each LTG is canonically represented.
Given a unate function, an ILP formulation which describes its
functionality as linear relationships searches the polytope vertices
For example, in Fig. 4, assume that we would like to remove target
wire a from the given LTG. After the grouping and decomposition, as
shown in Fig. 4(b), we remove target wire a. Then the objective gate
consisting of the inputs b, c, and d is useless according to Theorem
1. This is because the summation of 2, 1, 1 in the decomposition gate
〈2, 1, 1; 5〉 after removing a is less than 5.
Deﬁnition 2: An input in a single group LTG is critical if and only
if this LTG will become useless after removing this input.
Theorem 2: Given a single group LTG, an input xj with its
corresponding weight wj is critical if and only if it satisﬁes EQ(3),
where n is the number of inputs in this gate.
n∑
i=1,i =j
wi < T (3)
For example, in Figs. 4(a) and 4(b), input a is critical because
the summation of weights b ∼ d is less than the threshold value.
Similarly, inputs e and f are also critical because removing them
results in empty decomposition gates, and an empty gate is useless
by Deﬁnition 1.
Note that a critical input is important to the uselessness of a
threshold gate. Furthermore, the functionality of a gate strongly
depends on the relationship between the critical input and other
inputs.
Deﬁnition 3: An input is useless if and only if the output of this
LTG is intact when this input toggles under all input combinations.
Theorem 3: Given an input xj with its corresponding weight wj , xj
is useless if and only if it satisﬁes either EQ(4) or EQ(5) for each
input combination, where n is the number of inputs in this gate.
n∑
i=1,i =j
xiwi < T and (
n∑
i=1,i =j
xiwi) + wj < T (4)
(
n∑
i=1,i =j
xiwi) + wj ≥ T and
n∑
i=1,i =j
xiwi ≥ T (5)
For example, in Fig. 4(b), input d will become useless after
removing input c because it satisﬁes either EQ(4) or EQ(5) for all
input combinations of a and b.
An objective gate will have two potential outcomes after we
remove a target wire from it: a useless LTG or a normal LTG. If
the objective gate becomes useless after removing the target wire, a
dramatic functional loss occurs in the overall threshold network. To
prevent the difﬁculty from this situation in our rectiﬁcation scheme,
we modify the threshold value of the objective gate if the objective
gate becomes useless after this removal operation. The details about
this modiﬁcation will be addressed in the next section. Furthermore,
since the removal operation may incidentally create useless inputs,
which are not allowed in a normal LTG, we also remove them. On
the other hand, if the objective gate is still a normal LTG after the
removal operation, we do not change its threshold value.
D. Rectiﬁcation network construction
In this section, we introduce the method of adding rectiﬁcation
networks at other locations to rectify the changed functionality of the
original threshold network due to the target wire removal. Since the
construction of the rectiﬁcation network varies with the characteristics
of the target wire, we analyze the relationship among the target wire
and the other inputs, and divide the correction method into three cases
with respect to the characteristics of a target wire, as seen in the ﬂow
of Fig. 3.
Deﬁnition 4: A single group LTG has a critical-effect if and only if
there exists an assignment such that the output changes from 1 to 0
when each one of its inputs in this assignment changes from 1 to 0.
a
b
c
2
1
1
f3
a b c f
0 0 0 0
0 0 1 0
0 1 0 0
0 1 1 0
1 0 0 0
1 0 1 1
1 1 0 1
1 1 1 1
Figure 5: An LTG and its critical-effect vectors.
Theorem 4: Given a single group LTG, the LTG has a critical-effect
if it satisﬁes EQ(6), where n is the number of inputs in this gate.
n∑
i=1
xiwi = T (6)
An input assignment that satisﬁes the requirement of the critical-
effect for an LTG is called a critical-effect vector. For example, in
Fig. 5, input assignments 100 and 110 are the critical-effect vectors
of LTG 〈2, 1, 1; 3〉. This is because changing any 1 to 0 in these
assignments will also change the output from 1 to 0. Note that EQ(6)
is only a sufﬁcient condition of Theorem 4. However, EQ(6) is also a
necessary consition if the given LTG is obtained from an ILP-based
synthesis algorithm [31]. That means all critical-effect vectors of the
LTG satisfy EQ(6).
Case 1: The target wire is not critical: When the target wire
is not critical, the remaining objective gate after the removal will
not become useless. Thus, we preserve the functional relationship
among the inputs in the remaining objective gate by keeping the
threshold value intact. Since adding the rectiﬁcation network at the
transitive fanin cone of the objective gate will signiﬁcantly affect
the remaining functionality among other inputs, we only add the
rectiﬁcation network at the transitive fanout cone of the objective
gate in this case.
The critical-effect vector mentioned above can be used to further
analyze the functionality among all inputs of an LTG. Hence, we
will use it in this case to construct the rectiﬁcation network in our
algorithm.
Let us ﬁrst clarify the physical meaning of a critical-effect vector.
In Fig. 5, the LTG has two critical-effect vectors 101 and 110. When
considering 101, we ﬁnd that another input assignment 111 also
produces 1 after checking its truth table. This means that changing
the second input b does not change the output value. Thus, if input b
in this LTG is the target wire and has been removed, the remaining
objective gate preserves the subfunction with respect to these two
assignments 101 and 111. On the other hand, when considering
another critical-effect vector 110, we ﬁnd that vector 100 produces a
different output, 0. Thus, if input b in this LTG is the target wire and
has been removed, the remaining objective gate loses a subfuncion
with respect to these two assignments 110 and 100.
In summary, we observe that the loss of a subfunction only occurs
when removing a target input, which is assumed to be 1 in a
critical-effect vector. Thus, to construct the rectiﬁcation network, it is
important and necessary to have information about the critical-effect
vectors whose target input is assumed to be 1.
To search the critical-effect vectors of an LTG, we can exhaustively
build its truth table and then ﬁnd the input assignments satisfying
EQ(6). However, this method is not scalable. Fortunately, thanks to
the output evaluation mechanism in a positive-weight LTG, we can
make this search process practical and efﬁcient by deduction.
The method of rewiring for this case with the aid of the critical-
effect vectors is described as follows. Given a single group LTG and
a target wire xt, we ﬁrst remove any useless inputs after target wire
removal. Second, we get all the critical-effect vectors of the LTG,
where xt is assumed to be 1. Third, we collect all the inputs that are
assumed to be 1 in these critical-effect vectors. Then, the rectiﬁcation
wt after the removal. Second, we construct the rectiﬁcation network
that is xt only. Finally, we connect this rectiﬁcation network to each
input, respectively, in the remaining objective gate with an AND gate
at the transitive fanin cones.
For example, given an LTG and the target wire a in Fig. 7(a), the
threshold value is reduced to 4 from 10 by decreasing the weight of
a after the removal, as shown in Fig. 7(b). The rectiﬁcation network
is a only. Then, we connect this rectiﬁcation network to each input in
Fig. 7(b) using an AND gate, at the transitive fanin cones, as shown
in Fig. 7(e).
The validity of this rectiﬁcation can be explained in a similar
manner as in Case 2. When xt is a critical input, the output of the
objective gate under an input assignment that has the xt assumed
to be 0 produces 0. Thus, after we AND xt to each input in the
remaining objective gate, the resultant network produces 0 under the
vectors in the subspace of xt = 0. When xt = 1, the resultant
network will act as the original gate with the setting xt = 1.
For the same example in Figs. 7(a) and 7(e), when a = 0, the
functionality in Fig. 7(a) is the same as that in Fig. 7(e). When a = 1,
Figs. 7(a) and 7(e) also get the same result.
V. SIMPLIFICATION
After target wire removal and rectiﬁcation network construction,
the appearances of some LTGs in the threshold network may be
changed such that they cannot be canonically represented. Thus, in
this section, we introduce a simpliﬁcation procedure that transforms
a single group LTG to its canonical representation.
For example, given two LTGs 〈2, 1; 3〉 and 〈1, 1; 2〉, we recognize
that both LTGs represent the same function f(a, b) = ab, because
they both output 1 only at {a = 1, b = 1}. Since minimal weights
and threshold value reduce the implementation cost of an LTG, it
is desirable to minimize their values in an LTG [26]. An LTG is
canonical if and only if it represents a function using minimal weights
and threshold value. An LTG generated from the ILP-based synthesis
method [31] is also canonical.
Next, we describe the simpliﬁcation procedure as follows. First, a
larger-than-1 common divisor divides the weights and the threshold
value to get a more minimized representation if it exists. For example
in Fig. 8(a), given an LTG 〈4, 4, 6, 8; 18〉, a common divisor 2 divides
the weights and the threshold value, as shown in Fig. 8(b).
Then, the weights and the threshold value of an LTG are gradu-
ally decreased while keeping the functionality intact. A decrement
changes the functionality of an LTG is not allowed. To check if the
functionality changes or not after a decrement in the weight and the
threshold value, we only examine signiﬁcant vectors that can exactly
express the complete functionality of this LTG, rather than examine
the whole truth table of this LTG. We will further discuss this idea
later in the paper.
Next, we explain the method of decreasing weights and threshold
value. If we decrease a unique weight by 1 in an LTG, the threshold
value is decreased by 1 as well. However, we must simultaneously
decrease the weights of inputs that have the same weight by 1
owing to their symmetrical property. That is, if the weights of these
symmetric inputs become different after the decrement, the new
representation is nonequivalent to the original one. Additionally, the
corresponding threshold value is decreased by the number of 1 in
these same-weight inputs of any critical-effect vector.
For example, in Fig. 8(b), the critical-effect vectors of the LTG
〈2, 2, 3, 4; 9〉 are 0111 and 1011. Inputs a and b have the same weight
2, and the number of 1 in inputs a and b of the critical-effect vectors
is 1. Thus, the weights of a and b are both decreased from 2 to 1,
and these threshold value is decreased from 9 to 8, as shown in Fig.
8(c). The weight-decreasing operation is sequentially conducted and
checked until each weight reaches 1.
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 0
1 1 1 0 0
4 
4
6
8
a
b
c
d
f18
(a)
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 0
1 1 1 0 0
9
2 
2
3
4
a
b
c
d
f
(b)
8
1 
1
3
4
a
b
c
d
f
Decrease the weights in 
inputs a, b
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 0
1 1 1 0 0
(c)
7
1 
1
2
4
a
b
c
d
f
Decrease the weight in 
input c
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 0
1 1 1 0 0
(d)
6
1 
1
2
3
a
b
c
d
f
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 0
1 1 1 0 0
Decrease the weight in 
input d
(e)
5
1 
1
1
3
a
b
c
d
f
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 1
1 1 1 0 0
Decrease the weight in 
input c
(f)
5
1 
1
2
2
a
b
c
d
f
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 0
1 1 1 0 0
Decrease the weight in 
input d
(g)
3
1 
1
1
1
a
b
c
d
f
a b c d f
0 1 1 1 1
1 0 1 1 1
1 1 0 1 1
1 1 1 0 1
Decrease the weights in 
inputs c, d
(h)
Figure 8: The simpliﬁcation procedure for an LTG.
After each weight-decreasing operation, to verify if the function-
ality between the original LTG and the new LTG is intact or not is
necessary. First, some terminology used in our functionality checking
is introduced. A subvector of a vector is a vector whose input assumed
to be 1 is the proper subset of this vector. A supervector of a vector
is a vector whose input assumed to be 1 is the proper superset of this
vector. A brothervector of a vector is a vector which has the same
number of inputs assumed to be 1 as this vector.
For example, given a vector 011, its subvectors are 000, 001, and
010. Its supervector is 111. Its brothervectors are 101 and 110. To
determine if the functionality of an LTG after a weight-decreasing
operation is equivalent to the original LTG or not, we propose the
method introduced in Theorem 5.
Theorem 5: Given two single group LTGs, they are functionally
equivalent if and only if they produce the same outputs under all
critical-effect vectors and the brothervectors of all critical-effect
vectors.
Due to the special output evaluation mechanism of a positive-
weight LTG, an LTG under the subvector of a critical-effect vector
will output zero. Similarly, an LTG under the supervector of a
critical-effect vector will output 1. Thus, if the critical-effect vectors
for two LTGs are the same, their subvectors and supervectors will
have the same outputs. However, the output of an LTG under
the brothervectors of a critical-effect vector cannot be deduced by
the output of the critical-effect vector. Thus, we also simulate the
brothervectors of the critical-effect vectors. As a result, the outputs
of the whole input space of an LTG can be derived by using the
critical-effect vectors and their brothervectors.
In summary, after a weight-decreasing operation, if the outputs of
new LTG under the critical-effect vectors and their brothervectors are
the same as that of the original LTG, the functionality of the new
LTG is intact by Theorem 5.
Now we use the same example to demonstrate this simpliﬁcation
procedure. For the LTG 〈2, 2, 3, 4; 9〉, we ﬁrst get its critical-effect
vectors, 0111 and 1011, as highlighted, and their brothervectors. The
output values under these assignments are shown in Fig. 8(b).
For each iteration of weight-decreasing operation, we decrease
each input weight and the threshold value sequentially. Inputs a and
b have the same weight; therefore, they are simultaneously decreased.
The threshold value decrement for this situation has been addressed in
the previous paragraph. Next, we check the validity of this decrement
by comparing the outputs under these assignments, as shown in Fig.
8(c). Since these outputs are the same, this decrement is valid and
a new representation 〈1, 1, 3, 4; 8〉 is obtained. Next, the weight-
decreasing operation for input c is shown in Fig. 8(d). After checking
of CPU time reduction. For example, the b20 benchmark has 4431
gates and 14020 wires. [31] requires 364.52 seconds to resynthesize
the whole threshold network for meeting this new fanin number
constraint, while our approach only costs 58.22 seconds to reach
the same objective. The CPU time reduction is 84.0%.
According to Table II, our approach spent less CPU time, with
a ratio of 63.3% reduction, compared to [31], in a benchmark on
average. Furthermore, our approach is 7.1 times faster than [31].
This CPU time reduction increases with the growth of circuit size
due to local, instead of global, resynthesis in our approach.
Table II
THE COMPARISON WITH THE-STATE-OF-THE-ART [31] FOR RESYNTHESIS.
benchmark |gate| |wire| |rewiring| [31] ours impr.time(s) time(s) %
i2c 176 769 28 8.60 3.24 62.3
usb_phy 280 937 55 2.54 1.67 34.3
simple_spi 288 840 37 2.48 1.84 25.8
pci_spoci_ctrl 385 905 43 2.17 2.08 4.1
alu4 410 1407 50 1.73 1.68 28.9
s9234 554 1830 74 5.62 2.85 49.3
C3540 731 1688 101 3.16 2.68 15.2
dalu 810 2579 52 5.06 1.73 65.8
s13207 848 2235 46 2.92 2.32 20.5
C5315 879 2804 192 21.50 9.46 56.0
C6288 970 3485 155 10.65 7.33 31.2
rot 980 2878 121 11.60 4.80 58.6
C7552 1066 3866 110 23.00 3.76 83.7
tv80 1189 3485 349 24.12 14.69 39.1
spi 1646 4703 663 67.85 25.22 62.8
i10 1814 5893 130 54.50 6.37 88.3
systemcdes 1907 4766 474 127.80 22.31 82.5
des 1920 5180 420 83.70 16.75 79.9
aes_core 3417 13622 792 402.60 38.73 90.4
mem_ctrl 3455 14655 1031 210.56 36.32 83.9
s38417 4280 20139 941 142.20 31.22 78.0
b20 4431 14020 1463 364.52 58.22 84.0
ac97_ctrl 5732 17906 1330 288.87 46.33 83.9
b21 5844 13481 1575 177.85 51.86 70.8
usb_funct 6612 21613 1405 293.26 42.13 85.6
systemcaes 6885 22674 1163 286.40 38.73 86.5
s38584 6897 27750 1981 525.72 83.74 84.1
b22 7656 32711 1595 320.04 52.04 83.7
pci_bridge32 8344 29640 1480 355.52 46.12 87.0
b17 13460 39007 2216 941.67 102.12 89.2
wb_conmax 15719 47731 2544 1386.34 108.24 92.2
total 6154.55 866.58
average 198.53 27.95 63.3
ratio 7.1 1
VII. CONCLUSION AND FUTURE WORK
This paper proposes a new rewiring technique for threshold net-
works. It works through the process of ﬁrst removing a target wire
and then correcting the functionality of the threshold network by
adding its corresponding rectiﬁcation network with respect to the
characteristics of a target wire. It efﬁciently provides the capability
of logic restructuring. A simpliﬁcation procedure for canonicity that is
directly applied to a single LTG is also proposed. When the threshold
logic becomes a mainstream in the research of VLSI circuits, the
contributions of this work will facilitate the applications of logic
synthesis, veriﬁcation, and various optimization goals.
REFERENCES
[1] M. J. Avedillo and J. M. Quintanaa, “A Threshold Logic Synthesis Tool for RTD
Circuits,” in Proc. European Symp. on Digital System Design, Sep. 2004, pp. 624-
627.
[2] M. J. Avedillo, J. M. Quintana, H. Pettenghi, P. M. Kelly, and C. J. Thompson,
“Multi-Threshold Threshold Logic Circuit Design Using Resonant Tunnelling
Devices,” Electron. Lett., vol. 39, no. 21, Oct. 2003, pp 1502-1504.
[3] M. J. Avedillo, J. M. Quintana, A. Rueda, and E. JimCnez, “Low-Power CMOS
Threshold-Logic Gate,” Electron. Lett., vol. 31, no. 35, Dec. 1995, pp. 2157-2159.
[4] V. Beiu, J. M. Quintana, and M. J. Avedillo, “VLSI Implementations of Threshold
Logic-a Comprehensive Survey,” in Tutorial at Int. Joint Conf. Neural Networks,
2003.
[5] Berkeley Logic Synthesis and Veriﬁcation Group, “SIS: Synthesis
of both synchronous and asynchronous sequential circuits,”
http://embedded.eecs.berkeley.edu/pubs/downloads/sis
[6] P. Celinski, J. F. Lopez, S. Al-Sarawi, and D. Abbott, “Low Power, High Speed,
Charge Recycling CMOS Threshold Logic Gate,” Electron. Lett., vol. 37, Aug.
2001, pp. 1067-1069.
[7] S.-C. Chang, K.-T. Cheng, N.-S Woo, and M. Marek-Sadowska, “Postlayout Logic
Restructuring Using Alternative Wires,” IEEE Trans. Computer-Aided Design, vol.
16, pp. 587-596, June 1997.
[8] S.-C. Chang, M. Marek-Sadowska, and K.-T. Cheng, “Perturb and Simplify: Multi-
level Boolean Network Optimizer,” IEEE Trans. Computer-Aided Design, vol. 15,
pp. 1494-1504, Dec. 1996.
[9] K. J. Chen, K. Maezawa, and M. Yamamoto, “InP-Based High-Performance
Monostable-Bistable Transition Logic Elements (MOBILE’s) Using Integrated
Multiple-Input Resonant-Tunneling Devices,” IEEE Eletron Device Letters, vol.
17, pp.127-129, Mar. 1996.
[10] Y.-C. Chen, S. Eachempati, C.-Y. Wang, S. Datta, Y. Xie, and V. Narayanan,
“Automated Mapping for Reconﬁgurable Single-Electron Transistor Arrays,” IEEE
Design Automation Conf., 2011, pp. 878-883.
[11] Y.-C. Chen and C.-Y. Wang, “An Improved Approach for Alternative Wires
Identiﬁcation,” in Proc. Int. Conf. Computer Design, 2005, pp. 711-716.
[12] Y.-C. Chen and C.-Y. Wang, “Fast Detection of Node Mergers Using Logic
Implications,” in Proc. Int. Conf. on Computer-Aided Design, 2009, pp. 785-788.
[13] Y.-C. Chen and C.-Y. Wang, “Node Addition and Removal in the Presence of
Don’t Cares,” in Proc. Design Automation Conf., 2010, pp. 505-510.
[14] Y.-C. Chen and C.-Y. Wang, “Fast Node Merging With Don’t Cares Using Logic
Implications,” IEEE Trans. Computer-Aided Design, vol. 29, pp. 1827-1832, Nov.
2010.
[15] M. L. Dertouzos, “Threshold Logic: A Synthesis Approach”. Cambridge, MA:
M.I.T. Press, 1965.
[16] D. Goldharber-Gordon, M. S. Montemerlo, J. C. Love, G. J. Opiteck, and J. C.
Ellenbogen. “Overview of Nanoelectronic Devices,” in Proc IEEE, vol. 85, no. 4,
pp. 521-540, Jan. 1997.
[17] T. Gowda and S. Vrudhula, “Decomposition Based Approach for Synthesis of
Multi-Level Threshold Logic Circuits,” in Proc. Asia and South Paciﬁc Design
Automation Conf., 2008, pp. 125-130.
[18] T. Gowda, S. Vrudhula, and G. Konjevod, “A Non-ILP Based Threshold Logic
Synthesis Methodology,” in Proc. International Workshop on Logic and Synthesis,
2007, pp. 222-229.
[19] T. Gowda, S. Vrudhula, and G. Konjevod, “Combinational Equivalence Checking
for Threshold Logic Circuits,” in Proc. Great Lake Symp. VLSI, March 2007, pp.
102-107.
[20] P. Gupta, R. Zhang, and N. K. Jha, “Automatic Test Generation for Combina-
tional Threshold Logic Networks,” IEEE Trans. Computer-Aided Design, vol. 16,
pp.1035-1045, Aug. 2008.
[21] Z. Kohavi, “Switching and Finite Automata Theory”. New York, NY: McGraw-
Hill, 1978.
[22] C. Lageweg, S. Cotofana, and S. Vassiliadis, “A Linear Threshold Gate Implemen-
tation in Single Electron Technology,” in Proc. IEEE Coput. Soc. Workshop VLSI,
2001, pp. 93-98.
[23] C.-C. Lin and C.-Y. Wang, “Rewiring Using IRredundancy Removal and Addition,”
in Proc. Design, Automation and Test in Europe, 2009, pp. 324-327.
[24] S. Muroga, “Threshold Logic and its Applications”. New York, NY: John Wiley,
1971.
[25] A. Mishchenko, S. Chatterjee, and R. Brayton, “DAG-Aware AIG Rewriting: A
Fresh Look at Combinational Logic Synthesis,” in Proc. Design Automation Conf.,
2006, pp. 532-536.
[26] K. Maezawa, H. Matsuzaki, M. Yamamoto, and T. Otsuji, “High-Speed and Low-
Power Operation of A Resonant Tunneling Logic Gate MOBILE,” IEEE Eletron
Device Letters, vol. 19, pp.80-82, March 1998.
[27] C. Pacha, P. Glosekotter, K. Goser, W. Prost, U. Auer, and F. Tegude, “Reso-
nant Tunneling Device Logic Circuit,” Dortmund/Gerhard-Mercator University of
Duisburg, Germany, Tech. Rep., July 1999.
[28] M. Perkowski and A. Mishchenko, “Logic Synthesis for Regular Fabric Realized
in Quantum Dot Cellular Automata,” in Proc. Int. J. Multiple-Valued Logic and
Soft Comput., 2004, pp. 768-773.
[29] G. E. Sobelman and K.Fant, “CMOS Circuit Design of Threshold Gates with
Hysteresis,” in Proc. Int. Conf. on Circuits and Systems, vol. 2, 1998, pp. 61-64.
[30] R. O. Winder, “Threshold Logic.” Ph.D. dissertation, Princeton University, Prince-
ton, NJ, 1962.
[31] R. Zhang, P. Gupta, L. Zhong, and N. K. Jha, “Synthesis and Optimization of
Threshold Logic Networks with Application to Nanotechnologies,” in Proc. Design
Automation Test in Europe Conf., 2004, pp. 904-909.
[32] Y. Zheng, M. S. Hsiao, and C. Huang, “SAT-based Equivalence Checking of
Threshold Logic Designs for Nanotechnologies,” in Proc. Great Lake Symp. VLSI,
May 2008, pp. 225-230.
[33] http://iwls.org/iwls2005/benchmarks.html
1828 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 29, NO. 11, NOVEMBER 2010
a k-bounded depth to extract local ODCs. With larger values
of k, the method can find more node mergers but spends more
central processing unit (CPU) time.
The work in [11] proposed a fast simulator that computes
global but approximate ODCs from any depth. Although this
method cannot identify the complete set of node mergers,
it finds an average of 25% more node mergers compared
to the local ODC-based method with k = 5 [13]. Further-
more, the work in [4] extended the node-merging technique
for sequential circuit optimization by considering sequential
ODCs.
Although both the works in [13] and [11] proposed methods
to decrease the complexity of observability computation, they
cannot avoid a potentially large number of SAT solving calls.
Thus, in this paper, we propose a non-SAT-based method for
detecting node mergers. Given a target node, we compute the
mandatory assignments for the stuck-at 0 and 1 fault tests on
it using logic implications. Then, we can derive its substitute
nodes directly from the mandatory assignments without trial-
and-error checking. To achieve high efficiency, we do not
compute all mandatory assignments due to the exponential
time complexity, and thus, spend some quality. For circuit size
reduction, we extend the method with three techniques—wire
replacement, redundancy removal, and mandatory assignment
reuse—that can enhance its performance.
We conduct experiments on a set of IWLS 2005 benchmarks
[14]. The experimental results show that the proposed method
can quickly complement the local ODC-based method with
k = 5 by finding additional node mergers. Additionally, for
circuit size reduction, the proposed method has a speedup of
46 times for overall benchmarks while possessing a competi-
tive capability compared to the state-of-the-art method [11].
The rest of this paper is organized as follows. Section II
uses an example to demonstrate ODC-based node merging.
It also reviews the related concepts in very-large-scale inte-
gration (VLSI) testing used in this paper. Sections III and IV
present the proposed method for finding node mergers and its
application for circuit minimization. Finally, the experimental
results and conclusion are presented in Sections V and VI.
II. Preliminaries
A. Example
We use an example in Fig. 1 to demonstrate ODC-based
node merging. The circuit in Fig. 1(a) is presented by using
an and-inverter graph (AIG). Here, a, b, c, and d are primary
inputs (PIs). v1–v5 are 2-input and gates. Their connectivities
are presented by directed edges. A dot marked on an edge
indicates that an inverter inv is in between two nodes. In this
circuit, v1 and v3 are not functionally equivalent. Merging
them potentially affects the overall functionality. However,
their values only differ when d = 1 and b = c. Since b = c
further implies v2 = 0, which is an input-controlling value of
v5, the different values of v3 with respect to v1 are prevented
from being observed. Thus, v3 can be correctly replaced with
v1. The resultant circuit is shown in Fig. 1(b).
In this paper, a node to be replaced is referred to as a target
node and a node that can correctly replace a target node is
called a substitute node of the target node.
Fig. 1. Example of ODC-based node merging. (a) Original circuit.
(b) Resultant circuit of replacing v3 with v1.
For ease of discussion, we only consider circuits presented
as AIGs. Circuits having complex gates can also be handled
by first transforming them into AIGs.
B. Background
An input of a gate g has an input-controlling value of
g if this value determines the output value of g regardless
of the other inputs. The inverse of the input-controlling
value is called the input-noncontrolling value, e.g., the
input-controlling value of an and gate is 0 and its input-
noncontrolling value is 1. A gate g is in the transitive fanout
cone of a gate gs if there exists a path from gs to g.
The dominators [6] of a gate g (or a wire w) are a set of
gates G such that all paths from g (or w) to any PO have to
pass through all gates in G. Consider the dominators of a gate
g (or a wire w): the side inputs of a dominator are its inputs
that are not in the transitive fanout cone of g (or w).
In VLSI testing, a stuck-at fault is a fault model used to
represent a manufacturing defect within a circuit. The effect
of the fault is as if the faulty wire or gate were stuck at either 1
(stuck-at 1) or 0 (stuck-at 0). A stuck-at fault test is a process
for finding a test that can generate different output values in
the fault-free and the faulty circuits. Given a stuck-at fault f ,
if there exists such a test, f is said to be testable; otherwise,
f is untestable. To make a stuck-at fault on a wire w testable,
a test needs to activate and propagate the fault effect to a PO.
In a combinational circuit, an untestable stuck-at fault on a
wire indicates that the wire is redundant and can be replaced
with a constant value 0 or 1. Similarly, if a stuck-at fault on a
gate is untestable, all the wires led by the gate can be replaced
with a constant value.
The mandatory assignments (MAs) are the unique value
assignments to nodes necessary for a test to exist. Consider
a stuck-at fault on a wire w; the assignments obtained by
setting w to the fault-activating value and by setting the side
inputs of dominators of w to the fault-propagating values are
MAs. Then, these assignments can be propagated forward and
backward to infer additional MAs using logic implications.
Recursive learning [8], a learning technique in automatic test
pattern generation (ATPG), whose complexity is exponential
to the recursion depth can be used to infer more MAs. If the
MAs of the fault are inconsistent, the fault is untestable, and
therefore, w is redundant [12].
For convenience, in the rest of this paper, we use MAs(m =
sav) to denote the set of MAs for the stuck-at v fault test on m,
where m can be a node or a wire and v is a logical value 0 or 1.
1830 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 29, NO. 11, NOVEMBER 2010
pattern can simultaneously generate different values for ns and
nt , and make the value of nt observable through w(nt → ni).
Similarly, Condition 2 can be modified by reversing the value
of ns to find complemented substitute wires that replace a
target wire together with an additional inv.
Let us review the example in Fig. 2(a). The MAs for the
stuck-at 0 fault test on w(v1 → v5) are {v1 = 1, a = 0, b = 1,
c = 1, v2 = 1, v6 = 1} and the MAs for the stuck-at 1 fault test
on w(v1 → v5) are {v1 = 0, v6 = 0, c = 1}. Thus, w(v1 → v5)
can be replaced with w(v6 → v5). Next, using the same
method in Fig. 2(b), we can find that w(v1 → v6) can be
replaced with w(a → v6) together with an inv, as shown in
Fig. 2(c). Finally, we can remove v1 because it does not lead
any wires.
B. Redundancy Removal
As mentioned in Section II-B, if the MAs of a stuck-at
fault test on a node are inconsistent, the fault is untestable
and the node is redundant. Our method for substitute node
identification computes the MAs for the stuck-at 0 and stuck-
at 1 fault tests on a target node nt . Once we find that the MAs
are inconsistent, we can replace nt with a constant value 0 or
1 depending on the fault value. Thus, we can simultaneously
detect if nt is redundant without extra effort. Similarly, we use
the same method to detect and remove redundant wires in the
wire replacement technique.
C. MA Reuse
MA reuse is a method to reuse the computed MAs during
circuit optimization. According to the concept of fault collaps-
ing [2], which states that two equivalent stuck-at faults have
the same test set, some stuck-at fault tests on different nodes
can have the same MA set. Thus, we can reuse these MAs
when optimizing a circuit.
Let us consider computing the MAs for the stuck-at fault
tests on a node n with MA reuse. Suppose ni is a fanout node
of n and MAs(ni = sa0) has been computed. If there exists no
inv between n and ni, we can directly set MAs(w(n → ni) =
sa0) to MAs(ni = sa0) rather than re-compute the same MA
set. Additionally, if n only leads ni, we can set MAs(n = sa0)
to MAs(ni = sa0). However, if there exists an inv between n
and ni, we can set MAs(w(n → ni) = sa1) to MAs(ni = sa0),
and set MAs(n = sa1) to MAs(ni = sa0) when n only leads
ni. For each node, only the MAs of its stuck-at 0 fault test
can be reused.
D. Overall Algorithm
In circuit optimization, although the optimization orders of
selecting a target node, a substitute node, and a substitute wire
can affect the optimization results, it is difficult and might be
time-consuming to evaluate which replacement is best. Thus,
we use a heuristic optimization order in this paper. Each node
in a circuit is selected as a target node nt in the depth-first
search (DFS) order from POs to PIs, and it is replaced with
the substitute node that is closest to PIs.
However, if nt is determined having no substitute node, we
then perform wire replacement on the wires led by it. Each
wire led by nt is sequentially selected as a target wire and we
Fig. 3. Overall algorithm for circuit size reduction.
replace it with its substitute wire that is closest to PIs once
we find it. Since our objective is to remove nt , if one target
wire cannot be replaced, nt is non-removable, and thus, we
stop performing wire replacement on the remaining wires.
Fig. 3 shows the overall algorithm for circuit size reduction.
Given a circuit C, the algorithm iteratively selects a target node
nt in the DFS order from POs to PIs. At each iteration, from
steps 1–4, the algorithm finds the substitute nodes of nt by
computing MAs(nt = sa0) and MAs(nt = sa1), and replaces nt
with the substitute node that is closest to PIs. If nt is replaced,
the algorithm continues to consider the next target node.
However, if nt has no substitute node, the algorithm starts
performing wire replacement on the wires led by nt in step 5.
Similarly, for each target wire w(nt → ni), the algorithm finds
its substitute wires by computing MAs(w(nt → ni) = sa0)
and MAs(w(nt → ni) = sa1), and replaces it with the
substitute wire that is closest to PIs. If w(nt → ni) is replaced,
the algorithm continues to consider the next target wire.
Otherwise, the algorithm stops performing wire replacement
and returns to consider the next target node.
V. Experimental Results
We implemented our algorithms in C language within an
ABC [3] environment. The experiments were conducted on
a 3.0 GHz Linux platform (CentOS 4.6). The benchmarks
are from the IWLS 2005 suite [14]. Each benchmark is
initially transformed to an AIG format, and we only consider
its combinational portion. Additionally, the recursive learning
technique [8] is applied with the recursion depth 1 in our
algorithms. The experiments consist of two parts. The first
one is to show the efficiency and effectiveness of our method
for finding substitute nodes. The second one is to show the
capability of our method for circuit size reduction.
1832 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 29, NO. 11, NOVEMBER 2010
Table II summarizes the experimental results. Columns 1
and 2 list the benchmarks and the number of nodes in each
benchmark represented by AIG, respectively. Columns 3, 5,
and 7 list the percentage of circuit size reduction in terms
of node count achieved by our method without redundancy
removal, our method without wire replacement, and our overall
method, respectively. Columns 4, 6, and 8 list the CPU time
measured in seconds. Columns 9 and 10 list the corresponding
results reported in [11]. The maximal CPU time in Column
10 is 5000 s, which is the CPU time limit set by the work.
Table II shows that although the efficiency benefits of
our method come at the expense of the optimization quality,
our method can work together with the redundancy removal
and the wire replacement techniques to have a competitive
capability with a speedup of 46.3 times, compared to the
global ODC-based method.
VI. Conclusion
In this paper, we proposed an ATPG-based method for
node merging by using logic implications. The method only
requires two MA computations to find the substitute nodes of
a target node. The process that previously required many SAT
solving calls is thus reduced to be achievable in practically
linear time. The experimental results showed that the proposed
node-merging method can complement the local ODC-based
method with the bounded depth k = 5 by finding additional
node mergers. Moreover, we extend the node-merging method
with three techniques, wire replacement, redundancy removal,
and MA reuse, for circuit optimization. The experimental
results showed that the proposed optimization algorithm has
a competitive capability and expends much less CPU time
compared to the state-of-the-art method.
References
[1] M. S. Abadir, J. Ferguson, and T. E. Kirkland, “Logic design verification
via test generation,” IEEE Trans. Comput.-Aided Design, vol. 7, no. 1,
pp. 138–148, Jan. 1988.
[2] M. Abramovici, M. A. Breuer, and A. D. Friedman, Digital Systems
Testing and Design for Testability. Piscataway, NJ: IEEE Press, 1990.
[3] Berkeley Logic Synthesis and Verification Group. ABC: A Sys-
tem for Sequential Synthesis and Verification [Online]. Available:
http://www.eecs.berkeley.edu/∼alanmi/abc
[4] M. Case, V. Kravets, A. Mishchenko, and R. Brayton, “Merging nodes
under sequential observability,” in Proc. Des. Autom. Conf., 2008,
pp. 540–545.
[5] Y.-C. Chen and C.-Y. Wang, “Fast detection of node mergers using logic
implications,” in Proc. Int. Conf. Comput.-Aided Design, 2009, pp. 785–
788.
[6] T. Kirkland and M. R. Mercer, “A topological search algorithm for
ATPG,” in Proc. Des. Autom. Conf., 1987, pp. 502–508.
[7] A. Kuehlmann, “Dynamic transition relation simplification for bounded
property checking,” in Proc. Int. Conf. Comput.-Aided Design, 2004,
pp. 50–57.
[8] W. Kunz and D. K. Pradhan, “Recursive learning: A new implication
technique for efficient solutions to CAD problems—test, verification,
and optimization,” IEEE Trans. Comput.-Aided Design, vol. 13, no. 9,
pp. 1143–1158, Sep. 1994.
[9] A. Mishchenko, S. Chatterjee, and R. Brayton, “DAG-aware AIG rewrit-
ing: A fresh look at combinational logic synthesis,” in Proc. Design
Autom. Conf., 2006, pp. 532–536.
[10] A. Mishchenko, S. Chatterjee, R. Brayton, and N. Een, “Improvements
to combinational equivalence checking,” in Proc. Int. Conf. Comput.-
Aided Design, 2006, pp. 836–843.
[11] S. M. Plaza, K.-H. Chang, I. L. Markov, and V. Bertacco, “Node mergers
in the presence of don’t cares,” in Proc. Asia South Pacific Des. Autom.
Conf., 2007, pp. 414–419.
[12] M. H. Schulz and E. Auth, “Advanced automatic test pattern generation
and redundancy identification techniques,” in Proc. Int. Fault-Tolerant
Comput. Symp., 1988, pp. 30–35.
[13] Q. Zhu, N. Kitchen, A. Kuehlmann, and A. Sangiovanni-Vincentelli,
“SAT sweeping with local observability don’t cares,” in Proc. Design
Autom. Conf., 2006, pp. 229–234.
[14] IWLS. (2005) [Online]. Available: http://iwls.org/iwls2005/benchmarks.
html
On Undetectable Faults and Fault Diagnosis
Irith Pomeranz and Sudhakar M. Reddy
Abstract—The presence of an undetectable fault ui may modify the
response of a detectable fault dj to a test set used for fault diagnosis.
This may impact the accuracy of fault diagnosis based on the responses
of single faults. Many state-of-the-art diagnosis processes are based on
the responses of single stuck-at faults even though their goal is to
diagnose defects (including multiple defects) that are different from stuck-
at faults. Therefore, we study the effects of undetectable single stuck-
at faults on the accuracy of fault diagnosis based on the responses of
single stuck-at faults. For this purpose, we consider the cases where
the response of a double stuck-at fault ui&dj , which consists of an
undetectable fault ui and a detectable fault dj , is different from the
response of the single fault dj . We show that there are significant, yet
manageable, numbers of such faults in benchmark circuits under test
sets used for fault diagnosis. In all these cases, a fault diagnosis process
based on single stuck-at faults may not identify the locations of dj and
ui as candidate defect sites if a defect affects the sites of dj and ui. We
conclude that it is important to consider ui&dj during fault diagnosis
in order not to preclude the sites of dj and ui as candidate defect sites.
Index Terms—Diagnostic test generation, fault diagnosis, full-scan
circuits, stuck-at faults.
I. Introduction
Fault diagnosis is a process that identifies the likely loca-
tions of defects present in a chip after the chip produces a
faulty output response to a test set. Conceptually, the fault
diagnosis process compares the observed response of the chip
to the responses expected in the presence of modeled faults.
The defect is assumed to be present at one of the sites of the
modeled faults that best match the observed response. These
sites are referred to as candidate defect sites, and the faults are
referred to as candidate faults. For the results to be considered
accurate, the site of a candidate fault needs to point correctly
to the site of the defect.
Several fault diagnosis procedures [1]–[9] use single stuck-
at faults as a basis for diagnosis, noting that responses of com-
mon defects to test sets used for diagnosis can be represented
as deviations from the responses of single stuck-at faults. For
Manuscript received February 7, 2010; revised April 17, 2010. Date of
current version October 20, 2010. The work of I. Pomeranz and S. M. Reddy
was supported in part by Semiconductor Research Corporation, under Grants
2007-TJ-1643 and 2007-TJ-1642. This paper was recommended by Associate
Editor C.-W. Wu.
I. Pomeranz is with the School of Electrical and Computer Engineer-
ing, Purdue University, West Lafayette, IN 47907 USA (e-mail: pomer-
anz@ecn.purdue.edu).
S. M. Reddy is with the Department of Electrical and Computer
Engineering, University of Iowa, Iowa City, IA 52242 USA (e-mail:
reddy@engineering.uiowa.edu).
Digital Object Identifier 10.1109/TCAD.2010.2053476
0278-0070/$26.00 c© 2010 IEEE
IE
EE
 P
ro
of
2 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
reusing the logic implication results such that the number of
required logic implications can be saved.
We conduct experiments on a set of IWLS 2005 benchmarks
[38] and compare to the node-merging approaches in [14],
[28], and [37]. For replaceable node identification, as com-
pared to our prior ATPG-based approach [14], an average of
28% more nodes can be identified replaceable in a benchmark
circuit by using NAR. Additionally, an average of 72% of
replaceable nodes cannot be found by the SAT-based node-
merging approach with a bounded depth k = 5 [37], and thus,
the proposed approach could complement it.
For circuit size reduction, the proposed approach reduces
2873 more nodes than our prior ATPG-based node-merging
approach [14] for all the benchmarks, with an overall CPU
time overhead of only 4 min. Additionally, the optimization
capability is competitive with that of the SAT-based node-
merging approach [28], which is highly time-consuming.
Moreover, we apply the proposed method to SAT-based
bounded sequential equivalence checking (BSEC) [8], [30],
[33]. Our method is used as a preprocess to reduce the
computation complexity of SAT solving. Not only the
variable count that the SAT solver deals with is minimized
due to logic optimization but also the relationships among
the variables become tighter by logic restructuring. Thus, the
process of SAT solving could be facilitated. The effectiveness
of the proposed method is compared with a logic optimization
technique resyn2 [26]. The experimental results show that
when we use resyn2 to facilitate SAT-based BSEC, a
total of approximately 25 h are saved for verifying all the
benchmarks. However, when we integrate the proposed
method with resyn2, we can save approximately 39 h.
A related work studying SAT-controlled RAR rather than
NAR is presented in [32]. The work proposes a SAT-based
method for finding redundancies to replace a target wire.
Although the work aims to replace a wire rather than a node,
it also introduces a similar sufficient condition for identifying
an added node to replace a wire. However, the objective of our
paper is different from that of [32]. Additionally, our method
is an ATPG-based approach and is more complete, since we
consider a total of eight types of added nodes. Moreover,
we also propose an efficient NAR-based algorithm for circuit
optimization and SAT-based BSEC facilitation.
The remainder of this paper is organized as follows. Sec-
tion II uses an example to demonstrate the NAR technique and
formulates the problem considered in this paper. Section III
reviews the related concepts in very large-scale integrated
(VLSI) testing and our prior ATPG-based node-merging ap-
proach [14]. Section IV presents the proposed algorithm for
NAR. The application of NAR for circuit size reduction is in-
troduced in Section V. Section VI shows the application of the
NAR method on SAT-based BSEC. Finally, the experimental
results and conclusion are presented in Sections VII and VIII.
II. Example of NAR
We use an example in Fig. 1 to demonstrate the
difference between node merging and NAR. For ease
of discussion, the circuits considered in this paper are
Fig. 1. Example for demonstrating node merging and NAR. (a) Original
circuit. (b) Resultant circuit of replacing n5 with n6. (c) Resultant circuit of
adding n8. (d) Resultant circuit of replacing n6 with n8.
presented as and-inverter graphs (AIGs) [22], which
are an efficient and scalable representation for Boolean
networks. Circuits with complex gates can be handled
by transforming them into AIGs first. In the circuit of
Fig. 1(a), a, b, c, and d are primary inputs (PIs). O1 ∼ O4
are POs, and n1 ∼ n8 are 2-input and gates. Their connec-
tivities are presented by directed edges. A dot marked on an
edge indicates that an inverter (inv) is in between two nodes.
First, let us review the node-merging technique. In Fig. 1(a),
n5 and n6 have different functionalities. However, their values
only differ when n2 = 1 and a = c. Because a = c further
implies n1 = 0, which is an input-controlling value of n7,
the value of n5 is prevented from being observed at O1. This
situation makes the different values of n5 with respect to n6
never observed. Thus, n5 can be replaced with n6 without
altering the overall functionality of the circuit. The resultant
circuit is shown in Fig. 1(b). Here, n5 is considered a target
node and n6 is a substitute node of n5.
Next, let us consider n6 in Fig. 1(b). Suppose n6 is a target
node to be replaced. Because n6 does not have any substitute
node, the node-merging technique fails to replace it. However,
we can add a new node into the circuit to replace it. When we
add n8 into the circuit as shown in Fig. 1(c), the functionality
of the circuit is unchanged, because n8 does not drive any
node. Additionally, n8 can correctly replace n6. The resultant
circuit is shown as Fig. 1(d), where n8 drives n7 and O2.
Here, because n2 only drives n6, when n6 is replaced, n2 can
be removed as well. This example demonstrates that a node
which has no substitute node still can be replaced by a newly
added node, and the resultant circuit can be minimized if the
replaced node has one or more single-fanout fanin (SFoFi)
nodes. Thus, the NAR technique can replace a node which
cannot be replaced by the node-merging technique, and can
optimize a circuit as well. Note that although n6 and n8 are
functionally equivalent (n6 = (b∗d)∗ c = (b∗ c)∗ (d ∗ c) = n8)
in this example, it does not indicate that the proposed NAR
IE
EE
 P
ro
of
4 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
A. Sufficient Conditions for NAR
Because an NAR technique performs node replacement as
the node-merging technique, we can exploit Condition 1 to
check if an added node is an added substitute node. For
example, in Fig. 1(c), n6 is a target node and n8 is a node
added into the circuit. We find n8 satisfies Condition 1 that
n8 = 1 and n8 = 0 are MAs for the stuck-at 0 and stuck-at 1
fault tests on n6, respectively. Thus, we can conclude that n8
is an added substitute node for n6.
However, it is not efficient to add all possible nodes into
the circuit first and then exploit Condition 1 to identify
which are substitute nodes for the target node. Thus, we
transform the problem of finding an added substitute node
into finding its two fanin nodes that are originally in the
circuit.
Our objective now becomes finding two nodes such that
the added node driven by them will satisfy Condition 1. For
convenience, let nt denote a target node and na denote an
added node driven by two nodes nf1 and nf2. For ease of
discussion, we first suppose that na is directly driven by
nf1 and nf2 without any inv in between them. That is, the
functionality of na is nf1∧nf2. Next, we present two sufficient
conditions for such na. Finally, we also extend the sufficient
conditions for all eight different types of added nodes. The
first condition is presented in Condition 2.
Condition 2: If both nf1 = 1 and nf2 = 1 are MAs for
the stuck-at 0 fault test on nt , na = 1 is an MA for the same
test as well.
Because na is nf1 ∧ nf2, {nf1 = 1, nf2 = 1} implies na = 1.
Thus, if both nf1 = 1 and nf2 = 1 are MAs, na = 1 must be
an MA as well by logic implication.
When Condition 2 is held, na satisfies one half of
Condition 1 that na = 1 is an MA for the stuck-at 0 fault test
on nt . Thus, if we can further show that na = 0 is an MA
for the stuck-at 1 fault test on nt , we can conclude that na is
an added substitute node of nt . Based on this idea, the next
sufficient condition as presented in Condition 3 is proposed
to make na satisfy the other half of Condition 1. Here, let
imp(A) denote the set of value assignments logically implied
from a set of value assignments A, and MAs(nt = sav) denote
the set of MAs for the stuck-at v fault test on nt , where v is
a logical value 0 or 1.
Condition 3: If nf2 = 0 is a value assignment in
imp((nf1 = 1) ∪ MAs(nt = sa1)), na = 0 is an MA for the
stuck-at 1 fault test on nt .
To determine whether na = 0 is an MA for the stuck-at 1
fault test on nt , we can check if all the input patterns that can
detect the fault generate na = 0. If so, na = 0 is an MA. Let
T denote the set of input patterns that can detect the stuck-at
1 fault on nt . Based on the value of nf1, we classify T into
two subsets: the first one, Tnf1=0, and the second one, Tnf1=1,
which consist of the patterns generating nf1 = 0 and nf1 = 1,
respectively. Because nf1 = 0 implies na = 0, all patterns in
Tnf1=0 generate na = 0.
As for Tnf1=1, because imp((nf1 = 1) ∪ MAs(nt = sa1)) is
the set of unique value assignments that all patterns in Tnf1=1
generate, if nf2 = 0 is a value assignment in imp((nf1 = 1) ∪
Fig. 2. Eight different types of added substitute nodes and their correspond-
ing sufficient conditions. (a) Type 1. (b) Type 2. (c) Type 3. (d) Type 4.
(e) Type 5. (f) Type 6. (g) Type 7. (h) Type 8.
MAs(nt = sa1)), all patterns in Tnf1=1 must generate nf2 = 0,
which implies na = 0. As a result, when Condition 3 is held,
each pattern in T generates na = 0, and na = 0 is an MA for
the stuck-at 1 fault test on nt .
In summary, when Conditions 2 and 3 are held simultane-
ously, na = 1 and na = 0 are MAs for the stuck-at 0 and
stuck-at 1 fault tests on nt , respectively, and na is an added
substitute node of nt .
Note that none of nf1 and nf2 represents a particular fanin
node of na. When one fanin node of na is determined as
nf1, the other fanin node is nf2. Thus, although nf1 = 0 ∈
imp((nf2 = 1) ∪ MAs(nt = sa1)) is also a sufficient condition
for na = 0 to be an MA for the stuck-at 1 fault test on nt , we
do not state it in Condition 3. We ignore it by always selecting
the node having a value 1 as nf1.
B. Types of Added Substitute Nodes
In the last section, we suppose that an added node is directly
driven by two nodes without any inv in between them, and
then derive Conditions 2 and 3. In fact, these conditions can
be modified by reversing the values of nf1, nf2, or the stuck-at
fault for different types of added substitute nodes. We present
eight types of added substitute nodes and their corresponding
sufficient conditions in Fig. 2.
For example, Type 1 is the original added node we consider
before. By reversing the value of nf1 in Conditions 2 and 3,
IE
EE
 P
ro
of
6 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
Fig. 5. Algorithm for computing MAs with MA reuse.
and the optimization process is accelerated. The idea comes
from the concept of fault collapsing [2] that two equivalent
stuck-at faults have the same test set. Based on this concept,
when two stuck-at faults are equivalent, their corresponding
MAs are identical as well. Thus, only one MA computation is
required for them. Here, we simply derive two rules for MA
reuse as shown in Fig. 4.
First, consider computing MAs(nd = sa0) in Fig. 4(a). To
activate the fault effect, nd is set to 1. To propagate the fault
effect, the side inputs of dominators of nd are set to their
corresponding input-noncontrolling values. For simplicity, we
use P to denote these fault-propagating assignments. Then,
MAs(nd = sa0) can be obtained by performing a logic
implication of {nd = 1, P}. They are {nd = 1, n = 1, nj = 1,
imp(P)}. Next, consider computing MAs(n = sa0). n = 1 is
the fault-activating assignment. Because n drives only nd , the
dominators of nd and nd itself are dominators of n. Thus, the
fault-propagating assignments are {nj = 1, P}. MAs(n = sa0)
then can be obtained by performing a logic implication of
{n = 1, nj = 1, P}. They are {nd = 1, n = 1, nj = 1,
imp(P)}, which are identical to MAs(nd = sa0). Thus, when
we compute MAs(n = sa0), we can reuse MAs(nd = sa0).
Based on the same method, we also find that MAs(n = sa1)
equals to MAs(nd = sa0) in Fig. 4(b).
According to these two rules, for each node nd , only
MAs(nd = sa0) could be reused. Additionally, it is reused
when nd has a fanin node n which drives only nd .
Fig. 5 shows the algorithm for computing the MAs of the
stuck-at v fault on a node n with MA reuse. If n drives more
than one node, the algorithm directly computes MAs(n = sav).
On the other hand, if n drives only one node nd , the algorithm
then checks whether v equals Compl(nd , n). Here, Compl(nd ,
n) returns 1 if there is an inv between n and nd ; otherwise,
it returns 0. When v does not equal Compl(nd , n), the
algorithm computes MAs(n = sav) as well. However, if v
equals Compl(nd , n), the algorithm reuses MAs(nd = sa0)
and MAs(n = sav) equals MAs(nd = sa0).
D. Overall Algorithm
During the optimization process, each node in a circuit is
considered a target node, one at a time. We first find the
target node’s substitute nodes for replacement using the node-
merging technique [14]. However, if there is no substitute
node, we then consider performing NAR. In order to ensure
that each node replacement can reduce the circuit size, we
only perform NAR for the target nodes that have a fanin node
Fig. 6. Overall algorithm for circuit size reduction.
driving only one node. In this situation, when the target node
is replaced, the fanin node can be removed as well. Thus,
adding one node removes at least two nodes.
As for the optimization order, although the orders of select-
ing a target node, a substitute node, and an added substitute
node can significantly affect the optimization results, it is
difficult to evaluate the most effective optimization order.
Additionally, this evaluation process might be time-consuming
or fruitless. Thus, in this paper, we follow the optimization
order of selecting a target node and a substitute node used
by the node-merging algorithm in [14] for fair comparison. A
target node is selected from POs to PIs in the depth-first search
(DFS) order and is replaced with a substitute node that is
closest to PIs. Additionally, we replace a target node once we
find an added substitute node due to the inefficiency of finding
all added substitute nodes. When we search an added substitute
node, each MA node is selected as nf1 in a topological order
to identify the nf2 that is closest to PIs.
Fig. 6 shows the overall algorithm for circuit size reduction.
Given a circuit C, the algorithm iteratively selects a target
node nt in the DFS order from POs to PIs and replaces it if
applicable. At each iteration, in step 1, the algorithm computes
MAs(nt = sa0). If the MAs are inconsistent, it replaces nt with
0 and continues to consider the next target node. Otherwise,
if nt has a fanin node that drives only nt , the algorithm stores
the computed MAs(nt = sa0) for further reuse. Next, in step 2,
the algorithm computes MAs(nt = sa1). Similarly, if the MAs
in MAs(nt = sa1) are inconsistent, it replaces nt with 1 and
continues to consider the next target node. Otherwise, the
algorithm starts to find substitute nodes.
IE
EE
 P
ro
of
8 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
TABLE II
Experimental Results of Finding Replaceable Nodes by Using
the Node-Merging Approach [14] and Our Approach
Benchmark N [14] Our Approach
Repl. T (s) Repl. T (s) Impr. k > 5 SFoFi
C3540 1038 2.8 0.3 31.6 0.5 28.8 96.3 13.9
rot 1063 4.0 0.2 36.1 0.3 32.1 83.6 29.9
simple−spi 1079 2.4 0.1 21.7 0.3 19.3 74.8 15.4
i2c 1306 6.1 0.2 40.4 0.5 34.3 65.9 29.0
pci−spoci−ctrl 1451 11.7 0.6 43.4 1.5 31.7 59.7 31.9
dalu 1740 12.5 1.0 50.9 3.1 38.4 60.2 25.5
C5315 1773 1.9 0.2 15.7 0.3 13.8 66.7 16.7
s9234 1958 8.9 0.4 42.2 0.7 33.3 82.7 16.1
C7552 2074 2.9 0.4 33.3 0.7 30.4 71.3 15.8
C6288 2337 0.1 0.5 39.9 1.4 39.8 99.8 0.0
i10 2673 23.4 1.4 55.9 2.8 32.5 55.9 43.0
s13207 2719 5.8 0.6 32.8 1.7 27.0 78.0 19.6
systemcdes 3190 4.6 1.5 42.5 2.6 37.9 75.1 21.3
i8 3310 46.3 3.8 76.2 7.2 29.9 37.5 57.9
spi 4053 1.6 3.4 23.4 6.6 21.8 88.4 11.2
des−area 4857 1.6 5.6 18.3 13.3 16.7 87.1 17.3
alu4 5270 3.9 54.9 54.1 83.6 50.2 80.1 61.2
s38417 9219 1.9 1.5 25.2 2.4 23.3 84.7 12.4
tv80 9609 5.2 17.2 35.5 41.6 30.3 75.5 15.9
b20 12 219 6.9 17.3 36.2 34.6 29.3 59.4 11.3
s38584 12 400 4.4 17.0 35.4 66.2 31.0 87.4 13.1
b21 12 782 8.6 19.3 41.1 39.5 32.5 54.8 10.6
systemcaes 13 054 1.5 17.7 22.1 36.8 20.6 96.7 8.5
ac97−ctrl 14 496 0.7 3.2 9.9 7.5 9.2 85.2 7.8
mem−ctrl 15 641 9.8 98.8 22.0 178.0 12.2 22.9 22.9
usb−funct 15 894 2.3 6.3 21.6 16.7 19.3 62.6 14.0
b22 18 488 5.7 25.0 35.1 53.8 29.4 62.6 10.4
aes−core 21 513 2.1 15.2 37.5 39.9 35.4 90.6 9.2
pci−bridge32 24 369 1.3 21.7 15.2 47.2 13.9 86.0 8.2
wb−conmax 48 429 11.6 28.2 27.9 116.0 16.3 37.5 33.3
b17 52 920 3.0 174.5 33.0 533.8 30.0 48.7 9.6
des−perf 79 288 3.2 51.4 43.4 82.7 40.2 86.3 20.2
Average 6.5 34.4 27.8 72.0 19.8
Total 589.3 1423.7
Ratio 1 5.26
one shows the effectiveness of the proposed approach on
facilitating SAT-based BSEC.
A. Replaceable Node Identification
In the experiments, we compare the proposed approach
with our prior node-merging approach [14]. Each node in
a benchmark is considered a target node one at a time. We
separately use the node-merging approach and the proposed
approach to check how many nodes in a benchmark are
replaceable. A node is considered replaceable if it has a
substitute node or an added substitute node. Given a target
node, the proposed approach first finds its substitute nodes.
If the proposed approach fails to do so, it further finds
the added substitute nodes. Additionally, to demonstrate that
the proposed approach could complement the local ODC-
based node-merging approach [37], we measure how many
replaceable nodes that the local ODC-based node-merging
approach with a bounded depth k = 5 cannot find.
Table II summarizes the experimental results. Column 1
lists the benchmarks. Column 2 lists the number of nodes in
each benchmark represented as an AIG N. Columns 3 and
4 list the results of our prior node-merging approach. They
are the percentage of the number of replaceable nodes with
respect to N, and the CPU time T , respectively. Columns 5
and 6 list the corresponding results of the proposed approach.
Column 7 shows the improvements of the proposed approach
on the percentage of replaceable nodes. Let Nrep−k>5 denote
the number of replaceable nodes that cannot be found by the
reimplemented local ODC-based node-merging approach with
a bounded depth k = 5. Column 8 lists the percentage of
Nrep−k>5 with respect to the number of replaceable nodes. Ad-
ditionally, let Nrep−k>5−SFoFi denote the number of replaceable
nodes in Nrep−k>5 that have at least one SFoFi node. Column 9
lists the percentage of Nrep−k>5−SFoFi with respect to Nrep−k>5.
When a node having a SFoFi node is replaced, the fanin node
can be removed as well. Thus, the resultant circuit is reduced,
even though we add one node into the circuit.
For example, the benchmark C3540 has 1038 nodes. Our
prior node-merging approach found substitute nodes for 2.8%
of nodes with a CPU time of 0.3 s. The proposed approach
found that 31.6% of nodes have substitute nodes or added
substitute nodes with a CPU time of 0.5 s. Thus, the pro-
posed approach can find 28.8% more replaceable nodes.
Additionally, 96.3% replaceable nodes cannot be found by the
reimplemented local ODC-based node-merging approach with
a bounded depth k = 5. Among these nodes, 13.9% of them
have at least one SFoFi node.
According to Table II, our prior node-merging approach
can find substitute nodes for an average of 6.5% of nodes
in a benchmark. The overall CPU time for all benchmarks is
589.3 s. As for the proposed approach, it can find substitute
nodes or added substitute nodes for an average of 34.4% of
nodes in a benchmark. The overall CPU time is 1423.7 s.
As compared with our prior node-merging approach, the
proposed approach can find more replaceable nodes with a
reasonable CPU time overhead. The average number of re-
placeable nodes is 27.8% more with a ratio 5.26, and the CPU
time overhead is only 834.4 s for all benchmarks. Because the
proposed approach identifies much more replaceable nodes,
it has a better logic restructuring capability than that of the
node-merging approach.
Additionally, an average of 72.0% of the replaceable nodes
that identified by the proposed approach cannot be found by
the reimplemented local ODC-based node-merging approach
with a bounded depth k = 5. An average of 19.8% of these
nodes have at least one SFoFi node. Thus, it can be expected
that the proposed approach could complement the local ODC-
based node-merging approach [37]. They could work together
to obtain a better quality.
B. Circuit Size Reduction
In the experiments, we compare the proposed approach
with our prior ATPG-based node-merging approach [14] as
well as the SAT-based node-merging approach [28] for circuit
size reduction. To have a fair comparison with the SAT-based
node-merging approach, which focuses on post-synthesis opti-
mizations, we initially optimize each benchmark by using the
resyn2 script in the ABC package as performed by [28], which
IE
EE
 P
ro
of
10 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
used to demonstrate that our approach can work together with
the resyn2 script to obtain a better quality. Finally, we also
measure the spent CPU time by the SAT solver for solving
these optimized BSEC models.
The experimental results are summarized in Table V.
Columns 1 and 2 list the benchmarks and the number of flip-
flops (FFs) in each benchmark, respectively. Column 3 lists
the bounded unfolding depth k. Column 4 lists the spent CPU
time by the SAT solver for solving an original BSEC model.
The CPU time limit is 36 000 s. Columns 5–7 list the results
of using the resyn2 script to optimize the BSEC models. They
are the spent CPU time for SAT solving, the total CPU time,
and the saved CPU time compared to the CPU time shown
in Column 4. Columns 8–10 list the corresponding results of
using the resyn2 script followed by our approach to optimize
the BSEC models. Here, the saved CPU time is compared to
the total CPU time shown in Column 6.
For example, the benchmark b04 has 132 FFs. The SAT
solver spent 1615.7 s to solve its BSEC model which is
unfolded to 12 timeframes. When we optimized the BSEC
model by using the resyn2 script, the SAT solver spent 10.3 s
to solve it, and the total CPU time is 10.9 s. Thus, we
saved 1604.8 s (1615.7 − 10.9 = 1604.8). However, when
we optimized the BSEC model by using the resyn2 script
followed by our approach, the SAT solver spent only 0.1 s
and the total CPU time is 3.8 s. Thus, we further saved 7.1 s
(10.9 − 3.8 = 7.1).
The experimental results show that logic optimization and
restructuring could be a preprocess before SAT-based BSEC
for reducing the verification complexity. When using the
resyn2 script to optimize BSEC models, we can save a
total of 89344.1 s (approximately 25 h) for all the bench-
marks. Furthermore, when using our approach to complement
the resyn2 script, we can further save a total of 50463.8 s
(approximately 14 h) for all the benchmarks. Additionally, the
time overhead of the BSEC model optimization is less than
20 min (2121.6 − 936.9 = 1184.7). Thus, although the resyn2
script is effective for most of the benchmarks, our approach
can be combined with it to achieve a better speedup.
VIII. Conclusion
In this paper, we proposed an ATPG-based NAR approach
that can efficiently find an added node to replace a node in
a circuit. The NAR approach can replace a target node that
a node-merging approach cannot handle, thus enhancing the
capability of circuit restructuring.
We also proposed an efficient algorithm for circuit size
reduction based on the NAR approach. The techniques of
redundancy removal and MA reuse were engaged to make the
algorithm more efficient and effective. Moreover, we applied
the algorithm to facilitate SAT-based BSEC by reducing the
computation complexity.
The experimental results showed that the proposed algo-
rithm enhanced our prior ATPG-based node-merging approach
and could complement a SAT-based node-merging approach.
Additionally, it has a competitive capability of circuit size re-
duction and expends much less CPU time compared to another
SAT-based node-merging approach. The experimental results
also showed that the proposed algorithm can be integrated
with other optimization technique to obtain a better circuit
size reduction. For SAT-based BSEC, the proposed algorithm
can work together with other optimization technique to save
much equivalence checking time for all the benchmarks. All
these results showed the efficiency and effectiveness of the
proposed approach.
References
[1] M. S. Abadir, J. Ferguson, and T. E. Kirkland, “Logic design verification
via test generation,” IEEE Trans. Comput.-Aided Des., vol. 7, no. 1,
pp. 138–148, Jan. 1988.
[2] M. Abramovici, M. A. Breuer, and A. D. Friedman, “Digital systems
testing and testable design,” in Design for Testability. Piscataway, NJ:
IEEE Press, 1990.
[3] Berkeley Logic Synthesis and Verification Group. ABC: A System for
Sequential Synthesis and Verification [Online]. Available: http://www.
eecs.berkeley.edu/∼alanmi/abc
[4] A. Biere, A. Cimatti, E. M. Clarke, M. Fujita, and Y. Zhu, “Symbolic
model checking using SAT procedures instead of BDDs,” in Proc. Des.
Automat. Conf., 1999, pp. 317–320.
[5] A. Biere, A. Cimatti, E. M. Clarke, O. Strichmanm, and Y. Zhu,
“Bounded model checking,” Adv. Comput., vol. 58, no. 3, pp. 118–149,
2003.
[6] A. Biere, A. Cimatti, E. M. Clarke, and Y. Zhu, “Symbolic model
checking without BDDs,” in Proc. Tools Algorithms Construct. Anal.
Syst., 1999, pp. 193–207.
[7] M. Case, V. Kravets, A. Mishchenko, and R. Brayton, “Merging nodes
under sequential observability,” in Proc. Des. Automat. Conf., 2008,
pp. 540–545.
[8] L. C. L. Chang, C. H. P. Wen, and J. Bhadra, “Speeding up bounded
sequential equivalence checking with cross-timeframe state-pair con-
straints from data learning,” in Proc. Int. Test Conf., 2009, pp. 1–8.
[9] C. W. J. Chang, M. F. Hsiao, and M. M. Sadowska, “A new rea-
soning scheme for efficient redundancy addition and removal,” IEEE
Trans. Comput.-Aided Des., vol. 22, no. 7, pp. 945–952, Jul. 2003.
[10] S. C. Chang, K. T. Cheng, N. S. Woo, and M. Marek-Sadowska, “Post-
layout logic restructuring using alternative wires,” IEEE Trans. Comput.-
Aided Des., vol. 16, no. 6, pp. 587–596, Jun. 1997.
[11] S. C. Chang, L. P. P. P. Van Ginneken, and M. Marek-Sadowska, “Circuit
optimization by rewiring,” IEEE Trans. Comput., vol. 48, no. 9, pp. 962–
970, Sep. 1999.
[12] S. C. Chang, M. Marek-Sadowska, and K. T. Cheng, “Perturb and
simplify: Multi-level boolean network optimizer,” IEEE Trans. Comput.-
Aided Des., vol. 15, no. 12, pp. 1494–1504, Dec. 1996.
[13] Y. C. Chen and C. Y. Wang, “An Improved approach for alternative wire
identification,” in Proc. Int. Conf. Comput. Des., 2005, pp. 711–716.
[14] Y. C. Chen and C. Y. Wang, “Fast detection of node mergers using logic
implications,” in Proc. Int. Conf. Comput.-Aided Des., 2009, pp. 785–
788.
[15] Y. C. Chen and C. Y. Wang, “Fast node merging with don’t cares using
logic implications,” IEEE Trans. Comput.-Aided Des., vol. 29, no. 11,
pp. 1827–1832, Nov. 2010.
[16] Y. C. Chen and C. Y. Wang, “Node addition and removal in the presence
of don’t cares,” in Proc. Des. Automat. Conf., 2010, pp. 505–510.
[17] K. T. Cheng and L. A. Entrena, “Multi-level logic optimization by
redundancy addition and removal,” in Proc. Eur. Conf. Des. Automat.,
1993, pp. 373–377.
[18] F. S. Chim, T. K. Lam, and Y. L. Wu, “On improved scheme for
digital circuit rewiring and application on further improving FPGA
technology mapping,” in Proc. Asia South Pacific Des. Automat. Conf.,
2009, pp. 197–202.
[19] L. A. Entrena and K. T. Cheng, “Combinational and sequential logic
optimization by redundancy addition and removal,” IEEE
Trans. Comput.-Aided Des., vol. 14, no. 7, pp. 909–916, Jul. 1995.
[20] T. Kirkland and M. R. Mercer, “A topological search algorithm for
ATPG,” in Proc. Des. Automat. Conf., 1987, pp. 502–508.
[21] A. Kuehlmann, “Dynamic transition relation simplification for bounded
propery checking,” in Proc. Int. Conf. Comput.-Aided Des., 2004,
pp. 50–57.
[22] A. Kuehlmann, V. Paruthi, F. Krohm, and M. K. Ganai, “Robust boolean
reasoning for equivalence checking and functional property verification,”
IEEE Trans. Comput.-Aided Des., vol. 21, no. 12, pp. 1377–1394,
Dec. 2002.
 1 
出席 2011 IWLS + DAC 報告 
國立清華大學資訊工程學系 王俊堯 副教授 
2011 年 07 月 19 日 
    6/2 搭華航 CI008 直飛洛杉磯國際機場，在歷經十多個小時
的飛行後，很疲累地抵達洛杉磯國際機場，接著再轉機到聖地牙
哥，出機場時已經午夜 11 點多了，輾轉搭乘 local 的交通工具，
抵達位於聖地牙哥的飯店。 
 
2011 年邏輯與合成國際會議(International Workshop on Logic 
and Synthesis 2011)自 6 月 3 日起在聖地牙哥的 UC-SD 舉行，為
期三天。此次乃大會的第 20 屆，接受並安排發表的論文共有 25
篇，其中 21 篇為 regular presentations、4 篇為 posters。每個時段
只有一個場次進行，讓所有與會者皆能齊聚一堂。論文涵蓋積體
電路的合成、最佳化與驗證等主題，無論是 regular presentation
或 poster 論文，每篇內容可達八頁並收錄於論文集內。 
 
    筆者與隨行的研究生在會中發表一篇 regular 論文，內容為
針對臨界值邏輯 (threshold logic) 提出重接線的演算法。我們是
首次提出此方法的論文，其建立了重要的理論基礎，所以得到來
自學界(如 UC-Berkeley、University of Michigan、UIUC 等)的與
會者的注意與熱烈討論，收穫頗豐。    
 
    大會正式的晚宴也安排於第二天晚上舉辦，提供一個讓與會
者彼此認識以及分享研究成果與心得的社交機會。筆者是第三次
參加此會議，參加後感覺不虛此行，不但能和領域相近的學者專
家就筆者發表的論文成果進行意見交換，也對一些彼此感興趣或
筆者未曾接觸之研究題材有進一步的技術探討或初步認識。同時
筆者也遇見了許多有名的教授學者，如 R. Brayton，A. Mischehko 
(UC Berkeley)，T. Sasao (Kyushu Institute of Tech.)等人，並與他
們在餐桌上交談，這個經驗對我來說真的很寶貴。 
 
 1 
出席 2011 PROFIT 國際合作報告 
國立清華大學資訊工程學系 王俊堯 副教授 
2011 年 09 月 30 日 
   八月十日搭華航 CI511 直飛北京機場，在歷經 3 個多小時的
飛行後，順利抵達北京機場，輾轉搭乘 local 的交通工具，抵達
位於北京大學內的飯店。 
 
雖然 2011 PROFIT 國際合作會議於八月十四日才要在內蒙
古的呼和浩特舉行，但是為了參觀北京大學及大陸清華大學，所
以就提前出發了，在北京進行了幾日的私人訪問行程後，於八月
十三日前往會議地點呼和浩特。 
 
今年會議安排 2場 keynote speeches，分別是由國立清華大
學的兩位前任校長劉炯朗校長及陳文村校長發表專題演說，另外
還有浪潮集團有限公司董事長兼 CEO孫丕恕，創意電子 CEO石克
强及百度高级技術總監范麗也發表專題演說，內容涵蓋積體電路
的演進、資料中心的設立、以及搜尋引擎的介紹等主題。 
 
筆者與 PROFIT 委員的互動良好(如北京清華的汪玉教授，
UCLA 的 Jason Cong 教授，Lei He 教授，UCSB 的 Tim Cheng 教
授，及 UT Austin 的 David Pan 教授等)，我們討論彼此最近的研
究內容與心得，並交換意見，且對於最新的研究題目與方向互相
交流想法，最後談及學生素質及學生研究態度的差異，並交流一
些想法及做法，收穫很多。同時也討論到下一次會議的召開地點。 
 
 2 
接著筆者繼續參加 Design Automation Conference 2011 的學
術會議。2011 年設計自動化會議 (Design Automation Conference 
2011)自 6 月 5 日起在聖地牙哥的國際會議中心舉行，為期 5 天。
被大會接受並安排發表的論文共有 156 篇，接受率只有 23%。每
個時段有 6 個場次平行進行，每篇內容可達 6 頁並收錄於論文集
內。此次筆者也與隨行的研究生在會中發表一篇論文，內容為針
對單電子電晶體 (single-electron transistor) 的元件架構，提出自
動化合成電路的演算法。我們是首篇提出自動化程序的論文，其
將此領域帶入全新自動的階段，所以獲得許多與會者的注意。 
在 DAC 會議中，我也聆聽許多新的技術報告之發表及至參
展的公司攤位了解新產品的功能說明與市場走向。  
筆者在返國後分別攜回2011年邏輯與合成國際會議論文集一
冊及 2011 年設計自動化會議論文集一冊。 
 1 
Current detector 
1 
(a) (b) 
a⊕b 
a a’ 
b’ b 
Active high  
Active low 
Short  
Figure 2: (a) A SET array fabric. (b) An example of a
xor b.
a conducting nanowire or have a wrapped gate. Consequently,
this structure is not very regular and cannot be restructured to
implement a different function due to the physical etching process
involved in its realization. Furthermore, if any of the nanowire
segments or the wrap gates is defective, the whole circuit be-
comes non-functional. This is a significant limitation considering
that nanowires and few electron nanodevices have traditionally
suffered from the variability and reliability issues.
To solve the problem, a reconfigurable version of SET using
wrap gate tunable tunnel barriers was proposed [2] and the in-
depth device simulation to study the electrostatic properties was
presented [8]. This device can operate in three distinct operation
states: a) active b) open and c) short state based on the wrap gate
bias voltages. Such programmability leads to immense flexibility
in designing a circuit. The device simulation shows that this
device can provide an order of magnitude lower energy-delay than
CMOS device [8].
However, the synthesis of a BDD using the device in [2] is man-
ual rather than automated. The reason is that mapping a reduced
ordered BDD (ROBDD) into a planar SET array could be very
complicated, especially when the BDD has crossing edges, which
is typical in minimized BDDs. In this work, we address this
mapping problem and propose an automated mapping approach.
Instead of mapping a BDD directly, the proposed approach first
divides a BDD into a set of product terms that represent the
paths leading to the 1 terminal in the BDD. Then, it sequentially
maps these product terms. Since the mapping order of the prod-
uct terms affects the mapping results, we propose four sorting
heuristics to reduce area cost. Additionally, the automated map-
ping approach incorporates the granularity and fabric constraints
that are imposed in order to decrease the number of metal wires
used for programming the SET array and for supplying the input
signals, respectively [2].
We conduct experiments on a set of MCNC benchmarks [10].
The experimental results show that the proposed approach can
complete mapping within 1 second for most of the benchmarks.
The main contribution of this work is proposing an automated
synthesis tool for the promising energy-efficient SET array archi-
tecture.
The rest of this paper is organized as follows: Section 2 uses
an example to demonstrate the problem considered in this paper,
and introduces some notations. Section 3 presents the proposed
mapping approach. Section 4 discusses and addresses two map-
ping constraints. Finally, the experimental results and conclusion
are presented in Sections 5 and 6.
2. BACKGROUND
2.1 An example
A SET array can be presented as a graph composed of hexagons.
As shown in Fig. 2(a), like the hexagonal fabric mentioned above,
there is a current detector at the top that measures the current
coming from the bottom of the hexagonal fabric. All the vertical
edges of the hexagons are electrical short. All the sloping edges
can be configured as active high, active low, short or open. An
active high edge is controlled by a variable x. It is conducting
and non-conducting when x = 1 and x = 0, respectively. Con-
 
root node (0, 0) 
4 
x 1 4 3 2 0 -1 -2 -3 
y 
3 
2 
1 
0 
node (-3, 3) 
left edge 
right edge 
root node (0, 0) 
x 1  3 2 0 -1 -2 -3 
y 
3 
2 
1 
0 
node (-3, 3) 
left edge 
right edge 
-4 4 
Figure 3: An abstract diamond fabric.
versely, an active low edge is an electrical opposite of an active
high edge and it is controlled by a variable x′.
A Boolean function can be implemented using a SET array.
All the active edges at the same row of the hexagonal fabric are
controlled by a single variable, i.e., a primary input (PI). They
determine whether there exists a path for the current to pass
through, and thus, be detected at the top. If so, the functional
output of the array is 1; otherwise, it is 0. For example, Fig.
2(b) shows a SET array implementing a xor b. When a = 1
and b = 0, the current can be detected by passing through the
left path. However, if a = 1 and b = 1, the current cannot be
detected.
Thus, the addressed problem of this work is synthesizing a given
Boolean function into a SET array with minimized area, i.e., the
number of configured hexagons.
Previous work [2] tries to manually map a Boolean function
by directly mapping its BDD into a SET array. However, the
mapping process could be very complicated due to the structural
difference of a BDD and a SET array. For example, an ROBDD
usually has some crossing edges. Since a SET array is a planar
architecture, much effort is required to avoid having the crossing
edges in the ROBDD when mapping it into a SET array. Node
duplication could be a trivial method for solving this crossing
edge issue while not considering the area overhead. In addition,
determining the exact location of each ROBDD node in a SET
array is a challenge. Thus, to address this problem, we propose
a product term-based method. It first collects all the paths that
lead to the terminal 1 in the ROBDD, i.e., product terms. Then,
it maps each product term into a path in the SET array. The
proposed method simultaneously avoids the crossing edge and the
BDD node mapping issues.
For example, the product terms of a xor b are 10 and 01. Using
the proposed method, we first map 10 and then 01. Finally, we
obtain the resultant SET array as shown in Fig. 2(b), where the
left path is configured for 10 and the right path is for 01.
2.2 Notations
For ease of discussion, we use an abstract graph to present a
SET array. Compared to Fig. 2(a), only the configurable edges
are preserved as shown in Fig. 3. In this diamond fabric, each
node n, i.e., the root of a pair of left and right edges, has a unique
location (x, y). Based on the root node located at (0, 0), which
is below the current detector, the y value increases from top to
bottom. The x value increases and decreases from center to right
and left, respectively.
For simplification, let n.left and n.right denote the status of the
left and right edges of a node n, respectively. The status could
be empty, high, low, short, or open. empty indicates the edge is
not configured yet (is used primarily for algorithm illustration).
high, low, short, and open indicate the edge is configured as active
high, active low, short, and open, respectively. Additionally, let
n(x,y) denote the node located at (x, y).
3. AUTOMATED MAPPING
In this section, we first discuss the motivation of our method.
Next, we introduce two key mapping procedures. Finally, the
overall flow is presented. Here, we first assume that each edge
can be configured independently without any constraint. In the
879
48.3
= 
1 1 1 – 0 
0 1 0 – 0 
0 1 0 0 1 
 Active high 
Active low 
Short 
Open 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
1 1 1 – 0 
0 1 – – 0 
0 1 1 1 1 
 
3 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
3 
0 
1 
2 
3 
0 1 2 -1 
4 
5 
3 4 
(a) (b) 
(c) (d) 
4 
Figure 6: Incorrect mapping examples.
p2 = 111−, and p3 = 101−, sorted by ForInertiaSort as shown in
Fig. 5(a). First, let us consider p0. Starting from the root node
n(0,0), we first configure n(0,0).left as low for the first bit 0. Next,
we configure n(−1,1).right as high for the second bit 1. Using the
same method, we configure n(0,2).left and n(−1,3).right as high
and low for the last two bits 10, respectively. The mapping result
is shown in Fig. 5(b). Here, the decision of configuring the left
edge or the right edge of a node depends on its location (x, y). If
x < 0, we first try to configure its right edge. If inapplicable, we
then try to configure its left edge. Conversely, if x ≥ 0, we try
the left edge first and then the right edge.
Next, for p1, because the first two bits are the same as that
of the first product term, we partially reuse this mapping result.
Then, we configure n(0,2).right as low and n(1,3).left as short for
the last two bits 0−, respectively. The mapping result is shown
in Fig. 5(c).
For p2, after we configure n(0,0).right as high for the first bit 1,
we do not configure n(1,1).left as high for the second bit 1. This
is because if we do so, there will exist a path n(0,0) → n(1,1) →
n(0,2) → n(1,3) → n(0,4), which corresponds to an invalid product
term 110−. Thus, we configure n(1,1).right as high for the second
bit 1. Finally, n(2,2).left and n(1,3).left are configured as high and
short for the last two bits 1−, respectively. The mapping result
is shown in Fig. 5(d).
Next, let us consider p3. After finding n(0,0).right = high for
the first bit 1, we do not configure n(1,1).left as low for the sec-
ond bit 0. This is because it will construct a path for an invalid
product term 100−. Additionally, since n(1,1).right has been con-
figured as high, we expand the structure by configuring both
n(2,0).left and n(2,0).right as short, and start from n(3,1) for the
last three bits. The mapping result is shown in Fig. 5(e). Finally,
we configure all the non-configured edges as open, and obtain the
final mapping result in Fig. 5(f).
To avoid creating an invalid path, we need to prevent two paths
from merging and then branching during mapping. Thus, when
we detect a merging node, like n(0,2) for p2 or p3, we will check
if there exists only one path from n(0,2). If not, there possibly
exists an invalid path. Thus, we prevent the paths from merging.
With this checking rule, each path from top to bottom exactly
corresponds to one product term. In addition, from the viewpoint
of conducting paths, this checking rule is not enough and we have
to add another rule considering the conducting path issue. Fig.
6(a), (b) show two mapping examples, which are incorrect while
satisfying the merging and branching rule.
In Fig. 6(a), when the input pattern is 11101, which is not a
minterm, the current can be detected at the top. This is because
the right edge of n(−1,3), the left edge of n(1,3), and the right
edge of n(1,3) as highlighted are conducting simultaneously. This
partial conducting path forms like a bridge that connects two
paths such that the current can pass through the path n(1,5) →
n(2,4) → n(1,3) → n(0,4) → n(−1,3) → n(0,2) → n(−1,1) →n(0,0).
In addition, a partial conducting path could be composed of the
Mapping(set PTs) // PTs: product terms
1. Configure n(0,0).left and n(0,0).right based on the first bit values
of the product terms in PTs;
2. For each product term t in PTs
2.1. If (LeftConfigure(t, 0, 0)), continue;
2.2. If (RightConfigure(t, 0, 0)), continue;
2.3. Expand(t);
3. Configure all the edges that are not configured yet as open;
bool LeftConfigure(productterm t, int x, int y)
1. If n(x,y).left is inconsistent to the y
th bit in t, return 0;
2. If n(x−1,y+1) is a merging node and there is more than one path
from n(x−1,y+1), return 0;
3. If the configuration of n(x,y).left will make the left edge of n(x,y)
and the right edge of n(x−2,y) could be conducting simultane-
ously, return 0;
4. If n(x,y).left is empty, configure it based on the mapping rules;
5. If (x− 1 < 0)
5.1. If (RightConfigure(t, x− 1, y + 1)), return 1;
5.2. If (LeftConfigure(t, x− 1, y + 1)), return 1;
6. If (x− 1 ≥ 0)
6.1. If (LeftConfigure(t, x− 1, y + 1)), return 1;
6.2. If (RightConfigure(t, x− 1, y + 1)), return 1;
7. Undo n(x,y).left if necessary, and return 0;
bool RightConfigure(productterm t, int x, int y)
1. If n(x,y).right is inconsistent to the y
th bit in t, return 0;
2. If n(x+1,y+1) is a merging node and there is more than one path
from n(x+1,y+1), return 0;
3. If the configuration of n(x,y).right will make the right edge of
n(x,y) and the left edge of n(x+2,y) could be conducting simul-
taneously, return 0;
4. If n(x,y).right is empty, configure it based on the mapping rules;
5. If (x− 1 < 0)
5.1. If (RightConfigure(t, x + 1, y + 1)), return 1;
5.2. If (LeftConfigure(t, x + 1, y + 1)), return 1;
6. If (x− 1 ≥ 0)
6.1. If (LeftConfigure(t, x + 1, y + 1)), return 1;
6.2. If (RightConfigure(t, x + 1, y + 1)), return 1;
7. Undo n(x,y).right if necessary, and return 0;
bool Expand(productterm t)
1. Determine the expansion direction (left or right) based on the
first bit in t.
2. If the expansion direction is left, x = −2; otherwise, x = 2;
3. While(1)
3.1. Configure n(x,0).left and n(x,0).right as short if they are
empty;
3.2. If (x− 1 < 0)
3.2.1 If (RightConfigure(t, x− 1, 1)), return 1;
3.2.2 If (LeftConfigure(t, x− 1, 1)), return 1;
3.2.3 x = x− 2;
3.3. If (x− 1 ≥ 0)
3.3.1 If (LeftConfigure(t, x + 1, 1)), return 1;
3.3.2 If (RightConfigure(t, x + 1, 1)), return 1;
3.3.3 x = x + 2;
Figure 7: The algorithm of product term mapping.
edges at the different rows. For example, Fig. 6(b) shows a par-
tial conducting path that crosses two rows as highlighted. This
path, n(3,3) → n(2,2) → n(1,3) → n(0,4) → n(−1,3), constructs an
invalid conducting path for the input pattern 11111.
A necessary condition for causing a partial conducting path is
that there exist two pairs of two adjacent conducting edges: one
pair is two lower edges of a diamond that could be conducting si-
multaneously, and the other pair is two upper edges of a diamond
that could be conducting simultaneously. For example, in Fig.
6(a), the right edge of n(−1,3) and the left edge of n(1,3) are the
former, and the left and right edges of n(1,3) are the latter. One
simple method for avoiding partial conducting paths is to ensure
that one of the mentioned two pairs of two adjacent conducting
edges is never constructed. Thus, if a configuration results in a
merging node, we check if the two edges connecting to the merg-
ing node could be conducting simultaneously. If so, we avoid this
configuration. With this method, we can prevent two lower edges
of a diamond from conducting simultaneously. Fig. 6(c) and Fig.
6(d) show the correct mapping results for the product terms in
Fig. 6(a) and Fig. 6(b), respectively.
Additionally, because the root node has only two edges (left
881
48.3
Table 1: The experimental results of using differ-
ent product term sorting heuristics and mapping con-
straints.
Bench. PI PO PT
Constraint-free Granu. Fabric
Lex Inert. FInert. BFInert. FInert. FInert.
C17 5 2 8 *18 *18 20 20 58 66
cm138a 6 8 48 *116 158 120 120 360 438
x2 10 7 33 *149 152 153 154 725 790
cm85a 11 3 49 219 197 197 *195 608 528
cm151a 12 2 25 406 427 *400 *400 885 1045
cm162a 14 5 37 292 336 294 *287 1077 1163
cu 14 11 24 240 242 *238 *238 609 662
cmb 16 4 26 195 216 *170 *170 710 855
cm163a 16 5 27 275 *257 260 260 907 1029
pm1 16 13 41 337 342 *335 *335 1186 1239
pcle 19 9 45 *291 292 293 293 1553 1775
sct 19 15 142 1890 *1661 1725 1741 4665 5186
cc 21 20 57 618 658 *585 603 2214 2306
i1 25 16 38 632 650 *627 *627 1773 1920
lal 26 19 160 1968 2157 1832 *1799 7838 8684
pcler8 27 17 68 *737 850 *737 *737 3160 3435
frg1 28 3 399 5993 *5602 5612 5612 11029 13731
c8 28 18 94 *836 884 881 894 4663 4869
term1 34 10 1246 23494 25297 *22426 23856 70844 80293
count 35 16 184 1936 1861 *1336 1465 13509 14678
unreg 36 16 64 1288 *1259 1280 1280 4518 4632
b9 41 21 352 *6333 8650 6478 6542 24272 22089
cht 47 36 92 *2380 2390 *2380 *2380 7857 7934
apex7 49 37 1440 36252 44001 *35999 36317 123003 135543
example2 85 66 430 9737 10164 9623 *9494 53597 50471
Total 96632 108721 94001 95819 341620 365361
Best count 8 5 11 11
4.2 Fabric constraint
In SET array implementation, the inputs to the active edges in
a row are supplied by metal wires. We need two wires to supply
both the normal and complement of an input to a row. Each
edge is connected to either x or its complement x′ wires for the
row. The pattern of connections of x and x′ in a row defines the
SET fabric and it is fixed during manufacturing.
For example, using x to control all left edges and x′ to control
the right edges results in the symmetric fabric proposed in [2].
In our mapping tool, we use the symmetric fabric constraint. In
the future, we will extend our mapping tool to accept any fabric
specification.
In such an array, both (high, low) and (low, high) cannot
simultaneously appear at the same row in a SET array. Note
that the entire row pattern of (high, low) (or (low, high)) can be
changed to (low, high) (or (high, low)) by swapping the normal
value and its complement in the control input signals for the row.
To satisfy this symmetric fabric constraint, we need to identify
which combination ((high, low) or (low, high)) appears at a
certain row. One method is to follow the first configuration result
at the row. For example, if (high, low) is first configured at a
row, we then do not configure (low, high) at this row. Another
easy method is to allow only one of (high, low) and (low, high)
to appear in a SET array. For example, for a bit value 1 or 0, we
can always configure the left edge as high and the right edge as
low, i.e., only (high, low) is allowed. For simplification, we use
the second method in this work.
Fig. 9(b) shows the mapping result for the same set of prod-
uct terms in Fig. 5(a) considering the fabric constraint. In this
example, only (high, low), (short, short), and (open, open) are
allowed.
5. EXPERIMENTAL RESULTS
We implemented the algorithm in C language. The experi-
ments were conducted on a 2.67 GHz Linux platform (Red Hat
5.5). The benchmarks are from the MCNC benchmark suite [10].
For each benchmark, we separately map the Boolean function of
each primary output (PO), and measure the total number of con-
figured hexagons and the total CPU time. In the experiments, we
compare different product term sorting heuristics and mapping
constraints.
Table 1 summarizes the experimental results. Column 1 lists
the benchmarks. Except the C17 benchmark, all the benchmarks
have the crossing edge issue in their ROBDDs. Directly mapping
each of these ROBDDs into a SET array could be very diffi-
cult. Columns 2 and 3 list the number of PIs and POs in each
benchmark, respectively. Column 4 lists the number of computed
product terms. The remaining columns list the mapping results
in terms of the number of hexagons by using different sorting
heuristics and constraints. The number marked with “*” means
that it is the best result among all sorting heuristics. Columns 5
to 8 are the constraint-free mapping results by using LexSort, In-
ertiaSort, ForInertiaSort, and BackForInertiaSort, respectively.
Columns 9 and 10 are the mapping results of applying the gran-
ularity and fabric constraints by using ForInertiaSort only. This
is because the ForInertiaSort heuristic has better results for con-
sidering all benchmarks or large benchmarks in the experiments.
We omit the results by using the other sorting heuristics due to
page limit.
According to Table 1, there is no a specific sorting heuristic
that completely outperforms the others for all the benchmarks.
By all accounts, ForInertiaSort results in the best mapping for
considering all benchmarks. Additionally, when the constraints
are considered, the number of configured hexagons increases.
This is because the number of edges shared by different paths
decreases. As for the CPU time, the proposed method can map
each benchmark within 1 second except the term1 and apex7
benchmarks that spent approximately 6 seconds.
6. CONCLUSION
In this paper, we propose a product-term-based approach that
can efficiently map a Boolean function into a SET array. It solves
the problem of automatically mapping a BDD into a SET array
that previous works suffer from. The proposed approach sim-
plifies the mapping problem by transforming a BDD into a set
of product terms, and then individually mapping these product
terms. Additionally, four product term sorting heuristics are pro-
posed to enrich the approach. The granularity and fabric con-
straints can also be handled by the proposed approach. The
experimental results show its effectiveness and efficiency of map-
ping a set of MCNC benchmarks. Our automated mapping is a
key enabler for using the promising BDD technology.
7. REFERENCES
[1] R. Bryant, “Graph-based Algorithms for Boolean Function
Manipulation,” IEEE Trans. Computers, vol. 35, pp. 677-691,
Aug. 1986.
[2] S. Eachempati, V. Saripalli, V. Narayanan, and S. Datta,
“Reconfigurable Bdd-based Quantum Circuits,” in
Proc. Int. Symp. on Nanoscale Architectures, 2008, pp. 61-67.
[3] H. Hasegawa and S. Kasai, “Hexagonal Binary Decision Diagram
Quantum Logic Circuits Using Schottky In-Plane and Wrap Gate
Control of GaAs and InGaAs Nanowires,” Physica E:
Low-dimensional Systems and Nanostructures, vol. 11, pp. 149-154,
Oct. 2001.
[4] S. Kasai, M. Yumoto, and H. Hasegawa, “Fabrication of GaAs-based
Integrated 2-bit Half and Full Adders by Novel Hexagonal BDD
Quantum Circuit Approach,” in Proc. Int. Symp. on Semiconductor
Device Research, 2001, pp. 622-625.
[5] M. Keating, D. Flynn, R. Aitken, A. Gibbons, and K. Shi, Low
Power Methodology Manual: For System-on-Chip Design,
Springer, 2007.
[6] S. W. Keckler, K. Olukotun, and H. P. Hofstee, Multicore
Processors and Systems, Springer, 2009.
[7] C. Piguet, Low-power CMOS Circuits: Technology, Logic Design
and CAD Tools, CRC Press, 2006.
[8] V. Saripalli, L. Liu, S. Datta, and V. Narayanan, “Energy-Delay
Performance of Nanoscale Transistors Exhibiting Single Electron
Behavior and Associated Logic Circuits”, Journal of Low Power
Electronics (JOLPE), vol. 6, pp. 415-428, 2010.
[9] F. Somenzi, CUDD: CU decision diagram package - release 2.4.2,
2009. http://vlsi.colorado.edu/∼fabio/CUDD/
[10] S. Yang, “Logic Synthesis and Optimization Benchmarks, Version
3.0,” Tech. Report, Microelectronics Center of North Carolina,
1991.
[11] http://embedded.eecs.berkeley.edu/pubs/downloads/
espresso/index.htm
[12] http://www.intel.com/go/terascale/
883
48.3
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/24
國科會補助計畫
計畫名稱: 子計畫三：熱能導向的最大功率估計和分析(3/3)
計畫主持人: 王俊堯
計畫編號: 99-2220-E-007-003- 學門領域: 晶片科技計畫--整合型學術研究
計畫
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
