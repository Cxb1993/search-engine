mantic and spatial information in image collections to ad-
dress object localization problems [7, 8, 9, 10, 11]. Among
the proposed work, generative object model [7, 9, 11], slid-
ing window search [8, 10] and super-pixel based [7] meth-
ods have shown attractive results on Caltech and PASCAL
VOC datasets.
Without the need to use sliding windows or image super-
pixels, nor the assumption for any generative object mod-
els, we propose to extend the standard BOF model and con-
struct a visual attention map for each object class by learn-
ing image and class-specific visual words. This learning
process combines L2 or L1-regularized support vector ma-
chines and discriminating feature selection techniques, as
we detail later in Section 3.
Fig. 1 shows an example result using our proposed
method. Fig. 1a is an image from the Caltech-256 dataset,
and Fig. 1b shows the learned visual attention map of Fig.
1a. Brighter regions in Fig. 1b indicate larger probability
values, meaning that the associated pixels in Fig. 1a are
more likely to be the object of interest. A threshold be-
tween 0 and 1 can be applied to this map for further object
segmentation, as shown in Fig. 1c. Thus, one of the bene-
fits of our proposed approach is that localization results with
different confidence levels can be obtained by imposing var-
ious thresholds on the learned map. It is worth mentioning
that our method is not limited any specific type of image
descriptors, or any particular codebook learning, and fea-
ture encoding techniques. For simplicity, we choose to learn
and construct visual attention maps based on SIFT descrip-
tors and the associated BOF models, and our experiments
in Sect. 5 will validate our method for simultaneous object
recognition and localization in image collections.
The remaining of this paper is organized as follows. We
briefly review related work in Section 2. Section 3 intro-
duces our proposed method for visual attention map learn-
ing, and the use of our visual attention map for object cate-
gorization is discussed in Section 4. Section 5 presents our
experimental results and comparisons with different BOF
models, before conclusions are drawn in Section 6.
2. Related work
Object localization problems are typically addressed us-
ing supervised or weakly-supervised frameworks. Super-
vised methods require ground truth information at pixel-
level during training, i.e., each pixel in the training image
needs to denote the associated object label (or background).
Such information is often provided using bounding boxes
or pixel-wise object labels. This task has been progressed
by recent research and vision competitions, especially PAS-
CAL Visual Object Class (VOC) Challenges [5]. Despite
excellent results were reported (e.g. [12, 13]), this super-
vised framework requires human labors to annotate a vast
amount of bounding boxes or pixel-wise object labels for
the training data. Therefore, it cannot be easily generalized
to large-scale problems.
For weakly-supervised methods, only the knowledge
that the foreground object exists in each image is used in
training. The spatial information of the objects is not spec-
ified and thus needs to be learned simultaneously. Pre-
viously, Leibe et al. [14] combined both object recogni-
tion and segmentation into one task using an Implicit Shape
Model (ISM) [14]. They proposed a probability framework
that obtains per-pixel confidence measure by extracting im-
age patches around the interest points and casting proba-
bilistic votes. However, we find that the use of standard
interest points may not be sufficient for the cases when ob-
jects of interest are small or with low contrast in real-world
images, as we discuss later in Sect. 3. Instead, we advocate
the use of dense local descriptors to alleviate the aforemen-
tioned problems.
Viola and Jones [15] employed a cascade of filters to re-
ject unlikely regions of interest using sub-window search.
Although their method demonstrated robustness in frontal
face detection, as pointed out by Harzallah et al. [10], train-
ing a cascade of filters for a large number of classes is very
computationally expensive. Chum et al. [16] proposed an
exemplar model to reduce the number of sub-windows to be
searched. Unlike their proposed work attempting to find the
best sub-window or bounding box for the object, we pro-
pose to estimate the probability distribution of the spatial
occurrence of the object using a visual attention map, which
can be considered as a soft bounding box for the object of
interest.
Our work is inspired by the recent work of Marszałek et
al. [13, 17]. In [13], they proposed to constructe a ”seg-
mentation mask” by employing spatial relationships be-
tween features in a strongly supervised fashion. Different
from [13], which requires object boundaries as ground truth
information, our proposed method is a weakly-supervised
framework and does not need any segmentation in training.
In [17], a mask for each object is built by clustering many
hypothetical shape masks. While our work does not gen-
erate any hypotheses, we propose to learn visual attention
maps for different objects in a unified weakly-supervised
process. More specifically, we focus on the selection (or
extraction) of class and image-specific visual words from a
codebook of dense local image descriptors, and our exper-
imental results will validate the feasibility of this proposed
framework. The flow chart of our proposed work is shown
in Fig. 2. Although dense SIFT descriptors are used in this
paper, it is worth repeating that our proposed method can
be applied to codebooks learned from other types of local
descriptors.
We note that, although Bishop et al. [11] also used
weakly labeled image data and proposed to construct gen-
erative and discriminative object models for object detec-
thus is more computationally efficient. However, to over-
come the non-differentiability of both regularization and
loss functions in the SVM objective function, a L2-loss or
a logistic-loss term is typically used instead. In this paper,
we consider the L2-loss function in (2) and thus utilize the
L1-regularized L2-loss SVM.
Previously, Guyon et al. [22] and Chang et al. [23] used
the squared or the absolute value ofwj of the standard SVM
in (1), and they ranked the feature attributes in x accord-
ingly to achieve feature ranking. Inspired by their work, we
propose to train an one-vs-all SVM [24] (using L2 or L1-
regularized SVMs) to separate one object class from all the
others using the L1-norm visual word histograms. As a re-
sult, each SVM is capable of selecting class-specific visual
words according to wj , while those extracted from back-
ground clutter will be suppressed.
However, different from prior work, we do not select and
weight the visual words according to |wj | or |wj |2 in our
framework. We ignore the negative wj by setting them to
zero, and rank the positive ones of wj accordingly (we lin-
early scale the non-negative wj to the range [0, 1] as the
ranking score of the visual word). This is because that,
since the features x in (1) or (2) are histograms of visual
words, each entry xj corresponds to the number of occur-
rences of each visual word and thus is always non-negative.
In an one-vs-all SVM, the inner product between w and
the input x predicts how likely the query image x belongs
to the associated object class. The negative entries of w
would make this inner product value small (or negative),
and thus the corresponding visual words in x are less likely
of interest for that object class. Therefore, we only extract
the positive wj to select and to weight class-specific visual
words. This framework of discriminating visual word ex-
traction based on modified L2 or L1-regularized SVMs is
easy to implement, and we found that it is generally quite
effective. Detailed experimental results and comparisons
for object recognition and localization will support this, as
we discuss later in Sect. 5.
3.3. Construction and Refinement of Visual Atten-
tion Maps
Recall that, each visual word in the codebook corre-
sponds to a set of dense SIFT descriptors. After assigning
the weight (i.e. the scaled wj determined above) of each
visual word to the associated SIFT descriptors, a grayscale
mask based can be constructed accordingly (see Fig. 2c for
an example). We now discuss how to refine this mask as our
final visual attention map.
3.3.1 Gaussian Smoothing
Let us denote the mask obtained in Fig. 2c as AR(x, y).
Each entry of AR is a value between 0 and 1, and it will be
non-zero only if a SIFT descriptor is extracted at that loca-
tion. Unfortunately, we cannot use this AR(x, y) as the vi-
sual attention map for Fig. 2a, since the local neighborhood
of each SIFT descriptor (which contains spatial information
of the object) in Fig. 2a cannot pass through. Therefore,
we perform a Gaussian smoothing process to dilate the map
AR by
AE(x, y) = AR(x, y) ∗Kσ(x, y), (3)
where ∗ represents the convolution operator, AE is the di-
lated version of AR, and Kσ(x, y) is the Gaussian smooth-
ing kernel with width σ. Fig. 2d illustrates the dilated ver-
sion of Fig. 2c. To avoid energy dissipation, we normalize
this map (i.e.,AE ← AEmax(AE) ) after this smoothing process
is complete.
3.3.2 Cross Bilateral Filtering
As we can see, the boundary of the above smoothed result
AE is blurred in Fig. 2d, and it does not contain any bound-
ary information for the object in Fig. 2a. Inspired by [25], in
which the depth map of an image is processed by cross bi-
lateral filtering [26] to preserve object boundaries, we apply
the same idea and refine the smoothed map AE by an iter-
ative cross bilateral filtering operation. As a result, given
an input image and the associated object class label, our
final visual attention map would capture both appearance
and spatial information accordingly. In each iteration of the
cross bilateral filtering process, we calculate
AM (~x) =
1
Z
∫
AE(~x
′)Kσs(‖~x−~x′‖)Gσr (‖I(~x′)−I(~x)‖)d~x′2,
(4)
where Z is the normalization factor, ~x is a 2-d vector in-
dicating the pixel location, I is the intensity of the original
image in gray scale, Kσs and Gσr are gaussian kernels act-
ing in the spatial and intensity domains, respectively. AM
is the final visual attention map which we use for object lo-
calization. An example of the visual attention map of Fig.
2a is shown in Fig. 2e.
4. Visual Attention Map for Object Catego-
rization
We now discuss how to apply our visual attention map
to improve the state-of-the-art BOF model for object recog-
nition. Prior work using the standard BOF model assigns
equal weights to each of the visual words in an image, and
thus all visual words are considered equally important in
classifier training and testing. Since our visual attention
map can be considered as a probability map, where each
entry is a value between 0 and 1 and indicates how likely
the corresponding pixel in the original image belongs to the
object category of interest. To utilize this map for improved
Figure 4. The confusion matrices of recognition rates using (a) the standard BOF model and (b) our BOF model spatially weighted by the
visual attention maps constructed via L2-regularized L1-loss linear SVM. (c) our BOF model spatially weighted by the visual attention
maps constructed via L1-regularized L2-loss linear SVM.
is the same as prior work using this dataset. The number of
visual words K in codebook and visual word extraction is
300. From our observations, the learned visual attention
maps and the localization results are not sensitive to the
choice of K from 300 to 1000. Therefore, for the inter-
est of computation time, we choose a small K = 300 in this
report.
Figures 5 and 6 show some examples of the visual at-
tention maps (for the training images) learned by L2 and
L1-regularized SVMs, respectively. It is worth repeating
that, throughout this report, only the knowledge that the
foreground object exists in each image is used in training.
Without any ground truth masks given, our approach is able
to learn the visual attention maps for different objects and to
locate them in a large set of image collections. Objects with
different types of boundaries, shapes, or structures are suc-
cessfully emphasized by our visual attention maps. If some
threshold is used, segmented results of the visual attention
maps can be provided, and thus more detailed localization
results can be obtained for further uses. Note that the seg-
mented localization results are also shown in Figures 5 and
6, where a threshold 0.5 is used.
To locate the object of interest in a query image, we first
infer its class label by (5), then we apply the associated vi-
sual attention map to the query image. We note that, com-
paring the localization results of training images shown in
Figures 5 and 6, there is only negligible difference between
the visual attention maps learned by L2 and L1-regularized
SVMs. As we discuss later in Sect. 5.2, while both cases
significantly improved the standard BOF model in recogni-
tion, the one constructed by L1-regularized SVMs achieved
marginally better results than the case of L2-regularized
SVM did. Therefore, we choose to present localization re-
sults of query images in Fig. 7, which uses the visual atten-
tion maps learned by L1-regularized SVMs.
5.2. Object Recognition
After locating the objects of interest and weight their vi-
sual words, we design a set of one-against-all SVMs to rec-
ognize the eleven object classes from this dataset. Recall
that, while the standard BOF model treats all visual words
equally important, our approach identifies class-specific vi-
sual words and assigns larger weights to them, and those
without discriminating information or extracted from back-
ground clutter are suppressed by smaller values.
Fig. 4 illustrates the confusion matrices of recognition
results using the standard BOF models with dense SIFT de-
scriptors, and ours weighted by visual attention maps con-
structed by L2 and L1-regularized SVMs, respectively. The
parameter C for each one-against-all SVM is selected via
cross validation for all cases. From Fig. 4, we see that our
approach achieves better mean average precision (MAP)
than the one using the standard BOF model (60.36% and
59.27% vs. 54%). Comparing the diagonal entries in these
confusion matrices, it is clear that the recognition rate of
each object class is improved by our method, while the one
using the visual attention map learned by L1-regularized
SVMs outperforms all cases. This is expected, since L1-
regularized SVMs are capable of producing a sparse solu-
tion (i.e. the norm vector w), which identifies a small set of
features (i.e. visual words for object recognition) with sig-
nificant discriminating information. This explains why the
L1-regularized SVM is considered to be preferable for fea-
ture selection applications, including our extraction of dis-
criminating visual words. Furthermore, as mentioned ear-
lier, a sparse solution of the SVM also reduces the time to
calculate the inner products in (2), which is more computa-
tionally efficient and thus is preferable for large-scale clas-
sification problems.
Figure 7. Examples of object localization results of the test images using L1-regularized SVMs.
[6] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T.
Freeman. Discovering objects and their location in images.
In ICCV, 2005. 1
[7] L. Cao and L. Fei-Fei. Spatially coherent latent topic model
for concurrent segmentation and classification of objects and
scenes. In ICCV, 2007. 2
[8] T. Yeh, J. J. Lee, and T. Darrell. Fast concurrent object lo-
calization and recognition. In CVPR, 2009. 2
[9] Z. Tu, X. Chen, A. L. Yuille, and S.-C. Zhu. Image parsing:
unifying segmentation, detection, and recognition. In ICCV,
2003. 2
[10] H. Harzallah, F. Jurie, and C. Schmid. Combining efficient
object localization and image classification. In ICCV, 2009.
2
[11] I. Ulusoy and C. M. Bishop. Generative versus discrimina-
tive methods for object recognition. In CVPR, 2005. 2, 3
[12] C. Desai, D. Ramanan, and C. Fowlkes. Discriminative mod-
els for multi-class object layout. In ICCV, 2009. 2
[13] M. Marszaek and C. Schmid. Spatial weighting for bag-of-
features. In CVPR, 2006. 2
[14] B. Leibe, A. Leonardis, and B. Schiele. Combined object cat-
egorization and segmentation with an implicit shape model.
In ECCV workshop on statistical learning in computer vi-
sion, 2004. 2
[15] P. Viola and M. Jones. Rapid object detection using a boosted
cascade of simple features. In CVPR, 2001. 2
[16] O. Chum and A. Zisserman. An exemplar model for learning
object classes. In CVPR, 2007. 2
[17] M. Marszałek and C. Schmid. Accurate object localization
with shape masks. In CVPR, 2007. 2
[18] D. G. Lowe. Distinctive Image Features from Scale-Invariant
Keypoints. IJCV, 60, 2004. 3
[19] F. Jurie and B. Triggs. Creating efficient codebooks for vi-
sual recognition. In ICCV, 2005. 3
[20] B. Fulkerson, A. Vedaldi, and S. Soatto. Localizing objects
with smart dictionaries. In ECCV, Berlin, Heidelberg, 2008.
3
[21] C. Cortes and V. Vapnik. Support-vector networks. Machine
Learning, 20, 1995. 3
[22] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selec-
tion for cancer classification using support vector machines.
Machine Learning, 46, 2002. 4
[23] Y.-W. Chang and C.-J. Lin. Feature ranking using linear svm.
In JMLR Workshop and Conference Proceedings: Causation
and Prediction Challenge, volume 3, 2008. 4
[24] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–1874, 2008.
4
[25] Q. Yang, R. Yang, J. Davis, and D. Nister. Spatial-depth
super resolution for range images. In CVPR, 2007. 4
[26] F. Durand and J. Dorsey. Fast bilateral filtering for the dis-
play of high-dynamic-range images. SIGGRAPH, 2002. 4
[27] N. Cristianini and J. Shawe-Taylor. An introduction to sup-
port vector machines : and other kernel-based learning
methods. Cambridge University Press, March 2000. 5
[28] J. Yang, K. Yu, Y. Gong, and T. S. Huang. Linear spatial
pyramid matching using sparse coding for image classifica-
tion. In CVPR, 2009. 5
[29] J. C. van Gemert, J. M. Geusebroek, C. J. Veenman, and
A. W. M. Smeulders. Kernel codebooks for scene catego-
rization. In ECCV, 2008. 5
 6
tracking & categorization、detection and recognition、image sensing、segmentation & texture
等。CVPR 論文篩選嚴格，審稿以雙盲 (double blind)方式進行，每屆能錄取的論文品質
皆有一定的水準，在電腦視覺、影像與影片信號處理等主題領域當中都極具代表性與貢
獻。CVPR 當中，不論是上台口頭或是以海報展示形式報告的講者，不但實質內容充分，
講者準備上的態度也十分嚴謹，這是在一些論文品質良莠不齊的研討會中所看不到的。
也由於論文品質的提升，會議上各主題的論文皆吸引與會者相當大的注意，講者與聽眾
學術意見的交流與互動也非常頻繁，讓今年 CVPR 不管是口頭或海報論文發表的場合都
更加熱絡。 
 
二、與會心得 
在今年主要會議進行的 6月14日至6月 17日共五天其間，口頭論文發表形式以 single 
track 的形式進行，而海報展示、作品展示互動與廠商科技展覽等，則在同個時段內進行。
由於議程如此安排，可以讓所有與會者能夠親自聆聽所有口頭報告的論文發表，也不會
錯過其他想參與的海報或作品展覽，讓與會者有充分的時間互動交流。這也是其他相關
領域會議所以值得學習的部分。今年 CVPR 針對海報論文報告部分，更增加 poster 
spotlight presentation，讓海報論文發表者有機會上台，在 90 秒內推銷自己的論文以吸引
與會者的興趣，讓海報論文發表的場合更加熱絡。這點也值得很多會議學習，讓更多
poster 發表者有機會直接跟全場介紹自己的研究成果。 
本人參與會議期間積極參與所有場次，並且親自和與會者及報告者互動，以吸取其
他優秀研究學者的經驗。也由於藉由這樣一個跟各地傑出團隊互動的經驗，為本人往後
的研究方向有了更多啟發與方向。參與完整個會議之後發現，主要研究成果集中與歐美
無衍生研發成果推廣資料
已獲得件數 0 0 100%  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 1 1 100% 
人次
 
其他成果 
(無法以量化表達
之成果如辦理學術
活動、獲得獎項、
重要國際合作、研
究成果國際影響力
及其他協助產業技
術發展之具體效益
事項等，請以文字
敘述填列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
