 II
目錄 
 
中文摘要及關鍵詞(keywords)-----------------------------------------------------------------------------1 
英文摘要及關鍵詞(keywords) ----------------------------------------------------------------------------2 
 
前言                     -----------------------------------------------------------------------------3 
研究目的                 -----------------------------------------------------------------------------3 
文獻探討                 -----------------------------------------------------------------------------4 
研究方法                 -----------------------------------------------------------------------------5 
結果與討論（含結論與建議）----------------------------------------------------------------------------7 
 
Fig. 1. The proposed schematic structure of ARTC for node i; bold lines indicate the path of 
signals and thinner lines represent the path of a single scalar.------------------------------5 
 
Fig. 2. Comparison of packet delivery fraction -----------------------------------------------------10 
 
Fig. 3. Comparison of routing overhead --------------------------------------------------------------10 
 
Fig. 4. Comparison end-to-end delay ------------------------------------------------------------------11 
 
 
Table I. The specifications of variables of ARTC for node i. ---------------------------------------5 
 
Table II. Simulation Parameters  ----------------------------------------------------------------------8 
 
References --------------------------------------------------------------------------------------------------12 
 
本計畫成果發表於  2007安全管理與工程技術國際研討會 
 
 2
英文摘要 
Mobile wireless networks, called Ad-hoc networks, are networks with no infrastructure. A 
node communicates directly with the other nodes within adequate radio propagation and 
indirectly through multi-hope routing with all others. The route lifetime value is one of the most 
important parameters for the design of an on-demand ad-hoc routing protocol. This is to ensure 
that the routing table does not attempt to discover a new route and/or delete an existing active 
route within its lifetime. Protocols that used the adaptive route lifetime method found interesting 
results in minimizing Traffic overhead, Average End-to-End Delay, and Invalid Route Ratio. 
In this project, a reinforcement learning scheme for solving route lifetime value of mobile 
wireless Ad-hoc networks is proposed. We adopted a simple and robust reinforcement learning 
scheme, which consists of two sub-systems: an Expectation-return Predictor (EP) is a long-term 
policy evaluator and the other is a short-term rate selector which is composed of an Action-value 
Evaluator (AE) and a Stochastic Action Selector (SAS), to solve the problem. The reinforcement 
learning scheme is mainly composed of two neural networks. Route lifetime value can be 
adaptively determined by on-line learning. 
 
Keywords: wireless Ad-hoc networks, reinforcement learning, route lifetime. 
 
 
 
 
 
 
 
 4
mathematical tools to determine the values of adaptive route lifetime. In the following, the 
concepts and factors that will be used in a RL system with AODV are introduced and the method 
to evaluate its performance is also presented.  
a. Effect of path length on ART 
In mobile ad-hoc networks, node mobility causes paths between nodes to break frequently. 
Although using more hops may reduce the distance between paths, the increasing number of 
hops also introduces greater risk of route breakage. When the number of hops between the 
source and destination is high, the probability that the path will break because of node 
movement is also high. 
b. Effect of node mobility on ART 
Ad-hoc networks experience dynamic changes in network topology because of the unrestricted 
mobility of the nodes in the network. If the end nodes (source and destination) move frequently, 
then it is highly probable that their path will break. 
c. Effect of node transmission power on ART 
The routes lifetime used by nodes of ad-hoc network is highly sensitive to the transmission 
power of those nodes. Transmission power is the strength with which the signal is transmitted. 
Increased transmission power means larger transmission range. If the transmission power of a 
node is lower, then its signal will reach to few neighbors only and its links with those 
neighbors may have very weak and easy to break. Higher transmission power of a node will 
lead to high average number of its neighbors, and hence increase the lifetime of its routes. 
 
文獻探討 
Routing protocols can be classified as proactive and reactive. A proactive protocol constantly 
updates the routing table of each host so as to maintain a (close to) global view on the network 
topology. One representative proactive protocol is the DSDV (destination-sequenced 
distance-vector) protocol [8]. On the contrary, a reactive protocol searches for a path in an 
on-demand manner. This may be less costly than a proactive protocol when hosts mobility are 
high. Representative reactive protocols include DSR (dynamic source routing) [5], ZRP (zone 
routing protocol) [3], CBR [4], and AODV (Ad Hoc On Demand Distance Vector) [6]. A review 
of unicast routing protocols for MANET is in [8].  
A direction-prediction method is proposed in [9], where the lifetime of a link is defined based on 
the current locations, roaming velocities, and roaming directions of the two neighboring hosts. 
Then, the lifetime of a routing path is defined to be the minimum lifetime of each link in the path. 
In this paper, we present a RL scheme based on the current locations, relative roaming velocities 
between two neighboring hosts, and hop counts to predict the lifetime of a route in a MANET. 
We assume that each mobile host roams around following the random walk model. This differs 
from most existing works which directly calculate the lifetime of a wireless link based on the 
current locations and roaming directions of two neighboring hosts by assuming that their roaming 
directions do not change. Based on our analysis, extensive numerical and simulation results are 
presented to show this fact. 
 
 6
Topologically, RLCC consists of two neural networks. The state vector s is only composed 
of the state Hc, Rm, and d. The EP and the AE networks receive the state vector to produce an 
optimal control signal ART in response to the status of MANET. In practice, temporal difference 
(TD) methods learn their estimates in part on the basis of other estimates, that is, they learn a 
guess from previous experience. Since the process generates reinforcement signal rt at each 
transition from state st to state st+1, we may want Vt, which is the output of EP at state st, to 
predict the associative discounted cumulative reinforcement cost (or long-term utility index), 
0
k
t t kk
Q rγ∞ +==∑ , where γ  is the discounted factor, 0 1γ≤ ≤ . If γ = 0, the controller is 
“ myopic”. As γ  approaches 1, the controller becomes more farsighted. If the prediction of Vt 
should equal Qt, i.e., the predictions are accurate, then 
 
1 1 2 1 1
0 0
.k kt t k t t k t t
k k
V r r r r Vγ γ γ γ∞ ∞+ + + + + + +
= =
= = + = +∑ ∑  (1) 
 
Based on the above equations, TD error can be defined as the difference between the two 
sides of this equation, i.e., (V V′ − ). For better results, we combined TD methods with eligibility 
trace to obtain a more general method that may learn more efficiently. An eligibility trace is a 
temporary record of the occurrence of an event. The trace marks the memory parameters 
associated with the event as eligible states or actions are assigned credit or blame for the error. 
According to the TD algorithm, if V V′ >  which implies that the action ART is found to be better 
than previously expected, then the policy is modified to increase the merit of the action. On the 
other hand, if V V′ < which means that the policy is modified to decrease the merit. Hence the  
The RL ARTC performs two tasks: the action function and the evaluation function. The AE 
element maintains a separate estimated value for each state, in order to balance exploration and 
exploitation in the process of RL. At each synapse of the AE element exists two trace signals: the 
long-term trace Wij (t) that evaluates the performance of the jth action taken for state i and the 
short-term trace ˆ ( )ije t  that is required to update the long-term trace. The update rules of AE are 
 
? ( 1) ( ) (1 ) ( )ij ij ie t e t x tλ λ+ = + −                (2) 
?( 1) ( ) α ( ) ( ),ij ij ijw t w t r t e t+ = +                 (3) 
      
where λ is the AE trace decay rate; α is the positive learning rate; ˆ( )r t  is the internal 
reinforcement signal; ˆ ( )ije t  is the eligibility function at time t of input pathway ij which 
indicates how much credit an earlier state has for the result of current system status.  
The SAS element determines an action ART as follows: 
 
 8
states that the received signal strength is inversely proportional to the node distance square. The 
simulation analysis was performed with the parameters summarized in Table II. Each simulation 
run takes 300 simulated seconds. Each data point represents an average of at least five runs with 
identical traffic models, but different randomly generated mobility scenarios. For fairness, 
identical mobility and traffic scenarios are used across protocols. The random waypoint model 
was adopted for the mobility model. In this mobility model, a node randomly selects a destination. 
On reaching the destination, another random destination is targeted after different pause time in 
range of 0 to 300 seconds. The speed of movement of individual nodes ranges from 0 to 20 m/s. 
The direction and magnitude of movement was chosen from a uniformly distributed random 
number. 
 
Table II. Simulation Parameters 
Map and Hosts Physical Layer 
Map size  1500m x 300m 
Channel           11 Mb/s 
Bandwidth IEEE    802.11b 
Number of hosts   50 Channel Delay     10 μs 
Host enabled to 
Transmit 
5 
Channel Error 
Probability   
1 bit on 610  
 
Routing Layer Application Layer 
Control 
Message Size 
64 
byte 
Enabled Node         10 
HELLO  Interval    1 s 
Message packet 
Size 
512 byte 
Allowed HELLO  loss 2 Burst length      64 packets 
Delete period        4 s Send Packet Rate  3/s 
RREQ max 
Trials 
3 Burst Interval 
Normally 
Distributed 
in [0.1,3] 
1. On reaching the destination, another random destination is targeted after 
different pause time in range of 0 to 300 seconds.  
2. The speed of movement of individual nodes range from 0 to 20 m/s. 
3. The direction and magnitude of movement was chosen from a uniformly 
distributed random number. 
 
B. Performance Metrics 
Three key metrics were used for measuring performance: 
a. Packet delivery fraction: ratio of the data packets delivered to the destination to those generated 
by the CBR sources 
b. Routing Overhead: Number of routing packets by source to Number of received data by 
destination. This metric can be employed to estimate how many transmitted control packets 
 10
0 50 100 150 200 250 300
90
91
92
93
94
95
96
97
98
99
100
Pause time (seconds)
P
ac
ke
t D
el
iv
er
y 
R
at
io
 (%
)
10 enabled Nodes in 50 mobile Nodes 
Normal- AODV
RL- AODV    
LR- AODV    
 
Fig. 2. Comparison of packet delivery fraction 
 
0 50 100 150 200 250 300
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Pause time (seconds) 
R
ou
tin
g 
O
ve
rh
ea
d
 10 enabled Nodes in 50 mobile Nodes 
Normal- AODV
RL- AODV    
LR- AODV    
 
Fig. 3. Comparison of routing overhead 
 
