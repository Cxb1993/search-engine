all ROIs have been covered. The ROI is scanned at 10 
scales each a factor of 1.25 larger than the last.
Step 4: To locate the potential candidate, the rectangle 
feature of W u H pixels in the feature template can be 
computed as follows  
|),(|][
0,0
¦
dd
 
HyWx
yxdi
DWF
LH
,                     (1) 
where d
DWF
LH  is the nonsubsampled version of discrete 
wavelet transform (DWF, discrete wavelet frames) in 
the LH horizontal subband and }...,,1{ Ii , I = 9 is the 
number of  rectangles in the proposed feature template. 
At each location within ROI, check the decision rule of 
equation (2) that indicates whether the face template is 
satisfied or not. 
 1  2  3 
 4  5  6 
 7  8  9 
( [1] > [4] )  && ( [3] > [6] ) &&  ( [1] > [2]  ||  [3] > 
[2] )  && ( [5] > [4] ) && ( [5] > [6] ) && ( [8] > [7] ) 
&& ( [8] > [9] )                                                             (2) 
Step 5: Merge templates that can be counted as 
enclosing the same face since it is common to have 
multiple findings with small displacements horizontally 
and/or vertically.  
Step 6: Normalize all the candidate regions – resizing to 
24 u 24 pixels and using histogram equalization. 
Step 7: Classify the face candidate using the SVM
classifier. If the class corresponds to a face, we draw a 
rectangle around the face in the output image. 
Step 8: Normalize the detected face rectangle to size 
40×60 pixels .  An entropy-based smoothing filter is 
introduced to move the center from pixel to pixel in the 
rectangle to remove the coefficients located at outside 
the facial component regions. This continues until all 
pixel locations have been covered and facial component 
object are to be created for extraction. The entropy 
filtering of the pixels in the 3 × 3 (N  =  3)  
neighborhood defined by the mask is given by the 
expression 
),(log),(
1
_
1
0,
2
yxdyxd
N
entropyWavelet DWFLH
N
yx
DWF
LH¦

 
 . (3)
According to anthropometry, we could perform the 
inter-orientation projection along the horizontal and 
vertical axes respectively to locate human eyes and 
mouth. Search for the centroid in the facial component 
region, we approach region segmentation by finding 
meaningful boundary based on point aggregation 
procedure. Choosing the center pixel of the component 
region is a natural starting point and grouping points to 
form the region of interest with paying attention to 4-
connectivity would yield a clustering result, when no 
more pixels for inclusion in the region. After growing, 
the region centroid is relocated and therefore eyes, 
mouth, and face size are also known. An adaptation is 
finally carried out to mark the eyes and mouth positions 
and refine the bounding rectangle as an ellipse of fitting 
facial oval shape (Fig. 1(b)). 
    Note that one needs to do more works as the 
following step for the pose estimation of human face 
while the final objective of face segmentation is 
performing face recognition. 
Step 9: The pose estimation procedure presented in Fig. 
2 is defined in the image plane. The points ),( LL yx  and 
),( RR yx  as displayed in the following illustration are 
the coordinates of the left and right eyes, respectively, 
and the point ),( MM yx  is the center of the mouth. We 
define the pan T  of equation (3) as the angle between 
the lines passing through the right eye and the center of 
the mouth and passing through the left eye and the 
center of the mouth. The face image with rotation in 
plane T  can be rotated back to the upright position 
(orientation alignment) and then the corresponding 
view-based classifier based on the skin color ratio 
between the left cheek and the right cheek can be 
chosen for recognition. The three multiview face 
classifiers are built according to the following angles: -
60q~-20q, -20q~+20q, +20q~+60q. The color ratio range 
is determined as (-1.2, +1.2) for the frontal view face 
recognition.  
ST /180
2/)(
2/)(
tan 1 u¸¸¹
·
¨¨©
§

 
LRM
LRM
yyx
xxx .           (3) 
),( MMx
),( MyMx
),( RyRx
),( LyLx
T
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  © 2006
Y.Rodriguez, K.Kryszczuk, J.Czyz, L.Vandendorpe, J.Ng, 
H.Cheung, and B.Tang. “Face authentication competition on 
the BANCA database,” ICBA, Lecture Notes in Computer 
Science, vol. LNCS 3072, pp. 8-15, 2004. 
[11] http://www.humanscan.de/. 
[12] http://www.vision.caltech.edu/html-iles/archive.html. 
Fig. 1.  Function block diagram of the proposed approach. 
Input
Orientation 
Alignment
Segmentation
Output
Fig. 2.  Pose estimation. 
 (a)  (b) 
ʳ(c)
(d)
Fig. 3.  Live detection: (a) dim lighting, (b) bright lighting, (c) in crowds; and (d) photo detection.
FACE
CLASSIFICATION
WAVELET
FRAMES
(a) Input image; 
SKIN COLOR 
DETECTION
FACE LOCALIZATION
FACE
SEGMENTATATION
(b) Output image. 
The 18th International Conference on Pattern Recognition (ICPR'06)
0-7695-2521-0/06 $20.00  © 2006
 二、會議行程與內容 
此次會議自 0112-0114 共三天，會議開始前兩日為 tutorial section，主題涵
概網路技術、智產權管理、動態影音視訊壓縮等技術。本人之報告係被排
在 0113 的最後一個時段，所以聽眾顯得較少些，不過議程主席 Akiomi 
Kunisa (來自三洋北美研究中心)也都會提問以增加彼此間的互動熱鬧氣
氛。由於這個會議中心經常舉辦國際性的展出活動因此面積相當遼闊，每
間簡報室的空間也比一般在飯店舉辦會議的議事廳大上許多，再加上同時
有許多議程進行人員分散各處，所以感覺起來較不熱鬧。這次恰逢 25 週年
慶，因此大會在換場休息的餐點中也特別準備了印有 icce25 的大蛋糕供出
席人員享用以示慶祝。 
icce 之主題較偏重於消費電子相關領域，與本人在完全無損失浮水印之
研究範疇極為接近。與會者有些是以前在相關文獻上曾見過名字的作者，
親睹其本尊之演說另有一番感受。此外，本人更趁此機會到海報展示區去
瞭解其他作者在這類主題之研究成果，與他們交換心得並討論此技術未來
之發展方向，收穫可謂豐富。 
 
三、結論 
由於一般的浮水印技術必需修改原始影像的內容，往往會造成部份資訊的
失真。而本人提出的完全無損失浮水印技術則克服了這個缺點，在醫學影
