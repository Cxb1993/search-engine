2行政院國家科學委員會專題研究計畫成果報告
隱私相機: 自動地把路人從無預警的拍攝裡消除的相機
Privacy-aware cameras: automating the removal of accidental
passers from unintentional cameras
計畫編號：NSC 95 － 2221 － E － 002 － 150 －
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日
主持人：朱浩華 國立臺灣大學資訊工程學系
計畫參與人員：吳昊極 林東昀 游創文 臺灣大學資訊工程學系 網路與多媒體研究所
Abstract
Digital cameras have become one of the most widely
used gadgets in our everyday lives to capture and share
moments of day-to-day experiences, anytime, anywhere.
However, this ubiquity of cameras also raises serious
social privacy concerns for many people who wish to
retain their privacy and secrecy. In this project, we
propose the privacy-aware cameras to address one
aspect of this privacy concern–in the user of digital
cameras in public spaces where camera snapshots may
unintentionally catch co-located and walk-by passers.
By tracking the whereabouts of passers in the proximity
of a camera, our cameras can help to filter them out.
We have prototyped this privacy-aware camera and
performed live experiments. Experimental results
have shown that it works well given a small number
of passers.
I. Background
Digital cameras have become one of the most widely
used gadgets in our everyday lives to capture and share
moments of our day-to-day experiences, anytime, any-
where. Given the small form factor and low hardware
cost, digital cameras have become popular additions in
most of the mobile phones sold nowadays. Some new
mobile phones even have dual cameras –one camera
facing the front and the other one facing the back.
Based on reports from IDC [6], camera phones have
made up more than half of all mobile phones sold in
Europe and Japan in 2004. In the U.S., camera phones
are expected to reach past the 50% mark at the end of
2005. A parallel but complementary development is in
the high storage (1+ gigabyte) of flash memory cards
that can be plugged into these camera phones to store
almost unlimited number of high quality pictures. To-
gether, they enable a new freedom of digital photogra-
phy to snap pictures at every mindful or sometimes
mindless moment. However, this new freedom of digi-
tal photography also raises serious social privacy con-
cerns for many people who wish to retain their privacy
and secrecy. People are now living in public spaces full
of such privacy-intrusive digital cameras and camera
users.
In this project, we propose a technical solution to ad-
dress one aspect of this privacy concern –in the use of
digital cameras in public spaces where camera snap-
shots may unintentionally catch co-located and walk-by
passers who strongly object to any parts of them being
captured in other people’s pictures. Note that our solu-
tion is targeting for unintentional capture, not inten-
tional capture [1]. Unintentional capture means that the
picture takers have no intention of capturing these
passers in pictures. As a matter of fact, the picture
takers may consider these accidental passers as unde-
sirable background noises or unwanted intrusion ad-
versely affecting the picture quality.
Our proposed solution is called the privacy-aware
cameras. Cameras are enhanced to become (location)
aware of the presences of privacy-concerning passers in
the surrounding environments. Given this awareness,
whenever a picture is taken, the privacy-aware camera
can protect the passers’ privacy by automaticaly re-
moving the parts of the pictures that show the passers.
Although this idea seems sensible for the passers, it
raises a practical question for the camera owners–what
is the incentive for the picture takers to buy cameras
that help protect only the privacy of passers? Worse,
the removal of passers from the pictures can leave
odd-looking blank hole(s), which can further degrade
the quality of the pictures. To address this incentive
question, we must find an attractive benefit for the
picture takers. Our solution must ensure that the re-
movals of the passers are clean and invisible, as if the
picture is taken without the passers physically being
there. With this benefit, the picture takers can enjoy the
freedom in taking pictures at any time without having to
worry about passers, considered as undesirable back-
ground noises, showing up in their pictures. This bene-
fit can be illustrated in Figure 1. To provide this clean
and invisible passer removal, we incorporate a pictorial
composition technique called photomontage [7] that
combines additional picture(s) taken shortly after (refer
to these additional pictures as “backgroundframes”), to 
seamlessly and realistically repair the missing back-
ground left by the removed passers.
4We acknowledge this fact, and consider our pri-
vacy-aware cameras as an early effort to address this
problem. Since our work is yet a perfect solution, we
would like to state our assumptions.
 We assume that an existing localization infra-
structure is installed in the public space to locate
passers and cameras. There are a variety of
choices for indoor and outdoor localizations such
as GPS, WiFi, Zigbee, cellular network, ultra-
sound, RFID, etc. Most of these localization sys-
tems require each passer, target, and picture-taker
to wear or carry a mobile unit. We argue that this
assumption is realistic, considering that this mo-
bile unit can be incorporated into a mobile phone,
which is carried by most of people nowadays.
 In addition to carrying this mobile unit, each
passer and target is willing to inform nearby
cameras about his/her current location relative to
the camera under masked identify. We argue that
this assumption is not impractical, considering
that cameras only know that “there is a passer 
(person) standing at some degree angel to the
camera”. The cameras do not need to know the 
identity of any passer(s).
 Each passer and target sets his/her capture pref-
erence, as to which cameras are allowed to cap-
ture his/her image. This capture preference is
specified by each user on his/her mobile unit. By
default, the capture preference will answer to
no-capture to all unknown cameras.
 The privacy-aware camera (camera phone) is
embedded with sufficient processing and memory
capabilities to run low-resolution vision tracking
algorithm in real time and image processing algo-
rithm (photomontage) in non-real time.
IV. Design and Implementation
The design of our privacy-aware camera is shown in
Fig. 2. Its design consists of the following three phases:
Vision processing phase, Localization phase, and Im-
age processing phase. A general overview of these
phases is described using the following example: Joe, a
picture-taker, is taking a picture while Jane, an acci-
dental passer, walks into the picture. In the first phase,
when Joe clicks on the snapshot button on his camera,
vision processing system is invoked to track the posi-
tion of Jane and mark her pixel areas within the camera
view at the time of picture-capture. At the same time,
Joe’s camera broadcasts a message to Jane (and any al 
nearby persons), asking for her locations and capture
permission. In the second phase, Jane’s mobile unit 
responds to Joe’s camera about her location with a 
masked identify, e.g., “a passer at location <x,y> does
not wish to be captured by your camera”. Joe’s camera 
can then correctly label Jane’s pixel area within camera 
view as a passer, by combining Jane’slocation (at the
horizontal plane) relative to the camera and Jane’s 
position (at the vertical plane) within the camera view.
In the third phase, the camera automatically takes a
background frame when Jane walks outside of her pixel
region. Then, the camera runs the image processing
system, photomontage, to seamlessly repair the Jane’s 
pixel area in the original picture, using realistic back-
grounds from the background frames. The result is that
(1) Jane, the passer, is happy because she is protected
from unintentional capture, and (2) Joe, the pic-
ture-takers, is also happy because his picture is free
from Jane’s intrusions. More details about the three 
phases are elaborated as follows.
Fig 2.The design of privacy-aware camera
Vision processing phase
The first phase is the vision processing phase. This
phase is started when the picture taker is clicking on the
camera’s snapshot buton. Two tasks are performed 
during this phase: (1) tracking the positions of passers
& target in the picture’s vertical plane, and (2) inferring 
the relative angles between all passers and the camera
in the horizontal plane. The relative angles are needed
in the next localization phase to correctly label passers
in the picture for removal.
To track the positions of passers (and/or target) in the
camera view, we combine two vision algorithms: mo-
tion factor and human detection. The motion factor
facilitates the tracking of moving passers/targets con-
tinuously; whereas the human detection enables the
tracking of stationary passers/target in the camera. We
use the Intel’s OpenCV software package [12], which 
contains library for both motion factor tracking and
human detection.
Given the positions of passers and target in the camera
vertical view, we can calculate the relative angles be-
tween passers/targets and the camera on the horizontal
plane. The angels can be computed as follows. For each
camera, we calibrate a linear mapping function from a
passer’s pixel count (to the picture’s middle line on the
vertical plane) to his/her camera angle (on the horizon-
tal plane). This mapping function can be illustrated in
Fig 3.θis the angle between the camera and the passer
Camera Orienta-
tion Algorithm
Label passers Composition
Localization
system
Human tracking
Capture back-
ground
Segmentation
Vision process-
ing phase
Image processing
phase
Localization
phase
6privacy-aware camera checks whether the retrieved
frames cover all backgrounds left by the passers. Then,
the algorithm ends and returns all possible frames for
the composition task.
The goal of the segmentation task is to retrieve the part
of the passers in the picture. The segmentation task will
automatically identify the contour of the passers in the
picture and select the similar color pixels from the pixel
region of passers. Then the pixel region of the passes is
selected from the picture. Finally, the pixel regions of
the passers will be removed from the picture. Then,
the composite task is performed to fill the pixel regions
of the passers by overlapping the background frames
captured previously. Finally, to make the overlapping
seamless, the edge around the passers’ holes is 
smoothed by using pixel values around the holes.
Implementation
The prototype implementation of the camera is shown
in Fig. 5. The Cricket mobile unit [13] for tracking the
locations of the passers, targets, and the picture-takers
are shown in Fig. 6. For software, we have used Intel
OpenCV [12] vision package for human motion track-
ing, Adaboost [14] for human detection, and Photo-
montage [7] for clean and seamless passers’ removal.
Fig 5 The Camera consist of a Palmtop PC and a Logitech
Webcam
Fig 6 The cricket mote
V. Experimental setup, results, and prototype
limitations
We have conducted several live experiments to evaluate
the privacy-aware camera prototype. We define the
following performance metrics for evaluation in the
experiments:
 Capture delay measures the time duration between
the time the source picture is captured and the time
all overlay frames are captured;
 Human detection success rate measures the per-
centage of all pictures from which the vision
tracking phase correctly recognizes all the humans
who come within the camera view;
 Label success rate tracks the percentage of all
pictures from which the localization phase cor-
rectly labels all passer(s) and target(s) who come
within the camera view;
 Removal success rate tracks the percentage of all
pictures from which the image processing phase
correctly applies photomontage to remove all pass-
ers who come within the camera view.
These performance metrics can be affected by a number
of factors, such as the number of passers/target in
proximity of the camera, the passers’ walking speed, 
the accuracy of the vision tracking algorithm, and the
accuracy of our indoor localization system. In the ex-
periments, we choose to analyze only the first factor
and its effect on the evaluation metrics.. This is done by
varying the number of nearby passers within & outside
the camera view. As for other factors, the passers’ 
walking speed is set to be random human walking
speed. The vision tracking algorithm and the localiza-
tion system are fixed as described in the previous sec-
tion.
Performance metrics
Scenarios Capture
delay
(seconds)
Human
detection
success
rate
Label
success
rate
Passer
removal
success
rate
1T 1P 0O 1.7 96% 92% 89%
1T 1P 1O 2 94% 83% 75%
1T 2P 0O 2.5 81% 79% 75%
1T 1P 2O 2 97% 75% 71%
1T 2P 1O 2.8 79% 71% 63%
1T 3P 0O 3.2 58% 66% 41%
Table 1: Show the results of the live experiments. The ex-
periments evaluate 4 performance metrics: delay, human
detection success rate, label success rate, and passer removal
success rate. The experiments vary the number of nearby
passers within & outside the camera view: T is the number of
target, P is the number of passers within the camera view, and
O is the number of passers who are outside the camera view.
The experiments consist of six scenarios shown in
Table 1. For each scenario, we vary the number of
nearby passers within & outside the camera view: T
(target) is the number of target person (s) within the
camera view and should not be removed; P (inside
passers) is the number of passers within the camera
