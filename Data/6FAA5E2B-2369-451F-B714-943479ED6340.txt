  共 19 頁 第 2頁
 
比方說研究人員等常面對的相關研究論文的探勘或是專利的申請。明顯的，正確
的得到相關的研究論文的資訊可以節省相關的研究投資，也可以更正確、更輕鬆
的讓研究學者得到完整的相關研究勘查（survey），另外專利提出者或是判斷專
利申請並核發專利的單位也需要高查全率或地毯式的搜尋相關的專利文件。為了
避免地毯式搜索的曠日費時，高查全率的引擎有其重要性。 
本計畫企圖建立一個在確保一定正確率下具高查全率的探勘引擎，我們結合
使用者的資訊來達到高查全率的效果。由一般研究顯示 77%的使用者在使用搜尋
引擎時，只輸入一個英文單字來查詢 [16]，且 85%的使用者輸入少於三個英文
單字來查詢 [7]，而一個英文單字通常有許多意義，在這種情況下，關鍵字搜尋
引擎傳回來的資料，往往不是使用者所需的，這種情形，我們稱之為不完備查詢
（incomplete query），使用者無法明確定義所要找的資料，使得關鍵字搜尋引擎
的效能不彰。如圖一假設我們輸入字串「race」來搜尋「賽車」的資料，一般搜
尋引擎會回傳所有包含 race 單字的資料。但因 race 含有種族與競賽的概念，回
傳的資料並不為我們所希望的資料。除此之外，各種不同的概念也可能包含各種
子概念，如同競賽概念中又包含「賽車」與「賽馬」等各式各樣的競賽概念，這
些影響因素都會使搜尋引擎的效能降低。對這些問題，我們可以使用分群技術來
進行概念切割，以提升查全率。 
我們可以將不完備查詢所衍生出的問題分為幾類。第一種情形是使用者所輸
入的關鍵字為一個同字異義詞，如上「race」，可以解釋成「種族」或者「競賽」。
我們稱這個情形是從相混淆的幾個獨立概念中找出需要的某一特定概念，如圖一
(a)所示。第二種情形是使用者所輸入的關鍵字為一個主概念（concept，例如
computer），而其所想要尋找的資料為主概念中的子概念（subconcept，例如買電
腦）。我們稱這個情形是從主概念中找出需要的子概念，如圖一(b)所示。第一種
情形和第二種情形的不同點在於第一類相混淆的兩個詞在概念上相去甚遠；而第
二類中主概念和子概念符合一般概念的階層架構（hierarchical structure）。另外一
種情形是同義詞的狀況。譬如想要買電腦，應當輸入「computer store」或「computer 
shop」似乎都不是標準答案，因為「store」、 「shop」以及「buy」都是相近詞
意的詞，如圖一(c)所示。這些問題在文件探勘中極其重要，如前所述的專利資料
探勘，針對一個概念，我們除了要儘可能精確嘗試描述所需的子概念、小心避免
同字異義詞外，我們還需要有能力將所有相近的詞都嘗試作搜尋，以免遺漏重要
的回傳結果。 
第一種和第二種的問題主要需要的是正確率提升的技術，以及利用分群
（Clustering）的方法來進行概念的切割，而分群的方法將在文獻探討中會有比
較完整的說明。第三種問題主要需要的是查全率提升的技術，在這方面，我們考
慮從主概念的搜尋，將問題轉變為第一類或第二類的問題。在本計劃中，我們先
使用向量空間模型將文件轉換成為向量空間形式，之後使用 ISOMAP 將資料維
度降低到本質維度後，再使用 k-Means 演算法對資料進行分群。各技術說明在文
獻探討中加以詳述。 
  共 19 頁 第 4頁
 
由於在英文文件中有許多字詞是具有許多型態的，如 banana 及 bananas 都
是表達同一種物品，只是在數量上不同。我們需將這些具不同型態但為同源
的詞轉變為最原始的形式，如將 bananas 轉換成 banana。表一為文件前處理
之範例。 
 
ii. Term-by-Document 矩陣： 
經過文件前處理後，我得到文件中字詞的最根本形式，但文件只是一些
關鍵字詞 (Term) 的集合，我們需再將文件轉換成電腦能處理且有意義的形
式。在此使用過濾後的關鍵字詞 (Term) 及文件索引 (document index) 來建
構代表文件集合的 Term-by-Document 矩陣，而矩陣中的數值即是加權詞頻
(weighted term frequency)。加權詞頻最簡單的做法是根據詞在每一文件
(document)中，是否有出現過給予 0 或 1 的數據。表二為前處理後的文件，
表三為使用 Term Frequency 來建構 Term-by-Document 矩陣後的結果。定義
如下： 
TF(term frequency)：TF= ijf  
其中 fij為計算第 i 個詞在第 j 篇文章中出現的次數。由於 TF 為區域性 (local) 
的 term weighting 方式。這種作法雖然簡單卻無法有效表現某一詞在文件中
的重要性。另一個最常用的做法為 TF*IDF 。IDF 主要的含意是指當某一詞 
在每一文件中出現機率都很高時，則此詞的重要性也就相對的較低；反之當 
 
表二、經過前處理後的文件 
文件一： banana cranberry apple orange 
文件二： Haagen Daze apple chocolate banana 
文件三： apple dell microsoft chocolate 
 
表三、Term Frequency 
 文件一 文件二 文件三 
banana 1 1 0 
cranberry 1 0 0 
apple 1 1 1 
orange 1 0 0 
Haagen 0 1 0 
Daze 0 1 0 
chocolate 0 1 1 
dell 0 0 1 
microsoft 0 0 1 
 
  共 19 頁 第 6頁
 
II. ISOMAP 
在一般在空間上，我們猜想資料集的分佈在一個流形 (manifold)上。當資料
呈現在高維空間上時，我們無法直接觀察到這個流形，所以我們需要透過維度降
低的技術，將高維空間上的資料點映射到較低維度空間中，使我們能觀察到這個
流形，並利用這個觀察來進行各種的應用。而最能表現這個流形的維度，被稱為
本質維度 (intrinsic dimensionality)。由於資料在高維度空間中不一定呈現線性結
構，因此若直接使用如 PCA (Principal Component Analysis) 等全域技術進行降
維，結果將無法符合我們的需求。在期中報告時我們已利用幾個資料集比較過
ISOMAP 與 LLE 的差異，在這些資料集中 ISOMAP [8] 均較能表現出資料的真
正結構，因此我們選用 ISOMAP 技術來作為我們的維度降低技術。 
ISOMAP [8] 為利用非線性的方式對特徵集進行維度降低的技術。它的目的
在同時保有局部 (local) 與全域 (global) 的不相似 (dissimilar) 性質下，透過
MDS (MultiDimensional Scaling) [17] 將資料集投影到較低的維度空間中。於新維
度空間中，各資料點間有著如同原維度空間中的不相似性質，以及適當地呈現資
料集的本質。ISOMAP 的處理過程主要分為三個部分。第一部份為局部結構的建
立；第二部份為全域資訊的建立；最後一部份為低維度空間的映射。而這三部份
將分述如下。 
 
i. 第一部份 局部結構的建立： 
在局部結構的建立方面，ISOMAP 利用 k 個最鄰近鄰居 (kNN) 建立子結
構 (sub/local structure)，並用圖 (graph) 的形式來表示，使資料點與其 k 個
最鄰近鄰居皆具有一個邊(edge)，其中，邊長 (edge length) 用相對應兩資料
點間的不相似度來表示。在此我們使用一減掉各文章間的餘弦值來當作資料
點間的不相似度。 
 
ii. 第二部份 全域資訊的建立： 
於全域，ISOMAP 利用第一部分所產出結果，透過局部子結構間的連通
(connect) 性質，便可計算任兩資料點間的最短路徑 (shortest path)，並借此
做為全域的資訊。最後會產生一個資料點間的距離矩陣 (pairwise distance 
matrix) Dx=[dx(i,j)]，而 dx(i,j)為資料點 i 與資料點 j 之間的最短路徑。關於最
短路徑的計算，我們可以採用 Dijkstra 演算法，或是 Floyd 演算法來處理。
如此一來，在經過第一部分與第二部分的處理，整體的結構上，不僅保有局
部的性質亦包含了全域的資訊。 
 
iii. 第三部份 低維度空間的映射： 
最後一部分是將第二部分所產生的 Dx，透過 MDS 來找出各資料點於新
維度 m 上的坐標 (coordinate)，並使映射後的坐標資訊滿足式子(3)，其中
Dy(p)為各資料點間於 p 維度上的映射所產生的距離矩陣，而||A||L2 為兩距離
  共 19 頁 第 8頁
 
−2.5 −2 −1.5 −1 −0.5 0 0.5 1 1.5 2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
       （Ａ） 
−1.5 −1 −0.5 0 0.5 1 1.5 2
−1.5
−1
−0.5
0
0.5
1
1.5
2
       （Ｂ） 
−1.5 −1 −0.5 0 0.5 1 1.5
−1.5
−1
−0.5
0
0.5
1
1.5
2
       （Ｃ） 
−1.5 −1 −0.5 0 0.5 1
−1.5
−1
−0.5
0
0.5
1
       （Ｄ） 
−1 −0.5 0 0.5 1
−1.5
−1
−0.5
0
0.5
1
       （Ｅ） 
−1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
       （Ｆ） 
圖二、(A)為 ISOMAP 投影到二維的結果(k=7)，（B）為 L-ISOMAP 投影到二維
的結果(k=7, l=1000)，（C）為 L-ISOMAP 投影到二維的結果(k=7, l=500)，（D）
為 L-ISOMAP 投影到二維的結果(k=7, l=250)，（E）為 L-ISOMAP 投影到二維的
結果(k=7, l=100)，（F）為 L-ISOMAP 投影到二維的結果(k=7, l=50)。 
 
IV. Incremental ISOMAP 
由於 Landmark ISOMAP 或 ISOMAP 都是建立在 batch model 之下，在處理
過程中，必須先將所有的資料一次一併放入。在某些應用上，例如搜尋引擎，資
  共 19 頁 第 10 頁
 
的比較，利用之前所敘述的方法來建構的高查全率引擎是否符合之前所謂的要
求。資料集以 Google 所獲得的資料為分析的基礎。 
 
I. 使用降維技術與否於資料集分群上的比較： 
這部份我們要試驗資料集在有無維度降低的情況下，進行分群的效果。在這
裡我們設計了兩個不同的資料集，每個資料集皆各自包含二個不同類別的文章，
而每類別的文章各佔一百篇。第一個資料集包含兩個差異性比較大的文章類別，
一類為棒球相關文章，另一類為電腦相關文章。第二個資料集包含兩個差異性比
較小的文章類別，一類為棒球相關文章，另一類為曲棍球相關文章。分群的技術
方面，我們採用 k-Means 與高斯混合模型於降維與否的資料上進行分群。兩系列
實驗結果詳述如下。於這部份的實驗中，我們所需設定變數有四。包含，一、
ISOMAP 中 kNN 的 k，在這兩個實驗中皆預設為 10；二、k-Means 中的群數值
kk-Means，在這兩個實驗中我們皆預設為 2，而起始中心點以亂數自動產生；三、
高斯混合模型中的最小群數值 kGMM-Min，在這兩個實驗中皆預設為 1；四、高斯
混合模型中的最大群數值 kGMM-Max，在這兩個實驗中皆預設為 15。 
 
i. 實驗一：資料集中包含兩個差異性比較大的文章類別各一百篇文件，一
類為棒球相關文章，另一類為電腦相關文章。首先，我們嘗試的將原空
間上的資料分佈透過 MDS 映射到二維的平面上，如圖三(A)，從這個圖
中我們不難看出兩類的文件間有著不差的可分離 (separable) 性質，唯有
少數的資料點是重疊的，這也滿足了我們認為這資料集的文件為異性比 
 
(A) 
 
 
(B) 
圖三、將棒球相關文章(藍◇)與電腦相關文章(紅+)進行分群，(A)利用 MDS 將原
空間中的距離關係映射在二維的空間上，(B)經 ISOMAP 處理後並映射到二維空
間上的分群結果(k=10)。 
  共 19 頁 第 12 頁
 
(A) 
 
 
(B) 
圖四、 將曲棍球相關文章(藍◇)與棒球相關文章(紅+)進行分群，(A)利用 MDS
將原空間中的距離關係映射在二維的空間上，(B)經 ISOMAP 處理後並映射到二
維空間上的分群結果(k=10)。 
 
表七、曲棍球與棒球相關文章分群正確率評估 
分群方法 k-Means 高斯混合模型 
維度 原維度空間(維度：4916) 降維後空間(二維) 降維後空間(二維)
平均 Fmicro 0.5835 (0.7500) 0.9010 (0.9050) 0.8855 (0.8900) 
平均 Fmacro 0.5877 (0.7525) 0.9032 (0.9075) 0.8865 (0.8903) 
 
接著我們從分群正確率來看整體分群的成效如何，我們一樣將此實驗重
覆進行 10 次，並以 10 次實驗的平均正確率來表示各個分群結果的效果，
而正確率的評估方法與實驗一同相。實驗結果如表七所示。由於高斯混
合模型在原維度空間中無法完成實驗，所以在表七中沒有此實驗的結果
數據。關於正確率數值的部份包含兩個數值，括弧外的數值為 10 次實驗
的平均正確率，而括弧中的數值為 10 次中最佳的正確率。從實驗結果
中，我們發現原維度空間中所進行的分群結果相當不穩定，最好的單次
正確率為 0.75，但平均正確率卻只有約 0.58。反觀在降維後的分群結果，
不僅具有較好的正確率，亦呈現了相當穩定的結果。 
由實驗一與實驗二中，我們証實了降維技術對兩類相似的文件集或兩類
不相似的文件集，不僅於分群有著較穩定的結果，亦使正確率有一定程
度的提升。另外值得一提的是，高斯混合模型於此實驗中相當的成功，
不僅自動的從中資料集中找到正確的群數，亦在分群正確率上有著相當
優秀的表現。 
 
  共 19 頁 第 14 頁
 
在這部分我們進行了兩個實驗，第一個實驗利用“20_Newsgroups” dataset 中
的四類別文章 : alt.atheism、rec.autos、rec.sport.baseball、rec.sport.hockey 來做測
試，第二個實驗我們以 Google 所搜尋得到的資料來做測試驗證高查全率的搜尋
方法，我們相信 Google 搜尋引擎所得到的結果將可以推廣至其它如 Yahoo 等的
搜尋架構。 
 
−2.5 −2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
圖六、依照圖五將這 2700 篇文章利用 ISOMAP 投影到二維的結果，紅色代表
alt.atheism 藍色代表 rec.autos 綠色代表 rec.sport.baseball 黃色代表
rec.sport.hockey。 
 
i. 實驗一：假設我們目標在於尋找關於曲棍球的文章，先將四類文章 (2700
篇) 經 ISOMAP 做處理，隨即以 K-means 將之分為兩群 G1 & G2，再從
兩群中判斷哪一群和我們想要的文章比較有相關，再以此群為出發點遞
迴，直到找到我們想找的群為止。在這組實驗中，我們得到曲棍球文章
的正確率 (precision rate) 為 0.9111、查全率 (recall rate) 為 0.9225，同
時我們也將四類別的文章都區分開來，對於每類別文章的平均正確率 
(average precision rate) 為 0.9449、平均查全率(average recall rate) 為
0.9401。圖五顯示此四類文章的階層關係，圖六表示利用 ISOMAP 投影
到二維的結果，圖七為過程結果。 
 
  共 19 頁 第 16 頁
 
五、結語與未來的方向 
在實驗一和實驗二中，我們已經得到不錯的分群效果，但在大多數的文件分
類中，都會使用 bag of words 這樣的模型，在這樣的模型之下，所有的文字都會
被打散，並收集成一群不具順序性的字(unordered collection of words)。在這樣的
模型之下，所有字跟字之間的距離，就完全被忽略了。之所以要這樣使用，最主
要當然是為了處理上的方便，然而這樣卻也造成了部份資訊的闕漏，而這些資訊
卻有可能是區分同字異義的關鍵，因為同字異義的詞，通常會是上下文相關的
(contextual)，也就是說，在這樣的情況下，只要根據上下文的內容，就可以分辨
出所包含的意義。但是在傳統以 term-by-document 的方式來表示文章的情況下，
勢必就無可避免的要利用到 bag of words 的方式，因此打亂了字間的關係。為了
要保留文件中每個字之間的資訊，建構出不同於以往的模型來表示文件，就勢在
必行。 
一般而言，很多問題可以由一群 node 以及 link 構成的 graph 來描述，每一
個 node 代表著該事件的狀態、屬性或是行為，而 node 之間的 link 則代表著 node
間的相關性。所以相同類型的的事件上，會有著類似的 graph，或是存在著一小
部份的 graph 是可以用以界定出這些事件的差異。把這樣的想法應用在文件分類
的話，則是以 graph 來表示文件的模型，而每一個 graph 都表現了一份文件；graph
中的每一個 node，都是文件中的 keywords；node 間的 link，則是 keyword 之間
的強弱關係，也許是字之間的距離，也許是在某個範圍內同時出現的次數；以圖
八而言，keyword 2 和 keyword 3 之間，就有很強的關聯性，相對的，keyword 5
則和其他的幾個 keyword 都沒什麼關係。如果一來所有的文件，都透過這樣的
表示方式，形成許多的 graph。接著再定義出一套計算 graph 間的不相似度方式，
並套用到之前提過的降維演算法，把每個 graph 視作為一個點，投影到本質維度
上，再使用一些分群的方法，便可以將保留了比使用 term-by-document 的方法，
還要更多的資訊。 
 
 
圖八、示意圖，以 graph 來表示文件 
 
keyword 2  keyword 1  
keyword 4  
keyword 3  keyword 5  
  共 19 頁 第 18 頁
 
參考文獻 
[1] AK Jain and RC Dubes. (1988). Algorithms for Clustering Data. Prentice-Hall, 
Inc. Upper Saddle River, NJ, USA. 
[2] C. Biernacki, G. Celeux, and G. Govaert. (2000). “Assessing a Mixture Model for 
Clustering with the Integrated Completed Likelihood”. IEEE Trans. on Pattern 
Analysis and Machine Intelligence, 22(7):719-725. 
[3]  Carlo Tomasi. (2003). Estimating Gaussian Mixture Densities with EM - A 
Tutorial. 
[4] Dempster, A., Laird, N. and Rubin, D. (1997). “Maximum Likelihood from 
Incomplete Data via the EM Algorithm”, Journal of the Royal Statistical Society, 
Series B, 39(1):1-38. 
[5]  H. Hartly. (1958). “Maximum Likelihood Estimation from Incomplete Data”. 
Biometrics, 14:174-194. 
[6]  J.A. Bilmes. (1997). “A Gentle Tutorial of EM Algorithm and Its Application to 
Parameter Estimation for Gaussian Mixture and Hidden Markov Models.” 
Technical Report, University of Berkeley, ICSI-TR-97-021. 
[7]  Jansen, B. J., Sping, A., Bateman, J. and Sarace-vic, T. (1998). “Real Life 
Information Retrieval: A study of User Queries On The Web”, ACM SIGIR 
Forum, 32(1):5-17. 
[8] J. B. Tenenbaum, Vin de Silva, J. C. Langford. (2000). “A Global Geometric 
Framework for Nonlinear Dimensionality Reduction”, Science, 22:2319-2323. 
[9]  Jyh-Shing Roger Jang. Data Clustering and Pattern Recognition 
(http://neural.cs.nthu.edu.tw/jang/books/dcpr/index.asp). 
[10]  M. A. T. Figueiredo, A. K. Jain (2002). “Unsupervised Learning of Finite 
Mixture Models”, IEEE Trans. On Pattern Analysis and Machine Intellegence, 
24(3):381-396. 
[11]  M. I. Jordan. (1998). Learning in Graphical Models. Kluwer Academic 
Publishers. 
[12]  MacQueen, J.B. (1967). “Some Methods for Classification and Analysis of 
Multivariate Observations”, Proceedings of 5-yh Berkeley Symposium on 
Mathematical Statistics and Probability, Berkeley, University of California Press, 
1: 281-297. 
[13]  Michael W. Berry, Murray Browne. (2005). Understanding Search Engines – 
Marthematical Modeling and Text Retrieval 2nd. 
[14] Chakrabarti. (2003). Mining the Web - Discovering Knowledge from Hypertext 
Data, Morgan-Kaufmann Publishers. 
[15]  S. Roberts, D. Husmeier, I. Rezek, and W. Penny. (1998). “Bayesian Approaches 
to Gaussian Mixture Modeling”. IEEE Trans. on Pattern Analysis and Machine 
