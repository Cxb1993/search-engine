2We aim at analyzing different methods for automati-
cally identifying possible semantic composition. We ex-
plore two sources for service analysis: WSDL description
files and free textual descriptors, which are commonly
used in service repositories. We investigate two methods
for Web service classification for each type of descrip-
tor, Term Frequency / Inverse Document Frequency
(TF/IDF) [7] and context-based analysis [8], and a base-
line method. We define contexts as a model of a domain
for a given term, which is automatically extracted from
a fragment of text. In this work, contexts are created by
finding related terms from the Web. Unlike ontologies,
which are considered shared models of a domain, we
define contexts as local views of a domain [9]. Therefore,
contexts may be different for two fragments of informa-
tion, even though their domain might be the same. The
definition of context used throughout this paper extends
the definition of context in ubiquitous computing, which
employs context as any information that can be used
to characterize the situation of an entity [10]. In many
fields, context is used to describe the environment in
which a service operates. In our definition, it is used to
describe the related set of linguistic terms of a given text.
In this work we propose a context-based approach
to the problem of matching and ranking semantic Web
services for composition. First, we propose the use of
service classification, a process that matches a service
to a set of concepts, representing its affinity with a
given domain. For example, consider the services in
Figure 1. The context of service A would be a set of
geographical terms (such as address, city, and distance).
Therefore, it would be classified to a set of concepts
taken from a geographical ontology. Service B would be
classified to a business transaction ontology and service
C to a computer systems ontology. Second, we use the
classification and context information to improve the
process of service composition, ruling out compositions
of unrelated services. Given a suggested composition
between a number of services, we analyze the context
overlap between the services. The overlap is used to rank
the probability of the composition.
Our method is particularly relevant to exploratory
composition rather than automatic composition. An ex-
ploratory composition [11] is an iterative process in
which an automatic composer suggests possible options
for the composition and a human user decides regarding
the final composition. Thus, the process is suitable for
situations in which semantic annotations may be incom-
plete and incorrect. Our method can be used to rank
the possible candidate services for composition, which
is resilient to partial annotations.
To evaluate our method, we conducted a set of experi-
ments in which we compared our method to other meth-
ods, including pure TF/IDF and simple string matching.
We used a real-world data set, containing WSDL de-
scription and textual description of public Web services.
Our results show that the Web based context extraction
method analyzing both the WSDL description and the
textual description yields better results than the TF/IDF
method and string matching. In addition, the results
prove the advantage of integrating the analysis of both
the WSDL context descriptor and the service textual
descriptor. Furthermore, to test whether the model can
be used to provide useful recommendations of Web
service compositions, each Web service context extracted
from the WSDL is compared with all other Web ser-
vice contexts extracted from the textual description. The
experiments examine the classification of two points of
view, the analyzed “internal” view, the given WSDL de-
scription, and the “external” view, the text description of
the different services from which we are looking for pos-
sible compositions. The numerical estimation evaluates
what the distance is between the “internal” view and
the “external” view. The results provide high recall and
precision for the Web context extraction based method
compared to other methods. In addition, we show that
the proximity analysis of the context of each Web service
can serve as a useful tool for suggesting Web service
composition.
The rest of the article is structured as follows. Sec-
tion 2 presents an analysis of the service classification
methods. Section 3 describes the service composition
proximity analysis method. Section 4 discusses the differ-
ences between the different methods. Section 5 presents
experiments with matching Web services based on the
different service analysis methods and experiments with
the composition proximity analysis method. Section 6
describes the related work. Section 7 highlights our main
contribution and presents future work.
2 SERVICE CATEGORIZATION FOR COMPOSI-
TION
2.1 Overview
 
WSDL 
Service 
Description 
Ontology 
Matching 
Results 
Token 
Extraction 
 
Token 
Extraction 
 
Web Context 
Retrieval 
 
TF/IDF 
Ranking 
 
Baseline 
 
Categorization 
 
Fig. 2. The Web Service Categorization Process
4types are very different. By building an independent
corpus for each document type, irrelevant terms are
more distinct and can be thrown away with a higher con-
fidence. To formally define TF/IDF, we start by defining
푓푟푒푞(푡푖, 퐷푖) as the number of occurrences of the token 푡푖
within the document descriptor 퐷푖. We define the term
frequency of each term as:
tf (푡푖) =
푓푟푒푞(푡푖, 퐷푖)
∣퐷푖∣
We define 풟푤푠푑푙 to be the corpus of WSDL descriptors
and 풟푑푒푠푐 to be the corpus of textual descriptions. The
inverse document frequency is calculated as the ratio
between the total number of documents and the number
of documents which contain the term:
idf (푡푖) = log
∣풟∣
∣{퐷푖 : 푡푖 ∈ 퐷푖}∣
Here, 풟 is defined generically, and its actual instantia-
tion is chosen according to the origin of the descriptor.
Finally, the TF/IDF weight of a token, annotated as 푤(푡푖),
is calculated as:
푤(푡푖) = tf (푡푖)× idf 2(푡푖)
While the common implementation of TF/IDF gives
equal weights to the term frequency and inverse doc-
ument frequency (i.e., 푤 = tf × idf ), we chose to give
higher weight to the IDF value. The reason behind this
modification is to normalize the inherent bias of the
TF measure in short documents [16]. While traditional
TF/IDF applications were concerned with verbose docu-
ments (such as books, articles, and human-readable Web
pages), WSDL documents and the textual description of
services are relatively short. Therefore, the frequency of
a word within a document tends to be incidental, and
the document length component of the TF generally has
no impact.
 
 
 
 
 
 
 
 
 
 
 
 
Loan 
Payments.cloanpayments 
Repaid 
Years 
Payments 
Monthly 
Months  
Total 
Payments.cloanpaymentsbinding 
Amount 
Rate 
Fig. 5. An Example of the TF/IDF High Scored List of the
Equated Monthly Installment for Loan Amount
The token weight is used to induce ranking over
the descriptor’s tokens. We define the ranking using a
precedence relation ⪯푡푓/푖푑푓 , which is a partial order over
퐷, such that 푡푙 ⪯푡푓/푖푑푓 푡푘 if 푤(푡푙) < 푤(푡푘). The ranking is
used to filter the tokens according to a threshold which
filters out words with a frequency count higher than the
second standard deviation from the average frequency.
The effectiveness of the threshold was validated by our
experiments. Figure 5 presents the list of tokens which
received a higher weight than the threshold. Several
tokens which appeared in the baseline list (see Figure 4)
were removed due to the filtering process. For instance,
words such as “Body,” “String,” and “Post” received
below-the-threshold TF/IDF weight, due to their high
inverse document frequency.
2.4 Context Extraction
We define a descriptor 푐푖 from domain 풟풪ℳ as an
index term used to identify a record of information [17],
which in our case is a Web service. It can consist of a
word, phrase, or alphanumerical term. A weight 푤푖 ∈ ℜ
identifies the importance of descriptor 푐푖 in relation to
the Web service. For example, we can have a descriptor
푐1 = 푀표푟푡푔푎푔푒 and 푤1 = 36. A descriptor set {⟨푐푖, 푤푖⟩}푖
is defined by a set of pairs, descriptors and weights.
Each descriptor can define a different point of view of
the concept. The descriptor set defines all the different
perspectives and their relevant weights, which identify
the importance of each perspective.
By collecting all the different viewpoints delineated by
the different descriptors we obtain the context. A context
풞 = {{⟨푐푖푗 , 푤푖푗⟩}푖}푗 is a set of finite sets of descriptors.
For example, a context 풞 may be a set of words (hence
풟풪ℳ is a set of all possible character combinations)
defining a Web service and the weights can represent
the relevance of a descriptor to the Web service. In
classic IR, ⟨푐푖푗 , 푤푖푗⟩ may represent the fact that the word
푐푖푗 is repeated 푤푖푗 times in the Web service descriptor
document.
The context recognition algorithm was adapted from
[8]. The algorithm can formally be defined as follows:
Let 풟 = {푃1, 푃2, ..., 푃푚} be a set of textual propositions
representing a Web service, where for all 푃푖 there exists
a collection of descriptor sets forming the context 풞푖 =
{⟨푐푖1, 푤푖1⟩, ..., ⟨푐푖푛, 푤푖푛⟩} so that 푖푠푡(풞푖, 푃푖) is satisfied.
McCarthy [18] defines a relation 푖푠푡(풞, 푃 ), asserting that
a proposition 푃 is true in a context 풞. In our case the
adopted algorithm uses the corpus of WSDL descriptors,
풟푤푠푑푙, and textual description, 풟푑푒푠푐, as propositions 푃푖,
and the contexts describing the WSDL as descriptors 푐푖푗
with their associated weight 푤푖푗 . The context recognition
algorithm identifies the outer context 풞 defined by:
푖푠푡(풞,
푚∩
푖=1
푖푠푡(풞푖, 푃푖)).
The algorithm input is defined as a set of textual
propositions representing a Web service. Each textual
proposition is sent to a Web search engine. The set of de-
scriptors is extracted by clustering the Web pages search
results. The number of textual propositions from which
the same descriptor is extracted identifies the number
of references to the descriptor in the text. Similarly, the
number of Web pages that identify the same descriptor
6mapped to the same ontology. With context analysis,
they were matched separately to different ontologies.
Flexibility of mapping context to ontology with respect
to language has been proposed in [20]. Multilinguality
requirements necessitate the adaptation of the ontology
to different languages separately. Avoiding such multiple
efforts is desirable, both for the initial specification of the
ontology and for the ontology evolution. Here, the con-
text can serve as the translation mechanism, according
to which ontological concepts are interpreted in the local
language. Each ontology concept can be represented by
multiple contexts in different languages. To illustrate this
point, consider the English concept loan, representing
a concept in the field of finance. While in English the
concept can be represented with one word, in French
the concept would require two contexts: emprunt (used
when borrowing) and pr푒ˆt (used when lending). The
use of contexts to also represent the ontological concept
(such as loan) compensates for any under-specification
that may result from the universality of the ontology.
3 SERVICE RANKING FOR COMPOSITION
THE analysis of the ranking of services for possi-ble composition is based on the advantage that a
Web service can be separated into two descriptions: the
WSDL and a textual description of the Web service in
free text. The Web service WSDL descriptor and the
Web service textual descriptor have different purposes.
One describes ”how” the service should be used and
the other describes ”what” the service does. However,
they both describe the same service from different per-
spectives. If we are looking for possibilities of service
composition, the motivation of the comparison lies in
the investigation of how this service can be expanded in
comparison to what other services can do.
The section analyzes the possible compositions of each
pair of Web services. The goal is to supply a ranking,
a numeric estimation of the complementary relation be-
tween each two Web services. Due to the large number of
possible service combinations, it is impractical to check
all the options manually. The evaluation can provide the
Web service designer with a ranking that will identify
which services should be considered first. The overlap
between each pair of service textual description context
will be analyzed versus other Web services WSDL con-
text. This will form the analysis for each service, which
will allow a two-way context proximity comparison. The
proposed method yields a numeric estimation of the
extent to which a composition should be considered. Fur-
thermore, the method can suggest unique compositions
due to the idea that the Web service analysis is based on
semantic, rather than syntactic, meaning.
Figure 7 displays the process of service ranking based
on context analysis to suggest possibilities of compo-
sition. The process integrates the advantages of both
top ranking methods in the previously described cate-
gorization analysis. The process includes the following
core components: initial analysis, Web context retrieval,
and analysis of possible composition. Similar to the
previous section, when analyzing the possibilities for
Web services composition, we assume that each Web
service is described using a textual description (which
is part of the meta-data within UDDI registries) and
a WSDL document describing the syntactic properties
of the service interface. These two descriptions, as de-
scribed in Figure 3, serve as the input to the analysis
process. The initial processing step is similar to both the
textual service description and the WSDL and includes
token extraction and stop-list words. The second step
includes applying the context extraction method for both
the service textual description and the WSDL, resulting
in a context which is composed of a descriptor set. Both
the WSDL processing and the context extraction are fully
automated. The two descriptors sets for each Web service
are separate inputs for the composition analysis step,
which is the following key component.
 
 
 
 
 
 
 
 
 
 
 
 
Context Overlap 
Service Descriptors 
 . 
         . 
                  . 
 . 
         . 
                  . 
 
 . 
         . 
                  . 
 . 
         . 
                  . 
 
Service 
Description Service 
Composition 
Ranking 
WSDL Di 
 
WSDL 
Service Description Di 
 
WSDL D1 
 
WSDL D2 
 
WSDL Dn 
 
Service Description Dn 
 
Service Description D2 
 
Service Description D1 
 
 
Composition 
Analysis 
 
Web Context 
Retrieval 
 
Web Context 
Retrieval 
 
Token 
Extraction 
 
Token 
Extraction 
 
Fig. 7. Service Composition Context Analysis Process
To consider a composition of Web services, we an-
alyze the relation of the context of each service to
other services. We analyze the context of each service
in a bi-directional method. Each Web service context is
evaluated according to its proximity to other services
and the proximity of each of the other services to the
current service. Figure 7 on the bottom part displays
an enlargement of the bi-directional service description
context composition analysis. The composition analysis
compares each Web service WSDL descriptor context
with all other Web service textual descriptor contexts.
The output of the process is a ranked list of pairs of Web
services which could be considered for composition.
The results of the mapping of the Web services to onot-
logies in Section 2 serve as a prior stage to the current
process. Previously the Web services were mapped to
ontologies represented by concepts and their relations.
The current stage can select a single concept as input for
8which should be considered. The proposed method does
not perform the composition but rather suggests which
of the Web services should be considered and prioritizes
a list of possible compositions. Although the method
provides the developer with a ranking of which services
should be considered for composition based on semantic
similarity, the developer is still required to resolve the
structural integration related to the Web service struc-
tural or syntactical issues.
The output of the context overlap process to identify
possible compositions is a ranking of which services
should be considered for composition. Figure 8 dis-
plays textual descriptions for some of the high ranking
possible compositions identified by the algorithm for
composition based on semantic meaning. Figure 3 in-
cluded WSDL descriptions of three of the same services
and emphasized the difficulty of identifying composition
based on syntactic meaning.
4 DISCUSSION
THE service categorization, which is part of the ser-vice description provided by the service provider,
is insufficient in fully specifying the categorization, due
to the provider’s perspective and the terms he uses.
Another problem is that the provider is not aware of
all the existing ontologies and all their concepts when
providing a service. Furthermore, the provider cannot
be forced to supply a detailed description. These textual
descriptions usually contain a bare minimum of informa-
tion which sometimes does not add to the understanding
of the service.
Figure 9 depicts the relationships between the sets
of tokens produced by the different analysis methods,
i.e., TF/IDF and Web context extraction. The larger cir-
cle represents all the tokens extracted from the textual
description and the WSDL document. We define their
union as the baseline set. As the TF/IDF provides a mere
ranking of the original tokens, it is entirely contained in
the baseline set. The TF/IDF high scored set represents all
the tokens which received a higher score than a given
threshold. The tokens which are the result of the context
extraction method are part of the Web-based context set.
The method identifies existing baseline tokens and also
finds new words, based on a core of the baseline tokens.
Therefore, the set overlaps with the baseline set, contain-
ing new tokens which were not part of the baseline.
We now provide some insights regarding the various
elements of the diagram. The shaded part marked “A”
is the overlap of both methods. It contains all tokens
which belong to the Web context set and to the TF/IDF
high scored set. Both TF/IDF and the context analysis
methods decided that these certain keywords are rele-
vant for categorization. In our experiments, about 7%
of the terms in the context analysis belong to this part.
This overlap may serve as evidence of the importance of
these keywords. For example, categorizing a service that
monitors a workflow process yielded a very short token
 
Web-
based  
Context 
Tokens
 
WSDL 
Tokens 
Textual 
Description 
Tokens 
Baseline 
TF/IDF 
high 
scored 
 
A 
B 
C 
Fig. 9. Token Sets Generated by the Analysis Methods
and Their Inter-Relations
list. However, the token “workflow” appeared in the “A”
set, as it is unique enough to receive high TF/IDF weight
and relevant enough for Web search to be retrieved as
part of several extractions.
The shaded part marked “B” contains those keywords
in the baseline that TF/IDF deems irrelevant, while
the context analysis method believes otherwise. This
part, according to our experiments, constitutes 3% of
the keywords returned by the context analysis. Tokens
such as “message,” “request,” and “response” are typical
members of this set. Since they are frequent words in
the WSDL document, they are sometimes retrieved by
the context extraction algorithm. Thus, this set can be
used as a filter to remove from the context words which
were given a low ranking by TF/IDF and a high ranking
by context.
Part “C” marks the great advantage of the context
analysis over the TF/IDF method. While the latter has
to work within the limits of the baseline dictionary, the
context analysis method goes out to the Web, using it as
an external knowledge source, returning keywords that
are deemed relevant, although they were not originally
specified in the baseline description. 90% of the returned
keywords belong to this region. Some are indeed evalu-
ated as important while others are less so. For example,
the descriptor of a service which handles calculation of
financial derivatives was augmented with words such as
“trading” and “pricing”, which are useful for categorizing
the service under the “finance” domain.
Further investigation of the inter-relationships be-
tween the two methods can be performed. For example,
instead of analyzing the text in parallel, we can start with
TF/IDF, eliminating low score tokens and then process-
ing them with the context analysis method. To get the
best of both worlds, we need to combine these methods
in a way that they will overcome each other’s shortcom-
ings. Therefore, we can combine the tools at our disposal
along three dimensions. One dimension determines the
relevant description (WSDL vs. textual description, in
our case). A second dimension chooses whether to pre-
filter or not (baseline vs. TF/IDF filtering). The third
10
5.1.2 Classification Methods
The experiments examined four different methods for
service classification, as described in Section 2. The ser-
vice classification methods included:
1) WSDL Context The Context Extraction algorithm
described in Section 2.4 was applied to the name
labels of each Web service. Each descriptor of the
Web service context was used as a token.
2) WSDL TF/IDF Each word in the document was
checked for term frequency and inverse document
frequency (TF/IDF) as described in Section 2.3. The
set of highest ranking weighted value words was
used.
3) Description Context The Context Extraction algo-
rithm was applied to the textual description of the
Web services. Each descriptor of the Web service
context was used as a token.
The first set of experiments compared the three meth-
ods and a baseline, which included the original token
list extracted from the service descriptors. The actual
comparison was based on mapping the output of each of
the methods to the set of ontologies, using the ontology
string matching method described in Section 2.5.
The second set of experiments analyzed the service
composition ranking. The analysis measured the overlap
between the WSDL Context and Description Context of
each of the services. The results were compared to the
overlap of the token list extracted from services using
the baseline and the TF/IDF methods. The comparison
was based on the method for evaluating overlap and
composition described in Section 3.3.
5.2 Web Services Classification Results
In our first experiment we analyze the usefulness of
going beyond the baseline bag of tokens. Figure 11
compares the recall and precision of the methods of
classification. The results are displayed according to each
of the ontology sets. The X-label describes each of the
methods and the Y-label describes the level of recall and
precision.
The first World ontology set is supposed to encom-
pass all possible web services. The baseline method
achieved only 91.58% recall. The lowest classification
for the World set was 90.05% from the TF/IDF method.
The results indicate that the WSDL document does not
have enough information defined as name tokens to be
self descriptive. The result of 94.90% was achieved by
the WSDL Context method, which can be attributed
to the external tokens added to the baseline method.
Furthermore, the WSDL description based on free text
description of the service achieved 98.98% recall. The
WSDL descriptor results emphasize even further the
need for external description in addition to the basic web
service descriptive language. The precision in this case
is 100% in all methods since all of the services belong to
the World topic.
 
 
 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Baseline WSDL Context WSDL TF/IDF Descriptor Context
World
Recall
Precision
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Baseline WSDL Context WSDL TF/IDF Descriptor Context
Web Finance
Recall
Precision
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Baseline WSDL Context WSDL TF/IDF Descriptor Context
Entertainment
Recall
Precision
Fig. 11. Precision and Recall of All Methods
The Web Finance classification precision shows a
slight advantage again to the WSDL Context method
of 45.17%, followed closely by the WSDL Descriptor
Context precision of 44.54%, over the baseline 43.14%
and the TF/IDF 39.24% methods. However, in this case
there is a drop in the recall of the WSDL Context to
86.83% versus the higher recall result of the baseline
90.42%. The lower recall result of the TF/IDF 37.13%
can be explained due to the limited number of general
recurring tokens. In this case, the retrieval achieved the
same high results as the baseline 90.42%. Examination of
the recall results between the baseline and the TF/IDF
points to the high recall based on terms with high
frequency which are not topic specific. The high retrieval
rate and higher precision in both context based methods
in this case again indicate the importance of the external
Web description.
The Entertainment classification achieved very low
precision rates. This could be attributed to the Music
ontology, which was relatively big and diversified, and
as a result many of the web services were identified
mistakenly as relevant. The baseline achieved higher
recall results 88.89% than the WSDL context 85.19% and
TF/IDF 70.37%. Again we can see the importance of
the external description of the WSDL, which outper-
formed the other methods, achieving 100.00% recall and
12
Similar to the previous experiments, for each Web service
the repository provided a WSDL document and a short
textual description. The purpose was to analyze the
quality of the possible composition according to the
context overlap method described in Section 3.3. The
analysis of the set of Web services and the identification
of which services would be more likely to be com-
posed are performed by analyzing the overlap between
the services based on their context. We compare each
Web service WSDL descriptor context with another Web
service textual descriptor context. This integrates the
advantages of both of the top ranking methods, WSDL
Context and Description Context.
The performance of the integrated context method was
compared to that of the Baseline and TF/IDF methods.
For the Baseline and the TF/IDF methods the overlap
method was used to analyze the precision. For the
TF/IDF the list of terms which result after processing
the algorithm was analyzed for overlap.
The top ranking pairs for composition of each of the
methods were compared. To analyze the recall and the
precision for each of the methods, the top ranking pairs
were hand-labeled as relevant or irrelevant. An ideal
result for a recall versus precision graph would be a
horizontal curve with high precision value; a bad result
has a horizontal curve with a low precision value. The
recall-precision curve is considered by the IR community
the most informative graph showing the effectiveness of
the results.
Figure 13 displays the results of the Context Overlap,
TF/IDF Overlap, and Baseline Overlap precision for the
top ranking Web services identified as composition pairs.
The horizontal X-axis displays the total number of Web
services pairs considered. The vertical axis displays the
precision value for each composition analysis method.
 
Fig. 13. Precision of Service Matching According to
Contextual Ranking
The results indicate that the Context Overlap achieved
a precision level of 100% when the number of top ranked
Web services pairs considered was 14, 24, and 37. When
the set of the top ranked Web services is 54, the preci-
sion drops to 96.29%. The precision went down slightly
more to 94.32% when 88 top ranking Web services were
analyzed. Finally, for 151 Web service compositions the
precision reached 88.08%.
When comparing the Context Overlap to the Baseline
Overlap results, a difference of almost 30% is maintained
in the precision throughout the graph. The initial top
ranking 14 results for the Baseline achieved 71.43%,
slowly dropping to 58.94% for 151 service pairs. The
TF/IDF graph displays a fluctuating behavior, starting
from 57.14% at 14 pairs, reaching a peak of 70.83% at 24
pairs, and dropping twice and ending with a precision
value of 59.33% at 151 pairs. The TF/IDF fluctuation dis-
plays worse results than does the Baseline. This suggests
that the frequency of the words in the original text does
not improve the identification of possible compositions
of Web services. However, the high precision results
achieved by the Context Overlap display the importance
of using the Web as an external knowledge source.
In the example displayed in Figure 8 the Web service
which Calculates Equated Monthly Installment (EMI) For
the Loan Amount located in the center is identified for
composition with the two top services of Calculate load
payments and total to repay and Simple calculator for the
Context Overlap of 37 Web services. Similarly, the com-
position of the EMI service is identified for composition
with the Specialist email facility and FastQuote for Context
Overlap of the top 151 Web services composition.
Figure 14 displays the recall versus the precision of
all three methods. In the graph the X-axis represents the
recall and the Y-axis the precision.
The graph shows that the Context Overlap methods
supply the highest recall precision values throughout the
graph. As the recall increases over 50%, the precision
drops from 100% in the beginning and ranges between
53% and 78%. The TF/IDF precision drops from the
beginning much faster and when the recall increases, the
precision drops to 14%. The Baseline method displays
low recall-precision during most of the values.
The graph displays the advantage of the integrated
context method in comparing the WSDL description to
the textual description of all other services. The inte-
grated context method dominates, in both recall and
precision, both the TF/IDF and the Baseline methods.
6 RELATED WORK
THE field of Web service composition is very active.However, most approaches require clear and formal
semantic annotations to formal ontologies [23], [6], [4],
[22]. As most services which are currently active in the
World Wide Web do not contain any semantic anno-
tations, finding methods that enable composition with-
out semantic annotation is a necessity. Initial work has
been done in discovering services directly by querying
syntactic Web services through their WSDL documenta-
tion ([24] and [25]). Our work provides an analysis of
