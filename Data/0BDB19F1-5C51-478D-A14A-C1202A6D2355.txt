進行探討，利用此核心函數的特性篩選資料並確保所篩選之訓練資料於映射至特徵
空間(feature space)後仍具有支撐向量的特性，接者進一步探討其它核心函數，其最
終目標為根據核心函數之不同分別訂定出其適合的演算法。 
在濾波器的家族之中，WOS (weighted order statistics) 濾波器一直受到高度的注
目 [11, 12, 13, 14]。WOS 濾波器的優異性能使得它在眾多應用領域上獲得良好的應
用結果，這些應用領域包含了雜訊移除、影像回復、文字辨識等。 
雖然 WOS 濾波器具有完美的理論架構且已在眾多應用領域上獲得成功，然對於
WOS 濾波器在設計階段所遇到的問題卻一直未有重大突破。至目前為止 WOS 濾波
器的設計模式以 1994 年 Yin 所提出的利用類神經網路建構出 WOS 濾波器為最代表
性之著作，在傳統的學習模式下利用 Yin 所提出之設計模式設計出的 WOS 濾波器其
實際輸出與預期輸出的誤差為最小。然而此設計模式並非十全十美，它最嚴重的問
題在於它的學習階段所花費的時間過於漫長，以至於讓人無法接受 [15]。 
此外 Marshall、Yoo et al 及 Savin et al 等人也先後提出改進堆疊濾波器的演算法 
[13, 16, 17, 18],，其方法之一為利用機率分析的概念，針對所有可能的樣本統計其出
現機率後針對此結果進行訓練，此結果雖然似乎大幅改進了訓練時間，然當訓練資
料的工作視窗增大時其訓練時間幾乎呈現指數次方成長，這因素導致這些演算法僅
能侷限於部分案例。 
在此計畫中提案人以不降低實際輸出與預期輸出的誤差之前提下，提出一個新
的設計模式，在此設計模式下將可大幅度的改善學習階段所花費之時間且保留精確
度。此外，新的設計模式將原本的問題轉換成分類問題，為了提高 WOS 濾波器的能
力，我們於 WOS 濾波器的設計過程中引入支撐向量機器的技術，並利用萃取式支撐
向量機器 (extractive SVMs)來實現 WOS 濾波器，以提高 WOS 濾波器的應用層次，
並以實驗數據佐證此架構支正確性。此外根據上述所提之理論建構一個混合式的濾
波器，此濾波器可根據訊號特性自動選擇適合的濾波器來處理訊號提供較佳的處理
結果。 
（三）、研究方法 
3.1. 降低 WOS 濾波器設計複雜度 
若應用領域之輸入、輸出訊號為 K 元信號，則 WOS 濾波器可將訊號分解成 K-1
個階層來處理。假若 TNiiNiNini xxxxX ]1,,,,,,[ 1 −= ++−− LL ，其中假如 則 ，
否則 ，另外
nX i ≥ 1=ix
0=ix TNiiNiNiT twwwwW ],,,,,,[ 1 ++−−= LL ，則 WOS 可定義為 
⎪⎪⎩
⎪⎪⎨
⎧
<
≥
=
∑
∑
+
−=
+
−=
txw
txw
XWU
k
Ni
Nik
k
k
Ni
Nik
k
n
i
T
if,0
if,1
)(                                         (1) 
當應用領域之輸入、輸出訊號範圍為{0,1,…K-1}時，在threshold decomposition 
－ 2 － 
1≥miT XW 及 11 −≤+miT XW ，其改寫後的式子如 Eq. (3)及 Eq. (4)。 
∑+
−=
≥−
nk
nki
kk txw 11 , x1k is the kth component of 
m
iX 。                           (3) 
∑+
−=
−≤−
nk
nki
kk txw 12 , x2k is the kth component of 
1+m
iX 。                        (4) 
由上述兩式可得 1)(1 == miTi XWUy 與 0)( 12 == +miTi XWUy ，因此結合Eq. (3), Eq. (4)
與y1i, y2i可得到Eq. (5)與Eq. (6). 
( ) 112 11 ≥⎟⎠
⎞⎜⎝
⎛ −− ∑+
−=
nk
nki
kki txwy ,  is the kth component of x1k
m
iX 。
, x2k is the kth component of 
                  (5) 
( ) 112 22 ≥⎟⎠
⎞⎜⎝
⎛ −− ∑+
−=
nk
nki
kki txwy
1+m
iX 。                 (6) 
若 ( ) ( ) ( )[ ]nix +1, ， ( ) ( ) ( )ininii xxxx +−−= 11111 ,,,, LLr [ ]niininii xxxxx ++−−= 221222 ,,,,, LLr 且
[ ]Tw = niinini wwww ++−− ,,,,, 1 LL ， Eq. 
(6)可簡化為 
且將無法分類的資料考量在內的話，則 Eq. (5)及
( )( ) 112 111 ≥+−− iiTi txwy ξr , mi x1k is the kth component of X                   (9) 。 
( )( ) 1222 ≥iii ξ+−T txr , x  is the kth component of 12 − wy 2k 1+mX                (10) i 。
0, 21 ≥ii ξξ                                                            (11) 
線性支撐向量機器與 WOS 濾波器 
問題的
假設訓練資料為 ，有一最佳化問題如下： 
minimize 
根據上述式子，建構最佳化 WOS 濾波器的建構過程可以轉換為找出如下最佳化
最佳解。 
( ){ }Liii mX 1, =r
( ) ( )∑
=
++=Φ
L
i
ii
T Cwww
1
2121 2
1,, ξξξξ                          (12)              
s.t       ( )( ) 112 11 ii txw1 ≥+−− Tiy ξr , i=1,2,…,L                       (13) 
)( )( 122 ii txw1 ≥+−− T ξ2 2iy r , i=1,2,…,L                      (14) 
0≥w                                                  (15) 
－ 4 － 
C≤≤ iα0  , for i=1, 2, …, L，                                (25) 
Ci ≤≤ β0 , for i=1, 2, …, L，                                 (26) 
iγ≤0 , for i=1, 2, …, L，    
容，所採用 實驗對象為
將所有的訓練資料完美的分類，在此情況下每個訓練資料滿足下列條件： 
,,2,1,1) =≥ ，                              (28) 
練資料可用來建構出支撐
平面
訓練資料均為落於支撐平面上 與
練資料。而這些滿足 Eq. (28)限制條件且符合 的訓練資料又稱為支撐
向量(support vector)  
藉由觀察訓練資料於空間的分佈狀況可發現支撐向量幾乎均分佈於資料叢集
(data cluster)的周圍地區，我們可以利用此特性來簡化訓練資料的數量。接下來的定
使得訓練資料於映射前及映射後的相對
首先先
                                 (27) 
本計劃中以影像處理為實驗內 的 256x256 的灰階影像，
而這也代表有 65536 個輸入資料，但非線性支撐向量在解決大量訓練資料時若使用
現有的演算法時其結果均不甚理想。因此本計劃中將利用機率統計分析找出支撐向
量，此方法不僅可以降低訓練資料數量並能維持良好的分類結果。 
3.3. 利用機率統計分析找出支撐向量(support vectors)以降低訓練資料數量 
支撐向量機器的目標為找到一個最佳分割平面 0使得此分割平面可以
 T( +
=+bxwT
Libxwy ii L
因此在支撐向量機器中，若能找出 1=+ bxwT 與 1−=+ bxwT 這兩個支撐平面，事實
上也代表最佳分割平面也間接被找到了，而要找出這兩個支撐平面可藉由支撐向量
來建構出。雖然訓練資料數量非常多，然而除了少數的訓
外，其餘的訓練資料均扮演著可被忽略的角色。而這些用來建構出支撐平面的
，亦即落於 1這兩個平面的訓
。
1=+ bxwT −=+ bxwT
1)( =+ bxwy iTi
理說明了必存在一個或一個以上的核心函數
位置並不會作大幅度的改變。 
檢視核心函數 (kernel function) 。根據 Mercer 定理，核心函數
( ) ( ) ( )RRRK nn →×: 可表達成映射函數的內積 jiji ，其中映射函數
( ) mn 。一個 的核心函數 nn 可定義為
Tϕ= xxxxK ϕ,
monotonic →×:  RR →⋅ :ϕ RRRK
( ) ( ) ( ) ( ) 2222, jkjijkjinkji xxxR −−∈ ,,, xxxxthenxxxx ϕϕϕϕ −≥−≥∀  
Lemma 1：證明 monotonic 核心函數存在。 
Theorem 1：在資料空間(data space)內落於資料叢集(data cluster)周圍地區的訓練資料
當經過符合 Lemma 1 的映射函數將資料映射至特徵空間(feature space)
後，這些訓練資料仍落在資料叢集的周圍地區。 
－ 6 － 
在上一節中已提出一個適用於 radial basis function 的 ESV 演算法，且已證明此
演算法的正確性及其優異效率，然而資料類型千百種，適用的核心函數亦不相同，
因此本階段的預期目標將針對不同的核心函數根據其特性發展其適用的演算法。
此外經由實驗證實所提出的結合支撐向量機器與 WOS 濾波器設計的優異性
能，因此本階段中計畫以此改良後的 WOS 濾波器及線性濾波器建構出一個混合式的
濾波器，其結構圖如圖一所示，因 WOS 濾波器為非線性濾波器，其適用之處理對象
為 long-tailed noise (例如 Laplacian)，而 Alpha-trimmed mean 濾波器則適用於處理
short-tailed noise (例如 Gaussian)，因此此混合式濾波器可依訓練資料的類型利用合適
的濾波器進行處理。此外此混合式的濾波器利用 ESV 演算法對訓練資料進行前置處
理，將訓練資料分類成未損毀及損毀兩類資料，此前置處理可避免非必要性的濾波
處理。其輸出可以公式表示為 
 
( ) ( ) ( ) ( )( ) ( ) ( )iIdiiiMeaniiWOSiiiiWMH XFXFXFXF rrrr ββμμβμ −++−= 11,,_        (30) 
 
圖一、混合式濾波器 
（四
成四部份， 中「降低 濾波器設計複雜度」這部分為理論
推導外，其餘皆以實驗結果證實所提出之方法之實用性與正確性。 
分，第 驗證利 Ms 實現 S 濾波器的效能，此
部分之部分結果如下表所示，由表可看出其架構所提供之優異效能。 
SE errors WOS filter Adaptive Rank order Lp norm 
）、結果與討論 
在本計畫中共分 其 WOS
實驗驗證分成三部 一部分 用 SV WO
M
by SVMs neural filter filter WOS filter 
“Lenna” 5% noise 45 45 67.1 50.8 
“Lenna” 10% noise 80.2 80 90.7 82.8 
“Lenna” 15% noise 120.8 119 139.9 125.6 
“Boat” 5% nose 95 95.6 155.7 105.1 
“Boat” 10% nose 150.2 149 192.8 160.5 
“Boat” 15% nose 208.8 206 256.9 218.4 
－ 8 － 
三、參考文獻 
[1] V. N. Vapnik, Statistical learning theory, Wiley, New York, 1998. 
[2] V. N. Vapnik, The nature of statistical learning theory, Springer, New York, 
1995. 
[3] O. L. Mangasarian, “Generalized support vector machines,” In Advances in 
Large Margin Classifiers, A.J. Smola, P.Bartlett, B. Schölkopf, and 
C.Schuurmans, editors, MIT Press, Cambridge, MA, 2000, pp. 135-146. 
[4] B. Schölkopf and A. J. Smola, Learning with kernels: Support vector machines, 
regularization, optimization, and beyond, Cambridge, Mass: MIT Press, London, 
2002. 
[5] C. -C. Chang, C. -W. Hsu and C. -J. Lin, “The analysis of decomposition 
methods for support vector machines,” IEEE Trans. on Neural Networks, vol. 11, 
no. 4, 2000, pp. 1003-1008. 
[6] Y. -J. LEE and O. L. Mangasarian, ‘RSVM: Reduced support vector machines,’ 
Proceedings of the First SIAM International Conference on Data Mining, 
Chicago, 2001. 
[7] K. -M. Lin and C. -J. Lin, “A study on reduced support vector machines,” IEEE 
Trans. on Neural Networks, vol. 14, no. 6, 2003, pp. 1449-1459. 
[8] T. Joachims, “Making large-scale SVM learning practical,” in Advances in 
Kernel Methods-Support Vector Learning, B. Schölkopf, C. J. C. Burges and A. J. 
Smola (Eds.), MIT Press, Cambridge, MA, 1998. 
[9] J. C. Platt, “Fast training of support vector machines using sequential minimal 
optimization,” in Advances in Kernel Methods-Support Vector Learning, B. 
Schölkopf, C. J. C. Burges and A. J. Smola (Eds.), MIT Press, Cambridge, MA, 
1998. 
[10] R. E. Walpole and R. H. Myers, Probability and statistics for engineers and 
scientists, Macmillan, New York, 1993. 
[11] A. Gasterators and I. Andreadis, “A new algorithm for weighted order statistics 
operations,” IEEE Signal Processing Letters, vol. 6, no. 4, 1999, pp. 84-86. 
[12] P. Koivisto and H. Huttunen, “Design of weighted order statistic filters by 
training-based optimization," Sixth International Symposium on Signal 
Processing and its Applications, vol. 1, 2001, pp. 13-16. 
[13] S. Marshall, “New direct design method for weighted order statistic filters,” IEE 
Proc.-Vis. Image Signal Process, vol. 151, no. 1, 2004, pp. 1-8. 
[14] P. -T. Yu and W. -H. Liao, “Weighted order statistics filters: their classification, 
some properties and convension algorithm,” IEEE Trans. on Signal Processing, 
－ 10 － 
四、計畫成果自評 
本計畫中，我們首先推導如何利用二分法降低 WOS 濾波器設計複雜度，利
用此方法不僅可以大幅降低 WOS 濾波器的時間複雜度，其訊號處理之效果毅接
近最佳表現。另一方面，我們提出一種簡化支撐向量機器訓練階段的演算法，利
用此種演算法後可將 SVMs 的概念應用於 WOS 濾波器上，並以此發展出混合式
的濾波器，此種濾波器結合 WOS 濾波器與 Alpha-trimmed mean 濾波器，可針對
不同類型的雜訊(long-tailed noise 與 short-tailed noise)利用合適的濾波器進行處
理。此外此混合式的濾波器利用 ESV 演算法對訓練資料進行前置處理，可避免
非必要性的濾波處理。此計畫內容已撰寫成論文，並已被 Fundamenta Informaticae
國際期刊接受，近期即將登出，接受含與論文全文請參見附錄。 
本計畫之執行成果已達成主要之預期目標，而且對於未來解決相關問題亦有
所助益。藉此計畫之執行，我們更完整且深入地了解支撐向量機器上問題的相關
特性。總結本計畫的具體成果已達成預期目標並表列如下： 
1. 以理論推導利用二分法以簡化 WOS 濾波器處理訊號時的時間複雜度，並以
實驗結果證實其可行性。 
2. 以機率統計方式簡化支撐向量機器於訓練時所需之空間及時間，並有效的應
用在影像還原上，解決影像處理時資料量過大的問題。 
3. 研究的成果可供學術界參考，並有助於研究者解決支撐向量機器的相關問
題。 
4. 參與本計劃之人員可望在研究中獲得相關的知識，作為日後研究的基礎；並
可從思考問題上獲得邏輯的訓練。 
 
－ 12 － 
Fundamenta Informaticae XX (2008) 1–19 1
IOS Press
Extractive Support Vector Algorithm on Support Vector Machines
for Image Restoration
Chih-Chia Yao∗
Department of Computer Science and Information Engineering
Chaoyang University of Technology
Wufong, Taichung 41349, Taiwan
ccyao@cyut.edu.tw
Pao-Ta Yu
Department of Computer Science and Information Engineering
National Chung Cheng University, Taiwan
Ruo-Wei Hung
Department of Computer Science and Information Engineering
Chaoyang University of Technology
Abstract. The major problem of SVMs is the dependence of the nonlinear separating surface on the
entire dataset which creates unwieldy storage problems. This paper proposes a novel design algo-
rithm, called the extractive support vector algorithm, which provides improved learning speed and a
vastly improved performance. Instead of learning and training with all input patterns, the proposed
algorithm selects support vectors from the input patterns and uses these support vectors as the train-
ing patterns. Experimental results reveal that our proposed algorithm provides near optimal solutions
and outperforms the existing design algorithms. In addition, a significant framework which is based
on extractive support vector algorithm is proposed for image restoration. In the framework, input
patterns are classified by three filters: weighted order statistics filter, alpha-trimmed mean filter and
identity filter. Our proposed filter can achieve three objectives: noise attenuation, chromaticity reten-
tion, and preservation of edges and details. Extensive simulation results illustrate that our proposed
filter not only achieves these three objectives but also possesses robust and adaptive capabilities, and
outperforms other proposed filtering techniques.
Address for correspondence: C. -C. Yao, Department of Computer Science and Information Engineering, Chaoyang University
of Technology
∗This work is supported by National Science Council of the Republic of China under Grant NSC96-2221-E-324-047
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 3
most useful and efficient. It shows very good performance for the removal of long-tailed noise types
(e.g., Laplacian) and preserving the edges. On the other hand, alpha-trimmed mean filters provides bet-
ter performance for noise smoothing and for the short-tailed noise types (e.g., Gaussian). Our proposed
model preserves the advantages of WOS filters and alpha-trimmed mean filters. Hence, our proposed
filter can remove impulsive noise and gaussian noise, at the same time efficiently preserving the edges
and image details.
In our proposed model, two phases are adopted to restore image. First, the extractive support vector
algorithm is used to classify the corrupted and uncorrupted data. Then, a membership degree generated
by a neural controller is used to multiply the filtering results of WOS filter and alpha-trimmed mean
filter as the output. Experimental results and comparisons are illustrated in Section 5 to demonstrate the
excellent performance of our proposed model.
The remainder of this paper is organized as follows. In Section 2 we review the basic concept of
SVMs and the reduced support vector machines. In Section 3 the basic idea of the extractive support
vector algorithm is introduced. In Section 4 we describe the extractive support vector algorithm for
image restoration. Some of the experimental results are discussed in Section 5, and finally we draw our
conclusions in Section 6.
2. Basic Concepts
This section reviews concepts associated with support vector machines and reduce support vector ma-
chines.
2.1. Linear Support Vector Machines
Consider the training samples (xi, di), i = 1, 2, · · · , N , where xi is the input pattern for the ith sample
and di is the corresponding desired response; xi ∈ Rn and di ∈ {−1, 1}. The objective is to define a
hyperplane which divides the set of samples such that all the points with the same class are on the same
sides of the hyperplane.
Let wo and bo denote the optimum values of the weight vector and bias, respectively. Correspond-
ingly, the optimal hyperplane, representing a multidimensional linear decision surface in the input space,
is given by
wTo x + bo = 0 (1)
The set of vectors is said to be optimally separated by the hyperplane if it is separated without error and
the margin of separation is maximal. Then, the separating hyperplane wTx + b = 0 must satisfy the
following constraint:
di(wTxi + b) ≥ 1− ξi, i = 1, · · · , N (2)
ξi ≥ 0
According to Eq. (2), the optimal separating hyperplane is the maximal margin hyperplane with the
geometric margin 2‖w‖ . Hence the optimal separating hyperplane is the one that satisfies Eq. (2) and
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 5
When xi is replaced by its mapping in the feature space ϕ(xi), Eq. (6) becomes,
Q(α) =
N∑
i=1
αi − 12
N∑
i=1
N∑
j=1
αiαjdidjK(xi, xj) (11)
2.3. Reduced Support Vector Machine Formulation
Here, we outline the key modifications from standard SVMs to RSVM in [12, 19]. They start from
adding an additional term b
2
2 to the objective function of
min
1
2
(wTw + b2) + C(
L∑
i=1
ξ2i ) (12)
subject to di(wTφ(xi) + b) ≥ 1− ξi
It is known that, at the optimal solution, w is a linear combination of training data:
w =
L∑
i=1
diαiφ(xi). (13)
Let Qij = didjφ(xj)Tφ(xi), the dual form of Eq. (12) becomes
min
1
2
(αTQα + b2) + C
L∑
i=1
ξ2i (14)
subject to Qα + bD ≥ e− ξ
The idea of RSVM is to reduce the number of support vectors. It is achieved by randomly selecting
a subset of m samples for constructing w
w =
L∑
i∈R
diαiφ(xi). (15)
where R contains indices of this subset. By substituting Eq. (15) into Eq. (12), the optimization problem
becomes
minα,b,ξ
1
2
(α¯TQR,Rα¯ + b2) + C
L∑
i=1
ξ2i (16)
subject to QL,Rα¯ + by ≥ e− ξ
where α¯ is the collection of all αi, i ∈ R. Note that now m is the size of R. We use QL,R to represent the
submatrix of columns corresponding to R. Thus, there are still L constraints. Following the generalized
SVM simplified the term 12 α¯
TQR,Rα¯ to 12 α¯
Tα so RSVM solves
minα,b,ξ
1
2
(α¯T α¯ + b2) + C
L∑
i=1
ξ2i (17)
subject to QL,Rα¯ + by ≥ e− ξ
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 7
Proof: It can be trivially obtained the following statement:
‖ϕ(xi)− ϕ(xj)‖2 − ‖ϕ(xk)− ϕ(xj)‖2
= ϕ(xi)Tϕ(xi)− 2ϕ(xi)Tϕ(xj)− ϕ(xk)Tϕ(xk) + 2ϕ(xk)Tϕ(xj)
= K(xi, xi)− 2K(xi, xj)−K(xk, xk) + 2K(xk, xj)
(19)
If ‖xi−xj‖2 ≥ ‖xk−xj‖2 and radial-basis function (K(x, xj) = exp(− 12σ2 ‖x−xj‖2))
is adopted, then it implies K(xk, xj) ≥ K(xi, xj). Then we get
‖ϕ(xi)− ϕ(xj)‖2 − ‖ϕ(xk)− ϕ(xj)‖2
= 1− 2K(xi, xj)− 1 + 2K(xk, xj) ≥ 0
(20)
Therefore, ‖ϕ(xi) − ϕ(xj)‖2 ≥ ‖ϕ(xk) − ϕ(xj)‖2. That is, the radial-basis function
(K(xi, xj) = exp(− 12σ2 ‖xi − xj‖2)) is monotonic. 
On the contrary, polynomial function K(x, xi) = (1 + xTxi)P is not a monotonic kernel function.
It can be evidenced by substituting some numerical values into Eq. (19), for instance, xi = (2, 2), xj =
(−3,−3), xk = (−5,−5).
Theorem 1: The training patterns located on the surrounding of the class will be located on the
surrounding of the class in feature space after mapping by a transform function
which is satisfied with Lemma 1.
Standard deviation is helpful for selecting support vectors from data set. In above discussion we
have concluded that the support vectors are highly located on the surrounding of data cluster. It means
that those training patterns which are not located on the surrounding of data cluster could be ignored
without losing accuracy. According to Chebyshev’s theorem [20], among the data set there are at least
(1− 1
k2
)×100% data which are located within k standard deviations of the mean. Then we can conclude
that most of the training patterns which are not support vectors are located within k standard deviations
and can be discarded by using the Chebyshev’s theorem.
For example, when a data set is about normally distributed as in Fig. 2, the data set is conveniently
divided according to the number of standard deviations from the average. Approximately two-thirds
of the data values will be within one standard deviation of the average, on either side of the average.
Approximately 95% of the data will be within two standard deviations from the average.
Extractive support vector (ESV) algorithm :
In this algorithm, {(xi, di)}Ni=1 represents the training patterns, where di ∈ {−1, 1}. C1i and
C−1j represents the ith cluster in class 1 and the jth cluster in class −1, and dist(x, y) represents the
Euclidean distance between patterns x and y.
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 9
{0, 1, · · · , K − 1}, i ∈ {1, 2, · · · , N}. Then, the output y˜i = FH WM( Xi) denotes the filtering
operation of the H WM filter FH WM(·), where Xi = (xi−L, · · · , xi, · · · , xi+L).
The schematic diagram of H WM filters is shown in Fig. 4. The structure of an H WM filter com-
prises a summation combinator and three filters: the weighted order statistics (WOS) filter, the alpha-
trimmed mean filter and the identity filter, respectively.
In the proposed filters, corrupted data are restored by WOS filters and by alpha-trimmed mean filters
simultaneously. It is well known that WOS filters show very good performance for the removal of
long-tailed noise types (e.g., Laplacian). On the other hand, alpha-trimmed mean filters provide better
performance for noise smoothing and for the short-tailed noise types (e.g., Gaussian). Ideally, the filtering
algorithm should vary from pixel to pixel based on the local context. However, it is extremely hard, if not
impossible, to set the conditions under which a certain filter should be selected, since the local conditions
can be evaluated only vaguely in some portions of an image. By training the neural controller with a set
of input signals and desired signals, it acquires the function of a desired classifier.
Definition 5.1: The output of the H WM filter is defined by
FH WM ( Xi, μi, βi) = ((1 − μi)FWOS( Xi) + μiFMean( Xi))βi + (1− βi)FId( Xi) (21)
where μi ∈ [0, 1] and βi ∈ {0, 1}. FWOS and FMean are the outputs of a WOS filter and a alpha-trimmed
mean filter, respectively. FId( Xi) denotes the output of identity filter and FId( Xi) = xi.
In Eq. (21), βi denotes that the input signal is classified as noise-free or noise-corrupted and μi
denotes the balance between the outputs of WOS filter and alpha-trimmed mean filter. If μi = 0, then the
input signal is considered to be restored by WOS filter. If μi = 1, then the input signal is considered to
be restored by alpha-trimmed mean filter. However, since it is difficult to judge whether the input signal
should be filtered by WOS filter or alpha-trimmed mean filter, μi should take a continuous value from 0
to 1 to cope with any ambiguous cases.
4.2. The Design of H WM filters
In this work we propose a new efficient classifier approach to identify noise by using extractive support
vector algorithm. SVMs have been shown to provide a better performance than the traditional learning
machines [6, 8]. The advantage of SVMs is minimizing the risk of misclassifying not only the examples
in the training set, but also the unseen examples of the test set. The SVMs classify the input signals into
two classes. However, the difficulty in applying SVMs into image restoration is their massive data set.
For a 256 × 256 image, there are 65536 training patterns which make 65536 × 65536 elements on the
kernel matrix. This is hard to implement using just a conventional computer. After the introduction of
the extractive support vector algorithm, the number of training patterns can be greatly reduced.
In Fig. 4, ESVMs classifier classifies the input signals into two classes: noise-free and noise-
corrupted signals. Before the classification by the ESVMs classifier, the input vector must be extracted
from the filter window Xi to identify any noisy pixel. Let Qi be the input patterns for the ESVMs clas-
sifier and Oi be the output, where Oi ∈ {0, 1}. In supervised mode, Oi can be easily determined by
comparing the training image with uncorrupted image.
Definition 5.2: When the input signal is Xi = (xi−L, · · · , xi, · · · , xi+L), Qi is defined by Qi =
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 11
the following update rule.
wˆi(n + 1) =wˆi(n)− λ ∂εi
∂wi
=wˆi(n) + λ(yi − FWOS( Xi) + f(
2L∑
i=1
wˆipi − t)(FWOS( Xi)− FMean( Xi)))
× σpif(
2L∑
i=1
wˆipi − t)(1− f(
2L∑
i=1
wˆipi − t))
(26)
t(n + 1) =t(n)− λ∂εi
∂t
=wˆi(n)− λ(yi − FWOS( Xi) + f(
2L∑
i=1
wˆipi − t)(FWOS( Xi)− FMean( Xi)))
× σf(
2L∑
i=1
wˆipi − t)(1− f(
2L∑
i=1
wˆipi − t))
(27)
vji(n + 1) =vji(n)− λ ∂εi
∂vji
=vji(n)− λ(yi − FWOS( Xi) + f(
2L∑
i=1
wˆipi − t)(FWOS( Xi)− FMean( Xi)))
× σ2piqjf(
2L∑
i=1
wˆipi − t)(1− f(
2L∑
i=1
wˆipi − t))f(
2L∑
j=1
vjiqj − bˆi)(1 − f(
2L∑
j=1
vjiqj − bˆi))
(28)
bˆi(n + 1) =bˆi(n)− λ∂εi
∂bˆi
=vji(n) + λ(yi − FWOS( Xi) + f(
2L∑
i=1
wˆipi − t)(FWOS( Xi)− FMean( Xi)))
× σ2pif(
2L∑
i=1
wˆipi − t)(1− f(
2L∑
i=1
wˆipi − t))f(
2L∑
j=1
vjiqj − bˆi)(1− f(
2L∑
j=1
vjiqj − bˆi))
(29)
where λ is a learning rate. The learning process can be repeated from i = 1 to N or with more iterations.
4.4. Analysis of Time Complexity
The analysis of our proposed model can be divided into three parts: ESV algorithm, SVMs and H WM
filters.
Suppose there are N training patterns in the beginning, m remaining training patterns after step 2
and r testing patterns. On ESV algorithm, computing step 1 takes O(pN) times, where p is the number
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 13
method extracts a certain proportion, typically 11%, of the training set as the tuning set, which is a
surrogate of the testing set. For each training, the proposed method was applied to the rest of the training
data to obtain a filter and the tuning set correctness of this filter was computed. Table 2 and Table 3
show that the H WM filter achieves a significant improvement over the other filters by suppressing both
types of impulse noise and gaussian noise. Fig. 9 shows the comparative restoration results for the
image “Lena” corrupted by random-valued impulse noise at 20% of CWM, neural filters, fuzzy median
filter, classifier-augmented median filters and H WM filters, respectively. Moreover, Fig. 10 shows the
comparative restoration results for the image “Lena” corrupted by gaussian noise at σ = 25 of alpha-
trimmed mean filter, weighted filter, neural filters, fuzzy median filter, and iterative fuzzy control filter
and H WM filters, respectively. It is evident that the H WM filter produces a better subjective visual
quality restored image by more noise suppression and detail preservation.
6. Conclusion
In this paper, we propose an efficient algorithm to solve the problem of requiring massive storage for
kernel matrix. In the proposed algorithm, the support vectors are extracted based on statistics. The
advantage of our proposed algorithm is excellent performance and efficient computation time.
Besides, a hybrid filter which is based on ESV algorithm is proposed for image restoration. The
hybrid filter preserves the advantage of WOS filter and alpha-trimmed mean filter. The extensive simula-
tion results illustrated that that the proposed filter not only possess the capabilities of noise attenuation,
chromaticity retention, and preservation of edges and details but also possess robust and adaptive capa-
bilities.
wT xi + b = 1
wT xi + b = −1
separating hyperplanex
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
support vectors
2
‖w‖
Figure 1. The illustration of constructing optimal hyperplane by SVMs
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 15
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
x
x
x
x
x x
x
x
x
x
x x
x
x
x
x
x
x
x
x
•
•
•
•
•
•
x
•
x
•
x
x
x
x
o
•
o
•
•
•
•
••
o
•
oo
•
o
o
o
o
o
o
•
•
o
• •
•
o
o
o
o
o
o
o
x
x
x
x
x x
••
•
•
x x
•
•
x
(a) (b)
Figure 3. The diagram of ESV algorithm
FILTERS σ = 25 σ = 50 σ = 75 σ = 100
alpha-trimmed mean filter 143.0 362.4 668.7 1033.2
Weighted Filter 165.6 355.4 636.1 926.9
Neural filter 130.9 328.8 584.8 887.9
Fuzzy median filter 150.8 420.1 780.6 1246.3
Iterative Fuzzy Control Filter 152.4 319.2 550.7 876.8
H WM filter 113.1 298.1 545.1 895.2
Table 3. MSE comparison using different filters for ’Lena’ with Gaussian noises.
References
[1] Cristianini, N., Shawe-Taylor, J.:An introduction to support vector machines, Cambridge University Press,
Cambridge, 2000.
[2] Vapnik, V. N.:The nature of statistical learning theory, Springer, New York, 1995.
[3] Scho¨lkopf, B., Smola, A.J.:Learning with kernels: Support vector machines, regularization, optimization,
and beyond, Cambridge, Mass: MIT Press, London, 2002.
[4] Mangasarian, O.L., Musicant, D.R.: Successive overrelaxation for support vector machines, IEEE Trans. on
Neural Networks, 10(5), 1999, 1032-1037.
[5] Chapelle, O., Haffner, P., Vapnik, V.N.: Support vector machines for histogram-based image classification,
IEEE Trans. on Neural Network, 10(5), 1999, 1055-1064.
[6] Guo, G., Li, S.Z., Chan, K.L.: Support vector machines for face recognition, Image and Vision Computing,
19, 2001, 631-638.
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 17
Figure 6. The original data distribution of the checkerboard dataset
Figure 7. The data distribution of the checkerboard dataset after selecting by using ESV algorithm
[10] Platt, J.C.: Fast training of support vector machines using sequential minimal optimization, in Advances
in kernel methods-support vector learning, Scho¨lkopf, B., Burges, C.J.C., Smola, A.J.(Eds.), MIT Press,
Cambridge, MA, 1998.
[11] Burges, C.J.C.: A tutorial on support vector machines for pattern recognition, Data mining and knowledge
discovery, 2(2), 1998, 121-167.
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 19
[24] Yin, L., Astola, J., Neuvo, Y.: A new class of nonlinear filters - neural filters, IEEE Trans. on Signal Process-
ing, 41(3), 1993, 1201-1222.
[25] Arakawa, K.: Median filter based on fuzzy rules and its application to image restoration, Fuzzy Sets and
Systems, 77, 1996, 3-9.
[26] Chang, J.-Y., Chen, J.-L.: Classifier-augmented median filters for image restoration, IEEE Trans. on Instru-
mentation and Measurement, 53(2), 2004, 351-356.
[27] Stone, M.: Cross-validatory choice and assessment of statistical predictions, Journal of the Royal Statistical
Society, 36, 1974, 111-147.
[28] Farbiz, F., Menhaj, M.B.: A fuzzy logic control based approach for image filtering, Fuzzy Techniques in
Image Processing, Springer-Verlag, 2000.
C.-C. Yao, P.-T. Yu, R.-W. Hung / Extractive Support Vector Algorithm on SVMs for Image Restoration 21
Figure 10. Subjective visual qualities of restored image ’Lena’ (a) original image (b) corrupted image by σ = 25
Gaussian noise, and filtered by (c) alpha-trimmed mean filter (d) Neural filter (e) Fuzzy median filter (f) H WM
filter
