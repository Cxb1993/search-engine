 2.1 語言模型 
語言模型是用於估測一串詞序列 Twww ,,, 21 K 的機
率 ),,,(ˆ 21 TwwwP K 。而此一機率則可以分解為一系列的
條件機率組合， 
∏
=
−
=
T
i
iiT wwwPwwwP
1
1121 ),,|(ˆ),,,(ˆ KK            (2.1) 
在上式中，要給定從頭到尾的歷史詞序 121 ,,, −iwww K 是
不易實現的，於是皆會採用有限長度歷史序列的方式予
以近似， 
∏
=
−+−≅
T
i
iniiT wwwPwwwP
1
1121 ),,|(ˆ),,,(ˆ KK        (2.2) 
在上式中， ini ww ,,1 K+− 即語言模型中所謂的詞序列歷史
(history)。語言模型的參數估測，一般皆基於最大相似度
(maximum likelihood)法則。其估測方式是透過在準備好
的訓練文件中計算不同的字詞序列出現之次數， 
),,(
),,(),,|(ˆ
11
1
11
−+−
+−
−+− =
ini
ini
inii
wwC
wwC
wwwP
K
K
K           (2.3) 
其中， )(⋅C 表示所給定的詞序列在訓練文件中的出現次
數。 
由於語言模型中所採用的字詞都是任意組合、長度
不盡相同，最重要的是數量龐大，因此在訓練出來的語
言模型中，未曾出現的字串之語言模型機率有可能為
零，所以，需要使用語言模型平滑化技術以填補之[3]。 
 
2.2 平滑化技術 
在實際的情況中，許多詞與詞所形成的詞序列，並
不存在於訓練語料庫中，出現這種情況時，此稱為資料
稀疏(data sparseness)。雖然如此，並不表示這樣的詞序
列組合不會出現在測試集合中。舉例而言，在雙連語言
模型中，假設辭典中有八萬詞，則可能的雙連詞對數目
將會有六十四億個( 92 104.6000,80 ×= )，但是一般的訓
練語料庫中，通常只會有數千萬到數億則雙連詞序列，
所以，未出現事件(unseen event)是佔很大比例的，將使
得許多雙連詞序列的對應語言模型機率值為零，而會使
得語音辨識時，整句話的語言模型對應累積機率 )(WP
為零。 
為了解決上述問題，必須對於未出現詞序列事件的
機率值加以平滑化(smoothing)處理，「平滑化」即是少
許調整原本估測好的機率值，以避免有單項機率為零的
情況產生。其基本概念之做法就是扣除小部分的已出現
事件(seen event)的機率值，分配給未出現事件，使得未
出現事件的機率值不為零。目前，已有許多平滑化方法
被提出，以解決資料稀疏的問題， 底下介紹兩種最常使
用的平滑化技術。 
 
2.2.1 聶氏平滑法(Kneser-Ney Smoothing) 
聶氏平滑法是根據絕對折扣法(absolute discounting 
smoothing)所發展出來的，絕對折扣法的重點在於，每
次由已出現事件所扣除的折扣為一個固定的常數 D ，
1≤D ，同時，也建議  
21
1
2nn
n
D
+
=                              (2.4) 
其中， 1n 與 2n 分別代表出現1次與2次的 N 連詞序列數
目。 
假設我們正在預測以下這句話「壽星可以在生日那
天收到」的後接字詞，很明顯的「禮物」這個詞會比
「薪水」更合適。然而，如果訓練資料中，薪水的出現
次數比禮物多的話，那麼單連模型會選擇薪水作為後接
字詞。但是，薪水出現比較多次的場合只是在「收／
發」「薪水」的組合中比較多。所以，聶氏平滑法就是
基於我們對單詞 w出現在不同前後文的次數之估測而
來。  
聶氏平滑法的分配原則與凱氏平滑法有些類似，都
是將已出現事件的機率值做折扣，再分配給所有詞序
列，或分配給未出現事件。  
對於雙連語言模型的聶氏平滑法，我們定義以下幾
個名詞：  
1. 詞對 )( ,1 ii ww − 中的 iw 為「尾詞」  
2. 詞對 )( ,1 ii ww − 中的 1−iw 為「詞首」 
3. 在訓練語料中，尾詞之前所接的不同詞數為「前
接詞數」 
4. 在訓練語料中，詞首之後所接的不同詞數為「後
接詞數」 
 
聶氏平滑法又分為兩種：  
1. 聶 氏 內 插 法 (Kneser-Ney Interpolation 
Smoothing)，是聶氏於1991年提出的方法，以雙連語言
模型為例，公式如下：  
V
w
wC
DwwC
wwP
i
i
ii
ii
1)()(
}0,),(max{
)|(
1
1
1
1KN
×+
−
=
−
−
−
−
α
         (2.5) 
其中V 代表辭典中所有詞彙的個數。 
)(
),(
1
)(
1
11
0),(| )|(KN
1
1
1
−
−+
>
−
•×
=
−= ∑
−
−
i
i
wwCw
ww
i
wC
wCD
P
w
jij ii
α
 
則 代 表 所 有 被 折 扣 的 機 率 量 。 其 中 ，
{ }0),(|),( 111 >=• −−+ kiki wwCwwC 。 
(3.2)式主要的意義則是將被折扣的機率量 )( 1−iwα 與所有
以 1−iw 為詞首的詞序列內插，也就是將被折扣的機率量
)( 1−iwα 分配給所有以 1−iw 為詞首的詞序列，而非僅分配
給以 1−iw 為詞首，但 )( ,1 ii ww − 未出現於訓練語料者。分
配的方法是平均分配給所有可能的 iw ，又因為所有於辭
典中的詞都可能是 iw ，故平均分配給辭典中的V 個詞。 
 
 1
1
1
1
*
))(1(1
))(1(
n
nk
n
nk
r
r
d
k
k
r
+
+
+
−
+
−
=                        (2.16) 
 
三、 音節歷史為主之語言模型平滑化 
3.1 以類別為主之 N 連語言模型 
以類別為主之 N 連語言模型[1]，最常用者為 IBM
模型，又稱為 IBM clustering，此為一種硬分割(hard 
clustering)的方式，意即每一個字詞(word)只屬於某一個
類別(class)。以雙連語言模型形式的 IBM模型可表示為 
)|()|()|( 11 −− ×≈ iiiiii ccPcwPwwP             (3.1) 
當我們採用最大相似度估測時，上式中的機率可計算如
下， 
∑ −
−
−
=
=
c i
ii
ii
ccC
ccC
ccP
cC
wC
cwP
)(
)()|(
)(
)()|(
1
1
1
                  (3.2) 
 
3.2 以音節歷史為主之 N 連語言模型平滑化技術 
由上述之類別為主之語言模型衍生，在中文的字詞
中，尤其是在語音辨識之應用上，語音模型層次通常只
能辨識至音節(syllable)層次，對於音節以上至字詞之間
之對應關係是屬於一種一對多的關係。所以，我們在本
篇論文中提出一種採用音節序列取代原本字詞作為歷史
序列之作法。 
以音節為歷史之雙連語言模型之表示式為 
)|()|()|(
11 −×≈− iii wwwiiis ssPswPwwP             (3.3) 
其中， ws 表示字詞 w所對應之音節序列。 
最後，我們運用平滑化技術，將音節為歷史之雙
連語言模型與基本語言模型內插， 
)|()1()|()|(~ 111 −−− ⋅−+⋅= iisiiii wwPwwPwwP αα   (3.4) 
其中，α 為內插權重值，其大小由實驗資料決定。 
 
四、 效能評估 
在本篇論文所提出之以音節為歷史之語言模型平滑
化技術，整體之實驗流程如圖一所示。在實驗中，我們
使用了六個在 1998 至 1999 年間收集的文件集，分別來
自中華日報(chd1998-1999)、工商時報(ctc1998-1999)、
中時晚報(cte1998-1999)、中國時報(cts1998-1999)與聯合
報系(udn1998 與 udn1999)。這些新聞文件集，我們在最
長詞優先斷詞演算法之斷詞處理後，便採用語言模型研
究最常使用之工具集，SRILM[5]進行語言模型訓練與複
雜度評估。我們將這些文件集分為四種組合，分別是
tg0~tg3，每一種組合中我們各取前四份文件作為測試文
集，在此不取聯合報系之兩年文集作為測試集合，以評
估訓練與測試文集之複雜度，所以，tg0~tg3 之組合如表
一所示，其各種平滑化技術處理後之複雜度比較如表二
所示。 
 chd ctc cte cts udn98 udn99 
tg0 - + + + + + 
tg1 + - + + + + 
tg2 + + - + + + 
tg3 + + + - + + 
*加號(+)表示該訓練文集含有該份文件集；減號(-)則
否，即是將來作為測試文集之資料 
表一、訓練與測試文集之組合表 
 
 chd ctc cte cts 
tg0 610.2 161.7 187.4 14.6 
tg1 153.5 493.5 187.2 14.7 
tg2 153.3 161.5 402.2 15.3 
tg3 133.3 142.2 158.0 132.7 
表二(a)、Absolute discounting後之複雜度 
 
 chd ctc cte cts 
tg0 522.6 180.6 213.6 35.8 
tg1 177.7 438.0 214.2 36.0 
tg2 176.6 180.2 387.7 34.7 
tg3 157.3 164.2 191.9 158.6 
表二(b)、Kneser-Ney smoothing 
 
五、 結論與未來展望 
在本研究中，我們提出語言模型之改良構想，並從
中比較了現行最常被使用的語言模型技術之複雜度。從
實驗結果中可以發現，最常被採用的幾種語言模型平滑
化技術對於語言模型複雜度的改善均有不錯的成效。然
而，在中文文句的處理上我們發現一個和西方語言不同
之處，就是斷詞結果對語言模型之影響。斷詞之研究是
一個相當複雜也十分有挑戰性之議題，雖然過去有許多
研究成果已經提出，我們還是覺得其中應該還有值得進
一步研究的問題，比如：將語言模型應用至大詞彙連續
語音辨識系統中時，如何透過對斷詞結果之後處理提升
語言模型與語音辨識兩者之契合度是我們將繼續深入探
討的問題之一。 
參考文獻 
[1] Daniel Jurafsky and James H. Martin, Speech and Language 
Processing, Prentice Hall, 2009. 
[2] Chengxiang Zhai,John Lafferty, “A Study Of Smoothing Methods 
For Language Models Applied To Information Retrieval”, Carnegie 
Mellon University, 2004 
[3] Steven Young, et al., The HTK Book version 3, Microsoft 
Corporation, 2000. 
[4] Yi-Ling Lu,Hsin-Chia Fu, “The Study Of Language Model    
Enhancement For Mandarin Speech Recognition“,National Chiao 
Tung University, 2005 
[5] Andreas Stolcke, "SRILM - An Extensible Language Modeling 
Toolkit", in Proc. International Conference on Spoken Language 
Processing, vol. 2, (Denver, CO), pp. 901--904, September 2002. 
 5 
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                     日期：99 年 10 月 30 日 
一、參加會議經過 
ICASSP 是國際上從事聲學、語音、語言相關研究每年最大的研討會。在此研討會上，可以看到許
多在此一領域最熱門與最新穎的研究成果發表。該會議的研究發表方式是採用 Lecture 與 Poster
並行式，對於較有特色的文章會歸類到採用上台報告方式的 Lecture 議程，其他的則是歸類到
Poster 議程。然而，並不是在 Poster 議程發表的文章就 比較不好，能被此一研討會接受的論文，
皆是一時之選。會議每天都有排定一場由傑出研究成就的學者主講的演講，從中可以獲知在某一
領域中到目前為止最完整的研究成果整理。Lecture 與 Poster 議程每天皆有上午一場與下午兩場 
的安排，每一個場次皆為二小時，上午在十點前有一個中場休息時間，與會的學者可以在那時間
段裡，彼此寒暄或是就著研究內容、論文內容交換彼此的意見。在 Lecture 會場中，每一組別有
六篇論文發表，而同時段在 Poster 會場上繞一圈，則可以與每一篇論文的作者就研究海報上的內
容進行面對面的討論，是一種與 Lecture 完全不同的交流方式。 
二、與會心得 
計畫編號 NSC98－2221－E－168－039－ 
計畫名稱 多重層次為主之語言模型於大詞彙連續語音辨識之先期研究 
出國人員
姓名 
黃志賢 
服務機構
及職稱 
崑山科技大學資訊工程系 
助理教授 
會議時間 
99年 3月 14日至 
99 年 3 月 21 日 
會議地點 美國德州達拉斯 
會議名稱 
(中文)2010 年國際聲學、語音與信號處理研討會 
(英文)2010 IEEE International Conference on Acoustics, Speech 
and Signal Processing 
發表論文
題目 
(中文)無 
(英文) 
附件四 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
