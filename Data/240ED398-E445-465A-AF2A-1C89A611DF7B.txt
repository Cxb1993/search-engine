2Abstract
Traditionally information retrieval consists mainly of determining which documents of a collection contain
the keywords in the user query. However, a growing number of tasks, especially those related to Semantic
Web technologies and applications rely on accurately measuring the similarity between documents and online
texts. Instead of giving the absolute similarity degree of two documents, this research presents a semantic
corpus and Formal Concept Analysis-based procedure to build the concept map for a given set of documents
and quantify the semantic relations between the concepts. The proposed approach includes three algorithms–
a) the concept lattice constructing algorithm, b) the concept similarity measure, and c) the sub-lattice
similarity measure. The experiment results also demonstrate that our proposed approach has outstanding
performance in measuring the semantic relations of concepts and texts.
Keywords: Formal Concept Analysis, Similarity, Semantic Web, Ontology
1. Introduction and related works
Natural language is the means by which humans communicate. How to make computers understand
natural language and use it to interact with humans is the focus of research in natural language processing
(NLP). Recently the field of NLP presents a need for efficient algorithms and methodologies to evaluate the
similarity between online texts and web-based documents, which including the applications of intelligent
expert systems, QA systems, query-based information retrieval, machine translation, automatic summarization
systems, online learning, and etc. With the rapid development of the Semantic Web technologies and
applications, representing the inner meaning of web based documents and measuring the semantic similarity
between them has become the most popular research trend.
1.1. Classic IR models
Traditional technologies for detecting similarity between documents mainly focus on analyzing
co-occurrence words [1]. With its inherent simplicity, such model received great attention in past years and is
usually efficient in dealing with long texts. However, the method suffers from major drawbacks. First, it is a
binary criterion of words (i.e., two words are considered as the same or not the same) without any notion of a
grading scale, which prevents good measuring performance. In addition, it is hard to apply such method to
measure the similarity of short text and sentences since the co-occurring words may be rare or even null. The
other traditional model is the Vector Space Model (VSM) [2, 3]. In VSM, the words, phrases, sentences, or
articles are represented by a high dimensional vector, and the base space was constructed by all the
non-stopwords presented in the system. The natural language elements mapped in this vector space are
correlated to each other according with geometric distance of their associated vectors in this space. Wu et al.
presents a VSM-based FAQ retrieval system. The vector elements are composited by the question category
segment and the keyword segment[4]. A phrase-based document similarity measure is proposed by Chim and
Deng [5]. In [5], the TF-IDF weighted phases in Suffix Tree [6, 7] is mapped into a high dimensional term
space of the VSM. Very recently, Li et al. [8] presented a novel sentence similarity computation measure.
Their measure, took the semantic information and word order into account, which acquired good performance
in measuring, is basically a VSM based model.
Well known approaches for semantic space building is the Latent Semantic Analysis (LSA) model [9-11].
LSA, also known as Latent Semantic Indexing (LSI), is a fully automatic mathematical/statistical technique
that analyzes a large corpus of natural language text and a similarity representation of words and text passages.
In LSA, a set of representative words or phrases needs to be identified from a large number of contexts. A
term-document matrix which describes the occurrences of terms in documents is formed via the presence of
words in contexts. The matrix is then decomposed by singular value decomposition (SVD) into the product of
three other matrices. In this way, the dimensions are reduced and the original word by context matrix is then
reconstructed from the reduced space. One of the standard probabilistic models of LSA is the Probabilistic
Latent Semantic Analysis (PLSA), which is also known as Probabilistic Latent Semantic Indexing (PLSI) [12].
PLSA uses mixture decomposition to model the co-occurrence words and documents, where the probabilities
are obtained by a convex combination of the aspects. LSA and PLSA have been widely applied in information
4Definition 3. Formal Concept. A Formal Concept C in a context is a pair (O’, A’) that satisfies A’= Intent (O’)
and O’= Extent(A’),
i.e.,
C is a Formal Concept for O’ C and A’ C, Extent(Intent(O’)) = O’, and symmetrically,
Intent(Extent(A’)) = A’.
Definition 4. Concept Lattice. For a context T = (O, A, R) and two formal concepts C1 = (O1, A1), C2 = (O2, A2)
of T, a concept hierarchy relation is given by:
C1 C2 (O1⊆O2) (A1⊆A2)
All the formal concepts in T are ordered by the concept hierarchy relation and the ordering is called the
Concept Lattice of the context T.
3. Semantic similarity measurement algorithms
This section presents the semantic similarity measurement algorithms. The basic idea of our approach is
divided into three parts, and each part is designed as an independent algorithm which can be applied to other
knowledge representation systems.
Table 1. Concept Lattice Constructing Algorithm
Algorithm 1. BuildConceptLattice
01 :
02 :
03 :
04:
05:
06 :
07 :
08 :
09 :
10 :
11 :
12 :
13 :
14 :
15 :
Initialization :
Let O be the set of objects, A be the set of attributes, and C(O, A, R) be the set
of formal concepts in the context.
Lattice← ;
T← C(O, A, R);
FOR ALL c1 T DO
FOR ALL c2 T－{c1} DO
IF (c1 c2) { c3 T－{c1, c2} s.t. [(c1 c3) (c3 c2)]} THEN
Lattice← Lattice {{c1, c2}};
END IF
END FOR
END FOR
FOR ALL c T DO
IF { T－{c} s.t. [ = (O, ) = ( , A) T－{c, } s.t
((c ) ( ))]} THEN
Lattice← Lattice {{ , c}, {c, }};
END IF
END FOR
605 :
06 :
07 :
09 :
10 :
11 :
12 :
13 :
14 :
15 :
FOR ALL c1 LI DO
SIM← 0;
FOR ALL c2 LM DO
SIM←MAX(ConceptSimilarity (SIM, c2));
LM← LM {c2};
END FOR
LI← LI {c1};
SIMLATTICE←SIMLATTICE SIM;
END FOR
RETURN SIMLATTICE;
B. The Concept Similarity Algorithm
This section presents the second part of our approach–the concept similarity algorithm, which is listed
in Table 2. The algorithm accepts two concepts and returns the semantic similarity between them. The basic
idea is calculating the semantic distance instead of the general Euclid distance (i.e., the differences of the
number of attributes, instead of the number of hops).
C. The sub-lattice similarity measure
The third is the sub-lattice similarity measurement algorithm, which is constructed on the basis of the
concept-concept similarity measure (i.e. Algorithm 2). The algorithm accepts two sub-lattices S1 and S2
(concept sets of the original lattice), which represent the corresponding texts. This algorithm evaluates the
similarity degree between two sub-lattices by the following point of view: Let n, m be the number of objects
of S1 and S2. If n m, we can build an n m matrix and sum the largest number of each row. The algorithm
is shown in Table 3.
As mentioned above, our FCA-based algorithms can build the concept lattice automatically for a given
data set, and eventually return the semantic similarity degree of a given text pair (sub-lattices). Besides, the
concept similarity measure (Algorithm 2) can be applied to evaluate concept similarity with arbitrary FCA
structure.
4. Experimental Results
In this section, we compare FCA-based semantic approach to Jaccard coefficient [38], VSM [39], tfidf
Vector [40] on the Reuters-21578 text categorization test collection Distribution 1.0 [41], which is a classic
benchmark for text processing algorithms. The data set has used stop words and stemming using Porter’s
suffix stripping algorithm [42]. To enhance the diversity of information, the base data was randomly sampled
from Reuters-21578 and we scaled the number of concepts into the range [30~200] + 1%. The main
evaluation metrics were based on Precision, Recall, Accuracy, and fMeasure, the geometric mean of precision
and recall, which were defined as in [43]. Table 8 summarizes the characteristics of the dataset and presents
our experimental results, and Table 9 illustrates the average Accuracy and fMeasure in Jaccard coefficient,
VSM, and tfidf Vector under different thresholds.
Table 4. Average performance under different thresholds–the semantic approach
Similarity
Threshold
Precision
Sensitivity
(Recall)
Specificity Accuracy fMeasure
0.0 0.54 1.00 0.00 0.54 0.70
0.1 0.57 1.00 0.11 0.59 0.72
0.2 0.59 1.00 0.25 0.64 0.74
8[2] Salton, G., "The SMART Retrieval System - Experiments in Automatic Document Processing", Prentice
Hall Inc., Englewood Cliffs, NJ, 1971.
[3] Salton, G., and Lesk, M.E., "Computer evaluation of indexing and text processing", Journal of the ACM,
vol. 15, no. 1, pp. 8-36, 1968.
[4] Wu, C. H., Yeh, J. F., and Lai, Y. S., "Semantic segment extraction and matching for Internet FAQ
retrieval", IEEE Transactions on Knowledge and Data Engineering, vol. 18, no. 7, pp. 930-940, 2006.
[5] Chim, H., and Deng, X., "Efficient Phrase-Based Document Similarity for Clustering", IEEE Transactions
on Knowledge and Data Engineering, vol. 20, no. 9, pp. 1217-1229, 2008.
[6] Zamir, O. M. O., Etzioni, O., and Karp, R. M., "Fast and Intuitive Clustering of Web Documents", In
Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining, pp. 287-290,
1997.
[7] Zamir, O., and Etzioni, O., "Web Document Clustering: Feasibility Demonstration". In Proceedings of
the19th International ACM SIGIR Conference (SIGIR'98), pp. 46-54, 1998.
[8] Li, Y., McLean, D., Bandar, Z. A., O'Shea, J. D., and Crockett, K., "Sentence similarity based on semantic
nets and corpus statistics’, IEEE Transactions on Knowledge and Data Engineering, vol. 18, no. 8, pp.
1138-1150, 2006.
[9] Foltz, P. W., Kintsch, W., and Landauer, T. K., "The Measurement of Textual Coherence with Latent
Semantic Analysis", Discourse Processes, vol. 25, no. 2-3, pp. 285-307, 1998.
[10] Landauer, T. K., Laham, D., Rehder, B., and Schreiner, M. E., "How Well Can Passage Meaning Be
Derived without Using Word Order? A Comparison of Latent Semantic Analysis and Humans", In
Proceedings of the 19th Ann. Meeting of the Cognitive Science Soc., pp. 412-417, 1997.
[11] Landauer, T. K., Foltz, P. W., and Laham, D., "Introduction to Latent Semantic Analysis", Discourse
Processes, vol. 25, no. 2-3, pp. 259-284, 1998.
[12] Hofmann, T., "Probabilistic latent semantic indexing", In Proceedings of SIGIR ’99, pp. 50–57, 1999.
[13] Wu, C. H., Hsia, C. C., Chen, J. F., and Wang, J. F., "Variable-Length Unit Selection in TTS Using
Structural Syntactic Cost", IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4,
pp. 1227-1235, 2007.
[14] Bellegarda, J. R., "Exploiting latent semantic information in statistical language modeling", In
Proceedings of IEEE, vol. 88, no. 8, pp. 1279-1296, 2002.
[15] Zhou, K., Gui-Rong, X., Yang, Q., and Yu, Y., "Learning with Positive and Unlabeled Examples Using
Topic-Sensitive PLSA ", IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 1, pp.
46-58, 2010.
[16] Chou, T. C., and Chen, M. C., "Using Incremental PLSI for Threshold-Resilient Online Event Analysis
", IEEE Transactions on Knowledge and Data Engineering, vol. 20, no. 3, pp. 289-299, 2008.
[17] Kim, H., and Seo, J., "Cluster-Based FAQ Retrieval Using Latent Term Weights", IEEE Intelligent
Systems, vol. 23, no. 2, pp. 58-65, 2008.
[18] Kanejiya, S. P. D. and Kumar, A., "Automatic evaluation of students answers using syntactically
enhanced LSA", In Proceedings of the Human Language Technology Conference (HLT-NAACL 2003),
Workshop on Building Educational Applications using NLP, pp. 53-60, 2003.
[19] Gruber, T. R., "Toward principles for the design of ontologies used for knowledge sharing", International
Journal Human-Computer Studies, vol. 43, o.s 5-6, pp. 907-928, 1995.
[20] Berners-Lee, T., Hendler, J., and Lassila, O., "The semantic web", Scientific American, 2001.
[21] Gruber, T. R., "A translation approach to portable ontology specifications", Knowledge Acquisition, vol.
5, no. 3, pp. 199-220, 1993.
[22] Guarino, N., "Understanding, Building and Using Ontologies: A Commentary to Using Explicit
Ontologies in KBS Development", International Journal of Human and Computer Studies, vol. 46, no. 2-3,
pp. 293-310, 1997.
[23] Albert, L. K., "YMIR: an ontology for engineering design", Ph.D. Thesis, University of Twente, Twente,
The Netherlands, 1993.
[24] Van Heijst, G., Th. Schreiber, A. and Wielinga, B. J., "Using explicit ontologies in KBS development",
International Journal of Human and Computer Studies, vol. 46, no. 2-3, 1994.
[25] Guarino, N. and P. Giaretta, "Ontologies and knowledge bases: Towards a terminological clarification",
In Towards very large knowledge bases: Knowledge building and knowledge sharing, pp. 25-32, 1995.
[26] Uschold, M. and M. Gruninger, "Ontologies: principles, methods, and applications", Knowledge
Engineering Review, vol. 11, no. 2, pp. 93-155, Knowledge Engineering Review, 1996.
[27] Schreiber, A. T., Wielinga, B. J., and Jansweijer, W. H. J., "The KACTUS view on the 'O' Word", In
IJCAI Workshop on Basic Ontological Issues in Knowledge Sharing, pp. 159-168, 1995.
[28] RDF. Available from: http://www.w3.org/RDF/.
[29] XML. Available from: http://www.w3.org/XML/.
[30] RDF-Schema Available from: http://www.w3.org/TR/rdf-schema/.
[31] OWL-REF. Available from: http://www.w3.org/TR/owl-ref/.
[32] WordNet. Available from: http://wordnet.princeton.edu/.
國科會補助專家學者出席國際學術會議心得報告
日期： 年 月 日
一、參加會議經過
今年於韓國首爾舉辦的 2nd International Conference on Data Mining and Intelligent
Information Technology Applications (資料探勘與智慧型資訊應用技術研討會)是由首
爾大學與 IEEE 所合辦的聯合會議之一。此聯合會議是人工智慧與資訊處理領域的年
度重大盛事。能參與會議並在大會上宣讀論文實為本人之榮幸。這次所發表的論文
主要是提出一組以正規概念分析為基礎的文章相似度評估演算法，並以九年一貫英
語文學習相關教材為案例研究對象。此論文所設計的評估程序主要包含三個獨立的
演算法 – (a) 符合背景教材的 FCA 知識圖形化網格建構演算法，(b) FCA 圖形化
網格中概念對概念相似度評估演算法，以及(c) FCA 圖形化網格中文章對文章相似度
評估演算法。
補助編號 NSC 99－ 2221 － E－ 130 － 013－
計畫名稱
以知識本體與正規概念分析為基礎的整合型自我學習框架之研究 - 以九年一貫
英文學習為例
出國人員
姓名
劉振隆
服務機構
及職稱
銘傳大學 資訊傳播工程學系
會議時間
2011 年 9 月 30 日
至
2011 年 10 月 2 日
會議地點
韓國 首爾
會議名稱
(中文)第二屆資料探勘與智慧型資訊應用技術研討會
(英文)2nd International Conference on Data Mining and Intelligent
Information Technology Applications
發表論文
題目
(中文)以正規概念分析為基礎的概念建構與相似度評估演算法
(英文)FCA based Concept Constructing and Similarity Measurement
Algorithms
國科會補助計畫衍生研發成果推廣資料表
日期:2011/10/26
國科會補助計畫
計畫名稱: 以知識本體與正規概念分析為基礎的整合型自我學習框架之研究 - 以九年一
貫英文學習為例
計畫主持人: 李明哲
計畫編號: 99-2221-E-130-013- 學門領域: WEB 技術
無研發成果推廣資料
博士後研究
員 0 0 100%  
（外國籍） 
專任助理 0 0 100%  
其他成果 
(無法以量化表
達之成果如辦
理學術活動、獲
得獎項、重要國
際合作、研究成
果國際影響力
及其他協助產
業技術發展之
具體效益事項
等，請以文字敘
述填列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）
人數 0 
 
 
