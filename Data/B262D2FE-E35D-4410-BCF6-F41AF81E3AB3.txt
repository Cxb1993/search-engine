2I. 
	
 !"#
$%&'()*+,	-./01$23456789:;
<=>?@ABCDEFGHIJKLMN*OPQRSTUGHVW
XY$6ZC[\]<^_`abc\defg<=hKijklm
nKVJ+CD/X6Pcopqrsstuvs<=wxyK<=
XWzVW{|$6}~F,	-./456789K<=
:'p:;<=w.X
' ;¡¢£¤¥u¦§¨FO¡©ª«w ;¬­0
1®¯5°±²¬;³´µ-¢¶·¸¹º¥»,	¼½¾
¿kkÀ>ÁÂk´$«w+ÃÄÅK<=
f<=ÆXÇ<=_k<=_kÈ¼ÉkÊwx>ËÌ"Í+ÎÏÐÑÒÃw
&ÓÔÕÖ>­ Local Intelligent £¤×ØÙÚÛÜrÝÞßO¥uº¥;à
á®Oâãä;å;àÓÔæçè«wé|ÓÔÕ
Ö£¤ên´$w)ÐÑF
O¥uº¥ ;wëìí!gîïIðñIð!òóôæ5õÆö¡
¢C[÷éøâI%ùú¸¹û²üý7F1wþUdWÓ
Ó÷þã|þ	Uùú$
âãIð5õFO¥u|£Æ
|Ohn5åoE\]¿5°ÄÅ½¾ëÆ²	'
º
,ò<=Éef«<=XK>Ó
3	'
-.8>VW|OF
kK<=k«wkÀ>ÁÂkRFIDk !¿"
II. 
Due to the development of medical techniques and the lower birth rate of population, the aging
problem becomes an important issue over the world. The advance countries such as America, Euro,
and Japan, they all pay a lot attention on healthcare of the aging. They also invest a lot funding to
develop a comfort living space for the elders and disable people. We should note that the aging rate
of Taiwan is the just behind Japan. It is top 2 over the world. We can image that the people who
need long term care are much more than now. The needed of care human resources, elder houses,
aging communities and elder care environments will have large increase. If we don't have good
policy against this issue, the aging problem will have great impact on our country.
Our lab has good foundation on the development of robots, and we also publish a lot of
conference on the "data fusion" technology. This project step out from the technology, through the
body monitoring system, sensor network, ubiquitous technology, and multi-function robots to
4Ãé÷Õ|}~ñwtÆ²r}k²oF
ÆòówK\t8 !!&67jx%ÄÅU
Ið5õówèF1Æòó!g !5õ.k.n<
.1Äï;UÆw3ûC1
¼F¸¹A%g6ruíS´jx%ñÃ¹&
ïõ3Xw2Ke GATEWAYF5°±'!÷õ
 Aþtz !ûä²$6¡¢F»njxwÆö
(!£¤&5õ !&¥Ãû<I&âãntr}¦
§¨JK>U=©ª2X5°X«s¬­5°È¼®êÈ¼Ék<=
ÉPU¯°­rÎ±²UW3L;ä³´ÃwÈ¼É¼F
¸¹FG	:;»0"T¼µ¶ÃÄ2· !è¸-g2AB:¤n-
¢:	
°­²­¸:n¸
|~ÃòóVJKÊkBCkAk¹®>
º3 !
V. 
1. 	
%»í¼¼k£½¼kþhók+!¾|FMCU(Micro Control Unit)
%6t¿JK»íÀsÁÃ5°3X2X5°±(data
fusion)¸¹XÊÂwE8å£äÃe%ÄwÅ[Æt
ÁÓùúw÷IF¸¹!N¥%Ç¥pð 1
Sensor Board
Mote
ATmega128
IR Sensor
Motor Motor
Compass
Serial
Upper Layer
Bottom Layer
Po
w
e
rU
nit
ð 1%Ç¥
[ CrossbowÈÉ!YMicaz Mote% ATmega128LKPÊ¼¼
3ñ Zigbee õtË	%&ïÌFATMEGA128L ÍÎ'}¤k¤Ï
Ð 8ã¼Ñ"xFr 128K Flash ROM4K EEPROMÒ 4K Bytes SRAM
ÓªÔcÕþÖ%dÆ×Ã"xJKØAûþÈ¼FõyÙ CC2420
ÍÎ¿'Æé 2.4GHzJK¤h3n IEEE 802.15.40Újx£½Û;F¸¹¬
Ü
X¥wEAwâã>5°£½d;àä5°±
6(5)
(4)
( ) nTT yBBBb
a
a
1−∧
=	






= (4)
( )
( )
( )
( ) 		
	
	
	
	
	
	
	














+−−
+−
+−
+−
=
1)()1(
2
1
1
1)4()3(
2
1
1)3()2(
2
1
1)2()1(
2
1
)1()1(
)1()1(
)1()1(
)1()1(
nxnx
xx
xx
xx
B

(5)
[ ]Tn nxxxxy )(,),4(),3(),2( )0()0()0()0( = (6)
 	 t=k+1! "!"#$#
(7)%
( ) ( ) ( )kxkxkx )1()1()0( 11 ∧∧∧ −+=+ (7)
8φ M
M
θd
&4Eight sensors in a ring with 90˚ servo rotation capability
)*+1=@
 	CAB (opening angle)50 KHz Polaroid ultrasonic range sensor +D λ C
 0.6 DE , EF"	CAB α2 .
 F)*+ Sonar Cone Uniform Distribution
αθα
θ
α
θ
θ
+∠≤≤−∠







=
=

s
i
s
i eewhere
PR
R
P
1)(
2
1)(
(9)
 )(θP  sonar cone /GH	I'J s
ie∠ K s . sensor  i 
 F)*+ Sonar Cone/L?E.	I
0=• si
s
k et (10)
)cos(
)sin(
)cos(
)sin(
)cos(
ki
T
s
i
s
i
ki
ki M
e
e
M
M ϕφ
φ
⋅=
	
	







∠
∠
•	






(11)
 iM )*+M,G kϕ  sonar cone	B
0)( 21 =•− sislsk ett (12)
 )*+M4NHM,G"OI'4( .
&
& 5)*+O4( M!&
z
maxzminz
)(zσ
10
 SICK * LMS200 =	,
LA]NAG^,-T'()"_`
a?+8-TKES,GbcF-#TQ'+ LRF568'+
 LRF
d/
,-./U-45~45>e"_`a%Hfgh
#0 3D12Ri?WjkVWGb$#LRFXYa=.& 9"%
&9	,
a=
8=LRF"#aElamE0 LRFZB>
n12
B>o8=0(0jpqrQ 3D12Ri3=.& 10%
&103D12Ri[&
& 10 o LRF d-45~45 >./ao",G0 256 \]
4/Msr"t^#[&%58XYZ#duEv^6_`	,

d"VZ,GoE7\8$F'()w LRF-.
9d]oMatlab",G0GH"M,G/ab"&c
.& 12"%
&11:"x12 &12:E;&
X
Z
Y Range plane, R
d
12
&1a[n12=?%
(2) EE[n12x?%'()01(5FNx123`/
?%GY]`~R1(^[nEE124/12=
i]1n_ F"6/\0\?%eF
A%#ex?
&1bEE[n12=?%
(3) [n12x?%'()01(x12o1(i+
#12=/\?%GY]`~R1(^ F
"6/\?%cieFA%#ex
?%
& 1d[n12=?%
7. (Auto Docking and Recharge System)

byzd{|}~R'()12g$Q1pqh#>?
12TzABx6Q6B230e 4623
Global
Map
Planning
Optimal
Path
Planning
Motion
Planning
Global Map
Planning
Environment
Modeling
Local
Planning
Motion
Planning
Sensors Environment Modeling
Motion Planning
Obstacle Avoidance
14
& 1h6Q6:23$+&
(3) e H23VW6Q6B23uv4'()1n6:[ ¡
x
CXYe HI%
& 2ie H23J&
8. (Fire Detection)
123OEj;:x$+%.#kl=1(f¢m£ /"

.Ykn¤¥Ro?bp>
iR¦#kng"

q=kn1(
/x§¨%
kn¤56kn¤56oN|1(7rs¤1(mx>¤1
(mkl¤1(sroN|1(dkn¤t%
9. (Intruder Detection)

 CCD'('9u)!1(©o(0jpqdvw)x /]r
Q)!1(/g~y?zªf
RGzªxE(uv
ª«S#%
)x¤xjo CCD{ijkd&({i{id¥R
6~o-.¬"(6-­#^G|6®'()}l
k¯ ;:23'()! i¬"
&(o(0j
pq5S/'~­Jd#)x¤%
16
4. Ulyanov, S.V.; Yamafuji, K.; Miyagawa, K.; Tanaka, T.; Fukuda, T.; ”Intelligent fuzzy motion control of mobile
robot for service use,” Proceedings of Intelligent Robots and Systems 95. 'Human Robot Interaction and
Cooperative Robots',. 5-9 Aug. 1995.
5. Kim, K.R.; Lee, J.C.; Kim, J.H.; “Dead-reckoning for a two-wheeled mobile robot on curved surfaces,” IEEE
International Conference on Robotics and Automation, April 1996.
6. Yongxing Hao; Laxton, B.; Agrawal, S.K.; Lee, E.; Benson, E.; “Planning and control of UGV formations in a
dynamic environment: a practical framework with experiments,” IEEE International Conference on Robotics and
Automation, 2003. ICRA '03, 14-19 Sept. 2003.
7. P. Krammer and H. Schweinzer, “Localization of Object Edges in Arbitrary Spatial Positions Based on Ultrasonic
Data,” IEEE Sensors J., vol. 6, no.1, pp.203-210, Feb.2006.
8. H. Choset, K. Nagatani, “The Arc-Transversal Median algorithm: an approach to increasing ultrasonic sensor
accuracy,” IEEE Trans. Robot. Autom., vol.19 no.3, pp. 513-521, Jun,2003.
9. Ge Bin; Yasuda, G.; "Confirming the location and moving trend of small motion robot by color picture division"
Proceedings of the 4th World Congress on Intelligent Control and Automation,, 10-14 June 2002.
10. Conte, G.; Zulli, R.; "Hierarchical path planning in a multi-robot environment with a simple navigation function"
IEEE Transactions on Systems Man and Cybernetics, April 1995.
11. Park, M.G.; Lee, M.C.; "Experimental evaluation of robot path planning by artificial potential field approach with
simulated annealing " Proceedings of the 41st SICE Annual Conference , 5-7 Aug. 2002.
12. Steinhaus, P.; Walther, M.; Giesler, B.; Dillmann, R.; "3D global and mobile sensor data fusion for mobile
platform navigation"Robotics and Automation, 2004. Proceedings. ICRA '04. 2004 IEEE International Conference
on , April 26-May, 2004.
13. D. Castro, U. Nunes and A. Ruano, "Obstacle Avoidance in Local Navigation", in IEEE Mediterranean
Conference on Control and Automation MED 2002, Lisbon, July 2002.
14. Ren C. Luo and Kuo L. Su,"A Multivalent Multisensor Base Real-Time Sensory Control System for Intelligent
Security Robot", IEEE International Conference on Robotics and Automation (ICRA2003),PP.2394-2399.
15. Ren C. Luo Chieh F. Tsai,Kuo L. Su and Shih Y Lin,"Design a Multi-agent Multisensor-Base Remot Diagnosis
Intelligent Security Robot Through Internet", Interbation conference on Automation 2003,PP.102.
 
Automation 2007 
 
system. 
The main controller of the intelligent security 
robot is PC (Personal Computer) with a Pentium(R) 4 
CPU 3.2GHz and 1GB RAM. The lower body contains 
driver system, batteries and two DC servomotors. 
Each system has some function for 
implementation.  Every function can be finished by 
dividable device. And it transmits the experimental 
results to the main controller. 
 
 
 
Fig. 1.  System architecture of Intelligent Security 
Robot  
 
 
 
Fig. 2.  Mechanism of Intelligent Security Robot 
 
 
III. HARDWARE ARCHITECTURE 
 
The intelligent security robot hardware is composed 
with a lot of parts according to their functions (as shown in 
Fig.1, Fig.2), and the control center of the security robot is 
Personal Computer. In the motion control system, it will 
send command to control two DC servomotors through 
driver device.  In the sensory system, we use ultrasonic 
microprocessor to measure distance of obstacle using 16 
pieces ultrasonic sensors.  The transmission interface 
between IPC (main controller) and microprocessor is 
UART.  And we also use the laser range finder to detect 
the obstacles (Albert Diosi et al, 2005) and rebuild the 
environment map around the place where the intelligent 
security robot. There are some devices that communicate 
with main controller of the intelligent security robot such 
as CCD camera and wireless LAN. 
 
 
IV. EDGE FOLLOWING 
 
Wall following is a generally used skill for the 
mobile robot to escape from a trap. It also can be applied 
to follow a hallway. There are several experiments that 
apply a LRF to detect the surrounding wall orientation and 
position for following the wall. For a reactive behavior, 
they use fuzzy rules to fuse the range data and then infer 
the steering and driving velocities. Although the reactive 
wall following is feasible, designing a large amount fuzzy 
rules and membership functions is difficult and the 
computational cost is a problem for a parallel behavior 
system. The simplest way for wall following applies one 
or two degree or laser range sensors to detect the distance 
between wall and the robot, then the robot try to keep a 
fixed interval from the wall. Although these approaches 
are very efficient but using fixed sensors usually leads the 
robot only can drive at a constant speed. 
 
 
 
Fig. 3.  The software architecture of wall following 
 
The software architecture of wall following is shown in 
the Fig. 3. This system has two subsystems: object 
detection and representation is used to organize and 
classify the ultrasonic sensor data and transform them into 
the wall following controller, the wall following controller 
uses four inputs to generate the suitable robot direction 
and velocity for the robot in current condition and these 
parameters will also feedback to the wall following 
controller. 
Our approach for following a right wall here uses laser 
range finder for left wall (as shown in Fig. 4). The robot 
keeps the distance about 30 centimeters between the center 
of robot and the wall.  
 
 
Automation 2007 
 
The coefficient 1E  is the slope of the line we 
modeled to the edge, and wV  is the distance from the 
robot to the obstacle. 
 The thresholds of the maximum and minimum 
error (distance, slope) are set to avoid oscillation. The 
motion system will keep the distance between the robot 
and the wall, the distance is denoted di for a 
range minmax dididi !! . And the robot also keeps 
the slope, sl  for a range minmax slslsl !! . 
 
VI. TANGENT BUG ALGORITHM 
 
The tangent bug algorithm (Howie Choset et al, 
2005) is an obstacle avoidance algorithm. The robot 
keeps moving to the goal until encountered an obstacle. 
When encounter an obstacle, the robot will follow the 
edge of the obstacle until the robot can link from 
himself to the goal without crossing the obstacle. Again, 
the robot keeps moving to the goal until encountered 
the other obstacle. The detail of this algorithm describe 
as follow: 
 
 
Tangent Bug Algorithm 
------------------------------------------------------------------- 
Input:  A robot with a range sensor 
Output:  A path to Goal or a conclusion no such path 
exists 
------------------------------------------------------------------- 
1: while True do 
2: repeat 
3: Continuously move toward the point n 
{T, Oi} which minimizes d(x, n) + d(n, 
Goal) 
4: until 
 the goal is encountered or 
 The direction that minimizes d(x, n) + d(n, 
Goal) begins to increase 
 d(x, Goal), i.e., the robot detects a ³local 
minimum´ of d(炽, Goal) 
5: Chose a boundary following direction which 
continues in the same direction as the 
  most recent motion-to-goal direction. 
6: repeat 
7:  Continuously update dreach, dfollowed , and 
{Oi}. 
8:  Continuously moves toward n   {Oi} that 
is in the chosen boundary direction. 
9: until 
 The goal is reached. 
 The robot completes a cycle around the 
obstacle in which case the goal cannot be 
achieved. 
 dreach < dfollowed 
10: end while 
 
 
 
Fig. 6.  Oi: The discontinuous point 
 
Oi is the discontinuous point (shows as Fig.6 ), T 
is the robot body represented as a circle and x is the 
center of the robot. 
 
 
Fig. 7.  The path generated by Tangent Bug with zero 
sensor range. 
 
 
 
Fig. 8.  The path generated by Tangent Bug with 
infinite sensor range. 
 
As Fig. 7, Fig. 8 shows, the robot will have 
different intelligent with different sensor range. In Fig.7, 
we can see the edge following behavior clearly. Then In 
Fig.8, the sensor range is very large, so he will make 
better choice earlier for the path planning. 
 
VII. EXPERIMENTAL RESULT 
 
We use man-made cubes and metal cabinets as 
static obstacles for testing the motion planning in our 
laboratory. The robot can start at arbitrary location and 
move without collision. The LRF device and its 2D 
 
Automation 2007 
 
 
REFERENCES 
 
Albert Diosi and Lindsay Kleeman, ³Laser Scan 
Matching in Polar Coordinates with Application 
to SLAM´ Intelligent Robots and Systems. 
(IROS). IEEE/RSJ International Conference on 
2-6 Aug. 2005 Page(s):3317 - 3322 Digital 
Object Identifier 10.1109/ IROS.2005. 1545181 
(2005). 
Bernard, J.E., Vanderploeg, M.J., and Shannan, J.E, 
³/LQHDU$QDO\VLVRIDVehicle with Four-Wheel 
SWHHULQJ´SAE paper No. 880643 (1988). 
Diosi, A.; Taylor, G.; Kleeman, L, ³Interactive SLAM 
using Laser and Advanced Sonar´ Robotics and 
Automation. ICRA. Proceedings of the IEEE 
International Conference on 18-22 April 2005 
Page(s):1103 ± 1108 (2005). 
H. Kobayashi, and M. Yanagida, "Moving object 
detection by an autonomous guard robot," 
Proceedings of the 4th IEEE International 
Workshop on Robot and Human 
Communication, TOKYO, pp.323-326(1995). 
Howie Choset, Kevin Lynch, Seth Hutchinson, George 
Kantor, Wolfram Burgard, Lydia Kavraki, and 
Sebastian Thrun, ´Principles of Robot Motion, 
Theorey, Algorithms, and 
Implementation´,(2005). 
J. Minguez and L. Montano, "Nearness Diagram (ND) 
Navigation: Collision Avoidance in 
Troublesome Scenarios," Proceedings of the 
IEEE Transaction on Robotics and Automation, 
(ICRA), Vol.20, 2004, pp.45-59(2004). 
Myung-Jin Jung, Hyun Myung, Sun-Gi Hong, 
DongRyeol Park, Hyoung-Ki Lee and 
SeokWon Bang, ³Structured Light 2D Range 
Finder for Simultaneous Localization and 
Map-building (SLAM) in Home 
Environments,´ Proceedings of the 
Micro-Nanomechatronics and Human Science, 
2004 and The Fourth Symposium 
Micro-Nanomechatronics for 
Information-Based Society, 2004. Proceedings 
of the  International Symposium on 31 Oct.-3 
Nov. 2004 Page(s):371 ± 376(2004) 
Ren C. Luo, Te Yi. Hsu, Po Kai. Wang, and Kuo L. Su, 
"Multisensors Based on Dynamic Navigation 
for an Intelligent Security Robot," The 8th 
International Conference on Automation 
Technology, May 2005, pp. 504-509(2005). 
Seul Jung, Eun Soo Jang, T. C. Hsia, ³Collision 
Avoidance of a Mobile Robot Using Intelligent 
Hybrid Force Control Technique,´ Robotics and 
Automation. ICRA. Proceedings of the  IEEE 
International Conference on 18-22 April 2005 
Page(s):4418 ± 4423 (2005). 
S.O. Lee, Y.J. Cho, H.B. Myung, B.J. You, and S.R. 
Oh, "A stable target-tracking control for 
unicycle mobile robots," Proceedings of the 
IEEE/RSJ International Conference on 
Intelligent Robots and Systems, (IROS), Vol.3, 
pp.1822-1827(2000). 
 
Automation 2007 
robot, and explains sensor arrangement status. Section 
III presents the detection method of fire detection and 
intruder detection for intelligent security robot. The 
experimental result of fire detection and intruder 
detection using the mobile robot are implemented in 
section IV. Section V presents brief concluding 
comments. 
II. SYSTEM STRUCTURE 
The security robot is constructed using aluminum 
frame. The contour of the robot is cylinder. The 
diameter is 50 cm, and height is 170 cm. The security 
robot contains upper body and lower body. Fig. 1 
shows the hardware configuration of the security robot. 
The main controller of the security robot is PC with a 
Pentium-IV CPU and 1G RAM. The hardware devices 
have touch screen, CCD, LRF (Laser Range Finder), 
robot arms, sensors and sensory circuits, driver system 
and some hardware devices. 
The structure of the security robot contains eight 
parts. There are motion planning, vision, GSM message 
and alert signal, detection environment, sensor, robot 
arms, and auto-recharge docking systems. Fig 1 shows 
the function of the intelligent security robot. Fig. 2 is 
the hierarchy structure of the security robot, and 
includes many subsystems in the security robot. 
We can see there are four main systems of security 
robot in the Fig. 2. Through these four systems security 
robot can execute some tasks or interact with humans. 
The top-left blocks are image system. When security 
robot executes patrolling task in a building, the CCD 
Camera installed on security robot will acquire the 
images about what security robot see. The top-right 
blocks are security system. There are intruder sensor 
and fire sensor installed in security robot. When these 
sensors sense any intruders or flames, security robot 
will sound alarm to warn people. And then security 
robot sends the warning message by GSM module to 
guard and takes the intruder’s picture at the same time. 
The other security can find and locate the fire by 
security robot. The security can use the fire 
extinguisher of security robot to extinguish the fire. The 
bottom-left blocks are multi-interaction system. 
Security robot can use robot arms, eyes and voice 
system to interact with people. The bottom-right blocks 
are obstacle avoidance system. Because we expect 
security robot has ability to execute task in buildings. It 
must avoid obstacles in building automatically. So 
security robot has a method to perform the task of 
obstacle detect. 
Fig. 1.  The contour and structure of intelligent 
security robot 
Fig. 2.  The hierarchy structure of the security robot. 
The hierarchy structure of hardware system for the 
security robot is shown in Fig. 3. The sensory system 
has four variety subsystems. There are intruder 
detection, power detection, fire detection subsystem 
and motor planning subsystems. 
The subsystem can acquire sensory signal and 
processes these signals using amplifier and calibrate 
circuits, and transmit sensory data to the main 
controller using interface devices. We use the security 
robot kernel system to communication the sensory 
devices and control the sensory devices to execute the 
security tasks. In the intruder subsystem and motor 
planning subsystem, we use CCD camera to track the 
intruder and detect the obstacle by laser range finder 
and ultrasonic sensors. Then we use microprocessor to 
drive sensors and get distance value from obstacle 
using the sensor and transmit the distance value to the 
main controller using series interface (RS232). And 
then the robot can use the motion controller to move 
smoothly and avoid the obstacles. In the power 
subsystem, we use the current sensors to detect electric 
power of the battery. When the battery level gets in low, 
the robot will go to the recharge station automatically. 
In the fire detection subsystem, we use the temperature 
CCD 
camera
Emotion 
display 
Touch 
Screen 
Ultra Sonic
PCPower 
System 
Arm 
(7DOF)
LRF 
Fire sensorBody 
sensor
Emotion display and 
Voice system
Robot Arms  
Gesture Interface
Obstacle Avoidance Multi-interaction 
System
Fire detection 
Intruder detection
Security System 
Tracking System
Vision System 
Image System 
Security Robot 
Motion Planning
Collision Avoidance
Automation 2007 
iii WWW ' 

, i=0, 1, 2, where 

iW  and 

iW
represent the weight value after and before each update. 
 We use adaptive multisensor fusion algorithm to 
detect the fire. If the security robot confirms the fire, he 
will locate the fire by the fire location algorithm.  
Fig. 5.  The detection and diagnostic rule of the 
intruder system 
As the image shown in Fig. 5, we employ the three 
fire sensors in front of security robot. The three 
conditions are only one sensor detected fire, two 
sensors detected fire and three sensors detected fire. 
The flow chart of algorithm we used in the process of 
fire detection is shown in Fig. 6. The S1 means the 
right fire sensor in front of security robot, S2 is the 
middle sensor, and S3 is the right sensor.               
  When security robot executes its task during 
patrolling, it will detect whether there are any fire or 
not. If security robot affirms fire, security robot will 
follow the algorithm in Fig. 6 to change its direction to 
locate fire. The left condition indicates there is only one 
sensor detecting obstacle. If sensor NO.1 has detected 
obstacle, security robot will turn to right. If sensor 
NO.2 has detected obstacle, security robot will 
extinguish fire. If sensor NO.3 has detected obstacle, 
security robot will turn to left. The middle part is the 
condition that two sensors detected obstacle. If sensor 
NO.2 and NO.3 have detected obstacle, security robot 
will turn to left. If sensor NO.1 and NO.2 have detected 
obstacle, security robot will turn to right. And if sensor 
NO.1 and NO.3 have detected obstacle, security robot 
will extinguish fire. The right part is the condition that 
three sensors have detected obstacle. It means the fire 
in front of security robot. Security robot locates the fire 
and extinguishes fire. 
Fig. 6.  The detection and diagnostic rule of the 
intruder system 
Intruder Detection Method 
 We use rule-based method to detection intruder 
and diagnosis sensor error. First, we use body sensor 
and ultrasonic sensors to detect intruder. The detection 
and diagnosis rule is shown in Fig. 7. The detection rule 
of the intruder system has three levels. The high level 
uses body sensor to detect intruder. If the body sensor 
detect intruder, the output state is “Yes”, otherwise, the 
output state is “No”. We use ultrasonic sensors to detect 
intruder in the second level. In the third level, we use 
CCD camera to track intruder. We can get the detect 
results. They are intruder, intruder and sensor fault, 
sensor fault and normal, and can diagnose which sensor 
will be error. 
Fig. 7.  The detection and diagnostic rule of the 
intruder system 
Body sensor 
Ultrasonic 
sensors 
Ultrasonic 
sensors
Visual 
tracking 
system 
Visual 
tracking 
system 
Visual 
tracking 
system 
Visual 
tracking 
system 
Intruder
(Front)
Intruder
(Near) 
Intruder
(Beside)
normal Obstacle normal
Obstacle Fail 
Yes N
Yes
Yes Yes
Yes Yes Yes
NN
N N N N
Patrolling environment 
Detection
of fire?
One sensor 
detects fire
Two sensor  
detects fire 
Three sensor 
detects fire
S3 S2 S1 S2&S3 S1&S3 S1&S2
Turn  
right
Turn 
left
Locate and extinguish fire 
N
Yes
Automation 2007 
(c) Move to the intruder   (d) CCD camera of robot 
capture intruder’s picture 
(e) Intruder go away    (f) CCD camera of robot 
capture intruder’s picture 
Fig. 11.  The scenario of intruder detection. 
(a) Intruder condition     (b) Fire condition 
Fig. 12.  Mobile phone display massages 
    If the fire event is true, the security robot must 
alarm and sent the massage to the user using GSM 
module. Our GSM module can send the Chinese 
messages or English messages. The user can receive the 
massage using cell phone. The experimental result is 
shown in Fig. 12(a). In the intruder detection, the 
security robot must alarm and transmits the detection 
results to the cell phone (shown in Fig. 12(b)). 
V.  CONCLUSIONS 
 We have presented a method of intelligent 
modular which is applied to fire detection system. Fire 
detection system uses adaptive fusion method to fine 
out the adequate condition. However we use the fire 
location algorithm to locate the fire and extinguish the 
fire. We get the efficiency to be nice for the fire 
detection system. In the intruder detection, we use 
rule-based method to detection intruder and diagnosis 
sensor error. Finally, the fire detection and the intruder 
are very successful using our security robot. 
REFERENCES 
A. Neubauer, “Genetic algorithms in automatic fire 
detection technology,” Second International 
Conference On Genetic Algorithms in 
Engineering Systems: Innovations and 
Applications,  pp. 180-185(1997) 
C. W. Wang and A. T. P. So, “Building Automation In 
The Century,” in Proceedings of the 4th 
International Conference on Advance on 
Advances in Power System Control, Operation 
Management, APCOM-97, pp.819-824(1997) 
G. Healey, D. Slater, T. Lin, B. Drda and A.D. 
Goedeke, “A system for real-time fire 
detection,” in Proceedings of IEEE Computer 
Society Conference on Computer Vision and 
Pattern Recognition, ,pp. 605-606(1993) 
H. Ruser and V. Magori, “Fire detection with a 
combined ultrasonic-microwave Doppler 
sensor,” in Proceedings of IEEE Ultrasonics 
Symposium, Vol.1 , pp. 489-492(1998) 
Ren C. Luo, Kuo L. Su and Kuo Ho Tsai, “Fire 
detection and Isolation for Intelligent Building 
System Using Adaptive Sensory Fusion 
Method,” Proceedings of The IEEE 
International Conference on Robotics and 
Automation, PP.1777-1781(2002) 
Ren C. Luo, K. L. Su and K. H. Tsai, “Intelligent 
Security Robot Fire Detection System Using 
Adaptive Sensory Fusion Method,” The IEEE 
International Conference on Industrial 
Electronics Society , PP.2663-2668(2002).  
Ren C. Luo, Shin Yao Lin and K. L. Su, “A Multiagent 
Multisensor Based Security System for 
Intelligent Building,” IEEE/SICE/RSJ, 
International Conference on Multisensor Fusion 
and Intelligent for Intelligent System,
PP.311-316(MFI03) 
T. Derek and J. Clements-Croome, "What do we mean 
by intelligent buildings ?,” Journal of 
Automation in Construction, Vol. 6, PP. 
395-400(1997) 
Wang Xihuai, Xiao Jianmei and Bao Minzhong, “A 
ship fire alarm system based on fuzzy neural 
network,” in Proceedings of the 3rd World 
Congress on Intelligent Control and 
Automation, Vol. 3, pp. 1734 -1736(2000) 
Automation 2007 
V proposed brief concluding remarks. 
II. SYSTEM ARCHITECTURE  
A. Hardware Architecture 
The mobile robot for security task we developed is 
named as “Security Warrior” and shown in Fig. 2. The 
robot is specially designed and to be used for the 
purpose of security patrol and developed by our 
laboratory. The robot is kind of circular mobile robot 
and equips with main controller (employed by 
Pentium-4 with 512M RAM), sensory circuits, color 
camera, laser range finder, power system, and two 7 
D.O.F. arms. The structure of the hardware is shown in 
Fig. 1. 
Fig. 1 Hardware structure of mobile robot 
B. Software Architecture 
The software architecture is shown as Fig. 2. Task 
behaviors are executed by the kernel which is the 
central part of control and intelligent. The kernel can 
communicate with all the subsystems through the IPC 
(Inter Process Communication). 
Fig. 2 Software architecture of mobile robot 
C. Power Subsystem Architecture 
Power subsystem acts an important role in robot 
system. A good power subsystem can make robot 
extend working time and avoid robot shutdown 
suddenly. If the robot is capable of perceiving the state 
of the power, the robot can make correct decisions. The 
power management block is composed of AC-DC 
converter device, DC-DC power supply, batteries and 
relays. The relays are controlled by microcontroller in 
order to switch on or off some devices. Fig.4 shows the 
architecture of power management block. The 
microcontroller is AVR MEGA128 (ATMEL) single 
chip. There are ten bits and eight channel analog to 
digital converter for sensing the voltage and current. 
PC & 
Other Subsystem
Atmel
AVR
UART(RS232)
Voltage
Voltage
Current Sensor
Current Sensor
ADC
ADC ADC
ADC
Power
Control Signal
Power Subsystem
 Fig.3 Power subsystem architecture of mobile robot 
AVR
110Vac-24Vdc
Converter
12Vdc
Batteries series connection
110Vac Control signal
Switch
Power Block
PC & Other devices
Fig.4 The architecture of power block 
III. PROPOSED METHOD  
A. Battery SOC model 
For ideal systems, it is supposed that the power 
supply is ideal battery. In this assumption, the function 
of the current and voltage is always static until batteries 
are completely exhausted. Fig.5 shows the progress of 
Automation 2007 
Consider the first order Taylor series expansion for 
 xF  about the old value kx .
      kTkkkk xgxFxxFxF k '#' 1   (8) 
The kg is the gradient evaluated at the old guess kx .
 
kxxk xFg  {       (9) 
 1kxF  must be less than  kxF . The second term on 
the right-hand side of (8) must be negative. 
0 ' k
T
kkk
T pgxg
k
D      (10) 
We will select smaller value kD . So that, 
0k
T
k pg        (11) 
Any vector kp satisfies this equation is called descent 
direction. The performance function has to go down if 
we take a small enough step in this direction. This is 
steepest descent algorithm, but there are some 
disadvantages in this algorithm. In beginning stages, it 
can approximate fast to local minimum value, but it 
will tend to converge most slowly and move with 
zig-zag when it is near to local minimum value.
Second, we will introduce the other algorithm, 
Newton’s method. It is also a performance learning 
method. The derivation of the steepest descent 
algorithm was based on the first-order Taylor series 
expansion (8). Newton’s method is based on the second 
order Taylor series: 
      kkTkkTkkkk xAxxgxFxxFxF k '''#'  2
1
1
        (12) 
The principle of Newton’s method is to locate the 
stationary point of the quadratic approximation 
to  xF .And we take the gradient of this quadratic 
function with respect to kx' , and set it equal to zero. 
We can find the equation as shown as (13). 
0 ' kkk xAg       (13) 
Solving kx' , we can get equation (14). 
kkk gAx
1 '       (14) 
Then Newton’s method will be defined as follow. 
kkkk gAxx
1
1

       (15) 
This method will always search the minimum of a 
quadratic function in one step. Because Newton’s 
method is designed to approximate a quadratic 
function to  xF  and locate the stationary point of the 
quadratic approximation. There are also some 
disadvantages in Newton’s method. If the performance 
function  xF  is not quadratic, Newton’s method 
generally can not converge in one step. Actually, we 
can make sure that the function will converge at all. 
There are advantages and disadvantages in steepest 
descent and Newton’s methods. They are 
complementary each other. The Levenberg-Marquardt 
algorithm combines with steepest descent and 
Newton’s method. The steepest descent solves the 
problem of minimizing performance index, but the 
Levenberg-Marquardt solves the problem of least 
square of non-linear function. Basically, the 
performance of Levenberg-Marquardt method is like to 
steepest descent when it is farther to local minimum. 
On other hand, it is nearer to local to local minimum, 
the performance of Levenberg-Marquardt is like to 
Newton’s method. Therefore, Levenberg-Marquardt is 
a Hybrid method and switch between steepest descent 
and Newton’s method. 
In Fig.7, the flow chart shows the BP modeling 
procedure. This procedure includes three main tasks, 
the initialization stage, the forward processing, and 
backward processing. The initialization stage sets up 
the neural network architecture, determines the 
activation function (Log-sigmoid function, as shown as 
(3)) for the hidden layers, selects the training algorithm 
(Levenberg-Marquardt algorithm is implemented) and 
parameters, and initializes the weight matrices and bias 
with random values. 
Fig.7  Flowchart for the BP modeling procedure 
The forward processing is calculated is to calculate 
Automation 2007 
Fig.10 the discharging current predicted with LMBP 
In Fig.10, the blue line is the discharging current, and 
green line is the approximated line by LMBP. As Fig.10 
shows, we use the first ten minutes dataset to train 
LMBP and predict the function. The error between 
values of the measured and predicted is 3.63%. The 
battery SOC will be produced by the discharging 
current function predicted with LMBP and equation (4). 
Fig.11  SOC we estimated 
 When the SOC function is modeled, we can get 
the relative between the battery power remaining and 
discharging time. We estimate power consumption of 
the basic tasks robot executes. When robot executes 
complex tasks, he will know the consumption and how 
long the battery will be getting low. The research 
applying to robot will let robot be more intelligent on 
power management. 
V. CONCLUSION 
The experimental results show our supposition is 
successful. Neural network techniques are useful in 
estimating battery SOC function. Then we propose a 
three-layer artificial neural network – 
Levenberg-Marquardt back-propagation to approximate 
the battery discharge function and achieve power 
prediction.  
In our scenario, the robot extends his working time 
with docking and using power prediction. In some 
paper, the power management and subsystem 
scheduling is presented. We will not only develop static 
power prediction but also dynamic power prediction 
with kalman filter and real-time scheduling. 
REFERENCES 
Benini, L., Bruni, D., Mach, A., Macii, E., Poncino, M. 
“Discharge Current Steering for Battery 
Lifetime Optimization,” IEEE Transactions on 
Computers, Vol 52,  pp.985 – 995, Aug. 2003 
Chenghui, Cai., Dong, Du., Zhiyu, Liu., Jingtian Ge, 
“State-of-charge (soc) estimation of high power 
NI-MH rechargeable,” in Proc. 9th International 
Conference on ICONIP '02. pp. 824- 828 vol.2,  
18-22 Nov. 2002 
Eui-Young, Chung., Benini, L. and De, Micheli., G, 
“Dynamic power management using adaptive 
learning tree,” International Conference on
IEEE/ACM Computer-Aided Design, Digest of 
Technical Papers. pp. 274 – 279, 7-11 Nov. 1999. 
Luo, R.C., Liao, C.T., Su K.L. and Lin, K.C. ,“Automatic 
docking and recharging system for autonomous 
security robot,” IEEE/RSJ International 
Conference on IROS 2005 pp.2953 – 2958, 2-6 
Aug. 2005. 
MacKay, David J. C, “Bayesian Non-Linear Modeling For 
The Prediction Competition,” in Maximum Entroy 
and Bayesian Methods,1996 , pp. 221-234 
Martin T. Hagan, Howard B. Demuth and Mark Beale, 
Neural Network Design, International Thomson 
Publishing Inc. 1996. 
S. Piller, Marion, Perrin., A. Jossen. Methods for 
stateof-charge determination and their 
application. Journal of Power Sources 96(2001) 
113-120. 
Valdez, M.A.C., Valera, J.A.O., Jojutla, Ma. and Arteaga, 
O.P, “Estimating Soc in Lead-Acid Batteries 
Using Neural Networks in a 
Microcontroller-Based Charge-Controller,” 
International Joint Conference on IJCNN '06. pp. 
2713 – 2719,16-21 July 2006. 
Yongguo, Mei., Yung-Hsiang, Lu., Y Charlie, Hu. and 
C.S.G. Lee, “A case study of mobile robot's energy 
consumption and conservation techniques,” in 
Proc. 12th International Conference on ICAR '05. 
pp.492 – 497 July 18-20, 2005.
Automation 2007 
Mobile Edition without rewriting source code. The size 
of PDA screen is limited; user interface designed by 
Window Programming fit in PDA screen more suitably 
comparing to Web interface. The major feature of 
socket programming is that we don’t need to consider 
the kinds of programming language between PDA and 
mobile Robot.  
We have designed the architecture for the 
autonomous mobile robot in our laboratory to provide 
multilevel remote control modules for the PDA users. 
In direct control mode, via PDA one can send primitive 
commands to move the robot, to grip and lift arm, to 
pan and tilt the camera and grab images, to speak, and 
get the information what robot senses. In supervisory 
control mode, for example, users can also give the 
robot a goal position with respective to the robot 
coordinate system, then the robot will head and follow 
the goal by goal following and obstacle avoidance 
capabilities.  
In this paper, we give an overview of the remote 
control architecture in Section 2. Remote control modes 
are described in Section 3. Hardware base and software 
design of the architecture are described in detail in 
Section 4 and 5 respectively. Section 6 describes the 
services provided by the robot and the future 
applications based on this architecture.  
REMOTE CONTROL ARCHITECTURE 
VIA HAND HELD DEVICES  
To use the system, the user’s PDA must have the 
ability of connecting to the Internet. Then he can 
remotely control robot via user interface through 
Internet. If the user does not have an account, he can 
only view the image captured by the robot or the active 
movement of the robot operated by someone. Besides, 
if the user has an account, he can operate all of the 
mechanisms of the mobile robot by interacting with the 
control interface on the PDA. For example, he can 
select “Capture an Image” command and submit it. The 
vision system will capture an image and send it to the 
PDA. Once the server receives a new image, it will 
send the image to the user’s PDA, and the image will 
be shown in a window of the user interface. In 
supervisory mode, another example, a registered user 
sends a goal position where he wants to visit by click a 
point on the user interface. The robot receives the 
command, then move its body to head and follow the 
goal by the intelligence of goal following and obstacle 
avoidance. Simultaneously, the users can see the 
sequential live image; what the robot has seen shown 
on the PDA. 
The relation between hardware of the proposed 
system is shown in Figure 1. The communication 
between user’s PDA and mobile robot is through a 
wireless network interface in a local area network. 
User’s PDA in the world communicates with the 
security robot via Internet.
To improve transmission rate, we constructed 
several MIMO AP as bridge of WLAN. Robot will 
connect to the nearest AP. The range of RF signal reach 
to 70m. Even though, there are shelters around or robot 
patrol to rooms. Robot receive efficient RF signal. 
Fig. 1. The hardware configuration of the remote 
control architecture. 
REMOTE CONTROL MODES  
We consider two major steps of designing the 
system. At first, we consider of performing multi-level 
operations for the user on various remote applications. 
Next, limit the access of different control module to 
keep the necessary safety in mind. The remote control 
may be divided into four major control modes, namely, 
direct control mode, behavior programming mode, 
supervisory mode, and learning mode.  
Direct Control Mode 
We can control the mobile robot by sending the 
primitive command and necessary parameters. The 
robot will execute the command without any 
intelligence; that is, what the user had sent leads to the 
action of the robot directly. Only the user who 
understands the robot’s characteristics operates the 
robot as puppet in this mode. Direct control via the 
Internet with inherent high latency and low bandwidth 
is not suitable for robotic systems. However, when the 
robot fails to function and requires remote teaching, 
direct control is important. The following approaches 
have been proposed in the literature to address the 
Automation 2007 
1067 cm. The tactile sensors which are interleaved to 
provide 360 degrees coverage with 18 degrees 
resolution can detect contact with objects.  
The second is the user’s PDA. Linking to the 
Internet is the only requirement on user’s PDA.  
Fig. 2.  The contour and structure of autonomous   
       security robot 
Fig. 3.  The hierarchy structure of the security 
       robot 
SOFTWARE DESIGN 
 Based on the hardware configuration, software 
design for the system has been divided into two parts. 
The first is the communication between hand-held 
devices and Robot, and second is user interface.
Communication between Hand-held Devices and 
Robot
Because of the similarity of the operation systems 
of the PDA and robot daemon is similar (Microsoft 
Mobile Edition 2003 on PDA and Microsoft Windows 
XP on the robot), the communication between them can 
be implemented by applying socket programming. Two 
transmission ports are used to create two virtual 
channels in our system. One is for the transmission of 
image file and the other is for commands and other data. 
Figure 4 shows the communication between user’s 
PDA and robot daemon. The design leads the mobile 
robot to continuously execute new coming commands 
while transmitting a captured image with large amount 
of data. 
Fig. 4.  Communication between user’s PDA and 
       robot daemon 
User Interface 
The design of user interface is dependent on 
different control mode. At present, direct control mode 
is realized. The interface shown in Figure 5 is designed 
for the operator who has knowledge to the 
characteristics of the mobile robot.  
Windows in this user interface are included: 
1. The Motion Control Command window - a remote 
operator can select a command from a pull list in 
the window, press the SUBMIT button, and then 
the command and tasks schedule are sent to the 
robot daemon. General control commands are 
provided here.  
Fig. 5.  Remote control user Interface on PDA 
Mobile Robot Motion Planning and Human Tracking 
Using Range and Intensity Data Fusion 
Ren C. Luo1 Fellow, IEEE, An C. Tsai1, Chung T. Liao12
1Department of Electrical Engineering National Chung Cheng University 
168, San-Hsing, Min-Hsiung, Chia-Yi 621, Taiwan, R.O.C
2Department of Electronic Engineering 
Wu-Feng Institute of Technology 
Chiayi, Taiwan 
{luo, actsai, ctliao}@ia.ee.ccu.edu.tw
Abstract - Monitoring and tracking human from a 
mobile robot is an essential technology in robot 
applications. This paper presents a data fusion modeling 
methodology to detect and track human. With the range 
and data, our robot will detect and track the moving human 
using the method of human future extraction. Then the 
position of the human will be transformed to intensity data. 
In the experiment, the LRF and CCD camera are mounted 
collateral in the mobile robot. Each color image is 
simultaneously acquired with a range profundity scanning 
from a Laser Range Finder (LRF). The range profundity 
image shows the distance measurements from the LRF to 
boundaries of the environment. We propose a method to 
fuse image and range data into a hybrid image. The fused 
image, which contains color intensity and range profundity 
data for every pixel, is easily to extract the foreground 
objects such as human. We have successfully implemented 
human tracking by fusing range and intensity data.  
Index Terms - Laser range Finder, Image Data 
Fusion, human tracking, intelligent mobile robot.
I. INTRODUCTION
 Laser Range Finder (LRF) has been employed in 
intelligent mobile robot (IMR) in recent years. The 
primary motivation is their high resolution with direction 
with respect to acoustic sensors. For an IMR, the 
environmental interaction is completely dependent on the 
reliable extraction of information from its immediate 
surroundings. For example, the IMR performs various 
tasks which are manipulated in 3-D environment such as 
obstacle avoidance, object recognition, object tacking 
and SLAM etc.. Therefore the range information for an 
IMR running in 3-D space is important. 
Meanwhile, the applications of LRF in field of 
detecting and tracking moving object has developed for a 
long time [1, 2, 3, 4]. In the concept of tracking people 
using mobile robot with sensors, there are many merits of 
computing and algorithmic implement, robustness to 
environmental surveillance, tracking people, and so on. 
Zhao et al. [1] is proposed the method to track people 
with walking model in some indoor area. The method is 
proposed for using static LRF and vision. Nevertheless, 
many researchers are focused on tracking moving people 
with one or multiple static LRF. It can not be applied to 
robotic applications when robot is moving. Jae Hoon Lee 
et al. [5] presented another model to tracking people 
when the robot moves. However, the method Jae Hoon 
Lee proposed does not achieve range and intensity data 
fusing. 
Some methods to acquire range data to form the 
profundity image have been classified in the two types. 
In the field of computer and robot vision [6, 7] the 
sensing techniques to get the range profundity 
information from servoal 2D digital photographs or video 
images (such as in stereo vision or photogrammetry) [8, 
9, 10] are classified to passive type. The other one is 
composed by active techniques based on range sensors 
measuring directly the distance between the sensor and 
points in the real world [11, 12, 13] and some range 
sensors are commercially available. 
In this paper, the core of our approach is a method to 
extract the human features and track the human. Then the 
position of the human is transformed from range plane to 
the image coordinate system and fuse two kinds of data 
in the same coordinate system. This fused image with 
distance information can be used in high level decision 
for 3-D dynamic object tracking, recognition and SLAM 
etc. The paper is organized as follow. In the next section, 
we will introduce 3D scanner system and build the 
geometry model. In section III, we present the method of 
human features extraction and tracking in range data and 
describe the proposed fusing algorithm of color and 
range image. Finally, we present experimental results. 
II. SYSTEM CONFIGURATION
A . Robot System Configuration 
The robot we developed is named as “Security 
Warrior” and shown in Fig. 1. The robot is specially 
designed by our laboratory and to be used for the purpose 
of home-care service. The hardware includes main 
controller (employed by Pentium-IV with 512M RAM), 
sensory devices including color camera, laser range 
finder, ultrasonic range finder, wireless LAN and power 
recharging.
Fig.1 illustrates the ranger finder which consists of a 
scan-in-line range finder (SICK) and the extra servo-
controlled device to perform the tilt of the range finder. 
(the same as ICS). The A is 43u matrix mapping three-
dimensional world points to two-dimensional image 
points and is denotes as the parameters of camera 
calibration [14][15]. It can be obtained by using Pseudo-
Inverse Method [16]. 
III. PROPOSED METHOD
A.  System Diagram 
A lot of methods are proposed to extract human 
features from image data. In image plane, human and 
background are projected into the same plane. It is 
difficult to separate human from background. Therefore, 
we use the active data acquired from LRF to divide the 
human from background. 
Fig.4 The diagram of the proposed method 
B.  Human Feature Extraction and Tracking 
We proposed a method to extract the features from 
range data to denote the human. In Fig.5, the red points 
are the edge points of hands and body. ),( hbD  is the 
Euclidean distance between the center point of hand and 
body. bW , hW  is the width of body and hand respectively. 
Hand
LRF
Body
Wb
Wh
Wh
D(b,h)
Fig.5 Top view of a human 
In range data, the features are discontinuous with 
respect to background. Fig.6 shows the image and the 
range data with human. There are three types of human 
models when a human is detected. The first model 
denoted as 1Z  consists only the features of the human 
body. The second model denoted as 2Z  consists the 
features of two hands features and body. The last model 
consists of the features of the features of one hand and 
body. It is denoted as 3Z .
(a)                                     (b) 
Fig.6 (a) Image data, (b) Range data 
We estimate the gradient of range data, since depth of 
human future is discontinuous with respect to 
background.
»
»
»
¼
º
«
«
«
¬
ª
w
w
w
w
 »
¼
º
«
¬
ª
 
r
R
R
G
G
rR r TT
T
),(                                         (4) 
Fig.7 The gradient of range data 
We firstly filter out the smaller magnitude of the gradient 
of range data. However remained features still are not 
human feature. Some depth data of background are not 
consecutive.
In range data, the human features usually concave 
features with respect to background. According to the 
object contour characteristics, we use model matching to 
search the concave characteristics in the remained 
features. We normalize the remained of differential range 
data to 1 and -1. When the gradient is larger than 0, it is 
normalize to 1. Others is smaller than 0, it is set to -1. 
Then we can use the concave template to searching the 
contour characteristics and mark them. The concave 
model is shown as (5). 
> @1111  concaveC                                         (5) 
Using the concave model, a lot of human candidates 
will be found. Fig.8 is the results of concave matching 
and range data is plotted in Euclidean coordinate. In 
Fig.8, the concave features are denoted with red points. 
In three human models, the concave futures will be found 
with concave model as shown as below. 
}{CZ 11  , }C,C,{CZ 3212  , }C,{CZ 212            (6) 
LRF
scanning
LRF at 
horizontal 
scan line
Feature 
extraction
Human
Model fitting
LRF with all 
scan lines
Camera Fusing Classification
Database
Human model
The tilt angle *E  represents the angle from x-z plane to 
n*  and the pan angle *D  from x-y plane to n* . The zxm 
*
and yxm 
*  are the normal vectors of x-z plane and x-y
plane respectively. 
yx
yx
mn
mn

 **
**
*cosD                                                   (9) 
zx
zx
mn
mn

 **
**
*cosE                                                   (10) 
Where yxm 
*  is defined (1, 0, 1) and zxm 
*  is defined (1, 
0, 1). The unit of the angles is defined as degree. 
Second, for each pixel in the image, we can find its 
corresponding distance which is from the projected point 
to the original of ICS. We use inverse perspective 
projection to estimate the unit directional vector of the 
projected light ray. The pan and tilt angles of the line are 
used to look the distance up in the virtual range 
image *R .
Third, we suppose a point p in the real world 
coordinate system (as shown as Fig.10), and estimate the 
tilt IE  and pan angles ID  of p in ICS and the tilt RE
and pan angles RD  of p in RCS. In order to fuse two 
kinds of information, we have to combine with ICS and 
RCS. Fig. 10 illustrates the fact that the RCS* is 
translated to ICS and two kind angles of p are also both 
translated to ICS. 
Due to our purpose that is image and range data 
fusion, we must map the two angles of p in RCS* into 
two angles in ICS to search the color information of 
image which match the range data. 
The algorithms are summarized as below. 
Algorithm I: generating the virtual range image 
 Given T  and R, the algorithm generates the virtual 
range image *R  with respective to *RCS . Vector T
represents the translation vector from RCS to ICS. R is 
the range image with respective to RCS. 
Step 1: For each pixel RiP , ( ED , ) in R, calculate its 
corresponding 3D point RCSiP ,
( RCSiRCSiRCSi zyx ,,, ,, ) in RCS. 
Step 2: Translate RCSiP ,  into ICSiP , .
Step 3:  Project ICSiP , into *,RiP ( *,
* ED ) in the virtual 
range plane.  
Step 4: Vote the distance of *,RiP into the distances of its 
four adjacent pixels. 
 To follow the coordinate transformation algorithm, 
we designed the algorithm to perform the data fusion of 
the color and range data. 
Algorithm II: fusing range data with color data 
Given the color image I and the virtual range 
image *R , the fused image which each pixel represents 
(r, g, b, d) is obtained by the algorithm. 
Step 1: For each pixel  VU, in image plane I, use 
inverse perspective projection to find the unit 
direction vector of the line corresponding to the 
projecting ray. The camera model matrix A is 
used to calculate unit direction vector. 
Step 2: Find the projected pixel ( ID , IE ) of the unit 
direction vector in *R .
Step 3: Look up the distance of the pixel in *R  by using 
coordinate interpolation.  
IV. EXPERIMENTAL RESULT
 The approach we proposed has been implemented 
and tested on our robot. The robot is equipped with a 
SICK laser range finder and a color camera. Extra seveo-
controlled device is designed to perform the tilt of the 
laser range finder. The tilt and pan angles of the LRF are 
from -40 to 40 and from 0 to 180 in degree, respectively. 
Therefore, the dimension of range image is 81181u . The 
acquired color image is 240320 u . For range image, the 
intensity of the pixels represents the projecting distance. 
The translation vector T
*
 is fixed and can be measured in 
advance. They are 27 cm in z-axis and 48 cm in y-axis.
There is no displacement in x-axis because we align the 
x-axis of ICS parallel to the x-axis of RCS. 
A scenario of human robot interaction such as 
tracking is used to perform the fusing algorithm. Fig. 
11(a) shows the color image acquired by the camera. The 
fused image is shown in Fig. 11(b). Light intensity of 
pixel represents short distance from the robot. The 
experiment shows that our algorithm successfully fuses 
color image data with range data into a hybrid image. 
Thus, the human can be easily extracted by using the 
different distance of the pixels in the fused image. With 
using the method of the human feature extraction, our 
robot can detect the human and tracking the human. 
When the human move into the field of robot’s view, it 
will catch the image. The moving trajectory of the human 
will be produced by our method. Fig.12 shows the 
trajectory recorded by robot in 32 frames. 
.
Fig. 11. (a) the acquired color image. (b) the 
fused image. 
Face Detection and Tracking for Service Robots 
Ren C. Luo1 Fellow, IEEE, An C. Tsai1, Chung T. Liao12, Tung Y. Lin1
1Department of Electrical Engineering National Chung Cheng University 
168 University Rd., Min-Hsiung, Chia-Yi, 621, Taiwan, R.O.C
2Department of Electronic Engineering Wu-Feng Institute of Technology 
Chiayi, Taiwan 
{luo, actsai, ctliao, tylin}@ia.ee.ccu.edu.tw
Abstract – The objective of this paper is to design and 
implement face detection and tracking problem for the 
interaction of human and service robots. In the real-time 
application such as robot system, the initialization and lost 
track problems often limit the application. The method which 
combines with face detection algorithm based on AdaBoost and 
the object tracking method using Kalman filter is proposed in 
this paper. To carry out the initialization problem, the Global 
AdaBoost Face Detection (GAFD) algorithm is applied. It is 
powerful to detect multiple faces in the whole image. If there is 
a new face in image, a new face tracker and the new Local 
AdaBoost Face Detection (LAFD) are both generated. The 
tracker will predict the possible area of the face appearing. 
According to the prediction of the tracker, partial image which 
is called region of interest (ROI) is obtained for LAFD. In the 
experimental result, our approach has been successfully 
implemented and tested on service robot developed in our lab.  
Index Terms - Face detection, Face tracking, AdaBoost, 
Service robot, Human-robot interaction 
I. INTRODUCTION
The autonomous robot becomes a growing interest and a 
popular subject and has been implemented widely in many 
areas. Traditionally, the robots have used in helping the 
workers in the fields of industrials and science such as 
semiconductor factories. Nevertheless, in the near years the 
robots are expected to provide all kinds of service in human 
daily lives. In the fundamentals, the developed service robot 
has the following functions to perform such the specific and 
security service: autonomous navigation, supervises through 
internet, a remotely operated camera vision system, human-
robot interaction (HRI) and so on.  
These service robots for human lives will achieve 
various tasks rather than one specific task, and interact with 
people rather than the researchers. According to above, 
many experts have exerted to develop a communication 
between human beings and robots in order to realize an 
affective and collaborative relationship. The communication 
does not rely on traditional devices such as keyboards and 
mouses. For realizing an effective HRI system, it is very 
important that the robot is capable of searching out the 
human and keeping itself in front of the human. Therefore 
much service can be possible offered to the human. As 
noted above, the human face detection and human tracking 
are major issues in HRI. A definition of face detection is to 
determine whether or not there are any faces in the image 
[1]. If any faces are present, the image location and the size 
of detected face are performed. Tracking is mostly 
determined as finding the location of a target face given its 
position of previous state and some prior information two 
successive image frames [2].  
In the recently literatures, many experts research in the 
autonomous mobile robot. Some researches address in 
developing intelligent system of mobile robot [3-5], and 
face tracking of mobile robot. The tracking algorithm 
applied Kalman filter is presented by Bernhard Fröba et al 
[2]. André Treptow focuses on using thermal images and a 
fast contour model. They search a face region of an image 
and apply a face tracker in combination with face 
recognition and identify person [6]. Paul Viola et al present 
the robust real-time face detection with the integral image 
method and AdaBoost training algorithm [7]. Kwang Ho An 
et al. pay attention to use a small number of critical 
rectangle features selected and trained by Adaboost learning 
algorithm, and they detect the initial position, size and view 
of a face correctly [8]. The Xangdong Xie et al present a 
real-time tracking algorithm on eye feature tracking [9]. 
In this paper, we modify and improve the AdaBoost face 
algorithm for rapidly tracking face in the sequence image 
frames and propose a scheme that is well-done for the 
problems of initialization and lost tracking. The paper is 
organized as follows: Section II describes the system 
architecture of the service robot. Section III will present our 
approach and consist of the system diagram of the method 
we proposed, the AdaBoost face detection algorithm and the 
face tracking thesis. And the experimental results are shown 
in Section IV. Section V proposed brief concluding remarks. 
II. SYSTEM CONFIGURATION
A . Robot Hardware Architecture 
Fig. 1 Hardware structure of service robot 
The mobile robot we developed for service task is 
shown in Fig. 2. The robot is specially designed and to be 
used for home care service and developed by our laboratory. 
The robot is kind of circular mobile robot and equips with 
Fig.4   Timing of the detector and trackers executing. The 
blue arrows represent the executing timing of the detectors 
or trackers in the frame domain. 
B.  AdaBoost Algorithm 
There are many researches and reports about face 
detection in recent years [10-14]. We use face detection to 
complete human-robot interaction and track faces, so the 
performance of method to detect face can’t be too low. In 
our proposed methods, we apply an algorithm of a visual 
face detection framework which is capable of processing 
image extremely rapidly. The algorithm is a method for 
building a classifier by choosing a small number of major 
features with AdaBoost (Adaptive Boosting). The simple 
rectangle features used are Haar basis function proposed by 
Papageorgiou et al [15]. The Haar basis is a set basis 
functions and derived from the Haar wavelets [16]. 
Rectangle features can be performed very rapidly with an 
intermediate representation for the image which is called 
integral image by Papageorgiou et al. With the integral 
image, any rectangular sum of the pixel in the rectangular 
range can be computed. For learning classification functions, 
a feature set and a training set of positive and negative 
images are given. The assumption is that a very small 
number of these rectangular features can be combined to 
form an effective classifier. After some surveys and 
evaluations, AdaBoost algorithm is a suitable algorithm for 
our request. Although it spends a lot of time on training 
classifiers, the detection speed and accuracy of AdaBoost 
algorithm have nice performance. Therefore the AdaBoost 
algorithm is used both to choose the features and train the 
classifier. The AdaBoost algorithm is shown as follow. 
The property of AdaBoost is that it establishes classifier 
one by one, and each classifier has its order. At first the first 
classifier is established, and then the first classifier will be 
accessed according to the fault of second classifier. After 
the second classifier is established, the second classifier will 
be accessed according to the third classifier. It will repeat 
this action until a special condition is appeared. Finally the 
whole committee machine will give the different weight to 
each expert according to its accuracy. And then we will get 
the final results of classifiers.  
Fig.5   Algorithm of face detection 
The procedure of AdaBoost algorithm is described below 
as Fig. 5. 
1. Given training samples: 
( ){ }M
jjj
dp
1
,
=
                                                       (3) 
Each training datum jp has its corresponding desired  
jd , and m means iteration. Each iteration establishes a 
model mF  to learn. Suppose that there are J iterations. 
2. Establish initial distribution. Because there are M
training data, D1(j)=1/M. The establishment of 
distribution is to give each training datum with different 
probability and train them with neural network. The 
bigger the distribution of datum is, the higher its chosen 
probability will be. And it can be chosen repeating, 
each datum has the same initial probability with 1/M.
3. Choose the data according to the distribution. Provide 
for mF  to train and establish the mapping between jp
and jd .
4. According to the mF  established before to compute 
error. The definition of error is
( )( )¦ ≠= jjm dpFj jmm D:ε                                          (4) 
When ( )jm pF  and jd  is unequal, for example, the 
result of ( )jm pF  is class 1, but jd  is class 2, add up 
distribution corresponding jp . When the inputted jp
k k+T k+nT
k
Keep Track
k
Keep Track
Tracker 1
Lost Tracking
Η
Η
Η
Η
Η
Tracker 1 disabled
Frame No.
Frame No.
Frame No.
GFDA
Η
Η
Η
Η
Η
LFDA
Tracker i
LFDA
Η
Η
Η
Η
Η
Η
Η
Η
Η
Η
Given training samples ( ){ }Mjjj dp 1, =
From M training samples, 
establish initial distribution
for all j( )
M
jD 11 =
Choose data according to 
the distribution and train mF
Compute error ( ) ( )jDmdpFjm jjm ≠Σ= :ε
Compute
m
m
m ε
εβ
−
=
1
mD
( ) ( )
¯
®
­
×=+ 11
m
m
m
m Z
jDjD
β ( )
otherwise
dpF jjm =if
Update the distribution on
After established       is enoughmF
Final result ( )¦ =∈ dpFm mDd m:
1logmaxarg β Step 8
Step 7
Step 6
Step 5
Step 4
Step 3
Step 1
Step 2
When the face tracker produces the prediction results, the 
ROI will perform the range of the face possible appearing in 
next state and extract it from the scene. The ( )1+kROIi
contains the center position of the face which is what the 
tracker predicts. 
( ) ( )iii FeFpkROI ,1 =+                                          (17) 
( ) ( ) ( )( )1ˆ,1ˆ1ˆ ++=+= kykxkXFp iiii                      (18) 
( )1ˆ +⋅= keCFe ii                                                    (19) 
( )1+kROIi  is composed of iFp  and iFe  which is the 
predicted position and the size respectively. In equation 
(19), C is used to change the size of ROI and is large than 1. 
The ROI is shown as Fig.6 
Fig.6  The range of interesting 
IV. EXPERIMENTAL RESULT
We experiment our proposed method to our robot for 
human following under the scenario of human-robot 
interaction. In general, a person interacting to the robot 
always stands in front of the robot and the robot should be 
face to face with the person. It is convenient for the person 
to interact with the robot. Therefore, the robot needs to keep 
contact with the person in constant distant while the person 
moves arbitrarily around the robot. Fig. 7 shows the 
scenario of our experiment. The robot detects the person 
who is in front of the robot and then tracks him. When the 
person moves backward, the robot intelligently moves 
following the person. The proposed approach has been 
successfully implemented on our service robot. 
(a)                                      (b) 
(c)                                      (d) 
Fig.7   The service robot follows the user 
Fig.8 shows the result of multi-faces detecting and tracking. 
Robot are designed to follow the person first appeared in the 
image. At frame 35, the GAFD has detected the first person 
entering the view of the image. The state is created and 
stored in the tracking database by the tracker controller. 
When a new face enters the field of the robot’s view, it will 
also be detected and tracking. It is shown in Fig. 8. The 
robot moves itself to keep tracking to the first person. The 
trajectories of the multi-faces are captured and shown in 
Fig.9. Three kinds of colors dots denote the trajectories of 
three faces respectively. In the experiment, the parameter C
is assigned to 4 empirically. By using the combination of 
Kalman tracker, the concept of ROI and the LAFD, the 
executing time of LAFD is running so faster that the multi-
human tracking is performed well. 
(a) Frame 35                      (b) Frame 175 
(c) Frame 260                      (d) Frame 490 
(e) Frame 590                      (f) Frame 650 
( )kX i
( )1ˆ +kX i ( )1ˆ +⋅ keC i
( )1ˆ +⋅ keC i
( )1+kROIi
NCCU Security Warrior:
An Intelligent Security Robot System
Ren C. Luo1 Fellow, IEEE, Yi T. Chou1, Chung T. Liao12, Chun C. Lai1, An C. Tsai1
1Department of Electrical Engineering National Chung Cheng University 
168, San-Hsing, Min-Hsiung, Chia-Yi 621, Taiwan, R.O.C
2Department of Electronic Engineering 
Wu-Feng Institute of Technology 
Chiayi, Taiwan 
{luo, ytchou, ctliao, cclai, actsai }@ia.ee.ccu.edu.tw
Abstract – This paper introduces the intelligent security robot 
developed by National Chung Cheng University (NCCU). The 
robot is named ƎSecurity WarriorƎ and consists of six systems 
including vision, motion, robot arms, power estimation, remote 
supervise and sensory systems. The vision system is used to carry 
out human detection and tracking. The motion system is built by 
using embedded systems and used to achieve motion planning in 
real time. The robot arms are the useful devices for the robot. 
Equipped with the arms, our robot can perform more tasks such 
as gesture expression or grasp functions. In order to avoid the 
robot shutdown suddenly, the power estimation system has been 
employed. For the robot, the sensory system is one of the 
important parts. With the fire detection sensor, the security robot 
can detect fire alarm in the environment. Path planning are 
accomplished by using the multi-sensor fusion. Body sensors can 
detect intruders. Combining the sensory and remote supervising 
systems, the owner can get notice whether there are any situations 
occurred through the PDA and GSM module. We have 
successfully implemented and tested our security robot. 
Index terms: security, security warrior, security robot, 
intelligent robot 
I. INTRODUCTION
The autonomous robot becomes a growing interest and a 
popular subject and has been implemented widely in many 
areas. Traditionally, the robots have been used in helping the 
workers in the fields of industrials and science such as 
semiconductor factories. Nevertheless, in the near years the 
robots are expected to provide all kinds of service in human 
daily lives. In the fundamentals, the developed service robot 
has the following functions to perform such the specific and 
security service: autonomous navigation, supervises through 
internet, a remotely operated camera vision system, human-
robot interaction (HRI) and so on. 
Many researchers and companies have developed various 
kinds of multi-functions robots.  The famous R&D groups MIT, 
IROBOT in America and SONY, HONDA in Japan have 
invested in advance robotics such as entertainment robots, 
military robots, service robots and etc. The developments of 
robotics continually have been published in recently year by 
more and more research organizations. For example, ASIMO 
[4] is made by HONDA and University of Tokyo proposes 
Human Robot-HRP [5]. In Japan, security robot has become 
more and more popular by people and companies such as 
ALOSK[8], FSR[7] and SECOM [6]. 
In this paper, we pay attention to present the functions of 
our security robot. The paper is organized as follows: Section 
II describes the system architecture of the security robot. 
Section III will present the sensory system and the power, 
motion control system, arms system, and vision system is 
proposed in Section IV to VII respectively. And the 
experimental results are shown in Section VII. Section V 
presents brief concluding remarks. 
II. SYSTEM ARCHITECTURE
The security robot is constructed using aluminum frame. 
The diameter is 50cm, and height is 170cm. The major 
components are shown in Fig. 2. The system structure of the 
security robot contains six subsystems including remote 
supervising, motion, vision, sensory, robot arms and power 
estimation subsystems.  
For achieving the security jobs of the robot, a central 
control unit called intelligent kernel is essential to handle the 
cooperation between all the subsystems. Each subsystem is 
running as an independent agent. The intelligent kernel 
coordinates with the agents by inter-process communication 
(IPC). Two kinds of IPC including internetworking socket and 
share memory are implemented for the communication. Fig. 1 
shows the illustration of the communication methodology 
between intelligent kernel and the motion control agent. 
The main controller of the intelligent security robot is a PC 
(Personal Computer) with a Pentium(R) IV CPU and 1GB 
RAM. The lower body contains motor drivers, batteries and 
two DC servomotors. 
Fig. 1 The communication methodology between intelligent 
kernel and the motion control agent. 
Intelligent 
Kernel 
Share Memory 
Motion 
Control 
Agent 
DC-DC power supply, batteries and relays. The relays are 
controlled by microcontroller in order to switch on or off some 
devices. Fig. 7 shows the architecture of power management 
block. The microcontroller is AVR MEGA128 (ATMEL) 
single chip. There are ten bits and eight channel analog to 
digital converter for sensing the voltage and current. 
PC & 
Other Subsystem
Atmel
AVR
UART(RS232)
Voltage
Voltage
Current Sensor
Current Sensor
ADC
ADC ADC
ADC
Power
Control Signal
Power Subsystem
Fig. 7.  Power subsystem architecture of mobile robot 
AVR
110Vac-24Vdc
Converter
12Vdc
Batteries series connection
110Vac Control signal
Switch
Power Block
PC & Other devices
Fig. 8. The architecture of power block 
In this paper, we propose a method to estimate and predict 
the state-of-charge (SOC) [11] of car-use battery (lead acid 
battery) to solve this problem. We employ a three-layer 
feedforward back-propagation (BP) artificial neural work 
(ANN) on our mobile robot. The neural network will help us to 
approximate battery discharging function which is used to 
estimate discharging capacity of battery remaining when the 
robot executes some tasks such as patrol. With estimating 
capacity of batteries remaining, the robot will know when the 
power is getting low, and robot will change to docking mode. 
In experiment, we will define Levenberg-Marquardt back-
propagation (LMBP) [12] to approximate and predict the 
discharging current. The absolute errors between the measured 
and predicted are 3.63%. Furthermore, the predicted SOC will 
be applied to make a decision when mobile robot needs to 
recharge. 
V. MOTION CONTROL SYSTEM.
A. Motion System Hardware 
The motion control system is designed based on embedded 
system. It controls the DC motor by the theory of PID control. 
The speed of the robot can achieve up to 8 km per hour.  
B. Motion Planning Algorithm 
Fig. 9.  The software architecture of edge following 
The software architecture of edge following is shown in the 
Fig. 9. This system has two subsystems: object detection and 
representation is used to organize and classify the ultrasonic 
sensor data and transform them into the edge following 
controller, the edge following controller uses four inputs to 
generate the suitable robot direction and velocity for the robot 
in current condition and these parameters will also feedback to 
the edge following controller [9]. 
In order to model the edge, we use first degree Least 
Square Approximation to model the edge of the obstacle. As 
Fig. 10.shows, the robot follows the edge along the line 
modeled by the calculation result. 
We also use the tangent bug algorithm [3] to do obstacle 
avoidance. The robot keeps moving to the goal until 
encountered an obstacle. When encounter an obstacle, the 
robot will follow the edge of the obstacle until the robot can 
link from himself to the goal without crossing the obstacle. 
Again, the robot keeps moving to the goal until encountered 
the other obstacle.
Fig. 10.  The parameters used in edge following algorithm and 
the piece-wise edge modeling  
there is only one face in the image and the previous state of the 
tracked face is presented. The tracker employs the Kalman 
Filter [2, 9] to track and predict the state of the face between 
the sequential frames. According to the prediction of the 
tracker, partial image which is called region of interest (ROI) is 
obtained for LAFD. The LAFD is executed in high frequency 
up to real-time. 
The tracker controller maintains the states of tracking 
human faces and controls the executing timing both GAFD and 
LAFD. It is shown in Fig. 14. When GAFD obtains the states 
of faces denoted as equation (1) and (2), the tracker controller 
compares the states of face with tracking database. 
( ) ( ) ( )kX,k,XkX i21                                               (1) 
( ) ( ) ( )( )ksfkpfkX iii ,=                                              (2) 
In equation (2), the ( )kpfi  and ( )ksfi  depict respectively the 
position and the size of the ith detected face. A new LAFD will 
be generated for a newly appearing face. When a LAFD 
updates the state of tracking face, the tracker controller revises 
the state of tracking face in the database. 
Frame No.
Frame No.
Frame No.
k k+T k+nT
k
Keep Track
k
Keep Track
Tracker 1
Tracker i
Global Face Detector
Lost Tracking
Η
Η
Η
Η
Η
Tracker 1 disabled
Η
Η
Η
Η
Η
Η
Η
Η
Η
Η
Fig. 15.Timing of the detector and trackers executing 
VIII. EXPERIMENTAL DEMONSTRATION
A. Arm Demonstration 
Fig. 16.  Arm demonstration 
We send control signals to let all the joints of the arms move at 
the same time. The commands can transfer in proper order 
neither conflicting with each other nor conflicting to the other 
devices. It is shown in Fig. 16. All the joints can work together 
and smoothly. 
B. Collision Avoidance 
 (a) (b) (c) 
  (d)  (e)  (f) 
Fig. 17.  Obstacle avoidance with LSA& Tangent Bug 
Algorithm 
The robot is doing the Obstacle Avoidance (Fig. 17.). First, 
the laser range finder finds an object in the front. Shown in 
Fig.17 (a), the robot detect cardboard box and try to avoid it 
(Fig.17 (b) (c)). Then the robot attends to move to the original 
path, but it still an obstacle (shown in Fig.17 (d)). And the 
robot avoids the obstacle again. In Fig.17 (e) and (f), the robot 
keeps walking along the original path.  
The trajectory sketch program shows the moving trajectory 
when the robot doing the obstacle avoidance. 
C. Vision and Tracking 
In this paper, our approach is useful to human-robot 
interaction. By using AdaBoost face detect algorithm and the 
Kalman tracker, the approach we proposed has been 
successfully implemented on our service robot. The robot can 
detect the person who is in front of the robot. When the user is 
detected, the robot will track the face of the user and follow the 
person. The performance of the proposed tracking system is 
shown as Fig.18.  
   
(a)                                      (b) 
(c)                                      (d) 
Fig. 18.   The service robot follows the user 
1Final revisions to IEEE Transactions on Industrial Electronics 
Paper Manuscript ID  
TIE-00676-2006, entitled "Multilevel Multisensor Based Intelligent  
Recharging System for Mobile Robot."  
The type of submission: Regular paper 
Multilevel Multisensor Based Intelligent Recharging System for Mobile Robot  
Ren C. Luo1 Fellow, IEEE, Kuo L. Su2
1Department of Electrical Engineering, National Chung Cheng University 
160, Shang-Shing, Ming-Hsiung, Chia-Yi, Taiwan 621, R.O.C. 
2Department of Electrical Engineering,National Yunlin University of Science &Technology, 
123, Sec. 3, University Rd., Touliu, Yunlin, Taiwan 640, R.O.C. 
Email: luo@ia.ee.ccu.edu.tw, sukl@yuntech.edu.tw  
TEL: +886-5-272-0411 ext. 23270 
Abstract—Based on the sensor based detection method, this 
paper presents an intelligent recharging system for a mobile 
robot. Firstly, we design a flexible and reasonable intelligent 
recharging system for the mobile robot. It consists of a 
recharging station, a recharging device, and an intelligent power 
detection module. The recharging station is designed to have two 
DOFs such it can move along X-axis and rotate about the Z-axis.
Meanwhile, a mechanism with two guarding poles is designed to 
provide a connection between the recharging station and the 
robot. In the recharging device, four power sensors are used to 
measure the power variety. Meanwhile, the adaptive fusion 
method is used to detect and diagnose the power sensor status. 
Auto-protection circuits are also designed to prevent the short 
and overload conditions during the recharging process. In the 
intelligent power detection module, three power sensors are used 
to measure the power variety, and a redundant management 
method is used to detect and diagnose the power sensor status. 
The intelligent power detection module can transmit a decision 
output to the main controller using a RS232 interface. Then the 
main controller can decide an exact output to control the 
recharging current using a rule based method. Before practical 
implementation of the proposed method, computer simulation is 
performed and the results show its feasibility. Then based on the 
proposed method, different modules are implemented and the 
experimental results also verify the feasibility of this method. 
Keyword—mobile robot, adaptive fusion method, redundant
management method 
I. INTRODUCTION
With the robotic technologies changing with each passing 
year, mobile robots have been widely applied in many fields 
such as factory automation dangerous environment detection, 
office automation, hospital, entertainment, space exploration, 
farm automation, military and security system and so on. 
Recently more and more researchers take interests in the 
intelligent service robot, the examples such as ASIMO and 
AIBO. Design of security robots is the focus of our research 
and a security robot named Chung Cheng I has been build. 
The auto recharging system is designed for the 
Chung-Cheng I security robot. The robot has the shape of 
cylinder and its diameter, height, and weight are 50 cm, 130 
cm, and 100 kg, respectively. The robot is equipped with an 
IPC (Industry Personal Computer) as the main controller, 
some microprocessors, a touch screen, several sensor circuits, 
two DC servomotors and a color CCD. Meanwhile, it has four 
wheels to provide the capability of autonomous mobility. The 
recharging system has nine rechargeable batteries, three are 
used as the power supply of the two DC servomotors and the 
other six are used as the power supply of the IPC and sensory 
circuits [1, 2, 3]. 
  In this paper we want to design a multisensor based 
recharging system for the mobile robot such that the robot will 
not shutdown due to power shortage. The power detection 
module in the robot must be able to detect the currents and 
voltages of the batteries. When power shortage occurs, the 
robot must be able to go to the recharging station 
autonomously within ten minutes [3]. In the past literatures, 
auto-recharging systems of mobile robots have been studied 
by many researchers. Milo et al designed a new docking 
station for Pioneer 2DX mobile robot and presented a method 
to provide long-term autonomy by implementing autonomous 
recharging [4]. In the current detection method, Melia and 
Nelson postulated that monitoring of the power supply current 
could aid in the testing of digital integrated circuits [6, 7]. Levi 
is one of the first researchers to comment upon the 
characteristics of CMOS technology, which make it special 
amenable to IDD testing [8]. Malaiy and Su used IDD testing and 
estimated the effects of increased integration on measurement 
resolution [9, 10]. Frenzel proposed the likelihood ration test 
method, which can be applied on power-supply current 
diagnosis of VLSI circuits [11]. Horning and Hawkins et al 
reported on numerous experiments where current 
measurements have forecast reliability problems in devices 
which had previously passed conventional test procedures [12, 
13]. Then, researches dedicated to improving the accuracy of 
measuring currents [14, 15]. Maly et al proposed a build-in 
current sensor which provides a pass/fail signal when the 
current exceeds a set threshold [16, 17, 18]. 
The first work on robot recharging is made by Walters [5]. 
The author developed the first autonomous recharging mobile 
robots and employed a light beam to find a hut, which 
contains a light beacon and a battery recharger. Roufas [19] 
used four IR LED emitters and one IR receiver to implement 
the docking. Their method can allow six degrees of error. The 
robot can approach the specially designed recharging station 
using the map. Some systems are using vision-based method 
to implement docking [4, 21, 22, 23].Some of applications 
related to these subject areas are reported [27-29,31]. In this 
paper, we want to design a safe, convenient, and intelligent 
3(a)                   (b) 
Fig. 3. The robot docking mechanism. 
The robot docking mechanism is shown in Fig. 3(a), which 
is mounted in front of the robot. The recharging adapters are at 
the inside of the holes. Each hole is shaped as a cone in order 
to help the docking smoothly (Fig. 3(b)). Since both the 
connection pins and adapters are rigid, the docking station is 
designed for providing compliance for reasonable robot 
docking angle and offset. When the robot is approaching to the 
docking station with offset and docking angle, the guiding 
stick mounted on the docking station will guide the recharging 
pins for inserting into the adapters. Fig. 4 shows the 
mechanism of docking station. A little entry offset and angle 
are allowed when the robot is in the docking process. The 
maximum rotational angle and horizontal offset are 5 degree 
and 2 cm, respectively. 
Rotational axis 
Spring
Recharge pin
(a) Undocking condition 
Rotational axis 
Spring
Horizontal offset
Rotational angle
(b) Possible docking condition 
Figure 4. Docking Device: Two degrees of freedom. 
In the docking station, there are a blue landmark, a laser 
light and a safety device. The blue landmark guards the mobile 
robot to the docking station. If the batteries of the mobile 
robot have been full, the laser light trigs the mobile robot to 
leave. The safety detection device can detect recharging 
current using three current sensors, and use relay elements to 
control recharging currents. In the docking mechanism of the 
mobile robot, there are a CCD, an auto-switch and three 
current sensors. The CCD device catches the blue landmark to 
guard the mobile robot to the docking station. The detection 
acquires the laser light to trig the mobile robot. The 
auto-recharging switch has two relay elements. The DC 12V 
recharging current is controlled by one relay element, and the 
DC 36V recharging current is controlled by the other. In the 
intelligent power detection module, it can fuse an exact 
measurement value using four current sensors. The module 
can transmit measured values, decision outputs and sensory 
status to the main controller using series interface (RS232). 
Fig. 5. The current detection processing of the recharging device 
The hardware block diagram of the recharging device is 
shown in Fig. 5. The controller is a microprocessor, and the 
recharging current is detected using three DC current sensors 
(LEM-55P). The input signal has a limit switch. The output 
signal contains two safety switches, a laser light, a display 
console and RS-232 interface. The safety switch is used to 
turn on or turn off the recharging current. The prototype of the 
auto-recharging device is shown in Fig. 6. The recharging 
device can provide 20A current to DC 12V batteries, and 
provide 10A current to DC 36V batteries. 
Fig. 6. The prototype of the auto-recharging device 
The auto-recharging processing of the docking station is 
shown in Fig. 7. The mobile robot is guided using photo 
resisters by a laser light. The mobile robot touches the limit 
switch that is mounted on the docking station. The controller 
can provide the recharging current to the mobile robot after 10 
seconds. In the auto-recharging device, we use two steps to 
charge the batteries of the mobile robot. The reason is to 
extend the life of batteries. Firstly, we use low voltage to 
charge the batteries. The current sensors can detect the 
recharging currents to prevent the conditions of overload and 
short-circuited. In the normal condition, the recharging current 
is smaller than the threshold value for the low voltage 
recharging status. The system must change to high voltage 
recharge status, and provide recharging current to batteries. 
Finally, the saturation of the batteries will indicate the 
auto-recharging process is completed. In this case, the laser 
light will be on and the mobile robot can leave the docking 
station.   
5In the intelligent recharging system, we enhance detection 
precision using a two-level multisensor fusion architecture, 
which is shown in Fig. 12. Fusion methods and rule based 
methods are used in level one and level two, respectively. In 
level one, we use adaptive fusion method in recharging device, 
and decide an exact output for maximum and minimum 
condition of recharging current. We use redundant 
management method to get an exact measured value of 
recharging current. The maximum current fusion decides D0 
output, and transmits the signal to level 2. The D0 signal is a 
digital output. The recharging current is over the maximum 
recharging current, and the D0 signal is “high”; otherwise the 
D0 signal is “low”. In the recharging device, we use three 
current sensors to enhance the detection capability. We also 
use the adaptive data fusion algorithms to obtain the high 
reliable results in the fire detection system.  
In the detection system, the weight is a function of the 
probability of detection PD, the probability of false alarm PF 
and miss probability PM of the sensor. The relation of 
detection rule is shown in Fig. 13. We can determine PD and 
PF for different sensors through experiments. However, in 
reality PD, PM and PF may vary with time. Therefore, the 
weight function may vary with time. The principles of every 
sensor are not the same and the weight vales are time-varying 
in real conditions. Hence the weight values are not equal at all 
time and we can use the adaptive decision to obtain the 
optimal values. 
Fig. 12. The fusion architecture 
Fig. 13. The relation of detection rule 
At first, let us consider a binary hypothesis experimental 
problem with the following two hypotheses: 
H0:  minimum recharging current d  recharging current 
d  maximum recharging current 
H1:  recharging currentɧmaximum recharging current or 
  recharging currentɦminimum recharging current  
Then a priori probabilities of the two hypotheses are defined 
by P (H0) =P0, and P (H1) =P1 so that we can find the initial 
weight:
0
1
0 log P
P Z
                          (1) 
°¯
°
®
­
 
 

 

1If
1
log
1If
1
log
i
Fi
iM
i
Mi
iFi
U
P
P
U
P
PZ
   (2) 
The adaptive multisensor fusion architecture is shown in 
Fig. 14. S1, S2 and S3 represent current sensor #1, current 
sensor #2 and current sensor #3, respectively. These signals of 
sensors must be processed to be binary by comparison circuits. 
This means that these sensor signals must be high when 
recharging current is over the maximum recharging current or 
smaller than the minimum recharging current. Otherwise, their 
values are low. 
Fig. 14. Adaptive multisensor fusion architecture 
We can obtain the reinforcement updating rules are [25] :            
°
°
°
°
°
¯
°°
°
°
°
®
­
 
 
 
 
 '
¦
¦
f
 
f
 
1
0k
k
0i
0
0
0k
k
0i
1
0
0
1
1
1
!
)W(W1
1
!
)W(W1
11
11
HandUiIf
km
HandUiIf
km
HandUiIfm
HandUiIfm
W
i
i
i
i
i
       (3)  
                                             (4) 
iii ZZZ ˆˆˆ ' 
 , i=0, 1, 2, where iZˆ  and 

iZˆ
represent the weight value after and before each update.  
In the intelligent power detection module, we use the 
redundant sensor management method to detect and diagnose 
sensory status. The redundant measurements of a process 
variable are defined as [3, 26]. 
        EHXM                      (5) 
°
¯
°
®
­

 '
occursHWhene
m
occursHWhen
m
0
ˆ
1
0
ˆ1
ˆ1
ˆ
0Z
Z
7(a) The four measured values.
(b) The estimate value. 
Fig. 16. The experimental results for Case I 
Case I: If all measurements are consistent, i.e., Ii=0, 
i=1,2,3,4 the estimate 
4321
44332211ˆ
wwww
wmwmwmwmx


          (12) 
The experimental results are shown in the Fig. 16, in 
which the four sensory measurements are normal. This means 
that the four sensors are consistent. Then we can calculate 
xˆ  . Fig. 16-(a) presents four measured values for case I. Fig. 
16-(b) presents the estimate value and average value for case I. 
Case II: If two measurements m1, and m2 are faulty 
simultaneously and identically, then they are two consistent 
pairs, (m1, m2) and (m3, m4), that are mutually inconsistent. In 
this case, we can find I1=I2=I3=I4=2, Imin=>0, Imax=4. The 
result is that no estimate value can be obtained because there 
is a possible common-mode faulty. That is to say, we cannot 
decide which pair of (m1, m2) and (m3, m4) will be right. The 
two fault sensors may make the gain of the amplifier and 
calibration circuits a bias. Then the method is not applicable 
and we cannot acquire an exact estimate value. In this case, 
only the average value can be calculated. These results are 
shown in Fig. 17. 
(a) The four measured values.          (b) The average value. 
Fig. 17. The experimental results for Case II 
Case III: We suppose two measurements m1 and m4
simultaneously and identically, i.e., I1=I4=3, I2=I3=2. Because 
Imin=2>0, Imax=3 and Nmax=2 >1, the two measurements m2
and m3 that have Ii < Imax are tested on the next loop. The 
resulting estimate is 
32
3322ˆ
ww
wmwmx


                         (13) 
(a) The weight measured values.  (b) The estimate value and average value 
Fig. 18. The experimental results for Case III 
From the consistent measurements m2, m3 and the remaining 
measurements m1 and m4 are isolated as faulty. Fig. 18 
presents the experimental results for Case III, in which m1 and 
m4 are found to be fail and unidentifiable. That is to say, the 
sensor m1 may be suddenly broken, or the connected line is 
floating. The proposed method may diagnose and isolate the 
error sensor and estimate an exact measured value. Fig. 18-(a) 
presents four measured values for case III , and Fig. 18-(b) 
presents the estimate value and the average value using 
equation (13) for case III . 
Case IV: If m1, m2, m3 and m4 are mutually inconsistent. 
That is to say, I1=I2=I3=I4= 3, then no estimate value is 
possible because all measurements are inconsistent. In this 
case, the faulty result has been detected, but it can not be 
isolated unless addition information is available. In the case of 
IV, m1, m2, m3 and m4 are mutually inconsistent, and then the 
proposed method is fault. Therefore, the exact estimate value 
does not exist. These experimental results are shown in Fig. 19.
9[9] Y. K. Malaiya, “Testing stuck-on faults in CMOS integrated circuits,” 
Proceedings of International Conference on Computer-Aided Design, 
1984, pp.248-250. 
[10] Y. K. Malaiya and S. Y. H. Su, “A new fault model and testing 
technique for CMOS devices,” Proceedings of International Test 
Conference, 1982, pp.25-34 
[11] J. F. Frenzel, “Power-Supply Current Diagnosis of VLSI Circuits,” 
IEEE Transaction on reliability Vol. 43, No.1 1994, pp.30-38. 
[12] L. K. Horning et al, “Measurements of quiescent power supply current 
for CMOS ICs in production testing,” Proceedings of International Test 
Conference 1987, pp.300-309. 
[13] J. M. sodden and C. F. Hawkins, “Test considerations for gate oxide 
shorts in CMOS ICs,” IEEE DEsesign & Test, 1086 Aug, pp56-64 
[14] C. Crapuchettes, “Testing CMOS IDD on large devices,” Proceedings 
of International Test Conference, 1987. pp.310-315. 
[15] M. Keating and D. Meyer, “A new approach to dynamic IDD testing”, 
Proceedings of International Test Conference, 1987, pp.316-321. 
[16] L. R. Carley and W. Maly, “A circuit breaker for redundant IC 
systems,” Proceedings of Custom Integrated Circuits Conference, 1988, 
pp.27.6.1-27.6.6. 
[17] D. B. I. Feltham et al, “Current sensing for built-in testing of CMOS 
circuits,” Proceedings of International Conference on Computer Design, 
1988, pp.454-457. 
[18] W. Maly and P. Nigh, “Build-in current testing- Feasibility study,” 
Proceedings of International Conference on Computer-Aided Design, 
1988, pp.340-343. 
[19] K. Roufas, Y. Zhang, D. Duff and M. Yim, "Six degree of freedom 
sensing for docking using IR LED emitters and receivers," in The 7th 
Intl. Symp. On Experimental Robotics. 2000. Hawaii. 
[20] Yuta, S. and Hada, Y., “Long term activity of the autonomous robot – 
Proposal of a bench-mark problem for the autonomy,” Proceedings of 
the 1998 IEEE/RSJ Intl. Conference on Intelligent Robots and Systems, 
Victoria B.C. Canada, pp. 1871-1878, October 1998. 
[21] R.C. Arkin and R. R. Murphy, "Autonomous navigation in a 
manufacturing environment," IEEE Trans, Robot. Automat. vol. 6, pp. 
445-454, Aug. 1990. 
[22] B. W. Minten, R. R. Murphy, J. Hyams and M. Micire, 
"Low-order-complexity vision-based docking," Robotics and 
Automation, IEEE Transactions on , Volume: 17 , Issue: 6 , Dec. 2001 
pp.922 - 930 
[23] Barnes, N. and Z. Q. Liu, “Fuzzy Systems, Fuzzy Control for Active 
Perceptual Docking,” 2001. The 10th IEEE International Conference 
on, Volume: 3, 2-5 Dec. 2001, pp.1531 – 1534 
[24] Ren C. Luo, Chung T. Liao, Kuo L. Su, and Kuei C. Lin, “Automatic 
Docking and Recharging System for Autonomous Security Robot,” 
2005 IEEE International Conference on Intelligent Robots and Systems 
(IROS'05), Edmonton Alberta, Canada, August 2005 
[25] Ren C. Luo, Kuo L. Su, Kuo Ho Tsai, “Intelligent Security Robot Fire 
Detection System Using Adaptive Sensory Fusion Method,” IEEE 
International Conference on Industrial Electronic, Control, and 
Instrumentation (IECON’2002), pp.2663 – 2668. 
[26] H. P. Polenta, A. Ray and J. A. Bernard, “Microcomputer-based Fault 
Detection Using Redundant Sensors”, IEEE Transactions on Industry 
Application, Vol.24, No. 5, September/October, 1988, pp.905-912. 
[27] Lee, D.; Chung, W. “Discrete-Status-Based Localization for Indoor 
Service Robots” IEEE Transactions on Industrial Electronics,Volume 
53,  Issue 5,  Oct. 2006 pp1737 - 1746 
[28] Kim, G.; Chung, W. “Tripodal Schematic Control Architecture for 
Integration of Multi-Functional Indoor Service Robots”IEEE 
Transactions on Industrial Electronics, Vol 53, Issue 5, Oct. 2006 
pp1723 – 1736 
[29] S. Katsura, K. Ohnishi, "Semiautonomous Wheelchair Based on 
Quarry of Environmental Information," Trans. on Industrial 
Electronics, Vol. 53, no. 4, pp. 1373- 1382, June 2006. 
[30] Goodridge, S.G.; Kay, M.G.; Luo, R.C. “Multilayered fuzzy behavior 
fusion for real-time reactive control of systems with multiple sensors”  
IEEE Transactions on Industrial Electronics, Vol 43,  Issue 3,  June 
1996 pp387 – 394 
[31] Naoyuki Kubota, Kenichiro Nishida, "Perceptual Control Based on 
Prediction for Natural Communication of a Partner Robot," Trans. on 
Industrial Electronics, Vol. 54, no. 2, pp. 866-877, Apr. 2007.  
Automation 2007 
The Ninth International Conference on Automation Technology 
June 13-15, 2007, Taipei, Taiwan 
Multisensor Based Intelligent Mobile  
Security Robot System 
 
 
 
Ren C. Luo1, Yu-Feng Tseng2, Yi-Ju Chen2, Chung-Ta Liao2,3 
 
 
Keywords： security system, fire detection, intruder detection, robot
 
 
ABSTRACT 
 
The most importance role of the intelligent system 
is security system. In the paper, we focus on the fire 
detection and intruder detection of the security system, 
and use intelligent security robot to guard the safety for 
life and wealth in the intelligent building. We propose 
an adaptive fusion method for fire detection, and use 
flame sensor and temperature sensor to detect fire 
occurred. In reality, the phenomenon of the fire may 
have flame and high temperature. Then we use rule 
based method to detect intruder, and fuse body sensors, 
ultrasonic sensors and visual system to detect intruder. 
Finally, we implement the fire detection system in the 
intelligent security robot. If fire occurred, the security 
robot can find out the fire source using the fire 
detection system. In the intruder detection, we program 
the same scenario to detect the intruder using the 
intelligent security robot. When the intelligent security 
robot detects any fire and intruder, he will get alarm 
and use his voice to warn people. At the same time the 
intelligent security robot transmits the message of 
detected result to the user by using GSM module for 
fire and intruder event, and transmits the detection 
result to client computer through internet. 
 
I.  INTRODUCTION 
Intelligent buildings can provide safety, 
convenience, and efficiency for society in the 21st 
century. An intelligent building system (IBS) was 
integrated by many services and subsystems. 
One of the most important subsystems is the fire 
detection function system in an intelligent building 
(Neubauer, 1997). The fire event may involve hazard in 
life and wealth. In generally, the fire detection device is 
fixed on the wall. But the method is not flexibility to 
detect fire event, and it should use many fire detection 
devices in the building. It is not very convenience. In 
the past, the fire detection sensor generally used smoke 
sensors, but the smoke sensors often detected errors 
when anybody smokes in the chamber. So that 
multisensor fire detection methods are one of current 
important developments for fire detection technology in 
home and building. In recent years, many researchers 
propose some methods for fire detection in the field. 
Wang et al (2000) developed a multisensor fire 
detection algorithm using neural network. One 
temperature and one smoke density sensor signal were 
fused for ship fire alarm system. Healey et al. (1993) 
presented a real-time fire detection system using color 
video input. The spectral, spatial, and temporal 
properties of fire were used to derive the fire detection 
algorithm. The experimental results demonstrated 
system performance on a burning jet fuel. Neubauer 
(1997) applied genetic algorithms to an automatic fire 
detection system. The on-line identification of 
stochastic signal models for measured fire signals was 
presented. Ruser and Magori (1998) described the fire 
detection with a combination of ultrasonic and 
microwave Doppler sensor. Luo and Su (2002) use two 
smoke sensors, two temperature sensors and two flame 
sensors to detect fire event, and diagnosis which sensor 
is failure. 
 In generally, there are some conditions in the fire 
surrounding, such as, smoke may be density, flame 
happened and temperature may be increasing. Then we 
use the sensors to detect the fire and locate fire location. 
We also can extinguish the fire by the extinguisher of 
the security robot. We modify the adaptive fusion 
method to be applied in the fire detection system, and 
use microprocessor to implement the method for the 
goal. 
 The paper is organized as follows: Section II 
describes the system structure of the intelligent security 
Author for Correspondence: Ren C.Luo. 
 
1  Professor, Department of Electrical Engineering, National 
Chung Cheng University, Mni-Hsiung Chia-Yi, Taiwan, ROC.
2 Gradnate Student, Graduate Institute of Electrical 
Engineering,  National Chung Cheng University 
3  Instructor, Department of Electronic Engineering, Wu-Feng 
Institute of Technology, Mni-Hsiung Chia-Yi, Taiwan, ROC. 
 
Automation 2007 
sensors, flame sensors and smoke sensors to detect the 
fire, and extinguish the fire. We have the other systems, 
robot arm, wireless LAN, voice device and GSM 
module to help our robot execute the security task.  
For these subsystems, we use some sensors to 
implement the function. These sensors are listed in 
TABLE I. 
 
 
Fig. 3.  The hardware architecture of the security 
robot. 
 
TABLE I 
Tasks and Type of Sensors in the  Security Robot 
Task Sensors Examples 
Flame Sensor R2686 Fire detection 
Temperature Sensor AD590 
Body Sensor  
Ultrasonic Sensor Polaroid 6500 
Intruder 
detection 
CCD camera  
 
The arrangement of intruder detection sensors and 
other hardware are shown in Fig. 1. In the fire detection 
sensors, we combine smoke sensor, flame sensor and 
temperature sensor. The modular is fixed on the front 
side of the intelligent security robot. In the intruder 
sensors, we use body sensors, sixteen pieces ultrasonic 
sensors and laser range finder to arrange outside of the 
intelligent security robot. The intelligent security robot 
can transmits message to the user using GSM modern. 
The intelligent security robot communicates with cell 
phone using GSM (Global System for Mobile) modular. 
The GSM modular (WMOD2) was made by Wavecom. 
The module is a self-contained EGSM900/ GSM1800 
(or E-GSM900 /GSM1900) dual band module. 
 
 
III.  DETECTION METHOD 
 
There are many algorithms is applied in the sensor 
system. In the fire detection subsystem, we use adaptive 
multisensor fusion algorithm (Ren C. 2002) to detect 
the fire. And we use the fire location algorithm to find 
the direction and the location of the fire. In the intruder 
detection subsystem, we use rule-based method and 
combine body sensor, ultrasonic sensors and visual 
system to implement(2003). 
 
Fire Detection Method 
The adaptive decision fusion method was 
proposed by Ansari (Ansari 1996,1997). The parameter 
and detail of the equation is discussed in another paper 
(2002), and the reinforcement updating rules are: 
 
Fig. 4.  Adaptive multisensor fusion architecture 
 
The adaptive multisensor fusion architecture is 
shown in Fig. 4. S1, S2 and S3 represent smoke sensor, 
flame sensor and temperature sensor dependably. These 
signals of sensors must be processed to be binary 
digital by comparison circuits. That is to say, when 
these sensors detect fire happened. These sensor signals 
must be high; otherwise, when nothing be detected by 
these sensors, these sensor signals must be low. We 
want to extend the adaptive fusion method to detect fire 
happened and diagnose error sensor using 
microprocessor. We use the Taylor expansion equation 
for (1), we can get rule is: 
 
⎪⎪
⎪⎪
⎪
⎩
⎪⎪
⎪⎪
⎪
⎨
⎧
−=−−
+=+−
−=
+=
=Δ
∑
∑
∞
=
∞
=
0
0
0
0
0
1
0
1
1
!
)(1
1
!
)(1
11
11
k
0i
k
i
i
k
0i
k
i
i
0i
i
1i
i
i
H and U If     
k
WW
m
H and U If     
k
WW
m
H and U If                    
m
H and U If                    
m
W
   
(1) 
⎪⎪⎩
⎪⎪⎨
⎧
−
=Δ
∑∞
=0
0
0
1
0
!
1
1
k
k
occurs H When         
k
W
m
occurs H When                 
m
      
W
                 
(2) 
Fire 
Enviro-
nment
S
S
S
Σ 
W1 
W2 
W3 
Update Rule 
W
S1:Smoke Sensor 
S2:Flame Sensor 
S3:Temperature Sensor 
 
Automation 2007 
IV.  EXPERIMENTAL RESULTS 
 
 We implement the proposed method applying in 
the intelligent security robot. The intelligent security 
robot has the fire detection mode and intruder detection 
mode. We will show the experimental results of the two 
modes. 
 
Fire Detection Method 
We can see the flowchart of the fire detection 
mode in the Fig. 8. In the fire mode, the security robot 
can move in the free space and detect the fire in the 
patrol environment. The scenario of the fire detection 
experiment is shown in Fig. 9(a). When fire event 
happen, the security robot can detects flame source 
using the fire sensory team. Security robot can alarm to 
warn people and send a warning message to security 
via GSM module at the same time. Security robot can 
find and locate the preliminary location of fire by the 
fire detection method. It is shown in Fig. 9(b). The 
intelligent security robot can moves orientation to face 
the fire source. The scenario is shown in Fig. 9(c). 
When the security robot finds the location of the fire 
source, the security robot will use fire extinguisher 
which mount in security robot to extinguish fire. It is 
shown in Fig. 9(d). After extinguishing fire, he will 
detect if is there any fire or not. If the security robot 
can’t extinguish fire, he will let the security people to 
execute the task. 
 
 
Fig. 8.  The flowchart of the fire detection mode in the 
security robot. 
 
  
(a) The robot detect fire   (b)The robot is locating fire 
  
(c)The robot moves      (d)The robot extinguishes 
orientation to face the fire.    fire. 
Fig. 9.  The scenario of fire detection. 
 
Intruder Detection Method 
 We can see the flowchart of the intruder detection 
mode in the Fig. 10. Because security robot will 
execute its task during night, the people they met will 
all be thought as intruders. In the intruder mode, 
Security robot always detects intruders in the 
environment. The scenario is shown in Fig. 11(a). First, 
the security robot use body sensor to detect intruder. 
The scenario is shown in Fig. 11(b). When the Security 
Robot detects an intruder, he will alarm to warn people 
and sends a warning message to security via GSM 
module. And then, we use the CCD camera to capture 
the intruder. The security robot can take the intruder’s 
picture and capture the intruder’s face. The scenario is 
shown in Fig. 11(c)(d). The security people can use the 
information which was got by the security robot to 
execute the security tasks. The scenario is shown in Fig. 
11(e)(f). 
 
 
Fig. 10.  The flowchart of the intruder detection mode 
in the security robot. 
 
  
(a) Intruder detection    (b) Detect an intruder 
Detect an 
intruder?
To start alarm and tracking 
intruder
Intruder Mode
Send a message to guarder via 
GSM 
Capture a intruder face 
The end. 
Yes
No
B
B 
Detect a 
flame? 
To start alarm 
Fire Mode 
Yes 
No 
Finding and Locating the fire 
Extinguish the fire 
Send a message to guarder via 
GSM 
The end. 
Fire 
extinguish
Yes 
No
A 
A 
