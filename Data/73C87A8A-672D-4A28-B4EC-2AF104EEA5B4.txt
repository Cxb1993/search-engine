感測網路分散式救援機器人研發－總計劃 (3/3) 
計劃編號：NSC 98-2218-E-009-005 
執行期限：98/08/01~99/7/31 
主 持 人：徐保羅 國立交通大學電機與控制工程系 
 
 
摘要 
 串結式機器人具備優越的克服複雜地形能力，相當適合執行救援任務。本報告設計並實現分散式
運動控制策略，串結式機器人可以藉由單節之間的合作協調，克服複雜地形。同時，為了賦予機器人
自主學習的能力，以增進克服地形的效率，引入了螞蟻演算法進行機器人的行為選擇。藉由在台階地
形上的實驗結果驗證，經過第一次的螞蟻演算法訓練後，可縮減地形克服時間 55%。針對克服時間過
長的問題，本報告提出了序列訓練，使得第一次訓練時的克服時間標準差，較未加入序列訓練時，減
少了 77%。針對學習反效果問題，本報告提出了費洛蒙調整，即使經由多次訓練後，也能保證地形克
服時間趨於一致，證明了學習的可靠性。 
 
關鍵字：分散式機器人、螞蟻演算法、費洛蒙調整 
 
Abstract 
 The chain-type robot possesses great capability to overcome complex terrains so that it is suitable for 
rescue missions. The present design of distributed motion control for the chain-type robot is mainly applied 
to overcome complex terrains via appropriate coordination among distributed robot modules. Moreover, to 
improve the chain-type robot motion for complex environments, the learning procedure via the ant colony 
optimization (ACO) is used. Experimental results on the step motion show that ACO decreases the terrain 
conquering time by 55% after the first learning procedure. In addition, the sequential training is proposed to 
decrease the conquering time and the deviation of conquering time for the first learning procedure is 77% 
less than that without sequential training. The algorithm of pheromone adjustment is also proposed to 
2. 分散式行為模式多單元控制 
2.1. 分散式行為模式多單元控制 
如下圖 2-1 所示，串結式機器人在硬體機構上，即具有多單元(multi-unit)的特性。 
              
L i n k  1 J o i n t  1 L i n k  2 J o i n t  2
 
Unit 1 Unit 2
 
                 圖 2-1 串結式機器人硬體機構的多單元特性 
 
在此，將 link 1 和 joint 1 劃分為 unit 1，link 2 和 joint 2 劃分為 unit 2，以此類推。這種多單元的
特性，可以推展到軟體演算法中，成為分散式策略。軟體演算法的分散式架構如下圖 2-2 所示。在此
架構中，個別單元獨立決策，單元間再透過訊息(message)的溝通達成協調。 
Accelerometer ,
Motor Feedback
Action 
Output
Motor 
Feedback
Message 
Unit 1
Action 
Output
Unit 2 Behavior-based 
Motion Control
 
Behavior-based 
Motion Control
 
圖 2-2 串結式多單元分散式控制架構 
 
 
 
 
多單元(multi-unit)的演算法架構有以下兩種優點： 
(1) 可擴充性(extensibility)：當硬體機構多了一個 unit 或少了一個 unit 時，在軟體上僅需增加或
刪減處理該 unit 的模組即可，無需進行繁瑣的修改。 
(2) 簡易性(simplicity)：單一 unit 的演算法模組僅需考慮自身 unit 的決策即可，各個 unit 間再透
過 message 的傳送、接收，以達成協調。 
 個別單元決策介紹如下： 
                    圖 2-6 master 的 riser climbing 行為狀態圖 
     
 
 而上抬的關節轉速 stuckSpeed=MAX_SPEED  ( MAX_SPEED為馬達最大轉速)，則是由模糊推論
(fuzzy reasoning)的第二種型態，Tsukamoto 所提出的方法[6]，推演而來。使用此方法的好處為，可將
關節馬達的轉速命令，以一簡單的式子表示，無需進行繁複的解模糊化(defuzzification[6])運算。如下
圖 2-7 所示，  為偵測到的履帶馬達扭矩，v 為關節馬達轉速命令，利用模糊規則「 IF  is stuck   
,  THEN v is max」推論而得所需的轉速命令。 
(Motor Torque)
1
170 252 MAX_SPEED 
(110 rpm) v(Motor Speed)
max (v)
1
IF  is ,  THEN v is stuck max
Fuzzy Reasoning of the Second Type: Tsukamoto’s Method
( )stuck 
 
                        圖 2-7 由模糊推論推得所需馬達轉速命令 
 
 
 
 
 
 
 
Master
Slave
 
圖 2-11 此時需執行 tread landing 行為 
 
Master
Slave
 
圖 2-12 此時需執行 riser climbing 行為 
針對此問題，強化式學習往往是不錯的解決方案。而螞蟻演算法[7][8]可將候選解的機率分布以
費洛蒙濃度表示，並利用環境給機器人的反饋訊號，更新費洛蒙濃度，調整控制策略。由於螞蟻演算
法在求解過程中，並不會修改或刪除任何現有的解，僅僅改變解的機率分布，因此螞蟻演算法具有極
佳的搜索能力，不易陷入次佳解中。基於搜索能力強的優點，本論文引入螞蟻演算法，進行機器人的
行為協調，使得機器人可以在運動過程中，自主調整策略，增進克服地形的效率，且即使在缺乏距離
感測器的情況下，依然能透過詴誤學習，成功克服地形。 
3. 螞蟻演算法自主學習 
3.1. 螞蟻演算法簡介 
螞蟻演算法(ant colony optimization[7][8])係從自然界中，螞蟻群體的行為，得到靈感，進而發展
出來的一門學問。自然界中，螞蟻總能找到一條把食物搬回家的最短路徑。當發現食物時，兩隻螞蟻
同時離開巢穴，分別走兩條路線到食物處。較快回來的，因費洛蒙揮發較少，會在其路線留下較多的
費洛蒙(pheromone)作為記號。因此，其他同伴聞到較重的味道時，自然就會走較短的路線，然而，
會有少部分的螞蟻不會依循目前費洛蒙較多的路線去走，而會憑自己的意思走新的路，因此可以漸漸
地找到最短路徑。示意圖如下圖 3-1 所示。 
        
Without reinforcement,
pheromone evaporates.
Pheromone 
trail
 
                           圖 3-1 螞蟻演算法示意圖 
 
 螞蟻演算法的優點在於具有良好的搜索能力(exploration ability)。螞蟻演算法在求解的迭代過程
          
     圖 3-3 以螞蟻演算法進行行為協調示意圖 
在第 i 個中繼站上，選擇第 j 個行為的機率 )(tPij ，與第 i 個中繼站到第 j 個行為的路徑上的費洛蒙
濃度 )(tPhij ，關係如下式(3-1)： 



n
k
ikijij tPhtPhtP
1
)(/)()(                                         (3-1) 
    每次的決策循環結束後，將評估這次循環所執行的行為，對環境適應性的高低，並以此更新該路
徑上的費洛蒙濃度。費洛蒙更新如下式(3-2) 
)1(/)()1( stuckijij abtPhtPh                                 (3-2) 
b 為費洛蒙的衰減率， a為費洛蒙的加強率， stuck 為困住的程度，困住的程度愈小，代表該行為愈
能適應環境。愈大的b 與 a會造成現在的現在的狀態(困住的程度)對費洛蒙更新的影響愈大。然而，
愈小b 與 a卻造成歷史經驗(過去的費洛蒙濃度)對費洛蒙更新的影響愈大。為了兼顧從歷史經驗學
習，以及適應目前狀態的變化，b 與 a的值需加以權衡，在此選擇b 為 2.31， a為 100。 
 
3.3. 以序列訓練改善隨機選擇行為的缺點 
由於螞蟻演算法的隨機選擇特性，且如上頁圖 3-3 所示，總共有 10(5 個傾斜角分區2 個候選行
為)條費洛蒙路線需要進行最佳化，龐大的搜索空間會造成機器人需要花很多時間進行最佳解的搜
索，而導致克服時間過長。以圖 4-2(a)為例，在第一次訓練時，有將近 30 秒、超過 30 秒，甚至於將
近 80 秒的台階克服時間分布，這些克服時間都嫌過長。觀察機器人克服台階的過程，隨機的選擇的
特性會讓機器人陷於多次反覆下圖 3-4 至圖 3-7 的姿態，困住好一段時間後，才能擺脫困境。 
                   
Initialize Pheromone 
Trails
Choose Behavior Output 
Probabilistically 
According to Pheromone 
Update Pheromone Trails 
by Motor Torques
Stuck over 10 Seconds?
No
Yes
Sequential Training
Sequential Training Over?
Yes
no
Stuck?
Yes
No
 
圖 3-8 螞蟻演算法行為協調加入序列訓練後的流程圖 
 
3.4. 以費洛蒙調整改善學習反效果問題 
如下圖 3-9 所示，以螞蟻演算法進行行為協調時，當機器人困住的時候， 
才會執行行為，接著也才會去執行費洛蒙。但若機器人沒困住，則不會更新費洛蒙。 
 
圖 3-9 更新費洛蒙流程圖 
 
 
 如此一來，會造成如下圖 3-10 的現象。當機器人經由長時間的詴誤，終於擺脫困住的情況後，
僅在由困住變為未困住的那一瞬間，費洛蒙才會增加，無法達到像初始狀態那樣高的濃度。 
No 
Execute a behavior 
probabilistically 
Update 
pheromone Stuck? 
Execute a behavior
probabilistically
Update pheromone
Yes
No
Stuck?
Not stuck for 
over 5 seconds
Adjust pheromone
No
Yes
 
圖 3-1 螞蟻演算法結合費洛蒙調整流程圖 
 
 
4. 實驗結果與討論 
本研究已完成了串結式機器人的克服地形控制策略，並透過實驗驗證策略的 
可行性，分別說明如下： 
 
4.1. 兩節機器人實驗 
4.1.1. 台階地形實驗 
兩節串結式機器人的克服台階地形過程如下圖 4-1(a)(b)(c)(d)所示。 
       
 圖 4-1(a)         圖 4-1(b)   圖 4-1(c)    圖 4-1(d) 
 利用螞蟻演算法訓練兩節串結式機器人爬台階，每次實驗訓練了四次，每次訓練都是以上次訓練
所得的費洛蒙進行控制。訓練完四次後，再將訓練結果忘掉，重啟新的實驗，共 10 次實驗。下頁圖 
4.2(a)(b)分別為未加序列訓練，與加了提出的序列訓練後的實驗結果。 
With  
sequential training 
4.35 2.49  6.23  4.28 
    
 原始螞蟻演算法，與加上費洛蒙調整後的實驗結果分別如下圖 4-3(a)(b)所示。 
1 2 3 4
0
10
20
30
40
50
60
70
80
Run
C
o
n
q
u
e
ri
n
g
 T
im
e
 (
s
)
C
o
n
q
u
e
ri
n
g
 T
im
e
 (
s
)
1 2 3 4
0
10
20
30
40
50
60
70
80
Run
C
o
n
q
u
e
ri
n
g
 T
im
e
 (
s
)
C
o
n
q
u
e
ri
n
g
 T
im
e
 (
s
)
 
  圖 4-3(a) 未加費洛蒙調整    圖 4-3(b) 加了費洛蒙調整 
 
表 4-3 台階克服時間中位數(標準差) 單位：秒 
  1st run 2nd run 3rd run 4th run 
Without  
pheromone adjustment 
22 (19.01) 10 (2.65) 11 (10.55) 12 (7.34) 
With  
pheromone adjustment 
22 (9.05) 12 (4.53) 14 (3.44) 13.5 (2.73) 
 
 由上圖 4-3(a)(b)比較可發現，加了費洛蒙調整機制後，第三、四次訓練的克服時間都不會比第
二次訓練的克服時間分散，有效抑制學習反效果的問題。由表 4-3 也可發現，加入費洛蒙調整後，第
三、四次訓練的克服時間標準差，皆不會大於第二次訓練。同時，也由於第三、四次訓練的克服時間
分布並不會較第二次分散，第三、四次訓練的克服時間標準差與中位數差不多。 
 
 
 
 
 
Average 
Median 
        
圖 4-5(a)困住，嘗詴往上抬，依然困住 
 
     
   圖 4-5(b)改為往下壓，得以克服 
      
圖 4-5(c)先前的往下壓姿態，導致困住 
     
   圖 4-5(d)頭部拉平，以克服困境 
        
           圖 4-5(e) 成功通過 
 
 
5. 結論 
根據目前的研究成果做出以下兩點結論： 
 
1. 以串結式機器人單節之間的合作協調克服複雜地形： 
提出串結式機器人的串結式多單元分散式控制，透過 master 與 slave 角色的合作：master 的攀爬
豎板(riser climbing)與登上踏板(tread landing)行為、slave 遵從 master 的命令，得以克服複雜地形。雖
然目前實現在三節串結式機器人上，只需兩種角色，但未來機器人擴充為更多節時，可增添更多角色，
具有良好的擴充性。 
2. 透過學習，增進機器人克服地形的效率： 
引入螞蟻演算法，賦予機器人自主學習能力，藉由運動過程中的詴誤，自主 
調整策略，增進機器人克服地形的效率。在台階三種地形上，兩節串結式機器人實驗結果可得知，經
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
參與 2010 年 TI DSP 競賽，獲得佳作。 
 
近年各子計畫成果如下： 
 
子計畫一：期刊論文三篇，會議論文三篇。 
子計畫二：期刊論文四篇，會議論文二篇 
子計畫三：期刊論文二篇，會議論文二篇 
子計畫四；會議論文三篇 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
