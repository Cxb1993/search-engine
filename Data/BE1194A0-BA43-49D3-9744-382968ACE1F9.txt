ii 
中文摘要 
多重協定標籤交換(Multi-Protocol Label Switching, MPLS) 是一種先進的封
包傳遞技術。它具有第二層之高效能  (performance) 與訊務管理  (traffic 
management)的優點，以及第三層之可擴充性 (scalability) 與彈性 (flexibility)等
特性，我們可以預見 MPLS 將會是次世代網路的主流。 
本子計畫研究在多重協定標籤交換網路的環境下，實現快速繞路機制、服務
品質控制、以及虛擬私有網路的相關議題。我們以 IXP 2400 網路處理器為開發
平台，以兩年的時間完成上述題目的研究、設計與實作，建構出一個以 MPLS
技術為基礎的次世代網路基礎架構雛型，用以驗證 MPLS 在資料傳送、訊務工
程、服務品質保證、實現虛擬私有網路的優點，並提供子計畫二、三、四實驗的
平台，次世代的嶄新服務也得以得到驗證的機會。 
實作的部份分別於資料平面(Data Plane)以及控制平面(Control Plane)兩方面
進行，並利用 CP-PDK (Contrl Plane Product Development Kit)的技術將資料平面
與控制平面的訊息加以同步並整合。在資料平面方面，以 Intel IXP（Internet 
eXchange Processor） 2400 高階網路處理器為平台，以 Intel IXA （Internet 
eXchange Architecture）的軟體開發框架為基礎，實作完整的支援 DiffServ 和
TE-FRR 的 MPLS 邊界/核心路由器之資料平面；在控制平面方面，除了移植 TE
的路由協定 OSPF-TE 和 BGP 之外，為了實現 DiffServ，LDP (Label Distribution 
Protocol) 必須對 PHB / FEC signaling 予以擴充。而 RSVP-TE 延伸自 RSVP
（Resource Reservation Protocol），支援 Explicit routes 與資源保留的功能，是作
為建立具 QoS 保證與支援 TE 之 MPLS LSP 的重要 signaling protocol。在 MPLS 
VPN（VPLS）的部分，核心網路扮演一個 bridge 的角色，將同一 VPN 群組的
PE (Provider Edge)路由器全部橋接起來，提供相同 VPN 成員處於一個模擬的
LAN 環境之下。我們基於 Quagga 的控制平面架構，設計並實作 VPN Manager
並且於 PC 環境下實現 VPLS 資料平面橋接的功能。 
各層級的實作分別透過不同的實驗來驗證系統的功能，測試與評估系統的效
能。 
 
關鍵詞：多重協定標籤交換、訊務工程、服務品質、虛擬私有網路 
iv 
目錄 
中文摘要…………………………………………………………………..…………ii 
Abstract………………………………………………………………………………iii 
第 1 章 緣由與目的..........................................................................................1 
第 2 章 成果與討論..........................................................................................2 
2.1 支援 Fast Reroute、QoS Control 之 MPLS 路由器 ..................................2 
2.1.1 完整系統架構概觀..........................................................................2 
2.1.2 控制平面（Control Plane）概述 ...................................................3 
2.1.2.1 對 Quagga 軟體的擴充 ........................................................3 
2.1.2.2 Quagga 擴充軟體的實驗環境與功能驗證 ...........................4 
2.1.3 資料平面（Data Plane） ..............................................................14 
2.1.3.1 系統架構與處理流程..........................................................14 
2.1.3.2 資料平面功能驗證與效能測試..........................................19 
2.1.4 路由表同步更新機制的設計與實作(CP-PDK)...........................31 
2.1.4.1 設計原由與架構概述..........................................................31 
2.1.4.2 CP-PDK 功能驗證 ...............................................................43 
2.2 虛擬私有區域網路服務控制平面模組整合與其橋接器的實現............54 
2.2.1 系統架構的演進............................................................................54 
2.2.2 系統架構的改良............................................................................56 
2.2.3 VPLS Service 內部網路連線建立流程........................................58 
2.2.4 VPLS 服務功能驗證....................................................................59 
第 3 章 成果自評............................................................................................70 
參考文獻……………………………………………………………………………..71 
vi 
圖 2.1.3-18 建立 MPLS NHLFE Set 之指令使用方式...........................27 
圖 2.1.3-19 尚未發生 Link Failure 時帶有 Label 40 的封包 .................27 
圖 2.1.3-20 發生 Link Failure 後實施 FRR Swap-push 的封包 .............27 
圖 2.1.3-21 One-to-one transmission throughput vs packet size ..............28 
圖 2.1.3-22 DRR experiment 1 之實驗環境 ............................................29 
圖 2.1.3-23 DRR Scheduling result #1 .....................................................29 
圖 2.1.3-24 DRR experiment 2 之實驗環境 ............................................30 
圖 2.1.3-25 等量頻寬的流量結果 ...........................................................30 
圖 2.1.3-26 DRR Scheduling result #2 .....................................................30 
圖 2.1.4-1 Remote 架構之細節..............................................................31 
圖 2.1.4-2  VIDD Controller 之功能示意圖............................................32 
圖 2.1.4-3  Virtual Interfaces 示意圖 .......................................................33 
圖 2.1.4-4  路由封包在 CP 與 FP 之間的傳送流程概觀 ......................34 
圖 2.1.4-5 外來路由封包的傳送流程圖 ...............................................35 
圖 2.1.4-6 傳送自身的路由封包流程圖 ...............................................36 
圖 2.1.4-7 位於 Remote 端與 Local 端的路由表位置示意圖..............37 
圖 2.1.4-8 Remote 端的路由表...............................................................37 
圖 2.1.4-9 IXDP 2400 中的路由表 .........................................................38 
圖 2.1.4-10 IXDP 2400 路由表同步更新機制運作的三個步驟 ..........39 
圖 2.1.4-11 Listen On Kernel FIB Update...............................................40 
圖 2.1.4-12 Validate the RouteChange ....................................................41 
圖 2.1.4-13 RTSM 之程式流程圖...........................................................42 
圖 2.1.4-14 實驗環境架構 .....................................................................43 
圖 2.1.4-15 啟動 Remote 端上的 deamons............................................44 
圖 2.1.4-16 IXDP2400 中 Egress 端的啟動 ...........................................44 
圖 2.1.4-17 IXDP 2400 中 Ingress 端的啟動 .........................................45 
圖 2.1.4-18 Resource Manager 的啟動 ...................................................46 
圖 2.1.4-19 CPPUI 的啟動 ......................................................................46 
圖 2.1.4-20 透過 CPPUI 啟動 Gigaport 的結果....................................47 
圖 2.1.4-21 Virtual Interface 的啟動 .......................................................48 
圖 2.1.4-22 啟動 IXDP 2400 路由表同步更新機制 ............................48 
圖 2.1.4-23 分析目前的路由表 .............................................................49 
圖 2.1.4-24 未啟動 OSPF 時  Remote 端的路由表 ............................49 
圖 2.1.4-25 未啟動 OSPF 時  Local 端的路由表................................50 
圖 2.1.4-26 未啟動 OSPF 時  Router 1 上的路由表...........................50 
圖 2.1.4-27 啟動 OSPF daemon .............................................................51 
圖 2.1.4-28 啟動 OSPF 後  Remote 端的路由表 ................................51 
圖 2.1.4-29 啟動 OSPF 後  Local 端的路由表....................................52 
1 
第1章 緣由與目的 
自從一九九一年全球資訊網(WWW)推出，促使 Internet 商用化之後，Internet
便以驚人的速度發展。然而隨著 Internet 的蓬勃發展以及使用人數增加，雖然整
體頻寬不斷的提昇，現有 Internet 卻一直無法如專線網路般提供服務品質保證，
這使得希望透過 Internet 來傳輸語音、影像等多媒體的服務無法配合成長。究其
原因，現有 Internet 的架構只能提供 Best-Effort 服務實為主要因素。因此，讓
Internet 逐漸 Migrate 到一個能提供服務品質保證(QoS)的基礎架構是次世代網路
不得不的選擇。 
現有 Internet 採用 IP Datagram 的傳送方式，若要提供可靠的服務品質，必須
引進 Virtual Circuit 的技術，利用 Signaling Protocol 提供 Connection-oriented 的
End-to-end 傳送路徑。ATM 是一個採用 Virtual Circuit 的成熟技術，因此結合 ATM
技術與網際網路協定 (IP Protocol) 之方法紛紛被提出，從 LAN Emulation、
Classical IP Over ATM、 Ipsilon 的 IP Switch、Cisco 的 Tag Switch、IBM 的 ARIS、
ATM Forum 的 MPOA (Multi-Protocol over ATM)，一直到 IETF 制定的 MPLS 
(Multi-Protocol Label Switching) [1]相繼出現。其中 MPLS 整合了 Cisco 的 Tag 
Switching 與 IBM ARIS 的技術，並有世界等大廠所支持，成為各界最為看好的
標準，且目前已逐步在世界各地佈建中。 
MPLS 有著許多優點，如簡易的轉送機制 (Simplified forwarding)、有效地指
定路由 (Efficient explicit routing)、容易作流量分配工程 (Traffic engineering)、具
服務品質的路由 (QoS routing)、簡易網路管理 (Easier management)，尤其在 IPv6
出現後，它與 IP 會整合得更好，非常適合未來 B3G (Beyond 3G) 的核心網路。
要有效地達到 IP 網路上的多媒體傳送服務 (Voice over IP、Video over IP)、VPN、
Differentiated service、Multi-service 各種 QoS 的要求，MPLS 是最好的解決方法，
這也是 MPLS 可以成為次世代寬頻網路主流的重要因素。 
在過去三年我們曾經執行電信國家型計畫:”以應用服務品質為導向之差別式
MPLS 技術實作和研究”，在實作 MPLS Router 已累積不少珍貴的經驗。在 Intel 
IXP 1200 的網路處理器平台上架設了 MPLS Control Plane 和 Data Plane 所有基本
的元件，並且完成了 Topology Driven 的 LSP 建立方式和 Best-Effort 的 MPLS 功
能測試。本子計畫將以此 MPLS Router 雛型為基礎，延伸至 Network Processor IXP 
2400 平台以增加下列三大項功能: (1) Fast Reroute、(2) QoS Control，設計 
Classification、DiffServ Scheduling 的機制，驗證 MPLS 的相關控制機制與協定，
(3) 實現 VPN/VPLS 的架構，提供其他子計畫利用它來驗證相關的演算法與協
定，並實驗各種應用服務。未來 GMPLS over Optical Network [2]技術成為主流，
我們發展出的技術可以應用在骨幹設備上，將可提供使用者更快更好的服務。 
 
3 
2.1.2 控制平面（Control Plane）概述 
 Control Plane 負責 Routing 和 Signaling 等功能。圖 2.1.1-1 中 OSPF-TE [20] 
daemon 是具備 Traffic Engineering 能力的改良版路由協定，延伸自 Open Shortest 
Path First；當中採用了 CSPF（Constraint-based Shortest Path First）[2]路由演算法
來計算符合 TE 的路由。除了 OSPF-TE，完整的路由系統還包括了 RIP（Routing 
Information Protocol）與 BGP（Border Gateway Protocol）等協定[17]。 
 MPLS 是建立在 IP 網路的虛擬線路交換技術，在使用 MPLS 進行傳輸之前，
必須先透過 signaling protocol 來建立 LSP (Label Switched Path)；LDP（Label 
Distribution Protocol）就是專為 MPLS 而設計的最基本的 LSP signaling protocol，
其原理就是LSRs (Label Switching Router)之間會將LSP使用到的 incoming label / 
outgoing label 與 FEC (Forwarding Equivalence Class)之對應關係往上游或下游逐
級散播。為了實現 DiffServ，LDP 必須對 PHB (Per Hop Behavior) / FEC signaling
予以擴充。RSVP-TE 延伸自 RSVP（Resource Reservation Protocol），支援 Explicit 
routes 與資源保留的功能，是作為建立具 QoS 保證與支援 TE 之 MPLS LSP 的重
要 signaling protocol。 
 Zebra與MPLSd的目的，是為了在PC based的環境提供 control plane software
抽象和一致的底層介面，來對應到 IP與MPLS的實體資料平面。整個 control plane 
protocol software 建立於 Quagga 架構，使用到統一的系統 APIs；當中 VTYSH 是
內部 daemons 之間以及與 CLI (Command Line Interface)之間的標準通訊介面。更
多的資訊可參考[21]。 
2.1.2.1 對 Quagga 軟體的擴充 
Quagga 是一套擁有相當完整路由協定的軟體，但是它對於 traffic engineering
功能的擴充並不完備，如圖 2.1.2-1 所示，在 Quagga 之中包含一個鏈結狀態資料
庫（Link State Database）、路由計算（Routing calculator）、最短路徑優先路由表
（SPF routing table）和不完整的 TE Database，假如我們要在 Quagga 實現 CSPF，
我們必須擴充 Quagga。我們增加了兩個新功能分別是 local interface address 和
remote interface address [20]的設定，使得我們可以藉由 vty command 在 OSPF-TE 
node之下手動輸入 local interface address 和 remote interface address 的資訊以更新
TE Database 內容，並且新增兩個頻寬管理的功能: reduce unreserved bandwidth 和
increase unreserved bandwidth，讓我們可以手動增加或減少 unreserved bandwidth
的值。 
 
5 
 
圖 2.1.2-3 Traffic engineering 實驗環境 
在一開始我們比較在 NHP-CSPF 演算法尚未加入之前，LSP 的建立情況，
首先，我們利用新增至 Quagga 的 RSVP Daemon，使用 VTY socket 切換至 RSVP 
Daemon，如圖 2.1.2-4 所示，在 R1 下達建立 LSP 指令，目的端為 R4 且 LSP_ID
設定為 10。 
 
圖 2.1.2-4 CSPF 尚未加入的 LSP 建立 
在指令下達完成之後，我們於 R2 使用 Ethereal 封包檢視軟體抓取封包，證
實 LSP 的建立確實根據最短路徑優先（SPF）演算法，選擇一條最短的路徑，如
圖 2.1.2-5，圖 2.1.2-6 為 LSP 建立說明。 
7 
 
圖 2.1.2-7 最短路徑樹 
在啟用 NHP-CSPF 演算法和 OSPF-TE 的功能之後（TE-Class[i] UB 皆為
1.25M bytes），我們驗證 NHP-CSPF 是否有選擇較佳頻寬路徑的能力，為達到此
目的，撰寫了一支測試程式，檔名為 QoS_LSP（如圖 2.1.2-8），它的目的是建立
R1ÆR2ÆR4 的明確路徑並指定要求頻寬為 700k bytes，class type 為 0。 
 
 
圖 2.1.2-8 QoS_LSP 測試程式 
程式執行完成之後，我們於 R1 抓取封包觀察啟用的明確路徑功能是否正
常，如圖 2.1.2-9 所示，在 PATH massage 中的 EXPLICIT OUTE Object 確實攜帶
明確路徑資訊。 
9 
 
圖 2.1.2-11 R2-R4 Link 的頻寬變化情況 
執行 QoS_LSP 的另一個主要目的是為了將 UB（Unreserved Bandwidth）變
小，接下來我們將會加入 NHP-CSPF module，並且再一次要求 700K bytes 的頻
寬，class type 為 0。我們啟動 CSPF module（圖 2.1.2-12）和 SLA 程式（圖 2.1.2-13），
在 SLA 程式執行之後我們輸入目的端 R2 的位址以及要求 class type 為 0，頻寬
為 700K bytes 的資訊，CSPF module 隨即算出一條明確路徑，並傳回給 SLA，
SLA 將明確路徑和 admission control 映射（mapping）結果傳給 RSVP-TE 去建立
攜帶 traffic 參數和訊務工程的 LSP。我們可以發現，由於 R1ÆR2 的 Outgoing link 
TE-Class[0]和 TE-Class[1]的 UB 已經不足 700K bytes (剩下 550K bytes)，於是
CSPF module 選擇 R1 到 R3 的 Outgoing link，最後到達 R2 （如圖 2.1.2-14 實線
部分）。 
11 
192.168.4.254
192.168.4.254
192.168.5.254
192.168.2.76
192.168.3.49
192.168.2.95
192.168.4.76 192.168.3.76
192.168.2.76
192.168.3.49
192.168.5.49
192.168.2.95
R1
R2
R3
R4
 
圖 2.1.2-14 LSP 建立示意圖 
同樣的我們從 R1 抓取封包，觀察 PATH message EXPLICIT ROUTE Object
填入情況，如圖 2.1.2-15 所示，明確路徑為 R1ÆR3ÆR2。 
13 
 
圖 2.1.2-17 R3 到 R2 的 Outgoing link 頻寬變動情況 
 我們擴充了 OSPF-TE 的功能讓 NHP-CSPF module 得以順利實現，在 SLA
下達相關指令之後，由 NHP-CSPF module 所得到的明確路徑交由 RSVP-TE 開的
Explicit route 功能指定所要經過的網路節點，並經由 RSVP-TE 執行允入控制的
設計，達到資源控管的能力。最後，在 LSP 建立的同時，我們將允入控制所映
射到的結果，通知 OSPF-TE 執行相關資源變動的動作，完成了初步的整合架構，
並使得 TE 的能力更具完善。 
15 
flow。Dispatch loop 一般情況下以無窮廻圈為基礎，以 source block 和 sink block
作為廻圈的起始與末端，用以從/往外部接收/傳遞封包。為了簡化呈現，圖 2.1.3-1
只有 DiffServ Ingress/Egress Pipeline 標明了 source 跟 sink。Dispatch loop 會把經
常存取的變數與封包標頭 cache 在暫存器或 local memory，稱之為 dispatch loop 
variables 及 header cache。 
 圖 2.1.3-1 中，Ethernet RX、Ingress Scheduler、Ingress QM、CSIX TX、CSIX 
RX 是針對硬體設計而毋須作任何更動的 Driver Block [9]。Classifier、Meter、
WRED 則是專為 DiffServ 而設計的 building blocks。Ethernet ARP 負責從 L2 table
取得進行 Ethernet 封裝所需要的 next hop L2 資訊，若所需要的 L2 entry 不存在
或不合法，它會負責通知 CC 進行 ARP；否則它會將 L2 資訊傳遞到 Ethernet TX，
由TX負責真正的L2封裝工作。Ethernet TX也是Driver Blocks之一，然而Ethernet 
TX 在設計上卻沒有對支援 MPLS over Ethernet 作出考量，它會硬性地在封裝時
把 Ethernet Frame Type 欄位設定成 0x0800 （即 IP）；因此本計畫必須對 Ethernet 
TX 予以擴充，讓它能動態依照封包的類型填入適當的 Frame Type。 
Egress QM 和 Egress Scheduler 為了配合達成 DiffServ 的功能目的必須作出
延伸：1) 為了支援 EF class，Egress Scheduler 必須加入 Strict Priority（SP）成為
WRR/SP/DRR 三階層式排程器[23]。2) 為了配合 WRED 的計算，Egress QM 必
須能提供對於一個佇列之 up-to-date packet count 和 last idle timestamp 作為計算
average queue length 與 drop probability 的參數。3) 為了避免 DRR 演算法對 empty 
queue 進行操作，Egress QM 必須能把 queue length 告訴 Egress Scheduler，而
Scheduler 亦必須有能力接收這個訊息並作出適當的處理。 
MPLS FTN 在這個架構裏除了原本的任務外，還擔任了 DSCP Marker 的角
色，它必須能支援 E-LSP 與 L-LSP 對各種 outgoing PHB 之 marking 能力；而 ILM
亦必須額外對 incoming PHB determination 作出支援。此外，FTN 與 ILM 必須具
備機制與方法來進行 NHLFE 之間的切換，以實現對 TE-FRR 之支援。因此，針
對 MPLS 支援 DiffServ 與 TE-FRR 所需求的功能，以延伸/擴充的方式實作於 FTN
與 ILM 之內。 
17 
Preserved space for newly 
Imposed MPLS label
IPv4 header
Portion of L4 header
031
 
圖 2.1.3-3 Packet header cache allocation 
Dst MAC
Src MAC
Frame 
Type
IPv4 header
L4 header
L2 decap and byte align
 
圖 2.1.3-4 Byte-alignment for packet header caching 
¾ L2 classify：從 L2 header 裏 Frame Type 欄位擷取出封包類型。本計畫
把 incoming packet 分成兩類：IPv4 與 MPLS Labeled packet。L2 classifier
會把封包類型對應到預設的 MB，這個對應是利用設定 dispatch loop 內
的 dl_next_block 變數來完成。若封包是 MPLS 的，則 dl_next_block 會
被設定成 MPLS_ILM，告訴系統接下來以 ILM 進行高速交換；這意味
著封包正在網路核心傳遞中。而當封包為 IPv4 時，dl_next_block 會被
設定成 6-tuple classifier，目的是因為本架構可扮演 LSR 或 LER，單純
的 IP 封包無法讓路由器知道其角色，它只利用 classifier 嘗試替封包找
出所屬的 PHB；事實上只要不設定 rule，LSR 便不會確實執行 classifier。 
¾ ILM：當 ILM 收到 MPLS 封包，會以 incoming label 作為 index 來查找
NHLFE，若 LSP 受到備援路徑保護，ILM 會先參考 TE-FRR 資料結構
決定是否切換到 backup tunnel；然後根據 LSP 的 tunnel 型態，完成
incoming PHB determination；最後根據 NHLFE 進行封包轉送。從 ILM
離開的封包也分成 IP 與 MPLS，前者會再進行 IP forward，後者則直接
前往 Egress 作進一步處理。 
¾ 6-tuple classifier：利用封包的 L3/L4 標頭資訊作為依據對封包進行分
類，classifier 會從 packet header cache 取出 IP 標頭的 Source IP address、
Destination IP address、ToS、Protocol Type，以及 L4 標頭的 Source Port
與 Destination Port 共 6 個欄位，以 hashing 方式查找 Classification rule，
19 
進行排程，排程方式是以 Round-robin 為基礎挑選出一個 Queue 的 HOL
（Head of Line）封包，並透過 Scratch ring 向 QM 發出對這個封包
Dequeue 的請求。QM 對這個封包作出 dequeue 後，這個封包的 handle
會透過 Scratch ring 傳遞給 TX。 
圖 2.1.3-5 所示為 Data Plane Slow Path 的系統結構，Slow path 主要是由
CCs、System Application，及 user-space 的 configuration utilities 所組成。主
要負責建立、設定、協調與維護網路處理器的運作；此外，藉由繫結不同的
Building Blocks（如 CCs），Slow path 還負責次層級（如 Exception packet）
的封包處理。本計畫架構中，Slow path 處理來自 Ethernet Rx、MPLS，以及
IPv4 的 exception packet；另外還有針對 ARP 封包的處理；圖中把 ARP 封包
處理流程與其他 exception packet 區分開來獨立顯示。對於 exception packet
的處理，主要分成：1) Local delivery、2) IPv4 Fast Path Exception、3) MPLS 
Fast Path。為了避免造成系統複雜度過高，本架構並沒有對 exception packet
實施任何 QoS 保證的措施；因此，DiffServ building blocks 的 CCs 並不會負
責任何封包的處理。而任何將從 Slow path 離開而進入網路的 packet 都會經
由 QM CC 送往 Fast path 出口送出。 
 
圖 2.1.3-5 Slow-path 結構與 Exception Packet 處理流程 
更進一步，每一個 CC 事實上是為 Linux Kernel Module，在核心裏自成一個
核心執行緒。另方面，由於每個 CC 在初始化時，本身都必須向 CCI 註冊一個
message handler 以及一個以上的 packet handler，再加上 System Application 所建
立的繫結關係，CCs 之間以及 CCs 與 MBs 之間不但擁有完善的通信制度；還能
實現具效率的封包傳遞；Slow path 之所以能有效率地處理 exception packet 也是
因為這個原因。詳細的 CC 設計請見[9]。 
2.1.3.2 資料平面功能驗證與效能測試 
<功能驗證> 
21 
 
圖 2.1.3-8 Configuration utilities 之使用方法 
 首先是建立將會被使用到的 NHLFE，第一個參數指定了封包在傳
送出去前所需要經過之 Egress IXP 所屬的 blade，接著必須指定
output port；Next Hop ID 是本地端對 next hop 的識別，是用來取得
next hop 資訊（如 L2 info）的唯一指標。第 4 個參數是 push count，
讓 FTN 懂得要 push 多少層 label，最多四層；在這裏我們只做一層
tunnel，所以 push count 設為 1。接在 push count 之後的是 4 個會被
貼上的 labels，使用者必須以十六進制輸入 label 的數值。在這個實
驗我們會貼上一個 0x40 的 label 於 IP 標頭的前面。當這個設定被
執行後，會回傳一個 NHLFE handle 的值到螢幕上，若要使用到該
筆 NHLFE，則必須以此 handle 作為指標方能完成；因此，我們必
須記下來。本實驗中，回傳值為 1000001，其中最高 2 位數代表了
blade id，而 00001 則代表了其於 NHLFE Table 的 index。 
 接下來我們必須為這個 NHLFE 設定 LSP 對應的方式，即 ILM 或
FEC。然而在本實驗裏所使用的 FEC 不是 IPv4 LPM，而是 6-tuple 
classification。為此，本計畫延伸了 MPLS CC 及其 user mode 
utility，其中針對 LspCreate 指令，新增了一個 QoS LSP type。欄位
Tunnel Model，值 0 代表 Pipe；1 代表 Uniform；2 代表不支援 PHP
的 Short Pipe；3 則是支援 PHP 的 Short Pipe。欄位 LSP Type 以 0
代表 Non-DiffServ LSP；2 代表 E-LSP；3 代表 L-LSP。NhlfeType
其值為 0 時指向一般 NHLFE；2 代表所指向的是 NHLFE Set；3
代表指向的 NHLFE 是為 Swap-push 備援 LSP。EncapPktType 代表
了所封裝的封包類型，當被封裝的封包為 IPv4 時，此值為 6；若
為 IPv6 則此值為 7。其他欄位將用來填補 NHLFE 尚未具備的資訊。 
 Add6TRule 是用來新增 6-tuple classification rule 的工具，當中又分
成 fwd 與 qos 兩個部分。前者是建立 rule 關於封包轉送方面的資
訊，如 next hop id 等；後者則是建立 rule 所代表的 QoS 等級。由
於本實驗不需要從 classification 取得 QoS rule，因此，我們只須下
達 fwd 的部分。值得注意的是 next hop id 必須填入由 NhlfeCreate
所回傳的 1000001 中的 00001；next hop id type 欄位決定了進行封
23 
fwd 規則所需要的 flow_id、class_id、以及 color_id。flow id、class 
id、color id事實上代表了完整的PHB；而 output id則是要讓 fast path
判斷封包在 classifier 之後是要通往 Meter（1）還是 Marker（2）。 
 最後使用 qosconfig 的 AddTcm 來建立一個關聯到 flow id 的 meter 
instance。參數 Meter No.並不代表 meter instance，而是用來指定
meter 引擎，本計畫架構中只有一個 meter 引擎。<COLOR>_DSCP
與<COLOR>_OUT 代表著計量結果三種顏色所對應的 outgoing 
DSCP 以及 fast path 對封包前途（繼續處理 1 / 直接丟棄 2）的決
定。Srtcm 代表使用 Single Rate meter；接著輸入 srtcm 所使用的參
數。在這裏，由於 SJphone 進行 call setup 時，SIP 封包是以小型的
爆衝形式進入到 IXP，於是我們粗糙地將 CIR 設定為 100，總 bucket 
size 為 600，希望藉此計量到封包的不同顏色（黃色與紅色），並將
結果印記到 MPLS Exp 上去以作簡單的驗證。 
圖 2.1.3-12 所示為封包被標成 AF23 的情況，由於 source 在送出的 SIP 
Invite 時造成了短暫且很大的爆衝，在 token 空乏情況下 Meter 計量出
紅色；由於 destination 不會作出回應，於是 Invite timeout，SIP Cancel
被傳送，在 EBS 尚有 token 的情況下，封包被標成 AF22，如圖 2.1.3-13
所示。 
 
圖 2.1.3-12 被標為 AF23 的封包代表著被計量成紅色 
25 
 
圖 2.1.3-15 Link Failure Detection 
27 
 
圖 2.1.3-18 建立 MPLS NHLFE Set 之指令使用方式 
 
圖 2.1.3-19 尚未發生 Link Failure 時帶有 Label 40 的封包 
 
圖 2.1.3-20 發生 Link Failure 後實施 FRR Swap-push 的封包 
29 
SmartBits
Destination Host
Ethereal packet capturing
IXP 2400
Exp = 5, 1Gbps
Exp = 1, 1Gbps
 
圖 2.1.3-22 DRR experiment 1 之實驗環境 
由於 Exp = 1 代表 AF11，而 Exp = 5 則代表 AF21；所以它們會被分配
到 Queue 8 與 9。為了看出 DRR 的效果，我們以 8：2 的比例分配 queue 
8 與 9 的頻寬。圖 2.1.3-23 所示為擷取與分析的結果流量圖，最上方的
曲線是總流量；0.3 秒以前上方第二條曲線是為 AF11 traffic 進入 PC 的
流量，而最下方的曲線則是 AF21 traffic 進入 PC 的流量；在這段期間
的結果顯示了 DRR 排程的效果。然而由於 Input rate 與 output rate 落差
之大，backlogged packets 令佇列溢滿進而影響排程的結果。 
 
圖 2.1.3-23 DRR Scheduling result #1 
實驗二分別以等量與不等量的頻寬分配的結果作比較，以求更進一步確
定 DRR 排程的正確性。圖 2.1.3-24 為此實驗的環境，這次我們輸入三
條 traffics 來競爭 output link 的頻寬。由於本架構中 E-LSP 無法攜帶兩
種 PSC，故本實驗採用 L-LSP 的方法；但為了方便討論，source traffics
會攜帶值為 1、5 和 3 的 Exps 作為標記。圖 2.1.3-25 所示為等量分配的
結果；圖 2.1.3-26 則是以 5：3：2 的比例來分配 output link 的頻寬，相
對等量分配結果，traffics 的流量存在了差異。觀察 0 ~ 0.4 秒這段期間，
輸出流量的頻寬比例的確維持在 5：3：2 的狀況。在沒有啟動 WRED
的狀況下，0.4 秒以後佇列滿溢，導致效果無法維持。 
31 
的封包。正常情況下，inter-packet time 約為 5109 −× 秒，而從 Link 
failure 發生、偵測，及進行 MPLS FRR 共花費了 0.374444 秒；相
對正常情況多了 0.374354 秒。假若以手動方式製造 link failure 而
造成的誤差佔當中 30%，則 Link failure detection 與 MPLS FRR 的
時間花費約為 0.262048 秒。在 line rate 的情況下，仍能保持低於半
秒的效果是可以接受的；然而，顯然尚有非常大的改善空間。 
2.1.4 路由表同步更新機制的設計與實作(CP-PDK) 
2.1.4.1 設計原由與架構概述 
Remote 端的 Virtual Interface 
在 Intel IXA SDK 4.1 所提供的 CP-PDK 裡，有一個很重要的設計，就是所
謂的 Virtual Interface。透過這個 Virtual Interface 的設計，可以使得 Control Plane
與 Forwarding Plane 完全無縫地(seamlessly)整合起來，而無須做任何額外的變
更，這也是整個 CP-PDK 架構最主要的精神所在。以下我們將介紹使用 Virtual 
Interface 的理由，以及 Virtual Interface 的概念與實現。 
需要 Virtual Interface 的原因 
在本計畫中所設計的 CP-PDK 架構，如圖 2.1.4-1 所示。 
IXDP2401
User Space
Remote Host
Fedora Linux 
(Remote)
Control Plane
MontaVista 
(Local)
 Forwarding Plane
Kernel Spaceixdev0
Network Port
Console Port
Gigaports
ixdev1 ixdev2 ixdev3
Kernel device
Routing 
Process
？ ？ ？
 
圖 2.1.4-1 Remote 架構之細節 
在這種架構之中，很明顯地可以看出整個 IXDP 2400 路由器的 Control Plane
是在 Fedora Linux 上運作；而 Forwarding Plane 則是在 MontaVista Linux 上面運
作。然而，為了方便表示，以下章節中的內容，我們將 Fedora Linux 稱為 Remote
33 
 
圖 2.1.4-3  Virtual Interfaces 示意圖 
路由封包在路由器中的處理流程 
在 IXDP 2400 路由器之中，所有封包的進出都是透過 Ethernet Gigaport (光纖
埠並不在本計畫的討論之中)。其中，每一個欲離開 Ethernet Gigaport 的封包都必
須先經由 Microengine Layer 的 Egress Module 來處理之後，再將封包送出；相對
的，每一個已進入 Ethernet Gigaport 的封包，首先都必須經由 Microengine Layer
的 Ingress Module 來處理。因此，針對路由封包，我們將分為接收外來的路由封
包與傳送自身的路由封包兩個部份來討論。圖 2.1.4-4 表示路由封包在 Control 
Plane 與 Forwarding Plane 之間的雙向傳送流程概觀。 
35 
Route Protocol Data IP hdr VIP hdr
(Meta data)
IP hdr
(IPPROTO_VIP)
Route Protocol Data IP hdr
Route Protocol Data IP hdr VIP hdr
(Meta data)
IP hdr
(IPPROTO_VIP)
Route Protocol Data IP hdr
VIP hdr
(Meta Data)
Route Protocol Data IP hdr
 
圖 2.1.4-5 外來路由封包的傳送流程圖 
當路由封包透過 IP 隧道到達了另一端的 TCP/IP Stack 之後，該 Stack 會判斷
IP 標頭中的協定類型欄位，若是設定為 IPPROTO_VIP 時，便會將封包送到同樣
也是位於 Kernel 中的 VIP 模組進行處理，在此同時也會將 IP 隧道的標頭拿掉。
VIP 模組收到封包之後，便負責拆解並且分析 VIP 標頭，判斷封包是來自底層的
哪一個 Gigaport，然後再將封包送往相對應的 Virtual Interface。最後，Control Plane
上的 Routing Process 便會針對這個來自 Virtual Interface 的封包進行處理。以上的
流程請參考圖 2.1.4-5 所示。 
傳送 Remote 端自身產生的路由封包 
在這一個小節中，我們將要討論的是由 Remote 端 Control Plane 上的 Routing 
Process 所產生的封包，經過了哪些處理流程之後，最後才能被 Microengine 層的
Egress Module 送出。原則上，將路由封包送出的流程和接收封包的過程剛好是
相反的。當 Routing Process 決定欲送出路由封包時，首先會透過相對應的 Virtual 
Interface 送到 Kernel 中的 VIP 模組，此模組會分析該封包是來自哪一個 Virtual 
Interface，並且將這個資訊包裝成一個 VIP 標頭，貼在原有的路由封包之前；接
著第二步則是在 VIP 標頭前面再貼上一個 IP-in-IP Tunnel (IP 隧道技術)的 IP 標
頭，此 IP 標頭的目的位址是位於 Local 端 Kernel 之中的 TCP/IP Stack 中的特定
位址；而協定類型(Protocol Type)的欄位則是同樣會被設定為 IPPROTO_VIP 
(105)。 
37 
 
圖 2.1.4-7 位於 Remote 端與 Local 端的路由表位置示意圖 
本計畫中所設計的機制必須要有能力偵測到 Remote 端路由表的改變，並且
將此變化的結果送到 Local 端的路由表中，使得兩端的路由表內容得以同步。有
了這個同步更新機制之後，使用新路由並且經由 Local 端進出的封包就能夠正確
無誤地被轉送。 
兩端路由表之差異 
在先前的章節裡，為了簡化表示與我們所傳達的觀念，我們將 Remote 端與
Local 端的路由表都看成是一樣的格式。然而，在意義上，兩端的路由表所扮演
的功能角色也的確是一樣的；只是在實際情況中，這兩個路由表是有一些格式上
的差異。我們將在這個小節說明 Remote 端路由表與 Local 端路由表在格式上的
差異，因為這個差異將會影響到我們設計路由同步更新機制時的考量。 
由於Remote採用的是 Fedora Linux，因此Remote端的路由表即為一般 Linux
系統路由表的格式，如圖 2.1.4-8 所示。由左而右，分別是 Destaination 位址、
Gateway 位址、Netmask、Flags、Metric、Ref、Use、Iface 等欄位。 
 
圖 2.1.4-8 Remote 端的路由表 
39 
IXDP 2400 路由表同步更新機制 
為了方便表示，我們將 IXDP 2400 路由表同步更新機制以英文縮寫 RTSM 
(Routing Tables Synchronization Mechanism)來表示，組成 IXDP 2400 路由表同步
更新機制的程式，則是簡稱為 RTSM 模組。RTSM 所包含的同步功能，實際上就
是將那些在 Remote 端路由表的產生變化的路由，在 Local 端的路由表中新增或
是刪除，也就是將 Remote 端路由表的變化同步變更到 Local 端的路由表中。概
觀來看，整個 RTSM 的運作可以分為三個步驟，如圖 2.1.4-10 所示，第一步是
必須隨時監聽 Remote 端中路由表是否有發生變化；第二步則是檢查此筆變化的
路由是否合格；第三步則是將合格的路由透過第三章所介紹的 CP-PDK 寫入到
Local 端中的路由表中，讓 Local 端的路由表在第一時間內就可以和 Remote 端的
路由表內容一致，達到所謂同步的目的。因此，以下我們將針對這三個步驟分別
討論。 
IXDP2401
User Space
Remote Host
Fedora Linux (Remote)
Control Plane
MontaVista 
(Local)
Forwarding Plane  Forwarding Plane
Kernel Spaceixdev0 ixdev1 ixdev2 ixdev3
Routing 
Process
vi1-0 vi1-1 vi1-2 vi1-3
Routing Table Routing Table
Synchronization
CPPDK CP 
module
CP-PDK
1
Listen
2
Check
3
Write
RTSM
 
圖 2.1.4-10 IXDP 2400 路由表同步更新機制運作的三個步驟 
監聽 Remote 端路由表的變化 
這個步驟的作法是利用 Linux Kernel 中的 netlink 在 FIB 中建立一個 RAW 
Socket，用來監聽 FIB 中的變化。當 FIB 中發生變化時，便會將此變化傳回到
RTSM 模組，其流程如圖 2.1.4-11 所示。 
41 
 
圖 2.1.4-12 Validate the Route Change 
IXDP 2400 路由表的寫入/刪除 
確認過該筆路由變化為合格的路由時，接著就是將這個新增/刪除路由的內
容傳送到 CP-PDK 模組中，透過 CP-PDK 中控制訊息的傳遞路徑，把訊息內容
寫入 Local 端的路由表中，也就是說，我們所設計的 RTSM 必須和 CP-PDK 相整
合，所以亦可將 RTSM 視為 CP-PDK Control Plane Module 的一部分。 
根據我們之前比較過兩端路由表的差異之後，可以發現在Local端中的Prefix 
Table 與 NextHopTable 都各有一個 NextHopID 的欄位，這是 IXDP 2400 所設計出
來獨有的欄位；但是在我們從 Remote 端 Kernel 中所監聽到的每一筆路由訊息，
實際上並不會有這個欄位的資訊，因此我們必須想辦法找尋這個 NextHopID 的
資訊。作法就是無論我們是手動或是自動新增 NexHopIP 與其 NextHopID 時，我
們會將這個對應關係存到 RTSM 中的 NextHopMap 中，這樣 RTSM 就可以透過
這個 NextHopMap 來找尋相對應的 NextHopID。 
圖 2.1.4-13 為整個 RTSM 的程式流程圖。程式一開始會將目前 Remote 端路
由表中的所有路由都 Dump 出來，接著一筆一筆去檢查是否有合格的路由需要新
增到 Local 端的路由表中。處理完畢之後，便會開始對 Remote 端 Kernel 中的 FIB
進行監聽的工作，一旦發現有新的路由變化，便會開始針對路由進行檢查，當
RTSM 模組確認新增/刪除的路由為合格路由後，下一步則是查詢 NextHopMap，
43 
2.1.4.2 CP-PDK 功能驗證 
如圖 2.1.4-14 所示，首先我們先將 IXDP 2400 上 Port0 與 Port1 的 NextHop
分別設為 10.0.0.2 以及 11.0.0.2；其中，前者與 Router 1 相連接，後者則與 Host 2
相接。其中，Remote 端所採用的作業系統為 Linux Fedora Core 2，Kernel 版本為
2.6.6；而 IXDP 2400 所掛載的作業系統為 Montavista Linux 3.1，其 Kernel 版本
為 2.4。另外，在 Remote 端的 Control Plane 所運作的路由應用程式為 quagga 
0.973，我們將執行 quagga 中的 OSPF daemon 來驗證路由表的更新與同步。 
 
圖 2.1.4-14 實驗環境架構 
當 IXDP 2400 與 Router 1 上面的 quagga 程式啟動並且交換 OSPF 的路由資
訊之後，IXDP 2400 就會學習到 Router 1 上的另一個 Interface (192.168.30.1)；而
同樣地，Router 1 亦會學習到 IXDP 2400 上的 Port1 (11.0.0.1)。最後，我們將利
用 ICMP Ping，由 Host 2 ping Host 1，藉此來驗證 IXDP 2400 上的路由表是否有
及時更新。 
 
步驟 
step 1. 啟動 IXDP 2400 路由器 
將各機器如圖 2.1.4-14 連接好之後，就可以開始準備啟動 IXDP 2400 了，
首先必須先將 Remote 端的一些 deamon 打開，像是 DHCP、NFS、TFTP 等等，
如圖 2.1.4-15 所示。並且將 vip.o 模組掛上，這樣之後才可以啟動 Remote 端的
Virtual Interface。 
45 
 
圖 2.1.4-17 IXDP 2400 中 Ingress 端的啟動 
然後再接著啟動 Forwarding Plane 上的 Resource Manager，這個元件負責提
供 Microblok 一些可用的 APIs。只要利用 telnet 登入 IXDP 2400 之中，執行
ForwardingPlane 這個執行檔即可，過程與結果如圖 2.1.4-18 所示。 
47 
然後將事先寫好的一些 Script 檔 Load 進去，這些 script 檔包含了要建立哪
些 Interface，這些 Ineterface 的屬性等等。將這些資訊透過 CPPUI 讀入 CP-PDK
之後，再將兩個 Gigaport 插上網路線，結果就會如圖 2.1.4-20 所表示， Port0
與 Port1 的顏色會有所改變，表示 CP-PDK 已經成功地偵測到這兩個 Port 的啟動
了  
 
圖 2.1.4-20 透過 CPPUI 啟動 Gigaport 的結果 
在此同時，位於 Remote 端的 Kernel 中，也會偵測到這兩個 Port 的啟動，進
而觸發 VIDD 模組新增兩個 Virtual Interface，我們在 Remote 端輸入 ifconfig 的指
令，就可以看到這兩個 Virtual Interface，分別是 vi1-0 與 vi1-1，結果如圖 2.1.4-21
所示。完成以上這些步驟之後，整個 IXDP 2400 路由器就算是成功地啟動了。 
49 
 
圖 2.1.4-23 分析目前的路由表 
step 3. 驗證路由表之同步更新 
首先執行 Remote 端上的 Routing Process，也就是 quagga 中的 OSPF daemon；
同樣地，我們也同時在與 IXDP 2400 路由器 Port0 相連接的 Router 1 上啟動 OSPF 
daemon。 
為了驗證路由表的同步，圖 2.1.4-24 與圖 2.1.4-25 分別表示了在 OSPF 
daemon 尚未啟動之前，Remote 端上的路由表與 Local 端上的路由表內容；圖 
2.1.4-26 則是表示了 Router 1 上的路由表。 
 
圖 2.1.4-24 未啟動 OSPF 時  Remote 端的路由表 
51 
 
圖 2.1.4-27 啟動 OSPF daemon 
此時我們再來觀察位於 Remote 端的路由表以及 Local 端的路由表與圖 
2.1.4-24、圖 2.1.4-25 所顯示尚未啟動 OSPF 前的路由表有何差異。首先，圖 
2.1.4-28 表示 Remote 端上的路由表，可以很清楚地看出經過 OSPF 路由訊息的交
換之後，IXDP 2400 路由器學習到了位於 Router 1 上的 192.168.30.1 這個介面，
只要看到 Prefix 為 192.168.30.0/24 的封包，就會將它往 10.0.0.1 這個位於 IXDP 
2400 上的 Nexthop 送。 
 
圖 2.1.4-28 啟動 OSPF 後  Remote 端的路由表 
接著來看 Local 端的路由表，如圖 2.1.4-29 所示，根據我們在之前所提到的
RTSM 程式流程圖，首先，RTSM 會先去尋找 10.0.0.2 這個 NextHopIP 是否有相
對應的 NextHopID，由於這是一開始，因此在 Local 端上的路由表並沒有這個
NextHopIP 與 NexrHopID 的對應。因此，接著 RTSM 就會先新增一個 IP 位址為
10.0.0.2 的 NextHopIP，並且將其相對應的 NextHopID 設為 1，然後再將這個對
應關係存到 NextHopMap 中，以供日後查找。新增完這個新的 NextHop 之後，
程式會再一次地回頭找尋 10.0.0.2 所對應 NextHopID，這次就會找到所對應的
NextHopID (1)；然後將 Prefix 192.168.30.0/24 撘配 NextHopID (1)透過 CP-PDK
寫進 Local 端的路由表中。 
53 
 
圖 2.1.4-30 啟動 OSPF 後  Router 1 上的路由表 
現在我們利用 ICMP Ping Test 來驗證，圖 2.1.4-31 顯示了由 Host 2 Ping Host 
1 呈現互通的情形，顯示 IXDP 2400 路由器中的路由表確實可以正常運作。 
 
圖 2.1.4-31 利用 ICMP Ping Test 驗證路由表的正確性 
確認新增路由到 IXDP 2400 路由器中沒有問題之後，我們再將 OSPF daemon
中斷，如圖 2.1.4-32 所示，可以發現所有的路由表又恢復為原本圖 2.1.4-24、圖 
2.1.4-25、圖 2.1.4-26 的樣子。證實本計畫中的 IXDP 2400 路由表同步更新機制
的確可以正常地自動同步新增/刪除路由。 
 
圖 2.1.4-32 中斷 OSPF daemon 
55 
 
圖 2.2.1-2 Quagga-LDP-MPLS 
在 VPLS 服務中 RSVP-TE 負責建立外層 Tunnel LSP，而 RSVP-TE 亦是使用
ioctl 方式去更新 NHLFE/ILM table 的資訊，但 RSVP-TE 是採用 mplsadm library
去對底層作更新的動作，如圖 2.2.1-3 所示。 
quagga library
Ospfd/6
IP / Routing Table
Zebra
BGPd
RIPd/ng
  MPLS Linux / 
NHLFE,ILM
LDPd
Vtysh
ioctl
zebra
connect
VTY 
connect
NetLink
rapid
RSVPd
Mplsadm 
libary
Kernel
User
 
圖 2.2.1-3 Quagga_v1+ MPLS Linux 
如圖 2.2.1-4 所示，便是我們所實現的 VPLS 軟體架構圖。除了加入 VPLS
所需的 Signaling Protocol 之外，仍須加入 VPLS Manager Modules 作為整體的運
作以及協調的行為。當然必須在 Linux 網路核心部分必須加入 bridge 的功能，
才能夠使得 Core Network 模擬成 bridge 的裝置。 
57 
 
圖 2.2.2-1 原先版本的 VPLS 系統架構圖 
上圖 2.2.2-1 所示，為原先版本的 VPLS 系統架構圖。我們列出六個項目比
較兩者之間的主要差異： 
z 尚未考慮與 VPLS 資料平面的互動機制。 
z VPNMd、RSVPd、mplsd 未整合進入 Quagga 系統架構中。 
z VPNMd、LDPd、RSVPd 之間的溝通方式藉由 network socket 來達成，會造
成系統額外的負載。 
z 缺少一個共通介面用來更新 MPLS Linux 的 NHLFE/ILM table，其所欠缺的
模組就是 mplsd。 
z 缺少 brctld 模組用來控制 VPLS bridge。 
z 缺少 Label 資源管理的有效機制，其所欠缺的就是 LMd 模組。 
針對上述六點的差異，首先必須將所欠缺的模組加入 Quagga 中，接續更新
並統一模組之間的溝通機制，最後由於原先版本所採用的 MPLS for Linux 有些許
的 Bugs 會使得 Linux 系統嚴重當機，故將 MPLS for Linux 的版本需更新為 1.950
的版本。 
59 
2.2.4 VPLS 服務功能驗證 
環境介紹 
我們利用三台 PC 來建置 Service Provider Core Network，並假設我們需要提
供 VPLS 的服務，在核心網路中建置外層的 Tunnel 以及對應的內層
Pseudo-Wires，最終目的讓核心網路模擬成為一個 bridge 的裝置，進而建置出
VPLS 技術。 
我們透過以下幾種方式來呈現其成果。 
 透過 VPN Manager 建構出 VPLS Service 
 透過 ethereal 軟體擷取封包資訊 
 藉由各個常駐程式來顯示 VPLS instance 的相關資訊 
 在相同的 VPN 群組的成員，藉由 ARP table 資訊來察看是否可獲得不同
地域的成員資訊 
在相同的 VPN 群組的成員，藉由 sshd 的程式連線到彼此。 
軟體環境介紹 
 Operation System 
 Fedora Core 2 kernel 2.6.5 
 Singling Protocol 
 LDP 0.800 
 RSVP 0.700 
 Routing Protocol Suite 
 Quagga 0.97.3 
 MPLS Support 
 MPLS for Linux 1.950 (kernel 2.6.15-rc5) 
 Manager Modules 
 VPN Manager 
 Label Manager 
 Bridge Control 
 Mplsd  
 
Network Topology  
61 
 
圖 2.2.4-3 PE1 的 VPN 成員資訊 
如圖 2.2.4-3 所示，由 PE1 送往 PE2 的封包並且屬於 VPN ID 為 100 的群組，
PE1 會貼上外層 Tunnel Label 為 600 的標籤及內層 Pseudo-Wire 為 200 的標籤。
收到由 PE2 送往 PE1 的封包並且屬於 VPN ID 為 100 群組，其外層 Tunnel Label
為 700 的標籤且內層的 Pseudo-Wire Label 為 100 的標籤。下圖 2.2.4-4 則在 PE2
上顯示的 VPN 成員資訊。 
 
圖 2.2.4-4 PE2 的 VPN 成員資訊 
brctld 顯示 VPLS Brdige 橋接介面卡的資訊 
圖 2.2.4-5 與圖 2.2.4-6 分別為 PE1 與 PE2 上的 VPLS bridge device 裝置資訊，
相同的 VPN id 其 VPLS bridge 名稱皆為相同，故 VPN id 為 100 其 VPLS bridge
名稱為 br100，其中 interface 欄位資訊則表示所橋接的介面，而 mpls100 便是由
Pseudo-Wire 對應到彼此 PE 的 virtual tunnel interface，另外 eth1 則表示 PE1 所連
接到 VPN 100 成員的 customer-facing port，而 eth3 則表示 PE2 所連接到 VPN 100
成員的 customer-facing port。 
 
圖 2.2.4-5 PE1 的 VPLS bridge 資訊 
 
圖 2.2.4-6 PE2 的 VPLS bridge 資訊 
mplsd 顯示 mpls router 底層的 NHLFE/ILM table 資訊 
我們透過 mplsd 常駐程式來顯示底層 NHLFE/ILM table 資訊，首先說明 PE1
的底層資訊，如圖 2.2.4-7 所示。其中 NHLFE table 資訊，0x02 的 nhlfe_key 之值
代表送往 P 路由器會貼上 700 的標籤，0x04 的 nhlfe_key 之值表示將內層 200 的
標籤堆疊於外層700的標籤。另外指明送往nexthop為mpls100的封包其nhlfe_key
的值為 0x03。對於 ILM table 資訊:將 700 的標籤移除後，如果內層標籤為 100
63 
 
圖 2.2.4-9 PE2 底層的 NHLFE/ILM table 資訊 
擷取封包內容資訊 
 
圖 2.2.4-10 PE1 收到的 ARP request 封包內容 
我們由 A 電腦 ping B 電腦的 IP 位址觀測到圖 2.2.4-10～圖 2.2.4-15 的封包
資訊。上圖 2.2.4-10 為 PE1 送往核心網路的 ARP request 封包，其外層 600 的標
籤內層為 200 的標籤，並把由 customer-facing port 所收到的 ARP 封包封裝成
payload，其目的端的 MAC 位址填入廣播位址。下圖 2.2.4-11 則為由核心網路收
到的 ARP reply 封包，其外層 700 的標籤值及內層 100 的標籤值，且來源端的
MAC 位址已填入 B 主機 00:14:2a:22:c4:da 的 MAC 位址。 
圖 2.2.4-12則為 PE2收到由核心網路送來的ARP request的封包而圖 2.2.4-13
則為 PE2 送至核心網路的 ARP reply 的封包，並且貼上對應的內外層 MPLS shim 
header。 
65 
的封包資訊。 
 
圖 2.2.4-14 PE1 收到的 ICMP request 封包內容 
 
圖 2.2.4-15 PE1 收到的 ICMP reply 封包內容 
VPLS bridge MAC Leanring Table 
圖 2.2.4-16 為 PE1 的 MAC Learning Table，其中 port 1 為 mpls100 介面，將
遠端 PE2 customer-facing port 所連接的 B 電腦的 MAC address 記錄在表格中。圖
2.2.4-17 則為 PE2 的 MAC Learning Table，其中 port 1 為 mpls100 介面亦有記錄
到遠端 PE1 customer-facing port 所連接的 A 電腦的 MAC address。 
 
圖 2.2.4-16 PE1 的 MAC Learnig Table 
67 
 
圖 2.2.4-22 PE2 收到 SSH v2 建立連線的封包內容 
如圖 2.2.4-22 所示，由 B 電腦透過 putty 軟體採用 SSHv2 加密連線到 A 電
腦，我們可以看到 A、B 兩端會建立起 TCP 連線，進而進入 SSH 連線動作。由
圖 2.2.4-22 得知其由 ingress PE 的 customer-facing port 所收的封包，再經過封裝
後成為 VPLS frame 傳送至正確的 egress PE，收到由核心網路傳來的 VPLS frame
後 egress PE 藉由 Pseudo-Wires 的值來決定送往哪一個 tunnel interface，在這之前
會先將 VPLS frame 還原成原本的 ingress PE 收到的 ethernet frame 再送至 tunnel 
interface 上，最後 VPLS bridge 裝置查找 FDB 並比對目的端的 MAC 位址後再決
定將封包由正確的 egress customer-facing port 送出。 
 
圖 2.2.4-23 驗證成果的 VPLS 網路拓樸 (二) 
如圖 2.2.4-23 所示，我們提供兩個 VPN 群組的服務，並利用同一個外層
Tunnel 來承載兩個 VPN 群組的 L2VPN 的資料流。主要驗證 PE 是否能透過不同
Pseudo-Wire 值將客戶端的封包轉送到正確的 customer-facing port。 
如圖 2.2.4-24 為 PE1 的 VPLS bridge 資訊，其中 VPN 100 成員藉由 br100 的
virtual bridge device 來管轄，以及將 customer-facing port (eth3)與對應 remote PE
的 mpls100 橋接起來，我們可以看到 br100 的 MAC learning table 中記錄著遠端
69 
 
圖 2.2.4-26 A 電腦的 ARP Table 
 
圖 2.2.4-27 D 電腦的 ARP Table 
71 
參考文獻 
[1] R. Braden, D. Clark, S. Shenker, Intergrated Services in the Internet 
Architecture: an Overview, RFC1633 
[2] H-J. Chao, X. Guo, Quality of Service Control in High-Speed Networks, 
John Wiley & Sons, 2002. 
[3] S. Blake, D. Black, M. Carlson, E. Davies, Z. Wang, W. Weiss, An 
Architecure for Differentiated Services, RFC2475, 1998. 
[4] E. Rosen, A. Viswanathan, R. Callon, Multiprotocol Label Switching 
Architecture, RFC3031, 2001. 
[5] F-L. Faucheur, W. Lai, Requirement for Support of Differentiated 
Services-aware MPLS Traffic Engineering, RFC3564, 2003 
[6] D. E. Comer, Network Systems Design using Network Processors, Prentice 
Hall, 2006. 
[7] B. Carlson, Intel Internet Exchange Architecture and Applications, Intell 
Press, 2003. 
[8] Intel IXA 4.1 Documentation: IXP2400 Hardware Manual. 
[9] Intel IXA 4.1 Documentation: Framework Developer Manual. 
[10] D. Grossman, New Terminology and Clarifications for DiffServ, RFC3260, 
2002. 
[11] A. Charny, J.C.R. Bennett, K. Benson, J.Y. Le Boudec, A. Chiu, W. 
Courtney, S. Davari, V. Firouiu, C. Kalmanek, K-K. Ramakrishnan, Supplemental 
Information for the New Definition of the EF PHB, RFC3247. 
[12] J. Heinanen, F. Baker, W. Weiss, J. Wroclawski, Assured Forwarding PHB 
Group, RFC2597. 
[13] S. Floyd, V. Jacobson, Random Early Detection Gateways for Congestion 
Avoidance, IEEE/ACM Transactions on Networking, 1993. 
[14] M. Shreedhar, G. Varghese, Efficient fair queueing using deficit round-robin, 
IEEE/ACM Transactions on Networking, 1996. 
[15] L. Anderson, P. Doolan, F. Feldman, A. Fredette, B Thomas, LDP（Label 
Distribution Protocol）Specification, RFC3036, 2001. 
[16] R. Braden, L. Zhang, S. Berson, S. Herzog, S. Jamin, Resource ReSerVation 
Protocol （RSVP）, RFC2205, 1997. 
[17] D-E. Comer, Internetworking with TCP/IP – Volume 1. Fourth Edtion, 
Prentice Hall, 2004. 
[18] F. Le Faucheur, L. Wu, B. Davie, S. Davari, P. Vannanen, R. Krishnan, P. 
Cheval, J. Heinance, MPLS support of Differentiated Services, RFC3270, 2002. 
[19] P. Agarwal, B. Akyol, Time-To-Live Processing in MPLS MPLS networks, 
