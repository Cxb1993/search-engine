 
中 文 摘 要 ： 性別是人類一項重要的特徵，一個自動且有效的性別辨識系
統，可以廣泛的應用在安全 
監控、人機介面或消費者行為分析等領域，例如在消費者行
為分析方面，將數位廣告看板加 
上性別辨識系統，根據不同的性別播放不同的廣告，以吸引
更多潛在的顧客。不同性別之間 
的人臉外觀有顯著的差異，因此利用人臉特徵來判斷性別是
最直覺的方法，近年來，許多利 
用人臉與五官特徵來辨識性別的方法相繼被提出，但是在真
實的環境中，路上的行人經常會 
有像是戴墨鏡或是圍圍巾的裝扮，這些人臉部分遮蔽的情形
可能導致辨識率下降，因此本計 
畫提出一個適應型的性別辨識方法，首先在遮蔽分類的階段
偵測出被遮蔽的五官區塊，然後 
只選擇未遮蔽的區塊來辨識性別，利用這種動態選擇五官區
塊的方式來解決因部分遮蔽所造 
成的誤判並且提升整體的辨識率。此外，我們並提出一個進
出口人數統計的系統，是利用樣 
版比對的方式，判斷進出的物體是否為行人。首先利用背景
相減法來找出前景物，接下來偵 
測部分，利用特徵比對的方式，辨識進出的行人，過濾其他
的物體。最後則是利用追蹤來判 
定行人進出的方向，並統計各個方向的行人總數。實驗結果
驗證本計畫所提方法之可行性。 
中文關鍵詞： 性別辨識、安全監控、人數計算、行人偵測、特徵比對 
英 文 摘 要 ： Gender is a very important personal attribute 
inherent in human beings. Hence, an automatic 
and effective gender recognition system is desirable 
in various applications, such as intelligent 
surveillance system, human-computer interaction, and 
customer behavior analysis. Take customer 
behavior analysis as an example, the applying of 
gender recognition technology in digital signage 
can attract potential customers while demonstrating 
custom advertisements. Since human face 
conveys a clear sexual dimorphism, the using of 
facial features seems to be an intuitive way for 
gender recognition. In consequence, abundant gender 
recognition methods were proposed based on 
certain facial components. However, people may wear 
 1 
行政院國家科學委員會補助專題研究計畫期末報告 
 
人數計算、性別年齡辨識與眼神注視偵測應用於
櫥窗廣告系統之關注度估測 
 
計畫類別：■個別型計畫  □ 整合型計畫 
計畫編號： NSC99－2221－E－008－069－MY3 
執行期間：2010年 8月 1日至 2013 年 7月 31日 
 
計畫主持人： 范國清 
共同主持人：  
計畫參與人員：  
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
處理方式：除產學合作研究計畫、提升產業技術及人才培育研究計畫、列
管計畫及下列情形者外，得立即公開查詢 
          □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
          
執行單位：國立中央大學資訊工程學系 
 
中   華   民   國  102  年  8  月   15   日 
 3 
二、緣由與目的 
性別是人類一項重要的特徵，在我們的日常生活中，經常利用性別資訊來統計或是過濾
人口，因此在圖形識別的研究發展上，一個自動且有效的性別辨識系統，可以廣泛的應用在
安全監控、人機介面與消費者行為分析等領域。在安全監控方面，透過具有性別辨識的門禁
系統，過濾某個性別才能進入的場所，例如在洗手間或更衣室外，如果系統判斷出有異性進
入，則發出警訊以避免意外的發生；在人機介面的部分，經常應用於個人化機器人，讓機器
人根據不同的性別而產生不同的反應。性別辨識在消費者行為分析方面更扮演了相當重要的
角色，在這數位化的時代，許多的公共場所，例如車站、街道、購物商場、學校、醫院等地
方，我們越來越常看到數位看板(Digital signage)播放著行銷廣告或是活動資訊，數位看板的
優點就是能夠即時的更新並傳達訊息，當運用在廣告的領域時，如果能將數位廣告看板加上
攝影機與性別辨識系統，在廣告播放的時候，同時記錄觀看者的性別資訊，將能有效分析消
費者行為，了解各種產品或服務所能吸引到的顧客群，並且進一步針對不同的顧客群推出不
同的廣告，吸引更多的顧客駐足觀賞，提升購買的意願，另外，像是銀行的自動櫃員機
(Automated teller machine, ATM)、超商的多媒體服務機(例如 7-ELEVEN 的 ibon、全家的
FamiPort)、百貨公司的導覽機等 KIOSK 系統，一定會有機台無人使用的時候，或是在操作時，
必須等待的系統處理時間，如果能充分利用這些時間，根據靠近或是正在使用 KIOSK 的顧客
之性別，來播放不同的廣告，將是一個有效的行銷方案，也能吸引更多人使用 KIOSK 系統，
間接提升了廣告的曝光率。人類的人臉特徵傳達了許多重要的資訊，像是個人身分、性別、
年齡、情緒或是種族，因此利用人臉來判斷性別是最常用也是最有效的方法，但是在真實的
環境中，路上的行人經常會有像是戴墨鏡或圍圍巾的裝扮，以人類視覺感知來判斷，當人臉
發生部分遮蔽的情形時，可能導致沒有足夠的特徵資訊可以辨識身分，但是仍然可以從其他
未遮蔽的外觀特徵去猜測性別，因此本論文提出一個以人臉五官特徵為基礎之適應型性別辨
識方法，將人臉切割成左眼、右眼、鼻子與嘴巴等四個區塊，結合性別分類與遮蔽判定，選
擇未遮蔽且可靠的五官區塊，並且適時的加入頭髮特徵來判斷性別。傳統計算進出人數的方
式，不外乎是使用人力：於出入口安排工作人員站崗，利用計數器方式，隨時對進場人員進
行人數之計算。其缺點為耗費大量的人力資源，不合乎經濟效益。另外一種採用機械計算的
方式，是在出入口設置旋轉欄杆，利用人通過時欄杆的旋轉，來驅動計數器。此種方法雖然
可以節省人力資源，但是因為硬體的設計，讓人在進出時的行走方向受到限制。隨著科技的
進步，近年來對於進出人數的計算，已改由感測器來偵測人體，例如紅外線熱感器。不論在
成本或是效果上，都有很大的改善。但是當多個人靠在一起通過熱感器時，就會發生被判定
為一個大的熱源，而無法很精確的偵測出正確的人數。基於上述的理由，我們建立一套進出
的人數統計系統。利用影像處理和電腦視覺的技術，達到自動計算進出人數的功能，同時滿
足節省人力資源以及讓進出的行人自由行走的目的。 
 
三、相關研究 
性別辨識是最近廣泛發展的一個領域，大多數傳統性別辨識的文獻皆使用人臉或聲音為
 5 
其中，影像前處理的目的是偵測並正規化人臉與五官位置，流程如圖二所示，利用改良
的主動形狀模型Stacked Trimmed Active Shape Model (STASM)來偵測人臉上的76個特徵點，再
根據雙眼特徵點的連線做旋轉校正，然後利用這些特徵點的位置擷取左眼、右眼、鼻子與嘴
巴等五官區塊，並且將各個區塊的影像縮放至固定的大小，最後使用直方圖等化(Histogram 
equalization)增加每個五官區塊的影像對比度。 
 
圖二：影像前處理流程圖 
 
  特徵擷取與分類的步驟可分成兩個部分，第一個部分包含擷取五官區塊的主成份分
析(Principal component analysis, PCA)特徵以及偵測頭髮並擷取頭髮特徵，然後將五官區塊的
PCA特徵和頭髮特徵輸入隨機森林(Random Forest)做性別分類，第二個部分則是擷取方向梯
度直方圖(Histogram of oriented gradient, HOG)特徵，然後將HOG特徵輸入支持向量機(Support 
vector machine, SVM)分類遮蔽的情形，特徵擷取與分類器之間的關聯性如圖三所示。最後，
結合性別分類結果與遮蔽情況，選擇未遮蔽且可靠的五官區塊，並且適時的加入頭髮特徵來
判斷性別。 
 
圖三：特徵擷取與分類器之關係圖 
 
每張輸入影像的人臉與五官位置、人臉旋轉角度、背景光源等條件不盡相同，因此必須
透過人臉與五官偵測的過程，定位人臉特徵點並且擷取正規化的五官區域，本研究採用Stacked 
Trimmed Active Shape Model (STASM)定位人臉特徵點位置，此方法改進原始的主動形狀模型
(Active shape model, ASM)，使其能更準確的定位正面人臉影像。接下來會先介紹ASM與
STASM的演算法，然後將主動形狀模型與另外一個常用的主動外觀模型(AAM)做比較。  
ASM是利用形狀(shape)資訊，透過可形變的統計模型，反覆迭代搜尋人臉特徵點座標。在訓
練ASM模型之前，必須先在所有人臉影像上手動標記n個landmarks，landmark是指人臉上的特
徵點，通常為影像中明顯的特徵或形狀邊緣處，例如瞳孔或眼角等位置(圖四)，將landmarks
的座標串聯以形成一個形狀向量(shape vector)： 
 7 
 
圖六：含76個特徵點的shape model 
 
表一：特徵點標示的區域 
Region Index Amount 
Face outline 0-14 15 
Left eyebrow 21-26 6 
Right eyebrow 15-20 6 
Left eye 27-31, 68-71 9 
Right eye 32-36, 72-75 9 
Nose 37-47, 67 12 
Mouth 48-66 19 
Total 0-75 76 
 
  另外一個常見的特徵定位方法為主動外觀模型(Active appearance model, AAM)，除了
shape model之外還建立了外觀模型(appearance model)來描述紋理特徵；雖然AAM是基於ASM
所延伸出來的，但其實兩者各有優缺點，ASM搜尋的速度比AAM快很多，執行成功的比例也
比較高，AAM則是在定位的精準度上比ASM來的準確(表二)。 
 
表二：ASM與AAM在人臉特徵點定位之效能比較 
 
 
  本研究在前處理時的人臉與五官偵測，目標為定位五官大致的範圍，所以不需要特別的
精準，也不希望耗費過多的時間，只需要能夠找到人臉範圍並且至少定位一個五官區塊的位
置就算成功，因此採用STASM即可達成目標。若能成功定位人臉特徵點，才有辦法進行後續
的處理，因此在第五章的實驗結果分析中，將會先統計STASM的定位成功率，以確認人臉特
徵點定位的步驟是否能夠順利執行。在本研究所實驗的人臉資料庫中，定位的結果皆能達到
九成以上不錯的成功率。使用STASM定位人臉特徵點之後，接下來將根據雙眼特徵點的連線
 9 
表三：各五官區塊大小 
Component Image size (px) 
Left eye 30  30 
Right eye 30  30 
Nose 40  20 
Mouth 60  20 
 
  由於拍攝環境的光源影響，人臉影像可能會有偏亮或是偏暗的情況發生，所以利用直方
圖等化(Histogram equalization)來調整各個五官區塊的亮度也提高影像的對比度，直方圖等化
的步驟如下： 
(1) 假設影像的像素總數為n 、亮度值i出現的次數為ni，計算影像亮度值的機率密度函數p(i) 
 
 
(1) 
(2) 計算累積分布函數c(i) 
 
 
(2) 
(3) 根據累積分布函數，建立原影像亮度值的對應關係 
  (3) 
(4) 計算出新的亮度值 
  (4) 
 
  經過直方圖等化的過程，將原先的亮度分布均勻化，讓原本較亮、較暗或低對比度的影
像，調整為均勻亮度、高對比度的影像，使得影像更為清晰，以利於後續的特徵擷取與分類
處理。圖九為整個五官區塊選取與正規化(normalize)的流程示意圖。 
 
圖九：五官區塊選取與正規化 
 
除了人臉五官區塊，本研究也加入了頭髮為輔助特徵，接著將五官區塊與頭髮特徵各別
做性別分類，圖十為性別分類方法的流程圖，首先使用主成分分析(Principal component analysis, 
PCA)做特徵降維，然後將男、女生樣本輸入訓練隨機森林(Random Forest)性別分類器，透過
Random Forest除了可以分類性別，也能估計對於某個性別的信心程度(confidence)。 
 11 
 
圖十二：Flood fill演算法 
 
(3) 計算頭髮的特徵向量 
  如圖十一所示，設左右紅框的部分為左右頭髮的範圍，藍框的部分為瀏海所在的範圍，
統計所有紅框中頭髮所佔的pixel數量佔該區塊大小的比例做為髮量特徵值，並統計藍框中頭
髮所佔的pixel數量佔該區塊大小的比例做為瀏海特徵值，由髮量特徵值與瀏海特徵值組成一
個2維的頭髮特徵向量。 
  特徵擷取(feature extraction)的過程是將特徵從原本的特徵空間(feature space)轉換到另外
一個特徵空間，而在性別分類的部分，本論文採用效果較好的PCA特徵，將左眼、右眼、鼻
子、嘴巴與頭髮特徵投影到一個維度較低的子空間(subspace)。 
  PCA是一個常見的降維處理方法，用在去除不必要的維度，而Random Forest是一個包含
多個決策樹(Decision tree)的分類器，可處理大量的輸入變數，令N為訓練樣本的個數、M為特
徵變數的個數，首先依據以下的演算法建造每顆決策樹： 
(1) 從N個訓練樣本中，以重複取樣的方式取樣N次，形成一組訓練集，用來訓練單顆決策樹。 
(2) 在每顆決策樹的每個節點上進行分裂(split)時，從M個特徵變數中隨機選擇m個(m ≪ M)，
然後根據這 m個變數，計算其最佳的分裂方式。 
(3) 每棵決策樹都會完整的成長，並且不進行剪枝(pruning)。 
  依照上述流程，總共訓練t個決策樹組成Random Forest，然後對於每個測試樣本，將統計
所有決策樹的分類結果作為Random Forest的分類結果，如圖十三之Random Forest基本架構圖
所示，一般輸出的類別是由各別決策樹輸出的類別眾數而定，本論文則是採用計算各個決策
樹輸出的類別比例作為對於某類別的信心程度。 
 13 
SVM的目的就是希望margin越大越好，此時分類器的總誤差將會最小。 
 
圖十五：二維的SVM示意圖 [20] 
 
  對於一群在Rd空間中的資料點，SVM希望能夠在該空間之中找出一個最大間格的超平
面，此超平面可以將這群資料分成兩群(如A群和B群)，屬於A群的資料均位於超平面的同一
側，而B群的資料均位於超平面的另外一側。已知訓練樣本集：{xi, yi}, i = 1, 2, …, n，yi{-1, 
+1}，SVM希望能找到一個超平面f(x) = wTx + b，使所有yi = −1的點落在f(x) < 0的這一側，且
所有yi = +1的點落在f(x) > 0的那一側。 
  圖十六中，實線代表hyperplane，並將H1和H2兩條虛線稱之為support hyperplane，分別將
H1和H2定義為公式4.11和公式4.12。 
  (6) 
  (7) 
則H1到原點的距離為： ，H1到原點的距離為： ，因此H1和H2
之間的距離為： 。 
 
圖十六：SVM分割說明 
 
依照以上公式，在Rd空間中的資料點必須要滿足： 
    (8) 
  (9) 
上面兩式可改寫成：  
因此可以將SVM的問題歸納為： 
 15 
 
圖十七：FERET資料庫正臉影像之範例 
 
 
圖十八：AR資料庫正臉影像之範例 
 
  表三為資料庫中各類別的影像張數，FERET資料庫可分成未戴眼鏡與戴眼鏡兩個類別，
AR資料庫則分成未戴眼鏡、戴眼鏡、戴墨鏡與圍圍巾等四個類別。每張輸入的影像將透過人
臉與五官偵測的過程，以完成人臉特徵點的定位並且擷取正規化的五官區域，其中於STASM
五官定位的步驟，必須能夠找到人臉且至少定位一個五官區塊的位置，才能繼續進行後續特
徵擷取與分類的動作，圖十九(a)是一個定位成功的例子，圖十九 (b)則為失敗的例子，表四
為STASM在FERET和AR兩個人臉資料庫的定位成功率，皆能達到九成以上不錯的成功率。 
 
表三：人臉資料庫之實驗影像總數 
Database Total Normal Glasses Sunglasses Scarf 
FERET 591 / 403 528 / 392 63 / 11 - - 
AR 76 / 60 44 / 52 32 / 8 76 / 59 45 / 51 
Total - all situation 
Normal - not wearing glasses 
Glasses - wearing glasses 
Sunglasses - wearing sunglasses 
Scarf - wearing scarf 
 
 17 
  接下來的實驗中，採用從AR資料庫中選擇90張遮蔽與90張未遮蔽的影像來訓練各個五官
區塊的遮蔽分類器。首先比較不同的區塊特徵組合之辨識率，以兩個一組、三個一組、四個
一組或是五個一組的方式，計算性別信心程度的平均值並決定性別，然後統計其辨識率，以
圖二十與圖二十一的長條圖來呈現各組合之辨識率。 
 
(a) 
 
(b) 
 
(c) 
圖二十：不同區塊特徵組合之辨識率(FERET) 
(a) 兩個區塊的組合  (b) 三個區塊的組合  (c) 四個區塊的組合 
 
 19 
區塊都擁有最佳的性別區分能力，所以不能給予固定、不同比例的權重，因此本論文提出利
用動態選擇區塊的方式，然後計算各選擇區塊的性別信心程度之平均值來決定性別。 
  另外，從圖二十與圖二十一中可以發現，考慮的區塊越多辨識率越高，例如FERET中，
圖二十(c)的左眼、右眼、鼻子與嘴巴的組合之辨識率最高，但是如果直接加入頭髮特徵，反
而會拉低辨識率，因此採用4.3節所提出之適應型決策融合方法，將能有效保持辨識率甚至提
高辨識率，如圖二十二和圖二十三所示，其中紅色代表只考慮臉上五官區塊而不考慮頭髮的
特徵，紫色則考慮五官區塊並且加入頭髮為輔助特徵，可觀察到本論文所提出之方法能有效
將頭髮特徵化阻力為助力，在FERET與AR資料庫中分別有90.28 % 與93.94 % 的辨識率。 
 
圖二十二：適應型性別辨識之辨識率(FERET) 
 
 
圖二十三：適應型性別辨識之辨識率(AR) 
 
 21 
 
 
(1) 
 
(2) 
 
(3) 
 
(4) 
 
(5) 
 
(6) 
圖二十五：辨識失敗之範例 
 
 
↓ 
 
↓ 
 
(a) 
 
(b) 
圖二十六：頭髮偵測失敗 
 
  本實驗使用人臉資料庫中所有正面、未戴眼鏡且成功定位人臉特徵點之影像，整體而言，
本研究提出之方法的辨識率擁有一定的水準。 
 23 
 
圖二十七：前景偵測與陰影去除 
 
我們將前景物偵測後的結果影像，分割成長寬各 4 像素的子區塊，如圖 3.3 所示。每一
個包含 16 像素的子區塊中，若屬於前景點的個數超過設定的閥值，則該子區塊中的所有像
素皆設為前景點，否則將視為背景的一部分。圖二十八中，假設因背景相減法造成一個物體
分裂成上下兩塊，我們利用子區塊的概念，計算其中屬於前景點的個數，當 16 個像素點裡
有 8 個含以上的點屬於前景點，則將整個子區塊皆視為前景區塊。如此便可以解決物體分裂
和內部破碎的問題，並達到雜訊去除的目的。處理結果如圖二十九所示。 
 
 
圖二十八：子區塊示意圖 
 
 
 25 
在每個區塊下面，我們再將行人的行走姿勢，分為左腳在前、右腳在前和雙腳合併，搭配向
上和向下兩個行走方向，共 6 種樣板，如圖三十一 所示。樣板的大小，經由影片當中行人
大小的分析，設定為寬度 70 像素，長度 90 像素。樣板的總數為 72 個。 
 
 
圖三十一：樣板資料庫 
 
隨著樣板數目的增加，比對的時間也會相對增長，因此要如何在偵測準確率和效能之間
取得平衡，是一個很重要的問題。本論文根據樣板建立時所分割的區塊，將樣板分類，當要
進行樣板比對時，就針對前景物所在的區域，只用該區塊下的樣板去做比對，以節省比對所
有樣板的時間，如圖三十二。如此一來，樣板數目增加，對於外型不固定物體的比對，可以
 27 
實驗共測試了 32 段影片，影片解析度為 320*240 pixels，總時間約為 109 分鐘。其中室外的
環境，包含 6 段晴天、19 段陰天、3 段雨天共 28 段影片，室內的環境則測試了 4 段影片。
程式開發是使用 Borland C++ Builder 6.0，系統執行的平台為 P-43.0 GHz、512MB RAM，作
業系統為 Windows XP，程式的執行效能平均每秒可處理 8 張影格。下圖三十四為攝影機所
拍攝到的行人進出影像： 
 
 
圖三十四：行人進出影像 
 
在找出前景物之後，就利用樣板比對的方式，判斷物體是否為行人，或是其他非人的物體。
若是行人，則會用一個和樣板大小相同的黃色外框將行人框住。以下為行人辨識的結果： 
 
 
圖三十四：同一人在不同位置的辨識結果 
 29 
五、研究成果自評 
本研究建構出一性別辨識系統與人數計算系統，選取左眼、右眼、鼻子與嘴巴等四個五
官區塊，以及人臉外部的頭髮範圍，然後各別擷取PCA特徵來訓練Random Forest性別分類器，
同時擷取五官區塊的HOG特徵，輸入SVM訓練各區塊的遮蔽分類器，最後利用適應型的決策
融合，將性別分類結果與遮蔽情況做結合，選擇未遮蔽且可靠的五官區塊，並且適時的加入
頭髮特徵來判斷性別。相較於其他人臉性別辨識的研究，本適應型性別辨識採用動態選擇五
官區塊的方法，使性別辨識能調適於人臉部分遮蔽的情況。而進出口行人數量的自動統計系
統，可應用於展覽會場、百貨公司等需要計算人數的公共場所。利用影像處理和電腦視覺的
技術，達到自動計算進出人數的功能，同時滿足節省人力資源以及讓進出的行人自由行走的
目的。 
參考文獻 
[1] L. Cao, M. Dikmen, Y. Fu, and T. S. Huang, “Gender recognition from body,” in Proc. 16th 
ACM Int. Conf. Multimedia, 2008, pp. 725-728. 
[2] G. Huang and Y. Wang, “Gender classification based on fusion of multi-view gait sequences,” 
in Proc. 8th Asian Conf. Computer Vision, 2007, pp. 462-471. 
[3] X. Li, S. J. Maybank, S. Yan, D. Tao, and D. Xu, “Gait components and their application to 
gender recognition,” IEEE Trans. Syst. Man Cybern. C, Appl. Rev., vol. 38, no. 2, pp. 145-155, 
2008.  
[4] S. Yu, T. Tan, K. Huang, K. Jia, and X. Wu, “A study on gait-based gender classiﬁcation,” 
IEEE Trans. Image Process., vol. 18, no. 8, pp. 1905-1910, 2009. 
[5] L. T. Kozlowski and J. E. Cutting, “Recognizing the sex of a walker from a dynamic 
point-light display,” Percpt. Psychophys., vol. 21, pp. 575-580, 1977. 
[6] L. Lee and W. E. L. Grimson, “Gait analysis for recognition and classification,”  in Proc. 5th 
IEEE Int. Conf. Automatic Face and Gesture Recognition, Washington, DC, May 2002, pp. 
155-162. 
[7] S. Lee, Y. Liu, and R. Collins, “Shape variation-based frieze pattern for robust gait 
recognition,” IEEE Int. Conf. Computer Vision and Pattern Recognition, 2007. 
[8] S. Sarkar, P. J. Phillips, Z. Liu, I. R. Vega, P. Grother, and K. W. Bowyer, “The human id gait 
challenge problem: Data sets, performance, and analysis,” IEEE Trans. Pattern Anal. Mach. 
Intell., vol. 27, no. 2, pp. 162-177, Feb. 2005. 
[9] L. Wang, H. Ning, T. Tan, and W. Hu, “Fusion of static and dynamic body biometrics for gait 
recognition,” IEEE Trans. Circuits Syst. Video Technol., vol. 14, no. 2, pp. 149-158, Feb. 
2004. 
[10] J. Han and B. Bhanu, “Individual recognition using gait energy image,” IEEE Trans. Pattern 
Anal. Mach. Intell., vol. 28, no. 2, pp. 316-322, Feb. 2006. 
[11] A. F. Bobick and J. W. Davis, “The recognition of human movement using temporal 
  
1 
 
出席國際學術會議報告 
                                                            100 年 10 月 27 日 
 
報告人姓名 王宇晨 
服務機構及職稱 中央大學資工系博士生 
會議時間 
自民國 100 年 10 月 18 日至 100 年 10 月 21
日 會議地點 
西班牙 巴塞隆納 
經費來源 NSC97-2221-E-008-086-MY3 
會議名稱 
(中文) 2011 年 ICCST 國際研討會 
( 英文 ) 45th IEEE INTERNATIONAL CARNAHAN CONFERENCE ON 
SECURITY TECHNOLOGY(ICCST) 2011 
發表論文題目 (英文) The color identification of automobiles for video surveillance 
參與性質 Oral 
報告內容： 
一、參加會議經過 
此次參加 ICCST 2011 國際學術研討會，係於 2011 年 10 月 18 日至 10 月 18 日在西班牙巴塞
隆納舉行，會議地點是在巴塞隆納附近城鎮馬塔羅（Mataró）中的Tecnocampus Mataró Maresme 
University 舉行，學生除了在會議中發表一篇研究論文，並參加多場專題演講。 
2011/10/16：晚間 10:45 搭乘中華航空 CI 65 班機飛往阿姆斯特丹後轉搭荷蘭航空 KL 1673 班 
           機前往巴塞隆納機場。並於 10/17 號下午 15:00 抵達巴塞隆納機場，並前往預定 
           飯店登記住房。 
2011/10/18：下午至會場報到，並參與開幕晚會，由大會主席歡迎所有參與此次會議的長官及  
           學者們，過程中並介紹許多參與會議的各國相關國土安全防護的政府長官及西班 
           牙當地官員。 
2011/10/19：早上 9:00 大會正式開始，大會開幕吸引相當多學者的參與，首先有請大會主席 
           Gordon Thomas 簡單闡述本研討會議的歷史及主旨。今日有 6 場 Sessions，學生 
           參與 Biometrics 和 Biometrics II 兩場，主要描述相關生物認證機制應用於機場保 
           安及大城市安全監控上。 
2011/10/20：今日大會安排一場 Keynote speech 由 Fernando J. Sánchez Gómez 國家關鍵基礎設 
  
3 
 
2.建議與期許 
感謝國科會同意補助參加 ICCST 2011 國際學術研討會，有多國的與會學者，國內亦有多
所學校的學者專家參與會議，此次的研討會，讓學生不僅學習到影像處理領域相關的技
術及應用，也借由多場 sessions 的報告，學習到其它領域知識及安全監控在國土安全的
應用，如指紋辨識、掌紋辨識、機場 X 光機影像辨識和語音辨識等，藉由多面向的學習，
激發出新的應用領域及改進目前既有技術的方法，多場精采專業的演講，不僅讓學生學
習到新知外，更學習到報告時應有的台風、用詞、以及時間的掌控，在有限時間內做最
詳細的技術說明，並讓與會人員清楚了解所想傳達之意思，是學生需努力練習的。學生
此回參與國際學術研討會，很榮幸的獲得其他學者在研究面上的提點，讓學生可以修正
所提的方法，提高準確度及效能，並從他人的研究成果中，得到一些新的研究想法，可
說是獲益良多。 
 
三、此次研討會中所攜回之資料 
1.研討會會議議程及論文摘要集一本。 
2.研討會會議論文全文光碟片一片。 
     
四、其它 
附件一、論文接受通知函 
附件二、論文全文 
 The Color Identification of Automobiles for Video Surveillance 
 
Yu-Chen Wang1, Chen-Ta Hsieh1, and Chin-Chuan Han2, Kuo-Chin Fan1 
1Department of Computer Science and Information Engineering, 
National Central University, Taoyuan, Taiwan 
2Department of Computer Science and Information Engineering, 
National United University, Miaoli, Taiwan 
 
 
Abstract – Color identification of automobiles plays a significant in 
intelligent transportation systems (ITS). In this paper, a novel 
scheme for color identification of automobile is proposed using the 
taillight detection and a template matching module. The taillights of 
cars are detected to find the valid regions of interested (ROIs) for 
color identification. The color feature vectors generated by 3 by 3 
neighboring pixels are classified by a template matching strategy. 
Seven classes, red, yellow, blue, green, black, white, and gray, are 
identified in this work. Experimental results have been conducted to 
show the validity of the proposed method. The averaged accuracy 
rate 81.71% is achieved and the performance of this scheme is up to 
20 frames per second.  
 
 
Index Terms — Intelligent transportation system, region of 
interest, color identification of automobiles, taillight detection 
algorithm, color template matching algorithm. 
 
 
I.  INTRODUCTION 
 
Recently, the automobile color identification is increasingly 
demanded on urban roads for video surveillance. When an accident 
occurs, license plate (LP) numbers are the directly cues of escape 
cars. However, they are not the effective cues due to the missed and 
small LPs which are caused from the view angle and distance of 
witnesses. Witnesses only remember the automobile colors. The 
color identification from video data can assist police agencies in the 
crime prevention and the later investigation of crime events. 
Moreover, the government widely set up cameras on the road for 
traffic monitor or crime prevention. How to extract color of 
automobile effectively and also taking color as a metadata of other 
feature recognition is a hot issue of current research. 
Early, color feature descriptors have widely used in content 
based image retrieval (CBIR)[1-3]. High quality images with less 
illumination impact in their works. However, the color identification 
of automobiles in outdoor is susceptible to the camera design and 
environmental factors. First, the sequential images captured from the 
outdoor cameras are distorted by chromatic polarization and white 
balance. Second, the performance of identification is impacted by the 
illumination and weather in outdoor. Cameras capture the color 
features from the front, side, or back views of automobiles. 
Whichever views are captured, the over-exposure regions in images 
are always generated due to the sun light. As illustrated in Fig. 1, the 
classification errors always occur. Baek et al. [4] extracted the two-
dimensional histogram on the hue and saturation features. A multi-
class support vector machine (SVM)-based classifier was trained to 
identify color of automobiles. Tsia et al.[5] proposed a color model 
to reduce the illumination impact in outdoors. After the 
transformation on color spaces, a Bayesian classifier is used to 
identify the vehicle color and background. Chen et al. [6] applied the 
PCA-based technique to calculate the eigenvectors and the 
corresponding eigenvalues from the training samples represented in 
the RGB color space. The color pixels in space RGB are transformed 
to the new color space. The multiple instance learning is used to 
classify color. Brown[7] has evaluated four color feature descriptors, 
including standard color histograms, weighted color histograms, 
variable bin size color histograms and color correlograms, to identify 
vehicle colors for surveillance. Stokman et al.[8] proposed a fusion 
scheme to combine several color models. The selected and weighted 
color model is constructed for the detection of discriminatory and 
robust color features. Dule et al.[9] manually assigned the regions of 
interested (ROIs) on car hoods from the detected foreground objects. 
The color features in the half ROI are classified to determine the 
vehicle’s color. Practically, automatically finding the ROI is the 
critical issue in surveillance systems. Wu et al.[10] found the ROIs 
by integrating the results of background subtraction and those of 
color segmentation. An SVM-based classifier in a two-layer 
structure is then applied to classify the pixels in ROI. Li et al.[11] 
proposed a classification method using the template matching 
strategy. Before identification process, a color calibration procedure 
is executed to adjust the image colors [12]. The HSI color space is 
selected and the relative error distances are calculated to identify the 
automobile colors.  
 
 
(a) (b) 
 
(c) (d) 
Figure 1: The over-exposure regions (red regions). (a)(b): the frontal 
views, and (c)(d) the back views. 
 
In this paper, a novel scheme is proposed to identify the 
automobile colors in outdoors as shown in Fig. 2. The identification 
process consists of two modules: The location of a valid ROI and the 
classification using color features. Unlike the traditional background 
subtraction methods which are very sensitive to illumination 
changes, a taillight detection algorithm is proposed to obtain the 
  
(a) (b) 
 
(c) (d) 
Figure 4: (a) an original image, (b) the detected red regions, (c) a pair 
of taillights, and (d)the valid ROI. 
 
Many color spaces (e.g. RGB, HSV, YCrCb, CIELab...etc) are 
explored for object segmentation or classification using color 
features. In this study, color space CIELab model is adopted for 
representing car colors. From the previous studies, the colors in 
space CIELab are very close to human visual aspects. Due to the 
illumination of sun light, robust features should be extracted to 
reduce the noises and lighting impacts. The feature vectors of length 
27 are generated from the 3 by 3 neighboring pixels in space CIELab. 
They are classified with the trained prototypes by the template 
matching rule. Each feature vector is represented as { }
nkikikikiki
vvvvv ,........,,,
321
= , that subscript i denotes the ith 
template sample, subscript k represents the kth color index, and n is 
the vector dimension, 27 in this work. As to the parameters of 
template matching, k nearest neighbor (K-NN) strategy is performed 
to find the best matched prototype. In addition, the metric of 
Euclidean distance is adopted to assess the similarity of color 
features as defined as:  
 
( ) 2
1
1
2
, ∑
=
⎟⎠
⎞⎜⎝
⎛
−=
n
m
jkijki
ij
k mm
vvvvd
.
 (4) 
 
Here symbol kiv  denotes the color features of the kth trained 
samples, and symbol jv is the feature vectors in ROI. Using the 
results in section 3.A, the valid ROI is obtained by using taillight 
detection algorithm. The feature vector for each pixel in ROI is 
extracted and matching with the trained prototypes. The similarity is 
defined as the inversed Euclidean distance. The color of each pixel in 
ROI is classified by EQ. (5):  
 ( )jkiijkkk vvdC ,minarg= , (5) 
 
where Ck denotes the color prototype index. Finally, the distances 
from ROI to seven prototypes are calculated. The statistics data 
determine vehicle in which the color of the valid ROI area with 
highest percentage, and defines this color for the automobile. In this 
study, seven colors’ prototypes, e.g., black, gray, white, red, yellow, 
green and blue, are collected which frequently appear in city roads. 
 
 
III.  EXPERIMENTAL RESULTS 
 
In this section, some experiments are conducted to show the 
effectiveness of the proposed method. A stationary CCD camera was 
installed in outdoors. Six video clips are captured in 30 f/s in various 
sunny, cloudy, and rainy days. 8319 image frames of size 320 by 240 
were extracted from the video clips, and there are 3654 image frames 
possess the automobiles. Several illustrated images used in the 
experiments are shown in Fig. 5. The identification system is 
implemented in a PC-based platform and is developed by the 
Microsoft visual C++ 2008 and Opencv 2.1 tool kits.  
The taillights are first detected and a valid ROI is determined. The 
color features in ROI are extracted and matched with the trained 
prototypes by using the template matching strategy. The detection 
rates as shown in Table I are very high. In addition, 3579 taillight 
pairs are correctly paired from 3654 pair of taillights whose ground-
truths are manually assigned. The averaged detection rate for 
taillights is more than 98%. The proposed method could correctly 
detect the taillight regions from videos. The taillight pairs are 
verified by the geometric rules to generate the valid ROI, and the 
color features in ROIs are extracted to classify the vehicle colors. 
The automobile colors are determined by the dominative colors in a 
valid ROI area. The template matching algorithm considers the 
neighboring pixels to reduce the noise distortion. The window pixels 
of 3 by 3 are fused to generate a color feature of length 27 in CIELab 
color space. Since the cameras are installed in outdoors, the image 
quality of automobile colors is poor with much variation due to the 
illumination.  
In the experiments, the colors of ROIs are classified into seven 
classes using the template matching algorithm, e.g. color red, yellow, 
blue, green, black, white and gray. Several correct classified results 
are shown in Fig. 6. Each illustration correctly finds the valid ROI 
and identifies the correct color. The classification rates for six video 
clips are tabulated in Table II. The averaged accuracy rate is 81.71%. 
In addition, the identification performance in the proposed method is 
up to 20 frames per second. On the contrary, two mis-classification 
results are shown in Fig. 7. The errors are resulted from the invalid 
ROI. Invalid ROIs will generate the incorrect features. The locations 
of taillights for a van and a truck are different from those of a car. 
The truck taillights are located at the lower parts, and the windshield 
of a van is located between two taillights.  
 
TABLE I 
The detection rates of taillight regions. 
Video clips Match taillight pairs 
Number of total 
pair taillights detection rates 
1 450 455 0.989 
2 530 557 0.952 
3 432 457 0.945 
4 166 167 0.994 
5 981 996 0.985 
6 1020 1022 0.998 
Average 
Accuracy rate   0.98 
 
 
 consists of two modules: The location of a valid ROI and the 
classification using color features. The taillights are detected and a 
valid ROI is generated. The pixels in ROI are classified to determine 
the color of cars using color features. The neighboring pixels are 
considered to effectively reduce the illumination impacts and noises. 
From the conducted experimental results, the high performance of 
the proposed method is achieved. In the future works, the shapes of 
cars will be identified to correctly locate the valid ROI. The ROIs 
from the frontal view or the side view will be extracted for car color 
identification. In addition, more image processing algorithms will be 
designed to overcome the over-exposure problem. 
 
 
V.  ACKNOWLEDGEMENTS 
 
The work was supported by the National Science Council 
under grant no. NSC 99-2221-E-239 -030, and by the 
Technology Development Program for Academia of DOIT, 
MOEA, Taiwan under grant no. 99-EC-17-A-02-S1-032. 
 
 
VI.  REFERENCES  
 
[1]. I. K. Park, I. D. Yun and S.U. Lee “Color image retrieval 
using hybrid graph representation,” Image and Vision 
Computing, vol. 17, pp. 465-474, 1999. 
[2]. L. Cinque, G. Ciocca, S. Levialdi, A. Pellicanò and R. 
Schettini, “Color based image retrieval using spatial 
chromatic histograms,” Image and Vision Computing, vol. 19, 
pp. 979-986, 2001. 
[3]. G. Qiu, “Embedded colour image coding for content-based 
retrieval,” Journal of Visual Communication and Image 
Representation, vol. 15, pp. 507-521, 2004. 
[4]. N. Baek, S. M. Park, K. J. Kim, and S. B. Park, ‘‘Vehicle 
color classification based on the support vector machine 
method,’’ Communications in Computer and Information 
Science, vol. 2, pp.1133-1139, 2007. 
[5]. L. W. Tsai, J. W. Hsieh and K. C. Fan, ‘‘Vehicle detection 
using normalized color and edge map,’’ IEEE Transactions on 
Image Processing, vol. 16, pp.850-864, 2007. 
[6]. S. Y. Chen, J.W. Hsieh, J. C. Wu and Y. S. Chen, “Vehicle 
retrieval using eigen color and multiple instance learning,” 
International Conference on Intelligent Information Hiding 
and Multimedia Signal Processing, pp.657-660, 2009. 
[7]. L. M. Brown, “Example-based color vehicle retrieval for 
surveillance,” IEEE International Conference on Advanced 
Video and Signal Based Surveillance, pp.91-96, 2010. 
[8]. H. Stokman and T. Gevers, "Selection and fusion of color 
models for image feature detection," IEEE Transactions on 
Pattern Analysis and Machine Intelligence, vol. 29, pp. 371-
381, 2007. 
[9]. E. Dule, M. Gokmen and M.S. Beratoglu, “A convenient 
feature vector construction for vehicle color recognition,” 
Proceedings of the 11th WSEAS international conference on 
nural networks, evolutionary computing and Fuzzy systems, 
2010. 
[10]. Y. T. Wu, J. H. Kao, and M. Y. Shih, “A vehicle color 
classification method for video surveillance system 
concerning model-based background subtraction,” Pacific-
Rim Conference on Multimedia, vol. 6297, pp. 369-380, 2010. 
[11]. X. Li, G. Zhang, J. Fang, J. Wu and Z. Cui, “Vehicle color 
recognition using vector matching of template,” International 
Symposium on Electronic Commerce and Security, pp.189-
193, 2010. 
[12]. G. D. Finlayson, B. Schiele, J.L. Crowley, “Comprehensive 
color image normalization,” Proceedings of 5th European 
Conference on Computer Vision, vol. 1, pp. 475-490, June 
1998. 
[13]. Y. Y. Lu, C. C. Han, M. C. Lu and K. C. Fan, “A vision-
based system for the prevention of car collisions at night,” 
Machine Vision and Applications, vol. 22, pp. 117-127, 2011. 
 
 
VIII.  VITA 
 
Yu-Chen Wang was born in Taichung, Taiwan, R.O.C., in 1984. 
He received the B. S. degree in Department of Computer Science 
and Information Engineering from Min-Dao University, Changhua, 
Taiwan, in 2005. He received M. S. degree in the Department of 
Computer Science and Information Engine from Chung Hua 
University, Hsinchu, Taiwan, in 2008. He is currently pursuing the 
Ph.D degree in computer science at the National Central University. 
His research interests include pattern recognition, computer vision, 
and machine learning. 
Cheng-Ta Hsieh was born in Taipei, Taiwan, R.O.C., in 1983. 
He received the B. S. and M. S. degree in Department of Computer 
Science and Information Engineering from Chung Hua University, 
Hsinchu, Taiwan, Republic of China in 2005. He is currently 
pursuing the Ph. D degree in computer science at the National 
Central University. His research interests include pattern recognition, 
computer vision, and machine learning. 
Chin-Chuan Han received the BS degree in computer 
engineering from National Chiao-Tung University in 1989, and the 
MS and PhD degrees in computer science and electronic engineering 
from National Central University in 1991 and 1994, respectively. 
From 1995 to 1998, he was a postdoctoral fellow in the Institute of 
Information Science, Academia Sinica, Taipei, Taiwan. He was an 
assistant research fellow in the Telecommunication Laboratories, 
Chunghwa Telecom Co. in 1999. From 2000 to 2004, he worked 
with the Department of Computer Science and Information 
Engineering, Chung Hua University, Taiwan. In 2004, he joined the 
Department of Computer Science and Information Engineering, 
National United University, Taiwan, where he became a professor in 
2007. He is a member of the IEEE, the SPIE, and the IPPR in 
Taiwan. His research interests are in the areas of face recognition, 
biometrics authentication, video surveillance, image analysis, 
computer vision, and pattern recognition. 
Kuo-Chin Fan received the BS degree in electrical engineering 
from the National Tsing-Hua University, Hsinchu, in 1981, and the 
MS and PhD degrees from the University of Florida (UF), 
Gainesville, in 1985 and 1989, respectively. In 1983, he joined the 
Electronic Research and Service Organization (ERSO), Taiwan, as a 
computer engineer. From 1984 to 1989, he was a research assistant 
with the Center for Information Research at UF. In 1989, he joined 
the Institute of Computer Science and Information Engineering, 
National Central University, Chung-Li, Taiwan, where he became a 
professor in 1994, and from 1994 to 1997, he was the chairman of 
the department. Currently, he is the director of the Computer Center. 
His current research interests include image analysis, optical 
character recognition, and document analysis. He is a member of the 
SPIE. 
 
  
2 
 
最佳化分類法則的應用，這對於學生在後續的研究上有相當大的助益，除了新知上的增
進，本次報告也讓學生學習到該如何在短暫的時間內，用最簡單的用詞搭配投影片動畫
的解釋，讓各國的與會學者都能了解所提出的方法，並且能夠與台下學者有互動，感謝
與會學者的建議及鼓勵，讓學生可以在報告上更有自信且亦可修正所提出之方法，此型
獲益良多。 
 
三、此次研討會中所攜回之資料 
1.研討會會議議程及論文摘要集一本。 
2.研討會會議論文全文光碟片一片。 
     
四、其它 
附件一、論文接受通知函 
附件二、論文全文 
  
Abstract—In this paper, a novel with low complexity gaze 
point estimation algorithm in unaware gaze tracker is proposed 
which is suitable in normal environment. The experimental 
results demonstrate our proposed method is feasible and has 
acceptable accuracy. Besides, the proposed method has less 
complexity in terms of camera calibration process than 
traditional method. 
 
Index Terms—Gaze Point Estimation; Unaware Gaze 
Tracker; Voting Scheme 
 
I. INTRODUCTION 
nteractive Installation is the most popular issue in recent 
years. Such as using Hand Gesture, Human Posture, Eye 
Detection, Gaze Tracking, Speech recognition to control the 
computer, device or play games. The Gaze tracking can be 
used in many applications such as web usability, advertising, 
sponsorship or in communication systems for disable people. 
Numerous techniques of eye gaze trackers have been 
developed [1-13]. These eye gaze tracker found in literature 
can be divided into two groups, intrusive techniques and 
non-intrusive techniques, respectively. Intrusive methods 
usually use special devices to attach the eye skin or wear 
head-mounted to catch the user’s gaze in very close to the 
eyes [1].  
The most widely used current designs for eye trackers are 
using a non-contacting video camera to focus on eyes and 
records their movement. Compared with intrusive methods, 
the non-intrusive methods have the advantage of being 
comfortable during the process of gaze estimation [13]. 
Video-based eye trackers typically use the corneal reflection 
and the iris center as feature to track over time [2-12]. 
The gaze calibration procedure that identifies the mapping 
from pupil parameters to screen coordinates using neural 
network has become more popular for eye gaze tracker. 
Baluja and Pomerleau proposed a neural network method 
 
Manuscript received December 30, 2011; revised January 17, 2012.  
Chiao-Wen Kao is with the Department of Computer Science and 
Information Engineering, National Central University, Taoyuan, Taiwan. 
(e-mail: chiaowenk@gmail.com). 
Bor-Jiunn Hwang is with the Department of Computer and 
Communication Engineering, Ming Chuan University, Taoyuan, Taiwan. 
(e-mail: bjhwang@ mail.mcu.edu.tw). 
Kuo-chin Fan is with the Department of Computer Science and 
Information Engineering, National Central University, Taoyuan, Taiwan. 
(e-mail: kcfan@csie.ncu.edu.tw). 
Che-Wei Yang is with the Department of Computer and Communication 
Engineering, Ming Chuan University, Taoyuan, Taiwan. (e-mail: 
yang.bng86@gmail.com). 
Chin-Pan Huang is with the Department of Computer and 
Communication Engineering, Ming Chuan University, Taoyuan, Taiwan. 
(e-mail: hcptw@mail.mcu.edu.tw). 
without explicit features [3]. Each pixel of the image is 
considered as an input parameter of the mapping function. 
Once the eye is detected, the image of the eyes is cropped and 
then used as the input of ANN (Artificial Neural Network). In 
[9], authors proposed remote eye gaze tracker based on eye 
feature extraction and tracking by combining neural mapping 
(GRNN) to improve robustness, accuracy and usability under 
natural conditions. 
For 3D model-based approaches, gaze directions are 
estimated as a vector from the eyeball center to the iris 
centers [8]. A stereo camera system is constructed for 3D eye 
localization and the 3D center of the corneal curvature in 
world coordinates.  Points on the visual axis are not directly 
measurable from the image. By showing at least a single 
point on the screen, the offset to the visual can be estimated. 
The intersection of the screen and the visual axis yield the 
point of regard. 
The purpose of this paper is to propose a novel with low 
complexity gaze point estimation algorithm in unaware gaze 
tracker and which is suitable in normal environment.  
The remainder of the paper is organized as follows. In 
section II, the proposed Voting scheme algorithm is 
presented. The gaze evaluation model and results are carried 
out in section III. Finally, the paper ends with our conclusions 
with discussion and recommendations for future work in 
section IV. 
II. PROPOSED VOTING SCHEME ALGORITHM 
A gaze tracker is used to acquire eye movements. A 
general overview of the gaze tracker is shown in Fig. 1, 
comprising Face Detection, Eyes Detection, Eyes Tracking 
and Gage Estimation. Eyes Detection and Gaze Estimation 
are important functionality for many applications including 
driver’s physical condition analysis, helping disabled people 
operate computer, auto stereoscopic displays, facial 
expression recognition, and more. The eye positions should 
be calculated first to estimate the person’s gaze coordinates. 
This section describes an algorithm for tracking gaze 
direction on the screen. 
A. Preprocessing 
Several preprocessing steps must be done before 
performing gaze tracking, as shown in Fig. 1. Firstly, 
detecting face in image is a fundamental task for surveillance 
system. This paper use Haar-like Features which firstly 
proposed by Paul Viola and Jones to detect the face [14] [15]. 
Haar-like features are digital image features used in object 
detection and recognition. . Each classifier uses K rectangular 
areas to make decision which the region of the image likes 
predefined image or not. Fig. 2 exhibits the Haar-like shape 
features sets including Line features, Edge features and 
A Novel with Low Complexity Gaze Point 
Estimation Algorithm 
Chiao-Wen Kao, Bor-Jiunn Hwang, Che-Wei Yang, Kuo-Chin Fan, Chin-Pan Huang 
I 
 blocks, where N along the horizontal dimension and 
M along the vertical dimension. 
Step 3. Divide detected eye images into the same number 
of blocks. 
 
Predict horizontal position of iris center: 
Step 1. To get the center line of vertical dimension in each 
block, HBLij, for i=1,…N, j=1,…M. 
Step 2. Divide HBLij into N equal line segments, HBLij-k, 
k=1,…N. 
Step 3. Compute the vertical projection and mean of the 
HBLij-k, respectively. 
Step 4. Adaptive thresholds (Th) are obtained to quantify 
the mean values according to the method in 
[11-12].The quantified mean value Qij-k of each line 
segment is computed by (1). 
  
(1) 
Where ⌊x⌋ denotes the nearest integers less than or 
equal to x. y and ybase represent maximum and 
minimum mean values of HBLij-k, respectively. 
Step 5. Sum of the quantified mean value, Sik, is computed 
by (2) 
  
 (2)
 
S={Sik for i=1…N, k=1…N}
 
Step 6. Initial voting weight Wtik. The set SN is composed 
by the lowest of N values in S, where 
 
(3)
 
The block weights Wti are obtained by summing of the 
voting weight as (4). 
 (4)
 
Step 7. Finally, to find maximum value of Wti to determine 
the iris center of horizontal.  
 
Therefore, the candidate of horizontal position of iris 
center can be found by using the Voting scheme. 
 
Predict vertical position of iris center:  
Step 1. It’s a great similarity between getting the vertical 
and horizontal position. To get the center line of 
vertical dimension in Wti which computed by (3), 
VLj, for j=1,…M. 
Step 2. Divide VLj into N+2 line segments on average, 
VLj-k, k=1,…N+2. From the biological point of view, 
vertical eye movement is smaller. Therefore we 
divided segment into more detail in order to 
improve the accuracy.  
Step 3. Compute the horizontal projection and mean of the 
VLj-k, respectively. 
Step 4. Repeat the step 5~step 8 in Horizontal position of 
iris center prediction procedure. 
Step 5. Finally, select maximum value of VLj to represent 
the iris center of vertical in this block.  
 
Based on these procedures of Voting scheme, we can 
estimate the gaze position on the screen facilely. For example, 
assume the test object in full-screen is divided into 3*3 
blocks as shown in Fig.7. And the gray scale eye image is 
also divided into 3*3 blocks. Thus, we can get 9 center line 
segments in the blocks, as shown in Fig. 8. 
The results of computing the vertical projection and mean 
of each line segment of Fig. 8 are shown in Fig. 9 and Fig. 10, 
respectively. Based on Fig. 10, the quantified mean value and 
sum of the quantified mean value are performed by (1) and 
(2), respectively, the results are shown in Fig. 11. Initial 
voting weight is performed by (3) and then summing of the 
weights by (4), the results are shown in Fig. 12. The 
candidate of horizontal position of iris center is determined 
by selecting the lowest of three values as shown in Fig. 13. 
 
 
Fig. 7. Divide full-screen advertisement into 3*3 blocks 
 
 
Fig. 8. Example of divide grayscale eye image into 3*3 blocks 
 
The purpose of vertical position estimation is to determine 
the horizontal candidate. As experimenting, brightness spots 
on the iris that maybe influence the vote result. Hence, in the 
Voting scheme, more divided segments in vertical are 
performed to improve accuracy. Fig. 14 shows the estimation 
result, and the vertical position of iris center is determined in 
the block 5. 
   1Q  Thyy baseij-k
  
M
j kijik
Q
1
S





otherwiseWt
SSifWt
ik
Nikik
,0
 ,1
 
 
N
k iki
WtWt
1
 
 IV. CONCLUSION 
We have surveyed several categories of eye tracking 
systems from the different methods of detecting and tracking 
eye images to computational models of eyes for gaze 
estimation and gaze-based applications. However, most of 
systems setup increases have higher both the complexity and 
cost. Stated thus, we propose a novel unaware method, 
namely Voting scheme, to estimate gaze tracking based on 
appearance-manifolds. In this system, the user only sits in 
front of a computer and use the webcam on the monitor to 
capture the user’s image sequences. This method first 
calculates the histogram of grayscale eye image and use 
dynamic thresholds to quantify the pixel values. Then gaze 
direction on the screen can be predicted by using voting 
scheme. The experimental results demonstrate the 
effectiveness of proposed gaze tracking approach. Based on 
this, we have tried to find out how people look at content of 
website or advertisement. However, some problems still need 
to be solved. Firstly, the proposed method cannot deal with 
low resolution image sequences. In addition, the blurred or 
bad illuminated image sequences could affect the tracking 
result. Future work will be to deal with those problems and 
achieve more robust algorithm. 
 
ACKNOWLEDGMENT 
This work is supported by the National Science Council in 
Taiwan. The project contract number is NSC 
100-2221-E-130-024-. 
REFERENCES 
[1] Craig A. Chin, Armando Barreto et al, “Integrated electromyogram and 
eye-gaze tracking cursor control system for computer users with motor 
disabilities,” Journal of Rehabilitation Research & Development, 
Vol.45, Num.1 2008. 
[2] T. Cornsweet, H. Crane, “Accurate two-dimensional eye tracker using 
first and fourth Purkinje images”, Journal of the Optical Society of 
America, vol. 63, pp.921-928, 1973. 
[3] S. Baluja and D. Pomerleau, “Non-intrusive gaze tracking using 
artificial neural networks”,  School of Computer Science, CMU, CMU 
Pittsburgh, Pennsylvania 15213, Jan. 1994. 
[4] Shota Miyazaki1, Hironobu Takano1 and Kiyomi Nakamura, “Suitable 
Checkpoints of Features Surrounding the Eye for Eye Tracking Using 
Template Matching,” SICE Annual Conference, Step. 2007 
[5] Shinji Yamamoto and V.G. Moshnyaga, “Algorithm Optimizations for 
Low-Complexity Eye Tracking,” IEEE International Conference on 
Systems, Man, and Cybernetics, October 2009 
[6] Yali Li, Shengjin Wang, Xiaoqing Ding, “Eye/eyes tracking based on a 
unified deformable template and particle filtering,” Pattern 
Recognition Letters ,January 2010 
[7] Diego Torricelli, Michela Goffredo, Silvia Conforto, Maurizio 
Schmi,”An adaptive blink detector to initialize and update a 
view-based remote eye gaze tracking system in a natural 
scenario,”Pattern Recognition Letters,June 2009 
[8] Hirotake Yamazoe, Akira Utsumi, Tomoko Yonezawa, Shinji Abe, 
“Remote gaze estimation with a single camera based on facial-feature 
tracking without special calibration actions,” Proceedings of the 2008 
symposium on Eye tracking research & applications, pp. 26-28,March. 
2008 
[9] D. Torricelli, S. Conforto, M. Schmid and T. D’Alessio,”A 
neural-based remote eye gaze tracker under natural head 
motion”,Comput. Meth. Prog. Bio, pp. 66–78, 2008. 
[10] Hu-Chuan Lu, Guo-Liang Fang, Chao Wang and Yen-Wei Chen, “A 
novel method for gaze tracking by local pattern model and support 
vector regressor,” Signal Processing, vol. 90, issue 4, April 2010, 
pp.1290-1299. 
[11] Chiao-Wen Kao, Che-Wei Yang , Kuo-Chin Fan, Bor-Jiunn Hwang, 
Chin-Pan Huang,” AN Adaptive Eye Gaze Tracker System in the 
integrated Cloud Computing and Mobile Device,”ICMLC,July 2011 
[12] Chiao-Wen Kao, Che-Wei Yang, Kuo-Chin Fan, Bor-Jiunn Hwang, 
Chin-Pan Huang,” Eye gaze tracking based on pattern voting scheme 
for mobile device,” Instrumentation, Measurement, Computer, 
Communication and Control, Oct. 2011 
[13] Dan Witzner Hansen, Qiang Ji, “In the Eye of the Beholder: A Survey 
of Models for Eyes andGaze,” IEEE Transactions on Pattern Analysis 
and Machine Intelligence, vol. 32, No. 3, pp. 478-500, Jan. 2010. 
[14] Viola and Jones, "Rapid object detection using boosted cascade of 
simple features", Computer Vision and Pattern Recognition, 2001 
[15] Takeshi Mita, Toshimitsu Kaneko,Osamu Hori,”Joint Haar-like 
Features for Face Detection,”Proceedings of the Tenth IEEE 
International Conference on Computer Vision,2005. 
 
 
2 
 
and Transmission of Information and Intelligence -History and Outlook。本人並參與了 2 
場 session，分別是與語音辨識及無線通訊網路相關的主題。 
2011/12/06：晚間 10:45由洛杉磯國際機場搭乘長榮航空 BR-0015班機回台。 
 
 
二、與會心得 
1. Conference Program (大會論文發表) 
本人此次參加 2012 APSIPA，並以海報報告 Palmprint Verification using Gradient Maps and 
Support Vector Machines此篇論文，本論文中提出一個新的方法來避免各種在取像上的限
制，此外從這些蒐集的影像資料中擷取出感興趣的區塊，並利用此論文中提出之新的特
徵擷取方法來獲得影像中的特徵資訊，最後我們利用支援向量機的生物認證技術方法來
區分是否為本人，在支援向量機裡使用輻狀基底函數當作核心來訓練影像資料，最終得
到了 0.8344%的錯誤率。實驗結果驗證了我們方法的可靠性、可行性及可適性，我們相
信此論文中提出的論點是個值得信賴的方法。參與此次會議能獲得目前世界各國對於訊
號處理與視訊監控整合上的努力及應用，非常有助於計畫進展。 
2.建議與期許 
本人感謝國科會同意補助參加 APSIPA 2012國際學術研討會，APSIPA ASC是由亞太訊
號與資訊處理協會 APSIPA發起的一個權威國際峰會。每屆盛會都吸引超過 400 位來自
不同國家和地區學校、研究機構和相關產業界的著名專家和學者參與，展示訊號和資訊
處理領域的最新研究成果，討論科技和產業發展趨勢。會議論文水平高，EI收錄，優秀
論文推薦至 SCI 國際期刊發表，國際影響力高，有多國的與會學者，國內亦有多所學校
的學者專家參與會議，此次的研討會，讓本人不僅學習到影像處理領域相關的技術及應
用，也借由多場 sessions 的報告，學習到其它領域知識應用，如指紋辨識、掌紋辨識、
無線網路通訊和語音辨識等，藉由多面向的學習，激發出新的應用領域及改進目前既有
技術的方法，多場精采專業的演講，讓本人了解學界進展。本人此回參與國際學術研討
會，很榮幸的獲得其他學者在研究面上的提點，讓本人可以修正所提的方法，提高準確
度及效能，並從他人的研究成果中，得到一些新的研究想法，獲益良多。 
 
三、此次研討會中所攜回之資料 
1.研討會會議議程及論文摘要集一本。 
2.研討會會議論文全文光碟片一片。 
     
 
 
4 
 
接受通知 
 
 
Step 2: Border tracing. After the binarization step, the 
inner border tracing algorithm [11] is adopted to trace the 
contour of a palm shape. In the beginning, the starting point 
    is set at the middle point of the intersection line segment 
formed by the wrist and the bottom margin of the palm shape 
as shown in Fig. 1 (b). Then, all contour pixels of the palm 
shape are traced in the anti-clockwise direction. The eight 
neighboring directions are applied to describe the relative 
position of those traced points more precisely. The 
coordinates of contour pixels would be recorded sequentially 
as           . 
Step 3: Finger-webs locating. Since    and      have 
been determined in the previous steps, we can calculate the 
Euclidean distance between the middle wrist point and each 
contour point to draw a distribution diagram. The pattern of 
the diagram is similar to the geometric shape of a palm. To 
meet the speed and reliability requirements, a contour and 
curvature based method [12] is adopted to find the location of 
the four finger-webs (   ,    ,    , and     as indicated 
in Fig. 1 (b)) and the four finger tips. 
Step 4: ROI extraction. In this step, the locations of the 
second and the fourth finger-webs are selected as the datum 
points to determine the ROI from the palm region. One thing 
to be noted is that a user would put his/her hand at any 
position and direction with pegs-free device. So the final step 
is ROI normalization, and this step will need to handle 
rotation and scaling. The line    ⃡    is determined by connecting 
    and     as shown in Fig.1 (b). The palm image is 
rotated with angle   between    ⃡    and the vertical line. For the 
scaling part,     and     are employed to normalize each 
ROI to the same size. 
Fig. 1 (c) shows a normalized ROI. The proposed 
normalization process has two advantageous points. First, any 
displacement of a palm image can be adjusted. Second, the 
redundant information can be removed and the important 
information will be retained. 
 
       
     (a)                            (b)                            (c) 
Fig. 1. (a) Binarized palmprint image. (b) Location of starting point   and 
four datum points    ,    ,    , and    . (c) Normalized ROI. 
 
III. FEATURE EXTRACTION 
After pre-processing, the next step is to perform feature 
extraction. Usually, a palmprint is composed of principal line 
segments, wrinkles, and ridges. Principal line segments of a 
palmprint have been proven to be one of the most effective 
features in palmprint verification. By observing a segmented 
ROI carefully, line features possess lower grayscale values 
against the remaining regions. After the preprocessing stage, 
we describe how the principal line segments are extracted. 
A. Gradient Map Construction 
The line-like features can be considered as edges. 
Therefore, we can apply edge detectors to extract them. 
Conventional edge detection methods can be categorized into 
two types – gradient-based and laplacian-based. Since a 
Laplacian-based edge detector is more sensitive to noises, we 
adopt a gradient-based edge detector – the well-known Sobel 
edge detector to detect edges and then construct a gradient 
map. 
The direction of the gradient at point        can be 
calculated by four directional operators –   ,   ,   , and    
with the angle of   ,    ,    , and     , respectively. The 
function of these operators is expressed as follows: 
                    ,   (1) 
where    is the gradient map of decomposed images, and 
symbol   denotes the convolution operator. Every maximum 
received by applying the above four directional operators is 
selected as the representative gradient of its corresponding 
coordinate. 
B. Iterative Threshold 
In the constructed gradient map, many small wrinkles were 
detected together with regular edges. Usually, these wrinkles 
will affect the genuine matching and degrade the verification 
accuracy. Under these circumstances, we adopt the iterative 
threshold method proposed in [13] to eliminate the above 
mentioned redundant wrinkles. The detailed procedure is 
described as follows: 
Step 1: Initialize the threshold,      , by a random number 
ranging from 0 to 255 to segment the gradient map into two 
parts – feature points and non-feature points. 
Step 2: At the     iteration, calculate       and       as the 
mean of the non-feature points and the feature points, 
respectively. The threshold is determined by step 3 of the 
previous iteration.    and    can be calculated as follows: 
      ∑                                 , (2) 
      ∑                             , (3) 
where    and    represent the pixel number of non-feature 
points and feature points, respectively. 
Step 3: Set         as a new threshold for the next 
iteration.  
        (           )  .  (4) 
Step 4: Terminate the iteration process whenever  
              , otherwise return to step 2. 
The gradient map of an ROI can then be transformed into 
binary format and the principal feature points are preserved. 
C. Morphological Operation 
Although an iterative thresholding process can effectively 
eliminate some unwanted points, some holes and isolated 
blobs still remain. A morphological operator with square 
structure elements is adopted to solve the problem. By sliding 
the structure elements across an image and taking the 
convolution, one is able to process the image based on the 
provided morphological operators. 
 
(a) 
 
 
(b) 
Fig. 3. Verification results are evaluated by blue band. (a) ROC curve with 
different amount of negative samples. (b) Inside test and outside test. 
 
Unlike the matching method of Zhang et al. [14] by using 
the Hamming distance, our proposed method need to train the 
models. Finally, we compare the performance of our method 
with others which use the same database [15] - [17]. Among 
the three chosen methods, the best EER was 1.20%. However, 
our EER was as low as 0.8344% as shown in Fig. 4. 
 
 
Fig. 4. The EER value at blue band. 
VI. CONCLUSIONS 
This paper proposes an effective authentication method by 
constructing gradient map and only the important feature 
points are retained. SVM is then adopted to train a set of 
hyper-planes to separate different groups of samples with the 
one-against-all strategy. Since the different bands have 
different texture information, the performance is evaluated by 
using blue band. The proposed method received a low EER 
value as 0.8344%. The encouraging experimental results 
reveal that our proposed method is reliable for verification 
system. 
REFERENCES 
[1] T. Connie, A. Teoh, M. Goh, D. Ngo, “Palmprint recognition 
with PCA and ICA,” Proceedings of Image and Vision 
Computing New Zealand, pp. 227–232, 2004. 
[2] X. Wu, D. Zhang, K. Wang, “Fisherpalms based palmprint 
recognition,” Pattern Recognition Letters, Vol. 24, pp. 2829–
2838, 2003. 
[3] D. Hu, G. Feng, and Z. Zhou, “Two-dimensional locality 
preserving projection (2dlpp) with its application to palmprint 
recognition,” Pattern Recognition, 40, 1, pp. 339–342, 2007. 
[4] C. C. Han, H. L. Cheng, C. L. Lin, K. C. Fan, “Personal 
authentication using palmprint features,” Pattern Recognition, 
vol. 36, pp. 371–381, 2003. 
[5] C. L. Lin, Thomas C. Chuang, and K. C. Fan , “Palmprint 
Verification Using Hierarchical Decomposition”, Pattern 
Recognition, vol. 38, issue 12 , pp. 2639-2652, 2005. 
[6] D. Hu, W. Jia, and D. Zhang, “Palmprint verification based on 
principal lines,” Pattern Recognition, 41, 4, pp.1316–1328, 
2008. 
[7] A.W.K. Kong, D. Zhang, “Competitive coding scheme for 
palmprint verification,” in Proceedings of International 
Conference on Pattern Recognition, vol. 1, pp. 520–523, 2004. 
[8] A. Kumar, H. C. Shen, “Palmprint identification using 
PalmCodes,” in Proceedings of 3rd International Conference on 
Image and Graphics pp. 258–261, 2004. 
[9] X. Wu, D. Zhang, K. Wang, “Fisherpalms based palmprint 
recognition,” Pattern Recognition Letters, 24 (15) pp.2829–
2838, 2003. 
[10] C. L. Lin, and K. C. Fan,” Biometric Verification Using 
Thermal Images of Palm-dorsa Vein-patterns,” IEEE 
Transactions on Circuits and Systems for Video Technology, 
vol.14, no. 2, pp. 199-213, 2004. 
[11] M. Sonka, V. Hlavac and R. Boyle, “Image Processing, 
Analysis, and Machine Vision”, Second edition, PWS 
publishing, New York, 1999. 
[12] Lee T, Hollerer T and Handy AR, “Markerless Inspection of 
Augmented Reality Objects Using Fingertip Tracking,” IEEE 
International Symposium on Wearable Computers, pp. 83-90, 
2007. 
[13] T. W. Ridler and S. Calvard, “Picture Thresholding Using an 
Iterative Selection Method,” IEEE transaction on System, Man 
and Cybernetics, vol. 8, pp. 630-632, 1978. 
[14] David Zhang, Zhenhua Guo, Guangming Lu, Lei Zhang, and 
Wangmeng Zuo, “An Online System of Multi-spectral 
Palmprint Verification,” IEEE Transactions on Instrumentation 
and Measurement, vol. 59, no. 2, pp. 480-490, 2010.  
[15] V. Struc, N. Pavessic, “Phase congruency features for palmprint 
Veriﬁcation,” IET signal Process, vol. 3, issue 4, 2009. 
[16] Ali M., Ghafoor, M., Taj, I.A., and Hayat K., “Palm Print 
Recognition Using Oriented Hausdorff Distance Transform,” 
Frontiers of Information Technology (FIT), 2011. 
[17] Kumar, A., Hanmandlu, M., Madasu, V.K., and Vasikarla S., 
“A palm print authentication system using quantized phase 
feature representation,” Applied Imagery Pattern Recognition 
Workshop (AIPR), 2011. 
 
 
2 
 
and Transmission of Information and Intelligence -History and Outlook。本人並參與了 2 
場 session，分別是與語音辨識及無線通訊網路相關的主題。 
2011/12/06：晚間 10:45由洛杉磯國際機場搭乘長榮航空 BR-0015班機回台。 
 
 
二、與會心得 
1. Conference Program (大會論文發表) 
本人此次參加 2012 APSIPA，並以海報報告 Palmprint Verification using Gradient Maps and 
Support Vector Machines此篇論文，本論文中提出一個新的方法來避免各種在取像上的限
制，此外從這些蒐集的影像資料中擷取出感興趣的區塊，並利用此論文中提出之新的特
徵擷取方法來獲得影像中的特徵資訊，最後我們利用支援向量機的生物認證技術方法來
區分是否為本人，在支援向量機裡使用輻狀基底函數當作核心來訓練影像資料，最終得
到了 0.8344%的錯誤率。實驗結果驗證了我們方法的可靠性、可行性及可適性，我們相
信此論文中提出的論點是個值得信賴的方法。參與此次會議能獲得目前世界各國對於訊
號處理與視訊監控整合上的努力及應用，非常有助於計畫進展。 
2.建議與期許 
本人感謝國科會同意補助參加 APSIPA 2012國際學術研討會，APSIPA ASC是由亞太訊
號與資訊處理協會 APSIPA發起的一個權威國際峰會。每屆盛會都吸引超過 400 位來自
不同國家和地區學校、研究機構和相關產業界的著名專家和學者參與，展示訊號和資訊
處理領域的最新研究成果，討論科技和產業發展趨勢。會議論文水平高，EI收錄，優秀
論文推薦至 SCI 國際期刊發表，國際影響力高，有多國的與會學者，國內亦有多所學校
的學者專家參與會議，此次的研討會，讓本人不僅學習到影像處理領域相關的技術及應
用，也借由多場 sessions 的報告，學習到其它領域知識應用，如指紋辨識、掌紋辨識、
無線網路通訊和語音辨識等，藉由多面向的學習，激發出新的應用領域及改進目前既有
技術的方法，多場精采專業的演講，讓本人了解學界進展。本人此回參與國際學術研討
會，很榮幸的獲得其他學者在研究面上的提點，讓本人可以修正所提的方法，提高準確
度及效能，並從他人的研究成果中，得到一些新的研究想法，獲益良多。 
 
三、此次研討會中所攜回之資料 
1.研討會會議議程及論文摘要集一本。 
2.研討會會議論文全文光碟片一片。 
     
 
 
4 
 
接受通知 
 
 
Palmprint Verification using Gradient Maps and 
Support Vector Machines 
Chun-Wei Lu
†
, Ivy Fan
†
, Chin-Chuan Han
*
, Jyh-Chian Chang
#
, Kuo-Chin Fan
††
 and H.Y. Mark Liao
†
 
†
Inst. of Info. Sci., Academia Sinica, Taiwan 
††
Department of Comp. Sci. and Info. Eng., National Central University, Taiwan 
*
Department of Comp. Sci. and Info. Eng., National United University, Taiwan 
#
Department of Comp. Sci. and Info. Eng., Chinese Cultural University, Taiwan 
 
 
 
Abstract — With the urgent demand in information 
security, biometric feature-based verification systems have been 
extensively explored in many application domains. However, the 
efficacy of existing biometric-based systems is unsatisfactory and 
there are still a lot of difficult problems to be solved. Among 
many existing biometric features, palmprint has been regarded 
as a unique and useful biometric feature due to its stable 
principal lines. In this paper, we proposed a new method to 
perform palmprint recognition. We extract the gradient map of 
a palmprint and then verify it by a trained support vector 
machine (SVM). The procedure can be divided into three steps, 
including image preprocessing, feature extraction, and verification. 
We used the multi-spectral palmprint database prepared by 
Hong Kong PolyU [14] which included 6000 palm images 
collected from 250 individuals to test our method. The 
experimental results demonstrate our proposed method is 
reliable and efficient to verify whether the person is genuine or 
not. 
I. INTRODUCTION 
Recently, personal authentication has become a vital and 
highly demanded technique as a foundation of many 
applications, like security access system, time attendance 
system, and forensics science. In the past decade, many 
biometric features have been used for identification purpose, 
including fingerprint, iris, retina, face, hand geometry, and 
palmprint. A palmprint, as one of the extremely important 
biometric features, has attracted great attention in these years 
due to its reliable line structure, ridge structure, and texture 
information. Combining these features together makes a 
powerful feature representation scheme. However, there is 
still great room to improve the accuracy, the efficiency, and 
the power of anti-spoofing.  
In order to deal with the aforementioned problems, a large 
number of researches have been proposed in the past decade 
[1-10] [14-17]. Algorithms related to palmprint feature 
extraction can be categorized into three types: 1) subspace 
feature learning; 2) line-like feature extraction; and 3) texture-
based coding. The algorithms fall into the subspace learning 
category include principal component analysis (PCA) [1], 
fisher’s linear discriminant analysis (LDA) [2], and locality 
preserving projection (LPP) [3]. The algorithms belonging to 
this category consider a palmprint image as a whole and the 
representation of the image is accomplished by performing 
dimensionality reduction. Although a subspace-based 
representation scheme is suitable for representing palmprint 
texture information, the requirement of establishing a large 
database is difficult for most of the researchers working in 
this field. 
As to the line-like feature representation scheme, one needs 
to detect and extract salient line segments from a palm image. 
The line features of a palm image are, in fact, the most 
significant features that can be used to characterize a palm 
image. Han et al. [4] use Sobel and morphology operations to 
extract the palm features. Lin et al. [5] extract the principal 
palmprint features by applying the hierarchical decomposition 
mechanism to the region-of-interest (ROI) regions, which 
include directional and multi-resolution decompositions. 
Zhang et al. [6] proposed a modified finite random transform 
to explicitly extract the principal lines even though a 
palmprint image contains many long and strong wrinkles. 
Another direction of palmprint recognition research is to 
perform texture-based coding. Usually, texture-based features 
can better characterize a palmprint and they are robust to 
illumination change. In [7], Zhang et al. proposed a new 
palmprint coding approach which extracts the orientation 
information by Gabor filter and encodes the orientation 
response into a 3-bit competitive code. 
In this paper, we propose a four-step preprocessing process 
to identify the location of a normalized ROI. The details of 
feature extraction are described in order to construct the 
gradient map. SVM is employed and it is equipped with the 
one-against-all strategy to verify each individual palmprint. 
Finally, some experimental results are shown to demonstrate 
the effectiveness of the proposed approach. 
II. PREPROCESSING 
The whole procedure is described in details as follows: 
Step 1: Binary thresholding. To extract the foreground 
objects from the background, binarization is an essential step 
to convert a grayscale image into a binary form. By applying 
the mode method [11], the histogram of a grayscale image is 
analyzed first to automatically determine a local minimum 
between two local maxima as a threshold. When a grayscale 
value is lower than this threshold, we set the pixel value as 
‘0’; otherwise, the pixel value is set ‘255’ to segment the palm 
shape as shown in Fig. 1 (a). 
The two operations that we introduce to operate on the 
target image are dilation and erosion. By applying these two 
morphological operators, one is able to filter out noises and 
non-feature points. 
D. Count Filter 
After performing the above three steps, some salient points 
are retained in the processed ROIs, and a count filter [5] is 
employed to construct feature vectors. A count filter is 
suitable for our system, because it can characterize the pattern 
of principal palmprint features. The operation of this filter is 
expressed as follows: 
                    (         )          
 
 
 
 
 
 ,(5) 
where         is the number of           sitting inside the 
range of 
 
 
 
 
 
 block. 
After the operation of a count filter, we decompose the 
processed ROIs into several non-overlapping blocks, and 
calculate the number of feature points in each block to 
construct feature vectors. For example, if we decompose an 
image into 16 divisions along x and y axis, then the total 
number of blocks will be 16 * 16 = 256 (as shown in Fig. 2) 
and the dimension of a feature vector will be 256. Obviously, 
it can solve the offset problem of feature points in images. 
 
 
Fig. 2. The construction of gradient map with 16*16 blocks, and the 
dimension of a feature vector will be 256. 
 
IV. MODELING AND VERIFICATION 
The support vector machine (SVM) is an effective tool for 
data classification. The goal of SVM modeling is to find a set 
of appropriate hyper-planes in a high dimensional space and 
then use these hyper-planes to classify input samples. The 
middle of an SVM margin is an optimal hyper-plane to 
separate two classes. Therefore, this margin provides the 
largest distance from sample points to it. All points sitting on 
the boundaries are called support vectors which are calculated 
by numerical optimization during the training phase. While 
the samples are non-linearly separable, these samples would 
be mapped to a higher dimension feature space by a non-
linear function. Polynomial and radial basis function (RBF) 
are two non-linear kernel functions that are commonly used. 
Since the samples taken in our problem domain are high 
dimensional features, we project these samples into a two 
dimensional feature space by principal component analysis. 
By analyzing the data distribution empirically, we choose 
RBF as our kernel functions. 
A typical SVM is a binary classifier. However, the problem 
we encounter is a multi-classification problem. Therefore, we 
need to train a multi-class classifier. The way we adopted is to 
build a classifier of this sort through combining more than 
two binary classifiers and make them work together. Since the 
problem is an n-class problem, we adopted the one-against-all 
approach to construct a model. Since the one-against-all 
approach is exclusive, it can prevent the intruders from 
logging into the system. Therefore, we use it to evaluate the 
performance of the proposed method. 
V. EXPERIMENTAL RESULTS 
A. PolyU Multi-spectral Palmprint Database 
The PolyU multi-spectral palmprint images were collected 
from 250 volunteers. The database contains 195 males and 55 
females and their age distribution was from 20 to 60 years old. 
The palmprint images were collected at two different time 
periods within a window of 9 days. In each time period, all 
individuals were asked to provide 6 images for each of the left 
and right palms, respectively. Therefore, the total number of 
images on each band was 6,000 from 500 different palms. 
During the same time period, four images were collected from 
four different bands at one shot, including red, green, blue, 
and near-infrared illumination. The resolution of the original 
images in the database was 352 * 288 with 256 gray-levels. 
B. Palmprint Verification by Single Band 
Among the four-band images of the PolyU palmprint 
database, we chose the blue band to verify our proposed 
method because it has strong line structures. 
We use three values to evaluate the efficiency of our 
proposed method. First, the false acceptance rate (FAR) 
which measures the frequency of falsely accepted imposters. 
Second, the false rejection rate (FRR) which measures the 
frequency of falsely rejected genuine clients. Third, the equal 
error rate (EER) is defined as a value where FAR equals to 
FRR.  
In the first experiment, we applied three experiment sets 
with different amount of training samples. The first set which 
contained 31 individuals was selected at random as training 
data, and the rest of 469 individuals were used as testing data. 
The second and third set contained 51 and 101 individuals, 
respectively. The results of receiver operating characteristic 
(ROC) curves were evaluated by one-against-all strategy 
shown in Fig. 3 (a), it indicates the more training data would 
result in lower FAR and FRR. 
C. Anti-spoofing Verification 
A good biometric verification system should have 
capability of anti-spoofing to prevent the intruders from 
logging into the system. The second experiment is described 
as follows: we split the testing data into two parts, the inside 
group and the outside group. In the inside group, the users 
have registered in the database, and the outside group 
represents some intruders. In this experiment, 100 individuals 
were picked in random as genuine to train the models. The 
inside test verified the accuracy of genuine attempt, and the 
outside test was employed to evaluate the efficiency of 
intruder prevention. As shown in Fig. 3 (b), the EER value of 
inside test was as low as 0.5437% and the value of outside 
test was 1.193%. 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/09/16
國科會補助計畫
計畫名稱: 人數計算、性別年齡辨識與眼神注視偵測應用於櫥窗廣告系統之關注度估測
計畫主持人: 范國清
計畫編號: 99-2221-E-008-069-MY3 學門領域: 圖形辨識
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
在本計畫中，我們建構出一性別辨識系統與人數計算系統，選取左眼、右眼、
鼻子與嘴巴等四個五官區塊，以及人臉外部的頭髮範圍，然後各別擷取 PCA 特
徵來訓練 Random Forest 性別分類器，同時擷取五官區塊的 HOG 特徵，輸入 SVM
訓練各區塊的遮蔽分類器，最後利用適應型的決策融合，將性別分類結果與遮
蔽情況做結合，選擇未遮蔽且可靠的五官區塊，並且適時的加入頭髮特徵來判
斷性別。相較於其他人臉性別辨識的研究，本適應型性別辨識採用動態選擇五
官區塊的方法，使性別辨識能調適於人臉部分遮蔽的情況。而進出口行人數量
的自動統計系統，可應用於展覽會場、百貨公司等需要計算人數的公共場所，
同時，我們利用影像處理和電腦視覺的技術，達到自動計算進出人數的功能，
可同時滿足節省人力資源以及讓進出的行人自由行走的目的，由此可知，本計
畫的研究成果，不僅具備學術性，同時相當具有實用性，未來可為相關業者提
供技術轉移。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
