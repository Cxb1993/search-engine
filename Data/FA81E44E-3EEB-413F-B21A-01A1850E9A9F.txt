 2
environments. In addition, the 100 people 
speech corpus have also been collected by the 
PDA device. 
 
二、計畫緣由 
越來越多的語音辨識系統充斥著在我們
生活週遭的環境，語音辨識技術廣泛應用於
各個地方(電話語音服務、無障礙環境控制、
語音檢索服務..)，其中更以最近幾年開始大行
其道的智慧型行動裝置，智慧型行動裝置包
含了PDA、Palm、Smart Phone、PDA Phone
等等這些產品，其應用更是無所不在的在我
們日常生活中提供了許多方便的功能，在我
們觀賞的許多科幻電影中，許多的電腦都可
以直接和人類作互動式的交談，人們只要簡
單的透過聲音，就可以達到和電腦的互動，
而其中電腦要懂得人類發出的語音訊號所代
表的意義，需要進行的步驟，即是語音辨識。
而語音辨識相關之技術與其知識，則是我們
要深入研究以及探討的領域。 
現今當紅之嵌入式系統，由於行動裝置
體積的小巧，硬體所提供之傳統輸入方法，
大致上可以分為使用裝置所提供控制按鈕、
手寫輸入以及觸摸螢幕方式等輸入方式，相
較於語音輸入來說，就顯得不是那麼的直接
方便，透過語音之輸入方式，主要的關鍵技
術為「語音辨識技術」。透過語音辨識技術，
可 以 將 輸 入 的 語 音 訊 號 解 析 為 文 字
(Speech-to-Text)，延伸出的應用範圍非常廣
泛，如：聲控電話撥號、查號台、氣象查詢、
新聞檢索...等等諸多的服務，都可以經由使用
者「說」的動作，輕鬆的使用應用系統所提
供的服務。 
 
三、問題描述 
雖然嵌入式系統功能越來越強大，不過
以目前市面上常見之相關產品，其運算速度
相較於個人電腦(PC)來說，還是慢了許多。
語音辨識技術之運作過程需要大量的「浮點
(Float-Point)數學運算」，而浮點數學運算之
處理相較於「整數(Fixed-Point)數學運算」之
速度，要慢上許多，本篇論文即是在研究使
用整數數學運算方式來取代原本語音辨識過
程中需要大量使用浮點數學運算方式。 
語音辨識的領域非常廣泛，而且依照其
應用，實作方面也會有不同，在此我們著重
於研究發展出一套可用於行動裝置上之語音
辨識引擎。由於在行動裝置上之運算能力，
比起一般桌上型電腦之運算能力，要薄弱的
許多，所以在這裡我們也會研究如何有效率
的提升行動裝置上語音辨識引擎之運算速
度，以求能達到接近於桌上型電腦之運算速
度和語音辨識率。 
 
四、語料收集 
 建立初步的華語以及台語（閩南語）的 PDA
語音資料庫。這份語料是 2005 年由長庚大學
多媒體訊號處理實驗室錄製的一份語者不相
關(Speaker Independent, SI)語料，語料的規模
以及在訓練語句的部分和 2001 年所錄製的
TW01 語料相同，差別在於 TW01 的收集是使
用品質較好的麥克風，而 TW05 使用 PDA 收
集得到。TW05 錄製的環境與 Gang Corpus 相
同，是模擬辦公室無雜訊干擾的情況。下表
是關於此份語料的統計資。 
 
 訓練語料 測試語料 
語料 TW05 TW05_test 
語言 台語 
錄製媒介 PDA 
取樣頻率 16KHz 
錄音人數 男女各 50 位 男女各 5 位
容量 2.68GB 40.2MB 
長度 25hr 22min 
句數 47207 1000 
每句平均音節 2.86 2.99 
 4
159744.159742975.0 14 ≅=×=a ，由於 )(nS
為 16 bits 擴增到 32 bits 之資料，所以並不
用擔心資料溢位的問題，而整個式子重新改
寫成為： 
1414
2 2))1(159742)(()( −×−×= nSnSnS ，其中
frameSizen ≤≤2  
14
2 2)(410)( nSnS ×= ，其中
4106.4092025.0,1 14 ≅=×=n 。 
C. 漢明窗 (Hamming Window) 
乘 上 漢 明 窗 後 的 訊 號 為 ：
)()()( nWnSnS ×=′ ，但由於 )(nW 的值為浮點
數，所以我們必須建立一個表，而裡面的數
值為 hmSvnW ×)( ，在這邊我們取 hmSv為 214
來建立整數型態的漢明窗，我們可由圖 5-3，
得知我們取得不同 hmSv 值所建立出的漢明
窗值的分佈，並由圖 5-4 可判斷出漢明窗鑑別
度與離散度的差異。最後我們將原來的
)()()( nWnSnS ×=′ 改寫成為： 
142)()()( nHamTablenSnS ×=′  
因為漢明窗整數表已經被放大 214 倍，所以在
與訊號 ( )S n ∞相成後還要再除以 214來還原其
值。 
 
圖 5-3 不同倍率放大之整數漢明窗值的分佈
圖  
0 100 200 300 400
0
0.5
1
1.5
2
index of point
va
lu
e 
of
 e
ac
h 
po
in
t
Hamming Window scale by 21
0 100 200 300 400
0
0.5
1
1.5
2
x 104
index of point
va
lu
e 
of
 e
ac
h 
po
in
t
Hamming Window scale by 214
 
圖 5-4 放大倍率 21 與 214 的圖形 
 
D. 快速傅立葉轉換 (FFT) 
在 FFT 的步驟中，我們可以得知 FFT 的
公式如下： 
1
0
2
( ) ( ) , 0 1,
N
kn
N
n
jk N
N
X k x n W k N
where W e π
−
=
−
= ≤ ≤ −
=
∑  
其中， )sin()cos( kjke jk −=− ，在這邊我
們必須建立兩個 Table 給 FFT 使用，分別為
Cos Table 與 Sin Table，由於要經由 Cos 及 Sin
運算的值，是可以預先運算而得到的，我們
就可以根據這些值，去建立相對應的表來運
算之，而不需要去建立一個完整的 Cos 及 Sin
對 照 表 。 圖 5-5 為 DCT 演 算 法 ，
圖 5-5 DCT Algorithm in MFCC Procedure 
我們可得需要經過 Cos 運算的值為： 
pi_factor = PI / numChans; 
FOR cepNumCOunter = 1 TO numCepCoef DO 
cepstral[cepNumCounter] = 0.0f; 
Ftemp = cepNumCounter * pi_factor; 
FOR chanNum = 1 TO numChans DO 
cepstral[cepNumCounter] +=  
fbank[chanNum] * cos(ftemp * (chanNum – 
0.5)); 
    END DO 
 6
 Round 1 Round 2 Round 3 Round 4
Index 1 2
π  4π  8π  16π  
Index 2 π  2π  4π  8π  
 Round 5 Round 6 Round 7 Round 8
Index 1 32
π  64π  128π  256π
Index 2 16
π  32π  64π  128π
表格 5-1 Input Value for Calculating Sin in 
FFT Sub-routine 
 
圖 5-8 Sin Table 結構圖 
E.  三角帶通濾波器  
在尚未計算 ln 之前的訊號，經過三角帶
通濾波器的方程式如下： 
1
2
0
[ ] [ ] [ ] , 0
N
a m
k
S m X k H k m M
−
=
= < ≤∑ … 
5-2，其中 
⎪⎪
⎪
⎩
⎪⎪
⎪
⎨
⎧
+>
+<≤−+
−+
≤<−−−
−−
−≤
=
]1[,0
]1[][,
])[]1[(
)]1[(
][]1[,
])1[][(
])1[(
]1[,0
][
mfk
mfkmf
mfmf
kmf
mfkmf
mfmf
mfk
mfk
kHm  
與此我們必須要建立一個 Filter Table 來
儲存 ][kH m 的值，由方程式(5-2)我們可以把
2|][| kX a 拆 解 為 22 ][][ kXkX ir + ， 其 中
][][][ kXjkXkX irr ⋅+= ，為了避免 2][kX r 及
2][kX i 會造成資料溢位的情形，所以我們將
][kX r 及 ][kX i 各別除以 16： 
4][
4][
>>=
>>=
kX
kX
i
r  
然 後 我 們 可 以 推 得
4][][|][| 222 <<+= kXkXkX ira ，而 Filter Table 
我們則是建立了一個大小為 256 (針對一個頻
道)，並且將值放大 211 的 Filter Table，圖 5-9、
圖 5-10 顯示在 TIMIT 及 TCC300 Corpus 
中 2|][| kX a 根據不同放大倍率的 Filter Table
發生資料溢位的機率(只要頻譜乘以三角濾波
器的一點發生溢位)。 
0 1024 2048 2896 3327 4096 8192 16384
0
10
20
30
40
50
60
70
timit corpus EK maximum value with loWt table scale by 1024~8192
0% 0% 0% 0%
19.9219%
58.9844%
o
ve
r 
flo
w 
tim
es
 p
ro
ba
bi
lty
 (%
)
loWt table scale value  
圖 5-9 TIMIT 語料中 2|][| kX a 與不同倍率
Filter Table 產生溢位之機率圖 
儲存 Weight 
Cepstrum 所需的 
Sin 值 
儲存 FFT 副函式
所需的 Sin 值
儲存 FFT 所需的 
Sin 值 
12 個 Integer 
16 個 Integer 
2 個 Integer 
 8
必須要對 Y 軸的值放大並取整數，例如：
TCC300 在這邊必須還要乘上 896 來放大之。
而 X 軸的對應也不再是之前的一一對應，所
以在這邊我們必須要再利用 Binary Search 來
找出落在某範圍的 x 值所對應到的 y 值，因
此我們在計算 )ln(x 的時間，相對會較之前提
升一點，從 )1(Θ 提升到 )(log2 nΘ ，雖然所需
時間提升了一些，不過很明顯的效應是表的
大小降低很多，以此例我們建出的表大小約
是 15526 元素，約佔記憶體空間 0.06MB，與
之前的 1.08MB，約省下了 1.02MB 的空間，
降低了約 90%，如表格 所示。 
0 200 400 600 800 1000 1200
0
1
2
3
4
5
6
7
X value
Y 
va
lu
e
y = ln(x)
 
圖 5-12 整數 )ln(x  Table X軸與Y軸的相對
應關係 
 
查詢時間 表的元素
個數(總合) 
表的
個數 
表的大
小 
一般的
建表方
式 
)1(Θ  283211 
3 1.08MB
我們的
建表方
式 
)(log2 nΘ  15526 
1 0.06MB
表格 5-5 一般與我們的建表方式比較 
3.8 開平方根表格建立 
平方根的 X 軸與 Y 軸對應的趨勢圖基本
上與計算 ln的圖形有相當程度的類似，只是
開平方根的坡度相較之下比較陡許多，因此
我們也可以運用相同的方式來建立整數型態
的開平方根表 (Square Root Table)，而查表的
搜尋方式也是運用 Binary Search 來行運算
之。圖 5-13 顯示整數型態的開平方根 X 軸與
Y 軸相對應的關係。 
 
0 0.5 1 1.5 2 2.5 3
x 108
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
X value
Y 
va
lu
e
y = sqrt(x)
 
圖 5-13 整數 )(xSqrt  Table X 軸與 Y 軸的
相對應關係 
六、辨識核心方面 
本研究中實驗的硬體測試平台，分別為
個人電腦(PC)以及個人數位助理(PDA)，規格
如下表所示： 
 
 ASUS L3800筆記型
電腦 
宏達XDA II Pocket 
PC 
CP 等級 Intel Pentium 
1.7Ghz 
Intel XScale 
400Mhz 
內建記憶
體 
512 MB 128 MB 
作業系統 Windows XP Pro. PPC Phone Edition 
2003 
 
我們有系統的設計了一系列的實驗，以
求能在穩紮穩打的狀況下，將語音辨識核心
實作於嵌入式系統上，以下依序介紹每個階
段中我們所進行的實驗背景以及成果。 
 
 10
計初步構想為：將語音辨識過程中需要使用
浮點數學運算改為整數數學運算、以及盡量
減少使用乘法和除法之運算。 
 
z 嵌入式系統語音辨識引擎之建立 
本節之主要目的，在於建立 PDA 上之語
音辨識引擎系統，在本實驗中所建立的語音
辨識引擎，亦會當作之後相關實驗數據的基
本參考點(base-line)，在這個實驗中，語音辨
識過程使用的數學運算，我們會採用原本實
作語音辨識引擎程式碼時，所使用的浮點資
料型態所定義的運算程式碼。以下分別介紹
目前實驗室(多媒體信號處理實驗室，MSP 
Lab)發展用於個人電腦上的語音辨識軟體開
發套件：「福爾摩莎語音辨識軟體開發套
件」，及本研究的成果「福爾摩莎嵌入式系
統語音辨識軟體開發套件」 
我們定義一些運算式，用以分析統計本
論文中實驗的數據，定義如下： 
 
( )
( )
timeF :
timeF - timeIIncreasing rate of  computational speed : timeI
AccI - AccFReduction rate of  accuracy : AccF
浮點運算辨識花費時間,  timeI :整數運算辨識花費時間
AccF :浮點運算辨識率, AccI :整數運算辨識率
速度上升比率:
正確率下降比率:
 
接着我們進行建立基本參考點之實驗。
整個語音辨識過程中的步驟，我們將之分為
「 語 音 訊 號 特 徵 參 數 擷 取 (Feature 
Extraction)」及「聲學模型比對搜尋網路
(Modeling Search)」這兩大部分，我們將上一
節中所設定的相關參數套用在本節的實驗
中，結果如下表格所示： 
 
 
測試
語料
(句)
語料長
度 
(秒) 
特徵擷取
(秒) 
比對搜尋 
(秒) 
辨識成功 
(筆) 
100 106.8 301.591 1962.964 80
辨識總時間：2264.555 秒 
13.32%(特徵擷取所佔時間比)=特徵擷取/辨識總時間 
86.68%(比對搜尋所佔時間比)=比對搜尋/辨識總時間 
 
z 實驗結論 
在實驗的結果中，我們可以觀察到以下
幾個現象：語音訊號特徵參數擷取花費了
301.591 秒，所佔的時間比例是整個辨識過程
中的 13.32%，比對聲學模型及搜尋樹狀網路
花費了 1962.964 秒，則佔了整個辨識過程時
間比例的 86.68%，而語音辨識的正確率為
80%。 
 
z 高斯混合機率求取加速 
上一節的實驗結果我們得知，使用 PDA
進行語音辨識的運算速度十分緩慢，語音辨
識速度所需要花費的時間幾乎是到了無法忍
受的地步，於是我們初步構想為：將語音辨
識過程中的數學式予以簡化。首先我們由觀
測狀態機率的參數求取數學式來着手，以下
是基本的數學式定義： 
;
11
( ) ( ; , )
srS M
jsmj t jsm st jsm
ms
b o c N o Vμ
==
⎡ ⎤= ⎢ ⎥⎣ ⎦∑∏
v v v
 
但是實際在運算上，上面的數學式電腦
是無法處理的，這是由於在計算過程中，計
算數值有可能是非常接近零或者非常小的浮
點數值，對於電腦程式語言來說，程式語言
定義的資料型別所能表達數值的精準度範圍
不夠，在運算的過程中，數值有可能直接變
成零，導致計算的結果不正確。 
所以我們將數學式兩邊都取 log，寫成的式子
如下： 
 
 12
音模型的比對和搜尋演算法，修改為整數資
料型態的運算，下文為上一節實驗中我們修
改取出最大高斯混合機率值之後再計算觀測
狀態機率之數學式： 
 
( ) ( ) ( )( );1log ( ) max log log ( ; , )jsmj t jsm st jsmm Mb o c N o Vμ≤ ≤= +v v v 而其
中高斯混合機率之數學式為： 
11 ( ) ( )
21( ; , )
(2 ) | |
T
o V o
n
N o V e
V
μ μμ π
−− − −=
v v v vv v  
這些參數決定了此密度函數的特性，如函數
形狀的中心點、寬窄及走向等等屬性。 
 
我們將狀態觀測機率兩邊取 log 之後，數學
式為： 
 
( ) ( ) 1; 1 1log ( ; , ) log (2 ) | | ( ) ( )2 2 TnN o V V o V oμ π μ μ−= − − − −v v v v v v  
接着我們建立一些符號來代表數學式，以便
在實作程式碼時增加程式的可讀及維護性： 
( )log (2 ) | |nGConst Vπ=  
ddvariance v=  
 
將上述數學式整理，最後得到的我們實
際運用於程式中的數學式為： 
 
21 ( )log ()
2
oN GConst
variance
μ−= − ∑  
 
最後推導出來的式子(4.2-4)得知，若我們
要將數學式中的浮點資料型態數值改為整數
資料型態數值，在這裡我們可使用的方法
為：將數學式乘上一個可於程式中動態設定
調整的放大倍率，該可調整的放大倍率：
Scale。依照此原則，數學式我們可以寫成： 
 
21 ( * * )log * *
2 *
o Scale ScaleN Scale Scale GConst
variance Scale
μ⎛ ⎞−= − ⎜ ⎟⎝ ⎠∑  
 
我們將訓練好的聲學模型包含的參數，
在前置作業的時候，先取 log 函數以利之後的
計算，然後在程式一開始執行時就載入進行
語音辨識時所需要的相關語音資料，意即是
程式初始化的時候就已經將聲學模型中的相
關參數乘上該放大倍率，在使用者執行語音
辨識時不需要額外再進行放大倍率的動作，
也就是說實際在進行語音辨識時，只有 o (語
音特徵向量)需要放大，使得速度更加快不少。 
 
z 整數運算語音辨識實驗 
透過上一節中所推導出來的數學式，我
們進行整數運算的語音辨識實驗，我們將數
學式中的 Scale 設定為 10，以及設定在 Viterbi 
Search 中的網路加速策略中我們給的階段式
限制(稍後章節介紹)門檻值為 800，得到的實
驗結果與之前的實驗數據比較如下所示： 
 
實 驗 方 法
比對搜
尋(秒) 
辨識成
功率 
速度上
升比率 
正確率下
降比率 
基本浮點
語音辨識
效率 
1962.96 80%   
高斯混合
機率求取
加速 
1695.12 80% 15.8% 0% 
十倍放大
整數語音
辨識 
1695.12 80% 15.8% 0%
 
我們可以看到辨識速度很明顯的提升，
與基本浮點語音辨識效率表的辨識方法來
看，我們可以看到十倍放大整數運算語音辨
識，速度上升比率 457.38%，而正確率下降比
 14
10-2400 1084.743 79% 81.00% 1.00%
10-2800 1246.712 79% 57.00% 1.00%
10-3200 1395.103 79% 41.00% 1.00%
10-3600 1503.853 79% 31.00% 1.00%
10-4000 1604.466 79% 22.00% 1.00%
10-4400 1695.651 79% 16.00% 1.00%
10-4800 1761.074 79% 11.00% 1.00%
10-5200 1814.706 79% 8.00% 1.00%
10-5600 1856.348 79% 6.00% 1.00%
10-6400 1909.325 79% 3.00% 1.00%
 
由上表我們可以觀察到，給定的門檻值
越小的時候速度上升比率會上升，正確率下
降比率也會隨着上升，例如給定的門檻值為
1600 的時候，速度上升比率及正確率下降比
率分別為 170.00%和 2%，若門檻值為 800 的
時候，速度上升比率及正確率下降比率則分
別為 457.00%和 12.5%。 
 
經過不斷的增加門檻值，到最後正確率
下降比率會在 1.00%時趨於穩定，我們以圖表
的方式呈現出來： 
 
0.00%
100.00%
200.00%
300.00%
400.00%
500.00%
600.00%
700.00%
800.00%
900.00%
1000.00%
10-
040
0
10-
120
0
10-
200
0
10-
280
0
10-
360
0
10-
440
0
10-
520
0
10-
640
0
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
35.00%
40.00%
速度上升比率
正確率下降比率
 
 
經過本實驗後，我們知道將運算改為整
數運算之後，放大的倍率以及在搜尋網路中
使用階段式限制寬度的加速方法之間對應的
關係，在下節中，我們會嘗試使用更多不同
整數放大倍率來探討其對應的關係。 
 
z 多倍率放大加速策略實驗 
接着我們嘗試以不同的整數放大倍率來
進行實驗，我們初步的構想為：由於在語音
辨識過程中，我們將相關的浮點數值經過倍
率放大，正常來說放大的倍率越高，等於解
析度越高，則辨識率會更接近原參考點
(base-line)之值。以下我們分別將放大倍率調
整為五十倍及一百倍來證明我們的論點。 
 
下表為五十倍率放大之不同門檻值實驗
結果： 
 
五十倍 
放大實驗 
比對搜尋
(秒)
辨識 
成功率 
速度 
上升比率 
正確率
下降比率
50-0000 29.458 3% 6564.00% 96.00%
50-0400 57.681 4% 3303.00% 95.00%
50-0800 87.453 16% 2145.00% 80.00%
50-1200 124.267 29% 1480.00% 64.00%
50-1600 156.187 43% 1157.00% 46.00%
50-2000 182.187 48% 977.00% 40.00%
50-2400 206.117 60% 852.00% 25.00%
50-2800 240.442 64% 716.00% 20.00%
50-3200 263.795 70% 644.00% 12.00%
50-3600 297.336 70% 560.00% 12.00%
50-4000 334.175 70% 487.00% 12.00%
50-4400 373.821 72% 425.00% 10.00%
50-4800 405.988 72% 384.00% 10.00%
50-5200 431.351 73% 355.00% 9.00%
50-5600 481.675 74% 308.00% 8.00%
50-6400 568.971 76% 245.00% 5.00%
50-6800 597.33 78% 229.00% 2.00%
50-7200 641.773 77% 206.00% 4.00%
50-7600 682.338 77% 188.00% 4.00%
50-8000 724.799 77% 171.00% 4.00%
50-8800 807.102 79% 143.00% 1.00%
50-9200 843.081 79% 133.00% 1.00%
50-9600 878.582 79% 123.00% 1.00%
50-10000 915.307 79% 114.00% 1.00%
 16
比率趨於穩定時(0.00%)，速度上升
比率為 166.00%。 
 
此現象和我們一開始預期的相同：「語
音辨識過程中，我們將相關的浮點數值經過
倍率放大，正常來說放大的倍率越高，等於
解析度越高，則辨識率會更接近原參考點
(base-line)之值。」 
 
七、計畫成果自評 
 
在本計畫中，我們已經完成以下的工作
目標： 
經由不斷實驗所累積經驗及知識所實作出來
的ForSR-E SDK，已經為本實驗室於智慧型裝置
中開發語音辨識引擎打下了基礎，而且對於語音
辨識過程中由於浮點數學運算所花費的大量時
間，也改以使用整數數學運算來加速了智慧型裝
置進行語音辨識時所需要花費的時間；透過
ForSR-E SDK，我們也開發出一套於智慧型裝置
的系統，使得我們在學習語音辨識相關理論時，
可以結合實際的程式碼，用來快速了解進行語音
辨識需要進行的步驟。 
 
八、本計畫所發表之研究成果 
 
於計畫進行期間，我們總計發表以下與計畫
相關的專利2項、期刊論文6篇、以及12篇國際會
議論文。 
另外，我們也培養出11位碩士撰寫碩士論文
論文。 
 
專利： 
[2005] 
1. 呂仁園，江永進， “中文漢語語音辨識系統”，中
華民國發明專利，申請案號092121892，申請日
2003/08/08；專利證書發明第 I 232428 號，公告
日2005/05/11 
[2006] 
2. 呂仁園，江永進，謝鴻文，“能使語音播放與
文 字 顯 示 同 步 之 方 法 ”，  中華民國發明專
利，申請案號094125461，申請日2005/07/27；專
利證書發明第 I269191 號，公告日2006/12/21 
 
期刊論文： 
[2004] 
3. Ren-yuan Lyu, Min-siong Liang, Yuang-chin 
Chiang,  “Toward Constructing A Multilingual 
Speech Corpus for Taiwanese (Min-nan), Hakka, and 
Mandarin Chinese”, International Journal of 
Computational Linguistics & Chinese Language 
Processing, Vol.9, No. 2, Aug 2004, pp1-12 
[2005] 
4. Dau-Cheng Lyu, Ren-Yuan Lyu 
(Supervisor) ,Chun-Nan Hsu, Yuang-chin Chiang , 
“Modeling Pronunciation Variation for Bi-Lingual 
Mandarin/Taiwanese Speech Recognition”, 
International Journal of Computational Linguistics 
& Chinese Language Processing, Vol. 10, No.3, Sep 
2005, pp. 363-380 
[2006] 
5. Ren-Yuan Lyu, Min-Siong Liang, Dau-Cheng Lyu, 
Yuang-Chin Chiang, “Taiwanese Min-nan Speech 
Recognition and Synthesis”, book chapter in  
ADVANCES IN CHINESE SPOKEN 
LANGUAGE PROCESSING, edit by 
Chin-Hui Lee, Lin-shan Lee, etc.,  
published by World Scientific Publishing, 
2006,  ISBN 981-256-904-9 World Scientific, 
Chapter 17, pp. 387-406, 2006 
[2007] 
6. Dau-Cheng Lyu, Ren-Yuan Lyu (Supervisor) , 
Yuang-Chin Chiang, Chun-Nan Hsu, 
“Cross-Lingual Audio-to-Text Alignment for 
Multimedia Content Management," To appear in 
Decision Support Systems (DSS) (SCI, IF=0.946), 
 18
Pronunciations in Taiwanese and the Automatic 
Transcription of Buddhist Sutra with Augmented 
Read Speech”, Interspeech'2005 - Eurospeech — 
9th European Conference on Speech 
Communication and Technology, September 4-8 , 
2005 ,  Lisbon, Portugal, (included in EI) 
 
[2006] 
 
17.  Dau-cheng Lyu, Ren-yuan Lyu, etc, “SPEECH 
RECOGNITION ON CODE-SWITCHING AMONG 
THE CHINESE LANGUAGES/DIALECTS”,  
2006 IEEE International Conference on Acoustics, 
Speech, and Signal Processing 
(IEEE-ICASSP2006), May 14-19, 2006, Toulouse, 
France  (included in EI) 
18. Min-siong Liang, Ren-yuan Lyu, Yuang-Chin 
Chiang, “Using Speech Recognition Technique for 
Constructing a Phonetically Transcribed 
Taiwanese (Min-nan) Text Corpus”, The Ninth 
International Conference on Spoken Language 
Processing (Interspeech 2006 — ICSLP), Sep 17-21, 
2006, Pittsburgh, Pennsylvania, USA, (included in 
EI) 
19. Jiang-Chun Chen, Wei-Tang Hsu, J.-S. Roger Jang, 
Ren-Yuan Lyu, Yuang-Chin Chiang, 
“Formant-Based English Vowel Assessment For 
Chinese in Taiwan”, The Ninth International 
Conference on Spoken Language Processing 
(Interspeech 2006 — ICSLP), Sep 17-21, 2006, 
Pittsburgh, Pennsylvania, USA, (included in EI) 
 
[2007] 
20. Min-Siong Liang, Ren-Yuan Lyu, Yuang-Chin 
Chiang, “PHONETIC TRANSCRIPTION USING 
SPEECH RECOGNITION TECHNIQUE 
CONSIDERING VARIATIONS IN 
PRONUNCIATION”, 2007 IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (IEEE-ICASSP2007), ICASSP 2007, 
Honolulu, Hawai'i, U.S.A., April 15-20, 2007 
(included in EI) 
 
  
[附件] 所發表之論文全文 
 
R.-Y. Lyu et al. 
 
384 
Along with the democratic and economic achievements in Taiwan in recent years, 
there is a renewed confidence and interest to use Taiwanese as the main language 
of communication.  
Linguistically, Taiwanese is a branch of the Han (Chinese) language family 
possessing many Chinese characteristics such as being a syllabic and tonal 
language, using hànzì (Chinese characters) as the major orthography in its 
writing system, and having a unique and systematic way to pronounce these 
Chinese characters. However, Taiwanese does not have a strong written tradition. 
Up to the 19th century, Taiwanese speakers wrote using a form of literary Chinese 
(文言文), which would be mostly unintelligible nowadays. A writing system 
made up of entirely roman characters was developed for colloquial Taiwanese in 
the 19th century by Western missionaries to facilitate translations of the Bible. 
This system is commonly called Church Romanization, or “peh-oe-ji” (POJ) in 
Taiwanese. A new orthographic system called Hanlor, proposed by Dr. IokDik 
Ong in the 1960’s that uses both hànzì and roman characters, started to gain 
popularity. Since the 1990’s, Hanlor has become the main mode of writing for 
Taiwanese and has been frequently used by major newspapers. Just like the 
increasing usage of vernacular Chinese (白話文) in Mandarin, the use of the 
more literary writing form becomes increasingly rare in Taiwanese. 
Looking at Taiwanese phonetically, a majority of the Chinese characters used 
have multiple pronunciations. A character can be pronounced in the classic, 
literary way (文讀音, Wen-du-in) or in the “everyday” way (白讀音, Bai-du-in). 
It has been observed that if a word comes from classical literature, then Wen-du-
in is used to pronounce that word. But if the origin of the word is vague, the 
pronunciation of the word tends to vary. 
Taiwanese is a member of the Han language family, and not a dialect of 
Mandarin. Taiwanese, however, does have its own dialects. These varieties of 
Taiwanese can be classified as northern Taiwanese and southern Taiwanese, 
roughly corresponding to their origins from mainland China. The dialectal 
differences between them appear small and often insignificant to native speakers. 
As a result of well-developed transportation and communication systems, few 
pure dialectal tongues exist.  
1.1. Phonetic Structure of Taiwanese Syllables 
Phonetically, Taiwanese is a syllabic and tonal language with extensive tone 
sandhi rules. Similar to Mandarin, a syllable in Taiwanese can be defined by its 
three components: initial consonant, rhyme and tone; rhyme is also called final in 
speech community. There are 18 initials, and 47 finals2 which are made up of the 
R.-Y. Lyu et al. 
 
386 
tones. Syllables with these tones are shorter in duration, and are traditionally 
treated as tonal variations. However, in speech recognition, they are handled as 
different syllables. 
Taiwanese is known to be rich in tone sandhi, and in our on-going T3 
Taiwanese treebank – a bracketed corpus of more than 180,000 words3 – creation, 
we recently started to annotate the corpus with tone sandhi marks. Based on 
about 7,000 phrases/sentences, the tone sandhi rate is more than 80% in syllable 
count.  
There are two questions relevant to tone sandhi: when does the tone of a 
syllable change, and where does it change to. At the word level, in multi-syllabic 
words, most syllables would undergo tone changes except the final one. However, 
at the sentence level, the tone sandhi may appear even at the word boundary. This 
phenomenon seems closely related to the syntactic roles of words in a sentence, 
and is being studied in an on-going research.   
As for the where-to problem of the Taiwanese tone sandhi rules, the 
“Taiwanese boat”2 in Figure 1 illustrates the simplest way to recap the rules. 
Figure 1 also shows the tone sandhi rules of entering tones. 
Table 3. Tones in Taiwanese. Note that tone 8 and 9 exist only in tone sandhi. 
Tone 1 2 3 4 5 6 7 8 9 
Example 
東 
dong1 
洞 
dōng2 
擋 
doʛng3 
黨 
dòng4 
同 
dŏng5 
獨 
dok6 
督 
dōk7 
(獨) 
doʛk8 
(黨) 
dóng9 
Descrip-
tion 
high-
level 
mid-level low-
falling 
high-
falling 
falling-
rising 
high-
short 
mid-
short 
low-
short 
rising 
Traditional 
classes 
1(陰平) 
InPing 
7(陽去) 
IangChyu 
3(陰去) 
InChyu 
2(陰上) 
InSang 
5(陽平) 
IangPing 
8(陰入) 
InRu 
4(陽入) 
IangRu 
  
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. The major tone sandhi rules of Taiwanese. On the left is the Tone sandhi Boat which 
captures the rules neatly. On the right are the rules for entering tones: syllables ending with –p, –t, 
–k, or –h. 
 
 
 1 
 
2 3 4 
5 
9 (5) 
舒喉 調ｅ主要變調規
南 北 
海口 
-p,-t,-k -h 
7 
8 
6 
3 
6 
3 
2 
3 
7 
4 
2 
R.-Y. Lyu et al. 
 
388 
Table 4. The number of pronunciation of Formosa bilingual Lexicons, including classic literary 
pronunciation (Wen-du-in) and everyday pronunciation (Bai-du-in). 
 Taiwanese Wen-du-in 
pronunciation 
Taiwanese Bai-du-in 
pronunciation Total 
1-Syllable 6890 2377 9267 
2-Syllable 39840 36176 76016 
3-Syllable 8308 15214 23522 
4-Syllable 9119 4117 13236 
5-Syllable 438 399 837 
6-Syllable 225 94 319 
7-Syllable 125 28 153 
8-Syllable 52 22 74 
9-Syllable 2 2 4 
10-Syllable 8 2 10 
Total 65007 58431 123438 
 
Table 5. The distribution of words in two lexicons: Syu’s Hakka lexicon, and the CKIP. 
 1-Syl 2-Syl 3-Syl 4-Syl 5-Syl  
Syu 7322 9161 4948 2382 21  
CKIP 6863 39733 8277 9074 435  
 6-Syl 7-Syl 8-Syl 9-Syl 10-Syl Total 
Syu 3 0 0 0 0 23837 
CKIP 223 125 52 2 8 64792 
2. Speech Corpus 
To implement a speaker-independent automatic speech recognition system, it is 
essential to collect a large-scale speech database. However, the years of  
marginalization of Taiwanese makes this task difficult in at least in two ways. 
Firstly, such a mammoth undertaking requires a huge amount of funding, which 
is difficult to obtain as funding is typically limited. Secondly, due to the limited 
level of education, only a small number of speakers have the capability to write 
Taiwanese. This low literacy level makes Taiwanese text collection difficult, 
which in turn makes the collection of speech data difficult – be it collecting read 
speech from existing texts, or phonetically transcribing existing speech data. 
To overcome this problem, a moderate-sized speech database using only the 
lexicon is developed. In brief, we: (1) design sheets of phonetically-balanced 
words from the lexicon; (2) record the microphone and telephone speech of those 
words; and then (3) validate this speech database. The result is the ForSDat 
speech corpus which is detailed in the following subsections.  
R.-Y. Lyu et al. 
 
390 
Applying the algorithm to the three lexicons mentioned above, we identified a 
number of balanced-word sets. For Taiwanese, 446 balanced-word sets were 
generated, each sheet containing 200 syllables making up a total of 37,275 words. 
2.2. Speaker Recruitment and Recording System for Corpus Collection 
Several part-time assistants were employed to recruit speakers from around 
Taiwan. Each speaker was asked to record readings from one sheet, and the 
speaker, along with the assistant, received remuneration. We also noted the 
following information relating to the speaker: 
(i)  the name and gender of the speaker; 
(ii)  the age and birthplace of the speaker; 
(iii)  the location and time of the recording; 
(iv)  the number of years of education the speaker has completed. 
This speaker profile information would be useful later on for organizing the 
collected speech data. The user can also design experiments according to these 
profiles. 
Two systems were designed for collecting microphone and telephone speech 
for the ForSDat database. For the telephone system, the speakers dialed into the 
laboratory using a handset telephone. The input signal is in the format of 8K 
sampling rate with 8-bits µ-law compression. A speaker was given a prompt 
sheet before recording, and every word on the sheet was first played to the 
speaker before he/she recorded that word. The data gathered from different 
speakers were saved in different directories. 
Table 6. The statistics of utterances, speakers and data length for speech collected over microphone 
and telephone channels in Taiwanese and Mandarin. (MIC: microphone; TEL: telephone; also 
denoted in Name by sub-tag after dash). 
Name Channel Gender Quantity Train(hr) Test (hr) 
TW01-M0 Female 50 5.92 0.29 
TW01-M1 Male 50 5.44  
MD01-M0 Female 50 5.65 0.27 
MD01-M1 Male 50 5.42  
TW02-M0 Female 233 10.10 0.70 
TW02-M1 
MIC 
Male 277 11.66  
TW02-T0 Female 580 29.21 0.95 
ForSDAT 
TW02-T1 
TEL 
Male 412 19.37  
R.-Y. Lyu et al. 
 
392 
Fig. 2. The diagram of the one-pass speech recognizer. 
 
 ),|(maxarg SOCP  (1) 
Using standard probability theory, this expression can be equivalently written as  
 )()|(),|(maxarg CPCSPCSOP  (2) 
The three probability expressions in (2) are organized in such a way that 
acoustics of pronunciation and language information are contained in separate 
terms. In modeling, these terms are known as  
1. ),|( CSOP Tonal syllable acoustic model 
2. )|( CSP  Pronunciation model 
3. )(CP  The language model 
In this framework,9 Chinese character based decoding can be implemented by 
searching in a three-layer network composed of an acoustic model layer, a lexical 
layer, and a grammar layer. There are at least 2 critical differences between our 
framework and the conventional one. 1) In the lexicon layer, character-to-
pronunciation mapping can easily incorporate multiple pronunciations in multiple 
languages, including Japanese, Korean, and even Vietnamese which also use 
Chinese characters. 2) In the grammar layer, characters instead of syllables are 
used as nodes in the searching network. Under this ASR structure, it does not 
matter which language the user speaks. Whether it is Taiwanese, Mandarin or a 
mixture of them even in one sentence, the ASR outputs only the Chinese 
characters, making the framework language/dialect independent. In another 
work,10 we also used the framework to recognize Taiwanese-Mandarin code-
switching speech. 
 
Final Model 
Init. Model 
Feature. 
Extractor 
R.-Y. Lyu et al. 
 
394 
3.4. Tree-structured Language Searching Net 
A tree structure is a natural choice of representation for a large vocabulary 
lexicon, as many phonemes can be shared eliminating redundant acoustic 
evaluations. The advantage of using a lexicon tree representation is obvious: It 
can effectively reduce the state-search space of the trellis. Ney et al.14 reported 
that a lexical tree has a saving factor of 2.5 over the linear lexicon. Besides, the 
efficiency of using a lexical tree is substantial, not only because it results in 
considerable saving of memory for representing state-search space, but it also 
saves a significant amount of time by searching in far fewer potential paths. 
Figure 3 shows examples of a linear searching net and a tree-structured 
searching net. The perplexity of the linear searching net was found to be 5 while 
the tree-based one has a smaller perplexity of 4.89. 
 
 
Fig. 3. The examples of isolated linear (left) and tree-structured (right) searching net with their 
probability values. 
3.5. Pronunciation Modeling Using Pronunciation Variation 
The pronunciation model plays an important role in our proposed one-pass 
Chinese character based ASR engine.15 It not only provides more choices during 
decoding when the speaker exhibits variations in pronunciation, but also handles 
various speaking styles in different languages. As mentioned above, one Chinese 
character has more than two pronunciations in the combined phonetic inventory 
of Mandarin and Taiwanese. Accent and regional migration are also factors that 
influence the pronunciation or speaking style of speakers. In the following sub-
sections, we propose two different methods, knowledge-based and data-driven 
methods, for obtaining rules of pronunciation variation. 
3.5.1. Knowledge-Based Method 
As shown by Strik,16 information about pronunciation can be derived from 
knowledge sources, such as pronunciation dictionaries handcrafted by linguistic 
R.-Y. Lyu et al. 
 
396 
4. Taiwanese Text-to-Speech  
The TTS system proposed is composed of 3 major functional modules, namely a 
text analysis module, a prosody module, and a waveform synthesis module. The 
system architecture is shown in Figure 4. Since Taiwanese is a tonal language, 
we will describe the process of the tone sandhi rules for text analysis module. In 
the waveform synthesis module, the system adopts TD-PSOLA to modify 
waveforms by adjusting the prosody parameters of selected units so that the 
synthesized speech sounds more natural. Finally, the new TTS architecture is 
implemented in a multiple-level unit selection for a limited domain application. 
 
 
Fig. 4. The TTS system architecture is composed of 3 major functional modules: a text analysis 
module, a prosody module, and a waveform synthesis module. 
4.1. Word Segmentation and Mandarin-Taiwanese Translation 
(Sentence-to- Morpheme) 
Although the Hanlor orthography is the most common writing style of Taiwanese 
in contemporary Taiwan, all three types of written texts (see Section 1) can be 
analyzed by our text analysis module. Since there are no natural boundaries 
between two successive words, we must segment a Mandarin text into its word 
sequence first. The bilingual pronunciation dictionary is used as a basis for our 
word segmentation algorithm based on the sequentially maximal-length matching, 
which segments Mandarin sentences into maximal-length word combinations.21 
Finally, we directly translate Mandarin into Taiwanese word-for-word, and 
transcribe the Taiwanese words phonetically into ForPA. This segmentation, 
translation and transcription process is exemplified in Figure 5, where the input 
Mandarin sentence is “ ” (“he is very happy today”). 
Synthesis 
Module 
4521 tonal 
syllable units 
PSOLA 
Waveform 
Synthesis 
Prosody 
Generation 
Text Analysis Text 
Bilingual 
Lexicon 
Tone 
Sandhi 
 
Digit sequence 
processing rules 
Phonetic 
transcription 
Prosody 
Module 
Text 
Module 
R.-Y. Lyu et al. 
 
398 
Table 7. The rules for normalization of digit sequences. 
Position Pronunciation 
Ten Million Read 0-9 as EP 
Million Read 0-9 as EP 
Hundred 
Thousand 
No sound for 1, 2 as LP and others as EP. 
Ten 
Thousand 
If the digit is 0 in hundred thousand position, read 0-9 as EP. If the digit is not 0 
in hundred thousand position, 1, 2 read as LP, and others as EP. 
Thousand Read 0-9 as EP 
Hundred Read 0-9 as EP 
Ten No sound for 1, 2 as LP, and others as EP. 
Unit digit 
If the digit is 0 in hundred thousand position, read 0-9 as EP. If the digit is not 0 
in hundred thousand position, 1,2 read as LP, and others as EP. 
LP: classic, literary pronunciation, EP: everyday pronunciation. 
Table. 8. Tone sandhi rules for triple adjectives. 
lexical 1 2 3 4 5 6 7 
sandhi-tone 9 9 4 1 9 9 6 
 
other than the traditional 7 lexical tones mentioned previously – are handled. We 
map this “High-Rising” tone to digit 9, and call it tone 9. The tone sandhi rules 
for triple adjectives are summarized in Table 8. 
4.4. Evaluation of Text Analysis and Prosody Modules 
Following text analysis and prosody generation, an evaluative experiment is 
conducted. Its main target is to assess the accuracy rate of automatic transcription, 
which is produced by the text analysis and prosody modules, in comparison with 
manual transcription. A large amount of news reports are collected from the 
internet. The selection of these is random, without emphasis on any particular 
news category. Of these, a set of 200 sentences to cover all distinct Chinese 
characters are chosen. The comparative performance of manual and automatic 
transcription is shown in Table 9, with three sets of results: word segmentation, 
labeling and tone sandhi accuracy rates.22 
From Table 9, we infer that the system can segment and translate most 
articles accurately into Taiwanese words with over 97% accuracy. If we do not 
consider tone sandhi, the system can translate an article into its correct 
R.-Y. Lyu et al. 
 
400 
Table 10. The textual statistics of the Taiwanese New Testament Bible. 
Number of Chinese characters 278,633 
Number of chapters 27 
Number of sentences 39,171 
Number of distinct words 7,189 
 
From the statistics in Table 10, we find that the usage of duplicate words is 
highly frequent. If these much-duplicated syllables, words or sentences could 
serve as a gauge, the production cost of an audio book could be greatly reduced. 
But sounding natural is proportionally related to the number of synthesis units. 
Synthesis systems with fewer units tend to generate less natural-sounding speech. 
For instance, logically, any arbitrary sentence can be synthesized by all 4,609 
tonal syllables in Taiwanese, but the prosody in this synthesized sentence would 
rank low in naturalness. When words are used as synthesized units, a total of 
7,189 distinct words are required. However, the result of this would only be a 
slight improvement from the syllable-synthesis method. This is because, most 
Taiwanese words are monosyllabic anyway. Therefore, finding compromise units 
between the word and the sentence becomes a very important issue. 
From our observation, poor quality (least natural sounding) synthesis occurs 
often in the concatenation of monosyllabic words. In fact, the naturalness of 
multi-words (or multiple words) unit combined by multi-syllabic words could 
result in only a slight improvement. It is therefore necessary to divide the multi-
words units into two categories: concatenation of monosyllabic words and multi-
syllabic words. Preference is given to concatenating monosyllabic rather than 
multi-syllabic words as synthesis units. To find high-frequency and longer-length 
synthesis units, we adopted an evolution method of maximum maximal-length 
words matching, called maximal-multi-words matching, which is described as 
follows: 
Step 1.  Let Wi  be the i-th input sentence in the text corpus, where Wi is 
composed of Ni words and each word is separated by the equal sign 
denoted as { }1 2 3 ... iNi i i i iW w w w w= = = = = . The length of the 
matching pattern is set to n, i.e. the pattern is { }1 2 3 ...n ni i i i iW w w w w= = = = = , where n is smaller than or equal to Ni. 
Step 2.  Initially, let in N= , i.e. the matching pattern is iW . Let ( )niN w  denote 
the count of the pattern of the multi-words niW of the text corpus.  
Step 3.  If ( ) 0niN w ≠  and in N= , repeat Step 1 with the next sentence. Else, if 
( ) 0niN w = , repeat Step 2 with n = Ni-1. 
R.-Y. Lyu et al. 
 
402 
References 
1. Wikipedia. Available from http://en.wikipedia.org/wiki/Demographics_of_Taiwan (2006). 
2. YuangChin Chiang, A Course in Taiwanese Pinim, (in Taiwanese) AnKor Publishing, 
PingTong (2005). 
3. S.-Y. Zhou, T3 Taiwanese Treebank and Brill Part-of-Speech Tagger, (in Chinese), Master 
thesis of National TsingHua University, HsinChu, Taiwan  (2006). 
4. J. Wells, SAMPA (Speech Assessment Methods Phonetic Alphabet), 
http://www.phon.ucl.ac.uk/home/sampa/home.htm, April, (2003). 
5. J. L. Hieronymus, “ASCII Phonetic Symbols for the World's Languages: Worldbet,” 
Technical Report AT&T Bell Labs, (1994). 
6. R.-Y. Lyu et al. “Toward Constructing A Multilingual Speech Corpus for Taiwanese (Min-
nan), Hakka, and Mandarin,” ICLCLP Vol. 9, No. 2, (August 2004), pp. 1-12 
7. YuangChin Chiang,  An input method editor (IME) in Taiwanese Pinim, (2005). 
8. M.-S. Liang et al. “An Efficient Algorithm to Select Phonetically Balanced Scripts for 
Constructing Corpus,” NLP-KE, (2003). 
9. R.-Y. Lyu et al. “A Unified Framework for Large Vocabulary Speech Recognition of 
Mutually Unintelligible Chinese Regionalects,” In Proc. of ICSLP 2004, (2004). 
10. D.-C. Lyu et al.. “Speech Recognition on Code-Switching Among the Chinese Dialects,” In 
Proceedings of IEEE ICASSP’06, (2006). 
11. P. F. Wong and M. H. Siu, “Integration of Tone-related Feature for Chinese Speech 
Recognition,” in Proceedings on ICMI, (2002) pp. 476-479. 
12. D.-C. Lyu et al. “Large Vocabulary Taiwanese (Min-nan) Speech Recognition Using Tone 
Features and Statistical Pronunciation Modeling,” In Proc. of Eurospeech, (2003). 
13. D.-C. Lyu et al. “Speaker Independent Acoustic Modeling for Large Vocabulary Bi-lingual 
Taiwanese/Mandarin Continuous Speech Recognition,” In Proceedings of the 9th SST, 
Melbourne (2002). 
14. H. Ney et al. “Improvements in Beam Speech for 1000-Word Continuous Speech 
Recognition,” In Proc. of the ICASSP‘92, California, (1992), pp. 9-12. 
15. D.-C. Lyu et al.. “Modeling Pronunciation Variation for Bi-Lingual Mandarin/Taiwanese 
Speech Recognition” IJCLCLP, Vol. 10. no. 3. (2005), pp. 363-380. 
16. H. Strik and C. Cucchiarini, “Modeling Pronunciation Variation for ASR: Overview and 
Comparison of Method,” Speech Communication, Vol. 29, (1999) pp. 225-246. 
17. T. Fukada, et al. “Automatic Generation of Multiple Pronunciations Based on Neural 
Networks and Language Statistics,” In Proceedings of ESCA, (1998), pp. 103-108. 
18. N. Cremelie, and J. P. Martens, “In Search of Better Pronunciation Models for Speech 
Recognition,” Speech Communication 29, Vol. 4 (2), (1999), pp. 115-136,  
19. J. J Humphries et al. “Using Accent-Specific Pronunciation Modelling for Robust Speech 
Recognition,” In Proc. ICSLP-96,  (1996), pp. 2324-2327. 
20. M. Wester and E. Fosler-Lussier, “A Comparison of Data-Derived and Knowledge-Based 
Modeling of Pronunciation Variation,” In: Proc. ICSLP 2000, Vol. 4, (2000), pp. 270-273. 
21. M.-S. Liang et al. “A Bi-lingual Mandarin-To-Taiwanese Text-to-Speech System,” In 
Proceedings Int. Conf. on Spoken Language Processing (ICSLP), (2005). 
22. M.-S. Liang et al. “A Taiwanese Text-to-Speech System with Applications to Language 
Learning,” In Proc. ICALT 2004, Joensuu, Finland, (2004). 
23. K.-C. Chuang, Phrase-based Synthesis Units and Study of Phrase Tone-Sandhi for Taiwanese 
Text-To-Speech and Application, Master Thesis, University of Chang Gung, Taiwan, (2005). 
24. S. C. Kumar et al. “Multilingual Speech Recognition: A Unified Approach,” In Proc. of 
Eurospeech, Portugal, (2005). 
uppor
ARTICLE IN PRESSIn Taiwan, a rapidly growing volume of digital
spoken documents are in Taiwanese. Taiwanese (or
Min–Nan) is a “regionalect” [21] in the large family of
Han–Chinese languages. Though not an official lan-
guage, Taiwanese is one of the most popular languages
in Taiwan with approximately 70% of the population
fluent in it. Even so, there is no standard written form for
Taiwanese, so existing text documents in the language
are written in many different ways. Although the most
popular written form of Taiwanese is based on Chinese
characters, not all Taiwanese phrases can be written in
Chinese characters. Furthermore, without a commonly
agreed standard, many Taiwanese phrases are written in
several different ways in Chinese characters.
We need to be creative in managing the growing
volume of Taiwanese spoken documents, because
existing content management technologies, most of
which are based on information retrieval (IR) methods
for text-based content, cannot be applied directly to our
problem. Manual transcription using software packages,
such as Transcriber [31] and Praat [34], is prohibitively
expensive due to the lack of a standard written form for
Taiwanese and the lack of skillful human transcribers.
Automatic transcription by speech recognition is a
possible solution [23,35]. Over the last decade enor-
mous progress has been made in the field of speech
recognition such that word-level error rates of speech
recognizers can be as low as 10 percent under certain
conditions [10]. Therefore, automatic transcription has
been widely applied to the content management of
spoken documents [2,11,28,36]. For example, Cook et
al. [7] developed an automatic speech recognition
system to manage audio broadcast news documents.
However, applying speech recognition to automatic
transcription of spoken documents is not straightfor-
ward, even for English and Mandarin, the world's most
popular languages. Most spoken documents contain
spontaneous speech. Large vocabulary continuous
speech recognition currently only achieves high accu-
racy for read speech, as in dictation systems. Speech
recognition for spontaneous and conversational speech
is still a challenge, unless the application domain is
limited [3]. Since speech recognition for Taiwanese is
not as mature as that for English or Mandarin, directly
applying speech recognition to automatic transcription
may not be a viable solution in our case.
In this paper, we present an approximate, yet effective,
solution to this content management problem by aligning
Taiwanese spoken documents with related text documents
in Mandarin. The idea is to take advantage of the
abundance of Mandarin text documents available in our
2 D.-C. Lyu et al. / Decision Sapplication. Content management systems can index
Please cite this article as: D.-C. Lyu et al., Cross-lingual audio-to-text alignm
(2007), doi:10.1016/j.dss.2007.07.003Taiwanese spoken documents by indexing the aligned text
document in Mandarin. In other words, with our
approach, we can use text documents in Mandarin to
represent the content of our spoken documents. Text
documents can also compensate for the limitations of
speech recognition systems. In our experiment, although
our speech recognizer's performs for spontaneous
Taiwanese is not satisfactory (barely above 55%), we
still achieve a high (82.5%) alignment accuracy.
Aligning cross-lingual multi-media documents (spo-
ken and text) poses a number of challenges. These
challenges are summarized as follows:
1. How can we segment a continuous broadcast news
session into news stories so that we can align the
stories with text documents? How can we determine
the boundary between one news story and another in
an audio stream?
2. Once we have segmented an audio stream into news
stories, how can we determine which text document
contains the same news story as its audio counter-
part? Since the reports are in different languages,
should we define a common language and translate
the text documents into the common language? How
can we perform such translation? This is a compli-
cated issue because there is no one-to-one mapping
between Chinese characters and Taiwanese syllables.
3. How can spoken documents be transcribed into the
common language? If speech recognition techniques
are applied, how can they deal with spontaneous
speech, especially when a broadcast news story may
cover a number of domains?
To address these challenges, we integrate various
intelligent multi-media content management technolo-
gies, including audio classification, segmentation,
speech recognition, cross-lingual translation, and paral-
lel texts alignment. We provide a solution to cross-
lingual multi-media content management by integrating
our pervious research results in the technologies
mentioned above [3,17–20,22]. Moreover, with our
cross-lingual audio-to-text alignment approach, IR-
based content management techniques can be applied
to manage audio content so that users can access digital
audio Taiwanese news archives by sending conventional
keyword search queries in Chinese characters.
The remainder of this paper is organized as follows.
Section 2 gives an overview of our approach. Section 3
describes how we segment a broadcast news session into
a sequence of news stories. Section 4 reviews our speech
recognition method. Section 5 explains how to translate
t Systems xx (2007) xxx–xxxa text document in Mandarin into a sequence of
ent for multimedia content management, Decision Support Systems
problem as a sequence labeling problem. We solve the
problem by training a Hidden Markov Model (HMM)
that identifies homogeneous regions according to their
background conditions and the characteristics of their
audio signals. We report our empirical evaluation of the
performance of our audio segmenter.
3.1. Structure of a news session
Most broadcast news sessions follow a similar well-
defined structure. Fig. 2 illustrates the structure of a
typical session of Formosa Television News (FTVN) in
Taiwan. According to this structure, we define five types
of scenes:
A. Highlight: In this type of scenes, which last 30 to
60 seconds, the news anchor introduces the top
stories that will be covered in this session. At the
end of this segment, the anchor typically intro-
duces him/herself and the date of the broadcast.
B. Anchor's report: When the anchor is speaking, he/
session back to the anchor after naming him/
herself and mentioning the location where he/she
has been reporting.
D. Weather report: In this type of scenes, a weather
expert forecasts the weather for the next few
+days in Taiwan and worldwide. Usually, music
is played in the background.
E. Advertisement: An advertising scene usually lasts
30–240 s and consists of a series of 15, 30, or 60 s
TV commercials.
The statistics of the five scenes in a four-hour
broadcast news corpus are shown in Table 1. The top
two largest scenes are anchor's report and interview. We
define a pair of consecutive anchor's report and inter-
view as a story, as shown in Fig. 2(a). The goal of
segmentation is to identify and extract stories from the
audio stream of a broadcast news session. Table 1 also
categorizes the signal texture of the above scene types as
“speech”, “music”, “background sounds”, “silence”,
“speech with music”, or “speech with background
4 D.-C. Lyu et al. / Decision Support Systems xx (2007) xxx–xxx
ARTICLE IN PRESSshe reports a news story that may last 15 to 60 s. In
this type of scenes, only the anchor's voice is
heard, i.e., there is no background music or noise.
C. Interview: The anchor may transfer the session to
an on-site reporter or an expert on the topic. A
graphic animation and a sound track will often
accompany the voice-over of the reporter. At the
end of the interview, the reporter transfers theFig. 2. (a) Structure of a typical television broadcast news session on FTV
segmenter.
Please cite this article as: D.-C. Lyu et al., Cross-lingual audio-to-text alignm
(2007), doi:10.1016/j.dss.2007.07.003sounds.”
Since both interview and advertisement may contain
all kinds of signals, it is difficult to distinguish them. But
we can identify anchor's report easily because it only
contains speech signal. Nevertheless, we have to use a
set of discriminating features to distinguish these scene
types. In the next sub-section, we will describe the
features that we use to discriminate the above scenes.N. (b) An example of the output sequence of the HMM scene type
ent for multimedia content management, Decision Support Systems
sequence with different scene labels mark the bound-
aries of the scene types. We can then segment the audio
stream at those boundaries as the final result.
There is a strong point to be made for using HMM on
top of GMM as the audio segmenter instead of using
GMM only. Not taking the order of the scenes into
account, GMM alonemay produce an unreasonable scene
Since we need to handle multi-lingual speech in the
acoustic modeling, we use the International Phonetic
Alphabet (IPA) scheme to transcribe the results of
speech recognition. IPA is an inventory of common
phoneme symbols for describing the phonetic pronun-
ciation of almost all languages in the world, and is
widely used in multilingual speech recognition systems
[8,29,33]. With IPA, speech sounds perceived as the
same in different languages are transcribed with the
same phonemic symbols. Table 3 shows the statistics of
IPA in different phonetic levels for Mandarin and
Taiwanese. According to Table 3, combining two
languages in this manner reduces the number of
syllables by 21%, implying that, with the same training
6 D.-C. Lyu et al. / Decision Suppor
ARTICLE IN PRESStype sequence where some frames might have a label
different from their neighbor frames. If that happens,
those frames must be misclassified and should be labeled
with the same scene type as its neighbors because an audio
stream should be continuous and it is highly unlikely that
scene types would change abruptly or frequently.
Transition probabilities in HMM can naturally fix this
type of errors. Another advantage of HMM is that there is
no need to change the topology of a fully-connected
HMMmodel even if the scene type order is changed. For
example, suppose that in another TV station, anchor's
report may be followed immediately by weather report,
and several advertisement sessions may be inserted
between interview. In this case, we can update the
transition probabilities between new scene type pairs to
have non-zero values to adapt to the new scene type order.
3.4. Evaluation of story/scene segmentation
We used a four-hour corpus of Taiwanese audio
broadcast news to empirically evaluate our audio
segmenter. We used three hours of data for training
and the rest for testing. Both training and test sets were
manually labeled with five scene types. We recorded the
audio data in a 16 KHz, 16-bit form and sliced the audio
stream into audio frames with a three-second-window
duration and a half duration overlap. We then extracted
41 feature dimensions from the audio frames, as
described in Section 3.2. We applied the Expectation
Maximization (EM) algorithm to train the HMMmodel,
which contains five fully-connected states. The emis-
sion distributions tied to all states in the HMM model
are modeled by a mixture of 64 diagonal-covariance
Gaussian densities. With the trained HMM model, we
can perform the segmentation by applying the log-space
Viterbi decoding algorithm.
Table 2
Confusion table of the classification results for five scene types
Classified as→ A B C D E
A) Highlight 2
B) Anchors report 40
C) Interview 2 37 1
D) Weather report 2
E) Advertisement 2
Please cite this article as: D.-C. Lyu et al., Cross-lingual audio-to-text alignm
(2007), doi:10.1016/j.dss.2007.07.003Table 2 shows the accuracy of our HMM audio
segmenter. The diagonal entries show the number of
scenes that were correctly classified into their cor-
responding scene type, while the off-diagonal entries
show the number of misclassified scenes. Overall, our
segmenter achieves a classification rate above 95%.More
importantly, our segmenter can successfully detect the
boundaries of news stories — consecutive pairs of an-
chor's report and interview scenes.
4. Automatic speech recognition
In this section, we describe the algorithm for building
a HMM-based speech recognizer comprised of a multi-
lingual acoustic model, a bi-tonal-syllable-based lan-
guage model and a pronunciation variation model. The
contributions of these techniques have been published in
our previous works [6,17,19]. We also present the
baseline result of speech recognition for a bi-lingual test
corpus.
4.1. Acoustic modeling
Table 3
Statistics of the number of syllables (NS) and tones (Tone) of Mandarin
(M) and Taiwanese (T) and their linguistic units: the number of Tonal
Syllables (NTS), Initials (NI), Tonal Finals (NTF), and context-
dependent Initial/Tonal Finals (NCDIF)
M T M∪T M∩T
NS 408 709 925 192(21%)
Tone 5 7 9 3
NTS 1288 2878 3519 647(18%)
NI 17 19 22 14(63%)
NTF 295 225 416 104(25%)
NCDIF 1656 3496 4374 778(18%)
∩ and ∪ denote intersection and union of the two languages,
respectively.
t Systems xx (2007) xxx–xxxsample size, the amount of training data available for
ent for multimedia content management, Decision Support Systems
sequence can be segmented into three different word
sequences with valid syntax and semantics.
Table 5
Examples of Taiwanese terms in the reading and spoken styles
upport Systems xx (2007) xxx–xxx
ARTICLE IN PRESSthe bi-gram language model, and the pronunciation
variation model.
5. Cross-lingual translation
The cross-lingual translator is an important compo-
nent for aligning Mandarin text documents with
Taiwanese spoken documents. The goal of the translator
is to translate the Mandarin text into Taiwanese tonal
syllables, which can then be aligned with the tonal
syllable sequences obtained by applying speech recog-
nition to the spoken documents.
One of the best approaches to effective language
translation for human readers is to translate at the
sentence level [25]. However, since our goal is
automatic alignment, we only perform word-level, or
word-by-word translation. Word-by-word translation is
also simpler and achieves a higher accuracy than
sentence level translation.
5.1. Bi-lingual lexicons
Our word-by-word translation approach is based on
two bi-lingual lexicons.
1. Formosa Lexicon, which contains approximately
seventy thousand words derived from four major
Mandarin newspapers in Taiwan, and was translated
into Taiwanese by a linguist.
2. Gang's Taiwanese Lexicon, which contains approx-
imately thirty thousand words extracted from a
Taiwanese radio talk show. The vocabulary is the
same as the one that is used by native Taiwanese
Table 4
Statistics of the bi-lingual speech corpus for acoustic model training
and baseline testing (M: Mandarin, T: Taiwanese)
Training set Evaluation set
Language M T M T
No. of speakers 100 100 10 10
No. of utterances 43,078 46,086 1000 1000
No. of hours 11.3 11.2 0.28 0.28
8 D.-C. Lyu et al. / Decision Sspeakers in daily conversation. The author, a
professional linguist, translated the Taiwanese vo-
cabulary into Mandarin.
In both lexicons, each entry contains word and phrase
level Chinese characters accompanied by their tonal
syllabic pronunciation in both Mandarin and Taiwanese.
A Chinese character, which corresponds to a single-
syllable word in Taiwanese, usually has two distinct
types of pronunciation: one for classic, formal literature,
Please cite this article as: D.-C. Lyu et al., Cross-lingual audio-to-text alignm
(2007), doi:10.1016/j.dss.2007.07.003such as poems, and the other for oral, colloquial
expressions used in daily conversation. However, the
distinction between the two types is much more
complex than formal versus colloquial and can be
context dependent. For example, the first “one” and the
second “one” in “one hundred and one” are pronounced
differently in Taiwanese. Formosa Lexicon provides a
set of rules for different types of pronunciation.
Also, care must be taken when translating a text
document written in the reading style of Mandarin to the
spoken style of Taiwanese. Gang's Taiwanese lexicon
contains both styles in Chinese characters. We give
some examples in Table 5. For example, in the first row,
the first column shows the reading style of Mandarin for
the phrase “very long time” that may appear in a text
document. Our goal is to translate it to the spoken style
Taiwanese, appearing in the second column, though
both columns are written in Chinese characters.
5.2. Segmentation
Since Chinese does not have explicit word delimiters
(such as spaces used in English), we must use an explicit
word segmentation process for Chinese text documents.
However, the definition of a “word” in Chinese is
ambiguous, as an example of word segmentation
ambiguity given in Table 6, where the same characterTable 6
An example of ambiguity in Chinese word segmentation, from [15]
ent for multimedia content management, Decision Support Systems
and
uppor
ARTICLE IN PRESSreading Mandarin even if they refer to the same news
story.
7. Experimental evaluation
Fig. 3. Two examples of syllable sequence alignment between spoken
10 D.-C. Lyu et al. / Decision SWe divided the experimental evaluation of our
approach into two stages:
1. Evaluation of how well our components prepare
sequences of Taiwanese tonal syllables for align-
ment. That is, we evaluated the performance of the
speech recognition component and the language
translation component.
2. Evaluation of the accuracy of the alignment, given
imperfect sequences.
The purpose is to evaluate the feasibility of our cross-
lingual alignment approach for managing multi-media
cross-lingual contents.
7.1. Experiment setup
We designed the experiment procedure as shown in
Fig. 4. To perform stage 1 (S1), we used a collection of
40 spoken documents (audio streams) containing a news
anchor's speech. The collection of spoken documents
contains three thousand words according to manual
transcription. The outputs S1 and S2 are sequences in
Taiwanese tonal syllables. S1 was the output of our
Please cite this article as: D.-C. Lyu et al., Cross-lingual audio-to-text alignm
(2007), doi:10.1016/j.dss.2007.07.003speech recognizer, and S2 was derived from the manual
transcription by Taiwanese linguistic experts.
Next, we applied our Web information extraction
algorithm [5] to crawl four hundred text documents from
FTVN's web site for news stories in the same week as
text documents; the left side scores 0.9, and the right side scores 0.4.
t Systems xx (2007) xxx–xxxwhen the news stories of the spoken documents
occurred. The purpose was to evaluate whether the
alignment component can correctly match spoken
documents with a large number of text documents.
Among the 4000 text documents, 40 report exactly the
same news stories as the 40 spoken documents. A bi-
lingual expert translated these 40 text documents and the
results are denoted S4, while all 4000 text documents
were sent to our language translators and the output is
denoted as S3.
As mentioned in Section 4.2, we need a text corpus to
train the language model in order to improve automatic
speech recognition of spontaneous speech. We used S3
as the text corpus because it is in spoken Taiwanese
tonal syllables and the contents were extracted from
related news documents. The language model is a
syllable-based bi-gram model that contains about
480,000 Taiwanese tonal syllables.
7.2. Experimental results of speech recognition and
cross-lingual translation
The purpose of this experiment is to evaluate whether
we can automatically and effectively prepare Taiwanese
ent for multimedia content management, Decision Support Systems
12 D.-C. Lyu et al. / Decision Support Systems xx (2007) xxx–xxx
ARTICLE IN PRESS8. Conclusion and future work
We have described an approach to automatic cross-
lingual alignment of spoken documents and text docu-
ments. Its performance is comparable with manual
alignment, which is known to be very tedious and time
consuming. Our approach can be summarized as follows.
First, based on the structure of television news programs
we extract the anchor's reports and interviews, using a
HMM-based audio segmenter. Then, we apply speech
recognition to transcribe the news anchor's speech as a
tonal syllable sequence in Taiwanese.Meanwhile, a cross-
lingual translator translates text documents inMandarin to
tonal syllable sequences in Taiwanese, using bi-lingual
statistical lexicons. Once we have pairs of tonal syllable
sequences, we apply the DTW algorithm to align them
and select the best matches.
The experimental results show that our approach is
sufficiently accurate for use by search engines in content
management applications. This would allow users to
browse and search for audio spoken documents by
associating them with well-organized text documents.
We believe that our approach can be applied to similar
problems in other languages.
In our future work, we will try to improve our speech
recognition system by adapting the characteristics of news
anchors' voices to a read-based acousticmodel, and use the
model to recognize the anchors' speech. Thismay improve
speech recognition of spontaneous speech.Wewill also try
to implement a bootstrapping approach whereby correct-
ly aligned results can be used as training data to further
improve the performance of all components in the system
and reduce the need for human-labeled corpora.
References
[1] W. Abdulla, D. Chow, G. Sin, Cross-words reference template for
DTW-based speech recognition systems, Proceedings of Con-
vergent Technologies for the Asia-Pacific Region Conference,
Bangalore, India, 2003.
[2] P. Beyerlein, X. Aubert, R. Haeb-Umbach, D. Klakow,
Automatic transcription of English broadcast news, Proceedings
of the European Conference on Speech Communication
Technology, Budapest, Hungary, 1999.
[3] W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic, D.
Oard, M. Picheny, J. Psutka, B. Ramabhadran, D. Soergel, T.
Ward, W.-J. Zhu, Automatic recognition of spontaneous speech
for access to multilingual oral history archives, IEEE Transac-
tions on Speech Audio Processing 12 (4) (2004).
[4] L. Chaisorn, T.-S. Chua, The segmentation classification of story
boundaries in news video, Proceedings of Working Conference
on Visual Database Systems, Australia, 2002.
[5] C.-H. Chang, C.-N. Hsu, S.-C. Lui, Automatic Information
Extraction from Semi-Structured Web Pages by Pattern Discov-
ery, Decision Support Systems 35 (1) (2003).
Please cite this article as: D.-C. Lyu et al., Cross-lingual audio-to-text alignm
(2007), doi:10.1016/j.dss.2007.07.003[6] P.-J. Cheng, J.-W. Teng, R.-C. Chen, J.-H. Wang, W.-H. Lu, L.-F.
Chien, Translating unknown queries with web corpora for cross-
language information retrieval, Proceedings of ACM Conference
on Research and Development in Information Retrieval, Shef-
field, UK, 2004.
[7] G. Cook, J. Christie, D. Ellis, E. Fosler-Lussier, Y. Gotoh, B.
Kingsbury, N. Morgan, S. Renals, T. Robinson, G. Williams, An
overview of the SPRACH system for the transcription of
broadcast news, Proceedings of DARPA Broadcast News
Workshop, Herdon, VA, USA, 1999.
[8] V. Digalakis, P. Monaco, H. Murveit, Genones: Generalized
mixture tying in continuous hidden Markov model-based speech
recognizers, IEEE Transactions on Speech and Audio Processing
4 (44) (1996).
[9] S. Furui, K. Maekawa, H. Isahara, T. Shinozaki, T. Ohdaira,
Toward the realization of spontaneous speech recognition —
introduction of a japanese priority program and preliminary
results, Proceedings of The International Conference on Spoken
Language Processing, Beijing, China, 2000.
[10] J.L. Gauvain, L. Lamel, Large vocabulary continuous speech
recognition: advances applications, Proceedings of IEEE 88 (8)
(2000).
[11] A.G. Hauptmann, R.E. Jones, K. Seymore, S.T. Slattery, M.J.
Witbrock, M.A. Siegler, Experiments in information retrieval
from spoken documents, Proceedings of DARPA Broadcast
News Transcription and Understanding Workshop, Lansdowne,
VA, USA, 1998.
[12] X. Huang, A. Acero, H. Hon, Spoken language processing,
Prentice Hall, Upper Saddle River, NJ, 2001.
[13] P.-Y. Liang, J.-L. Shen, L.-S. Lee, Decision tree clustering for
acoustic modeling in speaker-independent mandarin telephone
speech recognition, Proceedings of the International Symposium
on Chinese Spoken Language Processing, Singapore, 1998.
[14] Z. Liu, Y. Wang, T. Chen, Audio feature extraction analysis for
scene segmentation classification, Journal of VLSI Signal
Processing Systems 20 (1-2) (1998).
[15] W.-K. Lo, H. Meng, P.C. Ching, Cross-language spoken
document retrieval using hmm-based retrieval model with
multi-scale fusion, ACM Transactions on Asian Language
Information Processing 2 (1) (2003).
[16] L. Lu, H. Jiang, H.-J. Zhang, A robust audio classification
segmentation method, Proceedings of ACM International
Conference on Multimedia, Ottawa, Ontario, Canada, 2001.
[17] R.-Y. Lyu, C.-Y. Chen, Y.-C. Chiang, M.-S. Liang, A bi-lingual
Mandarin/Taiwanese (Min–Nan), large vocabulary, continuous
speech recognition system based on the Tong-yong Phonetic
Alphabet (TYPA), Proceedings of the International Conference
on Spoken Language Processing, Beijing, China, 2000.
[18] R.-Y. Lyu, Z.-H. Fu, Y.-C. Chiang, H.-M. Liu, A Taiwanese
(Min–Nan) Text-To-Speech (TTS) System Based on Automat-
ically Generated Synthetic Units, Proceedings of the Internation-
al Conference on Spoken Language Processing, Beijing, China,
2000.
[19] D.-C. Lyu, M.-S. Liang, Y.-C. Chiang, C.-N. Hsu, R.-Y. Lyu,
Large vocabulary Taiwanese (Min–Nan) speech recognition
using tone features statistical pronunciation modeling, Proceed-
ings of the European Conference on Speech Communication
Technology, Geneva, Switzerland, 2003.
[20] D.-C. Lyu, R.-Y. Lyu, Y.-C. Chiang, C.-N. Hsu, Modeling
pronunciation variation for bi-lingual Mandarin/Taiwanese
speech recognition, Computational Linguistics & Chinese
Language Processing 10 (3) (2005).
ent for multimedia content management, Decision Support Systems
 Computational Linguistics and Chinese Language Processing 
Vol. 10, No. 3, September 2005, pp. 363-380                                363 
© The Association for Computational Linguistics and Chinese Language Processing 
[Received February 22, 2005; Revised July 22, 2005; Accepted August 15, 2005] 
Modeling Pronunciation Variation for Bi-Lingual 
Mandarin/Taiwanese Speech Recognition 
Dau-Cheng Lyu ∗,∗∗ , Ren-Yuan Lyu ∗, Yuang-Chin Chiang+ and 
Chun-Nan Hsu∗∗ 
Abstract 
In this paper, a bi-lingual large vocaburary speech recognition experiment based on 
the idea of modeling pronunciation variations is described. The two languages 
under study are Mandarin Chinese and Taiwanese (Min-nan). These two languages 
are basically mutually unintelligible, and they have many words with the same 
Chinese characters and the same meanings, although they are pronounced 
differently. Observing the bi-lingual corpus, we found five types of pronunciation 
variations for Chinese characters. A one-pass, three-layer recognizer was 
developed that includes a combination of bi-lingual acoustic models, an integrated 
pronunciation model, and a tree-structure based searching net. The recognizer’s 
performance was evaluated under three different pronunciation models. The results 
showed that the character error rate with integrated pronunciation models was 
better than that with pronunciation models, using either the knowledge-based or the 
data-driven approach. The relative frequency ratio was also used as a measure to 
choose the best number of pronunciation variations for each Chinese character. 
Finally, the best character error rates in Mandarin and Taiwanese testing sets were 
found to be 16.2% and 15.0%, respectively, when the average number of 
pronunciations for one Chinese character was 3.9. 
Keywords: Bi-lingual, One-pass ASR, Pronunciation Modeling 
1. Introduction 
Words can be pronounced in more than one ways according to a lexicon; i.e., they usually 
have multiple pronunciations. Words are also pronounced differently by different people, a 
                                                 
∗ Chang Gung University, Taiwan 
 E-mail: rylyu@mail.cgu.edu.tw 
+ National Tsing Hua University, Taiwan 
∗∗ Academia Sinica, Taiwan 
  E-mail: {daucheng, chunnan}@iis.sinica.edu.tw 
  
Modeling Pronunciation Variation for Bi-Lingual               365 
Mandarin/Taiwanese Speech Recognition 
matrix [Cremelie and Martens 1998; Riley et al. 1999; Kam et al. 2003; Fukada et al. 1997; 
1998; Yang et al. 2000; Holter et al. 1999; Torre et al. 1997]. Techniques that achieve higher 
scores are chosen to serve as pronunciation variation rules. 
In addition to the pronunciation variation within a word, substantial variation occurs 
across word boundaries [Finke et al. 1997; Fukada et al. 1998; Kessens et al. 1999.]. Due to 
the mono-syllabic nature of Mandarin and Taiwanese, pronunciation variation is complex, and 
we can identify five types of variation: (1) one orthography with pronunciation variation; (2) 
colloquial/literate switching; (3) tone sandhi; (4) one orthography with multiple 
pronunciations; (5) one pronunciation with multiple orthography. The first three types of 
variation occur in mono-lingual environment, while the last two occur in bi-lingual 
environments. Details will be given in Section 3. 
The goal of this study was to construct a Mandarin/Taiwanese bi-lingual large vocabulary 
speech recognizer. We implemented a one-pass recognizer based on a bi-lingual acoustic 
model, an integrated pronunciation model, and a word searching net with tree-structured nodes. 
Most of the state-of-the-art speech recognizers, for either Western or Oriental languages, are 
implemented with the one-pass search strategy [Odell 1994; Aubert 1999; Hagen 2001]. In the 
acoustic modeling, one phonemic inventory called ForPA (Formosa Phonetic Alphabet) is 
used to transcribe bi-lingual corpora. [Lyu et al. 2004] According to this inventory, the 
acoustic models for similar sounds across languages are shared. In addition, we use an 
algorithm based on a decision tree to cluster similar acoustic models by means of the 
maximum likelihood criterion. In the pronunciation modeling, we integrate knowledge-based 
and data-driven approaches. If only the knowledge-based approach is adopted, some variation 
in the speech corpus can not be covered at all, while if only the data-driven approach is 
employed, the variation for each new corpus has to be determined. However, the more 
variations for each word there are in the searching net, the more the recognition time and 
confusability will increase. To limit the number of pronunciation variations for each Chinese 
character, we adopt a score based on the relative frequency ratio and choose the best average 
number of pronunciation variations. Furthermore, the tree-structured net directly uses each 
Chinese character as a searching node, which is also a new trial in the ASR field of Chinese 
languages. 
This paper is organized as follows. Section 2 states the problem. Section 3 represents the 
proposed framework, which includes acoustic modeling, pronunciation modeling, and a 
searching net. In section 4, we report experimental results and analyze three different 
pronunciation models using a bi-lingual testing set. The final section is a summary. 
 
  
Modeling Pronunciation Variation for Bi-Lingual               367 
Mandarin/Taiwanese Speech Recognition 
character in a word for a recognition network. Doing so will not only unnecessarily enlarge 
the searching space but also increase the time spent on decoding. 
2. Generating multiple pronunciation lexicons efficiently is not a trivial task. 
3. The language model for mixed languages is hard to estimate. 
4. When new acoustic features like tones are added to the system, all 3 layers in syllable 
decoding and in the syllable-to-character converter should be modified. This also is not a 
trivial task. 
3. Our Approach 
Unlike some conventional approaches, which divide the recognition task into syllable 
decoding and character decoding, our proposed approach adopts a one-stage searching 
strategy, as shown in Figure 3, which decodes the acoustic feature sequence X directly to 
obtain the desired character sequence C*, no matter what languages are spoken. The decoding 
equation is, thus, as follows: 
* ( ) arg max ( | ).
C
C X P C X=                                            (1) 
In this framework, character decoding can be implemented by searching in a three-layer 
network composed of an acoustic model layer, a lexical layer, and a grammar layer, as shown 
in Figure 4. There are at least 2 critical differences between our framework and the 
conventional one. 1) In the lexicon layer, character-to-pronunciation mapping can easily 
incorporate multiple pronunciations caused by multiple languages, including Japanese, Korean, 
and even Vietnamese, which also use Chinese characters. 2) In the grammar layer, characters 
instead of syllables are used as nodes in the searching net. Under this ASR structure, we do 
not care which language the user speaks. No matter whether the language is Taiwanese, 
Mandarin or a mixture of them in one sentence, the ASR outputs the Chinese character only. 
This makes it language independent! 
As in other multi-lingual researches [Young et al. 1997; Waibel 2000], determining how 
to efficiently and easily combine two languages in the acoustic and pronunciation models is 
very important. In the following two subsections, we will describe various approaches to 
integrating these two models in order to improve the recognition performance of ASR 
systems. 
 
 
Figure 3. One-stage searching strategy for Chinese speech recognition 
  
Modeling Pronunciation Variation for Bi-Lingual               369 
Mandarin/Taiwanese Speech Recognition 
Furthermore, in order to more efficiently merge the similar part of the sound for one 
phoneme or triphone model in both languages, we used a tying algorithm based on a decision 
tree to cluster the HMM models by using the maximum likelihood criterion [Liang et al. 1998; 
Lyu et al. 2002]. For the question sets, we used phonetic knowledge to design a total of 63 
questions, including 10 language-dependent questions, 11 common questions, 28 Initial 
questions, and 14 Final questions. Then, the tree grew and split as we chose the optimal one 
among all the questions to maximize the increase in the likelihood scores or the decrease in 
uncertainty. Finally, the convergence condition was set to halt the growth of the decision tree. 
The acoustic model used in the experiment depended on the different splitting and 
convergence criteria adopted. 
3.2 Pronunciation Modeling 
The pronunciation model plays an important role in the Chinese character-based ASR engine 
[Liu et al. 2003; Huang et al. 2000]. It not only provides more choices during decoding if the 
speaker exhibits variations in pronunciation but also handles various speaking styles [Lyu et al. 
2004]. As mentioned above, one Chinese character has more than two pronunciations in the 
combined phonetic inventory of Mandarin and Taiwanese. The factors of accent and regional 
migration can influence the pronunciation or speaking style of speakers too. Therefore, we 
identify the most common pronunciation variations in Taiwan in Table 2. 
In Table 2, we list the five pronunciation variations that the Mandarin-Taiwanese 
bi-lingual recognizer can handle. Take the Chinese character "走" as an example. It is 
pronounced as "zau51" in Taiwanese and means "to run" but is pronounced "zou21" in 
Mandarin and means "to walk." 
On the other hand, the total number of pronunciations in the pronunciation model for the 
decoding process is also important, because the more pronunciations are included in the 
lexicon, the more time the decoding process will take, and the less accurate of the ASR results 
will be [Strik et al. 1999]. The pronunciation variations will generate both improvements and 
deterioration in the ASR system, so previous research tried to find the optimal method to 
efficiently control the average pronunciation variations for one word in one language 
[Kesssens et al. 2003]. Our task is harder than that which deals with only one language. The 
reason is that one Chinese character must be mapped to at least two pronunciation variations, 
so cross-language confusion increases. In the following sections, we will propose two 
different methods, knowledge-based and data-driven methods, for obtaining rules of 
pronunciation variation. 
 
 
  
Modeling Pronunciation Variation for Bi-Lingual               371 
Mandarin/Taiwanese Speech Recognition 
same Chinese character is calculated; 3) the pairs with high relative frequencies are kept as 
multiple pronunciation rules. 
As our Mandarin knowledge source, we adopted the CKIP lexicon 
(http://ckip.iis.sinica.edu.tw/CKIP/) as our pronunciation lexicon source; it contains about 
78,410 words. The length of one word in the lexicon varies from one Chinese character to ten, 
and the average of the length is 2.4 Chinese characters per word. As our Taiwanese knowledge 
resource, we adopted the Formosa lexicon (ForLex) [Lyu et al. 2000], which contains 104,179 
words. The average length of one word in it is 2.8 Chinese characters. The pronunciation 
variation for each Chinese character was assigned a probability, which was estimated based on 
the frequency count of the pronunciations observed in both lexicons. The number of 
pronunciation variations for one Chinese character was 1.2 in the CKIP lexicon, and 2.1 in the 
Formosa lexicon. The number of pronunciation variations for Taiwanese was larger than that 
for Mandarin. The reasons are that most of the Chinese characters used in Taiwanese carry a 
classic literature pronunciation and a daily life pronunciation and that Taiwanese has much 
richer tone sandhi rules. Thus, the average number of pronunciation variations for one Chinese 
character is increased. 
3.2.2 Data-Driven Approach 
Although the regular pronunciation variations can be obtained from linguistic and 
phonological information, such as a dictionary, this information is not exhaustive; many 
phenomena in real speech have not yet been described. Therefore, another approach to 
deriving pronunciation variations from acoustic clues is presented below. All of the steps are 
also shown in Figure 5. 
Figure 5. Diagram of pronunciation variations obtained with a data-driven 
approach. 
  
Modeling Pronunciation Variation for Bi-Lingual               373 
Mandarin/Taiwanese Speech Recognition 
calculated based on the number of branches for each node, using equation (2). In the case 
shown in Figure 7, the entropy is 2.29, and the perplexity is 4.89, which is smaller than that of 
the linear searching net shown in Figure 6. 
                   2logi i
i
entropy p p= −∑ ,                           (2) 
                       2entropyperplexity = .                              (3) 
 
Figure 6. An example of an isolated linear searching net with its probability value. 
 
 
Figure 7. An example of an isolated tree-structured searching net with its 
probability value. 
4. Experimental Results and Analysis 
4.1 Corpus 
All of the experiments employed a bi-lingual corpus, called ForSDa (Formosa Speech 
Database) [Lyu et al. 2004]. Both the training and testing data were read speech, which was 
recorded in the 16 kHz/16-bit wave-format in a normal office environment. The training set 
included a total of 89,164 utterances from 100 speakers, including 50 males and 50 females.  
Every speaker recorded speech in both languages. The utterances were phonetically balanced 
words, which were selected from a lexicon of about 40,000 words, using the phonetic 
abundant algorithm [Lyu 2003]. The length of the word varied from 1 to 6. The testing set 
included 2,000 utterances from 20 speakers; 10 speakers recorded speech in Taiwanese, and 
the other 10 speakers recorded speech in Mandarin. The statistics of the corpus employed here 
  
Modeling Pronunciation Variation for Bi-Lingual               375 
Mandarin/Taiwanese Speech Recognition 
Table 5. CER (Character Error Rate) results for three pronunciation models with two 
testing sets. PKW: pronunciation modeling using the knowledge-based method; 
PDD: pronunciation modeling using the data-driven approach; PKW+DD: 
pronunciation modeling using both PKW and PDD. 
 PKW PDD PKW+DD 
Test_M 20.1% 17.9% 16.2% 
Test_T 17.6% 18.3% 15.0% 
4.4 Error Analysis 
The addition of pronunciation variants to a lexicon increases the confusability, especially if 
the lexicon is large. Here, the large increase in confusability was probably the reason why 
only a small improvement or even deterioration in performance is found. The experimental 
results represented in Figure 8 show the CER performance as a function of the number of 
pronunciation variations for each Chinese character. It can be seen that the CER decreased 
when the average number of pronunciation variations increased. The lowest CER results were 
obtained when the number of pronunciation averaged 3.9. This was achieved using PKW+DD 
and by eliminating variants with probabilities smaller than 0.1. 
14
16
18
20
2.6 3.4 3.9 4.5 5.1
Average number of pronunciations per Chinese character
CE
R(
%
)
Test_M Test_T
 
Figure 8. CER performance for PKW+DD with different numbers of pronunciation 
variations per Chinese character. 
 
Moreover, the error types mentioned above can be classified into the following 3 sets. 
    A. Cross-language homophonic confusion 
    This kind of error is just like the fifth term in Table 1, and occurs when different Chinese 
words belonging to different languages have the same or similar pronunciation. Therefore, the 
  
Modeling Pronunciation Variation for Bi-Lingual               377 
Mandarin/Taiwanese Speech Recognition 
The experimental results showed that the CER could be improved by using the three 
different pronunciation models. The best performance was 16.2% and 15.0% for the testing 
sets Test_M and Test_T, respectively, where the perplexity was 15,249 for 30,000 words, and 
the PKW+DD pronunciation model was used. In addition, in order to limit the side effect where 
in the increase in the size of the pronunciation lexicon causes the performance to deteriorate, 
the average number of pronunciations for both languages was 3.9. 
The method proposed in this paper has been applied to two languages in the Chinese 
language family, but it can be easily extended to other languages or dialects. We have also 
discussed the major five pronunciation variations found in Taiwan. This is the first work, to 
the best of our knowledge, that has systemically investigated pronunciation variations in 
Mandarin and Taiwanese speech conversion to Chinese characters using ASR technology. 
References 
Aubert, X., “One pass cross word decoding for large vocabularies based on a lexical tree 
search organization,” In Proceedings of the European Conference on Speech 
Communication and Technology, 1999, Budapest, Hungary, pp. 1559-1562. 
Bacchiani, M., and M. Ostendorf, “Joint lexicon, acoustic unit inventory and model design,” 
International Journal of Speech Communication, 29(2-4), 1999, pp. 99-114. 
Chao, Y. R., Tone contour, http://en.wikipedia.org/wiki/Tone_contour/, 1979. 
Cremelie, N., and J.-P. Martens, “In search of pronunciation rules,” In Proceedings of the 
European Speech Communication Association (ESCA) Workshop on Modeling 
Pronunciation Variation for Acoustic Speech Recognition, 1998, Rolduc, Kerkrade, pp. 
103-108. 
Downey, S., and R. Wiseman, “Dynamic and static improvements to lexical baseforms,” In 
Proceedings of the Workshop on Modeling Pronunciation Variations, 1998, Roldue, pp. 
157-162. 
Finke, M., and A. Waibel, “Speaking mode dependent pronunciation modeling in large 
vocabulary conversational speech recognition,” In Proceedings of the European 
Conference on Speech Communication and Technology, 1997, Rhodos, Greece, pp. 
2379-2382. 
Fukada, T., and Y. Sagisaka, “Automatic generation of a pronunciation dictionary based on a 
pronunciation network,” In Proceedings of the European Conference on Speech 
Communication and Technology, 1997, Rhodos, pp. 2471-2474. 
Fukada, T., T. Yoshimura, and Y. Sagisaka, “Automatic generation of multiple pronunciations 
based on neural networks and language statistics,” In Proceedings of the European 
Speech Communication Association (ESCA) Workshop on Modeling Pronunciation 
Variation for Acoustic Speech Recognition, 1998, Rolduc, Kerkrade, pp. 103-108. 
Holter, T., and T. Svendsen, “Maximum likelihood modelling of pronunciation variation,” 
International Journal of Speech Communication, 29, 1999, pp. 177-191. 
  
Modeling Pronunciation Variation for Bi-Lingual               379 
Mandarin/Taiwanese Speech Recognition 
Liu, Y., and P. Fung, “Modeling partial pronuncia-tion variations for spontaneous Mandarin 
speech recog-nition,” International Journal of Computer Speech and Language, 17, 
2003, pp. 357-379. 
Liu, Y., and P. Fung, “Partial change accent models for accented Mandarin speech 
recognition,” In Proceedings of the IEEE Workshop on ASRU, 2003, St. Thomas, U.S. 
Virgin Islands. 
Liu, Y., and P. Fung, “State-Dependent Phonetic Tied Mixtures with Pronunciation Modeling 
for Spontaneous Speech Recognition,” IEEE Transactions on Speech and Audio 
Processing, 12, 2004, pp. 351-364. 
Lyu, D.C., B.H. Yang, M.S. Liang, R.Y. Lyu, and C.N. Hsu, “Speaker Independent Acoustic 
Modeling for Large Vocabulary Bi-lingual Taiwanese/Mandarin Continuous Speech 
Recognition,” In Proceedings of the 9th Australian International Conference on Speech 
Science & Technology, 2002, Melbourne, Australia. 
Lyu, D.C., M.S. Liang, Y.C. Chiang, C.N. Hsu, and R.Y. Lyu, “Large Vocabulary Taiwanese 
(Min-nan) Speech Recognition Using Tone Features and Statistical Pronunciation 
Modeling,” In Proceedings of the 8th European Conference on Speech Communication 
and Technology, 2003, Geneva, Switzerland. 
Lyu, D.C., M.S. Liang, Y.C. Chiang, C.N. Hsu and R.Y. Lyu, “Large Vocabulary Taiwanese 
(Min-nan) Speech Recognition Using Tone Features and Statistical Pronunciation 
Modeling,” In Proceedings of the European Conference on Speech Communication and 
Technology, 2003, Geneva, Switzerland. 
Lyu, R.Y., C.Y. Chen, Y.C. Chiang, and M.S. Liang, “Bi-lingual 
Manda-rin/Taiwanese(Min-nan), Large Vocabulary, Continuous Speech Recognition 
System Based on the Yong-yong Phonetic Alphabet,” In Proceedings of the 
International Conference on Spoken Language Processing, 2000, Beijing, China. 
Lyu, R.Y., D.C. Lyu, M.S. Liang, M.H. Wang, Y.C. Chiang, and C.N. Hsu, “A Unified 
Framework for Large Vocabulary Speech Recognition of Mutually Unintelligible 
Chinese "Regionalects",” In Proceedings of the 8th International Conference on Spoken 
Language Processing, 2004, Jeju Island, Korea. 
Lyu, R.Y., M.S. Liang, and Y.C. Chiang, “Toward Constructing A Multilingual Speech 
Corpus for Taiwanese (Minnan), Hakka, and Mandarin,” International Journal of 
Computational Linguistics and Chinese Language Processing, 9(2), 2004, pp. 1-12. 
Odell, J.J., V. Valtchev, P.C.Woodland, and S.J.Young, “A One Pass Decoder Design for 
Large Vocabulary Recognition,” In Proceedings of Human Language Technology 
Workshop, 1994, pp. 405-410. 
Peters, S.D., and P. Stubley, “Visualizing speech trajectories,”  In Proceedings of the 
European Speech Communication Association (ESCA) Workshop on Modeling 
Pronunciation Variation for Acoustic Speech Recognition, 1998, Rolduc, Kerkrade, pp. 
103-108. 
Polzin, T.S., and A.H. Waibel, “Pronunciation variations in emotional speech,” In 
Proceedings of the European Speech Communication Association (ESCA) Workshop on 
 Computational Linguistics and Chinese Language Processing 
Vol. 9, No. 2 , August 2004, pp. 1-12                                                                          1 
© The Association for Computational Linguistics and Chinese Language Processing 
Toward Constructing A Multilingual Speech Corpus for 
Taiwanese (Min-nan), Hakka, and Mandarin 
Ren-yuan Lyu*, Min-siong Liang+, Yuang-chin Chiang**   
Abstract 
The Formosa speech database (ForSDat) is a multilingual speech corpus collected 
at Chang Gung University and sponsored by the National Science Council of 
Taiwan. It is expected that a multilingual speech corpus will be collected, covering 
the three most frequently used languages in Taiwan: Taiwanese (Min-nan), Hakka, 
and Mandarin. This 3-year project has the goal of collecting a phonetically 
abundant speech corpus of more than 1,800 speakers and hundreds of hours of 
speech. Recently, the first version of this corpus containing speech of 600 speakers 
of Taiwanese and Mandarin was finished and is ready to be released. It contains 
about 49 hours of speech and 247,000 utterances. 
Keywords: Phonetic Alphabet, Pronunciation Lexicon, Phonetically Balanced 
Word, Speech Corpus 
1. Introduction 
To design a speaker independent speech recognition system, it is essential to collect a large-
scale speech database. Taiwan (also called Formosa historically), which has become famous 
for its IT industry, is basically a multilingual society. People living in Taiwan usually speak at 
least two of the three major languages, including Taiwanese (also called Min-nan in the 
linguistics literature), Hakka and Mandarin, which are all members of the Chinese language 
family. In the past several decades, most of the researchers studying natural language 
processing, speech recognition and speech synthesis in Taiwan have devoted themselves to 
research on Mandarin speech. Several speech corpora of Mandarin speech have, thus, been 
collected and distributed [Wang et al., 2000; Godfrey, 1994]. However, little has been done 
                                                        
* Dept. of Computer Science and Information Engineering, Chang Gung University, Taoyuan, Taiwan 
Email: rylyu@mail.cgu.edu.tw                Tel: 886-3-2118800ext5967, 5709 
+ Dept. of Electrical Engineering, ,Chang Gung University, Taoyuan, Taiwan 
** Inst. of Statistics, National Tsing Hua University, Hsin-chu, Taiwan 
  
Toward Constructing A Multilingual Speech Corpus for                         3 
Taiwanese (Min-nan), Hakka, and Mandarin 
years. However, both systems are inadequate for application to the other members of the 
Chinese language family, like Taiwanese (Min-nan) and Hakka. Among the phonetic systems 
useful for Taiwanese and Hakka, there are Church Romanized Writing (CR, also call Peh-e-ji, 
「白話字」) [Chiung 2001] for Taiwanese and the Taiwan Language Phonetic Alphabet 
(TLPA) [Ang 2002] for Taiwanese and Hakka. Because the same phonemes are represented 
using different symbols in Pinyin, CR and TLPA, it is confusing to learn these phonetic 
systems simultaneously. For example, the syllable “pa(八)” in TLPA and “pa(趴)” in CR may 
be confused with each other because the phoneme /p/ is pronounced differently in the two 
systems. 
Therefore, it is necessary to design a more suitable phoneme set for multilingual speech 
data collection and labeling [Zu, 2002][Lyu, 2000]. The whole phone set for the three major 
languages used in Taiwan is listed in Table 1 for four phonetic systems: MPA, Pinyin, IPA, 
and the newly proposed ForPA. Table 1 also lists examples of syllables and characters which 
contain the target phonemes. 
It is known that phonemes can be defined in many different ways, depending on the level 
of detail desired. The labeling philosophy adopted in ForPA is that when faced with various 
choices, we prefer not to divide a phoneme into distinct allophones, except in cases where the 
sound is clearly different to the ear or the spectrogram is clearly different to the eye. Since 
labeling is often performed by engineering students and researchers (as opposed to 
professional phoneticians), it is generally safer to keep the number of units as small as 
possible, assuming that the recognizer will be able to learn any finer distinctions that might 
exist within any context. Generally speaking, ForPA might be considered as a subset of IPA, 
but it is more suitable for application to the languages used in Taiwan. 
Table 1. The phone set for the three languages in Taiwan, represented as different 
phonetic systems. The Chinese character in parentheses followed by a 
syllable, is an example character used in Mandarin, e.g., “ba(八 )” is 
pronounced in Mandarin as syllable “ba”, without considering the tone. 
For phonemes not found in Mandarin Chinese, we use Chinese character 
pronounced as Taiwanese (T) or Hakka (H) to be example characters. For 
example, “bha(肉T)” meaning “肉” is pronounced “bha” in Taiwanese. 
 
 
  
Toward Constructing A Multilingual Speech Corpus for                         5 
Taiwanese (Min-nan), Hakka, and Mandarin 
3. The process of producing phonetically balanced word sheets 
Based on the three pronunciation lexicons transcribed in ForPA, we extracted sets of distinct 
syllables and inter-syllabic bi-phones from the three languages. The statistics of the phonetic 
units considered here are listed in Table 3. In order to collect speech data related to the co-
articulation effect of continuous speech, we extracted phonetically abundant word sets. 
Therefore, the chosen phonetic units were not only base-syllables, phones, and RCD phones, 
but also Initial-Finals, RCD Initial-Finals and inter-syllabic RCD phones. The process of 
selecting such a word set is actually a set-covering optimization problem [Shen et al., 1999], 
which is NP-hard. Here, we adopted a simple greedy heuristic approximate solution [Cormen, 
2001]. 
First, we set the requirements of the word set as to cover the following phonetic units: 
Base-syllables and Inter-syllabic RCD phones. Accordingly, the selected word set could cover 
all the phones, Initial-Finals, RCD phones, RCD Initial-Finals, Base-syllables and Inter-
syllabic RCD phones. In this way, we could obtain several sets of words for our balance-word 
data sheets. All the statistics of the phonetic units considered here are listed in Table 3. [Liang 
2003]. 
 
Table 3. The numbers of distinct subwod units for each of the three languages and 
their unions, where T: Taiwanese; H: Miaulik-Hakka M: Mandarin; ∪: 
union. 
Language Base syllable Phones Within-syllabic bi-phones Inter-syllabic bi-phones 
T 832 53 410 716 
H 683 53 327 696 
M 429 45 208 234 
T∪H 1134 70 583 1036 
T∪M 1055 64 486 809 
H∪M 939 71 435 797 
T∪H∪M 1326 78 600 1105 
3.1. Data sheets 
The process of producing data sheets is depicted in Fig.2. Before we produced the data sheets, 
we defined the sheets’ coverage rate. The coverage rate of the sheets was defined as the total 
number of base-syllables (or inter-syllabic phones) over the number of all possible distinct 
base-syllables (or inter-syllabic phones). The format of the data sheet is partially shown in 
Table 4. 
 
 
  
Toward Constructing A Multilingual Speech Corpus for                         7 
Taiwanese (Min-nan), Hakka, and Mandarin 
4.1. The telephone recording system 
The telephone system is set up in the Multi-media Signal Process Laboratory at Chang Gung 
University. The speakers dial into the laboratory using a handset telephone. Before recording, 
we give the speakers prompt sheets. The input signal is in format of 8K sampling rate with 8-
bits µ-law compression. The speakers utter words while reading the prompt sheet, and 
supervised prompt speech is played to help the speakers follow the prompt speech to finish the 
recording. After recording, all speech data are saved in a unique directory. Figure 3 shows the 
recording process carried out using the telephone system. 
Welcome words 
Input # of prompt 
Serial number 
 
4.2. The microphone recording system 
When we record a waveform into a computer, it is not convenient to type the file name 
necessary for saving it. Therefore, we use a good tool (DQS3.1) [Chiang 2002] to record 
speech. If we create a script in a specific form for this software, we can record the waveform 
easily and get a labeled file, which contains information of transcription using ForPA. Then, 
we simply set up the system on a notebook computer and take it wherever we want to record 
speech. 
5. Speaker recruiting 
We employ several part-time assistants to recruit speakers around Taiwan. Each speaker is 
Press 1 to listen to the next 
prompting speech 
Prompting speech 
Recording speech 
Finish recording 
Given by 
system 
Press 3 to listen to the 
prompting speech 
Press 1 to save the recorded waveform 
Thank you, bye bye!
Figure 3. The telephone recording system. 
 
  
Toward Constructing A Multilingual Speech Corpus for                         9 
Taiwanese (Min-nan), Hakka, and Mandarin 
“M0” means that the recording channel used was a microphone and gender was female, and so 
on. Every speaker has a unique serial number and speech data, which contain a transcription 
of waveforms made in the early stage and are stored in a unique folder named according to the 
serial number. The database structure is shown in Fig.5. All the statistics of the database are 
listed in Table 5. 
 
 
 
 
 
 
 
 
 
 
 
Figure 5. The structure of database for Taiwanese and Mandarin. (TW01: 
Taiwanese database collected in 2001; M0: the microphone channel 
was used and the gender was female, T1: the telephone channel was 
used and the gender was male; and so on. There is a transcription file 
for each unique speaker.) 
 
Table 5. The statistics of utterances, speakers and data length for speech 
collected over microphone and telephone channels in Taiwanese 
and Mandarin (MIC: microphone; TEL: telephone). 
Name Channel Gender Quantity Train(hr) Test (hr) 
TW01-M0 Female 50 5.92 0.29 
TW01-M1 Male 50 5.44  
MD01-M0 Female 50 5.65 0.27 
MD01-M1 Male 50 5.42  
TW02-M0 Female 233 10.10 0.70 
TW02-M1 
MIC 
Male 277 11.66  
TW02-T0 Female 580 29.21 0.95 
ForSDAT 
TW02-T1 
TEL 
Male 412 19.37  
 
 
ForSDAT 
TW01
MD01
TW02
Waveforms 
list
TCP …
.. 
.. 
TW01M01234
M0
M1
M0
M1
M1
M0
 
  
Toward Constructing A Multilingual Speech Corpus for                         11 
Taiwanese (Min-nan), Hakka, and Mandarin 
inturns in one work sheet, etc. These directories are also considered unusable. 
7.2. Step 2: phonetic transcription by means of forced alignment 
After the speech data is pre-processed, we validate it to determine whether the labels that 
consist of phonetic transcriptions correspond to the speech data. We use two methods to 
achieve this goal. First, we use HTK [Steven, 2002] to perform forced-alignment 
automatically on an utterance using all possible syllable combinations. We keep the highest 
scores for combinations to transcribe the speech. Secondly, we use the TTS (text-to-speech) 
technique to synthesize all the labels that were transcribed using HTK and then we transcribe 
the speech manually using more appropriate phonetic symbols. Finally, we can construct a 
relational database using ACCESS to record all the profiles of the speakers (see Fig.3) and 
what they recorded. Therefore, we can query the speech database using the SQL language to 
find the waveforms transcribed using the specific phones or syllables or even query who 
recorded the specific-phone waveforms. This step is on-going and will be finished soon. 
8. Conclusion 
Version 1.0 of this corpus containing the speech of 600 speakers of Taiwanese (Min-nan) and 
Mandarin Chinese has been finished and is ready to be released. We have collected the speech 
of 1,773 people, including 49.47 hours of speech and 247,027 utterances. As work on this 
project continues, more Hakka and Mandarin speech data will be collected. 
References 
Wang, H. C., F. Seide, C.Y. Tseng and L.S. Lee, “Mat-2000 – design, collection, and 
validation of a mandarin 2,000-speaker telephone speech database,” International 
Conference on Spoken Language Processing 2000, Beijing, China, 2000. 
Godfrey, J., “Polyphone: Second anniversary report,” International Committee for Co-
ordination and Standardisation of Speech Databases Workshop 94, Yokohama, Japan, 
1994. 
Zu, Y., “A super phonetic system and multi-dialect Chinese speech corpus for speech 
recognition,” International Conference on Spoken Language Processing 2002, Denver, 
USA, 2002. 
Lyu, R. Y., “A bi-lingual Mandarin/Taiwanese (Min-nan), Large Vocabulary, Continuous 
speech recognition system based on the Tong-yong phonetic alphabet (TYPA),” 
International Conference on Spoken Language Processing 2000, Beijing, China, 2000. 
 
PHONETIC TRANSCRIPTION USING SPEECH RECOGNITION TECHNIQUE
CONSIDERING VARIATIONS IN PRONUNCIATION
Min-Siong Liang, Ren-Yuan Lyu* and Yuang-Chin Chiang
Dept. of Electrical Engineering, Chang Gung University, Taiwan.
Dept. of Computer Science and Information Engineering, Chang Gung University, Taiwan.
Institute of Statistics, National Tsing Hua University, Taiwan.
*E-mail: renyuan.lyu@gmail.com
ABSTRACT
We propose a new approach for performing phonetic tran-
scription of speech and text that combines automatic speech
recognition (ASR) and grapheme -to- phoneme (G2P) tech-
niques. By augmenting the text with speech and using auto-
matic speech recognition with a sausage searching net con-
structed from multiple text pronunciations corresponding to
human speech utterance, we are able to reduce the effort for
phonetic transcription. By using a multiple pronunciation lex-
icon, a transcription error rate of 12.74% was achieved. Fur-
ther improvement can be achieved by adapting the pronunci-
ation lexicon with pronunciation variation (PV) rules and an
error rate reduction of 17.11% could be achieved.
Index Terms— Automatic Phonetic Transcription, Pro-
nunciation Variation, Chinese, Taiwanese, Dialect.
1. INTRODUCTION
Automatic phonetic transcription is gaining popularity in the
speech processing field, especially in speech recognition, text-
to-speech and speech database construction [1]. It is tradi-
tionally performed using two different approaches: an acous-
tic feature input method and text input method. The former is
the speech recognition task, or more specifically, the phoneme
recognition task. The latter is the G2P task. Both tasks, in-
cluding phoneme recognition and G2P remain unsolved tech-
nology problems. The state-of-the-art speaker-independent
(SI) phone recognition accuracy in a large vocabulary task is
currently less than 80%, far away from human expectations.
Although the accuracy of G2P tasks seems much better, it
relies on a “perfect” pronunciation lexicon and cannot effec-
tively deal with pronunciation variation issues.
This problem becomes non-trivial when the target text is
the Chinese text (C). The Chinese writing system is widely
used in China and the East/South Asian areas including Tai-
wan, Singapore, and Hong-kong. Although the same Chi-
nese character is used in different areas, the pronunciation
may be very different. Therefore, they are mutually unin-
telligible and considered different languages rather than di-
alects by most linguists. In this paper, we chose a text cor-
pus derived from the Buddhist Sutra (written collections of
Buddhist teachings). Buddhism is a major religion in Taiwan
(23% of the population). The Buddhist Sutra, translated into
Chinese text in a terse ancient style (Z) , is commonly
read in Taiwanese (Min-nan) . Due to lack of proper edu-
cation, most people are not capable of correctly pronouncing
all of the text. Besides, no qualified pronunciation lexicon
exists and very few appropriately computational linguistic re-
searches were conducted to support developing a G2P system.
Taiwanese uses Chinese characters as a part of the writ-
ten form, with its own phonetic system, which is very differ-
ent from Mandarin. This is in contrast to the case of Man-
darin, where the problem of multiple pronunciations (MP)
is less severe. A Chinese character in Taiwanese commonly
can have a classic literate pronunciation (known as Wen-du-
in, or “Z\¯” in Chinese) and a colloquial pronunciation
(known as Bai-du-in, or “ç\¯” in Chinese)[2]. In addi-
tion to MPs, Taiwanese also have a pronunciation variation
(PV) due to sub-dialectical accents, such as Tainan and Taipei
accents. We use the term MPs to stress the fact that variation
may cause more deterioration in phonetic transcription.
The traditional approach to transcribing Chinese Bud-
dhist Sutra text uses human dictation. A master monk or nun
reads the text aloud, sentence by sentence. The manual tran-
scription process is tedious and prone to errors. Since more
transcribed Sutras are planned, we are interested in how ASR
and G2P technology can help in this situation. Our task is to
discover which of them is actually pronounced. It is much
easier to acquire a person to record his/her reading of the text
than acquiring a transcribing expert. For marginalized lan-
guages with serious MPs and PV problems, this technique is
very useful.
2. THE PHONETIC TRANSCRIPTION TECHNIQUE
The flow chart shown in Fig. 1 is the framework of phonetic
transcription using the speech recognition technique. Based
on flow chart in Fig. 1, we define: s is the syllable sequence,
ducted using Eq. 3 depends only on the text input and are
referred as the language part performance.
What is proposed in this paper is an approach to inte-
grate both. Given a Chinese character sequence, based on
the MPs of each Chinese character, a much smaller recog-
nition net can be constructed. Take an example of a typical
text sentence “Ò1°”, which is shown in Fig. 2. We call
such a net as sausage net, which is named for its shape like a
sausage. Higher recognition accuracy can be expected due to
the smaller perplexity in the recognition net.
3.1. The Pronunciation Lexicons, Recognition Nets and
Results
The Formosa Lexicon could be used for a wide range of ap-
plications and tends to have a higher number of multiple pro-
nunciations in Taiwanese [2]. However, some pronunciations
do not appear in the Formosa Lexicon due to pronunciation
variations. Thus, the second lexicon, called the Sutra Lexi-
con, is derived from the Sutra itself to study what variations
exist from the Formosa lexicon to Sutra Lexicon. The per-
formance of phonetic transcription using the Sutra Lexicon
is looked upon as the upper bound performance for the pho-
netic transcription. In addition to the above two separate lexi-
cons, a combined lexicon is called the Enhanced Lexicon for
convenience. In recognition nets, the first is the free-syllable
net, denoted as the Free-Syl-Net. The other three search nets
are the sausage nets were constructed by filling in each node
of the net with the corresponding multiple pronunciations of
each Chinese character from each of the three pronunciation
lexicons. The nets are denoted the General-Sau-Net, Specific-
Sau-Net, and Enhanced-Sau-Net for the Formosa Lexicon,
Sutra Lexicon, and Enhanced Lexicon.
With the four search nets and acoustic models, the recog-
nition results are shown in Fig. 3. In addition, we also show
the result of only language, called G2P, with unigram. Through
observing the experimental results, neither G2P with unigram
nor Free-Syl-Net with adaptation model can reach acceptable
performance. Therefore, it is necessary to integrate the lan-
guage and acoustic parts. The General-Sau-Net could com-
pete with the Specific-Sau-Net. Thus, if the speaker indepen-
dent model could be adapted using some phonetically tran-
scribed speech data, the adapted speaker independent model
under the General-Sau-Net would be suitable for phonetic an-
notation task. Although some pronunciations of the Buddhist
Sutra Chinese characters do not appear in the Formosa Lex-
icon, the performance of the Formosa Lexicon Sausage Net
degrades not much more than the Sutra Lexicon Sausage Net.
So far, the Enhanced Lexicon Sausage Net includes all pos-
sible pronunciations of Sutra Chinese characters, but it may
increase the perplexity of the search net. Practically, some er-
rors result from pronunciation variations by our speech data
observation. Therefore, we determined that the performance
would get better by trivial adaptation of the Formosa Lexicon
¬° ¥À »¡ ªk
ko
sua
t
sue
sue
hua
t
ui
bo
bu
Fig. 2. The net is constructed from the multiple pronunciations of
each Chinese character from our Formosa Lexicons.
41.41
12.74
9.36
7.65
38.30
0
5
10
15
20
25
30
35
40
45
50
Free-Syl-Net General-Sau-Net Enhanced-Sau-Net Specific-Sau-Net
Sy
lla
bl
e 
Er
ro
r R
at
e(%
)
SI w/ adaptation
G2P(unigram)
Fig. 3. Syllable error rate (SER) under four searching nets. See text
in subsection 3.1 for notations.
Sausage net.
4. INCORPORATING PRONUNCIATION
VARIATION RULES
Because insufficient coverage of pronunciations in the search
net will severely degrade the recognition performance, some
approaches to extend the pronunciation coverage will be con-
sidered to help the overall performance. The simple way to
adopt the methodology of pronunciation variation is to ex-
pand the pronunciation lexicon using variation rules of the
form LBR → LSR where B and S represent the base form
and surface form of a central phone, and L, R are the left and
right contexts respectively [3]. To derive such rules, a speech
corpus with both canonical pronunciation and actual pronun-
ciation is necessary. We choose a subset of ForSDAT, called
ForSDAT-02, to derive PV rules shown in Table 1.
A small portion of the ForSDAT-02 was then manually
checked and the phonetic transcription of the transcript “cor-
rected” according to actual speech. The triphone-level con-
fusion table is built and used as a direct knowledge source
to derive the PV rules, where each cell in the table is looked
upon as a rule. The enormous number of rule set selections is
2P
2
, where P is the number of triphone. To make the problem
more solvable, some specially designed algorithms should be
developed. The mathematic definitions of the 3 kinds of stas-
tical measures are as follows:
1. Joint probability (JP) of the base form pronunciation
bi, and the surface form pronunciation sj , p(bi, sj) = nij/N .
Data-Driven Approach to Pronunciation Error Detection for Computer 
Assisted Language Teaching 
 
 
Min-Siong Liang1, Zien-Yong Hong2, Ren-Yuan Lyu2, Yuang-Chin Chiang3 
1.Dept. of Electrical Engineering, Chang Gung University, Taoyuan, Taiwan 
2. Inst. of Computer Science and Information Engineering, Chang Gung University,Taiwan 
3.Inst. of Statistics, National Tsing Hua University, Hsin-chu, Taiwan 
E-mail: {minsiong, renyuan.lyu}@gmail.com   Tel: 886-3-2118800 ext 5967 
 
 
Abstract 
 
This paper describes an approach to pronunciation 
error detection for Computer-Assisted Pronunciation 
Teaching (CAPT). We focus on how to find the real 
pronunciation of the user. The data-driven based 
method was used to generate pronunciation errors 
hypotheses instead of knowledge-based method. In the 
experiment results, the error rate of pronunciation 
detection can achieve 10.56%. Finally, we applied this 
technique into our CAPT system. 
 
1. Introduction 
 
This paper describes an approach to pronunciation 
error detection for Computer-Assisted Pronunciation 
Teaching (CAPT). For CAPT, many researchers have 
developed it using speech recognition and synthesis 
techniques [1]. So far, most approaches for CAPT used 
the Hidden Markov Models (HMM) log-likelihood-
based algorithm score [2], but few could detect and 
verify error for users. However, the report by Neri et al. 
[3] claimed that the effectiveness of learning depended 
on the corrective feedback of a CAPT system, which 
needed a precise pronunciation error detector. In 
addition, the pronunciation errors hypotheses often 
used linguistic knowledge, which was often language-
dependent and derived by more than one linguists [4], 
but the linguistic knowledge was sometimes 
contradictory with each other. In this paper, therefore, 
we used data-driven based method instead of 
knowledge-based method for generation of 
pronunciation errors hypotheses and proposed a new 
framework for CAPT. 
 We were trying to incorporate our approach to our 
mother-tongue language, i.e. Taiwanese. Unfortunately, 
due to lack of elementary education for this language, 
new generation in Taiwan can not speak and listen to it. 
Although Taiwanese uses Chinese characters as a part 
of the written form, with its own phonetic system, it is 
very different from Mandarin. This is in contrast to the 
case of Mandarin, where the problem of multiple 
pronunciations (MP) is less severe. A Chinese 
character in Taiwanese commonly can have a classic 
literate pronunciation (known as Wen-du-in, or “文讀
音” in Chinese) and a colloquial pronunciation (known 
as Bai-du-in, or “白讀音” in Chinese) [1]. In addition 
to MPs, Taiwanese also have a pronunciation variation 
(PV) due to sub-dialectical accents, such as Tainan and 
Taipei accents. We use the term MPs to stress the fact 
that variation may cause more deterioration. Finally, 
we also needed to find error patterns when students, 
who spoke Mandarin in daily life, started to learn 
Taiwanese. Therefore, it might be the better way to 
solve this problem using the data-driven approach. 
 In this paper, we focus on how to find the real 
pronunciation of the user and the detail will describe as 
the following sections. 
 
2. The Pronunciation Error Detection 
using Speech Recognition Technique 
 
The flow chart shown in Fig. 1 is the framework of 
phonetic transcription of utterances using the speech 
recognition technique. While the input is a speech 
waveform of a student and Chinese text of the lecture, 
the output is a phonetic transcription corresponding to 
pronunciation of the user. The entire framework can be 
divided into two major parts, i.e. an acoustic part and a 
language part. 
 Based on flow chart in Fig. 1, we define: s is the 
syllable sequence, while c and o are the input acoustic 
sequences and augmented character. The phonetic 
transcription target is to find the most probable syllable 
sequence s* given o and c. The formula is: 
* argmax ( | , )
s S
s P s o c
∀ ∈
=  (1) 
…   …  …   
a-m 0 0 … nPj … 0  
 1315 1102  Mj  107 N 
Table 1. Triphone-level confusion table, where the 
notations were described in section 3.2 
 
3.3. Ranking PV Rules with Data-driven 
Method 
 
Three kinds of statistical measures were used in 
this paper. They are (1) Joint probability, (2) 
Conditional probability, and (3) Mutual information of 
the base form pronunciation bi, and the surface form 
pronunciation sj. The mathematic definitions of the 
above 3 measures are as follows: 
 (1) Joint probability of bi and sj, 
( , ) /i j ijp b s n N=  
(2)Conditional probability of bi and sj, 
( | ) /j i ij ip s b n N=  
(3) Mutual information of bi and sj, 
( , )
( , ) log log( )
( ) ( )
i j ij ij
ij i j
i j ij iji j
p b s n n
I p b s N
p b p s N n n
= = ∗ ∗∑ ∑  
In all the above equations, nij is the number of (base-
form) triphone bi substitutions by the surface-form 
triphone sj that appear in a corpus, and 
 
iji j
N n=∑ ∑ , i ijjN n=∑ ,  
 p(bi, sj) represents the joint probability of (bi, sj),  
 p(bi) and p(sj) equal the marginal probability of  bi 
and sj, respectively. 
 Note that each pair (i,j), i ≠ j, corresponds to a 
substitution rule and we select those pairs (i,j) with 
higher scores of p(bi, sj), p(bi, sj)  and Iij to be the 
variation rules to extend the sausage net pronunciation. 
 
4. Evaluation of the Data-Driven methods 
 
The testing data was the spoken Taiwanese corpus of 
Buddhist Sutra (written collections of Buddhist 
teachings), which is collected by a nun. There are 533 
utterances in this speech data with total length of about 
46 minutes. 502 utterances, which include 5909 
syllables, are randomly chosen and reserved for testing 
while as another 31 utterances are used for acoustic 
model adaptation.  
 The experiment results were shown in Fig. 3. 
Under the speaker adaptation models, the result was 
12.7% with the CPN. The adapted speaker independent 
model under the correct pronunciation net could be 
considered as the baseline of the pronunciation 
detection task.  
 It is interesting to point out that, in Fig. 3, choosing 
different statistical measures will influence the 
achievable lowest SER. In these experiments, we 
found that MI is the best in terms of the rate of 
decrease in SER or the achievable lowest SER. In the 
MI-based method, the formula could avoid slow 
convergence using the Joint-Probability as weight 
when the base-form would get few variations. 
Consequently, the error rate of the performance of the 
MI method in error reduction was also better than JP 
and CP methods, respectively. Then, the error rate 
reduction of MI method was 17.11%. 
12.7
11.81
11
10.56
10
11
12
13
CPN JP Best CP Best MI Best
S
yl
la
bl
e 
Er
ro
r R
at
e 
(%
)
 
Fig. 3.  The experiment results of CPN (baseline) and three 
best results using different data-driven methods. 
 
5. Conclusion 
We have proposed a new approach to address the 
pronunciation detection for learning Taiwanese 
pronunciation. By using a MP lexicon, a transcription 
error rate of 12.7% was achieved. In addition, the 
adaptation of correct pronunciation net (CPN) with 
pronunciation variation rules was used instead of 
global pronunciation lexicon modification. Further 
improvement of an error rate reduction of 17.11% 
could be achieved. Finally, we have also applied this 
technique to our CAPT system. 
 
6. References 
 
[1] M.-S. Liang, et al., “A Taiwanese Text-to-Speech 
System with Applications to Language Learning”, In Proc. 
ICALT 2004, Joensuu, Finland, (2004). 
[2] S. Wei, et al., “Automatic Mandarin Pronunciation 
Scoring for Native Learners with Dialect Accent”, In Proc. 
Interspeech 2006, Pittsburgh, Pennsylvania, 2006. 
[3] A. Neri, et al., “ASR-based Corrective Feedback on 
Pronunciation: does it really work?”, In Proc. Interspeech 
2006, Pittsburgh, Pennsylvania, 2006. 
[4] J.-C. Chen, et al., “Formant-Based English Vowel 
Assessment for Chinese in Taiwan”, In Proc. Interspeech 
2006, Pittsburgh, Pennsylvania, 2006. 
[5] R.-Y. Lyu, et al., “Toward Constructing A Multilingual 
Speech Corpus for Taiwanese (Minnan), Hakka, and 
Mandarin”, IJCLCLP, Vol. 9, No. 2, August 2004,  pp. 1-12. 
Phonetic  
Transcription

¦p¬O§Ú»D
¬°¥À»¡ªk
Chinese  Text
(Sutra
)
Manual
Correction
Text, e.g.
Waveforms, e.g.
e.g.
ASR

Figure 1: The process of semi-automatic transcription of Chinese
text to Taiwanese pronunciation using ASR technique.
The formula is:
S∗ = arg max
S
P (S|O, W ) (1)
By the Bayes equation:
S∗ = arg max
S
P (S|W )P (O|S, W )
P (O|W )
= arg max
S
P (S|W )P (O|S, W )
(2)
Assume W ⊂ S, Eq. 2 is simplified as:
S∗ = arg max
S
P (S|W )P (O|S) (3)
The first part of Eq. 3 is independent of O and defined as lan-
guage model in recognition. The second part is the probability
of observation given the syllable sequence and defined as acoustic
part.
For the acoustic part, we can choose speaker dependent (SD)
or speaker independent model (SI). The speaker independent
model is trained from the ForSDAT-01 bilingual (Taiwanese and
Mandarin) speech corpus, which contains 200 speakers and 23
hours of speech, including both Taiwanese and Mandarin. All
the speech data are recorded in 16K, 16bits PCM format. We
use continuous Gaussian-mixture HMM models with feature vec-
tors of 52-dimension MFCC computed by using 20-ms window
frame and 10-ms frame shift. Context-dependent inside-syllabic
tri-phone models were built using a decision-tree state tying pro-
cedure.
In the task considered here, it is possible to use a set of SD
models for the experiment because the quantity of Sutra speech
recorded by the same master nun is quite large. The distribution of
Sutra speech is listed in Table 1. 160 utterances are randomly cho-
sen and reserved for testing while as another 31 utterances were
used for acoustic model development. Maximum Likelihood Lin-
ear Regression (MLLR) is then used to adapt speaker independent
models.
For the language part, the problem of multiple pronunciations
could be solved by using specially designed searching net. All the
searching nets were constructed according to multiple pronuncia-
tion lexicon described in next section. Finally, the pronunciation
variation rules would be incorporated in searching net to improve
the accuracy of transcription as discussed in the section 4.
3. Baseline Experiments in Sausage Network
For the Sutra transcription problem, in addition to each speech ut-
terance, its associated text in form of Chinese characters was also
Table 1: TBS (Taiwanese Buddhist Sutra) speech corpus.
Buddhist Corpus Category Utterance Time(min)
Train 2958 333.87
Development 31 2.56
Test 160 13.33
Total 3149 349.76
another input. Assume all syllables are independent with each
other. Therefore, the Eq. 3 can be rewritten as:
S∗ = arg max
S
P (s1|w1)...P (sn|wn)P (O|s1, s2, ..., sn) (4)
Here we encounter two problems: one is what syllable could be the
pronunciation for the Chinese character, the other is what probabil-
ity should be given for the pronunciation. By looking the pronun-
ciation up in pronunciation lexicon, the multiple pronunciations
of each Chinese character could be found. Based on the multiple
pronunciations of each Chinese character, a much smaller recogni-
tion net can be constructed. We will call such a net (with multiple
pronunciations) as a “sausage” net for its shape. Higher recog-
nition accuracy can be expected due to its smaller complexity in
the recognition net. Our task is then amount to how to construct
sausage nets and which acoustic model to choose.
3.1. The pronunciation Lexicons and the Recognition Net
There are three pronunciation lexicons available to us for the mul-
tiple pronunciations in Taiwanese of the Chinese characters.
The first is Formosa Lexicon, which contains about 123 thou-
sand words in Chinese/Taiwanese text with Mandarin/Taiwanese
pronunciations. It is a combination of two lexicons: Formosa
Mandarin-Taiwanese Bi-lingual lexicon and Gang’s Taiwanese
lexicon [4]. The former is derived from a Mandarin lexicon, and
thus many commonly used Taiwanese terms are missing due to
the fundamental difference between these two languages. The
Formosa lexicon as described above is a general-purpose lexicon.
It could be used for a wide range of applications, and tends to
have a higher number of multiple pronunciations. But some pro-
nunciations, which are actually pronounced by some experts, do
not appear in Formosa Lexicon due to pronunciation variations.
Sometimes, the Formosa Lexicon does not contain some ancient
characters of Sutra. Thus, the second lexicon, called Sutra Lexi-
con, is derived from the Sutra itself to study what variations exist
from Formosa lexicon to Sutra Lexicon. It is the pronunciations
collected from the published volumes of the Sutra. We expect
this lexicon to cover those words/characters/pronunciations which
were not contained in the Formosa Lexicon mentioned above. The
performance of Sutra Lexicon would be expected the best result
or the upper bound for the phonetic transcription considered here.
The third lexicon is not another source of pronunciation but just
the combination (union) of the previous two, and called Enhanced
Lexicon for convenience.
The three searching nets are the sausage nets generated from
each of the three pronunciation lexicons. Each searching net was
constructed by filling in each node of the net with the correspond-
ing multiple pronunciations of each Chinese character from the
pronunciation lexicon. The nets are denoted as General-Sau-Net,
Specific-Sau-Net, and Enhanced-Sau-Net for the general-purpose
In all the above equations, nij is the number of substitutions
of (base-form) triphone bi by the surface-form triphone sj that ap-
pear in a corpus, where N =
P
i
P
j nij , Ni =
P
j nij . While
p(bi, sj) represents the joint probability of (bi, sj), and p(bi),
p(sj) equal the marginal probability of bi and sj .
Table 2: Triphone-level confusion table, where nij represent the
number of variation from triphone bi to triphone sj , P is the num-
ber of surface-form and base-form, Ni =
P
j
nij , Mj =
P
i
nij
and N =
P
i
P
j
nij .
b-7 i-n . . . sj . . . b-o
b-7 237 0 . . . n1i . . . 30 267
i-N 0 84 . . . n2i . . . 0 1373
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bi ni1 ni2 . . . nij . . . niP Ni
241 1102 . . . Mj . . . 107 N
4.3. The Recognition Results
We adapt the Formosa (general-purpose) pronunciation lexicon ac-
cording to different sets of pronunciation variation rules. Then
the speech recognition task with sausage net and speaker adap-
tation was conducted as described in section 3, where the SER
achieved before the application of the pronunciation variation rules
was 13.94% as shown in Fig. 2, and would be looked upon as the
performance of the baseline setup in this section. In Fig. 3, we
could observe that it is truly helpful to decrease the SER by in-
creasing the coverage of searching net via the usage of PV rules.
The evidence is that the lowest error rate 12.04% is achieved by
utilizing the first 52 variation rules selected by Mutual-Information
(MI) method. Similar improvement would also be observed in the
best SER 12.88% and 12.51% achieved by Joint-Probability (JP)
method and Conditional-Probability (CP) method.
After applying more rules, the SER does increase. The situa-
tion will even become worse than the baseline experiments. There-
fore, it is an important way that how to gather the better rules into
the preceding rules and let the worse rules appear latter.
Although the JP-based method could make the error rate con-
verge more quickly than CP-based method, the performance also
degraded mostly quickly. This is because the score of CP-based
method would be normalized by his base-form count in contrast to
JP-based method. But sometimes few count of the PV-rule might
get the higher conditional probability due to few observations of
the base-form. Therefore, many insignificant and harmless PV-
rules of CP-based method were appeared in high rank and lead the
slowest convergence among these three methods.
In MI-based method, the formula could avoid the slow conver-
gence by using the Joint-Probability as weight since few count of
the base-form would get few numbers of variations. From observ-
ing the confusion table, the surface-form would have lower corre-
lation with these base-forms if many base-forms would transform
into the same surface-form. So we proposed the mutual informa-
tion between base-form and surface-form to calculate the correla-
tion of base-form and surface-form by the normalization of their
count. Consequently, the error rate of the MI rank converges most
quickly and the performance of MI method in error reduction is
also better than JP method and CP method respectively.
(52,12.04%)
(170,14%)
(17,12.88%)
(49,13.97%)
(162,14.06%)
(79,12.51%)
12
12.5
13
13.5
14
14.5
15
15.5
16
0 20 40 60 80 100 120 140 160 180 200
Rank
S
y
l
l
a
b
l
e
 
E
r
r
o
r
 
R
a
t
e
(
%
)
MI Rank
JP Rank  
CP Rank
Baseline
Figure 3: The recognition result (syllable error rate) v.s. the
number of ranked rules sorted according to different measures,
including mutual-information (MI), joint-probability (JP), and
conditional-probability (CP) as well as the Baseline criterion.
5. Conclusion
We have proposed a new approach to address the phonetic tran-
scription of Chinese text into Taiwanese pronunciation. The pro-
posed semi-automatic transcription of Chinese text to Taiwanese
pronunciation system has reached the 13.94% error rate as base-
line experiment. The further improvement by using pronunciation
variation rules had 13.63% error rate reduction.
6. References
[1] Soltau, H., “The IBM 2004 Conversational Telephony Sys-
tem for Rich Transcription”, In: Proc. ICASSP, Philadelphia,
USA, 2005, pp. I-205-I-208.
[2] Liang, M.-S., et al., “A Taiwanese Text-to-Speech System
with Applications to Language Learning”, In: Proc. ICALT,
Joensuu, Finland, 2004 pp. 91-95.
[3] Hain, T., “Implicit modelling of pronunciation variation in
automatic speech recognition”, Speech Communication 46,
2005, pp. 171-188.
[4] Lyu, Ren-yuan et al., “Toward Constructing A Multilingual
Speech Corpus for Taiwanese (Minnan), Hakka, and Man-
darin”, International Journal of Computational Linguistics &
Chinese Language Processing (IJCLCLP), Vol. 9, No. 2, Au-
gust 2004, pp. 1-12.
to allow the syllable recognizer to use the language-specific syllabic constraints 
during decoding process, and it is better than applying those constraints after syllable 
recognition. The most likely syllable sequence identified during recognition is optimal 
with respect to some combination of both the acoustics and linguistics. 
However, all these approaches were confronted with an apparent difficulty. That is, 
they use speech signal length in sentence level or 10-45 seconds as test speech and 
then the language is decided by which gets maximum number of unique phonetic unit 
is noted as the winner for the test utterance. In our case, code-switching speech, the 
length of language changing may be intra-sentence or word-based level, and we can 
not identify the language using above approach, because there may have at least two 
languages embedded in a test utterance. Therefore, we have to decide the language 
identity in a very short time of speech utterance.  
In this paper, we propose an alternative to deal with the code-switching speech LID 
task, which is SBDC (syllable-based duration classification). This framework could 
identify the language in syllable level which avoids the shortcoming of LID system in 
sentence or utterance-based utterances. Besides, to identify a language in each 
syllable that performs more precise language boundary in the code-switching speech. 
In this framework, we, firstly, extract acoustic and pitch features from code-switching 
utterance, secondly, the features are recognized as tonal syllable by our pervious 
recognizer [6]. Thirdly, by given the tonal syllable and its duration information, we 
use SBDC to identify the language for each common tonal syllable. Finally, the 
language smoother modifies the language identify in a statistical approach form 
training a code-switching speech corpus. 
The structure of the paper is as follows: A LVCSR-based LID system is introduced 
in Section 2. The phonetic characteristic between Mandarin and Taiwanese is 
introduced in Section 3. In Section 4 the SBDC-based LID system is described. 
Finally, the performed experiments and achieved results are presented. 
2   LVCSR-based LID 
It is known that LVCSR-based systems achieve high performance in language 
identification since they use knowledge from phoneme and phoneme sequence to 
word and word sequence. In [7], the LVCSR-based systems were shown to perform 
well in language identification. Unlike mono-lingual speech LID system [8], we 
implement a multi-lingual LVCSR-based system [9] as our code-switching speech 
LID baseline system. Fig 1 shows a block diagram of the system which includes two 
recognizers and each recognizer contains its won acoustic model and language model, 
such as AMT, LMT. 
In this paper, the multi-lingual LVCSR-based system requires significant tonal 
syllable level transcribed Mandarin and Taiwanese speech data for training the 
acoustic and language models. During the test phase, the recognizer is employed a 
unified approach to recognize each tonal syllable. The step of decoding translates 
each tonal syllable to its won language by phonetic knowledge. It is among the most 
computationally complex algorithms and achieves very high language identification 
accuracy. 
4   SBDC LID 
From the analysis of sec 3, we have an idea to discriminate Taiwanese from 
Mandarin by the duration discrepancy of the common tonal syllables. Thus, in this 
section, we develop a new approach to identify language for each tonal common 
syllable on code-switching speech. 
4.1 System Overview 
There are five components, including a feature extractor, a unified speech recognizer, 
a common tonal syllable extractor and a language smoother, in our code-switching 
LID system. Figure 3 illustrates the process, and the procedures are as the followings: 
1) The code-switching speech input utterance is pre-extracted into a sequence of 
MFCCs-based feature vectors )o,...,o,o(O TT 21= with the length (frame number) 
T.  
2) The unified speech recognizer [6] receives the features as the input and finds the 
best hypothesis tonal syllable )s,...,s,s(S NN 21= and its corresponding duration 
RD , where N is the distinct tonal syllables for all the languages and R is the real 
number which represents the duration of each hypothesis tonal syllable. 
According to the pronunciation dictionary, each of the hypothesis tonal syllable 
is further represented by the language code }c,t,m{L = , where m represents 
Mandarin, t represents Taiwanese and c means common language. The tonal 
syllables with the common language mean that they exist in both Mandarin and 
Taiwanese, and this kind of phenomenon is caused by the union phonetic 
representation of the unified speech recognizer. An example is shown in the 
figure 4.   
3) We only extract the speech segment with common language, 
cS , for 
discriminating between Mandarin and Taiwanese.  
4) The three parameters, 
TO , cS  and RD  are as the inputs to train the syllable-
based duration classifier (SBDC). The output is language specific tonal syllable, 
ctS  and cmS  for instance. This part will describe particularly in the section 4.2.   
5) In practice, the unit of code-switching language appears as a word whose unit 
exceeds duration. Under this assumption, the smoothing process is involved to 
eliminate the unreasonable language switching with a short interval by the 
language modeling after joining the parts of 
cmS , ctS  and tS , mS . The final 
output is 
m~S or t~S which is a tonal syllable with the language identity of 
Mandarin or Taiwanese. 
    (2)             )()|(),|(),,|(maxarg iicciRRciTiTi LPLSPSLDPDSLOP)(OL =  
The four probability expressions in (2) are organized in such a way that duration 
and common tonal syllable information are contained in separate terms. In modeling, 
these terms become known as  
1. )D,S,L|O(P RciT Common tonal syllable acoustic model.  
2. )S,L|D(P ciR Duration model.  
3. )L|S(P ic The phonetic language model. 
4. )( iLP The a priori language probability.  
Assuming that a priori language probability for each language on code-switching 
speech is equal, and phonetic language model for common hypothesis tonal syllable is 
also equal. The hypothesized language is determined by maximizing the log-
likelihood of language iL  with respect to speech TO  and is estimated as follows: 
 
  (3)                    )},|(log),,|({logmaxarg ciRRciTiTi SLDPDSLOP)(OL +=  
According to [3], the syllabic information is contained in two separate models: the 
syllabic acoustic model and the syllabic duration model, which are shown in Fig 5. In 
subsequent sections these models will simply be referred to as the acoustic model and 
the duration model. The acoustic model accounts for the different acoustic 
realizations of the syllabic elements that may occur across languages, whereas the 
duration model accounts for the probability distributions of the syllabic elements, and 
captures the differences that can occur in duration structures of different languages 
due to the boundary or segmented created by variations in the common tonal syllabic 
durations. This organization provides a useful structure for evaluating the relative 
contribution towards language identification that acoustic and duration information 
provide. 
 
Fig. 5. Illustration of syllable-based duration classifier component. 
4.3 Acoustic Model of SBDC 
The expression )L,D,S|O(P iRcT is called the acoustic model, which is used to 
capture information about the acoustic realizations of each of the common tonal 
training data. To ensure proper amounts of training data for each mixture of 
Gaussians, the number of Gaussian used to model each syllable in each language is 
determined by the amount of the training data.  
4.5 Language Smoother (LS) 
The goal of the language smoother is to modify the language identity to be more 
reasonable in language switching by an N-gram language model trained from a real 
code-switching corpus. An example is shown in Fig 7. 
 
Fig. 7. An example for merging language identification results by language smoother. 
 
5   Experiments and Results 
The goal of the experiment is to verify that SBDC-based system could have high 
accuracy syllable LID rate and to outperform a LVCSR-based system. In addition, we 
also evaluate the performance of our proposed approach is close to that on 
monolingual speech which maybe the upper bound performance on code-switching 
speech. 
5.1 Corpus and Experiment setup 
The speech corpus used in all the experiments were divided into three parts, namely, 
the training set, evaluating set  and the testing set. The training set consists of two 
mono-lingual Taiwanese and Mandarin speech data, which includes 100 speakers. 
Each speaker read about 700 phonetically abundant utterances in both languages. The 
evaluating set is to train the back-off bi-gram code-switching language model, which 
estimates the probability of language translation, and adapts the threshold of syllable 
duration quantizer. For testing data set, another 12 speakers were asked to record 
3000 Mandarin-Taiwanese code-switching utterances. Among these utterances, at 
least one Taiwanese word is embedded into a Mandarin carrier sentence. The length 
of each word is various from one to eight syllables. The statistics of the corpus used 
here are listed in Table 1. 
The acoustic features used in SBDC are the same with in [5], they are: mel-
frequency cepstral coefficients (MFCC) which includes 12 cepstral coefficients, 
Table 2. The LID accuracy rate for different approaches 
 LID accuracy rate (%) 
monolingual speech 88.05 
code-switching speech 10K 20K 
LVCSR-based 82.14 81.93 
SBDC 86.08 84.78 
SBDC+LS 87.53 85.91 
6   Conclusion 
In this paper, we used three clues: recognized common tonal syllable tonal syllable, 
the corresponding duration and speech signal, building a SBDC LID system to 
identify specific language from code-switching speech. The system’s architecture is 
factorized as HMM-based quantized duration acoustic model in tonal syllable, GMM-
based duration model and language smoother. The experimental results show a 
promising performance on LID accuracy rate to compare with the LVCSR-based 
system and the performance also approaches that in monolingual speech by using 
PSR method. 
References 
1. Zissman, M. A. "Comparison of four Applications to Automatic Language Identification 
of Telephone Speech," IEEE Trans. on Speech and Audio Proc., Vol. 4, No. 1, pp. 31-44, 
1996 
2. T. Nagarajan and Hema A. Murthy, "Language Identification Using Parallel Syllable-Like 
Unit Recognition," ICASSP, 2004 
3. Hazen, T. J., & Zue, V. W., "Segment-Based Automatic Language Identification," Journal 
of Acoustic Society of America, April 1997. 
4. Rongqing Huang, John H.L. Hansen, "DIALECT/ACCENT CLASSIFICATION VIA 
BOOSTED WORD MODELING," ICASSP 2005 
5. Pedro A. Torres-Carrasquillo, Douglas A. Reynolds, J. R. Deller, Jr., "Language 
Identification Using Gaussian Mixture Model Tokenization," Proc. ICASSP 2002, pp. I-
757-760. 
6. Dau-Cheng Lyu, Ren-Yuan Lyu, Yuang-chin Chiang and Chun-Nan Hsu, "Speech 
Recognition on Code-Switching Among the Chinese Dialects," ICASSP, 2006 
7. T. Schultz et al., "LVCSR-based Language Identification," Proc. ICASSP, pp. 781-784, 
Altlanta 1996. 
8. J.L.Hieronymus, S.Kadambe, "Robust Spoken Language Identification using Large 
Vocabulary Speech Recognition", ICASSP, vol.2, pp.1111-1114, Munich, Germany, Apr., 
1997 
9. Santhosh C. Kumar, VP Mohandas and Haizhou Li, "Multilingual Speech Recognition: A 
Unified Approach", InterSpeech 2005 - Eurospeech - 9th European Conference on Speech 
Communication and Technology, September 4-8, 2005, Lisboa, Portugal 
      The structure of the paper is as follows: A traditional multi-pass 
scheme includes the LID systems on single and code-switching 
utterances are introduced in Section 2. The new proposed one-pass 
speech recognizer is described in Section 3. In Section 4 the 
performed experiments and achieved results are presented. Finally, 
we draw some conclusions in Section 5. 
 
2. MUTI-PASS SPEECH RECOGNITION FOR 
CODESWITCHING UTTERANCES 
 
For the speech recognition on code-switching utterances, the 
traditional multi-pass scheme could be shown in figure2. Before 
doing the language-dependent speech recognition, the code-
switching utterances have to be processed via the language boundary 
detection and language identification (LID). Therefore, in the 
following, two approaches of automatic language identification 
frameworks on single language and code-switching utterances are 
introduced. 
 
 
Figure 2. The diagram of ASR for code-switching speech in a multi-pass 
approach. 
 
2.1. LID on single language utterances 
For the LID system development, the parallel syllable recognition 
(PSR) was adopted, which is similar to the method of parallel phone 
recognition(PPR), and this approach is widely used in the automatic 
LID researches. [6] Here, the reason to use syllable as the 
recognized result instead of phone is because both Taiwanese and 
Mandarin are syllabic languages. Another approach, which is called 
parallel phone recognition followed by language modeling (parallel 
PRLM), used language-dependent acoustic phone models to convert 
speech utterances into sequences of phone symbols with language 
decoding followed. After that, these acoustic and language scores 
are combined into language-specific scores for making an LID 
decision. Compared with parallel PRLM, PSR uses integrated 
acoustic models to allow the syllable recognizer to use the language-
specific syllabic constraints during decoding process, and it is better 
than applying those constraints after syllable recognition. The most 
likely syllable sequence identified during recognition is optimal with 
respect to some combination of both the acoustics and linguistics. 
      To further improve the performance, other information, such as 
articulatory, acoustic and prosodic features have also been integrated 
into an LID system. Mandarin and Taiwanese are also tonal 
languages, using rhythmic and intonation features can be looked 
upon as efficient cues for discriminating languages. 
      In the PSR approach, the utterances are identified to language 
names by the higher likelihood scores emanating from language-
dependent syllable recognizer. By allowing the syllable recognizer 
to use the Viterbi decoding rather than applying those constraints 
after syllable recognition, the most likely syllable sequence 
identified during recognition is optimal with respect to some 
combination of both the acoustics and syllables.. 
 
2.2. LID on mixed-language utterances 
Identifying mixed-languages in an utterance challenges the 
present LID system as described in the above (section 2.1). 
Segmenting such an utterance into two segments of different 
languages is crucial to the development of a LID system. For the 
mixed-language LID system proposed in this paper, we used the 
confidence scores to decide which region in one utterance belongs to 
its language by the recognized tonal syllables. This approach is 
similar to that mentioned in [4], which is used to detect the 
boundaries from English and Cantonese code-switching utterances 
by using bi-phone probabilities, and measuring the confidence that 
the recognized phones are in Cantonese. In our proposed approach, 
the confidence score is measured by likelihood of tonal syllables 
with the two languages during the same position of the recognized 
syllable sequences. For example, if a tonal syllable belongs to 
Taiwanese among a sequence of the results after decoding, then the 
boundary of the tonal syllable is fixed and the language inside the 
region of the tonal syllable is also identified. In order to train the 
language-specific acoustic models, each of the tonal syllables are 
tagged with the language information, and the acoustic models were 
trained according to their language specification. 
Besides, two main innovations are further used in this task for 
capturing the characteristic of the mixed-languages. Firstly, we add 
the prosody information into the feature vectors because many 
literatures have demonstrate the fact that prosodic features are very 
helpful to distinguish tonal languages [11]. The prosody can be 
considered in two phases, representing the phrase accentuation and 
the local accentuation, as in Fujisaki's work. [12]. Secondly, we used 
bi-syllable likelihood of both languages as confidence measurement 
for syllable-based language identification. These two innovations 
are due to the fact that both the languages (Mandarin and Taiwanese) 
considered here are tonal and syllabic languages. 
 
3. ONE-PASS RECOGNIZER 
 
It is known that, all of the spoken varieties of the Chinese languages 
share a common formal written language, if we ignore the difference 
between traditional and simplified orthographies adopted in Taiwan 
and mainland China, respectively. In Taiwan, there were 85% 
commonly shared lexicon items between Mandarin and Taiwanese, 
although a number of special characters which are unique to 
Taiwanese are sometimes used in informal writing. [3] Because of 
these special linguistic characteristics, we have proposed a Chinese 
character-based one-pass recognition scheme, which has been 
proven successful in dealing with multiple dialects of the Chinese 
language using a unified framework. [13] 
Unlike some conventional approaches, which divide the 
recognition task into language boundary detection and language 
identification, our proposed approach adopts a one-stage searching 
strategy, as shown in Figure 3. This approach is not only simpler 
than traditional multi-pass one, but also avoids the loss of hard 
decision in each stage, such as language boundary detection or LID, 
and it becomes easy to train and use integrated 
acoustic/pronunciation models. 
 
 
Figure 3. The diagram of ASR for code-switching speech in a one-pass 
approach 
 
In this framework, Chinese character-based decoding 
can be implemented by searching in a three-layer network 
composed of an acoustic model layer, a lexical layer, and a 
grammar layer. There are at least 2 critical differences 
between our framework and the conventional one. 1) In the 
lexicon layer, character-to-pronunciation mapping can easily 
incorporate multiple pronunciations in multiple languages, 
including Japanese, Korean, and even Vietnamese, which 
also use Chinese characters. 2) In the grammar layer, 
4.2. Experiment setup 
The acoustic features used in ASR subsystems are mel-frequency 
cepstral coefficients (MFCC) which includes 12 cepstral coefficients, 
normalized energy and prosody information. The first and second 
derivatives of parameters are also included. [16] Both the language-
dependent and bi-lingual HMM-based acoustic models of the 
syllable recognizer are trained using the corpora described above. 
The syllable accuracies are 63.67%, 61.69%, and 60.81% for 
Mandarin-only HMM, Taiwanese-only HMM and bi-lingual HMM, 
respectively.  
    In the pronunciation modeling, the average number of 
pronunciations for one Chinese character for each pronunciation 
lexicon was 1.2, 1.8 and 3.0 for Mandarin, Taiwanese and bi-lingual, 
respectively. Furthermore, we use the tree-structured searching nets 
with 2 kinds of vocabulary sizes, i.e., 10 thousands words and 20 
thousands words. All the words in the testing corpus are included in 
the searching net and thus the out-of-vocabulary rate is zero. 
Additionally, the outputs of the recognizer were Chinese characters; 
therefore, we evaluated the performance of the ASR in terms of the 
Chinese character error rate (CER). 
 
4.3. Results 
The experimental results for L2 and L3, which are outcomes of the 
LID subsystems in figure 4, are shown in <table 3>, where the 
average detection rate for them are 88.05% and 76.68%, 
respectively.  
 
 L2 L3 
For Mandarin 89.22% 79.12% 
For Taiwanese 86.87% 74.23% 
Average 88.05% 76.68% 
Table3. The experimental results of LID subsystems in figure 4  
 
The experimental results for R1, R2, R3 and R4, which are the final 
recognition outcomes of the whole system in figure 4 are shown in 
<table 4>, where two kinds of vocabulary-sizes were tested in the 
ASR subsystems, namely, 10 thousands words and 20 thousands 
words. One can see that the error rates for the 20K vocabulary task 
are 22.59%, 28.61%, 31.76% and 20.02% for R1, R2, R3 and R4, 
respectively. 
    It is interesting that R4 outperforms R1, the latter is based on 
manual language segmentation and manual LID but the former used 
the completely automatic approach. The critical point is that R4 uses 
the global information and does the soft decision on LBD and LID 
during the decoding process. On the other hands, R1 has only local 
information and does the hard decision in each step before ASR, 
though the decision is made by human.  In other words, even human 
could not do a correct decision under the fragment information. 
 
 10K 20K 
R1 14.22% 22.59% 
R2 20.7% 28.61% 
R3 23.20% 31.76% 
R4 13.31% 20.02% 
Table 4. The experimental results for the final recognition outcomes 
of the whole system in figure 4 
 
5. CONCLUSION 
 
In this paper, we compared two approaches to recognize a 
Taiwanese-Mandarin code-switching utterance as a Chinese 
character sequence. In the multi-pass ASR, three stages of 
processing are integrated, including language boundary detection, 
language identification and language-dependent speech recognition. 
We evaluated the performance of automatic approach in each stage, 
and also demonstrated the manual results as the golden benchmark. 
In each stage of the processing, the intermediate results are close to 
each other. On the other hand, the one-pass ASR proposed in this 
paper can deal with the code-switching utterance satisfactorily 
without extra language boundary detection and language 
identification. This paper presents an alternative to recognize one 
utterance in mixed-languages; the experimental results of 
performance on the Chinese character error rate demonstrate that it 
is a promising approach. 
 
6. REFERENCES 
 
[1] H. Y. Su "Code-switching between Mandarin and Taiwanese in 
Three Telephone Conversation: The Negotiation of 
Interpersonal Relationships among Bilingual Speakers in 
Taiwan," In Proc. of the Symposium about Language and 
Society, April, 2001 
[2] Joyce Y. C. Chan, P. C. Ching and T. Lee, "Development of a 
Cantonese-English Code-mixing Speech Corpus,"  In Proc. of  
Eurospeech, 2005. 
[3] C.M. Chen "Two Types of Code-Switching in Taiwan ," 15th 
Sociolinguistics Symposium, April 2004 
[4] Joyce Y. C. Chan, P.C. Ching, Tan Lee and Helen M. Meng, 
"Detection of Language Boundary in Code-switching utterances 
by Bi-phone Probabilities," ISCSLP, 2004 
[5] Zissman, M. A. "Comparison of four Applications to Automatic 
Language Identification of Telephone Speech," IEEE Trans. on 
Speech and Audio Proc., Vol. 4, No. 1, pp. 31-44, 1996. 
[6] Y. K. Muthusamy, E. Barnard and R. A. Cole "Reviewing 
Automatic Language Identification," IEEE Signal Processing 
Magazine, Vol. 11, Issue 4, 1994, pp. 33-41.  
[7] C. J. Shia, Y. H. Chiu, J. H. Hsieh and C. H. Wu, "Language 
Boundary Detection and Identification of Mixed-Language 
Speech Based on MAP Estimation,"  ICASSP, 2004. 
[8] T. Nagarajan and Hema A. Murthy, "Language Identification 
Using Parallel Syllable-Like Unit Recognition," ICASSP, 2004. 
[9] J. L. Rouas, J. Farinas and F. Pellegrino, "Automatic Modeling 
of Rhythm and Intonation for Language Identification," in Proc. 
of  15th ICPhS, Barcelona, Spain, 2003, pp. 567-570. 
[10]F. Cummins, "Speech Rhythm and Rhythmic Taxonomy," in 
Speech Prosody, Aix-en-Provence, France, 2002, pp. 121-126. 
[11]J. L. Rouas, J. Farinas, F. Pellegrino and R. A. Obrecht, 
"Modeling Prosody for Language Identification on Read and 
Spontaneous Speech," In Proc. of  ICASSP, 2003. 
[12]H. Fujisaki, "Prosody, Information and Modeling with Emphasis 
on Tonal Features of Speech," in workshop on Spoken Language 
Processing, Mumbai, India, Joundry 2003. 
 [13]R.Y. Lyu,  et al., "A Unified Framework for Large Vocabulary 
Speech Recognition of Mutually Unintelligible Chinese 
"Regionalects"," In Proc. of ICSLP, Jeju Island, Korea, 2004 
[14]International Phonetic Association, Handbook of the 
International Phonetic Alphabet., Cambridge University Press, 
New York, NY, 1999 
[15]D.C. Lyu, R.Y. Lyu, Y.C. Chiang and C.N. Hsu, "Modeling 
Pronunciation Cariation for Bi-Lingual Mandarin/Taiwanese 
SPEECH RECOGNITION," International Journal of 
Computational Linguistics and Chinese Language Processing, 
Vol. 10. no. 3. 2005, pp. 363-380 
[16]D.C. Lyu, et al., "Large Vocabulary Taiwanese (Min-nan) 
Speech Recognition Using Tone Features and Statistical 
Pronunciation Modeling," In Proc. of Eurospeech, 2003 
analysis are presented in section 6; conclusion is made 
in the final section.  
 
2. BI-LINGUAL RESOURCES 
  
    In order to connect the video news with the 
corresponding document news, three types of 
resources were collected for this work: bi-lingual TV 
news video; a bi-lingual speech corpus for automatic 
speech recognition and a corpus of Chinese news 
documents automatically mined from the Internet.  
 
2.1 TV News Data  
 
    First of all, the video data of bi-lingual TV news 
are collected from the Formosa Television News 
(FTVN)[5] and the Public Television Service (PTVS) 
were collected [19]. In general, a section of the news 
in Taiwan has half hours, and contains four main parts 
which are: anchor’s report, interviewers, weather 
report and advertisements/ music, which is shown in 
Figure 2. The distributions of these five parts are 20%, 
38%, 7%, 25% and 10% for average. In other words, 
in the half hour’s TV news is formed on a series of 
stories, and a story is composed of the first two 
parts –anchor’s report and interviewers. The 
interviewer is the more detailed content or extension 
of the anchor’s report. For each story, we can almost 
find the corresponding document in the websites. In 
order to recognize the anchor’s speech to text using 
automatic speech recognition, the diction of each 
anchor’s parts of a story in one section of TV news is 
necessary. The automatic diction technique  
 
 
Figure 2. The progress of TV news in half hour, 
which includes anchor’s speech, interviewers, 
weather report and advertisements/music. 
 
    The data is described in the following. In one 
section of FTVN, two anchors with 40 stories were 
extracted and the whole duration is about 13 minutes, 
and in the PTVS, we have one anchor and 22 stories 
with 6 minute. The video data is recorded as MEPG1 
format. However, for the automatic speech recognition, 
the video data is extracted the audio signal and stored 
as 16k Hz 16 bits.  In addition, in order to have a 
good performance in speech recognition, the anchor’s 
speech is selected. The reason is anchor’s speech is 
much clear and with little background music or noise. 
Furthermore, the automatic detecting anchor’s speech 
in one section of T V news is necessary, and the detail 
is addressed in Section 3. Another, for evaluating the 
performance of the automatic audio segment, we 
manually segment the video data, and transcribe the 
anchor’s speech to syllables by a language expert. 
Totally, there are about 4733 syllables labeled in the 
transcripts, and those transcriptions were prepared for 
the standard answer for the speech recognition in 
evaluation and index error analysis.  
 
2.2 Bi-lingual Corpus  
 
    In order to generate a bi-lingual speech 
recognizer, the language of Mandarin and Taiwanese 
read speech database is collected. The corpus is totally 
contained 23 hours with 120 speakers, among which, 
100 speakers for training the acoustic model and 
others for evaluating the performance of speech 
recognition. In the training set, every speaker records 
both languages, and in the evaluation set for each 
speaker, only single language is required. All of the 
data is recorded with 16k/16 bits microphone in an 
office environment. The all statistics of the corpus are 
listed in Table.1.  
 
Table 1. The statistics of the bi-lingual speech 
corpus for acoustic model training and baseline 
testing. M: Mandarin, T: Taiwanese 
                Training Set  Evaluation Set
Language  M T  M  T 
No. of Speakers 100 100 10 10
No. of Utterances 43078 46086 1000 1000
No. of Hours  11.3 11.2 0.28 0.28
 
2.3 Text Documents Data 
  
    The written documents consisting of 850k 
Chinese characters were obtained from FTVN and 
PTVS news website. Most of the data are 
automatically mined twice a day during one month in 
using web agent [2]. The documents is correspond to 
the TV news. They totally contain 3079 stories, i.e. 
about 100 stories for one day in average.  
    There are two main purposes for collecting these 
documents, training language model of the speech 
recognition and for aligning bi-lingual texts. The 
procedures are addressed in the following: First the 
texts were segmented into sentences and then 
normalized in order to better approximate a spoken 
form by the texts analysis module [21]. Second, each 
sentence have to segment as word or phrase level for 
transcribing that written in Chinese characters to 
phonetic representation in both languages. However, 
there is no natural boundary between 2 successive 
words in Chinese characters, the word segmentation is 
used the method of maximum sequence match to find 
the best segmentation. Third, in the cause to align the 
texts to the video by continuous speech recognition, 
we used the bi-lingual pronunciation lexicon for 
automatic translated the literature form of Chinese 
characters to spoken form of Taiwanese syllables. All 
the statistics of the TV news and text document data 
are listed in Table.2.  
 
ix. Spectrum Entropy, 
∑−
= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛⋅=
1
0 ][
1log][
N
k K
Kt kp
kpE
t
t
 
x. Spectrum Flux,  
   ,1
1∑ −+=
=
=
Tt
t
t fT
F
τ
τ
τ
 
number positive smallarbitrary an  is  and 
 ,)][log()][log(1 where
1
0
1
δ
δδ∑−
=
− +−+=
N
k
ttt kXkXN
f  
 
    There are two examples in Figure 4 and Figure 5 
to represent the concept that the features mentioned 
above are obvious to segment the boundary of speech, 
music or environment noise.  
 
Figure 4. The distribution of LSTER (Low Short 
Time Energy Ratio) on (a): speech signal and (b): 
music signal. 
 
 
Figure 5. The spectrum flux value on three types of 
signals. The durations of sample index during 
0~200; 200~35; 350~450 are signal of the speech; 
music and environment noise, respectively. 
 
    Therefore, we keep the speech part, and those 
parts are cut into many fragments by the above 
features. In order to detect the anchor’s speech from 
those fragments, the speaker-based hidden Morkov 
model (HMM) is also trained using MFCC features. 
The duration of one frame is 5 seconds, and shifts one 
second. Then, some fragments are identified as anchor 
reports. In general, an anchor’s section is continued 10 
seconds at least. We merge or discard the fragments 
that are identified as anchor’s speech which has more 
or less 10 seconds duration.  
     To compare the results of two different 
segmentation approaches, we look upon one as 
reference boundaries and the other as testing 
boundaries as shown in Figure 6. For each boundary 
in the reference boundaries, the nearest one in the 
testing boundaries are picked out and marked as the 
matching ones. The matching difference (in sec.) is 
also recorded simultaneously. Those who in the testing 
boundaries could not be matched to any one in the 
reference boundaries were marked as the insertion 
ones. Those who in the reference boundaries could not 
find an appropriate matching one in the testing 
boundaries were marked as the deletion ones.  It can 
be easily shown that 
Nmatching + Ninsertion = NTesting  ,
here Nmatching ,Ninsertion , and Ndeletion are the 
n, 
Nmatching + Ndeletion = NReference  , 
 
w
number of the matching, insertio and deletion 
boundaries respectively between the reference and 
testing set, while NReference and NTesting are the number 
of the boundaries in the reference set and testing set, 
respectively. 
 
 
Figure 6. The performance measurement between 
The insertion rate (I) , deletion rate (D), were 
 d
g*100%;  
   In order to evaluate the performance of the 
able 3 Segmentation consistency between manual and 
0.1 sec. 0.2 sec. 0.3 sec. 
2 segmentation approaches by matching, insertion 
and deletion rate 
 
 
thus efined as 
I=Ninsertion/NTestin
D=Ndeletion/NReference*100%; 
 
 
automatic audio TV news segmentation on our method; 
we used manual segmentation as the reference. The 
result is shown in Table 3 and Table 4. 
 
T
automatic where I, D are the boundary insertion and 
deletion rates 
 
I (%)  5.40 11.1 9.67
D (%) 2.77 11.1 22.22
 
able 4. The Classification results for four types, 
 D 
T
where A, B, C, and D represent anchors report, 
interviewers, weather report and advertisements/ 
music respectively. The first row is reference part, 
and the first column is testing part. 
 A B C
A % % % %100 0 0 0
B 5% 90% 5% 0%
C 0% 0% 100% 0%
D 0% 2 80% 0% 0%
 
   In Table 3 is addressed the insertion and deletion  
 
confidence measure which is the occurrence 
possibility, and eliminate the relatively small counts. 
The sum of all the occurred possibility of each 
character is then normalized to unity for fair 
competition in the Viterbi search.  
    In the searching net, we use a large-vocabulary 
. BI-LINGUAL TEXT ALIGNMENT  
   Bi-lingual text alignment is one of the research 
een the 
ngual 
 segmented word, there may be existed 
namic 
able 6. The number of pronunciation of Formosa 
CLP 
Ta e
DLP 
Ta e Total 
tree structured word net, because the perplexity can be 
reduced in the tree-structured searching net against the 
linear searching net. This part is trained using the web 
news documents. 
 
5
 
 
issues in machine learning for past few decades [9]. It 
also pointed out that translational equivalence is a 
relation that can be learned from data. The best 
translation models are those whose parameters 
correspond best with the sources of variance in the 
data. Therefore, we used the bi-lingual pronunciation 
lexicon as the data for trained a probabilistic 
translation model to translate the Chinese character 
documents to the Taiwanese tonal syllables.  
    In order to get more performance betw
spoken style text and document, the text analysis for 
text-to-speech technique most is included. The 
analysis module contains two main stages: language 
translation and digital sequence representation.  
    In language translation, we used the bi-li
pronunciation lexicons, ForLex (Formosa Lexiocn) 
and Gang's Taiwanese lexicon, with about 70k words 
as the knowledge source and then used a word 
segmentation algorithm based on the sequentially 
maximal-length matching in the lexicon. The ForLex 
is derived from Mandarin lexicon, and thus many 
commonly used Taiwanese terms are missing. The 
Gang's Taiwanese lexicon contains Taiwanese 
expressions from a sampling of radio talk show. A 
Chinese character in Taiwanese commonly can have a 
classic (Chinese) literature pronunciation (CLP) and a 
daily life pronunciation (DLP) [24]. Many of the 
characters have even more pronunciations. Those 
lexicons can provide optionally pronunciations for 
each segmented word. Some statistics of the two 
sub-lexicons are summarized in Table 6 and  Table 7. 
    In digital sequence representation, consider the 
following phrase as an example. “1221 公斤” is 
pronounced as “1( zit7) 千(cing1) 2( nng2) 百(bah4) 
2( ri2) 拾(zap3) 1( it7) 公斤 (gong1-gin1)”, where 
the first “1” and the last “1” are pronounced 
differently as “zit” (oral) and “it” (classic) respectively, 
and similarly, “2”’s are pronounced as “nng2” (oral) 
and “ri2” (classic) respectively. The manner of 
pronunciation depends on the position of the digit in a 
sequence, which can be summarized in rules. On the 
other hands, if a digit sequence does not represent a 
quantity, it is pronounced digit by digit as the classic 
pronunciation. For examples, “西元 1221 年” is 
pronounced as “西(se1) 元(quan1) 1(it7) 2(ri2) 2(ri2) 
1(it7) 年(ni2) ”, where all “1” and “2” are pronounced 
as their classic pronunciations “it” and “ri”, 
respectively 
    For each
not only one pronunciation. To deal with the 
multi-pronunciation problem, a network with word 
frequencies as node information and word transitional 
frequencies as arc information has been constructed 
for each sentence and a Viterbi search for the best 
pronunciation is then conducted. In the digital 
sequence analysis, each of almost all Taiwanese 
single-syllabic words has 2 distinct manners of 
pronunciation: one for classic literature such as the 
Chinese traditional poems, and the other for oral 
expression in daily lives. However, for digits, these 2 
manners of pronunciation exist in daily lives.  
    After the translation, we used the dy
time-warping approach to align the ASR results of the 
anchors speech into the FTVN and PTVS news 
documents. A story is one-to-one alignment and found 
the best document if the document and the result had 
the most hit points in syllable level. 
 
T
bi-lingual Lexicons, including classic literature 
pronunciation (CLP) and daily life pronunciation 
(DLP). 
 
iwanes iwanes
1-Syllable 2319 8040 10359
2-Syllable 2 41337 9222 70559
3-Syllable 7163 11367 18530
4-Syllable 55 15525 15580
5-Syllable 1 711 712
6-Syllable 0 497 497
7-Syllable 0 478 478
8-Syllable 0 195 195
9-Syllable 0 3 3
10-Syllable 0 2 20 0
Total 3087 860 11695 60 35
 
able 7. The distribution of Gang's Taiwanese 
 Taiwanese 
T
lexicon. 
1-Syl ble la 8153 
2- Syllable 46587 
3- Syllable 13241 
4- Syllable 2106 
5- Syllable 175 
Total 70262 
         
RIMENT RESULTS  
.1 Experimental Setup and Baseline Syllable 
   The feature extraction, acoustic modeling and 
6. EXPE
 
6
Recognition  
 
 
pronunciation modeling were used our pervious setup 
 
automatic collecting large and representative datasets 
in order to automatically evaluate and build the system 
using web agent techniques. Finally in order to 
evaluate our system extensive user experiments are 
required. User experiments for evaluating our service 
are planned for the future. 
 
8. REFERENCES 
] B. Logan, Pedro Moreno, Jean-Manuel Van 
[2] "Automatic 
[3] iang, R.Y. Lyu, C.N. 
[4] euse of 
[5] a Television News, 
[6] n, "An Investigation of 
[7] Coetzer, "Automatic 
[8] ts in Syllable-based 
a
[9] r 
[10] g, "Joint scene 
[11]  a Speech 
e
[12] , 
 
[1
Thong, Ed Whittaker, "An Example Study of An 
Audio Indexing System for The Web," 2000, 
Proc. of the International Conference on Spoken 
Language Processing, 2000, Beijing. 
C.H. Chang, C.N. Hsu and J.J. Lui, 
information extraction from semi-structured Web 
pages by Pattern Discovery" Decision Support 
Systems, 35(1):129-147, Special Issue on Web 
Retrieval and Mining, 2003. 
D.C. Lyu, B.H. Yang, M.S. L
Hsu, "Speaker Independent Acoustic Modeling 
for Large Vocabulary Bi-lingual 
Taiwanese/Mandarin Continuous Speech 
Recognition," Proc. of the 9th Australian 
International Conference on Speech Science & 
Technology, 2002, Melbourne, Australia. 
E. Jeong and C.N. Hsu, "Integration and R
Heterogeneous XML DTDs for Information 
Agents," Decision Support Systems, 2001, IAT, 
Japan. 
Formos
http://www.ftvn.com.tw/ 
G. Lu, and T. Hankinso
AutomaticAudio Classification and 
Segmentation," Proc, of the International 
Conference on Spoken Language Processing, 
2000, Beijing, pp.776-781. 
G. J. Prinsloo and M. W. 
syllabification and phoneme class labelling with a 
phonologically based hidden Markov model and 
adaptive acoustical features," International 
Journal of Computer Speech and Language, 4(3), 
July 1990, Pages 247-262. 
H.M. Wang, "Experimen
Retrieval of Broadcast News Speech in Mandarin 
Chinese," International Journal of Speech 
Communic tion, 32(1-2), pp. 49-60, Sept. 2000. 
I. Dan Melamed, "Empirical Methods fo
Exploiting Parallel Texts" 2001 
 J. Huang, Z. Liu, Y. Wan
classification and segmentation based on hidden 
Markov model," IEEE Transactions on 
Multimedia, 7(3), 2005, pp.538 – 550. 
 J.M. Van Thong, et al, "SpeechBot:
Recognition based Audio Indexing System for the 
Web," Proc. International Conference on 
Comput r-Assisted Information Retrieval, 2000. 
 J. Garfolo, E. Vorhees, C. Auzanne, V. Stanford
and B. Lund. "Spoken document retrieval track 
overview and results," Proc. of the 7th Text 
Retrieval Conference, 1998. 
[13] L. Lu, H. Jiang and H.J. Zhang "A robust audio 
classification and segmentation method," Proc. of 
Multimedia Conference, 2001, Ottawa, Canada. 
[14] L. Lu, H.J. Zhang, and H. Jiang, "Content 
analysis for audio classification and 
segmentation," IEEE Trans. on Speech and Audio 
Processing, 10, October 2002, pp.504-516. 
[15] L. Chaisorn and T.S. Chua, "The Segmentation 
and Classification of Story Boundaries in News 
Video," Proc. of 6th IFIP working conference on 
Visual Database Systems VDB6, 2002, Australia.  
[16] M. Judith, Kessens, C. Cucchiarini, and H. Strik, 
"A data-driven method for modeling 
pronunciation variation," International Journal of 
Speech Communication, 40, 2002, pp. 517-534. 
[17] P. Mermelstein, "Automatic segmentation of 
speech into syllabic units," International Journal 
of Acoustic Society American, 58, 1975, pp. 
880-883. 
[18] P.Y. Liang, J. L. Shen, L. S. Lee, "Decision Tree 
Clustering for Acoustic Modeling in 
Speaker-Independent Mandarin Telephone 
Speech Recognition," Proc. of the International 
Symposium on Chinese Spoken Language 
Processing, 1998, Singapore, pp. 207-211. 
[19] Public Television Service, http://www.pts.org.tw/ 
[20] R.Y. Lyu, et al, "A Unified Framework for Large 
Vocabulary Speech Recognition of Mutually 
Unintelligible Chinese Regionalects," Proc, of the 
International Conference on Spoken Language 
Processing, 2004, Jeju Island, Korea. 
[21] R.Y. Lyu, et al, "A Taiwanese (Min-nan) 
Text-to-Speech (TTS) System Based on 
Automatically Generated Synthetic Units," Proc, 
of the International Conference on Spoken 
Language Processing, 2000, Beijing. 
[22] V. Kamakshi Prasad, et al., "Automatic 
segmentation of continuous speech using 
minimum phase group delay functions," 
International Journal of Speech Communication, 
42(3-4), 2004, pp. 429-446. 
[23] Y. Liu, and P. Fung, "Partial change accent 
models for accented Mandarin speech 
recognition," Proc. of the IEEE Workshop on 
ASRU, 2003, St. Thomas, U.S., Virgin Islands. 
[24] Y.C. Chiang, M.S. Liang, H.Y. Lin, and R.Y. Lyu, 
"The Multiple Pronunciations in Taiwanese and 
the Automatic Transcription of Buddhist Sutra 
with Augmented Read Speech," Proc. of the 
European Conference on Speech Communication 
and Technology, 2005, Lisbon, Portugal.  
[25] Z. Liu, Y. Wang, T. Chen, "Audio feature 
extraction and analysis for scene segmentation 
and classification." International Journal of VLSI 
Signal Processing Systems, 1998, 20, pp. 61-79. 
 
 experimental results and analysis are presented in section 5; 
conclusion and future work are made in the final section. 
 
2. Multimedia Resources 
 
   Three types of resources were collected for this work: a 
bi-lingual speech corpus containing 11.5 hours; the manual 
segmented Taiwanese TV news data, and a corpus of Chinese 
news documents automatically obtained from the Internet.  
 
2.1 Taiwanese Speech Corpus 
 
   A Taiwanese read speech database produced by 55 male and 
55 female speakers over 16k 16 bits microphone was provided 
in Multi-media Signal Processing Laboratory at Chang Gung 
University in Taiwan. The statistics of the corpus considered 
here are listed in <table.1>. The duration of training data is 11.2 
hours, and is uttered by 100 speakers for a total of 46086 
utterances. For testing, we chose another 10 speakers, with 17 
minutes of speech of 1000 words out of a vocabulary of 40 
thousands words. 
 
 Training 
data 
Testing data
No. of Speakers 100 10
No. of Utterances 46086 1000
No. of Hours 11.2 0.28
No. of Syllable per Utterance 1.9 2.6
Table 1. The statistics of the Taiwanese speech corpus for 
acoustic model training and baseline testing. 
 
2.2 TV News Data 
 
   The video data of TV news from Formosa Television News 
(FTVN) were collected during a period of one week in 
November 2002 for the indexing purposes. The original video 
data record 1 hour morning news everyday, and the total 
amount of the data was 5 hours. A section of TV news generally 
contained four main parts which are: anchors, reporters, 
interviewees and weather report. An anchor and the following 
reporter parts are a pair and this pair is also defined as a story. A 
story almost can find the corresponding document in the FTVN 
website. Our research goal is automatic finding the linking of 
the story and the related document. Therefore, we manually 
segmented the anchor parts at this stage. Totally two anchors 
with 40 stories were segmented and the whole duration is about 
13 minutes. Then, the video stories were extracted the audio 
signal(right channel) is stored as 16kHz with a resolution of 16 
bits for speech recognition. We also manually transcribed this 
signal to Taiwanese tonal syllables with the help of a language 
expert. Totally about 2567 syllables are labeled in the 
transcripts. Those transcriptions were just prepared as the 
standard answer for the speech recognition in evaluation. 
 
2.3 Text Documents Data 
 
   The text resources consist of 840k Chinese characters from 
FTVN news website [3], and the most of the data are automatic 
mined twice a day from the November ’02 –December ’02 by a 
web agent [4]. We collected 7 classes of news, including sport, 
economics, art, life, social headline, and politics. There are a 
total of 3079 stories and about 100 stories everyday on average. 
In addition, the texts were preprocessed to remove undesirable 
material (table, lists, punctuation marks, etc) and categorized 
by the date. The texts were then further processed for language 
model training. First the texts were segmented into sentences 
and then normalized in order to better approximate a spoken 
form by the texts analysis module [5]. Because there is no 
natural boundary between 2 successive words in Chinese 
characters, we have to transcribe the Chinese characters to 
phonetic representation by word segmentation. In the cause to 
align the texts to the video by continuous speech recognition, 
we used the bi-lingual pronunciation lexicon (will mention in 
section 3.2) for automatic translating the literature form of 
Chinese characters to spoken form of Taiwanese tonal syllables. 
All the statistics of the TV news and text document data are 
listed in <table.2>.  
 
 TV news Document 
No. of Stories 
(Duration) 
13  
(3 min) 
27 
 (10 min) 
3079
100(Doc./day)
No. of Sentences 64 182 74k
No. of Characters 928 1969 840k
Table 2. The statistics of the TV news (extracted 2 anchors) 
and the FTVN text data (Nov. ’02-Dec. ’02) 
 
3. Automatic Speech Recognition Overview 
 
   In this index system, ASR plays an important role, and it can 
be divided into three parts: acoustic, pronunciation and 
language model. The general formula in speech recognition can 
be expressed as following: 
)]}|()|([max)({maxargˆ ii
i
vxpwvPwp
w
w ⋅=                             (1) 
where  is the  multiple pronunciation of the word w. 
 is the acoustic likelihood of pronunciation . The 
unigram probability distribution of the pronunciations of w is 
given by , subjected to the normalization constraint: 
iv thi
)|( ivxP iv
)|( wvP i
1)|( =∑ WvPN
i
i                                                                    (2)  
where N is the total number of pronunciations of word W. 
 
3.1 Acoustic Processing 
 
   In this part, we use a single phonetic set to cover all the 
sounds of Taiwanese and Mandarin speech. The phonetic 
transcription system called Formosa Phonetic Alphabet (ForPA) 
[6] was designed to transcribe three major languages 
(Mandarin, Taiwanese, Hakka) in Taiwan. Sounds which are 
represented by the same ForPA symbol share one common 
 dynamic programming algorithm. For evaluation our speech 
recognition in spontaneous speech, we used the manual 
transcriptions as the collect syllable answers for anchor speech, 
and the syllable accuracy rate are 42%, 40% for both anchors. 
The results are lower than the baseline, the main reason we 
thought is that the anchor’s speech is spontaneous, and the 
baseline speech is read. The spontaneous speech is affected by 
the speakers accent, speaking style. Therefore, the results are 
always worse than the read speech. In the third row, the correct 
number of index in anchor 1 is 11 and 22 for anchor 2 when we 
used the FTVN documents as the candidates. However, the 
news documents in the FTVN may not be exactly the same as 
the anchor’s speech in TV news. Therefore, we used the 
manual transcriptions as the candidate documents for another 
evaluation; the results showed that 100% index accuracy rate 
are achieved. Obviously, the main error of the index is not 
caused by the in the ASR error or bi-lingual texts alignment, 
but is caused by the inconsistency between the anchor’s speech 
and the FTVN documents. We analyzed two examples of the 
manual and FTVN documents, and the results are shown in Fig. 
2. The left side picture (average correlation 0.99) shows the 
example of the syllable points between the manual and FTVN 
documents in correct index, and right side picture (average 
correlation 0.4) shows the example of an error index.  
 
 Anchor 1 Anchor 2 
Syllable Accuracy Rate 
(bi-gram) 
 
42% 40%
Index Accuracy Rate  
(web documents from FTVN) 
85% 
(11/13) 
81% 
(22/27)
Index Accuracy Rate  
(manual transcribed document) 
 
100% 100%
Table 3. The results of the syllable accuracy rate and index 
accuracy rate in different type documents for two anchors.  
 
Figure 2. The index examples for the corresponding syllable 
points between the manual and web documents.  
 
6. Concluding Remarks and Future Work 
 
This paper presented the initial results of the Taiwanese TV 
news that was aligned to the FTVN news documents index 
system. Due to the popularity of the internet and multimedia, 
this research area has become very important. We are 
successful in integrating the automatic speech recognition and 
bi-lingual text alignment in the index system, and the results 
also showed that the index accuracy rate can achieve over 80%, 
and the most error is due to the mismatch between the anchor’s 
speech and the FTVN news documents.   
In the future work, firstly we will develop the automatic 
segmentation technique for the TV news, and collect more data 
for the Taiwanese speech recognition. The bi-lingual 
translation model must be improved for strengthening the text 
alignment, and reduce the mismatch of the anchor’s speech and 
the web news documents.  
 
7. Reference 
 
[1] Hsin-min Wang, "Experiments in Syllable-based Retrieval 
of TV News Speech in Mandarin Chinese," Speech 
Communication, 32(1-2), pp. 49-60, Sept. 2000 
[2] Jean-Manuel Van Thong, et al, “SpeechBot: a Speech 
Recognition based Audio Indexing System for the Web,” 
In Proc. International Conference on Computer-Assisted 
Information Retrieval (RIAO), 2000 
[3] http://www.ftvn.com.tw/ 
[4] Chia-Hui Chang, Harianto Siek, Jiann-Jyh Lu, Jen-Jie 
Chiou and Chun-Nan Hsu, “Reconfigurable Web wrapper 
agents” IEEE Intelligent Systems, 18(5):34-40, Special 
Issue on Web Information Integration, September/October 
2003  
[5] Ren-yuan Lyu, et al., “A Taiwanese (Min-nan) 
Text-to-Speech (TTS) System Based on Automatically 
Generated Synthetic Units”, ICSLP2000, Oct. 2000 
[6] Min-siong Liang, et al., “An Efficient Algorithm to Select 
Phonetically Balanced Scripts for Constructing a Speech 
Corpus,” 2003. Beijin, China 
[7] Dau-Cheng Lyu, et al., "Speaker Independent Acoustic 
Modeling for Large Vocabulary Bi-lingual 
Twaiwanse/Mandrain Continuous Speech Recognition," 
In Proc. SST 02, Melbourne, December 2002 
[8] Judith M. Kessens, Catia Cucchiarini, Helmer Strik, “A 
data-driven method for modeling pronunciation variation” 
Speech Communication, Vol 40, pp. 517 – 534, June 2003 
[9] Jean Veronis, "Parallel Text Processing Alignment and 
Use of Translation Corpora," Kluwer Academic, 2000 
[10] I. Dan Melamed, “Empirical Methods for Exploiting 
Parallel Texts” 2001 
[11] Cheung, L. Y. L., et al., "Some Considerations on 
Guidelines for Bilingual Alignment and Terminology 
Extraction", The 1st SIGHAN Workshop on Chinese 
Language Processing, COLING, Taipei, August 2002 
[12] Dau-Cheng Lyu, et al, "Large Vocabulary Taiwanese 
(Min-nan) Speech Recognition Using Tone Features and 
Statistical Pronunciation Modeling" In Proc. EuroSpeech, 
Switzerland, 2003. 
 2. THE UNIFIED FRAMEWORK OF 
MULTILINGUAL ASR 
2.1. A Unified Framework for Multilingual Speech 
Recognition 
Unlike the conventional approach which divides the 
recognition task as syllable decoding and character decoding, 
the new proposed approach adopt a one-state searching strategy 
as show in figure1, which decodes the acoustic feature sequence 
X directly to the desired character sequence C*, no matter what 
regionalects are spoken. The decoding equation can thus be 
shown as follows: 
              ),(maxarg* CXPC
C
=  
 
Fig 1 the decoding problem of Chinese speech recognition 
In a unified framework, shown as figure2, there are at least 2 
critical differences from the conventional one. One is in the 
lexicon layer, where the new framework adopts the 
character-to-pronunciation mapping which can easily 
incorporate the multiple pronunciations caused by multiple 
languages. For this purpose, we have used Formosa Lexicon [6]. 
Another one is in the grammar layer, where the character is 
adopted as the nodes of the searching net. This makes it be 
language independent! By the way, tone is a common feature 
for all Chinese languages. In the unified framework described 
above, it is easy to incorporate the tone feature into the system. 
There is no need to change the grammar layer. In another aspect, 
multilingual speech recognition is one of the most popular 
research topics recently in speech signal processing. It is 
essential to collect a large-scale multilingual speech database 
for research, especially for designing a speaker independent and 
multilingual speech recognition system. 
 
Fig 2 A Unified 3-layer frameworks for  multi-regionalect Chinese speech 
recognition 
2.2. Acoustic Modeling & Multiple Pronunciation Lexicon 
The performance of any ASR system is highly dependent on 
the quality of the acoustic models. In particular, we need to 
support several languages in embedded systems at the same 
time. Due to memory size and capacity of embedded system are 
not bigger than that of PC, we adopted the Initials and tonal 
context-independent Finals as the HMM models. The same 
phoneme (including tone) symbols in the transcript in trilingual 
share the same training data. There are two approaches in 
dealing with pronunciation variations, i.e., knowledge-based 
approach and data-driven approach. The former consists of 
generating variants by using phonological rules, and the later 
consists of performing phone recognition to obtain information 
on the pronunciation variation in the data. We adopt the 
rule-based approach for Taiwanese tone sandhi and data-driven 
approach based on confusion matrix for finding phoneme 
mapping between the real pronunciations and canonical 
pronunciations. The matrix is constructed by a 
dynamic-programming technique to align the recognition 
results with the canonical transcriptions. We chose the most 
variational pronunciations by the confidence scores which is the 
occurrence possibility. The sum of all the occurred possibility 
of each character is then normalized to unity for fair competition 
in the Viterbi search. 
2.3. Searching Strategies 
A time-synchronous Viterbi beam search based on the 
token-passing algorithm was used as the basic searching 
algorithm in the new framework. Not like the entire Viterbi 
trellis search to find the optimal path, we use several pruning 
techniques to accelerate the searching speed by using certain 
heuristics. Common heuristic in beam search can be divided 
into the following categories:. 
1. Using tree-based: 
The previous papers [7] have pointed out the benefits of 
tree-based lexicon, especially in the large vocabulary speech 
recognition. It can merge the same token in the network, reduce 
the search space greatly, and  prevent the memory squander 
while decoding. 
 2. Ranking the hypothesis:  
In the time-synchronous searching, at each frame, only N 
(N=1000, for instance) active candidates are reserved for 
branching out in the next frame by ranking the acoustic scores. 
 3. Using probability score difference as threshold:  
Any word hypothesis whose acoustic score is less than a 
threshold T (T=250 in log probability, for instance) from the 
maximal probability of this frame are discard. In other words, it 
is like hypothesis ranking, but it uses the score difference to 
keep the upper bound's candidates could branching out in the 
next frame.  
4. Using level constraint width:  
It's a combinational strategy of the second and the third 
heuristics mentioned above. During the search, the word 
hypotheses, which meet the above two requirements 
simultaneously are kept alive. Therefore, the number of  
hypotheses  is a function of the number of symbol levels 
recognized, just like the dynamic threshold for searching in a 
net. We believe that due to the tree-based lexicon, there is a 
strong constraint in the searching token. It's reasonable to keep 
more hypotheses alive in the initial because every token has 
opportunity to compete against others. Nevertheless, the more 
close to the end of the search, the more token restriction there is. 
For this reason, we should prune more hypotheses in the tail of 
the search.  
 lower than the case in  (T=300). However, the speed is much 
faster than the case in (T=300), therefore the level constraint 
width is a better beam search strategy to minimize the 
computational cost with a minimal decrease in recognition 
accuracy. 
 CAR (%) Speed (xRealtime) 
(1)+(2)+(3)+(T=200) 88.4 1.37 
(1)+(2)+(3)+(T=300) 93.5 4.26 
(1)+(2)+(3)+(4)+(T=200) 92.5 1.75 
Table 3 The character accuracy rate (CAR) of the task of the Tang-poem, 
under different combinations of searching strategies: (1). Using tree-based 
lexicon, (2), ranking the hypothesis, (3). Using probability score difference, (4). 
Using level constrain width. 
4.3. The Performance of PDA System 
On PDA, we use TW01 and MD01 as our training corpuses. 
Our system was established on the language-independent sets, 
with phoneme sharing between languages. The audio signal is 
16 kHz, and 39-dimensional MFCC features were computed. In 
order to test performance of our system on PDA, we make a 
simple experiment to understand effect on PDA. The test set, 
contains three kinds applications, contains 442 sentences (about 
904.5 seconds) recorded from 1 speaker. In every kind, we 
make a statistical table about sentences of different language. It 
is shown as Table 4. The second column is sentences of 
Mandarin. For example, these names of song are 10 sentences 
belong to Mandarin. 
Language 
Domain 
Sentence of 
Mandarin 
Sentences of 
Taiwanese 
Sentence of 
English 
TV 30 25 24 
Name 68 20 15 
Song 102 75 51 
Table 4 Statistics about sentences of difference language in four domains 
 
The test results shown in Table 5 are promising. The result 
was close to the monolingual system. The first column is 
application domain, e.g. the “TV” represents TV name. The 
second column is numbers of entire for speech recognition 
engine. In the forth column, SER is sentence error rate and the 
forth row is the lowest SER. This is because of the same first 
name in our testing names. It can also be seen that the length of 
speech doesn’t affect recognition time. The entirety velocity 
majors with the amount of vocabulary. Although the amount of 
vocabulary is not many, but this situation in lives of people is 
enough. The SER of the third is 94.93% which is high enough to 
accept it. By observing Table5, we find out the recognition time 
is longer in our system. This fact reveals   fixed-point operation 
is not applied and PDA do not support float-point accelerator 
yet. In another aspect, our ForSDat database is recorded on 
notebook at that time. Acoustic models and testing set isn’t the 
same condition of environment that is possible a factor which 
affects our recognition rate.  
 
Domain Vocabulary Length (s) SER(%) Time (s)
TV 79 2.431 94.93 23.57 
Name 103 1.569 84.87 35.02 
Song 228 2.087 88.60 90.92 
Table 5 The statistics of our system performance are vocabulary, length, time 
and SER (Sentence Error Rate) 
5. CONCLUSIONS AND FUTURE WORKS 
In this paper, we have investigated that the unified 
framework is robust when combining 2 Chinese regionalects, 
including Mandarin and Taiwanese. It seems still workable 
when some English words were added into the vocabulary list.  
Under the unified framework, we achieved promising results on 
PDA. We not only saved the computing resource required to 
perform the speech recognition engine but also provided 
multilingual ability in this framework. From these experiments, 
we have shown that the recognition rate is still acceptable when 
the engine was ported from PC to PDA. At present, the 
recognition rate is almost 80%~90%. Although the recognition 
speed is slow, this problem will be solved. In future, 
multilingual support will be a trend on handled devices. It is 
easy to use this framework to integrate more languages, such as 
Hakka, Cantonese, Shanghais and so on. This will truly increase 
the user-friendliness for the Chinese society. 
REFERENCES 
[1] S. H. Maes et al. “Conversational networking conversational protocols for 
transport, coding, and control,” Proc. Int. Conf. on Spoken Language 
Processing, October 2000. 
[2] R. C. Rose et al. “On the implementation of ASR algorithms for hand-held 
wireless mobile devices,” ICASSP ’01.  
[3] Olli Viikki, “Asr in portable in wirelss devices,” Automatic Speech 
Recognition and Understanding, 2001. ASRU ’01. 
[4] Dau-Cheng Lyu et al. "Large Vocabulary Taiwanese (Min-nan) Speech 
Recognition Using Tone Features and Statistical Pronunciation 
Modeling," In Proc. EuroSpeech 03, Geneva, September 2003 
[5] Ren-Yuan Lyu et al. “A Unified Framework for Large Vocabulary Speech 
Recognition of Mutually Unintelligible Chinese Regionalects,”  
ICSLP,2004. 
[6] Min-siong Liang et. al. “Construct a Multi-Lingual Speech Corpus in 
Taiwan with Extracting Phonetically Balanced Articles,”  INTERSPEECH 
2004 - ICSLP, Jeju island, Korea. 
[7] Hermann Ney, Ortmanns. S, “Progress in Dynamic Programming Search 
for LVCSR,” Proceedings of the IEEE, Vol. 88, pp. 1224-1240, August 
2000. 
[8] Xia Wang et al. “An Embedded Multilingual speech recognition system 
for Mandarin, Cantonese, and English” In Proc. Natural Language 
Processing and Knowledge Engineering, October 2003. 
[9] Jyh-Shing Roger Jang, Shiuan-Sung Lin, "Optimazation of Viterbi Beam 
Search Speecg Recognition," In Proc. Internal Symposium on Chinese 
Spoken Language Processing, Taipei, August 2002 
[10]  Mingkuan Liu et al. "Mandrain Accent Adaptation Based on 
Contest-Independent/Context-Depent Pronunciation Modeling," In 
Proc.ICASSP 00, 2000 
[11]  Mirjam Wester, "Pronunciation Modeling for ASR-knowledge-based and 
Data-drived Methods," Journal of Computer Speech and Language 
17(2003), pp. 69-85, 2003 
                                                          
 
in TLPA and "pa ( )" in Pinyin may be confused with each 
other because the phoneme /p/ is pronounced differently in the 
two systems. Therefore, it is necessary to design a more 
suitable phoneme set, the newly proposed ForPA, for 
multilingual speech data labeling [6]. 
2.2. Word Segmentation and Mandarin-Taiwanese 
translation (Sentence-to- morpheme) 
In spite of that the Han-Roman orthography is the most 
common style for writing Taiwanese in the contemporary 
Taiwan, all of three types of written texts (i.e. FC, FR and 
MCR scripts) can be analyzed by our text analysis module. 
Since there is no natural boundary between two successive 
words, we must segment text into word sequence first. In fact, 
each of Taiwanese single-syllabic words has 2 distinct 
manners of pronunciation: one for classic literature like poems, 
and the other for oral expression in daily lives. In our 
Mandarin-Taiwanese bi-lingual lexicon, each Chinese 
character string in the lexicon contains one pronunciation in 
Mandarin and at least two pronunciations in Taiwanese with 
corresponding transcription in ForPA. Each word in 
Taiwanese contains one literature (classic) pronunciation and 
at least one oral pronunciation. The statistics of Mandarin-
Taiwanese lexicon is shown in <Table.1>. We use the 
bilingual pronunciation dictionary as the knowledge source to 
do word segmentation algorithm based on the sequentially 
maximal-length matching, which segment Mandarin sentence 
into maximal-length words combination. The following 
paragraph will describe word segmentation algorithm: 
Step1: If S is the input sentence，transform S into character 
string C={C1 C2 C3 ...}, where the number of Chinese 
characters in string C is N, Ci is the minimum 
combination element and the maximum number of the 
Chinese character of one word in lexicon is n. 
Step2: Search the pattern with {C1 C2 C3 ... Cn} in the lexicon 
Step3: If the pattern exists, the pattern {C1 C2 C3 ... Cn} is 
denoted as a word. Repeat Step2 with a new pattern 
{Cn+1 Cn+2 Cn+3 ... Cn+n}. 
Step4: If the pattern does not exist, repeat Step2 with a new 
pattern {C1 C2 C3 ... Cn-1}. 
 Finally, we straightly translate Mandarin into Taiwanese 
word-by-word, and transcribe Taiwanese words phonetically 
in ForPA (see in Section 2.3). An example of the segmentation, 
translation and transcription progress is shown as <Fig 2>, 
where the input Mandarin sentence is “ ” (he 
is very happy today). 
 
 LP-Taiwanese OP-Taiwanese Total 
1-Syl 2319 8040 10359 
2-Syl 21337 49222 70559 
3-Syl 7163 11367 18530 
4-Syl 55 15525 15580 
5-Syl 1 711 712 
6-Syl 0 497 497 
7-Syl 0 478 478 
8-Syl 0 195 195 
9-Syl 0 3 3 
10-Syl 0 20 20 
Total 30875 86060 116935 
<Table 1>: The number of pronunciation of bi-lingual 
Lexicons, including literature pronunciation (LP) and oral 
pronunciation (OP) in Taiwanese (Syl: syllable). 
 
<Fig 2> The text analysis progress, which combine word 
segmentation, translation and labeling (see section 2.3), where 
the input Mandarin sentence is “ ”, “1-Syl 
Word” is denoted as one-syllable word and so on. 
2.3. Labeling (morpheme-to-phoneme) and Normalization 
of the digit sequences 
For each segmented word, there exists not only one Taiwanese 
pronunciation for it. To deal with the multiple-pronunciation 
problem, two stage strategies are used. The first stage: Choose 
the oral pronunciation first to do labeling. If the oral 
pronunciation does not exist, the literate pronunciation will be 
considered. The second stage: We build a searching network 
with pronunciation frequencies as node information and 
pronunciation transitional frequencies as arc information for 
each sentence. The best pronunciation is then conducted by 
Viterbi search. 
 Another important issue for text analysis is the 
normalization of the digit sequences. However, for digits, 
there are also 2 manners (literate and oral) of pronunciation 
exist in daily lives. The manner of pronunciation depends on 
the position of the digit in a sequence, which can be 
summarized in rules as shown in <Table. 2>. In addition, if a 
digit sequence does not represent a quantity, it is pronounced 
with the classic pronunciation. 
 
Position Pronunciation 
Ten 
Million 
Read 0-9 as oral pronunciation 
Million Read 0-9 as oral pronunciation 
Hundred 
Thousand
No sound for 1, 2 as literate pron. and others 
as oral pron. 
Ten 
Thousand
If the digit is 0 in hundred thousand 
position, read 0-9 as oral pron. If the digit is 
not 0 in hundred thousand position, 1,2 read 
as literate pron. and others as oral pron. 
Thousand Read 0-9 as oral pronunciation 
Hundred Read 0-9 as oral pronunciation 
Ten No sound for 1, 2 as literate pron. and others 
as oral pron. 
Unit digit If the digit is 0 in hundred thousand 
position, read 0-9 as oral pron. If the digit is 
not 0 in hundred thousand position, 1,2 read 
as literate pron. and others as oral pron. 
<Table.2>: The rules for normalization of the digit sequences 
where pron. denote “pronunciation”. 
3. Prosody analysis module 
Like Mandarin, Taiwanese is a tonal language. 
Traditionally speaking, it has seven lexical tones, two of 
which are carried in syllables ended with stop consonants, 
i2     gin2-a1-lit6   sim2-ging2   zim2  her4
1-Syl word 
2-Syl word 
Word  
Translation
Word  
Segmentation
Labeling
Input
reach over 97% accuracy. If we do not consider tone-sandhi, 
the system can translate article into correct pronunciation 
close to the 88% rate and the most errors happen in names and 
out of vocabulary. Because the Taiwanese has uniform tone-
sandhi rules, it is acceptable that the accuracy rate of tone-
sandhi is lower. 
 
 Expert1 Expert2 
Word Seg & Transfer 97.80% 98.76% 
Labeling 89.96% 88.27% 
Tone-sandhi 65.43% 62.43% 
<Table 5> The statistics of performance in parts of word 
segment and transfer, labeling and tone-sandhi. 
5. Waveform synthesis module 
 
Before we explain operation of synthesis module in the 
system, it is necessary to denote what INITIAL/FINAL is. An 
INITIAL/FINAL format can describe the composition of 
Taiwanese syllable. INITIAL is the initial consonant and 
FINAL is the vowel (or diphthong) part with an optional 
medial or a nasal ending [7].  
 There are many variety of synthesis method. We adopt 
the most popular method TD-PSOLA to modify the prosodic 
feature of selected units. Synthesis components are used to 
not only raise or lower pitch but also enlarge or shrink 
duration. After the analysis of tonal syllables, we can gather 
duration and short pause information in each syllable. By the 
information mentioned above, the synthesis speech will be 
accomplished in below cases: 
 
•  Case 1: if the syllable consist of unvoiced consonant (p-, 
t-, g-, k-, z-, s-, c-, h-), the system just modify duration of 
the unvoiced INITIAL, and modify duration and pitch of 
FINAL. 
• Case 2: the system will modify duration and pitch both 
on INITIAL and FINAL if there do not exist unvoiced 
INITIAL. 
• Case 3: replace the short pause with a zero-value section.  
Waveform synthesis module 
6. Applications with Taiwanese TTS system 
By adopting proposed Taiwanese TTS system, a Taiwanese 
talking electronic lexicon can be built. We can input 
Taiwanese or Mandarin words, and then the output is a list of 
Taiwanese words and corresponding Taiwanese speech. It is a 
good and friendly tool to support those who want to learn 
Taiwanese. On the other hand, as mentioned about Section 2.1, 
the ForPA is a more suitable phonetic alphabet set for 
Taiwanese. In order to spread ForPA, it is necessary to 
construct a Taiwanese interactive phonetic alphabet learning 
tool. When we type various kinds of existing Taiwanese 
syllables in ForPA, the tool will pronounce simultaneously. In 
addition, the TTS system also can sing in Taiwanese with 
music of song. 
7. Conclusion 
As shown in <Fig. 5>, we have successfully constructed a 
mandarin-to-Taiwanese TTS system. Hence, most Mandarin 
articles can be translated into Taiwanese and automatically 
converted to a speech signal in Taiwanese. This is great 
helpful for those who want to learn mother-tongue language in 
Taiwan. However, there are still a lot to do. In the future, we 
should improve tone-sandhi for more accurate speech 
synthesis. 
 
 
<Fig. 5> The interface of Taiwanese TTS system 
8. References 
[1] Chu, M., H. Peng, Y. Zhao, Z. Niu, E. Chang, “Microsoft 
Mulan - a bilingual TTS system”, ICASSP '03, Volume: 
1, pp. I-264 - I-267, 6-10 April 2003. 
[2] Chou, F. C., C. Y. Tseng, L. S.  Lee, “A set of corpus-
based text-to-speech synthesis technologies for Mandarin 
Chinese”, IEEE Transactions on Speech and Audio 
Processing, Volume: 10, Issue: 7, pp. 481 – 494, Oct. 
2002. 
[3] Chen, Sin-Horng, Shaw-Hwa Hwang and Yih-Ru Wang, 
“An RNN-based prosodic information synthesizer for 
Mandarin text-to-speech”, IEEE Transactions on Speech 
and Audio Processing, pp. 226 – 239, Volume 6,  Issue 3,  
May 1998. 
[4] Lin, Chuan-Jie and Hsin-Hsi Chen, “A Mandarin to 
Taiwanese Min Nan Machine Translation System with 
Speech Synthesis of Taiwanese Min Nan.” International 
Journal of Computational Linguistics and Chinese 
Language Processing, pp. 59-84, 4(1), February 1999. 
[5] Lyu, R. Y., Z. H. Fu, Y. C Chiang, H. M. Liu, “A 
Taiwanese (Min-nan) Text-to-Speech (TTS) System 
Based on Automatically Generated Synthetic Units”, 
ICSLP2000, Oct. 2000. 
[6] Liang, M. S., R. Y. Lyu, Y. C. Chiang, “An Efficient 
Algorithm to Select Phonetically Balanced Scripts for 
Constructing A speech Corpus”, IEEE-NLPKE 2003, 
October 26-29, 2003, Beijing, China. 
[7] Chou, F. C., C. Y. Tseng, “Corpus-based Mandarin 
Speech Synthesis with Contextual Syllabic Units Based 
on Phonetic Properties”, ICASSP98, pp. 893 s- 896 vol.2, 
12-15 May 1998. 
 Fig.1. Such a looped net will be called free-syllable net, if 
each syllable follows freely, or equally likely, another syllable. 
 
Fig. 1. Free syllable recognition net. 
 For our Sutra transcription problem, we have, in addition 
to each speech utterance, its associated text in Chinese 
character. Based on the multiple pronunciations of each 
Chinese character, we can construct a much smaller 
recognition net. An example for the utterance “ ” is in 
Fig. 2. We will call such a net (multiple pronunciations) 
sausage net for its shape, following [10]. Higher recognition 
results can be expected due to smaller complexity in the 
recognition net. Our task is then amount to how to construct 
sausage nets and which acoustic model to choose. 
Fig. 2. The sausage searching net. The net is constructed from 
the multiple pronunciations of each Chinese character from 
our Formosa Lexicons. The corresponding Chinese characters 
of multiple pronunciations are also shown. 
2.1. The pronunciation Lexicons 
There are three pronunciation lexicons for the multiple 
pronunciations in Taiwanese of the Chinese characters.  
 The first is Formosa Lexicon. It is a combination of two 
lexicons: Formosa Mandarin-Taiwanese Bi-lingual lexicon 
and Gang's Taiwanese lexicon [10][11][12]. The former is 
derived from Mandarin lexicon, and thus many commonly 
used Taiwanese terms are missing. The latter contains 
Taiwanese expressions from a sampling of radio talk show. 
Some statistics of the two sub-lexicons are summarized in 
Table 1 and 2. 
 
 CLP 
Taiwanese 
DLP 
Taiwanese Total 
1-Syllable 2319 8040 10359 
2-Syllable 21337 49222 70559 
3-Syllable 7163 11367 18530 
4-Syllable 55 15525 15580 
5-Syllable 1 711 712 
6-Syllable 0 497 497 
7-Syllable 0 478 478 
8-Syllable 0 195 195 
9-Syllable 0 3 3 
10-Syllable 0 20 20 
Total 30875 86060 116935 
Table 1. The number of pronunciation of Formosa bi-
lingual Lexicons, including classic literature pronuncia-
tion (CLP) and daily life pronunciation (DLP). 
 
 Taiwanese 
1-Syllable 8153 
2- Syllable 46587 
3- Syllable 13241 
4- Syllable 2106 
5- Syllable 175 
Total 70262 
Table 2. The distribution of 
Gang's Taiwanese lexicon. 
 Sutra Lexicon. The second lexicon is the Sutra derived 
lexicon. It is the pronunciations collected from the published 
volumes of the Sutra. (Only those in [7] are included for this 
experiment.) The general lexicon, used for wider range of 
applications, tends to have higher number of multiple 
pronunciations. We expect this lexicon to have a high “hit 
rate.”  
 The third lexicon is the combination of the previous two, 
and called Enhanced Lexicon. 
2.2. The Recognition Net 
For the searching net of the ASR, we have four choices.  
 The first is the free-syllable net. It is simply a looped net 
of all Taiwanese syllables in TBS, which the number of 
syllable is 467, denoted as F-Syl Net. 
 The other three searching nets are the sausage nets 
generated from each of the three pronunciation lexicons. The 
nets are denoted as FL-S Net, SL-S Net, and EL-S Net for the 
Formosa, Sutra, and Enhanced Lexicon respectively. 
 However, a lexicon is inevitably incomplete, and we could 
be confronted with the missing character problem, and the 
missing pronunciation problem. 
 The missing character problem is a problem since there 
are simply too many Chinese characters. With Unicode 
Standard contains more than thirty thousands Chinese charac-
ters, the Sutra still have some not in the Unicode character set. 
The Formosa Lexicon has much less distinct characters, and 
the missing characters problem is inevitable. When a missing 
character is encountered, we use all possible syllables as its 
multiple pronunciations. Fig. 3 illustrates this case. 
 
 
Fig. 3. The sausage searching net with missing character C0 
assuming all syllables as its possible pronunciations. 
 
The missing pronunciation problem is one that the real 
pronunciation of a character is not included in the lexicon, 
although it has entry for that character with different pronun-
ciation. Since each character could have potentially missing 
pronunciation in the lexicon, we do nothing to prevent the 
problem. It is thus the task of human expert to correct the 
missing pronunciation in the later stage. 
2.3. The Acoustic Models 
For the acoustic model, we can choose speaker dependent or 
speaker independent model.  
C0 C1 C2 
hiong
hiunn
ler zan
hiong
ler 
zan 
  
 Applying the rules to each syllable in the lexicon, we 
virtually end up with much larger number of multiple 
pronunciations for each Chinese character. With this 
adaptation in the lexicon, we call the sausage net so generated 
AL-S-Net. 
 
Error Pattern Error Count 
i-ng → i-n 126 
bh-er → bh-o 50 
i-m → i-n 44 
a-m → a-n 43 
inn-onn → inn-unn 29 
a-m → a-ng 26 
a-ng → a-m 25 
a-n → a-m 23 
i-m → i-ng 21 
b-er → b-o 19 
i-n → i-m 19 
d-er → d-o 18 
Statistics-
based rules 
i-a+ng → i-o+ng 17 
Table 7. The 13 most frequent substitution errors from 
the partially validated ForSDAT-TW02 corpus. 
 
 
 
 
 
 
3.3. The Result 
The result of the automatic annotation is, of course, not perfect. 
But with these annotations, we proceed to train speaker 
dependent models using the TBS speech corpus, and then 
perform the recognition on the 160 testing utterances with free 
syllable net. The results are in Table 8. 
 
Training Text Annotation Method Accuracy rate for TBS testing set 
(1) G2P[I] 75.56% 
(2) SI/FL-S-Net 77.30% 
(3) SI/EL-S-Net  80.16% 
(4) SI/AL-S-Net 72.96% 
* Manual annotated training corpus 82.49% 
* Speaker independent model with 
adaptation (Sec 2.4) 55.34% 
Table 8. Recognition results applying the four annotation 
approaches to the training corpus. See text for 
explanation of notations. The fifth row is for that using 
manually annotated training corpus. 
  
 We include the result from that using manual annotation in 
the 5th row for comparison. It should be the upper limit. From 
Table 8 it appears that every approach leads to satisfactory 
result, considering we are not using more powerful language 
model such as tri-gram. Also, when compared to the speaker 
independent model, it clearly shows the advantages of speaker 
dependent model. It also shows that restricting the search net 
to sausage net does overcome the advantages of speaker 
dependent model, as exhibited in the last section. Note that we 
do not use the sausage net in this experiment to show a 
welcome partial solution to the problem of not having 
phonetic transcription in the training corpus. 
 
 
4. Conclusions 
Taiwanese text corpus collection suffers from the multiple 
pronunciation problems. By further augmenting the text with 
read speech, we are able to reduce the effort for phonetic 
transcription of the text using automatic speech recognition 
with a sausage searching net constructed from the text 
corresponding to its speech utterance. Our approach shows 
advantages over, say, the method of re-labeling the training 
corpus of [1] which does not utilize the special multiple 
pronunciations. 
5. References 
[1] Kanokphara, Supphanat, Virongrong Tesprasit and 
Rachod Thongprasirt, “Pronunciation Variation Speech 
Recognition without Dictionary Modification on Sparse 
Database”, In Proc. ICASSP 2003, Hong Kong, 2003. 
[2] Liang, Min-siong, Ren-yuan Lyu, Yuang-chin Chiang, 
and Dau-Cheng Lyu, “A Taiwanese Text-to-Speech 
System with Applications to Language Learning,” In 
Proc. ICALT, Joensuu, Finland, 2004. 
[3] Tsai, Ming-Yi, Fu-chiang Chou, and Lin-shan Lee, 
“Improved pronunciation modeling by inverse word 
frequency and pronunciation entropy”, IEEE ASRU 2002, 
pp. 53-56. 
[4] Raux, Antoine, “Automated Lexical Adaptation and 
Speaker Clustering based on Pronunciation Habits for 
Non-Native Speech Recognition”, In Proc. International 
Conference on Spoken Language Processing, Jeju Island, 
Korea, 2004. 
[5] http://usinfo.state.gov/, U.S. Department of State's 
Bureau of International Information Programs, Dec, 2003. 
[6] Sik, DatGuan, The Four Basic Sutra in Taiwanese, 
DiGuan Temple, HsinChu, Taiwan, 2004. 
[7] Sik, DatGuan, Earth Store Sutra in Taiwanese, DiGuan 
Temple, HsinChu, Taiwan, 2004. 
[8] Siohan, Olivier, Bhuvana Ramabhadran and Geoffrey 
Zweig, “Speech Recognition Error Analysis on the 
English MALACH Corpus”, In Proc. ICSLP 2004, Jeju 
Island, Korea, 2004. 
[9] Ramabhadran, Bhuvana, Jing Huang and Michael 
Picheny, “Towards Automatic Transcription of Large 
Spoken Archives - English ASR the MALACH Project”, 
In Proc. ICASSP, Hong Kong, China, 2003. 
[10] Mangu, Lidia, Eric Brill and Andreas Stolcke, "Finding 
Consensus in Speech Recognition: word error 
minimization and other applications of confusion 
networks", Computer Speech and Language, 2000. 
[11] Liang, Min-Siong, Ren-Yuan Lyu, Yuang-Chin Chiang, 
“An Efficient Algorithm to Select Phonetically Balanced 
Scripts for Constructing A speech Corpus”, IEEE-
NLPKE 2003, October 26-29, 2003, Beijing, China. 
[12] Liang, Min-siong Dau-cheng Lyu, Yuang-chin Chiang, 
Ren-yuan Lyu, “Construct a Multi-Lingual Speech 
Corpus in Taiwan with Extracting Phonetically Balanced 
Articles”, In Proc. ICSLP 2004, Jeju Island, Korea, 2004. 
[13] Lyu, Ren-yuan et al., “Toward Constructing A 
Multilingual Speech Corpus for Taiwanese (Minnan), 
Hakka, and Mandarin”, ICLCLP Vol. 9, No. 2, August 
2004  pp. 1-12 
Taiwanese lexicon, and Syu's Hakka lexicon [7]. Some statistical 
information about the lexicon was listed in <Table 2>. With the 
Gang’s Taiwanese pronunciation lexicon transcribed in ForPA, 
we can extract the sets of distinct syllables and cross-syllable bi-
phones and tri-phones for Taiwanese. The statistics of phonetic 
units considered are listed in <Table.3> 
 Taiwanese 
1-Syllable 8153 
2- Syllable 46587 
3- Syllable 13241 
4- Syllable 2106 
5- Syllable 175 
Total 70262 
<Table 2>: The distribution of Gang's Taiwanese lexicon. 
 Distinct Number Total Number
Syllable 830 150349 
cross-word bi-phone 866 220611 
cross-word tri-phone 11760 300698 
<Table 3>: The distribution of the number of phonetic units for 
syllable, cross-word bi-phone and cross-word tri-phone in 
Taiwanese. 
III. THE PROCESS OF 
PHONETICALLY RICH CORPORA 
 In order to collect speech data with as much information 
about phonetic variations and keep the words as small as possible, 
we have to choose sheets to satisfy some criterions. Two sets of 
words are selected such that they covered all the cross-syllable 
bi-phones and tri-phones, and are called phonetically bi-phone 
rich and tri-phone rich word set respectively. In cross-syllable bi-
phone words set, total distinct cross-syllable bi-phones are 
selected before total distinct syllables are selected. In tri-phone 
words set, total distinct cross-syllable tri-phones are selected 
before total distinct syllables are selected and the similar 
procedure are used to collect bi-phone rich set. The selection of 
algorithm in detail will be described in the following statement. 
A The algorithm for selecting phonetically 
rich word set 
 Before we explain the algorithm, we define several notations 
as follows: }1:{ NiwW i ≤≤=  is the set of all words in the 
lexicon, where N is the number of words, iw  is the ith word. S(wi) 
are the sets of all distinct syllables and U(wi) are denoted as 
cross-syllable bi-phones or cross-syllable tri-phones in the word 
iw  respectively. *tC  is selected word in time t, 
},,{)( **1 tCCtW "=  is selected word set and 
)}(,),({)( **1 tCSCStS "=   is selected distinct syllables set and  
* *
1( ) { ( ), , ( )}tU t U C U C= "  is selected distinct bi-phones set or tri-
phone set till time t. )()( tWWtW c −= is non-chosen word set, 
)()()( tSWStS c −= is non-chosen distinct syllable set and 
( ) ( ) ( )cU t U W U t= −  is non-chosen distinct bi-phones set or tri-
phone set till time t. 
 Based on the notations of the above, the algorithm could be 
described as following steps: 
 
Step 1): Initially t=0 and we have 
(0)W W= , (0) ( )S S W= , (0) ( )P P W=  
Step 2):  
Choose the word *iw as *tC such that maximize the union of 
)1( −tS c  and )( iwS   , i.e. 
))()1((#maxarg
)1(
i
c
tWw
i wStSw
c
i
∩−=
−∈
 -- (1) then 
it wC =* ；If *iw is not unique in (1), choose 
( 1)
arg max# ( )
c
i
i i
w W t
w S w
∈ −
=  -- (2) as *tC ; If *iw is not unique 
in (1) and (2), choose the preceding index word as. *
tC  
)()1()( *t
cc CStStS −−=   ， *)1()( tcc CtWtW −−=  
1+= tt  
Step 3):  
If φ≠)(tS c  and φ≠)(tW c  repeat step 2 
else if  φ=)(tW c   exit the algorithm. 
else if φ=)(tS c   continue next step 
Step 4): 
Choose the word *iw as *tC such that maximize the union of 
( 1)cU t −  and ( )iU w   , i.e. 
( 1)
arg max#( ( 1) ( ))
c
i
c
i i
w W t
w U t U w
∈ −
= − ∩  -- (3) then 
it wC =* ；If *iw is not unique in (3), choose 
( 1)
arg max# ( )
c
i
i i
w W t
w S w
∈ −
=  -- (2) as *tC ; If *iw is not unique 
in (3) and (2), choose the preceding index word as. *
tC  
*( ) ( 1) ( )c c tU t U t U C= − −   ，  *)1()( tcc CtWtW −−=  
1+= tt  
Step 5):  
If φ≠)(tBc  and φ≠)(tW c , repeat the step 4 
else if φ=)(tB c  or φ=)(tW c ,  exit the algorithm. 
B The analysis of bi-phone rich and tri-
phone rich word sets 
 The statistics of the word set selected by the algorithm for 
selecting phonetically rich word sets is shown in <Table 4>. In 
<Table 4>, the number of syllables in tri-phone rich word set is 
about 7.9 times bigger than bi-phone rich word set. In order to 
compensate bi-phone rich word set, the other 9 bi-phone rich 
word sets are selected with the phonetically rich algorithm and 
appended to a new word set. Ideally, all reasonable syllable, bi-
phone and tri-phone units are 830, 1427 and 57227 in Taiwanese, 
but the collected units dose not reach 100% coverage rate which 
coverage equal distinct units is divided by all reasonable distinct 
units. Finally, the statistics of the word sets for recording is 
shown in <Table 5>. 
 
SDG-C is 2 times more than SDG-B, the model of SDG-C is not 
better than the model of SDG-B. It seems to mean that the 
amount of corpus could not influence recognition rate in a small 
model set, where the number of phone model is 50. 
 
free-syllable net
0.31
0.39
0.47
0.55
1 3 5 7 9 11mixture
re
co
gn
iti
on
 ra
te
P-SDG-B
P-SDG-T
P-SDG-C
 
<Fig 3> Syllable recognition rate with phone model for SDG-B, 
SDG-T and SDG-C corpora 
 
 In comparison between inside-syllable bi-phone and inside-
syllable tri-phone models, the number of inside-syllable tri-
phone models is 2.8 times more than inside-syllable bi-phone 
models. But the recognition rate of inside-syllable tri-phone 
models is decaded 3.05%, 5.69% and 1.77% shown in <Fig 4> 
for SDG-B, SDG-T and SDG-C with 12 Gaussian mixture 
respectively. It seems to mean the recognition is lower in bigger 
number of models for lack of abundant speech data. On the other 
hand, the cross-syllable bi-phone and tri-phone model is better 
than inside-syllable bi-phone and tri-phone model in <Fig 5>. 
 
free-syllable net
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
0.77
0.78
0.79
0.8
2 4 6 8 10 12mixture
re
co
gn
iti
on
 ra
te
IB-SDG-B
IB-SDG-T
IB-SDG-C
IT-SDG-B
IT-SDG-C
IT-SDG-T
 
<Fig 4> Syllable recognition rate with inside-syllable bi-phone 
(IB) and inside-syllable tri-phone (IT) model for SDG-B, SDG-T 
and SDG-C corpora 
V. CONCLUSION 
 In this paper, the corpora (SDG-B, SDG-C) are recorded by 
the cross-syllable bi-phone rich and cross-syllable tri-phone rich 
sheets. The corpus SDG-C is combined from SDG-B and SDG-C. 
With three corpora, we can research the difference in 
performance for these three corpora. 
 At first, we expect that the performance of tri-phone rich 
database is better than bi-phone rich database because the 
coverage rate of tri-phone rich database is bigger than bi-phone 
rich database in cross-syllable tri-phone units. In practice, the 
performance does not show significant difference between these 
two databases. It maybe has two reasons: first, the amount of 
corpus is not enough to train for cross-syllable tri-phone models. 
Second, the amount of corpora is adequate for cross-syllable bi-
phone models. In the future, we will collect more data to prove 
whether the performance of tri-phone rich models is better than 
bi-phone rich models. 
free-syllable net
0.63
0.68
0.73
0.78
1 1.5 2 2.5 3 3.5 4mixture
re
co
gn
iti
on
 ra
te
CB-SDG-B CB-SDG-T
CB-SDG-C CT-SDG-B
CT-SDG-T CT-SDG-C
 
<Fig 5> Syllable recognition rate with cross-syllable bi-phone 
(CB) and cross-syllable tri-phone (CT) model for SDG-B, SDG-
T and SDG-C corpora 
VI. REFERENCES 
[1] Hsin-min Wang, “Statistical Analysis of Mandarin Acoustic 
Units and Automatic Extraction of Phonetically Rich 
Sentences Based Upon a Very Large Chinese Text Corpus”, 
ICLCLP vol.3 no.2, August 1998 pp. 93-144 
[2] K. Arora, S. Arora, K. Verma and S.S. Agrawal, 
“Automatic Extraction of Phonetically Rich Sentences from 
Large Text Corpus of Indian Languages”, ICSLP 2004, Jeju, 
Korean, 2004. 
[3] R. Y. Lyu, et al., “A bi-lingual Mandarin/Taiwanese (Min-
nan), Large Vocabulary, Continuous speech recognition 
system based on the Tong-yong phonetic alphabet 
(TYPA),” ICSLP 2000, Beijing, China, 2000. 
[4] Dau-Cheng Lyu, et al., "Speaker Independent Acoustic 
Modeling for Large Vocabulary Bi-lingual 
Twaiwanse/Mandrain Continuous Speech Recognition," In 
Proc. SST, Melbourne, December 2002. 
[5] Min-siong, Laing et al., “A Taiwanese Text-to-Speech 
System with Applications to Language Learning,” In Proc. 
ICALT, Joensuu, Finland, 2004. 
[6] Ren-yaun Lyu et al., “Toward Constructing A Multilingual 
Speech Corpus for Taiwanese (Min-nan), Hakka, and 
Mandarin”, ICLCLP Vol. 9, No. 2, August 2004  pp. 1-12 
[7] Liang, M. S., R. Y. Lyu and Y. C. Chiang, “An efficient 
algorithm to select phonetically balanced scripts for 
constructing corpus,” IEEE NLP-KE, Beijing, China, 2003. 
2 
spoken Chinese. If we really do this, it will not only 
increase the unnecessary searching space, but also cost the 
decoding time too much. 
2. It is not trivial to generate the multiple pronunciation lexicons 
efficiently.  
3. The language model for mixed language is hard to estimate. 
4. When new acoustic features like tones were considered to be 
added to the system, all 3 layers in the syllable decoding and the 
syllable-to-character converter should be modified. This is not a 
trivial task at all. 
 
 To tackle with all the above difficulties, a unified framework 
for multi-regionalect Chinese speech recognition was proposed and 
described in the following section. 
3. A Unified Framework for multi-regionalect Chinese Speech 
Recognition 
 Unlike the conventional approach, which divides the 
recognition task as syllable decoding and character decoding, the 
new proposed approach adopts a one-stage searching strategy as 
shown in <fig.4>, which decodes the acoustic feature sequence X 
directly to the desired character sequence C*, no matter what 
regionalects are spoken. The decoding equation can thus be shown 
as follows: 
(3)               )|(maxarg)(* XCPXC
C
=  
 In such a new proposed framework, the character decoding 
can be implemented by searching in a 3-layer network composed of 
an acoustic model layer, a lexical layer, and a grammar layer as 
shown in <fig.5>. In this framework, there are at least 2 critical 
differences from the conventional one. One is in the lexicon layer, 
where the new framework adopts the character-to-pronunciation 
mapping which can easily incorporate the multiple pronunciations 
caused by multiple regionalects or even multiple “languages”, 
including Japanese, Korean and even Vietnam, which also use the 
Chinese characters more or less. Another one is in the grammar 
layer, where the character is adopted as the nodes of the searching 
net. This makes it be regionalect independent! By the way, tone is 
a common feature for all Chinese regionalects. In the unified 
framework described above, it is easy to incorporate the tone 
feature into the system. There is no need to change the grammar 
layer. 
4. Acoustic Modeling & Multiple Pronunciation Lexicon 
 For validating the new proposed frame work, a 
large-vocabulary bi-regionalect speech recognition system was 
constructed. First of all, a 42-dimension feature vector, consisting 
of MFCCs, their derivatives, and tonal parameters, was used as the 
feature vector [6]. In the acoustic modeling, we adopted the Initials 
and tonal context-independent Finals as the HMM models. The 
same phoneme (including tone) symbols in the transcription in both 
regionalects share the same training data.  
 There are two approaches to deal with pronunciation 
variations, i.e., the knowledge-based approach and the data-driven 
approach.[7][8] The former consists of generating variants by using 
phonological rules, and the later consists of performing phone 
recognition to obtain information on the pronunciation variation in 
the data. We adopt the rule-based approach to deal with the issues 
of Taiwanese tone sandhi, and the pronunciation variation between 
spontaneous and read speech. In another way, using data-driven 
approach based on confusion matrix for finding syllable mapping 
between the real pronunciations and canonical pronunciations. In 
knowledge-based approach, we use a statistical technique to build a 
mapping from each character to its multiple pronunciations by 
using the Formosa Lexicon [9]. Then every character has a reliable 
probability mapping to possible pronunciations. In the data-driven 
approach, the syllable confusion matrix is constructed by a 
dynamic-programming technique to align the recognition results of 
an evaluation data set. We chose the most variational 
pronunciations by the confidence measure which is the occurrence 
possibility, and eliminate the relatively small counts.  The sum of 
all the occurred possibility of each character is then normalized to 
unity for fair competition in the Viterbi search. 
 
 
5. Experiment Results 
5.1 Corpus and Task Description 
 Some information about the speech corpus used in the 
experiments is listed in Table 1. All the speech data were recorded 
using close-talk microphones in normal office environments. 
 The corpus was divided into training and testing sets. The 
training data set and one of the two testing data set were designed 
to be phonetically abundant. The other testing data set is a 
specially designed task with several thousand words derived from 
poems of the Tang Dynasty. All the statistical information for the 
whole speech database were also listed in Table1. The 
transcription levels for the training data sets and one of the two 
testing data sets are syllables. This means that for each utterance, 
there is a syllable sequence associated with it. Speakers were 
required to speak each utterance following the prompt of the 
syllable sequence. The other testing data set, i.e. the specially 
designed task, was further divided into 2 subsets for each of both 
Mandarin and Taiwanese according to different transcription levels. 
For those data sets with character level transcription, only Chinese 
character sequences were provided to the speakers when they were 
recording the speech data. Without the syllable level transcriptions, 
speakers spontaneously utter each sentence as correct as their 
literacy can achieve. In the worst case, the code switching 
phenomenon may occur, which means speakers change the 
language during uttering a sentence.  Take the sentence as an 
example “  “. When pronounced in Mandarin, it 
will be /mei3 guo2 zong2 tong3 bu4 si1 sian1 sheng1/; if 
pronounced in so-called “standard” Taiwanese, it will be /bhi1 
gok2 zong1 tong4 bo4 hi2 sen2 sinn1/. In the Testing data 2, it 
would be very probably pronounced as / bhi1 gok2 zong1 tong4 
bu4 si1 sian1 sheng1/, where the first 4 syllables are Taiwanese 
and the last 4 syllables are Mandarin. 
 For convenience of reference in this paper, each of all data 
sets was given a name, e.g., M100-PA represents the Mandarin 
training data set of 100 speakers, designed as phonetically 
abundant. 
 
5.2 Experiment Setup 
HMM acoustic models: 
 
 HMM-based acoustic models for two regionalects, namely 
Mandarin (M) and Taiwanese (T), and the mixing bi-regionalect (B) 
are trained by HTK tool [10]; using training data set M100-PA, 
T100-PA and B100-PA, respectively. The acoustic units chosen 
here were right-context dependent mono-initials and tonal-finals. 
Since all the transcriptions are based on the Formosa Phonetic 
Alphabet (ForPA)[9], the speech data labeled as the same acoustic 
unit in both regionalects are shared to train the Gaussian mixtures 
for that unit. The HMM for each acoustic unit has 3 states and each 
state has 2 to 8 Gaussian mixtures dependent on the occurrence of 
the training data for that state. 
 
Pronunciation lexicons: 
 
 In this paper, each character in the lexicon contains not only 
the multiple pronunciations but also their associated probabilities. 
The probability model was estimated in two ways. One is based on 
the equally probable distribution of pronunciations for each 
character in the Formosa Lexicon. The other was based on the tonal 
syllable confusion matrix which was constructed from the 
recognition results of the testing data set B20-PA using a traditional 
4 
Contest-Independent/Context-Depent Pronunciation 
Modeling," In Proc.ICASSP, 2000 
[3]. Guoliang Zhang, Fang Zheng and Wenhu Wu, "A Two-Layer 
Lexical Tree Based Beam Search in Continuous Chinese 
Speech Recognition," In Proc. EuroSpeech, Denmark, 
September 2001 
[4]. Tan Lee, Wai Lau, Y. W. Wong and P.C. Ching, "Using tone 
Information In Cantonese Continuous Speech Recognition," 
ACM Transactions on Asian Language Information 
Processing, Vol. 1, pp. 83 - 102, 2002 
[5]. Dau-Cheng Lyu, et al., "Speaker Independent Acoustic 
Modeling for Large Vocabulary Bi-lingual 
Twaiwanse/Mandrain Continuous Speech Recognition," In 
Proc. SST 02, Melbourne, December 2002 
[6]. Dau-Cheng Lyu, et al, "Large Vocabulary Taiwanese 
(Min-nan) Speech Recognition Using Tone Features and 
Statistical Pronunciation Modeling" In Proc. EuroSpeech, 
Switzerland, 2003. 
[7]. Mirjam Wester, "Pronunciation Modeling for 
ASR-knowledge-based and Data-drived Methods," Journal of 
Computer Speech and Language 17(2003), pp. 69-85, 2003 
[8]. Liu, Yi and Pascale Fung, “Partial change accent models for 
accented Mandarin speech recognition.” In Proceedings of the 
IEEE Workshop on Automatic Speech Recognition and 
Understanding, St. Thomas, U.S. Virgin Islands, December, 
2003. 
[9]. Liang M.S., R.Y. Lyu, Y.C. Chiang "An efficient algorithm to 
select phonetically balanced scripts for constructing corpus" 
NLP-KE, Beijing 2003 
[10]. Steve Yang et al. Hidden Markov Model Toolkit V3.1, 
Cambridge University Engineering Department, 2002 
 
Figure 1: the decoding problem of Chinese speech recognition 
 
Figure 2: a 3-layer grammar searching net for syllable decoding 
 
Figure 3: the syllable-to-character converter 
 
Figure 4: one-stage searching strategy for Chinese speech 
recognition 
 
Figure 5: a Unified 3-layer framework for multi-regionalect 
Chinese speech recognition 
 
Figure 6: a Free syllable net 
 
Testing 
Data 
M10-PA 
M10-ST 
T10-PA 
T10-ST 
B20-PA  
B10-ST 
AM M B T B B 
FSN M408 M408 T709 T709 B925 
Perplexity 408 408 709 709 925 
SER [%] 37.1 
60.3 
36.9 
58.6 
37.9 
77.3 
36.9 
75.2 
39.8 
70.8 
Table 2: Syllable Error Rate(SER) [%] Results for Free Syllable 
Net(FSN) Decoding, using different Acoustic Model (AM)  
 
Testing data B20-PA B10-ST M10-ST T10-ST B10-ST-Syl B10-ST-Chr 
AM B B B B B 
Lexicon 
Tree 
B30k B6446 M3223 T3223 B6446 
Voc. Size 30k 6446 3223 3223 6446 
SER[%]of 
Two-Stage 
16.1 24.6 5.6 37.1 26.9 22.2 
CER[%]of 
Two-Stage 
23.9 21.7 5.6 40.7 20.6 22.7 
CER[%]of 
Unified 
22.6 14.3 5.0 34.8 13.5 15.1 
CER-R[%] 5.9 34.1 10.7 14.5 34.5 33.4 
Table 3: Comparison of recognition performance for the two-stage 
and the unified framework in Syllable Error Rate (SER), Character 
Error Rate (CER) and CER reduction (CER-R) from the two-stage 
to the unified framework. 
 
 Training Data 
Phonetically Abundant 
Testing Data 1  
Phonetically Abundant 
Testing Data2 
 Special Task (Tang Poems) 
Regionalect Mandarin Taiwanese Mandarin Taiwanese Mandarin Taiwanese Mandarin Taiwanese 
 No. of Speakers 100 100 10 10 10 10 
No. of  Utterances 43078 46086 1000 1000 250 250 250 250 
No. of  Hours 11.3 11.2 0.28 0.28 0.14 0.14 0.13 0.14 
No. of  Syllables  
per Utterance 
2.7 
 
1.9 
 
2.5 2.6 5.9 5.9 5.9 5.9 
Transcription Level Syllable Syllable Syllable Syllable Syllable Syllable Character Character 
Speaking Style Read Read Read Read Read Read Spontaneous Spontaneous 
M100-PA T100-PA M10-PA T10-PA M10-ST-Syl T10-ST-Syl M10-ST-Chr T10-ST-Chr Names of data sets for 
reference in this paper B100-PA B20-PA B10-ST-Syl B10-ST-Chr 
Table 1: Information about the speech corpus 
 due to the construction of bilingual lexicon, this work 
becomes easier. In the following paragraphs, we will 
describe text analysis in detail. 
 
2.1. The Formosa Phonetic Alphabet (ForPA) 
 
The Mandarin Phonetic Alphabet (MPA, also called 
Zhu-in-fu-hao) and Pinyin (Han-yu-pin-yin) are the 
most widely known phonetic symbol sets to transcribe 
Mandarin Chinese. They have been officially used in 
Taiwan and Mainland China respectively for a long 
time. However, both two systems are designed only for 
Mandarin. It’s necessary to design a more suitable 
phoneme set to begin with multilingual speech data 
collection and labeling. An example of ForPA is listed 
in <Table 1> [7]. 
 
2.2. Word Segmentation and Mandarin-
Taiwanese transcription (Sentence-to-word) 
 
Since there is no natural boundary between two 
successive words, we must segment text into word 
sequence first. We use Mandarin-Taiwanese bi-lingual 
lexicons for text analysis. Each item in the lexicons 
contains a Chinese character string, which is 
transcribed into Mandarin with Formosa Phonetic 
Alphabet (ForPA). There is at least a Taiwanese word 
corresponding to a Mandarin word [7]. Every word in 
Taiwanese has at least two pronunciations, containing 
literature (classic) and oral pronunciations. The 
statistics of Mandarin-Taiwanese lexicon is shown as 
<table.2>. 
We use the bilingual pronunciation dictionary as the 
knowledge source and then apply a word segmentation 
algorithm based on the sequentially maximal-length 
matching in the lexicon. 
 
2.3. Labeling (morpheme-to-phoneme) 
 
For each segmented word, there may exist not only 
one pronunciation. To deal with the multiple-
pronunciation problem, two strategies are adopted. One 
is the oral pronunciation has priority for transcription. 
Another is that build a network with pronunciation 
frequencies as node information and pronunciation 
transitional frequencies as arc information has been 
constructed for each sentence. The best pronunciation 
is then conducted by Viterbi search. 
 
2.4. Normalization of the digit sequences 
 
Another important issue for text analysis is the 
normalization of the digit sequences. In fact, each of 
almost Taiwanese single-syllabic words has 2 distinct 
manners of pronunciation: one for classic literature like 
poems, and the other for oral expression in daily lives. 
However, for digits, these 2 manners of pronunciation 
exist in daily lives. The manner of pronunciation 
depends on the position of the digit in a sequence, 
which can be summarized in rules. In addition, if a 
digit sequence does not represent a quantity, it is 
pronounced digit by digit as the classic pronunciation. 
 
3. Prosody analysis module 
 
Like Mandarin, Taiwanese is a tonal language. 
Traditionally speaking, it has seven lexical tones, two 
of which are carried in syllables ended with stop 
vowels, such as /ak/ and /ah/ (called entering-tone 
traditionally) and the other five are carried in those 
without stop-vowels (called non-entering tone 
traditionally). Let’s define the number 1 to 7 to encode 
the 7 Taiwanese tones as follows: “1” High-Level 
（ like ） , “2” Mid- 1 Level （ like ） , “3” Low-
Falling （like ）, “4” High-Falling （like ）, “5” 
Mid-Rising（like ）, “6” High-Stop（like ）, “7” 
Mid-Stop（like ）.An example of these 7 tones with 
one corresponding Chinese character for each tone is 
shown in <table.3>. Some phonetic/acoustic 
characteristics, including contour of fundamental 
frequency (F0), the description of relative frequency 
level (RF), and the proposed tone-to-digit (TD) 
mapping are also shown. In this table, one can also find 
2 additional tones, namely “8” Low-Stop and “9” 
High-Rising, which are necessary for tone-sandhi issue 
discussed in next paragraph.  
The tone sandhi issue is relatively complex in 
Taiwanese. Every Taiwanese syllable has 2 kinds of 
tones called the lexical-tone and the sandhi-tone 
depending on the position it appears in a word or a 
sentence. One of the most frequently referred sandhi 
rules says that , for most cases, if a syllable appears at 
the end of a sentence, or at the end of a word, then it is 
pronounced as its lexical tone, otherwise, it is 
pronounced as its sandhi tone[2]. The sandhi rules for 
each lexical tone is as follows: 
(1) tone “1” will change to tone “2”; 
(2) tone “2” will change to tone “3”;  
(3) tone “3” will change to tone “4”; 
(4) tone “4” will change back to tone “1”;  
(5) tone “5” may change to tone “2” or tone “3” for 
two different major sub-dialects; 
(6) tone “6” will change to tone “8”; 
(7) tone “7” will change to tone “6”. 
The above is summarized in <fig.2>, which is called 
the “tone sandhi sailboat”. 
 alphabet learning tool, which consists of Taiwanese 
TTS component. When we type various kinds of 
existing Taiwanese syllables in ForPA, the tool will 
pronounce simultaneously. By the tool we can learn a 
new language phonetic system more quickly. The <fig 
6> shows the interface of interactive phonetic system 
learning tool. 
 
7. Conclusion 
 
As shown in <fig.4>, we have successfully 
constructed a Taiwanese TTS system from bi-lingual 
for contextual learning. Hence, the most Mandarin 
article can be transcribed into Taiwanese and automatic 
generation of a speech signal in Taiwanese. This is 
great helpful for those who want to learn mother-
tongue language in Taiwan as shown <fig 5><fig 6>.  
However, there are still a lot to do. In the future, we 
should improve tone-sandhi for more accurate speech 
synthesis. In the other hand, it is imperative to use 
signal processing techniques to smooth the waveform 
to reduce discontinuity in our future TTS system. 
 
8. Reference 
 
[1] Walsh, P., J. Meade., “Speech enabled e-learning for 
adult literacy tutoring”, The 3rd IEEE International 
Conference on Advanced Learning Technologies, 9-11 
July 2003, Page(s): 17 -21. 
[2] Ren-yuan Lyu, Zhen-hong Fu, Yuang-chin Chiang, Hui-
mei Liu, “A Taiwanese (Min-nan) Text-to-Speech (TTS) 
System Based on Automatically Generated Synthetic 
Units”, ICSLP2000, Oct. 2000 
[4] Ren-yuan Lyu, Chi-yu Chen, Yuang-chin Chiang, Min-
shung Liang, “A Bi-lingual Mandarin/Taiwanese(Min-
nan), Large Vocabulary, Continuous Speech Recognition 
System Based on the Tong-yong Phonetic Alphabet 
(TYPA)”, ICSLP2000, Oct. 2000, Beijing, China 
[5] Yuang-chin Chiang, Zhi-siang Yang, Ren-yuan Lyu, 
“TAIWANESE CORPUS COLLECTION VIA CONTINUOUS 
SPEECH RECOGNITION TOOL”, ICSLP2000, Oct. 2000, 
Beijing, China 
[6] Dau-cheng Lyu, Min-siong Liang, Yuang-chin Chiang, 
Chun-nan Hsu, Ren-yuan Lyu,”Large Vocabulary 
Taiwanese (Min-nan) Speech Recognition Using Tone 
Feature and Statistical Pronunciation Modeling” , 
Proceedings of 8th European Conference on Speech 
Communication and Technology (EuroSpeech 2003), Sep 
1-4, 2003, Geneva, Switzerlan 
[7] Min-siong Liang, Ren-yuan Lyu, Yuang-chin Chiang “An 
Efficient Algorithm to Select Phonetically Balanced 
Scripts for Constructing A speech Corpus”, Proceedings 
of IEEE International Conference on Natural Language 
Processing and Knowledge Engineering (IEEE-NLPKE 
2003), October 26-29, 2003, Beijing, China 
[8] Donovan, R. E. and P. C. Woodland, “A hidden markov-
model-based trainable speech synthesizer”, Comp. 
Speech  & Lang., 1999. 
[9]Yuang-chin Chiang, Ren-zyun Chen, Ming-jie Tian, Ren-
yuan Lyu, “DIMSU: A Speech Database with Pitch 
Marks”, Proceedings of Oriental COCOSDA 2003, Oct 
1-2, 2003, Sentosa, Singapore 
[10] Fu-chiang Chou, Chiu-yu Tseng, “Corpus-based 
Mandarin Speech Synthesis with Contextual Syllabic 
Units Based on Phonetic Properties”, ICASSP98 
 
Tables and Figures 
<fig.2> The Taiwanese tone sandhi rules 
 
<fig.3> The coverage rate of 200 sentences to the total 
sentences with respect to distinct Chinese character 
 
 
<fig 4> The interface of Taiwanese TTS system  
 
1 2 3 4
5
-p,-t,-k -h 
6 7 
 
8 6 
6 7
3 6
 Computational Linguistics and Chinese Language Processing 
Vol. 9, No. 2 , August 2004, pp. 1-12                                                                          1 
© The Association for Computational Linguistics and Chinese Language Processing 
Toward Constructing A Multilingual Speech Corpus for 
Taiwanese (Min-nan), Hakka, and Mandarin 
Ren-yuan Lyu*, Min-siong Liang+, Yuang-chin Chiang**   
Abstract 
The Formosa speech database (ForSDat) is a multilingual speech corpus collected 
at Chang Gung University and sponsored by the National Science Council of 
Taiwan. It is expected that a multilingual speech corpus will be collected, covering 
the three most frequently used languages in Taiwan: Taiwanese (Min-nan), Hakka, 
and Mandarin. This 3-year project has the goal of collecting a phonetically 
abundant speech corpus of more than 1,800 speakers and hundreds of hours of 
speech. Recently, the first version of this corpus containing speech of 600 speakers 
of Taiwanese and Mandarin was finished and is ready to be released. It contains 
about 49 hours of speech and 247,000 utterances. 
Keywords: Phonetic Alphabet, Pronunciation Lexicon, Phonetically Balanced 
Word, Speech Corpus 
1. Introduction 
To design a speaker independent speech recognition system, it is essential to collect a large-
scale speech database. Taiwan (also called Formosa historically), which has become famous 
for its IT industry, is basically a multilingual society. People living in Taiwan usually speak at 
least two of the three major languages, including Taiwanese (also called Min-nan in the 
linguistics literature), Hakka and Mandarin, which are all members of the Chinese language 
family. In the past several decades, most of the researchers studying natural language 
processing, speech recognition and speech synthesis in Taiwan have devoted themselves to 
research on Mandarin speech. Several speech corpora of Mandarin speech have, thus, been 
collected and distributed [Wang et al., 2000; Godfrey, 1994]. However, little has been done 
                                                        
* Dept. of Computer Science and Information Engineering, Chang Gung University, Taoyuan, Taiwan 
Email: rylyu@mail.cgu.edu.tw                Tel: 886-3-2118800ext5967, 5709 
+ Dept. of Electrical Engineering, ,Chang Gung University, Taoyuan, Taiwan 
** Inst. of Statistics, National Tsing Hua University, Hsin-chu, Taiwan 
  
Toward Constructing A Multilingual Speech Corpus for                         3 
Taiwanese (Min-nan), Hakka, and Mandarin 
years. However, both systems are inadequate for application to the other members of the 
Chinese language family, like Taiwanese (Min-nan) and Hakka. Among the phonetic systems 
useful for Taiwanese and Hakka, there are Church Romanized Writing (CR, also call Peh-e-ji, 
「白話字」) [Chiung 2001] for Taiwanese and the Taiwan Language Phonetic Alphabet 
(TLPA) [Ang 2002] for Taiwanese and Hakka. Because the same phonemes are represented 
using different symbols in Pinyin, CR and TLPA, it is confusing to learn these phonetic 
systems simultaneously. For example, the syllable “pa(八)” in TLPA and “pa(趴)” in CR may 
be confused with each other because the phoneme /p/ is pronounced differently in the two 
systems. 
Therefore, it is necessary to design a more suitable phoneme set for multilingual speech 
data collection and labeling [Zu, 2002][Lyu, 2000]. The whole phone set for the three major 
languages used in Taiwan is listed in Table 1 for four phonetic systems: MPA, Pinyin, IPA, 
and the newly proposed ForPA. Table 1 also lists examples of syllables and characters which 
contain the target phonemes. 
It is known that phonemes can be defined in many different ways, depending on the level 
of detail desired. The labeling philosophy adopted in ForPA is that when faced with various 
choices, we prefer not to divide a phoneme into distinct allophones, except in cases where the 
sound is clearly different to the ear or the spectrogram is clearly different to the eye. Since 
labeling is often performed by engineering students and researchers (as opposed to 
professional phoneticians), it is generally safer to keep the number of units as small as 
possible, assuming that the recognizer will be able to learn any finer distinctions that might 
exist within any context. Generally speaking, ForPA might be considered as a subset of IPA, 
but it is more suitable for application to the languages used in Taiwan. 
Table 1. The phone set for the three languages in Taiwan, represented as different 
phonetic systems. The Chinese character in parentheses followed by a 
syllable, is an example character used in Mandarin, e.g., “ba(八 )” is 
pronounced in Mandarin as syllable “ba”, without considering the tone. 
For phonemes not found in Mandarin Chinese, we use Chinese character 
pronounced as Taiwanese (T) or Hakka (H) to be example characters. For 
example, “bha(肉T)” meaning “肉” is pronounced “bha” in Taiwanese. 
 
 
  
Toward Constructing A Multilingual Speech Corpus for                         5 
Taiwanese (Min-nan), Hakka, and Mandarin 
3. The process of producing phonetically balanced word sheets 
Based on the three pronunciation lexicons transcribed in ForPA, we extracted sets of distinct 
syllables and inter-syllabic bi-phones from the three languages. The statistics of the phonetic 
units considered here are listed in Table 3. In order to collect speech data related to the co-
articulation effect of continuous speech, we extracted phonetically abundant word sets. 
Therefore, the chosen phonetic units were not only base-syllables, phones, and RCD phones, 
but also Initial-Finals, RCD Initial-Finals and inter-syllabic RCD phones. The process of 
selecting such a word set is actually a set-covering optimization problem [Shen et al., 1999], 
which is NP-hard. Here, we adopted a simple greedy heuristic approximate solution [Cormen, 
2001]. 
First, we set the requirements of the word set as to cover the following phonetic units: 
Base-syllables and Inter-syllabic RCD phones. Accordingly, the selected word set could cover 
all the phones, Initial-Finals, RCD phones, RCD Initial-Finals, Base-syllables and Inter-
syllabic RCD phones. In this way, we could obtain several sets of words for our balance-word 
data sheets. All the statistics of the phonetic units considered here are listed in Table 3. [Liang 
2003]. 
 
Table 3. The numbers of distinct subwod units for each of the three languages and 
their unions, where T: Taiwanese; H: Miaulik-Hakka M: Mandarin; ∪: 
union. 
Language Base syllable Phones Within-syllabic bi-phones Inter-syllabic bi-phones 
T 832 53 410 716 
H 683 53 327 696 
M 429 45 208 234 
T∪H 1134 70 583 1036 
T∪M 1055 64 486 809 
H∪M 939 71 435 797 
T∪H∪M 1326 78 600 1105 
3.1. Data sheets 
The process of producing data sheets is depicted in Fig.2. Before we produced the data sheets, 
we defined the sheets’ coverage rate. The coverage rate of the sheets was defined as the total 
number of base-syllables (or inter-syllabic phones) over the number of all possible distinct 
base-syllables (or inter-syllabic phones). The format of the data sheet is partially shown in 
Table 4. 
 
 
  
Toward Constructing A Multilingual Speech Corpus for                         7 
Taiwanese (Min-nan), Hakka, and Mandarin 
4.1. The telephone recording system 
The telephone system is set up in the Multi-media Signal Process Laboratory at Chang Gung 
University. The speakers dial into the laboratory using a handset telephone. Before recording, 
we give the speakers prompt sheets. The input signal is in format of 8K sampling rate with 8-
bits µ-law compression. The speakers utter words while reading the prompt sheet, and 
supervised prompt speech is played to help the speakers follow the prompt speech to finish the 
recording. After recording, all speech data are saved in a unique directory. Figure 3 shows the 
recording process carried out using the telephone system. 
Welcome words 
Input # of prompt 
Serial number 
 
4.2. The microphone recording system 
When we record a waveform into a computer, it is not convenient to type the file name 
necessary for saving it. Therefore, we use a good tool (DQS3.1) [Chiang 2002] to record 
speech. If we create a script in a specific form for this software, we can record the waveform 
easily and get a labeled file, which contains information of transcription using ForPA. Then, 
we simply set up the system on a notebook computer and take it wherever we want to record 
speech. 
5. Speaker recruiting 
We employ several part-time assistants to recruit speakers around Taiwan. Each speaker is 
Press 1 to listen to the next 
prompting speech 
Prompting speech 
Recording speech 
Finish recording 
Given by 
system 
Press 3 to listen to the 
prompting speech 
Press 1 to save the recorded waveform 
Thank you, bye bye!
Figure 3. The telephone recording system. 
 
  
Toward Constructing A Multilingual Speech Corpus for                         9 
Taiwanese (Min-nan), Hakka, and Mandarin 
“M0” means that the recording channel used was a microphone and gender was female, and so 
on. Every speaker has a unique serial number and speech data, which contain a transcription 
of waveforms made in the early stage and are stored in a unique folder named according to the 
serial number. The database structure is shown in Fig.5. All the statistics of the database are 
listed in Table 5. 
 
 
 
 
 
 
 
 
 
 
 
Figure 5. The structure of database for Taiwanese and Mandarin. (TW01: 
Taiwanese database collected in 2001; M0: the microphone channel 
was used and the gender was female, T1: the telephone channel was 
used and the gender was male; and so on. There is a transcription file 
for each unique speaker.) 
 
Table 5. The statistics of utterances, speakers and data length for speech 
collected over microphone and telephone channels in Taiwanese 
and Mandarin (MIC: microphone; TEL: telephone). 
Name Channel Gender Quantity Train(hr) Test (hr) 
TW01-M0 Female 50 5.92 0.29 
TW01-M1 Male 50 5.44  
MD01-M0 Female 50 5.65 0.27 
MD01-M1 Male 50 5.42  
TW02-M0 Female 233 10.10 0.70 
TW02-M1 
MIC 
Male 277 11.66  
TW02-T0 Female 580 29.21 0.95 
ForSDAT 
TW02-T1 
TEL 
Male 412 19.37  
 
 
ForSDAT 
TW01
MD01
TW02
Waveforms 
list
TCP …
.. 
.. 
TW01M01234
M0
M1
M0
M1
M1
M0
 
  
Toward Constructing A Multilingual Speech Corpus for                         11 
Taiwanese (Min-nan), Hakka, and Mandarin 
inturns in one work sheet, etc. These directories are also considered unusable. 
7.2. Step 2: phonetic transcription by means of forced alignment 
After the speech data is pre-processed, we validate it to determine whether the labels that 
consist of phonetic transcriptions correspond to the speech data. We use two methods to 
achieve this goal. First, we use HTK [Steven, 2002] to perform forced-alignment 
automatically on an utterance using all possible syllable combinations. We keep the highest 
scores for combinations to transcribe the speech. Secondly, we use the TTS (text-to-speech) 
technique to synthesize all the labels that were transcribed using HTK and then we transcribe 
the speech manually using more appropriate phonetic symbols. Finally, we can construct a 
relational database using ACCESS to record all the profiles of the speakers (see Fig.3) and 
what they recorded. Therefore, we can query the speech database using the SQL language to 
find the waveforms transcribed using the specific phones or syllables or even query who 
recorded the specific-phone waveforms. This step is on-going and will be finished soon. 
8. Conclusion 
Version 1.0 of this corpus containing the speech of 600 speakers of Taiwanese (Min-nan) and 
Mandarin Chinese has been finished and is ready to be released. We have collected the speech 
of 1,773 people, including 49.47 hours of speech and 247,027 utterances. As work on this 
project continues, more Hakka and Mandarin speech data will be collected. 
References 
Wang, H. C., F. Seide, C.Y. Tseng and L.S. Lee, “Mat-2000 – design, collection, and 
validation of a mandarin 2,000-speaker telephone speech database,” International 
Conference on Spoken Language Processing 2000, Beijing, China, 2000. 
Godfrey, J., “Polyphone: Second anniversary report,” International Committee for Co-
ordination and Standardisation of Speech Databases Workshop 94, Yokohama, Japan, 
1994. 
Zu, Y., “A super phonetic system and multi-dialect Chinese speech corpus for speech 
recognition,” International Conference on Spoken Language Processing 2002, Denver, 
USA, 2002. 
Lyu, R. Y., “A bi-lingual Mandarin/Taiwanese (Min-nan), Large Vocabulary, Continuous 
speech recognition system based on the Tong-yong phonetic alphabet (TYPA),” 
International Conference on Spoken Language Processing 2000, Beijing, China, 2000. 
 
 1
會議中文名稱：2007 IEEE 聲學、語音及訊號處理國際會議 (ICASSP-2007) 
參加會議人：呂仁園，長庚大學資訊系副教授 
 
會議英文名稱：2007 IEEE International Conference on Acoustics, Speech, and Signal Processing 
會議期間：April 15-20, 2007 
會議地點： Hawaii Convention Center, Honolulu, Hawaii, U.S.A. 
 
 
會議概況： 
 
  IEEE 聲學、語音及訊號處理國際會議 (2007 IEEE International Conference on Acoustics, 
Speech, and Signal Processing) 簡稱 IEEE-ICASSP，或 ICASSP。這個會議成立至今超過 30 年，
今年是第 32 屆。它是電機資訊學門中，訊號處理(Signal Processing)領域裡，規模最大，也是
公認最重要的國際學術會議。今年(2007)，大會總計收到來自全球共 2844 篇的投稿論文，其
中，1272 篇論文為大會所接受，論文接受率約為 44% (1272/2844)。 
 
  在總數 2844 篇的投稿中，依照次領域(Subfield)又可分為 14 個類別，這些類別依投稿論
文數之多寡排序列表如下（表一）： 
 Subfield Category No. of 
Submission 
Percentage % 
1 Speech Processing  573 20.1% 
2 Image & Multi-dimensional Signal Processing 465 16.4% 
3 Signal Processing Theory and Methods 443 15.6% 
4 Signal Processing for Communications 382 13.4% 
5 Sensor Array & Multi-channel Signal Processing 166 5.8% 
6 Audio & Electro-acoustics 161 5.7% 
7 Machine Learning for Signal Processing 141 5.0% 
8 Spoken Language Processing 99 3.5% 
9 Multimedia Signal Processing 97 3.4% 
10 Information Forensic Security 94 3.3% 
11 Bio Imaging and Signal Processing 91 3.2% 
12 Design & Implementation of SP Systems 69 2.4% 
13 Industry Technology Track 52 1.8% 
14 Signal Processing Education 11 0.4% 
 All 2844 100.0% 
（表一）2007 IEEE-ICASSP 投稿論文數統計，依論文數多寡排序。 
 
 3
抵達了夏威夷檀香山國際機場。 
   
「台灣教授及學生：添烜、良基、家德」 
   
  下了飛機等候提取行李時，又遇到了來自台灣的同行張添烜，同時還遇見了台大的陳良
基教授，陳教授是我的老師輩份，我向其打聲招呼，見他正與張添烜面授機宜，交代這次來
開會該如何蒐集會場資訊，以便來年(2009)此會議於台灣主辦時，可以提供有效的建議。此
外還遇到一位清華大學的博士生廖家德，有點靦顏，與當年我自己當博士生時出國的情境略
同。 
   
   
「台裔美國人導遊：許之，加長型禮賓車」 
   
  取得行李，走出機場，正欲研究如何搭乘交通工具前往旅館，這時，有位華人，拿著個
「華航精緻旅遊」的牌子在招呼旅客，我們便走上前去，他表明是來接機的，我們便讓他接
去，他開著一部白色加長型禮賓車，這種車我在台灣大概只看過一兩次，印象中就是像「唐
日榮」、「黃任中」那種既超級有錢，又愛炫耀，想藉機證明其很有「身份地位」，以便引來羨
慕眼光者才會買一輛在台灣街頭開來開去的車。結果，我們這三位（我、張添烜還有家德）
來自台灣的窮教授和窮學生，就這麼大剌剌的坐上這台白色加長型禮賓車。這位接待我們的
年輕華人也來自台灣，名叫「許之」，他告訴我們，他來的時候時間還太早，花店未開門，他
現在要先載我們到花店，拿個花圈給我們套上。哈哈哈，真有趣，不但有加長型禮賓車來相
迎，還有新鮮的花圈可帶在脖子上。拿了花圈後，許之帶我們去珍珠港參觀，其實我有點累
了，很想盡快回旅館安頓，但許之告訴我們一則旅館尚無法 check-in，再則帶我們市區半日
遊是他的責任，我看另兩位同行不置可否，便隨了他去，隨遇而安，跟著他們去了珍珠港，
一起瀏覽這個二次大戰時，美軍損失慘重之歷史地點。到達景點之後，許之停了車，我們三
位「遊客」走出車門，我赫然發現，停車場裡停了數十輛我坐的那種加長型禮賓車，其中數
輛還一字排開，還真的像一列火車車廂，真是壯觀，我對這個景象的興趣遠大於進去看那個
歷史古蹟珍珠港！便建議另兩位同行趕快「拍照留念」（我自己的相機還在行李中，尚未取出。） 
   
「珍珠港老兵」 
   
  許之引我們進了珍珠港內，還一邊講珍珠港的歷史，他還真是個好導遊，很會講歷史典
故，我便很專心的聽他講古，並適時回應之。我們在港內的博物館參觀了一下，無啥特別令
我留下印象之事物，走出館外，倒有一事令我感到趣味，那就是有位當時參與戰役的老兵，
看來應已經七、八十歲，就坐在館外，與人簽名照相（沒在收錢），我趨前向其致意，並讚他
勇敢，我以他為榮，並伸出手握他的手。豈知，他的手一碰到我的手，臉上立即露出痛苦表
情，似乎我握的太用力，把他的手握痛了！（可是我沒用什麼力啊！）我連忙再三道歉，有
點擔心惹上麻煩，這時，他卻大笑不已，我才知道他是開玩笑的，我上當了，四周圍的人通
通哈哈大笑起來，我也跟著大笑起來。 
   
   
 5
歡這種形式的演講，可以與來參觀壁報的同行學者，作面對面的交談，常常可以談得很深入，
並結交許多朋友。這個旅館有個陽台，視野很好，連彩虹都看的到。我就在陽台上邊看演講
資料，邊吃早餐，沒幾分鐘就引來了一大群鳥與我共進早餐，甚是有趣，我把達賴喇嘛請出
來，鳥兒連喇嘛的頭都爬上去了。 
   
  
   
「印度人：Venkat」 
 
  我在中午時分抵達會場，直接來到貼壁報的位置，由於前一位壁報講員尚未完成其壁報
的拆卸，我便與他交談起來，他名叫 Venkat，是個印度人，為人友善，我告訴他我對他作的
以 PDA 為平台的系統感到趣味，他就興致勃勃的為我解說之，又讓我親手操作該系統，經過
約 10 分鐘的互動，我們互道感謝及再見，他離開後，我則開始動手貼我的壁報了。 
   
「我的演講及聽眾：西班牙女孩 Alicia、、、、等等」 
 
  第一位來我壁報旁邊聽我解說的是一位西班牙籍的女孩，名叫 Alicia，我向她解釋我的
研究問題、所採取的方法以及所獲致的成果，她聽得興味盎然，並直說她自己的研究問題與
此非常相關，我的研究經驗將帶給她很好的借鏡；在往後的二小時裡，不斷有各國學者來到
面前聽我講解，其中也包含幾位本領域具國際性知名度的學者，像是日本籍的 Sadaoki Furui
教授等人。由於我全心投入壁報講解當中，自信又熱情的態度吸引了周圍的聽者。最後，甚
至欲罷不能，拖延到 4:00 以後，還有聽眾對我的壁報演講感興趣，而且，在我隔壁的 Alicia
小姐，還舉起她的相機幫我拍下我與最後一位聽眾的互動討論情形，真是令我十分感動，我
也從中獲得很大的成就感以及欣慰感。 
 
  
 
「一群中國人」 
 
 由我出場的任務圓滿達成之後，我便稍事休息，吃吃喝喝一下，大會提供很多水果，香
蕉、蘋果、各種瓜果，都是我喜歡的，便盡情的享用，其間又認識幾位來自中國或由中國出
去到美國求學、工作的朋友，一群華人在一起，便自然說起了華語，他們稱之為普通話，（只
要不扯上政治，）大家都相談甚歡。 
 
「微軟亞洲研究院副院長：洪小文」 
 
  傍晚時分，我準備離開會場，迎面走來一位熟面孔，是洪小文(Hsiao-wen Hon)，大名鼎
鼎的微軟亞洲研究院副院長，我直接向他打招呼：「嗨，小文你好！」他隨即認出是我，並很
親切的回應我，並詢問我的近況，我向他說近來特別對音樂訊號處理感興趣，他當場發表他
的高見，認為這個領域很有趣，他也在注意其中的一些問題，在得知我特別注意「音樂轉譯」
(Music Transcription)主題之後，他甚至提供建議說明這個主題該如何如何做、、、。真不愧
 7
  演講前，我發覺演講員 Furui 教授正獨自一人坐在前排座位上，我走向前去向其道聲早
安：「Furui San, O-Ha-Yo-Go-Za-Yi-Ma-Su!」，他也向我回聲：「O-Ha-Yo-Go-Za-Yi-Ma-Su!」，
我便回座。 
   
  時間到了，主持人開始介紹演講員，接著，Furui 教授便開始暢談 50 年來的語音識別技
術之演進，他把技術演進分為「史前，G0」、「第 1 代，G1」、「第 2 代，G2」、「第 3 代，G3」、
「第 3.5 代，G3.5」（目前）、以及即將到來的「第 4 代，G4」，其中各代起迄年份可以表示如
下： 
   
  G0< 1968<G1<1980<G2< 1990<G3<2000<G3.5< 2007<G4， 
   
  會後，我碰到 Furui 教授，我開玩笑的問他：「您把 1968 年訂為 G1 的起始年，是否是因
為您大學畢業於該年，並在該年正式投入語音科技的研發工作？」個性開朗隨和的 Furui 教
授大笑，回答說：「真是巧合，那年語音技術領域有個重要事件，原來是我自己入了行，哈哈
哈！」，我與 Furui 教授有過數面之緣，彼此算是舊識，我告訴他我暑假應會到日本去開另一
個會議，他則誠摯邀請我一定要順路拜訪他。我則對其邀請表達衷心之感謝。 
   
   
「芬蘭教授：Anssi Klapuri」 
   
  4 月 18 日，早上 9:50 開始，我去參加（聆聽）了一項口頭技術會議(Oral Technical Session)，
主題是關於音樂分析、分離以及轉譯的技術報告(Music Analysis, Separation and Transcription)。 
 
  我抵達會場時，第一篇論文報告已經接近尾聲，我就不去管他，先大致瀏覽第二篇論文
的題目到底是啥，原來題目是「經由『聲源─濾波器─衰減模型』之樂器聲音的分析」(“Analysis 
of Music Instrument Sounds by Source-Filter-Decay Model ”)，目光繼續瞄下去，赫然發現作者
是安西‧克拉普理(Anssi Klapuri)，來自芬蘭貪配爾科技大學(Tampere University of Technology, 
Finland)，克拉普理博士是音樂訊號資訊處理的大人物，年紀輕輕（比我年輕的機率很大）就
已是這個學術領域中的頭面人物，編著有「音樂轉譯的訊號處理方法」 (“Signal Processing 
Methods for Music Transcription”)一書，於 2006 年由史普玲格(Springer)出版公司所出版，我在
2006 年到法國參加 ICASSP2006 會議時，在大會買了這本書，回來後一直要好好詳讀一番，
只是將近一年來，仍然是東忙西盲(Busy & Blind)，沒能真正靜心拜讀之，真是可惜、、、。
閒話少說，言歸正傳，克拉普理先生在演講中分享了他的研究心得，主要是關於一項「音色
模型」(Timbre model)，「音色」這個觀念在聲音訊號中一直是最難描述的清楚，也最不易量
化的概念，我們可以說聲音的音量「很大或很小」，也可以說聲音的音高「很高或很低」，「音
量」(Volumn)及「音高」(Pitch)主客觀皆有描述之方，唯獨「音色」(voice quality or timbre)
這個概念，一直不甚了了，可能是因為其不易以單一維度的數量來量化之吧。克拉普理先生
當然也沒有在其演講中設法解答這個疑問，他只是籠統的說明傳統的訊號處理方法運用梅爾
譜頻(Mel-Frequency Cepstral Coefficients, MFCC)來表示「音色」這個包含在聲音中的概念，
他認為有些缺點，他於演講中列舉了那些缺點，其中最令人印象深刻的是，他認為梅爾譜頻
欠缺聲音訊號中，「音高」的資訊，而據他的觀點，「音高」應該會影響「音色」，於是，他要
 9
分享之後，也熱情的邀約我到他的壁報前面，為我解說其研究成果，依稀記得他當時展示了
個系統給我看，那個系統能夠從流行音樂的音樂訊號中，擷取出歌者的歌聲主旋律，我當場
覺得很敬佩，但也有點自卑，覺得我們自己作的研究成果不如人家。2003 年當時的心態總是
認為，前來學術會議發表論文，好像是在與人「比高下」，功夫比人強，則欣喜、驕傲、睥睨；
功夫比人弱，則自嘆、自卑、抬不起頭！「哈哈哈」，正因如此，自己錯失了多少寶貴的時間、
機會，可以用來結交朋友、可以用來自我成長？！與後藤博士後來尚有數面之緣，一次是 2005
年的英國倫敦的 ISMIR2005 會議，另一次（似乎）是 2006 年法國的 ICASSP2006 會議，但
都只是點頭說哈囉，並未深談。今年這次會議，我想找機會真正與他有些互動，便仔細聆聽
當下這篇論文的發表。 
   
「日本研究生：Katsutoshi Itoyama」 
   
  這篇論文是由後藤博士的學生（或同事？）Katsutoshi Itoyama 作口頭發表，內容很有趣，
而且講員還作了系統的展示，講完之後，我決定這場演講不能再畏縮、退卻、沈默，因此便
舉手發問，我問了這樣的問題：「我對你演講一開始所展示的系統極感興趣，但我似乎沒有聽
清楚，請問那是你自己依你所提出的理論、方法所獨立創建出來，或者那是一個可購得的軟
體商品，僅為你今天的演講提供開場白，作為概念展示(Concept Proof)之用？」當然，我是用
英語來表達上述問題，近年來，我逐漸放下當眾使用英語表達意見的恐懼退縮，逐漸對使用
英語溝通感到自在，主要原因並非我的英語能力有什麼實質的提升（比如字彙量增加、發音
變好等等），而是我在心態上有了重大的醒悟！這些醒悟整理如下： 
   
1. 我使用英語，目的是為了「自我練習」、「與人溝通」、「結交（不懂中文，只懂英文的）朋
友」，不是為了「向人展示我的英語有多好」以便可以「自豪」、「睥睨」。 
2. 我用我會的英語，想到什麼就說什麼，不求完美（字彙準確、文法無誤），不惜「拐彎抹
角」。 
3. 放鬆心情，仔細聆聽對方，以「字詞」為單位，不求「全句」聽明白講明白，隨時向對方
詢問、確認所聽到，保持交談、互動。 
4. 放開心胸，勇敢使用「自己的英語」。再怎麼「破」，也是「『自己的』英語」！有些人可
能「樂於指正」別人的英語，隨他們去，尊重他們、感謝他們、但不必隨之起舞。當下，我
有「『自己的』英語」，我的目的是「自我練習」、「與人溝通」、「結交（不懂中文，只懂英文
的）朋友」！ 
5. 感同身受那些對己身英語能力尚未自在的朋友，對其付出更大的耐心、包容與讚美，但千
萬不要「指正」！在他的周遭，他早已不缺指正。 
   
  我對台上講員以英語提出問題，但他似乎無法當下理解我的問題，有點楞住了，這時，
他的指導教授後藤博士便站起來替他「解圍」，回答了我的問題，並引起會場中許多歡樂的氣
氛。這種情形很常發生，特別常發生在日本朋友身上，我印象中很多日本朋友一直對己身的
英語感到相當不自在，台上這位講員看來像是學生（研究生）模樣，他當下無法回答我的問
題，或許是無法即時理解我的英語，也或許是無法理解我發問的實質內容，也許是理解了，
但一時沒有答案，也可能有了答案但一時不知如何用英語回答出來！總之，他對我回了個「歉
意的笑」，但他指導教授後藤博士也適時替他解了圍。我並因此而與後藤博士作了當眾交談。 
 11
  If you happen to have a ticket to sell, I will be very happy to buy it. 
  Please contact me at email address xxx@yyy.zzz or tell the front desk. 
  」 
   
「台灣教授：李琳山；香港教授：Victor」 
   
 我每隔一小時前往櫃臺(the front desk)，問問那位可愛的櫃臺小姐是否有人來賣票，順便
就與她聊天。一直到下午 5:00，還是沒人來賣票，看來這場晚宴當真很吸引人，有票的人都
不想錯過。我漸漸不感到失望，開始覺得有另種快樂油然而生，這是因為我一直可以去與那
位可愛的櫃臺小姐聊天，她是一位印度姑娘，濃眉大眼，印象中，印度人的確樂於助人，樂
於散播歡樂。最後一次前往櫃臺時，恰好遇到一群講中文的朋友，大部分來自台灣，有幾位
來自香港，其中有我的老師輩份者，他們正在呼朋引伴，準備晚宴時圍坐一桌，我向他們表
達羨慕之意，恭喜他們有機會去參加晚宴。其中有一人我不識，我向其自我介紹並詢問對方
姓名，他介紹他自己姓魏，朋友們叫他 Victor。旁邊剛好站著我的恩師李教授，李教授隨口
說：「他可是大名鼎鼎喔！」我略感汗顏，居然不識如此「大名鼎鼎」之人，但轉念一想，天
底下有多少「大名鼎鼎」者為我所不識，隨即釋懷。與 Victor 閒聊，知道他的研究專業是「資
訊安全」(Information Security)，當年又是台大電機系畢業，算來是老學長，因為是恩師李教
授所介紹，晚了李教授幾屆，算來也高我十二、三屆，我便稱呼其為「師叔」。Victor 很高興
的開始打開話匣，說了幾個典故、笑話、軼事，我們便見識出此人「說故事」的功力，大家
便愛聽他說話。他穿短褲涼鞋，又留著一大撮鬍子，有一半以上是白的，便取笑我的鬍子是
小巫，他的才是大巫，我心中突然想起我的好友江永進教授以及 CMU 的 Alan Black 教授，
照 Victor 的說法，他們一個是「巨巫」、另一個則是「超級巨巫」，一想到此，便大笑了出來。
Victor 愈說愈起勁，幾乎是欲罷不能，其他朋友陸續散去，參加晚宴去了，Victor 還繼續對我
發表很多見解，直到會場人群漸漸少了，Victor 說要去倒水，因為講的口乾舌燥了。之後便
不見人影。我也打道回旅館，今晚我得轉換旅館，從 Hotel Ala Moana 轉到 Hotel Holiday Inn
去，因為華航的旅遊套裝票含了三晚 Hotel Holiday Inn 的住宿券之故，為此，我還得搭一次
（來夏威夷第一次搭乘的）計程車咧！ 
　  
   
  
「Victor 帶我，勇『闖』晚宴場」 
 
  走出會場回旅館的途中，又碰到 Victor 師叔，他也沒晚宴票，就突然提議：「我帶你去勇
『闖』晚宴，我也算夏威夷『地頭蛇』，當年在此留學數年，認識了很多人，那些人現在都在
當老闆，我們只要到了晚宴會場，說不定就可以入場。」就這樣，師叔姪倆就坐著師叔開的
車，勇『闖』晚宴場！ 
 
  Victor 首先送我到 Hotel Holiday Inn，幫我卸下行李，這過程花了些時間，卻幫我省下一
趟計程車資，隨後直奔晚宴會場，其間確實找了一下路而又耽擱了些時間。等到了目的地時，
又開始找停車場，結果是在目的地附近找不到停車場，最後在離目的地相當遠之處才停好車，
這時，時間已經約晚上 8:30 了，Victor 估計一下我們走路的速度，便判斷我們走到晚宴場時，
 13
自」去過，見識了一下，也算增長見聞。我只覺得今天遇到 Victor，與其交上朋友，算是一
種奇緣！ 
   
「威基基海灘(Waikiki Beach)」 
 
  4 月 19 日，我從新搬入的旅館 Hotel Holiday Inn 醒來，決定今天不去會場，想要獨自一
人前往著名的威基基海灘(Waikiki Beach)，享受一天夏威夷海灘風情。用過早餐後，隨即整裝
前往，穿著短褲、球鞋、帶著 2 支登山手杖，協助我更輕鬆的走路。突然想起達賴喇嘛的一
句有關於他 48 歲才開始學英語的自述，大意如下：「我的『破英語』協助我與人溝通的更好、、、
當我出錯時，大家都覺得好笑、輕鬆、快樂、、、」(“My POOR English help me communicate 
BETTER,…., when I made mistakes, it help people LAUGH, RELAX and HAPPY….)。望著我的
右腳，一條因著幼時小兒麻痺而現在穿著金屬支架的腳，我突然領悟，照著達賴喇嘛的話語，
造了一句：「我那隻穿著支架的右腳，協助我行動更方便！沒有了它，我將無法獨自一人去遨
遊山林、嬉戲海灘！(“My POOR right leg help me move BETTER, without it I am not able to 
climb mountains and walk in the seashore by myself.”)」 人生體會，又進一層！ 
   
「路人」 
   
  一路上我抱持愉悅的心情，臉上是保持笑著的，我盡可能對遇到的任何人出聲打招呼，
左一句 ”Hello”，右一聲 ”Aloha”；絕大多數人(超過 90%)會報以相同的善意和笑容，偶而會
遇到神色凝重的人，我自己的善意和笑容，有時也能感染之，使他們那凝重的神色，稍稍褪
去。 
   
   
「海灘老夫妻，年輕日本女孩」 
   
  就這樣，我自己來到了海灘，先是買一份雪花刨冰，是葡萄口味的，昨天認識的 Victor
特別提到他自己在台北已經好久好久沒能吃到「雪花刨冰」，就是那種一整塊冰塊，放在一台
刨冰機上，刨出像片片雪花那種又細又綿密的冰，台語稱之為「挫冰」。我帶了一份雪花刨冰，
四處找尋適合落腳地點，來到一處椰樹蔭下，樹下是一片草坪，鳥兒時而飛上飛下，真是一
個好落腳處，旁有一對老夫妻，已經在那裡享受日光浴很久，還有一群年輕的日本姑娘，身
材姣好，容貌可愛，正談笑中。我在他們之中坐了下來，開始卸除身上行頭，主要是那支重
重的鐵製支架，這「鐵老兄」協助我千里行步，此時應給他一個休息處，裹在鐵老兄之下的
「我的右腳」，此時也可以卸除武裝，盡情享受這晚春初夏時節威基基海邊之太陽。 
  
  一時，那對老夫妻離開草坪，走進海水，享受潮來潮往的樂趣，年輕的日本姑娘或坐或
躺，千姿百態，美不勝收。這時一陣海風吹來，吹散了老夫妻的物品，老夫妻還兀自在海水
之中歡樂弄潮，我想幫忙撿拾，但「鐵老兄」仍在休息，因此，行動仍不便，心有餘而力不
足。這時，那幾位日本姑娘隨即起身，前往「拯救」逐漸飛遠的老夫妻的物品，他們經過我
的面前時，我脫口笑著讚聲：「Good girl !」，他們也報以燦爛的笑容，我們彼此都樂極了。老
夫妻回來後，對日本姑娘的「義舉」，也報以讚賞之情。 
 15
  
  
 
「海灘上的夕陽」 
 
  我繼續前行，見到一處旅館叢集之處，便走了進去，先解決人生三急之一急之後，便在
該處遊蕩，穿過一處開放式的餐廳，又來到了海灘，原來這旅館便依著海灘而建，這時海面
開始漲潮，我脫下球鞋襪子，赤腳走入海灘，讓一波波漲潮而來的海水沖洗我的雙足，此時
正是向晚時分，一輪火紅夕陽正緩緩向著地平線落去，我拿出攝錄影機，對著自己、對著沙
灘、對著人群、也對著夕陽，拍下了一幅幅瞬間的心情、景色，可作為他日的回憶。 
 
  
  
  
「夫妻檔小吃店」 
 
  肚子又餓了，這回是真的很餓，我沒有在那個餐廳點餐，卻來到旁邊一家小吃店，老闆
是一位熱情的年輕小伙子，理了個大光頭，燈光照上去還亮的很，我隨口問他：「你們有沒有
米飯？”Do you have rice?”」他往旁邊那家小吃店一比，說：「他有，我沒有！」我對其哈哈
一笑，他也隨即哈哈大笑，我見其友善熱情，便問他：「你有什麼好吃的呢？」他說他有雞肉
蔬菜大餅，我立即點了這樣，並點了一罐可樂，他一邊做著我所點的餐點，一邊快樂的與我
聊著，我來到他的店後面靠沙灘處找個椅子坐下，他送上我的餐點後隨即準備關店門打烊，
原來我是他今天最後一位客人。隔壁那家店也同時在關店門打烊，走出來一位女老闆，他們
互相打情罵俏一下，這時我才知道他們是夫妻檔，便一邊吃一邊與他們聊天，我讚美他們的
食物好吃，他們顯得很快樂，這時我突然望見天空上已經有好多星星了，還有一輪好細好細
如眉毛的上弦新月，由於已經西斜了，故那細細的眉毛是倒掛著的，倒像是一個微笑的弧形
了，我指著那個「微笑之月」旁邊一顆極亮的星星說著：「那顆星星好亮，有人知道是什麼星
嗎？」男老闆說：「是一顆行星(planet)。」女老闆說：「是土星(Saturn)，因為我看見它有一圈
光環。」我當場傻眼，說：「你看得到光環？？！！」女老闆極有自信的點頭說是，男老闆則
搭腔說：「對！她永遠是對的！」我也大笑點頭稱是，「對，你說的對，女人永遠是對的！」
他們夫妻與我互相道別，我也獨自一人享用了美味的晚餐、沁涼的海風、繁星與眉月，並結
束了美好的一天。 
 
 
「夏威夷會議最後一日」 
 
 4 月 20 日，此行中的最後一日了，昨天玩了一日，今日睡醒時已經接近中午，決定下午
再進入會場，聽聽最後的論文發表，挑了一個壁報會議，一個口頭報告會議，壁報會議是有
關「語音分析」(“Speech Analysis”)，口頭報告會議是個特別會議，是關於「音樂訊號之搜尋
與擷取」(“Search and Retrieval of Music Signals”)。 
 
 17
Goto 博士主講的題目是：”Active Music Listening Interfaces Based on Signal Processing”，在這
個壓軸裡，Goto 博士並沒有針對某個特定技術或特定問題做深入的探討，相反的，他描繪出
他心目中有關於「主動性聆聽」音樂之經驗，並做了精彩的展示，他把歷年來他帶領的研究
團隊所做出的研究成果，非常生動的鋪陳出來，佐以實際系統的展示，令人不得不佩服其展
演的功力。聽完演講後，大家報以熱烈的掌聲，主持人並宣布了會議結束。我趨前向 Goto 博
士致意，並對其演講表示讚嘆之意，隨即互道珍重再見。 
 
  
 
「人去樓空」 
 
我並不急著離開會場，但人群確實逐漸散去，我突然想拍一張「人去樓空」的照片來當紀念，
便又開啟電腦，照了一張空蕩蕩會場只剩下我一人的照片，並上了網寫了 email 傳給老婆，
告訴老婆我要離開了，也許再去逛逛商店，給她買個特別一點的禮物。 
 
 
   
 
「中國餐館老闆」 
 
 我重回三天前（4 月 17 日）晚上去吃的那家中國餐館吃晚餐，與那天不同，今晚倒是高
朋滿座，我與老闆打了聲招呼，並向他道賀今晚生意很好，並高聲說你這家餐館菜太好吃了，
我無法忘懷，必須再吃一次，才捨得離開夏威夷，老闆直說是託我的福，感謝我為其帶來好
運。我看我那晚坐的最角落的那張桌子尚無人入座，便直接走到那個位置，向老闆點了一道
餐點，便滿懷欣喜的享用美食。老闆走到我身邊詢問我滿意今晚餐點否？我直說好吃好吃，
並向老闆要了名片，向他說，哪天你回台北開家分店時，只用了你的名字當店名，我就知道
去光顧了。老闆聽了哈哈大笑，直誇我是他見過最會說話的人了，我則大笑著說：「彼此彼此」。 
 
「ICASSP 大會工作人員，看起來很有趣但有點詭異的店」 
 
  飯後走出餐館，我正想搭個計程車回旅館，中午從旅館搭車來時，連小費花了美金 7 元
車資（合台幣約 250 元），心想價錢還好，不必省這個錢去浪費時間及「腳」力，都花了 100,000
元旅費來到這裡了，卻去省那個 250 元計程車費，然後折磨了我寶貝的「腳」，以及浪費了更
寶貴的「時間」！這就叫做「愚不可及」！於是，就開始在路邊攔計程車，過了幾輛車，但
都沒停下來，我就到了對街去招手，過了 5 分鐘，還是沒招到車，這時兩位女生走來，身穿
ICASSP 大會制服，想來是大會工作人員，應是當地某大學研究所之研究生，他們非常善意的
告訴我，就在不遠的前面就有公車站，回我住的旅館應只要花美金 2 元，我很感謝他們告訴
我這件事，便也依其意往前走了一點路程，來到公車站牌，就在這時，我赫然發現，那天 Victor
告訴我的一家外觀看起來很有趣，但有點詭異的店，就在旁邊，我微微遲疑了一下，隨即做
了重大決定，「進去看看！」、、、、、、、。這之後的數小時，我體驗了人生當中，一次相
當震撼、驚奇、高峰、難忘的經驗！有了這個經驗，這次夏威夷之行，才算功德圓滿。而這
PHONETIC TRANSCRIPTION USING SPEECH RECOGNITION TECHNIQUE
CONSIDERING VARIATIONS IN PRONUNCIATION
Min-Siong Liang, Ren-Yuan Lyu* and Yuang-Chin Chiang
Dept. of Electrical Engineering, Chang Gung University, Taiwan.
Dept. of Computer Science and Information Engineering, Chang Gung University, Taiwan.
Institute of Statistics, National Tsing Hua University, Taiwan.
*E-mail: renyuan.lyu@gmail.com
ABSTRACT
We propose a new approach for performing phonetic tran-
scription of speech and text that combines automatic speech
recognition (ASR) and grapheme -to- phoneme (G2P) tech-
niques. By augmenting the text with speech and using auto-
matic speech recognition with a sausage searching net con-
structed from multiple text pronunciations corresponding to
human speech utterance, we are able to reduce the effort for
phonetic transcription. By using a multiple pronunciation lex-
icon, a transcription error rate of 12.74% was achieved. Fur-
ther improvement can be achieved by adapting the pronunci-
ation lexicon with pronunciation variation (PV) rules and an
error rate reduction of 17.11% could be achieved.
Index Terms— Automatic Phonetic Transcription, Pro-
nunciation Variation, Chinese, Taiwanese, Dialect.
1. INTRODUCTION
Automatic phonetic transcription is gaining popularity in the
speech processing field, especially in speech recognition, text-
to-speech and speech database construction [1]. It is tradi-
tionally performed using two different approaches: an acous-
tic feature input method and text input method. The former is
the speech recognition task, or more specifically, the phoneme
recognition task. The latter is the G2P task. Both tasks, in-
cluding phoneme recognition and G2P remain unsolved tech-
nology problems. The state-of-the-art speaker-independent
(SI) phone recognition accuracy in a large vocabulary task is
currently less than 80%, far away from human expectations.
Although the accuracy of G2P tasks seems much better, it
relies on a “perfect” pronunciation lexicon and cannot effec-
tively deal with pronunciation variation issues.
This problem becomes non-trivial when the target text is
the Chinese text (C). The Chinese writing system is widely
used in China and the East/South Asian areas including Tai-
wan, Singapore, and Hong-kong. Although the same Chi-
nese character is used in different areas, the pronunciation
may be very different. Therefore, they are mutually unin-
telligible and considered different languages rather than di-
alects by most linguists. In this paper, we chose a text cor-
pus derived from the Buddhist Sutra (written collections of
Buddhist teachings). Buddhism is a major religion in Taiwan
(23% of the population). The Buddhist Sutra, translated into
Chinese text in a terse ancient style (Z) , is commonly
read in Taiwanese (Min-nan) . Due to lack of proper edu-
cation, most people are not capable of correctly pronouncing
all of the text. Besides, no qualified pronunciation lexicon
exists and very few appropriately computational linguistic re-
searches were conducted to support developing a G2P system.
Taiwanese uses Chinese characters as a part of the writ-
ten form, with its own phonetic system, which is very differ-
ent from Mandarin. This is in contrast to the case of Man-
darin, where the problem of multiple pronunciations (MP)
is less severe. A Chinese character in Taiwanese commonly
can have a classic literate pronunciation (known as Wen-du-
in, or “Z\¯” in Chinese) and a colloquial pronunciation
(known as Bai-du-in, or “ç\¯” in Chinese)[2]. In addi-
tion to MPs, Taiwanese also have a pronunciation variation
(PV) due to sub-dialectical accents, such as Tainan and Taipei
accents. We use the term MPs to stress the fact that variation
may cause more deterioration in phonetic transcription.
The traditional approach to transcribing Chinese Bud-
dhist Sutra text uses human dictation. A master monk or nun
reads the text aloud, sentence by sentence. The manual tran-
scription process is tedious and prone to errors. Since more
transcribed Sutras are planned, we are interested in how ASR
and G2P technology can help in this situation. Our task is to
discover which of them is actually pronounced. It is much
easier to acquire a person to record his/her reading of the text
than acquiring a transcribing expert. For marginalized lan-
guages with serious MPs and PV problems, this technique is
very useful.
2. THE PHONETIC TRANSCRIPTION TECHNIQUE
The flow chart shown in Fig. 1 is the framework of phonetic
transcription using the speech recognition technique. Based
on flow chart in Fig. 1, we define: s is the syllable sequence,
transcribed text corpus. The results from the experiments con-
ducted using Eq. 3 depends only on the text input and are re-
ferred as the language part performance.
What is proposed in this paper is an approach to inte-
grate both. Given a Chinese character sequence, based on
the MPs of each Chinese character, a much smaller recog-
nition net can be constructed. Take an example of a typical
text sentence “Ò1°”, which is shown in Fig. 2. We call
such a net as sausage net, which is named for its shape like a
sausage. Higher recognition accuracy can be expected due to
the smaller perplexity in the recognition net.
3.1. The Pronunciation Lexicons, Recognition Nets and
Results
The Formosa Lexicon could be used for a wide range of ap-
plications and tends to have a higher number of multiple pro-
nunciations in Taiwanese [2]. However, some pronunciations
do not appear in the Formosa Lexicon due to pronunciation
variations. Thus, the second lexicon, called the Sutra Lexi-
con, is derived from the Sutra itself to study what variations
exist from the Formosa lexicon to Sutra Lexicon. The per-
formance of phonetic transcription using the Sutra Lexicon
is looked upon as the upper bound performance for the pho-
netic transcription. In addition to the above two separate lexi-
cons, a combined lexicon is called the Enhanced Lexicon for
convenience. In recognition nets, the first is the free-syllable
net, denoted as the Free-Syl-Net. The other three search nets
are the sausage nets were constructed by filling in each node
of the net with the corresponding multiple pronunciations of
each Chinese character from each of the three pronunciation
lexicons. The nets are denoted the General-Sau-Net, Specific-
Sau-Net, and Enhanced-Sau-Net for the Formosa Lexicon,
Sutra Lexicon, and Enhanced Lexicon.
With the four search nets and acoustic models, the recog-
nition results are shown in Fig. 3. In addition, we also show
the result of only language, called G2P, with unigram. Through
observing the experimental results, neither G2P with unigram
nor Free-Syl-Net with adaptation model can reach acceptable
performance. Therefore, it is necessary to integrate the lan-
guage and acoustic parts. The General-Sau-Net could com-
pete with the Specific-Sau-Net. Thus, if the speaker indepen-
dent model could be adapted using some phonetically tran-
scribed speech data, the adapted speaker independent model
under the General-Sau-Net would be suitable for phonetic an-
notation task. Although some pronunciations of the Buddhist
Sutra Chinese characters do not appear in the Formosa Lex-
icon, the performance of the Formosa Lexicon Sausage Net
degrades not much more than the Sutra Lexicon Sausage Net.
So far, the Enhanced Lexicon Sausage Net includes all pos-
sible pronunciations of Sutra Chinese characters, but it may
increase the perplexity of the search net. Practically, some er-
rors result from pronunciation variations by our speech data
observation. Therefore, we determined that the performance
¬° ¥À »¡ ªk
ko
sua
t
sue
sue
hua
t
ui
bo
bu
Fig. 2. The net is constructed from the multiple pronunciations of
each Chinese character from our Formosa Lexicons.
41.41
12.74
9.36
7.65
38.30
0
5
10
15
20
25
30
35
40
45
50
Free-Syl-Net General-Sau-Net Enhanced-Sau-Net Specific-Sau-Net
Sy
lla
bl
e 
Er
ro
r R
at
e(%
)
SI w/ adaptation
G2P(unigram)
Fig. 3. Syllable error rate (SER) under four searching nets. See text
in subsection 3.1 for notations.
would get better by trivial adaptation of the Formosa Lexicon
Sausage net.
4. INCORPORATING PRONUNCIATION
VARIATION RULES
Because insufficient coverage of pronunciations in the search
net will severely degrade the recognition performance, some
approaches to extend the pronunciation coverage will be con-
sidered to help the overall performance. The simple way to
adopt the methodology of pronunciation variation is to ex-
pand the pronunciation lexicon using variation rules of the
form LBR → LSR where B and S represent the base form
and surface form of a central phone, and L, R are the left and
right contexts respectively [3]. To derive such rules, a speech
corpus with both canonical pronunciation and actual pronun-
ciation is necessary. We choose a subset of ForSDAT, called
ForSDAT-02, to derive PV rules shown in Table 1.
A small portion of the ForSDAT-02 was then manually
checked and the phonetic transcription of the transcript “cor-
rected” according to actual speech. The triphone-level con-
fusion table is built and used as a direct knowledge source
to derive the PV rules, where each cell in the table is looked
upon as a rule. The enormous number of rule set selections is
2P
2
, where P is the number of triphone. To make the problem
more solvable, some specially designed algorithms should be
developed. The mathematic definitions of the 3 kinds of stas-
tical measures are as follows:
1. Joint probability (JP) of the base form pronunciation
