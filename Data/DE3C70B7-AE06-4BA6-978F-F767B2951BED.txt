 1
行政院國家科學委員會補助專題研究計畫成果報告 
※※※※※※※※※※※※※※※※※※※※※※※※※ 
※                                              ※ 
※     減少同步多線程錯誤路徑指令的提取      ※  
※                       ※ 
※※※※※※※※※※※※※※※※※※※※※※※※※ 
 
計畫類別：▓個別型計畫  □整合型計畫 
計畫編號：NSC 94－2213－E－036－017 
執行期間： 94 年 8 月 1 日至 95 年 7 月 31 日 
 
計畫主持人：謝忠健 
共同主持人： 
 
 
 
 
 
 
本成果報告包括以下應繳交之附件： 
□赴國外出差或研習心得報告一份 
□赴大陸地區出差或研習心得報告一份 
□出席國際學術會議心得報告及發表之論文各一份 
□國際合作研究計畫國外研究報告書一份 
 
 
執行單位：大同大學資工所 
 
 
中 華 民 國 95 年  月  日
 3
considered to be a great scheme not only the 
performance it achieved but also in the 
efficiency of implementation. However, 
ICOUNT fetch policy does not consider that 
the instructions may fetch from wrong path. 
Fetching instructions from wrong path 
wastes hardware resources and causes more 
power consumption. Here, we try to improve 
the fetch engine to reduce wrong-path 
instructions and to gain integral 
performance. 
 
Keywords: Simultaneous Multithreading 
(SMT), Fetch engine, ICOUNT, Wrong 
path. 
 
二、緣由與目的 
緣由： 
近來隨著 VLSI 技術的進步，在一個
晶片中所能容納的電晶體數也越來越多，
使得微處理器的效能得以不斷地提升；現
今的微處理器大都使用超級純量
（Superscalar）的架構，雖然多年來的相
關研究提出許多相關技術，無論是實際上
或是理論上用來增加超級純量
（Superscalar）架構的效能，最終發現其
有一定的瓶頸存在。於是乎為了突破傳統
超純量（Superscalar）架構的瓶頸，有所
謂的多線程（Multithreading）架構出現，
它的好處在於當一個線程的可執行率不好
時，它能將處理器使用權轉換到另一個相
對較好執行的線程，藉此來提升處理器的
利用度。而在多線程（Multithreading）架
構之後，由 Tullsen 等人提出的同步多線程
（Simultaneous Multithreading(SMT)）架
構，他能同時擁有超純量（Superscalar）
架構以及多線程（Multithreading）架構的
好處，也就是能同時提升指令平行度（ILP）
和線程平行度（TLP），來增進處理器的
使用率。 
在 Tullsen 等人提出的同步多線程
（SMT）架構底下，他們提出的 ICOUNT
提取策略如同先前提到，它是針對較好的
線程給予較多的硬體資源，但無法對提取
到錯誤路徑指令的情形有所改善。因此為
了改善這種情況，必須改進提取引擎中的
分支預測器單元，使分支預測的正確率提
高，以減少程式走向錯誤路徑的情形。而
在 Tullsen 等人提出的同步多線程（SMT）
架構底下的分支預測方法，他們所採用的
是在傳統超純量架構上有不錯效果的二層
預測分支法: gshare，其架構如下圖: 
 
Branch history register (BHR): 用來記錄
最近幾支分支指令的行為, 是跳躍(taken)
就記綠為 1; 不跳躍(not taken)則記錄為 0。 
Branch address: 為現在分支指令的程式
計數器(program counter)。 
Pattern history table(PHT): 做為 BHR 和
Branch address實行XOR運算後的對應表, 
2-bit counter 是做為下一個分支指令的預
測依據。11 或 10 就預測為 taken, 01 或 00
則預測為 not taken。 
2-bit counter 的更新: 當分支指令執行後, 
若為 taken, 將 counter 加 1;若為 not taken, 
則將 counter 減 1。最大值 11, 最小值 00。 
 
目的： 
過去被侷限在傳統超級純量（Superscalar）
架構的研究，隨著同步多線程（SMT）架
構的提出，在效能上有顯著的重大突破，
引發各界多方的研究與討論；相對於其他
早期提出的架構，同步多線程（SMT）架
構算是相當新的架構，所以其中還是有許
多值得探討與研究的地方，而 Tullsen 本人
在許多研究當中指出：在同步多線程
（SMT）架構的提取引擎中，提取策略
（ Fetch Policy）和分支預測（Branch 
Prediction）是整個架構效能的瓶頸所在，
本實驗室在過去的研究中曾提出比
ICOUNT 更好的提取策略，因此這次的研
究案中我們將會針對分支預測（Branch 
 5
          表二: 分支機制組態 
 
Selected Applications 
Integer 
Based 
gzip, vpr, gcc, mcf, crafty, gap, 
bzip2, twolf 
Floating Point 
Based 
meas, art, equake 
表三: 選取的 benchmarks 
 
 從圖二到圖四顯示了加入分支分類器
( Branch Classifier )的三種提取策略，相當
程度地減少了從錯誤路徑提取指令。我們
以 ICOUNT 作為基礎線分別與 FCC、FB、
FGAP 等提取策略比較。 
各種提取策略的說明如下: 
1. FCC：分支預測的機制使用 gshare 
predictor 與 confidence estimator。  
2. FB ：分支預測的機制使用 gshare 
predictor 與 biased branch filter。 
3. FGAP 分支預測的機制使用 integrated 
branch predictor。 
 
圖二: 2 線程 benchmarks 的錯誤路徑提取指令 
 
 
圖三: 4 線程 benchmarks 的錯誤路徑提取指令 
 
Parameter Value 
Pattern History Table (PHT) 2K 
Global History 
Register (GHR) 
11 bits 
Branch Target Buffer (BTB) 256, 4way 
Biased Table (BT) 256, 4way 
Biased Counter 4bits 
Taken/Not Taken Biased 
Threshold 
12 / 3 
Confidence Counter 4 bits 
Confidence Threshold 8 
Non-Confidence Threshold 2 
Miss Bit Counter 4 bits 
Gating Threshold 15 
 7
 
圖八: 2 線程 benchmarks 的整體 IPC 
 
 
圖九: 4 線程 benchmarks 的整體 IPC 
 
 
圖十: 6 線程 benchmarks 的的整體 IPC 
 
四、計畫成果自評 
    在此次計畫中的模擬結果顯示，我們
藉由減少錯誤路徑的指令提取與提高預測
的準確率來改善效能。在我們的研究結果
中FGAP的提取策略最多可以達到減少52%
的錯誤路徑指令提取並將預測的準確率提
高到 92%。然而，我們在研究中發現 FCC
的提取策略雖然有相當程度地減少了錯誤
路徑指令的提取，但是在整體的 IPC 卻有
稍微下降的情形。我們將限制效能改善的
可能原因整理如下: 
1. 每個時脈週期缺乏足夠的指令提取。 
2. Data dependence 與硬體資源的競爭。 
這兩點對於 SMT 架構的效能都有很大的影
響，因此對於某些效能評估程式的 IPC 會
有所影響。此外，我們未來計劃朝向 fetch  
policy 與 issue logic 等方向進行研究。 
五、參考文獻 
[1] P.-Y. Chang, M. Evers, and Y. Patt, 
“Improving Branch Prediction Accuracy by 
Reducing Pattern History Table 
Interference,” Proc. Int. Conf. on Parallel 
Architectures and Compilation Techniques, 
Oct. 1996 
 
[2] S. Eggers, J. Emer, H. Levy, J. Lo, and R. 
Stamm, and D. Tullsen, “Simultaneous 
multithreading: A platform for 
next-generation processors,” Technical 
Report 
TR-97-04-02, University of Washington, 
Department of Computer Science and 
Engineering, April 1997 
 
[3] A. El-Moursy, and D. Albonesi, 
“Front-end policies for improved issue 
efficiency 
in SMT processors,” 9th International 
Symposium on High-Performance Computer 
Architecture, pages 31-40, February 2003 
 
[4] H. Hirata, K. Kimura, S. Nagamine, Y. 
Mochizuki, A. Nishimura, Y. Nakase, and T. 
Nishizawa, “An elementary processor 
architecture with simultaneous instruction 
issuing from multiple threads,” In 19th 
Annual International Symposium on 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 94－2213－E－036－017 
計畫名稱 減少同步多線程錯誤路徑指令的提取 
出國人員姓名 
服務機關及職稱 
謝忠健   大同大學資工所 
會議時間地點 Orlando, Florida, USA 
會議名稱 4th International Conference on Computing, Communications and Control Technologies (CCCT 2006) 
發表論文題目 Load Speculation 
 
一、參加會議經過 
1.這是多個研討會聯合舉辦的，地點在美國佛州奧蘭多。 
2.每日早晨均有一位主講者對全體參與人員做專題報告： 
“Nanotechnologies : Its Application & Impacts”, 以色列 HIT 副校長, Azonlay. 
“The need for Inter Disciplinary Research Approaches in Information Assurance”, 美國密
西西北大學教授, Vaughn. 
“Materials in Today’s World : A case study of on-line Course Design”, 美國賓州大學教授, 
Howell 
 
 
二、與會心得 
 
本人參加 CCCT 2006, 全部有 64 篇論文發表, 範圍涵蓋計算、通訊與控制技術, 本人參與之
section 為 ”Applications in other Areas & Hybrid Application”, 有 4 篇論文, 但有一位沒到。除
本人外, 一位是捷克來的, 一位是日本來的, 本人並被推為 co-chair 之一。 
 
出國交流，有益研究與人際關係。 
 
 2
In this paper, we use a simple value predictor to 
predict the operand value to avoid register 
dependence and propose a structure called Load 
Forwarding History Table (LFHT) to exploit memory 
dependence speculation at run time. Data prediction 
is used to predict the value which will be used to 
calculate the effective address. The LFHT provides 
the information of disambiguation memory 
dependence for issuing load instruction out of order.  
Since we combine these two mechanisms, the 
predictor can help LFHT making more load 
instructions execute without waiting for the prior 
stores’ effective addresses calculated, this result in 
more load instructions will be issued earlier. When a 
load instruction is speculatively executed, 
instructions that are dependent upon the load 
instruction will also be speculatively executed. 
The organization of the rest of this paper is as 
follows. Section 2 surveys previously proposed 
related works. Section 3 illustrates whole structure in 
superscalar processor. Section 4 describes our CPU 
model and simulation environment. The performance 
is evaluated in section 5. Finally, the conclusion of 
this paper is presented in section 6. 
 
2 Related Works 
 
The traditional works on memory 
disambiguation were done in the context of compiler 
and hardware mechanisms for non-speculative 
disambiguation to ensure program correctness. More 
recently, works on speculative memory 
disambiguation have been hot topics in academia. 
Franklin and Sohi [2] proposed the address resolution 
buffer (ARB). The ARB indicates memory references 
into bins according to their address. The bins are used 
to cause a temporal order between references to the 
same address. The ARB is a structure based on bank. 
Multiple disambiguation requests can be dispatched 
in one cycle, provided that they are all to different 
banks.  
Chrysos and Emer used predictor to solve 
memory disambiguation problem in [5]. The goal of 
the designers is to be able to schedule load 
instructions as soon as possible without causing any 
memory order violations. The predictor proposed is 
based on store-sets. A store set for a specific load is 
the set of all stores upon which the load has ever 
depended. The processor adds a store to the store set 
of the load if a memory order violation is caused 
when the load executes before that store. In the next 
instance of the load instruction, the store set is 
accessed to determine which stores the load will need 
to wait for before executing.  
A. Yoaz., M. Erez., R. Ronen. and S. Jourdan 
designed a CHT predictor [7]. The CHT predictor  
provides a prediction about whether a load instruction 
will conflict with any store in the instruction window. 
Allocating a new entry only when a load collides for 
the first time and invalidating its entry when its state 
changes to non-colliding. It does not predict which 
store instruction the load will conflict with. Therefore, 
it is easier to design but it does not provide the best 
possible information for disambiguation purposes.  
Color sets [10] presents a simple mechanism 
which incorporates multiple speculation levels within 
the processor and classifies the load and the store 
instructions at run time to the appropriate speculation 
level. Each speculation level is termed as a color and 
the sets of load and store instructions are called color 
sets. These colors divide the load instructions into 
distinct sets, starting with the base color which 
corresponds to the no violation case. In other words, 
this set is the set of load instructions which have 
never collided with unready store instructions in the 
past. Each color in the spectrum represents increasing 
levels of aggressiveness in load speculation; a load 
instruction is allowed to issue only if its color is less 
 4
instructions inside the processor, one instruction 
calculates the effective address. In order to predict 
this instruction, we predict instruction’s operand so 
that we don’t have to wait prior instruction which this 
instruction depend on. 
Since we predict instruction’s operand, there 
are some instructions that don’t have their register 
dependence with prior instruction, these instructions 
don’t need to predict operand because they already 
have exactly register value. In our simulation, we 
only predict instructions with register dependence. 
But all load instructions must update the predictor, so 
that we can maintain predictor’s accurate rate. 
Data prediction helps speedup the effective 
address calculation for a load. The load then has only 
to wait on potential store aliases before issuing. 
When a load instruction uses data prediction, its prior 
instruction with register dependence still executes 
normally. Once the prior instruction calculation 
finishes, it checks its register value against with the 
predicted value to determine if the load’s operand 
was correctly predicted. If the operand was 
incorrectly predicted, the load is re-issued with the 
correct value, and the squash recovery action takes 
place after this load instruction. In our simulation, we 
used Stride predictor as default predictor. 
Value prediction has been studied for a long 
time and many schemes have been proposed [4, 6, 
11]. In this paper we use the simplest scheme, stride 
predictor, to predict the operand of a load instruction.  
A stride predictor keeps track of not only the last 
value referenced by a load, but also the difference 
between the last value of the load and the last load 
before that. This difference is called the stride. The 
predictor speculates that the new value seen by the 
load will be the sum of the last value seen and the 
stride. 
 
Figure 3.3 shows the value predictor (VP) 
position in architecture we modeled. When a load 
instruction is dispatched, its source register field is 
used to check whether this operand is available or not. 
If not, PC of a load instruction is used for data 
prediction, then the data is used for operand value. If 
we make wrong data prediction (mis-prediction), a 
recovery mechanism will take place when the actual 
operand is available. 
Instr. Fetch 
Unit
Decode Unit
Register Update 
Unit
Load/Store
Function Unit
Function Unit
Function Unit
Register file
The additional data 
path
verification
VP
access
 
Figure 3.3 Architecture data path with VP 
 
3.3 Load Forwarding History Table 
 
When a load is issued, it performs a lookup in 
the store buffer for a non-committed aliased store and 
it performs its data cache access in parallel. If a store 
alias is found, store data forward to load and the load 
has a shorter latency. If there is no store alias, and 
there is a data cache hit, the load has a longer latency 
because of the pipelined data cache. If there is a miss 
in the data cache, the miss will only be processed if 
no alias is found in the store buffer, load-forwarding 
behavior can detect store alias and forward store data. 
This way, load instructions can be issued out of order 
without waiting for prior stores executed [8, 9].  
Conventional disambiguation memory 
dependence mechanism unable to provide 
information for load instruction in the decode stage. 
For that reason, in order to exploit load-forwarding 
behavior and bring about all of these benefits, a 
mechanism is proposed: the load forwarding history 
table (LFHT). 
The internal structure of the load forwarding 
 6
shown in figure 3.5 to solve both memory 
dependence and register dependence. 
 
Instr. Fetch 
Unit
Decode Unit
Register Update 
Unit
Load/Store
Function Unit
Function Unit
Function Unit
LFHT
Register file
The additional data 
path
VP
 
Figure 3.5 Architecture data path with VP and LFHT 
 
4. EVALUATION 
METHODOLOGY 
 
In this section, we will describe our 
performance evaluation methodology. First, we 
explain the machine model used in our simulation 
study. Next, we describe the simulator set up and the 
benchmark programs used. 
 
4.1 Machine Model 
 
The simulator used in this work is derived 
from the SimpleScalar 2.0 and 3.0c tool set [3], a 
suite of functional and timing simulation tools. This 
architecture is based on the Register Update Unit 
(RUU), which is a mechanism that combines the 
instruction window for instruction issue, the rename 
logic, and the reorder buffer for instruction commit 
under the same structure. The instruction set 
architecture employed is the Alpha instruction set, 
which is based on the Alpha AXP ISA.  
Table 1 summarizes some of the parameters 
used in our baseline architectures. Table 2 presents 
the parameters of load forwarding history table. Table 
3 shows the architectures we studied in this 
evaluation. 
 
Table 1 Baseline Architecture Configuration 
Instruction fetch 8 instructions per cycle. 
Out-of-Order 
execution 
mechanism 
Issue of 8 instructions /cycle, 256 
entry RUU(which is the ROB and 
the IW combined), 128 entry 
load/store queue. Loads executed 
only after all preceding store 
addresses are known. Value 
bypassed to loads from matching 
stores ahead in the load/store 
queue. 2 cycle load forwarding 
latency. 
Architected 
registers 
32 interger, hi, lo, 32 floating 
point. 
Functional units 
(FU) 
8-integer ALUs, 8 load/store units, 
4-FP adders, 1-Integer MULT/DIV, 
1-FP MULT/DIV 
FU latency int alu--1, load/store--1, int 
mult--3, int div--12, fp adder--2, fp 
mult--4, fp div--12, fp sqrt--24 
L1 Instruction 
cache 
64K bytes, 2-way set assoc., 32 
byte block, 4 cycles hit latency. 
L1 Data cache 64K bytes, 2-way set assoc., 32 
byte block, 4 cycles hit latency. 
Dual ported. 
L2 unified cache 1024K bytes, 4-way set assoc., 64 
byte block, 12 cycles hit latency 
Memory Memory access latency (first-36, 
rest-4) cycle. Width of memory 
bus is 32 bytes. 
TLB miss 30 cycles 
 
Table 2 Configurations of LFHT  
 Configuration 
LFHT 2K entry 
4K entry (default table size)
8K entry 
LFHT with cycle clear 128 
512 
2K (default) 
8K 
 
Table 3 Architectures we studied 
Baseline Baseline architecture 
VP Baseline + VP 
VP + LFHT Baseline + VP + LFHT
VP + LFHT with Cycle 
Clear 
Baseline + VP + LFHT 
with Cycle Clear 
Perfect VP + Perfect 
LFHT 
Baseline + Perfect VP + 
Perfect LFHT 
Note: Cycle Clear is a keyword detailed in section 5.3 
 8
Number of load instructions ( floating-point )
0
1000
2000
3000
4000
am
mp app
lu art
equ
ake gal
gel luc
as
me
sa
mg
rid swi
m
 
Figure 5.2 Number of executed load instructions 
(Floating-point benchmark) 
Prediction Accurate Rate(Integer)
0.8
0.85
0.9
0.95
1
bzi
p2 gap gzi
p
par
ser
vor
tex
128
512
2048
8192
 
Figure 5.3 Accurate rate of VP (Integer benchmarks) 
Prediction Accurate rate (Floating-point)
0.85
0.9
0.95
1
amm
p
app
lu art
equ
ake galg
el luca
s
mes
a
mg
rid swi
m
128
512
2048
8192
 
Figure 5.4 Accurate rate of VP (floating-point 
benchmarks) 
     
5.1.2 Hit Rate 
 
Figure 5.5 shows the hit rate of VP with 128, 
512, 2048, 8192 entries in integer benchmarks, 
Figure 5.6 shows the results of Floating-point 
benchmarks. 
The average hit rate of integer benchmarks 
are 68% for 128, 85% for 512, 94% for 2048, 99% 
for 8192, of floating-point benchmarks are 80% for 
128, 99% for 512, 97% for 2048, 99% for 8192. 
    The main reason of low hit rate for some 
benchmarks is because there are more load 
instructions with different program counter so that 
larger table entries are needed to keep more load 
instructions. 
 
5.1.3 Available Rate 
 
Figure 5.7 shows available rate of VP with 
128, 512, 2048, 8192 entries in integer benchmarks, 
Figure 5.8 shows the result of Floating-point 
benchmarks. 
Available rate is calculated by: 
(Load instructions’ operand being correct predicted + 
Load instructions already with available operand) / 
Number of executed load instructions 
The average available rate of integer 
benchmarks are 75% for 128, 80% for 512, 83% for 
2048, 84% for 8192, of floating-point benchmarks 
are 88% for 128, 92% for 512, 94% for 2048, 95% 
for 8192. 
Available rate directly relates to hit rate, 
benchmarks with average higher hit rate have average 
higher available rate because more load instructions 
can be predicted as almost all benchmarks (exclude 
VORTEX) have almost same accurate rate (about 
average 95%). 
 
 
 
 
VP table hit rate(integer)
0
0.2
0.4
0.6
0.8
1
bzip
2
craf
ty gap gcc gzip mc
f
par
ser two
lf
vor
tex vpr
128
512
2048
8192
 
Figure 5.5 Hit rate of VP (integer benchmarks) 
 10
To prevent the LFHT from being too 
conservative, causing false data dependency, all alias 
bit fields in LFHT are cleared at a regular length of 
cycles. In this paper, we modeled 50,000 cycles clear 
(CC) for LFHT. 
Figure 5.16 shows integer benchmarks’ 
speedup over the baseline. The average speedups are 
14.5% without cycle clear, 16.1% with cycle clear. 
Figure 5.17 shows floating-point benchmarks’ 
speedup over the baseline. The average speedups are 
both 5% with or without cycle clear. 
Speedup is calculated by: (New schemes’ IPC – 
baseline’s IPC) / baseline’s IPC 
In our simulation, we also model a perfect 
scheme on both VP and LFHT, using this scheme to 
check how many performance we can get most from 
this scheme.  
With VP, we use perfect prediction on all load 
instructions. This means when a load instruction is 
dispatched to RUU, whether its operand is available 
or not, we can use it with actual value, because we 
can predict it with 100% accurate rate. 
With LFHT, perfect method means load 
instruction only wait for store instruction with same 
effective address. This means there are not have any 
unnecessary time spent on waiting store instructions’ 
effective addresses. 
The average speedup with perfect prediction 
in integer benchmarks is 20.5%, in floating-point 
benchmarks is 7.5%. Our simulation results in 
speedup of 16.1% for integer benchmarks, of 5% for 
floating-point. This is close to perfect case. 
The main reason of vortex’s performance 
worsen than baseline is due to its low accurate rate in 
value prediction, so it needs more cycles to squash 
instructions and re-fetch instructions to instruction 
windows. 
Hit rate of LFHT (integer)
0.85
0.9
0.95
1
bzip
2
craf
ty gap gcc gzip mc
f
par
ser two
lf
vor
tex vpr
4096
 
Figure 5.10 Hit rate of LFHT (integer benchmarks) 
Hit rate of LFHT (floating-point)
0.992
0.994
0.996
0.998
1
am
mp app
lu art
equ
ake galg
el luca
s
mes
a
mg
rid swi
m
4096
 
Figure 5.11 Hit rate of LFHT (floating-point 
benchmarks) 
0
2
4
6
bzi
p2
cra
fty gap gcc gzi
p mc
f
par
ser two
lf
vor
tex vpr
 
Figure 5.12 Cycles per load instructions spend on 
waiting its effective address after using LFHT 
0
1
2
3
4
bzi
p2
cra
fty gap gcc gzi
p mc
f
par
ser two
lf
vor
tex vpr
 
Figure 5.13 Cycles per load spend on waiting its ea 
after combine both VP and LFHT 
IPC (integer)
0
2
4
6
8
bzi
p2
craf
ty gap gcc gzip mc
f
par
ser two
lf
vor
tex vpr
baseline
VP(2K)
VP+LFHT
with CC
perfect
 
Figure 5.14 IPC (integer benchmarks) 
