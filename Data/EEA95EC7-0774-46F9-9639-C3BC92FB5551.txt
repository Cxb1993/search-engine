 2
brain to segregate sounds related to formants are the 
harmonicity combined with the frequency modulation (FM). 
Cues further used to group formants into vowels, syllables 
and meaningful sentences are fundamental frequency, FM, 
and pitch continuity. In this section, we demonstrate the 
extractions of the pitch (harmonicity) and the FM cues from 
outputs of a two-stage auditory model. 
 
Fig. 1. Spectrum estimation stage for pitch cue extraction. 
 
2.1. Spectrum Estimation Stage and Pitch Cue 
 
This stage models observed functions of the peripheral 
auditory processing. As shown in Fig.1, this stage is 
implemented by a bank of 128 overlapping asymmetric 
constant-Q bandpass filters. These constant-Q filters 
represent the frequency selectivity of the cochlea and are 
implemented equally spaced on the log-frequency axis 
covering a total of 5.3 octaves. Filters’ outputs are then fed 
into a first-order lateral inhibitory network (LIN) and 
further processed by an envelope extractor. Detailed 
mathematic formulations and descriptions of biophysical, 
psycho-acoustical evidences of this stage can be found in 
[6]. As shown in Fig.1, the output of this stage is referred to 
as an auditory spectrogram and represents a time-log, 
frequency energy distribution of the input signal. Intuitively, 
it is similar to a traditional FFT-based spectrogram plotted 
along the bark or mel scale. 
The harmonicity is considered the most prominent cue 
used by human to distinguish sounds among multiple 
sources. In our algorithm, an energy-based VAD (voice 
activation detection) is used to estimate voiced frames 
which possess the harmonic structure. In unvoiced frames, 
pitch is set to zero. Here, we adopt the two-dimensional 
AMDF (average magnitude difference function) method 
similar to [7] for pitch estimation. As shown in equation (1), 
the difference function encodes the periodicity of filter-bank 
output.  
1 2
1 2 1 2
( , , , )
( , ) ( , ) ( , ) ( , )
AMDF t x
y t x y t x y t x y t x
τ τ
τ τ τ τ= − − − − + − −
          (1) 
where τ1 and  τ2 are the time lags, y(t, x) is the auditory 
spectrogram at time t and in the frequency channel x. The 
highly overlapped 128 bandpass filters are actually 
sharpened by followed first-order LIN network. The 
equivalent filters can resolve up to around 10~12 harmonics 
with related to their sharpened bandwidths [6]. Such 
frequency resolution would divide the auditory spectrogram 
into resolved and un-resolved regions across the log-
frequency axis. By carefully choosing the time constant of 
the integrator in the envelope extractor, the resolved 
channels will show individual harmonics while the un-
resolved channels will only show the envelope of a 
harmonic complex comprised of multiple higher harmonics. 
Fr
eq
ue
nc
y 
(H
z)
Time (ms)
Auditory Spectrogram
100 200 300 400 500 600 700 800 900 1000
 125
 250
 500
1000
2000
Multiresolution Cortical Filters and Outputs
Upward Downward
Slow  Rate
Coarse Scale
Slow  Rate
Fine Scale
Fast Rate
Coarse Scale
Fast Rate
Fine Scale
Slow  Rate
Coarse Scale
Slow  Rate
Fine Scale
Fast Rate
Coarse Scale
Fast Rate
Fine Scale
   
Fig. 2. Spectro-temporal modulation decomposition for FM, 
AM, onset/offset cues extraction. 
 
Hence, the 2D AMDF function of the auditory 
spectrogram reveals the periodicity of individual harmonics 
(i.e., carriers) in the resolved frequency region. We use the 
measure of 
1 2
128
1 2
, 1
arg min ( , , , )
x
AMDF t x
τ τ
τ τ
=
∑  to estimate two 
pitches frame by frame. With a certain pitch during a time 
frame, we can generate a channel mask by examining the 
periodicity of each channel’s response pertaining to the 
pitch.  
 
2.2. Spectrum Analysis Stage and FM Cues 
 
The next stage of the auditory model mimics the spectro-
temporal selectivity of neurons observed in the primary 
auditory cortex (A1). The auditory spectrogram is analyzed 
by a family of neurons which are modeled by two-
dimensional filters tuned to three spectro-temporal 
modulation parameters: rate, scale and directionality. The 
rate parameter (in Hz) characterizes how fast/slow the signal 
energy varies along the temporal axis; the scale parameter 
(in cycle/octave) defines how broad/narrow the signal 
energy distributed along the log-frequency axis; the 
directionality parameter encodes the downward/upward 
direction of the FM sweep. Fig.2. demonstrates the multi-
resolution analysis by eight neurons which tune to different 
combinations of rate, scale and FM direction. The top panel 
shows the input auditory spectrogram from Fig.1. The eight 
larger panels at the bottom show the local energies of 
 4
task. The task is to separate the target speech from a mixture 
in the presence of n0~n9 interferences. We evaluate our 
system in two indices, the energy loss (EL) and the noise 
residue (NR) with the same definitions as in [9]. They are 
defined as follows: 
PEL=
Energy of units with EM=0 IBM=1
Energy of IBM  
PNR=
Energy of units with EM=1 IBM=0
Energy of IBM  
where the suffix “EL” and “NR” stand for energy loss and 
noise residue. The term “EM” represents our estimated 
mask and the term “IBM” stands for the Ideal Binary Mask 
where value is set to 1 if energy of the target is larger than 
the energy of the interference in that particular t-x unit. 
Performance results under n0~n9 interferences are shown in 
table 1. 
We test our system thoroughly by mixtures of two 
speech utterances from the Speech Separation Challenge 
[12]. The two-talker case with the 6dB TMR (target to 
masker ratio) is examined. The result is shown in table 2. 
 
 EL NR 
n0 11.03% 66.20  % 
n1 11.50 % 20.41 % 
n2 12.41 % 0.95% 
n3 14.42% 12.80 % 
n4 19.63 % 23.46% 
n5 30.81 % 68.97 % 
n6 10.78% 4.56 % 
n7 12.18 % 13.19 % 
n8 10.59 % 4.09 % 
n9 17.13 % 28.93 % 
 
Table 1.  Energy loss and noise residue by our proposed 
system in Cooke’s corpus. 
 
 EL NR 
6dB 26.03% 14.51% 
 
Table 2.  Energy loss and noise residue by our proposed 
system in Speech Separation Challenge.  
 
5. CONCLUSIONS AND DISCUSSIONS 
 
A computational auditory scene analysis algorithm is 
developed and examined in speech separation simulations. 
We postulate that there exist neurons at higher levels than 
A1 grouping perceptual cues, such as the pitch and FM, to 
form separated sound streams. The harmonic extraction and 
the dynamic programming are used to carry out the 
simultaneous and sequential grouping processes, 
respectively.  
In this study, we found masks produced by unit-by-unit 
labeling are usually trivial. It means temporal information 
from individual unit has no significant perceptual meanings. 
As a result, we expect larger time-frequency sections as a 
whole are better choices for grouping than the individual 
time-frequency unit. In addition, we tackle the simultaneous 
and sequential groupings separately. In our opinions, it is a 
valid engineering approach, but hardly a model with 
biophysical evidences for human ASA process. We believe 
grouping neurons beyond A1 also respond to spectral and 
temporal cues simultaneously. The cost function in dynamic 
programming should also consider effects of harmonics in 
adjacent frames rather than of the pitch only. 
 
6. ACKNOWLEDGEMENT 
 
This research is supported by the National Science Council, 
Taiwan with Grant No. NSC 98-2221-E-009-092. 
 
7. REFERENCES 
 
[1] B. C. J. Moore, An Introduction to the Psychology of Hearing, 
5th Ed., Elsevier Academic Press, 2003. 
[2] A. Hyvrinen, J. Karhunen and E. Oja, Independent Component 
Analysis. John Wiley & Sons, New York, 2001. 
[3] A. S. Bregman, Auditory Scene Analysis: the perceptual 
organization of sound, MIT Press, 1990. 
[4] M. Elhilali and S. Shamma, “A biologically-inspired approach 
to the cocktail party problem,” ICASSP 2006, V.637-V.640. 
[5] Y. Shao, S. Srinivasan, Z. Jin and D. Wang, “A computational 
auditory scene analysis system for speech segregation and robust 
speech recognition,” Computer Speech & Language, Volume 24, 
Issue 1, January 2010, Pages 77-93 (In Press).  
[6] T. Chi, Y. Gao, C.G. Guyton, P. Ru, and S. Shamma, “Spectro-
temporal modulation transfer functions and speech intelligibility,” 
J. Acoust. Soc. Am., vol. 106, no. 5, pp.2719-2732, 1999. 
[7] de Cheveigné, A & Kawahara, H, “YIN, a fundamental 
frequency estimator for speech and music,” J. Acoust. Soc. Am., 
vol. 111, no. 4, pp. 1917–1930, 2002.  
[8] T. Chi, P. Ru, and S.A. Shamma, “Multi-resolution spectro-
temporal analysis of complex sounds,” J. Acoust. Soc. Am., vol. 
118, no. 2, pp. 887-906, 2005. 
[9] Hu, G. and D. L. Wang . “An auditory scene analysis approach 
to monaural speech segregation.” Topics in acoustic echo and 
noise control: pp.485-515, 2006 
[10] Weintraub, M.(1985) . A theory an computational model of 
monaural auditory separation. PhD Thesis, Stanford University.  
[11] M. P. Cooke, Modeling Auditory Processing and 
Organisation, Cambridge University Press, Cambridge, UK, 1993. 
[12] M. Cooke and T.-W. Lee, “Speech separation and recognition 
competition.” 
http://www.dcs.shef.ac.uk/martin/SpeechSeparationChallenge.htm, 
Accessed 5 Apr. 2006. 
日再行關閉），在實際參加會議時間太短及安全雙重考量下，本人最後並未前
往 Los Angeles，但於會議期間聯絡主辦單位，以郵寄方式取得所有會議相關
資料。 
本人近年在台灣的研究都以應用導向為主，例如語音品質量測、語音分
離及音源分源、抗噪性語音辨識、新一代助聽器的研發等，但本人認為唯有
多了解聽覺神經生理和心理現象，方能在相關工程應用領域中有突破性的進
展，而此年會正是相關領域研究成果展示之場所。本期望在此次會議中能學
到新知並具體展現於未來的研究成果之中，但因天候關係失去與國際學者接
觸和交流的機會，實屬憾事。 
（三） 國際交流 
於造訪 U. of Maryland 期間，本人就目前參與的研究計畫及研究方向與
Dr. Shihab Shamma 進行多次討論。其中談到各語音資料庫之取得不易，在驗
証想法或演算法時倍感困難，例如 NIST（美國國家標準局）的 Speaker 
Recognition Evaluation（SRE）資料庫。 Dr. Shamma 隨即表示他有一位博士
生也在進行有關語者辨識的研究，或許可以將他們現有的 SRE 2008 資料庫與
我們實驗室分享，並隨即連絡NIST機構以了解其可能性。雖然最後因NIST 反
對而未能取得其資料庫的備份，但與 Dr. Shamma 確立了將來在資料庫分享上
的共識，相信對未來之相關研究有相當幫助。 
此次交流另一成效是與之前合作的 Dr. Elhilali 討論即將投稿之期刊論文
（Neural Basis of Timbre of Musical Instruments），以達與國際接軌之目的。 
98年度專題研究計畫研究成果彙整表 
計畫主持人：冀泰石 計畫編號：98-2221-E-009-092- 
計畫名稱：運算式聽覺場景分析於語音分離之研究 
量化 
成果項目 
實際已
達成數
（被接
受或已
發表） 
預期總達
成數(含實
際已達成
數) 
本計
畫實
際貢
獻百
分比
單位
備註（質化說明：如數個計畫共
同成果、成果列為該期刊之封面
故事...等） 
期刊論文 0 0 100%  
研究報告 /技術
報告 0 0 100%  
研討會論文 0 0 100% 
篇
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 5 5 100%  
博士生 2 2 100%  
博士後研究員 0 0 100%  
國
內 
參與計畫人
力 
（本國籍） 
專任助理 0 0 100% 
人次
 
期刊論文 0 1 100%  
研究報告 /技術
報告 0 0 100%  
研討會論文 1 2 50% 
篇
＇＇＇＇＇＇＇＇Spectro-Temporal 
Modulations for Robust Speech 
Emotion 
Recognition＇＇＇＇＇＇＇＇ in 
Best Student Paper Award Shortlist 
of InterSpeech 2010 Conference 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國
外 
參與計畫人
力 
（外國籍） 
專任助理 0 0 100% 
人次
 
 
