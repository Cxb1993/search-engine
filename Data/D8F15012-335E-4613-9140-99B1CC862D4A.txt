I 
Abstract 
 
Intelligent video surveillance system is discussed for years and applied in many areas, like 
home care systems, security in the public place and so on. Due to this technique development, it 
can lower down the product cost and decrease the error judgment by human.  
In this NSC project, we focused on the home care system which can analyze the human 
activities automatically. As the growing number of the elder population, more and more people 
cannot take care of their parents or the elderly all the time. There is definitely a demand to build a 
system with the ability to watch the elderly and alarm people when they have abnormal behaviors, 
such as falling down to the ground. In our concept we first want to recognize the human postures, 
so we set a CCD camera to grab the image containing the elderly and environment. And then we 
separate the person from the environment using the background subtraction method. After the 
object is subtracted, some parameters from the silhouette will be extracted and viewed as the input 
of our classification system. As for the classification system, we use the adaptive fuzzy rule-based 
system. It can generate the fuzzy rules automatically according to the feature parameters we extract, 
and give a good performance in classifying the postures.   
 
Keywords: Human Activities, Background Subtraction, Adaptive Fuzzy-Rule Based 
Classified System 
 
III 
Table of Contents 
Abstract I 
中文摘要 II 
Table of Contents .............................................................................................................................. III 
List of Figures ..................................................................................................................................... V 
List of Tables ....................................................................................................................................VII 
1 INTRODUCTION ............................................................................................................................1 
1.1 Motivation ...................................................................................................................... 1 
1.2 Purpose of Research ....................................................................................................... 2 
1.3 Overview of System Architecture .................................................................................. 3 
2 PRELIMINARIES ...........................................................................................................................5 
2.1 Introduction of Human Activities Identification ................................................................ 5 
2.2 Modeling of Human Body ................................................................................................. 5 
2.2.1 Model Based Approaches ....................................................................................... 6 
2.2.2 Non-Model Based Approaches ............................................................................. 10 
2.3 Human Activities Recognition Approaches ..................................................................... 12 
2.3.1 State-Space Approaches ........................................................................................ 12 
2.3.2 Template Matching Approaches ............................................................................ 16 
2.3.3 Silhouette Based Method ...................................................................................... 17 
3 IMAGE PROCESSING AND FEATURE EXTRACTION ........................................................23 
3.1 Moving Object Segmentation .......................................................................................... 24 
3.1.1 Block Diagram for Background Subtraction......................................................... 24 
3.1.2 Auto-Adjust Threshold Algorithm ........................................................................ 25 
3.2 Features Extraction........................................................................................................... 27 
3.2.1 Definition of Postures ........................................................................................... 28 
3.2.2 Definition of Feature Parameters .......................................................................... 30 
4 ADAPTIVE FUZZY RULE-BASED CLASSIFICATION SYSTEMS .....................................37 
4.1 Fuzzy Rule-Based Classification Systems ....................................................................... 37 
4.1.1 Fuzzy Rule Generation Procedure ........................................................................ 38 
4.1.2 Fuzzy reasoning .................................................................................................... 40 
4.1.3 Learning Algorithm ............................................................................................... 42 
4.2 Apply the AFRBCS in Posture Recognition ................................................................ 43 
5 EXPERIMENT AND RESULTS ..................................................................................................55 
V 
List of Figures 
 
Figure 1.1 Procedure of human posture recognition. .......................................................................3 
Figure 1.2 Procedure of human postures recognition. .....................................................................4 
Figure 2.1 Classification of human motion analysis [3] ...................................................................6 
Figure 2.2 A stick-figure human model [12] ......................................................................................7 
Figure 2.3 A 2D contour human model [13] ......................................................................................8 
Figure 2.4 A volumetric human model [23] ....................................................................................10 
Figure 2.5 Detecting and Tracking Human “blobs” with The PFINDER System [30] ................. 11 
Figure2.6 Hidden Markov Model (HMM) ......................................................................................14 
Figure2.7 Example of MEI and MHI [43] .......................................................................................17 
Figure 2.8 Segmentation of a grey-level image through the traditional-snake (a) and 
GVF-snake (b).Comparison between the final shape of the GVF-snake and the real 
human shape (c). ..............................................................................................................18 
Figure 2.9 Left image shows the active contour at the end of the deformation. Right graphics 
represent the relative Euclidean distance and histogram of radial projections. .......19 
Figure 2.10 Target pre-processing. A moving target region is morphologically dilated (twice) 
then eroded. Then its border is extracted. ....................................................................20 
Figure 2.11 The boundary is “unwrapped” as a distance function from the centroid. This 
function is then smoothed and external points are extracted. .....................................21 
Figure 3.1 Moving object segmentation diagram. ..........................................................................24 
Figure 3.2 (a) Background Image (b) Object Moving Image (c) Segmentation Result ..............25 
Figure 3.3 Histogram of AD values, x axis is the AD value of total image and y axis is the 
counter number .................................................................................................................................26 
Figure 3.6 Stand Postures .................................................................................................................28 
Figure 3.7 Sit Posture ........................................................................................................................29 
VII 
List of Tables 
 
Table 4.1 Generating Rules Index Form. ....................................................................................... 47 
Table 4.2 Training Patterns for Stand Postures .............................................................................. 48 
Table 4.3 Rules for Recognizing Standing Postures (the first 15 terms). ...................................... 49 
Table 4.4 Each   classβ Value of Rule 2060. .................................................................................... 49 
Table 4.5 Five postures and its corresponding feature parameters. ............................................... 50 
Table 4.6 Training Vectors vs.  max( ( )* ) p classCFμ x Values. ........................................................ 51 
Table 5.1 List of experiment equipment. ....................................................................................... 55 
Table 5.2 Hit rate for the demo images. ........................................................................................ 66 
 2
dangerous.  
The home care systems are expected to provide the functions that can monitoring the people 
inside the house and alarm the watcher when people in some dangerous situations. To give more 
applications even the human postures can be classified is also needed added. In our approach we 
want to establish a simple system with a PC and a CCD camera only to get the goal. More 
application it may be used in the other regions such as the robotics or some artificial science.  
 
1.2 Purpose of Research 
 
In the human action recognition part there are many tasks that are hard to solve. There are 
two main problems, one is the image processing part and the other is how to define the human 
action. 
In discussing the image processing, the images shot by CCD camera are usual including the 
moving object and the background. So how to subtract the object we want is a main task dealing 
with in our system. After getting the object the human action is defined in what descriptions is the 
next problem. As for the human action there are many words expressed in the similar way like 
action, behavior, motion, gesture, posture, deed, and movement. For simply dividing, one is the 
individual static frames like gesture and postures, and the other is continuous dynamic frames 
like action and behavior.  
Our research provides a solution to establish a home care system with the images processing 
and human action recognition. The image processing part will use a easy concept of background 
subtraction to complete, and the human action recognition here is focus on the static frame 
discussion. That is we mainly discuss on the postures recognition parts. The postures part is also 
can be view as the basic of the human behavior recognition, as you can recognize the human 
postures the human behavior will be more easily defined.  
 4
 
Figure 1.2 Procedure of human postures recognition.  
 
In figure 1.2, the vectors Xp are composed of the parameters we get from the procedure 
before, and they are viewed as the systems input. In the adaptive fuzzy rule based classified 
system, it is based on the original fuzzy rule based method with a little change in the generating 
fuzzy rules. The detail will be introduced in the following sections.  
 6
The difference between the two methodologies is in the process of establishing feature 
correspondence between consecutive frames. Methods which assume a priori shape models match 
the 2D image sequences to the model data. Feature correspondence is automatically achieved 
once matching between the image and the model data is established. As no a priori shape models 
are available, correspondence between successive frames is based upon prediction or estimation 
of features related to position, velocity, shape, texture, and shape. Conventionally, the 
representation of human body maybe uses the stick figures, 2D contours, or volumetric model. 
Section 2.2.1 and 2.2.2 would respectively discuss model based and non-model based approaches. 
Figure 2.1 shows the classification of human motion analysis. 
 
 
Figure 2.1 Classification of human motion analysis [3] 
 
2.2.1 Model Based Approaches 
 
 Model-based approaches build the body representation by fitting to the image data the 
predefined parameter values of a parametric body model. This approach can efficiently integrate 
shape knowledge and visual input, and are better for high-level identification of complicated 
motions. However, model-based approaches usually require additional processing steps of model 
selection and parameter estimation to fit the model to a given visual input. Addition of a new 
Human Motion Analysis
Non-Model Based Approaches Model Based Approaches 
Stick figures 2D contours Stick figures 2D contours Volumetric 
 8
configurations, given the known 2D projection, and required accurate extraction of 2D stick 
figures. 
The pioneer work of O’Rourke and Badler [27] used a volumetric model consisting of 24 
rigid segments and 25 joints. The surface of each segment is defined as a collection of 
overlapping sphere primitives. Sato et al. [28] developed a different approach that treats a moving 
human as a combination of various blobs of its body parts. Leung and Yang [13] applied a 2D 
ribbon model to recognize poses of human performing gymnastic movements. The emphasis of 
their work is to estimate motion solely from the outline of a moving human subject. The system 
consists of two major processes: extraction of human outline and interpretation of human motion. 
The 2D ribbon model is comprised of two components, the “basic” body model and the 
“extended” body model. The basic body model outlines the structural and shape relationships 
between the body parts, shown in Figure 2.3.  
 
 
Figure 2.3 A 2D contour human model [13] 
 
The extended model consists of three patterns: the support posture model, the side-view 
kneeling model, and side horse motion model. A modified edge detection technique was 
 10
 
Figure 2.4 A volumetric human model [23] 
 
All of these approaches must match each real image frame to the corresponding model, 
which represents the human body structure at an abstract level. This procedure is itself non-trivial. 
The complexity of the matching process is governed by the number of model parameters and the 
efficiency if human body segmentation. When fewer model parameters are used, it is easier to 
match the feature to the model, but more difficult to extract the feature. For example, the stick 
figure is the simplest way to represent a human body, and thus it is relatively easier to fit the 
extracted lines into the corresponding body segments. However, extracting a stick figure from 
real images needs more care than searching for 2D blobs or 3D volumes. 
 
2.2.2 Non-Model Based Approaches 
 
 As no predefined shape models are assumed, heuristic assumptions, which impose 
constraints on feature correspondence and decreasing search space, are usually used to establish 
the correspondence of joints between successive frames. The approaches of non-model based 
human motion analysis are applicable to more diverse situations. However, these approaches are 
sensitive to noise in general, because they lack any mechanism to distinguish noise from signal in 
 12
features to posture data. In the work proposed by Brand [34], the paths through the 3D state space 
obtained through training are modeled using an HMM where the states are linear paths modeled 
by multivariate Gaussians. As in [35] the moments (central) are found by synthesizing various 
poses. The moments are associated to the HMM. Altogether a sequence of moments is mapped to 
the most likely sequence of 3D poses. Obviously this approach is either offline or includes a 
significant time-lag, but it has the ability to resolve ambiguities using hindsight and foresight. 
 
2.3 Human Activities Recognition Approaches  
 
 For human activity or behavior recognition, most efforts have been concentrated on using 
state-space approaches [36] to understand the human motion sequence [37, 38, 39, 40]. Another 
approach is to use the template matching technique [41, 42, 43] to compare the feature which is 
extracted from the given image sequence to the pre-stored patterns during the recognition process. 
There are still some researches in discussing how to get the human feature from the silhouette 
image, like image skeletonization [5, 29], active contours based [1], and so on.  
The approaches which are related to human action recognition will be surveyed and 
introduced in this section. Besides, the recognition of human behavior is our future desired 
research. Some approaches of human behavior recognition will be presented below. 
 
2.3.1 State-Space Approaches 
 
Approaches using state-space models define each static posture as a state. These states are 
connected by certain probabilities. Any motion sequence as a composition of these static poses is 
considered a tour going through various states. Joint probabilities are computed through these 
tours, and the maximum value is selected as the criterion for classification of activities [3]. Most 
 14
 
Figure2.6 Hidden Markov Model (HMM) 
 
In order to define an HMM completely, following elements are needed. 
 N: the number of states in the posture graph. The model state at time t is denoted as qt , 
1≦qt≦ N , 1≦t≦ T. 
 T : length of the observation sequence. 
 S={ Si | 1≦i≦ N }：a set of model-states; if the state is k at time t, it is denoted as qt = Sk . 
 M：the number of observation model-states. 
 V={ vi | 1≦i≦ M }：a set of observation model-states. 
 A={ ija | 1≦i,j≦ N }：a state transition probability distribution, where   ija =P(qj at n+1 | 
qi at n ).               
 B={ bj(k) | 1≦j≦ N and 1≦k≦M } : an observed state probability distribution where 
bj(k)= P(vk at t | qt = Sj ). 
 Π={πi | 1≦i≦ N }: an initial state probability distribution , where  
   πi = P(qj = Sk). 
 λ = (A,B, π) : HMM parameter. 
 O={O1,O2,…,OT} : observable symbol sequence. 
 
The complete parameter set λ of the discrete HMM is represented by vector π and matrices A 
State 1 
State 2 State 3 
 16
2.3.2 Template Matching Approaches 
 
The template matching approaches analyze the individual frames first and then combine the 
results. The motion magnitude in each cell is summed, forming a high dimensional feature vector 
used for recognition. To normalize the duration of the movement, they assume that human motion 
is periodic and divide the entire sequence into a number of cycles of the activity. Motion in a 
single cycle is averaged throughout the number of cycles and differentiated into it fixed number 
of temporal divisions. Finally, activity recognition is processed to use the nearest neighbor 
algorithm. 
 In an early research, Herman [52] utilized the template matching of stick-figures of a given 
frame to analyze different poses of a person. The emotions and actions at a given frame based on 
the person’s pose are inferred. His stick figure was built by manually locating body parts, and he 
analyzed individual frames separately, without considering the interrelations between the frames 
in a sequence. Akita [53] used template matching of silhouette images to recognize different 
motions of a person in tennis play. Recent work by Bobick and Davis [41], they follows the same 
vein, but extracts the motion feature differently. They interpret human motion in an image 
sequence by using motion-energy images (MEI) and motion-history images (MHI). The example 
can see the figure 2.7. The MEI is binary images containing motion blobs. The motion images in 
a sequence are calculated via differencing between successive frames and then threshold into 
binary values. The MEI ( ),,( tyxET ) is combined by these motion images in time and is defined: 
 
    
1
0
( , , ) ( , , )T
i
E x y t D x y t i
τ −
=
= −U                                     (2.1) 
 
where D(x, y, t) is a binary image sequence indicating regions of motion, τ is critical in defining 
the temporal extent of a movement.  
 18
the next activities identifications. It means first the image subtractions for the moving object is all 
needed to be done. As the silhouette getting the next procedure is divided into some different 
method. [1] efforts the active contours features to represent the human postures, [2] take the 
Fourier transform concept to develop an image Fourier descriptor method. It will be introduced 
one by one. 
 
A. Active Contour Based Method  
In the research mentioned, Buccolieri [1] give a method using the active contours as the 
image features and take this as the NN input to model a systems in recognition the postures. The 
system architecture consists of five sequential modules that include the moving target detection 
process, two levels of segmentation process for interested element localization, features 
extraction of the object shape and a human posture classification system based on the Radial 
Basis Functions Neural Network. Moving objects are detected by using an adaptive background 
subtraction method with an automatic background adaptation speed parameter and a new fast 
Gradient Vector Flow snake algorithm for the elements segmentation is proposed. 
The GVF-snake method can separate the object as the figure 2.8, its detail derivation is not 
mentioned here. 
 
 
Figure 2.8 Segmentation of a grey-level image through the traditional-snake (a) and GVF-snake (b).Comparison 
between the final shape of the GVF-snake and the real human shape (c). 
 
 20
running, and even potentially, the target S gait. Unlike other methods, this does not require an a 
priori human model, or a large number of ‘pixels on target”. Furthermore, it is computationally 
inexpensive, and thus ideal for real-world video applications such as outdoor video surveillance. 
For getting the ‘star’ shape of a tracking human model it is using some image process from 
segmentation of the moving object and using morphology process like dilation and erosion it 
shows in the figure 2.10. The figure shows the whole preprocess procedure step by step. The 
process dilation is first fitting the hole in the original image then using erosion the make the 
image thinner. Finally grab the contour for the image and finish the pre-process.   
 
 
Figure 2.10 Target pre-processing. A moving target region is morphologically dilated (twice) then eroded. 
Then its border is extracted. 
 
As the pre-process finished, the data process part is then continued. It is shown in the figure 
2.11. The figure shows the main procedure in getting the star skeleton, that is first calculate the 
active contour like before and the take the DFT for the data curve, and by using low pass filter to 
get the low frequency characteristic. Finally do the inverse DFT can get another smooth curve 
and find the characteristic point to represent the apex of the star shape. And the whole diagram is 
shown in the figure 2.11.  
 22
the Fourier coefficients, denoted by 0 1{ , , }nf f −L . 
The coefficients with low index contain information on the general form of the shape and 
the ones with high index contain information on the finer details of the shape. The first coefficient 
depends only on the position of the shape and setting it to zero makes the representation position 
invariant. Rotation invariance is obtained by ignoring the phase information and scale invariance 
is obtained by dividing the magnitude values of all coefficients by the magnitude of the second 
coefficient 0 f . Since after normalization 0  f is always zero and 1  f is always one, we have n−2 
unique coefficients given by: 
 
12
1 1
( , , )nffFD
f f
−= L                                      (2.5) 
 
This descriptor is a shape signature and can be used as a basis for similarity and for retrieval. 
When we deal with Fourier descriptors of length n, we actually use only n − 2 coefficients. It is 
clear that Fourier descriptors describe a shape globally. The finer details are filtered out. Now 
consider two shapes indexed by Fourier descriptors FD1 and FD2. Since both Fourier descriptors 
are (n − 2)- dimensional vectors, we can use the Euclidian distance d as a similarity measure 
between the two shapes: 
 
23
0
( 1 2 )
n
j j
j
d FD FD
−
=
= −∑                                 (2.6) 
 24
3.1 Moving Object Segmentation 
 
In this plan, the method we applied in moving object segmentation is the background 
subtraction. Its main concept is to subtract the current frame from the background frame to 
separate the foreground object. More precisely, we take the difference operation to see the pixel 
value change from these two frames. So the problem now is to determine the suitable threshold 
value to separate the foreground and background. In this plan, the histogram method is used to 
count the difference pixel value and than the threshold value will be decided. More detail will be 
introduced in the next sections. 
 
3.1.1 Block Diagram for Background Subtraction 
 
 
Figure 3.1 Moving object segmentation diagram.  
 
In figure 3.1, it shows the segmentation procedure. In this method the main goal is to get the 
background and foreground image, so at first the CCD camera will automatically take a quick 
 26
(absolute difference), which is defined as： 
( , , ) ( , )  ,where 1 ~ , 1 ~ijAD F i j t B i j i dx j dy= − = = (3.3) 
It means that we first need to calculate the difference value for each pixel in the capture image, 
where dx means the resolution of the x dimension of the image and dy means the resolution of the 
y dimension of the image. After getting the AD value we make a histogram to count this AD 
value. More precisely, we take each pixel absolute difference from (dx , dy)=(1 , 1) to (dx , 
dy)=(480 , 640), and assign a row vector, HISTOGRAM(1, 260), to save this value. The length of 
this row vector is 260 in order to make sure that every pixel’s gray level difference from 0~255 
can be saved. For example, if the AD value is 23 we will put one count in the row vector (1, 23), 
i.e. the value of the HISTOGRAM (1, 23) will be added one. Figure 3.3 shows the histogram 
plot. 
 
Figure 3.3 Histogram of AD values, x axis is the AD value of total image and y axis is the counter number 
As long as we got the histogram, the Rules of Auto-Adjust Threshold will be realized in the 
next four steps. 
1. Find the globally maximal counter number.  
2. Search the first locally minimal counter number that is located at right side of globally 
maximal counter number 
c 
d
e 
 28
determine a man which is lying is not properly used here because the lying pose is the rotation of 
the standing pose. The other method, Active Contour, is a kind of contour based method. It first to 
find out the contour mass center and next to calculate the Euclidean norm between each point and 
the mass center. Then the distance data will be regard as the feature of the image. Above methods 
are quite useful for some special situation, but it usually takes time to process this algorithm. So 
in order to deal with the real time problem, we should take a quick process to find the proper 
feature. In the next paragraph, the method we use will be discussed.  
 
 
3.2.1 Definition of Postures 
 
Actually the obvious human pose in this plan is divided into five kinds of postures such as 
stand, sit, lie, kneel and bend. This section will show this kind of pictures and explain some 
definition. First we show the stand pose as the figure 3.6. 
 
 
Figure 3.6 Stand Postures 
In the standing poses the hands motions may be an effect of the parameter we define in the 
next section. So in this plan the most frequent poses people will act are included and discussed. 
For the purpose to get the real time postures recognition system, besides the standing postures we 
also consider the other four kinds of postures, such as sit, kneel, bend and lie. They are all shown 
 30
 
Figure 3.10 Lie Posture 
 
As getting these postures definition, it is now important to find the different feature 
expression for these different postures. In the next section the feature extraction methods are 
discussed. 
 
3.2.2 Definition of Feature Parameters 
 
In fact, in this plan we take the bounding box of human contour, see as the figure 3.11.  
   
Figure 3.11 Bounding box of the human silhouette and its parameters 
In the figure 3.11, L1 means the left boundary of the bounding box, R1 means the right 
boundary of the bounding box, H1 means the upper boundary of the boundary box and F1 means 
the lower boundary of the bounding box. M1 is calculated by (F1+H1)/2, it means the middle line 
of the bounding box. 
Intuitively, the most difference between the human pose is the ratio of height and width, and 
 32
find the outer bounding box H1, F1 and L1.  
In fact, the hand is rising or not will make the bounding box more complicated. It means that 
when the hand is spread like the figure 3.13, the width of the outer bounding box will become 
larger than the original width, i.e. the ratio also becomes smaller than the original stand pose ratio. 
To accurately determine the real human body boundary in this case we should modify the 
bounding box boundary.  
 
 
Figure 3.13 Human Hand Spread Plot 
 
 
Figure 3.14 Modified bounding box of human silhouette  
 
In further discussion, we modify the figure into figure 3.14. The ML1 and MR1 mean the 
left and right boundary after modifying. The M_Width means the distance between MR1 and 
ML1. The way we find the modified parameters is also using the same histogram way discussed 
 34
 
Figure 3.16 Definition of neck line plot 
In the next paragraph the two postures, kneel and bend will be discussed. Cause of the 
specialty of these two postures, that is the length-width ratio of these two postures are easily 
confused with the sit postures, it is necessary for finding another proper parameters.  
The main concepts of our feature extraction are based on the shape method. That is the 
formation of the postures shape is a kind of index to point out the different postures. Take the 
kneel postures for example, that is show a below in figure 3.17. In the figure the symbol KN_1 
means the parameter to define the kneel-pose line, and the BN_1 means the parameter to define 
the bend-pose line. More concisely, the other two parameters are defined to calculate the features. 
The line index of KN_1 is found by finding the maximum black pixel numbers from the half line 
of lower bounding box to the bottom line of the lower bounding box. It can be drawn as the fig. 
3.17 shown. The meaning of BN_1 line is also very clear seen as the fig. 3.18. In the bend-pose, 
the number of black pixels in the line BN_1 are obviously large than others. For the quick process 
procedure the line of BN_1 are chosen by take the half of the upper bounding box. For recoding 
the two parameters we use the symbol of Pixel(BN_1) and Pixel(KN_1) to stand for the pixel 
numbers of the KN_1 line and BN_1 line.  
 
 36
 
Figure 3.19 All of the Feature Parameters Plot. 
 
By summing all above up, we can derive five main feature parameters for our classifier 
system. It can be summarized as the five equations shown below. Figure 3.19 shows the 
parameters plot for the system. All the five parameters can be derived from the plot, ant the plot 
in other words is the summary of all the plots shown before. So in our processing, every image 
taken by the CCD camera will be through the image subtraction and feature extraction. Then we 
can get the feature extraction vector which has the five components, 
like 1 2 ( , , , , ) o m m k bx x x x x=x represents.  
 38
constructed. 
 
4.1.1 Fuzzy Rule Generation Procedure 
 
Let us consider the training patterns are normalized between the axis [0,1]. For example, if 
the training vectors have the dimension of 1 2X X×  then the dimension is [0,1] [0,1]× . Now 
suppose that we have n training vectors pair 1 2( , , , )  1, 2, ,p p p pmx x x p n= =x L L  which is 
divided into M classes ：class 1（C1）, class 2（C2）Lclass M（CM）. In our research, the training 
vectors have five components like length-width ratio, modified length-width ratioL , and the 
classified categories are stand, sit, kneel, bend and lie. The classification problem here is to 
generate fuzzy rules that divide the pattern space into M disjoint decision areas. For this problem, 
we employ fuzzy rules of the following type： 
1 2
:
       if  is in  and  is in and  is in 
       then classify  as class  with 
       1 , 1 , , 1 , 1
K
ij u
K K K
p i p j pm u
K
p ij u
R
x A x A x A
Cm CF CF
i K j K u K m M
=
= = = =
x
L
L
L
L L L L L
                  (4.1) 
Where the Kij uR L  the rule number, , , ,
K K K
i j uA A AL  are fuzzy sets on the unit interval [0,l], Cm  
means the consequent class (in this plan is five). The K means the numbers of the fuzzy subsets 
in the axis of universe [0,1]. In the plan we set the K equals to 5 like figure 4.4. 
 
Figure 4.1 Plot of membership function of K=5 subsets. 
 40
   1 2( ) ( ) ( )
K K K
CT i p j p u pm
p CT
x x xβ μ μ μ
∈
= ∑   L                                (4.3) 
2.  Find Class X (CX) by 
1 2max( , , , )CX C C CMβ β β β= L  In the plan which is represented as  
 
_ _ tan _ _ _max( , , , , )CX C sit C s d C kneel C bend C lieβ β β β β β=                        (4.4) 
 
If multiple classes take the maximum value in (4.4), the consequent CX of the fuzzy rule 
corresponding to the fuzzy subspace cannot be determined uniquely. Thus, let CX be dummy class 
and the procedure is terminated. Otherwise, CT is determined as CX in (4.4). 
3. Kij uCF L  is calculated by the following equations 
1
  ,  where 
1
K CX CT
ij u M
CT CX
CT
T
CF
M
β β ββ
β ≠
=
−= = −∑∑L
                          (4.5) 
Here if we take an example for a three class i.e. we just classified the pose into pose-sit, 
pose-stand and pose-lie. The CF value will be calculated as below. 
_ _ tan _
_ _ tan _
( )
_   ,  where 
( ) 2
C sit C s d C lieK
ij u
C sit C s d C lie
CF sit
β β β βββ β β
− += =+ +L     (4.6) 
Take more consideration, the meaning of 1 2( ) ( ) ( )
K K K
CT i p j p u pm
p CT
x x xβ μ μ μ
∈
= ∑   L  is to see 
the partitions of each pose in the fuzzy subsets. Then compare and find the maximum value to 
decide the rules which is guiding the decision of this pose. It is described as the equation (4.4). 
CF value are the index of fuzzy if-then rules, it represents how certain the rules in classifying the 
poses. After the parameters are getting by us the next step is to the main classified process i.e. the 
fuzzy reasoning parts. 
 
4.1.2 Fuzzy reasoning  
 42
{ }1 1max , , , | 1CX C C CT T Mα α α α= =L L                            (4.9) 
By combining these equations the classified problem can be easily done. When multiple 
classes take the maximum value of CTα , the classification of the unknown pattern will be rejected 
in this procedure. 
4.1.3 Learning Algorithm 
 
So far the classified system is almost finished except the miss classified part. The fuzzy 
reasoning part although can almost solve the classified problem but the blurred data. It means that 
it can be misclassified in the follow steps if we don not add the learning algorithm. To adjust the 
grades of certainty of the fuzzy rules, we use the following error correction-based learning 
procedure. 
 
1.  When px  is correctly classified by .
K
ij uR L  
_ _ 1(1 _ )K K Kij u ij u ij uCF NEW CF OLD CF OLDη= + −L L L                       (4.10) 
 
2. When px  is misclassified by .
K
ij uR L  
_ _ 2* _K K Kij u ij u ij uCF NEW CF OLD CF OLDη= −L L L                         (4.11) 
 
Where 1η and 2η  are learning constants. Generally, because the number of correctly 
classified patterns is much larger than that of misclassified patterns, the grade of certainty of each 
fuzzy rule tends to be increased to its upper limit (i.e., 1Kij uCF =L ) by (4.10) if we choose the 
proper 1η . In other way, the misclassified training patterns will be narrow down by choosing the 
proper  2η . By this assumption, we take the value for these tow constants  0 1 2 1η η< <  . A 
 44
in the plot it will show just only two dimensions to explain the concepts. In figure 4.6 x axis 
means the first element of the training vectors that is the original length-width ratio, and y axis 
means the second element of the training vectors that is the modified length-width ratio. In this 
figure the circle points represents stand postures, star points represents sit postures, diamond 
points represents lie postures, triangular points represents kneel postures and the pentagram 
means the bend postures. It is easily found that the boundary of these data is not well-defined. 
 
 
Figure 4.3 Training vectors of five postures distribution in the two dimensions (Xo,Xm) plot.  
 
For the kneel postures, parts of them have almost the same partitions with stand postures. It 
means that the tow dimensions can not character all of these five postures, so that’s why the kneel 
ratio and bend ratio are needed. Figure 4.7 will show another two different dimensions plot for 
differing kneel and stand postures. It can be found that kneel ratio can separate kneel posture 
from the other postures.  
So dose the bend posture once again we plot it as the figure 4.8. In the figure we use a 
 46
For generating rules, the data partitions in the fuzzy subsets are calculate. Take the standing 
postures for example, we will show how to generate the fuzzy rules for recognition the standing 
postures. At first we choose 15 training vectors for each pose, and each vector has the 5 elements 
as shown before. In the fuzzy membership functions we defined are distribution in the x axis [0,1], 
so we have to normalize the data into the interval [0,1]. The normalized equation is stated as 
below. 
min max min ( ) /( )normalize = − −x x x x x                                      (4.12) 
First of all as we get the training vectors the generating rules procedure is to calculate the 
CTβ  by using equation (4.3). For convenience to show the CTβ  values, the membership 
functions are assigned some index for them. That is the membership function of “small” we 
assign it as number 1, the membership function of “medium small” we assign it as number 2, and 
so on. The “don’t care” membership function is assigned as number 6. As mentioned before there 
are 56 7776 = rules before vanishing unnecessary dummy rules.  
In table 4.1 the rule 1 means the fuzzy if-then rule which represents as  
1 2 3
4 5
 1:
            If  is small and if  is small and if  is small 
            and if  is small and if  is small  then Class  with Kj ij u
Rule
x x x
x x C CF CF= L
. 
For the same inference the other rules are the same form as shown. As we define the index for 
these membership functions the next is to calculate tans dβ  so that we can use the tans dβ  value to 
design the rules for understanding the stand postures.  
And the tans dβ  can be calculated from the membership function rule by rule, that is we will 
fit the standing training patterns into these membership functions and see how they distribute. In 
the final results we have 675 if-then rules for standing postures, and it is too trivial to explain 
how to generate each rule by the tans dβ  values. So we just take some of these rules for example.  
 
 48
Stand 9 0.352448 0.858483 0.859735 0.228675 0.192778 
Stand 10 0.334406 0.934312 0.935649 0.267355 0.217936 
Stand 11 0.372614 0.988231 0.988933 0.280958 0.605972 
Stand 12 0.483742 1 1 0.333943 0.466719 
Stand 13 0.639042 0.774204 0.773661 0.224579 0.19415 
Stand 14 0.915131 0.674471 0.673494 0.213016 0.208518 
Stand 15 0.984026 0.75206 0.750975 0.276192 0.217523 
Table 4.2 Training Patterns for Stand Postures 
 
In the table 4.3 it shows parts of the nonzero tan  s dβ value terms. Cause of too large number 
of rules generated, table 4.3 just shows the first 15 rules. In the table the bolded sign means the 
values of tan  s dβ are larger than the other  classβ values. Although part of the tan  s dβ values are 
seems very small like Rule 2062, Rule 2065 Rule 2067and Rule 2068 and so on. It means the 
other   classβ values may be zero, so the zero  classβ value-rules may not act in recognizing 
standing postures.  
So in such a system, how to decide which rule is the postures precognition rule is decided by 
the   classβ values. Once we take the table 4.3 for example, as we mentioned before the bolded 
sign means the rules’   classβ  are larger than other postures. Like Rule 2060 the tan  s dβ value is 
equal to 0.168557, and its   sitβ value is equal to 0.0551, and the rest  classβ values are equal to 0, 
so the tan  s dβ value is the biggest of all. Then the Rule 2060 will be defined as the rule to 
recognize the standing postures, that is the rule can be written as  
 
1 2 3
4 5
tan tan 2060
 2060 :
            If  is MS and if  is ML and if  is ML 
            and if  is MS and if  is MS  
            then the posture is Class  with s d s d
Rule
x x x
x x
C CF CF=
. 
 
 50
 
tan
2060
tan
( ) / 4 0.169-(0.055/4) 0.919
( ) (0.169 0.055)
s d sit lie kneel bend
s d sit lie kneel bend
CF β β β β ββ β β β β
− + + += = =+ + + + + . 
 
For this example it can be more easily to explain the meaning of CF values. That is the meaning 
of the value is to see how certain the rule is in recognizing a posture. From the above equation we 
know that if the value of   sit lie kneel bendβ β β β+ + + is bigger than the tan  s dβ value, the CF value 
will become much smaller that is the rule is not efficient enough. For the same method the 
other   classβ values can be determined. As the same procedure all the rules’ CF values can be done, 
and based on these CF values we can do the fuzzy reasoning parts as the section 4.2.2.  
For reasoning part the value  ( )  p CFμ′ ′x   of each rule is needed to be calculated that we can 
use the values and find the  max( ( ) | ) Kp ij u ij uCF R Sμ ∈x L L  value to decide the training vector is 
which posture class. To explain the reasoning procedure we list some tables for it, the table 4.5 
shows the training vectors versus the postures. It is said that the first column shows the postures 
and each row shows the postures’ parameters. In the table 4.6 it shows the values of 
 ( ) |  Kp ij u ij uCF R Sμ ∈x L L  for the five postures, and the bolded signs mean the maximum values 
that can also represent as  max( ( ) |  ) Kp ij u ij uCF R Sμ ∈x L L  .  
 
 1x  2x 3x 4x 5x  
Standing 0.9128 0.9814 0.9815 0.369 0.1847 
Sitting 0.4761 0.3645 0.3634 0.1957 0.0934 
Lying 0.0223 0.3 0.2966 0.7507 1 
Kneeling 0.4741 0.9485 0.9459 0.9652 0.0768 
Bending 0.262 0.199 0.1986 0.0975 0.3266 
Table 4.5 Five postures and its corresponding feature parameters.  
 
 52
postures, 484 rules for sitting postures, 55 rules for lying postures, 403 rules for kneeling postures 
and 232 rules for bending postures. Figure 4.8 to figure 4.12 shows parts of the training postures 
pictures. 
 
 
 
Figure 4.7 Standing Training Patterns Images.  
 
 
 
Figure 4.8 Sitting Training Patterns Images 
 
 
 54
the  max( ( ) )p classCFμ x   values in the table 4.6. More precisely speaking, the value 0.6514 is the 
maximum value of these standing rules for the standing vector, which is the result of the equation 
(4.8). In other words here the values we calculate in the table 4.6 are defined in the equation (4.8) 
that is tan 0.6514 s dα = , t 0.0787 siα = , 0.1186 lieα = , 0.2135 kneelα = and  0.1131 bendα = . By 
using the equation (4.9), the largest   CTα value is tan 0.6514 s dα = so the output pose is standing. 
In the same reason the other four postures can be judged.  
Although most of the postures can be classified by using the first generated rules, there still 
exist few error classifying conditions. Like our classified system, in the first iteration (that is 
without the learning algorithm) there are 2 errors in classifying standing postures, 2 errors in 
classifying sitting postures, 4 errors in classifying lying postures, 1 error in classifying kneeling 
postures and 2 errors in classifying bending postures. So adding the learning algorithm is 
necessary.  
 By using the learning algorithm shown in the above section 4.2.3, we set the learning 
constants  1 0.001 η = and  2 0.1 η = . As the learning procedure it iterates about 20 times to get 
the 100% classified rate. In the next chapter we will introduce the whole system architecture and 
the testing demo pictures also be introduced.  
 56
As a reason for the assumption of the home care system, so the experiment is taken in the 
indoor environment. For the background subtraction method often we will take a more monotonic 
scene and with only one moving object. In the next section it will show the experiment results. 
 
5.2 Experimental Section  
 
For two kind of situation we take two different demo video to explain the result, one is the 
different postures recognition the other is ill-postures recognition. That is in the first demo we 
will show the image and output in recognizing which posture the image is. The second demo part 
will show the ill-posture, when a people is lying down in the floor.  
 
A. General Posture Recognition Demo 
In this part the demo will show the output of each image frame. In the output images, the 
white line index out the outer bounding box and the bottom label shows the postures recognition 
results. The figure 5.1 shows the demo frames one by one, the first 25 frames are the standing 
postures and the next 30 frames are sitting postures and the kneeling postures and bending 
postures. In the figure we just only show parts of the demo images, we cut the main postures parts 
to show the results. 
 58
  
(b) Frames of Sitting and Standing Postures 
 
 60
  
(d) Frames of Kneeling and Bending Postures. 
 
 62
 
 (a) Frames of standing postures. 
 64
 
 (c) Frames of sitting and lying postures. 
 
 66
table 5.2 it shows the hit rate for our classified system. In the figure 5.1 and figure 5.2 we just 
show the part frames from the original demo frames, so in the table 5.2 we will show the total 
frames of these two demo images and the hit rate. From table 5.2 the two demos, when the 
postures are transforming the classified rate will decrease that is the transition of two postures act 
a poor appearance.  
 
Postures Stand Sit Kneel Bend Total 
Demo(1) 47/50=94% 30/33=91% 27/27=100% 15/15=100% 119/125=95.2%
 
Postures Stand Sit Lie Total 
Demo(2) 50/56=90% 34/34=100% 10/10=100% 94/100=94% 
Table 5.2 Hit rate for the demo images.  
 
In this section we show the total system architecture and the demo results for our real time 
human postures recognition. Although cause of the processing time it just take three frames per 
second, the classified rate is also can hold on the 90 percentage. In the general situation that 
people action is not so variable in one second the system will work well.  
 68
Reference List 
 
[1]. F. Buccolieri, C. Distance and A. Leone, “Human Posture Recognition Using Active 
Contours and Radial Basis Function Neural Network”, IEEE conference on Advanced Video 
and Signal Based Surveillance, 2005. AVSS 2005, 15-16 Sept. 2005 Page(s):213–218 
Digital Object Identifier 10.1109/AVSS. 
[2]. Ronald Poppe and Mannes Poel, “Comparison of Silhouette Shape Descriptors for 
Example-based Human Pose Recovery”, Proceedings of the 7th International Conference on 
Automatic Face and Gesture Recognition (FGR’06), 2006. 
[3]. J.K. Aggarwal and Q. Cai, ”Human motion analysis: a review”, Nonrigid and Articulated 
Motion Workshop, 1997. Proceedings, IEEE, Page(s):90 – 102,16 June 1997. 
[4]. G. Johansson, “Visual motion perception”, Sci. Am. 232(6), 1975, 76-88. 
[5]. Hironobu Fujiyoshi, Alan J. Lipton and Takeo Kanade, “Real-Time Human Motion Analysis 
by Image Skeletonization”, IEICE TRANS. INF. & SYST., Vol. E87-D, No.1 January 2004. 
[6]. R. F. Rashid, “Toward a system for the interpretation of moving light display”, IEEE Trans. 
PAMI 2(6), 574-581, November 1980. 
[7]. J. A. Webb and J. K. Aggarwal, “Visually interpreting the motion of objects in           
space”, IEEE Comput. August 1981, 40-46. 
[8]. J. A. Webb and J. K. Aggarwal, “Structure from motion of rigid and jointed objects”, in Artif. 
Intell. 19, 1982, 107-130. 
[9]. S Kurakake and R. Nevatia, “Description and tracking of moving articulated objects”, in 
11 th  Intl. Conf. on Pattern Recognition, Hague, Netherlands, 1992, Vol. 1, pp. 491-495. 
[10]. D. Gavrila and L. Davis, “3D model-based tracking of human upper body movement: a 
multi-view approach”, In Proceedings of Int’l Symposium on Computer Vision, pages: 
253-258, 1995. 
 70
[22]. R. Jain and H. H. Nagel, “On the analysis of accumulative difference pictures from image 
sequences of real scenes”, IEEE Trans. on PAMI, 1(2): 206-214, 1979. 
[23]. D. Hogg, “Model-based vision: A program to see a walking person”, Image and Vision 
Computing 1(1), 5-20, 1983. 
[24]. K. Rohr, “Towards model-based recognition of human movements in image sequences”, 
CGVIP: Image Understanding, 59(1): 94-115, 1994. 
[25]. D. Marr and H. K. Nishihara, “Representation and recognition of the spatial organization of 
three dimensional shapes”, In Proc. R. Soc. London, volume B, pages 269-394, 1978. 
[26]. K. Rohr, “Human Movement Analysis Based on Explicit Motion Models”, chap 8, pp. 
171-198, Kluwer Academic, Dordrecht/Boston, 1997. 
[27]. J. O’Rourke and N. I. Badler, “Model-based image analysis of human motion using 
constraint propagation”, IEEE Trans. PAMI, 2: 522-536, 1980. 
[28]. K. Sato, T. Maeda, H. Kato, and S. Inokuchi, “CAD-based object tracking with distributed 
monocular camera for security monitoring”, Proc. 2 nd  CAD-Based Vision Workshop, 
Champion, PA, February 1994, pp. 291-297. 
[29]. Robert T. Collins, Alan J. Lipton and takeo Kanade, “A System for Video Surveillance and 
Monitoring”. 
[30]. C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland, “Pfinder: real-time tracking of 
the human body”, Trans. Pattern Anal. Mach. Intelligence 19(7), 780-785, 1997. 
[31]. T. B. Moselund and E. Granum, “Multiple cues used in model-based human motion capture”, 
in The Fourth International Conference on Automatic Face and Gesture Recognition, 
Grenoble, France, March 2000. 
[32]. T. Darrell, P. Maes, B. Blumberg, and A. P. Pentland, “A novel environment for situated 
vision and behavior”, in Workshop for Visual Behaviors at CVPR-94, 1994. 
[33]. A. Nakazawa, H. Kato, and S. Inokuchi, “Human tracking using distributed video systems”, 
in International Conference on Pattern Recognition, 1998. 
 72
[44]. J. K. Aggarwal and Sangho Park, “Human Motion: Modeling and Recognition of Actions 
and Interactions”, 3DPVT04 (640-647). IEEE Abstract. IEEE Top Reference. 0412 BibRef, 
2004. 
[45]. A. Wilson and A. Bobick, “Parameter Hidden Markov Models for Gesture Recognition”, 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(9), 1999. 
[46]. 張意政, “Non-rigid Motion Analysis for Human Body”, 國立清華大學電機工程研究所博
士論文, 1999. 
[47]. C. Becchetti and L. P. Ricotti, “Speech Recognition Theory and C++ Implementation”, New 
York: Wiley, 1999. 
[48]. L. Rabiner, “A Tutorial on Hidden Markov Models and Selected Applications in Speech 
Recognition”, Proceeding of the IEEE, vol. 77, No. 2, February 1989. 
[49]. M. Shah and R. Jain, “Motion-Based Recognition”, chapter 9, pages 201-226. Kluwer 
Academic Publishers, 1997. 
[50]. B. Min, H. Yoon, J. Soh, Y. Yang, and T. Ejima, “Visual recognition of static/dynamic 
gesture: Gesture-driven editing system”, Journal of Visual Languages and Computing, 10: 
291-309, 1999.  
[51]. J. Yang, Y. Xu, and C. S. Chen, “Human action learning via hidden markov model”, IEEE 
Transactions on Systems, Man and Cybernetics, pages 34-44, 1997. 
[52]. M. Herman, “Understanding body postures of human stick figures”, PhD plan, University of 
Maryland, 1979. 
[53]. K. Akita, “Image sequence analysis of real world human motion”, Pattern Recognition, 
17(1), 1984. 
[54]. 李乾丞, “Fast Human Posture Recognition by Heuristic Rules.”, 國立台灣大學電機工程
研究所碩士論文, 2006.
 74
可供推廣之研發成果資料表 
; 可申請專利  □ 可技術移轉                                     日期：97 年 1 月 31 日 
國科會補助計畫 
計畫名稱：智慧型居家看護影像監控系統 
計畫主持人：陳永耀 教授 
計畫編號：NSC-95-2221-E-002-234  學門領域：控制學門 
技術/創作名稱 即時姿態辨識監控系統 The Real-time Postures Recognition Monitoring System 
發明人/創作人  陳永耀教授 
技術說明 
中文：在本研究中，我們利用特定特徵變數的定義及採用邊界分割
處理人形姿態的特徵取樣問題，再配合上自適性模糊控制的演算法
成功的將其適用於一般性的姿態辨識，並解決ㄧ些較不易分辨姿態
的問題，以及在因背景或外界干擾所產生辨識雜訊的情況下更具強
健性。藉由系統的學習功能，讓我們可將其提高效能，將影像處理
達到即時監控系統的要求。在本計劃中我們成功了實踐一個即時的
辨識監控系統。 
英文：We present a real time postures recognition monitoring system. It 
has the potential to recognize the general postures and gives a good 
classified hit rate. In the plan, we first use some concepts to grab the 
human bounding box to get the feature parameters, like some ratio of 
the body length and width. We then present a method based on the 
fuzzy if-then rules systems to modify how the class distribute. In our 
method, the data with close characteristic may still have some 
boundary to separate them. That is with our method the auto generating 
rules can apply to this kind of classified problem and solve it well. The 
adaptive fuzzy rule based method can automatically adjust the grades 
of certainty of fuzzy rules. The method also has the learning ability to 
improve the error situation that is the ability to correct the errors. 
Finally we establish a real time system for recognizing the postures. 
可利用之產業 
及 
可開發之產品 
居家監控系統、居家安全及看護系統、金融保全系統、一般消費性
電子產品等等。 
技術特點 
採用一般影像攝影機單機，無須額外感測器（三維加速規、壓電器
或是壓力規），系統硬體成本具競爭力。且裝設架構簡易方便，體
積小維護容易，且不影響裝設地點原來之規劃及作息。 
推廣及運用的價值 
可輕易快速將技術移植於居家監控系統、居家安全及看護系統、金
融保全系統、一般消費性電子產品等等領域應用上。 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位
研發成果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
本次大會會議相關主題分為五大組別，包括: 
1. Group 1: Medical Ultrasonics 
Vice Chair: Stanislav Emelianov: University of Texas at Austin, USA 
2. Group 2: Sensors, NDE, and Industrial Application 
Vice Chair: Jafar Saniie: Department of Electrical & Computer Engineering, Illinois Institute 
of Technology, USA 
3. Group 3: Physical Acoustics 
Vice Chair: Yook‐Kong Yong, Rutgers University, USA 
4. Group 4: Surface Acoustic Waves 
Vice Chair: Peter Smith, McMaster University, Canada 
5. Group 5: Transducers and Transducer Materials 
Vice Chair: Scott Smith, GE Global Research, USA 
 
同時每個組別都安排了多場邀請演講: 
 
GROUP 1: 
z Lihong Wang, "High‐Resolution Photoacoustic Tomography" 
z Bjorn Angelsen, "New Methods of Nonlinear UltrasoundImaging" 
z Evan C. Unger, "Therapeutic Applications of Microbubbles ‐Sonothrombolysis and Beyond" 
z Juin‐Jet Hwang, "Portable Echo Imaging System" 
z Kjell Kristoffersen, "Real‐Time 3D Cardiac Imaging With2D Array Transducers" 
z Kullervo Hynynen, "Ultrasound Potentiated Therapy" 
z George Sutherland, "Myocardial Strain Rate Imaging" 
z Steven Feinstein, "Contrast‐Enhanced, Ultrasound Imaging of Atherosclerosis" 
z Zahi A. Fayad, "Multimodality (PET/CT, MR, and CT)Imaging of the Atherosclerotic Plaque and 
CMUTs" 
z Rajiv Chopra, "Integrating Ultrasound Transducers With MRI For Therapeutic And Diagnostic 
Applications" 
z Sung Min Rhim, "Piezoelectric Single Crystal for Medical Ultrasound Transducer" 
 
大會於 10月 28日開始，分為 6個 Track，總共 60個 Oral Sessions，6個 Poster Sessions。
第二天早上八點至九點三十分舉行 Keynote演講，大會邀請了 Columbia University的  Prof. 
V. Mow and Prof. E. Konofagou擔任本次大會 President’s Speaker，演講題目為: 
“Cartilage and Osteoarthritis: Biomechanics and Ultrasound” 
內容對生物力學及超音波有精闢之說明。 
 
本次參加會議發表之論文排定在 P3C場次，於 10月 30日發表，論文題目為  – 
1.  Split‐focused  ultrasound  for  breast  tumor  thermal  surgery  with  multidirectional 
heating (P3C‐5) 
2.  Investigation  of  dual  curved  ultrasound  phased  array  for  breast  tumor  thermal 
therapy (P3C‐7) 
 
 
