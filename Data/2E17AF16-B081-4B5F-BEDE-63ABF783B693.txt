 I
目錄 
目錄 .......................................................................  I 
中文摘要 ................................................................... II 
英文摘要 .................................................................. III 
I. 計畫緣由及目的.......................................................... 1 
II. 研究方法與步驟.......................................................... 1 
III. 研究成果............................................................... 13 
IV. 參考文獻............................................................... 21 
可供推廣之研發成果資料表 ................................................... 25 
 
 III
英文摘要 
This project proposes a three-year research plan on “Personalized Expressive TTS 
(Text-To-Speech) via Voice Conversion”. This research plan is composed of personalized term 
detection, speech synthesis of personalized speaking style and voice conversion. The aim of this 
research project is to provide speech synthesis technologies for user’s communication in the 
services of digital care and educational training application. The proposed system can synthesize 
the voice for any target speaker or emotion via the voice conversion technologies, and offer the 
sound of a target person or emotion for the user. These technologies combine the dialogue system 
and provide the sound of children for the user to mold the environment of elder’s “family” with 
sense of ownership for the elder care application. It can also provide the synthesized sound for 
home security or the children care application. The details are described as follows. 
In the first year, we focus on developing the approach to personalized frequent term 
extraction in the spontaneous speech. The word lattice combined with acoustic and language 
features are extracted to detect the personalized frequent terms. Pitch and duration model can be 
generated separately from the sentence, word and syllable levels via the hierarchical prosodic 
model. Linguistic features are adopted in the hierarchical prosodic model and personalized 
speaking style model. The prototype conversion system combined with the prosodic model and 
the spectral conversion technologies can be constructed.  
Prosody conversion is the main research issue of the second year. The “prosody hierarchy 
structure” can re regarded as the basis of pitch prediction model, and apply “dynamic features” to 
the unit of each hierarchical layer. We describe prosodic units as the supra-segmental units which 
occur in a hierarchy structure and reflect how brain processes speech; the latter preserve time 
correlation between adjacent units and result in more natural connection among each conjunction 
point. Applying this framework to HMM-based speech synthesis system, we can result a better, 
natural sounding speech. 
In the third year, we plan to develop a personalized expressive TTS via voice conversion. 
The conversion function for any target speaker or emotion via voice conversion is trained by a 
parallel speech corpus obtained from a source and a target speakers. Combined with the pervious 
personalized speaking style and prosodic model, the personalized expressive TTS system can be 
obtained. 
By the realization of this research project, an improvement in speech quality, including 
prosody and spectrum, of the personalized expressive speech synthesis will be obtained. The 
accomplishment of this project can be applied to other projects in the integrated project as the 
speech interface.  
 
 
Keywords: Speech Synthesis, Frequent Terms, Prosodic Model, Voice Conversion 
 
 
 2
韻模型(Prosodic Model)，其中包含個人化音韻特徵參數擷取、階層式句法結構，其階層包
含:句子層(Sentence Level)、詞層(Word Level)、單字層(Syllable Level)，藉由各層的細分，
採用音高目標模型(Pitch Target Model)將個人化音韻的音高參數加以擷取，並且透過前個工
作針對句法所做的分析結果，建立斷句估測(Break Prediction)的模型，最終完整建立個人化
音韻參數模型；3)以次音節頻譜的二維特徵值 (Two-Dimensional Eigen-Voice)為基本單元之
頻譜轉換，有鑒於過去傳統以框架(Frame)為基礎進行之頻譜轉換在時間軸上框架與框架之
間的不連續問題，會影響所合成出來的聲音品質，所以本研究提出一個以次音節(Subsyllable)
為單元之特徵值聲音做為基本轉換單元取代過去以框架(frame-based)為主的轉換方式，在時
間軸上的連續性能維持，以次音節為轉換單元(Subsyllable-based)進行頻譜轉換；4)整合上
述三個工作的成果，結合句法分析的結果和語料庫，以及階層式的音韻模型和次音節頻譜
轉換技術，搭配以特徵值聲音為基本單元的頻譜轉換技術，最終建構出針對文字內容分析
個人語者特色，並建立可產生個人化含情緒表達性語流之語音的 TTS 系統(Expressive 
TTS)。本計畫已完成一套能將文字轉換為具有個人化語者特色和表達性語音合成系統，以
下詳述各階段之研究工作。 
第一年度主要目標包括：1.)利用統計方法自動從訓練語料中萃取出個人習語，並將之
分類；2.) 被歸類為贅語之個人習語，利用贅語插入模型將它放入文章中最合適的位置；3.) 
對於非贅語類的個人習語，設計一同義詞對照表；4.) 藉由同義詞取代與插入贅語，將原始
輸入的文章修飾成帶有目標語者說話風格的文字內容。 
第一階段 
建立個人化慣用語句之句法分析(Syntactic Analysis)系統 
第一階段先建立個人化慣用語句句法分析(Syntactic Analysis)系統，利用所收集之日常
對話語料、詞庫加以整合分析，先從文字上找到每個人說話的習慣方式，並且加以分析後
建立特定語者的個人化句法模型，而第二階段主要在於建立階層式的音韻模型架構，藉由
階層式的架構，將語料從句群到詞組進行分層分類，並且在各階層擷取合適的音高目標模
型(Pitch Target Model)，將各階層的音韻特色和變化加以記錄，最後將個人化的句法模型和
音韻模型加以整合，並且加上頻譜轉換(Spectral Conversion)方式，建立一個具有個人化特
色語者轉換的雛型系統。其轉換的流程示意圖如圖一： 
 
Prosodic Analysis 
Syntactic Analysis
Prosodic 
Modeling
Spontaneous Speech 
Corpus Development
Personalized 
Speaking Style 
Modeling from 
Text Prosodic 
Conversion
Spectral 
Conversion
Converted 
Speech
Spontaneous  
corpus
Text and Speech 
Data Input
Text  Data
Speech  Data
 
圖一、個人化特色語者轉換系統流程圖 
 
 4
 
圖三、找尋慣用語演算法 
 
本研究想要萃取出來的 Idiolect 有兩種類型，一個是 N-gram idiolect，這種 Idiolect 是
adjacent 的，另一個是 phrasal template，表示 not adjacent 的。本研究利用 Xtract 這個演算
法來做 Idiolect extraction。Xtract 是第一個提出來可以萃取出 adjacent 和 not adjacent 
collocation 的方法。它主要的概念是先計算 strength 和 spread，把 two word collocation 先找
出來，第二步驟是利用之前所計算的 two word collocation 再找有沒有兩個字以上的
collocation。最後利用 parsing information 把錯的 collocation 過濾掉。Xtract 的演算法的第一
步驟先把以 headword 為中心 window size 10 的 co-occurred word 都抓出來，然後針對每個 
Co-word 與 Headword pair 都用這種表示法記錄起來，F 是出現的頻率 而 d 是 Co-word 與 
headword 的距離(單位是 word)，所有的記錄可以組成一張表格。可以算出每個 word pair
的 Strength，每一個 headword 會抓出很多 co-word，Strength 目的是把頻率不夠高的 co-word 
濾掉，Fi 是 co-word 與 headword 共同出現的次數，F-head 則是所有與 headword 共同出現
的 co-word 的次數的平均，Sigma-head 是 co-word 次數的標準差，這裡要設個門檻值 T0，，
它要求 co-word 的頻率要高於平均的 T0 個標準差，Strength 是為了確定兩個 word 是不是
有足夠的 recurrent 特性。外要計算的是 Spread Measure，因為同樣的 Co-word 可能會出現
在 windows 的任一個位置，如果它每個位置出現次數都差不多，表示它與 headword 之間
沒有特定的位置關系，所以我們用類似計算變異數的方式，來判斷分散程度，如果 co-word
在某個位罝出現頻率非常高，而在其它位置不是次數零就是非常小，則 spread 會較高，表
示它會出現在 headword 旁的特定位罝，門檻值 T1 就用來判斷 Spread 的高與低，Spread
是為了確定兩個 word 間有足夠的 Rigid combination 特性。 
Strength Measure 定義如下: 
( , ) i headhead co i
head
f fStrength W W σ−
−=        (1) 
where 
1
1 m
head i
i
f f
m =
= ⋅∑  and 2 2
1
( )
m
head i head
i
f f mσ
=
⎛ ⎞= −⎜ ⎟⎝ ⎠∑  
 6
就是個人習語，也有候選答案，和其它的 term, 行是針對每個 term 的上下文, 中間是計算
Term 的 Frequency。 
 
 
圖四、LSA 示意圖 
 
再來是贅語的處理，想要把贅語放到句子中間，大致的想法就是先觀查這個贅語出現
的地方，並記錄起來伴隨它出現的各種有用的 feature，訓練出統計模型，之後輸入一段文
章就可以利用統計模型找到最可能的地方，把贅語放進去，所以有很多方法都可以用來放
贅語，但是想要得到這些贅語的模型，得先想出好的 feature template 或是 decision，然而好
的 feature template 是很難設計出來的，所以本研究中提出一個 Idiolect Insertion Algorithm
將贅語放進句子。 
演算法的步驟如下： 
第一步驟，利用 Mining Association Rule 的方式，自動找到在 window size 內（我們是訂六
句話為一個 window 的大小）能使贅語發生的事件。 
第二步驟，假如在 window 內發現有 Idiolect 應該發生，就從 window 內與 example sentences
內，找到相度最大的兩個句子；如果這個 Idiolect 是在兩句中間的類型，就比較 sentence pair
的相似度。 
第三步驟，對於句中的 Idiolect，找到 example sentence 與 target sentence 前後 syntactic 
structure 一樣的地方，把 Idiolect 放入 target  sentence 中；若是兩句間的 Idiolect ，就直
接放在它們中間。 
以下為整個插入演算法的示意圖，假設文章有 20 句，我們就以 6 句為 window，看看
window 內有沒有讓 Idiolect 發生的 event。假設在第二個 Region 有某 event 使 Idiolect 發生，
就讓 Region 2 裡面每一句都與 Example sentence 比較相似度。 
最後相似度最大的 example sentence 20 與 sentence 5 之間，找到 syntactic structure 一樣
的地方，把 Idiolect 放進去，就完成了這個 region 的 Insertion。 
 
 8
同的類型，由於不同的語篇所要表達的意思不同，在音韻上的變化也會有所不同，因此，
在句子層中，每個句子也有根據不同的句型而有音高上的變化，參考表 12-1 句子將可以分
類為肯定句、疑問句、命令句、、等，例如:疑問句會在尾音有因高上揚的變化、又或者肯
定句具有隨正常說話呼吸方式音高漸低的走勢，不同的句型在音韻上也會有相當程度的不
同，藉由細部的劃分，將可以比語篇層更詳盡的描述更細部的音高變化，往下細分的詞層、
字層，將更細膩的去描繪單字到單音的音高變化走勢，藉由這樣的分層利用音高目標模型
去描述後，並且透過前個工作針對句法所做的分析結果，建立斷句估測(Break Prediction)
的模型，最終完整建立個人化音韻參數模型。 
 
圖六、階層式音韻模型(Hierarchical Prosodic Model)架構圖 
  
第二年工作主要提出了結合韻律階層及動態參數的音高預測模型，以“階層式音韻（音
高的韻律單元）架構＂作為音高預測的基礎，各層的音韻單元考慮“動態參數特性＂；前
者希望改善傳統音韻模型以小單元合成的不足，後者以動態參數生成演算法，保留時間上
的關聯性，使單元和單元之間的連接更加自然，更符合人類講話時的句調走勢變化，藉以
改善基於隱藏式馬可夫模型之合成語音的自然度。另外，在語音訊號的分析上，即將訊號
參數化並取得頻譜以及音高參數，使用的是 STRAIGHT 分析合成演算法，可以得到精確的
基頻參數以及頻譜參數。第一階段先建立個人化慣用語句句法分析(Syntactic Analysis)系
統，利用所收集之日常對話語料、詞庫加以整合分析，先從文字上找到每個人說話的習慣
方式，並且加以分析後建立特定語者的個人化句法模型，而第二階段主要在於建立階層式
的音韻模型架構，藉由階層式的架構，將語料從句群到詞組進行分層分類，並且在各階層
擷取合適的音高目標模型(Pitch Target Model)，將各階層的音韻特色和變化加以記錄，最後
將個人化的句法模型和音韻模型加以整合，並且加上頻譜轉換(Spectral Conversion)方式，
建立一個具有個人化特色語者轉換的雛型系統。其訓練的流程如下圖七： 
 
 10
使用者輸入文字後，經過前端的文字分析後，得到語言學上的資訊並進行韻律停頓的
預測，得到文字所對應的韻律階層架構，並產生(1)文字標記檔以及(2)各層的前後文相關資
訊，以(1)文字標記檔進行聲學及音長參數的預測，並以(2)各層的前後文相關資訊作為各層
音韻參數的預測，並結合各層音韻預測的結果產生對應的音高軌跡。將得到的聲學參數及
音高軌跡，根據對應的音長，得到各個音框的頻譜及音高值，最後經過 MLSA filter（Mel-Log 
Spectrum Approximation filter）得到合成的語音輸出。本年度研究目標在於提出應用韻律階
層及動態參數之音高預測模型方法，應用基於隱藏式馬可夫模型之中文語音合成器效能的
改進。基於 HMM 的語音合成器主要核心技術包括：(1)基於梅爾倒頻譜的聲音合成技術
（vocoding technique），包含了梅爾倒頻譜係數的分析，以及使用 MLSA filter 直接將梅爾
倒頻譜係數合成回語音訊號。(2)從 HMM 模型產生語音參數時，使用考慮參數動態特性的
參數生成演算法。(3)基於 Multi-Space probability Distribution HMM（MSD-HMM），考慮基
頻僅在濁音段有值，而在清音段沒有定義這樣的特性，使參數的維度在濁音段為一，清音
段為零。本研究中，以“Segmental Tonal Phone Model＂作為設計及定義 HMM 音素模型的
方式，此種模型定義的好處是能以最少的模型數，對中文五聲變化作詳盡的描述。為了達
到最佳的狀態合併分裂結果，我們對音素模型，考慮了音素相關、字相關、音韻詞相關、
音韻詞組相關與句相關等五大類問題，接著經由 STRAIGHT 方法進行參數擷取，語音訊號
首先經過固定音框大小的基頻分析，取得基頻數值，在提取頻譜參數時，根據各音框取得
的基頻數值計算基周週期，做快速傅利業轉換（Fast Fourier Transform, FFT）加框長度的適
應性校正，藉以去除因為非週期性震盪對頻譜在時頻上產生的影響，使分析得到更明確、
平滑的頻譜參數（Smoothed spectrum）。 
我們使用兩層（2-tier）的分類回歸樹模型，將前一層的預測結果，作為下一層的預測
基礎，韻律結構預測模型是使用兩層的 S-CART 架構，以 S-CART 作為 PW、PP 的預測模
型，特性如下：(1)以語言學上的資訊作為所有資料點的特徵向量。(2) supervised learning：
資料點有其所屬類別，在此預測模型中僅有兩類，邊界或非邊界。(3)在分裂時的問題集是
屬對每一維度分別找出所有可能的特徵值，以特徵質當作問題，所有的問題形成集合，此
集合產生所有可能的子集合，每個子集合就是一個問題集，最後對所有問題集試做一次分
裂，依據分裂標準找出可達到最佳分裂結果的問題集當作此維度上所使用的分裂問題集。 
(4)分裂標準：使分裂後節點的純度（purity）最大、熵值（entropy）最小，即每個節點中資
料點所屬的類別均相同，即最大化熵值降低的比率（Reduced Entropy Ratio, RER）。 
RER 算法如下： 
 ( )parent i i
i
parent
Entropy Entropy
RER =
Entropy
ω ×∑‐            (6) 
Entropy 的算法如下： 
 
1
= - log( )
iN
j
i j jp pEntropy
=
×∑       (7) 
其中 = ii N Nω  ， N 表示父節點的資料數目， iN 表示第 i 個子節點的資料數目， jP 代表
第 i 個子節點中的第 j 筆資料出現的機率。當 RER 值越大時，代表分裂之後子節點的純度
越高，所以每次分裂以“使得 RER 值最大的問題集＂進行分裂，持續分裂直到達到終止條
件為止（stopping criterion）。(5)終止條件：當要嘗試分裂的節點只剩下唯一或少於一定數
量的資料點時，或是當 RER 小於門檻值（threshold）時，達到終止條件，此一節點不再進
行分裂，並稱此節點為葉節點（leaf node）。(6)每一葉節點代表決策的類別，即決定資料
是否為邊界所在。 
 12
Q 的觀察機率及事前機率在合成之前，我們需要將 Fn 轉換回對應的 Legendre Coefficients ，
才能進一步算回音高軌跡。得到 1 2{ , , , }N=F F F F" ，並產生整個句子的模型 1 2{ , , , }Nλ λ λ= "Λ ，
模型的平均值向量 1 2[ , , , ]N′ ′ ′ ′= "μ μ μμΛ ，共變異數矩陣 1 2[ , , , ]Ndiag= U U U"UΛ ，我們希望找出使
得已知模型Λ產生F的機率最大的Α，亦即使得 ( | )P F Λ 有最大機率值的Α，其中 =F WA。最後
得到 -1=A R r，其中 1R= −′W U W且 1−′=r W U μ。 
 
接著 PW-/SYL-level 音高預測模型在合成端的參數生成，參數之生成示意圖如下： 
′ ′-1 -1 -1-1 W U W) (WU μ)A= R r = (
 
圖十、參數生成示意圖 
如圖，一段長 N 個音韻單元的文字，在文字分析後得到對應的文字訊息 nL ，經過音
高模型的預測取得葉節點的模型 nλ ，並得到模型參數 nF ，即平均值向量 nμ （mean vector）
及共變異數矩陣 nU 組合成的向量，其中 nμ 及 nU 的維度是三倍 Legendre Coefficients 維度
（包含靜態參數及一階、二階動態參數的維度），在合成之前，我們需要將 nF 轉換回對應
的 Legendre Coefficients nA ，才能進一步算回音高軌跡，這樣的轉換法稱作參數生成演算
法。我們將音韻單元參數串接成F ，得到 1 2{ , , , }N=F F F F" ，並產生整個句子的模型
1 2{ , , , }Nλ λ λ= "Λ ， 模 型 的 平 均 值 向 量 1 2[ , , , ]N′ ′ ′ ′= "μ μ μμΛ ， 共 變 異 數 矩 陣
1 2[ , , , ]Ndiag= U U U"UΛ ，我們希望找出使得已知模型Λ產生F的機率最大的Α，亦即
使得 ( | )P F Λ 有最大機率值的Α，其中 =F WA。將 ( | )P F Λ 取對數得到： 
11log ( | ) ( ) ( )
2
1 3                      log log 2
2 2
P
MN π
−′= − − −
− −
F F FΛ μ U μ
U
Λ Λ Λ
Λ
     (9) 
最後我們得到： 
-1=A R r        (10) 
其中 1R= −′W U W 而 1−′=r W U μ。 
 
 14
Precision by Different Strength and Spread
0%
20%
40%
60%
80%
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Strength
Pr
ec
isi
on
 R
ate
s 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
 
圖十二、Precision rate 測試結果 
 
下圖中，在 strength 0.7 spread 0.2 的時候，找到的 Idiolect 有這些，黑體字表示正確的。
所以 Precision rate 有 71.43%，當 spread 升到 0.3-0.4 時，找到的 Idiolect 數量變少了，其中
noise 減少的數量比正確的減少的數量多，因而 Precision 上升到 83.33%，當 spread 升到 0.5
時，找到的 Idiolect 正確的減少了，但 noise 並沒有減少，以至於 Precision 下降了。我們可
以找到一個最佳的 strength 與 spread 值，來使找到的 Idiolect 最多，而正確率最高。 
 
 
圖十三、Precision rate 針對不同參數設定之測試範例 
 
這個實驗是為了測試第三個參數 frequency constraint 對 precision 和 recall 的影響。我
們固定了 strength 與 spread value 在最佳的 0.1。 以陳水扁先生的語料而言，隨著 Frequency 
constraint 愈小，Recall rate 愈高，Precision 愈低。Frequency constraint 愈大，Recall rate 愈
低而 Precision rate 則愈高。在 Frequency constraint 等於 10 的時候，到達一個 compromise
的地方。同樣的實驗步驟，在少量的馬英九語料上，Precision 和 Recall 在 Frequency 
constraint 為 5 時，達到 compromise 的地方。同樣實驗步驟在語料量最大的李敖上，Precision
與 Recall 在 Frequency constraint 為 20 左右達到 compromise 的地方。因此，我們可以得知，
在小量語料，Frequency constraint 可以設小一點，在大量語料，Frequency 設高會有較好的
正確性。 
 16
最後 t-test 的 p-value 為 0.124，大於 alpha，固接受電腦與陳水扁是一樣的假設。 
 
表 II: 贅語插入之主觀式評估結果 
 
 
我們最後是評估同義詞取代的正確率。我們為每一個非贅語的 Idiolect 找到它的一個同
義詞。所以正確率算法是：分子是找到正確的同義詞，所謂正確是真正能放在上下文一樣
的句子中才算。分母是非贅語 Idiolect 的數目，最後乘上 100。我們可以看到三個人平均下
來是快到七成的正確率。正確率不高的原因在於，有些 Idiolect 是含有代名詞的，例如…這、
他、那是，等等，造成找不到同義詞的情況。這些問題，可以用 anaphora resolution 的一些
方法來作。在未來的研究中，將會處理這一個問題，使正確率再提升。 
第一年度所提之方法確能將原始不具有個人風格的文章，轉換為含有目標語者說話風
格之文章。主要目的即透過個人習語的萃取與生成，將原始文章轉換為帶有某語者說話風
格的文字訊息，達到說話風格模型化的目的。利用統計方法自動從訓練語料中萃取出個人
習語，並將之分類，並能將被歸類為贅語之個人習語，利用贅語插入模型將它放入文章中
最合適的位置，對於非贅語類的個人習語，設計一同義詞對照表，藉由同義詞取代與插入
贅語，將原始輸入的文章修飾成帶有目標語者說話風格的文字內容。最後將這個個人化文
字風格語料分析系統的結果與 TTS 系統進行結合，先將一般性的文章，針對目標語者，經
由文字分析，萃取出該特定之目標語者的個人風格之慣用語，將一般性文章轉換為帶有該
目標語者個人語者風格的文章，並且將帶有個人語者風格的文章經由 TTS 系統，搭配頻譜
和韻律模型轉換(Spectrum conversion and prosody conversion)，產生出帶目標語者有個人風
格的語音。 
第二年度研究中，使用的語料來自北京清華大學的語音合成語料庫（TsingHua–Corpus 
of Speech Synthesis，TH–CoSS），此語料庫主要是針對漢語普通話語音合成的研究、開發與
評測，以及語音學研究而設計的漢語語料庫。語料文本主要選自新聞，接下來各節的實驗
及評估中使用的語料為 TH–CoSS 中一位女性語者（FR00）的朗讀語料、陳述句句型。，
我們將 FR00 語料分作三個子集合，分別為各模型的訓練語料（音韻結構預測模型及音高
預測模型）、權重調整語料、以及測試語料。在音韻停頓預測模型效能的評估上，我們使用
三種客觀評估的方式，包括精確率（precision）、召回率（recall）以及正確率（accuracy），
個別的音韻停頓預測模型（PW S-CART 及 PP S-CART）其 precision 以及 recall 如表 III，
 18
 
圖十七、原始音高軌跡（藍虛線）與 U-CART(H)（紅實線） 
 
 
圖十八、原始音高軌跡（藍虛線）與 U-CART(H+D)（紅實線） 
 
在音韻停頓預測模型的評估上，我們採用客觀評估以及主觀評估兩個方向，分別對模
型的效能進行評估，評估分為主觀和客觀評估。客觀評估採用 RMSE 為標準，客觀評估由使
用者評分，由 1~5 分，分數越高表示越好，反之越差。 
使用均方根誤差（Root Mean Square Error, RMSE）作為評估標準，計算目標的音高
軌跡和預測的音高軌跡之間的誤差，其中目標的音高軌跡為 1 2( , ,..., )NX x x x= ，而預測的
音高軌跡為 1 2( , ,..., )NY y y y= ，RMSE 的計算式如下： 
2( )  
 
N
i i
i=1
x y
RMSE(X,Y)=
N
−∑
      (11) 
表 V: 客觀評估 RMSE 值（Hz） 
 
 20
析客觀評估與主觀評估的結果，在客觀評估的結果上，三種使用 CART 的模型
（U-CART(D)、U-CART(H)、U-CART(D+H)）在 RMSE 的數值上並沒有很大的差距，但在主觀
評估時，U-CART(H)在 MOS 及偏好測定上表現不佳，證明動態參數對於模型預測結果、各個
單元之間的串接流暢度上，確實有很大的影響，受測者在語音自然度的評分上，更在意的
是語音整體串接流暢的表現，任何在句中突高或突低的音高變化都會對自然度有很大的影
響。 
本計畫第二年度主要完成應用階層式韻律架構及動態參數之音高預測模型，改善基於
隱藏式馬可夫模型之語音合成器，將韻律結構以階層式的架構拆解，根據階層式的韻律架
構作音韻參數的量測，並導入動態參數於各韻律階層參數空間，以保留真實語音在時間序
列上的變化及關連性；在各層音韻模型的訓練上，在音韻詞及音節階層使用分類回歸樹模
型，在音框階層使用隱藏式馬可夫模型。各層經過模型的預測及參數生成演算法的計算後，
得到對應的音韻參數。 
根據實驗評估的結果，證明動態參數對於模型預測結果、各個單元之間的串接流暢度
上，確實有很大的影響，受測者在語音自然度的評分上，更在意的是語音整體串接流暢的
表現，任何在句中突高或突低的音高變化都會對自然度有很大的影響。顯示本研究提出的
方法，對於改善原有 HMM 音高預測模型上，有不錯的表現。最後，我們有以下幾點可供
未來繼續深入探討並改善的方向，本研究由於高層語音資訊的不足，在音韻階層架構最上
層只考慮到 PW，未來在實驗語料的收集上，可以納入更長的句組或篇章，得到更上層的
韻律單元對韻律訊息的影響，對階層的韻律架構有更完整的描述。在整個的語音韻律上，
目前只針對了最具影響的音高軌跡做預測，其他如音強、音節時長等韻律訊息，亦是未來
的一些研究方向。 
第三年主要完成將前兩年之音韻轉換 (Prosodic Conversion)和頻譜轉換 (Spectral 
Conversion)系統整合，並且對音韻模型 (Prosodic Model)加入更多的個人化參數特徵
(Feature)，最終目標在於完成建立一個具有個人化特色語者轉換的系統，主要在於整合前兩
年的系統，和文字轉語音系統結合(Text-to-Speech：TTS)，除了能從文字上去合成個人語者
特色的聲音外，也要能從使用者提供所需要的指定語者聲音進行轉換，並且於本年度對整
合後的系統進行效能評估，針對系統的結構和效能進行改進。整個數位教育訓練及照護系
統整合後，還必須評估所合成出來的語音，對於受照護者而言，評量是否能產生可以接受
語音，以及所謂個人化的語者特色是否有達到受照護者的需求，這幾點將關鍵到本系統在
合成個人化的語音時，能否提供實質的效果及應用。 
藉由之前所建立的音韻模型(Prosodic Model)，以及使用頻譜穩定區段為基本單元做頻
譜圖轉換的語音轉換(Voice Conversion)方法，加以整合。完成建立日常生活用語的語料庫，
平衡語料(Balanced Corpus Collection)的收集後，由於日常生活的對話，並非一般文章，或
者照稿宣讀的演講，所以在收集上必須設計過，由長時間的記錄方式，針對特定語者進行
收集其日常生活的對話，自然的對話並不像文章般，具有完整的句法結構，而是具有個人
化特色的單句夾雜個人的口頭禪或者斷句特色以及呼吸方式所組合而成，所有的句群有可
能並沒有明顯的分段，只有相同的主題和意義，所以在收集到這樣的日常生活對話後，還
必須進行人工標記(Tagging)和句群分類的前處理工作。我們目前採用的是即席演講的語
料，因為即席演講或者訪談性節目的內容，並非有預定的文字稿，對話者只有概括的意念，
而無特定要說的文字，所以每個人將會有每個人的詮釋方式，因此，我們就可以藉由這樣
的語料，來收集並且整理出，所謂的“個人化＂語者特色，然後結合句法分析(Syntactic 
Analysis)和音韻分析(Acoustic Analysis)的方法，並且在句法分析中，先針對文字部分，利用
Maximum Entropy 去找出具有個人化語者特色的慣用語位置，並且訂出 Rule Template，建
構出具有個人化特色的文字語者模型後，進行預測，以期能做到在拿到一段正常的對話時，
 22
Using Structural Syntactic Cost,” IEEE Trans. Audio, Speech, and Language Processing, 
Vol. 15, No. 4, pp.1227~1235, May, 2007. 
[11] C.H. Wu, C.C. Hsia, T.H. Liu, and J.F. Wang, “Voice Conversion Using 
Duration-Embedded Bi-HMMs for Expressive Speech Synthesis,” IEEE Trans. Audio, 
Speech, and Language Processing, Vol. 14, No. 4, pp.1109~1116, July, 2006. 
[12] C.C. Hsia, C.H. Wu, and J.Q. Wu, “Conversion Function Clustering and Selection Using 
Linguistic and Spectral Information for Emotional Voice Conversion,” IEEE Trans. 
Computers, Vol. 56, No. 9, pp. 1225~1233, September 2007. 
[13] R. Xu and Q. Lu, “Multi-Stage Chinese Collocation Extraction,” Proc. International 
Conference on Machine Learning and Cybernetics, pp. 3254-3259, 2005. 
[14] Q. Lu, Y. Li, and R. Xu, “Improving Xtract for Chinese Collocation Extraction,” Proc. 
IEEE Intl. Conf. NLPKE 2003, pp. 333-338, 2003. 
[15] H. Thomas, E. Charles, L. Ronald, and Stein Clifford, “Introduction to Algorithms,” Second 
Edition, MIT Press, 2001. 
[16] K.J. Chen and M.H. Bai, “Unknown Word Detection for Chinese by a Corpus-based 
Learning Method,” International Journal of Computational Linguistics and Chinese 
Language Processing, Vol.3, No.1, pp. 27-44, 1998. 
[17] D. Kauchak and R. Barzilay, “Paraphrasing for Automatic Evaluation,” Proc. HLT-NAACL, 
pp. 455-462, 2006. 
[18] 王小川, 語音訊號處理, 全華科技股份有限公司, 2004. 
[19] 程祥徽, 田小琳, “現代漢語＂, 書林出版有限公司. 
[20] 林燾, 王理嘉, “語音學教程＂ 五南圖書出版公司. 
[21] Monaghan, A.I.C. and Ladd, D.R., “Manipulating Synthetic Intonation for Speaker C 
haracterisation”, in Proc. of ICASSP, S7.11, pp. 453-456, 1991. 
[22] Lee, L. S., Tseng, C. Y. and Hsieh, C. J., “Improved Tone Concatenation Rules in a 
Formant-Based Chinese Text-to-Speech System”, IEEE Trans. on Speech and Audio 
processing, vol. 1, no.3, pp.287-294, July 1993. 
[23] Chen, S. H. and Wang, Y. R., ”Tone Recognition of Continuous Mandarin Speech Based on 
Neural Networks”, IEEE Trans. on Speech and Audio processing, vol. 3, no.2, pp.146-150, 
March 1995. 
[24] Wightman, C. W. and Ostendorf. M., “Automatic Labeling of Prosodic Patterns”, IEEE 
Trans. on Speech and Audio Processing, vol. 2, no. 4, pp. 469-481, October 1994. 
[25] Lin, X., Chen, Y., Lim, S. and Lim, C., “Recognition of Emotional State From Spoken 
Sentences”, IEEE 3rd workshop on Multimedia Signal Processing, pp. 469-473, 1999. 
[26] Chan, M. V., Feng, X., Heinen, J. A. and Niederjohn, R. J., “Classification of Speech 
Accents with Neural Networks”, Neural Networks, IEEE World Congress on Computational 
Intelligence., IEEE International Conference on, vol.7, pp. 4483-4486, 1994. 
[27] Andrej, L. and Frank, F., “Synthesis of Natural Sounding Pitch Contours in Isolated 
Utterances Using Hidden Markov Models”, IEEE Trans. on Acoustic, Speech and Signal 
Processing, vol. ASSP-34, no.5, pp.1074-1080, October 1986. 
[28] Pan, N. H., Jen, W. T., Yu, S. S., Yu, S. S., Huang, S. Y. and Wu, M. J., “Prosody Model in a 
Mandarin Text-to-Speech System Based on a Hierarchical Approach”, IEEE International 
Conference on Multimedia and Expo, vol. 1, pp. 448-451, 2000. 
[29] Chen, S. H., Hwang, S. H. and Wang, Y. R., “An RNN-based Prosodic Information 
Synthesizer for Mandarin Text-to-Speech”, IEEE Trans. on Speech and Audio Processing, 
vol. 6, no.3, pp.226-269, 1998. 
[30] Kim, S. H., and Kim, J. Y., “Efficient Model of Establishing Words Tone Dictionary for 
Korean TTS System”, in Proc. of Eurospeech, pp. 243-246, 1997. 
[31] Dong, M. and Lua, K. T., “Pitch Contour Model for Chinese Text-to-Speech Using CART 
and Statistical Model”, in Proc. of ICSLP, pp. 2405-2408, 2002. 
[32] Tao, J., “F0 Prediction Model of Speech Synthesis Based on Template and Statistical 
Method”, Lecture Nodes of Artificial Intelligence, Springer, 2004. 
[33] Sun, X., The Determination, Analysis and Synthesis of Fundamental Frequency, Ph. D 
Thesis, Northwestern University, 2002. 
 24
模型，在階層式音韻架構下，作音高預測的基礎研究，並在各層的音韻單元考慮動態參數；
希望改善傳統音韻模型在小單元上合成細膩度的不足，並且以動態參數生成語音參數，保
留時間上的關聯性，使單元和單元之間的連接更加自然，並且能更符合人類講話時的句調
走勢變化，藉以改善基於隱藏式馬可夫模型之合成語音的自然度。此一具有個人化特色之
文字分析模型的研究並且結合人類講話時的句調走勢變化預測模型的研究和合成具有自然
音韻的語音技術有一定之學術價值，並且已經於相關之學術期刊發表本研究之技術。 
 
 
 26
■ 可申請專利  ■ 可技術移轉                                     日期：98 年 5 月 27 日 
國科會補助計畫 
計畫名稱：應用語音轉換於個人化表達性文字轉語音之研究 
計畫主持人：吳宗憲         
計畫編號：NSC96-2221-E-006-155-MY3 學門領域：資訊二(工程處)
技術/創作名稱 應用韻律階層及動態參數之音高預測技術之研究 
發明人/創作人 吳宗憲 
中文： 
對於應用階層式韻律架構及動態參數之音高預測模型，分為下列四
項研究重點：(1)階層式韻律架構的預測及產生；(2)導入動態參數
生成演算法於各韻律階層；(3)運用分類回歸樹及隱藏式馬可夫模型
建立各層韻律模型； (4)參數提取使用 STRAIGHT（ Speech 
Transformation and Representation based on Adaptive Interpolation of 
weiGHTed spectrogram）。 
技術說明 英文： 
The goal of this research is to develop a pitch prediction model using 
prosody hierarchy structure and dynamic features and to investigate the 
improvement of naturalness for synthesized speech. More specifically, 
this research is aimed to: (1) Prediction and generation of prosody 
hierarchy structure; (2) Dynamic features for each hierarchical layer; 
(3) Building the pitch prediction model for each layer: CART for 
prosodic word and syllable level, HMM for frame level; (4) Feature 
analysis using STRAIGHT (Speech Transformation and Representation 
based on Adaptive Interpolation of weiGHTed spectrogram). 
可利用之產業 
及 
可開發之產品 
資訊檢索產業、數位內容產業、數位教學產業、 
情緒語音合成系統 
技術特點 
將韻律結構以階層式的架構拆解，根據階層式的韻律架構作音韻參
數的量測，並導入動態參數於各韻律階層參數空間，以保留真實語
音在時間序列上的變化及關連性；在各層音韻模型的訓練上，在音
韻詞及音節階層使用分類回歸樹模型，在音框階層使用隱藏式馬可
夫模型。 
推廣及運用的價值 
合成具有高自然度的語音，是目前最新的研究主題，採用 HMM 為
基礎搭配音韻預測的研究具有高度可調性和靈活度，且儲存空間
小，僅需儲存參數少量語料訓練即可合成，是目前最主流的合成器
研究方向。透過調整模型參數的方式，將機率模型調整成目標語者
的模型，並合成出目標語者的聲音。預期將來可以應用到嵌入式系
統 Embedded System (手機，PDA 等行動裝置上)。 
 
 
 2
3. PLEN-3: “Extending Signal Processing into New Application Areas” James Truchard, National 
Instruments  
4. PLEN-4: “Recent Advances in Radar Imaging of Building Interiors” Moeness Amin, Villanova 
University  
 
二、參與會議 
本人3月15日上午出國搭機飛抵達舊金山，而後轉往達拉斯，並於3月15日抵達拉達拉斯
之旅館，並熟悉達拉斯及大會場地之狀況，準備研討會之論文發表工作。大會於3月14日及15
日舉行Tutorial課程。本人此次參加IEEE國際聲學語音及信號處理國際會議(IEEE Signal 
Processing Society)學會相關會議及論文發表，分述如下： 
 
3月16日：註冊及ICASSP2010大會議程(一) 
1. 參與多場口頭及海報論文發表並與國內外學者專家討論研究相關問題。 
 
3月17日：ICASSP2010大會議程(二) 
1. 參與多場口頭及海報論文發表並與國內外學者專家討論研究相關問題。 
2. 參加大會晚宴。 
 
3月18日：ICASSP2008大會議程(三) 
1. 參與多場口頭及海報論文發表並與國內外學者專家討論研究相關問題。 
2. 發表論文 “Pronunciation Variation Generation for Spontaneous Speech Synthesis 
Using State-Based Voice Transformation” 
 
3月19日：ICASSP2008大會議程(四)  
1. 大會正式議程最後一天。 
 
3月20日：搭機返台。 
 
三、攜回資料 
1. 2010 International Conference on Acoustics, Speech, and Signal Processing (ICASSP2010)之論
文集光碟一片及大會手冊各乙份。 
 
四、心得 
1. 當我國學術水準達到某層次之後，如何進入大會或國際組織核心變成比發表論文更重
要。 
2. 此次會議議題相當廣泛，大家參與會議時討論亦非常熱烈，大會安排多場海報式發表，
使得大家可以針對自己興趣之主題充分與發表者深入討論，因此收獲相當多。 
3. 此次會議台灣參加之人員約十幾位，可說相當熱烈，也由於有多人參會，大家亦可交換
心得及了解目前在各個領域之研究情形。 
4. 經由這次會議的參與，不但得以認識一些相關領域之學者，互相交換研究心得，而且可
吸收最新資訊，對日後計劃之執行將有所助益。 
spontaneous speech; this presents the deletion effect of 
normal speech to the spontaneous speech. On the other 
hand, the horizontal lines (between the 3rd red line and 2nd
yellow line) show the insertion effect of normal speech to 
the spontaneous speech. The syllable boundaries of 
spontaneous speech with pronunciation variation can be 
identified by the aligned boundaries of normal speech 
using DTW. 
3. TRANSFORMATION FUNCTION MODELING 
The purpose of this study is to construct the 
transformation functions between the data samples of the 
read speech and spontaneous speech with pronunciation 
variations. Fig. 2 shows the flowchart of the proposed 
method.  
Figure 2: Flowchart of the proposed method. 
In the training phase, a parallel speech database is 
collected, and the feature vectors, consisting of spectrum 
and duration parameter vectors, are extracted for HMM 
training. The STRAIGHT algorithm is adopted to extract 
the spectral features. The transformation function is 
derived using the trained HMM for transforming from 
normal phone to variation phone state-by-state. The 
measure used for transformation function classification is 
estimated from the articulatory features to construct a 
transformation function classification and regression tree 
(F-CART) for spectrum transformation function selection. 
For each state, the state durations are determined and 
classified using the articulatory features to construct a 
duration classification and regression tree (D-CART) for 
duration scaling. In the synthesis phase, the linguistic and 
articulatory features of the input text are extracted for 
model selection from the F-CART and D-CART. The 
normal (read-style) speech is synthesized from the input 
text by HMM-based TTS system (HTS). Finally, 
spontaneous speech with pronunciation variation is 
synthesized from the converted mel-cepstral coefficients 
and scaled durations obtained from the F-CART and D-
CART using the MLSA filter. 
In transformation function modeling, the parallel 
observation sequences from normal and variation speech 
are used for training. The sequences X={x1, x2,…, xN }and 
Y={y1, y2,…, yN} are the parallel source (normal speech) 
and target (variation speech) feature (MFCC) sequences of 
N frames, respectively. A multi-dimensional linear 
regression model is adopted as the state-based 
transformation function for converting the normal speech 
X to the variation speech Y: 
 Y AX R         (1) 
where A is the linear transformation matrix and the 
residual matrix R is determined as the difference between 
the converted spectrum through transformation matrix A
and the target spectrum. In the training phase, a statistical 
modelӳ for the normal and variation speech features, 
which is estimated to maximize the likelihood function of 
their joint distribution, is modeled as [6]: 
   
0 1
1
, , , ( , ),
t t t
T
q q q q t t
q q t
P P q a bO O S      ¦ ¦ X Y X Y x y       (2) 
( , ) ( ) ( )
( ) ( ; , )
( ) ( ; , )
j t t j t t j t
j t t t j t j j
x x
j t t j j
b b b
b N
b N
 
 
 
y
x y y x x
y x y A x R Ȉ
x x ȝ Ȉ
       (3) 
where q={q1,q2,…,qT} denotes the T-state sequence shared 
by the source and target feature streams;Ӹis the initial 
probability; a is the transition probability from state qt-1 to 
qt and bj(.) means the state observation probability density 
function (PDF) for state j.  ; ,N x ȝ Ȉ  represents the 
Gaussian distribution of x with mean vectorʳ Ӵ and
covariance matrixʳӢ. The parameters for each state can be 
estimated using the EM algorithm [7]. The re-estimation 
formulas for state j can be derived as [6]: 
1
1 ( )
T
x
j t t
t
r j
T  
c  ¦ȝ x         (4) 
1
1 ( )( )( )
T
x x x T
j t t j t j
t
r j
T  
c c c  ¦Ȉ x ȝ x ȝ        (5) 
1
1 1
( )( ) ( )
T T
T T
j t t j t t t t
t t
r j r j

  
§ ·§ ·c  ¨ ¸¨ ¸© ¹© ¹¦ ¦A y R x x x       (6) 
1
1
( )( )
( )
T
t t j t
t
j T
t
t
r j
r j
 
 
c
c  
¦
¦
y A x
R
        (7) 
1
1
( )( )( )
( )
T
T
t t j t j t j t j
y t
j T
t
t
r j
r j
 
 
c c c c   
c  
¦
¦
y A x R y A x R
Ȉ
      (8) 
where͋ t(j) denotes the state occupancy probability of 
4827
For objective evaluation, the variation phone in the 
parallel speech corpus is considered as the target feature 
vector. Three different variation generation methods were 
adopted for comparison as follows:  1) MLLR adaptation 
from normal phone to variation phone; 2) duration scaling; 
3) phone-based variation transformation function. 
Experiments were conducted on the comparisons of the 
proposed state-based voice transformation method, 
adaptation method, duration scaling and phone-based 
pronunciation variation transformation methods. Fig. 3 
shows the MSE of all the collected speech data at the 
phone level for the four different methods. The analytical 
results indicate that the state-based pronunciation variation 
generation method can achieve lower MSE than the others. 
Figure 3: MSEs of different variation generation methods 
5.2 Evaluation via Formal Listening 
For subject evaluation, the speech spontaneity and 
quality of the proposed method were assessed based on 
formal listening test. Fifteen listeners were asked to 
compare each synthesized sentence with the conventional 
HTS, MLLR adaptation and the proposed method. The 
listeners were asked to give a score from one to five, with 
one for “bad” and five for “excellent”. Fig. 4 presents the 
results from the listening test.  
Figure 4: MOS Listening test 
The quality of the synthesized speech using HTS are 
generally better than that using adaptation and state-based 
transformation methods. The reason is because the speech 
quality directly synthesized by HTS without conversion 
was not affected by the errors from adaptation or 
transformation. However, the synthesized speech using the 
proposed state-based transformation method obtains much 
better spontaneity than the read speech synthesized by the 
HTS. In addition, the proposed state-based method only 
slightly degrades the speech quality compared to that 
generated from HTS. The MLLR-based adaptation method 
cannot obtain good speech quality as well as improve the 
spontaneity of the synthesized speech. 
6. CONCLUSIONS 
This study presented a method to improve the 
spontaneity property in conventional HMM-based speech 
synthesis. The pronunciation variation can be modeled by 
a linear transformation function trained from the parallel 
normal and variation speech data. The acoustic and 
articulatory features are considered for transformation 
function clustering to construct a function selection CART 
(F-CART) and a duration scaling CART (D-CART). The 
F-CART and D-CART are then adopted to select 
appropriate transformation function and duration 
information using the articulatory features, respectively. 
From the evaluation results, the proposed state-based 
transformation method can generate the pronunciation 
variations to effectively improve the spontaneity in 
spontaneous speech synthesis compared to the 
conventional HMM-based HTS and the MLLR-based 
adaptation methods. 
7. REFERENCES 
[1] C.-H. Wu, C.-C. Hsia, J.-F. Chen, and J.-F. Wang, 
“Variable-Length Unit Selection in TTS Using Structural 
Syntactic Cost,” IEEE Trans. Audio, Speech, and Language 
Processing, Vol. 15, No. 4, pp.1227-1235, May 2007. 
[2] H. Zen, K. Tokuda, T. Masuko, T. Kobayashi, and T. 
Kitamura, “Hidden Semi-Markov Model Based Speech 
Synthesis System,” IEICE Trans. on Information Systems,
vol.E90-D, no.5, pp.825-834, May 2007. 
[3] C.-C. Hsia, C.-H. Wu., and J.-Q.Wu, “Conversion Function 
Clustering and Selection Using Linguistic and Spectral 
Information for Emotional Voice Conversion,” IEEE Trans. 
Computers, Vol. 56, No. 9, pp. 1225~1233, Sep. 2007.  
[4] http://mmc.sinica.edu.tw/mcdc_e.htm The Mandarin 
Conversational Dialogue Corpus (MCDC). 
[5] H. Kawahara, “Speech representation and transformation 
using adaptive interpolation of weighted spectrum: vocoder 
revisited,” In Proc. of ICASSP1997, pp. 1303–1306, 
Munich, Germany, 1997.  
[6] Z. H. Ling, K. Richmond, J. Yamagishi, and R. H. Wang. 
“Articulatory control of HMM-based parametric speech 
synthesis driven by phonetic knowledge“. In Proc. 
Interspeech, pp.573-576, Brisbane, Australia, Sep. 2008.
[7] A. P. Dempster, N. M. Laird and D. B. Rubin, “Maximum 
Likelihood from Incomplete Data via the EM Algorithm,” 
J. R. Statist. Soc. B, vol. 39, pp. 1-38, 1977. 
[8] L. H. Cai, D. D. Cui, and R. Cai, “TH-CoSS, a Mandarin 
Speech Corpus for TTS,” Journal of Chinese Information 
Processing, vol. 21, no. 2, pp. 94-99, Mar. 2007.
4829

其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
