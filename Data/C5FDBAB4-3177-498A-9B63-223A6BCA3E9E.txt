同的鄰近節點。不同於單純的隨機漫步，我們的方法對不同
的路徑能夠給予不同的權重，如此透過優化兩種不同的跳躍
機率，能夠得到較好的相似計量。最後我們對 DBLP 資料庫進
行許多實驗做為測試，相較於隨機猜測，我們的方法在最小
化均方根誤差方面表現顯著。 
中文關鍵詞： 大型複雜網路，圖形分割，群集型演算法，同步化， 脈衝偶
和震盪器， 通用處理器共享，鍵節預測 
英 文 摘 要 ： There are three main results of this project: (i) a 
general probability framework for community 
detection, (ii) anchored desynchronization, and (iii) 
link prediction for social user-item networks.. For 
community detection, we develop a general 
probabilistic framework for detecting community 
structure in a network based on Newman＇s fast 
algorithm. The key idea of our generalization is to 
characterize a network (graph) by a bivariate 
distribution that specifies the probability of the 
two vertices appearing at both ends of a randomly 
selected path in the graph. With such a bivariate 
distribution, we give a probabilistic definition of a 
community and a definition of a modularity index.  
 
To achieve desynchronization of a system of identical 
nodes, various distributed algorithms based on 
pulse­oupled oscillators have been recently proposed 
in the literature . Though these algorithms are shown 
to work properly by various computer simulations, 
they are still lack of rigorous theoretical proofs 
for both the convergence of the algorithms and the 
rates of convergence for these algorithms. Motivated 
by all these, we consider the desynchronization 
problem in a system where there exists an anchored 
node that never adjusts the phase of its oscillator. 
For such a system, we propose a generic anchored 
desynchronization algorithm. 
 
The link prediction problem based on the union of a 
social network and a user-item bipartite network, 
called a social user-item network, has been a hot 
research topic recently. One of the key challenges 
for such a problem is to identify and compute an 
行政院國家科學委員會專題研究計畫期末報告 
大型複雜網路之研究--子計畫二：隨機複雜網路
及其應用（第 3 年） 
Random Complex Networks and Their Applications 
計畫編號: NSC 99-2221-E-007-010-MY3 
執行期限: 101年 08月 01日至 102年 07月 31日 
主持人: 張正尚 國立清華大學通訊工程研究所 
 
中文摘要 
這篇報告包含三個主要結果:(i) 網路分群的機率架構，(ii) 錨定去同步化
演算法，(iii) 社群使用者-項目網路之鏈結預測。 
 
我們以凝聚型演算法中最被廣泛討論的紐曼快速演算法作為基礎，建立一個
網路分群的機率架構。而這個機率架構的關鍵想法是我們考慮隨機選取任一路徑
的機率分布而不是隨機選取任一線段的機率分布。在這樣的機率分布下，我們對
於相關性度量法、群組、模組性指數給了一個機率上的定義，進而探討了以機率
分布為基礎的分群演算法。為了能做到更為精準的網路分群，我們提出了一系列
的以機率分布為基礎的分群演算法，這些演算法的計算複雜度可比擬紐曼快速演
算法。然而我們的架構提供了更多的自由度去選擇機率分布以及相關性度量法。
就以這一點而論，當我們在進行已知群組架構之隨機圖形的電腦模擬，相對於原
本的紐曼快速演算法，我們的演算法在精確度上得到了顯著的提升。 
 
許多基於脈衝偶和震盪器之分散演算法近來被應用於達到具有相同節點系統之
去同步化。雖然這些演算法皆由電腦模擬來說明工作可運行性但仍缺乏嚴謹理論
證明其收斂性質及收斂速率。另一方面在許多實際應用上，系統裡的節點並非全
部皆相同，特別是有一特殊節點必須和系統外溝通且可能無法調整本身時脈。本
報告以此為動機，我們考慮一個具有單一不變相位震盪器的系統，而在此系統上
探討其去同步化問題。根據此類系統特性，我們提出了一項一般性錨定去同步化
英文摘要 
There are three main results of this project: (i) a general probability framework 
for community detection, (ii) anchored desynchronization, and (iii) link prediction for 
social user-item networks..  
 
For community detection, we develop a general probabilistic framework for 
detecting community structure in a network based on Newman’s fast algorithm. The 
key idea of our generalization is to characterize a network (graph) by a bivariate 
distribution that specifies the probability of the two vertices appearing at both ends of 
a randomly selected path in the graph. With such a bivariate distribution, we give a 
probabilistic definition of a community and a definition of a modularity index.  
 
To achieve desynchronization of a system of identical nodes, various distributed 
algorithms based on pulse‐coupled oscillators have been recently proposed in the 
literature . Though these algorithms are shown to work properly by various computer 
simulations, they are still lack of rigorous theoretical proofs for both the convergence 
of the algorithms and the rates of convergence for these algorithms. Motivated by all 
these, we consider the desynchronization problem in a system where there exists an 
anchored node that never adjusts the phase of its oscillator. For such a system, we 
propose a generic anchored desynchronization algorithm. 
 
The link prediction problem based on the union of a social network and a 
user-item bipartite network, called a social user-item network, has been a hot research 
topic recently. One of the key challenges for such a problem is to identify and 
compute an appropriate proximity (similarity) measure between two nodes in a social 
user-item network. To compute such a proximity measure, in this report we propose 
using a random walk with two different jumping probabilities toward different 
neighboring nodes. To test our method, we conduct various experiments on the DBLP 
dataset. With a 3-5 year training period, our method performs significantly better than 
random guess in term of minimizing the root mean squared error. 
 
Keywords: Large complex networks, graph partitioning, clustering algorithms, 
synchronization, pulse-coupled osscilatiors, generalized processor sharing, link 
prediction 
 
 
23.3 Proximity Measure by Using a Random Walk . . . . . . . . . 66
3.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . 69
3.4.2 Root Mean Squared Error . . . . . . . . . . . . . . . . 70
3.4.3 Precision and Recall . . . . . . . . . . . . . . . . . . . 71
3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
43.3 RMSE for various α’s and β’s by using years 2005-2007 as the
training set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.4 RMSE for various α’s and β’s by using years 2003-2007 as the
training set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.5 RMSE for various α’s and β’s by using different weights for
years 2003-2007 as the training set: (i) the solid lines for the
weights 2, 5, 4, 10, 8 and (ii) the dash lines for equal weights. . 75
3.6 Precision of top-n predictors. . . . . . . . . . . . . . . . . . . 75
3.7 Recall of top-n predictors. . . . . . . . . . . . . . . . . . . . . 76
3.8 The best choices of the two parameters α and β for various
top-n predictors. . . . . . . . . . . . . . . . . . . . . . . . . . 76
6community, what would be the right index for measuring the performance of
a graph partition? Based on Newman’s fast algorithm [13], in this report we
will provide a general probabilistic framework for these questions. The key
idea of our framework is to characterize a graph by a bivariate distribution
that specifies the probability of the two vertices appearing at both ends of
a “randomly” selected path in the graph. With such a bivariate distribu-
tion, we can then define a community as a set of vertices with the property
that it is more likely to find the other end in the same community given one
of the two ends in a randomly selected path is already in the community.
To detect communities, we define a class of correlation measures that can
be used for measuring how two vertices (and two communities) are related.
Two communities are positively (resp. negatively) correlated if the value
of a correlation measure for these two communities is positive (resp. nega-
tive). As a generalization of Newman’s fast algorithm, we propose a class
of distribution-based clustering algorithms for community detection. Like
most agglomerative algorithms, our distribution-based clustering algorithms
start from viewing each vertex as a solely member in a community and then
repeatedly merge the two most positively correlated communities into a new
community until all the remaining communities are negatively correlated.
There are two theoretic results that can be proved for a distribution-based
clustering algorithm: (i) it guarantees that every community detected by
the algorithm satisfies the definition of a community under certain technical
conditions for the bivariate distribution, and (ii) the algorithm increases an
index in each merge of two positively correlated communities. Such an index
is also called a modularity index in this report as it is a generalization of the
original modularity index in [13] and it might be used as a performance index
for a graph partition. Once the bivariate distribution is given, the compu-
tation complexity of a distribution-based clustering algorithm is O(n2 log n)
for a graph with n vertices and it can be reduced to O(n(log n)2) follow-
ing the implementation in [2] by exploiting the “sparseness” of the bivariate
distribution.
One of the well-known problems of Newman’s fast algorithm is its resolu-
tion limit in detecting communities [12]. Our general probabilistic framework
might provide a solution for this problem. Note that for each choice of the
bivariate distribution there is one corresponding definition of a community
and one corresponding definition of a modularity index. Newman’s fast algo-
rithm simply corresponds to the special case that the bivariate distribution
is obtained from uniformly selecting a path with length 1. As such, its res-
8n× n adjacency matrix A, where
Avw =
{
1, if vertices v and w are connected,
0, otherwise.
(1.1)
Let m = |Eg| be the number of edges in the graph and kv be the degree of
vertex v. From the adjacency matrix, we then have
m =
1
2
n∑
v=1
n∑
w=1
Avw, (1.2)
and
kv =
n∑
w=1
Avw. (1.3)
Let cv be the community of vertex v and δ(cv, i) be the δ-function that
equals to 1 if cv = i and 0 otherwise. Then the fraction of ends of edges that
are attached to the vertices in community i, denoted by ai, can be represented
as follows:
ai =
1
2m
n∑
v=1
kvδ(cv, i). (1.4)
Let
eij =
1
2m
n∑
v=1
n∑
w=1
Avwδ(cv, i)δ(cw, j). (1.5)
When i = j, eij is the fraction of edges that join the vertices in community i,
and when i 6= j, eij is one-half of the fraction of edges that join the vertices
in community i and the vertices in community j.
In [15], Newman and Girvan proposed a modularity index Q as follows:
Q =
∑
i
(eii − a2i ). (1.6)
As explained in [13], if the fraction of within-community edges is the same
as what we would expect for a randomized network, then this quantity is
zero. Nonzero values represent deviations from randomness. The objective
10
1.3 A probabilistic interpretation of Newman’s
fast algorithm
To further understand the intuition behind Newman’s fast algorithm, we
provide a probabilistic interpretation for Newman’s fast algorithm in this
section.
Consider the graph G(Vg, Eg) as described in the previous section. Suppose
that an edge is selected uniformly from the m edges of the graph. Let (V,W )
be the bivariate random vector that represents the vertices at the two ends
of the random selected edge. Then we have
P((V,W ) = (v, w))
=
{
1
2m
, if vertices v and w are connected,
0, otherwise.
(1.8)
From (1.8), it follows that
P(V = v) =
n∑
w=1
P((V,W ) = (v, w)) =
kv
2m
, (1.9)
where kv is the degree of vertex v. Similarly,
P(W = w) =
n∑
v=1
P((V,W ) = (v, w)) =
kw
2m
. (1.10)
Let Si be the set of nodes in community i and Xi (resp. Yj) be the indicator
variable for the event that V is in community i (resp. W is in community
j). Specifically,
Xi =
{
1, V ∈ Si,
0, V /∈ Si, (1.11)
and
Yj =
{
1, W ∈ Sj,
0, W /∈ Sj. (1.12)
Using (1.9) yields
P(Xi = 1) = P(V ∈ Si) =
∑
v∈Si
kv
2m
=
1
2m
∑
v
kvδ(cv, i) = ai, (1.13)
12
of nodes {1, 2, . . . , n}, it is characterized by a bivariate distribution p(v, w),
v, w = 1, 2, . . . , n, for randomly selecting a pair of two nodes V and W , i.e.,
P(V = v,W = w) = p(v, w). (1.17)
Let pV (v) (resp. pW (w)) be the marginal distribution of the random variable
V (resp. W ), i.e.,
pV (v) =
n∑
w=1
p(v, w), (1.18)
and
pW (w) =
n∑
v=1
p(v, w). (1.19)
If p(v, w) is symmetric, then pV (v) = pW (v) for all v, and pV (v) is the
probability that a randomly selected node is v. In case that p(v, w) is not
symmetric, we can find a symmetric bivariate distribution p˜(v, w) by letting
p˜(v, w) =
1
2
(p(v, w) + p(w, v)).
1.4.1 From a graph to a bivariate distribution
As mentioned before, a network is commonly modelled by a graph G(Vg, Eg),
which in turn is characterized by an adjacency matrix A. The question is
how one obtains a bivariate distribution characterization from a graph model.
A direct approach is to follow the probabilistic interpretation in Section 1.3
that maps an adjacency matrix A to a bivariate distribution in (1.8). Let
α(A) be the sum of all the elements in a matrix A, i.e.,
α(A) =
∑
v
∑
w
Avw. (1.20)
Then one can rewrite (1.8) as follows:
P(V = v,W = w) =
1
α(A)
Avw. (1.21)
One problem of using the adjacency matrix is the resolution limit in com-
munity detection. As argued in [12], optimizing the modularity index Q in
[15] may fail to detect communities smaller than a scale that depends on the
14
Example 2 (A random walk on a graph) Consider a graph with an
n× n adjacency matrix A. As in (1.2) and (1.3), let m be the total number
of edges and kv be the degree of vertex v. A random walk on such a graph
can be characterized by a Markov chain with the n× n transition probability
matrix R = (Rv,w), where
Rv,w =
1
kv
Avw (1.24)
is the transition probability from vertex v to vertex w. The stationary proba-
bility that the Markov chain is in vertex v, denoted by piv, is kv/2m. Let β` be
the probability that we select a path with length `, ` = 1, 2, . . .. Then the prob-
ability of selecting a random walk (path) with vertices v = v1, v2, . . . , v` = w
is
β`piv1
`−1∏
i=1
Rvi,vi+1 . (1.25)
From this, we then have the bivariate distribution
p(v, w) = piv
∞∑
`=1
β`
∑
v2
. . .
∑
v`−1
`−1∏
i=1
Rvi,vi+1 . (1.26)
Since A is a symmetric matrix, it is easy to see that
pivRv,w =
1
2m
Av,w =
1
2m
Aw,v = piwRw,v
for all v and w, and the Markov chain is thus a reversible Markov chain [19].
This implies that
piv1
`−1∏
i=1
Rvi,vi+1 = piv`
`−1∏
i=1
Rvi+1,vi
and p(v, w) = p(w, v) is thus a symmetric bivariate distribution. To randomly
select a path with length not greater than 2, we can simply let β` = 0 for all
` > 2 and this leads to
p(v, w) =
β1
2m
Av,w +
β2
2m
n∑
v2=1
Av,v2Av2,w
kv2
. (1.27)
16
Example 5 (Correlation) Note that the correlation of two random vari-
ables X and Y , denoted by Correl[X,Y ], can be computed as follows:
Correl[X, Y ] =
Cov[X, Y ]√
Var(X)
√
Var(Y )
, (1.31)
where Var(X) (resp. Var(Y )) is the variance of X (resp. Y ). For two
indicator random variables X and Y , we then have
Var(X) = P(X = 1)− (P(X = 1))2, (1.32)
Var(Y ) = P(Y = 1)− (P(Y = 1))2, (1.33)
Correl[X,Y ]
=
P(X = 1, Y = 1)− P(X = 1)P(Y = 1)√
Var(X)
√
Var(Y )
. (1.34)
Clearly, the correlation of X and Y is also a correlation measure. In com-
parison with the covariance of X and Y in Example 4, the correlation of X
and Y always has a value between -1 and 1, and could be more suitable for
our computation later.
Example 6 (Mutual information) The mutual information of two ran-
dom variables X and Y (see e.g., [3]), denoted by I(X;Y ), can be computed
as follows:
I(X;Y ) =
∑
x,y
PX,Y (x, y) log
PX,Y (x, y)
PX(x)PY (y)
, (1.35)
where PX,Y (x, y) is the bivariate distribution of X and Y , PX(x) is the
marginal distribution of X, and PY (y) is the marginal distribution of Y . The
mutual information I(X;Y ), also known as a special case of the Kullback-
Leibler distance [3], is commonly used as a measure for the distance between
the bivariate distribution PX,Y (x, y) of two random variables X and Y and
the bivariate distribution if they were independent. When X and Y are inde-
pendent, we have PX,Y (x, y) = PX(x)PY (y). Thus I(X;Y ) = 0 if and only if
X and Y are independent. Let Sgn(x) be the sign function, i.e., Sgn(x) = 1
if x ≥ 0 and Sgn(x) = −1 if x < 0. Then it is easy to verify that
ρ(X,Y ) = Sgn(Cov(X, Y )) · I(X;Y ) (1.36)
is also a correlation measure.
18
Moreover, for all ` 6= k, update
P(Xk = 1, Y` = 1)
= P(Xi = 1, Y` = 1) + P(Xj = 1, Y` = 1), (1.40)
P(X` = 1, Yk = 1)
= P(X` = 1, Yi = 1) + P(X` = 1, Yj = 1). (1.41)
(P4) For all ` 6= k, compute ρ(Xk, Y`) and ρ(X`, Yk).
(P5) Repeat (P3) until either there is only one community left or all the
remaining pairs of communities have negative measures, i.e., ρ(Xi, Yj) < 0
for all i 6= j.
Distribution-based clustering algorithms are generalizations of Newmans’s
fast algorithm. In each iteration, the distribution is updated and then used
for computing the new correlation measures. This is different from most
distance-based clustering algorithms [7, 25], where the new distance between
clusters is updated directly. Note that there are at most n − 1 iterations in
the above algorithm and there are O(n) updates for the measures in (P4)
for each iteration. The hard part is to find the two communities that have
the largest correlation measure in (P3). If we simply use a linear search
to find the two communities that have the largest correlation measure in
each iteration, then its computational complexity is O(n2) and the overall
computational complexity for the above algorithm is O(n3). To reduce the
computational complexity, one can implement a sorted list for the measures
in (P2) and then insert every measure update into the sorted list. As each
insertion of a new update takes O(log(n)) steps (by using a binary search)
and there are O(n) updates in each iteration, the computational complexity
in each iteration can be reduced to O(n log(n)) and that yields O(n2 log(n))
computational complexity for the above algorithm. One can further reduce
the computational complexity by explore the “spareness” of the bivariate dis-
tribution. Suppose that we stop the algorithm in (P5) once all the remaining
pairs of communities do not have positive measures, i.e., ρ(Xi, Yj) ≤ 0 for
all i and j. In this case, we only need to maintain the list of measures
with positive values. From (C2) in Definition 3, it suffices to maintain the
pair of communities i and j with P (Xi = 1, Yj = 1) > 0. Two commu-
nities i and j are said to be connected if either P (Xi = 1, Yj = 1) > 0
or P (Xj = 1, Yi = 1) > 0. Suppose that there are only O(m) connected
pairs of them at the beginning. In view of (P2), we only need to maintain
(and update) the pair of connected communities. As such, instead of having
20
In the following theorem, we show (under certain technical conditions) that
the modularity index is non-decreasing in every iteration of any distribution-
based clustering algorithm and it indeed detects communities in the proba-
bilistic sense defined in Definition 7.
Theorem 9 Suppose that p(v, w) is symmetric.
(i) Then for any distribution-based clustering algorithm described in Section
1.4.3, the modularity index is non-decreasing in every iteration.
(ii) If, furthermore,
p(v, v) ≥ (pV (v))2, (1.45)
for all v = 1, 2, . . . , n, then every community detected by any distribution-
based clustering algorithm described in Section 1.4.3 is a community in
the probabilistic sense defined in Definition 7.
Proof. (i) Since we assume that p(v, w) is symmetric, it suffices to show
that
C∑
c=1
(
P(V ∈ Sc,W ∈ Sc)− (P(V ∈ Sc))2
)
(1.46)
is non-decreasing in every iteration. Suppose that community i and commu-
nity j are selected and grouped into a new community k in some iteration.
Thus, we have a new partition of {1, 2, . . . , n} with Sk = Si ∪ Sj. To prove
that the modularity index in (1.46) is non-decreasing after grouping commu-
nity i and community j into a new community k, we need to show that
P(V ∈ Sk,W ∈ Sk)− (P(V ∈ Sk))2
≥ P(V ∈ Si,W ∈ Si)− (P(V ∈ Si))2
+P(V ∈ Sj,W ∈ Sj)− (P(V ∈ Sj))2. (1.47)
Since Sk = Si ∪ Sj and Si ∩ Sj is an empty set, we have
P(V ∈ Sk,W ∈ Sk)
= P(V ∈ Si,W ∈ Si) + P(V ∈ Si,W ∈ Sj)
+P(V ∈ Sj,W ∈ Si) + P(V ∈ Sj,W ∈ Sj), (1.48)
and
P(V ∈ Sk) = P(V ∈ Si) + P(V ∈ Sj). (1.49)
22
We note that the condition in (1.45) is not satisfied for the choice of the
bivariate distribution in (1.8). Thus, Newman’s fast algorithm may not guar-
antee that every detected community is a community in the probabilistic
sense defined in Definition 7.
In (P5), when ρ(Xi, Yj) < 0 for all the remaining pairs of communities
i 6= j, we know from Theorem 9(i) that the modularity index Q in Definition
8 cannot be further improved and this yields the best structure (in terms
of the modularity index Q) by any distribution-based clustering algorithm
(for a symmetric bivariate distribution). In practice, one might continue the
process until there is only one community. In that case, a distribution-based
clustering algorithm generates a dendrogram (an ordered binary tree) that
specifies the order for two communities to be grouped into a new one in each
step.
1.5 Simulation results
In this section, we report our simulation results. We will consider three
distribution-based clustering algorithms: (i) covariance algorithm with the
correlation measure in Example 4, (ii) correlation algorithm with the corre-
lation measure in Example 5, and (iii) mutual information algorithm with
the correlation measure in Example 6. To map a graph with an adjacency
matrix A to a bivariate distribution p(v, w), we also consider the following
three types of functions in (1.23) of Example 1: (i) (λ0, λ1, λ2) = (0, 1, 0), i.e.,
f1(A) = A, (ii)(λ0, λ1, λ2) = (1, 1, 0), i.e., f2(A) = I+A and (ii)(λ0, λ1, λ2) =
(1, 0.5, 0.25), i.e., f3(A) = I+ 0.5A+ 0.25A
2.
1.5.1 Random graphs with known community struc-
ture
In this section, we report our simulation results for a large number of random
graphs with known community structure. For each random graph, there are
n = 128 vertices and they are divided into four known communities with
32 vertices each (as in [13]). Let zin be the parameter that represents the
average number of edges from a vertex to the other vertices in the same com-
munity. Also, let zout be the parameter that represents the average number
of edges from a vertex to the other vertices in different communities. Let
k be the average degree of a vertex. Then k = zin + zout. In our simula-
24
 
Figure 1.1: Performance of f1(A), f2(A) and f3(A) under the covariance
algorithm
 
Figure 1.2: Performance of f1(A), f2(A) and f3(A) under the correlation
algorithm
26
 
Figure 1.5: The network of friendships between each other in the karate club
observed by Zachary.
1.5.2 Karate club
Now, we apply our framework to a well-known set of real-world network data,
called “karate club.” The set of data was observed by Wayne Zachary [28]
over the course of two years in the early 1970s at an American university.
During the course of the study, the club split into two groups because of
a dispute within the organization, and the members of one group left to
establish their own club. The network of friendships between each other in
the karate club observed by Zachary is shown in Figure 1.5.
In Figure 1.6, we show the dendrogram generated by using the covariance
algorithm with f3(A) = I + 0.5A+ 0.25A
2. The algorithm is run until there
is only one community left. As such, we can cut through the dendrogram at
different levels to give divisions of the network into larger or smaller commu-
nities. As shown in Figure 1.6, the dendrogram generated by our algorithm
matches perfectly to original structure observed by Zachary. This is better
than the original Newman’s fast algorithm, where vertex 10 is classified in-
correctly. The intuition behind this is exactly the same as explained by the
illustrating example in Figure 1.4.
1.6 Conclusion
Based on Newman’s fast algorithm, in this report we developed a general
probabilistic framework for detecting community structure in a network. The
Bibliography
[1] U. Brandes, D. Delling, M. Gaertler, R. Goerke, M. Hoefer, Z. Nikoloski,
and D. Wagner, “On modularity clustering,” IEEE Transactions on
Knowledge and Data Engineering, vol. 20 , pp. 172-188, 2008.
[2] A. Clauset, M. E. J. Newman, C. Moore, ”Finding community structure
in very large networks”, Physical Review E, 2004.
[3] T. M. Cover and J. A. Thomas, Elements of Information Theory , New
York, NY: John Wiley & Sons, 1991.
[4] I. Dhillon, , Y. Guan, and B. Kulis, “Kernel k-means, spectral clustering
and normalized cuts,” In Proceedings of the 10th international conference
on knowledge discovery and data mining, pp. 551-556, 2004.
[5] L. Danon, Albert, Dı´az-Guilera1, J. Duch and A. Arenas, “Comparing
community structure identification,” Journal of Statistical Mechanics:
Theory and Experiment, vol. 2005, P09008, 2005.
[6] J. Duch and A. Arenas, “Community identification using extremal opti-
mization,” Physical Review E vol. 72, 027104, 2005
[7] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classficiation, John
Wiley & Sons, 2001.
[8] P. Erdo¨s and A. Re´nyi, “On random graphs,” Publicationes Mathematicae
Debrencen 6, 290, 1959.
[9] M. Girvan and M. E. J. Newman, ”Community structure in social and
biological networks”, Proc. Natl. Acad. Sci. U.S.A., vol. 99, p. 7821 ,2002.
28
30
[23] M. Rosvall and C. T. Bergstrom, “An information-theoretic framework
for resolving community structure in complex networks,” Proc. Natl.
Acad. Sci. U.S.A., vol. 104, no. 18, pp. 7327-7331, 2007.
[24] M. Rosvall and C. T. Bergstrom, “Maps of random walks on complex
networks reveal community structure,” Proc. Natl. Acad. Sci. U.S.A., vol.
105, no. 4 pp. 1118-1123, 2008.
[25] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Academic
Press, 2006.
[26] F. Wu and B.A. Huberman, “Finding communities in linear time: a
physics approach,” Eur. Phys. J., B38, pp. 331-338, 2004.
[27] R. Yuster and U. Zwick, “Fast sparse matrix multiplication,” ACM
Transactions on Algorithms, vol. 1, pp. 2-13, 2005.
[28] W. W. Zachary, “An information flow model for conflict and fission in
small groups,” Journal of Anthropological Research, vol. 33, pp. 452V473,
1977.
32
reaches the end of its cycle, i.e., its phase reaches 1, it fires and resets its
phase back to 0. The firing also notifies all the other nodes that it begins a
new cycle. Then it waits for the next node to fire and jumps to a new phase
according to a certain function. Its jumping function only uses the firing
information of the node fires before it and the node fires after it. It was shown
in [5] that the DESYNC-STALE algorithm achieves desynchronization, i.e.,
the phases of the n nodes are spaced as evenly as possible, if the new phase
of each jump in a node is moved toward to an ”estimated” midpoint of the
phases of two neighboring nodes. However, the rate of convergence of the
DESYNC-STALE algorithm is only conjectured to be O(n2) from various
computer simulations.
The original objective of the desynchronization algorithm in [5] is to adjust
the phase of each node so that the phases are spaced evenly. By so doing,
each node can receive the same amount of bandwidth in a wireless network
with a common channel. Pagliari, Hong and Scaglione [17] considered an
important extension of the fair resource scheduling scheme to the generalized
processor sharing (GPS) scheme [18] (called the proportional fair scheduling
scheme in [17]), where every node is assigned a weight and the amount of
bandwidth received by a node is proportional to its weight. If the weights
are rational numbers, a na¨ive implementation of the GPS scheme is simply
to have each node in the DESYNC-STALE algorithm to maintain an integer
number of nodes that is proportional to its weight. As addressed in [17],
such an approach is obviously inefficient as it increases the number of firings
for each node and thus increases the hardware complexity of each node and
the convergence time. Instead, Pagliari, Hong and Scaglione [17] proposed
a genuine algorithm with two oscillators in each node and showed that their
algorithm indeed converges in the ideal case (Theorem 4 of [17]), where the
up-to-date phase information is known. However, the convergence of the
stale case was only verified by computer simulations.
Though both the DESYNC-STALE algorithm in [5] and the extension of
the GPS scheme in [17] are shown to work properly by various computer
simulations, they are still several theoretical gaps, including the rate of con-
vergence of the DESYNC-STALE algorithm and the convergence of the stale
GPS scheme in [17]. On the other hand, all the nodes are not likely to be
identical in many practical applications. In particular, there might be a node
that needs to interact with the “outside” world and thus may not have the
freedom to adjust its local clock, e.g., the master node in Bluetooth, the
collector node in a wireless sensor network, and the master clock in paral-
34
the problem about error fluctuation due to overshooting in the DESYNC al-
gorithm, the orthodontics-inspired algorithm proposed by Taechalertpaisarn,
Choochaisri and Intanagonwiwat in [21] binds the phase of the nodes into
a chain if they find that the phase difference between each other is roughly
1/n, where n is the number of the nodes. On the other hand, Choochaisri
et. al. [4] proposed the DWARF algorithm in which each node adjusts its
phase according to the received repelling forces that is inversely proportional
to the phase difference between the nodes.
For multi-hop networks, J. Degesys and R. Nagpal extended their original
DESYNC algorithm in [7]. Moreover, Motskin et. al. [13] considered the
distributed vertex coloring problem and proposed a randomized desynchro-
nization algorithm that converges in O(∆ log n) rounds with high probability,
where ∆ is the maximum degree of the graph. By following a similar ap-
proach, Kang and Wong [9] proposed the M-DESYNC algorithm that divides
one single time period into time slots with equal lengths, where the number
of the time slots is determined by the maximum of its own degree and the
degrees of all the 1-hop neighbors. Then, each node tries to find one eligible
time slot that does not collide with an arbitrary 2-hop neighbor to trans-
mit. On the other hand, when an arbitrary node fires, the extended-desync
algorithm proposed by Muhlberger and Kolla [14] defines the previous and
successive phase neighbors as the node fires just before and after the node
fires among all the 2-hop neighbors respectively, and then updates the phase
in the same manner as the original DESYNC algorithm does in [5, 6]. In sum-
mary, the authors above used the information of the 2-hop neighborhood to
solve the hidden node problem in multi-hop networks.
The rest of the report is organized as follows. In Section 2.2, we propose
the general framework for our anchored desynchronization algorithms. Based
on the framework, we derive the system dynamics of our algorithms. In Sec-
tion 2.3, we consider the generic anchored desynchronization algorithms and
obtain the formal mathematically analysis for the rate of convergence of such
algorithms. We then extend the setting to the generalized processor shar-
ing scheme in Section 2.4, where we prove the convergence of our anchored
desynchronization algorithms. In Section 2.5, we propose algorithms that
use the information of the total number of nodes in the system and show
that they achieve perfect desynchrony in 3n− 4 rounds of firings. In Section
2.6, we compare the rate of convergence of the DESYNC-STALE algorithm
in [5] to that of our anchored desynchronization algorithm. In Section 2.7,
we conclude this report by addressing some further extensions.
36
φi(τ
+) = fi(φi(τ),∆b,∆f ), (2.1)
where fi(·, ·, ·) is a deterministic function available to node i.
Without loss of generality, we assume that the phases of the n nodes are
initially ordered as follows:
1 = φ0(0) > φ1(0) > φ2(0) > . . . > φn−1(0) > 0. (2.2)
Thus, node 0 is the first one to fire at time 0 and the phase of node 0 is reset
to 0, i.e., φ0(0
+) = 0. To ease our presentation, the initial firing of node 0
at time 0 is counted as the 0th time firing of node 0. Also, we define i⊕ 1 as
(i+ 1) mod n. As there is no clock drift, node 0 will fire for the mth time at
time m, m = 1, 2, . . .. In order to make sure that every node fires according
to the desired order, i.e., node 0, node 1, node 2, . . ., node n − 1, and then
node 0, we need the following non-overtaking condition.
(Non-overtaking condition) Suppose that node i⊕ 1 fires at some time τ
for some i = 1, 2, . . . , n− 1. Then, node i will adjust its phase such that its
phase satisfies the following inequality:
φi−1(τ) > φi(τ+) > φi⊕1(τ+) = 0. (2.3)
Note that there is no need to check the non-overtaking condition for node 0
as node 0 is the anchored node and it never adjusts its phase when other nodes
fire. The condition in (2.3) ensures that the order of the phases is preserved
after the adjustment of the phase of every node. By so doing, every node fires
exactly once in every unit of time. Also, except the anchored node, every
node adjusts its phase exactly once in every unit of time when its next node
fires.
Suppose that the non-overtaking condition is satisfied (which we will ver-
ify later for our algorithm). Then we can take a snap shot of the phases
immediately after node 0 fires at time m. Let
x(m) = (x1(m), x2(m), . . . , xn(m))
T ,
where
xi(m) =

1− φ1(m+) for i = 1
φi−1(m+)− φi(m+) for i = 2, 3, . . . , n− 1
φn−1(m+) for i = n
. (2.4)
38
Lemma 10 For some i = 1, 2, . . . , n − 1, suppose that the non-overtaking
condition in (2.3) is satisfied up to τi(m + 1)
+, i.e., the time immediately
after node i fires for the (m + 1)th time. Under Algorithm 1, the following
holds:
(i) The phase of node i when (or immediately before) node i⊕ 1 fires for the
(m+ 1)th time is
φi(τi⊕1(m+ 1)) = xi+1(m). (2.12)
(ii) The phase of node i immediately after node i⊕ 1 fires for the (m+ 1)th
time is
φi(τi⊕1(m+ 1)+) = fi(xi+1(m), xi(m), xi+1(m)). (2.13)
(iii) For i = 1, the phase of node i− 1 immediately after node i⊕ 1 fires for
the (m+ 1)th time is
φi−1(τi⊕1(m+ 1)) = φ0(τ2(m+ 1)) = x1(m) + x2(m).
(iv) For i > 1, the phase of node i− 1 immediately after node i⊕ 1 fires for
the (m+ 1)th time is
φi−1(τi⊕1(m+ 1)) = fi−1(xi(m), xi−1(m), xi(m)) + xi+1(m). (2.14)
Proof. (i) Since node i fires for the (m+1)th time at time τi(m+1) and its
phase is reset back to 0, we have φi(τi(m+1)
+) = 0. Its phase then increases
linearly before τi⊕1(m+ 1). In view of (2.11), it follows that
φi(τi⊕1(m+ 1)) = τi⊕1(m+ 1)− τi(m+ 1) = xi+1(m).
(ii) Since the non-overtaking condition in (2.3) is satisfied up to τi(m+1)
+,
we have from (2.11) that
∆f = τi⊕1(m+ 1)− τi(m+ 1) = xi+1(m). (2.15)
Similarly, for i > 1,
∆b = τi(m+ 1)− τi−1(m+ 1) = xi(m). (2.16)
40
2.3 Generic anchored desynchronization
In this section, we propose our generic anchored desynchronization algorithm
by choosing
fi(φi(τ),∆b,∆f ) = γφi(τ) + (1− γ)∆b +∆f
2
, (2.22)
for 0 ≤ γ < 1 and i = 1, 2, . . . , n − 1. Note that the phase adjustment rule
in (2.22) is exactly the same as that in the DESYNC-STALE algorithm in
[5]. For γ = 0, the rule simply adjusts the phase to the stale midpoint of
two neighboring nodes. As such, the parameter 1− γ is called the jump size
parameter in [5]. Thus, the only difference between the DESYNC-STALE
algorithm in [5] and ours is the anchored node. Though it was shown in [5]
that the DESYNC-STALE algorithm indeed achieves desynchronization, it
is still not clear what the rate of convergence is. With the insertion of the
anchored node, we are able to obtain a formal theoretical result for the rate
of convergence.
In the following lemma, we first derive the governing dynamics of the sys-
tem under (2.22).
Lemma 12 Suppose that the functions fi, i = 1, 2, . . . , n− 1, are chosen in
(2.22) with 0 ≤ γ < 1. Under Algorithm 1 and initial condition in (2.2), the
following holds:
(i) The phase of node i, i = 1, 2, . . . , n−1, immediately after node i⊕1 fires
for the (m+ 1)th time is
φi(τi⊕1(m+ 1)+) =
(1 + γ)
2
xi+1(m) +
(1− γ)
2
xi(m). (2.23)
(ii) For i > 1, the phase of node i− 1 immediately after node i⊕ 1 fires for
the (m+ 1)th time is
φi−1(τi⊕1(m+1)) = xi+1(m)+
(1 + γ)
2
xi(m)+
(1− γ)
2
xi−1(m). (2.24)
(iii) The non-overtaking condition is satisfied and thus xi(m) > 0 for all i
and m.
42
where x(m) = (x1(m), x2(m), . . . , xn(m))
T is the state vector andW = (Wij)
is the n× n tridiagonal matrix with
Wij =

1+γ
2
for i = j = 1 or i = j = n,
γ for i = j = 2, . . . , n− 1,
1−γ
2
for i = j − 1 = 1, 2, . . . , n− 1,
1−γ
2
for i = j + 1 = 2, . . . , n,
0 otherwise.
(2.30)
For 0 ≤ γ < 1, we have Wij ≥ 0. Moreover, both the row sums and the
column sums of W are equal to 1, i.e.,
∑n
i=1Wij = 1, j = 1, 2, . . . , n, and∑n
j=1Wij = 1, i = 1, 2, . . . , n. Such a matrix is known as a doubly stochas-
tic matrix (see e.g., the book by Marshall and Olkin [11]). In view of the
recursion for the state vectors in (2.29), there is a partial ordering, known as
the majorization ordering ([11], p 20, Theorem 2.A.4), among the sequence of
the state vectors x(m). As a direct consequence of the majorization ordering,
we know that
∑n
i=1 g(xi(m)) is decreasing in m for any convex function g
([11], p 108, Proposition 4.B.1). In particular,
∑n
i=1 |xi(m)− 1n | is decreasing
m. To further understand the rate of convergence, we need to identify the
eigenvalues of the matrix W and this is done in the following proposition.
Proposition 13 The n eigenvalues of the n × n matrix W are γ + (1 −
γ) cos( ipi
n
), i = 0, 1, . . . , n− 1. Thus, the largest eigenvalue of W is 1. More-
over, for 0 ≤ γ < 1, the absolute values of the other eigenvalues of W are
bounded above by γ + (1 − γ) cos(pi
n
). Thus, the second largest eigenvalue
modulus (SLEM) of W, defined as the maximum of the absolute values of
the other eigenvalue, is γ + (1− γ) cos(pi
n
).
Proof. Write
W = I− 1− γ
2
L, (2.31)
where I is the n × n identity matrix and L = (Lij) is the n × n tridiagonal
matrix with
Lij =

1 for i = j = 1 or i = j = n,
2 for i = j = 2, . . . , n− 1,
−1 for i = j − 1 = 1, 2, . . . , n− 1,
−1 for i = j + 1 = 2, . . . , n,
0 otherwise.
(2.32)
44
where µ is the second largest eigenvalue modulus (SLEM) of the transition
probability matrixW. Since we have shown in Proposition 13 that the SLEM
od W is γ + (1− γ) cos(pi
n
). It then follows from (2.34) that
n∑
i=1
|xi(m)− 1
n
| ≤ √n
(
γ + (1− γ) cos(pi
n
)
)m
. (2.35)
For large n, we have
cos(
pi
n
) ≈ 1− 1
2
(
pi
n
)2,
and
ln(1− (1− γ) pi
2
2n2
) ≈ −(1− γ) pi
2
2n2
.
Using these in (2.35) yields
ln
(√
n(γ + (1− γ) cos(pi
n
))m
)
≈ 1
2
lnn−m(1− γ) pi
2
2n2
. (2.36)
In view of (2.35) and (2.36), we conclude that the system achieves ²-desynchrony
in O(n2 ln(n
²
)/(1− γ)) rounds of firings.
We note that an alternative proof of this theorem is to use the upper bound
for the mixing time of a reversible Markov chain by the relaxation time in
Theorem 12.3 of the book [10].
2.4 Generalized processor sharing
The original objective of the desynchronization algorithm is to adjust the
phase of each node so that the phases are spaced evenly. By so doing, each
node can receive the same amount of bandwidth in a wireless network with
a common channel. An important extension of the fair resource scheduling
scheme is the generalized processor sharing (GPS) scheme addressed in [18,
17]. In the GPS scheme, every node is assigned a weight and the amount of
bandwidth assigned to a node should be proportional to its weight. For this
purpose, we will extend the generic anchored desynchronization algorithm so
that the phase differences are proportional to the weights. Our algorithm is
different from that in [17]. Specifically, let αi > 0 be the weight assigned
46
(iii) The non-overtaking condition is satisfied and thus xi(m) > 0 for all i
and m.
Proof. The proof for (i) and (ii) of this lemma is the same as that in Lemma
12. Analogous to the proof for the non-overtaking condition in Lemma 12,
we use the induction hypothesis that xi(m) > 0 for all i. From the induction
hypothesis and (2.40), it is clear that φi(τi⊕1(m+ 1)+) > 0 for 1/2 ≤ γ < 1.
On the other hand, we note from (2.41) and (2.40) that
φi−1(τi⊕1(m+ 1))− φi(τi⊕1(m+ 1)+)
= (1− βi)(1− γ)xi+1(m) + (1− γ)βi−1xi−1(m) + (γ + (1− γ)(βi−1 − βi))xi(m).
Since 0 < βi < 1 for all i, we have for 1/2 ≤ γ < 1
(γ + (1− γ)(βi−1 − βi)) > 2γ − 1 ≥ 0.
Thus, φi−1(τi⊕1(m+1)) > φi(τi⊕1(m+1)+) and the non-overtaking condition
is satisfied.
Note that the functions fi, i = 1, 2, . . . , n− 1, in (2.38) are generalizations
of those in (2.22). However, the choice of the parameter γ is more restricted
and it is now in [1/2, 1) to ensure the non-overtaking condition to be valid.
Analogous to the derivation for the recursive equation for the state vector in
(2.29), one can show that
x(m+ 1) = W˜x(m), (2.42)
where W˜ = (W˜ij) is the n× n tridiagonal matrix with
W˜ij =

1− (1− γ)β1, for j = i = 1,
(1− γ)(1− βi), for j = i+ 1,
γ + (1− γ)(βi−1 − βi), for j = i 6= 1 or n,
(1− γ)βi−1, for j = i− 1,
1− (1− γ)(1− βn−1) for j = i = n,
0, otherwise.
(2.43)
The governing dynamics in Lemma 15 leads to the following theorem.
48
each node that counts the number of firings between two successive firings of
a node. In this section, we assume that the information of the total number
of nodes is available to all the nodes. With this additional information, we
will show how the rate of convergence could be improved.
Anchored desynchronization with the information of the total number of
nodes would be rather easy to achieve if all the nodes knew which one the
anchored node was. Specifically, suppose that there are n nodes in the system
and every node knows that node 0 is the anchored node. Then node i knows
that its relative phase difference to node 0 should be i/n. After the first
round of firing, every node knows its relative order in the system and it can
then adjust its phase to the targeted position.
Motivated by this, our idea to speed up the rate of convergence is for
our anchored desynchronization to “learn” which node the anchored node
is (without adding additional hardware complexity). As the anchored node
never adjusts its phase, one simple way to recognize the anchored node is to
observe whether the firing times of a node are spaced exactly one unit apart.
As we have already stored the firing time of the next node in the previous
round, we can compare that with the firing time in the current round. If
the difference is exactly one unit of time, then the node treats its next node
as the anchored node. Clearly, after two rounds of firings, node n − 1 will
be the first node to identify the anchored node and it can then adjust its
relative phase to node 0 to 1/n. After that, node n − 1 has been set to the
targeted position and it will not adjust its phase further. Such a node will
be called a locked node (as its relative phase to the anchored node is locked).
A locked node behaves just like another anchored node. After another two
rounds of firings, node n − 2 will treat node n − 1 as the “anchored” node
and adjust its relative phase to node 1 to 1/n. Then node n− 2 becomes a
locked node. Repeating the process, one can see that node i adjusts its phase
to its targeted position after treating node i ⊕ 1 as the anchored node and
node i will not adjust its phase from that point on. Thus, the convergence
of the above algorithm works like a ripple propagating from node n − 1 to
node 1.
There is one catch for the above argument. In order for the algorithm to
work properly, we need to make sure the non-overtaking condition in (2.3) is
satisfied so that the relative order of firings remains unchanged. Thus, even
when a node treats its next node as the anchored node, it may not be able
to adjust its phase difference to 1/n as the non-overtaking condition might
be violated. For this, we have to put a restriction on the jump size and such
50
Fire Fire
(a) (b)
1
5
b
∆
b
β∆
0
1
2
3
4
0
1
2
3
4
Figure 2.2: (a) Illustration of the non-overtaking condition for the phase
adjustment of node i with the choice of functions in (2.49). (b) Nodes are
clustered near the anchored node.
nodes. Thus, after n rounds of firing, node n − 1 will be in its targeted
position, i.e., its relative phase difference to node 0 is 1/n. Similarly, node
n − i will be in its targeted position after n + 2(i − 1) rounds of firings (as
it takes two rounds to identify a locked node). What happens here is that
each node is moved toward the anchored node first and then back to its
targeted position like a boomerang. With the intuition in mind, we prove in
the following theorem that Algorithm 2 indeed converges in O(n) rounds of
firings if β is close to 1.
Theorem 17 Suppose that the functions fi, i = 1, 2, . . . , n − 1, are chosen
in (2.49) with β ≥ 1 − 2
n2
. Then Algorithm 2 achieves perfect desynchrony
in 3n− 4 rounds of firings.
For the proof of Theorem 17, we need the following result in Lemma 18.
Lemma 18 Suppose that node i has not been locked after node 0 fires at
time m0. Then for all j = 1, . . . , i and j ≤ m ≤ m0,
xj(m) ≤ j(1− β). (2.50)
Proof. Since node i has not been locked after node 0 fires at time m0,
the phase of node i is changed during each update and thus node i − 1
will not identify node i as a locked node. As such, we know that nodes j,
j = 1, 2, . . . , i have not been locked after node 0 fires at time m0 and they
follow the phase adjustment rule in (2.48).
52
Since
∑n
i=1 xi(n− 1) = 1,
xn(n− 1) = 1−
n−1∑
i=1
xi(n− 1) ≥ 1− n(n− 1)
2
(1− β) ≥ 1
n
,
where we use the assumption that β ≥ 1− 2
n2
in the last inequality. Note that
node n − 1 learns that node 0 is a locked node (in fact the anchored node)
after the first two rounds of firings. According to the phase adjustment rule
in (2.47) and (2.12) of Lemma 10,
φn−1(n+) = min
[ 1
n
, φn−1(n) + βxn−1(n)
]
= min
[ 1
n
, xn(n− 1) + βxn−1(n)
]
=
1
n
. (2.53)
Thus, xn(n) = 1/n and node n− 1 will be locked after n rounds of firings.
Now we assume that node n − i, i ≤ i0 (for some i0 ≥ 1) are all locked
after n + 2(i0 − 1) rounds of firings as the induction hypothesis. Clearly,
node n − (i0 + 1) learns that node n − i0 is locked after n − i0 fires for the
(n + 2i0)
th times. Suppose that node n − (i0 + 1) has not been locked after
n + 2i0 − 1 rounds of firings. Then we have from Lemma 18 that for all
i = 1, . . . , n− (i0 + 1),
xi(n+ 2i0 − 1) ≤ i(1− β). (2.54)
Since
∑n
i=1 xi(n+2i0−1) = 1 and xi(n+2i0−1) = 1/n for i = n−i0+1, . . . , n,
xn−i0(n+ 2i0 − 1) = 1−
n−i0−1∑
i=1
xi(n+ 2i0 − 1)−
n∑
i=n−i0+1
xi(n+ 2i0 − 1)
≥ 1− (n− i0)(n− i0 − 1)
2
(1− β)− i0
n
≥ 1
n
,
where we use the assumption that β ≥ 1− 2
n2
in the last inequality. According
to the phase adjustment rule in (2.47) and (2.12) of Lemma 10,
φn−(i0+1)(τn−i0(n+ 2i0)
+)
= min
[
1
n
, φn−(i0+1)(τn−i0(n+ 2i0)) + βxn−(i0+1)(n)
]
= min
[
1
n
, xn−i0(n+ 2i0 − 1) + βxn−(i0+1)(n+ 2i0 − 1)
]
= 1
n
.
(2.55)
54
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
X: 0.743
Y: 0.4866
1−r
X: 0.754
Y: 0.516
Anchored
1−2*r
DESYNC−STALE
r+(1−r)cos(2*pi/n)
Figure 2.3: Numerical results for the SLEM of the anchored desynchroniza-
tion algorithm with n = 5 (marked in blue) and that of the corresponding
DESYNC-STALE algorithm in [5] (marked in red) with respect to various
jump size 1− γ in (0, 1).
Note that the DESYNC-STALE algorithm may not converge if the jump size
is chosen to be 1 (the maximum jump size). This is because there is a root
−1 in (2.56) if the jump size 1 − γ is set to be 1 and n is an odd number.
As such, when the jump size is close to 1 and n is an odd number, the rate
of convergence of our algorithm is better than that of the DESYNC-STALE
algorithm in [5].
To better understand the rate of convergence of the DESYNC-STALE al-
gorithm in [5], let λ2 be one of the roots in (2.56) that corresponds to the
SLEM of the matrix in (10) of [5]. As shown in [5], the polynomial in (2.56)
is a stable polynomial and thus |λ2| < 1 for 0 < γ < 1. In the following, we
derive an approximation for |λ2|n.
Our approach is to represent the roots in (2.56) by their polar coordinates.
Specifically, we let λ = Reiθ, where R ≥ 0, 0 ≤ θ < 2pi and i = √−1. Note
that R must be larger than 0 as zero is not a root of (2.56) for 0 ≤ γ < 1.
Moreover, except the trivial root λ = 1, it was shown in [5] that |λ| < 1 and
thus 0 < R < 1 for 0 < γ < 1. Now we can rewrite (2.56) as follows:
λn = γ +
1− γ
2
(λ+
1
λ
). (2.57)
It then follows from (2.57) that
Rn cos(nθ) = γ +
1− γ
2
(R +
1
R
) cos(θ), (2.58)
and
Rn sin(nθ) =
1− γ
2
(R− 1
R
) sin(θ). (2.59)
56
It is of some interest to compare (2.66) with the SLEM for W in Proposi-
tion 13, i.e., γ + (1− γ) cos(pi
n
). This shows that the rate of convergence for
the DESYNC-STALE algorithm is faster than that of our anchored desyn-
chronization algorithm in the asymptotic regime. The intuition behind this
might be explained by considering the near perfect desynchrony scenario.
When the state of the system is near perfect desynchrony, the stale estimate
is almost the same as its true value. If this is the case, the n× n governing
matrix WD for the DESYNC-STALE algorithm could be modified from W
by “adjusting” the phase of the anchored node inW as well. Thus, we have
WDij =

1+γ
2
for i = 1, j = n or i = n, j = 1,
γ for i = j = 1, 2, . . . , n,
1−γ
2
for i = j − 1 = 1, 2, . . . , n− 1,
1−γ
2
for i = j + 1 = 2, . . . , n,
0 otherwise.
(2.67)
In conjunction with W in (2.30), we note that W (resp. WD) corresponds
to the distributed averaging problem in [22] over a line (resp. ring) graph
with n nodes. It is well-known (by using the Birkhoff decomposition [1] for
the doubly stochastic matrice in (2.67)) that the SLEM of WD is γ + (1 −
γ) cos(2pi
n
), which is smaller than that of W.
2.7 Conclusion
In this report, we considered the desynchronization problem in a system
based on pulse-coupled oscillators. Unlike the schemes in [5, 17], we assume
there exists an anchored node that never adjusts the phase of its oscilla-
tor. For such a system, we proposed a generic anchored desynchroniza-
tion algorithm similar to the DESYNC-STALE algorithm in [5]. Though
the only difference between our anchored desynchronization algorithm and
the DESYNC-STALE algorithm in [5] is the anchored node, we are able to
rigourously prove the rate of convergence of our algorithm. Specifically, we
show that our algorithm achieves ²-desynchrony in O(n2 ln(n
²
)) rounds of
firings. We also proved that our anchored desynchronization algorithm con-
verges even for the generalized processor sharing (GPS) scheme previously
studied in [17]. When the information of the total number of nodes in the
system is available to all the nodes, we proposed a set of algorithms that
achieve perfect desynchrony in 3n− 4 rounds of firings. In terms of the rate
Bibliography
[1] G. Birkhoff, Tres observaciones sobre el algebra lineal, Universidad Na-
cional de Tucuman Revista, Serie A, 5 (1946), pp. 147-151.
[2] S. Boyd, P. Diaconis, L. Xiao, Fastest mixing markov chain on a graph,
SIAM Review, 46 (2004), pp. 667-689.
[3] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, Randomized gossip algo-
rithms, IEEE Transactions on Information Theory, 52 (2006), pp. 2508-
2530.
[4] S. Choochaisri, K. Apicharttrisorn, K. Korprasertthaworn, P. Taechalert-
paisarn, and C. Intanagonwiwat, Desynchronization with an artificial
force field for wireless networks, SIGCOMM Computer Communication
Review, 42 (2012), pp. 7-15.
[5] J. Degesys, I. Rose, A. Patel, and R. Nagpal, Desync: self-organizing
desynchronization and TDMA on wireless sensor networks, International
Conference on Information Processing in Sensor Networks, 2007.
[6] J. Degesys, I. Rose, A. Patel, R. Nagpal, Desynchronization: the theory of
self-organizing algorithms for round-robin scheduling, First International
Conference on Self-Adaptive and Self-Organizing Systems, 2007.
[7] J. Degesys and R. Nagpal, Towards desynchronization of multi-hop
topologies, Second International Conference on Self-Adaptive and Self-
Organizing Systems, 2008.
[8] P. Diconis and D. Stroock, Geometric bounds for eigenvalues of markov
chains, The Annals of Applied Probability, 1 (1991), pp. 36-61.
[9] H. Kang, and J. L. Wong, A localized multi-hop desynchronization algo-
rithm for wireless sensor networks, IEEE INFOCOM, 2009.
58
60
[21] P. Taechalertpaisarn, S. Choochaisri, and C. Intanagonwiwat, An
orthodontics-inspired desynchronization algorithm for wireless sensor net-
works, IEEE 13th International Conference on Communication Technol-
ogy (ICCT), 2011.
[22] L. Xiao and S. Boyd, Fast linear iterations for distributed averaging,
System & Control Letters, 53 (2004), pp. 65-78.
[23] W.-C. Yueh, Eigenvalues of several tridiagonal matrices, Applied Math-
ematics E-Notes, 2005, pp. 66-74.. Available free at mirror sites of
http://www.math.nthu.edu.tw/~amen/
62
(resp. items) that are similar to the target user (resp. item) to come up with
the recommendations for the target user (resp. item). As such, one key step
for a neighborhood-based method is to compute the proximity (similarity)
measure [14] between two nodes in the bipartite network. On the other hand,
model-based methods (see e.g., [15, 16] for surveys of these methods) use the
bipartite network to learn predictive models that can be easily represented
by latent characteristics, e.g., the principal component analysis technique
[17, 8] can be used to convert the original user-item bipartite network into
another matrix with a much lower dimension, and thus greatly reduces the
computation efforts for link prediction. Model-based methods in general have
better performance than neighborhood-based methods as model-based meth-
ods incorporate the whole bipartite network into consideration instead of only
using proximity (similarity) measures in neighborhood-based methods. How-
ever, the drawback of a model-based approach is that latent characteristics
in general do not have clear physical meanings and it is difficult to see how
a model-based approach can be further improved.
As on-line social networks become popular, the information of social in-
teractions can also be recorded and used for personal recommendations. As
such, constructing personal recommendation systems that take on-line social
interactions into consideration has been a hot research topic [18, 11, 19, 20].
In addition to the user-item network that collects historical interactions be-
tween users and items, there is another social network (sometimes called
trust network) that characterizes the interactions among users. The main
advantage of using a social network is that cold-start users who have very
limited interactions on items can be predicted by their trusted friends. As
pointed out in [19], combining the prediction by a traditional collaborative
filtering method and that by a trust-based method can lead to much better
performance than using a single method alone.
Instead of using a combined approach, in this report we ask the question
whether prediction can be made directly from a social user-item network
(that consists of a social network and a user-item bipartite network). As for
neighborhood-based collaborative filtering methods, the key challenge is to
identify and compute an appropriate proximity measure between two nodes
in a social user-item network. There are many methods addressed in [14]
for computing the proximity (similarity) measures for a pair of two nodes
in a network, including methods based on node neighborhoods and methods
based on the ensemble of all paths. However, these methods cannot be
directly applied as now we have two graphs in a social user-item network.
64
as its nodes. The social network Gs is characterized by the N ×N adjacency
matrix A = (Ai,j), where Ai,j = 1 if user ui and user uj are friends of each
other, and 0 otherwise. A social user-item network G in this report is then
defined as the union of the user-item network Gb and the social network Gs,
i.e, G = Gb ∪Gs.
1
2 3
4
A B C
5
1
2 3
4
5
Users
Items
Bipartite Graph Social Graph
plus
Figure 3.1: An example of a social user-item network with 5 users and 3
items.
To illustrate this, we show in Fig. 3.1 an example of a social user-item
network with 5 users and 3 items. On the left hand side of Fig. 3.1, there is
a user-item bipartite network of 5 users and 3 items. On the right hand side
of Fig. 3.1, there is a social network of these 5 users. The adjacency matrix
A and the biadjacency matrix B are as follows:
A =

0 1 1 1 1
1 0 1 0 1
1 1 0 0 0
1 0 0 0 0
1 1 0 0 0
 , (3.1)
B =

1 0 0
1 0 0
1 0 1
0 1 0
1 1 1
 . (3.2)
As in [14], the link prediction problem of the social user-item network
considered in this report is to infer new interactions that are most likely
to occur between a user and an item in the future given a snapshot of the
66
proximity measures by using random walks have clear physical mean-
ings than other latent methods.
3.3 Proximity Measure by Using a Random
Walk
In this section, we propose using a random walk method to compute the
proximity measure between two nodes in a social user-item network. Recall
that a social user-item network consists of a social graph of users and a
bipartite graph with users as one sides of nodes and items as the other side
of nodes. We use the N ×N matrix A = (Ai,j) for the adjacency matrix of
the social network and the N ×M matrix B = (Bi,j) for the biadjacency
matrix of the user-item network. Now consider a random walker on a social
user-item network. The jumping probabilities for the random walker are
specified as follows:
(R1) The walker is on a user node:
• With probability α, the walker will walk to another user node, which
is selected uniformly among all its neighboring users.
• With probability 1 − α, the walker will walk to another item node,
which is randomly selected among all its neighboring items with the
probability proportional to the edge weight between that user and that
item.
(R2) The walker is on an item node:
• With probability β, the walker will walk to a user node, which is
randomly selected among all its neighboring users with the probability
proportional to the edge weight between that item and that user.
• With probability 1− β, the walker will stop at this item node.
Clearly, if we let X(t) be the node that the random walker visits at time
t. Then {X(t), t ≥ 0} forms an absorbing Markov chain. Let P1(u, v) be the
absorbing probability that a random walker is started from a user node u and
is absorbed by an item node v. Also, let P2(v, v
′) be the absorbing probability
that a random walker is started from an item node v and is absorbed by an
item node v′. Intuitively, a large absorbing probability between a user node
68
P1 = α ·WU,U · P1 + (1− α) ·WU,I · P2, (3.8)
P2 = β ·W I,U · P1 + (1− β) · I. (3.9)
By solving these two matrix equations, we can then obtain P1 as follows:
P1 = (1− α)(1− β) ·(
I − ((1− α)β ·WU,I ·W I,U + α ·WU,U) )−1 ·
WU,I . (3.10)
This can then be used to obtain P2 from (3.9).
Note that we have to calculate the inverse of an N × N matrix in (3.10).
The computational complexity of inverting a matrix is O(N3) by using the
Gauss-Seidel elimination. Since N is the number of users in a recommender
system, its value could be very large and thus computing the inverse of such
a matrix could be numerically challenging. Such a problem is commonly
known as the proximity inversion problem [26]. As described in [26], one way
to compute a good approximation for the inverse of a matrix (I−δC)−1 with
δ << 1 and C being a sparse matrix is to use the (truncated) power series
representation, i.e.,
(I − δC)−1 =
∞∑
k=0
δkCk ≈
Kmax∑
k=0
δkCk,
where Kmax is a constant that is large enough to guarantee a small error.
Then one can exploit the sparsity of the matrix C to compute Ck. However,
we will not explore this further in this report. On the other hand, if we only
would like to make personal recommendations for a specific user u, then we
only need to compute P1(u, ·) and there is no need to compute the whole
matrix P1. In that regard, it might be more efficient to use simulation to
estimate P1(u, ·). In this report, we will simulate the random walks for 10000
times for each user node u and estimate P1(u, v) by the ratio of the number of
times that the walk is absorbed by the item node v to 10000. The simulation
algorithm is described below.
70
paper together. On the other hand, for the user-item (author-conference)
network Gb, the edge weight between an author and a conference is the
number of papers published by that author in that conference. These two
graphs are processed in C by using the library of the igraph [27]. Once the
social user-item network is generated, we then run Algorithm 1 to estimate
the proximity measures P1(u, v) for the N = 30, 758 authors and the M =
4, 773 conferences. For the two parameters α and β, we run all the cases for
α from 0 to 0.9 and β from 0.1 to 0.9, where the steps of both are set to
0.1. By so doing, we obtain 90 matrices for the proximity measures. Among
these 90 matrices, we then compare their performance and identify the best
choice of the two parameters α and β.
3.4.2 Root Mean Squared Error
In the literature, the Root Mean Squared Error (RMSE) [25] is a frequently
used measure of the differences between the predicted values and the real
values. Let T (u, v) be the number of papers that author u published in
conference v in the test year of 2008, and T (u) =
∑M
v=1 T (u, v) be the number
of papers that author u published in the test year of 2008. Since P1(u, v)
is the absorbing probability of the random walk from user u to item v in
the social user-item network, it can be viewed as the probability that author
u will publish a paper in conference v in the future. As such, a reasonable
estimate for the number of papers that author u published in conference v
in the test year of 2008 is Tˆ (u, v) = P1(u, v) · T (u). Then, we compute the
root mean squared error by using the following equation:
RMSE =
√√√√ N∑
u=1
M∑
v=1
|Tˆ (u, v)− T (u, v)|2, (3.11)
If our prediction is good, at least the root mean squared error in (3.11) should
be much smaller than that from random guess. The root mean squared error
from random guess is to assign each conference with an equal probability
1/M , i.e.,
RMSE(Random)
=
√√√√ N∑
u=1
M∑
v=1
| 1
M
T (u)− T (u, v)|2 = 396.36. (3.12)
72
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
360
380
400
420
440
460
480
β
R
M
SE
 
 
α=0
α=0.1
α=0.2
α=0.3
α=0.4
α=0.5
α=0.6
α=0.7
α=0.8
α=0.9
random
Figure 3.2: RMSE for various α’s and β’s by using year 2007 as the training
set.
author to publish his/her papers in the test year. For the top-n predictor, we
rank the conferences for each author u according to the proximity measure
P1(u, v) and recommend the n largest ones. Also, precision and recall for the
top-n predictor are defined as follows:
precision(n) =
# matches of top–n
n
, (3.13)
and
recall(n) =
# matches of top–n
n′
, (3.14)
where n′ is the number of conferences in which a specific author published a
paper in the test year of 2008. For example, if we use the top-10 predictor
and there are 3 conferences that match the result, the precision is 3/10 = 0.3.
However, if the author actually published in 6 conferences in this test year,
then the recall in this situation is 3/6 = 0.5. Once the precision and the recall
for each user is computed, the precision and the recall is then computed by
averaging over the precision and the recall for each user.
In our experiments, we evaluate the precision and recall of the top-n predic-
tor for n = 1, 2, . . . , 50. For these experiments, we set α = 0.2 and β = 0.4.
In Fig. 3.6 and Fig. 3.7, we show the results for precision and recall as a
74
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
350
360
370
380
390
400
410
β
R
M
SE
 
 
α=0
α=0.1
α=0.2
α=0.3
α=0.4
α=0.5
α=0.6
α=0.7
α=0.8
α=0.9
random
Figure 3.4: RMSE for various α’s and β’s by using years 2003-2007 as the
training set.
α = 0.5 and β = 0.4 leads to the best result for the top-5 predictor. One
interesting finding is that one should choose large (resp. small) α and β for
small (resp. large) n.
3.5 Conclusion
In this report, we proposed using a random walk with different jumping prob-
abilities for computing the proximity measure of a social user-item network.
Such a proximity measure can then be used for link prediction in a social
user-item network. We also tested our method by using the DBLP dataset
and our experimental results show significant improvement in comparison
with random guess. In addition to link prediction, we believe our proximity
measure can also be applied for computing centralities of nodes (that can be
used for ranking nodes in a social user-item network). Another possible ex-
tension of our work is to use the proximity measure for community detection
in a social user-item network.
76
0 10 20 30 40 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
n
re
ca
ll
 
 
2007
5 years
5 years with weights
3 years
Figure 3.7: Recall of top-n predictors.
Figure 3.8: The best choices of the two parameters α and β for various top-n
predictors.
78
[9] J. S. Breese, D. Heckerman, and C. Kadie, “Empirical analysis of predic-
tive algorithms for collaborative filtering,” in Proceedings of the Four-
teenth conference on Uncertainty in artificial intelligence. Morgan
Kaufmann Publishers Inc., 1998, pp. 43–52.
[10] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry, “Using collabora-
tive filtering to weave an information tapestry,” Communications of the
ACM, vol. 35, no. 12, pp. 61–70, 1992.
[11] M. Jamali and M. Ester, “Trustwalker: a random walk model for com-
bining trust-based and item-based recommendation,” in Proceedings of
the 15th ACM SIGKDD international conference on Knowledge discov-
ery and data mining. ACM, 2009, pp. 397–406.
[12] D. M. Dunlavy, T. G. Kolda, and E. Acar, “Temporal link prediction us-
ing matrix and tensor factorizations,” ACM Transactions on Knowledge
Discovery from Data (TKDD), vol. 5, no. 2, p. 10, 2011.
[13] H. Ma, H. Yang, M. R. Lyu, and I. King, “Sorec: social recommendation
using probabilistic matrix factorization,” in Proceedings of the 17th ACM
conference on Information and knowledge management. ACM, 2008,
pp. 931–940.
[14] D. Liben-Nowell and J. Kleinberg, “The link-prediction problem for so-
cial networks,” Journal of the American society for information science
and technology, vol. 58, no. 7, pp. 1019–1031, 2007.
[15] Y. Koren and R. Bell, “Advances in collaborative filtering,” in Recom-
mender Systems Handbook. Springer, 2011, pp. 145–186.
[16] M. Al Hasan and M. J. Zaki, “A survey of link prediction in social
networks,” in Social network data analytics. Springer, 2011, pp. 243–
275.
[17] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, “Eigentaste: A
constant time collaborative filtering algorithm,” Information Retrieval,
vol. 4, no. 2, pp. 133–151, 2001.
[18] R. Andersen, C. Borgs, J. Chayes, U. Feige, A. Flaxman, A. Kalai,
V. Mirrokni, and M. Tennenholtz, “Trust-based recommendation sys-
tems: an axiomatic approach,” in Proceedings of the 17th international
conference on World Wide Web. ACM, 2008, pp. 199–208.
國科會補助計畫衍生研發成果推廣資料表
日期:2013/09/30
國科會補助計畫
計畫名稱: 子計畫二：隨機複雜網路及其應用
計畫主持人: 張正尚
計畫編號: 99-2221-E-007-010-MY3 學門領域: 網路
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
