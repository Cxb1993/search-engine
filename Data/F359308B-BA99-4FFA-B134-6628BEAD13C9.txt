I 
行政院國家科學委員會補助專題研究計畫成果報告 
 以表格為基礎的自動化圖形使用者介面驗收測試 
計畫編號：NSC 100-2221-E-027-054 
執行期間：100 年 8 月 1 日至 101 年 7 月 31 日 
計畫主持人：陳偉凱 
 
計畫參與人員：杜秉穎、王榮麒、胡朝翔 
              陳怡靜、張雄展、康芷瑜、王鈺 
國立台北科技大學資訊工程系 
 
摘要 
圖形使用者介面(Graphical User Interface，簡稱 GUI)的測試，一般是由測試
人員先編寫測試腳本(Test Script)，然後再自動執行測試腳本，以確認待測程式
(Application Under Test，簡稱 AUT)的是否正確。然而，一個大型的 AUT，其測
試腳本不免是複雜且龐大的。因此，測試人員需要一個良好的測試腳本編輯工
具，以減少維護測試腳本所需的成本。本計畫提出以表格呈現與編輯測試腳本之
方法，並且實作一個表格式的腳本編輯器，稱為 TabularSE，讓測試人員能夠使
用日常熟悉的表格型式來開發測試腳本。此外，本計畫以二個實驗，證實表格型
式測試腳本確實有助於提升測試腳本的可讀性，且 TabularSE 確實能提高編輯測
試腳本的便利性。 
 
關鍵詞：驗收測試、表格式測試腳本、GUI 測試 
Abstract 
  To perform Graphical User Interface (GUI) Testing, a tester usually writes a test 
script for the GUI and then executes the test script to verify whether the Application 
Under Test (AUT) works correctly. However, for a large AUT, the test script may 
become huge and complicated. Therefore, the tester needs a good test-script editing 
tool to reduce the maintenance cost. This project proposes a method of presenting and 
editing test scripts in tabular formats, and implements a tabular script editor. The 
proposed method allows the tester to use tabular formats, which are frequently used in 
everyone’s daily life, to develop test scripts. In addition, two experiments are 
conducted to verify whether a tabular test script does help improve readability and 
whether the proposed testing tool is more convenient. 
 
Keywords: Acceptance Testing, Tabular script editor, GUI testing
1 
一、 前言 
軟體測試是軟體開發過程中非常重要的一環，透過測試能找出潛藏在軟體裡
面的缺陷(Defect)，增加程式設計師對產品的信心。軟體測試依測試的目標可以
分成單元測試(Unit Testing)、整合測試(Integration Testing)、系統測試(System 
Testing)、驗收測試(Acceptance Testing)等。驗收測試是在交付軟體給客戶前進行
的測試活動，用來驗證軟體功能是否能符合客戶的需求。近年來，愈來愈多的軟
體使用圖形使用者介面(Graphical User Interface，簡稱 GUI)，使得元件之間的互
動變得更為複雜，這也讓透過 GUI 進行驗收測試在整個軟體開發過程中變得非
常重要。 
傳統的 GUI 測試，是由測試人員親自操作軟體，並觀察軟體狀態的變化，
以驗證程式是否正確，但這種方式十分浪費時間與人力，因此，自動化的 GUI
測試是很重要的。常見的自動化 GUI 測試，是將實際操作軟體的流程編寫(或錄
影)成測試腳本，腳本的內容主要描述使用者對某個元件(Component)發出某個事
件(Event)，並且於發出數個事件後，驗證軟體的狀態是否符合預期的結果。最後，
由測試工具自動播放該腳本，減少測試人員的操作步驟，達到自動化的目的。 
 
 
圖 1 待測程式小算盤 
 
GTT4 [1][2]是一個 Java Swing 的測試工具，主要以文字型式並輔以樹狀結構
的顯示介面來編輯(維護)測試腳本。圖一為待測程式小算盤的 GUI，其操作方式
與一般計算機軟體相同，在功能上提供基本運算能力：加、減、乘以及除法，整
個程式主要由一個文字方塊及數個按鈕組成，文字方塊則用來顯示計算後的結
果。假設我們想用 GTT4 為小算盤撰寫一個簡單的加法測試腳本，腳本內容如圖 
2，過程為：按下按鈕 1、按鈕 2、按鈕 add (+)、按鈕 3、按鈕 4 及按鈕 equal(=)，
最後驗證文字方塊是否顯示 46。 
圖 2 以簡單的「文字型式」表達腳本的內容(12 + 34 = 46」)，腳本中使用句
號(.)分隔元件與事件，其表示方式與一般的程式語言非常相似。然而，測試人員
並不一定具備撰寫程式的能力，若將文字型式的測試腳本給予測試人員時，他們
3 
亦可藉由編寫 VBScript 來控制腳本執行流程。雖然 QTP 有提供表格介面讓測試
人員能夠快速新增多筆測試資料(如參數)，但是測試人員卻無法直接透過表格撰
寫測試腳本。 
Fit (Framework for Integrated Test)為一套用來執行驗收測試與整合測試的工
具[4]。在 Fit 中，表格用來記錄著檢測軟體的相關資料，即需求文件上的規格；
Fixture 則是一段簡短的程式碼，負責搭起待測程式與表格之間的橋樑，讓表格
上的資料能夠順利進入軟體以進行檢測。但是，測試人員每完成一張表格時，需
額外撰寫程式碼(Fixture)來讀取表格上的測試資料。 
RF (Robot Framework)是一套 Python-based 的自動測試框架[5]。為了能夠以
表格的文法來呈現及編輯測試腳本，Robot Framework 定義了四種類型的表格：
Setting, Variable, Test case 及 Keyword table 來滿足撰寫腳本所需之需求。雖然測
試人員可藉由 API 文件了解表格的使用方式，但以手動(人工)的方式輸入指令卻
會增加錯誤的可能性。 
表 1 測試工具之功能比較表 
功能/屬性 QTP Fit RF GTT4 
圖形使用者介面 有 無 無 有 
和開發環境整合 無 有 無 有 
腳本呈現方式 
文字/程式
碼 
表格 表格
文字/表
格 
顯示執行結果 有 有 有 有 
錄影播放 有 無 無 有 
開放原始碼 否 是 是 是 
 
四、 研究方法 
表格之起源最早可以往前追朔至西漢司馬遷所著之典籍報任安書[3]，為司
馬遷給予獄中友人任安的回信。至今已經兩千多年，表格在現代生活中無所不
在，足見表格對社會之貢獻有多重要。本節主要說明將表格介面引入 GTT4 所能
帶來的優勢。 
提升腳本可讀性 
圖 3 是使用 GTT Macro 編輯器對待測程式小算盤撰寫之測試腳本，圖中
Calculator 為待測程式的名稱，Calculate 為一個巨集事件，此事件接受四個
參數，分別為：左手式、運算子、右手式及預期之運算結果，執行時依據使
用者輸入之運算子將左手式以及右手式進行運算，並將結果與使用者輸入的
5 
 
Macro 
Event Name 
Component 
Name 
Action Arg1 Arg2 Arg3 
 result Assert
Hello, 
Tom 
getText  
圖 5 利用表格建立一個基礎事件 
同樣的驗證事件，若使用 GTT Macro 編輯器，需先在 Component 頁籤
(圖 6)挑選需要驗證的元件，接下來切換至另一個頁籤 View Assert，依據函
式的字首(Prefix)於對應的下拉選單進行挑選，並且在下方的參數表格中設
定參數類型、參數名稱以及參數值。最後，在 Assert Value 欄位輸入預期的
結果，經過這些步驟才能建立出一個驗證事件。 
 
 
圖 6 建立驗證事件的使用者介面 
若僅建立一個驗證事件，其實很難看出使用表格編輯測試腳本的優勢。
不過，如果需要連續新增十個驗證事件至腳本中，對表格來說，僅需額外新
增十個空列，並且在對應的欄位內輸入元件名稱、Action 類型、預期的結果
為何、使用的函式及其參數值即可。但是，如果使用 GTT Macro 編輯器就必
須要連續操作十次驗證事件的編輯視窗才有辦法達成，二者相較之下便能清
楚看出使用表格編輯測試腳本的確輕鬆以及方便許多。 
減少所需認識的使用者介面 
GTT4 節點的種類非常多，在 GTT Macro 編輯器中與事件相關的節點就
有：基礎事件、巨集事件、引用巨集的事件及驗證事件等，每一種事件都各
自有其專屬的編輯視窗，也因此使用者必須學習各個視窗的操作方式，這對
使用者來說無疑是一種額外的負擔。 
7 
 
圖 9 TabularSE 的編輯區域 
為了讓測試人員在開發以及閱讀腳本上能夠更方便、快速，系統提供三個人
性化的設計： 
(a) 下拉式選單 
開發測試腳本時，藉由下拉式選單選擇元件名稱(圖 10)、操作事件(Action)，
測試人員不需要背誦大量的指令，也可避免輸入錯誤內容至表格中。 
 
 
圖 10 下拉式選單 
(b) 事件的參數提示 
系統提供事件參數提示之功能(圖 11)，方便測試人員選擇某一個事件後，能
夠立即得知需要附帶的參數，減少翻閱(查詢) API 文件的次數。 
 
 
圖 11 自動提示事件之參數 
(c) 自動更新表格的欄標題 
當表格中的事件類型相近(或相同)時，開啟自動更新欄標題後，系統會將參
數區域的欄標題更新為事件的參數名稱，使得測試人員在閱讀腳本時，能夠快速
了解某一欄參數的含義，如圖 12 所示。 
 
9 
本實驗的受測人數以及方法請參照表 3。本實驗邀請臺北科技大學 22
名資工系學生(含大四與研究生)、26 名電資專班大一學生參加實驗。為了讓
實驗過程的公平性，受測者平均分成 A、B 二組(每組 24 人)，實驗時未提供
任何物資或獎品。 
表 3 實驗一的參加人數與方法 
A 組：24 
實驗人數 
B 組：24 
待測程式(第一階段) 小算盤 
A 組：先表格，後文字 
腳本類型 
B 組：先文字，後表格 
待測程式(第二階段) 數獨 [7] 
A 組：先文字，後表格 
腳本類型 
B 組：先表格，後文字 
 
本實驗分成二階段(圖 13)，每個階段再細分成二個回合。第一階段讓受
測者閱讀小算盤的測試腳本，由於小算盤的特性，測試腳本中的事件幾乎都
相同，非常接近 Fit Column Table 的形式，在第一回合時 A 組人員閱讀表格
式的測試腳本，B 組人員閱讀文字式的測試腳本；第二回合時，變換閱讀的
腳本格式，由 A 組人員閱讀文字式的測試腳本，而 B 組人員則閱讀表格式
的測試腳本。第二階段是讓受測人員閱讀數獨的測試腳本，測試腳本中的事
件差異頗大，並不像是 Fit Column Table，在第一回合時 A 組人員閱讀文字
式的測試腳本，B 組人員閱讀表格式的測試腳本；第二回合時，變換閱讀的
腳本格式，由 A 組人員閱讀表格式的測試腳本，而 B 組人員閱讀文字式的
測試腳本。 
 
實驗簡介
(10 min)
實驗開始
A組：文字
B組：表格
(10 min)
A組：表格
B組：文字
(10min)
問卷調查
(5 min)
題目說明
數獨
(5 min)
題目說明
小算盤
(5 min)
A組：表格
B組：文字
(10 min)
A組：文字
B組：表格
(10 min)
實驗結束
 
圖 13 實驗一之流程圖 
表 4 及表 5 是實驗結果統計。由表 4 可知當測試腳本的內容趨近於 Fit 
Column Table 型式時，表格式的測試腳本確實比文字式的測試腳本更具可讀
11 
影響腳本的可讀性；最後，超過三分之二的人認為，將每列內容切割成數格
顯示，在可讀性上是有幫助，僅少數人持反對意見。 
 
0
10
20
30
40
Q1 Q2 Q3
答
題
之
人
數
正向答案 沒意見 負向答案  
圖 14 測試腳本可讀性之部分問卷答案統計圖 
實驗二：測試工具便利性之比較 
表 6 為本實驗之 GQM 分析，實驗目標是｢就受測者的觀點而言，使用
TabularSE 編輯測試腳本比 Robot Framework 更具便利性｣。本實驗設計一個
開發測試腳本的流程，讓受測者用測試工具實際開發測試腳本，除了記錄每
一種工具開發腳本所需時間外，還計算使用不同工具時的通過率(成功率)。
與第一個實驗相同，我們設計一份與測試工具之操作便利性相關的問卷，於
實驗結束前請每位受測者填寫此份問卷。最後，綜合這三項數據檢驗是否達
成期望之目標。 
表 6 實驗二之 GQM 分析 
Goal 就受測者的觀點而言，使用 TabularSE
編輯腳本比 Robot Framework 更具便
利性 
Question 在編輯測試腳本上，TabularSE 是否
比 Robot Framework 更方便？ 
Metric  編輯腳本所需之時間 
 使用工具編輯腳本之通過率 
 問卷調查之結果 
 
本實驗的受測人數、方法請參照表 9。本實驗邀請臺北科技大學資工系
與電資專班(合計 30 名)大二學生參加實驗，和實驗一相同，參加者被隨機
平均分成二組(每組 15 人)。且沒有提供任何物資或獎品。 
13 
意及同意的使用者歸類為正向答案，而勾選非常不同意及不同意的使用者則
歸類為負向答案。這三道問題的題目為：： 
Q1: 您覺得使用 TSE 編輯腳本比 RF 方便？ 
Q2: 您覺得 TSE 比 RF 容易學習？ 
Q3: 相較 RF，如果以後有機會的話，您比較想使用 TSE 編輯腳本？ 
從圖 16 可以發現：超過 83%的使用者都同意 TabularSE 在編輯腳本上
確實是很方便的；超過 80%的人認為 TabularSE 比 Robot Framework 容易學
習；超過 83%的受測者一致認同，未來如果還有撰寫腳本之需求，仍然會選
擇 TabularSE 做為開發腳本使用的測試工具。 
 
 
圖 16 測試工具便利性之部分問卷答案統計圖 
從實驗一的結果，可以得知｢表格式測試腳本確實比文字式測試腳本更容易
閱讀｣，且實驗二的結果也證實｢在編輯測試腳本上，TabularSE 確實比 Robot 
Framework 更方便｣，因此實驗驗證本計畫所提之方法(工具)確實能提升測試腳本
的可讀性且 提升測試腳本開發之便利性。 
 
六、 計畫成果自評 
本計畫執行順利，已依照原計畫目標，設計與實作一個表格式的腳本編輯
器，稱為 TabularSE，讓測試人員能夠使用日常熟悉的表格型式來開發測試腳本。
實驗結果證實表格型式測試腳本確實有助於提升測試腳本的可讀性，且
TabularSE 確實能提高編輯測試腳本的便利性。計畫成果已撰寫國內會議論文「以
表格呈現與編輯 GUI 測試腳本之方法」，發表於 2012 台灣軟體工程研討會。目
前正進一步改寫為國際會議論文中。 
 
七、 參考文獻 
[1]  GTT (GUI Testing Tool), May 01, 2010 [Online]. Available: http://gtt.sourceforge.net 
[2]  王榮麒，一個支援白箱 GUI 測試的 Eclipse 外掛，碩士論文，國立台北科技大學資訊工程
所台北，2007 年 
[3]  林雅真，史記體例及章法結構之研究，碩士論文，國立政治大學國文教學碩士在職專班，
15 
[21]  Atif M. Memon, “GUI Testing : Pitfalls and Process,” IEEE Computer, Volume: 35, Issue: 8, 
Aug. 2002, pp. 87-88 
[22]  Atif.M. Memon, Pollack, M.E.; Soffa, M.L, “Hierarchical GUI Test Case Generation using 
Automated Planning,” IEEE Transactions on Software Engineering, Vol. 27, No. 2 , pp 144-155, 
2001. 
[23]  Atif M. Memon, Mary Lou, Soffa Martha, and E. Pollack, “Coverage criteria for GUI testing,” 
Proceedings of the 8th European software engineering conference held jointly with 9th ACM 
SIGSOFT international symposium on Foundations of software engineering, vol. 26, pp. 256 - 
267, 2001 
[24]  Satadip Duta, “Abbot-A Friendly JUnit Extension for GUI Testing,” Java Developer Journal, 
April 2003, pp 8~12. 
[25]  Ostrand, Thomas, Anodidi, Aaron, Foster, Herber; Goradia, and Tarak, “A Visual Test 
Development Environment for GUI Systems,” Proceedings of ACM SIGSOFT International 
Symposium on Software testing and analysis (ISSTA), Vol. 23, 1998. 
[26]  Zunliang Yin, Chunyan Miao, Yuan Miao and Zhiqi Shen, Actionable Knowledge Model for 
GUI Regression Testing, the 2005 IEEE/WIC/ACM International Conference on Intelligent 
Agent Technology (IAT'05), pp. 165-168, 2005. 
[27]  Qing Xie and Atif M. Memon, “Rapid "Crash Testing" for Continuously Evolving GUI-Based 
Software Applications,” International Conference on Software Maintenance 2005 (ICSM'05), pp. 
473-482, Sept. 25-30, 2005 
[28]  Antti Kervinen, Mika Maunumaa, Tuula Paakkonen, and Mika Katara, “Model-Based Testing 
Through a GUI,” Proceedings of the 5th International Workshop on Formal Approaches to 
Testing of Software (FATES 2005), Springer LNCS, no. 3997, Edinburgh, Scotland, UK, July 
2005. 
[29]  Fevzi Belli, Finite-State Testing of Graphical User Interfaces, Proceedings. 12th International 
Symposium on Software Reliability Engineering (ISSRE 2001), pp.34-43, 2001. 
[30]  Richard K. Shehady and Daniel P. Siewiorek, A Method to Automate User Interface Testing 
Using Variable Finite State Machines, Twenty-Seventh Annual International Symposium on 
Fault-Tolerant Computing (FTCS), pp.80-88, 1997. 
[31]  Yanhong Sun and Edward L. Jones, “Specification-Driven Automated Testing,” Proceedings of 
the 42nd Annual Southeast Regional Conference, pp 140-145, 2004. 
[32]  Jenifer Tidwell, “COMMON GROUND: A Pattern Language for Human-Computer Interface 
Design”, 1999. December 10, 2008 [Online]. Available: 
http://www.mit.edu/~jtidwell/common_ground.html  
[33]  Jenifer Tidwell, “Designing Interfaces: Patterns for Effective Interaction Design”, O'Reilly 
Media, Inc. November 21, 2005. 
[34]  Kai-Yuan Cai, Lei Zhao, Hai Hu, Chang-Hai Jiang, “On the Test Case Definition for GUI 
Testing.” Quality Software, (QSIC 2005: 19-28), 2005. 
[35]  Woei-Kae Chen, Zheng-Wen Shen, and Che-Ming Chang, "GUI Test Script Organization with 
Component Abstraction," The Second International Conference on Secure System Integration 
and Reliability Improvement (SSIRI 2008), Yokohama, Japan, July 14-17, 2008. 
[36]  Woei-Kae Chen, Hsien-Ta Wu, Jung-Chi Wang, and Zheng-Wen Shen, "An Automatic GUI 
Testing Approach Using Hierarchical Finite State Machine," The Fourth Taiwan Conference on 
Software Engineering (TCSE08), June 13-14, 2008. 
[37]  Woei-Kae Chen, Sheng-Kai Wen, Zheng-Wen Shen, and Jung-Chi Wang, "Contract-based 
Automatic GUI Test Case Generation," The Fourth Taiwan Conference on Software Engineering 
(TCSE08), June 13-14, 2008. 
[38]  Woei-Kae Chen, Jung-Chi Wang, and Zheng-Wen Shen, "An Eclipse Plug-in for White-Box 
GUI Testing," The Third Taiwan Conference on Software Engineering (TCSE07), June 8-9, 
2007. 
國科會補助專題研究計畫出席國際學術會議心得報告 
                                日期：101 年 8 月 20 日 
 
 
一、參加會議經過 
    第 13 屆 ACIS 國際軟體工程、人工智慧、網路與平行/分散式計算研討會於 8/8 日至 8/10
日在日本京都舉辦，主辦單位是 ACIS (International Association for Computer and Information 
Science)，並由京都工業大學(Kyoto Institute of Technology)協辦。會議地點在京都車站旁的
Campus Plaza Kyoto(圖 1)。共發表 165 篇論文，並有超過 20 個國家的學者參與，論文全部收
錄於 IEEE Xplore，是一個不錯的國際會議。 
    本人於 8/8 日早上九點八點三十分到會場完成報到(圖 2)。報到時意外發現負責辦理報到
的服務人員中竟有人會講中文，親切之感頓時油然而生。大會準時在 9:20 開幕，主席 Teruhisa 
Hochin 說明大會的緣起與目的，接著第一場 Keynote 由 Prof. Morshed U. Chowdhury (Deakin 
University, Melbourne, Australia)介紹 Scalable RFID Security Protocol。然後第二場 Keynote 由
Noriyoshi Sano 介紹 Research Trend of Development Technologies for Automotive Control 
計畫編號 NSC 100-2221-E-027 -054 
計畫名稱 以表格為基礎的自動化圖形使用者介面驗收測試 
出國人員
姓名 陳偉凱 
服務機構
及職稱 
台北科技大學 
資訊工程系副教授 
會議時間 101 年 8 月 8 日至 101 年 8 月 12 日 會議地點 日本京都 
會議名稱 
(中文) 第 13 屆 ACIS 國際軟體工程、人工智慧、網路與平行/
分散式計算研討會 
(英文) 13th ACIS International Conference on Software 
Engineering, Artificial Intelligence, Networking and 
Parallel/Distributed Computing (SNPD 2012) 
發表題目 
(中文) 圖形使用者介面測試腳本之壞味道與重構的方法 
(英文) Bad Smells and Refactoring Methods for GUI Test Scripts 
 圖 3 Session 2A Software Testing 
 
圖 4 報告時使用的投影片 
 
二、與會心得 
    近幾年台灣在推動軟體工程的教育與研究不遺餘力，有很多進步。但是，在 SNPD 2012
中來自台灣的作者或是註冊的參與者卻相當少，反而是看到許多大陸的學者，舉例說，在本
人所報告的 Session 中，有兩篇論文是由來自大陸的學生所報告的，其指導教授則當場在台下
指導，由此可見，大陸對於學術研究的重視，同時也反映大陸對於參與學術會議的補助情形。 
    本人主要專注於軟體工程論文發表場次，在會場中本人與來至世界各地專家學者當面討
論報表結構模型的學理基礎，與實作的相關技術與議題，獲得諸多建議與經驗。除資訊技術
主題外，還對多媒體及網路等相關領域進行研討，相信本會議對學術交流與提升將具有一定
之貢獻。 
Bad Smells and Refactoring Methods for GUI Test Scripts
Woei-Kae Chen and Jung-Chi Wang
Department of Computer Science and Information Engineering
National Taipei University of Technology
Taipei, Taiwan
Email: {wkchen, t6599006}@ntut.edu.tw
Abstract—Testing the GUI (Graphical User Interface) of a
software application is typically accomplished by developing a
GUI test script composed of sequences of events and assertions.
A GUI test script is in a sense similar to the source code
of a program, since events and assertions are like source-code
statements, which are executed line by line. Therefore, like source
code, a GUI test script may have bad smells, and refactoring is
an effective technique that can eradicate bad smells, making the
script better and easier to maintain. This paper studies the bad
smells a GUI test script may have and the refactoring methods
that can be applied to remove the bad smells. A total of 11 bad
smells are identiﬁed and 16 refactoring methods are proposed.
The refactoring methods have been implemented in a GUI testing
tool, called GTT, to support the automatic refactoring of GUI
test scripts.
Keywords-GUI testing; test script; refactoring; smell;
I. INTRODUCTION
Software testing plays an important role in software de-
velopment through which software defects are uncovered. In
software testing, testing the correctness of a software applica-
tion through its graphical user interface (GUI) is called GUI
testing [1], which assumes increasing importance these days
since GUI is pervasive in all kinds of software applications.
In general, testing a GUI is accomplished by developing a
GUI test script (called simply test script in this paper), which
is composed of sequences of actions [1], [2]. An action is
either an event, which drives the GUI, or an assertion, which
afﬁrms the correctness of the application. The test script can
be automatically executed by GUI testing tools (e.g., [2]–[4])
to verify whether the application behaves correctly. Figure 1
shows a simple test script supported by Robot Framework [3].
The test script Login sequentially opens a login page, enters
account name into the account widget, enters password into
the password widget, and then submit the login form. To
simplify the reuse and also improve the maintainability of
a test script, keyword-driven testing (KDT) [3]–[5] has been
extensively used. In KDT, a keyword performs a number of
actions and/or some other keywords. Testers use keywords to
develop test scripts. For example, the Open Login page
of Fig. 1 is a keyword that is called by the Login keyword.
The idea behind KDT is similar to the procedural abstraction
used in a programming language (e.g, C), where a keyword is
analogous to a function (method).
Component abstraction (CA) [6] is a generalization of KDT,
allowing testers to perform object abstraction for a test script.
Several components in a GUI is grouped and considered as
an object, called a macro component, and each object is
Keyword Action Argument Argument
Open Login Page Open Browser http://host/login.html
Title Should Be Login Page
Login Arguments ${account} ${password}
Open Login Page
Input Text account ${account}
Input Text password ${password}
Submit Form
Fig. 1. A simple test script.
associated with a number of keywords, called macro events.
The GUI is hierarchically grouped into a macro model, which
uses macro components and macro events to represent the
structure and behavior of the GUI. A tester uses macro events
to develop a test script. Since there is a correspondence
between the macro model and the GUI, when the GUI is
changed or modiﬁed (e.g., a radio button is replaced with a
check box), testers need only little effort to maintain or repair
the test script. As an analogy, a KDT script is like a C program
constructed by using hierarchical function calls (keywords).
On the other hand, a CA script is more like a C++ program
constructed by using hierarchical objects (macro components)
and object interactions.
When preparing a test script, there will always be time
when some modiﬁcations need to be made. Sometimes testers
encounter problems when they try to modify the script. For
example, if there are many places in the script where the same
sequence of actions is being used, and that very sequence of
actions need to be modiﬁed, all the places with the same
sequence of actions need to be modiﬁed altogether. The
reason behind this tedious situation is that the script does not
effectively make use of keywords or macro events. Instead,
actions are copied directly. In the case of source code, this is
the equivalence of copying and pasting a piece of code without
using function (method) calls, which is known as duplicated
code and is a bad smell [7]. When a piece of code has a bad
smell, the code should be refactored to eliminate the bad smell
to improve code quality [7], [8]. Same for the test script. When
a bad smell shows up in a test script, a tester needs to apply
refactoring methods to get rid of the bad smell.
This paper studies the bad smells that may appear in a
test script, and the refactoring methods that can be applied to
remove the bad smells for both KDT and CA. A total of 11 bad
smells are identiﬁed and 16 refactoring methods are proposed.
These refactoring methods have been implemented in a GUI
2012 13th ACIS International Conference on Software Engineering,  Artificial Intelligence, Networking and Parallel/Distributed
Computing
978-0-7695-4761-9/12 $26.00 © 2012 IEEE
DOI 10.1109/SNPD.2012.10
289
Fig. 2. An example of Lack of Macro Events
(a) The test script
(b) The actual GUI structure
Fig. 3. An example of Inconsistent Hierarchy
an inconsistency in structure between the macro model and the
real GUI. In case that the test script needs modiﬁcation, it will
be difﬁcult for the tester to pinpoint the location of Action
in the test script, creating a maintenance difﬁculty.
Like code smells, having bad smells in a test script indicate
that the script may possibly have maintenance problems.
However, it is the tester’s decision whether to remove the
bad smells. If removal is necessary, the refactoring methods
discussed in the next section can be used.
III. GUI TEST SCRIPT REFACTORING
When a piece of code has bad smells, many code refactoring
methods [7] can be used to remove the smells and improve
the nonfunctional attributes of the code. Likewise, when a
test script has bad smells (Section II), a tester needs a set
of refactoring methods to help remove the smells. Many code
refactoring methods can be adapted to the refactoring of test
scripts. For example, when a variable, function, or class is
badly named, the code-refactoring method Rename can be used
to globally change the name of the variable, function, or class.
Similarly, in a test script, if a keyword, macro component,
macro event, or primitive event is badly named, the Rename
method can be applied.
To systematically obtain the refactoring methods applicable
to test scripts, all of the refactoring methods described in [7]
have been extensively studied. Those that are suitable for
TABLE II
A LIST OF TEST SCRIPT REFACTORING METHODS
Refactoring method Description Note
Rename Rename macro components, macro events,
primitive events
Add Parameter Add a macro event parameter
Remove Parameter Remove a macro event parameter
Rename Parameter Rename a macro event parameter
Extract Parameter Suppose a macro event e uses a constant
value c for one or more places. Create
a new parameter p for e and replace all
occurrences of c with p so that c is param-
eterized.
Inline Parameter Suppose a macro event e has a parameter
p. Replace all usages of p with a constant
value, and remove p from parameters of e.
Extract Macro Event Extract a series of actions inside a macro
event to create a new macro event
Inline Macro Event The inverse of Extract Macro event, i.e.,
replace a macro event call with all the
actions of the macro event being called
Move Component Move the primitive components of a macro
component into another macro component
CA only
Move Macro Event Move a macro event of one macro compo-
nent into another macro component
CA only
Move Macro Com-
ponent
Move a macro component of one macro
component into another macro component
CA only
Extract Macro Com-
ponent
Extract a part of macro component to form
a new macro component
CA only
Inline Macro Com-
ponent
Move all the attributes of one macro com-
ponent to another macro component and
remove the former macro component
CA only
Remove Middle Man Remove the macro event that delegates all
its responsibility to another macro event
CA only
Hide Delegate Hide the calls to the macro event CA only
Rename Window Ti-
tle
Rename the window title of a primitive
component
CA only
refactoring test scripts are kept, others discarded. In addi-
tion, by investigating the bad smells listed in Table I, new
refactoring methods are proposed. The results are shown in
Table II. Note that, for simplicity, the term macro event is
used interchangeable with the term keyword in the table and
the rest of the paper. Overall, 16 refactoring methods are
proposed in this paper. All of the refactoring methods have
been implemented in a GUI testing tool, called GTT, allowing
the testers to perform automatic refactoring (or semi-automatic
refactoring, since bad smells in the tests scripts are found
manually) . Note that, to perform a correct refactoring, the
refactoring tool must be aware of the syntax and semantics
of the script. For example, when renaming a variable in the
source code, the refactoring tool must identify and rename
all references of the same variable at once. Likewise, when
refactoring a test script, the Rename method must also do the
same. Without the refactoring tool, a tester can still perform
refactorings by modifying the test script line by line. However,
such modiﬁcations will be very tedious and time-consuming.
We use Extract Macro Event as an example to explain
how a test script is refactored using GTT. Extract Macro
Event corresponds to the Extract Method described in [7].
What it does is to extract a series of actions in a macro
event as a new macro event and make the original macro
291
TABLE IV
CROSSWORD SAGE TEST SCRIPT INFORMATION
KDT CA
Number of primitive components 69
Number of test cases 34
Statement Coverage 81.7%
Number of macro components NA 21
Number of macro events (key-
words)
45 49
Average number of nodes in each
macro component
NA 6.6
Average number of actions in each
macro event
4.3 4.1
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
1 6 11 16 21 26 31
Number of test cases
N
um
be
r o
f m
ou
se
 a
ct
io
ns
CA
KDT
Fig. 7. Accumulated number of mouse actions in testing Crossword Sage
/ 89:06) times slower than the CA script; for the mouse actions,
preparing the KDT script needed 1.5 (8375 / 5646) times more
actions than the CA script. Figure 7 shows the accumulated
number of mouse actions. The results indicated that, as the
number of test cases grew, the search cost of KDT eventually
grew far above that of CA, which made preparing the CA
script more efﬁcient than the KDT script, when the test script
became large.
After the test scripts were constructed, the authors examined
the test scripts and found a few bad smells, which are shown
in Table V, indicating that bad smells do appear in test scripts.
For the KDT script, the smells Duplicated Action, Long Macro
Event, and Shotgun Surgery were found. In addition, since the
KDT script treats the entire GUI as a single component, which
contained 45 keywords and 69 widgets, we also consider that
the KDT script had Large Macro Component smell (though,
technically speaking, a KDT script does not have any macro
components at all). For the CA script, we found more smells,
which include Feature Envy, Lack of Macro Events, and Lack
of Encapsulation that were related to objects. Note that having
more smells did not imply that the CA script was more
problematic than the KDT script. Instead, the object structure
of the CA script allowed the detection of more smells (see
Section II). Therefore, it was easier to identify problems in
the CA script.
Could the bad smells be removed? The tester was instructed
to follow Table III and apply refactoring methods using GTT
TABLE V
BAD SMELLS FOUND IN THE TEST SCRIPTS OF CROSSWORD SAGE
KDT CA
Duplicated Actions Duplicated Action
Long Keyword Long Macro Event
Shotgun Surgery Shotgun Surgery
Large Macro Component* Feature Envy
Lack of Macro Events
Lack of Encapsulation
TABLE VI
RTF WORD PROCESSOR TEST SCRIPT INFORMATION
KDT CA
Number of primitive components 39
Number of test cases 24
Statement Coverage 90.6%
Number of macro components NA 11
Number of macro events (key-
words)
47 67
Average number of nodes in each
macro component
NA 10.6
Average number of actions in each
macro event
3.3 2.6
refactoring tools. For the KDT script, the Duplicated Actions,
Long Keyword, and Shotgun Surgery smells were easily
removed in a few minutes. However, Large Macro Event was
unremovable, because the number of keywords and widgets
cannot be made smaller. For the CA script, all the smells
were also easily removed in a few minutes. When the tester
performed refactoring, GTT refactoring tools helped the tester
complete the refactorings quickly. What if such automatic
refactoring tools were not available? The authors asked the
tester to perform exactly the same refactorings line by line
without using the tools. It took several hours for the tester
to complete the refactorings. Therefore, the refactoring tools
were indeed very useful.
The second experiment repeated the ﬁrst for a different
application, RTF Word Processor [10]. From the experience of
the ﬁrst experiment, the tester was now aware of bad smells
and was instructed to avoid smells as much as possible when
developing test scripts. The information of the resulting test
scripts are shown in Table VI. The ﬁndings of the recorded
time and mouse actions (not shown in this paper for brevity)
were similar to the ﬁrst experiment. After the test scripts were
created, when the authors examined the test scripts, bad smells
were still found. For the KDT script, the smells were Long
Keyword, Shotgun Surgery, and Large Macro Component
(note: the Long Macro component smell is inevitable for
KDT). For CA, the only smell found was Feature Envy. The
results indicated that, as the number of test cases grew, CA
outperformed KDT both in time and in quality. This is partly
because a CA script is easier to manage than a KDT script,
which in a way makes the test script easier to avoid having
bad smells. Again, the tester were able to remove all the bad
smells in a few minutes using GTT refactoring tools.
From the experiment results, we can conclude that: (1) by
293
國科會補助計畫衍生研發成果推廣資料表
日期:2012/12/24
國科會補助計畫
計畫名稱: 以表格為基礎的自動化圖形使用者介面驗收測試
計畫主持人: 陳偉凱
計畫編號: 100-2221-E-027-054- 學門領域: 程式語言與軟體工程
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
