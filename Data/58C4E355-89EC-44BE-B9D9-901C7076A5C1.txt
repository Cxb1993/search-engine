II. VMP SYSTEM ENVIRONMENT 
Our VMP creates a virtualized multithreaded emulation 
environment between the real host machine and the 
applications, so that the end user can run his/her application 
softwares in a multithreaded style on an abstract machine 
which they desired. The platform consists of a multithreaded 
emulation engine to allow the users’ codes compiled into a 
parallel code for other machine on it. In VMP design, we 
enable the Linux-X86 machine to directly support the Alpha 
application binaries [8] but with a parallel execution style. The 
VMP operating environment and its virtual memory space are 
shown in Fig.1. 
Host
machine
Virtual ABI
Virtual 
machine 
platform Application 
binary for alpha
Virtualizing
software 
Host kernel
0x00000000 
0xffffffff
0xcfffffff 
Kernel/User linear addresses 
User
space
Kernel 
space
Hardware
Virtualizing 
software
OS
Application binary
 
Fig. 1. VMP operating environment and its virtual memory space. 
 
A program binary coded with an ABI (application binary 
interface) can run on any system that supports the same ABI 
without relinking or recompilation. Based on this principle, 
VMP can allow the guest applications executing on it with the 
same virtual ABI, which is composed of user ISA interface 
and a system call interface with some multithreaded 
instructions, and the VMP’s virtualizing software then 
emulates both user level instructions and system calls in 
parallel. 
III. VMP SYSTEM ARCHITECTURE AND DESIGN 
The VMP is implemented with many components for 
supporting Alpha application binaries with concurrent 
multithreaded design [9], and its system architecture is shown 
in the Fig.2. All of its components are described as follows. 
Guest Memory Image
Guest Memory Image
Loader 
Guest Memory Image
Emulation Engine
OS Call Emulator
Code Cache
Code Cache
Manage
fetch
decodeexecute
Data_start_address
Reg n-1
Reg 2
Reg 1
Reg 0
Program countercontext
Data_start_address
Reg n-1
Reg 2
Reg 1
Reg 0
Program counter
Data_start_address
Reg n-1
Reg 2
Reg 1
Reg 0
Program counter
contexts
Host Operation System  
Fig. 2. The system architecture of the VMP. 
A. Loader 
After the executable binary files are produced by the cross-
compiler builded on the Alpha host [8], the loader loads the 
application binary into a region of memory holding guest’s 
memory image according to the header of the executable 
binary files in the format of ELF standard on Linux OS. 
B. Threads contexts 
Each guest application has a corresponding context 
structure, which records all executing information about the 
thread of a guest application. The most significant part of the 
context are the value of user-level registers which store the 
user-managed values and state, and the thread’s system state 
which can be at the blocked, running or dead state. VMP will 
maintain the contexts for each thread at run-time to prevent 
the running threads and machine from error state. 
C. Multithreaded emulation engine, MEE 
The MEE of the VMP is a multithreaded ABI interpreter 
which involves the guest memory and all contexts of 
threads/process alive to emulate the guest codes in parallel. 
The interpreter’s main structure is a loop composed of fetch, 
decode and execution, and is responsible for emulation of the 
guest Alpha ISA [8] and ABI by multithreaded execution. For 
doing microprocessor’s functional verification, this engine can 
exploreds the detailed behaviour of Alpha in a multithreaded 
style. A g-share predictor and a branch target buffer are used 
for processing the branch instructions. MEE provides two 
multithreaded instruction queues for integer and floating 
instructions, a multithreaded rename table and a reorder buffer 
for supporting the out of order multithreaded execution. 
D. Guest memory image 
The guest memory image contains the application codes 
and data of the guest. It is divided into three segments which 
are stack, text and data. The data segment and text segment 
hold the data and instructions of the guest application, 
respectively, and the stack segment holds the runtime 
generated data. 
Memory protection among threads is done by a system call, 
mmap(), host machine provided. It can be used for mapping a 
region of the address space for which the run-time wants 
access control. If the guest application tries to an illegal access, 
and then the SIGSEGV signal is delivered. The access 
privilege type of the text segment of the guest application is 
set to execute and read, and the access privilege types of the 
other segments like data segment are set to read and write. 
For multi-threads concurrent execution, a run-time 
software-supported translation table is constructed for 
multithreads virtual addresses translation. This table is used 
for translating the address of the guest memory to the address 
of the host memory. How this method works for multi-thread 
application is shown in the Fig. 3. Each application has its 
complete guest address space mapped to different region of 
host memory space, so they can work independently and 
safely between each other.  
E. Code cache 
To increase the executing speed of the VMP, a code cache 
and cache management are involved. The interpreter wastes 
time at decoding the same instruction, so the code cache is 
used to hold the decoded information. When the same 
decoded instructions are fetched again, the interpreter uses the 
decoded information already in the code cache and directly 
executes them, and will not decode the instructions again. 
 - 2 -
run = 0
run = 1
forkterminate
blocked
lock fail
get lock
initial
 
 - 4 -
Fig. 5. The state diagram of a thread. 
 
design the barrier algorithm on the VMP without losing too 
much in communication between threads, and it results in 
good efficiency in the performance. We construct a barrier 
structure which provides a synchronization key for each 
thread. Each thread to be synchronized will be blocked until 
all relative threads have reached the barrier and then released 
one by one. We take two examples, the value accumulator 
from 1 to 100,000 and the JPEG encoder, on the VPM to 
explain its efficiency and flexibility with barriers. 
V. EXPERIMENT RESULTS 
   The operations of the value accumulator almost can con-
currently execute. From the Table 2, we can obviously see that 
the total executing time of the 2-thread case is about 1.5 times 
to that of the 1-thread case not the 2 times. The reason is that 
we must add some additional processing including MS 
synchronization instructions and the barrier mechanism to the 
original application for multithread execution. Therefore, the 
results show that total number of instructions in the 1-thread 
case is smaller than that in the 2-thread case. When the 
number of threads keeps increasing, the execution still in 
flexibility go on, but the performance is not scaleable 
increasing, because the IPC (instructions per cycle) reaches 
the limit of the number of the execution units that VMP offers. 
Table 2: The results of the value accumulator example. 
Thread 
No: TN. IPC 
Total 
cycles 
Instructions : 
Integer  floating load/store  syn.
1 2.978 100728 300046 0 1 0 
2 5.865 68221 400271 0 48 23 
4 5.838 68567 400510 0 83 42 
8 5.807 68984 400881 0 151 76 
 
In another experiment on the JPEG encoder, we divided a 
source image into several small parts for encoding. 
RGB2YUV, DCT and quantization stages for each partition 
can concurrently execute with several threads, and the VMP 
easy splits the JPEG encoding process into several threads 
responsible for one partition in advance, and then processes 
the stages of RGB2YUV, DCT and quantization 
simultaneously using a barrier synchronization. When all 
encoding threads reach the barrier, they are joined to a single 
main thread for finishing the entropy coding. The basic 
multithreaded execution flow of it is shown in Fig. 6. 
Different partitioned schemes can be chosen by users and all 
can be flexibly executed by VMP efficiently. 
The ratio of performance increment of the JPEG encoder is 
not the same as that of the value accumulator, because the 
threads of the JPEG encoder can not fully concurrently 
execute. If the JPEG encoder spends about 20% computation 
on the part of RGB2YUV, DCT and quantization, and then it 
only can increase 20% performance at most. From Table 3 we 
can see the performance increases not much with the 
increasing numbers of the split threads run on VMP.  
From Table 2 and 3, we can say that VMP is a flexible 
infrastructure for multi-threaded NoC design and verification, 
and it can explore a variety of NoC applications ranging from 
single thread designs to dynamically scheduled multiple 
threads designs with efficient synchronization structures and 
instructions. 
1 2
3 4
RGB2YUV
RGB2YUV
RGB2YUV
RGB2YUV
DCT
DCT
DCT
DCT
QUAN
QUAN
QUAN
QUAN
Barrier
Entropy coding
and writing into 
memory
source 
image 
 
Fig. 6. The JPEG encoder with synchronization. 
Table 3: The results of the JPEG encoder; TN: Thread no. 
TN. IPC Total cycles 
Instructions : 
integer       floating     load/store   syn
1 1.778 45816199 72543246 9883486 31391688 0
2 2.305 34956097 63867807 7418778 28085029 41
4 2.750 29301388 59683531 5753926 26478352 74
8 3.010 26762651 57713243 4992188 25999528 152
VI. CONCLUSION AND FUTURE WORK 
In this paper, the VMP fusing both instruction level 
parallelism (ILP) and fine-grain thread level parallelism (TLP) 
is presented. It is constructed with an emulation engine for 
virtual ISA and an OS call emulator for system calls to 
completely explore SoC/NoC multithreaded applications. 
From the results of evaluated benchmarks, it demonstrates 
VMP is a flexible and efficient platform for embedded 
systems multithreaded functional exploration. 
In the future, we will add additional MS-specific 
instructions into VMP for communicating between threads 
directly using internal registers not the guest memory to 
further increase its performance. On the other hand, we will 
try to add reconfigurability into VMP, and execute some 
threads on the reconfigurable function units instead of using 
platform’s computation units. 
Acknowledgements The research is partially supported by National 
Science Council under contract no. NSC-97-2221-E-006-253. 
References 
[1] A. Sangiovanni-Vincentelli, et al., “Benefits and challenges for platform-
based design”, Proc. of DAC, pp.409-414, 2004. 
[2]  J. E. Smith and R. Nair, Virtual Machines: Architectures, 
Implementations and Applications, Morgan Kaufmann Publishers, 2005. 
[3] J. An, e.t. a.l., "VMSIM: Virtual Machine Based a Full System 
Simulation Platform for Microprocessors’ Functional Verification," Proc. 
of the Third Interna. Conf. on Information Techn.: New Gen., 2006. 
[4] S. Brini, et al., “A flexible virtual platform for computational and 
communication architecture exploration of DMT VDLS modems”, Proc. 
of DATE, pp. 164-169, 2003 
[5]  http://fabrice.bellard.free.fr/qemu/   
[6] E. H. Debaere and J. M. Van Campenhout. Interpretation and Instruction 
Path Coprocessing, MIT Press, Cambridge, 2002. 
[7] A. Chernoff, M. Herdeg, R. Hookway, C. Reeve, N. Rubin, T. Tye, S. 
Yadavalli, and J. Yates. Fx!32 a profile-directed binary translator, 1998. 
[8] “Alpha 21264/EV67 Microprocessor Hardware Reference Manual," 
Compaq Comp. Corp., Product #DS-0028B-TE, 2000. 
[9] D. T. Marr,  e.t. a.l., “Hyper-Threading Technology Architecture and 
Microarchitecture,” Intel Technology Journal Q1, 2002 
[10] D.M. Tullsen, e.t. a.l., “Exploiting Choice: Instruction Fetch and Issue on 
an Implementable Simultaneous Multithreading Processor,” In 23rd 
Interna. Symp. on Computer Architecture, May, 1996.  
迴響。在過去的幾十年中，一般透過增加發射寬度和提高時鐘頻率來提高處理器
性能，利用超純量發射、亂序發射執行、超級管線、動態轉移預測、大容量晶片
內 Cache 等技術來開發 程式的指令級並行性(ILP)。但是，增加發射寬度使設
計的複雜性急劇增加，這一方面不利於時鐘頻率的提高，另一方面 也難以適應
深次微米技術下日益顯現的導線延遲大於閘延遲的負面影響，也使處理器設計驗
證成本難以承受。更重要的是，指令之間的資料和控制相關，可以開發的 ILP 也
有限，當發射寬度大於 4 時，大多數應用可以獲得的性能提升十分有限。提高
時鐘頻率，使管線級數增加，轉移預測失敗和 Cache 失效的代價也隨之增加，
這樣提高時鐘頻率獲得的性能十分有限，另外頻率提升使功耗也隨之增加，使晶
片封裝和冷卻代價也水漲船高。這些都使經典的超純量架構處理器難以進一步提
高處理器性能。 
從應用的角度看，如線上事務處理(OLTP)、決策支援系統(DSS)和 Web 服務
等這類應用的特點是具有豐富的執行緒級 並行性(TLP)而缺少 ILP，傳統處理器
架構難以開發，一般是採用多處理機系統開發。隨著積體電路技術的發展，利用
先進技術將多個處理器核集成到單個晶片上以開發 TLP 成為可能。因此，史坦
福大學的研究人員提出了單晶片多處理器(CMP)架構。CMP 架構的主要思想是
通過簡化超純量架構設計，將多個相對簡單的超純量處理器核集成到一個晶片
上，從而避免 線延的影響，並充分開發 TLP，提高吞吐量。通過簡化單核設計，
不僅有利於避免線延的影響，而且有利於時鐘頻率的提升，還有利於減少處理器
研製的時間週期。 
另外，CMP 架構能夠適應技術尺寸比例縮放，具有較好的可擴展性。因此，
CMP 架構成為當前和未來處理器發展的主要方向。但是，如何進行多核處理器
架構設計，以充分提高處理器性能是一個值得仔細研究和探索的問題。 
從某種意義上講，多核處理器就是一個晶片級的多處理機系統，成熟的多處理機
系統設計原理對多核處理器架構設計具有重要的參考價值。 從硬體角度看，多
核處理器架構設計主要包括兩個方面：單核架構設計與多核架構設計。 
的複雜性(單執行緒性能)與單核數量(多執行緒性能)之間進行取捨，以獲得最佳
性能。這與設計的應用目 標密切相關，對於高 TLP 應用來說，多個簡化的單
核設計優 勢明顯，但是對於低 TLP 的應用來說卻很不利，適度複雜的單核設
計才是更好的選擇。核數還受到晶片內 Cache 容量、晶片外帶寬和多核架構組
織等因素的限制。一個折中的方法是多 個核採用不同的架構設計，稱為“異構多
核”，而所有處理器核採用同一架構設計稱為“同構多核”。同構能簡化設計驗證
的代價，而異構能提高多核處理器對應用的適應性。 
單核架構設計還需要考慮的一個問題是：是否採用以及採用何種多執行緒技
術提單核資源利用率。單核多執行緒技術的使用能夠彌補多核架構資源利用率低
的缺點，雖然採用多執行緒技術會使單核設計複雜化，但是若能增加少量硬體實
現多 執行緒技術顯然是值得的，尤其是 SMT 技術獨特的靈活性十分具有吸引
力。這需要在硬體代價與可獲得的收益之間進行權衡。 
在當今的伺服器市場上，共用記憶體的多處理機系統是主流，這些系統硬體
支援單記憶體空間，且具有大量應用，主要包括對稱多處理機(SMP)和 Cache 一
致性非均勻存儲訪問(CC-NUMA)系統。其中 CC-NUMA 是 SMP 系統的擴展，
這二者是均勻存取(UMA)和非均勻存取(NUMA)的典型代表。這些系統的設計原
理對多處理器核的組織具有重要的參考價值。與 SMP 和 CC-NUMA 類似，多處
理器核組織架構可以有兩種：均勻 Cache 存取(UCA)和非均勻 Cache 存取
(NUCA)。UCA 架構中，多個處理器核與二級 Cache 通過互聯系統(匯流排或交
叉開關)互聯，所有處理器和對二級 Cache 存取延遲相同；在 NUCA 架構中，
每個處理器核具有本地二級 Cache，通過互聯系統對其他處理器核的二級 Cache
存取(延遲變長)。隨著晶片集中度的提高，也可以將三級 Cache 設計於晶片內。
需要強調的一點是，以上 4 種組織中，晶片內二級或三級 Cache 均為所有處理
器核共用。 
多核組織架構設計主要解決晶片內多核之間的互聯和通信機制問題。與多處
理機系統類似，多核之間的互聯可以採用匯流排或者交叉開關甚至多級網路實
用，而且都不是單純的某一種技術，而是在處理器架構的不同階 段部分採用或
者綜合採用了這些技術，比如 Piranha 綜合採用細粒度和 SMT 技術，Montecito
前端管線採用了分時多執行緒(TMT，類似粗粒度多執行緒)，而在存取時採用了 
SMT。在多核組織上，一般都考慮採用有效的匯流排或交叉開關互聯，提高核間
帶寬，另外，也採用了先進存取介面技術，提高存取頻寬，以緩解存取頻寬，如 
Niagara 利用 4 條 DDR2 存取通道，提供了 20 GB/s 的存取頻寬，Cell 採用兩
個 Rambus XDR 介面使存取頻寬高達 25.6 GB/s。 
值得一提的是 Niagara 和 Cell。Niagara 通過簡化單核，採用了 8 個順序
單發射單核，每個單核支援 4 執行緒的架構，能提供 32 個執行緒該處理器綜
合利用多核多執行緒架構提供了大量執行緒，是主要針對商業應用的典型。Cell
試圖為遊戲和多媒體、即時回應系統和科學計算等多種應用提供一個寬闊 的平
臺，因而採用了異構設計：順序雙發射的 PowerPC 核 (PPE) 和 8 個雙發射增
效核 (SPE)，其中 PPE 採用了 SMT 技術，SPE 有專用的 128 個 128 位寬暫
存器和 256KB 本地 SRAM 存儲器，支援整數和浮點 SIMD 運算，每個 SPE
通過專用的 DMA 通道與 PPE 相連，Cell 的異構設計和獨特的 SPE 設計值得
研究和參考。 
隨著集中度的提高，晶片上將集成更多的處理器核，另外隨著單核多執行緒
技術的應用，多核架構開發 TLP 的能力將更加強大，這可能推動並行編程模型
的普及。但是，並行編程模型由於其複雜性，遠沒有順序編程容易，而且大量應
用 都是基於順序編程模型的。因此，串列程式自動並行化成為一種的理想方法。
目前，對於具有可預測的控制流(如具有確定次數的迴圈)和可靜態確定的存取模
式(如順序讀取一個浮點多維陣列)的大量科學資料計算應用，可以利用編譯器實
現串列程式的自動並行化。但是對於一些非數值計算串列程式，由於其不規則的
控制流、不可預測的存取方式，或者難以靜態確定的記憶體資料相關性，編譯器
難以將其劃分為多個平行執行緒。對這類應用，如果能增加一些硬體支援，可能
有利於實現串列程式的並行化。 
