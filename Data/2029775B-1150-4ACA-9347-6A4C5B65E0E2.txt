ii
摘 要
本計畫為 3 年期研究計畫，針對 H.264/AVC 編碼核心技術及無線數位電視調
諧模器作研究。在 H.264/AVC 編碼核心技術，我們先發展一些高效率演算法，再
設計一些編解碼器重要之智產(IP) ，包括快速內在圖框預測(Intra prediction) 、內
容調適可變長度編解(CAVLC)、多重圖框移動估測法、可變方塊大小移動估測法、
子點高準度移動估測法(sub-pixel) 、位元率控制法、最佳化位元及失真模式選擇、
方塊效應補償法。並用 FPGA 硬體作 H.264系统整合，採用並列處理方式(parallel
processing)，並作水管時序控制(pipelined schedule)，可作即時編解碼展示。在無線
數位電視調諧模器中研究高效率多重相位濾波器(Polyphase filter) 、放大器及混波
器，並在 CIC 下線完成晶片測試。
本計畫目前已產出 6 篇期刊論文、8 篇會議論文。另一專利申請中。
iv
目錄
項 目 頁次
中文摘要 .......................................................................................................................... ii
英文摘要 ......................................................................................................................... iii
目錄 ................................................................................................................................. iv
研究內容 .......................................................................................................................... 5
一、 前言(Intriduction) ............................................................................................... 5
二、 研究目的............................................................................................................. 7
三、研究成果............................................................................................................... 8
結論 ……………………………………………………………………………109
參 考 文 獻……………………………………………………………………….110.
6Fig. A1 system architecture of H.264 encoder
Fig. A1 shows the system architecture of H.264 encoder. The coding includes intra mode
and inter mode. The intra mode must perform intra prediction to find the best mode to reduce
the spatial redundancy. The inter mode can perform high efficiency motion compensation
by using multi-frame motion estimation, variable block size motion estimation, sub-pixel
motion estimation.
8for intra-predictor. H.264/AVC standard specifies the coding mode with DC-prediction and
8-directional prediction for a 44 luminance block in the intra-prediction. The
intra-predicted block is computed with the boundary pixels of the neighboring blocks. The
best mode can be chosen to achieve better quality and lower bit-rate when the differential
error between the intra-predicted block and the original coded block is less. For a smooth
luminance block, 1616 block mode may be used to achieve better RD cost. As for 1616
block processing, H.264/AVC system had defined four coding modes that are DC, Vertical,
Horizontal and Plane modes. The predicted modes for the chrominance signal with 88
block are similar to those of 1616 luminance. Among these predicted modes, the 44 block
mode consumes more computation power than the others. Thus there were some fast
algorithms proposed to reduce the computational complexity [13-22]. Recently, H.264 High
Profile tool is developed to improve R-D performance by adding the 88 intra prediction
[23]. The prediction modes of 88 block extended are the same as those of 44 block.
However, ASIC chip is rarely realized to include the 88 intra prediction since the
implemented cost would become too high.
To implement a real-time intra-predictor, the fast algorithm for 44 block prediction is
presented to save about a half of computations compared to the full search. Based on the
algorithm, the parallel architecture is proposed with the pipelined schedule to realize a
real–time IP. The throughput rate can achieve 1.5 samples per cycle, and the speed is faster
than the previous works [24,25].
A2 FAST ALGORITHM FOR INTRA PREDICTION
In order to design a cost-effective intra-prediction IP, a fast intra-mode decision
algorithm is proposed to reduce the computations for 44 luminance block. There are 9
predicted-modes for a 44 block in H.264/AVC standard. The predicted direction of each
mode is illustrated in Fig. C A1. The vertical, horizontal, DC, diagonal down-left, diagonal
down-right, vertical-right, horizontal-down, vertical left and horizontal-up modes are
assigned with the number from 0 to 8, and corresponding to the symbols, EV, EH, DC, DDL,
DDR, VR, VL, HD, HU, respectively. The intra-prediction processing includes prediction
pixel generation, sum of differential (SAD) operation, and RD cost computation. In order to
reduce the computational complexity, a near pixel correlation integration (NPCI) approach is
proposed to early analyze the features of each mode [26]. Some of the modes can be early
rejected before the process enters to the H.264/AVC coding kernel. Because the predicted
pixels are high correlation along the predictive direction, the NPCI approach can merge the
high correlation pixels together to obtain a differential parameter. Fig. C A2 shows the
location of predicted pixels, P(0,0)~(3,3), and the boundary pixels. The computation for the
parameter of each mode is listed in Table A1. Our study [26] had presented the detail theory
and experimental results. With the simple computations, the nine parameters are used to
estimate for each coding mode. The parameter can stand for the feature of each block mode
10
Generally, the intra-prediction used the reconstructed pixel to predict the pixels.
Real-time intra-predictions all face the same problem that is a strong data dependency from
the YUV reconstructed reference pixels [24-25]. Referred to Fig. C A2, the predicted pixels
are from the up-side pixels (X, A, B,..) and left-side pixels (I, J, K, L) for the coding mode
searching. The up-side pixels are reconstructed from the previous row block coding results.
These reconstructed pixels had been stored at the frame memory. For the intra-prediction, the
last-line pixels of the previous row block can be restored to one line-buffer from the frame
memory, to reduce the frame memory accessing times. The up-side pixels can be read from the
line-buffer to the temporary register for the coding mode computing. The left-side pixels are
from the decoding results of the previous column block. The reconstructed pixels must be
processed from the intra-coder with transform, quantization, and to de-quantization, inverse
transform modules. For the current block intra-prediction, we must wait for the result of
previous block from intra-coder. If so, the pipelined schedule in Fig. C A3 for intra-prediction
and intra-coder cannot be achieved simultaneously. To break the problem of data dependency
for real-time operation, the reference pixel of left-side boundary can adopt the original pixel
for mode prediction, to save the waiting cycles for intra-coding. To prove this approach is
realizable, the coding performance is evaluated with the program JM while the reference
pixels are modified from the decoded pixels to the original ones. The results are shown in Fig.
C A5(a) and (b) for the “News”and “Carphone”sequences, respectively. Under our fast
algorithm, the pixel prediction selects the original (with reconstructed pixel) or modified (with
original sampling pixel) approach to compare the performance. Their results are very close in
the high bit-rate as using low QP. When QP is adjusted to high for low bit-rate coding, the
reconstructed pixels would appear large distortion due to quantization error. In fact, the
proposed fast algorithm is not an optimal search, and the selected mode is not the best one in
any case. As the original pixel instead of reconstructed pixels for the left-side processing, the
RD cost can be even improved a little for the intra-prediction in experiments. The approach
with original pixel instead of reconstructed pixel can be compatible to the H.264 decoder. For
a real coding system, the decoding loop in the encoder and decoder must operate on the same
state. The selected mode from intra-prediction would send to the intra-coder and -decoder. The
current coded pixel can be reconstructed with the previous reconstructed pixels and saved to
the frame memory. Thus the encoder and decoder will have the same data in the frame
memory.
Marco-Block (MB) is a basic processing unit for video coding. Figure 6 shows the
position of sixteen 44 blocks and its reference pixels for intra-prediction. The timing
schedule for data access is shown in Fig. C A7, for intra-coder. The intra-prediction of each
44 block spends 16 cycles. The up-side pixels are the reconstructed pixels of the previous
row block that can be loaded to the temporary register from the memory. The left-side pixels
employ the original sampling pixels since the reconstructed pixels are not available now. Then
12
negligible quality drop can be adopted in the chip design, in keep low-complexity circuit
design. The SAD computation uses 4 parallel subtractors to meet the real-time requirement.
The processing time for each block mode is 16/4=4 cycles. The computations of one
44-block require 16 cycles as 4 modes are selected. Hence, the processing time for one MB
needs 1616=256 cycles since there are sixteen 44-blocks. Simultaneously, the 1616 block
mode for luminance and 88 block mode for chrominance are computed too. H.264/AVC
defines the 4 predicted modes for 1616 luminance (Y) and 88 chrominance (UV). The
pixels of horizontal and vertical modes can directly copy from the boundary pixels. Since the
computational load is low, a common configurable core is used to share the operator to
compute the pixels of DC and plane modes for 1616 Y-block and 88 UV-block, to reduce
the hardware cost. For 1616 Y-block prediction, the computational time for one mode is
(256/4)=64 cycles as 4 subtractors used in parallel. The total is 644=256 cycles to compute
four modes of Y-block. For 88 UV-block, we spend 64/2=32 cycles for one SAD computing
as 2 subtractors used for U or V mode. Since U and V pixels contains four modes, this requires
3224=256 cycles too. In total, there are 4, 4 and 2 parallel subtractors employed to compute
SAD values for 44 Y-block, 1616 Y-block and 88 UV-block, respectively. The results of
SAD are sent to the mode decision modular in parallel. The mode can be decided with
rate-distortion criterion to find the best performance. However, for low-cost hardware design,
the final mode decision uses the simple cost function [19,21,22] as
=SAD+4P(Qp), A(1)
, in which P is either 1 or 0. If P=0, the cost function can be decided by only SAD value.
Otherwise, P=1, the function is dependent onwith the lambda from approximate exponential
approach of the quantization scale QP.
A. Architecture for 44 Block Prediction
Figure 9 shows the architecture of 44 prediction. The architecture includes two parts. One
is the pre-decision core using the proposed algorithm, and the other is the configurable
prediction to generate particular block pattern according to H.264/AVC standard. For regular
design, the computation for nine parameters is allocated to the hardware with the finite state
machine (FSM) according to Table A1. The FSM can select which pixels input to the
computational kernel for the parameter computing. For real-time purpose, we only spend one
or two clocks to compute one parameter. The nine parameters can be completely achieved
with 14 clocks. Finally, the comparator is used to find the 1st~4th minimum values from the
nine parameters with one clock time.
Fig. A10 illustrates the configurable architecture according to the proposed algorithm.
The boundary pixels and coded pixels are selected with multiplexes that are controlled by
FSM. The FSM flow is dependent on which one parameter computing. The circuit can read
four boundary pixels and four coded pixels from temporary registers per cycle. The
14
Vertical, and Plane modes. The implementation of Plane mode must occupy the significant
cycle count. For cost-effective design, a time-sharing method is adopted to generate the
predicted pixels for Y-Plane mode, UV-Plane mode, Y-DC mode and UV-DC mode with a
configurable architecture in the common circuit. Fig. C A13 shows the pipelined schedule for
computing the predicted pixel in these modes. The pixels can be directly copied from the
boundary data in Horizontal and Vertical modes without requiring computation. In the first 64
cycles, for Y and UV Horizontal mode computing, the boundary pixels can be directly read
from the temporary register for SAD computing in parallel. Meanwhile, the configurable
architecture can compute the predicted pixels of DC- and Plane-modes, as shown in Fig. C
A14. The FSM is used to control the configurable function for each mode computing with the
schedule in Fig. C A13. At the first 19 clocks, DC values of YUV blocks all can be computed
and the results are saved to the registers. Following, the pixels of U, V and Y-Plane mode are
successively calculated with 39, 39 and 143 cycles, respectively. The symbol Pa is the basic
parameters computing for H,V, a, b and c with Eq. (2) that are defined in H.264/AVC standard,
which must be calculated before computing the pixels of Plane mode. The total only uses 240
cycles to compute the pixels of two 88 chrominance-blocks and one 1616 luminance block
for DC- and Plane- modes prediction. The pixel of DC mode can be sent to SAD computing at
the 65th clock since the DC of YUV components value had been computed during 1~19
clocks. The predicted pixels of Plane mode all can be finished in the last 128 cycles, and its
SAD value can be computed in the last 64 cycles.
Figure 15 shows the configurable architecture that can compute the parameter Pa and the
predicted pixels for DC and Plane mode in the same circuit. The circuit can compute two
pixels and output two values per cycle. As for DC computing, the four boundary pixels are
selected with multiplexes. For 1616 DC mode, the boundary pixels P(-1,0)~(-1,15) and
P(0,-1)~(15,-1) can be read during 8 clocks. The 32 boundary pixels can be added to the
accumulator AR1. At the 9 ck, the constant 16 is added to the AR1 value and then it is
right-shifted by 4-bit to obtain the DC value from the output0. The DC pixel for 88 UV-block
can be computed with the same method, but allocated in the different time slot. Since the
number of boundary pixel is 16, the DC value of each U or V block is calculated with only 5
cycles. Thus the total cycle for all DC pixels computing is only 19 (=9+5+5) clocks.
As for Plane mode computing, first the parameters, H, V, a, b and c must be computed.
H.264/AVC had defined the parameters as
1])p[15,1,15](p[16a  ,
    

7
0x'
),1-,x'-6p-1,x'8p(*)1x'(H
16
Based on the parallel architecture, the high-speed circuit is realized with Verilog HDL
[29]. The prototyping processor for intra-prediction had been implemented with Xilinx
FPGA development software [30], as mapping to a device, XC3S200A. In order to make
sure the chip function, the test patterns with continuous blocks input to the circuit for testing.
First, C-programming is used to find the result of each computing unit according to our
algorithm. The same pattern is sent to the module of hardware to check whether the output
can meet our expectation. Then the entire intra-prediction chip consists of all sub-modules,
and the function can meet the result of C-Programming. The architecture mainly contains
four parts: fast algorithm core, Y 44 mode prediction, Y1616/UV88 mode prediction,
and cost evaluation and system control, which occupied about 17%, 27%, 51% and 6% of
full-chip area, respectively. The gate count of each module is listed in Table A4. The Verilog
coding file is sent to a workstation for cell-based design. The file is successfully synthesized
to the gate circuit with SYNOPSYS synthesis tools as TSMC 0.18um CMOS technology
employed. The total gate count is about 28k and the silicon area is about 2.8mm2. The
maximum power dissipation is about 9.6mW as the chip works at 105MHz.
Table A5 lists the comparisons with the recent architectures. The previous works [24,25]
used the sequential scheme for YUV processing, which requires more computational cycles.
In this study, a parallel architecture is proposed to process Y and UV signals at the same time
slot, to reduce the coding cycle. The intra-prediction time is only 256 cycles for one MB
coding. With the pipelined schedule in Fig. C A3, the processing time is only 256 cycles for
the full intra-coder to process 256(Y)+642(UV) pixels. The throughput rate can achieve 1.5
samples per cycle, and the number of MB processed is about 630k per second at the
frequency of 105MHz. The throughput rate of competing architectures is only about 0.3~0.4
sample/cycle since they process 256+642 pixels with about 1k cycles for one MB[24,25].
Although the proposed hardware overhead increases about 30%, the throughput rate can be
up about 5 times. Next, we define the hardware efficiency=throughput rate-2/gate-count. The
throughput rate-2 is defined with the number of MB processed per second. When VLSI
architecture can achieve higher throughput using less gate count, its hardware efficiency
becomes better. Results show that the hardware efficiency of the proposed architecture can
be promoted by about 3~6 times compared to the previous works.
In a typical H.264 stream, the intra coded frame may occur at an interval of 8~12
frames. However, for the P- and B-frames coding, the rate-distortion optimal (RDO) method
is used to find the best coding performance by checking all intra/inter-modes in JM [27]. The
intra-block may be used for inter-frames coding. Hence the real-time intra-prediction is
required for the H.264 coding kernel. In additional, a low-cost system, such as digital still
camera, can encode continuous pictures with I-frame only, to save the frame memory.
18
Table A1 The Computations of Predictive Parameters and its Processing Cycle
Parameters Clock-Requirement
EV=[ |(P(1,0)-B)+(P(1,3)-B)|+|(P(2,0)-C)+(P(2,3)-C)| ]>>1 1
EH=[ |(P(0,1)-J)+(P(3,1)-J)|+|(P(0,2)-K)+(P(3,2)-K)| ]>>1 1
DC=|(J-P(1,1))+(L-P(3,1))+(B-P(1,3))+(D-P(3,3))|>>1 1
DDL=|(D-P(3,0))+(E-P(1,2))+(E-P(2,1))+(F-P(0,3))|>>1 1
DDR=|(A-P(0,0))+(X-P(1,1))+(X-P(2,2))+(I-P(3,3))|>>1 1
VR= |(A-P(2,2)+(B-P(1,0))+(B-P(3,2))+(C-P(2,1))|>>2+
|(I-P(1,3))+(X-P(1,2))+(X-P(0,1))+(A-P(0,0))|>>2
2
HD= |(I-P(3,3))+(I-P(2,2))+(J-P(1,2))+(J-P(0,1))|>>2+
|(J-P(1,2))+(J-P(3,3))+(K-P(0,2))+(K-P(2,3))|>>2
2
VL= |(C-P(2,0))+(D-P(2,1))+(D-P(1,1))+(E-P(3,0))|>>2+
|(D-P(2,2))+(E-P(2,3))+(E-P(3,1))+(F-P(3,2))|>>2
2
HU= |(J-P(0,1))+(J-P(1,0))+(K-P(2,0))+(K-P(3,0))|>>2+
|(L-P(2,2))+(L-P(2,3))+(L-P(1,3))+(L-P(3,2))|>>2
2
20
Table A5. Comparisons with other Architectures
Huang etc. [24] Ku etc. [25] Proposed
Algorithm Partial Full
Search
Full Search
(Plane-mode
removal)
Fast Algorithm for 44 modes,
Full Search for other modes
YUV Processing Sequential Sequential Parallel
Interpolate Pixel
(per cycle)
4 4 6
#Subtractor for SAD 4 4 10
Pixel Parallelism 4 4 10
Processing Timing
for one MB(#cycle)
1280 1080 256
Throughput Rate-1
# Pixel/cycle
0.3 0.35 1.5
Throughput Rate-2
#MB/second
64.8K 163.6K 630K
Hardware Efficiency 15.1 18.2 52.6
Tech. Mapping 0.25um 0.18um 0.18um
Gate Count 19,871 19,246 28,513
SRAM Size(bit) 14k 16k 5k
Frequency 54MHz 117MHz 105MHz
Applications SDTV
(720480/30Hz,
4:2:0)
HDTV
(1280720/30Hz,
4:2:0)
HDTV
(19201080/50Hz 4:2:0)
22
Fig. A4 The interface of frame memory, register , and intra-coder.
Fig. A5(a)
Fig. A5(b)
Fig. A5(a) and (b) shows the references with the original reconstructed pixels and the
modified pixels with our approach for“News”and“Carphone”respectively.
Intra
Prediction
Intra
Encoding
Intra
Decoding
Dual
Port
Memory
Temporary Register (Reconstructed Pixels)
Reconstructed
Pixels
Original
Pixels
24
Fig. A9 The architecture of 44 block intra-prediction.
Pre-Decision
Core
Configurable
44 block
Prediction
Decoded Y- Pixel
Original
Y-Block
SAD
Coding
Mode
Decision
m=4
Mode
Fig. A8 The parallel architecture for intra-predictor.
Configurable
1616 block
Prediction
SAD
Configurable
88 block
Prediction
SAD
Decoded UV- Pixel
Original
UV-Block
Signal Controller
(FSM)
RAM
Y4x4
Boundary
Pixels
SAD Compare
:Previous
:Current
Fast
Algo.
Circuit
Mode
Controller
Prediction
Circuit
4
4
2
26
Fig. A11 The comparison circuit for mode selection.
Fig. A12 The pipelined schedule for 44 block prediction.
AND
R0
Di
Do
A
B
A<B
Ck-En
R1
Di Do
LoadLECk-En
A
B
A<B
AND
R3
Di Do
LoadLECk-En
A
B
A<B
OR
Co C1 C3
R4SAD
28
Fig. A15 The configurable architecture for 616 Y-block and 88 UV-block prediction.
MUX
Shift Left
Boundary pixels
MUX
0D0RD0aH
MUX
MUX
Boundary pixels
0cb
4H or
16H
MUX
a+b
V a RD1D1 0 MUX
Boundary pixels
MUX
4V or
16V b c 0 MUX
Boundary pixels
2’s decision 2’s decision
Shift Left
D0 RD0 D1 RD1
MUX
4 16
MUX
0 16 32
AR1 AR2
MUX MUX
MUX
0 16 32
Shift Right Shift Right
DeMUX DeMUX
a b cclip clipShift Left Shift LeftH V
4H or 16H 4V or 16V
Output 0 Output 1
Shift Number Control
Shift Number Control
input0 input1
30
B. CAVLC Technique
This study will be published on: “Forward computations for context adaptive variable
length coding design,”IEEE Trans. Circuits and Systems, part-II (accept 2010, April)
B1. INTRODUCTION
Recently, digital video systems, such as video-on-demand, video-phone, distance learning
and digital TV, have become popular products because of their convenience and high quality.
However, fast-paced innovation have put so much emphasis on advancing the video coding
technique, giving rise to H.264/AVC [1-2], which aims to improve the coding performance
of the previous standards [3-6]. The H.264 codec employs many advanced techniques
[31-33], such as intra-prediction, integer transform, context adaptive variable-length coding
(CAVLC), various block size, and multi-frame reference, among others. This study tackles
CAVLC technique.
For the end of video coding, the variable length coding (VLC) is used to remove
redundancy data and to transfer video data to the form of bit-stream. The conventional video
codec employs the fixed VLC table with the symbol (run, level) corresponding to the
codeword, because it is easy to be implemented in real systems. However, the video content
gets dramatically changed in real TV programs. The fixed VLC table is not suitable for all
sequences and compromises coding efficiency when the video feature is far from the
specified VLC table. The dynamic VLC table is therefore used to improve the coding
efficiency. However, the newly defined information must be transmitted to a decoder,
increasing the bit-rate. This makes the dynamic VLC table a rarely adopted system in real
applications.
The H.264/AVC codec employs the context-adaptive VLC method to promote the
efficiency of entropy coding. For flexible design, the CAVLC approach separates the
non-zero coefficients from its position in the coding procedure. The encoding parameters
contain five parts comprising the following: the number of total non-zero coefficients
(Total_coeffs); the number of the trailing one (Trialing_ones) and its sign (T1_sign); the
coefficient level; the total zeros (Total_zeros); and zero left (Zero_left) and run before
(Run_before) for each level. This approach can be applied to all video features while
keeping the same performance. However, the CAVLC encoder adopts the inverse scan to
find each parameter sequentially. Primarily, there is the need to compute the five parameters
in the specified order. Then, the VLC codeword is generated according to these parameters
with backward tracking. This coding scheme is against the high-speed implementation due to
the sequential order processing.
In this study, we present a parallel structure to implement CAVLC IP (intellectual
property) to meet the standards of high-speed HDTV applications. For real-time
32
since their levels are less than 3, except for the last level. However, the last level cannot
switch the tables.
When all levels are encoded, the codeword of total zeros corresponds to
Total_coeffs, which can be generated by using Table 5. In this example, the codeword is
“001”since the Total_coeffs = 6 and Total_zeros = 9. Next, the Run_before for each level is
encoded with backward tracking. The level (5) is 1, and the neighboring coefficient is
level(4)=-2. There are no zeros between level (5) and level (4), hence the Run_before=0 for
level (5), where Zero_left=Total_zeros. The codeword is “111”based Table 6. For the level
(4), we find two zeros between the level (4) and the level (3). The Run_before=2 is for the
level (4) and Zero_leftnext=Zero_left-Run_before=7. We can obtain the “101”codeword for
the level (4). With the same procedure, the codeword of Run_before for the next level can be
generated, and the encoding scheme is ended up to the level (0).
34
level tracking. The level module can sequentially generate the codeword of non-zero
coefficient to the output per cycle. One can process n non-zero coefficients with n cycles. At
the same time, the Run_before VLC codeword is collected following each non-zero
coefficient level. Each Run_before codeword is sequentially saved to the registers but
without output this time. When all the levels are finished, following the codeword of
Total_zeros can be sent to VLC packer with one cycle. Finally, the Run_before codeword of
all levels can be read in parallel order from the registers to the VLC packer in one cycle.
With this pipelined schedule, the maximum processing time is dependent on the number of
non-zero coefficient. The forward parameter computing requires 8 cycles since we read two
coefficients per cycle. This is the minimum processing time for our CAVLC core. Therefore,
the processing time for one block is between 8 and (NC+4) cycles.
Based on the timing schedule, we need to compute the CAVLC parameters within 8
cycles. When using the backward order, the parameters are calculated by tracking the last
coefficient to the first one. For this process, we require the temporal registers to save the
coefficients. The main drawbacks of backward computation are the longer processing time
and larger register size. In this study, we attempt a forward tracking approach to find the
parameters quickly and save time. To meet the timing requirement in Fig. C B2(a), we
propose a parallel architecture for computing parameters with forward approach, as shown in
Fig. C B3. The circuit can feed two coefficients from zig/zag scan, and uses the decision
core to quantize the input data, Coeff_1(11 bit) and Coeff_2(11 bit), to four one-bit states
(s1~4). Then all CAVLC parameters can be computed with decision states to reduce the
circuit complexity. The function of the decision core is given as









0s2else
1s2,0oeffCif
0s1else
1s1,0oeffCif
2_
1_
and









0s4else
1s4,1oeffCif
0s3else
1s3,1oeffCif
2_
1_
. B(1)
The states s1 and s2 can determine the input coefficients whether there is zero. The states s3
and s4 are used to check the input coefficients whether there is one. With the decision states
s1~s4, we only employ the binary counter circuit to compute the parameters Total_coeffs,
Total_zeros, Trailing_ones and its sign. To compute Run_before/Zero_left of each level, we
need the extra accumulator.
Computation for Total_Coeffs
We develop the decision function to find the Total_coeffs using a forward scheme as follows,
s1 s2 =





2__:
1__:
_:
numCoeffnumCoeff11
numCoeffnumCoeff01,10
numCoeffKeep00
B(2)
36
Computation for Zero_left and Run_before
The parameter Zero_left can be computed by the decision function as
s1 s2=































0_
__
___:11
1_
__
___:10
0_
1__
___:01
1_
1__
___:00
tempzeros
leftFstleftSnd
tempzerosleftSndleftFst
tempzeros
leftFstleftSnd
tempzerosleftSndleftFst
tempzeros
leftFstleftSnd
tempzerosleftSndleftFst
tempzeros
leftFstleftSnd
tempzerosleftSndleftFst
B(5)
where temporal registers Fst_left, Snd_left and zero_temp are used. And the Run_ before
value is decided by using two phases. The first is
tempzerosrunSndrunFst
else
runFst
ssstatepreviousif
___
0_
)01)2,1(_(



. (6)
And the second is
s1 s2=




















0_
0_:
1_
0_:
0_
1__:
1_
1__:
tempzeros
runSnd11
tempzeros
runSnd10
tempzeros
runFstrunSnd01
tempzeros
runFstrunSnd00
B(7)
The temporal registers Fst_run, Snd_run and zero_temp are used to compute the Run_before.
The zeros_temp register is used to record the status of zero-coefficient in the previous
processing time-slot. The forward computational schedule is shown in Fig. B7. The symbol
Fst (Snd) includes Fst_run (Snd_run) and Fst_left (Snd_left) to denote the value of
Run_before and Zero_left for the first (second) non-zero coefficient respectively. At the first
cycle, s1~s2=10, Fst_run and Fst_left are zeros. Since the Coeff_1 is a non-zero coefficient,
its Zero_left and Run_before correspond to the register Fst_run and Fst_left, so their values
38
that it employs too many input buffers. Chien [43] uses the SIPO (serial input/parallel output)
buffer to save one block data, and adopts the inverse scan scheme to compute CAVLC
parameters. The maximum processing cycle is limited at the scan stage or encoding stage.
For the computation of backward scanning [15,19], we first need to save the coefficient to
register, and then read the data from the last store to the first one. The disadvantages are that
memory must be used and its processing time is longer from the inverse computing. If the
length of data is N, the number of parallel unit is M, it needs N/M cycles to store the data
from the input with backward scanning. Also they must spend another N/M cycles to read
the data from memory for computing parameters. The total requires 2(N/M) cycles. The
proposed architecture adopts the direct forward computations for CAVLC parameters using
the simple decision function cores. The memory can be saved since the input data can be
immediately computed. Also, the inverse scanning cycle can be saved to shorten the
processing time. We only require N/M cycles to compute the parameters with forward
computation. The latency time of our circuit requires 9 cycles, which is 1/2 less than the
other competing ones. Moreover, a regular math rule is found to generate the Coeff_Token
codeword for all non-zero coefficients cases. This scheme can reduce the temporal registers
about 40~50% compared with full table design.
To evaluate the processing cycle in the H.264 system, QP=25 is used to test four
various sequences that are Foreman, Mobile, Stefan and Weather, where each sequence uses
50 frames with CIF format. The basic profile is used in H.264 setup. The number of non-zero
coefficients (NC) increases as the QP scale gets larger. Since our processing time is only
(NC+4) cycles, the processing cycle can be reduced when compared to [39,43]. The average
cycle is about 180~210 cycles per MB(marco-block). With the low motion sequence, such as
Weather, the processing cycle can be further reduced since its NC is small. However, we
need more cycles to encode the high variance sequence “Mobile”, because the maximum
cycle for one MB is about 210 on the average. The coding speed can achieve about 110MHz
to satisfy the HDTV systems and even the proposed chip is implemented using the low-cost
0.35um CMOS process. Within comparing the two architectures, we can improve the
processing cycle by keeping a low circuit complexity.
B5. CONCLUSIONS
This study presents a high-throughput CAVLC encoder IP for the H.264 system. With
its parallel architecture, the circuit can feed two coefficients from the zig/zag scanning
buffers per cycle. The pipelined schedule is split into two phases, where one is the parameter
computation and the other is the codeword generation. We proposed the efficient decision
functions, and then the parameters can be found at the end of the coefficient reading with the
forward computing approach. It has the following advantages: (1) it does not need the
inverse scanning cycle; (2) it can save the temporal registers; and (3) it can significantly
40
Table B3 VLC table for Coeff_token
Trailing_ones
( coeff_token )
Total_coeffs
( coeff_token )
0 <= nC < 2 2 <= nC < 4 4 <= nC < 8 8 <= nC nC = = -1
3 5 0000 100 0011 0 1010 0100 11 -
0 6 0000 0000 0111 1 0000 0011 1 0001 001 0101 00 -
1 6 0000 0000 110 0000 0110 0011 10 0101 01 -
2 6 0000 0001 01 0000 0101 0011 01 0101 10 -
3 6 0000 0100 0010 00 1001 0101 11 -
Table B4 VLC table for level
Lev-VLC0
Code no. Code Level
(1,2..)
Level’
(2,3..)
0 1 1 2
1 01 -1 -2
2 001 2 3
3 0001 -2 -3
… … … …
14 000000000000001xxxx 8 to15 9 to16
Lev-VLC1
Code no. Code Level
(1,2..)
Level’
(2,3..)
0 10 1 2
1 11 -1 -2
2 010 2 3
3 011 -2 -3
… … … …
14 000000010 8 9
Table 5 VLC table for total zero responding to Coeff_token
Total_zeros Total_coeffs( coeff_token )
1 2 3 4 5 6 7
7 0000 11 0011 011 0011 011 010 0001
8 0000 10 0010 0010 011 0010 0001 001
9 0000 011 0001 1 0001 1 0010 0000 1 001 0000 00
10 0000 010 0001 0 0001 0 0001 0 0001 0000 00
42
Table B7 The Gate Count for each Modular
Gate Count
Parameter Computations 4832
Coeff_token 1432
Level 1490
Total_zeros 760
Run_before 390
Packer(Barrel Shift) 6343
Total 15247
Table B8 Comparisons with Other Architectures
Chen [39 Chien[43] Proposed
CAVLC Parameter
Computation
Reverse Reverse Forward
Inverse Scan Cycle Need Need None
Input Buffer Two Block One Block (SIPO) None
Latency Cycles 17~32 17~32 9
Symbol/cycle 1 2 2
Math Expression for
Codeword
Level Level,
Coeff_Token(NC>8)
Level,
Coeff_Token(all),
Cycles Per MB* 400~450 290~330 180~210 (NC+4)
Process 0.18 um 0.18 um 0.35um
Speed 100MHz 125MHz 110MHz
Gate Count 23k 15k(included packer) 15k(included packer)
*QP=25
Fig. B1 Architecture of CAVLC encoder
Fig. B1 The architecture of CAVLC encoder.
44
Fig. B3 The parallel architecture for parameters computing.
Decision Function
s1 s3 s2 s4
Total_Coeffs
Counter
Total_Zeros
Counter
Trailing_Ones
Counter
Zero lefts/
Run_before
Core
s1 s2 s1 s2 s1 s2 s1 s3 s2 s4
5 4 4 4 2
11 11
Coeff_1 Coeff_2
Zigzag Scan Register
Total_zeros Zero_lefts & Run_before Trailing_onesTotal_coeffs
46
Fig. B7 The forward tracking for computing Zero_left and Run_before
Fig. B8 The simulation results for the proposed CAVLC encoder.
Output =>
Input =>
L1 =>
BS1 =>
BS0 =>
Total_coeffs =>
Trailing_ones =>
Total_zeros =>
48
C1. Search Algorithm
The fast algorithm with a 9-point block (9PB) concept is proposed to reduce the computational
complexity and to keep data reusable. The 9PB search adopted the center biased motion estimation
[16,17]. The processing flowchart is illustrated in Fig. C1. The 9PB method checks the vectors with
(h,v), (h,v-1), (h+1,v-1), (h+1,v), (h+1,v+1), (h,v+1), (h-1,v+1), (h-1,v) and (h-1,v-1), named as the
symbol 0~8 respectively, as shown in Fig. C 2(a). First, the zero center of 9PB is estimated to
compute the sum of absolute differential (SAD) value of 9 vectors. Then the minimum SAD cost is
found from 9 vectors. When the minimum SAD cost is located at the zero vector, we further check its
SAD value. If the value is less than one threshold, the current block with zero motion can be
confirmed. The search is terminated, and the result is zero vector for the output. Otherwise, the 2nd
minimum SAD cost is checked again. Its location may be at the corner or side. The corner and the
side point in 9PB is at the even number (2,4,6,8) and odd number (1,3,5,7), respectively, as shown in
Fig. C 2(a). If the location is at the side, one 9PB search is continuously used to compute the SAD of
the next nine vectors. The next 9PB block may be moved to four sides with top, bottom, left and right
with named 5,1,7,3 respectively. Since the current and next 9PB blocks are neighborhood, most of
reference video data can be reused to reduce memory bandwidth.
50
6 5 4
7 0 3
8 1 2
Fig. C2(a) The next search with one 9PB at the previous result at the side point.
6 5 4
7 0 3
8 1 2
Fig. C2(b) The next search with three 9PB at the previous result at the corner point.
For example, Fig. C 2(a) illustrates the best vector at“5”from the first 9PB step, and the next one
9PB search is moved to the top side to check the neighboring vectors. On the other hand, as the best
vector located at the corner point, three 9PB blocks are employed to find the next vector. For instance,
Fig. C 2(b) shows the corner at the top-right side, which the best vector is at “4”from the first 9PB
searching results. The search is continuously done until the best vector is not at the side or corner.
The search would be terminated while the best result is located at the center of 9PB block, or the
minimum SAD of the 9PB estimation is larger than that of the previous 9PB. The maximum
52
Based on this concept, one can compute SAD of a small block, and then to expand to a larger
searching block mode. H.264/AVC contains 1616, 168, 816, 88, 84, 48, 44 modes
for motion estimation. The primitive sub-block can use 44 as a basic unit to grow up to
various block size. Fig. C 3 illustrates the 1616 block is split to sixteen 44-blocks. First
SAD value of each 44-block is computed. A large block mode can be estimated by merging
the results of the 44-blocks. For the first 48 block mode, its SAD can be computed by the
addition of the SAD of Block44-0 and Block44-4 as
444044084   BlockBlockBlock , C(3)
where the symbol Block44-k denotes the SAD value of the kth 44-block in Fig. C 3. With
similar concept, the SAD of 84, 88, 816, 168, 1616 modes can be computed by
144044048   BlockBlockBlock , C（4）
544444144044088   BlockBlockBlockBlockBlock , C（5）
i
i
BlockBlock 

  44
7
0
0816 , C（6）








 
13
12
44
9
8
44
5
4
44
1
0
440168
i
i
i
i
i
i
i
i BlockBlockBlockBlockBlock , C(7)
i
i
BlockBlock 

  44
15
0
01616 . C(8)
All coding modes can be achieved by the combination with SAD values of primitive 44
blocks, to reduce the computational cost. When the various vectors of 44 blocks are
computed, its SAD value is used to compute RD cost. And the other block size can be
merged by 4x4 block, and also sent its SAD value is used to compute RD cost. Each block
coding mode with 44, to 1616 would be estimated in RD optimal kernel. Each block may
own the best vector since its RD cost is independently computed. Finally, the best block
mode can be selected as corresponding to a minimum RD cost.
54
Table C1. The Simulation Results with JM 13.2.
Sequence ΔPSNR ΔME time(%) ΔBit-Rate(%)
FME 0 -93.84 +0.33
Akiyo
Proposed 0 -94.85 +0.26
FME -0.07 -87.69 -0.53
Carphone
Proposed -0.06 -88.95 +0.21
FME -0.01 -93.43 -0.23
Claire
Proposed -0.02 -94.85 -0.33
FME -0.01 -78.92 -0.62
Coastguard
Proposed -0.02 -91.63 +0.32
FME -0.02 -92.35 -0.37
Container
Proposed -0.01 -94.15 0.08
FME -0.01 -83.42 -0.08
Mobile
Proposed -0.01 -91.92 +0.18
FME -0.03 -91.31 -0.48
News
Proposed -0.02 -93.92 +0.35
56
C4. VLSI Architecture
To realize a regular VLSI architecture for VBSME, the design employs the base of the
primitive 44 block. The input pixels are from the temporal buffers with 32 bits, as shown in
Fig. C 4. Thus this system can read 4 pixels with row-by-row per cycle. The partial SAD
value is accumulated with an accumulator (ACC). After 4 cycles, one can achieve a complete
SAD for one motion vector of 44 block. Fig. C 5 illustrates the computational kernel of PE
array with four PEs. The current coding pixels X00, X01.. and the reference pixels Y00, Y01
are read in parallel. With 4 subtractions and 3 additions, one can obtain the partial SAD from
one row per cycle. There are four partial SAD added to attain the entire SAD for one motion
vector of 44 block.
Fig. C5 The computational kernel of PE processing array.
The LPE (large PE) consists of three PE-arrays, as shown in Fig. C 6. The current
coding pixels X00, X01.. are read to PE array 1 computing. The pixels delay one and two
clocks with D-latch for PE array 2 and 3 processing, respectively. The reference pixels Y00,
Y01.. are read from the frame memory. To reduce the data bandwidth of the frame memory,
the temporal register array is used between the frame memory and the computational unit.
For parallel processing, three pixels are read per cycle. The frame memory used 24-bit as for
each pixel with 8-bit. The output data are sent to PE array 1~3 in parallel with three ports.
When the PE array 1 computes the vector (V, H), simultaneously the PE array 2 and 3
processes the vector (V, H+1) and vector (V, H+2), respectively. The symbol DSR is the
D-latch with right-shifted by 8 bits. If the PE array 1 deals with the Y00~Y03 pixels, the PE
array 2 will process Y01~Y04. Current DSR1 can keep the Y01~Y03 pixels that is reused
for the PE array 2. However, the Y04 pixel (MSB) must be read from the temporal register
array. With the similar way, the PE array 3 can calculate the Y02~Y05 from the DSR2 and
register array.
Y00 X01 Y01 X02 Y02 X03 Y04
ACC
X00
58
The timing schedule for LPE processing is shown in Fig. C 7. According to our 9PB
algorithm, nine vectors are estimated in each step. For the block 44-0 motion estimation in
Fig. C 3, the reference pixels, Y00~Y05 ..Y50~Y55, are reading for nine vectors computing.
The position of reference pixel is shown in Fig. C 8. Three pixels are read from the frame
memory per cycle, with left-to-right and top-to-bottom in order. At the first five cycles, there
are 3*5=15 pixels stored to the register array. Then PE array 1 can process the first row data
for X00~X03 and Y00~Y03 for the motion vector MV(0,0) at the 6th cycle. In the next cycle,
PE array 1 can compute the second row data and accumulate its result to accumulator. At the
same time, PE array 2 can process the first row data for X00~X03 and Y01~Y04 for
MV(0,1). The PE array 1, 2 and 3 simultaneously calculate the 3rd, 2nd and 1st row data for
the MV(0,0), (0,1), (02) respectively, at the 8th clock. The SAD of MV(0,0) can be achieved
from PE array 1 at the 9th clock. Continuously, SAD value of MV(0,1) and (0,2) is
completely computed from PE array 2 and 3 at the 10th and 11th clock, respectively.
Fig. C8 The pixel symbol of the reference memory.
Y00 Y01 Y02 Y03 Y04 Y05 Y06 Y07..
Y10 Y11 Y12 Y13 Y14 Y15 Y16 Y17..
…
Y40 Y41 Y42 Y43 Y44 Y45 Y46 Y47..
Y50 Y51 Y52 Y53 Y54 Y55 Y56 Y57..
……
60
As for the next column processing, the current coding pixels X00, X01 are read again,
and the reference pixel Y10, Y11 are read from the register array. The SAD values of
MV(1,0), (1,1) and (1,2) is processed by PE array 1, 2 and 3 respectively, which the timing
schedule likes those of MV(0,0), (0,1), (02) above mentioned. Hence the SAD of MV(1,0),
(1,1) and (1,2) can be attained at the 13th , 14th and 15th clock, respectively. Repeating this
procedure, the SAD of MV(2,0), (2,1) and (2,2) can be obtained at the 17th , 18th and 19th
clock, respectively. Based on the pipelined schedule, it spends only 12 cycles to achieve
SAD of 9 vectors for one 44 block.
In order to improve the throughput rate, eight LPE modules are employed in parallel.
The system diagram is shown in Fig. C 9. Each LPE module can read the coding pixels from
the input block memory with 32-bit, and the reference pixels from register array with 48-bit.
The size of register array is 1818 pixels since the 9PB algorithm estimates the motion
vector with 1 and the maximum block size is 1616. The output of register array requires
48×8 bytes for eight LPE processing in parallel. Eight LPEs can use the common pixels from
the register array to reduce the connection of wiring. The macro-block in Fig. C3 is split to
the top block (block 44-0~7) and bottom block (block 44-8~15) that are processed during
the first 12 cycles and the last 12 cycles, respectively. LPE 0 can process block 44-0 and
block 44-8, and LPE 1 can process block 44-1 and block 44-9. The LPE processing the
position of block is listed in Table 2. Each LPE processes two 44 blocks with 24 cycles.
Eight parallel LPE can handle sixteen 44 blocks for one MB. Hence the searching time is
only 24 cycles for one 9PB core to estimate nine vectors. The processing results for 44
block are merged to large block size according to C(3)~(8) with the pipelined stage. The
architecture will be described later. Finally, the minimum SAD value for each block size
mode can be found from the compared circuit with one clock.
62
Comp Reg
Comp Reg
Comp Reg
Comp Reg
SAD4x4_0
SAD4x4_1
SAD4x4_2
SAD4x4_3
Min of SAD4x4_0
Min of SAD4x4_1
Min of SAD4x4_2
Min of SAD4x4_3
Fig. C10 The minimum SAD searching for 44 block.
The SAD of various block size can be computed with the results of 44 blocks. Fig. C
10 illustrates the minimum SAD found by checking nine vectors, for the first four 44
blocks. Hence, the best sixteen SAD values of sixteen 44 blocks can be found for one MB.
Each 44 block owns the best vector corresponding to the minimum SAD value. When
merging 44 blocks with (3) and (4), the SAD value of 48 blocks and 84 blocks can be
achieved. Also, the minimum eight SAD values can be achieved from 48 blocks and 84
blocks, respectively, as shown in Fig. C 11. With the similar way, the SAD of 88 block can
consist of the SAD of block 84-0(=block 44-0 + block 44-1) and block 84-2(=block
44-4 + block 44-5) with (5), to reduce two adders.
Comp Reg
Comp Reg
Comp Reg
Comp Reg
SAD4x4_0
SAD4x4_1
SAD4x4_4
SAD4x4_5
Min of SAD8x4_0
Min of SAD4x8_0
Min of SAD8x4_2
Min of SAD4x8_1
Adder
Adder
Adder
Adder
SAD8x4_0
SAD8x4_2
SAD4x8_0
SAD4x8_1
Adder Comp Reg
Min of SAD8x8_0SAD8x8_0
Fig. C11 SAD computing and the minimum value searching for block 84, 48 and 88.
When four SAD values of 88 block are achieved, SAD values of the 168, 816 and
1616 blocks can be computed with result of 88 block, to reduce the number of adders.
Figure 12 shows that the SAD values of 168, 816 blocks can be combined with the SAD
of two 88 blocks as referring to (6) and (7). Finally, the SAD values of 1616 block can be
64
Fig. C13 Data reused with the overlapping block search.
Fig. C14 The maximum search range in horizontal direction.
Since the moving prediction may along the right or left horizontal direction, the
maximum searching distance is from –3m-1 to 3m+1 as including one center 9PB. The
maximum search range can achieve 3(2m+1) for the horizontal direction. With similar
concept, the prediction for other direction also can be done. However, when video sequences
have higher spatial correlation, its motion displacement would become larger. For example,
in the same video content, if the vector of CIF block is 8, the vector of HDTV block may
become 32 since HDTV spatial resolution is about four times as CIF. The proposed
18*18
18*18
18*18
3
Search Range
3*18
+3
3m
mth
Center
3m
66
There have been some published papers about the VBSME architectures recently
[47-50]. For comparisons, four papers are selected, and the results are shown in Table C3.
Yap and McCanny [47] presented 1-D based motion estimation with 16 PEs, and its design is
limited on low-resolution system, such as CIF format. For HDTV coding system, most of
VBSME cores [48-50] need 256 PEs for high-speed requirement. The chip throughput rate is
dependent on the operation frequency, searching range, and hardware architecture. However,
the ratio of throughput and gate count is close to 1 since the full search is used in these
competing methods. The chip performance can not be promoted. The proposed chip
employed only 96 PEs. Clearly the circuit complexity can be reduced. The searching range
of motion estimation for HDTV should be enlarged to cover wider motion displacement.
However, the computational complexity and memory bandwidth would become large as
searching range is expanded. In the proposed, the maximum search range can achieve
3×(2m+1), to cover 57×57 vectors as m=9. The searching range is wider than the competing
architectures, while keeping lower complexity than [48-49] for HDTV coding.
For HDTV applications, the processing cycle for one MB must be limited at a
specification to meet real-time requirement. There are many HDTV specifications. For
example, the lowest format is 1280×720, 60fps, YUV=4:1:1. The MB throughput rate
required (1280/16)×(720/16)×60×2=421.9k per second. The highest format is 1920×1080,
60fps, YUV=4:4:4, its throughput rate is near to 2M MB per second. The chip throughput
rate can be evaluated by (clock frequency/cycle per MB). The proposed chip process one
MB with only 24~240 cycles, and the throughput rate can achieve 416.6k to 4166k MB per
second, when the clock rate is 100MHz. On average, the throughput rate can achieve about
2M which can meet all HDTV specifications. However, the other architectures [48-50] must
pay higher hardware cost to meet the throughput rate for HDTV requirement because of
using low efficiency full-search algorithm. Now we turn to evaluate the hardware efficiency.
Generally speaking, high hardware efficiency is achieved with low circuit complexity and
high throughput rate. Hence hardware efficiency can be defined the ratio of throughput rate
and gate count. Clearly, the proposed chip has higher hardware efficiency than competing
ones.
C6. CONCLUSIONS
This paper presents a fast motion estimation algorithm with hardware-oriented design for
regular VLSI implementation. The fast motion estimation algorithm can reduce about 90%
processing time on average while the quality degraded slightly. With data reused design,
most of reference pixels are overlapped for continuous vector estimating to reduce the
memory accessing times. Based on the proposed 9PB algorithm, the chip is successfully
designed with regular architecture and pipelined scheduling control. It spends only 24 to 240
68
D. Multi-frame Motion Estimation
This study had be published on :
[1] “Fast multi-frame motion estimation for H.264/AVC system”, Journal of Signal, Image
and Video Processing, Vol. 4, No. 2, pp. 167-175, June, 2010.
[2] “Fast Multi-Frame Motion Estimation for H.264/AVC System”, IADIS International
Conference Computer Graphics and Visualization 2008, pp. 97-102.
The number of reference frames can be up to 16 in H.264 coder. Obviously the
complexity greatly increases compared to the previous standards, such as MPEG-2 and
H.263 [3-4]. Recently, many fast algorithms have been proposed to accelerate the speed of
motion estimation for multiple references [53-55]. Chen. et al [53] presented the composed
vector from the result of the previous frames with simple additions. Then the fine search is
used to improve the accuracy. Kim et al [54] used a temporary motion vector to predict the
optimal vector between successive frames with vector map. Then narrow search is used to
refine its accuracy. Su et al [55] took into account the correlation/continuity of motion
vectors among different reference frames to find the estimated vector. These algorithms
employ the previous vectors to calculate the next vector. However, these kinds of methods
are highly dependent on the previous results. The result of error estimation will be
propagated to the next stage. The accuracy may be fine if the correlation of inter-frames is
high. However, the quality is poor when the inter-frames lack continuity.
In this study, we do not take into account the computation of the pervious vectors to
estimate the current vector in order to avoid the error propagation. Alternatively, the adaptive
motion search is applied with the combination of adaptive full search [56], diamond search
[57-58] and three-step search [59]. The processing speed can be improved 6~15 times
compared to the full-search. The performance is better than the competing algorithm [5] due
to lower complexity and higher quality.
D2. Proposed Fast Algorithm
The H.264 coder can refer five frames for backward and forward motion estimation. In
this study, we only discuss with the backward prediction since the forward prediction is
similar to the backward one. Fig. D1 shows the backward prediction with five reference
frames. The adaptive search that refers to the previous matching parameter is proposed to
reduce the complexity of motion estimation. First, we compute SAD (sum of absolute
difference) for five frames at the zero-vector only (horizontal and vertical vectors are all
zeros), which can be given by
5to1njpiqfjpiqfSAD
N
i
N
j
ttnt  




 ,|),(),(|
1
0
1
0
1
)0,0( D(1)
where ft and ft-n are the current and the previous nth frame respectively, (q,p) is the position of
70
TH1=6 and TH2=5 can achieve the best quality in experiments. If the SAD of the
zero-vector of the nth reference is larger than X value, one can stop to search the motion
vector of the relative frame since poor motion estimation for block matching may be
expected. Finally, when only one frame is available, the frame is estimated with full search
to find the optimal vector. Otherwise, the non-processed frames are detected with
Y-parameter. If the SAD value of zero-vector is larger than Y-parameter, this implies that
there are large block differences between the reference frame and the coding frame. We need
wide-range search in order to cover the fast motion displacement. The three-step searcher is
employed to compute the SAD of 9 points to find each the possible motion direction in
global estimation. Then the best vector is found with the diamond search starting from the
result of three-step. Since some points have been computed with three-step search, its SAD
is saved and re-used for the diamond search to avoid re-computing. On the other hand, when
its SAD of zero-vector is less than Y-parameter, we directly use the diamond search to find
the best vector for this frame. The minimum SAD value is updated with the current SAD of
the best vector, and so X and Y parameters are re-computed after each frame processing. The
X and Y value gets possibly smaller and smaller when computing more reference frames.
Also, when SAD of remainder is over than the current X value, we stop to search this frame
immediately. Hence there is a high probability to reject the far frames, such as t4 and t5
frames, for motion estimation. Otherwise, we record the SAD of the nth frame if its SAD is
less than the current X value. When all frames are processed, the final vector is determined
from the minimum SAD by checking the recorded SAD results of t1~t5 frames.
Now we evaluate the computational complexity for the fast multi-frame motion
estimation. In the best case, the adaptive full search (AFS) is used to find the motion vector
for the first frame [56]. The motion estimation of 2~4th frames may be skipped in
low-motion and low variance blocks. The minimum searching point is only 25, as searching
2 vectors for the first frame. Now we consider the worse case. The AFS search requires
about 64 points for the first frame from the average of high-motion sequences. The following
2~4th frames all are coded with the diamond search (DS). The searching number of DS is 40
in maximum [57]. The total is 160 searching points for four-reference frames. Hence the
proposed algorithm only needs searching 25 to 224 points for five frames motion estimation.
The original full search requires to check 5(N+1)2 points as the searching range is N for
five references. Clearly our fast algorithm can efficiently reduce the computational
complexity.
D3. Simulations and Comparisons
For simulations, some typical video sequences with CIF format are used as our
benchmarks. According to our algorithm, the C programming is employed to implement the
motion estimation with five reference frames. For comparisons, the full search and the fast
72
the coding bit-rate from the proposed algorithm is very close to that of full search. However,
for very high motion sequences, such as “Carphone” and “Mobile”, the bit-rate increases
about 5~6%. However, in any sequence, our bit-rate increment is negative when compared to
the paper [53]. In other words, the accuracy of motion estimation from our algorithm is
higher than [53]. The proposed high-accuracy motion estimation can also achieve less
distortion-rate (RD) cost in JM coding. Fig. 4(a) and (b) shows the RD curve with various
coding-bits vs PSNR. Results demonstrate that our RD performance is better than the Chen.
et al [53], and is closer to full search.
Finally, the processing time for each algorithm is evaluated with various sequences that
each one includes 100 frames. Two kinds of CPU time are used. One is the total coding time
of H.264 system and the other is only for the part of motion estimation. The speed evaluation
for the full H.264 system is defined with two parameters, which can be given by
TT_speed up-1 ＝
ProposedoftimecodingTotal
5-FSoftimecodingTotal
, D（8）
TT_speed up-2 ＝
ProposedoftimecodingTotal
Paper[53]oftimecodingTotal
. D（9）
The former shows the factor of speed improvement compared to our fast algorithm and the
full search in full H.264 system. The latter is to compare the speed between our fast
algorithm and [53]. The results are shown in Table D8(a). Clearly, the speed of the proposed
algorithm can improve about 2 times and 1.2 times compared to the full search and the paper
[53] in entire H.264 coding system, respectively. The second comparison is to evaluate the
motion estimation only. Again, we compare our method to the full search and the fast
algorithm [53], which is given by
MET_speed up-1 ＝
ProposedoftimecodingEstimationMotion
5-FSoftimecodingEstimationMotion
, D（10）
MET_speed up-2 ＝
ProposedoftimecodingEstimationMotion
[53]PaperoftimecodingEstimationMotion
. D（11）
The results are listed in Table D8(b), for various sequences. The speed can improve about
5~11 times and 2~3 times compared to the full search and [53] respectively. Clearly, the
proposed algorithm can save the computational cost while keeping high image quality.
D4. Conclusions
This paper presents a fast motion estimation algorithm for multiple references. The
algorithm adaptively combines the well-known search methods to improve the coding
efficient for H.264 system. For low-motion sequences, the processing complexity can be
greatly reduced when the vector accuracy is similar to the full search. For high-motion
sequences, the algorithm can adaptively expand the searching window size to cover the
74
Table D3 The CPU Processing Time with Various Algorithm
Coding speed (second/frame)
FS_front5 FDVS+H_V [53] Proposed
Sequence Avg. Time Avg. Time Speed-up Avg. Time Speed-up
Flower 8.2184 1.9311 4.2558 1.3714 5.9927
Football 8.264 1.9359 4.2688 1.1055 7.4754
Irene_g 7.964 1.9371 4.1113 0.5545 14.3625
TV 8.25 1.9317 4.2708 0.4961 16.6297
Claire 7.9959 1.9324 4.1378 0.4362 18.3308
Susiecif 8.2455 1.9344 4.2626 0.6645 12.4086
Mad900 8.2509 1.9336 4.2671 0.5602 14.7285
Table D4 The Reference Frame used in Each Algorithm
The percentage of 5 Reference frames used
Sequence Algorithm Reference_1 Reference_2 Reference_3 Reference_4 Reference_5
FS_5 57.09% 14.63% 16.24% 7.31% 4.73%
FDVS+H_V [53] 87.09% 6.47% 3.50% 1.72% 1.22%
Flower Proposed 72.28% 11.00% 9.42% 4.51% 2.78%
FS_5 69.65% 10.08% 7.38% 7.47% 5.42%
FDVS+H_V [53] 83.32% 7.15% 4.63% 2.99% 1.91%
Football Proposed 72.52% 8.41% 6.84% 7.16% 5.07%
FS_5 42.67% 9.27% 11.48% 28.29% 8.29%
FDVS+H_V [53] 62.59% 5.95% 6.98% 19.81% 4.67%
Irene_g Proposed 45.62% 8.14% 9.93% 29.02% 7.29%
FS_5 51.04% 17.81% 10.82% 11.84% 8.49%
FDVS+H_V [53] 60.38% 15.37% 8.63% 9.17% 6.45%
Tv Proposed 53.92% 15.78% 10.45% 11.63% 8.23%
FS_5 76.74% 8.48% 8.03% 3.74% 3.02%
FDVS+H_V [53] 82.38% 7.19% 6.06% 2.39% 1.98%
Claire Proposed 78.09% 7.82% 7.74% 3.48% 2.88%
FS_5 58.45% 16.70% 9.61% 7.97% 7.28%
FDVS+H_V [53] 83.22% 8.91% 3.31% 2.60% 1.96%
Susiecif Proposed 65.79% 13.08% 8.59% 6.98% 5.56%
FS_5 54.63% 17.66% 10.36% 10.15% 7.20%
FDVS+H_V [53] 80.03% 9.94% 3.35% 4.45% 2.23%
Mad900 Proposed 60.85% 14.47% 9.00% 9.05% 6.63%
76
Table D6 PSNR performance for each algorithm in H.264 System
FS-5 FS-1 Paper[53] Proposed PSNR-1 PSNR-2
Akiyo 38.3 38.22 38.23 38.25 -0.05 +0.02
Carphone 36.86 36.57 36.6 36.66 -0.2 +0.06
Coastguard 33.85 33.79 33.8 33.82 -0.03 +0.02
Foreman 35.37 35.16 35.19 35.24 -0.13 +0.05
Container 35.95 35.89 35.91 35.92 -0.03 +0.01
Mthr_dotr 36.09 36.04 36.05 36.07 -0.02 +0.02
Silent 35.61 35.56 35.57 35.59 -0.02 +0.02
Mobile 33.05 32.92 32.93 32.96 -0.09 +0.03
Table D7 The coding bit-rate from each algorithm in H.264 System
FS-5 FS-1 Paper[53] Proposed Bitrate-1 Bitrate-2
Akiyo 26.78 26.96 26.93 26.89 +0.41 -0.15
Carphone 102.79 114.94 112.92 109.92 +6.94 -2.66
Coastguard 239.63 244.11 243.57 242.92 +1.37 -0.27
Foreman 120.57 128.84 127.77 126.4 +4.84 -1.07
Container 40.12 42.57 41.89 41.57 +3.61 -0.76
Mthr_dotr 87.02 89.36 88.91 88.22 +1.38 -0.78
Silent 91.36 92.4 92.23 91.96 +0.66 -0.29
Mobile 452.2 485.11 482.13 475.52 +5.16 -1.37
78
Fig. D1 The backward motion estimation with zero-vector computing for 5 reference frames.
t5
t4
t3
t2
t1
CURR
Reference t1~t5
Mv(0,0)
Mv(0,0)
Mv(0,0)
Mv(0,0)
Mv(0,0)
80
akiyo
37
37.5
38
38.5
39
39.5
1 12 23 34 45 56 67 78 89 100 Frame
PS
N
R FS-5
paper
proposed
Fig. D3 (a)
carphone
35
35.5
36
36.5
37
37.5
38
1 12 23 34 45 56 67 78 89 100
Frame
PS
N
R FS-5
paper
proposed
Fig. D3 (b)
Fig. D3 PSNR graphs of (a) Akiyo, and (b) Carphone sequences.
82
E. Rate Control
This study had been published on
[1] " Adaptive video coding control for real-time H.264/AVC encoder ", Journal of Visual
Communication and Image Representation, Vol. 20, pp. 463-477, Oct. 2009.
[2] Quantization Parameter Decision of Initial and Scene Change Frame in Real-Time
H.264/AVC”, IEEE Fifth International Conference on Intelligent Information Hiding and
Multimedia Signal Processing, 2009, pp. 218-221.
[3] "Adaptive Video Coding Control for H.264/AVC", IEEE International Conference on
Communications, Circuits and Systems 2008 (ICCCAS'08), Pages:834-837, 25-27 May,
2008.
The rate control for H.264/AVC has been proposed in recent years [60-66]. The control
schemes can be classified into two major categories, one is the optimal rate control [62-64]
and the other is the real-time rate control [65-66]. In the former, quadratic rate-distortion (Q2
R-D) model [62-63] and ρ-domain source model [64-65] are used for optimal rate control.
The latter is with the simple estimation method based on buffer level or encoding complexity,
which is similar to MPEG-2 TM5 [66] and [67] approaches. The initial QP can be
determined by the target bit rate and frame rate. In JVT-G012 [11], the QP of the first
I-frame of the first GOP, IQP1 is determined by the bits per pixel (bpp). To improve visual
quality [68], the initial quantization parameter can be computed dependent on image features.
One-pass rate control scheme is proposed with linear MAD prediction model to circumvent the
chicken and egg problem of conventional MPEG-4 R-D model [69]. The quantization step size is
determined with the allocation bits and its predicted encoding complexity in quadratic
formulation. The linear regression method is heavy computation that will increase clock latency
to update the parameters. It is unsuitable for real-time hardware implementation.
In this study, we propose the adaptive rate control for real-time H.264/AVC encoder. First,
a new initial QP determination method is based on Laplacian of Gaussian (LoG) operator to
estimate spatial correlation of I-frame. We can obtain temporal correlation with the difference
between inter-frames, and then utilize the spatial-temporal correlation to predict the bit allocation
and encoding complexity. In order to improve visual quality, we propose an adaptive video
coding control with real-time rate control by using adaptive GOP structure in H.264/AVC system.
In scene change case, we can be utilized the temporal correlation to detect the status of scene
change, and employ the LoG operator to obtain a suitable QP for a scene-change frame.
E2. Proposed Real-Time Coding Control Algorithm
The proposed algorithm for real-time rate control performed on MB layer is composed of
four techniques: 1) initial QP determination, 2) spatio-temporal correlation based rate control
algorithm, 3) adaptive GOP structure decision and 4) QP decision in scene-change frame.
84















,32.0
05.0
55.0
0002.052.7
55030,7.0
025.0
3.0
0022.00.11
3010,2
05.0
1.0
0015.02.20
10,6
025.0
0005.01.43
1
other
bpp
LH
.bpp.
bpp
LH
.bpp.
bpp
LH
.bpp
bpp
LH
QP
I
avg
I
avg
I
avg
I
avg
I E(3)
For the first frame of GOP and the scene frame, we can decide the initial QP from the
average of spatial edge information javgLH in various target rates.
Spatio-Temporal Based Rate Control
In order to save encoding time and achieve quality improvement, this paper presents a
real-time rate control scheme based on spatio-temporal correlation for low bit-rate applications.
The proposed algorithm is composed of three parts: frame-layer bit allocation, MB-layer bit
allocation and quantization parameter adjustment.
Frame-Layer Bit Allocation
In H.264/AVC rate control scheme, the target bit for the current frame is allocated
according to the encoding complexity of past frames and current buffer status. For high-motion
or scene-change frame, the target bits allocation becomes ineffectiveness with the encoding
complexity of the past frames. Therefore, an efficient method with based on spatio-temporal
correlation is proposed, which the current encoding complexity is predicted according to the
spatial correlation and temporal correlation for the current frame.
The spatial correlation is estimated with the global homogeneity of current frame. The
global homogeneity is accumulated by the local homogeneity of all MBs in the j-th frame as


MB
k
kj LHEdge
#
1
, E(4)
where MB# is the total number of MBs in picture. The temporal correlation of j-th frame is
computed with the difference between two adjacent frames by
pixel
m n
jj
j N
nmfnmf
Diff
 

 |),(),(| 1
. E(5)
The encoding complexity in the j-th frame can be computed by
jjj TpSpFC  )1(  , E(6)
where  is weighting factor, and its typical value is 0.75 in our experiments. The jSp and
86
where the 1iAvgDiff is the average difference in the previous GOP, and th1 and th2 are empirical
thresholds.
The budget bits of the current MB depends on the complexity of current MB, and the
budget bits of current MB is calculated at the beginning of MB level for each MB as
headMB
kp
pj
kj
r
bits
kj Bits
CMB
CMB
TMB 


#
,
,
, E(12)
where rT and #MB are the remaining budget bits and the total number of MB in the current
frame, and Bitshead is the header bits.
MB-Level Quantization Parameter Adjustment
H.264/AVC rate-control (RC) scheme, the Q2 R-D model and linear MAD model used the
linear regression method to update the model parameters. Nevertheless, the complexity of linear
regression method is too complex for real-time system. In additional, the H.264/AVC Q2 R-D
model cannot obtain accurate QP estimation in low bit-rate coding [67]. In order to realize the
RC algorithm for low bit-rate system, we employed the third R-D model method. The R-D
model has used a simple linear method for MPEG-2 coder with TM5 [66] as
C
Q
K
MADMB
s
actual
k
bits
k  E(13)
where the
bits
kMB and
actual
kMAD are the budget bits and the actual mean absolute
difference (MAD) in the current k-th MB, the sQ is the quantization step, K and C are two
constants. The actual MAD can be obtained after encoding the current MB. The MAD
prediction scheme is proposed in MB layer RC for real-time purpose.
The MAD prediction model based on the spatio-temporal correlation of the current MB is
proposed in this paper. The neighboring block of the current MB is illustrated in Fig.5, for
computing spatio-temporal correlation. In most of video sequences, the MAD of current MB
is approximately proportional to the spatial variance and the motion feature. Hence, we can
assume that if the neighboring blocks have high spatio-temporal correlation, the MAD value of
neighboring blocks also have high correlation, which can be expressed by
j
k
j
k
j
k
j
k
j
k
j
k
j
k
j
k
MAD
MAD
MAD
MAD
CMB
CMB
CMB
CMB 1
1
1
11
1
1
1 





  E(14)
where jkCMB is the encoding complexity in k-th MB in j-th frame based on spatio-temporal
88
correlation. For high-correlated video sequences, an enhanced P-frame (Pe) rather then I-frame is
used in order to reduce the temporal redundancy and save coding the bit rate for MPEG system
in [16]. However, the method is not available for H.264/AVC since its coding method is greatly
different between MPEG-2 and H.264 system. Recently, the method [14] proposed a
quantization decision rule for intra-frame with the entropy of the block histogram (EBH) method
when the sequence occurs rapid motion or scene change. However, the EBH method has heavy
computational complexity too.
This paper integrates the AGOP concept to H.264 RC algorithm with spatio-temporal
correlation based to improve visual quality. The GOP consists of two parts: the basic GOP
(BGOP) structure and the adaptive GOP (AGOP) structure. BGOP structure is based on
spatio-temporal correlation rate control above. AGOP/BGOP structure decision and quantization
parameter decision of Pe-frame are main function in AGOP module.
While inter-frames have high temporal correlation, the coding performance can be
improved when AGOP structure is employed. In order to detect the temporal correlation, we
present a criterion to computer the parameter of temporal correlation. The difference between the
accumulated MADs (DMAD) of two adjacent P-frames in each basic GOP structure is used to
compute the temporal correlation, which can be expressed as follows:



pN
j
jj MADMADDMAD
1
1)( , where 
 

N
i
M
j
jiji PSNM
MAD
1 1
,,
1
E(18)
where pN is the total number of P frames, jMAD denote the actual MAD of the j-th P-frame,
jiS , and jiP , are the original frame and the reconstructed frame from motion compensation,
respectively. When DMAD value is large, implying the changing difference becomes serious,
namely, the prediction error is high and the temporal correlation is low in the adjacent frames.
The following criterion is used to determine the next GOP structure with the BGOP or
AGOP structure. If the DMAD from the result for the last P-frame of BGOP structure is small,
which implies that the predicted error is low, then the AGOP structure is continuously employed
to increase the coding performance with the effective motion compensation. From the last
P-frame of the current GOP structure, the next GOP structure can be decided by


 
otherStructureBGOP
MADthdDMADifStructureAGOP
DecisionStructureNext LastPavg
,
, _ E(19)
where thd is empirical value and stands for the difference level threshold in AGOP structure
decision, and avg_LastPMAD denote the average MAD in the last P-frame of the current GOP.
When accumulated temporal difference DMAD in the current GOP is small, the high correlation
90
where 1iQP is the starting quantization parameter of the first frame of the previous GOP
coding and PQPSum (i-1) is the sum of QP for all P frames for previous GOP. The thresholds, th1
and th2 are adaptive computation from the SDMAD. In experiments, the th1 =4 and th2 =3 when
SDMAD > zero, otherwise the th1 =3 and th2 =2.
QP Decision in Scene-Change Frame
If the video sequence changes suddenly, the intra- frame coded reveals less distortion than
inter-coded ones. An appropriate QP calculation is necessary to achieve better quality in a
scene-change frame. In H.264/AVC rate control, the QP of scene-cut frame is not available with
the quantization parameter of the previous-frame. The method [17] can appropriately decide the
initial QP of intra-frames in during the inter-frame when scene change occurs, and consider the
past average encoding bits of inter and inter-frame to decide the appropriate QP. To improve
the coding quality, we can compute the temporal correlation and initial QP parameters for the
scene-change frame. The proposed method is composed of two parts: scene-change frame
detection and QP decision of scene-change frame.
D1. Scene-Change Frame Detection
In order to detect the scene-change frame, the spatio-temporal rate control (STP-RC) with
frame complexity is realized. H.264/AVC rate control [11] estimates the frame complexity with
the information of QP and actual encoded bits for past inter-frames. The frame complexity would
have fault information in the scene-change frame. Our STP-RC algorithm joints the temporal
correlation and the spatial correlation scheme with (6) to estimate the frame complexity. The
frame complexity ratio (FCR) between the frame complexity of two adjacent inter-frames to
detect scene-change frame as
thd
FC
FC
FCR
j
j 
1
E(22)
where thd is a threshold to detect scene-change parameter. If two adjacent inter-frames is similar
complexity, the FCR is close to one. Contrariwise, the FCR is sharp transition when occurring
scene change.
D2. QP Decision of Scene-Change Frame
The mode decision of H.264/AVC selects the intra-coded for MBs through rate-distortion
optimization (RDO) model in the scene change frame. However, its QP still used from the
previous frame. To improve the quality of scene-cut frame, we investigate the appropriate QP
value for the scene-change frame ( IjSCQP ). Initially, we can determine the appropriate QP using
the spatial correlation E(3). Further, the scene-change frame may occurs the BGOP structure or
92
The QP Decision of P-Frame:
The procedure of QP decision of P-frame can be subdivided to three sub-steps as
follows.
Step1) Frame-Level Complexity Estimation and Bits Allocation:
The frame complexity of current frame can be estimated from the homogeneity of
frame and the different between two adjacent P-frames using E(6). The target bits allocation
for each frame is calculated based on the frame complexity and target buffer level with E(9)
and E(10). Besides, the result of frame complexity is considered characteristic of current
frame to detect the occurrence of scene change, and the picture quantization scalar depends
on the variance of frame complexity.
Step2) MB-Level Complexity Estimation and Bits Allocation:
The complexity estimation of the current MB is calculated based on the local
spatio-temporal correlation and the characteristic of video sequence by using E(11). The
budget bits of current MB can be calculated using E(12). If the budget bit of current MB is
less than 0, QP of current MB should be greater than the QP of previous MB in order to
guarantee the encoding bits of frame to be close to the target bits. The QP can be modified
by
QPQPQP kjkj  1,, , E(26)
where QP may be 1 or 2 depending on the magnitude of QP.
Step3) QP Decision of current MB:
For the first MB in the current frame, QP is set to the average QP of previous
frame. Expect the budget bit of the first MB or the urrent MB is not enough, we can be
computed Qs value by using (13), and then convert it to the corresponding QP by using
E(17). To maintain the smoothness of visual quality, the QP is further adjusted as
  ,min,max 1,,1,, QPQPQPQPQPQP kjkjkjkj   E(27)
And the final QP is bounded in the range of 0 to 51.
In after encoding each P-frame, we must be calculated the DMAD by using E(18) to detect
the temporal-correlation of current GOP structure. The last P-frame of current GOP structure
is employed to decide the type of next GOP structure with E(19) and E(20).
The QP Decision of Pe-Frame: If current frame is in the first frame of AGOP structure,
Pe-frame is coded with lower bits than I-frame due to motion compensation. Compute the QP
of Pe-frame using (21), and the QP modified by estimated SDMAD to improve quality.
The QP Decision of Scene-Change Frame: If current frame is the scene change frame, we
can be computed the appropriate QP for scene-change frame by using E(3). Considering the
scene-change frame is location on the BGOP or AGOP structure, the appropriate QP is
94
the “Akiyo”, the 77-152 frames are from the “Carphone”, the 153-228frames are from the
“News”, the 229-304 frames are from the “Foreman”, the 305-380 frames are from the
“Salesman”, and finally the 381-450 frames from the“Claire”, where the frame size is 176×144.
For comparisons, we also tested this complex sequence using the JM 12.4 with test condition as
Table E1. Fig. E9 shows the PSNR curve comparison for proposed methods and JM12.4 at
47Kbps, where the result can be achieved the improvement of 1.74dB on the average compared
with the JM12.4 at the scene-cut frame. Specially, we have higher than the performance of
JM12.4 at the scene-cut frames 153th, 229th, and 305th. Fig. E10 shows the decode frames, where
the image can be improved a little.
E5. Conclusions
In this paper, we proposed a few new algorithms to improve video quality in low bit-rate for
H.264 /AVC rate control schemes. The initial QP decision scheme not only depends on the
target bit-rate, but also considers video content itself to obtain the appropriate QP. The
proposed STP-RC scheme is based on the spatio-temporal correlation to estimate the coding
bit-rate of current frame. The QP is further adjusted for the current macro-block to improve the
local image. The experimental result can save about 67% coding time compared to the original
JM rate control. Without backtracking parameter computing for each model, our algorithm is
suitable for real-time hardware implementation. The scene change frame is detected with the
temporal correlation, and the best QP can be found for the scene change frame according to the
spatial correlation of I-frame. Simulations demonstrated that the proposed methods achieve
better coding quality and save the coding time compared to JVT-G012 on JM 12.4 reference
software. Therefore, our algorithm is suitable for low-power and high quality H.264/AVC core
system.
96
Table E2. Performance comparison of the proposed scheme with JM 12.4. (56Kkbps)
Average PSNR (dB) Bit Rate (kbps) Gain(dB)
Sequence
JM12.4 InitialQP STP-RC AGOP JM12.4 InitialQP STP-RC AGOP InitialQP STP-RC AGOP
Akiyo 40.30 41.20 41.49 43.47 57.11 57.10 57.11 57.55 +0.90 +1.19 +3.17
Claire 41.11 41.40 41.64 42.58 57.07 57.06 57.04 57.63 +0.29 +0.53 +1.47
Container 35.59 35.91 35.82 37.42 57.06 57.07 56.77 57.42 +0.32 +0.23 +1.83
Hall 35.84 36.23 36.58 38.41 57.18 57.19 57.14 57.46 +0.39 +0.74 +2.57
News 33.45 33.91 33.82 35.94 57.16 57.14 57.14 57.42 +0.46 +0.37 +2.49
Salesman 34.35 34.54 34.47 37.45 57.19 57.06 56.91 57.59 +0.19 +0.12 +3.10
Silent 32.52 32.79 32.47 34.26 57.09 57.07 56.84 57.39 +0.27 -0.05 +1.74
Average 36.166 36.568 36.613 38.504 57.123 57.098 56.993 57.494 +0.403 +0.447 +2.338
98
Table E4. The encoding time (msec) of rate control comparison of the proposed scheme with JM 12.4.
Sequence Akiyo Claire Container Hall News Salesman Silent
JM12.4 628.89 2178.64 349.14 243.30 398.20 409.04 290.49
STP-RC 163.40 268.51 161.89 152.64 161.74 279.26 172.4156Kbps
△Time 0.74 0.88 0.54 0.37 0.59 0.32 0.41
JM12.4 1378.73 2482.21 472.87 280.00 402.23 412.98 278.23
STP-RC 187.01 258.06 154.96 157.98 170.71 264.64 155.9295Kbps
△Time 0.86 0.90 0.67 0.44 0.58 0.36 0.44
JM12.4 1549.88 2588.41 556.81 261.98 550.23 572.18 287.18
STP-RC 174.87 274.85 159.39 175.61 166.73 235.55 165.63133Kbps
△Time 0.89 0.89 0.71 0.33 0.70 0.59 0.42
JM12.4 1621.46 2629.26 642.68 314.13 804.09 725.14 338.56
STP-RC 175.50 270.42 173.46 187.12 166.13 280.94 173.43171Kbps
△Time 0.89 0.90 0.73 0.40 0.79 0.61 0.49
JM12.4 1661.03 2702.60 679.64 363.85 1068.87 918.29 419.56
STP-RC 149.31 257.09 176.92 160.96 171.09 250.96 171.14209.5Kbps
△Time 0.91 0.9 0.74 0.56 0.84 0.73 0.59
JM12.4 1713.50 2699.85 719.82 407.88 1247.81 1296.74 569.46
STP-RC 161.99 279.48 186.39 180.04 164.45 269.83 173.81247.1Kbps
△Time 0.91 0.90 0.74 0.56 0.87 0.79 0.69
Average 87% 89% 69% 44% 73% 57% 51%
100
15
20
25
30
35
40
45
1000 1500 2000 2500 3000 3500 4000 4500 5000
B
es
ti
ni
tia
lQ
P
bpp=0.025(19.5kbps)
bpp=0.05(38kbps)
bpp=0.1(76kbps)
bpp = 0.1
bpp = 0.05
bpp = 0.025
Akiyo
Carphone
Silent Foreman
Saleman News
Container
Coastiguard
(a)
12
14
16
18
20
22
24
26
28
30
1000 1500 2000 2500 3000 3500 4000 4500 5000
B
es
ti
ni
tia
lQ
P
bpp=0.15 (114kbps)
bpp=0.2 (152kbps)
bpp=0.25(190 kbps)
bpp = 0.15
bpp = 0.2
bpp = 0.25
Carphone
Silent
foreman
Akiyo
News
Container
Coastiguard
Saleman
(b)
Fig. E3. The relation between best initial quantization parameter and average local
homogeneity for (a) Reg-1 and (b) Reg-2.
102
31
32
33
34
35
36
37
38
39
40
41
1 31 61 91 121 151 181 211 241 271
Frame Nember
Y
SN
R
(d
B
)
JM12.4
IQP
IQP+STP-RC
IQP+STP-RC+AGOP
(a). Container, 56Kbps
27
29
31
33
35
37
39
41
43
1 31 61 91 121 151 181 211 241 271 301 331 361 391 421
Frame Number
Y
SN
R
(d
B
)
JM12.4
IQP
IQP+STP-RC
IQP+STP-RC+AGOP
(b). Salesman, 56Kbps
Fig. E6. PSNR curves for the proposed scheme.
104
0
200
400
600
800
1000
1200
1400
56 95 133 171 209.5 247.1
BitRate(Kbit/s)
En
co
di
ng
Ti
m
e
fo
rR
C
(m
se
c)
JM12.4 STP-RC
(a) News
0
100
200
300
400
500
600
56 95 133 171 209.5 247.1
BitRate (Kbit/s)
En
co
di
ng
Ti
m
e
fo
rR
C
(m
se
c)
JM12.4 STP-RC
(b) Silent
Fig.8. Comparison of encoding time for JM12.4 and our proposed scheme.
106
29.89dB 32.92dB
27.86dB 32.65dB
28.08dB 31.68dB
Fig. E10.Visual comparison among JM12.4 (left) and proposed (right) in scene change case. These frames are the
153th, 229th, and 305th frame from top.
108
G. Wireless TV Tuner
This study had been published on : “High-Performance active polyphase filter designfor digital TV tuner”,
Microelectronics Journal, vol.40 no.6 pp.966-972, June, 2009.
Fig. G1 the circuit of proposed 4 stages polyphase filter.
Generally, the chip implements the capacitor with two-layer metals. This will occupy large chip area,
particularly for the capacitance over pf. In fact, the capacitor also can be realized with MOS circuit to reduce
the silicon area. The gate to drain and source has the gate capacitor that can be used to design the polyphase
filter. The gate capacitor can be decided by
D
LW
Cg
 , where is the material factor, W and L is the
channel width and length of MOS respectively, D is the thickness of channel. The  and D is dependent on
the process that can not be changed in the design level. One can compute various W and L to get the wanted
capacitor. The drain and source of MOS is wired together to enhance the capacitance double. The MOS
capacitor is obtained from the gate and drain. The MOS capacitor occupies much less chip area than the metal
layers. For example, we only achieve several fF capacitor with square size from the metal layer. However,
with the same silicon area, the MOS capacitor is several pF. In order to reduce the chip area, all capacitors in
can be replaced by MOS capacitor. Fig. G1 shows the proposed active polyphase filter without using any
capacitor.
This study presents active polyphase filter for DVT tuner module. Table G1 show the chip feature of the
proposed filter. For the balance of output level, the four-stage polyphase filter is designed with MOS
components rather than capacitors. Comparisons with the conventional RC-based polyphase filter, there are
many advantages using this approach. (1) The silicon area can be greatly saved on-chip. (2) The signal level
can be gained about 25dB. (3) The chip quality is more stable as the process deviation. When the polyphase
filter consists of mixer, the image frequency can be rejected at least 35dB in DTV tuner band, as shown in Fig.
110
結論
本計畫已成功開發 H.264 關鍵性模組，並有前贍性及獨特性技術，受到國內外專家認
同，目前已產出 6篇期刊論文、8篇會議論文，成果豐碩。另有一專利申請中。
本計畫除學術研究外，另己開發出即時 H.264 硬體晶片碼，以最少複雜度達到實現
H.264 編解碼系統，並用 FPGA 完成 H.264 雛型系统展示，將來可技轉至エ業界。
112
[20]Y.D. Zhang, F. Dai and S.X. Lin,” Fast 4*4 intra-prediction mode selection for H.264”, IEEE International
Conf. Multimedia and Expo( ICME), pp.1151-1154, 2004.
[21]F. Pan, X. Lin, S. Rahardja, K. P. Lim, Z. G. Li, D. Wu, and S. Wu, ”Fast mode decision algorithm for
intraprediction in H.264/AVC video coding”, IEEE Trans. Circuits and Systems Video Technology, vol. 15,
no.7, pp. 813-822, July, 2005.
[22]Y.-C. Lin T. Fink and E. Belers, “Fast mode decision for H.264 based on rate-distortion cost estimation,” 
Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, pp. 1137-1140, Apr. 2007.
[23]Y. Yu and L. Wang, “A fast mode selection method for H.264 high profile,” Proc. IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing, pp. 681-684, Apr. 2008.
[24]Y.W. Huang, B.Y. Hsieh, T. C. Chen, L. G. Chen, “ Analysis, fast algorithm and VLSI architecture design 
for H.264 Intra frame coder”, IEEE Trans. Circuits and Systems Video Technology, vol. 15, no.3, pp. 378-400,
March, 2005.
[25]C.W. Ku, C.C. Cheng, G.S. Yu, M.C. Tsai, T.S. Chang, “ A high-definitionH.264/AVC intra-frame codec IP
for digital video and stil camera applications”, IEEE Trans. Circuits and Systems Video Technology, vol. 16,
no.8, pp. 917-928, Aug., 2006.
[26]S. C. Hsia and Y. C. Chou, ‘’Fast intra-prediction with near pixel correlation approach for H.264/AVC
system’, IET Image processing, vol. 2, no.4, pp.185-193, Aug. 2008.
[27]H.264 video coding reference software, http://bs.hhi.de/~suehring/tml/download
[28] K. H. Chen, J.I. Guo, and J.S. Wang “A high-performance direct 2-D transform coding IP design for
MPEG-4 AVC/H.264”, IEEE Trans. Circuits and Systems Video Technology, vol. 16, no.4, pp.472-483, April.,
2006.
[29]Samir Palnitkar, “Veriolg HDL,” Prentice Hal, Nj07458, 1996.
[30]Xilinx, the field programming gate array, Web: www.xilinx.com.
[31]Y.W. Huang, B.Y. Hsieh, T. C. Chen, and L. G. Chen, “Analysis, fast algorithm and VLSI architecture
design for H.264 Intra frame coder”, IEEE Trans. Circuits and Systems Video Technology, vol. 15, no.3, pp.
378-400, March, 2005.
[32]L. Yang, K. Yu, J. Li, and S. Li, ”An effective variable block-size early termination algorithm for H.264
vdeo coding”, IEEE Trans. Circuits and Systems Video Technology, vol. 15, no.6, pp. 784-788, June, 2005.
[33]F. Pan, X. Lin, S. Rahardja, K. P. Lim, Z. G. Li, D. Wu, and S. Wu, ”Fast mode decision algorithm for
intraprediction in H.264/AVC video coding”, IEEE Trans. Circuits and Systems Video Technology, vol. 15,
no.7, pp. 813-822, July, 2005.
[34]C.W. Ku, C.C. Cheng, G.S. Yu, M.C. Tsai, and T.S. Chang, “ A high-definition H.264/AVC intra-frame
codec IP for digital video and stil camera applications”, IEEE Trans. Circuits and Systems Video Technology,
vol. 16, no.8, pp. 917-928, Aug., 2006.
[35]R. R. Osorio and J. D. Bruguera,”High-throughput architecture for H.264/AVC CABAC compression
system”IEEE Trans. Circuits and Systems Video Technology, vol. 16, no.1, pp. 1376-1384, Nov, 2006.
[36] Iain E. G. Richardson , H.264 and MPEG-4 Video Compression: Video Coding for Next-generation
Multimedia, John Wiley & Sons, 2003.
[37]Samir Palnitkar, Verilog HDL, Prentice Hall, Nj07458, 1996.
[38]Xilinx, the programming logic, Web: www.xilinx.com.
[39]T. C. Chen, Y. W. Huang, C. Y. Tsai, B. Y. Hsieh and L. G. Chen “Architecture design of context-based
114
in H.264/AVC”, IEEE Transactions on Multimedia, vol.8, no.3, pp.457-466, June. 2006.
[55]Y. Su, and M.T. Sun, “Fast multiple reference frame motion estimation For H.264/AVC”, IEEE Trans.
Circuits and Systems for Video Technology, vol. 16, no.3, pp. 447-452, Mar. 2006.
[56]S. C. Hsia,”VLSI implementation for low-complexity full search motion estimation”, IEEE Trans. Circuits
and Systems on Video Technology, Vol. 12, N0. 7, pp. 613-619, July, 2002.
[57]S. Goel, Y.Ismail, M.A. Bayoumi, “Adaptive search window size algorithm for fast motion estimation”,
IEEE 48th Symp. Circuits and Systems, 7-10 Aug. 2005 Vol. 2, pp. 1557 -1560.
[58]J. Y. Tham, S. Ranganath, M. Ranganath, and A. A. Kassim, “A novel unrestricted center-biased diamond
search algorithm for block motion estimation”, IEEE Transactions on Circuits and System for Video
Technology, Vol.8, No.4, pp. 369-377. August, 1998.
[59]S. Zhu and K.-K. Ma, “A New diamond search algorithm for fast block-matching motion estimation,”
IEEE Transactions on Image Processing, vol.9, pp. 287-290, Feb. 2000.
[60]M. Jiang and N. Ling, Low-delay rate control for real-time H.264/AVC video coding, IEEE Trans on Multimedia,
vol. 8, no. 3, pp. 476-477, June 2006.
[61]W. Yuan, S. Lin, Y. Zhang, W. Yuan, and H. Luo, Optimum bit allocation and rate control for H.264/AVC, IEEE
Trans on Circuit and Systems for Video Technology, vol. 16, no. 6, pp.705-715, June 2006.
[62] Y. Liu, Z.G. Li, and Y.C. Soh, A novel rate control scheme for low delay video communication of H.264/AVC
standard, IEEE Trans on Circuit and Systems for Video Technology, vol. 17, no. 1, pp. 68-78, January 2007.
[63] D.K. Kwon, M.T. Shen, and C.-C.J. Kuo, Rate control for H.264 video with enhanced rate and distortion model,
IEEE Trans on Circuit and Systems for Video Technology, vol. 17, no. 5, pp. 517-529, May 2007.
[64]S.C. Lim, H.R. Na, and T.L. Lee, Rate control based on linear regression for H.264/MPEG-4 AVC, Signal
Processing: Image Communication, vol. 22, issue 1, pp. 39-58, January 2007.
[65]Y.K. Tu, J.F. Yang, and M.T. Sun, Rate-distortion modeling for efficient H.264/AVC encoding, IEEE Trans on
Circuit and Systems for Video Technology, vol. 17, no. 5, pp. 530-543, May 2007.
[66] MPEG-2, Test Model 5, Doc.ISO/IEC JTC1/SC29/WG11/93-400. 1993.
[67]S.M. Zhou, J.T. Li, J.H. Fei and Y.D. Zhang, Improvement on rate-distortion performance of H.264 rate control
in low bit rate, IEEE Trans on Circuit and Systems for Video Technology, vol. 17, no. 8, pp. 996-1006, August
2007.
[68]Z.G. Li, F. Pan, K.P. Lim, G. Feng, X. Lin, and S. Rahardja, Adaptive basic unit layer rate control for JVT, 7th
meeting JVT-G-012-r1, Pattaya, Thailand, March 2003.
[69]H. Wang and S. Kwong, Rate-distorting optimization of rate control for H.264 with adaptive initial quantization
parameter determination, IEEE Trans on Circuit and Systems for Video Technology, vol. 18, no. 1, pp. 140-144,
January 2008.
[70] T. Chiang and Y.Q. Zhang, A new rate control scheme using quadratic rate distortion model, IEEE Trans on
Circuit and Systems for Video Technology, vol. 7, no. 1, pp. 246-250, February 1997.
表 Y04
報告內容應包括下列各項：
一、參加會議經過
7/3 搭中華航空至大陸瀋陽市.
7/4 搭車至大連
7/5~7 註冊及參加研討會
7/8 参觀哈爾濱工業大學及附近風景
7/9 回瀋陽市
7/10 搭中華航空回國
二、與會心得
本次 IEEE國際信號處理系统研討會 2010 在中國大連中山大酒店舉行. 研
討會內容共分 17 個 section, 主要有影像處理, 視訊編碼, 語音編碼, 雷
達信號處理, 通訊系统信號處理, 等技術領域.
能與國際學者交換研究心得，收益良多。在此會議中個人在 section 14 中,發表兩篇
過去的研究成果。一篇為文件投影系统中數位信號處理 VLSI 實現,主要研究低複雜度高
品質取樣處理演算法及 VLSI 晶片實現, 此論文已登載在會議論文集第 V653-656 頁中。
另一篇為 H.264 系統中基於 9PB 尋找之快速移動估測演算法, 可節省計算量約 9成且可
保有原始影像品質,不管在速度或複雜度皆優於過去論文，並得到不錯的回應,此論文已
登載在會議論文集第 V657-660 頁。此篇論文作更有系統修改後將來轉投期刊論文。本
次會議最大收獲是能了解目前國外在信號處理領域發展趨勢及研究情況，並認識相關
領域的教授及工程師，增加視野寬度及吸收別人長處，將來有助於個人在研究及教學上
作參考。
三、考察參觀活動(無是項活動者省略)
參觀哈爾濱工業大學
四、建議
無
五、攜回資料名稱及內容 :
Proceedings of the 2010 2nd international conference on signal processing system
一本及 CD 一片。
六、其他
無
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 ■申請中 □無 
技轉：□已技轉 ■洽談中 □無 
其他：（以 100字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500字為限） 
本計畫研究H.264編碼技術, 先研究快速演算法, 以減少計算量, 主要技術包括快速演算
法及 VLSI 設計, 完成個別 IP電路, 最後整合 IP完成 H.264 編碼系統, 並可在 FPGA 板上
作即時展示. 可用較少 gate count 實現即時 H.264 編解碼系統 
本研究技術具有獨特性, 已發表 5篇期刊論文, 申請 2篇專利. 
視訊壓縮廣泛應用在數位電視及多媒體產業，具有龐大商機。編碼技術提供廣泛的應用，
可運用在網際網路及未來機械人視覺辨知系統中。對於未來在數位生活、視訊廣播、視訊
會議、視訊索引、視訊知覺問題之研究，具有很高學術及實用價值，研究成果將有利於提
升國內學術水準及產業界之競爭力 
 
 
