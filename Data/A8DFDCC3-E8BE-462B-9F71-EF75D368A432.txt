 1
在大量監控影片上有效率之搜尋架構 
An Effective Retrieval Framework for Large Scale Surveillance Videos 
 
國科會補助專題研究計畫 
計畫編號：NSC 99-2221-E-147-005- 
計劃執行：99 年 08 月 01 日 至 100 年 07 月 31 日 
計畫主持人：江政杰 德明財經科技大學 資訊科技系 助理教授 
 
中文摘要 
 
現代都市中數位監控系統已經廣泛的被使用在各個角落，隨之而來的是大量監控影片的產
生。使用者必須耗費大量的人力在一個長時間的影片中尋找監控目標，因此設計適當的系
統來協助使用者能快速找到目標是很重要的需求。本計畫針對監控影片設計一個快速的瀏
覽與查詢系統。我們首先擷取影片中的移動物體以保留監控影片中最主要的資訊，維持物
體在空間軸的座標，重新安排這些物體出現的時間軸位置，讓移動物體能更緊密重疊的出
現，以便產生短時間但仍保留所有移動物體資訊的濃縮影片。在我們設計的搜尋系統中，
使用者輸入適當的查詢條件後，系統產生對應濃縮影片為瀏覽機制，使用者在發現有興趣
的監控目標後，系統可以直接連結到此物體在原始影片的位置。我們同時設計一套量化的
實驗，實驗成果顯示本系統的瀏覽搜尋機制，可以大幅的降低使用者在監控影片內尋找目
標的時間。 
 
英文摘要 
 
Surveillance cameras have been widely installed in large cities to monitor and record human 
activities for different applications. Since surveillance cameras often record all events 24 
hours/day, it necessarily takes huge workforce watching surveillance videos to search for specific 
targets, thus a system that helps the user quickly look for targets of interest is highly demanded. 
In this project, we design a quick browsing and retrieval system for surveillance videos. Our 
basic idea is to collect all of moving objects which carry the most significant information in 
surveillance videos to construct a corresponding compact video by tuning positions of these 
moving objects. The compact video rearranges the spatiotemporal coordinates of moving objects 
to enhance the compression, but the temporal relationships among moving objects are still kept. 
Our system can generate a compact video according to the user query. When the user finds targets 
of interest in the compact video, this system can directly link to the corresponding original video. 
Our experiments present convincing results that our system can significantly decrease the search 
time in surveillance videos. 
 
 3
 
圖一、本計畫系統建構之流程圖 
 
簡單來說，我們的構想是針對單一固定畫面的攝影機建立一個兩階層的瀏覽搜尋機
制。使用者在輸入查詢條件後，系統會擷取出符合條件的移動物體並產生對應的濃縮影片，
讓使用者可以快速瀏覽內容。當使用者發現有興趣的目標，可以直接連結到原始監控影片
內觀看，由於濃縮影片的長度遠小於原始影片，又透過查詢條件過濾掉大部分無關的物體，
因此使用者可藉由本系統在很短的時間內搜尋很長的影片內容資訊。 
單一攝影機畫面能涵蓋的範圍很有限，但是如果考量各攝影機相關聯建置成攝影機網
路，又會將系統的環境建置複雜化，因此本系統所設計的瀏覽搜尋機制採用折衷方式：允
許使用者自行匯入欲處理的監控影片，各監控影片的時空關連並不予以考量。也就是說，
如果使用者要在多個監控影片內找尋目標，就將這些影片匯入整個系統內，系統的兩階層
瀏覽搜尋機制可協助使用者快速找到有興趣的目標，但是這些目標彼此之間的時空關聯就
不是本系統所考量的範圍。 
本研究報告的組織如下。在第 2 節我們將介紹根本計畫的相關研究；在第 3 節介紹如
何在影片中擷取移動物體，並建立適當之索引資訊。在第 4 節中，我們詳細說明濃縮影片
的設計機制，並舉實例說明我們的設計效果。第 5 節介紹我們設計的系統介面，並且在第
6 節展現實驗的內容與成效，以說明本系統在搜尋監控目標的成效。最後，我們在第 7 節
會針對本計畫的內容做出結論，並描繪未來可能的研究方向。 
 
 
2. 相關研究 
 
在目前影片的搜尋的資訊市場，最主要也是最簡單的方式仍是關鍵字搜尋：使用者輸
入某些關鍵字，由於影片事先已經加註不同的文字描述，因此這樣的關鍵字搜尋可以把影
片搜尋的問題，轉換成傳統的資訊檢索與搜尋。例如廣受消費者歡迎的 YouTube [25]，就
是一個最典型的關鍵字搜尋代表。使用關鍵字搜尋方式的最主要優點，在於關鍵字能夠貼
近使用者的語義資訊，而不必處理影片內容的資訊。但是這一特點同時也產生對應之弱點：
影片必須事前經過仔細的文字標註；如果這個步驟不夠仔細小心，關鍵字的搜尋效果將會
 5
3. 移動物體偵測與索引 
 
欲建立我們的濃縮影片快速瀏覽機制，第一個工作就是擷取監控影片中的移動物體資
訊。針對這個問題，過去有非常多的研究者已經發展出許多方法，但是在面對無特定範圍
種類的監控影片時，要能夠完美的取出所有的移動物體，還是一件非常困難的工作。在本
計畫中，我們並不特別設計移動物體偵測的方法，而是採用現有廣泛被應用的做法－建立
背景模型以取出前景物體並搭配 MeanShift [6]的視覺追蹤方法，來擷取出影片中的移動物
體資訊。針對影片中的取得之移動物體，我們將建立索引資訊，以提供使用者在語意資訊
上基本之搜尋機制。 
 
3.1. 移動物體擷取 
 
當輸入一個監控影片串流資訊後，首先建立此影片的背景模型。雖然我們假設監控攝
影機的視角是固定的，但是視野中物體來來去去，可能有物體例如汽車在長時間的停留後
必須被視為背景，同時考量長時間錄影時光影的變化，因此必須針對此固定視野的影片建
立動態的背景模型。我們採用的方法是高斯混合模型(Gaussian Mixture Model)，以 OpenCV 
[24] 內的函數來完成實作。當影片內某一 frame 的像素資訊與當時對應的背景模型差距太
大時，就被視為前景物體，經過 erosion 與 dilation 的運算，可以過濾零碎的雜訊，取得較
佳教完整的前景物體。 
透過背景模型取得的前景物體將是可能的移動物體，下一步我們針對這些可能的移動
物體進行追蹤，將各 frame 之間的移動物體資訊串接起來，同時可以過濾掉非移動物體的
前景雜訊。我們採用的視覺追蹤方法是 MeanShift，透過對前景物體追蹤而定義影片中移動
物體。要正確無誤的擷取出移動物體是很困難的問題，在真實環境中有很多的因素會造成
移動物體切割時的錯誤，例如光影變化、兩個物體交錯而過、被路燈樹木等外在物體遮蔽
等。在我們的工作中移動物體的切割並不完美，因此常可見到物體缺一塊的破碎狀態，但
是這對我們後面要建立的濃縮影片的影響並不會太嚴重，即使一個物體被切成多塊區域，
但是在濃縮的時候這些區域仍會出現，還是可以達成使用者瀏覽的目的。 
 
3.2. 建立物體索引資訊 
 
由於我們所建立的瀏覽架構是以移動物體為核心，因此對影片中的移動物體資訊建立
索引架構，將可提高系統運算的效能；如果能建立物體的語意資訊索引，更能提供使用者
在語意條件上的查詢。因此，我們針對移動物體建立兩類型的索引資訊。第一種索引資訊
包含每個移動物體的基本資料，例如物體的編號、影片中出現的位置(包含時間軸的座標、
在 frame 上的位置座標)、物體的面積、物體區域的長度與寬度、物體出現時間的長度等。
第二種索引資訊包含物體的語意資料，我們設計兩種語意資料來描述移動物體，包含物體
類型(有汽車、行人、摩托車、其他)與物體顏色(暗色、亮色、黃色、紅色)。 
由於系統使用的監控影片是位於街道環境，移動物體的類型設定為汽車、行人、摩托
車三類，自行車將被歸納為摩托車，其他例如動物、樹影等就被歸為其他。在物體顏色部
 7
能緊密的重疊出現在新濃縮影片上。在此機制下，原始監控影片內沒有任何移動物體的畫
面將會被省略，同時各物體彼此出現的時間資訊將被重置，以減少濃縮影片的長度。 
在 Video Synopsis [18]設計一個最佳化的方法來決定移動物體新的位置，圖四舉了三個
範例來說明 Video Synopsis 的設計方法：針對原始影片中各移動物體所在的時空位置，為
了能達到最大極限的濃縮，各物體出現的時段將被切割以便儘可能的擠在一起。Video 
Synopsis 的方法可以很高比例的濃縮影片內移動物體資訊，所以出現的濃縮影片長度可以
非常短；但是整個 Video Synopsis 內各移動物體之間的時間關係將被完全破壞。例如在原
始監控影片中物體 A 在物體 B 之前出現，但是為了能更大幅度的壓縮影片長度，在濃縮的
Video Synopsis 內 A 與 B 出現的位置與順序將全然不可測。因此在 Video Synopsis 的設計可
以快速的顯示影片中某個物體”有”或”沒有”出現，但是在監控影片的瀏覽與搜尋應用時，
各物體之間的時間關係，會是使用者在觀察時的一個很重要資訊。在濃縮影片上保留移動
物體的時間資訊，可以讓使用者在觀看時，更能掌握整個影片中各事件發生的情境。 
 
圖四、Video Synopsis 的設計概念。圖片上方是原始影片中移動物體的時空位置示意圖，而
下方是濃縮後在 Video Synopsis 中，各物體被切割並重疊的結果。此圖例取自[18] 
 
我們希望設計的濃縮影片中，除了可以保留所有的移動物體影像資訊，同時也能保留
物體在影片中相對的時間關係，但是這樣濃縮的效果將會大打折扣，濃縮影片的長度太長
就失去的濃縮的價值。因此，我們採用折衷的方式：影片濃縮時可以保留各移動物體出現
的時間順序。在我們的設計中，移動物體在濃縮影片出現順序，與照原始影片中出現的時
間順序相同，但是物體移動過程中就不控制彼此的時間關係。如圖五的範例中，兩台車 A
與 B 在原始影片中相隔時間 d 出現，所以在濃縮影片 A 將會比 B 早出現，但是或許 B 移
動速度較快有可能在濃縮影片中超車。也就是說，我們藉由保留物體部份時間關係(出現時
間順序)，以提供觀看者在監控影片上各事件的掌握，同時亦能達成大幅度的濃縮效果。 
 
圖五、我們設計的濃縮影片中，將保留移動物體在原始影片的出現順序 
 9
frame 數目來呈現。我們的系統預設 uf=30，而 dmax=uf 且 dmin=uf/3，代表著在濃縮影片中，
兩個相鄰物體時間間隔將在 10 到 30 frames 數之間，而這個範圍可以由使用者透過 uf 值的
控制而自行調整。 
決定各移動物體的新座標位置後，另一個問題是背景畫面的取得。由於攝影機視角固
定，因此整個背景基本上不會有什麼變動，但是在長時間的錄影下，光影、氣候、甚至停
車等因素就會變的很麻煩。我們的做法是每格一段時間(實作是 10 分鐘)去計算一張固定的
背景畫面，計算方式是以此 10 分鐘影片執行 median filtering。每個移動物體在濃縮影片第
一次出現時，取得此物體在原始影片中對應的固定背景圖片當底圖，將各移動物體貼在背
景圖片上，形成濃縮影片的個別 frame。 
 
 
 
 
 
圖六、包含三台移動汽車的濃縮圖片範例，frame 順序為右至而左上至下。車道上的三台汽
車依循彼此在原始影片出現的時間順序，車輛原先出現之間隔由右而左分別為 15秒與 3秒。 
 
我們以圖六的實際範例來說明濃縮影片設計的效果。圖六顯示一個濃縮影片的片段共
12 frames，播放的順序是由左而右由上而下，影片中包含三台黃、銀、黑色移動汽車（右
上半輛白色停車汽車為背景）。在原始影片中此三台車輛出現的時間間隔分別為 15 秒與 3
 11
到有興趣的監控目標，進而連結到此目標在原始影片的位置，而不必在冗長的原始影片中
如大海撈針的慢慢尋找。 
在系統中我們同時設計相關的輔助功能，以提高使用者操作上的便利，包含了影片播
放速度的調整、暫停與播放的切換、以及對應影片播放之時間位置的捲軸。由於濃縮影片
在播放時已經非常快速，因此可以自行調整播放速度以避免超過使用者視覺的負擔。這些
輔助的功能盡量以一般的影片播放操作來設計，以利使用者可以直覺的操作，減少使用上
的困難。 
 
 
6. 實驗結果與討論 
 
我們拍攝三個長時間監控影片來試驗濃縮影片的效果，同時測試本系統能達成的效
能。表一列出此三個影片的詳細資料，最長的時間超過一小時，最短的也有 40 分鐘，其中
vehicle3.avi 是在夜晚所拍攝。三者的濃縮影片長度直接與移動物體數量相關，這是因為越
多的移動物體就更需要濃縮影片的空間來存放，因此在 vehicle2.avi 內移動物體數量遠大於
其他二者，對應的濃縮影片長度也跟著大幅增加。這樣的設計基本上是合理的，因為監控
影片內移動物體是最重要的資訊，為了能夠保留所有移動物體資訊，所以使用更長的濃縮
影片來保留物體時間與空間的相關聯。 
 
表一、三個監控影片全部移動物體所產生之濃縮影片資料 
影片 原始影片長度 
(sec) 
移動物體數量 濃縮影片 frame 數 濃縮影片長度 
(sec) 
vehicel1.avi 3704 232 4064 135.5 
vehicel2.avi 3084 737 11278 375.9 
vehicel3.avi 2174 151 3288 109.6 
 
我們設計一個實驗來量化評量本系統在輔助搜尋目標的效能，基本的方法是找多位受
測者實際使用系統以記錄尋找監控目標所花費的時間。測試內容分成實驗組與對照組，本
計畫所設計的系統為實驗組，我們另外設計一套對照組系統，來驗證本計畫系統的搜尋效
率。最簡單的對照組設計就是完全不對監控影片處理，僅提供快轉等基本功能協助使用者
的操作；但是這樣的操作很明顯的會非常沒有效率，實質上並沒有太多比較的意義。因此
我們設計的對照組系統中，除了提供快轉等基本功能外，同時將影片中沒有任何物體移動
之片段移除，達到最基本的濃縮功能。在移除無任何前景動作的監控影片並搭配快轉功能，
技術上不但單純且操作上具有相當便利，以此為對照組在實驗數據上更具比較價值。在後
面量化的實驗中，我們以 vehicle1.avi 影片為資料，原始長度為 3704 秒，實驗組使用之濃
縮影片長度是 135.5 秒，而對照組使用之無空白原始影片長度是 997 秒。 
首先我們在 vehicle1.avi 的資料庫中隨機的出 22 個移動物體當作監控目標，前兩個編
號 A 與 B 為受測者練習用而不計測試時間，剩餘 20 個物體給予 1-20 的編號如圖八的圖示，
以編號圖示無順序的貼在原始影片此物體上共 15 frames (0.5 秒)。我們將此 20 個監控目標
 13
有 10 組的實驗組與對照組數據。我們將實驗結果列表於表三，顯示每個測試目標在原始影
片開始出現的時間點、實驗組與對照組找到此目標的平均時間、以及實驗組時間除此對照
組時間所得到的比率值。整體上尋找 20 個測試目標時，我們所設計的方法與單純消去空白
的方法所需的時間比例是 0.29，加速的效果大約是 3.4 倍。由於對照組的影片已經將原始
影片的空白處拿掉，長度只有原始影片大約 1/4，所以如果直接在完整之原始影片上測試，
差距將會更大。詳細分析表三的內容，我們可以看到在尋找所有的測試題目上，實驗組所
耗費的平勳時間皆小於對照組，且差距大體上都相當大，比率值小於 0.5 之間的測試題目
有 16 個。仔細分析表三的數據中，我們可以發現實驗組與對照組比率在 0.5 以上的測試目
標有 5、11、13、17，這些目標的共通點是離影片開始時間較近之處，因此濃縮影片的加速
效果較不明顯。 
 
表三、實驗組與對照組之監控目標搜尋時間 
目標編號 
ID 
目標出現位置
(sec) 
實驗組平均時間
(sec) 
對照組平均時間
(sec) 
比率 
1 1100.8 93.3 204.8 0.46 
2 646.7 42.5 95.7 0.44 
3 1700 125.8 392.5 0.32 
4 2955.2 153 641.7 0.24 
5 249.7 40 53.8 0.74 
6 1693.3 93.3 427.8 0.22 
7 3097.7 191.5 652.5 0.29 
8 229.2 23 50.2 0.46 
9 2815.7 177.7 607.3 0.29 
10 1832.3 105.3 486.5 0.22 
11 705.7 66.5 118.8 0.56 
12 1178 71 236.7 0.3 
13 495.3 52 69.5 0.75 
14 323.2 30.5 65 0.47 
15 1390 110.8 305 0.36 
16 2707.4 142.5 617.2 0.23 
17 181.7 25.8 38.8 0.66 
18 883.7 54.3 196 0.28 
19 2527 162.8 571.8 0.28 
20 3422 171.5 723.2 0.24 
average  96.66 327.74 0.29 
 
 
7. 結論與未來研究 
本計畫設計一個監控影片的搜尋系統，協助使用者面對長時間的監控影片上，可以快
 15
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 5, pp. 
603-619, 2002. 
[7] R. Datta, J. Li, and J. Z. Wang, “Content-Based Image Retrieval - Approaches and Trends of 
the New Age”, in Proceedings of the ACM SIGMM international workshop on Multimedia 
Information Retrieval, MIR, 2005. 
[8] R. Datta, D. Joshi, J. Li, and J. Z. Wang, “Image Retrieval: Ideas, Influences, and Trends of 
the New Age”, ACM Computing Surveys, vol. 40(2), pages: 1-60, 2008. 
[9] H. Ghosh, P. Poornachander, A. Mallik, and S. Chaudhury, “Learning Ontology for 
Personalized Video Retrieval”, in Proceedings of Workshop on Multimedia Information 
Retrieval on the Many Faces of Multimedia Semantics, pages: 39-46, 2007. 
[10] D. B. Goldman, C. Gonterman, B. Curless, D. Salesin, and S. M. Seitz, “Video Object 
Annotation, Navigation, and Composition”, in Proceedings of the 21st Annual ACM 
Symposium on User Interface Software and Technology, UIST, pages: 3-12, 2008. 
[11] L. Hollink and M. Worring, “Building a Visual Ontology for Video Retrieval”, in 
Proceedings of ACM International Conference on Multimedia, ACM MM, pages: 479-482, 
2005. 
[12] W.-M. Hu, D. Xie, Z.-Y. Fu, W.-R. Zeng, and S. Maybank, “Semantic-Based Surveillance 
Video Retrieval”, IEEE Transactions on Image Processing, vol. 16(4), pages: 1168-1181, 
April 2007.  
[13] Y.-G. Jiang, J. Wang, S.-F. Chang, and C.-W. Ngo, “Domain Adaptive Semantic Diffusion 
for Large Scale Context-Based Video Annotation”, in Proceedings of International 
Conference on Computer Vision, ICCV, Sep. 2009. 
[14] A. Kokaram, N. Rea, R. Dahyot, M. Tekalp, P. Bouthemy, P. Gros, and I. Sezan, “Browsing 
Sports Video: Trends in Sports-Related Indexing and Retrieval Work”, IEEE Signal 
Processing Magazine, vol.: 23(2), pages: 47-58, March 2006. 
[15] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain. “Content-Based Multimedia Information 
Retrieval: State of the Art and Challenges”. ACM Transactions on Multimedia Computing, 
Communications and Applications, vol. 2(1), pages: 1-19, 2006. 
[16] Y. Li, S.-H. Lee, C.-H. Yeh, and C.-C. J. Kuo, “Techniques for Movie Content Analysis and 
Skimming: Tutorial and Overview on Video Abstraction Techniques”, IEEE Signal 
Processing Magazine, vol.: 23(2), pages: 79-89, Mar. 2006. 
[17] H.-Z. Luo, J.-P. Fan, J. Yang, W. Ribarsky, and S. Satoh, “Analyzing Large-Scale News 
Video Databases to Support Knowledge Visualization and Intuitive Retrieval”, in 
Proceedings of IEEE Symposium on Visual Analytics Science and Technology, VAST, 
pages: 107-114, 2007. 
[18] Y. Pritch, A. Rav-Acha, and S. Peleg, “Nonchronological Video Synopsis and Indexing”, 
IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30(11), pages: 
1971-1984, Nov. 2008. 
[19] C.-F. Shu, A. Hampapur, M. Lu, L. Brown, J. Connell, A. Senior, and Y.-L. Tian, “IBM 
Smart Surveillance System (S3): a Open and Extensible Framework for Event Based 
Surveillance”, in Proceedings of IEEE Conference on Advanced Video and Signal Based 
Surveillance, AVSS, pages: 318-323, Sep. 2005. 
[20] R. Vezzani and R. Cucchiara, “ViSOR: VIdeo Surveillance On-line Repository for 
Annotation Retrieval”, in Proceedings of IEEE International Conference on Multimedia and 
 17
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
█ 達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：█已發表 □未發表之文稿 █撰寫中 □無 
專利：□已獲得 □申請中 █無 
技轉：□已技轉 □洽談中 █無 
其他：（以 100 字為限） 
 
研究成果已發表在 the 12th IAPR Conference on Machine Vision Applications, 
MVA, Japan, 2011 研討會, 並已撰寫期刊論文投稿至 SCI 期刊 
 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應
用價值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能
性）（以 500 字為限） 
 
本研究針對監控影片設計一套濃縮影片的機制，讓長時間的原始影片能濃縮成短時間
的濃縮影片，提供使用者能夠快速尋找目標。除了規劃濃縮影片的設計方法，我們同時
實作出一個搜尋系統，使用者可以整合多個監控影片，搭配適當查詢條件，符合條件的
移動物體將被彙整入濃縮影片內，如此使用者就可專注在有興趣的目標。我們的設計可
以兼顧監控目標的完整度，避免傳統影片快轉的搜尋方法可能的遺漏，同時提供方便的
機制讓使用者能快速找到不特定的監控目標。 
本計畫同時設計一套對照的實驗，測試我們所設計的搜尋系統在協助使用者尋找目標
的效能。在實驗中，我們的設計可將一個 61 分鐘的監控影片濃縮成 2.5 分鐘的短片，此
濃縮程度還可由使用者自行調整。在實驗設計中，使用者在 61 分鐘影片中，包含操作介
面時間，平均可以 200 秒左右完成 10 個監控目標的搜尋；其效能與對照組相比亦達顯著
差異。因此我們的實驗充分呈現本計畫成果在實際搜尋目標上的效能。 
由於現代都市安裝大量監控攝影機，大量監控影片的搜尋在各種應用中都非常重要，
本計畫的成果在未來將有很高的發展潛能。因此在計畫結束後，我們將在現有成果下，
持續將系統發展的更完善，並尋求相關廠商技術合作或移轉的可能性。 
 
 二、與會心得 
這次是我第二次參與 MVA 研討會，兩次的感覺都非常好，可以體驗到日本人工作態度的認真，
大致上來說，整個研討會舉辦的各項細節都不會疏忽。由於三月日本才歷經嚴重的地震與海嘯災難，
原本有些擔心研討會是否會受到影響。這次 MVA 2011 舉辦的地點在奈良，離地震的區域相當的遠，
所以在局勢平穩後並不會擔心去奈良是否有什麼危險。在會議舉辦之前，主辦單位多次寄信說明地
震對奈良的影響，讓參與者可以安心，因此這次出席研討會的外國人非常多；相較之下台灣來的參
與者反而很少見，整個研討會的過程中，可以講中文的機會很少。 
MVA 2011 研討會的主題是電腦視覺與相關之應用，在此研討會上發表的論文，大多是在理論上
向下發展，以實際應用的角度出發。不論是大會邀請的 keynote 演講，或者是不同場次發表的論文報
告，都有應用的趨勢。例如美國喬治亞理工學院的 James. M. Rehg 教授的演講，主題是介紹他的團隊
以電腦視覺的技術來分析幼兒行為，藉以輔助判斷是否有自閉症的可能，這真是令人意想不到的研
究題目。諸如此類的例子甚多，當聆聽不同的學者在論文簡報時，或者在 poster session 與作者直接
面對面交流時，常常感覺到：原來還有這樣的研究主題。雖然討論的題目與我現在正在進行的研究
不一定直接相關，但是跟不同地方的研究人員面對面溝通，看看不同的主題有什麼不同的特點，實
在是一個非常有趣的經驗，也是對充實自己研究能量很有效的方法。 
由於我的論文是以 poster 發表，因此在兩個小時的展示時間中，必須面對面與來參觀的人直接
溝通，說明我們的做法與設計的特點，同時為對方的疑惑解釋。用英文直接思考終究無法像平時的
順利，不過事前的準備還是很有幫助的，可以在關鍵的問題上比較有調理的說明。不論是講解給別
人聽，或者是去詢問其他人員的研究內容，在研究的思考上都有不同的觸發，這是平時自己悶著頭
想很難達到的效果，也是我最期待參加國際研討會的收穫。 
在研討會結束後，主辦單位舉辦一個 lab visit 活動，聯繫大阪附近的研究單位讓與會者參觀。這
真是難得的機會，可惜我早就定好隔天回台灣的機票，因為學校還有課也無法延期回台，時間上無
法趕得及參觀行程，這是本次參與研討會最惋惜的一點。當然，整個參與研討會的過程還是令人興
奮的，讓我在自己的研究過程中，可以有個空檔到外面走走，看看別人在做的研究，而不會侷限在
很小的範圍內。 
 
三、考察參觀活動(無是項活動者略) 
四、建議 
五、攜回資料名稱及內容 
論文集、論文光碟等會議資訊 
六、其他 
 
 
 
image. The compact video is the collection of all 
compact frames. The compact video not only compactly 
represents for a copious surveillance video but also 
preserves all essential components of moving objects 
appeared in the source video. Also, the temporal 
relationship among objects can be preserved. Moreover, 
inverted links from the moving objects in the compact 
video to those in the source video are also constructed. 
The user can browse the compact video to fast locate 
targets of interest and then the system can directly 
display the corresponding video clip selected by the user. 
Using our system, the user can spend only several 
minutes watching the compact video instead of hours 
monitoring a large number of surveillance videos. 
This paper is organized as the follows. Section 2 and 3 
introduce the details of moving object extraction and 
indexing, respectively, in our design. Section 4 describes 
the generation of the compact video. The details of the 
system design are presented in Section 5. We finally 
draw conclusions and future works Section 6. 
2. Moving Object Extraction 
To well extract objects from a video is an important but 
challenging task in an outside environment of the real 
world due to possible changes in lighting, weather, etc. 
Therefore, we assume that there is only a slight change in 
the environment in a short period of time (e.g., 10 minutes 
in our system). To simplify our work, it is also assumed 
that the image view of the surveillance camera is fixed. 
Based on our assumptions of the fixed camera view and 
the unchanged lighting, we construct the background 
model for each of video segment and employ the tracking 
process to identify moving objects appearing in the 
surveillance video. 
The background model is constructed for each of 
video segments. We set the length as 10 minutes for each 
segment and employ the openCV functions [14] to build 
a Gaussian background model in the implementation. 
Certainly, other complex models such as [4] can be 
applied. All pixels not belonged to the background parts 
in a video frame are assigned to the foreground. In order 
to be more robust, the erosion and dilation processes are 
applied to the foreground regions. Moving objects can be 
considered as the foreground parts when the background 
model is dynamically constructed. To avoid the influence 
of noises, we discard small connected components and 
perform the MeanShift method [5] to track each of 
moving objects. 
The foreground parts that are extracted according to 
the background modeling can form moving objects in 
video frames, and tracking these moving objects can 
improve the robustness. However, it is still difficult to 
avoid mistakes in the extraction. For example, one object 
may be split under the influence of the lighting, or two 
objects may be merged due to their being too close. All 
unexpected factors make it hard to exactly extract and 
track moving objects in general situations. However, this 
problem is not critically concerned in our work. Even 
though a moving object is fragmental, it finally appears. 
The user can watch them, either intact or fragmental, in 
the compact video to determine whether he clicks the 
object to directly link to the source of the surveillance 
video or not. 
3. Indexing 
All moving objects extracted from the source of 
surveillance videos are indexed and stored in the database. 
The main advantage is the flexibility: we can easily 
extend the capabilities of the system if keeping the 
information of moving objects. Different types of 
attributes of moving objects can be attached into the index. 
For each moving object, the stored information in our 
implementation contains: the start time and the end time 
of the appearance, the appearing coordinates in video 
frames, the width and the height of the minimal rectangle 
covered the object, and the real area of the connected 
component of the object. 
4. Compact Video 
 
Figure 2.  The compact frame is the fusion of the object 
snapshots in the time interval with n video frames by 
rearranging their temporal and spatial coordinates. 
After all of moving objects are indexed, the next task 
is to determine their new positions on the background 
image to generate the corresponding compact frame. 
Figure 2 depicts the basic idea of generating compact 
frames. Given continuous video frames in a time interval 
with length n, the corresponding compact frame is a 
collection of object snapshots which appear in the 
interval. We take one snapshot for each appearing 
moving object and paste it on the background image to 
form the compact frame.  
The most important factor in our design is how to 
determine the representative snapshots, in the n 
continuous frames, pasted on the compact frame for all 
moving objects. An intuitive idea is to compute the 
average of appearing areas for each object in frames of 
the time interval. This approach is simple but may 
produce fragmental objects. Hence, we adopt the median 
as the representative snapshot for each object in frames 
of the interval to form the compact frame. The new 
spatial coordinates of median snapshots in the compact 
frame are the same as whose coordinates in the original 
frames. Moreover, since compact frames are generated 
according to the time order of source video frames, the 
appearing order of moving objects in the compact videos 
can be kept. Preserving their time order is an important 
property for monitoring moving targets in surveillance 
videos. If not, it may be difficult to focus on correct 
targets when the user looks for subjects of interest. 
The time interval of frames, denoted as n, is a critical 
parameter to compress the source videos. This parameter 
can be adjusted by the user. In our design, we set n to 
120 as the default value. In order for the user to play the 
compact video more smoothly, two intervals of frames 
are overlapped in half. That is to say, the compression 
surveillance video. The user can browse the compact 
video to fast locate targets of interest and then the system 
can directly display the corresponding video clip selected 
by the user. Future work includes several directions to 
extend this work. The generation of the compact video 
may be improved to achieve more compression and to 
work on a camera network. Another direction includes 
attaching semantic-level attributes to the indexing to 
provide partial retrieval functions.  
Acknowledgement 
This work was in part supported by National Science 
Council, Taiwan, under Grant No. NSC 99-2221-E-147 
-005 and by Ministry of Economic Affairs, Taiwan, 
under Grant No. 99-EC-17-A-02-S1-032. 
References 
[1] J. Assfalg, M. Bertini, C. Colombo, and A. Del Bimbo. “Semantic 
Annotation of Sports Videos,” IEEE Multimedia, vol. 9, no. 2, pp. 
52-60, 2002. 
[2] S.-F. Chang, L. Kennedy, and E. Zavesky, “Columbia 
University's Semantic Video Search Engine,” Proceedings of 
ACM International Conference on Image and Video Retrieval, 
2007. 
[3] B.-W. Chen, J.-C. Wang, and J.-F. Wang, “A Novel Video 
Summarization Based on Mining the Story-Structure and 
Semantic Relations among Concept Entities,” IEEE Transactions 
on Multimedia, vol. 11, no. 2, pp. 295-312, 2009. 
[4] Y.-T. Chen, C.-S. Chen, C.-R. Huang, and Y.-P. Hung, “Efficient 
Hierarchical Method for Background Subtraction,” Pattern 
Recognition, vol. 40, no. 10, pp. 2706-2715, 2007. 
[5] D. Comaniciu and P. Meer, “Mean Shift: A Robust Approach 
toward Feature Space Analysis,” IEEE Transactions on Pattern 
Analysis and Machine Intelligence, vol. 24, no. 5, pp. 603-619, 
2002. 
[6] P. DeCamp, G. Shaw, R. Kubat, and D. Roy, An Immersive 
System for Browsing and Visualizing Surveillance Video, in 
Proceedings of ACM International Conference on Multimedia, 
2010. 
[7] W.-M. Hu, D. Xie, Z.-Y. Fu, and W.-R. Zeng, “Maybank S.: 
Semantic-Based Surveillance Video Retrieval,” IEEE 
Transactions on Image Processing, vol. 16, no. 4, pp. 1168-1181, 
2007. 
[8] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain, “Content-Based 
Multimedia Information Retrieval: State of the Art and 
Challenges,” ACM Transactions on Multimedia Computing, 
Communications and Applications, vol. 2, no. 1, pp. 1-19, 2006. 
[9] Y. Pritch, A. Rav-Acha, and S. Peleg, “Nonchronological Video 
Synopsis and Indexing,” IEEE Transactions on Pattern Analysis 
and Machine Intelligence, vol. 30, no. 11, pp. 1971-1984, 2008. 
[10] Y. Takahashi, N. Nitta, N. Babaguchi, “Video Summarization for 
Large Sports Video Archives,” Proceedings of IEEE International 
Conference on Multimedia and Expo, 2005. 
[11] B. T. Truong and S. Venkatesh, “Video Abstraction: A 
Systematic Review and Classification,” ACM Transactions on 
Multimedia Computing, Communications, and Applications, vol. 
3, no. 1, 2007. 
[12] Z. Xiong, X. Zhou, Q. Tian, R. Yong, and T. S. Huang, 
“Semantic Retrieval of Video - Review of Research on Video 
Retrieval in Meetings, Movies and Broadcast News, and Sports,” 
IEEE Signal Processing Magazine, vol. 23, no. 2, pp. 18-27, 
2006. 
[13] X.-D. Yu, L. Wang, Q. Tian, and P. Xue, “Multi-Level Video 
Representation with Application to Keyframe Extraction,” 
Proceedings of 10th International Multimedia Modeling 
Conference, 2004. 
[14] Open Computer Vision Library, 
 http://sourceforge.net/projects/opencvlibrary/. 
(a). Two consecutive compact frames with n=60. The left 
and the right are generated by the source frames marked by a 
blue and green rectangle, respectively, in (c). 
(b). Two consecutive compact frames with n=120. The left 
and the right are generated by the source frames in a red and 
purple rectangle, respectively, in (c). 
(c). Source video frames. Each one is selected per 10 consecutive source frames. 
Figure 5.  An illustration of compact frames. (a) and (b): two compact frames with parameter n=60 and n=120, 
respectively, (c): the corresponding source video frames.  
2----------------------------------------------
Submission information
Authors and title:
Cheng-Chieh Chiang*, Ming-Nan Tsai, Huei-Fang Yang
A Quick Browsing System for Surveillance Videos
Type of submission: Contributed paper
Type of presentation: Poster
Conference: The 12th IAPR Conference on Machine Vision Applications
Submission number: 92
----------------------------------------------
To access your workspace please log in at https://iapr.papercept.net/conferences/scripts/start.pl using your PIN 20941 and password
If you do not have your password then follow the link https://iapr.papercept.net/conferences/scripts/pinwizard.pl to retrieve it
----------------------------------------------
Dr. Yasuyo Kita
MVA2011 Program Chair
National Institute ofAdvancedIndustrialScienceandTechnology
(AIST)
1-1-1 Umezono
305-8568 Tsukuba
Japan
E-mail address: mva2011-cms@m.aist.go.jp
未在傳入訊息中找到病毒。
已透過 AVG 檢查 - www.avg.com 
版本: 9.0.872 / 病毒庫: 271.1.1/3467 - 發佈日期: 02/25/11 15:34:00
99 年度專題研究計畫研究成果彙整表 
計畫主持人：江政杰 計畫編號：99-2221-E-147-005- 
計畫名稱：在大量監控影片上有效率之搜尋架構 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 2 0 50%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
