1 Introduction
Consider a sequence, {Y1, Y2, · · · , Yn}, representing the output of a simulation from a covariance-
stationary stochastic process {Yi}ni=1, with an unknown mean μ and unknown positive variance
σ2. For example, Yi could be the delay time for the i-th packet at some node in a communication
network. Let μ be the performance measure we are interested in, Y n = Σni=1Yi/n be the point
estimator of μ, and var(Y n) be the quality measure of using Y n to estimate μ. Song and
Schmeiser (1995) showed that
nvar(Y n) = γ0σ2 − γ1σ
2
n
+ o(n−1), (1)
where γ0 = 1 + 2
∑∞
h=1 ρh is the sum of all correlations, γ1 = 2
∑∞
h=1 hρh is the sum of all
weighted correlations, and ρh = corr(Yi, Yi+h) is the lag h correlation of Yi and Yi+h, which
satisﬁes ρh = σ2O(δh) for h = 1, 2, . . . , δ ∈ (0, 1) to reﬂect a general correlation structure for
a wide range of stochastic processes, including waiting times in steady state M/M/1 queueing
system (Aktaran-Kalayc et al. 2007). The notation o(g(n)) represents the little o function in
that o(g(n))/g(n) → 0 as n → ∞. The notation O(g(n)) represents the big o function in that
|O(g(n))/g(n)| → c as n →∞ where c is a real value.
Most existing papers in simulation output assume that either the simulation run length or the
sample size n is known in advance. Moreover, many of them view data as being stored in inﬁnite
computer memory space. For example, direct (Moran 1975), regenerative (Crane and Iglehart
1975; Glynn and Iglehart 1986), spectral (Priestley 1981; Heidelberger and Welch 1981), non-
overlapping batch means (NBM) (Schmeiser 1982), overlapping batch means (OBM) (Meketon
and Schmeiser 1984), partial-overlapping batch means (PBM) (Welch 1987), standardized time
series (Schruben 1983; Glynn and Iglehart 1990), and its variation. The memory space for these
estimators is proportional to the sample size n; these estimators require O(n) space. Fishman
and Yarberry (1997) proposed Labtach.2 non-overlapping batch means requiring O(log2n) space.
Although O(log2n) algorithms require much small memory space than O(n)-memory algorithms,
Fishman and Yarberry (1997) require the knowledge of n a priori .
Throughout this paper, we use ﬁnite-memory algorithms to denote algorithms that require
O(1) memory storage and do not require the knowledge of n a priori . We explain in additional
detail the two following scenarios with examples to motivate the need for ﬁnite-memory algo-
rithms within the context of simulation output analysis. (1) Sample size is unknown a priori .
Consider creating a simulation model of pandemic inﬂuenza to evaluate the eﬀectiveness of dif-
ferent decision policies on disease spread and other performance measures (Ferguson et al. 2006
and Jenvald et al. 2007). The simulation run length (in terms of the number of patients) is
random, and so n is not known in advance. (2) Sample size is extremely large. One example is
that of using simulation to test data stream algorithms to estimate entropy (a measure of the
rate of transfer of information in a network) from input data that come at a very high rate —
a rate so high that it places stress on a limited computing infrastructure. Recent work on data
streaming algorithms can be found in Lall et al. (2006) and Zhao et al. (2007).
To the best of our knowledge, the dynamic non-overlapping batch means (DNBM) estima-
tor proposed by Yeh and Schmeiser (2000) and the dynamic partial-overlapping batch means
(DPBM) estimator proposed by Song (2007) are the only two existing ﬁnite-memory algorithms
for estimating the variance of the sample mean. The DNBM and DPBM are superior to tra-
ditional batch means estimators in terms of the criterion “parsimonious storage requirements”
(Goldsman and Schmeiser 1997). That is, a good variance estimator should meet the following
criteria: good statistical property, small storage, and fast computation, Song (2007) showed that
the DPBM estimator is a generalization of the DNBM and performs better than the DNBM in
terms of the mse. The disadvantage of using the DPBM is that the DPBM requires four times
of storage than DNBM. This paper proposes a computational version to implement the DPBN
which requires the same storage space as DNBM. Both the DNBM and DPBM (original and
computational versions) require O(n) computation time.
2
Before explaining DPBM, we need to deﬁne the notation used in developing DPBM. Let n
be the total number of observations, which is not known in advance. Let l be the pre-speciﬁed
memory size, which is also the size of the vector where all observations are stored. Let k be the
total number of times collapsing has occurred, k = 0, 1, 2, . . . , log2n/g	− 1, where g = l/8. Let
mk be the batch size, mk = 2k at the k-th collapsing step. Let L be the vector of size l where
DPBM stores batch sums.
A DPBM algorithm divides vector L into four vectors: A,B,C, and D by concatenation,
i.e., L = (A,B,C,D). Let Ak(i), Bk(i), Ck(i), and Dk(i) be the numerical values (batch
sums) stored in the i-th cell of vectors A, B, C, and D in the k-th iteration of collapsing,
respectively, where i = 1, 2, . . . , 2g. Let rA, rB , rC , and rD be the cells used to store the
latest observations in A, B, C, and D, respectively; where rA, rB , rC , rD = 1, . . . , 2g. Let mA,
mB, mC , and mD be the numbers of observations stored in A(rA), B(rB), C(rC), and D(rD),
respectively; where mA,mB ,mC ,mD = 1, . . . ,mk. Let b1, b2, b3, and b4 be the “numbers of full
batches” (in that each batch contains mk observations) in vectors A, B, C, and D, respectively,
where b1 = rA − 1 + mA/mk, b2 = rB − 1 + mB/mk, b3 = rC − 1 + mC/mk, and
b4 = rD − 1 + mD/mk.
The DPBM algorithm starts to collapse data when the data in vector A is full. Speciﬁcally,
a new observation is added in the current cell if the number of observations contained in the
current cell, mA, is less than the full batch size m, indicating that vector A is not full. As long
as the rA-th cell contains the same number of observations as the full batch size and the vector
is full, we collapse these 2g cells into g cells. After collapsing the vector, the full batch size is
updated by doubling the previous value. The logic used to collapse data into vector B, C, and
D is the same as that for vector A, but the data used for collapsing into B, C, and D diﬀer.
The order of collapsing in DPBM is D and C ﬁrst, and then B, and ﬁnally A.
In summary, the DPBM estimator for estimating the variance of the sample mean proposed
in Song(2007) at step k is
VˆDPBM(mk) =
1
db
[
b1∑
i=1
(
Ak(i)
mk
− Y n)2 +
b2∑
i=1
(
Bk(i)
mk
− Y n)2
+
b3∑
i=1
(
Ck(i)
mk
− Y n)2 +
b4∑
i=1
(
Dk(i)
mk
− Y n)2 ] , (4)
where mk = 2k is the batch size, s = mk/4 is the shift, b = (n−mk + s)/s is the total
number of batches, db = b((n/mk) − 1) is the denominator, bi, i = 1, 2, 3, 4 are the numbers of
full batches, and the batch means
Ak(i)
mk
= Y mk(i−1)+1,mk , (5)
Bk(i)
mk
= Y mk(i−1)+2s+1,mk , (6)
Ck(i)
mk
= Y mk(i−1)+s+1,mk , (7)
Dk(i)
mk
= Y mk(i−1)+3s+1,mk . (8)
It is noted that m−1k Ak(i) deﬁned in Equation (10) is the i-th non-overlapping batch means,
therefore the DNBM, deﬁned as VˆDNBM(mk) =
1
db
[∑b1
i=1(
Ak(i)
mk
− Y n)2
]
, is a special case of
DPBM.
4
(2.2.2) (k ≥ 1) Update E˙, E¨, then B˙, B¨, and B˘, ﬁnally collapse the vector A. Reset
mA,mB ,mC ,mD.
E˙ = E¨ = C` = D` = 0.
E˙ = 2B˙ − temp1 − temp2
E¨ = 2B¨ + 2B˘ − temp21 − temp22
D` = B`
B˙ = B¨ = B˘ = B` = 0.
temp1 = A(2) + A(3)
temp2 = A(2g − 2) + A(2g − 1)
B˙ =
∑g−1
i=1 [A(2i) + A(2i + 1)]
B¨ =
∑g−1
i=1 [A(2i) + A(2i + 1)]
2
B˘ =
∑g−1
i=1 [A(2i) + A(2i + 1)] × [A(2i + 1) + A(2i + 2)]
B` = A(2g)
A(i) = A(2i − 1) + A(2i), i = 1, . . . , g
mA = 1,mB = 2k,mC = 2k + 2k+1,mD = 2k−1
rA = g + 1
Step 3: Update the number of collapsing and the batch size. k = k + 1, m = 2k.
Step 4: Initialize the value mA and the sum stored in the current cell A(rA), A(rA) = 0,
mA = 1.
Step 5: Add the new observation yn in the current cell in A. A(rA) = A(rA) + yn
Step 6: Add the new observation yn in B`.
mB = mB + 1
B` = B` + yn
If mB = m then
mB = 0
B˙ = B˙ + B`
B¨ = B¨ + B`2
B˘ = B˘ + (temp2 × B`)
temp2 = B`
B` = 0
end
Step 7: Add the new observation yn in C`.
mC = mC + 1.
C` = C` + yn
If mC = m then
mC = 0
E˙ = E˙ + C`
E¨ = E¨ + C`2
C` = 0
end
Step 8: Add the new observation yn in D`.
mD = mD + 1.
D` = D` + yn
If mD = m then
mD = 0
E˙ = E˙ + D`
E¨ = E¨ + D`2
D` = 0
end
6
and
∑b3
i=1(
Ck(i)
m − Y¯n)2 +
∑b4
i=1(
Dk(i)
m − Y¯n)2
db
=
{
∑b3
i=1 Ck(i)
2
m2 − Y¯n[
∑b3
i=1 Ck(i)
2k−1 − b3 · Y¯n]}+ {
∑b4
i=1Dk(i)
2
m2 − Y¯n[
∑b4
i=1Dk(i)
2k−1 − b4 · Y¯n]}
db
=
{
∑b3
i=1 Ck(i)
2+Dk(i)
2
m2 − Y¯n[
∑b3
i=1 Ck(i)+Dk(i)
2k−1 − (b3 + b4) · Y¯n]}
db
.
4 Summary
Estimating the variance of the sample mean from a stochastic process is essential in assessing the
quality of using the sample mean to estimate the population mean. In addition, estimating the
variance of the sample mean is also crucial in calculating the conﬁdence and prediction intervals
of the population mean and the probability of selecting from alternatives correctly. Variance-
estimation algorithms requiring O(1) memory storage and do not requiring the knowledge of
n a priori are needed when either the sample size is unknown a priori or the sample size is
extremely large. To the best of our knowledge, the DNBM and DPBM are the only two existing
ﬁnite-memory algorithms for estimating the variance of the sample mean that do not require the
knowledge of the sample size a priori . The performance of the DPBM is better than the DNBM,
but the DPBM require more storage space than the DNBM. The proposed computational version
of DPBM improves the original version of the DPBM in that it requires one quarter of the storage
space than the original DPBM version and therefore it requires the same storage space as the
DNBM estimator.
Acknowledgements
This research is supported by the National Science Council of the Republic of China under Grant
No. NSC-93-2213-E-007-060.
References
Aktaran-Kalaycı, T., Alexopoulos, C., Argon, N.T., Goldsman, D. and Wilson, J.R. (2007)
Exact expected values of variance estimators for simulation. Naval Research Logistics, 54,
397–410.
Conway, R.W. (1963) Some tactical problems in digital simulation. Management Science, 6,
92–110.
Crane, M.A. and Iglehart, D.L. (1975) Simulating stable stochastic systems, III: regenerative
process and discrete-event simulations. Operations Research, 23, 33–45.
Ferguson, N.M., Cummings, D.A., Fraser, C., Cajka, J.C., Cooley, P.C. and Burke, D.S. (2006)
Strategies for mitigating an inﬂuenza pandemic, Nature, 442, 448–452.
Fishman, G.S., and Yarberry, L.S. (1997) An implementation of the batch means method,
INFORMS Journal on Computing, 9, 296–310.
Glynn, P.W. and Iglehart, D.L. (1986) Estimation of steady-state central moments by the
regenerative method of simulation. Operations Research Letters, 5, 271–276.
Glynn, P.W. and Iglehart, D.L. (1990) Simulation output analysis using standardized time
series. Mathematics of Operations Research, 15, 1–16.
Goldsman, D. and Schmeiser, B.W. (1997) Computational eﬃciency of batching method, Pro-
ceedings of the Winter Simulation Conference, 202–207.
8
