 2
一、中文摘要 
 
生物認證(或稱生物辨識)是以個人的生理特徵或行為特徵來辨識或驗證其身分的一
門技術。舉凡需要確認使用者身分的場合都是生物辨識技術可應用的範圍，例如門禁管
制、電子商務及資料存取等，而網際網路及行動通信的普及造成資訊安全防護之需求日
益殷切，更確定了生物辨識技術未來的發展潛力。近幾年生物特徵身分確認技術已引起
各界高度的關注，而電腦軟硬體科技、數位信號處理及圖形識別等技術的進步，更促成
這類技術的蓬勃發展，包括：指紋、語音、人臉、視網膜、虹膜、簽名、掌形及手勢等
多種生物特徵都可以作為身分確認之依據。利用生物特徵進行身分確認的好處很多，其
中比較為大家所公認的優點包括：(1)偽造及破解困難、(2)使用方便、(3)適用性廣泛。 
在生物辨識技術中，語者辨識是利用人類最自然的表達方式（即語音）作為辨識身
分的依據。相對於基於其他生物特徵的身分辨識系統，語者辨識技術具有方便使用的優
點，特別是可藉電話或麥克風等透過電話線和網路進行遠端身分辨識。未來銀行業務等
電子商務、個人網路資料的存取與其他資訊服務的安全管制皆可藉由語者辨識技術來達
成。本計畫的目標是開發基於語音的身分確認技術，研究的重點包括： 
1. 語者辨識基礎研究及技術開發。本計畫將開發極少量登錄語音條件下的語者辨識
技術，預計將藉由進行 MFCC 語音信號特徵向量後處理、改良辨識模型訓練方
法、結合語音辨識及語者辨識等方式來達成。另外，本計畫亦將進行假說測定的
理論分析及嘗試開發新的語音信號特徵參數擷取技術及新的辨識模型。 
2. 建立 NIST 語者辨識評比基準實驗平台。 
3. 開發基於語音之身分確認系統。 
4. 與整合型研究計畫『結合音訊與視訊之多模式身分確認之研究』(執行期限：
92/08/01-95/07/31)共同合作開發多模式身分確認技術及多模式身分確認雛形
系統。 
5. 開發其他相關關鍵技術，如語者分段、重疊語音偵測、語者分群等。 
 
關鍵詞：生物認證、生物辨識、語者辨識、語音辨識、語音信號處理、多模式 
 
 4
三、計畫緣由與目的 
 
美國麻省理工學院（MIT）將『生物測定學（Biometrics）』或『生物認證（Biometric 
Person Authentication）』稱為具有改變世界的十大最具潛力的科技之一。何謂生物認證
（或稱生物辨識）？簡而言之，就是以個人的生理特徵或行為特徵來辨識或驗證其身分
的一門技術。自動辨識使用者身分毫無疑問是達成安全（Security）防護的重要關鍵，
近幾年多模式（Multi-modal）生物特徵身分確認技術已引起各界高度的關注，而電腦軟
硬體科技、數位信號處理（Digital Signal Processing）及圖形識別（Pattern Recognition）
等技術的進步，更促成這類技術的蓬勃發展，包括：指紋、語音、人臉、視網膜、虹膜、
簽名、掌形及手勢等多種生物特徵都可以作為確認身分之依據。這些生物特徵可大致歸
納為生理特徵及行為特徵兩大類，簽名及語音一般視為行為特徵。利用生物特徵進行身
分確認的好處很多，其中比較為大家所公認的優點包括：(1)偽造及破解困難、(2)使用
方便、(3)適用性廣泛。舉凡需要確認使用者身分的場合都是這類技術可應用的範圍，例
如門禁管制、電子商務及資料存取等。 
在生物辨識技術中，語者辨識是利用人類最自然的表達方式（即語音）作為辨識身
分的依據。相對於基於其他生物特徵的身分辨識系統，語者辨識技術具有方便使用的優
點，特別是可藉電話或麥克風等透過電話線和網路進行遠端身分辨識。未來銀行業務等
電子商務、個人網路資料的存取與其他資訊服務的安全管制皆可藉由語者辨識技術來達
成。 
本計畫的研究目標是開發基於語音的身分確認技術，研究的重點包括：1)開發極少
量登錄語音條件下的語者辨識技術，藉由進行 MFCC 語音信號特徵向量後處理、改良辨
識模型訓練方法、結合語音辨識及語者辨識等方式來達成。2)進行假說測定的理論分析
及嘗試開發新的語音信號特徵參數擷取技術及新的辨識模型。3)開發基於語音之身分確
認系統。4)與整合型研究計畫『結合音訊與視訊之多模式身分確認之研究』(執行期限：
92/08/01-95/07/31)共同合作開發多模式身分確認技術及多模式身分確認雛形系統。5)
開發其他相關關鍵技術，如語者分段、重疊語音偵測、語者分群等。 
  
其中x = [Λ1(U), Λ2(U),…, ΛN(U)]T 是一個落在向量空間RN的N × 1 向量，w = [w1, w2,…, 
wN]T 是一個N × 1 的權重向量，而b是一個偏移量；其扮演的角色如同likelihood ratio
的門檻值。Ψ(x)形成一個所謂的線性鑑別分類器(Linear Discriminant Classifier)。這個線
性鑑別分類器將目標從原本是要解一個假說測定(Hypothesis Testing)的問題，轉移到要
尋找一組最佳化的參數w和b使得用戶(Clients)與冒用者(Impostors)的輸入語料能被分得
最開。 
線性鑑別分類器是針對可線性分割(Linearly Separable)的資料分佈所設計，然而，對
於真實世界中的資料，大部分的情形是不可線性分割(Nonlinearly Separable)的。因此，
我們提出一個使用基於核心的非線性鑑別分類器(Kernel-based Nonlinear Discriminant 
Classifier)，其基本想法是假如在原始空間RN的資料x是不可線性分割，我們希望經由一
個非線性映射(Nonlinear Mapping) Φ，可以將RN的資料都映射到一個較高維(甚至無限維)
的空間F，使得在F上的資料Φ(x)是可線性分割的，因此，我們可以將公式(1)的Ψ(x)改寫
如下: 
 )()( bT +Φ=Ψ xwx 。              (2) 
公式(2)形成了一個在原始空間RN的非線性鑑別分類器，但它卻是個建構在空間F上的線
性鑑別分類器。直接計算Φ(x)是困難的，然而，經由一些推導，可以使得在空間F上面
的運算只剩下考慮兩個向量之間的內積，而不需直接計算Φ(x)。兩個在空間F的向量的
內積可經由一個核心函數(Kernel Function) k來描述:  
k(x,y)=<Φ(x),Φ(y)>。              (3) 
我們使用兩個典型的基於核心的非線性鑑別分類器：KFD 與 SVM，來實現公式(2)，詳
細內容見參考文獻[2]。 
另外，我們也可以將 )|( λUp 的計算方式寫成如下的通式: 
))|( ),...,|(()|( 1 BUpUpHUp λλλ = ，       (4) 
其中H是一個計算B個背景模型(Background Models) {λ1, λ2,…, λB} 獲得的likelihoods的
一個轉換函數，例如算術平均、幾何平均、取最大值等。但上述函數都太過於簡單，可
能不足以描述非該使用者模型分佈的複雜特性。因此，我們提出 2 種線性組合的方法來
描述H，分別為權重算術組合，我們簡稱WAC (Weighted Arithmetic Combination) [3] 
B
∑
=
=
B
i
ii UpwUp
1
)|()|( λλ 。           (5) 
與權重幾何組合，我們簡稱 WGC (Weighted Geometric Combination) [4]。 
iw
i
B
i
λUpUp )|()λ|(
1=
∏= 。           (6) 
WAC與WGC可根據不同 )|( iUp λ 的重要性，適度地給予一個權重 。當所有權重值 都
相同時，WAC與WGC將可分別退化成算術平均與幾何平均。當滿足 ，其中 
;且 w
iw iw
1* =iw
)|(maxarg* 1 iBi λUpi ≤≤= i = 0， *ii ≠∀ ，則WAC與WGC將可退化成取最大值函數。
所以，我們可以將WAC與WGC視為傳統方法的一般式(General Form)。在WAC方法中，
假設B個背景模型都是高斯混合模型(GMMs)，則公式(5)將仍是一個混合高斯(Mixtures 
Gaussian)的probability density function (pdf)計算，我們可進一步說λ 是一個雙重組合高
斯混合模型，簡稱GMM2，其中單一GMM中各高斯分佈的權重由期望值最大(Expectation 
Maximization, EM)訓練法求得，而不同GMM間的權重 可經由鑑別式訓練法
(Discriminative Training)求得，這個部份我們採用KFD與SVM兩種技術。 
iw
為了解不同GMM間的權重 ，我們將公式(5)代入likelihood ratio的式子，並將
likelihood ratio取倒數後，可推導出一個類似公式(1)的線性鑑別分類器的式子，進一步
我們也可將它描寫成一個類似公式(2)的基於核心的非線性鑑別分類器的式子，來解權重
i，這樣對應回公式(5)的式子就不再是一個線性組合，而是一個非線性組合的式子來描
述H了。此外，描述
iw
w
λ 的GMM2 也不再是一個由簡單的線性組合背景模型 λi所構成，而
 6
其中 )λ,λ( )( nmS 是第m群內某一個語音段落Xn自己的高斯混合模型λn和群高斯混合模型
λ(m)間基於分歧計算所得的相似度： 
∑ −= =Jj jnjnmjmjnm 1 ,,)()()( )],;,(exp[)λ,λ( ΣΣ μμDS jw     (12) 
其中wj是高斯混合模型內各個mixture的權重，D(μj(m), Σj(m), μn,i, Σn,i)是兩個高斯混合模型
的分歧度，可由下式求得：  
( ) ( )( ) ( )({ ) }
( )( ){ } Rinmjinmj
in
m
jin
m
jin
m
jin
m
jin
m
j
jnjn
m
j
m
j
−′+
′+−+′−=
−−
−−−−
   Tr
2
1  
   Tr
2
1   
2
1
),;,(
2/1
,
2/1)(2/1
,
2/1)(
2/1
,
2/1)(2/1
,
2/1)(
,
)(1
,
1)(
,
)(
,,
)()(
μμμμ
μμμμμμμμμμ
μμ ΣΣD
  (13) 
其中μ是平均值向量，Σ是共變異矩陣，R 是特徵向量維度。由於公式(13)並未牽涉任何
likelihood 計算，因此，分群的效率將遠高於最大似然分群法，這個方法最後也是利用
基因演算法來實現，詳細內容見參考文獻[9]。 
我們從 2001 NIST 語者辨識評估語料中隨機選取 15 位男性語者共 197 段長短不一
的語音進行分群實驗，本研究提出的最大群內純度分群法、最大似然分群法和最小分歧
分群法得到的平均群內純度分別是 0.81、0.78 和 0.75，而傳統的階層式分群法得到的平
均群內純度則是 0.52，本研究提出的方法均獲得相當大的效能提升。 
除了改良分群方法以提高語者分群的正確率之外，還有一個重要議題是群數的決
定。常見的作法是在各種假設群數下分別進行分群最佳化，最後再用貝氏資訊基準
(Bayesian Information Criterion, BIC)來選擇最佳的群數。這樣的群數決定法並不
是很可靠，我們因此提出一個基於最小化 Rand Index 的分群法，此法最大的優點是將
分群與最佳群數決定合而為一，這個分群法也是利用基因演算法來實現，詳細內容見參
考文獻[10]。 
 
 
D.系統整合(結合語音辨識及語者辨識) 
 
我們設計及評估結合語音辨識及語者辨識之身分確認系統的架構。結合此兩種辨識
結果進行交叉比對的做法，將使系統更強健(Robust)且安全性高。我們建立一個結合語
音及語者辨識之身分確認系統，整個架構如圖 1 所示。此系統分為兩個階段，第一階段
要求使用者說出個人身分(User Identity)，通過之後進入第二階段要求使用者說出由系統
隨機產生的明碼(例如數字串)，這樣的做法好處是冒充者若利用盜錄之使用者語音入侵
系統，即使通過第一階段也將無法順利通過第二階段。由於已限定使用者說話內容，語
音辨識部分採用關鍵詞擷取(Keyword Spotting)技術取代大字彙連續語音辨識(Large 
Vocabulary Continuous Speech Recognition, LVCSR)，使系統辨識速度加快。由於第一階
段的語音辨識可知道使用者所宣稱之身分，所以語者辨識部分採用語者確認(Speaker 
Verification)技術取代需要把所有目標語者(Target Speakers)都計算一次相似度的語者識
別(Speaker Identification)技術，當系統存在的目標語者數目很多時，相對於語者識別，
語者確認的速度快很多。語音信號特徵向量部份，MFCC 特徵向量用於語音辨識，至於
MFCC 特徵向量經投影轉換之後的新向量則用於語者辨識，我們採用新的語者辨識特徵
向量可以將新使用者的登錄語料大幅降低到只要輸入”我是 xxx(姓名)；0123456789；
9876543210”即可順利訓練出其語者模型。 
在多模式生物辨識方面，我們與整合型研究計畫『結合音訊與視訊之多模式身分確
 8
 10
五、計畫成果自評 
 
本計畫的研究成果相當豐碩，我們共發表了12篇會議論文；在語者辨識研究方面，完成
三篇期刊論文投稿[13-15]，其中一篇期刊已獲接受發表[13]；在語者分群研究方面則發
表兩篇期刊論文 [16-17]。相關的實驗都是在NIST speaker recognition evaluation 
database、XM2VTS multi-modal database、ISCSLP2006 speaker recognition evaluation 
database等國際標準語料庫上進行。系統整合方面，我們完成結合語音辨識與語者辨識
的身分確認系統及結合語者辨識及人臉辨識的多模式身分確認系統。 
 
 
六、參考文獻 
 
[1]  Yi-Hsiang Chao, Hsin-Min Wang, and Ruei-chuan Chang, “GMM-Based Bhattacharyya Kernel 
Fisher Discriminant Analysis For Speaker Recognition,” in Proc. IEEE Int. Conf. Acoustics, 
Speech, Signal processing (ICASSP2005), March 2005. 
[2]  Yi-Hsiang Chao, Wei-Ho Tsai, Hsin-Min Wang, and Ruei-Chuan Chang, “A Kernel-based 
Discrimination Framework for Solving Hypothesis Testing Problems with Application to Speaker 
Verification,” in Proc. Int. Conf. on Pattern Recognition (ICPR2006), Aug 2006. 
[3]  Yi-Hsiang Chao, Wei-Ho Tsai, Hsin-Min Wang, and Ruei-Chuan Chang, “Improving the 
Characterization of the Alternative Hypothesis via Kernel Discriminant Analysis for Likelihood 
Ratio-based Speaker Verification,” in Proc. Int. Conf. on Spoken Language Processing 
(ICSLP2006), Sept 2006. 
[4]  Yi-Hsiang Chao, Hsin-Min Wang, and Ruei-chuan Chang, “A Novel Alternative Hypothesis 
Characterization Using Kernel Classifiers for LLR-based Speaker Verification,” in Proc. Int. 
Symposium on Chinese Spoken Language Processing (ISCSLP), Dec 2006. 
[5]  Yi-Hsiang Chao, Wei-Ho Tsai, Hsin-Min Wang, and Ruei-chuan Chang, “Improved Methods for 
Characterizing the Alternative Hypothesis Using Minimum Verification Error Training for 
LLR-based Speaker Verification,” in Proc. IEEE Int. Conf. Acoustics, Speech, Signal processing 
(ICASSP2007), April 2007. 
[6]  Yi-Hsiang Chao, Wei-Ho Tsai, Shih-Sian Cheng, Hsin-Min Wang, and Ruei-chuan Chang, 
“Evolutionary Minimum Verification Error Learning of the Alternative Hypothesis Model for 
LLR-based Speaker Verification,” in Proc. INTERSPEECH, Aug 2007. 
[7]  Wei-Ho Tsai, Shih-Sian Cheng, Yi-Hsiang Chao, and Hsin-Min Wang, “Clustering speech 
utterances by speaker using eigenvoice-motivated vector space model,” in Proc. IEEE Int. Conf. 
Acoustics, Speech, Signal processing (ICASSP2005), March 2005. 
[8]  Wei-Ho Tsai and Hsin-Min Wang, “Speaker Clustering of Unknown Utterances Based on 
Maximum Purity Estimation,” in Proc. European Conf. on Speech Communication and 
Technology (Eurospeech2005), Sept 2005. 
[9]  Wei-Ho Tsai and Hsin-Min Wang, “On maximizing the within-cluster homogeneity of speaker 
voice characteristics for speech utterance clustering,” in Proc. IEEE Int. Conf. Acoustics, Speech, 
Signal Processing (ICASSP2006), May 2006. 
[10]  Wei-Ho Tsai and Hsin-Min Wang, “Speaker Clustering Based on Minimum Rand Index,” in Proc. 
IEEE Int. Conf. Acoustics, Speech, Signal processing (ICASSP2007), April 2007. 
[11]  Hsien-Ting Cheng, Yi-Hsiang Chao, Shih-Liang Yeh, Chu-Song Chen, Hsin-Min Wang, and 
Yi-Ping Hung, “An efficient approach to multi-modal person identity verification by fusing face 
and voice information,” in Proc. IEEE Conf. on Multimedia and Expo (ICME2005), July 2005. 
[12]  Ping-Han Lee, Lu-Jong Chu, Yi-Ping Hung, Sheng-Wen Shih, Chu-Song Chen, and Hsin-Min 
Wang, “Cascading Multimodal Verification Using Face, Voice and Iris Information,” in Proc. 
IEEE Conf. on Multimedia and Expo (ICME2007), July 2007. 
[13]  Yi-Hsiang Chao, Hsin-Min Wang, and Ruei-Chuan Chang, “A Novel Characterization of the 
Alternative Hypothesis Using Kernel Discriminant Analysis for LLR-based Speaker 
Verification,” accepted by International Journal of Computational Linguistics and Chinese 
Language Processing. 
[14]  Yi-Hsiang Chao, Wei-Ho Tsai, Hsin-Min Wang, and Ruei-Chuan Chang, “Using Kernel 
GMM-BASED BHATTACHARYYA KERNEL FISHER DISCRIMINANT ANALYSIS FOR 
SPEAKER RECOGNITION 
 
Yi-Hsiang Chao1,2, Hsin-Min Wang1 and Ruei-Chuan Chang1,2 
 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer and Information Science, National Chiao Tung University, Hsinchu, Taiwan 
{yschao, whm}@iis.sinica.edu.tw, rc@cc.nctu.edu.tw 
 
ABSTRACT 
 
Clearly, the linear discriminant classifier is not robust enough to 
cope with most real-world data classification problems. Kernel 
Fisher Discriminant Analysis (KFDA) tries to increase the 
expressiveness of the discriminant based on the high order 
statistics of the data set. In this paper, we propose the 
GMM-based KFDA with the Bhattacharyya kernel to obtain a 
transformation, or called a speaker eigenspace, based on which 
the transformed MFCC features are more discriminative for 
speaker recognition. In our approach, the eigenspace is directly 
constructed from the complete GMM parameter set, rather than 
the supervectors considering mean vectors only as the eigenvoice 
approach. Moreover, FDA, which is believed to be more 
appropriate for classification accuracy than Principal Component 
Analysis (PCA), is applied for eigenspace construction. The 
speaker identification experiments show that the new features 
outperform the MFCC features, in particular when the amount of 
enrollment data for each speaker is very small. 
 
1. INTRODUCTION 
 
Speech recognition and speaker recognition are two different 
things. The objective in speech recognition is to minimize the 
inter-speaker variation while maximizing the intra-speaker 
variation among acoustic units, but vice versa in speaker 
recognition. Therefore, these two tasks should better use 
different signal traits as input. However, the most widespread 
feature parameters used to date in both tasks are Mel-Frequency 
Cepstral Coefficient (MFCC) features, which were originally 
designed to fulfill the demand of speech recognition. Though the 
MFCC-based Gaussian Mixture Model (GMM) [1] has been 
applied to speaker recognition in recent years, this approach 
performs well only when a large amount of enrollment data for 
each client speaker is available. In other words, in the training 
phase, the distribution of MFCC features from each client 
speaker should be wide enough to cover all possible 
pronunciations, in particular when the speaker recognition is 
conducted under the text-independent mode. In theory, speaker 
characteristics should be invariant to the size of enrollment data 
and different pronunciations of the same speaker. It is crucial to 
develop more reliable features that magnify the inter-speaker 
variation while reducing the intra-speaker variation for speaker 
recognition. 
Fisher Discriminant Analysis (FDA) [2] has been applied to 
feature transformation in many pattern classification problems. 
This technique is used to seek directions that maximize the 
between-class scatter while minimizing the within-class scatter. 
However, for most real-world data (e.g., speech frames) the 
linear discriminant is not complex enough. Therefore, Kernel 
Fisher Discriminant Analysis (KFDA) [3] tries to increase the 
expressiveness of the discriminant based on the high order 
statistics of the data set. 
In this paper, we want to find a speaker space that can better 
discriminate the speakers from each other. We propose the 
GMM-based KFDA with the Bhattacharyya kernel (BKFDA) to 
obtain a transformation, or called a speaker eigenspace, based on 
which the transformed MFCC features are more discriminative 
for speaker recognition. The rest of this paper is organized as 
follows: FDA and KFDA are briefly introduced in Section 2. The 
GMM-based BKFDA is presented in Section 3. Then, the 
application of GMM-based BKFDA to speaker identification is 
described in Section 4. Finally, the experimental results are 
discussed in Section 5, and concluding remarks are made in 
Section 6. 
 
2. FISHER DISCRIMINANT ANALYSIS WITH 
KERNELS 
 
2.1. Fisher Discriminant Analysis (FDA) 
 
Suppose that there are C classes and each class i has ni 
d-dimensional data samples, . We want to find a 
linear transformation matrix W for the original data samples 
such that the following Fisher’s criterion function J(W) is 
maximized, 
},..,{ 1
i
n
i
i ixxX =
,)J(
WSW
WSW
W
w
T
b
T
=       (1) 
where Sb and Sw are, respectively, the between-class and the 
within-class scatter matrices defined as follows, 
( )( ,
1
∑ −−=
=
C
i
T
iiib n mmmmS )    (2) 
.))((
1 1
∑ ∑ −−=
= =
C
i
n
j
T
i
i
ji
i
jw
i
mxmxS    (3) 
m and mi are, respectively, the overall sample mean vector and 
the sample mean vector of the ith class computed by, 
,1
1
∑=
=
C
i
iinn
mm     (4) 
,1
1
∑=
=
in
j
i
j
i
i n
xm     (5) 
4. APPLICATION TO SPEAKER RECOGNITION 
 
4.1. Eigenspace Construction 
 
Suppose that there are C training speakers (classes) and each 
speaker has his/her well-trained GMM. In the eigenvoice 
approach [7], the mean vectors of each speaker’s GMM are 
concatenated to form a supervector first. A speaker eigenspace is 
then constructed by performing PCA on these C supervectors. 
Considering the fact that PCA seeks directions that are efficient 
for representation whereas FDA seeks directions that are 
efficient for discrimination [8], for the speaker recognition task, 
it is believed that FDA is more appropriate for classification 
accuracy than PCA. However, it is impossible to calculate the 
within-class scatter matrix when FDA is performed on the above 
supervectors because each speaker (class) only has one 
supervector. The proposed GMM-based BKFDA is more 
appropriate for eigenspace construction than the standard 
eigenvoice approach because the eigenspace is directly 
constructed from the complete GMM parameter set rather than 
the supervectors considering mean vectors only.  
We first pool all training speakers’ data to train a Universal 
Background Model (UBM) [9] with R mixture components gr ,  
r=1,…,R ,  i . e . ,  =∪∪ CXX K1 ),...,1 ,,,(~ RrpG rrr =ΣµX . 
Since the UBM is a large GMM covering the distribution of all 
possible pronunciations from all speakers, we can let the vector 
wk lie in the span of R mixture components of UBM in F, i.e., 
. )(
1
∑ Φ=
=
R
r
rrrkk p gw α    (20) 
We then apply the Bayesian adaptation [10] to train the speaker 
GMMs from the UBM using the speaker specific training data. 
Since all speaker GMMs are adapted from the UBM, they have 
similar intra-speaker variation structures. The situation fits the 
requirement of making good use of FDA that all classes have 
similar within-class scatter structures. Let , 
same as Eq.(11), we need to maximize J(α
RRkk
T
k ×= 11 ],,[ αα Kα
k). Here, the matrices 
M and N are defined as, 
( )( ) ,
1
00 PηηηηPM 

 ∑ −−=
=
C
i
T
ii   (21) 
.   (22) )(
1
PKppPKPN 

 ∑ −=
=
C
i
T
i
T
iiii
iη is a R×1 vector with and is a 
R×1 vector with , K
∑ ⋅= =Rr irjirji kp1 ),()( ggη
∑= =Ci jiC 1 )()/1( η
)iq
0η
j0 )(η
,( pk gg
i is a R×R matrix 
with , P and P)( pqiK =
ip
i are diagonal matrices of 
size R×R whose rth diagonal elements are pr and pri, respectively, 
and is a R×1 vector whose rth element is pri. Again, the 
maximization problem can be solved by finding the leading 
eigenvectors of N-1M, as described in Section 2.2. 
 
4.2. Feature Transformation 
 
In the eigenvoice [7] or eigen-MLLR [11] approaches, the 
coordinate in the speaker eigenspace can be found and used to 
construct a model for a speaker based on a small amount of 
enrollment data. In the extreme case, the coordinate with respect 
to each speech frame can be obtained and regarded as a new 
feature (i.e., the so-called EMC features in [12]). The similar 
idea can be applied here. However, we can not obtain the 
projection of the feature vector x by computing ( )( )xw Φ⋅= kky , 
Kk ,...,1= , because the speaker eigenspace is constructed by 
models instead of features. To apply the Bhattacharyya kernel for 
feature transformation, we need to extend the feature vector x to 
a Gaussian . We can use the adaptation trick in the 
extreme case, which adapts a reference model from UBM with a 
single feature vector. Because UBM represents a distribution 
over a large space, a single feature vector will be close to only 
few mixture components of UBM. The likelihood values can be 
approximated well using only few best scoring mixture 
components [9]. For simplicity purpose, we choose the best 
mixture component for a feature vector x in adaptation, set the 
relevance factors to heavy emphasis this feature vector in the 
Maximum a Posteriori (MAP) formulation [9], and keep the 
covariance matrix unchanged, i.e., , where r* is 
the mixture component that has the maximal likelihood with 
respect to x. Therefore, the projection of a feature vector x can 
be expressed as the projection of g(x) onto w
),(~)( Σµxg N
)*rΣ,(~)( N xxg
k, i.e., ( )( )
( ) ( )( ) ( .)(,)(     
 )(
11
xggxgg
xgw
rr
R
r
rkr
R
r
rrk
kk
kpp
y
∑=Φ⋅Φ∑= )
Φ⋅=
==
αα  (23) 
In this way, we can obtain a K-dimensional new feature vector yT 
=[y1,…,yK] from a feature vector x using the GMM-based 
BKFDA. If C training speakers are available for eigenspace 
construction, K must be less or equal to C-1 because N-1M has at 
most C-1 eigenvectors. However, since the feature vector 
dimension d is usually much less than C, we can obtain at most d 
rather than C-1 eigenvectors when applying FDA in eigenspace 
construction. This is why FDA is widely used in reducing the 
dimension of feature vectors. In the proposed GMM-based 
BKFDA approach, the dimension of the new feature vector can 
be higher than that of the original feature vector. 
 
4.3. GMM-based Speaker Identification 
 
In this study, we apply the GMM-based BKFDA in 
GMM-based speaker identification. In the training phase, the 
MFCC features of each client speaker’s enrollment data are first 
transformed into the BKFDA features, and then used to train the 
speaker GMM. In the test phase, the BKFDA features 
transformed from the MFCC features of test utterances are used 
for speaker identification evaluation. 
 
5. EXPERIMENTS 
 
5.1. Experimental Setup 
 
The NIST 2001 cellular speaker recognition evaluation database 
[13] was used in the following experiments. We divided this 
database (including the development data and the evaluation data) 
into two subsets. The first subset consists of 90 female and 84 
male speakers. It was used to train the UBM first. Then, the 174 
speaker GMMs were adapted from the UBM using the speaker 
specific training data, respectively. Finally, the 174 speaker 
GMMs were applied in eigenspace construction. The second 
subset consisting of the remaining 22 females and 28 males was 
used for speaker identification evaluation. Each speaker has 
about 2 minutes of training data and 10 test segments on average. 
A Kernel-based Discrimination Framework for Solving Hypothesis Testing 
Problems with Application to Speaker Verification 
 
 
Yi-Hsiang Chao1,2, Wei-Ho Tsai3, Hsin-Min Wang1, and Ruei-Chuan Chang1,2 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Dept. of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
3 Dept. of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan 
E-mail: {yschao, whm}@iis.sinica.edu.tw, whtsai@en.ntut.edu.tw, rc@cc.nctu.edu.tw 
 
 
Abstract 
 
Real-word applications often involve a binary 
hypothesis testing problem with one of the two 
hypotheses ill-defined and hard to be characterized 
precisely by a single measure. In this paper, we 
develop a framework that integrates multiple 
hypothesis testing measures into a unified decision 
basis, and apply kernel-based classification techniques, 
namely, Kernel Fisher Discriminant (KFD) and 
Support Vector Machine (SVM), to optimize the 
integration. Experiments conducted on speaker 
verification demonstrate the superiority of our 
approaches over the predominant approaches. 
 
1. Introduction 
 
In many practical applications, one may be faced 
with the problem of making a binary decision, such as 
“yes/no” or “accept/reject”, with respect to an 
uncertain hypothesis which is known only through its 
observable consequences. Under a statistical 
framework, the problem is generally formulated as a 
test between a null hypothesis H0 and an alternative 
hypothesis H1 regarding some measurement L(⋅) on a 
given observation X: 
, )(  :  
 )(  : 
1
0
θ
θ
<
≥
XLH
XLH
   (1) 
where θ is the decision threshold. Depending on the 
applications, a number of measurements have been 
investigated, with the Likelihood Ratio (LR) measure 
in conjunction with parametric modeling being the 
most popular. Specifically, each of the hypotheses is 
represented by a set of probability-related parameters 
through a training process, and the probability of 
generating a given observation is then evaluated for 
each of the hypotheses’ parameter sets.  
However, in most applications, the alternative 
hypothesis is usually ill-defined and hard to be 
characterized precisely. One example is the problem of 
speaker verification, which aims to determine if a 
speaker is who he or she claims to be. Though the null 
hypothesis can be modeled straightforwardly using 
speech utterances from the speaker claimed by the test 
user, the alternative hypothesis does not involve any 
specific speaker, and thus lacks explicit data to model. 
Many approaches have thus been proposed in attempts 
to characterize the alternative hypothesis effectively 
and robustly, but none of them has been proven 
optimal. The pros and cons of the individual 
approaches motivate us to try to develop a framework 
that integrates multiple LR measures into a unified 
decision basis. To enable a reliable integration, this 
study formulates the hypothesis test as a problem of 
non-linear discrimination and applies kernel-based 
techniques, namely, Kernel Fisher Discriminant (KFD) 
[6] and Support Vector Machine (SVM) [7], to 
optimally separate the LR samples of the null 
hypothesis from those of the alternative hypothesis. 
 
2. Hypothesis testing measures 
 
From a speaker-verification point of view and 
without loss of generality, LR measure for a hypothesis 
testing problem comes in many choices. One simple 
approach [2] is to pool all speech data from a great 
amount of speakers, generally irrelevant to the clients, 
and train a single speaker-independent model Ω, 
named the world model. During a test, the possibility 
of an unknown utterance U being produced by the 
claimed speaker can be evaluated by 
),|(log)λ|(log)(1 Ω−= UpUpUL               (2) 
bias b is actually the decision threshold θ in Eq. (6), 
which can be determined through the trade-off between 
false acceptance and false rejection. 
 
3.2. Support Vector Machine (SVM) 
 
Alternatively, Eq. (7) can be designed with SVM, in 
analogy to a fusion classifier proposed in [8][9]. The 
goal of SVM is to seek a separating hyperplane in the 
feature space F that maximizes the margin between 
classes. Following [7], w is expressed as, 
,)(
1
∑=
=
l
j
jjjy xw Φα                      (10) 
which yields 
                 ,),()(
1
bky
l
j
jjj +∑=
=
xxx αΨ               (11) 
where each training sample xj belongs to one of the 
two classes identified by the label yj∈{−1,1}, j=1, 2,…, 
l. The coefficients αj and b can be solved using the 
quadratic programming techniques [11]. Note that αj is 
non-zero for a few support vectors, and is zero 
otherwise. A number of kernel functions exist, with the 
dot product kernel function, i.e., k(x, xj) = xjTx, being 
the simplest, and the Radial Basis Function (RBF) 
kernel function, i.e., k(x, xj) = exp(− ||x − xj ||2 / 2σ2), 
being the most popular, where σ  is a tunable parameter. 
The SVM with a dot product kernel function is known 
as Linear SVM. 
 
4. Experiments 
 
4.1. Experimental setup 
 
The proposed methods were examined via speaker-
verification experiments conducted on speech data 
extracted from the XM2VTSDB multi-modal database 
[12]. In accordance with “Configuration II” described 
in [12], the database was divided into three subsets: 
“Training”, “Evaluation”, and “Test”. In our 
experiments, “Training” was used to build the 
individual client’s model, while “Evaluation” was used 
to optimize w and b. Then, the performance of speaker 
verification was evaluated on “Test”. As shown in 
Table 1, a total of 293 speakers1 in the database were 
divided into 199 clients, 25 “evaluation impostors”, 
and 69 “test impostors”. Each speaker involved 4 
recording sessions taken at approximately one-month 
intervals, and each recording session consisted of 2 
shots. In a shot, every speaker was prompted to utter 3 
sentences “0 1 2 3 4 5 6 7 8 9”, “5 0 6 9 2 8 1 3 7 4”, 
and “Joe took father’s green shoe bench out”.  
                                                        
1  We discarded 2 speakers (ID numbers 313 and 342) 
because of partial data corruption. 
We used 12 (3×2×2) utterances/speaker from 
sessions 1 and 2 to train the individual client’s model. 
For each client, the other 198 clients’ utterances from 
sessions 1 and 2 were used to generate the world model 
or cohort models. Then, we used 6 utterances/client 
from session 3, along with 24 (3×4×2) 
utterances/evaluation-impostor to optimize w and b. In 
the performance evaluation, we tested 6 
utterances/client in session 4 and 24 utterances/test-
impostor, which gave 1,194 (6×199) client trials and 
329,544 (24×69×199) impostor trials. Each utterance, 
sampled at 32 kHz, was converted into a stream of 24-
order feature vectors, each consisting of 12 Mel-scale 
cepstral coefficients and their first time derivatives, by 
a 32-ms Hamming-windowed frame with 10-ms shifts. 
 
Table 1. Configuration of the speech database. 
Session Shot 199 clients 25 impostors 69 impostors
1 1 2 
1 2 2 
Training 
1 3 2 Evaluation 
1 4 2 Test 
Evaluation Test 
 
4.2. Experimental results 
 
The LR measures, L1(U), L2(U), L3(U), and L4(U), 
served as our baseline systems for performance 
comparison, in which each speaker model was 
represented by a Gaussian Mixture Model (GMM) [1] 
with 64 mixture components, while the world model 
was a GMM with 256 mixture components. We 
implemented a combined-LR system via FLD, Linear 
SVM, SVM, and KFD, respectively, where SVM and 
KFD used an RBF kernel function with σ=0.1. 1,194 
(6×199) client examples and 119,400 (24×25×199) 
impostor examples from “Evaluation” were used to 
optimize w and b. However, recognizing the fact that a 
kernel-based method can be intractable when a huge 
amount of training examples involves, we downsized 
the number of impostor examples from 119,400 to 
2,250 using a uniform random selection. 
Fig. 1 shows the results of speaker verification 
conducted on “Evaluation” with DET curves [10], 
obtained equivalently by adjusting the decision 
threshold, i.e., b or θ. Though this experiment was an 
inside test for our proposed framework, it can be 
observed that SVM and KFD perform better than FLD 
and Linear SVM. To verify the superiority of the 
combined-LR systems over the baseline systems, 
experiments were next conducted on “Test”. The 
Improving the Characterization of the Alternative Hypothesis via Kernel 
Discriminant Analysis for Likelihood Ratio-based Speaker Verification 
Yi-Hsiang Chao1,2, Wei-Ho Tsai3, Hsin-Min Wang1 and Ruei-Chuan Chang1,2 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
3 Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan 
{yschao,whm}@iis.sinica.edu.tw, whtsai@en.ntut.edu.tw, rc@cc.nctu.edu.tw 
 
Abstract 
The performance of a likelihood ratio-based speaker verification 
system is highly dependent on modeling of the target speaker’s 
voice (the null hypothesis) and characterization of non-target 
speakers’ voices (the alternative hypothesis). To better 
characterize the ill-defined alternative hypothesis, this study 
proposes a new likelihood ratio measure based on a composite-
structure Gaussian mixture model, the so-called GMM2. 
Motivated by the combined use of a variety of background 
models to represent the alternative hypothesis, GMM2 is 
designed with an inner set of mixture weights connected to the 
significance of each individual Gaussian density, and an outer 
set of mixture weights connected to the significance of each 
individual background model. Through the use of kernel 
discriminant analysis namely, Kernel Fisher Discriminant (KFD) 
or Support Vector Machine (SVM), GMM2 is trained in such a 
manner that the utterances of the null hypothesis can be 
optimally separated from those of the alternative hypothesis. 
Index Terms: speaker verification, likelihood ratio, kernel 
Fisher discriminant, support vector machine 
1. Introduction 
In essence, speaker verification is a hypothesis testing problem 
that is commonly solved by using a likelihood ratio (LR) test [1]. 
Given an input utterance U, the goal is to determine whether or 
not U was spoken by the target (hypothesized) speaker. 
Consider the following two hypotheses: 
.speaker target  thefromnot  is    :  
       speaker, target  thefrom is    : 
1
0
UH
UH
                  (1) 
The LR test can be expressed as 


<
≥=
, )reject  ( accept   
                    accept   
  
)|(
)|()(
01
0
1
0
HH
H
HUp
HUpUL θ
θ
          (2) 
where ,1 ,0  ),|( =iHUp i  is the likelihood of hypothesis Hi 
given the utterance U, and θ  is a threshold. H0 and H1 are 
called the null hypothesis and the alternative hypothesis, 
respectively. Mathematically, H0 and H1 can be characterized 
by some parametric models, such as λ  and λ , respectively; λ  
is often called an anti-model. Though H0 can be modeled 
straightforwardly using speech utterances from the target 
speaker, H1 does not involve any specific speaker, and thus 
lacks explicit data for modeling. The approaches that have been 
proposed to better characterize H1 can be collectively expressed 
in the following form [2]: 
)),|( ),...,|(()|( 1 NUpUpUp λλλ Ψ=                     (3) 
where Ψ() is a function of the likelihoods computed for a set of 
background models {λ1, λ2,…, λN}. For example, the 
background model set can be obtained from N representative 
speakers, called a cohort set [8], which simulates potential 
impostors. If Ψ() is an average function [1], the LR is computed 
using 
.)λ|(1log)λ|(log)(
1
1 


 ∑−=
=
N
i
iUpN
UpUL              (4) 
Alternatively, the average function can be replaced by 
various functions, such as the maximum [3] and the geometric 
mean [4]. A special case arises when N = 1, in which a single 
background model is usually trained by pooling all the available 
data from a large number of speakers. This is called the world 
model [2]. The LR in this case becomes 
),|(log)λ|(log)(2 Ω−= UpUpUL                        (5) 
where Ω denotes the world model. 
However, none of the LR measures developed so far has 
proved to be absolutely superior to the others, since the 
selection of Ψ() is usually application and training data 
dependent. In particular, the use of a simple function, such as 
the average, maximum, or geometric mean, is a heuristic that 
does not involve any optimization process. Thus, the resulting 
system is far from optimal in terms of verification accuracy. To 
better handle this problem, in this study, we formulate Ψ() as a 
combination of the likelihoods computed for all the background 
models. The combination is then optimized using kernel 
discriminant analysis such that the samples of the null 
hypothesis can be optimally separated from those of the 
alternative hypothesis.  
The remainder of this paper is organized as follows. Section 
2 presents the problem formulation of our approach for speaker 
verification. Section 3 introduces kernel discriminant analysis 
used in this work. Section 4 presents our experiment results. 
Finally, in Section 5, we present our conclusions. 
2. Problem formulation 
Our objective is to design a function ()Ψ  that best combines N 
background models according to their individual significance to 
the classifier. The combination is assumed to be of the form: 
,)|())|( ),...,|(()|(
1
1 ∑=Ψ= =
N
i
iiN UpwUpUpUp λλλλ   (6) 
The inner and outer mixture weights of GMM2 are 
estimated via the EM algorithm and the kernel discriminant 
analysis, respectively. That is to say, the GMM2 integrates the 
Bayesian learning and discriminative training algorithms. The 
objective is to optimize the classifier by considering the null 
hypothesis and the alternative hypothesis jointly. 
4. Experiments 
4.1. Experimental setup 
The speaker verification experiments were conducted on speech 
data extracted from the XM2VTSDB multi-modal database [12]. 
In accordance with “Configuration II” described in [12], the 
database was divided into three subsets: “Training”, 
“Evaluation”, and “Test”. In our experiments, we used 
“Training” to build the individual client’s model and anti-model, 
and “Evaluation” to optimize w and b. The performance of 
speaker verification was then evaluated on the “Test” subset. As 
shown in Table 1, a total of 293 speakers1 in the database were 
divided into 199 clients, 25 “evaluation impostors”, and 69 “test 
impostors”. Each speaker participated in 4 recording sessions at 
approximately one-month intervals, and each recording session 
consisted of 2 shots. In a shot, every speaker was prompted to 
utter 3 sentences “0 1 2 3 4 5 6 7 8 9”, “5 0 6 9 2 8 1 3 7 4”, and 
“Joe took father’s green shoe bench out”. Each utterance, 
sampled at 32 kHz, was converted into a stream of 24-order 
feature vectors, each consisting of 12 Mel-scale cepstral 
coefficients [5] and their first time derivatives, by a 32-ms 
Hamming-windowed frame with 10-ms shifts. 
 
Table 1. Configuration of the speech database. 
Session Shot 199 clients 25 impostors 69 impostors
1 1 2 
1 2 2 
Training 
1 3 2 Evaluation 
1 4 2 Test 
Evaluation Test 
 
We used 12 (2×2×3) utterances/speaker from sessions 1 and 
2 to train the individual client’s model, represented by a GMM 
with 64 mixture components. For each client, the other 198 
clients’ utterances from sessions 1 and 2 were used to generate 
the world model, represented by a GMM with 256 mixture 
components; B speakers were chosen from these 198 clients as 
the cohort. Then, we used 6 utterances/client from session 3, 
along with 24 (4×2×3) utterances/evaluation-impostor, which 
yielded 1,194 (6×199) client examples and 119,400 (24×25×199) 
impostor examples, to optimize w and b. However, recognizing 
the fact that a kernel-based classifier can be intractable when a 
huge amount of training examples involves, we downsized the 
                                                                
 
1 We discarded 2 speakers (ID numbers 313 and 342) because of 
partial data corruption. 
number of impostor examples from 119,400 to 2,250 using a 
uniform random selection method. In the performance 
evaluation, we tested 6 utterances/client in session 4 and 24 
utterances/test-impostor, which produced 1,194 (6×199) client 
trials and 329,544 (24×69×199) impostor trials. 
4.2. Background model selection 
We used B+1 background models, consisting of one world 
model and B cohort set models, to form the characteristic vector 
x in Eq. (8), and B cohort set models to form L1(U) in Eq. (4). 
Two cohort selection methods [1] were applied in this 
experiment. One selected the B closest speakers for each client, 
and the other selected the B/2 closest speakers plus the B/2 
farthest speakers for each client. The selection was based on the 
speaker distance measure [1], computed by 
,
)|(
)|(
log
)|(
)|(
log),(
ij
jj
ji
ii
ji Xp
Xp
Xp
Xp
d λ
λ
λ
λλλ +=              (15) 
where iλ  and jλ  were speaker models trained using the i-th 
speaker’s utterances iX  and the j-th speaker’s utterances jX , 
respectively. Two cohort selection methods yielded the 
following two (B+1)×1 characteristic vectors: 
,)](~  ...  )(~  )(~[ 10
Tc
B
c UpUpUp=x                          (16) 
and 
,)](~  ...  )(~  )(~  ...  )(~  )(~[ 2/12/10
Tf
B
fc
B
c UpUpUpUpUp=x  (17) 
where )|(/)|()(~0 λUpUpUp Ω= , )|(/)|()(~ closest   λλ UpUpUp ici = , 
),|(/)|()(~  farthest λλ UpUpUp ifi =  i = 1,…, B for Eq. (16), and i 
= 1,…, B/2 for Eq. (17). iclosest   λ  and i farthest λ  are the i-th 
closest model and the i-th farthest model of the client model λ , 
respectively. In the experiments, B was set to 20.  
4.3. Experimental results 
We implemented the proposed LR system via KFD with Eq. (16) 
(curve “KFD_w_20c”), KFD with Eq. (17) (curve 
“KFD_w_10c_10f”), SVM with Eq. (16) (curve 
“SVM_w_20c”), and SVM with Eq. (17) (curve 
“SVM_w_10c_10f”), respectively. Both SVM and KFD used an 
ERBF kernel function with σ= 5. For performance comparison, 
three systems, L1(U) with 20 closest cohort models (curve 
“L1_20c”), L1(U) with 10 closest cohort models plus 10 farthest 
cohort models (curve “L1_10c_10f”), and L2(U) were used as 
our baselines. 
Fig. 1 shows the results of speaker verification conducted 
on “Evaluation” with DET curves [13], obtained equivalently 
by adjusting the decision threshold, i.e., θ or b. Though this 
experiment was an inside test for our proposed LR system, it is 
clear that KFD performs better than SVM. To verify that the 
proposed LR systems are superior to the baseline systems, 
experiments were conducted on “Test”. The results, as shown in 
Fig. 2, confirm that both the proposed LR systems, SVM and 
KFD, outperform the baseline systems. We can also see from 
Fig. 2 that the performances of SVM and KFD are similar, but 
this is not the case in Fig. 1. We speculate that the KFD 
classifier might have over-learned the training examples. In 
addition, it can be seen that there is no significant difference in 
A Novel Alternative Hypothesis Characterization Using 
Kernel Classifiers for LLR-based Speaker Verification 
Yi-Hsiang Chao1,2, Hsin-Min Wang1, and Ruei-Chuan Chang1,2 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
{yschao, whm}@iis.sinica.edu.tw, rc@cc.nctu.edu.tw 
Abstract. In a log-likelihood ratio (LLR)-based speaker verification system, the 
alternative hypothesis is usually ill-defined and hard to characterize a priori, 
since it should cover the space of all possible impostors. In this paper, we pro-
pose a new LLR measure in an attempt to characterize the alternative hypothe-
sis in a more effective and robust way than conventional methods. This LLR 
measure can be further formulated as a non-linear discriminant classifier and 
solved by kernel-based techniques, such as the Kernel Fisher Discriminant 
(KFD) and Support Vector Machine (SVM). The results of experiments on two 
speaker verification tasks show that the proposed methods outperform classical 
LLR-based approaches. 
Keywords: Speaker verification, Log-likelihood ratio, Kernel Fisher Discrimi-
nant, Support Vector Machine. 
1   Introduction 
In essence, the speaker verification task is a hypothesis testing problem. Given an in-
put utterance U, the goal is to determine whether U was spoken by the hypothesized 
speaker or not. The log-likelihood ratio (LLR)-based [1] detector is one of the 
state-of-the-art approaches for speaker verification. Consider the following hypothe-
ses: 
H0: U is from the hypothesized speaker,  
H1: U is not from the hypothesized speaker. 
The LLR test is expressed as 
⎩⎨
⎧
<
≥=
, )reject  i.e., ( accept     
                          accept    
 
)|(
)|(
log)(
01
0
1
0
HH
H
HUp
HUp
UL θ
θ
 (1) 
where ,1 ,0  ),|( =iHUp i  is the likelihood of the hypothesis Hi given the utterance 
U, and θ  is the threshold. H0 and H1 are, respectively, called the null hypothesis and 
the alternative hypothesis. Mathematically, H0 and H1 can be represented by paramet-
ric models denoted as λ  and λ , respectively; λ  is often called an anti-model. 
Though H0 can be modeled straightforwardly using speech utterances from the hy-
pothesized speaker, H1 does not involve any specific speaker, and thus lacks explicit 
 3
database show that the proposed methods outperform classical LLR-based ap-
proaches. 
The remainder of this paper is organized as follows. Section 2 describes the analy-
sis of the alternative hypothesis in our approach. Sections 3 and 4 introduce the kernel 
classifiers used in this work and the formation of the characteristic vector by back-
ground model selection, respectively. Section 5 contains our experiment results. Fi-
nally, in Section 6, we present our conclusions. 
2   Analysis of the Alternative Hypothesis 
First of all, we redesign the function Ψ(⋅) in Eq. (2) as 
,))|(...)|()|(()( ).../(121 2121 NNNUpUpUp
αααααα λλλ +++⋅⋅⋅=Ψ u  (7) 
where TNUpUpUp )]|( ),...,|(),|([ 21 λλλ=u  is an N × 1 vector and iα  is the 
weight of the likelihood p(U | λi), i = 1,2,…, N. This function gives N background 
models different weights according to their individual contribution to the alternative 
hypothesis. It is clear that Eq. (7) is equivalent to a geometric mean function when 
1=iα , i = 1,2,…, N. If some background model λi contrasts with an input utterance 
U, the likelihood p(U | λi) may be extremely small, and thus cause the geometric 
mean to approximate zero. In contrast, by assigning a favorable weight to each back-
ground model, the function Ψ(⋅) defined in Eq. (7) may be less affected by any spe-
cific background model with an extremely small likelihood. Therefore, the resulting 
score for the alternative hypothesis obtained by Eq. (7) will be more robust and reli-
able than that obtained by a geometric mean function. It is also clear that Eq. (7) will 
reduce to a maximum function when 1* =iα , )λ|(log* maxarg 1 iNi Upi ≤≤= ; and 
0=iα , *ii ≠∀ . 
By substituting Eq. (7) into Eq. (2) and letting )...(/ 21 Niiw αααα +++= , i = 
1,2,…, N, we obtain 
⎩⎨
⎧
<
≥=
+++=
⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛⋅⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛=
⋅⋅⋅=
 reject,   
accept  
            
)|(
)|(log...
)|(
)|(log
)|(
)|(log          
)|(
)|(...
)|(
)|(
)|(
)|(log          
)|(...)|()|(
)|(log)(
2
2
1
1
21
21
21
21
θ
θ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λ
λλλ
λ
xwT
N
N
w
N
ww
w
N
ww
Up
Upw
Up
Upw
Up
Upw
Up
Up
Up
Up
Up
Up
UpUpUp
UpUL
N
N
 (8) 
where TNwww ] ..., ,[ 21=w  is an N×1 weight vector and x is an N × 1 vector in the 
space RN, expressed by 
 5
3.1   Kernel Fisher Discriminant (KFD) 
Suppose the i-th class has ni data samples, },..,{ 1
i
n
i
i ixxX = , i = 1, 2. The goal of the 
KFD is to find a direction w in the feature space F such that the following Fisher’s 
criterion function J(w) is maximized: 
,)(
wSw
wSw
w Φ
Φ
=
w
T
b
T
J  (12) 
where ΦbS and ΦwS  are, respectively, the between-class scatter matrix and the 
within-class scatter matrix defined as 
T
b ))(( 2121
ΦΦΦΦΦ −−= mmmmS  (13) 
and 
,))()()((
2,1  
∑ ∑ −Φ−Φ=
= ∈
ΦΦΦ
i
T
iiw
iXx
mxmxS  (14) 
where ∑ =Φ Φ= ins isii n 1 )()/1( xm , and i = 1, 2, is the mean vector of the i-th class in F. 
Let },..,,{ 2121 lxxxXX =∪  and 21 nnl += . Since the solution of w must lie in the 
span of all training data samples mapped in F [6], w can be expressed as 
.)(
1
∑ Φ=
=
l
j
jj xw α  (15) 
Let αT = [α1, α2,…, αl]. Accordingly, Eq. (11) can be re-written as 
.),()(
1
bkf
l
j
jj +∑= = xxx α  (16) 
Our goal therefore changes from finding w to finding α, which maximizes 
,)(
Nαα
Mααα T
T
J =  (17) 
where M and N are computed by 
T))(( 2121 ηηηηM −−=  (18) 
and 
,)(
2,1
∑ −=
=i
T
inni ii K1IKN  (19) 
respectively, where iη  is an l×1 vector with ∑ == ins isjiji kn 1 ),()/1()( xxη , Ki is an l
×ni matrix with ),()( isjjsi k xxK = , Ini is an ni×ni identity matrix, and 1ni is an ni×ni 
matrix with all entries equal to 1/ni. Following [6], the solution for α, which maxi-
mizes J(α) defined in Eq. (17), is the leading eigenvector of N-1M. 
 7
where iλ  and jλ  are speaker models trained using the i-th speaker’s utterances 
iX  and the j-th speaker’s utterances jX , respectively. Two cohort selection meth-
ods yield the following two (B+1)×1 characteristic vectors: 
Tc
B
c UpUpUp )](~  ...  )(~  )(~[ 10=x  (25) 
and 
,)](~  ...  )(~  )(~  ...  )(~  )(~[ 2/12/10
Tf
B
fc
B
c UpUpUpUpUp=x  (26) 
where )|(/)|(log)(~0 Ω= UpUpUp λ , )|(/)|(log)(~ closest   ici UpUpUp λλ= , and 
)|(/)|(log)(~  farthest  i
f
i UpUpUp λλ= . iclosest   λ  and i farthest λ  are the i-th closest 
model and the i-th farthest model of the client model λ , respectively. 
5   Experiments 
We evaluate the proposed approaches on two databases: the XM2VTSDB database 
[11] and the ISCSLP2006 speaker recognition evaluation (ISCSLP2006-SRE) data-
base [12].  
For the performance evaluation, we adopt the Detection Error Tradeoff (DET) 
curve [13]. In addition, the NIST Detection Cost Function (DCF) [14], which reflects 
the performance at a single operating point on the DET curve, is also used. The DCF 
is defined as 
)1( TargetFalseAlarmFalseAlarmTargetMissMissDET PPCPPCC −××+××= , (27) 
where MissP  and FalseAlarmP  are the miss probability and the false-alarm probabil-
ity, respectively, MissC  and FalseAlarmC  are the respective relative costs of detection 
errors, and TargetP  is the a priori probability of the specific target speaker. A special 
case of the DCF is known as the Half Total Error Rate (HTER), where MissC  and 
FalseAlarmC  are both equal to 1, and TargetP = 0.5, i.e., 2/)(HTER FalseAlarmMiss PP += . 
5.1   Evaluation on the XM2VTSDB Database 
The first set of speaker verification experiments was conducted on speech data ex-
tracted from the XM2VTSDB multi-modal database [11]. In accordance with “Con-
figuration II” described in [11], the database was divided into three subsets: “Train-
ing”, “Evaluation”, and “Test”. In our experiments, we used the “Training” subset to 
build the individual client’s model and the world model, and the “Evaluation” subset 
to estimate the decision threshold θ  in Eq. (1) and the parameters w and b in Eq. 
(11). The performance of speaker verification was then evaluated on the “Test” sub-
 9
(“L1_20c”), 2) L1(U) with the 10 closest cohort models plus the 10 farthest cohort 
models (“L1_10c_10f”), 3) L2(U) with the 20 closest cohort models (“L2_20c”), 4) 
L3(U) with the 20 closest cohort models (“L3_20c”), and 5) L4(U) (“L4”). 
Fig. 1 shows the results of the baseline systems tested on the “Evaluation” subset 
in DET curves [13]. We observe that the curves “L1_10c_10f” and “L4” are better 
than the others. Thus, in the second experiment, we focused on the performance im-
provements of our proposed LLR systems over these two baselines. 
 
 
Fig. 1. Baselines: DET curves for the XM2VTSDB “Evaluation” subset. 
Fig. 2 shows the results of our proposed LLR systems versus the baseline systems 
evaluated on the “Test” subset. It is clear that the proposed LLR systems, including 
KFD and SVM, outperform the baseline LLR systems, while KFD performs better 
than SVM. 
An analysis of the results based on the HTER is given in Table 2. For each ap-
proach, the decision threshold, θ  or b, was used to minimize the HTER on the 
“Evaluation” subset, and then applied to the “Test” subset. From Table 2, we observe 
that, for the “Test” subset, a 30.68% relative improvement was achieved by 
“KFD_w_20c”, compared to “L1_10c_10f” – the best baseline system. 
5.2   Evaluation on the ISCSLP2006-SRE Database 
We participated in the text-independent speaker verification task of the ISCSLP2006 
Speaker Recognition Evaluation (SRE) plan [12]. The database, which was provided 
by Chinese Corpus Consortium (CCC) [15], contained 800 clients. The length of the 
training data for each client ranged from 21 seconds to 1 minute and 25 seconds; the 
average length was approximately 37.06 seconds.  
 11
According to the evaluation plan, the ratio of true clients to imposters in the “Test” 
subset should be approximately 1:20. Therefore, we applied the 900 client samples 
and 18,000 randomly selected impostor samples to estimate the decision threshold, θ  
or b. The “Test” data consisted of 5,933 utterances. 
The signal processing front-end was same as that applied in the XM2VTSDB task. 
5.2.1    Experiment Results 
Fig. 3 shows the results of the proposed LLR system using KFD with Eq. (26) and B 
= 100 (“KFD_w_50c_50f”) versus the baseline GMM-UBM [2] system tested on 
5,933 “Test” utterances in DET curves. The proposed LLR system clearly outper-
forms the baseline GMM-UBM system. According to the ISCSLP2006 SRE plan, the 
performance is measured by the NIST DCF with 10=MissC , 1=FalseAlarmC , and 
05.0=TargetP . In each system, the decision threshold, θ  or b, was selected to mini-
mize the DCF on the “Evaluation” data, and then applied to the “Test” data. The 
minimum DCFs for the “Evaluation” data and the associated DCFs for the “Test” data 
are given in Table 3. We observe that “KFD_w_50c_50f” achieved a 34.08% relative 
improvement over “GMM-UBM”. 
 
 
Fig. 3. DET curves for the ISCSLP2006-SRE “Test” subset. 
Table 3.  DCFs for “Evaluation” and “Test” subsets (The ISCSLP2006-SRE task). 
 min DCF for “Evaluation” DCF for “Test” 
GMM-UBM 0.0129 0.0179 
KFD_w_50c_50f 0.0067 0.0118 
IMPROVED METHODS FOR CHARACTERIZING THE ALTERNATIVE HYPOTHESIS 
USING MINIMUM VERIFICATION ERROR TRAINING  
FOR LLR-BASED SPEAKER VERIFICATION 
 
Yi-Hsiang Chao1,2, Wei-Ho Tsai3, Hsin-Min Wang1 and Ruei-Chuan Chang1,2 
 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
3 Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan 
{yschao,whm}@iis.sinica.edu.tw, whtsai@en.ntut.edu.tw, rc@cc.nctu.edu.tw 
 
ABSTRACT 
 
Speaker verification based on the log-likelihood ratio (LLR) is 
essentially a task of modeling and testing two hypotheses: the null 
hypothesis and the alternative hypothesis. Since the alternative 
hypothesis involves unknown imposters, it is usually hard to 
characterize a priori. In this paper, we propose a framework to 
better characterize the alternative hypothesis with the goal of 
optimally separating client speakers from imposters. The proposed 
framework is built on either a weighted arithmetic combination or 
a weighted geometric combination of useful information extracted 
from a set of pre-trained anti-speaker models. The parameters 
associated with the combinations are then optimized using 
Minimum Verification Error training such that both the false 
acceptance probability and the false rejection probability are 
minimized. Our experiment results show that the proposed 
framework outperforms conventional LLR-based approaches. 
 
Index Terms— Speaker recognition, minimization methods, 
hypothesis testing, minimum verification error. 
 
1. INTRODUCTION 
 
Speaker verification is usually formulated as a statistical 
hypothesis testing problem and solved using a log-likelihood ratio 
(LLR) test [1]. Given an input utterance U, the LLR test for 
determining whether or not U is spoken by the hypothesized 
speaker is performed as follows 


<
≥=
, )reject  ( accept   
                    accept   
  
)|(
)|(
log)(
01
0
1
0
HH
H
HUp
HUp
UL θ
θ
      (1) 
where H0 represents that U is spoken by the hypothesized speaker 
(called the null hypothesis); H1 represents that U is not spoken by 
the hypothesized speaker (called the alternative hypothesis); 
)|( iHUp , i = 0 or 1, is the likelihood of hypothesis Hi given 
utterance U; and θ  is a decision threshold. In practical 
implementations, H0 and H1 are usually characterized by some 
parametric models, such as Gaussian mixture models (GMMs) [1]. 
However, even though H0 can be modeled straightforwardly using 
speech utterances from the hypothesized speaker, H1 does not 
involve any specific speaker, and hence lacks explicit data for 
modeling. Thus, a number of approaches have been proposed to 
better characterize H1. The common strategy is to generate one or 
multiple models using speech from a large number of non-
hypothesized speakers, and then compute the likelihood P(U | H1) 
using [2]: ( ),)λ|( )...,λ|(),λ|()|( 211 NUpUpUpHUp Ψ=          (2) 
where Ψ(⋅) denotes a certain function of the likelihoods computed 
for a set of background models {λ1, λ2,…, λN} representing the 
potential imposters. For example, if Ψ(⋅) is an arithmetic mean [1], 
the LLR is of the form 
,)λ|(1log)λ|(log)(
1
1 


 ∑−=
=
N
i
iUpN
UpUL             (3) 
where λ denotes a model generated for the hypothesized speaker. 
Alternatively, the arithmetic mean can be replaced by a maximum 
function [4], which yields the LLR  
),λ|(logmax)λ|(log)(
1
2 i
Ni
UpUpUL
≤≤
−=                (4) 
or by a geometric mean [5], which yields the LLR 
.)λ|(log1)λ|(log)(
1
3 ∑−= =
N
i
iUpN
UpUL                (5) 
A special case arises when N = 1, where a single background 
model is usually trained by pooling all the available data; this is 
called a world model [2]. The LLR in this case becomes 
),|(log)λ|(log)(4 Ω−= UpUpUL                         (6) 
where Ω denotes the world model. 
However, there is no theoretical evidence to indicate which 
method of characterizing H1 is optimal, and the selection of Ψ(⋅) is 
usually application and training data dependent. In particular, a 
simple function, such as the arithmetic mean, the maximum, or the 
geometric mean, is a heuristic that does not involve an 
optimization process. Thus, the resulting system is far from 
optimal in terms of verification accuracy. To better handle this 
problem, we propose a framework that characterizes the alternative 
hypothesis by exploiting information available from background 
models, such that utterances from the imposters and the 
hypothesized speaker can be separated more effectively. The 
framework is built on either a weighted geometric combination or 
a weighted arithmetic combination of the likelihoods computed for 
background models. In contrast to the geometric mean L3(U) or the 
arithmetic mean L1(U), which are independent of the system 
training, our combination scheme treats the background models 
unequally according to how close each individual is to the 
hypothesized speaker model, and quantifies the unequal nature of 
the background models by a set of weights optimized in the 
training phase. The optimization is carried out by Minimum 
If WAC is used, then 
.
)λ|(
)λ|(
)λ|(log ∑
−=


∑∂
∂−=∂
∂
j
jj
i
j
jj
ii Upw
Up
Upw
ww
L        (15) 
If WGC is used, then 
).λ|(log)λ|(log i
j
jj
ii
UpUpw
ww
L −=


∑∂
∂−=∂
∂        (16) 
The threshold θ in Eq. (9) can be estimated using [7]: 
,
)()1(
θηθθ ∂
∂−=+ UDkk                                 (17) 
where 
[ ]
[ ]. ))((1))((1                
))((1))((1              
)(
1
0
1
1
0
0
1
1
0
0
∑ −−⋅⋅−
∑ −−−⋅⋅=
∂
∂⋅∂
∂⋅∂
∂⋅∂
∂+∂
∂⋅∂
∂⋅∂
∂⋅∂
∂=∂
∂
∈
∈
HU
HU
ULsULsa
N
x
ULsULsa
N
x
L
L
d
d
s
s
xL
L
d
d
s
s
xUD θθθ
ll
          
(18)
 
In our implementation, the overall expected loss is set 
according to the Detection Cost Function (DCF) [8]: 
),1(            arg
arg
etTFalseAlarmFalseAlarm
etTMissMissDET
PPC
PPCC
−××+
××=
            (19) 
where PMiss is the miss (false rejection) probability, PFalseAlarm is the 
false alarm (false acceptance) probability, PTarget is the a priori 
probability of the target (hypothesized) speaker, and CMiss and 
CFalseAlarm are the relative costs of the missed error and false alarm 
error, respectively. A special case of DCF is known as the Half 
Total Error Rate (HTER), where CMiss and CFalseAlarm are both equal 
to 1, and PTarget = 0.5, i.e., HTER = (PMiss + PFalseAlarm) / 2. Then, 
approximating PMiss and PFalseAlarm by l0(U) and l1(U), respectively, 
we set the overall expected loss specifically as
 
).1()(             
)()(
arg1
arg0
etTFalseAlarm
etTMiss
PUC
PUCUD
−××+
××=
l
l
       (20) 
 
4. EXPERIMENTS 
 
4.1. Experiment setup 
 
We conducted speaker-verification experiments on speech data 
extracted from the XM2VTSDB multi-modal database [10]. In 
accordance with “Configuration II” described in [10], the database 
was divided into three subsets: “Training”, “Evaluation”, and 
“Test”. We used “Training” to build each client model and the 
background models, and used “Evaluation” to optimize the weights 
wi in Eq. (7) or Eq. (8), along with the threshold θ. Then, the 
speaker verification performance was evaluated on “Test”. As 
shown in Table 1, a total of 293 speakers1 in the database were 
divided into 199 clients, 25 “evaluation impostors”, and 69 “test 
impostors”. Each speaker participated in 4 recording sessions at 
about one-month intervals, and each recording session consisted of 
2 shots. In each shot, the speaker was prompted to utter 3 
sentences “0 1 2 3 4 5 6 7 8 9”, “5 0 6 9 2 8 1 3 7 4”, and “Joe took 
                                                 
1 We discarded 2 speakers (ID numbers 313 and 342) because of 
partial data corruption. 
father’s green shoe bench out”. Each utterance, sampled at 32 kHz, 
was converted into a stream of 24-order feature vectors, each 
consisting of 12 Mel-scale frequency cepstral coefficients [11] and 
their first time derivatives, by a 32-ms Hamming-windowed frame 
with 10-ms shifts. 
Table 1. Configuration of the speech database. 
Session Shot 199 clients 25 impostors 69 impostors
1 1 2 
1 2 2 
Training 
1 3 2 Evaluation
1 4 2 Test 
Evaluation Test 
 
We used 12 (2×2×3) utterances/speaker from sessions 1 and 2 
to train each client model, represented by a GMM with 64 mixture 
components. For each client, the other 198 clients’ utterances from 
sessions 1 and 2 were used to generate the world model, 
represented by a GMM with 256 mixture components. Meanwhile, 
B speakers were chosen from these 198 clients as the cohort [3] to 
yield B background models. Then, to optimize the weights, wi, and 
the threshold, θ, we used 6 utterances/client from session 3, along 
with 24 (4×2×3) utterances/evaluation-impostor over the four 
sessions, which yielded 1,194 (6×199) client samples and 119,400 
(24×25×199) impostor samples. In the performance evaluation, we 
tested 6 utterances/client in session 4 and 24 utterances/test-
impostor over the four sessions, which involved 1,194 (6×199) 
client trials and 329,544 (24×69×199) impostor trials. 
In addition, we used the B cohort set of models for L1(U) in 
Eq. (3), L2(U) in Eq. (4), and L3(U) in Eq. (5), and B+1 
background models, consisting of the B cohort set of models and 
one world model for our WAC and WGC methods. B was 
empirically set to 20. Two cohort selection methods [1] were used. 
One selected the closest B speakers for each client; and the other 
selected the closest B/2 speakers, plus the farthest B/2 speakers for 
each client. Here, the degree of closeness is measured in terms of 
the pairwise distance defined by [1]: 
,
)λ|(
)λ|(
log
)λ|(
)λ|(
log)λ,λ(
ij
jj
ji
ii
ji Xp
Xp
Xp
Xp
d +=               (21) 
where λi and λj are speaker models trained using the i-th speaker’s 
utterances Xi and the j-th speaker’s utterances Xj, respectively.  
 
4.2. Experiment results 
 
The proposed weighted combination methods were implemented in 
three ways: 1) WAC with the world model and the 10 closest 
cohort models, plus the 10 farthest cohort models 
(“WAC_w_10c_10f”); 2) WAC with the world model plus the 20 
closest cohort models (“WAC_w_20c”); and 3) WGC with the 
world model plus the 20 closest cohort models (“WGC_w_20c”). 
The MVE training for both WAC and WGC was initialized with an 
equal weight, wi, and the threshold θ was set to 0. The overall 
expected loss function D in Eq. (20) was set according to the 
HTER with CMiss = 1, CFalseAlarm = 1, and PTarget = 0.5. 
For the performance comparison, we used five systems as our 
baselines: 1) L1(U) with the 10 closest cohort models plus the 10 
Evolutionary Minimum Verification Error Learning of the Alternative 
Hypothesis Model for LLR-based Speaker Verification 
Yi-Hsiang Chao1,2, Wei-Ho Tsai3, Shih-Sian Cheng1,2, Hsin-Min Wang1, and Ruei-Chuan Chang1,2 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
3 Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan 
{yschao,sscheng,whm}@iis.sinica.edu.tw, whtsai@en.ntut.edu.tw, rc@cc.nctu.edu.tw 
 
Abstract 
It is usually difficult to characterize the alternative hypothesis 
precisely in a log-likelihood ratio (LLR)-based speaker 
verification system. In a previous work, we proposed using a 
weighted arithmetic combination (WAC) or a weighted 
geometric combination (WGC) of the likelihoods of the 
background models instead of heuristic combinations, such as 
the arithmetic mean and the geometric mean, to better 
characterize the alternative hypothesis. In this paper, we 
further propose learning the parameters associated with WAC 
or WGC via an evolutionary minimum verification error 
(MVE) training method, such that both the false acceptance 
probability and the false rejection probability can be 
minimized. Our experiment results show that the proposed 
methods outperform conventional LLR-based approaches. 
Index Terms: genetic algorithm, log-likelihood ratio, 
minimum verification error training, speaker verification 
1. Introduction 
The log-likelihood ratio (LLR) measure [1] is used in many 
speaker verification systems. Given an input utterance, U, the 
hypothesis test based on the LLR measure to determine 
whether or not U is spoken by the hypothesized speaker is 
expressed as 


<
≥−−=
, accept      0
 accept     0
 )|(log)λ|(log)(
1
0
H
H
UpUpUL θλ  (1) 
where H0 (the null hypothesis) represents that U is spoken by 
the hypothesized speaker; H1 (the alternative hypothesis) 
represents that U is not spoken by the hypothesized speaker; θ  
is a decision threshold; λ is the hypothesized speaker model; 
and λ  is the so-called anti-model or alternative hypothesis 
model. The λ  model is usually ill-defined because, ideally, it 
should cover the space of all possible impostors. Many 
approaches have thus been proposed to characterize the λ  
model. One simple approach pools the speech data from a 
large number of background speakers, and trains a single 
speaker-independent model λ0, called the world model or the 
Universal Background Model (UBM) [2]. The LLR measure 
in this case then becomes  
θ−−= )λ|(log)λ|(log)( 01 UpUpUL .             (2) 
Instead of using a single model to simulate potential 
impostors, a set of background models {λ1, λ2,…, λB} can be 
trained using speech from several representative speakers, 
called a cohort [3]. This leads to the following possible LLR 
measures, where the alternative hypothesis can be 
characterized by: 
(i) the likelihood of the most competitive cohort model [4], 
i.e., 
θ−−=
≤≤
)λ|(logmax)λ|(log)(
1
2 i
Bi
UpUpUL ;    (3) 
(ii) the arithmetic mean of the likelihoods of the B cohort 
models [1], i.e., 
θ−


 ∑−=
=
B
i
iUpB
UpUL
1
3 )λ|(
1log)λ|(log)( ;    (4) 
(iii) the geometric mean of the likelihoods of the B cohort 
models [4], i.e., 
θ−


∑−=
=
B
i
iUpB
UpUL
1
4 )λ|(log
1)λ|(log)( .     (5) 
Obviously, the LLR measures L2(U), L3(U), and L4(U) are 
derived by heuristic combination methods that do not include 
an optimization process. Thus, the resulting system is far 
from optimal in terms of verification accuracy.  
A more effective and robust LLR measure can be 
obtained by characterizing the alternative hypothesis as a 
weighted arithmetic combination (WAC) or a weighted 
geometric combination (WGC) of the likelihoods of the 
background models, instead of the above heuristic 
combinations [5]. The new combination scheme treats the 
background models unequally according to how close each 
individual is to the hypothesized speaker model, and 
quantifies the unequal nature of the background models by a 
set of weights optimized in the training phase. The 
optimization is performed by the minimum verification error 
(MVE) training method [5], which minimizes both the false 
acceptance probability and the false rejection probability.  
Traditionally, MVE training has been realized by the 
gradient descent algorithm [5-7]; however, the algorithm only 
guarantees to converge to a local optimum. In this paper, we 
propose a new evolutionary MVE training method for 
learning the weights of the WAC- or WGC-based LLR 
measure. We embed the MVE training in a genetic algorithm 
(GA) [8], which is a widely used optimization algorithm that 
usually converges to a near global optimum. To do this, we 
incorporate a new mutation operator, called the one-step 
gradient descent operator (GDO), into the genetic algorithm.  
The remainder of the paper is organized as follows. 
Section 2 presents the WAC- and WGC-based LLR measures 
and their relations to conventional LLR measures. In Section 
3, we describe how we embed MVE training in the genetic 
algorithm. Section 4 details the experiment results. Finally, in 
Section 5, we present our conclusions. 
2. WAC- and WGC-based LLR measures 
We briefly review the WAC- and WGC-based LLR measures 
in this section. In the WAC-based LLR measure, the 
If WGC is used, then 
).λ|(log)λ|(log
0
i
B
j
jj
ii
UpUpw
ww
L −=


 ∑∂
∂−=∂
∂
=
 (16) 
The one-step gradient descent operator (GDO) for the 
threshold θ is defined as 
,θηθθ ∂
∂−= Doldnew                                 (17) 
where newθ  and oldθ  are, respectively, the threshold θ in a 
chromosome after and before mutation; η is the step size; and 
θ∂
∂D is computed by 
[ ]
[ ]. ))((1))((1                
))((1))((1              
1
0
1
1
0
0
1
1
0
0
∑ −⋅⋅−
∑ −−−⋅⋅=
∂
∂⋅∂
∂⋅∂
∂⋅∂
∂+∂
∂⋅∂
∂⋅∂
∂⋅∂
∂=∂
∂
∈
∈
HU
HU
ULsULsa
N
x
ULsULsa
N
x
L
L
d
d
s
s
xL
L
d
d
s
s
xD θθθ
ll
        
(18) 
5) Survivor selection: We adopt the generational model [8], 
in which the whole population is replaced by its offspring. 
4. Experiments 
4.1. Experiment setup 
We conducted speaker-verification experiments on speech 
data extracted from the XM2VTSDB multi-modal database 
[11]. In accordance with “Configuration II” described in [11], 
the database was divided into three subsets: “Training”, 
“Evaluation”, and “Test”. We used “Training” to build each 
client model and the background models, and “Evaluation” to 
optimize the weights wi in Eq. (6) or Eq. (7), along with the 
threshold θ. Then, the speaker verification performance was 
evaluated on “Test”. As shown in Table 1, a total of 293 
speakers1 in the database were divided into 199 clients, 25 
“evaluation impostors”, and 69 “test impostors”. Each 
speaker participated in 4 recording sessions at about one-
month intervals, and each recording session consisted of 2 
shots. In each shot, the speaker was prompted to utter 3 
sentences “0 1 2 3 4 5 6 7 8 9”, “5 0 6 9 2 8 1 3 7 4”, and “Joe 
took father’s green shoe bench out”. Each utterance, sampled 
at 32 kHz, was converted into a stream of 24-order feature 
vectors, each consisting of 12 Mel-scale frequency cepstral 
coefficients [12] and their first time derivatives, by a 32-ms 
Hamming-windowed frame with 10-ms shifts. 
We used all the clients’ utterances from sessions 1 and 2 
to train a world model (UBM), represented by a Gaussian 
mixture model (GMM) [1] with 512 mixture components. To 
implement L1(U), for each client, we used 12 (2×2×3) 
utterances/client from sessions 1 and 2 to generate the client 
model, represented by a GMM with 512 mixture components, 
through UBM-MAP adaptation [2]. To implement the other 
LLR measures, for each client, we used 12 (2×2×3) 
utterances/client from sessions 1 and 2 to generate the client 
model, represented by a GMM with 64 mixture components, 
by using the expectation-maximization (EM) algorithm [12]. 
For each client, the B closest speakers were chosen from the 
                                                                
 
1 We omitted 2 speakers (ID numbers 313 and 342) because 
of partial data corruption. 
other 198 clients as the cohort [3] according to the degree of 
closeness measured in terms of the pairwise distance defined 
by [1]: 
,
)λ|(
)λ|(
log
)λ|(
)λ|(
log)λ,λ(
ij
jj
ji
ii
ji Up
Up
Up
Up
d +=            (19) 
where λi and λj are speaker models trained using the i-th 
speaker’s utterances Ui and the j-th speaker’s utterances Uj, 
respectively. In the experiments, B was set to 20, and each 
cohort model was represented by a GMM with 64 mixture 
components 
Table 1. Configuration of the speech database. 
Session Shot 199 clients 25 impostors 69 impostors
1 1 2 
1 2 2 
Training 
1 3 2 Evaluation
1 4 2 Test 
Evaluation Test 
 
To optimize the weights, wi, and the threshold, θ, we used 
6 utterances/client from session 3, along with 24 (4×2×3) 
utterances/evaluation-impostor over the four sessions, which 
yielded 1,194 (6×199) client samples and 119,400 
(24×25×199) impostor samples. To speed up the MVE 
training process, only 2,250 imposter samples randomly 
selected from 119,400 such samples were used. In the 
performance evaluation, we tested 6 utterances/client in 
session 4 and 24 utterances/test-impostor over the four 
sessions, which involved 1,194 (6×199) client trials and 
329,544 (24×69×199) impostor trials. We use the Detection 
Error Tradeoff (DET) curve [13] for the performance 
evaluation. In addition, we use the Half Total Error Rate 
(HTER), which reflects the performance at a single operating 
point on the DET curve. The HTER is defined as  
HTER = (PMiss + PFalseAlarm) / 2,                  (20) 
where PMiss is the miss (false rejection) probability and 
PFalseAlarm is the false alarm (false acceptance) probability. It 
is clear that the loss functions l0 and l1 in Eq. (10) will 
approximate PMiss and PFalseAlarm, respectively, if the scalar a 
in the sigmoid function is set to a sufficiently large value. In 
our experiments, a was set to 10, and x0 and x1 in the fitness 
function D in Eq. (9) were set to 0.5. Thus, the minimization 
of the fitness function D in Eq. (9) is equivalent to the 
minimization of the HTER. 
4.2. Experiment results 
We employed the proposed evolutionary MVE training 
methods in two LLR measures: 1) WAC with the world 
model plus the 20 closest cohort models 
(“WAC_GA_w_20c”); and 2) WGC with the world model 
plus the 20 closest cohort models (“WGC_GA_w_20c”). The 
population size of the GA was set to 100, and the crossover 
probability pc was set to 0.5. We also implemented MVE 
training using the gradient descent algorithm in two LLR 
measures: 1) WAC with the world model plus the 20 closest 
cohort models (“WAC_GD_w_20c”); and 2) WGC with the 
world model plus the 20 closest cohort models 
(“WGC_GD_w_20c”). 
For the performance comparison, we used five systems as 
our baselines: 1) L1(U), using a 512-mixture client GMM 
CLUSTERING SPEECH UTTERANCES BY SPEAKER USING  
EIGENVOICE-MOTIVATED VECTOR SPACE MODELS 
 
Wei-Ho Tsai, Shih-Sian Cheng, Yi-Hsiang Chao, and Hsin-Min Wang 
Institute of Information Science, Academia Sinica, Taipei, Taiwan, Republic of China 
{wesley,sscheng,yschao,whm}@iis.sinica.edu.tw 
 
ABSTRACT 
 
This study investigates the problem of automatically grouping 
unknown speech utterances based on their associated speakers. 
The proposed method utilizes the vector space model, which was 
originally developed in document-retrieval research, to 
characterize each utterance as a tf-idf-based vector of acoustic 
terms, thereby deriving a reliable measurement of similarity 
between utterances. To define the required acoustic terms that 
are most representative in terms of voice characteristics, the 
eigenvoice approach is applied on the utterances to be clustered, 
which creates a set of eigenvector-based terms. To further 
improve speaker-clustering performance, the proposed method 
encompasses a mechanism of blind relevance feedback for 
refining the inter-utterance similarity measure. 
 
1. INTRODUCTION 
 
For more than two decades, automatic recognition of speaker 
based on vocal characteristics has received tremendous attention 
in facilitating human-machine communications and biometric 
applications. As more recently speech starts being exploited as 
an information source, the utility of recognizing speakers’ voices 
are increasingly in demand in indexing and archiving the 
mushrooming amount of spoken data available. Traditional 
approaches to speaker recognition assume that some prior 
information or speech data are available from the speakers of 
concerned, while for the task of indexing or archiving, the basic 
strategy needs to be expanded to distinguish speakers in an 
unsupervised manner. As a result of this need, clustering speech 
utterances by speaker has emerged as a new challenging 
research problem [1-7], and the solutions to this problem are 
requiring to be further explored. 
To date, most of the speaker-clustering methods in 
existence can amount to a hierarchical clustering framework [1-
6]. This framework consists of three major components: a 
computation of inter-utterance similarity, a generation of cluster 
tree in either a bottom-up or top-down fashion according to 
some criteria on the similarity measure, and a determination of 
the number of clusters based on some termination conditions. 
Among the three components, the computation of inter-utterance 
similarity is of particular importance, because it crucially 
determines whether the generated clusters are related to speaker 
rather than other acoustic classes. Various methods based on 
cross likelihood ratio [3], generalized likelihood ratio [2], and 
Bayesian information criterion [4], etc., have been studied with 
the aim to produce larger values for similarities between 
utterances of the same speaker and smaller values for similarities 
between utterances of different speakers. However, since these 
similarity measures are performed entirely on the spectrum-
based features, which are known to carry various information 
besides the speaker voice characteristics, such as phonetic and 
environmental conditions, the resultant clustering system might 
be vulnerable when the utterances addressed are short and noisy. 
In our prior work reported in [5], we show that a better similarity 
computation can be carried out on a reference space trained to 
cover the generic voice characteristics inherently in all of the 
utterances to be clustered. Because of incorporating out-of-pair 
information into the similarity computation for every pair of 
utterances, the clustering can be more robust against the 
interference from non-speaker factors. 
As an extension of our prior work [5], this study further 
improves the speaker-clustering performance by primarily 
addressing one potential problem ignored in our prior work that 
the reference space is composed of intertwining voice 
characteristics rather than the most representative and 
statistically-independent ones. It is assumed that if the vocal 
characteristics of all the utterances to be clustered can be 
summarized as a set of the most representative and statistically-
independent elements, utterances from the same and different 
speakers may be better distinguished by examining utterances 
with these elements. This idea is implemented by a means 
analogous to eigenvoice [8], which applies eigen decomposition 
on the parameters of models trained from a number of speakers. 
In addition, to further exploit various useful information for 
inter-utterance similarity computation, we re-formulate the 
speaker-clustering problem from a perspective of document 
retrieval. As will be shown below, some related concepts in 
document retrieval, such as relevance feedback, can be very 
useful as well for speaker clustering. 
 
2. PROBLEM FORMULATION 
 
Let X1, X2, …, XN denote N isolated speech utterances in a 
certain spectrum-based feature representation, each of which 
was produced by one of the P speakers, where N ≥ P, and P is 
unknown. The aim of the speaker clustering is to partition the N 
utterances into M clusters such that M = P and each cluster 
consists exclusively of utterances from only one speaker. If 
viewed as a problem of document retrieval, the partitioning 
could be done with an objective that when any of the N 
utterances, say Xk, is chosen as an exemplar query to retrieve the 
relevant documents from the whole N-utterance set, the 
documents deemed most relevant in terms of same-speaker are 
the utterances within the cluster where Xk is located.  
Next, all the mean vectors of each utterance-dependent 
GMM are concatenated in the order of mixture index to form a 
super-vector, with dimension of D. Then, PCA is applied to the 
set of N super-vectors, V1, V2, …, VN, obtained from N 
utterance-dependent GMMs. This yields D eigenvectors, e1, 
e2, …, eD, ordered by the magnitude of their contribution to the 
between-utterance covariance matrix: 
,))((1
1
∑
=
′−−=
N
i
iiN
VVVVB    (3) 
where V  is the mean vector of all Vi for 1≤ i ≤ N. The D 
eigenvectors constitute an eigenspace, and each of the super-
vectors can be represented by a point on the eigenspace: 
,
1
,∑
=
+=
D
d
ddii eVV φ     (4) 
where φi,d, 1≤ d ≤ D, is the coordinate of Vi on the eigenspace.  
If each eigenvector is treated as an acoustic term, the 
importance of each term d, 1≤ d ≤ D, with respect to an utterance 
Xi can be characterized by tfid and idfd, computed using 
 ,ididtf φ=     (5) 
and 
 , 
1
1
)( 2 βσα +−+= deidfd
   (6) 
respectively, where σd is the standard deviation of φi,d for 1≤ i ≤ 
N, and α and β  are real constants for adjusting the sigmoid-
based idf. Eq. (6) essentially discounts the acoustic terms which 
reflect less variation between utterances and hence are 
considered with little discriminating power. 
 
4. BLIND RELEVANCE FEEDBACK 
 
In analogy with document retrieval, a further improvement for 
speaker clustering may be made by applying relevance feedback 
(RF) [12], which refines queries using information from the 
documents considered relevant by users (explicit RF) or by 
system itself (blind RF). A typical RF method in text document 
retrieval is to append words from found documents known to be 
relevant to the keyword string of query, and then repeat the 
retrieval process based on the new query. Its intuitive 
counterpart in our task may be carried out by concatenating one 
utterance with others deemed similar to that utterance, and re-
computing the similarity between the concatenated utterances. 
However, such an approach cannot control the amount of 
information appended from one utterance to another, and hence 
a severe propagation of error might happen whenever one 
utterance is concatenated with another different-speaker 
utterance. To apply RF more effectively, we propose to refine 
the tf-idf-based vectors of utterances, instead of using direct 
concatenation of utterances.  
The basic idea is that the tf-idf-based vectors of utterances 
from an identical speaker are supposed to resemble each other, 
and therefore these vectors may be further rectified via a 
weighted average of multiple vectors deemed similar such that 
they can be more close to each other. To this end, let R(i,k) 
denote the rank of inter-utterance similarity Su(Xi,Xk) among 
Su(Xi,X1), Su(Xi,X2), …, Su(Xi,XN) in descending order, where 1 
≤ R(i,k) ≤ N. A tf-idf-based vector of utterance Wi is rectified 
using 
, ˆ
1
1),(∑
=
−=
N
k
ki
kiR WW θ    (7) 
where θ is a constant smaller than one. Implicit in Eq. (7) is that 
the new vector of utterance is a weighted sum of highly-ranked 
utterances’ vectors. Using the rectified vectors, the inter-
utterance similarity can be refined before clustering is performed. 
 
5. EXPERIMENTAL RESULTS 
 
Speech data used in this study consisted of 197 utterances 
chosen from the test set of the 2001 NIST Speaker Recognition 
Evaluation Corpus [13]. The 197 utterances were spoken by 15 
male speakers, and the number of utterances spoken by each 
speaker ranged from 5 to 39. Speech features including 24 Mel-
scale frequency cepstral coefficients (MFCCs) were extracted 
from these data for every 20-ms Hamming-windowed frame 
with 10-ms frame shifts. 
Performance of the speaker clustering was evaluated on the 
basis of two metrics: cluster purity [2] and Rand Index [14]. The 
cluster purity, which indicates the extent of agreement in a 
cluster, is defined by  
,
2
1
∑
= ∗ ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛=
P
p m
mp
m n
nρ     (8) 
where ρm is the purity of the cluster cm, nm∗ is the total number of 
utterances in the cluster cm, nmp is the number of utterances in the 
cluster cm that are from speaker sp, and P is the total number of 
speakers involved. Eq. (8) follows that nm∗-1 ≤ ρk ≤ 1, in which 
the upper bound and lower bound reflect that all the within-
cluster utterances are from the same speaker or completely 
different speakers, respectively. To evaluate the overall 
performance of an M-clustering for N utterances, an average 
cluster purity is computed using 
.1
1
∑
=
∗=
M
m
mmnN
ρρ     (9) 
The Rand Index, which indicates the number of utterance 
pairs that are from the same speaker but are not grouped into the 
same cluster, and that are not from the same speaker but are 
grouped into the same cluster, is defined by 
,
2
1
2
1
1 1
2
1
2
1
2 ∑∑∑∑
= ==
∗
=
∗ −+=γ
M
m
P
p
mp
P
p
p
M
m
m nnn   (10) 
where n∗p is the number of utterances from speaker sp. The lower 
the index, the better the clustering performs. A perfect clustering 
should produce an index of zero. 
Fig. 2 shows the speaker-clustering results as a function of 
number of clusters. Here, “GLR” denotes the conventional 
hierarchical clustering method using the generalized likelihood 
ratio as an inter-utterance similarity measure [2]. “UU-GMM-
ADA” is the best method presented in our previous work [6], 
which can be considered as representing an acoustic term by an 
utterance-dependent GMM. “Eigenvoice” denotes the proposed 
VSM-based clustering method using the eigenvoice-motivated 
term representation.  The number of mixture components used in 
each of the utterance-dependent GMMs was empirically 
Speaker Clustering of Unknown Utterances Based on Maximum 
Purity Estimation 
Wei-Ho Tsai and Hsin-Min Wang 
Institute of Information Science, Academia Sinica, Taipei, Taiwan, Republic of China 
{wesley,whm}@iis.sinica.edu.tw 
 
Abstract 
This paper addresses the problem of automatically grouping 
unknown speech utterances that are from the same speaker. A 
clustering method based on maximum purity estimation is 
proposed, with the aim of maximizing the similarities of voice 
characteristics between utterances within all the clusters. This 
method employs a genetic algorithm to determine the cluster 
where each utterance should be located, which overcomes the 
limitation of conventional hierarchical clustering that the final 
result can only reach the local optimum. The proposed 
clustering method also incorporates a Bayesian information 
criterion to determine how many clusters should be created.   
1. Introduction 
Speaker clustering refers to the task of grouping unknown 
speech utterances together based on their associated speakers. 
With the burgeoning availability of digital audio material, 
speaker clustering is gaining importance as a means to index 
the voluminous spoken data accumulated daily for archival 
use [1]. It is hoped that by grouping same-speaker utterances 
into clusters, the human efforts required for indexing can be 
greatly reduced, from having to listen to each utterance to 
only having to check few utterances in each cluster. 
Currently, most speaker-clustering methods follow a 
hierarchical clustering (HC) framework [2-12], which 
computes the similarities of voice characteristics between 
utterances, and then sequentially merges the utterances 
deemed similar to each other (agglomerative clustering), or 
alternatively, separates the utterances deemed dissimilar to 
each other (divisive clustering). During the procedure of 
agglomeration or division, the nearest neighborhood selection 
rule is usually employed in an attempt to maximize the 
similarities between all the utterances in each cluster. 
However, since no consideration is taken in respect of the 
interaction between clusters, HC can only make each 
individual cluster as homogeneous as possible, but cannot 
guarantee that the homogeneity for all the clusters can be 
summed to reach a maximum. In particular, mis-clustering 
errors, arising from grouping different-speaker utterances 
together or segregating same-speaker utterances apart, can 
propagate down the whole process, and hence limit the 
clustering performance.  
To overcome the HC’s limitation, this study develops a 
new clustering method which aims to maximize the total 
number of within-cluster utterances from the same speakers. 
In distinction to HC, which performs optimization in a 
cluster-by-cluster manner, the proposed method searches for 
the best partitioning of utterances by considering all the 
clusters at the same time. This is done with the estimation of 
the so-called cluster purity [5], in conjunction with an 
optimization process based on a genetic algorithm [13] for 
attaining maximal cluster purity. 
2. Method Overview 
Our speaker-clustering system is designed with an aim to take 
as input N isolated utterances produced by P unknown 
speakers1, where P is also unknown, and to provide as output 
M clusters satisfying that M = P and each cluster consists 
exclusively of utterances from only one speaker. The 
clustering procedure begins with the computation of inter-
utterance similarities, followed by the determination of which 
utterances are similar enough to be grouped into a cluster. 
Since no information about the speaker population is available 
beforehand, the procedure also includes the estimation of the 
optimal number of clusters. 
The performance of speaker clustering is evaluated on the 
basis of cluster purity [5], defined by   
,
2
1
∑
= ⎟
⎟
⎠
⎞
⎜⎜⎝
⎛=
P
p m
mp
m n
nρ    (1) 
where ρm is the purity of the m-th cluster, nm is the total 
number of utterances in the m-th cluster, and nmp is the number 
of utterances in the m-th cluster that are produced by the p-th 
speaker. Eq. (1) follows that nm-1 ≤ ρm ≤ 1, in which the upper 
bound and lower bound reflect that all the within-cluster 
utterances are from the same speaker or completely different 
speakers, respectively. To evaluate the overall performance of 
M-clustering, we compute an average cluster purity: 
.1
1
∑
=
=
M
m
mmnN
ρρ    (2) 
3. Inter-utterance Similarity Computation 
The method for measuring the inter-utterance similarities is 
adapted from our previous work on “eigenvoice-motivated 
vector space” [12]. To begin, a Gaussian mixture model 
(GMM), which represents the generic characteristics of 
speakers’ voices, is created using the cepstral features of all 
the utterances to be clustered. This GMM is then adapted to 
model the individual voice characteristics of each utterance 
using maximum a posteriori estimation [14], and therefore N 
utterance-dependent GMMs λ1, λ2,…, λN are generated. 
Next, all the mean vectors of each utterance-dependent 
GMM are concatenated in the order of the mixture index to 
form a super-vector with dimension of D. Principal 
component analysis is then applied on the set of N super-
vectors, V1, V2, …, VN, obtained from λ1, λ2,…, λN. This 
yields D eigenvectors, e1, e2, …, eD, ordered by the degree of 
their contribution to the between-utterance covariance matrix: 
,))((1
1
∑
=
′−−=
N
i
iiN
VVVVB   (3) 
                                                          
1 Each utterance is assumed to be produced by only one speaker. 
incomplete. Clearly, the optimal number of clusters is equal 
to the speaker population, which is unknown and needs to be 
estimated.  
Our basic strategy for estimating the speaker population is 
to define a score for assessing a partitioning of the utterances 
based on how a large average purity can be achieved at the 
expense of increasing the number of clusters. This problem 
may be tackled from the standpoint of model selection. 
Specifically, if each of the possible partitionings with 
different numbers of clusters is considered as a model for 
characterizing the speaker information of the utterances, we 
choose the model that can produce the largest average purity 
and has the smallest number of clusters. Viewed in this 
manner, the Bayesian information criterion (BIC) [15], which 
is popular for solving model-selection problems, could be 
used to assess the clustering.  
The BIC scores a parametric model based on how well 
the model fits a data set, and how simple the model is:  
|,|log)(#  γ5.0)|Pr(log )(BIC OO Λ−Λ=Λ  (11) 
where #(Λ) denotes the number of free parameters in model Λ, 
|O| is the size of the data set O, and γ is a penalty factor. The 
larger the value of BIC(Λ), the better model Λ will perform. 
In another work on speaker clustering [6], BIC is applied to 
score a partitioning of an utterance collection, in which a 
cluster is represented by a uni-Gaussian density estimated 
from the feature vectors of the utterances, and the model Λ is 
a set of Gaussian densities. Since we convert each utterance 
from the feature vectors into a coordinate, our work differs 
from [6] by the way clusters are modeled, which is directly 
related to the clustering performance.  
Consider a model Λ, consisting of M parameters for 
classifying a set of N utterances from P unknown speakers. 
Each of the parameters represents an integer index to tag each 
of the utterances. The model is designed with such an aim 
that, by having all the utterances tagged, the utterances 
belonging to the same speakers are tagged with the same 
index. Thus, the likelihood Pr(O|Λ), which measures how 
well the model fits the data, is concerned with the probability 
that, given N indices h1, h2, …, hN for the N utterances, X1, 
X2, …, XN, the utterances tagged with the same indices come 
from the same speakers. Suppose that the true speakers of N 
utterances, o1, o2, …, oN, are statistically independent with 
each other. We compute the likelihood Pr(O|Λ) by  
[ ,))()(|)()(Pr(                
))(|)(Pr()|Pr(
 
1
N
N
i
ii
CCOO
ChOo
XXXX
XXO
==≈
===Λ ∏
=
&& ]
(12) 
where O(X) denotes the true speaker of an arbitrary utterance 
X tagged with index C(X), and Pr(oi = O(X)| hi = C(X))  
represents the probability that, given an arbitrary utterance X 
tagged with the same index as utterance Xi, the true speakers 
of utterances Xi and X are the same. For computational 
efficiency, all the probabilities Pr(oi = O(X)| hi = C(X)), 1≤ i 
≤ N, are approximated by , 
which is the probability that any two utterances X and 
tagged with the same index come from the same speaker. 
))()(|)()(Pr( XXXX CCOO == &&
X&
Assume that there are nm utterances tagged with m, and 
among these nm utterances there are nmp utterances from the  
p-th speaker. If we pick up one of the nm utterances twice at 
random, with replacement, the probability that both of the 
chosen utterances come from the p-th speaker is (n m p /n m ) × 
(n m p /n m ). Thus, the probability that two utterances tagged 
with m come from the same speaker is ΣPp=1(n m p /n m )2, which 
is also the cluster purity ρm defined in Eq. (1). Since the 
probability that one utterance tagged with m is n m /N, we can 
estimate the probability that any two utterances X and 
tagged with the same index come from the same speaker 
by  
X&
.))()(|)()(Pr(
1 1
2
ρ=⎥⎥⎦
⎤
⎢⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛=== ∑ ∑
= =
M
m
P
p m
mpm
n
n
N
nCCOO XXXX &&  (13) 
Approximating ρ  as ρ (H) in Eq. (10), the likelihood 
Pr(O|Λ) can be obtained with ρ (H)N. Accordingly, we can 
score a partitioning of N utterances having M clusters via 
.log  γ5.0)(log )Clustering-(BIC NMNM −= Hρ     (14) 
The BIC value should increase with the increase of the M 
value in the beginning, but will decline significantly after an 
excess of clusters is created. A reasonable number of clusters 
can, thus, be determined by 
. )Clustering-(BICmaxarg
2
* MM
NM ≤≤
=   (15) 
6. Experimental Results 
Our speech data was chosen from the test set of the 2001 
NIST Speaker Recognition Evaluation Corpus [16]. It 
consisted of 197 utterances spoken by 15 male speakers, each 
of whom spoke 5 to 39 utterances. The speech features 
including 24 Mel-scale frequency cepstral coefficients were 
extracted from these data for every 20-ms Hamming-
windowed frame with 10-ms frame shifts. 
In computing the inter-utterance similarities, the numbers 
of mixtures in the GMMs and the eigenvectors were 
empirically determined to be 128 and 150, respectively. In the 
GA optimization, the empirical parameter values used for the 
maximum number of generations, the population size, the 
crossover probability, and the mutation probability were 2000, 
5000, 0.5, and 0.1, respectively. For performance comparison, 
an agglomerative hierarchical clustering method was also 
implemented, in which the similarities between clusters were 
computed using the complete linkage of the inter-utterance 
similarities. Fig. 1. shows the speaker-clustering results as a 
function of the number of clusters. Here, “HC-GLR” and 
“HC-Eigenvoice” denote the agglomerative hierarchical 
clustering with the inter-utterance similarities computed using 
the generalized likelihood ratio [2,5,10] and our eigenvoice-
motivated approach [12], respectively. “MPE-Eigenvoice” 
denotes the maximum purity clustering with the eigenvoice-
based inter-utterance similarities. We can see that the 
proposed maximum purity clustering consistently yields 
better performance than the methods based on hierarchical 
clustering. When the number of clusters was specified as 
equal to the speaker population (M = P =15), the best average 
cluster purity of 0.81 was achieved with MPE-Eigenvoice, 
which signifies a relative improvement of more than 10%, 
compared to 0.72, obtained with HC-Eigenvoice.  
Next, to investigate the problem of speaker population 
estimation, the database was divided into several subsets 
involving speaker population sizes of 3, 6, and 9. A series of 
clustering experiments were performed on these subsets 
separately to examine if the optimal numbers of clusters 
ON MAXIMIZING THE WITHIN-CLUSTER HOMOGENEITY OF SPEAKER VOICE 
CHARACTERISTICS FOR SPEECH UTTERANCE CLUSTERING 
 
Wei-Ho Tsai and Hsin-Min Wang 
Institute of Information Science, Academia Sinica, Taipei, Taiwan, Republic of China 
{wesley,whm}@iis.sinica.edu.tw 
 
ABSTRACT 
 
This paper investigates the problem of how to partition unknown 
speech utterances into clusters, such that the overall within-cluster 
homogeneity of speakers’ voice characteristics can be maximized. 
The within-cluster homogeneity is characterized by the likelihood 
probability that a cluster model, trained using all the utterances 
within a cluster, matches each of the within-cluster utterances. 
Such probability is then maximized by using a genetic algorithm, 
which determines the best cluster where each utterance should be 
located. For greater computational efficiency, also proposed is an 
alternative solution that approximates the likelihood probability 
with a divergence-based model similarity. The method is further 
designed to estimate the optimal number of clusters automatically. 
1. INTRODUCTION 
Motivated by the increasing need for indexing and archiving the 
burgeoning amount of spoken data available universally, recent 
research on automatic classification of speech samples based on 
speakers’ voice characteristics has been extended from the 
traditional task of speaker identification/verification [1] to an 
unsupervised paradigm. This paradigm generally involves two 
problems: segmenting an audio recording into speech utterances 
that contain only one speaker’s voice, and grouping utterances 
from the same speaker into a cluster. These two problems are often 
addressed jointly and termed speaker diarization [2]. It is hoped 
that, by locating utterances from the same speaker, the human 
effort required for indexing speech data can be greatly reduced 
from having to listen to every long audio recording to only having 
to check a few utterances in each cluster. In this paper, we 
concentrate on the latter problem, referred to as speaker clustering 
hereafter. Given N speech utterances, each of which is assumed 
from one of the P unknown speakers, where N ≥ P, our aim is to 
partition the N utterances into M clusters, such that M = P, and 
each cluster consists of utterances from only one speaker.  
Currently, the most prevalent method for speaker clustering is 
a hierarchical clustering (HC) framework [3-8]. It computes the 
similarities of vocal characteristics between utterances, and then 
sequentially merges the utterances deemed similar to each other 
(agglomerative clustering), or alternatively, separates the utter-
ances deemed dissimilar to each other (divisive clustering). During 
the agglomeration or division, it is aimed to maximize the 
similarities between all the utterances within a cluster. However, 
existing similarity measures, such as Kullback Leibler distance [4], 
cross likelihood ratio [5,7], and generalized likelihood ratio [3,6], 
are performed entirely on the spectrum-based features. Such 
features are known to carry various types of information besides a 
speaker’s voice characteristics, e.g., phonetic and environmental 
information. As a result, there is no guarantee that the similarities 
between same-speaker utterances will always be larger than the 
similarities between different-speaker utterances, especially when 
the utterances are short and noisy. Since the similarity computation 
is independent of cluster generation, and the latter trusts the former 
completely, the inevitable errors of similarity computation can 
propagate down the whole process. In addition, cluster generation 
based on either agglomeration or division usually uses a nearest or 
farthest neighborhood selection rule to determine which utterances 
can be assigned to the same cluster. However, since the selection 
rule is commonly applied in a cluster-by-cluster manner, and no 
consideration is taken in respect of the interaction between clusters, 
HC can only make each individual cluster as homogeneous as 
possible, but cannot guarantee that the homogeneity for all the 
clusters can be summed to reach a maximum.  
To overcome the HC’s limitations, this study proposes finding 
the best partitioning of speech utterances by integrating the inter-
utterance similarity computation and the cluster generation into a 
unified process. The process iteratively assigns utterances to a set 
of clusters and creates a stochastic model for each cluster, which 
attempts to maximize the similarities between each cluster model 
and the within-cluster utterances. To enable an efficient and 
effective search for the best partitioning, we apply a model 
adaptation technique, model similarity comparison method, and a 
genetic algorithm [9] to solve this problem.  
2. MAXIMUM LIKELIHOOD CLUSTERING 
The proposed method begins with specifying how many clusters 
are to be generated. Given a specified number of clusters, M, the 
task is to assign N utterances X1, X2,…, XN to M clusters c1, c2, …, 
cM, where each utterance is represented by a frame-based feature 
vector stream, i.e., Xn = {xn,1, xn,2,…, xn,Tn}. Let hn denote the 
index of the cluster that an utterance, Xn, is assigned to, where hn 
is an integer between 1 and M. The goal of optimal clustering, 
therefore, is to produce a set of cluster indices, H∗ = {h∗1, h∗2,…, h∗N} 
satisfying h∗n = h∗k for any utterances Xn and Xk from the same 
speaker. To this end, we first create a Gaussian mixture model 
(GMM) λ(m) for each cluster cm, 1 ≤ m ≤ M, by using all the feature 
vectors of the utterances assigned to cm. Then, a certain level of 
agreement that the utterances assigned to the same cluster come 
from the same speaker is characterized by computing the 
likelihood probability, Pr(Xn|λ(m)), ∀ hn = m. Conceivably, the 
larger the value of Pr(Xn|λ(m)), the more suitable cluster cm for 
utterance Xn will be. Thus, by taking the likelihood probabilities 
for all the utterances into account, H∗ can be determined using 
[ ] ,),( )λ|Pr(log)λ|Pr(logmaxarg
1 1
)(* ∑∑
= =
−=
M
m
N
n
nnn
m
n mhδXXH H   (1) 
.),()λ,λ(logmaxarg
1 1
)(* ∑ ∑= == Mm Nn nnm mhS δHH   (7) 
We refer to this clustering method as minimum divergence 
clustering. Since Eq. (7) is independent on the length of utterance, 
the computation complexity can be dramatically reduced, 
compared to the maximum likelihood clustering method. 
4. ESTIMATION OF SPEAKER POPULATION SIZE 
The proposed method described above is developed on the basis 
that a certain number of clusters is specified in advance. However, 
the optimal number of clusters, equal to the speaker population 
size, is unknown and must be estimated. To do this, we propose 
examining all the possible partitionings of N utterances with the 
numbers of clusters ranging from 1 to N, and then selecting one of 
the partitionings associated with the level of within-cluster 
homogeneity as high as possible and the number of clusters as 
small as possible. Such selection can be made with the Bayesian 
information criterion (BIC) [6,14].  
The BIC scores a parametric model based on how well the 
model fits a data set, and how simple the model is:  
|,|log)(#  γ5.0)|Pr(log )(BIC OO Λ−Λ=Λ  (8) 
where #(Λ) denotes the number of free parameters in model Λ, |O| 
is the size of the data set O, and γ is a penalty factor. The larger 
the value of BIC(Λ), the better model Λ will perform. If we treat 
each of the possible partitionings as a parametric model for 
characterizing the speaker information of the utterances, the BIC 
for a model having M clusters can be conceptually computed by 
,log 
2
1),()λ,λ(log)Clusters (BIC
1 1
**)( NMmhSM
M
m
N
n
nn
m γδ −≡ ∑∑
= =
(9) 
where λ(m)∗ is the resulting GMM of cluster cm after the 
optimization according to Eq. (7). In Eq. (9), we use the 
divergence-based similarity measurement to represent how well 
the model fits the data, which approximates the probability 
Pr(O|Λ). The BIC value should increase with the increase of the M 
value initially, but will decline significantly after an excess of 
clusters is created. Thus, a reasonable number of clusters can be 
determined by choosing the partitionings that produces the largest 
value of BIC: 
. )Clusters (BICmaxarg
1
* MM
NM ≤≤
=   (10) 
5. EXPERIMENTS 
Our speech data consisted of 197 utterances chosen from the test 
set of the 2001 NIST Speaker Recognition Evaluation Corpus [15], 
which contains cellular telephone speech collected by the 
Linguistic Data Consortium. The 197 utterances were spoken by 
15 male speakers, and the number of utterances spoken by each 
speaker ranged from 5 to 39. Speech features, including 24 Mel-
scale frequency cepstral coefficients, were extracted from this data 
using a 20-ms Hamming-windowed frame with 10-ms frame shifts. 
The performance of speaker clustering was evaluated on the 
basis of two metrics: cluster purity [5] and the Rand Index [16]. 
Cluster purity is the probability that if we pick any utterance from 
a cluster twice at random, with replacement, both of the picked 
utterances are from the same speaker. Specifically, the purity of 
cluster cm is computed by 
( ) ,/ 2
1 *∑ == Pp mmpm nnρ    (11) 
where nm* is the total number of utterances in cluster cm, and nmp is 
the number of utterances in cluster cm that were produced by the p-
th speaker. The overall performance of M-clustering is evaluated 
by an average purity: ( ) .  
1 *
NnM
m mm∑ == ρρ    (12) 
The Rand Index, which indicates the probability that two 
randomly-selected utterances produced by the same speaker are 
grouped into different clusters, or that two randomly-selected 
utterances grouped into the same cluster are produced by different 
speakers, is defined by 
⎥⎦
⎤⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛+⎟⎟⎠
⎞
⎜⎜⎝
⎛⎥⎦
⎤⎢⎣
⎡
⎟⎟⎠
⎞
⎜⎜⎝
⎛−⎟⎟⎠
⎞
⎜⎜⎝
⎛+⎟⎟⎠
⎞
⎜⎜⎝
⎛= ∑∑∑∑∑∑
=== ===
P
p
p
M
m
m
M
m
P
p
mp
P
p
p
M
m
m nnnnn
1
*
1
*
1 11
*
1
*
22
  
2
2
22
χ
 (13) 
where n*p is the number of utterances from the p-th speaker. Note 
that the higher the value of purity, or the lower the value of Rand 
Index, the better the clustering performance is. 
Our first experiment was conducted to assess the performance 
of our speaker-clustering methods under the condition that the 
number of clusters is specified as the speaker population size (M = 
P = 15). For performance comparison, a baseline system using 
GLR-based similarity computation followed by agglomerative 
clustering [3,5] (referred to as GLR-AC hereafter) was also 
evaluated. Table I shows the performance of the GLR-AC system. 
We examined GLR computed with different numbers of 
component densities in Gaussian mixture modeling. Except for the 
single-Gaussian models, which were full covariance structures and 
trained via maximum likelihood estimation, all the GMMs used in 
this study comprised diagonal covariance matrices and were 
trained via MAP-adaptation. It can be seen from Table I that 
speaker-clustering performance is less sensitive to the structure of 
Gaussian mixture modeling, but is rather sensitive to the choice of 
linkage for inter-cluster similarity computation. Overall, complete 
linkage performs the best, whereas single linkage performs the 
worst, and average linkage is between these two extremes. 
However, all methods were far from accurate for this task. 
Table II shows the speaker-clustering results obtained by our 
proposed methods, namely, maximum likelihood clustering (MLC) 
and minimum divergence clustering (MDC). In GA optimization, 
the parameter values used for the maximum number of generations, 
the population size, the crossover probability, and the mutation 
probability were empirically determined to be 3000, 200, 0.3, and 
0.1, respectively. Comparing Table I with II, it is clear that both 
MLC and MDC are superior to GLR-AC. On the whole, both MLC 
and MDC yield an average cluster purity above 0.7 and a Rand 
Index below 0.4, which signifies a relative improvement of more 
than 30%, compared to the cluster purity of 0.52 and the Rand 
Index of 0.53 obtained with GLR-AC. In addition, we can see from 
Table II that the performance of MLC is slightly better than that of 
MDC. However, as mentioned earlier, MLC is rather 
computationally extensive, due to the need to compute Gaussian 
densities frame by frame. Quantitatively, MLC required 2000 
times the computational time of MDC for this clustering task, and 
took two weeks to complete a trial on a 3 GHz Pentium PC. This 
makes it difficult to use MLC to determine how many clusters 
should be generated if the speaker population size is not known in 
advance. Therefore, in the following experiments, we concentrated 
on examining the validity of MDC-based speaker clustering. 
SPEAKER CLUSTERING BASED ON MINIMUM RAND INDEX 
 
Wei-Ho Tsai1 and Hsin-Min Wang2 
 
1Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan  
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
1whtsai@en.ntut.edu.tw, 2whm@iis.sinica.edu.tw 
 
ABSTRACT 
 
This paper presents an effective method for clustering unknown 
speech utterances based on their associated speakers. The proposed 
method jointly optimizes the generated clusters and the number of 
clusters by estimating and minimizing the Rand index of the 
clustering. The Rand index, which reflects clustering errors that 
utterances from the same speaker are placed in different clusters, 
or utterances from different speakers are placed in the same cluster, 
reaches its minimal value only when the number of clusters is 
equal to the true speaker population size. We approximate the 
Rand index by a function of the similarity measures between 
utterances and employ the genetic algorithm to determine the 
cluster where each utterance should be located, such that the 
overall clustering errors are minimized. The experimental results 
show that the proposed speaker-clustering method outperforms the 
conventional method based on hierarchical agglomerative 
clustering in conjunction with the Bayesian information criterion 
to determine the number of clusters. 
 
Index Terms—Clustering methods, Speech processing, 
Speaker recognition  
 
1. INTRODUCTION 
 
With the burgeoning availability of digital audio material, speaker 
clustering is gaining importance as a means of indexing the 
voluminous spoken data accumulated daily for archival use [1-14]. 
Given N speech utterances produced by P speakers, the goal of 
speaker clustering is to partition N utterances into M clusters, such 
that M = P and each cluster consists exclusively of utterances from 
only one speaker. Since no prior information regarding the 
speakers involved and the speaker population size is available in 
most practical applications, solving the speaker-clustering problem 
usually involves characterizing the voice similarities between 
utterances, generating clusters based on those similarities, and 
determining the optimal number of clusters. 
Currently, the most popular method of speaker clustering 
generates a cluster tree by sequentially merging the utterances 
deemed similar to each other, and then cuts the tree via a Bayesian 
information criterion (BIC) [5,8,10-12,15], in order to retain an 
appropriate number of clusters. During the agglomeration 
procedure, the nearest neighborhood selection rule is usually 
employed in an attempt to maximize the similarities between all 
the utterances within each cluster. Since the interaction between 
clusters is not considered, this method can only make each 
individual cluster as homogeneous as possible; however it cannot 
guarantee that the homogeneity for all the clusters can finally be 
summed to reach a maximum. In particular, mis-clustering errors 
arising from grouping different-speaker utterances together can 
propagate down the whole process, and hence limit the clustering 
performance. In addition, the cluster tree is generated separately 
from the determination of the optimal number of clusters. Since the 
latter trusts the former completely, the inevitable errors from the 
former can propagate to the latter, which may lead to a poor 
estimation of the speaker population size. 
To overcome the above-mentioned limitations of the 
conventional method, we propose a new clustering method that 
jointly optimizes the generated clusters and the number of clusters 
by estimating and minimizing a metric called the Rand index 
[16,17]. This metric indicates the clustering errors that place 
utterances from the same speaker in different clusters, or place 
utterances from different speakers in the same cluster. We 
approximate the Rand index by a function of the similarity 
measures between utterances, and employ the genetic algorithm 
[18] to determine the cluster where each utterance should be 
located. The resulting clusters are thus optimized in a global 
fashion, rather than a pair-by-pair manner used in the conventional 
method. In addition, by exploiting a characteristic of the Rand 
index that it only reaches the minimal value when the number of 
clusters equals the true speaker population size, speaker clustering 
based on the minimization of the estimated Rand index also 
enables the resulting number of clusters to approach the optimum. 
 
2. PROBLEM FORMULATION 
 
For convenience of discussion, we begin by defining the following 
symbols. 
X1, X2,…, XN : N speech utterances to be clustered; 
s1, s2,…, sP : P unknown speakers involved in N utterances; 
c1, c2,…, cM : M clusters to be generated; 
on : index of the speaker producing utterance Xn; 
hn : index of the cluster that utterance Xn is assigned to; 
nm∗ : number of utterances in cm; 
n∗p : number of utterances spoken by sp; 
nmp : number of utterances in cm spoken by sp. 
The goal of speaker clustering is to produce a set of indices H = 
{h1, h2, …, hN} that satisfy  hi = hj for any Xi and Xj from the same 
speaker, and hi ≠ hj for any Xi and Xj from different speakers. 
Depending on the application, there are a number of ways to 
evaluate the performance of speaker clustering. This study uses 
two metrics: cluster purity [4] and the Rand index [4,16,17]. 
Cluster purity represents the probability that if we pick any 
utterance from a cluster twice at random, with replacement, both of 
and 
,),(),(2),()(ˆ
1 1
)()(
1 1
)()()( ∑∑∑∑
= == =
−Ω+=
N
i
N
j
ji
M
j
M
i
N
i
N
j
M
j
M
i
M oohhhhR δδδH  
(13) 
where δ(⋅) in Eqs. (10)−(13) is a Kronecker Delta function.  
However, as the computation of δ(oi, oj) requires that the true 
speaker of each utterance be known in advance, it is impossible to 
find H∗ directly from Eqs. (12) and (13). To solve this problem, we 
propose estimating δ(oi, oj) by means of the similarity measure 
between Xi and Xj. Specifically, 
, 
0 and , if  ,),(
0 and , if  ,),(
          if  ,             1
),(
maxmax
maxmax⎪⎩
⎪⎨
⎧
<≠
>≠
=
←
SjiSS
SjiSS
ji
oo
ji
jiji
XX
XXδ    (14) 
where S(Xi, Xj) denotes a certain similarity measure between Xi 
and Xj that could be either positive or negative, but cannot be zero, 
and Smax is the maximum among the similarities S(Xi, Xj), ∀ i ≠ j. 
In our implementation, S(Xi, Xj) is computed by the generalized 
likelihood ratio (GLR) [1,4]: 
S(Xi, Xj) = logPr(Xij |λij) − logPr(Xi |λi) − logPr(Xj |λj),      (15) 
where  Xij is the concatenation of Xi and Xj, and λi, λj, and λij are 
parametric models trained using Xi,  Xj, and Xij, respectively. 
Using this estimation, we can solve Eq. (12) by further assigning 
to Ω an arbitrary positive constant that ensures . 0)(ˆ )( ≥MR H
Given that neither a gradient-based optimization method nor an 
exhaustive search is applicable in this scenario, we propose using 
the genetic algorithm (GA) [18] to find H∗ by virtue of its global 
scope and parallel searching power. The basic operation of the GA 
is to explore a given search space in parallel by means of iterative 
modifications of a population of chromosomes. Each chromosome, 
encoded as a string of alphabets or real numbers called genes, 
represents a potential solution to a given problem. In our task, a 
chromosome is exactly a legitimate H(M), and a gene corresponds 
to a cluster index associated with an utterance. However, since the 
index of one cluster can be interchanged with that of another 
cluster, multiple chromosomes may amount to an identical 
clustering result. For example, the chromosomes {1 1 1 2 2 3 3}, 
{1 1 1 3 3 2 2}, {2 2 2 1 1 3 3}, and {1 1 1 5 5 4 4} represent the 
same clustering result derived by grouping seven utterances into 
three clusters. Such a non-unique representation of the solution 
would significantly increase the GA search space, and may lead to 
an inferior clustering result. To avoid this problem, we limit the 
inventory of chromosomes to conform to a baseform 
representation defined as follows.  
Let I (cm) be the lowest index of the utterance in cluster cm. 
Then, a chromosome is a baseform  
iff ∀ cm, cl ≠ {φ}, if m < l, then I (cm) < I (cl),   (16) 
where {φ} indicates that a cluster does not contain any utterance. 
Among the above chromosomes, {1 1 1 2 2 3 3} is a baseform, 
since the lowest index of the utterance in clusters c1, c2, and c3 is 1, 
4, and 6, respectively, which satisfies Eq. (16). In contrast, 
chromosomes {1 1 1 3 3 2 2} and {2 2 2 1 1 3 3} are not 
baseforms, since the lowest index of the utterance in clusters c1, c2, 
and c3 does not satisfy Eq. (16). In addition, chromosome {1 1 1 5 
5 4 4} implies that clusters c2 and c3 do not contains any utterance; 
hence it is not a baseform, either. However, it is conceivable that 
all the non-baseform chromosomes can be converted into a unique 
baseform representation by re-arranging the cluster indices. 
GA optimization starts with a random generation of 
chromosomes according to a certain population size, Z. Then, the 
fitness of all chromosomes is evaluated via the inverse of the 
estimated Rand index, i.e., )(ˆ1)( )()( MM RF HH = . Based on this 
evaluation, a particular group of chromosomes is selected from the 
population to generate offspring by subsequent recombination. To 
prevent premature convergence of the population, the selection is 
performed with the linear ranking scheme described in [19]. Next, 
crossover among the selected chromosomes proceeds by 
exchanging the substrings of two chromosomes between two 
randomly selected crossover points. A crossover probability is 
assigned to control the number of offspring produced in each 
generation. After crossover, a mutation operator is used to 
introduce random variations into the genetic structure of the 
chromosomes. This is done by generating a random number and 
then replacing one gene of an existing chromosome with a 
mutation probability. The resulting chromosomes that do not 
conform to the baseform representations are converted into their 
baseform counterparts.  
The procedure of fitness evaluation, selection, crossover, and 
mutation is repeated continuously, in the hope that the overall 
fitness of the population will increase from generation to 
generation. When the maximum number of generations is reached, 
the best chromosome in the final population is taken as the 
solution, H*. Note that the estimated speaker population size can 
be obtained by selecting the maximal value of the cluster index in 
H*. For example, if H* = {1 2 1 3 4 3 1}, the estimated number of 
speakers in a seven-utterance collection is 4. 
 
4. EXPERIMENTAL RESULTS 
 
The speech data used in this study consisted of six excerpts of 
broadcasts from the evaluation set of the 2002 Rich Transcription 
Broadcast News and Conversational Telephone Speech Corpus 
[20]. Each excerpt was segmented into speaker-homogeneous 
utterances, according to the annotation files in the corpus. Speaker 
clustering was then applied to each excerpt separately. Prior to the 
experiments, every speech utterance was converted from its digital 
waveform representation into a sequence of feature vectors, each 
of which consisted of 12 Mel-scale frequency cepstral coefficients 
(MFCCs) and 12 delta MFCCs. Then, the similarities between the 
utterances were computed using Eq. (15), in which all the 
parametric models are of a uni-Gaussian model with a full 
covariance matrix. 
In GA optimization, the parameter values used for the 
maximum number of generations, the population size, the 
crossover probability, and the mutation probability were 
empirically determined to be 2000, 5000, 0.5, and 0.1, 
respectively. For the performance comparison, we also 
implemented a baseline speaker-clustering system based on 
hierarchical agglomerative clustering (HAC) in conjunction with 
the Bayesian information criterion (BIC) to determine the optimal 
number of clusters [5]. In the agglomeration procedure, the 
similarities between clusters were computed using the complete 
linkage of the GLR-based inter-utterance similarities. In addition, 
in using the BIC, the penalty weight was set to one. 
Table 1 shows the speaker-clustering results. First, we 
evaluated the performance of the proposed minimum Rand index 
clustering (MRIC) by specifying the number of clusters a priori as 
the true number of speakers. This served as an upper bound of the 
performance that could be achieved by the automatic 
 1
 An Efficient Approach to Multimodal Person Identity Verification by Fusing 
Face and Voice Information 
Hsien-Ting Cheng 1,2, Yi-Hsiang Chao 1,3, Shih-Liang Yeh 1, Chu-Song Chen 1, Hsin-Min Wang 1, and Yi-Ping Hung 1,2 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan. 
2 Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan. 
3 Department of Computer and Information Science, National Chiao Tung University, Hsinchu, Taiwan.
Abstract 
This paper presents an effective method to combine speech 
recognition, speaker verification and face verification for 
biometric authentication. Our method provides a 
light-weight enrollment process and an easy-to-use 
verification interface. A multi-face/single-sentence 
strategy is used to combine voice and face verification 
modules, and support vector machine is employed for 
information fusion. Experimental results show that our 
method can achieve high verification accuracies. 
1 Introduction 
Fusing multimodal information for biometric-based 
authentication has been shown an effective way to boost 
the verification performance of using only a single 
modality [3][2][9][1]. In this paper, we present a 
framework of combining face and voice data for person 
identity verification. In our work, state-of-the-art 
approaches for face authentication and speaker verification 
are employed to build individual modules. Then, the face 
and voice modules are combined to achieve a highly 
effective person identity verifier. 
Our method employs the technique of keyword spotting, 
so that the identity-claim process becomes easy. 
Significant characteristics of our individual modules 
include: (1) We integrate both speech recognition and 
speaker verification techniques to prevent impostors who 
try to take the prerecorded speech of a valid speaker from 
entering in our system. (2) We develop an incremental 
learning method for kernel Fisher’s discriminant, so that 
the insertion/deletion of a new/old training image becomes 
more efficient. (3) A light-weight enrollment process is 
proposed, where the users have only to say very limited 
words for on-line training data collection. 
In the fusion module, since we can grab multiple images 
during the period of user’s saying a sentence, we adopt the 
strategy of multi-face/single-sentence integration for 
identity verification. Hence, the face verification module 
will be used multiple times for an image sequence in a 
verification task. To generate a single score of the face 
module for further fusion, we have investigated several 
strategies and made a useful suggestion. Finally, we use 
support vector machine (SVM) as the fusion classifier to 
make a decision. 
2 System Overview 
The basic building blocks of our system are introduced in 
Fig. 1. A person identity verification system shall include 
both enrollment and verification procedures. In the 
enrollment phase, a new user has to first type his name and 
say “I am xxx”. If the name the user types is consistent 
with the name extracted by keyword spotting from what 
he/she says, the user will be asked to read two digit strings: 
0~9 and 9~0. In the meanwhile, an image sequence of the 
user is captured by the camera. Then, the audio and video 
features extracted are used to train face and voice 
classifiers independently. A fusion classifier is trained 
later for fusing the face and voice classifier outputs. To 
users, the training process is of light weight since only few 
sentences have to be read and the required training images 
Figure 1. Building blocks of the person identity  
verification system. 
Face verification 
module 
Voice verification 
module
Fusion module
Accept/Reject 
Keyword spotting 
for person identity 
claim
Sentence “I am xxx” 
Sentence of 
random digits, 
eg.,“25782930
Image sequence 
Identity 
claim phase 
Verification 
phase 
Face cropping / 
Two-level DWT 
Relative image 
gradient 
Figure 2. Automatic face cropping and feature extraction.
 3
By applying Eq. (3), critical computations are updated 
from previous ones and fast incremental learning is 
achieved. 
4 Voice Verification Module 
4.1. Speaker Voice Modeling 
By its nature, speech is a temporal signal. In speech 
recognition or speaker recognition, speech is usually 
represented as a sequence of feature vectors characterized 
by some distributions. Techniques based on Gaussian 
Mixture Models (GMMs) [13] have been successfully 
applied to speaker recognition in recent years. The most 
common approach for learning GMMs is the 
expectation-maximization (EM) algorithm [8]. The 
number of mixture components is usually decided 
empirically. In our system, we apply a 
model-selection-based self-splitting Gaussian mixture 
learning algorithm, which is able to automatically find an 
appropriate number of mixture components according to 
the characteristics of training data [6]. 
4.2. Verification by Speech 
In speaker verification, the identity of the current speaker 
must be transmitted to the system beforehand and the task 
is to determine whether the current speaker is the claimed 
one or not. The log-likelihood ratio (LLR) [13] detector is 
the most popular method. Given an utterance X and a 
claimed speaker identity with the corresponding model Cλ , 
the LLR is calculated as, 
)|(log)|(log)( CC XpXpX λλ −=Λ      (4) 
where Cλ  is the so-called anti-model. Suppose there are 
N clients (i.e., speakers) in the system, Cλ  is selected as, 
).|(logmaxarg
,1, sC
Xp
CsNss
λλ λ ≠≤≤=        (5) 
The LLR, )(XΛ , obtained by Eq. (4) can be further 
normalized between 0 and 1 via the Sigmoid function. 
4.3. Integration of Speech Recognition and Speaker 
Verification 
In general, speaker verification techniques can be 
classified into text-dependent approaches and 
text-independent approaches. The former require the 
speaker to say certain keywords or sentences with the 
same textual content for both training and recognition 
trials, while the latter do not rely on a specific text being 
spoken. Our voice verification module runs in both modes. 
We use the keyword spotting technique [4] for speech 
recognition. The keyword lexicon contains the names of 
all clients and 500 random 8-digit strings dynamically 
generated by the system. In testing, a client first claims 
himself (or herself) by saying “I am xxx”. Then, the 
system will prompt an 8-digit string and request the client 
to say it. The client has to obey this strict request and utter 
the sentence correctly. Otherwise, he or she will be 
rejected in the speech recognition phase. The advantages 
of combining speech recognition and speaker verification 
are twofold: First, the client can claim himself (or herself) 
by speech conveniently. Second, the impostor taking the 
recorded speech from the valid speaker will be rejected 
because the contents of prerecorded speech and the 
prompted text are mismatched. 
5 Fusion Module 
The fusion module is the final building block in the system. 
It has to makes a binary decision of accepting or rejecting 
the claimed identity via two confident scores output from 
face and speech voice verification modules. In our work, a 
multi-face/single-sentence strategy is used, which is 
defined as combining n face verification outputs with a 
single speech verification output to make a decision, where 
n is the number of images taken during the period of 
saying a specified digit-string, and n is typically 10 in our 
system. This task can be achieved by training a binary 
classifier in the (n+1)-dimensional space. However, the 
classifier might be difficult to train since the face and 
voice data are unbalanced in such a feature space. In our 
work, we divide this task into two steps. First, a single 
score is conducted from the n face verification scores. 
Then, this score is combined with the voice verification 
score in the second step to make a decision. 
In the first step, we have used some statistical approaches 
including the selections of the maximum, minimum, 
median, mean, and trimmed-mean (the mean of the n 
values except the highest and the lowest ones) to conduct a 
single score. According to our experience, the 
trimmed-mean approach achieved the best performance, 
and is used in our current system. In the second step, 
making a single decision from the two scores of the face 
and voice modules is equivalent to doing a two-class 
classification, and we use SVM to achieve this purpose. 
In the past, many works [2][9] also adopted SVM as a 
fusion classifier. However, why SVM is good for fusion 
has not been well investigated yet. In [5], we have given a 
theoretical explanation about why the SVM with 
radial-basis-function (RBF) kernel is generally better than 
linear combination for classifier fusion. Based on this 
study, we choose the RBF kernel for the SVM and perform 
a complete model selection to conduct the SVM classifier 
with the minimal cross-validation error for the fusion task. 
 
CASCADING MULTIMODAL VERIFICATION USING FACE, VOICE AND IRIS
INFORMATION
Ping-Han Lee†, Lu-Jong Chu† Yi-Ping Hung‡ Sheng-Wen Shih§ Chu-Song Chen¶, Hsin-Min Wang¶
†Institution of Computer Science and Information Engineering, National Taiwan University, Taiwan
‡Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan
§Department of Computer Science and Information Engineering National, Chi Nan University, Taiwan
¶Institute of Information Science, Academia Sinica, Taiwan
ABSTRACT
In this paper we propose a novel fusion strategy which
fuses information from multiple physical traits via a cascad-
ing verification process. In the proposed system users are ver-
ified by each individual modules sequentially in turns of face,
voice and iris, and would be accepted once he/she is verified
by one of the modules without performing the rest of the ver-
ifications. Through adjusting thresholds for each module, the
proposed approach exhibits different behavior with respect
to security and user convenience. We provide a criterion to
select thresholds for different requirements and we also de-
sign an user interface which helps users find the personalized
thresholds intuitively. The proposed approach is verified with
experiments on our in-house face-voice-iris database. The ex-
perimental results indicate that besides the flexibility between
security and convenience, the proposed system also achieves
better accuracy than its most accurate module.
1. INTRODUCTION
With the advancement in biometrics, which verifies the identi-
ties of users via user’s physical traits, it becomes a legitimate
method for identity verification in recent years. One of the
merit of biometrics is that since physical traits are intrinsic to
each person, they are hard to be forged and missed. Physi-
cal traits applied in the literature of biometrics include finger-
prints, iris, face, hand vein, signature, voice, etc. [5]. While
most researchers focus on unimodal biometrics (i.e., utilizing
only one physical trait), it has been reported that multimodal
biometrics outperform the unimodal ones recently [7] [5].
One crucial problem in multimodal biometrics is how to
combine information from multiple physical traits. Generally
speaking [5], the strategy of fusion can be categorized into
feature level [3], matching score level [1] and decision level.
In [3], the feature vectors of palmprint and hand geometry are
concatenated as a new feature vector. The authors utilize the
This work is supported in part by National Science Council, Taiwan, un-
der grant NSC 94-2213-E-002-026 and by MOEA Technology Development
Program for Academia under grant 95I513.
normalized correlation coefficient to compute the matching
scores between two feature vectors. In [1], matching scores
resulting from face and voice classifiers are combined as a
new two-dimensional feature vector. The author train support
vector machine based on this new feature vector to perform
the classification. For decision level fusions, final decisions
are typically obtained by voting on multiple classifiers.
Despite of the improvement in accuracy, there are two is-
sues in the fusion strategies mentioned above. One issue is
that when classifiers with high accuracy such as an iris clas-
sifier, the improvement through the fusion may be limited. In
[8] the authors combine face and iris for identity verification
in the matching score level. They point out when iris sam-
ples are well aligned, the accuracy of the system fusing face
and iris is worse than a stand-alone iris verification module.
The other issue is the multiple sample acquisition processes
in multimodal systems is very intrusive to users, especially
when iris verification module exists in systems.
To address the two issues, we propose a new fusion strat-
egy that fuses multiple physical traits in a cascade structure,
in which users are verified with individual modules sequen-
tially in separate stages, each contains an unimodal module.
Once the user is verified with one module, he/she is accepted
and the verifications for the rest of the modules are avoided.
The modules in the proposed cascade structure are in turns of
face, voice and iris. Note that this sequence is in the order of
increasing module accuracy, and it happens to be also in the
order of increasing user intrusiveness. Through adjusting the
thresholds in each module, the proposed system has different
behaviors by levitating between accuracy and user intrusive-
ness.
2. SYSTEM OVERVIEW
The login process of the proposed system is a cascade struc-
ture illustrated in Fig. 1. In an environment with access con-
trol where there is a hallway and a door, an user is verified
with the face verification module without his/her notice when
approaching the door. If the face verification module accepts
sample plentiful threshold triplets and obtain the correspond-
ing TPEs. Then we select the best TPE using the criteria be-
low:
arg min
Tf ,Tv,Ti
(wsa · FARs + wsr · FRRs + (1)
wf · FRRf + wv · FRRv),
where wsa, wsr, wf and wv are corresponding weights. We
further enforce wsa + wsr + wf + wv = 1.
With TPEs selected through different weights settings,
the proposed system exhibits different behaviors towards se-
curity and convenience. In Table 1 we list three weights set-
tings. In Security Mode, we emphasize low FARs and we
also penalize highFRRs moderately. Since we neglectFRRf
and FRRv totally, the two values in the selected TPE may be
large. Thus the users will be asked to perform all the three ver-
ification tests frequently. In the Convenience Mode, better
user convenience is gained through sacrificing some system
security. We increase weights for all FRRs and give a small
weight for FARs. We also list a mode that favors neither
of the four attributes (FRRf , FRRv , FRRs and FARs),
which is Normal Mode.
Table 1. Three different weights settings.
Mode wsa wsr wf wv
Security 0.6 0.4 0 0
Convenience 0.1 0.2 0.4 0.3
Normal 0.25 0.25 0.25 0.25
4.3. User Interface
To help users select the threshold triplets that best satisfy their
needs, we design an user interface illustrated in Fig. 2. To
provide an intuitive interface to users, we define four bars
which are Intruder Rejection, Face Login Rate, Voice Lo-
gin Rate and Iris Login Rate as (1− FARs), (1− FRRs),
(1 − FRRf ) and (1 − FRRv), respectively. Users can use
these four bars on the left portion of the panel to adjust the
four main attributes hence satisfy their needs, with the aid of
the four buttons of different weights setting in the lower right
region of the panel. The upper right region of the panel shows
the detailed thresholds, FARs and FRRs for the face, voice
and iris modules. The resulting thresholds will be applied in
the proposed cascading framework and thus the system be-
haviors will change as users requested.
Each time an user set the value of certain attribute to x,
TPEs with the corresponding values of attributes y which sat-
isfy x − ε < y < x + ε are picked as a subset of all TPEs,
where ε is the tolerance. Then we use (1) together with the
selected weights setting to select one best TPE in this subset.
The suggested threshold triplet is returned on the upper right
region of the panel.
Fig. 2. The user interface.
Fig. 3. Some samples from our in-house face-voice-iris
database. The first row shows face samples and the second
row presents iris samples.
5. EXPERIMENTAL RESULTS
We conduct experiments on our in-house multimodal biomet-
rics database, which consists of face, voice and iris data. We
have two sessions of data in our database, each session con-
tains 10 triplets of samples. Data of two sessions are taken
between a week. We have 19 registered users with complete
data of two sessions in this database, which are composed of
14 males and 5 females. There is an additional single session
of 5 people who can serve as intruders. The face samples in-
clude some variations in lighting, pose and expression. We
manually crop faces and normalize these faces to the size of
50x50 pixels, with zero mean and unit variance. Iris sam-
ples are cropped automatically and resized to 256x128 pixels.
Fig. 3 illustrates some face and iris samples. Voice data are
recorded in a controlled environment, we also subtract back-
ground noise.
In sampling threshold triplets, 10 thresholds for each mod-
ules are equally sampled in the interval of [0, 1] for face and
voice verification modules. We sample 20 thresholds for iris
verification module since the iris verification module is more
sensitive to different thresholds. In our experiment, we use
data in session 1 for training. We use data in session 2 to
obtain total 2000 TPEs under the sampled threshold triplets.
To compare the accuracy of the proposed system with the
three individual modules, we select threshold triplets that re-
sult in small total error (FARs + FRRs) and plot them in
Fig. 4 together with the Receiver Operating Characteristic
(ROC) curves of the three modules. Note that each point of
the proposed system in this figure results from a TPE. We
Speech utterance clustering based on the maximization of
pe
niver
Taiw
pted
ition
es fr
. Th
the p
gene
the l
tche
r all
eac
ing
larit
hen
iterio
he s
ng.
ated with two processes. One is to segment speech data into
For more than two decades, automatic speaker recogni-
tion based on vocal characteristics1–3 has received a tremen-
dous amount of attention in research that facilitates human-
machine communications and biometric applications.
Nowadays, as speech is being exploited as an information
source, the utility of recognizing speakers’ voices is increas-
ingly in demand for indexing and archiving the mushroom-
ing amount of spoken data available universally. Traditional
approaches to speaker recognition assume that some prior
information or speech data is available about the speakers
concerned. Thus, speaker-specific models can be trained us-
ing the labeled speech data, and the likelihoods of unknown
test utterances can then be computed from the models,
thereby determining the identity of a speaker speaker iden-
tification, or determining if a speaker is who he/she claims
to be speaker verification. However, for indexing or ar-
chiving, the basic strategy needs to be expanded to distin-
guish between speakers when neither information about the
speakers’ voices nor the speaker population size is available.
As a result, unsupervised classification of speech data based
on speakers’ voice characteristics has emerged as a new and
challenging research problem;4–8 however, the solutions to
this problem require further investigation.
homogeneous utterances that contain only one speaker’s
voice. The other is to group together homogeneous utter-
ances from the same speaker into a cluster. The former is
usually referred to as speaker segmentation,8,9 while the lat-
ter is referred to as speaker clustering.10,7 A joint process
consisting of speaker segmentation and clustering, called
speaker diarization,11–14 was recently defined by the NIST
Speech Group.15 It is hoped that by locating speech segments
from the same speaker, the human effort required for index-
ing speech data can be greatly reduced from having to listen
to every long audio recording to only having to check a few
utterances in each cluster. In addition, by locating speech
utterances from speakers with similar voices,16–18 transcrip-
tion or recognition of speech messages can be carried out
more effectively by adapting acoustic models on a per cluster
basis, which exploits more adaptation data than on a per
utterance basis.
In this paper, we concentrate on the problem of speaker
clustering. Given N unlabeled speech utterances, each of
which is assumed to be from one of P speakers, where N
P and P is unknown, speaker clustering is defined as the
partitioning of N utterances into M clusters, such that M
= P, where each cluster consists exclusively of utterances
from only one speaker. For utterances that contain multiple
speakers, the partitioning is preferably performed after the
utterances are presegmented into speaker-homogeneous re-
gions. However, in order to focus on the fundamental tech-
aElectronic mail: whtsai@en.ntut.edu.tw
bElectronic mail: whm@iis.sinica.edu.twwithin-cluster homogeneity of s
Wei-Ho Tsaia
Department of Electronic Engineering, National Taipei U
Hsin-Min Wangb
Institute of Information Science, Academia Sinica, Taipei,
Received 2 June 2005; revised 19 June 2006; acce
This paper investigates the problem of how to part
clusters, such that each cluster consists of utteranc
clusters reflects the unknown speaker population size
certain number of clusters, corresponding to one of
maximizes the level of overall within-cluster homo
The within-cluster homogeneity is characterized by
trained using all the utterances within a cluster, ma
attain the maximal sum of likelihood probabilities fo
genetic algorithm to determine the cluster in which
computational efficiency, also proposed is a cluster
probability with a divergence-based model simi
within-cluster utterances. The clustering method t
clusters by adapting the Bayesian information cr
population size. The experimental results show t
conventional methods based on hierarchical clusteri
DOI: 10.1121/1.2225570
PACS numbers: 43.72.Pf, 43.72.Fx DOS
I. INTRODUCTIONJ. Acoust. Soc. Am. 120 3, September 2006 0001-4966/2006/1203aker voice characteristics
sity of Technology, Taipei, Taiwan
an
19 June 2006
unknown speech utterances into a set of
om only one speaker, and the number of
e proposed method begins by specifying a
ossible speaker population sizes, and then
ity of the speakers’ voice characteristics.
ikelihood probability that a cluster model,
s each of the within-cluster utterances. To
utterances, the proposed method applies a
h utterance should be located. For greater
criterion that approximates the likelihood
y between a cluster and each of the
examines various legitimate numbers of
n to determine the most likely speaker
uperiority of the proposed method over
© 2006 Acoustical Society of America.
Pages: 1631–1645
Classifying speech data by speaker is generally associ-© 2006 Acoustical Society of America 1631/1631/15/$22.50
algorithm,32 together with a model similarity comparison
method, to search for the best partitioning. In addition, the
utterances Xn and Xk are from different speakers. This
gives a large value of GLRX ,X  when utterances Xproposed clustering method further adapts the Bayesian in-
formation criterion to determine how many clusters should
be created.
The remainder of this paper is organized as follows.
Section II reviews a specific implementation of hierarchical
clustering, which is the most popular method of speaker
clustering. Section III introduces our proposed speaker clus-
tering method, called maximum likelihood clustering, with
the goal of maximizing the within-cluster homogeneity of
voice characteristics. In Sec. IV, we present an alternative
speaker-clustering solution, called minimum divergence
clustering, which aims to improve the efficiency of maxi-
mum likelihood clustering. Section V discusses the problem
of how to automatically determine the appropriate number of
clusters. Section VI summarizes the configuration of our
speaker-clustering system and analyzes its computational
complexity. Section VII presents our experimental results.
Finally, in Sec. VIII, we present our conclusions and discuss
the direction of future works.
II. REVIEW OF HIERARCHICAL CLUSTERING
To cluster speech utterances by speaker, it is necessary
to distinguish between utterances belonging to the same
speaker and those belonging to different speakers. A com-
mon strategy for this process is to measure the similarities of
voice characteristics between utterances and then determine
which utterances are similar enough to be considered as be-
ing from the same speaker. This section details a specific
implementation of this strategy, which serves as a baseline
solution in the current study.
A. Interutterance similarity computation
Let X1 ,X2 , . . . ,XN denote N speech utterances to be
clustered, each of which is represented by a certain
spectrum-based feature, e.g., the cepstral feature. The simi-
larities between utterances are measured on the basis of the
generalized likelihood ratio GLR.17 For any pair of utter-
ances, Xn and Xk, the GLR is computed by
GLRXn,Xk =
PrXnnkPrXknk
PrXnnPrXkk
, 1
or, equivalently,
GLRXn,Xk = log PrXnnk + log PrXknk
− log PrXnn − log PrXkk , 2
where n, k, and nk are stochastic models, e.g., Gaussian
mixture models GMMs, trained using Xn, Xk, and a con-
catenation of Xn and Xk, respectively. These stochastic mod-
els are designed to capture the relevant aspects of voice char-
acteristics underlying speech utterances. Implicit in Eqs. 1
and 2 is the presumption that if utterances Xn and Xk are
from the same speaker, model nk should be able to cover the
voice characteristics of the individual utterances appropri-
ately; hence, the likelihood probabilities PrXn nk and
PrXk nk would be large, compared to the case whereJ. Acoust. Soc. Am., Vol. 120, No. 3, September 2006n k n
and Xk are from the same speaker, and a small value oth-
erwise.
B. Cluster generation
After computing the interutterance similarities, the next
step is to assign the utterances deemed similar to each other
to the same cluster. This is commonly done by an agglom-
erative hierarchical clustering method,33 which consists of
the following procedure:
1. begin initialize M←N, and form clus-
ters ci← Xi , i=1,2 , . . . ,N
2. do
3. find the most similar pair of clusters,
say ci and cj
4. merge ci and cj
5. M←M −1
6. until M =1
7. end
The similarities between a pair of clusters, say ci and cj,
can be derived from the interutterance similarities, according
to one of the following heuristic measures:
1 complete linkage:
Sci,cj = min
Xnci,Xkcj
GLRXn,Xk , 3
2 single linkage:
Sci,cj = max
Xnci,Xkcj
GLRXn,Xk , 4
or
3 average linkage:
Sci,cj =
1
#i, j Xnci,Xkcj
GLRXn,Xk , 5
where #i , j denotes the number of utterance pairs involved
in the summation. Alternatively, the similarities between
clusters can be measured by concatenating all the utterances
within each cluster into a long utterance, and then computing
the GLR between the concatenated utterances. The outcome
of the agglomeration procedure is a cluster tree. The final
partition of the utterances is then determined by pruning the
tree that only has the desired number of leaves left.
III. MAXIMUM LIKELIHOOD CLUSTERING „MLC…
Although the above hierarchical clustering method is
popular for speaker clustering, it is far from optimal in a
number of respects. First, the similarities between utterances
are measured in a pairwise manner, which only considers
information about one pair of utterances at a time, and ig-
nores the fact that out-of-pair information can benefit simi-
larity computation for every pair of utterances. Obviously, a
better solution would be to characterize the similarities be-
tween all the utterances to be clustered in a global fashion,
rather than in a piecemeal manner. Second, hierarchical clus-
tering only attempts to make the voice characteristics withinW. Tsai and H. Wang: Speaker clustering 1633
After crossover, a mutation operator is used to introduce
random variations into the genetic structure of the chromo-Figure 1 shows a block diagram of GA-based optimiza-
tion. It starts with a random generation of chromosomes ac-
cording to a certain population size Z, say 200. The fitness of
all chromosomes is then evaluated and ranked on the basis of
the overall model likelihood, i.e.,
LG = 
m=1
M

n=1
N
log PrXnmgn,m . 9
As a result of this evaluation, a particular group of chromo-
somes is selected from the population to generate offspring
by subsequent recombination. The selection reflects the fact
that chromosomes with superior fitness have a higher prob-
ability of being included in the next generation than those
that are inferior. To prevent premature convergence of the
population, this study employs the linear ranking selection
scheme,34 which sorts chromosomes in increasing order of
fitness, and then assigns the expected number of offspring
according to their relative ranking. Note that after this opera-
tion, chromosomes with large fitness values will produce
several copies, while chromosomes with tiny fitness values
may be eliminated; hence, the total chromosome population
size does not change.
Next, crossover among the selected chromosomes pro-
ceeds by exchanging the substrings of two chromosomes be-
tween two randomly selected crossover points. For example,
the crossover made for chromosomes 1,1 ,1 ,2 ,2 ,3 ,3 and
1,2 ,3 ,1 ,2 ,3 ,2 generates 1,1 ,3 ,1 ,2 ,3 ,3 and
1,2 ,1 ,2 ,2 ,3 ,2, if the selected crossover points are 2 and
6, as the underlines indicate. However, as this example
shows, the resulting chromosomes, such as
1,1 ,3 ,1 ,2 ,3 ,3, may not conform to Eq. 8. Therefore,
the procedure for interchanging the clusters’ indices has to be
performed again to ensure all the offspring are baseforms. In
this example, the chromosome 1,1 ,3 ,1 ,2 ,3 ,3 is con-
verted to 1,1 ,2 ,1 ,3 ,2 ,2 by swapping index “2” with “3.”
In addition, a crossover probability is assigned to control the
ratio of the number of offspring produced in each generation
to the chromosome population size.
FIG. 1. Flow diagram of the genetic algorithm.J. Acoust. Soc. Am., Vol. 120, No. 3, September 2006somes. This is done by generating a legitimate random num-
ber and then replacing one gene of an existing chromosome
with this random number according to a mutation probabil-
ity. The resulting chromosomes are converted to the base-
form representations again, if necessary. Then, the procedure
of fitness evaluation, selection, crossover, and mutation is
repeated continuously, following the principle of survival of
the fittest, to produce better approximations of the optimal
solution. Accordingly, it is hoped that the overall model like-
lihood will increase from generation to generation. When the
maximum number of generations iterations Q, say 4000, is
reached, the best chromosome in the final population is taken
as the solution G*.
C. MAP estimation of the cluster model
As the above optimization procedure requires the cre-
ation of MZ GMMs during each GA iteration, the compu-
tational complexity can be too high to implement properly if
the parameters of the GMMs are estimated via the
expectation-maximization EM algorithm.35 To overcome
this problem, we propose applying model adaptation tech-
niques to generate cluster GMMs, instead of training them
from scratch. The basic strategy, stemming from the GMM-
UBM method for speaker recognition,3 is to create a cluster-
independent GMM using all the utterances to be clustered,
followed by an adaptation of the cluster-independent GMM
for each of the clusters using maximum a posteriori MAP
estimation.
Let =  j , j , j ,1 jJ denote the parameter set of
a cluster-independent GMM having J mixture Gaussian com-
ponents, where  j is the mixture weight,  j is the mean
vector, and  j is the covariance matrix. These parameters are
estimated via the EM algorithm. For each utterance Xn, with
Tn feature vectors xn,1 ,xn,2 , . . . ,xn,Tn, we compute the a
posteriori probability of each feature vector xn,t in the jth
mixture of GMM  as follows:
Prjxn,t =
 jNxn;t; j, j
l=1
J
lNxn,t;l,l
, 10
where N· is a Gaussian density function. Then, the follow-
ing parameters are computed and stored in a look-up table:
n,j = 
t=1
Tn
Prjxn,t , 11
En,jx = 
t=1
Tn
Prjxn,txn,t, 12
En,jxx = 
t=1
Tn
Prjxn,txn,txn,t , 13
where prime  denotes a vector transpose. Whenever a set
of cluster indices, g1 ,g2 , . . . ,gN, is assigned to N utterances,
the cluster GMMs, m=  j
m
, j
m
, j
m
,1 jJ, 1m
M, can be updated byW. Tsai and H. Wang: Speaker clustering 1635
V. ESTIMATION OF THE NUMBER OF SPEAKERS The value of BM should increase with the increase in
the value of M initially, but it will decline significantly afterThe proposed speaker-clustering methods described
above are based on specifying a certain number of clusters to
be generated in advance. In general, the greater the number
of clusters specified, the higher the level of homogeneity
within a cluster. However, if too many clusters are generated,
a single speaker’s utterances would spread over multiple
clusters; hence, the speaker clustering would not be com-
plete. Clearly, the optimal number of clusters is equal to the
speaker population size, which is unknown and must be es-
timated.
Consider a collection of N speech utterances to be par-
titioned into M clusters. The optimal value of M must be an
integer between 1 and N. Thus, if we produce a set of pos-
sible partitionings, in which the number of clusters ranges
from 1 to N, the task of determining the optimal value of M
would amount to selecting one of the N partitionings that
achieves the level of within-cluster homogeneity as high as
possible with the number of clusters as small as possible. To
realize such a selection, we adapt the Bayesian information
criterion BIC40 to score each of the possible partitionings,
thereby identifying the best one.
The BIC is a model selection criterion that assigns a
value to a parametric model based on how well the model fits
a data set, and how simple the model is:
BIC = log PrO −
1
2
 #  log O , 26
where  is a penalty factor generally equal to one, #
denotes the number of free parameters in model , and O is
the size of the data set O. The larger the value of BIC ,
the better model  will perform.
By treating each of the possible partitionings as a model
for characterizing speaker information in the utterances, we
can evaluate a partitioning with M clusters via the following
BIC-motivated score:
BM = 
m=1
M

n=1
N
log Sm*,ngn*,m −
1
2
M log N ,
27
where gn
* denotes the index of the cluster in which utterance
Xn is located according to the GA optimization for Eq. 25,
and m* is the resulting GMM of cluster cm after optimiza-
tion. In Eq. 27, we use the divergence-based similarity
measurement, m=1
M n=1
N log Sm* ,ngn* ,m, to repre-
sent how well the model fits the data. This approximates
the log probability log PrO . Here, the data is actually
a set of N utterance GMMs, which is further “modeled” by
M cluster GMMs, if M clusters are generated. Hence, the
size of data can be considered as the number of utterance
GMMs, i.e., O  =N, which does not depend on the utter-
ance duration. Moreover, since the configuration of the
data utterance GMMs are the same as that of the model
cluster GMMs, the number of free parameters in model
 can be considered independent of the number of Gauss-
ian densities used and the dimensionality of feature vec-
tors. This indicates that #M.J. Acoust. Soc. Am., Vol. 120, No. 3, September 2006an excess of clusters is created. Thus, a reasonable number of
clusters can be determined by choosing the partitioning that
produces the largest value of BM, i.e.,
M* = arg max
1MN
BM . 28
Note that in the original definition of BIC, the term
log PrO  can also be represented by LG* obtained with
MLC. However, due to the high computational complexity,
we find that MLC is unsuitable for the scenario that the true
speaker population size is unknown and needs to be esti-
mated. On the other hand, in a pioneering work19 on the use
of BIC for speaker clustering, the generation of clusters is
performed via the aforementioned GLR-based similarity
computation, followed by hierarchical clustering. Each of the
resulting clusters is then represented by a uni-Gaussian den-
sity estimated by using the feature vectors of the within-
cluster utterances; hence, the model, , is a set of Gaussian
densities. Since we have characterized every cluster by a
GMM, our work differs from Ref. 19 in that the proposed
model  is optimized during the generation of clusters and
directly reflects the overall homogeneity of within-cluster ut-
terances.
VI. SYSTEM CONFIGURATION AND ANALYSIS OF
THE COMPUTATIONAL COMPLEXITY
Figure 2 summarizes the implementation of our speaker-
clustering system. In the absence of knowing the true
speaker population size, the system in turn hypothesizes that
N utterances to be clustered can be from one speaker, two
speakers, . . ., or N speakers. For each of the possible speaker
population sizes, MDC is run with the number of clusters
specified as the hypothesized speaker population size. This
yields N partitionings optimized by GA, along with N BIC-
motivated scores. The system then outputs the partitioning
associated with the largest BIC-motivated score.
In view of the usability, it is worth comparing the com-
putational complexity of the proposed clustering system with
that of a GLR-based hierarchical clustering system. We ob-
serve that there are two factors which dominate the overall
computational time for both systems. The first factor arises
from the Gaussian mixture modeling of feature vectors, e.g.,
the generation of nk in Eq. 1, or the generation of cluster
model m in Eq. 25. The second factor arises from the
computation of Gaussian functions based on the models, e.g.,
PrXn nk, or Sm ,n. However, if the models are gen-
erated using the MAP adaptation, the first factor can be ig-
nored, compared to the second factor. Hence, the system
complexity depends mainly on how many Gaussian func-
tions need to be performed.
Consider an agglomerative hierarchical clustering sys-
tem, in which the intercluster similarities are measured on an
utterance-concatenation basis. If the system has generated
M +1 clusters c1 ,c2 , . . . ,cM+1, and is going to determine
which pair of clusters can be merged, it requires us to com-
pute M likelihood probabilities for every “long utterance”
formed by concatenating all the utterances within a cluster.W. Tsai and H. Wang: Speaker clustering 1637
ances in the second and third subsets. Feature vectors, each
consisting of 20 MFCCs, were extracted from these utter-
of the selected frames are from the same speaker. Thus, when
computing a frame-based purity by Eq. 29, n representsances for every 20-ms Hamming-windowed frame with
10-ms frame shifts. Prior to MFCC computation, voice ac-
tive detection42 was applied to remove salient nonspeech re-
gions that may be included in an utterance. The total nonsi-
lence numbers of feature vectors for “h4e-98-1,” “h4e-98-2,”
and “SRE-01” were 551 019 frames, 545 700 frames, and
418 625 frames, respectively.
B. Performance evaluation metrics
The performance of speaker clustering was evaluated on
the basis of two metrics: cluster purity,7,43,44 and the Rand
index.7,45,46 Cluster purity is the probability that if we pick
any utterance from a cluster twice at random, with replace-
ment, both of the selected utterances are from the same
speaker. Specifically, the purity of cluster cm is computed by
m = 
p=1
P
nmp
2
n
m*
2 . 29
where nm* is the total number of utterances in cluster cm, nmp
is the number of utterances in cluster cm produced by the pth
speaker, and P is the total number of speakers. From Eq.
29, it follows that n
m*
−1
m1, in which the upper bound
and lower bound reflect that all the within-cluster utterances
were produced by the same speaker and completely different
speakers, respectively. To evaluate the overall performance
of M-clustering, we compute an average purity
¯ =
1
Nm=1
M
nm*m. 30
The Rand index used in this study follows Ref. 7, which
indicates the level of disagreement in a partitioning. How-
ever, for ease of performance comparison, we represent the
disagreement as a probability instead of the number of utter-
ance pairs originally used in Ref. 7. Specifically, the Rand
index is defined by the probability that two randomly se-
lected utterances from the same speaker are placed in differ-
ent clusters, or that two randomly selected utterances placed
in the same cluster are from different speakers:
R =
m=1
M
nm*
2 + p=1
P
n*p
2
− 2m=1
M p=1
P
nmp
2
m=1
M
nm*
2 + p=1
P
n*p
2
, 31
where n*p is the number of utterances from the pth speaker.
The lower the value of R, the better the clustering perfor-
mance. Perfect clustering should produce a Rand index of
zero.
Note that the cluster purity and Rand index defined
above are calculated without taking the length of utterance
into account. However, in many applications, assigning a
long utterance into a wrong cluster can be more detrimental
than assigning a short utterance into a wrong cluster. To re-
flect this matter, we further compute the two metrics on the
basis of frame correctness. Specifically, a frame-based clus-
ter purity is defined by the probability that if we pick any
frame from a cluster twice at random, with replacement, bothJ. Acoust. Soc. Am., Vol. 120, No. 3, September 2006m*
the total number of frames in cluster cm, and nmp represents
the number of frames in cluster cm produced by the pth
speaker. Likewise, a frame-based Rand index is defined by
the probability that two randomly selected frames from the
same speaker are placed in different clusters, or that two
random selected frames placed in the same cluster are from
different speakers. In general, when evaluating a certain clus-
tering result, the value of frame-based purity is larger than
that of utterance-based purity, while the value of the frame-
based Rand index is smaller than that of the utterance-based
Rand index.
C. Experimental results
Our first experiment was conducted to assess the
speaker-clustering performance by assuming that the total
number of speakers is known; hence, the required number of
clusters can be specified as a priori. Figures 3 and 4 show
the performance of agglomerative hierarchical clustering for
subsets “h4e-98-2” and “SRE-01,” respectively, in which the
numbers of clusters were specified as 89 and 15. We exam-
ined different intercluster similarity measures along with
GLRs computed with different numbers of component den-
sities in Gaussian mixture modeling. The “concatenation” in
Figs. 3 and 4 stands for the intercluster similarity measured
by concatenating all the utterances within each cluster into a
long utterance, and then computing the GLR between the
concatenated utterances. Except for the single-Gaussian
models number of Gaussian mixtures=1, which were full-
covariance structures and trained via maximum likelihood
estimation, all the GMMs number of Gaussian mixtures
2 used in this study comprised diagonal covariance matri-
ces trained via MAP-adaptation from an utterance-
independent GMM. We observe from Figs. 3 and 4 that, of
the three linkages, complete linkage performs the best, which
almost always yields larger values of purity and smaller val-
ues of the Rand index than those of the others; single linkage
performs the worst, and average linkage is between the two
extremes. It can also be seen from the figures that concatena-
tion surpasses complete linkage in terms of the largest value
of purity and smallest value of the Rand index that can be
produced. However, there were no consistent results that
could indicate the optimal number of Gaussian mixtures used
in GLR computation.
Figures 5 and 6 show the speaker-clustering results ob-
tained by our proposed methods. Here, GLR-HC concatena-
tion represents concatenation shown in Figs. 3 and 4. In GA
optimization, the parameter values used for the maximum
number of generations Q, the chromosome population size Z,
the crossover probability, and the mutation probability were
determined to be 4,000, 200, 0.32, and 0.2, respectively, ac-
cording to the test on subset h4e-98-1. We can see from Figs.
5 and 6 that both MLC and MDC yield larger values of
purity and smaller values of the Rand index than most
GLR-HC concatenation cases can attain. Table I summarizes
the individual best speaker-clustering performance that
MLC, MDC, and GLR-HC concatenation achieved, in whichW. Tsai and H. Wang: Speaker clustering 1639
FIG. 5. Color online Performance of
the proposed maximum likelihood
clustering MLC and minimum diver-
gence clustering MDC for subset
“h4e-98-2,” in which “GLR-HC con-
catenation” represents “concatenation”
shown in Fig. 2. a Utterance-based
purity. b Frame-based purity. c
Utterance-based Rand index. d
Frame-based Rand index.FIG. 4. Color online Performance of agglomerative
hierarchical clustering for subset “SRE-02” in which
the number of clusters was specified as the true size of
speaker population, i.e., 15. Except for the single-
Gaussian models no. of Gaussian mixtures=1, which
were full-covariance structures and trained via maxi-
mum likelihood estimation, all the GMMs no. of
Gaussian mixtures2 comprised diagonal covariance
matrices trained via MAP adaptation. a Utterance-
based purity. b Frame-based purity. c Utterance-
based Rand index. d Frame-based Rand index.J. Acoust. Soc. Am., Vol. 120, No. 3, September 2006 W. Tsai and H. Wang: Speaker clustering 1641
data containing multiple speakers. This could be done by
either assigning each utterance to multiple related clusters,47
or presegmenting utterances into small speaker-
homogeneous regions and then clustering those regions. In
parallel, speaker segmentation may be improved with the aid
of speaker clustering.12 Specifically, speech segments as-J. Acoust. Soc. Am., Vol. 120, No. 3, September 2006signed to each cluster can be used to train a speaker-related
model, thereby examining the speaker change boundaries of
an audio recording in a manner of frame-by-frame recogni-
tion. Speaker clustering can then be performed on the up-
dated speech segments, and the segmentation and clustering
procedures repeated iteratively to attain the goal of speaker
FIG. 7. Color online Performance of
clustering subset “h4e-98-2” as a func-
tion of the number of clusters, in
which the number of component den-
sities used in Gaussian mixture model-
ing was 1 single-Gaussian model with
full covariance matrix. a Utterance-
based purity. b Frame-based purity.
c Utterance-based Rand index. d
Frame-based Rand index.
FIG. 8. Color online Performance of
clustering subset “SRE-02” as a func-
tion of the number of clusters, in
which the number of component den-
sities used in Gaussian mixture model-
ing was 32. a Utterance-based purity.
b Frame-based purity. c Utterance-
based Rand index. d Frame-based
Rand index.W. Tsai and H. Wang: Speaker clustering 1643
effects of the communication channel in auditory-like analysis of speech,”
Proc. European Conference on Speech Communication and Technology
(Eurospeech) 1991.
28R. Sinha, S. E. Tranter, M. J. F. Gales, and P. C. Woodland, “The Cam-
bridge University March 2005 Speaker Diarisation System,” Proc. Euro-
pean Conference on Speech Communication and Technology (Eurospeech)
2005.
29X. Zhu, C. Barras, S. Meignier, and J. L. Gauvain, “Combining speaker
identification and BIC for speaker diarization,” Proc. European Confer-
ence on Speech Communication and Technology (Eurospeech) 2005.
30S. E. Johnson and P. C. Woodland, “Speaker clustering using direct maxi-
misation of the MLLR-adapted likelihood,” Proc. International Confer-
ence on Spoken Language Processing (ICSLP) 1998.
31J. Pelecanos and S. Sridharan, “Feature warping for robust speaker verifi-
cation,” Proc. ISCA Odyssey Workshop 2001.
32D. E. Goldberg, Genetic Algorithm in Search, Optimization and Machine
Learning Addison-Wesley, New York, 1989.
33L. Kaufman and P. J. Rousseuw, Finding Groups in Data: An Introduction
to Cluster Analysis Wiley, New York, 1990.
34J. E. Baker, “Adaptive selection methods for genetic algorithm,” Proc.
International Conference on Genetic Algorithms and Their Applications
1985.
35A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from incom-
plete data via the EM algorithm,” J. R. Stati. Soc. 391, 1–38 1977.
36S. Kullback, Information Theory and Statistics Dover, New York, 1968.
37C. S. Huang, H. C. Wang, and C. H. Lee, “A study on model-based error
rate estimation for automatic speech recognition,” IEEE Trans. Speech
Audio Process. 116, 581–589 2003.
38F. Bimbot, I. Magrin-Chagnolleau, and L. Mathan, “Second-order statisti-
cal measures for textindependent speaker identification,” Speech Com-
mun. 17, 177–192 1995.
39H. Gish, “Robust discrimination in automatic speaker identification,”
Proc. IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP) 1990.
40G. Schwarz, “Estimating the Dimension of a Model,” Ann. Stat. 6, 461–
464 1978.
41LDC, http://www.ldc.upenn.edu/.
42The VIMAS speech codec, http://www.vimas.com
43J. Ajmera, H. Bourlard, I. Lapidot, and I. McCowan, “Unknown-multiple
speaker clustering using HMM,” Proc. International Conference on Spo-
ken Language Processing (ICSLP) 2002.
44I. Lapidot, “SOM as likelihood estimator for speaker clustering,” Proc.
European Conference on Speech Communication and Technology (Euro-
speech) 2003.
45L. Hubert and P. Arabie, “Comparing Partitions,” J. Classif. 2, 193–218
1985.
46W. M. Rand, “Objective criteria for the evaluation of clustering methods,”
J. Am. Stat. Assoc. 66, 846–850 1971.
47J. McLaughlin, D. Reynolds, E. Singer, and G. C. O’Leary, “Automatic
speaker clustering from multi-speaker utterances,” Proc. IEEE Interna-
tional Conference on Acoustics, Speech, and Signal Processing (ICASSP)
1999.J. Acoust. Soc. Am., Vol. 120, No. 3, September 2006 W. Tsai and H. Wang: Speaker clustering 1645
1462 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
[6], [17], have been examined and compared in much of the
literature. The generation of a cluster tree is done in either a
bottom-up (agglomerative) or a top-down (divisive) fashion,
according to some criteria derived from the similarity measure.
The bottom-up approach starts with each utterance as a single
cluster, and then successively merges the clusters in a pairwise
manner until one cluster contains all the utterances. In the
top-down approach, however, all the utterances start in a single
cluster, and the clusters are successively split until each cluster
contains exactly one utterance. The resulting cluster tree is then
cut via an estimation of the number of clusters to retain the best
partition. Representative methods for estimating the optimal
number of clusters are based on the BBN Metric [6] and the
Bayesian information criterion [7].
Among the three components in the above clustering frame-
work, the computation of inter-utterance similarities is of partic-
ular importance, which determines whether the generated clus-
ters are related to various speakers, rather than other acoustic
classes. However, existing similarity measures, based on KL
distance, CLR, or GLR, are performed entirely on the spec-
trum-based features, which are known to carry various types
of information besides a speaker’s voice characteristics, for ex-
ample, phonetic and environmental information. As a conse-
quence, speaker-clustering systems based on these similarity
measures are usually vulnerable when the utterances to be clus-
tered are short and noisy. To alleviate the problem, this study
proposes a novel inter-utterance similarity measurement, which
is carried out by projecting the utterances to be clustered onto
a voice characteristic reference space, and then examining the
degree of coincidence between the projection results of the ut-
terances. As will be illustrated in the following sections, the ref-
erence space is trained to cover the generic voice characteristics
inherent in all the utterances to be clustered; hence, the resulting
similarity measurement will be more robust against interference
from nonspeaker factors.
In addition to developing a more reliable inter-utterance
similarity measurement, we also investigate how to optimally
generate the clusters such that all the within-cluster utterances
are from the same speaker. Conventional approaches based
on either top-down or bottom-up hierarchical clustering use
a nearest neighborhood selection rule to determine which
utterances should be assigned to the same cluster. However,
during the procedure of splitting one cluster or merging two
clusters, the nearest-neighborhood selection rule is applied in a
cluster-by-cluster manner, rather than in a global manner that
considers all the clusters. As a result, hierarchical clustering
can only make each individual cluster as homogeneous as
possible, but cannot attain the ultimate goal of maximizing the
overall homogeneity. To solve this problem, we propose a new
clustering method that explicitly aims to maximize the total
number of within-cluster utterances from the same speakers.
This is done by estimating the so-called cluster purity [6], in
conjunction with an optimization process based on a genetic
algorithm [22], to find the best partitioning of utterances that
achieves maximal cluster purity.
The rest of this paper is organized as follows. Section II de-
scribes the specific problem we address, an overview of the
clustering framework we propose, and the performance assess-
ment method we use in this study. Section III introduces several
methods for creating a reference voice space that represents an
utterance as a projection vector, thereby measuring the similari-
ties between utterances. In Section IV, we describe how to gen-
erate clusters in accordance with the criteria derived from the
inter-utterance similarities. Section V discusses the problem of
how to automatically determine the appropriate number of clus-
ters. Section VI presents our experimental results. Finally, in
Section VII, we present our conclusions, and discuss the direc-
tion of future works.
II. TASK DEFINITION AND METHOD OVERVIEW
Let denote unlabeled speech utter-
ances in a certain spectrum-based feature representation, each
of which is produced by one of the speakers ,
where , and is unknown. The aim of speaker
clustering is to partition the utterances into clusters
, such that and each cluster consists
exclusively of utterances from only one speaker. For those
utterances that contain multiple speakers, the partitioning is
preferably performed after the utterances are presegmented into
speaker-homogeneous regions.1 However, in order to focus on
the fundamental techniques for speaker clustering, this study
does not investigate the speaker-segmentation problem and
only deals with utterances containing a single speaker.
The performance2 of speaker clustering is evaluated on the
basis of cluster purity [6], which indicates the level of agreement
in a cluster. For a cluster , the purity is computed by
(1)
where is the total number of utterances in cluster , and
is the number of utterances in cluster that are pro-
duced by speaker . From (1), it follows that ,
in which the upper bound and lower bound reflect that all the
within-cluster utterances were produced by the same speaker
or completely different speakers, respectively. To evaluate the
overall performance of -clustering, we compute an average
purity:
(2)
Fig. 1 shows the proposed speaker-clustering framework.
Prior to the inter-utterance similarity computation, a refer-
ence space, which represents some generic characteristics of
speakers’ voices, is constructed. The reference space is com-
posed of bases, where the basis is a general term referring
to a representative of the voice characteristics encoded in the
spectrum-based features. The reference space can be created
using either the set of utterances to be clustered or an extra
1Interested readers are referred to [9] for the study of clustering multi-speaker
utterances.
2Depending on the application, there are a number of other ways to evaluate
the speaker-clustering performance, such as the misclassification rate [17], clus-
tering efficiency [8], and the Rand Index [23]. This study chooses cluster purity,
because its computation is more application-independent and its scale is more
easily perceivable.
1464 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
Fig. 2. Example of a projection using a reference space constructed by utter-
ance-dependent Gaussian mixture modeling, in which the projection vectors
computed from a collection of nine utterances are shown in gray-scale repre-
sentation.
reliable similarity measure can be derived. Fig. 2 shows an ex-
ample of projection carried out on a collection of nine utterances
from three speakers. Dark regions in the resulting likelihood pat-
tern represent large likelihood values, while light regions repre-
sent small values. We can see from the likelihood pattern that
the whole projection vectors pertaining to the same speakers are
more similar than those pertaining to different speakers.
The concept of the projection method in Fig. 2 is adapted
from a prior study reported in [28]. A similar idea has also
been presented recently from the viewpoint of so-called trian-
gulation [16], in which each utterance is modeled as a single
Gaussian distribution. It is clear from speaker-recognition
research that a better speaker clustering performance may be
obtained by using a proper number of Gaussian components
in a mixture, rather than a single Gaussian density. However,
determining the proper number of Gaussian components in
GMMs is a difficult problem, especially when the duration of
the utterances might be rather diverse. Specifically, choosing
a larger number of Gaussian components is advantageous for
modeling the voice characteristics of long utterances more
accurately, but it is disadvantageous for the short utterances
that lack data for GMM parameter estimation. Meanwhile,
choosing too few Gaussian components may make it difficult to
distinguish between different-speaker utterances. We therefore
develop several alternative methods in the following subsec-
tions to sidestep this problem.
B. Utterance-Independent Vector Clustering
Instead of using utterance-dependent GMMs, we can create
a single, utterance-independent codebook with codewords
as a reference space using all the feature vectors of the utter-
ances to be clustered. The codebook can be considered as a uni-
versal model trained to cover the speaker-independent distribu-
tion of feature vectors. In our implementation, each codeword
consists of a mean vector and a diagonal co-
variance matrix . Training of the codebook is performed via
-means clustering algorithm, in which the distance between
feature vectors is computed on the basis of Mahalanobis dis-
tance.
The use of such a codebook-based reference space is mo-
tivated by the observation that, although the codebook as a
whole is a speaker-independent representation, a significant
proportion of the individual codewords tend to be speaker-de-
pendent. Specifically, we found that after vector clustering,
the feature vectors pertaining to a particular speaker do not
spread uniformly over all the clusters, but distribute primarily
in certain clusters. In other words, each of the speakers reflects
his/her own set of favorable codewords. This might be because
in unsupervised training of the codebook, there is usually more
than one codeword to represent a certain type of phonetic real-
ization. Many codewords that correspond to identical phonetic
fragments are generated to cover the variations of different
speakers or environmental conditions. This results in codewords
that are phonetically related, as well as speaker-related.
Viewing each codeword as a basis of the reference space, the
similarity between utterances can be measured by comparing
the distribution for the feature vectors of each utterance in the
codebook. To do this, each feature vector is assigned an index of
the closest codeword in terms of the Mahalanobis distance. The
projection value for each utterance with respect
to basis , can then be computed by using
(5)
C. Utterance-Independent Gaussian Mixture Modeling
Followed By Maximum A Posteriori (MAP) Adaptation
Alternatively, the problem concerning diverse utterance du-
ration mentioned in Section III-A might be handled better by
using some model-adaptation techniques developed in speech
or speaker recognition research. Our basic strategy is to create
an utterance-independent GMM using all the utterances to be
clustered, followed by an adaptation of the utterance-indepen-
dent GMM for each of the utterances using MAP estimation.
Let denote the parameter set
of an utterance-independent GMM having Gaussian compo-
nents, where is the mixture weight, the mean vector, and
the covariance matrix. For each utterance , with fea-
ture vectors , we compute the a posteriori
probability of each feature vector , in the Gaussian compo-
nent of GMM :
(6)
1466 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
is a tree of clusters. The final partition of the utterances is then
determined by pruning the tree that only has leaves left.
B. Maximum Purity Clustering
The principle behind hierarchical clustering is to make the
agreement within a newly generated cluster as large as possible.
However, when each agglomeration is performed, the hierar-
chical clustering cannot guarantee that the overall within-cluster
agreement will be maximized, since its decision does not con-
sider the interaction between the new cluster to be generated
and the existing clusters. Therefore, this clustering method is
not optimal. In particular, some misclustering errors, arising
from grouping different-speaker utterances together, can propa-
gate down the whole process, and limit the clustering perfor-
mance. To overcome this limitation, we propose a new clus-
tering method, which considers how to assign utterances to clus-
ters in a global fashion such that the overall within-cluster agree-
ment can be maximized.
Let denote the index of the cluster where an utterance
is located, and denote the true speaker index of utterance .
Note, is an integer between 1 and when the number of
clusters is specified a priori, and is an integer between
1 and if there are speakers involved. Our aim is to find a
set of cluster indices assigned for
utterances to be clustered that maximizes the average cluster
purity defined by (2), i.e.,
(19)
where is a Kronecker Delta function.
However, as the computation of cluster purity requires
that the true speaker of each utterance is known in ad-
vance, it is impossible to find from (19) directly. To
make this equation solvable, we need to estimate the term
in the absence of the ground
truth. Since
(20)
the estimation of hinges on
how to determine the term when utterances and
are located in the cluster . Motivated by Solomonoff et
al.’s work [6], we determine by using the following
approximation:
if
if , and
otherwise
(21)
where denotes the rank of inter-ut-
terance similarity among ,
in descending order, and is
the utterance most similar to , i.e., .
Implicit in (21) is the idea that two utterances, and ,
can only be considered as being from the same speaker
if the similarity is high enough to satisfy
. In addition, to avoid a possible
misjudgement arising from an oversized , we approximate
as the probability that utterances and belong
to the same speaker. This possibility is measured by comparing
the similarity with that of the two utterances that
most likely belong to the same speaker, i.e., . With
this approximation, an optimal may be found according to
(22)
We note that the solution to (22) remains nontrivial, since
a gradient-based optimization cannot be used in this scenario.
Moreover, it is infeasible to perform an exhaustive search that
would examine all possible solutions to determine the best one,
because there are possible combinations of cluster indices.
To overcome these difficulties, we apply the genetic algorithm
(GA) [22] to find by using its global scope and parallel
searching power.
The basic operation of the GA is to explore a given search
space in parallel by means of iterative modifications of a popu-
lation of chromosomes. Each chromosome, encoded as a string
of alphabets or real numbers called genes, represents a potential
solution to a given problem. In our task, a chromosome is ex-
actly a legitimate , and a gene corresponds to a cluster index
associated with an utterance. However, since the index of one
cluster can be interchanged with that of another cluster, multiple
chromosomes may lead to an identical clustering result. For ex-
ample, the chromosomes {1, 1, 1, 2, 2, 3, 3}, {1, 1, 1, 3, 3, 2,
2}, {2, 2, 2, 1, 1, 3, 3}, {2, 2, 2, 3, 3, 1, 1}, {3, 3, 3, 2, 2, 1, 1},
and {3, 3, 3, 1, 1, 2, 2} represent the same clustering result of
grouping seven utterances into three clusters. Such a nonunique
representation of the solution would significantly increase the
GA search space, and could lead to an inferior clustering result.
To avoid this problem, we limit the inventory of chromosomes
to conform to a baseform representation defined as follows. Let
1468 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
indices for the utterances, ,
the utterances tagged with the same indices come from the
same speakers. Suppose that the true speakers of utterances,
, are statistically independent of each other. We
compute the likelihood by
(26)
where denotes the true speaker of an arbitrary utterance
tagged with index , and
represents the probability that, given an arbitrary utterance
tagged with the same index as utterance , the true speakers of
utterances and are the same. For computational efficiency,
all the probabilities , ,
are approximated by ,
which is the probability that any two utterances and
tagged with the same index come from the same speaker.
Assume that there are utterances tagged with , and
among these utterances there are utterances produced
by speaker . If we pick one of the utterances twice at
random, with replacement, the probability that both of the
picked utterances come from is .
Thus, the probability that two utterances tagged with come
from the same speaker is , which is also
the cluster purity defined in (1). Since the probability that
one utterance tagged with is , we can estimate the
probability that any two utterances and tagged with the
same index come from the same speaker by
(27)
Approximating as in (24), the likelihood can
be obtained with . Accordingly, we can score a parti-
tioning of utterances having clusters via
BIC clustering (28)
The BIC-based score should increase with the increase of the
value in the beginning, but will decline significantly after an
excess of clusters is created. A reasonable number of clusters
can thus be determined by
BIC clustering (29)
VI. EXPERIMENTS
A. Speech Data
Our speech database comprised two subsets, respectively,
extracted from two corpora released by the Linguistic Data
Consortium [34]: the 1998 HUB-4 Broadcast News Evaluation
English Test Material (Hub4–98), which consists of broadcast
news speech recorded at a 16-kHz sampling rate, and the 2001
NIST Speaker Recognition Evaluation Corpus (SRE-01), which
consists of cellular telephone speech recorded at an 8-kHz sam-
pling rate. The first subset contained 428 speaker-homogeneous
utterances obtained by segmenting the episode “h4e-98-2” of
Hub4-98, according to the annotation file. This subset involved
89 speakers, in which the number of utterances spoken by each
speaker ranged from 1 to 27. The second subset, which stems
fromthe test setofSRE-01,contained197speaker-homogeneous
utterances spoken by 15 randomly selected male speakers.3 The
number of utterances spoken by each speaker ranged from 5 to
39. Speech features, each consisting of 24 Mel-scale frequency
cepstral coefficients (MFCCs), were extracted from these utter-
ances for every 20-ms Hamming-windowed frame with 10-ms
frame shifts. Prior to MFCC computation, voice active detection
[35] was applied to remove salient nonspeech regions that may
be included in an utterance.
B. Experimental Results
1) Comparison of Different Inter-Utterance Similarity Mea-
sures: Our first experiment was conducted to assess the perfor-
mance of various inter-utterance similarity measures proposed in
this study. Since a good similarity measure should consistently
produce larger values for similarities between utterances of the
same speaker and smaller values for similarities between utter-
ances of different speakers, the assessment can be done by exam-
ining if the produced values are well distinguished for these two
conditions. To do this, we compute two types of error probability
foreach inter-utterance similaritymeasure. One is the probability
that two same-speakerutterances are falsely judged as being from
different speakers (Type-I error), and the other is the probability
that two different-speaker utterances are falsely judged as being
from the same speaker (Type-II error). These two types of error
probability are subject to tradeoff, which can be represented by
the detection error tradeoff (DET) curve [36]. This representa-
tion takes as input the values of inter-utterance similarities, and
produces as output the corresponding Gaussian deviates of the
error probability distribution. The resulting DET curves are ap-
proximately straight lines, making it easier to visually compare
the performance of different methods.
Figs. 4 and 5 show the DET curves obtained by testing
subsets “SRE-01” and “h4e-98-2,” respectively. In the figures,
“UD-GMM,” “UI-VC,” “UI-GMM-ADA,” and “EV” denote the
methods used to create the reference spaces, respectively, by ut-
terance-dependent Gaussian mixture modeling (Section III-A),
utterance-independent vector clustering (Section III-B), ut-
terance-independent Gaussian mixture modeling followed by
MAP adaptation (Section III-C), and the eigenvoice-motivated
approach (Section III-D). In addition, “GLR” denotes the
generalized likelihood ratio-based similarity measure, which
served as a baseline for performance comparison. Briefly, GLR
between two utterances and is computed using
(30)
3Empirical evidence shows that male speakers usually can be well distin-
guished from female speakers. We therefore focused on investigating the speech
from single gender speakers.
1470 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
Fig. 5. DET curves obtained with various inter-utterance similarity measures for subset “h4e-98-2.” (a) GLR. (b) UD-GM. (c) UI-VC. (d) UI-GMM-AD. (e) EV.
(f) A summarization of the best performances in (a)–(e).
durations, we only examined each similarity measure in terms of
the average performance by using the same number of Gaussian
components. In addition, except for the single-Gaussian models,
which were full-covariance structures, all the GMMs used in
this study comprised diagonal covariance matrices. The thick
line in each figure indicates the best discriminating performance
1472 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
Fig. 7. Performance of speaker clustering obtained with various inter-utterance
similarity measures for subset “h4e-98-2.” (a) Complete linkage agglomerative
clustering. (b) Average linkage agglomerative clustering. (c) Single linkage ag-
glomerative clustering.
purity clustering method. In the GA optimization, the param-
eter values used for the maximum number of generations,
Fig. 8. Performance of speaker clustering obtained by conventional hierar-
chical clustering, and the proposed maximum purity clustering, in which the
inter-utterance similarities were computed on the basis of an eigenvoice-mo-
tivated reference space. (a) Tests on subset “SRE-01.” (b) Tests on subset
“h4e-98-2.”
the chromosome population size, the crossover probability,
and the mutation probability were empirically determined to
be 4000, 5000, 0.5, and 0.1, respectively. Fig. 8 shows the
results obtained with agglomerative hierarchical clustering
and maximum purity clustering, in which the inter-utterance
similarity was computed on the basis of the eigenvoice-mo-
tivated approach. It is clear that maximum purity clustering
outperforms agglomerative hierarchical clustering. When the
number of clusters was specified as equal to the speaker pop-
ulation size, we can see that the average purity was improved
from 0.72 (“SRE-01”) and 0.76 (“h4e-98-2”), yielded by the
agglomerative hierarchical clustering, to 0.81 (“SRE-01”) and
0.82 (“h4e-98-2”), yielded by the maximum purity clustering.
3) Automatic Determination of the Speaker Population Size:
Finally, the problem of automatically determining the speaker
population size was investigated. We computed the BIC-based
1474 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 15, NO. 4, MAY 2007
[4] H. Jin, F. Kubala, and R. Schwartz, “Automatic speaker clustering,” in
Proc. DARPA Speech Recognition Workshop, 1997, pp. 108–111.
[5] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, “Automatic segmen-
tation, classification and clustering of broadcast news audio,” in Proc.
DARPA Speech Recognition Workshop, 1997, pp. 97–99.
[6] A. Solomonoff, A. Mielke, M. Schmidt, and H. Gish, “Clustering
speakers by their voices,” in Proc. IEEE Int. Conf. Acoust., Speech,
Signal Process. (ICASSP), 1998, pp. 757–760.
[7] S. S. Chen and P. S. Gopalakrishnan, “Clustering via the Bayesian in-
formation criterion with applications in speech recognition,” in Proc.
IEEE Int. Conf. Acoust., Speech, Signal Process. (ICSLP), 1998, pp.
645–648.
[8] D. A. Reynolds, E. Singer, B. A. Carson, G. C. O’Leary, J. J.
McLaughlin, and M. A. Zissman, “Blind clustering of speech utter-
ances based on speaker and language characteristics,” in Proc. Int.
Conf. Spoken Lang. Process. (ICSLP), 1998, pp. 3193–3196.
[9] J. McLaughlin, D. Reynolds, E. Singer, and G. C. O’Leary, “Automatic
speaker clustering from multi-speaker utterances,” in Proc. IEEE Int.
Conf. Acoust., Speech, Signal Process. (ICASSP), 1999, pp. 817–820.
[10] S. E. Johnson, “Who spoke when?—Automatic segmentation and
clustering for determining speaker turns,” in Proc. Eur. Conf. Speech
Commun. Technol. (Eurospeech), 1999, pp. 2211–2214.
[11] B. Zhou and J. H. L. Hansen, “Unsupervised audio stream segmenta-
tion and clustering via the Bayesian information criterion,” in Proc. Int.
Conf. Spoken Lang. Process. (ICSLP), 2000, pp. 714–717.
[12] J. Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen, R. Schwartz, and
A. Srivastava, “Speech and language technologies for audio indexing
and retrieval,” Proc. IEEE, vol. 88, no. 8, pp. 1338–1353, Aug. 2000.
[13] R. Faltlhauser and G. Ruske, “Robust speaker clustering in
eigenspace,” in Proc. IEEE Workshop Automatic Speech Recog-
nition Understanding, 2001, pp. 57–60.
[14] I. Lapidot, H. Guterman, and A. Cohen, “Unsupervised speaker
recognition based on competition between self-organizing maps,”
IEEE Trans. Neural Networks, vol. 13, no. 4, pp. 877–887, Jul. 2002.
[15] J. Ajmera, H. Bourlard, I. Lapidot, and I. McCowan, “Unknown-mul-
tiple speaker clustering using HMM,” in Proc. Int. Conf. Spoken Lang.
Process. (ICSLP), 2002, pp. 573–576.
[16] Y. Moh, P. Nguyen, and J. C. Junqua, “Towards domain independent
speaker clustering,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal
Process. (ICASSP), 2003, pp. 85–88.
[17] D. Liu and F. Kubala, “Online speaker clustering,” in Proc. IEEE Int.
Conf. Acoust., Speech, Signal Process. (ICASSP), 2003, pp. 333–336.
[18] I. Lapidot, “SOM as likelihood estimator for speaker clustering,” in
Proc. Eur. Conf. Speech Commun. Technol. (Eurospeech), 2003, pp.
3001–3004.
[19] S. Kwon and S. Narayanan, “Unsupervised speaker indexing using
generic models,” IEEE Trans. Speech Audio Process., vol. 13, no. 1,
pp. 72–83, Jan. 2005.
[20] W. H. Tsai and H. M. Wang, “Speech utterance clustering based on the
maximization of within-cluster homogeneity of speaker voice charac-
teristics,” J. Acoust. Soc. Amer., vol. 120, no. 3, pp. 1631–1645, 2006.
[21] J. P. Campbell, “Speaker recognition: A tutorial,” Proc. IEEE, vol. 85,
no. 9, pp. 1437–1462, Sep. 1997.
[22] D. E. Goldberg, Genetic Algorithm in Search, Optimization and Ma-
chine Learning. New York: Addison-Wesley, 1989.
[23] L. Hubert and P. Arabie, “Comparing partitions,” J. Classification, vol.
2, pp. 193–218, 1985.
[24] W. H. Tsai, S. S. Cheng, and H. M. Wang, “Speaker clustering of
speech utterances using a voice characteristic reference space,” in Proc.
Int. Conf. Spoken Lang. Process. (ICSLP), 2004, pp. 2937–2940.
[25] W. H. Tsai, S. S. Cheng, Y. H. Chao, and H. M. Wang, “Clustering
speech utterances by speaker using eigenvoice-motivated vector space
models,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
(ICASSP), 2005, pp. 725–728.
[26] W. H. Tsai and H. M. Wang, “Speaker clustering of unknown ut-
terances based on maximum purity estimation,” in Proc. Eur. Conf.
Speech Commun. Technol. (Eurospeech), 2005, pp. 3096–3072.
[27] D. A. Reynolds and R. C. Rose, “Robust text-independent speaker iden-
tification using Gaussian mixture speaker models,” IEEE Trans. Speech
Audio Process., vol. 3, no. 1, pp. 72–83, Jan. 1995.
[28] W. H. Tsai, Y. C. Chu, C. S. Huang, and W. W. Chang, “Background
learning of speaker voices for text-independent speaker identifica-
tion,” in Proc. Eur. Conf. Speech Communication and Technology
(Eurospeech), 2001, pp. 767–771.
[29] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker verification
using adapted Gaussian mixture models,” Digital Signal Process., vol.
10, pp. 19–41, 2000.
[30] R. Kuhn, J. Junqua, P. Nguyen, and N. Niedzielski, “Rapid speaker
adaptation in eigenvoice space,” IEEE Trans. Speech Audio Process.,
vol. 8, no. 6, pp. 695–707, Nov. 2000.
[31] L. Kaufman and P. J. Rousseuw, Finding Groups in Data: An Intro-
duction to Cluster Analysis. New York: Wiley, 1990.
[32] J. E. Baker, “Adaptive selection methods for genetic algorithm,”
in Proc. Int. Conf. Genetic Algorithms Their Applicat., 1985, pp.
101–111.
[33] G. Schwarz, “Estimating the dimension of a model,” Ann. Statist., vol.
6, pp. 461–464, 1978.
[34] LDC. [Online]. Available: http://www.ldc.upenn.edu/
[35] The VIMAS Speech Codec.
[36] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przy-
bocki, “The DET curve in assessment of detection task performance,”
in Proc. Eur. Conf. Speech Commun. Technol. (Eurospeech), 1997, pp.
1895–1898.
Wei-Ho Tsai (M’04) received the B.S. degree in
electrical engineering from National Sun Yat-Sen
University, Kaohsiung, Taiwan, R.O.C., in 1995
and the M.S. and Ph.D. degrees in communication
engineering from National Chiao-Tung University,
Hsinchu, Taiwan, in 1997 and 2001, respectively.
From 2001 to 2003, he was with Philips Research
East Asia, Taipei, Taiwan, where he worked on
speech processing problems in embedded systems.
From 2003 to 2005, he served as Postdoctoral Fellow
at the Institute of Information Science, Academia
Sinica, Taipei, Taiwan. He is currently an Assistant Professor at the Department
of Electronic Engineering and Graduate Institute of Computer and Commu-
nication Engineering, National Taipei University of Technology, Taiwan. His
research interests include spoken language processing and music information
retrieval.
Shih-Sian Cheng received the B.S. degree in mathe-
matics from National Kaohsiung Normal University,
Taiwan, R.O.C., in 2000 and the M.S. degree in com-
puter science from National Chiao-Tung University,
Hsinchu, Taiwan, in 2002. He is currently pursuing
the Ph.D. degree in the Department of Computer Sci-
ence, National Chiao Tung University, Taiwan.
In 2002, he joined the Spoken Language Group,
Chinese Information Processing Laboratory, Institute
of Information Science, Academia Sinica, Taiwan, as
a Research Assistant. His research interests include
pattern recognition, speech processing, and neural networks.
Hsin-Min Wang (S’92–M’95–SM’05) received the
B.S. and Ph.D. degrees in electrical engineering from
National Taiwan University, Taipei, Taiwan, R.O.C.,
in 1989 and 1995, respectively.
In October 1995, he joined the Institute of Infor-
mation Science, Academia Sinica, Taipei, Taiwan,
as a Postdoctoral Fellow. He was promoted to Assis-
tant Research Fellow and then Associate Research
Fellow in 1996 and 2002, respectively. He is an
adjunct Associate Professor with National Taipei
University of Technology and National Chengchi
University. His major research interests include speech processing, natural
language processing, spoken dialogue processing, multimedia information
retrieval, and pattern recognition. He currently serves as an editorial board
member of the International Journal of Computational Linguistics and Chinese
Language Processing and as a board member and chair of the academic council
of the ACLCLP.
Dr. Wang was a recipient of the Chinese Institute of Engineers (CIE) Tech-
nical Paper Award in 1995. He is a life member of ACLCLP and IICM and a
member of ISCA.
表 Y04 
 
二、發表論文 
 
1. Improved Methods For Characterizing The Alternative Hypothesis Using Minimum 
Verification Error Training For LLR-Based Speaker Verification：Likelihood ratio 
( )|(log)λ|(log)( λUpUpUL −= )是目前最常被採用的語者確認方法，其中U 是輸入語音， )λ|(Up
是被宣稱使用者的模型輸出的 likelihood， )|( λUp 則是非該使用者的模型輸出的
likelihood。過去幾年，學者專家陸陸續續提出各種評估λ 的方式，但沒有一種被公認對
所有案例都是最有效的。這些方法可以歸納成通式： ))|( ),...,|(()|( 1 BUpUpHUp λλλ = ，其
中 H是一個計算 B個背景模型(background models) {λ1, λ2,…, λB}獲得的 likelihoods的一
個轉換函數，例如算術平均、幾何平均、取最大值等。本論文將該通式表示成
∑=
=
B
i
ii UpwUp
1
)|()|( λλ 或是 ∏=
=
B
i
w
i
iUpUp
1
)|()|( λλ ，並利用最小化確認錯誤(minimum 
verification error, MVE)訓練法來求得權重值(wi)的解，在語者確認實驗上獲得相當不錯的
辨識率提升。 
 
2. Phonetic Boundary Refinement Using Support Vector Machine：目前最廣為使用的自動音
素分段方式是利用以最大化相似度(maximum likelihood, ML)法則訓練得到的音素 HMM
模型，透過維特比(Viterbi)演算法，在語音信號上根據人工標記的音素序列(phoneme 
sequence)進行強迫校準(forced alignment)。由於ML訓練法則與希望達成的音素邊界誤
差最小的目標並不一致，傳統的 HMM模型切音無法提供可靠的精確度。有鑒於此，我
們過去提出以最小化邊界誤差(minimum boundary error, MBE)訓練法來取代傳統的ML
訓練方式。在強迫校準階段則將減損函數定義為實際邊界誤差，在包含大量候選邊界序
列的音素網格圖上進行基於MBE 的音素強迫校準(forced alignment)搜尋。本論文進一
步結合 SVM後處理進行邊界微調校正。 
 
3. Speaker Clustering Based on Minimum Rand Index：語者分群（或稱「非監督式的語者
辨認」）的目的是要將屬於相同語者的語音區段集中至同一群中，而屬於不同語者的語
音區段則被分開至不同群。語者分群的問題主要有三：(1)音段與音段之間的距離量測、
(2)分群演算法、(3)最佳群數決定。傳統的作法是先選定一種距離量測，利用由下而上或
是由上而下的階層式分群演算法建立分群樹，然後利用模型選擇法來決定最佳群數。本
論文利用待分群音段間的整體關連特性取代單一音段對距離計算，克服短音段間距離估
算的困難。有鑑於傳統的階層式分群法免不了會有錯誤傳遞(error propagation)的問題，
而常用的基於貝氏資訊基準(Bayesian information criterion, BIC)的群數決定法也不是很
可靠，我們進一步提出一個基於最小化 Rand Index的分群法，此法最大的優點是將分群
與最佳群數決定合而為一，因此不會發生錯誤傳遞，最後，這個分群法利用基因演算法
來實現。 
 
三、與會心得與建議 
 
由 IEEE Signal Processing Society所舉辦的聲學語音信號處理國際研討會(ICASSP)是國
際上關於信號處理及其相關應用研究規模最大，最負盛名的學術研討會。語音處理研究
領域的學者亦視其為年度最重要的研討會，所有知名學者專家幾乎都會與會發表最新研
究成果及互相討論、交換研究心得。此次會議台灣地區與會人數很多，除中研院外，包
括台大、清大、交大、成大、師大、台科大、中正、暨南大學等都有多位師生出席。
ICASSP2009 已由台大李琳山教授爭取在台北舉辦，故此行的台灣同行除發表論文外，
也同時背負觀摩的任務，讓 2009年在台北舉行的 ICASSP圓滿成功是大家共同的責任。
IMPROVED METHODS FOR CHARACTERIZING THE ALTERNATIVE HYPOTHESIS 
USING MINIMUM VERIFICATION ERROR TRAINING  
FOR LLR-BASED SPEAKER VERIFICATION 
 
Yi-Hsiang Chao1,2, Wei-Ho Tsai3, Hsin-Min Wang1 and Ruei-Chuan Chang1,2 
 
1 Institute of Information Science, Academia Sinica, Taipei, Taiwan 
2 Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
3 Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan 
{yschao,whm}@iis.sinica.edu.tw, whtsai@en.ntut.edu.tw, rc@cc.nctu.edu.tw 
 
ABSTRACT 
 
Speaker verification based on the log-likelihood ratio (LLR) is 
essentially a task of modeling and testing two hypotheses: the null 
hypothesis and the alternative hypothesis. Since the alternative 
hypothesis involves unknown imposters, it is usually hard to 
characterize a priori. In this paper, we propose a framework to 
better characterize the alternative hypothesis with the goal of 
optimally separating client speakers from imposters. The proposed 
framework is built on either a weighted arithmetic combination or 
a weighted geometric combination of useful information extracted 
from a set of pre-trained anti-speaker models. The parameters 
associated with the combinations are then optimized using 
Minimum Verification Error training such that both the false 
acceptance probability and the false rejection probability are 
minimized. Our experiment results show that the proposed 
framework outperforms conventional LLR-based approaches. 
 
Index Terms— Speaker recognition, minimization methods, 
hypothesis testing, minimum verification error. 
 
1. INTRODUCTION 
 
Speaker verification is usually formulated as a statistical 
hypothesis testing problem and solved using a log-likelihood ratio 
(LLR) test [1]. Given an input utterance U, the LLR test for 
determining whether or not U is spoken by the hypothesized 
speaker is performed as follows 


<
≥=
, )reject  ( accept   
                    accept   
  
)|(
)|(
log)(
01
0
1
0
HH
H
HUp
HUp
UL θ
θ
      (1) 
where H0 represents that U is spoken by the hypothesized speaker 
(called the null hypothesis); H1 represents that U is not spoken by 
the hypothesized speaker (called the alternative hypothesis); 
)|( iHUp , i = 0 or 1, is the likelihood of hypothesis Hi given 
utterance U; and θ  is a decision threshold. In practical 
implementations, H0 and H1 are usually characterized by some 
parametric models, such as Gaussian mixture models (GMMs) [1]. 
However, even though H0 can be modeled straightforwardly using 
speech utterances from the hypothesized speaker, H1 does not 
involve any specific speaker, and hence lacks explicit data for 
modeling. Thus, a number of approaches have been proposed to 
better characterize H1. The common strategy is to generate one or 
multiple models using speech from a large number of non-
hypothesized speakers, and then compute the likelihood P(U | H1) 
using [2]: ( ),)λ|( )...,λ|(),λ|()|( 211 NUpUpUpHUp Ψ=          (2) 
where Ψ(⋅) denotes a certain function of the likelihoods computed 
for a set of background models {λ1, λ2,…, λN} representing the 
potential imposters. For example, if Ψ(⋅) is an arithmetic mean [1], 
the LLR is of the form 
,)λ|(1log)λ|(log)(
1
1 


 ∑−=
=
N
i
iUpN
UpUL             (3) 
where λ denotes a model generated for the hypothesized speaker. 
Alternatively, the arithmetic mean can be replaced by a maximum 
function [4], which yields the LLR  
),λ|(logmax)λ|(log)(
1
2 i
Ni
UpUpUL
≤≤
−=                (4) 
or by a geometric mean [5], which yields the LLR 
.)λ|(log1)λ|(log)(
1
3 ∑−= =
N
i
iUpN
UpUL                (5) 
A special case arises when N = 1, where a single background 
model is usually trained by pooling all the available data; this is 
called a world model [2]. The LLR in this case becomes 
),|(log)λ|(log)(4 Ω−= UpUpUL                         (6) 
where Ω denotes the world model. 
However, there is no theoretical evidence to indicate which 
method of characterizing H1 is optimal, and the selection of Ψ(⋅) is 
usually application and training data dependent. In particular, a 
simple function, such as the arithmetic mean, the maximum, or the 
geometric mean, is a heuristic that does not involve an 
optimization process. Thus, the resulting system is far from 
optimal in terms of verification accuracy. To better handle this 
problem, we propose a framework that characterizes the alternative 
hypothesis by exploiting information available from background 
models, such that utterances from the imposters and the 
hypothesized speaker can be separated more effectively. The 
framework is built on either a weighted geometric combination or 
a weighted arithmetic combination of the likelihoods computed for 
background models. In contrast to the geometric mean L3(U) or the 
arithmetic mean L1(U), which are independent of the system 
training, our combination scheme treats the background models 
unequally according to how close each individual is to the 
hypothesized speaker model, and quantifies the unequal nature of 
the background models by a set of weights optimized in the 
training phase. The optimization is carried out by Minimum 
If WAC is used, then 
.
)λ|(
)λ|(
)λ|(log ∑
−=


∑∂
∂−=∂
∂
j
jj
i
j
jj
ii Upw
Up
Upw
ww
L        (15) 
If WGC is used, then 
).λ|(log)λ|(log i
j
jj
ii
UpUpw
ww
L −=


∑∂
∂−=∂
∂        (16) 
The threshold θ in Eq. (9) can be estimated using [7]: 
,
)()1(
θηθθ ∂
∂−=+ UDkk                                 (17) 
where 
[ ]
[ ]. ))((1))((1                
))((1))((1              
)(
1
0
1
1
0
0
1
1
0
0
∑ −−⋅⋅−
∑ −−−⋅⋅=
∂
∂⋅∂
∂⋅∂
∂⋅∂
∂+∂
∂⋅∂
∂⋅∂
∂⋅∂
∂=∂
∂
∈
∈
HU
HU
ULsULsa
N
x
ULsULsa
N
x
L
L
d
d
s
s
xL
L
d
d
s
s
xUD θθθ
ll
          
(18)
 
In our implementation, the overall expected loss is set 
according to the Detection Cost Function (DCF) [8]: 
),1(            arg
arg
etTFalseAlarmFalseAlarm
etTMissMissDET
PPC
PPCC
−××+
××=
            (19) 
where PMiss is the miss (false rejection) probability, PFalseAlarm is the 
false alarm (false acceptance) probability, PTarget is the a priori 
probability of the target (hypothesized) speaker, and CMiss and 
CFalseAlarm are the relative costs of the missed error and false alarm 
error, respectively. A special case of DCF is known as the Half 
Total Error Rate (HTER), where CMiss and CFalseAlarm are both equal 
to 1, and PTarget = 0.5, i.e., HTER = (PMiss + PFalseAlarm) / 2. Then, 
approximating PMiss and PFalseAlarm by l0(U) and l1(U), respectively, 
we set the overall expected loss specifically as
 
).1()(             
)()(
arg1
arg0
etTFalseAlarm
etTMiss
PUC
PUCUD
−××+
××=
l
l
       (20) 
 
4. EXPERIMENTS 
 
4.1. Experiment setup 
 
We conducted speaker-verification experiments on speech data 
extracted from the XM2VTSDB multi-modal database [10]. In 
accordance with “Configuration II” described in [10], the database 
was divided into three subsets: “Training”, “Evaluation”, and 
“Test”. We used “Training” to build each client model and the 
background models, and used “Evaluation” to optimize the weights 
wi in Eq. (7) or Eq. (8), along with the threshold θ. Then, the 
speaker verification performance was evaluated on “Test”. As 
shown in Table 1, a total of 293 speakers1 in the database were 
divided into 199 clients, 25 “evaluation impostors”, and 69 “test 
impostors”. Each speaker participated in 4 recording sessions at 
about one-month intervals, and each recording session consisted of 
2 shots. In each shot, the speaker was prompted to utter 3 
sentences “0 1 2 3 4 5 6 7 8 9”, “5 0 6 9 2 8 1 3 7 4”, and “Joe took 
                                                 
1 We discarded 2 speakers (ID numbers 313 and 342) because of 
partial data corruption. 
father’s green shoe bench out”. Each utterance, sampled at 32 kHz, 
was converted into a stream of 24-order feature vectors, each 
consisting of 12 Mel-scale frequency cepstral coefficients [11] and 
their first time derivatives, by a 32-ms Hamming-windowed frame 
with 10-ms shifts. 
Table 1. Configuration of the speech database. 
Session Shot 199 clients 25 impostors 69 impostors
1 1 2 
1 2 2 
Training 
1 3 2 Evaluation
1 4 2 Test 
Evaluation Test 
 
We used 12 (2×2×3) utterances/speaker from sessions 1 and 2 
to train each client model, represented by a GMM with 64 mixture 
components. For each client, the other 198 clients’ utterances from 
sessions 1 and 2 were used to generate the world model, 
represented by a GMM with 256 mixture components. Meanwhile, 
B speakers were chosen from these 198 clients as the cohort [3] to 
yield B background models. Then, to optimize the weights, wi, and 
the threshold, θ, we used 6 utterances/client from session 3, along 
with 24 (4×2×3) utterances/evaluation-impostor over the four 
sessions, which yielded 1,194 (6×199) client samples and 119,400 
(24×25×199) impostor samples. In the performance evaluation, we 
tested 6 utterances/client in session 4 and 24 utterances/test-
impostor over the four sessions, which involved 1,194 (6×199) 
client trials and 329,544 (24×69×199) impostor trials. 
In addition, we used the B cohort set of models for L1(U) in 
Eq. (3), L2(U) in Eq. (4), and L3(U) in Eq. (5), and B+1 
background models, consisting of the B cohort set of models and 
one world model for our WAC and WGC methods. B was 
empirically set to 20. Two cohort selection methods [1] were used. 
One selected the closest B speakers for each client; and the other 
selected the closest B/2 speakers, plus the farthest B/2 speakers for 
each client. Here, the degree of closeness is measured in terms of 
the pairwise distance defined by [1]: 
,
)λ|(
)λ|(
log
)λ|(
)λ|(
log)λ,λ(
ij
jj
ji
ii
ji Xp
Xp
Xp
Xp
d +=               (21) 
where λi and λj are speaker models trained using the i-th speaker’s 
utterances Xi and the j-th speaker’s utterances Xj, respectively.  
 
4.2. Experiment results 
 
The proposed weighted combination methods were implemented in 
three ways: 1) WAC with the world model and the 10 closest 
cohort models, plus the 10 farthest cohort models 
(“WAC_w_10c_10f”); 2) WAC with the world model plus the 20 
closest cohort models (“WAC_w_20c”); and 3) WGC with the 
world model plus the 20 closest cohort models (“WGC_w_20c”). 
The MVE training for both WAC and WGC was initialized with an 
equal weight, wi, and the threshold θ was set to 0. The overall 
expected loss function D in Eq. (20) was set according to the 
HTER with CMiss = 1, CFalseAlarm = 1, and PTarget = 0.5. 
For the performance comparison, we used five systems as our 
baselines: 1) L1(U) with the 10 closest cohort models plus the 10 
PHONETIC BOUNDARY REFINEMENT USING SUPPORT VECTORMACHINE
Hung-Yi Lo and Hsin-Min Wang
Institute of Information Science, Academia Sinica, Taipei, Taiwan, Republic of China
{hungyi, whm}@iis.sinica.edu.tw
ABSTRACT
In this paper, we propose using support vector machine (SVM)
to refine the hypothesized phone transition boundaries given
by the HMM-based Viterbi forced alignment. We conducted
experiments on the TIMIT speech corpus. The phone transi-
tions were automatically partitioned into 46 clusters accord-
ing to their acoustic characteristics and the cross-validation
using the training data; hence, 46 phone-transition-dependent
SVM classifiers were used for phone boundary refinement.
The proposed HMM-SVM approach performs as well as the
recent discriminative HMM-based segmentation. The best ac-
curacies achieved are 81.23% within a tolerance of 10 ms and
92.47% within a tolerance of 20 ms. The mean boundary dis-
tance is 7.73 ms.
Index Terms— Automatic phone alignment, support vec-
tor machine, reduced support vector machine
1. INTRODUCTION
Annotated speech corpora are indispensable to various areas
of speech research, e.g., speech recognition and speech syn-
thesis. Phoneme level annotation is especially important for
fundamental speech research. However, the development of a
large high-quality, manually labelled speech corpus requires
lots of human effort, and is time-consuming. To reduce the
human effort and speed up the labelling process, many at-
tempts have been made to utilize automatic phone alignment
approaches to provide initial phonetic segmentation for sub-
sequent manual segmentation and verification [1, 2, 3].
The most popular method of automatic phone alignemnt
is to adapt an HMM-based phonetic recognizer to align a
phonetic transcription with a speech utterance. Empirically,
phone boundaries obtained in this way should contain few se-
rious errors, since HMMs in general capture acoustic proper-
ties of phones; however, small errors are inevitable because
HMMs are not sensitive enough to detect changes between
adjacent phones.
In this paper, we propose using support vector machine
(SVM) [4, 5] to refine the hypothesized boundaries given by
the HMM-based Viterbi forced alignment. As will be detailed
in the following section, we adapt the reduced support vec-
tor machine (RSVM) [5] algorithm to overcome the compu-
tational difficulty of applying SVM in a task with a massive
data set. In our approach, a phone-transition-dependent SVM
classifier is applied to detect the true phone transition bound-
ary around each hypothesized boundary given by the initial
HMM-based segmentation. These SVM classifiers for detect-
ing boundaries of various phone transitions are trained in ad-
vance based on multiple discriminative features in addition
to MFCCs. We conducted automatic phone alignment exper-
iments on the TIMIT speech corpus. The proposed HMM-
SVM approach performs as well as the improved HMM-based
segmentation [3], which used a discriminative criterion, called
minimum boundary error (MBE), instead of the conventional
maximum likelihood (ML) criterion for HMM training. The
best accuracies achieved are 81.23% within a tolerance of 10
ms and 92.47%within a tolerance of 20 ms. The mean bound-
ary distance is 7.73 ms.
2. SUPPORT VECTORMACHINE
Support vector machine (SVM) has become one of the most
promising learning algorithms for classification as well as re-
gression, and has been successfully applied to many real-world
pattern recognition applications. SVM finds a separating sur-
face with a large margin between training samples of two
classes in a high dimensional feature space implicitly intro-
duced by a computationally efficient kernel mapping, and the
large margin implies a better generalization ability according
to the statistical learning theory [4]. The reduced support vec-
tor machine (RSVM) [5] algorithm to implement SVM was
proposed in an attempt to overcome the computational diffi-
culty as well as to reduce the model complexity in generating
a nonlinear separating surface for a massive data set.
2.1. Reduced support vector machine
Consider the problem of classifying data points into two classes,
A+ and A−. We are given a training data set {(xi, yi)}mi=1,
where xi ∈ χ ⊂ Rn is an input vector variable and yi ∈
{1,−1} is a class label, which indicates one of the two classes,
A+ and A−, to which the data point belongs. We represent
these data points by an m × n matrix A, where the i-th row
of the matrix A, Ai, corresponds to the i-th data point. We
use an m × m diagonal matrix D, Dii = yi, to specify the
2. For each one of the three phone transition classes, namely
sonorant to non-sonorant, sonorant to sonorant, non-
sonorant to non-sonorant, we apply the K-means al-
gorithm to cluster the phone transitions according to
their mean vectors. Note that only the phone transi-
tions with enough instances are considered in this step.
The number of clusters is determined according to the
cross-validation accuracy that the resulting SVM clas-
sifiers achieve in the training data.
3. We assign the phone transitions, which are ignored in
Step 2 due to sparse instances, to the nearest clusters
according to the distances between their mean vectors
and the cluster centers.
3.3. Input vector to SVM
For each partition subset, two discriminative features, namely
discriminative weighted entropy and discriminative subband
energy, are believed to be more specialized to each partition
subset. The discriminative weighted entropy is computed by
H = −
N∑
i=1
wei pi log pi, (4)
where wei is a weight vector and pi is the element of power
spectrum which is normalized to satisfy
∑N
i=1 pi = 1. The
weight vector of each partition subset is trained by linear SVM
using the vectors p log p extracted from the right frames next
to the true boundaries as positive samples and those from the
left frames as negative samples. The goal of training the
weight vector is to maximize the variation of the weighted
entropy feature close to the true boundary. The discriminative
subband energy is computed by:
Esub = Ej

argmax
j
Fj
, (5)
where Ej , j = 1, . . . , 9, is pre-defined subband energy and
the weight score Fj is:
Fj =
µ+j − µ−j
σ+j − σ−j
, (6)
where µj and σj are the mean and standard deviation of the
j-th subband energy for the training samples of the positive
or negative class.
After these two parameters are determined, the general
weighted entropy and the subband energy extracted in sec-
tion 3.1 are replaced by the discriminative weighted entropy
and the discriminative subband energy in the input vectors to
SVM in both training and testing phases.
3.4. Boundary recognition using SVM
For each phone transition subset, a SVM classifier is trained
by the RSVM algorithm for boundary detection using the aug-
mented vectors associated with the true boundaries as the pos-
itive training samples and the randomly selected augmented
vectors at least ±20 ms away from the true boundaries as the
negative training samples. Gaussian kernel with the weighted
Euclidean distance K(x, z′) = e−γ‖w(xi−xj)‖
2
2 is applied,
and the weight is used to emphasize the more important and
discriminative features. In the testing phase, the augmented
vectors associated with the speech frames around the hypoth-
esized boundary are examined by the SVM classifier accord-
ing to the partition to which the phone transition belongs, and
the frame index associated with the augmented vector with
the maximum classifier output is recognized as the refined
boundary.
4. EXPERIMENT RESULTS
4.1. Experiment setup
Our experiments were conducted on the TIMIT acoustic-phonetic
continuous speech corpus. TIMIT contains a total of 6,300
sentences, comprised of 10 sentences spoken by each of 630
speakers from 8 major dialect regions in the United States.
The TIMIT suggested training and testing sets contain 462
and 168 speakers, respectively. We discard the dialect sen-
tences (SA1 and SA2 utterances) and utterances with phones
shorter than 10 ms. The resulting training set and testing set
contain 3,696 sentences and 1,312 sentences, respectively.
The acoustic models for HMM-based segmentation con-
sist of 50 context-independent phone models, each represented
by a 3-state continuous density HMM (CDHMM) with a left-
to-right topology. Each frame of the speech data is repre-
sented by a 39-dimensional feature vector comprised of 12
MFCCs and log energy, and their delta and delta-delta coef-
ficients. The frame width is 20 ms and the frame shift is 5
ms. Utterance-based cepstral variance normalization (CVN)
is applied to all the training and testing speech. The acoustic
models were trained on the training speech according to the
human-labelled phonetic transcriptions and boundaries by the
Baum-Welch algorithm using the ML criterion with 10 itera-
tions.
By using the cross-validation on the TIMIT training data,
the number of phone transition cluster is 20 in the sonorant
to non-sonorant class, 16 in the sonorant to sonorant class,
and 10 in the non-sonorant to non-sonorant class. As a re-
sult, 46 SVM classifiers are used. In the refinement phase,
given the boundary of each phone transition obtained by the
HMM-based segmentation, 16 hypothesized boundaries ex-
tracted every 5 ms around the initial boundary within±40 ms
will be examined by SVM.
The proposed HMM-SVM approach was compared with
the improved HMMMBE-based segmentation [3]. The MBE
discriminative training approach was applied to manipulate
the above ML-trained HMMs with 10 more iterations.
SPEAKER CLUSTERING BASED ON MINIMUM RAND INDEX 
 
Wei-Ho Tsai1 and Hsin-Min Wang2 
 
1Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan  
2Institute of Information Science, Academia Sinica, Taipei, Taiwan 
1whtsai@en.ntut.edu.tw, 2whm@iis.sinica.edu.tw 
 
ABSTRACT 
 
This paper presents an effective method for clustering unknown 
speech utterances based on their associated speakers. The proposed 
method jointly optimizes the generated clusters and the number of 
clusters by estimating and minimizing the Rand index of the 
clustering. The Rand index, which reflects clustering errors that 
utterances from the same speaker are placed in different clusters, 
or utterances from different speakers are placed in the same cluster, 
reaches its minimal value only when the number of clusters is 
equal to the true speaker population size. We approximate the 
Rand index by a function of the similarity measures between 
utterances and employ the genetic algorithm to determine the 
cluster where each utterance should be located, such that the 
overall clustering errors are minimized. The experimental results 
show that the proposed speaker-clustering method outperforms the 
conventional method based on hierarchical agglomerative 
clustering in conjunction with the Bayesian information criterion 
to determine the number of clusters. 
 
Index Terms—Clustering methods, Speech processing, 
Speaker recognition  
 
1. INTRODUCTION 
 
With the burgeoning availability of digital audio material, speaker 
clustering is gaining importance as a means of indexing the 
voluminous spoken data accumulated daily for archival use [1-14]. 
Given N speech utterances produced by P speakers, the goal of 
speaker clustering is to partition N utterances into M clusters, such 
that M = P and each cluster consists exclusively of utterances from 
only one speaker. Since no prior information regarding the 
speakers involved and the speaker population size is available in 
most practical applications, solving the speaker-clustering problem 
usually involves characterizing the voice similarities between 
utterances, generating clusters based on those similarities, and 
determining the optimal number of clusters. 
Currently, the most popular method of speaker clustering 
generates a cluster tree by sequentially merging the utterances 
deemed similar to each other, and then cuts the tree via a Bayesian 
information criterion (BIC) [5,8,10-12,15], in order to retain an 
appropriate number of clusters. During the agglomeration 
procedure, the nearest neighborhood selection rule is usually 
employed in an attempt to maximize the similarities between all 
the utterances within each cluster. Since the interaction between 
clusters is not considered, this method can only make each 
individual cluster as homogeneous as possible; however it cannot 
guarantee that the homogeneity for all the clusters can finally be 
summed to reach a maximum. In particular, mis-clustering errors 
arising from grouping different-speaker utterances together can 
propagate down the whole process, and hence limit the clustering 
performance. In addition, the cluster tree is generated separately 
from the determination of the optimal number of clusters. Since the 
latter trusts the former completely, the inevitable errors from the 
former can propagate to the latter, which may lead to a poor 
estimation of the speaker population size. 
To overcome the above-mentioned limitations of the 
conventional method, we propose a new clustering method that 
jointly optimizes the generated clusters and the number of clusters 
by estimating and minimizing a metric called the Rand index 
[16,17]. This metric indicates the clustering errors that place 
utterances from the same speaker in different clusters, or place 
utterances from different speakers in the same cluster. We 
approximate the Rand index by a function of the similarity 
measures between utterances, and employ the genetic algorithm 
[18] to determine the cluster where each utterance should be 
located. The resulting clusters are thus optimized in a global 
fashion, rather than a pair-by-pair manner used in the conventional 
method. In addition, by exploiting a characteristic of the Rand 
index that it only reaches the minimal value when the number of 
clusters equals the true speaker population size, speaker clustering 
based on the minimization of the estimated Rand index also 
enables the resulting number of clusters to approach the optimum. 
 
2. PROBLEM FORMULATION 
 
For convenience of discussion, we begin by defining the following 
symbols. 
X1, X2,…, XN : N speech utterances to be clustered; 
s1, s2,…, sP : P unknown speakers involved in N utterances; 
c1, c2,…, cM : M clusters to be generated; 
on : index of the speaker producing utterance Xn; 
hn : index of the cluster that utterance Xn is assigned to; 
nm∗ : number of utterances in cm; 
n∗p : number of utterances spoken by sp; 
nmp : number of utterances in cm spoken by sp. 
The goal of speaker clustering is to produce a set of indices H = 
{h1, h2, …, hN} that satisfy  hi = hj for any Xi and Xj from the same 
speaker, and hi ≠ hj for any Xi and Xj from different speakers. 
Depending on the application, there are a number of ways to 
evaluate the performance of speaker clustering. This study uses 
two metrics: cluster purity [4] and the Rand index [4,16,17]. 
Cluster purity represents the probability that if we pick any 
utterance from a cluster twice at random, with replacement, both of 
and 
,),(),(2),()(ˆ
1 1
)()(
1 1
)()()( ∑∑∑∑
= == =
−Ω+=
N
i
N
j
ji
M
j
M
i
N
i
N
j
M
j
M
i
M oohhhhR δδδH  
(13) 
where δ(⋅) in Eqs. (10)−(13) is a Kronecker Delta function.  
However, as the computation of δ(oi, oj) requires that the true 
speaker of each utterance be known in advance, it is impossible to 
find H∗ directly from Eqs. (12) and (13). To solve this problem, we 
propose estimating δ(oi, oj) by means of the similarity measure 
between Xi and Xj. Specifically, 
, 
0 and , if  ,),(
0 and , if  ,),(
          if  ,             1
),(
maxmax
maxmax⎪⎩
⎪⎨
⎧
<≠
>≠
=
←
SjiSS
SjiSS
ji
oo
ji
jiji
XX
XXδ    (14) 
where S(Xi, Xj) denotes a certain similarity measure between Xi 
and Xj that could be either positive or negative, but cannot be zero, 
and Smax is the maximum among the similarities S(Xi, Xj), ∀ i ≠ j. 
In our implementation, S(Xi, Xj) is computed by the generalized 
likelihood ratio (GLR) [1,4]: 
S(Xi, Xj) = logPr(Xij |λij) − logPr(Xi |λi) − logPr(Xj |λj),      (15) 
where  Xij is the concatenation of Xi and Xj, and λi, λj, and λij are 
parametric models trained using Xi,  Xj, and Xij, respectively. 
Using this estimation, we can solve Eq. (12) by further assigning 
to Ω an arbitrary positive constant that ensures . 0)(ˆ )( ≥MR H
Given that neither a gradient-based optimization method nor an 
exhaustive search is applicable in this scenario, we propose using 
the genetic algorithm (GA) [18] to find H∗ by virtue of its global 
scope and parallel searching power. The basic operation of the GA 
is to explore a given search space in parallel by means of iterative 
modifications of a population of chromosomes. Each chromosome, 
encoded as a string of alphabets or real numbers called genes, 
represents a potential solution to a given problem. In our task, a 
chromosome is exactly a legitimate H(M), and a gene corresponds 
to a cluster index associated with an utterance. However, since the 
index of one cluster can be interchanged with that of another 
cluster, multiple chromosomes may amount to an identical 
clustering result. For example, the chromosomes {1 1 1 2 2 3 3}, 
{1 1 1 3 3 2 2}, {2 2 2 1 1 3 3}, and {1 1 1 5 5 4 4} represent the 
same clustering result derived by grouping seven utterances into 
three clusters. Such a non-unique representation of the solution 
would significantly increase the GA search space, and may lead to 
an inferior clustering result. To avoid this problem, we limit the 
inventory of chromosomes to conform to a baseform 
representation defined as follows.  
Let I (cm) be the lowest index of the utterance in cluster cm. 
Then, a chromosome is a baseform  
iff ∀ cm, cl ≠ {φ}, if m < l, then I (cm) < I (cl),   (16) 
where {φ} indicates that a cluster does not contain any utterance. 
Among the above chromosomes, {1 1 1 2 2 3 3} is a baseform, 
since the lowest index of the utterance in clusters c1, c2, and c3 is 1, 
4, and 6, respectively, which satisfies Eq. (16). In contrast, 
chromosomes {1 1 1 3 3 2 2} and {2 2 2 1 1 3 3} are not 
baseforms, since the lowest index of the utterance in clusters c1, c2, 
and c3 does not satisfy Eq. (16). In addition, chromosome {1 1 1 5 
5 4 4} implies that clusters c2 and c3 do not contains any utterance; 
hence it is not a baseform, either. However, it is conceivable that 
all the non-baseform chromosomes can be converted into a unique 
baseform representation by re-arranging the cluster indices. 
GA optimization starts with a random generation of 
chromosomes according to a certain population size, Z. Then, the 
fitness of all chromosomes is evaluated via the inverse of the 
estimated Rand index, i.e., )(ˆ1)( )()( MM RF HH = . Based on this 
evaluation, a particular group of chromosomes is selected from the 
population to generate offspring by subsequent recombination. To 
prevent premature convergence of the population, the selection is 
performed with the linear ranking scheme described in [19]. Next, 
crossover among the selected chromosomes proceeds by 
exchanging the substrings of two chromosomes between two 
randomly selected crossover points. A crossover probability is 
assigned to control the number of offspring produced in each 
generation. After crossover, a mutation operator is used to 
introduce random variations into the genetic structure of the 
chromosomes. This is done by generating a random number and 
then replacing one gene of an existing chromosome with a 
mutation probability. The resulting chromosomes that do not 
conform to the baseform representations are converted into their 
baseform counterparts.  
The procedure of fitness evaluation, selection, crossover, and 
mutation is repeated continuously, in the hope that the overall 
fitness of the population will increase from generation to 
generation. When the maximum number of generations is reached, 
the best chromosome in the final population is taken as the 
solution, H*. Note that the estimated speaker population size can 
be obtained by selecting the maximal value of the cluster index in 
H*. For example, if H* = {1 2 1 3 4 3 1}, the estimated number of 
speakers in a seven-utterance collection is 4. 
 
4. EXPERIMENTAL RESULTS 
 
The speech data used in this study consisted of six excerpts of 
broadcasts from the evaluation set of the 2002 Rich Transcription 
Broadcast News and Conversational Telephone Speech Corpus 
[20]. Each excerpt was segmented into speaker-homogeneous 
utterances, according to the annotation files in the corpus. Speaker 
clustering was then applied to each excerpt separately. Prior to the 
experiments, every speech utterance was converted from its digital 
waveform representation into a sequence of feature vectors, each 
of which consisted of 12 Mel-scale frequency cepstral coefficients 
(MFCCs) and 12 delta MFCCs. Then, the similarities between the 
utterances were computed using Eq. (15), in which all the 
parametric models are of a uni-Gaussian model with a full 
covariance matrix. 
In GA optimization, the parameter values used for the 
maximum number of generations, the population size, the 
crossover probability, and the mutation probability were 
empirically determined to be 2000, 5000, 0.5, and 0.1, 
respectively. For the performance comparison, we also 
implemented a baseline speaker-clustering system based on 
hierarchical agglomerative clustering (HAC) in conjunction with 
the Bayesian information criterion (BIC) to determine the optimal 
number of clusters [5]. In the agglomeration procedure, the 
similarities between clusters were computed using the complete 
linkage of the GLR-based inter-utterance similarities. In addition, 
in using the BIC, the penalty weight was set to one. 
Table 1 shows the speaker-clustering results. First, we 
evaluated the performance of the proposed minimum Rand index 
clustering (MRIC) by specifying the number of clusters a priori as 
the true number of speakers. This served as an upper bound of the 
performance that could be achieved by the automatic 
