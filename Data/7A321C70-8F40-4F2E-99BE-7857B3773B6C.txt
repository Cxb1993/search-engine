 2
1 前言與研究目的 
 
在生命體中，蛋白質是細胞內最重要、特殊的大分子，所有的抗體、酵素和荷爾蒙幾
乎都是蛋白質，而結締組織、肌肉纖維等基本上也都是由蛋白質所構成。由於各種蛋白質
展現出特有的性質並發揮功能，生物體內的各種活動才有可能發生。大部分之蛋白質是由
二十種胺基酸組成的序列，我們稱這些胺基酸序列為蛋白質一級結構。二級結構為在一級
結構上之螺旋(-Helix)與摺頁(-Sheet)等由氫鍵所造成的結構，蛋白質二級結構目錄
DSSP (Dictionary of Protein Secondary Structure) [Kabs83]將二級結構分為 8 種型態，而蛋白
質之二級結構會影響三級結構之摺疊方式。三級結構是指單一蛋白質之整體形狀或摺疊方
式，而四級結構用來描述由多個蛋白質所組成之蛋白質複合體之排列組合狀況。 
 
生物學家相信，蛋白質的三級結構與功能息息相關，兩個蛋白質的結構愈相似，它們
的功能也愈類似。蛋白質為了發揮它的本身功能，它們通常必須具有特定的形狀，亦稱摺
疊(fold)。當蛋白質摺疊發生錯誤，可能導致嚴重的後果。目前已知有二十種以上的疾病是
因蛋白質錯誤摺疊而形成，包括許多知名的疾病，例如，阿茲海默症(老人癡呆症，Dementia 
of the Alzheimer's Type (DAT))、狂牛病(牛海綿狀腦病，Bovine Spongiform Encephalopathy 
(BSE))、人類庫賈氏病(人類海綿狀腦病，Creutzfeldt-Jakob Disease (CJD))、漸凍人(肌萎縮
性脊髓側索硬化症，Amyotrophic lateral sclerosis (ALS))、帕金森氏症(Parkinson's Disease)
及許多癌症(Cancer)及其併發症等。 
 
蛋白質對生物體的維持與生命的存續是如此重要，瞭解蛋白質的結構對生物醫學的研
究之重要性不言而喻，但對於大量未知結構之蛋白質，在實驗室中逐一以 X 光繞射法或核
磁共振法找出其結構須耗費極大的資源與時間。為協助生物醫學專家有效縮小研究範圍以
提昇研究效率，本計畫運用資訊學上之預測與分類方法，並提出合適的演算機制，以提昇
蛋白質分類與結構預設之準確性。本計畫提出三項研究成果，並已分別於國際與國內研討
會發表[Ann09, Chan09, Hor09]，茲簡要說明如下。 
1. 蛋白質全原子骨幹結構之預測方法：針對蛋白質序列，利用同源概念及骨幹中
之氧原子特性，設計有效的方法來進行蛋白質三維結構的全原子預測，以提昇
全原子結構預測之正確性[Chan09]。 
2. 應用於蛋白質摺疊分類之特徵選擇與分類器合併方法：設計有效的蛋白質序列
特徵之挑選方法，並結合階層式學習架構分類器，以提昇蛋白質結構分類之正
確性[Hor09]。 
3. 計算變動長度編碼序列之附加條件限制之最長共同子序列演算法：為找出具有
特定特徵之相似蛋白質二級結構，並計算不同蛋白質之二級結構間之相似度，
我們提出更有效率的演算法解決變動長度編碼序列之限制共同子序列問題 
[Ann09]。 
 
 4
預測結果仍有改進的空間。 
 
經過文獻整理與資料分析後我們發現基於 AMBER 力場(AMBER force field) [Corn95] 
所定義的潛勢能量公式中，氧原子的計算是可以獨立於其他原子之外的。也就是說，我們
可以先將蛋白質骨幹上的全原子 (包含 C、N、C、O) 座標預測出來，之後再針對氧原子
的座標進行微調。微調之後，可以得到準確度更高的預測結果。實驗結果顯示，透過此方
式來進行微調，可以使預測的正確性更為提高。此外，為減少搜尋之可能解數量，本研究
團隊提出了兩階段的氧原子座標微調方法，先對較大之晶格進行搜尋，再於可能產生較佳
解之大晶格內以更高的解析度進行搜尋，此方法可以大幅減少搜尋氧原子較佳位置的時
間，如圖 1 所示。 
圖 1：氧原子座標兩階段微調方法 
 
表 2：蛋白質全原子骨幹結構預測比較表 
Protein RMSD(Å) 
PDB ID Length (bps) MaxSprout Adcock's
SABBAC 
v1.0 
SABBAC 
v1.2 Wang's Ours 
111M 154 0.42 0.31 0.29 0.21 0.26 0.25
1CTF 68 0.73 0.41 0.43 0.33 0.42 0.38
1IGD 61 0.44 0.34 0.36 0.37 0.36 0.35
1OMD 107 0.41 0.39 0.35 0.40 0.39 0.34
1SEMA 58 0.34 0.50 0.48 0.48 0.45 0.40
1TIMA 247 0.60 0.56 0.59 0.58 0.54 0.52
1UBQ 76 0.38 0.37 0.35 0.34 0.37 0.36
2CTS 437 0.45 0.37 0.40 0.38 0.34 0.31
2LYM 129 0.44 0.32 0.38 0.38 0.29 0.28
2MHR 118 0.54 0.33 0.50 0.44 0.39 0.35
2PCY 99 0.54 0.48 0.42 0.44 0.33 0.30
2OZ9 104 0.42 0.24 0.30 0.29 0.22 0.19
4PTI 58 0.44 0.51 0.53 0.37 0.42 0.42
5CPA 307      - 0.48 0.41 0.41 0.34 0.32
5NLL 138 0.46 0.42 0.37 0.38 0.39 0.34
mean - 0.472 0.402 0.411 0.387 0.367 0.341
variance - 0.100 0.089 0.084 0.084 0.078 0.076
 
 6
的貢獻是特徵選擇與分類器合併，不過在他們的研究中，特徵選擇僅限制在以組(feature set)
為單位，而且並未解決 HLA 的 early fault 問題。 
 
針對蛋白質結構分類問題，我們提出應用於蛋白質摺疊分類(protein fold classification)
之特徵選擇與分類器合併方法。為了能夠訓練兩個以上分類器，特徵選擇的進行乃以 Ding
與 Huang 等人所建議的特徵組合為初始特徵，並搭配信息獲取量(information gain)方法進行
篩選，因此特徵選擇的自由度擴展至以單一特徵為單位。在特徵選擇方面，先將特徵依信
息獲取量由大至小排序，然後計算平均由頭到尾每一特徵之平均信息獲取量，目的是希望
能以最少的特徵保留最多的信息獲取量。在分類器訓練方面，我們採用 Huang 等人[Huan03]
所提出的階層式學習法(HLA)，把蛋白質依其蛋白質種類與摺疊種類分別訓練，分類器則
是採用支持向量機器(SVM)。在合併分類器輸出方面，我們採用了數種資訊合併(information 
fusion)的方法，針對 HLA 之缺點進行改進，包含了多數決(MAJ)、加權多數決(WMAJ)、行
為認知空間法(BKS)以及階層式 BKS(HBKS)等方法。實驗結果顯示，BKS 在單一層的合併
效果最佳，而 HBKS 則可合併 HLA 所訓練出之結果。表 3 為分類器資訊合併策略比較表
在我們的研究中，針對四類蛋白質與 27 類摺疊分類正確率使用 BKS 最好的結果分別是
88.3%與 75.5%，比前人研究所得到之最佳結果 87%與 69.6%更好。 
 
表 3：分類器資訊合併策略比較表 
Method HLA NN RANK1 
(a)+(a+b) 
MAJ 
(a)+(a+b) 
WMAJ 
(a)+(a+b) 
BKS 
Accuracy of 
Level 1(%) 87.0 84.9 86.9 88.3 
Accuracy of 
Level 2(%) 69.6 68.1 69.5 75.5 
 
2.3 計算變動長度編碼序列之附加條件限制之最長共同子序列演算法 
在計算序列相似度(similarity)時，為了較快速地找出序列相同之區塊，有時會將相似度
問題轉換為相同度(identity)問題，最長共同子序列(longest common subsequence, LCS)問題是
一個常見的解法，LCS 問題已經有數十年的研究歷史。然而，有時候 LCS 所找到的答案，
雖然長度是最長的，但並未包含我們有興趣的資料。因此在 2003 年時，Tsai 提出了一個新
的問題，即附加條件限制之最長共同子序列(constrained LCS, CLCS)問題[Tsai03]。CLCS 問
題定義如下，給定兩序列 X 與 Y，以及限制條件序列 P，希望找到一個 X 與 Y 的共同子序
列 Z，使得 P 為 Z 的的子序列，且 P 的長度是符合條件的序列中最長的。 
 
對於 CLCS 問題，Tsai 於 2003 年提出了一個時間複雜度為 O(n2m2r)的方法，其中 n、
m、r 分別為序列 X、Y、Z 的長度。之後，Chin[Chin04]、Arslan [Arsl05]與本研究團隊[Peng03]
分別獨立提出了時間複雜度為 O(nmr)的動態規劃(dynamic programming, DP)方法。除了動
態規劃的解法外，最近 Iliopoulos[Ilio08]亦提出了套用 van Emde Boas tree (vEB tree)資料結
構的 Hunt-Szymanski 形式演算法，這類演算法不會考慮整個動態規劃表(DP lattice)，而只
 8
念[Apos99]，我們提出另一個執行時間為 O(NMr + rmin{q1, q2}+q3)的方法。其中 q1、q2
是所有的部分配對區塊中的下牆與右牆的元素數量，q3 是所有的完全配對區塊的牆上元素
數量。 
 
3 結論 
本計畫之研究目的為進行蛋白質結構分類與全原子預測，我們運用資訊學上之預測與
分類方法，並提出適合的演算機制，以提昇蛋白質分類與結構預測之效率與準確性。在蛋
白質結構預測方面，我們提出了蛋白質全原子骨幹結構之預測方法，首先針對蛋白質骨幹
上所有原子進行預測，再運用二階段微調方法修正氧原子之座標，使結構預測更為精確。
在蛋白質結構分類研究中，我們針對蛋白質三級結構提出摺疊分類之特徵選擇與分類器合
併方法，並設計可挑選有效蛋白質序列特徵之方法，結合階層式學習架構分類器，利用資
訊融合技術提昇蛋白質結構分類之正確性。為有效利用蛋白質二級結構之資訊以提昇蛋白
質結構分類之正確性，我們依據蛋白質二級結構序列之特性，設計出可計算變動長度編碼
序列之附加條件限制之最長共同子序列演算法，以找出具特殊二級結構之最相似蛋白質序
列。本計畫所提出之三項主要研究成果，已分別於國際與國內研討會發表，研討會論文請
參閱附錄資料。與本計畫相關之前一年度研究成果亦已被期刊接受。 
在蛋白質結構預測與分類研究領域中，本研究團隊於此年度計畫中已有不錯的研究成
果，以目前研究成果為基礎，未來除了考慮蛋白質一級結構之其他特性，另外亦可採用比
三級結構預測方法更為成熟的蛋白質二級結構預測方法，並運用本計畫所提出之可計算變
動長度編碼序列之附加條件限制之最長共同子序列演算法，更精確地找出可能的已知蛋白
質三級結構範本，結合更多的資訊與特徵以提昇蛋白質三級結構分類預測正確性。 
 
4 參考文獻 
1. [Ann08] H. Y. Ann, C. B. Yang, C. T. Tseng, and C. Y. Hor, "A fast and simple algorithm for 
computing the longest common subsequence of run-length encoded strings," Information 
Processing Letters, Vol. 108, pp. 360–364, 2008. 
2. [Ann09] H. Y. Ann, C. B. Yang, C. T. Tseng and C. Y. Hor, "Fast Algorithms for Computing 
the Constrained LCS of Run-Length Encoded Strings," Proc. of the 2009 International 
Conference on Bioinformatics and Computational Biology (BIOCOMP '09), Vol. 2, pp. 
646-649, Las Vegas, Nevada, USA, July 13-16, 2009. 
3. [Apos99] A. Apostolico, G. M. Landau, S. Skiena, "Matching for run-length encoded strings," 
Journal of Complexity, Vol. 15, pp. 4–16, 1999. 
4. [Arsl05] A. N. Arslan and O. Egecioglu, "Algorithms for the constrained longest common 
subsequence problems," International Journal of Foundations Computer Science, Vol. 16, No. 
6, pp. 1099-1109, 2005. 
5. [Berm08] H. M. Berman, "The Protein Data Bank: a historical perspective," Acta 
Crystallographica Section A: Foundations of Crystallography, Vol. A64(1), pp. 88-95, 2008. 
6. [Bunk95] H. Bunke, J. Csirik, "An improved algorithm for computing the edit distance of 
run-length coded strings," Information Processing Letters, Vol. 54, pp. 93–96, 1995. 
- 1 -
第二屆WSEAS生醫電子與生醫資訊國際會議
楊昌彪
國立中山大學資訊工程學系
第二屆 WSEAS 生醫電子與生醫資訊國際會議 (BEBI’09,2nd WSEAS
International Conference on Biomedical Electronics and Biomedical Informatics)是由
WSEAS所主辦的生醫相關領域之國際會議，其中也包含生物資訊。舉辦地點為
俄羅斯首都莫斯科(Moscow，俄文為 MOCKBA)之 Crowne Plaza Hotel Moscow
World Trade Centre，會期為 2009年 8月 20日至 8月 22日。同一期間在該地舉
行的會議尚有其他五個會議。換言之，此六個會議是個聯合會議。其他會議列表
如下：
1. 9th WSEAS International Conference on APPLIED INFORMATICS AND
COMMUNICATIONS (AIC '09).
2. 7th IASME / WSEAS International Conference on FLUID MECHANICS and
AERODYNAMICS (FMA '09).
3. 7th IASME / WSEAS International Conference on HEAT TRANSFER,
THERMAL ENGINEERING and ENVIRONMENT (HTE '09).
4. 9th WSEAS International Conference on SIGNAL PROCESSING,
COMPUTATIONAL GEOMETRY and ARTIFICIAL VISION (ISCGAV '09).
5. 9th WSEAS International Conference on SYSTEMS THEORY AND
SCIENTIFIC COMPUTATION (ISTASC '09).
WSEAS 之全稱為World Scientific and Engineering Academy and Society,，網
址為 http://www.wseas.org/。根據該網址介紹，該組織於 1996年以WSES名稱成
立，復於 2001年更名為目前名稱迄今。該組織是一個科學、工程與學術的非營
利組織，希望利用嶄新的數學方法與計算科技，來提升科學工程的研發與應用成
果。尤其特別重視數學、資訊科學、電子工程與其他科學工程領域之互動。此外，
該組織也推動研究計畫、獎助學者，主辦國際會議、研討會，出版書籍與期刊。
目前WSEAS 的總部位於美國威斯康新大學(University of Wisconsin)，分支機構
遍及北美洲、歐洲、亞洲、南美洲、澳洲等，台灣也有分支機構。目前WSEAS
Transactions 期刊共有 19 種，諸如 Circuits and Systems, Systems and Control,
Communications, Computers, Mathematics, Electronics, Biology and Biomedicine,
Information Science and Applications, Signal Processing, Acoustic and Music等。不
過可能出版年代不久，這些期刊均尚未列入 SCI 或 SCI Expanded。自今年九月
至年底預計舉辦之國際會議約 30個。由該組織網頁的簡介可知，該組織活動力
強，活動區域涵蓋全世界各地。該組織未來對於全世界各地學術活動的影響力，
- 3 -
1. Proceedings of the 9th WSEAS International Conference on APPLIED
INFORMATICS AND COMMUNICATIONS (AIC '09), Moscow, Russia, August
20-22, 2009.
2. Proceedings of the 2nd WSEAS International Conference on Biomedical
Electronics and Biomedical Informatics (BEBI '09), Moscow, Russia, August
20-22, 2009.
3. Proceedings of the 7th IASME / WSEAS International Conference on FLUID
MECHANICS and AERODYNAMICS (FMA '09), Moscow, Russia, August
20-22, 2009.
4. Proceedings of the 7th IASME / WSEAS International Conference on HEAT
TRANSFER, THERMAL ENGINEERING and ENVIRONMENT (HTE '09),
Moscow, Russia, August 20-22, 2009.
5. Proceedings of the 9th WSEAS International Conference on SIGNAL
PROCESSING, COMPUTATIONAL GEOMETRY and ARTIFICIAL VISION
(ISCGAV '09), Moscow, Russia, August 20-22, 2009.
6. Proceedings of the 9th WSEAS International Conference on SYSTEMS
THEORY AND SCIENTIFIC COMPUTATION (ISTASC '09), Moscow, Russia,
August 20-22, 2009.
這是我第一次去俄羅斯。往昔對於俄羅斯的刻板印象，覺得那是個不友善
的地方，警察可能處處監控你的行動。行前也有人告訴我，俄羅斯人不喜歡與外
國人打交道，所以不懂俄語者想在俄羅斯旅遊可能處處碰壁。歷經本次的研討會
經驗後，我對於俄羅斯印象完全改觀。每次我想至某個地方，都是靠許多陌生的
俄羅斯人幫忙，才到達目的地，即使言語不通的老婦人也都顯得樂意幫忙。莫斯
科讓人覺得相當進步，也具有氣質。
本次俄羅斯之行，我才發現原來俄文字母與英文字母有對應關係，例如俄
文的 P對應至英文 R，俄文 C對應至英文 S，俄文л對應至英文 L等等，詳如附
表所列。掌握對應表後，有些與英文近似的英文即可一目了然。例如，「銀行」
的俄文為Банк，利用對應表，換成英文字母即為 bank，與英文完全一樣；「俱樂
部」的俄文為Клуб，換成英文字母即為 club，與英文完全一樣；「超級市場」的
俄文Супермаркет，換成英文字母即為 supermarket ，與英文完全一樣；「餐廳」
的俄文為Рестораны，換成英文字母即為 Restorany與英文 Restaurant頗為相近；
「廁所」的俄文為туалет，換成英文字母即為 tualet，與英文 toilet也相似。
Refinement on O Atom Positions for Protein Backbone Prediction
HSIAO-YEN CHANG
National Sun Yat-sen University
Dept. of Computer Sci. and Eng.
Kaohsiung, TAIWAN
changhy@par.cse.nsysu.edu.tw
CHANG-BIAU YANG∗
National Sun Yat-sen University
Dept. of Computer Sci. and Eng.
Kaohsiung, TAIWAN
cbyang@cse.nsysu.edu.tw
HSING-YEN ANN
National Sun Yat-sen University
Dept. of Computer Sci. and Eng.
Kaohsiung, TAIWAN
annhy@par.cse.nsysu.edu.tw
Abstract: For given the 3D coordinates of Cα in a protein, the all-atom protein backbone reconstruction problem
(PBRP) is to rebuild the 3D coordinates of all major atoms (N, C and O) on the backbone. The previous works
show that the prediction accuracy of the 3D positions of the O atoms is not so good, compared with the other two
atoms N and C. Thus, we aim to refine the positions of the O atoms after the initial prediction of N, C and O atoms
on the backbone has been done by the previous works. Based on the AMBER force field, we modify the energy
function to a simplified one with the statistical data on the bond lengths and bond angles of amino acids. We find
that the position of each O atom can be calculated independently. Thus, we propose a two-phase refinement method
(TPRM) to efficiently tune the position of each O atom based on the modified energy function. The experimental
results show that our reconstruction results are more accurate than others. Moreover, the solutions obtained by our
method are more stable than the others. Besides, our method also runs much faster than the famous prediction tool,
SABBAC.
Key–Words: Bioinformatics, protein structure, backbone prediction, RMSD
1 Introduction
A protein is a linear chain of amino acids. Usually, we
say that there are 20 kinds of standard amino acids.
Sometimes the nonstandard amino acids may be in-
volved as the 21st kind of amino acid. To reveal
its corresponding function, the linear chain has to be
folded correctly. Therefore, one of the most important
things for biologists is to analyze the 3D conforma-
tions of proteins. X-ray crystallographic and nuclear
magnetic resonance (NMR)[17] are two major ways
to determine the 3D structure of a given protein. How-
ever, they are slow and costly. Therefore, the protein
structure prediction becomes one of important topics
in the field of structure biology.
Homology modeling [1, 8, 9, 15, 20] and ab
initio [8, 10–12, 19] are two major strategies for
protein structure prediction. Homology modeling is
also called knowledge-based modeling which utilizes
known structures to produce a template set and deter-
mines the appropriate templates according to the sim-
ilarity of conformations or sequences. On the other
hand, ab initio strategies do not require propaedeutic
of protein structures, but they depend on a principle of
molecular mechanics, thermodynamic and the experi-
ential potential functions. Force fields are sets of pa-
rameters and equations, which are derived from both
experimental work and high-level quantum mechan-
∗Corresponding author
ical calculations, that can be used in molecular me-
chanics simulations. Some methods or software pack-
ages of force fields are often used, such as AMBER
[5], CHARMM [4, 13] and OPEP [18].
Protein Data Bank (PDB) [3] is a famous
database which collects lots of solved 3D protein
structures. However, some of the proteins in PDB lack
for the important information, including the coordi-
nates of the major atoms N, C and O on the backbone
and the atoms on the side chains. That is, they have
only α-carbon coordinates. Many researches focus on
how to reconstruct (predict) other missed atoms’ co-
ordinates based on the given atom positions, e.g. α-
carbon coordinates. In this paper, we shall only study
the prediction of all-atom coordinates on the protein
backbone.
Given the 3D coordinates of the α-carbons in
protein and its corresponding amino acid sequence,
the all-atom protein backbone reconstruction problem
(PBRP) is to rebuild the 3D coordinates of all ma-
jor atoms (N, C, and O) on the backbone. In gen-
eral, there are two major strategies for solving PBRP,
one employs the similarity of small fragments, and
the other is to minimize the molecular energy. Some
approaches combine the above two kinds of methods
[2, 7]. There are also many methods proposed to solve
the PBRP, such as MaxSprout [8], Adcock’s method
[1], SABBAC [14] and Wang’s method [20].
SABBAC [14] is a famous online service for all-
An amino acid sequence and 
its
 
-carbon coordinates
Initial backbone structure
[Wang et al., 2009]
Refinement of the O-atom positions
The refined coordinates of all-atoms 
on the backbone
Figure 1: The flowchart of our refinement method.
C
CCC
N C C
N
O O
O
N φ


Figure 2: The illustration of three backbone dihedral
angles.
force, the energy items remain bond stretching, angle
bending and torsion angle changes. For torsion angle
changes, there are three dihedral angles on the pro-
tein backbone, φ, ψ and ω, as shown in Figure 2. The
backbone dihedral angles φ, ψ, and ω involve atoms
C-N-Cα-C, N-Cα-C-N, and Cα-C-N-Cα respectively.
One can see that these angles do not depend on the
positions of O atoms on the backbone. Therefore, the
only energy items that have to be considered are bond
stretching and angle bending. Accordingly, we define
the fitness function to involve only the bond stretch-
ing (C-O) and the angle bending (O-C-Cα and O-C-
N). Consequently, we can calculate the positions of all
O atoms on the backbone independently when coordi-
nates of N, C and Cα atoms are given.
Based on the independency of the O atoms, we
propose a two-phase refinement method (TPRM) to
refine all O atoms on the backbone. Our idea is il-
lustrated as Figure 3. We first define coarse moving
scope, denoted as ρ, to be the boundary of the cube
centering at the initial O position. The coarse reso-
lution within the coarse moving scope, denoted as δ,
is the number of grid points on each side of the cube,
where each grid point represents one candidate posi-
tion of the O atom. Accordingly, there are δ3 pos-
1.2  Å
(ρ, δ) = (±0.6Å, 5)
(λ, є) = (±0.12Å, 2)
τ = 3
Figure 3: The illustration of our two-phase refinement
method.
sible candidates. In the first phase of TPRM, within
the coarse moving scope, we try all δ3 possible can-
didates exhaustively. Then, we select τ candidate po-
sitions with smaller, i.e. better, energy, each as the
center of one fine cube bounded by the fine moving
scope, denoted as λ, to be further investigated. Let
the fine resolution of the fine cube be denoted by ². In
the second phase, in each of the fine cube, we also try
all ²3 possible positions exhaustively to seek out the
structure with minimum energy. In summary, for each
O atom, the amount of modified energy computations
(fitness function) is δ3 + τ²3.
In the reconstruction of all O atoms, the goodness
of each candidate position is measured by the fitness
function. The solution with a lower score is the better
one. As mentioned above, our fitness function only
considers the bonded potential energy which involves
the O atom on the backbone. Thus, we define the fit-
ness function as follows:
EO = Kb(b− b)2 +Kθ1(θ1 − θ1)2 +Kθ2(θ2 − θ2)2,
where Kb, Kθ1 and Kθ2 are the force constants for
the bond stretching (C-O) and the angle bending (O-
C-Cα and O-C-N), respectively; b, θ1 and θ2 are the
input bond length (C-O) and bond angles (O-C-Cα
and O-C-N), respectively; b, θ1 and θ2 are the stan-
dard (average) values of bond length (C-O) and bond
angles (O-C-Cα and O-C-N) extracted from PDB, re-
spectively. After calculation, the force constants and
standard values of the fitness function are obtained.
3 Experimental Results
The experiments are performed on a PC with AMD
AthlonTM 64 X2 dual core processor 3800+ 2.01 GHz
and 1.46 GB RAM. The operating system is Microsoft
Windows XP Professional Version 2002 Service Pack
2.
Our method has been tested on 32 proteins, refer-
ring to the experimental results proposed by Maupetit
Table 3: The execution time (seconds) of SABBAC
v1.2, Wang’s method and our method.
PDB SABBAC Wang’s Our
ID v1.2 method method
111M 51 4 10
1CTF 39 2 5
1IGD 38 1 3
1OMD 38 3 7
1SEMA 38 1 3
1TIMA 131 7 18
1UBQ 38 2 5
2CTS 488 13 32
2LYM 50 3 8
2MHR 39 3 8
2PCY 38 3 7
2OZ9 38 3 7
4PTI 37 1 3
5CPA 257 9 22
5NLL 50 4 10
1PXZA 374 10 26
1RKIA 39 2 6
1S7LA 76 4 11
1T70A 180 8 19
1TXOA 110 8 18
1V0ED 2327 19 49
1V7BA 63 5 13
1VB5B 192 8 20
1VKCA 50 4 10
1VR4A 40 3 7
1VR9A 50 3 8
1WMHA 38 2 5
1WMIA 38 2 5
1WPBG 83 4 11
1X6JA 37 2 5
1XB9A 53 3 7
1XE0B 54 3 7
Total 5174 149 375
Tables 3 shows the execution time of SABBAC
v1.2, Wang’s method and our method. According to
the publishers of SABBAC v1.2, their machine is sim-
ply a dual core processor PC, similar to ours. There-
fore, the required time of our prediction and SABBAC
would reflect the efficiency of algorithms, rather than
the performance of machines. As one can see that the
execution time required for our method is much less
than that for SABBAC v1.2. Note that, our execution
includes the execution time of Wang’s method. As
shown in Tables 3, the additional time spent by our
method after the Wang’s method is not too much.
4 Conclusion
In this paper, we propose a two-phase refinement
method to establish the 3D coordinates of all O atoms
on the protein backbone. First, we try all possible
grid positions in a coarse cube exhaustively. Then,
we select some candidate positions with smaller en-
ergy, each as the center of one fine cube to be further
considered. In each of the fine cubes, we also try to
test all possible grid positions exhaustively to seek out
the structure with minimum energy.
Furthermore, we modify the energy function to
a simplified one with the statistical data on the bond
lengths and bond angles of the 21 distinct amino acids
(including the nonstandard one), to improve the recon-
struction accuracy. The experimental results show that
the reconstruction accuracy of our method is better
than most of the previous works, such as MaxSprout
and Adcock’s method. In addition, we not only im-
prove Wang’s results but also outperform SABBAC in
most of the proteins. The solutions of our method gain
the same standard deviations of Wang’s method and
are more stable than the remaining previous works.
Besides, our method runs much faster than SABBAC
v1.2.
On the other hand, our method cannot deal with
all kinds of protein structures, such as nonstandard
residues. The future works include the modification
of the force constant of the energy function, and the
improvement of the prediction accuracy. Besides, the
refinement of all N and C atoms on protein backbone
might be achieved with other evolutionary algorithms
based on the AMBER force field.
References:
[1] S. A. Adcock, “Peptide backbone reconstruction
using dead-end elimination and a knowledge-
based forcefield,” Journal of Computational
Chemistry, Vol. 25, pp. 16–27, 2004.
[2] J. Arunachalam, V. Kanagasabai, and N. Gau-
tham, “Protein structure prediction using mutu-
ally orthogonal latin squares and a genetic algo-
rithm,” Biochemical and Biophysical Research
Communications, Vol. 342, pp. 424–433, 2006.
[3] H. M. Berman, J. Westbrook, Z. Feng,
G. Gilliland, T. N. Bhat, H. Weissing, I. N.
Shindyalov, and P. E. Bourne, “The protein data
bank,” Nucleic Acids Research, Vol. 28, pp. 235–
242, 2000.
[4] B. R. Brooks, R. E. Bruccoleri, B. D. Olafson,
D. J. States, S. Swaminathan, and M. Karplus,
“Charmm: A program for macromolecular en-
ergy, minimization, and dynamics calculations,”
Journal of Computational Chemistry, Vol. 4,
pp. 187–217, 1983.
[5] W. D. Cornell, P. Cieplak, C. I. Bayly, I. R.
Gould, K. M. Merz, Jr., D. M. Ferguson, D. C.
Spellmeyer, T. Fox, J. W. Caldwell, and P. A.
Kollman, “A second generation force field for
the simulation of proteins, nucleic acids, and or-
ganic molecules,” Journal of American Chemi-
cal Society, Vol. 117, pp. 5179–5197, 1995.
[6] Y. Duan, C. WU, S. Chowdhury, M. C. Lee,
G. Xiong, W. Zhang, R. Yang, P. Cieplak,
- 1 -
出席國際學術會議心得報告
計畫編號 NSC-97-2221-E-110-064
計畫名稱 蛋白質結構分類與全原子預測
出國人員姓名
服務機關及職稱
安興彥
國立中山大學資訊工程學系 博士班學生
會議時間地點
2009年 7月 13日至 7月 16日
美國拉斯維加斯(Las Vegas)之Monte Carlo Resort Hotel
會議名稱
2009年國際生物資訊與計算生物學研討會
(BIOCOMP'09, The 2009 International Conference on Bioinformatics and
Computational Biology)
發表論文題目 Fast Algorithms for Computing the Constrained LCS of Run-Length EncodedStrings
一、參加會議經過
資訊工程學系博士班學生 安興彥 於民國 98年 7月 13日至 7月 16日，在美國拉斯維
加斯 (Las Vegas, USA)參加 2009 年國際生物資訊與計算生物學研討會 (英文簡稱為
BIOCOMP'09)。我於 7月 12日中午抵達拉斯維加斯，隔天開始參與研討會。
BIOCOMP'09 為綜合型會議 WORLDCOMP'09 (The 2009 World Congress in Computer
Science, Computer Engineering, and Applied Computing) 底下的其中一個研討會，
WORLDCOMP'09 的網址為http://www.world-academy-of-science.org/worldcomp09/ws。本次
WORLDCOMP'09 共包含了 22 個不同的研討會，同時於拉斯維加斯舉行。自 2006 年起，
WORLDCOMP 每年皆在拉斯維加斯舉辦，今年已經是第四年。而其所涵蓋的研究課題均與
資訊工程領域有關：舉凡硬體設計、影像處理、資料探勘、數位學習、分散式計算、人工智
慧、軟體工程、電子商務，以及生物資訊等著名的研究主題，在 WORLDCOMP 底下均各自
有其專屬的研討會。我此次所參加的 BIOCOMP'09即為關於生物資訊的研討會。根據網站提
供之統計資料顯示，去年WORLDCOMP'08共有來自 82個國家，超過 2000位世界各地的學
者參加，而今年的參加人數則預估為 2500位。
與會人員一開始在大會議廳聽完開場的WORLDCOMP'09的專題演講之後，便分別前往
各自有興趣的會場聽取論文發表，我參加的是 BIOCOMP'09研討會，並且被安排在第四天(7
月 16日) 中午 12:20 ~ 12:40進行報告。本次 BIOCOMP'09論文集中共計收錄有 130篇一般
論文與短篇論文，其中短篇論文約 10 篇，另外有若干 poster 論文於研討會上發表(此為自行
估計值，BIOCOMP'09網站與論文集中並無明確統計數量)。我所發表與報告的論文為一般論
文，根據論文集所述，其中一般論文的錄取率約佔全部投稿論文的 27%。在 BIOCOMP的論
文發表中，有許多場次的論文與我的研究專長領域差異極大，一開始雖然曾經嘗試努力聽講，
- 3 -
除了專題演講之外，就是一般的論文發表。我所發表的論文 “Fast Algorithms for
Computing the Constrained LCS of Run-Length Encoded Strings”被安排在 7月 16日中午報告。
此篇論文針對既有的 Constrained LCS 問題，提出了新的想法以及演算法。在之前提出的
Constrained LCS 演算法中，在得到最終答案之前，整個動態規劃表 (Dynamic Programming
Lattice) 都必須被計算過。既使是使用 Hunt-Symanski 形式的演算法，也只能免去那些非配
對(mismatched)元素的計算。而我所發表的論文則是又進一步提出了時間及空間複雜度的改
進。
由於我被安排在早上場次的最後一位進行報告，時段並不是很好 (12:20 ~ 12:40)，因為
大部分的人可能都已經忍不住飢餓去用午餐，因此在報告之前，一度擔心會不會只剩下兩三
位聽眾願意留到這麼晚才去用餐。但在我進行報告的時候，才發現是我多慮了，仍有約有十
多位學者在現場。在進行論文報告之前，我先準備了一張投影片為與會學者介紹台灣及高雄，
我首先用世界地圖介紹台灣與高雄的地理位置，另外也介紹了台灣有名的慈濟功德會以及目
前正在舉行的高雄世界運動會。後來發現，當提到慈濟時，幾乎沒有人有反應，可能他們幸
運地沒遭遇過天災。但是當提到世界運動會與 ESPN 體育頻道時，有幾位與會學者便會心一
笑，表示他們平常都有在看體育頻道的習慣。
另外，我的指導教授楊昌彪老師特別提醒，報告的內容不可過於偏向演算法，因為聽眾
可能都是生物學家，而非資訊背景的學者，應將問題定義清楚說明。因此我的報告著重於讓
台下的聽眾知道我們研究的題目，以及我們所得到的結果，對於解題的細節則僅是帶過而已。
另外，為了將問題定義講得淺顯一些，我大量製作了概念式的圖表來解釋。原本因為是頭一
次以英文在國際會議上進行口頭報告，深怕我的英文會使台下的聽眾不知所云。不過，在我
報告後，台下竟然響起熱烈的掌聲，我猜想可能是聽眾覺得以我的英文程度可將報告內容說
明得如此清楚，他們覺得很難能可貴吧。當然，也可能是大家都肚子餓了，所以我報告完畢
後，終於可以用餐而鼓掌…。
另外值得一提的是，在我同一個 session中，也有一位台灣高速電腦中心的學者參與報告。
聽取他以及其他外國學者的報告之後，覺得雖然我們台灣的英文水準是不太好，但是聽眾是
否能夠吸收的關鍵並不完全取決於英文程度，講者的技巧更是一個決定性的因素。因為有些
學者英文很好，但是因為緊張的關係，報告的速度愈來愈快，導致重點都完全沒交代清楚；
但也有學者聽起來英文不太好，但是靠著穩健的台風，以及清晰的表達，仍然可以使聽眾瞭
解他的演說。我原本打算在 session結束後，找另一位台灣學者聊聊，只可惜他和多數報告者
一樣，在完成自己的報告後便直接離場了，所以無緣進一步認識他。
在每天會議行程結束時，我均利用晚上剩餘的時間參觀拉斯維加斯。後來，結束拉斯維
加斯的行程之後，我便轉往洛杉磯去參觀迪士尼樂園以及環球影成等著名觀光景點。此行到
美國去，讓我瞭解到自己英文程度的不足，但也發現到，如果願意開口說英文，其實英文的
聽說能力便會快速的提升。因此，讓我體認到回到台灣之後，仍應該繼續加強自身的英文能
力，否則是無法在激烈的競爭環境中生存。這次出席 BIOCOMP'09對我來說是相當寶貴的經
Fast Algorithms for Computing the Constrained LCS of
Run-Length Encoded Strings
Hsing-Yen Ann, Chang-Biau Yang, Chiou-Ting Tseng and Chiou-Yi Hor
Department of Computer Science and Engineering,
National Sun Yat-sen University, Kaohsiung 80424, Taiwan
Abstract— In this paper, we consider the constrained
longest common subsequence (CLCS) problem where the two
sequences X , Y and the constrained sequence P are all in
run-length encoded (RLE) format. The lengths of X , Y and
P are n, m and r, respectively, and the numbers of runs
in RLE format are N , M and R, respectively. Previously,
the CLCS problem can be solved by an O(nmr)-time
algorithm based upon dynamic programming (DP) technique
or an O(rR log log(n+m))-time Hunt-and-Szymanski-like
algorithm, where R is the total number of ordered pairs
of positions at which the two strings match. In this paper,
we show that after the sequences are encoded, the CLCS
problem can be solved in O(NMr+ r×min{q1, q2}+ q3)
time, where q1 and q2 denote the numbers of elements in the
bottom and right boundaries of the partially matched blocks
on the first layer, and q3 denotes the number of elements
of whole boundaries of all fully matched cuboids in the DP
lattice. If the compression ratio is good, our work obviously
outperforms the previously known DP algorithm and the
Hunt-and-Szymanski-like algorithm.
Keywords: design of algorithms, longest common subsequence,
run-length encoding, constrained LCS.
1. Introduction
The longest common subsequence (LCS) problem is a
well-known measurement for computing the similarity of
two strings. Given a sequence or string, a subsequence is
formed by deleting zero or more elements arbitrarily. The
LCS problem measures the length of the longest subse-
quence which is contained in the both given sequences. It can
be widely applied in diverse areas, such as file comparison,
pattern matching and computational biology. For example,
if two DNA sequences are given, the longer the LCS of
DNA is, the more similar the two DNA sequences are. The
most referred algorithm to solve the LCS is proposed by
Wagner and Fischer [1]. Other advanced algorithms can also
be found in some papers [2], [3], [4], [5].
Run-length encoding (RLE) is a well-known and simple
method for compressing strings [6]. It divides a string into
a sequence of runs where each run is a maximal repetitive
substring of an identical symbol and can be represented as
a pair, the symbol and the length. For example, a string
aaaddccccbbbbbb is encoded as a3d2c4b6 in the RLE format.
A famous application of RLE is the optical character recog-
nition (OCR) system, in which binary alphabet is used and
the RLE usually achieves good compression ratio. It would
be more useful and efficient if we can develop the algorithms
which depend on the parameters such as the numbers of runs
rather than the lengths of the uncompressed strings.
In our previous work [7], we considered the LCS problem
for RLE strings and proposed an efficient algorithm, based
on the dynamic programming (DP) technique, which out-
performs the previous works [8], [9], [10]. In this paper,
we consider a recent variant of the LCS problem, the
constrained LCS (CLCS) problem, which was first addressed
by Tsai [11]. We will solve it by using the idea similar to
our previous work [7].
Given two input sequences X , Y and a constrained se-
quence P , the CLCS problem is to find the longest common
subsequences Z of X and Y such that P is a subsequence
of Z. Tsai also proposed an algorithm to solve this problem
in O(n2m2r) time, where n, m and r are the lengths of X ,
Y and P , respectively. Two improved algorithms [12], [13]
based on the DP technique were presented independently
for solving this problem with O(nmr) time and space
complexity. Recently, Iliopoulos and Rahman [14] proposed
a Hunt-and-Szymanski-like algorithm [3] and employed a
special data structure, van Emde Boas tree [15], to solve
this problem in O(rR log log(n+m)) time, where R is the
total number of ordered pairs of positions at which the two
strings match. The Hunt-and-Szymanski-like algorithms are
very efficient when R is small. However, it should be noted
that R = O(nmr) in the worst case.
In this paper, we consider how to compute the CLCS of
the strings which are in RLE format. We first propose a
simple algorithm which extends the concepts of the pre-
vious works [8], [16]. Then we show how an improved
algorithm works, which employs the concept of our previous
work [7] and solves the CLCS problem in O(NMr +
r × min{q1, q2} + q3) time, where q1 and q2 denote the
numbers of elements in the bottom and right boundaries
of the partially matched blocks on the first layer, and q3
denotes the number of elements of whole boundaries of all
fully matched cuboids in the DP lattice. The definitions of
those blocks will be given in later sections.
The rest of this paper is organized as follows. In Section
2, we present the preliminary concepts and properties of
a8b3 c4 a5b8 c4 a4
b6
c
4
a12
a3
L0 L1 (p1=a)
a8b3 c4 a5b8 c4 a4
b6
c
4
a12
a3
a8b3 c4 a5b8 c4 a4
b6
c
4
a12
a3
L2 (p2=b)
(a) (b) (c)
v2
v1 v0
u2
u1 u0
Fig. 1
AN EXAMPLE OF THE DP LATTICE SOLVING THE CLCS PROBLEM. (A) THE LOWEST LAYER, L0 . (B) L1 , WHERE p1 = a. (C) L2 , WHERE p2 = b.
(a) (b) (c)
Fig. 2
(SEE [7]). THE ALGORITHMS WHICH SOLVE THE LCS PROBLEM FOR RLE STRINGS BY USING THE DP TECHNIQUE. (A) BUNKE AND CSIRIK [8]. (B)
LIU ET AL. [10]. (C) ANN ET AL. [7].
Corollary 1: (Constrained LCS of RLE strings). Given
two input strings X and Y , a constrained sequence P , and
two distinct symbols a and b, the following conditions hold:
1) CL(Xas1 , Y as2 , Pas3) = CL(Xas1−s, Y as2−s,
Pas3−s) + s, where s = min{s1, s2, s3}.
2) CL(Xas1 , Y as2 , P bt) = CL(Xas1−s, Y as2−s,
P bt) + s, where s = min{s1, s2}.
3) CL(Xas, Y bt, P ) = max{CL(Xas, Y, P ),
CL(X,Y bt, P )}.
4) (Merged white blocks).
CL(Xas11 as22 · · · asii , Y bt11 bt22 · · · btjj , P ) = max{
CL(Xas11 as22 · · · asii , Y, P ), CL(X,Y bt11 bt22 · · · btjj , P )
}, where ai′ 6= bj′ for each i′ ∈ [1, i] and j′ ∈ [1, j].
According to the facts shown in Corollary 1, we can apply
the concept of block splitting proposed by Bunke and Csirik
[8]. Note that the DP lattice to be divided for the LCS
problem [8] is in 2-dimensional space, however, the DP
lattice for solving the CLCS problem is in 3-dimensional
space. Therefore, in our new algorithm, a 3-dimensional DP
lattice is divided into N × M × R cuboids, where each
cuboid corresponds to a tuple of runs of X , Y and P .
For example, cuboid Ci,j,k is denoted as the tuple of RXi,
RYj and RPk. Ci,j,k is called a fully matched cuboid if the
symbols of RXi, RYj and RPk are identical. Ci,j,k is called
a partially matched cuboid if the symbol of RXi is identical
to the symbol of RYj and different from the symbol of RPk.
Otherwise, Ci,j,k is called a mismatched cuboid.
As shown in Figure 1, if only one layer of the DP
lattice is considered, the diagonal pattern blocks, gray blocks
and white blocks represent the slices of the fully matched
cuboids, partially matched cuboids and mismatched cuboids,
respectively. The behaviors of the gray blocks and white
blocks of Figure 1 are the same as the matched blocks and
mismatched blocks in Bunke and Csirik’s algorithm [8]. That
is, the elements in these blocks only depend on the other
elements on the same layer. However, the elements in the
diagonal pattern blocks depend on the lower layers.
We should also note that, only the boundary elements of
each cuboid of the DP lattice have to be evaluated. This
means that the evaluated elements of each cuboid form a hol-
low cuboid, as shown in Figure 3. It is clear that, under the
random access machine (RAM) model, we can easily apply
a8b3 c4 a5b8 c4 a4
b6
c4
a12
a3
v
1
v
2
v
3
v
4
v
5
u
0
u
2
u
1
v
0
v
8
v
7
v
6
a8b3 c4 a5b8 c4 a4
b6
c4
a12
a3
v
0
v
1
v
2
(a) (b)
Fig. 4
THE EVALUATIONS OF THE ELEMENTS ON THE BOUNDARIES. (A) THE EVALUATIONS BY TRACING THE FORCED PATHS, WHERE THE CURRENT
SYMBOL OF P IS b. (B) THE EVALUATIONS OF THE BOUNDARIES OF THE DIAGONAL PATTERN BLOCKS, WHERE THE CURRENT SYMBOL OF P IS a.
[7] H. Y. Ann, C. B. Yang, C. T. Tseng, and C. Y. Hor, “A fast and
simple algorithm for computing the longest common subsequence of
run-length encoded strings,” Information Processing Letters, vol. 108,
pp. 360–364, 2008.
[8] H. Bunke and J. Csirik, “An improved algorithm for computing the
edit distance of run-length coded strings,” Information Processing
Letters, vol. 54, no. 2, pp. 93–96, 1995.
[9] A. Apostolico, G. M. Landau, and S. Skiena, “Matching for run-
length encoded strings,” Journal of Complexity, vol. 15, no. 1, pp.
4–16, 1999.
[10] J. J. Liu, Y. L. Wang, and R. C. T. Lee, “Finding a longest
common subsequence between a run-length-encoded string and an
uncompressed string,” Journal of Complexity, vol. 24, no. 2, pp. 173–
184, 2008.
[11] Y. T. Tsai, “The constrained longest common subsequence problem,”
Information Processing Letters, vol. 88, no. 4, pp. 173–176, 2003.
[12] A. N. Arslan and O¨. Eg˘eciog˘lu, “Algorithms for the constrained
longest common subsequence problems,” International Journal of
Foundations Computer Science, vol. 16, no. 6, pp. 1099–1109, 2005.
[13] F. Y. L. Chin, A. D. Santis, A. L. Ferrara, N. L. Ho, and S. K. Kim, “A
simple algorithm for the constrained sequence problems,” Information
Processing Letters, vol. 90, no. 4, pp. 175–179, 2004.
[14] C. S. Iliopoulos and M. S. Rahman, “New efficient algorithms for the
LCS and constrained LCS problems,” Information Processing Letters,
vol. 106, no. 1, pp. 13–18, 2008.
[15] P. van Emde Boas, “Preserving order in a forest in less than logarith-
mic time and linear space,” Information Processing Letters, vol. 6,
no. 3, pp. 80–82, 1977.
[16] V. Freschi and A. Bogliolo, “Longest common subsequence between
run-length-encoded strings: a new algorithm with improved paral-
lelism,” Information Processing Letters, vol. 90, pp. 167–173, 2004.
[17] M. A. Bender and M. Farach-Colton, “The LCA problem revisited,” in
LATIN 2000: Theoretical Informatics, 4th Latin American Symposium,
Punta del Este, Uruguay, 2000, pp. 88–94.
[18] A. N. Arslan, “Regular expression constrained sequence alignment,”
Journal of Discrete Algorithms, vol. 5, no. 4, pp. 647–661, 2007.
2classification, respectively.
Motivated by Lin’s [8] approaches for feature
subset selection, in this paper, we try to select each
feature individually. Our feature selection is a filter-
ing method which takes advantage of information
gains. To preserve maximal discriminant informa-
tion with minimal features selected, we first rank
features according to their information gains. Then
we average these gains and locate the feature at
which maximal average gain drop occurs. Besides,
we also perform some well-known classifier com-
bination strategies. Our experimental results show
that the best accuracies are 88.3% and 75.7% for
the protein class and fold classification problem,
respectively.
The rest of this paper is organized as follows.
Section II introduces the basic material we use
to perform the classification task, including the
datasets, classifiers and feature evaluation method.
Section III depicts our strategy that is devised to
construct the entire architecture. In Section IV,
the experimental results are demonstrated and then
compared with the previous works. Finally, we give
our conclusion in Section V.
II. PRELIMINARIES
A. Data sets
The experimental data sets are obtained from
Ding’s website, including the training and test-
ing sets. These data sets were retrieved from the
structural classification of proteins (SCOP) [11].
According to the specification, any two proteins
with aligned sequences longer than 80 residues are
less than 35% and 40% identity within the training
and testing data, respectively. No pair of sequences
between the training and testing proteins are more
than 35% identity. Table I lists the numbers of
proteins and folds for the training and testing data.
There are totally 311 and 383 sequences for training
and testing, respectively. In the table, it is observed
that sequences can be divided into four classes, α, β,
α/β, and α+β. Thus, corresponding to these classes,
there are 54, 109, 115 and 33 training sequences and
61, 117, 143, and 62 testing sequences.
B. Support Vector Machines
Support vector machine (SVM) [15], [3] is a
well-established technique for data classification.
Given a training set of n-dimensional instances and
TABLE I
SUMMARY OF TRAINING AND TESTING PROTEINS.
No. of No. of
Class Fold Training Testing
Proteins Proteins
1.all-α
1.α1 13 6
2.α2 7 9
3.α3 12 20
4.α4 7 8
5.α5 9 9
6.α6 6 9
2.all-β
7.β1 30 44
8.β2 9 12
9.β3 16 13
10.β4 7 6
11.β5 8 8
12.β6 13 19
13.β7 8 4
14.β8 9 4
15.β9 9 7
3.α/β
16.(α/β)1 29 48
17.(α/β)2 11 12
18.(α/β)3 11 13
19.(α/β)4 13 27
20.(α/β)5 10 12
21.(α/β)6 9 8
22.(α/β)7 10 12
23.(α/β)8 11 7
24.(α/β)9 11 4
4.α+β
25.(α+β)1 7 8
26.(α+β)2 13 27
27.(α+β)3 13 27
Total 311 383
label pairs (xi, yi), i = 1, 2, . . . , N where xi ∈ Rn
and y ∈ {−1,+1}, the SVM solves the following
optimization problem:
min
w,b,ξi
1
2
wTw + C
N∑
i=1
ξi,
subject to yi(w
Tφ(xi) + b) ≥ 1− ξi,
ξi ≥ 0.
(1)
The function φ maps the training vectors xi
into a higher dimensional space, namely feature
space. SVM finds a linear separating hyperplane
with normal vector w and offset b that constitutes
the maximal margin in the feature space. The slack
variable ξi is introduced to ensure that feasible
solutions always exist. C denotes the penalty of
errors in the optimization problem. To describe
the similarity between vectors in the feature space,
the kernel function, K(xi,xj) ≡ φ(xi)Tφ(xj), is
defined. The kernel function also determines the
complexity of the target decision boundary. Among
4TABLE II
SUMMARY OF FEATURE SETS.
Symbol Feature Dimension
C Amino acids composition 20
S Predicted secondary structure 21
H Hydrophobicity 21
P Polarity 21
V Normalized van der Waals volume 21
Z Polarizability 21
A PCA of B feature set 311/54/109/115/33
B PCA of SB feature set 311/54/109/115/33
the extracted feature numbers are given as 54, 109,
115, and 33 for each feature set. PCA helps us
to use fewer features to represent the information.
Thus concatenating with the above mentioned 125
features, it yields 747, 233, 343, 355 and 191
features for further selection. Table II summarizes
all available features used in this paper. Each node
in the HLA ensemble owns its individually available
features.
III. PROTEIN FOLD CLASSIFICATION
A. Feature Selection and Combination
According to Huang’s results, they pointed out
that training involving ’CSHPVZ’ features obtains
a higher accuracy than that involving only ’B’ (or
’SB’) feature set. This may imply that ’CSHPVZ’
feature set are more significant than ’B’ (or ’SB’).
Besides, they also showed that combining ’B’ and
’SB’ with ’CSHPVZ’ features further improves the
accuracy. It implies that ’B’ and ’SB’ feature sets
do contain some extra competent information over
’CSHPVZ’ one. This motivates us to treat ’CSH-
PVZ’ and ’B+SB’ feature sets separately.
Since a lot of available features can be used,
this raises another issue: how many features should
be preserved? Although keeping only a subset of
features would simplify the behavior represented
by the classifier, it usually would deteriorate the
fitness to the data. Even though, we would probably
benefit from achieving a better generalization. Thus,
it seems reasonable to impose some constraints
to the learning algorithm [10]. Given the same
kernel function in the SVM training, the classifier’s
complexity may be determined by the number of
involved features.
In the feature selection stage, our idea to
keep features is as follows: to preserve maximal
discriminant information with the minimal number
of features selected. Thus, we adopt the idea
of average information gain as it represents the
tradeoff between the simplicity and goodness of
fitness of the model. Our procedure for feature
selection is given as follows:
Procedure Feature selection.
Input: Training data with feature set S, where |S| =
n and S may be ’CSHPVZ’ (125 features) or
’AB’ (108, 218, 230, or 66 features).
Output: Selected features, a subset of S.
Step 1: Calculate the information gain of each fea-
ture.
Step 2: Sort the features according to their informa-
tion gains in nonincreasing order and obtain
f1,f2,. . . ,fn .
Step 3: For each p, 1 ≤ p ≤ n, calculate the average
information gain as follows:
Fp = 1p
∑
1≤j≤p fj .
Set smoothing factor t = 1.
Step 4: For k1 = t, k2 = t + 1 , find the feature
at which local maximal drops of average
information gains occur according to:
p1 = arg max
1≤p≤n
Fp − Fp+k1
Fp−k1 − Fp . (4)
p2 = arg max
1≤p≤n
Fp − Fp+k2
Fp−k2 − Fp . (5)
Step 5: Check if selected feature subsets D(k1) =
[f1,f2,. . . ,fp1] and D(k2) = [f1,f2,. . . ,fp2]
are identical. If so, output D(k1) and then
stop.
Step 6: Build classifiers and use the training data
to validate if D(k2) outperforms D(k1). If
not, output D(k1) and then stop. If D(k2)
is superior and achieves 100% classification
rate for the training data, output D(k2) and
then stop. Otherwise, set t = t+ 1 and goto
Step 4.
k is the smoothing factor for the calculation of the
maximal drop and p serves as the cut-out point for
feature selection. To determine the suitable value
of k, we adopt a greedy approach to evaluate the
preserved features. That is, once an appropriate k
is obtained, the iteration is stopped. In this paper,
we find that k = 1 or 2 is capable of picking out
maximal drops and thus it is used through out our
experiments.
B. Majority Vote
The majority vote (MAJ) [14] assigns an un-
known input x to the most representative class
6TABLE IV
AN EXAMPLE OF THE HBKS TABLE.
Level 1 Level 2
Prediction Prediction True class
D1 D2 D3 D4 D5
P1-P4 F1-F6 F7-F15 F16-F24 F25-F27 F1-F27
...
...
...
...
...
...
TABLE V
SELECTED FEATURES AND CLASSIFICATION ACCURACIES (%)ON
THE TRAINING DATA.
k=1 k=2
Feature (a) (a+b) (a) (a+b)combination
Level 1 95.5(38) 96.4(64) 95.5(38) 96.4(64)
Level 2-1 46.3(2) 59.2(10) 100(22) 100(30)
Level 2-2 100(58) 100(85) 100(58) 100(85)
Level 2-3 100(53) 100(74) 100(53) 100(74)
Level 2-4 100(23) 100(28) 100(23) 100(28)
(a): The subset of the ’CSHPVZ’ feature set.
(a+b): The subset of the ’CSHPVZ’+’AB’ feature set.
IV. EXPERIMENTAL RESULTS
A. Feature Selection
We use the training data to compute the infor-
mation gains in the first level. Then we arrange the
training data according to their protein classes and
perform the same procedure in the second level.
By applying the approach mentioned in Section
II, we select features for the HLA-SVM trainings.
The smoothing factors, preserved feature numbers
(shown in parenthesis) and their corresponding ac-
curacies (in percentage) on the training data are
listed in Table V. In the table, (a) and (a+b) denotes
the subsets of ’CSHPVZ’ and ’CSHPVZ’+’AB’
feature sets, respectively. It is observed that except
for the first protein class in level 2, classification
accuracies are generally not affected by smoothing
factors. Besides, for this special case, the accuracy
is significantly low when k = 1. Thus, k = 2 is
adopted for the subsequent experiments. All selected
features are listed in Table VI. In this table, the se-
lected features are arranged in hierarchy. All entries
are futher divided into two parts, which represent
(a) and (a+b) feature sets. Each feature in the table
is expressed by concatenating its feature symbol,
as illustrated in Table II, and feature identifier. For
example, C2 denotes the second feature from the
amino acids composition set.
TABLE VI
SELECTED FEATURES.
Selected features
Level 1
C2,C3,C5,C6,C8,C9,C14,C15,C18,S1,
S2,S3,S6,S7,S9,S12,S16,H3,H4,H5,H7,
H8,H9,H13,H14,H16,H17,H18,H19,V1,
V4,V9,V13,P2,P7,P19,Z5,Z18
A1,A5,A24,A26,A53,A55,A61,A64,
A78,A81,A90,A110,A116,A117,A162,
A179,A192,B10,B14,B43,B48,B60,B90,
B127,B130,B140
Level 2-1
C1,C2,C3,C4,C5,C6,C9,C15,C16,C17,
S1,S2,S3,H4,H11,H12,V2,V7,V8,V12,
Z12,Z21
A8,A13,A21,A22,A38,B22,B36,B38
Level 2-2
C2,C4,C5,C6,C7,C10,C11,C12,C13,C14,
C17,C19,S4,S5,S6,S11,S14,S19,S20,H2,
H3,H5,H6,H11,H15,H16,H17,H18,H19,
H20,H21,V1,V2,V3,V4,V10,V12,V17,
V18,P1,P6,P8,P9,P11,P13,P14,P17,P20,
P21,Z3,Z4,Z5,Z7,Z11,Z12,Z14,Z16,Z18
A1,A6,A8,A20,A38,A41,A47,A48,A51,
A52,A56,A71,A87,A101,B4,B5,B28,
B36,B43,B45,B46,B48,B62,B65,B95,
B100,B102
Level 2-3
C3,C5,C6,C7,C8,C9,C11,C12,C14,C15,
C16,C17,C20,S3,S5,S6,S7,S8,S11,S12,
S14,S15,H3,H5,V1,V4,V13,V21,P2,P6,
P7,P8,P9,P10,P12,P13,P14,P19,P20,Z2,
Z3,Z5,Z6,Z8,Z10,Z11,Z13,Z15,Z16,Z17,
Z18,Z19,Z20
A1,A9,A34,A58,A59,A68,A89,A92,
A106,B8,B9,B12,B29,B41,B45,B59,
B69,B77,B81,B86,B88
Level 2-4
C1,C2,C3,C7,C8,C9,C10,C13,C14,C16,
S3,S14,S17,S19,V2,P3,P4,P5,Z11,Z17,
Z18,Z19,Z20
A1,B1,B7,B9,B28
B. Classification Accuracies
In this subsection, we shall focus on classification
accuracies, including HLA and HLA-fusion. The
accuracy is defined as Q =
∑
pi/N , where pi
denotes the number of testing targets that belong to
class i and are correctly classified, and N denotes
the total number of testing targets. Table VII shows
accuracies of all configurations for the testing data.
The (a) and (a+b) are HLA-SVMs trained with
’CSHPVZ’ and ’CSHPVZ’+’AB’ feature subsets.
During the recognition stage, each testing sequence
is first classified by the level 1 classifier. The level
1 classifier then determines which classifier in the
level 2 should carry out the subsequent classification
task. Finally, the designate level 2 classifier makes
the fold recognition. It shows that training with par-
tial features achieves better performance than that
with full features. In addition, the improvement in
level 2 are more significant than that in level 1. This
suggests that feature selection imposes more effects
on the data with small amount. If we compare
