以立體環場影像為基礎的虛擬實境系統 
 
    虛擬實境技術 (Virtual Reality) 是一種有著廣泛應用範圍的強而有力工具。 
一些著名的例子和應用領域包括了: 各種模擬器、遊戲軟體、醫學醫療、建築設計、
旅遊觀光和各式各樣視覺視效呈現。這個虛擬實境系統的主要新特點特色是─如相片
影像般真實的高解析度3D虛擬世界，其中無論場景或物件皆是可藉由一組高解析度的
立體全景環場影像來表現。不僅如此，那些在場景中會因自然力而移動的活動物件，
例如葉子，雲朵或者溪流等，也將會程度性地自然變化，增加其應俱的真實感。這個
計畫包括: 最後將完成一本將被由懷利(Wiley)出版社在2008年出版的關於環場影像
成像的專題著作書籍，以及建置實做一套背投影式銀幕之偏光式立體VR系統，並且製
作一個如相片影像般真實的立體世界可供觀賞。至於最後的階段，我們將著重在如何
使用影像式電腦繪圖技術，來提升虛擬環境世界中自然現象的真實感，並且運用自動
調整影像的視差來增進3D立體感知感覺。這個立體虛擬實境VR系統，將會特別適合運
用於虛擬導覽，或是某些3D遊戲，其場景真實感是非常重要但有限制遊走路徑類型的
遊戲。 
 
關鍵字: 虛擬實境、立體環場影像、影像式電腦繪圖技術  
 
 
A Stereo Panorama-based Virtual Reality System 
 
    Virtual reality (VR) is a powerful tool for a wide range of applications. Some 
well-known examples and domains include simulators, games, medicine, architecture, 
tourisms, and all sorts of visualizations. The major new feature of this virtual reality system 
is the hyper-resolution photorealistic 3D virtual world in which a set of 
super-high-resolution stereo panoramic images will be used for representing both scenes 
and objects. On top of that, the moving objects in the scene caused by nature’s forces, such 
as leaves, clouds or streams, will be animated to enhance the realistic appearance of the real 
environment. The project includes finalizing a monograph on panoramic imaging (to be 
published with Wiley in 2008), implementing a wide-angle screen back-projection stereo 
polarized VR system, and creating a photorealism quality virtual world that allows for 
stereo viewing. Towards a final stage, we aim to enhance the “natural” appearance of the 
virtual environment using image-based rendering techniques and to enhance the 3D 
perception by auto-adjustment of the image parallax. The developed VR system will be 
especially suitable for virtual touring or 3D game applications where the realism of the 
scene is critical and spatially limited navigating paths are sufficient. 
 
Keywords: virtual reality, stereo panoramic images, image-based rendering 
研究目的 
    此計畫的主要目的是要為宜大新設的資工所設立一個雖小卻完善的虛擬實境實
驗室，提供研究生實驗與學習之用，在理論與實作並重之下，培育研究生學術研究的
態度與方法。此實驗室的設置亦可視為一個暖身，因為數位多媒體是宜大資工所主要
希望發展的方向，因此本所於「96年教育部補助購置教學研究圖儀及設備計畫」中提
出建置大型 3D Show Room 的想法，期望於不久的將來可以設置一套更大且更先進的
虛擬實境系統，預計要採用大面弧型銀幕提升融入性，重點更會放在如何與宜蘭的人
文特色做完美的結合，這個遠景計畫不僅在技術面上將面臨挑戰，其數位內容的開發
與設計也會成為學生學習的課題。 
 
本計畫研究方面的三大目標如下: 
1. 發展或改進最新的影像式繪圖技術 (image-based rendering) 讓在場景中會因自
然力而移動的活動物件，例如葉子，雲朵或者溪流等，將會程度性地被動態化(成
動畫)以提升真實感。 
2. 發展根據觀賞者位置自動調整左右眼影像的視差 (image disparities) 來增進及
改善立體成像效果。由於影像式虛擬實境雖然成像真實但由於缺少景深資訊，所
以如果採用固定兩張影像視差的方式呈現，則在觀賞時必會產生無法聚焦等不適
的情形。 
3. 拍攝立體環場影像的相關技術研究，例如特殊相機拍攝方式 (i.e., a rotating 
line-sensor camera) 的外在相機參數校正與如何在拍攝時確保立體影像品質的
研討。 
 
本計畫實作部份的主要工作項目如下: 
1. 研究與採購所需設備 
2. 相關文獻收集與報告撰寫 
3. 計畫中提到首先要完成一本將被由UK Wiley出版社在2008年出版的關於環場影像
成像的專題著作書籍。 
4. 建置一套背投影式銀幕之偏光式立體虛擬實境系統。 
5. 對於新的高解析度相機做參數校正工作。 
6. 拍攝多組完整的高解析度環場與物件影像。 
7. 撰寫一個軟體程式可以播放立體環場影像，可自動調整視差。 
8. 製作一個有內涵的虛擬實境 demo 供觀賞。 
 
已上是原本計畫在兩年內要完成的工作與研究項目，其研究成果原計畫要成為2~3位
學生的部份碩士論文內容，以及發表1~3篇國際會議文章。所有的研究經驗與成果都
可以運用至將來更大型的虛擬實境系統。 
 
 
研究方法 
分別針對三個大目標敘述如下。 
 
1. 發展或改進最新的影像式繪圖技術 (image-based rendering) 讓在場景中會因自
然力而移動的活動物件，例如葉子，雲朵或者溪流等，將會程度性地被動態化(成
動畫)以提升真實感。 
 
We first investigate the methods that have been used to achieve the similar goals and 
we found two approaches: one is Panoramic Video Textures [1] and the other is 
Graphcut Textures [2], both illustrate very good results. They both can be applied to 
create animations of water waves and swinging trees. Currently, we are trying to 
implement methods introduced in both papers, namely min-cut optimization algorithm, 
which would help us to understanding two papers’ drawbacks and limitations. We hope 
to extent their works to include more different types of animated background in our 
panorama and even in “stereo” panoramas. (請參考附件四) 
 
 
2. 發展根據觀賞者位置自動調整左右眼影像的視差 (image disparities) 來增進及
改善立體成像效果。由於影像式虛擬實境雖然成像真實但由於缺少景深資訊，所
以如果採用固定兩張影像視差的方式呈現，則在觀賞時必會產生無法聚焦等不適
的情形。 
 
 There has not been many researches to deal with disparities during stereo viewing, few 
similar researches are OmniStereo [3] 和 [4,5]. However, in approaches [3] and [4], it 
assumes that we have a huge set of video data. The finial stereo panorama, having 
appropriate image disparity, will be generated from this video sequence. In our case, we 
have no such video sequence but only two panoramas for left and right eyes. In 
approach [5], it also deals with auto disparity adjustment “during” viewing, but it 
assumes that the depth information of each image pixel is known (due to the 3D model 
is given). In our case, we have no geometric knowledge of the 3D scene (although we 
can estimate “rough” depth values by image corresponding analysis). We like to 
investigate an algorithm that perhaps make use of methods mentioned in [3] or [4] and 
will achieve something similar to [5] in real time. We had a preliminary report on this 
issue, please see (附件五, 部份內容與公式暫時保留不公開). 
 
 
3. 拍攝立體環場影像的相關技術研究，例如特殊相機拍攝方式 (i.e., a rotating 
line-sensor camera) 的外在相機參數校正與如何在拍攝時確保立體影像品質的
研討。 
 
結果與討論 
 
。 計畫中提到的專書“Panoramic Imaging: Sensor-Line Cameras and Laser 
Range-Finders＂從計畫前的完成度40%到現在的90%，雖然仍然比預期完成的時間
晚了一些，但在 editing 期間發掘到的一些新問題與想法，讓我們更體會到 
camera geometric analysis 的重要，針對此特殊相機的參數校正的問題，以及
拍攝時要注意相機参數設定的細節，將會成為我們近期一些新的研究主題。目前
預計於2008年中完成所有修正工作，這段期間會將問題解決與最後校稿，雖然是
遲了半年左右，但寫書過程中的學習比迅速完成還值得。 
 
。 自動調整影像視差的研究結果首版(2007年7月初)請參考附件五，部份內容與公式
暫時保留不公開，因為我們的更新版即將於11月投稿於 2008 International 
Congress on Image and Signal Processing。而且，如果這方面接下來的研究成
果有更大的突破，便會考慮申請專利。 
 
。 拍攝立體環場影像的相關技術研究(已共同指導學生的國際合作方式進行)，成果
報告於附件六(註:此仍是進行中的部份成果報告)。另外有成果在實驗階段，預計
發表於 IEEE Computer Society Conference on Computer Vision and Pattern 
Recognition，這是 Computer Vision 領域裡十分重要的一個會議。 
 
。 相機參數校正研究與實際操作(結果整合於附件一)，此成果與經驗可適用於所有
相機與攝影機，對未來陸續的研究非常有貢獻。 
 
。 發展或改進最新的影像式繪圖技術目前只完成基本的軟體製作，達到於市面軟體
QuickTime VR 相同的功能，預計明年初才會有新的技術突破。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
成果自評 
成果分為學術性與技術性兩方面來整理。 
 
學術性方面:  
* 計畫中提到關於環場影像研究的專書已完成90%。 
* 自動調整影像視差的研究結果即將於11月投稿於 CISP2008 (初稿如附件五)。 
* 拍攝立體環場影像的相關技術研究(已共同指導學生的國際合作方式進行)，預計有 
  成果投稿於CVPR2008 (部分內容如附件六)。 
* 對於每一個研究主題與技術都進行相當完整的 survey and background study，成 
  果以 internal informal reports 的方式呈現於附件一、二、三，其內容將會是 
  碩士論文的一部分。 
 
技術性方面: 
* 研究設備的資料收尋、採購與設置。完成一座背投影式銀幕之偏光式立體虛擬實境 
  系統，如圖一。 
* 相機參數校正研究與實際操作(結果整合於附件一)。 
* 拍攝一組完整的高解析度立體環場影像為實驗基礎，如圖二。 
* 完成一個 Java 軟體程式(達到於市面軟體 QuickTimeVR 相同的功能)，which  
  allows interactive navigation of the 環場影像，如圖三。 
 
我們對於學術與技術上的成果都感到相當滿意，此自評標準必須考量到地域性(學校
與學生現況)、時間與金費的種種條件，我們面對的是一個從無到有的挑戰，凡事起
頭難，不僅僅是一個實驗室(研究團隊)的起頭，也是宜大資工所的起頭，在多重困難
的條件之下，非常慶幸學生的配合度極高，也都相當努力，才能有這些初步成果。 
 
 
研究與計畫的延續與自我期許 
已上成果只是個開始，我們期望此計畫可以繼續獲得補助，接下來還有更多值得研究
的VR相關題目，例如:發展以視覺為基礎的人機互動方式，希望不需要持特別硬體週
邊設備便可以在虛擬的世界中“自然＂自由遊走。 
 
另外，近期於學術性方面，我們期望於近期內在影像式繪圖技術上也能有小小突破，
預計可以將成果投稿至明年的國際會議。技術性方面會將自動調整視差的結果融入在
環場影像 player 的程式中，而且，如果調整視差這方面接下來的研究有更大的突
破，便會考慮申請專利。 
 
 
 
 
 
  
圖二: Stereo panorama (in reduced resolution). 
 
 
 
圖三: Panorama player, a web-based VR system. 
 
附件一 
 
   
(圖一) 魚眼鏡頭的失真圖示    (圖二) no distortion, pincushion, barrel 
 
    透過相機校正的技術，我們可以求得相機的內部和外部參數，而有了相機的
內部和外部參數後，我們就可以從二維的影像座標去求得物體的三維資訊，也可
以從已知的三維資訊去預估影像座標。在相機校正中，所要求得的參數有內部參
數(影像座標和相機座標之間的關係)和外部參數(世界座標中，相機的位置和方
向)，而相機的內部參數可以用來描述相機座標(camera coordinates)與影像座
標(image coordinates)之間的轉換，而相機內部的參數包括：焦距(focal 
length)、principal point(代表光軸(Optical axis)與成像平面的交點)、skew 
factor(因為鏡頭的關係，所以投射到影像座標時，會產會歪斜的現像)、aspect 
ratio(x 與 y 軸的像素比率)或 Scaling factors(x 軸或 y軸放大縮小的係數)
等，若相機內部的結構與鏡頭不變動的話，其內部參數是固定的，與相機擺放的
位置無關，但若是使用可變焦相機的話，其內部參數會隨著變焦焦點的不同而有
所改變。而相機的外部參數則是用來描述世界座標(world coordinate)與相機座
標(camera coordinates)間的轉換，而相機的外部參數包括：相機在三維座標中
的位置與拍攝方向，包含旋轉矩陣(rotation)與位移矩陣(translation)，相機
的外部參數與相機的擺放位置與拍攝方向有關，所以，當相機位置移動時，就需
要在重新校正一次相機的外部參數。 
(圖三) 
附件一 
 (圖四) 
 
‧ 也可以用一個平面透過平移來做校正，但為了準確知道移動路徑，通常需要
精密的移動平台（圖四右） 
優點：可以得到非常精密的校正結果，simple theory 
缺點：需要精密的 3D 校正物體，價格上是昂貴的，more elaborate setup 
Multi-Plane Calibration： 
    方法介於 Photogrametric Calibration 與 Self-Calibration 之間，只須連
續多次的使用 planar pattern imaged 來進行校正工作，也就是說，相機的位置
先固定，然後拍攝 planar pattern imaged，每拍攝完一張之後，planar pattern 
imaged 就做一些移動（translation）或轉動（rotation）然後再繼續的拍攝，
拍攝數張之後，就可以對相機的內部、外部參數做估計。 
例如：Z.Y.Zhang.[2] The technique only requires the camera to observe a 
planar pattern shown at a few (at least two) different orientations 
Calibration using 2D planer pattern(使用 2D 校正物):[8,11] 
‧ 需要觀察平面校正物所做的不同方向轉動或移的來做校正(圖五) 
‧ 不需要知道 2D 平面校正物的轉動或移動的路徑 
優點：安裝和校正參數的過程較為容易，為目前比較流行的使用方法 
   本實驗所使用的 Multi-planer 也屬於此類 
 
  (圖五) 
 
  (圖六) 
 
附件一 
三.方法: 
    方法這邊僅做簡單的介紹和一些需注意的事項，整個詳細的步驟和實驗的結
果數據將在實驗的章節裡進行說明。 
 
    在方法上使用了 Multiple planar patterns 來做此實驗。因為此方法不需
要昂貴的實驗儀器，只須要製作一個 checkerboard 黑白的方格，在操作上較為
簡單，且已有許多現成的軟體可以搭配使用。 
本方法為使用 Jean-Yves Bouguet[6]所製作出的 Matlab 軟體來做校正，此軟體
名稱為 Camera Calibration Toolbox，一開始先製作出 calibration object(校
正物體)，在此實驗中，使用 8*8 的 checkerboard 黑白的方格(圖一)來做相機校
正實驗，製作好方格後，架設二部攝影機，再將二部攝影機分為左右二邊對準校
正的方格，然後將左右二邊的攝影機各拍二十張照片，拍照時，每拍攝一張照片
後，就將校正方格移動或轉動一個方向或角度，然後再繼續拍攝，重複此步驟，
直到拍攝完左右二隻攝影機各二十張照片(拍攝二十張主要是樣本數較大，準確
度會較好)，當完成拍攝照片的動作後，我們就可以接著進行下一步驟，使用
Matlab 軟體搭配 Jean-Yves Bouguet 所製作出的程式，即可開始進行相機校正
的工作。 
 
    一開始執行後會出現如下（圖二）的圖示，點選 Standard，如果圖片的檔
案太大或者圖片的數量太多，才點選 Memory efficient。接下來會出現（圖三）
的方塊選項，然後即可進入相機校正的工作，從照片的讀取到最後 Calibration
的結果和進一步的圖示分析、誤差分析等。 
Read images 是將拍攝好的二十張照片讀入，讀入照片後，對我們的 multiple 
planar patterns 做 Extract grid corners，所以我們點選 Extract grid 
corners，然後選擇要補捉角點的 window size，一般通常設 wintx、winty 為５，
而得到的 window size 為 11*11 pixels，（window size 為 2*wintx、2*winty，
所以為 11*11pixels）（如圖四-1），而如果為廣角或魚眼的攝影鏡頭的話，我們
可以增加 window size，（圖四-2）是另外一種 window size，由於變形較為嚴重，
所以使用 wintx、winty 為８，所以使用的 window size 為 17*17。(下列網址為
window size 的比較) 
原始：http://www.vision.caltech.edu/bouguetj/calib_doc/htmls/example.html [6] 
比較：http://www.robots.ox.ac.uk/~cmei/Toolbox.html  [9] 
 
 (圖一) 
 
附件一 
[ ]
1
W
C
P
P R T ⎡ ⎤= ⎢ ⎥⎣ ⎦  
where R is a 3×3 rotation matrix, T is a 3×1 translation vector, and [ ]R T is a 3×4 
matrix. 相機座標和影像座標的關係為 
x= f c
c
x
z
 
y= c
c
yf
z
 
⇒  
0 0
0 0
1 0 0 1
c
c
c
x f x
y f
z
y
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥=⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
   , 
c
c C
c
x
y P
z
⎡ ⎤⎢ ⎥ =⎢ ⎥⎢ ⎥⎣ ⎦
 
From Ideal Image Plane to Actual Image Pixel 
Case1: γ=0 
 
x c
y c
u s x u
v s y v
= +⎧⎪⎨ = +⎪⎩
    ⇒
0
0
1 0 0 1 1
x c
y c
u s u x
v s v
⎡ ⎤ ⎡ ⎤ ⎡ ⎤⎢ ⎥ ⎢ ⎥ ⎢ ⎥=⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎣ ⎦ ⎣ ⎦
y
 
Case2: γ≠0 
    
 
x c
y c
u s x u
v s y v
′= +⎧⎪⎨ ′= +⎪⎩
  -------------------------(1) 
cos
cos
y yy
y
γ γ′= ⇒ =′  
附件一 
而得到５個內部參數和６個外部參數。 
    接下來就是計算內部參數和外和參數的數值，計算  
我們使用了 Jean-Yves Bouguet 所製作出的 Matlab 程式，此程式是使用了
Zhengyou Zhang[7]和參考 paper[10]的
的方法目前有非常多種，
方式來做校正，並求出其內部參數和外
參數。而完成了相機校正的工作。 
rners 之後，
會自動掉到下一張 image，直到完成二十張 image 為止。[6][10] 
部
 
 
五.實驗: 
    一開始先做好一個 checkerboard pattern，本實驗製作一個８＊８的
checkerboard pattern（圖一），且將二台 camera 先行定位（圖二），並大略的
量出二台相機間的距離，方便和最後的結果可以進行對照。將 checkboard 和
camera 定位好後，開始使用左、右二台相機分別對 checkboard 進行拍攝二十張
不同位置和角度的照片（圖三和圖四），左右相機同時拍攝完之後則換一個位置
或角度，再進行下一次的拍攝，依此步驟，直到左右相機都拍完二十張為止。由
於 Camera Calibration Toolbox 軟體，一次只能校正一台相機的內部參數，所
以我們先對左邊的相機所拍攝出來的 checkerboard pattern 先做校正的工作。
接著開始讀入 image，且點選四個 corners（圖五），點選完四個 co
 (圖一) （圖二） 
    
(圖三 left camera)                     (圖四 right camera) 
 
附件一 
 
  
(圖七) 
 
 
（圖八） 
 
 （圖九） 
註：圖九為 Extrinsic parameters(camera-centered) 
 
 （圖十） 
註：圖十為 Extrinsic parameters(world-centered) 
附件一 
 
 
（圖十三） 
 
 (圖十四) 
 
 (圖十五) 
 
   得到右邊相機的內部參數後，接著我們同樣按下 Show Extrinsic 按鈕，來觀
察外部參數的圖示和按下 Analyse error 來觀察誤差的分佈圖示（如圖十四），
同樣的我們也展示另一個角度所觀察的圖示（如圖十五）。 
 
    最後按下 Save 的動作，進行資料的儲存，並且將檔名改成 Calib_Results_ 
right.mat 以便之後做 Stereo Calibration 時可以使用，這樣我們就完成了右
邊相機的校正工作。 
 
    最後，我們進行左右二隻相機的校正工作，之前我們以經做好了左右的校正
資料和存檔工作，接著就先打開 Stereo Camera Calibration Toolbox 的工具，
接著我們點選 Load left and right calibration files，工具如下圖示（圖十
六）。 
附件一 
    然後讀入之前左右相機校正結果資料檔，在 Matlab 上鍵入 Calib_Results_ 
left.mat 和 Calib_Results_right.mat（如下圖十七）。輸入完之後，會顯示如
下的左右相機的內部參數資料和外部參數資料 (資料如下圖十八所示)。接著我
們點選 Run stereo calibration，進行校正的工作，結果如圖示(下圖為修正之
後的最終結果圖示，圖十九)。 
 
    由上圖我們可以得知最後的 Rotation vector 和 Translation vector 的結
果： 
Rotation vector :   
om=[0.00716  0.03666  0.01279]±[0.01491  0.01624  0.00144] 
Translation vector :   
T=[-208.06663  -0.86472  -1.79122]±[1.19957  1.03771  6.64815] 
 
最後我們點選 Show Extrinsics of stereo rig，來觀察左右二隻相機的位置和
相機和平面的相對位置圖： 
下圖紅色的框框表示左邊和右邊的相機位置，平面的部份則表示了我們先前拍攝
的二十張照片，圖示裡有會標示出哪個位置或角度是屬於第幾張照片，和相機的
距離位置為何等，下面我們拍攝了四個角度的圖片，以多個角度來做為結果的參
考。(圖二十) 
 
 
    
    
(圖二十，展示相機和 image plane 的位置關係) 
附件一 
[2] Zhengyou Zhang, “A Flexible New Technique for Camera Calibration,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 11, Nov 
2000. 
[3] Zhengyou Zhang, “Camera Calibration with One-Dimensional Objects,” IEEE 
Transactions on Pattern Analysis and Machine Intelligence, VOL. 26, No. 7, Jul, 2004 
[4] Fengjun Lv, Tao Zhao, Ramakant Nevatia, “Camera Calibration from Video of a 
Walking Human” IEEE TRANSACTIONS ON PATTERN ANALYSIS AND 
MACHINE INTELLIGENCE, VOL. 28, NO. 9, SEPTEMBER 2006 
[5] R. Cipolla, T. Drummond and D. Robertson ,”Camera calibration from vanishing 
points in images of architectural scenes” 
[6] Jean-Yves Bouguet ,”Camera Calibration Toolbox for Matlab” 
    http://www.vision.caltech.edu/bouguetj/calib_doc/
[7] Zhengyou Zhang 
http://research.microsoft.com/~zhang/
[8] 經濟部學界科專-技術報導 
http://140.113.87.114/cvrc/edm/skill_2.htm
[9] Christopher Mei,” Active Vision Group” 
http://www.robots.ox.ac.uk/~cmei/Toolbox.html  
[10] Yongjie Yan and Qidan Zhu , Zhuang Lin and Quanfu Chen “Camera Calibration 
in Binocular Stereo Vision of Moving Robot” Proceedings of the 6th World 
Congress on Intelligent Control and Automation, June 21 - 23, 2006, Dalian, 
China 
[11] Computer Vision Camera Calibration 
http://www.cs.rutgers.edu/~elgammal/classes/cs534/lectures/Calibration.pdf
[12] Jean-Yves Bouguet ,”Camera Calibration Toolbox for Matlab” 
http://www.vision.caltech.edu/bouguetj/calib_doc/htmls/parameters.html
[13] R. Y. Tsai. A versatile camera calibration technique for high-accuracy 3D 
machine vision metrology using off-the-shelf tv cameras and lenses. IEEE Journal of 
Robotics and Automation, 3(4):323–344, Aug. 1987 
 
附件二 
 
圖 1.1  Ivan e. Sutherland 教授 
 
 
圖 1.2  頭盔顯示器(Sword Of Damocles) 
 
 
圖 1.3  NASA Ames 建立的頭盔顯示器 
附件二 
     
圖 1.6  CAVE 
 
1994年，美國軍方發展出＂虛擬實境駕駛艙＂，利用網路做分散式的軍事作戰模 
擬。在這系統內運用網路做遠距的各軍隊間作戰模擬[4]。 
 
1996年，Tokyo Institute of Technology 發表為CAVE設計人機介面裝置SPIDAR，
它能空間定位並讓使用者感受到力回饋效果[5]。 
 
1997年，Grant 和Reid[6]推出Washout Filter，使得運動平台能在有限的運動空
間裡有效模擬加速度效果。 
 
2000年，加拿大亞伯達大學E. Sharlin等人[7]為CAVE發表了價錢較低的無線光
學式追蹤系統。 
   
 
 
 
 
 
 
 
附件二 
三. 虛擬實境的三大要素 
虛擬實境具備了三大要素(“ I＂)[8]「如圖 3.1 所示」。 
 
 
圖 3.1 虛擬實境所具備的三大要素 
融入性 
    虛擬系統的好壞就在於能不能讓使用者融入於系統之中，融入性的好處是可
以讓人專注在事物上去進行經驗認知或熟慮的思考，而虛擬系統則必須利用此心
理因素讓使用者去擺脫現實環境中的真實，沉浸於電腦的虛擬環境之中。 
  要使虛擬系統容易被使用者所接受約略有下列四項重點[9]： 
1. 虛擬環境中的事物要盡量充滿神秘感、製造新鮮度，激發使用者的探索慾望。 
2. 虛擬實境系統要適當的刺激，給予使用者滿足感。 
3. 避免外界之干擾，整個環境的設計中盡量避免外界的侵入，一但使用者被外
界干擾後，再進入虛擬世界時使用者會喪失些許的興致。 
4. 在虛擬系統中盡量的去製造事件去引導使用者，這樣的做法可以讓使用者認
同虛擬系統進而融入在虛擬的世界裡。 
 
   而虛擬實境的場景是由 3D 模型所建立的，參與者是可以透過不同的虛擬實
境的融入設備(如頭戴式、座艙式、桌上式、投影式…等)與虛擬 3D 世界中任意
用不同的視角(View Point)觀察週遭環境的變動。而使用者是否一定要使用週邊
附件二 
四. 虛擬實境的分類 
  在虛擬實境上的分類，主要的把它分成兩個標準來做分類：第一，以內容的
建構方式來做分類；第二，已呈現的方式來做分類。 
 
以內容的建構方式做分類 
  以內容的建構方式做分類，可分成幾何式、影像式和混合式三種「圖 4.1
所示」[13][14][15]。 
 
圖 4.1  虛擬實境以建構方式分類 
幾何式 
    利用 3d model 建構軟體（如 AutoCad、3D Studio max、Maya、TrueSpace）
建構我們所需的虛擬場景「如圖 4.2 所示」。 
  此購物商城就是以 TrueSpace 建模而成的。利用虛擬軟體的編輯等功能，給
予不同的物件不同的特性，甚至結合特定的裝置達到不同層次的互動效果。目前
較著名的系統軟體有 Division 公司的 DVS、Superscape 的 VRT、Sense 8 的 Wtk、
World up、Gemini 公司的 GVS 等。  
 
圖 4.2  購物商城 3D 模型 
附件二 
因為物體模型是以多邊形的方式呈現，因此只要減少在複雜的虛擬環境中的多邊
形面總數，便可加速顯像速率，達成互動式顯像速率。 
4. 影像快取顯像技術 :  
    所謂的影像快取顯像技術，是指將物體以幾何方式產生的影像 或以其它方
式所獲得的真實影像資料以階層式的架構保存起來，留待後面幾個畫面以此影像
代替幾何資料在進行顯像時使用。 
 
影像式 
    在一般的虛擬漫遊環境中，多是用前所述的 3D 繪圖方式實行。然而，如果
漫遊的範圍廣大，環境景觀複雜，為了求得逼真的效果，需要建構內含多達數十
萬甚至數百萬個多邊形的模型。在每秒至少需 24 次更新頻率的要求下，也只有
超級電腦可能辦到，這對於虛擬實境的普及化來說實在不是個好消息。這種情形
下，遂有另一種解決方式，就是影像式虛擬實境。 
    影像式虛擬實境的觀念很簡單。假設我們在一個蒙古包內，而蒙古包內壁貼
滿了蒙古包四周的風景照片，如果照片貼得天衣無縫，看上去就和沒有這個蒙古
包是一樣的。為了要將照片貼得天衣無縫，拍攝取景時要留意特別留下重複部
份，以便將圖接合。當然，接圖時也要用一些演算法將圖形作些變形，才能接得
順暢。至於圖片的接合，我們假設來源照片都是標準的透視投影，相機焦距不變，
且每張照片中心都在同樣的水平高度上。利用影像比對（Image Matching）可以
算出兩張照片的重覆部份，將之消去，並將影像柔化（Smoothing）以消除兩張
像片間的亮度差異。當然，原始影像品質仍然要儘量好，以增進接圖後的品質。 
    目前，這方面的產品有 Apple 的 QuickTime VR , RealSpace 的 RealVR 及工
研院電通所的 PanoVR。此種做法已成功的用到靜態環境，如美術館/博物館的
瀏覽系統。而此做法若要成功的用到其他 VR 應用，必須能有效的處理使用者與
場景或場景內物體間的互動，且必須去除使用者參觀路徑之限制，這些應用皆需
利用深度資料計算。這是目前尚待努力突破的。  
    這個技術分為兩大部份，一個為虛擬環境的遊走，另一則是從各種角度觀看
特定物件。  
  在虛擬實境的遊走方面，採用的是「全景照片」的技巧，可利用數位相機以
相機中心為旋轉軸且對著 360 度的四周環境去拍攝多張的影像，再利用影像處理
的方式，如歪斜影像的修正（Titled Image Correction）、影像彎曲（Image 
附件二 
為了能整合影像式與幾何式的各項優點，並汰除其缺點，單一的工具已不再
足夠，而必須整合多種工具，解決其間檔案格式轉換與相容性的問題，才能順利
建構出混合式虛擬實境的環境。  
以上三種虛擬實境的優缺點比較，「如表 4.1 所示」[13]： 
內容建構方式 優點 缺點 
幾何式 z 提供立體視覺。 
z 互動性佳，可與使用者產
生互動的效果。 
z 可以自由遊走，這類的效
果在許多的第一人稱遊戲
中已經運用的很純熟了。 
z 因為整個虛擬世界是用 3D
建構軟體所建構出來的，
所以可以產生虛擬場景。 
z 由於是 3D 建構軟體所
建構出來的，所以場景
會不夠真實。 
z 需要眾多的開發人員
的投入，開發的成本較
高。 
z 開發的時間較常。 
z 製作過程難度較高，通
常這類的軟體需要較
高的技術性和美工。 
z 硬體裝備需求較高，需
要大量且即時的計算。
z 場景大小受限於網路
頻寬，若是以網路來瀏
覽時，下載時間過長，
對於頻寬流量較小的
使用者而言，是很不方
便的等待。 
影像式 z 由於是利用真實照片去建
構的，所以場景真實自然。
z 拍攝製作簡單迅速，系統
開發時間較短。 
z 計算量大幅的縮減，不需
高檔的配備。 
z 較不受網路頻寬的限制，
下時間較短，有些甚至不
需下載外掛程式。 
這類的網頁已經在很多地方有
有建構，供人觀賞了。 
z 影像為 2D 平片，無法
提供場景的立體效果。
z 互動性較差，無法與使
用者跟環場影像產生
互動的效果。 
z 由於只是再同一個位
置拍攝，只能定點觀看
360°環場影像。 
z 由於是真實的照片所
建構的，無法處理不存
在的場景。 
z 無法自由的遊走。 
z 使用者亦迷失方向，必
附件二 
桌上型虛擬實境 
      為最經濟的虛擬實境系統，通常使用滑鼠、軌跡球、搖桿等設備做為輸入
設備，而用一般個人電腦螢幕為輸出即可，觀看者可以選擇配戴合適的立體眼
鏡。採用桌上型的虛擬實境，觀看者容易受到外界之干擾或分心，所以在融入性
上的表現就比較不突出。 
   此類的虛擬實境，已經經常的被運用到網路上，美國北卡羅來納州藝術博
物館透過影像式虛擬實境技術「如圖4.6所示」，可展現環場式的場景，將真實
的藝術館氣氛呈現於參觀者的顯示裝置上，以滑鼠、鍵盤等來提供場景的移動，
讓觀者彷彿置身於該展示空間中。 
 
 
圖4.6  美國北卡羅來納州藝術博物館影像式虛擬實境   
 
  桌上型的虛擬實境應用在娛樂遊戲層面已經具有相當蓬勃的發展，面對硬體
技術與軟體支援的提昇，讓遊戲畫面的真實臨場感更為擬真性。迷霧之島3D冒險
遊戲「如圖4.7所示」，遊戲內容的線索全靠玩家的觀察、想像、行動來進行解
迷，採用第一人稱視角的畫面處理，增加視野的瀏覽性與遊戲的刺激流暢度。 
   
  
 
圖 4.7  迷霧之島 3D冒險遊戲 
 
附件二 
  而最簡單也比較常見的方式為利用頭盔顯示器與感應手套「如圖 4.11 所
示」，將視覺、觸覺與聽覺與外界隔離，讓觀賞者融入虛擬世界當中。 
 
圖 4.11  融入型虛擬實境(HMD 與感應手套) 
 
投射型虛擬實境 
  融入型系統是足以展現虛擬實境的特色，讓觀賞者達到完全融入的效果，但
缺點是價格太過昂貴且僅供單人使用，多人使用狀況下、須考慮設備成本以及設
備連線複雜度大幅提高。而投射型虛擬實境利用數個大型投射螢幕隔離出一塊密
閉的觀賞空間，再配合數個投影機及立體聲音輸出裝置，將整個場景在使用者的
周圍投影出來，形成一個環場的場景，便可以提供多人參與體驗虛擬情境「如圖
4.12 所示」。 
 
  1992年，芝加哥大學發展"CAVE"。CAVE（CAVE Automatic Virtual 
Environment）為一具有三個投影螢幕的三度空間影像投射系統，是目前應用最
廣的投影式虛擬實境。 
因為需要有多個投影螢幕「如圖 4.13 所示」，所以畫面同步計算與視覺效
果同步就是所需面臨的問題，傳統的CAVE是靠高等級的多處理器工作站級伺服
器系統來達到同步顯示的效果，例如，芝加哥的伊利諾州立大學EVL實驗室
(Electronic Visualization Laboratory)所設計的CAVE是採用採用了四台Silicon 
Graphics Inc公司的工作站級電腦來作為顯示處理。但是隨著電腦科技的進步，
今日的個人電腦所擁有的計算速度已超過昔日工作站等級的電腦，而分散式計算
是今日電腦技術研發趨勢。因此，可藉由分散平行運算技術在多台個人電腦所構
成的電腦群組環境建構一個投射型的虛擬實境環境。硬體效能的高速成長，使得
附件二 
參考文獻 
[1] 林政宏，＂深入虛擬實境 VR,＂ 碁峰資訊，民國 86 年. 
 
[2] 董基良等合著，＂虛擬實境應用實例 ＂，瑋特擬真科技，2003.04。  
 
[3] 
 
[4] W. D. McCarty, S. Sheaby, P. Amburm, M. R. Stytz, and C.Switzer, ”A Virtual 
Cockpit for a Distributed InteractiveSimulation,”IEEE Computer Graphics 
&Application, Vol.14, No.1,pp.49-54, Jan, 1994. 
 
[5] Y. Cai, M. Ishii, M. Sato, “A Human Interface Device for CAVE SizeVirtual 
Workspace”, IEEE International Conference, Vol. 3,pp.2084-2089, 1996. 
 
[6] P. R. Grant and L. D. Reid,”Motion Washout Filter Tuning: Rulesand 
Requirements,”Journal of Aircraft, Vol.34, No.2, March-April,1997. 
 
[7] E. Sharlin, P. Figueroa, M. Green and B. Watson, “A Wireless, Inexpensive 
Optical Tracker for The CAVE,” Proceedings of IEEE 
on Virtual Reality, pp. 271–278, 2000. 
 
[8] G. Burdea, P. Coiffet, Virtual Reality Technology , JOHN WILEY & SONS, Inc. A 
Wiley-Interscience Publication,1994. 
 
[9] 林尚德，虛擬實境之物性顯示及建模方法之設計及應用研究，中原大學機械
工程學系，碩士論文，2003. 
 
[10] P. Astheimer, M. Poche, “Level-of-Detail Generation and Its Application in 
Virtual Reality”, Proceedings of the Virtual Rea-lity Software and Technology, 
pp299-309, 1994. 
 
[11] J. Cohen, A. Varshney, D. Manocha, G. Turk, H. Weber, P. Agarwal, F. Brooks, 
and W. Wright,“Simplification Enve-lops”, Computer Graphics, Vol 30(SIG-GRAPH 
96) 
 
 
[12] A. J. Schroede, J. A. Zarge, W. E. Lorensen, “Decimation of Triangle Meshes.”, 
附件三 
影像式繪圖技術 (Image-based Rendering) 
 
王浩駿  and 黃于飛 
 
1 INTROD. 
 
    What is the image-based rendering? The goal of IBR techniques which is to 
reproduce the scene correctly at an arbitrary viewpoint, with unknown or limited 
amount of geometry, in applications like virtual reality and computer games can be 
improved over conventionally rendered scenes is made to the view point. 
 
    Previous work on IBR reveals a continuum of image-based representations based 
on the trade-off between how many input images are needed and how much is known 
about the scene geometry. We classify the various image-based rendering techniques 
and their associated representations into two categories, namely pure image-based 
rendering and hybrid geometry as follows table: 
  
 
Figure 1: Classify the various rendering techniques and representative members. 
 
    Image-based rendering is difficult to categorize, because these categories should 
actually be viewed as a continuum rather than absolute discrete ones. Traditional 
texture mapping in the rendering spectrum is relies on very accurate geometric models 
but only a few images. In an image-based rendering system with depth maps, such as 
representative members existed in figure 1, the model consists of a set of image of a 
scene and their associated depth maps. When depth is available for every point in an 
image, the image can be rendered from any nearby point of view by projecting the 
pixels of the image to their proper 3D locations and re-projecting them onto a new 
 1
附件三 
direction (θ, φ), over any range of wavelengths (λ) and at any time (t), i.e., P = 
PP(7)( V , V , V , θ, φ, λ, t). x y z
When we take an image for a scene with a pinhole camera, the light rays passing 
camera’s center-of-projection (COP) are recorded. They can also be considered as 
plenoptic function. As image-based rendering is based on images, it adopts 
description. We define IBR under the plenoptic function framework as follows: 
 
Definition - image based rendering: Given a continuous plenoptic function that 
describes a scene, image-based rendering is a process of two stages – sampling and 
rendering. In the sampling stage, samples are taken from the plenoptic function for 
representation and storage. In the rendering stage, the continuous plenoptic function 
is reconstructed with the captured samples. 
 
6D surface plenoptic function 
The surface plenoptic function (SPF) is simplified from the full 7D plenoptic 
function. As we discussed, when radiance along a light ray through empty space 
remains constant, the plenoptic function can be represented by its values on any 
surface surrounding the scene. SPF chooses the surface as the scene surface itself. For 
regular scene surface with dimension 2, the SPF is 6D: position on the surface (2D), 
light ray direction (2D), wavelength (1D) and time (1D). Although it is difficult to 
apply SPF for capturing real scenes due to unknown scene geometry, SPF was used in 
Spectral Analysis for Sampling Image-Based Rendering Data for analyzing the 
Fourier spectrum of image-based rendering representations. However, a 6D function 
is still too much for a practical image-based rendering system to capture and render. 
 
5D plenoptic modeling 
By dropping out two variables, time t (therefore static environment) and light 
wavelength λ (hence fixed lighting condition), which is a 5D plenoptic function, P = 
PP(5)( V , V , V , θ, φ). This forms a 5D image-based rendering representation: 3D for 
the camera position, 2D for the cylindrical image. 
x y z
 
2.2 LIGHT FIELD 
 
In free space (in regions of space free of occluders), the light field is a 4D, not a 
5D function. An image is a two dimensional slice of the 4D light field. Creating a 
light field from a set of images corresponds to inserting each 2D slice into the 4D 
 3
附件三 
 
The warping can be implemented using an inverse formulation similar to the one 
used for texture mapping . Thus, given the (x, y) coordinates of a point in the image 
plane, one can directly obtain the corresponding (u, v) parameters in the cylindrical 
environment map. This guarantees proper re-sampling for all pixels of the new views. 
 
 
Figure 5: Cylindrical Panorama. VR Lab, NIU. 
 
 
1.2 CONCENTRIE MOSAIES 
 
Concentric mosaics are a generalization of cylindrical panoramas that allows the 
viewer to explore a circular region and experience horizontal parallax and lighting 
effects. In this case, instead of using a single cylindrical image, slit cameras are 
rotated along planar concentric circles. A series of concentric manifold mosaics are 
created by composing the slit images acquired by each camera along their circular 
paths. Thus, a cylindrical panorama is equivalent to a single mosaic for which the axis 
of rotation passes through the camera’s center of projection, such as the case of 
camera  in follows figure. In a set of concentric mosaics, all slit images associated 
to a given column are acquired at the same angle. 
0C
 
 
Figure 6: A concentric mosaic  is created by composing all the slit images 
acquired by camera  along its circular path. 
lCM
lC
 
 2
附件三 
It was observed in both light field rendering and lumigraph systems that as long 
as we stay outside the convex hull (or simply a bounding box) of an object (The 
reverse is also true if camera views are restricted inside a convex hull), we can 
simplify the 5D complete plenoptic function to a 4D lightfield plenoptic function, 
, just like the light field rendering (in section 2.2). ),,,()4( tsvupp =
 
 
Figure 7: One parameterization of the light field. 
 
 
Figure 8: A 2D light field image array. 
 
In figure7, we show an example where the two planes, namely the camera plane 
and the focal plane, are parallel. This is the most widely used setup. An example light 
ray is shown and indexed as . The two planes are then discretion so that 
a finite number of light rays are recorded. If we connect all the discretized points from 
the focal plane to one discretized point on the camera plane, we get an image (2D 
array of light rays). Therefore, the 4D representation is also a 2D image array, as is 
shown in figure 8. To create a new view of the object, we just split the view into its 
light rays, which are then calculated by quad-linearly interpolating existing nearby 
light rays in the image array. For example, the light ray  in figure 6 is 
),,,( 0000 tsvu
),,,( 0000 tsvu
 4
附件三 
 
Figure 9: Image with depth  
Left: View of a synthetic model of a cathedral.  
Center and right: New views of the cathedral obtained using 3D image warping. 
 
 
 
Figure 10: Each sampling ray may contain multiple samples. 
 
In this case, each element of the image consists of an ordered list of samples. 
LDIs that can be warped using Equation in occlusion compatible order. In figure 10, 
as a “pixel” is ready for warping, all samples along the corresponding ray are warped. 
The sample furthest from the novel center-of-projection is warped first and the closest 
is warped last. 
 
By sampling multi-layer surfaces from a single center of projection, LDIs can 
reduce the occurrence of disocclusion artifacts while retaining the efficiency of 3D 
image warping. Such a feature can be exploited in combination with polygonal 
techniques to achieve rendering speedups. While LDIs can reduce the occurrence of 
disocclusion artifacts, they cannot complete eliminate them. Like single-layer depth 
images, their effectiveness tend to be reduced as the desired viewpoint moves away 
from the LDI’s center-of-projection. Another issue (also present in single-layer images) 
is the fixed sampling density defined at the time of the construction of the LDI. In 
order to reduce aliasing, some filtering is required. Avoiding aliasing in texture 
 1
附件三 
is achieved by pre-warping the so-called relief textures and mapping the resulting 
images onto flat polygons. 
 
Relief textures, in turn, are parallel projection images with depth. The use of 
parallel-projection images with depth greatly simplifies the pre-warping, and the 
rendering of complete 3D objects. Figures 12 (left) show the construction process of a 
relief texture: the corresponding surface is ortografically projected onto a reference 
plane and depth is measured as the per-texel distance from the plane to the sampled 
point. Figure 12 (right) shows the corresponding color and depth components of the 
resulting relief texture. 
 
Figure 12: Relief textures. 
Left: Surface sampled to build a relief texture (orthographic projection).  
Right: Color image and depth map associated with a relief texture. Darker regions in 
the depth map indicate more distant surfaces. 
 
Relief texture and the parameters (relative position and orientation) of the 
polygon to be relief texture-mapped. As a result, it generates a pre-warped texture, 
which is conventionally mapped onto the polygon producing a correct view of the 
scene. 
 
4 DRAWBACK AND FUTURE DIRECTIONS 
 
Image-based rendering synthesizes realistic images from pre-recorded images 
without a complex and long rendering process as in traditional geometry-based 
computer graphics. The major drawback of image-based rendering is its inherent 
rigidity. Most image-based rendering techniques assume that the static illumination 
condition. Obviously, these assumptions cannot fully satisfy the computer graphics 
 3
附件三 
5 FINAL REMARKS 
 
We have surveyed recent developments in the area of image-based rendering, 
and in particular, categorized them based on the extent of use of geometric 
information in rendering.  
 
Image-based rendering is still an emerging field. From this survey, it is obvious 
that image-based rendering has attracted many researchers from various communities, 
including computer graphics, computer vision and signal processing. We believe that 
the cooperation of different communities will certainly bring a bright future for 
image-based rendering. 
 
 
6 REFERENCES 
 
[1] S.J. Gortler, R. Grzeszczuk, R. Szeliski and M.F. Cohen, “The Lumigraph,” In 
Computer Graphics, Vol. 30, No. Annual Conference Series, 1996, ACM SIGGRAPH, 
pp. 43-54. 
[2] M.M. Oliverira, “Image-Based Modeling and Rendering Techniques: A survey,” 
RITA-Revista de Informάtica Teόricace Aplicada, Vol. IX, No. 2, Sep. 2002, pp. 37-66. 
[3] B. Choudhury and S. Chandran, “A Survey of Image-based Relighting Techniques,” In 
The Proceedings of the International Conference on Computer Graphics Theory and 
Applications (GRAPP 2006), Aug. 2006, pp. 176-183. 
[4] H.Y. Shum and L.W. He, “Rendering with concentric mosaics,” Proc. SIGGRAPH ’99 
(Los Angeles, CA USA), In Computer Graphics, Vol. 33, No. Annual Conference Series, 
1999, ACM SIGGRAPH, pp. 299-306. 
[5] H.Y. Shum and S.B. Kang, “A Review of Image-based Rendering Techniques,” 
IEEE/SPIE Visual Communications and Image Processing (VCIP 2000), Jun. 2000, pp. 
2-13. 
[6] J.W. Shade, S.J Gortler, L.W. He and R. Szeliski, “Layered Depth Images,” In Computer 
Graphics, Vol. 32, No. Annual Conference Series, 1998, pp. 231-242. 
[7] M. Levoy and P. Hanrahan, “Light Field Rendering,” Proc. SIGGRAPH ’96, In 
Computer Graphics,, Vol. 30, No. Annual Conference Series, 1996, pp. 31-42. 
[8] C. Zhang and T. Chen, “A survey on image-based rendering-representation, sampling 
and compression,” In Image Communication, Vol. 19, No. 1, Jan. 2004, pp.1-28. 
[9] Alan Watt, 3D Computer Graphics third edition, Addison-Wesley Publishing Ltd, 2000. 
[10] Hyun-Jung Kate Shim, Project Image-based Relighting system, 
http://amp.ece.cmu.edu/project/IBRL/, 2001.   
 5
附件四 
 
 
圖一. 此為瀑布的 panoramic video texture 中的其中一個 frame，output 畫面
中瀑布會落下流動。 
 
此技術主要的問題在於如何整合出適合的 video 片段，即是把架設單一
video camera panning 所得到的大量影像資料中，找出那些能互相 stitch 成一
個時間上及空間上為完整週期的子區域，且輸出結果要趨近於輸入的影像。以他
的例子來說，在多次拍攝所得的資料中，去尋找完整的水花子區塊，且這些子區
塊 stitch 在一起後，會成為一個能隨時間做水花變化的完整週期運動片段。 
 
在尋找影像中的各完整子區塊其實是非常複雜的技術，這是因為每個子區塊
都是不規則形狀(圖二)，其中 t為時間，x是拍攝位置由左位移至右，灰底長條
矩形為每次的拍攝範圍，藍色不規則形區域則是各個水花子區塊，將適合的水花
子區塊 stitch 起來就變成下方該範圍的完整變化週期；也有簡單的方法只取矩
形(圖三)，水花區塊活動範圍只取固定範圍，但是呈現的結果卻相對失真許多(圖
四)。 
 
方法的一開始為做 video registration 全面大量調整步驟並同時校正所有
的 video frames，使用的是 feature-based alignment method；接著執行 min-cut 
optimization，它使用 3D Markov random field(MRF)將之前的三維度空間限制
條件至一維度，然後使用 dynamic programming 找出包含各子區塊最大範圍的矩
形(圖五)，再用 hierarchical min-cut 方法縮減其邊緣，找出子區塊的實際範
圍；最後再用 gradient-domain compositing 把 stitch 在一起的子區塊邊緣做
smooth，修飾接縫。 
 
附件四 
在處理的一開始先得將畫面區分靜態部分以及動態部份，這個步驟他們採人
工處理方式，過去曾經有人提出動態區由每個 pixel 隨時間變化的 threshold
來定義出來，但是這個方法在他們的 registration 步驟上有錯誤並且不安定，
因為 digital camera 在 MPEG 壓縮時產生的 noise 會使靜態區域的 pixel 變化高
於緩和變化的動態區域，例如，遇到漣漪這類情況即是如此。 
 
此外，動態區域的運動週期需符合現實情況，所以時間 t 需符合自然週期。
這裡他們給定週期長度範圍 L 最小 30 最大 60，這是因為週期小於 30 會顯的運
動反覆過於頻繁，若大於 60 則會過於消耗硬體資源，所以要再 30~60 間選擇一
個適合的最大週期長度。選擇的方法為把 frame t 與 frame ( t +週期最小值)
到 frame ( t +週期最大值)間的 frame 做比較，也就是 frame t 與 frame ( t+30, 
t+60 )比較，找到 frame t 與 t + L 使得矩形 RGB 值的差異和為最小者，就設定
最大的週期長度為 L-1，此為最佳的週期長度。 
 
他們的操作範圍非常相似於 graphcut video textures，實行在那些反覆或
類似反覆的運動上和有固定結構的隨機現象上的效果非常好，像是在樹的搖曳和
瀑布；但是在不定期的現象上的效果卻不怎麼好，像是在擁擠的餐廳內，所以在
拍攝時，有時有人經過或鳥類闖入鏡頭，他們則採取刪除的做法處理。另外，他
們的結果有時候會失敗，當他們在拍攝時，移動過於快速會使的週期變化上的取
得不足導致結果週期不完整。 
 
對於上述 paper，我們可以對他進行改進，例如，加上指定路徑，可以呈現
車子行走或者魚在水中游動的畫面，還有配上聲音，以及製作360度的環場video 
textures，甚至加上立體效果，使畫面看起來更有深度而更趨近於現實。 
 
 
 
附件五 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
● 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig 1. Geometry of image disparity. 
 
接著由左邊相似三角形可得知 
df D f efqL cdqLD f D e D fc
−= ⇒ =+ − +  
最後得到 
         (1)  
 
Ⅲ. INITIAL OPTIMAL PARALLAX 
   Fig. 2 為觀賞者位置、螢幕( Screen )、影像視差
(disparity)、虛擬立體深度感知聚焦點(虛擬成像)之間
的幾何關係圖。如果直接將 Fig. 1 所拍攝的兩張影像
整齊地重疊，在Ⅱ中得到視差的公式等於是 Fig. 2 中
的 值。此時，若是觀賞者站離在螢幕前 S 距離的
位置，人的左右兩眼的距離設為 B，則其立體深度感
知聚焦點的位置為 Q，此點落在螢幕前面(螢幕與人之
間), 因此其虛擬成像彷彿浮出螢幕。我們想要經由一
個平移(shift)，調整左右影像重疊的相對位置，將原
本在螢幕前的虛擬成像 Q 點，推至螢幕後視深為 D
的位置，藉此還原 Q 點原來的景深，使得整個虛擬世
界立體觀賞更加真實。 
qd
  假設左邊的影像是固定的，我們需將右邊的影像
往右平移 。在 Fig.2 中的實線顯示平移過後的
幾何關係，使得立體聚焦點從 Q 移動到 。 
'd dq q+
'Q
 
 
 
 
 
 
 
 
 
 
 
Fig 2. Geometry of stereo depth perception. 不同的
線條分別代表: “−−−−” 未經平移；“⎯⎯” 平移過
後；“−⋅⋅−⋅⋅−” 觀賞者向後移動後的幾何關係圖。  
 
 
以下為求得 'd dq q+ 的公式。由 Eq. (1) 可直接得知 
 
 
接著由 Fig. 2 實線相似三角形可得知 
 
 
最後相加得到 
 
 
 
Ⅳ. SUGGESTED VIEWING DISTANCE 
   然而計算出來的 值有一限制，就是不能大於
最大的雙眼聚焦容忍度 (human binocular fusibility)
的限制。此容忍度因人而異，因而我們採用 [2][3] 
所得到的一個客觀的研究統計數據，其結論是，若
以一般人兩眼距離平均為 6.5 公分為準，則估測出
最大的雙眼聚焦容忍度為 0.03 乘上觀看者離營幕
的距離。舉例，若此時離營幕 2 公尺，則螢幕上最
大左右眼視差不得超過 6 公分。 
'
qd
附件五 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Ⅵ. CONCLUSIONS 
   於觀賞投影式立體虛擬實境的同時，根據臨場
觀賞的位置，提出一個可自動調整左右影像視差的
演算法，目的是要改善觀賞立體影像的舒適性，降
低長時間觀賞立體影像所會造成的頭昏現象，確保
隨時雙眼都能聚焦，就算是在觀賞位置移動之後仍
可保持一致的立體深度感知。 
 
 
REFERENCES 
[1] C. Ware, C. Gobrecht, and M. Paton. 
“Dynamic adjustment of stereo display 
parameters”, IEEE Tran. on Systems, Man 
and Cybernetics, Part A, 28(1):56-65, 1998. 
 
[2] N. A. Valyrus, Stereoscopy. London, U.K.: 
Focal Press, 1966. 
 
[3] I.P. Howard and B.J. Rogers. “Binocular 
Vision and Stereopsis”, Oxford Psychology 
Series No.29. Oxford University Press, New 
York, 1995. 
 
[4] J. W. Roberts and O. T. Slattery. “Display 
characteristics and the impact on usability for 
stereo“, In SPIE Proc. Stereoscopic Displays 
128-137, San Jose, California, USA, January 
2000. 
 
  此演算法分為以下三個步驟： 
 
Step1. 
   以知左右兩張影像的相對應點 ( ),0xLi 和 ( ),0xRi ；
視差為 ；觀察者所站的位置為 P；眼距為 B。 di
   (omitted)為Zi。 
 
 
 
 
Step2. 
   (omitted) 
    
 
 
 
Step3. 
   (omitted) 
 
 
 
 
將每個求出的 'xLkm 加上 即是 ，分別是相對
應的左右影像點在營幕上的新位置。  
1dk
'xRkm
 
2 
 
Abstract 
The purpose of this project is to find out whether it is possible to acquire satisfactory 
symmetric panoramas under certain camera parameter constrains, in particular, the off‐
axis distance is limited to 18 cm. As background theory, symmetric panorama and 
parameter optimization are discussed in detail in this report.  Computational 
experiments as well as hands‐on experiment are conducted in this project. Result shows 
that in most indoor and outdoor scenes, satisfactory viewing can not be achieved with 
the 18 cm off‐axis distance constraint. 
 
 
 
 
 
 
 
 
 
 
 
 
4 
 
List of symbols 
W is the number of image columns of a symmetric panorama in pixel;  
f is focal length; 
τ is pixel size; 
1D is radius of the inner cylinder; 
2D  is radius of the outer cylinder; 
1H is the distance between a focal point and the point where that projection ray which 
is incident with the base plane intersects the inner cylinder (of radius  1D ); 
fH is the frontal viewing distance;  
SW is the width of screen resolution in pixel;  
WS is the width of the screen size; 
H is the image height in pixel; 
SH is the height of screen resolution in pixel; 
ωd is the disparity in pixel 
ωθ is the angular disparity in degree 
 
 
 
6 
 
 
Figure 1 the stereoscopic panorama imaging model (The figure is adopted from [4]) 
R  is the distance between the rotation axis and the camera’s focal point. The principal 
angle ω and  ω−360 is defined by the angle between the normal vector of the focal 
circle at the associated focal point, and the optical axis of the camera. [1] A panoramic 
pair of ω and  ω−360 is known as symmetric pair.  
    
2.1 RoI 
Range of interest (RoI) is the volume between two concentrated cylinders.  In figure 2, 
RoI is the region between inner cylinder and outer cylinder. Closest objects of interest 
define the inner cylinder with radius 1D , and the outer cylinder with radius  2D  is 
defined by furthest objects of interest. RoI is the geometry region that we mostly 
concern about.  
 
8 
 
(A) (B) show an outward case, and (C) (D) show an inward case.  R is off‐axis distance,   
and ‐   are principal angles.  1H is the distance between focal point C and the object at 
the inner border of RoI.  1V  is the height of the RoI. It is estimated at the inner cylinder. 
The relationship between  1H and  1V can be expressed in the following formula: 
)
2
tan(2 11
ϕHV =  
ϕ is one of the camera’s intrinsic parameters which specifies the camera lens’s vertical 
or angular field of view. Practically, it is easier to measure  1H than to measure 1V . The 
value of  1H should be larger than the shortest focusable distance, and it will be smaller 
than 12D . [2] 
 
2.3 Disparity, angular disparity and depth 
Disparity is defined by the horizontal parallax which is the difference in positions 
between two corresponding points in the left and right panorama, measured in the 
number of image columns between both on the image cylinder. Similarly, angular 
disparity is defined by the angle between two corresponding points in the left and right 
panorama of a symmetric panorama [2].  These concepts can be expressed 
mathematically. Consider a pair of symmetric panoramas  1E  and  2E  of size WH × , for a 
point  ),( yxP in  1E  , we can find a corresponding point  ),( dyxQ +  in  2E  .    d  is the 
disparity and angular disparity 
W
dπθϖ 2= , where W  is the number of image column of 
the symmetric panorama in pixel. Figure 4 illustrates these concepts. 
10 
 
We can see that decreasing (angular) disparity will reduce the number of depth layers. 
This may result in poor stereoacuity.  
 
2.4 The maximum disparity limit for human stereo fusion 
The maximum disparity for human stereo fusion is approximately equal to:  
×03.0 frontal viewing distance  
Stereo viewing requires that disparities for a RoI stay below the maximum disparity limit. 
If the disparities exceed the limit, this will cause double images, called dipopia , which 
results in uncomfortable stereo viewing as well as eyestrain [2]. 
1D
R
O
A B
A B
ωω−
ωθ
d
max
ωθ
°A
°B
°A °B
maxd
°ω°−ω
 
Figure 5. Maximum angular disparity and disparity limit for human stereo fusion 
12 
 
2.6 Spatial sampling  
Spatial sampling describes how a 3D space is sampled without considering underlining 
structures or the complexity of 3D scenes [3].   
 
Figure 6 shows the sampling structure in the outward case. (A) shows the top view of 
the camera model. (B) and (C) illustrate right and left stereo panoramas in which 
projection rays emitting panoramically from projection centers on the base circles. (D) is 
the result of superimposing (B) and (C). It reveals the basic structure of stereo samples. 
These samples are the concentric circles formed by the intersections of the projection 
rays [3].    
 
Figure 6. Sampling structure in the outward case (The figures are adopted from [3]) 
14 
 
 
 
where   
For proper stereo viewing, the angular disparity  ωθ must below the upper angular 
disparity limit [2]. For example, the indoor scene in Winter Garden has the following 
parameters: 
• radius of the inner cylinder  mD 11 =  
• mH 2.11 =  
• upper disparity limit  ωd = 123 pixels (as calculated in section 2.4) 
Based on the following formulas, we can calculate the upper angular disparity limit ωθ : 
1
12
H
DfW τ
π= =16223 pixels 
SWH
Hdω
ωθ 360= =18.42 degree 
Therefore, for achieving maximum spatial samples in this particular RoI 
• off‐axis distance  R should be 33 cm 
• principal angle ω should be 133.88° (inward case) 
16 
 
3. Drag the image window to the right until it is invisible on the PC screen.  By 
blocking one of the projectors, you will see the once disappeared image window 
is actually projected on the metallic screen. Keep blocking the projector and 
maximize the image window. 
4. Repeat steps 1‐2 to display the left panorama in a new IrfanView window. 
Maximize the window. By blocking one of the projectors, you can display either 
left or right panorama image on the metallic screen.  
5. Calculate disparity: 
• Given R ,  1D , 2D and  1H , we can calculate angular disparity  ωθ based on 
the formula in section 2.7. 
• Based on the formula 
H
WHd s
360
ω
ω
θ= , we can calculate disparity  ωd   
6. Find corresponding points in the left and right panoramas by moving the 
horizontal bars of the IrfanView windows. Make sure the distance (disparity) 
between two corresponding points (on the metallic screen) is the calculated 
result in step 5.  
 
4.0 Computational experiment 
I utilized the sample data provided in [2] for computational experiment purpose. 
 
The data represent the following four typical situations: 
18 
 
 
Figure 7 Situation 2 
 
 
Figure 7 Situation 3 
20 
 
calculation of upper angular disparity limit, please see section 2.7. Figure 8 illustrates 
the relationship between off‐axis distance, principal angle and angular disparity with 
this particular RoI.  
 
 
Figure 8 
 
From the calculation in section 2.7, we know that at 33 cm (off‐axis distance) we can 
achieve the maximum angular disparity 18.42°.  Therefore, our tripod with a slider of 
43.5 cm is capable of completing this hands‐on experiment. The following panoramic 
images are produced with different off‐axis distance and principal angles. 
  
 
Off‐axis distance: 22 cm; principal angle: +159°; angular disparity: 6° 
