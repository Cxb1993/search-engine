  
!
目錄 
遠儲拿?....................................................................................................................................................................... I 
Abstract ................................................................................................................................................................ II 
則?彼?...................................................................................................................................................................... 1 
科?彼1字?則?.............................................................................................................................................................. 2 
字傳權?啟?訊?.............................................................................................................................................................. 3 
遠?synchronization granularity 整?整?科?權?則?科?彼1 ....................................................................................... 3 
遠?少?拿?則字遠?都?字遠則?科?權?則?科?彼1.................................................................................................................. 3 
遠?lock 儲確彼?彼?字?確?李字科?權?則?科?彼1 ............................................................................................................. 3 
科?彼1字遠則?.............................................................................................................................................................. 5 
啟?彼?hybrid consistency management scheme ............................................................................................ 5 
測?彼?adaptive synchronization granularity 少?整?整? ..................................................................................... 5 
啟?字?implicit lock ........................................................................................................................................ 6 
傳?則?遠?訊?整?.......................................................................................................................................................... 7 
啟?李4字傳權?.............................................................................................................................................................. 8 
 
 II
 
Abstract 
 
In the scenario of emergency rescue operations, information sharing is the key to success or failure of the 
entire operation. However, efficient information sharing is difficult to achieve in such a scenario because there 
is no communication infrastructure at the disaster sites. Generally Speaking, the network condition is 
relatively reliable in the intra-site environment but relatively unreliable in the inter-site environment. The 
network partitioning problem may occur between two sites. Although we can exploits the replication 
technique used in data grid to improving the information availability in emergency rescue applications, the 
data consistency problem occurs between replicas. In this context, this paper proposes a middleware called 
"Seagull" to transparently manage the data consistency issues of emergency rescue applications. Seagull 
adopts the optimistic replication technique to provide the higher data availability in the inter-site environment; 
and adopts the pessimistic replication technique to provide the stronger data consistency guarantee in the 
intra-site environment. Moreover, it adopts an adaptive consistency granularity strategy that achieves the 
better performance of the consistency management because this strategy provides the higher parallelism when 
the false sharing happens. Lastly, Seagull adopts the transparency data consistency management scheme, and 
thus the users do not need to modify their source codes to run on the Seagull. 
 
Keywords: hybrid replication; optimistic replication; pessimistic replication; emergency and rescue; adaptive 
consistency granularity
 2
 
科?彼1字?則? 
少?訊?則?data grid 都?科?啟?字?都a字?點?訊?啟?字?(replication)則?字遠李字：?彼?測?啟?對?訊遠則?字?啟?則字(availability)： 李證
少?點?訊?則?啟?字?科?：?點科字?傳2李字字?字?字?字?字?遠?彼?啟?則都確 拿?則?對?訊遠：?字?李?署?字?彼?遠?遠?訊?彼?李?署?彼?啟?對?訊遠彼?啟?
李?則?字?：?字?字?字?遠?則?李?彼?少?署?則?啟?字?則?字?都?少?字?訊?測?少?點科字?傳2李字傳?科?對?訊遠則?訊儲傳?：?點?訊?拿?李?遠?對?少?則?
科?傳?：?啟?李?少?字e：?署?科?拿?科l李證訊?彼?對?訊遠則?署?點?署?儲確則科則?證?傳?grid 則?點?對?少?科?：?都?字?對?訊遠字?少?署?字?李?訊?
字g：?李?訊?啟?啟?彼?字3署?點?少?彼?啟?：? 
字?則?李證權?點?訊?啟?字?則?科?彼1署?少?：?少?都?字?都?李?則?署?少?則?字?彼?彼?少?觀?則?點?訊?權)科?(permanent data)：?都?
彼?科?彼1整?字?點科字?字?科?整?少?write-once read-many 則?對?訊遠：?李?都?彼?對?訊遠都?啟?都?科?字?少?彼?署?點科整?則科啟?測?整?彼?
都a字?：?字?字?都?字?傳2李字則科彼?字?則?字?少?則?：?彼?字?：?彼?對?少?：?字?啟?對?啟?觀對則字則?對?訊遠李?都?拿?則?data grid 彼2都 觀
啟?少?對?則?訊?對?：?傳字則?點科字0確 拿?字測測?字?觀?對?訊遠(mutable data)啟?字 遠 都 則 字?李字點科字?：?傳李李?：?署?字測測?mutable 
replica 訊儲：?replicas 少?傳?則?consistency 啟a證?測h觀?則?科?署?拿?拿?：?署?字?拿?grid users 對?少?拿?點?訊?則?少?字?字?
replicas 字?訊儲啟9確i少?則?啟?李a：?李?訊儲測h字?訊?署?都a字?conflict 則?啟?則h： 傳?李?都?李啟點?訊?則?少?拿?李證少?少?拿?則?啟?則h
傳?字?：?對?則?字?少?傳?則?對?訊遠啟?字?少?：?遠?科l對?訊遠則?少?拿?則字(data consistency)啟 署?科?拿?科?少?則?啟a證?：?署?確 拿?字0
字?啟?則2則?字?確?[1-24]：? 
字?李?：?字?科?彼1字?拿?則?字?確?：?科?字?字?字?少?拿?字?訊?證對整點則?遠?對?點?對?少?：?測?彼?少?拿?data management system
字?少?都?字?拿?彼?字?則?字?訊儲對?則?mutable replicas 傳?李?字?彼?：?彼?遠?權?對?訊遠少?拿?則字：?字?字e：?字?科?彼1字?拿?李4傳?則?
點科字?測?測?科?科?權?啟?測?李?啟?少?則?對?訊遠字?彼e：?字?科?權?啟?測?則?李?啟?署?少?：?科?李?訊?李證訊署都?字?字?測?啟?權?科?權?對?訊遠：?
啟?署?確?權?彼?整?拿?啟?測?李?啟?則?李啟啟?：?傳李李?字?科?權?啟?測?點?對?少?：?遠?都?對?訊遠則?字?彼e科?科?署?李?權?則?少?李a：?字?拿?
科?字?科?字?啟?彼?都?測?字?訊?彼確李證都?訊?都?測?：?李?拿?則?科?彼1李啟則?啟?訊?啟?對?李?少?都?傳?測?彼?少?拿?則?彼：則?對?彼都少?對?：?
 4
NFSv4 測?彼?少?implicit locking 儲確彼?：?NFSv4 servers 署?李?啟?對?點?訊?啟9locking 遠?都?啟?李a：?字?李?彼?字?則?
科?少?字?少?少?lock 則?遠?都?：?字?李?字?字?字遠則?字?彼?字?：?科?對?字?：?TLDFS 科?測?彼?explicit locking 儲確彼?：?測?彼?少?：?
TLDFS 拿?彼少彼?字?則?李?李?啟9lock 少 遠 都?：?字?李?字?訊?署?都?李啟彼?字?則?則?李?證啟：?訊?觀(彼2都?則?字?確?測?彼?少?implicit 
locking 儲確彼?：?觀?彼?字?則?傳字傳?署?測?lock 則?遠?都?少?李a：?字?彼2都?彼?拿?都?：?彼?對?彼?少?拿?則字：?測?遠?彼?字?則?則?拿?整?：? 
 6
署?字?訊儲李證字?拿?彼?字?則?確i少?對?訊遠彼?點?訊?則?少?字?字?科?(fragments)訊儲：?訊?觀(彼2都?署?啟?點?訊?啟?遠訊少?測)李啟字?拿?
fragments：?彼?啟?對?彼▇少?拿?fragment：?少?李?啟9對?訊遠少?拿?則字則?遠?都?：?字?李?則?字?字?少?都?字?拿?彼?字?則?對?字?少?字?
點?訊?傳?李?concurrent write：?傳?李?測?少?system performance：? 
啟?字?implicit lock 
字?則?訊?觀(彼2都?字?intra-site 科 啟?字?pessimistic replication scheme：?點?傳李字?傳?彼?字?write lock 彼?read lock
彼?則?證?啟?字?則?少?拿?則字：?傳李李?訊?觀(彼2都?證?字?啟?彼?implicit lock 則?字遠李字點李李?彼?字?則?李?啟?都?都?啟?字?字?彼?李?科?則?
字?彼：▇對確?則?點?訊?字?訊儲字?訊?都?少?拿?彼?字?則?▇遠：?字?李?：?intra-site 則?彼?字?則?則科點科 傳2李字少?傳?字?傳?權?字科彼?李?少?傳?
則?字?彼：啟a證?：?彼2都?李?傳李署?字?點科字?傳2李字則科彼?字?則?傳?字?字?彼?拿?彼少訊儲：?李?啟?啟?啟?字?拿?字?read/write lock：?都 確 ：
字?字?遠?傳?少?字?李?觀?點科字?傳2李字則科彼?字?則?李?科?：?測?李?legacy application 對?李?字?訊?觀(彼2都 少?則都署?對?彼?則?則?證?：? 
 
對?少?：?訊?觀(彼2都?少?彼2都?對?都?科?遠? 
 8
 
啟?李4字傳權? 
[1] T. Plagemann, E. Munthe-Kaas, and V. Goebel, “Reconsidering consistency management in shared 
data spaces for emergency and rescue applications,” in Model Management und 
Metadaten-Verwaltung. 
[2] S. Tikar, and S. Vadhiyar, “Efficient reuse of replicated parallel data segments in computational grids,” 
Future Generation Computer Systems, vol. 24, no. 7, pp. 644-657  
[3] L. Lamport, “Time, clocks, and the ordering of events in a distributed system,” Communications of the 
ACM, vol. 21, no. 7, pp. 558-565, 1978. 
[4] B. Charron-Bost, “Concerning the size of logical clocks in distributed systems,” Information 
Processing Letters, vol. 39, no. 1, pp. 16, 1991. 
[5] D. L. Mills, “Improved algorithms for synchronizing computer network clocks,” IEEE/ACM 
Transactions on Networking (TON), vol. 3, no. 3, pp. 245-254, 1995. 
[6] L. Guy, P. Kunszt, E. Laure et al., “Replica management in data grids,” in CERN, European 
Organization for Nuclear Research, Switzerland, 2002, pp. 278-280. 
[7] M. M. Deris, J. H. Abawajy, and H. M. Suzuri, “An efficient replicated data access approach for 
large-scale distributed systems,” in Proceedings of the 2004 IEEE International Symposium on Cluster 
Computing and the Grid, 2004, pp. 588 - 594  
[8] A. Domenici, F. Donno, G. Pucciani et al., “Replica consistency in a Data Grid,” Nuclear Instruments 
and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated 
Equipment, vol. 534, no. 1-2, pp. 24-28, 2004. 
[9] Y. Sun, and Z. Xu, “Grid replication coherence protocol,” in 18th International Parallel and Distributed 
Processing Symposium (IPDPS'04), Santa Fe, New Mexico, 2004, pp. 232. 
[10] D. Hildebrand, and P. Honeyman, “Scaling NFSv4 with parallel file systems,” in Proceedings of the 
Fifth IEEE International Symposium on Cluster Computing and the Grid (CCGrid'05), Cardiff, Wales, 
UK 2005, pp. 1039 - 1046   
[11] Y. Saito, and M. Shapiro, “Optimistic replication,” ACM Computing Surveys, vol. 37, no. 1, pp. 42-81, 
2005. 
[12] R.-S. Chang, and J.-S. Chang, “Adaptable Replica Consistency Service for Data Grids,” in 
Proceedings of the Third International Conference on Information Technology: New Generations, Las 
Vegas, Nevada, 2006. 
[13] A. Domenici, F. Donno, G. Pucciani et al., “Relaxed Data Consistency with CONStanza,” in 
Proceedings of the Sixth IEEE International Symposium on Cluster Computing and the Grid, 
Singapore, 2006. 
[14] L. Lamport, “Lower bounds for asynchronous consensus,” Distributed Computing, vol. 19, no. 2, pp. 
104-125, 2006. 
[15] J. Zhang, and P. Honeyman, “NFSv4 replication for grid storage middleware,” in Proceedings of the 
4th international workshop on Middleware for grid computing, Melbourne, Australia, 2006. 
[16] J. Zhang, and P. Honeyman, Naming, Migration, and Replication for NFSv4, Center for Information 
Technology Integration, University of Michigan, 535 West William Street, Ann Arbor, MI 48103-4978, 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 98-2221-E-426-004 
計畫名稱 急難救援格網：一個支援離線式存取與複本一致性的資料格網系統之研製 
出國人員姓名 
服務機關及職稱 
張志標 
立德大學資訊科學與應用學系助理教授 
會議時間地點 September 1-3, 2010, Melbourne, Australia 
會議名稱 12
th
 IEEE International Conference on High Performance Computing and 
Communications (HPCC2010) 
發表論文題目 An Adaptive Remote Paging System on Computational Grids 
 
 
一、參加會議經過 
 
本人所參加的國際會議是在 2010 年 9 月 1~3 日於澳洲墨爾本大學舉行的 12th IEEE 
International Conference on High Performance Computing and Communications (HPCC2010)。多
年來，HPCC 替產、官、學等三方的研究者與工程師們，提供了一個可以彼此交換與討論新
創意、研究成果與經驗的平台。歷屆會議以來，不斷地在高效能運算與通訊方面，以及分散
式系統研究領域中，與會者多有發表令人讚賞的論文與研發成就，不啻是個歷史悠久並且舉
足輕重的國際會議。會議論文之投稿人數與品質皆有相當高的水準，今年的論文投稿文張數
多達三百餘篇，接受率卻僅為 20%左右。本研究團隊之成果能在 HPCC2010上發表，實在是
一種殊榮與對本計畫研究成果之肯定。本會議也邀請了多位學術界與業界之知名人士，於會
議中進行 keynote addresses，使會議增添不少光彩與內容。演講之內容也針對近來日漸受到重
視之雲端計算等議題，多有著墨。藉著演講者豐富之學識與經驗，讓與會者獲益良多。尤其
是針對未來研究趨勢之剖析與經驗分享，更讓本人有如醍醐灌頂之感受，也對往後研究之課
題深具信心。 
 
 
 
 
 
 
 
     
會議實況 
An Adaptive Remote Paging System on Computational Grids 
 
Tyng-Yeu Liang   Po-Jen Lo   Min-Jyun Chen 
Ti-Hsin Wang 
Department of Electrical Engineering 
National Kaohsiung University of Applied Sciences 
Kaohsiung, Taiwan 
{lty,jen,hedy,daisy}@hpds.ee.kuas.edu.tw 
  Jyh-Biau Chang  
Department of Digital Applications 
Leader University 
Tainan, Taiwan 
andrew@mail.leader.edu.tw
 
 
Abstract—This paper is aimed at developing a novel remote-
paging system called Adaptive Grid Remote Pager (AGRP) on 
computational grids. Different to related work, this system is 
able to promise the effectiveness of remote paging. To achieve 
this goal, the resource broker of AGRP allocates only the grid 
nodes which are helpful for the performance of user programs 
to play the memory servers of remote paging. On the other 
hand, the memory servers automatically adapt their memory 
utilization through page migration to avoid the negative impact 
of memory competition from the local jobs at their execution 
nodes. Moreover, the memory clients always select the best one 
from their corresponding memory servers for each page-out 
request to minimize the latency of remote paging. We have 
implemented the proposed system in this paper. Our 
experimental results show that the proposed system indeed can 
dramatically improve the execution performance of the test 
program due to a significant reduction of the remote-paging 
latency and the avoidance in memory competition. 
 
Keywords-adaptive remote paging, computational grids, 
network latency, memory competition, page migration 
I.  INTRODUCTION 
Remote paging [1] is a software technique which 
aggregates free memory resources on computer networks to 
form a swapping space of virtual memory. With this 
swapping space, the virtual memory manager can swap 
memory pages between local memory and remote memory 
instead of between local memory and local disks. Because 
the latency of accessing remote memory via high speed 
network is smaller than that of accessing local hard disks, 
the execution performance of user programs is able to be 
effectively improved due to the cost reduction in page 
swapping especially when the problem size of the programs 
is much larger than the amount of available physical 
memory. Many past studies have put effort into the design 
and implementation of the remote paging system, and have 
proved the effectiveness of remote paging in computational 
clusters. 
Recently, Computational grids [2] have become an 
alternative solution for data-intensive problems. With the 
grid infrastructure, users can easily collect heterogeneous 
resources such as CPUs, memory, disks and databases 
which are geographically distributed on computer networks 
to emulate a single system image for resolving their 
problems without the awareness of resource location. Since 
computational grids can provide users with much more 
resource capability and transparency in resource allocation 
than computational clusters, some researchers recently 
started to study how to effectively exploit memory resources 
available in computational grids for remote paging. For 
example, Surendra Byna [3] proposed a service-oriented 
architecture to improve the scalability of memory sharing in 
grids. Rui Chu [4] developed a peer-to-peer based RAM 
grid to achieve good scalability and reliability in remote 
paging. Po-Sen Wang [5] developed a grid remote pager 
which can dynamically adapt the maximal amount of 
physical memory allowed for user programs to avoid 
memory competition and minimize the number of page 
swapping. However, several important issues specific for 
remote paging in grids did not receive attention from these 
works. 
For example, the capability of grid resources is 
heterogeneous and dynamic. In other words, the network 
latency of each client-server pair in grids usually is different 
and dynamically altered. As a result, remote paging in grids 
is not necessarily helpful for improving the execution 
performance of user programs unless the cost of remote 
paging is less than that of swapping pages into local disks. 
Therefore, it is essential to allocate the memory servers of 
remote paging onto proper grid nodes for execution. For the 
same reason, the memory client also has to carefully select 
the memory server for page-out requests when it is served 
by multiple memory servers at the same time. 
On the other hand, the remote jobs coming from grid 
users and the local jobs are allowed to be executed 
simultaneously by the same node. Memory competition will 
occur when the grid node is not able to simultaneously 
provide their local jobs and the memory servers with enough 
memory resources. Consequently, both of the performance 
of the local jobs and the memory servers will be degraded 
due to memory competition. In reaction to this problem, the 
server nodes in the RAM grid will withdraw their services 
by transmitting all of the memory pages to other server 
nodes when their memory loads are over a threshold. 
However, the immediate reaction of moving out all of the 
memory pages from the server nodes may be too hasty in 
out requests from the memory client. The process of the 
remote paging service in AGRP is briefly described as 
follows. 
 When a user program is initialized, the memory client 
will ask the resource broker for the allocation of the memory 
server. The resource broker will select a grid node that can 
serve as a memory server according to the resource 
information. The location of the selected grid node will be 
feedback to the memory client. After building up the 
connection with the memory server, the memory client will 
swap out a number of victim data pages to the memory 
server when it finds the amount of free physical memory at 
the execution node of the user program is below a certain 
threshold value. Conversely, it will swap in data pages from 
the memory server if these data pages should be accessed by 
the user program but did not present at the local memory. 
However, this page-in operation will result in another page-
out operation if the memory space allowed for the memory 
client is drained. It is worthy to say that when a memory 
client is simultaneously served by multiple memory servers, 
the server with the network latency and free memory space 
that can minimize the latency of remote paging and achieve 
load balance will be selected for paging out. 
On the other hand, the memory server receives/sends data 
pages from/to the memory client whenever it receives a 
page-in or page-out request. During the process of providing 
the remote-paging service, the memory server cyclically 
adapts its own memory utilization. When the memory server 
finds that the amount of memory pages it used is higher than 
the memory-utilization limit, it will move portions of data 
pages to other servers, and then notify the memory client of 
the new location of the migrated data pages for later page-in 
requests. In contrast, the memory server will continue to 
consume more physical memory for the data pages sent from 
the memory client or other memory servers if its memory 
utilization is lower than the memory-utilization limit. 
Besides, the memory server will withdraw its services, and 
move all of the data pages coming from the memory client to 
other servers when it finds that it becomes not helpful for the 
performance of user programs due to high network latency. 
 It is notable that the memory server will select another 
node from the memory server pool of the same client for 
page migration. In order to achieve the lowest latency of 
remote paging and load balance, the selection policy taken 
by the memory servers is the same as the one taken by the 
memory client for page-out requests. However, if all of the 
other memory servers run out of their allowed memory space, 
the root of these memory servers will ask the resource broker 
to deploy a new memory server for sharing their memory 
load. 
The dynamic memory allocation and release and the page 
fault handle in AGRP have been clearly described in the 
paper [5], the rest of this section will be focused on 
describing the allocation policy and the selection policy of 
AGRP, as well as the adaptation in the memory utilization of 
the memory servers. 
A. Allocation Policy 
The cost of remote paging must be smaller than that of 
local paging, i.e., the cost of swapping pages into local disks 
in order for maintaining the effectiveness of remote paging. 
To achieve this goal, a grid node can play the memory server 
as long as this node satisfies the qualification condition is 
stated as follows. 
                      sonldatclient +> *2                                           (1) 
In the above formula, datclient is the access time of the 
local disk in the memory client, nl is the network latency of 
page transfer between the memory server and the memory 
client, and so is the software overhead of the page in/out 
process. Since the cost of paging into disks consists of not 
only the access time of the local disk, the condition adopted 
by the proposed system is stricter than the one necessary for 
maintaining the effectiveness of remote paging. That is, the 
effectiveness of remote paging can be promised if the 
network latency of data transfer between the memory server 
and the memory client commits the condition stated below, 
                     2/)( sodatnl client −<                                          (2) 
Accordingly, the resource broker exams the qualification 
of available grid nodes by the above formula to decide if 
they can be a candidate of memory server or not when the 
request of server allocation arrives. After the qualification 
test, the resource broker will select the candidate node with 
the least network latency of page transfer as the memory 
server in order to maximize the performance benefit from 
remote paging. However, if there are multiple candidates 
with the smallest network latency of page transfer, the grid 
node with the largest free memory space will be allocated as 
the memory server for the sake of load balance. 
B. Selection  Policy 
When a memory client is simultaneously served by 
multiple memory sever, it should select one from these 
memory servers for each page-out request to minimize the 
latency of remote paging as previously discussed. In AGRP, 
the selection policy is to choose the memory server with the 
least network latency of data transfer as the destination node 
of the swapped-out pages. However, if multiple memory 
servers match the condition of the least network latency, the 
one with the most free memory space will be selected as the 
destination node. Any memory server will also use the same 
policy to select one from the other memory servers which are 
mapped to the same memory client as the receiver node 
whenever it intends to move out data pages to another server. 
C. Memory-utilization Adaptation 
In order to prevent memory competition, AGRP uses 
memory-utilization limit ( lM ) to constrain the amount of 
physical memory that can be used by the memory server. 
program can obtain enough memory resource for its work, 
and its performance can be significantly improved. On the 
other hand, the performance of the user program can also be 
improved after the memory server adapts its memory 
utilization because the latency of remote paging is reduced. 
 
 
Figure 2.  Impact of memory utilization adaptation 
C. Comparison of different page-migration policy 
In this experiment, we ran a test program which 
repeatedly acquires and releases a big chunk of memory 
about 300MB every 30 seconds on two different server 
computers in order to cyclically create the necessity of page 
migration. The parameter value of maxM is set as 600MB, and 
the network latency of remote paging is controlled to be 0.1 
ms. Basically speaking, the memory servers can exploit 
about 300 MB memory although the owner program 
cyclically occupies 300MB memory at the same machine for 
a short period of time. It is not necessary for the memory 
server to immediately move all of the data pages of the user 
program when the amount of memory used by the memory 
server is over the memory-utilization limit. Therefore, the 
immediately global policy is not proper when the memory 
load of the server computers remains heavy for a short period 
of time. The experimental result in Table 1 shows that when 
Server 1 does not adapt its memory utilization by page 
migration, the performance of the user program is seriously 
degraded due to memory competition. After page migration 
is executed, there is a significant improvement in the 
performance of the user program. Furthermore, the gradually 
partial policy is better than the immediately global policy for 
improving the effectiveness of remote paging. 
TABLE 1. IMPACT OF DIFFERENT PAGE-MIGRATION POLICIES 
Migration policy of 
Server 1 
no 
migration partial global 
Exe. time of SOR (sec) 21266.6 2568.2 4571.1 
 
The reason for this result is that the immediately global 
policy not only causes page thrashing among different 
memory servers but also increases the latency of remote 
paging since the memory servers are usually busy in page 
migration but handling the paging requests from the memory 
client as shown in Figure 3. In contrast, the situation of page 
thrashing disappears when the gradually partial policy is 
applied as shown in Figure 4. Consequently, the performance 
of the user program is further improved by remote paging. 
 
 
 
Figure 3.  Memory usages of servers under the global policy 
 
 
Figure 4.  Memory usages of servers under the partial policy 
0 
5000
10000
15000
20000
Ex
ec
ut
io
n 
Ti
m
e 
( s
ec
 ) 
no-adaptation 16859.600 16694.897 
adapation 441.809 2679.777 
owner program user program
pages are not distributed according to the order of network 
latency, neither remote paging nor memory competition is 
effectively resolved. 
 
 
 
Figure 7.  Memory usages of servers under applying  the proposed 
selection policy in the case of dynamic network latency 
In addition to the case of static network latency, the 
impact of server selection has been evaluated for the case 
which the network latencies of the memory servers are 
varied during the execution of the user program. To achieve 
this goal, we used two memory servers whose network 
latencies are initialized to be 1ms and 2ms while exchanged 
their network latencies to be 2ms and 1ms after the user 
program has been executed for 1000 seconds. Figure 7 is the 
memory utilization of Server 1 and Server 2 when the 
proposed server-selection policy is applied in remote paging. 
Since the value of maxM is 400MB, Server 1 uses 400MB 
memory and Server 2 uses 200MB for storing the data pages 
coming from the memory client before the network latencies 
of the memory servers are changed. However, after the 
network latencies of Server 1 and Server 2 are exchanged, 
the amount of memory exploited by Server 1 and Sever 2 
becomes 200MB and 400MB, respectively. The 
experimental result shows that AGRP indeed can effectively 
maintain the performance of remote paging by adapting itself 
to dynamic changes in the network latencies of the memory 
servers. 
In contrast, Server 1 and Server 2 use the same amount of 
memory spaces to store the user data pages when the 
network latency of memory servers is not considered for 
page-out requests as shown in Figure 8. Although the 
memory load between Server 1 and Server 2 is balanced, the 
performance of the user program is degraded because of the 
higher cost of remote paging as shown in Table 3. 
 
 
 
Figure 8.  Memory usages of servers under applying the random selection 
policy in the case of dynamic network latency 
TABLE 3. IMPACT OF DIFFERENT SELECTION POLICIES UNDER DYNAMIC 
NETWORK LATENCY 
Program exe. time (sec) User 
proposed selection policy 4670.1 
random selection policy 4889.7 
 
V. CONCLUSIONS AND FUTURE WORK 
We have successfully developed an adaptive remote 
paging system on computational grids in this paper. The 
proposed system not only can properly allocate grid 
resources for the execution of the memory servers to promise 
the effectiveness of remote paging but also can effectively 
adapts itself to minimize the negative impact of the memory 
competition from the local jobs of grid resources and the 
dynamic change in the network latency of the memory 
servers. Because of these advantages, the proposed system 
can effectively exploit the memory resources available in 
grids to improve the execution performance of data-intensive 
programs. On the other hand, we have also completed a 
series of experiments for evaluating the performance of the 
proposed system. Several conclusions are obtained as 
follows. First of all, careful allocation and selection of 
memory servers is essential to keep the effectiveness of 
remote paging on grids. The second conclusion is that a 
gradually partial policy of page migration is better than an 
immediately global policy for minimizing the frequency of 
page migration and maximizing the utilization of available 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/01/31
國科會補助計畫
計畫名稱: 急難救援格網：一個支援離線式存取與複本一致性的資料格網系統之研製
計畫主持人: 張志標
計畫編號: 98-2221-E-426-004- 學門領域: 平行與分散處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
