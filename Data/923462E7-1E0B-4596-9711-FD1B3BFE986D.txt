Keywords: genetic algorithms, genetic operations, Markov chains, clustering, Davies-Bouldin index. 
二、報告內容與參考文獻 
本次的專題結果已投稿並獲接受於 the 18th International Conference on 
Pattern Recognition (ICPR2006)， 此學術會議於 August， 20-24， 2006 在香
港舉行。 所發表的文章（第 3 頁）附錄於本頁之後。 
 
 
三、計畫成果自評 
 
本計畫執行成果所提之新的以基因演算法(Genetic Algorithm, GA)為基礎之分群(clustering)
技術，其不需執行 GA 運算而是藉由分析族群馬可夫鏈(population Markov chains) 以及一些基
因演算法操作運算的修改，此提出的技術效能遠遠超越現存的其它基因演算法分群(GA 
clustering)方法，執行成果顯示與當初所提之原計畫內容大致相符，也達到預期目標。本計畫的
執行不僅得以發表一篇於 ICPR06 conference 之論文， 還有一位碩士班學生利用此分群技術作
為其研究所需之工具。希望後續能以這次研究的成效與經驗，應用於更廣泛的領域。 
 
 
 
 
 
 
 
 
 
 
2. PRELIMINARY 
 
Genetic algorithms are search and optimization 
algorithms based on the principles of natural 
evolution. They have been frequently used in 
unsupervised clustering. In many theoretical studies 
of GAs analyses [11-13], the population Markov 
chain models have been adopted. Yong Gao et al. 
[13] proposed a novel genetic algorithm (called 
GANGO2) which needs neither to maintain a 
population nor to use the conventional genetic 
operators, and yet has the same search mechanisms 
as the classical GAs. They can be implemented by 
directly sampling the transition probability 
distributions instead of by applying the 
conventional genetic operators to evolve the 
populations. The theoretical analyses and their 
proposed theorem are introduced in this Section.  
 
Definition: Given a population X = (X’1, …, X’P), 
X’i = (xi1, …, xil), i = 1,…, P, for any positive 
integer 1≤ j ≤ l, let Ij0 and Ij1 denote the sets of 
indices of all the chromosomes of the population X 
that have respectively a zero or one at the j-th gene 
position, that is,  
 
 
 
 
 
Theorem: Consider the GA population Markov 
chain {X(k), generation k≥0}. Given X(k) = X, the 
conditional distribution of the j-th component 
xij(k+1) of individual X’i(k+1) is a zero-one 
distribution with the parameter uniquely determined 
by the characteristic of X and the mutation 
probability pm as 
 
pj(k+1,0)=P{xij(k+1)=0|X(k)=X}=aj+(1-2aj)pm  (2) 
pj(k+1,1)=P{xij(k+1)=1|X(k)=X}=bj+(1-2bj)pm  (3) 
 
Although the over-all performance of our previously 
proposed clustering algorithm, called PMCC, based 
on GANGO2 is fine, it still has some problems: (1) 
Although the fitter chromosome can immediately 
contribute to the creation of the other chromosomes 
of the later population, the initial population 
sometimes tends to influence the outcome during 
the entire evolution process. (2) The values of 
F(X(k+1)) and Fj1(X(k+1)) tend to unrestrictedly 
expand, and the effects will decay in the later and 
fitter chromosomes. (3) The average threshold, 
t(k+1), is a cumulative sum of the fitness values 
from duplicate individuals, so the use of this 
threshold tends to prematurely converge, especially 
when the dataset has more than 7 clusters. This 
motivates us to modify the PMCC algorithm to 
obtain an improved version, called WPMCC 
(Winner-Population Markov Chain Clustering) and 
described in the next section. 
 
3. THE PROPOSED CLUSTERING ALGORITHM 
 
This section describes in more depth how the 
proposed method is implemented.  
3.1. Binary Representation 
The cluster centers are selected from the data set. 
The chromosome length is equal to the size of the 
data set. The j-th gene of a chromosome 
corresponds to the j-th data point in the data set. If 
the j-th data point is selected to be a cluster center, 
the allele of the j-th gene in the chromosome is set 
to “1”; otherwise “0”. The number of clusters, 
denoted by K, is assumed to lie in the range [Kmin, 
Kmax], where Kmin is set to 2, and Kmax is commonly 
set to NN or 2/ , where N is the chromosome length 
(or the size of the input data), unless otherwise 
specified.  
3.2. Population Initialization 
Let P be the population size. First, an integer Kr for 
the r-th chromosome, r = 1, 2, …, P, is randomly 
selected from the range [Kmin, Kmax], and then Kr 
distinct data points are randomly chosen from the 
data set, the allele of the gene corresponding to the 
index of each of the chosen data points is set to “1”; 
while that of each of the remaining genes is set to 
“0”. For example, if N = 16, Kr = 3 for the r-th 
chromosome, and 3 data points randomly chosen 
from the data set have indices 3, 10, and 12, 
respectively, then the chromosome should be 0010 
0000 0101 0000. 
3.3. Fitness Function Evaluation 
},1 ,1{ I   },1 ,0{ I j1
j
0 PixPix ijij ≤≤==≤≤==
(1)1
)(
)(F       ,   
)(
)(F 
     , )X'()(F   , )X'()(F  , )X'()(
                                                                                                  
j
1
j
0
j
1
j
0
I
j
1
I
j
0
1
jjj a
XF
Xb
XF
Xa
fXfXfXF
i
i
i
i
N
i
i
−===
=== ∑∑∑
∈∈=
)/(75.1 lP ×≈
artificial and random data sets with a variety of 
numbers (in [Kmin, Kmax] = [2, 11]) of clusters were 
tested to evaluate the performance of the proposed 
method. These data sets are publicly available on 
the Website: http://pria.cs.tku.edu.tw. In our 
experiments, pm is automatically estimated by the 
equation pm           , pc = 0.9 as required in 
[16], P = C = 100, G = 100, and [Kmin, Kmax] = 
].2[ N,  Finally, the DB index was adopted to 
measure the validity of the clusters. For comparison, 
we performed both our methods and the GCUK 
method 10 runs on each data set. Figure 1 shows the 
average maximum fitness values resulting from 
these methods, having been tested 10 runs for each 
data set, respectively. It demonstrates that on the 
average the WPMCC algorithm indeed provides 
better fitness values than any of the other methods, 
especially when the dataset has more than 5 clusters. 
Figure 2 shows the average processing time per data 
point required by each method tested 10 runs for 
each data set, and demonstrates that the WPMCC 
algorithm is about 3 to 7 times faster than the 
GCUK-clustering method and a little bit faster than 
the PMCC method. Our experiments also show that 
the WPMCC algorithm converges before the 15th 
generation and has greater maximum fitness values 
than any of the others.  
 
5. CONCLUSIONS 
 
This paper modifies the previously proposed 
unsupervised clustering PMCC algorithm, to 
achieve an improved version: the WPMCC 
algorithm, which not only improves the premature 
convergence problem so as to provide a more stable 
clustering performance, but also improves the time 
efficiency. Using the Euclidean distance as the 
dissimilarity metric yields circular clusters. Such 
clusters for some of the test data may not as natural 
as those provided by people. In the future, we will 
test the other distance metric such as Mahalanoobis 
distance and point symmetry distance [17] against a 
variety of data sets with various shapes of clusters. 
In addition we are investigating the correlation 
between the convergence speed and the number of 
clusters in the data set and studying on 
similarity/dissimilarity metrics and expect to further 
improve the unsupervised clustering algorithm. 
6. REFERENCES 
 
[1] S. Theodoridis and K. Koutroumbas, Pattern 
Recognition, Academic Press, New York, 1999. 
[2] A. K. Jain, M. N. Murty, and P. J. Flynn, Data 
Clustering: A Review, ACM Computing 
Surveys, Vol. 31, No. 3, 1999, 264-323. 
[3] D. E. Goldberg, Genetic Algorithms in Search, 
Optimization and Machine Learning, 
Addison-Wesley, New York, 1989. 
[4] Z. Michalewicz, Genetic Algorithms + Data 
Structures = Evolution Programs, 
Springer-Verlag, New York, 1992. 
[5] V. V. Raghavan and K. Birchand, A clustering 
strategy based on a formalism of the 
reproductive process in a natural system, 
Proceedings of the 2nd International 
Conference on Information Storage and 
Retrieval, 1979, 10-22. 
[6] D. Jones and M. A. Beltramo, Solving 
partitioning problems with genetic algorithms, 
Proceedings of the 4th International Conference 
on Genetic Algorithms, 1991, 442-449. 
[7] G. P. Babu and M. N. Murty, Clustering with 
evolution strategies, Pattern Recognition, 27, 
1994, 321-329. 
[8] Ujjwal Maulik and Sanghamitra 
Bandyopadhyay, Genetic algorithm-based 
clustering technique, Pattern Recognition, 33, 
2000, 1455-1465. 
[9] Sanghamitra Bandyopadhyay and Ujjwal 
Maulik, Genetic clustering for automatic 
evolution of clusters and application to image 
classification, Pattern Recognition, 35, 2002, 
1197-1208. 
[10] Hwei-Jen Lin, Fu-Wen Yang and Yang-Ta Kao, 
Efficient Clustering based on Population 
Markov Chain, Proceedings of the IASTED 
International Conference on Modeling, 
Simulation and Optimization (ICMSO2004), 
2004, 117-123. 
[11] Y. Gao, Z. B. Xu, and G. Li, Theoretical 
analyses, new algorithms, and some 
applications of genetic algorithms: A review of 
some recent work, in Fuzzy Logic and Soft 
Computing, K. Y. Cai, G. Chen and M. Ying, 
Kluwer Academic, New York, 1999, 121-134. 
[12] Y. Leung, Y. Gao, and Z. B. Xu, Degree of 
population diversity: A perspective on 
