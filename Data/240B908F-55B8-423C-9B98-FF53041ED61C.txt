  1
摘 要 
在可分割負載分佈的方法中，典型的線性網路分佈方法是應用管線化(pipelined)通訊。
在本研究中，我們首先提出一個應用管線化通訊和多重配置(multi-installment)處理的方法，
如此便能減少初始的分佈時間以達到最佳的效能。考慮當起始時間成本(start-up costs)存在
的情況下，因為應用計算和通訊重疊方法的關係，處理器計算完成的時間並不一致，因此
我們提出一個考慮到起始時間來分佈負載的方法。在超立方體拓樸(hypercube topology)中，
典型的方法為每一階層在一次的傳送時間內把所要傳送的負載分配給下一階層，之後只計
算負載不再傳送，如此將會有許多的通訊閒置時間。在本研究中，我們提出二個應用管線
化通訊和多重配置處理的方法來解決通訊閒置時間的問題以達到最佳的效能。最後亦導出
所提出的演算法之平行執行時間函數和增速函數進行分析。 
關鍵詞：可分割負載, 網狀拓樸, 超立方體, 負載分佈, 效能分析, 管線化通訊, 增速. 
 
Abstract 
In the divisible load distribution, the classic method on linear arrays distributes divisible 
load by means of time intervals of variable length for pipelined communication. In this research, 
we first propose a method which employs multi-installment processing with pipelined 
communication to reduce initial distribution time and to achieve better performance. Considering 
situations that the start-up costs exist, the communication and computation cannot be finished at 
the same time. Therefore, we propose an improved method which employs pipelined 
communication to distribute divisible load on linear arrays for such situations. In the hypercube 
topology, the classic method distributes divisible load by means that each layer transmits one load 
fraction to next layer. This classic load distribution method carries out that only one 
communication period on each layer, resulting in too much communication idle-time on each 
layer. In this research project, we propose two algorithms which use pipelined communication 
with multi-installment to solve the communication idle-time problem and to achieve better 
performance. By our algorithms, we observed significant improvement of performance on the 
speedup diagram which shows an exponential curve of performance. The speedup could be 
excellent and very close to system limitation especially, when the computation-communication 
ratio exceeds a certain value. The closed form solutions to the function of parallel execution time 
and speedup based on the proposed methods are also derived. 
 
Keywords: Divisible load, mesh, hypercube, load distribution, performance analysis, pipelined 
  3
1. Introduction 
The divisible load theory (DLT) started with architecture of a linear array of processors 
[7] in 1988. Since then, other interconnection topologies have been investigated for the bus 
[5], [8], [10], linear array [2], tree [4], [6], [9], [11], [12], [13], hypercube [3], and mesh [1], 
[14]. A divisible load is a load that can be arbitrarily partitioned in a linear fashion and can 
be distributed to more than one processor to achieve a faster solution time. This means that 
each partitioned portion of the load can be independently processed on any processor on 
the network. The primary objective in the research of the divisible load theory is to 
determine the optimal fractions of the entire load to be assigned to each of the processors 
such that the total processing time of the entire load is minimal. 
 
Scheduling the loads of a parallel application on the resources of a distributed 
computing platform efficiently is critical for achieving high performance. In the literature 
[1], it proposes an algorithm which employs pipelined communication to distribute 
divisible load on k-dimensional mesh. However, this algorithm only distributes one load 
fraction to the last processor. In other words, the number of chunks depended on the 
number of processors. In the case of linear costs, the literature in [4] mentioned that 
infinitely small chunks would lead to an optimal multi-installment schedule, which implies 
an infinite number of installments. This principle motivates us proposing an improved 
algorithm which employs pipelined communication with multi-installment processing. 
 
The pipelined communications can be used for divisible load distribution on several 
static interconnection networks such as hypercube and tree. In the literature [3], it proposes 
an algorithm which used a manner of single-round to distribute divisible load on 
d-dimensional hypercube. We propose a new algorithm which employs pipelined 
communication to distribute divisible load on d-dimensional hypercube. And we propose a 
new algorithm which employs pipelined communication with multi-installment processing 
to distribute divisible load on d-dimensional hypercube. 
 
In this research project, we consider the hypercube and the mesh architectures with 
pipelined communication. The contributions of this research project are as follows: 
(1) We propose a method which employs multi-installment processing with 
  5
 
 
In the second year of this project, we consider a homogeneous hypercube network for 
analysis. All the links have the same communication speed and bandwidth. All the 
processors have the same processing capability. Each processor pi has ni separate ports to 
communicate with all other neighbors. In other words, processor pi can send/receive 
messages to all its neighbors simultaneously. A processor sends a load fraction to a 
neighbor, and then it can proceed with other computation and communication activities 
without waiting for the load fraction sending to be completed. This can carry out 
performing computation and communication simultaneously. However, a neighbor starts 
processing its load fraction only after receiving the entire load fraction from its predecessor. 
We assume that the time of returning the results can be neglected. 
TABLE 1 
Notations and Terminology 
N The number of processors. 
L The total load to be processed. 
Tcm The time to transmit a unit of load along a link. 
Tcp Tcp is the time to process a unit of load on a processor. 
β The computation-to-communication ratio of a processor in the node. This 
ratio is β= Tcp/Tcm. 
τj Interval j or the time duration of interval j. 
ρ ρ=β/(N-1). 
m m is the number of multi-installment intervals. 
x x is the temporary variable in each algorithm. The value of x is computed 
based on L and it is different in all the algorithms studies in this research 
project. 
Lj The amount of the load processed by processorj for algorithm. ∑  =
. 
	


 
The parallel execution time of algorithm A in a system of N processors. 



 
The speedup of algorithm A in a system of N processors. 
	,


 The parallel execution time of algorithm A with m installments in a system 
of N processors. 
,


 The speedup of algorithm A with m installments in a system of N 
processors. 
θcm The communication latency time to transmit the loads. 
θcp The computation latency time to transmit the loads. 
  7
3. Divisible Load Distribution on k-Dimensional Meshes Using Pipelined 
Communication with Multi-installment Processing  
3.1 The Classic Method on the Mesh Revisited (Algorithm Q) 
Following [1], Algorithm Q of the classic method that uses pipelined communications 
is illustrated by means of a distributing diagram as shown in Figure 3.1, with N = 4. In the 
time interval τ1, p4 transmits (1/β+1)2x units of load fraction to p3. In the time interval τ2, p3 
keeps (1/β+1)x/β units of load fraction for computing while transmits (1/β+1)x units of 
load fraction to p2 during receiving (1/β+1)x missing units of load fraction from p4. In the 
time interval τ3, p2 keeps x/β units of load fraction for computing while transmits x units of 
load fraction to p1 during receiving x missing units of load fraction from p3, which, in 
proper order, is receiving x units of load fraction from p4. In the time interval τ4, every 
processor computes a load fraction x. 
 
Figure 3.1  Distributing diagram of classic method on the linear array 
 
The Ljs are defined as follows: since processor pj computes (1/β+1)j-2x amount of load 
during τN-j+1, (1/β+1)j-3x amount of load during τN-j,…, till x amount of load during τN. The 
load fractions L0, L2, … , Ld are defined as 
  9
In our Algorithm M, that extends Algorithm Q with multi-installment processing is 
illustrated by means of a distributing diagram as shown in Figure 3.2, with N = 4. We 
divide time interval τ4 into 3 time interval. In the first time interval, p4 transmits ρ4-N+1x 
units of load fraction to p3 while every processor computes a load fraction ρ4-N+1x/β at the 
same time. In the second time interval, p3 transmits ρ4-N+1x units of load fraction to p2 
during receiving ρ4-N+1x missing units of load fraction from p4. Therefore, every processor 
computes a load fraction ρ4-N+1x/β. In the third time interval, p2 transmits ρ4-N+1x units of 
load fraction to p1 during receiving ρ4-N+1x missing units of load fraction from p3, which, in 
proper order, is receiving ρ4-N+1x units of load fraction from p4. Therefore, every processor 
computes a load fraction ρ4-N+1x/β. We divide and process the rest time intervals τ5 …τm+4 
as the same way we divide τ4. In the time interval τm+4-1, every processor computes a load 
fraction ρm-1x. 
 
 
Figure 3.2.  Distributing diagram of pipelined communication with multi-installment 
processing on the linear array 
 
We give a complete description of Algorithm M in Figure 3.3. Because the 
communication time must equal to the computation time on each installment, the notation 
ρ is defined as 
1−
=
N
βρ
       (3.2.1) 
  11 
( ) 11
1
1111111 1
12
=−
−
+














+++





++





++ −
−
xNx m
N
ρ
ρ
ρ
βββ K   (3.2.3) 
which yields 
( ) ( ) 1111
1
1
1
−−
−
+





+
=
−m
N
N
x
ρ
ρβ
ρ
β
β
     (3.2.4) 
and  
( )
( )
( ) ( ) 1111
1
1
1
111
1
1
11
1
1
1
1
1
−−
−
+





+








−
−
+





+
=
−
−
+





+=
−
−
−
−
−
m
N
m
N
m
N
N
N
xxL
ρ
ρβ
ρ
β
ρ
ρ
ρ
ββ
ρ
ρ
ρ
β
     (3.2.5) 
Consequently, 
( )
( ) ( )
( )
( ) ( )
11
1
1
1
1
1
1
11
1
1
1
1
1
1
1
11
1
11
1
1
11
−−
−
−
−
−
−
−






+
−





+
−
−
+




 +






+
−
−
+
=
−−
−
+





+
−
−
+





+
==
NN
m
N
m
m
N
m
N
N
M
N
N
N
LT
β
β
β
βρ
ρβ
ρ
β
β
β
βρ
ρ
ρ
ρ
ρβ
ρ
β
ρ
ρ
ρ
ββ
  (3.2.6) 
and 
( ) ( )
( ) 11
11
1
1
1
1
1
11
1
1
1
−
−
−−
−






+
−
−
+






+
−





+
−
−
++
= N
m
NN
m
M
N
N
S
β
βρ
ρ
ρ
β
βββ
βρρ
ρβ
  (3.2.7) 
In case of ρ=1, the load fraction Lj of the formula (3.2.2) will become infinite. In 
  13
( ) ( )
( )
1
11
1
11
11
11
−
−−






+
−+






+
−





+
−++
= N
NN
M
N
m
mN
S
β
β
β
βββ
ββ
  (3.2.13) 
3.3  Algorithm for Pipelined Communication with Start-up Cost 
On the theoretical side, infinite number of installments could lead to optimal 
performance. But on the practical side, the number of installment increases as start-up cost 
increases. In other words, the speedup does not necessarily increase as the number of 
processors exceeds certain values. Similarly, the speedup does not increase as the number 
of installment exceeds certain values. The timing diagram of Algorithm Q with start-up 
cost is shown in Figure 3.6. The communication time and the computation time of a load 
fraction w are given by wTcm+θcm and wTcp+θcp. 
 
 
Figure 3.6  Timing diagram for pipelined communication with start-up cost 
3.3.1  Improvement of Algorithm Q (Algorithm S) 
We propose an improved method which employs the pipelined communication with 
consideration of start-up cost to distribute divisible loads. In the first Algorithm S, it 
  15
 
Figure 3.8  Algorithm S 
 
We give a complete description of Algorithm S in Figure 3.8. The Ljs are defined as 
follows: since processor pj computes (1/β+1)j-2x amount of load during τN-j+1, (1/β+1)j-3x 
amount of load during τN-j,…, till x+∆ amount of load during τN. The load fractions L1, 
L2, … , LN are defined as 
∆+





+=
−
xL
j
j
1
11 β       (3.3.4) 
for all 1 ≤ j ≤ N. The condition L1 + L2 + … + LN = L gives 
LNx
NN
=∆+














++





+++





++
−− 12
1111111 βββ L   (3.3.5) 
which yields 
( )
111
1
−





+
∆−
= N
NL
x
β
β
      (3.3.6) 
and  
  17
ρ
m-1
x+ρm-1∆+ρm-2∆+…+∆. 
 
 
Figure 3.9  Distributing diagram of Algorithm MS on the linear array 
 
We give a complete description of Algorithm MS in Figure 3.10. The load fractions L1, 
L2, … , LN are defined as 
( ) 





−
−
−
−
∆
+−
−
+





+=
+
−
−
mxxL
m
m
j
j 11
1
1
11
1
1
1
ρ
ρρ
ρ
ρ
ρ
ρ
β   (3.3.10) 
for all 1 ≤ j ≤ N. The condition L1 + L2 + … + LN = L gives 
( ) LmNxN
x
m
m
NN
=





−
−
−
−
∆
+−
−
+














++





+++





++
+
−
−−
11
1
1
1111111
1
1
12
ρ
ρρ
ρ
ρ
ρ
ρ
βββ L
   (3.3.11) 
  19
( )






−
−
−
−
∆
+
−





−
−
+





+








−
−
+





+













−
−
−
−
∆
−
=
+
−
−+
m
N
m
NL
L
m
mN
m
Nm
N
11
1
1
11
1
1
11
11
1
1
1
11
ρ
ρρ
ρ
ρ
ρρ
ββ
ρ
ρ
ρ
βρ
ρρ
ρβ
 (3.3.13) 
Consequently 
( )
( )
( ) cpcp
m
cp
mN
m
Nm
cp
cpcpNcp
MS
N
mNm
T
N
m
NL
T
mNLTT
θθ
ρ
ρρ
ρ
ρ
ρρ
ββ
ρ
ρ
ρ
βρ
ρρ
ρβ
θθ
+−+





−
−
−
−
∆
+
−





−
−
+





+








−
−
+





+













−
−
−
−
∆
−
=
+−+=
+
−
−+
1
11
1
1
11
1
1
11
11
1
1
1
11
 (3.3.14) 
and 
( )
( )
( )






















+−
+





−
−
−
−
∆
+
−





−
−
+





+








−
−
+





+













−
−
−
−
∆
−
+
=
+−+
+
=
+
−
−+
cm
cpcp
m
mN
m
Nm
cm
cp
cpcpNcp
cpcpMS
N
T
mN
m
N
m
NL
T
L
mNLT
LT
S
θθ
ρ
ρρ
ρ
β
ρ
ρρ
ββ
ρ
ρ
ρ
βρ
ρρ
ρ
θβ
θθ
θ
1
11
1
1
11
1
1
11
11
1
1
1
11
(3.3.15) 
 
In case of ρ=1, the load fraction Lj of the formula (3.3.10) will become infinite. In 
practical, the load fractions L1, L2, … , LN are defined as 
( ) ( ) ,
2
1111
1
∆+
+−+





+=
−
mm
xmxL
j
j β     (3.3.16) 
  21
3.3.3  The Optimal Number of Installments and Number of Processors 
Considering situations that the start-up costs exist, the speedup does not necessarily 
increase as the number of processors exceeds certain values. Similarly, the speedup does 
not increase as the number of installment exceeds certain values. We first analyze the 
Algorithm Q which includes cause of the start-up costs. We add (N-1)max{θcp,θcm} start-up 
costs to the parallel execution time because of overlaps of computation and communication. 
Then we add θcp start-up cost to the parallel execution time for computing the final load 
fraction. By the formula (3.1.4), 
( ) { } ( ) { } 　cpcmcpN
N
cp
cpcmcpcpN
Q
N N
LT
NTLT θθθ
β
ββθθθ +−+
−





+






+
=+−+=
−
,max1
111
11
,max1
1
 (3.3.22) 
The function TNQ has an extreme at a number N, then TNQ’ = 0. 
{ }
　0,max
111
11log11
2
1
=+








−





+






+





+−
=
−
cmcp
N
N
cp
Q
N
LT
T
dN
d θθ
β
βββ
  (3.3.23) 
which yields 
{ }
　1
11log
112
,max
11log
log
−






+




















++






+
<
β
βθθβ
β
cmcp
cpLT
N     (3.3.24) 
 
We get the optimum number of processor at this range. If the number of processor N 
exceeds this range, the speedup does not increase. 
  23
( )
( )
( ) { } cpcmcpN
N
cp
M
N Nm
mN
m
LT
T θθθ
ββ
ββ
+−+
−−+





+








−+





+
=
−
,max1
11111
111
1
  (3.3.28) 
The function TNM has an extreme at a number m, then TNM’ = 0. 
( )
( ) { } 0,max1
11111
11111
2
1
=−+








−−+





+








−





+−





+
=
−
cmcp
N
NN
cp
M
N N
mN
NLT
T
dm
d θθ
ββ
ββββ
 (3.3.29) 
which yields 
( ) { } 11
1
,max1
11111
1
++





+−
−








+





++





+−
=
−
NNN
NLT
N
m
N
cmcp
NN
cp
β
β
β
θθ
βββββ
 (3.3.30) 
 
We get the optimal number of installment. If number of installment m exceeds this value 
then the speedup does not increase. 
3.4  Extension to k-Dimensional Meshes 
Following [1], the k-dimensional mesh Mk of size N = N1×N2×…×Nk is to be treated as 
a linear array M1 of size Nk, where each processor pj, 1 ≤ j ≤ Nk, is replaced by a 
(k-1)-dimensional sub-mesh Mk-1 of size N1×N2×…×Nk-1. Therefore, we distribute divisible 
load on each processor pj, 1 ≤ j ≤ Nk. When a load fraction is being processed, processor pj 
does not compute the load fraction alone but divides the load fraction according to our 
algorithm to smaller load fractions and distributes these load fractions on the entire 
(k-1)-dimensional sub-mesh Mk-1 to be process in parallel. The illustration k-dimensional 
meshes diagram is show in Figure 3.11. 
 
  25
 
Figure 3.12  Speedup of Algorithm Q, M 
 
Considering start-up cost exists, in Figure 3.13, we demonstrate the numerical values 
of the speedup of the two algorithms Q and S mentioned earlier. Assuming that Tcp = β = 
100, θcm/Tcp = 1, 10, θcp/Tcp = 2, 1 and L = 1E3, the maximum speedup of Algorithm S is 
greater than maximum speedup of Algorithm Q. The Algorithm S improves the 
performance of the Algorithm Q by degrees of the size of loads. Considering cause of 
start-up cost exists, the speedup of Algorithm Q is 
( ) { }
　
cm
cpcmcp
N
N
cm
cp
Q
N
T
N
L
T
L
S
θθθ
β
β
θβ
+−
+
−





+






+
+
=
−
,max1
111
11
1     (3.5.1) 
  27
 
Figure 3.14  Speedup of Algorithm M, MS 
 
In Figure 3.15, we demonstrate the numerical values of the speedup of the Algorithms 
Q. Assuming that Tcp = β = 100, θcm/Tcp = 200 and θcp/Tcp = 20, the speedup does not 
necessarily increase as the number of processors exceeds certain values. By the formula 
(3.3.24), we get the approximate value of N < 91.78, 194.49 and 395.65 on the total load L 
= 1E4, 1E5 and 1E6. 
  29
the total load L = 1E6, 1E7 and 1E8. 
 
Figure 3.16  Speedup of Algorithm M 
  31
 
Figure 4.1  Distributing diagram of classic method on the 3-dimensional hypercube 
 
4.2  Algorithms for Pipelined Communication (Algorithm P) 
According to the literature [4], [15], we know that an optimal schedule must be 
satisfying two properties: “there is no idle time between consecutive computation on each 
worker” and “all workers should finish computing at the same time”. Our Algorithm P that 
uses pipelined communications [1] is illustrated by means of a distributing diagram as 
shown in Figure 4.2, with d = 3. We number the chunks in the reverse order in which they 
are allocated to processors: the last chunk is numbered 0 and the first chunk is numbered d. 
In the time interval τ1, p0 transmits Lm0,3 units of load fraction to p1. And then in the time 
interval τ2, p1 keeps Lp2 units of load fraction for computing while transmits Lm1,2 units of 
load fraction to p2 during the time p1 is receiving Lm0,2 units of load fraction from p0. Then 
in the time interval τ3, p2 keeps Lp1 units of load fraction for computing while transmits 
Lm2,1 units of load fraction to p3 during the same time p2 is receiving 2×Lm1,1 units of load 
fraction from p1, which, in proper order, is receiving Lm0,1 units of load fraction from p0. In 
the time interval τ4, every processor computes a load fraction Lp0. The load fractions Lpj 
and Lmi,j are defined as 
djxPL jpj ≤≤= 1,  and djidjdixmL jijmi ≤+≤≤≤−≤≤= 1,1,10,,,  
  33
djidjdi
j
ji
j
id
ji
lj
lji
lj
id
lji
P
i
P
m
md
jimji
mjid
lji
P
i
P
pI
pO
m
pI
pO
pI
P
pI
P
pI
mpOP
pI
pO
pI
P
pI
mpOP
m
j
l
lj
ji
im
j
lm
j
l
lj
ji
im m
m
ji
j
lm mji
mji
j
l lji
l
i
j
i
jiij
i
i
i
j
i
jiij
ji
≤+≤≤≤−≤≤






−
−+






−
−−
+
+






−−
−−+






−−
−−
−+
+
+
=
−
+
+
−−+
++−−
−+
+
+
=
++=
=





 +
+=
+
=
∑
∏∏∑
∏∏∑
−
=
−
−+
+=
−
=
−
=
−
−+
+=
−+
−
=
−−+
−−+
−
=
−++
−
+
−++−
+
+
+
−
+
−++−
1,1,10
,
1
1
1
1
1
1
1
1
1
1
1
1
1
1
)(
)(
)(
)(
)()(
)(
)(
)(
)(
)(
)(
)(
2
1
1
1
1
22
1
1
1
1
1,1
2
1
1
2
11
1
2
2,222
1
1
1
1
1
1,111
,
L
  (4.2.3) 
and 
djj
d
jlj
d
lj
P
P
m
P
j
l
l
j
j
j
≤≤






−
−
+





−−
−
−
+
=
=
∑
−
=
−
1,
1
11
1
12
1
1
,0
β
β
    (4.2.4) 
where 
1
,
1
,1 01,10
+
===
i
P
mPP iβ . Let 0)( =∑
=
j
in
nf , if i > j. 
  35
10,
1
1
1
1
1
1
1
1
1 0,2
1
,1,1
11,1
,
11,1,
−≤≤




















−−
−
+






−−−
−−






−−−
−−
−
+
+
+
−=
+−=
∑
−−
=
−−−+
−−−−+
−
−−−−+−
di
id
dd
P
lid
ld
lid
id
ld
P
i
P
PP
m
PPP
d
id
l
lldidi
ididi
idi
ididiidi
β
β
 (4.2.7) 
From (4.2.5), we get that 11,1 −− = dd PP  and ∑∑
−
=
−
−
=
−=
1
1
,
1
0
0,
1 d
i
idi
d
i
id mPP β . 
 
Figure 4.4  Algorithm P 
 
We give a complete description of Algorithm P in Figure 4.4. The Ljs are defined as 
follows: since processor pj first computes x(Pj,d-j-Pd-j) amount of load, then computes xPd-j 
amount of load during τj+1 and xPd-j-1 amount of load during τj+2,…, till xP0 amount of load 
during τd+1. The load fractions L0, L1, … , Ld are defined as 
diPPxL idi
id
n
ni ≤≤





+=
−
−−
=
∑ 0,,
1
0
     (4.2.8) 
  37
In our Algorithm M, that extends Algorithm P with multi-installment processing is 
illustrated by means of a distributing diagram as shown in Figure 4.5, with d = 3. We 
divide time interval τ4 into 3 time interval. In the first time interval, p0 transmits ρ4-dM0,3x 
units of load fraction to p1 while every processor computes a load fraction ρ4-dM0,3x/β at the 
same time. In the second time interval, p1 transmits ρ4-dM1,2x units of load fraction to p2 
during receiving ρ4-dM0,2x units of load fraction from p1. Therefore, every processor 
computes a load fraction ρ4-dM0,2x/β. In the third time interval, p2 transmits ρ4-dM2,1x units 
of load fraction to p3 during receiving 2ρ4-dM1,1x units of load fraction from p1, which, in 
proper order, is receiving ρ4-dM0,1x units of load fraction from p0. Therefore, every 
processor computes a load fraction ρ4-dM0,1x/β. We divide and process the rest time 
intervals τ5 …τd+m-1 as the same way we divide τ4. In the time interval τd+m, every processor 
computes a load fraction ρm-1x. 
 
 
Figure 4.5  Distributing diagram of pipelined communication with multi-installment 
processing on the 3-dimensional hypercube. 
 
We are defined Mi,j as 
( )
( )
( )
( )
( )
( ) ( )
∏
−+
+=
+−+
−+
+
+
+
+
≤+≤≤≤−≤≤−
+
=
××××=
1
1
1
1
2
2
1
1
,
1,2,10,1
1
ji
in
jiji
ji
i
i
i
i
ji
djidjdi
n
nd
ji
pIpI
pO
pI
pO
pI
pO
M L
  (4.3.1) 
  39
∑
−
=
−





 −
=
1
0
1
d
m md
m
d
βρ       (4.3.3) 
Let Lj denotes the amount of loads processed by processor pj, where 0≤j≤d. The load 
fractions L0, L1, … , Ld are defined as 
diPPxxL idi
id
n
n
m
i ≤≤≠





++





−
−
=
−
−−
=
−
∑ 0,1,1
1
,
1
0
1
ρ
ρ
ρρ
   (4.3.4) 
The condition c(d,0)L0 + c(d,1)L1 + … + c(d,d)Ld = 1 gives 
1
1
12
0
,
1
0
1
=





+





+





−
−
∑ ∑
=
−
−−
=
− d
i
idi
id
n
n
m
d PP
i
d
xx
ρ
ρρ
    (4.3.5) 
which yields 
∑ ∑
=
−
−−
=
−






+





+





−
−
=
d
i
idi
id
n
n
m
d PP
i
d
x
0
,
1
0
1
1
12
1
ρ
ρρ
    (4.3.6) 
and 
∑ ∑
∑
=
−
−−
=
−
−
=
−






+





+





−
−
++





−
−
=
d
i
idi
id
n
n
m
d
d
d
n
n
m
PP
i
d
PP
L
0
,
1
0
1
,0
1
0
1
0
1
12
1
1
ρ
ρρ
ρ
ρρ
    (4.3.7) 
Consequently, 
∑ ∑
∑
=
−
−−
=
−
−
=
−






+





+





−
−








++





−
−
==
d
i
idi
id
n
n
m
d
d
d
n
n
m
M
d
PP
i
d
PP
LT
0
,
1
0
1
,0
1
0
1
0
1
12
1
1
ρ
ρρ
ρ
ρρβ
β    (4.3.8) 
and 
  41
( )
( )1
12
,0
1
0
0
,
1
00
−++
−+





+





==
∑
∑ ∑
−
=
=
−
−−
=
mPP
mPP
i
d
T
TS
d
d
n
n
d
d
i
idi
id
n
n
d
M
d    (4.3.15) 
4.4  Discussions of the Results 
 
Figure 4.9  Speedup of Algorithms C, P and M 
 
In Figure 4.9, we demonstrate the numerical values of the speedup of the three 
algorithms mentioned C, P, and M. It is assumed that β = 100. The following facts can be 
observed: The speedup SdC of Algorithm C grows slowly. The speedup SdP of Algorithm P 
is growing exponentially. When m = 10, the Algorithm M improves the Algorithm P with 
speed-up of 40.2% at d = 9. When ρ < 1, the speedup SdM of Algorithm M is growing 
slowly. 
 
  43
( ) kkk mdPm ,11,0 1−+=+  and kkk mdPm ,21,1 2
2
2
−
+=+  
( ) kkkk md
d
mdPP
,2,1
21,
2
−
>−>Q  
1,11,0 ++ >∴ kk mm  
Similarly, m0,k+1 > m1,k+1 > m2,k+1 >…> md-k-1,k+1 
 
REFERENCES 
[1] K. Li, “Improved Methods for Divisible Load Distribution on k-Dimensional Meshes 
Using Pipelined Communications,” IEEE Trans. Parallel and Distributed Systems, 
vol. 14, no.12, pp. 1250-1261, Dec. 2003. 
[2] B. Veeravalli and W. H. Min, “ Scheduling Divisible Loads on Heterogeneous Linear 
Daisy Chain Networks with Arbitrary Processor Release Times” IEEE Trans. Parallel 
and Distributed Systems, vol. 15, no.3, pp. 273-288, Mar. 2004. 
[3] X. Li, B. Veeravalli and C.C. Ko “Divisible Load Scheduling on a Hypercube Cluster 
with Finite-size Buffers and Granularity Constraints,” IEEE/ACM International 
Symposium, pp. 660–667, May 2001. 
[4] O. Beaumont, H. Casanova, A. Legrand, Y. Robert and Y. Yang, “Scheduling 
Divisible Loads on Star and Tree Networks: Results and Open Problems,” IEEE 
Trans. Parallel and Distributed Systems, vol. 16, no. 3, pp. 207-218 Mar. 2005 
[5] M. Drozdowski and P. Wolniewicz, “Out-of-core divisible load processing,” IEEE 
Trans. Parallel and Distributed Systems, vol. 14, no. 10, pp. 1048-1056 Oct. 2003. 
[6] K. Li, “Accelerating divisible load distribution on tree and pyramid networks using 
pipelined communications,” Parallel and Distributed Processing Symposium, 2004. 
Proceedings. 18th International, pp. 228, April 2004. 
[7] Y. C. Cheng and T. G, “Distributed Computation with Communication Delays,” 
IEEE Trans. Aerospace and Electronic Systems, vol. 24, no. 6, pp. 700-712, Nov. 
1988. 
[8] Jeeho. Sohn, Robertazzi. T.G. and Luryi. S, “Optimizing computing costs using 
