應用 Haar 離散小波分析法與類神經網路建構管制圖
非隨機樣式之辨識系統 
 
計畫編號：NSC 95-2221-E-155-052 
執行期間：95 年 08 月 01 日至 96 年 07 月 31 日 
計畫主持人：鄭春生 教授    元智大學工業工程與管理系 
 
一、中文摘要 
 
管制圖是統計製程管制  (statistical 
process control, SPC) 中之主要工具，它可用
來判斷製程是否存在可歸屬原因所造成之變
異。當製程存在可歸屬原因時，管制圖會呈
現特定的非隨機樣式變化，例如趨勢樣式、
週期型樣式、偏移樣式、系統型樣式、混合
型樣式等。 
本研究之目的為建立一個以類神經網路
為基礎之辨識系統，用以偵測並辨識非隨機
樣式的類型，以作為規劃矯正措施的依據。
首先，本研究利用 Haar 離散小波多重解析
擷取重要製程數據之特徵。製程數據經 Haar 
離散小波轉換  (discrete wavelet transform, 
DWT) 後 可 獲 得 小 波 尺 度 係 數  (scale 
coefficient) 與 小 波 係 數  (wavelet 
coefficient)。製程數據可以利用不同解析尺度
下的係數來表達，以呈現重要的特徵。其次，
本研究將建立一個監督式之類神經網路 
(artificial neural network, ANN) 作為非隨機
樣式的辨認系統。我們考慮各類非隨機樣式
同時存在與單一週期性樣式單獨發生之情
形，評估原始數據和小波之特徵擷取方式，
對於類神經網路之辨識績效的影響。 
模擬分析之結果顯示，利用 Haar 離散
小波多重解析之特徵值作為類神經網路之輸
入，可提高非隨機樣式之辨識率並且降低網
路結構之複雜性。 
 
關鍵字：統計製程管制、Haar 離散小波轉
換、特徵值擷取、類神經網路 
 
Control Chart Pattern Recognition Using 
Haar Discrete Wavelet Analysis and 
Neural Networks 
 
Abstract  
 
Control chart pattern recognition is an 
important task in statistical process control. 
This paper presents a neural network-based 
classifier for control chart pattern recognition. 
We apply Haar discrete wavelet transform 
(DWT) to extract distinguished features from 
raw data. The extracted features are used as the 
components of the input vectors. In addition, 
we will focus on single cyclic pattern and 
concurrent patterns that can be characterized 
using this classifier. Extensive comparisons 
based on simulation study indicate that the 
proposed neural network performs better than 
that using raw data as inputs. Our research 
concluded that the extracted features can 
greatly improve the performances of the neural 
network. 
 
Keywords: statistical process control, neural 
network, Haar discrete wavelet 
transform, extracted features. 
 
二、緒論 
 
管制圖為統計製程管制之主要工具。美
國 Western Electric Handbook [15] 指出管制
圖中常出現之非隨機樣式包含向上或向下趨
勢 樣 式  (increasing trend and decreasing 
trend)、週期型樣式 (cyclic pattern)、系統型
樣 式  (systematic pattern) 、 混 合 型 樣 式 
(mixture) 與偏移樣式  (upward shift and 
downward shift) 等五種類型。當管制圖出現
上述非隨機樣式時，應立即診斷原因並採取
矯正措施。 
為提升管制圖之靈敏度，過去常用區間
測試 (zone tests) 與連串測試 (run tests) 兩
種法則 [11, 15]，來判斷管制圖是否為管制外
或出現非隨機樣式。但這些法則與非隨機樣
 3
非隨機樣式的辨識外，類神經網路也常被用
應於非隨機樣式之參數估計 [6]。 
非監督式類神經網路如自組織映射類神
經網路 (self-organizing map, SOM) 與自適
應共振理論網路 (adaptive resonance theory 
network, ART) 也被應用於非隨機樣式之分
類 [10, 14]。 
 
3.3 小波分析於統計製程管制之應用 
 
最近小波理論也被應用於管制圖非隨機
樣式之偵測與辨識。Al-Assaf [1] 利用小波多
重解析  (multi-resolution wavelet analysis, 
MRWA) 擷取數據之特徵值並結合倒傳遞類
神經網路來進行非隨機樣式之辨認。其研究
成果顯示 MRWA 所獲得之特徵係數可以描
述數據之重要特徵，提升辨識績效。 
Bakshi [3] 提 出 以 多 尺 度 分 析 
(multiscale analysis) 之觀念，來進行製程數
據之分析。Aradhye 等人 [2] 應用 Haar 離
散小波多重解析度分析於製程平均值偏移之
偵測。 
 
四、辨識非隨機性樣式之類神經網路 
 
本研究是利用類神經網路來發展一個管
制圖非隨機樣式之辨識系統。類神經網路是
模仿人腦思考之一種資訊處理、運算系統。
類神經網路是由許多運算元連接所組成，各
運算元間之關係強弱是以權數來表示。類神
經網路之權數是利用訓練樣本經由學習法則
來調整，以獲得解決問題之知識。 
本研究採用監督式之倒傳遞類神經網路 
(back-propagation neural network)，它是一種
層狀前饋 (layered feed-forward) 網路架構，
包含了輸入層、輸出層及隱藏層。輸入層是
用來接受外界的輸入訊號。輸出層是用來將
經過此系統運算的結果輸送到外界。介於輸
入層及輸出層之間並列的運算元，則是用來
接受前層運算後之輸出值為輸入訊號，並經
運算後輸出訊號給下一層，這些位於中間層
次並列的運算元，則稱為隱藏層。在這些層
與層之間都有連接鍵相互連接，作為網路訊
號或資訊的傳遞路徑，以權數的方式來表
示 ， 又 稱 之 為 連 接 強 度  (connection 
strength)。在倒傳遞網路中，資料是以順向 
(forward) 的方式傳遞，當輸入層取得輸出訊
號後，經連接鍵加權以及轉換函數的運算
後，輸出其訊號做為下一層的輸入訊號。 
以下將針對訓練樣本之產生、特徵之擷
取、類神經網路架構、類神經網路之訓練、
效益評估指標與評估方法加以說明。 
 
4.1 訓練樣本之產生 
 
類神經網路是利用訓練樣本之學習來獲
得網路之連結權數。本研究之訓練樣本是以
蒙地卡羅模擬法來產生。管制圖之非隨機樣
式可表示為 
 
)()()( tdtntx ++= μ  (1) 
 
其中 μ 代表製程之平均值， )(tn 表示機遇原
因所造成之變異， )(td 則代表可歸屬原因 
(非隨機樣式) 所造成之變異。本研究所探討
之訓練樣本包含：(1) 單一週期性樣式單獨出
現於製程之情形，稱為訓練樣本 I；(2) 三種
非隨機樣式同時存在製程，稱為訓練樣本 
II。本研究在訓練樣本 I 中，週期性樣式將
只考慮不同週期參數之情形，並未區分不同
方向之改變。在訓練樣本 II 中，本研究所探
討之非隨機樣式包含趨勢樣式、週期型樣式
與偏移樣式等三種類型，其中趨勢樣式與平
均值偏移樣式將區分不同方向之變化。有關
非隨機性樣式之產生方法可參考文獻 [4]。圖
2 說明不同訓練樣本所包含之非隨機樣式種
類。類神經網路之訓練樣本必須包含各種樣
式之不同變異程度的數據，以獲得較佳之辨
識效果。在本研究中，各樣式之訓練樣本個
數均為相同。在訓練樣本 II 中，對於平均值
偏移樣式在偏移量較大之情況下，訓練樣本
將包含正常和已偏移之數據。此種方法將易
於偵測較大偏移量的數據。 
 5
表3 特徵組合實驗之範例 
實驗 
編號 輸入向量之組成元素 
特徵
個數
1A  )16(1
+F  16
2A  )8()8( 22
−+ + FF  16
3A  )8()4()4( 233
−−+ ++ FFF  16
4A  )8()4()2()2( 2344
−−−+ +++ FFFF   16
5A  )8()4()2()1()1( 23455
−−−−+ ++++ FFFFF  16
6A  )4()4()8( 332
−++ ++ FFF  16
7A  )16()8()4()2()1()1( 234566
−−−−−+ +++++ FFFFFF 32
 
4.3 類神經網路架構 
 
本研究所採用之類神經網路為監督式之
倒傳遞網路，包含一層輸入層、一或多層之
隱藏層與一層輸出層。倒傳遞網路之架構可
表示為 In - Hn - On ，其中 In 為輸入層之運算元
個數， Hn 為隱藏層之運算元個數， On 為輸出
層之運算元個數。 In 之數目是由輸入向量來
決定。為偵測非隨機樣式，類神經網路可採
用原始數據或由原始數據中所擷取之特徵作
為輸入向量。 
在網路架構 I 之情況我們將採用雙彎
曲函數 (sigmoid function) [5] 作為各層運算
元之轉換函數，此函數之輸出值範圍為 
[0,1]。在網路架構 II 之情況我們則採用雙曲
線正切函數 (hyperbolic tangent function)  [5] 
作為各層運算元之轉換函數，此函數之輸出
值範圍為 [-1,1]。採用雙曲線正切函數有利
於辨識非隨機樣式之方向性。除此之外，輸
出運算元採用雙曲線正切函數可以用來表達
兩種狀態，有利於減少輸出運算元之數目，
降低網路之複雜度。 
在網路架構 I ，二個週期同時出現製程
中時， On 由 2 個運算元所構成；當三個週
期同時出現製程中時， On 則由 3 個運算元
所構成。在網路架構 II， On 設定為 3，代表
三種非隨機樣式之類型，一個運算元可以表
達一種非隨機性樣式之不同方向的變化。各
類非隨機樣式之輸出表達方式如表 4-5 所
示。隱藏層運算元個數 Hn 並無固定之公式來
決定，大多以試誤法來決定。一般原則為若
Hn 太大則會導致網路產生過度記憶效果而
使一般性不佳，反之，若 Hn 太小，則無法學
習到輸入與輸出之關係。本研究經由多次實
驗後，將 Hn 設為 2/)( OI nn + 。 
 
表4 網路架構 I 輸出運算元之表達方式 
樣式種類 2 個週期之輸出 3 個週期之輸出
週期性樣式 1  ( 1  0 ) ( 1  0  0 ) 
週期性樣式 2  ( 0  1 ) ( 0  1  0 ) 
週期性樣式 3  － ( 0  0  1 ) 
隨機(管制內)樣式 ( 0  0 ) ( 0  0  0 ) 
 
表5 網路架構 II 輸出運算元之表達方式 
樣式類型 輸出 
上升趨勢樣式 ( 1  0  0 ) 
下降趨勢樣式 ( -1  0  0 ) 
週期型樣式 1 ( 0  1  0 ) 
週期型樣式 2 ( 0  -1  0 ) 
平均值向上偏移樣式 ( 0  0  1 ) 
平均值向下偏移樣式 ( 0  0  -1 ) 
 
偏誤項 (bias) 之使用與否將影響不同
方向之樣式辨識能力。因此，本研究在網路
設計上，特別考量網路偏誤項之使用時機，
在網路架構 I 中，由於週期性樣式並未區分
方向，故保留偏誤項與輸出層及隱藏層的連
接設定。然而在網路架構 II 中，我們考量非
隨機樣式之不同方向的變化，為了使類神經
網路對不同方向的樣式能有相同之辨識能
力，本研究將網路之偏誤項取消。 
 
4.4 類神經網路之訓練 
 
類神經網路之起始權數設定在一個極
小之範圍。學習速率設為 0.3，動量參數設
定為 0.4，這些倒傳遞網路之學習參數會隨
著訓練次數之增加而遞減。在訓練時，訓練
樣本是以隨機之次序送入類神經網路學習。 
本研究中所使用的網路學習終止條件
為學習次數法。學習次數太少易造成誤差太
大，相反的學習次數太多又將造成學習時間
過長。因此，本研究在網路架構 I，所使用
的學習次數設定為訓練樣本總數的  100 
倍。在網路架構 II，所使用的學習次數設定
為訓練樣本總數的 50 倍。本研究使用類神
經網路軟體 NeuralWorks Professinal II/Plus 
[12] 進行類神經網路的學習。 
 7
為了評估網路架構 II 之辨識績效，本研
究以固定型 I 誤差分別為 =α 0.1、0.15和0.2
三種基準下，計算正確辨識出非隨機樣式之
比率作為效益指標。研究結果顯示，網路架
構 II 在偵測各類非隨機樣式時，類神經網路
使用 5A 實驗之特徵值所獲得之正確辨識率
最佳。表10-12 分別顯示固定型 I 誤差為
=α 0.1、0.15與0.2時，趨勢樣式和平均值偏
移樣式在相同參數水準下，週期性樣式之週
期參數為 8 時所獲得結果之混亂矩陣。各樣
式之正確辨識率是經由 10000 次之模擬所
獲得，測試數據之產生和參數範圍如同訓練
樣本。 
類神經網路使用實驗 5A 所獲得之小波
轉換特徵作為輸入訊號，對於趨勢樣式和平
均值偏移樣式之改善率尤其顯著。表 13 說
明網路架構 II 在不同週期參數下，固定不同
型 I 誤差之整體改善率。表 13 顯示整體改
善率隨著型 I 誤差之遞減而改善幅度愈大。
以型 I 誤差固定在 0.1 的基準下，週期 8、
12 和 16 三種週期參數之整體改善率分別提
升 7.18%、7.43%和 11.37%。此外，由混亂
矩陣亦可得知，辨識系統對於同一種非隨機
樣式但不同方向之變化，具有相同或非常接
近之辨識能力。 
 
表 13 網路架構 II 使用 32 組原始數據和 5A
實驗特徵值之改善率 
型 I 誤差 
週期參數 0.1 0.15 0.2 
8=p  7.18% 4.33% 3.44%
12=p  7.43% 6.03% 5.02%
16=p  11.37% 5.56% 4.30%
 
為了探討不同視窗大小之影響性，本研
究再將視窗大小設為 32，此情況下網路將具
有相同數目之輸入元素。研究結果顯示，不
論類神經網路使用原始數據或是小波轉換特
徵值作為輸入訊號，正確辨識率都會隨著週
期性樣式之週期參數愈小而愈高之現象。表
14 說明網路架構 II 在不同週期參數下，以
固定三種型 I 誤差為基礎，類神經網路偵測
多類非隨機樣式之改善率。整體改善率隨著
型 I 誤差之遞減而改善幅度愈大。以型 I 誤
差固定在0.1的基準下，週期8、12和16三種
週期參數之整體改善績效分別提升20.57%、
24.75%和29.51%。 
 
表 14 網路架構 II 使用 32 組原始數據和 7A
實驗小波特徵值之改善率 
型 I 誤差
週期參數 0.1 0.15 0.2 
8=p  20.57% 14.61% 11.80%
12=p  24.75% 18.69% 15.33%
16=p  29.51% 19.36% 15.10%
 
六、結論 
 
本研究之結果可歸納為下列三項結論： 
1. 類神經網路使用 Haar 離散小波多重解析
所獲得之特徵值作為輸入向量，其成效優
於使用原始數據之類神經網路。 
2. 經由 Haar 離散小波多重解析特徵值之擷
取後，可使用較少之特徵數目獲得優於使
用原始數據之效益。因此，類神經網路可
以使用較少之輸入運算元個數，進而降低
網路運算時間與網路結構之複雜性。 
3. 研究結果顯示，適當的特徵可以提升類神
經網路之績效並降低類神經網路之複雜
度。 
在未來的研究上，主要任務為提升系統
的辨認率，為了要達成此目標，類神經網路
的學習是一項重要的關鍵，而非隨機樣式的
選擇與不同的小波轉換方式亦會影響其結
果。未來的研究重點如下： 
1. 針對不同的單一非隨機樣式進行探討，例
如單一趨勢樣式或單一平均值偏移樣式
等，利用小波轉換之數據擷取方式來提升
系統的辨認能力。 
2. 探討不同的小波轉換方式以擷取不同特徵
組合，研究其對於辨識績效之影響，例如
小波還原之硬門檻與軟門檻的不同小波轉
換方式。 
 
 9
表 9 網路架構 I 使用 )32(0R 和 6A 實驗特徵值之正確辨識率 ( )16,12,8(=p ) 
網路辨識結果     
週期樣式( 8=p )  週期樣式( 12=p )  週期樣式( 16=p ) 
輸入樣式 )32(0R  6A 實驗 )32(0R 6A 實驗 )32(0R  6A 實驗 
週期樣式( 8=p ) 0.8328 0.9165 0.1129 0.0585 0.0543 0.0249 
週期樣式( 12=p ) 0.0679 0.0866 0.5577 0.6278 0.3744 0.2856 
週期樣式( 16=p ) 0.0263 0.0399 0.1603 0.0619 0.8134 0.8982 
 
表 10 網路架構 II 使用 )32(0R 和 5A 實驗特徵值之正確辨識率 ( 8=p ， 1.0=α ) 
 網路辨識結果 
 上升趨勢 下降趨勢 週期樣式 向上偏移 向下偏移 
輸入樣式 )32(0R  5A 實驗 )32(0R  5A 實驗 )32(0R 5A 實驗 )32(0R 5A 實驗 )32(0R  5A 實驗
上升趨勢 0.7352 0.8446 0.0000 0.0000 0.0001 0.0002 0.0109 0.0197 0.0000 0.0000
下降趨勢 0.0000 0.0000 0.7350 0.8445 0.0000 0.0003 0.0000 0.0000 0.0096 0.0183
週期樣式 0.0000 0.0000 0.0000 0.0000 0.9448 0.9454 0.0001 0.0003 0.0000 0.0004
向上偏移 0.0043 0.0124 0.0000 0.0000 0.0015 0.0030 0.7248 0.7940 0.0000 0.0000
向下偏移 0.0000 0.0000 0.0047 0.0139 0.0013 0.0029 0.0000 0.0000 0.7241 0.7943
 
表 11 網路架構 II 使用 )32(0R 和 5A 實驗特徵值之正確辨識率 ( 8=p ， 15.0=α ) 
 網路辨識結果 
 上升趨勢 下降趨勢 週期樣式 向上偏移 向下偏移 
輸入樣式 )32(0R  5A 實驗 )32(0R  5A 實驗 )32(0R 5A 實驗 )32(0R 5A 實驗 )32(0R  5A 實驗
上升趨勢 0.8162 0.8842 0.0000 0.0000 0.0001 0.0003 0.0192 0.0298 0.0000 0.0000
下降趨勢 0.0000 0.0000 0.8167 0.8849 0.0001 0.0004 0.0000 0.0000 0.0178 0.0279
週期樣式 0.0000 0.0000 0.0000 0.0001 0.9560 0.9562 0.0003 0.0005 0.0000 0.0008
向上偏移 0.0102 0.0212 0.0000 0.0000 0.0023 0.0045 0.7962 0.8360 0.0000 0.0000
向下偏移 0.0000 0.0000 0.0112 0.0230 0.0022 0.0043 0.0000 0.0000 0.7966 0.8369
 
表 12 網路架構 II 使用 )32(0R 和 5A 實驗特徵值之正確辨識率 ( 8=p ， 2.0=α ) 
 網路辨識結果 
 上升趨勢 下降趨勢 週期樣式 向上偏移 向下偏移 
輸入樣式 )32(0R  5A 實驗 )32(0R  5A 實驗 )32(0R 5A 實驗 )32(0R 5A 實驗 )32(0R  5A 實驗
上升趨勢 0.8543 0.9082 0.0000 0.0000 0.0002 0.0004 0.0264 0.0392 0.0000 0.0000
下降趨勢 0.0000 0.0000 0.8544 0.9089 0.0001 0.0004 0.0000 0.0000 0.0249 0.0380
週期樣式 0.0000 0.0001 0.0000 0.0001 0.9628 0.9635 0.0007 0.0009 0.0000 0.0011
向上偏移 0.0164 0.0305 0.0000 0.0000 0.0032 0.0059 0.8318 0.8631 0.0000 0.0000
向下偏移 0.0000 0.0000 0.0182 0.0326 0.0029 0.0056 0.0000 0.0000 0.8324 0.8638
 
 
 2
參與此次國際研討會的國家人員，包括台灣、日本、韓國、大陸、伊朗、香港、
新加坡、泰國、印度…等十五國以上之品質人士，會中約有二百篇論文進行發表，
主要針對品質管理領域之應用相關主題共進行 48 場次的論文發表。研討會之議程
概述如下： 
 
2006/09/27上午：Registration, Opening of ANQ Congress 2006, Keynote Speech. 
    Keynote address by Dr. Noriaki Kano, Mr. Masamitsu Sakurai,  
        Dr. Yung-Ho Suh, Mr. N. Ramanathan. 
2006/09/27下午：Panel Discussion & Paper Presentation (Concurrent Session). 
2006/09/28全天：Panel Discussion & Paper Presentation (Concurrent Session), Farewell 
Dinner. 
2006/09/29上午：Industrial Visits. 
    1. Asia Pacific Breweries. 
    2. Systems on Silicon Manufacturing Company Ltd. 
    3. Singapore Technologies Electronics Ltd. 
    4. Philips Domestic Appliances. 
 
攜回資料名稱及內容： 
 1. 研討會議程及論文摘要 
 2. 研討會論文集光碟 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 4
2χ  control chart signals that a statistically significant shift in the mean has occurred, that is, it gives an 
out-of-control signal, as soon as 
UCL)()( 0
1
0
2 >−−= − μXΣμX kkk n 'χ  (1) 
where Σ  is a known covariance matrix. The upper limit on the control chart is UCL = 2,αχ p  where 2,αχ p  is 
the upper α100  percentage point of the 2χ  distribution with p degrees of freedom.  When the in-control mean 
vector and covariance matrix are unknown, they are often replaced by the sample estimator, X  and S , respectively. 
The test statistic for each sample that can be written as 
)()( 12 XXSXX −−= − kkk nT '  (2) 
In this form, the control procedure is usually called the Hotelling 2T control chart. 
Out-of-control signals in multivariate charts may be caused by one or more variables or a combination of 
variables. A disadvantage of multivariate control charts is that it is difficult to determine which of the p  variables 
(or which subset of them) is responsible for the signal. The standard practice is to plot univariate charts on the 
individual variables pXXX ,....,, 21 . However, this approach may not be successful.  
One useful approach to diagnosis of an out-of-control signal is to decompose the statistic into components that 
reflect the contribution of each individual variable.  This procedure consists of decomposing the 2T  statistic into 
independent parts, each of which is similar to an individual 2T  variate. Decomposition of a significant 2T  value 
detected in a multivariate control chart can provide a major source of information since the particular variable or 
group of variables contributing significantly to the signal can be identified. For more details, please refer to Mason et 
al. (1995), Runger et al. (1996), Mason and Young (1999). 
Some researchers have started to apply artificial neural networks (ANN) in statistical process control areas. 
Artificial neural networks have been successfully applied to univariate statistical process control. For an in-depth 
review of the applications of neural networks for process monitoring, the reader is referred to Zorriassatine and 
Tannock (1998). 
In the last few years, many researchers have investigated the application of artificial neural networks to 
multivariate statistical process control. Wang and Chen (2002) developed a neural-fuzzy model not only to identify 
mean shifts but also to classify their magnitudes. Chen and Wang (2004) also proposed a neural network approach 
for monitoring and classifying the mean shifts of multivariate process. Low et al. (2003) presented a neural network 
procedure for detecting variance shifts in a multivariate process. Zorriassatine et al. (2003) applied a neural network 
classification technique known as novelty detection to monitor multivariate process mean and variance. 
In recent years, the support vector machine (SVM) has been introduced as a new technique for solving a variety 
of learning, classification and prediction problems. There is also some research on using support vector machines to 
monitor process variation (Chinnam, 2002; Sun and Tsung, 2003). 
In this paper, a novel approach to identifying the sources of variability in multivariate process is presented. We 
formulate the interpretation of out-of-control signal as a classification problem. The proposed system includes a shift 
detector and a classifier. The traditional 2T  chart works as a mean shift detector. When an out-of-control signal is 
generated, a classifier will determine which variable is responsible for the mean shift. We consider two classifiers 
based on ANN and SVM. The proposed approach will be demonstrated by a bivariate process with sample size 
n =5. 
 6
nodes, and output layer with two nodes. The determination of the number of nodes in each layer is described as 
follows.  
In the proposed methodology, the number of components in a feature vector will determine the number of input 
nodes. The number of nodes in the output layer is determined by the number of different classifications desired and 
the output representation scheme. There are a number of ways to encode the multi-class classification. In the current 
research, it is feasible to use p  output nodes to represent the possible shifts in p -variate process. The expected 
output of the i th node is set to 1 if a mean shift has occurred in the i th variable.  
There is no clear-cut method to determine the optimal number of nodes required in the hidden layer. The 
number of nodes in the hidden layer is highly problem-dependent. From our experience of many experiments, the 
networks with too few hidden nodes tended to become less sensitive to small process changes. On the other hand, 
there was little improvement in performance with too many hidden nodes. The number of hidden layer nodes was 
chosen as 9, after experimentation with different network size. 
Back-propagation algorithm works with any differentiable transfer function. The most widely used is the 
sigmoid function with output values ranging from 0 to 1. It is defined as )1/(1)( xexf −+= , where the variable x  is 
the net input of a certain processing elements. Another transfer function is the hyperbolic tangent function with 
output values in the range -1 to 1. The hyperbolic tangent function is given by )/()()( xxxx eeeexf −− +−= . The output 
signals are all non-negative values, therefore, the sigmoid function was chosen in this study. 
 
2.3 Training Data 
 
Due to extensive demands for training data, Monte Carlo simulation was used to generate training data. A 
p -dimensional multivariate normal process was simulated by generating pseudo-random variates from a 
multivariate normal distribution whose mean was μ  and whose covariance matrix was a pp ×  matrix. For 
simplicity, it is assumed that the covariance matrix Σ  is known. In practice it will be necessary to collect data over 
a substantial amount of time when the process is in control to estimate Σ . By manipulating the value of μ , an 
in-control ( μ =0) or an out-of-control process ( μ ≠ 0) can then be simulated. 
The simulation was implemented using Minitab (2004) software. For simplicity, all variables were scaled such 
that each process variable has mean zero and variance one. With this approach, the covariance matrix is in correlation 
form; that is, the main diagonal elements are all one and the off-diagonal elements are the pairwise correlation ( ρ ) 
between the process variables.  
The proposed classifier is used to determine which variable is shifted once an out-of-control signal has been 
issued. In this research, the traditional 2T  chart will work as a mean shift detector. Each selected input vector must 
exceed the UCL of the 2T  chart. It is well known that the performance of 2T  chart depends on the value of μ  
and Σ  only through the values of the noncentrality parameter λ , where 
)()()( 0
1
0 μμΣμμμ
' −−= −λ  (4) 
The value of )(μλ  is often used to represent a measure of the distance of μ  from 0μ . This measure of 
distance is also called the “Mahalanobis distance” and the “statistical distance”. The shift size will be reported in 
terms of λ . Basically, large values of λ  correspond to bigger shifts in the mean. The value λ =0 is the in-control 
 8
3. SVM-Based Classifier 
 
3.1 Support Vector Machines 
 
The support vector machine method is a new and promising classification and regression technique. It has 
shown good generalization ability. The SVM is a powerful machine learning tool that is capable of representing 
non-linear relationships and producing models that generalize well to unseen data.  
A classification task usually involves with training and testing data which consists of some data instances. Each 
instance in the training set contains one target value (class label) and several attributes (features). The goal of a 
classifier is to produce a model which predicts target value of data instances in the testing set which are given only 
the attributes. 
SVM has been widely applied to build classifiers in many applications. The basic idea of applying SVM for 
solving classification problems can be stated briefly in two steps. First, SVM transforms the input space to a higher 
dimensional feature space through a non-linear mapping function. Second, it constructs the separating hyperplane 
with maximum distance from the closest points of the training set. It has been shown that maximizing the margin of 
separation improves the generalization ability of the resulting classifier. A simple description of the SVM is provided 
here, for more details please refer to Burges (1998). 
Consider the problem of separating the training set Niii y 1)},{( =x  with N  training vectors, where di R∈x  is the 
i th d -dimensional input vector (independent variables), }1,1{ +−∈iy  is known target. The training of SVM 
involves the solution of the following quadratic optimization problem: 
0
1))((subject to
2
1min
1
≥
−≥+
+ ∑
=
i
ii
T
i
N
i
i
T
by
C
ξ
ξφ
ξ
xw
ww
 (5) 
where C  is the penalty factor, w  is the vector of hyperplane coefficients, b  is a bias term and iξ  are 
parameters (errors) for handling nonseparable data. The index i  labels the N  training cases. The mapping 
function φ  is a non-linear transformation to map the input vectors into a high-dimensional feature space. Note that 
the variables iξ  are introduced to allow for some classification errors. The parameter C  determines the degree of 
penalty assigned to an error. It can be viewed as a tuning parameter which can be used to control the trade-off 
between maximizing the margin and the classification error. It should be noted that the larger the C , the more the 
error is penalized. Thus, C  should be chosen with care to avoid over-fitting. The basic concept of SVM can be 
thought of as a balance between the term )2/1( wwT  and the training errors. 
Constructing a separating hyperplane in the feature space leads to a non-linear decision boundary in the input 
space. Expensive calculation of dot products in a high-dimensional space can be avoided by introducing a kernel 
function satisfying )()(),( jijiK xxxx φφ= . The kernel function allows all necessary computations to be performed 
directly in the input space. Some popular kernel functions are defined in equation (6). 
 10
each with a different subset as the test set and with the union of the other nine subsets as the training set. The 
cross-validation was carried out in two stages. In the first stage, a search was made for an estimate of the penalty 
factor C  and the kernel parameter γ  that achieves the highest classification accuracy. In the second phase of 
training, the estimated values of these two parameters were used to train an SVM model using the entire training 
samples. Subsequently, this set of parameters was applied to the test dataset. Values for γ and C were checked in 
the [0.01, 20] and [2, 500] ranges, respectively. Different parameter settings will be used for data with different ρ  
values. 
The training of the SVM classifier used the training dataset described in Section 2.3. In this work, SVM was 
implemented in STATISTICA software (STATISTICA, 2003). The original data were mapped linearly onto the [-1,1] 
range. In this application the number of class k  is set to 4, i.e., }4 ,3 ,2 ,1{∈iy . 
 
4. Results and Discussion 
 
4.1 Comparison of Performance 
 
The comparisons of classification accuracy with different methods are presented in this section.  The results 
are reported for =ρ 0.1, 0.3, 0.5, 0.7 and 0.9. The performance of the decomposition method implemented in 
Minitab (2004) is used as a benchmark. For each )(μλ , 6000 testing samples were generated in the same way as the 
generation of training samples. Classification rates were approximated based on 6000 simulation runs at each change 
magnitude. The same test dataset was used to estimate the performance of ANN, SVM and decomposition method.  
In this application, the desired output of neural network is either 0 for non-shifted case or 1 for shifted case. Due 
to random noises, the output of neural network is a real number falling in the range of [0,1], not exactly of 0 or 1. 
Hence, an activation cutoff value, ca , is defined to determine if the network's output matches the target value. The 
neural network signals if the output is greater than ca . During application, the value of ca will be adjusted in order 
to match the required classification accuracy for 0=λ . 
Tables 2-4 provide the confusion matrices of classification results for various methods with varying values of 
ρ . The rows in the confusion matrix represent the desired results (i.e. shift type), while columns indicate the output 
as recognized by classifier. Entries (in boldface) along the diagonal indicate correct detection; while off-diagonal 
entries would indicate that the mean shift had been misclassified as the wrong type of class. The training accuracy 
and testing accuracy are recorded for ANN-based classifier and SVM-based classifier. It is also note that the 
significance level for the decomposition technique was set at 0.05. 
It is clear that there is strong agreement between the training results and the testing results, indicating no 
over-fitting problems with ANNs or SVMs. From these tables, we can observe that the method of ANN and SVM 
have similar classification performance. It is worth noting that ANNs and SVMs provide the same performance for 
shifts of the form [ 1δ ,0] or [0, 2δ ]. The decomposition method has the lowest average classification accuracy. For 
small ρ , the decomposition method performs better in classifying shifts of the form [ 1δ ,0] or [0, 2δ ] than shifts of 
the form [ 1δ , 2δ ] . The decomposition method is unable to classify shifts of the form [ 1δ ,0] or [0, 2δ ] . For all three 
methods, shifts of the form [ 1δ , 2δ ] are difficult to classify. This occurs when the shift size of one variable is much 
 12
Table 3. Confusion matrix for ANN 
  Actual ρ  Desired (0,0) (1,0) (0,1) (1,1) 
(0,0) 98.83/98.82 00.67/00.64 00.50/00.53 00.00/00.02 
(1,0) 00.46/00.49 89.49/89.53 00.24/00.23 09.82/08.74 
(0,1) 00.24/00.32 00.21/00.25 88.69/88.99 10.86/10.44 
0.1 
(1,1) 00.01/00.00 15.96/16.36 15.46/16.07 68.50/67.57 
(0,0) 98.76/98.73 00.71/00.69 00.53/00.57 00.00/00.01 
(1,0) 00.49/00.56 90.22/89.98 00.11/00.10 09.18/09.35 
(0,1) 00.33/00.30 00.07/00.11 91.38/91.12 08.22/08.47 
0.3 
(1,1) 00.01/00.01 14.93/14.85 15.11/15.45 69.94/69.69 
(0,0) 98.60/98.66 00.76/00.68 00.61/00.64 00.03/00.02 
(1,0) 00.43/00.35 90.38/90.97 00.04/00.04 09.15/08.64 
(0,1) 00.26/00.26 00.04/00.05 90.75/91.01 08.94/08.68 
0.5 
(1,1) 00.01/00.00 13.69/13.73 13.22/13.86 73.07/72.41 
(0,0) 98.33/98.51 00.82/00.69 00.75/00.72 00.10/00.08 
(1,0) 00.22/00.25 89.85/89.27 00.03/00.01 09.90/10.47 
(0,1) 00.14/00.19 00.00/00.01 90.15/90.51 09.71/09.29 
0.7 
(1,1) 00.00/00.00 11.44/11.23 11.60/11.23 76.96/76.54 
(0,0) 96.20/96.28 01.46/01.46 01.72/01.70 00.61/00.56 
(1,0) 00.07/00.09 90.71/90.70 00.00/00.01 09.22/09.21 
(0,1) 00.11/00.13 00.00/00.01 90.32/90.95 09.57/08.92 
0.9 
(1,1) 00.00/00.00 11.28/10.96 11.50/11.49 77.22/77.56 
(a/b) denotes (training/test) 
 
Table 4. Confusion matrix for SVM 
  Actual ρ  Desired (0,0) (1,0) (0,1) (1,1) 
(0,0) 98.86/98.82 00.53/00.53 00.50/00.53 00.11/00.12 
(1,0) 00.36/00.38 90.63/90.63 00.21/00.19 08.81/08.81 
(0,1) 00.38/00.39 00.19/00.22 90.43/90.39 09.00/09.00 
0.1 
(1,1) 00.01/00.00 17.21/17.40 17.32/17.37 65.47/65.23 
(0,0) 98.72/98.73 00.54/00.54 00.46/00.49 00.27/00.24 
(1,0) 00.29/00.29 92.11/91.96 00.13/00.09 07.47/07.66 
(0,1) 00.31/00.31 00.07/00.10 91.97/91.96 07.65/07.63 
0.3 
(1,1) 00.01/00.01 16.63/16.36 16,17/16.39 67.19/67.24 
(0,0) 98.83/98.66 00.42/00.46 00.39/00.46 00.36/00.43 
(1,0) 00.17/00.13 92.74/93.17 00.04/00.04 07.06/06.66 
(0,1) 00.17/00.16 00.04/00.05 92.63/92.91 07.17/06.89 
0.5 
(1,1) 00.01/00.00 16.08/16.16 15.06/15.88 68.85/67.96 
(0,0) 98.33/98.51 00.54/00.42 00.43/00.43 00.69/00.64 
(1,0) 00.07/00.05 93.39/93.18 00.03/00.01 06.51/06.76 
(0,1) 00.08/00.16 00.03/00.00 93.36/93.54 06.55/06.41 
0.7 
(1,1) 00.00/00.00 14.74/14.65 14.43/15.06 70.83/70.29 
(0,0) 96.19/96.27 00.49/00.49 00.49/00.46 02.83/02.78 
(1,0) 00.06/00.07 94.06/94.21 00.00/00.01 05.89/05.71 
(0,1) 00.07/00.05 00.00/00.01 93.93/94.23 06.00/05.71 
0.9 
(1,1) 00.00/00.00 14.53/14.34 14.46/14.30 71.01/71.36 
(a/b) denotes (training/test) 
