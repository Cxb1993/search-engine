- 2 - 
進行競爭；當 Couple 一起合作找出有共識的字來描述影像時，Blocker 的任
務就是搶先提出 Couple 可能達成共識的字來阻擋他們。因為這個設計，
Blocker必須盡全力來找到 Couple最可能使用的字，同時 Couple也會盡全力
來找到新奇有創意但又能夠達到共識的字來描述同一張影像，使得此遊戲的
結果同時具有高正確性及高分散性 (diversity)。我們在 Amazon Mechanical 
Turk 平台上實驗的結果，證明 KKB 是個好玩而且而產出高品質 image tags
的人際運算遊戲。因為此論文的品質，在ACM Human Computation Workshop 
2009發表後，即刻被邀稿收錄於一年發行四次的 ACM SIGKDD Explorations
雜誌。 
Selected Publication #2: On Formal Models for Social Verification [7] 
在這篇論文中，我們為人際運算系統中兩種最基本的人際驗證  (social 
verification) 機制提出正規模型。我們利用遊戲理論 (game theory) 來證明兩
種機制各自會達到的平衡點。我們的分析結果證明，與並行驗證 (sequential 
verification) 比較，循序驗證  (sequential verification) 可得到比較分散 
(diverse) 且描述性較高的結果，但是其結果正確性通常較並行驗證為低。我
們在 Amazon MechanicalTurk平台上進行實驗，讓使用者輸入與一個名詞相
關的同義詞，結果符合我們的遊戲理論分析。我們相信此文的分析結果將為
未來的人際運算系統設計的基石之一。 
Selected Publication #3: A Crowdsourceable QoE Evaluation Framework for 
Multimedia Content [2,8] 
此論文探討如何有系統有效率地量測使用者滿意度。傳統上，研究者皆使用
MOS (Mean Opinion Score) 來量測使用者對於多媒體內容品質的滿意度；也
就是說，請受試者對於特定的多媒體內容品質來打分數 (例如 1分至 5分) 再
取平均值來做為該多媒體內容的品質。但是，以絕對分數來評量滿意度一件
傷神耗力的工作，請受試者遠方而來進行實驗十分耗費資源（酬勞、車馬費
及行政支援），且有時間（配合受試者的作息時間）、硬體（電腦及實驗設備
- 4 - 
Kuan-Ta Chen, "KissKissBan: A Competitive Human Computation Game for 
Image Annotation," Proceedings of ACM Human Computation Workshop 
2009, 2009. 
7. Chien-Ju Ho and Kuan-Ta Chen, "On Formal Models for Social Verification," 
Proceedings of ACM Human Computation Workshop 2009, 2009. 
8. Kuan-Ta Chen and Chen-Chi Wu and Yu-Chun Chang and Chin-Laung Lei, 
"A Crowdsourceable QoE Evaluation Framework for Multimedia Content," 
Proceedings of ACM Multimedia 2009, 2009. 
 
以下茲將此八篇已發表著作列為附件。 
所有已發表論文也可於網站上開放公開下載，網址為： 
1. http://www.iis.sinica.edu.tw/~swc 
2. http://mmnet.iis.sinica.edu.tw/publication.html 
additional player, the blocker, whose objective is to stop the
matching from happening. By entering labels prior to the
matching process, the blocker provides a blocked word list.
Other players, the couples, would get penalties for guess-
ing the blocked words. In contrast to the taboo words, the
blocked words are not visible to the couples. While the
blocker is in competition with the other two players, he/she
is motivated to break the coalitions between the couples.
Therefore, KKB naturally provides a player-level cheating-
proof mechanism. Besides, the invisible blocked words set
the restrictions of the available words, thus the couples are
encouraged to provide more diverse labels to evade the re-
strictions.
This paper starts by briefly reviewing human computa-
tion and the image naming process in a psychological view.
We then introduce the design and implementation of KKB.
Finally, we explain the evaluation results and outline the
contribution and future work of this research.
2. RELATED WORK
In this section, we introduce the recent developments in
human computation games and give a brief overview on pic-
ture naming process in psychology.
2.1 Human Computation Games
Human computation aims to solve problems that are hard
for computers by utilizing human brain powers. For exam-
ple, Amazon Mechanical Turk1 provides a marketplace for
the developer to outsource human intelligence tasks. Hu-
man computation games, also called Games With A Pur-
pose (GWAP) [1], propose that using computer games can
gather human players and solve open problems as a side ef-
fect of playing. The GWAP approach has been shown to
be useful and is widely used in various domains, such as
image tagging [5, 6], commonsense collection [2], and music
annotation [3].
To ensure the quality of the collected labels, most GWAP
implementations adopt consensus opinions as the correct-
ness measure. Taking the ESP Game as an example, im-
age labels are generated by collecting descriptions which
both players agree in the game. In [7], Luis von Ahn pro-
posed three game-templates, which summaries their success-
ful experiences in deploying GWAPs. The three templates,
namely input-agreement games, output-agreement games,
and inversion-problem games, rely on player’s collaborations
to collect consensus opinions. In contrast to previous work,
we demonstrate the game design integrating competitive el-
ements.
2.2 Picture Naming
In the ESP Game [5], players perform the process of giv-
ing descriptions to the images. This process usually involves
naming objects in the image and is well studied as picture-
naming process. In psychology, an important measure in
picture-naming process is how fast a person can name a
given picture correctly. The naming latency is determined
by two main factors, namely the word frequency and age of
acquisition (AoA) [8]. Word frequency is the times of occur-
rence in large word corpus, and age of acquisition is the age
at which player learned the word. Typically, players name
faster when the words are learned earlier or have higher fre-
1https://www.mturk.com/mturk/
quency. While there are discussions about which factor is
more important [9, 10], we know that the word properties,
i.e. the word frequency and AoA, affect the player efforts
and the responsive time in naming the pictures.
In our game design, we adopt two time intervals, 7 seconds
and 30 seconds, in different game stages. Investigating the
differences between the image labels collected in two stages
may provide additional information besides the matching
events.
3. GAME MECHANISMS
KissKissBan (KKB) is designed to be played by three on-
line players. One of the players is the “blocker” and the
other two players are the “couples”. With the same image
presented, the couples try to match (Kiss) with each other
by typing the same word and the blocker tries to stop cou-
ples from matching (Ban). Actually, KissKissBan is named
by combining the objectives of game players.
The game rules are described as follows. In the begining
of each round, the blocker has 7 seconds to provide blocked
word list, which is the list of words he/she thinks couples
might match on. These blocked words are not visible to
the couples. After the 7 seconds of entering blocked words,
the couples have 30 seconds to match with each other. The
game time will decrease by 5 seconds if any couple types
the blocked word, i.e., being blocked. Also, agreeing on the
blocked word does not count as matching. The couples win
the round if they successfully match with each other within
the time limit, otherwise the blocker wins. Players switch
roles every 5 rounds in 15 rounds of the game.
3.1 Different Roles
There are two different roles in KKB:
• Blocker: Each player will play the blocker for 5 rounds
in the 15 rounds of the game. Though the blocker only
has 7 seconds to act in each round, he/she is able to
see every word the couples are typing during the game.
Monitoring the actions of the couples not only makes
the waiting process fun, but provides the blocker an
opportunity to stop the couples from achieving some
unified strategy. For example, the blocker could give
“a” as the blocked word if he/she founds the couples
try to match on “a” in every round.
• Couple: The objective of the couples is the same as the
players in the ESP Game: to guess what the partner is
typing. However, unlike the players in the ESP Game,
the couples in KKB cannot see what the blocked words
are. Therefore, the couples are encouraged to guess
harder words to avoid guessing the word in the blocked
words list.
3.2 Incentive Structure
KKB is a three-player zero sum game. In the current im-
plementation, the blocker loses 200 points and each couple
gains 100 points when the couples win; the blocker gains
200 points and each couple loses 100 points when the cou-
ples lose. While the blocker and the couples are strictly
competitive, the blocker is motivated to prevent the cou-
ples from cheating. Therefore, KKB provides a player-level
cheating-proof mechanism.
Another difference between the blocker and the couples is
their available time period for entering words. In order to
The quality of the labels is evaluated using the descrip-
tions provided by the ESP Game Dataset. In average, there
are 13.89 descriptions per image for the images we used.
Taking these descriptions as “partial” ground truth for im-
age labeling, the precision of our collected labels is 78.84%
and the recall is 70.02%. The precision is the ratio of our
labels to be correct. This result demonstrates the data qual-
ity of our collected labels. Actually, the correct ratio should
be much higher since we only use “partial” ground truth for
evaluation. The recall of 70.02% shows the diversity of KKB
labels.
4.2 Property of Collected Labels
To evaluate the data properties of collected labels, we re-
implemented the ESP Game for comparison. We will call
this re-implemented version as K-ESP. To simplify the com-
parison, there is no taboo word in K-ESP. After publishing
on Amazon Mechanical Turk, K-ESP has been played for
5977 rounds, and 4994 labels are collected.
In 4994 labels generated by K-ESP, 6.56 distinct labels
are collected for each image, whereas KKB has collected
11.54 distinct labels per image out of 5521 labels in total.
The data entropy calculated from K-ESP is 4.79, while the
data entropy in KKB is 7.18. These statistics suggest that
KKB motivates players to give more diverse labels. Table 1
gives an example of different labels generated by KKB and
K-ESP.
K-ESP ML-KKB BL-KKB
man * 21 beach * 3 sea * 9
beach * 10 water * 3 man * 8
karate * 5 sand * 3 ocean * 3
water * 1 sea * 2 black * 1
ninja * 1 china * 1
kungfu * 1 sand * 1
ocean * 1
Table 1: Labels produced by KKB and K-ESP. ML-KKB
is the list of the matching label produced by matching be-
tween couples, and BL-KKB is the list of blocking label by
matching blocker and one of the couples.
4.3 Gameplay Survey
To evaluate the gameplay, we conduct a questionnaire sur-
vey involving 17 participants, consisting of 11 males and 6
females. The participants are all college students, whose
ages are ranging from 21 to 25. Before answering the ques-
tions, they are required to play the game at least 2 times,
i.e. 30 rounds. Some of them have played the game over 10
times.
In the anonymous survey, the average enjoyability rating
is 3.76 out of 5, and 88 percent of the subjects claim that
they will play the game again. In addition, over 60% of the
players think it’s more fun and challenging to play as the
blocker.
5. CONCLUSION
We have presented KissKissBan, a competitive human
computation game for image annotation. While the ESP
Game and other GWAPs present using player collabora-
tions for collecting useful information, the main contribu-
tion of this paper is to demonstrate the integration of com-
petitive elements into human computation games. In the
design of KKB, we have two advantages over traditional hu-
man computation games: 1) KKB provides a player-level
cheating-proof mechanism which can alleviate coalition be-
tween players; 2) KKB motivates players to contribute more
diverse labeling and therefore collects a broader set of data.
Evaluations on Amazon Mechanical Turk and a gameplay
survey have shown KKB to be an efficient and fun game for
collecting diverse image annotations.
In our future work, we will explore how players behave
when they play as different game roles. The differences
between KKB labels, namely the matching labels and the
blocking labels, is also an important subject to investigate.
Acknowledgement
This work was supported in part by Taiwan E-learning and
Digital Archives Programs (TELDAP) sponsored by the Na-
tional Science Council of Taiwan under the grants NSC98-
2631-001-011 and NSC98-2631-001-013. It was also sup-
ported in part by the National Science Council of Taiwan
under the grants NSC97-2221-E-001-009.
6. REFERENCES
[1] Luis von Ahn. Games with a purpose. IEEE Computer
Magazine, 39(6):92–94, 2006.
[2] Luis von Ahn, Mihir Kedia, and Manuel Blum.
Verbosity: a game for collecting common-sense facts.
In CHI ’06: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 75–78.
ACM Press, 2006.
[3] Edith L. M. Law, Luis von Ahn, Roger B.
Dannenberg, and Mike Crawford. Tagatune: A game
for music and sound annotation. In International
Conference on Music Information Retrieval (ISMIR’
07), pages 361–364, 2003.
[4] Ingmar Weber, Stephen Robertson, and Milan
Vojnovic. Rethinking the ESP game. Technical report,
Microsoft Research, September 2008.
[5] Luis von Ahn and Laura Dabbish. Labeling images
with a computer game. In CHI ’04: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM Press, 2004.
[6] Luis von Ahn, Ruoran Liu, and Manuel Blum.
Peekaboom: a game for locating objects in images. In
CHI ’06: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 55–64.
ACM Press, 2006.
[7] Luis von Ahn and Laura Dabbish. Designing games
with a purpose. Commun. ACM, 51(8):58–67, 2008.
[8] J.B. Carroll and M.N. White. Word frequency and age
of acquisition as determiners of picture-naming
latency. The Quarterly Journal of Experimental
Psychology, 25(1):85–95, 1973.
[9] C.M. Morrison, A.W. Ellis, and P.T. Quinlan. Age of
acquisition, not word frequency, affects object naming,
not object recognition. Memory and Cognition,
20:705–705, 1992.
[10] C. Barry, K.W. Hirsh, R.A. Johnston, and C.L.
Williams. Age of acquisition, word frequency, and the
locus of repetition priming of picture naming. Journal
of Memory and Language, 44(3):350–375, 2001.
2not, however, be confused with QoS (Quality of Service), which refers to an objective system
performance metric, such as the bandwidth, latency, or packet loss rate of a communication
network.
Methods assessing QoE are conventionally classified as either subjective or objective. Sub-
jective methods, in particular Absolute Category Rating [2], directly ask human-beings to rate
their experience with some received media, also known as stimuli, on a categorical scale, the
most adopted of which being the Mean Opinion Score (MOS). A single human rating in a MOS
test is expressed as one of the ratings from 1 to 5. The numbers are also given names: Bad
for 1, Poor for 2, followed Fair, Good, and Excellent. The MOS for a certain stimulus is then
the arithmetic mean of individual ratings. The obvious drawback of subjective methods is the
personnel cost and time, especially if evaluation has to be repeatedly conducted in iterative and
incremental system development. In response, objective methods estimate QoE by analyzing
the delivered content automatedly, for example by looking for unnatural noise appearing in
a compressed audio clip. Unfortunately, no matter how sophisticated objective methods are,
intrinsically they cannot capture all dimensions of QoE. PESQ, for instance, gives inaccurate
predictions when used in conjunction with factors like sidetone, listening levels, loudness loss,
talker echo, and effect of delays in VoIP conversation. External factors, such as the production
quality of headsets (in acoustic QoE assessments) or the distance between viewer and display
(in visual QoE assessments), are not considered by objective methods because they are hard to
measure and quantify. Conventional subjective and objective approaches to assessing QoE remain
more complements than replacements of each other. Subjective experiments are still called for
to help develop mathematical models and authenticate results obtained from objective analyses
despite their lavishness.
Nonetheless, the subjective methodology is not without pitfalls. It may be burdensome for
one to map his own sensation onto the MOS scale, and the fact that it is concomitantly numeral
and nominal does not help, either. As a matter of fact, the literature has identified at least two
other problems with MOS:
1) Scale heterogeneity [3]. The options on the MOS scale are not something readily de-
fined and explained. Consequently, each subject may interpret the scale according to his
idiosyncratic preference and strategy. Some may tend to give higher ratings while others
give below average ones even if they share similar experience toward the same stimulus.
4are
(
n
2
)
possible pairs, each with two stimuli semantically equivalent at every second except for
their presentation quality. In our design, each pair corresponds to a decision or judgment to be
made by the subject. The
(
n
2
)
judgments in turn constitute one of the subject’s many runs of
a QoE experiment. (We do encourage participants to perform an experiment multiple times.)
The results of all runs of an experiment can be collectively summarized in a frequency matrix
resembling
T1 T2 T3 T4
T1 – a12 a13 a14
T2 a21 – a23 a24
T3 a31 a32 – a34
T4 a41 a42 a43 –
where T1, T2, ..., Tn are the n prepared stimuli (n = 4 in the above matrix) and aij denotes the
number of runs (not participants) where Ti is preferred to Tj . The total number of runs is of
course aij + aji.
The Bradley-Terry-Luce (BTL) model [5] states that the probability of choosing Ti over Tj ,
aij
aij+aji
, is a function associated with the “true” ratings of the two stimuli:
aij
aij + aji
=
pi(Ti)
pi(Ti) + pi(Tj)
=
eu(Ti)−u(Tj)
1 + eu(Ti)−u(Tj)
, (1)
from which we obtain u(Ti) = log pi(Ti) by maximum likelihood estimation. The numerals
u(Ti), i = 1, 2, ..., n are comparable with each other on an interval scale and are thus chosen as
the raw estimates of QoE score for T1, T2, ..., Tn, respectively. u(Ti) is negative since pi(Ti) is a
positive real number smaller than 1. To facilitate interpretation, we further shift and normalize
the raw estimates to within [0, 1], where the stimulus with the best QoE scores 1 and the one
with the worst scores 0.
A QOE ASSESSMENT PLATFORM
We beef up our methodological reform of QoE experiments with the proposition of an
assessment platform powered by paired comparison. We name it Quadrant of Euphoria as a
backronym of QoE. Such a platform is not complete without addressing the cost issue aforesaid.
Recent technology advances have made available ubiquitous Internet access and rich Internet
applications, giving rise to a generation of more participative and self-aware end-users, the
6may give erroneous feedback and still receive payment therefor. Such results are products of a
careless, perfunctory attitude, or more perturbingly, of dishonesty and malign conduct. Whatever
the reason is, erroneous ratings do increase the variances of QoE scores and lead to biased
conclusions.
Given a handful of results turned in by anonymous subjects, it is difficult, if not impossible,
for an experimenter to sift the wheat from the chaff. One may argue that we can compensate
problematic inputs by amassing more experiment runs than necessary, but the approach is valid
only if ill-natured users occupy a small portion of the crowd. However, since they may choose
random answers by ignoring instructions and effectively earn more than honest participants, they
are motivated to run (and sabotage) an experiment as many times as they can. We are in dire
need of a countermeasure to finally establish a theoretically consummate platform. Problematic
inputs must be pruned. Reward and punishment rules can also ensue to encourage a high caliber
of participation and thwart potentially uninterested subjects.
Ensuring Subject Consistency
For our purposes, it is asserted that preference is a transitive relation; that is, if some participant
prefers A to B and B to C, than he will normally prefer A to C. In light of this, we define
the Transitivity Satisfaction Rate (TSR) to be the number of judgment triplets (e.g., the three
preference relations between A, B, and C) satisfying transitivity divided by the total number of
triplets where transitivity may apply. The TSR is the quantification of a participant’s consistency
throughout a run of an experiment. A rule of thumb is that a fully attended subject can attain a
TSR higher than 0.8 without difficulty.
We suggest that experiment administrators posit before the tests specific requirements of a
paying result and possibly a warning to rogue participants. In our demonstrative studies, for
example, only the results with TSRs of 0.8 and above were rewarded, and never once did we
receive complaints.
In addition, we argue that there is no systematic way to cheat our system. The fact that
the order in which the pairs appear and the correspondence between key states and stimuli are
completely randomized and unavailable to outsiders contributes to this assertion. The only ways
a participant can achieve high TSR and get paid are to give sound and honest answers and,
though easily dismissed, to consistently make wrong judgments.
8analysis. In contrast, an Internet user may browse on the lower pane through the catalogue of
open experiments on our website and find the ones that interest him to participate. The catalogue
informs the user of the experiments’ name, type (Image, Audio, or Video as currently supported),
description, and payment level.
To Set Up and Conduct an Experiment
Experiments are maintained on Quadrant of Euphoria as profiles. The procedure by which
a researcher sets up a profile and conducts the experiment is shown in Figure 2. Stimuli must
be prepared beforehand and uploaded upon profile registration. Once registration is successful,
the researcher is given the hosted experiment URL, which he is free to publish to any Internet
community to gather the subject crowd. If monetary reward is involved, a micro-payment platform
such as MTurk is recommended. The researcher now awaits the crowd to perform the experiment.
Subjects may receive unique verification codes for complete and qualified results and use them
to prove to the researcher their eligibility for payment. Issuance of the codes can be set during
profile registration. Finally, the researcher decides on paying whom how much and collects data
for further analysis.
Experiment Interface
When a subject enters an experiment Web page hosted on Quadrant of Euphoria, he sees an
Adobe Flash application with a large upper pane (Figure 3) and immediately begins the first
paired comparison. Depending on the context of the experiment, the participant will be able to
view an image, hear an audio clip, or watch some visual content displayed on the upper pane.
Upon pressing and holding the spacebar, the (objective) quality of the content changes. Releasing
the spacebar restores the original quality at the start of the comparison. The subject then makes a
judgment on which spacebar state (pressed or released) corresponds to better (perceived) quality
by mouse-clicking one of the buttons beneath the display pane or by pressing left or right arrow
keys. The next paired comparison commences right after the decision, or the experiment ends
after all
(
n
2
)
comparable pairs are exhausted.
What the subject hears or sees is actually a dynamic interweaving of a pair of stimuli. One
stimulus in the pair is played out first. If the participant presses and holds the spacebar at,
say, the fifth second, the other stimulus will take over seamlessly and start playing from its
10
(a) Acoustic QoE assessment: spacebar released. (b) Acoustic QoE assessment: spacebar pressed.
Fig. 3: The experiment interface as seen by participants under both spacebar states.
Application Quality
Space Key
Released
SPACE KEY
User Action
VoteQuality with Space Key Released is Better
Space Key
Pressed
Space Key
Released
Space Key
Released
Space Key
Pressed
Ready to Make a Decision
Time
Quality with Space Key 
Pressed is Better
Fig. 4: The decision flow of a subject in an acoustic paired comparison.
Administration Interface
A profile is identified by its name, which along with a password is required when a researcher
logs in to manage his experiment. When registering a profile (Figure 1b), the researcher pro-
vides URLs of the stimuli and an e-mail address for password recovery. He can also set up
a TSR threshold to fend off disqualified results, and opt to show unique verification codes on
qualified ones. All but the profile name and the backup e-mail address are modifiable afterwards
(Figure 1c).
The logs of the experiment are zipped and available for download on the profile management
page. We also bundle in the ZIP archive the source code to infer QoE scores and draw diagrams
like Figures 5 and 6. The result of each participant makes up a text file with the name
datetime_ipaddr_profile_sname_vcode.txt,
where sname and ipaddr are respectively the name and IP address of the participant, datetime
12
1% 5% 8%
Frame copy
Loss rate
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Laboratory
MTurk
Community
1% 5% 8%
Frame copy w/ frame skip
Loss rate
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Laboratory
MTurk
Community
Fig. 6: QoE scores of a video clip repaired with two loss concealment schemes at various packet
loss rates.
1, 545 comparisons were performed by 62 subjects, including 10 for the laboratory, 15 from
MTurk, and 37 from the other Internet community. The inferred QoE scores of the six recordings
in Figure 5, properly normalized so that they are cross-comparable, exhibit general agreement
among the three settings. They also conform with our expectations that 1) higher loss rates lead
to lower QoE scores, and 2) G.722.1, operating at higher bitrates than G.728, is dominantly
more robust even with higher loss rates. Also, the graph manifests that subjects from different
participant sources reveal statistical equivalent perceptions in terms of their preferences for audio
codecs operating at different bitrates and loss rates.
Comparison of IPTV Loss Concealment Schemes
A benchmark video clip “Cheerleaders” was encoded with JM [10], the H.264/AVC reference
software, and again put into a Gilbert-Elliott channel to simulate packet loss events at rates 1%,
5%, and 8%. The three resulting streams were then decoded back and repaired with two loss
concealment schemes: frame copy (FC) and frame copy with frame skip (FCFS). FC hides errors
in a video frame by replacing a corrupted block with one at corresponding positions in previous
frames. FCFS works exactly as FC until the percentage of corrupted blocks in a frame exceeds
a certain threshold (10% in our experiments), then it simply drops that frame.
The inferred QoE scores in Figure 6 are properly normalized so that those of the same clip are
comparable. While it is unsurprising that QoE scores are negatively correlated to loss rates, we
note that there is no significant resultant discrepancy between the two loss concealment schemes.
14
bitrates. Due to the article’s length limit, we are obliged not to describe those studies in detail
but do include their statistics in Table I to give a hologram of how crowdsourcing jostles against
laboratory experiments.
Cost. We hired part-timers for laboratory experiments with an hourly pay of $8. They were
asked to perform the tests repeatedly within work hours. We also announced the recruitment on
the MTurk website and another Internet community with 1.5 million users. Only qualified results
(TSR ≥ 0.8) were rewarded for MTurk and the community. The compensations were 15¢ and
virtual currencies worth 1¢ respectively.
We estimate that laboratory experiments consumed 86% of our budget while producing only
43% of the judgments (4, 875 out of 11, 295). The cost per judgment was 3¢ on average, a lot
more expensive than that measured for MTurk (1¢) and community (0.07¢).
Quality. We define the Qualified Rate as the ratio of results in an experiment that yield a
TSR higher than 0.8. The Qualified Rates observed are usually around 60%–70%. The laboratory
experiments achieved the highest Qualified Rates in all cases except in the VoIP quality study.
Moreover, in the study of loss concealment schemes, the laboratory setting boasted a 69%
Qualified Rate, almost twice as high as those attained by both crowdsourcing strategies. Such
polarized statistics indicate that the quality of the video stimuli in this experiment are more
difficult to be differentiated. We attribute the superiority of laboratory subsjects in this case to
their proficiency acquired during the course of the experiment, since on average each one of them
made 115 paired comparisons, as opposed to a merely 18 by their crowdsourced counterparts.
Despite the sometimes dismal Qualified Rate, crowdsourcing can still produce results of the same
standard as laboratory runs after the removal of disqualified submissions. That our consistency
assurance is ticking is evident in the Avg. TSR column of Table I, where an unanimous 0.96 is
reached or surpassed.
Participant diversity. In addition to evaluating quality and cost aspects, we also emphasize
the diversity of participants in QoE-assessing experiments. Since the purpose of these studies
is to understand human perception of certain stimuli, e.g., multimedia content, a subject set as
diverse as possible enables us to collect broader opinions and infer more real QoE scores. From
this perspective, crowdsourcing is especially suitable for assessing QoE as it greatly increases
the participant diversity. In our experiments, crowdsourcing strategies contributed 97% to a total
of 298 subjects while costing only $24.31 or 14% of the budget therein.
16
• Integrated micro-payment mechanism;
• User facilities like a search box for open experiments or personalized participation tracking;
• Multi-dimensional consistency quantification superior to TSR;
• A “neutral” or “indifferent” option for paired comparison and subsequent model changes.
The ultimate goal, of course, is to render the free, open-access Quadrant of Euphoria as much
assisting to the research community as we are capable of.
ACKNOWLEDGEMENT
This work was supported in part by Taiwan E-learning and Digital Archives Programs (TEL-
DAP) sponsored by the National Science Council of Taiwan under the grants NSC98-2631-001-
011 and NSC98-2631-001-013. It was also supported in part by the National Science Council
of Taiwan under the grants NSC98-2221-E-001-017.
REFERENCES
[1] ITU-T Recommendation P.10/G.100/Amd.2, “New definitions for inclusion in
recommendation p.10/g.100,” 2008.
[2] ITU-T Recommendation P.910, “Subjective video quality assessment methods for
multimedia applications,” 2008.
[3] P. Rossi, Z. Gilula, and G. Allenby, “Overcoming scale usage heterogeneity: A bayesian
hierarchical approach,” Journal of the American Statistical Association, vol. 96, no. 453,
pp. 20–31, 2001.
[4] A. Watson and M. A. Sasse, “Measuring perceived quality of speech and video in
multimedia conferencing applications,” in Proceedings of ACM Multimedia 1998, 1998,
pp. 55–60.
[5] R. D. Luce, Individual Choice Behavior: A Theoretical Analysis. New York: Wiley,
1959.
[6] D. C. Brabham, “Crowdsourcing the public participation process for planning projects,”
Planning Theory, vol. 8, no. 3, pp. 242–262, 2009.
[7] R. Allio, “CEO interview: the InnoCentive model of open innovation,” Strategy &
Leadership, vol. 32, no. 4, pp. 4–9, 2004.
CONCURRENCY AND COMPUTATION: PRACTICE AND EXPERIENCE
Concurrency Computat.: Pract. Exper. 2009; 00:01–0 Prepared using cpeauth.cls [Version: 2002/09/19 v2.02]
The Design of Puzzle Selection
Strategies for GWAP Systems
Ling-Jyh Chen∗ , Bo-Chun Wang, and Kuan-Ta Chen
Institute of Information Science, Academia Sinica
128, Sec. 2, Academia Road, Taipei 11529, Taiwan
SUMMARY
The Games With A Purpose (GWAP) genre is a type of Human Computation that outsources certain
steps of the computational process to humans. By taking advantage of people’s desire to be entertained,
GWAP attracts people to play voluntarily, and also produce useful metadata as a by-product. The games
have shown promise in solving a variety of problems, which computer computation has been unable to
resolve completely thus far. In this paper, we propose a metric, called system gain, for evaluating the
performance of GWAP systems, and also use analysis to study the properties of GWAP systems. We argue
that it is important for GWAP systems to implement proper puzzle selection strategies in order to collect
human intelligence in a more efficient manner. Therefore, based on our analysis, we implement an Optimal
Puzzle Selection Strategy (OPSA) to improve GWAP systems. Using a comprehensive set of simulations, we
demonstrate that the proposed OPSA approach can effectively improve the system gain of GWAP systems,
as long as the number of puzzles in the system is sufficiently large.
KEY WORDS: Games With A Purpose; Human Computation; Collaborative Tagging
1. Introduction
In the last two decades, the Internet has undergone rapid growth in terms of its usage, population,
geographic distribution, and applications. Recent surveys of world-wide Internet usage reported that, in
2008, there were more than 100 million Facebook users [6], 258 million registered YouTube users [6],
and more than 16 million active subscriptions to Massively Multiplayer Online Games (MMOGs) [5].
The figures for Facebook and YouTube represent annual growth rates of 305% and 94%, respectively,
over the previous year. It is evident that Internet users today want to socialize and be entertained, in
addition to exploiting traditional applications, such as the WWW, FTP, and Email.
Among numerous emerging Internet applications, Games With A Purpose (GWAP) represents a
new paradigm that exploits people’s desire to be entertained by outsourcing certain steps of the
∗Correspondence to: Institute of Information Science, Academia Sinica, Taiwan
Contract/grant sponsor: National Science Council, Taiwan; contract/grant number: NSC 98-2631-H-001-013 and NSC 97-2631-
S-003-003
Copyright c© 2009 John Wiley & Sons, Ltd.
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 3
computation cannot currently resolve completely. In recent years, a substantial and increasing amount
of research effort has been invested in the area, and several GWAP systems have been developed for a
variety of purposes [1, 3, 4, 7–9, 11, 13, 16, 17, 19, 20, 23, 26, 28–31, 33].
Among them, the online ESP Game [26] was the first GWAP system (which was subsequently
adopted as the Google Image Labeler [1]); and it has been shown that the collected labels facilitate
more accurate image retrieval, help users block inappropriate images, and improve web accessibility.
In addition, the Peekaboom system [31] can help determine the location of objects in images; and the
Squigl system [2] and the LabelMe system [20] can provide the complete outlines of the objects in
an image. Phetch [28, 29] can provide image descriptions that improve web accessibility and image
searches, while the Matchin system [2] can help image search engines rank images based on which ones
look the best. The concept of the ESP Game has been applied to other problems. For instance, the Herd
It [3, 7], Major Miner [4], and TagATune [16] systems, which provide annotation for sounds and music,
can improve audio searches. The Verbosity system [30] and the Common Consensus system [17] collect
“common-sense” knowledge that is valuable for commonsense reasoning and enhancing the design of
interactive user interfaces. In [9], Bennett et al. collect the individual user preferences over image-
search results, and extract the consensus rankings from the preferences for the results of a query. The
Context-Aware Recognition Survey (CARS) system [33] uses ubiquitous sensors to monitor activities
in the home, while [8, 11, 13, 19] employ mobile social gaming for geospatial tagging. Moreover, [23]
applies human computation to ontology alignment and web content annotation for the Semantic Web
using a set of games, such as OntoPronto, SpotTheLinks, OntoTube, and OntoBay. Finally, Shenoy and
Tan [22] showed that it is possible to design environments in which humans cannot avoid processing
some of the tasks (and producing some useful outcomes), even though they are not actively trying to
do so.
In addition to designing new GWAP systems, several studies have investigated the performance
aspect of human computation recently [12, 14, 15, 27, 32]. For example, Ho et al. [14] proposed
to solve the coalition problem by integrating both collaboration and competition elements in image
labeling games. Gentry et al. [12] proposed a framework of vote-based human computation and gave
a probabilistic analysis of the reliability of the voting mechanism and design principles on the payout
function. Moreover, in [32], Weber et al. presented a machine learning-based model that can play
with the ESP game without looking at the image. Based on the model, they proposed an enhanced
scoring system for the ESP game to encourage users to contribute less predicable labels and therefore
improve the quality of the collected labels. Jain and Parkes [15] presented game theoretic analysis for
the ESP game, and they investigated the equilibrium behavior under different incentive mechanisms
and provided guidelines to design incentive mechanisms. Von Ahn and Dabbish [27] proposed a
set of evaluation metrics, such as throughput, lifetime play, and expected contribution, to determine
whether ESP-like GWAP systems are successful. Moreover, they proposed that the GWAP system has
to introduce challenges, competition, variety, and communication in the system in order to provide
incentive and strengthen a player’s motivation to stay in the game.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 5
3.3. The Inversion-problem Game: Verbosity
The Verbosity Game [30] is a popular inversion-problem game that employs human computation to
collect commonsense reasoning related to a word by asking one of the players to guess the input puzzle
(i.e., a word) based on the other player’s description of the puzzle. Like the ESP and TagATune games,
two players are selected at random to create a game; once again, they do not know each other. The
players take turns to play the roles of “Describer” and “Guesser”. The Describer is asked to provide
a number of words to describe the given input, and the Guesser has to guess the input based on the
Describer’s outputs.
To make the game easier, Verbosity provides the Describer with a set of sentence templates to
describe the input puzzle (e.g., “it looks like ” and “it is a type of ”). The Describer can
see all of the Guesser’s inputs, so he can adjust his playing strategy during the game round. Since
both players have to collaborate to win the game, the Describer must do his best to help the Guesser
guess the input. Since the game structure encourages players to enter correct information, the collected
descriptions are typically good enough to be used as official tags of the corresponding puzzles.
The inversion-problem game can be regarded as a special case of the input-agreement game, except
that (1) the number of outcomes per game round is limited (i.e., there are at most m outcomes in each
game round, and m is much smaller than the total number of possible outcomes); and (2) a game round
cannot be a failure in inversion-problem games (i.e., the two players will pass over a puzzle if they
cannot complete it using the first m descriptions). For instance, in the Verbosity game, there are only 6
sentence templates (i.e., m = 6).
4. Game Analysis
For the sake of efficiency, the GWAP system tries to collect human intelligence with the largest possible
aggregated outcomes in the shortest possible time, for each puzzle. There is a trade-off between these
two factors. On the one hand, to minimize the time required per puzzle, the system prefers that each
puzzle is played only once. The rationale is to maximize the number of games that players try to solve.
On the other hand, the system prefers to take as many outcomes as possible for each puzzle, which
results in the playing of fewer distinct puzzles. Thus, an optimal puzzle selection strategy that can
accommodate the two goals is highly desirable. To this end, we propose a metric to evaluate the system
gain of the GWAP system, and analyze the puzzle selection problem. We discuss the analysis in the
following subsections.
4.1. The Output-agreement Game: ESP
Let N be the number of puzzles that have been played at least once in the system, T be the average
time consumed per puzzle, and S be the total score of all the agreed labels. We define the system gain,
G, of the ESP game as follows:
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 7
4.2. The Input-agreement Game: TagATune
Similar to the analysis in subsection 4.1, let N be the number of puzzles that have been played at least
once in the system, T be the average time required to solve each puzzle, and O be the total score of all
the game outcomes. We define the system gain, G, of the TagATune game as follows:
G = ln(C1 × N
NT
)× ln(C2 × O
N
)
= ln(C1 × 1
T
)× ln(C2 × E[O]),
(7)
where C1 and C2 are two scaling constants that ensure ln(C1 × 1T ) > 0 and ln(C2 × E[O]) > 0
respectively, and E[O] is the expected total score of the game outcomes for each puzzle.
Clearly, the system gain increases as the average time required per puzzle (i.e., T ) decreases, and/or
as the average total score per puzzle (i.e., E[O]) increases. Suppose that each puzzle in the system
has the potential to yield a total of K outcomes, each of which is associated with one positive score
based on the outcome’s popularity. In addition, suppose the score of the k-th outcome is vk, and the
probability that the k-th outcome will be output is pk. For simplicity, we assume there are totally X
distinct scores (i.e., s1, s2, s3, ..., sX ) in the system, and that si = ei. We also assume thatKi outcomes
have the score si, and that Ki = eX−i (i.e., the total number of potential outcomes per puzzle (K)
derived by Equation 2. Then, we can obtain the expected total score of the new outcomes (per puzzle)
in the r-th game round, E[Or], by Equation 8, and the expected total score of all the outcomes after
the first r game rounds, E[O], by Equation 9.
E[Or] = (1− p1)r−1 × p1 × v1 + (1− p2)r−1 × p2 × v2 + . . .+ (1− pK)r−1 × pK × vK
=
K∑
k=1
(1− pk)r−1 × pk × vk.
(8)
E[O] =
r∑
i=1
E[Oi]
=
r∑
i=1
K∑
k=1
(1− pk)i−1 × pk × vk
=
K∑
k=1
pk × vk ×
r∑
i=1
(1− pk)i−1
=
K∑
k=1
pk × vk × 1− (1− pk)
r
1− (1− pk)
=
K∑
k=1
vk(1− (1− pk)r).
(9)
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 9
1. There are no game round failures in an inversion-problem game (i.e., Pf = 0). Thus, we can
rewrite Equation 10 as
t = ts +
Pp
Ps
× tp + Pt
Ps
× tt. (14)
2. The number of possible outcomes per game round is limited (i.e., m is limited); however,
Equations 8 and 9 still hold.
Therefore, similar to input-agreement games, we can obtain the system gain by
G = ln(C1 × N
NT
)× ln(C2 × O
N
)
≈ −
(
ln(r)− ln(
C1
t
)− ln(C2K)
2
)2
+ C,
(15)
where C is fixed with a value equal to ln(C1
t
)ln(C2K) +
(
ln(
C1
t
)−ln(C2K)
2
)2
. Note that C also
represents the largest possible system gain, which occurs when
r = e
ln(
C1
t
)−ln(C2K)
2 . (16)
5. Game Strategies
In this section, we present three puzzle selection algorithms for the GWAP system, namely the
Random Puzzle Selection Algorithm (RPSA), the Fresh-first Puzzle Selection Algorithm (FPSA), and
the proposed Optimal Puzzle Selection Algorithm (OPSA)†. We take the RPSA scheme’s performance
as the baseline (in terms of system gain). The heuristics-based FPSA scheme tries to maximize the first
component of Equations 1, 7, and 15 (i.e., minimize the value of T ); and the OPSA scheme tries to
achieve the largest possible system gain based on our analysis (as discussed in Sec. 4).
We use P to denote the set of all puzzles in the system, and we define the following three functions
used by the puzzle selection algorithms: 1) Select Random(P ), which randomly selects a puzzle from
the input puzzle set P ; 2) Select P layed(P ), which selects the puzzle from the input puzzle set P that
has been played most frequently; and 3) Select Fresh(P ), which selects the puzzle from the input
puzzle set P that has been played least frequently. We discuss the three algorithms in the following.
†Note that, in this paper, the presented OPSA strategy is designed to maximize the proposed system gain in GWAP systems (cf.
Equations 6, 13, and 16), and it may not always yield the optimal performance of the GWAP system if we consider additional
factors in the system gain metric, such as positive, negative, and zero outcomes.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 11
Algorithm 3 The Optimal Puzzle Selection Algorithm (OPSA).
1: Function OPSA
2: if {P1} is NOT empty then
3: p⇐ Select P layed(P1)
4: p.r ⇐ p.r + 1
5: if p.r = r then
6: Move p from P1 to P2
7: end if
8: Return p
9: else
10: if {P0} is NOT empty then
11: p⇐ Select Random(P0)
12: p.r ⇐ 1
13: if p.r < r then
14: Move p from P0 to P1
15: else
16: Move p from P0 to P2
17: end if
18: Return p
19: else
20: p⇐ Select Fresh(P2)
21: p.r ⇐ 1
22: Return p
23: end if
24: end if
parameters (Ps, Pp, Pt) to (0.6, 0.3, 0.1)¶. All the results are based on the average performance of 100
simulations.
6.1.1. The Optimal r
In the first set of simulations, we evaluated our analytical model’s accuracy in determining an optimal
r value for the ESP game. We assumed that the number of puzzles in the system was infinite, and that
all of them were unsolved at the beginning of the simulation (i.e., no labels were discovered for any
puzzles). Figure 1 shows the evaluation results in terms of the system gain for r values between 2 and
100, when the maximum score value X was fixed at 6. In the figure, the analysis curve is derived by
Equation 5, where E[s] can be obtained by Equation 3. The optimal r value is equal to 10.3353 when
puzzle, i.e., ts > tt. We also note that, the selection of the time scale used for ts, tp, and tt can be any time units (e.g., hours,
minutes, and seconds), as long as it satisfies the criteria that ln(C1 × 1T ) = ln(C1 × 1r×t ) > 0.¶It is important to ensure that Ps > Pp; otherwise, the players may feel frustrated when playing the game. Moreover, Pt must
be very small so that the players have sufficient time to solve the puzzles in order to increase the system efficiency.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 13
 0
 2
 4
 6
 8
 10
 12
 14
 2  3  4  5  6  7  8  9  10
r
X
Analysis
Simulation
Figure 2. Comparison of the optimal r values derived by simulations and analysis, where X varies between 2 and
10.
 1
 10
 100
 1000
 2  3  4  5  6  7  8  9  10
r 
a
n
d 
K
X
r
K
Figure 3. The relationship between the values of r and K in the logarithmic scale for various X values.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 15
 0
 5
 10
 15
 20
 0  1000  2000  3000  4000  5000
Sy
st
em
 G
ai
n
M
RPSA
FPSA
OPSA
Figure 5. Comparison of the system gain achieved by the OPSA, FPSA, and RPSA schemes with various numbers
of puzzles, where X is set to 6 and the system gain is calculated after 10,000 agreements are reached in each
simulation run.
6.2. Input-agreement Games and Inversion-problem Games
Next, we discuss the simulations performed to investigate the inner properties of the TagATune game.
We also evaluate the system gain of the three puzzle selection strategies. For simplicity, we set the
values of the two scaling constants C1 and C2 to 200 and 1 respectively∗∗. Moreover, we set the values
of the parameters (ts, tf , tp, tt) to (0.2, 0.2, 0.4, 0.15)††, and the parameters (Ps, Pf , Pp, Pt) to (0.5,
0.3, 0.1, 0.1)‡‡. All the results are based on the average performance of 100 simulations.
6.2.1. The Optimal r
In the first set of simulations, we evaluated the accuracy of our analytical model in determining an
optimal r value for the TagATune game. We assumed that the number of puzzles in the system was
infinite, and all of them were unsolved at the beginning of the simulation (i.e., no labels were discovered
∗∗The selection of the values of the two scaling constants depends on how we weight the two factors in the performance metric.
The values of C1 and C2 must satisfy the criteria that ln(C1 × 1T ) > 0 and ln(C2 × E[O]) > 0, as described in Equation 7.††Intuitively, tp > ts, since the players tend to pass a puzzle when they realize it will be difficult to reach an agreement; and
ts = tf , since they are both equal to the average time required for the players to reach an agreement. Moreover, the players
may be forced to terminate a game round simply because they do not have enough time to finish the puzzle, i.e., ts > tt. Note
that, the selection of the time scale used for ts, tp, and tt can be any time units (e.g., hours, minutes, and seconds), as long as it
satisfies the criteria that ln(C1 × 1T ) = ln(C1 × 1r×t ) > 0.‡‡It is important to ensure that Ps > Pp, otherwise the players may feel frustrated when playing the game. Moreover, Pt must
be very small so that the players have sufficient time to solve the puzzles in order to increase the system efficiency.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 17
 0
 0.2
 0.4
 0.6
 0.8
 1
 0 0.2
 0.4 0.6
 0.8 1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
r
Ps
Pp
Figure 7. The relationships between the values of r and different values of Ps and Pp, whereX = 3, Pp = 0, and
Pt = 1− Ps − Pf .
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 0  1000  2000  3000  4000  5000
Sy
st
em
 G
ai
n
M
RPSA
FPSA
OPSA
Figure 8. Comparison of the system gain achieved by the OPSA, FPSA, and RPSA schemes with various numbers
of puzzles, where X is set to 3 and the system gain is calculated after 5,000 agreements are reached in each
simulation run.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
PUZZLE SELECTION STRATEGIES FOR GWAP SYSTEMS 19
and the proposed puzzle selection strategy shows promise for use in the design and implementation of
future GWAP systems.
ACKNOWLEDGEMENT
We are grateful to the editors and anonymous reviewers for their insightful comments. This research was supported
in part by Taiwan E-learning and Digital Archives Programs (TELDAP) sponsored by the National Science
Council of Taiwan under NSC Grants: NSC 98-2631-H-001-013 and NSC 97-2631-S-003-003.
references
[1] Google Image Labeler. http://images.google.com/imagelabeler/.
[2] GWAP. http://www.gwap.com/gwap/.
[3] Herd It. http://www.herdit.org/.
[4] Major Miner. http://majorminer.org/.
[5] MMOGCHART. http://www.mmogchart.com/.
[6] TechCrunch. http://www.techcrunch.com/.
[7] L. Barrington, D. Turnbull, D. O’Malley, and G. Lanckriet. Herd It: Designing A Social Game to Tag Music.
In Human Compuation Workshop, 2009.
[8] M. Bell, S. Reeves, B. Brown, S. Sherwood, D. MacMillan, J. Ferguson, and M. Chalmers. Eyespy:
Supporting navigation through play. In ACM SIGCHI, 2009.
[9] P. N. Bennett, D. Maxwell, and A. Mityagin. Learning Consensus Opinion: Mining Data from a Labeling
Game. In WWW, 2009.
[10] J. P. Bigham, R. S. Kaminsky, R. E. Ladner, O. M. Danielsson, and G. L. Hempton. WebInSight: making
web images accessible. In ACM SIGACCESS, 2006.
[11] S. Casey, B. Kirman, and D. Rowland. The gopher game: a social, mobile, locative game with user generated
content and peer review. In International Conference on Advances in Computer Entertainment Technology,
2007.
[12] C. Gentry, Z. Ramzan, and S. Stubblebine. Secure distributed human computation. In ACM Electronic
Commerce Conference, 2005.
[13] L. Grant, H. Daanen, S. Benford, A. Hampshire, A. Drozd, and C. Greenhalgh. MobiMissions: the game of
missions for mobile phones. In ACM SIGGRAPH, 2007.
[14] C.-J. Ho, T.-H. Chang, J.-C. Lee, J. Y.-J. Hsu, and K.-T. Chen. KissKissBan: A Competitive Human
Computation Game for Image Annotation. In Human Computation Workshop, 2009.
[15] S. Jain and D. C. Parkes. A Game-Theoretic Analysis of Games with a Purpose. In Workshop on Internet
and Network Economics, 2008.
[16] E. Law and L. von Ahn. Input-agreement: A new mechanism for data collection using human computation
games. In ACM SIGCHI, 2009.
Copyright c© 2009 John Wiley & Sons, Ltd. Concurrency Computat.: Pract. Exper. 2009; 00:1–0
Prepared using cpeauth.cls
DevilTyper: A Game for CAPTCHA Usability Evaluation
Chien-Ju Ho1, Chen-Chi Wu2, Kuan-Ta Chen1, Chin-Luang Lei2
1Institute of Information Science, Academia Sinica
2Department of Electrical Engineering, National Taiwan University
ABSTRACT
CAPTCHA is an eﬀective and widely used solution for pre-
venting computer programs (i.e., bots) from performing au-
tomated but often malicious actions, such as registering
thousands of free email accounts or posting advertisement
on web blogs. To make CAPTCHAs robust to automatic
character recognition techniques, the text in the tests are
often distorted, blurred, and obscure. At the same time,
those robust tests may prevent genuine users from telling
the text easily and thus distribute the cost of crime preven-
tion among all the users. Thus, we are facing a dilemma,
that is, a CAPTCHA should be robust enough so that it
cannot be broken by programs, but also needs to be easy
enough so that users need not to repeatedly take tests be-
cause of wrong guesses.
In this paper, we attempt to resolve the dilemma by propos-
ing a human computation game for quantifying the usabil-
ity of CAPTCHAs. In our game, DevilTyper, players try
to defeat the devils as many as possible by solving CAPT-
CHAs, and player behavior in completing a CAPTCHA are
recorded at the same time. Therefore, we can evaluate
CAPTCHAs’ usability by analyzing collected player inputs.
Since DevilTyper provides entertainment purposes itself, we
conduct a large-scale study for CAPTCHAs’ usability with-
out the resource overhead required by traditional survey-
based studies. In addition, we propose a consistent and re-
liable metric for assessing usability. Our evaluation results
show that DevilTyper provides a fun and eﬃcient platform
for CAPTCHA designers to assess their CAPTCHA usabil-
ity and thus improve the CAPTCHA design.
Categories and Subject Descriptors
H.1.2 [Models and Principles]: User/Machine Systems—
Human factors; K.8.0 [Personal Computing]: General—
Games; H.5.2 [Information Interfaces And Presenta-
tion]: User Interfaces—User-centered design
General Terms
Design, Human Factors, Security
Keywords
Games with a purpose, GWAP, Human computation, Us-
ability, Optical character recognition, Human perception
1. INTRODUCTION
Preventing computer programs from performing automated
malicious tasks, such as registering thousands of free email
accounts, has been one of the most challenging tasks of sys-
tem administrators. For this issue, CAPTCHA (Completely
Automated Public Turing test to tell Computer and Humans
Apart) [12] is known as an eﬀective and widely used solution.
Although there are many types of CAPTCHAs [5, 8], text-
based CAPTCHA scheme, which asks users to recognize the
distorted text, is the most widely used and is adopted by
most commercial services, such as Google, Yahoo!, and Mi-
crosoft. For this reason, we focus on text-based CAPTCHA
schemes in this work. Text-based CAPTCHAs are machine-
generated images which contain obscure text. The common
procedures to generate such images often include distortions,
overlapping, clipping, and noise addition. These procedures
are performed to make image recognition algorithms unable
to resolve the text in the images. However, the distortion of
the text should be controlled to a reasonable level so that
human can still tell the text clearly.
As many CAPTCHAs have been broken by OCR (Optical
Character Recognition) or other recognition techniques [1,
4, 9], it is necessary to enhance the complexity of CAPT-
CHAs to make them robust enough against such attacks.
However, some enhancement procedures make the CAPT-
CHAs too diﬃcult to be recognized by human, e.g., with
too noisy background or too much text distortion. There-
fore, we need to seek the balance in the tradeoﬀ between
the human usability and the computational recognition chal-
lenge when designing or employing a CAPTCHA test. Since
there have been a great deal of works discussing the com-
putational recognition diﬃculty of such tests in the ﬁeld of
pattern recognition, we focus on quantifying the usability of
CAPTCHAs in this work.
The most intuitive way to assess the usability of CAPT-
CHAs is to ask numerous human subjects to solve assigned
CAPTCHAs repeatedly. However, such surveys are cost-
prohibitive if a large-scale study is required and the inves-
tigated CAPTCHAs are constantly updating. For exam-
ple, investigating how diﬀerent background noises aﬀect the
Figure 2: The screenshot of the game. The devils
move from the upside to the downside. Players will
lost life points if the devils reach the bottom of the
game screen.
also increases. Furthermore, each level is comprised of 10
mini-missions, each of which introduces diﬀerent numbers of
devils that the player needs to destroy to ﬁnish the mission.
As a player completes a mission, the number of devils he
needs to destroy will increase in the next mission.
3.2.2 During game
As shown in Figure 2, devils move from the upside to
the downside, and if a devil is not defeated by the player,
it will keep moving down and get oﬀ of the screen, which
indicate that the player get hurt by the devil. To target
a devil, the player must type the ﬁrst letter of its corre-
sponding CAPTCHA. Once the player targets a devil, he
is supposed to complete the remainder of the corresponding
CAPTCHA to defeat it; otherwise, the player can release
the target by pressing the SPACE key if he is unable to rec-
ognize the letters or want to destroy other devils ﬁrst. In
the game, the player character possesses two important at-
tributes: the score and the HP (health points), where the HP
is represented by a rectangle life bar and should be higher
than zero anytime to keep playing. If a devil reaches the
player character before being destroyed, the player is penal-
ized by decreasing the HP value. On the other hand, the
score accumulates if a devil is defeated. After defeating all
the devils in a mission, the current mission is completed and
the next mission will be started after a short time allowing
the player to take a breath.
3.2.3 After game
To motivate the involvement of players, the name and
score of players with high scores will be shown on the high
score list, as shown in Figure 3. From the perspective of
players, the ultimate goal of the game is to defeat as many
devils as possible and therefore achieve a high ranking and
score. Also, diﬀerent skill levels introduces diﬀerent levels of
challenge so that users would continue the play with higher
levels and feel accomplished. In [11], the authors mentioned
that game features such as skill levels, score keeping, and
high score list signiﬁcantly increase the joyfulness of game
Figure 3: The screenshot of the score board of Dev-
ilTyper.
play. As a result, we can achieve our study of CAPTCHAs’
usability while having players entertained through our game.
3.3 Implementation
To facilitate the game deployment, we implement Dev-
ilTyper using Adobe Flash. The game is now publicly avail-
able on the Internet1. Currently we provide six diﬀerent
CAPTCHA schemes and a plain-text CAPTCHA as a base-
line for readability evaluation. The word associates with
each devil is randomly selected from a dictionary and ren-
dered using a random CAPTCHA scheme. We log all play-
ers’ actions, including each key pressed by the player, the
time of keystrokes, the correctness of each keystroke, and
the aiming and release of a devil, to provide researchers for
statistical analysis.
3.4 Data Collection
In DevilTyper, we record all the actions performed by
players and therefore can derive various performance metrics
as follows:
• Finish time: the total time to solve a CAPTCHA.
• Rate of typing error: the number of CAPTCHAs with
wrong keystrokes divided by the number of all CAPT-
CHAs.
• Rate of timeout: the number of timeout CAPTCHAs
divided by the number of all CAPTCHAs. A CAPT-
CHA is regarded as “timeout” if it is targeted but the
player did not have any further action in 3 seconds.
• Rate of giving up: the number of CAPTCHAs given
up divided by the number of all CAPTCHAs. After
targeting a devil (and it’s corresponding CAPTCHA),
players can give up the devil by pressing SPACE key
before timeout.
• Rate of repeat typing: the number of repeat keystrokes
divided by the number of all keystrokes. In the game,
players may type the same character repeatedly to
avoid timeout.
1http://deviltyper.iis.sinica.edu.tw/
A B C D E F G
F
in
is
h 
tim
e
0
20
0
40
0
60
0
80
0
10
00
(a) Finish time
A B C D E F G
R
at
e 
of
 ty
pi
ng
 e
rr
or
0.
00
0.
05
0.
10
0.
15
0.
20
(b) Rate of typing error
A B C D E F G
R
at
e 
of
 ti
m
eo
ut
0.
00
0.
02
0.
04
0.
06
0.
08
0.
10
0.
12
0.
14
(c) Rate of timeout
A B C D E F G
R
at
e 
of
 g
iv
in
g 
up
0.
00
0.
01
0.
02
0.
03
0.
04
0.
05
0.
06
0.
07
(d) Rate of giving up
A B C D E F G
R
at
e 
of
 r
ep
ea
te
dl
ly
 ty
pi
ng
0.
00
0.
05
0.
10
0.
15
0.
20
(e) Rate of repeat typing
Figure 5: CAPTCHA comparisons. The CAPTCHAs are coded by A: AuthImage, B: Captcher, C: Kinravj,
D: SecurImage, E: Plain text, F: CoolCAPTCHA, and G: TgCAPTCHA.
in solving each CAPTCHA were recorded. The task was ac-
complished 500 times by 44 workers, and 7, 500 CAPTCHAs
solving tasks were performed.
A B C D E F G
DevilTyper Error Rate
MTurk Error Rate
MTurk Finish Time
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Figure 7: A comparison of CAPTCHA solving error
rates on Mechanical Turk and DevilTyper.
We compute the typing error rate and average ﬁnish time
for each CAPTCHA scheme and compare the results with
those obtained from DevilTyper. The normalized error rates
of CAPTCHA solving on Mechanical Turk and DevilTyper
are shown in Figure 7. From the graph, we can see that
the user performance are generally consistent no matter
the users are playing the DevilTyper game or performing
crowdsourcing tasks on Amazon Mechanical Turk. The re-
sults indicate that DevilTyper provides trustworthy user
performance data for CAPTCHA usability evaluation and
researchers can now use DevilTyper for such studies with a
much lower or even none monetary cost.
5. DESIGN FACTOR ANALYSIS
In this section, we analyze the eﬀect of various design
factors on the usability of CAPTCHAs. We ﬁrst investi-
gate whether the choice of characters aﬀects users’ perfor-
mance in solving CAPTCHAs. We then analyze the us-
ability of CAPTCHAs when two common text obscuration
procedures, distortion and noise, are applied.
5.1 Effect of Characters in CAPTCHA
Each CAPTCHA scheme has its own obscuration algo-
rithm to distort the text, which may have diﬀerent impacts
on the recognition diﬃculty of diﬀerent characters. For ex-
ample, as shown in Figure 8, “i” is hardly recognizable in
TgCAPTCHA since players may be confused to identify
whether it is a character or a random noise line. On the
other hand, in CoolCAPTCHA, “i” is much easier to rec-
ognize. Understanding the eﬀects of characters in diﬀerent
CAPTCHAs can help designers avoid the confusing charac-
ters and improve the CAPTCHA usability.
To evaluate the usability of characters with diﬀerent CAPT-
CHA schemes, we ﬁrst show how players react for characters
in plain text in Figure 9. As the graph shows, players have
diﬀerent typing error rate even though the words are not
distorted at all. This result may also be aﬀected by the de-
sign of the keyboard layout. To eliminate such factors, we
take users’ performance with the plain-text CAPTCHA as
the baseline and examine the relative performance with each
CAPTCHA scheme. Assuming the error rate for a character
c with plain text is Ec, and the error rate for character c
with CAPTCHA type X is E′c, then the performance degra-
dation rate is deﬁned by
E′c−Ec
Ec
. The relationship between
the performance degradation rate and CAPTCHA schemes
0.80 0.85 0.90 0.95 1.00 1.05 1.10
0.
00
0.
02
0.
04
0.
06
0.
08
0.
10
0.
12
X−Axis Wave
E
rr
or
 R
at
e
(a) Character Distance vs. Error rate.
0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2
0.
00
0.
02
0.
04
0.
06
0.
08
0.
10
0.
12
X−Axis Wave
E
rr
or
 R
at
e
(b) X-Axis Wave vs. Error rate
0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2
0.
00
0.
02
0.
04
0.
06
0.
08
0.
10
0.
12
X−Axis Wave
E
rr
or
 R
at
e
(c) Y-Axis Wave vs. Error rate
Figure 12: Users’ average error rates with CAPT-
CHAs rendered using diﬀerent distortion strategies.
(a) Long arcs noise
(b) Short arcs noise
(c) Short lines noise
Figure 13: TgCAPTCHA examples generated using
diﬀerent noise addition strategies.
Long Arcs
The long arcs parameter controls the number of long arcs
overlaid on the image, where the position, length, and cur-
vature of the arcs are randomly chosen. In our experiment,
we set this parameter between 0 and 5. From Figure 13(a),
we can see that the long arcs do not inﬂuence the usability
of the CAPTCHAs signiﬁcantly even when 5 long arcs were
added.
Short Arcs
Similar to long arcs, the short arcs parameter controls the
number of short arcs overlaid on the image. In our exper-
iment, the number of short arcs are randomly drawn from
the range 0 to 20. Interestingly, while long arcs do not im-
pact the CAPTCHA’s usability, short arcs do, as shown in
Figure 13(b). We believe it is due to the length of short arcs
are similar to that of the character strokes so that short arcs
are more likely to interfere with distorted text and increase
the diﬃculty of text recognition.
Short Lines
The short lines parameter controls the number of short lines
overlaid on the rendered CAPTCHA. As with long and short
arcs, the position, length, and direction of each segment
is randomly decided. Our results show that users’ average
error rates slightly but steadily increase with more short
lines, as shown in Figure 13(c). However, the impact of
short lines is slightly less than that of short arcs, which is
reasonable because arcs are more like the strokes of distorted
text and therefore more interference on readers’ recognition
is induced.
6. CONCLUSION
In this paper, we have proposed a human computation
game, DevilTyper, to facilitate the evaluation of CAPTCHA
usability. DevilTyper is fun to play—we had over 6, 500
Proceedings of Computer Vision and Pattern
Recognition Conference, pages 134–141, 2003.
[10] A. Rusu and V. Govindaraju. Handwritten
CAPTCHA: Using the diﬀerence in the abilities of
humans and machines in reading handwritten words.
In IWFHR ’04: Proceedings of the Ninth International
Workshop on Frontiers in Handwriting Recognition,
pages 226–231, 2004.
[11] L. von Ahn. Games with a purpose. Computer,
39(6):92–94, 2006.
[12] L. von Ahn, M. Blum, N. J. Hopper, and J. Langford.
CAPTCHA: Using hard ai problems for security. In In
Proceedings of Eurocrypt, pages 294–311, 2003.
[13] L. von Ahn and L. Dabbish. Labeling images with a
computer game. In CHI ’04: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326, 2004.
[14] L. von Ahn, M. Kedia, and M. Blum. Verbosity: a
game for collecting common-sense facts. In CHI ’06:
Proceedings of the SIGCHI conference on Human
Factors in computing systems, pages 75–78. ACM
Press, 2006.
[15] L. von Ahn, R. Liu, and M. Blum. Peekaboom: a
game for locating objects in images. In CHI ’06:
Proceedings of the SIGCHI conference on Human
Factors in computing systems, pages 55–64, New York,
NY, USA, 2006. ACM.
[16] S.-Y. Wang and J. L. Bentley. CAPTCHA challenge
tradeoﬀs: Familiarity of strings versus degradation of
images. In ICPR ’06: Proceedings of the 18th
International Conference on Pattern Recognition,
pages 164–167, 2006.
[17] J. Yan and A. S. El Ahmad. Usability of CAPTCHAs
or usability issues in CAPTCHA design. In SOUPS
’08: Proceedings of the 4th symposium on Usable
privacy and security, pages 44–52, New York, NY,
USA, 2008. ACM.
A Collusion-Resistant Automation Scheme for
Social Moderation Systems
Jing-Kai Lou†‡, Kuan-Ta Chen‡, and Chin-Laung Lei†
†Department of Electrical Engineering, National Taiwan University
‡Institute of Information Science, Academia Sinica
{kaeaura, ktchen}@iis.sinica.edu.tw, lei@cc.ee.ntu.edu.tw
Abstract—For current Web 2.0 services, manual examination
of user uploaded content is normally required to ensure its
legitimacy and appropriateness, which is a substantial burden
to service providers. To reduce labor costs and the delays caused
by content censoring, social moderation has been proposed as a
front-line mechanism, whereby user moderators are encouraged
to examine content before system moderation is required. Given
the immerse amount of new content added to the Web each
day, there is a need for automation schemes to facilitate rear
system moderation. This kind of mechanism is expected to
automatically summarize reports from user moderators and ban
misbehaving users or remove inappropriate content whenever
possible. However, the accuracy of such schemes may be reduced
by collusion attacks, where some work together to mislead the
automatic summarization in order to obtain shared benefits.
In this paper, we propose a collusion-resistant automation
scheme for social moderation systems. Because some user moder-
ators may collude and dishonestly claim that a user misbehaves,
our scheme detects whether an accusation from a user moderator
is fair or malicious based on the structure of mutual accusations
of all users in the system. Through simulations we show that
collusion attacks are likely to succeed if an intuitive count-
based automation scheme is used. The proposed scheme, which is
based on the community structure of the user accusation graph,
achieves a decent performance in most scenarios.
Index Terms—Bad Mouthing, Collusion Detection, Fraud De-
tection, Social Moderation, Web 2.0
I. INTRODUCTION
The emergence of Web 2.0 not only provides individuals
with new ways to gain publicity by posting their own writings,
photos, and videos on the Internet, but also promotes interper-
sonal communications over the Internet. With the advent of
new-generation Web technologies, notably AJAX [5], users
are making substantial contributions to numerous Web 2.0
sites, e.g., Wikipedia, the cosmic compendium of knowledge;
YouTube, the million-channel video sharing site; and MyS-
pace, the online metropolis. Among these examples, YouTube
attracts about one billion page views everyday, which clearly
manifests the popularity of Web 2.0 sites. However, there is
a downside to this phenomenon. The core concept of Web
2.0 is that people have the freedom to participate and to
share their content on the Internet. While most people behave
responsibly, a minority of users may abuse the freedom and
upload inappropriate content. For example, they may post
pictures that violate copyright laws, upload splatter movies on
a video sharing site, or use bots [2] to gain unfair advantages in
an online game. Hence, proper content censorship is essential
for Web 2.0 services that allow users to upload their own
content.
Currently, most Web 2.0 sites employ system moderators,
i.e., they hire official moderators to verify the legitimacy of
content uploaded by users. However, system moderation is
labor-intensive because of the vast amount of user content such
that it may become a heavy burden to the service providers.
Take the album Flickr as an example. According to [1],
it receives 4, 320, 000 photos everyday on average. If each
moderator can handle 100 photos per hour and works 8 hours
per day, Flickr would need to hire 5, 400 moderators working
to check all the uploaded pictures.
System moderation is not the only means of ensuring the
appropriateness of users’ content. Social moderation has been
proposed to solve the content censorship problem [8]. Under
this approach, all users, or only selected users, can report
content they consider inappropriate so that system moderators
can inspect and evaluate it. In this way, social moderation
reduces service providers’ labor costs and speeds up con-
tent censorship because popular sites normally have many
more user moderators than system moderators. However, user-
assisted moderation does not solve the problem completely, as
system moderators may be overwhelmed by the sheer volume
of user reports. If we assume that only 1% of photos uploaded
to Flickr are problematic and reported by user moderators, the
system moderators would still need to handle approximately
43, 200 reports each day. Clearly, such system moderation
requirements would still place a huge burden on Web 2.0
service providers.
One way to further reduce the labor required for sys-
tem moderation is to eliminate manual inspection by official
moderators as much as possible. This is our motivation for
proposing social moderation automation, which automatically
summarizes the reports submitted by users.
We consider a subset of a social moderation system in which
user moderation only contributes one-way, negative votes. In
other words, users are allowed to label content as inappropriate
based on their own judgments, but they cannot oppose other
users’ accusations against certain content. We call this system
an accusation-based social moderation system. For brevity, we
refer to it as a social moderation system hereafter. Although
the system can be used to detect inappropriate content or
misbehaving users, we assume that the system’s goal is to
detect misbehaving users.
A count-based scheme is probably the most intuitive way
to automate an accusation-based social moderation system,
as it identifies misbehaving users by considering the number
of accusations. Under the scheme, a user is deemed to have
misbehaved if the number of accusations he/she receives is
greater than a certain threshold.
However, the above approach may be made ineffective by
collusion attacks. Collusion is a secret agreement made be-
tween two or more people for fraudulent or deceitful purposes,
978-1-4244-2309-5/09/$25.00 ©2009 IEEE
Ăď
Đ
Ě
Ğ
Ĩ
Ő
Ś
ŝ
ũ
Ŭ





&
'
,
/
:
<
ď
Đ


&
,
Ğ'
Ő
Ś
/
:
<


ŝ
ũ
Ŭ
Ĩ
Ă
Ě
'ϭ
'Ϯ
'ϯ
Fig. 1. After apply the Girvan-Newman algorithm, the accusing graph is
partitioned into three communities.
Property 1. It is unlikely that an inter-community edge is an
accusing edge between a colluder and a victim.
Colluders share similar malicious purposes to falsely accuse
other users; thus a group of colluders should share similar
accusing tendencies. As mentioned in Section III-B, users that
share similar accusing tendencies tend to belong to the same
community. Furthermore, colluders and their victims tend to
be part of the same community because of the accusation
relation between them. Therefore, it is unlikely that an inter-
community edge is an accusing edge between a colluder and
a victim.
Property 2. It is unlikely that an inter-community edge is an
accusing edge between a careless accuser and an unfortunate
user.
Even though a user is honest, the user may accuse another
user because of misjudgment or by accident. Due to the nature
of error votes, careless accusers and unfortunate users should
be uniformly distributed among all the system’s users. As they
are unlikely to form large connected components, they tend to
form small communities that are isolated from other nodes
in the accusing graph. In other words, careless accusers and
unfortunates users tend to belong to the same community;
therefore, it is unlikely that an inter-community edge is an
accusing edge between a careless accuser and an unfortunate
user.
Property 3. An inter-community edge is most likely an ac-
cusing edge between an honest accuser and a misbehaving
user.
An inter-community edge can be one of the following
accusing edges: 1) between a colluder and a victim, 2) between
a careless accuser and an unfortunate user, and 3) between an
honest accuser and a misbehaving user. Based on Property 1
and Property 2, we deduce Property 3; An inter-community
edge is most likely an accusing edge between an honest
accuser and a misbehaved user.
E. Identifying Misbehaving Users
According to Property 3, inter-community edges probably
represent fair accusations; therefore, we use the edges to iden-
tify real misbehaving users. To do so, we define two features
to characterize the possibility that a user has misbehaved based
on the inter-community edges connected to that user. One is
called incoming accusations, denoted by IA, and the other is
called outgoing accusations, denoted by OA.
To explain IA and OA, we first define the community
indegree of an accused user, and the community outdegree
of an accused user. After partitioning an accusing graph into
several communities such as G1, G2, . . . , Gn, each community
always contains a number of accusers and a number of
accused users. We let ai,j represent the j-th accuser and
bi,p represent the p-th accused user in community Gi, where
i ∈ {1, 2, . . . , n}.
Community Indegree. For an accused user bo,p in community
Go, the community indegree of bo,p is the total number of
accusations made against bo,p by any user in community Gi
where i = o. It is denoted as in(bo,p).
Community Outdegree. For an accusing user ai,j in commu-
nity Gi, the community outdegree of ai,j is the total number
of accusations made against any user in community Go where
o = i. It is denoted as out(ai,j).
As for IA, we denote the incoming accusations about
bo,p as IA(bo,p), which means the number of accusations
made by accusers in other communities about bo,p; therefore,
IA(bo,p) corresponds to the number of inter-community edges
connected to bo,p. We can rewrite IA(bo,p) as
IA(bo,p) = in(bo,p).
We use the incoming accusation feature to approximate the
number of honest accusations made against a user. Thus, a
user u is more likely to have misbehaved if IA(u) is higher.
As for OA, we use Wo,p to represent users that accuse
a user bo,p and stay in the community Go. More precisely,
Wo,p = {ao,j ∈ Go|(ao,j , bo,p)}. Then, the outgoing accusa-
tion feature of the user bo,p is defined by
OA(bo,p) =
∑
w∈Wo,p
out(w).
As inter-community edges probably represent fair accusa-
tions made by honest users, OA(bo,p) can be seen an indicator
of the degree of honesty of accusers Wo,p, where a higher
OA(bo,p) value indicates that the accusers in Wo,p are more
honest. bo,p has probably misbehaved if OA(bo,p) is large.
F. Scheme Overview
We summarize our community-based scheme:
1) Apply the Girvan-Newman algorithm to partition the
accusing graph into a number of accusing communities.
2) Compute the feature pair (IA, OA) of each user based
on the inter-community edges related to the user.
3) Apply the k-means algorithm to partition all users into
two clusters based on their (IA,OA) pairs, and label
users in the cluster with larger (IA,OA) as misbehaved
users.
IV. PERFORMANCE EVALUATION
We use simulations to evaluate the performance of our
scheme in detecting misbehaving users in a social moderation
system. First, we describe the simulation setup, and then assess
the performance of both a counter-based scheme and the
proposed community-based scheme in different scenarios. In
M
et
ho
d
od
yͲ
ba
se
d
M
d
M
et
ho
m
m
un
it
y
un
tͲb
as
e
Co
m
Co
u
Fig. 4. Collusion resistance with different numbers of users and colluders.
M
et
ho
d
od
yͲ
ba
se
d
M
d
M
et
ho
m
m
un
it
y
un
tͲb
as
e
Co
m
Co
u
Fig. 5. Collusion resistance with different numbers of colluder groups of
different size.
collusion. As shown in Figure 4, our scheme always achieves
a collusion resistance rate of 84% or higher regardless of
the ratio of misbehaving users and colluders. This supports
our assertion that the inter-community edges in the accusing
graph are useful for distinguishing honest accusations from
accusations based on collusion.
We observe that the correctness of both schemes is always
higher than 90% in all the above scenarios, even when the
collusion resistance is low. This indicates that correctness is
not a useful index for quantifying the robustness of automation
schemes under collusion attacks. In the following evaluations,
we only consider collusion resistance.
E. Effect of Different Colluder Formations
We also investigate the effect of different colluder forma-
tions on the performance of the count-based and community-
based automation schemes. Here, we set the total number of
users at 800, and the number of misbehaving users at 80. The
number of colluder groups is gradually increased from 1 to
6 and the size of each group is increased from 10 to 35, as
shown in Fig. 5.
We observe the collusion resistance of the count-based
scheme decreases to around 70% when the size of each
collusion group is greater than 25. At the same time, the
collusion resistance of our community-based method still
holds at around 90% in all the scenarios.
V. CONCLUSION
To resolve the collusion problem, we propose a community-
based scheme that can determine whether an accusation is
honest or malicious based on the community structure of
an accusing graph. Through simulations, we show that our
proposed scheme outperforms the naive count-based scheme.
The evaluation results show that the collusion resistance of
our scheme is around 90% irrespective of the population size,
the number of misbehaving users, the number of colluders,
and different colluder formations. In contrast, the count-
based scheme fails to prevent collusion attacks in the same
scenarios. We believe that collusion-resistant schemes like the
one proposed in this paper will play an important role in the
design of social moderation systems for Web 2.0 services
VI. ACKNOWLEDGEMENT
This work was supported in part by Taiwan Information
Security Center (TWISC), National Science Council under
the grants NSC 97-2219-E-001-001 and NSC 97-2219-E-011-
006 and by the iCAST project sponsored by the National
Science Council under the grants NSC97-2745-P-001-001. It
was also supported in part by Taiwan E-learning and Digital
Archives Programs (TELDAP) sponsored by the National
Science Council of Taiwan under the grants NSC 96-3113-
H-001-010 and NSC 96-3113-H-001-012.
REFERENCES
[1] Flickr. http://www.flickr.com/.
[2] K.-T. Chen, J.-W. Jiang, P. Huang, H.-H. Chu, C.-L. Lei,
and W.-C. Chen. Identifying MMORPG bots: a traffic
analysis approach. In ACE ’06: Proceedings of the 2006
ACM SIGCHI international conference on advances in
computer entertainment technology, 2006.
[3] D. Cosley, S. K. Lam, I. Albert, J. A. Konstan, and
J. Riedl. Is seeing believing?: how recommender system
interfaces affect users’ opinions. In CHI ’03: Proceed-
ings of the SIGCHI conference on human factors in
computing systems, pages 585–592, 2003.
[4] C. Dellarocas. Immunizing online reputation reporting
systems against unfair ratings and discriminatory behav-
ior. In EC ’00: Proceedings of the 2nd ACM conference
on electronic commerce, pages 150–157, 2000.
[5] J. J. Garrett. Ajax: A new Approach to Web Applications,
Adaptive Path Essay Archive, 2005.
[6] H. Ino, M. Kudo, and A. Nakamura. Partitioning of
web graphs by community topology. In WWW ’05:
Proceedings of the 14th international conference on
World Wide Web, pages 661–669, 2005.
[7] S. K. Lam and J. Riedl. Shilling recommender systems
for fun and profit. In WWW ’04: Proceedings of the
13th international conference on World Wide Web, pages
393–402, 2004.
[8] C. Lampe and P. Resnick. Slash(dot) and burn: dis-
tributed moderation in a large online conversation space.
In CHI ’04: Proceedings of the SIGCHI conference on
human factors in computing systems, pages 543–550,
2004.
[9] M. E. J. Newman. Modularity and community structure
in networks. Proceedings of the National Academy of
Sciences of the United States of America, 103:8577,
2006.
[10] M. O’Mahony, N. Hurley, N. Kushmerick, and G. Sil-
vestre. Collaborative recommendation: A robustness
analysis. ACM Transactions on Internet Technology,
4(4):344–377, 2004.
[11] A. Whitby, A. Josang, and J. Indulska. Filtering out
unfair ratings in bayesian reputation systems. In Pro-
ceedings of the 7th international workshop on trust in
agent societies, 2004.
additional player, the blocker, whose objective is to stop the
matching from happening. By entering labels prior to the
matching process, the blocker provides a blocked word list.
Other players, the couples, would get penalties for guess-
ing the blocked words. In contrast to the taboo words, the
blocked words are not visible to the couples. While the
blocker is in competition with the other two players, he/she
is motivated to break the coalitions between the couples.
Therefore, KKB naturally provides a player-level cheating-
proof mechanism. Besides, the invisible blocked words set
the restrictions of the available words, thus the couples are
encouraged to provide more diverse labels to evade the re-
strictions.
This paper starts by briefly reviewing human computa-
tion and the image naming process in a psychological view.
We then introduce the design and implementation of KKB.
Finally, we explain the evaluation results and outline the
contribution and future work of this research.
2. RELATED WORK
In this section, we introduce the recent developments in
human computation games and give a brief overview on pic-
ture naming process in psychology.
2.1 Human Computation Games
Human computation aims to solve problems that are hard
for computers by utilizing human brain powers. For exam-
ple, Amazon Mechanical Turk1 provides a marketplace for
the developer to outsource human intelligence tasks. Hu-
man computation games, also called Games With A Pur-
pose (GWAP) [1], propose that using computer games can
gather human players and solve open problems as a side ef-
fect of playing. The GWAP approach has been shown to
be useful and is widely used in various domains, such as
image tagging [5, 6], commonsense collection [2], and music
annotation [3].
To ensure the quality of the collected labels, most GWAP
implementations adopt consensus opinions as the correct-
ness measure. Taking the ESP Game as an example, im-
age labels are generated by collecting descriptions which
both players agree in the game. In [7], Luis von Ahn pro-
posed three game-templates, which summaries their success-
ful experiences in deploying GWAPs. The three templates,
namely input-agreement games, output-agreement games,
and inversion-problem games, rely on player’s collaborations
to collect consensus opinions. In contrast to previous work,
we demonstrate the game design integrating competitive el-
ements.
2.2 Picture Naming
In the ESP Game [5], players perform the process of giv-
ing descriptions to the images. This process usually involves
naming objects in the image and is well studied as picture-
naming process. In psychology, an important measure in
picture-naming process is how fast a person can name a
given picture correctly. The naming latency is determined
by two main factors, namely the word frequency and age of
acquisition (AoA) [8]. Word frequency is the times of occur-
rence in large word corpus, and age of acquisition is the age
at which player learned the word. Typically, players name
faster when the words are learned earlier or have higher fre-
1https://www.mturk.com/mturk/
quency. While there are discussions about which factor is
more important [9, 10], we know that the word properties,
i.e. the word frequency and AoA, affect the player efforts
and the responsive time in naming the pictures.
In our game design, we adopt two time intervals, 7 seconds
and 30 seconds, in different game stages. Investigating the
differences between the image labels collected in two stages
may provide additional information besides the matching
events.
3. GAME MECHANISMS
KissKissBan (KKB) is designed to be played by three on-
line players. One of the players is the “blocker” and the
other two players are the “couples”. With the same image
presented, the couples try to match (Kiss) with each other
by typing the same word and the blocker tries to stop cou-
ples from matching (Ban). Actually, KissKissBan is named
by combining the objectives of game players.
The game rules are described as follows. In the begining
of each round, the blocker has 7 seconds to provide blocked
word list, which is the list of words he/she thinks couples
might match on. These blocked words are not visible to
the couples. After the 7 seconds of entering blocked words,
the couples have 30 seconds to match with each other. The
game time will decrease by 5 seconds if any couple types
the blocked word, i.e., being blocked. Also, agreeing on the
blocked word does not count as matching. The couples win
the round if they successfully match with each other within
the time limit, otherwise the blocker wins. Players switch
roles every 5 rounds in 15 rounds of the game.
3.1 Different Roles
There are two different roles in KKB:
• Blocker: Each player will play the blocker for 5 rounds
in the 15 rounds of the game. Though the blocker only
has 7 seconds to act in each round, he/she is able to
see every word the couples are typing during the game.
Monitoring the actions of the couples not only makes
the waiting process fun, but provides the blocker an
opportunity to stop the couples from achieving some
unified strategy. For example, the blocker could give
“a” as the blocked word if he/she founds the couples
try to match on “a” in every round.
• Couple: The objective of the couples is the same as the
players in the ESP Game: to guess what the partner is
typing. However, unlike the players in the ESP Game,
the couples in KKB cannot see what the blocked words
are. Therefore, the couples are encouraged to guess
harder words to avoid guessing the word in the blocked
words list.
3.2 Incentive Structure
KKB is a three-player zero sum game. In the current im-
plementation, the blocker loses 200 points and each couple
gains 100 points when the couples win; the blocker gains
200 points and each couple loses 100 points when the cou-
ples lose. While the blocker and the couples are strictly
competitive, the blocker is motivated to prevent the cou-
ples from cheating. Therefore, KKB provides a player-level
cheating-proof mechanism.
Another difference between the blocker and the couples is
their available time period for entering words. In order to
The quality of the labels is evaluated using the descrip-
tions provided by the ESP Game Dataset. In average, there
are 13.89 descriptions per image for the images we used.
Taking these descriptions as “partial” ground truth for im-
age labeling, the precision of our collected labels is 78.84%
and the recall is 70.02%. The precision is the ratio of our
labels to be correct. This result demonstrates the data qual-
ity of our collected labels. Actually, the correct ratio should
be much higher since we only use “partial” ground truth for
evaluation. The recall of 70.02% shows the diversity of KKB
labels.
4.2 Property of Collected Labels
To evaluate the data properties of collected labels, we re-
implemented the ESP Game for comparison. We will call
this re-implemented version as K-ESP. To simplify the com-
parison, there is no taboo word in K-ESP. After publishing
on Amazon Mechanical Turk, K-ESP has been played for
5977 rounds, and 4994 labels are collected.
In 4994 labels generated by K-ESP, 6.56 distinct labels
are collected for each image, whereas KKB has collected
11.54 distinct labels per image out of 5521 labels in total.
The data entropy calculated from K-ESP is 4.79, while the
data entropy in KKB is 7.18. These statistics suggest that
KKB motivates players to give more diverse labels. Table 1
gives an example of different labels generated by KKB and
K-ESP.
K-ESP ML-KKB BL-KKB
man * 21 beach * 3 sea * 9
beach * 10 water * 3 man * 8
karate * 5 sand * 3 ocean * 3
water * 1 sea * 2 black * 1
ninja * 1 china * 1
kungfu * 1 sand * 1
ocean * 1
Table 1: Labels produced by KKB and K-ESP. ML-KKB
is the list of the matching label produced by matching be-
tween couples, and BL-KKB is the list of blocking label by
matching blocker and one of the couples.
4.3 Gameplay Survey
To evaluate the gameplay, we conduct a questionnaire sur-
vey involving 17 participants, consisting of 11 males and 6
females. The participants are all college students, whose
ages are ranging from 21 to 25. Before answering the ques-
tions, they are required to play the game at least 2 times,
i.e. 30 rounds. Some of them have played the game over 10
times.
In the anonymous survey, the average enjoyability rating
is 3.76 out of 5, and 88 percent of the subjects claim that
they will play the game again. In addition, over 60% of the
players think it’s more fun and challenging to play as the
blocker.
5. CONCLUSION
We have presented KissKissBan, a competitive human
computation game for image annotation. While the ESP
Game and other GWAPs present using player collabora-
tions for collecting useful information, the main contribu-
tion of this paper is to demonstrate the integration of com-
petitive elements into human computation games. In the
design of KKB, we have two advantages over traditional hu-
man computation games: 1) KKB provides a player-level
cheating-proof mechanism which can alleviate coalition be-
tween players; 2) KKB motivates players to contribute more
diverse labeling and therefore collects a broader set of data.
Evaluations on Amazon Mechanical Turk and a gameplay
survey have shown KKB to be an efficient and fun game for
collecting diverse image annotations.
In our future work, we will explore how players behave
when they play as different game roles. The differences
between KKB labels, namely the matching labels and the
blocking labels, is also an important subject to investigate.
Acknowledgement
This work was supported in part by Taiwan E-learning and
Digital Archives Programs (TELDAP) sponsored by the Na-
tional Science Council of Taiwan under the grants NSC98-
2631-001-011 and NSC98-2631-001-013. It was also sup-
ported in part by the National Science Council of Taiwan
under the grants NSC97-2221-E-001-009.
6. REFERENCES
[1] Luis von Ahn. Games with a purpose. IEEE Computer
Magazine, 39(6):92–94, 2006.
[2] Luis von Ahn, Mihir Kedia, and Manuel Blum.
Verbosity: a game for collecting common-sense facts.
In CHI ’06: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 75–78.
ACM Press, 2006.
[3] Edith L. M. Law, Luis von Ahn, Roger B.
Dannenberg, and Mike Crawford. Tagatune: A game
for music and sound annotation. In International
Conference on Music Information Retrieval (ISMIR’
07), pages 361–364, 2003.
[4] Ingmar Weber, Stephen Robertson, and Milan
Vojnovic. Rethinking the ESP game. Technical report,
Microsoft Research, September 2008.
[5] Luis von Ahn and Laura Dabbish. Labeling images
with a computer game. In CHI ’04: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM Press, 2004.
[6] Luis von Ahn, Ruoran Liu, and Manuel Blum.
Peekaboom: a game for locating objects in images. In
CHI ’06: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 55–64.
ACM Press, 2006.
[7] Luis von Ahn and Laura Dabbish. Designing games
with a purpose. Commun. ACM, 51(8):58–67, 2008.
[8] J.B. Carroll and M.N. White. Word frequency and age
of acquisition as determiners of picture-naming
latency. The Quarterly Journal of Experimental
Psychology, 25(1):85–95, 1973.
[9] C.M. Morrison, A.W. Ellis, and P.T. Quinlan. Age of
acquisition, not word frequency, affects object naming,
not object recognition. Memory and Cognition,
20:705–705, 1992.
[10] C. Barry, K.W. Hirsh, R.A. Johnston, and C.L.
Williams. Age of acquisition, word frequency, and the
locus of repetition priming of picture naming. Journal
of Memory and Language, 44(3):350–375, 2001.
to analyze the systems’ outcomes if either mechanism is ap-
plied. By social verification, we denote the mechanism which
arranges the questions for users and determines the “correct-
ness” of users’ inputs based on their correlations. The two
social verification mechanisms we modeled are:
• Simultaneous verification: In this mechanism, users
are given the same question and asked to provide their
own answers. If some or all of the users agree on an
answer, their answers are considered “correct.” In our
analysis, we model this mechanism in the form of co-
ordination game.
• Sequential verification: This mechanism is like a pop-
ular game “Charade”. In its two-player case, a user A
first gets a question and responses with a set of de-
scriptions about the question. Then another user B
will receive the descriptions from A as the hints, and
B is asked to guess what the original question is. If
B’s answer matches the original question, then A’s de-
scriptions about the question are considered “correct.”
We model this mechanism by an extensive game with
imperfect information.
Based on the equilibrium analysis in the proposed game-
theoretic models, we analyze the effects of the two social
verification mechanisms and discuss how the systems’ out-
comes would be like if either mechanism is applied. Our
analysis results show that sequential verification leads to a
more diverse and descriptive set of outcomes than simulta-
neous verification, though the latter is stronger in ensuring
the correctness of the verified answers. Our experiments
on Amazon Mechanical Turk, which asked users to input
textual terms related to a word, confirmed our analysis re-
sults. We believe that our formal models for social verifica-
tion mechanisms will provide a basis for the design of future
human computation systems.
The rest of this paper is organized as follows. After re-
viewing related work in Section 2, we introduce the relevant
game theoretic models and solution concepts in Section 3.
In Section 4, we give a formal definition of the social verifi-
cation mechanisms and then model the two mechanisms as
two types of two-player game models. Equilibrium analysis
and existing application reviews are also explained. Sec-
tion 5 compares the two social verifications and describes
the experiments conducted on Amazon Mechanical Turk.
Section 6 contains come concluding remarks.
2. RELATED WORK
Most research projects on human computation focus on de-
veloping applications to solve problems that are intractable
for computers. The best-known example is the ESP Game [2],
a two-player online game, which motivates players to con-
tribute their cognitive skills in annotating images. While
playing the game, a player attempts to label a given image,
presented by the system, to match labels given by his online
partner. Inspired by the ESP Game, many applications have
been designed as games to solve a variety of computationally
hard problems, such as locating objects within an image [4],
collecting commonsense knowledge [5], and annotating mu-
sic [6]. The concept of turning games into productive tools
is called “Games With A Purpose” (GWAP) [1].
Despite the impressive progress in human computation, there
have been relatively few studies of the general design prin-
ciples and the theoretic foundation. In a review article [7],
Luis von Ahn proposed three game-structure templates, which
generalize instances of human computation games based on
their successful experiences in deploying GWAP. The tem-
plates, namely output-agreement games, inversion-problem
games, and input-agreement games, describe the game struc-
tures and suggest possible implementations in solving hu-
man computation tasks. The difference between the tem-
plates and our proposed social verifications is that we focus
on user behavior and resulting outcomes of the social in-
teractions. From this point of view, we reduce the three
game-structure templates to two social verification mecha-
nisms.
Since we are considering user interaction in human compu-
tation, game theory seems to be an appropriate approach to
better understand the incentive structure and user behavior
behind the system. To the best of our knowledge, the Pho-
toSlap game [8] was the first to apply game theoretic anal-
ysis in human computation games. A multi-player game,
PhotoSlap, has been developed to accomplish the task of
face recognition. By showing that the desired player strate-
gies lie in the subgame perfect equilibrium, the game de-
sign is shown to motivate users to contribute useful output.
Jain and Parkes [9] proposed a game theoretic analysis of
the ESP Game and conducted equilibrium analysis under
two preference settings, namely the match-early preference
and rare-word-first preference. However, these projects all
focused on analyzing specific applications. In contrast to
previous works, this paper models the abstractions of hu-
man computations, i.e., the social verification mechanisms.
Modeling the abstractions instead of the specific game makes
the analysis results applicable to other human computation
applications.
3. PRELIMINARIES
In this section, we introduce the relevant game models and
solution concepts in game theory [10].
3.1 Game Models
Definition 1. (Normal-form game) A game in nor-
mal form can be defined as a tuple Γ = (N, (Ai)i∈N , (ui)i∈N ),
where N is the set of players; and for each player i ∈ N ,
Ai is the set of available actions for player i, and ui :
(×i∈NAi) → R is the utility function mapping each action
profile of a game into a real-valued payoff for player i.
A two-player normal-form game can be described conve-
niently by a table, where one player’s actions are represented
by the rows and the other player’s actions are represented
by the columns. In this paper, we focus on a specific class
of normal-form game, called coordination game. In a two-
player coordination game, players can gain optimal payoff
by performing the same action.
Definition 2. (Extensive game with imperfect in-
formation) An extensive game with imperfect information
can be defined as a tuple Γ = (N,H,P, (Ii)i∈N , (ui)i∈N ),
where
In this paper, we model two two-player social verification
mechanisms: simultaneous verification and sequential veri-
fication. We give the formal definitions and analysis in the
following section.
4.1 Simultaneous Verification
4.1.1 Definition
Simultaneous verification describes the structure whereby
users give descriptions of a question simultaneously. The
descriptions are considered to be valid if two users agree on
the same description. Formally, given the question q ∈ Q,
each player i ∈ {1, 2} can choose a description di ∈ D. When
the two descriptions d1 and d2 match, users are rewarded
and the system produces an output.
Figure 2: Simultaneous verification mechanism in
human computation.
4.1.2 Game theoretic modeling
In a simultaneous-verification game, each player makes his
decision without knowing the other player’s decision. This
kind of game can be defined as a normal-form game. In
addition, simultaneous verification is characterized by three
properties: (1) every player in the game is identical with
respect to the game rules, i.e., changing the identity of the
players would not change their payoffs; (2) both players get
the same payoff; and (3) players get the optimal payoff when
they perform the same actions. Based on these character-
istics, simultaneous verification games can be modeled as a
normal-form game.
Definition 8. (Simultaneous verification game) Given
a question q and a set of possible descriptions D, a hu-
man computation game with simultaneous verification can
be modeled as a normal-form game, represented by a tuple
Γ = (N, (Di)i∈N , (ui)i∈N ), where N = {1, 2}, D1 = D2 = D
is the set of descriptions available for players, and (ui) is the
utility function of player i that satisfies the following three
conditions:
• u1 = u2 = u,
• u(d1, d2) = u(d2, d1),
• u(di, di) > u(di, dj) for all i 6= j.
4.1.3 Equilibrium analysis
The game models in which players receive an optimal payoff
when they choose the same strategy are called coordination
games. The equilibrium analysis is straightforward.
Theorem 1. A simultaneous-verification game has n =
|D| pure Nash equilibria, represented by the set S where
S = {(d, d) : d ∈ D}.
The proof of the Nash equilibrium analysis is trivial since
a rational player would not change his strategy if he takes
the same strategy as the other player. However, the result
of equilibrium analysis does not provide much useful infor-
mation. We still do not know how a rational player would
act since there are n equilibria that players can choose.
Player 1
Player 2
a1 a2 a3
a1 (1,1) (0,0) (0,0)
a2 (0,0) (1,1) (0,0)
a3 (0,0) (0,0) (1,1)
Table 2: An example of a simultaneous verification
game. The grey region marks the Nash equilibria of
the game.
In traditional game theoretic analysis, the optimal strategy
of the coordination game is a mixed strategy that randomly
chooses one of the strategies in the n pure Nash equilibria.
However, as the ESP Game shows, people can do much bet-
ter than randomly choose their actions. Players in a game
may coordinate to choose some preferred equilibria, which
are called the focal points (a.k.a Schelling points).
Originally introduced by Schelling [11], focal points are the
“prominent” or “salient” solutions to the game. For exam-
ple, consider two players who are both required to choose
an element from the set Z, and they only get a payoff when
they choose the same element. Suppose each player differ-
entiates the elements in terms of frequency, which means
“how many times he/she has heard them mentioned.” For
instance, if each element is a single word, the frequency of
the element is the times players have seen them in books
and newspapers for some period of time. A natural focal
point would be to choose the element with the highest fre-
quency [12]. Intuitively, players would choose the element
with higher frequency since the element is more “prominent”
than others.
Returning to the simultaneous verification game, players try
to choose an element d from the description set D given the
question q. For simplicity, assume that all players have the
same private description of the frequency function f(d|q),
which denotes how many times d appears when q is given.
In this case, a natural focal point would be to choose the
description d maximizing f(d|q). Since the frequency is the
statistic of the times of occurrence in a certain period, we
will rewrite it in the form of probability p(d|q) by normal-
ization. Following the formula of conditional probability,
p(d|q) = p(d, q)
p(q)
=
p(d)× p(q)× r(d, q)
p(q)
= p(d)× r(d, q),
where p(d, q) is the joint probability that elements d and q
will co-occur, and r(d, q) = p(d,q)
p(d)×p(q) is defined to represent
the relevance of the description d to the question q. When
d and q are irrelevant, i.e., totally independent, r(d|q) = 1.
Since the focal point of the game is to maximize f(d|q), i.e.,
p(d|q), we can conduct the following lemma.
Lemma 1. In a human computation game with simulta-
neous verification Γ = (N, (Di)i∈N , (ui)i∈N ), where each
(a) The ESP Game. Players are given
the image and required to give anno-
tation words.
(b) Matchin. Players are given a
pair of images and required to answer
which one the partner prefers.
(c) Squigl. Players are given a word-
image pair and required to give the
locations of the word in the image.
Figure 3: Human computation games with simultaneous verifications.
By limiting the size of the question space Q and the size
of the description space D to 2, the sequential verification
game can be represented in the form of game tree, as shown
in Figure 5. For the simplicity, we omit the payoff for the
system player.
Figure 5: An example of sequential verification mod-
eling in human computation. The payoff for the sys-
tem player is omitted.
4.2.3 Equilibrium analysis
Similar to the analysis in simultaneous-verification game,
we define a frequency function f in the following sequen-
tial equilibrium analysis. Given question space Q and the
description space D, f(q|d)q∈Q,d∈D denotes the private de-
scription of the players on “how many times question q ap-
pears when given description d”. We also rewrite it in the
form of probability p(q|d).
Players get optimal payoffs when player 2 guesses correctly
about the original question q given description d. The be-
havior of player 2 is similar to that of players in simultaneous
verification games, i.e. player 2 would guess the question q
maximizing p(q|d). However, the information we really care
about is the behavior of player 1, since it directly relates to
the system outputs. Under the belief that player 2 chooses
question q maximizing p(q|d), player 1 would choose the de-
scription d, such that p(q|d) ≥ p(qi|d) for all qi ∈ Q, i.e., q
is the most frequent element co-occurring with d than any
other questions. Player 2 would guess correctly if both play-
ers adopt these strategies. We write these strategies in the
form of sequential equilibrium in the following lemma.
Lemma 2. Given a question space Q, a description space
D, and a frequency function f shared by all players, an as-
sessment (β, µ) is a sequential equilibrium of the sequential-
verification game (N,H,P, (Ii)i∈N , (ui)i∈N ) if the following
condition holds for all qi ∈ Q:
• βsystem(φ)(qi) = 1|q| ,
• β2(I(d))(qi) =

1, if qi = argmaxq∈Qp(q|d)
0, otherwise.
• β1(qi)(d) =

1/|Dqi |, if d ∈ Dqi
0, otherwise.
• µ2(I(d))(qi, d) =

1/|Dqi |, if d ∈ Dqi
0, otherwise.
βi(I)(d) is the probability to choose description d for player
i in information set I, and Dqi is the set of d which satisfies
qi = argmaxq∈Qp(q|d) and d ∈ D. Intuitively, Dqi is the
set that contains all the descriptions that make question q
“prominent”.
Though the notations seem to be complicated, the results
are intuitive. The result of the sequential equilibrium anal-
ysis shows, for rational players: (1) player 1 would describe
the question q in the way that q is more frequently seen
than other questions given the description d. While there
are many possible descriptions satisfying the above condi-
tion, player 1 would choose one of them in random. (2)
player 2 would choose the answer which best fits the de-
scription, i.e. which has the highest frequency. To prove
the sequential equilibrium, the assessment should be exam-
ined if it satisfies sequential rationality and consistency. The
proof for sequential rationality is trivial since both players
cannot increase their payoff by changing the strategies. The
consistency of the assessment can be proved by setting the
elements of value 0 in β1 and µ2 to ² and normalizing the
value of the other elements.
4.2.4 Analysis of human computation games with se-
quential verifications
Peekaboom [4], a web game for locating objects in images, is
an example demonstrating the sequential verification, where
Moreover, the labels in SEQ Game is usually more descrip-
tive than that in SIM Game, as demonstrated in Table 3. On
average, each label contains 2.633 words in SEQ Game, and
1.002 words in SIM Game. While more words represents for
more descriptive powers, we can implicitly infer that labels
in SEQ Game is more descriptive.
Another interesting discovery is the ratio of the mis-spelling
words. There are 335 mis-spelling words in the 4368 labels of
SEQ Game, and no mis-spelling labels in SIM Game. This
result may be dure to the stronger verification in ensuring
the correctness in SIM Game.
SIM Game SEQ Game
add science with numbers
class adding, subtracting
calculus subject in calculations
school course with numbers
Table 3: Labels collected for “Mathematics.”
5.3 Discussion
This small-scale experiments have revealed some important
properties of the labels generated in two social verification
mechanisms. First, labels collected in sequential verification
mechanism are more descriptive and diverse than the labels
in the simultaneous verification mechanism. The result con-
firms the game theoretic analysis of player behavior in the
previous section.
Understanding player behavior and system outputs in differ-
ent social verification mechanisms is essential in designing
human computation applications. For example, adopting
sequential verification mechanism might be a better choice
when description diversity is important in the application.
However, additional mechanisms, such as spell checking,
should be applied to ensure the data quality. If the descrip-
tion space is limited, e.g., the two choices in Matchin, or
the output correctness is essential, simultaneous verification
mechanism would be a better fit.
6. CONCLUSIONS
In this paper, we propose formal models for two fundamental
social verification mechanisms, simultaneous verification and
sequential verification, in a game theoretic approach. Our
contributions are summarized below:
• We have shown that social verification mechanisms in
human computation games can be modeled by using
game theory. Since simultaneous and sequential verifi-
cations are fundamental, our models can be generally
applied to human computation systems.
• We have shown that simultaneous and sequential veri-
fications can lead to different system outcomes. While
sequential verification promotes a more diverse and de-
scriptive labeling, simultaneous verification is stronger
in ensuring the data correctness.
• Real-world experiments involving two mechanisms have
been conducted. Through analyzing the data gener-
ated by the different mechanisms, we summarize the
properties of the system output and give suggestions
for choosing mechanisms when designing new human
computation applications.
In our future work, we will identify other verification mecha-
nisms and explore the possibility of integrating competitive
elements into human computation. In addition, we will in-
vestigate other issues from a game theoretic perspective, in-
cluding the presence of malicious players and how to change
the incentive structure to achieve different system outcomes.
7. REFERENCES
[1] Luis von Ahn. Games with a purpose. IEEE Computer
Magazine, 39(6):92–94, 2006.
[2] Luis von Ahn and Laura Dabbish. Labeling images
with a computer game. In CHI ’04: Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM Press, 2004.
[3] Ingmar Weber, Stephen Robertson, and Milan
Vojnovic. Rethinking the ESP game. Technical report,
Microsoft Research, September 2008.
[4] Luis von Ahn, Ruoran Liu, and Manuel Blum.
Peekaboom: a game for locating objects in images. In
CHI ’06: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 55–64.
ACM Press, 2006.
[5] Luis von Ahn, Mihir Kedia, and Manuel Blum.
Verbosity: a game for collecting common-sense facts.
In CHI ’06: Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 75–78.
ACM Press, 2006.
[6] Edith L. M. Law, Luis von Ahn, Roger B.
Dannenberg, and Mike Crawford. Tagatune: A game
for music and sound annotation. In International
Conference on Music Information Retrieval (ISMIR’
07), pages 361–364, 2003.
[7] Luis von Ahn and Laura Dabbish. Designing games
with a purpose. Commun. ACM, 51(8):58–67, 2008.
[8] Chien-Ju Ho, Tsung-Hsiang Chang, and Jane
Yung-jen Hsu. Photoslap: A multi-player online game
for semantic annotation. In Twenty-Second Conference
on Artificial Intelligence (AAAI-07). AAAI Press,
July 2007.
[9] Shaili Jain and David C. Parkes. A game-theoretic
analysis of games with a purpose. In Internet and
Network Economics, 4th International Workshop
(WINE 2008), pages 342–350, December 2008.
[10] Martin J. Osborne and Ariel Rubinstein. A course in
game theory. The MIT Press, Cambridge,
Massachusetts, 1994.
[11] Thomas C. Schelling. The Strategy of Conflict.
Harvard University Press, Cambridge, Massachusetts,
1960.
[12] Robert Sugden. A theory of focal points. The
Economic Journal, 105(430):533–550, 1995.
[13] Severin Hacker and Luis von Ahn. Matchin: Eliciting
user preferences with an online game. In CHI ’09:
Proceedings of the SIGCHI conference on Human
factors in computing systems. ACM Press, 2009.
4. In MOS tests, participants are asked to grade the MOS
scores for stimuli; however, we do not know whether
they pay full attention to the scoring procedures, or
whether they just give ratings in a perfunctory man-
ner. To the best of our knowledge, there is no es-
tablished methodology for verifying whether a partic-
ipant gives false ratings either intentionally or care-
lessly. Consequently, it is hard to detect problem-
atic inputs, and the measurement accuracy may be
degraded due to the behavior of untrustworthy partic-
ipants.
In this paper, we propose the concept of crowdsourcing
experiments to the general public to achieve eﬃcient and re-
liable QoE evaluations. Crowdsourcing is a neologism that
means utilizing the general public’s wisdom rather than the
expertise of employees or contractors. Until recently, QoE
experiments were conducted in academic laboratories; how-
ever, with the advent of ubiquitous Internet access, it is now
possible to ask an Internet crowd to conduct experiments on
their personal computers. Our rationale is that, since the
size of such a crowd can be considerable, crowdsourcing al-
lows researchers to conduct experiments with a more diverse
set of participants at a lower economic cost than is possible
under laboratory conditions.
A major challenge of crowdsourcing QoE evaluations is
that not every Internet user is trustworthy. Since users per-
form experiments without supervision, they may give erro-
neous feedback perfunctorily, carelessly, or dishonestly, even
if they receive a reward for each experiment. For example, if
we were to ask an Internet crowd to rate several video clips
compressed by diﬀerent codecs and we received dozens of
ratings for each clip, it would be diﬃcult, if not impossible,
to determine which ratings were believable. Erroneous rat-
ings may increase the variance of the evaluation results and
lead to biased conclusions. One may argue that we could
compensate for problematic inputs by conducting more ex-
periments than necessary, but it would only be valid if dis-
honest and careless users comprise a small proportion of an
experiment’s participants. Moreover, since dishonest users
may choose random answers without following the experi-
ment regulations, they can earn rewards more easily than
other users; thus, they may be motivated to participate in
as many experiments as possible to maximize their rewards.
Therefore, we must ﬁnd a way to detect problematic inputs
in order to obtain reliable and high-quality evaluation re-
sults.
To resolve the above problem, we propose a crowdsource-
able framework, based on paired comparison, for multimedia
QoE evaluations. The framework not only enables us to ver-
ify the consistency of users’ inputs systematically, but also
addresses the disadvantages of the MOS rating test men-
tioned earlier. In a paired-comparison test, a participant is
simply asked to compare two stimuli simultaneously, and
vote (decide) which one has the better quality based on
his/her perception. Clearly, making a decision is simpler
than in the MOS test, as the ﬁve-scale rating is reduced to
a dichotomous choice. The features of paired comparison
are as follows:
1. Like MOS, paired comparison is generalizable across a
variety of multimedia applications. Thus, it can be ap-
plied to various genres of multimedia content without
any modiﬁcation.
2. The burden on participants is low, since they do not
have to map their sensation magnitude on a categorical
or numerical scale. They are only required to make
simple comparative judgments, and thereby avoid the
scale heterogeneity problem of MOS ratings [31].
3. We can apply probabilistic choice models [10] to ana-
lyze paired-comparison results and obtain QoE scores
on an interval scale [36]. The scale enables us to quan-
tify the discrepancy between the QoE of diﬀerent eval-
uated targets, and also allows us to compile an arith-
metically computable index for QoE management pur-
poses [7, 17].
4. The key property of paired comparison is that the ex-
periment results can be veriﬁed. The veriﬁcation ba-
sically relies on the transitivity property ; that is, if A
is preferred over B, and B is preferred over C, then
A should also be preferred over C by the same par-
ticipant. By employing this property, we can detect
inconsistent judgments and remove problematic data
before performing further analysis and modeling.
We demonstrate the eﬀectiveness and generalizability of
the proposed crowdsourceable experiment framework in four
case studies: the ﬁrst evaluates the quality of audio clips en-
coded by MP3 with diﬀerent bit rates; the second evaluates
the quality of VoIP speech with diﬀerent packet loss rates;
the third compares the quality of several video codecs that
have similar bandwidth usage; and the fourth compares two
loss concealment schemes for video playout. The studies
cover a variety of multimedia applications and various fac-
tors that may aﬀect the content’s quality. For each study, we
conducted both laboratory and crowdsourced experiments.
The former were performed by part-time employees under
supervision; and the latter were carried out by anonymous
Internet users who were interested in making some money.
The results show that, overall, the quality of data obtained
from the crowdsourced experiments was slightly lower than
that derived from laboratory experiments. Even so, because
of our approach’s ability to detect inconsistent inputs1, we
can still obtain comparable evaluation results at a lower eco-
nomic cost and with wider participant diversity. In addition,
supervision of the experiment does not require a physical
space or involve labor costs.
Our contribution in this work is three-fold:
1. We propose a crowdsourceable framework, which com-
prises paired comparison, consistency checking, prob-
abilistic choice modeling, and Web-based implementa-
tions, to quantify the QoE of multimedia content. The
advantages of our framework over traditional MOS rat-
ings are that 1) it facilitates crowdsourcing because it
supports systematic veriﬁcation of the participants’ in-
puts; 2) the rating procedure is simpler than that of
the MOS test, so the burden on participants is lower;
and 3) it derives interval-scale scores that enable fur-
ther quantitative analysis and QoE management [17].
2. Our crowdsourceable framework not only enables de-
tection of problematic inputs, but also makes “diﬀer-
entiated rewards” possible. That is, the reward for
performing an experiment can be based on the quality
1Before each experiment, we informed the participants that
no reward would be given if their inputs were not self-
consistent; all the participants seemed to accept the rule.
(a) Released state (b) Pressed state
Figure 1: The user interface for the acoustic QoE evaluation
experiment.
his/her performance. Several mechanisms [3, 13, 24] have
been proposed to integrate an incentive structure into rep-
utation systems in order to encourage good behavior and
stop bad behavior.
3. THE PROPOSED FRAMEWORK
In this section, we present our framework for evaluating
the QoE of multimedia content. We describe the experiment
designs for evaluating audio and visual multimedia content;
discuss how to assess the consistency of participants’ judg-
ments in order to remove problematic inputs; and explain
how to develop a statistical model based on the paired com-
parison results and how to estimate the QoE scores for the
evaluated multimedia content.
3.1 Experiment Designs
Suppose we have n algorithms for processing a series of au-
dio samples. The algorithms can be used for representation
purposes, e.g., audio encoding, or for handling impairments
due to errors in storage or transmission, such as error cor-
rection or loss concealment. We now present our experiment
design for evaluating the eﬀect of diﬀerent audio processing
algorithms on the QoE of audio recordings.
First, we need to select an audio clip, which we call the
“source clip,” as the evaluation target. We apply the n au-
dio processing algorithms to the source clip and generate n
diﬀerent versions of the clip, which we call the “test clips.”
Since all the test clips are processed, e.g., encoded, from
the same source clip, their content will be synchronized ex-
actly. That is, except for their presentation quality, every
second of the audio samples in each of the n test clips will
be semantically equivalent.
Second, we create an Adobe Flash-based system for users
to evaluate the n test clips. Performing experiments under
our system is quite simple, as a participant only needs to
use three keys, namely the SPACE key, the LEFT key, and
the RIGHT key. For an n-clip experiment, m =
(
n
2
)
paired
comparisons (rounds) are required. In each of the m rounds,
the system randomly picks a pair of test clips that has not
appeared yet, and randomly assigns one clip in the pair to
the Pressed state and the other to the Released state.
Once a round starts, the participant will hear one of the test
clips playing continuously, depending on whether or not the
SPACE key is pressed. The test clip associated with the
Pressed state will be heard if the SPACE key is pressed;
otherwise, the clip associated with the Released state will
be heard.
Even though the clip being played is switched back and
forth whenever the participant presses or releases the SPACE
key, it seems that the quality level of the source clip is con-
Application Quality
Space Key
Released
SPACE KEY
User Action
Vote
Quality with Space Key 
Released is Better
Space Key
Pressed
Space Key
Released
Space Key
Released
Space Key
Pressed
Ready to Make a Decision
Time
Quality with Space Key 
Pressed is Better
Figure 2: The concept ﬂow of an experiment participant in
an acoustic paired comparison.
(a) Released state (b) Pressed state
Figure 3: The user interface for the optical QoE evaluation
experiment.
trollable. This is because all test clips have identical seman-
tic content and timing structures. The design allows partic-
ipants to press and release the SPACE key in an exploratory
manner, and carefully listen to the diﬀerence in the quality
of the two states before deciding which state yields a more
pleasant experience.
Fig. 1 shows our system’s user interface. The large upper
pane provides state indicators in two colors (red vs. blue)
and glyphs (key pressed vs. key released) to indicate the cur-
rent state (Pressed vs. Released). We do not restrict the
time allowed for each round, and the test clips are played re-
peatedly. If the quality of two test clips diﬀers signiﬁcantly,
participants should be able to tell the diﬀerence easily and
make a decision within a few seconds. Sometimes the diﬀer-
ences in quality are quite subtle, so participants may require
an much longer time to make a decision2. In both cases,
once the participant is ready to make a decision, he/she can
press the LEFT key to indicate that the quality is better in
the Released state or the RIGHT key to indicate that the
quality is better in the Pressed state. The system proceeds
to the next round automatically after the participant has
voted, and informs the participant that the experiment is
ﬁnished once m paired comparisons have been made. Fig. 2
illustrates the ﬂow of an acoustic paired comparison experi-
ment. Readers can experience our experiment platform on-
line at http://mmnet.iis.sinica.edu.tw/link/da.
The experiment design for evaluating the QoE of optical
multimedia content, such as video clips, is very similar to
that used for acoustic content. We also generate n video
clips from a source clip with n processing algorithms and
conduct m =
(
n
2
)
paired comparisons for each experiment.
In each round, participants need to decide (vote) which state
(Pressed or Released) yields a better visual quality.
The user interface of our system for optical QoE evalua-
2In our experiments, each round normally took between 5
and 25 seconds.
the quantitative QoE scores for the quality levels being eval-
uated.
Assume that our experiment is composed of n quality lev-
els, T1, ..., Tn; thus, there are
(
n
2
)
quality-level pairs. We
denote the number of comparisons for the pair (Ti, Tj) as
nij , where nij = nji. The results of paired comparisons
can be summarized by a matrix of choice frequencies, rep-
resented as {aij}, where aij denotes the number of choices
that participants prefer Ti over Tj and aij + aji = nij .
T1 T2 T3 T4
T1 – a12 a13 a14
T2 a21 – a23 a24
T3 a31 a32 – a34
T4 a41 a42 a43 –
Table 1: A matrix of choice frequencies for four quality levels
By applying a probabilistic choice model [10] to the paired
comparison results, we can extract an interval-scale score for
each quality level. One of the most widely used models for
this purpose is the Bradley-Terry-Luce (BTL) model [6,27],
which predicts Pij , the probability of choosing Ti over Tj ,
as a function associated with the “true” ratings of the two
quality levels:
Pij =
π(Ti)
π(Ti) + π(Tj)
=
eu(Ti)−u(Tj)
1 + eu(Ti)−u(Tj)
, (1)
where u(Ti) = log π(Ti) is the estimated QoE score of the
quality level Ti, which can be obtained by using the maxi-
mum likelihood estimation method. We treat u(Ti) rather
than π(Ti) as our QoE score because it comprises interval-
scale metrics, but π(Ti) does not.
To evaluate the BTL model’s goodness of ﬁt with the
choice frequencies, we compare the likelihood L0 of the given
model and the likelihood L of the unrestricted model, which
ﬁts the frequencies perfectly. The test statistic −2 log(L0/L)
is approximately χ2-distributed with n − 1 degrees of free-
dom. The goodness of ﬁt of the model can also be used
to check the overall consistency of the paired comparison
results. The model can be expressed in a linear form as
Pij = H(u(Ti)− u(Tj)) and H(x) = ex1+ex . H(x) is a mono-
tonically increasing function, which implies that the BTL
model possesses the SST property. In other words, system-
atic violations of SST will preclude the validity of the BTL
model. Therefore, we can also ensure the consistency of
the paired comparison results by checking whether the BTL
model ﬁts the data well.
Model Interpretation
The computed u(Ti) for the quality level Ti from the ﬁtted
BTL model conforms to the relationship in Eq. 1. It must
be negative since u(Ti) = log π(Ti) and π(Ti) is a positive
real number smaller than 1. To extract interpretable QoE
scores, we normalize all the QoE scores between 0 and 1. By
so doing, the quality level with the highest QoE always has a
score of 1, and that with the lowest QoE always has a score
of 0. Thus it is more reasonable to include a “perfect,” or at
least “near-perfect,” quality level in the experiment if this
normalization approach is adopted. The rationale is that
it allows us to compare the QoE scores of diﬀerent quality
levels, assuming that the perfect scheme achieves a QoE
score of 1 and the worst scheme achieves a score of 0.
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Compression bitrate (Kbps)
Qo
E 
sc
or
e
32 48 64 80 96 128
Wake Me Up (Fast)
Garden of Graves (Slow)
Figure 4: QoE scores of MP3-compressed songs at diﬀerent
bit rates.
4. CASE STUDIES: ACOUSTIC QOE EVAL-
UATION
In this section, we present two case studies based on our
experiment design for acoustic QoE evaluations (cf. Sec-
tion 3.1). In the ﬁrst case, we study how the QoE of MP3-
encoded songs varies by altering the compression bit rates.
In the second case, we investigate how packet loss and speech
codec aﬀect VoIP speech quality. While these topics may
not be new to the research community, we consider that
they are good starting points to demonstrate the eﬃcacy of
our framework. All the QoE evaluations were performed in
three ways: in our own (physical) laboratory, crowdsourced
to MTurk users, and crowdsourced to an Internet commu-
nity. Here, we focus on the QoE evaluation results, which
were inferred from the combined frequency choices of partic-
ipants from the three sources, and their implications in each
case study. We discuss how the experiments were crowd-
sourced and then compare the performances of the labora-
tory and crowdsourced experiments in Section 6.
4.1 MP3 Compression Level
In audio compression, there is a trade-oﬀ between main-
taining good sound quality and reducing the size of the audio
data. A higher encoding bit rate usually yields better quality
output; however, the cost is a larger ﬁle, which increases the
demand for data storage and network bandwidth in stream-
ing applications. In this case study, we investigate the QoE
of MP3-compressed audio clips with diﬀerent compression
levels. We selected two English songs, the fast-paced “Wake
Me Up Before You Go Go” and the slow-paced “Garden of
Graves,” as the source clips. To obtain test clips, we con-
verted the songs into MP3 CBR format with six bit rate
levels, namely, 32, 48, 64, 80, 96, and 128 Kbps. Conse-
quently, for each song, we had 6 stimuli (quality levels) and(
6
2
)
= 15 paired comparisons in each experiment.
There were 127 participants, both part-time employees
and Internet volunteers (cf. Section 6), who performed 244
experiments on a total 3, 660 paired comparisons. For each
paired comparison, the participant used the interface shown
in Fig. 1 to indicate which quality level yielded a better lis-
tening experience. Using the probabilistic choice modeling
technique described in Section 3.4, we estimated the QoE
scores of the 6 compression levels for each song, and plot-
ted them on the graph shown in Fig. 4, where the vertical
bar denotes the 95% conﬁdence band of the score on each
point. From the graph, we observe that a higher bit rate
constantly leads to a higher QoE score. Also, the law of
Frame copy Frame copy w/ frame skip
Cheerleaders (Fast)
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Loss rate
1%
5%
8%
Frame copy Frame copy w/ frame skip
Mobile Calendar (Slow)
Qo
E 
sc
or
e
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Loss rate
1%
5%
8%
Figure 7: QoE scores of video clips decoded by diﬀerent loss
concealment schemes at diﬀerent packet loss rates.
800 Kbps. Interestingly, on the “Mobile Calendar” video,
WMV3 performs signiﬁcantly better than XVID at the same
bit rate. This indicates that WMV3 is generally better than
XVID; the exception is that XVID is comparable to WMV3
on fast-motion videos at high bit rates.
5.2 Loss Concealment Scheme
When designing a high-quality IPTV system, one of the
most challenging issues is how to deal with video packet
loss due to network loss or excessive variations in network
delay. A number of loss concealment schemes have been pro-
posed, e.g., the intuitive frame copy method and the more
sophisticated error resilient coding approach. In this case
study, we evaluated two loss concealment schemes, namely
the frame copy (FC) scheme and the frame copy with frame
skip (FCFS) scheme [35], under diﬀerent degrees of packet
loss. The FC scheme conceals errors in a video frame by re-
placing a corrupted block with the block in the correspond-
ing position in the previous frame. On the other hand, FCFS
is a hybrid scheme that integrates the frame copy and the
frame skip technique, which simply drops a frame that is cor-
rupted due to packet loss. In our implementation of FCFS,
if the percentage of corrupted slices in a frame exceeds 10%,
it will skip (i.e., drop) the frame; otherwise it will apply the
frame copy method to conceal the errors. The “Cheerlead-
ers” and “Mobile Calendar” video clips (Section 5.1) were
also used in this case study. We compressed both clips by
JM [1], the H.264/AVC reference software, and simulated
packet loss rates at 1%, 5%, and 8%, to obtain degraded
test clips. During the decoding process, we applied FC and
FCFS for loss concealment. Since there are three packet loss
rates and two loss concealment schemes, we obtained 6 test
clips.
A total of 91 participants performed 183 experiments that
involved 2, 745 paired comparisons. Fig. 7 shows the QoE
score for each test clip of “Cheerleaders” and “Mobile Calen-
dar.” We ﬁnd that FCFS performs slightly better than FC
on “Cheerleaders” when the loss rate is moderate (≤ 5%).
This may be because FCFS skips seriously corrupted frames
so that the subjects perceive better spatial quality. However,
when the loss rate is high (8%), FCFS drops a large number
of frames, so its QoE is inferior to that of FC. Interestingly,
the situation is reversed in the case of the “Mobile Calen-
dar” clip. FCFS outperforms FC at moderate to high loss
rates (≥ 5%). We believe this is because dropping frames
in a slow-motion video does not lead to signiﬁcant freezing
eﬀects. On the other hand, FC provides better QoE at the
1% loss rate. This is reasonable because, when the damage
caused by packet loss is small in a slow-motion video, FC
can easily repair most of the corrupted blocks. This case
study demonstrates two points: 1) the eﬀectiveness of our
framework for evaluating the QoE of video clips; and 2) the
eﬀect of loss concealment depends to a large extent on the
characteristics of the target video clips.
6. DISCUSSION
In this section, we compare the eﬃciency and eﬀectiveness
of laboratory and crowdsourced experiments performed for
the case studies in Section 4 and Section 5. The experiments
were conducted in the three ways:
• Laboratory: We recruited part-time workers at an
hourly rate of US$8. They were asked to perform the
experiments repeatedly during their working hours.
• MTurk: We posted each experiment as a HIT (Hu-
man Intelligence Task) on the Mechanical Turk web
site. If an experiment was qualiﬁed, i.e., it yielded a
TSR higher than 0.8, we paid the participant 0.15 US
dollars.
• Community: We posted an advertisement on the
website of an Internet community with 1.5 million mem-
bers to seek participants for our experiments. For each
experiment that qualiﬁed, we paid the participant an
amount of virtual currency that was equivalent to one
US cent.
In total we spent US$173.88 on 753 experiments, which
were performed by 298 participants and involved 11, 295
paired comparisons. The performances and costs of all the
participant sources in the case studies are summarized in
Table 2. Next, we discuss the diﬀerences between the lab-
oratory and crowdsourced experiments in terms of quality,
cost, and participant diversity.
Quality. We deﬁne the Qualiﬁed Rate as the ratio of
experiments that yield a TSR higher than 0.8. In the ex-
periments, the rate was generally between 60% and 70%.
The laboratory experiments achieved the highest rates in all
cases, except for the VoIP case study. Moreover, in the study
of loss concealment schemes, the rates for laboratory experi-
ments were as high as 69%, compared to approximately 35%
on both crowdsourcing sites. The latter rates indicate that it
is diﬃcult to diﬀerentiate the quality of video clips with dif-
ferent loss concealment schemes. We believe the superiority
of laboratory experiments in this case is due to the diﬀerence
in the participants’ proﬁciency. On average, the laboratory
participants and crowdsourcing participants performed 115
and 18 comparisons respectively. Hence, the former had
more opportunities to gain experience in distinguishing the
subtle diﬀerences in the quality of video clips.
We also checked the overall consistency of experiment re-
sults from three participant sources. After removing un-
qualiﬁed experiments, we computed the average TSR of the
Internet crowd without compromising the quality of the re-
sults; and, at the same time, achieve wider participant di-
versity at a lower monetary cost.
In the future, we plan to release our experiment platform
for public use. Currently the platform is under alpha release
and can be accessed via http://mmnet.iis.sinica.edu.tw/
link/qoe. Researchers will be able to publish their multi-
media content on the platform and utilize an Internet crowd
to conduct experiments at a lower cost. We hope that the
proposed crowdsourcing framework and platform for QoE
evaluations will prove helpful to researchers interested in
evaluating the quality of multimedia content.
Acknowledgements
The authors would like to thank Wei Tsang Ooi and the anony-
mous reviewers for their constructive comments. This work was
supported in part by the Taiwan E-learning and Digital Archives
Program (TELDAP), sponsored by the National Science Council
of Taiwan under grants NSC98-2631-001-011 and NSC98-2631-
001-013. It was also supported in part by the National Science
Council of Taiwan under grants NSC96-2628-E-001-027-MY3 and
NSC98-2221-E-001-017.
8. REFERENCES
[1] H.264/AVC reference software JM 15.1.
http://iphome.hhi.de/suehring/tml/.
[2] O. Alonso, D. E. Rose, and B. Stewart. Crowdsourcing for
relevance evaluation. SIGIR Forum, 42(2):9–15, 2008.
[3] A. Blanc, Y.-K. Liu, and A. Vahdat. Designing incentives
for peer-to-peer routing. In Proceedings of IEEE
INFOCOM 2005, pages 374–385, March 2005.
[4] P. Bordia. Face-to-face versus computer-mediated
communication: A synthesis of the experimental literature.
Journal of Business Communication, 34(1):99–118, 1997.
[5] D. Brabham. Crowdsourcing as a model for problem
solving: An introduction and cases. Convergence, 14(1):75,
2008.
[6] R. A. Bradley and M. E. Terry. Rank analysis of
incomplete block designs: I. the method of paired
comparisons. Biometrika, 39(3/4):324–345, 1952.
[7] K.-T. Chen, C.-Y. Huang, P. Huang, and C.-L. Lei.
Quantifying Skype user satisfaction. In Proceedings of
ACM SIGCOMM 2006, Pisa, Itlay, Sep 2006.
[8] K.-T. Chen, C. C. Tu, and W.-C. Xiao. OneClick: A
framework for measuring network quality of experience. In
Proceedings of IEEE INFOCOM 2009, April 2009.
[9] S. Choisel and F. Wickelmaier. Evaluation of multichannel
reproduced sound: Scaling auditory attributes underlying
listener preference. The Journal of the Acoustical Society of
America, 121(1):388–400, 2007.
[10] H. A. David. The Method of Paired Comparisons. Oxford
University Press, 1988.
[11] R. Dittrich, R. Hatzinger, and W. Katzenbeisser. Modelling
the eﬀect of subject-speciﬁc covariates in paired comparison
studies with an application to university rankings. Journal
of the Royal Statistical Society (Series C): Applied
Statistics, 47(4):511–525, 1998.
[12] B. Duﬀy, K. Smith, G. Terhanian, and J. Bremer.
Comparing data from online and face-to-face surveys.
Internation Journal of Market Research, 47(6):615–639,
2005.
[13] A. Fernandes, E. Kotsovinos, S. Otring, and B. Dragovic.
Pinocchio: Incentives for honest participation in
global-scale distributed trust management. In Proceedings
of iTrust2004, pages 63–77, 2003.
[14] C.-J. Ho, T.-H. Chang, and J. Y.-j. Hsu. Photoslap: A
multi-player online game for semantic annotation. In
Twenty-Second Conference on Artiﬁcial Intelligence
(AAAI-07), Vancouver, British Columbia, July 2007.
[15] C.-J. Ho and K.-T. Chen. On formal models for social
veriﬁcation. In Proceedings of Human Computation
Workshop 2009 (aﬃliated to ACM KDD 2009), Paris,
France, 2009.
[16] J. Howe. The rise of crowdsourcing. Wired Magazine,
14(6):176–183, 2006.
[17] Y. Ito and S. Tasaka. Quantitative assessment of user-level
QoS and its mapping. IEEE Transactions on Multimedia,
7(3):572–584, June 2005.
[18] ITU-R Recommendation BT.500-11. Methodology for the
subjective assessment of the quality of television pictures,
2002.
[19] ITU-R Recommendation P.800. Methods for subjective
determination of transmission quality, 1996.
[20] ITU-T Recommendation J.247. Objective perceptual
multimedia video quality measurement in the presence of a
full reference, 2008.
[21] ITU-T Recommendation P.862. Perceptual evaluation of
speech quality (PESQ), an objective method for end-to-end
speech quality assessment of narrow-band telephone
networks and speech codecs, 2001.
[22] R. Jain. Quality of experience. IEEE Multimedia,
11(1):96–97, Jan.-March 2004.
[23] S. Jain, Y. Chen, and D. C. Parkes. Designing incentives
for online question and answers forums. In 10th ACM
Electronic Commerce Conference (EC’09), 2009.
[24] R. Jurca and B. Faltings. An incentive compatible
reputation mechanism. In Proceedings of IEEE
International Conference on E-Commerce Technology,
pages 285–292, June 2003.
[25] A. Kittur, E. H. Chi, and B. Suh. Crowdsourcing user
studies with mechanical turk. In Proceedings of ACM
CHI’08, pages 453–456, 2008.
[26] C. L. Knott and M. S. James. An alternate approach to
developing a total celebrity endorser rating model using the
analytic hierarchy process. International Transactions in
Operational Research, 11(1):87–95, 2004.
[27] R. D. Luce. Individual Choice Behavior: A Theoretical
Analysis. Wiley, New York, 1959.
[28] J. N. S. Matthews and K. P. Morris. An application of
bradley-terry-type models to the measurement of pain.
Applied Statistics, 44:243–255, 1995.
[29] N. L. Powers and R. M. Pangborn. Paired comparison and
time-intensity measurements of the sensory properties of
beverages and gelatins containing sucrose or synthetic
sweeteners. Journal of Food Science, 43(1):41–46, 1978.
[30] P. Resnick, K. Kuwabara, R. Zeckhauser, and E. Friedman.
Reputation systems. Commun. ACM, 43(12):45–48, 2000.
[31] P. Rossi, Z. Gilula, and G. Allenby. Overcoming scale usage
heterogeneity: A bayesian hierarchical approach. Journal of
the American Statistical Association, 96(453):20–31, 2001.
[32] T. L. Saaty. A scaling method for priorities in hierarchical
structures. Journal of Mathematical Psychology,
15(3):234–281, 1977.
[33] M. V. Selm and N. W. Jankowski. Conducting online
surveys. Quality and Quantity, 40(3):435–456, 2006.
[34] A. Sorokin and D. Forsyth. Utility data annotation with
Amazon Mechanical Turk. In Computer Vision and
Pattern Recognition Workshops (CVPRW ’08), pages 1–8,
June 2008.
[35] S. Tasaka, H. Yoshimi, A. Hirashima, and T. Nunome. The
eﬀectiveness of a QoE-based video output scheme for
audio-video IP transmission. In Proceeding of ACM
Multimedia 2008, pages 259–268, Vancouver, Canada, 2008.
[36] A. Watson and M. A. Sasse. Measuring perceived quality of
speech and video in multimedia conferencing applications.
In Proceedings of ACM Multimedia 1998, pages 55–60.
ACM, 1998.
[37] K. B. Wright. Researching Internet-based populations:
Advantages and disadvantages of online survey research,
online questionnaire authoring software packages, and web
survey services. Journal of Computer-Mediated
Communication, 3(10), 2005.
接下來說明從我們收集到的資料中所觀察到的一些現象 1) Variability, Regularity, and 
Predictability 2) Spatial locality 3) Hosting dozens of games at the same time，一一介紹後帶
出了為什麼能達成前述的貢獻以及主要的研究方向「Concentrating and dispersing 
zone-based workload depend on server loading automatically」，之後也敘述了實驗方法、
實驗結果以及未來計劃。 
Conference 
“Change we are leading”是這次會議的主題，內容包含了 service computing, service level 
agreement, software, and virtualization of databases, networks, and hardware。而在 cloud 
computing 議題的目標在於提供一個 dynamically scalable, ubiquitous, and shared 
resources 給各式各樣的消費者及合作夥伴。 
What I have learned 
做學問必需非常嚴謹，因為一旦將研究發表後，如有任何瑕疵或提出了錯誤的結果，
都是很嚴重的事情。另外出國最重要的還是英語溝通的能力以及對研究領域的了解，只
有一切準備妥當了以後，才能有最好的收獲及迴響。 
98年度專題研究計畫研究成果彙整表 
計畫主持人：陳昇瑋 計畫編號：98-2221-E-001-017- 
計畫名稱：人際運算遊戲的理論建立與應用實作 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 2 0 100% 
人次 依照工作月份算
人力，實際上僅聘
任 1位專任助理，
聘期為 8個月。 
期刊論文 4 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 4 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
