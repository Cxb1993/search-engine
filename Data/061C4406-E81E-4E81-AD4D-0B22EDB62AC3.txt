2（一）計畫中文摘要：
關鍵詞：符號化影像，相似度檢索演算法，屬性關係圖
本計劃研究如何設計符號化影像的圖形表示方式，並發展一個演算法使其能在
符號化影像資料庫 (symbolic image database)上有效地進行影像的相似性檢索
(similarity retrieval) ，而相似性檢索是計算給定的查詢影像(query image)與資料庫影
像的相似度。我們使用影像中的高階特徵做為檢索依據，包括影像中所包含的物件，
以及物件之間的空間關係，這些特徵也被稱為語意資訊(semantic information)的一
種。這類查詢即是根據對影像物件的認知(包括物件類別及屬性)以及對物件之間空
間關係所做的推理。
在此計劃中，我們以 ARG(Attributed Relation Graph)為基礎將符號化影像表示
成一個「標籤圖形」(labeled graph)，使得這個標籤圖形能承載影像物件及空間型樣，
同時能夠在計算相似度時有較好的效率。此外，因為空間關係(spatial relation)除了
方向關係(directional relation)之外，還包括拓樸關係(topological relation)，因此我們
也將拓樸關係承載在此標籤圖形中，進而也納入相似度函數計算時的考量因素。
如此每張影像可以被表示成一個 ARG，並且在此 ARG 中標籤化頂點用來表示此圖形
中的物件，而標籤化的邊則是任一對物件間的空間關係。我們也提出基於 ARG 結構的演
算法來計算兩張影像的相似度，由於 ARG 結構的特性使得進行相似性檢索時具有較高的
效率。實驗證明我們所提的方法能支援有彈性的相似程度定義同時考慮方向關係與
拓樸關係，因此與其他方法比較具有較佳的檢索效率 (efficiency) 與效果
(effectiveness)。
4(一) 研究目的與文獻探討
Graphs are suitable for modeling a variety of real-world complex structures with the
benefits of being capable of characterizing the spatial, topological, and geometric properties
among the objects in the modeled data sets [1]. In many application areas, objects are symbolized
as vertices in a graph and the relationships between object pairs are represented as edges joining
the vertex pairs. In such graphs, each vertex is assigned a label to denote the object class and
each edge is assigned a label to convey the relation held between a pair of objects. For examples,
in a molecular structure an atom corresponds to a vertex and a bond between two atoms is
represented as an edge between two vertices. The vertices are assigned labels that stand for the
atom types (e.g. C, H) and the labels of edges are the bond types or the relative 3D orientations
between a pair of atoms in such structures [2], [3]. In geographical information systems (GIS),
objects or spatial elements are vertices and the relationships between object pairs are edges that
hold the information of adjacency, connectivity, and containment [4]. Another example is the
field of image retrieval, in which vertices denote the recognized objects in an image and edges
are the spatial or topological relationships between pairs of objects [5], [6], [7].
For the fields mentioned above, there are two major common characteristics among them. The
first common property is that labeled graphs seem to be the most appropriate representation for
modeling these applications. Second, one of the most interesting subjects among the applications
is to discover the recurring substructures. Taking the molecular biology as an example, mining
the frequent topological structures in order to identify novel RNAs or protein structures is vital
for researchers in this field [2], [3]. Finding the frequent common chemical substructures is also
helpful for toxicologists to classify new toxic substances [8]. There are still more applications
such as computer networks, pattern recognition, and browsing pattern analysis of the Web pages
that require mining frequent substructures.
The approaches to graph-based data mining are categorized by Washio and Motoda into five
groups [9], which are based on greedy search, inductive logic programming, inductive database,
mathematical graph theory, and kernel function, respectively. Among these approaches, the
mathematical graph theory based approach closely relates to the substance of graph based
modeling and thus gain much attention in these years. The recent work of graph theory based
methods include AGM (Apriori-based Graph Mining) [10], FSG (Frequent SubGraph discovery)
[1], gSpan (graph-based Substructure pattern mining) [11], and FFSM (Fast Frequent Subgraph
Mining) [8], etc. The common characteristics of these methods are briefly described as follows.
The first common feature is that each of these methods uses a certain kind of canonical label
6codes are identical.
As mentioned above, Ullmann proposed a backtracking method that significantly reduces
the size of the state space tree. In this method, a refinement procedure is employed that
eliminates unpromising successor nodes in the state space tree searching. It is still one of the
most commonly used methods for exact graph matching today [21].
Another outstanding algorithm VF2, also based on depth-first search (DFS) strategy,
employs some sophisticated feasibility rules that prune unpromising vertex pairs in the searching
on a state-space tree. According to the experimental results reported in [21] and [24], VF2
performs better for sparse or high structural regularity graphs. VF2, as well as Ullmann, can
work under the structural constraints as “synthetic rules” and label constraints as “semantic 
rules”. As an example of synthetic rules, if vertex v1 in graph G1 is mapped to vertex w1 in G2
and v2 mapped to w2, then the adjacency relation between (v1, v2) and (w1, w2) must be identical.
In addition, if label(v1) = label(w1), label(v2) = label(w2), and label(v1, v2) = label(w1, w2), where
label(v1) and label(v1, v2) are the labels of vertex v1 and edge (v1, v2), respectively, then this
mapping (v1 to w1 and v2 to w2) satisfies the semantic rule.
The work of FSG combines several types of vertex invariants to partition the vertices into
equivalence classes, by which the vertices in the same class have the same vertex invariants
such as vertex degrees, vertex labels, and vertex neighbor lists. In FSG, the canonical code of a
graph is defined to be the smallest code composed of the vertex labels followed by the
concatenation of the columns of the upper triangular adjacency matrix over all possible
permutations of the vertices. If the vertices of a graph with n vertices are partitioned into c
classes π1, π2,…, πc, then the number of different permutations need to be considered in order to
find the canonical code is Πi=1~c(|πi|!), which is substantially faster than the n! permutations
required by the earlier approaches [1]
83.1(b), respectively. Since image f1 has four different object symbols, the derived ARG G1 also
has four vertices. For an image with n objects, the number of vertices m is no greater than n, and
the number of arcs (including self-loops) is n(n-1)/2 since every object pair possesses a spatial
relation represented as an arc. Because the objects with the same symbol are represented by a
single vertex, the vertex labels do not duplicate among different vertices in an ARG. This feature
makes the manipulation of ARG more efficient than the ordinary labeled graphs with duplicated
vertex labels. The proposed method will utilize this advantage in finding the maximum common
subimage of two ARG’s efficiently.
A self-loop depicts a spatial relation between two objects with the same symbol, e.g., in Fig.
2(a) a self-loop with label SW of vertex A in G1 represents an object with symbol A is south-west
of another object also with symbol A in the original symbolic image f1. We say that in this case
one vertex is embedded in vertex A, i.e., m objects with the same symbol will be represented by
one vertex and m-1 invisible embedded vertices. We now define the size of an ARG.
Definition 1 The size of an ARG is the sum of the number of vertices, the number of arcs, and
the number of all embedded vertices.
We define the size of an ARG not merely by the number of vertices but also by the numbers
of arcs and embedded vertices. The number of embedded vertices is taken into account because
the embedded vertices stand for the invisible objects, and these objects should not be ignored.
The reason of including the number of arcs is that the arcs represent the spatial relations of object
pairs in an image, which contribute to the total similarity no less than multiple individual objects.
Actually, common objects in two images are the bases that span the intersection of the two
images, and the common spatial relations between the common object pairs (or common arcs
connecting the common vertices, from the view of graph) accumulate the similarity.
In summary, ARG is a labeled multi-graph with self-loops and without vertex label
duplication. It is easy to see that the time complexity of the building process of a derived ARG is
O(n2), where n is the number of objects in the original symbolic image. This is because the
number of the vertices m in the derived ARG is no greater than n, and the number of arcs in the
derived ARG is n(n–1)/2.
2. Similarity Computation of Two Symbolic Images
We say that a graph G = (V, E) is the common subgraph of two graphs G1 = (V1, E1) and G2
= (V2, E2) if there exist subgraph isomorphism from G to G1 and from G to G2. We call G a
maximum common subgraph (MCS) of G1 and G2, MCS(G1, G2), if there exists no other
common subgraph of G1 and G2 that is larger than G. Finding the MCS of two labeled graphs is
10
1
||||||
||
),(
1111
11
11 

GGG
G
GGsim .
From the perspective of similarity image retrieval, the similarity of two identical symbolic
images is 1.
3. 0sim (G1, G2)1, for any derived ARG pair G1 and G2.
It is obvious from our definition of G12 that 0|G12|min{|G1|, |G2|}. Hence,
(i)
||||||
||
),(
1221
12
21 GGG
G
GGsim


0
0||||
0
21



GG
According to steps of constructing the MCS of two derived ARG’s in section 2, |G12| = 0
means |V(G12)| = |E(G12)| = 0. Hence this equation depicts that the similarity of two ARG’s
without common vertices is 0.
(ii)
||||||
||
),(
1221
12
21 GGG
G
GGsim


|}||,min{|||||
|}||,min{|
2121
21
GGGG
GG

 (3)
if min{|G1|, |G2|} = |G1|, then 1
||
||
2
1 
G
G
,
and equation (3) becomes 1
||
||
||||||
||
),(
2
1
121
1
21 

G
G
GGG
G
GGsim
if min{|G1|, |G2|} = |G2|, then 1
||
||
1
2 
G
G
,
and equation (3) becomes 1
||
||
||||||
||
),(
1
2
221
2
21 

G
G
GGG
G
GGsim
The reason why the efficiency of ARG model is quadratic is that ARG model does not try to
find the associations of the objects between two images, even when some object symbols
duplicate. Recall that the worst-case time complexity is exponential when we intend to obtain a
mapping of objects between two images. Henceforth, unlike the methods in [16] and [25], the
duplications of object symbols do not degrade the efficiency of ARG model. Furthermore, in
ARG model the similarity of two images is accumulated by all type-i similar object pairs, thus no
spatial similarity contributed by object pairs will be ignored.
The spatial relations between a pair of objects include directional relation and topological
relation [16]. Egnehofer and Franzasa [15] defined eight fundamental topological relations:
disjoint, meet, contain, inside, overlap, cover, covered by, as shown in Fig. 3. Notice that disjoint,
meet, overlap, and equal are symmetric relations, e.g., if A meets B then B meets A. Besides,
relation contain is the complement of inside and cover is the complement of covered by, e.g., if A
contains B then B is inside A.
12
making these directional or topological common patterns. At last, the process of finding the
similarity of two images through ARGT is similar to that of the original ARG model; hence the
time complexity of ARGT model is also quadratic.
1. Experiments
In order to verify the efficiency and effectiveness of the proposed models ARG and ARGT,
we design two sets of experiments. Experiment 1 uses a synthetic dataset to record the average
comparison time of four methods, LCS_Clique, SIMR, 2D Be-string, and ARG for observing the
efficiencies of these methods. Experiment 2 employs a well-known image database which is used
in the Simplicity System [29], and the retrieval qualities of the above four methods and ARGT
are compared.
3.1. Experiment 1–Efficiency Test
A comparison is defined as the process of calculating the similarity between two images.
The synthetic dataset consists of 1000 symbolic images, and each image is represented by a
collection of objects with class symbols, x-axis/y-axis coordinates, and boundaries. The location
of each object is generated randomly within an 800600 image area. To observe how the number
of objects in an image can affect the performance of image comparisons, we equally divide the
1000 images into five groups, and the images in the same group have the same number of objects.
The images in group 1 have ten objects, in group 2 have fifteen objects, hence the number of
objects is increased by five to make the images in group 5 have thirty objects.
The images in each group are equally divided according to their symbol duplication rates,
and these rates are 0%, 20%, 40%, and 60%; hence each rate in a group contains 50 images. The
distribution of symbol duplication within each rate is random. For example of the rate 20%, an
image may have 20% of objects with symbol ‘A’ whereas another image may have 10% of ‘A’ 
and 10% of ‘B’. The purpose of designing symbol duplication is to simulate the real-world
behaviors of symbolic image databases since symbol repetitions occur frequently and in different
levels. A well-designed retrieval method is supposed to have steady average comparison time
under varying duplication rates.
Every image is taken in turn as the query image to be compared with all the images in the
same group including itself. Self-comparison is included because it happens frequently in image
retrievals that some images are very similar to or even the same as the query image. The average
comparison time for each group is obtained by averaging all the comparisons occurred within a
14
that ARG has significant efficiency improvement over the three well-known methods. Hence
ARG is especially suitable for the applications that have images with large numbers of objects.
3.2. Experiment 2–Effectiveness Test
The dataset for this experiment is the image database used in [29]. This dataset consists of
1000 images equally divided into 10 groups. The main themes of these ten groups are aborigines,
beaches, ancient buildings, buses, dinosaurs, elephants, flowers, horses, mountains, and foods in
the dish, respectively. Each image is preprocessed by us by a software-aided method with human
intervention to recognize the symbol, position, and boundaries of each object. The number of
objects in an image varies from 5 to 24 with average 13.2, and the symbol duplication rate in an
image varies from 0% to 82% with average 37%.
The Rnorm measure is used to assess the ranking quality here, which is also utilized by many
similarity image retrieval methods [5], [16], [25]. It is because in similarity image retrieval by
objects and spatial constraints, two images are relevant a bit as long as they have one or more
common objects. Thus any two images are “relevant”if their similarity degree is not zero.
However, we cannot say two text documents are relevant only if they have one common
keyword.
Calculating the Rnorm value of a given query requires two ranked lists of the database images
relative to this query image, where one list is from the domain expert and the other is from a
retrieval system. The values range from 0.0 to 1.0, and the higher values are for higher retrieval
qualities. Rnorm is defined as:
)1(
2
1
)(
max


S
SS
R sysnorm (4)
where S+ is the number of image pairs produced by a retrieval system that are in the correct
(same) order relative to the list produced by domain expert, S- is the number of image pairs that
are in the opposite order as those in the list of domain expert; and S+max is maximum possible
number of S+. As a simple example, consider three images I1, I2, and I3 are compared with a
given query image, and the lists from domain expert and from retrieval system are (I1, I2, I3) and
(I3, I1, I2), respectively. In this case, S+ is 1 because the relative position between I1 and I2 are the
same in both lists; S- is 2 because the relative positions between I1 - I3 and between I2 - I3 are
different in both lists. Since S+max is 3 for three image pairs. Thus the Rnorm value is (1+(1-2)/3)/2
= 0.33. For more examples of Rnorm measure, please refer to [5] or the appendix of [25].
For the five methods including LCS_Clique, SIMR, 2D Be-string, ARG, and ARGT, each
image is used as a query in turn to be compared with the images of the same group. For each
16
(三) 結論
For representation of symbolic images, models ARG and ARGT are proposed. The main
characteristics of the proposed models are their simplicity and efficiency. The proposed models
compare images by the contained objects and their spatial similarity. The time complexity is
quadratic in terms of the number of image objects by analysis, and the efficiency has been
verified by the experimental results. By constructing the spatial relation graphs of the compared
images, the similarity degree of these two symbolic images is efficiently assessed by calculating
the size of the maximum common subgraph of the two derived ARG’s. ARGT as the extension of
ARG considers topological relations as well as directional relations. The results of the conducted
experiments show that the efficiency of ARG and ARGT is steady when the number of image
objects increases. Besides, their efficiency is independent of the duplication rate of object
symbols. Therefore, the proposed models are especially suitable for the applications with the
images contain large numbers of objects. The retrieval quality has also been tested, and the
results show that the effectiveness of the proposed model is compatible with the three compared
well-known methods.
18
[16] A. V. Aho, J. E. Hopcroft, and J. D. Ullmann, The Design and Analysis of Computer
Algorithms, Addison-Wesley, Reading, Mass., 1974.
[17] J. E. Hopcroft and J. K. Wang,“Linear time algorithm for isomorphism of planar graphs,”
Annual ACM Symposium on Theory of Computing, pp.172-184, 1974.
[18] Z. Galil, C. M. Hoffmann, E. M. Luks, C. P. Schnorr, and A. Weber, “An O(n3log n) Las
Vegas isomorphism test for trivalent graphs,”Journal of ACM, Vol. 34, No. 3, pp513-531,
1987.
[19] D. Conte, P. Foggia, C. Sansone, and M. Vento, “Thirty Years of Graph Matching in 
Patern Recognition,” Int’l Journal of Patern Recognition and Artificial Inteligence, Vol. 18,
No. 3, pp. 265-298, 2004.
[20] L. Cordella, P. Foggia, C. Sansone, and M.Vento, “A (Sub)Graph Isomorphism Algorithm 
for Matching Large Graphs,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. 26, No. 10, pp. 1367-1372, 2004.
[21] P. Foggia, C. Sansone, and M.Vento, “A Performance Comparison of Five Algorithms for
Graph Isomorphism,” Proceedings of the 3rd IAPR-TC15 Workshop on Graph-based
Representation, 2001.
[22] B. D.McKay, “Nauty Users Guide,” available online at: http://cs.anu.edu.au /~bdm/nauty/,
2003.
[23] J. R. Ulmann, “An Algorithm for Subgraph Isomorphism,” Journal of ACM, Vol. 23, pp.
31-42, 1976.
[24] M. De Santo, P. Foggia, C. Sansone, and M.Vento, “A large database of graphs and its use 
for benchmarking graph isomorphism algorithms,” Pattern Recognition Letters, Vol. 24, pp.
1067-1079, 2003.
[25] G. Valiente, “Trading uninitialized space for time,” Information Processing Letters, Vol.
92, pp. 9-13, 2004.
[26] B. D. McKay, “Practical Graph Isomorphism,” Congressus Numerantium, Vol. 30, pp.
45-87, 1981.
[27] R. Neapolitan and K. Naimipour, Foundations of Algorithms, Massachusetts, D. C. Heath
and Company, 1996.
[28] W. D. Wallis, P. Shoubridge, M. Kraetz, andD. Ray, “Graph distances using graph union,” 
Pattern Recognition Letters Vol. 22, pp. 701-704, 2001.
[29] J. Z. Wang, J. Li, and G. Weiderhold, “Simplicity: Semantic Sensitive Integrated Matching 
for Picture Libraries,” IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol.
23, No. 9, pp. 947-963, 2001.
出席國際學術會議心得報告(一)
計畫編號 NSC97－2221－E－011－071
計畫名稱 以屬性關係圖為基礎的符號化影像檢索演算法
出國人員姓名
服務機關及職稱
徐俊傑
國立臺灣科技大學教授
會議時間地點
May 25~29, 2009
義大利羅馬
會議名稱 IEEE International Parallel & Distributed Processing Symposium
發表論文題目 Time-Efficient Power-Aware Scheduling for Periodic Real-Time Tasks
一、 參加會議經過
此次IEEE國際平行與分散式處理會議於2009年5月25日至29日於義大利羅馬舉
行，包含Workshop on Java and Components for Parallelism, Distribution and Concurrency，
Workshop on Nature Inspired Distributed Computing ， Workshop on High Performance
Computational Biology ， Advances in Parallel and Distributed Computing Models ， High-
Performance Power-Aware Computing，High Performance Grid Computing，Workshop on System
Management Techniques, Processes, and Services ….等11個workshop。參加者涵蓋全球從事
平行與分散式處理之研究人員與工程師，會議以口頭報告方式發表。主題除了有關平
行與分散式處理演算法、架構、軟體、與在各領域應用之外，還談到如何增進平行與
分散式系統計算效率、穩定度(stability)、容錯(fault tolerance)、最佳化
(optimization) 、排程與負載平衡(scheduling and load balancing) …等議題，
內容廣泛且豐富。另一個重點為格網 (Grid)架構研究與應用方面的論文很多，顯示
這方面的研究受到先進國家的重視。由於論文素質相當高，會場討論非常熱烈，會議
於5月29日圓滿結束。
出席國際學術會議心得報告(二)
計畫編號 NSC97－2221－E－011－071
計畫名稱 以屬性關係圖為基礎的符號化影像檢索演算法
出國人員姓名
服務機關及職稱
徐俊傑
國立臺灣科技大學教授
會議時間地點
October 12~15, 2008
美國聖地牙哥
會議名稱 IEEE International Conference on Image Processing
發表論文題目
一、參加會議經過
此次IEEE國際影像處理會議於2008年10月12日至15日於美國聖地牙哥舉行，參加
者涵蓋世界各地從事影像處理之研究人員與工程師，會議主題除了有關影像處理、編
碼、傳輸、儲存、擷取之外，還談到影像資料庫、文件影像處理與分析、生物醫學…
等議題，內容相當廣泛。個人除了聆聽影像處理方面的報告之外，主要還參與影像擷
取與應用方面的會議，並於各種場合與專家學者討論，以了解在這方面的研究現況，
會議於10月15日圓滿結束。
二、 與會心得
IEEE國際影像處理會議為影像處理與應用方面重要的國際會議，筆者從聆聽各場
次論文發表中，除了瞭解到各國在影像處理的研究成果，另外還特別著重影像擷取與
應用方面的研究現況與未來發展趨勢的瞭解。由於此次會議內容相當廣泛而深入，對
於個人在影像處理與應用方面的了解頗有助益，特別是影像索引與擷取方面的研究議
題與未來研究方向，更有莫大幫助。
proposed real-time scheduling algorithms. Finally, we
conclude in section 6.
2. Task Model
In this section, we introduce the assumptions and
notations, which are similar to those in [2, 5, 7]. We
focus our attention on synchronous, preemptive and
periodic task systems. In a periodic task set τ
={T1, … ,Tn} of n periodic real-time tasks, every task Ti
consists of a sequence of jobs Ji,1, Ji,2,…. A periodic task
Ti with an execution requirement Ti.e and a period Ti.p
has weight wi=Ti.e/Ti.p, where 0<wi<1. The release time
and deadline of each task Ti can be denoted as ri and di,
respectively. Because of synchronous task systems, the
release time of Ji,j is ri,j=(j-1)*Ti.p, and its deadline
denotes di,j=j*Ti.p. In this paper, every task period has
been transformed (specialized) as a harmonic by
algorithm Sr [7].
Task setτcontains all task in the system and its
subsets i = }and..|{ ** ikikk pppTpTT  , i' =
}|{ ** ikk ppT  and pTpTT ikki ..|{''  and }** ik pp  . U
and Uτ denote the total weights of a task set before and
after the period specialization[11], respectively. Tmin and
pmin denote the highest priority task and the shortest
period length among the tasks in τ, respectively.
All tasks are scheduled according to preemptive
RM policy. A task Ti in this schedule may consists of PIi,
HEi, REi and TEi, which denote the preemption interval,
head execution, resuming execution and tail execution of
Ti, respectively. Notably, TEi denotes the last REi in each
job of Ti. As shown in Figure 1(b), since the lengths of
task periods are harmonic, every job of task Ti has fixed
starting time Si and ending time Ei with respect to its job
release time. FITi denotes the first preemption time with
respect to its job release in task Ti. Since Si, Ei and FITi
are fixed in every job of Ti, the lengths of TEi, HEi and
REi of the jobs are also fixed. MaxREi, MREi and MaxPIi
denote the lengths of the maximum, minimum resuming
execution and maximum preemption interval,
respectively, of every job. Note that MREi excludes the
length of TEi. The important properties of these
parameters are discussed in Sec. 4.
The power/speed models are similar to those
proposed in [2]. The current processor speed at which Ti
is executing is denoted by
i' . The initial (default) speed
of Ti is denoted by ρi. In the beginning of every task, the
operating system sets ii'  , prior to any dynamic speed
adjustment. ECi,j denotes the early completion time
occurred in job Ji,j. The power consumption of the CPU
under the speed ρi is given by P(ρi), which is assumed
to be a strictly increasing convex function denoted by a
polynomial function of the second degree. The energy
consumed by processor during the time interval [t1, t2] is
 21 ))((),( 21
t
t
dttPttE  . Given that the processor speed can
be changed between a minimum speed ρ min (the
minimum supply voltage necessary to keep the processor
working) and a maximum speed ρmax. We also assume
that ρmax can be normalized within interval [0,1] (i.e. 0
≦ρmin≦ρmax≦1). In our framework, speed/voltage
assignments are determined at job-level [8]. Although
dynamic adjustments are performed, these occur only at
task dispatch or completion times.
Figure 1. (a)Periodic and (b) jitterless schedule
3. Smoothing Voltage Adjusting
In the sections 3.1 and 3.2, we propose the
framework of SVA based on the properties to be
proposed in Section 4.
Figure 2. bnP3,1 and anP3,1 after EC3,1=4.0
3.1 Additional System Information
Before presenting SVA scheme, we assume that the
following system information can be produced during
task execution in constant time.
ECi,j : denotes the absolute early completion time of Ji,j.
RE_counteri,j : denotes the number of REi that has
elapsed before ECi,j.
total_REi : denotes the total number of RE in every job of
Ti.
rest_REi,j : denotes the rest number of REi,js after ECi,j is
at least }.__,0max{ , jii counterREREtotal 
 

ekj Tpk ,,  

Figure 7. Algorithm Lower_Priority_Task
3.2.3 Lower_Prioirty_Task(Ji,j, bnPi,j)
In Figure 7, the algorithm presents an efficient way
of sharing bnPi,j among the tasks in 'i . For any ECi,j,
because there are at most n jobs can be released, line 1
takes O(n) time. Similarly, from line 2 to line 7 also takes
linear time with size n.
Figure 8. The example of Lower_Priority Task
Figure 9. The example of(a) RM and (b) SVA scheme
Notably, Sk,p and ρk,p denote the temporary starting
time and the speed of Jk,p, respectively. The algorithm
mainly applies HEs (or bnPk,p) and gathers the jobs of
which the starting times lie between ECi,j and ri,j+Ei.
Therefore, the jobs in δ as well as in ready queue have
to extend their execution times by advancing their
original starting times. However, the starting time and the
speed of their successor job have to be restored to Sk and
ρk, respectively.
3.2.4 Higher_Priority_Task
When Ji,j completes, the lengths of HEi, TEi, MREi
and MaxPIi can be obtained efficiently in Section 4. Let
t
idenote the job set consisting of all jobs that release at
time t and are eligible for speed reduction by using anPi,j.
Since rk,p (Jk,p ti ) is a multiple of pmin, we need to
consider the speed-adjustment only at time t= rk,p after
ECi,j.
In Figure 10, from line 3 to line 9, it deals with the
jobs that release from time t (denotes the multiplier of
pmin), and their periods do not overlap with TEi,j. Because
it is time-wasted to obtain the actual lengths of every REi,j
in Ti, we should predict the lengths of REi,j more
efficiently by deriving MREi in line 6. From line 10 to
line 16, because the actual length of the rest of TEi,j can
be obtained by t jiexecrest ,_ , the ending time of the jobs
in ',
t
ji can be postponed by sharing t jiexecrest ,_ . Notably,
from line 3 to line 10 in Figure 10, it will be awaked
whenever the starting time t has been reached.
Figure 10. Algorithm Higher_Priority_Task
the length of MREi can be proved easily when
MaxPIi<pmin. When MaxPIi≧pmin and MaxPIi starts at
time t, Ti has to be resumed at time t+
i
xE . Moreover,
since the lengths of all task periods are harmonic, there is
no jitter in every pmin period. Therefore, any REi
performed in the time interval [t+ ixE ,
i
i
MaxPI
p
MaxPI
p 



min
min ] cannot be preempted by the
other tasks inτ. □
Figure 12. The average number of RE in a task
Lemma 4. (The lengths of FITi)
Let Ti-Tmin and Ti.e<Ei-Si, the first interruption time of
every job in Ti can be written as
.min
min
p
p
S
FIT
i
i 



After obtaining FITi, the lengths of the head execution of
Ti can be written as
.iii SFITHE 
Lemma 5. If Tiand ixE ≠pmin×m, m=1, 2, 3, …, the
length of TEi is at least max{0, Ei- ixE }.
In virtue of the period specialization, for any pair of
tasks Ti, Tkτ , and ** ki pp  , they have to satisfy
2.
. pT
pT
k
i , where α=0, 1, 2,…. For all Ei, Proposition
6 shown below finds the shortest task period Tk.p such
that
Tk.p>Ei.
Proposition 6: The relative ending time of Ti should be
covered by a task period with length at least
 
.min
)/lg( min2 pp pEE
i
i

If the length
iEp does not exist among the tasks in τ, the
minimum Tk.p≧ iEp can be found in O(log n) time by
using a binary search technique, where n denotes the size
of τ.
Lemma 7: Given Ti.e<Ei-Si, the total number of RE in
every job of Ti is at least





 
iE
ii
i
p'
FITE
REtotal _ …………………(1)
where
iEp' denotes the shortest task period inτ such that
Tk.p≧ iEp .
4.3 The experiment results of HE, TE, RE, MRE and
MaxRE
In this section, we discuss how frequently and how
much these parameters RE, HE, TE, MRE and MaxRE
produced in DCTS schedule. We execute DCTS-RM
schedules for a number of randomly generated task sets.
Every size of task sets in x-axis is performed 20,000
times. To avoid un-schedulable scenarios after period
specialization (i.e. U>1), we choose the U of with size
n is less or equal to its least lower bound )12( /1  nn .
Figure 12 shows the average number of RE (including
MRE and MaxRE) per task versus the sizes of task set. It
also reveals how rarely the RE takes place. For example,
when the size of a task set is 22, the probability that RE
appears in a task is 2‰. This implies that it is
unnecessary to spend much time on computing the actual
lengths of RE.
Figure 13 shows the average lengths of HE, TE,
MRE and MaxRE per task versus the size of task set. For
example, in Figure 13(a), when the size of task set is 11,
the average length of HE per task is 1.8 time unit. The
average lengths of HE maintain at about 1.7 when the
size of is less than 21. Comparing with the lengths of
TE and MaxRE in Figure 13(b) and Figure 13(c)
respectively, the average lengths of HE are much greater
than those of TE and MaxRE, and thus are dominant the
length of job executions. Moreover, the average length of
TE is the second longest execution and greater than
MaxRE. This implies that a substantial energy-saving can
be obtained if the lengths of the rest of TE or HE can be
derived precisely in an early completion job. As for the
lengths of RE, Figure 13(c) presents that its tiny spread
between MRE and MaxRE. In the figure, MaxRE is very
close to MRE and not more than 0.11. Comparing Figure
13(c) with Figure 12, we conclude that the lengths of RE
is insignificant compared to those of HE and TE.
Saving_bound denotes a clairvoyant scheme that can
recognize the actual energy-saving in advance.
Figure 15. The average energy saving of Lower_Priority_Task
and Higher_Priority_Task
Figure 16. The total energy saving of given schemes
5.2.1. The performance of Lower_priority_Task and
Higher_priority_Task.
In the section, we evaluate the energy saving
produced by Lower_priority_Task and Higher_priority_
Task, separately. In Figure 14, the total energy saving
with respect to Lower_priority_Task and Higher_
priority_Task are shown as a function of the sizes of the
task set. Lower_priority_Task provides better
performance then Higher_priority_Task does especially
in the larger task sets. In Lower_priority_Task, the longer
the size of task set, the more energy saving it will obtain.
The energy saving of StaticOPT and Higher_priority_
Task are rather insensitive to the variations in the sizes of
. This is due to the fact that, in Figure 13(b) and Figure
13(c), the actual lengths of TE and RE are relatively
shorter than those of HE, and the use of MRE which to
predict the actual lengths of RE in the proposed scheme
may underestimate the actual available slack time in RE.
The total energy saving of SVA, illustrated as the SVA+
StaticOPT in Figure 14, is obtained by summing up those
of Lower_priority_Task, Higher_priority_Task and
StaticOPT. The average energy saving of tasks is shown
in Figure 15. Note that the average saving of StaticOPT is
obtained by dividing StaticOPT by the number of tasks in
, where U<1. Nevertheless, the average saving of
Lower_priority_Task and Higher_priority_Task is
obtained by dividing Lower_priority_Task and
Higher_priority_Task respectively by the number of
early-completion jobs in a system. The results point out
an advantage that the average saving earned by
Lower_priority_Task and Higher_priority_Task is kept
in the steady levels (around 15 and 5, respectively)
throughout the spectrum.
5.2.2 Effect of the sizes of.
Figure 16 presents the total energy saving of the
techniques varying with the sizes of , when the
probability of the early-completion for a job is 0.5. We
change the sizes of in this experiment between 2 tasks
and 20 tasks for every . The reason for presenting the
total energy saving of given schemes is that the tasks
using static speed reduction (StaticOPT) do not
necessarily complete early during their execution times.
Therefore, the average results may prejudice the
performance of StaticOPT. We have the following
observations:
5. Conclusions
In this paper, we propose a fast scheme for power-
aware hard real-time scheduling through dynamic voltage
scaling. To the best of our knowledge, the previous work
in the literature [1, 2, 8, 9, 14, 17] has to generate a
canonical (LCM) schedule before tasks execution.
Instead of generating an exponential time schedule in
advance, our scheme only derive the parameters of the
tasks (HE, TE, MRE, ect.) in O(n log n) time off-line and
perform speed reduction on-line only at job’s starting and
early-completion time.
In addition, SVA agreeing with the concept in [15,
16] attempts to reduce simultaneously the tasks’speeds
in the same level and leads a new breakthrough with
further energy saving. In Figure 14 and Figure 15, SVA
can save up to 70 percent of the energy over static
algorithm. It also offers an advantage over other state-of-
the-art intertask voltage scheduling schemes such as
DRA and AGR[1]. Finally, our scheme outperforms DRA
and AGR up to 25 and 20 percent of energy saving in the
considerable size of.
Reference
[1] Hakan Aydin, Rami Melhem, Daniel Mosse, and Pedro
Mejia-Avarez, “Power-Aware Scheduling for Periodic Real-
Time Tasks,” IEEE Trans. Computers, Vol. 53, no. 5, pp.
584-600 May 2004.
[2] Hakan Aydin, Rami Melhem, Daniel Mosse, and Pedro
Mejia-Avarez, “Dynamic and Aggressive Power-Aware
Scheduling Techniques for Real-Time Systems,” 
Proceedings of the 22nd IEEE Real-time Systems
Symposium (RTSS'01) . London, England, December 2001.
