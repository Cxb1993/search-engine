壹、 中英文摘要及關鍵字 
一、 中文摘要 
本研究延續前兩年專案「SOA新聞知識萃取與展現」，除了持續優化摘要處
理外，增加三項服務，分別為新聞品質評估(News Quality Evaluation, NQE)、新
聞推薦 (News Recommender, NR)，以及個人新聞應用程式 (Personal News 
Application, PNA)，期能有效改善新聞的品質，讓使用者可以不受限於時間與地
點，獲得客製化的視訊新聞。研究中，NQE 利用先前研發的自動摘要處理服務
(News Document Summerization, NDS)所萃取出的多文件摘要，結合相似度分析，
比較各報社同主題之新聞文件，其相似度愈高及表示新聞品質愈優良，並將結果
排行。NR 則結合內容與協同式過濾方式，分析讀者個人喜好與閱讀習慣，並透
過同類型讀者推薦，主動推播個人化新聞給讀者。而 PNA 實屬整合型軟體服務，
提供推薦新聞與閱讀相關新聞，一個化繁為簡、直覺化的操作介面，更利於手持
式裝置閱讀新聞時能得心應手。 
關鍵詞：新聞知識、新聞品質評估、新聞推薦、混和過濾 
二、 英文摘要 
This study based on our previous studies of Service-Oriented Architecture with 
the addition of News Quality Evaluation (NQE), News Recommender (NR), and 
Personal News Application (PNA) is to enhance news quality and provide 
customized news to users via pc and mobile devices, so that users can receive 
real-time news and Multimedia report whenever and wherever.  In the study, the 
NQE service is designed to determine news quality for the same event based on 
multiple document summarization and similarity comparison. The higher the 
similarity is;  the higher the rank is presumed to be.  A rank list will be 
published regularly. On the other hand, the NR service combines both 
content-based and collaborative filtering to provide a personalized news delivery 
service. The most important purpose of PNA is to provide a simple, fast, 
good-to-use, and intuitive interface for the users; moreover, it is expected to  
have a more convenient platform to read and share news  to other users with the 
same interests. 
Keywords: News Knowledge, News Quality Evaluation, News Recommender, 
Hibrid Approach 
  
為： 
 提升新聞報導品質，希透過多文件摘要比對同事件個別新聞撰寫內容，
客觀評定報社品質，讓讀者獲得質量皆佳之新聞內容。 
 為降低資訊過載，希結合內容取向與協同式過濾方式，建構合宜的新聞
推薦機制，提供讀者最適切的新聞主動推播服務。甚者，藉由相互分享
推薦共同之新聞文件，期望達到知識分享之綜效。 
 以圖形化的介面設計，強化使用者操作時閱讀上的可讀性、理解性、以
及新聞推薦時的便利性。 
 
3. 文獻探討 
3.1. 字詞處理技術 
在資訊檢索中，對於文件的處理需要使用字詞處理技術來分析文件，藉以篩
選出能代表該文件的特徵（feature）或關鍵字詞。目前針對中文斷詞處理，主要
分為詞庫斷詞法、N-Gram 選詞法與混合斷詞法[1, 2]。 
詞庫斷詞法亦即藉由事先建置好之詞庫對文件進行字詞分析；N-Gram 選詞
法則是將文件內容以二字詞(2-Gram, Bi-Gram)、三字詞(3-Gram, Tri-Gram)逐字進
行分割，若斷出字詞之詞頻高於門檻值，則視為一獨立字詞；混合斷詞法則是結
合以上兩種斷詞法，亦即先利用詞庫斷詞，再透過 N-Gram 找出剩下的新詞。斷
詞後的字詞群，要篩選出具代表性者，仍需計算該字詞在文件中的權重。 
字詞權重的給定是藉由計算該字詞在單一文件的重要性（local weight）及在
整個文件集的重要性（global weight）而來。目前最常使用的字詞權重計算方式
為 TFIDF（Term Frequency * Inverse Document Frequency），強調字詞在單一文件
出現次數愈多，且在總文件集分佈愈稀疏，則重要性愈高。 
 
3.2. 中文字詞分析 
中文文字不若西文文字有明顯的空白可以將句子中的各個字詞做區分，因此
中文關鍵詞擷取一直是資訊檢索領域發展的瓶頸。而中文斷詞最大的問題就是斷
詞組合的岐義性(ambiguity)、複合詞研究以及未知詞問題[3]。 
其中在近期許多的研究中，最常見的方法為詞庫斷詞法，大多皆由中央研究
院的斷詞系統 Chinese Knowledge and Information Processing (CKIP)之八萬字目
庫，將文章做斷詞後取出文章關鍵字，但是由於斷詞傳輸時易阻塞，且所斷出的
字詞又細又瑣碎，將原本重要的關鍵字淹沒，例如：世界衛生大會（WHA）即
將在 5月召開，使用CKIP斷詞後會出現，[世界] [衛生] [大會] [WHA] [即將][在][5
月][召開]如果要將 CKIP 詞庫所斷出之字詞，做進一步的編製，還必頇將結果做
篩選和過濾，因而增加後續資訊過濾的負擔。 
聯越高，此法是依段落原本在文章內的順序和連結個數的多寡來進行排
列的。把 bushy path 定義出來之後便從其中挑選出排行最高的 K 個段落
(Top K)即可作為摘要。 
 Depth First Path：在 bushy path 上的每一個節點都往外連接了一定數量
的節點，但是對於其他節點來說卻非絕對必要，因此當必頇對一個文件
產生摘要的時候，可能會影響到其結果的可讀性(readability)，為了避免
這個問題，Depth First Path 將會從一個重要的節點開始，典型的選擇是
第一個或是擁有最高連接數的節點，接下來再尋找最有相關的節點，以
此法繼續同理類推即可得到可讀性高的摘要了。 
 
3.4. 混合過濾 
混合過濾結合了協同過濾(collaborative filtering)以及內容式過濾(content 
based filtering)這兩種過濾方法，進行推薦[11]。新聞本身所具備的特徵屬性，和
眾多使用者對於新聞文章的閱讀經驗，對於推薦系統而言，是相當重要的資訊。
但是這些資訊當中，有些例如個人的「品味」、對於新聞喜好的「概念」，是系統
本身難以生成的訊息。所以透過協同式過濾方法，能夠共用不同使用者的經驗，
並且有利於推薦系統學習使用者喜好。探討內容式過濾的重點，它在於分析新聞
之間，相互關聯的特徵值，進而找出共同的關聯[12]，補充協同過濾不足的部分。
它亦為一種比較技術，主要針對每篇新聞的內容進行分析，透過統計方法，計算
出每篇新聞文章的相似度，經常使用貝氏定理來達到此一目的[11]。 
 
3.5. Android 平台 
為發展個人新聞應用程式，平台選擇上，我們評估了 iPhone、Windows Mobile 
6.0 及 Android；iPhone 在手機功能出發點較為人性化，系統執行速度快；而
Windows Mobile 6.0 是將視窗化的系統移植到手機上，執行速度略慢，使用貣來
較不方便。且 iPhone 的軟體開發自由度相較於 Windows Mobile 6.0 來說是較高
的。在 Open source 的風潮下，願意花錢買軟體的人越來越少，而近兩年較為熱
門的 Android，兼具以上兩個開發平台的優點，且手機的購買價格上也較低廉，
故我們選擇 Android 平台。 
 
4. 研究方法 
4.1. 系統建置方式 
如圖 2 所示，綠色以及紅色虛線的部份是先前所研發的系統範圍，該部分
包括執行下載新聞網站之新聞文件，再將文件進行斷句、斷詞、詞性標注、權重
計算與事件分群等處理，讓原本分散於網路上各新聞網站之新聞文件，能進一步
透過字詞與相似度的處理形成事件群集，繼而產出多文件摘要。 
4.2.1 新聞品質評估 
新聞品質評估的部分當中，如，新聞文件來源自 Yahoo!奇摩新聞
(http://tw.news.yahoo.com)。每篇新聞文件會先執行中文詞庫斷詞以擷取代表此篇
新聞的關鍵字詞組，並利用 Bushy Path 來抽取出此篇新聞的摘要。每篇新聞皆有
其代表的摘要及關鍵字詞組，依照此主題的多文件摘要來和各文件的單篇摘要做
相似度比較，相似度越高即是品質越好的新聞，再將品質最好的新聞進行排名。 
多文件摘要
文件A 文件B 文件C 文件D
NQE Service
相似度演算法
文件E
1. 文件C
2. 文件E
3. 文件B
4. 文件A
5. 文件D
 
圖 3 新聞品質評估方式 
 相似度計算 
本研究的研究架構如下錯誤! 找不到參照來源。所示，圖中新聞文件來源自
Yahoo!奇摩新聞(http://tw.news.yahoo.com)。每篇新聞文件會先執行中文詞庫斷詞
以擷取代表此篇新聞的關鍵字詞組，並利用 Bushy Path 來抽取出此篇新聞的摘要。
每篇新聞皆有其代表的摘要及關鍵字詞組，為加速分群速度，我們以關鍵字詞組
為分群判斷的依據。分群後我們將剔除群內文件數量小於 2 的群組，並將這些文
件數量不足的群組歸類於同一群。剩下的群組我們執行多文件摘要的擷取。當有
新的文件被存入時，系統會將此文件先與原本的群組判斷是否為相關新聞，若是
相關新聞則會納入相似度最高的群組中。若不是則歸類為其他未分類的群組中。 
每篇文件會先擷取出其摘要，使用方法係將文件內容依其句點來斷句。
𝑑𝑠𝑖 = (𝑠1, 𝑠2, … , 𝑠𝑗)，此處的𝑑𝑖為第𝑖篇文件，𝑠𝑗為第𝑖篇文件中的第𝑗段句子。斷句
後再進行斷詞作業(CCS Service)。𝑠𝑗 = (𝑡𝑗1 , 𝑡𝑗2 , … , 𝑡𝑗𝑘)，此處的𝑡𝑗𝑘為第𝑗段中的第𝑘
個字詞。之後使用 Jaccard Index 來判斷句子間的相似度，其相似度的判斷公式如
下： 
𝑠𝑖𝑚(𝑠𝑖 , 𝑠𝑗) = 𝑐𝑜𝑠𝜃 =
𝑑𝑖𝑑𝑗
|𝑑𝑖||𝑑𝑗|
= ∑
(𝑡𝑘(𝑠𝑖) ∩ 𝑡𝑘(𝑠𝑗))
(𝑠𝑖 ∪ 𝑠𝑗)
𝑛
𝑘=0
 (1) 
 
4.2.2 新聞推薦機制 
新聞特徵感興趣；”0”代表該使用者對於某一新聞特徵不感興趣。 
表 3 使用者偏好表 
User       Feature f1 f2 f3 … f10 
u1 0 1 0  1 
u2 1 1 1  0 
u3 0 0 0  1 
…
      
un 0 0 1 … 1 
 
系統除了計算新聞特徵值相似度之外，另外會透過每一使用者對於已讀過之
新聞文章評價結果，得到一使用者對於閱讀過的新聞評價矩陣。假設一位使用者
閱讀過 N 篇文章， 系統會找出每一篇新聞出現在個別使
用者閱讀記錄當中，出現的次數。每位使用者 U 以共有 m 位而成的集合，
如下表所示： 
表 4 使用者偏好表 
User       News n1 n2 n3 … nk 
u1 1 0 0  1 
u2 0 1 1  0 
u3 1 0 0  1 
…
      
un 0 1 1 … 1 
 
總結以上，當新聞推薦運行時，系統會先詢問使用者喜歡何種類型的新聞文
章，藉此取得每一位使用者閱讀新聞的喜好。透過此一資訊，內容式過濾會將每
一篇新的新聞文章與使用者的喜好做一比較，分析判斷彼此的相關性。使用者再
依照自己的喜愛，勾選新聞類型。往後系統找到類似的新聞，就會被推薦出來。 
  
 圖 6 新聞報社排行 
 
5.2. 新聞推薦系統 
 
 
新聞推薦系統的使用者介面如所示，此為開創帳號的畫面，系統會在這裡第
一次接觸到使用者的資料以及喜好，並把這些資訊存入資料庫當中。系統記錄使
用者屬性之後，這將會與 content-based filtering 做結合。 
 
圖 7 新聞推薦圖 
在新聞推薦的使用者介面，由於顧及到往後本服務將轉移到手機上，系統將
只呈現最新的新聞給登入系統的使用者以節省頁面空間。當使用者按下按我推薦
後，系統會記錄他閱讀過哪篇新聞。而右下方的相關新聞，是由系統內計算新聞
文章相似度的模組 NQE 所提供。 
  
6. 結論與未來研究方向 
由於網際網路便利，電子新聞已成為現代人獲取新知及了解世事最佳的管道，
為解決資訊過載，提供知識輕量化服務，本研究多年來致力於網路新聞文件處理，
藉由電子新聞閱讀平台的建置，整合事件偵測與追蹤及多文件摘要技術，發展出
視覺化主題地圖，以協助了解整個新聞事件的來龍去脈，並擴及手機平台展現網
頁，完整的讓人們能隨時快速的得到最正確、最容易理解的資訊。本年度除了持
續優化摘要處理外，並增加 NQE, NR 以及 PNA，期能有效改善新聞的品質，讓
使用者可以不受限於時間與地點，透過 pc 或 Android 手機均可獲得客製化的視
訊新聞。 
本實驗結果雖已提出創新成果，未來仍有許多值得再加強及深入研發的部分，
茲列示於下： 
 本研究並未處理同義詞，未來可考量將同義詞的辨析，加入字詞剖析部
分，或可降低文件間不同作者採用不同修飾詞的問題，對於群聚的相似
度比對與主題地圖的關聯式擷取應有正面的效果。 
 本研究雖以句子結構、字詞間的距離與權重擷取出關聯式，惟仍缺乏句
子語法與語意之判斷，可能造成所截取出的關聯式無法確切的表達原句
的實際意義，因此若要能讓關聯式準確的表達出原中文句意，未來在語
意及語法的判斷，仍有進一步研討的必要。 
 在新聞推薦系統的部分，期望未來能夠提出更有效率的推薦演算法，縮
短系統回應時間。並且改良推薦演算法，提高推薦出來的新聞文章更能
符合使用者期待。 
  
貳、 計劃成果自評 
藉由國科會工程處之自由軟體研發推動專案，本研究計劃得以透過開放原始
碼之機制，為建構自由軟體社群的風氣與環境盡一份心力。茲將本研究計劃成果
與貢獻整理如下: 
1. 有效評估不同報導來源的相關新聞事件，供瀏覽比較分析： 
透過本研究的新聞品質評估系統，可讓讀者即時掌握該事件最新發展動
態，及回顧該事件在各報社筆下的情形。新聞摘要不再僅是片斷零星的
組合，而是完整的多文件新聞摘要結果。 
2. 結合使用者喜好與群組式的新聞技術： 
本研究之新聞推薦系統之架構，可讓使用者推薦新聞給擁有相似喜好的
使用者群組，讓使用者更方便瀏覽新聞要項；簡化繁瑣的 WEB2.0 知識
分享精神，改以簡單的推薦動作來完成。 
3. 友善的直覺式圖形化使用者介面： 
透過本研究個人新聞應用程式架構，可有效彙整讀者所需的新聞瀏覽操
作以及推薦動作，方便使用者推薦新聞以及閱讀相關新聞。 
  
英文： 
This study proposed three services: News Quality Evaluation 
(NQE), News Recommender (NR), and Personal News 
Application (PNA) to enhance news quality and provide 
customized news to users via pc and mobile devices, so that 
users can receive real-time news and Multimedia report 
whenever and wherever. In the study, the NQE service is 
designed to determine news quality for the same event based 
on multiple document summarization and similarity 
comparison.  A rank list will be published regularly. On the 
other hand, the NR service combines both content-based 
analysis and collaborative filtering to provide a personalized 
news delivery service. The most important purpose of PNA is 
to provide a simple, fast, good-to-use, and intuitive interface 
for the users; moreover, it is expected to  have a more 
convenient platform to read and share news  to other users 
with the same interests. 
 
可利用之產業 
及 
可開發之產品 
 推薦系統應用 
藉由新聞推薦機制，讓使用者在閱讀新聞文章的同時，也能分
享彼此的閱讀經驗，達到知識的共享。此一技術同時亦可應用
於書目推薦、電子商務等。 
 有效匯整不同報導來源的相關新聞，供瀏覽比較分析： 
透過本研究的即時新聞，可讓讀者即時掌握各類新聞最新發展
動態，例如可讓商務人士隨時隨地得知各家新聞社最新的商業
消息，掌握時機。 
 觀光主題地圖應用： 
本研究可利用地名發展觀光主題地圖，與電子地圖業者進行技
術合作，除提供地方的旅遊資訊服務，使電子地圖能串連當地
的新聞在電子地圖上，結合觀光廣告，提供使用者當地的最新
消息以及業者可透過此平台做即時促銷活動，使用者可透過系
統知道身處的位置，何處有活動，有甚麼消息。 
 掌握最新影響投資的事件，做出有利於投資或決策的判斷： 
本研究成果除適用於新聞網站外，也適用於各種領域之文件整
理，尤其可開發為投資理財網站附加價值的產品。對於決策者
而言，採用本研究機制，可協助其有效蒐集相關論題的新聞或
文件做為決策判斷參考；對於投資者而言，可快速協助其掌握
最新影響投資的新聞，適時做出有利於投資或決策的判斷。 
  
出席 2010 DAMA 報告書 –Singapore, July 12-13 
出席國際會議報告書 
黃純敏 Chuen-min Huang 
國立雲林科技大學資管系副教授 
一、目的 
（一）計畫案研究目的 
網路時代的來臨帶來許多生活上的便利，使得上網人口激增。閱讀網路
新聞也已經成為日常生活獲取知識重要的一部分。然而新聞事件常因記者政
治立場不同或專業素養不一，使得同一事件有不同的報導。讀者如想客觀了
解實況，必須查閱不同報社報導，十分費時。此外，新聞有其延續發展性的
特點，同一事件常延續數日甚至更久。目前各家新聞平台並沒有事件追蹤機
制，導致讀者想獲知事件發展全貌，有其實質的困難。鑒於上述問題，本人
多年來致力於網路新聞文件之處理，目的在於提升新聞事件認知及縮短知識
內化的時間，結合智慧型手機加值軟體，為嵌入式系統投入更多創意構思與
實作成品，藉以提升文化水平，邁向智慧生活的社會。本計畫案主題：「SOA
架構知識萃取與展現」，為第三年計畫，本年度研究目的為：（一）提升新
聞報導品質，希透過多文件摘要比對同事件個別新聞撰寫內容，客觀評定報
社品質，讓讀者獲得質量皆佳之新聞內容；（二）為降低資訊過載，希結合
內容取向與協同式過濾方式，建構合宜的新聞推薦機制，提供讀者最適切的
新聞主動推播服務；（三）以圖形化的介面設計，強化使用者操作時閱讀上
的可讀性、理解性、以及新聞推薦時的便利性。 
（二）DAMA簡介：  
DAMA (Data Analysis, Data Quality & Metadata Management) 為GSTF 
(Global Science and Technology Forum)此一組織年度舉辦的研討會之一。
GSTF是亞太地區有名的企業策略管理顧問團體，為大學及研究機構提供產學
合作諮詢。該組織自2008年起積極舉辦研討會，包括：CGAT(Computer Game 
& Allied Technology), CCV(Cloud Computing and Virtualization Industrial 
Conference), BIDW(Business Intelligence & Data Warehousing) / DAMA。並出
版三種學術期刊： International Journal on Computing, International Journal on 
Bio informatics and Biotechnology, and Global Business Review。DAMA 
Conference主要研討議題著重於資料分析、資料品質、資料庫管理等，從資
訊管理觀點出發，並以提昇組織效能、強化組織優勢為目的。被接受的論文
將收錄於Asia Pacific Technology Forum (APTF) Conference Proceedings (紙本
及光碟版)。DAMD Conference 的論文也都被納入知名索引，包括ISI web of 
science and Elveseir SCOPUS 以及其他相關索引。本人希藉由參與此研討
會，除分享研究成果外，並透過面對面與國外研究同好意見交換，獲取不同
面向的建議。 
 
  
 
 
圖1 團體合照 
 
 
第二天，本人參與閉幕前的 Panel Discussion，此座談會於 4:30 – 5:30 p.m.在
Mandarin Suite 831進行，主題包括： Future of Business Intelligence, Cutting 
Edge Technologies for BIDW and DAMD, and Obstacles in Integrating Data 
Analysis (Mathematics) and Information Technology.由 Editor-In-Chief Prof. 
Kuldeep Kumar主持，本人(Assoc. Prof. Chuen-Min Huang)與其他三位 Session 
Chair (Assoc. Prof. Narasimha Bolloju, Prof. Mike Lin, Dr. Liwan Liyanage)共同
擔任 Panelist(如圖 2) 
 
  
四、建議  
 
參與國際研討會需準備事情繁多，從擬訂參與國際研討會起，前置作業的
論文擬題、寫英文稿、上網投稿；審查作業期間的耐心等待；直到通知論文
被接受，隨後，依照指示提交英文版全文，並按照規定，仔細檢查論文格式，
上網提交論文全文；註冊交費，以及後續的出國準備、旅程的安排、訂機票、
旅館；到現場的報告、參與各場次研討等，是一種時程的控管與學習成長的
訓練。由於國科會僅補助教師及博士班學生參與國際會議，卻將碩士生排
外，希望能鼓勵優秀碩士生也有提昇國際觀及視野的機會。藉此培養未來優
秀博士生候選人。建議部分補助碩士生參與國際會議，補助項目包括：機票
及註冊費全額補助；生活費至少補助一半。鼓勵我國優秀碩士生出訪，從事
更多學術交流、促進國民外交、拓展國際觀視野，間接促進國際交流、全球
化於無形。  
 
 
 
 
Date : 12th July 2010 (Monday) 
Venue : Mandarin Orchard Hotel, Singapore 
 
Mandarin Suite 832 (BIDW & DAMD) 
8:30 Opening of Registration Desk 
9:15-9:20 Opening Address 
Prof. Andy Koronios 
9:20-10:30 Keynote Speech 
"Taming the Beast.... Exploiting the Opportunities..... data!" 
Prof. Andy Koronios 
10:30-11:00 Coffee/Refreshment Break  
 Mandarin Suite 832 (BIDW) Mandarin Suite 831 (DAMD) 
11:00 – 12:20 Session: 
Business Intelligence through Data Mining 
 
Session Chair: 
Assoc. Prof Narasimha Bolloju 
City University of Hong Kong 
Hong Kong 
Session: 
Meta Data Tools and Methods 
 
Session Chair: 
Dr. Liwan Liyanage 
University of Western Sydney 
Australia 
11:00-11:20 
 
 
 
Paper ID: 36 (BIDW) 
Using data mining process standards: A case 
study - Profiling Internet Banking Users in 
Jamaica 
Dr Gunjan Mansingh 
The University of the West Indies 
Jamaica 
 
Paper ID: 13 (DAMD) 
A Proposed Semantics for the Sampled Values 
and Metadata of Scientific Values 
Dr. Joseph Phillips 
De Paul University 
USA 
11:20-11:40 Paper ID: 40 (BIDW) 
A Link-Based Method for Hiding Sensitive 
Patterns on Data Mining 
Asst. Prof. Yu-Chiang Li 
Southern Taiwan University 
Taiwan, Province Of China 
 
Paper ID: 17 (DAMD) 
The Specifications of a Generic QoS 
Metamodel for Designing and Developing 
Good Quality Web Services 
Dr Farid Meziane 
University of Salford 
United Kingdom 
11:40-12:00 Paper ID: 62 (BIDW) 
Impact of Missing Data and Data Imputation 
on Data Mining 
Prof. Chien-Hua Mike  Lin 
School of Business Cleveland State University 
United States 
Paper ID: 33 (DAMD) 
Computational Confidence for Decision 
Making in Health 
Stephen Lean 
Massey University 
New Zealand 
 
12:00-12:20 Paper ID: 41 (BIDW) 
System Dynamics Simulation for Customer 
Lifetime Value 
Dr. Jiann-Horng Lin 
Department of Information Management,  
I-Shou University 
Taiwan, Province Of China 
Paper ID: 9 (DAMD) 
A multilevel approach to interoperability in 
surveillance and reconnaissance 
Barbara Essendorfer 
Fraunhofer IOSB, Germany 
 
DAY 1 
 
 
 
 
 
 
 
Date : 13th July 2010 (Tuesday) 
Venue : Mandarin Orchard Hotel, Singapore 
 
Mandarin Suite 831 (BIDW & DAMD) 
9:30 Opening of Registration Desk 
9:30 – 10:50 
 
Session:  Business Intelligence Applications & Cloud Computing 
 
Session Chair 
Dr. Lisa Soon 
Central Queensland University 
Australia 
 
9:30-9:50 Paper ID: 38 (BIDW) 
Business Intelligence for Sustainable Competitive Advantage:  
Field Study of Telecommunications Industry 
 Azizah Ahmad 
Curtin University of Technology 
Australia 
 
9:50-10:10 Paper ID: 27 (BIDW) 
Log Mining for Query Recommendation in e-commerce 
Alper Kursat 
Anadolu University 
Turkey 
 
10:10-10.30 Paper ID: 45 (BIDW) 
Information Management and Accessibility in Business Intelligence in Cloud Vs Conventional 
Business Intelligence 
Venkata Surya Brahma Linga Sarma Somina 
BI DW Consultancy 
India 
 
10.30-10.50 Paper ID: 59 (BIDW) 
Outsourcing Data Mining Tasks to Cloud While Preserving Customer Privacy 
Yi-Ming Chen 
Department of Information Management, National Central University 
Taiwan, Province Of China 
 
 
 
 
 
 
10:50-11:10 Coffee/Refreshment Break  
DAY 2 
 
 
 
 
 
14:10-14:30 Paper ID: 35 (DAMD) 
A Rule Based Taxonomy of Dirty Data 
Lin Li 
Edinburgh Napier University 
Sohar, Sultanate of Oman, 
United Kingdom 
14:30-15:10 Session: Data Anaysis Methods 
14:30-14:50 Paper ID: 16 (DAMD) 
Implementation and Evaluation of Snapshots + Events Spatiotemporal Modelling Approach 
Akbar Ghobakhlou 
Auckland University of Technology 
New Zealand 
14:50-15:10 Paper ID: 14 (DAMD) 
The need for ICT induced Organizational Transformation among  
the Public sector banks in Sri Lanka 
Selvarajan .P 
University of Jaffna, 
 Sri Lanka 
15:10-15:30 Coffee Break 
15:30-16:30 Session: Aspects of Business Intelligence 
 
Session Chair: 
Prof. Kuldeep Kumar 
Bond University 
Australia 
15:30-15:50 Paper ID: 18 (BIDW) 
Conceptualizing a True Business Intelligence System 
Shyam A.V. 
Indian Institute of Management Kozhikode 
India 
15:50-16:10 Paper ID: 14 (BIDW) 
Estimating Consumer Demand From High-Frequency Data 
Dr. John Cartlidge 
University of Central Lancashire 
United Kingdom 
16:10-16:30 Paper ID: 51 (BIDW) 
Predicting student’s behavior in education using  
J48 algorithm analysis tools in WEKA environment 
Daya Gupta 
Delhi technological University 
India 
16:30-17:30 Panel Discussion 
 
- Future of Business Intelligence 
- Cutting Edge Technologies for BIDW and DAMD 
- Obstacles in Integrating Data Analysis (Mathematics) and Information Technology 
 
17:30-18:00 BIDW & DAMD Best Research Paper Award 
BIDW & DAMD Best Research Student Paper Award 
 
amount of search results [2]. 
 
This study proposes a revised hierarchical agglomerative 
clustering method (RHAC) to remedy the drawback of 
traditional agglomeration algorithm.   Google search 
engine was used to search web pages based on a set of 
pre-set queries to be our experimental set. We intended to 
adopt Latent Semantic Analysis (LSA) to extract the 
hidden implications from the corresponding web pages and 
exercised the concept of K-way to reduce time complexity 
of execution. It is expected to reduce the problems of 
“homograph” and “homophony” and improve the 
relationship of hierarchical agglomeration. 
 
2. RELATED WORK 
 
2.1. Clustering algorithm 
Clustering is the process of organizing objects into groups 
whose members are similar in some way. The goal of 
clustering is to determine the intrinsic grouping in a set of 
unlabeled data. The clustering algorithm can be divided 
into hierarchical clustering and non-hierarchical 
(partitional) clustering in general [3]. It is claimed that 
there is no absolute “best” criterion which would be 
independent of the final aim of the clustering. 
Consequently, it is the user which must supply this 
criterion, in such a way that the result of the clustering will 
suit their needs.  
 
HAC has four different strategies to calculate the distance 
between clusters: single-linkage, complete-linkage, 
average-linkage and Ward‟s method (WM).  But the 
structure of HAC is binary tree and if the database contains 
large noises, both factors will affect clustered precision. 
Moreover, many web page clusters still adopt hierarchical 
clustering even if the huge amount of data will cause low 
performance [2]. The reason that this algorithm may be 
suitable for all domains and the outcome of web page 
cluster is also better [4-6]. 
 
2.2. Web page cluster 
Due to the Internet contains a vast amount of disordered 
information, previous studies mainly focused on certain 
part of web pages to conduct clustering analysis. Generally 
web page information to be processed includes the page 
contents – to extract features directly, and the hyperlinks – 
to explore the potential relationships of the page and other 
documents. [7] 
 
Vlajic & Card differentiated web pages based on their 
property, such as organization, and then used Adaptive 
hypertext clustering (AHC) to compute the similarity 
between the pages and hyperlinks[8]. Hou & Zhang 
applied similarity matrix to produce a series of divisions 
based on hyperlink information and form hierarchical web 
page clustering [7]. 
 
Due to the rigidity of classification method, 
Mukhopadhyay & Sing adopted co-citation method to 
build clusters. They claimed that if two web pages cite the 
same resource, there exists a possibility of relationships 
between the pages, therefore the two pages would be 
clustered in the same group [9]. To solve the 
high-dimensional problem of vector space model (VSM), 
Maggini, Rigutini, & Turchi adopted the approaches of 
Singular Value Decomposition (SVD) and Concept Matrix 
Decomposition (CMD) plus K-means respectively to 
explore relationships of terms. The result showed that 
CMD plus K-means outperformed SVD plus K-means 
[10]. 
 
Due to the high-dimensional VSM would influence 
clustered efficacy, Guandong addressed Probabilistic 
Latent Semantic Analysis (PLSA) which integrates 
modified k-mean clustering algorithm[11]. The modified 
k-mean algorithm allows documents to repeatedly appear 
in different clusters. However the data set of this method 
had been pre-categorized, it didn‟t meet the objective of 
web clustering. Moreover, the data set was based on 
specialized domain (knowledge management) document, 
so it was also unable to apply in general application. 
 
2.3. Latent semantic analysis 
Latent Semantic Analysis (LSA) is a technique used in 
natural language processing of analyzing relationships 
between a set of documents and the terms they contain by 
producing a set of connotations related to the documents 
and terms. Landauer indicated that LSA could estimate 
connoted knowledge in document content and 
appropriately illustrate how human being processes the 
deduction of  knowledge [12]. Since the original 
term-document matrix is presumed too large for the 
computing resources, the essential concept of LSA is to 
utilize low-rank approximation to the term-document 
matrix. Generally, LSA uses two fundamental techniques: 
Singular Value Decomposition (SVD) and Dimension 
Reduction. SVD abstractly transforms connoted knowledge 
into semantic space from documents. Dimension Reduction 
is meant to eliminate anecdotal instances of terms and 
further deduce connoted knowledge in document. It can be 
recognized as the extension of VSM [13] and its primary 
application would be solving the problem of synonym and 
polysemy[14]. 
 
3. RESEARCH METHOD 
3.1 Process framework 
The process framework of this study illustrates as Fig.1. 
The keywords were chosen based on popularity 
consideration, such as The Da Vinci Code. The selected 
keywords were then operated by Google Search to acquire 
related web pages and the contained hyperlinks. The results 
were stored in the Google Search Result Collector. Term 
processing is to conduct term segmentation and term 
weighting for further clustering analysis. 
 
3.2. Google search result collector 
The Google search result collector was designed to 
automatically download search results and the extended 
one layer of pages through hyperlink. This collector also 
removed noises such as HTML tag, propaganda and 
advertisement from web pages. We only retained website 
title, URL, and context. 
 
X = 
 Movie Religion 
Louvre 
Museum 
Conviction Gospel 
Mona 
Lisa 
Artist 
Tom 
Hanks 
A1 0 0 1.1426 0 0 55.031 102.54 0 
B1 0 2.367 4.4259 0 0 0 0 0 
B2 0 2.4617 0 0 0 0 0 0 
B3 1.4112 12.484 11.318 8.5845 0 0 8.5812 0 
F1 11.317 0 21.059 0 0 19.768 0 36.837 
F2 5.6829 5.673 0 0 0 0 0 18.6334 
M1 0 3.1979 0 2.6562 91.079 0 0 0 
M2 0 15.097 0 7.797 102.12 4.0831 7.6086 0 
 
Figure 2. Matrix X 
 
Σ= 
138.2763 0 0 0 0 0 0 0 
0 116.7893 0 0 0 0 0 0 
0 0 49.99276 0 0 0 0 0 
0 0 0 20.3182 0 0 0 0 
0 0 0 0 12.40953 0 0 0 
0 0 0 0 0 4.062085 0 0 
0 0 0 0 0 0 2.130856 0 
0 0 0 0 0 0 0 0.146636 
Figure 3. Matrix Σ 
 
Figure 4. The SVD of The Da Vinci Code 
 
In Fig. 4, we looked for the "elbow" and kept the number 
of factors above the elbow. On this graph, the elbow is 
either at 3 or 4, so we would keep two or three factors and 
further produce the matrix X‟. The idea is that when there 
are substantive factors, the slope of the line will be steep. 
When the factors correspond to error or random numbers, 
the slope will be flat. The elbow is the place where we 
move from the good, substantive factors to the bad, error 
factors. This is analogous to moving from the cliff to the 
plain.  
 
With the help of „Movie‟ and „Tom Hanks‟ contained in F2 
of matrix X, We observed that „Louvre Museum‟ and 
„Mona Lisa‟ increases their values from 0 to 7.0039 and 
4.6985F2, respectively, even though they didn‟t appear in 
F2. On the other hand, the value of „Religion‟ decreases 
from 12.484 to 0.5200 in row B3 of matrix X‟ as Fig. 5 
after conducting SVD and Dimension Reduction. This 
phenomenon explains that the ‟Religion‟ is not important 
as it appears to be in the book of „The Da Vinci Code‟. 
 
X’ = 
 Movie Religion 
Louvre 
Museum 
Conviction Gospel 
Mona 
Lisa 
Artist 
Tom 
Hanks 
A1 -0.035 0.9280 1.8712 0.6509 -0.125 54.874 102.61 -0.376 
B1 0.4987 0.1475 0.8210 0.0432 0.2230 0.5959 -0.219 1.6090 
B2 0.0392 0.0342 0.0648 0.0162 0.2448 0.0542 -0.003 0.1265 
B3 1.1619 0.5200 2.0398 0.2086 1.6657 5.0057 6.2486 3.7312 
F1 11.830 3.0012 19.597 0.7520 -0.338 17.558 1.1817 38.154 
F2 4.2626 1.1091 7.0039 0.2830 0.4411 4.6985 -2.614 13.755 
M1 -0.089 8.9436 -0.160 5.0662 90.376 -0.182 0.0330 -0.312 
M2 0.0870 10.283 0.2721 5.8192 102.71 4.1330 7.6425 0.2336 
Figure 5. Matrix X’ 
 
3.4.3. Second approach – RHAC for the second 
step 
HAC is based on the union between the two nearest 
clusters with the binary tree structure.  With the difficulty 
in dealing with large number of dimensions and large 
number of data items because of time complexity, 
traditional HAC has its blame for performance inefficiency. 
In this study, we propose RHAC to resolve this problem. 
The process of RHAC is described as follows: 
1. Each web page indicates one cluster Ci. 
2. Calculate the average-linkage distances of all 
clusters and locate the distance in clusters Ci ,…, 
Cj with a greater value than threshold. Next, 
apply Ward‟s method. Proceed to step 3, if the 
distance value is less than threshold, otherwise 
reduce the number of merged clusters and apply 
Ward‟s method again. 
3. Merge Ci ,…, Cj as a new cluster. 
4. If the numbers of clusters are more than expected, 
repeat steps 2-3. 
  The equation 5 shows Ward‟s method. The u represents 
the average value of ji cc  ... . 
  2
...
,..., uaCCd
ji cca
ji 

 (5) 
 
4. EXPERIMENTAL DESIGN 
4.1. Evaluation criteria 
Precision and entropy were used as two evaluation criteria 
to measure the performance of two approaches. [19]. 
1. Precision 
  Precision means that the ratio of the retrieved web pages 
is correctly clustered. This measurement will help us 
understand the degree of density for each cluster centering 
on concept i. The formula shows as Equation 6. 
 
j
ij
N
N
ji,PrecisionP   (6) 
Nj is the number of web pages in cluster j. Nij denotes 
the number of web pages with the same concept i in cluster 
j.  
 
2. Entropy 
  Entropy is a measure of "disorder" (the higher the 
entropy, the higher the disorder). Apply this concept in this 
experiment is to identify the degree of uncertainty of 
clusters. From that implication, we may recognize the level 
of homogeneity (similarity) of a cluster. The equation of 
entropy is shown in Equation 7. Ej denotes the entropy of 
cluster j. The entropy is the sum of all clustered entropies 
at the identical layer, such as Equation 8. 
log( )ij ijiEj p p   (7) 
1
( )
m
j
c j
j
N
E E
N
 
 
(8) 
4.4.3. Performance 
The performance in time complexity of RHAC is shown in 
Figure 10, while the difference of the three thresholds is 
not so obvious.  
 
Figure 10. Performance of three keywords  
 
After the pilot test, we selected threshold B as our 
experimental design. 
 
4.5. Evaluation of HAC and RHAC 
In this section we will evaluate the result of HAC and 
RHAC. RHAC is conducted based on threshold B and 
Dimension Reduction of LSA to13 and then compare with 
that of HAC. The cluster results of „the Da Vinci Code‟ by 
both approaches are listed in Figs. 11 and 12, respectively. 
Where Ci indicates cluster number in level j. 
 
Fig. 11 shows that both C1 and C4 in level 1 include the 
concepts of the book (B1, B2) and religion (M2, M1) of 
The Da Vinci Code. The movie of The Da Vinci Code (F1, 
F2) merges with Da Vinci (A1) and book (B3), 
respectively. C1 and C2 both include the book, movie and 
religion concepts in level 2. It explains the limitation of 
HAC for its tendence to merge with the shortest distance of 
objects and then cause multiple clusters having the same 
concept in the same level. 
 
 
Figure 11. The clustered result of HAC 
 
Figure 12 presents the clustered result of RHAC. In level 1, 
only B1, B2 and B3 pass the threshold of average-linkage 
and merge with Ward‟s method. The concept of the newly 
merged cluster centers on the book of the Da Vinci Code. 
The threshold of level 2 doubles that of level 1 and then the 
three clusters of level 1 would be merged in level 2. The 
concept centers on the book and movie of The Da Vinci 
Code. M1, M2 in religion are also merged in level 2. The 
reason that Da Vinci (A1) does not merge with any cluster 
may due to these web pages discuss the relationship of Da 
Vinci with other topics, such as paint, does not discuss the 
book as The Da Vinci Code or the religion as Magdala.  
 
 
Figure 12. The clustered result of RHAC 
 
The result drawn from the experiments of traditional HAC 
and RHAC reveals that RHAC cooperates with LSA will 
accurately extract the concept of web pages. Furthermore, 
RHAC will produce new semantic spaces and increase 
precision. Performance of the hot topics was compared by 
using two algorithms shown in Figs. 13, 14 and 15. The 
precision and entropy were measured by / 2  and up 
to one level, where H indicates the height of hierarchical 
clustered result. Since the height of HAC is 9, so we chose 
the average value of level 5 and level 6 to measure. The 
average height of RHAC is usually 3. If the height is 4 with 
level 1, yet without processing any merge, then the level 2 
will be regarded as level 1.  
 
Our study shows that precision of RHAC is higher than 
0.99 and the entropy is less than 0.003. Both are superior to 
traditional HAC shown in Figs. 13, 14. The overall 
performance is also better than that of traditional HAC as 
Fig. 15.  
 
 
Figure 13. Precision 
 
 
Figure 14. Entropy 
 
  
出席 2010 DAMA 報告書 –Singapore, July 12-13 
出席國際會議報告書 
黃純敏 Chuen-min Huang 
國立雲林科技大學資管系副教授 
一、目的 
（一）計畫案研究目的 
網路時代的來臨帶來許多生活上的便利，使得上網人口激增。閱讀網路
新聞也已經成為日常生活獲取知識重要的一部分。然而新聞事件常因記者政
治立場不同或專業素養不一，使得同一事件有不同的報導。讀者如想客觀了
解實況，必須查閱不同報社報導，十分費時。此外，新聞有其延續發展性的
特點，同一事件常延續數日甚至更久。目前各家新聞平台並沒有事件追蹤機
制，導致讀者想獲知事件發展全貌，有其實質的困難。鑒於上述問題，本人
多年來致力於網路新聞文件之處理，目的在於提升新聞事件認知及縮短知識
內化的時間，結合智慧型手機加值軟體，為嵌入式系統投入更多創意構思與
實作成品，藉以提升文化水平，邁向智慧生活的社會。本計畫案主題：「SOA
架構知識萃取與展現」，為第三年計畫，本年度研究目的為：（一）提升新
聞報導品質，希透過多文件摘要比對同事件個別新聞撰寫內容，客觀評定報
社品質，讓讀者獲得質量皆佳之新聞內容；（二）為降低資訊過載，希結合
內容取向與協同式過濾方式，建構合宜的新聞推薦機制，提供讀者最適切的
新聞主動推播服務；（三）以圖形化的介面設計，強化使用者操作時閱讀上
的可讀性、理解性、以及新聞推薦時的便利性。 
（二）DAMA簡介：  
DAMA (Data Analysis, Data Quality & Metadata Management) 為GSTF 
(Global Science and Technology Forum)此一組織年度舉辦的研討會之一。
GSTF是亞太地區有名的企業策略管理顧問團體，為大學及研究機構提供產學
合作諮詢。該組織自2008年起積極舉辦研討會，包括：CGAT(Computer Game 
& Allied Technology), CCV(Cloud Computing and Virtualization Industrial 
Conference), BIDW(Business Intelligence & Data Warehousing) / DAMA。並出
版三種學術期刊： International Journal on Computing, International Journal on 
Bio informatics and Biotechnology, and Global Business Review。DAMA 
Conference主要研討議題著重於資料分析、資料品質、資料庫管理等，從資
訊管理觀點出發，並以提昇組織效能、強化組織優勢為目的。被接受的論文
將收錄於Asia Pacific Technology Forum (APTF) Conference Proceedings (紙本
及光碟版)。DAMD Conference 的論文也都被納入知名索引，包括ISI web of 
science and Elveseir SCOPUS 以及其他相關索引。本人希藉由參與此研討
會，除分享研究成果外，並透過面對面與國外研究同好意見交換，獲取不同
面向的建議。 
 
  
 
 
圖1 團體合照 
 
 
第二天，本人參與閉幕前的 Panel Discussion，此座談會於 4:30 – 5:30 p.m.在
Mandarin Suite 831進行，主題包括： Future of Business Intelligence, Cutting 
Edge Technologies for BIDW and DAMD, and Obstacles in Integrating Data 
Analysis (Mathematics) and Information Technology.由 Editor-In-Chief Prof. 
Kuldeep Kumar主持，本人(Assoc. Prof. Chuen-Min Huang)與其他三位 Session 
Chair (Assoc. Prof. Narasimha Bolloju, Prof. Mike Lin, Dr. Liwan Liyanage)共同
擔任 Panelist(如圖 2) 
 
  
四、建議  
 
參與國際研討會需準備事情繁多，從擬訂參與國際研討會起，前置作業的
論文擬題、寫英文稿、上網投稿；審查作業期間的耐心等待；直到通知論文
被接受，隨後，依照指示提交英文版全文，並按照規定，仔細檢查論文格式，
上網提交論文全文；註冊交費，以及後續的出國準備、旅程的安排、訂機票、
旅館；到現場的報告、參與各場次研討等，是一種時程的控管與學習成長的
訓練。由於國科會僅補助教師及博士班學生參與國際會議，卻將碩士生排
外，希望能鼓勵優秀碩士生也有提昇國際觀及視野的機會。藉此培養未來優
秀博士生候選人。建議部分補助碩士生參與國際會議，補助項目包括：機票
及註冊費全額補助；生活費至少補助一半。鼓勵我國優秀碩士生出訪，從事
更多學術交流、促進國民外交、拓展國際觀視野，間接促進國際交流、全球
化於無形。  
 
 
 
 
Date : 12th July 2010 (Monday) 
Venue : Mandarin Orchard Hotel, Singapore 
 
Mandarin Suite 832 (BIDW & DAMD) 
8:30 Opening of Registration Desk 
9:15-9:20 Opening Address 
Prof. Andy Koronios 
9:20-10:30 Keynote Speech 
"Taming the Beast.... Exploiting the Opportunities..... data!" 
Prof. Andy Koronios 
10:30-11:00 Coffee/Refreshment Break  
 Mandarin Suite 832 (BIDW) Mandarin Suite 831 (DAMD) 
11:00 – 12:20 Session: 
Business Intelligence through Data Mining 
 
Session Chair: 
Assoc. Prof Narasimha Bolloju 
City University of Hong Kong 
Hong Kong 
Session: 
Meta Data Tools and Methods 
 
Session Chair: 
Dr. Liwan Liyanage 
University of Western Sydney 
Australia 
11:00-11:20 
 
 
 
Paper ID: 36 (BIDW) 
Using data mining process standards: A case 
study - Profiling Internet Banking Users in 
Jamaica 
Dr Gunjan Mansingh 
The University of the West Indies 
Jamaica 
 
Paper ID: 13 (DAMD) 
A Proposed Semantics for the Sampled Values 
and Metadata of Scientific Values 
Dr. Joseph Phillips 
De Paul University 
USA 
11:20-11:40 Paper ID: 40 (BIDW) 
A Link-Based Method for Hiding Sensitive 
Patterns on Data Mining 
Asst. Prof. Yu-Chiang Li 
Southern Taiwan University 
Taiwan, Province Of China 
 
Paper ID: 17 (DAMD) 
The Specifications of a Generic QoS 
Metamodel for Designing and Developing 
Good Quality Web Services 
Dr Farid Meziane 
University of Salford 
United Kingdom 
11:40-12:00 Paper ID: 62 (BIDW) 
Impact of Missing Data and Data Imputation 
on Data Mining 
Prof. Chien-Hua Mike  Lin 
School of Business Cleveland State University 
United States 
Paper ID: 33 (DAMD) 
Computational Confidence for Decision 
Making in Health 
Stephen Lean 
Massey University 
New Zealand 
 
12:00-12:20 Paper ID: 41 (BIDW) 
System Dynamics Simulation for Customer 
Lifetime Value 
Dr. Jiann-Horng Lin 
Department of Information Management,  
I-Shou University 
Taiwan, Province Of China 
Paper ID: 9 (DAMD) 
A multilevel approach to interoperability in 
surveillance and reconnaissance 
Barbara Essendorfer 
Fraunhofer IOSB, Germany 
 
DAY 1 
 
 
 
 
 
 
 
Date : 13th July 2010 (Tuesday) 
Venue : Mandarin Orchard Hotel, Singapore 
 
Mandarin Suite 831 (BIDW & DAMD) 
9:30 Opening of Registration Desk 
9:30 – 10:50 
 
Session:  Business Intelligence Applications & Cloud Computing 
 
Session Chair 
Dr. Lisa Soon 
Central Queensland University 
Australia 
 
9:30-9:50 Paper ID: 38 (BIDW) 
Business Intelligence for Sustainable Competitive Advantage:  
Field Study of Telecommunications Industry 
 Azizah Ahmad 
Curtin University of Technology 
Australia 
 
9:50-10:10 Paper ID: 27 (BIDW) 
Log Mining for Query Recommendation in e-commerce 
Alper Kursat 
Anadolu University 
Turkey 
 
10:10-10.30 Paper ID: 45 (BIDW) 
Information Management and Accessibility in Business Intelligence in Cloud Vs Conventional 
Business Intelligence 
Venkata Surya Brahma Linga Sarma Somina 
BI DW Consultancy 
India 
 
10.30-10.50 Paper ID: 59 (BIDW) 
Outsourcing Data Mining Tasks to Cloud While Preserving Customer Privacy 
Yi-Ming Chen 
Department of Information Management, National Central University 
Taiwan, Province Of China 
 
 
 
 
 
 
10:50-11:10 Coffee/Refreshment Break  
DAY 2 
 
 
 
 
 
14:10-14:30 Paper ID: 35 (DAMD) 
A Rule Based Taxonomy of Dirty Data 
Lin Li 
Edinburgh Napier University 
Sohar, Sultanate of Oman, 
United Kingdom 
14:30-15:10 Session: Data Anaysis Methods 
14:30-14:50 Paper ID: 16 (DAMD) 
Implementation and Evaluation of Snapshots + Events Spatiotemporal Modelling Approach 
Akbar Ghobakhlou 
Auckland University of Technology 
New Zealand 
14:50-15:10 Paper ID: 14 (DAMD) 
The need for ICT induced Organizational Transformation among  
the Public sector banks in Sri Lanka 
Selvarajan .P 
University of Jaffna, 
 Sri Lanka 
15:10-15:30 Coffee Break 
15:30-16:30 Session: Aspects of Business Intelligence 
 
Session Chair: 
Prof. Kuldeep Kumar 
Bond University 
Australia 
15:30-15:50 Paper ID: 18 (BIDW) 
Conceptualizing a True Business Intelligence System 
Shyam A.V. 
Indian Institute of Management Kozhikode 
India 
15:50-16:10 Paper ID: 14 (BIDW) 
Estimating Consumer Demand From High-Frequency Data 
Dr. John Cartlidge 
University of Central Lancashire 
United Kingdom 
16:10-16:30 Paper ID: 51 (BIDW) 
Predicting student’s behavior in education using  
J48 algorithm analysis tools in WEKA environment 
Daya Gupta 
Delhi technological University 
India 
16:30-17:30 Panel Discussion 
 
- Future of Business Intelligence 
- Cutting Edge Technologies for BIDW and DAMD 
- Obstacles in Integrating Data Analysis (Mathematics) and Information Technology 
 
17:30-18:00 BIDW & DAMD Best Research Paper Award 
BIDW & DAMD Best Research Student Paper Award 
 
amount of search results [2]. 
 
This study proposes a revised hierarchical agglomerative 
clustering method (RHAC) to remedy the drawback of 
traditional agglomeration algorithm.   Google search 
engine was used to search web pages based on a set of 
pre-set queries to be our experimental set. We intended to 
adopt Latent Semantic Analysis (LSA) to extract the 
hidden implications from the corresponding web pages and 
exercised the concept of K-way to reduce time complexity 
of execution. It is expected to reduce the problems of 
“homograph” and “homophony” and improve the 
relationship of hierarchical agglomeration. 
 
2. RELATED WORK 
 
2.1. Clustering algorithm 
Clustering is the process of organizing objects into groups 
whose members are similar in some way. The goal of 
clustering is to determine the intrinsic grouping in a set of 
unlabeled data. The clustering algorithm can be divided 
into hierarchical clustering and non-hierarchical 
(partitional) clustering in general [3]. It is claimed that 
there is no absolute “best” criterion which would be 
independent of the final aim of the clustering. 
Consequently, it is the user which must supply this 
criterion, in such a way that the result of the clustering will 
suit their needs.  
 
HAC has four different strategies to calculate the distance 
between clusters: single-linkage, complete-linkage, 
average-linkage and Ward‟s method (WM).  But the 
structure of HAC is binary tree and if the database contains 
large noises, both factors will affect clustered precision. 
Moreover, many web page clusters still adopt hierarchical 
clustering even if the huge amount of data will cause low 
performance [2]. The reason that this algorithm may be 
suitable for all domains and the outcome of web page 
cluster is also better [4-6]. 
 
2.2. Web page cluster 
Due to the Internet contains a vast amount of disordered 
information, previous studies mainly focused on certain 
part of web pages to conduct clustering analysis. Generally 
web page information to be processed includes the page 
contents – to extract features directly, and the hyperlinks – 
to explore the potential relationships of the page and other 
documents. [7] 
 
Vlajic & Card differentiated web pages based on their 
property, such as organization, and then used Adaptive 
hypertext clustering (AHC) to compute the similarity 
between the pages and hyperlinks[8]. Hou & Zhang 
applied similarity matrix to produce a series of divisions 
based on hyperlink information and form hierarchical web 
page clustering [7]. 
 
Due to the rigidity of classification method, 
Mukhopadhyay & Sing adopted co-citation method to 
build clusters. They claimed that if two web pages cite the 
same resource, there exists a possibility of relationships 
between the pages, therefore the two pages would be 
clustered in the same group [9]. To solve the 
high-dimensional problem of vector space model (VSM), 
Maggini, Rigutini, & Turchi adopted the approaches of 
Singular Value Decomposition (SVD) and Concept Matrix 
Decomposition (CMD) plus K-means respectively to 
explore relationships of terms. The result showed that 
CMD plus K-means outperformed SVD plus K-means 
[10]. 
 
Due to the high-dimensional VSM would influence 
clustered efficacy, Guandong addressed Probabilistic 
Latent Semantic Analysis (PLSA) which integrates 
modified k-mean clustering algorithm[11]. The modified 
k-mean algorithm allows documents to repeatedly appear 
in different clusters. However the data set of this method 
had been pre-categorized, it didn‟t meet the objective of 
web clustering. Moreover, the data set was based on 
specialized domain (knowledge management) document, 
so it was also unable to apply in general application. 
 
2.3. Latent semantic analysis 
Latent Semantic Analysis (LSA) is a technique used in 
natural language processing of analyzing relationships 
between a set of documents and the terms they contain by 
producing a set of connotations related to the documents 
and terms. Landauer indicated that LSA could estimate 
connoted knowledge in document content and 
appropriately illustrate how human being processes the 
deduction of  knowledge [12]. Since the original 
term-document matrix is presumed too large for the 
computing resources, the essential concept of LSA is to 
utilize low-rank approximation to the term-document 
matrix. Generally, LSA uses two fundamental techniques: 
Singular Value Decomposition (SVD) and Dimension 
Reduction. SVD abstractly transforms connoted knowledge 
into semantic space from documents. Dimension Reduction 
is meant to eliminate anecdotal instances of terms and 
further deduce connoted knowledge in document. It can be 
recognized as the extension of VSM [13] and its primary 
application would be solving the problem of synonym and 
polysemy[14]. 
 
3. RESEARCH METHOD 
3.1 Process framework 
The process framework of this study illustrates as Fig.1. 
The keywords were chosen based on popularity 
consideration, such as The Da Vinci Code. The selected 
keywords were then operated by Google Search to acquire 
related web pages and the contained hyperlinks. The results 
were stored in the Google Search Result Collector. Term 
processing is to conduct term segmentation and term 
weighting for further clustering analysis. 
 
3.2. Google search result collector 
The Google search result collector was designed to 
automatically download search results and the extended 
one layer of pages through hyperlink. This collector also 
removed noises such as HTML tag, propaganda and 
advertisement from web pages. We only retained website 
title, URL, and context. 
 
X = 
 Movie Religion 
Louvre 
Museum 
Conviction Gospel 
Mona 
Lisa 
Artist 
Tom 
Hanks 
A1 0 0 1.1426 0 0 55.031 102.54 0 
B1 0 2.367 4.4259 0 0 0 0 0 
B2 0 2.4617 0 0 0 0 0 0 
B3 1.4112 12.484 11.318 8.5845 0 0 8.5812 0 
F1 11.317 0 21.059 0 0 19.768 0 36.837 
F2 5.6829 5.673 0 0 0 0 0 18.6334 
M1 0 3.1979 0 2.6562 91.079 0 0 0 
M2 0 15.097 0 7.797 102.12 4.0831 7.6086 0 
 
Figure 2. Matrix X 
 
Σ= 
138.2763 0 0 0 0 0 0 0 
0 116.7893 0 0 0 0 0 0 
0 0 49.99276 0 0 0 0 0 
0 0 0 20.3182 0 0 0 0 
0 0 0 0 12.40953 0 0 0 
0 0 0 0 0 4.062085 0 0 
0 0 0 0 0 0 2.130856 0 
0 0 0 0 0 0 0 0.146636 
Figure 3. Matrix Σ 
 
Figure 4. The SVD of The Da Vinci Code 
 
In Fig. 4, we looked for the "elbow" and kept the number 
of factors above the elbow. On this graph, the elbow is 
either at 3 or 4, so we would keep two or three factors and 
further produce the matrix X‟. The idea is that when there 
are substantive factors, the slope of the line will be steep. 
When the factors correspond to error or random numbers, 
the slope will be flat. The elbow is the place where we 
move from the good, substantive factors to the bad, error 
factors. This is analogous to moving from the cliff to the 
plain.  
 
With the help of „Movie‟ and „Tom Hanks‟ contained in F2 
of matrix X, We observed that „Louvre Museum‟ and 
„Mona Lisa‟ increases their values from 0 to 7.0039 and 
4.6985F2, respectively, even though they didn‟t appear in 
F2. On the other hand, the value of „Religion‟ decreases 
from 12.484 to 0.5200 in row B3 of matrix X‟ as Fig. 5 
after conducting SVD and Dimension Reduction. This 
phenomenon explains that the ‟Religion‟ is not important 
as it appears to be in the book of „The Da Vinci Code‟. 
 
X’ = 
 Movie Religion 
Louvre 
Museum 
Conviction Gospel 
Mona 
Lisa 
Artist 
Tom 
Hanks 
A1 -0.035 0.9280 1.8712 0.6509 -0.125 54.874 102.61 -0.376 
B1 0.4987 0.1475 0.8210 0.0432 0.2230 0.5959 -0.219 1.6090 
B2 0.0392 0.0342 0.0648 0.0162 0.2448 0.0542 -0.003 0.1265 
B3 1.1619 0.5200 2.0398 0.2086 1.6657 5.0057 6.2486 3.7312 
F1 11.830 3.0012 19.597 0.7520 -0.338 17.558 1.1817 38.154 
F2 4.2626 1.1091 7.0039 0.2830 0.4411 4.6985 -2.614 13.755 
M1 -0.089 8.9436 -0.160 5.0662 90.376 -0.182 0.0330 -0.312 
M2 0.0870 10.283 0.2721 5.8192 102.71 4.1330 7.6425 0.2336 
Figure 5. Matrix X’ 
 
3.4.3. Second approach – RHAC for the second 
step 
HAC is based on the union between the two nearest 
clusters with the binary tree structure.  With the difficulty 
in dealing with large number of dimensions and large 
number of data items because of time complexity, 
traditional HAC has its blame for performance inefficiency. 
In this study, we propose RHAC to resolve this problem. 
The process of RHAC is described as follows: 
1. Each web page indicates one cluster Ci. 
2. Calculate the average-linkage distances of all 
clusters and locate the distance in clusters Ci ,…, 
Cj with a greater value than threshold. Next, 
apply Ward‟s method. Proceed to step 3, if the 
distance value is less than threshold, otherwise 
reduce the number of merged clusters and apply 
Ward‟s method again. 
3. Merge Ci ,…, Cj as a new cluster. 
4. If the numbers of clusters are more than expected, 
repeat steps 2-3. 
  The equation 5 shows Ward‟s method. The u represents 
the average value of ji cc  ... . 
  2
...
,..., uaCCd
ji cca
ji 

 (5) 
 
4. EXPERIMENTAL DESIGN 
4.1. Evaluation criteria 
Precision and entropy were used as two evaluation criteria 
to measure the performance of two approaches. [19]. 
1. Precision 
  Precision means that the ratio of the retrieved web pages 
is correctly clustered. This measurement will help us 
understand the degree of density for each cluster centering 
on concept i. The formula shows as Equation 6. 
 
j
ij
N
N
ji,PrecisionP   (6) 
Nj is the number of web pages in cluster j. Nij denotes 
the number of web pages with the same concept i in cluster 
j.  
 
2. Entropy 
  Entropy is a measure of "disorder" (the higher the 
entropy, the higher the disorder). Apply this concept in this 
experiment is to identify the degree of uncertainty of 
clusters. From that implication, we may recognize the level 
of homogeneity (similarity) of a cluster. The equation of 
entropy is shown in Equation 7. Ej denotes the entropy of 
cluster j. The entropy is the sum of all clustered entropies 
at the identical layer, such as Equation 8. 
log( )ij ijiEj p p   (7) 
1
( )
m
j
c j
j
N
E E
N
 
 
(8) 
4.4.3. Performance 
The performance in time complexity of RHAC is shown in 
Figure 10, while the difference of the three thresholds is 
not so obvious.  
 
Figure 10. Performance of three keywords  
 
After the pilot test, we selected threshold B as our 
experimental design. 
 
4.5. Evaluation of HAC and RHAC 
In this section we will evaluate the result of HAC and 
RHAC. RHAC is conducted based on threshold B and 
Dimension Reduction of LSA to13 and then compare with 
that of HAC. The cluster results of „the Da Vinci Code‟ by 
both approaches are listed in Figs. 11 and 12, respectively. 
Where Ci indicates cluster number in level j. 
 
Fig. 11 shows that both C1 and C4 in level 1 include the 
concepts of the book (B1, B2) and religion (M2, M1) of 
The Da Vinci Code. The movie of The Da Vinci Code (F1, 
F2) merges with Da Vinci (A1) and book (B3), 
respectively. C1 and C2 both include the book, movie and 
religion concepts in level 2. It explains the limitation of 
HAC for its tendence to merge with the shortest distance of 
objects and then cause multiple clusters having the same 
concept in the same level. 
 
 
Figure 11. The clustered result of HAC 
 
Figure 12 presents the clustered result of RHAC. In level 1, 
only B1, B2 and B3 pass the threshold of average-linkage 
and merge with Ward‟s method. The concept of the newly 
merged cluster centers on the book of the Da Vinci Code. 
The threshold of level 2 doubles that of level 1 and then the 
three clusters of level 1 would be merged in level 2. The 
concept centers on the book and movie of The Da Vinci 
Code. M1, M2 in religion are also merged in level 2. The 
reason that Da Vinci (A1) does not merge with any cluster 
may due to these web pages discuss the relationship of Da 
Vinci with other topics, such as paint, does not discuss the 
book as The Da Vinci Code or the religion as Magdala.  
 
 
Figure 12. The clustered result of RHAC 
 
The result drawn from the experiments of traditional HAC 
and RHAC reveals that RHAC cooperates with LSA will 
accurately extract the concept of web pages. Furthermore, 
RHAC will produce new semantic spaces and increase 
precision. Performance of the hot topics was compared by 
using two algorithms shown in Figs. 13, 14 and 15. The 
precision and entropy were measured by / 2  and up 
to one level, where H indicates the height of hierarchical 
clustered result. Since the height of HAC is 9, so we chose 
the average value of level 5 and level 6 to measure. The 
average height of RHAC is usually 3. If the height is 4 with 
level 1, yet without processing any merge, then the level 2 
will be regarded as level 1.  
 
Our study shows that precision of RHAC is higher than 
0.99 and the entropy is less than 0.003. Both are superior to 
traditional HAC shown in Figs. 13, 14. The overall 
performance is also better than that of traditional HAC as 
Fig. 15.  
 
 
Figure 13. Precision 
 
 
Figure 14. Entropy 
 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/11/05
國科會補助計畫
計畫名稱: SOA架構之新聞知識萃取與展現
計畫主持人: 黃純敏
計畫編號: 98-2220-E-224-011- 學門領域: 自由軟體暨嵌入式系統 
研發成果名稱
(中文) SOA架構之新聞知識萃取與展現
(英文) Building News Knowledge Extraction and Representation System Based on Service 
Oriented Architecture
成果歸屬機構
國立雲林科技大學 發明人
(創作人)
黃純敏,陳俊廷,鄭唯毅,尹君耀,
林重佑,盧韋秀,陳聰宜,黃世源,
王秋淞
技術說明
(中文) 本研究提出的NQE(News Quality Evaluation)、NR(News Recommender)，以及
PNA(Personal News Application)三個服務，能有效改善新聞的品質，提供客製
化的資訊給使用者。研究中，新聞品質評估功能利用新聞多文件摘要，比對同主
題的個別新聞文件之撰述重點，以相似度高低做為新聞品質排名依據。在新聞推
薦服務上，我們使用了混合式過濾方式紀錄使用者對於新聞主題的喜好，並透過
新聞同好者的推薦，主動推播客製化的新聞給個別使用者。個人新聞應用程式主
要之目的在於提供使用者一個簡單快速的直覺化操作介面，讓使用者能夠有更方
便使用的平台閱讀新聞，並且推薦給其他讀者進一步閱讀此主題新聞所屬的相關
新聞報導。
(英文) This study proposed three services: News Quality Evaluation (NQE), News 
Recommender (NR), and Personal News Application (PNA) to enhance news quality and 
provide customized news to users via pc and mobile devices, so that users can receive 
real-time news and Multimedia report whenever and wherever. In the study, the NQE 
service is designed to determine news quality for the same event based on multiple 
document summarization and similarity comparison. A rank list will be published 
regularly. On the other hand, the NR service combines both content-based analysis and 
collaborative filtering to provide a personalized news delivery service. The most 
important purpose of PNA is to provide a simple, fast, good-to-use, and intuitive interface 
for the users; moreover, it is expected to have a more convenient platform to read and 
share news to other users with the same interests.
產業別 資訊服務業；研究發展服務業；其他專業、科學及技術服務業；廣播電視業；觀光及旅遊服務業
技術/產品應用範圍
财網頁雜訊過濾 
網頁雜訊過濾技術為一種資料的前置處理，針對任何關於文章、結構化文件、電子資訊
等有關文字應用領域皆適合，可利用該技術去除任何影響閱讀品質之雜訊，或是任何需
利用文字讀取、處理之資訊系統均可利用。 
财中文詞庫斷詞 
中文詞庫斷詞屬於資料前置處理，可適用於任何需要為文章(如：網路新聞、部落格文章、
社群網站文字資訊等)進行主題字詞擷取、剖析、讀取之資訊系統。 
财新聞自動偵測與追蹤 
新聞自動偵測與追蹤為一整合性之工具，可協助任何使用者有效率的獲得及關注其感興
趣的議題上，如新聞議題的追蹤、財經和股價資訊過去記錄、名人紀事等。 
财自動摘要 
自動摘要主要能在短時間內自動的將大量文字資訊及文件整理出重點，並且形成言簡意
賅之摘要，對於文字工作者、新聞從業人員、補教業者有莫大幫助。 
财動態知識圖 
動態知識圖能彙整知識，並將之轉換為圖形化方式展現，任何具有重要資訊或知識內容
均可應用，如新聞知識庫萃取展現、圖書知識庫之圖形化介面、人物傳記與歷史之呈現。
 
财新聞品質排名 
新聞品質排名可針對各種需進行優劣程度比較之中文資訊，皆可運用。無論是網路上的
多方來源或者是在資訊系統內之操作都能夠應用自如，如同事件中部落格之文章品質排
名、熱門音樂評比、3C產品優劣比較等。 
财新聞推薦 
新聞推薦針對使用者的個人資訊，以及新聞的內容做分析及處理，所以適用於需要對使
用者喜好、分析商品特徵的工作內容，如電子商務之產品行銷、廣告等。
技術移轉可行性及
預期效益
财網頁雜訊過濾 
技術移轉可行性高，效益佳。網頁雜訊過濾能迅速、準確的過濾不必要的資訊，並大幅
度提升使用者閱讀上的觀感，在資訊科技領域中能提升字詞文章後序處理的精確度與流
暢度。 
财中文詞庫斷詞 
技術移轉可行性高，效益佳。中文詞庫斷詞搭配相關主題詞庫，能在相關的領域文章中
迅速擷取主題詞彙，滅少傳統斷詞技術所耗費之時間。 
财新聞自動偵測與追蹤 
技術移轉可行性高，效益佳。新聞自動偵測與追蹤適用於各個議題與事件的追蹤得以呈
98年度專題研究計畫研究成果彙整表 
計畫主持人：黃純敏 計畫編號：98-2220-E-224-011- 
計畫名稱：SOA 架構之新聞知識萃取與展現 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 1 1 100%  
研討會論文 5 3 60% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 8 8 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
