  2
as S = { iX }. The FKM clustering algorithm is based on minimizing the following distortion: 
J = ij
k
j
N
i
q
ji du∑∑
= =1 1
,  (1) 
with respect to the cluster representatives Cj and memberships jiu , , where N is the number of data points; 
q is the fuzzifier; k is the number of clusters; and ijd  are the squared Euclidean distances between data 
points iX  and cluster representatives Cj. It is noted that jiu ,  should satisfy the following constraint: 
∑
=
k
j
jiu
1
, = 1, for i = 1 to N. (2) 
The major process of FKM is mapping a given set of representative vectors into an improved one through 
partitioning data points. It begins with an initial set of cluster centers and repeats this mapping process 
until a stopping criterion is satisfied. It is supposed that no two clusters have the same cluster 
representatives. In the case that two cluster centers coincide, a cluster center should be perturbed to avoid 
coincidence in the iterative process. If data point ijd  = 0, then jiu ,  = 1 and liu ,  = 0 for l ≠ j. The fuzzy 
k-means clustering algorithm is now presented as follows. 
(1) Input an initial set of cluster centers SC0 = {Cj(0)} and the value of ε. Set p = 0.  
(2) Given the set of cluster centers SCp, compute ijd  for i = 1 to N and j = 1 to k. Update membership 
jiu ,  using the following equation: 
jiu ,  = ( )
1
1
)1/(1
)1/(1 1
−
=
−
−
⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛∑k
l
q
il
q
ij d
d  (3) 
 If ijd  = 0, set jiu ,  = 1. 
(3) Compute the center for each cluster using equation (4) to obtain a new set of cluster representatives 
SCp+1.  
Cj(p+1) = ∑
∑
=
=
N
i
q
ji
N
i
i
q
ji
u
u
1
,
1
, X
  (4) 
(4) If )(-)1( pp jj CC + < ε for j = 1 to k, then stop, where ε > 0 is a very small positive number. 
Otherwise set p + 1 → p and go to step (2).  
 
The major computational complexity of FKM is come from steps (2) and (3). However, the computational 
complexity of step (3) is much less than that of step (2). Therefore the computational complexity, in terms 
of the number of distance calculations, of FKM is O(Nkt), where t is the number of iterations.  
  4
Λ = '
1 1
'
, ij
M
j
N
i
q
ji du∑∑
= =
- ⎟⎟⎠
⎞
⎜⎜⎝
⎛ −∑∑
==
M
j
q
ji
N
i
i u
1
'
,
1
1λ  (7) 
where iλ  is the Lagrangian multiplier and ', jiu  is the membership between data point Xi and its jth 
nearest cluster center. Differentiating Λ with respect to ' ,sru  gives 
'
,sru
Λ
∂
∂  = rijqsr duq λ−− '1' , )(  (8) 
Solving '
,sru
Λ
∂
∂  = 0 with respect to ' ,sru  gives 
'
,sru  = 
1
1
'
−
⎟⎟⎠
⎞
⎜⎜⎝
⎛ q
rs
r
qd
λ ,  for s = 1 to M. (9) 
Substituting equations (9) into (6) and using a bit of algebra, we will obtain 
rλ  = 1
1
1
1
'
1
−
=
−
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛∑
q
M
j
q
rjd
q  (10) 
Combining equations (9) and (10) gives 
'
,sru  = ( )
1
1
)1/(1
'
)1/(1' 1
−
=
−
−
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛∑M
j
q
rj
q
rs d
d , for r = 1 to N and s = 1 to M (11) 
If Cj∈NNTi is the lth nearest neighbor of Xi, set jiu ,  = ',liu ; otherwise let jiu ,  = 0, where jiu ,  are the 
memberships between points Xi and cluster centers Cj.  At this stage, we may rewrite equation (5) as: 
J = ij
k
j
N
i
q
ji du∑∑
= =1 1
,  (12) 
where ijd  is squared Euclidean distance between Xi and Cj.  It is noted that  
ijd = ( ) ( )jitji CXCX −−  (13) 
where t denotes transpose. Differentiating equation (12) with respect to jiu ,  gives 
)(2
1
, ij
N
i
q
jiu XC −∑
=
= 0 (14) 
Solving equation (14) for Cj gives 
  6
to j) and * 1, +jlu  = 1 due to 
*
)1( +jld  = 0. Let 
'
ids  = ∑
=
−
⎟⎟⎠
⎞
⎜⎜⎝
⎛j
l
q
ild1
)1/(1
'
1 , where 'ild  is the squared Euclidean 
distance between iX  and cluster representative lC . From equation (11), we may obtain 
'
ids  = '
,
)1/(1
'
1
ji
q
ij
u
d
−
⎟⎟⎠
⎞
⎜⎜⎝
⎛
 (18) 
It is noted that *, piu  = ( )
1
1
1
)1/(1
*
)1/(1* 1
−
+
=
−
−
⎟⎟⎠
⎞
⎜⎜⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛∑j
r
q
ir
q
ip d
d  (i = 1 to N and p = 1 to j+1) and *ild  = 
'
ild  (i = 1 to 
N and l = 1 to j). Using equation (11), we may obtain   
*
, piu = ( )
1)1/(1
*
)1(
')1/(1* )1(
−−
+
−
⎟⎟
⎟
⎠
⎞
⎜⎜
⎜
⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛+
q
ji
i
q
ip d
dsd  (i = 1 to N and p = 1 to j+1) (19) 
Note here that 
*
, plu  = 0 (p = 1 to j) and 
*
1, +jlu  = 1 (20) 
Substituting equations (18) to (19) into (17) gives 
J*(Xl) = ( ) *
1
1
1 ,1
)1/(1
*
)1(
)1/(1* )1( ip
j
p
N
lii
q
ji
i
q
ip dd
dsd
−
+
= ≠=
−
+
−∑ ∑ ⎟⎟⎟⎠
⎞
⎜⎜
⎜
⎝
⎛
⎟⎟⎠
⎞
⎜⎜⎝
⎛+  (21) 
where * 1, +jid  is the squared Euclidean distance between Xi and C*j+1 and C*j+1 = Xl.  
 In the case that j ≥ M, J*(Xl) is calculated by the following equation: 
J*(Xl) = *
1 1
*
, )( ip
M
p
N
i
q
pi du∑∑
= =
 (22) 
where *ipd  are the squared Euclidean distances between Xi and their pth nearest cluster centers from 
SC*(j+1) = {C*1, C*2, …, C*j, C*j+1} = {C1, C2, …, Cj, Xl}, where Xl is a data point in the set S. Let 
iNNT  consist of M nearest neighbors, which are in SC(j) = {C1, C2, …, Cj}, of Xi. Similarly denote 
*
iNNT  as the set consisting of M nearest neighbors from SC*(j+1) for Xi. Let rn = d( ][nNNTi , Xi) be the 
squared Euclidean distance between Xi and its nth nearest cluster center ][nNNTi . For a data point Xi, if 
d(Xi, Xl) ≥ rM, then Xl should not be in *iNNT . In this case, we have 
  8
Now, we would like to present the modified GFKM below. 
Modified GFKM  
(1) Input k and set SC(1) = {C1} and j = 1, where C1= ∑
=
N
i
i N
1
/)( X , N is the number of data points and k 
is the number of clusters.  
(2) Use the cluster center selection algorithm to determine Q. Let SC*(j+1) = {C*1, C*2, …, C*j+1}= 
{C1,…, Cj, Q}. 
(3) Use SC*(j+1) as the set of initial cluster centers for GFKM to generate SC(j+1) = {C1, C2, …, Cj+1}. 
(4) Set j = j+1. If j > k stop; otherwise go to step (2).  
4. 結果與討論 
To evaluate the performance of the proposed algorithms, a data set is used. This data set can be 
found in reference [12]. In the first example, the “Landsat Satellite” data set consists of 6,435 data points 
with 36 attributes. These data points are classified into 6 classes. The corresponding attribute values are 
integer and in the range of 0 to 255.  In the second example, the “Image Segmentation” data set has 2310 
data points with d = 19, where d is the data dimension. All the data points in this data set are divided into 
7 classes and the attribute values are real. In the third example, the “Yeast” data set is used. This data set 
consists of 1484 data points with 8 real attributes. The data points in the third example are classified into 
10 classes. More detailed descriptions for these three data sets can be found in reference [12]. In these 
examples, the proposed algorithms are compared to FKM and HKM in terms of computing time and the 
percentages of data points being classified correctly. For FKM and the proposed approaches, Xi is 
assigned to class j (cluster j) if jiu ,  > liu , , where j ≠ l.  
All computing is performed on an Intel Xeon 2.40 GHz PC with 4GB of memory. All programs are 
implemented as console applications of Microsoft Visual Studio 2008 and are executed under Window 
Server 2008 R2. 
 
Example 1: The “Image Segmentation” data set. 
In this example, the “Image Segmentation” data set has 2310 data points with d = 19. All the data 
points in this data set are divided into 7 classes. Table 1 shows the classification results of MGFKM with 
various values of M and the best classifications of FKM and HKM with 100 runs. From table 1, we may 
find that MGFKM with q = 1.5 and M = 6 gives the best classification accuracy. The best classification 
accuracy of FKM with 100 runs holds when q = 1.1. Compared to the best clustering result of FKM, 
MGFKM with q = 1.5 and M = 6 can increase the classification accuracy by 2.00%. Table 2 gives the 
computing time of HKM, FKM and MGFKM. From table 2, we also find that HKM with 100 runs has the 
least computing time and FKM with 100 runs possesses the highest computational complexity. Compared 
with of HKM with 100 runs and q = 1.1, MGFKM with q = 1.5 and M = 6 can decrease the computing 
time by 48.5%. 
 
Table 1: The classification accuracy of MGFKM and the corresponding classification accuracies of the 
  10
[6] U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Advances in Knowledge 
Discovery and Data Mining. MIT Press, Boston MA., 1996,  
[7] D. Liu, and F. Kubala, “Online speaker clustering,” Proc. IEEE Conf. on Acoustic, Speech, and 
Signal Processing, vol. 1, pp. 333-336. 2004.  
[8] P. Hojen-Sorensen, N. de Freitas, and T. Fog, “On-line probabilistic classification with particle 
filters,” Proc. IEEE Signal Processing Society Workshop, vol. 1, pp. 386-395, 2000. 
[9] M. Eirinaki and M. Vazirgiannis, “Web mining for web personalization,” ACM Trans. on Internet 
Technology, vol. 3, no. 1, pp. 1-27, February 2003. 
[10] I. Berget, B. H. Mevik, and T. Naæs, “New modifications and applications of fuzzy c-means 
methodology,” Computational Statistics & Data Analysis, vol. 52, no. 5, pp. 2403-2418, 2008. 
[11] J. Z. C. Lai and T. J. Huang, “Fast global k-means clustering using cluster membership and 
inequality,” Pattern Recognition. Vol. 43, no. 5, pp. 1954-1963, May 2010. 
[12] http://archive.ics.uci.edu/ml].   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  12
技術特點 
我們所提的方法將有較佳之影色分割結果。 
推廣及運用的價值 
尚未成熟 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研發成果
推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：賴榮滄 計畫編號：99-2221-E-019-042- 
計畫名稱：使用廣義的模糊 k均值分群法來分割影像 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 1 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
