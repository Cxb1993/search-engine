techniques will significantly shorten their lifetime 
or even cannot be employed. 
To cope with this problem, an emerging and hot 
research topic called ’distributed video coding, 
i.e., DVC’ has been devoted to reduce the complexity 
for video encoding to the order of that for still 
image encoding (e.g., JPEG-2000). Nevertheless, DVC 
cannot actually meet the low-complexity requirement 
because the video acquisition process still relies on 
sampling complete raw data and compression. Recently, 
an emerging information theory, called 
compressive/compressed sensing/sampling (CS), has 
been an important research topic. CS delivers a 
different viewpoint of sampling/sensing and can be 
employed to various applications. Based on the 
sparsity of data in some transform domain, CS 
can ’directly’ acquire compressed signal which is 
sparse or compressible (sparse with respect to some 
basis). In addition, compressive sensing-based 
compressed data has been shown to be computationally 
secure, which can be viewed as ’encrypted’ data to 
some extent. 
In this three-year project, we have planned to 
develop a low-complexity image/video acquisition and 
coding method, and study its applications on 
multimedia. Our framework can be embedded in a CS-
based single pixel camera in the future. When the 
serve or receiver receives the CS-based compressed 
data, we can employ some convex optimization- or 
greedy-based CS recovery algorithms to reconstruct 
the original signals. 
英文關鍵詞： Low-complexity video coding、distributed video coding 
(DVC)、compressive sensing (CS)、compressed image 
sensing (CIS)、sparse representation、distributed 
compressive video sensing (DCVS)、single-pixel camera
 
中文摘要 
現今數位多媒體資料的擷取與壓縮皆分成兩階段實施，再利用現行壓縮標準來進行壓縮，
導致部分資料先被儲存再被丟掉浪費的情形。以現有視訊壓縮標準 (例如: H.264/AVC) 而
言，編碼端利用複雜的運動估測運算來達到良好的壓縮效率。然而，對於低複雜度的行動
視訊元件或適用於無線感測網路的視訊感測器而言，複雜的編碼法會使其電源快速耗盡縮
短其生命期或根本不適合使用。 
為了解決此問題，一個熱門研究議題- “分散式視訊編碼” 致力於將視訊編碼複雜度降低至
單張影像編碼 (例如: JPEG-2000) 複雜度。然而，該方法依然架構於傳統視訊元件上，仍
需要耗費大量記憶體來暫存龐大的原始擷取資料，再進行壓縮。最近，一種新的資料取樣
技術-“壓縮感測(compressive/compressed sensing/sampling)”受到相當的注目，成為重要的研
究議題。壓縮感測改變人們對 sensing/sampling 的看法，它也能廣泛應用在不同領域。利
用資料本身或於某個基底之下具有 “稀疏表示” 的特性，壓縮感測可以直接取得壓縮的資
料。另外，以壓縮感測取得的壓縮資料已被證明具有計算上安全性，亦即可視為加密過的
資料。 
在這三年計畫中，我們提出以“分散式視訊編碼”及“壓縮感測”技術為基礎，研究低複雜度
視訊擷取與壓縮及其在多媒體之應用。 我們的方法在未來可以植基於一種以壓縮感測為基
礎之低複雜度硬體架構-“單像素照相機。” 基於此架構，我們可以實現 “直接” 取得壓縮及
加密的影像/視訊資料，而不需在編碼端暫存完整原始資料且不需要執行高複雜度運動估測
運算。待接收端或伺服器取得壓縮及加密後的視訊資料，我們利用所提出的具可靠性之多
媒體資料重建技術來同時還原及解密原始視訊/影像資料。 
 
關鍵詞:  
低複雜度視訊編碼、分散式視訊編碼、壓縮感測、壓縮影像感測、稀疏表示、分散式視訊
壓縮感測、單像素照相機。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 4
報告內容 
前言 
 傳統的視訊編碼標準通常是基於一個具有複雜編碼端 (complex encoder) 及簡單解碼端 (simple 
decoder) 的架構。其中編碼端的計算複雜度大約是解碼端計算複雜度的 5~10 倍，這種架構比較適用於
視訊 “廣播 (broadcasting)” 或單純的視訊資料儲存 (例如: DVD 或 VCD) 的應用。亦即由一個具有強
大運算能力的編碼器負責擷取視訊資料及執行視訊編碼，再將編碼後的視訊串流 (video stream) 傳送
到各個接收端，而各接收端只需支援一般運算能力的解碼器即可。然而，考慮行動多媒體運算 (mobile 
multimedia computing) 及無線多媒體感測網路 (wireless multimedia sensor networks)的應用時，傳統高
複雜度的編碼架構已經不適合應用於新的視訊編碼應用。例如: 若將視訊編碼器嵌入行動視訊元件 
(mobile video device) 或無線視訊感測器 (wireless visual sensor)，則必須考慮此類視訊元件通常為低功
率 (low power) 或低運算能力 (low computation capability) 且大多為電力耗盡隨即丟棄而不回收充電
的元件，因此無法使用太複雜的視訊編碼法 (例如: H.264/AVC)。另外，隨著雲端計算 (cloud computing)
的興起，資料擷取端 (或編碼端) 的工作量應該越來越小 (或複雜度越來越低)。複雜的後端資料處理
工作或應用 (例如: 資料解碼或資訊檢索等) 應該透過網路交由網路上的伺服器 (server) 來處理。因為
網路伺服器可支援強大硬體、運算能力及無限的電源。因此，相對於傳統編碼架構，簡單編碼端搭配
複雜解碼端的架構需求已儼然形成。有鑒於此，在這三年計畫中，我們提出以“分散式視訊編碼”及“壓
縮感測”技術為基礎，研究低複雜度視訊擷取與壓縮及其在多媒體之應用。 
 
研究目的 
 傳統上為了降低視訊編碼複雜度，大部分的研究著重於 fast motion estimation 相關技術。Motion 
estimation (ME) 的計算複雜度可能達到整體壓縮流程計算複雜度的 80%以上，然而，以目前最新的
H.264/AVC video 而言，除了 ME 以外，尚有其他複雜的運算例如: in-loop de-blocking filter 運算。根據
我們以目前最新的 H.264/AVC software JM 17.0 來測試，即使針對 ME 運算減少其複雜度超過 90%，對
整體壓縮運算量而言，可能只減少70%的計算複雜度。因此，一個熱門的研究議題-DVC (distributed video 
coding)在 encoder 完全捨棄 ME 運算並將編碼計算複雜度降低至類似單張影像編碼複雜度 (例如: 
H.264/AVC intraframe 或 JPEG-2000)。然而，即使對於單張影像編碼，軟硬體複雜度依然包含: (i) 必須
先取得完整視訊原始資料 (raw data): 必須耗費大量記憶體來暫存該 raw data;及 (ii) 接著必須進行轉
換 (例如: discrete cosine transform, i.e., DCT 或 discrete wavelet transform, i.e., DWT) 來消除空間資料的
重複性 (redundancy)、quantization 及 entropy coding 等步驟。我們認為既然必須將某些 redundancy 消除，
當初又何必耗費記憶體來暫存完整 raw data，何不一開始就 “直接” 取得需要的資料就好。近年來，壓
縮感測(compressed/compressive sensing/sampling)理論與技術的發展，讓直接擷取壓縮資料變得可能。 
本三年計畫研究的主要目的為(i)第一年提出低複雜度 power-rate-distortion (PRD)-optimized 視訊壓
縮技術; (ii)第二年提出植基於 compressive sensing (CS)-based single-pixel camera 的分散式視訊壓縮感測 
(distributed compressive video sensing, i.e., DCVS) 技術; (iii) 第三年研究壓縮影像感測 (compressive 
image sensing)技術。 
 
文獻探討 
 在這三年計畫中，我們研究兩大主題，分別是分散式視訊編碼與壓縮感測。這兩大議題可分別在
http://www.discoverdvc.org 與 http://dsp.rice.edu/cs 找到幾乎目前所有文獻。相關文獻探討請參考計畫書
與所發表之論文，在此便不再贅述。 
 6
Video Sensing,’’ Proc. 28th Picture Coding Symposium (PCS), Japan, Dec. 07~10, 2010. 
[6]Li-Wei Kang, Chao-Yung Hsu, Hung-Wei Chen, Chun-Shien Lu, Chih-Yang Lin, and Soo-Chang Pei, 
``Feature-based Sparse Representation for Image Similarity Assessment,’’ IEEE Trans. on Multimedia, Vol. 13, 
No. 5, pp. 1019-1030, 2011. 
[7]``Distributed Compressive Video Sensing,’’ will be submitted to an international journal. 
[8]``Compressive Sensing-based Image Sensing,’’ will be submitted to an international journal. 
 
第三年成果 
[1]Hung-Wei Chen, Chun-Shien Lu, and Soo-Chang Pei, ``Fast Compressive Sensing Recovery with 
Transform-based Sampling,’’ Proc. Workshop on Signal Processing with Adaptive Sparse Structured 
Representations, June 27-30, Edinburgh, UK, 2011. 
[2]Li-Wei Kang, Chin-Yang Lin, Hung-Wei Chen, Chia-Mu Yu, Chun-Shien Lu, Chao-Yung Hsu, and 
Soo-Chang Pei, ``Secure Transcoding for Compressive Multimedia Sensing,’’ Proc. IEEE Int. Conf. on Image 
Processing, Brussels, Belgium, Sept. 11-Sept. 14, 2011. 
[3] ``Compressive Image Sensing with Turbo Fast Recovery,’’ submitted to an international conference. 
[4]``Compressive Image Sensing with Lower-Frequency Measurement Sampling and Fast Recovery,’’ 
submitted to an international journal. 
[5]``Feasible Sparse Signal Recovery via Iteratively Refining L2-norm Solution with Performance Bound 
Analysis,’’ submitted to an international journal. 
 
參考文獻  
Distributed Video coding (DVC) 
[1] B. Girod, A. M. Aaron, S. Rane, and D. Rebollo-Monedero, “Distributed video coding,” Proceedings of 
the IEEE, vol. 93, no. 1, pp. 71-83, Jan. 2005. 
[2] F. Pereira, L. Torres, C. Guillemot, T. Ebrahimi, R. Leonardi, and S. Klomp, “Distributed video coding 
selecting the most promising application scenarios,” Signal Processing: Image Communication, vol. 23, 
no. 5, pp. 339-352, June 2008. 
[3] T. Wiegand and G. J. Sullivan, “The H.264/AVC video coding standard,” IEEE Signal Processing 
Magazine, vol. 24, no. 2, pp. 148-153, March 2007. 
[4] http://www.discoverdvc.org 
 
Compressive/Compressed Sensing/Sampling (CS) 
[5] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. Kelly, and R. G. Baraniuk, 
“Single-pixel imaging via compressive sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 
83-91, Mar. 2008. 
[6] E. Candès and M. Wakin, “An introduction to compressive sampling,” IEEE Signal Processing Magazine, 
vol. 25, no. 2, pp. 21-30, March 2008. 
[7] L. W. Kang and C. S. Lu, “Distributed compressive video sensing,” in Proc. of IEEE Int. Conf. on 
Acoustics, Speech, and Signal Processing, Taipei, Taiwan, April 2009, pp. 1169-1172. 
[8] M. A. T. Figueiredo, R. D. Nowak, and S. J. Wright, “Gradient projection for sparse reconstruction: 
application to compressed sensing and other inverse problems,” IEEE J. of Selected Topics in Signal 
 8
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
  達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：已發表 未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 無 
技轉：□已技轉 □洽談中 無 
其他：（以 100 字為限） 
 
 
 
 
 10
國科會補助計畫衍生研發成果推廣資料表 
日期：100 年 11 月 19 日 
國科會補助計畫 
計畫名稱：先進分散式視訊編碼技術之研究 (第三年) 
計畫主持人：呂俊賢 
計畫編號：NSC 97-2628-E-001-011-MY3  
學門領域：資訊學門二 (EB) 
研發成果名稱 
（中文）壓縮影像感測 
（英文）Compressive Image Sensing 
成果歸屬機構 中央研究院 發明人 (創作人) 
呂俊賢、陳泓瑋 
技術說明 
（中文）（200-500 字） 
壓縮影像感測(Compressive Image Sensing)此一技術可同時完成
data acquisition and compression, 而得到已壓縮資料, 不需
儲存原 raw data, 再用 compression codec 去獲得 compressed 
data,以便節省 storage and communication overhead.在壓縮影
像感測的研究中, 為了從measurements得到較佳重建品質, 一格
常見作法是力用 image 之 transform coefficients 間的
dependency or correlation patterns. 然而, 目前文獻中的方
法在重建品質與重建速度方面, 仍有相當大的改善空間. 
在這計劃中, 我們研究一個快速的壓縮影像感測, 其計算複雜度
僅 O(m^2), 其中 m表示 a measurement vector y=\phi x 之長度,x
是 signal x of length n, \phi 是 sampling matrix \phi with 
dimensionality mXn.為了達成較佳重建品質與重建速度, 我們研
究一個新的 sampling matrix 設計方法以及研究不同之 sensing
方式. 
我們的貢獻包括: 
(i)由於我們發展一個 closed-form solution for compressive
sensing recovery, 故重建速度相當快; 
(ii)由於我們的方法在 sensing 中能 sample 重要 transform 
coefficients, 重建品質能維持如像 JPEG images 水準; 
(iii)除了研究 1D sensing, 我們也發展 2D separate sensing 以
便同時完成大張影像之 acquisition 與 compressionzed images.
實驗結果也顯示, 我們的方法在重建品質與重建速度方面皆優於
state-of-the-art. 
 12
視訊監控網路、無線視訊感測網路、行動視訊會議及用完即丟棄
的數位相機/攝影機)。 
     註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
will warp Ki,t to the viewpoint of Wj,t (i = 0, j = 1, and t = 45) to get 
K’i,t (the second reference frame of Wj,t). Then, each initial 
significant symbol of Wj,t will be compared with the co-located 
symbol of K’i,t (step (d)) to determine the true SDS symbols (step 
(e)), whose compressed position information will be sent back to 
Vj. Finally, the coefficients corresponding to each true significant 
SDS symbol are quantized and encoded jointly with the coding 
mode information via run-length and entropy coding. In addition, 
each block with intra mode is encoded using H.264/AVC intra 
encoder [2] while each block with skip mode is skipped. On the 
other hand, similar to [6], the key frames of adjacent VSNs can be 
further compressed while they are transmitted through the same 
intermediate node toward the AFN. 
V0
V1
K0,45
W1,45
Warping
(d) Block-based SDS extraction and comparison
(e) True significant symbols extraction
Quantization and 
entropy-encoding
Compressed bitstream 
for the blocks with inter 
mode in W1,45
K’0,45
R1,45 = K1,44
(a) Co-located blocks comparison
(b) Block-based SDS extraction and comparison
(c) Initial significant symbols extraction
Initial significant symbols for W1,45
SDS for K’0,45
Target 
scene
Significant wavelet 
coefficients for W1,45
 
Fig. 1. An example for our multiview video codec. 
 
3. PRD OPTIMIZED RESOURCE ALLOCATION 
Since block coding mode is related to the RD performance, it is 
important to characterize the relationship between the available 
resources and the RD performance. The major objective is to 
optimize the reconstructed video quality and maximize the lifetime 
for a VSN under current resource constraints. In this section, the 
proposed PRD model for video quality optimization is briefly 
addressed. More details can be found in [10]. 
3.1. Block coding mode determination 
First, without performing motion estimation, for a non-key frame 
consisting of Nb blocks, the motion activity for each block is 
estimated by the SAD between itself and its reference block. A 
block with larger motion activity tends to be with intra mode 
whereas that with smaller motion activity tends to be with skip 
mode. All the blocks in a non-key frame are sorted in a decreasing 
order based on their motion activities. Assume that there are NIntra, 
NInter, and NSkip blocks determined to be coded with intra, inter, and 
skip modes, respectively, where NIntra + NInter + NSkip = Nb. Let {Bi, 
i = 1, 2, …, NIntra}, {Bi, i = NIntra + 1, NIntra + 2, …, NIntra + NInter}, 
and {Bi, i = NIntra + NInter + 1, NIntra + NInter + 2, …, NIntra + NInter + 
NSkip (= Nb)} denote the sets of blocks with intra, inter, and skip 
modes, respectively. Let X, Y, and Z, respectively, denote NIntra/Nb, 
NInter/Nb, and NSkip/Nb, where X+Y+Z=1. The optimal 
determination of X, Y, and Z according to the current resources is 
equivalent to the determination of the coding mode for each block, 
described in Secs. 3.2-3.3. 
3.2. Power-rate-distortion model 
In the proposed PRD model, our non-key frame encoding 
procedure can be roughly viewed as the combination of several 
“atom operations,” including the intra-mode block encoding (DCT 
and quantization), the inter-mode block encoding (hash extraction, 
hash exchange, hash comparison, and quantization), and the 
entropy encoding operations. The encoding operation for a block 
with skip mode is ignored due to only the coding mode information 
being encoded, which is included in the entropy encoding 
operation. Let the normalized computational complexity for the 
intra encoding, inter encoding, and entropy encoding operations be 
C1, C2, and C3, 0 ≤ C1, C2, C3 ≤ 1, respectively. For the available 
resources consisting of the encoding power P (watt = Joule per 
second) and target bit rate R (bits per pixel, i.e., bpp), the 
computational complexity for non-key frame encoding per second 
can be expressed as: 
F×(C1×X + C2×Y + C3×R) ≤ Φ(P),                          (1) 
where F is the normalized frame rate, 0 ≤ F ≤ 1, and Φ(P), 0 ≤ Φ(P) 
≤ 1, is the normalized power consumption for P transformed by the 
power function Φ(•) under the assumption that the DVS (dynamic 
voltage scaling, a CMOS circuit design technology) is employed 
[7]. To optimally decide the coding mode for each block according 
to the current resources (P and R), an RD function for non-key 
frame encoding should be derived and minimized. 
The classic RD function can be expressed as [7]: 
( ) .s.t,21min
1
22∑
=
−
⋅=
b
i
i
N
i
R
i
b
R N
D γσ ,1
1
RR
N
bN
i
i
b
=∑
=
                   (2) 
where Ri is the bit rate of the i-th block, 2iσ  is the variance (the 
maximum possible distortion) of the i-th block, and γ is a model 
parameter related to encoding efficiency. Based on the Lagrangian 
multiplier technique, the minimum distortion obtained by the 
optimal bit allocation can be expressed as: 
R
NN
i
i
bb
D γσ 2
1
1
2 2−
=
⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛
= ∏ .                                  (3) 
Based on Eq. (3), obviously, the RD function for a block with intra 
mode can be expressed as: 
R
NN
N
NN
i
IntraiIntra
InterIntra
b
IntraIntra
D +
−
=
⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛
= ∏ γσ 2
1
1
2
, 2
,                   (4) 
where 2
,Intraiσ  is the variance of the i-th block with intra mode. On 
the other hand, a block with inter mode includes some significant 
coefficients (corresponding to the significant SDS symbols) being 
entropy-encoded, and the other insignificant coefficients being 
skipped and predicted by the corresponding coefficients in its 
reference block. Hence, the RD function for a block with inter 
mode can be expressed as: 
,12
1
2
,
2
1
1
2
, ∑∏ +
+=
+
−
+
+=
+⋅⎟⎟⎠
⎞
⎜⎜⎝
⎛
=
InterIntra
Intra
InterIntra
b
InterInterIntra
Intra
NN
Ni
Interi
Inter
R
NN
NNNN
Ni
InteriInter N
D δσ
γ   (5) 
where 2
,Interiσ  is the variance of the pixels corresponding to the 
significant coefficients in the i-th block with inter mode. 2
,Interiδ  is 
the MSE between the pixels corresponding to the insignificant 
coefficients and the corresponding pixels in its reference block. 
Note that in the block coding mode decision process, for a block 
with inter mode, only the first reference block from the same VSN 
is considered. This is because prior to actual video encoding, it is 
unworthy to waste power to perform hash data exchanges between 
VSNs. In addition, for a block with skip mode, the RD function is 
simply the MSE (denoted by 2
,Skipiδ ) between the block and its 
reference block as: 
.1
1
2
,∑
++=
=
b
InterIntra
N
NNi
Skipi
Skip
Skip N
D δ
                                   (6) 
3.3. Power-Rate-Distortion Optimization 
Based on Eqs. (4)-(6), the overall RD function of a block in our 
video codec for non-key frame encoding can be expressed as: 
DOverall = (1/Nb)×(NIntra×DIntra + NInter×DInter + NSkip×DSkip) 
            = X×DIntra + Y×DInter + Z×DSkip.                                         (7) 
To minimize DOverall based on optimally selected X, Y, and Z, 
where Z = 1 – X – Y, under the constraint shown in Eq. (1), DOverall 
RD performance and encoding complexity, were used for 
performance evaluation. 
0
30
60
90
120
150
180
210
240
270
300
330
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 R (bpp)
MSE Φ(P)=0.05 (Estimated) Φ(P)=0.05 (Actual)
Φ(P)=0.1 (Estimated) Φ(P)=0.1 (Actual)
Φ(P)=0.5 (Estimated) Φ(P)=0.5 (Actual)
Φ(P)=0.7 (Estimated) Φ(P)=0.7(Actual)
Φ(P)=1.0 (Estimated) Φ(P)=1.0 (Actual)
 
Fig. 3. The curves “Estimated” show the PRD performance obtained from 
our PRD model, whereas the curves “Actual” show the actual PRD 
performance obtained from our video codec for the Ballroom sequence. 
 
The average PRD performances for the three adjacent VSNs 
listed in Table 1 are shown in Fig. 4(a) and (b), respectively, for 
the Ballroom and Exit sequences. For the Ballroom sequence, it 
can be observed from Fig. 4(a) that the PSNR performance gains 
of our multiview video codec at Φ(P) = 1.0 above those of the 
H.264 No motion are from 0.1 to 4 dB. The PSNR performance 
gains of our multiview codec at Φ(P) = 1.0 above those of the 
H.264 Intra are from 2 to 6 dB. The RD performance of our codec 
at Φ(P) = 0.5 is very close to that of our codec at Φ(P) = 1.0, 
especially at higher bit rates. The RD performance of our codec at 
Φ(P) = 0.05 is very poor. The PSNR performance gains of our 
codec at higher powers can significantly outperform our single-
view codec. Similar results can also be observed from Fig. 4(b) for 
the Exit sequence. More specifically, our codec can outperform the 
three approaches used for comparisons, especially at high power 
and low bit rates. That is, when the power is high, our encoder can 
efficiently exploit the available bit rates to optimize the video 
quality, even though the bit rate is low. In addition, with the 
benefits of exploiting the reference frames from the adjacent view, 
our encoder can have more skipped SDS symbols or skipped 
blocks, which can save more bit rates. 
26
28
30
32
34
36
38
40
0 100 200 300 400 500 600 700 800 900 1000 Bitrate (kbps)
PSNR (dB)
Proposed (Φ(P) = 1.0) Proposed (Φ(P) = 0.5)
Proposed (Φ(P) = 0.1) Proposed Single
H.264 No motion (GOP = ∞) H.264 No motion (GOP = 2)
H.264 Intra (GOP = 1)
26
28
30
32
34
36
38
40
0 100 200 300 400 500 600 700 800 900 1000 Bitrate (kbps)
PSNR (dB)
Proposed (Φ(P) = 1.0) Proposed (Φ(P) = 0.5)
Proposed (Φ(P) = 0.1) Proposed Single
H.264 No motion (GOP = ∞) H.264 No motion (GOP = 2)
H.264 Intra (GOP = 1)
 
Fig. 4. The RD performances for the (a) Ballroom, and (b) Exit sequences. 
 
Although it is claimed that the proposed codec and the existing 
codecs used for comparisons are all with low-complexity encoder, 
it is still important to compare their encoding complexities. The 
respective average encoding time per frame for the Ballroom 
sequence of the evaluated codecs, measured on a Pentium-4 PC 
with 3.40GHz CPU and 1.49GB RAM at different bit rates is 
shown in Fig. 5. The encoding time of the proposed codec includes 
the time consumed in interview hash data exchanges, where the 
hash data size is relatively small (e.g., average 3.78 kbit, i.e., about 
0.16% of the original frame size, per frame). By considering the 
typical data transmission rate, 40 kbps, for a common sensor node 
[1] (actually, the rate may be higher for a VSN, e.g., 250 kbps [1] 
or 1 Mbps [6]), it takes only 0.09 seconds to achieve hash data 
exchange per frame. Fig. 5 shows that the encoding complexity of 
our codec is lower than those of the three H.264/AVC low-
complexity codecs, even though when the full power is applied. 
1.8
2.1
2.4
2.7
3.0
3.3
0 200 400 600 800 1000 Bitrate (kbps)
Average
encoding
 time (sec)
Proposed (Φ(P) = 1.0) Proposed (Φ(P) = 0.5)
H.264 No motion (GOP = ∞) H.264 No motion (GOP = 2)
H.264 Intra (GOP = 1)
 
Fig. 5. The average encoding time per frame for the Ballroom sequence. 
 
5. CONCLUSIONS 
In this paper, we have proposed a PRD model to characterize the 
relationship between the available resources and the RD 
performance of our low-complexity multiview video codec. Based 
on this model, the resource allocation can be efficiently performed 
at the encoder while optimizing the reconstructed video quality. 
Analytic results have been provided to verify the resource 
scalability and accuracy of the proposed PRD model. For future 
work, the distortion induced by wireless video transmission will be 
integrated into the current distortion function to form a complete 
end-to-end distortion function. More precise theoretical analyses, 
such as the optimal achievable video quality based on available 
resources and the minimum resource requirements based on 
acceptable video distortion, need to be derived to provide a 
practical guideline in preparation and deployment for a WVSN. 
 
ACKNOWLEDGEMENT 
This work was supported in part by National Science Council, Taiwan, 
ROC, under Grants NSC95-2422-H-001-031 and NSC 95-2221-E-001-022. 
 
REFERENCES 
[1] I. F. Akyildiz, T. Melodia, and K. R. Chowdhury, “A survey on 
wireless multimedia sensor networks,” Computer Networks, 2007. 
[2] T. Wiegand and G. J. Sullivan, “The H.264/AVC video coding 
standard,” IEEE Signal Processing Mag., vol. 24, pp. 148-153, 2007. 
[3] A. Smolic et al., “Coding algorithms for 3DTV-a survey,” IEEE Trans. 
on Circuits and Sys. for Video Tech., vol. 17, pp. 1606-1621, 2007. 
[4] C. Guillemot et al., “Distributed monoview and multiview video 
coding: basics, problems and recent advances,” IEEE Signal 
Processing Magazine, vol. 24, no. 5, pp. 67-76, Sept. 2007. 
[5] X. Guo et al., “Wyner-Ziv-based multi-view video coding,” IEEE 
Trans. on Circuits and Sys. for Video Tech., vol. 18, 2008. 
[6] M. Wu and C. W. Chen, “Collaborative image coding and transmission 
over wireless sensor networks,” J. Advances Signal Process., 2007. 
[7] Z. He and D. Wu, “Resource allocation and performance analysis of 
wireless video sensors,” IEEE Trans. on Circuits and Systems for 
Video Technology, vol. 16, no. 5, pp. 590-599, May 2006. 
[8] L. W. Kang and C. S. Lu, “Multi-view distributed video coding with 
low-complexity inter-sensor communication over wireless video sensor 
networks,” Proc. IEEE Int. Conf. on Image Processing, Sept. 2007. 
[9] C. S. Lu and H. Y. M. Liao, “Structural digital signature for image 
authentication: an incidental distortion resistant scheme,” IEEE Trans. 
on Multimedia, vol. 5, no. 2, pp. 161-173, June 2003. 
[10] L. W. Kang and C. S. Lu, “Power-rate-distortion optimized resource 
allocation for low-complexity multiview distributed video coding,” 
TR-IIS-08-003, IIS Technical Report, Institute of Information Science, 
Academia Sinica, 2008. 
[11] L. W. Kang and C. S. Lu, “Low-complexity Wyner-Ziv video coding 
based on robust media hashing,” Proc. IEEE MMSP, Oct. 2006. 
scalar quantization and channel encoding. Hence, the complexity 
of the DVC encoder is similar to that of still image encoder 
consisting of transformation, quantization, and entropy encoding. 
2.3. Compressive sensing (CS) 
Assume that a sparse basis matrix Ȍ with size N×N can provide a 
K sparse representation for a real value signal x with length N. 
That is, x can be represented as x =Ȍș and ș with length N can be 
well approximated using only K << N non-zero entries. CS [3]-[8] 
states that x can be accurately reconstructed by taking only: 
M = O(Klog(N/K)),                                    (2) 
where K < M << N, linear and non-adaptive  measurements from: 
y = Ɏx = ɎȌș = Aș,                                    (3) 
where y is an M×1 vector, Ɏ is an M×N measurement matrix that is 
incoherent with Ȍ, and A = ɎȌ. More specifically, the M 
measurements in y are random linear combinations of the entries of 
ș, which can be viewed as the compressed and encrypted version 
of x. Currently, it is unclear that how to efficiently quantize and 
entropy-encoded the M measurements [5], which will be left for 
the future research. To reconstruct ș from y, CS is based on 
solving the convex optimization problem [3]-[10] (e.g., linear 
programming or GPSR [9]) or some iterative greedy algorithms 
(e.g., OMP [11]). Finally, x can be reconstructed via ~~ x , 
where ~ is the reconstructed ș. 
2.4. Compressive image/video sensing 
In compressive image sensing, if an image x can be sparsely 
represented using a basis Ȍ (e.g., DWT), x can be compressed via 
the CS technique in Eq. (3) and reconstructed via some CS 
recovery algorithms [3]-[11]. On the other hand, compressive 
video sensing has been first proposed in [8], where each video 
block, at the encoder, is classified to be either sparse or non-sparse 
via a CS test. Each sparse block is compressed via CS, whereas 
each non-sparse block is fully sampled. 
2.5. Distributed compressive sensing (DCS) 
Distributed compressive sensing (DCS) [6] exploits both intra-
signal and inter-signal correlation structures. Consider a sensor 
network scenario, several sensors measure signals that are each 
individually sparse in a certain basis and also correlated among 
sensors. In DCS, each signal is independently measured via a CS 
technique and jointly reconstructed at a collection point collecting 
measurements from multiple sensors. 
 
3. DISTRIBUTED COMPRESSIVE VIDEO SENSING (DCVS) 
In this section, the joint sparsity model of our DCVS is first 
described in Sec. 3.1 to show the guideline for exploiting the 
statistical dependencies among successive frames. In our DCVS 
encoder described in Sec. 3.2, each frame is independently 
compressed via a CS measurement process. In our DCVS decoder, 
each frame is jointly reconstructed using our modified GPSR 
incorporating the proposed initialization derived from DVC side 
information generation and the proposed stopping criteria derived 
from statistical dependencies among successive frames, described 
in Secs. 3.3-3.6. Note that our DCVS, at its current status, is only 
designed for single-view videos, which can be extended to 
multiview video scenario, and can be applicable in a WVSN. 
3.1. Joint sparsity model 
To exploit the correlation among successive frames, similar to [6], 
the joint sparsity model in our DCVS can be described as follows. 
Assume two successive frames, xt and xt+1, in the same scene are 
visually similar, where t is the time instant. That is, xt and xt+1 
should have similar common portion and respective unique 
portions. Conceptually, the two frames can be expressed as: 
xt = xC + xt_U,                                           (4) 
xt+1 = xC + xt+1_U,                                     (5) 
where xC is the similar/common portion between xt and xt+1, xt_U 
and xt+1_U are the unique portions of xt and xt+1, respectively. By 
treating xt as a reference frame for xt+1, in conventional video 
coding, the encoder will perform motion estimation to find the 
predictor (similar to xC) for xt+1 and encode the difference (similar 
to xt+1_U) between xt+1 and its predictor. Hence, the compression of 
xt and xt+1 can be achieved by compressing xt and xt+1_U. Assume a 
sparse basis matrix Ȍ can provide Kt and Kt+1_U sparse 
representations for xt and xt+1_U, respectively, as: 
xt = Ȍșt, ||șt||0 = Kt,                                         (6) 
xt+1_U = Ȍșt+1_U, ||șt+1_U||0 = Kt+1_U,              (7) 
where șt and șt+1_U are the sparse representations of xt and xt+1_U, 
respectively, and ||ș||0 is the number of nonzero entries in ș, i.e., Ɛ0 
norm of ș. Usually, Kt  Kt+1_U, and based on Eq. (2), Mt  Mt+1_U, 
where Mt and Mt+1_U are the number of measurements of xt and 
xt+1_U, respectively. Although, in DCVS, it is impossible to find 
xt+1_U at the encoder due to low-complexity restriction, it can be 
confirmed that the number of measurements (Mt+1) of xt+1 can be 
smaller than that (Mt) of xt if the correlation between them can be 
adequately exploited by treating xt as a reference frame for xt+1. 
3.2. DCVS encoder 
In DCVS, a video sequence consists of several GOPs (group of 
pictures), where a GOP consists of a key frame followed by some 
non-key frames. Conceptually, each key frame serves as a 
reference frame for its neighboring non-key frames. At our DCVS 
encoder shown in Fig. 1, without performing motion estimation, 
without needing any prior knowledge about correlation among 
successive frames, and without performing extra tasks (no 
additional burden) in the CS process described in Sec. 2.3, each 
frame xt (key frame or non-key frame) with size N is compressed 
via the CS measurement process (Eq. (3)) as: 
                           yt = Ɏxt,                                          (8) 
where yt is the measurement vector with size Mt×1 and Ɏ is the  
Mt×N measurement matrix described later. Based on the joint 
sparsity model in Sec. 3.1, the measurement rate (MR) of a key 
frame should be larger than that of a non-key frame. The MR for a 
frame Xt can be defined as MRt = Mt/N. 
CS measurement
yt = Фxt
Each frame xt
Measurement vector 
(compressed frame) yt  
Fig. 1. Our DCVS encoder. 
Here, the exploited measurement matrix Ɏ is the scrambled 
block Hadamard ensemble (SBHE) matrix [7], which takes the 
partial block Hadamard transform, followed by randomly 
permuting its columns. SBHE has been shown to satisfy the five 
requirements, including near optimal performance, university, fast 
computation, memory efficient, and hardware friendly, and 
outperform several existing ones (e.g., the Gaussian i.i.d matrix 
and the binary sparse matrix) [7]. The sparse basis matrix Ȍ used 
in this paper is DWT basis. 
After performing the CS process, the measurement vector yt for 
each frame xt (the compressed version for xt) will be transmitted to 
the decoder. At the decoder, each key frame is reconstructed via 
the GPSR algorithm [9] while each non-key frame is reconstructed 
via GPSR incorporating with the statistical dependencies among 
successive frames, to be addressed in Secs. 3.3-3.6. 
3.3. Gradient projection for sparse reconstruction (GPSR) 
At the decoder, each key frame xt = Ȍșt with size N is 
reconstructed via GPSR [9], which solves the convex 
unconstrained optimization problem as: 
1170
(b) MR is middle (20% < MR  70%): if Eq. (10) with TĮ = 0.05 or 
Eq. (13) is satisfied, the algorithm will stop.  
(c) MR is high (MR > 70%): if Eq. (14) is satisfied, the algorithm 
will stop. 
The above-mentioned thresholds, TĮ and TF, are empirically 
decided, and fixed for all test video sequences. Finally, xt can be 
reconstructed via 
ttx ~~  , where t~  is the final solution obtained 
by GPSR. Our DCVS decoding procedure is summarized in Fig. 2. 
Measurement vector yt
for each non-key frame
Initialization by 
SI generation
Reconstructed 
previous key frames
GPSR 
optimization
Stopping 
criteria 
(a)-(c)Non-stop Stop
ttx ~~ Reconstructed non-key frame tx~  
Fig. 2. Our DVCS decoder. 
 
 
4. SIMULATION RESULTS 
In this paper, two CIF (frame size: 352×288) video sequences (300 
Y frames for each), Coastguard and Foreman, with GOP size = 3, 
and different measurement rates (MRs) were employed to evaluate 
the proposed DVCS method. For example, the average MR = 30% 
means that the MRs for each key and non-key frames are 50% and 
20%, respectively. The three known sparse signal reconstruction 
algorithms, GPSR [9], TwIST [10], and OMP [11], with default 
settings were used for comparisons with our DVCS. The three 
algorithms were applied to each frame individually. For OMP [11], 
the reconstruction complexity will be too expensive if it is directly 
applied to a whole frame. As suggested by [8], OMP can be 
individually applied to each 32×32 block with good trade-off 
between CS efficiency and reconstruction complexity. The four 
evaluated algorithms used the same measurement matrix, SBHE [7] 
and the same basis matrix, DWT. The four algorithms possess the 
same low-complexity encoder (the same CS measurement process). 
The average PSNR performances at different average MRs for 
the two sequences are shown in Fig. 3(a) and (b), respectively. The 
average reconstruction complexities (in seconds) for obtaining Fig. 
3(b) are shown in Fig. 4(a). The average PSNR performances at 
different reconstruction complexities at MR = 30% for the 
Foreman sequence are shown in Fig. 4(b). It can be observed from 
Figs. 3 and 4(a) that the PSNR performances of our DCVS can 
outperform or be comparable with the three known algorithms, 
especially at low MRs, with lower or comparable reconstruction 
complexities. At lower MRs, initializing by SI in our method can 
achieve good performances while at higher MRs, all the four 
algorithms can achieve similar performances. For the Coastguard 
sequence with slower motions, the SI is more accurate than that of 
the Foreman sequence, and better performance can be achieved. 
Based on Fig. 4(b), the PSNR performances of our DCVS can 
significantly outperform the three known algorithms at the same 
reconstruction complexities. 
 
  
(a)                                                        (b) 
Fig. 3. The MR-PSNR performances for the: (a) Coastguard and (b) 
Foreman sequences. 
  
(a)                                                        (b) 
Fig. 4. (a) The reconstruction complexities for the Foreman sequence. 
(b) The PSNR performance at different reconstruction complexities for 
the Foreman sequence. 
 
5. CONCLUSIONS 
In this paper, a distributed compressive video sensing (DCVS) 
framework is proposed to simultaneously capture and compress 
videos at the low-complexity encoder and efficiently reconstruct 
videos at the decoder. For future researches, the key components, 
such as measurement matrix and reconstruction algorithm, in 
compressive video sensing should be designed based on video 
characteristics. The theoretical number of measurements for signal 
perfect reconstruction in Eq. (2) should also be further reduced 
with side information incorporated. In addition, efficient 
quantization and entropy coding techniques for CS measurements 
should be investigated to achieve complete video compression. 
 
ACKNOWLEDGEMENT 
This work was supported in part by National Science Council, ROC, under 
Grants NSC 95-2422-H-001-031 and NSC 97-2628-E-001-011-MY3. 
REFERENCES 
[1] F. Pereira et al., “Distributed video coding: selecting the most 
promising application scenarios,” Signal Processing: Image 
Communication, vol. 23, pp. 339-352, 2008. 
[2] C. Guillemot et al., “Distributed monoview and multiview video 
coding: basics, problems and recent advances,” IEEE Signal 
Processing Magazine, vol. 24, no. 5, pp. 67-76, Sept. 2007. 
[3] J. Romberg, “Imaging via compressive sampling,” IEEE Signal 
Processing Magazine, vol. 25, no. 2, pp. 14-20, Mar 2008. 
[4] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. 
Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive 
sampling,” IEEE Signal Processing Mag., vol. 25, pp. 83-91, 2008. 
[5] V. K. Goyal, A. K. Fletcher, and S. Rangan, “Compressive sampling 
and lossy compression,” IEEE Signal Processing Mag., vol. 25, 2008. 
[6] M. F. Duarte, M. B. Wakin, D. Baron, and R. G. Baraniuk, “Universal 
distributed sensing via random projections,” Proc. ACM/IEEE Int. Conf. 
on Information Processing in Sensor Networks, 2006. 
[7] L. Gan, T. T. Do, and T. D. Tran, “Fast compressive imaging using 
scrambled hadamard ensemble,” Proc. EUSIPCO, 2008 (Matlab code 
available from http://thanglong.ece. jhu.edu/CS/). 
[8] V. Stankovic, L. Stankovic, and S. Cheng, “Compressive video 
sampling,” Proc. EUSIPCO, Lausanne, Switzerland, August 2008. 
[9] M. A. T. Figueiredo, R. D. Nowak, and S. J. Wright, “Gradient 
projection for sparse reconstruction: application to compressed sensing 
and other inverse problems,” IEEE Journal of Selected Topics in Signal 
Processing, vol. 1,no. 4, pp. 586-597, Dec. 2007 (Matlab code 
available from http://www.lx.it.pt/~mtf/GPSR). 
[10] J. M. Bioucas-Dias and M. A. T. Figueiredo, “A new TwIST: two-step 
iterative shrinkage/thresholding algorithms for image restoration,” 
IEEE Trans. on Image Processing, vol. 16, Dec. 2007 (Matlab code 
available from http://www.lx.it.pt/~bioucas/ TwIST/TwIST.htm). 
[11] T. Blumensath and M. E. Davies, “Gradient pursuits,” IEEE Trans. on 
Signal Processing, vol. 56, June 2008 (Matlab code available from 
http://www.see.ed.ac.uk/~tblumens/sparsify/ sparsify.html). 
[12] “AviSynth MSU frame rate conversion filter,” http://www. 
compression.ru/video/frame_rate_conversion/index_en_msu.html. 
1172
  
addition, a DVC algorithm using CS was proposed11, where at the decoder, each block in a CS frame is reconstructed 
with respect to the basis (dictionary) formed from a set of spatially neighboring blocks of previous decoded neighboring 
key frames. Similarly, a distributed compressed video sensing (DISCOS) framework was also proposed12, where the 
major core is also to assume each block in a CS frame can be sparsely represented with respect to the dictionary formed 
from a set of spatially neighboring blocks of previous decoded neighboring key frames. Here, we denote the two above-
mentioned schemes11-12 as the “local dictionary”-based scheme for their major core employing the local blocks extracted 
from the neighboring frames as the dictionary for each block in a CS frame. 
Similar to rate control/allocation for conventional video coding13 or DVC14, measurement rate allocation is very critical 
for a block-based CS video encoder. Here, the measurement rate (MR) for a signal (e.g., an image or an image block) is 
defined as: 
N
MMR = ,                                                                                   (1) 
where N is the length of the signal (e.g., the number of pixels in a block or an image), M is the number of measurements, 
i.e., the number of acquired samples, and M < N. 
Nevertheless, to keep the complexity of a CS-based video encoder be low, a unique characteristic is that CS can 
“directly” capture compressed video data without temporally storing the raw data. Hence, it is hard to accurately perform 
measurement rate allocation for each block without accessing the raw data. To the best of our knowledge, this issue was 
only roughly mentioned in the compressive video sensing framework8, where each block is determined to be either 
sparse or non-sparse by predicting the sparsity based on the previous reference frame (or key frame) being 
conventionally/fully sampled and transformed using the block-based discrete cosine transform (DCT). Each sparse block 
is compressively sampled whereas each non-sparse block is fully sampled. The major problems of this approach include: 
(i) it is required to periodically support fully sampled reference frame whose raw data are needed to be temporally stored 
and some transformation operation performed is required, which indeed violate the original intention of CS-based data 
compression and cannot be compatible with the CS-based single-pixel camera5; and (ii) the measurement rate allocation 
is too rough to only allocate either a certain rate or full rate for each block. 
In this paper, we propose a novel block-based distributed compressive video sensing (DCVS) framework with feedback 
channel supported, which is extended from our recently developed global dictionary-based DCVS15. We focus on 
studying dynamic measurement rate allocation for DCVS, which can adaptively adjust measurement rates by estimating 
the sparsity of each block via feedback information. Note that the support of feedback channel is usually a common 
assumption in most DVC researches1. The major characteristics of our DCVS include: (i) Dynamic measurement rate 
allocation: the target average measurement rate for each frame can be properly allocated to each block in the frame 
based on the estimated sparsity via feedback information. (ii) CS-based single-pixel camera-compatible: only CS 
random projection process is individually performed for each frame or each block, which is compatible with the single-
pixel camera architecture5. In the frameworks11,12, it is required to support standard MPEG-X/H.26X intra-frame encoder 
to encode each key frame (similar to conventional I frame), which is more complex and incompatible with the single-
pixel camera architecture5. (iii) Global-dictionary based sparse representation: to reconstruct a frame, a global 
dictionary, trained from a set of blocks extracted from the neighboring reconstructed frames together with the side 
information generated from them, is used as the basis of each block. The major advantages of our DCVS include: (a) 
more efficient utilization of available measurement rates; (b) the basis for a frame can be adaptively constructed based on 
neighboring reconstructed frames, which is better than using fixed basis (e.g., DWT or DCT basis); (c) extracting more 
blocks globally for dictionary training can provide better basis for representing blocks with large motions; and (d) even if 
the qualities of the training blocks from neighboring frames are not good enough, the trained dictionary may still provide 
good basis for the blocks in a frame. The fact can be similarly explained by dictionary-based image denoising based on 
the dictionary trained from the blocks extracted from a noisy image itself16,17. In the works11,12, for each block in a CS 
frame, a set of local (spatially neighboring) blocks are extracted from the neighboring reconstructed key frames to form 
its basis without training. Such local dictionary-based basis may not work very well for block with (very) large motion. 
In addition, such schemes highly rely on the qualities of neighboring reconstructed key frames. The performance may be 
degraded due to poorly reconstructed neighboring key frames. Other technical comparisons can be found in Table 1 of 
Sec. 4. An additional property is the inherent computational secrecy of measurements18 which can be only reconstructed 
at the decoder via the same secret key for constructing measurement matrix as the one used in random projection (data 
Proc. of SPIE Vol. 7744  77440I-2
  
perform optimal measurement rate allocation to each frame (or block) before performing random projection (data 
acquisition). Then, each frame (or block) x can be compressed via random projection to get its measurement vector y = 
Фx, where Ф is a measurement matrix6. At DCVS decoder, the reconstruction of x from y and Ф can be formulated as: 
1
2
22
1min θτθθ +− Ay ,                                                                         (5) 
where θ is a set of sparse coefficients with respect to a basis Ψ that is incoherent with Ф, x = Ψθ, A = ФΨ, τ is a non-
negative parameter, ||v||2 is the ℓ2 norm of v, and ||v||1 is the ℓ1 norm of v. Eq. (5) indicates a convex unconstrained 
optimization problem, which can be solved via certain iterative algorithm7. For reconstructing various types of frames, 
different basis functions Ψ or trained dictionaries will be employed, as described later in Secs. 3.3~3.5. 
 
Figure 1. Proposed DCVS with dynamic measurement rate allocation. 
3.2 DCVS encoder with dynamic measurement rate allocation 
At DCVS encoder shown in Figure 1, without performing motion estimation, each key frame xt viewed as a column 
vector with length N is compressed via frame-based random projection as yt = Фxt, where yt is the measurement vector 
with length Mt, Mt < N, forming the compressed version of xt, which will be transmitted to the decoder. Ф is an Mt×N 
measurement matrix6 described later. Given a target average measurement rate MRave, we simply set the measurement 
rate MRt of each key frame xt to MRave. Hence, the number of measurements of a key frame xt is aveMRNM t ×= . 
On the other hand, each CS frame xt consisting of B non-overlapping blocks, bti, i = 1, 2, …, B, is compressed via block-
based random projection by individually projecting each bti viewed as a column vector with length Nb via yti = Фbti, 
where yti is the measurement vector with length Mti, Mti < Nb, and Ф is an Mti×Nb measurement matrix6. The vectors yti, i 
= 1, 2, …, B, forming the compressed version of xt, will be transmitted to the decoder. 
Similar to key frame, we set the measurement rate MRt of a CS frame xt to MRave which will be adaptively allocated to 
each block bti in the frame based on its estimated sparsity via feedback information. Recall from Eq. (2) that the number 
of required measurements for reconstructing a block highly depends on the sparsity of the block. Hence, sparser blocks 
need fewer measurements whereas less sparse blocks need more measurements. Nevertheless, at the encoder, no raw 
block data can be available and the basis for a CS frame cannot be known which is adaptively constructed at the decoder. 
Hence, we propose to estimate the sparsity of a block based on its spatially co-located block in the previous 
reconstructed frame at the decoder. Then, the estimated number of measurements for compressively sampling current 
block can be obtained from the feedback information, addressed in Sec. 3.6. 
Here, the used measurement matrix Ф is the scrambled block Hadamard ensemble (SBHE) matrix6, which takes the 
partial block Hadamard transform, followed by randomly permuting its columns. SBHE has been shown to satisfy the 
five requirements, including near optimal performance, university, fast computation, memory efficient, and hardware 
friendly. Therefore, it can be seen from Figure 1 that our DCVS encoder is indeed memory and computation efficient. 
3.3 DCVS decoder for key frame reconstruction 
At DCVS decoder, each key frame xt can be reconstructed via solving the convex unconstrained optimization problem 
described in Eq. (5) as: 
1
2
22
1min ttt Ay
t
θτθθ +− ,                                                                       (6) 
Proc. of SPIE Vol. 7744  77440I-4
  
of blocks in the two frames should also be similar. Because the training basis of xt+1 depends on xt and its succeeding key 
frame that is unavailable before compressing xt+1, we use the dictionary trained in the previous GOP, which has existed 
at the decoder, to simulate the basis Dt of xt and find the sparse representation of each block bti with respect to Dt by 
solving Eq. (4) to get αti. The simulated basis Dt should be similar to the real basis Dt+1 used for reconstructing xt+1 to 
some extent if GOP size is small enough. We are also currently investigating the achievable performance by comparing 
with the performance upper bound when the next key frame is assumed to be available. Then, we use the sparse 
representation αti of bti to predict that (α(t+1),i) of the spatially co-located block b(t+1),i in xt+1, Bi ,1,2,K= . Actually, it is 
not easy to use the number of nonzero coefficients (obtained by performing some CS reconstruction algorithm) of the 
sparse representation of a block to estimate its real sparsity. Alternately, based on the fact that the complexity and 
sparsity of an image are highly correlated21, we propose to exploit the variance of the coefficients of each block to 
perform measurement rate allocation. Based on the variance of estimated α(t+1),i, denoted by v(t+1),i, for b(t+1),i and the 
target measurement rate MRt+1 of xt+1, we allocate the number of measurements for each block b(t+1),i as 
( ) ( )
( )
( )NMR
v
v
M tB
j
it
jt
it ××
∑
= +
=
+
+
+
1
1
,1
,1
,1 ,                                                            (8) 
where N is the frame size. The allocation strategy implies that more complex (less sparse) blocks will be allocated more 
measurements, and vice versa. Then, the information including M(t+1),i will be sent back to the encoder via the feedback 
channel for compressively sampling xt+1. 
Furthermore, after reconstructing a CS frame xt, if its next frame xt+1 is also a CS frame, we just use the variance of the 
coefficients of each block in xt to estimate that of the spatially co-located block in xt+1 because the bases of the two CS 
frames in the same GOP are identical. Then, the measurements rate allocation can be similarly performed using Eq. (8), 
which will be sent back to the encoder for compressively sampling xt+1. 
 
Key frame (t - 1)
CS frame t
Key frame (t + 1)
yt-1 = Φxt-1
yt = Φxt
CS capturing
yt+1 = Φxt+1
CS capturing
Reconstruction 
with DWT 
basis
Reconstruction 
with trained 
dictionary
Side information (t)
Reconstructed frame (t)
Reconstructed frame (t-1)
Side information 
generation
Reconstructed frame (t+1)
Reconstruction 
with DWT 
basis
Dictionary 
Training
Dictionary (t)
CS capturing
 
Figure 2. The block diagram of our DCVS. 
4. SIMULATION RESULTS 
In this paper, several QCIF (frame size: 176×144) video sequences (51 Y frames for each) with GOP size = 2, and 
different measurement rates (MRs) were employed to evaluate the proposed DVCS with dynamic measurement rate 
allocation (denoted by Proposed). For training the dictionary for each CS frame consisting of several non-overlapping 
16×16 blocks, the parameter settings are described as follows. The dictionary size was set to 256×256, i.e., Nb = 16×16 = 
Proc. of SPIE Vol. 7744  77440I-6
  
The average PSNR (dB) performances of CS frames at different MRs for the News, Foreman, and Football video 
sequences are shown in Tables 2-4, respectively, where it can be observed that the PSNR performances of the proposed 
DCVS can outperform the three schemes for comparison, especially at lower MRs and for large-motion sequences. In our 
scheme (Proposed), available measurement rates can be more efficiently utilized. It can also be observed from Table 4 
that the PSNR performances obtained from the four evaluated schemes are somewhat poor (< 25 dB). The major reasons 
include: (i) the frame contents of the Football sequence are somewhat complex, which may not be exactly sparse signals 
with respect to most bases, and (ii) the motions of the sequence are very large so that it is hard to find a good dictionary 
for a CS frame from its neighboring key frames. It is worth noting that the dictionary training of our DCVS can reveal 
some “denoising” capability to obtain a basis better than that of the Local-Dict scheme11-12 without relying on dictionary 
training. It should be noted that the ranges of PSNR values presented in this paper are lower than those presented in the 
papers11-12. The major reason is that in the papers11-12, each key frame is encoded using the H.264/AVC encoder which is 
very efficient, but also very complex and single-pixel camera-incompatible, resulting in better basis for CS frame 
reconstruction, while in this paper, all frames are encoded based on compressive sensing. 
Table 2. The performances of the News sequence. 
MR(%) 10 20 30 40 
Proposed 21.01 24.75 27.43 28.94 
Proposed W/O15 16.44 23.75 26.67 28.65 
Local-Dict11-12 15.09 22.18 25.74 28.12 
Frame-DWT6 14.85 21.87 23.93 26.24 
 
Table 3. The performances of the Foreman sequence. 
MR(%) 10 20 30 40 
Proposed 23.41 26.33 28.22 29.92 
Proposed W/O15 16.98 25.90 27.87 29.68 
Local-Dict11-12 14.80 23.94 26.82 29.40 
Frame-DWT6 13.58 22.29 24.06 26.25 
 
Table 4. The performances of the Football sequence. 
MR(%) 10 20 30 40 
Proposed 20.10 21.63 23.40 24.95 
Proposed W/O15 17.11 21.08 22.53 23.85 
Local-Dict11-12 15.08 18.45 19.47 20.72 
Frame-DWT6 15.68 20.10 22.08 24.00 
5. CONCLUSIONS 
In this paper, a distributed compressive video sensing (DCVS) framework via global dictionary-based sparse coding with 
measurement rate allocation is proposed to directly capture compressed video for CS-based single-pixel camera 
architecture. The simulation results have shown that the available measurement rates can be more efficiently utilized and 
the trained global dictionary can provide better basis for video reconstruction than using the DWT basis and local 
dictionary-based basis. For the future works, several important issues need to be investigated in depth for achieving a 
complete CS-based video coding system are descried as follows. (i) Frame-level measurement rate allocation: The 
available measurements should be adaptively allocated to each frame based on its sparsity. (ii) Measurement rate 
allocation without needing feedback channel. (iii) Adaptive measurement matrix learning: If a measurement matrix can 
be adaptively learned based on the characteristics of current signal to be captured20, the number of captured 
measurements should be reduced while preserving a certain performance. (iv) Measurement quantization22: Real 
measurement values should be properly quantized to get the best tradeoff between the number of quantization levels and 
quantization loss. (v) Bit allocation and entropy coding for measurements22-23. (vi) Fast dictionary training at the decoder. 
(vii) More efficient algorithm solving the convex optimization problem. (viii) More robust algorithm solving the convex 
optimization problem against quantization errors and transmission errors or other error resilience techniques. (ix) More 
accurate side information generation: If more accurate side information for a CS frame can be generated, the trained 
dictionary can provide much sparser representation for this frame, resulting in better compression performance. 
Proc. of SPIE Vol. 7744  77440I-8
  
13. Kwon, D. K., Shen M. Y. and Kuo, C.-C. Jay, "Rate control for H.264 video with enhanced rate and distortion 
models," IEEE Trans. on Circults and Systems for Video Technology 17(5), 517-529 (2007). 
14. Brites, C. and Pereira, F., "Encoder rate control for transform domain Wyner-Ziv video coding," Proc. IEEE Int. 
Conf. on Image Processing, 5-7 (2007). 
15. Chen, H. W., Kang, L. W. and Lu, C. S., "Distributed compressive video sensing via global dictionary-based sparse 
coding," submitted to Proc. IEEE Int. Conf. on Image Processing (2010). 
16. Aharon, M., Elad, M. and Bruckstein, A. M., "The K-SVD: an algorithm for designing of overcomplete dictionaries 
for sparse representation," IEEE Trans. on Signal Processing 54(11), 4311–4322 (2006) (Matlab codes available 
from http://www.cs.technion. ac.il/~ronrubin/software.html). 
17. Elad, M. and Aharon, M., "Image denoising via sparse and redundant representations over learned dictionaries," 
IEEE Trans. on Image Processing 15(12), 3736–3745 (2006). 
18. Rachlin, Y. and Baron, D., "The secrecy of compressed sensing measurements," Proc. Allerton Conf. on 
Communication, Control, and Computing, (2008). 
19. Kang, L. W., Lu, C. S. and Hsu, C. Y., "Compressive sensing-based image hashing," Proc. IEEE Int. Conf. on 
Image Processing (2009). 
20. Duarte-Carvajalino, J. M. and Sapiro, G., "Learning to sense sparse signals: simultaneous sensing matrix and 
sparsifying dictionary optimization," IEEE Trans. on Image Processing 18(7), 1395-1408 (2009). 
21. Perkio, J. and Hyvarinen, A., "Modelling image complexity by independent component analysis, with application to 
content-based image retrieval," Proc. Int. Conf. on Artificial Neural Networks (2009). 
22. Dai, W., Pham, H. V. and Milenkovic, O., "Distortion-rate functions for quantized compressive sensing," Proc. 
IEEE Information Theory Workshop on Networking and Information Theory (2009). 
23. Goyal, V. K., Fletcher, A. K. and Rangan, S., "Compressive sampling and lossy compression," IEEE Signal 
Processing Magazine 25(2), 48-56 (2008). 
24. Schulz, A., Velho, L. and da Silva, E. A. B., "On the empirical rate-distortion performance of compressive sensing," 
Proc. IEEE Int. Conf. on Image Processing (2009). 
25. Duarte, M. F., Wakin, M. B., Baron, D. and Baraniuk, R. G., "Universal distributed sensing via random projections," 
Proc. ACM/IEEE Int. Conf. on Information Processing in Sensor Networks, 19-21 (2006). 
26. Divekar A. and Ersoy, O., "Compact storage of correlated data for content based retrieval," Proc. Asilomar Conf. on 
Signals, Systems and Computers (2009). 
27. Cevher, V., Sankaranarayanan, A., Duarte, M. F., Reddy, D., Baraniuk, R. G. and Chellappa, R., "Compressive 
sensing for background subtraction," Proc. European Conf. on Computer Vision (2008). 
28. Cossalter, M., Tagliasacchi, M. and Valenzise, G. "Privacy-enabled object tracking in video sequences using 
compressive sensing," Proc. IEEE Int. Conf. on Advanced Video and Signal Based Surveillance (2009). 
29. Tagliasacchi, M., Valenzise, G. and Tubaro S., "Hash-based identification of sparse image tampering," IEEE Trans. 
on Image Processing 18(11), 2491-2504 (2009). 
30. Wright, J., Ma, Y., Mairal, J., Sapiro, G., Huang, T. and Yan, S., "Sparse representation for computer vision and 
pattern recognition," to appear in Proceedings of the IEEE. 
31. Elad, M., Figueiredo, M. A. T. and Ma Y., "On the role of sparse and redundant representation in image processing," 
to appear in Proceedings of the IEEE. 
32. Mairal, J., Elad, M. and Sapiro, G., "Sparse representation for color image restoration," IEEE Trans. on Image 
Processing 17(1), 53-69 (2008). 
33. Mairal, J., Sapiro, G. and Elad M., "Learning multiscale sparse representations for image and video restoration," 
SIAM Multiscale Modeling and Simulation 7(1), 214-241 (2008). 
34. Fadili, M. J., Starck, J. L. and Murtagh, F., "Inpainting and zooming using sparse representations," The Computer 
Journal 52(1), 64-79 (2009). 
35. Yang, J., Wright, J., Huang, T. and Ma, Y., "Image super-resolution via sparse representation," to appear in IEEE 
Trans. on Image Processing. 
36. Haupt, J., Bajwa, W. U., Rabbat, M. and Nowak, R., "Compressed sensing for networked data," IEEE Signal 
Processing Magazine 25(2), 92-101 (2008). 
37. Wright, J., Yang, A. Y., Ganesh, A., Sastry, S. S. and Ma, Y. "Robust face recognition via sparse representation," 
IEEE Trans. on Pattern Analysis and Machine Intelligence 31(2), 210-227 (2009). 
38. Yang, A. Y., Gastpar, M., Bajcsy, R. and Sastry, S. S., "Distributed sensor perception via sparse representation," to 
appear in Proceedings of the IEEE. 
Proc. of SPIE Vol. 7744  77440I-10
generated from some distribution, the incoherence between Ɏ and 
D should be usually high enough. 
3. PROPOSED DCVS FRAMEWORK 
3.1. Problem Formulation 
In DCVS, a video sequence consists of several GOPs (group of 
pictures), where a GOP consists of a key frame followed by some 
CS frames. At DCVS encoder, each key frame or each block in a 
CS frame can be compressed via CS random projection to get its 
measurement vector. Here, the used measurement matrix is the 
scrambled block Hadamard ensemble (SBHE) matrix [5], which 
takes the partial block Hadamard transform, followed by randomly 
permuting its columns. At DCVS decoder, the reconstruction of a 
frame or a block can be formulated as an l1-minimization problem. 
Here, the sparse coefficients with respect to different basis 
functions (or dictionaries) depending on various types of frames 
are solved via the “sparse reconstruction by separable 
approximation (SpaRSA)” algorithm [6]. Employed different basis 
functions or learned dictionaries will be described in Secs. 3.3~3.5. 
3.2. DCVS Encoder 
At DCVS encoder shown in Fig. 1, without acquiring complete 
raw video data and performing motion estimation, each key frame 
1R ×∈ Ntx  viewed as a column vector is compressed via frame-
based random projection as yt = Ɏxt, where 1R ×∈ tMty  is the 
measurement vector, Mt < N, forming the compressed version of xt,
which will be transmitted to the decoder. NM t ×∈ Rĭ  is the 
measurement matrix [5]. On the other hand, each CS frame xt
consisting of B non-overlapping blocks, 1R ×∈ bNtib  viewed as a 
column vector, i = 1, 2, …, B, is compressed via block-based 
random projection by individually projecting each bti via yti = Ɏbti,
where 1R ×∈ tiMtiy  is the measurement vector, Mti < Nb, and  
bti NM ×∈ Rĭ  is the measurement matrix [5]. The vectors yti, i = 
1, 2, …, B, forming the compressed version of xt, will be 
transmitted to the decoder. 
Key frame Frame-based 
random projection
Measurements Frame-based 
Reconstruction
Reconstructed 
Key frame
CS frame
Side information 
generation
Dictionary 
learning
Block-based 
Random projection
Measurements Block-based 
Reconstruction
Reconstructed 
CS frame
DCVS Encoder DCVS Decoder
Fig. 1. Proposed DCVS with dictionary learning. 
3.3. DCVS Decoder for Key Frame Reconstruction 
At DCVS decoder, each key frame xt can be reconstructed via 
solving the l1-minimization problem as: 
1
2
22
1min ttt Ay
t
θτθ
θ
+− ,                            (1) 
where yt is the received measurement vector, yt = Ɏxt, A = ɎȌ, Ɏ
is the measurement matrix [5], Ȍ is the DWT basis, 1R ×∈ Ntθ  is 
the sparse coefficient vector to be solved via SpaRSA algorithm [6] 
with respect to Ȍ, and Ĳ is a non-negative parameter. Finally, the 
key frame xt can be reconstructed via ttx θ
~~ Ψ= , where tθ
~  is the 
solution of șt minimizing Eq. (1). For achieving the goal of 
independent reconstruction of a key frame, a general-purpose basis, 
DWT basis, for image representation is employed. 
3.4. DCVS Decoder for CS Frame Reconstruction 
At DCVS decoder, each CS frame xt can also be reconstructed via 
solving the l1-minimization problem for each block bti, i = 1, 2, …, 
B, in xt as: 
1
2
22
1min tititti Ay
ti
ατα
α
+− ,                            (2) 
where yti is the received measurement vector for bti, yti = Ɏbti, At = 
ɎDt, Ɏ is the measurement matrix [5], PNt b
×
∈ RD , Nb  P, is 
the learned dictionary for xt, described in Sec. 3.5, 1R ×∈ Ptiα  is 
the sparse coefficient vector to be solved via SpaRSA algorithm [6] 
with respect to the basis Dt, and Ĳ is a non-negative parameter. 
Similarly, bti can be reconstructed via tittib α
~D
~
= , where tiα
~  is 
the solution of tiα  minimizing Eq. (2). That is, each block bti in xt
can be sparsely represented as a linear combination of the atoms 
(column vectors) in Dt. Finally, the CS frame xt can be 
reconstructed by integrating tib
~
, i = 1, 2, …, B.
3.5. Dictionary Learning for CS Frame Reconstruction 
If the basis/dictionary for an image can be learned based on the 
training samples/atoms extracted from the image itself, this basis 
should provide much sparser representation for the image. 
Although, it is impossible to obtain such the basis from an image 
itself to be reconstructed at decoder, a good dictionary learned 
from the training samples generated from the neighboring frames 
of a video frame to be reconstructed may be still obtained. Based 
on the general fact that the contents of successive frames in the 
same scene of a video should be similar, a frame can be well-
predicted based on its side information possibly generated from the 
interpolation of its neighboring reconstructed frames. 
At DCVS decoder, for a CS frame xt, its side information It can 
be generated from the motion-compensated interpolation (MCI) of 
its previous and next reconstructed key frames, respectively, 
denoted by xt-j and xt+j. MCI technique has been successfully used 
for side information generation in DVC [2]. Then, we use the three 
frames, xt-j, It, and xt+j to learn the dictionary (basis) for this CS 
frame xt as follows. First, we extract Q training patches bNi Ru ∈ ,
i = 1, 2, …, Q, from xt-j, It, and xt+j, where each frame is divided 
into several non-overlapping blocks. For each block in the three 
frames, we extract the 9 training patches including the nearest 8 
blocks overlapping this block and this block itself, where each 
extracted patch bNi Ru ∈  can be viewed as a column vector. 
Second, we apply the K-SVD algorithm [10] to these Q training 
patches to learn the dictionary PNt b
×∈ RD , Nb  P, for xt, where 
Dt is an overcomplete dictionary containing P atoms. With respect 
to Dt, each block bti in xt can be sparsely represented as a sparse 
coefficient vector 1R ×∈ Ptiα  and, usually, ||Įti||0 << Nb. Using the 
learned dictionary for all the blocks of a CS frame can usually 
provide sparser representation for the frame than using a fixed 
DWT basis. 
An illustrative example of the Foreman QCIF video sequence 
at measurement rate (MR, defined by the number of acquired 
measurements divided by the number of pixels of a frame) = 0.3 
shown in Fig. 2 is used to demonstrate the efficiency of DCVS 
- 211 -
capability to obtain a basis better than that of the “W/O dictionary 
learning” scheme without relying on dictionary learning. 
Proposed
W/O dictionary learning
Frame-DWT
Proposed
W/O dictionary learning
Frame-DWT
(a)                                              (b) 
Fig. 3. The (a) MR-PSNR and (b) Bitrate-PSNR performances 
of the Foreman Sequence. 
Proposed
W/O dictionary learning
Frame-DWT
Proposed
W/O dictionary learning
Frame-DWT
(a)                                                (b) 
Fig. 4. The (a) MR-PSNR and (b) Bitrate-PSNR performances 
of the Mobile Sequence. 
Proposed
W/O dictionary learning
Frame-DWT
Proposed
W/O dictionary learning
Frame-DWT
(a)                                               (b) 
Fig. 5. The (a) MR-PSNR and (b) Bitrate-PSNR performances 
of the Silent Sequence. 
On the other hand, to explore the compression efficiency in 
terms of PSNR-bitrate performances, we quantized each 
measurement via a nonuniform quantizer with 8 levels, generated 
using Lloyd’s algorithm [12]. Then, we encoded each quantized 
measurement using an entropy encoder designed by Huffman 
coding, where each measurement was averagely encoded by 2.9 
bits. The average PSNR performances at the four different bitrates 
(bits per pixel, i.e., bpp), respectively, obtained by encoding the 
measurements for MR = 10%, 20%, 30%, and 40% for the three 
evaluated sequences, are shown in Figs. 3(b), 4(b), and 5(b), 
respectively, where it can be observed that the proposed DCVS can 
outperform or be comparable to the Frame-DWT and “W/O 
dictionary learning” schemes. That is, with some quantization 
noises, adaptive learned dictionaries can reveal some “denoising” 
capability and provide much better bases, resulting in better 
reconstructed quality. In addition, the average number of bits (2.9 
bits) for encoding a measurement using the entropy encoder is very 
close to that (3 bits) using fixed-length encoder. The reason is that 
the measurement matrix spreads the energy of a signal uniformly 
across the measurements, so that each measurement is nearly 
allocated the same number of bits [13]. 
5. CONCLUSIONS 
In this paper, a single-pixel camera-compatible dictionary learning-
based distributed compressive video sensing (DCVS) framework is 
proposed to directly acquire compressed video. The simulation 
results have shown that the learned dictionary can provide better 
basis for video reconstruction than using the DWT basis and 
dictionary without learning-based basis. For the future works, 
several important issues need to be investigated in depth are 
described as follows. (i) Adaptive measurement matrix learning: (ii) 
Optimal measurement quantization and allocation. (iii) Bit 
allocation and entropy coding for measurements. (iv) Fast 
dictionary learning. (v) More efficient algorithm solving the l1-
minimization problem. 
6. REFERENCES 
[1] T. Wiegand and G. J. Sullivan, “The H.264/AVC video coding 
standard,” IEEE Signal Processing Magazine, vol. 24, no. 2, pp. 148-
153, March 2007. 
[2] F. Dufaux, W. Gao, S. Tubaro, and A. Vetro, “Distributed video 
coding: trends and perspectives,” EURASIP Journal on Image and 
Video Processing, Article ID 508167, 2009. 
[3] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. 
Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive 
sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 83-91, 
March 2008. 
[4] E. Candès and M. Wakin, “An introduction to compressive sampling,” 
IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21-30, March 
2008. 
[5] L. Gan, T. T. Do, and T. D. Tran, “Fast compressive imaging using 
scrambled hadamard ensemble,” in Proc. of European Signal 
Processing Conf., Switzerland, Aug. 2008. 
[6] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, “Sparse 
reconstruction by separable approximation,” IEEE Trans. on Signal 
Processing, vol. 57, no. 7, pp. 2479-2493, July 2009. 
[7] L. W. Kang and C. S. Lu, “Distributed compressive video sensing,” in
Proc. of IEEE Int. Conf. on Acoustics, Speech, and Signal Processing,
Taipei, Taiwan, April 2009, pp. 1169-1172. 
[8] J. Prades-Nebot, Y. Ma, and T. Huang, “Distributed video coding 
using compressive sampling,” in Proc. of Picture Coding Symposium,
Chicago, Illinois, USA, May 2009. 
[9] T. T. Do, Y. Chen, D. T. Nguyen, N. Nguyen, L. Gan, and T. D. Tran, 
“Distributed compressed video sensing,” in Proc. of IEEE Int. Conf. on 
Image Processing, Cairo, Egypt, Nov. 2009, pp. 1393-1396. 
[10]M. Aharon, M. Elad, and A. M. Bruckstein, “The K-SVD: an 
algorithm for designing of overcomplete dictionaries for sparse 
representation,” IEEE Trans. on Signal Processing, vol. 54, no. 11, pp. 
4311-4322, Nov. 2006. 
[11]R. Rubinstein, M. Zibulevsky, and M. Elad, “Efficient implementation 
of the K-SVD algorithm using batch orthogonal matching pursuit,” CS 
Technical Report, Technion - Israel Institute of Technology, 2008. 
[12]W. Dai, H. V. Pham, and O. Milenkovic, “Distortion-rate functions for 
quantized compressive sensing,” in Proc. of IEEE Information Theory 
Workshop on Networking and Information Theory, June 2009. 
[13]V. K. Goyal, A. K. Fletcher, and S. Rangan, “Compressive sampling 
and lossy compression,” IEEE Signal Processing Magazine, vol. 25, 
no. 2, pp. 48-56, 2008. 
- 213 -
1020 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011
represented in terms of a dictionary or transferred as a linear
combination of dictionary atoms, so as to achieve efficient
feature representation and robust image similarity assessment.
In this paper, the term “atom” means a typical pattern or basic
unit learned from a set of training data. A dictionary consisting
of several atoms can provide sparse representations of the data
as a linear combination of a few atoms.
In this paper, we adopt the SIFT feature1 [3] as the basis of
our feature-based image similarity assessment scheme. The rea-
sons, in terms of the robustness and applicabilities of SIFT, are
briefly described as follows. In the literature, SIFT is one of
the most pervasive and robust image features, and it has been
widely used in several multimedia applications, such as image
retrieval [4], [5], recognition [6]–[8], copy detection [8], [9],
and near-duplicate detection [10], [11]. In addition, in a recent
performance evaluation, the SIFT descriptor has been shown
to outperform other local descriptors [12]. Current SIFT-based
image retrieval approaches are usually based on building in-
dices for SIFT feature descriptors that are extracted from local
image regions. Then, the descriptors are quantized into visual
words defined in a pre-constructed vocabulary. Finally, image
retrieval can be achieved through a text retrieval technique [4].
Moreover, for SIFT-based image recognition, an efficient archi-
tecture, called a vocabulary tree, was proposed [5]. Based on
quantized SIFT feature descriptors, the support vector machine
(SVM) or nearest-neighbor (NN) techniques are usually used
for image recognition [7].
In this paper, we study sparse representation and matching
techniques of SIFT features for realizing our idea of quantifying
image information and similarity assessment between images.
We also show that the proposed feature-based sparse representa-
tion for image similarity assessment (FSRISA) technique can be
broadly applied to numerous multimedia applications through
proper problem formulations. In Sections I-A–E, we briefly re-
view the SIFT technique and explore the two aspects of the SIFT
feature, namely, representation and matching, followed by a pre-
sentation of the overview of the proposed scheme.
A. SIFT
SIFT [3] is a powerful technique extensively used in the
community of computer vision and pattern recognition to
detect and describe local features in images. Roughly speaking,
SIFT transforms an image into a large collection of descriptors
(feature vectors), each of which is invariant to image transla-
tion, scaling, and rotation, is partially invariant to illumination
changes, and is robust to local geometric distortion. The main
stages of SIFT include: 1) scale-space extrema detection; 2)
keypoint localization; 3) orientation assignment; and 4) key-
point descriptor generation.
B. Representation of SIFT Feature
To extract SIFT features from an image, keypoints are local-
ized first, based on scale-space extrema detection. Then, one or
more orientations, based on local image gradient directions, will
be assigned to each keypoint. Finally, a local image descriptor
is built for each keypoint, based on the image gradients in its
1Nevertheless, any feature descriptors can be used in the proposed framework.
local neighborhood. In the standard SIFT descriptor represen-
tation, each descriptor is a 128-dimensional feature vector [3].
Usually, hundreds to thousands of keypoints may be extracted
from an image.
To make the SIFT feature more compact, the bag-of-words
(BoW) representation approach quantizes SIFT descriptors via
vector quantization technique into a collection of visual words
based on a pre-defined codebook, such as visual vocabulary [4]
or vocabulary tree [5]. Also, advanced compression for SIFT
features has been investigated recently [13].
C. Matching of SIFT Feature
To evaluate the similarity between two images based on their
SIFT features, the most straightforward scheme is to perform
keypoint matching by treating each image as a set of keypoints
and conducting direct keypoint-set mapping [3]. The similarity
between the two images is based on the number of matched
keypoints, between the two sets of keypoints.
Moreover, for the BoW representation-based approach
[4], the similarity between SIFT features can be measured
by matching their corresponding visual words via histogram
matching [14]. Typically, the computational complexity of the
direct keypoint matching approach is higher than that of the
BoW-based approach. Nevertheless, the outcomes of the direct
keypoint matching approach are usually more reliable than
those of the BoW-based approach suffered from quantization
loss [11].
D. Overview of the Proposed Scheme
In this paper, a scheme of feature-based sparse representation
for image similarity assessment (FSRISA) is proposed. SIFT is
adopted as the representative feature detector in our framework.
To compactly represent SIFT feature of an image, we propose
construction of the basis (dictionary), consisting of the proto-
type SIFT atoms via dictionary learning that forms the feature,
called “dictionary feature,” of the image. To assess the simi-
larity between two images based on their dictionary features,
we propose formulating the problem as a sparse representation
problem, where we perform sparse coding and calculate the
reconstruction error for each SIFT descriptor of a test image.
Then, based on a voting strategy, we can define a similarity value
(matching score) between the two images. We also apply our
FSRISA to three multimedia applications, including image copy
detection, retrieval, and recognition, by properly formulating
them to their corresponding sparse representation problems.
The major novelties and contributions of this paper include:
1) a feature-based image assessment approach is proposed to
quantify how much information present in a reference image
can be extracted from a test image by integrating image feature
extraction and sparse representation; 2) the inherent discrimina-
tive characteristic of sparse representation is exploited to assess
the similarity between two images by performing sparse coding
with respect to the dictionary integrated from the two dictionary
features of the two images, respectively; 3) efficient feature rep-
resentation can be achieved by representing features in terms
of linear combination of dictionary atoms; and 4) the proposed
FSRISA provides a bounded similarity score, i.e., , for de-
tected features to quantify the similarity between two images.
1022 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011
where with length is the sparse coefficient vector
of with length of . of size
is the joint dictionary concatenating and , and
is an error tolerance.
To solve the sparsest solution for , (2) can be cast to an
-minimization problem as [19]
(3)
where is a positive real number parameter. In this paper, we
apply an efficient sparse coding algorithm, called the sparse re-
construction by separable approximation (SpaRSA) algorithm
[20] to solve (3) in order to find the sparse representation
of with respect to the dictionary . SpaRSA is a very effi-
cient iterative algorithm, where each step is obtained by solving
an optimization subproblem involving a quadratic term with di-
agonal Hessian plus the original sparsity-inducing regularizer.
Of course, (3) can be directly solved via a greedy algorithm,
such as OMP [17] and other -minimization algorithms.
It is expected that the positions of nonzero coefficients in
(or the selected atoms from ) should be highly concentrated
on only one sub-dictionary (e.g., or ), and the remaining
coefficients in should be zeros or small enough. Also, it is
intuitive to expect that the atoms for sparsely representing
should be mostly selected from the sub-dictionary learned
from the feature vectors extracted from the image itself, in-
stead of . If the parameters for learning the two dictionaries
( and ) can be adequately tuned, the manner of atom selec-
tion in the sparse coding process may be changed accordingly.
That is, we intend to make the sparse coefficients (or the used
atoms) solved by performing sparse coding for more con-
sistent with our expectation to help for similarity assessment.
More specifically, we expect will use more atoms from
to represent it when and are visually different. On the other
hand, we expect will use more atoms from to represent
it when and are visually similar. The details are described
in the seventh paragraph of this subsection.
Based on the obtained solution of (3), we can calculate the
reconstruction error as . By letting the elements
in , corresponding to the atoms from , be zeros, we can get
the reconstruction error , using only the atoms from for
reconstructing . On the other hand, by letting the elements in
, corresponding to the atoms from , be zeros, we can get
the reconstruction error , using only the atoms from for
reconstructing . If , it is claimed that the atoms
from are more suitable for representing than those from
, and will get a vote. Otherwise, if , is more
suitable to be represented by the atoms from (the dictionary
learned from the feature vectors itself) than , and will
get a vote. Considering all the SIFT feature vectors of , ,
, the obtained percentage of votes from and
are denoted by and , , , respectively.
Based on the voting strategy, we define the similarity between
the two images, and , as
(4)
where the range of - is , which can be shifted
to , resulting in the defined in (4). Larger
indicates that more atoms from learned from
can well represent the feature vectors extracted from
. This implies that a considerable amount of information
(denoted by ) presented in can be extracted [via spare
coding by solving (3)] from . On the other hand, smaller
indicates most suitable atoms for representing
extracted from are from learned from itself. This im-
plies that less/no information presented in can be extracted
from . Hence, the larger the is, the more similar
the images and are.
Obviously, if is visually very different from , is larger
than . Nevertheless, if is visually similar to , will not
be larger than in all instances. That is, better (or similar) re-
construction performance for may be achieved using as
the dictionary than using due to some feature vectors ex-
tracted from being able to be matched by the feature vec-
tors extracted from . To achieve this goal, we propose three
rules for tuning the parameters used by K-SVD for learning the
two dictionaries, and : 1) the number of the atoms in
should be larger than that in ; 2) the number
of iterations K-SVD performs for learning should be larger
than that for learning ; and 3) the number of
the target sparsity , i.e., the number of nonzero coefficients
for representing each feature vector for learning , should be
larger than that for learning . According to
the rules designed above, when is visually similar to and
is finer than , the -minimizer for solving (3) may prefer
more promising atoms from than to reconstruct , re-
sulting in and larger . Otherwise, when
is visually different from , most atoms for reconstructing
will still be selected from , resulting in and smaller
. The proposed FSRISA technique is summarized in
Algorithm I and illustrated in Fig. 2.
The major goal of performing sparse coding with respect to
the dictionary consisting of and , instead of only , can
be addressed as follows. When (reference image) is visu-
ally different from (test image), and are significantly
different. In this scenario, the idea behind our FSRISA is some-
what related to that of sparse coding-based image classification
approach [15], [21] or sparse coding-based image decomposi-
tion approach [22]. We use the similar concept to quantify the
similarity between and , which may be interpreted as ei-
ther 1) classifying into or itself; or 2) decomposing
into the components of and/or those of itself. When
is visually similar to , and are similar, which is
enforced to that is finer than in FSRISA. Hence, the
above discussions are also valid in this scenario. Moreover,
why we do not perform sparse coding with respect to only one
dictionary can be explained as follows. When performing
sparse coding for the feature vectors of with respect to a
dictionary consisting of atoms which may be not suitable
for sparsely representing them, the sparse coding procedure
still attempts to minimize the reconstruction errors. Based on
our experience, it is usually not well distinguishable from re-
construction errors obtained with respect to either related or
unrelated dictionaries. On the other hand, it is not easy to de-
1024 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011
techniques have emerged to search for duplicates and forg-
eries [24], [25]. Image copy detection can be achieved via
content-based copy detection approach, which measures the
similarity/distance between an original image and its possible
copy version through comparing their extracted image features,
where SIFT-based features have been recently investigated
[8], [9]. In this section, we study content-based image copy
detection by applying the proposed FSRISA approach.
A user can perform image copy detection to detect possible
copies of her/his original image from the Internet or an image
database. To detect whether a test image is actually a copy of
a query image with the dictionary feature of size ,
we first extract the SIFT feature vectors , ,
and learn the dictionary feature of size of , such
that is finer than . Then, we perform -minimization by
solving (3) for each with respect to , and
voting to get the percentages of votes, and , with respect
to and , respectively. Finally, based on (4), the similarity
between and can be calculated as . Given an
empirically determined threshold , if , then
can be determined as a copy version of . Otherwise, and
can be determined to be unrelated. The computational com-
plexity for performing the FSRISA-based image copy detection
can be also similarly analyzed via (8).
B. Image Retrieval via FSRISA
The most popular image retrieval approach is content-based
image retrieval (CBIR), where the most common technique is to
measure the similarity between two images by comparing their
extracted image features.
In the proposed scheme, for a query image, we extract its
dictionary feature (with atoms) and transmit it to an image
database, where each image is stored together with its dictio-
nary feature and original SIFT feature vectors. For comparing
the query image and each database image , ,
size_of_database, where size_of_database denotes the total
number of database images, we apply the proposed FSRISA
scheme to perform the -minimization (3) for each SIFT
feature vector of with respect to the dictionary consisting
of the dictionary features of the two images. Then, we calculate
the reconstruction errors of all the stored feature vectors of
and perform voting to get the similarity value between the
two images (4) to be the score of . Finally, we retrieve the
top database images with the largest scores. Similarly, the
computational complexity of comparing the two images can be
analyzed via (8), except that the complexity for extracting the
dictionary feature of each database image can be excluded due
to the fact that the process can be performed in advance during
database construction.
C. Image Recognition via FSRISA
Consider a well-classified image database, where each class
includes several images with the same object, but with different
variations. Given a query image, a user may enquire to which
class the image belongs. For image recognition, sparse repre-
sentation techniques have been extensively used [21], [28]. The
major idea is to exploit the fact that the sparsest representation
is naturally discriminative. Among all of the subsets of atoms in
a dictionary, it selects the subset that most compactly expresses
the input signal and rejects all of the others with less compact
representations. More specifically, image recognition/classifica-
tion can be achieved by representing the feature of the query
image as a linear combination of those training samples from
the same class in a dictionary. Moreover, the conclusions in
[21] claimed that their sparse representation-based face recog-
nition algorithm should be extended to less constrained condi-
tions (e.g., variations in object pose or misalignment). In order
not to incur such a constraint, both variability-invariant features
and sparse representation should be properly integrated. In ad-
dition, in [21], a dictionary consists of several subsets of image
features (down-sampled image pixels were used), where each
subset contains the features of several training images belonging
to the same class. Nevertheless, if the number of classes, the
number of training images in each class, or the feature dimen-
sion of a training image is too large, the dictionary size will be
very large. It will induce very high computational complexity
in performing sparse coding for the feature vector(s) of a query
image.
In this paper, we propose an image recognition approach,
where we assess the similarity between a query image and each
class of training images based on the proposed FSRISA. In the
training stage, for the th image class, , where
denotes the number of classes in an image database, we extract
the SIFT feature vectors for each image as the training samples.
Then, we apply K-SVD [16] to learn the dictionary of size
to be the “dictionary feature” of the th image class,
where denotes the length of a SIFT feature vector
and denotes the number of atoms of .
In the recognition stage, for a query image , we extract the
SIFT feature vectors , , where denotes
the number of SIFT feature vectors, and the dictionary feature
. Then, we apply FSRISA to assess the similarity between
and the th image class, , by performing the
-minimization [similar to (3)] to obtain the sparse representa-
tion coefficients for of , with respect to the dictio-
nary consisting of and . Then, we calculate the
reconstruction errors for with respect to and , re-
spectively, and perform voting for each . Based on (4), we
can calculate the similarity between and the th image class,
denoted by Sim( , Class-i). Finally, the query image can
be determined to belong to the th class with the largest Sim( ,
Class-i).
Moreover, the computational complexity for assessing the
similarity between the query image and the th class of
images can be also approximately analyzed based on (8), where
the dictionary feature extraction for the th class of images
can be performed in advance during database construction. In
our image recognition approach, the sparse coding procedure
should be performed for a query image and each image class,
which is indeed computationally expensive. The complexity
of our approach (slightly cheaper than or similar to that of
the two-stage approach proposed in [28]) can be improved
by applying more efficient sparse coding techniques, such as
multi-core OMP [18].
It is also worth noting that, if the feature size of a query image
is crucial for online applications, each SIFT feature vector
can be further compressed via compressive sensing [19].
1026 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011
Fig. 4. Similarity values obtained from the PSNR, VIF, and our FSRISA between the Lena image and its manipulated images. (a) JPEG compression   
 	
       
. (b) Blurring     
       . (c) Brightness and contrast adjusting
           
. (d) Noising         	   . (e) Scaling and cropping
    
       
. (f) Rotation            

. (g) Flipping   
 	       . (h) Shearing    	     
   
. (i) Rippling     	  
 
   . (j) Irrelevant image            .
Fig. 5. Comparison of ROC curves obtained using the proposed FSRISA-based
image copy detection scheme and the “feature points hash” scheme [25].
near-duplicate detection in [9], [10], [24], and [25]. We treat
each of the ten images as a query image and its 204 manipulated
versions as the test images. The parameter settings for applying
FSRISA to each query image and each test image are the same
as those settings for the reference image and test image, respec-
tively, used in Section IV-A.
To evaluate the true positive rate (TPR), the proposed scheme
was conducted between each original image and its 204 manip-
ulated versions. To evaluate the false positive rate (FPR), the
proposed scheme was conducted for each original image and
the 204 manipulated versions of each of the other nine images.
The receiver operating characteristic (ROC) curves (TPR-FPR
Fig. 6. Probability distributions of the FSRISA similarity values between the
Lena image and its 204 manipulated versions and those between the Lena image
and the 204 manipulated versions of each of the other nine test images.
curve) obtained from the proposed scheme by adjusting the
threshold , and the “feature points hash” scheme [25] with
public source code available for the ten images are shown in
Fig. 5. It can be observed from Fig. 5 that the performance of the
proposed FSRISA-based scheme can significantly outperform
that of the “feature points hash” scheme.
On the other hand, we also give an example for demonstrating
the discrimination of our FSRISA. In Fig. 6, the probability dis-
tributions of the FSRISA similarity values between the Lena
image and its 204 manipulated versions, and those between the
Lena image and the 204 manipulated versions of each of the
other nine images, are displayed. It can be observed from Fig. 6
that our FSRISA can indeed discriminate between related and
unrelated images.
1028 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011
mantic meanings. It should be noted that the FSRISA values
between an image and its related images, shown in Fig. 8, are
smaller than those shown in Fig. 6. This is because, in Fig. 8, FS-
RISA is used to assess the similarities between a query image
and the images with the same semantic meaning, but different
appearances in the same class, instead of the manipulated ver-
sions of the query image.
D. Evaluation of FSRISA-Based Image Recognition
To evaluate the proposed FSRISA-based image recogni-
tion scheme, we used the COIL-20 [30] and COIL-100 [31]
datasets. We followed the setup for simulations provided in
[32], where randomly selected 36 images from each class were
used for training samples and the remaining 36 images were
used for testing. We repeated the simulations for ten times with
different randomly selected training images and averaged the
recognition rate obtained from each run. We also evaluated
our scheme by using the Caltech-101 dataset consisting of 101
image categories with high shape variability [33]. We followed
the common setup to randomly select 5, 15, and 30 training
images per category, respectively, and test on the rest images.
We repeated the simulations for ten times with different ran-
domly selected training images and averaged the recognition
rate obtained from each run. It should be noted that similar
to our image retrieval application, using only single feature
(SIFT-based feature) may not work very well for recognizing
objects with overly complex background. Hence, we only
employed the three above-mentioned datasets to investigate
sparse representation of SIFT features and find its usefulness in
image recognition application.
The parameter settings for applying FSRISA to each query
image and each image class are the same as those settings
for the reference image and test image, respectively, used in
Section IV-A. That is, we set the parameters , , ,
, , and based on the settings for , , , , ,
and , respectively, used in Section IV-A, where and
are replaced by and , respectively ( denotes the
number of SIFT feature vectors for the class ).
The number of SIFT feature vectors extracted from an image
in the COIL-20/100 dataset is usually small, and hence, is
not large. When applying our scheme to the Caltech-101 dataset,
we adopted the online dictionary learning algorithm to learn a
dictionary [18] and the implementation of OMP provided in [18]
to perform sparse coding, which are both highly efficient multi-
core implementations.
For testing the COIL-20 dataset, the recognition rates ob-
tained using FSRISA, the best ones reported in [34] (denoted
by “neighborhood-preserving projections”) and [35] (denoted
by “invariant moment”), respectively, are shown in Table III
for comparison. For testing the COIL-100 dataset, the recog-
nition rates obtained using FSRISA, the best ones reported in
[36] (denoted by “distributed sparse representation”), [32] (de-
noted by “SVM-based”), and [37] (denoted by “bipartite graph
matching”), respectively, are shown in Table IV for comparison.
For testing the Caltech-101 dataset, the recognition rates ob-
tained using FSRISA, the results reported in [38] (denoted by
“SVM-KNN”), [39] (denoted by NBNN), and [7] (denoted by
ScSPM), are shown in Table V for comparison. It can be ob-
served from Tables III–V that the performances obtained using
TABLE III
RECOGNITION RATES FOR EVALUATING COIL-20 DATASET
TABLE IV
RECOGNITION RATES FOR EVALUATING COIL-100 DATASET
TABLE V
RECOGNITION RATES FOR EVALUATING CALTECH-101 DATASET
the proposed FSRISA can outperform or be comparable to those
of the schemes used for comparisons.
E. Experimental Comparisons Between Sparse Coding-Based
and Traditional Approaches
In this subsection, we experimentally demonstrate the advan-
tage from sparse coding techniques by evaluating the following
two kinds of comparisons. First, to evaluate the impact of sparse
coding-based feature representation strategy, we compare two
kinds of approaches denoted by: 1) “BoW-Traditional:”
BoW-based feature ;
and 2) “Sparse-Traditional:” sparse coding-based feature
, as follows. A clas-
sical example of “BoW-Traditional” approach realized by using
BoW and SVM can be found in [6], where the best recognition
rate (53.90%) for the Caltech-101 dataset was reported. A
good example of “Sparse-Traditional” approach realized by
using sparse coding and SVM can be found in [7], where the
reported best recognition rate (73.20%) for the same dataset
can significantly outperform the one reported in [6].
Second, to evaluate the impact of sparse coding-based
matching strategy, we compare two kinds of approaches de-
noted by: 1) “Sparse-Traditional;” and 2) “Proposed:” sparse
coding-based feature coding-based
matching. Based on Table V, “Proposed” approach can
slightly outperform “Sparse-Traditional” approach (denoted by
“ScSPM” [7]). Another example for “Sparse-Traditional” also
realized by using sparse coding and SVM for testing COIL-100
dataset can be found in [36]. Based on Table IV, “Proposed” ap-
proach can slightly outperform “Sparse-Traditional” approach
(denoted by “Distributed sparse representation” [36]). Even if
the improvement of the proposed scheme seems to be limited,
a unique characteristic of our scheme is to define a similarity
assessment metric, which can be widely applicable in several
multimedia applications.
1030 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 5, OCTOBER 2011
Li-Wei Kang (S’05–M’06) has been with the Insti-
tute of Information Science, Academia Sinica, Taipei,
Taiwan, as an Assistant Research Scholar since Au-
gust 2010. His research interests include multimedia
content analysis and multimedia communications.
Dr. Kang served as an Editorial Advisory Board
Member for the book Visual Information Processing
in Wireless Sensor Networks: Technology, Trends
and Applications (Hershey, PA: IGI Global, 2011),
a Guest Editor of the Special Issue on Advance in
Multimedia, Journal of Computers, Taiwan, 2010,
a Co-organizer, Special Session on Advanced Techniques for Content-Based
Image/Video Resizing, 2011 Visual Communication and Image Processing
(VCIP2011), Special Session on Image/Video Processing and Analysis, 2011
APSIPA Annual Summit and Conference (APSIPA2011), and a reviewer/TPC
member for several international conferences and journals. He won the two
paper awards presented by 2006 and 2007 Computer Vision, Graphics, and
Image Processing Conferences, Taiwan, respectively.
Chao-Yung Hsu is currently pursuing the Ph.D.
degree in the Graduate Institute of Communication
Engineering of National Taiwan University, Taipei,
Taiwan.
He has been a research assistant with the Institute
of Information Science, Academia Sinica, Taipei,
Taiwan, since 2003. His research interests include
multimedia signal processing, data hiding, and
digital halftoning.
Hung-Wei Chen received the B.S. degree from
National Taipei University of Technology, Taipei,
Taiwan, in 2006 and the M.S. degree from National
Dong-Hwa University, Hualien, Taiwan, in 2008.
He is currently pursuing the Ph.D. degree in the
Graduate Institute of Communication Engineering
of National Taiwan University, Taipei, Taiwan.
He has been a research assistant with the Institute
of Information Science, Academia Sinica, Taipei,
Taiwan, since 2008. His research interests include
image/video compression, multimedia signal pro-
cessing, and compressive sensing.
Chun-Shien Lu (M’99) received the Ph.D. degree
in electrical engineering from National Cheng-Kung
University, Tainan, Taiwan, in 1998.
From October 1998 to July 2002, he joined the
Institute of Information Science, Academia Sinica,
Taipei, Taiwan, as a postdoctoral fellow for his
military service. From August 2002 to July 2006, he
was an assistant research fellow at the same institute.
Since July 2006, he has been an associate research
fellow. His current research interests mainly focus
on various topics (including signal processing and
security) of multimedia, sensor network security, and compressive sensing.
Dr. Lu organized a special session on Multimedia Security in the 2nd and
3rd IEEE Pacific-Rim Conference on Multimedia, respectively (2001–2002). He
co-organized two special sessions (in the area of media identification and DRM)
in the 5th IEEE International Conference on Multimedia and Expo (ICME),
2004. He was a guest co-editor of EURASIP Journal on Applied Signal Pro-
cessing, special issue on Visual Sensor Network in 2005. He has owned two U.S.
patents, three ROC patents, and one Canadian patent in digital watermarking.
He won the Ta-You Wu Memorial Award, National Science Council in 2007 and
was a co-recipient of a National Invention and Creation Award in 2004. Since
July 2007, he has served as a member of the Multimedia Systems and Applica-
tions Technical Committee of the IEEE Circuits and Systems Society. He is cur-
rently an associate editor of the IEEE TRANSACTIONS ON IMAGE PROCESSING.
He is a member of ACM.
Chih-Yang Lin (M’10) received the Ph.D. degree in
computer science and information engineering from
National Chung-Cheng University, Chiayi, Taiwan,
2006.
After graduating, he served in the Advanced
Technology Center of the Industrial Technology
Research Institute of Taiwan (ITRI) from 2007 to
2009. Then, he joined the Institute of Information
Science (IIS), Academia Sinica, Taipei, Taiwan, as
a postdoctoral fellow. Currently, he is an Assistant
Professor in the Department of Computer Science
and Information Engineering, Asia University, Taichung, Taiwan. His research
interests include computer vision, digital rights management, image processing,
and data mining.
Soo-Chang Pei (SM’89–F’00) was born in Soo-Auo,
Taiwan, in 1949. He received the B.S.E.E. degree
from the National Taiwan University, Taipei, Taiwan,
in 1970 and the M.S.E.E. and Ph.D. degrees from
the University of California, Santa Barbara, in 1972
and 1975, respectively.
Since 1984, he has been a Professor with the De-
partment of Electrical Engineering, National Taiwan
University, Taipei, Taiwan. He was an Engineering
Officer with the Chinese Navy Shipyard from 1970 to
1971. From 1971 to 1975, he was a Research Assis-
tant with the University of California, Santa Barbara. He was a Professor and the
Chairman of the Electrical Engineering Department, Tatung Institute of Tech-
nology, Taipei, Taiwan, from 1981 to 1983, and was with the National Taiwan
University as the Chairman of the Electrical Engineering Department from 1995
to 1998, and the Dean of the College of Electrical Engineering and Computer
Science from 2003 to 2009, respectively. His research interests include digital
signal processing, image processing, optical information processing, and laser
holography.
Dr. Pei was the President of the Chinese Image Processing and Pattern Recog-
nition Society in Taiwan from 1996 to 1998 and is a member of Eta Kappa Nu
and the Optical Society of America. He became an IEEE Fellow in 2000 for
his contributions to the development of digital eigenfilter design, color image
coding, and signal compression and to the electrical engineering education in
Taiwan. He was a recipient of a National Sun Yet-Sen Academic Achievement
Award in Engineering in 1984, the Distinguished Research Award from the
National Science Council from 1990 to 1998, an Outstanding Electrical Engi-
neering Professor Award from the Chinese Institute of Electrical Engineering in
1998, the Academic Achievement Award in Engineering from the Ministry of
Education in 1998, the Pan Wen-Yuan Distinguished Research Award in 2002,
and the National Chair Professor Awards from the Ministry of Education in 2002
and 2008, respectively.
Compressive Image Sensing: Turbo Fast Recovery with Lower-Frequency
Measurement Sampling
Chun-Shien Lu(   )
Institute of Information Science,
Academia Sinica, Taipei 115, Taiwan
Email: lcs@iis.sinica.edu.tw
Hung-Wei Chen(   )
Inst. Info. Sci., Academia Sinica and
Graduate Inst. Comm. Eng., NTU, Taiwan
Email: hungwei@iis.sinica.edu.tw
Soo-Chang Pei(  )
Graduate Inst. Comm. Eng.,
Nat’l Taiwan Uni., Taipei, Taiwan
Email: pei@cc.ee.ntu.edu.tw
Abstract—In order to get better reconstruction quality from
compressive sensing of images, exploitation of the dependency or
correlation patterns among the transform coefficients has been
popularly employed. Nevertheless, both recovery quality and re-
covery speed are not compromised well. In this paper, we study a
new image sensing technique, called turbo fast compression image
sensing, with computational complexity O(m2), where m denotes
the length of a measurement vector y = φx that is sampled
from the signal x of length n via the sampling matrix φ with
dimensionality m×n. In order to leverage between reconstruction
quality and recovery speed, a new and novel sampling matrix
is designed. Our method has the following characteristics: (i)
recovery speed is extremely fast due to a closed-form solution is
derived; (ii) certain reconstruction accuracy is preserved because
significant components of x can be reconstructed with higher
priority via an elaborately designed φ. Our method is particularly
different from those presented in the literature in that we focus
on the design of a sampling matrix without relying on exploiting
certain sparsity patterns. Simulations and comparisons with
state-of-the-art CS methodologies are provided and demonstrate
the feasibility of the proposed method in terms of reconstruction
quality and computational complexity.
keywords: Compressive sensing, Measurement, Reconstruc-
tion, Sampling, Sparsity
I. INTRODUCTION
We first describe the background regarding compressive
sensing in Sec. I-A and then discuss related work in Sec. I-B.
A. Background
Compressive sensing (CS) has received much attention
recently due to its revolutionary development in simultane-
ously sensing and compressing signals with certain sparsity.
Moreover, the architecture of the so-called single-pixel camera
[14], [27] has promoted the practicability of compressive
image sensing (CIS). CS is mainly composed of two steps.
Let x denote a k-sparse signal of length n to be sensed, let φ
of dimensionality m×n represent a sampling matrix, and let y
be the measurement of length m. At the encoder, a signal x is
simultaneously sensed and compressed via random projection
and the obtained samples are called measurements y in the
context of compressive sensing. They are related via random
projection as:
y = φx. (1)
The measurement rate is defined as 0 < mn < 1 or 0 <
m
n <<
1, which indicates the compression ratio (without quantization)
without storing the original signal of length n. At the decoder,
the original signal x to be sensed can be perfectly recovered
by means of convex optimization or greedy algorithms if the
relationship between m and k is satisfied [6].
For convex optimization-based CS algorithms, sparse sig-
nal recovery will be time-consuming and intractable if  0-
minimization is adopted. 0-minimization seeks to find k non-
zero entries of a signal if the signal is k-sparse in either
the time/space or transform (e.g., DCT, wavelet, etc) domain.
The solution can become more tractable if the constraint
of 0-minimization is relaxed and 1-minimization is used
instead. Several algorithms relying on 1-minimization have
been presented in the literature.
On the other hand, 2-norm solution is easy to calculate
but the solution is not sparse, which violates the fundamental
assumption of compressive sensing. Nevertheless, some ap-
proaches [19], [20], [25], [29] try to iteratively approximate
0-norm or 1-norm solution from their 2-norm counterpart.
We have also studied a fast sparse signal recovery algorithm
based on iteratively refining 2-norm solution [23].
In addition to convex optimization, non-convex program-
ming (or greedy) algorithms like Orthogonal Matching Pur-
suit (OMP) [37] is an alternative for sparse signal recovery.
Basically, OMP has been recognized as a “fast” algorithm
with time complexity O(kmn) with reasonable reconstruction
quality in some cases. The problem here is that can we come
up with a new solution to sparse signal recovery that is
simultaneously faster and more accurate than OMP and other
state-of-the-art compressive sensing recovery algorithms?
On the other hand, in the context of compressive sensing
(CS) [11], the constraint of sparsity enables the possibility
of sparse signal recovery from measurements (far) fewer
than the original signal length. Moreover, the measurements
generated from random projection of the original signal via
a sampling matrix are equally weighted; i.e., no one is more
significant than the others. Thus, CS is inherently weakened
in handling less-sparse signals such as highly textured images.
The problem here is that can we yield weighted measurements
so that less-sparse signals can be fast reconstructed while
maintaining good reconstruction quality?
In this paper, we address the above problems of achieving
fast and accurate CS recovery with less sparsity constraint.
The idea is to investigate an elaborate design of the sampling
matrix φ that can directly capture “important” measurements.
With these important measurements, the quality original signal
[31]. In fact, each block signal is treated as a 1D signal for
sensing and recovery.
Although block-based image sensing seems to be feasible,
it still incurs the sensor calibration problem. The other is
to employ 2D separate sensing [33], [34]. That is, separate
sensing is conducted along the row and column directions,
separately. Moreover, due to separate sensing strategy, the
problem of storage overhead for storing a sampling matrix
in a resource-limited sensor can be efficiently solved. As can
be seen later, the storage overhead for 2D sensing of an image
of size 1024× 1024 is the same as that for 1D sensing of an
image patch of size 32× 32. Thus, it is apparent that due to
the constraint of storage overhead, 1D block-based sensing is
unfavorable to sense images/patches of large sizes.
In this paper, we study a new compressive image sensing
algorithm via an elaborate design of sampling matrices. The
paper is an extended and complete version of our prior work
[10] in terms of methodological descriptions and analyses,
technical comparisons with related works, and extensive exper-
iments. The details will be described in the following sections.
III. PROPOSED METHOD: TURBO FAST COMPRESSIVE
IMAGE SENSING
We first describe the idea behind our method in III-A.
Then, the proposed turbo fast CS recovery algorithm based on
an elaborately designed sampling matrix is presented in Sec.
III-B. We investigate how it can be conducted via 1D block
sensing and 2D separate sensing. The computation complexity
of recovery and reconstruction quality will be studied.
A. The Idea
Although it is promising to take the concept of clustered
sparsity or tree-structure of transform coefficients into con-
sideration within the compressive sensing framework, we find
two weaknesses for this paradigm of compressive sensing.
The first we notice is that the CS inversion time is still not
computationally efficient. The crux is that the inference for
exploiting some specific sparsity patterns is time-consuming.
In view of this, we seek an alternative so that our CS recovery
time can be significantly reduced while the reconstruction
quality obtained from our method and state-of-the-art algo-
rithms can be comparable.
In order not to spend time in tracing larger transform
coefficients, we propose to sample only those transform
coefficients that are situated at lower frequencies. This is
reasonable because in image compression like JPEG the higher
frequency components will be quantized with larger quantiza-
tion intervals while the lower frequency components can be
preserved with higher priority. Inspired by the principle of
JPEG compression, our CS recovery quality will be designed
to mimic JPEG compressed images. That is, we do not seek
“perfect reconstruction,” which is practically hard to achieve,
due to natural images are usually not sparse.
The most important but unique characteristic that distin-
guishes our method from the existing ones is that our method
can only sample m important measurements with m equal to
the desired degree of sparsity k, and can turbo fast recon-
struct the original signal x approximately from the sampled
measurement vector y with computational complexity O(m2).
The extensive experimental results indicate that our method
indeed is very fast and can keep reconstruction quality up to
the degree of JPEG compression approximately.
The second concern is that compressive sensing conven-
tionally relies on the assumption of sparsity to reconstruct the
original signal from (far) fewer measurements. As studied in
[39], CS is only suitable for so-called sparse signals while
principal component analysis (PCA) is more suitable to deal
with non-sparse or noisy signals.
However, many natural signals inherently containing tex-
tured components are a kind of non-sparse signals. The as-
sumption of sparsity and the exploitation of structured sparsity
do not conform to the property of less-sparse signals. In view
of this, another goal of our method is to target this problem.
Our strategy is intuitive and empirical observations [41] again
suggest that it is better to preserve important measurements
sampled from lower frequency components in some transform
domains.
Our method is particularly different from those reviewed in
Sec. I-B in that we focus on the design of a sampling matrix
while others concern to exploit the dependency or correlation
patterns among the transform coefficients.
B. Turbo Fast Compressive Sensing Recovery via Sampling
Matrix Design
We start from the random projection, y = φx, and observe
that if important information of x can be sampled and stored in
y, then it is possible to approximately reconstruct x with fewer
important measurements in a fast way. The goal is feasible
by designing a new and novel sampling matrix. Our method
has been readily incorporated with single-pixel cameras for
compressive image sensing.
In this subsection, we describe the proposed 1D sensing
of an image patch/block and 2D separate sensing of a whole
image, respectively.
1) 1D Block Sensing: For 1D sensing of an image patch,
we introduce a 1D linear operator T and impose it to random
projection to obtain:
Ty = T (φx), (2)
where x is regarded as a small image or an image patch
of reasonable size. Eq. (2) is further derived based on the
principle of linear operations [28] as:
Ty = T (φx) = (T 2φ)(Tx), (3)
where T 2 denotes 2D linear operator. Please refer to Appendix
A for details.
Eq. (3) reveals that the positions at lower frequencies of
transformed vector Tx indicate important transformed coef-
ficients and Ty indicates important measurements since they
are linear combinations of significant transformed coefficients.
In order to sample “important” transformed coefficients
from Tx and speed up recovery, we design a new sampling
recovery. Thus, the computation complexity of recovery is
in the order of O(m2). As we shall see later in Sec. V,
our method is the fastest CS recovery algorithm among the
methods used for comparisons.
C. Mutual Incoherence
In the literature, a sampling matrix is usually a Gaussian
random matrix or a Bernoulli random matrix taking value
±1 with equal probability. A good sampling matrix, which
preserves incoherence [6] with a dictionary or transform basis,
is desired for efficient recovery. Mutual incoherence leads to
perfect recover with a higher probability under the condition
that m ≥ ck ·O(log(n/k)) holds.
Our results show that random matrix conventionally adopted
in the compressive sensing literature is more incoherent than
the sampling matrix proposed in this paper with either K-SVD
[1] or DCT. We have to, however, point out that since the
sparsity assumption is rarely satsified with respect to natural
images, which are usually less sparse, our method theoretically
sacrifices mutual incoherence between the sampling matrix
and the corresponding dictionary but is practically feasible in
reconstructing signals with higher quality and speed.
D. Sparsity vs. Reconstruction Quality
Following the above descriptions, the impact of sparsity
on reconstruction quality is discussed here. According to our
empirical observations, almost all compressive sensing algo-
rithms proposed so far fail to reconstruct a natural image with
quality (in terms of PSNR or SSIM [38]) superior to simple
interpolation. For example, consider a natural image, Peppers,
of size 50 × 50. When a bicubic interpolation technique is
adopted to generate an enlarged image of size 100 × 100,
the obtained PSNR is 26.76dB (between the original Peppers
and the resultant interpolated Peppers). However, within the
framework of compressive sensing, if the original image has
the size of 100 × 100 and the number of corresponding
measurements is 50 × 50, the obtained image obtained from
the measurements via OMP has PSNR 17.06dB, which is
far lower than the one obtained using bicubic interpolation.
Similar results can also be observed from many other CS
recovery algorithms.
In view of these results, we propose a new CS recovery
algorithm that does not follow the convention of CS. More
specifically, our method is an alternative of CS in the sense that
we seek to pursue approximate recovery instead of theoretic
perfect reconstruction that is the ultimate goal of CS. Our
observations and results are strongly supported by the fact
that natural images are not highly or sufficiently sparse,
which somewhat violates the fundamental assumption of CS.
However, we also have to clarify that for those applications
exhibiting sufficient sparsity the random sampling matrix
inherently used in compressive sensing may be better than
the one presented in this paper.
E. Approximate Recovery vs. Perfect Recpnstruction
It is known that under the constraint of m ≥ ck ·O(log nk )
conventional compressive sensing methods can achieve perfect
reconstruction if the available number of measurements and
the sparsity of the signal to be reconstructed satisfy the above
relation. However, perfect reconstruction is usually not prac-
tical because for many signals with large k, sometimes only
small M is available, leading to violation of m ≥ ck·O(log nk ).
In this paper, we aim to explore a more practical solution to
CS recovery. More specifically, based on the elaborate design
of the sampling matrix, we do not seek perfect reconstruction
but approximate reconstruction. Depending on the selected
m, which is assumed to be k, turbo fast and approximate
reconstruction can be achieved. Approximate reconstruction
has been readily useful for many CS-based image/video ap-
plications [42], [43].
In this paper, as described in Appendix A, a Haar matrix is
exploited as T to design the sampling matrix Φ such that the
original signal x can be approximately reconstructed from as
many measurements as the number of transform coefficients
sampled via Eq. (5) and Eq. (6).
V. EXPERIMENTAL RESULTS
Several experiments were conducted to verify the perfor-
mance of the proposed turbo fast CIS methods in terms of
reconstruction quality and speed.
State-of-the-art CS algorithms [11], including orthogonal
matching pursuit (OMP) [37], Lasso, TS-BCS-MCMC [21],
TS-BCS-VB [22], and model-based CS (MCS) [3]1, were
chosen for comparisons under different measurement rates
(MRs). There are two signal models, wavelet trees and block
sparsity, used in MCS. In our experiments conducted here,
2D wavelet trees and block sparsity in [3] were adopted. The
default settings of all source codes were employed in our
experiments to better guarantee the good performances of the
aforementioned methods for fair comparisons.
All experiments were conducted in Matlab 7.11 (R2010b)
with Intel CPU Core i7 930 (2.80 GHz) and 6 GB RAM under
OS Windows 7 Enterprise edition 64-bit. For simulations of
image sensing, several images with different sizes and spar-
sities, including Baboon, Barbara, Cameraman, Flintstones,
Lena, and Peppers, were adopted.
For 1D sensing, all the methods used for comparisons were
conducted in a block-wise manner. That is, image sensing
and recovery were executed for each 32 × 32 block [30].
In our method, as derived in Appendix A, two Haar matri-
ces, H1024×1024 and H(1024×MR)×(1024×MR) were employed.
However, if RAM is larger, then the block size can be
permitted to be larger.
For 2D sensing in our method, we currently only employ
2D DCT because the design of 2D wavelet filters in terms of
matrix forms need to be further studied.
A. Reconstruction Quality
The recovery quality is measured in terms of PSNR (in dB)
and structural similarity (SSIM, 0 ≤ SSIM ≤ 1) indexing [38],
respectively. The bigger, the better. Table I and Table II show
1Our experiments conducted on MCS show that block sparsity is better
than 2D tree in both reconstruction quality and speed.
TABLE I
RECOVERY QUALITY COMPARISON OF CS ALGORITHMS UNDER DIFFERENT MEASUREMENT RATES (MRS) FOR CAMERAMAN IMAGE.
Methods Metrics MR (1.56%) MR (3.13%) MR (6.25%) MR (12.5%) MR (25.0%) MR (50.0%)
OMP PSNR(dB) 7.41 14.67 15.86 17.47 20.06 23.68
(Sparsify toolbox) SSIM 0.04 0.32 0.35 0.41 0.53 0.69
Lasso PSNR(dB) 13.82 15.39 17.24 19.04 21.76 25.96
(Sparsify Lab) SSIM 0.33 0.38 0.43 0.49 0.62 0.77
Model-based CS PSNR(dB) 6.38 7.42 9.18 14.92 21.99 23.05
(CoSaMP+block sparsity) [3] SSIM 0.06 0.07 0.08 0.26 0.64 0.70
TS-BCS-VB PSNR(dB) 17.11 17.98 19.10 20.20 22.62 26.78
[22] SSIM 0.46 0.50 0.56 0.63 0.73 0.85
TS-BCS-MCMC PSNR(dB) 16.87 17.90 19.33 20.59 23.11 27.39
(wavelet tree) [21] SSIM 0.46 0.51 0.55 0.64 0.74 0.85
TS-BCS-MCMC PSNR(dB) 7.68 11.64 18.10 21.08 23.87 28.15
(DCT tree) [21] SSIM 0.08 0.26 0.48 0.60 0.75 0.87
Our Method (1D sensing: PSNR(dB) 18.32 18.39 19.84 22.15 24.82 29.06
Haar wavelet-based) SSIM 0.57 0.58 0.64 0.73 0.82 0.92
Our Method (1D sensing: PSNR(dB) 18.42 18.46 19.38 22.36 25.60 30.22
DCT-based) SSIM 0.58 0.58 0.61 0.69 0.79 0.90
Our Method (2D sensing: PSNR(dB) 20.74 21.84 23.26 24.94 27.46 31.42
DCT-based) SSIM 0.58 0.63 0.70 0.78 0.87 0.94
TABLE II
RECOVERY QUALITY COMPARISON OF CS ALGORITHMS UNDER DIFFERENT MEASUREMENT RATES (MRS) FOR BARBARA IMAGE.
Methods Metrics MR (1.56%) MR (3.13%) MR (6.25%) MR (12.5%) MR (25.0%) MR (50.0%)
OMP PSNR(dB) 15.84 16.55 18.00 20.26 23.12 27.76
(Sparsify toolbox) SSIM 0.19 0.22 0.31 0.47 0.64 0.82
Lasso PSNR(dB) 14.76 16.70 19.15 21.70 24.89 29.99
(Sparsify Lab) SSIM 0.19 0.26 0.38 0.54 0.72 0.88
Model-based CS PSNR(dB) 7.13 7.82 9.81 16.32 23.82 25.03
(CoSaMP+block sparsity) [3] SSIM 0.04 0.05 0.06 0.31 0.69 0.74
TS-BCS-VB PSNR(dB) 18.46 19.30 20.31 21.37 22.35 23.85
[22] SSIM 0.37 0.41 0.46 0.54 0.62 0.74
TS-BCS-MCMC PSNR(dB) 18.19 19.48 20.85 22.04 23.31 25.19
(wavelet tree) [21] SSIM 0.36 0.42 0.48 0.55 0.65 0.77
TS-BCS-MCMC PSNR(dB) 8.81 11.41 18.07 22.49 24.57 29.07
(DCT tree) [21] SSIM 0.06 0.20 0.43 0.59 0.73 0.89
Our Method (1D sensing: PSNR(dB) 19.25 19.32 21.18 22.98 25.49 30.24
Haar wavelet-based) SSIM 0.48 0.50 0.56 0.65 0.79 0.93
Our Method (1D sensing: PSNR(dB) 19.31 19.38 20.27 22.60 25.41 33.03
DCT-based) SSIM 0.48 0.49 0.52 0.61 0.77 0.93
Our Method (2D sensing: PSNR(dB) 22.47 23.24 23.92 24.54 25.70 30.88
DCT-based) SSIM 0.56 0.61 0.67 0.73 0.82 0.93
impractical because before sensing we have no image data that
can be used for wavelet transform. On the contrary, our method
can directly sample signals in the time/space domain without
needing such a pre-processing step.
Another characteristic of our method is that the original
signal x can be approximately reconstructed from as many
measurements as the number of transform coefficients sampled
via Eq. (5) and Eq. (6).
VI. CONCLUSIONS AND FUTURE WORK
Fast and accurate compressive sensing recovery is still a
challenging issue, and has received considerable attention in
the literature. In this paper, we do not follow the tradition of
imposing certain sparsity patterns on a CS recovery algorithm.
On the contrary, we propose to design a new and novel sam-
pling matrix for the purpose of preserving important measure-
ments. Under this circumstance, turbo fast and accurate CIS
recovery with closed-form solution of computation complexity
O(m2) can be achieved.
We have also studied 1D sensing and 2D separate sensing
strategies for 1D signals and 2D images, respectively. Com-
pared to 1D sensing, 2D separate sensing is found to be par-
ticularly feasible in compressive sensing of large-scale images
in terms of storage and computation overhead reduction and
reconstruction quality improvement.
The issues, including the impact of noisy measurements on
our methods and the exploration of CS characteristics for our
method, deserve further studying.
ACKNOWLEDGMENT
This work was supported by National Science Council,
Taiwan, under grants NSC 97-2628-E-001-011-MY3 and NSC
fact, the Haar matrix is used in our method to decompose the
sampling matrix, as indicated in Eq. (3).
(a) (b)
Fig. 3. Comparison of Haar Transforms: (a) Haar matrix decomposition; (b)
conventional Haar decomposition.
REFERENCES
[1] M. Aharon, M. Elad, and A. M. Bruckstein, “The K-SVD: an algorithm for
designing of overcomplete dictionaries for sparse representation,” IEEE
Trans. on Signal Processing, 2006.
[2] S. D. Babacan, R. Molina, and A. K. Katsaggelos, “Bayesian compressive
sensing using Laplacr prior,” IEEE Trans. on Image Processing, vol. 19,
no. 1, pp. 53-63, 2010.
[3] R. Baraniuk, V. Cevher, M. F. Duarte, and C. Hedge, “Model-based
Compressive Sensing,” IEEE Trans. on Information Theory, vol. 56, no.
4, pp. 1982-2001, 2010.
[4] R. Baraniuk, V. Cevher, and M. B. Wakin, “Low-Dimensional Models for
Dimensionality Reduction and signal Recovery: A Geometric Perspec-
tive,” Proceedings of The IEEE, vol. 98, no. 6, pp. 959-971, 2010.
[5] T. Blumensath and M. E. Davies, “Iterative hard thresholding for com-
pressed sensing,” preprint, 2008.
[6] E. Candes and J. Romberg, “Sparsity and incoherence in compressive
sampling,” Inverse Problems, vol. 23, no. 3, pp. 969-985, 2007.
[7] P. J. Carrgues, “Sparse coding models of natural images: algorithms
for efficient inference and learning of higher-order structure,” Ph. D.
dissertation, 2009.
[8] V. Cevher, C. Hedge, M. F. Duarte, and R. G. Baraniuk, “Sparse signal
recovery using Markov random fields,” Proc. Workshop on Neural Info.
Proc. Sys. (NIPS), 2008.
[9] V. Cevher, P. Indyk, C. Hedge, and R. G. Baraniuk, “Recovery of clustered
sparse signals from compressive measurements,” Proc. Sampling Theory
Appl. (SAMPTA), 2009.
[10] H. W. Chen, C. S. Lu, and S. C. Pei, “Fast Compressive Sensing Recovery
with Transform-based Sampling,” Proc. Workshop on Signal Processing
with Adaptive Sparse Structured Representations, pp. 110, June 27-30,
Edinburgh, UK, 2011.
[11] Compressive Sensing Resources: http://dsp.rice.edu/cs
[12] S. Dekel, “Adaptive compressed image sensing based on wavelet-trees,”
report, 2008.
[13] S. Deutsch, A. Averbuch, and S. Dekel, “Adaptive compressed image
sensing based on wavelet modeling and direct sampling,” Proc. SAMPTA,
2009.
[14] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. Kelly,
and R. G. Baraniuk, “Single-pixel imaging via compressive sampling,”
IEEE Signal Processing Mag., vol. 25, pp. 83-91, 2008.
[15] Y. C. Eldar and M. Mishali, “Robust recovery of signals from a structured
union of subspaces,” IEEE Trans. on Information Theory, vol. 55, no. 11,
pp. 5302-5316, 2009.
[16] Y. C. Eldar, P. Kuppinger, and H. Bolcskei, “Block-sparse signals:
Uncertainty relations and efficient recovery,” IEEE Trans. on Signal
Processing, vol. 58, no. 6, pp. 3042-3054, 2010.
[17] L. Gan, “Block compressed sensing of natural images,” Conf. on Digital
Signal Processing (DSP), Cardiff, UK, July 2007.
[18] L. Gan, T. T. Do, and T. D. Tran, “Fast Compressive Imaging Using
Scrambled Block Hadamard Ensemble,” Proc. 16th European Signal
Processing Conference (EUSIPCO), Lausanne, Switzerland, August 2008.
[19] I.F. Gorodnitsky, and B.D. Rao, “Sparse Signal Reconstruction from
Limited Data Using FOCUSS: A Re-weighted Minimum Norm Algo-
rithm,” IEEE Trans. on Signal Processing, vol. 45, no. 3, pp. 600-616,
1997.
[20] Z. Harmany, D. Thompson, R. Willett, and R.F. Marcia, “Gradient
Projection for Linearly Constrained Convex Optimization in Sparse Signal
Recovery,” Proc. IEEE ICIP, pp. 3361-3364, 2010.
[21] L. He and L. Carin, “Exploiting structure in wavelet-based bayesian
compressed sensing,” IEEE Transactions on Signal Processing, vol. 57,
no. 9, pp. 3488-3497, 2009.
[22] L. He, H. Chen, and L. Carin, “Tree-Structured Compressive Sensing
with Variational Bayesian Analysis,” IEEE Signal Processing Letters, vol.
17, no. 3, pp. 233-236, 2010.
[23] S. H. Hsieh and C. S. Lu, “Fast Sparse Signal Recovery via Iteratively
Refining L2-Norm Solution,” submitted.
[24] J. Huang, T. Zhang, and D. Metaxas, “Learning with structured sparsity,”
Proc. Int. Conf. on Machine Learning (ICML), 2009.
[25] M. Hyder, and K. Mahata, “An Approximate L0 Norm Minimization
Algorithm for Compressed Sensing,” Proc. IEEE ICASSP, pp. 3365-3368,
2009.
[26] L. W. Kang and C. S. Lu, “Distributed Compressive Video Sensing,”
Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, pp.
1169-1172, Taipei, Taiwan, 2009.
[27] F. Magalhaes, F. M. Araujo, M. V. Correia, M. Abolbashari, and F.
Farahi, “Active Illumination Single-pixel Camera Based on Compressive
Sensing,” Journals of Applied Optics, vol. 50, no. 4, pp.405-414, 2011.
[28] N. Merhav and V. Bhaskaran, “A transform domain approach to spatial
domain image,” HPL-94-116, Technion City, Haifa 32000, Israel, 1994.
[29] H. Mohimani, M. Babaie-Zadeh, and C. Jutten, “A Fast Approach for
Overcomplete Sparse Decomposition Based on Smoothed L0 Norm,”
IEEE Trans. on Signal Processing, vol. 57, no. 1, pp. 289-301, 2009.
[30] S. Mun and J. E. Fowler, “Block compressed sensing of images using
directional transforms,” Proc. IEEE Int. Conf. Image Processing, pp.
3021-3024, 2009.
[31] S. Mun and J. E. Fowler, “Residual reconstruction for block-based
compressed sensing of video,” Proc. Data Compression Conference
(DCC), pp. 183-192, 2011.
[32] D. Needell and J. Tropp, “CoSaMP: Iterative signal recovery from in-
complete and inaccurate samples,” Applied and Computational Harmonic
Analysis, vol. 26, no. 3, pp. 301-321, 2009.
[33] Y. Rivenson and A. Stern, “Compressed Imaging with a Separable
Sensing Operator,” IEEE Signal Processing Letters, vol. 16, no. 6, pp.
449-452, 2009.
[34] Y. Rivenson and A. Stern, “Practical compressive sensing of large
images,” 16th Int’l Conf. on Digital Signal Processing, July 2009.
[35] P. Schniter, “Turbo reconstruction of structured sparse signals,” Proc.
44th Annual Conf. on Information Sciences and Systems (CISS), 2010.
[36] M. Stojnic, F. Parvaresh, and B. Hassibi, “On the reconstruction of block-
sparse signals with an optimal number of measurements,” IEEE Trans.
on Signal Processing, vol. 57, no. 8, pp. 3075-3085, 2009.
[37] J. A. Tropp and A. C. Gilbert, “Signal Recovery From Random Measure-
ments Via Orthogonal Matching Pursuit,” IEEE Trans. on Information
Theory, vol. 53, no. 12, pp. 4655-4666, 2007.
[38] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
assessment: From error visibility to structural similarity,” IEEE Trans. on
Image Processing, vol. 13, no. 4, pp. 600-612, 2004.
[39] Y. Weiss, H.-S. Chang, and W. T. Freeman, “Learning compressed
sensing,” Proc. Allerton, 2007.
[40] X. Wu, X. Zhang, and J. Wang, “Model-guided adaptive recovery of
compressive sensing,” Proc. Data Compression Conf., pp. 123-132, 2009.
[41] E. Y. Yam and J. W. Goodman, “A Methematical Analysis of the DCT
Coefficient Distributions for Images,” IEEE Trans. on Image Processing,
vol. 9, no. 10, pp. 1661-1666, 2000.
[42] G. Yu, G. Sapiro, and S. Mallat, “Image Modeling and Enhancement via
Structured Sparse Model Selection,” Proc. IEEE International Conference
on Image Processing (ICIP), Hong Kong, 2010.
[43] G. Yu, G. Sapiro, and S. Mallat, “Solving Inverse Problems with Piece-
wise Linear Estimators: From Gaussian Mixture Models to Structured
Sparsity,” submitted, arxiv.org/abs/1006.3056, 2010.
[44] G. Yu and G. Sapiro, “Statistical Compressive Sensing of Gaussian
Mixture Models,” submitted, arxiv.org/abs/1101.5785, 2011.
[45] L. Yu, J. -P. Barbot, G. Zheng, and H. Sun, “Compressive Sensing for
Cluster Structured Sparse Signals: Variational Bayes Approach,” 2011.
 2
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 99 年 11 月 1 日 
一、參加會議經過 
本人此次純出國參加 ACM 國際多媒體會議(ACM MM)，並無論文發表。ACM MM conference 是
多媒體領域重要之主流會議之一，今年已是第 18 屆舉辦。ACM 每年舉辦一次，基本原則由歐、美、
亞洲輪流主辦。同時，ACM TOMCCAP 之 editor board meeting，Springer Multimedia System Journal 
meeting 與 ACM SIGMM meeting 也同時在 ACM MM 舉行。眾所周知 ACM MM 論文以低錄取率著稱，
論文能見度高，參加 ACM MM 是知悉此領域的研究現況與未來發展的最佳管道。 
ACM MM 一向提供各式各樣的研究成果發表管道，包括 oral paper、poster paper、Multimedia Grand 
Challenge (2009 在北京舉辦時所發起)、open source software competition、technical demonstration、and 
doctoral symposium。論文發表共分為 Content track、Application track、System track、and Human-Centered 
Multimedia track 等 4 類。以論文接受率而言，今年 oral+poster paper 共有 974 篇論文投稿，oral paper
接受率為 17%；poster (short) paper 接受率為 32%。整體而言，application track 的論文有明顯增加趨勢。 
ACM MM 會議共分五天舉行，第一天為 tutorial day，第五天為 Workshop day。第二天至第四天為
main conference 論文發表。第二天 Oct. 26 上午的 keynote talk 是由 Dr. Duncan Watts (Yahoo! Research, 
USA)主講”Using the Web to do Social Science”。這個 talk 基本上結合 social network 與 web 等兩大熱門
議題。第二天其餘時間我則參與 oral paper 與 poster paper sessions。尤其我選擇聆聽 content track sessions 
(Automatic Image Tagging 與 Learning Concepts in Images)的論文發表。另外也參加 Panel Session 1 (The 
計畫編號 NSC 97-2628-H-001-011-MY3 
計畫名稱 先進分散式視訊編碼之研究 
出國人員
姓名 呂俊賢 
服務機構
及職稱 
中央研究院資訊所 
副研究員 
會議時間 
99 年 10 月 25 日
至 
99 年 10 月 29 日 
會議地點 
 
Firenze, Italy 
會議名稱 
(中文) 
(英文) ACM Multimedia Conference (ACM MM) 
發表論文
題目 
(中文)無 
(英文)無 
 
 4
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 100 年 1 月 31 日 
一、參加會議經過 
本人於民國 100 年 1 月 23 日至 1 月 28 日出國參加 IS&T/SPIE Electronic Imaging 會議，這會議(以
下簡稱 IS&T/SPIE EI)，是由與 electronic imaging 相關共 20 多個子會議所組成。IS&T/SPIE EI 每年舉
辦一次，基本原則都在美西舉行。本人參加之子會議名稱為``Media Watermarking, Security, and Forensics 
XIII。’’由此會議名稱便可知今年已舉辦第 13 屆，事實上，在 10 多年前 multimedia security 最盛行時，
IS&T/SPIE 便在 IEEE 與 ACM 之外，發起 multimedia security 相關之會議，過去這幾年來，會議名稱
也做過幾次些許修改。此項 IS&T/SPIE Media Watermarking, Security, and Forensics 會議特性是 program 
committee members 與參與會議多為該領域重要之研究者，此項會議非以低錄取率著稱，但確是該領域
必要參加之學術研討會，發表之論文可立即被相關學者/參與者注意到。 
IS&T/SPIE Media Watermarking, Security, and Forensics 會議論文發表共分三天舉行，每天第一場皆
是 keynote talk，其餘時間由單一個 oral session 進行。Jan. 24 第一場 keynote 是由 Dr. Bruce Davis 
(Digimarc Corp.) 主講``Signal Rich Art: Enabling the Vision of Ubiquitous Computing。’’這個 talk 主要目
的是講者從 digital watermarking 與 data hiding 發展之演進，來談論其如何能應用於 mobile device 與
mobile computing 上。在這場 keynote 之後開始的論文報告，在早上第一場的 oral session 是 Security，
計畫編號 NSC 97-2628-H-001-011-MY3 
計畫名稱 先進分散式視訊編碼之研究 
出國人員
姓名 呂俊賢 
服務機構
及職稱 
中央研究院資訊所 
副研究員 
會議時間 
100 年 1 月 23 日
至 
100 年 1 月 28 日 
會議地點 
 
San Francisco, US 
會議名稱 
(中文) 
(英文)IS&T/SPIE Electronic Imaging 2011: Media Watermarking, Security, 
and Forensics XIII 
發表論文
題目 
(中文) 
(英文)Homomorphic encryption-based secure SIFT for privacy-preserving feature
extraction 
 6
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 100 年 5 月 31 日 
一、參加會議經過 
本人此次出國參加的 IEEE ICASSP 會議，是含蓋語音、訊號處理、甚至多媒體領域重要之主流會
議之一，今年已是第 36 屆舉辦。ICASSP 每年舉辦一次，基本原則由歐、美、亞洲輪流主辦。同時，
IEEE Trans. on Image Processing 之 editor board meeting 與一些 IEEE Signal Processing Society 轄下之
technical committees 也在 ICASSP 舉行。ICASSP 論文能見度高，參加 ICASSP 是知悉此領域的研究現
況與未來發展的最佳管道。今年共約 2946 篇論文投稿，比去年多 5%，錄取率為 49%。來自臺灣總投
稿篇數為 113 篇，為全世界排第 8。ICASSP 同其它 IEEE 多媒體相關會議，非以低錄取率取勝，卻是
該領域必要參加之學術研討會。今年所投稿的 ICASSP 論文中，以 speech processing、 image, video, and 
multidimensional signal processing、and signal processing theory and methods 等這三大類為最多。 
ICASSP 會議論文發表共分四天舉行，每天第一場皆是 plenary talk，其餘時間由多個 oral sessions
與 poster sessions同時進行。May 24第一場 plenary talk是由Dr. Henry Tirri (Senior Vice President, Head of 
Nokia Research Center, Palo Alto, CA, USA) 主講``Making Sense of a Zettabyte World。’’這個 talk 主要目
的是簡介配置 radio 的 computers dominate 整體 sense 周邊資訊的能力，與這些龐大資料流的特性，及
如何處理這些龐大資料。在接下來的 regular sessions，我選擇``Compressive Sampling and Sparse 
計畫編號 NSC 97-2628-H-001-011-MY3 
計畫名稱 先進分散式視訊編碼之研究 
出國人員
姓名 呂俊賢 
服務機構
及職稱 
中央研究院資訊所 
副研究員 
會議時間 
100 年 5 月 22 日
至 
100 年 5 月 27 日 
會議地點 
 
Prague, Czech 
會議名稱 
(中文) 
(英文) 36th IEEE International Conference on Acoustics, Speech, and Signal 
Processing (ICASSP 2011) 
 
 
發表論文
題目 
(中文)無 
(英文)無 
 8
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 100 年 7 月 1 日 
一、參加會議經過 
本人此次出國參加的 SPARS 會議，是專屬 compressive sensing 與 sparse representation 領域的會議，
今年是第 4 屆舉辦。由於 compressive sensing 與 sparse representation 是近來相當熱門研究議題，SPARS
會議從 2005 年開始，每兩年舉辦一次，這四次皆在歐、美舉行。SPARS 吸引相關研究者參與，故論文
能見度高，參加 SPARS 是知悉此領域的研究現況與未來發展的最佳管道。今年共約 108 篇論文發表(含
8 篇 plenary talks)，我們是唯一來中華民國的論文發表與參加者。基本上，SPARS 最主要特色是鼓勵參
加並參與討論，非著重於低錄取率。今年 SPARS 並有多達 8 場 plenary talks，皆由該領與知名學者演
講，這是其他知名國際會議也少有的特色。論文發表分為 oral 與 poster presentations。 
SPARS 會議論文發表共分四天舉行，每天第一場皆是 plenary talk，其餘時間由 2 個 oral sessions
同時進行。第一天 June 27 第一場 plenary talk 是由 Prof. Yi Ma (MSRA, Beijing and UIUC)主講``TILT and 
RASL: For Low-Rank Structures in Images and Data。’’這個 talk 主要目的是講者利用 sparsity 於 image 與
video 之應用。Prof. Yi Ma 提出 robust PCA 去 extract low-rank structures in images and videos，其中 visual 
data D 可分為 low-rank component A 與 sparse noise/corruption E。Prof. Yi Ma 並列舉出許多廣泛於
computer vision and pattern recognition 方面的應用，可說是在使用 sparsity 於 computer vision 應用發揮
計畫編號 NSC 97-2628-H-001-011-MY3 
計畫名稱 先進分散式視訊編碼之研究 
出國人員
姓名 呂俊賢 
服務機構
及職稱 
中央研究院資訊所 
副研究員 
會議時間 
100 年 6 月 27 日
至 
100 年 6 月 30 日 
會議地點 
 
Edinburgh, UK 
會議名稱 
(中文) 
(英文) 4th Workshop on Signal Processing with Adaptive Sparse Structured 
Representations (SPARS 2011) 
 
發表論文
題目 
(中文) 
(英文) Fast Compressive Sensing Recovery with Transform-based 
Sampling 
 10
signals 是 temporally evolving signals(e.g., video, audio, MRI, etc)。不過兩篇都偏重於 signal processing 與
theoretic analysis。 
在第三天 June 29 第一場 plenary talk 是由 Prof. D. Donoho (Stanford University)主講``Precise 
Asymptotics Results in Compressed Sensing。’’講者今天的主題包含 Information Theory, Mathematic 
Statistics, and Heuristic Clustering，特別的是 Prof. Donoho 提到 precise asymptotic results on mean squared 
error and other characteristics of a range of recovery algorithms in a range of high-dimensional problems from 
sparse regression and compressed sensing。演講一開始，Prof. Donoho 以 Pediatric MRI 為例，說明
compressed sensing 優越之處。Prof. Donoho 也提到 strict sparsity too strong for real applications，這點與
我們在 compressive image sensing 的見解類似。隨後，我選擇``#11 Compressed Sensing Theory’’這 lecture 
session 聆聽。在``Weighted Lp Constrains in Noisy Compressed Sensing’’這篇論文，作者提到 BPDN (Basis 
Pursuit Denoising) can only adapt to AWGN (from MAP standpoint)，為了處理 generalized Gaussian noise 
(GGN)，作者提出 Basis Pursuit for Generalized Gaussian Noise (BPGGN)。在``Spread Spectrum for 
Universal Compressive Sensing’’論文，作者探討(1) universal sampling strategy does not depend on sparsity 
basis; (2) energy of MRI images concentrate in low-frequencies; (3) analyze the relation between coherence, 
and the relationship between m and k。下午第二場 plenary talk 是 Prof. M. Vetterli (EPFL and US Berkeley)
主講``Sampling in the Age of Sparsity’’。如題目所示，講者先介紹 sampling 的發展演進，接著介紹
nonlinear sampling 以及 nonlinear approximation in wavelet spaces。除了 sparsity，Prof. Vetterli 也介紹其
發展的 finite rate of innovation 之概念。總之，structured sparsity=finite rate of innovation but not equal to 
sparse samples。最後，Prof. Vetterli 更強調``Power/Energy is the key driver!’’。 
在第四天 June 30 早上的 plenary talk 是由 Prof. J. A. Tropp (CalTech, USA)主講``Finding Structure 
with Randomness。’’基本上，主要概念是 randomized algorithms 對於 approximate matrix factorization 有
相當加速效果。講者利用 random projection，巧妙轉換 matrix multiplications 為小矩陣相乘。關鍵在於
現今已有許多 architecture(e.g., GPU, multi-core, distributed systems)可用作matrix multiplication，節省data 
transfer 反而是必要的。Prof. Tropp 更提出實例說明，``The Decompositional Approach to Matrix 
Computation’’是 Top-Ten scientific problems 來證實此研究之重要性。隨後，我選擇``#17 Dictionary 
Learning’’ session 聽講。在論文``Approximate Message Passing for Bilinear Models’’，作者使用 Prof. 
Donoho 在 plenary talk 提到相同方法，` `Approximate Message Passing (AMP)’’，研究 dictionary learning，
來解決 dictionary 遭遇 noises 以及 L1-norm minimization 需大量 training samples 的問題。下午第二場
plenary talk 是 Prof. S. Wright (UW-Madison)主講``Gradient Algorithms for Regularized Optimization’’。重
點包含``Many applications need structured, approximate solutions of optimization formula, so a weighted 
regularization term is added for inducing a particular kind of structure in the solution.’’ Prof. Wright 為
gradient algorithms 作一番詳細 review。在接下來的 lecture session，我挑選``#21 PCA/ICA/BSS’’聆聽，
在``Two Proposals for Robust PCA using Semidefinite Programming''論文，作題提出(1)maximum mean 
absolute deviation rounding (MDR)與(2)low-leverage decomposition (LLD)兩個方法，有別於其他 robust 
PCA，作者強調 principled foundation。在論文``Finding Sparse Approximation to Extreme Eigenvectors: 
Generalized Power Method for Sparse PCA and Extensions’’，作者詳實提出 generalized power method for 
sparse principle component analysis。紮實的數學分析令人印象深刻。 
我們的論文``Fast Compressive Sensing Recovery with Transform-based Sampling’’，排定以 poster 方
式發表，時間為 June 29 與 June 30 兩天下午各 40 分鐘。我們的方法精心設計一 sampling matrix，達到
turbo fast approximation 的目的。在討論期間，也發現有其他 researchers 同樣著手於 sampling matrix 設
計，也發現有將 Wyzer-Ziv ocding 與 compressive sensing 結合的方法，不過我們兩年前已有類似成果。 
 12
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                                   日期： 100 年 8 月 28 日 
一、參加會議經過 
本人此次出國參加的 ICDSC 會議，屬於 distributed smart cameras 領域的會議，今年是第 5 屆舉辦。
近年來，由於視訊監控與物件追蹤的重要，distributed smart cameras 是近來相當熱門研究議題，ICDSC
會議從 2007 年開始，每年舉辦一次。今年 ICDSC 並與 Advanced Concepts for Intelligent Vision Systems 
(ACIVS 2011)合辦。ICDSC 的論文發表分為 oral 與 poster presentations。由於 ICDSC 議題集中，是屬
於小型會議，parallel sessions 不多，而且時間安排並不緊湊。 
ICDSC 會議論文發表共分四天舉行，每天第一場 9:00am 開始皆是 invited talk，其餘時間分為 oral 
session, poster session, 與 demo session。在今年 ICDSC 中，可發現主要研究議題為 visual sensor network, 
distributed computer vision, embedded systems and applications, video coding, and security and privacy。尤
其 8 月 24 日的一個 oral session “Video Coding on Visual Sensor Networks’’，我特別感興趣其中來自主辦
國比利時Vrije Universiteit Brussel與 Interdisciplinary Institute for Broadband Technology (IBBT)的兩篇關
於 distributed video coding (DVC)的論文。在 DVC 方面，本人在幾年前也曾研究過，即使 DVC 在現今
mobile devices and sensors 非常受用，但深感仍受傳統 codec (如 MPEGX and H.26X)所排擠。在 ICDSC
發現 DVC 文章非常興奮，在作者另外展示其系統時，也與其討論一些問題，如(1) time complexity 
comparison with DISCOVER’s scheme; (2) rate-distortion performance comparison with H.264 with no 
motion mode; (3) the reasons for studying DVC。 
 
計畫編號 NSC 97-2628-H-001-011-MY3 
計畫名稱 先進分散式視訊編碼之研究 
出國人員
姓名 呂俊賢 
服務機構
及職稱 
中央研究院資訊所 
副研究員 
會議時間 
100 年 8 月 22 日
至 
100 年 8 月 25 日 
會議地點 
 
Gent, Belgium 
會議名稱 
(中文) 
(英文) 5th ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC 
2011) 
發表論文
題目 
(中文)無 
(英文)無 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/11/30
國科會補助計畫
計畫名稱: 先進分散式視訊編碼技術之研究 
計畫主持人: 呂俊賢
計畫編號: 97-2628-E-001-011-MY3 學門領域: 影像處理 
研發成果名稱
(中文) 壓縮影像感測
(英文) Compressive Image Sensing
成果歸屬機構
中央研究院 發明人
(創作人)
呂俊賢,陳泓瑋
技術說明
(中文) We study a new image sensing technique, called compression image 
sensing (CIS), with computational complexity O(m^2), where m denotes 
the length of a measurement vector y=\phi x that is sampled from the 
signal x of length n via the sampling matrix \phi with dimensionality 
mXn. 
In order to balance between reconstruction quality and recovery speed, 
we propose a novel sampling matrix and study different kinds of 
sensing. 
The contributions of this project include: 
(i) reconstruction speed is extremely fast due to a closed-form 
solution for CS recovery being derived; 
(ii) certain reconstruction accuracy is preserved because significant 
components of x can be reconstructed with higher priority via an 
elaborately designed \phi; and 
(iii) in addition to conventional 1D sensing, we also study 2D 
separate sensing to enable simultaneous acquisition and compression of 
large-sized images.
(英文) In this project, we study a new image sensing technique, called compression image 
sensing (CIS), with computational complexity O(m^2), where m denotes the length of a 
measurement vector y=\phi x that is sampled from the signal x of length n via the 
sampling matrix \phi with dimensionality mXn. 
In order to balance between reconstruction quality and recovery speed, we propose a 
novel sampling matrix and study different kinds of sensing. 
The contributions of this project include: 
(i) reconstruction speed is extremely fast due to a closed-form solution for CS recovery 
being derived; 
(ii) certain reconstruction accuracy is preserved because significant components of x can 
be reconstructed with higher priority via an elaborately designed \phi; and 
(iii) in addition to conventional 1D sensing, we also study 2D separate sensing to enable 
simultaneous acquisition and compression of large-sized images.
產業別 其他專業、科學及技術服務業
技術/產品應用範圍
本研究成果適合用於低功率視訊元件之視訊壓縮，例如: 低階的行動視訊電話、數位像機
/攝影機、無線視訊監視器及無線視訊感測器等等相關產業。
技術移轉可行性及
預期效益
本研究具有以下特色: (i) 可以低複雜度 “直接” 擷取壓縮影像資料，而不必先擷取
完整原始資料再進行壓縮; (ii) 可於擷取壓縮影像資料時，同時達到影像加密效果; 
(iii) 具快速的影像資料重建; 及 (iv) 可應用於行動視訊元件及無線多媒體感測網路
之影像資料擷取 (例如: 視訊行動電話、行動數位像機/攝影機、無線低功率視訊監控網
路、無線視訊感測網路、行動視訊會議及用完即丟棄的數位相機/攝影機)。
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
Sung-Hsien Hsieh and Chun-Shien Lu, ` `Fast Compressed Sensing Recovery 
Via Iteratively 
Refining L2-Norm Solution,’’ 24th CVGIP conference, Taiwan, R.O.C., 
2011 (佳作論文獎)。 
 
Chun-Shien Lu, Hung-Wei Chen, and Soo-Chang Pei, ``Compressive Image 
Sensing: Turbo 
Fast Recovery With Lower-Frequency Measurement Sampling,’’ 24th CVGIP 
conference, Taiwan, 
R.O.C., 2011 (佳作論文獎)。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
