Histogram-based Interest Point Detectors
Wei-Ting Lee and Hwann-Tzong Chen
Department of Computer Science, National Tsing Hua University
30013 Hsinchu, Taiwan
Abstract
We present a new method for detecting interest points
using histogram information. Unlike existing interest point
detectors, which measure pixel-wise differences in image
intensity, our detectors incorporate histogram-based rep-
resentations, and thus can find image regions that present
a distinct distribution in the neighborhood. The proposed
detectors are able to capture large-scale structures and dis-
tinctive textured patterns, and exhibit strong invariance to
rotation, illumination variation, and blur. The experimen-
tal results show that the proposed histogram-based inter-
est point detectors perform particularly well for the tasks
of matching textured scenes under blur and illumination
changes, in terms of repeatability and distinctiveness. An
extension of our method to space-time interest point detec-
tion for action classification is also presented.
1. Introduction
Detecting distinctive invariant low-level features in im-
ages is a fundamental aspect of many computer vision tasks.
The effectiveness of existing local feature detectors has
been well demonstrated through varied vision applications,
e.g., [8], [4], [16], [23], [27]. Thorough experimental com-
parisons and performance evaluations on popular interest-
point and local-feature detectors are also available in the lit-
erature of computer vision [21], [24]. More recently, Tuyte-
laars and Mikolajczyk have presented an overview of local
invariant feature detectors [28]. These evaluations and liter-
ature surveys provide a systematic way to gain insight into
the characteristics of widely used feature detection meth-
ods. It has been shown that interest point detectors such as
the Harris corner detector [11] and the Hessian-based inter-
est point detectors [2], [15] are important building blocks of
various local invariant feature detectors. For example, the
Harris-Affine and Hessian-Affine detectors [18], [19], [21]
are based on affine normalization around Harris and Hessian
points; the SURF detector [1] relies on the determinant of
the Hessian matrix for selecting the location and the scale;
the SIFT detector [17] eliminates unstable edge responses
by analyzing the Hessian matrix of the intensity surface.
The Harris corner detector [11] is considered to be one of
the most reliable interest point detectors [24]. It is popular
owing to its robustness to rotation, illumination variation,
and image noise. Briefly, the Harris detector uses the sec-
ond moment matrix, also called the auto-correlation matrix,
to explore the local statistics of image intensity variations,
with patches shifted by a small amount in different direc-
tions. The locations of interest points can be identified by
analyzing the trace and the determinant of the second mo-
ment matrix, derived from first-order derivatives of image
intensity function. The second moment matrix can be di-
rectly extended to RGB color space by combining the three
color channels [22]. The underlying idea of the Hessian-
based detectors is similar to the Harris corner detector. The
Hessian detector explores the second-order Taylor expan-
sion of the (Gaussian convolved) intensity surface, and the
resulting Hessian matrix that consists of the second-order
derivatives describes the local image structures. Similarly,
the trace and the determinant of the Hessian matrix can also
be used to decide the interest points. More detailed discus-
sions on interest point detectors and local feature detectors
can be found in [21], [24], [28].
This paper presents a new approach to the detection of
interest points using histogram information. The proposed
detectors are able to identify interest points that exhibit a
distinctive distribution of low-level features in a local area.
Whereas histogram-based representations have been widely
used by the feature descriptors, e.g., HOG [6], SIFT [17],
GLOH [20], existing interest point detectors simply use
pixel-based (intensity or color) representations to charac-
terize local features. For images consisting of highly tex-
tured objects such as brick walls or trees, using pixel-based
information to detect interest points may yield too many re-
sponses of less stable corners—Shifting a textured patch
by a small amount may cause a significant increase in the
sum of squared differences, even though the variations in
the distributions of texture patterns should be insignificant.
Moreover, since popular descriptors for matching or object
recognition are built from histograms of low-level features,
the feature descriptions extracted from the locations of Har-
and
mY =
∑
(xi,yi)
∈Ω(x,y)
yi w(xi − x, yi − y)1{b(xi,yi)=k} . (9)
Matrix H(x, y) captures the histogram structure of the
local neighborhood around pixel (x, y). If the absolute val-
ues of both eigenvalues of H(x, y) are large, then a shift
(∆x,∆y) in any direction will result in a significant drop
of the Bhattacharyya coefficient, and therefore, the his-
togram h(x, y) of some low-level image features around
(x, y) should be quite dissimilar to the histograms around
neighboring pixels (x +∆x, y +∆y). We consider such a
pixel to be an interest point. The problem of identifying in-
terest points can be handled through observing the eigenval-
ues of the Hessian matrix corresponding to the local Bhat-
tacharyya coefficient. As shown by Harris and Stephens for
the Harris corner detector [11], we also do not need to com-
pute the eigenvalues explicitly. The absolute values of the
eigenvalues for Hessian matrix H(x, y) can be modeled by
a response function R on the determinant and the trace:
R(H) = det(H)− κ trace2(H) , (10)
where we use κ = 0.1 for the experiments presented in this
paper. If a Hessian matrix H has a high response, it is more
likely that its both eigenvalues are of large absolute values.
The response function is used to decide whether a pixel is
an interest point. Non-maximum suppression is applied to
the responses of all pixels, and local maxima are selected as
nominated interest points.
3. Extracting Local Invariant Regions for
Matching
We describe in this section how to apply our interest
point detector to the matching tasks that rely on the de-
tection of local invariant regions. We present two possible
choices of histogram-based representations for our method,
and discuss the process of selecting the scale.
3.1. Histogram-based Image Representations
We may represent an image patch by a histogram of low-
level image features. Different types of histogram repre-
sentations can be incorporated into our method to build the
histogram-based interest point detectors. Described below
are two types of histograms that are tested in our experi-
ments.
Color Histogram. Color histograms are commonly used
in object tracking and image retrieval as the image repre-
sentation. We employ the color representation proposed
by Comaniciu et al. [5] to describe local image structures.
We quantize each color channel (256 levels assumed) in
RGB color space into 8 bins, and obtain a histogram with
8×8×8 = 512 bins. The quantization function is given by
b(x, y) = ⌊Rx,y/32⌋×82+⌊Gx,y/32⌋×8+⌊Bx,y/32⌋+1,
where Rx,y , Gx,y , and Bx,y are the RGB values of pixel
(x, y). By plugging the function b into the weighted his-
togram in (1), as well as mX and mY in (8) and (9), we
obtain the Hessian matrix H(x, y) in (4) for identifying in-
terest points.
Oriented Gradient Histogram. Intensity gradients can
also be used as the low-level features for constructing his-
tograms. We quantize the orientation of gradient into 8 bins,
each of which covers a 45-degree angle. The magnitude of
gradient is also divided into 8 bins, and thus the resulting
histogram contains 8 × 8 = 64 bins. Because the magni-
tude of gradient might provide useful information for de-
scribing local regions, the formulation of histogram in (1)
can be modified to include the magnitude of gradient:
hk(x, y)
=
1
Z
∑
(xi,yi)
∈Ω(x,y)
w(xi − x, yi − y) ‖g(xi, yi)‖α 1{b(xi,yi)=k} ,
(11)
where ‖g(xi, yi)‖ is the magnitude of gradient at pixel
(xi, yi), and α is a scaling parameter. The equations of mX
and mY in (8) and (9) also need to be modified correspond-
ingly to get the Hessian matrix.
3.2. Scale Selection for Detecting Local Invariant
Features
Scale issue aside, the proposed histogram-based detec-
tion algorithm can be used as a stand-alone interest point de-
tector. Nevertheless, for vision applications such as match-
ing or object recognition, it is critical to find local features
that are invariant to scale changes. We may explore the scale
space by taking account of σ of the weighted histogram in
(1). A small variation ∆σ can be added to σ, and we get an
equation of the Bhattacharyya coefficient subject to σ and
σ+∆σ, similar to the formulation in (2). In our implemen-
tation we use a more straightforward approach by detecting
interest points at each given scale based on the response
R(H) in (10). In addition to the scale, the ‘shape’ of the
local region can also be determined by analyzing the Hes-
sian matrix H . The estimation of scale and shape helps to
extract the feature descriptions more faithfully for match-
ing. We show in next section that the local invariant regions
selected by our method are effective in matching scenes un-
der blur and illumination variations. Fig. 1 illustrates some
examples of detected interest points using color histograms
and oriented gradient histograms.
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
bark
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
boat
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
graf
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
ubc
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
bark
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
boat
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
graf
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
ubc
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
(a) (b) (c) (d)
Figure 2. Repeatability and matching score. (a) Scale change for the textured scene. (b) Scale change for the structured scene. (c) Viewpoint
change. (d) JPEG compression. Our detectors do not perform very well on these data sets, in comparison with the best results of other
popular detectors presented in [21].
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
bikes
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
leuven
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
trees
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
wall
re
pe
at
ab
ilit
y 
%
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
bikes
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
leuven
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
trees
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 6.5
0
10
20
30
40
50
60
70
80
90
100
wall
m
a
tc
hi
ng
 s
co
re
 
 
color
gradient
haraff
hesaff
MSER
(a) (b) (c) (d)
Figure 3. Repeatability and matching score. (a) Blur for the structured scene. (b) Illumination change. (c) Blur for the textured scene. (d)
Viewpoint change for the textured scene. Our detectors perform very well on these data sets, in comparison with the best results reported
in [21].
4.2. Action Classification with Histogram-based In-
terest Point Detectors in Space-Time
In this experiment, we show how to extend our
histogram-based method to the detection of space-time in-
terest points, and apply the detector to the problem of ac-
tion classification. Similar ideas have been explored in [14],
[25], [26] for event and action analysis.
Detecting Space-Time Interest Points. In this section
we show how to use our method to detect space-time inter-
est points in a video. We employ the human action database
provided by [10]. The database contains ten kinds of ac-
tions, and for each action we use nine videos of the same
action performed by different people. We ignore the silhou-
ette information since our method does not assume a known
background. We choose the 64-bin oriented gradient his-
togram to perform our detector, and use a 3D Gaussian with
σ = 2 for computing the weighted histogram. Given a video
p(y + δy) and q can be approximated by the second-order Taylor
expansion
ρ =
L∑
k=1
√
pk(y + δy) · qk (12)
≈
L∑
k=1
√
pk(y)qk +
1
2
L∑
k=1
∇pk(y)δy
√
qk
pk(y)
+
1
2
δy
T
H(y) δy (13)
where H(y) is the Hessian matrix of ρ at the location y. We may
replace ∇pk(y)δy by (pk(y + δy)− pk(y)) and obtain
ρ =
L∑
k=1
√
pk(y + δy) · qk
≈ 1
2
L∑
k=1
√
pk(y)qk +
1
2
L∑
k=1
pk(y + δy)
√
qk
pk(y)
+
1
2
δy
T
H(y) δy . (14)
Let q = p(y), and the auto-similarity based on the Bhattacharyya
coefficient is given by
ρ˜ =
L∑
k=1
√
pk(y + δy) · pk(y)
≈ 1
2
L∑
k=1
√
pk(y)pk(y) +
1
2
L∑
k=1
pk(y + δy)
√
pk(y)
pk(y)
+
1
2
δy
T
H(y) δy . (15)
Since p(y) is a probability distribution, we have
∑L
k=1
pk(y) =∑L
k=1
pk(y + δy) = 1, and it follows that
ρ˜ ≈ 1 + 1
2
δy
T
H(y) δy . (16)
References
[1] H. Bay, T. Tuytelaars, and L. J. V. Gool. Surf: Speeded up
robust features. In ECCV (1), pages 404–417, 2006.
[2] P. Beaudet. Rotationally invariant image operators. In 4th
Int. Joint Conf. Patt. Recog., pages 579–583, 1978.
[3] A. Bhattacharyya. On a measure of divergence between two
statistical populations defined by their probability distribu-
tions. Bull. Calcutta Math. Soc., 35:99–110, 1943.
[4] M. Brown and D. G. Lowe. Recognising panoramas. In
ICCV, pages 1218–1227, 2003.
[5] D. Comaniciu, V. Ramesh, and P. Meer. Real-time tracking
of non-rigid objects using mean shift. In CVPR (2), pages
142–149, 2000.
[6] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection. In CVPR (1), pages 886–893, 2005.
[7] G. Dorko´ and C. Schmid. Maximally stable local description
for scale selection. In ECCV (4), pages 504–516, 2006.
[8] R. Fergus, F.-F. Li, P. Perona, and A. Zisserman. Learn-
ing object categories from google’s image search. In ICCV,
pages 1816–1823, 2005.
[9] B. J. Frey and D. Dueck. Clustering by passing messages
between data points. Science, 315:972–976, 2007.
[10] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri.
Actions as space-time shapes. IEEE Trans. Pattern Anal.
Mach. Intell., 29(12):2247–2253, 2007.
[11] C. Harris and M. Stephens. A combined corner and edge
detection. IEEE Trans. Pattern Anal. Mach. Intell., pages
147–151, 1988.
[12] T. Kadir and M. Brady. Saliency, scale and image descrip-
tion. International Journal of Computer Vision, 45(2):83–
105, 2001.
[13] T. Kadir, A. Zisserman, and M. Brady. An affine invariant
salient region detector. In ECCV (1), pages 228–241, 2004.
[14] I. Laptev and T. Lindeberg. Space-time interest points. In
ICCV, pages 432–439, 2003.
[15] T. Lindeberg. Feature detection with automatic scale selec-
tion. International Journal of Computer Vision, 30(2):79–
116, 1998.
[16] D. G. Lowe. Object recognition from local scale-invariant
features. In ICCV, pages 1150–1157, 1999.
[17] D. G. Lowe. Distinctive image features from scale-invariant
keypoints. International Journal of Computer Vision,
60(2):91–110, 2004.
[18] K. Mikolajczyk and C. Schmid. An affine invariant interest
point detector. In ECCV (1), pages 128–142, 2002.
[19] K. Mikolajczyk and C. Schmid. Scale & affine invariant in-
terest point detectors. International Journal of Computer Vi-
sion, 60(1):63–86, 2004.
[20] K. Mikolajczyk and C. Schmid. A performance evaluation
of local descriptors. IEEE Trans. Pattern Anal. Mach. Intell.,
27(10):1615–1630, 2005.
[21] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman,
J. Matas, F. Schaffalitzky, T. Kadir, and L. J. V. Gool. A
comparison of affine region detectors. International Journal
of Computer Vision, 65(1-2):43–72, 2005.
[22] P. Montesinos, V. Gouet, R. Deriche, and D. Pele´. Matching
color uncalibrated images using differential invariants. Im-
age Vision Comput., 18(9):659–671, 2000.
[23] C. Schmid and R. Mohr. Local grayvalue invariants for
image retrieval. IEEE Trans. Pattern Anal. Mach. Intell.,
19(5):530–535, 1997.
[24] C. Schmid, R. Mohr, and C. Bauckhage. Evaluation of in-
terest point detectors. International Journal of Computer Vi-
sion, 37(2):151–172, 2000.
[25] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift de-
scriptor and its application to action recognition. In ACM
Multimedia, pages 357–360, 2007.
[26] E. Shechtman and M. Irani. Space-time behavior based cor-
relation. In CVPR (1), pages 405–412, 2005.
[27] J. Sivic, F. Schaffalitzky, and A. Zisserman. Object level
grouping for video shots. In ECCV (2), pages 85–98, 2004.
[28] T. Tuytelaars and K. Mikolajczyk. Local invariant feature
detectors: A survey. Foundations and Trends in Computer
Graphics and Vision, 3(3):177–280, 2007.
Figure 1. Left: A landscape photo with casual composition. Mid-
dle: A photograph taken by professional photographer William
Neill. (From Landscapes of the Spirit, available on Flickr
http://www.flickr.com/photos/williamneill) Right: Our method
achieves good composition by imitating the artwork of William
Neill. The composition presents the same placement of the hori-
zon line and a similar layering effect as in the middle image.
To model the composition styles used in professional
photography, we need an image representation that is able
to capture the structural features and the spatial layout of
salient content. With an appropriate image representation
we could obtain a similarity measure to compare images
based on the resemblance in their composition. In this work,
we consider the low-dimensional global image represen-
tation derived from the GIST descriptor, which has been
shown to be a good image representation for scene classifi-
cation and scene matching [13], [17], as well as for depth
estimation [19]. The GIST descriptor computes oriented
edge filter responses at different scales aggregated into spa-
tial bins of locations. The arrangement of structures within
an image can be characterized by the GIST descriptor, and
thus we can measure the similarity between the geometric
patterns and scene structures presented in two images by
comparing their GIST descriptors.
Besides the scene structures, we also wish to model the
visual saliency in an image, since the placement of salient
elements in the photo is also an important compositional is-
sue. The layout of the salient elements needs to draw the
eye into the photo and also has to be well balanced and
pleasing to the eye. We try to assess the layout of salient
elements by adopting the techniques developed for saliency
detection and visual attention, which have long been stud-
ied in computer vision, e.g., [9], [10], [14]. We combine the
information provided by the GIST descriptor and the result
of saliency detection, and use the coupled representation to
model the composition of photos. The effectiveness of inte-
grating gist and saliency has also been addressed by Siagian
and Itti in their model of human vision for scene classifica-
tion [16].
Our work is also related to the image editing methods for
image warping, scene completion, and retargeting/resizing,
e.g., [1], [5], [7], [8], [20], which can be considered as al-
ternative ways of modifying the photo composition. Gal
et al. [7] present a method for feature-aware texture warp-
ing. Their method is able to retain the shape of the regions
of foreground objects while changing the aspect ratio of
the image. For the application of modifying the compo-
sition of images, their method may be used to adjust the
relative scales of different parts in the image. Avidan and
Shamir [1] present the seam carving algorithm for image
retargeting. The algorithm aims to reduce the image size
by removing less important seams of pixels from the origi-
nal image such that the resized image preserves most of the
perceptually significant parts. The patch transform algo-
rithm presented by Cho et al. [5] divides an image into non-
overlapping patches, and reorganizes the patches to form a
new image, subject to user-specified constraints such as the
spatial locations of patches. The patch transform can be ap-
plied to various image editing tasks, e.g., image retargeting,
or changing the location of foreground object. Hays and
Efros [8] present a scene completion algorithm that, given
an input image, looks for similar scenes in two million im-
ages using the GIST descriptor, and then fills the holes in
the input image with good patches extracted from the se-
lected similar scenes. Their algorithm can be used to pro-
duce contextually valid and visually pleasing composite im-
ages. The main limitation of applying these image editing
methods to automatic photo composition is that the methods
would tinker with the image content and inevitably change
the real structures within the image. Moreover, the image
editing methods still require suitable image representations
and evaluation criteria for characterizing good composition.
Deselaers et al. [6] present the pan-zoom-scan method
for automatic video cropping. Their method does not
change the scene structures in images, and can find the
best cropped viewing area for each image in a video se-
quence through panning and zooming. Santella et al. [15]
describe an interactive method for cropping photos based
on the information provided by eye tracking. Through user
studies they show that their gaze-guided method performs
better than fully-automatic cropping approach such as [18].
Unlike our goal of composition, the aforementioned meth-
ods focus on modeling visual saliency and attention in im-
ages, but do not take into account more complex compo-
sitional components. Lalonde et al. [12] propose the use
of a physically-based sky model to analyze the information
available in the visible sky. The model can be applied to
the segmentation of the sky and cloud layers, and the bi-
layered representation for sky and clouds is useful for data-
driven sky matching. For landscape and outdoor photog-
raphy, the bi-layered representation might provide helpful
hints to modify visual balance in the composition.
2. Formulation of the Search Problem
For convenience sake, instead of using an active PTZ
camera to take pictures directly, we simply try to simu-
late the process of observing a panoramic scene through a
viewfinder and searching for good views at different zooms.
Nevertheless, the approach presented in this paper for the
virtual environment can be easily adapted to real control-
(a) (b) (c)
Figure 3. (a) The 6 × 4 spatial bins used to compute the GIST
descriptor. (b) & (c) The saliency maps at two scales 32 and 64,
also divided into 6× 4 spatial bins.
Finally, we represent an image I using the GIST descrip-
tor G(I) and the saliency descriptor S(I), and the dissimi-
larity in composition between two images can be measured
by
ρ(I, I ′) =
∥∥G(I)−G(I ′)∥∥ + λ∥∥S(I)− S(I ′)∥∥ , (2)
where λ is a weighting parameter to determine the signif-
icance of the two properties in comparing photo composi-
tion: i) the structural and geometric patterns in the scene
described by GIST, or ii) the layout of visual saliency rep-
resented by the saliency descriptor. We use λ = 0.2 in the
experiments.
4. Algorithm
The neighborhood graph of exemplars. The search
problem in (1) needs to find the most suitable exemplar
for taking a picture within the panorama. It would be im-
practical to examine the entire set of exemplars if we hope
to achieve an active searching performance. To make the
search feasible, we pre-process the exemplar set and build a
neighborhood graph on the exemplars. This graph structure
helps the algorithm to explore locally in the set of exem-
plars for better candidates. For each exemplar E in the set
E , we find the k nearest neighbors of E according to the
dissimilarity measure ρ(E, ·), and connect them to E. We
use an exemplar set containing more than a hundred pho-
tos. We construct the neighborhood graph with k = 4, and
have found it to be effective for the problem. Fig. 4 shows
the dissimilarity matrix and the neighborhood graph of the
exemplars used in our experiments.
The stochastic search algorithm. Recall that the config-
uration of the viewfinder is defined by the position of the
viewing center, z, and the size of the viewing area, s. We
can use the configuration to extract the image I(P ; z, s)
of the corresponding view within the panorama, and then
compare the extracted image with a chosen exemplar E by
ρ(I(P ; z, s), E). Instead of finding the optimal solution
to the problem in (1), we present a stochastic search algo-
rithm to find approximate solutions as recommendations for
shooting the scene. At the end of the search, we expect the
(a) (b)
Figure 4. (a) The dissimilarity matrix of the exemplars E used in
our experiments. (b) The neighborhood graph of E .
algorithm to suggest a candidate view that would yield good
photo composition. The search algorithm is summarized as
follows.
• Input: A panorama P and the neighborhood graph built
from the exemplar set E . A starting position of the center of
view, z0 = [x0 y0]T . The initial size of the view, w0 × h0.
• Initialization: Let s0 denote the scale parameter as s0 =
[w0 h0]
T
. We randomly choose (⌊log2 |E|⌋ + 1) exem-
plars from E , and pick among them the exemplar E0 that
has the smallest dissimilarity to the initial view I(P ; z0, s0).
The following steps are repeated for the initial configuration
{z0, s0, E0}.
• For t = 1, . . . , T :
1. Find a better exemplar in the neighborhood graph.
Et = argmin
E′∈Ω
ρ(I(P ; zt−1, st−1), E
′) , (3)
where Ω = {Et−1} ∪ N (Et−1) is the union of Et−1
and its neighbors.
2. Update the size of view. Re-scale the size of the view
by±5% of the current size and compute the dissimilar-
ity between the re-scaled view and the exemplar Et. If
the dissimilarities are not improved, then keep the cur-
rent view size; otherwise update the scale parameter st
of the view according to the new size.
3. Find a better position of the view. Generate M ran-
dom vectors, {v(m)}Mm=1 (typically M = 8), from a
two-dimensional normal distribution N(0, I) with zero
mean and identity covariance matrix. Solve
v∗ = argmin
v′∈{v(m)}∪ {0}
ρ (I(P ; zt−1 + Av
′, st), Et) ,
(4)
where A is diagonal scaling matrix for controlling the
step size of a move. Update the position by zt ←
zt−1 +Av
∗
.
If the position zt does not change during the previous τ
consecutive iterations, then do the next step for greedy
local search; otherwise, continue the iteration.
(a) First session
(b) Second session
Figure 6. User study. (a) The first session of the assessment: A
random pair of images corresponding to the initial view and the
suggested view are both presented to the viewer. The positions of
the two images may be switched at random. The viewer is asked to
determine which image has better photo composition. The viewer
needs to vote for ‘left’ or ‘right’, or click the button ‘the same’
if it is hard to judge which one is better. Each viewer is required
to evaluate 100 pairs of images. (b) The second session of the
assessment: Similar to the first session, but this time we also show
the viewer the model image (the selected exemplar), and ask the
viewer which image looks more like the model image. The viewer
can answer ‘left’, ‘right’, or ‘the same’.
0 2 4 6 8 10 120
10
20
30
40
50
60
Viewer
V
ot
es
 
 
initial view
suggest view
the same
0 2 4 6 8 10 120
10
20
30
40
50
60
70
Viewer
V
ot
es
 
 
initial view
suggest view
the same
(a) First session (b) Second session
Figure 7. The vote distributions of the 12 viewers.
Acknowledgments. This research was supported in part
by NSC grant 96-2221-E-007-132-MY2 and NTHU grant
98N2935E1. We thank the artists for giving us permission
to use their photographs in our paper.
References
[1] S. Avidan and A. Shamir. Seam carving for content-aware
image resizing. ACM Trans. Graph., 26(3):10, 2007.
[2] S. Bae, S. Paris, and F. Durand. Two-scale tone management
for photographic look. ACM Trans. Graph., 25(3):637–645,
2006.
[3] S. Banerjee and B. L. Evans. In-camera automation of pho-
tographic composition rules. IEEE Transactions on Image
Processing, 16(7):1807–1820, 2007.
[4] Z. Byers, M. Dixion, W. D. Smart, and C. M. Grimm. Say
cheese! experiences with a robot photographer. AI Mag.,
25(3):37–46, 2004.
[5] T. S. Cho, M. Butman, S. Avidan, and W. T. Freeman. The
patch transform and its applications to image editing. In
IEEE Conference on Computer Vision and Pattern Recog-
nition, 2008.
[6] T. Deselaers, P. Dreuw, and H. Ney. Pan, zoom, scan: Time-
coherent, trained automatic video cropping. In CVPR, 2008.
[7] R. Gal, O. Sorkine, and D. Cohen-Or. Feature-aware textur-
ing. In Proceedings of Eurographics Symposium on Render-
ing, pages 297–303, 2006.
[8] J. Hays and A. A. Efros. Scene completion using millions of
photographs. ACM Trans. Graph., 26(3):4, 2007.
[9] X. Hou and L. Zhang. Saliency detection: A spectral residual
approach. In CVPR, 2007.
[10] L. Itti, C. Koch, and E. Niebur. A model of saliency-based
visual attention for rapid scene analysis. IEEE Trans. Pattern
Anal. Mach. Intell., 20(11):1254–1259, 1998.
[11] J. Kinghorn and J. Dickman. Perfect Digital Photography.
McGraw-Hill/Osborne, 2005.
[12] J.-F. Lalonde, S. G. Narasimhan, and A. A. Efros. What does
the sky tell us about the camera? In ECCV, 2008.
[13] A. Oliva and A. B. Torralba. Modeling the shape of the
scene: A holistic representation of the spatial envelope.
International Journal of Computer Vision, 42(3):145–175,
2001.
[14] A. Oliva and A. B. Torralba. Contextual guidance of eye
movements and attention in real-world scenes: The role of
global features on object search. Psychological Review,
113(4):766–786, 2006.
[15] A. Santella, M. Agrawala, D. DeCarlo, D. Salesin, and M. F.
Cohen. Gaze-based interaction for semi-automatic photo
cropping. In CHI, pages 771–780, 2006.
[16] C. Siagian and L. Itti. Rapid biologically-inspired scene clas-
sification using features shared with visual attention. IEEE
Trans. Pattern Anal. Mach. Intell., 29(2):300–312, 2007.
[17] J. Sivic, B. Kaneva, A. Torralba, S. Avidan, and W. T. Free-
man. Creating and exploring a large photorealistic virtual
space. In First IEEE Workshop on Internet Vision, 2008.
[18] B. Suh, H. Ling, B. B. Bederson, and D. W. Jacobs. Au-
tomatic thumbnail cropping and its effectiveness. In UIST,
pages 95–104, 2003.
[19] A. B. Torralba and A. Oliva. Depth estimation from im-
age structure. IEEE Trans. Pattern Anal. Mach. Intell.,
24(9):1226–1238, 2002.
[20] L. Wolf, M. Guttmann, and D. Cohen Or. Non-homogeneous
content-driven video-retargeting. In ICCV, 2007.
1□赴國外出差或研習
□赴大陸地區出差或研習
■出席國際學術會議
□國際合作研究計畫出國
心得報告
計 畫 名 稱電腦視覺相關演算法在影像
與視訊瀏覽支應用
計 畫 編 號NSC
96-2221-E-007-132-MY2
報 告 人
姓 名
陳煥宗
服 務 機 構
及 職 稱
清華大學資訊工程系
助理教授
會議/訪問時間
地點
June 20 - June 25, 2009
Miami, USA
會 議 名 稱 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR 2009)
發表論文題目Histogram-based Interest Point Detectors
Report of CVPR 2009
The full name of the conference is IEEE Computer Society Conference on Computer Vision and Pattern
Recognition. It is held in Miami this year. I attended the workshops (June 20-21) and the main conference (June
22-24). This report summarizes some of the interesting talks and papers that might be worth further study.
---
June 20
---
Workshop on Egocentric Vision
Keynote session I: “First-Person, Inside-Out Vision”
The keynote speaker is Prof. Takeo Kanade from Carnegie Mellon University. In the beginning of his talk he
introduces the Quality of Life Technology (QoLT) Center at CMU. He talks about first-person inside-out vision
design for indoor localization. He emphasizes the importance of human involvement in the computer vision
systems. Egocentric vision is more than eye-tracking and life-log. Humans should be able to help the computer
vision systems, such as directly access to the visual field, or gaze information for total-situation understanding.
Prof. Kanade shows some videos of real-time stereo, k-d vision (see through the wall), and human-system
symbiosis.
“Egocentric Recognition of Handled Objects: Benchmark and Analysis”
The speaker is Xiaofeng Ren. He presents a database of ten sequences captured b a high-quality wearable video
camera. Each sequence includes 42 day-to-day objects handled in typical manners. Totally 100,000
high-resolution color frames are recorded. A SIFT+SVM recognition system is used as a baseline algorithm for
the problem. The experimental results show that the location prior and temporal consistency/smoothing are
helpful to improve the performance.
“Temporal Segmentation and Activity Classification from First-Person Sensing”
3images of some object category and find representative example images for that object category. This work
involves detecting salient objects in images, predicting saliency, and eliminating junk. The authors use
k-mediods clustering and choose the mediod images of each cluster as the iconic images. The relevance of the
chosen iconic images is evaluated using Amazon’s Mechanical Turk, which is quite popular recently in 
computer vision community for collecting ground-truth data or for evaluation. This work gives me an idea that,
when we try to tackle VOC 2009, we may divide the test images into ‘simple’ cases and ‘hard’ cases, and use 
different algorithms to solve different cases.
“Towards Automated Large Scale Discovery of Image Families”
The authors gather 11,000 images with over 6,300 families. They use Normalized Cuts, Agglomerative
Clustering, and Connected Component Labeling for finding families and clusters. They evaluate different global
features such as SIFT descriptor for the whole image, Gist, HOG, and BoW. They also try local features, such as
RANSAC affine and region affine features. Local features are matched by randomized kd-trees. The
experimental results show that i) larger BoW models are better, ii) TF-IDF improves a lot, iii) Agglomerative
Clustering is better than Normalized Cuts, iv) local features perform better than global features.
Another invited talk of this workshop is given by Prof. Fei-Fei Li from Stanford University. She introduces the
website of ImageNet (http://www.image-net.org/). ImageNet is an image database organized according to the
WordNet hierarchy. It contains more than 30K visual synsets, and each synset includes more than 1,000 images.
The database may be useful for non-parametric object recognition. I think we can use it for VOC 2009
competition.
---
Workshop on Visual Place Categorization
This workshop has a website http://categorizingplaces.com/ containing presentation slides and datasets. I only
attend three talks of this workshop, but I find that the talk of Alyosha Efros in the afternoon seems interesting.
The PDF file of his presentation slides is available online. His talk provides a lot of useful information. The
take-home messages shown in his presentation slides: i) categorization is not a goal in itself; ii) skipping explicit
categorization might make things easier, not harder; ii) keeping around al your data isn’t so bad—you never
know when you will need it.
“What should we mean by ‘scene’?”
The talk is given by David Forsyth. He talks about the concept of “scenes as object bags”. Scenes are not 
described by names but by properties and attributes.
“Models for Joint Labeling of Objects and Scenes”
The speaker is Bernt Schiele. The presentation slides are available on the website. The main idea is use dynamic
conditional random fields for joint object and scene labeling.
---
5“Bundling Features for Large Scale Partial-Duplicate Web”
The presenter is Qifa Ke. This approach tries to add geometric constraints among features. A bundled feature is
built from SIFT features enclosed within an MSER region. Consistency in geometric order is used for matching
two bundled features. This approach might be helpful for finding familiar objects in images.
---
June 22, Morning Poster Session
“SIFT-Rank: Ordinal Description for Invariant Feature Correspondence”
The method is very simple: sort the SIFT descriptor and compare. The experimental results show that SIFT-Rank
outperforms SIFT on scene matching tasks. I think this approach can be used to replace heuristic criteria, e.g.
thresholding, as an additional step for verifying the correspondence.
.
“Recognizing Indoor Scenes”
The descriptors are comprised of a Gist descriptor for the whole image and a spatial pyramid of visual words for
ROI. The ROI information is collected by manually segmented images. I think we may try the pyramid of visual
words technique for tasks similar to “Video Google”.
“Constrained clustering via spectral Regularization”
The idea is to incorporate the pairwise constraints in the embedding space rather than in the similarity matrix.
The optimization is solved by semidefinite programming.
---
June 22, Afternoon Session: Image Enhancement & restoration
“High Dynamic Range Image Reconstruction from Handheld Cameras”
The problem addressed in this paper can be considered as “deblurring images of diferent exposures”.  In 
addition to the response curve, the blur kernel regarding camera shake also needs to be estimated. Much
attention has been given to similar topics of computational photography. I would also be interesting to solve for
other parameters and unknowns, e.g. depth, super-resolution, and color spectrum.
“Polarization: Beneficial for Visibility Enhancement?”
The conclusion made by the authors: “… if the only goal is signal discrimination over noise (and not color or
radiance recovery) in haze, the use of polarization in both approaches is unnecessary: polarization rarely
improves the SNR over an average of unpolarized images acquired under the same acquisition time.
Nevertheless, under a single frame constraint, the use of a single polarized image is beneficial.” The paper is a 
bit complex (containing 50 equations); it would need further reading to understand in what situation these
conclusions and claims apply.
7“Object Detection using a Max-Margin Hough Transform”
This is another paper that uses the Hough voting technique. This paper is also from Prof. Malik’s lab at UC 
Berkeley.
---
June 23 Morning Poster Session
I found the following papers quite interesting and worth further studying:
“Tour the World: Building a Web-scale Landmark Recognition Engine”
This is a project of Google. They gave a demo on Wednesday (June 24).
“Visibility Constraints on Features of 3D Objects”
“P-Brush: Continuous Valued MRFs with Mormed Pairwise Distributions for Image Segmentation”
---
June 23 Afternoon Session: Face Recognition
I attended the following three talks:
“Implicit Elastic Matching with Random Projections for Pose-Variant Face Recognition”
“Joint and Implicit Registration for Face Recognition”
“A Compressive Sensing Approach or Expression-Invariant Face Recognition”
---
June 23 Afternoon Poster Session
We presented our paper “Histogram-based Interest Point Detectors” in this session.
---
June 23 Paper Awards Session
The best paper of CVPR’09 is “Single Image Haze Removal Using Dark Channel Prior.” This work is done by 
Microsoft Research Asia.
Best paper – honorable mention: “Understanding and Evaluating Blind Deconvolution Algorithms.” This work 
is done by Levin, Weiss, Durand, and Freeman.
Best student paper: “Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment.” The presenter is 
Ce Liu. He just graduated from MIT. He joins Microsoft Research New England as a postdoc.
Best student paper– honorable mention: “A Tensor-Based Algorithm for High-Order Graph Matching”
---
June 23 Evening, PAMI TC Meeting
I attended the PAMI TC Meeting in the evening. After a debate, the PAMI-TC membership approved that “no 
