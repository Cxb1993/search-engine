 2
行政院國家科學委員會專題研究計畫成果報告 
 
計畫編號：NSC 95-2221-E-194-071 
執行期限：95 年 8 月 1 日至 96 年 7 月 31 日 
 
主持人：劉興民 國立中正大學資訊工程學系 damon@computer.org 
共同主持人： 
計畫參與人員：洪紹鑫、鄭靜怡、邱志明、魏台軍、江中竣、 
侯君霖、陳志彰、薛毓弘、陳星任、張智傑、李世昱、蔡景昇 
 
 
一、中文摘要 
 
電腦圖學技術的進步，促使著動畫、
遊戲、電影與科學模擬系統的蓬勃發展，
既逼真又擬人的畫面效果已不再是遙不可
及的事情了。然而，這每一幕幕逼真畫面
的背後隱含著無數的龐大計算。例如，光
影計算、衣物模擬、流體模擬、以及其他
的物理運算與非物理運算。如何快速地、
有效率地產生這些畫面則是目前重要的研
究發展趨勢。 
人們對於娛樂媒體的強烈需求、以及
硬體廠商的積極開發，帶領著圖形硬體效
能的無限起飛。同時，因為圖形硬體內含
的平行性架構特質，使得其效能以遠勝過
個人電腦的中央處理器，並且以相當快的
速度在發展著，幾乎每年就提昇了一倍的
運算能力 (相對於中央處理器每十八個月
提昇一倍)。不僅如此，圖形硬體上的高階
程式語言也逐漸成熟，因此能更方便且有
效率地使用圖形硬體之效能，對於以往大
量又耗時的圖形計算，似乎有了解決之道。 
因此圖形硬體不再只是個繪製畫面的
硬體元件，而是能解決一般性運算的高效
能平行式模組，近年來吸引著國內外學者
紛紛投入研究。本計劃的目的便是希望能
將現有的圖形計算方法應用至圖形硬體之
架構上，解決目前耗時的龐大計算量之問
題，來加快系統開發，並降低成本。 
 
關鍵詞：可程式的圖形硬體、電腦繪圖、平
行計算 
 
 
Abstract 
 
Advancement of computer graphics 
technology vigorously impels the 
development of cartoons, games, films and 
scientific simulation systems. The realistic 
and anthropomorphic pictures have already 
no longer been a thing out of reach. However, 
enormous calculating is responsible for these 
lifelike pictures in every scene. For example, 
shadow computing, cloth simulation, fluid 
simulation and other physics and non-physics 
calculations are essentially required. One of 
currently important trends is how to produce 
these pictures fast and efficiently. 
Strong demand for the amusement 
media as well as ambitious development 
from the hardware manufacturers leads the 
advance of graphics hardware. Meanwhile, 
due to the implicit parallel structure, graphics 
hardware makes beyond its efficiency 
surpassing the central processing unit of the 
personal computers. Graphics hardware is 
developing at a quite quick speed; nearly 
doubling its performance capacity each year. 
Moreover, the high-level programming 
language on the graphics hardware has also 
been gradually integrated, so that we 
presently are able to use the performance of 
graphics hardware wisely and efficiently. It 
seems more likely that there is a way to solve 
such large number of time-consuming 
graphics computations. 
Graphics hardware is nowadays no 
longer a hardware component that can render 
pictures, it can also challenge parallel 
techniques in the reference frame of finite 
element models. Gourret et al. [1] simulated 
a hand grasping an object using a finite 
element model for both hand and the object. 
O’Brien and Hodgins [6] used a finite 
element model to simulate elastic brittle 
objects producing impressive animations of 
fracture. 
 
GPU-Based Applications 
Several programming techniques and 
strategies of GPGPU (General Purpose 
computation on Graphics Processing Unit) 
were proposed [7, 8, 9] and lots of 
BPU-based particle system on both the 
simulation and rendering of the dynamical 
scene was proposed in [10]. They used 
fragment shader to implement the simulation 
and rendered up to one million particles in 
real-time. In Kipfer et al. [11], not only a 
versatile particle system was shown, but also 
an optimized Bitonic sorting routine was 
carried out. They minimized both the number 
of instructions to be executed in the fragment 
program and the number of texture 
operations in order to gain performance. 
More comprehensive developments of 
GPU-based applications were illustrated in 
Owens et al. [12]. 
  
四、研究方法 
 
Figure 1: System architecture flow chart with 
read-back render. 
 
System Architecture 
System architecture diagram is 
illustrated in Figure 1. The program begins 
with an initialization routine which reads a 
surface mesh model and creates a mass 
spring model. Once the mass spring model is 
created, we encapsulate the mass spring 
model into several textures, in order to store 
the model into graphics card memory that 
allows shader program to use easily and 
efficiently. 
After initialization, we compute the 
deformation of mass spring model in every 
discrete time-step which we call it 
integration phase. Integration phase is 
implemented in fragment shader. The 
fragment shader extracts mass spring model 
information by fetching texture data. During 
the integration phase, object deformation is 
simulated by considering both inner force 
and outer force. After inherent parallel 
computation, the position and velocity 
information of mass nodes are written back 
to textures for next time-step integration. In 
order to display the deformed model, 
read-back operation is required which fetches 
the mass nodes or vertices position data back 
to CPU side after each integration phase is 
completed. 
 
Integration Phase 
For every time-step, the position and 
velocity information of mass nodes must be 
recalculated to generate the object 
deformation. The way to invoke shader 
processing is to execute graphics pipeline via 
OpenGL commands. We draw a quadrilateral, 
with one-to-one mapping to position texture, 
in order to send each texel into fragment 
processor exactly once. Some unnecessary 
computing will occur if the number of mass 
nodes is not equal to the size of position 
texture. 
Due to the parallel structure of graphics 
hardware, the internal reaction of each mass 
node can be efficiently solved. However, 
since there is no data communication 
between pixels, each spring is calculated 
twice at every time-step. The new position 
and velocity of mass nodes are easily 
calculated when we have the information of 
internal force and external force. 
 
Rendering Phase 
In the simulation, the result of 
Integration Phase is written to textures and 
read-back to CPU for rendering purpose. It 
 4
frame rates of the deformable object 
simulation performed by two different GPUs 
are also separately listed in Table 2. The Intel 
Pentium 4 3.4GHz CPU is used to 
collaborate with these two graphics 
hardwares. The model rendering time is also 
estimated. Therefore, the frame rate here 
represents the overall system frame rate. 
 
 
Figure 3: Performance comparison of 
GPU-based deformable object simulation on 
different GPU processors. 
 
The frame rates comparison of these 
two graphics processors is illustrated more 
clearly in Figure 3. The frame rate also drops 
quickly as the size of mass nodes and springs 
gets bigger. However, most of the 
deformable objects can be simulated with an 
interactive or even real-time frame rate. 
Using the advanced GPU processors, the 
real-time deformable object simulation can 
be guarantied. 
 
CPU vs. GPU 
We compare the frame rates of 
CPU-based simulations with those of 
GPU-based simulations in this section, see 
Figure 4 and Figure 5. Figure 4 and Figure 5 
are illustrated according to the experimental 
results in Table 1 and Table 2. The only 
difference between Figure 4 and Figure 5 is 
the scale of frame rates. Figure 5 allows us to 
view the changes of frame rate in bigger-size 
models more easily. 
We have found that the throughput of 
deformable object simulation can be greatly 
enhanced using graphics hardware. With the 
parallel nature of graphics hardware, the 
mass nodes integration can be solved 
efficiently. The overall system frame rate can 
be greatly improved to be able to apply for 
any real-time system, which is unable to be 
achieved using CPU processor as the size of 
mass nodes and springs get bigger. 
According to this experiment, the 
performance of deformable object simulation 
can be improved at least ten times using 
advanced graphics hardwares. 
 
 
Figure 4: Performance comparison of 
CPU-based simulation and GPU-based 
simulation. 
 
 
Figure 5: Performance comparison of 
CPU-based simulation and GPU-based 
simulation (with different FPS scale). 
  
六、計畫成果自評 
 
In this work, we implemented a 
deformable object simulation on modern 
graphics hardware. With the aid of advancing 
graphics hardware, we have shown the 
performance of deformable object simulation 
can be enhanced or can even achieve 
real-time frame rates. Furthermore, basic 
GPU-based collision detection and user 
interactions will be added to the simulation 
system that the collision detection and user 
interaction is often the fundamental criteria 
of deformable simulation applications. 
We have selected the Mass Spring 
 6
vol.2. 
[3]  Choi, K.S., Sun, H., Heng, P.A., and Cheng, 
J.C.Y. 2002. A scalable force propagation 
approach for We-based deformable 
simulation of soft tissues. Proceedings of the 
7th International Conference on 3D Web 
Technology, pp. 185-193. 
[4]  Matyka, M. and Ollila, M. 2003. A pressure 
model for soft body simulation. Proceedings 
of SIGRAD, UMEA. 
[5]  Bro-Nielsen, M. 1998. Finite element 
modeling in surgery simulation. Proceedings 
of the IEEE, vol. 3, pp. 490-503. 
[6]  O’Brien, J.F. and Hodgins, J.K. 1999. 
Graphical modeling and animation of brittle 
fracture. Proceedings of SIGGRAPH, pp. 
137-146. 
[7]  Buck, I. 2004. GPU computation strategies 
and tricks. GPGPU: General-Purpose 
Computation on Graphics Hardware. 
[8]  Fernando, R., Harris, M., Wolka, M., and 
Zeller, C. 2004. Programming graphics 
hardware. Eurographics tutorial. 
[9]  Lefohn, A. 2005. GPU memory model 
overview. GPGPU: General-Purpose 
Computation on Graphics Hardware. 
[10]  Kolb, A. Latta, L., and Rezk-Salama, C. 
2004. Hardware-based simulation and 
collision detection for large particle systems. 
Proceedings of SIGGRAPH Conference on 
Graphics Hardware, pp. 123-131. 
[11]  Kipfer, P. Segal, M., and Westermann, R. 
2004. Uber-flow: a GPU-based particle 
engine. Proceedings of SIGGRAPH on 
Graphics Hardware, pp. 115-122. 
[12]  Owens, J.D., Luebke, D., Govindaraju, N, 
Harris, M., Kruger, J., Lefohn, A.E., and 
Purcell, T.J. 2005. A survey of general 
purpose computation on graphics hardware. 
Eurographics, State of the Art Reports, pp. 
21-51. 
[13]  Chang, L.F. and Liu, D.S.M. 2006. 
Deformable object simulation in virtual 
environment, Proceedings of ACM 
SIGGRAPH International Conference on 
Virtual Reality Continuum and Its 
Applications, Hong Kong, China, pp. 
327-330. 
[14]  Chang, L.F. and Liu, D.S.M. 2005. 
Real-time deformable object simulation using 
the GPU, Proceedings of the 2005 Computer 
Graphics Workshop, Taipei, Taiwan, p. 29. 
[15]  Chang, L.F. 2006. Real-time deformable 
object simulation using the GPU. Master 
thesis. Computer Science Dept, National 
Chung Cheng Univ, Chiayi, Taiwan. 
[16]  Chang, C.C. 2007. GPU-based BVH for 
deformable object collision detection. Master 
thesis. Computer Science Dept, National 
Chung Cheng Univ, Chiayi, Taiwan. 
[17]  Gavindaraju, N., Henson, M., Lin, M., and 
Manocha, D. 2005. Interactive visibility 
ordering and transparency computations 
among geometric primitives in complex 
environments. Proceedings of ACM 
Symposium on Interactive 3D Graphics and 
Games.
 8
出席國際學術會議心得報告 
                                                             
計畫編號 95-2221-E-194-071 
計畫名稱 
Fast and efficiently solving massive and time-consuming graphics computation 
using programmable graphics hardware, ［研究利用可程式的圖形硬體來快速
且有效率地解決大量且耗時的圖形計算］ 
出國人員姓名 
服務機關及職稱 
劉興民 
國立中正大學資訊工程學系助理教授 
會議時間地點 4月 2日至 4月 7日 美國夏威夷州檀香山市 
會議名稱 IEEE Symposium Series on Computational Intelligence 
發表論文題目 
1. Real-time image-based stylistic rendering using graphics 
hardware acceleration 
2. An efficient data management scheme based on spatial and 
temporal characteristics in virtual environments 
3. Using hypergraph-based clustering scheme for traversal prediction 
in virtual environments 
 
一、參加會議經過 
 
這個研討會是由著名的 IEEE Computational Intelligence Society所支持。此次會
議最大的特點是由原先 12個不同主題而獨立的 symposia結合而成為一個
conference，讓它的規模更大了。這些原先主題多樣化的研討會包括了：image and 
signal processing，data mining，bioinformatics，games，scheduling，multicriteria 
decision-making，security and defense applications，foundations and theory，
reinforcement learning，approximate dynamic programming，swarm intelligence，
artificial life，evolvable and adaptive hardware。這個新又更大規模的研討會提供
了相關學者專家在如何利用電腦來從事更智慧型的計算技術上有一個國際性交
流的機會。 
 
此次會議舉行的地點在風光明媚的夏威夷州檀香山市。議程安排第一天排滿了
全天 30場次的 tutorial。我因為行程安排上的原因，只能在第二天的清晨到達，
很遺憾的不得不錯過所有的 tutorial。也因為行程緊湊，顧不得時差的情況下，
在當日傍晚時分做了第一場的論文報告。在旅館略為休息後，為次日培養充沛
大會的網址是： 
http://ieee-ssci.org/ 
 
另外附幾張相關照片於後。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
在大會現場專注地論文發
表報告 
II. RELATED WORKS
A. NPR 
The number of literatures on NPR drawing algorithms is 
large including issues of stroke placement and orientation 
[8], silhouette tracing [9], and simulations of particular 
media [11]. 
Point-based representation is one of the most used in 
NPR drawing algorithms. The representation is used for 
models of very high geometric complexity. With a point-
based representation, the surface of a 3D object is described 
by a set of sample points without further topological 
information such as triangle mesh connectivity. The lack of 
topological information leads to simpler and more efficient 
rendering, simplification, level-of detail control, and 
texturing for very complex models [18]. 
Recent research in the field of real-time NPR exploits 
current graphics hardware. Praun et al. [19] implemented a 
technique for real-time hatching of 3D shapes. Freudenberg 
et al. [3] developed a similar technique for real-time 
halftoning. They made extensive use of the programmable 
rendering pipeline and texture blending capabilities of 
current graphics hardware. Nvidia presented an image-space 
technique to render edges of 3D shapes onto a screen-
aligned quad [2]. It samples adjacent texture values to 
process encoded normals and detects discontinuities in the 
normal-buffer. ATI extended this approach by detecting 
discontinuities in the normal-buffer, z-buffer, and Id-buffer 
[15]. This way, edges of 3D shapes, regions in shadow, and 
texture boundaries can be outlined [16]. 
The last decade has seen a blossoming of work on NPR 
algorithms in a variety of styles [4]. Much of this work 
addresses the production of still images, while some systems 
for rendering 3D scenes have addressed the challenge of 
providing temporal coherence for animations [17]. 
Most work in NPR has focused on algorithms that are 
controlled by parameter setting or scripting the designer has 
no direct control over where marks are made. An inspiration 
for our work is the technique for direct WYSIWYG (what 
you see is what you get) painting on 3D surfaces proposed 
by Hanrahan and Haeberli [6], which is now available in 
various commercial modeling systems. These tools let the 
designer paint texture maps directly onto 3D models by 
projecting screen-space paint strokes onto the 3D surface 
and then into texture space, where they are composed with 
other strokes. Strokes then remain fixed on the surface and 
do not adapt to changes in lighting or viewpoint. In contrast, 
strokes in our system are automatically added, removed, or 
modified in response to changes in lighting or viewing 
conditions [21].
B. GPU-based applications 
Recently, programmable GPU has been explored to 
enhance the performance. Harris et al. presented a 
physically-based, visually-realistic interactive cloud 
simulation using GPU [7]. The clouds were modeled using 
partial differential equations describing fluid motion, 
thermodynamic processes, buoyant forces, and water phase 
transitions. Kruger and Westermann introduced a 
framework for the implementation of linear algebra 
operators on GPU, thus providing the building blocks for 
the design of more complex numerical algorithms [13]. 
Purcell et al. implemented a modified photon mapping 
technique which uses breadth-first photon tracing to 
distribute photons using the GPU [20]. 
In the field of particle system, Kipfer et al. presented a 
method for simulating particle systems on GPU and 
implemented inter-particle collision by quickly sorting the 
particles on GPU to determine potential colliding pairs. 
Kolb et al. constructed a GPU particle system simulator that 
supports accurate collisions of particles with scene 
geometry. In it, depth maps were used to detect the 
penetrations or collisions by the computation with GPU. 
Kruger created a particle system for interactive visualization 
of steady 3D flow fields on uniform grids [12]. The entire 
process, from vector field interpolation, integration to curl 
computation, and finally geometry generation and rendering 
of the stream ribbons, is performed by GPU. Work more 
related to our system is cloth simulation. Green [5] 
implemented a simple cloth simulation that executes on 
GPU using fragment programs and float point buffer. Zeller 
[22] presented a robust method to simulate cloth on GPU 
that includes mesh-cutting techniques. A more complete 
survey on a set of GPU-assisted algorithms and systems is 
given by Owens [10].
III. METHOD
Our system consists of the following steps. 
1. Tracing silhouette edges using depth map. 
2. Tracing crease edges using normal map. 
3. Using GPU to speedup edge-detection algorithm. 
We can consider edges into two kinds. 
Silhouette edges: edges adjacent to a polygon which 
faces towards the camera and another polygon which faces 
backward the camera. 
Crease edges: edges between two front-facing or back-
facing polygons whose dihedral angle is above a threshold. 
The threshold value defines the number of crease edges.
439
Proceedings of the 2007 IEEE Symposium on Computational
Intelligence in Image and Signal Processing (CIISP 2007)
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−
−
−
=
101
202
101
xS                
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡ −−−
=
121
000
121
yS
Let I(x, y) be the image of normal map or depth map. 
Vertical and horizontal edge images of I are both computed 
using discrete 2D convolution: 
Finally, we use the Sobel filter to find out the edges using 
a threshold T: 
Let ),(),(),( yxIyxIyxI mageyx =+ , 
⎭⎬
⎫
⎩⎨
⎧
<
≥
=
TyxI
TyxI
yxEdge
mage
mage
),(  if  ;0
),(  if  ;1),(
We exploit both normal map and depth map to find 
crease edges and silhouette edges. Combining the two kinds 
of edges yields a final edge map. 
Although we can get a complete edge map, we observe
that there are too many line segments existing in edge map. 
User becomes more difficult to concentrate on the contour 
of the scene due to those small fragments. Besides, we 
notice that all these redundant edges are mostly crease 
edges, since they often occur in arc surfaces. Face normals 
in arc surface usually change abruptly; making our system 
detects many crease edges there. We therefore add an extra 
filter to remove these redundant lines after getting Edge(x, 
y).The objective of filtering is that we want to remove the 
edges which are crease edges with depth values close to 
their neighbors. This filtering traces the depth map in a way 
that if a pixel is considered a component of an edge, filter 
calculates the difference of the depth value between this 
pixel and its neighbors. If the difference is below a 
threshold Tz, we remove this pixel from edge map, which 
implies this pixel is no longer a component of the edges. 
Entries in extra filter are:
1z 2z 3z
4z 0z 5z
6z 7z 8z
1z - 8z  are neighboring pixels of 0z ( 0z  is the same as 
),( yxIz ). 
04030201 22(),( ZZZZZZZZyxI z −+−+−+−=
)22 08070605 ZZZZZZZZ −+−+−+−+
⎭⎬
⎫
⎩⎨
⎧
<
≥
=
Zz
Zz
TyxI
TyxI
yxEdge ),(  if  ;0
),(  if  ;1),(
The results are shown in Figure 5. We can see there are 
many redundant lines in Figure 5(right). The result in Figure 
5(left) is cleaner. Therefore, users can focus on the 
important contours using the extra filtering. 
  
2) GPU application 
GPU is different from the traditional CPU sequential 
programming model. The graphics pipeline can be 
illustrated as in Figure 6. The programmable components of 
graphics hardware are vertex and fragment processor. In 
GPU, We use the Frame Buffer Object (FBO), which is an 
OpenGL extension that allows rendering results to a special 
frame buffer which can be directly read in as texture. Since 
we want to use fragment processor to compute the physical 
simulation data, the results need to be sent to frame buffer 
after fragment program terminates. Using FBO, we can 
store the computational result to FBO and later retrieve it 
back as fetching textures. Since the input data and 
computational result retain in FBO, it achieves a better 
performance because it avoids extra data copying from 
frame buffer to texture by calling glCopyTexSubImage2D 
OpenGL function. Furthermore, less memory is required 
because there is only one instance of the image [14]. 
Figure 6. Graphics pipeline
xx SyxIyxI ⊗= ),(),(
yy SyxIyxI ⊗= ),(),(
Figure 5. Right is the result without extra filtering. Left is the result with 
extra filtering.
441
Proceedings of the 2007 IEEE Symposium on Computational
Intelligence in Image and Signal Processing (CIISP 2007)
