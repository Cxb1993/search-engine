行政院國家科學委員會專題研究計畫成果報告
基於證據累加的叢集整合技術之強韌化與功能延伸
Robustification and Functionality Extension of
Evidence-Accumulation-Based Cluster Ensembles
計畫編號：NSC－98-2221-E-009-146-
執行期間：2009年8月1日 至 2010年7月31日
主持人：王才沛 國立交通大學資訊工程學系(所)
中文摘要
叢集化是一個可以在沒有分類資訊的資料當中，將相關的資料點區分成叢集的方法。
叢集化演算法的種類很多，但並沒有一個方法可以對所有的資料與叢集性質都產生好的結
果。叢集整合 (cluster ensemble) 技術是近年的一個新趨勢，其做法是對同一組資料產生多
個不同的叢集化結果，再結合這些個別結果來產生一個具有共識的、更穩定也更能代表實
際資料分佈的分群。叢集整合的優點最近已逐漸被證實，也有愈來愈多的應用出現在不同
的領域。
本計畫的整體目標是以證據累加叢集法(evidence-accumulation clustering) --也就是基於
co-association 矩陣的叢集整合方法--為基礎，研討改善其強韌性的方法，以及進行模糊與
清晰叢集整合的比較。一方面，我們將證據累加叢集法與強韌叢集法做結合來改進其應用
到雜訊環境與未知叢集數量的問題時的效能。另一分面，我們也發現更適合與強韌叢及法
結合的模糊叢集整合也比清晰叢集整合具有更好的收斂性質。
關鍵詞：叢集整合、證據累加、共識叢集、強韌叢集法
Abstract
Clustering is a process that groups unlabeled data points into clusters. There are a large variety
of clustering methods, but none can generate good clustering results for all types of data and
cluster characteristics. Cluster ensemble is a new trend in recent years. Its approach is to generate
multiple clustering results out of the same data set, and then combine the individual clustering
results to form a consensus partition of the data that is more stable and more representative of the
actual data distribution. As the benefits of cluster ensemble are gradually recognized in recent
years, there are a growing number of applications in various fields.
The overall purpose of this two-year project is to start with evidence-accumulation clustering,
that is, the clustering ensemble methods based on co-association matrices, and investigate
methods that can improve its robustness as well as comparing the performances of crisp and fuzzy
cluster ensembles. First, we combine evidence-accumulation clustering with robust clustering
algorithms to improve its performance in problems that involve noisy data and unknown numbers
of clusters. On the other hand, we also find that fuzzy cluster ensembles, which work more
naturally with robust clustering methods, also exhibit better convergence characteristics
合可以更真實的代表該資料點的特性。兩個使用了軟性叢集整合的研究 [4,5] 都是使用來得
到軟性的個別分群。在 [5]，作者比較了在 CSPA，MCLA 及 HBGF 使用原方法（即使用清
晰的分群 (crisp partition) 為輸入）與使用軟性分群的效果，並且發現使用軟性分群可以得
到較佳的最終分群結果。[4] 提到由軟性分群算出 co-association 矩陣的方法，但並未將結果
與對應的使用清晰分群的結果做比較。[11] 亦討論了使用模糊 k均値產生個別分群及得到
最終模糊分群的模糊叢集整合方法，並分析了在不同階段進行去模糊化 (defuzzification) 的
影響。我們在這部分的主要問題是分析一個之前未被處理的問題，也就是個別分群的個數
對清晰與模糊叢集整合是否有不同的影響。
只有少數研究有處理雜訊/離群值對叢集整合的影響。[2] 的做法是在進行叢集化之前
就先透過其他方法來辨認並去除離群值，但所用的方法假設了離群值只佔整體資料的少部
分。[12] 是利用 co-association 矩陣辨認離群值的方法，但仍須由使用者指定所要去除的可
能是離群值的資料點的比例，而這在真實應用中多是未知的。我們在此考慮使用強韌叢集
演算法產生的分群來建立所謂的強韌叢集整合，藉以結合已有的技術來得到更實用可靠
的，可在具有未知雜訊/離群值比例時得到較準確結果的叢集整合。
二、研究內容
A. 強韌叢集整合
現階段我們已針對使用強韌叢集法 (robust clustering) 的叢集整合進行初步分析。對於
有雜訊或離群值的資料，若欲在叢集化過程中排除降低或排除這些雜訊或離群值對叢集
化結果的影響，一般有兩個方式可以採用：第一個方式是在進行叢集化之前，先使用一
個目的是在判斷哪些點是雜訊或離群值的演算法，將可能的雜訊或離群值去除後，再進
行原先的叢集化演算法。原有的 EAC 演算法即是使用這個方式。
判斷哪些資料點是雜訊或離群值的第二個方式是使用強韌叢集法，也就是在進行叢集
化的同時判斷各資料點是雜訊或離群值的可能性，並且隨著叢集化的結果逐漸更新，這
些可能性也隨著更新。這是我們在本年度進行的計畫中所要分析的方法。已下列出我們
使用雜訊叢集法 (noise clustering; NC) [13] 的一些實驗結果。雜訊叢集法在模糊 C 均值
(fuzzy c-means; FCM) 中另加一個虛擬的"雜訊叢集"。可能是雜訊的資料點將在這個雜訊
叢集中有較高的歸屬程度 (membership)，因而降低了在其他"真正"的叢集當中的。我們
的做法是只使用雜訊叢集之外的"真正"的叢集當中的歸屬程度來計算 co-association 矩
陣。基本的雜訊叢集法由一個參數（資料點到雜訊叢集之間的距離）來控制，然而我們
也可以允許每一個真正的叢集使用不同的距離。
在使用如 FCM 或類似的軟性叢集演算法 (soft clustering algorithms) 所產生的叢集來計
算 co-association 矩陣時，計算每個 co-association 矩陣元素的公式為
 
  








Hh
k
c
h
cj
h
ciij
h
uu
H
s
1 1
)()(* 1 (1)
其中 H 是叢集整合中不同分群的總數，kh代表第 h (1hH)個分群 Ph當中的叢集個數，
而 uci(h)代表在 Ph當中，資料點 xi在第 c 個叢集當中的歸屬程度。FCM 的歸屬程度公式為




k
q qi
ci
cj
m
d
d
u
1
2
2
1
1
)(
1
(2)
而 NC 的歸屬程度公式為
圖二：使用 FCM（上圖）與 NC（下圖）來計算 vi（公式(4)）的比較。
我們在圖三中顯示在兩個不同資料組中，用以上方法去除離群值再進行階層聚合所得
到的最終分群結果。
圖三：使用強韌叢集整合的叢集化結果（兩組模擬資料）。不同顏色
代表最終分群中的每個叢集，而小黑點則是被判斷為離群值而未包括
在最終分群中的點。
除了 NC 之外，我們也使用了可能性模糊 C 均值 (Possibilistic fuzzy c-means;
PFCM)[14]，使用其所算出的兩種歸屬程度（模糊性及可能性）的乘積來計算 co-association
矩陣的方法，其結果與使用 NC 的結果類似，但 NC 的效果稍佳。
B. 清晰與模糊叢集整合的收斂速度比較
這部分我們使用的是與原EAC論文相似的合成資料。下圖四是所使用的資料組圖，而
下圖五則是叢集準確率隨個別分群個數H的變化圖。其中hEAC與fEAC分別代表清晰與模
糊叢集整合。我們可以清楚看出fEAC總是比hEAC更快收斂。
國科會補助專題研究計畫項下出席國際學術會議心得報告
日期：99 年 10 月 31 日
一、參加會議經過
2009/8/21 抵達濟州島
2009/8/22 返回
二、與會心得
This is a beautiful place for a conference. And this is the most important international conference in
the field of fuzzy set theory, fuzzy systems, and applications. However, maybe probably due to travel
issue, there are not a lot of people in the conference. Due to another conference commitment on 8/23
(CVGIP in Taiwan) I could only attend this conference for a short time.
This is the first time I'm a session chair for the whole session. I feel that I could do better in, say,
time management. It was also a little awkward to transition between the roles as the session chair and a
presenter. I learned that even such rarely discussed tasks of basic academic services take some
experience and preparation.
三、考察參觀活動(無是項活動者略)
無
四、建議
無
五、攜回資料名稱及內容
會議論文集
六、其他
附件：於會議發表之論文
計畫編號 NSC 98 － 2221 － E － 009 － 146 －
計畫名稱 基於證據累加的叢集整合技術之強韌化與功能延伸
出國人員
姓名
王才沛
服務機構
及職稱
國立交通大學資訊工程系助理
教授
會議時間
98 年 8 月 20 日至
98 年 8 月 24 日
會議地點
韓國濟州島
會議名稱
(中文)2009 國際模糊系統會議
(英文)2009 IEEE International Conference on Fuzzy Systems
發表論文
題目
(中文)比較清晰與模糊證據累加叢集法
(英文) Comparing Hard and Fuzzy C-Means for Evidence-Accumulation
Clustering
be very beneficial as well. Research works in this direction
have appeared only very recently. Yang, Lv, and Wang [20]
studied co-association matrix based soft cluster ensembles
generated with three different fuzzy similarity measures.
Punera and Ghosh [11] compared the performance of
ensembles of crisp or soft base clusterings obtained using EM;
a crisp clustering is obtained by assigning each pattern to the
most likely cluster in the corresponding soft clustering. The
combination methods tested include Cluster-based Similarity
Partitioning Algorithm (CSPA) [4], Meta-CLustering
Algorithm (MCLA) [4], and Hybrid Bipartite Graph
Formulation (HBGF) [7]. The evaluation is based on
normalized mutual information (NMI) with the ground-truth
partition. The conclusion in [11] is that using soft clusterings
does improve the correctness of the final clusterings.
Avogadri and Valentini [14] used FCM to generate the base
clusterings; the corresponding hard clusterings are obtained
by either alpha-cut or assigning each pattern to the most likely
cluster. Their results also indicated better accuracy of fuzzy
versus hard base clusterings, but only for a synthetic data set
of 3 well-separated hyperspherical clusters. Both [11] and [14]
study only the cases where the true number of clusters is
known and used as the number of clusters for all the base
clusterings as well as the combined clustering.
In this paper, we focus on the performance comparison of
EAC using either HCM or FCM as the base clustering
generator. These two versions are subsequently denoted as
hEAC and fEAC in this paper. We specifically analyze two
aspects that have not been analyzed in the literature: The
speed of convergence in terms of the number of base
clusterings needed to produce a stable combined clustering,
and how hEAC and fEAC are affected by noise points in the
data. For the remainder in this paper, we start with the
description of both the crisp and soft versions of EAC,
followed by the description of our experiments and results,
and the conclusion.
II. EVIDENCE-ACCUMULATION CLUSTERING
A. Crisp Evidence-Accumulation Clustering
Our description here follows the algorithm in [5]. Assume
that the set X ={x1, x2,…, xn} contains n patterns (data points).
Let P ={C1, C2,…, Ck} be a crisp clustering (partition) of X.
Here k is the number of clusters in P, and the clusters, C1,
C2,…, Ck, are disjoint non-empty subsets of X, and the union
of all the clusters in P is the same as X. For a data set X it is
possible to obtain many different P. Let a cluster ensemble
consists of N clusterings of X: P1, P2, …, PN. As we allow
each individual clustering to have a different number of
clusters, we use kq to represent the number of clusters in Pq
(1qN).
A nxn co-association matrix, S(q) = [sij
(q)], is computed for
each clustering Pq. To determine its elements, let ci
(q)
represent the cluster index of xi in Pq. Then sij
(q) is given by
the following formula:


 
.,0
,1 )()()(
otherwise
cc
s
q
j
q
iq
ij . (1)
For the cluster ensemble, the overall co-association matrix,
denoted as S* = [sij
*], is simply the average of all the S(q):



Nq
q
ijij sN
s
1
)(* 1 . (2)
The co-association matrix is a similarity matrix that can
then be fed into various algorithms for relational data
clustering. Hierarchical agglomeration with single or average
linkage is the method of choice in EAC because it does not
require a pre-specified number of clusters. Such algorithms
generate a hierarchy of clusterings. When the number of
clusters is unknown, the maximum-lifetime criterion is used
to select a clustering from the hierarchy. We use kf to
represent the number of clusters in this selected clustering.
B. Soft Evidence-Accumulation Clustering
Similar to [11], here the term "soft" refers only to the base
clusterings in an ensemble, meaning that these individual
clusterings are soft. However, the combined clustering may
still be crisp. This is the case in this paper as we follow the
method in [5] and use hierarchical agglomeration to generate
the combined clustering.
A soft clustering is represented by a partition (or
membership) matrix U = [uti], where uti is the membership of
xi in the tth cluster. For a probabilistic clustering, the
membership matrix satisfies the condition
1,
1
 

k
t
tiui . (3)
Here k is the number of clusters. Such a clustering can be
obtained with EM, which is the method used in [6] and [11],
or FCM, which is the method used in [14] and in this paper.
A straightforward extension of (1) for computing
co-association matrix from a membership matrix is [6]



qk
t
q
tj
q
ti
q
ij uus
1
)()()( , (4)
or when put in matrix form,
 Tqqq )()()( UUS  . (5)
The superscripts (q) in U(q) and uti
(q) indicate that the
memberships are for the qth clustering within the ensemble.
This form of aggregating the memberships is just the
algebraic product form of t-norms in fuzzy set theory. It is
used in [6] and [14], and is the method used in our
experiments. Other forms of t-norms can also be used. An
example is the minimum, as used in [13]:



qk
t
q
tj
q
ti
q
ij uus
1
)()()( ],min[ . (6)
Once we have obtained the co-association matrix, the
process of extracting the combined clustering is the same as
the original (crisp) EAC.
III. EXPERIMENT SETUP
its hEAC counterpart.
For a more quantitative comparison regarding the
dependence of the performance on N, let us now consider the
value of N needed for the clustering accuracy to reach 95% of
its maximum value in each case. For the 12 cases here (6 data
sets with combined clustering selection based on maximum
lifetime or known k*), the median N needed is 2 for the fuzzy
versions and 11 for the corresponding crisp versions. It is
evident then that fEAC converges much faster than hEAC
with respect to N. Similar observation can be made for kf
when the maximum lifetime criterion is used. This is a clear
indication that FCM is an attractive option for generating the
base clusterings when the total number of base clusterings is
constrained by, say, available system resource.
Fig. 5 displays the clustering performance (kf and accuracy
when using the maximum lifetime criterion, and accuracy
when selecting the combined clustering with k* clusters)
versus N for the three real data sets. The overall best
performer is clearly fEAC-AL, although hEAC-AL is better
for the Pen-digits data set when using the maximum lifetime
criterion. More importantly, faster convergence versus N for
fEAC compared to hEAC is still evident here.
Overall, fEAC is better than hEAC when N is very small,
and the main reason is because fEAC has much faster
convergence. On the other hand, the performance of fEAC
and hEAC for larger N is comparable for most cases. In order
to provide insight to these observations, we show in Fig. 6 the
co-association matrices of using hEAC (left column) and
fEAC (right column) with three different N. The data set is
half rings. We can see that while the co-association matrix
becomes progressively fuzzier for hEAC as N increases, the
co-association matrix for fEAC does not change much. The
most significant difference here is between the co-association
matrices between hEAC and fEAC at N=1. With only a single
clustering, the co-association matrix is very crisp for hEAC
but quite fuzzy already for fEAC. This directly results from
the fact that with a clustering, HCM clusters are disjoint and
FCM clusters overlap with one another. When N is large, the
co-association matrices for hEAC and fEAC are more similar,
as the "fuzziness" here is more of the result of averaging over
many clusterings. Therefore, we can infer that the
overlapping between FCM clusters has an effect of
"fuzzifying" the co-association matrix similar to averaging
over several clusterings. The faster convergence of fEAC is
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
12
8
4
0
12
8
4
0
Number of Clusterings, N
N
um
be
ro
fC
lu
st
er
s
in
th
e
F
in
al
C
lu
st
er
in
g,
k f
(a) (b) (c)
(d) (e) (f)hEAC-ALfEAC-AL
hEAC-SL
fEAC-SL
k* = 5 k* = 5 k* = 4
k* = 4 k* = 2
k* = 3
Figure 2. The number of clusters in the final clustering vs. the number of clusterings in each ensemble. (a)-(f) are plots for the
synthetic data set S1-S6, respectively. The value of k* is also indicated in each plot.
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
1.0
0.8
0.6
0.4
0.2
0
Number of Clusterings, N
C
lu
st
er
in
g
A
cc
ur
ac
y (a) (b) (c)
(d) (e) (f)
1.0
0.8
0.6
0.4
0.2
0
hEAC-AL
fEAC-AL
hEAC-SL
fEAC-SL
Figure 3. The clustering accuracy vs. the number of clusterings in each ensemble. The number of clusters in the final clustering is
selected using the maximum-lifetime criterion. (a)-(f) are plots for the synthetic data set S1-S6, respectively.
fEAC has much higher accuracy compared with hEAC when
the numbers of clusterings are low. We believe this enhances
the usefulness of EAC because it provides a possible solution
to the original drawback of EAC that a large number of
clusterings is required for convergence. We also provide the
insight into this observation by directly comparing the
co-association matrices of hEAC and fEAC for different
numbers of base clusterings. In addition, our experiments also
show that fEAC is more tolerant of noise than hEAC.
There are still many research problems related to soft
cluster ensembles that can be pursued. A more
comprehensive study (such as one by varying the
fuzzification factor) can lead to a more clear understanding of
the phenomena associated with fuzzy cluster ensembles.
Another direction is the use of non-probabilistic partitions
(such as those obtained with possibilistic c-means [22] and its
derivatives) as the base clusterings in the ensemble. Overall,
we believe that the combination of fuzzy approaches and
cluster ensembles will be of great values in future research
and applications.
REFERENCES
[1] A.K. Jain, M.N. Murty, and P.J. Flynn, "Data Clustering: A Review",
ACM Computing Surveys, vol. 31, pp. 264-323, 1999.
[2] S. Theodoridis and K. Koutroumbas, Pattern Recognition (3rd Ed.),
San Diego, CA: Academic Press, 2006.
[3] A.K. Jain and M.H.C. Law, "Data clustering: A user’s dilemma",
Lecture Notes on Computer Science, vol. 3776, pp. 1–10, 2005.
[4] A. Strehl and J. Ghosh "Cluster ensembles -- a knowledge reuse
framework for combining multiple partitions", J. Machine Learning
Research, vol. 3, pp. 583-617, 2002.
[5] A.L.N. Fred and A.K. Jain, "Combining multiple clusterings using
evidence accumulation", IEEE Trans. Pattern Analysis Machine
Intelligence, vol. 27, pp. 835-850, 2005.
[6] X.Z. Fern and C.E. Brodley, "Random projection for high dimensional
clustering: A cluster ensemble approach", Proc. 20th Int'l Conf.
Machine Learning (ICML), 2003.
[7] X.Z. Fern and C.E. Brodley, "Solving cluster ensemble problems by
bipartite graph partitioning", Proc. 21th Int'l Conf. Machine Learning
(ICML), ACM International Conference Proceeding Series, vol. 69, p.
36, 2004.
[8] A. Topchy, A.K. Jain, and W. Punch, "A mixture model for clustering
ensembles", Proc. SIAM Int'l Conf. Data Mining, pp. 379-390, 2004.
[9] A. Topchy, A.K. Jain, and W. Punch, "Clustering ensembles: Models of
consensus and weak partitions", IEEE Trans. Pattern Analysis Machine
Intelligence, vol. 27, pp. 1866-1881, 2005.
[10] B. Minaei-Bidgoli, A. Topchy and W. F. Punch, “Ensembles of 
partitions via data resampling”, Proc. 2004 Int'l. Conf. Information
Technology, pp. 188-192, 2004.
[11] K. Punera and J. Ghosh, "Soft Cluster Ensembles", in Advances in
Fuzzy Clustering and its Applications, Ed. J. Valente de Oliveira and W.
Pedrycz, Wiley, 2007.
[12] P. Viswanath and K. Jayasurya, "A fast and efficient ensemble
clustering method", Proc. 2006 Int'l Conf. Pattern Recognition (ICPR),
pp. 720-723, 2006.
[13] P. Hore, L. Hall, and D. Goldgof, "A Cluster Ensemble Framework for
Large Data sets", Proc. 2006 IEEE Int'l. Conf. System, Man, and
Cybernetics, pp. 3342-3347, 2006.
[14] R. Avogadri and G. Valentini , "Ensemble clustering with a fuzzy
approach", Supervised and Unsupervised Ensemble Methods and their
Applications, pp. 49-69, Springer, 2008.
[15] H. Luo, F. Kong, and Y. Li, "Clustering mixed data based on evidence
accumulation", Lecture Notes on Computer Science, vol. 4093, pp.
348-355, 2006.
[16] J.C. Bezdek, Pattern Recognition with Fuzzy Objective Function
Algorithms, New York: Plenum, 1981.
[17] L.I. Kuncheva and D.P. Vetrov, "Evaluation of stability of k-means
cluster ensembles with respect to random initialization", IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 28, pp. 1798-1808,
2006.
[18] Z. Yu, Z. Deng, H.-S. Wong, and X. Wang, "Fuzzy cluster ensemble
and its application on 3D head model classification", Proc. 2008 IEEE
Int'l Joint Conf. Neural Networks, (IJCNN), pp. 569-576, 2008.
[19] J. Gllavata, E. Qeli, and B. Freisleben, "Detecting text in videos using
fuzzy clustering ensembles", Proc. 8th IEEE Int'l Symposium on
Multimedia, pp. 283-290, 2006.
[20] L. Yang, H. Lv, and W. Wang, "Soft cluster ensemble based on fuzzy
similarity measure", Proc. IMACS Multiconf Comp Eng Systems Appl,
pp. 1994–1997, 2006.
[21] A. Asuncion and D.J. Newman, UCI Machine Learning Repository
[http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA:
University of California, School of Information and Computer Science,
2007.
[22] R. Krishnapuram and J.M. Keller, "A possibilistic approach to
clustering", IEEE. Trans. Fuzzy Systems, vol. 1, pp. 98-110, 1993.
21ps03-vidmar
Figure 6. Co-association matrices (half-rings data set) at different N. Left
and right columns are obtained using HCM and FCM, respectively. N =
1, 4, and 40 for the top, middle, and bottom row, respectively.
AL, mode A
SL, mode A
AL, mode B
SL, mode B
0.6
0.4
0.2
0.0
-0.2
0.2 0 0.2 0.4 0.6 0.2 0 0.2 0.4 0.6
Clustering Accuracy Difference (Noiseless Data)
C
lu
st
er
in
g
A
cc
ur
ac
y
D
iff
er
en
ce
(N
oi
sy
D
at
a)
N = 8 N = 20
Figure 7. Clustering accuracy difference (accuracy of fEAC minus
accuracy of hEAC) with noiseless data (horizontal axis) vs. noisy data
(vertical axis). Modes A and B refer to the use of maximum-lifetime
criterion and the use of known k* for selecting the combined clustering,
respectively.
98 年度專題研究計畫研究成果彙整表 
計畫主持人：王才沛 計畫編號：98-2221-E-009-146- 
計畫名稱：基於證據累加的叢集整合技術之強韌化與功能延伸 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 7 3 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
