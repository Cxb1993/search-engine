Der-Chiang Li , Yao-San Lin
Department of Industrial and Information Management, National Cheng Kung University, 1st University Road, Tainan 70101, Taiwan
 2006 Elsevier B.V. All rights reserved.
* Corresponding author. Tel.: +886 6 2757575x50501; fax.: +886 6 2374252.
E-mail address: lidc@mail.ncku.edu.tw (D.-C. Li).
European Journal of Operational Research 184 (2008) 169–184
www.elsevier.com/locate/ejor0377-2217/$ - see front matter  2006 Elsevier B.V. All rights reserved.Keywords: Density estimation; Neural networks; Simulation; Small dataset learning; Stochastic processes; Time series
1. Introduction
Management in the early stages is diﬃcult but critical because its results are used to detect problems in
advance and correct management policy in time. This concern is even more clear and deﬁnite for systems pro-
ducing costly items. We basically use statistical theories to overcome the challenge of learning in the early
stages. Since the size of training samples is always a key factor in determining a learning machine’s knowledgeReceived 15 November 2005; accepted 19 October 2006
Available online 14 December 2006
Abstract
Acquiring knowledge in manufacturing systems in the early stages always has a challenging task due to the lack of suf-
ﬁcient data. This makes it hard for the derived management model to reach a reliable and stable level. Li and Lin (2006)
developed a useful method that can deal with the problem of knowledge acquisition based on a small data set. However,
this method assumes all data are collected at the same time, since they treat the data set as a source (from one population)
of a priori knowledge for learning. In fact, instead of being a random data set, these collected data can be time dependent,
that is, they tend to be a sequence of observations, occurring at diﬀerent times. The consideration of this dependence prop-
erty in the data will beneﬁt the knowledge acquisition in the early stages by expanding the learning model from an inde-
pendent model to a dependent model. This research expanded the intervalized kernel density estimator (IKDE) presented
in Li and Lin (2006) to a more general form to improve the learning model in the early stages. The general model, called
GIKDE, joints the concepts of time series and stochastic processes in order to deal with both independent and dependent
data sets. The Virtual Sample Generation process based on GIKDE was also developed to produce extra information for
expediting the learning. Results obtained from the application of the model to a control charts data, using a back-prop-
agation neural network as the learning tool, show that this unique approach is an eﬀective method of knowledge acqui-
sition for a manufacturing system in the early stages.Stochastics and Statistics
Learning management knowledge for manufacturing systems
in the early stages using time series data
*doi:10.1016/j.ejor.2006.10.008
goal, given a 3-D view of an object is, to generate new views from any other direction through mathematical
transformations. Samples generated through those new views are called virtual samples that a learning
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 171machine can use for verifying an instance more precisely. Niyogi et al proved that the process of creating vir-
tual samples is mathematically equivalent to incorporating prior knowledge.
With continuous data, Li et al. (2003) proposed a method that used Functional Virtual Population to create
more samples for training. The idea is to expand, directed by the selected criterion, the domain of each attri-
bute or variable in a small data set to form a new virtual sample set. The expanding strategy includes data
decreasing and increasing. Li et al. suggested three methods for domain expanding: increasing, decreasing,
and mixed. In their research, deciding which policy is the best choice is still a process of trial-and-error.
Bayesian Network uses prior knowledge together with conditional probability distributions of data in the
learning; however, a small data set still leads Bayesian Network to a poor performance. Onisko et al. (2001)
proposed a method that uses Noisy-OR gates to reduce data requirement. The use of Noisy-OR gates is a good
approach to deal with small sample cases in approximating the conditional probability distribution. However,
for easier computation, the assumption of conditional independence between variables in their research creates
a limit to its applications. Strictly speaking, Bayesian Network can more suitably be applied to nominal data
rather than numeric data.
When learning with small data sets, fuzzy theory is another way to overcome the insuﬃcient information
whose membership function provides various degree of data ambiguity. Li et al. (2006) developed the data-
fuzziﬁcation technology based on fuzzy theory for improving the scheduling knowledge of FMSs, which only
contain a small data set for learning.
Furthermore, the small data set in hand can be treated as a population that its density function can facil-
itate to generate samples of larger sizes. There kind of samples, called virtual samples, can approximate the
unknown information of the data. Virtual sample generation (VSG) makes the size of small data sets assessed
large enough to be learned from. To employ VSG, there are still some requirements such as the tools to gen-
erate the virtual samples and the method to model time dependent data.
2.1. Kernel density estimation and intervalization
For generating virtual samples, the population’s information is required. Density estimation (DE) can
approximate the density function of a population through emphasizing the concentration of data points. Ker-
nel density estimation (KDE) is the common tool of DE, which employs appropriate kernel functions to esti-
mate the density functions of populations in practice. Many studies (Gangopadhyay and Cheung, 2002; Gu,
1998; Hall and Kang, 2001) are devoted to pursuing the optimal setting of parameters in KDE and this issue is
still an open problem. Notably, when the size of a data set is small, KDE would produce a unreliable estima-
tor. Li and Lin (2006) proposed the intervalized kernel density estimation (IKDE) to overcome the problems
of estimation with small data sets. IKDE has a unique function of intervalization that can vary the smoothing
parameter according to sample sizes and data locations.
Intervalization can also express the variables throughout a wider range of data sets collected in the early2. Learning with small data sets
Analysts hope to acquire more training data before conducting a learning task, since learning based on a
small data set faces the problem of insuﬃcient information. In the sense that samples can somehow suﬃciently
resemble the entire data, Gu et al. (2001) presented a statistical approach to eﬃciently ﬁnd a right sample size,
called Statistical Optimal Sample Size (SOSS). In the SOSS approach, they at ﬁrst deﬁne the sample quality Q,
a decreasing function of the average information divergence, which is the measurement of diﬀerences between
samples of SOSS and the entire sample set. SOSS is the size at which its Q suﬃciently approaches the one com-
puted from the entire sample set, given a large data set. The way to search for SOSS is iterative and time-con-
suming, using an algorithm with a pre-set acceptable quality level.
In Virtual Data Generation, mostly used in Pattern Recognition, Niyogi et al. (1998) used prior knowledge
obtained from the given small training set to create virtual examples for improving the image recognition. Thestages, which have small sizes and large variation. When the dependent character occurs, IKDE is limited
i i
conce
^
that t
impact on the data trend. It also partially stresses the Markovian property (Bre´maud, 1999; Ross, 1996,
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 1732003; Taylor and Karlin, 1998) even though it is not a stochastic process. That is, we enable the sequence
of wd to provide ﬂexible adjustment of time intensity to the model.
An increasing sequence can be a good choice for deciding the type of a weight sequence. There are three
types of increasing weight series considered in this study: including noninformative, common increasing,
and Martingale types. The ﬁrst and the last one are degenerative cases of increasing series; they have determi-
nate forms as the number of weight elements. For predicting the amount or distribution of the next input, the
Martingale type, {1,0, . . .}, suggests being fully concerned with the present data without considering the past
information since the next input will be aﬀected by the current data only.
As show in Table 1, the noninformative type of weight series assumes all kernels occurring at the same time
will share equal inﬂuence. Hence, each element is equal to one another, w1 = , . . . , = wd = 1/d, in noninforma-
tive types. For example, one can use w1 = w2 = w3 = 1/3 if one traces back three stages of data for establishing
the GIKDE for f(xt+1) with a noninformative weight series. Actually, this type of weight series always regards
a sequence of time dependent data as a set of random data. The common increasing type allows the weights to
grow as time goes by, so that in the case of d = 3, fw1 ¼ 12 ;w2 ¼ 14 ;w3 ¼ 14g, assigns half a percentage to the last
kernehe inﬂuence power from the past data diminishes over time, that is, the more recent data have morethe summation of all wd’s equal one (i.e. dwd = 1) so that {wd, dP 1} would satisfy f tþ1 dx ¼ 1. WhenR
f^ tþ1 dx 6¼ 1, we need to divide every wd by the amount of
R
f^ tþ1 dx. We use {wd, dP 1} to represent the inten-
sity of time and it is an arbitrary sequence from the initial stage to the last. {wd, dP 1} represents the conceptrned. This means that {wd,dP 1} plays the role of weights in the general form. In addition, we requireP R3.2. Weighted kernels series for showing the time trend
In IKDE, it sums all the kernels by dividing corresponding data size, ni, and intervalized smoothing param-
eters, hi. The proposed GIKDE here is also a weighted summation of kernel functions but employs {wd,dP 1}
as the coeﬃcient sequence to replace all 1/n h ’s, where d is the inspection window since there are d kernelsThe retroactive eﬀect is the principal objective for us to employ time series theory. Under the assumption of
time dependency, not all data share the same importance in capturing the trend of the next input data. Since
each data has been assigned to a particular kernel, we introduce the inspection window, d, to facilitate the
determination of kernel number. The inspection window indicates that ﬁnding the GIKDE for predicting
the next input data should trace back the referred information by d stages. That is, determining the GIKDE
of f(xt+1) will take the last and the recent d kernels of {X1, . . . ,Xt} into account. Since each kernel covers one
data, d suggests that the GIKDE of f(xt+1) consider only {Xtd+1, . . . ,Xt1,Xt}.
Furthermore, the argument of kernel functions needs be modiﬁed. Thus, we introduce G(X1, . . . ,Xt), the
location function of the tth kernel, to replace the location parameter, lt. Time series theory is a useful tool
for capturing the path changing of certain sequential input data, and the path that we mention here is equiv-
alent to the sample path in stochastic processes (Ross, 1996, 2003; Taylor and Karlin, 1998). When using time
series theory for modeling kernel locations, the tth location, G(X1, . . . ,Xt), it can be set using the conditional
expected value of Xt+1; that is, G(X1, . . . ,Xt)  E[Xt+1jX1, . . . ,Xt]. A linear combination of {X1, . . . ,Xt} can
also represent this conditional mean, that is
E½X tþ1jX 1; . . . ;X t ¼ a0 þ b1X 1 þ    þ btX t;
where bi means the coeﬃcient of Xi and a0 denotes the constant term.
This is analogous to an autoregressive (AR) model of order t, AR(t), deﬁned as
X tþ1 ¼ a0 þ b1X 1 þ    þ btX t þ etþ1;
where et+1 follows a normal distribution (et+1  N(0,r2)) as a random error of the tth stage. AR models shall
help summarize the past input data and the order t will include all of them. In early stages, where t is small,
hence every input data at diﬀerent times (since the initial time) may have more or less inﬂuence on the next (or
present) in the early stages. In other words, G(X1, . . . ,Xt) has a general form equaling AR(t).l and the reminder of the percentage to the other two kernels before the last kernel.
The GIKDE is actually an estimator of conditional densities when possessing the assumption of time
dependency. f^ tþ1 provides a very informative summary of the sequential data, {X1, . . . ,Xt}, and it is a density
Ste
each class presents a speciﬁc pattern of data trends. To simplify the work, we selected a part of SCCTS with
two c
recor
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 175istic or pattern of the corresponding class. We consider the case in which multiple classes occur in a single
1 Several density functions with speciﬁc forms have special sampling methods, such as the Polar method for normal or bivariate normal
distriblasses—Normal (Class A) and Cyclic (Class B) as shown in Table 2. All SCCTS data in all classes were
ded with observational values for 60 sequential times and they repeatedly displayed the main character-Step 2: determine the central parameter G(X1, . . . ,Xtj1).
For j = 1 to d.
G(X1, . . . ,Xtj1) = E[XtjjX1, . . . ,Xtj1] = a0 + b1X1 +    + btj1Xtj1.
Step 3: form GIKDE f^ tþ1 ¼
Pd
j¼1wjKj
xGðX 1;...;X tj1Þ
hj
 
.
Step 4: generate virtual data sets with desired size, based on f^ tþ1.
4. Experimental study
Using Virtual Sample Generation (VSG) to improve the quality of knowledge acquisition using a small
data set collected in the early stages is a strategy of early learning. When the data in the early stages is insuf-
ﬁcient for a stable learning, our experiment results show that generating virtual samples analogous to the ori-
ginal small set through GIKDE can help stabilize the knowledge-learning process.
4.1. The monitoring case in Quality Control
We selected the data set from Synthetic Control Chart Time Series (SCCTS). SCCTS (Alcock and Mano-
lopoulos, 1999), collected on KDD (downloadable from KDD’s website http://kdd.ics.uci.edu/databases/syn-
thetic_control/synthetic_control.html), contains time series data from six diﬀerent classes of control charts,p 1: decide d, {wj}, Kj, hj.For concisely summarizing the whole processing works, the applying procedure can be listed in steps asestimator conditioned by the past data, that is,
f^ tþ1ðxÞ ¼ f^ ðxtþ1Þ ¼ f^ ðxjX 1; . . . ;X tÞ: ð4Þ
3.3. Monte Carlo methods for generating samples
Based on GIKDE, for generating more data from the original small population, a generation method
extracts a set of samples agreeing with the original data set and guarantees the quality of them. Monte Carlo
methods are the most common approaches to help extract the desired samples. We employ the methods tied in
with the inverse method for sample generation using the GIKDE.
The inverse method is a useful tool for variates generation, regardless of distributions having speciﬁc form
density functions.1 When generating variates or drawing samples from certain distribution, {wd,dP 1} also
serves as a guide for stratiﬁed sampling and simulation.
The common stratiﬁed sampling divides the range of density distributions into several mutually exclusive
areas and the corresponding proportional samples are drawn from data within each nonoverlapping range.
Similarly, we sample data with stratifying by time or stages since Formula (2) sums weighted kernels of dif-
ferent stages. Weights of each stage indicate the proportions of the corresponding kernels in the sampling. For
example, we draw wdN samples from the dth kernel if N samples are necessary.
As a result, generating variates using the GIKDE turns to generating variables from a set of kernels. These
kernels often have plain, not complicated, forms, which makes variates generation from kernels easier.utions etc.
In
that
1 6 i
mean
collec
requi
In
{XijC
data
d < m
excep
case o
mated
trary
Fi
Fi
tincti
Th
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 177between collected data and virtual samples. Now we estimate the probability density (or mass) function of
a discg. 2 displays the graphics of these estimators explained above in diﬀerent weight series. It shows that dis-
on among these graphics does come from various types of weight series.
e estimation of f(ct+1) by employing GIKDE for the discrete data is mainly to keep the consistencef^ ðxtþ1jCtþ1 ¼ AÞ ¼
1ﬃﬃﬃ
2p
p 1
2
eðx28:8361Þ =2 þ 1
4
eðx23:4917Þ =2 þ 1
4
eðx25:5037Þ =2
with the common increasing weight series;
1ﬃﬃﬃ
2p
p eðx28:8361Þ
2=2
with the Martingale weight series;
>>>>>:
ð7aÞ
f^ ðxtþ1jCtþ1 ¼ BÞ ¼
1
3
ﬃﬃﬃ
2p
p ðeðx32:5914Þ2=2 þ eðx36:5829Þ2=2 þ eðx40:4475Þ2=2Þ
with the noninformative weight series;
1ﬃﬃﬃ
2p
p 1
2
eðx32:5914Þ
2=2 þ 1
4
eðx36:5829Þ
2=2 þ 1
4
eðx40:4475Þ
2=2
 
with the common increasing weight series;
1ﬃﬃﬃ
2p
p eðx32:5914Þ
2=2
with the Martingale weight series:
8>>>>>><
>>>>>>:
ð7bÞcoeﬃcients, a0, b0 are constant terms, and e’s represent random errors.
nally, we formulated the estimators of ft+1(xjCt+1) by inserting the numerical values as the following,
1
3
ﬃﬃﬃ
2p
p ðeðx28:8361Þ2=2 þ eðx23:4917Þ2=2 þ eðx25:5037Þ2=2Þ
with the noninformative weight series;
2 2 2
 
8>>>>>><f^ ðxtþ1jCtþ1 ¼ BÞ ¼
X
j¼1
wjKðx ARðjÞÞ: ð6bÞ
Note that although both of these two estimators employ AR(1), AR(2), and AR(3), the reference data are dis-
tinct data. For Xt+1jCt+1 = A, each corresponds to AR(1) = a0 + a1X10 + e, AR(2) = a0 + a1X10 + a2X9 + e,
and AR(3) = a0 + a1X10 + a2X9 + a3X6 + e respectively while AR(1) = b0 + b1X8 + e, AR(2) = b0 + b1X8 +
b2X7 + e, and AR(3) = b0 + b1X8 + b2X7 + b3X5 + e for Xt+1jCt+1 = B, where a1, . . . ,a3, b1, . . . ,b3 are arbi-that Gaussian kernel functions and identical smoothing parameters are employed at each time stage i, so
hi = h = 1. Letting the location function be autoregressive models, Gj(X1, . . . ,Xi) = Gj ({XijCi = A,
6 t}) = AR(j), one can obtain the predicted value from the AR models. Note that AR models can output
ingful values based on data from at least four stages. Hence, to estimate the conditional pdf, one should
t data from at least four stages for each class. In SCCTS, the selection of t = 10 should satisfy the
rement.
this 10-stage data, {XijCi = A,1 6 i 6 10} = {X2,X6,X9, X10} are collected while obtaining
i = B,1 6 i 6 10} = {X1,X3,X4,X5,X7,X8}, that is, four data belong to Class A and the remaining six
fall into Class B. Now, the setting of d should be less than both numbers of data collected for each class,
in{4,6}. The estimation work is for both conditional variables (Xt+1jCt+1 = A and Xt+1jCt+1 = B),
t when the arguments of location functions concern the data in the corresponding class only. For the
f f^ ðxtþ1jCtþ1 ¼ AÞ, Gj yields an output based on {X2,X6,X9,X10}. That is the diﬀerence between the esti-
conditional pdf and the pure pdf. With employing AR models, we have
f^ ðxtþ1jCtþ1 ¼ AÞ ¼
X3
j¼1
wjKðx ARðjÞÞ; ð6aÞ
3and
f^ ðxtþ1jCtþ1 ¼ BÞ ¼
Xd
j¼1
wjKj
x GjðfX ijCi ¼ B; 1 6 i 6 tgÞ
hj
 
:rete random variable, Ct+1 through GIKDE. The largest diﬀerence from discrete cases is in the selection
and t
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 179ence in mathematics, concerning the learning machine and learning accuracies. Trial-and-error is an alterna-
tive way since the optimal size is not too large to search. As to the second factor, we can use f^ ðctþ1Þ, (8), as the
assistance to estimate the proportion for every class at the future stage (t + 1), like the value of f^ ðc11Þ for each
class in Table 3. We try various sizes of virtual sample sets here with the same composition for sureness. Var-
ious sizes of virtual sets are generated by the VSG step, including n = 10,15,20,25,30.
When processing VSG based on a set of random samples, the percentages of virtual samples generated for
each class has relation to the composition of the original data. Nevertheless, for the case of time-dependent
data sets, true proportions of classes should rely on sequence information obtained from the data because they
vary from time to time. Here, we use the estimator of f(ct+1) heuristically by adding the weight series to empha-
size the time eﬀect.
A BPN (back-propagation network) is employed for learning the early classiﬁcation knowledge in this
research. Variables concerned in this structure include the amount of input value Xi, the diﬀerence of two suc-
cessive inputs DXi (i.e. Xi+1 = Xi + DXi), and the class (or label) of Xi, Ci. We assign the ﬁrst two variables to
the input neurons and the last neuron to the only output of the current BPN structure. The time index i has set
a range from one to ten, that is, 1 6 i 6 t and t = 10.
Software packages often provide functions to help decide the mystery parts of BPN—the hidden layer and
its neurons. Here, we adopt neural network packages—Pythia, to process the learning work. Note that any
software package of Neural Network is applicable to this research.
We used the trained 2-4-1 neural network structure to predict the classiﬁcation of virtual sample sets gen-
erated form three types of weight series. For the set of size n = 10, we labeled these generated virtual data as
X 111; . . . ;X
10
11 (i.e. X
1
tþ1; . . . ;X
n
tþ1Þ, since they are drawn from f^ ðx11jC11Þ. Naturally, the proportion of classes
within these 10 virtual data follows f^ ðc11Þ (i.e. f^ ðctþ1ÞÞ. The outputs of the trained BPN, C111; . . . ;C1011, help
decide the prediction of C11, C

11. The most class of C
1
11; . . . ;C
10
11 is assigned to C

11. For example, if the number
of A observed exceeds that of B, then C11 will equal A and vice versa. With available f^ ðx11jC11Þ ((7a) and
Fig. 3), we are able to ﬁnd the estimated conditional expectation of E[X11jC11] and treat it as the optimal
X11, as in the following:
X 11 ¼ E½X 11jC11: ð9Þ
Therefore, we use (9) and the virtual 10-data set (n = 10) to determine the prediction of stage 11. Besides,
our experiments also work on the cases of n = 15,20,25,30 and the corresponding predictions are shown in
Table 4a using three types of weight series.
We also applied IKDE, directly using ANN, and Logistic regression to this case for comparing the perfor-
mances of these methods with GIKDE. Directly using ANN is to adopt only ANN for the learning task with-
out a
are g
4.2. T
W
conste size of a virtual sample set involves two crucial factors: the total number of necessary virtual samples
he constituent of a virtual sample set. To determine the size of virtual samples needs a conscientious infer-Since the GIKDE provides a base for VSG, we generate virtual samples based on the estimated f^ ðxtþ1jCtþ1Þ.
Certainly, the generation work should proceed for every class. We adopt Monte Carlo method, which is com-
monly used for producing desired random numbers, to generate virtual samples following the distribution of
Xt+1jCt+1.
Th
Table 3
The estimates computed using (8) with three types of weight series
Types of increasing weight series f^ 11ðAÞ f^ 11ðBÞ
Noninformative 0.328 0.672
Common increasing 0.346 0.654
Martingale 0.4 0.6pplying GIKDE and VSG steps. The network layout is the same as shown in Fig. 3. Since no virtual data
enerated in the latter two methods, the original training set is used for learning knowledge.
he scheduling case in Production Management
e applied GIKDE to another case, the scheduling problem of a Flexible Manufacturing System (FMS)
ructed as shown in Fig. 4. This FMS consists of one load/upload station, three automatic guided vehicles
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 181Fig. 4. The FMS simulation model layout consists of one load/upload station, three AGVs, four CNC machine, and four pairs of input/
output buﬀers for the CNC machines.between the inputs (buﬀer sizes, arrival rate of parts, and AGV’s speed) and the desire output with only a small
amount of data (12 data). With regarding ‘‘No’’. in Table 5 as the ordinal number, we can treat these data like
40 sequential events; the ﬁrst 12 data imply that occurred in the early stages. Due to the collected data con-
taining all possible values of the scheduling rules when operating GIKDE, we select the ﬁrst 12 data for the
training set.
With the assumption that these three variables are independent, the work to estimate f(x1,x2,x3jC) can be
transformed to estimate f(x1jC)f(x2jC)f(x3jC) for constructing the sources of virtual sample sets. The processes
of yielding f^ ðx1jCÞ; f^ ðx2jCÞ, and f^ ðx3jCÞ through GIKDE are similar to the steps demonstrated in the last
case. Three kinds of weight series lead to three estimates of f(x1,x2,x3jC), and therefore we generated the cor-
responding virtual data sets according to f^ ðx1; x2; x3jCÞ tied in each type of weight series. ANN is the learning
tool for acquiring scheduling knowledge from the virtual data sets and the setting of layout for dealing with
this case includes one hidden layer and four hidden neurons, as shown in Fig. 5.
For testing the performance of the knowledge learned from the virtual data sets, we apply it to the rest 28
data and acquire the learning accuracy presenting the percentage of correct classiﬁcations in the testing data.
Table 5
The whole scheduling data set (Li and Lin, 2006): X1 for the buﬀer size, X2 for the arrival rate of parts, X3 for the speed of automobile
general motors, and C for the best scheduling rule
No. X1 X2 X3 C No. X1 X2 X3 C
1 9 30 50 SPT(2) 21 9 20 100 EDD(3)
2 8 20 50 FCFS(1) 22 10 30 110 FCFS(1)
3 9 30 60 SPT(2) 23 7 20 110 FCFS(1)
4 8 30 50 SPT(2) 24 7 20 60 SPT(2)
5 10 20 60 SPT(2) 25 8 30 70 FCFS(1)
6 10 30 50 SPT(2) 26 8 30 80 FCFS(1)
7 10 30 60 SPT(2) 27 9 40 60 SPT(2)
8 10 30 70 SPT(2) 28 13 20 90 EDD(3)
9 8 30 50 SPT(2) 29 11 30 90 SPT(2)
10 7 20 60 SPT(2) 30 10 40 70 FCFS(1)
11 7 20 60 SPT(2) 31 10 30 120 EDD(3)
12 7 30 50 EDD(3) 32 9 20 100 EDD(3)
13 7 30 60 SPT(2) 33 7 20 110 FCFS(1)
14 7 30 70 EDD(3) 34 10 20 70 SPT(2)
15 9 20 60 SPT(2) 35 10 30 70 SPT(2)
16 7 30 60 EDD(3) 36 10 20 80 SPT(2)
17 7 20 70 EDD(3) 37 11 30 90 SPT(2)
18 7 20 70 EDD(3) 38 8 40 90 SPT(2)
19 10 20 60 SPT(2) 39 7 30 90 SPT(2)
20 10 20 70 SPT(2) 40 9 20 100 EDD(3)
that the GIKDE results in more preferable performance than the IKDE. Even though the learning accuracies
D.-C. Li, Y.-S. Lin / European Journal of Operational Research 184 (2008) 169–184 183vary by the selection of weighted series, the results in Table 4b show the GIKDE are more suitable for the
time-dependent data sets then the IKDE.
The diﬀerence between learning accuracies (Tables 4a, 4b) can be ascribed to the inﬂuence coming from
diﬀerent types of weight series. Noninformative types treat the time dependent data as a set of random
samples. In other words, when the process data comes into a stable situation, a noninformative weight series
can help GIKDE acquire information without considering the time order. Thus, GIKDE with noninformative
weight series forms a special case—IKDE. Martingale types show distinct results because they always take the
most recent information into account. GIKDE with Martingale weight series can help with learning at earlier
stages since there is not much data far from the present.
The task of f^ ðctþ1Þ resulted in the early stages not to directly predict C11 but to roughly estimate the pro-
portion of each class among the virtual sample set. That is, a virtual sample set contains virtual data of each
class in unequal size and its composing rates are determined based on f^ ðctþ1Þ.
As to the location function of GIKDE, G(X1, . . . ,Xi)  E[Xi+1jX1, . . . ,Xi] is the deﬁnition of location func-
tions while E[Xi+1jX1, . . . ,Xi] = bX, so that the AR models can be a common case of various types. For the
parameter j of AR(j), the stages considered must be more recent when estimating the next input with a small
p and with large p might be improper. Moreover, p is always ﬁnite and small since we ﬁnd the estimator using
small data sets in the early stages. One kernel can cover more than one data drawn at the same time and here
we discuss only the simplest case for convenience. In data collection processes, there may exist cases wherein
two or more data may come in at the same time. An easy way to treat this situation is to take average X i at
time i and ﬁnd GðX 1; . . . ;X tÞ.
When a sequential prediction is necessary in the learning task, it needs to iterate the same algorithm, includ-
ing the GIKDE and VSG. For example, if we attempt to predict (X11,C11), (X12,C12) and (X13,C13), we can
put ðX 11;C11Þ (available after the processing algorithm) into the prediction process of (X12, C12) and so on.
The sequential prediction requires a data set covering enough stages for a more reliable result.
The parameter d indicates the number of stages that user would trace back. This user-deﬁned parameter
gives researchers the option to include the amount of past information. A large d does bring more informa-
tion, yet small d lessens the computational complexity.
Computational complexity suggests the possible time cost in worst cases. We all know that an operation
can lead to time-consuming work when the involved data set is large. Since we focus on the case of small data
sets, any order in big O can only slightly aﬀect a bitty n. It is worthy to take little computational cost for a
signiﬁcant accuracy lifting.
Acknowledgement
This research is supported by the National Science Council Taiwan, ROC.
References
Alcock, R.J., Manolopoulos, Y., 1999. Time series similarity queries employing a feature-based approach. In: Seventh Hellenic Conference
on Informatics. Ioannina, Greece.
Bre´maud, P., 1999. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues. Springer, New York.5. Discussion
In this study, we established a learning algorithm for small data sets. The GIKDE provides ﬂexibility for
users to adjust the time intensity according to the demand from data. In the experimental study, it is observedthan IKDE for production management. It also observed that neither directly using ANN nor Logistic regres-
sion perform well since only insuﬃcient data are available.
The gradual decrease of the prediction performance occurs as the size of virtual samples increases, espe-
cially when virtual samples have a size exceeding two times the number of the original available data sets.Fan, J., Yao, Q., 2003. Nonlinear Time Series—Nonparametric and Parametric Methods. Springer, New York.
