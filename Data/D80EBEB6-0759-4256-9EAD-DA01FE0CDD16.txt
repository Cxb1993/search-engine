[7] select the key frame set and then calculate the radial
project vectors to form the 180 vectors for the subsequent
DCT for content matching. Coskun et al. [8] propose to
use the spatial-temporal transform to explicitly employ the
temporal data. They apply the spatial and temporal domain
normalization and then compute the 3D-DCT to include the
temporal information for constructing the hash.
In this research, we aim at developing an effective and
efficient content-based copy detection scheme, which will
employ both the spatial and temporal features. The proposed
scheme is detailed in Section II, followed by the experimen-
tal results in Section III and the conclusion in Section IV.
II. THE PROPOSED SCHEME
Fig. 1 shows the framework of the proposed scheme.
To begin with, the shot-change detection will be applied
to the original videos as the first dimension reduction and
then the key frames will be selected from these detected
shot-change frames for further processing. The data in the
compressed bit-streams will be employed to avoid expanding
the videos to the full resolution, given that the original
videos have already been compressed by such video codec
as MPEG-1 or 2. Next, the spatial features will be extracted
from the key frames by using the SVD and VQ techniques
as the second dimension reduction. The corresponding VQ
indices of frames will be used as the indices for content
retrieving. The lengths of shots will be recorded as the
temporal features for content matching. We can also term
the temporal feature as the “signature” since it represents
a unique property of a video. In other words, we archive
both the extracted spatial and temporal features of the videos
into the database. For an investigated video, we will apply
the same procedures to obtain its spatial/temporal features,
which will be matched with those stored in the database
to determine whether the investigated video is a segment
belonging one of the original videos.
A. The shot-change detection
As mentioned before, the compressed-domain approach
is adopted to achieve the efficiency in our scheme. We
first extract the I-frames, Ii and Ij , from the two adjacent
GOP’s, GOPi and GOPj , respectively. We compute the
color difference by
Dc(Ii, Ij) =
K∑
k=1
∣∣∣∣DCBIi
k
−DC
B
Ij
k
∣∣∣∣ , (1)
where BIik (B
Ij
k ) are the k
th 8 × 8 block of Ii (Ij) and K
is the number of blocks in a frame. If Dc(Ii, Ij) is larger
than a threshold TI , a scene change is identified as occurring
between Ii and Ij . It should be noted that only DC values
are used. Next, we calculate the percentage of macroblocks
that are intra-coded, denoted by Pr(I)p , in all the predicted
frames in GOPi. The predicted frame with the largest Pr
(I)
p ,
Shot-change detection Key-frame selection
Temporal feature 
generation (Signature)
Spatial feature 
generation (VQ indices)
Set of shot-change frames Set of key-frames
Signature of 
1, 2, …, N
Indices of
1, 2, …, N
Video 1 VQ indices of 1 Signature of 1
Video 2 VQ indices of 2 Signature of 2
… … …
Video N VQ indices of N Signature of N
Archive VQ indices & 
Signature in Database
Video 1
Video 2
…
Video N
VQ indices & 
Signature
Generation 
VQ indices & 
Signature
Generation 
Signature
Indices
Investigated 
Video
Search
Retrieve
signature
Signatures
match?
An illegal 
duplicate?
Figure 1. The framework of the proposed scheme.
denoted by Pm, is chosen and compared with the other
threshold TP . Pm will be chosen as the frame of scene
change if Pr(I)pm > TP and its DC histogram is different from
that of Ii. Otherwise, Ij will be chosen as the candidate.
The drawback of the compressed-domain methodology is
the possible lack of accuracy, especially when such effects as
fading and dissolving happen between two shots. However,
what we need is the representative frames of videos so these
frames without hard cuts may be ignored in our applications.
The major challenge here is to select the same frames if
the investigated video is a transcoded copy of an original
one. According to our experience, as there are many scene-
change frames in ordinary videos, a more restrict threshold
may help to achieve a stable scene change detection.
B. The key-frame selection
Among the extracted scene-change frames, we will select
the key frames that can better represent the video. In our
opinions, the same scene may appear in continuous shots.
For example, during the conversation of two persons in a
video, the shots on each person’s face may be shown peri-
odically. We will eliminate these shot-change frames so that
the computation on content matching can be further reduced.
Given the shot-change frame set: S = {Si|i = 1 . . . N}. We
will calculate the quartiles from their DC values to obtain
a vector [Q1i , Q
2
i , Q
3
i ] for Si. Then we calculate the sum of
difference of the quartiles of the nearby shots, Si and Sj by
Dq(Si, Sj) =
3∑
k=1
|Qki −Qkj |. (2)
Lkp-1 Lkp Lkp+1 Lkp+2 Lkp+3
t
Lkp-2
Lk(t)
Lkp
Lkp+3
Figure 3. The construction of Lk(t).
The shot lengths will be stored in the database with the
positions of key frames and their VQ indices. The storage
cost is limited as the total stored data volume is around 15K
bytes for these video data. Then we select two 10-minute
videos, one with 11 key frames and the other with 12 key
frames, as the investigated videos, which will be matched
with all the 10 videos in the database. It is worth noting that
the computation on the spatial/temporal feature extraction
and searching in the database can be done very efficiently
and cost less than 0.1 second on an Intel P4 machine at
2.2GHz and with 2G RAM. We found that ρ in Eq.(7)
is always equal to 1 when these two investigated videos
are matched with their archived data while smaller than
−0.7 when the videos are not the same but some matches
in VQ indices exist. The correct scene change detection
helps to make all the matched VQ indices generate positive
responses, i.e. 11 matches and 12 matches in the two videos,
although we only require 3 matches in our scheme. In fact,
the spatial features help to reduce the number of necessary
matching processes significantly while the temporal feature
matching can produce very low false identification rates.
In addition, we crop the two investigated videos and then
transcoded the segments with MPEG-1 with a lower bit-
rate equal to 2/3 the original bit-rate, for producing smaller
tested videos. Some over-segmentations and scene-change
detection misses do happen in the transcoded video and
the changes of shot lengths in certain parts of the videos
will contribute negative values for calculating ρ. However,
there are still quite many matched shot lengths since the
video contents are not affected much by the transcoding
process. These data will yield large positive values so that
the response values ρ higher than 0.65 are observed in
all the matched cases and thus help us identify whether
the investigated videos are segments of the videos whose
information is archived. Therefore, the experimental results
show that our scheme is quite reliable even when the videos
are transcoded.
IV. CONCLUSION AND FUTURE WORK
We proposed an efficient content-based copy detection
scheme based on the spatial and temporal feature extraction
and matching. The key-frames are selected to generate the
spatial features, which are used as the anchor points for the
temporal feature matching. Future work will be increasing
the reliability of the fast scene-change detection and enabling
the spatial features to survive such geometrical modifications
as cropping, scaling and changes of aspect ratio, which may
be applied on transcoded digital videos.
REFERENCES
[1] W. Hsu, T. S. Chua, and H. K. Pung, “An integrated color-
spatial approach to content-based image retrieval,” in Proc.
ACM Multimedia, 1995.
[2] Y. Zhuang, Y. Rui, T. S. Huang, and S. Mehrotra, “Adaptive
key frame extracting using unsupervised clustering,” Proc. of
IEEE Int Conf on Image Processing, pp. 866–870, 1998.
[3] Wolf, “Key frame selection by motion analysis,” IEEE Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing, pp. 1228–1231, 1996.
[4] T. Wang, Y. Wu, and L. Chen, “An approach to video key-
frame extraction based on rough set,”Multimedia and Ubiqui-
tous Engineering, 2007. MUE ’07. International Conference
on, pp. 590–596, 2007.
[5] T. Liu, H.-J. Zhang, and F. Qi, “A novel video key-
frame-extraction algorithm based on perceived motion energy
mode,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 13, pp. 1006–1013, 2003.
[6] P.-H. Wu, T. Thaipanich, and C.-C. J. Kuo, “A suffix array
approach to video copy detection in video sharing social
networks,” in IEEE International Conference on Acoustics,
Speech, and Signal Processing, Taipei, Taiwan, Apr. 2009.
[7] C. D. Roover, C. D. Vleeschouwer, F. Lefebvre, and B. Macq,
“Robust video hashing based on radial projections of key
frames,” IEEE Transactions on Signal Processing, 2005.
[8] B. Coskun, B. Sankur, and N. Memon, “Spatio-temporal
transform based video hashing,” IEEE Transactions on Mul-
timedia, pp. 1190–1208, 2006.
[9] P.-C. Su, C.-C. Chen, and H.-M. Chang, “Towards effective
content authentication for digital videos by employing feature
extraction and quantization,” in IEEE Trans. on Circuits and
Systems for Video Technology, to appear.
[10] I. Katsavounidis, C.-C. Kuo, and Z. Zhang, “A new initial-
ization technique for generalized Lloyd iteration,” in IEEE
Signal Processing Letters, vol. 1, Oct. 1994, pp. 144–146.
condition and variation of vehicle colors. Then, the multiple instance 
learning is applied to more accurately locate the desired vehicle image. 
After these two stages, the similarity of the vehicle images will be 
computed and then sort the final results. The paper seems interesting as it 
can serve as a pretty useful tool to help identify the vehicles by using color 
information only. 
 “Towards Efficient Copy Detection for Digital videos by Using Spatial and 
Temporal Features,” by Chung-Chi Tsai, Chin-Song Wu, Ching-Yu Wu, and 
Po-Chyi Su, National Central University, Taiwan. 
I presented this paper for my colleague. We would like to provide a 
computationally efficient approach for the problem video copy detection. 
In this research, we use not only the visual (spatial) features, but also the 
temporary feature to increase the accuracy of video matching. The spatial 
feature is computed by the Singular Value Decomposition (SVD) of a 
down-sampled frame, and the temporary feature is extracted by the 
proposed shot change detection method. I was asked a few questions by 
the scholars from Japan after the session. 
 “Driving Situations Understanding through Content Rhythm Analysis,” by 
Yu-Dun Lin, Wen-Yu Tseng, Cheng-Jie Hsum, and Chia-Hung, Yeh, National 
Sun Yat-Sen University, Taiwan. 
This paper aims at identifying on-road vehicle events, like lane changing, 
vehicle passing, safe distance, and zig-zag driving, etc. They extract the 
content rhythm by temporal scanning and recording the continuous frames. 
Based on the recorded content rhythm, some frequent traffic events can be 
detected efficiently. 
 “Ear Detection Based on Arc-Masking Extraction and AdaBoost Polling 
Verification,” Hao-Chung Shih, Chian C. Ho, Hsuan T. Chang, and Chin-Song 
Wu, National Yunlin University of Science and Technology, Taiwan 
Because the human ears have different shapes and characteristics, the idea 
of this paper is to detect the human ears for future recognition usage. First 
of all, the arc-masking is applied to the original image to extract the edge 
map. And the multi-layer mosaic will be applied to obtain the ear 
candidates. Then Ada-boost pooling verification will choose the most likely 
ear candidate as the final output. 
無衍生研發成果推廣資料
期刊論文 0 1 100% 
A Practical 
Design of Video 
Copy Detection 
Scheme by 
Employing 
Spatial and 
Temporal Feature 
Matching＇＇ in 
preparation. 
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
Chung-Chi Tsai, 
Po-Chyi Su and 
Chin-Song 
Wu,＇＇ Towards 
Efficient Copy 
Detection for 
Digital Videos 
by Using Spatial 
and Temporal 
Features.＇＇ 
The Fifth 
International 
Conference on 
Intelligent 
Information 
Hiding and 
Multimedia 
Signal 
Processing, 
Kyoto, Japan, 
Sep. 12-14, 
2009. 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 10 10 100%  
博士生 1 1 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
 
