green. Using a microarray scanner these colours 
are automatically stored as real positive and 
negative numbers that can be used as the data of 
array. DNA microarray technology thus provides 
biologists with the ability to measure the relative 
expression values of thousands of genes in a 
single experiment by measuring the colour of the 
fluorescence. Here the term expression values 
will be explicitly used to indicate the relative 
expressions of the genes in the test sample 
versus the reference sample, possibly normalised 
by subtracting the mean and dividing by the 
standard deviation of some subset of the 
measurements. This is the data that can be used 
as an input to mathematical clustering 
procedures.  
Microarrays have revolutionized the study 
of genome by allowing researchers to study the 
expression of thousands of genes simultaneously 
for the first time. The ability to simultaneously 
study thousands of genes under a host of 
differing conditions presents and immense 
challenge in the fields of computational science 
and data mining. In order to properly 
comprehend and interpret expression data 
produced by microarray technology, new 
computational and data mining techniques need 
to be developed. The analysis and understanding 
of microarray data includes a search for genes 
that have similar or correlated patterns of 
expression. Among the various frameworks in 
which pattern recognition has been traditionally 
formulated, statistical approach, neural network 
techniques, and methods imported from 
statistical learning theory are among those that 
have been applied in microarray data analysis [1]. 
Various approaches have been used in  
microarray data analysis are described as 
follows. 
Simple approaches: In order to understand the 
connection between microarray data and 
biological function, one could ask the following 
fundamental question: how do expression levels 
of specific genes, or gene-sequences, differ in a 
control group versus a treatment group? In other 
words, in a controlled study, if a specific 
biological function (condition) is present, how 
does expression levels change when the function 
(condition) is turned off (absent) (or visa versa). 
This line of questioning could be useful in, for 
instance, determining the effect of a specific 
genomic treatment of a disease. In such a case, 
the question would be: has the treatment 
changed the expression level(s) of specific 
(target) gene(s)/gene-sequence(s) to noticeably 
different levels. If so, then have the changes in 
expression levels resulted in elimination (or 
alleviating the symptoms) of patient’s condition? 
If the answers to the above questions are “yes”, 
then a correlation between the specific 
genes/genesequences that show changed levels, 
and the biological function, can be drawn. We 
believe that a better and more biologically 
relevant method of analysis would be to consider 
expression patterns of related (or neighboring) 
genes to determine the “on” or “off” state of the 
gene currently under observation. The folding 
technique as it is, does not allow this type of 
analysis. Similar to the Fold approach, T-test is 
another simple method applied in gene 
expression analysis. It manipulates logarithm of 
expression levels, and requires comparison 
computation against the means and variances of 
both treatment and control groups. The 
fundamental problem with T-test is that it 
requires repeated treatment and control 
experiments, which is both tedious and costly. 
As a result, small number of experimental 
repetition could affect the reliability of the 
mean-based approach [2,3]. 
Karhunen–Loe`ve expansion :  
Karhunen–Loe`ve expansion [4], as it is known 
in pattern recognition, is also known as principal 
component analysis (PCA), or singular value 
decomposition (SVD) [5] in statistics. Use of 
SVD in microarray data analysis has been 
suggested by various researchers [6-11]. PCA is 
a linear mathematical technique that finds base 
vectors that expand the problem space (gene 
expression space). These vectors are called 
principal components (PCs). In expression data 
analysis, the vectors are also called mean 
expression vectors, eigengenes. 
Bayesian Belief Networks (BBN): Another 
sophisticated approaches, Bayesian probabilistic 
framework has been suggested to analyze 
expression data [12-14]. An alternative approach 
to the SVD methodology described above is to 
use prior knowledge of the regulatory network's 
architecture to design competing models then 
use Bayesian belief networks to pick the model 
that best fits the expression data [15]. Gifford 
and co-workers have used this approach to 
distinguish between two competing models for 
galactose regulation [16]. Friedman and 
co-workers have used Bayesian networks to 
analyze genome-wide expression data in order to 
identify significant interactions between genes in 
a variety of metabolic and regulatory pathways 
[17,13]. 
K-means Clustering (Partitioning): 
K-means is a divisive clustering approach. It 
partitions data (genes or experiments) into 
groups that have similar expression patterns. k is 
the number of clusters (also sometimes called 
buckets, bins, or classes) into which the user 
believes the data should fall. The number k is an 
input value given to the algorithm by the user. 
The k-mean clustering algorithm is a three steps 
process. In the first step, the algorithm randomly 
“One way to address it is to use a fully Bayesian 
estimation procedure. It might not work with 
data having temporal structure, say gene 
expression of the same population of cells 
measured at a different number of time points” 
[25]. 
 
Gene Shaving:  
Gene shaving is a recently developed and 
popular statistical method for discovering 
patterns in gene expression data. The original 
algorithm uses the PCA algorithm and was 
proposed by Hastie, Tibshirani et al. [26]. Later 
variations are under development, some of 
which include the replacement of the PCA step 
with a nonlinear variety. Gene shaving is 
designed to identify clusters of genes with 
coherent expression patterns and large variation 
across the samples. Using this method, the 
authors have successfully analyzed gene 
expression measurements made on samples from 
patients with diffuse large B-cell lymphoma and 
identified a small cluster of genes whose 
expression is highly predictive of survival [26].  
There are two varieties to the original 
shaving algorithm: supervised (partially 
supervised), or unsupervised. In supervised and 
partially supervised shaving, available 
information about genes and samples (outcome 
measure, known properties of genes or sample, 
or any other a priori information) can be used to 
label their data as a means to supervise the 
clustering process and ensuring meaningful 
groupings. Gene shaving also allows genes to 
belong to more than one cluster. These two 
properties (the ability to supervise and multiple 
groupings for the same gene) make gene shaving 
different from most hierarchical clustering and 
other widely used methods for analyzing gene 
expression data.  
The most prominent advantage of the 
shaving methods proposed by Hastie, Tibshirani 
et al. were best expressed by the authors 
themselves: “(shaving methods) search for 
clusters of genes showing both high variation 
across the samples, and coherence (correlation) 
across the genes. Both of these aspects are 
important and cannot be captured by simple 
clustering of genes or setting threshold of 
individual genes based on the variation over 
samples”[27].  
The drawback of this method is the 
computational intensity of the algorithm. The 
shaving process requires repeated computation 
of the largest principal component of a large set 
of variables. Some variant approaches should 
consider replacement algorithms for SVD that 
are less computationally intensive.  
Support Vector Machine (SVM):  
SVM is a supervised machine learning 
technique in the sense that vectors are classified 
with respect to known reference vectors. “SVM 
solve the problem by mapping the 
gene-expression vectors from expression space 
into a higher-dimensional feature space, in 
which distance is measured using a 
mathematical function known as a kernel 
function, and the data can then be separated into 
two classes” [18]. Gene expression vectors can 
be thought of as points in an n-dimensional 
space. For microarray analysis, sets of genes are 
identified that represent a target pattern of gene 
expression. The SVM is then trained to 
discriminate between the data points for that 
pattern (positive points in the feature space) and 
other data points that do not show that pattern 
(negative points in the feature space). “With an 
appropriately chosen feature space of sufficient 
dimensionality, any consistent training set can be 
made separable”. [25] SVM is a linear technique 
in that it uses hyperplanes as separating surfaces 
between positive and negative points in the 
feature space. Specifically, SVM chooses the 
hyperplane that provides maximum margin 
between the plane surface and the positive and 
negative points. This feature provides a 
mechanism to avoid over fitting. Once the 
separating hyperplanes have been selected, “the 
decision function for classifying points with 
respect to the hyperplane only involves dot 
product between points in the feature space”, 
which carries a low computational burden. [27]  
Because the linear boundary in the feature 
space maps to a nonlinear boundary in the gene 
expression space, SVM can be considered as a 
nonlinear separation technique. The important 
advantage of SVM is that it offers a possibility 
to train generalizable, nonlinear classifiers in 
high-dimensional space using a small training 
set. For large training sets, SVM typically selects 
a small support set that is necessary for 
designing the classifier, thereby, minimizing the 
computational requirements during testing. 
Furey, et al. [28] praises SVM as follows: “It 
(SVM) has demonstrated the ability to not only 
correctly separate entities into appropriate 
classes, but also identify instances whose 
established classification is not supported by the 
data. It performs well in multiple biological 
analyses, having remarkable robust performance 
with respect to sparse and noisy data”.  
One of the drawbacks of SVM is its 
sensitivity to the choice of a kernel function, 
parameters, and penalties. For instance, if the 
kernel function is not appropriately chosen for 
the training data, SVM may not be able to find a 
separating hyperplane in feature space. [27] 
Choosing the best kernel function, parameters 
and penalties for the SVM algorithm can be 
difficult in many cases. Because of the 
learning algorithm. Then, the gene expression 
data for each cluster based on the DFC algorithm 
are regarded as training data set and feed into 
feedforward neural networks for function 
approximation. For the distance-based outlier 
detection approach, we will be use statistics 
theory – robust estimation approach, 
M-estimators and GM-estimators etc [36]. 
Finally, DB outlier detection approach is applied 
into the microarray data of ovarian cancer. 
本計畫的主要目的是 
我們將藉由前一年個別型計劃所提出 
－ 動態函數聚集法則方法去對於連續取樣之
微陣列資料所得到基因表示資料取得其聚集
分布。對所得到的每一聚集之 Gene expression 
data，利用我們先前已發表 Support Vector 
Interval Regression Networks (SVIRNs) 建立
其每一聚集之基因表示區域函數 (Gene 
expression interval function) 。然後，透過 
SVIRNs 對每一基因表示區域函數做區域回
歸分析( Interval Regression Analysis)。首先，
我們會先建立正常的 Ovarian 微陣列資料之
基因表示區域函數。然後，根據有限 Ovarian 
cancer 的微陣列資料所得到基因表示資料，利
用此資料與其相對應基因表示區域函數比
較，判斷此基因是否為 Ovarian cancer 的特殊
基因。因為連續取樣之 Ovarian cancer 的微陣
列資料取得有限，所以很難正確取得其聚集之
分布。因此，我們利用正常的 Ovarian 微陣
列資料之基因表示區域函數與從 Ovarian 
cancer 微陣列資料取得基因作一比對。找出
Ovarian cancer 的特殊基因。此項做法與傳統
做法－利用聚集法則做法不同。  
在軟體發展過程，藉由Matlab 軟體的分析去
驗證所發展出來的 Support Vector Interval 
Regression Networks for Clusters of Interval 
Regression Analysis之正確性與適用性。同時，
對於找出Ovarian cancer 的特殊基因問題－利
用比較常用的聚集方法與我們所提出方法之
差異性。 
三、研究方法及成果 
In the second year, a novel approach, 
termed as support vector interval regression 
networks (SVIRNs), is applied to constructing 
an interval regression (functions) for each cluster. 
Based on the DFC algorithm, we can be obtains 
properly clusters for the gene expression data of 
ovarian microarray. Due to the ovarian cancer 
microarray are hardly obtained (i.e. only a few 
ovarian cancer of microarray data can be 
obtained), the clustering results for a limit 
microarray data are insignificance. When the 
interval regression of each clusters are 
constructed, the special gene are easily found by 
the gene expression data of ovarian cancer 
microarray. That is, whether the gene expression 
data of ovarian cancer can be fall into the 
interval regression for the same gene or not. If 
gene expression data is fall into interval 
regression for the same gene, we can say that the 
genes are not important gene for ovarian cancer. 
Otherwise, we can say that the genes are 
important gene for ovarian cancer. The support 
vector interval regression networks (SVIRNs) is 
simply stated as follows. 
For the interval regression analysis, the 
proposed structure of SVIRNs can be 
constructed by the support vector regression 
(SVR) approach, where +SVRN  is the upper 
side of SVIRNs and -SVRN  is the lower side of 
SVIRNs which is shown in figure (1). In the 
SVIRNs, the SVR approach provides the initial 
structure of SVIRNs ( )xf r , the maximum ( )xf r0  
and minimum ( )xfo r  limitation of interval 
regression. Due to the SVR approach is 
equivalent to solving a linear constrained 
quadratic programming problem under the fixed 
structure of SVR, the number of hidden nodes 
and adjustable parameters are easy obtained. 
Based on the initial structure of SVIRNs ( )xf r , 
the output of SVIRNs push up to the upper side 
data that not exceed the maximum limitation of 
interval regression and pull down to the lower 
side data that also not exceed the minimum 
limitation of interval regression by 
independently learning algorithms. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure1 : The structure of SVIRNs is shown. 
 
In order to determine the maximum limitation 
and minimum limitation of interval regression, 
the SVR approach with proper ε  can provide 
∑
+
1w
+
2w
+
Lw
xr
1
sSVRN +
)(xf r+
∑
−
1w
−
2w
−
Lw
1
sSVRN−
)(xf r−
+b
−b
+
1G
+
2G
+
LG
−
1G
−
2G
−
LG
SVRNs
  
 
 
 
 
 
 
 
 
 
Figure 4. A variable number of up-regulated 
genes and down-regulated genes.with SVIRNs 
for the OVCA III. 
四、結論 
In this project, we develop a new SVIRNs 
for the ovarian cancer microarray data analysis. 
This new approach can provide variable gene 
selection for the up-regulated genes and 
down-regulated genes on the microarray data. 
Besides, the proposed method can also overcome 
the block effect on microarray data. Simulation 
results are provided to show the validity and 
applicability of the proposed approach. 
五、參考文獻 
[1] Szabo A.et alVariable selection and pattern 
recognition with gene expression data 
generated by the microarray 
technology.Math Biosci (2002) 176(1), 
71-98. 
[2] Baldi, P. et al. A Bayesian framework for the 
analysis of microarray expression data: 
regularized t-test and statistical inference of 
gene changes (2001) Bioinformatics, 17(6) , 
509-519. 
[3] Pan, W. A comparative review of statistical 
methods for discovering differentially 
expressed genes in replicated microarray 
experiments (2002) Bioinformatics 18(4), 
546-54. 
[4] Mallat, S. G. (1999) A Wavelet Tour of 
Signal Processing (Academic, San Diego), 
second edition. 
[5] Anderson, T. W. (1984) Introduction to 
Multivariate Statistical Analysis (Wiley, NY) 
second edition. 
[6] Alter, O. et al. Singular Value 
Decomposition for Genome-wide Expression 
Data Processing and Modeling (2000) Proc. 
Natl. Acad. Sci. USA 97, 10101-10106. 
[7] O. Alter, P.O. Brown and D. Botstein, 
Singular value decomposition for 
genome-wide expression data processing and 
modeling. Proc Natl Acad Sci USA 97 
(2000), pp. 10101-10106. 
[8] N.S. Holter, A. Maritan, M. Cieplak, N.V. 
Fedoroff and J.R. Banavar , Dynamic 
modeling of gene expression data. Proc Natl 
Acad Sci USA 98 (2001), pp. 1693-1698. 
[9] N.S. Holter, M. Mitra, A. Maritan, M. 
Cieplak, J.R. Banavar and N.V. Fedoroff, 
Fundamental patterns underlying gene 
expression profiles: simplicity from 
complexity. Proc Natl Acad Sci USA 97 
(2000), pp. 8409- 8414. 
[10] Raychaudhuri S, Stuart JM, Altman RB. 
Principal components analysis to 
summarize microarray experiments: 
application to sporulation time series. Pac 
Symp Biocomput 2000, 455-466. 
[11] Wu, C., Berry, M., Shivakumar, S. and 
Mcarty, J. 1995. Neural networks for 
full-scale protein sequence classification: 
sequence encoding with singular value 
decomposition. Machine Learning.21: 
177-193. 
[12] N. Friedman, M. Linial, I. Nachman and D. 
Pe'er, Using Bayesian networks to analyze 
expression data. J Comput Biol 7 (2000), 
pp. 601- 620. 
[13] A. Drawid and M. Gerstein , A Bayesian 
system integrating expression data with 
sequence patterns for localizing proteins: 
comprehensive application to the yeast 
genome. J Mol Biol 301 (2000), pp. 1059- 
1075. 
[14] D.K. Gifford , Blazing pathways through 
genetic mountains. Science 293 (2001), pp. 
2049- 2051. 
[15] Hartemink AJ, Gifford DK, Jaakkola TS, 
Young RA. Using graphical models and 
genomic expression data to statistically 
validate models of genetic regulatory 
networks. Pac SympBiocomput 2001, 
422-433. 
[16] D. Pe'er, A. Regev, G. Elidan and N. 
Friedman, Inferring subnetworks from 
perturbed expression profiles. 
Bioinformatics 17 Suppl 1 (2001), pp. 
S215-S224. 
[17] Quackenbush, J. Computational Analysis of 
0 2000 4000 6000 8000 10000
-4
-2
0
2
4
6
8
Portrait No.
G
en
e
s 
E
xp
re
ss
io
n 
M
ag
ni
tu
de
OVCAIII
Gene 
 
 
 
Ged 
