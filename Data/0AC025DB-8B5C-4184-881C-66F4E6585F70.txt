2 
 
 
研究方法及步驟 
圖 1 是本研究之網路儲存系統架構，由三個部分所組成；一個客戶端(Client)、
五個網路儲存(Network Storage)以及一個中介資料伺服器(Metadata Server)，彼此
之間可以透過網路(網際網路、區域網路以及無線網路)進行連線。客戶端會發出
各種檔案存取操作，例如；讀取(Read)、寫入(Write)、創造(Create)或是查詢(Lookup)
等等。每一個網路儲存就是一部儲存裝置，具有特定儲存能力(磁碟陣列功能，
例如；RAID 0，RAID 1 或是 RAID 5 等等)、儲存容量、記憶體大小以及 CPU
運算能力。經由網路互連提供客戶端各種儲存服務且彼此之間相互合作以組成、
維護以及協調儲存叢集之運作。中介資料伺服器負責整個網路儲存系統的儲存資
源配置與規劃，藉由取得每一個網路儲存的狀態，例如；剩餘儲存容量、可用網
路頻寬、系統負載、運算能力以及擔任主要網路儲存與附屬網路儲存數目等資訊
建構出符合客戶端需求之儲存叢集。藉由動態地調整儲存叢集內的網路儲存數目，
提供客戶端虛擬化儲存資源，意謂儲存容量沒有上限。特別地，中介資料伺服器
僅負責網路儲存系統整體儲存資源配置與管理，至於每一個儲存叢集內的儲存資
源配置與動態調整則是交由主要網路儲存負責。藉由階層式管理，不僅可以有效
地降低中介資料伺服器負載，更可以即時地動態調整儲存叢集來確保每個客戶端
的儲存服務品質不受影響。 
 
圖 1 網路儲存系統架構圖 
圖 2(A)是 Create 請求概要流程圖。在網路儲存系統中，中介資料伺服器會
依據客戶端的儲存服務品質需求建構出一個對應的儲存叢集。在本例中儲存叢集
會由一個主要網路儲存以及兩個附屬網路儲存組合而成。接下來中介資料伺服器
會把儲存叢集相關組態(儲存容量配置、儲存服務品質需求、網路連線資訊以及
擔任主要或是附屬網路儲存)傳送給這三個網路儲存。在每一個網路儲存完成儲
存叢集組態設定之後，客戶端會從中介資料伺服器取得主要網路儲存的網路資訊，
例如；網路(IP)位址，傳輸協定(TCP 或是 UDP)以及通訊埠(Port Number)等等來
建立網路連線。客戶端就可以透過所建立的網路連線傳送 Create 請求到主要網
路儲存，主要網路儲存在接收到 Create 請求之後；就會決定在儲存叢集中哪個
4 
 
 
圖 2(B) Lookup 請求之概要流程圖 
圖 2(C)是 Read/Write 請求概要流程圖。客戶端經由網路連線將 Read/Write
請求傳送到主要網路儲存，主要網路儲存會根據 Read/Write 請求依序尋找快取、
遠端儲存庫，以及檔案系統。由於快取資料更新、遠端儲存庫資料不完整以及無
法在檔案系統找到該檔案以至於無法確認負責 Read/Write 請求之網路儲存，這時
候主要網路儲存會把該Read/Write請求轉發至儲存叢集內所有的附屬網路儲存。
當接收到 Read/Write 請求時，附屬網路儲存只需要尋找快取以及本身的檔案系統
即可，無須執行遠端儲存庫查詢。一旦搜尋到該檔案，就會經由檔案系統執行檔
案讀寫操作，然後傳送回應訊息至客戶端。最後該附屬網路儲存會更新本地儲存
庫並回傳訊息給主要網路儲存以更新該檔案之相關儲存資訊於遠端儲存庫。相反
地，如果收到 Read/Write 請求之附屬網路儲存無法找到該檔案，就忽略該請求而
不進行任何處理。 
Client
Network 
Storage
Network 
Storage
Network 
Storage
Network 
Storage
Network 
Storage
Metadata 
Server
A01
A04
A03A02
A05
 
圖 2(C) Read/Write 請求之概要流程圖 
圖 3 是 Create 請求之細部流程圖，也就是圖 2(A)完整的請求處理流程。一
開始客戶端會與儲存服務供應商(Storage Service Provider)簽定服務等級合約以
確保儲存服務品質包括網路頻寬大小、儲存容量、資料可獲得性或是存取延遲上
限等等。中介資料伺服器就會根據客戶端的儲存服務品質需求建構出一個專屬的
儲存叢集，該儲存叢集內由一個主要網路儲存以及一到多個附屬網路儲存組成，
6 
 
Master Network Storage Receives a CREATE Request (Directory 
Name, File Name) from a Client
Metadata Server Configures a Storage Cluster (Master and Slave Network Storages) 
Determine a Network Storage to Handle the CREATE Request
Transfer the CREATE Request to the Slave 
Network Storage 
Is the Master Network 
Storage Selected?
YES
NO
Create the File on the Designate Directory
Assemble a Response Message
Transmit the Response Message to the Client
Create the File on the Designate Directory
Update the Directory and File Access Statistics to Local Repository
Update the Directory and File Access Statistics into Local Repository
Assemble a Response Message
Transmit the Response Message to the Client
Assemble and Send a Message with Directory Name, File Name 
and Slave Network Storage Information to Master Network Storage
Transmit Configuration Information to Each Network Storage in the Storage Cluster
Mater Network Storage Stores Corresponding 
Information into the Remote Repository
 
圖 3 Create 請求之細部流程圖 
圖 4 是 Lookup 請求之細部流程圖，也就是圖 2(B)完整的請求處理流程。主
要網路儲存從客戶端接收 Lookup 請求，會依序查詢快取、遠端儲存庫以及檔案
系統來確認所查詢目錄與檔案所在之網路儲存。如果所查詢的目錄與檔案不是放
置在主要網路儲存，主要網路儲存就必須轉發 Lookup 請求，因此在搜尋順序上
遠端儲存庫會優先於檔案系統以降低請求處理延遲時間。如果搜尋時發現該目錄
與檔案是屬於主要網路儲存，接著就會更新本地儲存庫的存取統計資料。然後會
8 
 
存會更新存取統計資料到本地儲存庫，然後建構一個回應訊息傳送給客戶端。最
後必須把該檔案、所在目錄及附屬網路儲存的網路連線等相關資訊傳送給主要網
路儲存使其更新遠端儲存庫。下次再收到對相同檔案進行 Read/Write 請求，主要
網路儲存就可以直接轉發該請求而不需要進行儲存叢集內的請求廣播，可以降低
不必要的網路開銷。 
 
圖 5 Read/Write 請求之細部流程圖 
圖 6 是新儲存叢集建構之概要流程圖。目前網路儲存系統中已存在一個儲存
叢集並且由一個主要網路儲存以及兩個附屬網路儲存所組成。每一個網路儲存會
定期或是根據本身狀態變化程度動態地通知中介資料伺服器以更新整個網路儲
10 
 
測，則該檔案必須再連續通過四次檢測(3-2+4=5)才可歸類在綠區，以此類推。 
 
圖 7 檔案儲存區分類流程圖 
圖 8 是儲存叢集檔案配置調整之概要流程圖。一開始在建構新儲存叢集時，
中介資料伺服器會通知該儲存叢集內的所有網路儲存關於儲存服務品質需求以
及資源配置組態資訊。主要網路儲存就可以根據這些資訊決定新檔案配置在哪個
網路儲存上。每個附屬網路儲存會通知主要網路儲存其儲存服務品質現況，舉例
來說，當附屬網路儲存中屬於黃區以及紅區的檔案存取(存取次數、存取時間或
是使用頻寬)導致綠區儲存服務品質可能影響程度低於 30%，則不做任何處理。
一般而言，位於紅區檔案對於儲存服務品質會比位於黃區檔案來的高。然而當可
能影響程度大於 30%，則該附屬網路儲存會把此參數通知主要網路儲存。主要網
路儲存就會考量整個儲存叢集內所有網路儲存狀態來調整新檔案配置機率。接下
來如果有一個新的 Create 請求到達，則儲存服務品質可能影響程度較高的附屬
網路儲存就會有較低的機率被指派，而儲存服務品質可能影響程度較低的附屬網
路儲存就有較高機率被指派來處理該請求。之後主要網路儲存會轉發該請求然後
由被指派之附屬網路儲存來執行並且更新其本地儲存庫以及傳送回應訊息給客
戶端。最後也要傳送訊息給主要網路儲存以更新其遠端儲存庫。 
 儲存
擴充
會根
接下
網路
網路
儲存
效益
結論
本專
載、
伺服
以避
叢集
來完
存資
 
圖 10 是擴
的儲存服
。此請求會
據服務等
來中介資
儲存來完
儲存。對
叢集大小
。 
 
利提出新
保證每一個
器負責整
免中介資
內的網路
成儲存叢
源能夠有
充現有儲
務品質可能
由主要網
級合約來配
料伺服器會
成儲存叢集
客戶端而言
)以及保證
圖 1
穎的儲存虛
客戶端的
體儲存資源
料伺服器成
儲存狀態動
集容量擴充
效且動態地
存叢集概要
影響程度
路儲存負責
置新網路
把儲存叢
擴充。最後
，本研究
儲存服務品
0 擴充現有
擬化方法
儲存服務品
管理而儲
為網路儲
態地執行
以保證每
進行配置
12 
流程圖。
皆已達到 9
通知中介
儲存，以避
集組態傳送
，原先儲
能夠提供無
質並且能
儲存叢集
 
與系統，它
質以及動
存叢集內則
存系統的效
檔案配置、
一個客戶端
，因此儲存
 
舉例來說
0%，這意味
資料伺服器
免客戶端儲
給主要網
存叢集會由
限制的儲
夠使得網路
之概要流程
能夠有效地
態地調整儲
由主要網
能瓶頸。主
檔案搬移或
的儲存服
資源使用
，當網路叢
著該儲存
。接著中
存服務品
路儲存以
三個網路
存容量(藉
儲存系統
圖 
降低中介
存資源配
路儲存負責
要網路儲
是呼叫中
務品質。無
效率可以大
集內所有網
叢集需要進
介資料伺服
質受到影響
及新增之附
儲存增為四
由動態地調
發揮最大儲
資料伺服器
置。中介資
管理，因此
存會根據儲
介資料伺服
疑地，整個
大地提昇
路
行
器
。
屬
個
整
存
 
負
料
可
存
器
儲
。 
 8
Management)，因此會議內容相當豐富且緊湊。 
本次參與會議主題涵蓋範圍如下； 
• Algorithms and Techniques 
• 3G & 4G Mobile Communication Services 
• Agents and Multi-Agents systems for ICT Integrated Circuits for communications 
• Antennas & Propagation 
• Automation, Control and Robotics 
• Bioinformatics and Bioengineering 
• Biosignal Processing 
• Business Information Systems 
• Broadband & Intelligent networks 
• Computational Intelligence 
• Communication Systems 
• Data Base Management 
• Data Mining and Data fusion 
• E-Commerce & E-government 
• E-Health & Biomedical applications 
• E-Learning & E-Business 
• Emerging technologies & Applications 
• Fuzzy, ANN & Expert Approaches 
• Grid and Cluster Computing 
• ICT & Banking 
• ICT & Education 
• ICT & Intelligent Transportation 
• ICT in Environmental Sciences 
• Image Analysis and Processing 
• Image & Multimedia applications 
• Information & data security 
• Information indexing & retrieval 
• Information Processing 
• Information systems & Applications 
• Internet applications & performances 
• Knowledge Based Systems 
• Knowledge Management 
• Knowledge Management & Decision Making 
• Machine Learning 
• Machine Vision & Remote sensing 
• Management Information Systems 
• Mobile networks & services 
 10
另外，從各國專家學者的研究報告，發現與本人研究領域相似的主題探討上，還有更多
不同的研究方法可以提升性能。除此之外，本次會議在澳洲雪梨舉行，舉辦地點相當舒
適，非常適合學術研討，到了澳洲，可以感受到澳洲的歷史與人文藝術，拓展了自己的
國際視野，這是非常寶貴的經驗。 
三、建議 
    非常感謝國科會經費補助，讓我能夠順利地出席 2011 ICTTA 國際學術會議並發表
研究成果。除了瞭解最新研究方法與發展情況之外，也認識幾位國外教授學者，讓我眼
界大開。經由參與此次國際學術會議，自己在專業知識方面充實不少並且瞭解自己在專
業能力上要持續不斷的提升才能有國際競爭力。最後對於自己的研究成果能夠在國際學
術場合中展現是一件非常有成就感的事情。最後，期望國科會能對出席國際學術會議提
高補助經費與次數，相信對於國內學者的國際學術交流會有所助益。 
四、攜回資料名稱及內容 
1. 2011 ICTTA 國際學術會議論文集一本 
2. 大會附贈手提袋一只 
3. 出席 2011 ICTTA 會議證明書一份 
 
 
 
 
 
 
2011 International Conference on Telecom Technology and Applications (ICTTA 2011)   
 
- 2 - 
If  the  above  requirements  are met by  the  set deadlines, All  registered papers  of  ICTTA 2011 will be 
published  in  the Conference proceedings, and will be  indexed by Thomson  ISI Proceedings. About 10 
selected  papers  from  the  registered  ones will  be  published  in  the  International  Journal  of  Computer 
Theory and Engineering (IJCTE, ISSN: 1793­8201) free of charge.. 
Maybe some unforeseeable events could prevent a few authors not to attend the event to present their papers, so 
if you and your co­author(s) could not attend ICTTA 2011 to present your paper for some reasons, please 
inform us. And we will send you, the official receipt of registration fee and proceedings after ICTTA 2011 
free of charge.  
Please strictly adhere to the format specified in the conference template while preparing your final paper. If you 
have any problem in preparing the final paper, please feel free to contact us via ictta@vip.163.com. For the most 
updated  information  on  the  conference,  please  check  the  conference  website  at http://www.ictta.org/    The 
Conference Program will be available at the website in April, 2011. 
Finally, we would like to further extend our congratulations to you and we are looking forward to meeting you in 
Sydney, Australia! 
Yours sincerely, 
 
ICTTA 2011 Organizing Committees 
http://www.ictta.org/   
Sydney, Australia 
parameter in the packet headers of a specific flow, the PDD can support different degrees of bandwidth 
sharing. However, the PO is incapable of action. 
 
  Corresponding author. Tel.: + 886-7-6678888 ext. 6404; fax: +886-7-6679999 
   E-mail address: juipin.yang@msa.hinet.net 
2. Pushout with Differentiated Dropping Scheme 
In the first place, the definition of a flow is a stream of packets which have the same source and 
destination IP address. We calculate average packet arrival rate for each active flow to represent their 
intensity estimation. An active flow with large intensity means that their arrival packets have higher 
probability to be discarded. The intensity estimation is an exponential weighted moving average, similar to 
the random early detection (RED) [9]. Let kjA ,  be equal to the amount of arrival packet size belonging to 
active flow j during time interval k. The duration of a time interval is of TR  and the link capacity is of C . 
Furthermore, the pW  is a constant that affects the dependency of estimation on short-term or long-term 
traffic conditions. Particularly, we modify the rule that is in charge of the maintenance of flow index in the 
ActiveList of the DRR. When active flow j has empty queue, we do not remove flow j from the ActiveList 
unless 0, kjA . This can reduce the frequency to insert or remove flow indices in the ActiveList. The 
intensity of active flow i during time interval j, is computed as: 
kjp
T
kj
pkj IwRC
A
wI ,
,
1, )1(                                        (1) 
After flow intensity estimation, we would like to design a weight function. The main idea of the PDD is 
that those flows with lower flow intensity will be assigned a lower weight. There are many probable choices 
to assign the weights. The simplest approach is to make the weight of each flow linearly increase relative to 
its flow intensity. However, the may lead to insufficient discrimination. Therefore, a suitable weight function 
can greatly upgrade the fairness of bandwidth sharing. 
Based on our simulations and analysis, we design a non-linear weight function in which active flows 
with lower intensity are given relatively smaller weights, while active flows with higher intensity are given 
relatively larger weights. We select a particular weight function in equation (2) because it can differentiate 
active flows. Here,   is a constant and kjWF ,  is the weight of active flow j during time interval k.  


ln
))1(1ln( ,
,
kj
kj
I
WF
                                               (2) 
Next, the PDD maintains two flow indices that have the maximum and sub-maximum products of queue 
length and weight in the max and submax variables. When the full is filled up, the PDD estimates the product 
of queue length and weight of the arrival packet. By comparing its product with that of flow max and submax, 
three actions are involved to deal with the arrival including acceptance, pushout or discard. Equation (3) 
means that at least h  resided packets of flow queue max will be pushed out (from the head of flow queue 
max), in order to make sufficient room to accommodate the arrival packet with jA  size. Also, the same rule 
is applied to the flow queue submax. 
j
h
i
i AS 1 max, }min{                                                               (3) 
Where iSmax,  is ith  packet size of flow queue max. 
3. Simulation Results 
The simulation results also show that PO is partial to heavily loaded flows. For example, flow 10 
obtains the largest normalized bandwidth ratio near 1.15 when B  is set at 10 Kbytes. This may encourage 
network users to violate congestion control mechanisms because they are able to occupy more bandwidth. 
Therefore, this may greatly aggravate congestion. In the PDD, the flow 8 obtains the largest normalized 
bandwidth ratio near 1.08. Higher traffic intensity will result in relatively higher weights, so flow 10 properly 
obtains ideal normalized bandwidth ratio near 1.02. When B  is set at 60 Kbytes, both schemes equip with 
sufficient buffer size that contribute to individual approximately best fairness. From the simulations, the 
PDD not only achieves good fairness but also benefits to congestion control mechanisms.  
In Figure 2, we discuss the effect of different average traffic loads which consist of [1 1.1 1.2 1.3 1.4 
1.5 1.6 1.7 1.8 1.9 (Mbps)]. All flows have close average traffic loads, so their weights are approaching. In 
the PO, the flow 10 occupies the largest normalized bandwidth ratio near 1.21 when B  is set at 10 Kbytes. 
As for the PDD, the flow 10 obtains the largest normalized bandwidth ratio near 1.12 when B  is set at 10 
Kbytes. Besides, flow 1 obtains better fairness than that of Figure 3 under various buffer sizes. The reason is 
that the probability of a full buffer becomes lower, so more arrival packets of flow 1 are admitted to enter the 
buffer. From Figure 1 and Figure 2, the PDD always provides better fairness than the PO under different 
traffic conditions.  
 
Fig. 2: Normalized bandwidth ratio versus per flow under different buffer sizes  
and average traffic loads from 1 Mbps to 1.9 Mbps 
In Figure 3, we change the configuration that consists of different number of flows and average traffic 
loads. There are 30 flows, indexed from 1 to 30, and flows transmit several times more than max-min fair 
rate. We describe average traffic load for each flow as [0.3333 0.6667 1.0 1.3333 1.6667 2.0 ……. 10 
(Mbps)]. According to max-min fairness, each flow should obtain average bandwidth of 0.3333 Mbps. In the 
PO, the flow 30 occupies the largest normalized bandwidth ratio near 1.03 when B  is set at 30 Kbytes. As 
for the PDD, the flow 30 obtains less normalized bandwidth ratio near 0.87 when B  is set at 30 Kbytes. The 
reason for this phenomenon in the PDD is the same as that of Figure 3. However, the flow 30 performs 
worse because more flows have approaching average traffic loads to flow 30. The PDD provides flow 1 with 
much better fairness than that of PO when B  is set at 30 Kbytes. The PO scheme always allocates more 
bandwidth to the heavily loaded flow, but the PDD protects the lightly loaded flows and punishes the heavily 
loaded flows by giving less bandwidth. In a word, the PDD has much better performance than the PO no 
matter fairness or congestion control.  
國科會補助計畫衍生研發成果推廣資料表
日期:2011/07/28
國科會補助計畫
計畫名稱: 在網際網路中雲端儲存技術之研究與開發
計畫主持人: 楊瑞賓
計畫編號: 99-2221-E-158-003- 學門領域: 網路
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
