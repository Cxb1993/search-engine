 2 
II. 研究目的與背景 
一、數位電視、DVB-H、與影音編碼 
電視的數位化可以提昇電波頻譜的使
用效率，提供民眾更高品質的視聽服務及多
元的選擇，並促進平面顯示器、數位電視機
及數位內容等相關產業的發展，因此歐美日
等先進國家自十餘年前即開始推動廣電產
業的數位化。美國於 2009年 6月 12日起全
面停止播送類比地面廣播電視信號，開始進
入全數位電視廣播的時代，歐洲國家從 2010
年起陸續停播類比電視信號，日本於今
(2011)年全面停播類比廣播電視，台灣地區
則預計於明(2012)年全面停播類比地面廣播
電視。 
除了透過傳統地面廣播、衛星廣播、與
有線廣播等接收技術之外，近年來數位電視
也開始強調移動接收與手持裝置技術。在手
持式行動裝置（如手機或 PDA）上收看電
視節目，可以讓廣大的行動通信用戶隨時隨
地獲取他們需要的資訊和喜愛的節目，也可
以讓通訊業者提供更多樣化的多媒體加值
服務。 
在國際間目前已（試）營運的手持式電
視系統中，DVB-H 標準是目前聲勢最浩大
且市佔率較高者，而且對於數位電視地面廣
播系統採用 DVB-T標準者（如我國），採用 
DVB-H 將可相容於現有地面廣播傳輸模式
與終端接收設備；也因此，本計畫以實作 
DVB-H標準的視訊與音訊解碼為目標。 
DVB-H 的視訊編碼建議選用 H.264 及
VC-1 規格，音訊編碼則建議採用 MPEG-4 
HE AAC v2及 3GPP的 AMR-WB+。由於
H.264 與 MPEG-4 AAC 格式是高效率影音
壓縮的主流，目前提供 DVB-H 服務的電
台，絕大部份都是使用 H.264 與 MPEG-4 
AAC 規格，所以本計畫將僅實作 H.264 與
MPEG-4 HE AAC v2解碼器。 
二、H.264視訊編碼 
H.264 （ 亦 稱 為  MPEG-4 AVC, 
Advanced Video Coding）是 ITU與 ISO/IEC 
等國際標準組織所共同制定的最新視訊編
碼標準，它能提供高效率的視訊壓縮，及壓
縮視訊在各種不同網路環境的有效表示。
H.264 用不到一半的位元率，就可以達到與
MPEG-1/2 相同的視訊品質。壓縮效率的改
進，代表傳輸或儲存成本的降低與使用便利
性的提升，因此 H.264已成為數位電視、影
音串流、行動視訊、高密度光碟等各項多媒
體服務的關鍵技術。 
VCL 
(Video Coding Layer)
NAL
(Network Abstraction Layer)
RTP/IP H.32XMP4 Format
MPEG-2 
SystemsTCP/IP 其他
編碼資料
視訊壓縮
NAL單元
格式化與封裝
傳輸資料流
Transport 
Layer
H.264/AVC
定義範圍
傳輸網路
 
圖二：H.264的分層架構與傳輸環境 [10] 
H.264 標準在架構上包含 VCL (Video 
Coding Layer)與 NAL (Network Adaptation 
Layer)兩層，如圖二所示。VCL為視訊壓縮
的部份，其技術核心包括動作估計、轉換編
碼、預測編碼、去區塊效應濾波、及熵編碼
等技術。NAL提供 VCL編碼資料與實際網
路之間的介接，進行編碼資料的格式化，添
加必須的檔頭資訊，再封裝成適當的傳輸單
元。除了實際的視訊封包之外，NALU也可
以傳送獨立之參數集(Parameter Set)，做為
解碼的輔助資訊。 
H.264標準雖不求向後相容，但它和之
前的 H.26x/MPEG 標準仍有許多雷同之
處，基本上還是以區塊動作估計與轉換編碼
為主體。H.264的編碼端架構如圖三所示，
相較於過去的視訊壓縮技術，H.264 主要的
 4 
再 接 下 來 的 處 理 ， 類 雜 訊 取 代
(Perceptual Noise Substitution, PNS)模組，主
要是判斷各頻帶中是否有類似雜訊的聲
音；若有，則不需要傳送該頻帶的頻譜係
數。經上述處理後之頻譜值，再用量化
(Quantization)與源編碼(Source Coding)，進
行壓縮與碼率控制。 
所謂的MPEG-4 HE AAC則是MPEG-4 
AAC LC 加上頻譜複製 (Spectral Band 
Replication, SBR)的工具所組合而成。SBR
將低頻的頻譜複製到高頻，故高頻部份只記
錄概略之能量數值，以節省編碼之位元數。
SBR 之 analysis QMF (Quadrture Mirror 
Filter) bank 為 32-band complex QMF，而
synthesis QMF 則 為 64-band complex 
QMF。至於 MPEG-4 HE AAC v2 則是在
MPEG-4 HE AAC 中加上了參數立體聲 
(Parametric Stereo, PS)之工具，其編碼示意
圖如圖六所示。由此圖可知，較低頻之訊號
經 PS 編碼器取出參數後，再送入 AAC 編
碼器編碼低頻部份（開關向上），至於高頻
部份，則只由 SBR 計算頻譜之包絡，而不
編碼頻譜值。 
Audio
Signal
(fs)
QMF
Analysis
PS
Encoder
(Mono)
SBR
Encoder
QMF
Synthesis
AAC
Encoder
2:1
Resampler
Bit-stream 
Multiplex
Stereo Parameters
fs/2
Bit-stream
圖六：MPEG-4 HE AAC v2之編碼示意圖 
四、Android Media Player 
Android是由Google及開放手機聯盟所
發展的一個基於Linux作業系統的開放手持
式裝置軟體架構，Android 提出了一個應用
軟體開發框架，讓應用程式可以用類似元件
的方式來組成。此外，Android 將軟體架構
分成數個層次，讓軟體的開發可以依功能區
隔開來，並且可以很容易地移植到不同的硬
體平台之上。 
Android MediaPlayer為 Android的基礎
多媒體框架，內部主要分為七大部分：PV 
Player、PV Author、Codec、Packet Video 
Multimedia Framework (PVMF)、Operating 
System Compatibility Library (OSCL) 、
Common、Open MAX。PV Player 和 PV 
Author 提供音訊和視訊的錄製及播放的功
能，並透過 Linux的硬體驅動程式和硬體溝
通。由於 PV Player和 PV Author使用的都
是原始資料的形式，所以必須先透過編解碼
器來進行編碼或解碼。PVMF是 Android判
斷檔案格式及編碼內容的框架，自動決定要
用哪一個 Parser來解開或包裝檔案，並選擇
適當編解碼器進行編碼或解碼，Common則
存放各種整體框架的共通功能。OSCL提供
包括資料型態、記憶體管理、執行等基本函
式。Open MAX分為 DL、IL、AL三種層級，
讓編解碼器的開發更簡易，其中。DL 掌管
直接和硬體連接的部分，包含對 CPU、顯示
晶片、數位訊號處理器等的最佳化和驅動程
式的接口；IL提供編解碼器函式庫，硬體編
解碼加速器亦可經由 Open MAX IL和上層
連結；AL 控制錄影、放影、各種感測器及
顯示器等使用者介面。 
 
III. 研究與實作方法 
DVB-H 傳送系統的影音資料封裝方式
如圖七所示。視訊資料經過 H.264 VCL 層
壓縮，再由H.264 NAL層封裝成為 NAL 單
元(NAL unit, NALU)，一個 NALU 包含 1
個位元組的標頭，與一段影像語法元素
(syntax elements)。NALU標頭的後 5個位元
說明其編碼型態(type)，其他位元說明此
NALU 的重要性。TS 102 005 技術規範規
定，對於手持式裝置 IP Datacast 的應用，
H.264 的影像資料應該用單一的 NALU 或
非交錯模式的 RFC 3984 直接封裝成一個 
RTP 封包。至於音訊資料則在使用 RTP通
訊協定之前，先用 RFC 3640來包裝。RTP
封包有 12 位元組的標頭，包含承載資料型
 6 
理。Start Bit等於 1、或是 Start Bit和 End Bit
都是 0 時，因為 NALU 還不完整，所以先
暫存在 Temp Buffer，等到 End Bit 等於 1
時，再把 Temp Buffer和後面的資料存放到
Video Buffer裡。type等於 24(11000)的話，
一個 RTP 封包裡面會有不只一個 NALU，
所以要先找出這些 NALU 的長度，再依序
將這些 NALU存入 Video Buffer裡。 
音訊 RTP 封包的處理流程如圖十所
示。音訊以 Audio Unit (AU)為封包的基本單
位，也就是說，音訊 RTP封包的 payload即
是一個或一個以上的 AU。AU (RFC 3640)
的結構如圖十一所示，共包含三個區段
(section)：AU Header Section（不一定有）、
Auxiliary Section（不一定有）、以及一定得
出現的 AU Data Section。AU Header Section
的前兩個位元組(AU-headers-length)標示其
後所有 AU標頭區段的總長度(in bits)，後面
跟著若干個 AU 的標頭，AU-headers-length
的值若為 0x0010時，表示只有一個 AU。每
一個 AU header 的前 13 位元代表這個 AU
的大小，再來的 3位元代表 AU的索引值。
根據這些資訊即可將 HE AAC 的音訊資料
依序存入 Audio Buffer中。 
接收RTP封包
讀取下一個AU的
Data Section，並依
序存入Audio Buffer
解析RTP內AU個數
解析各 AU長度及
索引值
讀取AU Header 
Section
最後一個AU
結束
是
否
 
圖十：音訊 RTP處理流程圖 
  
 
圖十一：RTP Payload Format結構 (RFC 3640) 
本計畫在處理 H.264 和 HE-AACv2 時
還另外加入錯誤隱藏的機制。我們根據 RTP
封包的 sequence number，判斷是否有封包遺
失的情形發生。遺失視訊時，以其前後封包
的畫面類型(I,P,或 B)做判斷，補上最接近所
需類型的封包(畫面片段)，遺失音訊封包時
則直接貼上前一個封包。未做錯誤隱藏和增
加錯誤隱藏的視訊比較如圖十二所示。 
 
 
圖十二：未做錯誤隱藏(上圖)和增加錯誤隱藏
(下圖)的解碼視訊比較 
 8 
Main VIDEO VideoRTP AudioRTP RTSPServer IPDC
setDecoderInfor(Video_SDP_Structure)
getRTPPackets(ip , audioPort , videoPort , vecor<A_RTP*> , vecor<A_RTCP*> , vecor<V_RTP*> , vecor<V_RTCP*>)
read():string
rtp packets
processVRTP(vector<V_RTP*> , vector<H264Structure*> , vector<ParameterSet*> , video_sdp *, int &, int &)
to_H264(vector<V_RTP*> , vector<H264_Structure*> , vector<ParameterSet*>)
processARTP(vector<A_RTP*> , vector<AACStructure*> , SDP)
to_AAC(vector<A_RTP*> , vector<AACStructure*> , SDP)
addData(H264Structure*data)
addData(AACStructure*data)
setVideoTimestamp(unsigned int timestamp)
work()
回傳是否成功
setVideoTimestamp(unsigned int timestamp)
圖十六：本計畫之循序圖(Sequence Diagram)
表二：本計畫重要之功能需求 
需求編號 優先順序 需求描述 
VIDEO-F-01 1 
能對 H.264視訊壓縮格
式正確解碼。 
VIDEO-F-02 1 
能對 HE AAC V2音訊
壓縮格式正確解碼。 
VIDEO-F-03 1 
H.264視訊封包遺失時
可以正確處理並做錯誤
隱藏機制 
VIDEO-F-04 1 
AAC音訊封包遺失時可
以正確處理並做錯誤隱
藏機制 
VIDEO-F-08 1 
可以將視訊和音訊串流
同步 
VIDEO-F-09 1 
將同步之後的影音串流
傳給MAIN。 
二、系統設計 
開發設計階段會將需求規劃階段列舉
之需求做更詳細的描述，並且對各種可能實
行方式進行評比，選擇出最佳的解決方案。
如表三為本計畫針對 DVB-H編解碼器選擇
所做的比較。 
 10 
本專案完成之軟體已上傳至自由軟體
鑄造廠(OSSF)，整合型計畫第一年度成果獲
得自由軟體暨嵌入式系統計畫績優團隊獎
（第二年度績優團隊仍在評選中），第二年
度成果亦獲得評審委員高度肯定，因而獲邀
在 2011 年 10 月，代表國科會參加「2011
台北國際發明暨技術交易展」的實際展示。
參與計畫學生以計畫內容為主題（作品名
稱：手持式行動數位電視 DVB-H 接收系
統），參加 2010 開放原始碼創新應用開發
大賽（主辦單位：經濟部工業局、執行單位：
財團法人資訊工業策進會），榮獲學生組優
等（獎金 10萬與獎座乙座）。 
 
VII. 參考文獻 
[1] EBU, Digital Video Broadcasting (DVB); 
Transmission system for handheld terminals 
(DVB-H), ETSI EN 302 304, v1.1.1, 2004.  
[2] EBU, Digital Video Broadcasting (DVB); 
Specification for the use of video and audio 
coding in DVB services delivered directly 
over IP protocols, ETSI TS 102 005, v1.3.1, 
2007.  
[3] G. Faria, J. A. Henriksson, E. Stare, and P. 
Talmola, “DVB-H: digital broadcast services 
to handheld devices,” Proceedings of the 
IEEE, vol. 94, no. 1, pp. 194 - 209, Jan. 2006.  
[4] M. Kornfeld and G. May, “DVB-H and IP 
datacast－broadcast to handheld devices,” 
IEEE Trans. Broadcasting, vol. 53, no. 1, 
pp.161- 170, March 2007.  
[5] Advanced Video Coding for Generic 
Audiovisual Services, ISO/IEC 14496-10, 
Version 8, Apr. 2007. 
[6] SMPTE, Proposed SMPTE standard for 
television: VC-1 compressed video bitstream 
format and decoding process, SMPTE 421M. 
[7] G. J. Sullivan and T. Wiegand, “Video 
compression from concepts to the H.264/AVC 
standard,” Proceedings of the IEEE, vol. 93, 
no. 1, pp. 18-31, Jan. 2005. 
[8] D. Marpe, T. Wiegard, and G. J. Sullivan, 
“The  H.264/MPEG4 advanced video coding 
standard and its applications,” IEEE 
Communications Magazine, vol. 44, no.8, pp. 
134-143, Aug. 2006.  
[9] M. Horowitz, A. Kossentini, and A. 
Hallapuro, “H.264/AVC baseline profile 
decoder complexity analysis,” IEEE Trans. 
Circuits Syst. Video Technol., vol. 13, no. 7, 
pp. 704-716, July 2003. 
[10] 楊士萱、陳柏源，H.264/AVC 技術與
應用簡介，影像與識別，vol. 13, no. 2, June 
2007。 
[11] ISO/IEC: Information Technology - 
Coding of audio-visual objects - Part 3: Audio, 
ISO/IEC 14496-3, 2009. 
[12] 3GPP, Audio codec processing functions; 
Extended Adaptive-Multi-rate – Wideband 
(AMR-WB+) codec; transcoding functions, 
3GPP TS 26.090 (ETSI TS 126 290). 
[13] S. Meltzer and G. Moser, “MPEG-4 HE 
AAC v2 — audio coding for today’s digital 
media world,” EBU Technical Review, pp. 
1–12, Jan. 2006.  
[14] IETF, RTP payload for transport of H. 
264, RFC 3984, 2005. 
[15] IETF, RTP payload for transport for 
generic MPEG-4 elementary streams, RFC 
3640, 2003. 
[16] IETF, RTP, A transport protocol for real 
time applications, RFC 3550, 2003.  
[17] IETF, Real Time Streaming Protocol, 
RFC 2326,1998.
出席國際學術會議心得報告 
                                                             
計畫編號 NSC 99-2220-E-027-002 
計畫名稱 嵌入式 DVB-H 接收系統之開發－子計畫一:DVB-H 視訊解碼軟體之開發(2/2) 
出國人員姓名 
服務機關及職稱 
唐偉倫 
國立台北科技大學資訊工程系碩士班學生 
會議時間地點 
100 年 7 月 18 日至 100 年 7 月 21 日 
University of Seville, 西班牙巴塞維亞 (Seville, Spain) 
會議名稱 2011 International Conference on Signal Processing and Multimedia Applications (SIGMAP 2011) 
發表論文題目 What are good CGS/MGS configurations for H.264 quality scalable coding? 
 
一、參加會議經過 
 
2011 International Conference on Signal Processing and Multimedia Applications 
(SIGMAP, 2011)，2011 年 7 月 18 日至 21 日在西班牙巴塞維亞 University of Seville 舉行，大
會主要議程集中於 18 日至 20 日三天。本人於 7 月 16 日傍晚搭乘荷蘭航空 KL878 班機，經
15 小時之飛行，於 7 月 17 日清晨抵達阿姆斯特丹史基普機場，接著轉搭乘荷蘭航空
KL1665 班機，經 2 小時飛行抵達西班牙巴塞隆納，再轉乘西班牙斯班航空 JK6606 班機，
經 1 個半小時的飛行，於 7 月 17 日傍晚抵達賽維亞聖保羅機場，於 7 月 18 日開始參與大會
各項活動。 
7 月 18 日開幕典禮後接著進行 panel，由 National Ilan University 的 Han-Chieh Chao 和
University Rey Juan Carlos 的 Enrique Cabello 等人演說，題目是 Green Computing and 
Communication Systems。內容是有關於 Green Computing 近年來的目標發展以及會遇到的挑
戰。上午結束之後，本人發表的論文安排於 7 月 18 日下午 16:15-17:45 的 session 2。報告過
程還滿順利的，結束後和與會人士進行專業交流。接著 17:45-18-45 的 keynote speech 由
National Ilan University 的 Han-Chieh Chao (趙涵捷校長)主講，講題是 WiMAX? A Case Study 
on Minimizing Construction Cost for IEEE 802.16j Multi-hop Relay Networks。由於現在電信服
務供應商方面的 WiMAX 覆蓋範圍並不理想。為了解決此問題，IEEE 802.16j 使用中繼傳輸
的技術來處理覆蓋問題。他們提出了 Supergraph Tree 演算法以最低成本的方式來佈署基站
和中繼站的位置。 
7 月 19 日的 keynote speech 由 University of Hamburg 的 Winfried Lamersdorf 主講，講題
是 Paradigms of Distributed Software Systems。主要是關於分散式軟件系統的應用以及遇到的
問題。大會主要議程於 7 月 20 日結束。本人則於賽維亞多停留數日，於 7 月 22 日搭乘西班
Shih-Hsuan Yang
寄件者: "SIGMAP Secretariat" <no-reply@insticc.org>
日期: 2011年6月3日 上午 11:57
收件者: <shyang@ntut.edu.tw>
主旨: SIGMAP - Camera Ready Submission
b第 1 頁 (共 1 頁)(B)
2011/8/3(B)b
Dear Prof. Shih-Hsuan Yang,
We would like to inform you that we have just received the camera-ready version of the following 
paper at SIGMAP: 
--------------------------------------------------------------
Title: WHAT ARE GOOD CGS/MGS CONFIGURATIONS FOR H.264 QUALITY SCALABLE 
CODING?
Paper number: 68
Submitted on Date/Time: 6/3/2011  4:57 AM
Submission type: position paper
Co-Authors: Shih-Hsuan Yang;Wei-Lune Tang;
--------------------------------------------------------------
Please make sure that you have followed the camera-ready paper format and preparation guidelines 
for the proceedings. If necessary, you may change the paper and upload it again. Only the last 
version will count. 
As you know, the publication of any paper in the conference proceedings requires that at least one of 
the authors be registered.
If you have any questions please don’t hesitate to contact the conference secretariat.
Best regards,
SIGMAP Conference Management System
INSTICC office
Av. D.Manuel I, 27A-2.Esq. 
2910-595 Setubal
Portugal
This message was automatically sent by PRIMORIS (the INSTICC conference management system) 
so please DO NOT reply to this email. 
If you need help please contact:sigmap.secretariat@insticc.org
  
CGS layer. Inter-layer prediction may be employed 
to increase compression efficiency of CGS. 
However, the number of available bit rates is 
restricted to the number of selected QPs (CGS layers) 
and more layers generally imply worse coding 
efficiency. To increase the flexibility of bit stream 
adaptation and to improve the coding efficiency, 
MGS additionally provides the capability to 
distribute the CGS enhancement layer transform 
coefficients into more layers. Grouping information 
of the transform coefficients is signaled in the slice 
headers, and thus, a CGS layer that corresponds to a 
certain QP can be partitioned into several MGS 
layers and separately packetized. Pulipaka et al 
(2010) conducted some statistical analyses of SVC, 
including the rate distortion and rate variability 
distortion performances. Görkemli et al (2010) 
compared MGS fragmentation configurations of 
SVC, including the slice mode and extraction 
methods, for their rate-distortion performance. 
In this paper, we test various CGS/MGS options 
for H.264 SVC using the official reference software 
JSVM (Joint Scalable Video Model) (JSVM 
Software Manual, 2010/2011). Throughout the 
comprehensive experiments, unusual rate-distortion 
behavior for some configurations of SVC options 
was discovered. It is generally believed that an 
additional quality layer (more received bits) should 
always improve the quality for SVC. However, we 
find that adding an MGS sub-layer in some cases 
may conversely decrease the PSNR. We thus 
conduct more tests to explore this anomaly. The rest 
of this paper is organized as follows. In Section 2, 
we briefly review the H.264 SVC techniques, 
particularly in details for CGS and MGS. 
Experiments on H.264 quality scalability with 
various JSVM CGS/MGS configurations are given 
in Section 3, which also demonstrates the 
aforementioned oddity. Some discussion and future 
work are given in Section 4. 
 
2 H.264 SCALABLE VIDEO 
CODING  
H.264 includes two layers in structure: video coding 
layer (VCL) and network abstraction layer (NAL). 
Based on the core coding tools of the non-scalable 
H.264 specification, the SVC extension adds new 
syntax for scalability (ITU-T Rec. H.264, 2009). The 
representation of the video source with a particular 
spatio-temporal resolution and fidelity is referred to 
as an SVC layer. Each scalable layer is identified by 
a layer identifier. In JSVM, three classes of 
identifiers, T, D, and Q, are used to indicate the 
layers of temporal scalability, spatial scalability, and 
quality scalability, respectively. A constrained 
decoder can retrieve the necessary NAL units from 
an H.264 scalable bit stream to obtain a video of 
reduced frame rate, resolution, or fidelity. The first 
coding layer with identifier equal to 0 is called the 
base layer, which is coded in the same way as non-
scalable H.264 image sequences. To increase coding 
efficiency, encoding the other enhancement layers 
may employ data of another layer with a smaller 
layer identifier.  
Temporal scalability provides coded bit streams 
of different frame rates. The temporal scalability of 
H.264 SVC is typically structured in hierarchical B-
pictures. In this case, each added temporal 
enhancement layer doubles the frame rate. These 
dyadic enhancement layer pictures are coded as B-
pictures that use the nearest temporally available 
pictures as reference pictures. The set of pictures 
from one temporal base layer to the next is referred 
to as a group of pictures (GOP). It is found from 
experiments that the GOP size of 8 or 16 usually 
achieves the best rate-distortion performance 
(Schwarz and Marpe, 2007). Note that the GOP size 
also determines the total number of temporal layers 
(no. of temporal layers = (log2 GOPsize) + 1). 
Each layer of H.264 spatial scalability 
corresponds to a specific spatial resolution. In 
addition to the basic coding tools of non-scalable 
H.264, each spatial enhancement layer may employ 
the so-called interlayer prediction, which employs 
the correlation from the lower layer (resolution). 
There are three prediction modes of inter-layer 
coding: inter-layer intra prediction, inter-layer 
motion prediction, and inter-layer residual prediction. 
Accordingly, the up-sampled reconstructed intra 
signal, the macroblock partitioning and the 
associated motion vectors, or the up-sampled 
residual derived from the colocated blocks in the 
reference layer, are used as prediction signals. The 
inter-layer prediction shall compete with the intra-
layer temporal prediction for determining the best 
prediction mode. 
Quality scalable layers, which are the main 
concern of this paper, have identical spatio-temporal 
resolution but different fidelity levels. H.264 offers 
two options for quality scalability, CGS (coarse-
grain quality scalable coding) and MGS (medium-
grain quality scalability). An enhancement layer of 
CGS is obtained by requantizing the (residual) 
texture signal with a smaller quantization step size 
(quantization parameter, QP). CGS incorporates the 
inter-layer prediction mechanisms very similar to 
those used in spatial scalability, but with the same 
  
plus 1, to its maximum. (The extra one is added for 
accomplishing the reference pictures of the 
hierarchical B structure.) Hence, 289 frames are 
used for image sequences of 300 frames. 
EncodeKeyPictures controls the drift of 
prediction. The value set to 1 means that pictures 
with MGS (Q > 0) refinement are coded as key 
pictures. Only minor variation is observed if we use 
another EncodeKeyPictures value and the 
global rate-distortion trend does not change. JSVM 
allows only three 3 CGS layers if we set 
CgsSnrRefinement = 0. Therefore, we set 
CgsSnrRefinement = 1 (MGS) along with 
appropriate LayerCfgs. It is also found that the value 
of CgsSnrRefinement will not change the 
coding results if no more than 3 CGS layers are used. 
Thus, 3-layer CGS (CgsSnrRefinement = 0) 
and 3-layer MGS (CgsSnrRefinement = 1) have 
almost identical rate-distortion results.  
The parameter NumLayers specifies the total 
number of spatial/CGS layers. (Recall that CGS is 
regarded as a special case of spatial scalability.) In a 
Layer Configuration File that corresponds to a 
specific QP, the parameter MGSVectorMode 
specifies whether MGS is used, i.e., whether the 
transform coefficients of the CGS layer are written 
into several MGS quality layers according to 
MGSVectorX. The parameter MGSVectorX 
specifies the number of transform coefficients in the 
Xth MGS sub-layer, i.e., the Xth position of the vector 
in the zigzag order. 
 
 
   
                 (a)                        (b)                         (c) 
   
 (d)                        (e)                         (f) 
   
                 (h)                        (i)                         (j) 
Figure 2: Test sequences, (a) Foreman, (b) Mobile, (c) 
Tempete, (d) City, (e) Bus, (f) Flower, (h) Soccer, (i) 
Football, (j) Harbour. 
 
Table 1. Encoding parameters. 
(a) main.cfg 
Parameter Value Remarks 
FrameRate 30.0  
FramesToBeEncoded 289 No. of frames 
GOPSize 16  
CgsSnrRefinement 1 1: MGS; 0: CGS
EncodeKeyPictures 1 MGS 
MGSControl 2 
ME+MC with EL,
closing prediction 
loop at lowest and 
highest rate point
SearchMode 4 FastSearch 
SearchRange 32 In full pels 
NumLayers 4 CGS layers 
LayerCfg layer0-3.cfg 
Layer 
configuration file
(b) layer3.cfg 
Parameter Value Meaning 
SourceWidth 352 Input frame width 
SourceHeight 288 Input  frame height
InterLayerPred 2 
Inter-layer Prediction 
(0: no, 1: yes, 
2:adaptive) 
MGSVectorMode 1 MGS vector usage selection 
MGSVector0 4 Specifies 0th position of the vector 
MGSVector1 4 Specifies 1st position of the vector 
MGSVector2 8 Specifies 2nd position of the vector 
QP 20 Quantization parameters  
 
In the following, we present the simulation 
results for two cases, 4 rate points and 8 rate points. 
Due to the page limit, we primarily present the 
results for the three sequences Foreman, Tempete, 
and Flower. The results for 4 rate points are shown 
in Figure 3. Three SVC configurations are examined: 
(i) 4-layer CGS, QP = 36-28-24-20; (ii) 3-layer CGS, 
QP = 36-28-20(4-12); (iii) 2-layer CGS, QP = 36-
20(4-4-8). A number in parentheses after a QP value 
denotes an MGSVector. Therefore, Configuration 
(ii), 36-28-20(4-12), indicates that 2 MGS sub-layers 
exist for the CGS layer with QP = 20 and these two 
sub-layers consist of 4 and 12 transform coefficients, 
respectively. As shown in Figure 3, the inserted 
MGS sub-layers degrade PSNR performance. 
However, incorporating MGS significantly reduces 
the encoding time as compared to using CGS alone, 
as shown in Figure 4. On the other hand, whether 
MGS is used has little effect on the decoding time. 
  
31
33
35
37
39
41
300 800 1300 1800 2300 2800 3300 3800 4300
36‐32‐30‐28‐26‐24‐22‐20
36‐32‐26(4‐4‐8)‐20(4‐4‐8)
36‐28(4‐4‐8)‐20(2‐4‐5‐5)
Y‐P
SN
R (d
B)
bit rate (kbits/s)  
Tempete (257) 
29
31
33
35
37
39
41
43
400 900 1400 1900 2400 2900 3400 3900 4400 4900
36‐32‐30‐28‐26‐24‐22‐20
36‐32‐26(4‐4‐8)‐20(4‐4‐8)
36‐28(4‐4‐8)‐20(2‐4‐5‐5)
 
Flower (241) 
Figure 6: Rate-distortion results (8 rate points) with JSVM 
9.19.13.  
 
0
5000
10000
15000
20000
25000
30000
35000
40000
foreman(289) tempete(257) flower(241)
36‐32‐30‐28‐26‐24‐22‐20
36‐32‐26(4‐4‐8)‐20(4‐4‐8)
36‐28(4‐4‐8)‐20(2‐4‐5‐5)
se
co
nd
 
Figure 7: Encoding time comparison (8 rate points) with 
JSVM 9.19.13. 
 
 
4. DISCUSSION AND FUTURE 
WORK 
The effects of CGS/MGS configurations of H.264 
SVC are investigated in this paper. For four or fewer 
rate points, CGS coding alone gives better coding 
performance despite of its high encoding complexity. 
For more rate points, adding MGS sub-layers to 
existing CGS layers may give better or worse rate-
distortion performance as compared to using CGS 
alone. It is observed that some CGS/MGS 
configurations may cause an unexpected PSNR drop 
with an increased bit rate. The drop occurs at the 
first MGS sub-layer of the last CGS layer that has 
the smallest QP. Although this phenomenon is less 
significant in the latest version of JSVM, the 
problem is not fully resolved. As a consequence, one 
may be hesitant to use MGS due to its un-stability. 
The reasons behind this anomaly are under study. By 
investigating the source code and simulation results, 
our current finding and conjecture are towards the 
residual coding and the inter-layer prediction of 
blocks smaller than 8x8. 
ACKNOWLEDGEMENTS 
This work is supported in part by the National 
Science Council, R.O.China, under Grant NSC 99-
2220-E-027-002. 
REFERENCES 
Ohm, J.-R., 2005. ‘Advances in scalable video coding’, 
Proceedings of the IEEE, vol. 93, no. 1, Jan, pp. 42-54. 
ITU-T Rec. H.264, 2009. (MPEG-4 AVC), Fifth Edition, 
May (including SVC and MVC extensions). 
Schwarz, H. and Marpe, D., 2007. ‘Overview of the 
scalable video coding extension of the H.264/AVC 
standard’, IEEE Trans. Circuits Syst. Video Technol., 
vol. 17, no. 9, Sep, pp. 1103-1102. 
Pulipaka, A., Seeling, P., Reisslein, M and Karam, L.J., 
2010. ‘Overview and traffic characterization of coarse-
grain quality scalable (CGS) H.264 SVC encoded 
video’, Consumer Communications and Networking 
Conference (CCNC), Jan, pp.1-5. 
Görkemli, B., Şadi, Y. and Tekalp, A.M., 2010. ‘Effects of 
MGS fragmentation, slice mode and extraction 
strategies on the performance of SVC with medium-
grained scalability’, IEEE International Conference 
Image Processing (ICIP), Sep, pp.4201-4204. 
JSVM Software Manual, 2010/2011. Version: 9.19.9 
(CVS tag: JSVM_9_19_9), January 16th, 2010. JSVM 
Software Manual, Version: 9.19.13 (CVS tag: 
JSVM_9_19_13), May 4, 2011. 
99 年度專題研究計畫研究成果彙整表 
計畫主持人：楊士萱 計畫編號：99-2220-E-027-002- 
計畫名稱：嵌入式 DVB-H 接收系統之開發--子計畫一:DVB-H 視訊解碼軟體之開發(2/2) 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 0 0 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 1 1 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 2 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
