0 
目錄 
中文摘要 1 
英文摘要 1 
一、前言與研究目的 2 
二、文獻探討 2 
三、研究方法 6 
四、成果與討論 11 
參考文獻 12 
附件 15 
 
2 
一、前言與研究目的 
美國國家高速公路運輸安全管理局(NHTSA)的統計顯示，打瞌睡原因造成意外發生的機
率(提升4倍)比開車講手機（提升30%）或撥打手機(提升3倍)還要高，以德國為例，25%的車
禍事件是由於駕駛人員精神不濟或是過於疲累而引起，一項在美國的問卷調查顯示，50%以
上的人承認曾經累到開車時睡著，而就反應時間進行研究統計，接近80%意外碰撞發生前，
其分心時間不到三秒，Mercedes-Benz公司研究發現，如增加0.5秒的反應時間，可避免60%的
追撞事故，如能增加到1.5秒，則90%的意外事故可避免。然而這些因駕駛員分心（如打瞌睡、
開車講手機或撥手機等）造成的人員損傷，以全球統計，受傷人數高達1,800萬人，死亡人數
則為110萬人，財產損失更高達1.8%全球國民生產毛額。因此利用輔助預警系統，提升駕駛人
的注意力，增加反應時間，降低危險發生機率，則是確保行車安全的不二法門，智慧型車輛
則是最有效的手段。 
在現代化社會中，車輛交通意外發生頻繁，造成社會成本急遽增加。台灣每年平均事故
發生件數高達六萬多件，依據統計，其中九成以上肇事因素皆人為所致。面對交通事故發生
率居高不下，研發駕駛安全監控系統(Driver Support System, DSS)之必要性與日劇增，若能事
先警告駕駛注意突發狀況，可預防意外事故之發生，大幅增進道路駕駛安全性，此亦屬於智
慧型運輸監控系統(Intelligent Transportation System, ITS)重要之研究課題之一，本計畫將針對
車用安全提出一套輔助系統，分為兩部分：一個是監控駕駛人的頭部行為，另一為：監控車
輛的行為，估計與前車輛的距離，與監控前車輛的行為，主要搭配CCD相機，取得駕駛人員
頭部的影像資料，動態即時偵測頭部的行為，若超出某一個預定的範圍之外，並持續一段時
間，藉此判別駕駛人員是否打瞌睡，此外，藉由兩支攝影機，偵測前方車輛的車牌，進而準
確推估前方車輛距離，避免碰撞，藉由多重資訊來源，整合判斷駕駛人員的精神狀況，與前
方車輛行為，截長補短，以彌補單一模式的盲點，以建置一套危險駕駛預警系統。 
二、文獻探討 
近年來，在智慧型運輸系統(Intelligent Transportation System, ITS)、自動化高速公路系統
(Automated Highway Systems, AHS)或是先進車輛控制與安全系統(Advanced Vehicle Control 
and Safety Systems, AVCSS)幾項議題，不論在學術界或是產業界均是相關受到重視的研究題
目，主要研究為致力於提升行車安全與便利性，更甚達成減少能源消耗，增進道路自動化，
以及增加道路所能容納的車流量。在國際上有一個受到眾人矚目的PATH計畫，目前完成了基
於基礎建設的實作方法，此計畫運用在高速公路上，裝置有許多分散式磁性元件，另外於實
驗車上加裝磁力感測器，採用的方法可以想像在機場輸送帶上，旅客只需固定排序站好藉由
手扶梯設備移動到機場的另一個位置，這個設計，可以讓高速公路上的車隊，於全速前進時，
僅需要相當短的行車車距，如此可以大量提升高速公路的行車容量[1]。另外專家指出，未來
的5年內汽車的製造成本，電子裝置將占其整體的25%以上，汽車公司豐田(Toyota)與日產
(Nissan)宣佈共同合作開發智慧軟體，並制訂軟體標準化，同時招攬其他日系汽車公司，希望
藉由團隊合作的開發，降低汽車的製造成本，提高日系汽車的競爭力[2]。除了透過基礎建設
4 
大或啟動座椅震動提醒裝置，進而達到打瞌睡警告與防撞機制。國內智慧車輛的研發與技術
引進包括：近紅外線夜視系統、前方行人偵測系統、車道維持系統(Lane Keeping System；LKS)
與全速域適應性巡航控制系統，其目的皆是適時提出警告，甚至自動迴避或煞車，降低意外
發生機率，此一類型研究以電腦視覺技術，偵測外在環境的變化，透過不同機制提醒駕駛人
員。然而預防意外發生最有效的方法，卻是駕駛人員全神貫注與良好的駕駛習慣，因此根本
解決之道是預防駕駛人員打瞌睡與提升他的注意力。雪山隧道開通，對於車輛安全性要求很
高，國內大客車業者引進含有「防瞌睡警報」功能的智慧型巴士，其造價為一般公車的2.5倍，
該裝置以駕駛員的頭為中心畫「圓」，以紅外線掃描，當駕駛員打瞌睡頭晃動超過警戒區就
會發出警示聲。利用這些特徵，判斷駕駛人員的精神狀況，適時提出警告，以避免意外發生，
畢竟「快樂出門，平安回家」終究是乘車時最高指導原則。 
本計畫是以視覺為基礎的打瞌睡偵測，我們將以影像為基礎進行眼睛偵測，後續工作便
可以眼睛區域為基礎，進行眼睛是否閉合一段時間的判斷。首先我們會偵測出人臉，接著以
人臉區域為範圍進行眼睛區域的偵測。現行常見的物件偵測方法有人臉偵測有相當多的研究
成果，目前人臉偵測的演算法主要可以分為：(1)特徵式及(2)影像式。其中特徵式(Feature-based)
演算法又可分為動態活動模型(Active shape models)、低階分析(Low-level analysis)等，低階分
析包含邊緣 (Edges)、灰階(Gray-level)、膚色 (Skin color)、動態資訊(Motion)、影像測量
(Generalized measure)等方式。影像式 (Image-based) 演算法為基礎的演算法則包含線性子空
間法(Linear subspace)如PCA，類神經網路如幅射基底函式類神經網路，以及統計類的方法如
支援向量機等。 
1. 動態活動模型：動態活動模型是利用影像的特徵，依據人臉在影像中如亮度、邊緣
等差異性擷取活動式範圍，通常利用動態輪廓(Active contour)或曲線(Snake)方式實
作，找出人臉的邊界，而後續的改良方式除了提升了型態在邊緣的地標點位置
(Landmark point)準確性，也加入了分群法，避免型態差異過大，另外在搜尋的機制
上有較佳的延展性，然而此一類型偵測器必須限制特定偵測範圍，且無法滿足及時
偵測之要求[24]。 
2. 低階分析：此一類型偵測器著重於資訊融合，色彩，動態位移量，五官特徵與幾何
資訊等等，皆是資訊融合的最佳資訊，因此適用於多專家系統，然後針對個個結果
進行投票，以決定其最後結果。換句話說，低階分析是以影像處理為主要的方法，
如以 Sobel 濾鏡，搭配邊緣為基礎的 SNoW 分類器，架構一個快速的人臉特徵分類
方法。又或者以邊緣偵測使用 Canny 濾鏡，再將轉換後影像輸入到多階層 B 木條曲
線(Multilevel B-Splines)取得特徵值，最後搭配支援向量機來分類人臉；Canny 濾鏡為
John F. Canny 於 1986 發展出來而名，使用了多階的演算法來偵測一個大範圍的邊
緣。另外許多研究在 RGB 色彩空間進行膚色偵測，RGB 色彩空間是以光的三原色
紅、綠、藍為刺激值混合出某一色彩的顯像方式，三原色中每一種色彩的值都介於
0-255 的範圍，透過紅、綠、藍等值的大小組合來決定像素的顔色。然而 RGB 存在
著亮度因素的問題，使用 RGB 色彩空間無法有效萃取出人臉膚色，故提出將 RGB
先行轉換至 YCbCr 色彩空間之後，再進行偵測膚色的方式，最後經由眼睛及嘴巴兩
項特徵比對特徵，確認是否為人臉[25][26]。 
3.  主成份分析：主成份分析之主要目的在「轉換」原始變項使之成為一些互相獨立的
線性組合變數，而且經由線性組合之後，得到的主成分仍保有原變數最多的資訊，
6 
點 可利用直線方程式 表示，因此在圖像中的一個點對應參數平面中的一條直線，圖像中的
一條直線對應參數平面中的一個點。對圖像上所有的點進行霍夫變換，在最後取參數平面中
最多直線交錯的點為表示為一直線。這樣就在圖像中檢測出了直線。在實際應用中，透過霍
夫轉換偵測直線，通常會採用參數方程式。 
三、研究方法 
本計畫分為兩大模組：分別為「駕駛人員監控」與「前方車輛監控」兩大模組，其中駕
駛人員監控包括「人臉偵測」與「打瞌睡偵測」，而前方車輛監控則包含有：「前車車牌偵
測」、「前方車輛後車燈偵測」、「前車距離計算」與「前車任意變換車道偵測」等多項功
能。國科會支援本計畫採購嵌入式行車記錄器(如圖一所示)，可以取得前方車輛與駕駛人員
的臉部影像，透過影像處理與電腦視覺技術，建構一個防打瞌睡與車輛防撞預警系統。 
 
 
圖一：嵌入式行車記錄器 
模組一：駕駛人員監控模組 
駕駛人員監控模組中，人臉偵測模組主要開發以視覺為基礎之複雜環境打瞌睡行為分析
系統，希望達成對駕駛人員進行自動臉部偵測(包含正臉與側臉)、眼睛疲勞偵測等多項工作，
若駕駛人員處於不專心或疲勞狀態，除頭部容易不規律或沈睡擺動，臉部眼皮也呈現緩慢合
閉的節奏，如能藉由本安全監控預警系統，偵測出駕駛者的瞌睡行為，在即時偵測後，提前
發出警告訊息，提醒保持警覺心，以增進行車安全。由於車輛內為一動態空間監控環境，其
光線變化與與內部晃動相當劇烈，無法直接利用廣泛使用的背景相減法(background 
subtraction)作臉部偵測與臉部追蹤，故系統改採用由粗到細的策略來進行人臉偵測，也就
8 
  
  
圖四：人臉打瞌睡結果影像序列 
 
模組二：前方車輛行為監控 
前方車輛行為監控則包含有：「前方車輛距離量測」與「前車任意變換車道偵測」 
(a) 前方車輛距離量測： 
此一技術與聖約翰科技大學電子系合作，共同發展夜間前方車輛距離量測方法，需具備
前車車牌偵測、前方車輛後車燈偵測、前車距離計算等步驟，概略說明如下：首先偵測前方
車輛之後車燈位置，如圖五所示後車燈偵測流程圖，由於影像中，車輛後車燈屬於穩定的紅
色特徵，可提供資訊，計算與前方汽車的距離，因此將原始輸入影像的RGB轉成YCbCr色彩
空間，作為主要運算空間，將亮度與彩度色調的資訊分離開，其中將Cb-Cr > threshol即為紅
色區域，標記為可能的車燈候選區域，再利用幾何的關係，找出後車燈的位置PL、PR，如圖？？
所示後車燈偵測結果。利用車燈的像素點寬度 ( )KLR DN ，計算與前車的距離 KD 如下： 
( ) ( ) SKLR
H
K WDN
N
DW ×= max_max (1) 
( ) ( ) HSKLR
H
HKK WDN
N
DWD θθ cot
2
1cot
2
1 max_
max ××=×=
 
(2) 
其中 ( )KDWmax 與 max_HN 分別為環境與影像的最大寬度與像素點個數。但是由於每一款式
的後車燈接不盡相同，兩個車燈的寬度皆不相同，造成估計前方車輛的距離困難，因此可以
10 
  
(a)原始影像 (b) YCrCb轉換後影像 
  
 
 
圖八：前車後車燈偵測範例 
(b) 前車任意變換車道偵測 
本步驟偵測行車紀錄的錄影畫面中的車道線，再佐上述步驟偵測結果，判斷前方車輛是
否變換車道的狀態。而車道線偵測方法，先將影像轉換為灰階影像，接著利用車道線與路線
顏色有明顯對比的特性進行二值化，接著利用Hough Line Transform找出畫面中的直線。由於
行車紀錄器鏡頭的擺設角度，我們可以預設畫面中地平線以上的區域是不感興趣的，直接利
用遮罩的方式略過此區域的偵測，流程圖如圖九所示。而車道線偵測結果如圖十所示。 
 
 
圖九：車道線偵測系統架構圖 
12 
參考文獻 
[1] D. Yanakiev and I. Kanellakopoulos, “Longitudinal control of automated CHVs with 
significant actuator delays”, IEEE Transactions on Vehicular Technology, vol. 50, no. 5, 
pp. 1289 – 1297, Sept 2001. 
[2]  財團法人國家實驗研究院科技政策研究與資訊中心資訊服務處 
[3]  S. Sheikholeslam and C. A. Desoer, “Longitudinal control of a platoon of vehicles with 
no communication of lead vehicle information: a system level study,” IEEE Transactions 
on Vehicular Technology, vol. 42, no. 4, pp. 546 -554, Nov 1993. 
[4]  J. Mar and F. J. Lin, “An ANFIS controller for the car-following collision prevention 
system,” IEEE Transactions on Vehicular Technology, vol. 50, no. 4, pp. 1106 -1113, 
July 2001. 
[5]  S. C. Warnick and A. A. Rodriguez, “A systematic antiwindup strategy and the 
longitudinal control of a platoon of vehicles with control saturations,” IEEE 
Transactions on Vehicular Technology, vol. 49, no. 3, pp. 1006 -1016, May 2000. 
[6]  C. Hatipoglu, K. Redmill, and Ü. Özgüner, “Steering and lane change: a working 
system,” IEEE International Conference on Intelligent Transportation Systems, Boston, 
MA, pp. 272–277, 1997. 
[7]  Cem Hatipoglu, Ümit Özgüner, and Keith A. Redmill, “Automated lane change 
controller design,” IEEE Transactions on Intelligent Transportation Systems, vol. 4, no. 
1, pp. 13–22, March 2003. 
[8]  A. Broggi, M. Bertozzi, A. Fascioli, and G. Conte, “Automatic vehicle guidance: The 
experience of the ARGO vehicle,” Singapore: World Scientific, Apr 1999. 
[9]  T. Suzuki and T. Kanade, “Measurement of vehicle motion and orientation using optical 
flow,” IEEE International Conference on Intelligent Transportation Systems, Tokyo, 
Japan, pp. 25–30, Oct. 1999. 
[10] http://www.saab.com/main/GLOBAL/en/pressreleases/1/index.shtml 。 
[11] http://www.youtube.com/watch?v=pFv0dmsZ1fI 
[12] R.L. Hsu, M. Abdel-Mottaleb, and A.K. Jain, “Face detection in color images”, IEEE 
Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 5, pp. 696 – 706, 
May 2002. 
[13] V. Vezhnevets, “Face and facial feature tracking for natural human-computer interface”, 
Graphics & Media Laboratory, Dept. of Applied Mathematics and Computer Science of 
Moscow State University Moscow, Russia, 2002. 
[14] P. Smith, M. Shah, and N. da Vitoria Lobo, “Monitoring head/eye motion for driver 
alertness with one camera”, Proceedings, 15th International Conference on Pattern 
Recognition, vol. 4, pp. 3-7, Sept 2000. 
[15] Gauvain, J. and L. Lori, “Large vocabulary continuous speech recognition: advances and 
applications”, Proc. of the IEEE, vol. 88, no. 8, Aug 2000. 
[16] Huang, X., A. Acero and H. Hon, Spoken Language Processing - A guide to theory, 
algorithm, and system development, Prentice Hall, 2001. 
[17] Y. Wang, D. Shen, and E. K. Teoh, “Lane detection using spline model,” Pattern 
Recognition Letters, vol. 21, no. 8, pp. 677-689, 2000. 
[18] H.Y. Cheng, B. S. Jeng, P. T. Tseng, and K. C. Fan, “Lane detection with moving 
vehicles in the traffic scenes,” IEEE Trans. on Intelligent Transportation Systems, vol. 7, 
no. 4, pp. 571-582, 2006. 
[19] Y. U. Yim and S. Y. Oh, “Three-feature based automatic lane detection algorithm 
(TFALDA) for autonomous driving,” IEEE Trans. on Intelligent Transportation Systems, 
vol. 4, no. 4, pp. 219-225, 2003. 
[20] Y. He, H. Wang, and B. Zhang, “Color-based road detection in urban traffic scenes,” 
IEEE Trans. on Intelligent Transportation Systems, vol. 5, no. 4, pp. 309-318, 2004. 
[21] G. Toulminet, M. Bertozzi, S. Mousset, A. Bensrhair, and A. Broggi, “Vehicle detection 
14 
Vehicles Symposium, pp. 698-703, June 2007. 
[41] A. A. Assidiq, O. O. Khalifa, M. R. Islam, and S. Khan, “Real time lane detection for 
autonomous vehicles, “International Conference Computer and Communication 
Engineering (ICCCE), pp. 82-88, May 2008. 
[42] C. R. Jung and C. R. Kelbel, “Lane following and lane departure using a linear-parabolic 
model,”Image and Vision Computing, vol. 23, no. 13, pp. 1192-1202, Nov 2005. 
[43] C. D’Cruz and J. J. Zou, “Lane detection for driver assistance and intelligent vehicle 
Application, ”International Symposium Communications and Information Technologies, 
pp. 1291-1296, Oct 2007. 
 
Y.-Y. Lu et al.
Fig. 1 The system architecture for the prevention of car collisions
found. It is hard to work at night because of the noisy envi-
ronment. In [13], the distance is measured from the informa-
tion of two contiguous image frames. However, the relative
distance is not enough for safety requirements. Many lane
detection methods [14–16] are also used for car collision
prevention. The edge features of lanes are extracted by var-
ious gradient operators in these approaches. Unfortunately,
the extracted results are of a low quality due to the blur and
unclear edges in the dark. Moreover, a lot of time is needed
to process the various light conditions at night.
In previous work [17], a circuit-based system was designed
for distance measurement at night. The number of pixels in
images from two detected taillight locations was counted to
calculate the distance between two cars. One parameter was
limited in [17]: the width between two taillights had to be
known and fixed. An average value was used for distance
estimation. This resulted in inaccurate results. To eradicate
this constraint, a license plate (LP) with a fixed size was
detected [18]. It refines the width between the taillights and
increased the estimation accuracy. In this study, four main
modules, as shown in Fig. 1, constitute the preserving sys-
tem: the taillight detection, the distance measurement, the
LP detection, and the alarm decision modules. The first two
modules estimated the distance from the car in front in real
time. When the system was initialized, the parameter, i.e.,
the taillight width, was first set as an average value. This
parameter was used until we keep away from the frontal car
at a safe distance. The CCD camera is triggered and zoomed
in to grab a close-up image of LP. On detecting the LP, the
measuring parameters were updated for accuracy enhance-
ment. Finally, an alarm was made when the car in front was
too close. These procedures were continuously run to keep a
safe distance from the car in front during driving.
The proposed vision-based scheme is quite different from
those in the previous works [17] even though the geomet-
rical formulas for distance measurement were very similar.
An electronic circuit was designed for distance measurement
in the previous works. Signals in channel R were extracted
and the distance was estimated from the designed circuit.
Two shortcomings exist in the previous work. First, signals
extracted from channel R are usually at saturation states due
to the strong and complex light population. The taillight
extraction should be performed in a simple and dark space.
The designed circuit is too sensitive to light population to
extract the taillights exactly. In the new vision-based method,
two taillights are extracted from a light-polluted environment
such as lamplights on a highway. Second, the adaptability of
the circuit-based method is very low. The width of two tail-
lights should be fixed by using an average value because there
is no referred template, i.e., LP. Using the new approach, LP
whose size is fixed is detected and used for a referred cue
of the taillight width. Accordingly, the measuring scheme
is modified. The parameter, i.e., taillight width, is dynam-
ically measured in the proposed method. Furthermore, the
experimental conditions between them are quite different.
The experiments in the previous work are simple and the
testing car remained stationary. The experiments in this study
have been done on the driving cars on a highway.
2 Distance measurement
Distance measurement using visual features is based on the
proportionality of similar triangles. As is known, the number
of pixels in the image of an object is inversely proportional
to the distance from the camera. Two grabbed images are
shown in Fig. 2b and c. Two taillights of width WS are the
detected targets for distance estimation. Consider the mini-
mal distance hx at which two taillights are grabbed simulta-
neously as shown in Fig. 2a. If a car was located at distance
HK , the number of pixels between two taillights NL R(HK )
is obtained from the grabbed image. The relation between
them can be represented in Eq. (1). By rearranging the sym-
bols, the distance from the car in front to the CCD camera is
calculated as follows:
hx + Hk
hx
= Wmax
WS
= Nmax
NL R (Hk)
, (1)
and
Hk =
(
Nmax
NL R (HK )
− 1
)
hx . (2)
Here, symbols Wmax and Nmax denote the width of field
of view (FOV) and its corresponding image width at a dis-
tance hx + HK . Two constraints are imposed on this scheme:
(1) The minimum distance hx must be known, and (2) the
referred width WS must be fixed. One possible solution is
to set width WS to be the average width between various
taillights. A new mechanism is proposed to solve these two
problems.
123
Y.-Y. Lu et al.
Fig. 3 The configuration of the
proposed system
Fig. 4 The computation of parameter θH
in this study. Another approach is to use the LP to refine the
parameters. Since LPs are issued by governments, the width
WP can be pre-defined. This can enhance the accuracy of
distance estimation. The taillight width is replaced with the
variable WP to precisely calculate the value HK . Using this
feature, the taillight width Ws is updated and the estimated
distance is calculated as follows:
Nmax
NP (HK )
WP = NmaxNL R (HK )Ws, (5)
and
HK = 12
Nmax
NP (HK )
WP cot θH . (6)
3 Taillight and license plate detection
As shown in Fig. 2, the extracted widths between car tail-
lights in image frames depend on the distance from the car
in front. Since the driven car and the one in front of it are in
the same lane, the region of interest (ROI) for taillight detec-
tion was reduced to the central region of a grabbed image
as shown in Fig. 6a. The ROI was manually defined as the
region above the skyline in the middle of an image. Because
of the reduction of ROI, the computational cost was signif-
icantly reduced. Subsequently, the color and intensity fea-
tures in ROI were analyzed to find the taillight locations.
The detected taillights shown in Fig. 6g were drawn using
red lines. The width between two taillights NL R(HK ) was
thus obtained by counting the pixels between two red points.
The procedure for taillight detection is briefly described
in the following. In analyzing the images, it is difficult to
distinguish the taillight regions from the streetlamp regions
in the RBG color space. All their values are at the saturation
state. The pixels within the ROI in the RGB color space were
converted to the YCbCr color space by Eq. (7).⎡
⎣ YCb
Cr
⎤
⎦ =
⎡
⎣ 0.229 0.587 0.114−0.168 −0.331 0.499
0.50 −0.419 −0.081
⎤
⎦
⎡
⎣ RG
B
⎤
⎦ . (7)
Due to the light population and dark background as shown
in Fig. 6, the taillights colors are not shown in red. Though
they are frequently at the saturation levels in RGB chan-
nels, strong responses occur around the taillights in channel
YCbCr. In the illustrated example, the image pixels around
the taillights had high values in channel Cr. In addition, the
taillights and the other bright pixels had high values both in
channels Y and R. Thereafter, the taillights were the regions
with high brightness in channels Y, R, and Cr as shown in
Fig. 6b–d.
Backpropagation neural network (BPNN) classifier has
widely been utilized in the pattern recognition. In this study,
this well-known classifier was used to classify the image
123
Y.-Y. Lu et al.
Fig. 6 Taillight detection using
the NN classification. a An
original color image, b the
transformed image in channel Y,
c the transformed image in
channel R, d the transformed
image in channel Cr, e the output
of a trained NN, f the labeled
image, g the detected taillights
An original color image The transformed image in channelY 
The transformed image in channel R The transformed image in channel Cr 
The output of a trained NN. The labeled image. 
The detected taillights.
(a) (b)
(d)(c)
(e) (f)
(g)
features to the strong classifier. When more features are
added, computational time rapidly increases for detecting
the LPs. The efficiency of a strong classifier is significantly
improved by using a cascaded structure which was proposed
by Viola and Jones [18]. The basic idea of cascaded structure
is as follows: Most of the background regions are efficiently
filtered out by the weaker classifiers, and the detected LP
should satisfy the strong classifier, e.g., a set of weaker clas-
sifiers. During the training phase, 1,200 positive and 12,000
negative samples were collected to train the cascaded detec-
tors over a period of 7 days. A Haar-like function with a
window of 40 by 20 is adopted for LP detection. 228,056
randomly generated features were evaluated to choose 12
weaker classifiers for constructing a cascaded strong classi-
fier.
Generally, object detection is a difficult work in a low
luminance condition. Fortunately, LP possesses the alpha-
betic and numerical digits in high contrasts. Edge detection
is a popular and effective process to find the text locations.
However, the unwanted noises such as the car shapes, tail-
lights, lane lines, …, etc., are also extracted. In this study, the
edge information is first used and an AdaBoost-based detec-
tor is then used to find the exact LP location. The detection
process is summarized as follows:
Step 1: Extract the ROI image from the region between two
taillights which are detected from the taillight detec-
tion module.
Step 2: The ROI image is processed by a Canny edge detec-
tor. Filter out the regions with low edge densities.
123
Y.-Y. Lu et al.
parameter (e.g. a negative acceleration), the human response
time, the current speed, and the needed braking distance, are
considered in calculating a safe distance. The brake parame-
ter depends on the current speed. This should be determined
by users. In addition to the above four factors, the speed of
the car in front was not considered. To illustrate the compu-
tation of safe distance, an example is given in the following.
The brake parameter relative to the current speed is set as
−10 m/s2 on the highway. Under this condition, a car will be
slowed down from 108 km/h to 72 km/h in 1 s in an emer-
gency. Similarly, a car at 36 km/h (10 m/s) will be stopped
in 1 s. Consider the variables in Eq. (8), symbols V0, V, S,
and a represent the current speed, the speed after braking,
the brake distance, and the acceleration, respectively. When
the car speed is slowed down from 108 km/h to 72 km/h,
i.e., from 30 m/s to 20 m/s, a braking distance S = 25 m from
Eq. (8) is needed. According to statistics, the human response
time for an accident is within a range of 0.2 to 0.3 s [19]. Sup-
pose the maximum response time of a person is 0.5 s; then
the safe distance can be calculated from Eq. (9) in which
symbols DS, S, and t represent the safe distance, the brake
distance, and the response time. The safe distance is equal to
the braking distance plus the running distance of a car at the
speed before the human response (at 0.5 s). In other words,
the safe distance is DS = 25 + 0.5 × 30 = 40 m.
V 2 = V 20 + 2aS, S =
V 2 − V 20
2a
(8)
DS = V0t + S (9)
Second, the distances from the car in front were estimated
using the taillight and LP features as described in Sect. 3.
According to the pre-defined LP width, the width of two tail-
lights was precisely re-calculated and the distance estimation
was improved. When the measured distance from the car in
front is larger than the safe distance, the driven car is in a safe
state. Otherwise, an alarm will be set off. Furthermore, when
the driven car is at a safe distance (with no collision risk), the
CCD camera is triggered for LP detection. Parameter WS is
then updated.
The implemented algorithm is briefly summarized as
follows:
Step 1: Grab the video sequences and initialize parameter
Ws as the average width between taillights.
Step 2: Detect the two taillights of the car in front in ROI.
Step 3: Count the number of pixels between the two tail-
lights, and estimate the distances using Eq. (4).
Step 4a: When the car in front is close, an alarm is set off.
Step 4b: When the distance is in a zooming range, zoom
the CCD camera, grab a close-up image, and detect
the LP. If the LP is detected, update the measuring
parameter WS using Eq. (5).
Step 5: Repeat Steps 1 to 4 for always keeping at a safe
distance.
5 Experimental results
In this section, some experimental results are presented to
demonstrate the effectiveness of the proposed method. In the
experiments, the images of 720 by 480 pixels were grabbed
from a DV camera type Sony DCR-SR300. The parameters
in this study were set as WP = 32 cm and cot θH = 2.21.
The quality of optical lens in current commercial products is
good enough. The optical distortion is small. In addition, the
distortion of a grabbed image could be improved using the
calibration techniques and the predefined patterns [20,21].
Furthermore, the ROI in this study is located at the center
area where the distortion is the smallest. The proposed algo-
rithm could be executed in real time: 32 ms per frame on
average on a Pentium IV PC machine. Three experiments
were implemented: The first one was to estimate the dis-
tance between two cars when stationary, the second one was
to estimate the distance of two running cars, and the third
one was to estimate the distance on the highway.
In the first experiment, a car was stopped at a distance from
a camera and the images were grabbed. The distances were
set from 5 to 50 m in a 5 m increase. The ground truth of dis-
tance was obtained by a laser distance measurer. In addition,
the widths of an LP (e.g. 0.32 m) and that of two taillights
(e.g. 1.52 m) were manually assessed. The distances from the
car in front were measured from the images. The experimen-
tal results for various distances are tabulated in Table 1. In
the first condition (case a), the width of two taillights, 1.52 m,
was known after being manually measured. On average, the
errors were all below 5%, and the averaged error was 1.6%.
In the second condition (case b), when the distance between
two taillights was unknown, an average width of 1.305 m,
averaged from 70 cars, was used to measure the distances.
More than 10% error was obtained. In the third and the fourth
conditions (cases c and d), the width WS was refined by the
detected LP. This value was re-calculated at distances 5 and
10 m. The estimated distances and errors are tabulated in col-
umns (c) and (d) in this table. From this table, the accuracy
was improved by more than 8%.
In the second experiment, the images were grabbed from
two running cars. The distance between the two cars was
measured by a device with laser reflection distance measure-
ment (e.g., Leica DISTO A5). The relative speed between
the two cars was limited to a range of 5–15 km/h. All ground
truth data were collected from the laser measurer as shown
in Fig. 9a. The distances were also estimated from the cor-
responding images. The average taillight width was manu-
ally initialized as 1.305 m in this experiment. The taillight
widths for cars1 and 2 were recalculated as 1.2 and 1.34 m
123
Y.-Y. Lu et al.
Fig. 10 Two detection results
of LPs on the highway
Fig. 11 Taillight detection in
various conditions. a–e The dark
background, and f, g the bright
background
123
 
國科會補助專題研究計畫項下出席國際學術會議心得報告 
                              日期： 98 年 10 月 30   日 
計畫編號 NSC 97-2221-E-239 -023 -MY2 
計畫名稱 未來車－打瞌睡警告及防撞系統 
出國人員
姓名 
韓欽銓 服務機構
及職稱 
國立聯合大學資訊工程學系 
教授 
會議時間 
99 年 10 月 4 日
至 
99 年 10 月 11 日 
會議地點 瑞士蘇黎克(Zurich) 
會議名稱 
(中文) 2009 年 IEEE 國際卡拉漢安全技術研討會 
(英文)2009 年 IEEE International Carnahan Conference on Security 
Technology 
發表論文
題目 
(中文) 利用金字塔幻覺方法，強化模糊人臉影像或車牌影像 
(英文) The Interpolation of Face/License-plate Images Using 
Pyramid-based Hallucination 
附件四 
(a) 08.40 - 10.00: Sensor and Detection Technology(8a) and Miscellaneous(8b) 
(b) 10.30 - 11.30: Biometrics(9a) 
二、與會心得 
此一研討會議係源起 1967 年於美國創立，隸屬美國電機電子工程師學會(IEEE)，為國際
上安全科技領域存在最久、最具權威之學術研討會議，本會固定於每年十月份召開國際學術
研討會議，每兩年交由非美加地區國家主辦。其組織分為國際執行委員會議（為大會之實際
行政決策機構）、技術諮詢委員會議（負責技術與論文審查）及國際研討會議等三層組織。亦
是國際上少數以「中華民國 Republic of China」稱呼我國之國際學術組織。本次參加此研討會，
除與各國學者做學術技術交流之外，還發表兩篇文章，並與參與會議人士詳細討論，引起與
會人士興趣，獲得肯定。 
此次論文重點於『機場安全議題』上，除了邀請瑞士警政安全專家顧問，針對機場安全
的實務經驗與面對的問題，進行專題演講，也有四場的論文發表，為本次安全議題的主軸，
此外，生物特徵確認技術仍受到重視，也有三場的論文發表，其餘的包括資訊、通訊安全、
感應偵測追蹤技術、電腦系統網路入侵偵測等等，也都有多篇論文發表。安全科技業由傳統
之錄影監測逐步發展至今日以反恐怖活動為主之資訊安全技術與生物特徵辨識兩大類主題，
其中，當資訊分享所帶來的好處大於資訊保密時，應考量藉由資訊等級及種類之定義、分發
與管制等，並善用資訊科技以降低資訊統合與分享所帶來的負面效應。網路駭客攻擊與病毒
肆虐日益嚴重，資訊隱藏與資料保護機制是國防安全機制上不可或缺的架構，尤其防範電腦
設備或機密資料遭受竊取，以確保整個國家之安全，恐怖攻擊事件之後，個人身份的識別相
對凸顯其重要性，尤其在機場安全控管上，更是重要。 
三、考察參觀活動(無是項活動者略) 
無參觀考察活動 
四、建議 
本研討會屬於安全議題的研討會，在 22 位的執行委員會議中指導委員，國內就佔三位，
皆是國安單位人員，可見此一議題之研討會已受國內重視，建議多多鼓勵國內研究單位參加，
藉由彼此了解與強勢曝光，於國際學術佔一席之地，尤其國內目前之資訊安全相關產業蓬勃
發展，各種安全監控之軟硬體技術可與世界先進技術接軌，甚至可超越，卡拉漢國際年會議
經常鼓勵參加者，經由他們於社會中的工作影響力，協助與會得人士於安全議題上的經驗傳
承與諮詢，藉由此一研討會，除可提升我國在國際間的學術地位，亦可以國外友邦之國安人
員進行交流，一舉數得。 
五、攜回資料名稱及內容 
Proceeding of 43th IEEE International Carnahan Conference on Security Technology 會議論
文一本，包括此次所有發表論文內容。 
六、其他 
活動照片 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
本計畫一發表一篇國外期刊論文，預計繼續撰寫研究成果，發表一篇國外期刊
論文。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
