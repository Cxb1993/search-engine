 1
前言 
本計畫之主要目標是以研究多種顯微成像之影像分析技術為主軸，包含高解析度光
學、原子力顯微鏡（Atomic Force Microscopy, AFM）、光學同調斷層掃瞄術(Optical Coherence 
Tomography, OCT)等具高影像解析度與特殊掃瞄功能之顯微影像系統，並且依照其不同之
需求發展演算法與軟體系統。 
 
研究目的 
我們以神經細胞於顯微影像上之分割為分析之重點，進行相關之研究。其主要成果為
發展一個多階分水嶺分割方法並結合模糊理論系統來自動化分割細胞。整體而言，利用多
階分水嶺分割方法來偵測細胞可能的位置，有助於處理因為在同一張影像上明亮度不均
勻，以單一階分水嶺分割方法無法找到細胞的問題；在細胞分割過程中，以三組模糊規則
分別對細胞偵測、給予動態輪廓模型權重、確認細胞等部分加入細胞的特性與影像上空間
資訊，此有助於針對不同的細胞狀態，給予適合之分割條件；另外，所採用之粒子群尋優
法演算法，對於訓練大量的模糊規則參數將提供有效率之參數估算。接續細胞分割之研究
成果，我們將進行細胞之對位與重構。藉由連續影像對位之資訊，分割出一些在先期研究
中未能分割出的細胞。首先以全域性疊代最近點演算法（Iterative Closest Point, ICP），找到
連續影像間相對應的細胞，再用局部性 ICP 對位法，針對各組對應細胞作微調，以達到較
佳之對位結果。一旦在連續影像上之細胞連結關係建立，可進而針對未分割出的細胞做補
償之動作。整體而言，以對位分割之方式來幫助因為一些受影像品質影響而未能分割出的
細胞，進而重構出神經以提供臨床上神經修復評估之參考。 
另外，我們研究顯微鏡儀器之探針模型，以原子力顯微鏡(AFM)來掃描細胞之 3D 形
貌。其主要研究為結合細胞之 3D形貌與其影像上的紋理(texture)資訊，以重構細胞之完整
型態。所採用之方法為分別找出在 3D 形貌與紋理這兩張影像之特徵點，以 ICP 對位方法
將這兩張影像對位，進而找出對應之參數。 
除了顯微影像與原子力顯微鏡，我們也專注在光學同調性斷層掃描(OCT)影像之分割方
法上。OCT是一種具有高解析度類似超音波的新成像技術。然而其所伴隨產生的雜訊也將
造成分割上的一個難題。本階段之研究為提出一個自動化的分割方法於 OCT影像上，所採
用之影像有視網膜 OCT 影像與手部皮膚 OCT 影像。視網膜厚度的量測是診斷有無罹患青
光眼的重要指標，因此分割視網膜內不同的組織層對於診斷是一個關鍵因素。而手部皮膚
厚度之量測，主要為提供在接受刺激前後皮膚吸收劑量的變化程度。我們提出一個自動化
的分割方法，其適用於偵測視網膜組織與皮膚組織之 OCT影像上。所採行的方法首先用濾
波器來降低影像之雜訊，接下來用動態輪廓模型來分割不同組織層。不同於傳統動態輪廓
模型，我們在模型中加入以主成分分析為基礎的相似性能量，來輔助不同組織層之分割。
由實驗可以看出，所提出的方法用於 OCT影像上有不錯之結果。 
 
 
 3
研究方法 
(一) 神經細胞之分割與重構於顯微影像之研究 
(1)方法簡介 
首先，我們以顯微影像上之神經細胞分割為分析之重點，進行相關之研究。其主要成
果為發展一個多階分水嶺分割方法，並結合模糊理論系統來自動化分割細胞。 
圖一(a)為一張典型之神經細胞顯微影像，神經細胞之構造為軸突區域加上一黑色髓素
區域，其包圍著軸突，如圖一(b)之單一神經細胞放大圖所示。單一神經細胞分別為內圈與
外圈所環繞，內圈輪廓之灰階變化從軸突往髓素方向為亮到暗，外圈輪廓之灰階變化從髓
素往背景方向為暗到亮，我們將依據此特性作為細胞內外圈偵測之條件。 
 
 
 
 
 
 
 
 
   
 
 
 
     (a)                                  (b) 
圖(一) 顯微影像 (a) 典型之神經細胞切片顯微影像(影像大小256 × 256) (b) 單一個神經
細胞放大之影像，其中A箭號所指的是軸突(axon)，B箭號所指的是髓素 (myelin sheath，
MS) 
 
 
傳統之分水嶺分割法可藉由調整其氾濫參數(immersion)，以達到不同之分割結果。圖
(二)為顯示在不同之氾濫參數，所得到不同之分割區域。 
 
 
 
 
 
 
 
(a) (b)                   (c)                  (d) 
 
 
A 
B 
 5
2
2
2
( )( | , ) exp , 1,2...,r
i
i ir
i ir irA
ir
xf x  i mμμ σ σ
⎛ ⎞− −= =⎜ ⎟⎝ ⎠
, 
 
(2) 
其中， irμ 與 2ir  σ 分別為在第 r個規則中第 i個特徵值之平均數與標準差。 
輸出類別定義為 y，將藉由最大化θ 之概似估計所得到: 
 
1
max( ( | ))
R
r r rr
y L x CFθ
=
= , 
         2
1
( | ) log( ( | )), { , }, 1, 2,... .r
i
m
r r i r r ir irA
i
where  L x f x   i mθ θ θ μ σ
=
= = =∑  
 
 
 
(3) 
 
其中， 2{ , }, 1, 2,... , 1, 2,...ir ir  i m  r Rθ μ σ= = = ，θ 的維度為 2m R× × ，m為特徵值個數，R為模糊
規則個數。 
 
這些模糊參數之最佳化將由 PSO來完成。訓練參數之過程如下所述: 
步驟一: 有 N組訓練樣本，每一組樣本表示為 ( , ), 1, 2,...,n nx y  n N= 。其中， 
       1 2( , ,..., )n n n nm x x x x= 為一個 m×1的向量，描述 m個特徵值; yn為第 n組樣本之分類 
      結果。                     
步驟二: 設生成個數(generations number, G)、初始化疊代索引 g、c1、c2.  
 
步驟三: 以隨機方式設定每個粒子的位置 X 及其速率V 。 
 
步驟四: 將粒子的位置代入評估函數以求得每個粒子的適合度。 
x belong to the class
( | ),
1, 2,..., (4)
p
t r r p
 t
1)   L x  
                       t M                                                                                              
φ θ=
=
∑
1
arg max (5)
M
t tt
2)                                                                                                            φ φ′ ==  
        3) 對第  thr  個規則來說， rCF 定義如下: 
           
1
1
, where (6)
1
M
t t
r M
t
t tt
t
CF  =    =                                                                         
M
φ φ φφ
φ
′
= ′≠
=
−
−∑∑
   
4) 計算適合度，此值設定為分類之正確率。 
 
步驟五: 每一個粒子的適合度與該粒子所經歷的最佳適合度(lbesex)比較，假若新的適合度
比粒子的最佳適合度佳，則以新的位置及適合度取代粒子的最佳解位置及適合度; 
每一個粒子的最佳適合度與群體的最佳適合度( gbest )比較，假若粒子的最佳適合
度比群體的最佳適合度佳，則以粒子的最佳解位置及適合度取代所有粒子的最佳
解位置及適合度。 
 7
    
 
圖(三) 細胞分割之流程圖 
 
 
接下來，我們將在顯微影像上進行神經細胞之對位。接續細胞分割之研究成果，我們
將進行細胞之對位與重構。藉由連續影像對位之資訊，分割出一些在先期研究中未能分割
出的細胞。首先以全域性疊代最近點演算法（Iterative Closest Point, ICP） [24]，找到連續
影像間相對應的細胞，再用局部性 ICP 對位法，針對各組對應細胞作微調，以達到較佳之
對位結果。一旦在連續影像上之細胞連結關係建立，可進而針對未分割出的細胞做補償之
動作。整體而言，我們欲以對位分割之方式來幫助因為一些受影像品質影響而未能分割出
的細胞，進而重構出神經以提供臨床上神經修復評估之參考。 
如圖(四)所示，(a-c)分別為 3張連續影像，代表某 2條神經細胞之連續切片影像，在第
一張與第三張影像，可以看到有 2個已被偵測之細胞(實線區域輪廓)，而在第 2張影像上，
只偵測到細胞#1，而細胞#2 未能被偵測出來。在實際神經顯微影像上，如圖(五)所示，這
些在之前的研究未能被偵測出的細胞，主要是由於其形狀較狹長或是色彩分布較不均勻，
而不符合所定的規則所致。所以，我們將結合連續影像上的資訊，以對位分割方法，來幫
助偵測未能分割出的細胞。 
 9
 
 
 
 
圖(六) 對位分割流程圖 
 
 
(2.1) 多階分水嶺分割法 
對一張欲分割之細胞顯微影像，氾濫參數改變由大至小，執行分水嶺分割法，即為多
階分水嶺分割法，於此部分分割出之區域稱為 pre-candidate細胞。 
 
(2.2) 候選細胞篩選 (模糊規則 I) 
然而，分水嶺分割法所得到之區域，因為事先未給予細胞之資訊，所以大都不為完整之
細胞區域。於此，我們會針對得到之分水嶺區域，擷取三個特徵值來篩選出候選細胞區域，
並以模糊規則 I來幫助候選細胞之篩選。以下為敘述三個所需之特徵值: 
 
 11
則 I 之輸入值，三個輸出值分別為邊界在內圈、外圈與非細胞。若是 pre-candidate 細胞通
過模糊規則 I而被歸為邊界在內圈或外圈時，其將成為候選細胞並存在於一陣列裡。 
 
 
(2.3) 動態輪廓模型內能量項權重之給予(模糊規則 II) 
接下來用動態輪廓模型(Active contour model, ACM)對候選細胞之邊界做形變。以下為
ACM之表示式: 
          
1
1 internal 2 external 3 thickness0
1
( ( ) ( ) ( ) )
N
i
E v E v E v E dsλ λ λ
=
= + +∑∫  
 
(13) 
其中， internalE 、 externalE 與 thicknessE 分別為內部、外部與厚度能量， 1( )vλ 、 2 ( )vλ 、 3( )vλ 分
別為其權重。 
 
z 內部能量 
其中， ),( jiv 為在第 ith條搜尋線上第 jth個搜尋點。 ),( jiv′ 為對點(i, j)作一階微分表示連接性。
),( jiv ′′ 為對點(i, j)作二階微分表示曲率。 
 
z 外部能量 
其中， )1,(),( +−=Δ jigjiggi ， ),( jig 與 )1,( +jig 分別為在第 ith條搜尋線上第 jth與 j+1th
個搜尋點。gmax 與 gmin分別為在第 ith條搜尋線上最大與最小灰階值。 
 
z 厚度能量 
 一般而言，細胞之內圈與外圈有著相似的形狀，其之間的期望厚度為d。因此，如果在內
圈上之點w(i)向外擴張到位置v(i, j)，其之間的距離越接近d， thicknessE 會越小。                  
在形變的過程中，在第 i條搜尋線上，snaxel w(i)將會移到其能量值最小的地方。對所
有搜尋線上的 snaxel將會計算一平均能量值，在達到最小化此平均能量值後即停止形變。 
 
由於不同種類之候選細胞其邊界特性不一樣，因此必須針對不同種類候選細胞的特
性，給予適合之 ACM 權重。我們訂定模糊規則 II 來幫助 ACM 權重的給予。於此，定義
二個特徵值當作模糊規則 II 之輸入值，一為此候選細胞座落之位置為內圈或外圈; 二為此
          
2 2
internal
| ( , ) | | ( , ) |min ,  1 ,
m
v i j v i jE
s
⎛ ⎞′ ′′+= ⎜ ⎟⎝ ⎠
 
 
(14) 
          external
max min
igE
g g
Δ= −  
 
(15) 
          
2
( , ) ( )
thickness
v i j w i dE
d
− −⎛ ⎞= ⎜ ⎟⎝ ⎠  
 
(16) 
 13
調準 (alignment)。基於最小平方誤差之準則，可以找出 T，所採用之公式如下: 
其中，T為一剛性轉換參數， ( ) ( )i iT s R s t= ⋅ + KK K ，R為旋轉參數而 tK為位移向量。 
將點集合 S對位到點集合M，所用的兩個步驟為: 
(1)找出對應關係； 
(2)計算旋轉與位移參數。 
於 (1)，基於在第 (k-1)次之轉換參數 1 1( , )k kR t− −
K 找出在 S 與 M 二組點集合之對應性
1{( , ( ))} s
N
k jj  C j = 。 
於(2)，基於目前之對應關係 1{( , ( ))} sNk jj  C j = ，計算第 k次之轉換參數 ( , )k kR t
K ，如下所示: 
 
(2.6) 局部性 ICP 
一旦從全域性 ICP找到配對之細胞，接下來將執行局部性 ICP之細胞對位。其作法為
對每一組配對之細胞輪廓，各依等角度間距取 20點，進行局部性 ICP對位。此部分主要讓
各組配對之細胞，能針對自己局部性的資訊，找到一組較佳之對位參數。 
 
(2.7) 確認連接性以補償來分割出之細胞 
未分割出的細胞將藉由上述對位分割之幫助來給予補償。如圖(四)所示，細胞#2 可以
在第一與第三張影像上偵測到，然而卻未在第二張影像上出現。因此，藉由第一與第二張
影像之全域 ICP參數，可以先製造一個假的細胞#2於第二張影像上。然後由這個假的細胞
#2往前與往後各一張影像作局部性 ICP對位，找其相對應的細胞#2。若能找到相對應之細
胞，表示此假的細胞#2具有連接前後張影像的條件，於是有其存在之必要。因此，此假的
細胞#2，就會被增加到第二張影像中。根據以上的確認連接性規則，利用連續影像的資訊，
來幫助細胞之分割。 
 
(二) 原子力顯微鏡影像之細胞重建與顯示 
(1)方法簡介 
另外，我們研究顯微鏡儀器之探針模型，以原子力顯微鏡(AFM)來掃描細胞之 3D形貌。
其主要目的為結合細胞之 3D 形貌與其影像上的紋理(texture)資訊，以重構細胞之完整型
態。圖(八)為由 AFM掃描一個細胞所得到之細胞表面高度，而圖(九)為此細胞之紋理影像。 
所採用之方法為分別找出在 3D形貌與紋理這兩張影像之特徵點，以 ICP對位方法將這
兩張影像對位，進而找出對應之參數。然而，在此之前必須找到欲對位之兩張影像的特徵
點，以進行 ICP對位。在此，以 Harris之角點偵測法來進行特徵點之偵測。 
 
 
                2( ) 2, ( ) {1,2,..., } 1
min || ( ) ||
p
m
N
i j iT j i N i
T s m∈ =
−∑ K K   (18) 
                
T
2
( ) 2
R R=I , det( ) 1, 1
(R , ) arg min || R ||
s
k
m
N
k k j c j
 R  t j
t s t m
= =
= ⋅ + −∑KK KK K   (19) 
 15
其中，C(x, y)為一矩陣，描述著局部鄰域的灰階分布。 
對每個點(x, y)，計算C(x, y)之特徵值， 1λ 與 2λ 。如果 1λ 與 2λ 都大於某一門閥值，此點為角
點。 
 
2.2重覆最近點演算法(ICP) [26] 
分別對表面高度與紋理影像執行，Harris角點偵測後，可得到兩組點集合: P與 Q。一般
而言，藉由最小化能量函數之總和平方誤差，以找到 P、Q 這二組點集合之最佳配對轉換
參數。能量函數定義如下: 
其中，T為轉換函數 ( )T z Rz t= + ; 
p為點集合P中的一個點; 
q為點集合Q中的一個點; 
( )iφ 為在點集合P內，最靠近點q之點索引。 
 
轉換函數 T最佳化之過程如下: 
步驟一: 尋找二組點集合 P、Q之對應。 
步驟二: 計算轉換參數 T，滿足最小化等式(4)。 
步驟三: 應用轉換參數 T於點集合 P。 
步驟四: 重複步驟一~三，直到終止條件到達。 
 
在找到兩組點集合之轉換參數後，即可知在高度與紋理影像上兩組特徵點之轉換關
係，再應用內插函數，可以找出其他非特徵點之轉換參數。 
 
(三) 組織層之分割於光學同調性斷層掃描影像之研究 
(1)方法簡介 
OCT 是一種具有高解析度類似超音波的新成像技術。然而其所伴隨產生的雜訊也將造
成分割上的一個難題。本研究為提出一個自動化的分割方法於 OCT影像上，所採用之影像
有視網膜 OCT影像與手部皮膚 OCT影像。 
 視網膜厚度的量測是診斷有無罹患青光眼的重要指標，因此分割視網膜內不同層的組
織對於診斷是一個關鍵因素。如圖(十)所示，為一張典型之視網膜 OCT影像，欲分割的組
織層分別為：Nerve fiber layer (NFL)、 outer nuclear layer (ONL)與 choriocapillaries (ChCap)。 
而手部皮膚厚度之量測，主要為提供在接受刺激前後皮膚吸收劑量的變化程度[27-29]。如
圖(十一)所示，為一張典型之手部皮膚 OCT 影像欲分割的組織層分別為角質層(stratum 
corneum，以下簡稱 SC)，介於表皮(epidermis)與真皮(dermis)之組織層(以下簡稱 ED) 。 
 
      
2
2
2
( , ) [ ( , ) ( , )]
[ ( , )] [ ( , ) ( , )]
[ , ]
[ ( , ) ( , )] [ ( , )]
i i i i
w
x i i x i i y i i
w w
x i i y i i x i i
w w
C x y f x y f x x y y
      f x y                f x y f x y x
           x y
yf x y f x y          f x y
= − + ∂ + ∂
⎡ ⎤ ∂⎡ ⎤⎢ ⎥= ∂ ∂ ⎢ ⎥⎢ ⎥ ∂⎣ ⎦⎢ ⎥⎣ ⎦
∑
∑ ∑
∑ ∑
 (22) 
      2( )
1
( ) (| ( ) |)
Q
i
i
E v w p T qφ
=
= −∑  (23) 
 17
(, , , 1 1, 1, 1 1, , 1, , 1 1, 1, 1
, 1 , , 1 , 1 , , 1 , 1 , , 1 , , 1 , ,
( ) 2( ) ( )
4
2( ) 2( 2 2 ) 2( )
t dt t t t t t t t
x y x y x y x y x y x y x y x y x y x y x y
t t t t t t t t t t
x y x y x y x y x y x y x y x y x y x y x y x y x
dtI I b b I c c I b b I
           a a I a a a c c c I a a I
+
− + + − + + + + + +
− − − + − + +
= + − + + + + + +
+ − + + + + + + +
)
1
, 1 1, 1, 1 1, , 1, , 1 1, 1, 1( ) 2( ) ( )
y
t t t t t t
x y x y x y x y x y x y x y x y x y           b b I c c I b b I
+
− − − − − − + − − +
+
+ + + − +
接下來，會運用擴散張量Ｄ到局部性之結構，Ｄ與 Jρ擁有相同之特徵向量。Ｄ內之元素可
以表示成如下: 
其中 1λ與 2  λ 為領導系數， 1λ 2 21 2max(0.01, 1 exp( ( ) / )), μ μ κ= − − − 2 0.01λ = 。 
以離散之方式來近似此擴散張量可由下式表示： 
其中，t為離散尺度，dt為尺度等級，x，y為在影像上之位置座標，I t為在尺度 t之影像。 
a，b，c為擴散張量之元素。 
 
(2.3) 影像分割-動態輪廓模型 
以皮膚 OCT影像為例，如圖(十一) ，我們會對 SC層進行分割，其為在影像上每一行
由上往下搜尋第一個最強的邊界。先以 sobel運算子在經過前處理之影像進行邊緣檢測。接
下來在每一行由上往下於每一個像素點，定一 11×11 大小之搜尋視窗，如果此搜尋視窗內
有超過 10 個邊緣點(edge)，此一像素點即被選為候選點，並停止本行之搜尋。在每一行都
選出候選點後，即有了 SC層之始位置，接下來，以 snake model來找出 SC層。動態輪廓
模型定義如下： 
其中， intenalE 與 extenalE 分別為內部連續性與外部影像之能量項，α與 β分別為各自之權重。 
在動態輪廓模型形變的過程中，以每隔 5 個候選點選出一個當成 snaxel，進行動態輪廓模
型形變，其收斂條件為等式(30)達到最小化為止。在得到 snaxel後，用 Bspline函數連接起
來，便是 SC層，如圖(十二)所示之綠色邊界。接下來，要分割 ED層。以 SC層為參考，
將其下移 25個像素點的位置，當成 ED層之初始位置(如圖(十三)所示之白色邊界)。若是以
等式(30)來當作 ED層收斂最小化之依據，其分割之結果將會失敗(如圖(十四)所示之黃色箭
頭)。因此，我們改良等式(30)，藉由加入一相似性能量，來幫助 ED 層之分割。改良後之
     ( )21,2 1 ( ) 42 211 22 11 22 12s s s s sμ = + ± − + , (27) 
    
a b
D
b c
⎛ ⎞= ⎜ ⎟⎝ ⎠ , 
             1 2 11 221 2 2 2
11 22 12
( )( )1 ,
2 ( ) 4
s sa
s s s
λ λλ λ⎛ ⎞− −⎜ ⎟= + +⎜ ⎟− +⎝ ⎠
1 2 12
2 2
11 22 22
( ) ,
( ) 4
sb
s s s
λ λ−= − +  
             1 2 11 221 2 2 2
11 22 12
( )( )1
2 ( ) 4
s sc
s s s
λ λλ λ⎛ ⎞− −⎜ ⎟= + −⎜ ⎟− +⎝ ⎠
 
(28) 
 
(29) 
    ( )1
0 intenal extenal
E E E dsα β= ⋅ + ⋅∫ , (30) 
 19
(2.4) 計算特徵影像: 
針對所要分割之組織層(如 SC層)，我們會先取出一些具代表性的紋理影像，如圖(十
五) 中 11×11之橘色方框所示。並利用 PCA對其作分解，進而找出具代表性的特徵向量。 
 
 
 
圖(十五) 一些具代表性的紋理影像於 SC層 
 
令一組訓練之紋理影像序列為 1{ ,..., }MI I ，M為此組影像張數。每一張影像 Ii可以用一
長度為 N2之向量 iΓ 來表示，而平均影像可定義為 1
1
M
i
i
Mμ −
=
= Γ∑ 。然後，每張紋理影像減去
平均影像之灰階值可得 iΦ ( i i μΦ = Γ − )。    
接下來，計算共變異數矩陣 C ( 1 TC M BB−= ，其中， 1 2[ , ,..., ]MB = Φ Φ Φ )。然後對 C執
行主成分分析(PCA)，可以得到 21,..., Nu u 個特徵向量。為了達到有效率之計算，我們將原本
維度為 2 2N N× 之矩陣求解問題轉為M M× ，此將減少計算之時間，而所得之特徵向量為
1,... Mv v ，其中， iu 與 iv 之間的關係為 i iu Bv= 。 
 
(2.5) 紋理影像辨識 
當得到一張新的紋理影像 ′Γ，便藉由 ( )Tk kw u μ′= Γ − 將其轉到特徵空間，其中，M ′為前
M ′大的特徵向量個數。每一張輸入之紋理影像，將由一特徵向量 1{ ,..., }T Mw w ′Ω = 來表示。
接下來，此組代表紋理影像之特徵向量，將與事前訓練好之投影到特徵空間之多類紋理影
像計算距離。在此，輸入之紋理影像與那一類紋理影像最相似，所計算出的距離值將會最
小，即 min || ||l llε = Ω−Ω 。 lε 將作為相似性能量項參考之用。 
在分割過程中，採即時辨識方式，計算相似性能量以作為動態輪廓模形形變之依據。其
作法為在 snaxel上定義一個 11×11之搜尋方框，取此方框內影像為輸入之紋理影像。參考
紋理影像辨識之作法，將此紋理影像投影到特徵空間，可以得到一組相似性之數值， lε ，
將此值正規化後，可得 ( ( ))S v s ( ( ( )) /lS v s zε= )，以此值作為此 snaxel之相似性能量。 
 
 
 
 
 
 
結果與討論 
……… …
 21
(十六) (c-d)。由此可見，氾濫參數 l 定太高，將只找到部分細胞，而定太低，會有過度分
割之結果。最後，與改良過的分水嶺演算法 [36] 作比較，如圖(十六) (e)所示，其方法因為
沒有考慮細胞之資訊，所以不能分割出完整之細胞區域。 
 
 
 
 
 
 
 
 
 
                    (a)                               (b) 
                    
 
 
 
 
 
 
 
 
(c)                             (d) 
 
 
 
 
 
 
 
 
(e)                              (f) 
圖(十六)  比較不同方法之分割結果(a) 一顯微影像 (b) OTSU演算法 (c) 單階分水嶺分割
方法，氾濫參數 l =30 (d) 單階分水嶺分割方法，氾濫參數 l =2 (e) 改良過的分水嶺演算法 
(f)所提出之方法. 
 
 
(2) 模糊規則 I之正確率評估 
表二呈現在經過模糊規則 I後之 TP與 FP值。 
一般而言，細胞呈圓形或橢圓形的資訊加入到模糊規則 I，所以有少數細胞狀似狹長形將被
排除在外，造成 FN。此部分雖然會影響到之後權重之給定，但影響不大，因此在本研究中
 23
 
圖(十七) 對位分割之結果。以綠色所框選出的區域為有找到相對應之細胞，以紫色所框選
出的區域為執行對位分割法後所得到之補償效果。 
 
 
圖(十八) 3-D顯示細胞對位之結果。 
 
 
 
(二) 原子力顯微鏡影像之細胞重建與顯示 
圖(十九)為將影像對位後，所得到每個點其相對應之高度與紋理資訊，以 3D顯示表面
重構之結果。 
 
 25
 
(a)  
 
(b)  
 
(c)  
圖(二十一) 視網膜影像分割之結果: (a)前處理後的 OCT 影像 (b)傳統動態輪廓模型 (c)所
提出之方法 
 
 
另外，所提出的分割方法，應用於 10張手部皮膚 OCT影像作為評估之用。為了評估正
確性，分別請 2個專家在這十張影像上手動割出 SC與 ED層當成標準。評估方式如下： 
 
其中 ( )iSCe c 與 ( )iEDe c 分別為在第 i行之 SC與 ED層分割誤差。 
lg ( )a orithmiSC c 與 lg ( )a orithmiED c 分別為用提出之方法在第 i行之 SC與 ED層分割結果。 
( )istandardSC c 與 ( )istandardED c 分別為參考標準在第 i行之 SC與 ED層分割結果。 
 
由表四可看出十張手部皮膚 OCT影像之平均分割結果。 
 
 
  
 
 
             
lg
lg
( ) | ( ) ( ) |
( ) | ( ) ( ) |
a orithm
a orithm
i i i
SC standard
i i i
ED standard
e c SC c SC c
e c ED c ED c
= −
= − , (36) 
表四  十張皮膚 OCT影像之分割誤差 
PCA-based snake Traditional snake  
SC layer ED layer SC layer ED layer 
Average 
 Error 
0.63 1.66 1.15 2.94 
 27
[13] C. D. Ruberto, A. Dempster, et al., Analysis of infected blood cell images using 
morphological operators, Image and Vision Computing 20 (2) (2002) 133-146.  
[14] L. Yang, P. Meer, et al., Unsupervised segmentation based on robust estimation and color 
active contour models, IEEE Trans. ITB. 9 (3) (2005) 475-486. 
[15] S. Wang, and M. Wang, A new detection algorithm (NDA) based on fuzzy cellular neural 
networks for white blood cell detection. IEEE Trans. ITB. 10 (1) (2006) 5-10. 
[16] Vladimir Zagrodsky, Vivek Walimbe et al., Registration-Assisted Segmentation of 
Real-Time 3-D Echocardiographic Data Using Deformable Models, IEEE Trans. Med. Imag., 
21(9) (2005), 1089-1099. 
[17] O. Gerard, A. C. Billon, J.-M. Rouet, M. Jacob, M. Fradkin, and C. Allouche, Efficient 
model-based quantification of left ventricular function in 3-D echocardiography, IEEE Trans. 
Med. Imag., 21(9) (2002), 1059-1068. 
[18] L. A. Zadeh, Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets and Systems 1 
(1978) 3-28. 
[19] L.A. Zadeh, Toward a theory of fuzzy information granulation and its centrality in human 
reasoning and fuzzy logic, Fuzzy Sets and Systems 90 (2) (1997) 111-127. 
[20] H. Ishibuchi, T. Nakashima, Improving the performance of fuzzy classifier systems for 
pattern classification problems with continuous attributes, IEEE Trans. Industrial Electronics 
46 (6) (1999) 1057-1068. 
[21] H. Ishibuchi, T. Nakashima, Effect of rule weights in fuzzy rule-based classification systems, 
IEEE Trans. Fuzzy Systems 9 (4) (2001) 506-515. 
[22] M. Hamidi, A. Borji, Color image segmentation with CLPSO-based fuzzy, International 
Journal of Computer Science and Network Security 7 (6) (2007) 215-221. 
[23] J. Kennedy, R. C. Eberhart, Particle swarm optimization, Proceedings of the 1995 IEEE 
International Conference on Neural Networks (1995)1942-1948. 
[24] P. J. Besl and N. D. McKay, A method for   registration of 3-D shapes, IEEE Trans. 
Pattern Anal. Mach. Intell., 14(2) (1992) 239-256. 
[25] C. Harris, and M.J. Stephens, A combined corner and edge detector, Alvey Vision 
Conference, (1988)147-152. 
[26] A. W. Fitzgibbon, Robust Registration of 2D and 3D Point Sets,  Proceedings of the British 
Machine Vision Conference, (2001) 662- 670.  
[27] S. Mitragotri, Sonophoresis: A 50-year journey, DDT, 9 (2004) 735-736. 
[28]X. Xu and Q. Zhu, Feasibility of sonophoretic delivery for effective skin optical clearing, 
IEEE Biomed. Eng., 55(4) (2007) 1432-1437. 
[29]X. Xu and Q. Zhu, Sonophoretic delivery for contrast and depth improvement in skin optical 
coherence tomography, IEEE Journal of selected topics in Quantum electronics 14(1) (2008) 
56-61. 
[30] H. M. Salinas and D. C. Fernández, Comparison of PDE-Based Nonlinear Diffusion 
Approaches for Image Enhancement and De-noising in Optical Coherence Tomography, 
IEEE Transactions on Medical Imaging, 26(6) (2007) 761-771. 
[31] P. Perona and J. Malik, Scale-Space and Edge Detection Using Anisotropic Diffusion, IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 12(7) (1990) 629-639. 
 29
計畫成果自評 
 
符合原計劃之內容，所採用之方法應用於實際醫學影像中有不錯之成果，已經將其整
理並且投稿於國際期刊與會議論文，如下所示。  
Journal Paper: 
 
[1] Yi-Ying Wang, Yung-Nien Sun, “Segmentation of Nerve Cells via Multi-Level Gradient 
Watershed and Fuzzy Systems,” submit to Artificial Intelligence in Medicine, 2008.[請見附錄一] 
[2] Y. N. Sun, Y.Y. Wang, S.C. Chang, L.W. Wu and S.T. Tsai, “Color-Based Tumor Tissue 
Segmentation for the Automated Estimation of Oral Cancer Parameters,” submit to Microscopy 
Research and Technique, 2008. [請見附錄二] 
 
Conference Paper: 
[3] Yi-Ying Wang, Yung-Nien Sun, Chou-Ching K. Lin, and Ming-Shaung Ju, “Nerve Cell 
Segmentation via Multi-Scale Gradient Watershed Hierarchies,” 28th Annual International Conf. 
IEEE Engineering in Medicine and Biology Society, pp. 6698-6701, Sept., 2006. [請見附錄三] 
[4] Yi-Ying Wang, Shao-Chien Chang, Li-Wha Wu, Sen-Tien Tsai and Yung-Nien Sun, “A 
Color-Based Approach for Automated Segmentation in Tumor Tissue Classification,” 29th 
Annual International Conf. IEEE Engineering in Medicine and Biology Society, pp. 6576-6579, 
August, 2007. [請見附錄四] 
[5] Yi-Ying Wang, Yung-Nien Sun, Chou-Ching K. Lin, and Ming-Shaung Ju, “Automated 
Detection of Retina Layers in Optical Coherence,” International Symposium on Biomedical 
Engineering, Taiwan, 2008. [請見附錄五] 
[6] Yi-Ying Wang, Chou-Ching K. Lin, Yung-Nien Sun, “Automated Segmentation of Human 
Skin Layers in Optical Coherence Tomography,” Visual Information Processing, Southern Taiwan 
University, Tainan, Taiwan, pp. 17-20, 2008. [請見附錄六] 
[7] Yi-Ying Wang, Yung-Nien Sun, “Automated Layers Detection in Skin Optical Coherence 
Tomography,” International Forum on Medical Imaging in Asia 2009. [請見附錄七] 
 
 31
Contemporary cell segmentation tools are 
based generally on standard methods such as 
thresholding, the Hough transform, 
morphological operations, the active contour 
model (ACM), or the watershed transform 
approach. Regarding the threshold method, 
Weyn et al. [4] presented a system that can 
segment a large number of cells in a 
microscopic image, but the segmentation 
results depend significantly on the threshold 
value. In addition, this method may require 
human intervention in each segmentation step. 
With regard to watershed segmentation [5], 
the immersion level of the watershed is a key 
factor in determining segmentation results. 
For example, if the contrast between the cells 
and the background is significant, then the 
cells can be found at the coarsest level. On the 
contrary, if the contrast is weak or unclear, 
then the cells may be distinguishable only at 
the finest level. Thus, watershed transform 
using only one immersion level may fail to 
extract all the cells. In addition, hierarchical 
watershed segmentation was introduced in 
image processing by Vanhamel et al. [6]. 
Vanhamel proposed a nonlinear, scale-space 
diffusion method combined with gradient 
watersheds for color image segmentation. 
Watershed segmentation can provide useful 
initial contours if the contours closely 
approximate the true cell positions. But this 
cannot be achieved by using the diffusion 
method, which changes the blur scale 
sequentially due to the dislocation of the true 
boundary in the resultant image. 
The active contour model (ACM) is a 
deformable shape model that offers a powerful 
approach to image segmentation because it 
can incorporate continuity and curvature 
together with a priori knowledge, e.g., the 
shape of objects. Although powerful, 
traditional ACM has limitations. In general, 
traditional ACM tends to fail when the objects 
to be segmented are diffuse or have low 
contrast with respect to the background. 
Natural differences in cell morphology, 
response to staining, or changes in lighting 
conditions can cause contrast variations from 
one cell to another in a microscopic image or 
from slide to slide. Thus, cell segmentation 
using only one set of empirically-determined 
parameters to control the ACM energy 
weights cannot possibly handle all cell cases. 
On the other hand, the thousands of cells that 
are located close together in a micrograph can 
result in the inclusion of adjacent cells as a 
single cell in the ACM, resulting in erroneous 
segmentation. 
Some cell segmentation methods 
combine multiple stages, e.g., Mouroutis et al. 
[7] applied a segmentation method based on 
the compact Hough transform (CHT) to 
determine the possible locations of cell nuclei 
and then used a likelihood function to 
optimize the cell boundaries. Mouroutis’ 
method performs especially well when nuclei 
are in the process of division. Fok et al. [8] 
first obtained rough estimates of the inner 
boundaries of cells by the elliptical Hough 
transform and then used ACM to deform the 
inner cell boundaries and refine the outer cell 
boundaries. In a further step, false axon 
detection was applied to improve detection 
performance. Garrido et al. [9] presented an 
automatic cell segmentation approach based 
on deformable templates. Garrido first 
obtained cell locations using a reformulated 
Hough transform, then fit an elliptical 
approximation to the locations, and finally 
refined the results by Grenander’s deformable 
template model [10]. Garrido’s approach was 
robust and could handle images with severe 
 33
large number of such AAs surrounded by 
MSs. 
The MWFS algorithm combines 
multi-level watershed segmentation with three 
fuzzy systems. This section briefly reviews 
how algorithm will be used in later sections of 
this paper. It will be demonstrated that 
sequential changing of the immersion levels in 
the multi-level watershed segmentation 
scheme described below allows the scheme to 
find possible cells with high positional 
accuracy. The essential concept is stepwise 
variation of the immersion level from coarse 
to fine. If l is the number of the immersion 
level, then lower values of l result in more 
detected regions. This relationship is 
demonstrated in Fig.2. For example, there are 
only a few detected regions in Fig. 2(c) where 
l =30, but, in Fig. 2(h), the number of detected 
regions is very high where l = 5.   
The watershed process detects possible 
cells, but some of these are false alarms. Thus, 
it is desired to test these possible cells and 
reject false alarms. For this task, we embed a 
priori knowledge of general cellular 
characteristics into a set of fuzzy rules and use 
these rules to test each possible cell. Fuzzy 
rules are used because of their accuracy and 
ease of design during digital processing. 
Fuzzy logic was first introduced by Zadeh 
[17-18] and has found wide application in 
control problems. A fuzzy rule-based strategy 
can use a priori knowledge to generate rules 
with high ability to recognize objects or 
conditions. Many fuzzy rule-based 
applications are found in the areas of feature 
selection and pattern recognition. 
Considerable research has been devoted to 
making fuzzy systems more robust and 
efficient. For example, Ishibuchi et al. [19-20] 
studied the effects of rule weights in 
rule-based systems to improve classification 
performance. Hamidi et al. [21] studied the 
learning parameters of membership functions 
and reduced the number of rules.  
When properly applied, fuzzy rules can 
be trained to adapt themselves so that they can 
be characterized in many parts, which is 
beneficial in performing cell segmentation in 
complex images. The presented method uses 
three fuzzy systems which: 1) obtain 
classification decisions on cell identification; 
2) set ACM weights; and 3) eliminate false 
alarms. The parameters of the fuzzy systems 
are solved by PSO as described below.  
The rule base of the generated fuzzy 
classification system is: 
1 1
1 2
rule
IF is and...and is
THEN ( , ,... ) belongs to 
the class with , 1,2,...
th
r r
m m
m
r r
r  
 x   A  x   A
 x x x x  
C   CF  r R
=
=
      (1) 
where , 1, 2,...,riA i m = are the fuzzy sets of 
the rth generated fuzzy rule and m is the total 
number of features used to define x; 
[0,1]rCF ∈  is the grade of certainty of the rth 
fuzzy rule; {1,2,..., }rC M∈ , and M is the 
number of classes in the output. We assume 
the membership function of the fuzzy set riA  
is the most popularly-used Gaussian 
membership function, defined as: 
2
2
2
( )( | , ) exp , 1,2...,r
i
i ir
i ir irA
ir
xf x  i mμμ σ σ
⎛ ⎞− −= =⎜ ⎟⎝ ⎠
,  (2)           
where irμ  and 2irσ are the mean and 
variance of the membership function of the ith 
feature in the rth rule, respectively. 
 The output can be determined as y by 
maximizing the likelihood estimator of θ  as: 
1
max( ( | ))
R
r r rr
y L x CFθ
=
= ,              (3)           
 35
the extracted features of each pre-candidate 
cell to determine whether it meets the 
properties of a cell.  
Step 4: If the pre-candidate cell is approved by 
fuzzy system I, the designation is changed to 
candidate cell, and it is put into the candidate 
buffer.   
Step 5: Repeat Step 3 through Step 5 until all 
pre-candidate cells in the current level have 
been checked. 
Step 6: If the current immersion level is not 
the finest, the immersion level is decreased 
from l to l – 1; return to Step 1 or go to Step 7. 
If a pre-candidate cell has been added to the 
candidate buffer in a coarse level, then the 
detected cells in the subsequent finer levels at 
the same position will no longer be included 
in the candidate buffer. 
Step 7: Select a candidate cell from the buffer 
and apply rule-based ACM (fuzzy system II) 
to adjust the cell contour, after which the 
designation is changed to refined cell.  
Step 8: Fuzzy system III is used to check 
whether each refined cell is a false alarm; if it 
is not a false alarm, the designation is changed 
to confirmed cell, else the cell is eliminated. 
Step 9: Repeat Step 7 through Step 9 until 
every candidate cell in the buffer has been 
checked. 
 In the above-mentioned algorithm, Steps 
1 through 6 are in the cell detection phase, in 
which candidate cells are collected and stored 
in the candidate buffer. Step 7 and Step 8 are 
the refinement and confirmation phases, 
respectively. The contours of candidate cells 
are refined in Step 7 and the false-positive 
cells are rejected in Step 8. 
 
III. CELL DETECTION PHASE  
This section describes the details of 
initial processes involved in cell detection by 
the proposed algorithm. The subsequent 
sections will describe refinement and 
confirmation of cell segmentation. 
A. Multi-Level Watershed Segmentation 
In MWFS, the gradient magnitude image 
is computed and used as the input to the 
watershed transformation to obtain the initial 
watershed regions. The multi-level function is 
achieved by stepwise changing of the 
immersion level with each recursive 
implementation of the watershed 
transformation. We merge the region with its 
neighbors when the difference of the mean 
gray level is less than 20. All detected regions 
at this stage are designated as candidate cells, 
and, while each may possibly be a correctly 
detected cell, the uncertainty level is still high.  
B. Automatic features extraction  
In general, the results of watershed 
segmentation locate either the inner contour of 
a cell (Fig. 4(a)), the outer contour of a cell 
(Fig. 4(b)), an interstice between cells (Fig. 
4(c)), or a sub-region in the MS area (Fig. 
4(d)). Candidate cells such as those shown in 
Figs. 4(a) and 4(b) are valid detections, 
whereas candidate cells such as those shown 
in Figs. 4(c) and 4(d) are invalid. Fuzzy 
system I therefore checks each candidate cell 
to see if it meets a set of parameters that can 
reliably distinguish between the desired and 
undesired cell types of Fig. 4(a-d). 
The following introduces the three 
features that are used as inputs to fuzzy 
system I. 
(1) Region size 
 The size, sizeS , of a candidate cell is one 
of the features used to verify correct detection. 
For example, if a candidate cell is too large, it 
may include several cells, and, if a candidate 
cell is too small, it cannot be a cell. Thus, if 
 37
buffer are then refined by using additional 
fuzzy systems, as described in the following 
section.  
 
IV. CELL REFINEMENT PHASE 
A. Defining the Weights of ACM Energy Terms 
of Fuzzy System II. 
 To achieve correct segmentation, we use 
the relationship between the candidate cell and 
its local neighbors to adjust ACM energy 
weights. Two features, spatial distance and 
location, are used in fuzzy system II for 
setting the weight of energy terms. The first 
feature is based on the distance between the 
candidate cell and its neighbors. This feature 
is the distance from the snaxel of the 
candidate cell, along the boundary normal, to 
the border of the nearest neighboring cell. The 
second feature is defined on the basis of 
whether the snaxel is located on an inner or an 
outer boundary. This feature can be calculated 
by using Eq. (11). 
 
B. Fuzzy Rule-based ACM  
After candidate cells have been identified 
by fuzzy system I, the resulting fuzzy cell 
contours typically appear as shown in Fig. 6. 
At this point in our process, these detected 
contours are mostly inner boundaries, with the 
remainder being outer boundaries or false 
alarms. These candidate cell contours are used 
as initial estimates to find/refine the outer 
boundary by applying our fuzzy rule-based 
ACM.  
 Fuzzy system II deals with the 
assignment of the ACM weights 
( , , )internal external thicknessλ λ λ  based on the two 
local features, distance and location. Based on 
our experience, these three weights are given 
the values ( , , )internal external thicknessλ λ λ =  (0.4, 0.1, 
0.5) or (0.4, 0.5, 0.1) or (0.9, 0.1, 0.0) or (0.5, 
0.5, 0.0). Table I summarizes the four rules of 
fuzzy system II.  
With reference to Table I, rules 1 and 2 are 
aimed at cases where the initially-located 
boundary is an inner boundary. Rule 1 
indicates that if the distance between a snaxel 
and its neighbor is low (i.e., narrow 
intercellular space), then we reduce the weight 
of the external term (energy term 2) due to our 
concern that the ACM could expand to the 
boundaries of other cells. In addition, we 
increase the energy weight to the internal term 
(with its contour-smoothing effect) (
1λ = 0.4) 
and thickness (
3λ = 0.5) in order to maintain 
shape. Rule 2 deals with the situation where 
the intercellular space is high. In this case, we 
increase the weight of the external term when 
searching for the outer boundary. Rules 3 and 
4 are aimed at cases where the initially located 
contour is an outer boundary, i.e., the location 
value in Table I is low. In this situation, the 
weight of the third energy term, thickness, is 
defined as zero for both rules 3 and 4, since 
the boundary no longer evolves toward the 
outside by distance d. In rule 3, the 
intercellular distance is low, so the weight of 
the external term is reduced, and the internal 
term is increased. In rule 4, the intercellular 
distance is high, so the weight of the external 
term is increased, and the internal term is 
reduced. 
 
1) Defining the ACM Energy Terms 
By defining the object contour as a 
parametric curve using a B-spline, we obtain 
an energy model defined as  
1
1 internal 2 external 3 thickness0
1
( ( ) ( ) ( ) )
N
i
E v E v E v E dsλ λ λ
=
= + +∑∫ ,           
(13)                                            
where internalE  is the internal energy, externalE  
 39
by fuzzy ACM. If the outer boundary of a cell 
has been correctly located, then the pixels 
located in the inside of the boundary are dark 
MS area, while the pixels located outside of 
the boundary are light intercellular area. Then 
the characteristics of the refined contour can 
be used as input to Eqs. (10) and (11) for 
calculating the location value. If the calculated 
value is large and negative, then the average 
grey value going from inside to outside the 
boundary changes from dark to light, which is 
expected of a properly identified cell.  
2) Conflict  
A properly identified cell should have no 
overlap with any other cell. So we define a 
measure lconflict based on an overlap ratio as 
defined in Eq. (18). Candidate cells are put 
into the buffer sequentially, with those from 
the coarser levels being put in the buffer 
before those from the finer levels. Following 
this sequence on a one-by-one basis and 
beginning with the first-in candidate cell, each 
cell in the buffer is subjected to fuzzy ACM 
boundary refinement. After fuzzy ACM, the 
outer contour of the MS is formed and the MS 
area can be calculated. The refined cell is then 
checked for area overlap with its neighboring 
cells that have been confirmed. If the MS area 
of the refined cell has a large overlapping 
proportion (lconflict) with its neighboring 
confirmed cells, we regard the refined cell as a 
false cell. Thus, this process preserves the 
cells identified at the coarser levels. 
 Overlapping area in MS  ,
MS area   conflict
 l =       (18)                             
 
3) Circularity  
We define shaped  as a ratio of distances 
that are the minimum to the maximum 
distances from the center of a refined cell to 
its boundary. If the value of shaped  is close to 
1, the region is nearly circular in shape. In 
contrast, as the region narrows and elongates, 
then the value of shaped  approaches zero. 
Because a cell usually forms a circular (or 
elliptical) shape, shaped  values can be a useful 
measure.  
         
2 2
2 2
min{( ) ( ) | 1, 2,..., } ,
max{( ) ( ) | 1, 2,..., }
i c i c
shape
i c i c
x x y y i Nd
x x y y i N
− + − == − + − =
(19)           
where ( , )i ix y  are the coordinates of sample 
boundary pixels and ( , )c cx y are the center 
coordinates. N is the number of sample pixels 
located on the region boundary. Here N is 
equal to 18 with a rotational angle of 20°.  
 
VI. EXPERIMENTAL RESULTS AND 
DISCUSSION 
In the experiments, we used four test 
micrographic images of 768 × 768 pixel size. 
During watershed segmentation, the 
immersion level l was varied from 20 to 8. 
These l values were chosen based on our 
experience. Higher and lower values of l did 
not improve the number of detected cells. First, 
the features and parameters of the three fuzzy 
systems were needed, so experts marked the 
contours of typical cells manually in a set of 
similar micrographs. Based on these contours, 
we measured the specified features mentioned 
above in the discussion of each of the three 
fuzzy systems. The feature data were input to 
PSO to find optimal parameters for use in 
these fuzzy systems.  
 
A. Evaluation of Experimental Results 
Verification and evaluation of the system 
were achieved by comparing the segmentation 
 41
weaken the performance under uneven 
illumination (e.g., Fig. 7(e)). 
(2) Accuracy of fuzzy system I 
Table II shows the TP and FP of initial 
cell segmentation. Typically, nerve cells are 
assumed to be circular or strongly elliptical. 
The present methodology uses pre-defined, 
rule-based cell classification to identify and 
confirm cells that have shapes that are circular 
or strongly elliptical. However, nerve 
cross-sections show a small number of nerve 
cells with thin, narrow, or irregular shapes. It 
is easy for such cells not to be identified as 
cells and therefore excluded by the present 
methodology. Although these detection 
failures will affect the assignment of the ACM 
weight of other candidate cells, the error 
introduced is relatively small and is neglected 
in this paper.  
Moreover, the results show that there is a 
large FP value in the initial cell detection. This 
is because, in the consideration of the initial 
boundaries of cells, some interstices between 
cells have also been included by fuzzy system 
I. Although this produces a large initial FP 
value, these false alarm regions will be 
eliminated in fuzzy system III.  
(3) Accuracy of fuzzy system III 
Visual comparison of the FP values after 
executing fuzzy system I (Table II) with the 
FP values after executing fuzzy system III 
(Table III) shows that the number of false 
alarms (FPs) is reduced significantly. Since 
the purpose of fuzzy system I is to provide 
sufficient extraction of all possible candidate 
cells, the FNs are reduced while FPs are 
increased at the same time. The FP issue is 
subsequently resolved through the use of 
fuzzy system III, which is specifically 
designed to eliminate the falsely detected cells. 
Together, fuzzy systems I and III afford 
complementary functions to support an 
effective and simple detection mechanism for 
nerve cell segmentation. 
With regard to FN, there are no strategies 
in the presented method to reduce this FN 
value. The development of techniques for 
improving this aspect of our approach will be 
a part of our future research.  
(4) Accuracy of MWFS results 
The final cellular outlines obtained by 
MWFS were compared to the results obtained 
by manual segmentation of the same 
micrograph. Manual segmentation was 
performed independently by two experts. The 
mean variation of the two manually-derived 
outlines was a low 2%, indicating the 
averaged manual segmentation results were 
suitable for use as ground truth. MWFS and 
averaged manual segmentation were 
compared in terms of Area similarity Λ  and 
Hausdorff distanceΦ , summarized in Table 
III. While a value of 1 for Λ  is perfect, 
MWFS compared with manual segmentation 
gives an average Λ  of 0.92 ± 0.00, 
indicating good performance of the proposed 
system. Likewise, MWFS compared with 
manual segmentation give an average Φ  of 
1.34 ± 0.03 pixels. TP rates of the test images 
range from 0.93 to 0.95, with a mean TP rate 
of 0.94. When considering the global average 
performance of the proposed MWFS method, 
the average detection rate is above 90%, as 
shown in Table III. Fig. 8 shows a typical 
segmentation result. MWFS thus is shown to 
have good performance by comparison to the 
results of manual segmentation, although there 
is room for further improvement. 
 
VII. CONCLUSION 
One of the common tasks in neurological 
research is nerve cell segmentation and cell 
 43
gradient watershed hierarchies, EMBS'06 28th 
Annual International Conference of the IEEE 
(2006) 4310-4313. 
[12] P. Bamford, B. Lovell, Unsupervised cell 
nucleus segmentation with active contours, 
Signal Processing 71 (1998) 203-213. 
[13] C. D. Ruberto, A. Dempster, et al., Analysis 
of infected blood cell images using 
morphological operators, Image and Vision 
Computing 20 (2) (2002) 133-146.  
[14] L. Yang, P. Meer, et al., Unsupervised 
segmentation based on robust estimation and 
color active contour models, IEEE Trans. ITB. 
9 (3) (2005) 475-486. 
[15] S. Wang, and M. Wang, A new detection 
algorithm (NDA) based on fuzzy cellular 
neural networks for white blood cell detection. 
IEEE Trans. ITB. 10 (1) (2006) 5-10. 
[16] J. Kennedy, R. C. Eberhart, Particle swarm 
optimization, Proceedings of the 1995 IEEE 
International Conference on Neural Networks 
(1995)1942-1948. 
[17] L. A. Zadeh, Fuzzy sets as a basis for a theory 
of possibility, Fuzzy Sets and Systems 1 (1978) 
3-28. 
[18] L.A. Zadeh, Toward a theory of fuzzy 
information granulation and its centrality in 
human reasoning and fuzzy logic, Fuzzy Sets 
and Systems 90 (2) (1997) 111-127. 
[19] H. Ishibuchi, T. Nakashima, Improving the 
performance of fuzzy classifier systems for 
pattern classification problems with 
continuous attributes, IEEE Trans. Industrial 
Electronics 46 (6) (1999) 1057-1068. 
[20] H. Ishibuchi, T. Nakashima, Effect of rule 
weights in fuzzy rule-based classification 
systems, IEEE Trans. Fuzzy Systems 9 (4) 
(2001) 506-515. 
[21] M. Hamidi, A. Borji, Color image 
segmentation with CLPSO-based fuzzy, 
International Journal of Computer Science and 
Network Security 7 (6) (2007) 215-221. 
[22] D.P. Huttenlocher, et al., Comparing images 
using the hausdorff distance, IEEE Trans. 
PAMI. 15 (9) (1993) 850-863.  
[23] N. Otsu, A threshold selection method from 
gray-level histograms, IEEE Trans. SMC. 9 (1) 
(1979) 62-66.  
[24] V. Grau, A. U. J. Mewes, M. Alcañiz, 
Improved watershed transform for medical 
image segmentation Using Prior Information, 
IEEE Trans. Medical Imaging 23 (4) (2004) 
447-458. 
 
 
 
 
 
 
 
 
 
 
 
 
(a) 
 
 
 
(b) 
Figure1. Microscopic image: (a) typical 
nerve-fiber cross-section, 256 × 256 pixels; 
(b) enlarged single nerve cell (axon), with 
arrow A pointing to the axon body and 
arrow B pointing to the myelin sheath (MS) 
 
 
 
 
 
 
 
 
 45
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Flowchart illustrating the proposed cell segmentation process. 
 
 
 
 
 
 
 
 
 
 Finest level? 
Yes 
Fuzzy system III: 
False Alarm Removal 
Is a buffer empty? 
No 
Candidate cell buffer  
Rule-based ACM 
Fuzzy system I: 
Cell Detection 
Push the region into a 
buffer 
1l l= −
Has every  
region in current level 
been checked by Fuzzy 
system I? 
No 
Is the region a cell? 
Yes 
Yes 
Store all candidate cell 
regions 
Yes 
No 
Segmentated cells. 
Cell Detection Cell Refinement 
Watershed Region Merge 
Automated Feature 
Extraction by Watershed 
Segmentation with 
Immersion Level = l . 
No 
Fuzzy system II: 
Define weights of energy 
terms of ACM 
Select a candidate cell 
from the buffer 
 47
Traditional watershed segmentation with level 
= 30; (d) Traditional watershed segmentation 
with level = 2; (e) Grau’s improved watershed 
segmentation; (f) MWFS. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 8. Final MWFS cell segmentation 
result. Overlap with green contours on cell 
boundaries.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABLE I  
RULES IN FUZZY SYSTEM II 
 
 
  TABLE II 
  PERFORMANCE OF FUZZY SYSTEM I 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Image ID Detected
 cell nu
mbers 
FPs FNs 
Image 1 972 162 53 
Image 2 780 166 19 
Image 3 906 174 14 
Image 4 835 149 12 
Rule 
No.
Distance Location Output 
( , , )internal external thicknessλ λ λ
1 low high (0.4, 0.1, 0.5) 
2 high high (0.4, 0.5, 0.1) 
3 low low (0.9, 0.1, 0.0) 
4 high low (0.5, 0.5, 0.0) 
 49
附錄二 
Color-Based Tumor Tissue Segmentation for the Automated Estimation 
of Oral Cancer Parameters 
 
Abstract 
This paper presents an automatic color-based 
feature extraction system for parameter 
estimation of oral cancer from optical 
microscopic images. The system first reduces 
image-to-image variations by means of color 
normalization. We then construct a database 
which consists of typical cancer images. The 
color parameters extracted from this database 
are then used in automated online sampling 
from oral cancer images. Principal component 
analysis (PCA) is subsequently used to divide 
the color features into four tissue types. Each 
pixel in the cancer image is then classified 
into the corresponding tissue types based 
upon the Mahalanobis distance. The 
above-mentioned procedures are all fully 
automated; in particular, the automated 
sampling step greatly reduces the need for 
intensive labor in manual sampling and 
training. Experiments reveal high levels of 
consistency among the results achieved using 
the manual, semi-automatic and fully 
automatic methods. Parameter comparisons 
between the four cancer stages are conducted, 
and only the mean parameters between early 
and late cancer stages are statistically different. 
In summary, the proposed system provides a 
useful and convenient tool for automatic 
segmentation and evaluation for stained 
biopsy samples of oral cancer. This tool can 
also be modified and applied to other tissue 
images with similar staining conditions. 
 
Key words: Feature extraction, principal 
component analysis, tumor tissue classification 
 
1. Introduction 
Oral cancer is one leading cause of cancer 
death in the world. Betel quid chewing, alcohol 
drinking and smoking are three well recognized 
risk factors for oral cancer by World Health 
Organization. Due to the prevalent use of betel 
quid among young males, the incidence rate of 
this cancer is increasing at an alarming rate in 
Taiwan (Tsai et al., 2004). Moreover, the 
five-year survival rate for oral cancer still lags 
behind most of other cancer types despite of 
therapeutic regimen advancement. Therefore, 
early detection of oral cancer holds the key to 
find against this disease. To have a better 
management for such cancer patients, we have a 
considerable interest in developing a better 
staging system for oral cancer. 
The stage of a cancer is a clinical descriptor,  
 
 
 
 
 
typically using the numbers I to IV, and taking 
into account such factors as tumor size, how 
deeply the tumor has penetrated, the invasion of 
adjacent organs, and the state of metastasis. 
Cancer staging is important, because the stage at 
diagnosis is the most powerful predictor of 
survival, and treatments often are based on stage. 
Tissue evaluation based on microscopic images is 
an important part of the diagnosis, staging and 
prognostic tracking of cancer. One aspect of 
optical molecular expression during cancer 
progression. Assis et al. (2005) studied the 
expression of bcl-2 in rat tongue mucosa exposed 
to cigarette smoke. Jung et al. (2005) studied the 
early diagnosis of oral cancer based upon 2-D and 
3-D OCT (optical coherence tomography) images. 
To identify areas of oral cancer, Jiang et al. (2004) 
 51
Eigenvectors (Kalaba, 1981) are determined that 
represent the directions of maximum variance for 
the desired four classes. Then, given a test sample 
and projecting it to the trained Eigenspace, we 
can compute the Mahalanobis distance (Duda et 
al., 2001) between the sample and each class. 
Mahalanobis distance is a well-known measure to 
select the best class that the sample belongs to 
from the C given classes. As the Mahalanobis 
distance is normalized by the covariance matrix 
in each class, the classification results are thus 
less influenced by noise. Once the Mahalanobis 
distance is calculated, the sample is simply 
classified to the class with the minimal distance.  
Finally, CBFE can analyze these classified 
tumor tissues and obtain statistics regard different 
parameters which are important to cancer staging, 
like vascular number, vascular area, and nuclear 
number. In this way, the CBFE can provide these 
significant statistics to doctors rapidly, 
automatically and accurately to promote 
improved diagnosis in the clinical setting. 
 
2. Materials and System Overview 
2.1 Microscopic Image Samples of Oral 
Tumors 
The oral cancer microscopic images used in 
this study were provided by the Institute of 
Molecular Medicine, College of Medicine, 
National Cheng Kung University. Tumor vascular 
and other related tissues were stained using 
anti-CD34 antibodies. CD34 is a 110-kDa 
transmembrane glycoprotein expressed on 
leukemia cells, stem cells and some fibroblasts. 
The anti-CD34 is considered the best immuno- 
histochemical endothelial marker for tri-color 
staining (Cui et al., 1996). Other detailed 
references can refer to Marc et al. (2006) and 
Tashbaeva et al. (2007). 
Our experimental images are composed of 
52, 55, 53 and 56 images of stages I, II, III and IV, 
respectively. Each stage contains microscopic 
images taken from 6 patients, with 7 to 10 images 
from the pathological sections of each patient. 
Since we wanted to obtain statistics for vessels in 
whole images and nuclei located only in tumors, 
we asked experts to determine the tumor regions 
manually. Examples of these expert-determined 
tumors are outlined by yellow in each image of 
Fig. 1. 
 
2.2 Hardware and Software 
Acquired images were of 2048×1536 pixel 
resolution and stored in Windows Bitmap (BMP) 
format. All image processing algorithms were 
developed in C++ programming language. CBFE 
experiments were implemented on a conventional 
PC with a Pentium P4 3.4 GHz processor and 1 
GB memory. In order to obtain fast throughput, 
microscopic images were down-sampled to 
432×324 pixels for near real-time color image 
analysis. Such reduced images are convenient 
because of the faster processing time, and are 
adequate for general clinical use. However, if 
more details are desired, the user can map the low 
resolution image onto the corresponding high 
resolution image. 
 
2.3 Significant Characteristics of Histological 
Sections 
Two typical color micrographs of oral cancer 
stained with anti-CD34 antibody are shown in 
Figs. 1 (a) and (b). Arrow 1 points to a blood 
vessel, arrow 2 to a nucleus, arrow 3 to an area of 
dark background, and arrow 4 to an area of light 
background. There are some useful and regular 
characteristics in these images. For example, 
anti-CD34 antibodies have stained the blood 
vessels red, the nuclei blue or purple, the dark 
background dark gray, and the light background 
light gray. The blood vessels usually are stained 
unevenly and their shapes are irregular. The 
nuclei are more uniformly stained and their 
shapes are close to circular. In addition, the 
anti-CD34 stained samples showed two different 
staining characteristics in the micrograph 
background. Thus we gave background two 
distinct classifications, dark background and light 
background. 
 
2.4 Technological Processes of The Proposed 
Scheme 
We utilize these color-based properties for 
segmentation, classification and quantitative 
analysis of nuclei and blood vessels in anti-CD34 
stained microscopic images of oral tumors. It is 
apparent that the algorithm is composed of two 
separate phases, a learning phase and a 
classification phase. 
The learning phase begins by manually 
selecting a small number of typical micrographs 
from a larger image database, and treating them 
as standard images for use in color normalization 
pre-processing. The selected standard images are 
 53
Because the input microscopic images all are 
normalized to a common color distribution, we 
first collect a large number of typical oral cancer 
images and use them as the reference database. 
From the database, the distribution parameters of 
four tissue types can be obtained by one-time 
manual selection, and then can be used in all 
subsequent automatic training processes.  
In this algorithm, we determine the 
clustering of a given pixel, according to the 
Euclidean distance in the RGB space between the 
pixel and the cluster center. In a given image, we 
select the candidate pixels based upon the 
distribution parameters obtained from the a priori 
database. We then inspect each selected candidate 
pixel and its 3×3 neighborhood. If the pixel and 
its eight neighbors belong to the same cluster 
center, then the pixel and its neighboring pixels 
all are selected as the candidate samples, 
otherwise ignore the candidate pixel. As shown in 
Fig. 3, we take a pre-defined number of training 
samples with 3×3 pixel blocks from the given 
sample images in each class. The blue blocks 
represent vessel samples, the yellow blocks 
nucleus samples, the pink blocks dark 
background samples, and the green blocks light 
background samples.  
To alleviate the problems of outliers and 
cluster overlap, we first calculate the mean of 
each cluster in the RGB color space, and preserve 
the sample points representing the 95% nearest to 
the mean of each cluster. As a final step, the mean 
of each cluster is re-calculated and saved for later 
classification.  
 
3.3 Principal Component Analysis and 
Classification 
PCA is a linear transformation that 
transforms the data to a new coordinate system 
such that the data with the largest variance lying 
on the first principal component, the second 
largest on the second principal component etc… 
These principal components are the Eigenvectors 
that are ordered according to their corresponding 
Eigenvalues and form the Eigenspace. Here, we 
use the singular value decomposition (Deprettere, 
1988) to obtain these Eigenvectors for higher 
accuracy and speed. Based on the given training 
samples, the Eigenspace provides the largest 
discriminatory space where we can verify which 
class the given sample belongs to. When a new 
sample is projected onto the Eigenspace, the 
Mahalanobis distances are calculated to represent 
the distances between the sample and the four 
given classes. Then the sample is classified to the 
class that has the minimum Mahalanobis distance. 
 
3.4 Post-processing 
The proposed classification method 
categorizes each pixel independently. It does not 
consider neighborhood information, so there are 
many small fragmentary regions in our 
classification results. These unnecessary regions 
are eliminated using a rank filter, which puts a 
5×5 mask on each pixel in the classification result, 
and classifies the pixel to the class with the 
largest number in its neighborhood.  
 
4. Experimental Results and Discussion 
4.1 Evaluation of Classification Precision and 
System Stability 
We define true positive (TP ) rate and false 
positive ( FP ) rate to measure the validity of the 
proposed method to automatically classify oral 
tumor tissues. TP rate is defined as the total 
number of tumor tissue detected manually by 
experts (ground truth) divided by the number of 
tumor tissue classified by the algorithm to the 
tumor class; FP rate is defined as the total 
number of algorithm-detected tumor tissue that do 
not exist in ground truth over the number of 
tumor tissue that exist in ground truth but are 
miss-classified by the algorithm to other classes.  
In the semi-auto mode, the training samples 
are manually selected for each new batch of slides 
during the learning phase. To evaluate the 
accuracy of the semi-auto mode, we compare the 
results of semi-auto mode with those of manual 
segmentation. Here, we asked three experts to 
categorize tissues in two selected images. Notably, 
the standard deviations are small for each of the 
tissue classes determined by experts, indicating 
the reliability of the manual selection. Comparing 
the accuracy of semi- automatic method to 
manual segmentation, the mean TP rates in the 
vessel and dark background classes are above 
95%, and those in the other two classes are above 
92%. Therefore, we confirm that the results of 
semi-automatic method are highly correlated with 
the manual results by experts.  
Because the semi-automatic results are 
almost identical with theirs, it is reasonable to 
conclude that the semi-auto method can replace 
full-manual segmentation with accuracy 
 55
In the experiment, the three parameters AVA, 
ANA and NAR were found to not satisfy the 
normal distribution assumption. Therefore, we 
transformed AVA and ANA to logarithmic space 
and calculated the square root of NAR, thereby 
changing their distributions to normal 
distributions. The other parameters retained their 
original values. The late-to-early stage difference 
for each parameter was calculated as the mean of 
the Late Stage minus the mean of the Early Stage.  
Observing the p-values and t  statistics, it is 
apparent that significant differences existed both 
in VN and log (AVA) between the early stage and 
late stage. The mean value for the parameter VN 
in the late stage was higher than that of the early 
stage (t = 2.21; p-value = 0.04). With regard to 
log (AVA), the mean value from the late stage was 
lower than from the early stage (t = -2.64; p-value 
= 0.02). This is because AVA is equal to VA/VN; 
consequently, if VN increases with stage, then 
AVA must decrease. As is common clinical 
understanding, the average vascular area in the 
later stages of cancer is larger than in earlier 
stages. The observed statistics agree with these 
published results (Sasahira T, et al., 2007; Kanae 
N, et al., 2001), thereby providing strong 
validation of the statistics. The observable 
characteristics of cell nuclei have been considered 
possible indicators of cancer staging. In 2003, 
Loukas et al. presented a method purely for 
automatic counting of cancer nuclei, but did not 
compare nuclei statistics between the early and 
later stages of cancer. However, the experimental 
results demonstrated no significant inter-stage 
differences in the statistical parameters related to 
nuclei. As we had expected to identify some 
parametric differences pertaining to tumor nuclei 
within early stage cancer, we will collect 
additional oral cancer specimens and subdivide 
the early stage more finely. These new parameters 
will be investigated carefully and compared to the 
other vessel-related parameters in the near future.  
 
4.3 Comparing The Proposed Scheme with 
k-means Clustering 
The classification results of the proposed 
method and k-means clustering (MacQueen, 1967) 
are compared in this section. The k-mean 
clustering method is a distance-based 
classification tool that assigns each point to the 
closest class. As we also use a distance-based 
method (Mahalanobis distance) to classify pixels, 
both k-mean and the proposed method are 
well-suited for comparison. The cluster number k 
and the cluster centers must be set before 
applying k-means clustering. We set k = 4 to 
match the 4 tissue types; i.e., vessel, nucleus, dark 
background and light background. The k cluster 
centers were initiated randomly.  
Fig. 5 (a) shows an oral cancer image, Fig. 5 
(b) depicts the results of k-means clustering, Fig. 
5 (c) is the result of the proposed method, and the 
ground truth is in Fig. 5 (d). Comparing the 
results of the proposed method to the k-means 
clustering, the k-means clustering misclassifies 
many areas of nuclei to vessels (in red). With 
regard to statistical performance, the mean TP 
rates averaging from the four stages are 49.11%, 
64.17%, 58.55% and 79.6% for the four classes; 
the mean FP rates are 4.54%, 13.43%, 13.37% 
and 19.22% for the four classes. The proposed 
method is more accurate than the k-means 
clustering in both TP and FP rates.  
 
5. Conclusions 
This paper presents an automatic 
classification and segmentation method for the 
microscopic imaging of oral cancer. To deal with 
problems of different lighting conditions and 
patient-to-patient variations in staining, the 
proposed method applies an initial color 
normalization step to all new micrographic data. 
After color-normalization, we apply the automatic 
selection of training samples to make the 
segmentation process more efficient; this is 
especially relevant in clinical settings wherein the 
handling of large numbers of images may be 
necessary. After extracting four types of features, 
PCA learning and color classification are 
conducted.  
For system evaluation, we compared the 
accuracy of fully-manual, semi-automatic and 
fully-automatic methods, identifying a high level 
of consistency. The first conclusion we drew from 
this is that the semi-auto method was accurate 
enough to replace the full-manual method, as a 
means to establish ground truth while evaluating 
the fully-automatic method. This is important, 
because the expert-based manual method is 
laborious and time intensive.  
The second main conclusion we drew is that 
the fully-automatic method also was highly 
accurate. We distinguished between four 
commonly observed tissue types in oral cancer 
 57
Liotta LA, Kleinerman J et al. 1974. Quantitative 
relationships of intravascular tumor-cells, 
tumor vessels, and pulmonary metastases 
following tumor implantation. Cancer 
Res., 34(5), 997-1004. 
Loukas CG, Wilson GD et al. 2003. An image 
analysis-based approach for automated 
counting of cancer cell nuclei in tissue 
sections. Cytometry Part A, 55(1), 30-42. 
Loukas CG, Linney A. 2005. On a 
relaxation-labeling algorithm for quantitative 
assessment of tumour vasculature in tissue 
section images. Computers in Biology and 
Medicine, 35(2), 157-171.  
MacQueen JB. 1967. Some Methods for 
classification and Analysis of Multivariate 
Observations. Proceedings of 5-th Berkeley 
Symposium on Mathematical Statistics and 
Probability, 1, 281-297.  
Marc PP et al. 2006. Immunohistochemical 
Expression of Endothelial Markers CD31, 
CD34, von Willebrand Factor, and Fli-1 in 
Normal Human Tissues. Journal of 
Histochemistry & Cytochemistry, 54(4), 
385-395. 
Reinhard E, Ashikhmin M et al. 2001, Color 
transfer between images. IEEE Computer 
Graphics and Applications, 21(5), 34-41. 
Rodríguez R, Alarcon TE, Pacheco O. 2005. A 
new strategy to obtain robust markers for 
blood vessels segmentation by using the 
watersheds method. Computers in Biology 
and Medicine, 35(8), 665-686.  
Rodríguez R. 2006. A strategy for blood vessels 
segmentation based on the threshold which 
combines statistical and scale space filter 
application to the study of angiogenesis. 
Computer Methods and Programs in 
Biomedicine, 82(1), 1-9.  
Rajan A, Kusum J. et al. 2002. Angiogenesis as 
an independent prognostic indicator in 
node-negative breast cancer. Anal Quant 
Cytol Histol, 24(4), 228-233. 
Ruderman DL, Cronin TW, and Chiao CC. 1998. 
Statistics of cone responses to natural images: 
implications for visual coding. J. Optical Soc. 
of America, 15(8), 2036-2045. 
Sharma S. 1996. Applied Multivariate 
Techniques, Wiley, 375-378. 
Su HH, Chu ST et al. 2006. Spindle Cell 
Carcinoma of the Oral Cavity and 
Oropharynx: Factors Affecting Outcome. J 
Chin Med Assoc, 69(10), 478-483. 
Sasahira T, Kirita T et al. 2007. The expression of 
receptor for advanced glycation end products 
is associated with angiogenesis in human 
oral squamous cell carcinoma. Virchows 
Arch, (450), 287-295. 
Tsai WC, Tsai ST et al. 2004. The mRNA profile 
of genes in betel quid chewing oral cancer 
patients. Oral Oncol. (40), 418-26. 
Tashbaeva RE et al. 2007. Cellular 
Characterization of Multidrug Resistance 
P-glycoprotein, Alpha Fetoprotein, and 
Neovascular Endothelium-Associated 
Antigens in Canine Hepatocellular 
Carcinoma and Cirrhotic Liver. Veterinary 
Pathology, (44), 600-606. 
Wang YY, Chang SC, Wu LW, Tsai ST, Sun YN. 
2007. A Color-Based Approach for 
Automated Segmentation in Tumor Tissue 
Classification. IEEE Engineering in 
Medicine and Biology Society, (1), 
6576-6579. 
 
 
(a) Batch of patient A                   
 
 
 (b) Batch of patient B 
 
Figure 1. Microscopy images of oral cancer 
stained with anti-CD34. Arrow 1 points to a 
vessel. Arrow 2 points to a nucleus. Arrow 3 
 59
 
(c) 
 
 
 
 
 
 
 
 
 
(d) 
 
Figure 4. Comparing the fully-automatic, 
semi-automatic and manual methods: (a) original 
image; (b) result of the proposed method; (c) 
result of the semi-automatic method; (d) result of 
manual segmentation by experts. 
 
 
 
 
 
 
 
 
 
(a) 
 
 
(a) 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(b) 
 
 
 
 
 
 
 
 
 
 
 
 
(c) 
 
 
 
 
 
 
 
 
 
 
 
 
(d) 
Figure 5. Comparing the classification results 
produced by our method and k-means clustering 
(a) original image; (b) result of k-means 
clustering; (c) result of our method; (d) ground 
truth. 
 
 
 
 
 
 
 
 
 61
附錄三 
 
 63
 
 
 
 
 
 
 
 65
附錄四 
 
 
 
 67
 
 
 
 
 69
附錄五 
 
應用於光學同調性斷層掃描影像之自動化視網膜偵測 
AUTOMATED DETECTION OF RETINA LAYERS IN OPTICAL 
COHERENCE TOMOGRAPHY (OCT) IMAGES 
王怡穎(Y.Y. Wang)1, 孫永年(Y.N. Sun)1*, 林宙晴(C.C. K. Lin)2 and朱銘祥(M.S. Ju)3 
1成功大學資訊工程學系(Visual System Laboratory, Department of CSIE, NCKU, Tainan, Taiwan, R.O.C.) 
2成功大學醫學系神經科 (Department of Neurology, NCKU Hospital, Tainan, Taiwan, R.O.C.) 
3成功大學機械工程學系(Department of Mechanical Engineering, NCKU, Tainan, Taiwan, R.O.C.) 
 
Abstract 
Retinal thickness is an important measure used in 
glaucoma diagnosis. Therefore, detection of 
various retinal layers is an important task in 
computer aided diagnosis. Optical coherence 
tomography (OCT) is a new imaging modality that 
is similar to ultrasound but with much higher 
resolution. OCT is usually used in thickness 
measurement of the retina. This study presents a 
new method for detecting retinal layers in OCT 
images automatically. The proposed method firstly 
handles the speckle problem by 
coherence-enhanced diffusion filtering. Retinal 
layers are then segmented by using an active 
contour model driven by a PCA-based (principal 
component analysis) similarity force. The 
proposed algorithm has been verified on real OCT 
images and achieved good experimental results. 
 
Keywords: OCT, PCA, active contour model 
 
 
1. INTRODUCTION 
Optical coherence tomography (OCT) can 
provide in vivo images of retina with near- 
microscopic resolution. OCT is a useful tool for 
the assessment of ocular diseases. The goal of the 
present study is the automatic detection of retina 
layers in OCT images. Fig. 1 shows a typical OCT 
image. The three yellow arrows point to three 
boundaries that are to be segmented in the image. 
In general, OCT images suffer from speckle noise, 
which poses a major limitation on OCT image 
quality. Speckle noise usually is a random 
deterministic interference pattern which occurs in 
images formed by coherent radiation interacting 
with a medium containing many sub-resolution 
scatters. To help resolving the speckle noise 
problem, we first apply image pre-processing to 
reduce noise. The presented method starts with a 
complex diffusion process to reduce speckle 
noises while preserving retinal edges [2-3]. 
Coherence- enhanced diffusion is then applied to 
improve the discontinuities in the de-noised image 
and to obtain structural coherence information [4]. 
After image pre-processing, we apply an active 
contour modeling (ACM) to segment retina layers.  
 
 
2. METHODS 
 
2.1.1 Nonlinear complex diffusion approach 
A nonlinear complex diffusion (NCD) 
approach is adopted from [1] and is summarized 
below. The NCD equation is 
( (Im( )) )I d I I
t
∂ = ∇⋅ ∇∂                  (1)             
 71
Based on Eq. (6), we again search the best 
positions according to the normal direction of each 
snaxel. Finally, by splining all the optimized 
snaxels we can obtain the ChCap layer.  
 
 
3. MODIFIED DEFORMABLE MODEL  
 
The snake model in Eq. (6) can mostly 
define a boundary surface successfully, but it fails 
in some places where the optimized snaxels are 
entrapped by strong edges. To resolve this problem, 
we add some similarity force into the snake 
algorithm. We observe that various layers in OCT 
image can have only several types of texture. The 
orange colored windows in Fig. 1 illustrate some 
typical textures of the ONL layer. The similarity 
force in the energy function E is used to look for 
the desired textures in the OCT image. The energy 
function of the snake can be modified as  
1 2
0
| ( ) | ( ( )) ( ( ))E v s G v s S v s dsα β γ′= ⋅ + ⋅ + ⋅∫  (7)     
where | ( ) |v s′ , ( ( ))G v s and ( ( ))S v s are the 
continuity, image and similarity forces, 
respectively; α , β and γ  are the weighting 
parameters. We set the values ofα , β  and γ to 
be 0.4, 0.2 and 0.4, respectively. In the following, 
we will introduce the computation of the similarity 
force.  
 
3.1 Computation of Eigen-images 
First we select typical textures and then use 
PCA (principal component analysis) to decompose 
them and analyze their characters. To the best of 
our knowledge, this is the first application of 
similarity force for texture in boundary 
segmentation.  
Let a training set of texture image be 
1{ ,..., }MI I , where M is the number of texture 
images. Each image Ii is represented with a vector 
iΓ of length N2 describing an N by N image. The 
average texture image of the training set is defined 
by 1
1
M
i
i
Mμ −
=
= Γ∑ . Then each texture image 
subtracts this average from the vector 
by i i μΦ = Γ − . Then the covariance matrix is 
defined by 
1 ,TC M BB−= where 1 2[ , ,..., ]MB = Φ Φ Φ . Then 
the PCA is used to decompose covariance matrix 
into N2 orthogonal eigenvectors 21,... Nu u so as to 
best describe the distribution of these images. To 
achieve more efficient computation, we reduce the 
originally 2 2N N×  matrix problem to a smaller 
problem with M M× matrix by solving the 
matrix TB B  whose Eigenvectors are 1,... Mv v . The 
relationship between iu  and iv  is i iu Bv= .  
 
3.2 Texture recognition 
In the on-line recognition process, an input 
image ′Γ  is first projected to the Eigen-space by 
( )Tk kw u μ′= Γ −  for 1,...,k M ′= . M ′ is the 
index corresponding to the M ′  largest 
Eigen-values. The components form a vector 
1{ ,..., }
T
Mw w ′Ω =  which describes the 
contribution of each Eigen-image in representing 
the input texture image. Thus Ω  provides the 
best description to the input texture. On the other 
hand, the vector lΩ is the pre-defined texture 
vector that best describes the lth texture class, 
obtained from the texture database. The method 
minimizes the Euclidian distance min || ||l llε = Ω−Ω  
to select the best l as the texture class. 
 
3.3 Calculation of similarity force 
In the deform process, a snaxel in the search 
line can define an 11×11 window and take it as an 
input texture image. With reference to section 3.2, 
we project the texture image into the Eigen-space. 
Through the Euclidian distance, we can find the 
 73
S
E
  
附錄六 
 
AUTOMATED SEGMENTATION OF HUMAN SKIN LAYERS IN OPTICAL 
COHERENCE TOMOGRAPHY  
YI-YING WANG1, CHOU-CGING K. LIN2 and YUNG-NIEN SUN1 
 
1Visual System Laboratory, Department of CSIE, NCKU, Tainan, Taiwan 
2 Department of Neurology, NCKU Hospital, Tainan, Taiwan 
E-mail: ynsun@mail.ncku.edu
 
 
Abstract: 
In-vivo skin layer measurement is 
important in the examination of the intra and 
inter-layer variations of skin tissues before and 
after certain stimulation or treatment. Therefore, 
detection of skin layers is an important task in 
computer-aided diagnosis. Optical coherence 
tomography (OCT) is a new imaging modality 
that is similar to ultrasound but with much 
higher resolution. This study presents a new 
method for detecting skin layers in OCT 
images automatically. The proposed method 
firstly handles the speckle problem by 
nonlinear complex diffusion and 
coherence-enhanced filtering. Skin layers are 
then segmented by using an active contour 
model driven by a PCA-based (principal 
component analysis) similarity force. The 
proposed algorithm has been verified on real 
OCT images and achieved good experimental 
results. 
 
Keywords:  
OCT; PCA; active contour model 
1. Introduction 
Optical coherence tomography (OCT) can 
provide in vivo images of skin with near- 
microscopic resolution. The goal of the present 
study is the automatic detection of skin layers 
in OCT images. Fig. 1 shows a typical OCT 
image. 
 
 
 
 
 
 
 
 
 
 
 
 
Fig.1 OCT image of skin. The arrows 
indicate two boundaries to be segmented, 
namely the stratum corneum layer (SC), and 
the layer between epidermis and dermis (ED). 
 
The two yellow arrows in the image point 
to the boundaries to be segmented; one is the 
stratum corneum (SC), which is the outermost 
layer of skin; the other is the layer between 
epidermis and dermis (ED). Some reports [1-3] 
have examined OCT image to find if treatment 
by sonophoretic delivery improved the distance 
between the SC and ED. For such applications, 
curves which are well fitted to the SC and ED 
layers in OCT image are necessary. In general, 
OCT images suffer from speckle noise, which 
poses a major limitation on OCT image quality. 
Speckle noise usually is a random deterministic 
interference pattern which occurs in images 
formed by coherent radiation interacting with a 
medium containing many sub-resolution 
scatters. To help resolving the speckle noise 
problem, we first apply image pre-processing to 
reduce noise. The presented method starts with 
a complex diffusion process to reduce speckle 
noises while preserving edges [4-5]. 
Coherence- enhanced diffusion is then applied 
to improve the discontinuities in the de-noised 
image and to obtain structural coherence 
information [6]. After image pre-processing, we 
apply an active contour modeling (ACM) to 
segment layers. 
2. Methods 
2.1. Nonlinear complex diffusion approach 
A nonlinear complex diffusion (NCD) 
approach is adopted from [7] and is 
summarized below. The NCD equation is 
  75
define a boundary surface successfully, but it 
fails in some places where the optimized 
snaxels are entrapped by strong edges. To 
resolve this problem, we add some similarity 
force into the snake algorithm. We observe that 
various layers in OCT image can have only 
several types of texture. The orange colored 
windows in Fig. 1 illustrate some typical 
textures of the SC layer. The similarity force in 
the energy function E is used to look for the 
desired textures in the OCT image. The energy 
function of the snake can be modified as  
1 2
0
| ( ) | ( ( )) ( ( ))E v s G v s S v s dsα β γ′= ⋅ + ⋅ + ⋅∫       
(7)     
where | ( ) |v s′ , ( ( ))G v s and ( ( ))S v s are the 
continuity, image and similarity forces, 
respectively; α , β and γ  are the weighting 
parameters. We set the values ofα , β and γ to 
be 0.4, 0.2 and 0.4, respectively. In the 
following, we will introduce the computation of 
the similarity force. 
2.5. Computation of Eigen-images 
First we select typical textures and then use 
PCA (principal component analysis) to 
decompose them and analyze their characters. 
To the best of our knowledge, this is the first 
application of similarity force for texture in 
boundary segmentation.  
Let a training set of texture image 
be 1{ ,..., }MI I , where M is the number of texture 
images. Each image Ii is represented with a 
vector iΓ of length N2 describing an N by N 
image. The average texture image of the 
training set is defined by 1
1
M
i
i
Mμ −
=
= Γ∑ . Then 
each texture image subtracts this average from 
the vector by i i μΦ = Γ − . Then the covariance 
matrix is defined by 1 ,TC M BB−=  
where 1 2[ , ,..., ]MB = Φ Φ Φ . Then the PCA is 
used to decompose covariance matrix into N2 
orthogonal eigenvectors 21,... Nu u so as to best 
describe the distribution of these images. To 
achieve more efficient computation, we reduce 
the originally 2 2N N×  matrix problem to a 
smaller problem with M M× matrix by solving 
the matrix TB B  whose Eigenvectors are 1,... Mv v . 
The relationship between iu  and iv  
is i iu Bv= . 
2.6. Texture recognition 
In the on-line recognition process, an input 
image ′Γ  is first projected to the Eigen-space 
by ( )Tk kw u μ′= Γ −  for 1,...,k M ′= . M ′ is the 
index corresponding to the M ′  largest 
Eigen-values. The components form a vector 
1{ ,..., }
T
Mw w ′Ω =  which describes the 
contribution of each Eigen-image in 
representing the input texture image. Thus Ω  
provides the best description to the input 
texture. On the other hand, the vector lΩ is the 
pre-defined texture vector that best describes 
the lth texture class, obtained from the texture 
database. The method minimizes the Euclidian 
distance min || ||l llε = Ω−Ω  to select the best l as 
the texture class. 
 
2.7. Calculation of similarity force 
In the deform process, a snaxel in the 
search line can define an 11×11 window and 
take it as an input texture image. With reference 
to section 2.6, we project the texture image into 
the Eigen-space. Through the Euclidian 
distance, we can find the most similar texture 
class to the input texture. The similarity force 
of the snaxel is calculated by ( ( )) /lS v s zε= , 
where z is a constant value. After searching the 
optimal position of each snaxel based on Eq. 
(7), we then use the B-spline function to 
connect all the selected snaxels. 
 
3. Experimental results  
Fig. 3 (a) shows a speckle reduced skin 
OCT image, while (b) and (c) show the results 
of the traditional and proposed methods, 
respectively. Visual inspection shows that ED 
layer obtained by the traditional active contour 
model is easily trapped by the strong edge 
indicated by the yellow arrows in Fig. 3 (b). At 
the same locations pointed out by arrows in Fig. 
3 (c), it is clear that the proposed similarity 
force gives better accuracy than the traditional 
method.  
 
  77
[3] X. Xu and Q. Zhu, “Sonophoretic delivery for 
contrast and depth improvement in skin optical 
coherence tomography,” IEEE Journal of selected 
topics in Quantum electronics Vol.14, No.1, pp. 
56-61, 2008. 
[4] H. M. Salinas and D. C. Fernández, “Comparison of 
PDE-Based Nonlinear Diffusion Approaches for 
Image Enhancement and Denoising in Optical 
Coherence Tomography,” IEEE Transactions on 
Medical Imaging, Vol. 26, No. 6, pp.761-771, 2007. 
[5] P. Perona and J. Malik, “Scale - Space and Edge 
Detection Using Anisotropic Diffusion,” IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence, Vol. 12, No. 7, pp. 629-639, 1990. 
[6] D. C. Fernández et al, “Automated detection of 
retinal layer structures on optical coherence 
tomography images,” Optics Express, Vol. 13, 
No.12, pp.200-216, 2005. 
[7] B. Bouma and G. J. Tearney, Eds., Handbook of 
Optical Coherence Tomography, 1st ed., New York: 
Dekker, 2001. 
[8] J. Weickert, “Coherence-Enhancing Diffusion 
Filtering,” International Journal of Computer 
Vision, Vol. 31, No. 2/3, pp. 111-127, 1999. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  79
Image segmentation of the skin image begins 
with a sobel operator for edge detection. From a 
typical skin image in Fig. 1, we first want to 
segment the surface of SC which is the first strong 
edge in each column. Searching along each 
column from top to bottom, the first edge point 
that has more than 10 edge points in its 11×11 
neighborhood is selected as the first strong edge 
point. 
After detecting all the first strong edge points, 
we then adopt a snake algorithm to refine the edge 
points and construct the SC surface. The energy 
function of the snake is expressed as 
1 2
0
0.5 | ( ) | ( ( ))E v s G v s dsα β′= ⋅ ⋅ + ⋅∫             
(1)                                        
where | ( ) |v s′  and ( ( ))G v s control the continuity 
and image forces, respectively, whileα and β are 
the weighting parameters. We set the values of 
α and β as 0.5 and 0.5, respectively. We sample 
some points as the initial snaxels from every five 
edge points. After contour deformation, we use a 
B-spline function to connect all the optimized 
snaxels.  
Next, we want to segment the ED layer. 
Based on the SC, we move down the SC contour 
25 pixels as an initial position of desired layer. 
Then we select sample points as the initial snaxels 
from every five initial points. Based on Eq. (1), we 
again search the best positions according to the 
normal direction of each snaxel. Finally, by 
splining all the optimized snaxels we can obtain 
the ED layer. 
2.2 Modified deformable model 
The snake model in Eq. (1) can mostly define 
a boundary surface successfully, but it fails in 
some places where the optimized snaxels are 
entrapped by strong edges. To resolve this problem, 
we add some similarity force into the snake 
algorithm. We observe that various layers in OCT 
image can have only several types of texture. The 
orange colored windows in Fig. 1 illustrate some 
typical textures of the SC layer. The similarity 
force in the energy function E is used to look for 
the desired textures in the OCT image. The energy 
function of the snake can be modified as  
1 2
0
| ( ) | ( ( )) ( ( ))E v s G v s S v s dsα β γ′= ⋅ + ⋅ + ⋅∫      
(2)     
where| ( ) |v s′ , ( ( ))G v s and ( ( ))S v s are the continuity, 
image and similarity forces, respectively; 
α , β and γ  are the weighting parameters. We set 
the values ofα , β and γ to be 0.4, 0.2 and 0.4, 
respectively. 
2.3 Computation of Eigen-images 
First we select typical textures and then use 
PCA to decompose them and analyze their 
characters. Let a training set of texture image 
be 1{ ,..., }MI I , where M is the number of texture 
images. Each image Ii is represented with a vector 
iΓ of length N2 describing an N by N image. The 
average texture image of the training set is defined 
by 1
1
M
ii
Mμ − == Γ∑ . Then each texture image 
subtracts this average from the vector 
by i i μΦ = Γ − . Then the covariance matrix is 
defined by 1 ,TC M BB−=  
where 1 2[ , ,..., ]MB = Φ Φ Φ . Then the PCA is used 
to decompose covariance matrix into N2 
orthogonal eigenvectors 21,... Nu u so as to best 
describe the distribution of these images.  
2.4 Texture recognition 
In the on-line recognition process, an input 
image ′Γ  is first projected to the Eigen-space by 
 
參加 第十六屆醫學與生物力學國際研討會 報告 
 
 
報告人姓名 孫永年教授 
服務機構 
及職稱 
國立成功大學 
資訊工程學系 
會議時間 
地點 
97.07.23-97.07.26
美國 匹茲堡 
本會核定 
補助文號 
NSC96-2221-E006-247
會議名稱 
 
(中文) 2008年 第十六屆醫學與生物力學國際研討會 
(英文) The XVIth International Conference on Mechanics in 
Medicine and Biology 
 
發表論文題
目 
 
(中文) 以對位方式利用鍵結模型進行多姿態手骨核磁造影
之影像分割 
(英文) “Registration-based Hand bones Segmentation with 3D 
Articulated Model for Multi-posture MR Images” 
(中文) 利用三維模型與色彩特徵之多影像手指運動追蹤 
(英文) Multiple-View Finger Tracking Using 3-D Model and Color 
Features for Therapeutic Evaluation  
(中文) 從三維與四維超音波影像序列追蹤肌肉運動之研究 
(英文) Muscular Motion Tracking from 3D and 4D Ultrasound 
Image Sequence 
 
 
 
 
 
為匹茲堡大學與卡內基美隆大學之參觀與交流。此次我們所發表三篇文章，均被安
排在醫學影像之時段報告。這三篇論文分別為：（1）以對位方式利用鍵結模型進行
多姿態手骨核磁造影之影像分割 “Registration-based Hand bones Segmentation with 
3D Articulated Model for Multi-posture MR Images”，（2）利用三維模型與色彩特徵
之多影像手指運動追蹤“Multiple-View Finger Tracking Using 3-D Model and Color 
Features for Therapeutic Evaluation”，與（3）從三維與四維超音波影像序列追蹤肌肉
運動之研究  “Muscular Motion Tracking from 3D and 4D Ultrasound Image 
Sequence＂。 
 
在第一場口頭論文發表是由陳昕辰同學報告，題目是「以對位方式利用鍵結模型進
行多姿態手骨核磁造影之影像分割」。由於探討手部內部組織的運動，骨頭的資訊是
相當重要的。因此我們發展一個從離散的姿勢序列影像中分割手骨的方法，日後將
應用於探討手或者其他鏈結組織的內部結構之運動情形。我們先建立一個手部模型
並且利用影像與手部幾何特性將它自動化放入目標影像最契合的位置，達到骨頭分
割的目的。而參考模型的建立，包含骨頭紋理樣板的建立以及手部幾何關係的量化。
接著模擬相鄰骨節的關節機制，使得此模型能夠具備被驅使的條件。最後，我們談
論到兩階段的對位方法，介紹如何利用骨頭以及鄰近區域的紋理特徵來作為第一階
段的對位，接著利用骨頭內灰階均質性的最佳化作為第二階段的對位進一步微調對
位結果。然後展示分割結果與應用。 
    然而報告中有來自美國及中國大陸學者之英文發問，陳同學之英文報告相當
不錯，解答提問之經驗稍微不足，多參加幾次會議及多加練習，日後將可應付自如。
發問者提出如何應用與研究動機之問題，除欣賞我們的創意，也提供重要參考改革
意見。而我們由其它論文發表者所討論之三維對時間之運動量測模型討論中，也引
發了未來研究方向的新構想。 
 
在第二場口頭論文發表為「利用三維模型與色彩特徵之多影像手指運動追蹤」，
報告內容主要形容一個由四部像機同步擷取影像，並用以追蹤並分析手部運動之系
統。該系統使用三為鍵結式手模型配合以彩色特徵以分析追蹤手部影像，同時借電
 
 
 
究群從不同的角度切入問題，並提出各自的解決方法，亦十分有趣。會議中亦互相
交換意見，並探討互訪即將來共同合作相關研究計畫之可能性，獲得正面之回應與
實質之結論。除了將繼續進行神經學及顛癇症之相關研究之外，本研究群之林教授
並將於未來一年前往訪問研究。由於會議論文數量龐大，藉由聽取他人的簡報，除
了可以讓我們了解他人的研究思考及處理方法外，亦可以與國際相關研究學者互
動，進而了解國際生物醫學力學相關研究的趨勢走向，以及最新的研究成果。而在
會議中遇到許多國外教授均為多年不見，能有機會深談敘舊，可遇不可求。而國內
數位教授學者也在會場多所相聚長談，是難得的機會與收穫。 
 
 在聽過這些議程中之部分文章發表後，感覺能經常參加此類研討會，吸取新知
識與觀摩別人的作法，對我們的研究是有益的。尤其所做面對面的溝通，則是我們
在網路交流較不易達到的效果。而在醫學影像相關的發表中，有關於臨床的複雜問
題，通常親自討論可得更大益處。故十分感謝國科會對此次研討會之補助，並希望
在研究上能更努力更有收益，而再有機會參加此類國際性之討論。最後，在聽過多
場國際學生之論文發表之後，覺得我國學生的臨場經驗較為不足，英文提問之回答
也需加強訓練，這或許是我國高等教育國際化急待改進的一環。 
 
三、攜回資料名稱及內容 
 會議論文集一本。 
   
 
 
 
 
參加 2008 生物醫學力學國際研討會發表之三篇文章如下: 
 
[1] H.C. Chen, I.M. Jou and Y.N. Sun*, “Registration-based Hand Bones Segmentation 
with 3D Articulated Model for Multi-posture MR Images,” 16th INTERNATIONAL 
CONFERENCE ON MECHANICS IN MEDICINE AND BIOLOGY PITTSBURGH, 
PA, USA, 23rd-25th July 2008. 
 
[2] Cheung-Wen Chang, Sheng-Pin Ho, Yung-Nien Sun*, L.C. Kuo, Chou-Ching K. Lin, 
Ming-Shaung Ju, F.C. Su, I-Ming Chou, “MULTIPLE-VIEW FINGER TRACKING 
USING 3-D MODEL AND COLOR FEATURES FOR THERAPEUTIC 
EVALUATIONS,” 16th INTERNATIONAL CONFERENCE ON MECHANICS IN 
MEDICINE AND BIOLOGY PITTSBURGH, PA, USA, 23rd-25th July 2008. 
 
[3] Yuh-Hwan Liu*, Chi-Yi Lin, Shu-Min Chen, Chou-Ching Lin, Chung-I Huang, 
Yung-Nien Sun, “MUSCULAR MOTION TRACKING FROM 3D AND 4D 
ULTRASONIC IMAGE SEQUENCES,” 16th INTERNATIONAL CONFERENCE 
ON MECHANICS IN MEDICINE AND BIOLOGY PITTSBURGH, PA, USA, 
23rd-25th July 2008. 
 
 
 
 
 
 
 
 
 
 
 
 Conclusion and future research ones by using constraints on bone geometry. The 
resulting transform parameters at this step contain six 
values, i.e. tX, tY, tZ, rX, rY and rZ for each bone. They are 
the translation along the x, y and z axis, and the rotation 
angle around the x, y and z axis of the world coordinate 
system, respectively.  
 
We proposed a new approach to segment the hand 
bones from MRI volumes of different postures based on 
the results of automatic registration. We construct an 
articulated reference model from MRI of the first 
posture. We then drive the reference model to position 
each bone automatically. The stable segmentation 
results based on registration are then obtained by 
integrating the information of intensity, texture of bone 
and hand geometry. The proposed method not only is 
more automatic than the existing methods but achieves 
good results for clinical applications. 
 
Results and Discussion 
 
In the experiments, we apply the proposed approach 
to the MRI volumes of different postures and show only 
the results of posture 3. In Figure 1, the boundaries (the 
colored contours) of the segmented bones are overlaid 
onto the MR images. It clearly reveals that they are very 
close to the real boundaries of bones, even for the 
regions with artifacts which are with non-uniform 
intensity distribution, non-clear boundary or nearby the 
tendons that have similar intensity as bones. Above all, 
manual intervention is almost unnecessary for 
segmenting hand bones in all the intermediate postures.  
In the future, we will investigate the bone shape 
deformation after rigid registration to make the 
detection of bone boundary more accurate. Besides, we 
will analyze the hand kinematics based on the 
segmentation results of different postures and extend the 
current research to handle the inter-subject data. 
 
 Acknowledgements 
 
The authors would like to express their appreciation 
for the grant under contract NSC 95-2221-E-006-239-
MY2 from the National Science Council, Taiwan, 
R.O.C. Also, this work made use of shared facilities 
supported by the Program of Top 100 Universities 
Advancement, Ministry of Education, Taiwan, R.O.C. 
 
References 
 
[1] Ryu, J.H., Miyata, N., Kouchi, M., Mochimaru, M., 
Lee, K.H. (2006): ‘Analysis of Skin Movements 
with Respect to Bone Motions using MR Images’, 
Journal of Biomechanics, 39, pp. 844-852   Figure 1: Segmentation result (blue color) and hand 
region (orange color) of posture 3; fitness between 3D 
model and 2D MR image slices. Meanings where the 
arrows direct to: 1: non-uniform intensity distribution. 2: 
non-clear of boundary. 3, 4: nearby the tendons.  
[2] Sotoca, J.M., Inesta, J.M., Belmonte, M.A. (2003): 
‘Hand Bone Segmentation in radioabsorptiometry 
images for computerized bone mass assessment’, 
Computerized Medical Imaging and Graphics, 27, 
pp. 459-467  
As we design the penalty constraint, the proposed 
method especially takes the hand geometry into account 
and thus enhances the result of optimization. Figure 2 
shows the effects of the penalty constraint that plays an 
important role for the stability of segmentation. In this 
figure, the target bone of registration is indicated by an 
arrow. In the center figure, the target bone collides with 
its PAB as the solution is trapped in the local minimum. 
In the third column, a more desired result is obtained by 
the proposed method. Over all, the segmentation results 
are very close to the manual results by clinical experts. 
 
  
 
Figure 2: (Left) Initial position, (Center) Registration 
without Epenalty, (Right) Registration with Epenalty. 
 
[3] Kamojima, S., Miyata, N., Ota, J. (2004): 
‘Identification of Position and Orientation of Hand 
Bones from MR Images by Bone Model 
Registration’, IEEE/RSJ Int. conf. on Intelligent 
Robots and Systems, Japan, pp. 2021-2027 
[4] Wang, H., Vergeest, J.S.M., Song, Y., Wiegers, T. 
(2007): ‘Automated 3D scan multi-view registration 
based on rotation estimation’, The 15th Int. Conf. 
on Computer Graphics, Visualization and Computer 
Vision, Plzen, Campus Bory, pp. 137-145  
[5] Lin, J., Wu, Y., Huang, T.S. (2000): ‘Modeling the 
constraints of human hand motion’, Proc. of the 
Workshop on Human Motion, Los Alamitos, CA, 
pp.121-126
Collision 
16th INTERNATIONAL CONFERENCE ON MECHANICS IN MEDICINE AND BIOLOGY 
Pittsburgh, PA, USA, 23rd-25th July 2008 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1: The flowchart of the proposed system 
 
   
   
Fig. 2: Top: results of classical mean-shift algorithm. Bottom: 
results of illumination-robust mean-shift algorithm. 
 
(a) (b) 
Fig.3: The proposed 3D model and its projection on four views 
for the motion of semi-spherical grasping. (a) The initial model. 
(b) The grasping motion of model. 
 
Fig. 4: Joint angles in three axes ( x axis:  pronation/supination, 
y axis: adduction/abduction, z axis: flexion/extension) of MCP 
and IP joints of thumb during pinch with index finger. 
 
Conclusions 
 
The complex occlusion problem of fingers motion is a 
quite challenging task due to the complexity in 
processing the articulated finger motion with a high 
degree-of-freedom. Although partial occlusions occur in 
some frames, the proposed strategy successfully 
conquers these difficulties to correctly reconstruct 3D 
markers from multiple views. And the illumination-
robust mean-shift algorithm is designed to handle the 
occurrence of abruptly markers jumping in time 
sequence. In the experiments, the marker positions 
tracked by the proposed are more reliable than by those 
by the conventional method. The resulting model 
parameters provide useful kinematics information of 
fingers. They are valuable to the diagnoses and 
treatments of hand diseases. 
 
 
References 
 
[1] C. W. CHANG, L. C. KUO, Y.T. CHENG, F. C. SU, 
I.M.JOU, AND Y. N. SUN, (2007) ‘Reliable Model-
based Kinematics Analysis System for Articulated 
Fingers’ IEEE Conf. on Biomedical Enginerringing 
(EMBS), Lyon. 2007, pp. 1003-18. 
[2] L.-C. KUO, F.-C. SU, H.-Y. CHIU, AND C.-Y. YU, 
(2002) ‘Feasibility of using a video-based motion 
analysis system for measuring thumb kinematics’, 
Journal of Biomechanics,  35 (11), pp. 1499-1506. 
 [3] M BRAY, E KOLLER-MEIER, L VAN GOOL, (2004)’ 
Smart particle filtering for 3D hand tracking’ 
Automatic Face and Gesture Recognition, sixth 
international conference,  pp. 675- 680.  
 [4] GREGORY S. RASH!, P.P. BELLIAPPA, MARK P. 
WACHOWIAK, NAVEEN N. SOMIA, AMIT 
GUPTA,(1999),’A demonstration of the validity of a 
3-D video motion analysis method for measuring 
finger flexion and extension’, Journal of 
Biomechanics 32, pp. 1337-1341. 
 [5] ZY.ZHANG, ‘Determining the epipolar geometry and 
its uncertainty: a review’ (1998): The International 
Journal of Computer Vision, 27. 
[6] D. COMANICIU, V. RAMESH, AND P. MEER, (2003): 
‘Kernel-Based Object Tracking’, IEEE Trans. PAMI,  
25 (5), pp. 564-575. 
[7] ARULAMPALAM, M.S.  MASKELL, S.  GORDON, 
N.  Clapp, T. (2002): ‘A tutorial on particle filters for 
online nonlinear/non-Gaussian Bayesian tracking’, 
Signal Processing, IEEE Transactions, 50 (2), pp. 
174-188. 
 
 
 
 
 
Illumination-robust 
mean-shift algorithm to 
get 2D marker position 
 Particle filter 
Set up the 
correspondence 
and reconstruction 
markers  
Generate particles 
Based on prior 
1( | , )t tp x x m−  
Color Feature extraction 
Observation model building and 
evaluation likelihood, get 
( ) , 1 ~it i nπ =  
Weighting particles and get estimated 
state 
Pre-frame state   
1ˆtx −  
ˆtx  
 
 
Multiple-view 
finger images  
