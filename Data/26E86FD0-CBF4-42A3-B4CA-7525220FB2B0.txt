目錄
一、 前言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
二、 研究目的 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1
三、 文獻探討 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2
四、 研究方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3
五、 結果與討論 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4
六、 計畫成果自評 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9
七、 參考文獻 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
八、 附錄 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14
II
三、 文獻探討
傳統的檔案系統, 無法支援平行的環境需求, 所以對於需要大量資料傳輸的場合, 便顯
得有點力不從心; Sun Microsystem 的 NFS (Network File System)[20] 就是屬
於這樣的一個例子。 NFS 雖然廣泛的使用在 Unix 作業系統上, 但是它卻缺乏平行處
理的能力, 無法應付同時而大量的存取。 此外, 因為是單一伺服器的架構, 所以它也無
法提供容錯的機制, 更無檔案分斷 (Striping) 的能力。 最大的好處是他提供了一個簡
單而一致的存取介面, 好讓不同的節點可以存取同一份檔案映象, 這在某種程度上, 也
是屬於單一檔案系統。 目前仍有一些叢集系統使用 NFS 當作其上的檔案系統, 而仰賴
MPI-IO[17] 來處理平行的議題。而平行檔案系統, 在以前是屬於大型電腦的專屬品(例
如: Intel Paragon 的 PFS[13], IBM SP2 的 VESTA[3]); 並無法應用到現今的叢
集系統上。 PVFS[2] 算是一個在 Linux 上免費可取得而且成熟的產品, 而且已被應用
於許多的叢集伺服器上。但是他採用的檔案分斷概念,並沒有提供一個良好的容錯機制。
簡單的說, 如果叢集系統裡面的其中一個輸出入節點 (I/O Nodes) 故障, 整個檔案系
統便無法取得, 除非這個故障的節點能被修復。 如果這個被修復的節點並無法提供故障
前的檔案片段, 那麼整個檔案系統便會完全的故障, 無法再繼續提供服務。
分散式 RAID 的概念, 最早由 Stonebraker 等人所提出[24], 之後被應用於 Swift
檔案系統上[16]。 因此,Swift 檔案系統支援了 RAID 0, 4, 5 等技術。 在 Swift 檔案系
統裡, 只要錯誤不是發生在 metadata 伺服器上, 單一的錯誤就可以被修正。Zebra[6]
檔案系統使用了類似於 Swift 的概念, 但是使用記錄檔的方式來作寫入的動作, 比起
Swift, 這樣的寫入有著更好的效能。 xFS[25] 進一步的把 Zebra 檔案系統的集中式
伺服器分散化, 此外也採用了全域式的快取機制[4]。 GFS[23] 及 GPFS[22] 使用特
製的網路連接器, 把磁碟系統跟用戶端(Clients) 連接起來。 這樣的作法間接使得伺服
器的概念不復存在, 也解決了可能因為伺服器故障所引起的錯誤。 然而這些特製的網路
架構耗費甚鉅, 也無法使用在一般的叢集電腦裡。 OSM (Orthogonal Striping and
Mirroring)[12]提出了新的概念 (RAID-x), 可以增進平行程式寫入檔案系統的效能,
除此之外,它也解決了在叢集系統裡面可能遭遇的可擴充性問題。然而,其採用映射 (mir-
roring) 的方式來保護資料, 卻會大大的消耗整個叢集系統的網路頻寬。
CEFT-PVFS[28] 檔案系統, 就像 PIOUS[18] 及 Petal[15]一樣, 也是透過映射
的方式來保護資料免於流失。從它的命名方式我們可以知道,它也是根據 PVFS[2]而來
的。 它所實作的方式乃是直接在 PVFS 上面建立映射的機制, 整個的感覺就好像是一
個 RAID-10 的機制。 然而, 它也遭遇了使用映射的問題-消耗太多的網路頻寬。除此之
外, 它還有著資料同步的問題, 如何確保資料在備份節點跟工作節點間的一致, 這也導
致了一些問題。 CSAR[19] 使用類似 HP AutoRAID[26] 的技術, 利用階層化的概念,
建立了 RAID-1 跟 RAID-5 在同一個系統上。 他們將 RAID-1 跟 RAID-5 的好處
同時集中在一個系統上, 不但可以避免寫入效能低落的問題, 也解決了網路頻寬被大量
消耗的狀況。CSAR 一樣是建立在 PVFS 上面, 但是其假設所有的資料都必須平均的
散佈在所有的節點上。 除此之外, 他的容錯機制是利用客戶端的函式庫來達成, 這也意
味著, 傳統的應用程式必須重新編譯、 甚至於改寫, 才有辦法利用其所提供的容錯機制。
2
Metadata Server
IOD
IO Node
IOD
MGR
IO Node
IOD
IO Node
IOD
IO Node
IOD
...
Data 
Message SIO SHM
圖 1: 可靠性檔案系統(Reliable Parallel File System)
須”的定義就有賴我們的快取置換機制來達成。 我們也推導出了使用快取可以減少的硬
碟存取數目, 這也證明了我們的快取機制對於整個檔案系統的效能是有著決定性的影響
的。
五、 結果與討論
圖1顯示了我們第一年的可靠性檔案系統的雛形, 如同我們在前一節所提到的, 我們使
用 RAID的概念來補強 PVFS的容錯能力。為了方便實作起見, 我們將 RAID-4的概
念實做在 PVFS的架構裡。這樣的架構有一各好處,我們不用去更改每一個檔案節點的
配置, 而直接把檔案的同步資料儲存在metadata 伺服器上, 透過這個集中式的 meta-
data server, 我們在確保資料的繁冗性時又可以維持資料的一致性,而無須透過其他的
同步機制。 圖1中的 SIO 就是用來儲存同位資料的 Daemon, 我們修改了部分的函式,
好讓同位資料可以送達至 metadata 伺服器上。 這樣的設計有個好處, 因為 metadata
伺服器會知道每個節點的狀態, 所以一旦發生錯誤, 它可以立刻反應, 便可以產生一個
新的檔案節點在其上, 然後動態的修復受損的資料。而為了解決 RAID 架構裡面, 因為
必須維護同位資料, 導致寫入效能低落的問題 (因為每一次的寫入都必須包含兩次讀寫
跟兩次寫入); 我們也設計了一個機制, 稱為延後寫入 (delay write), 這可以使得寫入
的效能大大提升 (因為是直接寫入到緩衝區裡, 而非直接寫入到硬碟), 當有連續小檔寫
入時, 改善尤其可觀。 此延後寫入的方式, 意味著, 檔案只有在關閉或讀寫的時候, 才會
被寫回整個平行檔案系統, 這樣可以增加若干程度的寫入效能, 並且也可以省掉處理資
料一致性的問題, 圖1中 SHM 就是代表這樣的一個寫入緩衝區。
我們使用了 Bonnie[1]來測試我們的可靠性檔案系統, 並與 PFVS 的結果作比較。
根據實驗的結果, 我們可以發現, 在讀取效能方面, 我們的可靠性系統只約比 PVFS 略
低 3%-5%, 引起的原因應該是由於我們加入了額外的通訊機制, 用來偵測錯誤以及決
4
由此可知我們的高可靠性檔案系統的檔案可得性是遠高於其他兩種架構的。而且比起第
二種架構, 我們所花費的費用較低廉, 比較適合於一般的叢集系統上。 如果連群組特性
都考慮進去, 那麼我們可以進一步的將這三種架構的 MTTF 圖繪製於圖2。
檔
案
節
點
數
群組數目
M
T
T
F
(小
時
)
101
102
103
104
105
106
1
2
3
16
64
112
160
208
256
MRPVFS
PVFS RAID
圖 2: 三種架構的 MTTF 比較圖
而在架構方面,我們改進了第一年所提的 SHM 模式, 改用同位快取表來取代 SHM
, 並且對於 metadata 伺服器作了備援的動作,用來保護 metadata 不會因為伺服器的
故障而無法存取, 導致整個系統無法再提供服務。 圖3 所示為我們所提的同位快取表的
示意圖, 在同位快取表裡面的每一行, 都包含了同一個檔案裡面的片段資料, 這些片段
資料的最小單位是一個區塊(在我們的設計裡面是 1 KBytes), 並被分散的儲存在不同
的檔案節點上面, 每一行的片段資料都會有一個相對應的同位檢查碼區塊。 另外還有存
取計數欄位 (Reference Count), 用來表示這一行裡面的區塊被讀或寫的次數。寫入和
讀取分別有相對應的位元記錄著,寫入佔據比較高的 log2N 位元, 而寫入佔據比較低的
log2N 位元, 因此存取計數欄位一共需 2 ∗ log2N 位元, 其中 N 是叢集系統裡面用來
當作檔案節點的數目。 除此之外, 還有一個欄位用來儲存 inode 以及檔案 offset 的資
訊, 透過這個欄位, 我們才有辦法把檔案中的區塊, 跟這個同位快取表做相對應的連結,
概念就好像電腦架構裡面的快取記憶體; 這個欄位一共有 96 位元, 其中 64 位元用來
儲存 inode , 而 32 位元用來儲存檔案偏移量的高 32 位元, 在這個概念下, 我們的系
統可以提供單一檔案的最大容量為 232+12+10 = 16Peta 位元組。
為了避免同位快取表裡面的資料尚未寫入檔案, 以造成資料流失, 我們實做了清洗
(Flush) 的機制。 在這個機制裡面, 我們提供了三個清洗的時機, 第一個就是當有衝突
發生, 也就是不同的區塊要寫入同位快取表裡面的同一各地方時, 被置換的那個會立即
被寫入到檔案,而置換的機制則是看誰的存取計數欄位比較大。第二個時機我們稱為”就
6
Metadata Server
IO Node 1
IOD
MGR
...
File
Mdata
PCT (D1)
IO Node 2
IOD
File
PCT (D2) 
IO Node N -1
IOD
File
PCT (DN-1)
IO Node N
IOD
File
PCT  (P1)
Client
libpvfs
←Ⅰ: Access metadata
→Ⅱ: Access metadata
↓Ⅲ: Clients directly transfer data from I/O nodes
圖 4: 可靠性檔案系統架構圖: 使用分散式同位快取表
DTag Dirty BitPTag LRef GRef Data Payload
96 bits 96 bits 1 bit 31 bits 32 bits 16 KBytes
圖 5: 快取區塊的結構: DTag 用來表示目前這個快取區塊的識別字,PTag則是用來說
明這個快取區塊保留給某個特定的區塊識別字。 如果有寫入發生, 則 dirty bit 會被設
定,LRef 記錄了這個快取區塊被命中的次數,而 GRef 則表示同一個檔案分段所貢獻出
來的整體快取命中次數。
圖7為使用 pvfs test.c 這個工具程式所量測出來的讀取效能。 Pvfs test.c 是一個
MPI 的程式, 因此可以用來作為同時存取的測試, 在這個例子裡面, 我們使用了八個檔
案節點來提供服務,其中每一個檔案節點除了提供檔案服務外,也都充當客戶端使用。而
圖8則顯示了在相同的組態下, 我們的可靠性檔案系統的寫入效能圖。 我們同時也量測
了 PVFS 的效能, 我們可以發現在同樣的組態下, 因為我們使用了分散式快取, 這使得
不管是寫入或者讀取的效能, 我們的可靠性檔案系統都優於 PVFS。
此外, 因為 PVFS 也提供了讓傳統的應用程式直接使用該檔案系統的能力,所以我
們也想知道在這個介面下, 我們的可靠性檔案系統跟 PVFS 間的差別。 圖9及圖10顯
示了這個測試的結果。由圖上我們可以發現,改善的程度不若使用MPI 程式來的好,這
可以歸因於使用 POSIX 介面時, 所有存取檔案的動作被序列化了, 因此整個分散式快
取所能提供的助益便有所折扣。
六、 計畫成果自評
對於平行檔案系統的研究, 有幾項突破, 摘錄如下。 基於 PVFS 而開發的平行檔案
8
圖 9: 使用 Bonnie++ 來測試讀取效能 圖 10: 使用 Bonnie++ 來測試寫入效能
系統, 具有高可靠性的支援。 這種高可靠性的支援, 對於現今需要大量資料作運算或查
詢的場合, 提供了容錯的能力; 這種容錯的能力, 超越了原本用磁碟陣列所能提供的磁
碟容錯。 當叢集電腦裡面的某個節點因為硬體故障 (電源、 網路、 主機板.. 等等), 而無
法提供服務時, 透過我們所開發的平行檔案系統, 可以輕易的回復那個故障節點所儲存
的資料, 讓所有的運算或者查詢動作可以繼續進行, 也可以避免資料的流失。 ”Reliable
Parallel File System”[7] 在 ICS(International Computer Symposium) 2004 獲
得 Best Paper Award。這篇 Paper除了引入軟體磁碟陣列,用來提供平行檔案系統一
個容錯的機制,也提出了所謂的 ”Delay Write” 技巧,用來改善磁碟陣列的寫入效能。”
Modularized Redundant Parallel Virtual File System”[8] 發表於 ACSAC05, 改
良了原先的 ”Delay Write”技巧,提出了所謂的 PCT (Parity Cache Table), 用來進
一步改進寫入的效能。。除此之外,我們也量化了平行檔案系統的MTTF (Mean Time
To Failure), 這可以作為規劃可靠性平行檔案系統時, 一個量化的依據。 而我們分散式
的同位快取表格機制, 也發表於[9], 至於更一般化的 Striping 則發表於[10]。 而我們也
將這三年來關於可靠性檔案系統的研究, 整理成一篇期刊文章, 目前已被接受[11]。 另
外, 我們也將這個系統移植到 Windows 2003 上面, 目前已有初步的結果, 並發表於
NCS 2005[27]上面, 這對於建構一個異質性的網路存取系統, 有著莫大的幫助。
在這三年期的計畫裡, 我們完成了單一的檔案系統介面, 可以用來解決需要資料密
集運算的場合。 我們同時也提出了新的快取機制, 可以改善 Striping 檔案系統的整體
效能。
七、 參考文獻
[1] Tim Bray. Bonnie file system benchmark.
http://www.textuality.com/bonnie/.
[2] Philip H. Carns, Walter B. Ligon III, Robert B. Ross, and Rajeev Thakur.
PVFS: A parallel file system for linux clusters. In Proceedings of the 4th
10
[12] Kai Hwang, Hai Jin, and Roy S.C. Ho. Orthogonal striping and mirror-
ing in distributed RAID for I/O-centric cluster computing. IEEE Trans.
Parallel Distrib. Syst., 13(1):26–44, January 2002.
[13] Intel Supercomputer System Division. Paragon System User’s Guide,
May 1995.
[14] Pankaj Jalote. Fault Tolerance in Distributed Systems. Prentice Hall
PTR, 1994.
[15] Edward K.Lee and Chandramohan A. Thekkath. Petal: Distributed vir-
tual disks. In High Performance Mass Storage and Parallel I/O: Tech-
nologies and Applications, chapter 27, pages 420–430. IEEE Computer
Society Press and Wiley, New York, NY, 2001.
[16] Darrell D. E. Long and Bruce R. Montague. Swift/RAID: A distributed
RAID system. Computing Systems, 7(3):333–359, Summer 1994.
[17] Message Passing Interface Forum.MPI2: Extensions to the Message Pass-
ing Interface, 1997.
[18] Steven A. Moyer and V. S. Sunderam. PIOUS: a scalable parallel I/O
system for distributed computing environments. In Proceedings of the
Scalable High-Performance Computing Conference, pages 71–78, 1994.
[19] M. Pillai and M. Lauria. CSAR: Cluster storage with adaptive redun-
dancy. In Proceedings of the 2003 International Conference on Parallel
Processing, pages 223–230, Kaohsiung, Taiwan, ROC, October 2003.
[20] Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob
Lyon. Design and implementation of the Sun Network File System. In
Proceedings Summer 1985 USENIX Conference, pages 119–130, 1985.
[21] Ghemawat Sanjay, Gobioff Howard, and Leung Shun-Tak. The google file
system. In 19th ACM symposium on Operating systems principles, pages
29–43. ACM Press, 2003.
[22] Frank Schmuck and Roger Haskin. GPFS: A shared-disk file system for
large computing clusters. In Proc. of the First Conference on File and
Storage Technologies (FAST), pages 231–244, January 2002.
[23] Steven R. Soltis, Thomas M. Ruwart, and Matthew T. O’Keefe. The
Global File System. In Proceedings of the Fifth NASA Goddard Confer-
ence on Mass Storage Systems and Technologies, pages 319–342, College
Park, MD, September 1996.
12
14
IEICE TRANS. INF. & SYST., VOL.Exx–??, NO.xx XXXX 200x
PAPER Special Section on Parallel/Distributed Processing and Systems
Reliable Parallel File System with Parity Cache Table
Support
Sheng-Kai HUNG†a), Nonmember and Yarsun HSU†b), Member
SUMMARY Providing data availability in a high perfor-
mance computing environment is very important, especially in
this data-intensive world. Most clusters either equip with RAID
(Redundant Array of Independent Disks) devices or use redun-
dant nodes to protect data from loss. However, neither of these
can really solve the reliability problem incurred in a striped
file system. Striping provides an efficient way to increase I/O
throughput both in the distributed and parallel paradigms. But
it also reduces the overall reliability of a disk system by N fold,
where N is the number of independent disks in the system. Par-
allel Virtual File System (PVFS) is an open source parallel file
system which has been widely used in the Linux environment. Its
striping structure is good for performance but provides no fault
tolerance. We implement Reliable Parallel File System (RPFS)
based on PVFS but with reliability support. Our quantitative
analysis shows that MTTF (Mean Time To Failure) of our RPFS
is better than that of PVFS. Besides, we propose a parity cache
table (PCT) to alleviate the penalty of parity updating. The
evaluation of our RPFS shows that its read performance is al-
most the same as that of PVFS (2% to 13% degradation). As to
the write performance, 28% to 45% improvement can be achieved
depending on the behavior of the operations.
key words: cluster, reliability, parallel virtual file system, re-
dundant array of independent disks, parity cache table
1. Introduction
In this data-intensive world, it is an increasingly sig-
nificant requirement to sustain high data throughput
in a computing environment. However, I/O subsystem
has been the bottleneck of a computing system[1] so
far, especially for clusters. A cluster is an open archi-
tecture consisting of loosely coupled computers. Un-
like commercial machines which have proprietary par-
allel file systems (such as PFS on the Intel Paragon[2],
VESTA[3], [4] on the IBM SP2 machine), clusters lack
for parallel file systems support. Lacking for parallel
file systems support, these Beowulf-like clusters cannot
gain performance benefits from traditional distributed
file systems (such as NFS[5]) even when using MPI-
IO[6] for data accessing.
Parallel Virtual File System (PVFS)[7] has been
developed to enhance concurrent data accesses in the
cluster environment. It is good at achieving high
read/write throughput and providing POSIX interfaces
for legacy applications. However, it is usually used as a
Manuscript revised June 30, 2006.
†The authors are with the Department of Electrical En-
gineering, National Tsing Hua University, HsinChu, Taiwan
a) E-mail: phinex@hpcc.ee.nthu.edu.tw
b) E-mail: yshsu@ee.nthu.edu.tw
temporary space rather than a persistent storage. Ap-
plications running on it should load data and store com-
puting results in another persistent storage. The reason
is that PVFS does not store any redundant information
in its striping structure. Using striping without keep-
ing parities, its MTTF (Mean Time To Failure) may
be lower by a factor of 1
N
[8], where N is the number of
I/O nodes in the cluster system. The lower MTTF a
system has, the less availability it could provide.
Data availability of PVFS can be improved by
purchasing additional disks and using either hardware
RAID[8] (Redundant Array of Independent Disks) con-
trollers or software RAID techniques in each of the I/O
nodes. This method has two disadvantages, one is the
cost incurred, and the other is its low MTTF. The
first one is obvious because everyone wants to reduce
the cost of constructing a COTS (Commodity Off The
Shelf) cluster system as much as possible. The other
can be thought of the result of the Amdahl’s law[9],
since other components within a node collectively dom-
inate the node’s faults. If one of the nodes in a cluster
using PVFS fails, even if its disks are RAIDed, the
striped data within the node can not be accessed un-
less we replace the broken components with new ones.
Using RAID in each node can just only guarantee data
availability within the node, but what if other compo-
nents of the node fail? This would cause the node to
fail and can not cooperate with others, which in turn
makes the whole file system out of services.
In order to provide high data availability in PVFS,
we use the concept of distributed RAID[10] and propose
a parity cache table (PCT) to reduce the penalty in-
curred by parity calculations. Our reliable parallel file
system (RPFS) can not only provide low hardware cost
but also offer higher data availability. The write per-
formance can be improved and the problem of “small
writes” can be alleviated with PCT.
This paper is organized as follows: related research
topics are covered in section 2; the system architecture
of our RPFS will be shown in section 3; a quantitative
analysis of the overall availability will be presented in
section 4; then we will discuss the performance evalua-
tion in section 5; finally we will make some conclusions
in section 6.
16
IEICE TRANS. INF. & SYST., VOL.Exx–??, NO.xx XXXX 200x
Metadata Server
SIOD
IO Node
IOD
MGR
...
Data 
Message
Parity Cache
Table
Mirrored
Metadata Server
SIOD
Mdata Parity
File
IO Node
IOD
File
IO Node
IOD
File
IO Node
IOD
File
Meta 
Data
Parity
Fig. 2 System Architecture: We store the parity information
in the metadata server. A parity cache table (PCT) is used to
alleviate the penalty of updating parities.
these small writes to a bigger write or several bigger
writes. In other words, concurrent parity updating re-
quests are cached and completed in our parity cache
table (PCT). This relieves the problem of concurrent
parity writes incurred in the traditional RAID-4. Our
PCT makes the difference between RAID-4 and RAID-
5 less significant. We will describe this algorithm in
more detail in the following subsections.
The architecture of our reliable parallel file system
(RPFS) is shown in Fig. 2. In RPFS, we use metadata
server as a spare I/O node to store parities of the whole
file system. SIOD is a spare I/O daemon which comes
from the original IOD daemon. We use SIOD to gather
data being read or written from I/O nodes to clients.
Data gathered is cached in PCT and in turn used to
produce the corresponding parity. Using the concept
of RAID with parity support, each write process must
recalculate the parity. The recalculation process in-
cludes reading part of the old data whenever needed,
XORing this part with newly written data and finally
writing the new data along with parity. Updating par-
ity is a high-cost process, and we propose a technique
to alleviate this impact.
3.1 Parity Striping and Updating
Fig. 3 shows the structure of our parity cache table
(PCT). In the distributed paradigm, low level block
information is meaningless. Every disk at each node
has its own block sequences used to store stripes of a
file. We can not relate blocks on different nodes with
their corresponding parity blocks, unless we perform
an initialization process just like what does in a tradi-
tional RAID controller. We use the file offset in dif-
ferent stripes of a file to determine if they belong to
the same parity block (i.e. they are used to construct
the same parity block). This high level abstraction of
blocks can ignore the real block size used in the un-
96-bit Tag
...
96-bit Tag
96-bit Tag
96-bit Tag
B1
B1
B1
B1 B2
B2
B2
B2
... ... ...
...
...
... ...
BN
BN
BN
BN
...
Reference Count
Reference Count
Reference Count
Reference Count
...
...
...
Offset N
File 1
Offset M
File 2
/1024 % 4096N       
/1024 % 4096M       
Fig. 3 Parity Cache Table Structure: The offset is used to de-
termine which entry in the PCT should store the corresponding
block.
derneath file system, and thus support heterogeneous
environments.
As we mentioned before, updating parity is a high-
cost process and may reduce write performance. We
construct a PCT, like the buffer cache used in a tradi-
tional Unix file system. It has 4 K entries, with each
entry contains N (the number of I/O nodes) blocks, a
96-bit tag, and a (N+log2 N)-bit reference count. Each
entry generate the corresponding parity block which is
maintained in the metadata server. Data being read or
written is brought into PCT in the unit of 1 K. The
reference count of an entry records the number of indi-
vidual writes (higher N -bit) and reads (lower log2 N -
bit) in the data blocks of the entry. Higher N bits
relate to write operations. By checking these N -bit lo-
cations, dirty blocks can be flushed into disk correctly.
Lower log2 N bits indicate the number of reads which
contribute to the same parity block happening in the
entry. Whenever a read or a write operation happens,
the corresponding bit of the field is updated.
A 96-bit tag stores the inode number (64 bits)
along with the reduced file offset (32 bits out of 54
bits). Since different blocks of the same file or from dif-
ferent files may hash to the same entry, we should com-
pare this tag whenever a read or written block needs
to be brought into the PCT entry. Using this scheme,
the maximal size of a file which our RPFS can store
is 2(32+12+10) = 16 Peta bytes. However, the size of
this tag is a configurable parameter when we build our
RPFS. Depending on the requirements of the RPFS,
we can adjust its size to save memory space and lower
the overhead of tag comparison. Fig. 4 shows the de-
tail process of how to match a block with the file offset
being read or written.
When small writes happen, we should read the
“unwanted data” to calculate the parity. Here, the
“unwanted data” means those which application does
not need at this time but are used for calculating par-
ity only. The number of these kinds of reads become
large when the number of small writes increases. By
using PCT, we can aggregate several write operations
into a single operation. Besides, if some blocks have
18
IEICE TRANS. INF. & SYST., VOL.Exx–??, NO.xx XXXX 200x
By the analysis of RAID technique in the paper[8],
we know that the MTTF of a system using RAID disks
can be expressed as:
MTTFRAID=
(MTTFD)
2
(D+C ∗nG) ∗(G+ C − 1) ∗MTTR
(3)
• D is the total number of data disks
• G means the data disks in a group
• C is the number of check disks (disks contain parity
information) in a group
• nG denotes the number of groups (each group has
a check disk at least)
• MTTR means mean time to repair
Using distributed RAID-4 technique in a cluster
environment, each I/O node can be used as a parity
node or a data node. This tells us that D + C ∗n G =
(G+C) ∗n G = N. Using this equation, we can rewrite
equation 3 as equation 4.
MTTFRAID =
(MTTFD)
2
N ∗ ( NnG − 1) ∗MTTR
(4)
Applying equation 4 into equation 2, we can ex-
press the final form of MTTFPRAID as:
MTTFPRAID = 1/(
N2 × ( NnG − 1)×MTTR
(MTTFD)2
+
N
MTTFS
)
= 1/(
N2 × ( NnG − 1)×MTTR
(MTTFD)2
+
1
MTTFPV FS
−
N
MTTFD
)
≤ 1/(
1
MTTFPV FS
−
N
MTTFD
)
=
MTTFPV FS ∗MTTFD
MTTFD −N ∗MTTFPV FS
(5)
Applying equation 4 in our RPFS, we can get
MTTFRPFS. The MTTF of our RPFS can be shown
in the following equation. We can do that since both
I/O nodes and metadata server can tolerate at most
one fault.
MTTFRPFS =
(MTTFNode)
2
N ∗ ( NnG − 1) ∗MTTR
=
(
1
1
MTT FD
+ 1
MTT FS
)2
N ∗ ( NnG − 1) ∗MTTR
(6)
Substituting MTTFPV FS into MTTFRPFS,
equation 6 can be rewritten as:
MTTFRPFS =
N2 ∗ (MTTFPV FS)
2
N ∗ ( NnG − 1) ∗MTTR
Table 1 MTTF of these three systems
MTTF(hours) Number of Nodes Group Size
PVFS 1136 8 -
PRAID 1249 8 1
RPFS 368949 8 1
Fig. 5 MTTF Comparison: The MTTF of our RPFS is better
than that of PVFS either with RAID or not. The more groups
used in our RPFS, the better MTTF we can provide.
=
N ∗ n G
(N −n G)
×
(MTTFPV FS)
2
MTTR
≥
N ∗n G
N
×
(MTTFPV FS)
2
MTTR
= nG×
(MTTFPV FS)
2
MTTR
(7)
In the real case, MTTF or MTTR is usually mea-
sured by hours. If we could make MTTR as small as
possible(such as 1 hour), we could rewrite the above
equation as:
MTTFRPFS ≥
n G ∗MTTF 2PV FS . (8)
This means that our RPFS can provide a much
better MTTF than PVFS or PVFS with each node’s
disks RAIDed. Besides, we can also increase nG to im-
prove the MTTF of our RPFS more. In other words,
as the number of nodes increase, the term MTTFPV FS
in equation 8 becomes lower (MTTFPV FS is inversely
proportional to N). To maintain an almost constant
MTTF when the number of nodes increase, we can in-
crease the number of parity groups in the system.
For example, MTTF of a disk is no less than
100,000 hours and MTTR is shorter than 4 hours. As
to the MTTR of a system, its value is less than that of
a disk, since it should account for the failure of power,
CPU , network . . . etc. We assume that MTTF of a
system is usually 10,000 hours. Using the quantity, we
20
IEICE TRANS. INF. & SYST., VOL.Exx–??, NO.xx XXXX 200x
Fig. 9 Write Performance Test: PCT acts as a buffer to cache
data begin written. The limited PCT space causes the perfor-
mance drop around 40 MB. RPFS without PCT suffers two-fold
number of writes compared with PVFS.
limited buffer space causes the performance of RPFS
PCT to drop around 40 MB. As to RPFS without PCT,
every write involves updating the corresponding parity
block. The number of writes of RPFS without PCT
is two-fold than that of PVFS. As in the case of read,
only small workload are cached. This means that only
small writes are considered to prevent the metadata
server from overloading. The performance gain of us-
ing PCT is around 28%. Without PCT, parity updat-
ing penalty is around 25%. Fig. 10 shows the results
of random writes. Since random writes implies small
workload, the number of replacements are reduced in
PCT. Compared with the results of sequential writes,
the performance drop of RPFS with PCT is not so ob-
vious. As to RPFS without PCT, it has almost the
same performance as shown in Fig. 9. RPFS with PCT
has performance gain around 30% compared with that
of PVFS. We show the results of read-modify-write in
Fig. 11. 45% performance gain proves that our PCT
can cache the previous read and is beneficial for read-
modify-write.
6. Conclusions
We have successfully designed and implemented a
RPFS which can supply data to clients even if one
of the I/O nodes fails due to any reason. Our RPFS
can provide almost the same read performance com-
pared with PVFS. As to the write performance, with
the help of PCT, the write performance is also compa-
rable. When the number of clients increase, the meta-
data server could become a bottleneck in our design.
For small writes, data are cached in the server and all
we need to do is to extend the network bandwidth which
the metadata server could afford. For large writes, the
data being read or written are not sent to the server
and no overloaded situation happens. In this case,
Fig. 10 The Performance of Random Write: Compared with
the results of sequential writes, the performance drop of RPFS
with PCT is not so obvious. RPFS without PCT has almost
the same performance as shown in Fig. 9, with parity updating
penalty around 25%. RPFS with PCT has performance gain
around 30% compared with that of PVFS.
Fig. 11 Read-Modify-Write Performance Test: Write benefits
from the previous read data using PCT. Performance gain of
RPFS with PCT is around 45%.
we can equip the metadata server with a more pow-
erful network interface (such as 10 Gigabit ethernet or
Myrinet...) or with multiple network interface cards.
On the other hand, if the number of I/O nodes increase,
we should increase the number of parity group to main-
tain a almost constant MTTF. This means that we have
disjointed metadata servers in the system, which can
share the overall load in the system.
References
[1] R. Thakur, E. Lusk, and W. Gropp, “I/O in parallel ap-
plications: The weakest link,” The International Journal
of High Performance Computing Applications, vol.12, no.4,
pp.389–395, Winter 1998.
[2] Intel Supercomputer System Division, Paragon System
User’s Guide, May 1995.
