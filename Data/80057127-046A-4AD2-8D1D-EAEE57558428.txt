2007 International Conference on Artificial Intelligence and Pattern recognition (AIPR-07)
404
a good candidate because of its ability to generalize in
high-dimensional spaces, such as spaces spanned by
texture patterns. The appeal of SVM is based on its strong
connection to the underlying statistical learning theory.
That is, an SVM is an approximate implementation of the
structural risk minimization (SRM) method [11]. For
several pattern classification applications, SVM have
already been shown to provide better generalization
performance than traditional techniques, such as neural
networks [12][13].
The aim of this paper is to illustrate the potential of
DWT and SVM in CCL defects detection and
classification. Accordingly, a method for multi-class
texture classification using SVM is described. The
proposed method incorporates the DWT and inverse
DWT with the SVM classifier, with the features fed
directly into the SVM classifier. As a result, the features
obtained from the restored defects image are nonlinearly
mapped into the SVM architecture. Actually, in a SVM,
feature classification is implicitly performed by a kernel,
which is defined as the dot product of two mapped
patterns. It is also shown that one kernel can perform the
same operations as the conventional feature classification
method. Since SVM was originally developed for two-
class problems, its basic scheme is extended to multi-
texture classification by adopting the one-against-all and
one-against-one decomposition method. This works by
applying SVM to separate one class from all the other
classes, or separate each pair among all classes. Thereafter,
feature detection and classification are both performed in
accordance with a unique criterion referred to as the
classification rate.
2. Methodology
2.1 Wavelet transform for defect detection
Wavelets are functions generated from one single
function by dilations and translations. The basic idea of
the wavelet transform is to represent any arbitary function
as a superposition of wavelets. Any such superposition
decomposes the given function into different scale levels
where each level is further decomposed with a resolution
adapted to that level [14].
The DWT is identical to a hierarchical sub-band
system where the sub-bands are logarithmically spaced in
frequency and represent an octave-band decomposition.
By applying DWT, the image is actually divided i.e.,
decomposed into four sub-bands These four sub-bands
arise from separate applications of vertical and horizontal
filters. The sub-bands labeled LH1, HL1 and HH1
represent the finest scale wavelet coefficients i.e., detail
images while the sub-band LL1 corresponds to coarse
level coefficients i.e., approximation image. To obtain the
next coarse level of wavelet coefficients, the sub-band
LL1 alone is further decomposed and critically sampled.
Similarly, to obtain further decomposition, LL2 will be
used. This process continues until some final scale is
reached.
2.2 Wavelet reconstruction
The 2-D wavelet analysis operation consists of
filtering and down-sampling horizontally using 1-D
lowpass filter L and highpass H to each row in the image
and produces the coefficient matrices fL(x, y) and fH(x, y).
Vertically filtering and down-sampling follows, using the
lowpass and highpass filters L and H to each column in
f(L) and f(H), and produces four subimages f(LL), f(LH),
f(HL), and f(HH) for one level decomposition. The 2-D
pyramid algorithm can iterate on the smooth subimage
f(LL) to obtain four coefficient matrices in the next
decomposition level. For images, there exist an algorithm
similar to the one-dimensional case for two-dimensional
wavelets and scaling functions obtained from one-
dimensional ones by tensorial product. This kind of two-
dimensional DWT leads to a decomposition of
approximation coefficients at level j in four components:
the approximation at level j + 1, and the details in three
orientations (horizontal, vertical, and diagonal). The
inverse DWT performs a single-level two-dimensional
wavelet reconstruction with respect to either a particular
wavelet or particular wavelet reconstruction filters that
you specify.
2.3 Support vector machine classifier
SVMs [15-22] have recently been proposed as popular
tools for learning from experimental data. The reason is
that SVMs are much more effective than other
conventional nonparametric classifiers (e.g. the neural
networks, nearest neighbor, k-NN classifier) in term of
classification accuracy, computational time, land stability
to parameter setting. The supervised neural network has
the task of discriminating over the state space where the
class decision boundaries are complex or ambiguous. The
network should be a local approximation type and should
incorporate formalism in its design for obtaining adequate
generalization performance. One of the neural network
models that fulfill these requirements is support vector
machine (SVM). The SOM prototype vectors create
piecewise linear class boundaries that are usually not
effective for resolving the class ambiguity over all the
regions of the state space. Moreover, even if the learning
procedure enlarges the SOM adaptively until each of its
neurons represent unambiguously a single class, this
solution addresses only the minimization of the training
error and ignores the generalization performance. In the
2007 International Conference on Artificial Intelligence and Pattern recognition (AIPR-07)
406



N
ji
jijiji
N
i
i yyW
1,1
)()
2
1
)( x(x (10)
Now, let )()(),( jijiK xxxx  , we can rewrite
above equation as



N
ji
jijiji
N
i
i KyyW
1,1
)(
2
1
)( x,x (11)
where K is called a kernel function and must satisfy
Mercer’s theorem [12]. Finally, the decision function
becomes:
)),(sgn()(
1



N
i
iii bKyf xxx  . (12)
Typical kernel functions are the following:
Linear Kernel
zx)z,x( K . (13)
Polynomial Kernel
dcoefK )zx()z,x(  , (14)
whereγand coef are constants and d is a degree.
Gaussian Radial Basis Kernel
)zxexp()z,x(
2 K (15)
whereγis a constant.
Sigmoidal Neural Network Kernel
)zxtanh()z,x( coefK   (16)
whereγand coef are constants.
2.4 Multiclass Support Vector Machines
Two main approaches have been suggested for
applying SVMs to multiclass classification. One is the
one-against-all strategy to classify between each class and
all the remaining; the other is the one-against-one strategy
to classify between each pair. In each, the underlying basis
has been to reduce the multiclass problem to a set of
binary problems, enabling the basic SVM approach to be
used. In the one-against-all approach, a set of binary
classifiers, each trained to separate on class from the rest,
is undertaken, and the input vector allocated to the class
for which the largest decision value was determined [22].
The ith SVM is trained from the training samples where
some examples contained in the ith class have “+1”labels,
and other examples contained in the other classes have “-
1”labels. Specifically, with this approach, where n is the
number of classes
   iTi bxw (17)
where i=1,…, n.
The data x then belong to the class, for which the above
decision function has the largest value, i.e.,
   iTini b  xwmaxargxofclass ,,1 (18)
The second method of reducing a multiclass problem to a
series of binary ones to enable the application of the basic
SVM model for multiclass classification is the “one-
against-one”approach. In this approach, a series of
classifiers is applied to each pair of classes, with the most
commonly computed class label kept for each input. The
application of this method requires n(n-1)/2 classifiers or
machines be applied to each pair of classes and a strategy
to handle instances in which an equal number of votes are
derived for more than one class for a pattern [22]. Once
all n(n-1)/2 classifiers have been undertaken, the max-win
strategy is followed. Specifically, if
   jlTjl bxwsgn evaluates x to be in jth class,
then the vote for the jth class is incremented by one; else
that for the lth class is increased by one. Finally, the data
vector x is predicted to belong to the class with maximum
number of votes.
3. Experimental results
In this section, we present the experimental results
followed the previous section approach on a variety of
structural and statistical textures found in real samples to
evaluate the performance of the proposed defect detection
and classification method. All experiments are
implemented on a personal computer, and images are
256×256 pixels with 8-bit gray levels. The approach is
divided into four steps: (1) preprocessing the original
images; (2) wavelet transform and inverse wavelet
transform; (3) feature extraction; and (4) support vector
machine classification.
3.1 Defects classification and preprocessing
Among the collected samples, there are five major
categories of defects on the CCL surface that include
pinholes, convex dots, flashes, oxidation, and other
defects. Based on the characteristic of these three
categories of defects, we classified into three geometric
categories, namely, points, lines, and planes categories.
The major preprocessing steps in this study include
smooth filtering, sobel filtering, close morphological
operation, and binary thresholding technique. The main
objective of preprocessing is to perform the image
processing operation on the source image to enhance and
smooth images, accentuate or extract image edges, or
remove `noise' from an image. They allow images to be
separated into their low and high spatial frequency
components. The three different kinds of defect examples
are illustrated as follows:
2007 International Conference on Artificial Intelligence and Pattern recognition (AIPR-07)
408
diagonal subimages can effectively enhance the defect
regions in the restored image. It also efficiently separates
defects from the background in the corresponding binary
images. Therefore, the features extractions are preferred
to be selected from the defects in diagonal subimage.
There were 37 selected features in this study, including
area, perimeter, elongation, convex perimeter,
compactness, number of holes, roughness, Euler number,
invariant moments obtained from the geometric features;
and contrast, energy, entropy, local homogeneity, cluster
shade, and cluster prominence obtained from co-
occurrence matrix, respectively. The spatial feature does
not need to consider the related parameters, whereas the
features selected from the co-occurrence matrix should
consider the interval (number of pixel) and angle. The
operating characteristic curves for the six features are very
similar to each other in terms of the interval and angle.
The number of pixels is set to d=1 and the angle is set to 0
degrees.
3.4 Support vector machine
After obtaining the features extracted from defect
subimage, involving the above described approach, we
used the support vector machine for classifying five main
defects categories on the CCL surface. The SVMs were
constructed with LIBSVM version 2.6 [24] software to
validate the classification capability of the proposed
wavelet and inverse wavelet reconstruction approach.
Although the generalization ability of the SVM is
relatively robust to variations in the parameter settings,
the method used to define the parameter γ and cost 
parameter C in the kernel function RBF will ensure that
high accuracy is obtained. Accordingly, the parameters
were set in the range of  =[ 24,23,…,2-10 ],
C=[ 212,211,…,2-2 ] using 15*15 combinations for testing
the classification accuracy. All of the data were divided
into 2/3 training data set (60 data) and 1/3 (30 data)
testing data set. All of the experiments were conducted
using the one-against-all and one-against-one via holdout
method and k-fold-cross-validation methods, k equal to 5.
Table 1 lists the number of training data set and testing
data set labeled as “+1” and “-1.”
Table 1. Training/Test set for one-against-all
method (5 classes)
Training Set Testing Set
Classes Number of
labeled
“+1”
Number of
labeled“-
1”
Number
of
labeled
“+1”
Number
of
labeled
“-1”
1 10 50 5 25
2 13 47 7 23
3 23 37 12 18
4 7 53 3 27
5 7 53 3 27
Table 2. Classification results using
one-against-all (C=22, =24~ C=212,=2-10)
Classes Training
Accuracy
Test
Accuracy
1 100% 83.33%
2 100% 76.67%
3 100% 60.00%
4 100% 90.00%
5 100% 90.00%
Average 100% 80.00%
Table 3. Cross-validation results for
one-against-all
Classes Accuracy
1 82.22%
2 77.78%
3 61.11%
4 88.89%
5 88.89%
As shown in Table 2, it can be seen that the test accuracies
were ranged from 60% to 90% by adjusting the parameter
C and from C=2-2,=24 to C=212,=2-10. The results
also showed that the smaller number of training data or
testing data can reach the higher testing accuracies in the
SVM classification, such class 4 and class 5. The lowest
accuracy is class 3, only 60% accuracy, because the class
3 defects were expanded non-uniformly and likely to be
misclassified with the other defects types. Moreover, the
experiments were not considered the defect angle that was
also made the more test error on class 3. On the other
hand, the class 4 and 5 included oxidation defects spread
all over the surface and user-defined small spots can be
easily distinguished from the other defects types.
Therefore, class 4 and 5 were reaching the highest
accuracies 90% in the experiments. Using the cross-
validation test with k equal to 5, Table 3 also showed the
similar results with the holdout approach. Namely, the
class 4 and 5 reached the highest accuracies and the class
3 showed the lowest accuracy in the one-against-all multi-
class classification experiments.
Table 4. Training/Test set for one-against-one
method (5 classes)
2007 International Conference on Artificial Intelligence and Pattern recognition (AIPR-07)
410
[3] R.W. Conners, C.W. Mcmillin, K. Lin, and R.E.
Vasquez-Espinosa, “Identifying and locating surface
defects in wood”, IEEE Trans. on Pat. Anal. and
Mach. Intel. PAMI-5, 1983, pp.573-583.
[4] L.H. Siew, and R.M. Hogdson,“Texture measures for
carpet wear assessment”, IEEE Trans. on pat. Anal.
and Mach. Intel., 10, 1998, pp.92-105.
[5] K.V. Ramana, and B. Ramamoorthy, “Statistical
methods to compare the texture features of machined
surfaces”, Pat. Recog., 29, 1996, pp.1447-1459.
[6] C.H. Chen, and G.G. Lee, “On digital mammogram
segmentation and microcalcification detection using
multiresoultion wavelet analysis”, Grap. Mod. and Im.
Proc., 6, 1997, pp.3606-3609.
[7] T. Chen, and C.-C.J Kuo, “Texture analysis and
classification with tree-structured wavelet transform”,
IEEE Trans. on Im. Proc., 2, 1993, pp.429-441.
[8] K. Maruo, T. Shibata, T. Yamaguchi, M. Ichikawa,
and T. Ohmi, “Automatic defect pattern detection on
LSI wafers using image processing techniques”,
IEICE Trans. on Elec., E82-C, 1999, pp.1003-1012.
[9] C.S. Lee, C.-H. Choi, J.Y. Choi, and S.H. Choi,
“Surface defect inspection of cold rolled strips with
features based on adaptive wavelet packets”, IEICE
Trans. on Inform. Sys. E80-E, 1997, pp.594-604.
[10] H. Sari-Sarraf, and J.J.S. Goddard, “Robust defect
segmentation in woven fabrics”, Proceedings of the
IEEE Comp. Soc. Conf. on Comp. Vis. and Pat.
Recog., 1998, pp.938-944.
[11] Vapnik, V.N., The nature of statistical learning
theory, Springer Verlag, New York, 1995.
[12] B.Schokopf, K. Sung, C.J.C Burges,. F. Girosi, P.
Niyogi, T. Pogio, and V. Vapnik, “Comparing
support vector machines with Gaussian kernels to
radial basis function classifiers”, IEEE Trans. on sign.
Proc., 45(11), 1997, pp.2758-2765.
[13] Haykin, S., Neural Network—A comprehensive
foundation, second edition Prentice Hall, 1999.
[14] S. Mallat, “A theory for multiresolution signal
decomposition: the wavelet representation”, IEEE Pat.
Anal. and Mach. Intel., 11(7), 1989, pp.674-693.
[15] I. El-Naqa, Y. Yongyi; M.N. Wernick,; N.P.
Galatsanos,; and R.M. Nishikawa, “A support vector
machine approach for detection of
microcalcifications”, Medi. Im., 21(12) (2002) 1552-
1563.
[16] W. Liyang; Y. Yongyi; R.M. Nishikawa, and J. Yulei,
“A study on several Machine-learning methods for
classification of Malignant and benign clustered
microcalcifications”, IEEE Trans. on Med. Im.,
24(3) , 2005, pp.371-380.
[17] O. Chapelle, P. Haffner,and V.N. Vapnik, “Support
vector machines for histogram-based image
classification”, IEEE Trans. on Neur. Net., 10(5),
1999, pp.1055-1064.
[18] K. Kwang, I. Keechul, J. Jung; and H. Kim,
“Texture-based approach for text detection in images
using support vector machines and continuously
adaptive mean shift algorithm”, IEEE Trans. on Pat.
Anal. and Mach. Intel., 25(12), 2003, pp.1631-1639.
[19] W. Liyang; Y. Yongyi; R.M. Nishikawa, M.N.
Wernick, and A. Edwards, “Relevance vector
machine for automatic detection of clustered
microcalcifications”, IEEE Trans. on Medi. Im.,
24(10), 2005, pp.1278-1285.
[20] Q. Song, W. Hu, and W. Xie;“Robust support vector
machine with bullet hole image classification”, IEEE
Trans. on Sys., Man and Cyber., Part C, 32(4), 2002,
pp.440-448.
[21] K.I. Kim, J.S. Keechul, H. Park, and H. J. Kim,
“Support vector machines for texture classification”,
IEEE Trans. on Pat. Anal. and Mach. Intel., 24(11),
2002, pp.1542-1550.
[22] C.W. Hsu, and C.J. Lin, “A comparison of methods
for multiclass support vector machines”, IEEE Trans.
on Neu. Net., 13(2), 2002, pp.415-425.
[23] N. Otsu,“A Threshold Selection Method from Gray-
Level Histograms”, IEEE Trans. on Sys., Man, and
Cyber, 9(1), 1979, pp.62-66.
[24] C.C. Chang, and C.J. Lin, “LIBSVM : a library for
support vector machines”, Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm, (2001).
