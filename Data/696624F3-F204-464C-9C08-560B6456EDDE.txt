phrases are essentially defined in terms of two 
languages. Such phrases might not respect the 
individual languages very well. Some specific phrase 
pairs and phrases might then be induced. 
Such a huge and noisy phrase translation table is 
likely to introduce estimation errors when estimating 
the phrase translation probability as well as 
searching (decoding) errors during the training and 
decoding phases. The large search space might also 
degrade the speed of the decoding process. To improve 
the performance of the current phrase-based SMT, it 
is thus necessary to optimize the phrase segmentation 
as well as phrase alignment models by jointly 
considering the results of word alignment and a non-
heuristic model for phrase segmentation. By doing 
this, it might significantly improve the quality and 
speed of the decoding process and thus the 
translation fluency. 
In particular, an EM algorithm is proposed to conduct 
phrase segmentation for the source and target 
language corpora, respectively, independent of each 
other. The phrase alignment algorithm is then applied 
to such well-segmented phrases, with good estimates 
for phrase translation probabilities, which are based 
on the word alignment statistics. Jointly using the 
word alignment and phrase segmentation results 
quantitatively, instead of heuristically, to produce 
a quality phrase translation table and their 
translation probability is thus possible. 
 
英文關鍵詞： Phrase Alignment, Word Alignment, Phrase 
Segmentation, Phrase-Based Statistical Machine 
Translation System, Phrase-Based Decoding 
 
精簡報告 共 10 頁 第 2 頁
以最佳化之單語詞組分斷法及雙語詞彙對應法為本的詞組對
應模式
A Phrase Alignment Model Based on Optimized Monolingual Phrase
Segmentation and Bilingual Word Alignment
計畫編號：NSC 99-2221-E-260-031
執行期限：2010 年 08 月 01 日至 2011 年 12 月 31 日
主持人：張景新 (jshin@csie.ncnu.edu.tw)
執行機構及單位名稱: 國立暨南國際大學資訊工程學系
計畫參與人員：張景新，蕭燿晟、林子翔、王民謙、彭維剛
摘 要
目前效能最佳的統計式機器翻譯
系 統 (statistical machine
translation systems) 屬於詞組為
本的系統 (phrase-based SMT). 而
其核心成份, 則是一個詞組翻譯表
(phrase translation table). 多數
所謂的詞組, 是由詞彙對應 (word
alignment) 的結果衍生得來的; 衍
生方式多數倚賴某些直覺, 來找出一
些與詞彙對應結果“一致”的詞組
對 (phrase pairs). 因此, 深受詞
彙對應之正確率, 及用來判斷所謂
“一致性” 的直覺所影響. 由於
缺 乏 客 觀 的 詞 組 分 斷 (phrase
segmentation) 標準, 大量與詞彙對
應 “一致” 但雜亂的詞組對就此
產生. 同時, 這樣的詞組其實是由兩
種語言共同決定的, 因此, 未必充分
遵循個別語言的詞組結構. 有些奇特
的詞組對及詞組, 就可能因而衍生.
如此龐大而雜亂的詞組翻譯表,
極可能在估測翻譯機率時, 引入估測
誤差. 並在解碼的過程中, 引入搜尋
(解碼) 誤差. 龐大的搜尋空間也可
能導致解碼的速度惡化. 若想進一步
改善目前 PBSMT 的效能, 就有必要對
詞組分斷及詞組對應等模式進行最
佳化處理, 一併考慮詞彙對應的結
果, 及使用非直覺式的詞組分斷模式.
如此, 解碼的品質及速度都有可能顯
著提升從而明顯改善翻譯的流暢度.
為此, 本文特別提出一個 EM 演
算法, 分別對來源語及目標語個別做
最佳的詞組分斷處理, 而不受另一語
言的干擾. 之後, 再針對這些斷好的
詞組, 以一個詞組對應模型來找出最
好的詞組配對. 如此一來, 不依靠直
覺, 而是同時定量使用詞彙對應及詞
組分斷處理的結果, 來產生高品質的
詞組翻譯表及其機率, 就有可能實
現。
關鍵詞: 詞組對應, 詞彙對應, 詞組分斷, 詞
組為本之統計式機器翻譯系統, 詞組為本之
解碼程序
Abstract
The phrase translation table is the
core model component of the
state-of-the-art phrase-based statistical
machine translation (SMT) systems.
Most phrases are induced from word
alignment results by using some
heuristics to find phrase pairs that are
“consistent” with the word alignment 
results. The phrase translation table is
thus affected by the word alignment
accuracy as well as the heuristics to
find consistent phrase pairs. Without an
精簡報告 共 10 頁 第 4 頁
Figure 1. Learning phrase pairs from word
alignment results using“consistency”heuristics.
[Kohen 05]
1.3 Problems with Word-Alignment
Induced Phrases
The phrase translation table constructed in
this way is thus significantly affected by the
heuristics to find consistent phrase pairs as well
as the word alignment accuracy. There is no
quantitative justification for such phrase pairs,
and there is also no overall optimization criteria
to tell how good to express individual sentences
in terms of these phrases. Without an objective
optimization criterion for phrase segmentation,
as seen above, a large number of consistent yet
noisy phrase pairs may be generated.
Furthermore, the phrases are essentially
defined in terms of two languages. Such
phrases might not respect the individual
languages very well. Some specific phrase pairs
and phrases might be induced from a bilingual
corpus of limited size simply because the other
language has adjacent words or phrases that
would not generate inconsistent phrase pair.
Such a huge and noisy phrase translation
table is likely to introduce estimation errors for
estimating the phrase translation probability
during the training process since part of the
probability mass will be discounted from
normal phrase patterns and assigned to the large
amount of noisy phrases. It may also introduce
significant searching (decoding) errors during
the decoding process of a phrase-based SMT
since many possible hypotheses will be created
from the noisy phrase pairs. The large search
space might also degrade the speed of the
decoding process in addition to introducing
search errors. These cases include some
equivalent yet wasteful hypotheses, like ‘did
not’plus‘give’versus‘did not give’.
1.4 Optimizing Phrase Segmentation
To reduce the phrase table and thus the cost
in decoding, one might want to define phrases
using a quantitative optimization function,
instead of some heuristic criterion such as
“consistency”. It is also desirable to define
phrases in terms of the individual language,
instead of defining the phrases based on two
languages. The reasons are many folds.
First of all, the phrases are supposed to
generate fluent translation hypotheses that
respect the phrase structures of a single
language (namely the target language). In other
words, it is a good idea to define phrases that
would maximize the language model of a single
language (i.e., the target language). Bilingual
corpora, on the other hand, are useful for train
word pairs that carry the same “meanings”, but
not the syntax in another language. In other
words, the best use of bilingual corpora might
be the training of the cross-lingual parameters
that maximize the translation model of an
SMT.
Secondly, bilingual corpora are rare in
comparison to monolingual corpora. Therefore,
inducing phrases from bilingual corpora may
not only introduce noisy phrase pairs but also
suffer from OOV (out-of-vocabulary) phrases,
which are not seen from the smaller bilingual
corpora. A phrase-based SMT with unknown
phrases will not able to match the input source
sentence or generate candidate hypotheses
using (bilingually) unseen phrases. Either way
will result in unsatisfactory translation.
Therefore, we propose to pre-segment the
source and target language sentences into
phrases that maximize their respective language
models. This can be easily done with a simple
phrase-based n-gram model quite efficiently
without supervision. Such basic phrases can
then be used in conjunction with word
精簡報告 共 10 頁 第 6 頁
to the input tokens and ‘words’ as the
segmented tokens, as opposed to using ‘word’
and ‘phrases’ respectively in the phrase
segmentation task. The segmentation
optimization task can then be formulated as
follows.
 
   
  
 
 
1
1 1
1 1
1
*
1 1 1
1 1
, | 1,
1
( ) arg max ( | )
arg max ( ) arg max log ( )
arg max log ( )
( ) max log ( )
n
n n
n k n
n k
n k
n m n
S S c
S S c S S c w Sw S
n k n
m n k
S S c c k L
n k
S S c w S
S c P S w c
P w P w
c P w c
c P w






  


 

 
 
 
  



Eq. (1)
In equation (1) we have the following
notations.
,
1 1
1
1
*
1
: input characters ( , , )
: segmented words
: a generic segment
L: maximum length of a segment (in characters)
( ) : all segmentations expandable from input
( ) : the best segmentation for
n
n
m
n
n
c c c
w
S
S c
S c
 
 
1
the first n characters
. , : all segmentations derived by concatenating . with string
( ) : probability of the best segmentation for the first k charactersk
S s S s
c
The first line in equation (1) means to find
the best segmentation as the one with the
highest score defined by the product of
individual words. The second means to use the
sum of log probabilities instead. With some
mathematical manipulation to factor out
common log probability factors, we have the
line before the last line, which indicates that the
best score of a string of n characters can be
computed by adding the maximum partial score
for the prefix string of n-k characters plus a
local score corresponding to the log probability
of the segment expanded by the last k
characters. We therefore have the following
recursive relationship for computing the score
of the best segmentation associated with a
longer string of length i, in terms of the best
scores of shorter strings of length i-k (K=1,L)
and the log probability for the last k characters.
 1 11,( ) ( ) max log ( )
(0) 0
i i
m i kk L
i c i k P w c  


    

Eq. (2)
Given a set of unigram model parameters
representing the (log) probabilities of
individual segments, the best segmentation can
then be found by scanning from left to right and
at each segmentation boundary, i, compute the
partial maximum score for a prefix string, and
memorize with length of the last word, k, leads
to the best segmentation. Upon completion, we
trace back from the rightmost boundary, and
skipping leftward by the number of characters,
k, that leads to the best segmentation. With
such a dynamic programming technique, it is
unnecessary to really expand all segmentation
patterns for finding the best one.
The probability P(w) must be estimated in
an unsupervised manner in the current case,
since we normally do not have a large
segmented text for supervised training. This
can be done by using an EM algorithm to
estimate the expected count of a word segment,
and then normalizing the expected count by the
sum of all expected counts of all possible
word-segments. The expected count for a word
segment is in proportional to the probability of
the segmentation“path”where it appears; if the
word segment appears multiple times in
multiple paths, the total expected count can be
estimated by summing over all such “paths”
(i.e., segmentation patterns) that have the two
ends of the word as two of the best
segmentation boundaries. The following
formulation highlights this idea.
 1 1
1 1
1
: ( | ) ( )
ˆ: ( ) ( | ) ( )
(ˆ )
: ( )
(ˆ )
n n
i
m n
w S
n
i
w SSent c S S c
w
Model P S w c P w
E Step c w P S c w w
c w
M Step P w
c w


 
 
   
 

 

Eq. (3)
精簡報告 共 10 頁 第 8 頁
3 Experiments
3.1 Experiment Settings
To evaluate the effects of EM-based phrase
segmentation over the size of translation pairs
and the translation quality, in BLEU score
[Papineni 02], a few experiments are
conducted.
The dev2006 sub-corpus from the Third
Workshop on Statistical Machine Translation
(http://www.statmt.org/wmt08/) of the ACL
2008 conference is used for this purpose. It
includes a parallel corpus of 5 languages; each
language contains 2000 sentences. The English
and French parts of this corpus have more than
53K and 55K words, respectively; they are
independently phrase segmented using the
proposed EM algorithm for phrase-based
unigram model. The phrase-segmented phrases
are then symbolized as phrase-based tokens.
Such tokens are then aligned using the Giza++
package as if they were word-based tokens.
Eventually, we have phrase-aligned translation
pairs and their phrase translation probabilities.
Such a translation model is then used in Moses
decoder to evaluate its translation performance,
in comparison with the performance of the
native Moses decoder, which is based on
word-alignment induced phrase translation
model.
Since the maximum phrase length is a major
parameter for phrase segmentation as well as
Moses decoding, the maximum phrase lengths
of 4, 8, 12, 16 and 20 words are used in the
experiments. Furthermore, to know whether
less frequently occurred phrases have
significant impact over the translation
performance, the cutoff frequencies of 1~5
words are considered. This means that, in
different EM-based phrase segmentation
sub-models, the converged probability of a
phrase is reset to zero if it occurs less than the
frequency threshold. The special case with
cutoff frequency 1 therefore means that there is
nothing intentionally made unknown.
3.2 Summary of Results
The following tables and their figures
summarize some interesting results of a few
experiments.
Table 1. Number of phrase pairs as a
function of maximum phrase length.
(T: cutoff frequency threshold)
Table 1 indicates that both native Moses and
EM phrase segmentation produce more and
more phrase pairs at different speed as the
maximum phrase length increases. As expected,
EM phrase segmentation produce much smaller
number of phrase pairs than native Moses.
4 8 12 16 20
Moses 36122 58149 64979 65563 65646
T=1 3952 15483 26795 36014 43352
T=2 3808 12014 19741 26302 31692
T=3 3758 11189 17936 23588 28058
T=4 3680 10907 17530 22976 27431
T=5 534 1216 1734 2115 2400
精簡報告 共 10 頁 第 10 頁
Bibliography (參考文獻)
[Brown 90] Brown, Peter F., J. Cocke, Stephen
A. Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty, Robert L.
Mercer, and Paul S. Roossin. 1990. “A 
statistical approach to machine translation.”
Computational Linguistics, 16(2):79–85.
[Brown 93] Brown, Peter F., Stephen A. Della
Pietra, Vincent J. Della Pietra, and R. L.
Mercer. 1993. “The mathematics of 
statistical machine translation: Parameter
estimation.” Computational Linguistics,
19(2):263–311.
[Chiang 92] Tung-Hui Chiang, Jing-Shin
Chang, Ming-Yu Lin and Keh-Yih Su,
"Statistical Models for Word Segmentation
and Unknown Word Resolution,"
Proceedings of ROCLING-V, pp. 123-146,
Taipei, Taiwan, R.O.C., 1992.
[Chiang 05] Chiang, David, 2005. “A 
Hierarchical Phrase-Based Model for
Statistical Machine Translation,” Proc.
ACL-2005, pages 263–270, 2005.
[Kohen 05] Tutorial course slides, ESSLLI
2005.
[Lin 93] Lin, Ming-Yu, Tung-Hui Chiang and
Keh-Yih Su, "A Preliminary Study on
Unknown Word Problem in Chinese Word
Segmentation," Proceedings of ROCLING VI,
pp. 119-142, 1993.
[Och 99] Franz Josef Och, Christoph Tillmann,
and Hermann Ney, “Improved Alignment
Models for Statistical Machine Translation,”
in Proc. EMNLP/WVLC, 1999
[Och 00a] Och, Franz J. and Hermann Ney.
2000. “A comparison of alignment models
for statistical machine translation.” In 
COLING ’00: The 18th International 
Conference on Computational Linguistics,
pages 1086–1090, Saarbr ¨ ucken, Germany,
August.
[Och 00b] Och, Franz Josef and Hermann Ney.
2000. “Improved statistical alignment
models.” In Proceedings of the 38thAnnual
Meeting of the ACL, pages 440–447.
[Och 04] Och, Franz Josef and Hermann Ney.
2004. “The alignmenttemplate approach to
statistical machine translation.”
Computational Linguistics, 30:417–449.
[Papineni 02] Papineni, K., Roukos, S., Ward,
T., and Zhu, W. J. (2002). "BLEU: a method
for automatic evaluation of machine
translation" in Proceedings of ACL-2002,
40th Annual Meeting of the Association for
Computational Linguistics pp. 311--318.
[Shen 06] Shen, Wade, Brian Delaney and Tim
Anderson, 2006. “The MIT-LL/AFRL
IWSLT-2006 MT System,” Proc. of the
International Workshop on Spoken Language
Translation (IWSLT) 2006, pp. 71-76, Kyoto,
Japan, 27 November 2006.
[Sproat 03] Sproat, Richard and Thomas
Emerson. “The First International Chinese 
Word Segmentation Bakeof”, 2003.
(http://www.sighan.org/bakeoff2003/bakeoff
_instr.html)
[Zhou 04] Zhou, Yu, Chengqing Zong, and Bo
Xu, 2004. “Bilingual Chunk Alignment in 
Statistical Machine Translation,” In 
Proceedings of IEEE International
Conference on Systems, Man & Cybernetics
(SMCC2004), Hague, Netherlands, 2004.
2(參加會議經過、與會心得、考察參觀活動、建議、攜回資料名稱及內容、其他)
Conference: COLING-2010 (the 23rd International Conference on Computational Linguistics)
Date: 2010/08/23 ~ 08/27. Venus: BICC, Beijing, PRC.
The biennial conference COLING is one of the largest international conference on CL (Computational
Linguistics) and NLP (natural language processing). It was ranked the top-1 conference in these fields in
parallel with the annual ACL (Association for Computational Linguistics) meetings.
This year, COLING-2010 (the 23rd International Conference on Computational Linguistics) is organized
by the Chinese Information Processing Society of China (CIPS) and is held in the Beijing International
Convention Center (BICC), PRC, which is near the 2008 Olympic Games Center (known as the "Bird Nest")
on August 23-27, under the auspices of the International Committee on Computational Linguistics (ICCL).
COLING-2010 covered a broad spectrum of technical areas related to natural language and
computational linguistics. The conference included sessions for full papers, oral presentations, poster
presentations, demonstrations, tutorials, and workshops. Original and unpublished research papers on all
aspects of computational linguistics were published in its conference proceedings (in CDROM format). It is
therefore a significant event of the NLP community in 2010.
For the main conference, the 155 oral papers included in the hardcopy proceedings published by
Tsinghua University Press, as well as the 334 papers included in the electronic proceedings (which have the
same 155 oral papers plus 179 poster papers) are selected from among 815 effective submissions among more
than 840 submissions received. The very selective acceptance rate of 19.02% for oral presentations (155/815
submissions) indicates the extremely high quality of the papers. An additional 21.96% (179/815) are selected
for poster presentations to bring the overall acceptance rate to 40.98% (334/815).
Professor Chu-Ren Huang (Academia Sinica, Taiwan) is proudly elected the COLING 2010 Program
Committee Co-chairs in corporation with Dr. Dan Jurafsky this year. There are also several professors from
Taiwan for presenting their papers in the conference. Partially under the sponsorship of the current project, I
was able to attend this conference, and the Executive Board meeting of the AFNLP (Asian Federation for
Natural Language Processing, the joint federation of major CL & NLP associations in Asian countries) during
the COLING conference.
The corpus-based and statistical based NLP researches, as usual, are still the main stream in the
conference and workshops. Chinese language processing is also a major field of research. A significant
number of Chinese NLP papers were submitted. Therefore, Chinese NLP is becoming an important future
stream in the NLP communities. In early years, papers from Taiwan stand for a significant percentage of the
submissions. However, we need to pay more efforts to keep our strength in such top-rank conference,
according to the current status of submissions. To be competitive against other international researchers, NLP
research works should be strongly supported in Taiwan.
99 年度專題研究計畫研究成果彙整表 
計畫主持人：張景新 計畫編號：99-2221-E-260-031- 
計畫名稱：以最佳化之單語詞組分斷法及雙語詞彙對應法為本的詞組對應模式 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 1 100% 
預計整理後,將完
整結果發表於國
內/國外(優先)相
關期刊. 
研究報告/技術報告 0 0 100%  
研討會論文 0 1 100% 
篇 
預計整理後,發表
精簡結果於國內/
國外(優先)之研
討會論文. 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 4 4 100% 
經由本計劃, 訓
練多名碩士研究
生獨立研究之能
力, 已有具體成
效. 
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 1 100% 
預計整理後,將完
整結果發表於國
內/國外(優先)相
關期刊. 
研究報告/技術報告 0 0 100%  
研討會論文 0 1 100% 
篇 
預計整理後,發表
精簡結果於國內/
國外(優先)之研
討會論文. 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
國外 
技術移轉 件數 0 0 100% 件  
