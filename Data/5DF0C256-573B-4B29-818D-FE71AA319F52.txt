 附件一 
行政院國家科學委員會補助專題研究計畫 ■ 成 果 報 告   □期中進度報告 
 
高效能固態硬碟管理方法暨存取策略 
 
計畫類別：■個別型計畫   □整合型計畫 
計畫編號：98-2221-E-009-157-MY3 
執行期間：  100 年 8 月 1  日至 101  年 7 月 30  日 
 
執行機構及系所： 國立交通大學資訊工程系 
 
計畫主持人：張立平 
共同主持人： 
計畫參與人員：毛超遠，黃鼎傑，黃聖閔，洪政猷 
 
 
 
成果報告類型(依經費核定清單規定繳交)：□精簡報告  ■完整報告 
 
本計畫除繳交成果報告外，另須繳交以下出國心得報告： 
□赴國外出差或研習心得報告 
□赴大陸地區出差或研習心得報告 
■出席國際學術會議心得報告 
□國際合作研究計畫國外研究報告 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
           □涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
中   華   民   國   101 年  10  月 24   日 
 
行政院國家科學委員會專題研究計畫成果報告 
 國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
□ ■達成目標 
□ 未達成目標（請說明，以 100 字為限） 
□ 實驗失敗 
□ 因故實驗中斷 
□ 其他原因 
說明： 
 
 
 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 □無 
技轉：□已技轉 □洽談中 □無 
其他：（以 100 字為限） 
 
本計劃之學術成果包括三篇國際會議論文以及兩篇國際期刊論文。其中包括頂尖
的國際會議 Design Automation Conference 以及頂尖的國際期刊 ACM 
Transactions on Embedded Computing Systems。 
附件二 
 3
AFinal Report of 98-2221-E-009-157-MY3:
Data management and access policies for high-performance
solid-state disks
Principle Investigator: Li-Pin Chang, National Chiao-Tung University
Solid-state disks had succeeded the past three years in the market of consumer electronics, personal com-
puting, and enterprise computing. There are two future directions of flash storage devices: consumer-level
storage cards and high-performance solid-state disks. This project aims at the performance and lifetime
issues of solid-state disks. In the duration of this three-year work, we have successfully developed 1) a
write-buffer management algorithm, 2) a channel management algorithm, and 3) a wear-leveling algorithm
for high-end flash storage. This final report will be focused on the third year result, i.e., the wear-leveling
algorithm. For the results of the first two year work, please refer to the mid-term reports.
Additional Key Words and Phrases: Solid-state disks, flash memory, write buffer, multichannel architec-
tures, wear leveling.
1. INTRODUCTION
Solid-state disks employ flash memory as their storage medium. The physical charac-
teristics of flash memory differ from those of hard drives, necessitating new methods
for data accessing. Solid-state disks hide flash memory from host systems by emulating
a collection of logical sectors, allowing systems to switch from a hard drive to a solid-
state disk without modifying any existing software and hardware. Solid-state disks are
superior to traditional hard drives in terms of shock resistance, energy conservation,
random-access performance, and heat dissipation, attracting vendors to deploy such
storage devices in laptops, smart phones, and portable media players.
Flash memory is a kind of erase-before-write memory. Because any one part of flash
memory can only withstand a limited number of write-erase cycles, approximately
100K cycles under the current technology [Samsung Electronics 2006], frequent erase
operations can prematurely retire a region in flash memory. This limitation affects
the lifetime of solid-state disks in applications such as laptops and desktop PCs, which
write disks at very high frequencies. Even worse, recent advances in flash manufactur-
ing technologies exaggerate this lifetime issue. In an attempt to break the entry-cost
barrier, modern flash devices now use multilevel cells for double or even triple density.
Compared to standard single-level-cell flash, multilevel-cell flash degrades the erase
endurance by one or two orders of magnitude [Samsung Electronics 2008].
Without wear leveling, localities of data access inevitably degrade wear evenness
of flash memory in solid-state disks. Partially wearing out a piece of flash memory
not only decreases its total effective capacity, but also increases the frequency of flash
erase for free-space management, which further speeds up the wearing out of the rest
of the flash memory. A solid-state drive ceases to function when the amount of its
worn-out space in flash exceeds what the drive can manage. Wear-leveling techniques
ensure that the entire flash wears evenly, postponing the first appearance of a worn-
out memory region. However, wear leveling is not free, as it moves data around in flash
to prevent solid-state disks from excessively wearing any one part of the memory. As
reported in [Chang et al. 2010], these extra data movements can increase the total
number of erase operations by ten percent.
Wear-leveling algorithms include rules defining when data movement is necessary
and where the data to move to/from. These rules monitor wear in the entire flash, and
intervene when the flash wear develops unbalanced. Wear-leveling algorithms are part
of the firmware of solid-state disks, and thus they are subject to crucial resource con-
straints of RAM space and execution speeds of solid-state disks’ microcontrollers (or
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:3
because it does not perform proactive data movement in contrast to the static wear-
leveling algorithm, and it tries not to intervene in flash wear unless wear leveling is
cost effective.
Modern solid-state disks equip with multiple channels for parallel flash opera-
tions. In this study, a channel refers to a logical unit that independently processes
flash commands and transfers data. Multichannel designs boost the write through-
put but introduce unbalanced wear of flash erase units among channels. Prior work
address this issue by dispatching write requests to channels on a page-by-page basis
[Chang and Kuo 2002; Dirik and Jacob 2009] (a page is the smallest read/write unit of
flash). Dispatching data at the page level requires page-level mapping, whose imple-
mentation requires considerable RAM space for large flash. Additionally, this approach
could map logically consecutive data to the same channel and degrade the channel-
level parallelism in sequential read requests. This study introduces a novel channel-
level wear leveling strategy based on the concept of reaching “eventually even” channel
lifetimes. The basic idea is to align channels’ lifetime expectancies by re-mapping data
among channels. The proposed approach has many benefits, including 1) it does not
require a channel-level threshold for wear leveling, 2) it incurs very limited overhead,
and 3) it requires only a small RAM-resident data structure.
In summary, this study has the following contributions:
1. An efficient block wear-leveling algorithm with a tiny RAM footprint.
2. A dynamic threshold-adjusting strategy for block wear leveling.
3. An algorithm for wear leveling at the channel level.
The rest of this paper is organized as follows: Section 2 reviews flash characteristics
and prior work on flash translation and wear leveling. Section 3 presents an block-
level wear-leveling algorithm, and Section 4 describes an adaptive tuning strategy for
this algorithm. Section 5 introduces a strategy for wear leveling at the channel level.
Section 7 concludes this paper.
2. PROBLEM FORMULATION
2.1. Flash Management
2.1.1. Flash-Memory Characteristics. Solid-state disks use NAND flash memory (flash
memory for short) as their storage medium. A piece of flash memory is a physical ar-
ray of blocks, and each block contains the same number of pages. Typically a flash page
is of 2048 plus 64 bytes. The 2048-byte portion stores user data, while the 64 bytes is
a spare area for mapping information, block aging information, error-correcting code,
etc. Flash memory reads and writes in terms of pages, and overwriting a page requires
erasing. Flash erases in terms of blocks, each of which consists of 64 pages. Under
the current technology, a flash block can only sustain a limited number of write-erase
cycles before it becomes unreliable. A single-level-cell flash block endures 100K cy-
cles [Samsung Electronics 2006], while this limit is 10K or less in multilevel-cell flash
[Samsung Electronics 2008].
Solid-state disks emulate disk geometry using a firmware layer called the flash-
translation layer (FTL). FTLs update existing data out of place and invalidate old
copies of the data to avoid erasing a flash block every time before rewriting a piece
of data. Thus, FTLs require a mapping scheme to translate disk sector numbers into
physical flash addresses. Updating data out of place consumes free space in flash, and
FTLs must recycle flash space occupied by invalid data with erase operations. Before
erasing a block, FTLs copy all valid data from this block to other free space. Garbage
collection refers to a series of copy and erase operations for reclaiming free space.
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:5
Table I. Comparison of existing algorithms for block-level wear leveling.
Algorithm Principle RAM-resident data structures re-
quired
Threshold
tuning
Static wear leveling
[Chang et al. 2010]
Static wear leveling A block erase bitmap Manual
Group wear leveling
[Jung et al. 2007]
Hot-cold swapping Average erase counts of block groups Manual
Dual-pool
wear leveling
[Chang and Du 2009]
Cold-data migration All blocks’ erase counts and their re-
cent erase counts
Manual
Remaining-
lifetime leveling
[Agrawal et al. 2008]
Cold-data migration All blocks’ age information (remain-
ing lifetimes) and block-data temper-
ature (update frequencies)
Manual
Lazy wear leveling
(this study)
Cold-data migration An average erase count of all blocks Automatic
block. For each of the found logical block, the FTL collects valid data from the log block
and the data block of this logical block, copies these valid data to a new spare block,
and re-maps the logical block to the copy-destination spare block. Finally, the FTL
erases all the involved data blocks and the log blocks into spare blocks. This procedure
is referred to as merge operations or garbage collection. For example, in Fig. 1(a), for
garbage collection the FTL collects the valid data scattered in the data blocks at pbns
0 and 2 and in the log blocks at pbns 6 and 3, write them to the spare blocks at pbns 7
and 8, and then erases the four old flash blocks at pbns 0, 2, 6, and 3 into spare blocks.
Hybrid mapping FTLs exhibit some common behaviors in the garbage-collection pro-
cess regardless of their designs, i.e., garbage collection never involves a data block if
none of its page data have been updated. In Fig. 1(a), erasing the data blocks at pbn 5
cannot reclaim any free space. Similarly, in Fig. 1(b), erasing any of the log blocks does
not involve the data block at pbn 5. This is a potential cause of uneven flash wear.
2.2. The Need for Wear Leveling
This section first introduces prior methods, discusses their drawbacks, and then point
out how the method to be proposed improves upon these shortcomings.
2.2.1. Block-Level Wear Leveling. Block-level wear leveling considers the wear evenness
of a collection of flash blocks. Let the erase count of a flash block denote how many
write-erase cycles this block has undergone. There have been three representative
techniques for this problem: Static wear leveling, Hot-cold swapping, and Cold-data
migration. Static wear leveling moves static/immutable data away from lesser worn
flash blocks, encouraging the flash-translation layer to start erasing these blocks.
Flash vendors including Micron [Micron R© 2008] and Spansion [Spansion R© 2008] rec-
ommend using this approach. Chang et al. [Chang et al. 2010] described a design of
Static wear leveling. However, Chang and Du [Chang and Du 2009] found Static wear
leveling failed to achieve even block wear on the long-term, because Static wear lev-
eling could 1) move static/immutable data back and forth among lesser worn blocks
and 2) erase a flash block even if its erase count is relatively large. Hot-cold swap-
ping exchanges data in a lesser worn block with data from a badly worn block. Jung
et al. [Jung et al. 2007] presented a hot-cold swapping design. However, because the
oldest block has a very large (and perhaps still the largest) erase count, Chang and Du
[Chang and Du 2009] found that Hot-cold swapping risks erasing the most worn flash
block pathologically.
Cold-data migration relocates infrequently updated data (i.e., cold data) to exces-
sively worn blocks to protect these blocks against garbage collection. Preventing badly-
worn blocks from aging further is not equal to increasing the wear of lesser-worn blocks
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:7
[Dirik and Jacob 2009] proposed allocating channels to incoming page write requests
using the round-robin policy. Even though dynamic channel binding has better flexi-
bility of balancing the block wear across all channels, it has two drawbacks: 1) it adds
extra channel-level mapping information to every logical page, resulting in larger map-
ping tables and 2) it could map consecutive logical pages to the same channel, severely
degrading the channel-level parallelism in sequential-read requests.
Instead of dynamic channel binding, this study considers static channel binding.
Static channel binding uses fixed mapping between logical pages and channels. With
static mapping, effectively every channel manages its free flash pages with its own in-
stance of flash-translation layer. The most common strategy for static channel binding
is the RAID-0-style striping [Agrawal et al. 2008; Park et al. 2010; Seong et al. 2010].
RAID-0 striping achieves the maximum channel-level parallelism in sequential read
because it maps a collection of consecutive logical pages to the largest number of chan-
nels. We must point out that RAID-0 striping cannot automatically achieve wear lev-
eling at the channel level. This is because, as reported in [Chang 2010], hot data (fre-
quently updated data) are small, usually between 4 KB and 16 KB. RAID-0 striping
statically binds small and hot data to some particular channels, resulting in imbal-
anced write traffics among channels. We found that, under the disk workload of a
Windows desktop, a four-channel architecture had a largest and a smallest fractions
of channel-write traffic of 28% and 23%, respectively. Thus, flash blocks from different
channels wear at different rates. Extending the scope of block-level wear leveling to
the entire storage device is not a feasible solution here, because it requires dynamic
channel binding.
3. BLOCK-LEVEL WEAR LEVELING
This section presents an algorithm for wear leveling at the block level. This algorithm
does not deal with channels so logically all flash blocks are in the same channel.
3.1. Observations
This section defines some key terms for the purpose of presenting our wear-leveling
algorithm in later sections. Let the update recency of a logical block denote the time
length between the current time and the latest update to this logical block. The update
recency of a logical block is high if its latest update is more recent recent than the
average update recency. Otherwise, its update recency is low. Analogously, let the erase
recency of a physical block be the time length since the latest erase operation on this
block. Thus, immediately after garbage collection erases a physical block, this block
has the highest erase recency. A physical block is an senior block if its erase count is
larger than the average erase count. Otherwise, it is a junior block.
Temporal localities of updating logical blocks affect the wear of physical blocks. As
previously mentioned, if a physical block is mapped to an unmodified logical block,
then garbage collection will avoid erasing this physical block. On the other hand, up-
dates to logical blocks produce invalid data in flash blocks, and thus physical blocks
mapped to recently modified logical blocks are good candidates for garbage collection.
After a physical block is erased by garbage collection, it either serves a data block or
a log block. Either way, this physical block is again related to recently modified logical
blocks. So if a physical block has a high erase recency, then it will quickly accumulate
many erase counts. Conversely, physical blocks lose momentum in increasing their
erase counts if they are mapped to logical blocks having low update recency.
Figure 2 provides an example of eight physical blocks’ erase recency and erase
counts. Upward arrows mark physical blocks recently increasing their erase counts,
while an equal sign indicates otherwise. Block a is a senior block with a high erase
recency, while block d is a senior block but has a low erase recency. The junior block
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:9
Algorithm 1 The lazy wear-leveling algorithm
Input: v: the victim block for garbage collection
Output: p: a substitute for the original victim block v
1: ev←eraseCount(v)
2: if (ev − eavg) > ∆ then
3: repeat
4: l← lbnNext()
5: until lbnHasPageMapping(l)=FALSE
6: erase(v);
7: p← pbn(l)
8: copy(v, p); map(v, l)
9: ev ← ev + 1
10: eavg ← updateAverage(eavg, ev)
11: else
12: p← v
13: end if
14: RETURN p
wear leveling will be notified that a senior block is again selected as a victim of garbage
collection, and will perform another re-mapping operation for this senior block.
3.3. Interacting with Flash-Translation Layers
This section describes how Lazy wear leveling interacts with its accompanying
firmware module, the flash-translation layer. Algorithm 1 shows the pseudo code of
Lazy wear leveling. The flash-translation layer calls Algorithm 1 after it moves all
valid data out of a garbage-collection victim block and before it erases this block. The
input of Algorithm 1 is v, the pbn of the victim block. This algorithm performs re-
mapping whenever necessary, and then returns a pbn. Note that this output pbn may
be different from the input pbn. The flash-translation layer erases the flash block at
the pbn returned by Algorithm 1. The discussion in this section is based on hybrid
mapping. See later sections for using Lazy wear leveling with page-level mapping.
For the example of SAST in Fig. 1(a), suppose that the flash-translation layer decides
to merge data of the logical blocks at lbns 0 and 1. The flash-translation layer calls
Algorithm 1 before erasing each of the four physical blocks at pbns 0, 2, 6, and 3.
For the example of FAST in Fig. 1(b), because FAST recycles the oldest log block at a
time, the flash-translation layer calls Algorithm 1 before erasing the log block at pbn 6
and the two related data blocks at pbns 0 and 2. The rest of this section is a detailed
explanation of Algorithm 1.
In Algorithm 1, the flash-translation layer provides the subroutines with leading
underscores, and wear leveling implements the rest. In Step 1, eraseCount() obtains
the erase count ev of the victim block v by reading the victim block’s page spare area,
in which the flash-translation layer stores the erase count. Step 2 compares ev against
the average erase count eavg. If ev is larger than eavg by a predefined threshold ∆, then
Steps 3 through 10 will carry out a re-mapping operation. Otherwise, Steps 12 and
14 return the victim block v intact. The loop of Steps 3 through 5 finds a logical block
whose update recency is low. Step 4 uses the subroutine lbnNext() to obtain l the next
logical block number to visit, and Step 5 calls the subroutine lbnHasPageMapping() to
check if the logical block l has any related mapping information in the page-mapping
table. As mentioned previously, to give junior blocks equal chances of getting erased,
the subroutine lbnNext()must evenly visit all logical blocks. At this point, it is reason-
able to assume that lbnNext() produces a linear enumeration of all lbns.
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:11
FAST has a very large number of log blocks and the host frequently modifies a logical
block. On the one hand, FAST can indefinitely postpone merging this logical block.
On the other hand, Lazy wear leveling does not use this logical block for re-mapping
because its page updates keep leaving information in the page-mapping table. As a
result, the (flash) data blocks mapped to this logical block can never attract attention
from both garbage collection and wear leveling.
A simple enhancement based on the bitmap lbMod[] deals with this problem. When
FAST erases the oldest log block, for every piece of page data in this log block, regard-
less of whether it is valid or not, FAST finds the the logical block number of this logical
page and clears the corresponding bit in lbMod[], as if FAST did not delay merging
logical blocks. Note that SAST does not require this enhancement, because to improve
log-block space utilization SAST will not indefinitely delay merging logical blocks.
3.5. Lazy Wear Leveling and Page-Level Mapping
Although Lazy wear leveling is primarily designed for hybrid mapping, its concept
is applicable to page-level mapping. Like in hybrid mapping, in page-level mapping
Lazy wear leveling copies data having low update recency to senior blocks to prevent
these blocks from aging further. However, different from hybrid mapping, page-level
mapping does not use logical block [Gupta et al. 2009], so Lazy wear leveling needs a
different strategy to find data having low update recency.
This study proposes using an invalidation bitmap. In this bitmap, one bit is for a
flash block, and each bit indicates whether a flash block recently receives a page inval-
idation (i.e., 1) or not (i.e., 0). All the bits are 0 initially, and there is a pointer referring
to the first bit. The bit of a flash block switches to 1 if any page in this block is updated
(i.e., invalidated). Whenever Lazy wear leveling finds the erase count of a victim block
larger than the average by ∆, it advances the pointer and scans the bitmap. As the
pointer advances, it clears bits of 1’s until it encounters a bit of 0. Lazy wear leveling
then copies valid data from the flash block owning this zero bit to the victim block.
This scan-and-copy procedure repeats until it writes to all pages of the victim block.
Notice that garbage-collection activities do not alter any bits in the bitmap.
The rationale behind the design is that, in the presence of temporal localities of
write, if a flash block does not receive page invalidations recently, then this block is
unlikely to receive more page invalidations in the near future. The invalidation bitmap
resides in RAM, and it requires one bit per flash block. Compared to the page-level
mapping table, the space overhead of this bitmap is very limited.
4. SELF TUNING FOR BLOCK-LEVEL WEAR LEVELING
Lazy wear leveling subjects the evenness of block wear to a threshold parameter ∆. A
small value of ∆ targets even wear in flash blocks but increases the frequency of data
movement. This section presents a dynamic tuning strategy for ∆ for achieving good
balance between wear evenness and overhead.
4.1. Overhead Analysis
Consider a piece of flash memory consisting of nb physical blocks. Let immutable log-
ical blocks map to nbc out of these nb physical blocks. Let the sizes of write requests
be multiples of the block size, and let write requests be aligned to block boundaries.
Suppose that the disk workload uniformly writes the mutable logical blocks. In other
words, the flash-translation layer evenly increases the erase counts of the nbh=nb−nbc
physical blocks.
Let the function f(x) denote how many blocks garbage collection erases to process a
workload that write x logical blocks. Consider the case x = i × nbh × ∆, where i is a
non-negative integer. As all request sizes are multiples of the block size and requests
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:13
Values of Δ
O
v
e
r
h
e
a
d
 r
a
t
io
s
g(Δ)=K/(2Δ)
Δcur Δnext


Solve K cur  using Δcur and (g Δcur)
Find Δnext at which the tangent slope to
(g Δnext)=K cur (2/ Δnext)  is λ.

tangent slope=λ(g Δcur)
Fig. 4. Computing∆next subject to the overhead growth limit λ for the next session, according to∆cur and
the overhead ratio g(∆cur) of the current session.
the average and the smallest can be accounted by a constant ratio, which is further
included in the runtime-measurable coefficient K. Thus, we have
g(∆) =
K
2∆
. (1)
When ∆ is small, a further decrease in ∆ rapidly increases the overhead ratio. For
example, decreasing ∆ from 4 to 2 doubles the overhead ratio.
4.2. A Strategy of Tuning ∆
Small ∆ values are always preferred in terms of wear evenness. However, decreasing
∆ value can cause an unexpectedly large increase in overhead. The rest of this section
introduces a ∆-tuning strategy based on the overhead growth rates.
Under realistic disk workloads, the coefficient K in g(∆) may vary over time. Thus,
wear leveling must first determine the coefficient K before using g(∆) for ∆-tuning.
This study proposes tuning ∆ on a session-by-session basis. A session refers to a time
interval in which Lazy wear leveling contributed a pre-defined number of erase counts.
Refer to this number as the session length. The basic idea is to findKcur of the current
session and use this value to find ∆next for the next session.
The first session begins with ∆=16 (in theory it can be any number). Let ∆cur be
the ∆ value of the current session. Figure 4 illustrates the concept of the ∆-tuning
procedure. During a runtime session, Lazy wear leveling separately records the erase
counts contributed by garbage collection and wear leveling. At the end of the current
session, the first step (in Fig. 4) computes the overhead ratio f
′(x)−f(x)
f(x) , i.e., g(∆cur),
and solves Kcur of the current session using Equation 1, i.e., Kcur = 2∆cur × g(∆cur).
The second step uses g(∆next)=Kcur/(2∆next) to find∆next for the next session. Basi-
cally, Lazy wear leveling tries to decrease∆ until the growth rate of the overhead ratio
becomes equal to a user-defined limit λ. In other words, we are to find the ∆ value at
which the tangent slope to g(∆next) is λ. Let the unit of the overhead ratio be one per-
cent. Therefore, λ=-0.1 means that the overhead ratio increases from x% to (x+0.1)%
when decreasing ∆ from y to (y-1). Now solve dd∆g(∆next) =
λ
100 for the smallest ∆
value subject to λ. Rewriting this equation, we have
∆next =
√
100
−λ
√
g(∆cur)∆cur.
For example, when λ=-0.1, if the overhead ratio g(∆cur) and∆cur of the current session
are 2.1% and 16, respectively, then∆next for the next session is
√
100
0.1
√
2.1%× 16 = 18.3.
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:15
Table II. Symbol definitions.
Symbol Description
w The total amount of data written to the flash storage during [t−, t)
e¯ The write-erase cycle limit of flash blocks
nb The total number of flash blocks in a channel
y The total number of channels
Ci The i-th channel
eci The sum of all block erase counts in the channel Ci
uci The utilization of the channel Ci. Note that
∑
uci=1
u′ci The expected utilization of the channel Ci
ri The erase ratio of the channel Ci
x The total number of stripes
Si The i-th stripe
usi The utilization of the stripe Si. Note that
∑
usi=1
ui,j The utilization of the logical block at the stripe Si and the channel Cj
Note that
∑x−1
i=0 ui,j = ucj and
∑y−1
j=0 ui,j = usi
Fig. 6. Aligning the lifetime expectancies of two channels Ci and Cj for channel-level wear leveling. (a)
These two channels reach their end-of-life at different times. (b) Change channel utilizations uci and ucj to
u′ci and u
′
cj
, respectively, such that the lifetime difference becomes zero (i.e., d=0).
5.2. Aligning Channel Lifetime Expectancies
Provided that block wear leveling is effective, the erase counts of blocks in the same
channel will be close, and the wear of a channel can be indicated by the sum of all
block erase counts in this channel. Recall that the utilization of a channel stands for
the fraction of host data arriving at this channel. Even though data updates are out
of place at the block level, they do not change the mapping between logical pages and
channels, so temporal localities have affinity with channels. Thus, channel utilizations
do not abrupt change and the wear of channels increase at steady (but different) rates.
This study proposes adjusting channel utilizations to control the wear of channels
for an “eventually even” state of channel lifetimes. In other words, the idea is to project
channels’ lifetime expectancies to the same time point. Figure 6 is an example of two
channels Ci and Cj . Let every channel have the same total number of flash blocks nb.
Let a flash block endures e¯ write-erase cycles, and let the erase count of the channel
Ci, denoted by eci , be the sum of all block erase counts in this channel. Let a channel
reaches its end of life when its erase count becomes e¯×nb. Let t be the current time, and
let w be the total amount of host data written in the time interval [t−, t). Let uci ≤ 1 be
the utilization of the channel Ci. Thus, in this time interval the total amount of host
data arriving at the channel Ci is uciw. Let the erase counts of the channel Ci at time
t− and t be etci and e
t−
ci , respectively. Let the erase ratio of Ci during [t
−, t) be ri, defined
as ri =
etci
−et−ci
uciw
. As Fig. 6(a) shows, eci increases by riuciw = etci−et
−
ci in this time period.
Table II is a summary of symbols.
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:17
 
4000 4000 4500 3000
1.40 1.10 1.20 1.00
0.20 0.25 0.25 0.30
0.20 0.26 0.21 0.33
0.20 0.26 0.21 0.33
0.20 0.26 0.21 0.33
Fig. 7. Swapping logical blocks among channels for channel wear leveling. (a) Before the swap and (b) after
the swap.
Channel wear leveling should reduce the total number of logical blocks swapped.
We found that, in real workloads a stripe of a high utilization usually has two logical
blocks whose utilization difference is large. This is because frequently updated data
are small and they do not write to all channels [Chang 2010]. Thus, the swapping
begins with the stripe whose utilization is the highest. The following is a procedure to
find and swap a pair of logical blocks:
Step 1: Find the two channels Cm and Cnwhich have the largest positive value of
(ucm -u′cm) and the smallest negative value of (ucn-u
′
cn), respectively.
Step 2: Find the stripe Si subject to the following constraints:
(a) Si have the largest utilization among all stripes.
(b) In this stripe Si, the two logical blocks at Cm and Cn have not yet been swapped
in the current invocation of channel-level wear leveling.
(c) ui,m > ui,n and (ui,m − ui,n)≤min(ucm − u′cm ,|ucn − u′cn |).
Step 3: Exchange the channel mapping of the two logical blocks found in Step 2.
Step 4: Change ucm and ucn to (ucm -(ui,m-ui,n)) and (ucn+(ui,m-ui,n)), respectively.
Step 5: Swap ui,m and ui,n.
In each invocation, channel wear leveling repeats Steps 1 through 5 until 1) uci=u′ci
for every i or 2) the total number of logical blocks swapped is larger than a pre-defined
limitation. Figure 7 is an numeric example of channel wear leveling. In this example,
the channel lifetime limit e¯nb is 10,000. Figure 7(a) shows the initial data layout and
utilizations of logical blocks, channels, and stripes. Channel wear leveling solves the
expected channel utilizations using u′c3=
1.4×(10000−3000)
1.0×(10000−4000)=1.63u
′
c0 , u
′
c2 = 1.07u
′
c0 , u
′
c1 =
1.27u′c0 , and u
′
c3+u
′
c2+u
′
c1+u
′
c0 = 1. It then selects the stripe S0 whose utilization is the
highest, and swaps its two logical blocks at the channels C2 and C3. This swap changes
uc2 from 0.25 to 0.22 and and uc3 from 0.3 and 0.33. Next, channel wear leveling selects
the stripe S3 whose utilization is the second highest and swaps two more logical blocks.
Figures 7(b) shows the results after these swaps. The adjusted channel utilizations
match their expected utilizations.
This study proposes caching the utilization information of a small collection of most-
frequently written stripes. Our experiments will show that a small cache is sufficient
to effective channel wear leveling.
6. CONCLUSION
This study tackles three problems of wear leveling: block-level wear leveling, adaptive
tuning for block wear leveling, and channel-level wear leveling. Block-level wear lev-
, Vol. V, No. N, Article A, Publication date: January YYYY.
A:19
CHANG, L.-P. 2010. A hybrid approach to nand-flash-based solid-state disks. Computers, IEEE Transactions
on 59, 10, 1337 –1349.
CHANG, L.-P. AND DU, C.-D. 2009. Design and implementation of an efficient wear-leveling algorithm for
solid-state-disk microcontrollers. ACM Trans. Des. Autom. Electron. Syst. 15, 1, 1–36.
CHANG, L.-P. AND KUO, T.-W. 2002. An adaptive striping architecture for flash memory storage systems
of embedded systems. In 8th IEEE Real-Time and Embedded Technology and Applications Symposium.
187–196.
CHANG, Y.-H., HSIEH, J.-W., AND KUO, T.-W. 2010. Improving flash wear-leveling by proactively moving
static data. IEEE Transactions on Computers 59, 1, 53 –65.
DIRIK, C. AND JACOB, B. 2009. The performance of pc solid-state disks (ssds) as a function of bandwidth,
concurrency, device architecture, and system organization. In Proceedings of the 36th annual interna-
tional symposium on Computer architecture. ISCA ’09. ACM, New York, NY, USA, 279–289.
GLOBAL UNICHIP CORP. 2009. GP5086 Datasheet. http://www.globalunichip.com/4-10.php.
GUPTA, A., KIM, Y., AND URGAONKAR, B. 2009. Dftl: a flash translation layer employing demand-based
selective caching of page-level address mappings. In ASPLOS ’09: Proceeding of the 14th international
conference on Architectural support for programming languages and operating systems. ACM, 229–240.
JUNG, D., CHAE, Y.-H., JO, H., KIM, J.-S., AND LEE, J. 2007. A group-based wear-leveling algorithm
for large-capacity flash memory storage systems. In CASES ’07: Proceedings of the 2007 international
conference on Compilers, architecture, and synthesis for embedded systems. ACM, 160–164.
KANG, J.-U., KIM, J.-S., PARK, C., PARK, H., AND LEE, J. 2007. A multi-channel architecture for high-
performance NAND flash-based storage system. J. Syst. Archit. 53, 9, 644–658.
LEE, S.-W., PARK, D.-J., CHUNG, T.-S., LEE, D.-H., PARK, S., AND SONG, H.-J. 2007. A log buffer-
based flash translation layer using fully-associative sector translation. Trans. on Embedded Computing
Sys. 6, 3, 18.
MICRON R©. 2008. Wear-Leveling Techniques in NAND Flash Devices. Micron Application Note (TN-29-42).
PARK, C., CHEON, W., KANG, J., ROH, K., CHO, W., AND KIM, J.-S. 2008. A reconfigurable ftl architecture
for nand flash-based applications. ACM Trans. Embed. Comput. Syst. 7, 4, 1–23.
PARK, S.-H., PARK, J.-W., KIM, S.-D., AND WEEMS, C. C. 2010. A pattern adaptive nand flash memory
storage structure. IEEE Transactions on Computers 99, PrePrints.
ROSEN, K. 2003. Discrete mathematics and its applications. McGraw-Hill New York.
SAMSUNG ELECTRONICS. 2006. K9F8G08B0M 1Gb * 8 Bit SLC NAND Flash Memory. Data sheet.
SAMSUNG ELECTRONICS. 2008. K9MDG08U5M 4G * 8 Bit MLC NAND Flash Memory. Data sheet.
SEONG, Y. J., NAM, E. H., YOON, J. H., KIM, H., CHOI, J.-Y., LEE, S., BAE, Y. H., LEE, J., CHO, Y.,
AND MIN, S. L. 2010. Hydra: A block-mapped parallel flash memory solid-state disk architecture. IEEE
Transactions on Computers 59, 905–921.
SHANG, P., WANG, J., ZHU, H., AND GU, P. 2011. A new placement-ideal layout for multiway replication
storage system. Computers, IEEE Transactions on 60, 8, 1142 –1156.
SPANSION R©. 2008. Wear Leveling. Spansion Application Note (AN01).
, Vol. V, No. N, Article A, Publication date: January YYYY.
 2
今年的會議地點在德國的 Dresden。由於台灣沒有直飛 Dresden，因此我是由
瑞士轉機進入。Dresden 過去是東德的工業大鎮，而這一次選在 Dresden 舉辦更有
另一個意義：Global Foundry（中譯格羅方德），是從超微（Advanced Micro Devices，
AMD）spin off 出來的 IC 生產中心，就座落於 Dresden。因此，這次的 DATE 的 
Keynote 除了邀請到德國知名的 Bosche 公司之外來演說之外，亦邀請到了 Global 
Foundry 的高層來解說他們最新的 IC 製程技術。 
我在本次 DATE 發表一篇論文，論文主題是跟固態硬碟的韌體演算法有關。這
篇論文的動機是，目前的固態硬碟控制器之計算能力越來越強大，從過去使用 8051 
搭配 16KB DRAM 的解決方案，到現在已經演化到雙核心 ARM-based 處理器，搭配 128 
MB DRAM 的水準了。在過去，因為控制器的記憶體太小，無法負擔頁級快閃轉換層
所產生的巨大轉址表。而現在記憶體已經夠大，許多固態硬碟內部已經開始採用頁
級快閃轉換層，但是相對應用在頁級轉換之中的垃圾收集演算法，居然還停留在 1999
年由Chiang提出的DAC 以及1995 年由 Kawaguchi提出的 Cost-and-Benefit 演算
法。這兩個演算法都是基於數百 MB 的快閃記憶體來設計，其產生的資料結構負擔根
本不是目前動輒數百 GB 的固態硬碟可以實作的。本篇論文即提出一個適用於頁級轉
換的垃圾收集策略，其目標是能夠達成匹敵 DAC 的效能，但是卻僅使用非常非常少
的記憶體。 
這次論文發表時間是排在第一天 Keynote 之後的第一場。基本上報告的過程
是很順利，期間也遇到一些來自新加坡的學者，我們一起討論了一些非揮發性記憶
體的研究課題，並交換一些心得。不過，這次 DATE 與快閃記憶體管理相關的論文並
不多，整體算下來應該只有我們這一篇以及另外一篇由香港邵子立教授發表關於區
 4
比如將快取記憶體區分為兩塊，一塊是 SRAM 一塊是 STT-RAM 這樣。這種主題做完
之後，STT-RAM 還有什麼題目可以作，的確值得觀察。 
 
三、考察參觀活動(無是項活動者略) 
 
DATE 除了論文發表之外，同時也舉辦有工業界展覽（Exhibition）。我席間也
找了機會去參觀。席間大多是 IC 設計軟體相關的攤位，與我的研究主題不是很相關。
不過倒是遇到一些成大與清大的教授，也順便交換了一些研究心得。 
 
四、建議 
 
  無。 
 
五、攜回資料名稱及內容 
 
  本次會議攜回會議論文集之光碟片一片。 
 
六、其他 
 
  無。 
 
Dual Greedy: Adaptive Garbage Collection for
Page-Mapping Solid-State Disks
Wen-Huei Lin
Department of Computer Science
National Chiao-Tung University
Hsin-Chu, Taiwan, ROC
linwh.tw@gmail.com
Li-Pin Chang
Department of Computer Science
National Chiao-Tung University
Hsin-Chu, Taiwan, ROC
lpchang@cs.nctu.edu.tw
Abstract—In the recent years, commodity solid-state disks have
started adopting powerful controllers and implemented page-
level mapping for flash management. However, many of these
models still use primitive garbage-collection algorithms, because
prior approaches do not scale up with the dramatic increase of
flash capacity. This study introduces Dual Greedy for garbage
collection in page-level mapping. Dual Greedy identifies page-
accurate data hotness using only block-level information, and
adaptively switches its preference of victim selection between
block space utilization and block stability. It can run in constant
time and use very limited RAM space. Our experimental results
show that Dual Greedy outperforms existing approaches in terms
of garbage-collection overhead, especially with large flash blocks.
I. INTRODUCTION
Solid-state disks offer a less invasive way to deploy flash
storage in mobile computers. They implement a firmware
layer, i.e., the flash-translation layer, to emulate a collection of
disk sectors and hide flash management from the hosts. Flash
memory is a kind of erase-before-write non-volatile memory,
and the unit of erasure is much larger than that of read/write.
Thus, flash-translation layers handle data updates in a out-of-
place manner and adopt a mapping scheme to translate logical
sector numbers into physical flash locations.
Choosing the resolution of address mapping is an important
design issue of flash-translation layers. Although page-level
mapping has a natural appeal of its high write-performance, it
requires very large mapping tables which are too large to fit
in the RAM space of disk controllers. Thus, many entry-level
flash-storage devices such as thumb drives adopt block-level
mapping for the minimal table footprints. Many solid-state
disks adopt hybrid mapping for good balance between write
performance and table size. For example, solid-state disks
based on the disk controller GP5086 from Global Unichip
implement hybrid mapping and store their mapping table in
the 64 KB embedded SRAM. Recently, advanced solid-state
disks started adopting powerful controllers. For example, the
Intel 510 series employ Marvell’s dual-core disk controller
Van Gogh with an accompanying 128 MB DDR RAM chip.
This work is in part supported by a research grant NSC-98-2221-E-009-
157-MY3 from National Science Council, Taiwan, ROC and a research project
form Global Unichip Corp.
978-3-9810801-8-6/DATE12/ c©2012 EDAA
These solid-state disks implement page-level mapping as their
controllers have rich computational resources to do so.
Out-of-place updates produce outdated data in flash mem-
ory. Flash-translation layers must timely perform garbage col-
lection that erases flash space occupied by stale data into free
space. In addition to flash erasure, garbage collection could
also involve copying valid data. Because garbage-collection
activities could delay the processing of host requests, flash-
translation layers adopt two strategies to reduce this impact: 1)
data separation, which identifies frequently updated data (i.e.,
hot data) and writes data having similar update frequencies to
nearby flash space, and 2) victim selection, which prevents
garbage collection from copying valid data that will get
invalidated in the near future.
Even though prior studies have proposed various tech-
niques of garbage collection for page-level mapping [2], [4],
[7], many commodity page-mapping solid-state disks still
use primitive garbage collection algorithms. This is mainly
because, when these prior approaches were introduced, the
mainstream flash capacity was quite small at that time. These
prior techniques do not scale up to gigabyte-level flash. In
addition, these prior techniques use many parameters that
require manual tuning, making them not useful in real prod-
ucts. Recent studies investigated garbage collection for hybrid
mapping [8], [9], but these results are not applicable to page-
level mapping.
This study presents Dual Greedy for efficient garbage col-
lection in page-level mapping. Dual Greedy is designed to be
adaptive and scalable. It requires only block-level information
but identifies data hotness at the page level. When selecting
victim blocks for garbage collection, Dual Greedy dynamically
switches its preference between block space utilization and
block stability. Dual Greedy requires no static parameters and
is adaptive to various types of host workloads. The execution
time of Dual Greedy is not related to the flash capacity, i.e., it
can run in constant time. Compared to prior approaches, Dual
Greedy is simple yet very efficient.
The rest of this paper is organized as follows: Section
II introduces the fundamental issues of garbage collection
for page-level mapping. Section III presents a strategy for
identifying hot data and separating hot data from non-hot
data in flash. Section IV shows an adaptive policy for victim
0100000
200000
300000
400000
F
re
q
u
e
n
c
ie
s
Page lifetimes
Fig. 3. The frequency distribution of page lifetimes produced by servicing
100,000 page-write requests of the disk workload from a Windows desktop.
performance.
C. Victim Selection
The well-known greedy policy [7] picks up a block of the
largest amount of invalid data for garbage collection. However,
realistic workloads have temporal localities of write, and even
though a block is having the smallest amount of valid data
among all blocks, this block may may continue to receive page
invalidation. Thus, the greedy policy could result in premature
erasure of flash blocks.
The cost-and-benefit policy proposed by Kawaguchi et al.
[7] tries to avoid premature block erasure by giving lower
priority of garbage collection to flash blocks that recently
receive page invalidations. This method scores every flash
block by a heuristic function a×(1−u)2u and erases the most-
scored block for reclaiming free space. Note that a and u
in this function stand for the age and space utilization of a
block, respectively. Chiang et al. proposed Cost Age Times
(i.e., CAT) that takes both garbage collection and wear leveling
into consideration using the function u((1−u)×a)×t , where t is
the block erase count [2]. CAT erases the least-scored block.
However, both cost-and-benefit and CAT may need to re-score
all blocks to select a victim of garbage collection. They suffer
from poor scalability as the mainstream flash capacity has
grown to several gigabytes when we wrote this paper [11].
III. DATA SEPARATION POLICY
A. The Basic Data Structure: Multi-LRI Lists
The garbage-collection algorithm to be proposed is based
on an essential data structure called multi-LRI (least-recently
invalidated) lists. Figure 2 depicts the structure of the multi-
LRI lists. This data structure has p parallel lists, where p is the
total number of pages in a flash block. The elements of the lists
are flash blocks. A flash block hooks on the level-i list if this
block has i pages of valid data. Thus, the level-one list consists
of flash blocks having exactly one page of valid data. If a flash
block at level i receives a page invalidation, then this block is
promoted to the (i-1)-th level list as the rightmost element (i.e.,
the list tail) at the new level. This way, the leftmost blocks
(i.e., the list heads) are the least-recently invalidated blocks of
every list. Note that the “level-0” list is not in the multi-LRI
lists because blocks without any valid page data are no doubt
the best candidates for garbage collection.
Fig. 4. Computing page lifetimes with block-level information. Each block
stores the times of its first page write and its latest page invalidation.
B. Identifying Hot Data by Page Lifetimes
Realistic workloads have temporal localities of writing disk
sectors. If a disk sector is recently modified by the host, then
this sector will be modified for many times in the near future.
Let the lifetime of a flash page (not a logical page) denote
the total number of host write-requests arrived at the storage
device during the period between writing new data to this page
and invalidating (updating) data in this page. A valid flash page
does not have a lifetime as its data are not invalidated yet. The
rest of this paper adopts the total number of host write-requests
arrived as the logical unit of time.
We collected the page lifetimes produced by servicing
100,000 page writes after a short warm-up of 400,000 page
writes under the disk workload of a Windows desktop2. Note
that page invalidations are irrelevant to flash management
because they are host-level behaviors. Figure 3 shows that
the frequency distribution of the collected page lifetimes is
bimodal. In other words, the page lifetimes contributed by
hot data are much shorter than that contributed by non-hot
data. Thus, this study proposes using a lifetime threshold
of identifying hot data. When a new page write arrives, it
invalidates the latest copy of the page data and produces a page
lifetime. If this page lifetime is shorter than the threshold, then
the data of the new page write is recognized as hot data. The
flash-translation layer should use two flash blocks and write
hot data and non-hot data separately to these two blocks3.
A technical question is then how to adaptively set the
lifetime threshold for different types of workloads. Let the
top level of the multi-LRI lists be the highest level which has
at least one block. This study proposes using the largest page
lifetime of the blocks at the top level as the lifetime threshold.
Because hot data produce most of the invalid pages in flash,
blocks having many hot data will reach the top-level list in
a short period of time. Thus, the largest page lifetime of the
top-level blocks is a good initial reference of data hotness.
Provided that hot/non-hot separation is effective, there will be
more blocks reach to an even higher level of the multi-LRI
lists, and this change will further refine the age threshold.
2This is the PC workload in our experiment section.
3The flash-translation layer uses another flash block for writing garbage-
collected data, because many of these data are non-hot or even immutable
and they should not be mixed with new data.
TABLE I
CHARACTERISTICS OF THE EXPERIMENTAL WORKLOADS
region because they are not more stable than the only top-
level block. Dual Greedy selects the block whose utilization
is the lowest among the list heads not in the shadowed region.
This is because if the list head of a level is not more stable
than the top-level block, then none of the blocks at this level
would be. In the worst case that no list heads are more stable
than the top-level block, then Dual Greedy selects the only
top-level block as the victim.
The proposed victim selection strategy requires blocks’ lat-
est page-invalidation times to compute block dormant periods.
The data separation policy also utilizes this information and
can share it with the victim-selection policy.
V. EXPERIMENTAL RESULTS
A. Experimental Setup and Performance Metrics
We have built a simulator for performance evaluation.
This simulator implements representative designs of flash-
translation layers: 1) FAST [8], which is based on hybrid
mapping, 2) page-level mapping with the greedy policy [7]
(PL+greedy), 3) DAC [2], which is based on page-level
mapping and uses the cost-age-time policy (CAT) for garbage
collection, 4) SuperBlock [6], which uses page-level mapping
inside of groups of flash blocks, and 5) Dual Greedy. Notice
that PL+greedy uses separated blocks for writing new data and
garbage-collected data. DAC and SuperBlock adopted their
best parameter settings found by off-line exhaustive search.
DAC ignored wear leveling for its best performance of garbage
collection.
There are three types of workloads in our experiments.
The first workload was collected from a desktop PC running
Windows XP whose file system was NTFS (i.e., the PC
workload), the second workload is generated from a portable
media player (i.e., the SEQ workload) that repeatedly wrote
large files. The last one (i.e., the RND workload) is obtained
from the industrial-standard benchmark tool Iometer with a
4KB request size and 100% random write. Table I is a
summary of these workloads. This study adopts the total erase
count as the primary performance metric of garbage collection.
B. Host Workloads
This experiment adopts the geometry of a typical NAND
flash [11] whose block size and page size are 512 KB and
4 KB, respectively. Let the over-provisioning ratio be the
fraction of flash space provided as spare space. For example,
if the logical disk is 40 GB, then with a 2.5% over-provision
ratio the flash size is 40×(1+2.5%)=41 GB.
Figure 7(a) shows the results under the PC workload. This
type of workload has many temporal localities of write. Dual
Greedy greatly outperformed FAST, and this advantage is
obvious especially when the over-provision ratio was large.
This is because large spare size improves the effectiveness
of hot/non-hot separation. In contrast, the merge-based FAST
did not much benefit from using large spare space. Compared
to SuperBlock, Dual Greedy does not confine page-level
mapping to small block groups, and thus Dual Greedy has
more flexibility of separating hot data from non-hot data.
Dual Greedy also surpassed PL+greedy because the proposed
policies for data separation and victim selection performed
better than the primitive counterparts in PL+greedy.
The experiment for the SEQ workload used smaller over-
provision ratios to increase the pressure of garbage collection.
Figure 7(b) shows that, benefited from switch merges, FAST
delivered better performance than PL+greedy when the spare
size was very small. However, this advantage quickly dimin-
ished as the over-provision ratio increased, because PL+greedy
could always find blocks having no valid data for erasure.
Figure 7(c) shows the results under the RND workload. Su-
perBlock suffered from log-block thrashing under this random-
write pattern even when the flash spare size was very large.
FAST performed poorly when flash spare size was small
because of its extremely large associativity of log blocks.
Adding more flash spare space effectively relieved FAST of
this problem. DAC and Dual Greedy performed as good as
PL+greedy when the spare size was large, but neither of them
can outperform PL+greedy. This is because the RND workload
has a purely random write pattern. Any predictions on write
behaviors based on locality will not be useful. Thus, a flash-
translation layer could switch to the greedy policy when it
detects that the host is accessing the solid-state disk with a
purely random write pattern.
Recall that DAC requires considerable resources of time
and space, and it needs manual parameter-tuning. Dual Greedy
used very limited resources, but achieved the same or even
better performance than DAC. Later sections will provide more
discussion of resource requirements.
C. Flash Geometry
It is important to examine the performance of Dual Greedy
with coarse-grained flash geometry because parallel flash
structures such as planes, gangs, and interleaving groups would
effectively increase the flash block size. Garbage collection
becomes more difficult when the block size is large because
erasing a large flash block could involve more data copying.
This experiment adopted two geometry settings whose page
sizes and block sizes were 4 KB/512 KB and 8 KB/1 MB,
respectively. Because these two settings have different block
sizes, the Y-axis of Fig. 7(d) indicates the total numbers of
bytes erased from flash. The workload was the PC workload,
and the over-provisioning ratio was 5%. Figure 7(d) shows
that Dual Greedy had the smallest increase (i.e., 1.3 times) on
the total byte erased when switching to large blocks.
D. The Needs for Adaptiveness and Overhead Analysis
A design goal of Dual Greedy is to eliminate the needs for
static parameters. This experiment takes DAC as an example
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/28
國科會補助計畫
計畫名稱: 高效能固態硬碟管理方法暨存取策略
計畫主持人: 張立平
計畫編號: 98-2221-E-009-157-MY3 學門領域: 計算機結構與計算機系統
無研發成果推廣資料
件數 0 0 100% 件  技術移轉 
權利金 0 0 100% 千元  
碩士生 12 12 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
計畫衍生之產學合作案共兩案，包括 99 年創意電子、100 年建興電子。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
