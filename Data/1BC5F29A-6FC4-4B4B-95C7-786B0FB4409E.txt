 1 
摘要 
 監視攝影機在日常生活中的運用已經愈來愈
普及了，尤其是在都會地區。在我們先前的研究上，
對於未來無所不在攝影機(UbiCam)環境的到來，提
出一個新的服務，稱做GPS-VT(GPS-based Visual 
Tracking) ，其結合全球定位系統 (GPS: Global 
Positioning System)與視覺追蹤方法，當移動物體在
任何廣角攝影機的可視範圍內(FOV: Field-of-View)
都可以根據其即時的GPS座標進行追蹤。而為了將
GPS-VT 實現在主動式 PTZ(pan/tilt/zoom) 攝影機
上，在本研究中提出另一個新的方法，這個方法包
括PTZ攝影機的校正、GPS座標與pan，tilt和zoom值
的轉換，並且結合影像處理為主的追蹤方法來對目
標物進行精準地追蹤。在實驗中我們驗證了PTZ攝
影機校正的準確性以及實現了多台PTZ攝影機之間
的handover，這些結果說明了本研究提出之GPS-VT
在PTZ攝影機上應用的可行性。 
關鍵字：位置相依服務、物體追蹤、攝影機校正、
行動運算、無所不在運算 
 
Abstract 
The installation of cameras is becoming 
evermore pervasive, especially in urban areas. In 
response to this ubiquitous camera (UbiCam) 
environment, a new service, called GPS-VT 
(GPS-based Visual Tracking), incorporating the global 
positioning system (GPS) and the visual tracking 
function, was proposed in our previous study, whereby 
an object can be tracked when it is within any 
field-of-view (FOV) of any wide-angle camera 
according to its real-time GPS coordinates. In order to 
realize GPS-VT on a pan/tilt/zoom (PTZ) camera, a 
new approach is now proposed in this study. This 
approach includes the calibration of the PTZ camera, 
the transformation of GPS coordinates to pan, tilt and 
zoom values, and the incorporation of an image-based 
tracker for the precise tracking of the target object. A 
prototype system is implemented to demonstrate the 
feasibility of GPS-VT for the PTZ camera. 
Keywords: location-based service, object tracking, 
camera calibration, mobile computing, ubiquitous 
computing. 
 
1 Introduction 
The revolution in information and communication 
technology (ICT) has enhanced the accessibility and 
functionality of diverse information products. 
Surveillance cameras are one such product; their 
importance lies in their contribution to the security and 
safety of daily life. ICT enables the real-time image of a 
surveillance camera to be accessed any time, any place 
and via virtually any device. Currently, cameras are 
widely installed along streets and throughout 
communities, especially in urban areas. Clearly, the 
presence of the ubiquitous camera (UbiCam) is part of 
the future trend. 
The global positioning system (GPS) is also 
popularly used in a variety of outdoor environments. 
The accuracy of a differential GPS (DGPS) receiver 
can be achieved within 1-5 m. Many location-based 
services (LBSs) have been developed based on the GPS 
system, e.g., car navigation, vehicle security, location 
tracking of children or elderly persons, etc. GPS can 
also be incorporated within certain intelligence services. 
For example, the GPS coordinates of vehicles can be 
used to estimate freeway traffic flow or to detect traffic 
accidents or trouble spots. In addition, GPS receivers 
have been integrated with various kinds of consumer 
electronics, such as mobile devices, so that a moving 
object (whether a person or a vehicle) can be tracked 
more easily.  
However, for the most part, current GPS-based 
tracking systems display the location of a moving 
object on a digital map. Unfortunately, it can be 
difficult to realize the condition of the object just by a 
dot on the map or its historical trace. If surveillance 
cameras could be incorporated into a system which 
would provide real-time visual images, a better tracking 
service than the conventional GPS-based tracking 
system would be possible. Therefore, a tracking service, 
called GPS-VT (GPS-based Visual Tracking), was 
proposed in our previous study (Liao and Chu, 2009). 
Various methods are needed for realizing GPS-VT on 
different kinds of moving objects or cameras. In our 
previous study, GPS-VT was designed for tracking a 
walking person with multiple wide-angle cameras; the 
principle and corresponding prototype will be presented 
in detail later.  
An approach has now been designed to fulfill 
GPS-VT on active, i.e., pan/tilt/zoom (PTZ), cameras. 
Firstly, a calibration step was used to estimate the 
needed parameters. Then, the GPS coordinates of a 
target object were used to compute the pan/tilt/zoom 
values which were then employed to control the 
camera’s aim related to the target object. The target 
object could then be located among all of the objects in 
the area. After the target was located, it was tracked 
using an image-based tracker. The GPS coordinates 
were used to ensure that the object was correctly 
tracked by the tracker. A prototype was implemented 
based on the proposed approach.  
The rest of this study has been organized as 
follows: Section 2 reviews some related works; Section 
3 presents the previous result and the principle of 
GPS-VT for wide-angle cameras; Section 4 presents the 
proposed approach for realizing GPS-VT on the PTZ 
camera; Section 5 describes the experimental study and 
the prototype; and Section 6 presents the conclusion 
and future work indicated by this study. 
 
2 Related Works 
The image-based moving object tracking technique is 
an important concept and has been studied for many 
years. Objects can be tracked under various camera 
types, object sizes, levels of brightness, or other 
conditions. In recent years, many studies have proposed 
methods for incorporating ad hoc techniques, such as 
PTZ cameras, for object tracking. There have been a 
number of interesting studies which have incorporated 
these ad hoc techniques. Cui et al. (2008) designed a 
tracking method incorporating multiple laser scanners 
and a camera; the laser scanner was used to detect a 
new person moving into the field-of-view (FOV) of the 
camera. When a person was detected, a 
mean-shift-based tracker was used to track the person. 
The initial values of the mean-shift-based tracker were 
generated from the detection results of the laser 
scanners. A decision-level Bayesian fusion method was 
also utilized for combining the tracing of the laser 
scanners and the visual tracking results in order to 
 3 
target object and which remain unaffected by such 
influences. In addition, GPS coordinates provide useful 
information for the cooperation of multiple cameras 
which can be used to predict the next possible camera 
before the object leaves the FOV of the current camera. 
The object's image from the current camera can be 
matched with the objects of the next camera in order to 
shorten the time required to locate the target object. 
The realization of GPS-VT must take into 
consideration the following factors: 
1. Camera type: In general, a camera is either 
passive, i.e., wide-angle or active, i.e., 
pan/tilt/zoom (PTZ). Distinct methods are needed 
for the camera calibration and operation.  The 
method designed for a wide-angle camera is 
usually less complicated than that for a PTZ 
camera. 
2. Moving speed: An object may have many 
characteristics; however, the moving speed is the 
most important one for the integration of GPS 
coordinates and objects in a real-time image. So, a 
person walking and a moving vehicle are two 
typical objects for realizing GPS-VT. 
The GPS-VT function of the precise tracking on a 
PTZ camera of a person walking was the objective of 
this study. Our method is presented in the next section. 
   
4 GPS-VT for PTZ Camera 
In order to realize GPS-VT on a PTZ camera, the first 
step was the camera calibration, the primary purpose of 
which is to estimate the camera position and related 
parameters for the computation of the desired pan, tilt 
and zoom values from the GPS coordinates. GPS 
coordinates are in degrees, minutes and seconds of 
latitude and longitude (DMS format). However, this 
format was unsuitable for the computation of pan, tilt 
and zoom values. Therefore, the GPS coordinates were 
converted to the TM2 (2-degree Transverse Mercator) 
format in meters. Details of the camera calibration 
follow. 
When a PTZ camera is calibrated, the operation of 
the GPS-VT service is divided into three phases: 
detection, locating and tracking, as shown in Figure 2. 
The distance to the target object is used to detect 
whether or not an object is within the surveillance area 
of a PTZ camera. When an object is within the 
surveillance area, the camera then enters the locating 
phase. The PTZ camera is controlled using the pan, tilt 
and zoom values computed from its GPS coordinates. 
The moving vectors of the objects in the FOV of the 
camera are matched with those of the pan and tilt 
values. This procedure is repeated until the target object 
is located. The process then enters the tracking phase. 
The object is tracked using the image-based object 
tracking method. The graph-cut tracker used in this 
study estimates the location of the target object in the 
current frame according to the image of the target 
object in the previous frame. The PTZ camera is 
controlled according to the result of tracker in order to 
keep the target within the FOV centre. At this phase, 
the GPS coordinates are used to check whether the 
target is being accurately tracked.  Tracker failure 
occurs when the difference between the current pan/tilt 
values and those computed from the GPS coordinates 
are larger than a pre-defined threshold. Once there is 
tracker failure, the process returns to the detection 
phase and attempts to locate and track the target again. 
Of course, the process is stopped when the target leaves 
the surveillance area of the PTZ camera.  These 
phases are described in the following sub-sections. 
 
 Tracking phase
 Detection phase
Receive 
GPS coordinates
Is target within
surveillance area?
No
Submit pan/tilt/zoom values 
to camera
Adjust zoom and 
centralize the target
Graph-cut tracking
Compute pan/tilt/zoom values
from GPS coordinate
Calculate 
target distance
Yes
 Locating phase            
Motion detection
Vector angle and 
object center matching
No
Is target located? Is target out-of-FOV?
Yes
Yes
NoIs target out of 
surveillance area?
Yes
No
 
Figure 2: Process of GPS-VT for PTZ camera 
 
4.1 Camera Calibration 
Camera calibration is used to estimate the parameters 
needed for the transformation from the TM2 coordinate 
to the pan/tilt/zoom values. For a PTZ camera, pan, tilt 
and zoom are operated separately. Therefore, the 
calibrations for pan, tilt and zoom were performed 
separately. 
Three point correspondences are needed for the 
calibration. Each correspondence consists of a TM2 
coordinate and the corresponding pan, tilt and zoom 
values, denoted as Ci={(xi, yi), (pi, ti, zi)}, for 1  i  3. 
The calibration and operation of GPS-VT for a PTZ 
camera are shown in Figure 3. The upper part of the 
Figure shows the calibration process. Three point 
correspondences are used to evaluate the camera 
position, (x0, y0), and the related parameters. Then, the 
desired pan/tilt/zoom values, (p’, t’, z’), for a given 
TM2 coordinate, (x’, y’), can be computed from (x0, y0) 
and the related parameters. The computation method 
for the camera position and the parameters for the 
pan/tilt/zoom values is presented below. 
 
Computation of camera position: 
For the computation of the pan/tilt/zoom values it is 
necessary to know the distance between the given 
position and the camera. Most cameras are installed on 
the wall of a building. The position of a camera is 
neither easily nor precisely acquired using a GPS 
receiver. Therefore, the camera position is derived from 
three point correspondences. For a one point 
correspondence, a line can be established from the (xi, 
 5 










0
0
'
'
arccot'
xx
yy
p     (5) 
 
Computation of tilt value: 
For all TM2 coordinates with the same distance to the 
camera position, (x0, y0), the corresponding tilt values 
are identical. That is, the TM2 coordinates form a 
concentric circle. According to this principle, three 
point correspondences can be modeled by three 
concentric circles, as shown in Figure 7. There exists q 
degree between the horizon line and the sight line of 
zero tilt degree. The height of the camera is denoted as 
h, the parameter used here, not the actual height of the 
camera. The parameters, q and h, are computed first 
and then used to compute the tilt value, t’, for a given 
(x’, y’). The equations are derived as follows. 
(x0, y0)
h
q
(horizontal line)
q+t2
(sight 
line of 
zero ti
lt angle
)
t2
Camera
(x1, y1) (x2, y2)
(x', y')
q+t1
t1
t'
(x3, y3)
t3
q+t3
TM2 coordinate system
X
Y
 
Figure 7: Model of tilt value computation 
  According to a trigonometric formula, the 
calculation of the distance from (x1, y1) to (x2, y2), 
denoted as 
3d , is as shown in Eq. (6): 
     
     
     htqtqddd
htqdyyxx
htqdyyxx



21213
22
2
02
2
02
11
2
01
2
01
cotcot
cot
cot
 (6) 
  The equation of 
3d can be expanded further by 
using the trigonometric formula as shown in Eq. (7): 
   
   
   
   
          
            
2121
2
21
2
21
2
2
1
13
cotcotcotcotcotcot
cotcotcotcotcot
cotcot
1cotcot
cotcot
1cotcot
ttqttq
ttqtt
tq
tq
tq
tq
h
d









 (7)
 
  Let m1 = cot(t1)  cot(t2), n1 = cot(t1) + cot(t2), and 
k1 = cot(t1)  cot(t2); then the above equation is 
rewritten as Eq. (8):  
  
   
11
2
2
13
cotcot
1cot
kqnq
qm
h
d


  (8) 
  Similarly, let m2 = cot(t2)  cot(t3), n2 = cot(t2) + 
cot(t3), and k2 = cot(t2)  cot(t3); then the computation 
of 
2d  is  as shown in Eq. (9): 
  
   
22
2
2
22
cotcot
1cot
kqnq
qm
h
d



 
 (9) 
Then, Eq. (8) is divided by Eq. (9). The result is as 
shown in Eq. (10): 
  
   
   
  
   
   
   
   
0
cot
cot 
cotcot
cotcot
1cot
cotcot
cotcot
1cot
312221
312221
2
3221
1212
2
2
2121
2
1
1
2
2
22
2
11
2
1
2
1
2
3












dkmdkm
qdnmdnm
qdmdm
kmqnmqm
kmqnmqm
qm
kqnq
kqnq
qm
d
d
 (10)
 
Let  =
321 yym   212 yym ,  = 3221 yynm   
2112 yynm , and  = 3221 yykm   2112 yykm ; then the 
above equation can be rewritten as Eq. (11): 
    0cotcot 2   qq   (11) 
Therefore, the solution of Eq. (11) is shown in Eq. 
(12): 
 


2
4
cot
2

q  (12) 
There are two solutions for Eq. (12); however, the 
negative solution is not considered in the calibration 
model. The equation of the two parameters, q and h, is  
shown in Eq. (13): 
    
   
21
3
21
213
2
cotcot
cotcot
2
4
arccot
tqtq
d
h
htqtq
ddd
q











 



  (13) 
  Then, the desired tilt value, t’, can be computed 
according to Eq. (14). 
     
q
h
d
t
htqyyxxd










'
arccot'
'cot'''
2
0
2
0  (14) 
 
Computation of zoom value: 
The zoom value is computed according to the distance 
between the object and the camera in order for the 
object size in the camera image to remain close at 
different distances. The model for computing the zoom 
value is shown in Figure 8. 
(x0, y0)
h
Camera
(x1, y1)
(x', y')
r1
r'
r2
(x2, y2)
 
 
z'
 
 z1
 
 z2
TM2 coordinate system
X
Y
 
Figure 8: Model of zoom value computation  
  According to the triangular formula, the distance 
from the camera to the target object can be computed 
from the parameter h and the distance between (x0, y0) 
and (x’, y’). The equation is shown in Eq. (15): 
 7 
5 Prototype and Experimental Study 
In order to evaluate the performance of the proposed 
approach, the computation of the camera position and 
the corresponding parameters were first examined. 
Then, a prototype based on the approach was used to 
demonstrate GPS-VT for a PTZ camera.  
For the computation of the camera position, a 
camera was installed at two different locations. Four 
and five point correspondences were recorded at two 
separate locations. Only three point correspondences 
were needed in our approach; therefore, four and ten 
combinations of point correspondences were used 
separately for the parameter computation. These results 
are listed in Tables 1 and 2. In Table 1, the four point 
correspondences are listed in the upper part of the Table. 
All four combinations, the corresponding computed (x0, 
y0) and the diff computed using Eq. (3) have also been 
listed. The pre-defined threshold for diff was 0.01, i.e., 
1 centimeter. Three combinations satisfied the criterion 
and have been marked in bold. The average camera 
position of these three combinations was used to 
estimate the desired parameters, , q, h, a, and b. They 
are listed in the lower part of Table 2. The fourth 
combination failed to compute the camera position with 
diff under the threshold. The failure was caused by the 
inaccuracy of the acquired GPS coordinates, which 
meant that the computation of the camera position 
could fail if only three point correspondences have been 
established. If one more point correspondence were 
established, it would ensure the successful estimation of 
the camera position and the related parameters. 
 
Table 1: Camera calibration of the first location under 
four point correspondences 
Point 
correspondences 
(xi, yi),(pi, ti, zi) 
C1 
(220987.561, 2662763.612), 
(6.974, -6.375, 9710) 
C2 
(220938.22, 2662752.749), 
(-51.2925, -14.7, 8500) 
C3 
(220988.529, 2662739.356), 
(25.4962, -8.625, 9550) 
C4 
(220952.118, 2662736.219),   
(-5.0992, -21.375, 7800) 
All combinations (x0, y0) diff 
C1, C2, C3 
(220963.4, 
2662713.456) 
0.00945809 
C1, C2, C4 
(220943.2, 
2662724.119) 
0.00979009 
C1, C3, C4 
(220939.2, 
2662715.119) 
0.00889765 
C2, C3, C4 
(220900, 
2662699.919) 
4.77293484 
Estimated parameters 
Average (x0, y0) (220939.7286, 2662718.501) 
 40.37 
q 4.2 
h 11.61 
a 40.99 
b 7049.14 
 
Table 2 shows the ten combinations for five point 
correspondences. Five combinations satisfied the 
threshold of diff. The average camera position and 
desired parameters are listed in the lower part of this 
Table. The above results showed that the proposed 
method estimated the camera location precisely. 
In advance, a prototype was implemented to 
demonstrate GPS-VT for PTZ cameras in a practical 
environment. Its screen shot is shown in Figure 11. The 
real-time camera image is displayed on the left-hand 
side. The image size is 320240 pixels. The pan/tilt 
values computed from the received GPS coordinates are 
listed on the right-hand side. The binarization result of 
the camera image is shown at the lower right part of the 
screen shot.  
 
Table 2: Camera calibration of the second location 
under five point correspondences 
Point 
correspondences 
(xi, yi), (pi, ti, zi) 
C1 
(221022.989, 2662733.307), 
(-43.8685, -28.575, 8594) 
C2 
(221031.351, 2662744.028), 
(-38.8443, -22.5, 8750) 
C3 
(221030.822, 2662735.062), 
(-35.4698, -26.4, 8002) 
C4 
(221027.192, 2662750.353), 
(-43.0437, -21.45, 9198) 
C5 
(221030.581, 2662766.263), 
(-45.8932, -18.15, 9198) 
All combinations (x0, y0) diff 
C1, C2, C3 
(221010.5, 
2662693.607) 
0.00968400 
C1, C2, C4 
(221012.9, 
2662678.607) 
0.00999166 
C1, C2, C5 
(221011.4, 
2662696.107) 
0.00971103 
C1, C3, C4 
(221014.4, 
2662689.007) 
0.00968946 
C1, C3, C5 
(221010.5, 
2662693.707) 
0.00992145 
C1, C4, C5 
(221000, 
2660999.907) 
0.09140611 
C2, C3, C4 
(221000, 
2660999.962) 
0.26047700 
C2, C3, C5 
(221196.3, 
2662721.162) 
0.00998040  
C2, C4, C5 
(221000, 
2660999.928) 
0.24197309 
C3, C4, C5 
(221000, 
2660999.962) 
0.36222465 
Estimated Parameters 
Average (x0, y0) (221011.94, 2662690.207) 
 58.43 
q 21.98 
h 55.95 
a 21.372 
b 7015.43 
 
 
Figure 11: Screen shot of prototype 
In addition to the tracking of a single PTZ camera, 
the handoff between two PTZ cameras was included in 
the demonstration as well. Therefore, two PTZ cameras 
were installed outside two buildings on the campus. 
Their surveillance areas partially overlapped, as shown 
in Figure 12. The corresponding FOVs of the two 
cameras are illustrated in this figure.  
The tracking demonstration of the prototype 
system is shown in Figure 13. As shown in Figure 13(a), 
the target object entered the surveillance area of the 
first camera and the camera was controlled using the 
pan, tilt and zoom values computed from the GPS 
 9 
Liao, H.C. and Chu, P.T. (2009) ‘A novel visual 
tracking approach incorporating global positioning 
system in a ubiquitous camera environment’, 
Information Technology Journal, Vol. 8,  No. 4, 
pp.465-475. 
Miyaki, T., Yamasaki, T. and Aizawa, K. (2007) 
‘Visual tracking of pedestrians jointly using Wi-Fi 
location system on distributed camera network’, 
Proceedings of IEEE International Conference on 
Multimedia and Expo, pp.1762-1765. 
Qureshi, F.Z. and Terzopoulos D. (2009) ‘Planning 
ahead for PTZ camera assignment and handoff’, 
Proceedings of the Third ACM/IEEE International 
Conference on Distributed Smart Camera, Aug. 
30-Sept. 2, Como, Italy. 
Song, K.T. and Tai, J.C. (2006) ‘Dynamic calibration 
of pan-tilt-zoom cameras for traffic monitoring’, 
IEEE Transactions on System, Man, and 
Cybernetics-Part B: Cybernetics, Vol. 36, No. 5, 
Oct., pp. 1091-1103. 
Weigel, H., Lindner, P. and Wanielik, G. (2009) 
‘Vehicle tracking with lane assignment by camera 
and lidar sensor fusion’, Proceedings of the 2009 
Intelligent Vehicles Symposium, pp.513-520. 
Xu, Y. and Song, D. (2009) ‘Systems and algorithms 
for autonomously simultaneous observation of 
multiple objects using robotic PTZ cameras 
assisted by a wide-angle camera’, Proceedings of 
the 2009 IEEE/RSJ International Conference on 
Intelligent Robots and Systems, Oct. 11-15, pp. 
3802-3807. 
Yang, C.S., Chen, R.H., Lee, C.Y., and Lin, S.J. (2008) 
‘PTZ camera based position tracking in 
IP-surveillance system’, Proceedings of the Third 
International Conference on Sensing Technology, 
Nov. 30-Dec. 3, Tainan, Taiwan, pp. 142-146. 
Zhang, C., Wu, J. and Tu, G. (2006) ‘Object tracking 
and QOS control using infrared sensor and video 
cameras’, Proceedings of the 2006 IEEE 
International Conference on Networking, Sensing 
and Control (ICNSC '06), pp.974-979. 
 
 
 
 11 
可供推廣之研發成果資料表 
 □ 可申請專利  █ 可技術移轉                                    日期：99 年 9 月 13 日 
國科會補助計畫 
計畫名稱：室內外環境無縫融合之 GPS 視覺追蹤方法的研究 
計畫主持人：廖珗洲         
計畫編號：NSC 98-2221-E-324-028   學門領域：影像處理 
技術/創作名稱 結合 GPS 定位系統與 PTZ 攝影機之視覺追蹤方法 
發明人/創作人 廖珗洲 
技術說明 
中文：一般以即時影像為主的視覺追蹤技術不易辨識移動物體身份
資料，而本技術結合現有 GPS 定位系統與 PTZ 攝影機之後，可以
針對任一移動物體提供視覺追蹤的服務，並配合 PTZ 攝影機涵蓋
較大監視區域的特性，運用單一 PTZ 攝影機就可以有效地監控一
個廣場上發送 GPS 座標的移動物體。 
英文：It is difficult to identify the moving objects for the general visual 
tracking techniques simply based on real-time images. In this 
technique, the incorporation of GPS system and PTZ (Pan/Tilt/Zoom) 
cameras enables the identification and tracking of any moving object. 
In advance, a PTZ camera can be used to monitor a large open area. It 
is efficient to monitor multiple objects with GPS receivers by using a 
single PTZ camera. 
可利用之產業 
及 
可開發之產品 
監視系統、車隊管理系統 
技術特點 
 提供視覺資訊來強化 GPS 追蹤系統的不足 
 提供多攝影機中相同物體比對與辨識能力 
推廣及運用的價值 
 拓展 GPS 追蹤系統相關產業規模 
 加速以 GPS 系統的普及程度 
 區隔現有產品市場，提供系統技術優勢 
※ 1.每項研發成果請填寫一式二份，一份隨成果報告送繳本會，一份送 貴單位研發成
果推廣單位（如技術移轉中心）。 
※ 2.本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。 
※ 3.本表若不敷使用，請自行影印使用。 
  
 
附件二 
果顯示幾點結論：第一：攻擊方式都是非常短暫即時的，在 Honeynet 分析出來之後沒辦
法在當下採取任何動作。第二：攻擊頻率跟時間無關，沒有前期的徵兆可以得知。而後
續的觀察也得到下面幾個重點：第一：幾乎百分之一百都是自動攻擊，很少有手動攻擊
的行為。第二：入侵 OS 是目前大部分攻擊的主要目標。第三：攻擊的類型或者頻率都
是無法預測的外在因素。然後，作者談到從早前 PC 到手機，以及目前智慧型手機的攻
擊方式，主要因素在於只要是系統是開放式的架構，則容易有攻擊的發生，因此目前
iPhone 或者 BlackBerry 手機因為是封閉式的系統，因此還沒有病毒或者惡意軟體被發
現，但是目前許多開放式系統的智慧型手機則可能是下一波容易發生危安的問題。因此，
綜整各種未來的安全問題的攻擊方式，包含：e-mail，SMS 簡訊以及 VoIP，由於 VoIP 的
攻擊成本與可追蹤性較低，因此需要特別注意與防範這類的攻擊，包含透過標準的 SIP
協定來冒名使用別人的撥打額度，或者從事非法的行為。最後，演講者以最近長達兩年
的 VoIP Honeynet 的 Log 進行分析，確實 VoIP 類型的攻擊有逐漸頻繁的趨勢，來強調這
類攻擊有需要開始重視。由於個人也曾經以特徵分析的方式來進行病毒的偵測，而聽了
這場演講之後在 VoIP 攻擊的偵測或許也有發揮的空間。 
另外，這次會議印象最深的是參與一場題目為「Emergent Issues on E-Learning」的
Workshop，這個 Workshop 一共安排了三位主持人，首先每位主持人都先提出一些個人
的想法，有主持人是針對目前 IT 領域 Top 20 的議題進行分享，當然第一名是 Mobile 
Application Development，而 E-learning 也是前五名之一。另一位主持人則是談到各種課
程的 learning profile，有的課程必須透過重複練習的方式來學習，有的則是著重於透過溝
通方式的學習，各種類型的課程應該專注在其學習 profile 或許會比較恰當。第三位主持
人則是說明一些目前 E-learning 的狀況與問題，包含：E-learning 對於主動的學生確實有
可以提升學習成效，然而對於被動的學生則是有反效果，另外，是否有最佳的 E-learning
系統也提出來跟與會學者討論。好幾位學者非常主動的分享個人想法，有的談到課程建
構費用的成本太高，有的提到特定類型課程不容易透過 E-learning 方式來進行，如：數
學類型的課程。特別有一位學者提到 E 與 learning 的中間有一個減號(minus)，用來比喻
E 化之後應該會降低學習的效果，來表示對於目前 E-learning 的成效上有所質疑。事實
上，目前有許多 E-learning 的系統，但是這些都是著重所有課程共用的功能，在個別課
程的使用上則有所不足，然而個別課程客製化又不是一件容易的工作。因此，在個人的
教學經驗中，覺得如何透過 IT 技術來提升上課的互動與交流或許也是可以思考的方向，
而且在課堂的互動上也可以設計出可以讓所有課程共用的功能，來提升上課學生的注意
力與吸收程度，這是面對目前主動性較為不佳學生的可能方式。由於個人正開始運用嵌
入式觸控面板來進行電子課桌方面的系統開發，期望先從國小學童的課堂互動上面來嘗
試，或許可以提供另一種提升學習效果的途徑。 
 在會議過程中除了上面這些內容之外，還吸收了相當多其他學者在特定議題上的研
究成果，對於個人在這方面的研究上有相當大的助益。 
98年度專題研究計畫研究成果彙整表 
計畫主持人：廖珗洲 計畫編號：98-2221-E-324-028- 
計畫名稱：室內外環境無縫融合之 GPS 視覺追蹤方法的研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 0%  
研究報告/技術報告 1 2 100%  
研討會論文 0 0 0% 
篇 
 
論文著作 
專書 0 0 0%   
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 4 4 100%  
博士生 0 0 0%  
博士後研究員 0 0 0%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 0% 
人次 
 
期刊論文 0 1 100% 目前成果投稿 SCI期刊審稿中。 
研究報告/技術報告 0 0 0%  
研討會論文 0 0 0% 
篇 
 
論文著作 
專書 0 0 0% 章/本  
申請中件數 0 0 0%  專利 已獲得件數 0 0 0% 件  
件數 0 0 0% 件  
技術移轉 
權利金 0 0 0% 千元  
碩士生 0 0 0%  
博士生 0 0 0%  
博士後研究員 0 0 0%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 0% 
人次 
 
 
