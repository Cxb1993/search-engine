Abstract
This project explores learning nonlinear recursions
for time series analysis and blind deconvolution. Un-
der the assumption that given time series are linear
convolutions of independent sources, this work em-
ployes MLPotts(Multilayer Potts perceptrons) learn-
ing to estimate parameters of moving average models
as well as the original independent sources. Learn-
ing nonlinear recursions includes three steps, which
are respectively for paired data preparation, MLPotts
network construction and independent source etxrac-
tion. Based on the attained independent source, the
unknown moving average model could be further es-
timated. By numerical simulations, it is shown that
learning nonlinear recursion is eﬀective for time series
analysis, blind deconvolution and blind equalization.
Keyword: moving average convolution model,
multilayer perceptrons, multilayer Potts percep-
trons, blind equalization, blind deconvolution, hidden
Markov model
1 Introduction
In this work, single channel observations are assumed
as linear convolution of an independent source. It is
non-trivial to extract the orginal independent source
when the convolution model is not given. This work
explores learning nonlinear recursions based on data
driven function approximation by MLPotts networks,
then applying the proposed novel method to estimate
the convolution model and extract the original inde-
pendent source.
The auto regression structure underlying observa-
tions of a given time series is assumed nonlinear in
this work. Auto regression is aimed to learn nonlin-
ear recursions. The goal is to express the observation
in terms of predecessors. This work applies MLPotts
learning to construct nonlinear recursions for auto re-
gression. Single channel observations are composed
of deterministic and non-deterministic components.
The approximation based on reconstructed nonlinear
recursions well extracts the embedded deterministic
component. The approximating error is regarded as
the component that is non-deterministic and is fur-
ther employed to estimate the unknown convolution
model.
Main advantages of leaning nonlinear recursions
by function approximation of MLPotts networks are
summarized as follows.
1. The neural processing element in a multi-
layer neural network is extended to novel Potts
perceptrons, which can realize arbitrary post-
nonlinear fucntions. This significantly helps de-
velopment of multiple filters for auto regression.
2. MLPotts learning by the Leveberg Marquardt
method is potential for construction of essential
nonlinear recursions for auto regression.
3. The porwerful MLPotts learning could eliminate
deterministic components from single-channel
observations. This helps robust estimation of the
original independent component.
2 Time series analysis and
blind deconvolution
2.1 Problem statement
Let S = {s[t]}t denote a set of independent identi-
cal source instances and X = {x[t]}t denote single
channel observations. Under assumption that X is a
convolutive or moving average result of the original
source, we have
x[t] =
τ−1X
i=0
ais[t− i], t ≥ τ (1)
where a = [a0, ..., aτ−1]T denotes a convolution struc-
ture and T denotes matrix transition. Equation (1)
defines the moving average convolution(Mac) model.
Blind deconvolution is defined to seek both S and a
for given only X.
2.2 Nonlinear recursion
To resolve blind deconvolution, this work applies
learning multilayer neural networks to explore non-
linear recursion of given observations. The following
1
3 Multilayer Potts perceptrons
Multilayer Potts perceptrons(MLPotts)[8] are re-
viewed in this section. MLPotts learning will be fur-
ther applied to resolve blind deconvolution in the up-
coming section.
The MLPotts network is composed of Potts per-
ceptrons that employ the K-state transfer function
to encode the external field,
g(h; c,σ2) = (g1(h; c,σ
2), ..., gK(h; c,σ
2))T , (4)
where
gk(h; c,σ2) =
f(h; ck, σ2)
KP
l=1
f(h; cl, σ2)
.
c denotes a knot vector and f(h; ck, σ2) denotes a
normal probability density function with mean ck and
covariance σ2.
Sigmoid-like transfer functions of traditional per-
ceptrons can be expressed in terms of weighted K-
state transfer functions, such as
tanh(βh) = cTg(h; c,σ2),
when K = 2, c = (−1 1)T and σ2 = 1β . Traditional
perceptrons can be exactly emulated by Potts per-
ceptrons.
Both MLPotts and MLP are projective neural net-
works. The external field is formed by projecting a
vector input x on a receptive field wm,
hm = wTmx,
where m indices Potts perceptrons in an MLPotts
network. The function of an MLPotts network is
expressed to sum up multiple post-nonlinear projec-
tions,
GK(x;θ) =
MX
m=1
ψ(wTmx; rm, cm,σ
2
m)
=
MX
m=1
rTmg(w
T
mx; cm, σ
2
m), (5)
where wm, cm, σ2m and rm respectively represent the
receptive field, knot vector, common variance and
posterior weight of a Potts perceptron and θ denotes
collection of all network parameters.
An MLPotts network reduces to an MLP network
in case ofK = 2, σ2m = 1, cm= (−1 1)T and rm= (rm
rm)T for all m. In the occasion, the network function
becomes
G2(x;θ) =
MX
m=1
rm tanh(w
T
mx). (6)
4 Numerical simulations
We test the proposed novel method for time series
analysis, blind separation and blind deconvolution in
this section. We employ the Levenberg and Mar-
quardt(LM) method to train the MLPotts network
for reconstruction of the nonlinear recursive structure
embedded within given single channel observations.
4.1 Random source
In this subsection, time instances of the independent
source is a sample from a uniform distribution. We
realize the nonlinear recursive structure by MLPotts
learning. The details of testing the proposed novel
method is summarized by the following procedure.
1. Use matlab built-in function rand.m to generate
a set of instances, S = {s[t]}t.
2. Ask the user to specify a filter, a =
[a0, ..., aτ−1]T by selecting periodic triangular
functions, such as sin, cos or tanh, and the length
of the filter, τ .
3. Convolve S through a convolution structure a to
create single channel observations, {x[t]}Nt=τ+1.
4. Form paired data, D = {(ex[t], x[t])}t,
where ex[t] = (x[t− τ ], ..., x[t− 1])T .
5. Apply MLPotts learning [9] to train an MLPotts
network subject to paired data in D. Let θopt
denote parameters of the obtained MLPotts net-
work.
6. Set bx[t] to GK(ex[t];θ) for all t.
3
Figure 4: Blue: unknown convolution model. Red:
reconstructed convolution model
Figure 5: Blue: unknown convolution model. Red:
reconstructed convolution model
Figure 6: HMM-generated independent source S,
unknown convolution model, single-channel observa-
tions X and the filter estimated by fdeconv(X,S)
to a convolution model results in single-channel ob-
servations that are processed by the proposed novel
method.
The HMM model has two hidden states. The tran-
sition probabilities of hidden states are denoted by
A =
µ
π11 π12
π21 π22
¶
,
where πij denotes the transition probability from
state i to state j.
The matlab program that simulates data genera-
tion of HMM was download from the web site of [18].
Figures 6-8 show numerical results for this example.
In figure 8 the unknown convolution model is success-
fully estimated.
Let {bs[t]} denote the estimated independent
source. We define the error rate by
λ =
1
2N
X
t
|s[t]− ξ(bs[t])| ,
where ξ denotes the sign function. In the previous
example, for τ = 20, the proposed novel method with
M = 3,K = 31 attains bs[t] whose λ value equals
0.003636.
5
Neural Computation, vol. 9, no. 7, pp. 1483-
1492, Oct. 1997.
[8] J.M. Wu, "Multilayer Potts percetrons with
Levenberg-Marquardt", to appear in IEEE
Trans. on Neural Networks.
[9] Magnus Norgaard, Ole Ravn, Niels K. Poulsen
and Lars K. Hansen "Homepage for the
Book:Neural Networks for Modelling and Con-
trol of Dynamic Systems ",Springer-Verlag, Lon-
don, 2000
[10] Nan Xie and Henry Leung, Member, IEEE
"Blind Equalization Using a Predictive Radial
Basis
Function Neural Network"IEEE TRANSAC-
TIONS ON NEURAL NETWORKS, VOL. 16,
NO. 3, MAY 2005
[11] K. J. Pope*,† and R. E. Bogner "Blind Signal
Separation"DIGITAL SIGNAL PROCESSING
6, 5—16 (1996)
ARTICLE NO. 0002
[12] Yong Fang and Tommy W. S. Chow "Blind
Equalization of a Noisy Channel by Linear Neu-
ral Network"IEEE TRANSACTIONS ON NEU-
RAL NETWORKS, VOL. 10, NO. 4, JULY 1999
[13] Rajoo Pandey"Feedforward neural network for
blind equalization with PSK signals"Neural
Comput & Applic (2005) 14: 290—298 DOI
10.1007/s00521-004-0465-5
[14] SHUN-ICHI AMARI, FELLOW, AND AN-
DRZEJ CICHOCKI, MEMBER "Adaptive
Blind Signal Processing–Neural Network Ap-
proaches"
[15] Carles Anton-Haro, Jose A. R. Fonollosa, and
Javier R. Fonollosa "Blind Channel Estima-
tion and Data Detection Using Hidden Markov
Models" IEEE TRANSACTIONS ON SIGNAL
PROCESSING, VOL. 45, NO. 1, JANUARY
1997
[16] Simone Fiori "Fast Fixed-Point Neural Blind-
Deconvolution Algorithm"IEEE TRANSAC-
TIONS ON NEURAL NETWORKS, VOL. 15,
NO. 2, MARCH 2004
[17] Simone Fiori "Geodesic-based and projection-
based neural blind deconvolutionalgorithms"
Received 31 October 2006; received in revised
form 18 May 2007; accepted 28 August 2007
Available online 4 September 2007
[18] Olivier Cappe "A set of MATLAB functions for
blind deconvolution of discrete signals "ENST
Dpt. TSI / CNRS-URA 820 46 rue Barrault,
75634 Paris cedex 13, France. cappe at tsi.enst.fr
November 6, 1998
7
報告內容應包括下列各項： 
一、參加會議經過 
我在會議開始的前兩天，也就是當地時間 6月 27 日傍晚飛抵位於美國佛羅里達洲的奧蘭多市，
調整時差並熟悉奧蘭多市公眾運輸系統後，於 6月 29 日至大會會場報到，隔天參加開幕式。因
為 CCCT 是 IMETI 2008 (The international multi-conference on engineering and 
technological Innovation)聯合會議的兩個主會議之一，所以除了電腦計算、通訊及控制等技
術領域外，還包括科學教育、運輸工程管理、人工智慧、cybernetics、數學、統計等相關領域
的學者參加。很幸運地，今年 IMETI 大會邀請到任教於芝加哥伊立諾大學數學、統計暨計算機
科學系 Professor Louis H. Kauffman(Department of Mathematics, Statistics, and Computer 
Science, University of Illinois at Chicago)擔任 keynote speaker，講演的主題是有名的
繩結理論(knot theory)。因為我在博士班時曾經讀過 Kauffman 教授的繩結理論著作，一直對
他的研究有著濃厚的興趣，特別是與量子力學相關的理論。在講演中 Kauffman 教授以深入淺出
的方式介紹繩結理論，並闡述與計算理論的 halting problem 的關聯性，並以繩結理論所發展
的相關方法，進一步對人類的自我下了一個的遞迴式定義，令人印象深刻。當天下午，我又參
加 Kauffman 教授兩個多小時的專題演講，我坐在第一位，也幫忙 Kauffman 教授裝置投影設備，
Kauffman 教授親自帶領四位自願的資深教授用兩條繩子操作 knot theory 的計算系統，其過程
隱然在展示一部具量子計算特性的計算架構與組織雛型，我覺得非常有趣，且獲益良多。除此
之外，我還選了幾個和智慧型計算有關的 session 聽演講。 
我的演講排在 7 月 1 日下午，同一 session 有兩位美國教授、一位韓國學者和我共發表四篇論
文，每人發表時間為三十分鐘。我在演講中，展示了許多獨立成份分析技術的研發成果，包括
應用 AEM 獨立成份分析技術分析孕婦心電圖以分離出胎兒心電圖、分析聽覺刺激產生的腦 ERP
訊號以探討不同刺激條件下獨立 ERP 訊號的可分辨性、以獨立成份分析進行線行混合臉影像解
密問題等。這一部份的內容主要是問題導向，我先丟出一個問題，show 出所要分析的原始資料，
再進一步說明應用我們多年研發的獨立成份分析技術可以有效的得到獨立訊號，解決相關問
題，並一一展示分析成果。接著我說明論文中所提出的 LOOK 近似法，並展示數值模擬成果。
除了人工迴旋混合訊號分析外，我想最能引起與會者注意的莫過於雙麥克風錄音分離應用。我
們所用的 benchmark 是沙克研究院所提供的雙麥克風下的人聲與搖滾音樂的混合錄音。這組訊
號相當具有公信力，因為沙克研究院的網站只公布錄音訊號，而沒有獨立音源。我逐一播放兩
組錄音訊號，在任一組錄音訊號中，與會者都可以同時聽到人聲和搖滾音樂聲，接著我播放經
過 LOOK 近似技術分析得到的獨立音源，與會者可以分別聽到其中一個音源都是人聲，而另一
個音源都是搖滾音樂聲。我的演講也在成果展示後進入尾聲。我很高興與會者專注參與，在他
們的問題中，我確信我所展示的成果已引起他們對獨立成份分析問題的興趣與關注。 
二、與會心得 
我覺得跨領域會議有助於不同領域研究學者間的交流，這次參加 IMETI 2008，能聽到 Kauffman
教授的專題演講，覺得獲益良多。有機會聽可惜年輕人參加的比較少。 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
 
 
 
四、建議 
 
 
 
 
 
五、攜回資料名稱及內容 
 
 
2II. Leave-one-out optimal kernel approximation
Since the convolutive structure has a great influence on forma-
tion of observations, its estimation is very crucial for retrieving
independent sources. The convolutive structure that emulates
transmission of an independent source to a channel of obser-
vations is associated with a linear kernel. The optimal kernel
based method that operates following the strategy of leave-one-
out approximation is shown feasible for blind separation of con-
volutive mixtures of independent sources.
A. Optimal kernel estimation
Assume
x [t ] =
τ−1X
i=0
Ais [t − i ] + n [t ] (7)
where all n [t ] denote Gaussian noises with zero mean and
a small common variance, and {Ai ∈ R}τ−1i=0 denotes a linear
kernel. Given one observation {x [t ] ∈ R}Tt=1 and one source
{s [t ] ∈ R}Tt=1 , optimal kernel estimation aims to find a linear
kernel which minimizes the following mean square error
E ({Ai}) = 1
T
TX
t=1
bn [t ]2 ,
=
1
T
TX
t=1
Ã
x [t ]−
τ−1X
i=0
Ais [t − i ]
!2
,
where the noise n [t ] is approximated by
bn [t ] = x [t ]− τ−1X
i=0
Ais [t − i ] . (8)
To minimize E, we set
∂E
∂Aj
= 0, j = 0 , . . . , τ − 1
Calculating the partial derivative of E with respect to each Ai ,
∂
∂Aj
1
T
TX
t=1
Ã
x [t ]−
τ−1X
i=0
Ais [t − i ]
!2
= 0, j = 0 , . . . , τ − 1 ,
we have the following linear equation,
TX
t=1
x [t ] s [t − j ] =
τ−1X
i=0
Ai
TX
t=1
s [t − i ] s[t − j ], j = 0 , . . . , τ − 1
(9)
Let b be a column vector with elements denoted by
bj=
TX
t=j+1
x[t ]s[t − j ].
Q be a matrix with rows denoted by
Qj+1 =
µ
TP
t=j+1
s[t ]s[t − j ] · · ·
TP
t=τ
s[t − τ + 1 ]s[t − j ]
¶
,
where j runs from 0 to τ −1 , A be a matrix with rows denoted
by Aj . We reform the linear equation (9) as follows
b = QA. (10)
If Q is non-singular, we get
A = Q−1b. (11)
Otherwise the optimal kernel could be approximated by
A = Q
³
Q
0
Q
´−1
Q
0
b. (12)
For multi-channel sources, we rewrite equation (7) as
x [t ] =
τ−1X
i=0
(A1is1 [t − i ] + · · ·+Adisd [t − i ]) + n [t ]
=
dX
l=1
τ−1X
i=0
Alisl [t − i ] + n [t ] (13)
where
{sl [t ] ∈ R}Tt=1 , l = 1 , . . . , d
denotes d independent sources and
{Ali ∈ R}τ−1i=0 , l = 1 , . . . , d
denotes a linear kernel through which d sources are transmit-
ted to form the observation. The optimal kernel estimation is
applicable for the case with multiple sources. In this occasion,
the correspondent matrices Q and b in the right hand side of
equation (11-12) can be further expressed.
B. Leave-one-out approximation
The source signals and mixing structure are unknown for
blind separation [16]. Following the mixing model in equation
(6), multi-channel observations are convolutive mixtures of in-
dependent sources. For real situation, each of them may be as-
sumed to have its own dominant source. Following the assump-
tion, subtracting from a single channel observation approxima-
tion of non-dominant sources through an optimal kernel estima-
tion could achieve a dominant source. The recurrent optimal
kernel method is proposed for blind separation of convolutive
mixtures of independent sources. The proposed method oper-
ates as an iterative process, by which all of currently-estimated
independent sources are separately updated in a random se-
quence at each iteration. For each observation, the correspon-
dent dominant source is set to compensate for the approximat-
ing error of the other intermediate non-dominant sources. The
proposed method is summarized by the following stepwise pro-
cedure and shown in figure 2
1. Given observations {Xi}di=1 , where Xi denotes the ith chan-
nel observation with length t ,
Xi = {Xi [1 ] , . . . ,Xi [T ]} .
2. Set Si (0 ) = Xi for i = 1 , . . . , d , and l = 1 , where Si (l)
denotes the ith independent source estimated at lth iteration,
Si (l) = {Si,l [1 ] , . . . ,Si,l [T ]} .
3. For i = 1 to d,
(a) Approximate Xi [t ] by
dP
j<i
A
0
j
eSj (l , t) + dP
j>i
A
0
j
eSj (l − 1 , t)
and use the equations (10-12) to determine all Aj with j 6= i ,
where Aj is a column vector of τ elements andeSj (l , t) = (Sj ,l [t − τ + 1 ] ,Sj ,l [t − τ + 2 ] , . . . ,Sj ,l [t ])0 ,
where t = 1 , . . . ,T .
(b) Set Si,l [t ] = Xi [t ]−
dP
j<i
A
0
j
eSj (l , t)− dP
j>i
A
0
j
eSj (l − 1 , t).
4. l = l+ 1 . If a halting condition holds, i.e.
1
Td
dX
i=1
|Si (l)− Si (l − 1 )| < 10−6 ,
terminate, otherwise goto step 3..
4where t = 1 , . . . ,T .
(b) Set Si,l [t ] = Xi [t ]−
dP
j<i
A
0
j
eSj (l , t)− dP
j>i
A
0
j
eSj (l − 1 , t).
(c) Set h = Si (l) and update Si (l) = g (h) by equation (18).
4. Increasing β by an annealing schedule and go to step 3. until
a terminating condition is satisfied, i.e.
1
Td
X
t
gK (h [t ] ; c)
0
gK (h [t ] ; c) > 0.7
by equation (17).
IV. Numerical simulations
We first test the optimal kernel estimation for signal approxi-
mation, and then show eﬀectiveness of the proposed convolutive
ICA algorithm for blind separation of convolutive mixtures of
independent sources. Artificial source data,
s [t ] = (s1 [t ] , s2 [t ] , s3 [t ] , s4 [t ])
0
=


sign(cos(2πt/155 ))
sin(2πt/800 )
sin(2πt/300 + 6 cos(2πt/60 ))
sin(2πt/90 )

 ,
for t = 1 , . . . , 2000 ,
which have been used by Amari et al. in [1] serve as independent
signals. We use matlab programs to simulate the proposed con-
volutive ICA algorithm and test its eﬀectiveness for blind sepa-
ration of artificially created observations and real world sound
recordings.
A. Signal approximation
If a single channel observation is known to be linear mixtures
of a set of independent sources, given both the observation and
sources, the optimal kernel estimation characterized by equa-
tion (10-12) for signal approximation is aimed to estimate the
unknown mixing structure. The artificial source data s [t ] used
in this subsection are shown in Figure 3. All Ali in equation
(13) are generated randomly and uniformly with n [t ] = 0 for all
distinct τ . Through the randomly created mixing structure, in-
dependent sources form one channel observation x [t ]. Then by
equation (10-12) we can estimate all Ali for signal approxima-
tion. For diﬀerent τ and source signals, numerical simulations
of the optimal kernel estimation for signal approximation are
described as follows.
1. τ = 10 : If four channel source signals are employed
s [t ] =


sign(cos(2πt/155 ))
sin(2πt/800 )
sin(2πt/300 + 6 cos(2πt/60 ))
sin(2πt/90 )


to generate one channel observation. The mean square error
between the given observation and approximating signal is listed
at the first row of Table I.
2. τ = 20 : The same experiment is repeated for τ = 20. The
mean square errors between the given observation and the ap-
proximating signal are given in the second row of Table I.
3. τ = 50 : The same experiment is repeated for τ = 50, and
the mean square errors between the given observation and the
obtained approximation are listed in Table I.
TABLE I
Mean square errors between observations with τ = 50.
Artificial source data Mean square error
Four channel source signals τ = 10 5.572647e-009
Four channel source signals τ = 20 2.507220e-008
Four channel source signals τ = 50 5.692306e-008
B. Convolutive independent component analysis
In this subsection, we test the proposed convolutive ICA al-
gorithm with both artificial data and recordings of music and
speech. In the artificial data, all observations are convolutive
mixtures of independent sources s [t ] through the mixing model
of equation (6) with d = 4 and τ = 1 , . . . , 6 . The recordings of
music and speech were downloaded from the homepage provided
by the author of the work [3]. The two sources are respectively
oriented from the speech of a man from one to ten in English
and rock musics. There are two microphones for recording these
signals simultaneously. The recordings are modeled as convo-
lutive mixtures of independent source signals with d = 2 and
τ = 61 through the structure described in equation (6).
B.1 Artificial data
All of the randomly generated Anm in equation (6) are re-
stricted to be diagonally dominated and non-negative. To re-
duce influence in amplification of observations, we normalize
all Anm such that kAnmk = 1 . Each channel of observations
is dominated by a distinct channel of source. By the proposed
convolutive ICA algorithm, blind separation of convolutive mix-
tures of independent sources with variant τ are described as
follows, where the parameters in equation (17-18) used in nu-
merical simulations are listed in Table II. With τ = 6 the linear
convolutive mixtures of artificial source data are shown in Fig-
ure 4, and the recovered sources are shown in Figure 5.
TABLE II
Parameters used in the convolutive ICA algorithm for variant τ .
K c iteration number cputime
τ = 1 41 [−1 , 1 ] 995 81.1250
τ = 2 41 [−1 .25 , 1 .25 ] 931 80.5000
τ = 3 41 [−1 .5 , 1 .5 ] 884 81.5313
τ = 4 41 [−1 .75 , 1 .75 ] 851 82.1094
τ = 5 41 [−2, 2] 834 82.5156
τ = 6 41 [−2.25, 2.25] 828 83.2031
B.2 Blind separation of real world signals
The proposed convolutive ICA algorithm is shown eﬀective
for blind separation of real world signals shown in Figure 7. We
will use the proposed convolutive ICA algorithm to separate
background musics from speeches of a man, trying to erase spe-
cific signals for noise cancellation. We use matlab programs to
read recording signals, and the amplification of output signals is
between −1 and 1 . Figure 6 shows the recording of two micro-
phones. The peak in the figure is the speech of a man and the
background music appears not oscillating strongly. The result
of separation is shown in Figure 7, where we can clearly identify
speech of a man and background music. In this case , we set
K= 21 and c ∈ [−1 , 1 ] for numerical simulations.
60 200 400 600 800 1000 1200 1400 1600 1800 20
-1
0
1
A
0 200 400 600 800 1000 1200 1400 1600 1800 20
-1
0
1
B
0 200 400 600 800 1000 1200 1400 1600 1800 20
-1
0
1
C
0 200 400 600 800 1000 1200 1400 1600 1800 20
-1
0
1
D
Fig. 3. Artificial independent source data. (A) sign(cos(2πt/155)). (B)
sin(2πt/800). (C) sin(2πt/300+6 cos(2πt/60)). (D) sin(2πt/90). t =
1, . . . , 2000.
0 200 400 600 800 1000 1200 1
-5
0
5
A
0 200 400 600 800 1000 1200 1
-5
0
5
B
0 200 400 600 800 1000 1200 1
-5
0
5
C
0 200 400 600 800 1000 1200 1
-5
0
5
D
Fig. 4. Linear convolutive mixtures of 4 independent sources with τ = 6.
0 200 400 600 800 1000 1200 1
-1
0
1
A
0 200 400 600 800 1000 1200 1
-1
0
1
B
0 200 400 600 800 1000 1200 1
-1
0
1
C
0 200 400 600 800 1000 1200 1
-1
0
1
D
Fig. 5. Recovering sources by convolutive ICA with τ = 6.
0 2 4 6 8
-1
-0.5
0
0.5
1
A
0 2 4 6 8
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
B
Fig. 6. The recordings of music and speech were downloaded from the
homepage provided by the author of the work [3]
0 2 4 6 8
-1
-0.5
0
0.5
1
A
0 2 4 6 8
-1
-0.5
0
0.5
1
B
Fig. 7. Separated music and speech by convolutive ICA with τ = 61.
報告內容應包括下列各項： 
一、參加會議經過 
我在會議開始的前兩天，也就是當地時間 6月 27 日傍晚飛抵位於美國佛羅里達洲的奧蘭多市，
調整時差並熟悉奧蘭多市公眾運輸系統後，於 6月 29 日至大會會場報到，隔天參加開幕式。因
為 CCCT 是 IMETI 2008 (The international multi-conference on engineering and 
technological Innovation)聯合會議的兩個主會議之一，所以除了電腦計算、通訊及控制等技
術領域外，還包括科學教育、運輸工程管理、人工智慧、cybernetics、數學、統計等相關領域
的學者參加。很幸運地，今年 IMETI 大會邀請到任教於芝加哥伊立諾大學數學、統計暨計算機
科學系 Professor Louis H. Kauffman(Department of Mathematics, Statistics, and Computer 
Science, University of Illinois at Chicago)擔任 keynote speaker，講演的主題是有名的
繩結理論(knot theory)。因為我在博士班時曾經讀過 Kauffman 教授的繩結理論著作，一直對
他的研究有著濃厚的興趣，特別是與量子力學相關的理論。在講演中 Kauffman 教授以深入淺出
的方式介紹繩結理論，並闡述與計算理論的 halting problem 的關聯性，並以繩結理論所發展
的相關方法，進一步對人類的自我下了一個的遞迴式定義，令人印象深刻。當天下午，我又參
加 Kauffman 教授兩個多小時的專題演講，我坐在第一位，也幫忙 Kauffman 教授裝置投影設備，
Kauffman 教授親自帶領四位自願的資深教授用兩條繩子操作 knot theory 的計算系統，其過程
隱然在展示一部具量子計算特性的計算架構與組織雛型，我覺得非常有趣，且獲益良多。除此
之外，我還選了幾個和智慧型計算有關的 session 聽演講。 
我的演講排在 7 月 1 日下午，同一 session 有兩位美國教授、一位韓國學者和我共發表四篇論
文，每人發表時間為三十分鐘。我在演講中，展示了許多獨立成份分析技術的研發成果，包括
應用 AEM 獨立成份分析技術分析孕婦心電圖以分離出胎兒心電圖、分析聽覺刺激產生的腦 ERP
訊號以探討不同刺激條件下獨立 ERP 訊號的可分辨性、以獨立成份分析進行線行混合臉影像解
密問題等。這一部份的內容主要是問題導向，我先丟出一個問題，show 出所要分析的原始資料，
再進一步說明應用我們多年研發的獨立成份分析技術可以有效的得到獨立訊號，解決相關問
題，並一一展示分析成果。接著我說明論文中所提出的 LOOK 近似法，並展示數值模擬成果。
除了人工迴旋混合訊號分析外，我想最能引起與會者注意的莫過於雙麥克風錄音分離應用。我
們所用的 benchmark 是沙克研究院所提供的雙麥克風下的人聲與搖滾音樂的混合錄音。這組訊
號相當具有公信力，因為沙克研究院的網站只公布錄音訊號，而沒有獨立音源。我逐一播放兩
組錄音訊號，在任一組錄音訊號中，與會者都可以同時聽到人聲和搖滾音樂聲，接著我播放經
過 LOOK 近似技術分析得到的獨立音源，與會者可以分別聽到其中一個音源都是人聲，而另一
個音源都是搖滾音樂聲。我的演講也在成果展示後進入尾聲。我很高興與會者專注參與，在他
們的問題中，我確信我所展示的成果已引起他們對獨立成份分析問題的興趣與關注。 
二、與會心得 
我覺得跨領域會議有助於不同領域研究學者間的交流，這次參加 IMETI 2008，能聽到 Kauffman
教授的專題演講，覺得獲益良多。有機會聽可惜年輕人參加的比較少。 
 
 
三、考察參觀活動(無是項活動者省略) 
 
 
 
 
 
四、建議 
 
 
 
 
 
五、攜回資料名稱及內容 
 
 
2II. Leave-one-out optimal kernel approximation
Since the convolutive structure has a great influence on forma-
tion of observations, its estimation is very crucial for retrieving
independent sources. The convolutive structure that emulates
transmission of an independent source to a channel of obser-
vations is associated with a linear kernel. The optimal kernel
based method that operates following the strategy of leave-one-
out approximation is shown feasible for blind separation of con-
volutive mixtures of independent sources.
A. Optimal kernel estimation
Assume
x [t ] =
τ−1X
i=0
Ais [t − i ] + n [t ] (7)
where all n [t ] denote Gaussian noises with zero mean and
a small common variance, and {Ai ∈ R}τ−1i=0 denotes a linear
kernel. Given one observation {x [t ] ∈ R}Tt=1 and one source
{s [t ] ∈ R}Tt=1 , optimal kernel estimation aims to find a linear
kernel which minimizes the following mean square error
E ({Ai}) = 1
T
TX
t=1
bn [t ]2 ,
=
1
T
TX
t=1
Ã
x [t ]−
τ−1X
i=0
Ais [t − i ]
!2
,
where the noise n [t ] is approximated by
bn [t ] = x [t ]− τ−1X
i=0
Ais [t − i ] . (8)
To minimize E, we set
∂E
∂Aj
= 0, j = 0 , . . . , τ − 1
Calculating the partial derivative of E with respect to each Ai ,
∂
∂Aj
1
T
TX
t=1
Ã
x [t ]−
τ−1X
i=0
Ais [t − i ]
!2
= 0, j = 0 , . . . , τ − 1 ,
we have the following linear equation,
TX
t=1
x [t ] s [t − j ] =
τ−1X
i=0
Ai
TX
t=1
s [t − i ] s[t − j ], j = 0 , . . . , τ − 1
(9)
Let b be a column vector with elements denoted by
bj=
TX
t=j+1
x[t ]s[t − j ].
Q be a matrix with rows denoted by
Qj+1 =
µ
TP
t=j+1
s[t ]s[t − j ] · · ·
TP
t=τ
s[t − τ + 1 ]s[t − j ]
¶
,
where j runs from 0 to τ −1 , A be a matrix with rows denoted
by Aj . We reform the linear equation (9) as follows
b = QA. (10)
If Q is non-singular, we get
A = Q−1b. (11)
Otherwise the optimal kernel could be approximated by
A = Q
³
Q
0
Q
´−1
Q
0
b. (12)
For multi-channel sources, we rewrite equation (7) as
x [t ] =
τ−1X
i=0
(A1is1 [t − i ] + · · ·+Adisd [t − i ]) + n [t ]
=
dX
l=1
τ−1X
i=0
Alisl [t − i ] + n [t ] (13)
where
{sl [t ] ∈ R}Tt=1 , l = 1 , . . . , d
denotes d independent sources and
{Ali ∈ R}τ−1i=0 , l = 1 , . . . , d
denotes a linear kernel through which d sources are transmit-
ted to form the observation. The optimal kernel estimation is
applicable for the case with multiple sources. In this occasion,
the correspondent matrices Q and b in the right hand side of
equation (11-12) can be further expressed.
B. Leave-one-out approximation
The source signals and mixing structure are unknown for
blind separation [16]. Following the mixing model in equation
(6), multi-channel observations are convolutive mixtures of in-
dependent sources. For real situation, each of them may be as-
sumed to have its own dominant source. Following the assump-
tion, subtracting from a single channel observation approxima-
tion of non-dominant sources through an optimal kernel estima-
tion could achieve a dominant source. The recurrent optimal
kernel method is proposed for blind separation of convolutive
mixtures of independent sources. The proposed method oper-
ates as an iterative process, by which all of currently-estimated
independent sources are separately updated in a random se-
quence at each iteration. For each observation, the correspon-
dent dominant source is set to compensate for the approximat-
ing error of the other intermediate non-dominant sources. The
proposed method is summarized by the following stepwise pro-
cedure and shown in figure 2
1. Given observations {Xi}di=1 , where Xi denotes the ith chan-
nel observation with length t ,
Xi = {Xi [1 ] , . . . ,Xi [T ]} .
2. Set Si (0 ) = Xi for i = 1 , . . . , d , and l = 1 , where Si (l)
denotes the ith independent source estimated at lth iteration,
Si (l) = {Si,l [1 ] , . . . ,Si,l [T ]} .
3. For i = 1 to d,
(a) Approximate Xi [t ] by
dP
j<i
A
0
j
eSj (l , t) + dP
j>i
A
0
j
eSj (l − 1 , t)
and use the equations (10-12) to determine all Aj with j 6= i ,
where Aj is a column vector of τ elements andeSj (l , t) = (Sj ,l [t − τ + 1 ] ,Sj ,l [t − τ + 2 ] , . . . ,Sj ,l [t ])0 ,
where t = 1 , . . . ,T .
(b) Set Si,l [t ] = Xi [t ]−
dP
j<i
A
0
j
eSj (l , t)− dP
j>i
A
0
j
eSj (l − 1 , t).
4. l = l+ 1 . If a halting condition holds, i.e.
1
Td
dX
i=1
|Si (l)− Si (l − 1 )| < 10−6 ,
terminate, otherwise goto step 3..
4where t = 1 , . . . ,T .
(b) Set Si,l [t ] = Xi [t ]−
dP
j<i
A
0
j
eSj (l , t)− dP
j>i
A
0
j
eSj (l − 1 , t).
(c) Set h = Si (l) and update Si (l) = g (h) by equation (18).
4. Increasing β by an annealing schedule and go to step 3. until
a terminating condition is satisfied, i.e.
1
Td
X
t
gK (h [t ] ; c)
0
gK (h [t ] ; c) > 0.7
by equation (17).
IV. Numerical simulations
We first test the optimal kernel estimation for signal approxi-
mation, and then show eﬀectiveness of the proposed convolutive
ICA algorithm for blind separation of convolutive mixtures of
independent sources. Artificial source data,
s [t ] = (s1 [t ] , s2 [t ] , s3 [t ] , s4 [t ])
0
=


sign(cos(2πt/155 ))
sin(2πt/800 )
sin(2πt/300 + 6 cos(2πt/60 ))
sin(2πt/90 )

 ,
for t = 1 , . . . , 2000 ,
which have been used by Amari et al. in [1] serve as independent
signals. We use matlab programs to simulate the proposed con-
volutive ICA algorithm and test its eﬀectiveness for blind sepa-
ration of artificially created observations and real world sound
recordings.
A. Signal approximation
If a single channel observation is known to be linear mixtures
of a set of independent sources, given both the observation and
sources, the optimal kernel estimation characterized by equa-
tion (10-12) for signal approximation is aimed to estimate the
unknown mixing structure. The artificial source data s [t ] used
in this subsection are shown in Figure 3. All Ali in equation
(13) are generated randomly and uniformly with n [t ] = 0 for all
distinct τ . Through the randomly created mixing structure, in-
dependent sources form one channel observation x [t ]. Then by
equation (10-12) we can estimate all Ali for signal approxima-
tion. For diﬀerent τ and source signals, numerical simulations
of the optimal kernel estimation for signal approximation are
described as follows.
1. τ = 10 : If four channel source signals are employed
s [t ] =


sign(cos(2πt/155 ))
sin(2πt/800 )
sin(2πt/300 + 6 cos(2πt/60 ))
sin(2πt/90 )


to generate one channel observation. The mean square error
between the given observation and approximating signal is listed
at the first row of Table I.
2. τ = 20 : The same experiment is repeated for τ = 20. The
mean square errors between the given observation and the ap-
proximating signal are given in the second row of Table I.
3. τ = 50 : The same experiment is repeated for τ = 50, and
the mean square errors between the given observation and the
obtained approximation are listed in Table I.
TABLE I
Mean square errors between observations with τ = 50.
Artificial source data Mean square error
Four channel source signals τ = 10 5.572647e-009
Four channel source signals τ = 20 2.507220e-008
Four channel source signals τ = 50 5.692306e-008
B. Convolutive independent component analysis
In this subsection, we test the proposed convolutive ICA al-
gorithm with both artificial data and recordings of music and
speech. In the artificial data, all observations are convolutive
mixtures of independent sources s [t ] through the mixing model
of equation (6) with d = 4 and τ = 1 , . . . , 6 . The recordings of
music and speech were downloaded from the homepage provided
by the author of the work [3]. The two sources are respectively
oriented from the speech of a man from one to ten in English
and rock musics. There are two microphones for recording these
signals simultaneously. The recordings are modeled as convo-
lutive mixtures of independent source signals with d = 2 and
τ = 61 through the structure described in equation (6).
B.1 Artificial data
All of the randomly generated Anm in equation (6) are re-
stricted to be diagonally dominated and non-negative. To re-
duce influence in amplification of observations, we normalize
all Anm such that kAnmk = 1 . Each channel of observations
is dominated by a distinct channel of source. By the proposed
convolutive ICA algorithm, blind separation of convolutive mix-
tures of independent sources with variant τ are described as
follows, where the parameters in equation (17-18) used in nu-
merical simulations are listed in Table II. With τ = 6 the linear
convolutive mixtures of artificial source data are shown in Fig-
ure 4, and the recovered sources are shown in Figure 5.
TABLE II
Parameters used in the convolutive ICA algorithm for variant τ .
K c iteration number cputime
τ = 1 41 [−1 , 1 ] 995 81.1250
τ = 2 41 [−1 .25 , 1 .25 ] 931 80.5000
τ = 3 41 [−1 .5 , 1 .5 ] 884 81.5313
τ = 4 41 [−1 .75 , 1 .75 ] 851 82.1094
τ = 5 41 [−2, 2] 834 82.5156
τ = 6 41 [−2.25, 2.25] 828 83.2031
B.2 Blind separation of real world signals
The proposed convolutive ICA algorithm is shown eﬀective
for blind separation of real world signals shown in Figure 7. We
will use the proposed convolutive ICA algorithm to separate
background musics from speeches of a man, trying to erase spe-
cific signals for noise cancellation. We use matlab programs to
read recording signals, and the amplification of output signals is
between −1 and 1 . Figure 6 shows the recording of two micro-
phones. The peak in the figure is the speech of a man and the
background music appears not oscillating strongly. The result
of separation is shown in Figure 7, where we can clearly identify
speech of a man and background music. In this case , we set
K= 21 and c ∈ [−1 , 1 ] for numerical simulations.
