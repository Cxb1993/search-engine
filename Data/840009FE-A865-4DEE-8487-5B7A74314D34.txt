行政院國家科學委員會專題研究計畫期中報告 
計算中難度與亂度的關係研究 (2/3)  
計畫編號：NSC 95-2221-E-001-015 
 執行期限：95 年 8 月 1日至 96 年 7 月 31 日 
主持人：呂及人   中央研究院資訊科學研究所 
 
一、中文摘要 
 
當今計算理論研究中最重要的問題之
一，乃是瞭解隨機性（randomness）對於
計算的幫助。是否所有多項式時間的隨機
演算法，都能被轉化為確定式的演算法？
一種典型的去隨機化的方法，乃是建構所
謂的虛擬亂數產生器。一個產生虛擬亂數
的重要方法，乃是由一個難以計算的函
數，將其難度轉換為隨機度。若要由一個
只稍具難度的函數出發，則關鍵的一步，
是要將此函數轉化為一個具有高度難度的
函數。在本計畫的第一年，我們證明此難
度放大程序若以黑盒式來進行，則其計算
需要很高的複雜度。 
在此計畫的第二年（今年度），我們證
明了若要以黑盒式來將難度由(1-δ)/2 放大
到(1-δk)/2，即使不限制難度放大程序的計
算複雜度，則難度放大的過程仍然會引進
Ω(klog(1/δ)) 單 位 的 非 一 致 性
（non-uniformity）。此外，我們也證明，
由難度轉換為亂度的程序，若以黑盒式來
進行，亦有類似的複雜度與非一致性的低
限。我們的證明方式，乃是建立起這種難
度放大程序（及難度轉換為亂度的程序）
與某種錯誤更正碼之間的關係，而後再證
明此種錯誤更正碼的極限。 
 
關鍵詞：亂度、難度、黑盒式難度放大程序、
常數深度線路、可列式錯誤更正碼 
 
Abstract 
 
One of the most fundamental problems is 
theoretical computer science is to understand 
the power of randomness in computation. 
Can all randomized polynomial-time 
algorithms be converted into deterministic 
ones? One potential way of doing 
derandomization is to construct the so-called 
pseudorandom generator, which stretchs a 
short random seed into a much longer 
“random-looking” string. An important 
paradigm of generating pseudo-randomness 
is to convert from the hardness of some 
computation problems. In complexity theory, 
a typical source of hardness comes from a 
Boolean function for which any small 
(polynomial) size circuit must make errors 
on some portion of the inputs. We define the 
hardness of a function as the fraction of 
errors any small circuit must make. One 
would like to start from a function that is 
only slightly hard, and then a key step is to 
amplify the initial hardness to obtain a new 
function which is very hard. In the first year 
of this project, we show that such hardness 
amplification procedures, when realized in a 
black-box way, basically require a high 
computational complexity to carry out.  
In the second year (this year), we show 
that such a black-box hardness amplification, 
from hardness (1-δ)/2 to hardness (1-δk)/2, 
must be inherently non-uniform, even without 
of f’ against much smaller circuits. 
Furthermore, one would like f’ to stay in the 
same complexity class of f, so that one could 
establish the relation among hardness 
assumptions within the same complexity 
class. 
Two issues come up from those works on 
hardness amplification of Boolean functions. 
The first is on the complexity of the 
amplification procedure. All previous 
amplification procedures going from 
worst-case hardness (α = 2-n) to average-case 
hardness (α’ = 1/2–2-Ω(n)) run in exponential 
time. As a result, such a hardness 
amplification is only known for functions of 
at least exponential time complexity. Then a 
natural question is: can it be done for 
functions in lower complexity classes. For 
example, given a function in NP which is 
worst-case hard, can we transform it into 
another function in NP which is average-case 
hard? Only for some range of hardness (e.g. 
starting from mild hardness, with α = 
1/poly(n)) is this known to be possible [21, 
12, 9, 13, 6]. The second issue is that 
hardness amplification typically involves 
non-uniformity in the sense that hardness is 
usually measured against non-uniform 
circuits. In fact, one usually needs to start 
from a function which is hard against 
non-uniform circuits, even if one only wants 
to produce a function which is hard against 
uniform Turing machines. This is why most 
results on derandomizing BPP are based on 
non-uniform assumptions (except [10, 18]). 
In light of the discussion above, one 
would hope to show that some hardness 
amplification of Boolean functions is indeed 
impossible. However, it is not clear what this 
means, especially given the possibility (in 
which many people believe) that 
average-case hard functions may indeed exist. 
One important type of hardness amplification 
is called black-box hardness amplification. 
First, the initial function f is only given as a 
black-box to construct the new function f’. 
That is, there is an oracle Turing machine 
Amp such that f’ = Ampf, so f’ only uses f as 
an oracle and does not depend on the internal 
structure of f. Second, the hardness of the 
new function f’ is proved in a black-box way. 
That is, there is an oracle Turing machine 
Dec, such that if some algorithm A computes 
f’ correctly on α’ fraction of the input, then 
Dec using A as an oracle can compute f 
correctly on α fraction of the input. Again, 
Dec only uses A as an oracle and does not 
depend on the internal structure of A. We call 
Amp the encoding procedure and Dec the 
decoding procedure. In fact, almost all 
previous hardness amplification results 
(except [10, 18]) are done in a black-box 
way, so it is nice to establish impossibility 
results for such type of hardness 
amplification.  
 
三、文獻探討 
 
Viola [20] gave the first lower bound on 
the complexity required for black-box 
hardness amplification of Boolean functions. 
He showed that to transform a worst-case 
hard function f into a mildly hard function f’, 
both against circuits of size 2o(n), the 
encoding procedure Amp can not possibly 
belong to the complexity class ATIME(O(1), 
2o(n))). This rules out the possibility of doing 
such hardness amplification in PH, which 
explains why previous such procedures all 
require a high computational complexity. He 
also showed a similar lower bound for 
black-box construction of PRG from a 
worst-case hard function.  
Trevisan and Vadhan [18] observed that a 
black-box hardness amplification from 
worst-case hardness corresponds to an 
of amplifying hardness beyond 1/4, the need 
of non-uniformity can be shown using the 
Plotkin bound [14] from coding theory. 
We consider hardness amplification in a 
general range and obtain a quantitative bound 
on the amount of non-uniformity. More 
precisely, we show that to amplify hardness 
from (1-δ)/2 to (1-ε)/2, the decoding function 
Dec must need an advice of Ω(log(δ2/ε)) bits. 
Thus, when ε =  δk, an advice of length Ω(k 
log(1/δ)) is necessary, and when ε ≤ cδ2 for 
some constant c, such hardness amplification 
must be inherently non-uniform. Our result 
generalizes that of Trevisan and Vadhan [18]. 
Finally, we consider black-box 
constructions of PRG from hard functions. For 
this, we use the connection between the 
error-reduction codes and PRG constructions. 
Then from the lower bounds we derive for 
such codes (when we consider hardness 
amplification), we immediately obtain similar 
lower bounds for black-box constructions of 
PRG from hard functions. 
 
六、參考文獻 
 
[1] Laszlo Babai, Lance Fortnow, Noam 
Nisan, and Avi Wigderson. BPP has 
subexponential time simulations unless 
EXPTIME has publishable proofs. 
Computational Complexity, 3(4), pages 
307--318, 1993. 
[2] Manuel Blum and Silvio Micali. How to 
generate cryptographically strong sequences 
of pseudo random bits. In Proceedings of the 
23rd Annual IEEE Symposium on 
Foundations of Computer Science, pages 
112--117, 1982. 
[3] Merrick L. Furst, James B. Saxe, and 
Michael Sipser. Parity, circuits, and the 
polynomial-time hierarchy. Mathematical 
Systems Theory, 17(1), pages 13--27, 1984. 
[4] Oded Goldreich, Noam Nisan, and Avi 
Wigderson. On Yao's XOR lemma. Technical 
Report TR95-050, Electronic Colloquium on 
Computational Complexity, 1995. 
[5] Johan Hastad. Computational limitations 
for small depth circuits. PhD thesis, MIT 
Press, 1986. 
[6] Alexander Healy, Salil P. Vadhan, and 
Emanuele Viola. Using nondeterminism to 
amplify hardness. In Proceedings of the 36th 
ACM Symposium on Theory of Computing, 
pages 192--201, 2004. 
[7] Russel Impagliazzo. Hard-core 
distributions for somewhat hard problems. In 
Proceedings of the 36th Annual IEEE 
Symposium on Foundations of Computer 
Science, pages 538--545, 1995. 
[8] Russell Impagliazzo, Ronen Shaltiel, and 
Avi Wigderson. Extractors and 
pseudo-random generators with optimal seed 
length. In Proceedings of the 32nd ACM 
Symposium on Theory of Computing, pages 
1--10, 2000. 
[9] Russel Impagliazzo and Avi Wigderson. 
P=BPP if E requires exponential circuits: 
derandomizing the XOR lemma. In 
Proceedings of the 29th ACM Symposium on 
Theory of Computing, pages 220--229, 1997. 
[10] Russel Impagliazzo and Avi Wigderson. 
Randomness vs. time: de-randomization 
under a uniform assumption. In Proceedings 
of the 39th Annual IEEE Symposium on 
Foundations of Computer Science, pages 
734--743, 1998. 
[11] Nathan Linial, Yishay Mansour, and 
Noam Nisan. Constant depth circuits, Fourier 
transform, and learnability. Journal of the 
ACM, 40(3), pages 607--620, 1993. 
[12] Noam Nisan and Avi Wigderson. 
Hardness vs randomness. Journal of 
Computing System Science, 49(2), pages 
149--167, 1994. 
出席國際學術會議心得報告 
                                                             
計畫編號 95-2221-E-001-005- 
計畫名稱 計算中難度與亂度的關係研究(2/3) 
出國人員姓名 
服務機關及職稱 
呂及人 
中央研究院副研究員 
會議時間地點 96 年 7 月 9 日至 7月 13 日  波蘭 Wroclaw 市 
會議名稱 34th International Colloquium on Automata, Languages and Programming(ICALP 2007) 
發表論文題目 On the Complexity of Hard-Core Set Constructions
 
一、參加會議經過 
The International Colloquium on Automata, Languages and Programming (ICALP)
會議為歐洲計算理論學會（EATCS）所主辦的年會，是歐洲計算理論研究的最重要會議，今年
已是第 34 屆。參與會議的學者不只來自歐洲，更有不少來自世界各地的頂尖學者。傳統的
ICALP 會議分為兩個 track：Track A - Algorithms, Automata, Complexity and Games、及
Track B - Logic, Semantics, and Theory of Programming。自前年起新增一個 track： Track 
C - Security and Cryptography Foundations。今年亦沿襲此形式，分為這三個 tracks。
這使得 ICALP 會議，涵蓋了計算理論研究中，幾乎所有重要的主題。相較之下，以美洲為主
的計算理論會議，則主要專注在 Track A 的主題。 
今年論文的接收數／投稿數分別為：Track A：41／149、Track B：24／59、Track C：
11／34。我的論文屬於 Track A，接受率約 27.5%，競爭相當激烈。我的論文發表在第一天，
題目為“On the Complexity of Hard-Core Set Constructions＂，研究計算理論中一個重
要概念 Hard-Core Set 的幾個複雜度問題。幾位與會者對我們的結果表示興趣，與我們有不
少的討論，讓我受益匪淺。 
此次會議內容，涵蓋了計算理論研究中各種重要的題目。此外也有六場的invited 
talks，分別由Bernard Chazelle、Ivan Damgård、Fedor Fomin、Gordon Plotkin、Michael 
O. Rabin、及Fred Schneider主講，講題分別為: Ushering in a New Era of Algorithm Design、
A "Proof-Reading" of some Issues in Cryptography、Subexponential Parameterized 
Algorithms、The Algebraic Theory of Effects、Highly Efficient Secrecy-Preserving 
Proofs of Correctness of Computations, and Applications 、及 Credentials-Based 
Authorization: Evaluation and Implementation。六位講者都是計算理論界的傑出學者，
演講內容也相當精采，涵蓋當前計算理論研究的各個重要領域，讓我有不少的收穫。 
在今年的會議中的兩個重頭戲，乃是並頒發了 2007 年的 EATCS Award 給 Dana Scott 教
授 (Carnegie Mellon University, USA)，以及頒發 Honorary Ph.D. degree 給 Michael O. 
Rabin 教授(Harvard University, US)，兩位恰巧是 1976 年 Turing Award 的共同得主。今
年與 ICALP 會議同時舉行的還有邏輯界的重要會議：22nd Annual IEEE Symposium on Logic 
in Computer Science (LICS)以及 Logic Colloquium 2007，此外還有不少相關的 workshops，
