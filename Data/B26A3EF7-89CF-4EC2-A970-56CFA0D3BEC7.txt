中文關鍵詞： 新世代自動語音辨識系統，發音特徵變化點，語音屬性，整
合式語音音節端點與屬性偵測器 
英 文 摘 要 ： In the next-generation automatic speech recognition 
paradigm, two types of speech detectors, i.e., 
landmark (to find the articulation change points in 
time) and attribute (to find the manner and place of 
the articulatory) detectors are the fundamental 
building blocks to reliably phone, word or phrase 
detection. Especially, landmark detectors are the 
most important front-end for the following ’event 
merge’ and ’evidence verification’ stages. 
 
In this project, we will focus on developing accurate 
and reliable landmark detectors and studying the 
optimal way to integrate them with our well-
established attribute detectors (done in previous 
projects). The following items will be carefully 
studied and implemented: 
 
(1) Phone boundary detector using sample-based 
acoustic parameter－ 
High-resolution sample-based landmark detectors will 
be developed using articulation parameters. The 
sample-based acoustic features were proposed to model 
the rapid spectral changes in speech signal. Both the 
precision and accuracy of the sample-based phone 
boundary detector were shown to be better than those 
of frame-based algorithms. 
(2) Force alignment of Mandarin － 
The proposed sample-based acoustic features were also 
used in the force alignment of Mandarin speech, two 
databases, TCC-300 and Treebank databases were force 
aligmnet in this project. 
(3) Force alignment of Hakka － 
Hakka were the most frequently used dialects in 
Taiwan. In this project, the cross-dialect capacities 
of the proposed sample-based acoustic festures were 
cross-examined using Hakka dialects database. 
(4) Applications of phone boundary detector using 
sample-based acoustic parameter － 
After phone boundary detection, the speech signal was 
1 
 
 
 
新世代自動語音辨識技術–第二階段 
– 國語及方言之音節階層事件偵測及其相關研究 
 
 
計畫編號：97-2221-E-009-080-MY3 
 
  
3 
 
4.1.2 國語 Treebank 語料庫簡介 .....................................................................................33 
4.1.3 客語語料庫簡介......................................................................................................34 
4.2 類音素標示位置起始值....................................................................................................34 
4.3 TCC300 語料庫實驗結果分析 ..........................................................................................41 
4.4 Treebank 語料庫實驗結果分析 .........................................................................................44 
4.5 使用客語四縣語料庫之實驗結果....................................................................................46 
4.5.1 音素端點偵測實驗結果..........................................................................................46 
4.5.2 自動語音分段實驗結果..........................................................................................47 
第五章 使用音段式語音發音方法辨認器.....................................................................................49 
5.1 音段式發音方法辨認器之參數抽取................................................................................49 
5.2使用音段式發音法法辨認器辨認結果.............................................................................50 
計畫成果自評...................................................................................................................................52 
完成工作項目...........................................................................................................................52 
已發表之論文...........................................................................................................................53 
參考文獻...........................................................................................................................................54 
附錄一...............................................................................................................................................56 
附錄二...............................................................................................................................................57 
附錄三...............................................................................................................................................59 
 
 
  
5 
 
工程的觀點去探討語言學上的一些現象。 
 
 
  
7 
 
 
In brief summary, the cross-dialect boundary and attribute detector proposed in this sub-project 
will provide other sub-projects the necessary components to successfully build the next-generation 
automatic speech recognition paradigm. Moreover, the proposed sample-based acoustic parameters 
will be cross-examined with linguistic knowledge. 
9 
 
之內的比率高達 79.75%，與傳統ML-trained HMM 模型其百分比 71.23%相比，提昇許多；
然而其自動音素分段位置只有 7.89%的邊界在人工標記位置誤差 20 ms 之外。此外，也可進
一步使用其它圖形識別的方法如支撐向量機[4]（Support Vector Machine，SVM）、類神經網
路[5]（Neural Network，NN），皆可用來對 HMM 之自動分段位置再作進一步地修正以獲得
更好的結果。 
而在Metric-based 方法中，我們知道語音信號在一個音素中穩定的信號，其聲學參數變
化的速率就是決定一個音素邊界的重要線索，回顧一些文獻如 Rabiner[6]使用頻譜轉換量測
（Spectral transition measure）的音素端點偵測方法，應用在 TIMIT 語料庫[7]其效能可達到
在誤差 20ms 的容忍範圍內，只有 15%的音素端點位置為偵測漏失 （Missed Detection rate，
MD）、22.0% 誤報率（False Alarm rate，FA）。Kotropoulos[8]結合 Kullback-Leibler（KL）距
離及貝式資訊法則（Bayesian Information Criterion，BIC）所提出的 DISTBIC 演算法來偵測
語音信號之音素邊界端點，其效能在 NTIMIT 語料庫亦可達到 25.7% MD 與 23.3% FA的結
果。 
在先前的語音分段或是端點偵測的研究，無論 model-based 或 metric-based 的方法中，
常用的語音信號參數多與信號頻譜相關；這些參數描述了發音特徵使得語音信號的特性不同，
且一般假設語音信號在短時間內為穩定的特性，故使用音框式（frame-based）的聲學參數，
例如梅爾倒頻譜係數（Mel-Frequency Cepstral Coefficients，MFCCs）。然而，在做頻譜分析
時會造成時間與頻譜（time-spectrum）上之不確定性（uncertain），所以頻譜參數越精確就會
犧牲時間精確度；但在音框式的架構中必須要讓頻譜解析度越精細，以提昇辨認音素能力，
而發音器官變化很快的音素如爆破音，其音長可能小於一個音框，使得音框式的方法之語音
分段位置與實際正確端點位置之間產生誤差，因此對於音素端點偵測及自動語音分段之研究
來說，提昇時間解析度，必可降低大量因音框之時間解析度所造成的誤差。 
除此之外，在李錦輝教授所提出之 detection-based ASR 中，我們認為 phone boundary 
detection 擔任了一個提出系統”同步信號”的重要腳色，如圖 1.1 所示。有了 phone boundary
資訊後，不論語音特徵偵測器(attribution detector)或語音辨認的解碼(decoder)工作都可以同步
進行，將有助於提升系統效能。而使用語音信號取樣點為單位的 phone boundary detector 更
可大幅提高同步信號的精確性。 
 
 
圖 1.1: 使用 phone boundary detection 的同步語音解碼系統示意圖 
11 
 
transform）來求取輸入信號的波封是一個適當且普遍的方法，其中 ( [ ])H x n 為輸入信號 [ ]x n 的
希爾伯特變換，若輸入信號為頻段之能量 [ ]x n ，其 ( [ ])H x n 即為語言學家所使用信號波封，
如下式： 
  [ ][ ] [ ] [ ] [ ] = [ ]    for 1, ,6ij ni i i d iy n x n j x n h n e n e i
      
其中 
  
 1 / ,      i s  o d d  a n d  0 2
[ ]
0,                      otherwise
d
n N n n N
h n
  
 

         (2-1) 
圖 2.1即為語音信號經波封檢測器輸出之波封結果，其表示語音信號的輪廓，但是觀察
輪廓時卻沒有明顯的規則可做為分辨音素端點的依據，故轉而觀察語音信號在使用六個頻段
中之分佈，並依此分佈之特性來區分不同的音素。 
 
圖 2.1：取樣式語音波封聲學參數範例，由上至下分別表示音素層級之人為時間標記的文字
轉寫、語音信號、音高軌跡（pitch contour）、語音信號之波封 
另外，考慮語音信號之波封受到喉頭震動的影響（尤其在音高較低的男性影響越顯著），
其會造成語音信號的特性與喉頭震動的周期產生某種程度的關聯性或是造成語音信號的不
連貫性，使得波封出現不是預期該有的波動而產生失真。為避免如以上所述之影響，藉由調
整波封檢測器的低通濾波器頻寬（passband bandwidth）、截止頻率的衰減斜率來達到其參數
物理意義之目的。由簡單的頻寬-濾波器階數定性分析發現，低通濾波器頻寬在 30Hz 至 50Hz
之間並使用相同之濾波器階數，其語音信號波封的輸出結果沒有太大的差異，但其波封變動
卻與不同之濾波器階數影響最大，圖 2.2即是顯現出以上所述之觀察結果。 
13 
 
上升率來分別找到對應每個頻段其信號波封變動量大的端點。如圖 2.4各頻段的波封上升率
可以對應於聲譜圖（spectrogram）的顏色深淺程度，也就對應至各頻段信號波封的大小變化；
語音信號在六個頻段之中之分佈由強(亮)轉弱(灰暗)，其轉變程度越大上升率越高。然而，
觀察每個頻段之波封上升率為局部最大值之端點，其會因為信號波封變動量的不同而使得在
某一段時間內各頻段之端點位置並不一致，要如何在此一區段時間選擇一個適當的音素轉換
端點，將在下節討論。 
 
 
圖 2.3：取樣式聲學參數之上升率範例，由上至下分別表示音素層級之人為時間標記的文字
轉寫、語音信號、聲譜圖、語音信號之波封、波封之上升率 
15 
 
間的變動，求取頻譜熵的上升率。 
 
圖 2.5：取樣式頻譜熵聲學參數範例，由上至下分別表示音素層級之人為時間標記的文字轉
寫、語音信號、聲譜圖、頻譜熵、頻譜熵之上升率 
2.1.3 頻譜 KL距離 
將頻譜視為一個機率分佈的問題，因此可以利用頻譜 KL 距離來描述兩段時間點之頻譜
相似程度。在語音信號中計算兩點不同時間(n 與 m)的頻譜 KL距離， ( , )KLd n m ，可以由下式
表示： 
   
6
1
[ ]
, [ ] [ ] log
[ ]
i
KL i i
i i
E n
d n m E n E m
E m
 
   
 
          (2-5) 
而本研究目前為考慮相鄰語音信號取樣點之頻譜信號分佈特性，則將(2-5)式改寫為以下： 
   
6
1
[ ]
[ ] [ 1] log
[ 1]
i
KL i i
i i
E n
d n E n E n
E n
 
    
 
          (2-6) 
不同音素轉換的時候，其發音的方法或是部位也會跟著轉移，使得不同音素之語音信號
轉換至頻譜上的分布情形也會跟著不同，頻譜 KL距離即是度量在頻譜間的相似程度，且此
一度量之特性具有一致性。那麼經由簡單調整一個臨限值（threshold），即可初步地得到一序
列（sequence）經由頻譜 KL距離所挑選出來是具有音素端點可能性的位置。 
藉由聲譜圖可以清楚地觀察到在相鄰音素之間的信號分佈變化，如圖 2.6 中同一音素內
之頻譜信號分佈為局部穩定的狀態，並在不同音素轉換的區域音其頻譜分佈差異大，使頻譜
KL距離明顯增大。 
17 
 
 
圖 2.7：不同階數之波封檢測器對頻譜 KL距離的影響，由上至下分別表示波封檢測器使用
40Hz 之 641階、321階、161階 FIR 低通濾波器輸出結果所計算的頻譜 KL距離、音素層級
之人為時間標記的文字轉寫、語音信號、聲譜圖 
  
40 Hz  
19 
 
起始時間（voice onset time，VOT），指的是爆破音成阻後持阻到除阻時間，語音學上會將此
段短停頓的產生視為爆破音時長的一部份。但在音素端點的偵測內，其語音信號的特性上卻
是有著極大的差異。故 TIMIT 語料庫的音素時間標記將此種情形也納入音素時間標記的範
疇中，而對該爆破音之標音前的短停頓給予合適的標記符號，其對應的標記符號如下表 3.1。 
另外，我們知道英語為 consonant-vowel-consonant 之音節結構，簡稱為 CVC。例如以
（rime structure）表示單音節的英文詞 cat，其音節頭（onset）為“c”，音節核為“a”，音節尾
（coda）為“t”。而子音在 CVC 音節結構內的位置不同會其發音也不盡相同，以本計畫之音
素端點偵測的觀點，我們無須了解其音素在結構內的關係，但若以音素端點切割的方面考量，
就必須考慮音節結構對音素端點的影響。 
 
圖 3.1：音素層級之文字轉寫對應於語音信號的人為時間標記 
 
表 3.1：爆破音對應之短停頓標記符號。 
stops b d g p t k jh ch 
closure intervals bcl dcl gcl pcl tcl kcl dcl tcl 
 
TIMIT 語料庫之訓練語料與測試語料分別為 462 位語者之 4620 個語句與 168 位語者
1680個語句所建構而成，在本計畫中使用音素層級之文字轉寫的人為時間標記之所有訓練語
料來訓練音素端點偵測器的模型，並以測試語料驗證本計畫所提出方法之效能。 
 
3.2 音素端點偵測系統 
儘管在不同語言之中，人類的發音系統之構造對語音的影響，在一段語句內即顯現出其
音素的語音特性皆與發音部位以及發音方法有非常大的關聯性。由第二章所述，本計畫提出
取樣點式聲學參數的聲學特性來描述這些語音信號中不同語音屬性的變化，藉由量測這些變
化來找出可能為音素端點的位置，這意謂著進行語音的標記中並不需要完整的音素辨認流程，
也不需使用到非常準確的音素標記位置，即可簡化語料庫繁複處理的過程。 
端點偵測器以音素層級之人為時間標記文字轉寫來訂定目標函數的兩種轉移狀態，分別
為音素端點（T）、非音素端點（nT），對所有由候選端預選（Candidate Pre-selection）所選取
之候選端點對應文字轉寫標記目標函數的種類，並用於端點偵測器的訓練。其中，對於每個
候選端點其包含了自身端點的聲學特性及其與前後相鄰候選端點之間的音段聲學特性，最後
經由多層感知器的學習特性，反覆疊代訓練將音素端點與非音素端點的語音特性做分類，並
藉此模型達到音素端點偵測的目的。 
21 
 
         1 ,  1  and KL KL KL KL KL dd n d n d n d n d n Th           (3-2) 
則代表為挑選出來的候選端點值，最後得到這一序列音素的候選端點， ; 1,...,jc j N 。 
經過預選擇步驟後，會將音素端點候選者之數目大量降低，也就是可以降低音素端點偵
測器之運算量。 
在此實驗過程中依照觀察頻譜 KL距離與人為時間標記之間的關係發現一些現象，舉例
來說對於人為時間標記中之摩擦音至母音、流音之間的音素轉換端點，在聲譜圖中可觀察到
端點兩邊頻譜信號分佈的差異極大如圖 3.2 中的（/k/-/l/）、（/t/-/ix/）之轉換端點，圖中可以
看到人為時間標記的位置並不一定是相鄰區域中頻譜 KL距離局部極大值的端點，而是黑色
箭頭所指向的端點；另外，圖中偏右旁的（/k/-/l/）音素轉換端點之相鄰區域中並無特別大的
頻譜 KL距離，那麼要如何選擇最適當的音素候選端點能減少訓練音素端點偵測器所需要達
到收斂的次數? 此問題即為先前所描述其人為時間標記之語料庫其標音員之主觀性所產生
時間標記位置之不一致性的問題。 
 
圖 3.2：調整音素候選端點之範例，由上至下分別表示音素層級之人為時間標記的文字轉寫、
語音信號、聲譜圖、音素候選端點、頻譜 KL 距離 
因此，本計畫提出一個演算法用以挑選出候選端點序列中最佳的音素候選端點作為半監
督式學習的目標（Target）。 
其演算法的敘述如下： 
(1) 在人為之時間標記音素端點之相鄰區域選擇適當的範圍，本計畫使用相鄰音素端點
之中點作為上限（Upper bound，UB）與下限（Lower bound，LB）且前後以不超過
30毫秒的範圍作為挑選候選端點的區域 R。 
(2) 在區域R內頻譜KL距離挑選出來之候選端點即為第 i個音素端點之候選端點子序列
 , ; 1,...,i jc j k ，並將此子序列依候選端點與該音素端點之距離由近至遠排序。 
23 
 
 
另外，我們對候選端點，
jc ，左右各取一小段語音信號來量測其相似度。這兩小段信號
的區間，
jB
及
jB
，它們分別表示為 
[ , 1]j j j jB c r c
     , [ , ]j j j jB c c r
   ,  
其中 
jr

 
and 
jr
  為兩的區間內的語音樣本數分別為  
min 1 min
1 min 1 max
max max 1
,                1
1 ,    1
,                1
j j
j j j j j
j j
r c c r
r c c r c c r
r r c c


 

   

      

  
 
及 
min 1 min
1 min 1 max
max max 1
,           
,   
,          
j j
j j j j j
j j
r c c r
r c c r c c r
r r c c


 

  

    

 
 
其中 maxr 及 minr 及音段最大及最小長度。如果我們將此兩音段之子頻段信號波封參數視為高
斯分布。則我們可以使用下列 KL 距離來描述這兩音段的相似度， 
1 1 1 11 1[ ] [( )( )] [( ) ( )( )]
2 2
T
KL jD c tr tr    
   
                         (3-4) 
上式中，及  分別表示兩音段子頻段信號波封參數之平均向量；  及  為子頻段信號
波封參數之變異矩陣。
 
接著，考慮相鄰候選端點之時間關聯性與其端點間語音特性之相關性，對於每個候選端
點建立一個 27 維的聲學參數向量，對於第 k 個候選端點，
kc ，其聲學參數向量包括以下聲
學參數： 
(1) 目前候選端點及前、後候選端點之參數： 
 [ ],  [ ],  [ ],  [ ],  [ ]; 0, ,6KL k KL k k k i kd c D c H c H c E c i   
其中 [ ]s jH c 為頻譜熵之一階差量。 
(2) 目前音段及前、後音段之參數： 
    1 1 1 1, ,  , ; 1, ,6 ,  ,  i k k i k k k k k kES c c ES c c i c c c c       
其中
1 1,  k k k kc c c c   表示目前端點與前後相鄰端點之時間資訊。 
最後，由語音信號所抽取之每個聲學參數向量皆存在聲學參數檔案內，以提供後級音素
25 
 
 Step5：重覆 Step1到 Step4至收斂 
MLP-based phone 
boundaries detector
Target Function 
Re-labeling
Manual labeled 
boundary
[                         ]
Candidates from objective 
measures
Acoustic parameters 
of candidates
Target Function
Detector output
 
圖 3.5：音素端點偵測器模型反覆疊代之流程圖 
 
3.4 音素端點偵測實驗結果分析 
使用 TIMIT 語料庫來驗證本計畫提出音素端點偵測器的偵測效能，並依照 TIMIT 語料
庫所建議訓練語料 4620 個語句及測試語料 1680 個語句的分類，用於音素偵測實驗。首先，
表 3.2統計了訓練語料與測試語料所處理的語音取樣點、音素邊界候選端點（Candidate）以
及語料中所要偵測之音素邊界總數（Phone boundary）的數據，由此可推得訓練語料約 1314
個語音取樣點也就是平均約 82.125毫秒有一個音素端點的存在，而測試語料則是平均每音素
端點相隔約 82.83 毫秒，皆與平均音素長度為 50至 100毫秒或是約為 5~10個音框長度的統
計量相符；挑選音素邊界候選端點時適當設定臨限值，分別在訓練語料及測試語料挑選出
534189與 194201 個可能為音素邊界的候選端點，以提供音素端點偵測器的訓練及測試。 
在實驗中，我們使用了兩種類神經網路架構，多層感知器(MLP)及 Recurrent Neural 
Network(RNN)，其隱藏層神經元數目分別為 75 及 80 個。最後比對經人為標記的音素層級
之文字轉寫而得到偵測音素邊界端點其誤報率與偵測漏失率相等時之錯誤率（Equal error 
rate，EER）效能為 11.6%與 8.6%。而偵測漏失率與誤報率的定義如下式表示： 
偵測漏失率為未偵測到之音素邊界端點個數 D在總音素邊界端點個數 N 中所佔的比例。 
Miss Detection rate 100
D
%
N
              (3-5) 
誤報率表示誤偵測為音素邊界端點個數 I 在總音素邊界端點個數 N 與 I 之總和中所佔的比
例。 
False Alarm rate 100
I
%
I N
 

            (3-6) 
27 
 
邊界的比例，並計算被偵測到音素端點落在相同或是相鄰音框之內的包含比例，以評量本計
畫之音素端點偵測器之效能好壞。其中表 3.3 顯示在 EER 的情形下，偵測到的音素邊界端點
在不同絕對偏差值內（5、10、15毫秒）的包含比率，而在相同音框內為 41.72%，相鄰音框
範圍內為 87.32%，兩種評量之實驗結果皆優於 Rabiner（27%/ 10ms, 70%/ 20ms），可易見時
間解析度較細的取樣點式的音素端點偵測方法有較高的效能。圖 3.7顯示了音素端點偵測器
之實驗結果與人為標記之間的差異在不同絕對偏差值的差異的區間內，佔有總音素端點個數
的比例。絕對偏差值越小代表著與人為標記位置越相近，亦表示偵測出之音素候選端點越準
確。  
表 3.3：使用音框式計算音素邊界偵測結果的方式的統計結果，音框平移為 10ms 
Methods In the same frame within 1 frame 
HMM 27.5% 67.3% 
Rabiner’s [17] 22.8% 59.2% 
MLP 36.0% 73.9% 
RNN 37.3% 77.0% 
 
圖 3.7：音素端點偵測器實驗結果與人為標記之絕對偏差值直方圖 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
< 5 ms < 10 ms < 15 ms < 20 ms < 25 ms < 30 ms < 35 ms < 40 ms < 45 ms < 50 ms 
In
cl
u
si
o
n
 r
a
te
  
Absolute Boundary Error 
HMM MLP(EER) RNN(EER) 
29 
 
(1) 前後相鄰音素為摩擦音 
摩擦音發音時會由於發音器官彼此靠攏而形成狹窄的氣流通道，使得氣流通過通道時造
成摩擦產生出聲音，如發出 s 的音必須讓氣流通過閉合牙齒之間的縫隙來產生。摩擦音在頻
譜上的分佈多集中在高頻部分。圖 3.8 舉出前後音素為（/k/、/s/）皆屬於摩擦音的分類，由
音素端點偵測器輸出概似度的觀察中，在（/k/、/s/）音素的區間中所有的音素候選端點之概
似度皆非常地低，亦即偵測器不認為這些候選端點是音素的端點。 
 
圖 3.8：音素端點偵測前後音素為摩擦音之範例，由上至下分別表示音素層級之人為時間標
記的文字轉寫、語音信號、聲譜圖、音素端點偵測器輸出之概似度 
(2) 前後相鄰音素為鼻音 
鼻音發音時口腔中的軟顎下垂，造成氣流無法通往口腔而轉往通過鼻腔發音，如發出/m/
的音時，須雙唇緊閉來讓氣流通過鼻腔產生，也因為如此使得鼻音在頻譜上的分佈多集中在
聲譜圖之低頻部分。圖 3.9 舉出前後音素為（/m/、/n/）皆屬於鼻音的分類，在（/m/、/n/）
音素的區間中，相鄰音素頻譜間平滑的變化造成音素候選端點的個數較少；僅觀察語音波形
也亦難標記正確的音素端點位置，這也就是前後音素為鼻音時偵測漏失率較高的原因之一。
即便音素端點偵測器輸出概似度藉由調整臨限值後，增加偵測出候選端點之個數，其音素候
選端點仍與人為標記位置有一段誤差存在。 
 
圖 3.9：音素端點偵測前後音素為鼻音之範例，由上至下分別表示音素層級之人為時間標記
的文字轉寫、語音信號、聲譜圖、音素端點偵測器輸出之概似度 
(3) 前後相鄰音素為母音 
31 
 
比的，亦即在不同的發音方式的轉換期間語音信號頻譜的劇烈變化容易產生誤報的情形，然
而本計畫以取樣點式聲學參數挑選音素候選端點的方式與傳統音框解析度對照之下，在此情
況卻是更加容易產生較多的音素候選端點，可能造成誤報率增高的情形。故以下分析在前後
音素為不同發音方法時誤報率的差異並作討論。塞擦音、摩擦音以及母音等發音方式之邊
界。 
 
  
33 
 
置來訓練一個自動端點標示偵測器，以進行第二階段更進一步地修正。由於 TCC-300 語音
資料庫沒有人工標記的音素切割位置，利用 HTK（Hidden Markov Toolkit）使用 SAT（speaker 
adaptation transform，feature MLLR）及 SA（speaker adaptation，MLLR）技術訓練 HMM 類
音素模型，獲得較佳的 HMM 模型後進行強迫對齊（force alignment）之自動標示結果，作
為 TCC-300語料庫之類音素初始切割位置，以提供本計畫使用。 
聲調
音節
聲母
介音
韻母
韻腳
主元音 韻尾
 
圖 4.1：國語音節結構圖 
4.1.2 國語 Treebank語料庫簡介 
Treebank語料庫包含425個語句且含有56237個音節，由一個專業的女性播音員所錄製。
此語料庫屬於麥克風朗讀語音，主要目的是為提供語音韻律標記與建立韻律模型之研究。語
句取樣頻率皆為 16000 赫茲（Hertz），取樣位元數為 16位元，副檔名為*.pcm。 
在文字轉寫方面，因Treebank語料庫內含人為時間標記之音節與聲/韻母層級文字轉寫，
本計畫以此兩種層級之文字轉寫作為實驗結果之標準答案以評量實驗結果之效能。另外，藉
由 HTK toolkit 訓練音節以及聲/韻母（initial/final）語音單元之 HMM模型，對語料庫進行強
迫對齊，獲得初始自動分段位置用於實驗使用與測試。選擇梅爾倒頻譜係數作為語音聲學參
數，參數設定為 38 維，其中包含 12 階的梅爾倒頻譜係數與能量之對數值（log energy）及
其一階差量與二階差量並扣除原本的能量對數值總計 38 維，音框長度（frame length）設為
32毫秒，音框平移（frame shift）設為 5毫秒。 
Treebank 語料庫在訓練及測試語料的選擇上，扣除語句中含有英文的 4個語句，剩下 421
句以 9：1的比例隨機選取，得訓練語料為 379 句和測試語料為 42句。 
35 
 
MLLR）後的出語者調適 HMM模型來做 TCC-300的類音素單元之初始自動語音分段位置，
接著利用此初始位置依照發音方法的不同做分類，如表 4.3。並由初始位置當作參考位置再
利用取樣式聲學參數的特性來調整音素端點之標記位置。以下比較 HMM 之初始位置及以取
樣式聲學參數特性修正後之語音分段位置。 
表 4.3：國語語音發音方法的分類表。 
先前在觀察 HMM 自動語音分段位置的準確度時，發現短停頓常會有無法標記出來或是
標記位置錯誤之情形，而使得某些音素之平均音長有過長的現象，如塞擦音與爆破音等。在
此本計畫使用信號波封與各頻段之信號波封來判斷語音段是否為短停頓的狀態。由圖 4.2可
以觀察到短停頓中各個頻段之信號波封與其它有語音信號的地方相比其數值幾乎非常地低
且根據語音屬性不同而有不同的頻譜分佈情形。在此，簡單以信號波封與各頻段之信號波封
來標記短停頓的端點。短停頓標記修正之演算法如下： 
(1) 前端點：在原端點位置之前後 30 毫秒的範圍內，判斷語音波形之波封是否小於波
封之臨限值而得到一個交集點，再經由交集點附近距離 10 毫秒內來判斷各個頻段
之信號波封是否小於頻段波封之臨限值的條件作聯集來決定是否有短停頓的狀
態。 
(2) 後端點：在原端點位置之前後 30 毫秒的範圍內，判斷語音波形之波封是否大於波
封之臨限值而得到一個交集點，再經由交集點附近距離 10 毫秒內來判斷各個頻段
之信號波封是否大於頻段波封之臨限值的條件作聯集來決定是否有短停頓的狀
態。 
發音方法(Manner) 發音方法對應之音素 
爆破音 Stop b p d t g k 
鼻音 Nasal m n (n_n) (ng)   
摩擦音 Fricative f s x h sh  
塞擦音 Affricate q j c z zh ch 
流音 Liquid l r     
韻母音 Vowel others      
37 
 
 
圖 4.3：國語語句端點位置自動調整(摩擦音、塞擦音)演算法則之範例，最上方兩列標音位置
分別表示是 HMM 自動語音分段及修正後之語音標記位置；接著由上至下的圖形分別表示語
音波形、聲譜圖、頻譜 KL距離、頻譜熵、頻譜熵上升率 
 
爆破音切割位置的修正時，由波形與頻譜觀察中發現通常在爆破音開始的時候會有短停
頓出現，接著波封會有急遽上升的現象，故本計畫使用波封之上升率來描述其現象。如圖 4.4
中(a)、(b)小圖所示，在爆破音結束的地方，也是音素轉換的端點。 
爆破音程式修正演算法如下式： 
(1) 後端點：找到此一區段波封上升率的相對極大值，並在該極大值之位置找到頻譜
KL距離的相對極大值。 
(2) 前端點：利用後端點的位置當做參考位置，判斷前面是否有短停頓，有則利用短停
頓的方式偵測前端點，若無短停頓則搜尋此一區段之頻譜 KL距離的相對極大值。 
(a) (b)
 
圖 4.4：國語語句端點位置自動調整(爆破音)演算法則之範例：(a) /d/ 和(b) /g/ 音素最上方兩
列標音位置分別表示是 HMM自動語音分段及修正後之語音標記位置；接著由上至下的圖形
分別表示語音波形、聲譜圖、語音波封上升率、頻譜 KL距離 
39 
 
 
本計畫所建構之自動語音分段系統是分為兩階段式（Two-stage）的端點位置修正
（boundary refinement）。第一階段以 MFCC 聲學參數利用 HMM模型進行強迫對齊而得到初
始的語音分段位置；第二階段由本計畫提出之取樣點式聲學參數經多層感知器對不同語音單
元分類訓練端點偵測器，並依此架構對第一階段所得到之初始語音分段位置做更細部的調整，
最後系統輸出對應於語音單元之文字轉寫的自動語音分段位置。圖 4.7展示了自動語音分段
系統之流程圖，其主要與音素端點偵測器架構的差別是在於目標函數的定義。自動語音分段
系統之模型描述了語言之音節結構對應至語音分段之關聯性。 
MLP-based 
Boundary Detector
Candidate 
Target Labeling
HMM 
Force Alignment 
Segmentation result
Viterbi Search
Speech unit 
Segmentation
Candidate 
Pre-selection
Speech Signal
Feature Extraction Iterative Training
Sample-based 
Acoustic Feature
 
圖 4.7：使用多層感知器架構之自動語音分段系統流程圖 
然而，需要做自動語音分段處理的文字轉寫必須根據基本語音單元並依照音節結構來訂
定目標函數的種類，以提供端點偵測器的學習。藉由任務的不同來選擇適當的語音單元來進
行處理，例如語音合成系統所需要的單元約在聲母/韻母甚至是音節的語音單元；語音辨識
系統則可能需要小至音素等語音單元。同屬聲調語言之國、客語的音節結構，在本計畫中選
擇處理的語音單元為客語語料庫為聲/韻母的語音單元，國語語料庫為類音素以及音節單元。
以下將說明選擇不同基本語音之單元其目標函數之訂定方式： 
 音節層級 
以音節結構之音節層級來訂定語音信號所對應的兩個類別（class），分別為靜音（S）與
音節（V），依照不同類別彼此之間的轉移狀態，定義五種目標函數分別是 IS、SV、IV、VS、
VV 等轉移狀態，如圖 4.8 表示。每個由抽取聲學參數過程中所得到的候選端點皆須要進行
目標函數的標記，圖中之 IS 轉移狀態代表該候選端點仍為靜音狀態，SV轉移狀態表示該候
選端點是由靜音狀態轉換至音節狀態，依此類推…。其中需要特別注意的是圖中 VV的轉移
41 
 
（VS）；另外，模擬音節之間無靜音的現象中，本計畫亦將鼻音韻尾至聲母與韻母至聲母的
轉移狀態定義為相同的目標函數（VC）。 
IS IV
VS
SC CVIC
VC
INVN
VS
VC
S C V N
 
圖 4.10：類音素層級目標函數之轉移狀態圖 
由上述不同層級之目標函數轉移狀態的訂定，訓練不同文字轉寫之基本語音單元使用的
音素端點偵測器來達到自動語音分段的目的。 
 
4.3 TCC300語料庫實驗結果分析 
實驗結果以音框式抽取參數的 HMM架構，作強迫切割所獲得的類音素層級初始自動分
段位置來比較，並觀察本計畫自動分段位置之精準度是否有進一步地提升。由第四章所述，
在得到對應類音素層級之音素端點偵測器後，將 HMM的類音素自動分段結果以端點偵測器
所產生的概似度經正規化後作為分數，進行維特比搜尋並限制搜尋範圍在初始位置前後 100
毫秒之內，最後得到本計畫之類音素層級自動語音分段結果。 
首先，以下列舉兩個語音波形比較音素端點與 HMM 的自動分段位置。由下列圖 4.11、
4.12之中，可由方圈之圈選處的音素端點位置觀察到，無論是音節與音節之間的短停頓或是
聲母與韻母之間的端點位置都非常準確，尤其是母音和塞擦音、摩擦音之間的邊界端點與
HMM 之分段位置相比確實精準許多，而在聲譜上觀察這些端點位置可看出頻譜分佈差異極
大，亦是正確的端點位置。圖 4.12所示之方圈圈選處，我們亦可發現在母音轉變至鼻音韻尾
的情形，其音素端點位置之準確度仍能保持良好的水準；而在爆破音前的短停頓亦能調整至
適當的端點位置。由上述實驗結果在語音波形的觀察下，顯示了取樣點式聲學參數對 HMM
之自動分段位置做修正後，其自動分段之效能確有提升。 
43 
 
測器所提供之分數進行維特比搜尋。因此，起始分段位置之誤差過大亦難在搜尋空間找
到最佳的候選端點，使得端點位置產生偏差。 
4. 類音素音節結構與候選端點個數在該音段過少所引起端點位置標記誤差偏大的情形 
由於本計畫是將韻母定義為介音以及韻腳除去鼻音韻尾後所組成，但是在韻母音中雙母
音中的音素的變化卻是容易造成本研究方法的端點位置標記誤差增大，例如「作（/zuo/）
為（/wei/）」，韻母（/wei/）中即可分為介音/wu/、主元音/ei/和韻腹/eh/以及韻尾/yi/，其
中在聲譜圖內介音至主元音的變化卻是較（/o/-/wu/）變化明顯。然而候選端點在這些變
化較為明顯的地方容易挑選出來，進而使標記位置錯誤。 
 
圖 4.13：實驗方法與人為標記位置之誤差在不同絕對偏差值的包含比率直方圖，藍色線(左
側)為本計畫所提出之方法，紅色線(右側)為使用 HMM之初始自動分段位置 
然而圖 4.14 為實驗結果所有邊界端點與人為標記位置的統計，以下本計畫依續節音素
端點偵測應之誤報率與偵測漏失率的分析結果，將實驗結果依不同發音方法所對應的包含比
率做比較，觀察自動語音分段之效能好壞。首先由圖 4.14中在絕對偏差值為 15毫秒的範圍
內，圖(a)的整體曲線一開始便急遽拉升至近 80%以上的包含比率，但在圖(b)包含比率之整
體曲線則是呈現相較緩慢速度的提高。在圖(a)中，由摩擦音與塞擦音的包含比率相較於圖(b)
之結果差距逾 40%，代表著本研究方法確實有助於對此類發音方法之邊界端點來提升自動分
段的準確度。然而圖(a)、(b)的結果中發音方法為靜音之曲線趨勢差異為最大，其中隱含著
在 HMM的自動分段結果中，短停頓不易標記出來抑或是不夠準確的情況，此一現象亦顯現
出本研究方法對於音節間短停頓的修正，有大幅度地改進。 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
100% 
0 5 10 15 20 25 30 35 40 45 
Absolute deviation (ms) 
45 
 
 
圖 4.15：實驗方法與人為標記位置之誤差在不同絕對偏差值的包含比率直方圖，藍色線(左
側)為本計畫所提出之方法，紅色線(右側)為使用 HMM之初始自動分段位置 
本計畫同樣將聲/韻母層級的端點測器以修正 HMM 自動語音分段之實驗結果顯示了與
TCC300 語料庫類音素層級以及 Treebank 語料庫音節層級實驗相似之實驗結果，在此不多作
敘述。而將音節層級與聲/韻母層級之實驗結果相比，可以發現左側音節層級和右側聲/韻母
層級有一段包含比率的差距，且聲/韻母層級之實驗結果較佳，如下圖 4.16 所示。此圖凸顯
出不同音節結構的層級對自動語音分段效能的影響，其原因將在以下做討論。 
由於音檔抽取之取樣點式聲學參數以及挑選音素候選端點的過程為相同步驟，則影響效
能差異的關鍵即是對候選端點依照不同層級音節架構所標記之目標函數，而目標函數中所代
表不同分類之間的轉移狀態即為自動分段所能調整的端點位置。對描述語音單元中邊界端點
轉移狀態的定義分別由音素端點偵測只有一種描述音素邊界端點的轉移狀態；音節層級轉移
狀態有兩種；聲/韻母層級則有四種；最後類音素層級共有五種描述語音邊界端點轉移狀態
的目標函數。然而，轉移狀態的個數也象徵候選端點對描述語音單元邊界端點的分類，與只
有一種描述音素邊界端點轉移狀態相比，若邊界端點的類型適當地增多便能顯現各轉移狀態
統計特性的差異，減低因輸入候選端點之聲學參數特性相似讓端點偵測器產生混淆的可能
性。 
語音之結構亦隱含前後轉移狀態之間的順序關聯性，以聲/韻母層級舉例，目前候選端
點為聲母的狀態，則下一個候選端點就只能為聲母至韻母的轉移狀態或仍是聲母的狀態而不
會跳過結構中的分類。在另一方面，此順序之關聯性也可能造成分段位置之絕對偏差值增大，
如類音素層級之鼻音韻尾分類，欲在韻母狀態之後尋找最佳韻母至鼻音韻尾轉移狀態之候選
端點，但語音信號卻有鼻音弱化的現象，使得維特比搜尋在該音段選擇轉移狀態相對較大的
端點造成自動分段效能變差的情形。 
因此綜合以上所述且考量語音信號所挑選出候選端點數目以及觀察候選端點之位置，自
動語音分段屬聲/韻母與類音素之層級較為合適，也就是圖 4.16聲/韻母層級之實驗結果較佳
的原因。 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
90% 
0 5 10 15 20 25 30 35 40 45 50 
Absolute deviation (ms) 
47 
 
 
圖 4.17：偵測客語語句音素端點之範例，由上至下的圖形分別代表 HMM 分段位置、語音波
形、聲譜圖以及端點偵測器輸出音素候選端點之概似度 
4.5.2 自動語音分段實驗結果 
利用第四章所述之自動語音分段的方法，來印證在客語語料庫的效能，相比於 TCC300
語料庫其差異在於基本語音單位不同，客語為聲/韻母層級而 TCC300語料庫為類音素層級。
因此目標函數為根據聲/韻母層級所訂定並進行自動語音分段的方法，最後得到對客語語料
之自動分段位置。 
客語語料庫因無人為時間標記的資訊，故以下列舉兩個語音波形的範例來比較本研究方
法與 HMM自動分段的準確度。圖 4.18、圖 4.19之中，可由方圈之圈選處觀察到一些現象，
其音節之間邊界或是聲母與韻母之間的端點位置也都能修正至較為準確的位置，在爆破音與
短停頓的交界或是不同發音方式的轉換點尤其明顯，由上述實驗結果在語音波形的觀察下，
顯示了本研究所提出之自動語音分段方法對 HMM 之自動分段位置做修正後，其自動分段之
效能亦能有所提升。 
 
圖 4.18：客語語句自動語音分段之範例一，由上至下的圖形分別表示 HMM分段位置及音素
端點偵測之分段位置、語音波形、聲譜圖 
 49 
 
第五章 使用音段式語音發音方法辨認
器 
 
 在取樣式聲學參數之音素端點偵測器之應用方面，在計畫中我們也做了在發音方法
(pronunciation manner)辨認的實驗。 
 語音信號經取樣式聲學參數之音素端點偵測器後，會被切割成一連串語音段(speech 
segment)；我們可以讓取樣式聲學參數之音素端點偵測器操作在低偵測漏失率的工作點，
當然會有較高的假警報率，但是每段的平均長度會大於音框式系統中的音框移動值(10 
msec)。接著，計畫中將以音段作為發音方法的辨認單元，而非傳統系統中使用固定長
度及距離的音框(frame)為單位做辨認。而我們所使用的音段長度約略與音素的長度相當，
所以對同一語音信號平均音段數會小於傳統的音框數。但在語音信號變化快的地方，我
們所使用的音段長度會隨之變短，所以可以將較短之音節描述得較精確。 
5.1 音段式發音方法辨認器之參數抽取 
 在語音信號被切割成音段後，每一個取樣式聲學參數在此音段會形成一個時間函數
(或曲線)；例如最基本的語音信號波封與子頻段信號波封，如圖 5.1 所示。我可可將這
些曲線取出固定為度的參數來做為該音段發音方法辨認器之輸入參數。在此，我們使用
discrete legengre 多項式的參數[16]來表示一段曲線。 
 discrete legengre 多項式就是對一段參數曲線， ( ); 0, , 1f i i N  ，用下列基底函式
來表示， 
0
1/2
1
1/2 22
2
1/2
3
3
1
12 1
( 2) 2
180 1
( 1)( 2)( 3) 6
2800
( 1)( 2)( 2)( 3)( 4)
                    
i
N
i N i
N N N
i N i i N
N N N N N N N
i N
N N N N N N




 
 
 
      
            
        
         
           
  
   
       
3 2 2
2 2
6 3 2 ( 1)( 2)
    1.5
10 20
i i N N i N N
N N N N N
         
        
       
     (5.1) 
也就是一個時間函數(或曲線)可以用下列基底函式來表示 
 51 
 
用待辨認音框及前後個 4 個音框(共 9 個音框)之 MFCC 參數(共 108 維參數)作為發音方
法辨認器之輸入參數，使用 MLP 類神經網路做辨認器。其以音框為單位(frame-based)
之發音方式辨認結果也並列於表 5.1中。 
 
表 5.1 : 使用音段式語音發音辨認方法之辨認結果 
Pronunciation manner 
Segment-based 
Recog. Rate (%) 
Frame-base 
Recog. Rate(%)[17] 
Fricative  79.4  85.2  
Stop  78.8  72.5  
Glide  68.0  56.5  
Vowel  90.5  89.0  
Nasal  75.4  77.5  
Silence  89.2  92.2  
Total 83.3  82.1  
 
在表 5.1中，可以發現對長度較短的音素如：stop 及 glide，使用音段式語音發音方
法辨認方法，其辨認率可以大幅改善。 
若將音段換算為 frame-based的辨認率，使用音段式語音發音方法辨認器其總音框辨
識率為 83.15%，也較[17]中的結果為佳。 
另我們驚訝的是我們使用較低的頻率解析度所求的知參數還能獲得較佳的結果；所
以在音段式語音發音方法及位置辨認或偵測器上，也就是 detection-based ASR上之應用，
將還有進一步探討的空間。 
  
 53 
 
公司須會購買之語料庫(中華民國計算語言學會)，TCC-300語料庫之類音素端點
標示資料可以授權發行，將對 TCC-300 語料庫使用者有極大的助益。 
 
已發表之論文 
(1) You-Yu Lin, Yih-Ru Wang, “Sample-based Phone-like Unit Automatic Labeling in 
Mandarin Speech, “, Proc. of ROCLING 2009, Taichung, ROC. pp. 137-149, Sept. 
2009. 
(2) You-Yu Lin, Yih-Ru Wang and Yuan-Fu Liao, “Phone Boundary Detection using 
Sample-based Acoustic Parameters, “, Proc. of INTERSPEECH-2010, Makuhari, 
Japan, pp. 1397-1400, Spet., 2010. 
(3) Yih-Ru Wang, “A Two-stage Sample-based Phone Boundary Detector using 
Segmental Similarity Features, “, Proc. of INTERSPEECH-2011, Florence, Italy, pp. 
413-416, Aug., 2011. (本篇論文之內容未詳列於本報告，故列於附件，本論文基
本上在偵測出音素端點後，如同第五章中一樣在使用音段參數來幫助，可以做
到更好的音素端點偵測。) 
 
已投稿之論文 
(4) Yih-Ru Wang, You-Yu Lin, “High-Resolution Phone Boundary Detection using 
Sample-based acoustic Parameters, ”, submitted to IEEE Trans. on Audio, Speech 
and Language and Processing. 
 
 
  
 55 
 
3043-3054, Dec., 1992. 
[15] B.-H. Juang, W. Hou and C.-H. Lee, “Minimum classification error rate Methods for  
Speech Recognition,” IEEE Trans. Speech and Audio Processing, vol. 5, no. 3, pp. 
257-265, May 1997. 
[16] Sin-Horng Chen and Yih-Ru Wang, ' Vector Quantization of Pitch Information in 
Mandarin Speech, ', IEEE Trans. on Communications, Vol. 38, No. 9, pp. 1317-1320, 
Sept., 1990. 
[17] Sabato Marco Siniscalchi, Jinyu Li, Chin-Hui Lee, "A study on lattice rescoring with 
knowledge scores for automatic speech recognition",  INTERSPEECH-2006,  pp. 
1319-1322. 
[18] 林宥余，”使用取樣點式語音聲學參數之音素端點偵測”, 交通大學碩士碩文，
民國 99 年。 
 57 
 
el bottle bcl b aa tcl t EL         
母音 
iy beet bcl b IY tcl t           
ih bit bcl b IH tcl t           
eh bet bcl b EH tcl t           
ey bait bcl b EY tcl t           
ae bat bcl b AE tcl t           
aa bott bcl b AA tcl t           
aw bout bcl b AW tcl t           
ay bite bcl b AY tcl t           
ah but bcl b AH tcl t           
ao bought bcl b AO tcl t           
oy boy bcl b OY               
ow boat bcl b OW tcl t           
uh book bcl b UH kcl k           
uw boot bcl b UW tcl t           
ux toot tcl t UX tcl t           
er bird bcl b ER dcl d           
ax about AX bcl b aw tcl t         
ix debit dcl d eh bcl b IX tcl t     
axr butter bcl b ah dx AXR           
ax-h suspect s AX-H s pcl p eh kcl k tcl t 
其他 
SYMBOL DESCRIPTION                     
pau pause silence                   
epi epenthetic silence                   
h# begin/end marker (non-speech events)           
1 primary stress marker               
2 secondary stress marker               
 
 
附錄二 
中文音素分類對照表 
表一、國語 21 類聲母表 
編號 拼音 注音 編號 拼音 注音 編號 拼音 注音 
1 zh ㄓ 8 g ㄍ 15 t ㄊ 
2 ch ㄔ 9 k ㄎ 16 n ㄋ 
 59 
 
附錄三 
Yih-Ru Wang, “A Two-stage Sample-based Phone Boundary Detector using Segmental 
Similarity Features, “, Proc. of INTERSPEECH-2011, Florence, Italy, pp. 413-416, Aug., 2011. 
(本篇論文之內容未詳列於本報告，故列於附件，本論文基本上在偵測出音素端點後，如
同第五章中一樣在使用音段參數來幫助，可以做到更好的音素端點偵測。) 
 
 
 
 
 
 
 
 
 
 
1/ ,  is odd and 0< 2
[ ]
0, otherwise
n N n n N
h n
  
 

  
The envelope of the i-th sub-band signal is denoted by 
[ ]ie n . Besides, the envelope of the original speech signal, 
0[ ]e n , is also extracted. The cutoff frequency of the low-pass 
filter is set to 30 Hz. 
The low-passed KL distance was used in probability 
theory to measure the similarity of two distributions. In this 
study, we use it to measure the similarity of two adjacent 
speech samples represented by six sub-band envelopes, 
{ [ ]; 1, ,6} for  and 1.ie m i m n n    The KL distance is 
implemented by first normalizing the six sub-band signal 
envelopes [8]  by 
 
6
1
[ ]
[ ]
[ ]
i
i
j
j
e n
E n
e n



 (2) 
Then, the sample-based KL distance is calculated by 
  
6
1
[ ]
[ ] [ ] [ 1] log
[ 1]
i
KL i i
i i
E n
d n E n E n
E n
 
    
 
  (3)
 
Spectral entropy is commonly used in measuring the 
flatness of a speech power spectrum in a frame-based system 
[7]. In this study, it is extended to the sample-base spectral 
entropy defined by 
   
6
1
[ ] [ ] log [ ]i i
i
H n E n E n

        (4) 
Its value will be small in the fricative/affricate and nasal 
parts of speech signal.  
The similarity of the signals around the boundary 
candidate can also be a useful measure of signal change. For 
each boundary candidate, 
jc , the feature vectors 
 [ ]; 0, ,6iE n i   in its two neighborhood windows jB

 
and 
jB
  are assumed to be normal distributed. Here, 
jB

 
and 
jB

 
are defined as 
  [ , 1]j j j jB c r c
    , [ , ]j j j jB c c r
   ,  
where the lengths, 
jr

 
and 
jr
 , are defined by 
              
min 1 min
1 min 1 max
max max 1
          , 1          
1 , 1
          , 1         
j j
j j j j j
j j
r c c r
r c c r c c r
r r c c


 

   

      
   
   
and 
min 1 min
1 min 1 max
max max 1
       ,          
,
      ,         
j j
j j j j j
j j
r c c r
r c c r c c r
r r c c


 

  

    
   .
 
The maximum and minimum window lengths, maxr and 
minr , are set to 5 ms and 10 ms, respectively in this study. The 
KL distance at boundary candidate, 
jc , can be defined as the 
KL distance of the pdfs of the feature vectors in 
jB

 
and 
jB
 , 
i.e., 
1 1
1 1
[( )( )]1
[ ]
2    [( ) ( )( )]
KL j T
tr
D c
tr    
 
   
 
     
      
  
      
, (5) 
where   and   are the means of feature vectors in jB
  and 
jB
 ; and  and   are the covariance matrices. 
The normalized sub-band signal envelope, sample-based 
KL distance and spectral entropy and their delta terms are 
effective parameters for modeling the short-term spectral 
changing rate. They are used as the input features of the first-
stage phone boundary pre-selection. 
2.1.1. Sample-based boundary detection by neural 
networks 
A boundary candidate pre-selection procedure is first used 
to reduce the number of data needed to be processed in the 
following boundary detection. The selected boundary 
candidates are those samples having larger speech signal 
changing rate. Thus, the sample-based KL distance is 
employed for boundary candidate pre-selection. A simple peak 
picking method with threshold is used to select all samples 
which satisfy the following constrains as candidates 
    
[ ] [ 1],  [ ] [ 1],  and [ ]KL KL KL KL KL dd n d n d n d n d n Th     ,(6) 
where Thd is a threshold. The sequence of boundary candidates 
is denoted as { ; 1 , }j cc j N . 
The average normalized sub-band envelope of the segment, 
1[ , ]k kc c , is defined by  
 
1
1 1
1
[ , ] [ ] / 1
k
k
c
i k k i k k
n c
ES c c E n c c

 
 
 
    
 
     (7)  
Then, a 27-dimensional feature vector is constructed for 
each boundary candidate. For the candidate at time kc , its 
feature vector includes the following acoustic parameters, 
(1) Features from current boundary candidates :  
 [ ],  [ ],  [ ],  [ ],  [ ]; 0, ,6KL k KL k k k i kd c D c H c H c E c i  ,  
where [ ]jH c  is the delta term of the sample-based 
spectral entropy. 
(2) Features from adjacent segments 
1[ , ]k kc c and 
1[ , ]k kc c  : 
      1 1 1 1[ , ], [ , ]; 0, ,6 , ,i k k i k k k k k kES c c ES c c i c c c c       
Lastly, two neural network-based classifiers, a multi-layer 
perceptron (MLP) and a recurrent neural network (RNN), are 
used to screening these phone boundary candidates. 
2.2. Similarity measure of acoustic signal segments 
After pre-selecting some boundary candidates in the first 
stage, we then verify each of them in the second stage. A new 
similarity measure based on CCGMM representation is 
introduced to calculate the distance of the two acoustic 
segments around a candidate for its verification. 
For a speech segment k, the pdf of its acoustic feature 
vectors,  1 6[ ] [ ], , [ ]o n E n E n , can be modeled by 
1
( [ ]) ( [ ]; , )
L
k lk kl
l
p o n c N o n 

   ,   (8) 
where { ( [ ]; , );   1, , }klN o n l L    
 
is a set of Gaussian 
distributions with common covariance matrix which is used as 
reached by the proposed method tested on the TIMIT database. 
The performance is only 1.3% higher than the HMM phone 
recognizer with MMAE criterion realignment. Nevertheless, 
the accuracy of the proposed system is much higher than the 
HMM recognizer. 42.1% and 81.9% of boundaries detected 
were within 5- and 15-ms error tolerance from manual labeling 
results. 
5. Acknowledgements 
This work was supported by the National Science Council, 
Taiwan, ROC, under the project with contract NSC 97-2221-
E-009-080-MY3. 
6. References 
[1] D. T. Toledano, L. A. H. Gomez and L. V. Grande, “Automatic 
phonetic segmentation,” Speech and Audio Processing, IEEE 
Transactions on , vol.11, no.6, pp. 617-625, Nov. 2003. 
[2] S. Dusan, L. Rabiner, “On the Relation between Maximum 
Spectral Transition Positions and Phone Boundaries,” in Proc. 
Interspeech 2006,  pp. 17–21. 
[3] G. Almpanidis, M. Kotti, and C. Kotropoulos, “Robust Detection 
of Phone Boundaries Using Model Selection Criteria With Few 
Observations,” IEEE Transactions on Audio, Speech, and 
Language Processing, vol.17, no.2, pp.287-298, Feb. 2009. 
[4] You-Yu Lin, Yih-Ru Wang and Yuan-Fu Liao, “Phone 
Boundary Detection using Sample-based Acoustic Parameters, 
“ Proc. of INTERSPEECH-2010, Makuhari, Japan, pp. 1397-
1400, Spet., 2010. 
[5] Yih-Ru Wang and Chi-Han Huang, ' Speaker-and-environment 
Change Detection in Broadcast News using the Common 
Component GMM-based Divergence Measure, ' ,  Proc. of 
ICSLP 2004, Jeju island, Korea, pp. 1069-1072, Oct. 2004. 
[6] Yih-Ru Wang, “ The Signal Change-point Detection using the 
High-order Statistics of Log-likelihood Difference Functions, “, 
Proc. of ICASSP 2008, Las Vegas, USA, pp. 4381-4384, April, 
2008. 
[7] Sharlene A. Liu, “Landmark detection for distinctive feature-
based speech recognition,” J. Acoust. Soc. Am. 100(5), pp. 
3417-3430, Nov. 1994. 
[8] H. Misra, S. Ikbal, H. Bourlard, and H. Hermansky, “Spectral 
entropy based feature for robust ASR,” in Proc. ICASSP 2004, 
pp. 193–196. 
0
0.05
0.1
0.15
0.2
0.25
0 0.05 0.1 0.15 0.2 0.25
F
A
 r
a
te
MD rate 
1-stage(MLP)
1-stage(RNN)
2-stage system
Rabiner[2006]
HMM
 
Figure 1 : The MD rate vs. FA rate curves for sample-based phone boundary detectors, two-stage sample-based phone boundary detector and 
HMM system. 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
< 5 ms < 10 ms < 15 ms < 20 ms < 25 ms < 30 ms < 35 ms < 40 ms < 45 ms < 50 ms
In
cl
u
si
o
n
 r
a
te
 
Absolute Boundary Error
HMM
1-stage RNN(EER)
2-stage system
 
Figure 2 : The normalized cumulative histogram of the absolute deviation between automatically detected boundaries and manually labeled 
result for difference methods. 
 一、參加會議經過 
本次會議於 99年 9月 26至 30日於日本幕張舉辨，其詳細議程如下。 
Monday 27 September 2010 
09:00 Conference Registration (to 17:00) 
10:00 Opening Ceremony [Hall A/B] 
11:00 Keynote 1 [Hall A/B] 
 Mon-Ses1-K: Keynote 1: Steve Young - Still Talking to Machines (Cognitively 
Speaking) [Hall A/B] 
12:00 Lunch 
13:30 Conference Session Mon-Ses2 
 Mon-Ses2-O1: ASR: Acoustic Models I [Hall A/B] 
Mon-Ses2-O2: Spoken dialogue systems I [201A] 
Mon-Ses2-O3: Speech Perception I: Factors Influencing Perception [201B] 
Mon-Ses2-O4: Prosody: Models [302] 
Mon-Ses2-P1: Speech Synthesis I: Unit Selection and Others [International 
Conference Room A] 
Mon-Ses2-P2: ASR: Search, Decoding and Confidence Measures I [International 
Conference Room B] 
Mon-Ses2-P3: Special-purpose speech applications [International Conference 
Room C] 
Mon-Ses2-P4: Speech analysis [International Conference Room D] 
Mon-Ses2-S1: Special Session: Models of Speech - In Search of Better 
Representations [301] 
15:30 Coffee Break 
16:00 Conference Session Mon-Ses3 
 Mon-Ses3-O1: Systems for LVCSR [Hall A/B] 
Mon-Ses3-O2: Speaker characterization and recognition I [201A] 
Mon-Ses3-O3: Source separation [201B] 
Mon-Ses3-O4: Speech Synthesis II: HMM-based Speech Synthesis [302] 
Mon-Ses3-O5: Multi-modal signal processing [301] 
Mon-Ses3-P1: Paralanguage [International Conference Room A] 
Mon-Ses3-P2: ASR: Speaker Adaptation, Robustness Against Reverberation 
Conference Room C] 
Tue-Ses2-P4: SLP systems [International Conference Room D] 
Tue-Ses2-S1: Special Session: Fact and Replica of Speech Production [301] 
15:30 Coffee Break 
16:00 Conference Session Tue-Ses3 
 Tue-Ses3-O1: ASR: Acoustic Models II [Hall A/B] 
Tue-Ses3-O2: Language Processing [201A] 
Tue-Ses3-O3: Speech and audio segmentation [201B] 
Tue-Ses3-O4: Prosody: Analysis [302] 
Tue-Ses3-P1: Speaker characterization and recognition III [International 
Conference Room A] 
Tue-Ses3-P2: Systems for LVCSR and rich transcription [International 
Conference Room B] 
Tue-Ses3-P3: Phonetics [International Conference Room C] 
Tue-Ses3-P4: Speech Production II: Vocal Tract Modeling and Imaging 
[International Conference Room D] 
Tue-Ses3-S1: Special Session: Quality of Experiencing Speech Services [301] 
18:15 ISCA General Assembly [Hall A/B] 
19:15 Student Reception, Y's Buffet 
Wednesday 29 September 2010 
08:00 Conference Registration (to 17:00) 
08:30 Keynote 3 [Hall A/B] 
 Wed-Ses0-K: Keynote 3: Chiu-yu Tseng - Beyond Sentence Prosody [Hall A/B] 
09:30 Coffee Break 
10:00 Conference Session Wed-Ses1 
 Wed-Ses1-O1: ASR: Acoustic Model Adaptation [Hall A/B] 
Wed-Ses1-O2: SLP systems for information extraction/retrieval [201A] 
Wed-Ses1-O3: Speech representation [201B] 
Wed-Ses1-O4: Voice Conversion [302] 
Wed-Ses1-P1: Prosody: Language-specific models [International Conference 
Room A] 
Wed-Ses1-P2: ASR: Language Modeling and Speech Understanding I 
[International Conference Room B] 
Wed-Ses1-P3: First and second language acquisition [International Conference 
Room C] 
08:00 Conference Registration (to 17:00) 
08:30 Special Highlight Session (to 12:00) [Hall A/B] 
 Thu-SHS: Speech Translation Technology: Communication beyond Language 
Barriers [Hall A/B] 
09:30 Coffee Break 
10:00 Conference Session Thu-Ses1 
 Thu-Ses1-O2: Physiology and Pathology of Spoken Language [201A] 
Thu-Ses1-O3: Pitch and glottal-waveform estimation and modeling II [201B] 
Thu-Ses1-O4: ASR: Feature Extraction II [302] 
Thu-Ses1-P1: Speaker diarization [International Conference Room A] 
Thu-Ses1-P2: Multi-Modal ASR, Including Audio-Visual ASR [International 
Conference Room B] 
Thu-Ses1-P3: Speaker and language recognition [International Conference Room 
C] 
Thu-Ses1-P4: Source localization and separation [International Conference Room 
D] 
Thu-Ses1-S1: Special Session: Social Signals in Speech [301] 
12:00 Lunch 
13:30 Conference Session Thu-Ses2 
 Thu-Ses2-O1: New Paradigms in ASR II [Hall A/B] 
Thu-Ses2-O2: Spoken Language Understanding and Spoken Language Translation 
II [201A] 
Thu-Ses2-O3: Signal processing for music and song [201B] 
Thu-Ses2-O4: Modeling first language acquisition [302] 
Thu-Ses2-P1: ASR: Acoustic Models III [International Conference Room A] 
Thu-Ses2-P2: Spoken dialogue systems II [International Conference Room B] 
Thu-Ses2-P3: Discourse and Dialogue [International Conference Room C] 
Thu-Ses2-P4: Voice activity and turn detection [International Conference Room 
D] 
Thu-Ses2-S1: Special Session: INTERSPEECH 2010 Paralinguistic Challenge 
[301] 
15:30 Coffee Break 
16:00 Closing Ceremony [Hall A/B] 
 
二、與會心得 
PHONE BOUNDARY DETECTION USING SAMPLE-BASED ACOUSTIC 
PARAMETERS 
You-Yu Lin1, Yih-Ru Wang1 and Yuan-Fu Liao2 
1 Institute of Communication, National Chiao Tung University, 
 Hsinchu, Taiwan, ROC 
2
 Department of Electronic Engineering, National Taipei University of Technology,  
Taipei, Taiwan, ROC 
rossi0927.cm97g@g2.nctu.edu.tw, yrwang@mail.nctu.edu.tw, yfliao@ntut.edu.tw 
 
Abstract 
A sample-based phone boundary detection algorithm is proposed 
in this paper. Some sample-based acoustic parameters are first 
extracted in the proposed method, including six sub-band signal 
envelopes, sample-based KL distance and spectral entropy. Then, 
the sample-based KL distance is used for boundary candidates pre-
selection. Last, a supervised neural network is employed for final 
boundary detection. Experimental results using the TIMIT speech 
corpus showed that EERs of 13.2% and 15.1% were achieved for 
the training and test data sets, respectively. Moreover, 43.5% and 
88.2% of boundaries detected were within 80- and 240-sample 
error tolerance from manual labeling results at the EER operating 
point. 
 
Index Terms: Speech segmentation, speech analysis.  
1. Introduction 
Automatic phonetic segmentation is a historic and basic problem in 
speech signal processing. Although a lot of researches had been 
done in the past [1], an automatic phonetic segmentation algorithm 
with high accuracy and precision is still a state-of-the-art work. 
Without knowing the text of the speech signal, it becomes a phone 
boundary detection problem which is more difficult than the phone 
boundary alignment problem. An accurate phone boundary 
detector is important and essential for speech processing 
engineering and linguistics. 
The most popular approach to automatic phonetic 
segmentation, when knowing the text of the speech signal, is the 
HMM-based time-alignment method. The performance is usually 
evaluated by using the percentage of detected boundaries with 
errors smaller than 20ms as a figure of merit. In [2], the minimum 
boundary error (MBE) criterion was used in the training of the 
HMM method. For TIMIT acoustic-phonetic continuous speech 
corpus, the minimum MBE-trained HMMs can identify 79.75% of 
human-labeled phone boundaries within a tolerance of 10ms, 
compared to 71.23% achieved by the conventional ML-trained 
HMMs. Moreover, there are only 7.89% of automatically labeled 
phone boundaries having errors larger than 20ms. Besides, some 
other techniques, like SVM, fuzzy logic and neural network [3], 
were also proposed to refine the HMM aligned boundaries in order 
to get more accurate boundary positions. 
In the automatic boundary detection problem, without 
knowing the text of the speech signal, the rate of acoustic signal 
change is the most important cue for decision making. In [4], the 
spectral transition measure, which is in fact the norm of delta 
MFCC, was used to find the phone boundaries. 15.4% miss 
detection (MD) and 22.0% false alarm (FA) rates were achieved 
for the TIMIT training data set. In [5], the model selection 
technique, DISTBIC, was used to perform the phone boundary 
detection. The DISTBIC first used the Kullback-Leibler (KL) 
distance to find the boundary candidates, and then employed the 
Bayesian information criterion (BIC) to further verify those 
candidates. 25.7% MD and 23.3% FA rates were achieved for the 
NTIMIT database. 
In those past studies, they adopted the frame-based approach 
to use acoustic features like MFCCs in phone boundary detection. 
The main drawback of the approach lies in the incapability of the 
frame-based features to model the rapid spectral changes in speech 
signal. Besides, the time resolution of the frame-based approach is 
too coarse for phone boundary detection. This motivates us to 
propose a sample-based phone boundary detection algorithm in 
this paper. 
The paper is organized as follows. Section 2 presents the 
proposed sample-based phone boundary detection algorithm. Its 
performance is examined by simulations discussed in Section 3. 
Some conclusions are given in the last section. 
2. The proposed sample-based phone 
boundary detection algorithm 
In the proposed sample-based phone boundary detection algorithm, 
speech signal is first processed sample-by-sample to extract some 
broadband spectral parameters. Then, the sample-based KL 
distance and spectral entropy are extracted from these spectral 
parameters.  They and/or their delta terms can be used to model the 
rate of signal change. The sample-based KL distance is then used 
to determine the boundary candidates. Lastly, a supervised neural 
network is used to detect the phone boundary from those 
candidates. In the following subsections, we discuss these three 
parts in detail. 
( )1 1 1 1[ , ], [ , ]; 1, ,6 , ,i k k i k k k k k kES c c ES c c i c c c c− + − += − −⋯  
(3) Two indicators used to indicate the first and last candidates 
in an utterance.  
 
Unlike the previous studies using a simple threshold value for 
acoustic parameter [4] or using a model selection criterion [5], a 
supervised multi-layer perceptron (MLP) is employed for the 
phone boundary detection in the proposed system. A problem to be 
solved is how to decide the phone boundary targets for training the 
MLP classifier. Conventionally,  phone boundary targets are 
manually labeled. But, those manually labeled boundaries may not 
be consistent with the boundaries found from an objective measure 
such as the sample-based KL distance used in the pre-selection 
procedure.  
An iterative procedure that integrates target selection and MLP 
training is proposed in this study for the phone boundary detector. 
For each manually labeled phone boundary, im , a target for MLP 
detector is chosen from one of the candidates in the interval 
[ ],i im mδ δ− + , where δ denotes the allowable tolerance between 
the manually and automatically detected phone boundaries. The 
procedures of the training algorithm is listed in detail in te 
following:  
(1) Select initial targets from the candidates with the largest 
sample-based KL distance [ ]KL id c  where 
[ ],i i ic m mδ δ∈ − + ;  
(2) Train the MLP-based phone boundary detector using the 
given targets; 
(3) Select the candidate with largest output of the MLP 
detector, i.e. the most probable candidate in 
[ ],i im mδ δ− + , as the new target. 
Repeat (2) and (3) until convergence. 
3. Experiments and results 
The TIMIT speech corpus was used to evaluate the effectiveness of 
the proposed sample-based phone boundary detection algorithm. 
The numbers of phone boundaries in the training and testing parts 
of TIMIT were 172460 and 62465, respectively. The total numbers 
of samples were 2.27*108 and 8.29*107 for training and testing 
data sets. In average, there were 12.2 phone boundaries in 1 sec, or 
one boundary per 1310 samples. 
First, the envelopes of six sub-band signals were found from 
the speech signal. Then, the sample-based KL distance and spectral 
entropy were extracted. An example is shown in Fig. 2. The 
threshold value of KL distance was properly chosen, in our 
experiment, to preselect the boundary candidates. In all, 473353 
and 182858 candidates of the training and test data sets were 
selected. Only 0.21% speech samples, or in average one out of 476 
samples, were preselected as boundary candidates. And those 
candidates were sent to the following MLP-based boundary 
detector. 
The MLP-based phone boundary detector was trained using the 
iterative target selection and MLP training algorithm mentioned 
above. The number of hidden neurons was empirically set to 75. 
An example of the MLP output is displayed in Fig. 2. Note that the 
output of MLP is in the range of [-1,1]. As shown in the figure that 
the output of the MLP detector is larger when the signal changes 
more rapidly. 
A threshold for detector output was then used to decide the 
phone boundaries. The curves of MD (Miss Detection) rate vs. FA 
(False Alarm) rate for the training and test data sets were shown in 
Fig. 4. By the definition given in [5], FA was defined as (number 
of false alarms)/(number of actual boundaries + number of false 
alarms). 13.2% and 15.1% EERs were achieved for the training 
and test data sets, respectively. Compared with the performance of 
[4], which is 15.4% MD and 22% FA rates for the TIMIT training 
data set, about 20% EER reduction was achieved. Besides, it is 
worthy to note that [4] is a frame-based approach. Since it can’t 
distinguish the boundaries within the same frames, the actual FA of 
[4]  will be a little higher.  
 
 
Fig. 4. MD vs. FA rates for the proposed phone boundary detector, 
where (15.4%,22%) indicated the performance in [4]. 
 
The normalized cumulative histogram of the absolute deviation 
between the automatically detected boundaries and the manually 
labeled ones was shown in Fig. 5. As shown in the figure, 43.5% 
and 88.2% of boundaries detected were within 80- and 240-sample 
error tolerance from the manual labeling results at the EER 
operating point.  While in [4], only 27% boundaries detected were 
within the same frame of the manually labeled boundaries and 70% 
were within one-frame (10-ms) tolerance. In our method, after 
converting the answers into frame-based resolution, 42% 
boundaries detected were within the same frame of the manually 
labeled boundaries and 87% were within one-frame tolerance. This 
shows that the proposed sample-based boundary detection method 
performed better.  
A preliminary error analysis was done. First, some phone 
boundaries were essentially difficult to detect because the acoustic 
signals surrounding those boundaries are changing slowly. At the 
EER operating point, 28.6% of the boundaries between {vowels, 
semivowels, glides} and {vowels, semivowels, glides} are not 
detected, i.e., misses. Basically, the filter band [6] used in this 
paper was designed for pronunciation manner and landmark 
detection. The MD rates for other boundaries between phones with 
the same pronunciation manner were also higher than average. The 
MD rate for fricative-fricative boundaries was 30.5%; and 51.8% 
for nasal-nasal boundaries. Finally, lots of short silences in front of 
fricative and semivowel, usually less than 5 ms, were not labeled in 
TIMIT, but those boundaries were correctly detected in the 
proposed method. 
國科會補助專題研究計畫項下出席國際學術會議心
得報告 
                                     
日期： 2011 年 6月 1  日 
                                 
 
 
 
 
計畫
編號 
 
NSC-97-2221-E-009-080-MY3 
計畫
名稱 
新世代自動語音辨識技術–第二階段 
– 國語及方言之音節階層事件偵測及其相關研究 
出國
人員
姓名 
王逸如 
服務機
構及職
稱 
交通大學副教授 
會議
時間 
100年 5月 22
日~100年5月
27 日 
會議地
點 
捷克-布拉格(Prague) 
會議
名稱 
（中文）第 36 屆國際聲學、語音與信號處理會議 
（英文）The 36th International Conference on 
Acoustics, Speech and Signal Processing 
發表
論文
題目 
（中文）使用階層式韻律模型於豐富中文語音辨認 
（英文）Enriching Mandarin Speech Recognition by 
Incorporating A Hierarchical Prosody Model 
二、與會心得 
在 5/25下午 4:15到 6:15 以海報型式發表論文，似乎韻律幫助中文語
音辨認好像沒有吸引大家的興趣，因為來觀看並提出問題的人明顯比旁
邊所報告的poster還少，不過仍與多位學者作了許多意見交流與討論，
除了讓對方更了解我們的研究，同時也對自己的研究有更深一層的了解；
根據 ICASSP2011的統計，台灣發表的數量為 113篇佔第八名，顯現台
灣佔有一些影響力，值得稱讚，而我們所投的領域 Speech Processing
總共有 582 submitted papers，而 Accepted papers 只有 281，約 48%
的錄取率，而這個領域也是 ICASSP 2011中最熱門的一塊。 
為了接下來的研究，也聽了一些與 adaptation 相關的主題，幫助我有
更清楚的概念，以及目前大家的作法是什麼，6 篇的報告有 4 篇都是
feature-space transform 上 作 研 究 ， 並 且 有 一 半 是 作 rapid 
adaptation的。 
三、考察參觀活動(無是項活動者略) 
  無 
四、建議 
無線網路需改善，因為用起來很不方便，有時甚至無法使用。 
 
五、攜回資料名稱及內容 
會議行程手冊一本：內容包含了所有的 oral與 poster的日程安排。 
論文隨身碟一隻：內容包含了會議所有投稿之內容。 
一個袋子。 
六、其他 
  無 
features within those constituents. A consecutive prosodic 
state sequence of a constituent forms its prosodic feature 
pattern. In this study, three types of prosodic states are used 
respectively for syllable pitch contour, syllable duration, and 
syllable energy level. 
BG/PG
PPh PPh PPh
PW PW
SYL SYL SYL
PW
SYL
PW
SYL SYL
PW
SYL
B3 B3
B2
B1/B0
B4 B4
B2
B1/B0  
Fig.1: The hierarchical structure of the prosody model used 
in the study [5]. 
3. THE PROPOSED ASR FRAMEWORK  
Basically, the task of the proposed ASR decoding is to find 
the best linguistic transcriptions { , , }l/  W POS PM ,  
prosody tags { , }p/  B P , and acoustic segmentation sb  for 
the given input acoustic features { , }a pX  X X  based on the 
following MAP criterion: 
, ,
, ,
, , arg max ( , , , , , | , )
                  arg max ( , , , , , , , )
l p s
l p s
l p s s a p
s a p
P
P
/ / b
/ / b
  / / b  b
 b
W POS PM B P X X
W POS PM B P X X
    (1) 
where 1{ }
Mw W  is a word sequence;  1{ }Mpos POS  is a 
POS sequence; 1{ }
Mpm PM  is a PM sequence; M is the 
total number of words; 1{ }
NB B  is a break type sequence 
with  Bn{B0, B1, B2-1, B2-2, B2-3, B3, B4} ; { , , } P p q r  
is a prosodic state sequence with 1{ },
Np p 1 { },Nq q  and 
1{ }
Nr r  representing prosodic states for pitch-level, 
duration, and energy-level, respectively; N is the total 
number of syllables ; aX  is a frame-based spectral feature 
sequence (i.e., MFCCs and their derivatives); and 
{ , , }p  X X Y Z  is a prosodic-acoustic feature sequence with  
X, Y, and Z  representing sequences of syllable-related 
features, inter-syllable-related features, and inter-syllable 
differential features, respectively. 
To make Eq.(1) mathematically tractable, we set the 
following assumptions: (1) Like the conventional AM, aX  
depends only on W; (2) pX  depends on both prosody tags 
p/  and linguistic features l/ ; (3) Syllable-related prosodic 
feature X is independent of inter-syllable-related prosodic 
features Y and Z; (4) Break tag B depends mainly on nearby 
linguistic features l/ ; and (5) Prosodic state tag P depends 
on nearby break tag B. Based on above assumptions, Eq.(1) 
is rewritten as 
^
`
, ,
, ,
arg max ( , | ) ( | , , ) ( , | , , )
                   ( | ) ( | ) ( , , )
l p s
l p s
a s s p l s p l
l
P P P
P P P
/ / b
  / / b
| b b / / b / /
 /
X W X Y Z
P B B W POS PM
  (2) 
where ( , | )a sP bX W is an AM; ( | , , )s p lP b / /X and 
( , | , , )s p lP b / /Y Z are prosody acoustic models that describe 
the variations of syllable’s and inter-syllable’s prosodic-
acoustic features; ( | )P P B  is a prosodic state model that 
describes the variations of prosodic state sequence; 
( | )lP /B  is a prosody-syntax model that describes the 
relation between break tags and the surrounding linguistic 
features l/ ; and ( , , )P W POS PM is a factored LM that 
describes the relations among W, POS and PM.  
In this study, a two-stage recognition approach shown in 
Fig.2 is adopted. In the first stage, a word lattice is generated 
by the conventional HMM method with a syllable-based AM 
and a word-bigram LM. In the second stage, a rescoring 
scheme basing on Eq.(2) is used to find the best recognized 
word sequence.  
 
 
Fig. 2: A block diagram of the two-stage ASR decoding. 
In the following subsections, we elaborate the models in 
Eq.(2) in more details. 
 
3.1.  The Factored LM 
The factored LM estimates the joint probability of words 
W  and their associated POS  and PM  by 
1
2
1 word-trigram LM
1 1 1
factored POS model factored PM model
( , , ) ( | )
        ( | , , ) ( | , , )
M
i
i i
i
i i i i i i i i
P P W W
P POS W POS PM P PM W POS PM


 
  
|

W PM POS
   (3) 
Here, the factored language model approach [9] is applied to 
the modeling of POS and PM. The SRILM toolkit [10] is 
used to train these models. 
 
3.2.  The Prosodic State Model 
The prosodic state model is further divided into three sub-
models as  
1 1 1
1 1 1 1 1 1
2
( | ) ( | ) ( | ) ( | )  ( ) ( ) ( )
     ( | , ) ( | , ) ( | , )
N
n n n n n n n n n
n
P P P P P p P q P r
P p p B P q q B P r r B     
 
 |
ª º« »¬ ¼
P B p B q B r B
  (4) 
5053
examine the performance of the proposed method. All test 
data were prargraphic utterances with average length of 32 
seconds. 
A text corpus was employed to train both the word-
bigram LM and the factored LM to be used respectively in 
the first- and second-stage decoding. The corpus contains in 
total about 139 million words and is formed by combining 
the following three corpora: (1) Sinorama: a news magazine 
with 9.87 million words; (2) NTCIR: an IR test bench 
consisting of several domains with 124.4 million words; and 
(3) Sinica Corpus: general text corpus collected for language 
analysis with 4.8 million words. A CRF-based tagger was 
employed to segment the corpus into word/POS sequences. 
For simplicity, PMs were categorized into four classes: 
comma, period, dot, and non-PM. A 60,000-word lexicon 
was also constructed based on the word frequency. 
First, the first-stage decoding using the syllable-based 
HMM models and the word-bigram LM was performed. A 
word error rate (WER) of 29.8% was achieved. Besides, a 
word lattice was also generated for the use of rescoring in 
the second-stage decoding. The word coverage rate of the 
lattice is 90.4%. This is the oracle performance of the 
second-stage decoding.  
In the second-stage rescoring process, we first tested the 
performance of rescoring using the factored LM (FLM) to 
replace the word-bigram LM without involving any prosodic 
model. The recognition performance is shown in Table 1. 
The error rates of word, character, and base-syllable were 
24.4%, 18.1%, and 12%, respectively. By incorporating the 
prosodic model, the performance improved to 20.7%, 14.4%, 
and 9.6%. It represented 3.7%, 3.7%, and 2.4% absolute (or 
15.2%, 20.4%, and 20% relative) error reduction over the 
baseline (factored LM). This is a significant improvement. 
We also find from Table 1 that the error rates of POS and 
PM were 24.0% and 14.7% for the baseline scheme and 
were improved to 21.0% and 12.9% for the proposed ASR 
method. 
 
Table 1: Error rates of word, character, base-syllable, POS 
and PM recognition attained on the TCC300 corpus. 
 WER CER SER POS PM 
Baseline  
(factored LM) 24.4 18.1 12.0 24.0 14.7 
Baseline + 
prosody model 20.7 14.4 9.6 21.0 12.9 
 
By an error analysis, we found that the performance 
improvement mainly resulted from the corrections of tone 
recognition errors and word segmentation errors. This 
confirmed our expectation because both inter-syllable breaks 
and syllable tones were properly modeled in the hierarchical 
prosody model. So, the proposed ASR framework is very 
promising.  
 
5. CONCLUSIONS 
In this paper, a new ASR framework for Mandarin speech 
was discussed. Its main contribution lies in adopting a 
sophisticated hierarchical prosody model to properly 
incorporate abundant prosodic information into the 
conventional HMM-based ASR framework. Experimental 
results confirmed that the proposed method is capable of 
correcting word segmentation errors and tone recognition 
errors. 
 
6. ACKNOWLEDGEMENTS 
The work was supported by the National Science Council, 
Taiwan, under the project with contract NSC 98-2221-E-
009-075-MY3. The authors would like to thank the 
ACLCLP for providing the TCC300 Corpus. 
 
7. REFERENCES 
[1] X. Lei et al., “Word-level tone modeling for Mandarin speech 
recognition,” in Proc. ICASSP 2007, pp. IV-665-IV-668 
[2] S. Ananthakrishnan and S. Narayanan, “Improved speech 
recognition using acoustic and lexical correlates of pitch accent 
in a N-best rescoring framework,” in Proc. ICASSP 2007, pp. 
IV-873-IV876 
[3] K. Chen et al., “Prosody Dependent Speech Recognition on 
Radio News Corpus of American English,” IEEE Trans. on 
Speech and Audio Proc., Vol. 14 no.1, pp.232-245, Jan. 2006 
[4] M. Ostendorf, I. Shafran, and R. Bates, “Prosody Models For 
Conversational Speech Recognition,” in Proc. 2nd Plenary 
Meeting Symp. Prosody and Speech Process. 2003:147–154 
[5] C.-Y. Chiang et al., “Unsupervised joint prosody labeling and 
modeling for Mandarin speech,” J. Acoust. Soc. Am., 125, No. 
2, pp.1164-1183, Feb. 2009 
[6] C.-Y. Tseng et al., “Fluent speech prosody: Framework and 
modeling,” Speech Communication, 46, pp. 284-309, 2005 
[7] K. Silverman et al., “ToBI: A standard for labeling English 
prosody,” in Proc. ICSLP, 1992, vol. 2, pp. 867-870. 
[8] A. Batliner et al., “The prosody module,” in Verbmobil: 
Foundations of Speech-to-Speech Translation, edited by W. 
Wahlster, 2000 
[9] J. A. Bilmes and K. Kirchhoff, “Factor language models and 
generalized parallel backoff,” in Proc. of HLT/NACCL, 2003, 
pp. 4-6. 
[10] A. Stolcke, “SRILM – An extensible language modeling 
toolkit,” in Proc. ICSLP, 2002 
[11] L. Breiman, J. Friedman, R. Olshen, and C. Stone, 
“Classification and Regression Tree,” Wadsworth, Belmont, 
1984 
[12] Mandarin microphone speech corpus – TCC300, 
http://www.aclclp.org.tw/use_mat.php#tcc300edu.  
[13] P. Beyerlein, “Discriminative model combination,” in Proc. 
ICASSP 1998, pp. 481-484. 
5055
97 年度專題研究計畫研究成果彙整表 
計畫主持人：王逸如 計畫編號：97-2221-E-009-080-MY3 
計畫名稱：新世代自動語音辨識技術– 第二階段--國語及方言之音節階層事件偵測及其相關研究 
量化 
成果項目 實際已達成
數（被接受
或已發表）
預期總達成
數(含實際已
達成數) 
本計畫實
際貢獻百
分比 
單位 
備 註 （ 質 化 說
明：如數個計畫
共同成果、成果
列 為 該 期 刊 之
封 面 故 事 ...
等） 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 1 1 100% 
篇 
 
論文著作 
專書 0 0 100%   
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國內 
參與計畫人力 
（本國籍） 
專任助理 0 0 100% 
人次 
 
期刊論文 0 0 100%  
研究報告/技術報告 0 0 100%  
研討會論文 2 3 100% 
篇 
 
論文著作 
專書 0 0 100% 章/本  
申請中件數 0 0 100%  專利 已獲得件數 0 0 100% 件  
件數 0 0 100% 件  
技術移轉 
權利金 0 0 100% 千元  
碩士生 0 0 100%  
博士生 0 0 100%  
博士後研究員 0 0 100%  
國外 
參與計畫人力 
（外國籍） 
專任助理 0 0 100% 
人次 
 
國科會補助專題研究計畫成果報告自評表 
請就研究內容與原計畫相符程度、達成預期目標情況、研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）、是否適
合在學術期刊發表或申請專利、主要發現或其他有關價值等，作一綜合評估。
1. 請就研究內容與原計畫相符程度、達成預期目標情況作一綜合評估 
■達成目標 
□未達成目標（請說明，以 100 字為限） 
□實驗失敗 
□因故實驗中斷 
□其他原因 
說明： 
2. 研究成果在學術期刊發表或申請專利等情形： 
論文：■已發表 □未發表之文稿 □撰寫中 □無 
專利：□已獲得 □申請中 ■無 
技轉：□已技轉 □洽談中 ■無 
其他：（以 100 字為限） 
3. 請依學術成就、技術創新、社會影響等方面，評估研究成果之學術或應用價
值（簡要敘述成果所代表之意義、價值、影響或進一步發展之可能性）（以
500 字為限） 
本計畫中，所提出之 
(1) 提出了一個使用語音信號取樣點為處理單元的語音音素端點偵測器。此方法與過去使
用的使用音框為語音信號處理單元的方法，是一個全新的方法；在系統複雜度上遠低於音
框式的方法，在效能方面，所偵測到的端點也具有更好的正確度與解析度。 
(2) 將上述使用語音信號取樣點為處理單元的語音音素端點偵測器方法使用於國語及客
語語料之類音素端點自動分段上，亦獲得良好之結果；並完成兩個國語與料庫及一個客語
語料庫之自動類音素端標示工作。 
(3) 在語音信號做音素端點偵測之後，也使用音段式為單位做了語音辨認方面之基本應用
之研究，完成了:(A)使用音段式為單位的兩階段式音素端點偵測器；(B)語音信號之發音
方法辨認器。 
 
本畫畫中提出的上述方法在學術上均屬創新之研究方法。計畫內容已發表三篇國內外學術
研討會論文，(1)部分也已投稿 IEEE Trans. on Audio, Speech and Language and 
Processing。(2)部分提供之類音素端點自動分段將對語言方面研究及教學學者及做語音
處理研究之學者有很大貢獻。(3)部分對 detection-based ASR 之研究提出了使用音段為
處理單位的系統架構概念，將有助於 detection-based ASR 的發展。 
本計畫中所提出之使用語音信號取樣點為處理單元的語音音素端點偵測器，可將語音信號
切割為一個個接近音素大小的音段，所以接著可以使用以音段為處理單位的語音辨認系統
