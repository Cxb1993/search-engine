應用於視訊監控之運動物體追蹤技術研究 
計畫編號：NSC 95-2221-E-009-106-MY2 
執行期限：96年8月1日至97年7月31日  
主持人：王聖智 (交通大學電子工程系教授)  
計畫參與人員：黃敬群  范博凱  蕭晴駿 (交通大學電子所研究生)  
 
一. 中文摘要 
本計劃中我們提出兩套應用於視訊監控
的技術: (a)多攝影機協同監控與(b)人物的姿
勢估測及分析。在多攝影機協同監控的技術
上，我們提出一套應用於多台主動式攝影機之
分工協調系統，對於空間中大約已知臉部之位
置與朝向的人群，進行攝影機的分工與協調。
每一台攝影機將會負責拍攝一小部分人群的
臉部，並且設法調整攝影機的旋轉角度以及放
大倍率，使人臉可以清晰地在畫面中呈現。在
此，我們對於人臉在畫面中清晰與否的評斷標
準為：人臉是否正面朝向負責拍攝的攝影機，
以及人臉在影像中的解析度。透過本系統，我
們可以安排各個主動式攝影機的旋轉角度與
放大倍率，盡可能地拍攝場景中所有人的臉
部，以獲得理想的人臉拍攝角度與解析度，便
於清楚地辨識每個人。另外，在人物的姿勢估
測及分析的技術上，我們提出一個在多攝影機
環境下，利用人體模型估測目標人物的姿勢與
行為。我們使用流形嵌入技術中的拉普拉斯特
徵映射，將三維人形的幾何形狀忠實地轉移到
另一個容易切割分析的高維度空間，正確地切
割出三維人形的各個部位並且找出三維人形
的骨骼架構，以利後續行為分析的動作。當擷
取出三維人形的骨骼架構後，我們利用粒子群
體最佳化在高維度空間中有效地找出最佳姿
態估測結果。我們的系統由影像的擷取至姿態
的估測完全自動化，並且不需要在人體上貼附
感應物，即可結合肢體的運動限制和時間軸上
的動作流暢限制，估測多種動作。 
 
關鍵詞：攝影機校正、動態攝影機控制、攝影
機協調機制、姿態估測、流形嵌入技術。 
 
ABSTRACT 
 
In this project, we proposed two algorithms for 
the application of video surveillance: (a) a 
camera coordination system for surveillance and 
(b) human body pose estimation method. For the 
camera coordination system, we coordinate 
multiple PTZ cameras to capture the face 
pictures of monitored targets. Given the 
positions and orientations of people’s faces in 
the 3-D space, this system dynamically controls 
the panning, tilting, and zooming of all PTZ 
cameras, trying to acquire better shots of targets’ 
faces. The adopted criteria include people’s 
facing directions with respect to the cameras and 
the resolutions of the facial images. Unlike other 
approaches, we do not limit our PTZ cameras to 
the capture of only one target at one time. 
Instead, the proposed system coordinates all PTZ 
cameras to capture as many high resolution 
frontal faces as possible. With this system, the 
faces in the scene can be better captured and the 
identity of each monitored target can be well 
discerned. For human body pose estimation 
method, we propose a 3D human body pose 
estimation method for a multi-camera motion 
capture system. The reconstructed human body 
is transformed into a high dimensional space 
using our modified Laplacian Eigenmap. In this 
eigenspace, the body parts can be segmented 
more efficiently and easily. Then, the 3D 
skeletons of the human body are extracted to 
obtain the kinematic information. Finally, pose 
estimation is performed by fitting a prior 3D 
model to the extracted skeleton via particle 
swarm optimization (PSO). Furthermore, with 
our proposed human model, the motion 
constraints can be easily combined with the 
optimization process. Temporal consistency of 
the pose estimation results is also achieved by 
adding temporal constraints over PSO. Our 
method can deal with various kinds of motion 
and has robust pose estimation results. 
 
Keywords: Camera calibration, dynamical PTZ 
to capture people’s high-resolution images, with 
each PTZ camera monitoring a single person at 
one time. A scheduling algorithm is proposed to 
control the movement of all PTZ cameras so that 
each pedestrian will be captured at least one time 
before the pedestrian leaves the scene. The 
performance of their system is evaluated over a 
virtual train station scene which is synthesized 
by computer animation.  
 
2.2 Human body pose estimation method 
As for the human body pose estimation method, 
we mainly discuss markerless motion capture 
systems, which have drawn much attention in 
recent years. A markerless approach can be 
decomposed into several submodules: 
initialization module, tracking module, pose 
estimation module, and recognition module. 
In the proposed motion capture system, we 
mainly focus on the initialization module and the 
pose estimation module. The module of 
initialization aims to obtain reliable prior 
knowledge for pose estimation and recognition. 
Due to error propagation, incorrect prior 
knowledge may lead to incorrect pose estimation. 
In the following paragraphs, we’ll first introduce 
a few algorithms that are related to initialization 
and pose estimation. In this project, we focus on 
model-based pose estimation for multi-camera 
systems.  
In [7], the reconstruction of a “visual hull” 
based on images from multiple cameras is 
introduced. In this approach, a visual hull is 
defined as the 3D shape formed by the 
intersection of visual cones projected from the 
2-D silhouettes. The visual hull of an object can 
be thought to be a close approximation of the 
object based on the observations from different 
viewpoints. 
Regard the 3-D shape human model, Mikic 
[8] adopted a twist framework that has been used 
to model the kinematic chains for robots. Sixteen 
rotation axes and five kinematic chains of the 
body joints are formulated using twists and 
product of exponentials. Relative to the 
torso-centered coordinate system, the rotation 
and shift of the other body parts can be easily 
manipulated. Pose estimation is performed by 
first doing template fitting and then using 
Bayesian network for refinement. However, the 
initialization based on template fitting cannot 
deal with self occlusion and the target person has 
to dress in tight clothes. 
Instead of using shape models, Menier [9] 
adapted skeleton models to fit medial axis points 
extracted from visual hulls. This approach 
reduces the dependency on the dimension of 
human body, and these 3D medial axis points 
represent the observed skeleton data. A generic 
skeleton model is then fitted with the observed 
skeleton data based on maximum a posteriori 
(MAP) estimation. The pose estimation of the 
first frame is based on the fitting process, while 
non-parametric belief propagation is used to 
predict the pose of the following frames. 
Due to the high dimensionality of the search 
space and the complexity of the fitness 
evaluation function, some researchers have 
adopted the particle swarm optimization (PSO) 
[10] method to perform pose estimation. 
Robertson [11] applied PSO to perform skeleton 
model fitting in a conference room environment, 
where the pose estimation is required only for 
the upper body. PSO is chosen for its ability to 
deal with nonlinear and non-convex optimization 
problems. Hierarchical and parallel PSO fitting 
is proved to be robust and computationally 
inexpensive. 
As mentioned earlier, model based motion 
capture systems have the advantages of 
complexities reduction and robustness. However, 
a good initialization is required to ensure that the 
system commences with a good body parts 
labeling and initial guess. More reliable 
initialization approaches based on manifold 
learning have been proposed recently.  
In manifold learning, manifold embedding 
is a topic about how to find a transformed space 
for the manifold that preserves the connectivity 
and algebraic properties. Several approaches, 
such as Laplacian Eigenmap [12], have been 
proposed in this field. In [13] Sundaresan 
proposed a segmentation approach for pose 
estimation based on Laplacian Eigenmap. In this 
approach, different branches in the normal space, 
such as separated body parts, are transformed 
into distinguishable 1-D smooth curves in the 
With the definitions of Nθ and NW, we then 
define the evaluation function Eval( ) for the 
face capture of the j-th target by the i-th camera.  
( ) ( ) ( )
1 1
m n
Eval AP ap N N Wij ij W ij
i j
θθ= ∑ ∑= = .        (5) 
In (5), m is the number of cameras and n is the 
number of targets. AP denotes a set of camera 
assignments  
{ },  1,2, , ,  1,2, ,AP ap i m j nij= = =K K ,        (6) 
with apij’s representing the binary assignment 
parameters. apij is equal to 1 if the i-th camera is 
assigned to monitor the j-th target, and apij is 
equal to 0 otherwise. Hence, for a camera 
assignment AP, Eval(AP) represents the overall 
observation levels of the n targets by all m 
cameras. When more targets can be better 
observed by their corresponding cameras, with 
smaller shoot angles and larger face widths, we 
have a larger Eval(AP). Hence, the goal of the 
proposed camera coordination system is simply 
to find the optimal camera assignment that 
reaches the largest Eval(AP). Moreover, as these 
n targets keep moving within the monitored 
scene, we need to adaptively adjust the 
assignment of cameras to achieve the most 
preferable observation.  
Besides, to simplify the problem, we also 
add one extra constraint over (5). The constraint 
is 
1,  1,2, ,
1
m
ap k nik
i
= =∑
=
K .                 (7) 
This constraint implies that we only take into 
account the camera view that is assigned to the 
target even though that target may also appear in 
some other views.  
 
3.2. Significance Weight 
In theory, we can always find an optimal AP for 
the evaluation function at any time instant. 
However, people’s behavior is highly versatile. 
It is very likely that even with the optimal 
camera assignment we still cannot clearly 
capture all people’s faces at some time instants. 
The proposed system cannot guarantee that all 
people’s faces are always clearly observed.  
To deal with this problem, we assign each 
target a significance weight to represent the 
priority of that target. This weight will increase 
if the target hasn’t been clearly observed in the 
past few moments. On the contrary, if that target 
has already been clearly observed for a while, 
we decrease its significance weight.  
The adjustment of significance weight 
includes three different states: raise, hold, and 
decline. When the face of a target cannot be 
unclearly captured, we linearly increase its 
significance weight. When the weight is raised, 
the system will pay more attention to that target 
and it’s more likely that the target can be better 
observed. Once if the system has adjusted its 
camera coordination to take clear facial picture 
of that target, the significance weight will be 
held at a high value for a while. This “hold” state 
is make sure the target’s face can be clearly 
observed for a long enough period, but not just a 
short glimpse. After the hold state, the 
significance weight of the target is then 
decreased gradually to zero as long as the 
target’s face can be clearly captured. An 
example of the switching of these three states is 
illustrated in Fig. 3.  
 
Fig. 3   Variation of significance weight 
To realize the concept of importance weight, 
we add penalty term into the definition of Eval( ), 
as expressed below. 
( ) ( ) ( )( )
1 1
m n
Eval AP ap N N W pvij ij W ij ij
i j
θθ= −∑ ∑= = ,   (8) 
where the penalty term pvij is defined as 
kswcfpv jijij ⋅⋅= .                   (9) 
In (9), swj stands for the significance weight of 
the j-th target, cfij represents the clear factor of 
the j-th target with respect to the i-th camera, 
and k is a controlling scalar. The clear factor cfij 
time
Initial State 
Raise State 
Hold State 
Decline State 
T T
Significance 
weight 
The inertia factor is chosen to be some value 
within [0.8, 1].  
However, due to the inclusion of the 
constraint (7) in the evaluation function (8), the 
DBPSO algorithm needs further modification. In 
DBPSO, the d-th bit of the i-th particle is 
updated based on (13). However, after the 
update, the new particle may violate the 
constraint (7). To fix this problem, we slightly 
modify the DBPSO algorithm based on the 
following concept. 
Assume for the k-th target, its corresponding 
assignment bits are denoted as 
},...,,{ 21 mkkk apapap . If more than one ikap  
is set to 1. Then we retain the assignment bit that 
has the highest possibility to be 1, while set the 
other assignment bits to zero. In our approach, if 
the previous best value or the best global value 
of an assignment bit is 1, that assignment bit has 
the highest probability to be 1. If more than one 
assignment bits have the highest probability to 
be 1, then we randomly pick up one of them to 
be 1 and set the others to 0. 
On the other hand, if none of the assignment 
bits are set to 1, we still apply the same strategy 
to correct the assignment. That is, if the previous 
best value or the best global value of an 
assignment bit is 1, that assignment bit will have 
the highest probability to be 1.  
 
4. TECHNICAL ILLUSTRATION –HUMAN 
BODY POSE ESTIMATION 
 
In the other way, we proposed an efficient 
initialization process and a robust markerless 
pose estimation system. The goal of pose 
estimation is to capture the motion of a specific 
person. The motion of the articulated body parts 
is described using some parameters of a generic 
human model. Using images from a set of 
synchronized and calibrated cameras, we can 
reconstruct the visual hull based on volume 
intersection. Then the 3D voxel data are 
transformed into an embedding space using our 
modified Laplacian Eigenmap technique. Body 
parts segmentation is done in the eigenspace and 
the skeletons of the body parts are extracted 
individually. Finally, we fit the human model 
into the skeleton data using the PSO algorithm. 
Pose estimation is then iteratively performed for 
optimization. In Fig. 1, we show the flowchart of 
the proposed system. 
 
4.1. System initialization 
The performance of a model-based system 
heavily relies on the accuracy of the 
initialization results. The embedding-based 
initialization exploits the manifold embedding 
methods and has the advantage of lower model 
dependency. Inspired by Sundaresan’s method 
[13], we develop a modified Laplacian 
Eigenmap to efficiently extract the kinematic 
information from the visual hull. In the 
following sections, we’ll explain more details 
about Sundaresan’s method and our initialization 
method. 
4.1.1 Proposed initialization method 
Inspired by Sundaresan’s algorithm, we develop 
our initialization method based on a 
modification of the Laplacian Eigenmap.   
Given n points v1, v2,…, vn in the p 
dimension, Laplacian Eigenmap aims to find its 
transformation u1, u2, …, un in the r dimension 
to minimize the object function: 
2
,
Ei j ij
i j
−∑ u u ,                    (17) 
where E is the adjacency matrix of the graph 
constructed from v1, …,vn. That is, if vj is in the 
neighborhood of vi, then Eij is equal to 1. 
Otherwise, Eij is set to zero.  
Besides (17), an extra constraint is added for 
the minimization of the object function. The 
constraint says 
[ ]1
U DU
where U
T
T
n
I=
= Lu u
             (18) 
D is a diagonal matrix, whose element Dii 
represents the degree of Node i. This constraint 
is to normalize the scaling factor when manifold 
embedding is performed. We can unroll 
Equation (18) to obtain the following 
constraints:  
same as Sundaresan’s method. 
 
B.2. Spline Propagation 
Starting from the end of the (P+1) nodes, the 
nearest N points are selected. Unlike 
Sundaresan’s method, we don’t have to count 
the number of outliers. The site values are 
also fitted using a 6-D spline. 
 
B-3. Spline Termination 
The process of spline propagation continues 
until the distance between the end of the 
spline and the origin is less than a pre-defined 
threshold. 
 
In Fig. 4, we show the comparison between 
the Sundaresan’s method and ours. It can be seen 
that our method map the trunk part to the origin 
of the eigenspace. Hence, after spline fitting, the 
trunk is well detected and the limb parts of the 
human body, especially the left arm, can be 
successfully extracted.   
Once the segmentation of human body parts 
is done, we can extract the skeleton of the visual 
hull. Skeleton extraction has the advantage of 
feature reduction. Furthermore, skeletons encode 
the information of kinematic motion and won’t 
deform in any pose.  
In our approach, each body part is 
individually transformed into a 1-D eigenspace 
based on the LE algorithm. The smallest nonzero 
eigenvalue represents the most important 
dimension that corresponds to the trend of the 
body part. Spline fitting is performed and the site 
values that encode the geometric relation in the 
normal space are calculated along this dimension. 
The skeleton extraction is then performed by 
finding a 3-D spline h which minimizes (22) 
2
 
( )i i
v some body parti
s
∈
−∑ v h      (22) 
 
 
(a) (b) 
 
(c) (d) 
 
(e) (f) 
 
(g) (h) 
Fig. 4: Comparisons between Sundaresan’s 
method and ours (a) input image (b) the color 
representation for the segmentation results 
(c)(e) the segmentation result in the 6-D 
eigenspace using original LE (g) the 
segmentation result in the normal space using 
original LE (d)(f) the segmentation result 
based on our modified method (h) the 
segmentation result in the normal space using 
our modified method. 
 
In Fig. 5, we show an example of the skeleton 
extraction process. 
   
(a)                    (b) 
Fig. 5: The skeleton extraction result using the 
proposed method (a) one of the eight input 
images (b) the extracted skeleton 
 
 
2nd body part
3rd body part
4th body part
5th body part 
6th body part 
unfitted 
globally best position. Gradually, most particles 
will converge to the optimal position which has 
the minimal value of f(p). 
Besides the use of the PSO method, we also 
need to ensure the temporal consistency between 
frames. The motion changes between the current 
frame and its previous frame should be smoothly 
changing. To ensure the temporal consistency, 
we propagate the values of the estimated 
parameters from the current frame to the next 
frame. In other words, we restrict the values of 
the parameters for the next frame to be within 
some range around the estimated parameters at 
the current frame. However, since an incorrect 
estimation may also propagate over time, we add 
a re-initialization mechanism for each frame. 
When the fitting error is larger than some 
pre-defined threshold, we will re-initialize the 
whole pose estimation process based on the 
current frame only. This can prevent the 
propagation of errors. 
 
5. SIMULATION RESULTS 
 
5.1 Multi-camera coordination system 
Fig. 6 shows a few sets of images captured at 
different time instants. In our experiments, the 
test videos are synthesized by ObjectVideo 
Virtual Video (OVVV) [17], which is a publicly 
available visual surveillance simulation test bed. 
By using OVVV, we can easily design various 
kinds of scenario and camera setups. In Fig. 4, 
there are nine people walking around in the 
scene. Each person is assigned a color and we 
use this color to plot a bounding box for that 
person. The synthesized scene is captured by 
four static cameras and four PTZ cameras, 
locating at different positions. In each figure, the 
left four frames denotes the pictures captured by 
the static cameras, while the right four frames 
are the pictures captured by the PTZ cameras. 
The use of static cameras can help the reader to 
easily realize the relations among these nine 
people; while the images captured by the PTZ 
cameras demonstrate the results of camera 
coordination. 
 
Table 1 The statistical results of all test 
sequences 
 
Average 
Score 
Unclear 
Rate 
SEQ-1 (9) 7.5355 0.0200 
SEQ-2 (9) 5.0644 0.0393 
SEQ-3 (6) 7.6656 0.0056 
SEQ-4 (6) 8.0526 0.0017 
SEQ-5 (7) 8.3151 0.0138 
Average 7.3266 0.0161 
 
 
Time 15 
 
Time 110 
 
Time 135 
Fig. 6 Experimental results of the test sequence 
SEQ-1 
Table 1 shows the statistical results of all 
experimental sequences. The “average score” is 
the average score for all people in a sequence. 
“Unclear Rate” is the average ratio of the 
zero-score time over the total time. The highest 
average score is 10 and the lowest is 0. Here, the 
frame size of each camera is 320×240 for all 
experiments. We let the value of thmax, thmin, and 
thθ be 50, 15, and π/2, respectively for all 
experiments. The upper bound of the unclear 
period is set to 20.  
 
 
