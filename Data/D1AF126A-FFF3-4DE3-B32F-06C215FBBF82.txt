 II
中文摘要 
本計畫目的在情境式超媒體展示自動合成系統技術研發，本實驗室在前幾年的研究中已經
開發出一套高階超媒體編輯軟體 HyperMod 系統，能夠將原本一般極複雜的編輯過程簡化
為少數步驟，這使得包括類神經網路等機器學習的一些技術得以應用在學習並模擬人工超
媒體編輯的概念與流程，從而可以建立一套智慧型超媒體自動產生系統。本計劃的主要內
容即在探討在 HyPMod 架構下，類神經網路應用在智慧型超媒體自動產生的機制。計劃主
要成果包括各核心機制分析，並實做一套智慧型超媒體自動產生系統。 
 
關鍵詞: 情境式超媒體展示，超媒體自動合成系統 
 
英文摘要 
In the current web-based environment, to make an “affective” and aesthetic hypermedia 
presentation requires collaboration between content experts, computer engineers and multimedia 
designers. This process usually is very time consuming and labor intensive. In this project, we 
developed an adaptable affective hypermedia generation system which can automatically generate 
affective hypermedia presentations. In our previous work, the HyPMod system significantly 
facilitates the authoring of visual-pleasing hypermedia presentations.  By greatly reducing the 
number of hypermedia authoring operations, the human authoring process is now possible to be 
simulated using a computer system. In this project, based on the HyPMod system, we proposed a 
framework based on neural-network for an adaptable styling system for affective hypermedia 
presentations. A system has been implemented to realize the framework.  
 
  
Keywords: Affective hypermedia; intelligent hypermedia generation 
using conventional hypermedia authoring software (e.g., Director, Flash) is far too complicated to 
allow a system to learn and simulate the whole process. In our previous work, we designed a 
HyPMod hypermedia authoring framework that contains the as illustrated in Figure 1. In our 
HyPMod system, we have modularized the hypermedia presentation into a number of media 
handlers. This significant reduces the authoring steps and provides a promising pragmatic 
platform on which an intelligent hypermedia generation system could be realized. The following 
describe the novel framework using neural network on which the intelligent hypermedia 
generation could be realized to simulate the authoring process. 
 
Figure 1: system framework of the HyPMod authoring system 
 
The possible knowledge training and learning environment using neural network is illustrated in 
Figure 2 (a). The neurons of the input layer are parameters for style handler combination. The 
neurons of the output layer are the emotion state in PAD emotion dimension of the final 
hypermedia presentation PAD. Given appropriate number of the training patters (the parameters 
of the handler combination), we expect that the neural network can be trained to give the PAD for 
any handler combination patterns. Note that many combination patters could achieve the same 
emotion state PAD. However, to develop an intelligent styling system based emotion state PAD, it 
is the inversion of the trained neural network serve the need (Figure 2 (b)). We use genetic 
algorithm (GA) to solve the problem of inversion of neural network. 
 
Figure 2: neural network approach for composing handlers to generate a desired target affective presentation  
 
 2
 Figure 5: the operations of neural network in the first step 
Second Step: Searching Handler Combination Patterns Using GA 
The interaction between GA and neural network is as illustrated in Figure 6. The workflow of the 
second step is described in the following steps: 
 Step 1: define the GA parameters:  
The parameter required for running genetic algorithms including the size of a chromosome 
population, the crossover and mutation probabilities, and the number of training epochs. The 
fitness function is defined to the Euclidean distance between the target (P,A,D) and the (P’,A’,D’) 
computed based on the trained neural network.  
 Step 2: Randomly generate an initial population of chromosomes. Each chromosome represents 
a handler combination that has been transformed to binary bit stream.  
 Step 3: Input an individual chromosome into the neural network. Based on the knowledge base, 
neural network will output a (P’,A’,D’) for that handler combination. Input the (P’,A’,D’) into 
fitness function to get the fitness of the chromosome. 
 Step 4: repeat step 3 until all the individuals in the population have been processed. 
 Step 5: choose a group of the chromosomes with the highest fitness level. The chromosomes in 
this group are allowed to repopulate in the next epoch. 
 Step 6: repopulation: Each chromosome chooses randomly a partner for mating. Once two 
organisms have been chosen for crossover, their genetic information will be merged inn order to 
create a new organism. The split position is determined randomly.  
 Step 7: Mutation: A new organism is created by randomly modifying some of its genes. This 
can be done right after reproduction on the newly created child or as a separate process. 
 Step 8: go to step 2, and repeat the process until a specified number of generations has been 
considered. 
 Step 9: decode each chromosome bit stream into the handler combination parameters based on 
the mapping information of the bit stream and each value of parameters. Finally, we could get 
multiple handler combinations which might closely match to the emotion request. 
 4
Based on the above framework, we have implemented a system. Figure 8 shows a snapshot of 
various affective hypermedia presentations generated by our system.  
 
Figure 8: a snapshot of various affective hypermedia presentations generated by our system. 
Conclusion and future work. 
This project developed a framework for intelligent affective hypermedia generation system. The 
neural network and genetic algorithm appears to be promising technologies to be applied in the 
automatic composition of the media handlers that are integrated to be a hypermedia presentation 
with the target affect state.   
參考文獻 
1. Cheng-Yu Lu, Jen-Shin Hong and Samuel Cruz-lara, "Emotion Detection in Textual 
Information by Semantic Role Labeling and Web Mining Techniques", TFIT 2006 
計畫成果自評 
本計畫主要產出為一套“超媒體展示之調適性情感意像呈現系統”, 及與法國學者共同開發
之情緒偵測技術- “Emotion Detection in Textual Information by Semantic Role Labeling and 
Web Mining Techniques” (發表於 2006 年TFIT 台法資訊科技研討會), 計畫所開發之超媒體
合成資訊系統現在繼續由暨大資工系精進中, 將來在數位博物館領域會有相當的應用, 研
究內容與原計畫相符 
 6
暨大於本計畫出國中共派出七名成員赴南錫大學研討，成員包括本計畫資工系洪政欣副教
授、博士生陳可力、賴建穎、林佳宏, 另資工系石勝文副教授、遠距教學中心洪聖豪組長、
資工系博士生周家德等亦另案補助一同前往研討。本次出國主要包括兩個重點，第一在既有
合作計畫中細部討論情境多媒體系統的技術發展細節，第二在討論有關影像處理及電腦視覺
領域的可能合作方向。 
 
在情境多媒體系統的技術發展方面， 雙方合作長期目標在於研發由自動合成多媒體內容的相
關技術，包括: 
 由暨南資工系負責多媒體自動合成系統的開發，包括intelligent styling, intelligent 
scheduling。 
 由雙方合作開發由各國文字內容自動偵測”情境”機制開發, 包括emotion, subject, topic, 
event, location, time 等資訊，此情境資訊將提供多媒體自動合成系統參照進行合宜的
hypermedia styling。 
其中emotion 部份將由博士生呂承諭負責執行，在雙方合作下目前呂承諭以初步完成英文句
子之情感的系統， 並在第三屆中法資訊科技研討會發表 “Emotion Detection in Textual 
Information by Semantic Role Labeling and Web Mining Techniques”一篇學術論文，類似
Semantic Role Labeling 及Web Mining 的技術將用來偵測包括emotion, subject, topic, event, 
location, time 等資訊。 
 
 
 
   
 為了改善視線點的計算，進而提出使用連續兩張 frame 中 2D/2D 點的對應關係，藉由
這方法來得到另外的資訊。視線點的定義為影像和模型中的 2D/3D 對應點所要計算的最小 
cost function，就像是 2D/2D 相對應的 key-points 可以在連續兩張影像上自動的被擷取出來
和對應。 
   
 
 即時的攝影機場景追蹤 
這題目是開發了一個不需要標記得即時多平面環境的攝影機追蹤系統。其中最主要的貢
獻是： 
1. 所選擇的模型可以在攝影機小幅度的移動中減少參數的波動，此外也可以證明使用模
型的選擇方式可以改善視線點的準確性和減少顯著的漂移問題。 
2. 一般的 vision-based 的追蹤方法無法跟上使用者快速或是任意的移動，因此開發了一
個混合系統，可以用來改善 vision-based 的追蹤方法。 
