 2 
clusters. Divisive methods start with one cluster of all documents and recursively split the most 
suitable cluster. Many clustering algorithms belong to hierarchical methods, such as HAC (Voorhees, 
1986), STC (Zamir & Etzioni, 1998), and DIVCLUS-T (Chavent, Lechevallier, & Briant, 2007). 
Partitional methods find the clusters by partitioning the entire document collection into either a 
predetermined or an automatically derived number of clusters. Partitional methods can be 
considered an optimization procedure that tries to create high quality clusters according to a 
particular criterion function to achieve high intra-cluster similarity and low inter-cluster similarity. 
Many clustering algorithms belong to partitional methods, such as K-means (MacQueen, 1967), 
SRE (Zha, He, Ding, Simon, & Gu, 2001), and k-Attractors (Kanellopoulos, Antonellis, Tjortjis, & 
Makris, 2007). 
In this project, we propose a novel clustering algorithm, called On-The-Fly Document 
Clustering (OTFDC). OTFDC is based on a connective semantic relation between the clusters (as 
defined as Section 3), and it has the following four properties: (1) it can be applied to multilingual 
web documents, (2) it improves the clustering performance of any search engine, (3) it uses 
unsupervised learning to automatically identify potentially relevant knowledge without using any 
corpus, and (4) it provides clustering results generated on the fly and fitted into search engines. 
This project is organized as follows. Section 2 introduces related work in the area of document 
clustering. Section 3 discusses OTFDC in detail. We then present some preliminary simulation 
results in Section 4. Finally, Section 5 concludes the project. 
 
 
 4 
represents mutual information between each new domain and the domains already in the document. 
Dai et al. (2007) presented a co-clustering based method to learn an unlabeled data set in an 
unknown domain. The learning process starts with a labeled data set from a known domain which is 
called an in-domain, and applies the learning knowledge to a related but different domain which is 
called an out-of-domain. 
Further researchers have used the query concept to solve the clustering problem. Chang et al. 
(2006) constructed the query concepts to express the user’s information needs, rather than trying to 
reformulate the initial queries. To construct the query concepts, they extracted all document features 
from each document and then clustered these document features into primitive concepts that are 
used to form query concepts. Li et al. (2008) used the query sequence to rearrange clustering results. 
They claimed a query sequence is frequent if it occurs in more than a certain percentage of the 
documents in all document sets. 
 
2.2 Online document clustering systems 
Several online document clustering systems have been proposed in literature. WebCat (Giannotti, 
Nanni, Pedreschi, & Samaritani, 2003) uses Transactional K-Means to generate desirable clustering 
results. CIIRarchies (Lawrie & Croft, 2003) uses a probabilistic technique to extract sentences and 
builds the cluster labels in a hierarchy via a recursive algorithm. Lingo (Osinski & Weiss, 2004) 
uses a combination of phrases and Singular Value Decomposition (SVD) on a term-document 
matrix to find meaningful long cluster labels. SHOC (Zhang & Dong, 2004) uses the suffix array for 
sentences extraction and presents the cluster labels in a hierarchy via Suffix Tree Clustering (STC). 
 6 
search engines. In order to calculate the final ranking of the web documents, we first need to gather 
all the relevant web documents and their ranking which appear in the search results of the search 
engines. Next, we calculate the weight of all relevant web documents by calculating the dot product. 
We need to define a new function called User Behavior Function (UBF) that can be used to 
calculate the weight of all relevant web documents, as shown in the following equation: 
 
 (1) 
 
where  is the user preference of first item;  is the relative order of the object obj in an item list 
l; and  is an important quality control parameter. A strong user’s preference with a large  value 
leads UBF to decrease less as illustrated in the case of  = -0.2 (Figure 1). 
 
<----------Fig. 1----------> 
 
Next, the weight of all the relevant web documents can be derived from UBF, as shown in the 
following equation: 
 
 
(2) 
 
where n is the number of search engines;  is the weight of a web document wpi,m when the 
 8 
association (NISO, 2005). If there is an equivalent relation between Clusteri and Clustert, these two 
clusters express the same concept. For example, (“photocopies”, “Xeroxes”) and (“freedom”, 
“liberty”) exist in an equivalence relation, respectively. A hierarchical relation is based on degree of 
superordination and subordination, where the superordinate cluster may represent a class or a whole, 
and the subordinate cluster refer to its members or parts. For example, (“mountain regions”, “Alps”) 
and (“nervous system”, “brain”) exist in a hierarchical relation, respectively. An associational 
relation includes associations between clusters that are neither equivalent nor hierarchical, though 
the clusters are semantically associated to such an extent that the relation between them should be 
made explicit. For example, (“flour”, “wheat”) and (“Greek civilization”, “Socratic method”) exist 
in an association relation, respectively. 
 
Definition 2 (HSR). (Clusteri,Clustert) exist in a High-level SR (HSR) feature if and only if 
(Clusteri,Clustert) exist in a SR feature and TR(Clusteri,Clustert)1, where R(Clusteri,Clustert) is 
the correlation coefficient between two clusters (as defined later in this section) and T is the 
threshold value (as defined later in Section 4). HSR implies that Clusteri and Clustert not only exist 
in a SR feature but also R(Clusteri,Clustert) is greater than or equal to the lower bound of T. For 
example, (“p2p”,“peer-to-peer”) exist in a HSR feature since we can find a “p2p is an abbreviation 
for peer-to-peer” relation and R(“p2p”,“peer-to-peer”) is 100% which is greater than or equal to T 
(According to the follow-up example and simulation results, we obtain R(“p2p”,“peer-to-peer”) is 
100% and T is 0.801). 
 
 10 
1 Algorithm OTFDC (Clusteri as String) 
2 { 
3 (Clustert, R(Clusteri,Clustert)) = Call OTFDC_Core (Clusteri); 
4 Foreach (Clustert) 
5 { 
6 Append Clustert into FinalClusterSet; 
7 Append R(Clusteri,Clustert) into FinalCorCoefSet; 
8 (FinalClusterSet, FinalCorCoefSet) = Call OTFDC (Clustert); 
9 } End of Foreach; 
10 (FinalClusterSet, FinalCorCoefSet) = Sort FinalClusterSet according to its 
correlation coefficient FinalCorCoefSet; 
11 Return (FinalClusterSet, FinalCorCoefSet); 
12 } End of Algorithm; 
 
OTFDC is recursively invoked on OTFDC_Core that generates a number of the candidate 
clusters for each recursive call. The main assumption of OTFDC_Core is if many important clusters 
link to an important web document, then OTFDC_Core can use the reverse links of an important 
web document to find these important clusters. For example, in Figure 3, “p2p”, “peer-to-peer”, and 
“decentralized p2p” clusters link to an important web document 
“en.wikipedia.org/wiki/Peer-to-peer”. Thus, OTFDC_Core can use the reverse links of 
“en.wikipedia.org/wiki/Peer-to-peer” to find the “p2p”, “peer-to-peer”, and “decentralized p2p” 
 12 
14 Compute the correlation coefficient; 
15 If (R(Clusteri,CandidateClustert)T and 
CandidateClustertFinalClusterSet) 
16 { 
17 Append CandidateClustert into FinalClusterSet; 
18 Append R(Clusteri,CandidateClustert) into FinalCorCoefSet; 
19 } End of If; 
20 } End of Foreach; 
21 (FinalClusterSet, FinalCorCoefSet) = Sort FinalClusterSet according to its 
correlation coefficient FinalCorCoefSet; 
22 Return (FinalClusterSet, FinalCorCoefSet); 
23 } End of Algorithm; 
 
Then, we use the following example to illustrate how to use OTFDC_Core to solve the single-pass 
clustering problem. 
 
Example 1 
OTFDC_Core refers to our previous project which developed the search engine vector voting (SVV) 
method described in section 2.3. Table 1 shows the rank-order distribution and the partial SVV 
results among four major search engines when the user query is “p2p” (Line 3 and 9). In order to 
calculate , s,i and  need to be defined. Initially, the two random numbers are generated 
 14 
 
<----------Table 4----------> 
 
In OTFDC_Core analysis, we also assumed the candidate clusters derived from an important 
web document were more likely to be related to the source cluster. Thus, OTFDC_Core only 
considers the candidate cluster t when the weight of a web document wpt,m  is greater than 
Threshold T (Line 11 and 12). For example, OTFDC_Core does not care if CandidateClustert is 
“mfpnet” since the weight of wpt,m (“en.wikipedia.org/wiki/Peer-to-peer”) is 0.65124 which is less 
than T (0.801). 
To realize the similarity degree of any two clusters, OTFDC_Core uses the following equation 
to calculate the correlation coefficient between any two clusters (Line 14). 
 
R(Clusteri,Clustert)=min(Normi,Normt)/Normi (4) 
 
where Normt is the normalized weight of wpt,m. The candidate cluster is t and R(Clusteri,Clustert) is 
the correlation coefficient between Clusteri and Clustert. For example, in Table 4, 
R(“p2p”,CandidateClustert) is the correlation coefficient between “p2p” and all the partial candidate 
clusters CandidateClustert.  
According to equation 4, we can guarantee the upper bound of R(Clusteri,Clustert) is less than 
or equal to 1 while min(Normi,Normt) is less than or equal to Normi. OTFDC_Core also uses T as 
the lower bound of R(Clusteri,Clustert) to achieve HSR, as shown in the following equation. 
 16 
value (Line 15 in OTFDC_Core).  
A sample procedure of OTFDC has been conducted to produce a number of candidate clusters 
with the correlation coefficient in response to the user query called “p2p”, as illustrated in Figure 4. 
OTFDC sorts all the candidate clusters according to their corresponding correlation coefficient (R). 
For example, R is 100% for “peer-to-peer”.  
 
<----------Fig. 4----------> 
 
Moreover, OTFDC also can be applied to solve multilingual clustering problems when the 
source of OTFDC is multilingual web documents. Figure 5 is the clustering results of “mlb” derived 
from the source OTFDC of Chinese web documents. 
 
<----------Fig. 5----------> 
 
4. Experimental Results 
Three types of performance were performed using various clustering algorithms. First, twelve 
students were selected from National Dong Hwa University to judge the quality of clustering results 
for 39 top queries on the Internet during 2007. Second, we use a computer simulation to determine a 
suitable Threshold (T) value to use in OTFDC. Third, we also simulate OTFDC applied to Google, 
Yahoo, and Vivisimo in order to verify whether OTFDC can improve the clustering performance of 
 18 
first N cluster labels computed by each system.  
The results of the experiment for P@3 are shown in Table 5
3
. The number in every digit cell 
(except the last row) is the average P@3 score of each system on a given evaluated query for twelve 
students. For example, OTFDC has an average P@3 scored 80.95% when the query is “anna nicole 
smith”. The number in the last row is the average P@3 score of each system on all evaluated 
queries for the twelve students. 
 
<----------Table 5----------> 
 
The average scores of OTFDC, Credo, Carrot2, Cats, Highlight, and Vivisimo are 78.51%, 
54.29%, 72.98%, 49.27%, 69.62%, and 74.48%, respectively. Moreover, we have extended P@3 
analysis to P@5, P@7, and P@10 analyses. According to Table 6, we can find the lowest score, 
Cats, for all analyses since its results are restricted to a fixed set of topic and the second lowest score, 
Credo, for all analyses since its results are not always in sentence form. 
 
<----------Table 6----------> 
 
Next, we use SPSS 11.0 for Windows to analyze the results of above experiment. Because the 
results of SPSS are tedious, and thus we refer the reader to (Chen, 2008b) for a full report. 
Considering the performance of OTFDC, Credo, Carrot2, Cats, Highlight, and Vivisimo. We used 
                                                 
3
 All of the experiment results are shown in http://cayley.sytes.net/OTFDC_questionnaire/calculate_quest.php.  
 20 
“en.wikipedia.org/wiki/P2P” (“openp2p.com”) in “p2p” and “peer to peer” are 1 and 2 (2 and 3), 
respectively. Then, we obtain .  
The motivation of T is to minimize the average distance of TKi and TKj. Thus, equation (7) 
implies the following three properties: (1) there are more WPs than WPu, which implies that the 
web documents returned from TKi and TKj are closer to each other; (2) 
 becomes much greater, which implies that the rank of WPs 
appearing in TKi and TKj are fairly similar; (3) the top rank of WPs have greater impact on DISTj 
than the rear rank, which implies that the dominant factor of DISTj is the rank of WPs. 
 Then, we use Mean of DIST (MDIST) to measure the average distances of TKi and all ECKs 
when the number of TKi is 1, as shown in the following equation: 
 
 
(8) 
 
On the other hand, we also use Mean of MDIST (MMDIST) to judge the average distance of 
TKi and all of the ECKs when the number of TKi is not 1, as shown in the following equation: 
 
 
(9) 
 
Figure 6 shows a MMDIST curve based on different T values when OTFDC randomly 
generated 50 s
4
. We find the minimum value of MMDIST is 3.271 when T attains 0.800. 
                                                 
4
 In this project, we randomly selected 1000 queries from metaspy (InfoSpace, 2008) which people are currently 
 22 
and parse them by Sphinx tool (Aksyonoff, 2008); they are returned from Google, Yahoo, and 
Vivisimo in order to form a term-document matrix.  
For the comparison sample, Google and Yahoo are derived from Google AdWords (Google, 
2008) and Yahoo Search Marketing (Yahoo, 2008) keyword suggestion tools. The comparison 
sample of OTFDC-like, Google-OTFDC, Yahoo-OTFDC, and Vivisimo-OTFDC, are the clustering 
labels which are generated from OTFDC applied to its origin term-document matrix. 
In this simulation, we also use MMDIST to judge the clustering performance. Table 8 is shows 
the MMDIST achieved by Google, Google-OTFDC, Yahoo, Yahoo-OTFDC, Vivisimo, and 
Vivisimo-OTFDC when T is set to be 0.801, as described in previous studies. Obviously, 
Google-OTFDC, Yahoo-OTFDC, and Vivisimo-OTFDC outperform their respective origins, Google, 
Yahoo, and Vivisimo. Specifically, when OTFDC is applied to Google and Yahoo, it can improve 
the performance of keyword suggestion tools by at least 17.47 percent. According to the simulation 
results of Vivisimo and Vivisimo-OTFDC, OTFDC also can improve the clustering performance of 
Vivisimo by 7.58 percent. That is, the simulation confirmed the HSR and CHSR feature are superior 
to the original SR feature. 
 
<----------Table 8----------> 
 
 
5. Conclusions 
In this project, we presented a novel clustering algorithm, called On-The-Fly Document 
 24 
2008/10/28, from http://www.sphinxsearch.com/ 
Berkhin, P. (2002). Survey of clustering data mining techniques.   Retrieved 2008/10/28, from 
http://www.ee.ucr.edu/~barth/EE242/clustering_survey.pdf 
Carpineto, C., & Romano, G. (2004a). Concept Data Analysis: Theory and Applications: John Wiley 
and Sons Press. 
Carpineto, C., & Romano, G. (2004). CREDO.   Retrieved 2008/9/22, from http://credo.fub.it/ 
Carpineto, C., & Romano, G. (2004b). Exploiting the Potential of Concept Lattices for Information 
Retrieval with CREDO. Journal of Universal Computer Science, 10(8), 985-1013. 
Chang, Y., Kim, M., & Raghavan, V. V. (2006). Construction of query concepts based on feature 
clustering of documents. Information Retrieval, 9(3), 231-248. 
Chavent, M., Lechevallier, Y., & Briant, O. (2007). DIVCLUS-T: A monothetic divisive hierarchical 
clustering method. Computational Statistics and Data Analysis, 52(2), 687-701. 
Chen, L. C. (2008a). Cayley Search Engine.   Retrieved 2008/9/22, from 
http://cayley.sytes.net/english/ 
Chen, L. C. (2008b). LSD Test Results.   Retrieved 2008/10/28, from 
http://cayley.sytes.net/OTFDC_questionnaire/output.htm 
Chen, L. C., & Luh, C. J. (2005). Web page prediction from metasearch results. Internet Research: 
Electronic Networking Applications and Policy, 15(4), 421-446. 
Chen, L. C., Luh, C. J., & Jou, C. (2005). Generating Page Clippings from Web Search Results 
Using a Dynamically Terminated Genetic Algorithm. Information Systems, 30(4), 299-316. 
Cios, K. J., Pedrycz, W., Swiniarski, R. W., & Kurgan, L. A. (2007). Data Mining: Knowledge 
 26 
Kerne, A., Koh, E., Sundaram, V., & Mistrot, J. M. (2005). Generative semantic clustering in spatial 
hypertext. Paper presented at the Proceedings of the 2005 ACM symposium on Document 
engineering. 
Lawrie, D. J., & Croft, B. (2003). Generating Hierachical Summaries for Web Searches. Paper 
presented at the Proceedings of the 26th Annual International ACM Conference on Research 
and Development in Information Retrieval. 
Li, Y., Chung, S. M., & Holt, J. D. (2008). Text document clustering based on frequent word 
meaning sequences. Data and Knowledge Engineering, 64(1), 381-404. 
Lycos. (2007). Top Search Terms for 2007.   Retrieved 2008/9/22, from 
http://50.lycos.com/121007.asp 
MacQueen, J. B. (1967). Some Methods for classification and Analysis of Multivariate Observations. 
Paper presented at the Proceedings of the 5th Berkeley Symposium on Mathematical 
Statistics and Probability. 
Malerba, D., Esposito, F., & Ceci, M. (2002). Mining HTML Pages to Support Document Sharing in 
a Cooperative System Lecture Notes in Computer Science, 2490, 794-798. 
NISO. (2005). ANSI/NISO Z39.19-2005 Guidelines for the Construction, Format, and Management 
of Monolingual Controlled Vocabularies: NISO Press. 
Osinski, S., & Weiss, D. (2004). Conceptual clustering using lingo algorithm: Evaluation on open 
directory project data. Paper presented at the Proceedings of New Trends Intelligent 
Information Processing and Web Mining. 
Osinski, S., & Weiss, D. (2005). A Concept-Driven Algorithm for Clustering Search Results. IEEE 
 28 
in Document Retrieval. Information Processing and Management, 22(6), 465-476. 
Weiss, D., & Osinski, S. (2004). Carrot Clustering Engine.   Retrieved 2008/9/22, from 
http://demo.carrot2.org/ 
Weiss, D., & Stefanowski, J. (2003). Web search results clustering in Poish: experimental 
evaluation of Carrot. Paper presented at the Proceedings of the New Trends in Intelligent 
Information Processing and Web Mining Conference. 
Wu, B. (2003). Highlight: Concept Hierarchy Developer.   Retrieved 2008/9/22, from 
http://highlight.njit.edu/ 
Wu, Y.-F., & Chen, X. (2003). Extracting features from Web search returned hits for hierarchical 
classification. Paper presented at the Proceedings of the International Conference on 
Information and Knowledge Engineering. 
Yahoo. (2008). Start Advertising with Yahoo! Search Marketing.   Retrieved 2008/9/22, from 
https://signup13.marketingsolutions.yahoo.com/signupui/signup/loadSignup.do 
Yao, Y. Y. (1995). Measuring Retrieval Effectiveness Based on User Preference of Documents. 
Journal of the American Society for Information Science, 46(2), 133-145. 
Zamir, O., & Etzioni, O. (1998). Web Document Clustering: a Feasibility Demonstration. Paper 
presented at the Proceedings of the 21st International ACM SIGIR Conference on Research 
and Development in Information Retrieval. 
Zha, H., He, X., Ding, C., Simon, H., & Gu, M. (2001). Bipartite graph partitioning and data 
clustering. Paper presented at the Proceedings of the 10th international conference on 
Information and knowledge management. 
 30 
The Rank-order distribution among four major search engines when the user query is “p2p” 
 Google Yahoo MSN Askjeeve  
en.wikipedia.org/wiki/Peer-to-peer 1 1 1 1 3.62054 
isohunt.com 2 2 3 2 2.56627 
openp2p.com  0 4 0 4 0.96483 
en.wikipedia.org/wiki/P2P 6 7 2 5 1.93127 
A “0” denote a web document cannot be found by any search engine. 
All the examples presented in this project were collected on 3/1/2008.
 
 
 
 
 
 
 
 
 
 
 
 
Table 2 
The values of s,i and  
 32 
query is “p2p” 
wpi,m  
  Normi 
en.wikipedia.org/wiki/Peer-to-peer 3.62054 
3.62054 
100% 
isohunt.com 2.56627 70.881% 
en.wikipedia.org/wiki/P2P 1.93127 53.342% 
openp2p.com 0.96483 26.649% 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 4 
The parameters of the partial candidate clusters for wpt,m is “en.wikipedia.org/wiki/Peer-to-peer” 
 34 
Query OTFDC Credo Carrot2 Cats Highlight Vivisimo 
anna nicole smith 80.95% 55.48% 77.62% 55.24% 69.76% 71.43% 
badoo  73.81% 57.14% 76.19% 45.24% 66.67% 73.81% 
beyonce  73.81% 52.38% 75.95% 47.62% 66.67% 71.43% 
britney spears  76.19% 59.52% 71.43% 52.38% 69.05% 71.43% 
club penguin  73.81% 52.38% 69.05% 45.24% 64.29% 76.19% 
dailymotion  76.19% 54.76% 73.81% 47.62% 64.29% 71.43% 
disney  73.81% 50.00% 76.19% 47.62% 73.81% 71.43% 
ebuddy  76.19% 54.76% 69.05% 45.24% 71.43% 76.19% 
facebook  78.57% 52.38% 69.05% 52.38% 64.29% 73.81% 
fantasy football  83.33% 57.14% 69.05% 50.00% 71.43% 71.43% 
fergie  78.57% 57.14% 69.05% 50.00% 73.81% 78.57% 
fifa  80.95% 50.00% 69.05% 52.38% 73.81% 71.43% 
golf  76.19% 59.52% 73.81% 47.62% 71.43% 76.19% 
heroes  73.81% 57.14% 76.19% 47.62% 69.05% 78.57% 
hi5  76.19% 57.14% 73.81% 47.62% 71.43% 78.57% 
iphone  83.33% 50.00% 69.05% 52.38% 64.29% 76.19% 
jessica alba  78.57% 54.76% 73.81% 45.24% 69.05% 76.19% 
kazaa  76.19% 54.76% 76.19% 50.00% 66.67% 73.81% 
lindsay lohan  80.95% 50.00% 76.19% 50.00% 71.43% 73.81% 
 36 
Average 78.51% 54.29% 72.98% 49.27% 69.62% 74.48% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 6 
Average P@3, P@5, P@7, P@10 scores for different online systems 
 38 
# s Optimal MMDIST Optimal T # s Optimal MMDIST Optimal T 
50 3.271 0.800 550 3.276 0.750 
100 4.420 0.800 600 3.178 0.800 
150 3.638 0.800 650 4.577 0.825 
200 3.878 0.775 700 4.012 0.800 
250 3.133 0.825 750 4.005 0.750 
300 4.474 0.775 800 3.888 0.850 
350 4.502 0.825 850 4.574 0.825 
400 4.236 0.800 900 4.587 0.775 
450 3.130 0.850 950 4.560 0.800 
500 4.501 0.775 1000 4.142 0.825 
 
 
 
 
 
 
 
 
Table 8 
The distribution of MMDIST when OTFDC applied to different search engines (T=0.801). 
 40 
950 4.139 3.536 14.57% 4.064 3.454 15.01% 3.635 3.716 2.23% 
1000 4.074 3.519 13.62% 3.921 3.597 8.26% 4.042 3.599 10.96% 
Average 4.162 3.429 17.47% 4.209 3.460 17.61% 3.840 3.553 7.58% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 42 
...
Clusteri
 
 Clusterid
Clusterd
...
Clusterdt
  Clustert
...
 
 
 
Fig. 2. The connectivity relation HSR
*
 consists of the pairs (Clusteri, Clustert). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 44 
 
Fig. 4. A sample run of OTFDC (Source is English web documents) when the user query is “p2p”. 
The experiment search engine (http://cayley.sytes.net/english) uses OTFDC to produce a number of 
candidate clusters when the source of OTFDC is English web documents. 
 
 
 
 
 
 
 
 
 46 
 
Fig. 6. Mean of Mean of DISTance Curve for 50 s. This curve estimates the average distance 
TKi and all of ECKs when the number of of TKi is 50. 
 
 
 
 
 
 
 
 
 
 
