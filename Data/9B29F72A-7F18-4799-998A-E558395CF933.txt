Abstract 
The microarray data of ovarian cancer consists of tens of thousands of genes on a genomic scale. 
To avoid higher computational complexity, it needs gene selection to find the gene subsets that are 
able to classify ovarian cancer. In this project, an intelligent hybrid algorithm for microarray data of 
ovarian cancer is proposed. It applies scatter search to obtain suitable parameter settings for support 
vector machine and decision tree. Additionally, it selects a subset of beneficial genes without reducing 
the accuracy of classification, and provides the decision rules for medical experts and biologists to 
evaluate the block effect of selected genes. From experimental results, it shows that the proposed 
algorithm can obtain a better parameter setting, reduce unnecessary genes, and significantly improve 
the accuracy of classification for ovarian cancer. 
 
Keywords: Intelligent Hybrid algorithm, Ovarian Cancer, Microarry Data, Decision Tree, Scatter 
Search. 
摘要 
微陣列資料包含之變數多達成千上萬個，處理卵巢癌微陣列資料分類時，須將此高維度資
料作屬性篩選以節省計算複雜度。本計畫中應用混合式智慧型演算法於卵巢癌微陣列資料，其
使用支持向量機(Support Vector Machine)、決策樹(Decision Tree)及分散式搜尋法(Scatter 
Search)之混合式演算法來有效地判讀癌症/腫瘤資料。本計畫中提出以分散式搜尋法用來選擇
決策樹及支持向量機所需的參數，並可降低高維度資料而不影響分類結果。其中，由於決策樹
可開發用來產生決策方案，所以決策樹可以根據目標基因及特徵基因，提供生醫學者判讀卵巢
癌之Microarray Data法則。由模擬結果得知，本計畫中提出之混合式智慧型演算法，可由分散
式搜尋法來選擇所需參數，降低計算複雜度以確定本方法能夠快速、改善卵巢癌微陣列資料分
類正確率。 
 
關鍵字 : 混合式智慧型演算法，卵巢癌，微陣列資料，決策樹，分散式搜尋法 
 
1 Introduction 
 Ovarian cancer is one of the gynecological cancer deaths for women in the United States [1-3]. It 
is important to early detect ovarian cancer, because 70% of women with the epithelial ovarian cancer 
are not diagnosed until the disease is spread to upper abdomen [4]. Recently, microarray technologies 
have been developed that can be used to simultaneously assess the level of expression of genes [5-10]. 
Then, there is increasing interest in the emphasis of gene selection and cancer classification for 
microarray data [11-14]. The microarray data of ovarian cancer contains genes with tens of thousands 
of dimension and could harm the performance of the accuracy of classification for the formation of 
decision rules. Thus, the gene selection is an important issue for evaluating the accuracy of 
classification of microarray data. Several studies have been used to select genes from microarray data 
such as correlation methods, nonparametric scoring approach, Bayesian variable selection approach, 
and machine learning methods [15-30]. For above literature, they have not simultaneously considered 
the gene selection and the optimal parameter setting for the microarray data of ovarian cancer. 
Furthermore, there is no systematic approach for achieving a better insight into global gene expression 
analysis. In previous study, author has proposed an integrated algorithm for gene selection and 
classification applied to microarray data of ovarian cancer [31]. Its result can interpret the variable 
gene selection and block effect in microarray data, but there is no decision rule for biological 
interpretations. In this project, an enhanced algorithm with gene selection and classification rules for 
ovarian cancer is proposed. The real microarray data of ovarian cancer is obtained from China Medical 
patterns ,i jx x where ( ), ,i j i jk x x x x= . The nonlinear SVM can map the input data into a high-
dimension feature space via a mapping function ( )xφ . By virtue of constructing the feature space, we 
can substitute ( )xφ  into Eq. (4) and have the following:  
( ) ( )
1 , 1
1 ( )= ,
2
N N
i i j i j i
i i j
jMax L y y x xα α α αα φ φ= =−∑ ∑                       (7) 
Given a kernel function ( ), ( ), (k x y x yφ φ= ) , Eq (7) is showed as follow: 
1 , 1
1( )= ( , )
2
N N
i i j i j i
i i j
jMax L y y k x xα α α αα= =−∑ ∑                                  (8) 
Several kernel functions help the SVM in finding the optimal solution. The most frequently used 
such functions are the polynomial kernel, sigmoid kernel and radial basis kernel function (RBF) [34]. 
The RBF ( ) 2, exp( )i j i jk x x x xγ= − −  is the most often used in general cases, since it can classify 
multi-dimensional data unlike a linear kernel function. Furthermore, the RBF needs fewer parameters 
than the polynomial kernel. The two major parameters of the RBF ( C and γ ) applied in SVM have to 
be set appropriately. The accuracy of classification is very high in the training stage and very low in 
the testing stage if C is set too large. The partitioning outcome in the feature space is dominated by the 
parameterγ . An excessive value for parameter γ may lead to over-fitting, while a disproportionately 
small value may result in under-fitting [35]. In this project, scatter search will be conducted to find the 
best values for these two parameters, C and γ . 
2.2 Scatter Search (SS) 
Scatter search (SS), proposed by Glover in 1977, is an evolutionary approach that starts with a 
collection of reference solutions obtained by applying preliminary heuristic processes [36]. SS uses 
strategies for search diversification and intensification based on formulations for combining decision 
rules and problem constraints. Glover presented a simplification of the description for SS method 
known as SS template in 1998. It is considered as a milestone in SS literature and has regarded as the 
main reference for most of the SS implementations up to date [37]. It has proved effective in a variety 
of optimization problems and has shown potential for solving various applications [38-39]. 
Generally, there are five principal components in SS. (1) The diversification generation method 
(DGM) generates trial solutions that satisfy a critical level of diversity by using an arbitrary trial 
solution as an input. (2) The improvement method (IM), a local optimizer, transforms trial solutions 
obtained from DGM into enhanced feasible trial solutions. (3) The reference set update method 
(RSUM) builds and maintains a reference set consisting of high-quality and diverse solutions. The 
reference set is the basis for creating new combined solutions. (4) The subset generation method (SGM) 
is conducted to the reference set and then produces a subset of solutions as a basis for creating 
combined solutions. (5) The solution combination method (SCM) transforms a given subset of 
solutions produced by the SGM into one or more combined new solutions. Since above components 
can be implemented in a variety of ways, SS is very adaptable to solve different problems. 
2.3 Decision Tree (DT) 
Decision tree is an important area of artificial intelligence. It is a rule induction approach that 
utilizes a divide-and-conquer strategy to recursively partition the data set into smaller subdivisions by 
generating a tree-like structure [40-45]. This tree-like structure is composed of a root node (formed 
from all of the data), a set of internal nodes (splits), and a set of terminal nodes (leaves). The C4.5 rule 
is one of the primary approaches in DT [46]. There are two major phases for C4.5. One is growth 
phase and the other is pruning phase [43]. For the growth phase, C4.5 uses an information entropy 
implement SS, the solution P is uniformly generated in the diversification generation method. In 
reference set update method, the size of the reference set is 1 2b b b RefSet= + = . Construction of the 
initial reference set starts with selecting b1 best solutions (solutions with the highest accuracy of 
classification) from P, and these solutions are added to RefSet. For each solution in the solution P, the 
minimum Euclidean distance to the solutions in RefSet is calculated. The solution with the maximum 
of the minimum distances is selected, and this solution is then added to RefSet. Thus the minimum 
distances are updated accordingly. The resulting reference set has b1 high-quality solutions and b2 
diverse solutions. In subset generation method, the size of subsets is set to 2; that is, only subsets 
consisting of all pair-wise combinations of solutions in RefSet are considered. In solution combination 
method, the method employed consists of finding linear combinations of reference solutions. Each 
combination of two reference solutions, denoted as 'X  and "X , are employed to create three trial 
solutions. These three trial solutions are (1) 'X X d= − , (2) 'X X d= + , and (3) "X X d= + , 
where  and u is a random number with values within [0,1]. In the hybrid process of 
SS and SVM, n variable followed with two variables, C and 
" '( )d u X X= − / 2
γ , must be established if n number of 
genes are selected, The value of n variables ranges between 0 and 1. The corresponding gene is not 
selected if its value is less than or equal to 0.5. Conversely, the corresponding gene is selected if its 
value is greater than 0.5. For the hybrid process of SS and DT, two decision variables, designated M 
and CF, are necessary. SS will set the proper parameters of M and CF to increase the accuracy of 
classification. The proposed algorithm is repeated until the stop criterion has met. Thereafter, the 
accuracy of classification, selected genes and decision rules are reported. 
4. Simulation Results 
In simulation, the stop criterion is that the maximal iteration (m_iter=500) has met. We need to 
identify the values of parameters for SS, SVM and DT. For SS, the four parameters are set as P=20, 
b=20, and . The searching range of parameter C of the SVM is between 0.01 and 50,000, 
while the searching range of parameter 
1 2= 1b b = 0 γ  of the SVM is between 0.0001 and 50. Meanwhile, the 
searching range of the parameter M of DT is between 2 and 10, and the searching range of parameter 
CF of DT is between 0.01 and 0.5 [47].  
To verify the performance of the proposed algorithm, various approaches include the hybrid 
process of SVM and SS, the hybrid process of DT and SS, DT, and SVM are used to compare the 
simulation results. For the hybrid process of SVM and SS, SS is used to perform gene selection and 
adjust the parameters of SVM. For the hybrid process of DT and SS, SS is also used to perform gene 
selection and adjust the parameters of DT. It is noted that SVM cannot perform gene selection. For fair 
comparisons, the same values of parameters are used for these compared approaches. Simulations are 
performed to see which approaches can find the best accuracy of classification with selected genes. 
The k-fold approach is used to evaluate the accuracy of classification for the microarray data of 
ovarian cancer obtained form China Medical University [48]. This study set k as 3; that is, the data was 
divided into three portions. Two portions of data are retrieved as training data and the other one is used 
for testing data. The simulation results are shown in Table 1 and Table 2. As shown in Table 1, the 
accuracy of classification of the proposed algorithm is 96.4376% and it outperforms other approaches. 
The proposed algorithm has the minimal selected genes among these compared approaches. Six 
selected genes are extracted from 9600 genes from the proposed algorithm. These genes are 
“peptidylprolyl isomerase D (cyclophilin D)”, “fibronectin 1 (4390)”, “cadherin 4, type 1, R-cadherin 
(retinal)”, “matrix metalloproteinase 2 (gelatinase A, 72kDa gelatinase, 72kDa type IV collagenase)”, 
“protein tyrosine phosphatase, receptor type, K”, “Human insulin-like growth factor binding protein 5 
(IGFBP5) mRNA”. Additionally, the decision rules for the selected genes and classification results can 
[12] K. E. Lee, N. Sha, E. Dougherty, M. Vannucci and B.K. Mallick, Gene selection: a bayesian 
variable selection approach, Bioinformatics 19 (2003) 90-97.  
[13] P. J. Park, M. Pagano and M. Bonetti, A nonparametric scoring algorithm for identifying 
informative genes from microarray data, in: Russ B. Altman, A. Keith Dunker, Lawrence Hunter, 
Kevin Lauderdale, and Teri E. Klein (eds.) Pacific Symposium on Biocomputing 2001, World 
Scientific, New Jersey (2001) 52-63.   
[14] A. Brazma,  P. Hingamp, J. Quackenbush, G. Sherlock, P. Spellman, C. Stoeckert, J. Aach, W. 
Ansorge, C. A. Ball, H. C. Causton, T. Gaasterland, P. Glenisson, F. C. Holstege, I. F. Kim, V. 
Markowitz, J. C. Matese, H. Parkinson, A. Robinson, U. Sarkans, S. Schulze-Kremer, J. Stewart, R. 
Taylor, J. Vilo and M. Vingron, Minimum information about a microarray experiment (MIAME)-
toward standards for microarray data, Nat Genet 29 (2001) 365-371. 
[15] T. Z. Tan, C. Quek and G. S. Ng, Ovarian cancer diagnosis using complementary learning fuzzy 
neural network, in: Proceedings of the 2005 IEEE International Joint Conference on Neural 
Networks (IJCNN 2005), IEEE Service Center, Piscataway, New Jersey (2005) 3034-3039. 
[16] J. D. Schaffer, A. Janevski and M. R. Simpson, A Genetic Algorithm approach for discovering 
diagnostic patterns in molecular measurement data, in: Proceedings of the 2005 IEEE Symposium 
on Computational Intelligence in Bioinformatics and Computational Biology, IEEE Computer 
Society, La Jolla, California (2005) 1-8. 
[17] A. Bertoni and G. Valentini, Randomized maps for assessing the reliability of patients clusters in 
DNA microarray data analyses, Artificial Intelligence in Medicine 37 ( 2006) 85-109. 
[18] J. Khan, J. S. Wei, M. Ringnér, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab, 
C.R. Antonescu, C. Peterson and P. S. Meltzer, Classification and diagnostic prediction of cancers 
using gene expression profiling and artificial neural networks, Nature Medicine 7 (2001) 673-679.  
[19] F. Chu, W. Xie and L. Wang, Gene selection and cancer classification using a fuzzy neural 
network, in: S. Dick, L. Kurgan, P. Musilek, W. Pedrycz, and M. Reformat (eds.) Proceedings of 
NAFIPS 2004, Annual Meeting of the North American Fuzzy Information Processing Society, 
IEEE, Banff, Alberta, Canada (2004) 555-559.  
[20] B. Ni and J. Li, A hybrid filter/wrapper gene selection method for microarray classification, in: 
Proceedings of 2004 International Conference on Machine Learning and Cybernetics 4. IEEE 
Systems, Man and Cybernetics Society, New York (2004) 2537-2542.  
[21] Jin-Hyuk Hong and Sung-Bae Cho, The classification of cancer based on DNA microarray data 
that uses diverse ensemble genetic programming, Artificial Intelligence in Medicine 36 
( 2006) 43-58. 
[22] T. S. Furey, N. Cristianini, N. Duffy, David W. Bednarski, Michèl Schummer and David Haussler, 
Support vector machine classification and validation of cancer tissue samples using microarray 
expression data, Bioinformatics 16 (2000) 906-914. 
[23] Te Ming Huang and Vojislav Kecman, Gene extraction for cancer diagnosis by support vector 
machines-An improvement, Artificial Intelligence in Medicine 35 (2005) 185-194. 
[24] P. C. H. Ma, Keith C. C. Chan, X. Yao and D. K. Y. Chiu, An evolutionary clustering algorithm for 
gene expression microarray data analysis, IEEE Transactions on Evolutionary Computation 10 
(2006) 296-314. 
[25] M, Schummer, W. V. Ng, R. E. Bumgarner, P.S. Nelson, B. Schummer, D. W. Bednarski, L. 
Hassell, R. L. Baldwin, B. Y  Karlan and L. Hood, Comparative hybridization of an array of 
21,500 ovarian cDNAs for the discovery of genes overexpressed in ovarian carcinomas, Gene 85 
(1999) 238-375. 
[26] K. Wang, L. Gan, E. Jeffery, M. Gayle, A. M. Gown, M. Skelly, P. S. Nelson, W.V. Ng, M. 
Schummer, L. Hood and J. Mulligan, Monitoring gene expression profile changes in ovarian 
carcinomas using cDNA microarray, Gene 229 (1999) 101-109. 
[48] H. -T. Lin, and C. -J. Lin, A study on sigmoid kernels for SVM and the training of non-PSD 
kernels by SMO-type methods, Technical report, University of National Taiwan, Department of 
Computer Science and Information Engineering (2003) March, 1-32. 
 
 
 Start 
 
 
Initialize the microarray data of ovarian 
cancer 
 
 
 
 
 
 
Use SS and SVM to select genes and to 
classify cancer tissues 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: The flow chart of the proposed algorithm 
 
 
 
 
 
Output the accuracy of classification, 
selected genes and decision rules 
No 
End
Use SS and DT to build the decision rules 
from the selected genes 
Evaluate the accuracy of classification 
Meet the stop 
criterion? 
Yes 
 
 
 
計畫成果自評 
本研究內容與原研究內容相同，完成的工作項目為： 
1. 建立使用支持向量機、決策樹及分散式搜尋法之混合式智慧型演算法來有效地判讀癌症/腫瘤
資料。 
2.  分析混合式智慧型演算法於卵巢癌微陣列資料之效能。 
3.  驗證混合式智慧型演算法的正確性。 
4. 相關研究成果已刊登於 KES IDT 2009 國際研討會、Fuzz 2009 國際研討會及投稿於國際期
刊 Applied Soft Computing。 
本計畫可使從事本研究之人員獲得多方面的整體訓練計畫及下列具體成果： 
1. 本研究完成後，可使國人在微陣列資料問題的研究更上一層樓。 
2. 藉著獲得新卵巢癌微陣列及繼續探討新法則、新架構的推衍，其後續相關研究成果亦可發表
於國際期刊上。 
  
 
1st KES International Symposium  
on 
Intelligent Decision Technologies 2009  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
報告人: 李 仁 鐘 
 E-mail: johnlee@hfu.edu.tw 
  
 
 
中華民國九十八年四月三十日 
Incorporating Hopfield Neural Networks into Ant Colony System 
for Traveling Salesman Problem 
Yu-Lin Weng 1, Chou-Yuan Lee2, and Zne-Jung Lee 3* 
1 Dept. of Information Management, ST. Mary’s Medicine Nursing and Management College. ylweng@smc.edu.tw 
2 Dept. of Information Management, Lan Yang Institute of Technology. yuan@orion.ee.ntust.edu.tw 
3* Dept. of Information Management, Huafan University.  johnlee@hfu.edu.tw
Abstract  
In this paper, the approach of incorporating Hopfield neural networks (HNN) into ant colony 
systems (ACS) is proposed and studied. In the proposed approach (HNNACS), HNN is used to 
find a plausibly good solution, which is then used in ACS as the currently best tour for the 
offline pheromone trail update. The idea is to deposit additional pheromone to ACS to enhance 
the search efficiency. From simulation results, the search efficiency of HNNACS is better than 
other existing algorithms.1
1. Introduction 
Traveling salesman problem (TSP) is a well-known combinatorial optimization problem. It is 
known as a classical NP-complete problem, which has an extremely large search space [2]. Some 
previous methods, such as simulated annealing (SA) [10] and GA [12] are also widely employed 
to solve TSP. Even though those approaches could find the best solution in those simulated cases, 
the search efficiency did not seem good enough. As a consequence, a novel approach that 
combines Hopfield neural networks (HNN) and ant colony system (ACS) is proposed and has 
been shown to have excellent search performance in this paper.  
Artificial ants are capable of mimicking the behavior of real ants [1]. In an ACS [2], ants are 
capable of exploring and exploiting possible solutions with the use of pheromone information 
left on the ground when they traversed. Each ant collects pheromone information and problem 
characteristics for building solutions individually and the pheromone information will also be 
modified to accumulate experience of this ant. With such mechanisms, this multi-agent (ant) 
algorithm is capable of solving optimization problems efficiently [3]. However, ACS may still 
suffer from inefficiency in various applications. HNN is a parallel network and can be used to 
                                                 
This research was partly supported by the National Science Council, Taiwan, R.O.C. under grant NSC 97-2221-E-211 -017. 
 
neurons and how the weights of the network are determined. 
An appropriate Hopfield energy function and the corresponding weights are defined. The 
following energy function is used in [9]:   
1 2
2
3 4
, 1 , 1
2 2
      ( ),
2 2
xi xj xi yi
x i j i i x y x
xi xy xi y i y i
x i x y x i
V VE s s s s
V Vs N d s s s
≠ ≠
+ −
≠
= +
⎛ ⎞+ − + +⎜ ⎟⎝ ⎠
∑∑∑ ∑∑∑
∑∑ ∑∑∑
        (3)            
where V1, V2, V3 and V4 are positive coefficients [8] and all indices in Eq. (3) are defined in 
modulo N. For convenience, double-indices are used. For instance, sxi denotes the output of the 
neuron corresponding to city x and tour position i. The distance between cities x and y is dxy. The 
first three terms of the energy function correspond to the constraints of TSP and measure the 
degree of whether the solution is a valid path. Assume that the Hopfield net reaches a steady state 
with outputs only either ‘0’ or ‘1’. The first term is zero if and only if each row has not more than 
one ‘1’, which is the constraint that each city is allowed to be visited only once. The second term 
is zero if and only if each column has not more than one ‘1’, which corresponds to the constraint 
that each position of the tour can only be occupied by one city. The third term is to meet the 
constraint that exactly N cities are visited on the tour. The final term penalizes tours with long 
distances. This term is directly proportional to the length of the tour [9]. Hence, the state with the 
lowest energy corresponds to the optimal tour. 
Equation (3) can easily be transformed into the form of the Hopfield energy function as 
shown in Equation (2). Define  
, 1 2 3 4 , 1 ,(1 ) (1 ) ( )xi yj xy ij ij xy xy j i j iw V V V V d 1δ δ δ δ δ δ+ −= − − − − − − +     (4) 
where ijδ  is 1 (i=j) or 0 (otherwise). Then, the energy function in Equation (3) can be rewritten 
as 
         2, ,
1
2 2xi yj xi yj xix y i j x i
CE w s C Ns N= − − +∑∑∑∑ ∑∑            (5) 
It can be shown that this is a Lyapunov equation [9]. This energy function is similar to that 
described in Equation (2) and then can easily be implemented as the Hopfield energy function 
for an HNN. 
3. Ant Colony System 
 In the algorithms, three updating rules are required. They are the state transition rule, the local 
updating rule, and the global updating rule. The state transition rule is also referred to as the Ants 
generation and activity, the local updating rule is online updating rule, and the global updating 
rule is the offline updating rule. Considering an ant at time t positioned at node r, the state 
transition rule is to define the next node as s, which is governed by 
4. The Proposed Algorithm  
In the proposed approach, HNN is used to find a plausibly good solution (locally optimum), 
which is then used in Ant colony Systems (ACS) as the currently best tour for the offline 
pheromone trail update. The idea is to deposit additional pheromone to ACS to enhance the 
search efficiency and such an algorithm is referred to as HNNACS. The idea of HNNACS is to 
use HNN in finding plausibly good solutions and the found solutions are deposited with 
pheromone by the offline pheromone trail update rule. The proposed algorithm HNNACS is 
given in the following. 
 
Procedure: The proposed approach HNNACS 
Begin 
Initialize xis  at random and set parameters V1, V2, V3, V4 of HNN; 
0t ← ; 
While (not converge to generate feasible solution) do  
Conduct HNN to obtain a feasible solution H and calculate the length of the found solution as 
Chnn; 
     If a feasible solution H is found  
           Apply offline pheromone updating rule by using the found best solution; 
       ; 1t t← +
End; 
    While (ACS is not stopped) do 
While (not all ants have generated solutions) do 
 Apply ant’s generation and activity to generate solutions of each ant; 
               Apply the online pheromone updating rule for each ant; 
          End; 
          Apply the offline pheromone updating rule;  
    End; 
    Find out the best path and minimum length of tour;   
End; 
 
In the first phase of HNNACS, a binary vector xis  is randomly generated as the initial vector for 
an HNN and is evaluated via the energy function in Equation (5). Then, the HNN is in function 
to obtain a feasible solution. The obtained solution is deposited with pheromone by the offline 
pheromone trail update rule. Even though the selection of initial vectors may have influence on 
the search performance, all vectors used can have nice convergent behaviors in our experiment. 
We shall report the results in the next section. After the solution H for HNN is obtained and the 
length of H is calculated as Chnn, the offline pheromone trail update is performed to provide clues 
for exploiting possibly good solutions in ACS. The solution H is used as the currently optimal 
solution for ACS and thus, is used to update the pheromone trail by Equation (9) with Δτij (t) = 
1/Chnn. In the second phase, three main processes of ACS are performed in the sequence of ant’s 
generation and activity, online updating rule, and offline updating rule. In other words, traditional 
ACS as stated in Section 3 is employed.  
