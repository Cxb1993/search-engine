1. Introduction
Cluster analysis is a method of grouping data with similar characteristics into larger
units of analysis. Since Zadeh (1965) first articulated fuzzy set theory which gave rise to
the concept of partial membership, based on membership function, fuzziness has received
increasing attention. Fuzzy clustering, which produces overlapping cluster partitions, has
been widely studied and applied in various areas (Bezdek et al., 1999). In fuzzy clustering,
the fuzzy c-means (FCM) clustering algorithms are the best known and most powerful
methods used in cluster analysis (Bezdek, 1981; Yang, 1993). Next, we introduce this
FCM clustering algorithm as follows.
Let X = {x1, x2, · · · , xn} be a data set in a p-dimensional Euclidean space Rp with its
norm denoted by || · ||. For a given c, 2 ≤ c ≤ n, V = {v1, v2, · · · , vc} denotes the cluster
centers where vi ∈ Rp. Let U = (uij)c×n ∈Mfcn be a fuzzy partition matrix, where
Mfcn =
{
U = (uij)c×n|∀i, ∀j, uij ≥ 0,
c∑
i=1
uij = 1, 0 <
n∑
j=1
uij < n
}
.
Then the FCM objective function is defined as follows:
J(U, V ;X) =
c∑
i=1
n∑
j=1
umij ||xj − vi||2
where the weighting exponent 1 < m < ∞ presents the degree of fuzziness. Thus, the
FCM algorithm is iterated through the necessary conditions for minimizing J(U, V ;X)
with the following update equations:
vi =
∑n
j=1 u
m
ijxj∑n
j=1 xj
, i = 1, 2, · · · , c,
and
uij =
||xj − vi||−2/(m−1)∑c
k=1 ||xj − vk||−2/(m−1)
, i = 1, 2, · · · , c; j = 1, 2, · · · , n.
Recently, Wang et al. (2004) used Iris data (Fisher, 1936) to demonstrate that the
performance of FCM is affected by different feature-weight, but if the feature-weights are
not properly chosen for FCM, the algorithm performs poorly. It is therefore important
2
measure of variability, defined as
s =
√∑n
i=1(xi − x¯)2
n− 1 , where x¯ =
1
n
n∑
i=1
xi.
The foregoing measure of dispersion has been expressed in terms of units of the variate.
It is thus difficult to compare dispersions in different populations unless the units happen
to be identical; and this has led to a search for measures independent of the variate scale,
that is to say, pure numbers. In practice, Karl Pearson’s coefficient of variation has been
used extensively, defined by
V =
s
x¯
.
On the other hand, if we have a random sample x = {x1, x2, · · · , xn} ⊂ Rp, where
xj = (xj1, xj2, · · · , xjp) represents the jth sample for j = 1, 2, · · · , n, then the coefficient
of variation of the ith feature is defined as
Vi =
√∑n
j=1(xji − x¯i)2/(n− 1)
x¯i
, where x¯i =
1
n
n∑
j=1
xji, i = 1, 2, · · · , p.
Based on the above discussion, it is sensible to give greater weight to the feature with
larger variation and less weight to smaller variation. Thus, the ith feature-weight wi is
defined as the expectation of the normalized Vi, i.e.
wi = E(Wi) =
∫
ωdFi(ω), where Wi =
Vi∑p
i=1 Vi
, i = 1, 2, · · · , p, (1)
and Fi is the distribution function of Wi. Obviously, the weights wi are normalized with∑p
i=1wi = 1. But how do we get wi? Analytical calculations for wi would be very
unpleasant, because Fi is is more complicated under the general model for the random
sample x. Another approach can employ the same functional of the sample (or empirical)
distribution function Fˆi, which in this instance is the sample mean,
∫
ωdFˆi(ω). (The
empirical distribution is that probability measure that assigns to a set a measure equal
to the proportion of a sample values that lie in that set.) This argument is not always
practicable − a probability density is just one example of a functional of Fi that is not
directly amenable to this treatment. Therefore a more flexible approach is needed. While
4
of interest t(z). The ideal bootstrap estimate of the expectation of t(z) is
eˆ = EFˆ t(z
∗),
where Fˆ is the empirical distribution function which is defined to be the discrete distribu-
tion that puts probability 1/n on each value zi, i = 1, 2, · · · , n, and z∗ = {z∗1 , · · · , z∗n} is
drawn randomly and with replacement from z. Unless t(z) is the mean or some other sim-
ple statistic, it is not easy to compute eˆ exactly. The advantage of the bootstrap method
is that eˆ can be estimated through Monte-Carlo simulation. If B bootstrap replications
are conducted then we can approximate the ideal bootstrap estimate by
eˆB =
1
B
B∑
b=1
t(z∗b),
where each z∗b is a sample of size n drawn randomly and with replacement from z and
t(z∗b) is the value of the statistic t evaluated for z∗b. With probability one conditional on
z, eˆB converges to eˆ as B → ∞. Furthermore, the expected value of eˆB, conditional on
z, equals eˆ. Therefore, eˆB is an unbiased approximation to eˆ, and the performance of eˆB
may be reasonably described in terms of conditional variance,
V ar(eˆB|z) = B−1V ar(t(z∗)|z) = B−1
(
E(t(z∗2)|z)− eˆ2
)
.
In many problems of practical importance, V ar(t(z∗)|z) is asymptotic to either a constant
or a constant multiple of n−1, as n→∞.
The bootstrap algorithm for estimating feature-weights
1. SelectB independent bootstrap samples x∗b = {x∗b1 , x∗b2 , · · · , x∗bn } ⊂ Rp, b = 1, 2, · · · , B,
where x∗bj = (x
∗b
j1, x
∗b
j2, · · · , x∗bjp) are drawn with replacement from x = {x1, x2, · · · , xn} ⊂
Rp.
2. Evaluate the bootstrap replication corresponding to each bootstrap sample using
wˆ∗i (b) =
V ∗i (b)∑p
i=1 V
∗
i (b)
, where V ∗i (b) =
√∑n
j=1(x
∗b
ji − x¯∗bi )2/(n− 1)
x¯∗bi
,
6
WFCM algorithm
Fix m > 1 and 2 ≤ c ≤ n − 1 and set ` = 0. Give any ² > 0 and the initial prototypes
V (0) = {v(0)1 , · · · , v(0)c }.
S 1. Find µ
(`)
ij by update equation (3).
S 2. Find v
(`+1)
i by update equation (2) with (U, V )
(`).
IF maxi ||v(`+1)i − v(`)i || < ² then stop;
ELSE ` = `+ 1 and return to S 2.
3. Experimental results
First, we compare the feature-weight methods proposed by Wang et al. (2004), Modha
and Spangler (2003), Pal et al. (2000) and Basak et al. (1998) with our method on Iris
data. This data set contains three classes, i.e., three varieties of Iris flowers, namely,
Iris Setosa, Iris Versicolor and Iris Virginica consisting of 50 samples each. Each sample
has four features, namely, Sepal Length (SL), Sepal Width (SW), Petal Length (PL) and
Petal Width (PW). This data set has been used in many research investigations related to
pattern recognition and has become a sort of benchmark-data. The feature-weights of Iris
data corresponding to feature name (SL, SW, PL, PW) are shown in Table 1 based on the
different methods. It is well known that the best two features are found to be PL and PW
for Iris data. All four methods reflect this result except Modha and Spangler’s method.
Furthermore, it is hard to discriminate the relative importance of PL and PW from the
proposed feature-weight vector. This result is in agreement with Pal et al. (2000) and
demonstrates that our proposed feature-weight method performs comparable to or better
than the other three methods.
In the FCM, the weighted exponent m is a parameter that greatly influences the
performance of FCM. Recently, Yu et al. (2004) developed a theoretical basis for se-
lecting the weighted exponent in the FCM. Based on theoretical analysis of Yu et al.
(2004), any m > 1 is a valid for the Iris data. Therefore, we consider different values
of m on four different feature weights in the WFCM algorithm and we choose initial
8
on the red pants. From Table 2, we see: (i) Wang et al. method can not identify the
rectangular mark on the red pants; (ii) the left-side gray pants is not clearly segmented
by Pal et al. and Basak et al. method. The original clothes image contains three colors:
white for the backgroud, and ivory for the coat, and a red for the belt. From Table 2, the
proposed method and other methods sucessfully segmented the tree image. Finally, the
last origional clown image contains various colors. The methods of proposed, Wang et
al. and Pal et al. & Basak et al. provided a better segmentation result than the others.
For example, the clothes has the yellow triangular mark in the original clown image. But
the segmentation result obtained by Modha and Spangler became green triangular mark.
Furthermore, the yellow belt on the hat was misclassified as gree color in the segmented
image by Modha and Spangler.
4. Conclusion
In this paper we used a bootstrap method to select the feature-weights in WFCM.
The proposed approach gives reasonable weights to each feature through the statistical
variation viewpoint based on a bootstrapping approach. Based on the proposed feature-
weight method, WFCM performs well on the synthetic and real data and outperforms
Wang et al. (2004), Modha and Spangler (2003), Pal et al. (2000) and Basak et al.
(1998). From the experimental results, the proposed approach shows a good reliability. We
therefore recommend that all those concerned with feature-weight assignment in WFCM
try our proposed approach.
References
Bezdek, J.C., 1981. Pattern Recognition with Fuzzy Objective Function Algorithms.
Plenum Press, New York.
Bezdek, J.C., Keller, J.M., Krishnapuram, R., Pal, N.R., 1999. Fuzzy Models and Algo-
rithms for Pattern Recognition and Image Processing. Kluwer Academic Publishers,
10
Table 1. The error rates with different feature-weights for Iris data
m feature-weight methods feature-weight vector Error rates
1.5 Wang et al. (0.0001, 0.0002, 1.000, 0.1640) 9/150
Modha & Spangler (0.12, 0.36, 0.10, 0.42) 14/150
Pal et al. & Basak et al. (0.0584, 0.1944, 0.9656, 0.6035) 9/150
Our method (0.1017, 0.1031, 0.3365, 0.4586) 9/150
Equal weight (i.e. FCM) (0.25, 0.25, 0.25, 0.25) 17/150
2.0 Wang et al. (0.0001, 0.0002, 1.000, 0.1640) 10/150
Modha & Spangler (0.13, 0.34, 0.10, 0.43) 13/150
Pal et al. & Basak et al. (0.0584, 0.1944, 0.9656, 0.6035) 9/150
Our method (0.1017, 0.1031, 0.3365, 0.4586) 9/150
Equal weight (i.e. FCM) (0.25, 0.25, 0.25, 0.25) 16/150
5.0 Wang et al. (0.0001, 0.0002, 1.000, 0.1640) 10/150
Modha & Spangler (0.17, 0.39, 0.08, 0.36) 15/150
Pal et al. & Basak et al. (0.0584, 0.1944, 0.9656, 0.6035) 9/150
Our method (0.1017, 0.1031, 0.3365, 0.4586) 8/150
Equal weight (i.e. FCM) (0.25, 0.25, 0.25, 0.25) 15/150
10.0 Wang et al. (0.0001, 0.0002, 1.000, 0.1640) 10/150
Modha & Spangler (0.18, 0.45, 0.07, 0.30) 16/150
Pal et al. & Basak et al. (0.0584, 0.1944, 0.9656, 0.6035) 9/150
Our method (0.1017, 0.1031, 0.3365, 0.4586) 7/150
Equal weight (i.e. FCM) (0.25, 0.25, 0.25, 0.25) 12/150
12
