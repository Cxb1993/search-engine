2 
 
行聽者語音標記，經由統計分析標記人員所標記的資料，我們將一致性超過 80%的語句
留下，作為後續訓練與測試用之語料。 
傳統的客服系統每日為企業處理大量的客戶要求。有些客服系統會將通話過程記錄
保存以供事後分析[2]。但是這些記錄只能顯示發生過爭吵的事實，而客戶可能已經在那
次對話中對該企業失去信任，即使事後補救，也已經對企業形像造成了損害。客戶流失
的原因之一是沒有立即安撫客戶並解決問題，若是有一套客服系統能立刻偵測客戶與客
服人員間的爭吵情況，並提示資深客服人員介入處理，將可在短時間內化解客戶與客服
人員的衝突，有助於提升企業形象，保持客戶與企業間的良好關係。因此本研究的第二
個目標是將我們所發展出來的情緒辨識引擎應用到 VoIP 客服系統上，我們在實驗室建
置了一部以 Proxy模式運作 SIP-PBX伺服器，並將所有進出該伺服器的封包導引到封包
分析與蒐集器，把與 SIP 相關的封包濾出，建立電話之 Session 與抽取電話語音封包，
接著將語音自所收到的封包中還原並送入語音辨識引擎，情緒辨識引擎根據分類器對語
句作情緒辨識。我們提出的這套架構，可以將所擷取之 VoIP 語音封包進行即時的情緒
辨識並將結果呈現在管理人員的電腦上，以進行後續之因應措施，例如在發生爭吵時，
就能即時啟動其他的控管機制，以助提升企業形象。 
 
三、文獻探討 
在長句中文連續語音情緒辨識方面，到目前為止除我們這幾年做了一些像是語句情緒單
元切割等初步的研究之外，並無其他相關文獻討論，因此這裡僅就短語句語音情緒辨識
的相關研究來探討。 
　 在[3]中，研究所使用的語料庫是由兩位女性語者在不同情緒（包括 normal、anger、
boredom、happiness 及 sadness）下所錄製，分別表達出 12 個不同字數的句子，一共有
591句短情緒語音，選用的語音參數為梅爾刻度頻譜係數，經 LBG演算法量化成固定長
度的特徵向量，除此之外，還利用中文語音以母音為基礎的特性，對訓練所得到的特徵
向量作情緒特徵之強化，以作為在不同情緒下之語音標準參考樣本（standard reference 
patterns），接著利用動態扭曲演算法，計算出測試樣本與參考樣本之最小距離來做辨識，
此系統的平均辨識率約 50%。 
　 在[4]的研究中， 語音參數是使用 13階MFCC係數，情緒狀態包含 anger, happiness, 
neutral 及 sadness.，分類器是使用音素階級模型所訓練出的 HMM，辨識率可以達到
76.12%。 
　 在[5]中，作者比較 3種不同的分類器在語音情緒辨識率上的表現，這 3種分類器分
別是 kernel-based 分類器（作者所提出的）、內插語言模型分類器及互訊息（mutual 
information）分類器，實驗結果顯示作者所提出的分類器表現優於其他兩個分類器，辨
識率可以達到 80.6%。 
　 在[6]的研究中，作者嘗試使用類神經網路分類器來辨識語音情緒種類，語料庫包含
391個句子，參與錄製的語者共有 11位，情緒種類只針對 angry及 neutral兩種，特徵參
數包含 pitch、energy及 rhythm三種，實驗結果顯示辨識率可以達到 86.1%。 
　 在[7]的研究中，作者使用雙模方式，結合語音和臉部表情來加強語音情緒辨識的不
4 
 
擇，得到在音情緒辨識上較有效的特徵包括 Mel-Frequency Cepstral Coefficients 
(MFCC)、LPC Cepstral Coefficients (LPCC)及 Log Frequency Power Coefficients (LFPC)
特徵[11]。 
分類器的目的是將擁有相似特徵的東西分成一類，由於考量未來應用時可能會需要
處理上百組電話同時連線，因此需要選用一個計算複雜度低但精確度還可被接受的分類
法。常用於語音情緒辨識相關研究的分類器包括 Support Vector Machine (SVM)、Hidden 
Markov model (HMM) 、 Gaussian mixture model (GMM) 及 Weighted D-KNN 
(WD-KNN)[11]等，其中 WD-KNN 因為是採用距離向量做為辨識依據，計算複雜度較
低，且經實驗證實其辨識率高[11]，所以本研究採用WD-KNN 做為分類器。 
Voice over Internet Protocol (簡稱 VoIP或 IP 電話)，是一種透過網際網路 (Internet)
來實現的電話通訊，過去 IP 電話主要在大型企業的內部網路使用，隨著網路的普及，
IP 電話亦被廣泛地應用在長途電話服務上。由於情緒辨識是分析數位化後的數位語音
訊號，傳統電話需要經過數位到類比轉換器轉換後才可以取得數位化的語音訊號，而
VoIP的語音串流，只要經過適當的解碼器來解碼，即可由攔截下來的封包中取得數位化
之語音訊號。再加上近幾年 VoIP 網路電話的發展日趨成熟，它又具有可以整合電話通
訊與數據網路合而為一，以及適當運用可以大量節省電話通訊費用的特性，越來越多的
企業改採網路電話方式佈建其電話網路。基於以上數點，本研究乃以 VoIP 網路電話為
平台，實作一個實驗性結合語音請緒辨識功能之客服中心系統。 
 
圖 1: VoIP 通訊流程示意圖 
 
VoIP通訊流程 
圖 1 顯示在 VoIP環境中，聲音訊號從來電端到目的端的基本流程，1為聲電轉換，
通過話筒(或麥克風)等裝置將聲波轉換為電信號。2 為數位化，將電信號按照設定的取
樣方法(例如 PCM)轉換為數位信號。3為語音編碼，將語音資料做編碼(例如 G.711)以降
6 
 
 
圖 3: 結合語音情緒辨識引擎之 VoIP 系統架構 
 
 
圖 4: 具情緒辨識引擎之 VoIP客服系統運作流程 
 
在客服系統中，每一條線路都是各自獨立的，同一時間可能有多條線路在同時使
用。所以在實作的程式中，為了區別不同的 Session，我們建立 Session 物件，並以 LinkList 
的結構做管理。每一個 Session 物件被建立時，會記錄相關的參數在標頭中，包括，來
源號碼、來源 IP、建立時間、語音編碼、使用緩衝區等資訊。當通話結束後，相對應的
8 
 
一定的次數後，以紅色畫面呈現生氣的狀態，並於偵測到情緒緩和後，回復到綠色狀態。 
   
(a)     (b) 
圖 5: 語音情緒辨識結果展現。(a)正常狀態(綠色)；(b)異常狀態(紅色) 
 
五、結果與討論 
在本研究中，我們邀請了 34 位配音員協助我們錄製中文情緒語音，經由參與研究
之兼任助理與其他研究生的協助，耗費一個多月的時間將接近 40 個小時的情緒語音切
割並存檔，接著邀請 40餘位未參與錄音的人進行情緒評分與標記，最後經過統計分析，
將一致性超過 80%的語句留下，構建了一個到目前為止，文獻有記載的最大之中文情緒
語料庫。 
企業若是對於客戶的意見及需求不能妥善解決，將會嚴重的損害企業形象，久而久
之便會失去客戶的信賴而失去競爭力。然而這些狀況往往可能僅是因為客服人員對危機
處理不夠即時而造成。在本研究中，我們實作了一套結合 VoIP 與情緒辨識的客服系統
架構。我們使用模組化的設計來開發各自獨立功能的子系統，包含封包擷取、封包解析、
語音解碼、錄音及情緒辨識引擎。經由這些子系統的整合運作，建立一套能偵測爭吵發
生並提出警示之系統。經由這套系統的監控，當通話過程發生爭吵時，能立即的提出警
示，讓資深的客服人員能即時發現並協助處理。藉此改善傳統客服對於危機處理不夠即
時的缺點。 
未來，我們將持續對新錄製的情緒語料庫進行分析研究，例如標記者的性別及年齡
與情緒的判定的關聯性、語者的性別與標記者對情緒辨識一致性的關聯性等。另外，我
們也希望能解決幾個問題來完善系統。首先是持續擴充辨識引擎使用的語料庫來改善辨
識引擎的準確性。接著可以增加自動通知資深客服人員的功能，例如，透過後端控制系
統自動將通話轉接到資深客服人員專用的話機，以避免警示發出卻無人接管的情形。 
 
10 
 
14. Jun-Heng Yeh, Tsang-Long Pao, Ren-Chi Tsao and Ren-Fu Luo, "Analyze Multiple 
Emotional Expressions in a Sentence," Proceedings Advanced Intelligent Computing 
Theories and Applications, LANI 6216, pp. 657-662, Springer, 2010 
15. 羅仁甫、包蒼龍、曹仁旗，VoIP 客服系統結合負面語音情緒偵測之研究，2010 資
訊科技與應用研討會，Jan. 2010 
16. 葉俊亨，連續中文語音情緒辨識之研究，大同大學資訊工程研所 博士論文 2010 
17. 曹仁旗，VoIP 客服系統結合負面語音情緒偵測之研究，大同大學資訊工程研所 碩
士論文 2010 
18. 羅仁甫，語料參數與聽者在語音情緒辨識之關聯性研究，大同大學資訊工程研所 碩
士論文 2010 
 
七、計畫成果自評 
本計畫的執行成果，在論文發表方面，計發表了一篇期刊論文、兩篇國際研討會論
文及一篇國內研討會論文。參與計畫的人員除一位博士班兼任助理及兩位碩士班兼任助
理外，尚有另外一位博士班研究生與三位碩士班研究生參與部分研發工作，對於相關理
論與實務的人才培育，提供了甚大的助益。 
 在實作成果方面，我們建構了一個到目前為止最大的中文語音情緒語料庫，未來可
以使我們在情緒辨識的可信度方面大大的提升，另外我們也實作了一套具語音情緒辨識
功能的 VoIP 客服系統的雛形，假以時日，將各子系統發展到更完善的地步後，應可實
作到現有的電話語音客服系統上，提升顧客對客服系統的滿意度，進而提升企業之形象
與競爭力。 
表 Y04 
過往甚密，因此有不少台灣的學者參與協助辦理這個會議，包括致贈與會者的背包與紀念品都
是在台灣訂作的，因此未來應該繼續爭取與主辦單位配合，增加本地廠商的商機。 
    由於參加國際研討會可以提升學者的國際視野，藉由和與會者的交流，可以達到同儕互相
砥礪學習之效果，在國際化十分重要的今天，絕對會有正面的影響，因此產官學界都應該大力
鼓吹學者多多參與各項研討會，但要能形成風潮，在經費補助方面需要更為積極，尤其是對於
私校教師來說，爭取經費是屬於比較弱勢的一群，國科會在這方面應該可以再主動一些。 
 
四、攜回資料名稱及內容 
本次共攜回論文集一冊，論文集只包含會議議程及論文摘要，電子檔則收錄於所附贈之 MP3 播
放器中。 
 
五、其他 
 
 
 
 
 
 
       SMC 2009 
other western languages. In Mandarin, the morpheme is formed 
by a sound plus a tone. Commonly used Mandarin characters 
are more than 10,000, all pronounced as monosyllables. A 
sound is the combination of an initial and a final in Mandarin. 
There are 21 initials and 16 finals. Tone is an important and 
distinct part of Mandarin. Each syllable of a Mandarin 
character needs to pronounce with a right tone or otherwise the 
meaning may be quite different. There are four tones plus one 
neutral. However, tones are probably the most difficult part of 
Mandarin. The combination of the initial, final and tone yields 
around 1,340 Mandarin sounds. In Mandarin, the initial is a 
consonant that begins the syllable while the final is a vowel that 
ends it. By monosyllabic property of Mandarin speech, we can 
consider that each Mandarin word utterance will have the same 
frame size if it is spoken at the same speed. Hence, we propose 
to use the distance-based classifier in the recognizer. 
B. Acoustic Feature Extraction 
According to the study of speech production, human can 
produce various sound by varying the shape of mouth and 
vocal tract. These properties are short-time stationary in terms 
of frequency domain response. In order to recognize a speech, 
we need to get features not only from time domain but also 
from the frequency domain. 
The Mel-frequency cepstral coefficients (MFCC) have been 
shown to be more effective than other features in the speech 
recognition research. To obtain MFCCs, the input signal is 
preemphasized and divided into fixed length frames which may 
overlap with its adjacent frame to avoid the boundary effect. A 
hamming window function is applied to the frame before the 
short-time log-power spectrum is computed. Then the spectrum 
is smoothed by a bank of triangular filters, in which the pass-
bands are laid out on a frequency scale known as Mel-
frequency scale. The filtering is performed by using the DFT.  
By the monosyllabic property of Mandarin as described 
previously, the number of samples of a Mandarin utterance can 
be viewed as fixed. Therefore, the MFCC of the frames in a 
word utterance forms the feature vector. 
C. Visual Feature Extraction 
The features for visual speech information extracted from 
image sequences can be geometric features, model based 
features, visual motion feature, and image based features[11], 
[13]. Geometric feature based techniques assume that certain 
measures such as height, width or area of the mouth opening 
are important. Petajan [25] developed an audio-visual 
recognition system that was based on geometric features of the 
mouth opening, like height, width and area. A simple threshold 
technique was first used to find the mouth area. Potamianos 
[13] and Zhi [10] described another semi-automatic lip-reading 
system which was based on the extraction of the lip contour 
features from the previous marked points. In its 
implementation, the speaker’s lip contours were extracted from 
the image sequence. The motion-based feature approach 
assumes that visual motion during speech production contains 
relevant speech information. Visual motion information is 
likely to be robust to different skin reflectance and speakers.  
D. Color Processing of Face Image 
The RGB color model is widely used in computer vision 
because the red, green, and blue colors are the most natural 
color form and can be used to reconstruct almost all the visible 
colors. However, its inability to separate the luminance and 
chromatic components of a color hides the effectiveness of 
color in object recognition from image. To separate the 
luminance and chromatic components, various color space 
transform methodologies can be used, such as the normalized 
RGB space, HSL(hue, saturation, and luminance), HIS(hue, 
intensity, and saturation), HSV(hue, saturation, and value), and 
YCbCr. 
The choice of an appropriate color space is an important 
factor for successful image segmentation and feature 
extraction. The measurement of skin reflectance, light spectral 
power distribution and camera channel sensitivities allow the 
computation of ideal RGB values for different skin types. The 
conversion to normalize color space (r, g) chromaticity is 
defined as 
 r  = R / (R+G+B),  (1) 
 g = G / (R+G+B).  (2) 
This transform can reduce the brightness dependency of skin 
color in an image. The conversion between RGB color and 
luminance is defined as 
 Y = 0.3R + 0.59G + 0.11B.  (3) 
In our automatic visual feature extraction operation, we use the 
normalized color space and the luminance. 
E. Classifier 
The nearest neighbor classifier can be used to improve the 
performance of the voice identification [1]. The WD-KNN 
classifier [6] is a weighted-distance classifier, derived from the 
basic unweighted-discrete KNN method. A short description of 
WD-KNN is given as follows. The collected features are split 
into data elements x1,…, xt, t being the total number of training 
samples. The space of all possible data elements is defined as 
the input space X. A feature map is defined to be a function that 
takes an input element in the input space and maps it to a point 
in the feature space. In general, we use φ to define a feature 
map and get 
 FX: →φ   (4) 
When a test sample y and a specified distance measure are 
given, we obtain the k nearest neighbors belonging to class j, 
( )yj lkN , , which can be defined as: 
 
( ) ( ) ( ){ }jcNN lkj lk =∈= zyzy :,, , (5) 
where the cardinality of the set ( )yj lkN ,  is equal to k.  
       SMC 2009 
at 0.65Deye from the bottom of nose region. Now, the bounding 
box for the mouth region can then be obtained. We can then 
compute the horizontal and vertical histogram of the binarized 
mouse image. From these two histograms, we can estimate the 
shape of the mouth. The center of the widest peak will define 
vertical position of the medial point of the mouth. By choosing 
the widest peak, the possibility of detecting the nose instead of 
the mouth is avoided. Fig 3(a) shows the detected bounding 
box of the eyes, while (b) and (c) illustrate the bounding boxes 
of the nose and mouth extracted from the face image, 
respectively. 
B. Lip Feature Extraction 
To split the lip region from the background, we use two 
threshold operations. The mouth image inside the bounding 
box is first transformed into the gray level image and processed 
by applying histogram equalization. Then a threshold is applied 
to the image which transforms the image into binary image. 
The gray scale image, equalized image and binary image of the 
mouth are shown in Fig. 4(a), (b) and (c), respectively. The 
erosion operation is then applied to the image to eliminate the 
unwanted noises. Based on the resulting image, we extract the 
lip region from its surroundings by finding the largest 
connected region. 
Segmentation results from above processing steps are 
demonstrated in Fig. 5. The selected feature points are marked 
by small squares.. We observed that the feature points are all 
well match the true lip area. From the segmented lip image, we 
are able to extract the key feature points on the lips.  
In our system, the bounding box is first extracted from the 
original image. Then we apply the proposed algorithm to 
automatically locate the six feature points on the mouth image. 
By locating the six feature points of the mouth for every video 
frame, we get the motion and geometric features for the 
recognizer. The main features, corners of the mouth, are then 
found as the cues for geometric parameters and motion vectors. 
In our system, the geometric parameters, including contour 
width, height, perimeter, area and motion feature vectors from 
subsequent frames are used as the feature vectors. Finally, these 
geometric parameters and motion vectors for selected feature 
points formed the feature vectors.  
 
 
                    
(a) gray scale mouth region                  (b) histogram equalization 
 
                  
(c)binarization                                       (d)erosion 
Figure 4.  Process of  mouth region image processing. 
 
 
Figure 5.  The extracted lip feature points used in the proposed system. 
C. Audio Feature Extraction 
The MFCC feature is the audio feature for our Mandarin 
audio-visual speech recognition system. The MFCC feature 
vector contains 12 cepstral coefficients extracted from the Mel-
frequency spectrum of the frame with normalized log energy. 
As described in Section II, we need to build the input 
feature vector for the WD-KNN classifier. The elements of the 
input space are mapped into points in a feature space F. In our 
work, a feature space is a real vector space of dimension n, ℜn. 
Hence, each point fi in F is represented by an n-dimensional 
feature vector: 
 fi = (MFCCi1, …, MFCCip, VFi1, ..., VFiq) (8) 
where p, q are the dimension of MFCC and Visual Feature(VF), 
respectively, and 
 n = p + q.  (9) 
By the monosyllabic sound property of Mandarin, the 
number of frames of a Mandarin utterance can be considered as 
fixed. The dimension p of MFCC feature is combined from all 
frames of a word utterance by average. 
IV. EXPERIMENTAL RESULTS 
We applied the feature extraction algorithm on the audio-visual 
speech corpus. As the audio-visual database is concerned, there 
have been efforts in creating database for the audio-visual 
research area. Most of these databases are in English or other 
language, such as Tulips1, AVLetters, M2VTS [8], CUAVE, 
etc. Thus, to perform the audio-visual speech recognition in 
this research, we need to build an audio-visual database of 
Mandarin speech. The audio-visual database was recorded from 
40 speakers. The video is in color with no visual aids given for 
lip or facial feature extraction. In this database, each individual 
speaker was asked to speak 40 isolated Mandarin digits, facing 
a DV camera. 
In this experiment, the video stream is a sequence of 17 to 
25 images for each utterance. Not all of the image sequences 
for Mandarin utterance were used in the recognition. In WD-
KNN  or distance-based classifiers, since the distance between 
the feature vectors is computed, the size of each feature vector 
must be the same. Visual features are selected from fixed 
number of images from each utterance. 
The audio recognizer use 12 MFCC features extracted from 
speech sample at 8 kHz. The WD-KNN classifiers were 
implemented with k equals to 10.The selection of weight used 
in the WD-KNN is an important factor for the recognition 
無衍生研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
