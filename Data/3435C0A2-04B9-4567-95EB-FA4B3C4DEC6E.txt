The advent of media-sharing sites like Flickr and YouTube has drastically increased the 
volume of community-contributed multimedia resources on the web. However, due to their 
magnitudes, these collections are increasingly difficult to understand, search and navigate. 
With the kind support from NSC, we investigated methods to improve search quality (by 
reranking) and recommend supplementary information (i.e., search-related tags and canonical 
images) by leveraging the rich context cues, including the visual content, high-level concept 
scores, time and location metadata. Through rigorously conducting the research for the 
project, we devised fruitful and significant results in terms of publications [1][2][3], research 
community services [4][5][6], and joining prestigious international technical committee 
[7][8][9][10][11]. We append the accepted (also submitted) papers in this report. We also 
pointed some important directions in this research paradigm for the research community 
[2][4][5][6].  
 
Publications: 
[1] “Online Reranking via Ordinal Informative Concepts for Context Fusion in Concept Detection 
and Video Search,” Yi-Hsuan Yang, Winston H. Hsu, and Homer H. Chen, IEEE 
Transactions on Circuits and Systems for Video Technology (TCSVT), vol. 19, no. 12, pp. 
1880–1890, Dec. 2009. 
Abstract: To exploit the co-occurrence patterns of semantic concepts while keeping the 
simplicity of context fusion, a novel reranking approach is proposed in this paper. The 
approach, called ordinal reranking, adjusts the ranking of an initial search (or detection) list 
based on the co-occurrence patterns obtained by using ranking functions such as ListNet. 
Ranking functions are by nature more effective than classification-based reranking methods 
in mining ordinal relationships. In addition, the ordinal reranking is free of the ad hoc 
thresholding for noisy binary labels and requires no extra offline learning or training data. To 
select informative concepts for reranking, we also propose a new concept selection 
measurement, wc-tf-idf, which considers the underlying ordinal information of ranking lists 
and is thus more effective than the feature selection algorithms for classification. Being 
largely unsupervised, the reranking approach to context fusion can be applied equally well to 
concept detection and video search. While being extremely efficient, ordinal reranking 
outperforms existing methods by up to 40% in mean average precision (MAP) for the 
baseline text-based search and 12% for the baseline concept detection over TRECVID 2005 
video search and concept detection benchmark. 
 
[2] “Knowledge Discovery from Community-Contributed Multimedia,” Tao Mei, Winston H. Hsu, 
Jiebo Lui, IEEE Multimedia Magazine, October-December, 2010. 
 
[3] “Adaptive Learning for Multimodal Fusion in Video Search,” Wen-yu Lee, Po-Tun Wu, 
Winston H. Hsu, IEEE Pacific-Rim Conference on Multimedia (PCM) 2009. 
Abstract: Multimodal fusion had been shown prominent in video search for the sheer volume 
of video data. The state-of-the-art methods address the problem by query-dependent fusion, 
Travel Support: 
We also thank the travel support for this project, which has helped the PI attend the premier 
conference in computer vision, IEEE Conference on Computer Vision and Pattern Recognition 
(CVPR) 2010. The goal for the visit is that we are hoping to have further publications in the 
international conference and need to first sense the publication culture and technical interests for 
the community. The other goal is to attend the technical program committee (TPC) meeting for 
ACM Multimedia 2010. 
 
This is the first time for PI to attend CPVR. The number of registered participants is very high 
(~1800). It’s apparent that computer vision is an active research fields. We also met quite familiar 
researchers from multimedia (majorly in image/video analysis and retrieval) and machine 
learning communities. It deems that the boundaries between different research communities are 
getting blurred.  
 
The further quick snapshot for the travel is summarized in the travel report.  
YANG et al.: ONLINE RERANKING VIA ORDINAL INFORMATIVE CONCEPTS FOR CONTEXT FUSION IN CONCEPT DETECTION AND VIDEO SEARCH 1881
Fig. 1. Architecture of the proposed ordinal reranking framework for context
fusion, with applications to video search and concept detection. The initial
result from a text-only search model or a pre-trained concept detector is
taken as an approximation of the target semantics. A ranking algorithm is
then employed to mine the co-occurrence patterns of extracted features (i.e.,
concept detection scores [4]) to rerank the initial result.
It was first explored in prior work for concept detection
[15]–[18] and then extended to video search under a reranking
framework [5]–[7], which aims to leverage the co-occurrence
patterns between target semantics and extracted features (high-
level concept detectors or low-level visual features) to refine
(by reordering) the result of an initial text-based search. A
typical approach, referred to as classiﬁcation-based reranking
[7] in this paper, takes the higher rank and lower rank results
of a baseline system as pseudopositive and pseudonegative
examples to train a discriminative model and regards the
normalized classification score for each object in the ini-
tial list as its reranked score. Since reranking is largely
unsupervised, it can be applied equally well to context
fusion in both concept detection and video search tasks.
Salient performance gain over baseline methods has been
reported in [7]. In addition, the simplicity of the keyword-
based search paradigm is maintained under the reranking
framework.
Though the classification-based reranking method has the
advantage that existing classification methodologies can be di-
rectly applied, it is not free of problems. First, the formulation
of learning as a minimization of classification errors neglects
the underlying ordinal information of the initial list. Second,
the classification-based reranking resorts to an ad hoc mech-
anism for determining the threshold for noisy binary labels.
In addition, determining the pools of the pseudopositive and
pseudonegative sets, which is vital to the system performance,
is not straightforward.
In this paper, to exploit contextual information for concept
detection and visual search, we propose a novel reranking
method, called ordinal reranking, that employs ranking algo-
rithms such as RankSVM [11] and ListNet [20] to learn the
co-occurrence patterns between target semantics and features
extracted from the initial list. Since the objective function is to
minimize errors in object ranking, ordinal reranking is more
effective and efficient for mining ordering information and free
of the ad hoc thresholding problem.
Fig. 1 gives an illustrative example. A baseline model,
which can be a text-based search model or a pre-trained
concept detector, retrieves video shots (basic video retrieval
units) that match the target semantics “Find shots of boats.”
Besides false positives (e.g., images with an anchorperson or
crowds), there are still certain relevant shots ranked low due
to the semantic gap or the lack of associated keyword anno-
tations. From the noisy initial ranked list, ordinal reranking
mines the co-occurrence patterns and identifies “ocean” and
“outdoor” as the relevant concepts. Reranking is then made
by reordering the shots with high search scores linearly fused
by these relevant concepts.
We further investigate concept selection methods that au-
tomatically select informative concepts for reranking in an
unsupervised fashion. Considering visual objects (video shots
or images) as documents and concepts as visual terms,
we improve the c-tf-idf (concept tf-idf2) measurement [10]
by incorporating the ordinal information provided by the
initial list. The new measurement, weighted c-tf-idf (wc-
tf-idf), has promising performance and further improves
reranking.
Besides being extremely efficient, ordinal reranking outper-
forms existing reranking methods and improves up to 40% in
mean average precision (MAP)3 for the baseline text-based
search and 12% for the baseline concept detection, when
evaluated on the TRECVID 2005 video search and concept
detection benchmark [1].
In summary, the primary contributions of the paper include
the following.
1) To our best knowledge, the proposed ordinal reranking
method represents one of the first attempts that utilize
ranking algorithms for reranking (Section IV). Because
the underlying ordinal information is better exploited,
ordinal reranking outperforms existing reranking meth-
ods in both effectiveness and efficiency.
2) As far as we know no feature selection measure has been
designed specifically for reranking. We adapt the famous
tf-idf measurement to the reranking framework by taking
the ordering of the objects into account (Section V).
3) An extensive performance study including comparisons
to existing reranking methods, parameter sensitivity test,
and analysis of the result of concept selection, is con-
ducted (Section VI).
II. Related Work
As shown in Table I, context fusion approaches can be cat-
egorized into “offline” and “online” methods. Offline methods
use annotations of training data to discover the contextual
information, while online methods approximate the initial
result of a baseline system as pseudoground truth to further
rerank the initial result. Below we briefly review some existing
methods.
2
“tf-idf” stands for term-frequency inverse-document-frequency [14].
3MAP: mean average precision, a performance metric used in TRECVID
for concept detection and search [1].
Authorized licensed use limited to: National Taiwan University. Downloaded on December 9, 2009 at 19:40 from IEEE Xplore.  Restrictions apply. 
YANG et al.: ONLINE RERANKING VIA ORDINAL INFORMATIVE CONCEPTS FOR CONTEXT FUSION IN CONCEPT DETECTION AND VIDEO SEARCH 1883
dj a feature vector Xj = (Xj1, Xj2, . . . , XjM) is extracted,
where M is the dimension of the feature space. The purpose of
learning-to-rank is to train a ranking model f (·) that accurately
predicts the relevance score of test data by leveraging the
co-occurrence patterns between X and Y. More specifically,
for the training set D we obtain a list of predicted relevance
score Z = (z1, z2, . . . , zN ) = (f (X1), f (X2), . . . , f (XN )). The
objective of learning is to minimize the total loss L(Y,Z),
where L is a loss function for ranking.
Many existing ranking algorithms take object pairs as
instance in learning. These pairwise approaches formulate
the learning task as classification of object pairs into two
categories (correctly ranked and incorrectly ranked) and train
classification models for ranking. The use of SVM, boosting,
or neural network as the classification model leads to the
methods RankSVM [11], RankBoost [29], and RankNet [30].
Though the pairwise approach offers advantages, it ignores
the fact that ranking is a prediction task on a list of objects.
In addition, the pairwise approach is time-consuming as the
operation on every possible pair is of O(N2) complexity.
The listwise approach ListNet proposed in [20] conquers
these shortcomings by using score lists directly as learning
instances and minimizing the listwise loss between the initial
list and the reranked list. In this way, the optimization is
performed directly on the list, and the computational cost
can be reduced to O(N), making online reranking applications
possible. Our experiment (described later in Section VI) shows
ListNet is surprisingly efficient and even outperforms the well
known pairwise approach RankSVM.
More specifically, to define a listwise loss function, authors
of [20] first employs top-one probability to transform a list of
ranking scores into a probability distribution. Given the scores
of all the objects, the top-one probability P(yj) of an object
dj represents the probability of dj being ranked on the top
P(yj) = (yj)∑N
n=1 (yn)
=
exp(yj)∑N
n=1 exp(yn)
(1)
where (·) is an increasing and strictly positive function such
as the exponential function [20]. Since the list of scores is
modeled as a probabilistic distribution, a metric such as the
cross entropy can be used to measure the distance (listwise
loss) between the original score list and the predicted one
L(Y,Z) = −
N∑
j=1
P(yj) log(P(zj)). (2)
In [20], a linear neural network model is employed as the
ranking model, predicting the ranking score in the form of a
linear weighted sum
zj = f (Xj) = 〈W,Xj〉 (3)
where 〈·, ·〉 denotes inner product and W = (w1, w2, . . ., wM) is
a weighting vector. To minimize (2), we can derive its gradient
with respect to W as
W =
∂L(Y,Z)
∂W
=
N∑
j=1
(
P(zj) − P(yj)
)
Xj (4)
and then use the gradient descent procedure to update W at a
learning rate η
W ← W − η × W (5)
where W is initially set to zero. The learning process ter-
minates when the change in W is less than a convergent
threshold δ. The values of η and δ are determined empirically,
and a parameter sensitivity test over them is presented in
Section VI-B.4.
B. Learning-to-Rank Versus Reranking
Reranking and learning-to-rank differ in a number of as-
pects. First, while learning-to-rank requires a great amount
of supervision, reranking takes an unsupervised fashion and
requires no ground truth from the initial concept detection or
search results. The online training set D is made up of the
objects of the initial results. The associated relevance scores
assigned by the baseline method are taken directly as the
pseudo ground truth Y. No manual labeling, extra training
data or offline learning is needed. Second, for learning-to-rank
the ranking algorithm f (·) is trained in advance (the training
data can consist of multiple queries) to predict the relevance
scores for arbitrary queries, while for reranking f (·) is trained
at runtime by cross validation (described later) specifically for
each query.
As described in Section II, existing approaches mine the
co-occurrence patterns via statistical [5], [6] or classification
[7], [19], [28] methods. Despite that both learning-to-rank and
reranking explore ordinal information, little effort has been
made to incorporate ranking algorithms into the online rerank-
ing framework. To our best knowledge, this paper represents
one of such attempts.
IV. Ordinal Reranking
The input to ordinal reranking is a list of objects D and
the corresponding relevance scores Y assigned by a baseline
model (for either search or concept detection). We assume
that feature extractions (e.g., concept detections) for each
visual object are computed in advance. For these N objects
in D, the corresponding M-dimensional features can form an
N × M feature matrix X. A concept score is a real value in
[0, 1] that indicates the confidence of existence of the specific
concept. The major steps of ordinal reranking are as follows
(also illustrated in Fig. 1).
1) Concept Selection: Select informative concepts via a
concept selection method (cf. Section V) to reduce the
feature dimension to M ′.
2) Randomly partition the dataset into F -folds D =
{D(1),D(2), . . . , D(F )}.
3) Employment of Ranking Algorithms: Hold out one fold
of data D(1) as the test set and train the ranking algorithm
f (i) using the remaining data. Predict the new relevance
scores Z(i) of the test set D(i). Repeat until each fold
is held out for testing once. The predicted scores of
different folds of objects are then combined to form the
new list of scores Z = {Z(1), Z(2)), . . . , Z(F )}.
Authorized licensed use limited to: National Taiwan University. Downloaded on December 9, 2009 at 19:40 from IEEE Xplore.  Restrictions apply. 
YANG et al.: ONLINE RERANKING VIA ORDINAL INFORMATIVE CONCEPTS FOR CONTEXT FUSION IN CONCEPT DETECTION AND VIDEO SEARCH 1885
TABLE II
Performance Comparison of Various Reranking Methods on the TRECVID 2005 Search Task Using ‘‘Text-Okapi’’ as the Baseline
# Reranking Algorithm Feature Set Feature Selection MAP Improvement (%) Time/Query
1 Baseline Text-only – 0.087 – –
2 IB [6] Low-level – 0.105 20.7 18 s
3 SVM [7] cp374 Mutual information 0.112 28.7 17 s
4 RankSVM cp39 – 0.103 18.4 1 h
5 ListNet cp39 – 0.113 30.0 0.2 s
6 ListNet low-level – 0.105 20.7 0.9 s
7 ListNet cp374 – 0.116 33.3 1.4 s
8 ListNet cp374 c-tf-idf 0.118 35.6 0.4 s
9 ListNet cp374 wc-tf-idf 0.121 40.0 0.4 s
consists of 277 international broadcast news video programs
and accumulates 170 h of videos from six channels in three
languages (Arabic, English, and Chinese). The time span is
from October 30 to December 1, 2004. The automatic speech
recognition and machine translation transcripts are provided by
the National Institute of Standards and Technology. The video
data is segmented into shots and each shot is represented by
a few keyframes (subshots). In the following experiments, we
evaluate the performance at shot level in terms of average
precision (AP), which approximates the area under a non-
interpolated recall/precision curve. Since AP only shows the
performance of a query, we use MAP, which is simply the
mean of APs for multiple queries, to measure average perfor-
mance over sets of different queries in the test data. See more
explanations in [1].
We first apply the reranking approach to the search task,
where 24 query topics are provided with ground truth anno-
tations. Since TRECVID evaluates the search results over the
top 1000 shots, we use the top 1300 subshots (which typically
encompass the top 1000 shots) returned by the text-based
search method “text-okapi” [6] for reranking.
We also apply ordinal reranking for the concept detection
of the 39 LSCOM-Lite concepts [4] over a set sampled from
the TV05 development data.5 Since TRECVID evaluates the
high-level concept detection results over the top 2000 shots, we
use the top 2600 subshots returned by the baseline detection
method from [17] for reranking. In a supervised fashion, the
concept detection accuracy is generally much higher than that
from the search baseline.
For feature representation, we adopt the detection scores
of pre-trained concept detectors [4] for the LSCOM (cp374)
and LSCOM-Lite (cp39) lexicons [17] to provide high-level
semantics. The LSCOM concept lexicon is a set of 374
visual concepts which were annotated over an 80-h subset
of the TRECVID data. The LSCOM-Lite lexicon is with
39 concepts and is an early version of LSCOM. Low-level
visual features (low-level) including 5 × 5 grid color mo-
ments and 4 × 6 Gabor textures [6] are also included to
compare against the high-level concept scores. Though they
are primitive feature representations, prior work such as [6],
[17] has shown their excellence in image retrieval and concept
detection.
5Dataset available online: http://mpac.ee.ntu.edu.tw/˜yihsuan/reranking/.
The implementation of RankSVM is based on the software
SVMlight [33] with default parameters. ListNet is implemented
in MATLAB. The programs are executed on a regular Intel
Pentium server.
B. Reranking for Video Search
1) Comparison of Variant Reranking Methods and Feature
Sets: We first conduct experiments on TV05 video search
task with variant reranking methods and feature sets without
feature selection. Empirically, we set the fusion weight α to
0.5 for simplicity and use fivefold cross validation to conduct
reranking. The learning rate η and convergent threshold δ for
ListNet are empirically set to 0.005 and 1e−4, respectively
(parameter sensitivity tests are presented later). While the
MAP of the text-based search baseline “text-okapi” is 0.087,
existing methods [6], [7] improve the MAP to 0.105 and 0.112
respectively, as shown in the second and third rows of Table II.
Note that [6] uses low-level visual features without feature
selection, whereas [7] utilizes mutual information to select the
75 most informative concepts among cp374 for reranking.
We first compare the performance of ordinal reranking
with different ranking algorithms, namely, RankSVM and
ListNet. As shown in the fourth row of Table II, RankSVM
is extremely time-consuming and thus get abandoned in the
following experiments. On the contrary, thanks to the linear
kernel, ListNet is surprisingly efficient and takes less than one
second to rerank a single query. The efficiency of ListNet
makes it superior to the method described in [6], which needs
a clustering process, and to the method described in [7],
which uses non-linear optimization and ad hoc thresholding.
Moreover, the fact that ListNet improves the MAP to 0.113
with a small concept lexicon cp39 further demonstrates its
effectiveness in reranking.
We then evaluate the performance of ListNet with variant
feature sets (rows 5–7 of Table II). Despite the less salient
performance gains that can be provided, low-level visual
features still offer contextual cues that augment the baseline
text-based methods and improve the MAP to 0.105. It is not
surprising that reranking based on low-level visual features
does not perform as well as that based on high-level concepts
since concepts tend to capture both the visual similarities and
the semantic correlations. In addition, the visual patterns of
target semantics may not be consistent or evident enough,
bringing noises to the reranking procedure. For example, a
Authorized licensed use limited to: National Taiwan University. Downloaded on December 9, 2009 at 19:40 from IEEE Xplore.  Restrictions apply. 
YANG et al.: ONLINE RERANKING VIA ORDINAL INFORMATIVE CONCEPTS FOR CONTEXT FUSION IN CONCEPT DETECTION AND VIDEO SEARCH 1887
Fig. 3. Average precisions of baseline and reranked search results for each
query in TRECVID 2005. The reranking algorithm is ListNet, with 150
most informative concepts selected from LSCOM 374 concepts by wc-tf-idf.
Ordinal reranking improves the result of almost every query and improves the
MAP from 0.087 to 0.122 (40% relative improvement).
Fig. 4. MAPs for applying ListNet to rerank search baseline “text-okapi”
with two feature sets as we change the fusion weight α from 0 to 1 at an
increasing step of 0.05. Optimal performance is achieved by setting α at 0.6.
set by the baseline model. From Table III it can also be
observed that, for the query “hu jintao (152),” despite the top
selected concepts are mostly correct, the AP is not improved
after feature selection. The reason is we have fixed the number
of selected concepts to be 150 for all the queries. Actually, we
found the overall performance can be further improved if the
number of selected concepts is query-dependent. For example,
if only the top three ranked concepts are used for reranking
“hu jintao (152),” the AP can reach 0.192. Yet we leave it as
part of the future works.
3) Discussion of the Reranking Performance for Video
Search: Fig. 3 depicts the APs achieved by the baseline
method, reranking using ListNet with cp374 as the feature
set, and reranking using ListNet with the 150 most infor-
mative concepts selected by wc-tf-idf. Remarkably, the per-
formance improvements of ordinal reranking over the base-
line are consistent—almost all queries are improved. Among
them, salient improvements are observed for queries with
higher initial AP, such as “omar karami (151),” “tony blair
(153),” and “soccer goal (171).” Concept selection by wc-
tf-idf further enhances the result, especially for “soccer goal
(171),”which has strong contextual links with many concepts.
Fig. 5. MAPs for applying ListNet with different numbers N of subshots to
be reranked. This result shows the MAP, which is evaluated using the top
1000 subshots, can be further improved by using a larger N, which provides
more training data and increase the possibility to improve the recall rate by
retrieving subshots which are ranked lower in the initial list.
However, as existing reranking method [7], for queries which
lack viable methods for getting a good initial search result,
such as “map iraq (155)” and “tall building (170),” ordinal
reranking does not offer improvements since the contextual
patterns are difficult to discover. This may be an inherent
limitation of the reranking methodology, which heavily relies
on the quality of the initial model. We do not discuss this issue
further, leaving it as part of the future research.
4) Parameter Sensibility: We are also interested in the
performance with different values of weight α for fusing text-
based relevance scores Y and context-based reranked scores Z
in (6). To analyze the impact of fusion weights, we compare
the performance with different fusion weights ranging from 0.0
(totally text-based) to 1.0 (totally reranked) with an increasing
step of 0.05 and plot the results in Fig. 4. Interestingly, we
discover the influence of α similar to what has been reported
in [5]. When α is close to 1, the reranking process relies
almost entirely on the contextual similarities and ignores the
text search prior; hence, the performance degrades sharply.
When α = 1 (totally reranked), the performance of the process
is similar to that of the purely text-based method. In addition,
the text modality and context modality carry important infor-
mation, and the fusion of both gives rise to optimal reranking,
showing the two modalities are quite complementary. The
same phenomenon is observed when either cp39 or cp374 is
used as the feature set.
We then conduct parameter sensibility test on the values of
the learning rate η and the convergent threshold δ for ListNet.
Being used in (5), η controls the degree of updating of the
weights W and influences the convergence time. As shown in
Table IV, the performance of ListNet is rather invariant to the
changes of η as long as it is set to a moderately small value.
A large η may result in an oscillation of W and degrade the
performance. On the other hand, δ controls the degree of the
ranking algorithm fitting the training data and also influences
the convergence time. It can be observed in Table IV that
setting δ too small overfits the data and has negative effect on
the reranking performance. To balance the convergence time
and the reranking performance, we have set η and δ to 0.005
and 1e−4, respectively (the cell marked with * in Table IV).
Finally, as N determines the number of subshots to be
reranked and the size of training data, it is also interesting
to know whether we can improve the MAP by using a large
Authorized licensed use limited to: National Taiwan University. Downloaded on December 9, 2009 at 19:40 from IEEE Xplore.  Restrictions apply. 
YANG et al.: ONLINE RERANKING VIA ORDINAL INFORMATIVE CONCEPTS FOR CONTEXT FUSION IN CONCEPT DETECTION AND VIDEO SEARCH 1889
Fig. 6. Average precision of baseline and reranked results for each target concept in the TRECVID 2005 benchmark. Here ListNet is adopted as the reranking
algorithm, with 25 most informative concepts selected from LSCOM 374 concepts by wc-tf-idf. Ordinal reranking improves the result of almost every concept
detector and improves the MAP from 0.369 to 0.413 (12% relative improvement).
for visual search since the initial concept detection accuracy
has been relatively high (e.g., MAP 0.369 in Table V).
Results shown in Table V indicate that the reranking
approach is also effective for refining the baseline concept
detectors: using cp374 as the feature set, ListNet improves the
MAP from 0.369 to 0.410 (11.1%) over the detection baseline.
If the 25 most informative concepts selected by wc-tf-idf are
used, the MAP is further improved to 0.413 (11.9%), which
significantly outperforms existing context fusion methods. In
addition, ordinal reranking is still remarkable efficient for
concept fusion, taking less than one second to refine the result
of a concept. Moreover, as shown in Fig. 6, the performance
improvements of ordinal reranking over the baseline are also
consistent for concept fusion.
From Table V it can also be observed that context fusion
based on cp39 only slightly improves the result. This is not
surprising since cp39 is actually an essential subset of cp374
and thus the contextual links between the cp39 concepts are
much limited. The other interesting observation is related to
the optimal number of concepts for use in ordinal reranking.
While the optimal number of concepts is around 150 for
video search, the optimal number of concepts is merely 25,
which implies that most informative concepts are successfully
selected by wc-tf-idf. The cause of this notable effectiveness is
twofold. First, the contextual patterns among cp374 (LSCOM
concepts) and the target semantics (39 LSCOM-Lite concepts)
are in nature strong. Second, the initial accuracies of the
concept detectors are already high, making the contextual
patterns easy to be discovered.
We also tabulate the top ranked concepts among cp374 in
Table VI, without excluding the identical concept as the target
concept in cp374. As Table VI shows, most target concepts
rank the identical one in top three, and the contextual rela-
tionship among top selected concepts are intuitively correct.
For example, people marching, funeral, and parade are ranked
high for “Crowd,” and ground vehicle, road, streets are ranked
high for “Car.” In addition, we observe that wc-tf-idf is also
adept at discovering contextual links which query expansion
or keyword matching easily fail to. For example, street battle
and weapons are ranked high for both “Desert” and “Explo-
sion fire,” while map, charts, and studio are ranked high for
“Weather.” These relationships may be less salient literally, yet
are essentially correct since the makeup of TRECVID videos
are mostly news videos [7].
VII. Conclusion
In this paper, we have exploited the contextual information
for visual search and concept detection and proposed a novel
reranking algorithm called ordinal reranking for mining the
co-occurrence patterns between the target semantics and the
extracted features. This ranking-based reranking algorithm is
more effective and efficient than existing reranking methods.
Moreover, because ordinal reranking directly optimizes the
ordering of an initial list obtained by a baseline system, it is
free of ad hoc thresholding for noisy binary labels and requires
no extra offline learning processes or training data. Besides,
as there has been rare feature selection measure specifically
designed for reranking, we also propose a novel measurement,
wc-tf-idf, to select informative concepts and further improve
the performance of reranking.
Because ordinal reranking is largely unsupervised, it can
be applied equally well to context fusion in both concept
detection and video search tasks. An extensive performance
study is conducted on the TRECVID 2005 benchmark to
evaluate the performance of ordinal reranking and concept
selection for the two tasks. Results show that ordinal reranking
is much more efficient and effective than existing reranking
methods and improves the MAP up to 40% over the text-based
search results and 12% over the concept detection baselines.
The proposed ordinal reranking approach is general enough
to be applied to problems in other multimedia domains such
as media-rich social networks and blogs that have strong
contextual links between pieces of information that come from
multiple sources.
References
[1] National Institute of Standards and Technology. Text Retrieval
Conference Video Retrieval Evaluation [Online]. Available:
http://www-nlpir.nist.gov/projects/trecvid
Authorized licensed use limited to: National Taiwan University. Downloaded on December 9, 2009 at 19:40 from IEEE Xplore.  Restrictions apply. 
Knowledge
Discovery from
Community-
Contributed
Multimedia
Tao Mei
Microsoft Research Asia
Winston H. Hsu
National Taiwan University
Jiebo Luo
Kodak Research Laboratories
T
he prevalence of image- and video-
capturing devices and the advent of
media-sharing services such as Flickr
and YouTube have drastically
increased the volume of community-contributed
multimedia. For example, there are reportedly
more than four billion images in Flickr and
24 hours of new videos are uploaded to You-
Tube every minute. Such a vast amount of
photos, videos, and music shared via websites
is bound to exert a profound social impact
on human society and poses a new challenge
for developing efficient indexing, search, min-
ing, and visualization approaches for manag-
ing such large-scale media data.
Social media is augmented with rich con-
text, such as user-provided tags, comments,
geolocations, time, device metadata, and so
on. These bits of context data are promising
resources to exploit for benefiting a wide
variety of applications, such as annotation, rec-
ommendation, questioning and answering, ad-
vertising, and (cultural) activity discovery.
The goal of this special issue is to present a
concise reference of state-of-the-art efforts in
such attempts for knowledge discovery over
large-scale, community-contributed multime-
dia, and in particular the opportunities and
challenges in this nascent arena. We have
selected five articles that represent ways to ex-
ploit user-contributed photos and videos for
several applications and that identify the theo-
retical challenges associated with managing
such multimedia data.
The articles
Among the many forms of rich context asso-
ciated with media, tags are used most in a wide
variety of applications (for example, search, or-
ganization, recommendation, and visualiza-
tion). However, large-scale, weakly tagged
images might suffer seriously from the problem
of tag uncertainty, which in turn can prevent
users from being able to use the application ef-
fectively. To begin with, Fan et al. briefly survey
the current attempts in improving tagging
quality associated with images and videos.
They propose a cross-modal, tag-cleansing algo-
rithm that integrates the visual similarity con-
texts of the weakly tagged images with the
semantic similarity contexts of their tags.
Rather than watching the broadcasts alone,
sports fans are keen to share their passions
through social media (for example, Facebook,
YouTube, blogs, and Twitter). Traditionally in
the multimedia research community, sports
event detection (or summarization) is performed
through supervised or unsupervised content
analysis. The next article, by Smits and Hanjalic,
augments common content-analysis approaches
with collaborative (community) tagging. The
two aspects compensate for the shortcomings
of each other: automatic content analysis helps
guide users toward potentially interesting high-
light candidates, while tagging itself validates
and enriches the detected results through a col-
laborative effort. This naturally brings the moti-
vated fans into the loop.
The rich context of social media, such as
user-provided tags and geolocations, creates
rich sets of georeferenced photos. Newsam
presents a research survey in the third article
that focuses on the knowledge discovery in ef-
fective crowdsourcing what-is-where on the
surface of the earth. In particular, the author
illustrates recent results of leveraging large col-
lections of georeferenced photos to solve three
[3B2-14] mmu2010040016.3d 21/10/010 12:26 Page 16
Guest Editors’ Introduction
1070-986X/10/$26.00 c 2010 IEEE Published by the IEEE Computer Society16
Adaptive Learning for Multimodal Fusion in
Video Search
Wen-Yu Lee, Po-Tun Wu, and Winston Hsu
National Taiwan University, Taiwan
{majorrei,frankwbd}@cmlab.csie.ntu.edu.tw
winston@csie.ntu.edu.tw
Abstract. Multimodal fusion had been shown prominent in video search
for the sheer volume of video data. The state-of-the-art methods ad-
dress the problem by query-dependent fusion, where modality weights
vary across query classes (e.g., object, sports, scenes, people, etc.). How-
ever, provided the training queries, most of the prior methods rely on
manually pre-defined query classes, ad-hoc query class classification, and
heuristically determined fusion weights, which suffer from accuracy is-
sues and are not scalable to large-scale data. Unlike prior methods, we
propose an adaptive query learning framework for multimodal fusion.
For each new query, we adopt ListNet to adaptively learn the fusion
weights from its semantically-related training queries dynamically se-
lected by K-nearest neighbor method. ListNet is efficient for optimizing
the performance in search ranking rather than classification. In general,
the proposed method has the following advantages: 1) No pre-defined
query classes are needed. 2) The multimodal query weights are automat-
ically and adaptively learned without ad-hoc hand-tuning. 3) The query
training examples are selected according to the query semantics and re-
quire no noisy query classification. Experimenting in large-scale video
benchmarks (i.e., TRECVID), we will show that the proposed method
is scalable and competitive with prior query-dependent methods.
Key words: Multimodal Fusion, Video Search, TRECVID, ListNet,
Query-Dependent
1 Introduction
One of the most critical challenges for video search is how to automatically
leverage the strengths from different search modalities or methods (e.g., text,
content-based image retrieval, concept search, people search, etc.) to improve
the search quality. It had been shown that promising results can be achieved
by query-dependent fusion, where modality weights vary according to different
query classes (e.g., object, scenes, sports, people, etc.) [1–5]. It is natural since
there might be different significances for different query types. For examples,
it might weight more on textual information for searching certain celebrities in
news videos (if transcripts are available), or more on colors as search for outdoor
scenes in home videos.
Adaptive Learning for Multimodal Fusion in Video Search 3
<X , W>
X
Query Feature 
Extraction
“a Ship or a Boat”
Query X1
X2
Xm
KNN
= <w1, w2, …, wm>
Modality Weights 
q
Q=(q(1), q(2),…, q(k))
q
X1(i), X2(i),…, Xm(i) Label(Y(i))
.
.
.
Multimodal 
Query
Learning Method
(ListNet) 
Multimodal 
Fusion
fq
q
=
D =
i=1
k
W
Fig. 1. The flowchart of our video search system, detailed in Section 3. This figure illus-
trates how we handle an incoming query – comprising text descriptions (or keywords)
and image (video) examples. The system contains four components: multimodal query,
query feature extraction, multimodal fusion, and learning method. For the new query
q and by its query feature, we use KNN method to find semantically related training
queries Q with associated multimodal search scores and labels in D. Then ListNet is
applied to learn (optimized) query modality weights w’s over D. For the new query, we
derive series of search scores X by a certain number (i.e., m) of search modalities or
methods, which are later fused by the weights W learned in ListNet. The final results
are then ranked by the fused search scores.
2 Related Works
In the past years, there are many works focusing on multimodal fusion for
video search. Recently, great improvements in automatic video search have been
achieved based on query-dependent fusion models, which aim to associate query-
specific retrieval strategies with a few pre-defined query classes. Table 1 sum-
maries the multimodal fusion in the related works.
In [1][2], the multimodal fusion weights are learned in pre-defined query
classes. Then, [3][4] used both query classes and other auxiliary relevance infor-
mation for training the modality weights. In [3], Kennedy et al. adopted semantic
space and query performance data to learn query classes by clustering. When a
new query arrives, they only use the semantic space information for query clas-
sification. In [4], Yan et al. presented a probabilistic latent query analysis model
(pLQA) to deal with all queries. They first find the meaningful latent query
classes using the pLQA model, and then fuse them to obtain a set of modality
weights for each query class.
In [5], Xie et al. provided a dynamically-created query class system. This
system can dynamically create query classes once an incoming query is received.
For each training query, the system measures a semantic distances between this
Adaptive Learning for Multimodal Fusion in Video Search 5
weights learned specifically for the new query. The multiple search modalities
are flexible for the proposed framework and those used in this experiment will
be described in Section 4.1.
3.1 Query Feature for Selecting Relevant Training Queries
In the proposed framework, for a new query, we select a few semantically relevant
queries Q from the training queries for learning adaptive query weights. For this
purpose, each query needs to be represented by its query features, which describe
its semantic meanings for matching. We then adopt KNN for identifying those
relevant training queries in the query feature space, as illustrated in Figure 1.
We will further investigate the impact for K in Section 4.2.
For query feature fq, we will experiment two types of query features: au-
tomatic and manual. For automatic query features, as suggested in [5], we use
PIQUANT engine to tag the query text with more than one hundred semantic
tags in an extensive ontology [11]. The tags include person, geographic, entities,
objects, actions, and etc., which will be mapped to nine binary feature dimen-
sions such as “Sport,” “Named-Person,” “Unnamed-Person,” “Named Entity,”
“Event,” “Object,” “Scene,” “Vehicle,” and “Violence.”
Avoiding being biased by the stability issue for the automatic query feature
extraction tools, we also investigate another query feature set by manually tag-
ging the queries following the rules in [5]. We manually map the semantic tags to
the nine binary feature dimensions as above. We will investigate the automatic
vs. manual query features for query-dependent fusion in Section 4.2.
3.2 Learning to Rank: ListNet
In this section, we brief learning to rank methods (e.g., [14, 17, 7]), which have
shown effective for ranking documents for text search. Provided the training
queries and related documents and their relevance scores by different retrieval
methods, Learning to rank algorithms will learn the fusing weights maximizing
ranking performance over the training queries.
Among them, the most promising is ListNet [7]. The main difference be-
tween ListNet and other ranking methods is that ListNet uses listwise approach
for training. Most of other learning methods are using pairwise approaches,
which formulate the learning task as classification of object pairs into two cate-
gories (correctly ranked and incorrectly ranked) and train classification models
for ranking. The use of SVM, boosting, or neural network as the classification
model leads to the methods RankSVM [14], AdaRank [16], and RankNet [17],
respectively. These methods are extremely time-consuming comparing with List-
Net [19].
For learning-to-rank, following we define the ranking framework, for List-
Net [7]. Given a set of (prior or training) queries Q = (q(1), q(2), ..., q(k)), each
query q(i) is associated with a list of objects D(i) = (d
(i)
1 , d
(i)
2 , ..., d
(i)
N(i)
), where
Adaptive Learning for Multimodal Fusion in Video Search 7
4 Experiments
4.1 Experiment Setup
TREC Video Retrieval Evaluation (TRECVID2) hosted by NIST annually, pro-
vides large-scale news video data and set of queries to evaluate video retrieval
performance. We evaluate our proposed approach on TRECVID 2005 (TV05)
and TRECVID 2006 (TV06). The collection of the former dataset contains about
170 hours of broadcast news video from 6 channels in 3 languages (Arabic, En-
glish, and Chinese), while the latter includes about 160-hour videos from the
same source. For each dataset, it comprises 24 query topics. The query topic
consists of a short text description and some query examples to illustrate user’s
information need. The query performance is evaluated in terms of average preci-
sion (AP). Since AP only shows the performances of a single query, we use mean
average precision (MAP) on multiple queries for the overall system performance.
Multiple Search Modalities
We take four search results of different modalities for our experiment. They are
concept, content based image retrieval, single-text and multi-text, respectively.
In this part, we will introduce how to generate our multimodal query modalities
in our work.
Concept: The concept search is to map textual queries to those pre-defined
concept lexicons. In this work, we adopt the 374 pre-trained LSCOM detectors
by Columbia University (Columbia374) [9]. Thereby each photo is associated
with a 374-dimensional (real-valued) concept vector. The query is then mapped
into 374-dimensional concept vector by measuring query and concept similarities
by Google snippets. See more explanations in [12]. The retrieval results are then
scored by the fusion over 374 concept detection scores respectively weighted by
their query-to-concept mapping weights.
Content based image retrieval: Here are two components for generating
example modality; one is multiple example content-based retrieval (MECBR),
and the other is Multi-Bag SVM (MBSVM) method [10]. Then we fused these
two components to get its average. MECBR is adopted to generalize the conven-
tional query-by-example (single) paradigm. For a single query image, each image
is represented by certain feature vectors and the similarity score is calculated by
averaging Euclidean distances of edge direction histogram and Gabor texture.
MAX is conducted for fusing scores of all query examples as final ranking list.
In MBSVM, we formulate the retrieval as a classification problem. The pos-
itive examples are from the user query; the pseudo-negative examples are ran-
domly sampled. We set the ratio of positive example to pseudo-negative example
is 1 to 5. We train multiple SVM models with the same positive examples and
newly randomized negative example. The final SVM model corresponds to the
2 http://www-nlpir.nist.gov/projects/trecvid/
Adaptive Learning for Multimodal Fusion in Video Search 9
0.07
0.075
0.08
0.085
k=1 k=3 k=5 k=7 k=9
MAP manual automatic
Fig. 2. For adaptive multimodal fusion by KNN+ListNet, we first compare query fea-
tures: automatic vs. manual. As k=7, the manual annotations help archive the best
performance; k=3 for automatic annotations.
categories, “People,” “Sport,” “Scene,” “Object,” and “Others” following the
rules in [8]. We evaluate variant configurations based on these pre-defined query
classes along with our proposed method. The configurations include:
Average: Query independent fusion – fusing search modalities by averaging.
Note that the search scores have been normalized between [0, 1] by sigmoid
function.
Leave-one-out (per class): Using leave-one-out method for each query class.
We alternately leave one query out, and then learn its appropriate modality
weights by ListNet. Now the training query subset are those queries in the pre-
defined classes rather than those selected by KNN.
Leave-one-out (all queries): Using leave-one-out method for all queries in
TV05 and TV06. We learn the weights by ListNet over the remaining queries.
KNN+ListNet (K=7): The proposed method over manual query features and
K = 7 for KNN.
The comparison results are shown in Table 2. We found that KNN+ListNet
outperforms the baseline method – averaging. Meanwhile, it is also competitive
with ListNet over queries in manually defined query classes. Based on these
experiments, we hypothesize that the proposed method might relieve the need
for pre-defined query classes and ad-hoc query classification methods. It also
confirms again that, as shown in Figure 2, using all other queries (i.e., K =∞)
does not guarantee better performance.
Interestingly, in “Other” class for Table 2, even it is difficult to locate appro-
priate set of query classes for these diverse queries, ListNet+KNN still achieves
the best against other configurations.
4.4 Meta Fusion on Past TRECVID Search Submissions
In this experiment, we would like to verify if our query modality learning method
could be applied on ranking lists for meta fusion and if the proposed framework
still woks for dozens of search modalities. To verify our hypothesis, we use the
past submission data for automatic search in TV05 and TV06. We selected
21 automatic submissions out of TV05 and 76 automatic submissions out of
Adaptive Learning for Multimodal Fusion in Video Search 11
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-1 -0.5 0 0.5 1
M
A
P
Weight
   Weight vs MAP
Fig. 3. In the meta fusion experiment, taking dozens of past TRECVID submissions
as search modalities, we can see that there exists a positive correlation between the
search performance (in MAP) and the learned weights. It shows that these (high-
dimensional) modality weights can be systematically learned by the proposed method.
Note that these high-dimensional weights cannot be manually defined. The slopes for
the TV05 and TV06 are 0.3877 and 0.3457, respectively.
Table 3. The MAP comparison in meta fusion experiment. Among them,
KNN+ListNet (k=7) outperforms the other methods and even the best performance
in the submissions.
TV05 0.069 0.1241 0.1196 0.119
TV06 0.029 0.0917 0.0887 0.0867
Average
KNN+ListNet
(K=7)
Leave one out
(all queries)
Best Map
(past submission)
References
1. R. Yan, et al.: Learning query-class dependent weights in automatic video retrieval.
In: ACM Multimedia, (2004)
2. T.-S. Chua, et al.: TRECVID 2004 search and feature extraction task by NUS PRIS.
In: NIST TRECVID Workshop, (2004)
3. L. S. Kennedy, et al.: Automatic discovery of query-class-dependent models for
multimodal search. In: ACM Multimedia, (2005)
4. R. Yan and A. G. Hauptmann.: Probabilistic latent query analysis for combining
multiple retrieval sources. In: ACM SIGIR, (2006)
5. L. Xie, et al.: Dynamic multimodal fusion in video search. In: IEEE ICME, (2007)
6. L. Kennedy, et al.: Query-adaptive fusion for multimodal search. In: IEEE, Vol. 96,
No. 4, (2008)
7. Z. Cao, et al.: Learning to rank: from pairwise approach to listwise approach. In:
ACM ICML, pp. 129–136, (2007)
8. G. Hauptmann, et al.: Video Retrieval Based on Semantic Concepts. In: IEEE, Vol.
94, No. 4, (2008)
 1 
出席國際學術會議心得報告  
                                                             
計畫編號 NSC 98-2221-E-002 -149 - 
計畫名稱 大規模社群媒體中的影像搜尋與推薦	 
出國人員姓名	 
服務機關及職稱	 
徐宏民	 
國立臺灣大學	 資訊網路與多媒體研究所	 
會議時間地點 June 12 ~ June 19, 2010 
會議名稱 IEEE Conference on Computer Vision and Pattern Recognition  (CVPR) 2010 
發表論文題目 
We thank the travel support for this project, which has helped the PI attend the 
premier conference in computer vision, IEEE Conference on Computer Vision 
and Pattern Recognition (CVPR) 2010. The goals for the visit include: 
• We are hoping to have further publications in the international 
conference and need to first sense the publication culture and technical 
interests for the community.  
• We attend the technical program committee (TPC) meeting for ACM 
Multimedia 2010. 
 
一、參加會議經過 
 
 
Please see the report in the following. 
 
 
 
二、與會心得 
 
 
Please see the report in the following.  
 3 
has long been the Singular Value Decomposition. However, in the presence of missing 
data and outliers this method is not applicable, and unfortunately, this is often the case in 
practice. In this paper the authors present a method for calculating the low-rank 
factorization of a matrix which minimizes the L1 norm in the presence of missing data. 
Their approach represents a generalization the Wiberg algorithm of one of the more 
convincing methods for factorization under the L2 norm. By utilizing the differentiability 
of linear programs, we can extend the underlying ideas behind this approach to include 
this class of L1 problems as well. We show that the proposed algorithm can be efficiently 
implemented using existing optimization software. We also provide preliminary 
experiments on synthetic as well as real world data with very convincing results. 
 
 
Best Paper Honorable Mention: "Modeling Mutual Context of Object and Human Pose 
in Human-Object Interaction Activities," Bangpeng Yao and Li Fei-Fei. Detecting 
objects in cluttered scenes and estimating articulated human body parts are two 
challenging problems in computer vision. The difficulty is particularly pronounced in 
activities involving human-object interactions (e.g. playing tennis), where the relevant 
object tends to be small or only partially visible, and the human body parts are often self-
occluded. The authors observe, however, that objects and human poses can serve as 
mutual context to each other – recognizing one facilitates the recognition of the other. In 
this paper they propose a new random field model to encode the mutual context of 
objects and human poses in human-object interaction activities. They then cast the model 
learning task as a structure learning problem, of which the structural connectivity 
between the object, the overall human pose, and different body parts are estimated 
through a structure search approach, and the parameters of the model are estimated by a 
new max-margin algorithm. On a sports data set of six classes of human-object 
interactions [12], the authors show that the mutual context model significantly 
outperforms state-of-theart in detecting very difficult objects and human poses. 
 
 
Best Student Paper: "Visual Event Recognition in Videos by Learning from Web Data,"  
Lixin Duan, Dong Xu, Wai-Hung Tsang, and Jiebo Luo. The authors propose a visual 
event recognition framework for consumer domain videos by leveraging a large amount 
of loosely labeled web videos (e.g., from YouTube). First, they propose a new aligned 
space-time pyramid matching method to measure the distances between two video clips, 
where each video clip is divided into space-time volumes over multiple levels. They 
calculate the pair-wise distances between any two volumes and further integrate the 
information from different volumes with Integer-flow Earth Mover’s Distance (EMD) to 
explicitly align the volumes. Second, they propose a new cross-domain learning method 
in order to 1) fuse the information from multiple pyramid levels and features (i.e., space-
time feature and static SIFT feature) and 2) cope with the considerable variation in 
feature distributions between videos from two domains (i.e., web domain and consumer 
domain). For each pyramid level and each type of local features, they train a set of SVM 
classifiers based on the combined training set from two domains using multiple base 
kernels of different kernel types and parameters, which are fused with equal weights to 
obtain an average classifier. Finally, they propose a cross-domain learning method, 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/12/06
國科會補助計畫
計畫名稱: 大規模社群媒體中的影像搜尋與推薦
計畫主持人: 徐宏民
計畫編號: 98-2221-E-002-149- 學門領域: 影像處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
Through rigorously conducting the research for the project, we devised 
fruitful and significant results in terms of publications [1][2][3], 
research community services [4][5][6], and joining prestigious 
international technical committee [7][8][9][10][11]. We append the 
accepted (also submitted) papers in this report. We also pointed some 
important directions in this research paradigm for the research 
community [2][4][5][6].  
 
[1] ＇＇＇＇Online Reranking via Ordinal Informative Concepts for 
Context Fusion in Concept Detection and Video Search,＇＇＇＇ Yi-Hsuan 
Yang, Winston H. Hsu, and Homer H. Chen, IEEE Transactions on Circuits 
and Systems for Video Technology (TCSVT), vol. 19, no. 12, pp. 
1880–1890, Dec. 2009.  
[2] ＇＇＇＇Knowledge Discovery from Community-Contributed 
Multimedia,＇＇＇＇ Tao Mei, Winston H. Hsu, Jiebo Lui, IEEE Multimedia 
Magazine, October-December, 2010. 
[3] ＇＇＇＇Adaptive Learning for Multimodal Fusion in Video 
Search,＇＇＇＇ Wen-yu Lee, Po-Tun Wu, Winston H. Hsu, IEEE Pacific-Rim 
Conference on Multimedia (PCM) 2009.  
[4] Guest Editor for IEEE Multimedia Magazine Special 
Issue, ＇＇＇＇Knowledge Discovery over Community-Contributed 
Multimedia Data: Opportunities and Challenges.＇＇＇＇ 
[5] Organizer for The 17th International Conference on Multimedia 
Modeling (MMM) 2011 Special Session, ＇＇＇＇Large Scale Rich Media Data 
Management,＇＇＇＇ with Prof. Jialie Shen and Prof. Shuicheng Yan. 
[6] Keynote speech for Global Software Service (GSS) TechEd, Taipei, 
August 2010 
[7] Organizing Committee (Publicity Co-Chiar) for ACM Multimedia 2010, 
Florence, Italy. 
[8] Organizing Committee (Technical Program Co-Chair) for ACM Multimedia 
2012, Nara, Japan. 
[9] Technical program committee for ACM Multimedia 2010 (content track).
[10] Technical program committee for WWW 2010 (rich media track).  
[11] Technical program committee for ACM SIGIR 2010. 
 
  
 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
科 
教 
處 
計 
畫 
加 
填 
項 電子報、網站 0  
 
