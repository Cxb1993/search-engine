省電與性能最佳化技術:從應用面至系統面之探討 -  
子計畫二：多處理器系統晶片最佳化方法與工具設計 
執行期間：95年 8月 1日至 98年 10月 31日 
計畫編號：NSC 95-2221-E-002-097-MY3 
計畫主持人：洪士灝 
一、中文摘要 
隨著多媒體技術、寬頻技術、以及半導體製造技術的進步，多媒體通訊的時代已經來臨。
舉凡影像電話、視訊會議、數位電視、網路電視、網路視訊串流、隨選多媒體等應用，如
雨後春筍般出現。在如此多樣性的服務中，整個多媒體通訊系統佈建的成功與否，關鍵仍
在於執行效能的提升（包含了服務品質的保證與系統資源的有效運用），與降低系統之耗
能，因此，具備高效能低耗能優勢的多核心處理機已經廣泛使用在今天的產品設計上。然
而，多核心程式的平行化、偵錯、最佳化，都需要花費許多時間，缺乏效能最佳化方法與
工具，使許多軟體開發者卻步不前，這是我們在這個計畫中所研究解決的問題。 
 
在這三年中，本研究計劃在第一年發展出解析多處理器系統與應用軟體所需的工具庫，包
括: 執行效能剖析工具(Performance profiling)，應用軟體追蹤工具(Application tracing)，平台
模擬工具(Platform simulation) ，以及消耗功率分析工具(Power analysis)。第二年整合系統
化使用上述工具解析多處理器系統上應用軟體之效能及耗電的法則，並於網路，多媒體，
資訊安全等方面，從事個案研究。在第三年我們研究針對上述案例中硬軟體互動行為之最
佳化的法則，提出降低共同記憶體通訊量(shared-memory communication)的方法，由軟體設
計上強化效能。在三年的研究期間中，我們成功地與其他子計畫整合，在實際平台應用我
們的開發的技術，也驗證了我們建構的工具的優點。 
 
關鍵字：效能分析 效能工具 多核心系統晶片 多處理機系統 效能最佳化 嵌入式系統 
 
二、英文摘要 
As the multimedia, communication and IC technology progress steadily, we are now in the 
multimedia and communication era. Integrated multimedia & communication applications are 
becoming popular, such as video conferencing, digital TV, video on demand etc. For this type of 
applications, how to improve system performance (including QoS guarantee and efficient 
resource utilization) and reduce power consumption in the multimedia communication system is 
critical. Thus, to pursue higher performance and lower power, multiprocessor systems-on-chips 
(MPSoC) have been widely used in Today's computer systems. However, unlike a traditional 
design, Unfortunately, it takes enormous amount of efforts to develop and port embedded 
applications onto multicore systems, and very few performance tools and optimization strategies 
We believe that the optimization problem can be better solved in future systems by using a 
variety of software and hardware approaches that are well-designed to complement one another. 
These will provide improved compiler analysis and code optimization techniques, driven by 
ample performance monitoring support in the hardware. We propose an integrated application 
development environment that systematically and dynamically coordinates the use of individual 
techniques and tools during an interactive process of application tuning and execution. By 
exploiting dynamic information gathered at run time so that the optimizations are responsive to 
actual run time behavior as data sets change and installed systems evolve, such an environment 
would be capable of achieving well-tuned codes with substantially reduced human intervention. 
 
While parallel computing offers an attractive perspective for the future, developing efficient 
parallel applications today is a labor-intensive process that requires an intimate knowledge of the 
machines, the applications, and many subtle machine-application interactions. Optimizing 
applications so that they can achieve their full potential on parallel machines is often beyond the 
programmer’s or the compiler’s ability; furthermore its complexity will not be reduced with the 
increasingly complex computer architectures of the foreseeable future. The challenges for 
building such a complicated multiprocessor system on a chip arise even more when performance 
and power efficiency are concerned: (1) The complexity is very high in designing, simulating, 
and analyzing a MPSoC, (2) Lack of experience and tools to guide application-specific 
optimization of a MPSoC [13], (3) Optimization of applications and middleware for a specific 
MPSoC represents a difficult task even for experienced programmers and system vendors. 
 
During 1992-2000, over the 8-year period, we developed various performance optimization 
techniques for parallel applications running on massively parallel systems 
[11,22,23,24,25,26,27,28]. We studied how application performance can be optimized 
systematically. We show how insights regarding machine-application pairs and the weaknesses in 
their delivered performance can be derived by characterizing the machine, the application, and 
the machine-application interactions. We describe a general performance tuning scheme that can 
be used for selecting and applying a broad range of performance tuning actions to solve major 
performance problems in a structured sequence of steps, and discuss the interrelationship among 
and between performance problems and performance tuning actions. To guide programmers in 
performance tuning, we developed a goal-directed performance tuning methodology that employs 
hierarchical performance bounds to characterize the delivered performance quantitatively and 
explain where potential performance is lost. To reduce the complexity of performance tuning, we 
developed an innovative performance modeling scheme to quickly derive machine-application 
interactions from abstract representations of the machine and application of interest. As a result, 
we were able to unify a range of research work and significantly improves the state-of-the-art in 
parallel application development environments [11]. 
 
During 2000-2005, we applied the performance tuning methodologies that we developed over the 
early years and helped improve many important database, network, and security applications 
running on large-scale scalable parallel systems. Working with top-notch researchers and 
engineers at Sun Microsystems, we helped optimize the performance of network protocols such 
 
合作 
期間 
合作 
單位 計畫名稱 
主持或
協同 產出技術與貢獻 
94~96 
Sun 
Microsystems 
以平行計算技術為基
礎的新微處理機系統
晶片 (CMT processor 
SoC)作為下一代通訊
市場使用之 ATCA平
台之效能評估技術 
主持 
產出一套能夠在缺乏原始碼的前提下，
分析ATCA平台上客戶端應用軟體的效
能的工具，使系統工程師在客戶保密原
始碼的情況下，得以獲取寶貴的效能資
訊，因而 Sun的系統硬軟體設計能夠及
早針對ATCA平台的應用做出效能的最
佳化。 
95-96 廣達研究院 
改進其發展中之商用
中階儲存伺服器
(enterprise-class 
storage server system)
之效能 
與郭大
維教授
共同主
持 
研發適用於嵌入式平台的效能分析工
具，找到效能瓶頸，創新之編譯器最佳
化(compiler optimization)技術與
caching/prefetching技術，提昇其處理速
度(I/O per second)達六倍之多，對本產
品日後的正式量產與市場價值貢獻卓著 
97-98 信億科技 
分析與改進
Linux-based 
Network-Attached 
Storage (NAS) 上的
Samba Services的效能
以及實現 Storage 
Virtualization 
主持 
改進後之 Samba server程式碼效能較原
有 Linux內建之 Samba server高出
50%，搭配硬體的 TCP/IP Offload 
Engine，整體效能可提昇至 300%。本計
畫之程式碼已移交給信億科技使用於後
續產品研發。 
97-100 
經濟部 
工研院 
晶心科技 
凌陽科技 
清大 
交大 
研發異質多核心嵌入
式平台之系統軟體與
效能分析機制 (學界
科專計畫之 B3分項) 
B3分項
主持人 
強化對本土多核心嵌入式平台的軟體支
援，實際成果包括:多核心間高效能通訊
程式庫(inter-core communication 
library)、多核心虛擬平台(virtual 
platform)、微作業系統核心
(nanokernel) 、多核心節能即時排程
(energy-efficient real-time scheduling)等
技術。其中的多核心虛擬平台，是世界
首創，唯一能夠高速模擬異質多核心嵌
入式平台的技術，我們已為此成果申請
專利以及持續發表學術成果。 
Table 1: 相關產學合作計畫與成果 
The results are categorized into three categories and discussed separately in the subsections 
below. 
 
4.1 Platform simulation environment for performance and power analysis 
 
System-level simulation aims to model the application behaviors, the interaction between 
application and system software (shared library and operating system), status of shared resources, 
parallel-programming on MPSoC systems. The library has been implemented and validated on 
the IBM Cell platform. 
 
Also, we pay attention to the optimization techniques to improve system performance and power 
consumption. In [53, 59, 63], we evaluated the performance impact of different local memory 
designs on MPSoC systems [53, 63], proposed a memory management policy to better utilize 
local memory inside the processor [59] and developed a efficient inter-processor communication 
mechanism on embedded multicore environment [65]. In [55, 74], we proposed an algorithm to 
minimize the overhead introduced by context switches on the dynamically reconfigurable 
platforms. On the other hand, we focus on the selection of compiler flags [51, 52, 66] to further 
optimize the implementation of novel algorithms. We developed an automatic compiler flag 
selection framework to find good combinations of compiling options. The tool is tested on a 
product level storage server. 
 
This work was published in the form of eight Mater Theses [47, 51, 52, 53, 59, 63, 68, 73] and 
later converted to publish in the international conferences: RTCSA 2008 [56], ICESS 2009 [66], 
EUC 2007 [55], Journal of Signal Processing [74], RTCSA 2009 [65] and ASP-DAC 2010 [75]. 
 
4.3 Performance optimization for domain-specific applications 
 
In storage applications, we worked with Quanta and ACARD on the enterprise-class and 
middle-edge storage servers, respectively. By proposing a new caching mechanism [45, 57, 46], 
we achieved up to six times performance improvement on the enterprise-class servers. Also, we 
successfully modeled and evaluated the caching algorithm [46] in the complex computer system. 
This could be used as system design stage to estimate the performance of novel caching 
algorithms before implementing them.  
 
As for the middle-edge storage servers, three milestones are achieved in this direction. First, we 
have accelerated of Samba service [58, 61] (a widely used data sharing protocol for the 
interoperability among Windows, Unix and Linux) by a factor of 1.28 with the same hardware 
budge. From our experiments, our experimental results suggested that there is potential 2.62x 
speedup can be achieved by offloading the computation into a dedicated hardware. Second, we 
successfully introduced a virtualization layer for the hybrid data storage [70]. The virtualization 
layer automatically moves the frequently used data (Hot data) into high-speed storage devices, 
such as Ramdisk, and the Cold data will be moved to the lower speed devices. Thanks to the 
virtualization layer, up to 3x performance improvement is achieved compared to data access on 
the hard disks. Third, we have established a Network-Attached Server (NAS) on the Sony 
PlayStation3 [71]. In addition to the must-have functionalities for the NAS, we also added several 
useful applications onto the device: Samba, FTP, Rsync, and p2p download service. Besides, we 
evaluated multimedia applications on PS3 and parallelized one of the interesting applications: 
surveillance application. We achieved the overall speedup factor of 9.52 with six data accelerators 
on PS3. 
 
[7]  Eric L. Boyd and Edward S. Davidson. Hierarchical Performance Modeling with MACS: A 
Case Study of the Convex C‐240. Proceedings of the 20th International Symposium on 
Computer Architecture, pp. 203‐212, May 1993.  
[8]  Karen A. Tomko and Santosh G. Abraham, Data and program restructuring of irregular 
applications for cache‐coherent multiprocessors, In 1994 Proc. International Conference on 
Supercomputing, pages 214‐255, Manchester, England, July 1994.  
[9]  Eric L. Boyd, Waqar Azeem, Hsien‐Hsin Lee, Tien‐Pao Shih, Shih‐Hao Hung, and Edward 
S. Davidson. A hierarchical approach to modeling and improving the performance of 
scientific applications on the KSR1. In Proceedings of the 1994 International Conference on 
Parallel Processing, Vol. III, pp. 188‐192, 1994.  
[10] Tien‐Pao Shih. Goal‐Directed Performance Tuning for Scientific Applications. Ph.D. 
Dissertation, Department of Electrical Engineering and Computer Science, The University 
of Michigan, Ann Arbor, June 1996.  
[11] Shih‐Hao Hung. Optimizing Parallel Applications. Ph.D. Dissertation, Department of 
Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, June 
1998.  
[12] John L. Hennessy and David A. Patterson. Computer Architecture: A Quantitative Approach, 
Morgan Kaufmann Publishers, Inc., 1990.  
[13] Ann H. Hayes, et al. Debugging and Performance Tuning for Parallel Computer Systems. 
IEEE Computer Society Press, 1996.  
[14] Ian Foster. Design and Building Parallel Programs: Concepts and Tools for Parallel Software 
Engineering. Addison‐Wesley, 1994.  
[15] Steven S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann, 
1997.  
[16] Denis Sheahan, Developing and Tuning Applications on UltraSPARC T1 Chip 
Multithreading Systems, Sun BluePrints™ OnLine—December 2005.  
(http://www.sun.com/blueprints/1205/819‐5144.pdf)  
[17] Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. Compiler Principles, Techniques and 
Tools. Addison‐Wesley, 1986.  
[18] Peter L. Deutsch and Allan M. Schiffman. Efficient Implementation of the Smalltalk‐80 
System, In the 11th ACM SIGACT/SIGPLAN Symp. on Principles of Programming 
Languages, Salt Lake City, 1984.  
[19] James Gosling, Bill Joy, and Guy Steele. The Java Language Specification. Addison‐Wesley, 
1996.  
[20] S.‐H. Hung and P. Bhattacharya. On the Delivered Performance of the Sun Cryto Accelerator 
1000. The Sun Users Performance Group 2002 Conf., Honululu, Hawaii, April. 2002.  
[34] Security ‐ Principles and Practice, 2nd Ed. Prentice Hall, 1998.  
[35] The Network Security Services (NSS) Project Homepage. Overview of NSS Open Source 
Crypto Libraries. http://www.mozilla.org/projects/security/pki/nss/overview.html.  
[36] The OpenSSL Project Homepage. http://www.openssl.org/.  
[37] The Apache Software Foundation. http://www.apache.org/.  
[38] R. L. Rivest, A. Shamir, and L. M. Adelman. On Digital Signature and Public Key 
Cryptosystems, Technical Report, MIT/LCS/TR‐212, MIT Laboratory for Computer 
Science, January 1979.  
[39] ANSI. American National Standard for FInancial Institution Key Management. ANSI X9.17, 
1985.  
[40] Gheith A. Abandah. Tools for Characterizing Distributed Shared Memory Applications. 
Technical Report HPL‐96‐157, HP Laboratories, December 1996.  
[41] D. Bailey, et al. The NAS Parallel Benchmarks. Technical Report RNR‐94‐07, NASA Ames 
Research Center, March 1994.  
[42] Kadayif, I., Kandemir, M., and Karakoy, M. An energy saving strategy based on adaptive 
loop parallelization, in Proceedings of the Design Automation Conference, New Orleans, 
LA, June 2002.  
[43] George Karypic and Vipin Kumar. METIS: Unstructured Graph Partitioning and Sparse 
Matrix Ordering System Version 2.0. Technical Report, The University of Minnesota, 1995.  
[44] Karen A. Tomko and Edward S. Davidson. Profile driven weighted decomposition. In Proc. 
1996 ACM International Conference on Supercomputing, pp. 165‐172, May 1996.  
 
（以下論文為本研究計畫贊助之成果）  
 
[45] Chien‐Cheng Wu. Performance Evaluation of Caching and Prefetch Strategies on a Storage 
Server. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of Computer Science and 
Information Engineering, National Taiwan University, July 2007.  
[46] Jia‐Siang Chen. Performance Optimization on a RAID System: Design and Implementation 
of a Fast Indexing Table for Disk Caching. Master Thesis (Advisor: Shih‐Hao Hung), Dept. 
of Computer Science and Information Engineering, National Taiwan University, July 2007.  
[47] Shu‐Jheng Huang. Developing New Tracing and Performance Analysis Techniques for 
Embedded Applications. Master Thesis (Advisor: Shih‐Hao Hung), Graduate Institute of 
Networking and Multimedia, National Taiwan University, July. 2007. 
[48] Yi‐Di Lin, Automating Server Application Performance Modeling Process on Solaris System 
via D‐Trace and Trace‐Driven Analysis. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of 
Computer Science and Information Engineering, National Taiwan University, July 2007.  
[60] Jiang-Rui Chen. A System-Level Performance and Power Simulation Environment for 
Embedded Computer Systems. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of 
Computer Science and Information Engineering, National Taiwan University, July 2008. 
[61] Chang-Jun Huang. Performance Optimization of the Samba write serviceon Linux-based 
Network-Attached Storage Systems. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of 
Computer Science and Information Engineering, National Taiwan University, July 2008. 
[62] Feng-Xu Zhuang. Power Estimation for Embedded Processors based on Verilog-to-SystemC 
Conversion. Master Thesis (Advisor: Shih‐Hao Hung), Graduate Institute of Networking 
and Multimedia, National Taiwan University, July. 2008. 
[63] Kuan-Ju Chen. Evaluating Task Migration Overhead for Heterogeneous Multi-Core 
Architectures. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of Computer Science and 
Information Engineering, National Taiwan University, July 2008. 
[64] Yuh-Hung Liaw. Performance Modeling for Multicore Embedded Processors based on 
Verilog-to-SystemC Conversion. Master Thesis (Advisor: Shih‐Hao Hung), Graduate 
Institute of Networking and Multimedia, National Taiwan University, Jan. 2009. 
[65] Yu-Hsien Lin. Chiaheng Tu, Chi-Sheng Shih and Shih-Hao Hung, Zero-Buffer Inter-Core 
Communication Protocol for Heterogeneous Multi-core Platforms, The 15th IEEE 
International Conference on Embedded and Real-Time Computing Systems and  
Applications (RTCSA 2009), Beijing, China, Aug. 2009. 
[66] Shih-Hao Hung, Chia-Heng Tu, Huang-Sen Lin and Chi-Meng Chen. An Automatic 
Compiler Optimizations Selection Framework for Embedded Applications, The 6th 
International Conference on Embedded Software and Systems (ICESS 2009), HangZhou, 
Zhejiang, China, May 2009. 
[67] Zong-Cing Lin. Big Integer Factorization on Parallel Computers. Master Thesis (Advisor: 
Shih‐Hao Hung), Dept. of Computer Science and Information Engineering, National Taiwan 
University, July 2009. 
[68] Wen-Long Yang. Development of a High-Performance Inter-processor Communication 
Library for Multicore Platform. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of 
Computer Science and Information Engineering, National Taiwan University, July 2009. 
[69] Wen-Chang Hsu. Fast and Non-intrusive Profiling of Application Performance with a Virtual 
Platform. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of Computer Science and 
Information Engineering, National Taiwan University, July 2009. 
[70] Hao-Hsiang Chang. Design and Software Support of Hybrid Storage Devices. Master Thesis 
(Advisor: Shih‐Hao Hung), Dept. of Computer Science and Information Engineering, 
National Taiwan University, July 2009. 
[71] Yu-Jen Chen. Design and Implementation of Network Attached Storage Servers with the 
IBM Cell Platform. Master Thesis (Advisor: Shih‐Hao Hung), Dept. of Computer Science 
and Information Engineering, National Taiwan University, July 2009. 
Scalable Lossless High Definition Image Coding on 
Multicore Platforms 
Shih-Wei Liao2, Shih-Hao Hung2, Chia-Heng Tu1, Jen-Hao Chen2 
 
1Graduate Institute of Networking and Multimedia 
2Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 106 
{liao, hungsh, d94944008, r94125}@csie.ntu.edu.tw 
Abstract. With the advent of multicores in all processor segments including 
mobile, embedded, desktop and server ones, we are in the new era of 
multiplying computing power via scaling the number of cores. The multicore 
approach is more versatile and programmable than the ASIC approach. For 
instance, the same multicore product can be adapted to the ever-improving 
potpourri image processing standards. Developing ASIC modules for each 
standard would pose million-dollar start-up cost and time-to-market 
disadvantage. However, the multicore approach is a two-edge sword: 
Unleashing its multiplying power presents significant programming challenges. 
The harmony between the multiplying power and programming productivity is 
the holy grail in this field. This paper addresses the challenge in the Digital 
Cinema domain. This paper presents an oblivious parallelization paradigm in 
both compressing and decompressing images via JPEG2000 on multicore 
platforms with maximum productivity. This approach dramatically reduces 
compression and decompression time in performing JPEG2000 lossless 
encoding and decoding algorithms on high definition images in almost real time 
without any extra hardware acceleration. By boosting parallelism coverage, the 
high resolution images could be compressed and decompressed in near real 
time: 15 images decoded/encoded per second. To the best of our knowledge, we 
are the first to propose a software-based coding solution using commodity 
multicores to achieve near real-time performance result for JPEG2000. This 
cost-effective approach could be applied to digital cinema on devices with 
multicores. 
Keywords: JPEG2000, Lossless, Multicore SoC, Parallelization, Digital 
Cinema, Embedded System, Image Compress, Image Decompress. 
1 Introduction 
Full HD is the standard for the next generation digital TVs. The 2K and 4K digital 
cinema are also on their way. This kind of huge data flow poses difficult challenges 
on both storage space and transmission bandwidth. Therefore, compression is a 
necessity. Furthermore, while consumer product users can tolerate minor image 
Both approaches use parallelization to speed up processing. The JPEG2000 standard 
can be roughly divided into three levels of parallelization paradigms.  
The highest level is the oblivious parallelization paradigm. In such scheme the 
image is divided into several segments and coded independently. The middle level is 
the tile level parallelization paradigm, in which image is segmented into tiles, and 
coded altogether into single JPEG2000 code stream. The lowest level is the embedded 
block coding level parallelization paradigm, which partition each sub-band result 
from DWT (Discrete Wavelet Transforms) into separate code blocks.  
Among hardware solutions, Shirai et al. [3] adopt the oblivious parallelization 
paradigm to build a system based on the JPEG2000 acceleration boards. There are in 
total four acceleration boards in the system, connected via PCI-X bus. Each 
acceleration board is responsible for a quarter of the image; each color component is 
further delegated to four JPEG2000 ASICs. Using these systems, real-time 4K SHD 
(4096x2160) quality international conference has been held between Kyoto, Japan 
and San Diego, USA. Literature [4] shows that embedded code blocking is the most 
time consuming part of JPEG2000 process. There are several hardware 
implementations that focus on this portion [5, 6, 7].  
Software solutions such as Jasper [8, 9], JJ2000 [10], and Kakadu [11], mostly run 
in a single process. Threading techniques, used in [11] and [12], are deployed to 
parallelize embedded code blocking process as hardware solutions do. In this paper, 
we evaluate Jasper [8] and Kakadu [11] performance with high resolution, real-world 
pictures. As the result in Figure 1 shows, Kakadu [11] outperforms Jasper in single 
process with multithreading performance. Furthermore, both coders fail to 
encode/decode in real-time, 15 frames per second. Based on this result, we hereafter 
choose Kakadu [11] as our baseline software-based JPEG2000 encoder/decoder. Note 
that the hardware specification will be given in Section 4. 
1.3 Scaling State-of-the-Art Software Solution 
Kakadu uses threading to parallelize time-consuming portions. In this paper we 
investigate Kakadu's scalability in detail. As the figure in Section 2.2 shows, Kakadu 
fails to scale beyond three times speedup even on an 8-core machine with 32 
CoolThreads. This poor scalability makes Kakadu unsuitable for meeting real-time, 
high-resolution performance requirements. This motivates us to propose a software 
implementation of oblivious parallelization paradigm on multicore architecture. 
Our multicore implementation has three distinct advantages: First, it has time-to-
market productivity advantage. It takes less than three man-months to deploy. Second, 
performance scales with increased number of processor cores. Our result shows that 
the multicore approach can meet real-time Full HD constraints. Even 2K SHD can be 
processed in real time. Third, the high parallelism coverage of our implementation 
removes the limitation induced by the 20% sequential portion in the Kakadu 
encode/decode process.  
In summary, the paper demonstrates the feasibility of multicore solutions in the 
Digital TV/Cinema domains which was traditionally dominated by proprietary 
ASICs. In addition, we are the first to propose the software-based oblivious 
wavelet transform. Each intra-tile’s frequency characteristics are kept in the 
coefficients. Third, wavelet transform is followed by quantization, which is 
responsible for quantizing and partitioning the coefficients into code-blocks. 
Quantization is also the stage making decision of rate control (distortion). Fourth, 
code-blocks, the fundamental entities of tier-1 coding, are independent coded and the 
result, compressed data, is fed into next stage. Finally, those compressed data 
generated by previous stage is organized as packets (code-stream) by tier-2 coding. 
Due to the diverse functionalities that JPEG2000 have, especially in that 
JPEG2000 provides both lossless and lossy coding scheme, there are many 
applications that could benefit from its nature capabilities, such as image distribution 
through internet, security systems over network, digital photography and medical 
imaging. From the applications above, we could know that JPEG2000 is an image 
coding technology for the higher quality and smaller data size. Thus, as long as the 
demands in seeking for perfect image quality exist, the need for JPEG2000 will not 
stop. 
2.2 Traditional Approach in Parallelizing JPEG2000 Coding 
Much work has been done in the literature in either parallelizing or optimizing the 
performance of JPEG2000 coding [12]. They take advantage of the multi-level 
parallelization opportunities in JPEG2000 standard, such as tiles, sub-bands, and 
code-blocks. In this section we would discuss about how far as they go and where the 
obstacles are. One typical obstacle is the poor scalability with respect to the number 
of cores. 
Based on the literature and our experiments below [12, 14], the maximum speedup 
obtained from previous work is 3. Also, previous work typically does not focus on 
high resolution images or real-world images. Thus, we first run tests on our machines 
and compare the performance of two JPEG200 coders, Jasper and Kakadu. Next, we 
determine the maximum speedup we could get from these software implementations 
and choose the better one as our baseline. Note that the detailed information about the 
machine environment and tested images will be given in Section 4. 
Performance comparison between Jasper and Kakadu is presented in Figure 2. 
Apparently, Kakadu outperforms Jasper in the entire spectrum of images ranging 
from 300 mega to 100 mega pixels. Single process and single thread are used in both 
runs. We will only experiment with Kakadu in the remaining paper, since Jasper is 
slower.  
Figure 3 and 4 show the speedup of encoding an image by Kakadu on both HP 
DL380 with 4 logical processors and SUN Fire T2000 with 32 CoolThreads, 
respectively. The resolution of the image is 3648x2736. The speedup trend shown in 
Figure 3 and 4 demonstrate the parallelism coverage of Kakadu is not high enough. 
That is why speedup shown in Figure 3 remains below 3 while the number of tiles 
increases. As we introduce more threads, run on SUN Fire T2000 with 32 
CoolThreads enabled, the speedup shown in Figure 4 remains below 1.15. It is worthy 
to note that the speedup trend of decoding the image on both machines shares the 
same characteristics with Figure 3 and 4, respectively. 
3.1 Illustration and Rationale 
Our proposed approach aims at boosting the parallelism coverage yet helping 
programmer’s productivity at the same time. Intuitively, the speedup would be N if 
we could process N pieces of the original image in parallel rather than the whole 
image in serial. Figure 5 illustrates the concept of oblivious parallelization paradigm. 
Note that the nature of JPEG2000 supports such style of parallel coding.  
In comparison, although it is a common and effective way to process image coding 
in parallel, tiling may lead to the block effect in the resulting image. Besides, the 
parallelism coverage is lower than our oblivious parallelism scheme. To alleviate the 
block effect, researchers also propose other parallelized regions such as wavelet 
transform and tier-1 coding. However, the coverage is still lower. 
 
 
Fig. 5. Oblivious Parallelization Paradigm 
The overhead of dividing and merging data in Figure 5 would be negligible, since 
the encoding operation (ppm format to jp2 format) occurs mostly when the image 
capturing device reads out the raw data and stores (transforms) them into memory 
card or embedded memory in a compressed format, say jp2, and the decode operation 
(jp2 format to ppm format) takes place mostly when the compressed file are going to 
be displayed on the machine where the jp2 file located. On either circumstance, the 
proposed approach will mainly stress the memory system and file system. Oblivious 
parallelism paradigm will overlap the CPU and memory/file accesses better than the 
CPU-focused tiling approach. 
Intermediate file size in jp2 format is another possible issue. However, as our 
experiment in Section 4 shows, only less than 10% space overhead results, even if we 
divide an image of 2048x1036 into 140 pieces. 
Finally, the division and blocking effect would not be a concern if we only focus 
on lossless JPEG2000 coding. The essence of reversible transformation is that either 
encoded or decoded data would be the same, regardless of the number of pieces we 
process concurrently. 
3.2 Modeling the Computation Power of Platform 
To bound the JPEG2000 coding time for different images, we propose a Random-
Generated Image Algorithm. The algorithm generates an image with the same 
contribute to digital devices that perform image coding, but also extend to other field, 
such as medical image coding. 
We first describe the experimental setups and the input images. Table 1 outlines 
our environments, which are commodity main stream machines on the market. Note 
that the experiments done on HP DL380 machine are all run with 4 logical processors, 
while the experiments run on SUN Fire T2000 are with either 32 CoolThreads, or 
varied CoolThreads, to observe the relationship between the number of cores and 
speedup. The mapping between tasks and execution units in multicore is handled 
either by user level library or operating system, where the tasks refer to sub-images to 
be encoded or decoded. Table 2 listed the resolution of real-world images used in this 
paper. All the images, ranging from 300 mega to 1000 mega pixel (MP), are 
transformed into ppm file format before the experiment. 
Table 2.  Testing images information (captured by different digital cameras).  
Resolution Image size (.ppm) Pixels 
3648x2736 
3488x2616 
3264x2448 
29,241 KB 
29,388 KB 
25,735 KB 
9,980,928 
9,124,608 
7,990,272 
3072x2304 20,736 KB 7,077,888 
3008x2000 17,211 KB 6,016,000 
2816x2112 
2592x1944 
2400x1800 
2272x1704 
2048x1536 
17,051 KB 
14,416 KB 
12,359 KB 
11,342 KB 
9,000 KB 
5,947,392 
5,038,848 
4,320,000 
3,871,488 
3,145,728 
 
 
 
Figure 6 depicts the trend while the image resolution increases. It is obvious that 
the higher the image resolution is the longer execution time it needs to process 
(decode/encode) the image. As expected, random-generated images (uniform-
distributed images) cost more time than the real-world images do. Unrelated pixels 
result in extra processing time. Also, this leads to larger size of compressed file, jp2 
file, than that of the original ppm file. This property could help us analyze the 
Fig. 7. Linear Regression Functions of 
encoding Real-World and Uniform 
Distributed Images on HP DL380. 
Fig. 6. Performance trend on both Real-
World and Uniform Distributed images. 
  
Fig. 9. Speedup on Encoding/Decoding a 2048x1536 image with Different Virtual Processors 
on HP DL 380 and  SUN Fire T2000. 
 
Fig. 10. Size overhead introduced by Oblivious Parallelization Paradigm. 
5 Conclusion and Future Work 
This paper presents an oblivious parallelization paradigm to boost parallelism 
coverage in high definition image compression and decompression for Digital Cinema 
domain. To the best of our knowledge, we are the first to propose a pure software-
based solution on JPEG2000 image coding. Near real-time performance could be 
obtained from proposed method. 
The contributions of this paper are three-fold. First, we show that images for 2K 
Digital Cinema could be compressed or decompressed in real-time, 24 frames per 
second, if the hardware resources are adequate. Second, the potential scalability and 
overhead of oblivious parallel paradigm are well presented. Finally, we have also 
proposed the Random-Generated Image Algorithm to predict image coding 
performance for specific hardware and software combination. Furthermore, according 
to our results, this modeling could be done faster by running few numbers of random-
generated images. This could help to shorten the time for finding upper-bound 
execution time of image coding on specific platform. 
Task Scheduling for Context Minimization in
Dynamically Reconfigurable Platforms
Nei-Chiung Perng and Shih-Hao Hung
Department of Computer Science and Information Engineering
National Taiwan Univeristy
106 Taipei, Taiwan
{d90011,hungsh}@csie.ntu.edu.tw
Abstract. Dynamically reconfigurable hardware provides useful means to re-
duce the time-to-prototype and even the time-to-market in product designs. It also
offers a good alternative in reconfiguring hardware logics to optimize the system
performance. This paper targets an essential issue in reconfigurable computing,
i.e., the minimization of configuration contexts. We explore different constraints
on the CONTEXT MINIMIZATION problem. When the resulting subproblems
are polynomial-time solvable, optimal algorithms are presented.
1 Introduction
Dynamically reconfigurable hardware (DRH) allows partial reconfigurations to provide
different functionalities over a limited number of hardware logics, compared to the pop-
ular Application-Specific Integrated Circuit (ASIC) approach. It was recently raised by
many researchers that the DRH technology is very suitable to deal with the dynamism
of multimedia applications [1,2]. In such applications, instructions might be partitioned
into coarse-grained tasks with a partial order and loaded onto dynamically reconfig-
urable devices for executions! Reconfigurability has recently become an important is-
sue in the research community of embedded systems especially for FPGAs [3,4,5,6]. An
FPGA configuration context, also referred to as a context, is the basic element to load a
hardware description of a task. A multi-context system is a system with more than one
FPGA chips or an FPGA chip with its configurable logic blocks being partitioned into
several (equal-sized) areas. Although multi-context systems allow simultaneous task
executions on different contexts, many implementations only allow the loading of one
task at a time in reality [7].
One way to avoid the waiting time of task loadings is to pre-load proper hardware
descriptions before the run time. In particular, Hauck [8] presented the concept of con-
figuration prefetching in which the loading duration was overlapped with computations
to reduce the overheads. Harkin et al. [9] evaluated nine approaches of hardware/soft-
ware partitioning to provide insights in the implementation methodology for reconfig-
uration hardware. A genetic algorithm (GA) was also presented by the same authors
for run-time reconfiguration [10]. Yuh et al. [11] developed a tree-based data structure,
called T-tree, for a temporal floorplanning to schedule all the reconfigurable tasks with a
simulated-annealing-based algorithm. Ghiasi et al. [12] proposed an efficient optimal al-
gorithm to minimize the run-time reconfiguration delay in the executions of applications
T.-W. Kuo et al. (Eds.): EUC 2007, LNCS 4808, pp. 55–63, 2007.
c© IFIP International Federation for Information Processing 2007
Task Scheduling for Context Minimization in Dynamically Reconfigurable Platforms 57
time interval. The third condition requires the loading of contexts should be done one
by one. The three conditions are defined formally as follows:
Condition 1 (In-Time Loading). ∀τi, T(τi) + li ≤ S(τi) and S(τi) + ei ≤ D.
Condition 2 (Non-Overlapping Configuration Contexts). ∀ (τi, τj) pair, C(τi) =
C(τj) if any two time intervals (T(τi),S(τi) + ei] and (T(τj),S(τj) + ej] have
a non-null intersection.
Condition 3 (Mutual Exclusion on Loading). ∀ (τi, τj) pair, T(τi) + li ≤ T(τj)
or T(τj) + lj ≤ T(τi).
A reconfiguration plan is optimal if it is feasible, and the number of its required con-
texts, i.e., the largest ID of the assigned contexts, is equal to the minimum number
of required contexts of all feasible reconfiguration plans. We shall show later that this
optimization problem is NP-complete.
3 Problem Properties
In this section, we prove the NP-completeness of the CONTEXT MINIMIZATION
problem and later explore subproblems in which polynomial-time solutions exist. For
the sake of clarity, we transform this optimization problem into an equivalent decision
problem by providing a bound on the number of FPGA configuration contexts. The
decision version of the CONTEXT MINIMIZATION problem is to find a solution with
the number of required contexts no larger than a given number M .
3.1 NP-Complete Subproblems
We shall show the NP-completeness of two subprograms of the CONTEXT MIN-
IMIZATION problem under two constraints: (1) The execution time of each task is
identical, i.e., ∀i, ei = E. (2) The loading duration of each task is negligible, i.e.,
∀i, li = 0. Before we show the NP-completeness of the two subproblems, we shall
first define the PRECEDENCE CONSTRAINED SCHEDULING (SS9 in [14]), that is
NP-complete:
Given a set T of tasks, the execution time ei of every task τi ∈ T is 1, a given number
M ∈ Z+ of processors, a partial order ≺ of tasks ∈ T , and a deadline D ∈ Z+, the
problem is to derive a schedule σ over the M processors so that the deadline and the
partial order of task executions are satisfied. In other words, ∀τi, τj ∈ T , τi ≺ τj
implies σ(τj) ≥ σ(τi) + ei, where σ(τi) denotes the starting time of task τi.
Theorem 1. The CONTEXT MINIMIZATION problem under the constraint ∀i, ei = E
and li = 0 is NP-complete, where ei and li denote the execution time and the loading
duration of task τi, respectively.
Proof. This subproblem is indeed the PRECEDENCE CONSTRAINED SCHEDUL-
ING problem. 
Task Scheduling for Context Minimization in Dynamically Reconfigurable Platforms 59
task arbitrarily (Step 3) and load the task onto the context with the earliest available
time (Step 4). The loading time, starting time, and context ID of the task are updated
accordingly (Steps 5-7). The earliest possible time for the next context loading is then
updated (Please see Step 8 and Condition 3). The algorithm reports a failure if any task
misses the deadline (Steps 9-11). The time complexity is O(n × log M).
Theorem 4. Algorithm 1 is optimal in the sense that it always derives a solution if any
feasible solution exists.
Proof. The correctness of this theorem follows directly from the fact that all tasks are
of the same execution time and loading duration and share the common deadline. 
Figure 3.2 shows four optimal reconfiguration plans of four tasks, where the number
M of FPGA configuration contexts ranges from 1 to 4. An interesting packing of tasks
is shown in the figures, and the impacts of the mutual exclusion constraint, i.e., Con-
dition 3, are clearly illustrated. Note that the shaded rectangles denote the loadings of
tasks onto contexts, and white rectangles denote task executions.
Fig. 1. Four reconfiguration plans over four different numbers of contexts
Lemma 1. Algorithm 1 needs no more than MB = max{n, EL +1} contexts to derive
a feasible reconfiguration plan, where n is the number of tasks.
Proof. The correctness of this lemma follows directly from the facts that loading dura-
tions can not be overlapped with each another, and a loading duration can be overlapped
with any execution time as long as the three feasibility conditions are satisfied. 
Lemma 1 provides an upper bound on the maximum number of contexts over that Al-
gorithm 1 could derive a feasible reconfiguration plan. Figure 2 shows reconfiguration
plans for two task sets, i.e., one with E ≤ L and the other with E > L, where different
numbers of FPGA configuration contexts are tried. Note that when n × (L + E) ≤ D,
only one context is needed to derive a feasible reconfiguration plan.
Although we show that the CONTEXT MINIMIZATION problem becomes tractable
when ≺= ∅, and ∀i, ei = E and li = L, one question remains. That is how difficult
Task Scheduling for Context Minimization in Dynamically Reconfigurable Platforms 61
and there are two critical paths in the intree. The CP rule provides a way to assign tasks
priorities, where a task in the front of a critical path is assigned a higher priority, as
shown in Figure 3. In fact, the priority assignment of tasks follows a topological order.
Tie-breaking is done in an arbitrary way.
Algorithm 2
Input: A task set T (∀τi ∈ T, τi = (E, 0)), ≺ as a tree, a deadline D, and M contexts
Output: A feasible reconfiguration plan 
1: Assign priorities to tasks according to the CP rule.
2: S = { τi : τi ∈ T does not have any predecessor }.
3: while S is not empty do
4: Remove the task τi with the highest priority from S.
5: Locate context Mj with the earliest idle time Ij .
6: T(τi) = S(τi) = Ij .
7: C(τi) = j.
8: S = S ∪ {τj}, where τi ≺ τj if all of the predecessors of τj have been scheduled.
9: if S(τi) + E > D then
10: Return failure.
11: end if
12: end while
Algorithm 2 derives a feasible reconfiguration plan whenever possible: Tasks are first
assigned priorities according to the CP rule (Step 1). Initially, S is set as the set of ready
tasks in T (Step 2), where a ready task is a task with all of its preceding tasks in the
partial order complete. Each iteration of the loop between Step 3 and Step 12 is to load
a task onto a proper context. The ready task with the highest priority is loaded onto the
context with the earliest possible idle time. The loading time and the starting time of
the task is set as the earliest possible idle time of the context (Step 6), where ∀i, li = 0.
The context ID of the task is then set (Step 7). After the task is scheduled, any ready
task resulted from the scheduling join the ready task pool (Step 8). If the deadline is
violated, then the algorithm reports a failure (Steps 9 and 10). The time complexity of
Algorithm 2 is O(n2).
Theorem 5. Algorithm 2 is optimal in the sense that it always derives a solution if any
feasible solution exists.
Proof. The optimality of the algorithm is based on the proof of the CP rule in [15]. 
A Task Set with E ≤ L. As shown in the previous section, the CONTEXT MINI-
MIZATION problem becomes more tractable when a partial order is restricted in a tree
fashion, compared to that shown in Theorem 1. Another question is whether we could
trade the partial-order constraint with any other constraint to keep the CONTEXT MIN-
IMIZATION problem being tractable. In this section, we shall show that the CONTEXT
MINIMIZATION problem remains tractable by the constraint ∀i, ei = E, li = L, and
E ≤ L. In such a case, the partial order among tasks could be arbitrary.
Task Scheduling for Context Minimization in Dynamically Reconfigurable Platforms 63
References
[1] Kneip, J., Schmale, B., Moller, H.: Applying and implementing the mpeg-4 multimedia
standard. IEEE Micro 19(6), 64–74 (1999)
[2] Noguera, J., Badia, R.M.: Multitasking on reconfigurable architectures: Microarchitec-
ture support and dynamic scheduling. ACM Transactions on Embedded Computing Sys-
tems 3(2), 385–406 (2004)
[3] De Micheli, G., Gupta, R.K.: Hardware/software co-design. Proceedings of the IEEE 85(3),
349–365 (1997)
[4] De Hon, A., Wawrzynek, J.: Reconfigurable computing: What, why, and implicatios for
design automation. In: Proceedings of the 36th ACM/IEEE Conference on Design Automa-
tion, pp. 610–615 (1999)
[5] Hauck, S.: The roles of FPGA’s in reprogrammable systems. Proceedings of IEEE 86(4)
(1998)
[6] Wolf, W.: FPGA-Based System Design. Prentice-Hall, Englewood Cliffs (2004)
[7] Xilinx Inc.: XAPP151 Virtex Series Configuration Architecture User Guide (v1.7) edn.
(2004)
[8] Hauck, S.: Configuration prefetch for single context reconfigurable coprocessors. In:
ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (1998)
[9] Harkin, J., McGinnity, T.M., Maguire, L.P.: Partitioning methodology for dynamically
reconfigurable embedded systems. IEE Proceedings on Computers and Digital Tech-
niques 147(6), 391–396 (2000)
[10] Harkin, J., McGinnity, T.M., Maguire, L.P.: Modeling and optimizing run-time reconfigu-
ration using evolutionary computation. ACM Transactions on Embedded Computing Sys-
tems 3(4), 661–685 (2004)
[11] Yuh, P.H., Yang, C.L., Chang, Y.W.: Temporal floorplanning using the T-tree formulation.
In: Proceedings of ACM/IEEE International Conference on Computer-Aided Design (2004)
[12] Ghiasi, S., Nahapetian, A., Sarrafzadeh, M.: An optimal algorithm for minimizing run-time
reconfiguration delay. ACM Transactions on Embedded Computing Systems 3(2), 237–256
(2004)
[13] Resano, J., Mozos, D., Catthoor, F.: A reconfigurable manager for dynamically reconfig-
urable hardware. IEEE Design and Test of Computers 22(5) (2005)
[14] Garey, M.R., Johnson, D.S.: Computers and Intractability. W. H. Freeman and Company,
New York (1979)
[15] Pinedo, M.: Scheduling Theory, Algorithms, and Systems, 2nd edn. Prentice-Hall, Engle-
wood Cliffs (2002)
  
applications. Based on the Moduletracer, we have built 
an extendable and feasible framework that helps the 
programmers to retrieve profile data easily. This portable 
framework provides a set of tools that retrieves accurate 
elapsed time of each routine without hardware support.  
The rest of this paper is organized as following. 
Section 2 introduces some related work of instrumentors 
and profilers. Section 3 introduces a Trace Collection and 
Trace Post-Processing framework (TCPP). In Section 4, 
we describe our toolkit which is based on the TCPP 
framework. The results and validations are shown in 
Section 5. Before making a conclusion, we use a case 
study to demonstrate the convenience of our toolkit in 
Section 6. Finally, the Section 7 states the conclusion. 
2. Related Work 
A variety of techniques has been developed to 
measure and analyze computer system performance. In 
this section, we discuss the tools that are related to 
retrieving execution traces and performance profile. We 
divided this section into two sections, the instrumentor 
and the profiler. The instrumentor inserts extra code in a 
program during the compilation process, and the profiler 
is responsible for collecting performance statistics during 
the runtime. 
2.1. Instrumentor 
 
Figure 1.  The ATOM instrumentation process. 
Instrumentation can be performed at various stages: in 
the source code, at compile time, post link time, or at run 
time. ATOM [4], a pioneer in static binary 
instrumentation, is developed on top of a general-purpose 
binary code editing framework, as shown on Figure 1. 
ATOM provides the common infrastructure which is 
resembled by most code instrumentation tools. With the 
infrastructure, a user can customize a tool by defining the 
details in the instrumentation routine (User Instrumenting 
Code) and the analysis routine (User Analysis Code). 
With these two files compiled into object code, ATOM 
creates a custom executable code rewriter. The input of 
this rewriter is an executable program and the output of 
this rewriter is the instrumented program. ATOM is a 
powerful instrumentor for building a diverse set of 
profiling tools, but it works only on few architectures. 
 
 
Figure 2.  Software architecture of PIN. 
Intel’s PIN [7] is a software system that performs run-
time binary instrumentation on Linux applications. Its 
usage model is similar to the ATOM API, which allows a 
tool to insert calls to facilitate instrumentation at arbitrary 
locations in the executable. Figure 2 illustrates PIN’s 
software architecture. PIN consists of a virtual machine, 
code cache, and an instrumentation API invoked by 
Pintools. After PIN gains control of the application, the 
components of virtual machine work together to execute 
the application. The JIT (Just-in-Time) compiler 
instruments the application code. Then the instrumented 
application is lunched by the dispatcher. Since it sits 
above the operating system, PIN works only on user-
level codes. The SUIF compiler [9] is a research 
compiler that has been used to investigate automatic 
parallelization, interprocedural analysis, and memory 
disambiguation. Halt, the Harvard Atom-Like Tool, 
allows SUIF users to instrument many aspects of 
program behavior. The user can point out interesting 
parts of a program by labeling them with SUIF 
annotations. Then Halt looks for these annotations, and 
inserts analysis routines that match the type of the 
annotation. Unfortunately, SUIF failed to handle GCC 
extensions, which prevented them from working on 
codes that use these extensions, such as Linux device 
driver and kernels. 
Another technique, FLAT [10], uses GCC to obtain 
the basic block counts of a program. Although this is an 
easy method, developers need to handle the new function 
of GCC on frequently releasing version. 
2.2. Profiler 
Profiling is a technique for identifying performance 
bottlenecks in programs by measuring the time spent in 
each subroutine as the program runs. A profiler produces 
an execution profile. Gprof [1] uses timers for statistical 
sampling of the program counter to estimate time spent in 
  
contains two components: (1) a data structure for the 
events in the trace and (2) a set of APIs for trace 
processing. This data structure consists of the parameters 
of events and the environment variables of the traced 
program, such as the thread id. The trace post-processing 
stage uses the APIs to process the traces. For example, 
The trace post-processing tools read the next record of 
traces by using read_next(). In our path profiling scheme, 
the trace post-processing tool checks whether a record is 
a function entry or not.  
In addition, by providing the trace interface, the user 
does not have to be aware of the architecture-dependent 
issues, such as endianness, as the interface handles fields 
properly for the user. Ideally, a standardized trace 
interface should allow us to choose/replace the pre-
processing, trace collection, and trace post-processing 
tools freely without worrying about their compatibility. 
3.5. Transformation and Simulation 
The trace collected by trace collector is fed to a 
program that simulates the behavior of a hypothetical 
process or transforms it to another format for further 
consumption. To reduce the effort of developing analysis 
tools, the development of a post-processing tool can take 
advantage of the APIs specified by the trace interface to 
retrieve the traces without having the understand the 
internal format of the traces. 
3.6. Visualization 
The visualization stage makes the profile information 
readable and comprehensive. Representing the profile 
data without unnecessary details is critical to a good 
performance report. Some users may like visual graphs 
better than text reports. Our survey shows that there are 
tools that utilize a standard interface to provide variety of 
visualization, such as Gnuplot, ParaGraph [14], and 
Graphviz [15]. 
4. Moduletracer 
Moduletracer is a toolkit based on TCPP framework. 
The motivation we developed this toolkit is that there are 
few completely portable profilers or performance 
analysis tools. There are far more performance analysis 
tools developed for x86-based platforms than for the 
other microprocessors such as PowerPC. Furthermore, 
many tools do not work for programs running in the 
kernel space. This is unfortunate as many kernel 
module/functions are important to application 
performance. Without good trace/profiling tools, it is 
very difficult to locate and quantify performance 
bottlenecks and for complex applications. 
We will describe the architecture of Moduletracer in 
Section 4.1. Section 4.2 and 4.3 describes that how 
Moduletracer uses traces to implement the two important 
features, path profiling and execution time profiling. In 
the last section, Section 4.4, we introduce the mechanism 
of gathering traces from the kernel space and the user 
space. 
4.1. Architecture 
Figure 4 gives an overview of Moduletracer, but there 
is a little bit difference between Moduletracer and TCPP 
framework, and we will describe the differences in 
Section 4.2. CIL [16], a source-to-source translator, is 
used in Moduletracer to break down the constructs of a C 
program into simpler constructs, which enable us to find 
interesting events for instrumentation. Note that CIL is 
capable of handling the GNU C extensions, which may 
be critical in practice as these extensions are used in 
Linux kernel and drivers. 
The input of CIL is the source files to be 
instrumented. During the initialization of CIL, a pre-
defined plug-in (instrumentation rule) is loaded from a 
file called instrumentation library. CIL uses the 
instrumentation rule to identify the events of interests and 
then inserts a trace handler (a.k.a. probe) at specific 
places based on the rule. 
The trace handler is a routine in the Trace Collecting 
Handler Library. Figure 5 gives a high-level overview of 
the trace handler used in path profiling. The trace handler 
has to set the data structures defined by the trace 
interface and put the data structure in the storage. Once 
the path profiling plug-in is activated, CIL automatically 
inserts Function_Entry() at the prologue of the 
instrumented routines and Function_Exit() at the 
epilogue of those routines. 
 
 
Figure 4.  The overview of Moduletracer. 
Note that GNU GCC also provides a similar 
mechanism for instrumentation, but it is not as flexible as 
CIL in selecting the events of interests, which limits the 
  
 
Figure 7.  An example of CCT generated by Moduletracer. 
4.3. Execution Time Profiling 
For execution time profiling, the execution time of a 
routine can be gathered in three methods: 
 
I. Measuring the elapsed time: This method 
measures the execution time of a routine by 
measuring the elapsed time from the routine entry 
to the routine exit. For this method to work, the 
trace handler has to compute the elapsed time by 
recording the time of entry and the time of exist. 
II. Sampling: The sampling method, used in Gprof, 
samples the value of the program counter at some 
interval. The execution time of a routine is 
estimated by weighing its sample counts. The 
more a routine is sampled, the longer its executing 
time should be. 
III. Counting instructions: This method infers 
execution time of a routine by counting the 
instruction executed within the routine. This 
method is used in FLAT [10]. Moduletracer take 
the first approach which measures the execution 
time. However, it is impossible to gather accurate 
elapsed time for a routine in a time-sharing 
system, as the routine can be preempted by the 
operating system to execute another process. 
Thus, the elapsed time measured by Moduletracer 
would contain the execution time from other 
processes. This is a general problem which also 
occurs to Gprof. The problem is more serious on a 
busy time-sharing system, so one should avoid 
running other programs for profiling the execution 
time. 
 
In order to gather the elapsed time of routines, we 
insert probes immediately before and immediately after 
each routine call. Unfortunately, this method has one 
disadvantage: the additional probes affect the accuracy of 
measuring elapsed time of routines. For example, in the 
right of the Figure 8 is the result after adding probes to 
each routine call in the left of the Figure 8. In the main 
function, we use two probes, Function_entry() and 
Function_exit(), to measure the total execution time of A. 
The total execution time of A includes the execution time 
of its body and the additional probes instrumented for 
evaluating B. These additional probes can lead us 
overestimate the total execution time of Function A.  
 
 
Figure 8.  A simple program without and with probes. 
4.4. Level Measuring 
We developed a software technique, called level 
measuring, to overcome the overestimation problem. 
This technique uses the CCT retrieved from path 
profiling to guide the whole process of execution time 
profiling. The level information for the program with 
CCT shown on Figure 9(a) can be generated by the back-
end of Moduletracer as shown on Figure 10(a). With the 
level information, Moduletracer profiles the execution 
time for the routines on one level for a profiling run, and 
the whole profiling process requires iterative profiling 
runs to collect the execution time profiles for routines on 
all levels. For example, Moduletracer measures the 
elapsed time of A during the first run, B and C during the 
second run, and D, E, and F during the third run. 
Consequently, with level measuring, Moduletracer 
provides an effective solution to the overestimation 
problem, but certain issues remain to be addressed, which 
are discussed in the following subsections. 
4.4.1. Caller and Callee on the Same Level. Figure 9(b) 
shows the B called by C. Since B and C are on the same 
level (Level 1), the elapsed time of C measured by 
Moduletracer includes the execution time of B. This 
results in an overestimation of the elapsed time for C. 
  
function on an execution path has the same self execution 
time. Therefore, Moduletracer calculates the average 
execution time for a recursive function instead of the 
absolute execution time. 
4.4.3. Common Caller-Callee from Different Paths. 
Common caller-callee relationship on different execution 
paths may cause overestimation, as shown on Figure 
12(A). To measure the elapsed time of F on level two, 
Moduletracer inserts probes in each C, and the elapsed 
time of D is overestimated since A and D both call a 
common pattern C->F. This problem is similar to the R-
>R pattern in previous section, but if we choose to ignore 
the pattern, C->F, we will lose the elapsed time of F. We 
resolve this overestimation problem by delay measuring, 
as illustrated on Figure 12(B). Originally, Moduletracer 
measures the elapsed time of F on level two and four. 
Now, with delay measuring, it delays the measuring 
process of F on level two to the level four. However, in 
doing this, we will lose the calling context information of 
F on level two. We cannot measure the elapsed time of F 
on the execution path, A->C->F. We can derive the 
average execution time of F for the entire application. 
 
 
Figure 12. Two examples of delay measuring. 
On the execution time profiling used in Gprof, this is 
the same problem which they suffer [17]. Figure 12(C) 
shows a different case. C marked by number one calls F 
and G while C marked by number two only calls F. We 
still merge these two subtrees together for generating 
level information. Figure 12(D) shows the result after 
merging. Level measuring helps us eliminate the 
additional probe overhead. With a high-resolution clock 
(for example, PowerPC’s clock is accurate to 
nanosecond), Moduletracer can acquire high-resolution 
timestamps and derive accurate execution time profiles 
via level measuring. Besides, Moduletracer profiles an 
application with software mechanism (level measuring), 
so that it works for various architectures under many 
operating systems. Therefore, Moduletracer is one tool 
that we can use to profile applications across multiple 
platforms. 
4.5. Kernel Space and User Space 
An important aspect for the trace generator is the 
speed of the trace storage, which can easily become the 
bottleneck of a trace generator. Unlike applications 
running in the user space, an application running in the 
kernel space often has timing constraints. The additional 
overhead caused by a profiling/tracing tool can affect the 
execution flow of an application. The storage for storing 
traces should be as fast as possible and provide a 
sufficient amount of storage space. 
 
 
Figure 13. The overview of the architecture of Moduletracer 
in the kernel space. 
Moduletracer provides two sets of trace handlers. One 
is for user-space applications and the other is for kernel-
space applications. These two sets of trace handlers have 
the same functionalities. The only difference is the 
mechanism of storing traces. For programs running in the 
kernel space, Moduletracer uses the High-Speed Data 
Relay Filesystem (Relayfs) [18] to store the traces. The 
main idea behind the relayfs is that each data flow is put 
into a separate channel and each channel is a file in 
memory. The channel consists of sub-buffers which are a 
set of small memory blocks. Figure 13 gives an overview 
of the architecture of Moduletracer in the kernel space, 
which is similar to Linux Trace Toolkit (LTT) [19]. 
When a trace handler is hit, the trace handler temporarily 
stores the trace records via the Virtual File System. When 
the sub-buffer boundaries are crossed, the daemon is 
notified and starts to transfer the file from relayfs to 
physical file system. In the user space, applications 
usually have less timing constraints, so Moduletracer 
directly stores traces to a physical file system, e.g. a disk. 
5. Results and Validation 
In this section, we compare the hot paths generated by 
Moduletracer and VTune. In our experiments, we used 
the programs from SPEC CPU 2000 benchmarks and 
lossless JPEG image compression package. These 
programs were compiled with the GNU GCC and were 
linked with the trace handlers together. After collecting 
traces, the back-end of Moduletracer decodes the 
addresses in traces to symbols by using the Binary File 
  
[7]  C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. 
Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood. PIN: 
Building Customized Program Analysis Tools with 
Dynamic Instrumentation, In Proceedings of the 2005 
ACM SIGPLAN Conference on Programming Language 
Design and Implementation, pp.190-200, 2005. 
[8]   G. Ammons, T. Ball, and J. R. Larus. Exploiting Hardware 
Performance Counters with Flow and Context Sensitive 
Profiling, SIGPLAN Not., vol.32, no.5, pp.85-96, 1997. 
[9]  R. P. Wilson, R. S. French, C. S. Wilson, S. P. 
Amarasinghe, J. M. Anderson, S. W. K. Tjiang, S.-W. 
Liao, C.-W. Tseng, M. W. Hall, M. S. Lam, and J. L. 
Hennessy. SUIF: An Infrastructure for Research on 
Parallelizing and Optimizing Compilers, SIGPLAN Not., 
vol.29, no.12, pp.31-37, 1994. 
[10] D. C. Suresh, W. A. Najjar, F. Vahid, J. R. Villarreal, and 
G. Stitt. Profiling Tools for Hardware/Software 
Partitioning of Embedded Applications, In Proceedings of 
the 2003 ACM SIGPLAN Conference on Language, 
Compiler, and Tool for Embedded Systems, pp.189-198, 
2003. 
[11] Perfsuite, an Accessible, Open Source Performance 
Analysis Environment for Linux, 
http://perfsuite.ncsa.uiuc.edu/. 
[12]  OProfile, http://oprofile.sourceforge.net/. 
[13] M. Arnold, and B. G. Ryder. A Framework for Reducing 
the Cost of Instrumented Code, SIGPLAN Not., vol.36, 
no.5, pp.168-179, 2001. 
[14] M. T. Heath, and J. A. Etheridge. Visualizing the 
Performance of Parallel Programs, IEEE Softw., vol.8, 
no.5, pp.29-39, 1991. 
[15]  Tutorials for Understanding How to Use Graphvize, 
http://www.graphviz.org/. 
[16] G. C. Necula, S. McPeak, S. P. Rahul, and W. Weimer. 
Cil: Intermediate Language and Tools for Analysis and 
Transformation of C Programs, In Proceedings of the 11th 
International Conference on Compiler Construction, pp. 
213-228, 2002. 
[17] N. Froyd, J. Mellor-Crummey, and R. Fowler. Low-
Overhead Call Path Profiling of Unmodified, Optimized 
Code, In Proceedings of the 19th Annual international 
Conference on Supercomputing, pp.81-90, 2005. 
[18]  High-Speed Data Relay Filesystem, 
http://www.opersys.com/relayfs/. 
[19] K. Yaghmour, and M. R. Dagenais. Measuring and 
Characterizing System Behavior Using Kernel-Level 
Event Logging, In Proceedings of the Annual Conference 
on USENIX Annual Technical Conference, pp.2-2, 2000. 
[20]  PAPI, http://icl.cs.utk.edu/papi/. 
 
 
  
the simulation results over the TPC-C benchmark. 
Finally, Section 6 states the conclusion. 
2. Related Work 
Disk cache can improve I/O throughput and avoids 
repeated physical I/Os for a storage server [1, 5, 9-11]. 
Smith [1] has described and evaluated the performance of 
adding a disk cache in storage system. Some work [9, 12] 
has proposed that cache system consists of two-level 
caches using volatile and non-volatile memory devices to 
enhance the cache capacity and cache hit-ratio. Another 
work [10] showed that a storage system would achieve 
better performance if the first level cache is designated 
with low set-associativity and a small block size while 
the second level cache is designated with fully-
associativity and a large block size. 
Various prefetching techniques have been proposed 
[1-7, 13-15]. Some studies showed that how to predict 
future file accesses via the hints from applications [4, 13] 
or the access patterns from the past [5-7]. Lei and 
Duchamp’s work [7] builds access trees which capture 
dependencies between referenced files and prefetches 
files according the access trees. Tran and Reed’s work 
[14] builds time series models to predict temporal access 
patterns, and then prefetches I/O requests during 
computation intervals to hide I/O latency. Vellanki and 
Chervenak’s work [6] uses a probability tree to record 
past access patterns to predict future accesses and 
analyzes the cost-benefit of prefetching.  
However, aggressive prefetch can cause problems. 
Methods [6, 14] pay attention to figure out the optimal 
amount of prefetching, but the management of tree 
structures in their algorithm is both time and space 
consuming. The adaptive prefetch scheme in this work 
we proposed dynamically finds an appropriate prefetch 
size for a disk volume with little CPU and memory 
overhead. 
3. Two-Level Disk Cache Management 
This section describes the two-level caching 
techniques which we developed to enhance the 
performance of QSS. Cache hierarchy [11] is commonly 
seen in the design of a modern processor. The state-of-
the-art processors today usually have multiple levels of 
caches in one chip. The first level cache (L1) is the 
fastest and always operates checking, the second (L2) is 
relatively slow but has more capacity to cache data and 
operates checking if L1 misses. With a multi-level cache 
hierarchy, the processor reduces the average memory 
access time. 
In this paper, we use the term disk cache to refer to the 
cache memory in the storage server. We define two terms: 
stripe and stripe-unit size. A stripe is a collection of data 
across an array of hard drives and the granularity stored 
in one hard drive of a stripe of the array is called the 
stripe-unit size. In addition, L1 cache to refer to the fast 
caching scheme, and L2 cache to refer to the existing 
caching mechanism in the QSS caching architecture. 
 
 
Figure 1. Data-Transfer process in two-level caching 
techniques. 
We modified the existing storage cache system 
management by adding a two-level memory hierarchy 
that indexes the cached data with an improved speed, as 
shown in Figure 1. When data requests are processed, the 
L1 cache is first searched and results in one of the two 
possible scenarios: (1) the data is a hit in the L1 cache 
and responded back to the client immediately, or (2) the 
data is a miss in the L1 cache and the system has to do 
the regular data-transfer path: striping, hashing, and 
indexing before responding the data to the client. 
Based on our performance profile of the data-transfer 
process paths, we found that striping and hashing 
together consume up to 88% of processor time in a 
benchmark which mostly access data from the disk cache 
in existing caching mechanism (L2 cache) in the QSS. 
During the benchmark, the processor was 100% busy, 
and thus it seemed obvious that the existing 
implementation of disk cache was the performance 
bottleneck. Therefore, the L1 cache is proposed in order 
to reduce disk cache access time and processing overhead 
of the storage controller. 
4. Prefetch Mechanisms 
Prefetch is another effective technique for improving 
I/O performance. Prefetch asynchronous I/Os amortize 
disk seek time and rotational delays over large sequential 
reads. The common technique for prefetch is sequential 
prefetch of the following blocks. Others, such as adaptive 
prefetches [4, 5, 13, 14] have been proposed to prefetch 
blocks via heuristic hints and access patterns. Some 
  
cleared, if it has been actually referenced after the 
prefetch. The state machine has a set of five states. Each 
state is associated with a prefetch policy and two 
thresholds for cache-hit ratio and prefetch-hit ratio, as 
shown in Table 1. The term prefetch-hit ratio is defined 
as the ratio between the count of prefetch-hits and that of 
cache-hits. The input set for the state machine is based on 
the cache-hit ratio and prefetch-hit ratio. 
 
Table 1.  Thresholds and polices to adaptive prefetch state 
machine. 
  Threshold& 
polices 
States 
Cache 
hit rate 
Prefetch 
hit rate Prefetch Policy 
A 50% 20% Prefetch 1 stripe 
B 60% 50% Prefetch 2 stripe 
C 75% 75% Prefetch 4 stripe 
D 85% 85% Prefetch 8 stripe 
E 95% 85% Prefetch 16 stripe 
 
The finite-state machine modeling adaptive 
prefetching is defined as a sextuple ( , , , , , )S So δ ωΣ Γ , 
where: 
Σ ={HH, HL, LH, LL}. (Input alphabets) 
HH = high cache-hit rate, high prefetch-hit rate 
HL = high cache-hit rate, low prefetch-hit rate 
LH= low cache-hit rate, high prefetch-hit rate 
LL = low cache-hit rate, low prefetch-hit rate 
Γ = S . (output alphabet refers to next state) 
S = {A, B, C, D, E}. (set of states) 
So = {A}. (initial state) 
: S Sδ × Σ → (state-transition function) 
: Sω × Σ → Γ (output function) 
 
For input alphabets, Σ , high cache-hit rate means the 
cache-hit rate is over the cache-hit threshold; high 
prefetch-hit rate means the prefetch-hit rate is over the 
prefetch-hit threshold. Figure 2 shows the state diagram 
for our adaptive prefetch strategy. The initial state is A, 
which has the threshold of cache-hit ratio set to 50% and 
the threshold of prefetch-hit ratio set to 20%. A state 
transition may happen only when a cache miss occurs 
over the width of a sliding window. When a state 
transition happens, it resets the sliding window so that 
the next state transition will not happen again until the 
sliding window is full. The values shown in Table 1 are 
carefully selected based on our cost evaluation models 
stated in equation (1). They can be fine tuned by the 
users to achieve better results for their applications.  
We further explain the scenarios of state diagram 
shown in Figure 2. In state A, the input HL keeps state 
machine stay in A, policy need not be changed because 
it’s quite beneficial in this state. Otherwise, it transits to 
state B and prefetches more. If cache hit and prefetch hit 
rates are also low in state A, transit to state B may cover 
the working-set with a little overhead of prefetching. 
State B is much the same as state A, except that LL will 
return to state A for overestimate. Sometimes, it may 
repeat between A and B states, but we can solve this 
problem by adding counters to stop or increase the 
monitor period. State C is a transition stage to higher 
aggressively prefetching, transiting to state D if both 
cache hit and prefetch hit are high. In state E, the prefetch 
size is the largest, and cost of prefetching can be very 
expensive. If cache-hit rate becomes low, it returns to 
state C not state D for conservative estimation. We 
design a heuristic scheme to determine a better prefetch 
strategy adaptive to short transactions or streaming 
transactions. Based on the previous successful prefetch 
history, the methodology emphasizes on the 
aggressiveness of prefetch in terms of the amount of 
prefetch blocks, rather than what to be prefetched for 
future uses. Potentially, one can combine these two 
methods to form a better prefetch strategy, and this is to 
be explored in our future work. 
 
 
Figure 2. State machine of adaptive prefetch. 
5. Simulation Results 
We list the specification of target storage system and 
the configuration in our simulation environment. And, we 
evaluate the performance of two-level cache and adaptive 
prefetching via trace-driven simulation. We discuss our 
traces, simulation models, and present the simulation 
results. Finally, we validate our simulation results with 
profiles measured from the server. 
5.1. The Target System Platform 
QSS is a mid-range direct-attached storage server 
which supports RAID5 technologies and fiber channel 
connections to the host computers. The embedded 
processor on the system runs RAID and cache software 
on top of the Linux operating system, along with the 
network and disk controller drivers to handle the entire 
operation of the storage system. The design of QSS 
represents a typical design of storage server in its class, 
and thus our performance evaluation and proposed 
enhancement techniques are also applicable to many 
  
sequential prefetching 16 stripes always had the lowest 
miss rates, but its performance was not the best. 
 
 
Figure 3. Comparisons of miss rates over TPC-C. 
 
Figure 4. Comparisons of the miss rates of prefetch 
strategies over TPC-C. 
 
Figure 5. Breakdowns of CPU execution time. 
 
Table 5 lists TPS and average response time for TPC-
C traces. In the last row, it shows that the speedups of 
adaptive prefetch over the average performance of 
sequential prefetch range from 9% to 20% in terms of 
TPS, and 12% to 29% in terms of average response time, 
where the Sequential Average series was derived by 
averaging the TPS of the five fixed-size sequential 
prefetch strategies. 
5.4. Validation 
We used measured data to validate our simulation 
results. The two-level caching scheme were implemented 
and profiled in the QSS by our colleagues [19]. Our 
performance measurement showed the following: 
 
I. The performance of the QSS is pushed to its 
physical I/O limit in the case where requests are 
hit in the cache. 
II. The enhancement achieves a 5.51-fold increase of 
throughput by generating requests internally rather 
than from clients through fiber channel. 
III. The performance overhead added by the L1 cache 
is within about 4.5% of the original cache search 
mechanism and requires very little memory space. 
 
We found that the speedup of adding our two-level 
caching scheme obtained by our simulation model is 5.7 
described in the [20], while we got a very close result 
from the real platform which showed the speedup of 5.51 
times [19]. 
6. Conclusions 
In this paper, we studied the caching and prefetching 
mechanisms based on a production storage server. Two 
schemes, a two-level caching scheme and an adaptive 
prefetch strategy, were developed to improve the speed 
of caching and efficiency of prefetching. We evaluated 
the performance of our schemes and compared them with 
conventional prefetch techniques.  Our two-level caching 
improved average cache access time and reduced 
processor execution time significantly for cache hits. The 
optimal prefetch size for a sequential strategy depends on 
the application and is difficult to choose in advance. Our 
strategy does not fix the prefetch size and thus is suitable 
for a wider range of applications.  
7. Acknowledgements 
This work was supported in part by a grant from the 
Quanta Computer Inc., a grant from the National Science 
Council (95C2443-2), and a grant from Excellent 
Research Projects of National Taiwan University 
(96R0062-AE00-07). 
 
 
 
 
 
Zero-Buffer Inter-Core Process Communication
Protocol for Heterogeneous Multi-core Platforms
Yu-Hsien Lin Chiaheng Tu Chi-Sheng Shih Shih-Hao Hung
Department of Computer Science and Information Engineering
Graduate Institute of Networking and Multimedia
National Taiwan University, Taipei, Taiwan 106
cshih@csie.ntu.edu.tw
Abstract—Executing functional components in pipeline on
heterogeneous multi-core platforms can greatly improve the
parallelism but require great amount of data communication
among processes and threads. Our studies showed that existing
inter-process/thread communication protocols consist of many
unnecessary memory copies and prolong the execution of the
applications on heterogeneous multi-core platforms. Our exper-
iments also showed that the communication overhead accounts
40% to 50% of total execution time in average for H.264 video
decoding applications. In this paper, we present the design and
implementation of zero-buffer inter-core process communication
protocol, named NTU ICPC, to shorten communication overhead
for pipeline executed applications on heterogeneous multi-core
platforms.
NTU ICPC uses polling-base mail notification to avoid un-
necessary context switches, and designs a memory subsystem
to manage the input and output data between the senders
and receivers. The protocol was implemented and evaluated
on heterogeneous multi-core platform for several use scenario
including H.264 encoding process. The evaluation results show
that the communication overhead on sender side is independent
of the data size and that on receiver side is greatly shortened,
compared to several inter-process communication (IPC) protocols
including mailbox, message queue, and shared memory. When
encoding H.264 video clips, the encoding frame rates increase for
more than 30%.
Index Terms—Heterogeneous Multi-core Processor, Operating
Systems, Distributed Systems.
I. INTRODUCTION
With multi-core platforms, there are many means to in-
crease system throughput. Two typical approaches are data
parallelism and functional parallelism. Data parallelism is
usually adapted in homogeneous multi-core platforms for data-
intensive applications. In this approach, the input data are
divided into numbers of equal-sized data subset and the data
subsets are then processed by different cores in parallel. Image
rendering for video/image display is one typical application for
this approach. Nevertheless, functional parallelism is usually
adapted in heterogeneous multi-core platforms for computation
intensive applications. In this approach, one application is
decomposed into several functions which are executed in
pipelined manner on different cores to complete the work.
Video encoding/decoding is one typical application for this
approach.
In this work, we are interested in performance enhance-
ment for pipelined executed applications on heterogeneous
multi-core platforms. In comparison to single-core processor
architecture, multi-core architecture greatly enhances the paral-
lelism, increases system throughput, and shortens the response
time. When the pipelined schedules are carefully designed,
the major trade-off and performance bottleneck become the
communication among cores. Figure 1 uses the pipelined
schedule of a simplified H.264 decoding flow as an example
to illustrate. In this simplified decoding flow, there are four
subtasks to decode one frame: PD, deQ, IDCT and MC. The
notation “i-PD” represents the PD subtask of i-th frame. For the
sake of clarity, each subtask is mapped onto a different core,
i.e. core A for PD, core B for deQ, core C for IDCT, and core D
for MC. As we can see in Figure 1, when core B starts subtask
deQ for the 1st frame, core A is ready to process subtask PD
for the 2nd frame; therefore the system can decode multiple
frames concurrently. Figure 1 also shows that the output of
Fig. 1. Pipelined Processing in Multi-Core Architecture
the subtask on one core is the input of another subtask on
another core. Therefore, the data should be transferred from
one core to another one. We call this kind of communication
the Inter-Core Process Communication, shorted by ICPC.
When a periodic task is divided into several subtasks and
the subtasks may allocate to different core/processor in the
system, there are frequently inter-core/inter-process communi-
cation among the subtasks. Take H.264 encoding process as
an example. Table I lists the communication overhead within
the total execution time. When message queue is used to
exchange data among the subtasks, more than 50% of the
execution time are used for communication; when mailbox or
share memory are used, more than 45% of the execution time
are used for communication. The above observations show
that reducing the communication overhead can greatly improve
1
system in one single chip. On SoC, the processing elements
are located in the same chip and the physical distance between
processing elements are shortened which introduces higher
performance and lower power consumption. For the sake of
clarity, we list the important features for heterogeneous multi-
core platform in the following.
1) Each core has its own working space and
2) Each core can access others’ working space.
Since the memories are on the same chip, the cores naturally
have the ability to access each of them in an SoC platform.
Based on the observations shown above, an effective and
efficient inter-core process communication protocols should
have the following desired features.
1) Autonomy and parallelism: senders and receivers
should be able to read from and write to their buffers
simultaneously.
2) Flexibility: the protocol should support different types of
communication pattern such as “multi-senders to single
receiver” and “single sender to multi-receivers”,
3) Asynchronous communication: the protocol should
support asynchronous communication to reduce the
blocking/idle time,
4) High throughput: Avoid unnecessary memory copy and
inter-core interrupts to improve performance, and
5) High portability: operating system and processor depen-
dent functions should be avoided to increase portability
of the protocol.
NTU ICPC protocol will provide the above desired features
and we will discuss its design and implementation in the
following sections.
III. INTER-CORE PROCESS COMMUNICATION PROTOCOL
DESIGN
Traditional inter-process communication protocols use in-
terrupts to ensure the timeliness and several memory copies
to ensure the autonomy for sending and receiving processes.
Interrupt mechanism is effective and efficient when data ex-
change occurs randomly. As shown in Figure 1, the execution
order of the functional components have a fixed sequence
and the data exchange among the processes occur periodically
between a fixed pair of sender and receiver. When the inter-
core communication frequently occurs with a known sequence,
we may avoid using interrupts and memory copy to eliminate
context switches and unnecessary memory copy operations.
Figure 2 illustrates the call flow for NTU ICPC protocol.
The sending process, named sender for short, sends the receiv-
ing process a data ready mail, including the receiver ID and
pointer to the data buffer. When there is no error for sending
the mail, the sender can continue its work without waiting for
the mail being read and data being retrieved. On the other
hand, the receiving process, named receiver for short, checks
its mailbox when expecting a mail. When there is any unread
mails in the mailbox, it requests the data to be copied into its
local buffer and sets the mail being read.
m a i l  " d a t a  r e a d y "  m e s s a g e
R e c e i v e rS e n d e r
c o p y  d a t a  t o  a p p l i c a t i o n  d i r e c t l y
c a h n g e  m e s s a g e  t o  " f i n i s h "
c h e c k  m a i l b o x
Fig. 2. NTU Inter-Core Process Communication Protocol
To speed up the inter-core communication and ensure
its correctness, NTU ICPC consists of three major compo-
nents: mail notification, direct data movement, and circular
buffer management. The protocol is specifically designed for
pipeline scheduled periodic processes on heterogeneous multi-
core platforms. We discuss the rationale of using the above
three mechanisms and how NTU ICPC protocol works in the
following.
A. Major components of NTU ICPC protocol
Mail notification Traditional mailbox protocol sends the
whole data in the mail to the receiving processes on behalf
of the sending process. The service copies the transmitted
data from sender’s buffer into the mailbox buffer. When the
receiving process reads the mail, the data is copied again
from mailbox buffer to receiver’s buffer. When the sender
and receiver are located in different memory domains, this
approach assures the autonomy for both parties. However, on
heterogeneous multi-core processors, the sender and receiver
are located on the same memory domain. It is not necessary
to use so many memory copies. Although mail service is
not suitable for large data size movement, it is an effective
communication service when the sender and receiver agree on
the communication in advance. The mail notification service
in NTU ICPC uses a short message to notify the receiving
processes and does not copy the data to mailbox buffers.
When the sending process sends a mail to its receivers to
notify that the data is ready, the sender continues its work after
the mail is accepted by the mail service. The mail resides in
receiver’s mailbox until the receiver checks its mailbox. After
the receiving process reads the mail, it changes the status of the
mail to read. In case of stalled mails, the sending processes
will be notified, which will be discussed later. Although the
mail will remain unread if the receiving process does not check
its mailbox, fortunately, this will not be the case in the targeted
application domains.
Circular buffer management As pointed out earlier, the
data sent by the sending process is not copied into mailbox’s
3
C o r e  A
( S e n d e r )
C o r e  B
( R e c e i v e r )
C o d e
I n p u t  B u f f e r
O u t p u t  B u f f e r
D a t a
C o d e
I n p u t  B u f f e r
O u t p u t  B u f f e r
D a t a
M a i l b o x Mailbox
(1 )
(2 ) (3 )
(4 )
(5)
( S e n d e r )
t h e  r e s u l t s  m u s t  
" W r i t e  B a c k "
t o  m e m o r y
( S e n d e r )
t h e  m a i l  m u s t  
" W r i t e  B a c k "
t o  m e m o r y
( R e c e i v e r )
" I n v a l i d a t e "  t h e  c a c h e  l i n e  
b e f o r e  r e a d i n g
( R e c e i v e r )
" I nva l i da te "  t he  
c a c h e  l i n e  b e f o r e  
t r a n s m i s s i o n
( R e c e i v e r )
t h e  r e s u l t s  m u s t  
" W r i t e  B a c k "
t o  m e m o r y
Fig. 3. Operations to Assure Cache Coherency
When the sender conducts the computation using its input
data, the results are wrote to the output buffer. To ensure that
the results are stored in the output buffer rather than in sender’s
cache, a ’write back’ is issued. This operation is illustrated
by the dash line labeled by (1). The second case, labeled
by (2), ensures that the correct mails are sent to receiver’s
mailbox. To do so, we must make sure the mail is written
to the receiver’s mailbox. If the mailbox’s address is cached
in the sender’s cache, a “Write back” operation is need so
that the receiver can get the updated mail. The third case,
labeled by (3), ensures that the receiver receives the up-to-
date mail. When the receiver is ready to receive its input data,
an “Invalidate” operation is issued since the mailbox’s address
may have been cached in receiver’s cache. It forces the receiver
to get mail from the memory directly instead of fetching out
of date mail from cache. The forth case, labeled by (4) in the
figure, ensures that the receiver copies the correct data into its
buffer. After getting the “data ready” mail, the receiver starts
to copy data from sender’s output buffer to its input buffer.
Likewise, before copying data, we should “Invalidate” the
cache line corresponding to sender’s output buffer in receiver’s
cache to ensure the receiver will get the updated data. The
fifth case, labeled by (5), ensures that the receiver writes its
computation results into the output buffer, not its local cache.
This step is equal to the first step described earlier.
C. Comparison with traditional IPC protocols
NTU ICPC are different with traditional IPC protocols in
several perspectives including its memory management and
how the services are provided. In Table II, we compare
the major features of NTU ICPC with that of three IPC
communication protocols including message box, mailbox, and
share memory.
Traditional IPC protocols are mostly implemented as system
services to be efficient. Hence, when the service is called, an
OS trap which consists of mode switch, software interrupts
or context switch. On the other hand, NTU ICPC is a user
mode service module. Hence, while requesting the service,
there is no mode switch. Second, we compare the way that they
wait for certain events including empty buffer, full buffer, and
Mailbox MsgQ Share Memory NTU ICPC
(IPC) (IPC) (IPC) (ICPC)
Enter Protocol OS trap OS trap OS trap Function Call
(context switch) (context switch) (context switch)
Wait for Event Blocking time Blocking time Blocking time Idle Time
(context switch*2) (context switch*2) (context switch*2)
Copy to Short message Queue Management, Managed by No
Shared Buffer Divided Data, Application
Critical Section
Number of 2 to 4 2 to 4 2 to 4 1
Memory
Exit Protocol Return to Return to Return to Function Return
User Mode User Mode User Mode
(context switch) (context switch) (context switch)
Memory Fixed Size for Fixed Size for Varied Size Varied Size
Requirement Mailbox Shared Queue (User Define) According to
Transmission Data
TABLE II
SUMMARY OF THE DIFFERENCE AMONG MAILBOX, MESSAGE QUEUE,
SHARED MEMORY AND NTU ICPC
next message. In traditional IPC protocols, the waiting process
may be removed from the run queue to yield the CPU. When
the message arrives, the process is switched back to the run
queue again. This behavior will introduce certain overheads
such as at least two context switches. On the other hand, NTU
ICPC uses busy waiting to wait for the event and therefore
reduces these overheads. Although busy waiting may lead to
infinite wait or message lost, the waiting only occurs when the
receiver is ready to receive messages before the sender sends.
Such waiting time is common for all protocols because the
transmission data is not ready.
In terms of shared buffer, traditional asynchronous IPC pro-
tocols copy the transmission data should be copied to shared
buffer. Moving data into shared buffer consist of memory
copy (which will be discussed later), matadata management,
critical section, etc. In NTU ICPC protocol, the transmission
data are retrieved directly from circular buffer and no buffers
are shared between senders and receivers. Hence, there is no
overhead in this category. In traditional IPC protocols, the most
time consuming operations are moving memory data back and
forth between service buffer and requesting processes. All the
traditional IPC protocols requires 2 to 4 memory movement for
each request, depending on implementation. On the other hand,
there is only one memory copy from circular buffer to input
buffer directly in NTU ICPC. Last, the memory requirement
for different protocols vary. For mailbox and message queue
protocols, they only require a fixed size of memory to be used
for the mailbox and the queue. For share memory protocol, the
shared memory is often allocated by the requesting processes.
The size is the maximum transmission data size at least. For
our protocol, the memory requirement is more than other ones;
we need at least twice the sum of all transmission data sizes
in the system to support circular buffer mechanism.
In summary, NTU ICPC protocol eliminates the chance
of context switches and eliminates unnecessary memory copy
operations to improve the efficiency of the communication.
How these designs affect the performance will be evaluated in
Section V.
IV. SOFTWARE ARCHITECTURE AND IMPLEMENTATION
Traditional IPC protocols for single-core system are mostly
implemented as a system service of the operating system. Such
5
C o r e  A Core B
C o d e
I n p u t  B u f f e r
O u t p u t  B u f f e r
/ I n p u t  B u f f e r
D a t a
C o d e
O u t p u t  B u f f e r
D a t a
R e c e i v e r
S u b t a s k
S e n d e r
S u b t a s k
Fig. 6. Working Spaces Reside on the Same Memory Region
P l a t f o r m  d e p e n d e n tO S  d e p e n d e n t
M a i l
S u b s y s t e m
I O  B u f f e r
S u b s y s t e m
Mu l t i -
R e c e i v e
Mu l t i -
S e n dSend R e c e i v e
A P I
P r o t o c o l  L a y e r
M a i n  F u n c t i o n  L a y e r
P o r t i n g  L a y e r
Fig. 7. ICPC Service Module Software Architecture
layer, main function layer, and protocol layer. An ICPC proto-
col may use certain functions which are platform or operating
system dependent. We include all those platform or operating
system dependent functions in porting layer to provide a
uniform interface to upper layers. Main function layer consists
of I/O buffer subsystem and mailbox subsystem. These two
subsystems are responsible for direct data movement, circular
buffer and mail notification mechanism used in NTU ICPC
protocol. NTU ICPC protocol is implemented in protocol layer.
The protocol layer provides application interfaces (API) for
the processes to communicate with each other. The following
describe each layer in detail.
Protocol Layer Based on the protocol described in Sec-
tion III, protocol layer provides all the functions of NTU
ICPC protocol. It includes the connection configuration, send,
receive, sendtomany, and receivefrommany. The
connection configuration pairs the senders and receivers for
data exchange. The set of senders and receivers is not supposed
to be changed after any data exchange starts. Changing the
configuration may lost the unread data on the receiver side,
which needs to be retransmitted. How to use these functions
to complete the inter-core communication will be described at
the end of this section.
Main Function Layer - IO Buffer Subsystem The IO
buffer subsystem is responsible for managing input and out-
put buffers. When the connection is set, it allocates input
and output buffers for all the requests and manage them
according to the connection topology information specified by
the configuration. (We will discuss the connection topology
later.) Allocating input and output buffers follows the following
principles:
1) The input and output buffers for each process should be
allocated in its accessible working space,
2) If the process working spaces are located in the same
memory, the sender’s output buffer and the receiver’s
input buffer will be set as the same one so as to achieve
zero-copy communication,
3) The size of input buffer is equal to the number of its
senders and the size of each input buffer is the maximum
size of transmission data, and
4) Each process have at least two output buffers to support
circular buffer and the size of each output buffer is the
size of maximum transmission data.
In the configuration phase, the subsystem uses the “memory
map” function provided by porting layer to map the allocated
input and output buffers’ physical address to the caller’s
address space. So that each process has the access permission
to access its input and output buffers. After the configuration,
the IO buffer subsystem keeps the address information of all
input and output buffers. When a data exchange request arrives,
the sender’s output buffer and receiver’s input buffer are known
and ICPC service module can transfer data between them
directly. To support circular buffer, the IO buffer subsystem
also provides a function to advance the output buffer pointer
when function send() is called.
Main Function Layer - Mailbox Subsystem Mailbox
subsystem supports the mailing services to send and receive
mails and follows the following principles to allocate mailbox.
1) Each process’ mailbox is allocated in its own working
space,
2) for each receiver, one mailbox is allocate for each sender,
and
3) length of each mailbox entry is 32 bits.
In order to reduce the overhead, a single word (32 bits)
message format is used so as to send the mail in one store
instruction.
Porting Layer Porting layer consists of the functions that
are platform or operating system dependent, which is memory
management. In NTU ICPC service module, it is critical to
access and manage the memory space directly for the sake of
efficiency and correctness. On some platforms, the operating
system’s memory management unit (MMU) may use virtual
memory mechanism to manage the memory space. It means
that the service module cannot access the memory space
directly by physical address or no permission to access even we
have the virtual address mapped. On such platforms, memory
mapping service provided by MMU will be wrapped to provide
the memory management service for the upper layer. Using this
7
message ranges from 4 bytes to 4 kilo bytes1. Figure 10
illustrates the task flow of an H.264 encoding process. ME and
MC functions covered in the shaded rectangle may take up to
90% [10] of the total execution time and are usually executed
on DSP core to shorten the execution time and increase the
encoding rate. The second set of experiments emulates the
above application workload. When emulating H.264 encoding
D S P
E n t r o p y
E n c o d e r
M E M C I D C T
d e Q
QD C T-
+
F r a m e  N - 1
F r a m e  N
C o m p r e s s e d
F r a m e
H . 2 6 4  E n c o d i n g
m a c r o
b l o c k
b l o c k
Fig. 10. Task partition for H.264 Encoding Partition
process, we measure the average number of encoded frames
per second, denoted by fps, to represent the throughput of the
application.
Table III lists the parameters for H.264 encoding process.
We conduct the experiments for different resolution of the
video clips, ranging from the most popular one to the one
for future use. The size of macro blocks is set to 16 bytes by
16 bytes and Chroma sampling ratio is 4 : 2 : 0. The size
of message blocks are set to 4x4 bytes. (When the size of
messages are equal to the size of macro blocks, the message
queue and mailbox may behavior similar to share memory.)
Parameter Value
Macro Block Size 16 * 16 bytes
Block Size 4 * 4 bytes
Chroma Subsampling 4 : 2 : 0
Video Resolution 176 * 144 (QCIF)
320 * 240 (QVGA)
352 * 288 (CIF)
640 * 480 (VGA)
TABLE III
PARAMETER CONFIGURATION OF EXPERIMENT 2
B. Evaluation Results
Figure 11 and 12 show the measured time overhead in
the use scenario that the sending process sends the message
before the receiving process is ready. Figure 11 illustrates the
time overhead on sending processing. The time overhead for
three IPC protocol increase when the message size increases.
It is clear that the increase is caused by the memory copy
operations conducted for the sending process. In particular,
shared memory protocol requires least number of memory
copies and bookkeeping information for the messages and
1The maximum size of the message is limited by the size of internal memory
on the processor.
leads to better performance. On the other hand, the mailbox
protocol has to manage the messages in the mailboxes to allow
random access and leads to greater overhead. In the NTU
 0
 100
 200
 300
 400
 500
 600
 700
 800
 1  4  16  64  256  1024  4096  16384
Ti
m
e 
(µs
)
Data Size (bytes)
ARM Send Timing Overhead for Sender First Case
Early Time
Mailbox
Message Queue
Share Memory
NTU ICPC
Fig. 11. ARM “Send” Timing Overhead for Sender First Case
ICPC protocol, the sending process only sends a short message
to notify the receiving process. There is no memory copy
operation when the message sending function is called. When
NTU ICPC is used in the applications, the sending process will
not be blocked to wait for the completion of memory copy.
As shown in the figure, the time overhead for NTU ICPC is
constant despite of the increasing message size.
Figure 12 shows the time overhead on the receiving process
when the message is sent before the receiving process is
ready. In this case, there is no blocking time on the receiving
process for all the evaluated protocol. The time overhead on
receiving process for the three IPC protocols are similar to
that on sending process: The time overhead increases when
the size of transmitted data increases. The time overhead
for NTU ICPC protocol is only a fraction of that for the
other protocols. The drive data move mechanism eliminates
the unnecessary memory copy operations and greatly improve
the performance. Note that circular buffer mechanism ensures
memory consistency and efficiency.
 0
 100
 200
 300
 400
 500
 600
 700
 800
 1  4  16  64  256  1024  4096  16384
Ti
m
e 
(µs
)
Data Size (bytes)
DSP Receive Timing Overhead for Sender First Case
Mailbox
Message Queue
Share Memory
NTU ICPC
Fig. 12. DSP “Receive” Timing Overhead for Sender First Case
9
An Automatic Compiler Optimizations Selection Framework for Embedded 
Applications 
Shih-Hao Hung12, Chia-Heng Tu1, Huang-Sen Lin2, Chi-Meng Chen1 
1Graduate Institute of Networking and Multimedia 
2Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, 106 Taiwan 
{hungsh, d94944008, r94922102 and r94944018}@csie.ntu.edu.tw 
 
 
Abstract 
 
Optimizing compilers provide users with compiler 
options to maximize program performance. The selection 
of compiler options is important as the resulted 
performance can vary significantly. The best combination 
of compiler options is not only dependent on the program 
itself, but it also is highly related to the configuration of the 
system and the architecture of the processor that the 
program runs on. The determination of the best 
combination of compiler options is very complicated, as its 
complexity grows exponentially with the number of the 
optimization options the compiler offers. Many previous 
work attempts to shorten the search time by reducing the 
complexity of the problem. However, most of them focus 
on computational intensive applications, which run with 
little or no invocation of kernel functions and device 
input/output activities, which often dominate system 
performance in specific embedded environment, such as 
network appliance.  
This paper aims at system-wide compiler optimizations 
selection for embedded applications. We proposed an 
automated framework to judiciously select the compiler 
options not only for the control software in the user space 
but also for the associated kernel functions which perform 
the I/O operations for an embedded application. For this 
framework, we implemented compiler optimization 
selection algorithms and evaluated its efficiencies with and 
without performance monitoring hardware support. We 
argue that our framework is a platform-independent and 
system-level compiler options selection framework. Our 
experience in optimizing the performance of the embedded 
application on a production storage appliance show that an 
I/O-intensive application composed by various kernel 
modules device drivers under Linux can be optimized 
effectively and systematically. 
1. INTRODUCTION 
 
Modern compilers provide various optimization options 
for users to choose to obtain higher program performance. 
Proper selection of optimization options is not easy, 
especially for average users who do not have in-depth 
understanding of the compiler options, the effects of the 
optimizations and the interactions among the options. 
Therefore, most compliers offer a few optimization levels, 
and each optimization level performs a certain code 
analyses to determine the settings for a fixed set of 
optimization options. For example, three optimization 
levels, -O1, -O2 and -O3, are provided by the GNU 
Compiler Collection (GCC) [1]. To obtain the best 
performance, a user usually applies the highest 
optimization level, –O3, to have the compiler perform the 
most extensive code analysis and expects the compile-
generated code to deliver the highest performance.  
Studies have been made on compiler option searching 
algorithms to select a set of compiler options for optimal 
performance. However, most of these works were 
developed to optimize user-space programs and compute-
intensive codes [2-10], rather than system software 
(control-plane codes). As Today’s embedded applications 
often handle input/output (I/O) activities with concurrent 
system operations, the overall system performance may be 
dominated by the device-dependent (driver) operations and 
the operating system (kernel) functions. For example, a 
control software usually comes with user codes and system 
software to form a complete embedded application. Thus, 
to optimize the embedded application, both layers of 
software need to be optimized as a whole.  
In this paper, we propose to optimize embedded 
applications by setting compiler optimization options 
properly via a framework which extends the previous 
compiler options search methods to cover multiple layers 
of software. The framework contains tools which help a 
developer conduct a heuristic search to find a set of 
compiler options for the entire application to perform well: 
During the run time of the targeted application, a profiling 
tool may be deployed by the framework to record detailed 
performance metrics; the framework then adopts a compiler 
options selection method to decide on the optimization 
options for the compiler to produce a new binary code for 
the following execution. The procedure is repeated 
(MB/sec) for a network-attached storage server. After the 
experiments, in step 8 and 9, the implemented algorithm 
determines proper compiler options for the next run if it is 
necessary. Otherwise, a suitable options combination has 
been found. This automated process is mostly done by the 
client machine by our tool except that our tool sends 
compiler options to the host machine to get the code 
recompiled by the compiler on the host machine.  
In our experiment, the host-client communications and 
data exchanges were done by NFS (Network File System) 
[12] via a private Local Area Network (LAN). NFS allows 
users from one computer to access files resident in 
computers over the network as if the network devices were 
attached to its local disks.  
To tune the application in user space, we have written a 
simple script to run the program, obtain the performance 
data and decide whether to terminate the selection process 
based on current algorithm.  Different from user-space 
applications, for kernel-space applications (e.g., operating 
systems and kernel modules), selecting a good software 
version toward better performance requires rebooting 
system and acquires different software version (kernel 
image) across the LAN from the host machine. To facilitate 
this process, we setup an environment to conform PXE 
(Pre-Execution Environment) network booting protocol 
[13]. Following the protocol, the client machine will first 
initialize BIOS, perform DHCP request and PXE broadcast 
over the network, and download the PXE environment (and 
configurations) once connected with host machine (PXE 
server). Based on the pre-programmed configurations, the 
client machine can then boot from the updated kernel 
version and gather performance data during this run. After 
that the client machine is told to reboot and perform the 
above processes until the termination condition of current 
algorithm is met. In addition, to automate the whole 
options selecting process controlled by the client machine 
after booting up, auto login console in linux and SSH login 
without password [14] are used in our framework. 
 
Client Machine
(Embedded System)
Proposed Framework for Automatic Compiler Options 
Selection of Control Software on Embedded System
Host Machine
(Server)
Private LAN
Compiled Kernel 
Image and Module
Power On
Search for 
Kernel Image
Download 
Kernel Image
Search Module
Acquired Module
Performance Analyzing
Record Performance 
Results
1
6
5
4
3
2
7
Compiler Options 
Selection Analysis8
Continue Selection Process:
Setup Compiler Options for Next Run9
10
Apply New 
Compiler Options
&
Compile Kernel 
Image and Module
Meet Termination 
Condition
Halt Found Suitable Options Combination  
Figure 1. The flow chart for optimizing an embedded Linux-based network appliance with our framework. 
 
4. COMPILER OPTIONS SELECTION 
ALGORITHM 
 
Within the automatic framework, the algorithm used to 
find suitable compiler options is important, as it affects the 
total search time and the code quality. An ideal algorithm 
results in the best performance in short time. However, it is 
difficult for existing algorithms to achieve both goals. In 
addition, some algorithms require hardware assist. Thus, 
we integrated several algorithms into our framework to 
accommodate the requirements from the users, the 
size, the power consumption or etc. In this work, we use 
RIP to measure the performance of IOPS with and without 
one optimization, Fi, in a set of options, B, which are 
denoted as MB(Fi=1) and MB(Fi=0), respectively. Further, 
B(Fi=0) expresses in a set of compiler option {F1=1, F2=1, 
… , Fi=0, …, Fn=1}, where n is the number of compiler 
options. The RIP is defined as formula: 
( 0) ( 1)( )
( 1)
B i B i
i
B i
M F M FRIP F
M F
                (1) 
This formula indicates that the RIP of compiler option, 
Fi, is the performance effects given by turning off the 
option, Fi, in a set of compiler option B. For example, 
RIPB(Fi) < 0 means MB(Fi =1) > MB(Fi=0). Namely, for the 
program, the performance improves relative to baseline 
when user turns off the ith compiler flag in option set B. 
Note that the baseline of this algorithm applies all 
optimizations. 
To avoid redundant computation in CE algorithm, we 
propose a modified CE algorithm. Experiments show no 
performance degradation while we using the modified CE 
algorithm. Moreover, the proposed modification improves 
search time by 4% relative to the original CE algorithm, 
which results from removing redundant computation of 
RIPB(Fi) in the original CE algorithm, if the RIPB(Fi) in the 
previous run has been calculated. The corresponding 
modifications in bold font are given below.  
 
Algorithm 1  Modified CE Algorithm 
Input variables: 
B: the baseline (default) configuration. 
Fi: the configuration of ith compiler options (=on or off). 
MB: performance of the baseline option combination. 
n: the number of compiler options available in the 
experiment. 
SS: search space of compiler options selection (=2n, while 
available operations for those n flags are either on or 
off). 
Output variables: 
B: represents selected compiler options set. 
Steps: 
1. Setup SS={ F1, F2, … , Fi, …, Fn} and B = {F1=1, 
F2=1, … , Fi=1, …, Fn=1}. 
2. Compile the program with baseline configuration B 
and measure the performance under baseline 
configuration, MB. 
Also, calculate RIPB(Fi) for each optimization Fi, 
where i is in SS set. Check if RIPB(Fi) has already 
computed in the step 3 from previous iteration. If 
yes, use the pre-computed value. 
3. If exist any Fi such that RIPB(Fi) < 0, then 
Let Neg={N1, N2, …, Nj} be the set of 
optimization options, where RIPB(FN1)<0, and j is 
the number of options correspond to the case. 
Elements in Neg are sorted in an increasing order, 
that is RIPB(FN1) < RIPB(FN2) < … < RIPB(FNj). 
Eliminate N1 from SS and set N1 to 0 in B. 
For each k from 2 to j, compute RIPB(FNk). If 
RIPB(FNk)<0, then remove Nk from SS and set 
Nk to 0 in B. Store the results, which might be 
used in the next iteration. 
else 
Go to step 5. (RIPB(Fi)≧ 0, for each option, Fi, in 
B) 
4. Repeat 2 and 3. When RIPB(Fi)≧ 0, for each option, Fi, 
in B, go to step 5. 
5. B represents the selected compiler option set. 
5. EXPERIMENTAL RESULTS 
 
This section demonstrates the application of the 
proposed framework on a Linux-based embedded storage 
server application. The target storage server is a production 
model from the Quanta Computer Inc., which supports 
RAID-0 disk arrays and fiber channel interfaces. The 
performance bottleneck is caused by the processor on the 
controller, which spends most of its time in executing the 
RAID-0 kernel module, facilitating disk caching, handling 
file system operations, and driving the fiber channel 
interfaces. The entire software collection poses a great 
challenge for the programmers to optimize the compiler 
options manually, as it contains many kernel modules and 
the kernel modules and drivers can execute asynchronously 
and concurrently. Thus, it makes a good target to optimize 
with our compiler optimization framework.  
We first list the experiment environment and describe 
the implementation of three heuristic algorithms. Then, we 
introduce the performance metric used to evaluate the 
storage server and our experience in optimizing the 
performance of the embedded application on a production 
storage appliance. We show that an I/O-intensive 
application composed by various kernel modules and 
device drivers under Linux can be optimized effectively 
and systematically. 
5.1 Experimental Environment 
 
Table 1 lists the settings in our experimental experiment. 
Two machines were used: host and client, referring to 
development PC and target storage server, respectively. 
Performance measurement was done on the client side with 
various performance tools such as Intel’s VTune 
Performance Analyzer [15] and our workload generator, 
quick search is needed and may be further used to improve 
application performance dynamically on-site based on the 
input. On the other hand, the CE algorithm is easy to apply 
on target system regardless of the capability of underlying 
hardware. This could be important when the target 
embedded processors are unable to provide 
sufficient/accurate performance profiles for LRM model 
due to lack of hardware performance monitoring support. 
Under the circumstance, the Modified CE algorithm 
becomes the primary choice.  
Table 2. Speedup of compiler options search time. 
 CE Modified CE LRM 
Application Performance 1 1 0.98 
Search Time (hours) 34.5 33.2 0.32 
Relative Speed 1 1.04 106.6 
 
6. CONCLUSION AND FUTURE WORK 
 
In this paper, we proposed a framework to facilitate 
performance tuning for embedded applications which may 
invoke kernel functions and device I/O operations. We 
discussed three compiler options selection methods which 
were implemented in our framework to effectively support 
various applications. We presented our results with a full 
blown storage server application. The results showed that 
the compiler options selection algorithms delivered good 
performance for our kernel-space application with different 
search time. On a platform which has hardware 
performance profiling support, the user may choose the 
faster LRM algorithm to obtain the result quickly. Without 
performance profiling support, the user can still use the 
Modified CE algorithm provided in the framework. By 
removing redundant computations, our Modified CE 
algorithm reduced the options set search time by 4% with 
respect to the original CE algorithm.  
In the future, we hope to incorporate more option 
selection algorithms and figure out ways to combine and 
apply the algorithms effectively and systematically with 
our framework. 
7. ACKNOWLEDGEMENTS 
 
This work was supported in part by a grant from the 
Quanta Computer Inc., a grant from the National Science 
Council (95C2443-2), and a grant from Excellent Research 
Projects of National Taiwan University (96R0062-AE00-
07). 
8. REFERENCES 
 
[1] GCC online documentation. 
http://gcc.gnu.org/onlinedocs/ 
[2] Pan, Z., and Eigenmann, R., "Fast, Automatic, 
Procedure-Level Performance Tuning", in PACT, 
2006, pp. 173-181. 
[3] Pan, Z., and Eigenmann, R., "Fast and Effective 
Orchestration of Compiler Optimizations for 
Automatic Performance Tuning", in CGO, 2006, pp. 
319-332. 
[4] Pan, Z., and Eigenmann, R., "Rating Compiler 
Optimizations for Automatic Performance Tuning", in 
SC, 2004, pp. 14-24. 
[5] Whaley, R.C., and Dongarra, J.J., "Automatically 
Tuned Linear Algebra Software", in SC, 1998, pp. 1-
27. 
[6] Haneda, M., Knijnenburg, P.M.W., and Wijshoff, 
H.A.G., "Generating New General Compiler 
Optimization Settings", in SC, 2005, pp. 161-168.  
[7] Kisuki, T., Knijnenburg, P.M.W., and Boyle, 
M.F.P.O., "Combined Selection of Tile Sizes and 
Unroll Factors Using Iterative Compilation", in PACT, 
2000, pp. 237-246. 
[8] Granston, E., and Holler, A., "Automatic 
Recommendation of Compiler Options", in FDDO, 
2001. 
[9] Chow, K., and Wu, Y., "Feedback-Directed Selection 
and Characterization of Compiler Optimizations", in 
FDDO, 2001. 
[10] Cavazos, J., Fursin, G., Agakov, F., Bonilla, E., 
O'Boyle, M.F.P., and Temam, O., "Rapidly Selecting 
Good Compiler Optimizations Using Performance 
Counters", in CGO, 2007, pp. 185-197. 
[11] Agakov, F., Bonilla, E., Cavazos, J., Franke, B., 
Fursin, G., Boyle, M.F.P.O., Thomson, J., Toussaint, 
M., and Williams, C.K.I., "Using Machine Learning 
to Focus Iterative Optimization", in CGO, 2006, pp. 
295-305. 
[12] Network File System (NFS). 
http://nfs.sourceforge.net/ 
[13] PXELINUX. http://syslinux.zytor.com/pxe.php 
[14] SSH login without password. 
http://www.linuxproblem.org/art_9.html 
[15] Intel VTune Performance Analyzer. 
http://www.intel.com/cd/software/products/asmo-
na/eng/vtune/239144.htm 
J Sign Process Syst
loadings is to pre-load proper hardware descriptions
before the run time. In particular, Hauck [8] presented
the concept of configuration prefetching in which the
loading duration was overlapped with computations to
reduce the overheads. Harkin et al. [9] evaluated nine
approaches of hardware/software partitioning to pro-
vide insights in the implementation methodology for re-
configuration hardware. Genetic algorithms (GA) and
a neural network architecture were also presented by
the same authors for run-time reconfiguration [10, 11].
Yuh et al. [12] developed a tree-based data structure,
called T-tree, for a temporal floorplanning to schedule
all the reconfigurable tasks with a simulated-annealing-
based algorithm (SA). Ghiasi et al. [13] proposed an
efficient optimal algorithm to minimize the run-time re-
configuration delay in the executions of applications on
a dynamically adaptable system under assumptions on
several restricted implementation constraints. Noguera
and Badia [2, 14] introduced a two-version dynamic
scheduling algorithm for reconfigurable architectures
with or without a prefetching unit. Resano et al. [15]
proposed a way to revise a given task schedule by
considering reconfiguration to minimize the latency
overheads. Fu and Compton [16, 17] proposed the
multi-threaded environment of reconfigurable comput-
ing and modeled it as a Multi-Constraint Knapsack
Problem (MCKP).
This paper targets one essential implementation
issue in the reconfigurable computing: That is the min-
imization of the required number of FPGA configu-
ration contexts, where the deadline and precedence
constraints of an application are given. Different from
many previous results, we consider the optimization
problem without a given schedule (that comes with
fixed execution intervals). The NP-completeness of the
CONTEXT MINIMIZATION problem is first proved.
Several additional constraints on the loading time
and the execution time of a task and the precedence
constraints of tasks (and their resulting subproblems)
are explored. In particular, several subproblems that
are NP-complete or polynomial-time solvable are ex-
ploited. Optimal algorithms are presented for subprob-
lems that are polynomial-time solvable. The objective
is provide insights on how and why difficult the CON-
TEXT MINIMIZATION problem is. We then present
a heuristic-based greedy algorithm for the CONTEXT
MINIMIZATION problem in a more general case.
The capability of the proposed greedy algorithm is
evaluated by a series of experiments. The results could
not only be applied to minimize the hardware cost in
reconfigurable computing but also benefit the designs
of hardware/software co-design tools with dynamic
reconfiguration.
The rest of this paper is organized as follows: Section
2 defines the CONTEXT MINIMIZATION problem.
Section 3 explores several subproblems of the CON-
TEXT MINIMIZATION problem under several addi-
tional constraints. Optimal algorithms for subproblems
that are solvable in polynomial time are presented.
Section 4 proposes a heuristics-based greedy algorithm.
Section 5 evaluates the capability of the proposed
algorithm. Section 6 is the conclusion.
2 Problem Definition
In this paper, we are interested in the derivation of a
reconfiguration plan  with the objective to minimize
the number of required FPGA configuration contexts
in a multi-context FPGA platform. The reconfiguration
plan should be derived based on a given task set T,
a partial order of task precedences ≺, and a common
deadline D. Each task τi in a task set T is denoted as
τi = (ei, li), where ei is the required execution time, and
li is the loading (configuration) duration to load task τi
onto a context. A precedence constraint τi ≺ τ j in the
partial order ≺ requires that task τ j can only start its
execution after the completion of task τi. A precedence
constraint might exist because the latter task needs to
read from the output of the former task. We are inter-
ested in the minimization of the maximum time span
of the task execution in T. The problem is modeled as
a performance requirement, i.e., the common deadline
D. Any solution to the targeted problem is a reconfig-
uration plan , in which we have a loading time T(τi),
an execution starting time S(τi), and a configuration
context ID C(τi) for each task τi. A solution should
also satisfy the given partial order of task executions
and the common deadline. The problem is formally
defined as follows.
Problem 1 (CONTEXT MINIMIZATION) Given a
set T of n tasks (τi = (ei, li) ∈ T, 1 ≤ i ≤ n) with a par-
tial order ≺ and a common deadline D, the problem is
to find a reconfiguration plan  with the minimum num-
ber of required FPGA configuration contexts without
violating the partial order of task executions and the
common deadline.
A reconfiguration plan is feasible if and only if the
following three conditions are satisfied: The first con-
dition requires each FPGA context being loaded in
time. The second condition requires that any two tasks
should not use the same context in any overlapped
time interval. The third condition requires the loading
J Sign Process Syst
A task set ( ), , a deadline , and contexts
A feasible reconfiguration plan
1: .
2: is not empty
3: Remove an arbitrary task from .
4: Locate context with the earliest idle time .
5: .
6: .
7: .
8: .
9:
10: Return failure.
11:
12:
Algorithm 1
Input:
Output:
while do
if then
end if
end while
a ready task arbitrarily (Step 3) and load the task onto
the context with the earliest available time (Step 4). The
loading time, starting time, and context ID of the task
are updated accordingly (Steps 5–7). The earliest possi-
ble time for the next context loading is then updated
(Please see Step 8 and Condition 3). The algorithm
reports a failure if any task misses the deadline (Steps
9–11). The time complexity is O(n × log M).
Theorem 4 Algorithm 1 is optimal in the sense that it
always derives a solution if any feasible solution exists.
Proof The correctness of this theorem follows directly
from the fact that all tasks are of the same execution
time and loading duration and share the common dead-
line. 	unionsq
Figure 1 shows four optimal reconfiguration plans of
four tasks, where the number M of FPGA configuration
contexts ranges from 1 to 4. An interesting packing of
Figure 1 Four reconfiguration plans over four different numbers
of contexts.
tasks is shown in the figures, and the impacts of the mu-
tual exclusion constraint, i.e., Condition 3, are clearly
illustrated. Note that the shaded rectangles denote the
loadings of tasks onto contexts, and white rectangles
denote task executions.
Lemma 1 Algorithm 1 needs no more than MB = max
{n,  EL + 1} contexts to derive a feasible reconfiguration
plan, where n is the number of tasks.
Proof The correctness of this lemma follows directly
from the facts that loading durations can not be over-
lapped with each another, and a loading duration can
be overlapped with any execution time as long as the
three feasibility conditions are satisfied. 	unionsq
Lemma 1 provides an upper bound on the maximum
number of contexts over that Algorithm 1 could derive
a feasible reconfiguration plan. Figure 2 shows recon-
figuration plans for two task sets, i.e., one with E ≤ L
and the other with E > L, where different numbers of
FPGA configuration contexts are tried. Note that when
n × (L + E) ≤ D, only one context is needed to derive
a feasible reconfiguration plan.
Although we show that the CONTEXT MINI-
MIZATION problem becomes tractable when ≺= ∅,
and ∀i, ei = E and li = L, one question remains. That
is how difficult the problem is if we relax some of
the above constraints! Suppose that we still have tasks
being independent, i.e., ≺= ∅, but every task might
have a different execution time, i.e., ei = e j for some
τi, τ j ∈ T. Even if we let ∀i, li = 0, the CONTEXT
MINIMIZATION problem is intractable because the
problem is indeed the MINIMUM MULTIPROCES-
J Sign Process Syst
Algorithm 2
Input: A task set ( ), as a tree, a deadline , and contexts
Output: A feasible reconfiguration plan
1: Assign priorities to tasks according to the CP rule.
2: = : does not have any predecessor .
3: while is not empty do
4: Remove the task with the highest priority from .
5: Locate context with the earliest idle time .
6: .
7: .
8: , where if all of the predecessors of have been scheduled.
9: if then
10: Return failure.
11: end if
12: end while
3.2.3 A Task Set with E ≤ L
As shown in the previous section, the CONTEXT MIN-
IMIZATION problem becomes more tractable when a
partial order is restricted in a tree fashion, compared to
that shown in Theorem 1. Another question is whether
we could trade the partial-order constraint with any
other constraint to keep the CONTEXT MINIMIZA-
TION problem being tractable. In this section, we shall
show that the CONTEXT MINIMIZATION problem
remains tractable by the constraint ∀i, ei = E, li = L,
and E ≤ L. In such a case, the partial order among
tasks could be arbitrary.
An optimal algorithm for this problem is as the same
as Algorithm 1 except two minor modifications: (1)
Step 3: Remove any arbitrary ready task τi ∈ T, and
(2) Step 4: Use the context Mj which is idle in the last
iteration. The algorithm is referred to as Algorithm 3.
The time complexity of Algorithm 3 is O(n).
Theorem 6 Algorithm 3 is optimal in the sense that it
always derives a solution if any feasible solution exists.
Proof The optimality of the algorithm is based on the
fact that the minimum number of the required contexts
must be 1 or 2, unless there is no feasible solution. If the
summation of loading durations and execution times of
all tasks is less than the common deadline, i.e., n × (L +
E) ≤ D, the answer is 1; Otherwise, the answer is 2.
Algorithm 3 ALAP-Greedy Algorithm
Input: A task set in which each task is associated with its execution time and loading duration,
i.e., , a common deadline , and a given number of FPGA contexts
Output: A feasible reconfiguration plan
1: Derive the ALAP value of each task .
2: .
3: while is not empty do
4: Remove ready task with the smallest from , where its release time is . (A tie is broken first by the largest
and then the largest . If all parameters are identical, the tie-breaking is done arbitrarily.)
5: Locate context with the earliest idle time .
6: .
7: .
8: .
9: .
10: if then
11: Return failure.
12: end if
13: end while
J Sign Process Syst
is loaded first because its ALAP value is the minimum,
and its execution time is larger than that of τ1. After
τ3 is loaded onto the first context, τ1 is loaded onto
the second context. As shown in Fig. 5b, τ2 is loaded
later because its ALAP value is earlier than that of τ4.
A reconfiguration plan is derived over two contexts.
Figure 5c shows a schedule derived by a novice greedy
algorithm in which the first task in the ready queue is
always picked up. Suppose that the ready queue has τ1
and τ3, and τ1 is in the first one in the queue. After τ1
and τ3 are loaded onto the first and second contexts,
respectively, τ4 must be loaded onto the third context
to meet the deadline. The increasing of the number of
required contexts in Fig. 5c is because of the missing of
the considerations of task characteristics.
5 Simulation Results
We shall evaluate the capability of the proposed
ALAP-Greedy algorithm in the derivation of recon-
figuration plans for the CONTEXT MINIMIZATION
problem without any constraint. Two set of experi-
ments were done to evaluate the precision of the de-
rived solutions to optimal solutions and the superior-
ity of the ALAP-Greedy algorithm over other novice
greedy algorithms.
5.1 Experiment Setup
The task sets and the precedence constraints used in
our experiments were generated by the tool task graphs
for free (TGFF) [21]. Each task in a task set was gener-
ated with an execution time and a loading duration. The
deadline of a task set was set as two times of the total
execution time of the critical path in the corresponding
partial order.
The first experiment was designed to identify the
deviation between the number of required contexts
Figure 6 Experimental results of ALAP-Greedy algorithm.
Figure 7 Comparison between different greedy algorithms.
derived by the ALAP-Greedy algorithm and that of an
optimal solution. Let ALG be the number of required
contexts derived by the heuristic approach, and OPT
be the optimal number of required contexts obtained by
a brute force search. The primary performance metric is
the ratio of ALG to OPT, i.e., ALG/OPT, which was
referred to as the approximation ratio in this paper. The
optimality approaching level, defined as the percentage
of the number of derived solutions were equal to opti-
mal solutions, of the ALAP-Greedy algorithm was also
evaluated. 100 independent experiments were run for
each parameter configuration.
The other set of experiments was designed to
show the reason why we chose ALAP-Greedy
approach. Three more heuristics, ASAP-Greedy,
ALAP1-Greedy, and FIFO-Greedy were introduced:
(1) The ASAP-Greedy algorithm always picks up
the ready task with the smallest as-soon-as-possible
(ASAP) value, where the value denotes the earliest
time when the corresponding task is ready to start
its execution. (2) The ALAP1-Greedy algorithm also
picks up the ready task with the smallest ALAP value
as the ALAP-Greedy algorithm, nevertheless, a tie-
breaking is done first by choosing the task with the
largest loading time and then the execution time. (3)
The FIFO-Greedy algorithm always picks up the first
task in the ready queue. The performance metric was
the ratio of the context number required by the derived
solution to the task number of the given task set, which
is referred to as the context-task ratio here.
J Sign Process Syst
ary computation. ACM Transactions on Embedded Comput-
ing Systems, 3(4), 661–685.
11. Harkin, J., Morgan, F., Hall, S., Dudek, P., Dowrick, T., &
McDaid, L. (2008). Reconfigurable platforms and the chal-
lenges for large-scale implementations of spiking neural net-
works. In International conference on field programmable
logic and applications (pp. 483–486), September.
12. Yuh, P. H., Yang, C. L., & Chang, Y. W. (2004). Temporal
floorplanning using the T-tree formulation. In Proceedings
of ACM/IEEE international conference on computer-aided
design.
13. Ghiasi, S., Nahapetian, A., & Sarrafzadeh, M. (2004). An
optimal algorithm for minimizing run-time reconfiguration
delay. ACM Transactions on Embedded Computing Systems,
3(2), 237–256.
14. Noguera, J., & Badia, R. M. (2006). System-level power-
performance tradeoffs for reconfigurable computing. IEEE
Transactions on Very Large Scale Integration Systems, 14,
730–739.
15. Resano, J., Mozos, D., & Catthoor, F. (2005). A reconfig-
urable manager for dynamically reconfigurable hardware.
IEEE Design and Test of Computers, 22(5), 452–460.
16. Fu, W., & Compton, K. (2005). An execution environment
for reconfigurable computing. In IEEE symposium on field-
programmable custom computing machines (pp. 149–158),
April.
17. Fu, W., & Compton, K. (2008). Scheduling intervals for
reconfigurable computing. In IEEE symposium on field-
programmable custom computing machines (pp. 87–96),
April.
18. Garey, M. R., & Johnson, D. S. (1979). Computers and in-
tractability. San Francisco: W. H. Freeman.
19. Pinedo, M. (2002). Scheduling theory, algorithms, and sys-
tems, (2nd edn.). Englewood Cliffs: Prentice Hall.
20. Gerez, S. H. (1999). Algorithms for VLSI design automation.
New York: Wiley.
21. Dick, R. P., Rhodes, D. L., & Wolf, W. (1998). TGFF: Task
graphs for free. In Proceedings of international workshop on
hardware/software codesign.
Nei-Chiung Perng received his PhD degree in Computer Science
and Information Engineering from National Taiwan University,
Taipei, Taiwan, in 2006. He also holds a BS and an MS in
Computer and Information Science from National Chao Tung
University in 1999 and 2001, respectively. His research interests
include real-time systems and scheduling algorithms.
Shih-Hao Hung was born in Taipei, Taiwan. He has broad inter-
ests in computers since 1981 and specializes in parallel processing,
network computing, application performance, e-commerce secu-
rity. He graduated with a B.S. degree in Electrical Engineering
from the National Taiwan University (NTU) in 1989. After his
two-year military service with the Taiwanese Marine, he worked
as a full-time teaching assistant at the NTU in 1991. He received
MSE and Ph.D. degrees in Computer Science and Engineering
from the University of Michigan, Ann Arbor, in 1994 and 1998,
respectively, where his main focus was performance optimization
of applications on parallel and high-performance supercomput-
ers. He continued his research as a post-doctoral research fellow
at the University of Michigan from 1998 to 2000.
Dr. Hung moved to California and joined Sun Microsystems,
Inc. in 2000, when Sun was most successful during the .com
era. As a Staff Engineer, his work at Sun was focused on high-
performance network applications and security enhancement on
Sun’s enterprise servers. In February 2005, Dr. Hung returned
to Taiwan and became a faculty member with the Department
of Computer Science and Information Engineering at NTU,
where he established the Performance, Applications, and Secu-
rity (PAS) Laboratory to research on ideas to improve system
performance and network security for computer applications
from embedded processors to supercomputers.
Chia-Heng Tu received his B.S. and M.S. in Computer Science
from the National Taiwan Ocean University, Taiwan in 2003 and
2005, respectively. Currently, he is a Ph.D. student in Gradu-
ate Institute of Networking and Multimedia, National Taiwan
University, Taiwan. His research interests include performance
analysis tools and compiler techniques for accelerating appli-
cations on multicore platform, and methods and tools for fast
system-level performance simulation and modeling.
 framework that was developed to facilitate trace generation 
and measurements via instrumentation of a program. TCPP is a 
pure-software (hardware-independent) profiler framework 
consisting of the trace collection and trace post-processing, as 
shown in Fig. 1. The trace collection records the events of a 
running application as a stream, and the trace post-processing 
converts/analyzes the stream into readable and useful 
information for the human. The benefit of traces is that they 
can be post-processed to perform a very detailed analysis. 
Based on the concepts of the TCPP, we developed the 
ParallelTracer toolkit for evaluating applications performance 
on heterogeneous multicore platforms, such as the IBM Cell 
platform. Each Cell processor chip contains a Power 
Processing Element (PPE) core and eight Synergistic 
Processing Elements (SPE’s) cores. The PPE is responsible for 
running the operating system (e.g. Linux) and the SPE’s are 
intended to be used as computational engines.  
The following subsections describe the major concepts of 
the TCPP and the work that we have done to port the 
ParallelTracer to the Cell platform. 
TABLE I 
EXPERIMENT ENVIRONMENT 
 Current impl. of 
TCPP  
ParallelTracer  
Sequential vs. Parallel 
programs Sequential Parallel 
User and Kernel space 
programs tracing support User + Kernel User 
Features: Path profiling Timeline diagram 
 
Execution time 
profiling  
 
  
Fig. 1. The architecture of trace collecting and trace post-processing framework (TCPP framework). 
 
A. Preprocessing 
The core of TCPP framework is a pure-software instrumentor 
that generates traces by inserting probes at certain locations of a 
program (i.e., instrumentation points) without altering the flow 
of an application. The input to the instrumentor is the source 
programs to be instrumented. Then, the probes (events relevant 
trace handlers) will be inserted at particular program points, 
which are either specified by users or pre-determined locations 
of the occurrence of program constructs. Once the 
instrumentation process is done, the instrumented programs can 
be compiled with the native compiler toolchain. 
The instrumentor supplies mechanisms for users to specify 
the starting point and the end point of the events that can be 
recognized by the instrumentor. The probes inserted by the 
instrumentor can cause substantial overhead in terms of 
execution time and code size, and should be taken into account 
in the design and use of the instrumentor.  
For supporting parallel programming on the Cell platform, 
IBM provides a customized C/C++ compiler with certain 
language extensions and associated libraries, which are not 
recognized by open-sourced source-to-source instrumentors. 
Thus, we pre-programmed the instrumentor to identify the 
starting and end points for those customized program constructs 
using ANother Tool for Language Recognition (ANTLR) [7]. 
As a result, our instrumentor recognizes the Cell-specific 
language extensions (e.g., architecture specific data types, 
programming directives, etc.) which cannot be parsed directly 
by those publicly available source-to-source instrumentors.  
B. Collection 
Based on a pre-defined format, this stage generates the trace 
 developing analysis tools, the development of a post-processing 
tool can take advantage of the API specified by the trace 
interface to retrieve the traces without having to understand the 
internal format of the traces. For our ParallelTracer toolkit, we 
wrote a trace convertor to transform the trace data into the Open 
Trace Format (OTF) [8] accepted by Vampir, a popular 
graphical visualization tool for performance analysis.  
 
 
(a) RC5 full performance evaluation view. 
 
 
 (b) RC5 zoomed-in performance evaluation view. 
Fig. 3. Timeline diagram of RC5 application. 
E. Visualization 
The visualization stage makes the profile information 
readable and comprehensive. Representing these data without 
unnecessary details is critical to a good performance summary 
report. Such a summary is useful for programmers to identify 
performance bottlenecks and improve the programs in a timely 
manner. Some users may like visual graphs better than text 
reports. Since there are existing tools that utilize a standard 
interface to provide variety of visualization, such as Gnuplot, 
Graphviz [9] and Vampir [10], we simply take advantage of 
these tools through the standard interface. 
III. CASE STUDY ON THE CELL PROCESSOR 
In this section, we demonstrate our tools for performance 
analysis of the program running on the Cell platform. We used 
our toolkit to collect traces for a benchmark application which 
utilizes a multithreaded RC5 encryption/decryption library 
functions. Based on the traces, we evaluated the computation 
time, communication time, and data volume transferred on each 
processor and analyzed the interprocessor communications. In 
addition, we examined the overhead caused by our toolkit, in 
terms of execution time and code size. 
A. Experimental Setup 
We performed experiments on the IBM BladeCenter QS21 
[11], which consists of two multi-core Cell processor chips 
clocked at 3.2GHz. The QS21 is running with Red Hat 4.1.2 
Linux and using GCC compiler v.4.1.1. A block cipher 
application, RC5, is chosen as the workload to evaluate our 
toolkit. The RC5 implementation [12] is written using a data 
parallel model, i.e., the data is partitioned by the PPE and 
distributed onto multiple SPE cores for the encryption and 
decryption. In the experiment, a PPE was used to partition 4MB 
input data and four SPE cores were used to encrypt the data 
assigned by the PPE. Note that since the performance results of 
encryption and decryption are similar, we use the encryption 
results to demonstrate the ability of our toolkit. 
B. Experimental Results 
Fig. 3(a) illustrates the runtime behavior of the RC5 
application with the timeline diagram. Detailed performance 
behaviors can be observed by zooming in the visual graph, as 
shown in Fig. 3(b). The lines between processors indicate the 
communication operations took place between the processor 
cores. The bars in brown color represent the wait time for 
processor cores waiting for the termination of the 
communication operations. Computation time is denoted by the 
 [5] Hung, S.-H., Huang, S.-J., and Tu, C.-H., "New Tracing 
and Performance Analysis Techniques for Embedded 
Applications", in Proceedings of the 14th IEEE 
International Conference on Embedded and Real-Time 
Computing Systems and Applications, 2008, pp. 143-152. 
[6] Chen, T., Raghavan, R., Dale, J.N., and Iwata, E., "Cell 
Broadband Engine Architecutre and Its First 
Implementation - a Performance View", IBM Journal of 
Research and Development, 2007. 
[7] ANTLR: ANother Tool for Language Recognition. 
http://www.antlr.org/ 
[8] Open Trace Format. http://www.paratools.com/otf.php 
[9] Tutorials for Understanding How to Use Graphvize. 
http://www.graphviz.org/ 
[10] Vampir: A commercial post-mortem trace visualisation 
tool. http://www.vmpir.eu/index.html 
[11] IBM BladeCenter QS21. 
http://www-03.ibm.com/systems/bladecenter/hardware/ser
vers/qs21/index.html 
[12] Cellbuzz, Georgia Tech Cell BE Software. 
http://sourceforge.net/projects/cellbuzz/ 
 
 
 
0%
2%
4%
6%
8%
10%
12%
14%
16%
18%
20%
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
2K 4K 8K 16
K
Input Data 
Size = 2MB
Input Data 
Size = 4MB
Input Data 
Size = 8MB
Input Data 
Size = 2MB
Input Data 
Size = 4MB
Input Data 
Size = 8MB
Input Data 
Size = 2MB
Input Data 
Size = 4MB
Input Data 
Size = 8MB
1 SPE 2 SPE's 4 SPE's
Pe
rf
or
m
an
ce
 o
ve
rh
ea
d
Trace buffer size (bytes)
Single-buffer Double-buffer
 
Fig. 4. Performance overhead on RC5 application with different trace buffer sizes and input data sizes. 
 
 Hardware/Software Co-Design 的論文，主要敘述如何針對應用設計硬體架構以及討論最
佳化的技術: 
 
 A Novel Approach to Hardware Architecture Design and Advanced Optimization 
Techniques for Time Critical Applications (Archana Kalyansundar, Rita Chattopadhyay) 
此篇由來自印度 Siemens 公司的專家，分享他們利用 TI TMS320DM642 平台設計多
媒體應用的實務經驗。 
 
 Advanced Optimization and Design Issues of a 32-Bit Embedded Processor Based on 
Produced Order Queue Computation Model (Hiroki Hoshino, Ben A. Abderazek, Kenichi 
Kuroda) 
此篇由來自日本愛知大學的學者，發表一套他們認為可以用以設計高效能的 32 位元
嵌入式處理機的方法，並且以 FPGA 合成處理機的方式，進行實際的驗證。 
 
 A Low-Cost Embedded Controller for Complex Control Systems (Zhe Peng, Longhua Ma, 
Feng Xia) 
此篇由來浙江大學的學者，討論如何設計出價廉物美的嵌入式控制器提供控制系統
使用。 
 
 Single Instruction Dual-Execution Model Processor Architecture (Taichi Maekawa, Ben 
A. Abderazek, Kenichi Kuroda) 
此篇又是由來自日本愛知大學的學者所發表，提出所謂 Single Instruction 
Dual-Execution 的新型處理器架構，主要還是針對他們在先前所提出的 Queue and 
Stack Programming Model，能提供較佳的運算能力。 
 
我們的論文由同行的博士班學生涂嘉恆於 12 月 19 日發表在 Embedded Software 
Optimization 的 Session。這是涂同學第一次出國參加國際會議，我認為這類的經驗是國
內學生所欠缺的，讓國內學生多參加這類會議，可增進他們的國際觀，是在國內所學不
到的。涂同學經驗不足，有些緊張，所以我建議他放輕鬆，放慢速度，以事先準備好的
方式發表即可。涂同學在發表時，時間控制大致良好，只是講話時缺乏抑揚頓挫，比較
無法引發聽眾的熱情。這也是一般中日韓的學生，由於英語會話訓練的不足，較容易產
生的問題。 
晚宴中，大會頒發 Best Paper Award，由台大郭大維教授的研究團隊得到，我們在場的台
灣人也與有榮焉。 
圖二 郭大維教授團隊獲得 Best Paper Award 
 
二、與會心得 
 
Internal Conference on Embedded and Ubiquitous Computing (EUC) 是近年來在嵌入式系
統(embedded systems)領域逐漸受到重視的國際會議。由於嵌入式系統著重將包羅萬象的
應用與廣泛的技術層面相結合，藉以提升技術整合，並且特別深入幾個熱門的嵌入式系
統研發的方向，包括硬軟體統合設計(hardware-software co-design)、即時系統(real-time 
systems)、嵌入式軟體最佳化(embedded software optimization)、節能設計(power-aware 
design)、感測器網路(sensor networks)等，並且每年都在主要會議的之前或同時舉辦研討
會(workshops)來倡導對一些新興領域的研究。 
 
由於我國、日本、韓國三個亞太國家一向積極培植嵌入式系統設計產業，在 2004 年由日
本開始主辦此一國際會議、2005 年在日本 Nagasaki、2006 年在韓國 Seoul，均有不錯的
參與率，而會議論文的品質亦逐年增加。2007 年由台灣大學在台北主辦 EUC2007 之時，
本人擔任 registration co-chair 及擔任第二屆 Workshop on Embedded Software Optimization
的主辦人，當時論文的接受率為 30%左右，而參加的學者大半來自外國，除了日韓學者
為大宗之外，其他歐美等國也有數十人來訪，總計近兩百人參與盛會，也讓許多國內的
學者就近於國外交流互動。 
 
