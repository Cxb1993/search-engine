 1 
	 
□期中進度報告	 
行政院國家科學委員會補助專題研究計畫	 
þ期末報告	 
	 
結構化與分層化之統計式機器翻譯系統之研究	 
	 
	 
計畫類別：þ個別型計畫	 	 	 □整合型計畫	 
計畫編號：NSC	 98－2221－E－007－091－MY2	 
執行期間：98	 年	 08	 月	 01	 日至	 101	 年	 07	 月	 31	 日	 
	 
執行機構及系所：國立清華大學資訊工程學系	 
	 
計畫主持人：	 	 	 	 張俊盛教授	 
共同主持人：	 
計畫參與人員：	 
	 
	 
	 
	 
本計畫除繳交成果報告外，另含下列出國報告，共	 ___	 份：	 
□移地研究心得報告	 
□出席國際學術會議心得報告	 
□國際合作研究計畫國外研究報告	 
	 
	 
	 
處理方式：除列管計畫及下列情形者外，得立即公開查詢	 
	 	 	 	 	 	 	 	 	 	 	 	 □涉及專利或其他智慧財產權，□一年□二年後可公開查詢	 
	 
中	 	 	 華	 	 	 民	 	 	 國	 	 	 	 年	 	 	 	 月	 	 	 	 日
 3 
國科會補助計畫衍生研發成果推廣資料表	 
日期：	 	 	 年	 	 月	 	 日	 
國科會補助計畫	 
計畫名稱：結構化與分層化之統計式機器翻譯系統之研究	 
計畫主持人：張俊盛	 	 	 	 	 	 	 	 
計畫編號：NSC	 98－2221－E－007－091－MY2	 領域：資訊工程	 
（中文）	 
研發成果名稱	 
（英文）	 
成果歸屬機構	 
	 發明人	 
(創作人)	 
	 
（中文）	 
	 
	 
	 
（200-500字）	 技術說明	 
（英文）	 
產業別	 
	 
技術/產品應用範圍	 
	 
技術移轉可行性及預
期效益	 
	 
	 	 	 	 	 註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。	 
	 
	 
	 
 5 
稍有冷卻的跡象，分場的觀眾比往年少。反觀語意學的分場卻爆滿，許多人席地而坐。	 
	 
各分場的論文報告也十分精采，讓參與者在會議的短短幾天內，	 就可以對學界目前的發展有
了全面的了解。	 今年看到機器學習仍然是主要的研究方法，但是應用的方式，應用的範圍，
持續地擴大，有許多令人驚豔的研究成果。例如，分析句子中逗號的使用的統計模型，以及寫
作中後設言談、轉折語，以及內容文字的二元分類器。	 
	 
二、與會心得	 
在本次會議了解目前世界頂尖研究單位、研究主題的趨勢及方法，受益頗豐。我們的研究特別關
注電腦輔助語言學習與電腦輔助翻譯，著重深入分析語料庫與網路，提供學習語言、翻譯術語與
專有名詞的效率。此一特殊的研究角度與作法，也引起許多學者的高度興趣，對我們的研究有深
刻的印象。這表示我們對於語言學習領域的耕耘在國外已受到重視。在會議第二天	 poster	 展示
現 場 發 表 論 文 FLOW:A	 First-Language-Oriented	 Writing	 Assistant	 System （
http://nlp-service1.cs.nthu.edu.tw/FLOW/index.html），並在第三天口頭報告	 Learning to Find 
Translations and Transliterations on the Web。都有很好的機會和與會學者交換心得，包括微軟亞洲
研究院的林清祐博士、John	 Hopkins	 University	 的	 Callison-Burch	 教授、Yarowsky	 教授、新加坡
南洋理工大學的	 Francis	 Bond	 教授、ETS	 	 的	 Joel	 Tetreault	 博士等。	 
	 
我們發表海報論文、口頭報告論文時，得到參與討論者許多進一步研究發展的寶貴建議。許多
學者認為我們的雙語並呈的作法，可以同時幫助學習英語與華語。這樣可以呼應目前外國人學
習華語的需求的快速成長。	 
三、發表論文全文或摘要	 
近年來，最新的跨語言研究系統都依賴雙語平行語料庫。然而，即使是在一個非常大的平行語
料庫，還是很難找到術語、專有名詞的翻譯。在本文中，我們提出了一種新的方法，接受尋找
術語為輸入，在網路上尋找適當的翻譯。在我們的方法中，我們使用少量術語與其翻譯，用來
搜尋搜索引擎，以得到含有術語及其翻譯的中英混雜網頁摘要。然後，我們在摘要中自動註明
術語、翻譯、其他的標籤，加入特徵值，自動生成的可以訓練條件隨機場模型，產生自動標註
的程式。在執行時，我們一樣送出輸入的術語，得到中英混雜的網頁摘要，並執行，標註翻譯
的位置，辨識翻譯統計翻譯，最後輸出在摘要中出現最多次的翻譯。初步的實驗和評價顯示，
我們的方法，很明確地整合各種翻譯相關的特徵值，產生優於前人研究的系統。	 
四、建議	 
參加此研討會，不僅讓我們將我們的研究介紹到國際，並汲取相關研究人員寶貴的建議。我們接
觸到相當多知名外國學者，此次，與多位重量級的學者分享我們的研究，且彼此交換意見，讓我
們的研究能夠有更多的應用及改善，實屬難得的經驗。	 
	 
參與本次研討會，了解到國外研究單位在系統製作、跨領域整合上，比國內做得更好。另外，國
外的學者在	 open	 source/open	 data	 也更加重是，許多論文也都提供了研究中所用到的資料和原始
 7 
結構化與分層化之統計式機器翻譯系統之研究	 
計畫編號：NSC	 98－2221－E－007－091－MY2	 
執行期間：98	 年	 08	 月	 01	 日至	 101	 年	 07	 月	 31	 日	 
	 
執行機構及系所：國立清華大學資訊工程學系	 
	 
計畫主持人：	 	 	 	 張俊盛教授	 
 
 
Abstract 
In recent years, state-of-the-arts crosslinguistic 
systems has been based on parallel corpora. 
However, it is difficult to find translation of 
technical terms and named entities even in a very 
large parallel corpus. In this paper, we present a 
new method for finding translations on the Web for 
a given term. In our approach, we use a small set of 
terms and translations to obtain mixed-code 
snippets returned by a search engine. We then 
automatically annotate the data with translation 
tags, automatically generate features to augment the 
tagged data, and automatically train a conditional 
random field model for identifying translations. At 
runtime, we obtain mixedcode webpages containing 
the given term, and run the model to extract 
translations as output. Preliminary experiments and 
evaluation show our method cleanly combining 
various features, resulting in a system outperform 
previous work.   
General Terms 
Algorithms, Experimentation, Languages. 
Keywords 
Machine Learning, Cross-lingual Information Retrieval, 
Information Extraction, Machine Translation. 
1. INTRODUCTION 
The phrase translation problem is critical to many 
cross-language tasks, including statistical machine translation, 
cross-lingual information retrieval, and multilingual 
terminology (Bian and Chen 2000, Kupiec 1993). Such 
systems typically use a bilingual lexicon or a parallel corpus 
to train a probabilistic phrase translation model. However, the 
out of vocabulary problem (OOV) is hard to overcome even 
with a very large training corpus due to the Zipf nature of 
word distribution, and the fact that new words, technical 
terms, and named entities arise frequently. On the other hand, 
the rise of Internet has lead to an unprecedented buildup of 
multilingual texts on the Web. Specifically, there are an 
abundant of webpages consisting mixed-code text, namely 
text in more than one language. We observe that the 
mixed-code webpages typically are written in one language 
but interspersed with some sentential or phrasal translations 
in another language. By retrieving and identifying such 
translation counterparts on the Web, we can cope with the 
OOV problem caused by the limited coverage of dictionaries 
and parallel corpora. 
Consider a Wikipedia title, “Named-entity recognition.” 
The best places to find the Chinese translations for 
named-entity recognition are probably not some parallel 
corpus or dictionary, but rather mixed-code webpages, 
mentioning this term in both Chinese and English. The 
following example is a snippet returned by the Bing search 
engine for this webpage for the query “named entity 
recognition”: 
http://zh.wikipedia.org/zh-hk/問答系統 : 從系統內部來
看，問答系統使用了大量有別於傳統資訊檢索系統自然語
言處理技術，如自然語言剖析  (Natural Language 
Parsing)、問題分類 (Question Classification)、專名辨識 
(Named Entity Recognition)等等。 
In this snippets, the authors mention several technical 
 9 
describe in detail the problem statement and the proposed 
method. Finally in Section 5, we report evaluation results and 
error analysis, and conclude in Section 6.  
2. RESOURCES 
In this work, we rely on several resources that are 
available on the Internet. These resources are used as seeds to 
obtain and label training data, as gold standard for automatic 
evaluation, and as external knowledge for generating 
features. 
2.1 Wikipedia 
In this work, we extract bilingual title pairs from the 
English and Chinese edition of Wikipedia as gold standard 
for evaluation, and as seeds to automatically collect and label 
training data from the Internet through querying search 
engines. 
The number of entries in English Wikipedia had been 
growing exponentially from 2001 to 2008, with some 20,000 
new articles are created monthly by thousands of volunteers 
around the world, making it a excellent source to find new 
words and terms. As of 2012-0202, the English Wikipedia 
has 3,861,652 articles, making it the most well-establish 
edition of Wikipedia. 
Entries of the same topic among different language 
edition of Wikipedia are interlinked with language links. 
However, only a small percentage of these articles are linked 
to editions of other languages. The Chinese Wikipedia 
contains only 398,206 articles, roughly 1/10 the size of the 
English Wikipedia. Only near 5% of the entries in the English 
Wikipedia contain language links to their Chinese 
counterparts. The method propose in this paper can be used to 
find translation of those English terms, thus speeding up the 
process of building a more complete multilingual Wikipedia. 
As will be described in Section 4, we extracted the titles of 
linked English-Chinese article pairs for training and testing 
purposes. 
The content of Wikipedia can be freely download or 
access online at http://en.wikipedia.org/wiki/Wikipedia_ 
Database_download . We use the Google Freebase Wikipedia 
Extraction (WEX) instead of the official dump. The WEX is 
a processed version of the official dump, with the Wikipedia 
syntax transformed into XML for machine readability. The 
WEX database can be freely obtained at http:// 
wiki.freebase.com/ wiki/WEX . 
2.2   WordNet 
WordNet is a freely available, handcrafted lexical 
semantic database for English. Started its development back 
in 1985 at Princeton University by a team of cognition 
scientists, WordNet was originally intended to support 
psycho-linguistic research. Over the years, WordNet has also 
been increasingly popular in the fields of information 
retrieval, natural language processing, artificial intelligent. 
Through each releases, WordNet has grown to be a 
comprehensive database of concepts in the English language. 
As of today, the stable 3.0 version of WordNet contains 
extensively a total of 207,000 semantic relations between 
150,000 words organized in over 115,000 senses. 
Senses in WordNet are represented as synonym sets 
(synsets). A synset contains one or more words that expresses 
the same meaning.  In addition, WordNet also provides 
other information for each synsets including a short definition, 
example sentences, and estimated frequency. For example, 
the synset {block, city_block} is defined as a rectangular area 
in a city surrounded by streets, while as synset {block, cube} 
is defined as a three-dimensional shape with six square or 
rectangular sides. WordNet also records various semantic 
relations between its senses. These relations includes 
hypernyms, hyponyms, coordinate terms, holonym, meronym, 
hypernym. 
The WordNet database can be freely download or access 
online at http://wordnet.princeton.edu/ . 
2.3   S inica Bilingual WordNet 
The Sinica Bilingual WordNet is part of the publicly 
accessible Sinica Bilingual Ontological WordNet (Sinica 
BOW). (Huang. 2003.) In this work, we treat the Sinica 
Bilingual WordNet as a bilingual dictionary, and use it as an 
external knowledge source to generate features for training 
the CRF model. 
 11 
Recently, some methods in the literature has also aimed to 
exploit mixed code webpages for word and phrase translation. 
Nagata et al. (2001) presented a system for finding English 
translations for a given Japanese technical term in 
Japanese-English from search engine results. Their method 
extracts English phrases appearing near the given Japanese 
term, and scores translation candidates based on counts and 
location. Cao and Li (2002) also propose an EM algorithm to 
to find general base noun phrase translation in Web data. 
More recently, Kwok et al. (2005) focused on named entity 
phrase and implemented a name finder cross-language 
information retrieval system based on Chinese-English 
webpages. Wu et al. (2005) proposed a method to learn a set 
of surface patterns to find translations appearing in shorter 
distance. Mixed-code webpage snippets are obtained by 
querying a search engine with English terms for Chinese 
webpages. In their research, they discovered the most 
frequent pattern is when the translation appears immediately 
besides another (example (b)), covering 46%. Their results 
also indicates strict parenthetical pattern only covers at most 
29%. 
More recently, researchers have begun to explore the 
hyperlinks in web page as a source of bilingual information. 
Lu, et al. (2004) propose a method for mining translations of 
web queries from anchor text directly as well as in a 
transitive fashion. In a followup research, Cheng, et al (2004) 
propose a method for translating unknown queries with web 
corpora for cross-language information retrieval. Similarly, 
Gravano (2006) also propose systems and methods for using  
anchor text as parallel corpora for cross-language information 
retrieval. 
In a study more closely related to our work, Lin et al. 
(2008) proposed a method that performs word alignment 
between translations and phrases within parentheses in 
crawled webpages. In the paper, they also proposed an novel 
and automatic evaluation method based on Wikipedia. The 
main differences from our work is that in Lin et al. (2008), 
the alignment is done heuristically using a competitive 
linking algorithm proposed by Melamed (2000), while we use 
a learning based approach to align words and phrases. 
Moreover, they only consider Parenthetical Translations, thus 
may limit the coverage of translations that can be discovered 
on the Web. In their work, they assume translation occurs 
immediately before the left parenthesis. By using the 
parenthetical pattern only, they were able to extract a 
significant amount of translation pairs from downloaded 
webpages without a given list of target English phrases. 
However, by restricting surface pattern to parenthetic only, 
many translation pairs in the webpages may not be captured. 
These may include translation pairs that are further apart. In 
our work, we exploit surface patterns differently as a soft 
constraint, while use an approach similar to  Lin et al. (2008) 
to evaluate our results.  
In contrast to the previous work in phrase and query 
translation and transliteration, we present a learning-based 
approach that uses  annotated data to develop the system. 
However, we do not require human intervention to prepare 
the training data, but instead  makes use of language links in 
Wikipedia to automatically obtain labeled data. The 
annotated data is further augmented with features indicative 
of translation and transliteration relations, obtained from 
external lexical knowledge sources publicly available on the 
Web. The trained CRF sequence labeler is then used to find 
translations on the Web for a given term.   
4. METHOD 
Submitting an English phrase (e.g., “named-entity 
recognition”) to search engines to find translations or 
transliteration is a good strategy used by translators (Quoh 
2006). Unfortunately, the user has to sift through snippets to 
find the translations. Such translations usually exhibit 
characteristics related to word translation, word 
transliteration, surface patterns, and proximity to the 
occurrences of the original phrase. To find translations for a 
given term on the Web, a promising approach is 
automatically learning to extract phrasal translations or 
transliterations of phrase using the conditional random fields 
(CRF) model. To avoid  human effort of preparing 
annotated data for training the model, we use an automatic 
 13 
Wikipedia language links to obtain seed data, retrieve 
mixed-code snippets returned by a search engine, and 
augment feature values based on external knowledge sources. 
Our learning process is shown in Figure 1. 
4.3.1   Retrieving and tagging snippets 
In the first stage of the training phase, we extracted 
Wikipedia English titles and their Chinese counterpart using 
the language links as the seed data for training. We use the 
English titles to query a search engine (e.g., Google or Bing) 
with the target webpage language set to Chinese. This 
strategy will bias the search engine to return Chinese 
webpages interspersed with some English phrases. We then 
automatically label each Chinese character of the returned 
snippets, using the common BIO notation, with B, I, O 
indicating respectively beginning, inside, and outside of 
translations. An additional tag of E is used to indicate the 
occurrences of the given term (e.g., support vector machine).  
Figure 3. Examples tagged snippet for title pairs (support 
vector machine, 支持向量機) and (luminous flux, 光通量). 
1. ...1995/O 年/O 提/O 出/O 的/O 支/B 持/I 向/I 量/I 機/I 
(/O support/E vector/E machine/E，/O SVM/O )/O 以/O 
訓/O 練/O ... 
2. ... 發/O 光/O 原/O 理/O 不/O 同/O 。/O 光/B 通/I 量/I 
luminous/E flux/E 光/O 源/O 在/O 單/O 位/O 時/O 間
/O ... 
 
 
The output of this stage is a set of tagged snippets that 
can be used to train a statistical sequence classifier for 
identifying translations. Sample tagged snippet, 
automatically generated from bilingual Wikipedia titles (e.g., 
support vector machine, 支持向量機 zhi chi xiang liang ji), 
is shown in Figure 3. The E tags are designed to provide 
proximity cue for labeling the translation and capture 
common surface patterns of phrase and translation in mixed 
code data. For example, in Figure 3 the translation 支持向量
機 (zhichi xiangliang ji) is tagged with one B tag and four I 
tags, followed by the left parenthesis and three E tags. The 
translation 光通量 (guangtong liang) is tagged with one B 
tag and two I tags, immediately followed by two E tags. Such 
sequences (i.e. B I I I I O E E E, and  B I I E E) are two of 
many similar common patterns.  
Note that we do not attempt to produce word alignment 
information as done in Lin et al. (2008). In contrast, we will 
only use the BIO labeling scheme to indicate phrasal 
translations, leading to smaller number of parameters 
required to estimate during the training process. 
4.3.2   Generating translation feature 
We generate translation features using external bilingual 
resources. We use φ2 score proposed by Gale and Church 
(1991) to measure the correlations between an English word 
and a Chinese character: 
 
where e is an English word and f is a Chinese character 
occurring in bilingual phrase pairs. 
The scores are calculated by counting co-occurrence of 
Chinese characters and English words in the publicly 
available in bilingual dictionaries or termbanks. In the Table 
1, we show the contingency table calculated by counting 
co-ocurrence in Bilingual WordNet and NICT termbank for 
(向 xiang, vector), (量 liang, vector), (機 ji, machine). The 
statistical association between  an English word (e.g., vector) 
and its counterpart translation  (e.g., 向  (xiang)) is 
indicated by the high count of co-occurrence, as well as the 
lower values of two reverse diagonal cells. From the 
contingency tables we can calculate the corresponding φ2 
scores: 0.06530, 0.02880, and 0.09068. 
To generate features for each token, we calculate adjusted 
logarithm value of φ2: 
 
where e is a word in the given English phrase E, and f is 
the Chinese character in a snippet. This feature value is 
rounded to whole number in order to limit the number of 
distinct feature values. In Table 2 we show the φ2 scores of 
each Chinese character in snippets from searching Google 
with the given terms support vector machine and luminous 
 15 
force them align with the Romanized Chinese characters. We 
then calculate the conditional probability of Romanized 
Chinese character and its English counterpart. Example 
output of three Romanized Chinese characters and their top 
English counterparts is shown in Table 4. 
However, generating transliteration features for each 
Chinese character (Romanized) tends to produce a lot of false 
positives. Therefore, we assume that a named entity 
transliterated into at least two Chinese characters, and 
generate the transliteration feature of a Chinese character 
taking into consideration of the preceding and following 
characters. Admittedly, we will probably miss some 
transliteration cases, such as Jane Eyre and 簡愛 (jian-ai), 
but that represents only a small lose. 
In general, this strategy works quite well for our 
purpose. For example, given character sequence 喬布斯
(qiao-bu-si) and the term Steve Jobs, to calculate the 
transliteration score for the Chinese character 布 (bu), we 
calculate the probability of 喬布 (qiao-bu) and 布斯 (bu-si) 
being part of transliteration of Steve or Jobs: 
 
To calculate the conditional probability for the Chinese 
bi-characters qiao-bu given the English term jobs, we 
generate all substring xy of jobs, into which qiao-bu can be 
transliterated: 
 
With this probabilistic value, we then generate the 
transliteration feature values in a similar way as described in 
Section 3.2.2: 
 
We show an example of the data tagged with transliteration 
features in Figure 5. 
Figure 5. Example of transliteration features given 
Georges Braque to find the Chinese transliteration “喬治
·布拉克” 
1. ... 法-fa/0 國-guo/0 立-li/0 體-ti/2 主-zhu/2 義-yi/0 畫
-hua/0 家-jia/4 喬-qiao/7 治-zhi/7 ·/0 布-bu/8 拉-la/8 
克-ke/4 (/0 georges/E braque/E  )/0 ... 
2. 第-di/0 62/0 屆-jie/0 艾-ai/3 美-mei/3 獎-jiang/0 頒
-ban/0 獎-jiang/0 典-dian/0 禮-li/0 》/0(/0 the/0 62th/0 
Emmy/E Award/E )/0 
 
Figure 6. Example training data. 
 word distance TR TL label 
 第                   14 0 0 O 
 62 13 0 0 O 
62th 屆                   12 0 0 O 
 艾                   11 3 0 B 
Emmy 美                   10 3 0 I 
Award 獎                   9 0 5 I 
 頒                   8 0 0 O 
awarding 獎                   7 0 0 O 
 典                   6 0 0 O 
ceremony 禮                   5 0 0 O 
 》                   4 0 0 O 
 (                    3 0 0 O 
 the                  2 0 0 O 
 62th                 1 0 0 O 
 Emmy 0 0 0 E 
 Award 0 0 0 E 
 )         -1 0 0 O 
 
4.3.4   Generating distance feature 
Finally, we generate the distance feature, and train a CRF 
model. The distance feature is intended to exploit the fact that 
translations tend to occur near the given term, as pointed out 
by Nagata et al. (2001) and Wu et al. (2005).  
Therefore, we incorporate the distance as an additional 
feature type, to impose a soft constraint on the locational 
relations between a translation and its English counterpart.  
An example data showing all three kinds of feature and 
label is shown in Figure 6. This example shows that the given 
term Emmy Award, has a Chinese counterpart that is part 
transliteration (Ammy with a transliteration 艾美 ai-mei) and 
part translation (Award with the translation 獎 jiang). This 
is precisely the kind of cases our method is designed to 
handle, with use of both translational and transliterational 
features. 
 17 
We also used the English-Chinese Bilingual WordNet 
(see www.aclclp.org.tw/doc/bw_agr_e.PDF) and The 
bilingual WordNet, translated from the original Princeton 
WordNet 1.6 has 99,642 synset entries, with a total of some 
270,000 translation pairs.  
For transliterational features, we extract person or 
location entries in Wikipedia using such categories as “birth 
in ....” to find articles describing a person, and categories 
such as “Cities in ....” and “Capitals in ....” to find title 
describing a geographic location. A total of some 15,000 
bilingual names of persons and 24,000 bilingual names of 
places was obtained and forced aligned.  
To compare our method with previous work, we used a 
similar evaluation procedure as described in Lin et al. (2008). 
We ran the system and produced the translations for these 
2,181 test data, and automatically evaluate the results using 
the metrics of coverage and exact match precision based on 
the Wikipedia language links. We remove all search snippets 
from the wikipedia.org domain to ensure a strict separation of 
training and test datasets. 
This precision rate is an under-estimations, since a term 
may have many alternative translations that does not match 
exactly with one single reference translation. To give a more 
accurate estimate of real precision, we resorted to manual 
evaluation.  
We selected a small part of the 2,181 English phrases, and 
manually evaluated the results. Additionally, to see how 
effective the method can be used to help expand the scope of 
Chinese Wikipedia, we also carried out evaluation using 
Wikipedia English titles without a Chinese language link. 
We report the results of automatic evaluation in Section 
4.1, and the results of manual evaluation in Section 4.2. 
5.1   Automatic Evaluation 
In this section, we describe the evaluation based on the set 
of 2,181 English-Chinese title pairs extracted from Wikipedia 
as the gold standard, and automatically evaluate coverage and 
exact match. Coverage is measured by the percentage of titles 
with translations produced by our system.  
Table 7. Automatic evaluation results. 
system coverage exact match top5 exact 
match Full (En-Ch) 80.4% 43.0% 56.4% 
-TL 83.9% 27.5% 40.2% 
-TR 81.2% 37.4% 50.3% 
-TL-TR 83.2% 21.1% 32.8% 
LIN En-Ch 59.6% 27.9% not reported 
LIN Ch-En 70.8% 36.4% not reported 
LCD 
(En-Ch) 
10.8% 4.8% N/A 
NICT 
(En-Ch) 
24.2% 32.1% N/A 
 
When our system finds translations, we produce the most 
frequent translations as output, and check for exact match 
against the reference answer. Table 7 shows the results we 
have obtained as compared to the results reported by Lin et al. 
(2008). 
We explored the performance differences of the systems 
with different features. The systems evaluated are: 
• Full: our system that uses all three features. 
• -TL : our system that do without the transliteration feature. 
• -TR : our system that do without the translation feature. 
• -TL-TR : our system that only uses the distance feature. No 
external knowledge used.  
• LIN En-Ch : the results reported in the Lin et al. 2008 paper 
for their system targeting Chinese parenthetical translations. 
• LIN En-Ch : the results reported in the Lin et al. 2008 paper 
for their system targeting English parenthetical translations 
•  LCD : the LDC2.0 English to Chinese bilingual dictionary 
with 161,117 translation pairs. (reported in Lin et al. 2008) 
• NICT : the freely available NICT technical term bilingual 
dictionary with 1,138,653 translation pairs. 
Notice that although Lin et al. (2008) also used bilingual 
Wikipedia title pairs to evaluate, they used a earlier snapshot 
of the Wikipedia and worked with full webpages downloaded 
from the Internet, but without a list of given terms. We 
worked with the list of English terms given as input, but work 
with search engine snippets only. It is often difficult to 
compare systems with different experimental settings. 
Nevertheless, the evaluation results seem to indicate that our 
method compares favorably with the results reported in 
previous work. 
 19 
considered as correct. In Table 9, we shows extracted 
candidates and frequency counts for 8 example terms. 
Translation candidates are marked using the same A, B, 
P, and E tags as in Table 8, plus an additional tag, M, to 
indicate an exact match. For the given term money laundering,  
the system extract 27 exact matches (洗錢) and 2 correct 
alternative (洗黑錢) and only 1 erroneous output, from 30 
snippets returned from the search engine. While technical 
terms  like money laundering tend to have literal translations 
and result in more exact matches, movie titles are often 
translated into Chinese with completely different meanings. 
For example, the official Chinese title for the movie, Music 
and Lyrics in Taiwan is “K-歌-情人” (meaning karaoke, 
song, lovers respectively). The system was able to extract 18 
partial match and 2 exact match base on surface pattern and 
modest translation feature value for music and 歌 (ge, song) 
of 0.00013. For the given term colony, the system extracted 
菌落 (colony of fungi or bacteria), a correct translation with 
a different sense.  
Table 9. Extracted candidates and frequencies. 
given term freq candidate  
money laundering 27 洗錢 M 
 2 洗黑錢 A 
 1 洗錢宣傳 E 
Music and Lyrics 18 歌情人 P 
 2 K歌情人 M 
flyback transformer 14 變壓器 P 
 3 回掃變壓器 M 
 2 返馳式變壓器 A 
 2 返馳變壓器 A 
colony 15 菌落 B 
 2 科羅尼海島酒店 B 
 2 殖民地 M 
Osman I 8 奧斯曼 P 
 5 奧斯曼一世 M 
bubble sort  20 排序 P 
 19 泡排序 A 
 17 氣泡排序 M 
 9 泡沫排序 A 
 4 泡泡排序 A 
 
Other extracted answers include: transliteration, 科羅
尼海島酒店 (Island Colony), a name of a hotel, and the 
exact-match translation, 殖民地  (area under political 
control of another country). For the given term bubble sort, 
the partial translation 排序 makes the top-1 translation (with 
count of 20), while the top-2 to top-5 are either exact-match 
or acceptable translations. 
5.2.2   Evaluating entries without a language link 
In this section, the proposed method is tested on entries 
in the English Wikipedia that does not have a Chinese 
counterpart. We tried to sample a list of technical terms from 
the unlinked portion of Wikipedia. This is not easy because 
any volunteer can add a new entry, and administrator also 
add entries for managerial purposes (for example, stubs). We 
intended to test phrases that have significant Web 
representation, and are likely to be referred to with 
translations. Thus, we randomly selected 95 articles from 
English Wikipedia titles. The selection criteria are as follows:  
(1) Titles have a count of over 2,000 in Google Web 1T. 
(2) Titles must consist of at least three English words.  
(3) Titles must not be a proper name for songs, books, 
movies, and magazines.  
Table 10 shows the evaluation results. Interestingly, the  
system provides correct translations for over 50% of the 
cases, and correct or partial correct translations for almost 
90% of the cases. 
Let us note that this learning-based approach to mining 
translation and transliteration on the Web is an original 
contribution of our work. Previous work such as Wu et al 
(2005) and Lin et al. (2008) simply use occurrence statistics 
to identify translations, which is roughly equivalent to our 
translational or transliterational features (see Sections 4.3.2 
and 4.3.3). While Lin et al. (2008) use prefixes of 3 letters to 
provide a make-shift model of transliteration, we model the 
name-transliteration relations directly using an EM algorithm. 
Moreover, we also take note of their pattern of appearance to 
allows more effective extraction of relevant translations with 
distance features (see Section 4.3.3). This is a very important 
point to be made that combining inherent features, as well as 
derived from external knowledge sources in a machine 
learning model allow us to cover more relevant translations, 
while filter out many invalid translations.  
 21 
(9) Huang, C. R. 2003. Sinica BOW: Integrating Bilingual 
WordNet and SUMO Ontology. In Proceedings of the 2003 
International Conference on Natural Language Processing and 
Knowledge Engineering, 825-826. 
(10) Knight, K., Graehl, J. 1998. Machine Transliteration. 
Computational Linguistics 24(4), 599-612. 
(11) Koehn, P., Knight, K. 2003. Feature-Rich Statistical 
Translation of Noun Phrases. In Proceedings of the 41st 
Annual Meeting on Association for Computational Linguistics, 
311-318. 
(12) Kupiec, J. 1993. An Algorithm for Finding Noun Phrase 
Correspondences in Bilingual Corpora. In Proceedings of the 
31st Annual Meeting of the Association for Computational 
Linguistics, 17-22. 
(13) Kwok, K. L., Deng, P., Dinstl, N., Sun, H. L., Xu, W., Peng, P. 
and Doyon, J. 2005. CHINET: A Chinese Name Finder System 
for Document Triage. In Proceedings of 2005 International 
Conference on Intelligence Analysis. 
(14) Lafferty, J., McCallum, A. and Pereira,  F. 2001. Conditional 
Random Fields: Probabilistic Models for Segmenting and 
Labeling Sequence Data. In Proceedings of the Eighteenth 
International Conference on Machine Learning. 
(15) Lin, D., Zhao, S., Durme, B.V. and Paşca, M. 2008. Mining 
Parenthetical Translation from the Web by Word Alignment, In 
Proceedings of ACL 2008, 994-1002. 
(16) Li, Y., Grefenstette, G. 2005. Translating Chinese Romanized 
Name into Chinese Idiographic Characters via Corpus and Web 
Validation. In Proceedings of Conférence en Recherche 
d'Information et Applications (CORIA), 323-338. 
(17) Lu, W. H., Chien, L. F. and Lee, H. J. 2004. Anchor Text 
Mining or Translation of Web Queries: A Transitive 
Translation Approach. ACM Transactions on Information 
Systems 22(2), 242-269. 
(18) Melamed, I. D. 2000. Models of Translational Equivalence 
Among Words, Computational Linguistics. 
(19) Nagata, M., Saito, T. and Suzuki, K. 2001. Using the Web as a 
Bilingual Dictionary. In Proceedings of 39th. ACL Workshop 
on Data-Driven Methods in Machine Translation, 95-102. 
(20) Qu, Y.  and Grefenstette, G. 2004. Finding Ideographic 
Representations of Japanese Names Written in Latin Script via 
Language Identification and Corpus Validation. In Proceedings 
of the 42nd Annual Meeting of the Association for 
Computational Linguistics,183-190. 
(21) Quah, C. K. 2006. Translation and Technology, Palgrave 
Textbooks in Translation and Interpretation, Palgrave 
Macmillan. 
(22) Smadja, F. and McKeown, K. R. 1996.  Translating 
Collocations for Bilingual Lexicon. Computational Linguistics. 
(23) Sproat, R. and Shih, C. 1990. Statistical Method for Finding 
Word Boundaries in Chinese Text, Computer Processing of 
Chinese and Oriental languages. In Proceedings of Computer 
Processing of Chinese and Oriental Languages 4(4), 336-351.  
(24) Zhang, Y., Huang,  F. and Vogel, S. 2005. Mining 
Translations of OOV Terms from the Web through 
Cross-Lingual Query Expansion. In Proceedings of the 28th 
Annual International ACM SIGIR, 669-670. 
 2 
and	 evaluation）、語意學（semantics）、自動摘要（summarization）、社群媒體（social	 media）
、多語言處理（multilinguality）等等。機器翻譯最受關注，論文比例居最高。但是，今年似乎稍
稍有冷卻的跡象，分場的觀眾比往年少。反觀語意學的分場卻爆滿，許多人席地而坐。	 
	 
各分場的論文報告也十分精采，讓參與者在會議的短短幾天內，	 就可以對學界目前的發展有
了全面的了解。	 今年看到機器學習仍然是主要的研究方法，但是應用的方式，應用的範圍，
持續地擴大，有許多令人驚豔的研究成果。例如，分析句子中逗號的使用的統計模型，以及寫
作中後設言談、轉折語，以及內容文字的二元分類器。	 
	 
二、與會心得	 
在本次會議了解目前世界頂尖研究單位、研究主題的趨勢及方法，受益頗豐。我們的研究特別關
注電腦輔助語言學習與電腦輔助翻譯，著重深入分析語料庫與網路，提供學習語言、翻譯術語與
專有名詞的效率。此一特殊的研究角度與作法，也引起許多學者的高度興趣，對我們的研究有深
刻的印象。這表示我們對於語言學習領域的耕耘在國外已受到重視。在會議第二天	 poster	 展示
現 場 發 表 論 文 FLOW:A	 First-Language-Oriented	 Writing	 Assistant	 System （
http://nlp-service1.cs.nthu.edu.tw/FLOW/index.html），並在第三天口頭報告	 Learning to Find 
Translations and Transliterations on the Web。都有很好的機會和與會學者交換心得，包括微軟亞洲
研究院的林清祐博士、John	 Hopkins	 University	 的	 Callison-Burch	 教授、Yarowsky	 教授、新加坡
南洋理工大學的	 Francis	 Bond	 教授、ETS	 	 的	 Joel	 Tetreault	 博士等。	 
	 
我們發表海報論文、口頭報告論文時，得到參與討論者許多進一步研究發展的寶貴建議。許多
學者認為我們的雙語並呈的作法，可以同時幫助學習英語與華語。這樣可以呼應目前外國人學
習華語的需求的快速成長。	 
三、發表論文全文或摘要	 
近年來，最新的跨語言研究系統都依賴雙語平行語料庫。然而，即使是在一個非常大的平行語
料庫，還是很難找到術語、專有名詞的翻譯。在本文中，我們提出了一種新的方法，接受尋找
術語為輸入，在網路上尋找適當的翻譯。在我們的方法中，我們使用少量術語與其翻譯，用來
搜尋搜索引擎，以得到含有術語及其翻譯的中英混雜網頁摘要。然後，我們在摘要中自動註明
術語、翻譯、其他的標籤，加入特徵值，自動生成的可以訓練條件隨機場模型，產生自動標註
的程式。在執行時，我們一樣送出輸入的術語，得到中英混雜的網頁摘要，並執行，標註翻譯
的位置，辨識翻譯統計翻譯，最後輸出在摘要中出現最多次的翻譯。初步的實驗和評價顯示，
我們的方法，很明確地整合各種翻譯相關的特徵值，產生優於前人研究的系統。	 
四、建議	 
參加此研討會，不僅讓我們將我們的研究介紹到國際，並汲取相關研究人員寶貴的建議。我們接
觸到相當多知名外國學者，此次，與多位重量級的學者分享我們的研究，且彼此交換意見，讓我
們的研究能夠有更多的應用及改善，實屬難得的經驗。	 
	 
國科會補助計畫衍生研發成果推廣資料表
日期:2013/02/04
國科會補助計畫
計畫名稱: 結構化與分層化之統計式機器翻譯系統之研究
計畫主持人: 張俊盛
計畫編號: 98-2221-E-007-091-MY2 學門領域: 自然語言與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
