行政院國家科學委員會補助專題研究計畫 
■成果報告 
□期中進度報告 
 
總計畫：新世代自動語音辨識技術之研究– 第二階段--跨環
境之強健性語音屬性與事件偵測器研究  (3/3) 
Cross-Environment Speech Attribute and Landmark Detection for Robust 
Speech Recognition (3/3) 
 
 
計畫類別：□個別型計畫 ■整合型計畫 計畫
編號：97-2628-E-027-003-MY3  
執行期間： 99 年 8 月 1 日至 100 年 7 月31 日 
 
 
執行機構及系所：台北科技大學電子工程系 
計畫主持人：廖元甫 台北科技大學電子工程學系 副教授  
共同主持人： 
曾淑娟  中央研究院資訊所  副研究員  tsengsc@gate.sinica.edu.tw 
王新民 中央研究院資訊所 副研究員  whm@iis.sinica.edu.tw 
王逸如 交通大學電信工程學系  副教授  yrwang@cc.nctu.edu.tw 
 
 
計畫參與人員：郭志忠 工研院前瞻技術中尚 副主任  cck@itri.org.tw 
林奇嶽  (博士生)、張雅玟  (研究助理) 
成果報告類型(依經費核定清單規定繳交)：□精簡報告 ■完整報告 
本計畫除繳交成果報告外，另須繳交以下出國尚得報告： 
□赴國外出差或研習尚得報告 
□赴大陸地區出差或研習尚得報告 
□出席國際學術會議尚得報告 
□國際合作研究計畫國外研究報告 
 
 
處理方式：除列管計畫及下列情形者外，得立即公開查詢 
□涉及專利或其他智慧財產權，□一年□二年後可公開查詢 
 
 
中 華 民 國 100 年 10 月 27 日 
目錄 
 
一、簡介 
1.1 最小錯誤率麥克風陣列 
1.2 強健性新特徵參數求取 
1.3 聯合語者、雜訊環境與語音內容因素分析 
二、最小錯誤率麥克風陣列 
2.1  S-LIMABEAM 
2.2 最小錯誤鑑別式 
三、強健性新特徵參數求取 
3.1 最小分類錯誤法準則 
3.2 轉換矩陣最佳化演算法 
3.3 最小分類錯誤法準則應用於聲音事件分類 
四、聯合語者、雜訊環境與語音內容因素分析 
4.1 JSEC 模型表示 
4.2 JSEC 系統架構 
4.3 特徵空間的估計 
五、實驗結果 
5.1 最小錯誤率麥克風陣列實驗結果 
5.2 強健性新特徵參數求取實驗結果 
5.3 聯合語者、雜訊環境與語音內容因素分析實驗結果 
六、結論 
七、計畫成果 
7.1 已發表的論文 
7.2 相關的博碩士論文 
一、簡介 
2.1 S-LIMABEAM 
 
從以前所探討的陣列處理來介紹到延遲相加演算法，在以前的這些傳統陣列處理
都只有著重在如何依據研究者的需求條件來去設計陣列參數的調整來對應到的
照射圖樣（Array Pattern）[1]。大部分都是企圖去強健性其訊號的波形，比如消
除訊號上的雜訊、提高訊號的訊雜比……等等。但是如果一套語音辨識系統並不
是直接去調整語音訊號波形的準位資訊，而是透過大量的語音資料用來訓練語音
模型，然後再進入統計圖樣去分類。所以陣列響應的輸出訊號波形並不是我們所
在意的，而從語音訊號所抽取出來的語音特徵才是我們所要去注重的。並且利用
最大概似演算法（Maximum Likelihood，ML），當我們透過陣列響應的調整去改
變語音特徵使得其語音訊號更符合語音模型的統計圖樣，那麼即可提昇機率分數，
最後在貝氏決策理論的原則下辨識率即可提昇。 
    在此前提，一個已知答案的校正語句是必需的。如此可由已知的答案就可以
將校正語句所求得到的語音特徵去根據標準答案所對應的機率分佈去往機率較
高的方向所調整。此外陣列參數也已將目前的說話環境特性都已考慮進來，經過
調適之後，機率分數會上昇。而且陣列參數會對此時的環境所有特性進行調適。
無論是外界雜訊或是房間的殘響。此一調適是針對語音特徵的修正而非對傳統訊
號波形的強健性做調適。而且在校正語句時，必需包含一定數量的辨認最小單位，
也就是音節。其當進行測試時才能會有相對應的改善。 
    陣列響應與語音特徵最明顯而且最直接的關係為梅爾三角濾波器，下圖 2.1
為系統的運作方塊圖。其中 X0、X1 為麥克風 1、麥克風 2 的輸入。它們經過所
對應到每個濾波器的係數，在經過前處理之後，在進入梅爾三角濾波器，再去求
取相對應的能量就可以得到對數梅爾係數的語音特徵。然後到馬可夫模型的語音
辨認器進行辨識。並且藉由最大概似演算法來求取對數的機率分數，再來調適陣
列參數，使其已知答案的校正語句機率分數能有所提昇。 
 
距離給拉開，目標是為了把正確的語句與其他語句更不相似。我們所提出的最小
錯誤鑑別式演算法，其架構如圖2-2所示。調適方式為利用後級的語音辨認器，
計算錯誤率，並依錯誤率高低之回饋，以最小錯誤率準則（MCE）調整麥克風
陣列參數。 
    根據貝氏決策理論，對於任意的語音特徵參數觀察結果 X，並且將其歸類於
Cj 的風險，即條件風險（Conditional Risk）如下： 
,
1
( | ) ( | )
M
i j i j
j
R C X c P C X

                     （2.1） 
其中， ( | )
j
P C X ：表示語音特徵參數的觀察結果 X 時，文句
j
C 的事後機率。 
            
,j i
c ：為成本函數（Cost Function），即觀察屬於
j
c 卻分類於
i
c 所需
要付出的成本。 
            M ：所有可能分類，共M 個。 
在評估一個分類器的效能是使用期望損失（The Expected Loss），定義為： 
( ( ) | ) ( )L R C X X P X dX                    （2.2） 
其中， ( )P X ：語音特徵參數的觀察結果 X 的機率，它與辨認器無關。 
      ( ( ) | )R C X X：語音特徵參數的觀察結果 X ，並且其歸類於 ( )C X 的損失。 
    我們先去定義零壹損失函數（The Zero-One Loss Function）做為損失函數，
如（4.3）式，那麼期望損失將對應到辨認的錯誤率。 
,
0,
,
1,
j i
i j
c i j M
i j

 

                     （2.3） 
    零壹損失函數是代表說假如辨認正確則不用承擔任何損失否則必須承擔一
單位的損失。 
則（4.1）就可以改寫成： 
( | ) ( | )
1 ( | )
i
j
i j
i
R C X P C X
P C X


 

                     （2.4） 
( | ) ( )
( | )
( )
i i
i
P X C P C
P C X
P X
                    （2.5） 
其中， ( )
i
P C ：為語言模型（Language Model）有關。故通常定義為相同的值。 
   ( | )iP X C ：為事後機率，將已分類為 iC 的觀察結果為 X 的語音特徵參數的機
率。 
的機率分數更具競爭性的分類集合。，即為混淆分類（confusing classes）又或者
是競爭分類（competing classes）的集合。 
    在式子（4.9）裡，
i
M 並不是固定的集合，她會隨著語音模型參數 與觀察
的語音特徵參數 x 而作改變。並且該式在 是為不連續的[8]，所以在這利用梯度
下降法（gradient descent）裡並不適用，因此我們再去定義一個連續的誤辨值的
公式為： 
             1 /
,
1
( ) ( ; ) log exp[ ( ; ) ]
1
i i j
j j i
d X g X g X
M



 
     
 
       （2.10） 
    其中， 是一個正數，藉著改變 的數值，也就可以改變式子裡具有影響力
的競爭分類個數。假設令   ，則一個極端的誤辨值的數學式為： 
                       ( ) ( ; ) ( ; )
i i j
d X g X g X                    （2.11） 
    分類
j
C 是除了分類
i
C 以外，與觀察到的語音特徵參數結果 x 的機率分數最
大的類別當 ( ) 0
i
d X  有就代表說發生了分類錯誤，如果 ( ) 0
i
d X  代表說分類正
確為了要更進一步完成目標函數的定義我們就把誤辨值公式代入一個損失函數
（Loss Function）其等價於辨認錯誤率。數學式如下： 
                        ( ; ) ( ( ) )
i i
X d X                         （2.12） 
    損失函數一般為連續性的函式，範圍為[0,1]的函式，主要用在MCE的Sigmoid
數學是表示如下： 
                       
1
( )
1 exp( )
d
d 

  
                     （2.13） 
    其中， ：一般大於等於1， ：一般大於等於0。也就是說當辨認錯誤率越
高則損失亦越大。 
 
 
 
 
 
 
 
 
 
 
 
 
 
所以把第(3.2)式代入第(3.1)式就能讓辨識率和轉換矩陣具有函數關係。 
第二步是根據貝氏分類法則所定義的錯誤分類量測函數(misclassification 
measure)，如下式所示， 
     , ; , ;d Y g Y q g Y q                       (3.3) 
其中，  , ;g Y q   表示正確分類的辨認分數，  , ;g Y q  表示最接近正確分類的
錯誤分類分數，也就是除了正確類別之外，和音訊參數Y 相似度最大的類別。因
此當 ( ) 0d Y  ：表示辨認錯誤， ( ) 0d Y  ：表示辨認正確。 *q 與 q 分別為正確分
類與相似度最大之錯誤分類的最佳狀態序列(實作中將使用 NBest search)。 
第三步是把第(4.3)式嵌入成本函數(cost function)之中，就能用此函數來估測
分類決策的正確與否，定義如下式， 
 
 
1
( ) =
1 exp - ( )+
l d Y
d Y 
                   (3.4) 
其中，  和 會影響之後調適過程，通常設定成 1  和 0  。 
3.2 轉換矩陣最佳化演算法 
最後就是遞迴的調適程序，我們利用廣義機率遞減演算法 (Generalized 
probabilistic descent; GPD)進行迭代演算來實現最小錯誤分類法則，其通式如下式
所示。 
 1 ;n n n n n nU l Y                          (3.5) 
其中， 是主要調適的對象，而我們的調適對象是轉換矩陣
1~
[ ]
l l D
E e  ，所以可
以把第(4.5)式改寫成下式，針對每個基底向量的每個元素 j 做迭代運算， 
 
( )
;
( 1)= ( )-
lj lj
lj lj n
lj
e e n
l Y
e n e n
e


 


                 (3.6) 
其中，
( ; ) ( )
lj lj
l Y l d d
e d e
   

  
，推導過程如下所示， 
**
ˆ
*
2
1 ˆ
1
( , ; )
t
t
T
tl q kl
tj
tlj q kl
y
g Y q x
e T


 
        
 
              (3.13) 
同理， 
 
ˆ
2
1 ˆ
1
( , ; )
t
t
T
tl q kl
tj
tlj q kl
y
g Y q x
e T


 
    
 
 
              (3.14)                           
由(4.13)、(4.14)、(4.7)和(4.8)式可得 
( ; ) ( )
lj lj
l Y l d d
e d e
   

  
 
 
*
*
ˆ ˆ
2 2
1 ˆ ˆ
1
( ) 1 ( )
t t
t t
T
tl tlq kl q kl
tj
t
q kl q kl
y y
l d l d x
T
 

 
   
       
  
  
     (3.15) 
因此，最後得到遞迴式， 
( 1) ( )
lj lj n
e n e n      
*
*
ˆ ˆ
2 2
1 ˆ ˆ
1
( ) 1 ( )
t t
t t
T
tl tlq kl q kl
tj
t
q kl q kl
y y
l d l d x
T
 

 
   
      
  
  
  
 ( )
lj n
e n    
*
*
ˆ ˆ
2 2
1 ˆ ˆ
( )
1
( ) 1 ( )
t t
t t
lj lj
T T
T
t l t lq kl q kl
tj
t
q kl q kl
e e n
x e x e
l d l d x
T
 

 

     
      
  
  
  
透過以上這個遞迴式，我們就能調適出所求的轉換矩陣。 
3.3 最小分類錯誤法準則應用於聲音事件分類 
在實作上，必須先有已知的HMM模型和初始的轉換矩陣，我們會考慮先架在語
料濾波器的架構上先對上一章方式求出的轉換矩陣做微調，但是如此有可能會先
被不管是主成分分析或是線性鑑別分析的準則所框架住而無法發揮調整的效果，
所以實驗出來的結果效能提升應該是有限，所以之後可以考慮隨機的轉換矩陣來
運用，而模型的部分則是還是採用語料濾波器參數所訓練出的模型來使用。 
 
四、聯合語者、雜訊環境與語音內容因素分析 
 
 
我們進一步提出主要研究於強健性語音辨認上，我們提出聯合語者、雜訊環境與
語音內容因素分析(Joint Speaker and Noisy Environment and Speech Content Factor 
Analysis；JSEC)，主要是透過聯合因素分析，在特徵空間做即時語音辨認模型補
償(online recognition model compensation)，使得調適出來的模型與測試環境能夠
其中： 
msp、mnon：由語音參數串接而成的超向量，模型參數串接而成的超向量。 
xsp、xnon：特徵雜訊環境空間的投影量，初始假設平均值為 0 變異數為 1 的標準
高斯分佈。 
ysp：特徵語者特徵空間的投影量，初始假設平均值為 0 變異數為 1 的標準高斯分
佈。 
rsp：語音內容特徵空間的投影量，初始假設平均值為 0 變異數為 1 的標準高斯分
佈。 
zsp、znon：獨特因素特徵空間的投影量，初始假設平均值為 0 變異數為 1 的標準
高斯分佈。 
usp、unon：特徵雜訊環境特徵空間。 
vsp：特徵語者特徵空間。 
dsp、dnon：獨特因素特徵空間。 
gsp：語音內容特徵空間。 
 
4.2 JSEC 系統架構 
圖 4.2 是 JSEC 之系統流程圖，在訓練端，我們將訓練語料做 Force-alignment，
變成不同語音內容的片段語句。具有語音特性的片段語句訓練一個名為 speech
的聲學模型，我們便是利用這種方式來降低最後重建模型之參數量。並且對具有
語音特性的片段語句做標記分類，接著再依序估算雜訊、語者、說話內容，最後
是獨特因素的特徵空間，分別以 usp、vsp、gsp、dsp 表示。非具有語音特性的片段
語句，則訓練一個 non speech 的模型，並且僅對不同雜訊做標記，同於具有語音
特性的部分。接著依序估算雜訊與獨特因素特徵空間，分別以 unon 與 dnon 表示。 
speech model 的平均值所構成的超向量作為基準，就像是傳統的 MAP 語者調適
一樣，而由不同混合成分的共變異數∑𝑐串接而成的對角矩陣∑則可作為參數估測
的初始值。在模型參數估測之前先定義系統的機率假設。 
 
波氏統計 
我們先使用波氏統計[14]主要是以average speech model的平均值、變異數以及權
重來計算機率統計量。假設語者s以及語者特徵向量          ，對於每一個混合
成分c，我們定義波氏統計如下： 
   c t
t
N s c 
                                               (4.3) 
     c t t c
t
F s c Y m 
                                        (4.4) 
       
*
c t t c t c
t
S s diag c Y m Y m
 
   
 

                             (4.5) 
 
其中： 
 
     代表語者特徵向量於時間時落於混合成分的事後機率，而 𝑐代表average 
speech model中混合成分的c平均值。接著設N(s)為CF x CF的對角矩陣，其中的
對角區塊是由 𝑐(s) (c=1,...,C)所構成。設F(s)為CF x 1的超向量，是由每一個 𝑐(s) 
(c=1,...,C)串接而成。設S(s)為CF x CF的對角矩陣，其中的對角區塊是由 𝑐(s) 
(c=1,...,C)所構成。 
 
 
4.3.1 語者、雜訊環境、語音內容特徵空間 
 
求得波式統計量之後，由參考文獻[13]我們可以計算出具有語音特性的語者、雜
訊環境特徵空間，和非語音特性的雜訊環境特徵空間。 
語者、雜訊環境、語音內容特徵空間估算方法相同，但是語音內容算出來的隱藏
變數 rsp，必須儲存起來給測試端使用，因為在辨認的時候並不知道要說哪些語
音內容,先假設每一個 model 平均值在哪裡，再利用 ML 法重估超參數取得 gsp 之
後，即可將說話內容投影到對應位置。 
 
由於 usp, vsp, gsp, unon 其估計方法相同，以下我們以具有語音特性的特徵語者
(Eigen-voice)模型為例，求取 vsp。 
 
我們利用 Expectation Maximization(EM)演算法進行 10 次的疊代，反覆更新 vsp
使之趨於定值： 
 
隱藏變數ysp事後分佈 
先假設隱藏變數ysp(s)是平均值為0變異數為1的標準高斯分佈，當我們輸入語
音資料後就像MAP語者調適一樣會有不同的分佈。根據[12]假設，令         
  
Ƈ  ∑ 𝑑𝑖𝑎𝑔  (     𝐸[𝑧
 
     ])
                             
(4.13)  
  
 
  ∑        
                                           
(4.14) 
對每一個混和成分 c =1,...,C 、每一個混合成分的元素 f =1,...,F ，設 i = ( c -1)F + 
f，令 ui 代表 u 的第 i 列，而 Ƈi代表 Ƈ的第 i 列，因此特徵特徵空間 dsp 的更新
公式可表示成： 
 
  
𝑣𝑖𝑈𝑐  Ƈ𝑖   (i=1,....,CF)                                    (4.15) 
 
上述的表示式，可以從參考文獻[13]得到相關的表示。 
 
4.3.3 投影量 x, y, r, z 的估計 
當我們在訓練端得到求取出的具有語音特性的超參數 usp、vsp、gsp、dsp，以及非
語音特性的超參數 unon、dnon 後，測試端的參數再依照具有語音特性之影響因素，
經過估算而得到個別雜訊影響之投影量 xsp、語者影響之投影量 ysp、說話內容之
投影量 rsp、獨特因素之投影量 zsp，非語音特性之影響因素一樣經過估算而得到
投影量 xnon、znon。 
語音特性和非語音特性之投影量算法一樣，我們以估算語音部分的投影量為例： 
 
雜訊影響之投影量xsp 
假設                    
    ∑    
                                      
    
(4.16) 
xsp= E[x(s)] = 
     
     𝑢    
    ∑    
                                      
(4.17) 
 
語者影響之投影量ysp 
假設                    
    ∑    
                                 (4.18) 
 ysp= E[y(s)] =     
     𝑣    
    ∑    
                              (4.19) 
 
語音內容影響之投影量rsp 
假設    𝑟        𝑟      𝑟
    ∑   𝑟
         𝑟       𝑟                
(4.20) 
  rsp= E[r (s)] =    𝑟
     𝑔   𝑟
    ∑   𝑟
         𝑟                    
(4.21)
 
 
獨特因素之投影量zsp 
 
 
其中圖左上角為乾淨語音頻譜，右上角為被房間殘響特性影響後的頻譜，可見此
房間具有頻率選擇效應，且每個音素的頻譜都被拉長，因此會造成辨認率下降。
左下角則為經過延遲相加處理後的頻譜，可以看出其並沒有太多改善。最後右下
角的頻譜為經過MCE處理後的頻譜，我們可以發現其頻譜被校正到相當於原來接
近乾淨之頻譜，包括高頻部分被壓抑，與每個音素的延遲效應被改善。 
 
 
 
5.2強健性新特徵參數求取實驗結果 
 
使用的是 RWCP(Real World Computing Partnership)中所錄製的 105 種聲音，
聲音類型包括不同材質物體的敲擊聲，或是特殊器具的響聲，例如電話鈴聲等等。
錄製聲音的設定以及語料庫的內容概況如表 5.1 所示，音檔取樣頻率是 8khz，因
為每個事件檔案數並不相同，因此我們取各事件檔案數的 60％作為訓練用，其
餘則為測試。聲音來源有不同材質物品的敲擊，或是特定的動作和具有特徵的聲
音。每種聲音來源型態各取一種類在最後的附錄做細部分類說明。 
表 5.1  RWCP 語料庫 
聲音來源型態 種類 數量 總檔案數 
Collision 
Wood 12 1187 
Metal 10 1000 
Plastic 6 550 
5.2.1  最小分類錯誤法調適音訊特徵參數辨識結果 
表 5.3 所示為我們使用最小分類錯誤法去調適完轉換矩陣後，轉換出新參數
去測試的辨認結果。由於建構在 Filter230+PCA 的架構上去進行調適轉換矩陣的
程序，所以平均錯誤率或是每種維度設定的錯誤率比 Filter230+PCA 都降低了一
些。 
表 5.3  最小分類錯誤法調適音訊特徵參數錯誤率 
  
Filter230+MCE 
39 50 90 150 
20 2.59 2.26 1.73 1.62 
15 2.83 2.35 1.93 1.80 
10 3.87 3.40 2.76 2.50 
5 5.58 5.04 4.30 3.69 
0 9.24 8.61 7.36 6.25 
-5 16.54 15.93 15.86 14.06 
平均 4.82 4.33 3.62 3.17 
 
 
5.2.2 比較與分析 
我們先觀察本論文中所提到各種類音訊特徵參數的平均最佳錯誤率比較，如
表 5.4 所示，可以看出雖然傳統音訊特徵參數中，梅爾倒頻譜參數加上位移差分
化倒頻譜參數和 MVA 消雜訊處理過後，可以達到平均 4.13％的錯誤率，但是我
們利用圖樣特徵的概念所產生的語料驅動濾波器參數，尤其是經過主成分分析法，
錯誤率則能達到 3.29％，而再經過最小分類錯誤法調適，錯誤率能降到更低的。 
表 5.4  所有參數最佳平均錯誤率 
  平均最佳錯誤率 
MFCC 5.55 
MFCC+SDC 4.96 
MFCC+MVA 4.31 
MFCC+SDC+MVA 4.13 
Gabor160 7.08 
Gabor160+PCA 4.73 
Gabor350 6.95 
Gabor350+PCA 4.43 
 圖 5.1  各種類音訊特徵參數在不同測試組合錯誤率 
我們再從不同 SNR 的角度去觀察各種類音訊特徵參數的錯誤率，圖 5.4 為
SNR 是 20dB 和 15dB，圖 5.5 和 5.6 所示分別為 SNR 從 10dB 到-5dB 的錯誤率。
從以下三張圖觀察各種類音訊特徵參數對不同程度雜訊的抵抗能力。 
從圖 5.4 可以看出在低雜訊情況中，除了賈柏濾波器參數和線性鑑別分析出
的語料濾波器產生的音訊特徵參數之外都在 3％之內，其他種類的音訊特徵參數
的差距大約在 1.5％之內。 
從圖 5.5 開始到圖 5.6 可以發現當雜訊越高，假如傳統音訊特徵參數不做消
除雜訊的動作，與新音訊特徵參數的差距會越來越大，尤其是線性鑑別分析出的
語料驅動濾波器所產生的參數在高雜訊的情況下，錯誤率幾乎降到和主成分分析
和最小分類錯誤法調適出語料驅動濾波器所產生的音訊特徵參數一樣低，因此雖
然 Filter230+LDA 參數比較不能抵抗通道效應，但卻能有效抵抗高雜訊，所以若
能改善通道效應的影響，此音訊特徵參數應該能表現得更好。另外，主成分分析
和最小分類錯誤法調適出語料濾波器所產生的音訊特徵參數則不管在哪一種程
度的雜訊環境下都能表現得很好。 
 
0
1
2
3
4
5
6
7
8
SetA SetB SetC
E
rr
or
 R
at
e 
(%
) 
MFCC
MFCC+SDC
MFCC+MVA
MFCC+SDC+MVA
Gabor160
Gabor160+PCA
Gabor350
Gabor350+PCA
Filter230+PCA
Filter230+LDA
Filter230+MCE
 圖 5.4  各種類音訊特徵參數在高雜訊下錯誤率 
另外我們再針對新音訊特徵參數取不同維度時的錯誤率比較，如表 5.5 所示，
可以觀察出只要是跟主成分分析有關的參數，通常都會維度越高，錯誤率越低，
只有線性鑑別分析的例外，原因可能是濾波器選擇方式和原則不同，不過在維度
最低的情況下，線性鑑別分析出的語料驅動濾波器所產生的音訊特徵參數錯誤率
卻是最低，所以如果是在維度有限制的情況下，這個方法是較佳的。 
表 5.5 新音訊特徵參數不同維度錯誤率 
  39 50 90 150 
Gabor160+PCA 5.88 5.48 4.95 4.73 
Gabor350+PCA 6.20 5.78 4.74 4.43 
Filter230+PCA 5.11 4.63 3.75 3.29 
Filter230+LDA 4.33 4.43 4.44 4.20 
Filter230+MCE 4.82 4.33 3.62 3.17 
 
 
由表5.5的結果我們可以知道以圖樣特徵的概念來進行聲音事件分類是可行的，
但是賈柏濾波器必須手動設定濾波器參數，因此也可以知道是那些圖樣特徵，而
語料驅動濾波器是分析語料之後自動產生，因此我們取前幾個維度的語料驅動濾
波器進行繪圖，便於讓我們瞭解到底是哪些圖樣特徵。 
 
 
0
5
10
15
20
25
SNR0 SNR-5
E
rr
or
 R
at
e 
(%
) 
MFCC
MFCC+SDC
MFCC+MVA
MFCC+SDC+MVA
Gabor160
Gabor160+PCA
Gabor350
Gabor350+PCA
Filter230+PCA
Filter230+LDA
Filter230+MCE
每個乾淨音段先經特定的通道效應，再依各種訊雜比(SNR20、SNR15、SNR 10、
SNR 5、SNR 0 和 SNR -5dB)加上不同的加成性雜訊。 
訓練語料混合各種訊雜比及不同環境雜訊的複合情境訓練訓練模式。測試語料
部分則是依照原本 Aurora2自行建立的不同通道效應與加成性雜訊，共分成 A、
B、C 三種測試組合。 
本論文採用梅爾倒頻譜係數，及聲學模型為連續性密度的隱藏式馬可夫模型，
模型的狀態轉移只停留在原始狀態，及由左至右轉移到下一個相鄰的狀態。 
數字聲學模型的單位為全詞模型，十一個英文數字聲學模型(0～9 和 oh)。每個
聲學模型有 16 個狀態，每個狀態含 3 個高斯分布模型。除數字聲學模型外，還
有靜音(silence)模型和停頓(short pause)模型。辨識效能評估上，採取辨識詞錯誤
率，這考慮了刪除型錯誤、插入型錯誤和取代型錯誤。我們實驗分為二類：simple 
backend (3 mixture)和 complex backend (20 mixture)，如表 5.6 所示。 
 
Backend Speech model Silence model Short pause model 
Simple 
16 state, each state 3 
mixture 
3 state, each state 6 
mixture 
1 state, each state 6 
mixture 
Complex 
16 state, each state 
20 mixture 
3 state, each state 64 
mixture 
1 state, each state 64 
mixture 
表 5.6 複合情境訓練模式各種參數組合辨識結果 
 
另外我們實驗對照需要用到我們先前提出的 JSE 方法，其模型可表示為： 
M = m + ux(s) + vy(s) + dz(s)                                    (4.24) 
其中 m 為初始模型中所有平均值串成的超向量(super-vector)；v、u、d 分別為特
徵聲音、特徵雜訊、獨特因素之特徵空間；vy(s)、uxh (s)、 dz(s)分別為人聲、雜
訊、獨特因素在各自特徵空間的平均偏移量。和 JSEC 最大不同在於少考慮了講
話內容因素，模型的特徵向量比 JSEC 龐大。 
 
5.3.1 特徵空間分析 
我們想要得知此方法是否正確，能不能有效地將影響因素個別分開，所以先使用
simple backend 分析特徵空間並畫出特徵空間投影圖，目的是讓各種不同雜訊的
測試語料，能投影到正確的特徵空間上。在建構特徵空間時，我們先估算已經做
好分類資訊的統計量，接著再依照 u、v、g、d 之順序，逐一估算個別之特徵空
間。爲了方便分析，我們取前兩維的特徵向量作 x 軸和 y 軸，建構一個二維空間，
首先以雜訊類型(地下鐵、人聲、汽車、展覽會)做分析，我們採用七種 SNR(clean、
SNR20、SNR15、SNR10、SNR5、SNR0、SNR-5)做特徵空間分析，其結果如圖
5.8、圖四所示。 
 圖 5.10、simple backend JSEC 語者特徵空間投影圖 
 
在圖 5.10 中，我們可以看到其投影結果，很明顯依照語者的不同被分成兩邊，
我們以「o」與「+」的符號分別表示男生與女生的特性。 
最後是語音內容之特徵空間投影分析，其結果如圖 5.11 所示： 
 
圖 5.11、simple backend JSEC 語音內容之特徵空間投影圖 
我們可以看到圖 5.11 其投影結果，依照語音內容被分開，而比較類似的音，例
如 oh、four，似乎會比較靠近，而複合情境的點(digit)大約是在所有點的平均位
置。由以上三種不同影響因素之特徵空間投影圖，我們預測辨識效果應當不錯。 
 
 圖 5.14、simple backend JSEC 語者最佳維度比較圖 
 
圖 5.14 我們可以看到語音內容的最佳維度是 8 維。 
 
5.3.3 Complex backend 
由於調適模型中的變異數、轉移機率與權重影響很小，因此先假設與比較的 MVA、
JSE 相同。維度測試首先固定語者(S) 55 維，語音內容(T) 6 維，雜訊(N)分別以
14 維、20 維和 24 維做測試後可得以下結果： 
 
 
圖 5.15、complex backend JSEC 雜訊環境最佳維度比較圖 
 
由圖 5.15，我們可以看到雜訊的最佳維度是 20維，因此我們接著固定雜訊 20維，
語音內容一樣取 6 維，再取語者 55 維、60 維和 70 維，做測試可以得到以下結
果： 
6.18%
6.19%
6.20%
6.21%
S 60、T 6、N 20 S 60、T 8、N 20 S 60、T 10、N 20 
錯
誤
率
 
JSEC 語音內容最佳維度 
4.30%
4.35%
4.40%
4.45%
4.50%
4.55%
4.60%
4.65%
S 55、T 6、N 14 S 55、T 6、N 20 S 55、T 6、N 24 
錯
誤
率
 
JSEC 雜訊環境最佳維度 
 圖 5.18 simple backend 各系統方法不同環境之比較圖 
 
 
圖 5.19 simple backend 各系統方法不同 SNR 之比較圖 
 
但在調適模型所需的參數量的方面，如表 5.7和圖 5.20。我們可以從圖 5.20發現，
調適模型所需的參數量明顯比原本 JSE 的方法降低很多，JSE 比 JSEC 多 9 – 10
倍的參數量，而 JSEC 只比 MVA 多了 4 倍的參數量，效能更好、運算量更小。 
 
simple backend 
模型所需參數 MVA JSE JSEC 
mean 21528    21528   2808     
variance 21528    21528   2808     
weight 552    552   72     
Transition 3598    3598   358     
u - 430560   - 
5.00%
5.50%
6.00%
6.50%
7.00%
7.50%
8.00%
8.50%
9.00%
9.50%
10.00%
seta setb setc avg
MVA
HEQ
ETSI
JSE
JSEC
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
20 15 10 5 0 avg
MVA
HEQ
ETSI
JSE
JSEC
 圖 5.21 complex backend 各系統方法不同環境之比較圖 
 
圖 5.22 complex backend 各系統方法不同 SNR 之比較圖 
 
圖 5.21 和圖 5.22 可發現，mixture 從 3 拉到 20 之後，JSEC 錯誤率 4.37%，可
以和 JSE 錯誤率 4.46%相當，甚至更低一點。 
最後我們一樣統計了調適模型所需的參數量，如表三和圖 5.20 所示： 
 
complex backend 
模型所需參數 MVA JSE JSEC 
mean 147264    147264   22464    
variance 147264    147264   22464    
weight 3776    3776   576    
Transition 3598    3598   358    
u - 2945280   - 
v - 8835840   - 
d - 147264   - 
unon - - 149760    
3.80%
4.00%
4.20%
4.40%
4.60%
4.80%
5.00%
5.20%
seta setb setc avg
MVA
JSE
JSEC
0.00%
5.00%
10.00%
15.00%
20.00%
20 15 10 5 0 avg
MVA
JSE
JSEC
七、計畫成果 
 
7.1 已發表之論文 
 
1.  Yuan-Fu Liao and I-Yun Xu, “Subband Minimum Classification Error 
Beamforming for Speech Recognition in Reverberant Environments,” ICASSP, 2010 
2.  Yuan-Fu Liao, Chia-Hsing Lin , We-Der Fand, “Minimum Classification Error 
Based Spectro-Temporal Feature Extraction For Robust Audio Event Classification,” 
InterSpeech, 2011 
3. 王瑞璟、吳聖堂、廖元甫、林政賢，基於聯合語者與雜訊環境因素分析之強
健性語音辨認，NST 2010 
4. 林家興、陳彥廷、廖元甫，基於改良特徵參數之強健性聲音事件辨識系統，
NST 2009 
5. 林家興、方偉德、廖元甫、林政賢，基於鑑別式特徵參數求取之強健性聲音
事件分類，NST 2010 
6. 陳仕錚、吳明龍、廖元甫，快速次頻帶最大概似機率麥克風陣列演算法，NST 
2009 
7. 陳仕錚、莊皓程、廖元甫、廖憲正，快速次頻帶最大概似機率之麥克風陣列
演算法之進一步改良，NST 2010 
8. 許溢允、陳韋孙、廖元甫，次頻帶最小語音辨認錯誤率波束成形演算法，NST 
2009 
9. 黃重傑、呂紹禾、廖元甫，基於調頻諧波加雜訊模型之強健性基頻求取，NST 
2010 
10.楊志民、王瑞璟、廖元甫，基於機率主成份分析之強健性語音辨認，NST 2009 
 
7.2 相關的碩博士論文 
 
1. 許溢允，次頻帶最小語音辨認錯誤率波束成形演算法，台北科技大學，2009 
2. 林家興，基於鑑別式特徵參數求取之強健性聲音事件分類，台北科技大學碩
士論文，2010 
3. 吳聖堂，聯合語者、雜訊環境與說話內容因素分析之強健性語音辨認，2011 
• Speech&and&SL&Generation,&Speech&Synthesis&
• ASR&6&Signal&Processing&and&Acoustic&Modeling&
• ASR&6&Architecture,&Search&and&Linguistic&Components&
• ASR&6&Robustness&and&Adaptation&
• LVCSR&and&its&Applications&
• Technologies&and&Systems&for&New&Applications&
• Spoken&Dialogue&System,&Spoken&Language&Understanding,&Speech&Translation,&
and&Information&Retrieval&
• Application,&Evaluation,&Standardization,&SL&Resources&
v%ph?Kæ®×/ãáÙöã
 
PHONE&BOUNDARY&DETECTION&USING&SAMPLE4BASED&ACOUSTIC&
PARAMETERS&
You4Yu&Lin,&Yih4Ru&Wang&and&Yuan4Fu&Liao&
A"sample)based"phone"boundary"detection"algorithm"is"proposed"in"this"paper."Some"
sample)based"acoustic"parameters"are"first"extracted"in"the"proposed"method,"
including"six"sub)band"signal"envelopes,"sample)based"KL"distance"and"spectral"
entropy."Then,"the"sample)based"KL"distance"is"used"for"boundary"candidates"
pre)selection."Last,"a"supervised"neural"network"is"employed"for"final"boundary"
detection."Experimental"results"using"the"TIMIT"speech"corpus"showed"that"EERs"of"
13.2%"and"15.1%"were"achieved"for"the"training"and"test"data"sets,"respectively."
Moreover,"43.5%"and"88.2%"of"boundaries"detected"were"within"80)"and"240)sample"
error"tolerance"from"manual"labeling"results"at"the"EER"operating"point."
 
2 Ï@±õæ¸  
 
æ,þæ¸Ï@°~±õ	§cËe£ÿ ISCA Fellow	×o
-^<âüÓ¦¯è¤	@£ÿ¢þÙJó¶çËe
½V¯ keynote speaker 
3 ±õäÏã survey 
 
3.1 Keynote 
 
RU[|! keynote speaker	5z

! Steve Young, University of Cambridge	äā Still Talking to Machines 
(Cognitively Speaking) 
 
 
! Tohru Ifukube, Research Center for Advanced Science and Technology, The 
University of Tokyo	äā  Sound-based Assistive Technology Supporting 
"Seeing", "Hearing" and "Speaking" for the Disabled and the Elderly 
 
 
! Chiu-yu Tseng, Institute of Linguistics, Academia Sinica, Taiwan 	äā 
Beyond Sentence Prosody 
 JøalS³âýq=É HPGýqG!J«Í.0 üÀ(SYL) 
ýqß(PW)ýq³â(PPh)ECÅ(BG)ýq=Å(PG)9âÁ(Discourse) 
 
3.2 Paper survey 
 
/Ññ.Kæ	v%ìÐé¯¼Øõ modulation 
frequency¡s8:
 
 
An&Auditory&Based&Modulation&Spectral&Feature&for&Reverberant&Speech&
Recognition&
Hari&Krishna&Maganti&(Fondazione&Bruno&Kessler&6&IRST,&Trento,&Italy)&
Marco&Matassoni&(Fondazione&Bruno&Kessler&6&IRST,&Trento,&Italy)&
In"this"paper,"an"auditory"based"modulation"spectral"feature"is"presented"to"improve"
automatic"speech"recognition"performance"in"presence"of"room"reverberation."The"solution"
is"based"on"extracting"features"from"auditory"processing"characteristics,"specifically"
gammatone"filtering"based"long)term"modulation"spectral"features"to"reduce"sensitivity"to"
environmental"noise"and"further"preserve"the"important"speech"intelligibility"information"in"
the"speech"signal"essential"for"ASR."Experiments"are"performed"on"Aurora)5"meeting"
recorder"digit"task"recorded"with"four"different"microphones"in"hands)free"mode"at"a"real"
meeting"room."For"comparison"purposes"the"recognition"results"obtained"using"standard"
ETSI"basic"and"advanced"front)ends"and"conventional"features"with"standard"feature"
compensation"are"tested."The"experimental"results"reveal"that"the"proposed"features"provide"
reliable"and"considerable"improvements"with"respect"to"the"state"of"the"art"feature"extraction"
techniques."
Kailash&Patil&(Johns&Hopkins&University)&
Sriram&Ganapathy&(Johns&Hopkins&University)&
Nima&Mesgarani&(Johns&Hopkins&University)&
Hynek&Hermansky&(Johns&Hopkins&University)&
In"this"paper"we"propose"to"incorporate"features"derived"using"spectro)temporal"receptive"
fields"(STRFs)"of"neurons"in"the"auditory"cortex"for"the"task"of"phoneme"recognition."Each"
of"these"STRFs"is"tuned"to"different"auditory"frequencies,"scales"and"modulation"rates."We"
select"different"sets"of"STRFs"which"are"specific"for"phonemes"in"different"broad"phonetic"
classes"(BPC)"of"sounds."These"STRFs"are"then"used"as"spectro)temporal"filters"on"
spectrograms"of"speech"to"extract"features"for"phoneme"recognition."For"the"phoneme"
recognition"task"on"the"TIMIT"database,"the"proposed"features"show"an"improvement"of"
about"5%"over"conventional"feature"extraction"techniques."
 
Cross4lingual&and&Multi4stream&Posterior&Features&for&Low4resource&LVCSR&
Systems&
Samuel&Thomas&(Johns&Hopkins&University)&
Sriram&Ganapathy&(Johns&Hopkins&University)&
Hynek&Hermansky&(Johns&Hopkins&University)&
We"investigate"approaches"for"large"vocabulary"continuous"speech"recognition"(LVCSR)"
system"for"new"languages"or"new"domains"using"limited"amounts"of"transcribed"training"
data."In"these"low"resource"conditions,"the"performance"of"conventional"LVCSR"systems"
degrade"significantly."We"propose"to"train"low"resource"LVCSR"system"with"additional"
sources"of"information"like"annotated"data"from"other"languages"(German"and"Spanish)"and"
various"acoustic"feature"streams"(short)term"and"modulation"features)."We"train"multilayer"
perceptrons"(MLPs)"on"these"sources"of"information"and"use"Tandem"features"derived"from"
the"MLPs"for"low"resource"LVCSR."In"our"experiments,"the"proposed"system"trained"using"
only"one"hour"of"English"conversational"telephone"speech"(CTS)"provides"a"relative"
improvement"of"11%"over"the"baseline"system."
 
Latent&Perceptual&Mapping:&A&New&Acoustic&Modeling&Framework&for&Speech&
Recognition&
Shiva&Sundaram&(Deutsche&Telekom&Laboratories,&Ernst6Reuter6Platz67,&Berlin&
10587.&Germany)&
Jerome&Bellegarda&(Apple&Inc.,&3&Infinte&Loop,&Cupertino,&95014&California.&USA.)&
While"hidden"Markov"modeling"is"still"the"dominant"paradigm"for"speech"recognition,"in"
recent"years"there"has"been"renewed"interest"in"alternative,"template)like"approaches"to"
acoustic"modeling."Such"methods"sidestep"usual"HMM"limitations"as"well"as"inherent"
issues"with"parametric"statistical"distributions,"though"typically"at"the"expense"of"large"
PHONE BOUNDARY DETECTION USING SAMPLE-BASED ACOUSTIC 
PARAMETERS 
You-Yu Lin1, Yih-Ru Wang1 and Yuan-Fu Liao2 
1 Institute of Communication, National Chiao Tung University, 
 Hsinchu, Taiwan, ROC 
2 Department of Electronic Engineering, National Taipei University of Technology,  
Taipei, Taiwan, ROC 
rossi0927.cm97g@g2.nctu.edu.tw, yrwang@mail.nctu.edu.tw, yfliao@ntut.edu.tw 
 
Abstract 
We proposed a sample-based phone boundary detection algorithm. 
Some sample-based acoustic parameters are extracted in the 
proposed method, including six sub-band signal envelopes, 
sample-based KL distance and spectral entropy. Then, the sample-
based KL distance is employed for boundary candidates pre-
selection. Next, a supervised neural network is trained for final 
boundary detection. The experimental results using the TIMIT 
speech corpus show that EERs of 13.2% and 15.1% were achieved 
for the training and test data, respectively. Moreover, 43.5% and 
88.2% of boundaries detected were within 80- and 240-sample 
error tolerance from manual labeling results at the EER operation 
point. 
 
Index Terms: Speech segmentation, speech analysis.  
1. Introduction 
Automatic phonetic segmentation is a historic and basic problem in 
speech signal processing. Although lots of research had been done 
in the past [1], an automatic phonetic segmentation algorithm with 
high accuracy and precision is still a state-of-the-art work. Without 
knowing the text of the speech signal, it becomes a phone 
boundary detection problem which is more difficult than the phone 
boundary alignment problem. An accurate phone boundary 
detector is important and essential for speech processing 
engineering and linguistics. 
The most popular approach to automatic phonetic 
segmentation, when knowing the text of the speech signal, is the 
HMM-based time-alignment method. The performance is usually 
evaluated by using the percentage of detected boundaries with 
errors smaller than 20ms as a figure of merit. In [2], the minimum 
boundary error (MBE) criterion was used in the training of the 
HMM method. For TIMIT acoustic-phonetic continuous speech 
corpus, the minimum MBE-trained HMMs can identify 79.75% of 
human-labeled phone boundaries within a tolerance of 10ms, 
compared to 71.23% achieved by the conventional ML-trained 
HMMs. Moreover, there are only 7.89% of automatically labeled 
phone boundaries having errors larger than 20ms. Besides, some 
other techniques, like SVM, fuzzy logic and neural network [3], 
were used to refine the HMM aligned boundaries in order to get 
more accurate boundary positions. 
In the automatic boundary detection problem, without 
knowing the text of the speech signal, the rate of acoustic signal 
change is the most important cue for decision making. In [4], the 
spectral transition measure, which is in fact the norm of delta 
MFCC, was used to find the phone boundaries. 15.4% miss 
detection (MD) and 22.0% false alarm (FA) rates were achieved 
for the TIMIT training data set. In [5], the model selection 
technique, DISTBIC, was used to perform the phone boundary 
detection. The DISTBIC first used the Kullback-Leibler (KL) 
distance to find the boundary candidates, and then employed the 
Bayesian information criterion (BIC) to further verify those 
candidates. 25.7% MD and 23.3% FA rates were achieved for the 
NTIMIT database. 
In the past studies, they adopted the frame-based approach to 
use acoustic features like MFCCs in phone boundary detection. 
The main drawback of the approach lies in the incapability of the 
frame-based features to model the rapid changes in speech signal. 
Besides, the time resolution of the frame-based approach is too 
coarse for phone boundary detection. And, it doesn’t need such a 
high frequency resolution as used in frame-based system. This 
motivates us to propose a sample-based phone boundary detection 
algorithm in this paper. 
The paper is organized as follows. Section 2 presents the 
proposed sample-based phone boundary detection algorithm. Its 
performance is examined by simulations discussed in section 3. 
Some conclusions are given in the last section. 
2. The proposed sample-based phone 
boundary detection algorithm 
In the proposed sample-based phone boundary detection algorithm, 
speech signal is processed sample-by-sample to extract some 
broadband spectral parameters. The proposed sample-based KL 
distance and spectral entropy are extracted.  They and/or their delta 
terms can be used to model the rate of signal change. Then, the 
proposed sample-based KL distance is then used to determine the 
boundary candidates. Lastly, a supervised neural network is used 
for detecting phone boundary from those candidates. In the 
following subsections, we discuss these three parts in detail. 
 
(1) Features from current boundary candidates :  
( )
( ) 0
[ ], [ ], [ ] ; 1, , 1,
[ ], [ ]; 1, ,6 , [ ]
KL j j j
i k i k k
d c H c H c j k k k
E c E c i E c
Δ = − +
Δ = Δ⋯
 
, where [ ]jH cΔ and [ ]i kE cΔ  are the delta terms of spectral 
entropy and ith sub-band normalized envelope, 
(2) Features from adjacent segments for boundary candidate: 
( )1 1 1 1[ , ], [ , ]; 1, ,6 , ,i k k i k k k k k kES c c ES c c i c c c c− + − += − −⋯ ,  
(3) Two indicators used to indicate the first and last candidate 
in an utterance.  
 
Unlike the previous studies using a simple threshold value for 
acoustic parameter [4] or using model selection criterion [5], a 
supervised multi-layer perceptron (MLP) was trained for the phone 
boundary detection in our proposed system. The problem is how to 
decide the target of MLP classifier, in other word, to decide the 
phone boundary from an objective measure. Even for a database, 
the phone boundaries were manually labeled. Those manually 
labeled boundaries may not be consistent with the boundaries 
found from an objective measure such as the sample-based KL 
distance used in pre-selection procedure.  
An iterative target selection and MLP training algorithm was 
used for the phone boundary detector. For each manually labeled 
phone boundary, im , a target for detector was chosen from one of 
the candidates in the interval [ ],i im mδ δ− + , where δ denotes 
the allowable tolerance between the manually and automatically 
detected phone boundaries. The procedures of the training 
algorithm were  
(1) Select initial targets from the candidates with largest 
sample-based KL distance, [ ]KL id c , where 
[ ],i i ic m mδ δ∈ − + ;  
(2) Train the MLP-based phone boundary detector using the 
given targets; 
(3) Select the candidate with largest output of MLP detector, 
i.e. the most probable candidate, in [ ],i im mδ δ− + as the 
new target of MLP-based detector. 
Repeat (2) and (3) until convergence. 
3. Experiments and results 
The TIMIT speech corpus was used to evaluate the effectiveness of 
the proposed sample-based phone boundary detection algorithm 
proposed in this paper. The numbers of phone boundaries in the 
training and testing parts of TIMIT were 172460 and 62465. The 
total numbers of samples were 2.27*108 and 8.29*107 for training 
and testing, respectively. In average, there were 12.2 phone 
boundaries in 1 sec, or one boundary per 1310 samples. 
First, the envelopes of sub-band signals were found from the 
speech signal. Then, the sample-based KL distance and spectral 
entropy were extracted from speech signal. An example was shown 
in Fig. 2. The threshold value of KL distance was properly chosen, 
in our experiment, to pre-select the boundary candidates. In all, 
473353 and 182858 candidates of training and test data were 
selected. Only 0.21% speech samples, or in average one of 476 
samples, were treated as boundary candidates. And those 
candidates were sent to the following MLP-based boundary 
detector. 
The MLP-based phone boundary detector was trained using the 
interactive target selection and MLP training algorithm mentioned 
above. The number of hidden neurons was set to 75. An example 
of MLP detector output was given in Fig. 2, the output of MLP is 
in the range [-1, 1]. The output of MLP detector will be larger 
when the signal changes rapidly. 
A threshold value for detector output can be used to decide the 
phone boundaries result. The MD (Miss Detection) vs. FA (False 
Alarm) rate curves of training and test data were shown in Fig. 4. 
By the definition in [5], FA was defined as (number of false 
alarm)/(number of actual boundaries + number of false alarm) in 
Fig. 4. 13.2% and 15.1% EERs were achieved for training and test 
data, respectively. In comparison to the performance in [4], 15.4% 
MD and 22% FA rates were shown for TIMIT training data set; 
about 20% EER reduction was achieved. Beside, in frame-based 
approach, it can’t distinguish the boundaries within the same 
frames, thus only one answer was needed to detect. The FA shown 
in frame-based will be a little lower than actual value.  
 
 
Fig. 4. MD vs. FA rates for the proposed phone boundary detector, 
where (15.4%,22%) indicated the performance in [4]. 
 
The normalized histogram of the absolute deviation between 
automatically detected boundaries and manually labeled result was 
also shown in Fig. 5. 43.5% and 88.2% of boundaries detected 
were within 80- and 240-sample error tolerance from manual 
labeling results at the EER operation point.  While in [4], 27% 
boundaries detected was within the same frame of the manually 
labeled boundaries and 70% was within one frame (10 msec) 
tolerance. In our method, after converted the answers into frame-
based resolution, 42% boundaries detected was within the same 
frame of the manually labeled boundaries and 87% was within one 
frame tolerance. We can see that the accuracy of our sample-based 
boundary detection method was also better.  
Some preliminary error analysis was done. First, some phone 
boundaries were essentially difficult to detect because the acoustic 
signal was changing smoothly. At the EER operation point, 28.6%, 
of the boundaries between {vowels, semivowels, glides} and 
{vowels, semivowels, glides} are miss detected. Basically, the 
filter band [6] used in this paper, was designed for landmark and 
pronunciation manner and landmark detection. The MD rates for 
other boundaries between phones with same pronunciation manner 
were also higher than average. The MD rate for fricative-fricative 
boundaries was 30.5%; nasal-nasal was 51.8%. Finally, lots of 
國科會補助計畫衍生研發成果推廣資料表
日期:2011/11/02
國科會補助計畫
計畫名稱: 跨環境之強健性語音屬性與事件偵測器研究
計畫主持人: 廖元甫
計畫編號: 97-2628-E-027-003-MY3 學門領域: 自然語言處理與語音處理
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
