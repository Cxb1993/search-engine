 2 
1 Introduction 
A major objective of a clustering strategy is 
to find concept structures in data. Typically, data 
clustering represents partitioning or 
decomposing a data set into classes so that the 
data points in one group are similar to each other 
and are as different as possible from the data 
points in other classes. Data clustering approach 
can help elucidate the relationship among and 
distribution of massive data sets. Generally, 
clustering can be performed using partitioning 
clustering algorithms, hierarchical clustering 
algorithms, density-based clustering algorithms 
and grid-based clustering algorithms [1], [5]. 
The greatest challenge faced by density-based 
clustering algorithm is the setting of the 
parameters and the running performance. The 
input parameters involving the region in the 
object for searching neighborhoods and the 
minimum number of objects are difficult to 
determine by users, and the parameters will 
detrimentally affect the results of clustering. 
This process may therefore take too much time. 
DBSCAN is one of the most well-known 
existing density-based approaches [2]. The 
grid-based clustering approach identifies clusters 
as a multi-solution grid data structure. It 
segments the data space into pieces like 
grid-cells. Grid-based clustering schemes deal 
with these grid-cells as data points, thus 
grid-based clustering is more computationally 
efficient than other forms of clustering. 
Although the clustering of data sets using 
grid-based clustering algorithm is very fast, it 
cannot eliminate the problem of indentation 
boundaries. CLIQUE is one of the most 
well-known existing grid-based methods [1]. 
However, its cluster boundaries are either 
horizontal or vertical, owing to the nature of the 
rectangular grid. Additionally, the number of the 
grid-cells is so uncertain that the number of cut 
grid-cells is unknown. This work presents two 
new clustering algorithms, named 
DG-CLUSTERING and GF-DBSCAN, which 
combine a grid-based algorithm with the 
density-based approach for clustering, 
respectively. Experimental results indicate the 
proposed algorithms perform very well and are 
simple to implement. 
 
2 Related Works 
DBSCAN is one of the earliest clustering 
algorithms that are concerned with density [2]. 
Its execution requires two parameters (Eps, 
MinPts), where Eps denotes the region in the 
object for searching neighborhoods, and 
Minpts represents the minimum number of 
objects. The data points in a high-density 
cluster can be expanded similarly until all of 
the data points have been visited. DBSCAN 
can deal with data in irregular shapes very 
effectively, and can also filter noise from the 
data set. Establishing the DBSCAN parameters 
is difficult. Users commonly do not know 
whether the values of the parameters are 
optimized. Additionally, DBSCAN must 
expand all data points, which takes 
considerable time. There are two related 
well-known approaches named IDBSCAN [3] 
and FDBSCAN [4]. 
Objects in grid-based algorithm are 
distributed into multiple dimensions. Each 
grid-cell has many points. Grid-cells are placed 
into clusters and according to dimension objects. 
 4 
An independent object is counted as a single 
object. Three above objects are in the same place 
are called a crowd. Data sets are divided into 
squares. If a square contains more than three data 
sets, that square is a valuable grid-cell. When data 
sets loose density, large squares can be used. When 
data sets have a high-density, small squares are 
utilized. For example, the right half of the data sets 
have a high-density and left half of the data sets 
have a loose-density in Fig. 4. Fig. 5 displays the 
results of dividing the data sets using a 4x4 
grid-cell (cutting parameter). Each grid-cell has 
more than three of the above data sets and may be 
connected as a cluster. Otherwise, the data sets will 
be deemed as noise.  
When the grid-cells with high-density have 
been identified, it needs to connect grid-cells with 
high-density toward up, down, left and right four 
directions to construct a cluster, as Figs. 6 and 7 
shown. Some conditions demand special attention. 
If the number of grid-cells with neighbors is fewer 
than three, then this grid-cell is regarded as noise, 
as Fig. 8 revealed. 
 
Fig. 4. An instance depicts the right half of the data 
sets have a high-density and left half of the data sets 
have a loose-density. 
 
Fig. 5. An instance that displays the results of 
dividing the data sets using a 4x4 grid.  
 
Fig. 6. An instance to illustrate the extended 
directions of grid-cell with high-density. 
 
 
Fig. 7. An instance to describe the grid-cells with 
high-density connects to the other grid-cells with 
high-density and then how to construct a cluster. 
 
Fig. 8.  An instance to explain if the number of 
grid-cells with neighbors is fewer than three, then 
this grid-cell is regarded as noise. 
 
This section illustrates the DG-CLUSTERING 
clustering algorithm as follows: 
DG-CLUSTERING(gct) 
SetOfCells = CutDataSets(SetOfPoints,gct) 
For i From 1 To SetOfPoints.size Do 
   Point = SetOfPoints.get(i) 
Cell = JudgeCellPosition(Point, SetOfCells) 
   Point.cellId = Cell.index 
Cell.accumulate++ 
If Cell.accumulate >=3 
      Cell.valid = true 
 6 
MinPts are compared with other data in 
neighboring cells to generate a new cluster. 
Those clusters have different areas. The search 
points of Cluster 3 are seen only in the 
neighboring cells. In Fig.12(b), the new cluster 
embraces the edges of other clusters, which have 
core points in other clusters. Accordingly, these 
clusters can be merged. Fig.12(c) displays the 
result of clustering, in which three clusters are 
formed. 
 
Fig. 11. The cluster merging process. 
Fig. 12. The clustering procedure of GF-DBSCAN. 
This section describes in detail the steps of the 
GF-DBSCAN algorithm. 
Step 1. Select datasets and initialization 
parameters. The GF-DBSCAN clustering 
algorithm requires input two parameters: the 
radius (Eps), and the minimum number of points 
(MinPts), as described below: (1) Radius (Eps): 
this parameter has two functions. First, the 
radius of the data points can be considered as the 
core point of the neighborhood points. Second, 
the radius determines the grid-cell length of each 
side. In other words, each cell has the radius Eps. 
(2)Minimum number of points (MinPts): To form a 
cluster, the point searching the neighborhoods points 
must be bigger than or be equal to the points. 
Otherwise, it is viewed as a noise point. 
Step 2. Partitioning the data into several cells. Data 
are partitioned into cells according to the parameter 
Eps. Each point must be defined in a specific cell. 
Each cell has a unique serial number.  
Step 3. Read data and search the neighborhoods cells. 
Read the data in point order until all points are 
defined, then stop the algorithm. The search scope of 
a point is obtained as soon as the point is read.  
Step 4. Determine whether the number of points is 
less than MinPts. The number of data points is 
determined from the radius scope. If the number of 
points in a group is greater than or equal to the 
 8 
density-based approach is the setting of the 
parameters and the running performance. The 
proposed DG-CLUSTERING clustering algorithm 
may simplify setting of the parameters by users. This 
study presents a simple and powerful rapid cluster 
method. In addition, this proposed 
DG-CLUSTERING algorithm has very high 
clustering accuracy and filtering rates, and is faster 
than the well-known DBSCAN and CLIQUE 
schemes. It is observed that the proposed 
DG-CLUSTERING clustering algorithm is feasible 
in various data mining applications. 
    
(a) Data set1     (b) Data set 2   (c) Data set 3  
  
(d) Data set 4    (e) Data set 5 
Fig. 13. The original datasets with 575,000 objects 
for experiment. 
 
Table 1: The clustering results using the proposed DG-CLUSTERING algorithm by five datasets 
with 575,000 objects, and all with 15% noise. Time cost denotes execution time of cost ((in 
millisecond: ms); CCR represents the clustering correctness rate (%), while NFR indicates the noise 
filtering rate (%). 
Dataset Cluster No. NFR CCR Time cost 
Dataset1 10 98.969% 99.919% 312 (ms) 
Dataset2 4 99.093% 99.792% 344 (ms) 
Dataset3 5 99.267% 99.583% 343 (ms) 
Dataset4 4 99.104% 99.798% 348 (ms) 
Dataset5 2 99.245% 99.570% 342 (ms) 
 
Table 2: Comparisons with CLIQUE and DG-CLUSTERING using 575,000 objects data sets (dataset2) 
with 15% noise; time cost denotes execution time of cost (in millisecond: ms); CCR represents the 
clustering correctness rate (%), while NFR indicates the noise filtering rate (%). 
Algorithm Cluster No. NFR CCR Time cost 
CLIQUE 4 95.920% 98.763% 1953(ms) 
DG-CLUSTERING 4 99.093% 99.792% 344(ms) 
 
Table 3: Comparisons with DBSCAN and DG-CLUSTERING using 115,000 objects data sets (dataset2) 
with 15% noise; time cost denotes execution time of cost (in millisecond: ms); CCR represents the 
clustering correctness rate (%), while NFR indicates the noise filtering rate (%). 
Algorithm Cluster No. NFR CCR Time cost 
DBSCAN 4 95.555% 99.999% 1258404 (ms) 
DG-CLUSTERING 4 96.866% 99.820% 62 (ms) 
  
 10
References: 
 
[1] Agrawal, R., Gehrke, J., Gunopulos, D. and 
Raghavan, P., “Automatic Sub-Space 
Clustering of High Dimensional Data for Data 
Mining Applications,” In Proc. of ACM 
SIGMOD Int. Conf. MOD, pp. 94-105, 1998. 
[2] Ester, M., Kriegel, H. P., Sander, J., Xu, X. 
“A Density-Based Algorithm for 
Discovering Clusters in Large Spatial 
Databases with Noise,” In: Proceedings of 
International Conference on Knowledge 
Discover and Data Mining, 1996, pp. 
226-231. 
[3] Borah, B., Bhattacharyya, D. K. “An 
Improved Sampling-Based DBSCAN for 
Large Spatial Databases,” In: Proceedings of 
International Conference on Intelligent 
Sensing and Information, pp. 92-96, 2004. 
[4] Liu, Bing. “A Fast Density-Based Clustering 
Algorithm for Large Databases,” In: 
Proceedings of International Conference on 
Machine Learning and Cybernetics, 2006, 
pp. 996-1000. 
[5] McQueen, J. B. “Some Methods of 
Classification and Analysis of Multivariate 
Observations,” In: Proceedings of the 5
th
 
Berkeley Symposium on Mathematical 
Statistics and Probability, 1967, pp. 281-297. 
 
 計畫成果自評 
本研究計畫就研究內容而言，與原計畫可
謂相符，並已達成預期目標。本研究計劃
於原計畫書內提出以 DG-CLUSTERING
及 GF-DBSCAN 資料叢集及其相關之方
法，此研究之成果已分別發表於： 
1.計有八篇相關之論文已發表於國際上接
受率較低的學術會議，並有兩篇被收錄於
Applied Intelligence (SCI)及 International 
Journal of Business Intelligence and Data 
Mining期刊。 
2.獲得屏東科大全額經費補助，已委由專利
事務所向美國及國內申請約有 7 個發明專
利。 
3.過去之資料叢集技術普遍存在著下列數
種問題：（1）執行叢集分析的時間過長、
（2）叢集之正確性過低、(3)無法濾除雜
訊、（4）群體邊緣呈鋸齒化現象、（5）無
法執行大型資料集、（6）無法執行高維度
叢集分析。為消除上述六種叢集的缺陷，本
研 究 計 畫 在 DG-CLUSTERING 和
GF-DBSCAN 演算法中是以密度為基礎，並
輔以格子式切割資料來達到分群的目的。其
中 DG-CLUSTERING 方法極為簡單,僅需
輸入一個參數，甚至可稍加修改不必輸入任
何參數即能執行叢集之工作。經由實驗結果
證實本研究計畫所提出之叢集技術及其衍
生之技術確實可解決以上之問題，本研究計
畫所產出之技術在執行多種叢集資料分析
(資料集 575,000 筆)不但正確率與雜訊率
除率極接近 100%,其執行速度皆少於 1 秒，
就筆者所知，此應為全球目前最快之叢集技
術。 
4.此技術已申請美國與中華民國專利,其結
果榮獲屏科大 2010 年全校教師研究成果競
賽創新服務類第一名及工程類第二名,並獲
選國科會 2010 年台北國際發明展暨交易展
參展作品。本技術可應用推廣至各層面，如
電子商務、影像處理與感測網路方面。 
 
 
 2 
Diagnostics, Prediction and Control: Capabilities and Myths、Evolving 
Connectionist Systems: Methods, Tools, Applications、Wireless Network、
Image processing、Computational Intelligence in Financial Forecasting、
Unification of Neural and Wavelet Networks and Fuzzy Systems 及
Independent Component Analysis 等多項熱門議題，發表論文甚多。 
而筆者之論文被安排於 8 月 2 日 Session 1 14:00-16:00 之 regular session 中
口頭論文發表(Oral Presentation)，Paper number 為 56。本論文係探討有關
於在 Data Mining 的領域中之分群技術(data clustering)創新研發，主要的目
的是為了挖掘隱含在大量資料庫中之有用的資訊，提供決策者有用之參考依
據。而資料分群技術是資料探勘的領域中最常用的資料分析技術，而分群正確
性與時間成本是目前學者主要研究發展的目標。處於資訊科技不斷快速發展的
今日，其中利用資料挖掘(Data Mining)技術以支援電子商務或顧客關係管理為
一極熱門且可以實際幫助企業進行決策支援之重要研究議題。企業資料庫中產
生大量之資料，因此有效能及有效率的分析這些資料之方法是企業急切需要
的，此亦導致以資料探勘技術去挖掘企業內隱含的資訊在近年來有愈受歡迎之
趨勢。一般而言，資料挖掘的方法中資料叢集可謂扮演相當重要的角色。資料
叢集是人們使用資訊的主要活動，其應用範圍極為廣泛。資料叢集之目的是讓
我們可以洞悉資料之內部結構或讓我們發展一企業策略以客製化個別顧客之叢
集以增強企業效率。由於資料叢集可應用於趨勢分析、顧客剖析、相似性搜尋、
資料分類、影像處理、影像壓縮、圖型辨識、多重物流配送、網路入侵與攻擊
行為分析、在無線感測網路中之資料傳播機制等及其它應用，所以此一研究領
域也隨之成為熱門且實際可應用於企業決策與工業技術上之重點科技。本論文
所提出的叢集分析技術能針對密度不同之資料庫來進行資料分群的作業，由於
以密度為基礎之叢集分析方法其擁有正確率高、可濾除雜訊之優點，但是其執
行分群之效率卻令人詬病，為解決此一問題，筆者遂提出一具新穎性與進步性
之叢集分析技術，經由實際實驗結果，證實本論文所研提之叢集分析技術確實
較國際上相關之叢集分析技術為佳。 
會議中，在眾多的議題內，筆者特別注意的是與 Data mining、Neural 
network、Intelligent system 有關的議題：其中有許多議題之內容其研究結
 4 
面研究的專家學者，不但介紹自己給大家認識之外，也簡略告訴與會國際學
者台灣目前在此方面的發展與應用。筆者透過此次的論文發表，深切體會到
專業知識與技能及英文溝通的重要性，可謂受益菲淺。日後唯有不斷地汲取
新知，並不斷在研究領域上研發創新，加強英文的溝通能力，始能在國際學
術領域內受到重視。 
 
 
所發表之論文 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 6 
for sets, DBSCAN scans all data points in the 
database, and draws a circle of radius (ε) 
around each data point Q. The number of data 
points within a circle is set as a neighborhood. 
If a neighborhood is greater than the set MinPts, 
then its data points are set as expansion seeds, 
if P is the core point, and its neighborhood is 
less than the set MinPts, then these data points 
are considered as a Border Point. DBSCAN can 
conduct clustering on disordered patterns, and 
has noise filtering capacity, as well as clusters 
that can be stabilized. However, parameter 
setting is difficult, which also affects the 
clustering result. 
B. IDBSCAN Algorithm 
 IDBSCAN is a density-based data 
clustering scheme developed by Borah et al. in 
2004 [1]. This method applies a Marked 
Boundary Object to determine the data point of 
an expansion seed when searching for 
neighborhood to add in expansion seeds. 
Assuming that the core point is P(0,0), the 
eight marked objects may be defined as: A(0,
ε), B(ε/√2,ε/√2), C(ε, 0), D(ε/√2, -ε/
√2}), E(0, -ε), F(-ε/√2, -ε/√2), G(-ε, 0), 
H(-ε/√2},ε/√2). If P indicates the core 
point, and it satisfies the set density condition, 
then the algorithm finds within the 
neighborhood the closest point to these eight 
marked boundary objects, and sets these data 
points as the expansion seeds. Since these seeds 
may be selected using multiple marked 
boundary objects, the algorithm requires only 
one instance of input is needed. The number of 
seeds added is below (3d-1), where d represents 
the dimension of the database. 
C. KIDBSCAN Algorithm 
 KIDBSCAN is a density-based clustering 
method presented by Tsai and Liu in 2006 [3]. 
They searched for marked boundary objects 
with IDBSCAN, and found that inputting data 
sequentially from low-density database causes 
remnant seed searching, resulting in poor 
expansion results. To decrease the number of 
sample instances, KIDBSCAN performs 
expansion by inputting elite points. It has three 
parameters, elite point, radius and MinPts. The 
execution steps are as follows. (1) Adopt 
K-means algorithm to find the K numbers of 
the centroid within the database, then find the 
K data points closest to these centroid and 
define them as elite points, because K-means 
can discover these elite points quickly. (2) 
Move the K elite points to the very front of the 
database. (3) Execute the IDBSCAN algorithm. 
Experimental results prove that KIDBSCAN 
performs data clustering quickly. 
 
III.  THE PROPOSED DBSCALE ALGORITHM 
 The processing procedure of searching for 
neighborhood data is very time consuming in 
the traditional DBSCAN algorithms. Therefore, 
to shorten the time consumed, this work 
presents "Density-BaSed Clustering Algorithm 
for Large databasEs" (DBSCALE), a new 
algorithm that reduces the number of searches 
for neighbors and reduce the number of 
expansion seed approaches. This section 
introduces the method, concept, 
implementation procedures, algorithm and 
complexity analysis. 
A. The improved method and concept 
 Searching for neighborhood data in 
density-based clustering algorithms is 
time-consuming. The time taken in DBSCAN 
is defined by regionQuery(), such that the time 
complexity of searching for neighborhood data 
is O(n) where n denotes the database size. Data 
points that have been clustered do not need to 
be input again into the neighborhood search 
criteria when searching for neighborhood data, 
as show in Fig. 1. Thus, the index array 
required for the neighborhood search is 
readjusted after each completion of clustering 
task. This neighborhood search index array is 
defined as for use at the beginning of the 
algorithm, and it stores the unclassified data 
points, with storage in Boolean type. 
DBSCALE requires n bytes of memory in the 
entire database. Although DBSCALE uses 
more memory space than conventional 
IDBSCAN, it reduces the time consumed. The 
time cost of the next neighborhood search 
following completion of a clustering task is 
O(n-c), where n denotes the database size, and 
c represents the size of the cluster set data size 
after the clustering task. 
 The proposed procedure redefines the 
eight Marked Boundary Objects from 
IDBSCAN algorithm. This proposed data 
points exclude the expansion seeds from the 
data near P. Because these seeds have large 
coverage, adding expansion seeds increases the 
cost of search time; these seeds then join the 
 8 
DBSCALE algorithm are described as follows:  
Step 1: Initialize all parameters, and define a 
new ClusterID. 
Step 2: Begin scanning all data points within 
the entire database. For data points 
belonging to the ClusterID of those 
unclassified data, implement the 
ExpandCluster processing procedure. 
The database is the set of data points; 
the Point represents the core point; the 
ClusterID denotes the current cluster 
ID; ε  indicates the radius, and 
MinPts represents the minimum 
number of included points. 
Step 3: If the data point returned by the 
expansion procedure function is a 
noise data point, then go directly to 
Step 2, until the Datasets database has 
been fully scanned. If an expansion 
data point is returned, then update the 
new ClusterID, and alter the index 
array of unclassified data, and then go 
to Step 2.  
Step 4: End the algorithm when all data points 
have been processed. 
 The implementation steps for the 
ExpandCluster processing procedure are as 
follows. 
Step 1: Search for neighborhood data within the 
range of radiusεin the unclassified 
cluster index. If the number of 
neighborhood data is less than MinPts, 
then leave the procedure, and return 
the core point as the noise data point. 
Otherwise, go to Step 2.  
Step 2: Set the core point as the current 
ClusterID.   
Step 3: If the seed data points is empty then end 
the expansion processing procedure, 
otherwise go to Step 4.  
Step 4: Search for the marking boundary point 
within neighborhood data, and add in 
the expansion seeds.  
Step 5: Set all unclassified data points and 
neighborhood data points that are noise 
data as the current ClusterID.   
Step 6: Extract the first seed from the 
expansion seeds; define it as the core 
point, and then delete it.  
Step 7: In the unclassified data index, search 
for neighborhood data within the range 
of radiusεof the core point. If the 
number of neighborhood data is greater 
than MinPts, then go to Step 3. 
C. Complexity analysis 
 Generally, to compute the complexity of a 
density-based clustering algorithm in a 
d-dimensional database space, if d represents 
the number of dimensions, n denotes the size of 
database, and s indicates the number of 
neighborhood data. The time of complexity can 
be divided into three parts as the following 
description:   
 1) In the overall framework, DBSCALE 
and existing DBSCAN relative clustering 
algorithms have the same complexity as O (n 
log n). 
 2) In the expansion seeds processing 
procedure, the complexity of DBSCAN equals 
O(sd), the complexity of IDBSCAN does not 
exceed (3d-1), while DBSCALE does not 
exceed (3d-3), where d represents the number 
of dimensions, and s indicates the number of 
neighborhood data. 
 3) In the neighborhood searching procedure, 
the complexity of traditional DBSCAN relative 
algorithms equal O(n), while DBSCALE equals 
O(n-c), where n denotes the database size, and 
c represents the size of the cluster set data size 
after the clustering task. 
 In summary, the proposed DBSCALE 
algorithm may reduce the time cost of 
neighborhood data searching. Moreover, 
expansion seeds are selected to increase 
computational efficiency. 
IV.  EXPERIMENTAL RESULTS 
 The clustering algorithm was implemented 
in the C# language in Microsoft Visual Studio 
2005 on a notebook computer with a 1.6 GHz 
Intel CPU, with 1G of RAM, running Windows 
XP operation system. The clustering 
correctness rate and noise filtering rate 
experiments were performed with eight 
different pattern datasets. The pattern image 
size was 900x700 pixels. The number of 
datasets within Dataset 1 to Dataset 8 was 
115,000 of 2-dimensional data, which includes 
15,000 (15%) noise data points, and a fixed 
radius (ε= 6), as Fig. 4 shows. 
 The experiments were performed on four 
algorithms in total.  The radius (ε) was fixed, 
then according to the pattern data density to 
adjust its MinPts to conduct the experiment. 
國科會補助計畫衍生研發成果推廣資料表
日期:2010/12/21
國科會補助計畫
計畫名稱: 新的高效能資料群集技術之建構及其實務應用
計畫主持人: 蔡正發
計畫編號: 98-2221-E-020-029- 學門領域: 人工智慧 
研發成果名稱
(中文) 以格子為導向之資料分群方法
(英文)
成果歸屬機構
國立屏東科技大學 發明人
(創作人)
蔡正發,邱建昇
技術說明
(中文) 本發明係關於一種資料分群方法，特別是一種以格子為導向之資料分群方法，提
供一種以格子為導向之資料分群方法，可藉由判斷高低密度格的標準，以達到提
高雜訊濾除率的目的。一種以格子為導向之資料分群方法係包含：一參數設定步
驟設定參數；一切割步驟將具有數資料點之資料集切割成網格狀；一搜尋步驟找
出一個未擴散之高密度格加入一種子表；一分群判斷步驟從該種子表選一種子判
斷是否為高密度格；一分群擴散步驟將該種子內所有資料點指定為同一群集，並
將該種子周圍尚未擴散之網格加入該種子表；及一終止判斷步驟判斷是否終止。
(英文) A grid-based data clustering method comprises: setting parameters; dividing a dataset 
having a plurality of data points into several grids; finding a clustered grid with 
unextended high-density from those grids and joining the clustered grid into a seed l
產業別 資訊服務業
技術/產品應用範圍 本發明係關於一種資料分群方法，特別是一種以格子為導向之資料分群方法。
技術移轉可行性及
預期效益
本研發成果可申請專利及技術移轉至相關技術需求之廠商
註：本項研發成果若尚未申請專利，請勿揭露可申請專利之主要內容。
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
此技術已申請美國與中華民國專利,其結果榮獲屏科大 2010 年全校教師研究成
果競賽創新服務類第一名及工程類第二名,並獲選國科會 2010 年台北國際發明
展暨交易展參展作品. 本技術可應用推廣至各層面,如電子商務、影像處理與感
測網路方面。 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
