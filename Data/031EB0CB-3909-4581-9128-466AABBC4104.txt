possible to their respective due date. If a job is completed earlier than its due date, it has to be 
held as inventory and incurs an inventory cost. On the other hand, a tardy job completion results 
in penalties, such as loss of customer goodwill and damaged reputation. In a proportionate flow 
shop (PFS) problem, jobs have to be processed through a fixed sequence of machines, and 
processing time for each job is equal on all machines. Unlike the flow shop scheduling problem 
under certain constraints that has attracted considerable attention, the PFS problems have 
recently gained considerable attention.  
This project describes a novel hybrid algorithm that combines a column generation (CG) 
approach and constructive heuristics to a common due date on proportionate flow shop 
scheduling problem with parallel machines (PFSPM), that is, the PFSPM problem is also 
referred to as a proportionate flexible flow shop problem. The objective of the problem is to 
minimize the total weighted earliness and tardiness of job completion time from a common due 
date on a single machine. The problems with due date restriction have received considerable 
attention in the last two decades due to the introduction of new approaches of inventory 
management such as JIT manufacturing environment. To the best of our knowledge, there is no 
published paper for dealing with the PFSPM problem to minimize total weighted earliness and 
tardiness (TWET). The combined approach bases on column generation and the some 
properties from V-shaped schedule on a single machine. This combined approach adopts a CG 
approach to effectively handle job assignments to machines, and the constructive heuristics to 
construct an optimal sequence for tardy jobs on a single machine. 
 
 
Keywords: Proportionate flow shop, Total weighted earliness and tardiness, common due date, 
column generation, Constructive heuristic 
 
 
1. Introduction  
 
The just-in-time (JIT) concept in many industrial manufacturing systems has become 
interest in machine scheduling problems. In the JIT scheduling environment, the product should 
be finished as close to the due date as possible. In the last two decades, many papers have been 
published in the scheduling area. It is desired to have jobs completed at time as close as possible 
to their respective due date. If a job is completed earlier than its due date, it has to be held as 
inventory and incurs an inventory cost. This is often the case when the products are physically 
large, then the buffer space in between two successive machines may have a limited capacity, 
causing blocking. On the other hand, a tardy job completion results in penalties, such as loss of 
customer goodwill and damaged reputation. In a JIT production environment, due dates can 
2 
date for a PFFS scheduling problem. 
The PFFS problem can be defined as follows. There are s stages in series S={1,2,…,s}; each 
stage i∈S consists of m identical parallel machines M={1,2,…,m} that have to process n jobs 
N={1,2,…,n}. One machine cannot be assigned to two jobs at a time, and each job can be 
processed by only one machine at each stage and preemption is not allowed. Each job j∈N 
consists of a chain of s operations Oij (j∈N;i∈S) and each operation requires a processing time 
pij = pj (i.e., p1j = p2j =. . .= psj = pj) on any one of the machines at stage i with equal machine 
speed, and has a positive weight wj (or priority). As a flexible flow shop, which implies the 
operation Oi-1,j is preceded by operation Oij, that is, the execution of Oij cannot start before the 
execution of Oi-1,j has been finished. Operation Oij has to process on machine k∈M at stage i∈S, 
and requires an uninterrupted period of length pij. 
The problem of minimizing total weighted deviations of job completion times from a 
common due date is described as follows. Let Cj (j∈N) denote the completion time of job j that 
has to complete at the last stage and should ideally be completed exactly on its due date d, which 
is common to the jobs on a single machine. We assume that this common due date is unrestricted 
large, that is, due dates that are large enough to not influence the assignment of the jobs 
completing before it. Then, the earliness of the j-th job is defined 
  Ej=max{0,d – Cj} 
And the tardiness of the j-th job 
  Tj=max{0,Cj – d} 
The objective is to focus on a schedule σ with minimum total weighted earliness and 
tardiness, obtaining 
  F(σ)=∑ (α=nj 1 j Ej+βjTj) 
whereαj and βj are positive weights, denoted as FFc ||∑ (α=nj 1 j Ej+βjTj) for the flexible flow 
shop problem. Minimizing the total weighted earliness and tardiness in a proportionate flexible 
flow shop is denoted by FFc | Pij =Pj| (α∑ =nj 1 j Ej+βjTj). 
 Minimizing the total weighted completion time in a two stages flow shop is already 
NP-hard [20]. Błażewicz et al. [27, 28] proposed a two-machine flow shop problem with 
weighted late work criterion and common due date. Shakhlevich et al. [13] reported a O(n2) time 
algorithm to solve a proportionate flow shop with minimum total weight completion time.   
The flexible flow shop problem was first addressed by Salvador [29]. Wittrock showed that 
the flexible flow shop scheduling problem is a NP-hard one [30]. Brah and Hunsucker [31] 
developed a branch and bound algorithm for the problem. However, these algorithms can only 
solve problems with a small size. Xiao et al. [32] and Wang and Li [33] also developed genetic 
algorithm for flexible flow shop to minimize the makespan. However, genetic algorithm is 
4 
PFS problem and the common due date scheduling problems on each single machine. These 
properties will be used later to restrict the search space for finding an optimal schedule for the 
FFc | Pij =Pj|∑ (α=nj 1 j Ej+βjTj) problem. 
Property 1: There exists an optimal schedule on a single machine which is: 
(1) There is no idle time between jobs [36]. 
(2) An optimal schedule has the so-called V-shaped property [4]. The early jobs are scheduled in 
the non-decreasing order of theαj/Pj ratio , that is, according to the Weighted Longest 
Processing Time first (WLPT) rule. The tardy jobs are scheduled in the non-increasing order 
of theβj/Pj ratio, that is, according to the Weighted Shortest Processing Time first (WSPT) 
rule. 
(3) An optimal schedule exists in which either the processing time of the first job starts at time 
zero or one of the jobs completes exactly at the due date d [37]. 
In a given schedule for the problem, the corresponding single machine schedule on a given 
machine consists of two parts: the early schedule, consisting of the jobs completed before or on 
time d; the tardy schedule, consisting of the jobs completed after time d, and then combine them 
to form a single machine schedule. Due to the three properties, the value of common due date is 
irrelevant. 
 
Property 2: An optimal schedule exists that is a permutation schedule for the PFS problem; that 
is, flow shops that do not allow job sequence changes between machines are called permutation 
flow shops. These flow shops maintain the same sequence or permutation of jobs throughout. 
 
Let Cj denote the completion time of job j at the last stage in any schedule. Consider a 
permutation PFS problem with s stages in series, 
Cj = p∑ =ji 1 i + (s – 1) max{p1, . . . , pj}  s∈S, j∈N   
For any job in the early schedule, we have that  
   Ej = d – Cj =∑ p−= 11ji i + (s – 1) max{p1, . . . , pj} 
the total weighted earliness in an early schedule for the PFS problem is computed: 
 
∑ =nj 1 αjEj= α∑∑ −== 111 jinj j pi + (s – 1)∑  α=nj 1 j max{p1, . . . , pj}  
Note that an optimal V-shaped schedule, the first job of a tardy schedule starts exactly at the due 
date.  
6 
2.2 The Restricted Master Problem (RMP) 
 
The problem is first formulated into a master problem with a set partitioning formulation. 
This formulation has an exponential number of columns during the processing of the CG 
procedure; each column represents an early schedule or a tardy schedule on a single machine at 
each stage. Let Ω be the set of all early partial schedules and tardy partial schedules, including 
the empty tardy schedule, and σ be any partial schedule (early or tardy schedule). For each job j 
(j∈N), let ajσ =1 if a partial schedule σ∈Ω includes job j and 0 otherwise. an+1,σ =1 only if σ is an 
early schedule, and an+2,σ =1 only if σ is a tardy schedule.  
Let Cj(σ) denote the completion time of job j on the machine k at the last stage s in σ and 
C(σ) be the total cost of any partial schedule σ∈Ω. 
For any early or tardy partial schedule σ∈Ω, define 0-1 variables, yσ =1 if a partial schedule 
σ∈Ω is selected and 0 otherwise. Then the master problem can be formulated as the set 
partitioning problem denoted as SPF. 
Min C(σ)y∑
Ω∈σ σ
subject to 
   a∑
Ω∈σ jσ
yσ=1,       (1) Nj∈∀
∑
Ω∈σ
 an+1, σ yσ≤m      (2) 
∑
Ω∈σ
 an+2, σ yσ≤m       (3) 
yσ∈{0,1},   Ω∈∀σ          (4) 
 
M1
M3
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6
7
7
7
8
8
8
9
9
9
0 50 100 150 200 250 t
M2
1
1
1
 
Figure 1. Gantt chart for the example of the PFS problem for tardy jobs in WSPT order 
 
M1
M3
2
2
2
4
4
4
5
5
5
6
6
6
7
7
7
8
8
8
9
9
9
0 50 100 150 200 250 t
M2
3
3
3
1
1
1
 
Figure 2. Gantt chart for the final sequence of the example in a PFS problem for tardy jobs 
8 
Step 5: If no such column with negative reduced cost can be generated, then stop. The linear 
programming relaxation problem is solved and the CG procedure is then terminated. 
  
 
2.3 Column Generation Approach 
 
In the column generation approach, the linear programming relaxation is obtained that the 
constraint (4) is relaxed to 0 y≤ σ≤ 1. This is because the column generation is a generalized 
linear programming for which an optimal solution of the relaxed problem is the lower bound of 
the integer optimal solution problem. 
As the number of partial schedules on a machine, it is impossible to explicitly list all the 
columns when solving RMP. Instead, we use the column generation to generate necessary 
columns into RMP. To solve the restricted master problem, we apply the standard column 
generation in which the restricted master problem is a linear programming problem and can be 
solved efficiently. Each column represents an early schedule or a tardy schedule on one machine 
and is generated by solving a single machine subproblem. This procedure starts with a limited 
number of columns, that is, some initial set Ω of early and tardy schedules are needed to compute 
the initial dual variables. The initial solution has to be provided to the RMP and generate 
columns with the most negative reduced cost iteratively. To generate initial columns are 
becoming important, poorly selected initial columns lead the algorithm lost.  
 
2.4 Single Machine Subproblems 
 
The main idea behind column generation is that the occurrence of variables (columns) with 
negative reduced cost is not verified by enumerating all variables, but rather by solving an 
optimization problem. This optimization problem is called the pricing problem and is defined as 
the problem of finding the variable with minimum reduced cost to be added the restricted master 
problem. If neither an early schedule, nor a tardy schedule with negative reduced cost exists, then 
the column generation procedure will be terminated and the problem for FFc 
|Pji=Pj| [α∑ =nj 1 jEj+βjTj] is solved.  
Letπ j denote the dual variable value corresponding to job j (j∈N) in constraint (1), andλ 1 and 
λ 2 denote the dual variable value corresponding to constraint (2) and (3). Then the reduced cost 
rσ of any column σ∈Ω is given by: 
 rσ = C(σ) - a∑ =nj 1 jσπ j - λ 1- λ 2 
We solve the pricing problem by finding the early schedule and tardy schedule with minimum 
reduced cost among all early and tardy schedules. To that end, we use two dynamic algorithms to 
find an early schedule with minimum reduced cost and a tardy schedule with minimum reduced 
cost.   
In case of an early schedule, the vector an+1,σ =1 and an+2,σ =0, we essentially have to minimize 
10 
O(nP) time and space. 
 
Dynamic Algorithm 2 
To generate the tardy schedules with negative reduced cost in a similar fashion. Reindex the 
jobs in order of non-increasingβj/Pj ratios, settling ties according to non-decreasing processing 
time. Let V (j, t) denote the minimum reduced cost in a tardy schedule in which the last job 
completes at time t. 
V (j, t)=  
⎩⎨
⎧
∞
==−
otherwise,
0and0if, 2 tjλ
Recursive relation: 
For j=1,…,n, t=0,…,  ∑ =ji isP1
V (j, t)=min{V (j-1, t),V (j-1, t-spj) +βjt – π j }    (6) 
The optimal value is computed: 
  
Pt≤≤0
min V (n, t) 
where P= . Then the tardy schedule with minimum reduced cost is solved by computing ∑ =nj jsp1
Pt≤≤0
min V (n, t). Run both dynamic algorithms to determine the early schedule with minimum 
negative reduced cost and the tardy schedule with negative reduced cost. If both V(n, t) 0 and ≥
V (n, t)≥0, then the column generation procedure will be terminated and the problem has solved 
to optimality. If not, the new columns (the early schedules or the tardy schedules) are generated 
to be added into the restricted master problem. It is not necessary to have one column with the 
most negative reduced cost into the restricted master problem, if more than one columns with a 
negative reduced cost are available, then add multiple such columns to the restricted master 
problem. After the value of V(n, t) or V (n, t) has been determined, the optimal sequence is 
obtained through a simple backtracking procedure. 
 
3. Branch and Bound Algorithm 
 
A linear programming relaxation solved by column generation is not necessarily integral, so 
the branch and bound procedure now considers the LP relaxation of one of the subproblem and 
solves it. If the optimal solution is integral, in this case, each value of yσ is either 1, or 0, then the 
branch of the tree does not have to be explored. If the optimal solution is not integral, then a 
fractional variable should be selected to branch on. For solving our problem, traditional 
branching on the y variable may create problems along a branch where a variable has been set to 
zero or one does not work in combination with column generation, that is, the branching yσ = 0 
means that this partial schedule is excluded, as pricing problem may generate this partial 
schedule (column) again when solving a single machine subproblem. Our branching strategy is 
12 
4. Constructive heuristic algorithm 
 
This study simply further characterizes an optimal schedule for the PFS problem. Let Δ* is 
the set containing m early schedules and tardy schedules that an integral solution of the linear 
relaxation problem LSPF is obtained. First, we introduce the concept of a new-max job and 
segment. Notably, the jobs have been renumbered according to the processing order in each tardy 
schedule σ generated by applying the DP1 algorithm for solving a subproblem. Let job j be a 
new-max job in tardy schedule σ∈Δ* on a single machine if and only if pj > max{p1, . . . , pj-1} 
j∈N, and tardy schedule σ is partitioned according to the these new-max jobs. Let jobs ω1, . . . , ωr 
be the new-max jobs in σ. Then σ∈Δ* is partitioned into r so-called segments, the ith of which 
contains job ωi and all jobs between jobs ωi and ωi+1.  
 
Property 4 (Shakhlevich et al.): In any optimal schedule of the PFS problem the jobs in the 
same segment, including the new-max job are in order of non-increasing wj/pj ratio. 
 
To reconstruct an optimal sequence for each tardy schedule σ∈Δ* on a single machine at each 
stage, this study presents a constructive heuristic algorithm, called WSPT-MCI (WSPT with 
Minimum Cost Insertion), which was based on the idea of a schedule developed by the 
job-insertion technique proposed by Shakhlevich et al., who built an optimal schedule in 
polynomial time for the PFS problem. 
 
WSPT-MCI algorithm  
 
For each machine schedule σ in Δ* 
Step 1: First the jobs in σ are arranged in the WSPT order, settling ties according to 
non-decreasing processing time. Let ℑ1 denote the sequence consisting of job 1;  
set j = 2. 
Step 2: A job sequence in ℑj is constructed by inserting job j in ℑj-1 out of j possible positions, 
the one yielding the minimum total weighted completion time is kept as the best 
sequence; in the event of several possibilities existing, then the one in which job j is 
inserted latest is selected. Set j = j + 1. 
Step 3: If j ≤ n', return to step 2 until all jobs in σ have been sequenced. Where n′ is the 
number of jobs to be processed in σ. 
Step 4: An optimal sequence in σ is obtained by scheduling jobs in order of occurrence in ℑn′. 
End_for   
14 
Table 1: Results for problem with processing time drawn form the distribution [1, 30] 
 
n m IP-LP Gap WB ANN CG 
20 2 0.04% 30 3.5 465 
30 2 0.12% 15 11.5 1911 
40 2 0.15% 10 21.5 4523 
20 4 0.15% 36 3.8 286 
30 4 0.23% 20 5.2 812 
40 4 0.11% 8 12.3 1431 
20 6 0.01% 47 1.2 158 
30 6 0.09% 35 5.1 908 
40 6 0.05% 15 8.5 942 
 
6. Conclusion 
 
We have proposed a combined column generation approach and constructive heuristic 
algorithm for solving the class of proportionate flexible flow shop problem with a large common 
due date. Using this algorithm, we were able to solve problems with up to 40 jobs to optimality 
by solving the linear programming relaxation of a set covering formulation of the problem. From 
the computational results show that the integrality gap is extremely small, and also few nodes 
need to be explored in branch and bound tree, and many test problems are solved at the root node 
without branching.  
An interesting topic for future research is the special case of proportionate flexible flow shop 
problem with the processing time Pji=pj/si,, where si is the speed of each machine at each stage 
and jobs have distinct due date. 
 
References 
 
1. C.M. Hino, D.P. Ronconi and A.B. Mendes, Minimizing earliness and tardiness penalties in a single-machine 
problem with a common due date. European Journal of Operational Research, 160, 190-201, 2005 
2. J.J. Kanet, Minimizing the average deviation of job completion times about a common due date.  Naval 
Research Logistics Quarterly, 28, 643-651, 1981 
3. M.R. Garey, R.E. Tarjan and G.T. Wilfong, One-processor scheduling with symmetric earliness and tardiness 
penalties. Mathematics of Operation Research, 13, 330-348, 1988  
4. K.H. Baker and G.D. Scudder, Sequencing with earliness and tardiness penalties: a review. Operation Research 
30, 22-36, 1990 
5. Van den Akker JM, Hoogeveen JA and Van De Velde SL, Combining column generation and lagrangean 
relaxation to solve a single-machine common due date problem. INFORMS J Comput 14(1),37–51, 2002 
6. R.J.W. James, Using tabu search to solve the common due date early/tardy machine scheduling problem. 
Computers and Operations Research, 24, 199-208, 1997 
7. Q. Hao, Z. Yang, D. Wang and Z. Li, Common due date determination and sequencing using tabu search. 
16 
Applications, 38-43,2008. 
26. P.-C. Hsu, D. -F Shiau and Yueh-Min Huang. Blending operations with blending range controls on primitives 
subsequent blends", International Computer Symposium, 334-1339, 2004 
27. J. Błażewicz, E. Pesch, M. Sterna and F. Werner, The two-machine flow shop problem with weighted late work 
criterion and common due date. European Journal of Operational Research, 165, 408-415, 2005a 
28. J. Błażewicz, E. Pesch, M. Sterna and F. Werner, The two-machine flow shop problem with weighted late work 
criterion and common due date. Computers & Industrial Engineering, 49, 611-624, 2005b    
29. M.S. Salvador, A solution to a special class of flow shop scheduling problem. In Symposium of the Theory of 
Scheduling and Applications, Springer Verlag, Berlin., 83-91, 1973 
30. R.J. Wittrock, An adaptable scheduling algorithms for flexible flow lines. Operations Research, Vol.33, No.4, 
445-453, 1988 
31. S.A. Brah and J.L. Hunsucker, Branch and Bound algorithm for a flow shop with multiple processors. Eur. J. 
Oper. Res., Vol. 51, 88-99, 1991 
32. W.D. Xiao, P.F. Hao ,S. Zhang and X.H Xu , Hybrid flow shop scheduling using genetic algorithms.  IEEE 
Proceedings of the 3th World Congress on Intelligent Control and Automation, Vol.1, 537-541, 2000 
33. L. Wang and D.W. Li, A scheduling algorithm for flexible flow shop problem. IEEE Proceedings of the 4th 
World Congress on Intelligent Control and Automation, Vol.4, 3106-3108, 2002 
34. J.M. ven den Akker, J.A. Hoogeveen and S.L. van de Velde. Parallel machine scheduling by column generation, 
Operations Research, 47, 862-872, 1999 
35. Z.L. Chen and W.B. Powell, Solving parallel machine scheduling problems by column generation. INFORMS 
Journal on Computing, Vol. 11, No.1, 78-94, 1999 
36. T.C.E. Cheng and H.G. Kahlbacher, A proof for the longest/job/first policy in one/machine scheduling. Naval 
Research Logistics, 38, 715-720, 1991 
37. J.A. Hoogeveen and S.L.Van De Velde, Scheduling around a small common due date. European Journal of 
Operational Research, 55, 237-242, 1991 
38. Lasdon L.S., Optimization theory for large systems, MacMillan, New York, 1970 
39. M.A. Quaddus , A generalized model of optimal due date assignment by linear programming. Journal of the 
operation Research Society, 38, 353-359, 1987 
  
18 
imposed by jobs on machines representing one or more
stages of a multi-stage flow shop scheduling problem [2, 3].
A special case of the two-machine flow shop with the
objective of minimizing total completion time (TCT) was
shown to be NP-hard [4]. Thus, the general multi-stage
PFFS problem is also NP-hard and difficult to solve.
The PFS scheduling problem was first addressed by Ow
[5] and Pinedo [6]. Unlike the flow shop scheduling
problem under certain constraints that has attracted consid-
erable attention, PFS problems have garnered considerable
attention recently. Edwin Cheng and Shakhlevich [7]
described that the PFS problem is an important and
ubiquitous class of industrial scheduling problems. Hou
and Hoogeveen [8] described a situation of a PFS problem,
in which job j corresponds to a customer order with an
associated quantity qj; the processing time of job j on one
machine is then proportionate to qj. On the other hand, a
general model is the model in which the processing time pij
of job j on machine i is a function of qj and speed si of
machine i; i.e., pij=qj/si. Most recently, Koulamas and
Kyparisis [9] showed that the tight worst-case ratio bound
of the SLDR heuristic is 3/2 according to Choi et al. [10].
The other PFS problem can be found in Ref. [11]. Pinedo
[1] observed that PFS problems in a number of cases are
similar to their single-machine counterparts. Such cases
include minimizing TCT, maximum lateness, the total
number of tardy jobs, and total tardiness.
The total weighted completion time (TWCT) is one of the
most important criteria, which is associated with production
environments that inventory levels and manufacturing cycle
times are critical concerns. In a shop environment, the central
managerial goal is to sustain production with the minimum
level of work-in-process and finished goods inventories
while delivering jobs on time. Usually, the objective is to
minimize the total completion time. However, the impor-
tance or value of each job may not be the same. For situations
for which some customers may place larger orders than other
customers in reality, certain decision policies may require
that customers be treated differently according to certain
priorities or weights. In other words, for each job the cost
criterion not only depends on the completion time but also on
the weight of the corresponding job. With regard to its
complexity, Shakhlevich et al. [12] were the pioneers to
address that the PFS problem with TWCT minimization
differs significantly from single-machine or flow shop
problems with the same objective. They prove that the
problem with TWCT minimization in a PFS can be solved
optimally in O(n2). Subsequently, Choi et al. [13] also
considered a two-machine PFS problem with different
machine speeds to minimize the TWCT objective. Estevez-
Fernandez et al. [14] also proposed a cooperative PFS game
related to the PFS problem to allocate cost savings based on
the TWCT objective in a fair way.
Minimizing TWCT in a PFFS problem differs signifi-
cantly from PIMS problems. Because of the intractable
nature of the PFFS problem with TWCT minimization,
Huang and Shiau [15] were the first attempt to extend the
PFS problem proposed by Shakhlevich et al. [12] in that
any number of stages has only a single machine at each
stage to a PFFS problem with several identical machines in
parallel at each stage that minimizes the TWCT objective.
They solved the PFFS problem with strong lower bounds
using column generation (CG) approach. In addition, a
dynamic programming algorithm according to optimal
properties of the PFS problem is designed to verify global
optimality of the solution for the linear programming
relaxation of the minimization problem by checking if
there exist one or more variables (columns) that were added
to the linear program with negative reduced cost. Column
generation approaches have led to state of-the-art branch-
and-bound (also called branch and price) algorithms for
such archetypical combinatorial optimization as production
scheduling and parallel machine scheduling problem, and
CG has proved an effective means of solving PIMS
problems with a remarkably strong lower bound [16, 17].
However, these papers reported that the CG approach
requires much computation time when the ratio of the
number of jobs to the number of machines is relatively
large; i.e., many columns generated are not very useful,
thereby slowing the algorithm. For example, the average
CPU time for an instance with the number of jobs n=60
and the number of machines m=8 is significantly lower
than that for an instance with n=60 and m=4.
The particle swarm optimization (PSO) was first pro-
posed by Kennedy and Eberhart [18]. In PSO, a swarm of
particles spread in the space and the position of a particle
presents a solution. Each particle would move to a new
position decided with the global experience and the
individual experience heading for the global optimum.
Salman et al. [19] presented PSO for task assignment
problem; they also showed that the proposed PSO-based
algorithm solution quality is better than that of genetic
algorithm (GA) in most of cases. In addition, the PSO
algorithm runs faster as compared with GA. Tseng and Liao
[20] demonstrated that the proposed PSO algorithm out-
performs over GA and ant colony optimization for the flow
shop scheduling problem. According to the research work
by Wang [21], a combination of different heuristics and a
combination of heuristics and artificial intelligence (neural
networks, GAs, tabu search, and simulated annealing)
search techniques can greatly improve the quality of solutions
and also be a future trend of algorithm development. Kuo et al.
[22] proposed a hybrid PSO combined with an individual
enhancement scheme to solve flow shop scheduling; the
objective is to minimize makespan. Tasgetiren et al. [23]
involved a local search scheme based on insert and
Int J Adv Manuf Technol
[12]. These lemmas will be used throughout the article to
restrict the search space for finding an optimal schedule for
the FFc pij ¼ pj
 Pn
j¼1 WjCj problem. For a single-machine
scheduling problem with the objective of minimizing
TWCT, jobs follow the order of weighted shortest process-
ing time (WSPT) in an optimal schedule [31]. However, a
PFS problem with TWCT minimization markedly differs
from a single-machine scheduling problem with the same
objective.
Lemma 1 (Shakhlevich et al. [12]): an optimal schedule
exists that is a permutation schedule for a PFS problem;
that is, flow shops that do not allow job sequence changes
between machines are called permutation flow shops. These
flow shops maintain the same sequence or permutation of
jobs throughout.
Let Π denotes any schedule of a PFS problem, and
Cj(Π) be the completion time of job j at the last stage in Π.
Consider a permutation PFS problem with s stages in
series,
Cj
Y 
¼
Xj
i¼1 pi þ s 1ð Þmax p1; . . . ; pj
 
s 2 S; j 2 N ð1Þ
Then,C(Π) be the TWCT according to Π and is computed by
C
Y 
¼
Xn
j¼1 wjCj ¼
Xn
j¼1
Xj
i¼1 wjpi
þ s 1ð Þ
Xn
j¼1 wj max p1; . . . ; pj
 
s 2 S; j 2 N
ð2Þ
Example 2 Consider a PFS problemwith jobsN={1, 2, 3, 4},
machines M={1, 2}, the processing times of the jobs p=
(4, 5, 6, 2) and the weights of jobs w=(30, 25, 20, 5). Let
Π=(1, 2, 3, 4) be a permutation schedule. Figure 2
illustrates the example.
Thus, C1(Π)=8, C2(Π)=14, C3(Π)=21, C4(Π)=23. The
following calculation is illustrated according to Eq. 1
for C3(Π).
C3 Πð Þ ¼ p1 þ p2 þ p3 þ 2 1ð Þmax 4; 5; 6f g
¼ 4þ 5þ 6þ 6 ¼ 21:
Hence, the value of TWCT based on Eq. 2 is C(Π)=1,125.
Equation 2 illustrates that the first term on the right
hand size is minimized by scheduling the jobs on a single
machine according to the WSPT order; i.e., jobs processed
on the same machine must be scheduled in order of
nonincreasing wj/pj ratio (Smith [31]). The second term on
the right hand size is minimized by scheduling the jobs
according to the shortest processing time (SPT) rule; i.e.,
jobs are scheduled in order of nondecreasing processing
time (Shakhlevich et al. [12]). Restated, for an optimal
schedule of a PFS problem with the TWCT objective, jobs
are executed not simply according to the rule imposed by
the WSPT order, but also by scheduling the jobs based on
the SPT rule. Therefore, the next lemma holds in an
optimal schedule of a PFS problem with the objective of
minimizing TWCT.
Lemma 2 (Shakhlevich et al. [12]): an optimal schedule
exists if for two jobs j and k both wj/pj≥wk/pk and pj≤pk,
then job j precedes job k.
The next lemma further states characterizations in an
optimal schedule of a PFS problem. First, this study
introduces the concept of a new-max job and segment. Let
job j be a new-max job in Π if pj > maxj2Nfp1; . . . ; pj1g.
Let jobs a1, . . ., ar be the new-max jobs according to Π.
Note that pa1 < . . . < par . Then Π is partitioned into r so-
called segments, S
Q
1 ; . . . ; S
Q
r :S
Q
i contains job ai and all jobs
between jobs ai and ai+1.
M1
M3
3
2 4 6
7 8
0 50 100 150 t
M2
1
5
M1
M3
3
2 4 6
7 8
M2
1
5
M1
M3
3
2 4 6
7 8
M2
1
5
0 50 100 150 t
0 50 100 150 t60 80 90 130 140
Fig. 1 Gantt chart of the PFFS
schedule in example 1
Int J Adv Manuf Technol
not be inserted before the any jobs 1, 2, or 3 with
processing time p1≤p2≤p3≤p4. Furthermore, job 3 must
be inserted immediately in front of a new-max job
according to lemma 3. Thus, job 3 either reallocates
immediately in front of job 2 or job 1 (Fig. 5a–c).The
reader can verify that the optimal order obtained after
reordering jobs 1, 2, 3, and 4 is (3, 1, 2, 4 , 5, 6, 7, 8, 9)
with a minimal objective value of 221,520. The next
describes the reordering of job 5, which is the same
situation as the reordering of job 3. The optimal order after
reallocating job 5 is (3, 5, 1, 2,4 , 6, 7, 8, 9) with a minimal
objective value of 220,040 (Fig. 5d–f). Moreover, this
study describes the insertion in which job 6 is
reordered and the optimal order obtained after reallo-
cating job 6 is ( 3, 5, 1, 6, 2, 4 , 7, 8, 9) with a minimal
objective value of 219,240 (Fig. 5g–h). Finally, the
optimal order obtained for this problem after reordering
job 8 is ( 3, 5, 8, 1, 6, 2, 4 , 7, 9) with a minimal objective
value of 219,110 (Fig. 5i–j). Figures 5k–l show the
structure of the optimal schedule of the PFS problem.
The Gantt chart for the optimal schedule of the example is
given in Fig. 6. Thus, the new-max jobs according to the
natural order remain the new-max jobs during the
proposed process of finding an optimal order for a PFS
problem (Estevez-Fernandez et al. [14]).
4 Particle swarm optimization
The PSO algorithm is a multi-agent general meta-heuristic
method, and can be applied extensively in solving many
difficult problems [18]. The PSO consists of a swarm of
particles in the space; the position of a particle is indicated
by a vector which presents a solution (a schedule). PSO is
initialized with a population of Np random particles and
searches for the best position (solution or schedule). In
every generation or iteration, the local best and global best
are determined through evaluating the performances in
terms of the fitness values of current population of particles.
A particle moves to a new position obtaining a new
solution guided by the velocity (a vector). Hence, the
velocity plays an important role in affecting the characters
of creating new solution. There are two experience
positions used in the PSO; one is the global experience
position of all particles, which memorizes the global best
solution obtained from all positions (solutions) of all
particles; the other is the individual experience position of
each particle, which memorizes the local best solution
acquired from the positions (solutions) of the corresponding
particle has been at. These two experience positions and the
inertia weight of the previous velocities are used to
determine the impact on the current velocity. The velocity
retains part of prior velocity (the inertia) and drives particle
toward the direction based on the global experience
position and the individual experience position. Thus, the
particles can derive new positions (solutions) by their own
inertia and experience positions.
Let the search space is D dimension space (the number
of dimension is corresponding to the parameters of
solutions) and the population consists of Np particles. X ti ¼
fX ti1; . . . ;X tiDg;X tiD be the particle i with D dimension space
(i=1, . . ., Np) at iteration t. A position X ti has a rate of
position change called velocity V ti ¼ fV ti1; . . . ;V tiDg. The
individual experience is a position Pti ¼ fPti1;Pti2; . . . ;PtiDg,
the local best position for the ith particle until iteration t
(called pbest) . Additionally, Ptg ¼ fPtg1;Ptg2; . . . ;PtgDg
represents the global best position obtained from Pti among
all the population of particles achieved at iteration t (called
gbest). The velocity and position of particles could be
performed by the Eqs. 3 and 4 in the PSO algorithm.
V tþ1id ¼ w V tid þ c1 rand1 ðPtid  X tidÞ þ c2
 rand2 ðPtgd  X tidÞ ð3Þ
X tþ1id ¼ X tid þ V tþ1id ð4Þ
Jobs 1 2 3 4 5 6 7 8 9
pj 20 30 10 30 10 20 30 10 40
wj 200 270 80 210 69 130 180 59 200
wj/pj 10 9 8 7 6.9 6.5 6 5.9 5
Table 1 Job data for example 3
M1
M3
2
2
2
3
3
3
4
4
4
5
5
5
6
6
6
7
7
7
8
8
8
9
9
9
0 50 100 150 200 250 t
M2
1
1
1
Fig. 4 Gantt chart for the
example 3 of the PFS problem
in the WSPT order
Int J Adv Manuf Technol
uniformly distributed in [0, 1], influencing the tradeoff
between the global and local exploration abilities during
search. A PSO algorithm is described as follows:
1. Initialization: initialize a population of particles with
random positions and velocities on D dimensions in the
search space.
2. Update: calculate the velocity of each particle by Eq. 3
and move to the next position according to Eq. 4.
3. Evaluation: evaluate the fitness of each particle in the
population. Update pbest and gbest positions if necessary,
i.e., if the current pbest position is better than the old pbest
position, then set the pbest position such that it has the
position as the current particle’s position. In the same
manner, if the current gbest position is better than old
gbest position, reset the current gbest position such that it
has the position of the current particle.
4. Termination: stop the algorithm if a specified stopping
criterion is reached; go to step 2, otherwise.
5 Tabu search strategy
Tabu search (TS) is a well-known meta-heuristic that starts
from an initial solution and improves it iteratively to find
a near-optimal solution [32]. The TS allows the search to
explore solutions that do not decrease the objective
function value if these solutions are not forbidden. For
the last two decades, the TS has been also widely applied
to solve numerous combinatorial optimization and parallel
machine scheduling problems [33–36]. Barnes and
Laguna [33] and Dorn et al. [37] illustrated that the TS
can outperform simulated annealing, genetic algorithm
and branch-and-bound, and random search methods. The
basic TS is based on inserting and swapping moves to
generate a neighborhood from the current schedule. An
insert move removes a job from one machine and inserts it
into another. A swap move chooses a pair of jobs and
switches their machine assignments. The best neighbor-
hood schedule is then selected, and the neighborhood
generation scheme is then applied again, starting from the
best schedule. This procedure is iterated until a best
schedule satisfying a user-defined termination condition is
obtained.
The TS approach consists of the following elements and
will be presented in Section 6.4.
1. Initial solution
2. Neighborhood structure
3. Selection of the best neighbor
4. Tabu list and aspiration criteria
5. Intensification and diversification scheme
6. Stopping criteria
6 Combined TPEPSO and TS to FFc pij ¼ pj
 Pn
j¼1WjCj
This section describes how to integrate particle’s position
into a schedule with two different positional representa-
tions, the SPVR and APVR, and combine both into a two-
phase encoding PSO (TPEPSO) algroithm. In the first
phase, An SPVR is designed based on the smallest position
value (SPV) rule to transform continuous position values
into job sequences of the discrete FFc pij ¼ pj
 Pn
j¼1WjCj
problem. In the original PSO, particles in a swarm learn
from the gbest position even though the current gbest is far
from the global optimum. After some generations, popula-
tion diversity is markedly reduced and may become trapped
into a premature convergence, such that the particles in a
population become too similar and the gbest position
cannot be improved. Therefore, during the second phase,
an APVR combined with a TS is adopted starting from the
current particle position to maintain the population diversity
that can escape from a local optima. Additionally, a
candidate list strategy is designed for a TS to reduce search
time and improve iteration performance.
Jain et al. [38] showed that the solution quality of a TS
is primarily affected by its initial solution. In other words,
the purpose of the TPEPSO algorithm is to provide good
and diverse initial solutions for the TS. By hybridizing the
complementary properties of TPEPSO and TS, the hybrid
TPEPSO algorithm combines the cooperative and com-
petitive characteristics of TPEPSO and the TS. Figure 7
shows the hybrid TPEPSO framework for solving the
FFc pij ¼ pj
 Pn
j¼1 WjCj problem.
6.1 Solution representation
The most important issue in designing a successful PSO
algorithm is to define a solution representation (encoding
scheme) that constructs a direct relationship between the
problem domain and PSO particles. Each particle represents
a solution (a schedule) for the FFc pij ¼ pj
 Pn
j¼1WjCj
problem.
6.1.1 Sequence position value representation
In the SPVR, a stream of n integers is used to represent
a processing sequence of jobs for a particle. The position
of particle i is defined as X ti ¼ X ti1;X ti2; . . . ;X tin
 
;
X tij 2 I ; n 2 N , where X tij represents the sequence position
value of job j for the ith particle at iteration t. Since
evaluation of each particle in the swarm requires the
determination of the processing sequence of jobs for the
FFc pij ¼ pj
 Pn
j¼1 WjCj problem. Let s
t
i be a processing
sequence of jobs implied by particle X ti at iteration t. It can
be described as s ti ¼ s ti1; s ti2; . . . ; s tin
 
, where s tij represents
the assignment of job j of the ith particle in the sequence at
Int J Adv Manuf Technol
According to the SPV rule (Fig. 8), the SPV in X
»tþ1
i is
X
»tþ1
i5 ¼ 2:11, and job 5 is assigned as the first job s tþ1i1 ¼ 5
in the s tþ1i sequence; the second-smallest position value is
X
»tþ1
i2 ¼ 2:38, and job 2 is assigned as the second job
s tþ1i2 ¼ 2 in the s tþ1i sequence, this process continues until
all jobs are assigned. Thus, the obtained processing sequence
is s tþ1i ¼ ð5; 2; 3; 1; 4; 6Þ and the position values of a new
particle can be constructed as X tþ1i ¼ 4; 2; 3; 5; 1; 6ð Þ.
Furthermore, each particle X ti has a corresponding X
»t
i and
s ti during iteration t and the new actual position values of
X
»t
i must be recorded to calculate the actual position value
of job j for particle i via Eq. 5. This representation
constructs new solutions, since positions of each particle
are updated in each iteration during the PSO process,
resulting in different sequences for each iteration to be
evaluated.
Once a sequence of jobs for particle X ti is obtained, a
schedule pti ¼ fpti1; pti2; . . . ; ptimg of particle X ti for the
FFc pij ¼ pj
 Pn
j¼1 WjCj problem can be constructed by
applying a generalized List Scheduling (LS) algorithm.
The LS algorithm constructs a schedule by iteratively
assigning jobs to machines according to the sequence at
the first stage, where a machine available early has a
high probability of getting the next job in the sequence;
this process continues until all jobs are assigned.
During the second stage, jobs are processed as soon as
possible based on job completion time from the first
stage. To guarantee that jobs assigned to the same
machine are in the optimal order, the WSPT-MCI
algorithm described in Section 3 is applied to determine
the optimal sequence when jobs are added in the WSPT
order. i.e., the jobs in each ptik k ¼ 1; 2; . . . ;mð Þ on a
machine form the optimal order after applying the
WSPT-MCI algorithm.
Example 5 Consider six jobs to be scheduled in a two-stage
PFFS problem with three machines at each stage. Job
processing times are p=(2, 5, 3, 2, 1, 2). Given a
processing sequences ti ¼ 5; 2; 4; 1; 6; 3f g, the LS algo-
rithm converts this sequence into a schedule for the
FFc pij ¼ pj
 Pn
j¼1 WjCj problem (Fig. 9).
During stage 1, for the sequence s ti ¼ 5; 2; 4; 1; 6; 3f g,
the LS algorithm constructs the schedule by assigning the
first job s ti1 ¼ 5 to machine 1, the second job s ti2 ¼ 2 to
machine 2 and the third job s ti3 ¼ 4 to machine 3. Since
machine 1 is available early with minimum completion
time, the next job, s ti4 ¼ 1, is assigned to machine 1; this
continues until all jobs are assigned. During the second
stage, jobs are processed as soon as possible based on job
completion time from stage 1. Thus, the obtained schedule
corresponding to particle X ti is p
t
i ¼ fpti1; pti2; pti3g,
pti1 ¼ 5; 1; 3f g, pti2 ¼ 2f g, and pti3 ¼ 4; 6f g.
6.1.2 Absolute position value representation
The other solution representation, APVR, is based on the
machine assignments for particles. Salman et al. [19] used
such a particle representation to solve a task assignment
problem efficiently for distributed computing systems. The
APVR indicates the search space of n dimensions for an n
job assignment problem (n∈N). Each dimensional value
belongs to the discrete set M={1, 2, . . ., m}, where m is the
numbe r o f mach ine s a t e ach s t age fo r t he
FFc pij ¼ pj
 Pn
j¼1WjCj problem. The position of particle
i is defined as X ti ¼ X ti1;X ti2; . . . ;X tin
 
, X tij 2 I , where X tij is
the machine to which corresponding job j is assigned for
particle i at iteration t. The following example considers a
problem with eight jobs and three stages; each stage has
three identical parallel machines. Given a schedule
corresponding to particle X ti as p
t
i ¼ fpti1; pti2; pti3g,
pti1 ¼ 1; 3f g, pti2 ¼ 2; 4; 6f g, and pti3 ¼ 5; 7; 8f g. Figure 10
shows the mapping between a one possible job assignment
instance to a particle position in the PSO domain (for the
programming simplicity, machine numbering starts at 0 and
m–1). Thus, X ti ¼ X ti1;X ti2; . . . ;X ti8
  ¼ 0; 1; 0; 1; 2; 1; 2; 2ð Þ.
When applying this encoding scheme, identifying the job
number assigned to each machine is convenient. Therefore,
a particle X ti can be converted easily into a schedule p
t
i and
the optimal order of jobs for each ptik k ¼ 1; 2; . . . ;mð Þ can
be determined in the same manner as described above.
This study further describes the position values of X ti in
the APVR. The PSO algorithm uses the new velocity
M1
M3
2
6
3
0 5 t
M2
5
4
Stage 1
M1
M3
M2Stage 2
0 5 t
1
10
10
8
2
6
35
4
1
LS
t
i (5, 2, 4, 1, 6, 3)
π ti  = {π ti1 , π ti2 , π ti3 }
π ti1 = {5, 1, 3}, π ti2 = {2} and π ti3 = {4, 6}
Fig. 9 The machine assignment of jobs using the LS algorithm
Int J Adv Manuf Technol
s ti of particle X
t
i from the latest iteration of the first phase
using the LS algorithm (Fig. 9). Furthermore, the absolute
position values of Pti and P
t
g must be determined in the
APVR for subsequent calculations.
6.2 Fitness of particles
The next stage of the hybrid TPEPSO algorithm is to
measure the quality of a particle in terms of a fitness value.
To evaluate each particle’s objective value in the swarm, the
current particle X ti must be converted to a feasible schedule
pti ¼ fpti1; pti2; . . . ; ptimg for the FFc pij ¼ pj
 Pn
j¼1WjCj
problem as described in Section 6.1, and the hybrid
TPEPSO algorithm uses the best solution to continue
search until the termination criterion is reached. Recall that
the optimal order of the jobs in each ptik can be determined
by the WSPT-MCI algorithm. As shown in Fig. 1, the
fitness function fitness pti
 
be the objective function of ith
particle at iteration t according to Eq. 2;
Fitness pti
  ¼
Xn
j¼1 wjCj p
t
i
 
¼
Xm
k¼1
X
j2pttk
Xj
l¼1
wjpl þ s 1ð Þ
X
j2pttk
wjmax p1; . . . ; pj
 
2
4
3
5
m 2 M ; s 2 S; j 2 N
ð6Þ
6.3 Initial population
The initialized population of particles is constructed
randomly for the hybrid TPEPSO algorithm. In the first
phase, the initialized continuous position values and
continuous velocities are generated randomly by Eqs. 7
and 8, which are based on the work by Tasgetiren et al.
[23]. The initial position values of particles are constructed
uniformly as follows:
X
»0
ij ¼ xmin þ xmax  xminð Þ  U 0; 1ð Þ ð7Þ
where xmin=0.0, xmax=4.0, and U(0,1) is a uniform random
number between 0 and 1. After the continuous position
value X
»0
ij of each job j (j=1, 2, . . ., n) corresponding to
particle i is initialized, the sequence position value X 0ij of
each job j for particle X 0i can then be determined using the
SPV rule. The initial velocities of particles are generated as
follows:
V 0ij ¼ vmin þ vmax  vminð Þ  U 0; 1ð Þ ð8Þ
where vmin=−4.0, vmax=4.0, and U(0,1) is a uniform
random number between 0 and 1.
At the start of the iteration in the second phase, the initial
position values of particleX ti in the APVR can be constructed
based on its corresponding schedule pti obtained by decoding
the sequence s ti of particle X
t
i from the latest iteration of the
first phase using the LS algorithm. The initial velocities of
the particle can be generated based on the work by Salman
et al. [19] for exploring a diverse solution region starting
from the current position of particle X ti .
6.4 Incorporation of TS into the TPEPSO algorithm
A local search method based on TS is incorporated in the
evolutionary process of the second phase to balance
exploitation and exploration. Incorporating TS into the
TPEPSO process as a local improvement procedure for
each particle enables the algorithm to maintain population
diversity and escape from local optima. In the proposed
TPEPSO algorithm, when a particle is to perform a TS, it
should first be converted into a feasible solution (a
schedule) to the FFc pij ¼ pj
 Pn
j¼1WjCj problem. The
solution is then used as the initial TS solution for certain
steps and replaces the current particle position with the new
solution obtained by the TS.
6.4.1 Initial solution
To generate an initial solution for the TS, the current particle,
X ti , is converted into a schedule p
t
i ¼ fpti1; pti2; . . . ; ptimg
for the FFc pij ¼ pj
 Pn
j¼1WjCj problem at iteration t
(Fig. 9), and the optimal order of jobs in each sub-
schedule ptik can be identified by applying the WSPT-
MCI algorithm. Previous research demonstrated that the
final solution quality of a TS is primarily affected by its
initial solution [36, 38]. Therefore, the purpose of the
TPEPSO process is to provide good and diverse initial
solutions for the TS during the evolution process.
Notably, the TS is only used in the latter iterations to
explore the space for solution improvement rather than at
the start of iterations during the TPEPSO process. Since
the promise solution of particle X ti is obtained when the
first phase is complete.
6.4.2 Neighborhood structure
Insert moves and swap moves are two of the frequently
used move types in parallel machine scheduling problems.
An insert move removes a job from one machine and
inserts it into another. A swap move chooses a pair of
jobs and switches their machine assignments. Swap moves
involving jobs on different machines do not change the
number of jobs on machines. For situations for which the
neighborhood of a solution is large to evaluate, this study
designs a candidate list strategy to restrict the number of
solutions examined during an iteration. Therefore, a good
candidate list strategy can save time and improve iteration
performance. Since the swap neighborhood is already
Int J Adv Manuf Technol
6.5 Updating velocity and position
The hybrid TPEPSO algorithm uses the new velocity
derived by Eq. 3 to update the current particle position to
a new position. For the FFc pij ¼ pj
 Pn
j¼1 WjCj problem,
this study defines the velocity of particle i as V ti ¼
V ti1;V
t
i2; . . . ;V
t
in
 
, V tij 2 R, where V tij is the movement of
job j for particle i from the current position value to the
other position during iteration t. Thus, Eq. 3 can be
rewritten as
V tþ1ij ¼ w V tij þ c1 rand1 ðPtij  X tijÞ þ c2
 rand2 ðPtgj  X tijÞ ð9Þ
Where Pti ¼ fPti1;Pti2 . . . ;Pting, Ptij 2 I is the pbest
position the ith particle obtained until iteration t, and
Ptg ¼ fPtg1;Ptg2 . . . ;Ptgng, Ptgj 2 I , is the gbest position
obtained from Pti among all particles achieved during
iteration t.
In the SPVR, a particle moves to the next position
according to Eq. 5 (Section 6.1.1). Thus, the actual position
values obtained must be converted into a sequence using
the SPV rule. Moreover, the particle moves to the next
position in the APVR according to Eq. 4 and can be
rewritten as
X tþ1ij ¼ X tij þ V tþ1ij ð10Þ
In the hybrid TPEPSO algorithm, since the TS can
explore the population diversity of the solution space, some
particles in the swarm may be replaced with better solutions
obtained by the TS before applying the TPEPSO algorithm.
Furthermore, the pbest position will be improved by TS.
Generally, the value of each component in V ti can be
clamped to the range [−Vmax, +Vmax] to control excessive
roaming of particles outside the search area. That is, if V tij is
smaller than –Vmax, then set V tij=−Vmax; if V
t
ij is greater than
Vmax, then set V tij=Vmax. Notably, the boundary of handling
position X tij is only applied in the APVR of the second
phase. If the position of X tij is smaller than 0, then set
X tij ¼ 0; if X tij is greater than or equal to m, then set
X tij ¼ m 1, where m denotes the number of parallel
machines at each stage (note that the machine numbering
in the code starts from 0 and m–1); i.e., the feasible region
of position is [0, m).
7 Experimental results and discussion
The hybrid TPEPSO approach described in Section 6 was
implemented in Visual C++ to conduct necessary experi-
mentation. The experiments were tested on a Pentium 4–
3.4 GHz CPU with 512 MB of RAM. The performance of
the proposed hybrid TPEPSO was evaluated on a set of
benchmark problems given by Huang and Shiau [15] using
a column generation (CG) algorithm. They formulated the
FFc pij ¼ pj
 Pn
j¼1WjCj problem into a restricted master
problem with a set partitioning formulation, whose linear
relaxation is solved efficiently by a CG method. To
demonstrate the robust theory of the CG method on the
optimal solution, van den Akker et al. [42] showed that a
remarkably strong lower bound on the optimal solution
value can be computed by formulating the problem as an
integer linear program (usually a set covering or set
partitioning problem) with a huge number of variables
and then solving the linear programming relaxation through
a CG method. They also mentioned that the convergence
can be guaranteed through a CG approach, but at the
expense of a large running time.
The set of benchmark instances consists of seven
different numbers of jobs (n=20, 30, 40, 50, 60, 80, and
100) with three different numbers of stages (s=3, 4, and 5)
and seven different numbers of machines at each stage (m=
3, 4, 5, 8, 12, 16, and 20). Processing time pj of each job j is
an integer number uniformly drawn from [1, 20]. The
weight wj of each job j is an integer number uniformly
drawn from [1, 100]. To test the performance of the
proposed hybrid approach, ten independent runs were
evaluated for each problem instance. Huang and Shiau
[15] showed that the number of stages does not signifi-
cantly affect the efficiency of their algorithms. However, in
the extreme case in which the number of stages is very
large, and the optimal order of jobs on a machine is based
on the SPT rule, thus the weight of jobs does not affect the
optimal schedule [12]. Therefore, this study does not
differentiate between the number of stages for a fixed n
and m. For simplicity, the number of stages in all
experimental results is fixed to three for each combination
of n and m.
7.1 Hybrid TPEPSO parameter settings
The preliminary experiments use small and medium-sized
problems (n=20, 30, 40, 50, and m=3, 4, 5) to identify the
best configurations for the hybrid TPEPSO algorithm.
Parameter selection is a very important issue for the hybrid
TPEPSO algorithm. To determine suitable parameter
settings, the hybrid TPEPSO algorithm is run ten times
for each problem instance. The following ranges of
parameter values are tested (Kennedy et al. [43] and He
et al. [44]): Np=(20, 200), c1=(0.5, 4.0), c2=(0.5, 4.0),
and w=(0.8, 1.2). Experimental results indicate that
population size is twice the number of jobs; this is slightly
better than a fixed swarm size. Parameters c1 and c2 are
tested between 0.5 and 4.0 in increments of 0.1, and the
acceleration coefficients are c1=c2=2, which is consistent
Int J Adv Manuf Technol
time, no significant difference exists between the PSOSPVR
and PSOAPVR algorithms. Notably, the PSOTPR algorithm
produces a lower mean PD increase in TWCT than the
PSOSPVR and PSOAPVR algorithms; the mean PD was
reduced from both 2.18% and 1.92% to 1.01%, and the
number of best-known solutions found increased from both
1.08 and 1.58 to 3.50 on overall average.
Table 2 also shows the impact of the TS in the PSOTPR
algorithm on solution quality. As is seen, the PSOTPR+TS
algorithm obtains the best-known solutions for all instances
85.8% of the time while for the PSOTPR algorithm obtains
the best-known solutions 35.0% of the time. To summarize,
extensive use of the PSOTPR algorithm generated results no
worse than 1.01% from the best-known solutions. However,
incorporating of TS into the TPR (PSOTPR+TS) encoding
scheme further improves experimental results to 0.85% for
the mean PD increase in TWCT. Regarding the execution
times, the additional CPU time needed is minor on overall
average. Therefore, the TS is an efficient way of using the
time saved by applying the proposed candidate list strategy,
especially for larger problems.
As a further comparison, additional tests are conducted
for large problems (n=60, 80, and 100 and m=8, 12, 16,
and 20). In total, 260 problem instances are tested. The
PSOSPVR algorithm slightly outperforms the PSOAPVR
algorithm in terms of the mean PD increase in TWCT
(Table 3). Test results show that both the PSOSPVR and
PSOAPVR algorithms do not generate the best-known
solutions. Furthermore, for a given number of jobs with a
large number of machines, the performance of the PSOAPVR
algorithm is poor. The effects of converting search space
from the continuous domain into the discrete domain must
be determined further. Mapping will yield the better results
due to the use of the PSOAPVR algorithm. Moreover, as the
number of jobs increases, the PSOTPR algorithm can still
find the best-known solutions for 60×3×4 and 80×3×4
problems, and the mean PD increase in TWCT, which is a
response to the large problems, is reduced from both 5.17%
and 6.10% to 3.15%.
Finally, the TS incorporated into the PSOTPR algorithm
(PSOTPR+TS) can find all best-known solutions, except for
those for the 80×3×20 and 100×3×20 problems. Never-
theless, the minimum PD increase in TWCT for problems
80×3×20 and 100×3×20 are 0.54% and 1.28%, respec-
tively, both of which are small. Additionally, increasing the
number of machines increases CPU times; for small
problems, the additional CPU time needed is minor, but
becomes significant for large problems, as expected. This is
because most of computation time is spent on the TS
process. As mentioned in Section 6.4, solution quality by
the TS is primarily affected by its initial solution.
Therefore, the TS is only utilized in the second phase to
explore the space for solution improvement, not at the startTa
b
le
3
P
er
fo
rm
an
ce
co
m
pa
ri
so
n
of
di
ff
er
en
t
P
S
O
en
co
di
ng
sc
he
m
es
fo
r
la
rg
e
pr
ob
le
m
s
P
ro
bl
em
s
P
S
O
S
P
V
R
P
S
O
A
P
V
R
P
S
O
T
P
R
P
S
O
T
P
R
+
T
S
n
×
s×
m
M
in
M
ax
A
vg
N
B
t a
v
g
M
in
M
ax
A
vg
N
B
t a
v
g
M
in
M
ax
A
vg
N
B
t a
v
g
M
in
M
ax
A
vg
N
B
t a
v
g
60
×
3
×
4
1.
01
6.
41
3.
66
0
1.
51
1.
12
4.
61
3.
01
0
1.
02
0.
00
2.
54
1.
04
2
0.
86
0.
00
0.
83
0.
20
7
4.
43
60
×
3
×
8
1.
14
7.
05
3.
85
0
2.
68
2.
04
8.
05
4.
97
0
1.
65
1.
12
4.
11
2.
46
0
1.
42
0.
00
1.
88
0.
63
4
13
.4
7
60
×
3
×
12
3.
30
8.
43
4.
94
0
4.
21
2.
98
8.
70
5.
34
0
2.
34
1.
55
4.
27
2.
69
0
2.
63
0.
00
1.
36
0.
78
4
19
.7
9
60
×
3
×
16
2.
21
6.
12
4.
23
0
5.
48
4.
21
9.
07
6.
12
0
3.
15
1.
79
4.
42
3.
05
0
3.
50
0.
00
1.
54
0.
76
2
28
.2
1
80
×
3
×
4
1.
23
7.
49
3.
98
0
2.
31
1.
36
4.
88
3.
07
0
1.
08
0.
00
2.
47
1.
25
1
1.
05
0.
00
1.
20
0.
38
5
7.
50
80
×
3
×
8
1.
87
7.
28
4.
41
0
3.
94
1.
59
8.
25
5.
02
0
1.
74
1.
05
3.
95
1.
75
0
1.
69
0.
00
2.
25
0.
72
4
15
.4
5
80
×
3
×
12
2.
37
8.
58
5.
63
0
5.
19
3.
45
10
.5
6.
72
0
2.
59
1.
46
4.
78
3.
12
0
3.
18
0.
00
2.
34
1.
06
2
24
.2
3
80
×
3
×
16
2.
59
8.
31
5.
12
0
7.
07
3.
71
11
.2
6.
95
0
3.
20
2.
04
4.
99
3.
27
0
4.
74
0.
00
2.
86
1.
54
2
35
.0
7
80
×
3
×
20
2.
50
7.
30
4.
30
0
8.
42
4.
12
14
.3
8.
21
0
4.
02
2.
13
6.
34
4.
78
0
5.
88
0.
54
3.
08
2.
29
0
49
.5
9
10
0
×
3
×
8
2.
12
8.
21
4.
76
0
4.
33
2.
33
8.
41
4.
17
0
1.
85
0.
86
3.
78
2.
12
0
2.
31
0.
00
2.
47
1.
51
2
21
.3
6
10
0
×
3
×
12
2.
59
9.
11
5.
94
0
6.
05
2.
89
11
.1
7.
30
0
2.
55
1.
95
5.
30
4.
12
0
4.
60
0.
00
2.
88
1.
67
1
35
.1
8
10
0
×
3
×
16
4.
01
11
.0
8.
11
0
7.
21
4.
12
10
.9
8.
84
0
3.
21
3.
87
6.
78
5.
20
0
5.
83
0.
00
3.
02
2.
23
1
50
.7
7
10
0
×
3
×
20
5.
02
11
.5
8.
27
0
9.
13
5.
85
15
.3
9.
62
0
4.
14
4.
31
8.
81
6.
16
0
7.
19
1.
28
3.
90
2.
57
0
79
.0
5
M
ea
n
2.
46
8.
22
5.
17
0.
00
5.
19
3.
06
9.
65
6.
10
0.
00
2.
50
1.
70
4.
81
3.
15
0.
23
3.
45
0.
14
2.
28
1.
26
2.
62
29
.5
5
Int J Adv Manuf Technol
compares with the proposed hybrid TPEPSO using the
other set of benchmark instances with the processing time
and weight of each job uniformly drawn from [1, 20] and
[1, 100], respectively.
Tables 4 and 5 summarize comparison results obtained
by the CG, HCGA, PTS, and PSOTPR+TS algorithms. Both
the HCGA and PSOTPR+TS algorithms obtain the best-
known solutions (Table 4). However, the PSOTPR+TS
algorithm performs better than the HCGA in terms of
number of best-known solutions found and average time,
even though the HCGA also employs a TS-based tech-
nique. This can be explained in that the PSOTPR algorithm
provides better initial solutions for the TS than the HCGA
during the evolution process, indicating that solution
quality of the TS is principally affected by its initial
solution [38]. Furthermore, the best solutions yielded by
PSOTPR+TS are better than that of the TS algorithm used
alone.
For the large problems (Table 5), the PSOTPR+TS
algorithm found all the best-known solutions produced by
the CG algorithm, except for the 80×3×20 and 100×3×20
problems while the HCGA algorithm found the best-known
solutions for only the 60×3×4, 60×3×8, 80×3×4, and
80×3×8 problems. Moreover, the PTS algorithm cannot
find the best-known solution for all large problems.
Regarding the CPU time requirement of PSOTPR+TS and
CG, the PSOTPR+TS algorithm consumes much less com-
putation time than the CG algorithm; notably, the CG
algorithm requires considerable CPU time for an instance
with an n/m ratio of >10. The reasons accounting for
slowing the CG algorithm are found in [15]. Therefore, we
conclude that the PSOTPR+TS algorithm is very stable and
robust in terms of solution quality and computation time.
8 Conclusions
To the best of our knowledge, this is the first attempt at
two-phase encoding scheme in a PSO for parallel-identical-
machine scheduling problems. Previous study solved the
PFFS problem with strong lower bounds using a CG
approach. However, the CG approach requires considerable
computation time when the n/m ratio is relatively large.
Thus, this study combines the TPEPSO algorithm and TS
to solve the PFFS problem with TWCT minimization. By
hybridizing the complementary properties of TPEPSO and
TS, the proposed hybrid TPEPSO algorithm combines the
cooperative and competitive characteristics of TPEPSO and
TS. Moreover, a candidate list strategy is designed for TS,
which saves time and improves iteration performance.
The performance of the proposed hybrid TPEPSO
algorithm was evaluated using a set of benchmark problems
given by [15]. A comparison of experimental results with
the existing approaches demonstrates the robustness of the
hybrid TPEPSO in terms of solution quality and execution
time. Moreover, the proposed hybrid TPEPSO is much
faster than the CG approach for all the instances. In
summary, experimental results are encouraging and prom-
ising for other parallel machine scheduling problems.
Further research should extend the TPEPSO algorithm by
incorporating other local search methods for complex PFFS
problems with different machine speeds at various stages; i.
e., the PFFS problem with processing time of job j at stage i
is pij=pj/si, where si is machine speed at stage i.
References
1. Pinedo ML (2002) Scheduling: theory, algorithms, and systems,
2nd edn. Prentice-Hall, Englewood Cliffs
2. Ruiz R, Vázquez-Rodríguez JA (2010) The hybrid flow shop
scheduling problem. Eur J Oper Res 205:1–18
3. Linn R, Zhang W (1999) Hybrid flow shop scheduling: a survey.
Comput Ind Eng 37:57–61
4. Garey M, Johnson D, Sethi R (1976) The complexity of flow shop
and job shop scheduling. Math Oper Res 1:117–129
5. Ow PS (1985) Focused scheduling in proportionate flow shops.
Manage Sci 31:852–869
6. Pinedo ML (1985) A note on stochastic shop models in which
jobs have the same processing requirements on each machine.
Manage Sci 31:840–845
7. Edwin Cheng TC, Shakhlevich N (1999) Proportionate flow shop
with controllable processing times. J Sched 2:253–265
8. Hou S, Hoogeveen H (2003) The three-machine proportionate
flow shop problem with unequal machine speeds. Oper Res Lett
31(3):225–231
9. Koulamas C, Kyparisis GJ (2009) A note on the proportionate
flow shop with a bottleneck machine. Eur J Oper Res 193:644–
645
10. Choi BC, Yoon SH, Chung SJ (2007) Minimizing maximum
completion time in a proportionate flow shop with one machine of
different speed. Eur J Oper Res 176(2):964–974
11. Allahverdi A, Savsar M (2001) Stochastic proportionate flowshop
scheduling with setups. Comput Ind Eng 39(3):357–369
12. Shakhlevich NV, Hoogeveen H, Pinedo ML (1998) Minimizing
total weighted completion time in a proportionate flow shop. J
Sched 1:157–168
13. Choi BC, Yoon SH, Chung SJ (2006) Minimizing the total
weighted completion time in a two-machne proportionate flow
shop with different machine speeds. Int J Prod Res 44(4):715–
728
14. Estevez-Fernandez A, Mosquera MA, Borm P, Hamers H (2008)
Proportionate flow shop games. J Sched 11:433–447
15. Huang YM, Shiau DF (2008) Combined column generation and
constructive heuristic for a proportionate flexible flow shop
scheduling. Int J Adv Manuf Technol 38(7/8):691–704
16. Van den Akker JM, Hoogeveen JA, Van De Velde SL (1999)
Parallel machine scheduling by column generation. Oper Res
47:862–872
17. Chen ZL, Powell WB (1999) Solving parallel machine scheduling
problems by column generation. INFORMS J Comput 11(1):78–
94
18. Kennedy J, Eberhart RC (1995) Particle swarm optimization. In:
Proceedings of IEEE International Conference on Neural Networks.
pp 1942–1948
Int J Adv Manuf Technol
國科會補助計畫衍生研發成果推廣資料表
日期:2011/09/05
國科會補助計畫
計畫名稱: 結合行產生法與建構啟發式解決具有共同期日之成比例的彈性產流式生產排
程問題
計畫主持人: 蕭德芳
計畫編號: 99-2221-E-242-006- 學門領域: 生產系統規劃與管制
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
