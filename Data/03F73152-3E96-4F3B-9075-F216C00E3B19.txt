algorithm will be able to perform the tasks of 
localization and mapping in long-distance and large-
area environments. 
英文關鍵詞： Mobile Robot； Simultaneous Localization, Mapping and 
Moving Object Tracking； Moving Object Detection； 
Image Feature Detection and Description； Map 
Management. 
 
 2 
環境下進行同時定位、建圖、與移動物體追蹤
(Simultaneous Localization, Mapping, and Moving 
Object Tracking, SLAMMOT)任務。Wang et al. [2007]
探討 SLAMMOT的理論架構，並使用LRF實現 SLAM
與移動物體的交互多模式(interactive multiple model, 
IMM)追蹤。Sola et al. [2007]在動態環境下使用立體視
覺實現 SLAM，以及應用規則庫(rule-based)偵測移動
物體。以上兩者都是將 SLAM 與移動物體追蹤(MOT)
分開處理，以降低計算複雜度。本計畫以增加狀態變
數方式嘗試結合 SLAM 與 MOT，建構視覺式
SLAMMOT 系統。 
應用於 SLAM 的視覺式感測模組所萃取的低階
影像特徵(low-level image features)或地標，分為點特
徵(point features)與區域特徵(region features)兩種型
式：點特徵通常搭配周圍的影像貼片(image patch)進
行特徵的偵測與追蹤[Davison et al. 2007]；區域特徵
則通常搭配尺度空間 (scale space) 與描述向量
(description vector)進行特徵的偵測與追蹤[Bay et al. 
2008]。使用點特徵進行機器人的自我定位的優點是架
構簡單與容易實現，缺點是尺度與方位固定的影像貼
片在追蹤過程中容易發生特徵遺失或誤判，因為攝影
機的運動會改變拍攝的距離與角度，使得特徵的影像
貼片改變以致追蹤遺失或誤判。使用區域特徵的優點
是影像特徵的尺度與方向不變(scale and orientation 
invariant)，缺點是運算相對耗時，典型範例為 Lowe 
[1999] 所提的尺度不變特徵轉換 (Scale Invariant 
Feature Transform, SIFT)方法，無法達到快速即時的運
算效果，vSLAM[Karlsson et al. 2005]的影像特徵偵測
即是使用 SIFT 方法。Bay et al. [2008]提出以盒子過濾
器(box filter)結合積分影像(integral image)方法進行區
域特徵偵測，稱為加速強健特徵(Speeded-Up Robust 
Features, SURF)，可以達到即時運算的要求。本計畫
主持人在 99 年國科會計畫(NSC99-2221-E-032- 064)
中使用 Bay et al.所提的 SURF 方法，規劃機器人即時
視覺式 SLAM 所需的感測模組，改善文獻中[Davison 
et al. 2007]使用點特徵追蹤容易遺失或誤判的缺點。
本計畫進一步針對 SURF 的偵測與描述向量方法，提
出修改的程序，以便在動態環境中應用。 
本研究發展單眼視覺式同時定位、建圖、與移動
物體追蹤演算法，應用在行動機器人系統上，使其能
在未知的室內動態環境中進行同時自我定位、建圖、
與 追 蹤 物 體 (SLAMMOT) 的 任 務 。 並 且 針 對
SLAMMOT 過程中移動物體偵測與大範圍地圖建
立，提出新的演算法。也針對影像特徵偵測與描述、
以及具未知輸入項的移動物體之追蹤方法，提出改善
方案。最後，發展的演算法在手持攝影機(hand-held 
camera)上測試。 
 
2. 研究目的 
行動機器人視覺式同時定位、建圖與移動物體追
蹤的能力，一直是機器人實際應用時所面臨最重要也
最具有挑戰性的議題。因為多目標追蹤估測演算法、
影像特徵偵測與描述、以及移動物體偵測等演算法皆
是費時的運算程序，要有效地整合成為即時運作的系
統，必須根據其基礎理論與應用工具深入探討，並且
選擇適當的實現方法，以便行動機器人系統在動態環
境中能即時與準確地進行同時自我定位與建圖。另
外，受限於機器人的感測能力，研究人員一直挑戰更
好更快的感測模組，結合不同的 SLAMMOT 演算法，
以便應用在處於不同環境中的行動機器人系統。目前
影像處理領域已發展出很多適合機器人即時運作所
需的特徵偵測與描述演算法，修改後可做為機器人的
視覺式感測模組。針對實現即時視覺式動態環境中定
位與建圖的目標，本研究直接切入困難度較高的視覺
式 SLAMMOT 之研究議題，整體性探討機器人的運
動模型、視覺式感測模組、以及移動物體偵測等模組
的搭配與整合問題，解決即時多目標追蹤問題。並且
應用在機器人於室內動態環境中定位與建圖的任務
上，期望與國外相關的研究進度縮小差距甚至超越。
我們以處理多目標追蹤的方法，搭配適當的視覺式感
測裝置，整合為行動機器人的同時自我定位、地圖建
立、與移動物體追蹤演算法，在未知且大範圍的動態
環境中進行巡航任務。 
 
3. 文獻探討 
3.1 機器人定位與建圖相關文獻 
SLAM 問題由 Smith et al. [1990]提出後，目前文
獻中針對具非線性系統特性的機器人 SLAM，主要有
兩類解決方法：分別為 EKF SLAM 與機率式 SLAM
兩類[Durrant-Whyte and Bailey 2006]。EKF SLAM 
[Dissanayake et al. 2001]對機器人與地圖同時進行估
測，其優點為系統近似線性與輸入為已知情況下，運
算過程中收斂速度較快。但是，在 SLAM 後件部
(posterior)為高次非線性的狀況下，其運算結果無法保
證收斂；針對高維度地圖時，EKF 需要之矩陣運算將
會非常龐大與複雜。機率式 SLAM 則使用粒子過濾器
(particle filter)針對 SLAM 的後件部進行估測，藉由粒
子之移動與權重計算以搜尋最佳解[Dellaert et al. 
1999]。因為在過程中不需對 SLAM 後件部線性化，
沒有 EKF SLAM 所遇到線性化後的問題，因此其演算
法的強健性較 EKF SLAM 為佳。Murphy and Russell 
[2001]提出分解機率式 SLAM 問題的概念，使得機率
式 SLAM 得以實現。Montemerlo [2003]根據此概念提
出定位與建圖的分解求算再整合的方法。機率式
SLAM 的缺點則是在於所建立地圖精確度較低。EKF 
SLAM 與機率式 SLAM 各有優缺點，本計畫使用狀態
變數關聯性較高的 EKF 估測方法，進行 SLAMMOT
的發展，以確保在動態環境中得到較精確的地圖與移
動物體訊息。 
 
3.2 移動物體偵測與追蹤的相關文獻 
移動物體偵測是視覺式 SLAMMOT 的重要關
鍵，將移動物體從靜態地圖特徵中分辨出來，以免
SLAM 演算法失效。Sola et al. [2007]以規則庫(rule 
based)偵測靜止或移動的物體。Lin and Wang [2010]
則以新增特徵時 EKF 共變異數矩陣運算之差異值，分
辨該特徵為靜止或移動狀態。本質矩陣 (essential 
matrix)[Longuet-Higgins 1981, Hartley 1997]對於相同
場景不同視角的兩張影像之像素具有相關聯限制條
件(correspondence constraints)。應用在電腦視覺中，
假如兩張影像中相關聯的特徵像素位置與攝影機內
部參數為已知時，利用此限制條件可以估測攝影機在
 4 
物體集合，即一般化物體(generalized objects)。以貝氏
機率表示 SLAMMOT 的後驗機率為 
111:11:111
11
:1:1
),,(
)(),(),(
),,(
−−−−−−
−− ⋅∝ ∫∫
kCkkkkCk
kkkCkCkkCkk
kkkCk
dd|p
|p|p|p
|p
YY
YYY
Y
xzux
uxxxz
zux
  (6) 
雙重積分部份為 SLAMMOT 問題的預測階段，其中
) , | ( 1 kCkCkp uxx − 與 ) | ( 1−kkp YY 分別是機器人與一般
化物體的運動模型。在預測階段，使用方程式(3)將 k-1
時刻的後驗機率經由狀態運動模型，預測出 k 時刻的
先驗狀態。而 p(zk|xCk,Yk)則為感測模型，在更新階段，
使用方程式(4)藉由 k 時刻的量測 zk，將先驗狀態更新
為後驗狀態。假設機器人與一般化物體的狀態可以各
自獨立被預測；SLAM 狀態與移動物體的行動模式無
相互影響；以及量測資料 z 可被分解為靜止目標的量
測資料 zm與移動物體的量測資料 zo，則可用擴增狀態
卡爾曼過濾器(ASKF)概念[Smith et al. 1990, Deaves 
1999]與方程式(5)EKF 估測方法，求解方程式(6)的
SLAMMOT 問題[Wang et al. 2007]。本研究針對以上
所提的移動物體偵測與量測可分解之假設、以及
SLAM 求解方法，提出可實現的方案。 
 
B. 單眼視覺式 SLAMMOT 的運動模型 
用手拿著攝影機在環境中自由移動時，如圖 1 所
示，或者將攝影機裝攝在機器人上巡航時，對攝影機
系統而言，具有未知的輸入項 uk，這是手或機器人的
動作所引起的。使用 EKF 針對方程式(1)的非線性系
統進行狀態估測，當輸入 uk為未知時，可以考慮此未
知輸入為系統的加速度項，當系統以近似等速度進行
運動時，未知輸入可視為具高斯分佈的雜訊，典型的
範例是 Davison et al. [2007]所發展的 MonoSLAM。假
設自由移動的攝影機進行等速度運動，而加速度則為
手持外力所施加的脈衝雜訊(impulse noise)所引起，攝
影機的速度雜訊定義為 
⎥⎦
⎤⎢⎣
⎡
Δ
Δ=⎥⎦
⎤⎢⎣
⎡
t
ta
w
w
αω
v  
其中 a 與α 分別為攝影機的線性加速度與角加速
度， tΔ 為取樣時間，而 vw 與 ωw 是加速度所造成的
速度雜訊。使用增加狀態卡爾曼過濾器(ASKF)的概
念，將包含未知速度輸入的機器人狀態、地圖特徵、
與移動物體等都視為系統狀態，增加在狀態向量之
中，表示為 
TT
n
TTT
n
TTT
C ],,,, [ 21 1121 oooYYY LL,,,,xx =    (7) 
其中 xC為攝影機狀態，包含攝影機的位置 r、角度φ、
線性速度 v、角速度ω；Yi 是 n1 個地圖特徵點中第 i
個特徵的狀態向量，也就是特徵的空間三維座標；oi
是 n2個移動物體中第 i 個移動物體的狀態向量。攝影
機狀態 xC表示為 ( )
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
+
+
Δ++
Δ++
=
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
==
−−
−−
−−−
−−−
−−−
11
11
111
111
111
)(
)(
 , ,
kk
vkk
kkk
vkkk
k
k
k
k
kkCkCk
w
w
tw
tw
wf
ω
ω
ω
ωφ
ω
φ
v
v
v
uxx
rr
   (8) 
方程式(7)的地圖特徵點 Yi 在世界座標中的三維
座標向量，如圖 2 所示，可以表示為 
C
ik
W
Ckik hRrY +=  (9) 
其中 TCizCiyCixCi hhh ][=h 為視線(ray)向量，是特徵點
相對於攝影機中心的三維座標； WCR 為攝影機座標系
{C}(camera frame)相對於世界座標系{W}的旋轉矩陣
(rotation matrix)。使用相對於世界座標的基本旋轉
(elementary rotations)[Sciavicco and Siciliano 1996]表
示為 
⎥⎥
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢⎢
⎢
⎣
⎡
−
−+
+−
=
yxyxy
zxzyxzxzyxzy
zxzyxzxzyxzy
W
C
cccss
cssscccssssc
sscscsccsscc
φφφφφ
φφφφφφφφφφφφ
φφφφφφφφφφφφ
R
(10) 
其中 φφ cosc = ； φφ sins = ；φx、φy與φz為相對於
世界座標軸的轉動角度。單眼攝影機無法即時得知特
徵點的深度，一般會藉由兩時刻的狀態來估算，然而
這種方法會有延遲現象，因此，Civera et al. [2008]提
出反深度參數化的方法來克服延遲的問題。反深度參
數化是假設估測過程中反深度為高斯函數分佈，特徵
點一旦被偵測，立即指派一組初始值給特徵點狀態。
初始化的特徵點狀態向量代入 EKF 狀態中，進行 EKF
預測與更新的遞迴程序，持續將特徵點影像反深度收
斂。本研究使用六維的反深度狀態向量初始化特徵點
的狀態： 
T
i
W
i
W
iiziyixi rrr ][ ρψθ=Y    (11) 
其中 rix、riy及 riz是第 i 特徵點被偵測時攝影機的估測
位置； Wiθ 與 Wiψ 分別是視線向量的經緯角度，定義
如圖 3 所示； iρ 則為特徵點的影像深度之倒數(反深
度)。視線向量 Wih 可以描述為 
) ,(1 Wi
W
i
i
C
ik
W
C
W
i θ ψρ mhRh == ; 
⎥⎥
⎥⎥
⎦
⎤
⎢⎢
⎢⎢
⎣
⎡
=
)()(
)(
)()(
) ,(
W
i
W
i
W
i
W
i
W
i
W
i
W
iθ
ψθ
ψ
ψθ
ψ
cossin
sin
coscos
m  
而 m 是單位視線向量。依據文獻[Civera et al. 2008]所
定義的反深度線性指標收斂情形，可以將方程式(11)
的特徵點六維狀態向量轉換為方程式(9)的三維座
標，而共變異數矩陣也必須依據誤差傳遞法則(error 
propagation law)進行轉換。 
本研究使用等速度模型(CV)描述方程式(7)中移
動物體 oi的狀態向量，表示為 
[ ]TTiTi
T
zyxi
W
i
W
iiziyixi rrr
vp
o == ]vvv[ ρψθ (12) 
將移動物體假設成等速度的狀態，並且將移動物體的
加速度視為白雜訊，則 
twkkk Δ+= −11-vv  
) ,(11-1-
W
i
W
i
i
ikkk θt ψρ mrpp +=Δ+= v   (13) 
其中 w 為時間序列的程序雜訊。 
使用 ASKF 規劃 SLAMMOT 會增加狀態變數的
維度，進而增加共變異數矩陣的運算量。此問題可以
 6 
 
圖 4 SLAMMOT 流程圖 
 
D. 影像特徵偵測與特徵描述 
以影像特徵建立環境地圖時，影像特徵必須能夠
強健地被比對到。如圖 4 流程所示，本研究使用加速
強健特徵(SURF)方法[Bay et al. 2008]，期望能夠從影
像中快速且強健地偵測出影像特徵。在 99 年的國科
會計畫(NSC99-2221-E-032-064)中，使用 Bay et al.所
提供的 SURF 偵測、描述與追蹤程序[OpenCV 2010]，
每張影像 100 個特徵的計算時間約 70~80ms，動態影
像情況下的成功辨識率可達 50%~60%。計算時間仍然
太長且辨識率仍然偏低，本計畫規劃改善 SURF 的偵
測、描述與追蹤程序，以符合 SLAMMOT 更嚴苛的
計算時間與辨識率要求。 
SURF 的優點是相對於影像平移(translation)、尺
度(scale)改變、與旋轉(rotation)時，SURF 的描述向量
(description vector)仍然保持不變，有利於影像特徵的
偵測與追蹤；此外，相對於流明(illumination)改變與
影像仿射轉換(affine transformation)，SURF 的描述向
量也能大略保持不變。SURF 的缺點仍是計算量大，
應用在 SLAMMOT 時會是不小的負擔。SURF 偵測尺
度不變特徵的基本概念是求算 Hessian 矩陣的行列式
值[Mikolajczyk and Schmid 2001]，Bay et al.使用盒子
過濾器(box filter)與影像迴積(convolution)取代費時的
Hessian 矩陣行列式求算。Hessian 矩陣行列式以下列
近似方程式求算 
2
approx.det )()( xyyyxx wDDDΗ −=    (17) 
其中 Dij 是影像使用相對應的盒子過濾器迴積後的影
像，w 是權重常數。特徵偵測與追蹤程序規劃為：首
先，經由檢測影像 Hessian 矩陣行列式的極值(extreme 
value)偵測出特徵點。其次，使用高維度的向量描述
被偵測到的特徵點所具備的獨特性質，如圖 5 所示。
在每個細分的子區域中將有 4 維的特徵向量，表示為
v=(Σdx,Σdy,Σ|dx|,Σ|dy|)，Σdx、Σdy、Σ|dx|、與 Σ|dy|分別為
x 與 y 方向的 Haar 小波反應值與其絕對值的總和，如
圖 5 所示的影像特徵描述向量之總計維度為
4×4×4=64。最後，兩張連續影像偵測出特徵後，可以
進行特徵比對，方法是計算相對應之描述向量的尤基
里德距離 d，再使用最鄰近比率比對策略[Lowe 2004]
進行影像特徵的比對，則兩張影像的特徵可以有效率
地被比對出。比對比率 r 定義為最小尤基里德距離 d1st
相對於次小尤基里德距離 d2nd 的比值，如果比對比率
小於 0.7，則兩個特徵被比對出。 
本研究針對 SURF 的影像特徵偵測、特徵描述、
與追蹤比對等方面，提出修改的程序，目的是降低計
算時間與提高特徵辨識率。特徵偵測程序方面，減少
較大音階(octave)特徵的偵測；特徵描述程序方面，降
低 SURF 描述向量維度。特徵追蹤方面，只新增特定
距離範圍內的特徵，以及規劃特徵偵測與描述向量計
算函式的程序。最後，探討修改盒子過濾器(box filter)
的可能性，以簡化特徵偵測程序與增加辨識率。 
dx dy
 
圖 5 SURF 的描述向量 
 
E. 移動物體偵測 
在圖 4 流程圖中的移動物體偵測(moving object 
detection)功能，本研究提議使用本質矩陣(essential 
matrix)的概念進行移動物體的偵測。本質矩陣
(essential matrix)定義為 
RRtE
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
−
−
−
== ×
0
0
xy
xz
yz
tt
tt
tto
][    (18) 
其中 t=[tx ty tz]T為平移向量(translational vector)；[t]×是
與 t 向量外積之矩陣的表示式；R 是旋轉矩陣(rotation 
matrix)。假設對靜態物體上的特徵點拍攝兩張影像，
兩張影像中相關聯(correspondence)的像素點，具有下
列的限制條件(constraint)[Longuet-Higgins 1981] 
0=′Cd
TC
d hEh    (19) 
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
= −
1
1
y
x
C
C
d I
I
Kh   
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
′
′
=′ −
1
1
y
x
C
C
d I
I
Kh  
⎥⎥
⎥
⎦
⎤
⎢⎢
⎢
⎣
⎡
=
100
0 0
0
vf
uff
v
uCu
C
α
K  
其中[Ix Iy]T 與 [ ]Tyx II ′′ 分別為第一張影像與第二張
影像中相關聯之像素點的像素座標；而 Cdh 與 Cdh′ 為相
關聯之特徵點的正規化影像座標。方程式(18)平移向
量 t 與旋轉矩陣 R 可以由 SLAMMOT 估測而得。因
此，方程式(19)可以用來偵測特徵點是否位在靜態物
體上。使用此方法偵測移動物體，可以在影像平面立
即判斷物體處於動態或靜態。 
 
4. 單眼視覺 EKF SLAMMOT 測試範例 
此範例使用本論文所建立的系統，實驗中手持攝
影機於室內環境中持續執行 SLAMMOT 任務，突然
一個人走出來，經本質矩陣辨識出移動物體後，將此
動態地圖特徵置入 SLAMMOT 系統中進行追蹤任
務，證明此系統若觀測到移動物體，不會影響 SLAM
 8 
 
(e) 385th影像：動態地圖特徵狀態收斂。 
 
(f) 391th影像：動態地圖特徵狀態持續更新位置到穩定
狀態。 
 
(g) 410th影像：動態地圖特徵狀態穩定追蹤。 
 
(h) 620th影像：完成 EKF MonoSLAMMOT，並回到初
始位置。 
圖 8 EKF SLAMMOT 在室內環境的八張截圖 
 
5. 完成的研究項目 
本研究使用本質矩陣的極線限制條件於單眼視覺
進行移動物體偵測與追蹤，在移動物體的偵測與追
蹤，資料關聯與地圖管理策略，以及攝影機校正與影
像修正程序等三方面，分別提出具體與實用的方案。
最後，實測所發展的單眼視覺式 EKF SLAMMOT 系
統，測試系統的移動特徵偵測功能(MOD)、地面基準
(ground truth)的誤差、路徑閉合的能力、執行長距離
的 SLAM 任務、以及移動物體的偵測與追蹤能力。本
計畫具體的貢獻包括： 
a. 利用本質矩陣與靜態物體在兩張影像平面的像素
座標所構成的位置限制條件，進行移動物體的偵
測； 
b. 規畫移動物體的資料關聯程序、以及移動物體特徵
管理的新增、刪除與更新策略； 
c. 整合以上方法建立單眼視覺式 EKF SLAMMOT 的
系統架構，並應用於所發展的系統，實測地面基準
(ground truth)與進行室內環境閉迴路以及圖書館長
距離地圖建立；並且實測使用單眼視覺進行同時定
位建圖與移動物體的偵測與追蹤，於系統啟動初期
可達到近 20Hz 的即時性。 
 
6. 自我評價 
本研究主要貢獻在於學術方面，在影像區域特徵偵測
與追蹤、機器人 EKF SLAMMOT、以及建圖與地圖管
理等方面，目前已趕上其他國家研究進度，甚至在某
些方向已推進研究深度。實作方面，Matlab SLAMMOT
模擬與分析、區域特徵偵測與追蹤演算法、視覺式
SLAMMOT、以及與控制所需的機電整合與人工智慧
所發展的技術，可應用於相關產業的發展。 
本研究的主要工作項目包含理論與實作兩方面。
共培訓 1 位博班研究生，3 名碩班研究生與 6 名大學
部學生。研究生主要學習影像特徵偵測與追蹤軟體設
計、視覺式 SLAMMOT 系統設計與實作，以及機器人
模擬與分析、建圖與地圖管理、網路通訊、與合作巡
航等方面的理論推導與實現。而參與的大學生獲得機
器人機構設計與驅動、自我定位、建圖、與巡航等實
作方面的技術養成，以及基本研究程序方面的訓練。
主持人獲得與國際相關研究團隊在自主行動機器人的
理論與實務方面，相互競爭與學習的機會。研究成果
已投稿發表三篇期刊論文，以及國內外舉辦的國際研
討會兩篇，如下所列： 
 
A. 發表的期刊論文 
a. Wang, Y.T., C.T. Chi and Y.C. Feng, 2012, Robot 
Simultaneous Localization and Mapping Using Speeded-Up 
Robust Features, Applied Mechanics and Materials. (EI) (in 
press) 
b. Wang, Y.T., C.H. Sun, and M.J. Chiou, 2012, Detection of 
Moving Objects in Image Plane for Robot Navigation using 
Monocular Vision, EURASIP Journal on Advances in Signal 
Processing, 2012:29. (SCI, EI) 
c. Wang, Y.T., C.T. Chi, and S.K. Hung, 2012, Robot Visual 
SLAM in Dynamic Environments, Advanced Science Letters, 
Vol.8, pp.229-234. 
 
B. 發表的國際研討會論文 
a. Yin-Tien Wang, Kuo-Wei Chen, and Ming-Jang Chiou, 
2012, Moving Object Detection Using Monocular Vision, 
the 12th International Conference on Intelligent 
Autonomous System (IAS-12), Jeju Island, Korea on June 
26 - 29. 
b. Wang, Y.T., C.T. Chi, and S.K. Hung, 2011, Robot Visual 
SLAM in Dynamic Environments, The First International 
Conference on Engineering and Technology Innovation 
(ICETI2011), Nov. 11-15, Kenting, Taiwan. 
 10 
[41] Thrun, S., 2002, Robotic mapping: A survey, in 
Exploring Artificial Intelligence in the New Millennium, 
G. Lakemeyer and B. Nebel, Eds.. San Mateo, CA: 
Morgan Kaufmann, ch. 1. 
[42] Thrun, S., W. Burgard, and D. Fox, 2005, Probabilistic 
Robotics, The MIT Press, Cambridge, MA. 
[43] Tsai, R.Y., 1987, A versatile camera calibration technique 
for high accuracy 3D machine vision metrology using 
off-the-sheff TV cameras and lenses, IEEE Journal of 
Robotics and Automation, 3(4), pp.323-344. 
[44] Wu, C.Y., and L.C. Fu, 2007, An integrated robotic 
vSLAM system to realize exploration in large indoor 
environment, Proceedings CACS International Automatic 
Control Conference. 
[45] Yamaguchi, K., T. Kato and Y. Ninomiya, 2006, Moving 
obstacle detection using monocular vision, Proceedings 
of IEEE Intelligent Vehicles Symposium, pp.288-293. 
[46] Zhang, Z., 1998, A flexible new technique for camera 
calibration, Microsoft Research Technical Report 98-71. 
[http://research.microsoft.com/~zhang] 
[47] Zhang, Z., 2000, A Flexible New Technique for Camera 
Calibration, IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 22, no. 11. 
 
 
scale and orientation of the image patches will be chan-
ged. The detection and matching of Harris corner might
fail in this case, unless the variances in scale and orien-
tation of the image patches are recovered. Instead of
detecting corner features, some works [7,8] detect the
features by using the scale-invariant feature transform
(SIFT) method [9] which provides a robust image fea-
ture detector. The unique properties of image features
extracted by SIFT method are further described by
using a high-dimensional description vector [9]. How-
ever, the feature extraction by SIFT requires more com-
putational cost than that by Harris’s method [6]. To
improve the computational speed, Bay et al. [10] intro-
duced the concept of integral images and box filter to
detect and extract the scale-invariant features, which
they dubbed speeded-up robust features (SURF). The
extracted SURF must be matched with the landmarks in
the map of a SLAM system. The nearest-neighbor (NN)
searching method [11] can be utilized to match high-
dimensional data sets of description vectors.
In this article, an online SLAM system with a moving
object detector is developed based on the epipolar con-
straint for the corresponding feature points on image
plane. The corresponding image features are obtained
using the SURF method [10] and the epipolar constraint
is calculated using an estimated essential matrix. Moving
object information is detected in image plane and inte-
grated into the MOT process such that the robustness
of SLAM algorithm can be considerably improved, parti-
cularly in highly dynamic environments where sur-
roundings of robots are dominated by non-stationary
objects. The contributions in this article are twofold.
First, we develop an algorithm to solve the problems for
MOD in image plane, and then the algorithm is inte-
grated with the robot SLAM to improve the robustness
of state estimation and mapping processes. Second, the
improved SLAM system is implemented on a hand-held
monocular camera which can be utilized as the sensor
system for robot navigation in dynamic environments.
The SLAM problem with monocular vision will be
briefly introduced in Section 2. In Section 3, the pro-
posed algorithm of MOD is explained in detail. Some
examples to verify the performance of the data associa-
tion algorithm are described in Section 4. Section 5 is
the concluding remarks.
2. SLAM with a free-moving monocular vision
SLAM is a target tracking problem for the robot system
during navigating in the environment [12]. The targets
to be tracked include the state of the robot itself as well
as of the landmarks and moving objects in the environ-
ment. The state sequence of the SLAM system at time
step k can be expressed as
xk = f (xk−1,uk−1,wk−1) (1)
where xk is the state vector; uk is the input; wk is the
process noise. The objective of the tracking problem is
to recursively estimate the state xk of the target accord-
ing to the measurement zk at time step k,
zk = g (xk, vk) (2)
where vk is the measurement noise. A hand-held
monocular vision, as shown in Figure 1, is utilized in
this article as the only sensing device for the measure-
ment in SLAM system. We treat this hand-held vision
sensor as a free-moving robot system with unknown
inputs. The states of the system are estimated by solving
the recursive SLAM problem using the extended Kal-
man filter (EKF) [12]
xk|k−1 = f (xk−1|k−1,uk−1, 0) (3a)
Pk|k−1 = AkPk−1|k−1ATk +WkQk−1W
T
k (3b)
Kk = Pk|k−1HTk (HkPk|k−1H
T
k + VkRkV
T
k )
−1 (3c)
xk|k = xk|k−1 + Kk(zk − g(xk|k−1, 0)) (3d)
Pk|k = (I− KkHk)Pk|k−1 (3e)
where xk|k-1 and xk|k represent the predicted and esti-
mated state vectors, respectively; Kk is Kalman gain
matrix; P denotes the covariance matrix, respectively; Ak
and Wk are the Jacobian matrices of the state equation f
with respect to the state vector xk and the noise variable
wk, respectively; Hk and Vk are the Jacobian matrices of
the measurement g with respect to the state vector xk
and the noise variable vk, respectively.
2.1. Motion model
Two coordinate systems are set at the world frame {W}
and the camera frame {C}, as shown in Figure 2. The
state vector of the SLAM system with MOT in Equation
(1) is arranged as
x = [xC m1 m2 · · · mn O1 O2 · · · Ol]T (4)
xC is a 12 × 1 state vector of the camera including
the three-dimensional vectors of position r, rotational
angle j, linear velocity v, and angular velocity ω, all in
world frame; mi is the three-dimensional (3D) coordi-
nates of ith stationary landmark in world frame; Oj is
the state vector of jth moving object; n and l are the
number of the landmarks and of the moving objects,
respectively.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 2 of 22
at time step k is assumed to be among the possible r
modes
Mj ∈ M = {M1 , M2 , · · · , Mr}
Given r motion models, the object state ok in Equation
(7) is estimated. Instead of using the IMM estimator, a
single CV model is utilized in the paper. That is, the
moving objects are also presumed to move at a CV
motion model. Their coordinates in 3D space are
defined as
ojk =
[
pjk−1 + (vjk−1 + wjk−1)t
vjk−1 + wjk−1
]
(8)
where pjk and vjk are the vectors of the position and
linear velocity of jth moving object at time step k,
respectively.
2.2. Vision sensor model
The measurement vector of the monocular vision sys-
tem is expressed as
zk = [z1k z2k · · · zmk]T
m is the number of the observed image features in
current measurement. The perspective projection
method [14] is employed to model the transformation
from 3D space coordinate system to 2D image plane.
For one observed image feature, the measurement is
denoted as
zik =
[
Iix
Iiy
]
=
⎡
⎢⎢⎢⎣
u0 + fu
hCix
hCiz
+ αCfu
hCiy
hCiz
v0 + fv
hCiy
hCiz
⎤
⎥⎥⎥⎦ for i = 1, 2, . . . , m (9)
where fu and fv are the focal lengths of the camera
denoting the distance from the camera center to the
image plane in u- and v-axis, respectively; (u0, v0) is the
offset pixel vector of the pixel image plane; aC is the
camera skew coefficient; hCi = [h
C
ix h
C
iy h
C
iz ]
T is the ray
vector of ith image feature in camera frame. The 3D
coordinates of ith image feature or landmark in world
frame, as shown in Figure 2, is given as
mi = [Xi Yi Zi ]T = r + RWC h
C
i (10)
RWC is the rotational matrix from world frame to cam-
era frame, represented by using the elementary rotations
[15],
RWC =
⎡
⎣ cφycφz sφxsφycφz − cφxsφz cφxsφycφz + sφxsφzcφysφz sφxsφysφz + cφxcφz cφxsφysφz − sφxcφz
−sφy sφxcφy cφxcφy
⎤
⎦ (11)
where cj = cosj and sj = sinj; jx, jy and jz are the
corresponding rotational angles in world frame. We can
utilize Equatoin (10) to calculate the ray vector of an
image feature in camera frame. The coordinates of the
feature in image plane are obtained by substituting
Equations (10) and (11) into Equation (9) with aC = 0
Iix = u0+fu
cφycφz(Xi − rx) + cφysφz(Yi − ry) − sφy(Zi − rz)
(cφxsφycφz + sφxsφz)(Xi − rx) + (cφxsφysφz − sφxcφz)(Yi − ry) + cφxcφy(Zi − rz) (12a)
Iiy = v0+fv
(sφxsφycφz − cφxsφz)(Xi − rx) + (sφxsφysφz + cφxcφz)(Yi − ry) + sφxcφy(Zi − rz)
(cφxsφycφz + sφxsφz)(Xi − rx) + (cφxsφysφz − sφxcφz)(Yi − ry) + cφxcφy(Zi − rz) (12b)
Moreover, the elements of the Jacobian matrices Hk
and Vk are determined by taking the derivative of zi
with respect to the state xk and the measurement noise
vk. The Jacobian matrices are obtained for the purpose
of calculating the innovation covariance matrix in EKF
estimation process [16].
2.3. Feature initialization
Because of the lack of one-dimensional range informa-
tion in image, how to initialize features becomes an
important topic. Some researchers have successfully
solved this problem either in time-delayed method [16]
or un-delayed method [17]. The un-delayed method will
be utilized in this research. When an image feature is
selected, the spatial coordinates of the image feature are
calculated by employing the method of inverse depth
parameterization [17]. Assume that there are m image
features with 3D position vectors, yi, i = 1,...,m, which is
described by the 6D state vector
yˆi = [ rˆWix rˆ
W
iy rˆ
W
iz θˆ
W
i ψˆ
W
i ρˆi ]
T (13)
rˆW = [ rˆWix rˆ
W
iy rˆ
W
iz ]
T indicates the estimated state of the
camera when the feature was observed, as shown in Fig-
ure 2; ρˆi is the estimated image depth of the feature;
θˆWi and ψˆ
W
i are the longitude and latitude angles of
the spherical coordinate system which locates at the
camera center. To compute the longitude and latitude
angles, a normalized vector ηWi in the direction of the
ray vector is constructed by using the perspective pro-
ject method:
ηWi = R
W
C (φˆ
W)
[
Iix − u0
fu
Iiy − v0
fv
1
]T
(14)
Therefore, from Figure 2, the longitude and latitude
angles of the spherical coordinate system can be
obtained as
θˆWi = tan
−1
(
ηWiz
ηWix
)
(15a)
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 4 of 22
3. Moving object detection and tracking
In the flowchart of Figure 4, the function block of MOD
is designed based on the concept of the pixel coordinate
constraint of a static object in image plane. An object in
space is represented by the corresponding features in
two consecutive images. The pixel coordinate constraint
equation for these corresponding image features can be
expressed as
hCd
TEh′Cd = 0 (24)
where hCd and h′Cd are the homogenous normalized
image coordinates of the corresponding features
abstracted from two consecutive images, images 1 and
2, respectively. They are defined as
hCd = K
−1
C
⎡
⎣ IxIy
1
⎤
⎦ h′Cd = K−1C
⎡
⎣ I′xI′y
1
⎤
⎦ ; andKC =
⎡
⎣ fu αCfu u00 fv v0
0 0 1
⎤
⎦
KC is the matrix of camera intrinsic parameters which
can be obtained from Equation (9). Note that the image
coordinates are defined in camera frame. The essential
matrix E in Equation (24) is defined as [20]
E = [t]×R =
⎡
⎣ 0 −tz tytz 0 −tx
−ty tx 0
⎤
⎦R (25)
where R is the rotation matrix and t is the translation
vector of the camera frame with respect to world frame;
[t]× is the matrix representation of the cross product
with t. The rotation matrix and translation vector would
be determined using EKF estimator. Therefore, the
essential matrix E can be calculated accordingly.
Usually, the pixel coordinate constraint in Equation (24)
is utilized to estimate the state vector and according
essential matrix. Given a set of corresponding image
points, it is possible to estimate the state vector and the
essential matrix which optimally satisfy the pixel coordi-
nate constraint. The most straight-forward approach is
to set up a total least squares problem, commonly
known as the eight-point algorithm [21]. On the con-
trary in SLAM problem, the state vector and the essen-
tial matrix is obtained using the state estimator. We
could further utilize the estimated state vector and the
essential matrix to investigate whether a set of corre-
sponding image points satisfy the pixel coordinate
Figure 3 64-dimensional description vector for SURF.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 6 of 22
of the presence of noise. Define the distance D to repre-
sent the pixel deviation of the corresponding point from
the epipolar line in image plane, as shown in Figure 7,
D =| aI
′
x + bI′y + c√
a2 + b2
| (28)
D is utilized in this article to denote the pixel devia-
tion from the epipolar line which is induced by the
motion and measurement noise in state estimation pro-
cess. Depending on how the noise related to each con-
straint is measured, it is possible to design a threshold
value in Equation (28) which satisfies the epipolar con-
straint for a given set of corresponding image points.
For example, the image feature of a static object in the
first image is located at (Ix, Iy) = (50,70) and then the
camera moves 1 cm in zc-axis. If the corresponding
image feature (I′x, I
′
y) in the second image is constrained
within a deviation I′y is limited in a range, as shown in
Figure 7, as I′x varying from 1 to 320.
There are two situations that the pixel coordinate con-
straint in Equation (24) will result in trivial solutions.
First, if the camera is motionless between two consecu-
tive images, we can see from Equation (25) that the
essential matrix becomes a zero matrix. Therefore, the
object status could not be obtained by investigating the
pixel coordinate constraint. In this case, we assume the
camera being stationary in space and compute the pixel
distance in between the corresponding features in two
consecutive images to determine the object status. Sec-
ond, if the image feature of a static object in the first
image is located near the center of image plane (Ix, Iy) =
(160,120), the coefficients of the epipolar line obtained
from Equation (26) are zero. Therefore, any point in the
second image will satisfy the equation of the epipolar
line in Equation (27). This situation is simulated and the
result is depicted in Figure 8. In the simulation, the
pixel coordinate in the first image Ix varies from 0 to
320 and Iy is fixed at 120. The corresponding feature
(I′x, I
′
y) in the second image is limited within the pixel
coordinate constraint hCd
TEh′Cd < 10−3. We can see that
the range of the corresponding I′y is unlimited when (Ix,
Iy) is close to (160,120), as shown in Figure 8. In real
applications, those features located in a small region
near the center of the image plane need a special treat-
ment because the epipolar line constraint is not valid in
this situation.
4. Experimental results
In this section, the experimental works of the online
SLAM with a moving object detector are implemented
on a laptop computer running Microsoft Window XP.
The laptop computer is Asus U5F with Intel Core 2
Duo T5500 (1.66 GHz), Mobile Intel i945GM chipset
and 1 Gb DDR2. The free-moving monocular camera
utilized in this work is Logitech C120 CMOS web-cam
with 320 × 240-pixels resolution and USB 2.0 interface.
The camera is calibrated using the Matlab tool provided
by Bouquet [23]. The focal lengths are fu = 364.4 pixels
and fv = 357.4 pixels. The offset pixels are u0 = 156.0
pixels and v0 = 112.1 pixels, respectively. We carried out
three experiments including the SLAM task in a static
environment, SLAM with MOT, and people detection
and tracking.
Figure 5 Epiploar plane of two camera views.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 8 of 22
visited again and the covariance of the state vector is
reduced further.
The map size and the sampling frequency (Hz) at each
frame are depicted in Figure 18. The map size increases
to about 145 features at the first loop-closure and
remains at almost constant size (about 200 features at
the third loop-closure). That is, the SLAM can rely on
the map with old landmarks for localization when it
repeats visiting the same environment. The high sam-
pling frequency in Figure 18 is about 20 Hz when the
map size is small. When the map size increases, the low
sampling frequency keeps at about 5 Hz.
The deviations of the camera pose in xyz-axis are
plotted in Figure 19. The camera pose deviations
decrease suddenly at each loop-closure, because the old
landmarks are revisited and the camera pose and land-
mark locations are updated accordingly. We also can
see from Figure 19 that the lower-bound of the pose
deviation is further decreased after each loop-closure.
4.2. SLAM with MOT
The camera is carried to move around at one corner of
our laboratory in this example. Meanwhile, the SLAM is
implemented to map the environment and estimate the
camera pose, as well as to detect and track a moving
object. The estimated camera pose and landmarks are
illustrated in a 3D plot as shown in Figure 20. The
ellipses in the figure indicate the landmarks obtained
from the extracted image features. The rectangular box
represents the free-moving monocular camera and the
solid line depicts the trajectory of the camera. More
detail image frames of experimental results are illu-
strated in Figures 21, 22, 23, 24, 25, 26, 27, 28, 29 and
30. For each figure, the captured image is shown in the
left panel and the top-view plot is depicted in the right
panel of each figure. As shown in Figure 21 for the
52nd image frame, the SLAM system starts up and cap-
tures four image features with known 3D coordinates.
After the start-up, some stationary features with
unknown status are extracted and treated as new land-
marks for mapping, as shown in the 98th frame in Fig-
ure 22. These stationary landmarks are initialized using
inverse depth parameterization [17]. The system demon-
strates a stable implementation of SLAM as shown in
Figures 22, 23, 24, and 25. In Figure 26, soon after the
feature no. 30 is detected, it is discriminated from the
stationary features using the proposed MOD algorithm
and then treated as a moving object. This moving object
0 50 100 150 200 250 300
0
50
100
150
200
I’x
I’
y
 ↓
 ↑
Range of I’y for D < 3
Figure 7 Deviation from the epipolar line.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 10 of 22
Figure 9 Three-dimensional map and camera pose estimation.
Figure 10 31st frame: the SLAM system starts up with four known features, 0, 1, 2, 3.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 12 of 22
Figure 14 2265th frame: before the loop is closing.
Figure 15 2340th frame: the first time the camera reaches the loop-closure.
Figure 16 3355th frame: the second time the camera reaches the loop-closure.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 14 of 22
Figure 20 Three-dimensional map and camera pose estimation.
Figure 21 52nd frame: system start-up with four known features, 0, 1, 2, 3.
Figure 22 98th frame: three new features are initialized.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 16 of 22
Figure 26 615th frame: feature 30 on the object is deteced and initialized with large image depth uncertainty.
Figure 27 621st frame: the object stops moving and the image depth uncertainty of the feature is reduced.
Figure 28 634th frame: the image depth uncertainty of the tracked feature is further reduced and treated as a 3D point.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 18 of 22
Figure 32 365th frame: moving objects are detected.
Figure 33 386th frame: the detected moving objects are tracked.
Figure 34 392nd frame: the system performs SLAM and MOT.
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 20 of 22
22. QT Luong, OD Faugeras, The Fundamental Matrix: Theory, Algorithms, and
Stability Analysis. Int J Comput Vision. 17(1), 43–75 (1996). doi:10.1007/
BF00127818
23. JY Bouguet, Camera Calibration Toolbox for Matlab, http://www.vision.
caltech.edu/bouguetj/calib_doc/ (2011)
doi:10.1186/1687-6180-2012-29
Cite this article as: Wang et al.: Detection of moving objects in image
plane for robot navigation using monocular vision. EURASIP Journal on
Advances in Signal Processing 2012 2012:29.
Submit your manuscript to a 
journal and benefi t from:
7 Convenient online submission
7 Rigorous peer review
7 Immediate publication on acceptance
7 Open access: articles freely available online
7 High visibility within the fi eld
7 Retaining the copyright to your article
    Submit your next manuscript at 7 springeropen.com
Wang et al. EURASIP Journal on Advances in Signal Processing 2012, 2012:29
http://asp.eurasipjournals.com/content/2012/1/29
Page 22 of 22
11 
R E S E A R  
landmark 
Flg. 1. Binocular vision sensor system. 
The rest of this paper is organized as follows: Section 2 
describes the motion and sensor models of the SLAM using a 
free-moving camera sensor. The proposed algorithm of moving 
object detection is presented in Section 3. In Section 4, the exper- 
imental works are carried out on a real system to verify the 
performance of the proposed algorithm. The concluding remarks 
are addressed in the last section. 
2. SLAM WITH FREE-MOVING CAMERAS 
SLAM is a target tracking problem for the robot system during 
navigating in the environment.' The targets to be tracked include 
the state of the robot itself as well as of the landmarks in the map. 
If the environment is dynamic, the targets will include the moving 
objects in the environment. The state sequence of the SLAM 
system at time step k can be expressed as 
where xi is the state vector; u, is the input; w, is the process 
noise. The objective of the tracking problem is to recursively 
estimate the state x, of the target according to the measurement 
z, at time step k, 
z ,  = g ( x , ,  v,)  ( 2 )  
vk is the measurement noise. The states of the system are 
estimated by solving the recursive SLAM problem using the 
extended Kalman filter (EKF),' 
where x ~ ~ ~ - ~  and xkln represent the predicted and estimated state 
vectors, respectively; K,  is Kalman gain matrix; P denotes the 
covariance matrix; A, and W, are the Jacobian matrices of the 
state equation f with respect to the state vector xk and the noise 
variable w,, respectively; Hk and V, are the Jacobian matrices 
of the measurement g with respect to the state vector x, and the 
noise variable v , ,  respectively. 
2.1. Motion Model 
A hand-held binocular vision is utilized in this paper as the only 
sensing device for the measurement in SLAM system. The cam- 
era can be carried by an unknown person, robot, or other moving 
body. We treat this hand-held camera as a free-moving robot 
n u v .  3CI.  Lett. 8, 229-234, 15 Apr[[ 3@.;? 1 
system with unknown inputs. Two coordinate systenl~ -_ I I 
the world frame (W} and the camera frame {C). rerpcctiuely, , 
shown in Figure 1. The origin of the camera frame is located at 
the center of left-camera. The state vector of the SLAM system 
in ELI. (1) is arranged as: 
x,  is a 12 x 1 state vcctor of the camera including the three- 
dimensional (3D)  vectors of position r, rotational angle 4, linear 
velocity v, and angular velocity w ,  all in world frame; mi is the 
3D coordinates of ith stationary landmark in world frame; 0.  
is the state vector of jth moving object; n and I are the num: 
her of the landmarks and of the moving objects, respectively. 
The motion of the hand-held camera is presumed to be at con- 
stant velocity ( C V )  and constant angular velocity. The accelera- 
tion is caused by an impulse noise from the external force. The 
state of ith stationary landmark at time step k is represented by 
3D coordinates in space, 
In the motion model of moving object tracking (MOT), the 
targets to be tracked include the state and motion mode of the 
moving object in the environment. The state of the MOT system 
at time step k can be expressed as 
ojk = [ o ~ ~ s ~ ~ ] ~ ,  for j = 1 , 2 ,  . . . , I ( 6 )  
where ojk and sjk are the state and motion mode of jth moving 
object, respectively. The moving objects are also presumed to 
move at a CV motion model in this work. That is, the motion 
mode si, is assumed to be known initially. The coordinates of 
the moving objects in 3D space are defined as 
Wherepj, = [Xjk qk Z j k l T  is the 3D coordinates of jth mov- 
ing object; vik and wjk are the velocity and the process noise of 
jth moving object, respectively. 
2.2. Sensor Model 
The measurement vector of the binocular vision sensor is 
expressed as 
Z ,  = [ Z ' , Z ~ ~  ' . . z , ~ ] ~  (8) 
m is the number of the observed image features in the current 
measurement. The perspective projection methodI4 is employed 
to model the transformation from 3D space coordinate system to 
2D image plane. For ith observed image feature, the measure- 
ment is denoted as 
where i = 1 ,2 ,  . . . , m; j denotes left- or right-camera, , j  = L or 
R; I, and I, are the pixel coordinates of image features; f, and 
f,, are the focal lengths of the camera denoting the distance from 
the camera center to the image plane in u- and v-axis, re'spec- 
tively; (uo,vo) is the offset pixel vector of the pixel image plane; 
. . 
h: = [ hix hyy h!z]T is the ray vector of ith image feature in 
R E S E A R C H  A R T I C L E  
Fig. 4. The triangular plane is formed by three reference points in 3D 
space. 
(a) Choose three points a, b and c from the feature set C,,. The 
' gravity center of the triangular plane formed by points a,  17 and 
c is determined as 
where (hi,, hi,, h,,) is the 3D coordinate of the features in the 
camera frame, i =a, h and c. The spatial distances between the 
gravity center and the points a, b and c can be calculated and 
expressed as z, aand E; 
(b) Find the corresponding points a, b and c in the map 
database. The distances between these three points are given as 
- - 
ab, bc and E which had been calculated in previous sampling 
time. A planar coordinate system can be established in the tri- 
angular plane formed by points a, b and c. Let the origin of the 
coordinate system is located at point a and the x-axis points to b. 
Therefore, the location of point h is determined as (x,, y,) = 
(ab, 0) and the coordinate ( x c ,  y,) of point c is obtained as 
Nav. scl. Lett. 8, 22--~,4, 13 ~ p t i i  2cq2 
I 
The gravity center (dx, d,,) of the triangalar plane formed by 
, 
points a, b and c is calculated as 
The distances between the gravity center and the point 
-- 
c can be calculated and expressed as do, db and z. 
Repeat the Steps (a) and (b) to choose the first t h e  points 
which satisfy the following condition 
where e is an error threshold. Therefore, the points o, 17 and sat- 
isfy Eq. (17) are chosen to be the reference points. One example 
for choosing the reference points is depicted in Figures 3 and 4, 
In this case, the free-nloving camera begins the SLAM task from 
the origin of the world frame. The rectangular box represents 
the binocular camera and the solid curve indicates the estimated 
trajectory of the camera. The red (dark) dots denote the map 
landmarks. As depicted in Figure 3, point no. 309, 238, and 274 
(square marks) are chosen as reference points. The triangular 
plane for~lled by these three reference points is depicted in 3D 
space, as shown in Figure 4. 
After three reference points are chosen, the spatial distances 
between the reference points and each feature in set C,, are fur- 
ther investigated to detemune whether this feature is static or 
moving. Note that the proposed MOD algorithm is also indepen- 
dent of the state estimator. Therefore, the kidnapping problem in 
SLAM can be automatically resolved in dynamic environments. 
4. EXPERIMENTAL DETAILS 
In this section, the experimental works of the online SLAM with 
a moving object detector is carried out on a free-moving binoc- 
ular vision system. The cameras utilized in this work are Log- 
itech C12bCMOS web-cam with 320 x 240 pixels resolution and 
USB 2.0. The notebook computer is Asus USF with Intel Core 2 
Duo T5500 (1.66 GHz), Mobile Intel i94SGM chipset and IGb 
DDR2. First example demonstrates the kidnapping problem when 
the camera sensor is occluded during the SLAM process. The 
state estimator will be recovered until the camera is relieved from 
Flg. 5. 172nd frame: before the camera is occluded. 
Fig. 10. 1781st frame: The 2nd time of loop-closure. 
frame 
Fig. 11. Map size and sampling frequency. 
t 1st loop-closure c 2 n d  loop-clos 
frame 
Fig. 12. The deviation of the camera pose estimation. 
more image features are extracted and treated as stationary land- 
marks, as indicated in square marks in Figure 8 for the 1030th 
frame. In Figure 9 for the 1075th frame, the camera comes to 
the place it had visited before and the loop is closing. Some old 
landmarks (with number less than 100) are captured again, and 
the covariance of the state vector is reduced gradually. The sec- 
ond time of loop-closure is depicted in Figure 10 for the 1781st 
frame. In this frame, old landmarks are visited again and the 
covariance is reduced further. 
The map size and the sampling frequency (Hz) at each frame 
are depicted in Figure 11. The map size increases to about 
180 features at the first loop-closure and remains at almost con- 
stant size (about 200 features at the third loop-closure). That is, 
Adv. Sci. Lett. 8, Z Z L L J ~ ,  15 April 2R33 
the SLAM relies on the map with old landmarks for localiza- 
tion when it repeats visiting the same environment. high 
sampling frequency in Figure 11 is about 18 Hz when the map 
size is small. When the map size increases, the sampling fie- 
quency keeps as high as 7 Hz. 
The deviations of the camera pose in xyz-axis are plotted in 
Figure 12. The pose deviations increase suddenly at each loop- 
closure, because the old landmarks are measurcd as at slightly 
different locations. However, the pose deviations are reduced dra- 
nlatically short aftcr the loop-closure when the locations of old 
landmarks are updated. We also can see from Figurc 12 that the 
lower-bound of the pose deviation is further decreased after each 
loop-closure. 
5. CONCLUSIONS 
In this research, we developed an algorithm for detection and 
tracking of moving objects to improve the robustness of robot 
vislial SLAM system. SURFS are also utilized to provide a robust 
detection of image features and a stable description of the fea- 
tures. Experimental works have been carried out on a binocular 
vision system. The results showed that the binocular SLAM sys- 
tem with the proposed algorithm has the capability to support 
a robot system simultaneously navigating and tracking moving 
objects in dynamic environments. 
Acknowledgments: This work was supported in part by the 
National Science Council in Taiwan under grant no. NSC100- 
2221-E-032-008 to Y. T. Wang. The authors deeply appreciated 
the helpful comments and suggestions of the reviewers, which 
have improved the presenlation. 
References and Notes 
1. R. Smith, M. Self, and P. Cheeseman. Aulonomous Robot Vehicles. 
edited by I.J. Cox and G.T. Wilfong. Springer-Verlag, New York (1990). 
pp. 167-193. 
2. N. Karisso'n. E. D. Bernardo, J. Ostrowski, L. Goncalves. P. Pirjanian, and 
M. E. Munich, Proceedings of IEEE lnternational Conference on Robotics and 
Automation, Barcelona. Spain (2005). p. 24. 
3. R. Sim, P Elinas, and J. J. Little, lnt. J. Comput. Vision 74, 303 (2007). 
4. A. J. Davison. I. D. Reid. N. D. Molton. and 0. Stasse, IEEE T: Patfern Anal. 
29, 1052 (2007). 
5. M. Montemerlo and S. Thrun. FastSLAM. Springer-Verlag. Berlin (2007). 
6. L. M. Paz, P. Pinies, J. D. Tardos, and J. Neira, IEEE T: Robot. 24, 946 (2008). 
7. C. C. Wang, C. Thorpe. S. Thrun, M. Hebert, and H. Durrant-Whyte, Int. 
J. Robot Res. 26, 889 (2007). 
8. S. Wangsiripitak and D. W. Murray, Proceedings of IEEE lnternational Con- 
ference on Robotics and Automation. Kobe. Japan (2009), p. 375. 
9. Y. T. Wang, Y. C. Feng, and D. Y. Hung, Proceedings of IEEE lnternational 
Instrvmentation and Measurement Technology Conference. HangZhou, China 
(2011), p. 1078. 
10. C. Bibby and I. Reid, Proceedings of Robotics: Science and Systems 111. 
Atlanta. Georgia (2007). 
11. H. Zhao, M. Chiba. R. Shibasaki, X. Shao, J. Cui, and H. Zha. Proceedings of 
the IEEE lnternational Conference on Robotics and Automation. Pasadena, 
California (2008). 
12. H. Bay. T. Tuytelaars, and L. Van Gool, Proceedings of the Ninth European 
Conference on Computer Vision. Graz, Austria (2006). 
13. Y. T. Wang, D. Y Hung, and C. H. Sun, J. Inf. Sci. Eng. 27, 1823 (2011). 
14. S. Hutchinson, G. D. Hager, and F! I. Corke, IEEE 'I: Robot. Automat. 12, 651 
(1996). 
15. T. Lindeberg. Int. J. Comput. Vision 30. 79 (1998). 
16. D. G. Lowe, Int. J. Comput. Vision 60. 91 (2004). 
Received: 1 June 201 1 .  Accepted: 31 August 201 1.  
 In this paper, we propose a novel algorithm for map building based on modified SURF feature 
detection and recognition procedures. A binocular vision is utilized as the sensing device to 
implement the visual SLAM tasks. Meanwhile, two experiments on a real system are carried out in 
static SLAM scenes to validate the proposed algorithm. 
Speeded-Up Robust Features 
Lindeberg proposed the concept of automatic scale selection to overcome the disadvantage of 
Harris corner [4]. He established a Hessian matrix whose elements are the convolution of the image 
and Laplacian of Gaussians (LoG). Then the feature points can be detected by investigating the 
determinant of the Hessian matrix. Lindeberg’s method has the advantages of stability and high 
repeatability. Lowe [5] replaced the LoG by Difference of Gaussians (DoG). Bay et al. [7] utilized the 
box filter to approximate LoG and the determinant of the Hessian matrix. The box filter was further 
combined with the method of integral image [8] to reduce the image processing time. They coined 
this method of feature detection as SURF [7]. 
In order to improve the robustness of feature representation and reduce the computational cost, we 
modify the SURF detection method in two aspects [11]: first, we utilize the box filters only in 2 
low-level octaves for feature detection to reduce the computation cost. Second, we set up a threshold 
value (Dthreshold) as the lowest limit for the determinant of Hessian matrix in order to control the 
number of detected features in an image. The value of Dthreshold can be obtained by an online 
procedure. Our purpose is that, even in the dull background or environment with fewer features, there 
will be enough number of features detected. 
After the features are detected from the image, the description vector is utilized to represent the 
characteristics of features. For the orientation of a feature, Bay et al. [7] computed the Haar wavelet 
responses in the x and y direction of the feature area. The orientation of a feature is defined at the 
direction with the largest sum of the Haar wavelet responses. Furthermore, a high-dimensional 
description vector is utilized to describe the uniqueness of the feature. First, choose a square area with 
the center located in the feature point and its direction along the direction of the feature. Second, 
divide the square area into 4x4 sub-areas. There is a 4D descriptor vector for each sub-area, expressed 
as v=(Σdx, Σdy, Σ|dx|, Σ|dy|), where Σdx, Σdy, Σ|dx|, and Σ|dy| are the sums of the Harr wavelet 
responses and their absolute values in x and y direction which is defined by the coordinate along the 
orientation of the feature. This will result in total dimensions of 4x4x4=64 for the description vector. 
The Nearest-Neighbor (NN) search method [12] is the most popular method for matching 
high-dimensional description vectors. The Euclidean distance d between the arbitrary point p in P set 
and the query point q is usually defined by the norm l2. The distance between two descriptors would 
be treated as the basis of feature matching. Meanwhile, we use dmatch as the threshold of the Euclidean 
distance and as a judgment whether the matching between two descriptors is successful. When the 
Euclidean distance d is less than dmatch, the matching is successful, 
 
match1
2
2 dqpqpd
n
i ii    )( . (1) 
 
One example is implemented to validate the modified SURF algorithm for image feature detection 
and matching. An image with 320×240 pixels is captured by a CMOS webcam. The modified SURF 
algorithm is applied to detect image features on the image. We use box filters only in 2 low-level 
octaves. The detecting results using different Dthreshold values are depicted in Fig. 1. The figure shows 
that the number of detected features is varied from 837 to 41 when the value of Dthreshold is changed 
from 0 to 10000. However, the computation time is almost the same for different value of Dthreshold. 
 
2 ICETI2012
 bookshelf (1.5m×2m floor dim.) in our laboratory. The resultant map and the camera pose estimation 
are plotted in Fig. 3. In this figure, the estimated states of the camera and landmarks are illustrated in 
a 3D map plot. The camera is carried to move from first image frame and circles around the bookshelf 
three times. The SLAM system also starts up from first image frame and captures image features with 
unknown positions. These features will be initialized and stored as landmarks in the map. The SLAM 
system builds the environment map and estimates the camera pose concurrently, when the camera is 
carried to circle around the bookshelf. The 32nd, 645th, and 1484th frame of color images in Fig. 3 
belong to first camera circulation around the bookshelf. The results show that the SLAM task is 
successfully implemented. In each color image, the (blue) circular marks indicate the landmarks 
extracted from the captured image with an unknown image depth, while the (red) square marks 
represent the landmarks with a known and stable image depth. The estimated states of the camera and 
landmarks are illustrated in 3D map in the center of Fig. 3. The red (dark) ellipses represent the 
uncertainty of the landmarks which have known image depths and the green (light) ellipses denote the 
uncertainty of the landmarks which have unknown image depths. Meanwhile, the trajectory of the 
estimated camera pose is plotted as solid lines in 3D map. The 2298th frame in Fig. 3 is the end of the 
first camera circulation. At this location, the camera comes to the place it had visited before and the 
trajectory loop is closing. Some old landmarks, for example landmark No. 41 in this image, are 
captured again and the covariance of the state vector is reduced gradually. In the color images of 
2739th and 3039th frames, which belong to second camera circulation, the landmark numbers under 
450 are successfully refound by the SLAM system. The 3331st and 4209th in Fig. 3 are the end of the 
second and third camera circulation, respectively. The second and third times of loop-closure are 
expected at these two locations. In these frames, old landmarks are visited again and the covariance of 
the state vector is reduced further. 
 
 
Fig. 2. Flowchart for the robot SLAM 
Conclusions 
In this research, we developed an algorithm for building a persistent map to improve the 
robustness of robot visual SLAM system. The SURF method is utilized to provide a robust detection 
of image features and a stable description of the features. Using the modified SURF algorithm, the 
experimental work has been carried out on a real system. The results showed that the binocular 
SLAM system with the proposed algorithm has the capability to support robot systems 
simultaneously navigating and mapping in the environment. 
Acknowledgments: This paper was partially supported by the National Science Council in 
Taiwan under grant no. NSC100-2221- E- 032-008 and NSC101-2632-E-032-001-MY3. 
4 ICETI2012
 1 
國科會補助專題研究計畫出席國際學術會議心得報告 
                              日期： 101 年 10 月 25 日 
一、參加會議經過 
2012 第 12 屆智慧型自主系統國際研討會(12th International Conference on Intelligent 
Autonomous System, IAS-12)於 2012 年 6 月 26 日至 29 日，在韓國濟州舉行。研討會是由
Intelligent Autonomous Systems (IAS) Society、IEEE Robotics and Automation Society 與
Institute of Control, Robotics and System (ICROS, Korea)等單位共同主辦。其最主要的目的是
透過舉辦研討會、展示技術發展成果、以及探討未來發展方向等方式，讓工業界與學術界
研究人員利用會議平台相互交流，以推動智慧型自主系統的科學與技術在各領域的發展。
四天會議共有 5 個全席議程(plenary sessions)、24 個特別議程(special sessions)、16 個正規
議程(Regular sessions)、與 1 個海報議程(Poster session)進行。共有來自多個國家的研究人
員參與發表 184 篇論文與 18 篇海報。 
全席議程有五位來自日本、美國、德國、韓國的貴賓講員，演講議題相當具有前瞻性，
演講的內容水準很高。筆者報告題目為：「使用單眼視覺偵測移動物體(Moving Object 
Detection Using Monocular Vision)」，此為國科會補助的計畫(機器人於室內動態環境之視覺
式輔助巡航，NSC100-2221-E-032-008)。此論文針對機器人同時定位、建圖、與移動物體
追蹤問題及其在室內動態環境中之應用進行研究。研究的議題包括移動物體偵測、影像特
徵偵測與描述、以及在未知的動態環境中巡航等。此論文完成即時視覺式同時定位、建圖、
與移動物體追蹤演算法，可以應用在智慧型行動機器人系統上，使機器人在未知的動態環
境中具備同時定位與建圖的能力，以順利執行巡航任務。此論文提出動態環境中移動物體
偵測的演算法，也改良影像特徵偵測與描述方法，以及整合應用於手持攝影機或行動機器
計畫編
號 NSC100-2221-E-032-008 
計畫名
稱 
機器人於室內動態環境之視覺式輔助巡航 
出國人
員姓名 王銀添 
服務機構
及職稱 
淡江大學機械與機電工程學系
副教授 
會議時
間 
101 年 6 月 26 日至 
101 年 6 月 29 日 會議地點 
Jeju Island, Korea(韓國濟州) 
會議名
稱 
(中文) 第 12 屆智慧型自主系統國際研討會 
(英文) 12th International Conference on Intelligent Autonomous System (IAS-12) 
發表題
目 
(中文) 使用單眼視覺偵測移動物體 
(英文) Moving Object Detection Using Monocular Vision 
Moving Object Detection Using Monocular 
Vision 
Yin-Tien Wang, Kuo-Wei Chen, and Ming-Jang Chiou 
Department of Mechanical and Electro-Mechanical Engineering, Tamkang University 
{ytwang@mail; 699370218@s99; 698371464@s98}.tku.edu.tw 
Abstract. This paper presents an algorithm for moving object detection (MOD) in 
robot visual simultaneous localization and mapping (SLAM). The algorithm is de-
signed based on the defining epipolar constraint for the corresponding feature 
points on image plane. An essential matrix obtained using the state estimator is 
utilized to represent the epipolar constraint. Meanwhile, the method of speeded-up 
robust feature (SURF) is employed in the algorithm to provide a robust detection 
for image features as well as a better description of landmarks and of moving ob-
jects in visual SLAM system. Experiment is carried out on a hand-held monocular 
camera to validate the performances of the proposed algorithm. The results show 
that the integration of MOD and SURF is efficient for robot navigating in dynamic 
environments. 
Keyword: Moving object detection; simultaneous localization and mapping; 
speeded-up robust features; monocular vision. 
1 Introduction 
In recent years, more and more researchers solve the simultaneous localization and 
mapping (SLAM) as well as the moving object Tracking (MOT) problems concur-
rently. Wang et al. [1] developed a consistency-based moving object detector and 
provided a framework to solve the SLAMMOT problems. Bibby and Reid [2] 
proposed a method that combines sliding window optimization and least-squares 
together with expectation maximization to do reversible model selection and data 
association that allows dynamic objects to be included directly into the SLAM es-
timation. Zhao et al. [3] used GPS data and control inputs to achieve global con-
sistency in dynamic environments. There are many advantages to cope with 
SLAM and MOT problems simultaneously: for example, mobile robots might 
navigate in a dynamic environment crowded with moving objects. In this case the 
SLAM could be corrupted with the inclusion of moving entities if the information 
of moving objects is not taken account. Furthermore, the robustness of robot local-
3 
tection in image plane, and then the algorithm is integrated with the robot SLAM 
to improve the robustness of state estimation and mapping processes. Second, the 
improved SLAM system is implemented on a hand-held monocular camera which 
can be utilized as the sensor system for robot navigation in dynamic environments. 
2 SLAM with a Free-moving Monocular Vision 
SLAM is a target tracking problem for the robot system during navigating in the 
environment [12]. The targets to be tracked include the state of the robot itself as 
well as of the landmarks and moving objects in the environment. The state se-
quence of the SLAM system at time step k can be expressed as 
 ( )111 ,, −−−= kkkk wf uxx  (1) 
where xk is the state vector; uk is the input; wk is the process noise. The objective of 
the tracking problem is to recursively estimate the state xk of the target according to 
the measurement zk at time step k, 
 ( )kkk vg ,xz =  (2) 
where vk is the measurement noise. A hand-held monocular vision, as shown in Fig. 
1, is utilized in this paper as the only sensing device for the measurement in SLAM 
system. We treat this hand-held vision sensor as a free-moving robot system with 
unknown inputs. The states of the system are estimated by solving the recursive 
SLAM problem using the extended Kalman filter (EKF) [12], 
 )0 , ,( 1111 −−−− = kkkk|k f uxx |  (3a) 
 Tkkk
T
kkkkkk WQWAPAP 11|11| −−−− +=  (3b) 
 11|1| )(
−−− += TkkkTkkkkTkkkk VRVHPHHPK  (3c) 
 0)) ,(( 11 −− −+= k|kkkk|kk|k g xzxx K  (3d) 
 ( ) 1−−= k|kkkk|k PHKIP  (3e) 
where xk|k-1 and xk|k represent the predicted and estimated state vectors, respectively; 
Kk is Kalman gain matrix; P denotes the covariance matrix, respectively; Ak and Wk 
are the Jacobian matrices of the state equation f with respect to the state vector xk 
and the noise variable wk, respectively; Hk and Vk are the Jacobian matrices of the 
measurement g with respect to the state vector xk and the noise variable vk, respec-
tively. 
5 
where p(ok|sk,z1:k) is state inference; z1:k is the set of measurements for time t=1 to k; 
p(sk|z1:k) is the mode learning. The EKF-based interacting multiple model (IMM) 
estimator [13] can be utilized to estimate the motion mode of a moving object. The 
state is computed at time step k under each possible current model using r filters, 
with each filter using a different combination of the previous model-conditioned es-
timates. Given r motion models, the object state ok in Eqn. (7) is estimated. Instead 
of using the IMM estimator, a single CV model is utilized in the paper. That is, the 
moving objects are also presumed to move at a CV motion model. Their coordi-
nates in 3D space are defined as 
 ⎥⎥⎦
⎤
⎢⎢⎣
⎡
+
Δ++=
−−
−−−
11
111
jkjk
jkjkjk
jk
t
w
wp
o
v
)(v
 (8) 
where pjk and vjk are the vectors of the position and linear velocity of jth moving 
object at time step k, respectively. 
2.2 Vision Sensor Model 
The measurement vector of the monocular vision system is expressed as 
 Tmkkkk ][ 21 zzzz L=  (9) 
m is the number of the observed image features in current measurement. The per-
spective projection method [14] is employed to model the transformation from 3D 
space coordinate system to 2D image plane. The 3D coordinates of ith image fea-
ture or landmark in world frame, as shown in Fig. 1, is given as 
 Ci
W
C
T
iiii ZYX hRr +== ][m  (10) 
where i indicates ith observed image feature, i=1,2, …,m; WCR  is the rotational ma-
trix from world frame to camera frame; TCiz
C
iy
C
ix
C
i hhh ][=h  is the ray vector of ith 
image feature in camera frame. 
2.3 Speeded Up Robust Features (SURF) 
The basic concept of a scale-invariant method is to detect image features by in-
vestigating the determinant of Hessian matrix H in scale space [15]. In order to 
7 
where R is the rotation matrix and t is the translation vector of the camera frame 
with respect to world frame; [t]× is the matrix representation of the cross product 
with t. The rotation matrix and translation vector would be determined using EKF 
estimator. Therefore, the essential matrix E can be calculated accordingly. Usually, 
the pixel coordinate constraint in Eqn. (11) is utilized to estimate the state vector 
and according essential matrix. Given a set of corresponding image points, it is 
possible to estimate the state vector and the essential matrix which optimally satisfy 
the pixel coordinate constraint. The most straight-forward approach is to set up a 
total least squares problem, commonly known as the eight-point algorithm [19]. On 
the contrary in SLAM problem, the state vector and the essential matrix is obtained 
using the state estimator. We could further utilize the estimated state vector and the 
essential matrix to investigate whether a set of corresponding image points satisfy 
the pixel coordinate constraint in Eqn. (11). First, define the known quantities in 
Eqn. (11) as a constant vector 
 [ ] [ ] 111 −−= CTCTyx IIcba EKK )(  (14) 
Eqn. (14) is further rearranged as an equation of the epipolar line constraint in im-
age plane for two corresponding points [20], 
 0=+′+′ cIbIa yx  (15) 
Eqn. (15) indicates that the pixel coordinate of the corresponding feature in second 
image will be constrained on the epipolar line. Motion and measurement noise is 
involved in the state estimation process. If the measurement of image points is 
subject to noise, which is the common case in any practical situation, it is not 
possible to find a corresponding feature point which satisfies the epipolar constraint 
exactly. That is, the corresponding feature point might not be constrained on the 
epipolar line because of the presence of noise. Define a distance D to represent the 
pixel deviation of the corresponding point from the epipolar line in image plane 
[21]. The pixel deviation D is induced by the motion and measurement noise in 
state estimation process. Depending on how the noise related to each constraint is 
measured, it is possible to design a threshold value which satisfies the epipolar 
constraint for a given set of corresponding image points. 
4 Experimental Results 
In this section, the experimental works of the online SLAM with a moving ob-
ject detector are implemented on a laptop computer running Microsoft Window XP. 
The laptop computer is Asus U5F with Intel Core 2 Duo T5500 (1.66GHz), Mobile 
Intel i945GM chipset and 1Gb DDR2. The free-moving monocular camera utilized 
in this work is Logitech C120 CMOS web-cam with 320×240-pixels resolution and 
USB 2.0 interface. The camera is calibrated using the Matlab tool provided by 
9 
Fig. 3. 365th frame: moving objects are detected 
 
Fig. 4. 386th frame: the detected moving objects are tracked 
 
Fig. 5. 410th frame: moving object tracking 
5 Conclusions 
In this research, we developed an algorithm for detection and tracking of mov-
ing objects to improve the robustness of robot visual SLAM system. SURFs are al-
so utilized to provide a robust detection of image features and a stable description 
of the features. Three experimental works have been carried out on a monocular vi-
sion system including SLAM in a static environment, SLAM with moving object 
tracking, and people detection and tracking. The results showed that the monocular 
SLAM system with the proposed algorithm has the capability to support robot sys-
tems simultaneously navigating and tracking moving objects in dynamic environ-
ments. 
Acknowledgments. This paper was partially supported by the National Science Council in Tai-
wan under grant no. NSC100-2221-E-032-008  to Y.T. Wang. 
國科會補助計畫衍生研發成果推廣資料表
日期:2012/10/24
國科會補助計畫
計畫名稱: 機器人於室內動態環境之視覺式輔助巡航
計畫主持人: 王銀添
計畫編號: 100-2221-E-032-008- 學門領域: 智慧型機器人
無研發成果推廣資料
其他成果 
(無法以量化表達之成
果如辦理學術活動、獲
得獎項、重要國際合
作、研究成果國際影響
力及其他協助產業技
術發展之具體效益事
項等，請以文字敘述填
列。) 
無 
 成果項目 量化 名稱或內容性質簡述 
測驗工具(含質性與量性) 0  
課程/模組 0  
電腦及網路系統或工具 0  
教材 0  
舉辦之活動/競賽 0  
研討會/工作坊 0  
電子報、網站 0  
科 
教 
處 
計 
畫 
加 
填 
項 
目 計畫成果推廣之參與（閱聽）人數 0  
 
