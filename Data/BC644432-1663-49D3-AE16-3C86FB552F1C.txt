 1
 
 
 
?????????????? 
Evolution Algorithms for Solving Non-Linear 
Multi-dimensional Knapsack Problem 
 
???? 
Abstract 
In this research, we consider the nonlinear knapsack problem (NKP) which is a generalization of 
linear case of knapsack problems. It is of considerable interest as indicated by surveys of 
operations research in industry over the last decades. Moreover, several important models in 
operations research are special cases of NKP, such as reliability redundancy optimization 
problems, nonlinear resource allocation problem, etc. Some approaches to the exact solution of 
NKP have been suggested. However, as the dimension increases, the computing time needed for 
these methods increases rapidly. The theory of complexity also indicate that knapsack problem 
and its special cases belong to a class of hard problems – so called NP-hard problems. Thus, a 
number of papers have addressed heuristic approaches to NKP. Traditionally, the optimization 
methods generate a deterministic sequence of computation and the methods are applied to a 
single point in the solution space. The disadvantage is that only few portions of the solution space 
are searched and the quality of solution cannot be improved by iterative execution of the methods. 
In the last three decades, it is found that probabilistic approaches which perform population-based 
search is more promising. Because the population of solutions undergoes a simulated evolution, 
the quality of solutions can then be improved iteratively by implementing some cooperation 
mechanism. In this research, we aim to investigate the population-based probabilistic search 
method such as genetic algorithm for NKP. 
Key Word: nonlinear, integer programming, knapsack problems, reliability redundancy 
optimization, resource allocation, probabilistic search method, genetic algorithm. 
 
???? 
????????????????,???????????????,???????
????????????????????,??????????????????
?,??????????????????,???????????????????,
?????????????????,?????????????????????,
???????????????????? NP-hard,???????????????
????,????????????????????????,??????????,
???????????,???????????????????,????????
??????????????,?????????????,????????,??
???????,?????????????,?????????,?????????
????????????????????,???????????,???????
??? 
???: ???,????,????, ????,????,??????, ????? 
 3
[Morin 1976]have been suggested. However, as the dimension increases, the computing time 
needed for these methods increases rapidly. The theory of complexity [Chern 1990] [Magaz 
1984a] [Chern 1992] also indicate that knapsack problem and its special case belong to a class of 
hard problems – so called NP-hard problems. This class contains many well-known and 
much-studied problems that are widely recognized as being difficult and have been confounding 
the experts for half century [Garey 1979]. It is commonly conjectured that no efficient 
(polynomial) algorithms exists in this class. However, problems of high dimensionality often 
arise in the mathematical modeling of applied problems, for which it is sufficient to obtain a good 
approximation to the optimum. In view of this fact, computational efficient methods need to be 
sought for the approximate solution of problems of high dimensionality. Traditionally, the 
following heuristic approaches are commonly used for the knapsack problems: (1) Local search 
algorithms and their generalization: In this approach, a neighborhood is defined for the current 
solution and a ranking procedure is defined for estimating the quality of neighborhood solutions. 
In the simple local search method, at each iteration, the algorithm searches for a better solution in 
the neighborhood. If such a solution exists, it becomes the new current solution and the procedure 
goes on. Otherwise, a local optimal solution is obtained and stop. In general, the larger the 
neighborhood, the better the solutions there are. However, it will take more computing time for 
searching the neighborhood. The traditional Local search algorithms include [Ohtag 2000] 
[Korne 1989] [Ohtag 1995, 2000] [Koche 1974] [Parso 1976] [Peter 1976] [Peter 1967] [Dabae 
1979] [Krush 1969] [Weing 1966] [Loulo 1979] [Senju 1968] [Toyod 1975]. These methods are 
highly dependent on the initial solution. In order to escape from a local optimal solution and 
obtain better solutions, some multi-start strategies are also proposed [Echol 1968] [Pursg 1980] 
[Roth 1970][Angel 2005] [Brays 2004] (2) heuristic approach by the relaxation of constraints 
[Hille 1969] [Haul 1998] [Riha 1997]. The heuristic approaches by ranking procedures are 
usually called “greedy heuristic” [Kohli 1992, 1995] [Loulo 1979] [Ohtag 1995, 2000]. It is 
probably the most widely used heuristic for solving discrete optimization problems. This is due to 
the fact it is extremely simple to grogram and has a very low computing time. Roughly speaking, 
as the name indicates, this approach consists of changing the current solution in a locally best 
way to obtain a new solution. A general formulation is given as follows: (a) For each j, assign 
some efficiency score Ej to variable xj. Restrict attention to variables that have not previously 
been given specific values or to variables whose range have not been significantly restricted on a 
recent iteration. (b) Choose one or more variables with highest scores. (c) If all variables have 
been constrained to specific values, stop. Otherwise, go to (1). It also can be regarded as a 
steepest ascent scheme since it goes through the whole set then pick the best one in each iteration. 
In spite of the simplicity of this framework, a variety of specific heuristic and exact algorithms, 
old and new are encompassed within it. However, it has propensity to prematurely converge to a 
local optimum. One way of avoiding this is to allow the exploration of a wider range of solution 
space by introducing randomness in the search procedure. Thus, the modern heuristic methods 
which include ant algorithm, genetic algorithm and simulate annealing are designed to escape 
from the local optimum traps. Although they still do not guarantee an optimum solution, they 
have been proved to work exceptionally well in practice and have many advantages including 
 5
           
1
subject to ,    1, 2, ... , ,
n
ij j i
j
a x b i m
=
≤ =∑                      
0 ,    integer,  1, 2, ... , ,j j jx u x j n≤ ≤ =                
 
where all cj, aij and bi are nonnegative values. Without loss of generality, we assume that 
 
(i) ui is +∞ or a positive integer, j = 1, 2, . . . , n. 
(ii) ,   1,2, ... , ,   1,2, ... , .ij ia b i m j n≤ = =  (In order to guarantee that each item can be packed) 
(iii) 
1
,    1, 2, ... , .
n
ij j i
j
a u b i m
=
> =∑  (To avoid trivial constraints.) 
 
In order to obtain high quality approximation solution for LMKP, we instead solve the linear 
programming relaxation of LMKP as follows. 
(Relaxed LMKP)  
1
maximize    ,
n
j j
j
c x
=
∑                                          
                      
1
subject to ,    1, 2, ... , ,
n
ij j i
j
a x b i m
=
≤ =∑                         
0 ,    1, 2, ... , ,j jx u j n≤ ≤ =                           
Let ( )jx
∗ ∗=x  be the optimal solution of relaxed LMKP. We may then solve the following 
reduced 0-1 knapsack problem.  
(P1)  
1
maximize    
n
j j j j
j
c x x x∗ ∗
=
  −   ∑                                  
                      
1
subject to ,    1, 2, ... , ,
n
ij j j j i
j
a x x x b i m∗ ∗
=
  − ≤ =  ∑                
0,  1,    1, 2, ... , ,jx j n= =                       
where k is the greatest integer such that k ≤ k and k is the smallest integer such that k ≥ k. 
We may also consider the following alternate reduced problems. 
 
(P2)  
1
maximize    1
n
j j j j
j
c x x x∗ ∗
=
  − −  ∑                                
                     
1
subject to 1 ,    1, 2, ... , ,
n
ij j j j i
j
a x x x b i m∗ ∗
=
  − − ≤ =  ∑              
0,  1,    1, 2, ... , ,jx j n= =                       
 
(P3)  
1
maximize    0.5
n
j j j j
j
c x x x∗ ∗
=
  − +  ∑                               
                    
1
subject to 0.5 ,    1, 2, ... , ,
n
ij j j j i
j
a x x x b i m∗ ∗
=
  − + ≤ =  ∑              
0,  1,    1, 2, ... , ,jx j n= =                     
 
(P4)  
1
maximize    0.5
n
j j j j
j
c x x x∗ ∗
=
  − −  ∑                              
                     
1
subject to 0.5 ,    1, 2, ... , ,
n
ij j j j i
j
a x x x b i m∗ ∗
=
  − − ≤ =  ∑               
 7
GAs was first proposed by Holland [Holla 1975] and its usual form is given by Goldberg 
[Goldb 1989]. It initializes with a set of individuals called a population. Each individual in the 
population is called a chromosome, standing for a possible solution to the given problem. The 
quality of each chromosome called fitness is evaluated by a fitness function which is usually 
defined based on the objective function of the given problem. The process of creating new 
populations from old ones is called a generation. To form a new population, it uses three 
operators: crossover, mutation and selection. First, crossover operator selects two chromosomes 
to exchange information between them. Then, mutation operator changes some genes of some 
chromosomes in order to introduce new information. After applying these two disruptive 
operations, new chromosomes, called offspring, are produced and should be evaluated again to 
assign proper fitness. A certain portion of offspring are generated through crossover-mutation 
process. Finally, selection operator copies some chromosomes from parents and offspring based 
on their fitness to form a new population. The crossover-mutation-evaluation-selection cycle is 
repeated until the user-specified termination criterion is reached. 
In order to identify the effective and ineffective items, we adopt the optimal dual solution of 
LP relaxation of the 0-1 MKP as the surrogate coefficients (si). Suppose xj has surrogate ratio 
j
SRe which is defined by equation (1) and without loss of generality, we assume that 
1 2  ... nSR SR SRe e e≥ ≥ ≥ . By the theory of linear program, we have the following properties: (i) the 
items with 1jSRe =  are mainly the basic variables in the optimal solution of the LP-relaxed MKP. 
(ii) the items with 1jSRe ≤  are the nonbasic variables at lower bound in the optimal solution of 
the LP-relaxed MKP. (iii) the items with 1jSRe ≥  are the nonbasic variables at upper bound in the 
optimal solution of the LP-relaxed MKP. So we will divide the items (variables) of a given 0-1 
MKP into three parts: The first part (denoted by P1) includes the items with larger surrogate ratios 
( 1jS Re >  , the second part (P2) is composed of the items with 1jSRe ≅  and the third part (P3) is 
made up of the items with smaller surrogate ratios ( 1jSRe < ). The basic idea is to find two 
positions (CP1 and CP2) to divide {1, 2, ... , n} into three parts such that P1 = { j | j < CP1}, P2 = 
{ j | CP1 ≤ j ≤ CP2} and P3 = { j | j > CP2} . We will let xj =1, j∈ P1 and xj = 0, j∈ P3 before 
executing the proposed genetic algorithm. A procedure for identifying CP1 and CP2 is given as 
follows. Let 
min{ | 1,   1,2, ... , }jmin SRC j e j n= = =  and max{ | 1,   1,2, ..., }.jmax SRC j e j n= = =  
For a given parameter W, 0 < W < 1, we let 
1 2,  .min maxCP C W n CP C W n= − × = + ×  
Computational results show that W = 0.15 will give good performance for the problems with 
1,000 ≤ n ≤ 10,000 and 500 ≤ m ≤ 5,000. For the larger problem, we suggest to use smaller w. 
 
There exist three schemes for GAs to deal with a constrained optimization problem, 
including penalizing scheme, encoding scheme and repairing scheme. The repairing-based GAs 
also use a repairing operator additionally after crossover and mutation operations. The 
penalizing-based GAs for the 0-1 MKP frequently suffer from the feasibility problem. It may 
converge in a completely infeasible population, especially when the ratio of the feasible region to 
the search space is low. Existing encoding-based GAs for the 0-1 MKP have been shown to have 
weaker locality, heritability and heuristic bias, which are essentials for good performance of 
evolutionary search [Raidl 2005]. In addition, the recently work based on these two strategies for 
the 0-1 MKP [Gottl 2000] [Raidl 1999] [Uyar 2005] and some of Raidl’s [Raidl 2005] still can 
not be more effective than the repairing-based GAs proposed by Chu and Beasley [Chu 1998]. In 
this research, we adopt the repairing scheme and the procedure of proposed GA is shown in 
Procedure 1. 
A Hybrid Genetic Algorithm 
Procedure 1: Proposed genetic algorithm 
begin 
 9
experiments to define a efficiency measure. 
 
Table 5.1 Outline of the three genetic algorithms 
Algorithm CGA [Chu 1998] RGA [Raidl 2005] HGA 
Preprocessing None None Variable reduction procedure 
Initialization Randomization 
Based on the optimal 
solution of LP-relaxed 
MKP 
Based on the surrogate 
information and the optimal 
solution of LP-relaxed 
Crossover Uniform Uniform Fitness-guided 
Mutation Uniform Uniform Heuristic 
Repairing Based on surrogate information 
Based on surrogate 
information 
Hybridize surrogate information 
with greedy heuristics 
Selection (µ+λ) selection (µ+λ) selection Hybridize roulette wheel selection with (µ+λ) selection 
Remark: HGA is the proposed GA. 
We code HGA by using C language and develop it on the integrated development 
environment called Dev C++. The LP-relaxed MKP is solved by using the mathematical 
programming solver called LINGO (Version 8.0). For Chu and Raidl’s algorithms, since the 
original codes are not available to us, we code them ourselves based on the descriptions 
mentioned in the corresponding papers. All of the computational study are ran on the PC with 
Pentium 4 2.8G Hz CPU. 
To illustrate the computational results, we define the following notations and performance 
indexes. 
(1)   α: the tightness ratio, which is used to set bi as in Equation (2). 
1
,  1, 2, ... , . ni ijjb a i mα == =∑                      (2) 
(2)   β: the density ratio, which is the percentage of nonzero values in the constraint matrix [aij].  
(3)   z*: the best objective value found. 
(4)   Avg z*: average of z*. 
(5)   lpgap = 100*( LPz − * z )/ LPz (in %) where LPz  is the optimal objective value of the 
LP-relaxed MKP. 
(6)   Avg lpgap: average of lpgap (in %). 
(7)   rgap = 100*( *maxz − * z )/ *maxz (in %) where *maxz  is the highest objective value among the 
three algorithms. 
(8)   Avg rgap: average of rgap (in %). 
(9)   *t : the time used to reach z* (in seconds). 
(10)  Avg t*: average of t* (in secs). 
(11)  G : total generations ran. For each algorithm, on average, each generation examines 50 
offspring (solutions). 
(12)  T : the total execution time. 
(13)  Avg T : average of T. 
(14)  #best : the frequency of obtaining *maxz (including ties). 
We first conduct the computational study based on two popular 0-1 MKP problem sets. The 
largest problem of them is up to 2500 items and 100 constraints. HGA will be evaluated on these 
publicly accessible problem sets and compared with the results from literature. 
 
5. 1  Medium-scale problems from OR-Library 
The first medium-scale problem set consists of 30 large 0-1 MKPs in OR-Library [Beasl 
1990] (available at http://mscmga.ms.ic.ac.uk). Each of them contains 500 items and 30 
 11
Method CPLEX B&B-O HGA CPLEX B&B-O HGA CPLEX B&B-O HGA
Avg z* 115497  115520 115560 216151 216180 216213 302366  302373 302401
Remark 1: Each set has 10 problems. 
 2: The version of CPLEX is 6.5.2. The results are from [Osori 2002]. In all of the         
30 problems, it terminated because of reaching 250 MB treesize memory. 
 3: B&B-O is the branch and bound algorithm developed by [Osori 2002]. The         
results are from [Osori 2002]. In all of the 30 problems, it terminated because of 
reaching 10800 seconds solution time (CPU: Pentium III 450 MHz). 
 
5. 2  Medium-scale problems from Hearin center for enterprise science 
The second medium-scale problem set contains 5 largest 0-1 MKP from Hearin center for 
enterprise science (available at http://hces.bus.olemiss.edu/tools.html). Each of them has different 
(n, m) and the largest one contains 2500 items and 100 constraints. They were generated by 
Glover [Glove 1996]. For each problem, we derive its reduced version based on the proposed 
variable reduction procedure (W = 0.15). The resulting problem set is denoted by HCES-R and 
the original one is denoted by HCES. 
 
Table 5.5 Experimental results of the three algorithms for no redundant reduced problems 
Problem set HCES-R 
Method CGA RGA HGA 
Avg z* 49671 49718 49725 
Avg lpgap(%) 0.1691 0.0871 0.0007 
Avg rgap(%) 0.0972 0.0152 0 
Avg t*(secs) 909 591 4550 
Avg G 1367292 1526951 578219 
Avg ?(%) 4.8771 4.9556 4.9700 
#best 0 0 5 
Remark 1: The running time of each algorithm for each problem is 5400 seconds. 
 2: CGA, RGA solve HCES; HGA solves HCES-R. 
 3: Each set has 5 problems and the average terms are the average of the results from 5 
problems. 
 
Table 5.6 Experimental results of the three algorithms for no redundant reduced problems 
Problem set HCES-R 
Method CGA RGA HGA 
Avg z* 49671 49718 49725 
Avg lpgap(%) 0.1691 0.0871 0.0007 
Avg rgap(%) 0.0972 0.0152 0 
Avg t*(secs) 909 591 4550 
Avg G 1367292 1526951 578219 
Avg ?(%) 4.8771 4.9556 4.9700 
#best 0 0 5 
Remark 1: The running time of each algorithm for each problem is 5400 seconds. 
 2: Each set has 5 problems and the average terms are the average of the results from 5 
problems. 
 
 13
Table 5.9 Experimental results of the three algorithms 
Problem set α = 0.01 α = 0.1 α = 0.25 
Method CGA RGA HGA CGA RGA HGA CGA RGA HGA 
Avg z* 66930 67890  69053 715120 715976 716987 715120 715976 716987 
Avg lpgap(%) 8.706 7.397 5.811 0.799 0.680 0.540 0.799 0.680 0.540
Avg rgap(%) 3.074 1.684 0.000 0.260 0.141 0.000 0.260 0.141 0.000
Avg t*(secs) 8535  6098  4721 8405 7786 7391 8405  7786  7391 
Avg G 3103  3060  25535 2880 2869 12450 2880  2869  12450 
#best 0  0  10  0  0  10  0  0  10  
Remark 1: For each problem in these problem sets, n = 7500, m = 500 and β = 1. 
 2: CGA and RGA solve non-reduced problems. HGA solves reduced problems. 
 
Table 5.10 Experimental results of the three algorithms 
Problem set β = 0.25 β = 0.5 β = 0.75 
Method CGA RGA HGA CGA RGA HGA CGA RGA HGA 
Avg z* 680854 681799 683760 835946 836660 837974 993060 993614 994522 
Avg lpgap(%) 1.299 1.162 0.878 0.760 0.675 0.519 0.500 0.445 0.354
Avg rgap(%) 0.425 0.287 0.000 0.242 0.157 0.000 0.147 0.091 0.000
Avg t*(secs) 8395  7402  3500 6823 7317 5640 7342  6613  6536 
Avg G 4525  4438  16571 5031 6193 18024 5008  4947  19081 
#best 0  0  10  0  0  10  0  0  10  
Remark 1: For each problem in these problem sets, n = 5000, m = 500, α = 0.25. 
 2: CGA and RGA solve non-reduced problems. HGA solves reduced problems. 
 
6. Computational Results for LMKP 
The genetic algorithm Procedure 1 is then applied to solve (P1) ,(P2), (P3), and (P4). 
 
Table 6.1 Experimental results with W =0.15 for automatic generated general LP problems. 
Problem Set n = 5000 
Reduced Problem (P1) (P2) (P3) (P4) 
Avg z* 11152091 11152059 11054267.3 11152029 
Avg lpgap(%) 0.0123 0.0126 0.8895 0.0128 
Avg t*(secs) 4562 5267 1 2852 
Avg G 189535 33932 2131000 264099 
#best 9 4 0 0 
Remark 1: For each problem in these problem sets, m = 500, α = 2~10 and β = 1. 
k     2: The running time of each algorithm for each problem is 9000 seconds. 
Remark 3: Each set has 10 problems and the average terms are the average of the results 
4: (P1), (P2), (P3) and (P4) solves reduced problems with no redundant constraint. 
 
Table 6.2 Experimental results with W =0.15 for automatic generated general LP problems. 
Problem Set n = 7500 
Reduced Problem (P1) (P2) (P3) (P4) 
 15
Avg z* 10528492 10528461 10508738 10528324 
Avg lpgap(%) 0.0173 0.0176 0.2052 0.0189 
Avg t*(secs) 2957 4748 2 4949 
Avg G 93488 71912 1709200 107506 
#best 8 1 0 1 
Remark 1: For each problem in these problem sets, n = 5000, α = 2~10 and β = 1. 
Remark 2: The running time of each algorithm for each problem is 9000 seconds. 
Remark 3: Each set has 10 problems and the average terms are the average of the results 
4: (P1), (P2), (P3) and (P4) solves reduced problems with no redundant constraint. 
 
Table 6.6 Experimental results W =0.15 for automatic generated general LP problems. 
Problem Set β =0.25 
Reduced Problem (P1) (P2) (P3) (P4) 
Avg z* 10148288 10148320 8619459 10148065 
Avg lpgap(%) 0.0396 0.0393 15 0.0418 
Avg t*(secs) 3758 3463 5200 3636 
Avg G 29000 48574 35301 40347 
#best 6 8 0 0 
Remark 1: For each problem in these problem sets, m=500, n = 5000, α = 2~10. 
Remark 2: The running time of each algorithm for each problem is 9000 seconds. 
Remark 3: Each set has 10 problems and the average terms are the average of the results 
4: (P1), (P2), (P3) and (P4) solves reduced problems with no redundant constraint. 
 
Table 6.7 Experimental results with W =0.15 for automatic generated general LP problems. 
Prob set β =0.5 
Reduced Problem (P1) (P2) (P3) (P4) 
Avg z* 10118245 10118218 9262403 10118082 
Avg lpgap(%) 0.0251 0.0253 8.4927 0.0267 
Avg t*(secs) 4334 3976 2674 2326 
Avg G 106102 32104 126541 140857 
#best 7 5 0 0 
Remark 1: For each problem in these problem sets, m=500, n = 5000, α = 2~10. 
Remark 2: The running time of each algorithm for each problem is 9000 seconds. 
Remark 3: Each set has 10 problems and the average terms are the average of the results 
4: (P1), (P2), (P3) and (P4) solves reduced problems with no redundant constraint. 
 
Table 6.8 Experimental results with W =0.15 for automatic generated general LP problems. 
Prob set β =0.75 
Reduced Problem (P1) (P2) (P3) (P4) 
Avg z* 10546807 10546748 10143211 10546655 
 17
[Coit 1996b] Coit, D.; Smith, A., Solving the redundancy allocation problem using a combined neural 
network/genetic algorithm approach, Computers & Operations Research 23 (6): 515-526 JUN 1996. 
[Corne 1999] Corne, David; Dorigo, Marco; and Fred Glover, New Ideas in Optimization, The McGraw-Hill 
Companies. 
[Dabae 1979] Dabaev, Dzh. A., Mamedov, K. Sh., and Mekh\tiev, M. G., “Methods of constructing suboptimal 
solutions of multi-dimensional knapsack problems,” U.S.S.R. Comput. Maths Math Phys., Vol.18, 82-91. 
[Dasgu 1999] Dasgupta, D. (ed.), Artificial Immune Systems and Their Applications, Springer-Verlag. 
[Dorig 1992] Dorigo, M., Optimization, Learning and Natural Algorithms, (in Italian). PhD thesis, 
Dipartimento di Elettronica, Politecnico di Milano, IT. 
[Dorig 1996] Dorigo, M., V. Maniezzo, and A. Colorni, “The ant system: Optimizatoin by a colony of 
cooperating agents,” IEEE Transactions on Systems, Man, and Cybernetics - Part B, 26(1): 29-41. 
[Dorig 2004] Dorigo, Marco; Thomas Stützle, Ant colony optimization, Cambridge, Mass., MIT Press, c2004. 
[Drexe 1988] Drexel, A., “A simulated annealing approach to the multiconstraint zero-one knapsack Problem,” 
Computing, 40:1-8. 
[Dyer 1998] Dyer, M. E.; Walker, J., Dominance in multi-dimensional multiple-choice knapsack problems, 
Asia-Pacific Journal of Operational Research,Volume 15, Number 2. 
[Echol 1968] Echolos, R. E.; Cooper, L, Solution of integer linear programming problems with direct search, 
Journal of the Association of Computational Machinery, 15, 75-84. 
[Elhed 2005] Elhedhli, S., Exact solution of a class of nonlinear knapsack problems, Operations Research Letters 33 
(6): 615-624 nov 2005. 
[Elhed 2005] Elhedhli, S., Exact solution of a class of nonlinear knapsack problems, Operations Research Letters 33 
(6): 615-624 nov 2005. 
[Feder 1983] Federgruen, A., Solution techniques for some allocation problems, Mathematical Programming 
25 : 13 1983. 
[Frevi 1986] Freville, Arnaud; Plateau, G., Heuristic and reduction methods for multiple constraints 0-1 linear 
programming problem, European Journal Of Operational Research (24), Pages 206-215. 
[Frevi 2004] Freville, Arnaud, The multidimensional 0/1 knapsack problem: An overview, Pages 1-21, EJOR 
Volume 155, Issue 1, Pages 1-265. 
[Gallo 1980] Gallo, G., Quadratic Knapsack-Problems, Mathematical Programming study 12 : 132 1980. 
[Garey 1979] M. R. Garey and D. J. Johnson, Computers and Intractability, Freeman and Co.. 
[Garfi 1972] Garfinkel, Robert S.; Nemhauser, George L., “Integer Programming,” John Wiley & Sons, New 
York. 
[Gen 1997] Gen, M.; Cheng, R., Genetic Algorithms and Engineering Design, Wiley, New York. 
[Gen 1998] Gen, M.; Cheng, R.; Sasaki, M.; Jin, Y., Multiple-choice knapsack problem using genetic 
algorithms, In Parsaei, H; Gen. M.; Tsujimura, Y.; Wong J. (editors), Advances in Engineering design and 
Automation Research H. Integrated Technology Systems, Maui, HI. 
[Glove 1996] Glover, F.; Kochenberger, G. A., Critical event tabu search for multidimensional knapsack 
problems, In I. H. Osman and J. P. Kelly, editors, Meta-Heuristics: Theory & Applications, Kluwer. 
[Hamme 1997] Hammer, P., Efficient methods for solving quadratic 0-1 knapsack problems, INFOR 35 : 170 
1997. 
[Hanaf 1998] Hanafi, Said ; Freville, Arnaud, An efficient tabu search approach for the 0-1 multidimensional 
knapsack problem, European Journal Of Operational Research (106)2-3, pp. 659-675 
[Hanaf 1998] Hanafi, Said; Freville, Arnaud; Abdellaoui, A. El, Comparison of heuristics for the 0-1 
multidimensional knapsack problems, In I. H. Osman and J. P. Kelly, editors, Meta-Heuristics: Theory & 
Applications, Kluwer. 
[Haul 1998] Haul, C.; Voss, S., Using surrogate constraints in genetic algorithm for solving multidimensional 
knapsack problems, In D. L. Woodruff, Advances in Computational and Stochastic Optimization, Logic 
Programming and Heuristic Search, Kluwer. 
[Hausm 1978] Hausmann, D.(ed.), Integer Programming and Related Areas (1976-1978), A Classified 
Bibliography, Lecture Notes in Economic and Mathematical Systems, 160. 
[Hochb 1994] Hochbaum, D., Lower and upper-bounds for the allocation problem and other nonlinear optimization 
problems, Mathematics of Operations Research 19 (2): 390-409 may 1994 
[Hochb 1995] Hochbaum, D., A nonlinear knapsack-problem, Operations Research Letters 17: 103 1995. 
 19
[Miche 1996] Michelon, P., Lagrangean methods for the 0-1 quadratic knapsack problem, European Journal of 
Operational Research 92 : 326 1996  
[Melma 2000] Melman, A.; Rabinowitz, G., An efficient method for a class of continuous nonlinear knapsack 
problems, SIAM Review 42 (3): 440-448 sep 2000.  
[Misra 1991a] Misra, K., Search procedure to solve integer programming-problems arising in reliability-design of a 
system, International Journal of Systems Science 22 (11): 2153-2169 NOV 1991. 
[Misra 1991b] Misra, K., An algorithm to solve integer programming-problems - an efficient tool for 
reliability-design, Microelectronics and Reliability 31 (2-3): 285-294 1991  
[More 1991] More, J., On the solution of concave knapsack-problems, Mathematical Programming, 49 : 397 1991. 
[Morin 1976] Morin, T.; Marsten, R., Algorithm for nonlinear knapsack problems, Management Science 22 (10): 
1147-1158 1976. 
[Nakag 1999] Nakagawa, Y; Iwasaki, A., Modular approach for solving nonlinear knapsack problems, IEICE 
Transactions on Fundamentals of Electronics Communications and Computer Sciences E82a (9): 1860-1864 sep 
1999. 
[Nemha 1989] Nemhauser,Georgr; Wolsey, L., Integer and Combinatorial Optimization. Wiley-Interscience, New 
York 1989. 
[Ng 2001] Ng, K.; Sancho, N., A hybrid 'dynamic programming/depth-first search' algorithm, with an application to 
redundancy allocation, IIE Transactions 33 (12): 1047-1058 DEC 2001  
[Ohtag 1995] Ohtagaki, H. ; Nakagawa, Y. ; Iwasaki, A., et al., Smart greedy procedure for solving a nonlinear 
knapsack class of reliability optimization problems, Mathematical and Computer Modelling 22 (10-12): 261-272 
nov-dec 1995. 
[Ohtag 2000] Ohtagaki, H.; Iwasaki, A; Nakagawa, Y., et al., Smart greedy procedure for solving a multidimensional 
nonlinear knapsack class of reliability optimization problems, Mathematical and Computer Modelling 31 (10-12): 
283-288 may-jun 2000  
[Parda 1991] Pardalos, P., Algorithms for the solution of quadratic knapsack-problems, Linear Algebra and Its 
Applications, 152 : 69 1991. 
[Parso 1976] Parsons, J. A., Approximating 0-1 programming problems,” Journal of System Management, 
Vol.27, No. 4, 42-45. 
[Pater 1974] Patersen, C. C., “A capital budgeting heuristic algorithm using exchange operations,” AIIE 
Transactions, 143-150. 
[Pater 1967] Patersen, C. C., “Computational experience with variants of the Balas algorithm applied to the 
selection of R&D projects,” Management Science Vol. 13, No. 9. 
[Pirku 1987] Pirkul, H. “A heuristic solution procedure for the multiconstraint zero-one knapsack problem,” 
Naval Research Logistics Quarterly, Vol.34-2 161-172. 
[Raidl 1999] Raidl, G. R., “Weight-codings in a genetic algorithm for the multi-constraint knapsack problem,” 
Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress on , Volume: 1 , 1999 -603. 
[Ramir 2004] Ramirez-Marquez, J.; Coit, D.; Konak, A., Redundancy allocation for series-parallel systems using a 
max-min approach, IIE Transactions 36 (9): 891-898 SEP 2004. 
[Riha 1997] Riha, W.; Walker, J., An efficient algorithm for the lagrangean dual of nonlinear knapsack problems with 
additional nested constraints, Journal of Computational and Applied Mathematics 78 (1): 9-18 feb 3 1997. 
[Rinno 1993] Rinnooy Kan, A. H. G.; Stougie, L.; vercellis, C., A class of generalized greedy algorithms for the 
multi-knapsack problems, Discrete Applied Mathematics, 42, 279-290. 
[Robin 1992] Robinson, A., On the continuous quadratic knapsack-problem, Mathematical Programming 55 : 
99 1992. 
[Schri 1986] Schrijver, Alexander, Theory of linear and integer programming, Chichester ; New York : Wiley. 
[Senju 1968] Senju, S. and Toyoda, Y., “An Approach to Linear Programming with 0-1 Variables,” 
Management Science, Vol. 15, No, 4 pp. B196-B207. 
[Sierk 2002] Sierksma, Gerard, Linear and integer programming : theory and practice, New York : Marcel 
Dekker. 
[Singh 1994] Singh, H; Misra N., On redundancy allocations in systems, Journal of Applied Probability 31 (4): 
1004-1014 dec 1994. 
[Smith 1995] Smith, J.; Chikhale, N., Buffer allocation for a class of nonlinear stochastic knapsack-problems, Annals 
of Operations Research 58: 323-360 1995. 
[Sun 2005] Sun, X.; Wang, F.; Li, D., Exact algorithm for concave knapsack problems: linear underestimation and 
