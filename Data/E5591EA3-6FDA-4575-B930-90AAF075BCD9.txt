行政院國家科學委員會補助專題研究計畫成果報告 
計畫名稱: 大規模分散式網路數位內容複製偵測技術之研究 
計畫編號: NSC 96-2218-E-027-003 
執行期限: 96/08/01-97/07/31  
主持人: 國立台北科技大學資訊工程系 王正豪 助理教授  
e-mail: jhwang@csie.ntut.edu.tw  
一、計畫摘要 
數位多媒體內容如: 文件, 圖像, 影音等, 很容易以極低成本複製並散佈且品質不會
失真, 降低內容擁有者釋出內容的意願. 因此數位版權管理(digital rights management)相關
研究便試圖運用各種資訊安全及浮水印等技術來保護數位內容. 本計畫提出了一個大規模
分散式網路數位內容複製偵測之架構, 希望經由分散在網路上的內容過濾架構, 來有效偵
測已經散佈出去的數位內容, 以提供另一層保護.  
首先, 我們以近似複製比對的方式, 來決定兩個數位檔案是否可能互為複製修改版本. 
我們基於內容比對, 利用最常見的數位內容類型其不同特徵, 發展出有效的近似內容複製
偵測方法. 其次, 我們透過網際網路上最常見的 World-Wide Web 平台,來探討如何利用分散
式特性與互助查詢方式, 設計出快速且有效的 Peer-to-Peer 數位內容複製偵測架構. 初步研
究驗證了此架構的可行性, 相關研究成果並已經發表國際研討會論文三篇, 另有一篇研討
會論文及一篇期刊論文審查中, 其中 ICADL 2007 論文更榮獲最佳論文獎.  
 
關鍵詞: 數位內容保護, 數位版權管理, 複製偵測, 近似複製, 對等式計算(同儕式計算) 
 
Abstract 
Digital contents are subject to copying and manipulation with very low costs. This 
impedes the willingness of content owner for releasing their valuable contents. Research on 
digital rights management (DRM) usually employs information security and watermarking 
techniques for protecting digital contents. In this project, we proposed a large-scale distributed 
architecture for digital content copy detection on the Internet. The goal is to provide a second line 
of defense for digital content protection. 
First, we determined if two files are possible replicas by near-duplicate detection. Since 
we focus on content-based similarity, different methods have been designed according to their 
characteristics in different content types. Second, we designed a peer-to-peer (P2P) architecture 
and tested our method in the most common platform of World-Wide Web (WWW). We utilized 
the distributed characteristics and collaborative processing and the preliminary results showed the 
feasibility and potential of an efficient and effective architecture for digital content detection. We 
have published three international conference papers, and submitted one conference paper and 
one journal paper for review, all on important international conferences and journals. The ICADL 
2007 paper won the Best Paper Award.  
 
Keywords: digital content protection, digital rights management, near-duplicate, copy detection, 
peer-to-peer computing 
 
 
圖一 大規模網路內容近似複製偵測架構 
如圖一所示, 我們先收集數位內容並抽取特徵, 索引在 Object Feature Database. 其次
當典藏單位想要了解受保護內容被複製使用的狀況時, 可以向系統提出內容查詢, 經由系
統比對傳回其可能的複製情形. 不同於傳統網頁查詢, 此查詢是以內容而非關鍵詞來查詢. 
由於網路資料量龐大, 其中收集, 索引, 查詢, 比對等工作都是以分散式方法進行處理的.  
1. 大規模網路內容過濾技術 (Large-scale Internet content filtering) 
本技術應用 distributed IR[5], 經由分散在網路上彼此合作的 filter 收集並整合使用者
感興趣的資訊. 其中有幾項重要議題: 資料收集(data collecting), 資料放置(data placement), 
查詢轉送(query forwarding), 結果整合(result merging). 考慮資料來自 Web, 我們應用分散
式網頁收集方法, 利用 P2P 的優點, 將收集, 儲存, 分析網頁的工作, 分散到不同 peer 上. 
資料收集的問題, 便成為如何有效切分 Web 並分給不同 peer 來負責收集. 資料放置的問題, 
由於多媒體資料量大, 其傳輸會對網路和 peer 造成極大的負擔, 因此收集到的內容直接儲
存在負責收集的peer上, 不做資料的移動. 當使用者查詢時, 才將查詢轉送給各peer並整合
查詢結果傳回給使用者.  
(1) 有效率的 Peer-to-Peer 架構 
我們採用 hybrid P2P 架構[18], 在每個 P2P domain 中, 由一個 super-peer 負責管理
domain 裡的各個 general peer. General peer 主要功能如下: (1) Crawling 模組負責收集指定的
網頁內容. 我們採用 Nutch[12]為基礎加以修改網頁擷取等部分, 成為分散式 crawler. (2) 
Coordinating 模組負責處理系統中 P2P 溝通, 包括: URL 轉送, 查詢轉送等訊息. (3) 
Application 模組負責處理查詢, 並使系統很容易能加入其他跨典藏單位的應用, 如: 圖像/
影片複製偵測等. (4) Management 模組負責管理各模組運作情形, 如: access logs, status 
reports 等, 並提供管理者界面以維護整個系統運作. 
 
圖二 Super peer 及 general peer 功能模組與溝通 
如圖二所示, 此架構中 super peer 的工作十分重要, 包括: 所負責 domain 的協調與管
Internet Content Filtering 
Collaborative Copy Detectionquery 
collect 
Object Feature DB
compare
Potential object copies
index
Protected 
content  
Internet
 
三、計畫結果與討論 
本計畫執行期間, 我們發表了國際研討會論文三篇[6, 7, 8], 另有一篇研討會論文[15]
及一篇期刊論文審查中[14], 其中 ICADL 2007論文[6]更榮獲最佳論文獎(Best Paper Award).  
1. 大規模網路內容過濾技術 (Large-scale Internet content filtering) 
主要提出了一個P2P 分散式架構來過濾網路內容, 並在URL分配, 網頁收集, 以及整
體 P2P 架構的效率, 都有初步成果[16], 也驗證了此架構的可行性. 未來若考慮實際應用所
須應付的大量內容及眾多使用者, 須要再進一步探討其擴充性 (scalability). 
2. 基於內容的近似複製偵測技術 (Content-based near-duplicate copy detection)  
主要在文件[7], 圖像[15], 及影音[8]等內容複製偵測技術, 都有初步成果, 詳述如下. 
首先在文件近似複製偵測方面, 如表一所示, 比起常見的 shingles 方法, 除了前處理所須時
間相近外, 不論在索引或是搜尋時間上, 我們提出的方法都有相當優異的表現, 儲存空間
的需求也因此大為降低. 且準確率(precision)及查全率(recall)並未明顯下降, 如表二所示. 
表一. 文件近似複製偵測效率評估 
演算法 前處理時間 (s) 轉換時間 (s) 索引大小 (MB) 索引時間 (s) 搜尋時間 (s)
Shingles 17.226 241 1,185 32,350 103 
Proposed 16.41 5.536 100 474 0.094 
表二. 文件近似複製偵測有效性評估 
演算法 Top-180 Recall Top-180 Precision Top-180 F1 
Shingles 0.9722 0.9722 0.9722 
Proposed 0.9722 0.9722 0.9722 
其次我們實際整合了前述 P2P 架構的協同式圖像近似複製偵測, 亦有不錯成果. 如表
三初步實驗顯示, 比起常見的 ordinal measure, 我們的方法得到較佳的準確率和查全率; 但
由於圖像特徵計算較複雜, 效率則略差, 如表四所示. 不過利用5個 peers協同處理, 可將每
次查詢的平均回應時間(average response time)降為 1 秒多, 尚在使用者可接受的範圍內.  
表三. 協同式圖像近似複製偵測有效性評估 
演算法 Recall Precision F-Measure 
Ordinal Measure 0.584 0.892 0.706 
Proposed 0.945 0.987 0.966 
表四. 協同式圖像近似複製偵測效率 - 查詢的平均回應時間 
演算法 Standalone Peer 5 Collaborative Peers 
Ordinal Measure 2.8 s 1.6 s 
Proposed 3.2 s 1.9 s 
未來在不明顯影響查詢有效性的前提下, 若要進一步提高查詢效率, 一方面要簡化圖
像特徵的計算; 另一方面, 分散式查詢處理(distributed query processing)也是值得探討的議
題, 也就是在資訊有限的情形下, 如何將查詢只轉送到部分必要的 peer 而非全部.  
此外我們的方法也應用在新聞文件組織整理[6], 以及網頁事件相關內容偵測[7]等方
面, 初步成果可見其可行性. 未來更可進一步探討其他應用, 如 blog 文章引用關係, 以及其
他多媒體類型檔案的複製偵測, 如語音, 音樂等, 有效性及效率有待更多實驗來驗證. 
 
 
 
出席國際學術會議心得報告 
                                                             
計畫編號 NSC96-2218-E-027-003 
計畫名稱 大規模分散式網路數位內容複製偵測技術之研究 
出國人員姓名 
服務機關及職稱 
王正豪, 國立台北科技大學資訊工程系, 助理教授 
會議時間地點 Nov. 2-5, 2007, Fremont Marriott Hotel, Silicon Valley, USA 
會議名稱 The 2007 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2007) 
發表論文題目 Finding Event-Relevant Content from the Web Using a Near-Duplicate Detection Approach 
 
一、參加會議經過 
此行前往美國加州 Fremont, Silicon Valley 參加 2007 IEEE/WIC/ACM International 
Conference on Web Intelligence (WI 2007), 此次會議依然是 WI 2007 與 IAT 2007 
(Intelligent Agent Technology)共同舉行. 研討會在 Fremont Marriott Hotel舉行, 會中有來
自世界各地在 data mining, information retrieval與 machine learning相關領域專家學者約
300多人與會, 論文的接受率為 regular paper 16%, short paper 23%, 論文作者分別來自美
國, 中國, 日本, 法國, 義大利, 德國, 韓國, 瑞士, 英國, 加拿大, 香港, 澳洲, 新加坡, 
荷蘭, 希臘, 等國. 其中不少是跨單位甚至跨國的研究. 今年台灣共有 5 篇論文發表, 表
現不錯, 其中, 1 篇為高宏宇老師(NCKU)的 full paper, 另 4篇為 short paper, 分別為何建
明老師(Academia Sinica), 陳信希老師(NTU), 盧文祥老師(NCKU), 以及我們的 paper. 大
會安排了 6場 keynote/invited talks, 13個 workshops, 8場 tutorial以及許多 industry talks.  
第一天主要為 tutorial, 分別介紹 Peer-to-Peer Distributed Data Mining及Web and Text 
Mining for Opinion/Trend Analysis等主題. 同時也有 parallel workshops, 如 IWI, WPRS, 
RSS 等, 同時進行. 會中遇到中研院的老師, 如: 許鈞南老師, 廖純中老師, 以及楊凱翔
博士, 彼此討論交換意見. 下午有幾場 industry talks, 主要由 IBM Almaden Research 
Center的研究人員談有關 Information Mashups. 晚上是Welcome Reception, 除了歡迎與
會者, 也頒發了 Best paper award.  
第二天大會正式 sessions展開, 包括WI及 IAT的 keynote speech,第一場是由 Richard 
Karp (U.C.Berkeley)主講 Computer Science as a Lens on the Sciences, 他是 1985年 Turing 
Award 得主, 1994 年 ACM Fellow, 年紀已經有些長, 教科書上有名的 Rabin-Karp string 
searching algorithm 就是他提出的, 不過目前研究主要 focus 在 computational molecular 
biology等相關領域, 也是他這次 keynote speech的其中一個主題. 接下來的兩場 keynote 
分別由 Jeff. Bradshaw(Ihmc)談 Human Automation Teamwork, 以及 Dieter Fensel (U. 
Innsbruck) 談 Service Web 3.0. 會場遇到同樣台灣來的 Wen-Hsiang Lu (NCKU)及
Hung-Yu Kao (NCKU), 他們在下午分別有一篇 short paper以及 full paper要報告. 
 
